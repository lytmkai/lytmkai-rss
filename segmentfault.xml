<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[【完整回放】2022 HVV实战专题 梓源 ]]></title>    <link>https://segmentfault.com/a/1190000047494665</link>    <guid>https://segmentfault.com/a/1190000047494665</guid>    <pubDate>2025-12-22 22:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>HVV 红蓝对抗流程：教育视角下的信息安全培训<br/>引言<br/>随着信息技术的迅猛发展，网络安全成为了各个行业的重要课题。在这个背景下，HVV（红蓝对抗）作为一种有效的网络安全演练方式，越来越受到重视。红蓝对抗通过模拟攻击（红队）与防御（蓝队）的对抗，帮助组织识别安全漏洞，并提高整体安全防护水平。本文将从教育视角出发，探讨HVV红蓝对抗中的信息收集、内网横向攻击和蜜罐反制等关键流程及其在信息安全培训中的应用。<br/>一、信息收集</p><ol><li>信息收集的意义<br/>在红蓝对抗的第一阶段，信息收集是红队成功攻击的基础。这一阶段主要目的是获取目标环境的相关信息，包括网络拓扑、开放端口、服务、系统版本等。通过系统化的信息收集，红队能够制定有效的攻击策略。</li><li>教育实践中的应用<br/>在信息安全培训中，教育者可以通过模拟演练让学员掌握信息收集的技能。以下是几个推荐的培训活动：</li></ol><p>1.工具介绍与实践：教授学员使用工具如Nmap、Recon-ng等进行网络扫描和信息收集。<br/>2.案例分析：分析历史上的网络攻击案例，让学员识别信息收集如何影响攻击结果。<br/>3.小组合作：组织学员团队进行攻防对抗演练，增强协作与实践能力。</p><p>二、内网横向攻击</p><ol><li>横向攻击的概念<br/>内网横向攻击是在入侵成功后，利用已获得的权限继续渗透内网其他系统。这一过程涉及许多技术手段，比如凭证盗窃、利用信任关系等。</li><li>教育实践中的应用<br/>在内网攻击的教育培训中，可以通过设计完整的攻防演练以提高学员的意识和技能：</li></ol><p>4.红队攻击演示：由专业人员演示如何在内网内进行横向攻击，包括使用Mimikatz等工具。<br/>5.蓝队防御策略：教授蓝队如何监控和防止横向攻击，例如，通过网络流量分析和用户行为监测工具识别异常活动。<br/>6.实战演练：模拟真实环境中的内网攻击，让学员在对抗中学习和应用防御策略。</p><p>三、蜜罐反制</p><ol><li>蜜罐技术概述<br/>蜜罐是一种主动防御技术，旨在通过设置虚假的脆弱系统或服务，引导攻击者进行攻击，从而收集数据和分析其行为。这对于识别攻击模式与手法，提升防御能力具有重要意义。</li><li>教育实践中的应用<br/>在信息安全教育中，蜜罐的应用可帮助学生深化对网络攻击行为的理解：</li></ol><p>7.蜜罐的搭建与配置：手把手教导学生如何创建和配置蜜罐系统，理解其工作原理及价值。<br/>8.攻击数据分析：分析通过蜜罐收集到的攻击数据，帮助学生识别攻击者的行为模式和常用工具。<br/>9.开发反制策略：培训学员利用蜜罐数据制定更有效的网络防护措施，提高整体安全防护能力。</p><p>结语<br/>通过HVV红蓝对抗流程的全面介绍，可以看出信息安全教育在现代网络防护体系中的重要性。信息收集、内网横向攻击和蜜罐反制，这些环节既是红蓝对抗的基本组成部分，也可以有效地融入信息安全培训之中。教育者通过设计科学的教学活动，使学员能够在实践中巩固理论知识，提升自身的安全防护能力，从而更好地应对日益复杂的网络安全挑战。</p>]]></description></item><item>    <title><![CDATA[kubernetes实战与源码剖析-专享 资源999it点top ]]></title>    <link>https://segmentfault.com/a/1190000047494668</link>    <guid>https://segmentfault.com/a/1190000047494668</guid>    <pubDate>2025-12-22 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Kubernetes 实战与源码剖析：核心组件运行机制与企业级部署全解析<br/>Kubernetes（K8s）作为现代云原生架构中的核心技术之一，已经成为企业级应用部署与管理的标准。自2014年由Google开源以来，Kubernetes以其强大的容器编排能力、灵活的扩展性和高可用性，逐渐形成了一套成熟的生态体系。本文将从教育、科技、人文发展和经济等多个角度深入探讨Kubernetes的核心组件及其企业级部署所带来的变革。<br/>教育：培养未来的IT人才<br/>随着Kubernetes的普及，相关的教育培训也应运而生。开发者、运维工程师以及IT管理人员需要掌握K8s的基础知识和实践能力。越来越多的高等院校以及职业培训机构开始开设Kubernetes相关课程，帮助学生和职场人士提升技能。<br/>此外，在线教育平台如Coursera、Udemy等也提供了丰富的Kubernetes课程。Kubernetes的学习不仅促进了计算机科学的学术研究，更推动了实践与理论的结合。通过实际项目的训练，学员们能够更好地理解Kubernetes的工作机制，从而有助于未来的职场竞争力。<br/>科技：推动技术的演进与创新<br/>Kubernetes的出现大大提升了云计算技术的灵活性与可管理性。它通过抽象化基础设施，使得开发者可以专注于应用程序的开发，而不必过于关注底层环境的配置。这种分离不仅提高了开发效率，也加速了新技术的迭代与创新。<br/>在技术演进方面，Kubernetes支持微服务架构，在大规模分布式系统中表现尤为出色。通过Docker等容器技术，K8s让应用的构建、测试、交付与部署过程变得更加自动化与标准化。此外，其丰富的生态工具链，例如Helm、Istio和Prometheus等，进一步增强了Kubernetes的功能，使得开发者和运维工程师能够更高效地管理复杂的微服务环境。<br/>人文发展：影响团队合作与文化<br/>Kubernetes的 adoption 不仅局限于技术层面，它还引发了文化与团队合作的变革。随着DevOps理念的深入，K8s使得开发与运维之间的墙变得模糊，促进了跨部门的协作。团队中的开发人员可以更快速地反馈和迭代，运维人员则通过集中的管理面板和自动化的监控工具减轻了日常运维的负担。<br/>此外，Kubernetes的开源特性使得全球的开发者能够参与到生态建设中，推动了一种共享知识和资源的文化。无论是通过贡献代码、解决Bug，还是撰写文档和教程，K8s的社区能够促进人们之间的合作和交流，成为一种推动技术人文发展的力量。<br/>经济：促进企业的数字化转型<br/>在经济层面，Kubernetes的企业级应用能够显著降低IT基础设施的成本，提高资源利用率。K8s的弹性扩展能力使得企业能够根据需求进行资源的灵活配置，从而避免了资源的浪费。这对于预算有限的中小企业尤为重要，它们可以在不进行大规模投资的情况下，充分利用云计算带来的灵活性。<br/>同时，使用Kubernetes的企业能够更快速地实现产品的上线和迭代，使企业在市场竞争中保持灵活性和响应速度。这种高效的开发和运维流程，使得企业能够更好地满足客户需求，从而提升整体业务的竞争力。<br/>结论<br/>Kubernetes的核心组件和企业级部署不仅在技术领域产生了深远影响，也在教育、人文和经济层面引发了一系列变革。无论是推动技术的创新与应用，还是促进团队文化的变革、帮助企业实现数字化转型，Kubernetes都以其独特的方式在现代科技的发展进程中扮演着重要角色。<br/>在未来，随着Kubernetes生态的不断完善，我们有理由相信，将有更多的机会去探索这一技术所带来的可能性，并为相关领域的进一步发展做出贡献。</p>]]></description></item><item>    <title><![CDATA[每日一个C++知识点|对象资源传递机制 图形学爱好者Wu ]]></title>    <link>https://segmentfault.com/a/1190000047494605</link>    <guid>https://segmentfault.com/a/1190000047494605</guid>    <pubDate>2025-12-22 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>C++是一门对内存资源配置要求较高的语言，其中对象资源传递在C++开发中无处不在，下面我将在浅拷贝、深拷贝、左值右值、移动语义、完美转发这5个方面层层递进地讲解C++对象资源传递机制，争取做到知识串联，深入浅出~</p><h2>浅拷贝</h2><p>我们从一个实际场景入手：写一个Image类，存储图片的像素数据，代码如下：</p><pre><code class="cpp">#include &lt;iostream&gt;
using namespace std;

// 图片类：管理堆内存中的像素数据
class Image {
public:
    // 构造函数：分配堆内存（相当于“买快递箱装图片数据”）
    Image(int w, int h) : width(w), height(h) {
        // 每个像素占4字节（RGBA），分配一大块堆内存
        pixels = new char[width * height * 4]; 
        cout &lt;&lt; "构造函数：分配内存，图片尺寸：" &lt;&lt; width &lt;&lt; "x" &lt;&lt; height &lt;&lt; endl;
    }

    // 析构函数：释放堆内存（相当于“扔掉快递箱”）
    ~Image() {
        if (pixels != nullptr) {
            delete[] pixels;
            cout &lt;&lt; "析构函数：释放了内存" &lt;&lt; endl;
        }
    }

private:
    int width, height;
    char* pixels; // 指向像素数据的指针（核心资源）
};

int main() {
    Image img1(1000, 1000); // 创建1000x1000的图片
    Image img2 = img1;      // 拷贝img1到img2
    return 0;
}</code></pre><p>这段代码看起来没问题，却会触发内存错误。原因就是编译器默认拷贝方式是<br/><code>浅拷贝</code></p><p>那么什么是浅拷贝呢？浅拷贝只拷贝成员变量的值，不拷贝资源本身，会造成两个对象共享同一块堆内存，相当于两个快递单号指向同一个快递箱。当多个对象共享资源，析构函数运行时就会崩溃。在上述代码中只把<code>img1</code>的<code>pixels</code>指针地址复制给<code>img2</code>,没有把资源本身复制一份，导致程序结束后<code>析构时双重释放</code>，<code>img2</code>先析构释放内存，<code>img1</code>析构时又去释放已经被释放的内存，直接崩溃</p><h2>深拷贝</h2><p>那么怎么解决浅拷贝带来的程序崩溃问题呢？一个简单的方法是使用<code>深拷贝</code>。深拷贝不仅拷贝成员变量，还为新对象重新分配资源并复制数据，使每个对象拥有独立资源，提升安全性。下面我们给<code>Image</code>类添加深拷贝构造函数（在原有代码的基础上直接添加，其他地方保持不变）：</p><pre><code class="cpp">// 深拷贝构造函数：参数是const左值引用（const 类名&amp;）
Image(const Image&amp; other) {
    // 第一步：复制基础属性
    width = other.width;
    height = other.height;
    // 第二步：关键！重新分配新的堆内存（买新快递箱）
    pixels = new char[width * height * 4]; 
    // （实际开发中会复制像素数据，这里重点是“新分配内存”）
    cout &lt;&lt; "深拷贝构造函数：新分配了内存" &lt;&lt; endl;
}</code></pre><p>以上拷贝构造函数会在编译器中自动执行，因而无需在main函数添加。</p><p>此时再运行代码，<code>img1</code>和<code>img2</code>各自拥有独立内存，程序正常结束。但是深拷贝的内存分配和数据复制会带来巨大性能开销，如果是为了处理临时数据而产生这么大的开销，有点浪费资源。那么我们可不可以在深拷贝完成之后对临时数据进行删除呢？</p><p>假设我们有一个函数，生成一张临时的<code>Image</code>对象:</p><pre><code class="cpp">// 返回临时Image对象（无名字，是“即将销毁”的右值）
Image createWhiteImage(int w, int h) {
    Image temp(w, h);
    return temp;
}</code></pre><p>因为<code>Image temp(w, h)</code>是在函数里实现的，也就是在栈内实现的，所以对象在函数执行时可以自动创建，函数运行结束后自动释放销毁;</p><p>再用深拷贝接收这个临时对象：</p><pre><code class="cpp">Image img3 = createWhiteImage(2000, 2000); // 深拷贝：耗时耗内存</code></pre><p>这样就实现了在深拷贝完成之后对临时数据进行删除，但是这就像 “把快递里的东西复制一份，再把原快递箱扔掉”，完全没必要，这时候移动语义就该登场了。</p><h2>左值和右值</h2><p>在了解移动语义之前，我们需要了解一个重要的概念——<code>左值</code>和<code>右值</code></p><p>左值通常在等号左边，右值通常在等号右边，<code>但是</code>，<code>左值并非是在等号左边的对象，右值也并非是在等号右边的对象</code></p><p>左值是有名字、能取地址的对象，是持久存在 的对象。</p><p>右值是无名字、不能取地址的临时对象，是即将销毁的对象。</p><p>在上述代码中，<code>img1</code>是左值，<code>int a = 10</code>;中的<code>a</code>是左值;<code>createWhiteImage()</code>的返回值是右值。了解左值和右值的基本概念后，我们就能在移动语义中使用它们了~</p><h2>移动语义</h2><p>移动语义本质是转移右值的资源所有权，而非执行资源拷贝，所以可以达到减少资源浪费的效果</p><h3>移动构造函数</h3><p>要实现移动语义，需要给Image类添加移动构造函数：</p><pre><code class="cpp">// 移动构造函数：参数是右值引用（类名&amp;&amp;），通常加noexcept
Image(Image&amp;&amp; other) noexcept {
    // 第一步：“偷”走源对象的资源（仅复制指针地址，无内存分配）
    width = other.width;
    height = other.height;
    pixels = other.pixels;

    // 第二步：关键！将源对象置为空（作废原快递单号，避免析构冲突）
    other.pixels = nullptr;
    other.width = 0;
    other.height = 0;

    cout &lt;&lt; "移动构造函数：直接转移资源，无内存分配！" &lt;&lt; endl;
}</code></pre><p>此时再接收临时对象：</p><pre><code class="cpp">Image img3 = createWhiteImage(2000, 2000); // 触发移动构造，瞬间完成</code></pre><p>这就像 “直接把快递箱的地址改成自己的，不用复制里面的东西”，性能直接拉满~</p><h3>std::move</h3><p><code>std::move</code>是把左值 “伪装” 成右值的小工具，如果想把左值的资源转移给其他对象，可以用<code>std::move</code></p><pre><code class="cpp">Image img4(1500, 1500); // 左值
Image img5 = std::move(img4); // 触发移动构造，img4变为空</code></pre><p>注意：它只是强制转换类型，不会真的移动数据</p><h2>完美转发</h2><p>移动语义解决了临时对象的拷贝问题，但在模板函数中，会遇到新问题：参数的左值和右值属性会丢失。</p><p>如下代码所示：</p><pre><code class="cpp">template &lt;typename T&gt;
void wrapper(T x) {
    Image img = x; // 无论x是左值还是右值，都触发深拷贝
}

// 调用：传入右值，却还是深拷贝
wrapper(createWhiteImage(1000, 1000));</code></pre><p>由于模板参数x是拷贝后的对象，已经变成了左值，丢失了原来的右值属性</p><p>这时候需要用到<code>完美转发</code>来解决上述问题，<code>完美转发</code>在模板中保留参数的左值 / 右值属性，它需要两个核心要素：<code>万能引用</code>和<code>std::forward</code></p><h3>万能引用</h3><p><code>T&amp;&amp;</code>是万能引用符号，仅在模板中使用，能绑定左值或右值</p><h3>std::forward</h3><p><code>std::forward</code>：根据参数的原始类型，转发为左值或右值</p><p>用代码举例如下：</p><pre><code class="cpp">template &lt;typename T&gt;
void wrapper(T&amp;&amp; x) { // 万能引用
    Image img = std::forward&lt;T&gt;(x); // 完美转发：保留属性
}

// 测试：属性保留
Image img6(800, 800);
wrapper(img6); // 传入左值，触发深拷贝（符合预期）
wrapper(createWhiteImage(800, 800)); // 传入右值，触发移动构造</code></pre><p>完美转发就像 “快递包装不拆，直接原封不动转发”，确保参数的属性不丢失。</p><h2>总结</h2><p>以上便是C++对象资源传递机制的主要内容，从浅拷贝、深拷贝、左值右值、移动语义、完美转发层层递进，如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494607" alt="" title=""/></p><ol><li>浅拷贝：绝对禁用（除非类无动态资源），会导致内存崩溃。</li><li>深拷贝：解决浅拷贝的内存崩溃，但需要更多内存开销。</li><li>移动语义：处理右值的性能方案，用资源转移代替拷贝，std::move可把左值转为右值。</li><li>完美转发：在模板中使用，保障移动语义在参数传递中生效。</li></ol><p>如果这篇文章文章对你有用的话, 欢迎点赞收藏加关注哦~</p>]]></description></item><item>    <title><![CDATA[硬件+软件协同交付怎么落地？2025 软硬件项目管理工具对比 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047494367</link>    <guid>https://segmentfault.com/a/1190000047494367</guid>    <pubDate>2025-12-22 20:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文深度测评 ONES、Polarion、Codebeamer、Helix ALM、Jama Connect、SpiraTeam、Nuxeo、Hansoft、Nifty 等软硬件协同管理工具，帮助团队打通需求-缺陷-版本管理全流程。</p><h2>软硬件协同交付的难点</h2><p>在复杂系统研发里，软件团队习惯以迭代节奏驱动交付，硬件团队则以阶段评审与变更控制驱动质量。两种节奏并行并不矛盾，真正让项目失控的往往是：软硬件共享的关键对象没有被同一套机制锁定、追溯与复现。</p><p>我把常见痛点归为四类，但每一类背后都指向同一个本质——“对象与链路的缺失”：</p><ul><li>需求追溯断点：系统需求 → 子系统需求 → 软硬件分解需求 → 设计/实现 → 测试验证之间缺少稳定链接，影响分析只能靠经验与会议。</li><li>变更与版本对齐困难：硬件 ECR/ECN、固件/软件版本、测试基线、发布包（制品）各自为政，出现“同名不同物”，最后反噬质量与周期。</li><li>验证闭环效率低：测试结果停留在报告，没回到需求与缺陷对象上沉淀，回归成本上升，质量趋势难量化。</li><li>组织协同摩擦大：系统工程、硬件、嵌入式、应用、测试、供应商使用不同系统，信息在 IM、邮件、表格漂移，责任边界与决策记录不清晰。</li></ul><p>因此，讨论“软硬件项目管理工具”时，我更关心的不是看板够不够炫，而是它能否承载一条可运行的 ALM 数字主线（Digital Thread）：把需求管理、缺陷管理、版本管理、制品管理放进同一套“对象—关系—基线—证据”的体系。</p><h2>软硬件协同管理工具盘点与对比</h2><h4>ONES——集成化、本地化与软硬件协同落地的现实路径</h4><p>核心功能与定位：<a href="https://link.segmentfault.com/?enc=e5avVvM9zgTPFYHt%2BxXqzg%3D%3D.pnHP2SxjdQvt6WKOD2GO%2Bg%3D%3D" rel="nofollow" target="_blank">ONES</a> 研发管理平台覆盖流程、进度、协作、效能改进等，并支持 Agile / Waterfall / Hybrid 乃至 IPD 等不同方法论在同一套流程与数据上协同；同时提供企业级权限与审计等治理能力，适配云或本地部署的合规要求。</p><p>适用场景：</p><ul><li>软硬件并行交付需要统一节奏与透明化协同；</li><li>希望降低工具碎片化与集成成本，用一体化平台先跑通“需求—执行—测试—交付协作”的主干，再逐步扩展工具链；</li><li>在硬件研发管理上，ONES 能把硬件项目最关键的“计划—里程碑—依赖—变更协同”做成可执行的工程节奏；</li><li>在软件研发管理上，ONES 更接近 ALM 的“闭环能力”，强调覆盖 需求管理、路线图、迭代（Sprint）管理、质量控制、发布管理等生命周期关键环节。</li></ul><p>优势亮点：</p><ul><li>一体化降低推广成本：很多组织不是“工具买不起”，而是“流程推不动”。平台集成度越高，PMO 越容易建立统一模板、度量口径与跨团队协作机制。</li><li>本地化落地更可控：在私有化、合规、培训与持续运营上，本地化支持往往决定了工具是否能成为组织能力的一部分。</li></ul><p>局限性：</p><p>若组织追求某些系统工程专用能力的“极致深度”，仍建议用 POC 验证追溯粒度、复杂权限与跨域集成上限。<br/><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdhI10" alt="" title=""/></p><h4>Polarion（Siemens）</h4><p>核心功能与定位：强调在统一平台定义、构建、测试与管理复杂系统，并保持端到端可追溯与可视化；同时强调面向审计/合规的追溯与变更控制能力。</p><p>适用场景：当你的组织需要系统工程级追溯、合规审计证据、跨团队协作的“单一事实源（Single Source of Truth）”，Polarion 更容易体现价值。</p><p>优势亮点：</p><ul><li>追溯不是报表，而是决策机制：影响分析从“开会问人”变成“在关系图上确认范围”，这会直接改变变更治理效率。</li><li>适合做组织级模板化：把 IPD 评审点、基线策略、交付证据输出固化为可复用资产，越大规模越有复利。</li></ul><p>局限与不足：</p><p>推行成本主要在“治理”而非“安装”：数据模型、权限边界、评审门禁要先统一。<br/>如果组织还未建立基线纪律，平台越强，反而越暴露管理短板——这不是坏事，但要有心理预期。<br/><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnnys" alt="" title="" loading="lazy"/></p><h4>Codebeamer（PTC）</h4><p>核心功能与定位：官方强调“需求管理 + 风险 + 测试与验证 + 产品线管理能力”在一个平台内，并主打“连接式 ALM”；同时提到可与 CI/CD、源码、PLM 等工具联动，服务更大的数字主线。</p><p>适用场景：多版本并行、变体/产品线工程、受监管行业（质量与合规压力高），以及希望把 ALM 放进更大的数字主线架构的组织。</p><p>优势亮点：</p><ul><li>把风险拉回主线：风险与需求、验证联动，比“表格式风险管理”更能落地到工程动作。</li><li>可配置性适合复杂流程：对成熟 PMO/质量体系团队，能把流程沉淀为平台能力。</li><li>局限与不足：</li><li>可配置意味着需要能力：没有流程架构与平台治理能力时，容易“配置过度/流程过重”。</li></ul><p>建议从一个产品线或一个系统域试点，把追溯粒度、基线与审批机制定住，再扩张。</p><h4>Helix ALM（Perforce）</h4><p>核心功能与定位：以需求、测试、缺陷/问题为核心对象形成追溯闭环；适合把验证结果与缺陷处理纳入同一条主线。</p><p>适用场景：嵌入式/工业软件团队想先把“需求—测试—缺陷”跑通，并逐步接入自动化验证与证据沉淀。</p><p>优势亮点：当验证证据能够回链到需求与缺陷，质量趋势才有统计意义，回归成本也更可控（尤其是多版本并行时）。</p><p>局限与不足：在“跨硬件变更、供应商协作、复杂门禁”上通常还要补集成与治理设计，才能形成真正端到端数字主线。</p><h4>Jama Connect（Jama Software）</h4><p>核心功能与定位：偏“需求与评审协作 + 追溯与影响分析”。其官方特性页明确 Review Center 用于实时协作评审与集中管理评审意见/批准记录。</p><p>适用场景：需求评审频繁、跨组织对齐成本高、且希望把评审记录纳入审计证据的系统工程团队（含供应商与外协参与）。</p><p>优势亮点：把评审从会议纪要变成结构化证据：对合规行业来说，评审记录往往就是审计准备工作量的大头。在 Jama 的公开内容中，也能看到关于缩短评审周期与降低审计准备时间的案例口径（建议按自身流程用 POC 验证）。</p><p>局限与不足：Jama 更强在需求与评审侧；要形成“从需求到制品”的发布闭环，通常仍需与开发、构建、制品库等工具链组合。</p><h4>SpiraTeam（Inflectra）</h4><p>核心功能与定位：倾向把需求、测试、计划、风险与缺陷整合为“基础 ALM 闭环”。<br/>适用场景：中型团队希望较快获得“闭环感”：需求能落到测试、缺陷能回到需求、发布有证据可查。</p><p>优势亮点：对“先跑通、再优化”的组织更友好：先建立对象体系与追溯，再谈规模化与深度集成。</p><p>局限与不足：面对多组织、多供应商、强合规的大规模场景时，数据模型/权限/流程会更快触顶，需要更强平台或更成熟的组合方案。</p><h4>Hansoft（规划侧更强）</h4><p>核心功能与定位：偏研发计划、组合与节奏治理，适合把多团队的里程碑与迭代节奏拉齐。</p><p>适用场景：硬件阶段门禁（EVT/DVT/PVT 等）与软件迭代并行，需要“节奏对齐 + 资源协调”的 PMO。</p><p>局限与不足：Hansoft 更像推进层补强；审计级追溯与证据链仍建议回到 ALM 主干。</p><p>ALM/系统工程工具的关键，不是“多功能”，而是能否把追溯链与基线机制做成组织级的默认动作。</p><h4>Nuxeo（Hyland Nuxeo）</h4><p>核心功能与定位：更像内容服务平台（Content Services）与数字资产底座；官方文档明确其提供原生 REST API，用于远程集成与构建自定义界面/能力。</p><p>适用场景：</p><p>大量规范、设计文档、验证报告、供应商资料需要版本/权限/流程/审计统一；<br/>希望把“证据库”与 ALM 主线关联，形成可审计的交付链。</p><p>优势亮点：把证据当资产管理：当组织进入合规与规模化交付阶段，证据不是“交付后补材料”，而应该伴随对象自然生成并归档。</p><p>局限与不足：它不替代 ALM 主线；如果证据无法回链到需求/测试/缺陷对象，最终仍会变成“更大的资料库”。</p><p>内容平台的价值在“证据可治理、可复用”，但前提是证据必须回链到工程对象。</p><h2>2025 的演进趋势与选型建议</h2><p><strong>趋势判断（你可以用作年度规划的判断框架）：</strong></p><ul><li>ALM 更强调单一事实源与合规追溯</li><li>“需求评审 + 证据链”产品化加速</li><li>数字主线从口号走向工程落地</li><li>本地化与可运营性成为现实权重</li></ul><p>不同规模与行业的建议（给硬件研发经理 / 系统工程 / PMO / 研发总监）：</p><ul><li>中小团队（几十人）：优先“最小闭环可运行”，一体化更容易落地；目标是把需求、缺陷、测试与发布证据先连起来。</li><li>中大型组织（数百到数千人）：采用“三层架构”，系统记录层先稳定，再用推进层做组合治理，用证据层做资产沉淀；避免一次性铺开导致推广失败。</li><li>强合规行业（汽车/医疗/工业控制等）：把“追溯、基线、证据链”放在第一优先级，推进效率排第二；合规不是文档工作量，而是工程对象是否可审计、可复现。</li></ul><p>硬件研发数字化转型的本质，是把系统工程与 IPD 的治理逻辑固化为“可复用的流程资产 + 可审计的数据资产”。当你用 ALM 数字主线把需求、缺陷、版本、制品锁进同一条链，软硬件协同交付才真正具备规模化复制能力。</p><h2>FAQ</h2><p>Q1：软硬件协同交付到底需要哪些“软件能力”？<br/>A：至少要具备需求管理、缺陷管理、版本管理、制品管理，并用测试管理与变更管理把证据链闭环，比如 ONES 等；协作工具只解决推进，不能替代审计级追溯。</p><p>Q2：一体化平台和最佳组合怎么选？<br/>A：如果你们的主矛盾是“推广与统一口径”，一体化更稳；如果你们有强集成能力与成熟治理体系，最佳组合可以在特定环节做到极致，但维护成本更高。</p><p>Q3：为什么我用了很多工具，交付还是混乱？<br/>A：通常是“基线策略缺失 + 追溯粒度不统一”。没有基线，版本与制品无法复现；没有统一追溯，影响分析与审计证据只能靠人扛。</p>]]></description></item><item>    <title><![CDATA[Verilog端口类型解析 星星上的柳树 ]]></title>    <link>https://segmentfault.com/a/1190000047494374</link>    <guid>https://segmentfault.com/a/1190000047494374</guid>    <pubDate>2025-12-22 20:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>“理解端口类型，是Verilog模块设计的关键。”<br/>在Verilog设计中，端口是模块与外界交互的桥梁。不同类型的端口——输入、输出与双向——在数据流向与信号驱动方式上有着严格的规则。若定义不当，不仅会引发编译错误，还可能导致仿真行为与硬件实现不一致。掌握Verilog端口类型的使用原则，能帮助设计者构建结构清晰、逻辑可靠的电路系统。</p><p>1、端口类型与信号流向Verilog模块的端口可分为三类：输入端口（input）、输出端口（output）和双向端口（inout）。它们决定了信号在模块间的流动方向，并影响端口的数据类型与驱动方式。输入端口：信号从外部流入模块内部。输出端口：模块内部生成信号并输出至外部。双向端口：信号可在模块间双向流动，常用于总线接口。<br/>端口类型不仅定义通信方向，更隐含了信号驱动规则：哪些信号可以被过程赋值、哪些必须通过连续赋值驱动。</p><p>2、输入端口：永远是nets在Verilog中，输入端口（input）始终被视为net类型。这意味着输入端口不能在过程块（always或initial中）被赋值，而只能接收外部驱动。<br/>例如：module adder (  input [3:0] a, b,  output [4:0] sum);  assign sum = a + b;endmodule<br/>这里，输入端口a与b都是nets类型，用于连接外部信号源。Verilog之所以规定输入端口为net，是因为它们仅承担信号传递功能，不具备存储能力。<br/>注意：若在模块内试图对输入端口赋值，如a = 1;，编译器会报错——因为nets不能在过程块中驱动。</p><p>3、输出端口：由驱动方式决定类型输出端口（output）的类型取决于其驱动方式，这一特性是Verilog端口规则的核心。若输出由过程块（如always）驱动，则必须声明为变量（reg或logic）类型。若输出由连续赋值（assign）语句驱动，则应为net类型。<br/>示例1（连续赋值驱动）：module and_gate (  input a, b,  output y);  assign y = a &amp; b;  // y为net类型endmodule<br/>示例2（过程赋值驱动）：module dff (  input clk, d,  output reg q);  always @(posedge clk)    q &lt;= d;           // q为变量类型endmodule<br/>这种区分源于Verilog的设计哲学：连续赋值用于组合逻辑，过程赋值用于时序逻辑。若错误声明输出类型，会导致编译器拒绝执行或仿真异常。例如，在过程块中驱动一个net类型输出，将产生非法赋值错误。</p><p>4、双向端口：始终为nets双向端口（inout）用于信号在模块间的双向流动，例如数据总线或IO接口。Verilog规定，双向端口必须为net类型，以确保信号的共享与驱动一致性。<br/>示例：module io_buffer (  inout wire data);  assign data = enable ? out_data : 1'bz; // 高阻态控制输出endmodule<br/>在此例中，data可根据控制信号enable决定是否驱动输出。当enable关闭时，data进入高阻态（z），允许外部模块驱动该信号。<br/>双向端口通常在FPGA或ASIC设计中用于总线系统，要求信号线在不同模块之间协调驱动，否则会出现信号冲突。</p><p>5、常见错误与调试建议Verilog端口类型规则明确，但新手在实践中常犯以下错误。过程赋值给netsalways @(posedge clk)  y = d; // 错误：nets不能在过程块中赋值正确做法是将输出定义为变量：output reg y;<br/>输出端口类型声明错误如果输出端口未被过程块驱动，却声明为reg，也会引发语义冲突。output reg y;  assign y = a &amp; b; // 错误：reg不能由assign驱动正确写法应为：output y;assign y = a &amp; b;<br/>双向端口错误声明为变量双向端口必须是net，否则多个驱动器会引发逻辑冲突：inout reg data; // 错误应改为：inout wire data;<br/>端口方向混淆部分初学者在设计模块时忽略信号方向，导致数据流不一致。应确保上层模块的输出连接下层模块的输入，双向端口则需明确高阻态控制。</p><p>6、Verilog-2001的改进：端口内联声明在Verilog-2001标准中，允许在模块头部直接定义端口类型与方向：module adder (  input wire [3:0] a, b,  output reg [4:0] sum);<br/>这种写法既简洁又直观，避免了早期Verilog-1995中必须分两步声明的繁琐结构。此外，现代工具也支持logic关键字替代reg，使代码兼容性更强。</p><p>7、端口类型选择的设计原则总结Verilog端口类型规则：<br/><img width="723" height="166" referrerpolicy="no-referrer" src="/img/bVdnrCr" alt="" title=""/><br/>设计时的关键思路是：谁驱动信号，就决定信号的类型。组合逻辑使用nets，时序逻辑使用reg，双向接口保持net结构。<br/>端口是模块交互的语言，类型是这门语言的语法。理解输入、输出与双向端口的类型规则，是写出可综合、高可靠Verilog代码的基础。掌握端口类型与驱动的关系，就能在设计初期避免许多低级错误，让电路在仿真与硬件中都能“说得通”。</p><pre><code>                    END</code></pre><p>《EDA网院》出品 · 与全球工程师一起探索芯片的世界</p>]]></description></item><item>    <title><![CDATA[Linux 麒麟系统安装 gcc-7.3.0 rpm 包步骤 无邪的课本 ]]></title>    <link>https://segmentfault.com/a/1190000047494389</link>    <guid>https://segmentfault.com/a/1190000047494389</guid>    <pubDate>2025-12-22 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><ol><li>找到 rpm 文件</li></ol><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=BKVJwZz2pP%2Bw0luTfFd%2FAQ%3D%3D.4KR6dxHrr4gqbg15fMrohDVrTuGTabJuCRSj9Fe3Jyy7K5p054K%2FRYq7t3Qs5roY" rel="nofollow" title="https://pan.quark.cn/s/9aac910b9f81" target="_blank">https://pan.quark.cn/s/9aac910b9f81</a>，下载完一般在 <strong>下载</strong>​ 目录，文件名：</p><pre><code>gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p><p>先确认一下：</p><pre><code>ls ~/下载/gcc-7.3.0*</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果是英文环境：</p><pre><code>ls ~/Downloads/gcc-7.3.0*</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>2. 打开终端</h3><p>右键桌面 → “打开终端”，或者按 <code>Ctrl + Alt + T</code>。</p><ul><li><ul><li>*</li></ul></li></ul><h3>3. 切换到 rpm 所在目录</h3><pre><code>cd ~/下载</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>英文路径：</p><pre><code>cd ~/Downloads</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>4. 检查是否已经安装了 gcc</h3><p>先试试：</p><pre><code>gcc --version</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果提示 “command not found” 就是没装；如果有版本号，想换版本就继续往下看。</p><ul><li><ul><li>*</li></ul></li></ul><h3>5. 安装 rpm 包</h3><p><strong>推荐方法</strong>（会自动装依赖）：</p><pre><code>sudo yum install ./gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>注意 <code>./</code> 不要漏，表示安装当前目录的文件。</p><p>如果用 rpm 直接装（不推荐，容易缺依赖）：</p><pre><code>sudo rpm -ivh gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果报错缺少依赖，就用 yum 补包，比如：</p><pre><code>sudo yum install glibc-devel</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>6. 验证安装结果</h3><p>装完后运行：</p><pre><code>gcc --version</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>看到类似：</p><pre><code>gcc (Kylin) 7.3.0</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>说明安装成功。</p><ul><li><ul><li>*</li></ul></li></ul><h3>7. 常见问题</h3><ul><li><strong>权限不够</strong>：命令前加 <code>sudo</code>。</li><li><strong>依赖缺失</strong>：尽量用 <code>yum install</code> 安装 rpm 包，让系统自己找依赖。</li><li><p><strong>已有旧版本</strong>：可先卸载：</p><pre><code>sudo yum remove gcc</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p></li><li><strong>安装后命令找不到</strong>：关闭终端重新打开，或执行 <code>source ~/.bashrc</code>。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[推荐用于制造业的设备智能助手有哪些核心功能与应用场景？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047494197</link>    <guid>https://segmentfault.com/a/1190000047494197</guid>    <pubDate>2025-12-22 19:05:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>设备智能助手的定义与核心功能<br/>在现代制造业转型升级的关键阶段，人工智能技术的引入正在重构生产管理的智能化水平。设备智能助手作为这一趋势的核心产物，本质上是一种基于人工智能技术的生产辅助系统，它能够通过多模态感知、知识推理和自然语言交互，为生产管理者、工艺工程师和一线操作人员提供实时决策支持和问题解决方案。<br/>设备智能助手的核心价值在于其具备跨领域知识整合能力和实时响应能力，它不仅仅是一个工具，更是制造业知识体系的数字化载体。例如，在注塑生产线管理中，智能助手可以同时理解材料特性、模具设计、温度控制、设备运行数据等多维度信息，实现从“经验驱动”到“数据驱动”的管理范式转变。<br/>其功能体系主要包括：<br/>即时问题诊断：面对设备故障或生产异常时，系统可在数秒内完成多维度分析，提供从简单排查到复杂修复的建议；<br/>工艺优化指导：基于历史数据和行业最佳实践，推荐参数组合并预测效果；<br/>质量标准解读：针对模糊或争议的检测标准，提供权威解释和操作参考；<br/>动态学习能力：通过持续交互不断优化知识库，适应不同企业的个性化需求。<br/>设备智能助手在制造业中的关键作用<br/>在制造业的生产现场中，设备智能助手扮演着“技术参谋”与“生产管家”双重角色。它不仅提高了生产效率，还显著降低了因设备异常、工艺失误和质量波动所导致的生产风险与成本。<br/>提升响应速度是智能助手带来的最直观改变。当生产线突发异常时，传统响应流程可能需要等待技术专家或查阅大量文档，而智能助手能够在几分钟内提供解决方案。<br/>增强决策科学性也是其重要优势。设备智能助手通过融合实时数据与知识图谱，能够为复杂场景下的工艺调整、设备调度和质量控制提供可靠依据。例如，在焊接工艺中，系统可根据材料特性实时推荐最佳参数，显著减少试错成本。<br/>此外，智能助手还促进了知识沉淀与传承，将原本分散在资深工程师经验中的专业知识系统化、结构化，避免了人才流失对企业技术能力的影响。<br/>设备智能助手的应用案例<br/>一、注塑生产线的智能优化案例<br/>，2025年在广东某企业案例中，系统在检测到产品出现翘曲后，通过算法推荐改用稍低的熔体温度并延长冷却时间，使产品合格率从87%提升至96%。同时，系统还提供了预期效果分析，帮助企业评估调整后的风险。<br/>二、焊接工艺的智能参数推荐<br/>在某新能源汽车电池生产车间，设备智能助手被用于焊接工艺管理。面对不同材料的新产品导入，系统能够根据材料特性自动推荐焊接参数，并提供前期调试建议，缩短工艺开发周期。在2025年的测试中，系统对新导入的某型号电池壳体的焊接参数推荐准确率达到92%，不仅大幅减少了工人的调试时间，还显著提升了焊接质量的一致性。<br/>三、设备操作标准化与培训<br/>广域铭岛Geega平台在成都汽车焊装车间的设备智能助手将焊接操作转化为 12项关键参数，实时抓拍工人的操作动作，给出评分和优化建议。新员工通过AI教练的指导，操作合格率迅速提升至 90%，接近老师傅水平。培训周期缩短 70%，设备操作错误率降低 80%。</p>]]></description></item><item>    <title><![CDATA[PMO实战：AI研发效能度量（DORA×SPACE）路线图 PM老周 ]]></title>    <link>https://segmentfault.com/a/1190000047494227</link>    <guid>https://segmentfault.com/a/1190000047494227</guid>    <pubDate>2025-12-22 19:04:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>DORA 2025 报告指出：AI 采用率上升可能伴随吞吐与稳定波动，根因在于交付基本功与治理护栏没跟上。本文用 DORA×SPACE 给 PMO 一套 AI 研发效能度量路线图：先对齐口径，再做可对照试点，最后规模化治理，并说明如何把 AI 放进研发管理流程、跑成持续改进闭环。</p><h4>阅读本文你将获得：</h4><ul><li>一套可复用的 AI 研发效能度量指标体系：DORA（结果）× SPACE（机制）</li><li>一条从 0 到规模化的 PMO 路线图（含试点实验与护栏）</li><li>一张“症状—机制—指标”映射思路，避免只谈使用率与工时</li><li>一个“把 AI 放进流程”的落地方式：以 <a href="https://link.segmentfault.com/?enc=RN0lj9059UTCqWIrIkQWpA%3D%3D.Pq34Tkbdigs7gcyvtndSGDt3o6AgPFK5%2Fk%2BQ9UpNx9A%3D" rel="nofollow" target="_blank">ONES Copilot</a> 为例（不绑定工具，可替换）</li></ul><h2>AI 研发效能度量、DORA、SPACE 到底是什么</h2><p><strong>1. AI 研发效能度量是什么？</strong></p><p>一句话：用可验证的数据衡量“AI 是否让交付更快更稳、更可持续”，并据此驱动改进。它不等于“AI 使用次数”，也不等于“节省工时”，而是把 AI 放进端到端交付系统，衡量系统结果与系统机制。</p><p><strong>2. DORA 四项指标（软件交付性能）</strong></p><p>DORA 给出衡量软件交付结果的“四项关键指标（Four Keys）”。</p><ul><li>部署频率（Deployment Frequency）</li><li>变更前置时间（Lead Time for Changes）</li><li>变更失败率（Change Failure Rate）</li><li>故障恢复时间（Time to Restore Service）</li></ul><p><strong>3. SPACE 五维（开发者生产力）</strong></p><p>SPACE 强调：生产力不能用单一指标定义，需要从五个维度组合观测：满意度与幸福感、绩效、活动、沟通协作、效率与心流。</p><p>总结一下，从治理语言翻译：DORA 看“交付结果”，SPACE 解释“为什么结果变成这样”。</p><h2>方法论：PMO 的 DORA×SPACE 路线图（从0到规模化）</h2><p>我在不少企业的复盘会上听过类似对话：研发说“写得更快了”，测试说“回归更重了”，运维说“变更更频繁但故障没少”，而 PMO 最难回答的是：“领导问 ROI，我们只剩使用次数。”</p><p>很多时候，问题不在 AI，而在落地方式——把 AI 当成 IDE 插件，只能优化局部；PMO 关心的是端到端交付系统。更有效的做法，是把 AI 放进研发管理流程里：让它落在“工作项、文档、动态总结、项目数据洞察”这些可治理、可追溯的对象上。比如 ONES Copilot，就是围绕这些对象提供智能创建工作项、文档生成、总结动态、筛选查找与数据洞察等能力，并强调透明、负责、可控的原则。</p><h4>路线图总览（一屏版）</h4><ul><li>对齐目标与边界</li><li>建立指标字典（DORA×SPACE×AI三层）</li><li>试点做成实验（基线/对照/护栏/停机条件）</li><li>规模化治理（权限、透明追溯、质量门禁）</li><li>变成持续改进引擎（洞察→决策→行动闭环）</li></ul><h4>第 1 阶段：对齐目标与边界（2–4 周）</h4><p>AI 不是“买工具项目”，而是“交付系统再设计项目”。PMO 先把三件事写清楚：<br/>北极星目标（建议一条就够）：例如，在不提高变更失败率的前提下，将核心服务 Lead Time 缩短 30%。</p><ul><li>度量对象边界：按服务/产品线拆分，避免大盘混算。</li><li>AI 治理边界（先立护栏）：把“责任归属、权限控制、透明追溯”写进章程。</li></ul><p>ONES Copilot 的原则表述（透明优先、负责、可控、人类监督与审查）很适合直接转译成 PMO 的治理条款：关键输出必须经人类评审，权限遵从所有者设定，动作通过日志可追踪。</p><p>这一步看似慢，但它决定后面你能不能把数据说清、把风险控住。</p><h4>第 2 阶段：建立指标字典（4–6 周）</h4><p>AI 研发效能度量最怕“指标很全、口径在吵”。建议分三层建字典：</p><p><strong>2.1 结果层：DORA 四项（速度×稳定）</strong></p><p>先把 DORA 四项跑起来，并统一口径：</p><ul><li>Lead Time 起点/终点：提交→合入→发布？</li><li>部署频率：只算生产成功发布还是包含灰度？</li><li>失败定义：回滚算不算、事故等级如何映射？</li></ul><p>DORA 官方对“四项指标”的定义与定位非常清楚，适合作为口径基准。</p><p><strong>2.2 机制层：SPACE 五维（解释原因）</strong></p><p>用 SPACE 解释 DORA 的变化，优先选“低摩擦、可持续”的采集方式：</p><ul><li>S：双月脉搏问卷（工具摩擦、认知负荷、信任与焦虑）</li><li>A：PR 周期、评审等待、变更批次</li><li>C：评审往返轮次、跨团队依赖、返工原因</li><li>E：WIP、等待占比、被打断次数</li><li>P：与产品结果/OKR挂钩，但避免落到个人产出考核</li></ul><p>SPACE 的五维定义与“不要用单指标衡量生产力”的主张，是这层的理论底座。</p><p><strong>2.3 AI 层：从“使用率”升级为“贡献率”</strong></p><p>把 AI 指标拆成三类，避免“热闹但无用”：</p><ul><li>Leading（采用与熟练度）：覆盖哪些工作类型（需求/文档/排障/测试/编码）、有效采纳率</li><li>Guardrail（风险护栏）：AI 相关缺陷占比、安全告警趋势、团队信任度</li><li>Lagging（交付影响）：最终回扣 DORA 四项趋势</li></ul><p>如果你的 AI 能把动作留痕，PMO 的口径工作会轻很多。以 ONES Copilot 为例，它强调所有生成结果可通过日志与标记追踪，动作被详细记录，并要求关键输出经人类评审才进入协作流程——这类机制能显著降低“AI 到底改了什么”的争议，让复盘更像治理而不是辩论。<br/><img width="723" height="422" referrerpolicy="no-referrer" src="/img/bVdnrz4" alt="" title=""/></p><h4>第 3 阶段：试点与实验设计（6–12 周）</h4><ul><li>试点要像实验：有基线、有对照、有护栏、有停机条件。推荐从“管理链路低风险闭环”先赢一仗</li><li>智能创建工作项：把原始反馈/会议纪要转成结构化工作项</li><li>文档生成与优化：减少知识沉没，提高可复用性</li><li>总结动态与相似工单：让评审与决策基于事实</li><li>自然语言筛选与查找：降低检索成本，把信息流做顺</li></ul><p>这些能力与对象（工作项、文档、动态、数据洞察）天然更利于 PMO 度量与治理。</p><p><strong>试点章程建议包含 5 个硬要素</strong></p><ul><li>基线期：2–4 周先跑通数据，识别自然波动</li><li>成功标准：Lead Time ↓，且 CFR 不上升（或可控下降）</li><li>护栏指标：评审等待不许失控；回归缺陷不许飙升</li><li>停机条件：稳定性连续恶化且无法解释时，立即收敛范围</li><li>复盘节奏：每两周一次（DORA 看结果 + SPACE 找原因 + 行动项闭环）</li></ul><p>记住：试点不是证明“AI 很强”，而是证明“系统交付更好”。</p><h4>第 4 阶段：规模化与治理（3–6 个月）</h4><p>规模化的关键，不是“全员开通”，而是“护栏前置”。PMO 建议制度化三件事：</p><ul><li>权限与可控：不同角色能用 AI 写入哪些字段？哪些输出必须二次确认？</li><li>透明与追溯：AI 参与过的内容要能回溯来源与改动</li><li>质量门禁：自动化测试、评审清单、灰度与回滚预案</li></ul><p>DORA 2025 的提醒非常现实：如果没有小批量与健壮测试机制，改进开发过程不一定带来交付改善；规模化时更要把这些基本功变成制度，而不是寄希望于“工具自带魔法”。</p><h4>第 5 阶段：把路线图做成“持续改进引擎”（长期）</h4><p>到这一阶段，AI 不再是一个项目，而是一种能力：组织学会在不确定性中持续改进。</p><p>建议沉淀三张“管理可读”的图：</p><ul><li>DORA 结果看板：速度与稳定趋势</li><li>SPACE 机制看板：摩擦点与瓶颈</li><li>AI 影响图谱：哪些场景值得加码，哪些要收敛或加护栏</li></ul><p>你会发现，PMO 的工作开始从“追数字”升级为“给洞察、推行动”：用数据把讨论从“感觉更忙”拉回到“系统哪里卡住了、下一步改什么”。</p><h2>FAQ：</h2><p><strong>Q1：AI 研发效能度量最小可行版本（MVM）是什么？</strong></p><p>A：先跑通 DORA 四项趋势，再用 SPACE 选 3–5 个低摩擦指标解释波动；AI 指标只做“采用、护栏、交付影响”三层即可。</p><p><strong>Q2：DORA 与 SPACE 是竞争关系吗？</strong></p><p>A：不是。DORA 是交付结果，SPACE 是机制解释；对 PMO 来说是“结果+因果”的组合拳。</p><p><strong>Q3：PMO 如何回答“AI 的 ROI”？</strong></p><p>A：用“护栏下的交付改善”回答：Lead Time 是否缩短、CFR 是否可控、恢复是否更快；同时用 SPACE 证明摩擦点是否减少，而不是用使用次数。</p><p><strong>Q4：怎样避免 AI 让稳定性变差？</strong></p><p>A：坚持小批量、强测试、质量门禁与可追溯审查；把 AI 输出纳入流程治理，而不是只在个人侧加速产出。</p><p><strong>Q5：ONES Copilot 在这套体系里扮演什么角色？</strong></p><p>A：它是一种“把 AI 放进可治理对象”的落地方式：围绕工作项、文档、动态总结与数据洞察，让 PMO 更容易做口径对齐、留痕追溯与闭环复盘；同样的治理逻辑也可迁移到其他平台。</p><h2>结尾：让 AI 成为“可度量的改进资产”</h2><p>AI 时代，PMO 的价值不是做更漂亮的周报，而是让组织具备一种能力：用 DORA 看结果，用 SPACE 找原因，用试点实验验证杠杆，用治理护栏放大收益。</p><p>如果你的组织已在使用 ONES 研发管理平台，那么 ONES Copilot 围绕工作项、文档、动态总结、查找筛选与数据洞察的能力，可以成为你把 AI 研发效能度量跑成闭环的“天然载体”；如果你使用的是其他平台，也同样可以借鉴这套思路：工具会变，治理逻辑不变——把 AI 放进流程、纳入度量、接受审查，才能真正把它变成组织的交付能力。</p>]]></description></item><item>    <title><![CDATA[Hologres Dynamic Table：高效增量刷新，构建实时统一数仓的核心利器 阿里云大数据]]></title>    <link>https://segmentfault.com/a/1190000047494247</link>    <guid>https://segmentfault.com/a/1190000047494247</guid>    <pubDate>2025-12-22 19:03:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数据架构逐步走向实时化与一体化的过程中，如何高效处理“大量历史 + 少量新增”的业务数据，已成为建设统一数仓与实时数仓时绕不开的关键挑战。</p><p>传统全量刷新方式在面对亿级历史数据时，往往面临刷新延迟高、计算成本大、链路复杂等问题。为了解决这些痛点，业界逐渐形成了一种新的数据处理范式——Dynamic Table（动态表），它通过声明式语法自动维护物化结果，并支持高效的增量刷新能力。</p><p>阿里云 Hologres 作为高性能实时数仓引擎，原生提供了 Dynamic Table，并基于有状态增量计算模型，在多表关联、聚合等复杂场景下展现出显著性能优势。本文将深入解析 Hologres Dynamic Table 的技术原理与实践价值。</p><h2><strong>为什么需要增量刷新能力？</strong></h2><p><img width="723" height="397" referrerpolicy="no-referrer" src="/img/bVdnrzG" alt="image.png" title="image.png"/></p><h3>典型业务场景</h3><p>在电商、互联网、金融等行业，以下场景极为常见：</p><p><strong>实时运营分析</strong></p><ul><li>订单、支付、退款等多源数据，按用户、商品、活动等维度进行关联与聚合，形成实时运营看板；</li><li>运营与业务团队希望以分钟级、甚至更高频率刷新核心指标。</li></ul><p><strong>用户与商品特征构建</strong></p><ul><li>将用户信息、行为数据、订单数据、商品信息、支付信息等多张表进行 Join，生成统一的特征宽表；</li><li>这些宽表通常是推荐、风控、画像等应用的输入，需要稳定、快速地更新。</li></ul><p><strong>资金与交易监控</strong></p><ul><li>交易流水、账户信息、风控结果等数据源不断产生新的记录；需要以较短的刷新间隔计算聚合指标或风控特征。</li></ul><p>这些场景的共同特点是：</p><blockquote><strong>历史数据规模庞大（亿级），但每次新增或变更的数据量很小（通常 &lt;1%）。</strong></blockquote><h3>传统数据加工的局限</h3><p>在缺少增量引擎的情况下，常见做法是：</p><ul><li>使用一条或多条 SQL 定义目标宽表或汇总表；</li><li>定时执行全量计算（如 INSERT OVERWRITE、CTAS），每次从基表完整扫描数据、执行多表 Join、再进行聚合；</li><li>通过调度系统编排上游任务和下游任务之间的依赖。</li></ul><p>这种模式存在若干明显不足：</p><ul><li><strong>刷新时延受限：</strong>数据规模增大后，每次全量扫描和计算耗时较长。当业务希望从“每小时刷新”提升到“每 5 分钟刷新”时，往往需要成倍增加计算资源，也可能遇到物理资源上限。</li><li><strong>计算成本较高</strong>：即使每次只有 1% 左右的数据发生变化，全量模式仍需对 100% 数据进行扫描和计算，CPU、IO 和网络资源利用效率不高。</li><li><strong>链路复杂，维护成本高</strong>：为降低单任务压力，工程实践中常将复杂逻辑拆解为多层中间表，形成较长的任务链路。链路越长，依赖越多，维护难度和变更风险也随之上升。</li></ul><p>因此，在“历史数据量大、实时数据实时产生”的场景下，引入真正高效的增量刷新机制，是提升数据时效与降低资源利用率的关键。</p><h2>二、什么是 Dynamic Table？Hologres 的增量刷新如何工作？</h2><p>Dynamic Table是当前一种主流的声明式数据处理架构，该架构可以自动处理并存储一个或者多个基表（Base Table）对象的数据关联、聚合结果，内置不同的数据刷新策略，业务可以根据需求设置不同的数据刷新策略，实现数据从基表对象到Dynamic Table的自动流转，满足业务统一开发、数据自动流转、处理时效性等诉求。</p><p>增量（incremental）、全量（full）是Dynamic Table的两种不同刷新方式，底层实现原理具有显著的差异：</p><ul><li><strong>全量刷新</strong>是指每次执行刷新时，都以全量的方式进行数据处理，并将基表的关联、聚合结果物化写入Dynamic Table，其技术原理类似于INSERT OVERWRITE。</li><li><strong>增量刷新</strong>模式下，每次刷新时只会读取基表中<strong>新增</strong>的数据，根据中间聚合状态和增量数据计算最终结果并更新到Dynamic Table中。相比全量刷新，增量刷新每次处理的数据量更少，效率更高，从而可以非常有效地提升刷新任务的时效性，同时降低计算资源的使用。</li></ul><p>增量Dynamic Table的计算遵循如下计算模式：</p><ol><li><strong>增全量刷新</strong>阶段：Dynamic Table的第一次刷新会把已有的所有历史数据都进行计算，即相当于把所有历史数据都当作增量来进行计算。这一阶段一般耗时比较长。</li><li><strong>增量刷新</strong>阶段：在增全量刷新完成后，以后的每次Dynamic Table刷新仅针对增量数据进行计算。理论上这些刷新应该耗时较短。</li></ol><p>在增量 Dynamic Table 的实现上，业界存在不同的技术路径。一种常见的做法是采用<strong>无状态增量计算模型</strong>：每次刷新时，系统仅基于源表的变更数据，重新推导整个查询逻辑，而不持久化任何中间计算状态。这种方式虽然节省存储，但在面对复杂查询（如多表关联、去重聚合等）时，往往需要反复扫描大量历史数据，导致刷新效率低下，甚至在某些场景下增量计算的开销反而超过全量。</p><p>相比之下，Hologres 采用了<strong>有状态增量计算模型</strong>：在首次全量构建 Dynamic Table 时，同步生成并持久化关键的中间状态（例如聚合结果、多表 Join 的中间产物等）。后续的增量刷新只需将新增或变更的数据与这些状态表进行高效合并，无需重复处理历史数据。这种设计以有限且可控的额外存储开销为代价，<strong>显著提升了复杂场景下的刷新性能和资源利用率，尤其适合“海量历史 + 少量增量”的典型业务负载</strong>。<br/><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdnrzH" alt="image.png" title="image.png" loading="lazy"/></p><h2>三、Hologres 增量刷新的实战优势</h2><p>我们通过三个典型场景验证 Hologres 的性能表现（测试环境：Hologres 15CU Serverless，竞品采用相似规格）。</p><p>对于增量数据，本实验模拟两种场景：</p><ol><li>Append only：即源表的增量数据只有新增（Insert），没有修改（update和delete）。这在日志、埋点数据表中非常常见。</li><li>Retraction（回撤）：源表的增量数据包含Insert、Update和Delete的数据。这适用于数据库类的表。</li></ol><p>对于Append only的源表，很多增量计算算子都可以大幅简化，状态表也可以大幅缩小。所以在性能环节，可以看到Append only源表的增量计算性能会更好。</p><h3>场景 1：单表聚合（COUNT DISTINCT + SUM）</h3><p>该场景使用的工作负载如下所示：<br/><img width="681" height="135" referrerpolicy="no-referrer" src="/img/bVdnrzI" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres建表SQL如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL_AGG with (
    auto_refresh_mode = 'incremental', 
    auto_refresh_enable = 'false', 
    freshness = '1 minutes'
)AS 
SELECT
  PRODUCT_ID,
  COUNT(DISTINCT USER_ID) AS UV,
  SUM(LINE_AMOUNT) AS SUM_LINE_AMOUNT,
  MAX(QUANTITY) AS MAX_QUANTITY
FROM ORDER_DETAIL
GROUP BY PRODUCT_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录）：</p><table><thead><tr><th><strong>刷新</strong></th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算刷新耗时(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td>ORDER\_DETAIL: 10M</td><td> </td><td>1.5</td><td>1.3</td><td>4.6</td><td>3.9</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>7.8</td><td>2.4</td><td>0.59</td><td>0.48</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>8.1</td><td>2.7</td><td>0.49</td><td>0.41</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>7.8</td><td>2.9</td><td>0.45</td><td>0.3</td></tr></tbody></table><p>可以看到Hologres增全量刷新阶段较慢，但后面的每次增量刷新都很快，符合预期。</p><p>无状态增量刷新性能较差的主要原因是无状态增量执行计划变得更加复杂，包含36个计算节点，且变更数据触及了大量分区，需要从源表重新扫描计算所有的数据。在有回撤数据时，处理数据变更前后因果关系会导致计算逻辑会变得更加复杂，进一步变慢，增量计算显得没有意义。</p><p>Hologres增量刷新快的核心原因是每次增量计算都是基于上次计算生成的状态表（State），这极大的简化了增量计算逻辑。此例中Hologres中各表存储大小如下所示（源表是Retraction的情况），结果表很小，而因为min/max/count distinct这两种聚合函数与sum/count这类不同，状态表需要存储对应列的所有原始数据，所以相比结果表要大很多。</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>252 MB</td><td>1 MB</td><td>93 MB</td></tr></tbody></table><h3>场景 2：两表 Join（订单 + 明细）</h3><p>两表Join是大数据处理中一种较为简单的基本场景，测试使用的具体工作负载如下图所示<br/><img width="723" height="221" referrerpolicy="no-referrer" src="/img/bVdnrzU" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres建表SQL如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL
WITH (
  auto_refresh_mode = 'incremental', 
  auto_refresh_enable = 'false', 
  freshness = '1 minutes'
) AS 
SELECT
  o.ORDER_ID,
  o.ORDER_DATE,
  o.ORDER_STATUS,
  oi.ORDER_ITEM_ID,
  oi.PRODUCT_ID,
  oi.QUANTITY,
  oi.UNIT_PRICE,
  oi.LINE_AMOUNT
FROM ORDERS o
JOIN ORDER_ITEMS oi
  ON o.ORDER_ID = oi.ORDER_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录），无状态增量刷新引擎在数据含有回撤的时候执行计划包含50个节点，源表大部分分区的数据被反复读取参与聚合，导致性能不佳。而数据不包含回撤（Appendonly）时，无状态增量刷新执行计划相对简单，含有35个节点，且新增数据不会触及太多分区，表现良好，体现出了增量计算的意义。</p><table><thead><tr><th> </th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td><strong>ORDERS</strong>: 5M<br/><strong>ORDER\_ITEMS</strong>: 10M</td><td> </td><td>10</td><td>8.7</td><td>9.2</td><td>6.29</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>22</td><td>1.1</td><td>2.2</td><td>0.68</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>19</td><td>2.2</td><td>1.1</td><td>0.50</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>22</td><td>1.7</td><td>1.9</td><td>0.51</td></tr></tbody></table><p>Hologres中各表存储大小如下所示，在这种场景下状态表存了源表的部分列数据，实际存储小于源表。</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>370 MB</td><td>350 MB</td><td>228 MB</td></tr></tbody></table><h3>场景 3：五表复杂 Join（订单 + 用户 + 商品 + 支付等）</h3><p>多表Join是一种相对更常见、真实的场景，测试具体使用的工作负载如下:<br/><img width="723" height="268" referrerpolicy="no-referrer" src="/img/bVdnrzT" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres的测试脚本如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL
WITH (
  auto_refresh_mode = 'incremental', 
  auto_refresh_enable = 'false', 
  freshness = '1 minutes'
) AS 
SELECT
  o.ORDER_ID,
  o.ORDER_DATE,
  o.ORDER_STATUS,
  u.USER_ID,
  u.USER_NAME,
  u.EMAIL,
  u.STATUS AS USER_STATUS,
  oi.ORDER_ITEM_ID,
  oi.PRODUCT_ID,
  p.PRODUCT_NAME,
  p.CATEGORY,
  p.PRICE       AS PRODUCT_PRICE,
  oi.QUANTITY,
  oi.UNIT_PRICE,
  oi.LINE_AMOUNT,
  pay.PAYMENT_ID,
  pay.PAY_AMOUNT,
  pay.PAY_METHOD,
  pay.PAY_TIME
FROM ORDERS o
JOIN USERS u
  ON o.USER_ID = u.USER_ID
JOIN ORDER_ITEMS oi
  ON o.ORDER_ID = oi.ORDER_ID
JOIN PRODUCTS p
  ON oi.PRODUCT_ID = p.PRODUCT_ID
LEFT JOIN PAYMENTS pay
  ON o.ORDER_ID = pay.ORDER_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录），无状态增量计算引擎无论是只包含插入还是有回撤，性能都相对较差，原因也是类似的。五表Join的场景下，增量计算的执行节点超过140个，会大量读取源表数据。</p><table><thead><tr><th> </th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td><strong>ORDER\_ITEMS</strong>: 10M<br/><strong>PAYMENTS</strong>: 4.9M<br/><strong>ORDERS</strong>: 5M<br/><strong>PRODUCTS</strong>: 50K<br/><strong>USERS</strong>: 500K</td><td> </td><td>23</td><td>23</td><td>30</td><td>22</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>55</td><td>26</td><td>3.9</td><td>2.8</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>58</td><td>27</td><td>3.2</td><td>1.8</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>60</td><td>26</td><td>2.9</td><td>1.8</td></tr></tbody></table><p>Hologres中各表存储大小如下所示，状态表会存储每一次Join的中间结果</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>594 MB</td><td>1218 MB</td><td>1094 MB</td></tr></tbody></table><h2>四、有状态增量计算：为何更高效？</h2><p>基于前面的实验结果，不难看出，无状态增量计算方案适用条件其实比较苛刻，在很多场景中基于少量数据的增量计算开销甚至会超过第一次的全量计算，丧失了增量计算的意义。</p><p>而相比较之下，Hologres的有状态增量计算方案可以适用于大多数的场景，通常只需要满足增量数据较少这一个条件即可。下图以单表聚合场景(<code>sum(value) group by key</code>)为例，展示了有状态方案的基本原理，增量计算过程中可以从状态表中直接获取历史数据的聚合结果，而不需要基于历史表的原始数据重新计算。此外在读取状态表数据时，也进一步引入了OLAP查询中常用的runtime filter优化，在增量数据较少的场景中，大幅减少状态表数据读取量，使刷新性能得到了显著的提升。<br/><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdnrzY" alt="image.png" title="image.png" loading="lazy"/></p><p>有状态方案一个显著的缺点是与无状态方案相比会需要占用额外的存储空间用于状态表的存储。如上述实验数据所示，实际额外存储大小通常与涉及到的源表、结果表的大小相关。</p><p>这一缺点通常是可以接受的，因为在业务实践操作中，这部分存储一般是可控的,不会无限增长。这是因为：</p><ul><li>分区表的场景中，只有活跃分区需要状态表，历史分区转全量刷新后不再需要状态表（这个过程是自动的，分区不再活跃后会自动清理状态表）。因此只有最近的一两个分区才需要状态表，这极大地减少了状态表的存储空间。因此Hologres Dynamic Table增量计算状态表没有Flink常见的状态膨胀问题。</li><li>非分区表场景中，可以为状态表配置合适的TTL，丢弃一些不再需要的历史状态减少存储空间</li></ul><h2>总结：Hologres Dynamic Table 的核心价值</h2><p>面对企业日益增长的实时分析需求，Hologres 的 Dynamic Table 通过有状态增量计算引擎，从根本上解决了传统方案在复杂查询下“增量不增效”的痛点。它不仅大幅缩短了数据刷新延迟，还显著降低了计算资源消耗和运维复杂度，真正实现了“写一次 SQL，自动高效更新”的体验。无论你是构建实时看板、用户画像宽表，还是风控特征管道，Hologres 都能以稳定、高性能、低成本的方式支撑你的核心数据链路。</p><h2>附录</h2><h3>无状态 &amp; 有状态增量计算底层原理对比分析</h3><p>从上面的实验结果来看，Hologres的有状态实现方案在大多数的场景中是要优于无状态增量计算引擎的，本小节将对两者的底层计算原理进行对比分析，说明造成这种性能差异的根本原因。</p><h4>符号说明</h4><p><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnrAd" alt="image.png" title="image.png" loading="lazy"/></p><h4>多表Join场景</h4><h5>无状态实现Hologres Dynamic Table：高效增量刷新，构建实时统一数仓的核心利器</h5><p>无状态多表Inner Join的增量计算公式如下（原理可参考<a href="https://link.segmentfault.com/?enc=H5kzk0H1LZqTSiA40XsElg%3D%3D.vRiV9k43KJDTqDPLpt3d68HbVvooo7Gjfe%2FiT0%2B7Fp8k%2FXl64BZJ55gfKgM6mz27" rel="nofollow" target="_blank">论文</a>），根据实际执行计划推测某无状态增量计算引擎的多表Join增量计算应该也是基于该公式实现的<br/><img width="723" height="151" referrerpolicy="no-referrer" src="/img/bVdnrAe" alt="image.png" title="image.png" loading="lazy"/><br/>该方案主要会有如下弊端：<br/><img width="723" height="155" referrerpolicy="no-referrer" src="/img/bVdnrAf" alt="image.png" title="image.png" loading="lazy"/></p><h5>有状态实现</h5><p>Hologres的有状态多表Join计算公式原理大致如下，因为所有的中间计算结果状态会通过状态表（State）持久化保存下来，因此可以按照两表Join的公式做简单展开<br/><img width="723" height="79" referrerpolicy="no-referrer" src="/img/bVdnrAh" alt="image.png" title="image.png" loading="lazy"/></p><p>有状态方案以额外的存储开销为代价换来了：<br/><img width="723" height="104" referrerpolicy="no-referrer" src="/img/bVdnrAk" alt="image.png" title="image.png" loading="lazy"/></p><h4>单表聚合场景</h4><p>单表聚合场景相对较为简单，无状态、有状态两种方案的计算公式原理分别如下：<br/><img width="723" height="118" referrerpolicy="no-referrer" src="/img/bVdnrAn" alt="image.png" title="image.png" loading="lazy"/></p><p>公式大体是比较相似的，主要区别是Hologres的有状态方案持久化存储了历史数据的聚合结果，因此有以下优势：</p><ul><li>不需要重新计算历史数据的聚合结果</li><li>State表数据量极小，Join 操作可以显著减少读取数据量</li></ul>]]></description></item><item>    <title><![CDATA[Excelize 开源社区荣获 2025 红山开源明星项目三等奖 xuri ]]></title>    <link>https://segmentfault.com/a/1190000047494249</link>    <guid>https://segmentfault.com/a/1190000047494249</guid>    <pubDate>2025-12-22 19:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 11 月 16 日，由红山开源平台主办的“2025 红山开源大会”，在北京香山颐和宾馆红山厅成功落下帷幕。为表彰在开源领域做出卓越贡献的项目，大会特别设立 2025 年度红山开源平台明星项目颁奖环节。</p><p>Excelize 开源社区荣获了 2025 红山开源明星项目三等奖，活动详情详见文章：<a href="https://link.segmentfault.com/?enc=0fY6kt4dAVh%2F%2F9WhDy5GMQ%3D%3D.ESXqDX15mNQMCo4fjdWRAJqISF83iqSEbZF3gcRBXtOWBV3YsgWIWOEFmeOondtIYp4%2FYGkjOpfbayeKXECKtw%3D%3D" rel="nofollow" target="_blank">2025 红山开源大会圆满收官，共筑开源生态新未来</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494251" alt="Excelize 荣获 2025 红山开源明星项目三等奖" title="Excelize 荣获 2025 红山开源明星项目三等奖"/></p><p>Excelize 是用于操作电子表格办公文档的开源基础库，开源地址：<a href="https://link.segmentfault.com/?enc=1AAjbIbAFAnNBfhvHDkiOQ%3D%3D.Qvq75emknm8lr13YXPatxBnlACpUJ12YHQLIOVYRBk6BX10OMY7DYj7I%2FmxYwcwX" rel="nofollow" target="_blank">github.com/xuri/excelize</a>，遵循 BSD 3-clause 开源协议，基于 ISO/IEC 29500 国际标准。可以使用它来读取、写入由 Excel 、WPS 、OpenOffice 等办公软件创建的电子表格文档。支持 XLAM / XLSM / XLSX / XLTM / XLTX 等多种文档格式，高度兼容带有样式、图片 (表)、透视表、切片器等复杂组件的文档，并提供流式读写支持，用于处理包含大规模数据的工作簿。可应用于各类报表平台、云计算、边缘计算等系统。自 2016 年开源以来已成为云原生应用尤其是 Go 语言开发者在处理电子表格办公文档时的热门选择，正在被广泛应用于大型互联网公司、中小企业客户和初创公司。</p><p>Excelize 开源基础库曾荣获 2025 上海开源创新菁英奖——优秀开源项目奖、2025 年 GitCode 百大开源项目、2022 年中国开源创新大赛一等奖、2018 年开源中国码云最有价值开源项目 GVP (Gitee Most Valuable Project) 等奖项。</p>]]></description></item><item>    <title><![CDATA[招聘终极战场：AI重构首轮筛选的精准与效能革命 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047494313</link>    <guid>https://segmentfault.com/a/1190000047494313</guid>    <pubDate>2025-12-22 19:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>招聘终极战场：AI重构首轮筛选的精准与效能革命<br/>国务院《关于深入实施“人工智能+”行动的意见》明确划定：2027年，70%的岗位面试将由AI或智能体完成。当智能化浪潮不可逆地渗透企业运营，你的招聘体系是否仍停留在简历海投、人力初筛、凭感觉提问的“前AI时代”？效率鸿沟已悄然拉开——当对手用AI数小时内完成千人筛选与精准初评，你的团队是否还在无尽简历与重复问答中消耗战略时间？HR向“数据驱动的决策伙伴”转型，已非未来愿景，而是当下生存必备技能，而这场转型的关键，始于面试智能化的两大核心：评估精准度与候选人体验。</p><p>一、数据驱动决策：超越直觉的可验证精准<br/>招聘最大的隐性成本是选错人，AI面试智能体将“精准”定义为可严格验证的标准：评分结果既通过与资深面试官“背靠背”对比实验，又经受效标效度与重测信度等心理学指标检验，从“辅助参考”进阶为“决策依据”。这种精准贯穿评估全流程：<br/>•一问多能：单题同步评估多项胜任力，无缝衔接初筛与专业复试，评估效率提升超50%；<br/>•智能追问：依据回答即时生成深度问题，如资深面试官般捕捉逻辑漏洞与能力闪光点，杜绝表面化评判；<br/>•简历深挖：自动解析简历关键信息与模糊点，生成递进式提问链，既验证真实性，又挖掘文本掩盖的胜任力；<br/>•全维度覆盖：兼顾通用素质与编程、财务等专业领域精准评估，同步解放HR与业务面试官。<br/>二、体验即品牌：让面试成为雇主形象加分项<br/>糟糕的AI面试体验足以劝退顶尖人才，AI面试智能体将“拟人化交互”作为技术核心，让面试成为雇主品牌传播的重要载体：<br/>•情绪感知交互：识别语速、语调中的紧张或犹豫，通过人性化引导帮助候选人展现最佳状态；<br/>•无缝对话流：无需手动操作“开始/停止”，答案结束后自动衔接下一题，还原真人对话的自然节奏；<br/>•高拟真视觉呈现：唇形与语音精准同步，大幅削弱传统虚拟面试的“机械感”；<br/>•实时答疑能力：候选人可随时咨询职位、团队、福利等问题，AI精准解答，成为传递企业信息的智能窗口。<br/>三、流程革命：招聘全链路迈入“无人驾驶”<br/>面试智能化仅是起点，AI人才寻访智能体将自动化延伸至招聘最前端的寻访环节。它并非简单群发工具，而是能自主完成“筛选-沟通-索要简历-系统录入”全链条动作的智能系统：<br/>•极速启动：30-60秒完成初始化，无需人工值守即可独立运作；<br/>•智能筛选：按预设条件精准筛选平台简历，锁定目标候选人；<br/>•拟人化沟通：发起自然对话，遍历回复所有未读消息，信息缺失时主动“索要简历”；<br/>•系统无缝同步：将获取的简历自动同步至企业ATS，形成完整数据闭环。<br/>这不仅将HR从重复劳动中彻底解放，更通过大模型技术，将寻访动作从“机械执行”升级为“有判断力的决策”。<br/>四、实践验证：顶尖组织的共同选择<br/>AI招聘解决方案已获得西门子中国、招商银行、阿里巴巴国际、TCL及浙江大学等上千家领先企业与高校的认可。通过引入该系统，这些组织成功将招聘流程搭建在智能化基座之上，实现了效率与精准度的双重飞跃，为行业树立了可复制的实践标杆。<br/>智能化趋势已明确，观望即是最大风险。AI招聘的下一轮竞争，始于对新模式的验证与落地。当精准评估、优质体验与全流程自动化形成闭环，招聘将不再是成本消耗，而是驱动企业人才竞争力提升的核心引擎。</p>]]></description></item><item>    <title><![CDATA[FundingRate/资金费率套利 云梦量化科技 ]]></title>    <link>https://segmentfault.com/a/1190000047494330</link>    <guid>https://segmentfault.com/a/1190000047494330</guid>    <pubDate>2025-12-22 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>资金费率介绍</h2><p>由于永续合约没有传统意义上的到期日/交割日，也就没有传统的价格收敛机制，因此需要使用一种新的机制来确保永续合约价格与现货价格锚定在一起，这就是资金费率机制。资金费率是多空双方之间定期交换的费用，通常每8小时结算一次。如果资金费率为正，多头支付空头；如果资金费率为负，空头支付多头。\<br/>当合约价格比现货价格高时，资金费率通常为正，鼓励交易者做空合约以推动价格下跌；同样当合约价格低于现货价格时，资金费率通常为负，鼓励交易者做多合约以推动价格上涨。通过这种机制，永续合约的价格能够与现货价格进行锚定。</p><h2>资金费率的定价</h2><p>根据某安的资金费率定价公式:</p><p>$$
\begin{align}
funding\_rate &amp;= avg\_premium\_index + clamp(interest\_rate - avg\_premium\_index, -0.05\%, 0.05\%) \\
funding\_rate &amp;= clamp(funding\_rate, lower\_bound, upper\_bound)
\end{align}
$$</p><p>其中avg_premium_index是平均溢价指数，表示合约与现货的偏离程度，interest_rate是利率，一般是日化0.03%，lower_bound和upper_bound分别是资金费率的下限和上限。<br/>其中对于平均溢价指数的计算公式为(以8小时周期为例):</p><ol><li><p>首先计算每个时间点的溢价指数, 溢价指数 = [Max(0, 冲击买价 - 价格指数) - Max(0, 价格指数 - 冲击卖价)] / 价格指数</p><pre><code>- 注：其中提到的冲击买价为使用200USDT*最大杠杆的资金冲击买盘，冲击卖价同理，价格指数为各个交易所的现货价格加权平均得到的价格</code></pre></li><li>然后根据不同的时间段赋予不同的权重，具体为，第1分钟权重为1，第2分钟权重为2...第480分钟权重为480，根据时间权重计算得到平均溢价指数</li></ol><p>对于这里提到的溢价指数计算公式，其实可以简单理解为（合约价-现货价）/现货价</p><p>在这个定价公式中，$interest\_rate$是一个非常重要的参数，这个参数决定了使用合约的资金成本（对于多头而言）。考虑此时合约价与现货价格一致，或者合约价与现货价的偏离程度在一个非常小的范围内，那么此时对于$interest\_rate-avg\_premium\_index$就应该位于-0.05%到0.05%的范围内，此时资金费率就等于$funding\_rate=avg\_premium\_index+(interest\_rate-avg\_premium\_index)=interest\_rate$，也就是说此时资金费率就等于利率。那么对于这个日化0.03%的利率来说，年化收益率为0.03%*365=10.95%。对于多头来说，这是非常高的资金成本，但是对于空头来说，却是一个非常好的收益来源。此时考虑买入现货卖出合约以赚取这部分无风险收益。这里选择反向（Reverse）合约，因为反向合约的资金费率更加稳定，长期来看要高于正向（Linear）合约的资金费率收益。</p><h2>买入现货卖出反向合约</h2><p>做空反向合约的盈亏计算方式为:</p><p>$$
\begin{align}
PnL_{short} = number\_of\_contracts * contract\_value * (\frac{1}{exit\_price} - \frac{1}{entry\_price})
\end{align}
$$</p><h3>无风险</h3><p>从直觉上来看，买入现货卖出等量的合约应该是无风险的，实际上也是如此。<br/>举例来说：假设当前现货价格为100000，对应反向合约价格为100000，此时买入1个现货，同时卖出1个反向合约，合约面值为100000。</p><ol><li>如果价格上涨到110000，此时现货价值上涨到110000，现货部分盈利10000；而合约部分亏损为100000 * (1/110000 - 1/100000)=0.0909个现货，乘上当前价格110000，合约部分亏损为10000，总体盈亏为0</li><li>如果价格下跌到90000，此时现货价值下跌到90000，现货部分亏损10000；而合约部分盈利为100000 * (1/90000 - 1/100000)=0.1111个现货，乘上当前价格90000，合约部分盈利为10000，总体盈亏为0</li></ol><p>再看是否存在爆仓风险，假设合约开仓时合约价格为P0, 此时为P, 考虑买入一个现货并卖出等量的反向合约，此时合约面值为1*P0<br/>币本位合约的维持保证金计算方式为:</p><p>$$
\begin{align}
% 先计算权益
equity\_value = 1 * P + 1 * P0 * (\frac{1}{P} - \frac{1}{P0}) * P = P + P0 - P = P0 \\
equity\_quantity = \frac{equity\_value}{P} = \frac{P0}{P} \\
% 计算维持保证金
MM = \frac{MMR*P0}{P}
\end{align}
$$</p><p>其中MMR为维持保证金率<br/>由于MMR为一个小于1的常数，因此有$equity\_quantity=\frac{P0}{P}&gt;\frac{MMR*P0}{P}=MM$，也就是说不会爆仓。</p><h3>收益分析</h3><p>获取了2024-09-30到2025-09-30期间某安上的主流的8个现货（图表标题为其市值排名）的反向合约资金费率数据<br/>计算累计资金费率收益如下图所示：<br/><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdnrBG" alt="" title=""/><br/>可以看到基本上都是一条直线往上走，但其中有三个反向合约的累计资金费率收益分别是5.04%, 7.01%, 6.83%, 远小于其余的5个反向合约。考虑到分散风险的角度，构建等权重的组合，计算其累计资金费率收益如下图所示：<img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdnrBH" alt="" title="" loading="lazy"/><br/>进一步将其中收益较低的三个剔除，构建剩余5个的等权重组合，计算其累计资金费率收益如下图所示：<img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdnrBI" alt="" title="" loading="lazy"/><br/>可以发现其总收益可以达到9.67%，接近理论上的10.95%的无风险收益率。</p><h2>结论</h2><p>由于资金费率定价公式中的设定利率较高，导致这个市场上的无风险收益率也非常高。<br/>这使得通过买入现货并卖出反向合约，可以赚取一个非常高的无风险收益率，可以达到接近10%左右。并且通过组合分散风险，可以进一步提升收益的稳定性。</p><p>此文章内容由云梦量化科技高频策略实习生skylen创作投稿。</p>]]></description></item><item>    <title><![CDATA[怎么实现制造业数字化转型？实战案例解析 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047493984</link>    <guid>https://segmentfault.com/a/1190000047493984</guid>    <pubDate>2025-12-22 18:13:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在全球制造业加速向智能化、网络化演进的今天，工业数字化转型已不再是可选的优化路径，而是企业生存与发展的战略刚需。它超越了传统信息化升级的范畴，是一场涵盖研发、生产、供应链、设备管理与服务体系的全方位重构，其核心在于通过数据驱动、智能协同与平台赋能，打通“研、产、供、销、服”全链条，实现从经验决策向智能决策、从孤立系统向生态协同的根本跃迁。<br/>在这一转型浪潮中，广域铭岛作为中国工业互联网领域的先锋力量，凭借其自主研发的Geega工业互联网平台，构建了“平台+解决方案+工业软件”的完整生态体系，为制造业提供了可落地、可复制、可进化的数字化转型范式。其实践表明，真正的工业数字化转型，必须实现技术、流程与理念的三重突破。<br/>在研发端，广域铭岛的FastWorx设计研发协同平台，以BOM管理、三维工艺引擎与AI驱动的工艺专家系统为核心，打通了从概念设计到产品交付的全周期闭环。通过结构化知识管理与智能校核，企业零部件复用率提升35%，研发周期显著缩短，工程师从重复性劳动中解放，创造力得以释放。这标志着研发正从“人驱动”转向“数据+算法驱动”。<br/>在生产端，设备数字化协同与数字孪生技术成为提质增效的关键。广域铭岛在领克汽车成都工厂部署的焊接质量预警系统，整合3000+焊点参数，实现虚焊、飞溅等缺陷的实时识别与自动预警，将问题排查时间从72小时压缩至5分钟。其设备数字孪生模型可精准模拟反应釜、工业机器人等关键装备的运行状态，实现预测性维护，使设备综合效率（OEE）提升20%以上，运维成本大幅降低。<br/>在供应链与质量管理层面，广域铭岛的GOS-知识库系统融合多源异构数据采集、AI知识图谱与区块链技术，构建了端到端的智能协同网络。在百矿集团，系统通过实时优化氧化铝浓度，单吨铝电解能耗降低300千瓦时，年节电费超7000万元；在汽车产业链中，其GECP碳管理平台实现全生命周期碳足迹追踪，助力企业应对CBAM等绿色贸易壁垒。同时，智能计划助手将排产时间从6小时缩短至1小时，质量不良率从8%降至0.8%，真正实现了“质量可追溯、成本可预测、响应可敏捷”。<br/>当前，工业数字化转型正从单点技术应用迈向系统性生态构建。广域铭岛的实践印证：成功的转型不是工具的堆砌，而是以平台为中枢，打通数据孤岛、重构业务流程、激活组织潜能的系统工程。随着5G、AI大模型与量子计算等技术的深度融合，工业数字化将加速向“自感知、自决策、自优化”的智能体时代演进。<br/>未来，谁能率先完成这场从“制造”到“智造”的蜕变，谁就能在新一轮全球产业竞争中占据制高点。广域铭岛等领先服务商，正以技术为笔、数据为墨，为制造业绘制一幅智能、绿色、韧性的新蓝图——这不仅是技术的升级，更是中国制造业迈向高质量发展的必由之路。</p>]]></description></item><item>    <title><![CDATA[工业物联网IIOT如何重塑整车制造行业？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047494018</link>    <guid>https://segmentfault.com/a/1190000047494018</guid>    <pubDate>2025-12-22 18:12:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>导言：制造业数字化转型的新引擎<br/>当前全球制造业正经历着以工业物联网为代表的第四次工业革命浪潮。作为工业4.0的核心技术，IIoT通过将传感器、设备和系统相互连接，实现了制造过程的数字化、网络化和智能化。在技术密集型的汽车制造领域，IIoT正在从根本上改变传统生产模式，推动制造业向智能化、柔性化方向发展。从供应链管理到生产线优化，从质量管控到能效提升，IIoT技术正在汽车制造的全价值链中发挥着越来越重要的作用。<br/>值得注意的是，IIoT的应用已经超越了简单的设备联网和数据采集阶段，正在向智能决策和自主优化演进。通过大数据分析、人工智能和数字孪生等技术的深度融合，IIoT正在帮助汽车制造企业构建更加智能、高效的生产体系。这种转变不仅提升了制造效率，更重要的是重塑了汽车制造业的竞争格局。<br/>IIoT技术的核心价值与应用场景<br/>工业物联网在整车制造中的应用价值主要体现在三个层面：设备层、系统层和决策层。在设备层面，通过部署各类智能传感器和物联网终端，实现了生产设备运行状态的实时监测和预警。在系统层面，通过数据集成和交互，打破了传统制造系统中的信息孤岛，实现了生产过程的协同优化。在决策层面，基于大数据分析和人工智能算法，为企业管理层提供了更加科学、精准的决策支持。<br/>具体到应用场景，IIoT技术在质量控制方面的表现尤为突出。传统制造模式下，质量检测往往依赖于人工抽检，存在效率低、漏检率高的问题。而通过部署机器视觉检测系统和智能传感器，实现了对产品质量的全程监控和实时预警。例如在焊接工序中，智能监测系统可以实时采集焊接电流、电压等参数，通过算法分析及时发现问题，将质量缺陷消灭在萌芽状态。这种转变不仅提升了产品质量，更重要的是建立了可追溯的质量管理体系。<br/>典型案例：IIOT赋能汽车制造业创新实践<br/>在具体应用中，广域铭岛为某大型汽车制造企业部署了完整的数字孪生系统。通过构建虚拟工厂，实现了对物理工厂的实时映射和动态仿真。更值得一提的是，平台通过人工智能算法，实现了生产参数的自主优化，使整车生产效率提升了22%，同时能耗降低了15%。在质量管理方面，系统能够实时采集超过2000个质量参数，通过机器学习算法建立质量预测模型，将产品缺陷率降低了40%。广域铭岛通过构建供应链协同平台，实现了与300多家供应商的实时数据交互。平台通过智能算法预测零部件需求，优化库存管理，将库存周转率提升了35%，同时确保了生产物料的及时供应。<br/>吉利集团的数字化转型堪称典范。该集团在全球四大研发中心部署了IIoT平台，实现了从冲压、焊接、涂装到总装的全流程数字化监控。特别是在宁波工厂，他们通过引入数字孪生技术，构建了与物理工厂完全对应的虚拟镜像，能够提前发现并解决生产线切换过程中的各种问题。这一创新实践使新车型导入时间缩短了40%，同时将整车生产效率提升了25%。<br/>特斯拉则在智能制造的另一条路径上走得更深。其超级工厂采用了完全自动化的生产体系，通过机器视觉和AI算法实时监控焊接质量，实现了99.99%的缺陷检出率。同时，工厂的能源管理系统也通过IIoT技术实现了能耗的精细化控制，将每台汽车的能源消耗降低20%以上。这种极致的自动化和智能化，使特斯拉在激烈的市场竞争中保持了领先优势。<br/>未来展望：IIoT驱动制造业创新发展<br/>随着5G、人工智能、数字孪生等新技术的成熟应用，IIoT在汽车制造领域的应用将更加深入。未来，我们可以预见更加智能的自主决策系统，更加柔性化的生产模式，以及更加协同的制造生态。工业互联网平台如广域铭岛等，将继续推动制造业向网络化、智能化、服务化方向转型升级。<br/>然而，IIoT的深度应用仍面临诸多挑战，包括数据安全、系统集成、人才培养等方面。制造企业需要制定清晰的数字化转型战略，选择合适的技术合作伙伴，建立完善的数据治理体系。只有这样才能真正释放IIoT技术的价值，在数字化浪潮中赢得竞争优势。<br/>最终，IIoT技术的成功应用不在于技术的先进性，而在于能否为企业创造实际价值。制造企业应该以业务需求为导向，以价值创造为目标，循序渐进地推进数字化转型。通过IIoT技术的创新应用，汽车制造业必将迎来更加智能化、高效化的未来。</p>]]></description></item><item>    <title><![CDATA[[未解决]RuntimeError: CUDA environment is not correct]]></title>    <link>https://segmentfault.com/a/1190000047494023</link>    <guid>https://segmentfault.com/a/1190000047494023</guid>    <pubDate>2025-12-22 18:12:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用Win10重现代码的时候，其使用了Chainer库。然后按照requirements.txt安装了所有的依赖，但还是报错：</p><blockquote>RuntimeError: CUDA environment is not correctly set up</blockquote><p>然而我是已经装好CUDA和cuDNN的</p><p>仔细看了一下，下一行说的是：</p><blockquote>(see <a href="https://link.segmentfault.com/?enc=3NMk1keVMjpQq5Ib0wC1ww%3D%3D.T6C8C6JvoXubTosLNjjl1MsWAofd9dsjnNqJWfsiTIRDIWoeV%2FrCYyBWEH7mMl7N" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).No module named 'cupy'</blockquote><p>看样子是没有安装cupy这个库，按链接点了进去，在README找到了另一个界面<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>使用命令看一下CUDA的版本</p><blockquote>nvcc --version</blockquote><p>就直接命令行安装对应版本咯</p><blockquote>pip install cupy-cuda100</blockquote><p>然而，在我直接命令行安装最新版本的cupy之后，又报错了。我的版本是：</p><blockquote>cupy_cuda100-9.1.0</blockquote><p>报错提示：</p><blockquote>RuntimeError: CUDA environment is not correctly set up<br/>(see <a href="https://link.segmentfault.com/?enc=hc3tfXOx4Tm25Ft2b%2FmIMg%3D%3D.I9%2BZRAfLmtRK2tu2EGX%2BzgqvkPKGCNhFHYA7Qi%2FT1srLdHQrD8QJhhP8kF%2BfHgoo" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).CuPy is not correctly installed.</blockquote><p>我去上面看了看warning</p><blockquote>CuPy (cupy-cuda100) version 9.1.0 may not be compatible with this version of Chainer.<br/>Please consider installing the supported version by running:<br/>  $ pip install 'cupy-cuda100&gt;=7.7.0,&lt;8.0.0'</blockquote><p>啊，这意思是版本的问题咯？</p><p>那我直接指定一下版本</p><blockquote>pip install cupy-cuda100==7.7.0</blockquote><p>然后发现下载的巨慢，就找了篇博文参考<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></p><p>于是决定使用清华源<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>：</p><blockquote>pip install -i <a href="https://link.segmentfault.com/?enc=9pgCEICnPPVCYWIuTGrgrw%3D%3D.lj8IWDwSQ0maHZ613LHradK1Kh3878xIuLX7RiPw1fhBqpIRByYFHvRkOW9El0EI" rel="nofollow" target="_blank">https://pypi.tuna.tsinghua.edu.cn/simple</a> cupy-cuda100==7.7.0</blockquote><p>然而还是有问题。这个包大概280MB，下载到100+MB的时候就会报错，好像是HTTP ERROR ，说是超时还是什么的</p><p>命令行界面是有显示包的下载地址的，于是我直接CTRL+单击，在浏览器打开链接进行下载；或者将此链接丢到IDM什么的。</p><p>下载完whl文件后，命令行转到该文件夹，然后</p><blockquote>pip install filename.whl</blockquote><p>据说whl文件类似与压缩文件<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，所以直接pip install 加上完整的文件名就好</p><p>但问题还是没得到解决，后续的报错依然是：</p><blockquote>(see <a href="https://link.segmentfault.com/?enc=lEogseUKVKdF3krIHi10Sg%3D%3D.h84Qxfloes%2BLCxqmh8x8jjUQiWKGa3w2eX%2BnbNV9IuamJCL8ap6%2FtFjEV8WVfx6H" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).CuPy is not correctly installed.</blockquote><p>我又尝试了7.8.0的版本，还是不行</p><p>根据之前的warning：</p><blockquote>CuPy (cupy-cuda100) version 9.1.0 may not be compatible with this version of Chainer.<br/>Please consider installing the supported version by running:<br/>  $ pip install 'cupy-cuda100&gt;=7.7.0,&lt;8.0.0'</blockquote><p>符合要求的版本在7.7.0和8.0.0之间，而7.9.0版本是没有的。所以理论上只有7.7.0和7.8.0两个版本，而这两个版本都不行。</p><p>后来在chainer的官网<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>上看到一段话：</p><blockquote>We are automatically testing Chainer on all the recommended environments above. We cannot guarantee that Chainer works on other environments including Windows and macOS (especially with CUDA support), even if Chainer may seem to be running correctly.</blockquote><p>可能是系统为Win10的原因</p><p>不过在cpu模式下，这份代码运行良好，复现成功，chainer库没有报错。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=ngyLay%2F4E89dFlmsslMLgw%3D%3D.TRLMFVQ3EAlgqdtwto3iurIqZlyukm1Cs31K61sfZpITKdRha0TCjE8TEZN%2FnWiixmhltdP98uc3YQnNA6Es2w%3D%3D" rel="nofollow" target="_blank">Installation — CuPy 9.1.0 documentation</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=zyO1%2ByX9kYmYlXq2O0moOw%3D%3D.quYq5QYTOvmsQ4hSeGlL8TA8r%2BJPh3z%2BV%2FDQumRJTq60md2nnihTk7Ekoy%2FueTL0aF1P6TPdaXCvesvNJ9hE%2Fw%3D%3D" rel="nofollow" target="_blank">cupy-cuda安装下载报错以及速度太慢的问题</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=jrqWdxQ9S9%2FRgvSPUQ307g%3D%3D.hbbX18MNS%2FeV8OGHDlqHyRss4ROcs8VGBNTI3UFcsvU0%2FNI32DxZUySdx%2FxcNS1l" rel="nofollow" target="_blank">pypi 镜像使用帮助</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"><a href="https://link.segmentfault.com/?enc=Ti57zgPzpMezjFGOm1U4LA%3D%3D.x5eJiyqJCsJrrxS%2Fb3tMfulNnq77Cc3cY9Rt0%2FfUzXSVgO42R7KdHD%2B1OO3DC4xc33tb%2BhvbEGLfOrT4hJzYSQ%3D%3D" rel="nofollow" target="_blank">通过whl文件更新或安装python包</a>  <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"><a href="https://link.segmentfault.com/?enc=oH6L4cAP%2F%2FhZqkJjf9NWTg%3D%3D.mGl1cy7Cl8E6d%2B2NMn7gJUocfMINdUWbV%2FFi5L6XNr621jDazX0VSHT3Z%2FL7gWv4" rel="nofollow" target="_blank">Installation — Chainer 7.7.0 documentation</a>  <a href="#fnref-5" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[Windows PowerShell使用curl登录校园网 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494053</link>    <guid>https://segmentfault.com/a/1190000047494053</guid>    <pubDate>2025-12-22 18:11:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.抓包分析登录过程</h2><p>这里尝试过两种方法：一是直接用新版edge的DevTools，即按F12键；另一种则是使用Wireshark软件。下面分别演示这两种方法。</p><h5>方法一：使用DevTools</h5><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw2" alt="image.png" title="image.png"/><br/>首先打开校园网的登录页面<br/><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrw3" alt="image.png" title="image.png" loading="lazy"/><br/>接着按F12进入DevTools，选择NetWork的选项卡，默认开始记录网络活动<br/><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw4" alt="image.png" title="image.png" loading="lazy"/><br/>然后返回登录页面，输入账号密码点击登录</p><p><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrw7" alt="image.png" title="image.png" loading="lazy"/><br/>此时去到DevTool的窗口，查看抓取的结果。抓取结果默认是按照时间顺序排列的，如果不是可以使用WaterFall的选项卡进行升序或者降序的排列<br/><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrxb" alt="image.png" title="image.png" loading="lazy"/><br/>在抓取的网络活动当中，发现有名为login.php的活动，猜测其为登录的关键步骤。上图是其中的某一个login.php的活动，其向目标网址发送了一个post请求，且携带的参数当中包含账号和密码。此活动应该是登录的关键步骤。</p><h5>方法二：使用WireShark</h5><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw2" alt="image.png" title="image.png" loading="lazy"/><br/>同样打开校园网的登录页面<br/><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrxc" alt="image.png" title="image.png" loading="lazy"/><br/>启动WireShark，选择网络接口进行抓取。一般来说，抓取的网络接口都是直接对外的总连接的接口，即波动较大的网络。如果不清楚选择哪一个，也可以打开网络中心看一看对外连接的是哪一个网络接口。</p><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrxd" alt="image.png" title="image.png" loading="lazy"/><br/>然后返回登录页面，输入账号密码点击登录</p><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnrxe" alt="image.png" title="image.png" loading="lazy"/></p><p>由于已知登录界面的主机的ip，所以在这里就直接输入了ip和协议进行过滤。如果不清楚，还是需要去F12的界面查看。关于过滤规则详见<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><h2>2.构造数据包模拟登录</h2><p>从F12和WireShark的抓取结果来看，登录过程主要是本地主机向登录页面发送了一个POST的请求，并携带账号密码进行验证。构造数据包的方法也有两个，一个是curl命令行构造，一个是python脚本构造</p><h5>方法一：使用curl命令</h5><p><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrxf" alt="image.png" title="image.png" loading="lazy"/><br/>原本是参照某篇博文<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>进行构造命令，然后发现Windows下Powshell的命令格式有所不同，并不能使用这篇博文的方式。后续根据报错查找，另一篇博文<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>展示了改进方法。关于Powshell下curl命令的使用方法，这里有两篇博文也有很好地说明。<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>也可以参照该博文<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>使用curl进行登录。</p><h5>方法二：使用python脚本</h5><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnrxg" alt="image.png" title="image.png" loading="lazy"/></p><p>python代码如下：</p><pre><code class="python">import requests

login_url = ""

data = {
    "opr":"",
    "userName":"",
    "pwd":"",
    "ipv4or6":"",
    "rememberPwd":""
}

doc = requests.post(login_url,data).text
print(doc)</code></pre><p>以上代码参考了该博文<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup></p><h2>3.总结</h2><p>在抓包过程当中，DevTools的目标更明确，指定对某一个具体的页面的网络活动进行抓取，可以看到该页面的ip地址、端口等等；相比之下Wireshark的目标更宽泛，所有通过网络接口的数据均进行抓取，需要过滤才能看到目标页面的信息。不仅目标宽泛，WireShark抓取的数据也更多，过滤后，目标页面的TCP三次握手的过程也能够看到。</p><p>此次抓取的是登录过程，后续也抓取过退出过程，方法基本一致。退出过程的目标地址和登录过程的目标地址一样，只是携带的参数不同。</p><p>除此之外，也尝试过模拟登录搜狐邮箱，同样参考这篇博文<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，但并没有成功。分析是携带的参数不对，除了时间戳这个变量之外，每次登录的参数还受其他变量的影响。而且其使用的是加密的https的方式，还需要对WireShark进行配置才能解密数据包<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>。</p><p>还尝试过抓取B站的弹幕，参考的这两篇博文<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup><sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup>，但都没有成功。原因是并没有找到储存弹幕的xml文件，也就没有找到弹幕的服务器地址和获取弹幕的请求参数。不过有大佬做了API<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup>，可以直接使用API获取弹幕。除了用API之外，其实也可以用BeautifulSoup等python库，因为我发现弹幕是可以直接在网页的HTML文件里面找到的。直接爬取HTML文件，根据规则过滤，拿到弹幕也是可以的。但是在F12的界面，初始的HTML文件是没有弹幕的。猜测是有脚本从服务器获取了弹幕，然后按照某种规则填充了进去。如果可以从脚本开始分析，或许能够找到存储弹幕的服务器和请求弹幕的参数。</p><p>有博文<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup>指出了之前获取弹幕XML文件的两种方法依然适用：</p><blockquote>1、<a href="https://link.segmentfault.com/?enc=7FQI20iaFUTzcrSBGS714A%3D%3D.OWdIhRuTSTzgloQrXy2yR1o3vgG8TJqaWfEKBjmxDNM%3D" rel="nofollow" target="_blank">https://comment.bilibili.com/</a>视频的cid.xml<br/>2、<a href="https://link.segmentfault.com/?enc=J%2F%2FRaTATrm9BI0eHVH5DxQ%3D%3D.VNGNW16qKPxOAn0BWUiyHrWs8m7XosEjtbkGjk4Lte7jk2TqDRc%2FkjOa1%2BmxQpm0" rel="nofollow" target="_blank">https://api.bilibili.com/x/v1/dm/list.so?oid=</a>视频的cid</blockquote><p>获取cid的方法可以去上面的博文里面查看。</p><p>虽然以上的两种方法依然适用，但是我现在，仍旧没有在F12的页面当中找到XML的传输记录</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=zqr98zU9eXcB%2F6Tly4MHnw%3D%3D.V2ZF82muL0oNzX5zJ5eA7NhjZrbnBajZe6TRRy2LH0JQzKWXY9Wu0chuVQi2xHF51qdvlkF%2BNk3EnVMi44xy4g%3D%3D" rel="nofollow" target="_blank">Wireshark 常用过滤方法</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=33kj06YJ6YdVBT68RquKqg%3D%3D.q1a%2FaF0Ybhp4C4TI6tB9Nn9Iwafp03xv0X2Zqbfn0F4im6ThvnD4HfSEz%2Fj5n9gBKeR36fsBqTaSi9M4jxSoIQ%3D%3D" rel="nofollow" target="_blank">curl命令登录网页</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=0M2AGzL17pUK04v7eLZeqg%3D%3D.TJ18EMA4tdZeI2nldSt1S8xIMESyKSkA9NjjcrnF66BmIBOcyV8VSfAnbxReETMW" rel="nofollow" target="_blank">在 Windows PowerShell 中执行命令：curl -X POST –data，报错：Invoke-WebRequest : 找不到接受实际参数“POST”的位置形式参数。的分析解决</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=BfmNaug1scTkq6SJ94DRHQ%3D%3D.9bgKuF%2BHEORxt1M8yIhVcqi%2BNjbwMVw9wLwf73n3Ot04guUb83w%2BaWxpQY8xhADm" rel="nofollow" target="_blank">在PowerShell中使用curl(Invoke-WebRequest)</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=WKJkvVrlFlZUZk7VPQF8qg%3D%3D.jFX%2F3pSAaN8gMJutKfthrOpnMNNSM1e8clcL7ZaRZZwYRqLOxUzb9Sq9TW7Igt9pFzIjmzaTLACji8QkvstiUBzOOsD%2B7lICyLvGJ4UcTccaFLIM9ULzoLmmVfiRwOiR5EY%2BC3D1WrT5wMJlCg4nzKXhx0OI6QvN4mX7gQydPnqyL8VoICV4nx4exiEqK4m3" rel="nofollow" target="_blank">Invoke-WebRequest</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"><a href="https://link.segmentfault.com/?enc=35LXEX%2BTlttP%2FLpfpcpSEA%3D%3D.Z0rQjRhVnSKVsJBPn4fa8AA9Gq%2BttnJmWQ0CcEEm6Ss6APQQ5mWXaM9SJoCnT3UOoc93v9mcfY%2FBlcGlFSy0Jg%3D%3D" rel="nofollow" target="_blank">curl命令实现上网认证登录</a>  <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"><a href="https://link.segmentfault.com/?enc=vQ7iOnStfgdMpZbbOkY4fw%3D%3D.73CcbDwGa%2FUVJtiDNU8jnXk44yvKrBQoZRKTYdUhpUSVdr9PAXGaMahwAdrl9dzNzUKN5AcYGUpJ26flVisO7g%3D%3D" rel="nofollow" target="_blank">搜狐网模拟登录案例</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"><a href="https://link.segmentfault.com/?enc=kSAox2OwjcfFQp0iTE1hzA%3D%3D.HzRe3dwvT19su4JdC5S4o3%2FC7Ahs68aX038z3Rc87EKia3OyXO8EPaTB18UexEGd78KmdmKLIRWWTSgKhoJY%2FA%3D%3D" rel="nofollow" target="_blank">配置Wireshark抓取https数据包</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"><a href="https://link.segmentfault.com/?enc=iIiJFH5YeIIVtL3NGd4liQ%3D%3D.FK17eytEck7XYw5iIB%2BsCnK77kGpkSxOFCyfmbGaHR2%2BV7DlxViLkYnCmgdO0NxUX2VmoTTYdLjivA2T6gt1Og%3D%3D" rel="nofollow" target="_blank">python爬虫----b站的弹幕获取</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"><a href="https://link.segmentfault.com/?enc=w08FUYErLIpz4eKyfKfBlQ%3D%3D.SHkDCYVUNRQiRxy8qKEOSM3Zm9XB33wgTiXXg8hax%2BkS6%2BHk9i8SFJ45Vv0zhQ0J" rel="nofollow" target="_blank">Python爬虫爬取Bilibili弹幕过程解析</a> <a href="#fnref-10" class="footnote-backref">↩</a></li><li id="fn-11"> <a href="https://link.segmentfault.com/?enc=i9c47e40WZUk3uaNkjkpOg%3D%3D.pO6J7MByht4Iy%2F2%2BOaTx%2FrW4SrimV66CwzkZeki8rOdbH3%2BxmzyMml25Sg3sCNdODWX5TBxf003qYHsW%2BhKCRw%3D%3D" rel="nofollow" target="_blank">bilibili_api，仅用 3 行代码获取B站（弹幕、评论、用户）数据</a> <a href="#fnref-11" class="footnote-backref">↩</a></li><li id="fn-12"> <a href="https://link.segmentfault.com/?enc=mpoUqRIU6dHiP4%2B%2FemSPVQ%3D%3D.Wz8IQQQ8eNgXPo9HCU0j2BX3lG8zIPDeNXG1KY1FJnEuHKqs%2BEDA2AgtoatZ3bCX1VevBD06rKVcf6grN7WPBQ%3D%3D" rel="nofollow" target="_blank">感谢大佬，B站弹幕也能抓取啦~</a> <a href="#fnref-12" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[存疑]Spyder修改新建py文件的模板 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494065</link>    <guid>https://segmentfault.com/a/1190000047494065</guid>    <pubDate>2025-12-22 18:10:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我用的是Anaconda，因此spyder安装在Anaconda的路径下面，参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>寻找文件</p><blockquote>"D:\Software\Anaconda3\Lib\site-packages\spyder\plugins\editor\plugin.py"</blockquote><p>在Spyder的安装文件夹当中，找到plugins文件夹，再去往editor文件夹，修改其中的plugin.py</p><p>打开plugin.py之后，在文件当中搜索<kbd>date</kbd>，可以找到这么一段代码</p><pre><code class="python">VARS = {
'date': time.ctime(),
'username': username,
}</code></pre><p>将<kbd>'date'</kbd>后面的<kbd>time.ctime()</kbd>改成<kbd>time.strftime("%Y-%m-%d %H:%M:%S")</kbd>，即可改变新建py文件中头部注释的时间格式</p><p>当然这个或许可以尝试着进一步说明一下</p><p>在Spyder当中找到用于新建py文件的模板<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></p><blockquote>tools-&gt;preferences-&gt;editor-&gt;advanced settings-&gt;Edit template for new modules</blockquote><p>模板当中有一个<kbd>%(date)s</kbd>，猜测此处的<kbd>date</kbd>与上文plugin.py文件当中字典变量<kbd>VARS</kbd>的<kbd>date</kbd>应该是同一个东西，即模板的<kbd>date</kbd>引用了plugin.py文件的<kbd>VARS:date</kbd>，那我们就可以尝试更多的操作了。</p><p>我之前曾经在plugin.py的<kbd>VARS</kbd>当中加了一个<kbd>folderpath</kbd>变量，然后使用函数获取将当前的工作目录，赋值给这个变量。之后在temple.py文件当中使用<kbd># python -u "%(folderpath)s\"</kbd>来引用这个变量。当时是成功了的，但是不记得那个Spyder的版本号了。</p><p>本文标题当中有<kbd>[存疑]</kbd>，也就是，我这里现在出问题了，没有正常运行。之前用Spyder的时候，做了这个修改，大概都是一年前的事情了，当时并没有写博文把过程记录下来，只是收藏了这一篇关键的博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>。关于Spyder，早就不用了，后续也没再升级版本。今天整理浏览器收藏夹，看到了这篇博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>，于是就按照回忆重新做了一遍流程，但很尴尬的是，第一步就出问题了。</p><p>问题出在<kbd>%(date)s</kbd>上，似乎其并不能成功引用plugin.py文件的<kbd>VARS:date</kbd>，导致新建文件当中显示的是<kbd>%(date)s</kbd>的原文，并不是我们所期望的当前时间。而且我之前明明已经修改过plugin.py文件，在其中加入了<kbd>VARS:folderpath</kbd>变量，现在也没了。</p><p>猜测原因可能有以下两种：</p><ul><li>有可能是在修改plugin.py文件之后，Spyder进行了升级，因此我修改过的那个文件被新文件所覆盖。但这并不能解释为何<kbd>%(date)s</kbd>失效</li><li>或者就是我找错了地方，并不在 <kbd>"D:\Software\Anaconda3\Lib\site-packages\spyder\plugins\editor\plugin.py"</kbd>这个路径当中，因为这个路径本是我如今摸索出来的，并不一定是我之前修改成功的那个路径。我是Anaconda版本的Spyder，所以博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>当中的路径并不能直接使用，是按照其中像<kbd>plugins</kbd>、<kbd>editor</kbd>这样的关键字摸索的。就算我之前修改成功了，那也是摸索的路径。而且当时还没有对路径进行记录，导致如今又摸索了一遍路径。但不管怎么说，还是有那个问题，这并不能解释为何<kbd>%(date)s</kbd>失效。</li></ul><p>因为<kbd>%(date)s</kbd>是Spyder默认的东西，不管是升级抑或是找错文件，temple.py所引用的<kbd>date</kbd>都应该不会出什么问题。</p><p>猜测有可能是版本的问题，因为我打开尘封已久的Spyder不久，就弹窗提示更新，然而被我拒掉了。由于相当长一段时间内应该都不会再用Spyder，所以本文的探索到此为止了。希望还在用的小伙伴多多探索，然后发博文分享方法。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=J5YjTd1w8hH1c7TFdlRzBg%3D%3D.po46lP6IQNDx9%2FWhxvr8mBIa3yStzs0HzcyxU01DxL694QTshNMMCpkI609uXZpu0OYg9wUSwBXk5pd4JiQ0AA%3D%3D" rel="nofollow" target="_blank">Winpython Spyder template.py模板日期格式的修改</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=xgekzlYbGI4cFNl1jPIZxg%3D%3D.zNpClaaVgArW%2FcWJWUjMLiU%2F%2BHKu1%2Byh2NxrYtaB3oR1I7Ur5gyuxrXK1tvHbgx8ecVw2brWAiCd69z5G8MZyg%3D%3D" rel="nofollow" target="_blank">Spyder python文件抬头默认内容自定义</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[pandas计算某列每行带有分隔符的数据中包含特定值的次数 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494073</link>    <guid>https://segmentfault.com/a/1190000047494073</guid>    <pubDate>2025-12-22 18:09:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>某次做一个数据的处理，要计算用户的粉丝数量，数据集大概是这样的：</p><table><thead><tr><th>传播节点微博用户id</th><th>关注用户ids</th></tr></thead><tbody><tr><td>ae26e5e3db7626dcaf6819ce5492d534</td><td>"04e9dc04d4b600d574d67b298a7dea7d,···"</td></tr><tr><td>a845733e3729a136889c07d275bcc3c5</td><td>"aebe49645667a02eae6ab6734ade24eb,···"</td></tr><tr><td>68e605feb5344fd413587b4245946c24"</td><td>77c471d3aba195b1322800602a93dc72,···"</td></tr></tbody></table><p>这里的数据，都是经过脱敏处理后的id，即每个用户和他们的关注列表。“关注用户ids”应该是字符串类型，每一行由双引号包裹，由逗号作为id之间的分隔符。要计算用户的粉丝数量，就是看他们在所有用户的关注列表当中出现了多少次，也就是要对“关注用户ids”列出现的各个id进行计数。</p><p>参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>进行以下处理</p><pre><code class="python">countN = dataI0['name'].str.split('|', expand=True).stack().value_counts()
# 计算各元素出现的次数</code></pre><p>其中，不能对数据框的列<kbd>Chart3Part['关注用户ids']</kbd>直接应用<kbd>split</kbd>，而需要先调用<kbd>str</kbd>。</p><ul><li>其中的<kbd>expand=True</kbd>是按逗号对每一行进行分割后，将其扩展成多列。</li><li><kbd>stack()</kbd>则是构造二级行索引，在原本的行索引上，将列作为二级行索引。可参考博文<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，也可见下文当中的实验三</li><li><kbd>value_counts()</kbd>是对值出现的次数进行计数，其返回值是一个pd.Series，name为被计数的列的名字，index为被计数的项，值为出现的次数</li></ul><p>博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>当中还有另一种方法，先单独对每一行进行处理，再从总的视角进行计数</p><pre><code class="python">countN = pd.Series(Counter([y for x in dataI0['name'] for y in x.split('|')]))</code></pre><p>完整的示例如下：</p><pre><code class="python">import pandas as pd

dataI0 = pd.DataFrame(data=["book","fish","icecream|book","fish","campfire|book"],columns=["name"])
print(dataI0)
# 创建数据框

#             name
# 0           book
# 1           fish
# 2  icecream|book
# 3           fish
# 4  campfire|book

# 方法一----------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True).stack().value_counts()
print(countN)
# 对name列进行计数

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64

# 方法二---------------------------------------------------------------------------------------
from collections import Counter

countN = pd.Series(Counter([y for x in dataI0['name'] for y in x.split('|')]))
print(countN)

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64

# 实验一---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|')
print(countN)
# 此处是对每一行的数据按照'|'进行分割，每一行返回的是分割后的数组

# 0              [book]
# 1              [fish]
# 2    [icecream, book]
# 3              [fish]
# 4    [campfire, book]
# Name: name, dtype: object
        
# 实验二---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True)
print(countN)
# expand=True，即分割后扩展为多列，不足之处使用None补充

#           0     1
# 0      book  None
# 1      fish  None
# 2  icecream  book
# 3      fish  None
# 4  campfire  book

# 实验三---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True).stack()
print(countN)
# stack()，即将列变为二级行索引，原行索引为一级行索引，也就是说现在的数据框只有一列了
# 所有的数据都在这一列，直接对这一列进行计数即可

# 0  0        book
# 1  0        fish
# 2  0    icecream
#    1        book
# 3  0        fish
# 4  0    campfire
#    1        book
# dtype: object

# 实验四---------------------------------------------------------------------------------------
countN = [y for x in dataI0['name'] for y in x.split('|')]
print(countN)
# 这里的思路是利用列表推导式，对每一行按照'|'进行分割后，对分割的结果进行枚举，最后得到一个列表
# 所有的结果都在列表当中

# ['book', 'fish', 'icecream', 'book', 'fish', 'campfire', 'book']

# 实验五---------------------------------------------------------------------------------------
countN = Counter([y for x in dataI0['name'] for y in x.split('|')])
print(countN)
# 博文的思路是对列表应用Counter，直接计数
# 之后再转换成为pd.Series

# Counter({'book': 3, 'fish': 2, 'icecream': 1, 'campfire': 1})

# 实验六---------------------------------------------------------------------------------------
countN = pd.Series([y for x in dataI0['name'] for y in x.split('|')])
print(countN)
# 也可以直接转换成pd.Series，所有的数据都在这一列了
# 效果与上文stack()后的结果相似

# 0        book
# 1        fish
# 2    icecream
# 3        book
# 4        fish
# 5    campfire
# 6        book
# dtype: object
    
# 实验七---------------------------------------------------------------------------------------
countN = pd.Series([y for x in dataI0['name'] for y in x.split('|')]).value_counts()
print(countN)
# 在pd.Series的基础上，也可以直接使用value_counts()

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64</code></pre><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=Yr9ZonvfNzdsxs09VIXtdQ%3D%3D.l%2Be60DrUt0yWjV5Z9C4FYCUGN%2ByuL%2FuSQKAiafr2RC66qzWa0GTEI6Pq16kZqeJI" rel="nofollow" target="_blank">python – 计算列在Pandas中包含特定值的次数</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"><a href="https://link.segmentfault.com/?enc=7M7NU1JeOKzdKQcN%2Bn5LUw%3D%3D.AgYXvQShWwbpEd7vrs4xzb3zVlRuK6bkuCjFlst4CVcENMCc9nmu4Iuie60xrSVC" rel="nofollow" target="_blank">Pandas 数据堆叠 stack</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[eBPF + OpenTelemetry：适用于任何应用的零代码自动化仪表盘 俞凡 ]]></title>    <link>https://segmentfault.com/a/1190000047494078</link>    <guid>https://segmentfault.com/a/1190000047494078</guid>    <pubDate>2025-12-22 18:08:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p><em>本文介绍如何将 eBPF 与 OpenTelemetry 结合，实现自动化、零代码的分布式追踪和可观测性系统，了解 Odigos、Beyla 和 OpenTelemetry eBPF 等工具的工作原理、适用场景以及如何在生产环境中设置。原文：<a href="https://link.segmentfault.com/?enc=wa7uSRiZ%2BZFlD2ptXWW6Fg%3D%3D.MY3Q7Dlojto%2BTLXgsdC9PeasoRGb4EY%2F51zY3%2FnU8fwwycGmmxYgW%2Fj3m%2FqAiIDjKdHG5uB2iewyJwsaeQTBKg2fVXa6wDJbd4Mb3NDYNs8%2BIGPZf1UTkSds8KaWy1Uh" rel="nofollow" title="Using eBPF with OpenTelemetry: Zero-Code Auto-Instrumentation for Any Application" target="_blank">Using eBPF with OpenTelemetry: Zero-Code Auto-Instrumentation for Any Application</a></em></p><p>如果能在所有服务间实现完整的分布式追踪，<strong>而无需添加一行仪表盘代码</strong>，怎么样？</p></blockquote><p>传统 OpenTelemetry 仪表盘需要添加 SDK、配置导出器，并用 span 包装代码。虽然功能强大，但需要付出不少努力，尤其是系统中有数十种不同语言的服务时。</p><p><a href="https://link.segmentfault.com/?enc=eLVzoQTw8WH%2B6NVgKFLBpw%3D%3D.dQyKP%2FE8qZ%2FKlZJA%2BnGPdzlGmr639psCpZ0yDN%2BmHN1K6faQuLV6SYnD8NszTL6HCFdJq%2Bv5yjFoTbXgxnSltiFDgADGaghE2RvL5mm5ecoLHDN1XWN5%2FVmdfXXn5cLA" rel="nofollow" title="eBPF" target="_blank">eBPF</a> 完全改变了这一模式。通过从 Linux 内核观察应用，基于 eBPF 的工具可以自动生成兼容 OpenTelemetry 的追踪、指标和配置文件，而无需触及应用代码 。</p><p>本文将介绍如何将 eBPF 与 OpenTelemetry 结合，实现强大的零代码可观测性。</p><hr/><h2>1. 问题：大规模仪表盘</h2><p>传统 OpenTelemetry 仪表盘遵循以下模式：</p><ol><li>为每个服务添加 OTel SDK</li><li>配置导出器</li><li>测量入口点（HTTP 处理程序，gRPC 方法）</li><li>为重要操作添加 span</li><li>跨服务边界传播上下文</li><li>每个服务、每种语言都要重复</li></ol><p>对于只有少量服务的小团队来说，还算可以管理。但请考虑：</p><p>|场景|挑战|<br/>|-|-|<br/>| 50+ 微服务|需要数周时间集成跨所有服务的 SDK|<br/>|多语言技术栈|Go、Python、Node.js、Java、Rust 等不同的 SDK……|<br/>|遗留服务|代码不容易修改，没人愿意碰|<br/>|第三方服务|无法访问源代码|<br/>|快速部署|新服务出现的速度比测量它们的速度还快|</p><p>结果呢？可观测性缺口。有些服务有追踪，有些没有。未安装测量的服务中断上下文传播。系统一定程度上正在裸跑。</p><hr/><h2>2. eBPF 如何实现自动化测量</h2><p>eBPF 通过从应用外部 —— 内核层面观察应用来解决这个问题。</p><h5>eBPF 能看到什么</h5><p>由于 eBPF 会钩入内核函数和系统调用，可以观察到：</p><p>|层|eBPF 看见|<br/>|-|-|<br/>|网络|每一次 TCP 连接、HTTP 请求/响应、DNS 查询|<br/>|系统调用|文件 I/O，进程创建，内存分配|<br/>|用户功能|函数通过 uprobe 进入/退出（如果有符号）|<br/>|语言运行时|Go、Node.js、Python、Java 运行时内部结构|</p><h5>这些如何成为 OpenTelemetry 数据</h5><p>基于 eBPF 的自动化测量工作原理：</p><ol><li>将<strong>探针附加</strong>到已知入口点（HTTP 库、gRPC 处理器、数据库驱动程序）</li><li>从请求中<strong>提取上下文</strong>（从头部提取追踪 ID、请求元数据）</li><li>利用内核时间戳<strong>测量时序</strong></li><li><strong>关联</strong>相关请求/响应对</li><li><strong>导出</strong>为标准 OpenTelemetry Protocol（OTLP）数据</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494080" alt="" title=""/></p><h2>3. eBPF + OpenTelemetry 架构</h2><p>典型生产配置如下：</p><h5>组件</h5><p>|组件| 职责|<br/>|-|-|<br/>|eBPF 代理|运行在每个节点上，连接 eBPF 程序，生成遥测数据|<br/>|OTel 收集器|接收、处理 OTLP 数据，并导出到后端|<br/>|后端|存储和可视化追踪/指标（OneUptime、Jaeger、Tempo）|</p><h5>部署模式</h5><h6>模式 1：DaemonSet（Kubernetes）</h6><pre><code class="yaml"># 运行在每个 node 上的 eBPF 代理
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ebpf-auto-instrumenter
spec:
  selector:
    matchLabels:
      app: ebpf-agent
  template:
    spec:
      hostPID: true      # eBPF 需要
      hostNetwork: true  # 网络追踪需要
      containers:
      - name: agent
        securityContext:
          privileged: true  # eBPF 需要</code></pre><h6>模式 2：Sidecar（每个 Pod 一个）</h6><pre><code class="yaml"># eBPF 代理作为 sidecar (更为隔离，更多开销)
spec:
  containers:
  - name: my-app
    image: my-app:latest
  - name: ebpf-sidecar
    image: ebpf-agent:latest
    securityContext:
      privileged: true</code></pre><h6>模式 3：独立（非 Kubernetes）</h6><pre><code># 直接在主机上运行
sudo ./beyla --config config.yaml</code></pre><h2>4. 工具比较：Odigos vs Beyla vs Pixie</h2><p>下面比较几种将 eBPF 与 OpenTelemetry 结合起来的工具。</p><h5>Grafana Beyla</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|HTTP/gRPC 自动监测|<br/>|语言|Go, Python, Node.js, Java, Rust, .NET, Ruby|<br/>|输出|OTLP (追踪 + 指标)|<br/>|部署|独立二进制或 Kubernetes|<br/>|许可证|Apache 2.0|<br/>|最佳实践|简单部署，Grafana 技术栈用户|</p><h5>Odigos</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|带上下文传播的全分布式追踪|<br/>|语言|Go, Python, Node.js, Java, .NET|<br/>|输出|OTLP（追踪）|<br/>|部署|Kubernetes 原生(operator)|<br/>|许可证|Apache 2.0|<br/>|最佳实践|Kubernetes 环境，分布式追踪|</p><h5>Pixie</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|全栈可观测性，带集群内存储|<br/>|语言|Go, C/C++, Python, Node.js, Java, Rust|<br/>|输出|Pixie 格式（可导出为 OTel）|<br/>|部署|仅限 Kubernetes|<br/>|许可证|Apache 2.0|<br/>|最佳实践|调试、临时查询、全面可视化|</p><h5>快速决策指南</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494081" alt="" title="" loading="lazy"/></p><hr/><h2>5. 设置 Beyla（Grafana 的 eBPF 自动化测量）</h2><p>Beyla 是入门基于 eBPF 的 OpenTelemetry 测量的最简单方式。</p><h5>前置条件</h5><ul><li>Linux 内核 5.8+（支持 BTF）</li><li>Root/privileged 访问</li><li>目标应用程序正在运行</li></ul><h5>安装</h5><pre><code class="bash"># 下载最新版本
curl -LO https://github.com/grafana/beyla/releases/latest/download/beyla-linux-amd64.tar.gz
tar xzf beyla-linux-amd64.tar.gz
sudo mv beyla /usr/local/bin/</code></pre><h5>配置</h5><p>创建 <code>beyla-config.yaml</code>：</p><pre><code class="yaml"># beyla-config.yaml
open_port: 8080  # 测量进程监听端口

# 或目标的可执行名称
# executable_name: "my-service"

# 或进程 ID
# pid: 12345

# OTLP 导出配置
otel_traces_export:
  endpoint: http://localhost:4317  # OTel 收集器

otel_metrics_export:
  endpoint: http://localhost:4317
  
# 可选: 添加资源参数
attributes:
  kubernetes:
    enable: true  # 自动检测 K8s 元数据
  
# 采样 (可选)
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.1"  # 10% 采样</code></pre><h5>运行 Beyla</h5><pre><code class="bash"># 通过配置文件执行
sudo beyla --config beyla-config.yaml

# 或者通过环境变量
sudo BEYLA_OPEN_PORT=8080 \
     OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 \
     beyla</code></pre><h5>Kubernetes 部署</h5><pre><code class="yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: beyla
  namespace: observability
spec:
  selector:
    matchLabels:
      app: beyla
  template:
    metadata:
      labels:
        app: beyla
    spec:
      hostPID: true
      serviceAccountName: beyla
      containers:
      - name: beyla
        image: grafana/beyla:latest
        securityContext:
          privileged: true
          runAsUser: 0
        env:
        - name: BEYLA_OPEN_PORT
          value: "8080,3000,9090"  # 测量端口
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otel-collector.observability:4317"
        - name: BEYLA_KUBE_METADATA_ENABLE
          value: "true"
        volumeMounts:
        - name: sys-kernel
          mountPath: /sys/kernel
          readOnly: true
      volumes:
      - name: sys-kernel
        hostPath:
          path: /sys/kernel</code></pre><h5>Beyla 采样数据</h5><p>运行后，Beyla 会自动生成：</p><p><strong>追踪</strong>：</p><ul><li>HTTP 服务器 span（方法、路径、状态、时长）</li><li>HTTP 客户端 span（外出请求）</li><li>gRPC span（方法，状态）</li><li>SQL 查询 span（如果使用支持的驱动）</li></ul><p><strong>指标</strong>：</p><ul><li><code>http.server.request.duration</code>（直方图）</li><li><code>http.server.request.body.size</code></li><li><code>http.client.request.duration</code></li><li><code>rpc.server.duration</code></li><li><code>rpc.client.duration</code></li></ul><hr/><h2>6. 为 Kubernetes 设置 Odigos</h2><p>Odigos 提供了更全面的分布式追踪，并实现了自动上下文传播。</p><h5>安装</h5><pre><code class="bash"># 安装 Odigos CLI
brew install odigos-io/homebrew-odigos-cli/odigos

# 或者直接下载
curl -LO https://github.com/odigos-io/odigos/releases/latest/download/odigos-cli-linux-amd64
chmod +x odigos-cli-linux-amd64
sudo mv odigos-cli-linux-amd64 /usr/local/bin/odigos</code></pre><h5>部署到 Kubernetes</h5><pre><code class="bash"># 在集群里安装 Odigos
odigos install

# 创建:
# - odigos-system namespace
# - Odigos operator
# - Instrumentor DaemonSet
# - OTel Collector (可选)</code></pre><h5>配置目的地</h5><pre><code class="bash"># 添加可观测性后端
odigos ui

# 或通过 CLI
odigos destination add oneuptime \
  --endpoint https://otlp.oneuptime.com \
  --api-key YOUR_API_KEY</code></pre><h5>测量命名空间</h5><pre><code class="bash"># 测量命名空间中的所有工作负载
odigos instrument namespace my-app-namespace

# 或指定工作负载
odigos instrument deployment my-service -n my-namespace</code></pre><h5>Odigos 运作方式</h5><p>Odigos 比简单的 eBPF 追踪更智能：</p><ol><li><strong>语言检测</strong>：自动检测运行时（Go、Java、Python 等）</li><li><strong>合适的测量方式</strong>：Go 使用 eBPF，Java/Python 注入代理</li><li><strong>上下文传播</strong>：确保跨越服务边界追踪上下文</li><li><strong>无代码更改</strong>：所有注入均发生在运行时</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494082" alt="" title="" loading="lazy"/></p><hr/><h2>7. 自动获取的内容</h2><p>以下是基于 eBPF 工具能自动获取或者不能获取的内容：</p><h5>自动获取</h5><p>|信号|详情|<br/>|-|-|<br/>|HTTP 服务器请求|方法、路径、状态码、时长、消息头|<br/>|HTTP 客户端请求|发送请求，含目的地和时间|<br/>|gRPC 调用|方法、状态、时长（包括服务器和客户端）|<br/>|数据库查询|查询文本、时长、数据库类型（因工具而异）|<br/>|DNS 查询|域、查询时间、结果|<br/>|TCP 连接|源、目的、传输字节数|<br/>|TLS 握手|证书信息，握手时间|</p><h5>部分获取（因工具/语言而异）</h5><p>|信号|局限性|<br/>|-|-|<br/>|消息队列|Kafka/RabbitMQ 的支持各不相同，可能需要手动设置|<br/>|自定义协议|需要特定工具的支持|<br/>|内部函数调用|只有在符号信息可用的情况下|<br/>|业务逻辑上下文|无法推断用户 ID、订单 ID 等信息|</p><h5>无法获取（需要手动测量）</h5><p>|信号|为什么|<br/>|-|-|<br/>|自定义 span 属性|eBPF 不知道业务域|<br/>|应用错误|异常详情，stack trace（部分）|<br/>|自定义指标|业务关键绩效指标（KPI），转化率|<br/>|Baggage/Context|自定义传播数据|</p><hr/><h2>8. 将 eBPF 数据与手动测量进行关联</h2><p>最佳方法通常是混合式：基础覆盖用 eBPF，重要细节用手动测量。</p><h5>策略：分层测量</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494083" alt="" title="" loading="lazy"/></p><h6>示例：混合配置</h6><pre><code class="golang">// Go 服务 - eBPF 自动捕获 HTTP 处理
// 为重要业务逻辑添加手动 span

func (s *OrderService) CreateOrder(ctx context.Context, req *OrderRequest) (*Order, error) {
    // eBPF已经获取：HTTP POST /orders、计时、状态

    // 业务逻辑细节的手动 span
    ctx, span := tracer.Start(ctx, "order.validate")
    err := s.validateOrder(ctx, req)
    span.End()
    if err != nil {
        // 手动：添加 eBPF 看不到的错误细节
        span.RecordError(err)
        span.SetStatus(codes.Error, "validation failed")
        return nil, err
    }
    
    // eBPF 自动捕获数据库调用
    // 手动：添加业务上下文
    ctx, span = tracer.Start(ctx, "order.save")
    span.SetAttributes(
        attribute.String("order.customer_id", req.CustomerID),
        attribute.Float64("order.total", req.Total),
        attribute.Int("order.items_count", len(req.Items)),
    )
    order, err := s.repo.Save(ctx, req)
    span.End()
    
    return order, err
}</code></pre><h6>确保相关性有效</h6><p>为了让 eBPF span 和手动 span 出现在同一条追踪中：</p><ol><li><strong>相同的 Trace ID</strong>：eBPF 工具从输入请求中提取 <code>traceparent</code></li><li><strong>上下文传播</strong>：手动 span 必须使用相同的上下文</li><li><strong>一致导出</strong>：eBPF 和手动测量都导出到同一个收集器</li></ol><pre><code class="yaml"># OTel Collector 配置合并两个源
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    
  # 添加一致的资源属性
  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert

exporters:
  otlp:
    endpoint: https://oneuptime.com/otlp
    headers:
      x-oneuptime-token: ${ONEUPTIME_TOKEN}

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [otlp]</code></pre><hr/><h2>9. 性能开销</h2><p>关键问题： 运行基于 eBPF 的自动化测量的成本是多少？</p><h5>测量额外开销</h5><p>|工具|CPU 开销|内存|时延影响|<br/>|-|-|-|-|<br/>|Beyla|1-3%|~50-100MB|&lt; 1ms|<br/>|Odigos|2-5%|~100-200MB|&lt; 2ms|<br/>|Pixie|2-5%|~500MB-1GB|&lt; 1ms|</p><p><em>注意：实际开销因工作负载、采样率和追踪端点数量而异。</em></p><h5>增加额外开销的因素</h5><p>| 因素| 影响| 缓解措施|<br/>|-|-|-|<br/>| 高请求量|更多 eBPF 事件待处理| 增加采样|<br/>| 追踪太多端点| 连接太多探针| 要有选择性|<br/>| 全载荷捕获| 用于复制数据的内存/CPU|禁用或限制|<br/>| 低采样率| 更多数据需导出| 使用头部采样|</p><h5>降低开销</h5><pre><code class="yaml"># Beyla 示例: 通过采样降低开销
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.01"  # 1% 采样

# 隔离高数据量、低价值的端点
routes:
  ignored:
    - /health
    - /ready
    - /metrics</code></pre><hr/><h2>10. 局限性及何时使用手动测量</h2><p>eBPF 自动测量功能强大，但并非魔法，需要知道取舍。</p><p><strong>在以下情况下使用 eBPF 自动化测量</strong>：</p><p>✅ 需要在多个服务中快速实现基线可观测性</p><p>✅ 不能修改应用代码（遗留版本，第三方代码）</p><p>✅ 需要一致的 HTTP/gRPC/数据库追踪，而不是每个服务单独设置</p><p>✅ 需要网络层面的可视化（连接、DNS）</p><p>✅ 身处混合语言的 Kubernetes 环境中</p><p><strong>需要使用手动测量</strong>：</p><p>✅ 需要自定义业务属性（用户 ID、订单 ID、功能标志）。</p><p>✅ 需要详细的错误信息和 stack traces</p><p>✅ 需要自定义指标（业务关键绩效指标、特定事件的计数器）</p><p>✅ 追踪没有 eBPF 支持的非 HTTP 协议</p><p>✅ 需要跨服务上下文的 baggage 传播</p><p>✅ 要控制 span 名称和结构</p><h5>eBPF 自动化测量的局限性</h5><p>|限制|详情|<br/>|-|-|<br/>|仅限 Linux|不支持没有 Linux 内核的 Windows、macOS 或容器运行时|<br/>|内核版本|需要 5.x 以上才能获得最佳效果，部分功能需要 5.8 以上|<br/>|特权访问|必须提升权限运行（安全考虑）|<br/>|符号可用性|剥离符号的 Go 二进制可执行文件会降低可见度|<br/>|加密流量|TLS 检查需要额外设置|<br/>|应用上下文|无法从网络数据推断业务含义|</p><hr/><h2>11. 最佳生产实践</h2><h5>安全考量</h5><p>运行 eBPF 代理需要提升权限。降低风险：</p><pre><code class="yaml"># Kubernetes: 严格使用 RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ebpf-agent-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
# 避免授予不必要的权限</code></pre><pre><code class="yaml"># 尽可能使用 seccomp 配置文件
securityContext:
  seccompProfile:
    type: RuntimeDefault</code></pre><h5>限制资源</h5><pre><code class="yaml">containers:
- name: ebpf-agent
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi</code></pre><h5>过滤与抽样</h5><pre><code class="yaml"># 不要追踪每件事 —— 专注于重要的事情
routes:
  patterns:
    - /api/*        # Trace API calls
    - /graphql      # Trace GraphQL
  ignored:
    - /health       # Skip health checks
    - /metrics      # Skip metrics endpoint
    - /favicon.ico  # Skip static assets

# 通过采样控制数据量
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.1"  # 10% in production</code></pre><h5>逐步推广</h5><pre><code class="bash"># 从非生产环境开始
odigos instrument namespace staging

# 验证开销和数据质量
# 然后扩展到生产环境
odigos instrument namespace production</code></pre><h5>监控</h5><pre><code class="yaml"># 导出 eBPF 代理指标
prometheus:
  port: 9090
  path: /metrics

# 代理有问题时告警
# - 高 CPU 使用率
# - 事件丢失
# - 导出失败</code></pre><hr/><h2>12. 结论</h2><p>基于 eBPF 的自动化测量代表了可观测性的范式转变。通过将测量迁移到内核级，我们可以：</p><ul><li><strong>消除测量负担</strong>：不再按服务集成 SDK</li><li><strong>实现全覆盖</strong>：观察任何应用，任何语言</li><li><strong>减少盲点</strong>：发现那些被忽视的服务</li><li><strong>加快上线速度</strong>：新服务可立即被观测到</li></ul><p>但并不能完全取代传统测量。最佳可观测性策略结合了：</p><ol><li>eBPF 用于基线基础设施层级可视化</li><li>针对特定框架上下文的自动化测量库</li><li>为业务关键范围和自定义属性提供手动测量</li></ol><p>像 Beyla 和 Odigo 这样的工具让入门变得前所未有的简单。如果应用运行在 Kubernetes 和 Linux 上，只需要几分钟就可以实现整个分布式追踪技术栈。</p><hr/><h2>要点</h2><ol><li>eBPF 通过从内核观测应用实现<strong>零代码仪表化</strong></li><li><strong>OpenTelemetry 兼容性</strong>意味着 eBPF 数据会流入现有可观测栈</li><li><strong>选择合适的工具</strong>：Beyla 简化应用，Odigos 支持 Kubernetes 分布式追踪，Pixie 负责调试</li><li><strong>混合方法效果最佳</strong>：eBPF 用于覆盖，手动测量用于业务环境</li><li><strong>开销低</strong>（1-5% CPU），但要监控并使用采样</li><li><strong>安全问题</strong>：eBPF 需要特权，授权范围要适当</li><li><strong>从小处开始</strong>：先从非生产环境开始，再扩展到生产环境</li></ol><hr/><h2>延伸阅读</h2><p><a href="https://link.segmentfault.com/?enc=8rS61RcxeoPPcZNrskShnA%3D%3D.yWYeDN1rFKlSFKHoEAEMW%2FGzrHqF0n74%2FuzNqRyjvPfwipl7Emw%2BdigB3tUCmhUj5opWZ%2BtBeJaQOHRewhmchRqBnbUux3gUpUbXuIkfP1U%2BgD9SPjFDloVc0V9lcITJ" rel="nofollow" title="What is eBPF and How Does It Work?" target="_blank">What is eBPF and How Does It Work?</a> —— 深入探讨 eBPF 基础知识</p><p><a href="https://link.segmentfault.com/?enc=KD42xNyg5WGQG%2FB2CrgnUw%3D%3D.puICwmFgszwjsR4ctBcCupEfRWAUBnNl8qcv61AhMb8CrNHYZUpPJoNoMv7LEX%2BThjihR8AjOy3BpSe3lWfzaqYPjxaTSoB3urUVH%2FHj0Q%2BSYt6GDB3DEXq6n7Tw3UmQ" rel="nofollow" title="Traces and Spans in OpenTelemetry" target="_blank">Traces and Spans in OpenTelemetry</a> —— 理解分布式追踪</p><p><a href="https://link.segmentfault.com/?enc=9J5ycn9OXLrrVvbn7Y31eA%3D%3D.7yciEnjaJhg3q7Qnk%2F%2FL7bPufcmb7o%2BMbYDx6eQuWM6FXFl08YnbFFwt83hW0EqDfCQmITekntFdVzBVqnZlN0l5dI9W95NQMtLuveyEihcXlnCW8Jz1j%2FVG7R4tqNc3" rel="nofollow" title="What are Metrics in OpenTelemetry?" target="_blank">What are Metrics in OpenTelemetry?</a> —— 指标基础</p><p><a href="https://link.segmentfault.com/?enc=qBm0rFHA4siHBzhbwO%2FxhQ%3D%3D.VS58lWU81YvHE4cg5%2FnpheZv%2B5CpenwKgqkMZA5fV5zBEf%2FFCNTREapFQZmNysXXlkZ2gj4lfm9YNMGywjc7dVu3CdihMxLIx8Kofw9JQ%2F%2BoO38%2F9%2BTkvBm71YqrLwCCBiYcUj9JqmHygSea8swEqg%3D%3D" rel="nofollow" title="Logs, Metrics &amp; Traces: The Three Pillars" target="_blank">Logs, Metrics &amp; Traces: The Three Pillars</a> —— 完整的可观察性概述</p><p><a href="https://link.segmentfault.com/?enc=ycDnA7QgIJqzJiWgIgplFw%3D%3D.X7Oh6BFW9rRwXhFN17q8S0DrnXYPgajVTcSTYc0Pi2QaHtLevs%2BtXQtpaa%2BaPxGVyQp5yeElV78sr1h%2FELqo4l9PoBjvHDsC5HiF7FvCOeQ%3D" rel="nofollow" title="Basics of Profiling" target="_blank">Basics of Profiling</a> —— 需要更深入的性能洞察时</p><hr/><blockquote>Hi，我是俞凡，一名兼具技术深度与管理视野的技术管理者。曾就职于 Motorola，现任职于 Mavenir，多年带领技术团队，聚焦后端架构与云原生，持续关注 AI 等前沿方向，也关注人的成长，笃信持续学习的力量。在这里，我会分享技术实践与思考。欢迎关注公众号「DeepNoMind」，星标不迷路。也欢迎访问独立站 <a href="https://link.segmentfault.com/?enc=kojG2x4nVW9nQASnfwHI0Q%3D%3D.9Cxcxx28t2gDkYYuzPS2%2F5CDzNMvr%2BbDkPP8wdwiY%2Bk%3D" rel="nofollow" title="www.DeepNoMind.com" target="_blank">www.DeepNoMind.com</a>，一起交流成长。</blockquote><p>本文由<a href="https://link.segmentfault.com/?enc=dmWMj5frZVr6HncVLM7Jxg%3D%3D.7%2BlKZ5bt1vJb0NSB221%2FuN4T%2FO1cC7KgVoY9YF1bnjU%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[一文认识:低代码平台是什么,低代码的本质,未来发展以及适合哪些行业? 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047494089</link>    <guid>https://segmentfault.com/a/1190000047494089</guid>    <pubDate>2025-12-22 18:08:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>最近有不少朋友问我：</p><p>“你之前说用低代码平台搭了个公司内部系统，这玩意到底是啥？我们公司的人能用得上吗？”</p><p>说实话，低代码/零代码这些年在互联网行业已经烂大街，但在有些行业，比如制造业/工程行业，甚至是工程公司、施工单位、监理、设计院里，其实都还属于“新鲜物件”。</p><p>今天我就写一篇不需要任何IT背景也能看懂的低代码科普——让你知道它到底是什么、能解决什么问题、对制造业/工程行业有没有价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494091" alt="image.png" title="image.png"/></p><p><strong>一、低代码技术的本质解构</strong></p><p>1、低代码定义</p><p>低代码开发平台（Low-Code Development Platform, LCDP）是一种通过可视化建模和配置而非传统手工编码来创建应用程序的开发环境。低代码的核心在于它将软件开发从专业程序员的专属领域，转变为业务人员也能参与的过程。</p><p>技术层面上，低代码平台通常包含以下关键组件：</p><p>可视化建模工具：通过拖拽UI组件、定义数据模型、配置业务流程的可视化界面</p><p>模型驱动架构：系统自动将可视化模型转换为可执行代码</p><p>预构建模块库：包括常见业务功能模块（表单、报表、工作流等）</p><p>集成连接器：与现有系统（ERP、CRM、数据库等）的标准接口</p><p>一键部署机制：简化测试、部署和运维流程</p><p>2、低代码与零代码的区别</p><p>理解低代码技术，首先要做好区分：</p><p>零代码平台：面向完全无编程背景的业务用户，通过完全可视化的方式构建应用，适用于标准化程度高、逻辑相对简单的场景</p><p>低代码平台：保留一定程度的手工编码能力，允许专业开发者扩展平台功能，处理复杂业务逻辑和集成需求</p><p>实际应用中，大多数平台处于两者之间，既提供可视化构建能力，也开放API和自定义代码接口，形成灵活的技术栈。</p><p>3、技术演进</p><p>低代码并非全新概念，其思想可追溯到20世纪90年代的第四代编程语言（4GL）和快速应用开发（RAD）工具。然而，现代低代码平台的突破在于：</p><p>云计算架构：基于云原生技术，实现弹性扩展和便捷访问</p><p>移动优先设计：天然支持移动端应用生成</p><p>AI增强：部分平台集成AI能力，如智能表单识别、流程优化建议、代码自动编写</p><p>开放生态：丰富的第三方组件市场和模板库</p><p><strong>二、制造业/工程行业为何特别适合“低代码”？</strong></p><p>1、行业痛点与数字化的矛盾</p><p>拿工程行业举例，工程业务具有独特特征，这些特征既创造了数字化需求，也构成了实施障碍：</p><p>高度项目化与临时性：每个工程项目都是独特的临时性组织，需求差异大、生命周期有限。传统定制开发成本高、周期长，往往项目结束了系统才上线。</p><p>业务流程碎片化：从材料管理、进度控制到质量安全监督，工程管理涉及数十个专业流程，且不同企业、不同项目流程各异。</p><p>现场与办公室的割裂：大量业务发生在施工现场，但数据处理和分析需要在办公室完成，传统系统难以无缝衔接。</p><p>人员流动性大：项目团队随项目开始而组建、随项目结束而解散，系统培训成本高。</p><p>2、低代码与工程管理的契合点</p><p>需求匹配度</p><p>工程企业的IT需求呈现典型的二八分布：20%的核心系统（如财务、ERP）需要高度标准化和稳定性，适合采购成熟产品；80%的业务应用（如专项检查、临时报表、项目特定流程）需求分散、变化频繁、生命周期短——这正是低代码的优势领域。</p><p>成本效益比</p><p>传统软件开发模式下，一个中等复杂度的业务系统（如材料验收系统）开发成本通常在50-60万元，后续每次修改需数万元和数周时间。低代码平台将初始构建成本降低至传统模式的20%-50%，修改成本降低90%以上，且时间缩短为小时或天级。</p><p>业务与技术融合</p><p>工程管理的核心是专业知识——规范标准、工艺流程、安全要求、材料特性等。传统开发模式中，业务人员需将需求翻译给开发人员，存在信息失真风险。低代码平台使业务专家能够直接参与甚至主导系统构建，确保业务逻辑的准确性。</p><p>3、工程行业的低代码应用成熟度曲线</p><p>根据技术采纳生命周期理论，工程行业的低代码应用正处于从早期采用者向早期大众过渡的阶段：</p><p>创新者（2015-2018）：少数大型工程企业试点</p><p>早期采用者（2019-2021）：更多企业尝试用于非核心业务</p><p>早期大众（2022-现在）：开始在核心业务流程中应用</p><p>晚期大众（预计2025年后）：成为标准工具之一</p><p><strong>三、工程行业低代码应用场景解析</strong></p><p>1、材料管理系统</p><p>传统工程材料管理多依赖Excel表格，面临版本混乱、数据滞后、缺乏协同等问题。基于低代码构建的材料管理系统可实现：</p><p>多维数据建模</p><p>材料主数据（规格、型号、技术参数）</p><p>供应商信息（资质、评价、历史合作）</p><p>库存状态（在途、在库、已领用）</p><p>价格信息（合同价、市场价、历史价）</p><p>智能业务流程</p><p>采购申请→审批→订单生成→到货验收→入库→领用→结算的全流程数字化</p><p>自动关联设计用量与实际消耗，预警超耗风险</p><p>移动端扫码验收，自动匹配采购订单</p><p>数据分析与可视化</p><p>材料成本占工程造价的实时分析</p><p>供应商绩效自动评价</p><p>库存周转率、资金占用分析</p><p>2、质量安全管理</p><p>低代码平台可构建的质量安全管理系统超越传统纸质检查表，实现：</p><p>检查标准数字化</p><p>将规范条文转化为可执行的检查项</p><p>根据不同工程类型（房建、市政、公路）配置不同检查模板</p><p>支持图文并茂的问题描述</p><p>闭环整改流程</p><p>问题发现→整改通知→整改实施→复查验证的全流程追踪</p><p>自动分配责任人、设定整改期限、发送提醒</p><p>严重问题自动升级通知机制</p><p>风险预警与分析</p><p>基于历史数据的常见问题预测</p><p>安全隐患趋势分析</p><p>质量安全绩效可视化看板</p><p>3、进度管理系统</p><p>传统进度管理依赖Project或简单甘特图，难以应对工程变更。低代码进度管理系统提供：</p><p>多级计划联动</p><p>总进度计划→月计划→周计划→日计划的层层分解与关联</p><p>实际进度与计划进度的可视化对比</p><p>关键路径动态计算与预警</p><p>进度数据自动采集</p><p>移动端现场进度填报</p><p>与BIM模型关联，可视化展示进度状态</p><p>自动关联工程量完成情况</p><p>延误影响分析</p><p>进度延误对后续工序的自动影响分析</p><p>资源冲突预警</p><p>进度索赔资料自动整理</p><p>4、协同办公与流程审批</p><p>工程项目的多方参与方（业主、设计、施工、监理、分包）需要高效协同。低代码可构建：</p><p>统一协同平台</p><p>设计图纸在线审查与批注</p><p>工程联系单、变更签证的数字化流程</p><p>会议纪要、指令通知的自动分发与确认</p><p>智能审批流引擎</p><p>根据金额、类型、紧急程度自动路由审批流程</p><p>移动端审批，支持手写签名</p><p>审批时限监控与超时提醒</p><p>知识积累与复用</p><p>问题处理经验的知识库积累</p><p>优秀施工方案的模板化</p><p>常见技术问题的解决方案库</p><p><strong>四、低代码平台的技术评估与选型指南</strong></p><p>1、工程行业选型关键指标</p><p>选择低代码平台时，工程企业应重点关注以下维度：</p><p>工程适配性</p><p>是否支持离线操作（应对施工现场网络不稳定）</p><p>移动端体验是否流畅</p><p>是否支持拍照、定位、扫码等工程常用功能</p><p>扩展与集成能力</p><p>与常用工程软件（AutoCAD、Revit、Project）的集成能力</p><p>与现有系统（财务、人力资源）的数据接口</p><p>自定义组件的开发支持</p><p>数据安全与合规</p><p>数据存储位置和备份机制</p><p>权限控制粒度（能否实现项目部、公司多级权限）</p><p>操作日志和审计跟踪</p><p>成本结构透明性</p><p>许可模式（按用户、按应用、混合模式）</p><p>隐藏成本（培训、定制开发、维护）</p><p>长期总拥有成本（TCO）估算</p><p>2、主流平台特性对比</p><p>国内平台</p><p>钉钉宜搭/飞书多维表格</p><p>优势：与办公平台深度集成，用户无需额外账号；模板丰富，上手极快</p><p>适用场景：中小企业内部管理、审批流程、简单数据收集</p><p>工程适用性：适合行政管理、人事考勤等通用场景，专业工程功能需较多定制</p><p>织信</p><p>优势：业务流程引擎强大；支持复杂业务逻辑；有较多工程行业案例</p><p>适用场景：中等复杂度的业务系统，如材料管理、质量检查</p><p>工程适用性：较高，有专门的项目管理行业模板</p><p>简道云/轻流</p><p>优势：表单和流程设计简单直观；数据处理能力强；移动端体验好</p><p>适用场景：数据收集和流程审批类应用</p><p>工程适用性：适合巡检、验收、报验等现场数据采集场景</p><p>国外平台</p><p>Microsoft Power Apps</p><p>优势：与Office 365生态无缝集成；AI能力强大；企业级安全控制</p><p>适用场景：已有微软生态的企业，需要深度定制和复杂集成</p><p>工程适用性：高，但需要较强的技术能力配置</p><p>Airtable</p><p>优势：表格界面直观；视图类型丰富；自动化能力强</p><p>适用场景：数据管理和协作类应用</p><p>工程适用性：适合项目管理、设备台账等表格密集型场景</p><p>3、平台选型决策框架</p><p>建议采用四阶段评估法</p><p>第一阶段：需求梳理</p><p>列出3-5个优先实施场景</p><p>明确功能需求、用户规模、集成需求</p><p>评估内部技术能力（是否有IT支持）</p><p>第二阶段：平台初选</p><p>选择3-4个符合基本要求的平台</p><p>申请试用账号，进行原型构建</p><p>评估学习曲线和开发效率</p><p>第三阶段：深度验证</p><p>选择一个典型业务场景进行完整构建</p><p>测试性能、移动端体验、离线能力</p><p>评估长期成本和技术支持</p><p>第四阶段：试点实施</p><p>选择一个小范围试点项目</p><p>收集用户反馈，评估实际效果</p><p>制定推广计划和培训方案</p><p><strong>五、实施策略与成功要素</strong></p><p>1、渐进式实施路径</p><p>低代码应用的成功往往取决于实施策略。建议采用小步快跑、迭代优化的方法：</p><p>第一阶段：单点突破（1-3个月）</p><p>选择一个痛点明显、范围清晰的场景</p><p>快速构建最小可行产品（MVP）</p><p>收集反馈，建立信心</p><p>第二阶段：横向扩展（3-6个月）</p><p>基于成功经验，扩展至相关业务领域</p><p>建立内部低代码开发能力</p><p>制定应用开发和治理规范</p><p>第三阶段：纵向深化（6-12个月）</p><p>构建更复杂的集成应用</p><p>建立企业级低代码平台</p><p>培养业务部门自主开发能力</p><p>第四阶段：生态构建（12个月以上）</p><p>形成低代码开发社区</p><p>与合作伙伴共享应用模板</p><p>探索创新应用场景</p><p>2、组织能力建设</p><p>技术工具的成功应用离不开组织能力的支撑：</p><p>角色定义与培养</p><p>公民开发者（业务人员）：掌握基础平台操作，能构建简单应用</p><p>低代码专家：精通平台高级功能，能设计复杂业务逻辑</p><p>平台管理员：负责用户管理、权限控制、性能监控</p><p>治理机制建立</p><p>应用上线审批流程</p><p>数据安全和隐私保护规范</p><p>系统维护和升级计划</p><p>知识管理与传承</p><p>建立内部模板库和组件库</p><p>定期分享最佳实践</p><p>形成持续学习的文化</p><p>3、避免常见陷阱</p><p>根据行业实践，工程企业应用低代码需特别注意：</p><p>避免过度定制：不是所有需求都适合低代码实现，核心复杂系统仍需专业开发</p><p>防止数据孤岛：确保低代码应用与核心系统数据互通</p><p>管理期望落差：明确低代码的优势和局限，设定合理预期</p><p>关注技术债务：即使可视化开发，也需要良好的设计和文档</p><p><strong>六、低代码的未来趋势与展望</strong></p><p>1、技术趋势</p><p>低代码平台的未来演进将呈现以下趋势：</p><p>AI辅助开发：</p><p>自然语言描述自动生成应用</p><p>智能推荐业务流程和界面设计</p><p>自动优化应用性能</p><p>行业垂直化：</p><p>面向工程行业的专用组件和模板</p><p>集成行业标准（如BIM标准、工程计量规范）</p><p>预置行业最佳实践流程</p><p>混合开发模式：</p><p>低代码与专业代码的无缝协作</p><p>微服务架构下的低代码模块</p><p>边缘计算与低代码结合</p><p>2、低代码在工程数字生态中的定位</p><p>未来工程行业的数字生态将是多层结构：</p><p>基础平台层：ERP、BIM、项目管理核心系统</p><p>低代码应用层：业务部门自主构建的敏捷应用</p><p>数据智能层：基于大数据的分析和决策支持</p><p>现场物联层：传感器、无人机、智能设备的实时数据</p><p>低代码将成为连接各层的粘合剂，实现快速创新和灵活适应。</p><p>3、给工程企业的行动建议</p><p>基于当前技术成熟度和行业实践，建议工程企业：</p><p>立即行动：</p><p>组织低代码技术研讨会，提升管理层认知</p><p>选择1-2个试点场景，启动小范围尝试</p><p>培养首批公民开发者，建立内部能力</p><p>中期规划：</p><p>制定企业低代码战略和治理框架</p><p>建立低代码卓越中心（CoE）</p><p>将低代码纳入数字化转型路线图</p><p>长期愿景：</p><p>形成业务与技术融合的创新文化</p><p>构建基于低代码的数字化敏捷能力</p><p>探索低代码与新兴技术（AI、IoT、数字孪生）的融合应用</p><p><strong>七、结语</strong></p><p>低代码不是要替代专业开发人员，也不是要解决所有IT问题。它的核心价值在于：在专业开发与业务需求之间建立新的平衡点，让工程企业能够以更低的成本、更快的速度响应业务变化。</p><p>对于长期面临IT资源不足、业务需求多变困境的工程企业，低代码提供了一个务实而有效的选择。它降低了数字化的门槛，让更多企业能够享受技术带来的效率提升和业务创新。</p><p>工程行业的数字化不是一蹴而就的革命，而是渐进式的演进。低代码技术正是这一演进过程中的重要助推器——它不是万能的魔法棒，但确实是一把打开数字化大门的钥匙，让更多工程人能够参与其中，共同塑造行业的数字未来。</p><p>正如一位工程项目经理在使用低代码平台后所说：“我们终于不再是被动等待IT部门排期的业务部门，而是能够主动解决自己问题的建设者。”这种从使用者到创造者的角色转变，或许才是低代码带给工程行业最深刻的价值。</p>]]></description></item><item>    <title><![CDATA[中国新冠疫情的探索性空间数据分析 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494094</link>    <guid>https://segmentfault.com/a/1190000047494094</guid>    <pubDate>2025-12-22 18:07:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>这个东西主要是2020年上半年，疫情在家里上网课，某一门课程的期末大作业。作业参考了一篇分析SARS的论文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>，将其中的部分方法应用于新冠疫情的数据，主要是计算莫兰指数。我的方向跟这个没啥关系，也不太清楚这些指标的具体含义，仅仅是将他们计算了出来，并稍作解释。下面就直接贴代码和参考文献了。</p><p>先贴一下参考文献部分，我也不太了解莫兰指数的具体细节，就不多叙述了，自己都是看这些博客尝试理解的</p><ul><li><a href="https://link.segmentfault.com/?enc=ZsZ0y37I6iCiNA360oN61w%3D%3D.DBE0jGYaKeeZ%2FnV1ZwcikVXp0zq2lyt7Y6GH1T%2BURXONPtdWNW8ir%2FEdBvNhYKhgFJniXbDR2V9nxqcWVoIalg%3D%3D" rel="nofollow" target="_blank">python计算莫兰指数(Moran's I)并绘制地区热力图——以中国各省pm2.5为例</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></li><li><a href="https://link.segmentfault.com/?enc=LeVeRNd63QlmxhDww9jiJw%3D%3D.FAGxYlHkTTT3OLTkUt8rRjteF%2Bwn7twv4moWPv1dRwCZHKkr6q%2F4qOTMbslSJjCd9qUiJekbMet5TmwxbbwFKQ%3D%3D" rel="nofollow" target="_blank">空间统计：Moran's I（莫兰指数）</a><sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup></li><li><a href="https://link.segmentfault.com/?enc=cgE4Rx2NNv0SNg8oWb2ERg%3D%3D.8XcToCkQiq4YD9D8zT1F%2BEOlTivjWgJlklpWOFNFNhsAefygJDPGJp0bfwa4ZEmxR8j2uNBM8NwG%2F5heUD8flA%3D%3D" rel="nofollow" target="_blank">莫兰指数（Moran's I）的小总结</a><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup></li><li><a href="https://link.segmentfault.com/?enc=ZMN9WjsERqh1R2pI2Bl4zQ%3D%3D.pUsSkJU6RddjgIjh0ofD5370keCEed%2BcDiQpwqM3xoEfD4Y6B4RK0H6wePy1ACcteKY51%2FhSajstm%2F4wapZLvw%3D%3D" rel="nofollow" target="_blank">白话空间统计之四：P值和Z得分（中）</a><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></li></ul><p>下面是数据处理和分析的主要过程，数据就不贴了，大家参考流程就好</p><pre><code class="python">#%%

import pandas as pd

#%%


# 得到累计确诊数据

globalData = pd.read_csv('time_series_covid19_confirmed_global.csv')

# 读入全球的时间序列累计数据

chinaData = globalData[ globalData['Country/Region'] == 'China']

# 选出中国的省份和地区，33个

taiwanData = globalData[ globalData['Country/Region'] == 'Taiwan*']

# 再选出台湾地区

taiwanData['Country/Region'] = 'China'
taiwanData['Province/State'] = 'Taiwan'

chinaData = pd.concat([chinaData, taiwanData])

# 合并成新的中国数据框

chinaData = chinaData.reset_index(drop=True)

# 对数据框的索引进行重新编排

chinaData.to_csv('ChinaData.csv',index=0)

#%%

import matplotlib.pyplot as plt

# 累计确诊画折线图

chinaData = pd.read_csv('D:\WHO\ChinaData.csv',low_memory=False)

datelist = list(chinaData.columns[4:119])

# 抽取1/22/20 - 5/15/20的疫情数据

for i in range(0,34):
    x = datelist
    y = list(chinaData.loc[i,datelist])
    plt.plot(x,y)

plt.axvspan('1/22/20', '3/8/20',facecolor='#2E8B57',alpha=0.1)
plt.axvspan('3/8/20', '5/15/20',facecolor='#FFFF00',alpha=0.1)
plt.axvline(x='3/8/20')
plt.xticks(x, x, rotation=90)
plt.tick_params(labelsize=10)
plt.legend(range(0,34))
plt.show()

# 绘制折线图

#%%

# 今日确诊减去昨日确诊得到每日新增

increaseData = chinaData[chinaData.columns[0:4]]

datelist = list(chinaData.columns[4:119])

# 取出列名，即日期，形成列表

# 每一列减去前一列，即为该日新增

for i in range(1,115) :

    increasenew = chinaData[datelist[i]]-chinaData[datelist[i-1]]

    increasenew = pd.DataFrame(increasenew,columns=[datelist[i]])

    increaseData = pd.concat([increaseData, increasenew],axis=1)

increaseData.to_csv('increaseData.csv',index=0)

#%%

# 给每日新增画曲线图

increaseData = pd.read_csv('increaseData.csv',low_memory=False)

datelist = list(increaseData.columns[4:118])

# 取出新增数据

for i in range(0,34):
    x = datelist
    y = list(increaseData.loc[i,datelist])
    plt.plot(x,y)

plt.axvspan('1/23/20','3/8/20',facecolor='#2E8B57',alpha=0.1)
plt.axvspan('3/8/20','5/15/20',facecolor='#FFFF00',alpha=0.1)
plt.axvline(x='3/8/20')
plt.xticks(x, x, rotation=90)
plt.tick_params(labelsize=10)
plt.legend(range(0,34))
plt.show()

#%%

# 按阶段汇总确诊人数

chinaData = pd.read_csv('ChinaData.csv',low_memory=False)

statData = chinaData[chinaData.columns[0:4]]

statData['stage1'] = chinaData['3/8/20']

statData['stage2'] = chinaData['5/15/20'] - chinaData['3/8/20']

statData['total'] = chinaData['5/15/20']

#%%
# 拿到死亡人数

globalData = pd.read_csv('05-15-2020.csv')

chinaData = globalData[ globalData['Country_Region'] == 'China']

taiwanData = globalData[ globalData['Country_Region'] == 'Taiwan*']

taiwanData['Country/Region'] = 'China'
taiwanData['Province/State'] = 'Taiwan'

chinaData = pd.concat([chinaData, taiwanData])

chinaData = chinaData.reset_index(drop=True)

statData['death'] = chinaData['Deaths']

statData.to_csv('statData.csv',index=0)

#%%

# 计算空间权重矩阵

import pandas as pd

import numpy as np

from geopy.distance import geodesic

statData = pd.read_csv('D://WHO//statData.csv')

def DISTMatrix(SW,Dist):
    for i in range(0,34):
    #循环34个省份
    
        for t in range(i+1,34):
        # 循环其他省份
        
            cityDistance = geodesic(
                (statData.loc[i,'Lat'],statData.loc[i,'Long']),
                (statData.loc[t,'Lat'],statData.loc[t,'Long'])
                ).km
            # 计算两个地方的直线距离
            # 输出结果的单位为Km
            
            if(cityDistance&lt;=Dist):
                SW[i,t] = 1.0
                SW[t,i] = 1.0
            # 判断，若小于条件
            # 则标记为1，视为接壤
            # 该省对该省自身视为不接壤
                
# 定义空间权重矩阵
# 输入参数Dist
# 两个地方的距离小于Dist即视为接壤
# 由参数带入的矩阵SW
# 即为Dist条件下的
# 空间矩阵结果


def MatrixSelect(dist):
    SWName = 'SW' + str(dist/1000)+'.csv'
    SW = np.mat(np.zeros((34,34)))
    DISTMatrix(SW,dist)
    SW = pd.DataFrame(SW)           
    SW.to_csv('D://WHO//'+SWName,index=0,header=None)
    
# 根据传入的参数dist
# 输出空间权重矩阵的csv文件

setDist = 900
while (setDist&lt;=2000):
    MatrixSelect(setDist)
    setDist = setDist + 100
    
# 计算900-2000为参数的各空间权重矩阵
    

#%%

import pandas as pd

import numpy as np

import Moran

statData = pd.read_csv('statData.csv')

stage1 = np.matrix(statData['stage1'])

stage2 = np.matrix(statData['stage2'])

total = np.matrix(statData['total'])

death = np.matrix(statData['death'])

DataList = [stage1,stage2,total,death]

# 读取四个阶段的数据，形成列表
# 作为指数计算的内层循环
# 即对外层每一个矩阵，计算一遍该列表

FileList = ['SWM.csv']
i = 0.9
while ( i &lt; 2.1 ):
    i = round(i,2)
    FileName = 'SW'+str(i)+'.csv'
    FileList.append(FileName)
    i = i + 0.1
    
# 将权重矩阵的文件名形成列表，作为指数计算的外层循环

#%%
 
MoranChart = pd.DataFrame(columns=range(0,12))
# 创建数据框存储莫兰值，Z值，和p值

for i in range(0,13) :
# 循环13个空间权重矩阵
    
    SW = pd.read_csv(FileList[i],header=None)
    # 读取文件
    
    SW = np.matrix(SW.values)
    # 转换成矩阵
    
    ##########################
    print()
    print(i)
    ##########################
    
    for t in range(0,4):
    # 循环四个数据组
        
        MoranChart.loc[i,(3*t):(3*t)+2]  = Moran.Index(DataList[t], SW)

MoranChart.to_csv('D://WHO//MoranChart.csv',
                  index=0,header=None)

#%%

# 选出各阶段p值最小的数据，绘制莫兰散点图

SW = pd.read_csv('SW1.5.csv',header=None)

SW = np.matrix(SW.values)

Moran.Plot(stage1,SW)

Moran.Plot(total,SW)

Moran.Plot(death,SW)

SW = pd.read_csv('SW0.9.csv',header=None)

SW = np.matrix(SW.values)

Moran.Plot(stage2,SW)
</code></pre><p>下面是计算莫兰指数等用到的模块，我也是从别的地方找的，然后抽取出来这些部分</p><pre><code class="python">
import numpy as np

import matplotlib.pyplot as plt

from scipy.stats import norm

def Index(X,W):
    
    #x为行矩阵，W为权重矩阵（0-1矩阵，且对称）
    #X = np.matrix([8,6,6,3,2])
    #W = np.matrix([[0,1,1,0,0],[1,0,1,1,0],[1,1,0,1,0],[0,1,1,0,1],[0,0,0,1,0]])
    
    #预处理
    #X = np.matrix.transpose(X)
    X = X.T
    
    #对输入的行矩阵进行转置
    X = X/1.0
    
    #保证X中的数据为浮点型
    W = W/1.0
    #保证W中的数据为浮点型
    
    # # 求向量X的长度
    n = len(X)
    
    # # 求矩阵W每行之和
    # W_sum = np.sum(W,axis = 1)#axis=1表示对矩阵的行求和
    # W_standard = W/(W_sum*np.ones(n))#对W进行标准化
    
    # 对矩阵进行标准化
    Wsum = W.sum(axis=1)
    # 矩阵按行求和
    Locate0 = np.where(Wsum == 0)[0]
    # 取出和为0的行号
    Wsum[Locate0] = 1.0
    # 将这些行的和赋值为1.0
    # 否则0会出现在分母上
    W_standard = W/Wsum
    # 进行标准化
    
    #对x进行标准化
    xm = np.mean(X)
    sx = np.std(X)
    z = (X-xm)/sx
    
    #求标准化后的纵坐标，这里的x和W都进行了标准化
    Wz = W_standard*z
    #z = np.matrix.transpose(z)
    #A = np.vstack([z, np.ones(n)]).T
    
    #求线性回归系数，得出的直线斜率就是moran'I值
    z_lstsq = np.linalg.lstsq(z,Wz,rcond=-1)
    
    #moran'I值检验
    WW = W_standard
    E_I = -1.0/(n-1)
    S0 = np.sum(WW)
    WW1 =np.multiply(WW+WW.T,WW+WW.T)
    S1 = np.sum(WW1)/2.0
    WW2 = np.sum((WW+WW.T),axis = 1)
    S2 = np.sum(np.multiply(WW2,WW2))
    Var_I = (n*n*S1-n*S2+3*S0*S0)/((n*n-1)*S0*S0)-E_I*E_I
    Z_I = (float(z_lstsq[0])-E_I)/np.sqrt(Var_I)
    
    
    # 计算该z值对应的p值
    Z_p = norm.cdf(Z_I)
    
    
    print('moran`I值为:',float(z_lstsq[0]))
    print('Z值为:',Z_I)
    print('p值为',Z_p)
    
    return float(z_lstsq[0]),Z_I,Z_p
    # 依次返回 莫兰值，Z检验值，P概率值

def Plot(X,W):
    
    #x为行矩阵，W为权重矩阵（0-1矩阵，且对称）
    #X = np.matrix([8,6,6,3,2])
    #W = np.matrix([[0,1,1,0,0],[1,0,1,1,0],[1,1,0,1,0],[0,1,1,0,1],[0,0,0,1,0]])
    
    #预处理
    #X = np.matrix.transpose(X)
    X = X.T
    
    #对输入的行矩阵进行转置
    X = X/1.0
    
    #保证X中的数据为浮点型
    W = W/1.0
    #保证W中的数据为浮点型
    
    # # 求矩阵W每行之和
    # W_sum = np.sum(W,axis = 1)#axis=1表示对矩阵的行求和
    # W_standard = W/(W_sum*np.ones(n))#对W进行标准化
    
    # 对矩阵进行标准化
    Wsum = W.sum(axis=1)
    # 对矩阵按行求和
    Locate0 = np.where(Wsum == 0)[0]
    # 找出和为0的行
    Wsum[Locate0] = 1.0
    # 讲该行的和赋值为1.0
    # 否则即0出现在分母位置
    W_standard = W/Wsum
    # 对矩阵标准化
    
    #对x进行标准化
    xm = np.mean(X)
    sx = np.std(X)
    z = (X-xm)/sx
    
    #求标准化后的纵坐标，这里的x和W都进行了标准化
    Wz = W_standard*z
    #z = np.matrix.transpose(z)
    #A = np.vstack([z, np.ones(n)]).T
    
    #求线性回归系数，得出的直线斜率就是moran'I值
    z_lstsq = np.linalg.lstsq(z,Wz,rcond=-1)

    #求拟合值,拟合的直线一定过原点
    z_z = z*z_lstsq[0]
    
    #画moran'I散点图
    plt.tick_params(labelsize=30)
    plt.plot(z,Wz,'ro')
    plt.plot(z,z_z)
    plt.grid()
    plt.xlabel('x axis',fontsize=30)
    plt.ylabel('Wx axis',fontsize=30)
    plt.title('Moran`I Scatter Plot',fontsize=30)
    plt.show()
</code></pre><div class="footnotes"><hr/><ol><li id="fn-1"> [范新生;应龙根. 中国SARS疫情的探索性空间数据分析[J]. 地球科学进展, 2005, 20(3): 282-291.](<a href="https://link.segmentfault.com/?enc=ERXNAhU%2BhPEgwkJCWpPeNA%3D%3D.X0MQAfH5f5hkKw3YBQhA5n6rngE5otPs7zxGlrfnxaZDVuUMcwHwea%2FH4ANtuI%2BS" rel="nofollow" target="_blank">http://www.adearth.ac.cn/CN/Y2005/V20/I3/282</a>) <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=wUfqo%2FLmZr1giHhiCpBu%2BQ%3D%3D.emxIblmEmJ3rlhuj9KLZUpfZfTf9pa%2BihoG4LAoafrS8%2Fd5nPN%2FYJxObb%2F5lts0EWKmMPVgpBBBslP7h%2FfTygA%3D%3D" rel="nofollow" target="_blank">python计算莫兰指数(Moran's I)并绘制地区热力图——以中国各省pm2.5为例</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=tUpbjVlgxnrf%2F3CWfzzS8g%3D%3D.x5gC%2B4VhmbF6xl5PIsHevrCXdCSM1%2Fm%2Fg527tKUy4r6lc3spUwFD8NMLEpKmVZFki2qtcnilVNXxnUX0XCo2Bw%3D%3D" rel="nofollow" target="_blank">空间统计：Moran's I（莫兰指数）</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=SeXI66WdCKUv1Mc4VT3pEw%3D%3D.SNpM%2F5S0ScPhHLYNy6m8eIBo3Su4K7akbXWCimZjBFKwgjkmrbow0BAPN2IbXiyweHCPuanaqXccZWmeiEmA4g%3D%3D" rel="nofollow" target="_blank">莫兰指数（Moran's I）的小总结</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=ddtRBGHnw9QqmjV%2Ft4pUgg%3D%3D.lqxaJv3nX%2FxWhVjJFx0Z3EcUuyTlyp0jm2S9ASDdKRo2wd6cZ%2BQZSUVrRvTl0FaGB7ZqGZCh0%2BXwFc3ATBIaBQ%3D%3D" rel="nofollow" target="_blank">白话空间统计之四：P值和Z得分（中）</a> <a href="#fnref-5" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[腾讯镜像解决gradle报错：ERROR: Could not install Gradle dis]]></title>    <link>https://segmentfault.com/a/1190000047494108</link>    <guid>https://segmentfault.com/a/1190000047494108</guid>    <pubDate>2025-12-22 18:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>配置Android Studio的时候经常报错gradle，解决某一次报错的时候发现了这个东西<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>。</p><p>按照路径</p><blockquote>D:\Project\Studule\gradle\wrapper\gradle-wrapper.properties</blockquote><p>打开文件，将</p><blockquote>distributionUrl=https\://services.gradle.org/distributions/gradle-6.5-bin.zip</blockquote><p>替换成</p><blockquote>distributionUrl=https\://mirrors.cloud.tencent.com/gradle/gradle-6.5-bin.zip</blockquote><p>此外我还参考了其他博客</p><ul><li><a href="https://link.segmentfault.com/?enc=mdJ3EhzmMlJ2Ng2ZpKzTeQ%3D%3D.zRp48Jn0N45PH5atFE6zE1qnR7uGux6yDAClc%2FUHzARKOU3wGbj7iAAdOJRU8QLX" rel="nofollow" target="_blank">ERROR: Could not install Gradle distribution from 'https://services.gradle.org/distributions/grad...</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></li></ul><p>之前有过一个想法，给某个课表APP做一个桌面插件，但一直没时间，先把一些参考文献列在下面。挖个坑，万一哪天填了，就独立出来写一篇博文</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=P2j8yHqezMn4dpuZWm6LSQ%3D%3D.gPwTSbSKifoM2XOrTYERp3EYh9dYx%2FbrhR5iZMSgS60T%2BKsewJVX1yRq6%2FURiFve%2BMoqWrRlkYSke95Iwx8M9w%3D%3D" rel="nofollow" target="_blank">在widget实现复杂布局（Listview，GirdView）以及RemoteViewsService、RemoteViewsFactory的用法</a></li><li><a href="https://link.segmentfault.com/?enc=7Sc35fLWx4MMaY06mjpIuw%3D%3D.lnGftz7e%2BVE98ClHDDDGkKeamlYBavSFK4%2FYxuD40CFLbbXzfib1344UB8xQzTtGUw85sAMeYuy%2FsMHElVeGdQ%3D%3D" rel="nofollow" target="_blank">App Widgets 详解四 RemoteViews、RemoteViewsService和RemoteViewsFactory</a></li><li><a href="https://link.segmentfault.com/?enc=KDRyL185KLdLh9iQj%2FKfwA%3D%3D.Wb7C%2Fk7RzqVQ5l7vm99DfbKlb0yEI21%2BQBVH7HX1xvxWzl4jX%2B2JZOvvIrN4EEWp" rel="nofollow" target="_blank">Android桌面小部件AppWidget开发</a></li><li><a href="https://link.segmentfault.com/?enc=uiC4%2Fv7xbi76xaOd5euChA%3D%3D.qkIzpxoWcPYmRT1hBd4My3yrwMDqFYgjpkWIQvIulcE7smlcm%2FhdI%2FuJnyoBaakEbUZhZPqN3yl2I8Xs%2BwY0Lw%3D%3D" rel="nofollow" target="_blank">开发安卓桌面widget</a><br/><a href="" target="_blank"/></li></ul></blockquote><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=xsJrQ5u8ZLvwmml%2B6Uw5Tg%3D%3D.YQ46f7IDJsyyquPlNvgcPrpBlT5Wxzc6X1PWeF3q4NQsrGeFHHwgv8ikVHAW6lOg" rel="nofollow" target="_blank">Index of /gradle/</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=0Q%2FVWLYLS2JaetRtx07YLw%3D%3D.X7g1%2FdfESxzhBLt4lyj1NoBBEn9%2BY9rJ2vrGOeUqP%2F75Zsojy2eQox1d1ouzIFzM" rel="nofollow" target="_blank">ERROR: Could not install Gradle distribution from 'https://services.gradle.org/distributions/grad...</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]狗熊会人才计划第6期选拔作业 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494127</link>    <guid>https://segmentfault.com/a/1190000047494127</guid>    <pubDate>2025-12-22 18:05:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>数据就不贴了，给个描述吧</p><pre><code class="python"># &lt;class 'pandas.core.frame.DataFrame'&gt;
# RangeIndex: 4492 entries, 0 to 4491
# Data columns (total 7 columns):
#  #   Column    Non-Null Count  Dtype
# ---  ------    --------------  -----
#  0   标题        4417 non-null   object
#  1   标题链接      4492 non-null   object
#  2   brief     4483 non-null   object
#  3   keywords  3272 non-null   object
#  4   发布时间      3817 non-null   object
#  5   新闻类别      4492 non-null   object
#  6   采集时的时间    4492 non-null   object
# dtypes: object(7)
# memory usage: 245.8+ KB</code></pre><p>大概就是这7列，还需要预处理一下</p><p>先对发布时间处理，好奇这是什么时段的新闻</p><pre><code class="python">dataI0['发布时间'].sort_values()
# Out[7]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 4356           NaN
# 4357           NaN
# 4358           NaN
# 4359           NaN
# 4360           NaN
# Name: 发布时间, Length: 4492, dtype: object</code></pre><p>啊这...有缺失值来着，忘记处理了</p><pre><code class="python">dataI0['发布时间'].dropna().sort_values()
# Out[8]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 2374       21-4-23
# 2373       21-4-23
# 2371       21-4-23
# 2369       21-4-23
# 2372       21-4-23
# Name: 发布时间, Length: 3817, dtype: object</code></pre><p>噢，数据格式还不统一。由于上面是排序后的结果，所以我们可以直接找到所有不合格式的数据，它们排序后都在一块儿</p><pre><code class="python">dataI0.loc[2369:2393,'发布时间']
# Out[10]:
# 2369    21-4-23
# 2370    21-4-23
# 2371    21-4-23
# 2372    21-4-23
# 2373    21-4-23
# 2374    21-4-23
# 2375    21-4-23
# 2376    21-4-23
# 2377    21-4-22
# 2378    21-4-22
# 2379    21-4-22
# 2380    21-4-22
# 2381    21-4-22
# 2382    21-4-22
# 2383    21-4-22
# 2384    21-4-22
# 2385    21-4-22
# 2386    21-4-22
# 2387    21-4-22
# 2388    21-4-22
# 2389    21-4-21
# 2390    21-4-21
# 2391    21-4-21
# 2392    21-4-21
# 2393    21-4-21
# Name: 发布时间, dtype: object</code></pre><p>噢，这个需要找找资料<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><pre><code class="python">tmpDF = dataI0.loc[2369:2393,'发布时间']
# 取出时间格式不同的这几个样本

tmpDF = pd.to_datetime(tmpDF,format="%y-%m-%d")
# 按照'两位数年份-月份-天数'的格式进行转换读入时间类型

dataI0.loc[2369:2393,'发布时间'] = tmpDF.dt.strftime("%Y-%m-%d")
# 按照'四位数年份-月份-天数'的格式从时间类型当中输出

dataI0['发布时间'].dropna().sort_values()
# 去除缺失值之后对数据进行排序
# 可以看到新闻的发布时间为2020-10-08到2021-04-24

# Out[47]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 1934    2021-04-24
# 1935    2021-04-24
# 1936    2021-04-24
# 828     2021-04-24
# 0       2021-04-24
# Name: 发布时间, Length: 3817, dtype: object</code></pre><p>这里是直接使用了pandas自带的模块处理，也还有其他方法，可以参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>做完这个才想起来好像有点不对...考虑了缺失，没考虑重复，赶紧给去个重，按着新闻的链接作为唯一标识。</p><pre><code class="python">dataI0 = dataI0.drop_duplicates(['标题链接'])
# 网址链接相当于数据的id
# 根据id对网址进行去重</code></pre><p>接下来，嗯，考虑补全数据。我觉得这个思路就很混乱，处理缺失、去除重复，应该是一开始就要做的事情吧...上面是在干啥，咋突然处理了一波时间格式。</p><pre><code class="python">dataM0 = dataI0.loc[dataI0['标题'].isnull(),['标题链接','标题']].reset_index(drop=True)
# 先抽取出标题缺失的数据，拿到新闻的网址链接
import requests
from lxml import etree

for i in range(0,dataM0.shape[0]):
    print(i,'-'*10)
    # 打印序号
    htmlT = requests.get(dataM0.iloc[i,0])
    # 打开链接
    htmlT.encoding = 'utf-8'
    # 使用utf-8编码
    htmlT = etree.HTML(htmlT.text)
    # 取出其中的文本
    dataM0.iloc[i,1] = htmlT.xpath('/html/body/div[12]/div[1]/div[1]/h1/text()')[0]
    # 根据xpath取出新闻标题，需要精确到text()
    print(dataM0.iloc[i,1],'\n')
    # 打印新闻标题

dataI0.loc[dataI0['标题'].isnull(),'标题'] = dataM0['标题'].values
# 直接使用数据框赋值不可取
# 应当将取values后再赋值
# 因为数据框之间的赋值好像需要看index</code></pre><p>这里需要说一下，我之前习惯的都是selenium的爬虫，但是这次不顶用了，所以换了个方式。但是etree搞下来的数据，它的xpath竟然需要精确到<kbd>text()</kbd>，这是我没有想到的。按selenium的经验，我只把路径写到了<kbd>h1</kbd>，然后怎么搞都拿不到文本，后来查了博文才知道<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>。这个事情说来神奇，当时查的其他博文<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，都写着可以到<kbd>h1</kbd>，然后用<kbd>i.text</kbd>拿到文本，但我这里就是不行。另外，这里还需要注意编码的问题，我调成<kbd>htmlT.encoding = 'utf-8'</kbd>才正常，主要参考了博文<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>。</p><p>至于selenium为啥不行呢，我感觉根本原因是我太菜了不会用，直接原因如下：</p><pre><code class="python"># from selenium import webdriver
#
# driver_path = "D:\Software\Anaconda3\msedgedriver.exe"
# # msedgedriver.exe 的路径
#
# browser = webdriver.Edge(executable_path=driver_path)
# # 打开浏览器
#
# browser.get(dataM0.iloc[0,0])
# # 打开网页
#
# rowCount = browser.find_element_by_xpath("/html/body/div[12]/div[1]/div[1]/h1")
# # 选择表格
#
# # /html/body/div[13]/div[1]/div[1]/h1
# # /html/body/div[12]/div[1]/div[1]/h1
# # 在第一次爬取的时候div是12，第二次打开网页就是13
# # 可以每次循环都新打开浏览器，每次循环关闭浏览器
# # 这样似乎有点浪费资源，放弃selenium</code></pre><p>接着处理一下keywords。访问了一下原网页发现，原网页当中没有keywords，看来需要自己动手。考虑用TF-IDF，使用jieba库<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>。这里有一个神奇的点，本来是打算从brief当中抽取关键词，毕竟文本信息更多。但brief里面竟然不仅仅有这条新闻的消息，还有其他的诸如推荐的新闻等的文本...于是放弃brief，改用标题。</p><pre><code class="python">dataM0 = dataI0.loc[dataI0['keywords'].isnull(),['标题','keywords']].reset_index(drop=True)
# 抽取keyword缺失的行

from jieba.analyse import extract_tags
# from jieba.analyse import textrank

# for keyword, weight in extract_tags(dataM0.iloc[0,0], withWeight=True):
#     print('%s %s' % (keyword, weight))
# for keyword, weight in textrank(dataM0.iloc[0,0], withWeight=True):
#     print('%s %s' % (keyword, weight))

for i in range(0,dataM0.shape[0]):
    print(i, '-' * 10)
    # 打印序号
    keyW = extract_tags(dataM0.iloc[i,0])
    dataM0.iloc[i, 1] = ','.join(keyW[0:3])
    # 对关键词进行拼接
    print(dataM0.iloc[i, 1])


dataI0.loc[dataI0['keywords'].isnull(),'keywords'] = dataM0['keywords'].values
# 直接使用数据框赋值不可取
# 应当将取values后再赋值
# 因为数据框之间的赋值好像需要看index</code></pre><p>然后发布时间的话，可以直接从网址里面提取，用<kbd>split</kbd>进行文本处理就好</p><p>brief缺失的话，直接丢掉那些行吧，我印象缺失的行不到1%</p><p>下面开始进行可视化分析吧，好像也没其他预处理了</p><p>首先想做一个按天为单位的新闻发布量时序图，然后就出问题了。天数太多，坐标轴密密麻麻地重叠，这不行啊。查了查资料<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，应当将坐标轴刻度间隔调大。</p><pre><code class="python">import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator

countN = dataI0['发布时间'].value_counts()
countN.sort_index(inplace=True)
sns.lineplot(countN.index,countN.values)
x_major_locator=MultipleLocator(100)
#把x轴的刻度间隔设置为100，并存在变量里
ax=plt.gca()
#ax为两条坐标轴的实例
ax.xaxis.set_major_locator(x_major_locator)
#把x轴的主刻度设置为10的倍数
plt.show()
# 按天统计的发布趋势图</code></pre><p>其他的图，大概如下：</p><ul><li>新闻类别统计条形图</li><li>各类新闻在月的粒度上发布量时序图</li><li>发布量最大那天的新闻类别条形图</li><li>发布量最大那天的新闻类别饼图</li><li>根据keyword绘制词云</li></ul><p>贴一下部分代码吧</p><pre><code class="python"># 21年4月20日的新闻类别饼图
plt.pie(countN.values,startangle = 90,counterclock = False)
plt.legend(countN.index, loc=0)
plt.show()

# 按照关键词绘制词云
from wordcloud import WordCloud
#用来正常显示中文
plt.rcParams["font.sans-serif"]=["SimHei"]

wordK = dataI0['keywords'].values
wordK = ','.join(wordK)
wordK = wordK.replace(',',' ')

from PIL import Image
import numpy as np

mask_pic = Image.open("pic.jpg")
mask_pic_array = np.array(mask_pic)

wc=WordCloud(
    font_path='C:\\Windows\\Font\\simkai.ttf',
    background_color="white",
    mask = mask_pic_array)
wc.generate(wordK)
plt.imshow(wc,interpolation="bilinear")
plt.axis("off")
plt.show()
# 可惜并没有按照图片显示出来</code></pre><p>噢，对了，这个地方还需要调一下设置，要不然中文会乱码</p><pre><code class="python">plt.rcParams['font.sans-serif'] = ['SimHei'] #用来显示中文标签
plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号</code></pre><p>以上，大概就是我提交的报名作品，然后就没过...嗯，只能去玩儿别的了</p><p>在以上的基础上，还想过做一个情感分析，等于说每一条新闻增加一列数据，然后再进行一些分析。或者是做<kbd>embedding</kbd>去训练一个新闻分类的模型，但意义不大，数据量太小。</p><p>这份报告大概是从当天中文的<kbd>12：53</kbd>开始做，然后在截止时间<kbd>20：00</kbd>的<kbd>19：40</kbd>完成，然后检查检查改了几个小错误，还是准备的不够啊。据说这个报名从<kbd>06月</kbd>初就开始了，到<kbd>07月10日</kbd>左右结束...而我是当天才开始做的。</p><p>然后关于狗熊会的培训和选拔也有点迷惑。怎么说呢，就是，人才计划貌似是培养数据分析能力的，是先想选题再去找数据？大概吧，我感觉选题在前。然后此次选拔呢，说的是，“根据附件提供的数据（news.csv），自行确定选题，完成一份数据分析报告。”，就有种数据在选题之前的感觉。当然事实上，数据肯定在选题之前，我们必然是要根据数据做选题的。但是，真实情况下的数据远比选拔附件提供的数据丰富详实，从而能做更多的分析。或许考核的也有其他的一些考虑，例如对数据集的补充能力？先确定选题，然后自己去爬新闻数据，大大扩充数据集，然后再做分析？可能是我的思路太过于局限所给的数据集了，当然也有时间不足的原因。</p><p>再贴一遍数据集的描述：</p><pre><code class="python"># &lt;class 'pandas.core.frame.DataFrame'&gt;
# RangeIndex: 4492 entries, 0 to 4491
# Data columns (total 7 columns):
#  #   Column    Non-Null Count  Dtype
# ---  ------    --------------  -----
#  0   标题        4417 non-null   object
#  1   标题链接      4492 non-null   object
#  2   brief     4483 non-null   object
#  3   keywords  3272 non-null   object
#  4   发布时间      3817 non-null   object
#  5   新闻类别      4492 non-null   object
#  6   采集时的时间    4492 non-null   object
# dtypes: object(7)
# memory usage: 245.8+ KB</code></pre><p>纵观全局，我好像只做了新闻类别、发布时间这两个特征的分析。拿标题抽取了keyword，然后做了词云，这俩特征就没用了。标题链接？作为标识去了个重，然后拿它补充了标题，没了。采集时间？好像没啥思路。主要是感觉没意义吧，还要对采集人的行为做一做分析？或许选拔也有这方面的考虑吧，时间紧，没有过多思考。</p><p>此外，“人才计划的选拔，考察个人能力，而非团队合作能力。”，嗯，考察对指定数据集的分析能力？如果是这样就好了。我提交的报告属于探索性数据分析，开始做的时候是没有选题的，但是探索之后有结论，极其囿于数据集本身。说起来，这选拔好像考核的是传统的数据分析能力啊，要选题要汇报的那种，是带着目的去分析数据....而我提交的是探索性数据分析报告，凉凉。其实一开始也想到了这个问题，但是想了想，这能选啥题？带着啥目的去分析这个数据集？发布趋势？那不一张时序图就搞定了？类别分析？嗯...好像可以，哪个时段哪个类别发的最多，最多的类别又是怎样的发布趋势。但还是感觉不够，太少了，两张图就搞定。事实上，我在探索性数据分析的报告中，做的也就是发布趋势分析和类别分析。但是不是以明确选题的方式，而是混杂地统一扔进了探索性数据分析。或者应该叫数据挖掘，这或许就是区别了，某种程度上数据为先和选题为先的差别。</p><div class="footnotes"><hr/><ol><li id="fn-1">[[python+pandas]数据预处理-时间格式转换](<a href="https://link.segmentfault.com/?enc=44NKDSS33DznB68TSl2kKA%3D%3D.T8RbGfLQzHsznwp9n0kudm1Yj%2B803ngmFF%2Fqtt3VZ8kTEmIVN1%2FFLCnZjQGH%2B6H1b82i9gyZ%2F0NR94CEzRARPA%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/Apuls1/article/details/108784021</a>)  <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"><a href="https://link.segmentfault.com/?enc=zwKuaPCIPr51Zk5H1Avdtg%3D%3D.NMM7FJvfvRfrJ5%2BB1PUN3DNgcU8imaAFKlZkEfpHMwJFXry%2Fz6zUqhD4M%2By08Xq%2F6aE3RH%2FnwMcmS1DUbHlBGA%3D%3D" rel="nofollow" target="_blank">爬虫解析库之xpath、Beautifulsoup的对比使用</a>  <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"><a href="https://link.segmentfault.com/?enc=gCCQsw2C7yoP9ZbD6wu87w%3D%3D.qsQC%2FVqouhJEoWebuVwCwu%2BI4zCHSdVRk4%2FFgTUgviR%2FyQcgqaepzUI1jVEAVo6NqBD0TY8DhF0TQlOrlxg6Yg%3D%3D" rel="nofollow" target="_blank">Python爬虫之xpath的详细使用（爬虫）</a>  <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"><a href="https://link.segmentfault.com/?enc=qb8eDR6fJtAx7r9EOpK5Hg%3D%3D.EAGNCPIeg7mVk3ZQKf2NMs2%2F9kwa8WVpkm%2FhDzci1%2FBEE3QdRGd%2FK%2BEzUsrRNBYf" rel="nofollow" target="_blank">爬取知乎热榜标题和连接 （python，requests，xpath）</a>  <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"><a href="https://link.segmentfault.com/?enc=kvul8SMYr5laYJH5EVSOqw%3D%3D.881K2isOnwAtG3DSJMUq8x0H7xxR3WqPrICvc55971b8pN4G6xjXAM5LV1wJ%2BwQ4" rel="nofollow" target="_blank">Python request中文乱码问题解决方案</a>  <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"><a href="https://link.segmentfault.com/?enc=u6gleIP1Rwm635i2FWe4Bw%3D%3D.KTmaetnsws3sCo79kG3rL%2FOIH8WfE73vf3SEOJiHwyvBXtGNPoUe6ZaGnmy6t4YH" rel="nofollow" target="_blank">如何用Python提取中文关键词？</a>  <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"><a href="https://link.segmentfault.com/?enc=Zh%2BGBhLQ3MDduuqhwogScQ%3D%3D.dcEasM7HWbD%2BJ9T455LuIspBGLTzrZkfoL1kYVAc424Ng6C7dEtq%2BJo6zOe758hR" rel="nofollow" target="_blank">Python设置matplotlib.plot的坐标轴刻度间隔以及刻度范围</a>  <a href="#fnref-7" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[你敢在周五下午发布代码吗？只有写单元测试的人才配有的“松弛感” HuiZhu ]]></title>    <link>https://segmentfault.com/a/1190000047494131</link>    <guid>https://segmentfault.com/a/1190000047494131</guid>    <pubDate>2025-12-22 18:04:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>“改一行代码，崩整个系统。”</p><p>这句听起来像段子的玩笑，却是无数开发者心中真实的恐惧。</p><p>问你一个扎心的问题：<strong>如果现在让你重构核心业务里的那个 <code>calculatePrice</code> 函数，你敢立马点上线吗？</strong></p><p>大多数人的回答是沉默。因为我们心里没底。我们写的代码就像没有地基的房子，看着光鲜，实则摇摇欲坠。一旦需要修改，就像是在玩叠叠乐，生怕抽错一块木条，整个大厦瞬间坍塌。</p><p>这种恐惧的根源，就是<strong>缺乏单元测试</strong>。我们都知道测试重要，但我们更知道写测试有多痛苦：</p><ul><li><strong>枯燥</strong>：构造数据、Mock接口、写断言，比写业务逻辑还累。</li><li><strong>耗时</strong>：写功能1小时，写测试3小时，老板还催着上线。</li><li><strong>难维护</strong>：业务变了，测试红一片，还得回头修测试。</li></ul><p>于是，我们选择了“裸奔”。</p><p>但今天，我想给你一个拒绝“裸奔”的理由，以及一件<strong>防弹衣</strong>。有了它，你不再需要在那枯燥的断言中消耗生命。你只需要把代码丢给AI，它就能还你一套覆盖率100%的测试用例。</p><p>这不是偷懒，这是把你的脑力从“重复劳动”中解放出来，去思考更有价值的架构设计。</p><h2>为什么你需要这位“AI质检员”？</h2><p>在传统的开发流程中，单元测试往往是“二等公民”。但在AI时代，它应该是你的“贴身保镖”。</p><p>这套<strong>单元测试生成AI指令</strong>，不仅仅是帮你生成几行 <code>assert</code>。它是一位深谙 <strong>TDD（测试驱动开发）</strong> 和 <strong>代码质量</strong> 的老练工程师。</p><p>它能做到你懒得做的事：</p><ol><li><strong>穷举边界</strong>：你只想到了正常输入，它想到了空值、负数、超长字符串。</li><li><strong>隔离依赖</strong>：你嫌Mock麻烦，它自动帮你把数据库和API请求都Mock好。</li><li><strong>规范命名</strong>：你的测试叫 <code>test1</code>，它的测试叫 <code>test_invalid_email_returns_false</code>。</li></ol><h2>核心指令：让AI以此为生</h2><p>这套指令经过精细打磨，融合了业界标准的测试方法论。它不玩虚的，直接输出可运行、高质量的测试代码。</p><h3>🧬 单元测试生成AI提示词</h3><pre><code class="markdown"># 角色定义
你是一位资深的测试开发工程师，拥有10年以上的软件测试经验，精通各类单元测试框架（如JUnit、pytest、Jest、Mocha、NUnit等）和测试方法论（TDD、BDD）。你深谙代码质量保证的最佳实践，能够针对各种编程语言和业务场景，设计出高效、全面、可维护的单元测试用例。

# 任务描述
请为以下代码生成完整的单元测试用例，确保测试覆盖全面、结构清晰、易于维护，帮助开发者提高代码质量和系统可靠性。

**输入信息**:
- **待测代码**: [粘贴需要测试的代码]
- **编程语言**: [如: Python/Java/JavaScript/TypeScript/C#/Go等]
- **测试框架**: [如: pytest/JUnit/Jest/Mocha/NUnit等，可选，AI可根据语言推荐]
- **业务背景**: [简要说明代码的业务功能，可选]
- **特殊要求**: [如: 需要Mock外部依赖、性能测试、边界测试等，可选]

# 输出要求

## 1. 测试代码结构
- **测试文件头部**: 必要的导入语句和测试配置
- **测试类/模块组织**: 按被测功能合理分组
- **测试方法命名**: 采用清晰的命名规范（如: test_功能_场景_预期结果）
- **测试数据准备**: 合理的setUp/tearDown或fixture设计
- **断言语句**: 明确的预期结果验证

## 2. 测试覆盖维度
- **正常路径测试**: 验证预期输入的正确输出
- **边界条件测试**: 极值、空值、临界值测试
- **异常处理测试**: 错误输入、异常抛出验证
- **参数化测试**: 多组输入数据的批量验证（如适用）
- **Mock/Stub测试**: 外部依赖的隔离测试（如适用）

## 3. 质量标准
- **覆盖率**: 力争达到核心逻辑80%以上的分支覆盖
- **独立性**: 每个测试用例相互独立，无依赖顺序
- **可读性**: 测试意图清晰，便于理解和维护
- **可重复性**: 测试结果稳定，多次运行结果一致
- **执行效率**: 测试运行快速，避免不必要的等待

## 4. 格式要求
- 输出完整可运行的测试代码
- 每个测试方法添加简要注释说明测试目的
- 提供测试执行命令
- 如有Mock需求，提供Mock配置代码

## 5. 风格约束
- **代码风格**: 遵循对应语言的编码规范（如PEP8、Google Style等）
- **注释语言**: 中文注释说明测试意图
- **专业程度**: 适合中级开发者阅读和维护

# 质量检查清单

在完成输出后，请自我检查:
- [ ] 测试用例是否覆盖了所有公共方法
- [ ] 是否包含正常路径和异常路径测试
- [ ] 边界条件是否得到充分验证
- [ ] 测试命名是否清晰表达测试意图
- [ ] Mock/Stub使用是否合理
- [ ] 测试代码是否可以直接运行
- [ ] 是否提供了测试执行说明

# 注意事项
- 不要测试语言内置功能或第三方库的正确性
- 避免测试私有方法（除非有特殊需求）
- 测试数据应具有代表性，避免过于简单或过于复杂
- 对于有外部依赖的代码，优先使用Mock隔离
- 异步代码需要使用对应的异步测试方法

# 输出格式
请按以下顺序输出:
1. 📊 **测试策略概述**: 简要说明测试设计思路
2. 📝 **完整测试代码**: 可直接运行的测试文件
3. 🔧 **执行说明**: 测试运行命令和依赖安装
4. 📈 **覆盖率分析**: 测试覆盖的功能点清单
5. 💡 **优化建议**: 代码质量或可测试性改进建议（如有）</code></pre><h2>实战：从“不敢动”到“随便改”</h2><p>口说无凭，我们来看一个真实的<strong>Python</strong>案例。</p><p><strong>场景</strong>：你写了一个邮箱验证函数 <code>validate_email</code>，逻辑看起来很简单：要有 <code>@</code>，要有 <code>.</code>，不能为空。</p><p>但是，当你把这段代码交给AI，并使用上述指令时，它不仅测试了 <code>user@example.com</code>（正常路径），还狠狠地“刁难”了你的代码：</p><ul><li><strong>边界测试</strong>：<code>a@b.c</code>（最短有效邮箱）</li><li><strong>异常测试</strong>：传入 <code>None</code> 或 <code>123</code>（非字符串输入）</li><li><strong>特殊字符</strong>：<code>user.name+tag@example.com</code>（合法但少见）</li></ul><p>AI生成的测试代码会包含这样的<strong>参数化测试</strong>，把所有可能性一网打尽：</p><pre><code class="python">    @pytest.mark.parametrize("invalid_email", [
        "plainaddress",
        "@missingusername.com",
        "username@.com",
        "username@com",
        "username@-example.com",
    ])
    def test_various_invalid_emails(self, invalid_email):
        """参数化测试: 多种无效邮箱格式"""
        assert validate_email(invalid_email) is False</code></pre><p>这就是<strong>专业</strong>。它替你考虑了那些你可能要在半夜两点修Bug时才会想到的情况。</p><h2>3个让AI写好测试的“骚操作”</h2><ol><li><strong>Mock一切外部依赖</strong>：<br/>告诉AI：“这个函数调用了数据库，请用 <code>unittest.mock</code> 把 <code>db.query</code> 隔离掉。” 这样你的单元测试就不需要连真实的数据库，跑得飞快。</li><li><strong>针对遗留代码</strong>：<br/>对于那些没人敢动的“祖传代码”，你可以把代码贴给AI，然后说：“请生成一组<strong>特征测试（Characterization Test）</strong>，记录它现在的行为。” 这样你就得到了一张安全网，保证重构时不会破坏现有逻辑。</li><li><strong>测试驱动修复（TDD）</strong>：<br/>发现Bug了？先把复现Bug的条件告诉AI，让它生成一个<strong>会失败的测试用例</strong>。然后你再去修代码，直到测试变绿。这才是修复Bug的正确姿势。</li></ol><h2>写在最后</h2><p>单元测试，是工程师给自己的一份<strong>职业保险</strong>。</p><p>它让你在面对复杂的业务变更时，依然能保持从容；它让你在周五下午发布代码时，依然能期待一个愉快的周末。</p><p>不要让“没时间”成为借口。有了这套指令，你离“代码自信”只差一次复制粘贴。</p><p>现在，去给你的核心代码穿上这件“防弹衣”吧。<strong>愿你的控制台，永远是一片生机盎然的绿色。</strong></p>]]></description></item><item>    <title><![CDATA[喜报｜枫清科技AI4S智能体荣获InfoQ2025中国技术力量榜单「AI Agent最具生产力产品」]]></title>    <link>https://segmentfault.com/a/1190000047494136</link>    <guid>https://segmentfault.com/a/1190000047494136</guid>    <pubDate>2025-12-22 18:03:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494138" alt="图片" title="图片"/></p><p>12月19日，InfoQ 极客传媒携手模力工场评选的“2025中国技术力量榜单”（下称“技术榜单”）正式揭晓。枫清科技（Fabarta）“AI4S通用智能体和场景智能体” 凭借自主研发的AI智能体技术与科研场景的商业化落地能力，成功入选该榜单“AI Agent最具生产力产品”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494139" alt="图片" title="图片" loading="lazy"/></p><p>枫清科技依托在AI4S（AI For Science）科研平台建设与智能体技术研发方面的长期积累，构建了以“通用智能体 + 场景智能体”为核心的双轮驱动科研赋能体系，覆盖从文献理解、知识挖掘到实验设计与执行的科研全流程，可有效提升科研效率、降低试错成本，加速科研成果的产出与转化。</p><h3>通用智能体：面向科研共性需求的效率引擎</h3><p>AI4S通用智能体聚焦科研活动中的高频共性场景，覆盖文献处理、数据解析与知识挖掘等关键环节，系统性缓解科研人员在“信息过载”和“处理效率不足”方面的核心痛点，主要包括：</p><ul><li>文献智能处理智能体<br/>支持科研论文与文献的智能问答、多语言翻译、摘要生成等功能。该智能体基于深度科研语义理解能力，可精准解析PDF、Word 等多种格式文献，自动提取专业术语、核心观点及关键信息，显著提升文献研读与知识获取效率。</li><li>专利深度解析智能体<br/>依托专利检索与语义理解技术，自动提炼专利的技术问题、技术效果、创新点及权利要求内容，支持技术演进路径分析与自由问答。通过智能体能力实现自动化专利检索、上位词/下位词生成及专利精准度分析，并结合工作流引擎与本地知识库，实现从专利检索到内容总结的全链路自动化，大幅提升专利分析效率。</li><li>科研报告生成智能体<br/>内置多类型科研报告模板，可根据结构化或非结构化数据自动生成规范化科研报告；同时支持从用户已有报告中抽取模板结构，结合多源数据内容，实现科研报告的自动化生成与复用。</li></ul><h3>场景智能体：面向垂直领域的科研能力放大器</h3><p>AI4S场景智能体聚焦化工新材料、生物医药等专业领域，通过“行业知识体系 + 智能体技术”的深度融合，解决复杂实验设计与科研任务执行中的关键难题，典型应用场景包括：</p><ul><li>科研实验设计智能体<br/>面向不同科研实验场景，内置实验设计规则与领域知识体系，结合具体科研目标生成初步实验方案。该智能体通过交互式界面支持科研人员与智能体协同工作，对实验方案进行迭代优化与定制调整，提高实验设计的科学性与可执行性。</li><li>科研任务执行智能体<br/>针对科学实验中的复杂任务执行场景，支持实验建议生成、任务调度与长耗时任务管理（如分子动力学模拟等）。通过异步通信与状态持久化机制，该智能体突破传统智能体在长周期任务中的性能瓶颈，显著提升科研任务的执行效率与稳定性。</li></ul><p>枫清AI4S 智能体体系采用“智能体 + 工作流”协同架构，融合大模型的语义理解能力与多模态处理技术，支持跨学科、跨领域科研文献与数据的深度解析；在交互层面，该方案突破传统单一“问答”模式，集成知识图谱可视化与分析组件，更贴近真实科研工作流程，为科研人员提供高效、直观、可持续演进的智能化科研支撑。</p><p>2025中国技术力量榜单以“洞察AI变革，见证智能未来”为核心，围绕AI基础设施、工程与部署、智能体生产力、行业应用、数据智能、AI Coding、具身智能与开源等八大方向展开评选。本届评选历经两个多月的案例征集与多轮评审，获得来自200多家企业与团队的申报，覆盖云计算厂商、AI 初创公司、行业龙头企业、开源社区、科研及创新团队，共计300多个案例进入初审环节。</p><p>参评此次技术榜单的项目共同构成了一幅极具代表性的中国AI全栈创新实践图谱。该榜单不仅是对过去一年企业与团队技术实力的阶段性“体检报告”，也是对产业未来演进方向的一次集中洞察。</p><p>此次获奖，彰显了枫清科技在技术前瞻性、场景落地价值、生产力提升等方面的关键突破。枫清科技坚持以知识引擎与大模型双轮驱动，已在科研、化工、金融、制造、能源等多个领域落地企业场景的智能体解决方案。未来，枫清科技将深耕产业智能化升级，打造AI时代数智融合新引擎。</p>]]></description></item><item>    <title><![CDATA[powshell的tree命令 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494143</link>    <guid>https://segmentfault.com/a/1190000047494143</guid>    <pubDate>2025-12-22 18:03:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在给项目写README的时候需要列一下目录，主要参考某博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>这个命令可以在dos和powshell当中运行，效果如下:</p><pre><code>D:\Project\Studule\app\src\main\java&gt;tree /f
卷 Data 的文件夹 PATH 列表
卷序列号为 00690061 7876:8A9C
D:.
└─com
    └─stu
        └─studule
            ├─activity
            │      MainActivity.java
            │      OptionActivity.java
            │      SyllabusActivity.java
            │      UpdateCourseActivity.java
            │
            ├─adapter
            │      LessonAdapter.java
            │
            ├─dao
            │      CourseDao.java
            │
            ├─pojo
            │      Course.java
            │
            ├─remote
            │      MyRemoteAppWidget.java
            │      MyRemoteService.java
            │      MyRemoteViewsFactory.java
            │
            ├─util
            │      JsonDataUtil.java
            │
            └─view
                    RoundTextView.java
                    TimeTableView.java
                    
PS D:\Project\Studule\app\src\main\java&gt; tree /f /a
卷 Data 的文件夹 PATH 列表
卷序列号为 7876-8A9C
D:.
\---com
    \---stu
        \---studule
            +---activity
            |       MainActivity.java
            |       OptionActivity.java
            |       SyllabusActivity.java
            |       UpdateCourseActivity.java
            |
            +---adapter
            |       LessonAdapter.java
            |
            +---dao
            |       CourseDao.java
            |
            +---pojo
            |       Course.java
            |
            +---remote
            |       MyRemoteAppWidget.java
            |       MyRemoteService.java
            |       MyRemoteViewsFactory.java
            |
            +---util
            |       JsonDataUtil.java
            |
            \---view
                    RoundTextView.java
                    TimeTableView.java</code></pre><p>但是在dos窗口当中，<kbd>tree</kbd>、<kbd>tree /f</kbd>都会有一些字符重叠，需要加上<kbd>/a</kbd>才能清晰显示。不过不影响复制，粘贴到markdown当中还是正常的。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=Cns08Fz%2Fco56ARMzeyZ81Q%3D%3D.8S%2BiMUVYFr8cgIkLWpPOjk%2FTGHALqLa%2FprFUetopZchBFLJc421slinQI2zxkNPJEKMicYiMLrjNqpnIL6DX1WjkFqwEwP%2B1DpzQ95IXeow%3D" rel="nofollow" target="_blank">Powershell-查询当前文件目录层级结构</a> <a href="#fnref-1" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[中奖]第九届“泰迪杯”挑战赛A题 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494148</link>    <guid>https://segmentfault.com/a/1190000047494148</guid>    <pubDate>2025-12-22 18:02:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>问题概述</h4><p>题目<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>如下：</p><p><img width="617" height="628" referrerpolicy="no-referrer" src="/img/bVdnryN" alt="image.png" title="image.png"/></p><p>赛题有2个点，分别是：</p><ul><li><p>确定数据指标</p><p>即确定哪些特征是决定财务造假与否的关键特征</p></li><li><p>预测造假公司</p><p>训练模型，然后跑测试数据即可</p></li></ul><h4>预处理</h4><ul><li>首先使用missingno<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，对全局数据进行观测，看一看缺失值等情况</li><li>然后删去无用的特征列</li><li>删去缺失值占比过多的特征列</li><li>使用pd.interpolate()<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>对缺失值占比较小的特征列进行补充，也可以参考<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></li></ul><h4>第一题</h4><p>最开始的想法是直接跑树模型，然后看看谁的权重大就选谁，然而问题出在样本比例上。</p><ul><li>首先对整体来看，正样本的数量远远大于负样本。不均衡的情况下，树模型虽然有所缓解，但估计还是够呛</li><li>更惨的是，第一题要求的是各行业的财务造假关键指标。数据一共几十个行业，有些行业没有造假，全是正样本。这样的情况无法用树模型处理，其他模型也不行。</li></ul><p>后来查到了一个方法，Null Importances<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup><sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup><sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>。</p><p>其思路大概是，先用正确的标签计算一下各个特征对于分类的重要性，然后打乱标签，再计算特征的重要性。如果一个特征真的对分类有用，那么他应该在真实的标签下展示高重要性，而在错误的标签下展示低重要性。</p><p>对于第一题而言，我们分两类情况来考虑：</p><ul><li><p>对于整个行业没有造假记录的数据来说：</p><p>先对各个数值特征（好像所有的特征都是数值特征？）计算方差，取方差较小的特征为重要特征。因为该行业没有造假，所以其与造假相关的特征应当表现出聚集的趋势，即都没有造假，也就是方差较小的特征。然后随机赋予标签，计算其互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>（mutual_info_classif）。然后用前一个的特征集合减去后一个的特征集合，留下的即为关键特征。</p></li><li><p>对于整个行业有造假记录的数据来说</p><p>先对正确的标签计算互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>，然后随机赋予标签，再计算其互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>。取两者的差集为关键特征。</p></li></ul><h4>第二题</h4><p>这一题将数据分成了2个行业，制造业和非制造业。虽然样本还是很不均衡，但至少，正负样本都有。</p><ul><li>首先进行特征选择，使用LinearSVC<sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup></li><li>然后将数据丢进模型训练，并使用网格调参<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup></li><li>最后走一遍stacking<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup><sup id="fnref-13"><a href="#fn-13" class="footnote-ref">13</a></sup></li></ul><h4>后记</h4><p>其实模型训练的结果并不乐观，因为样本分布的不均衡。后来有一些其他想法：</p><ul><li>在模型融合的时候，加大树模型的权重，因为树模型对分布不均衡有所缓解</li><li>使用一些其他方法补充数据，例如SMOTE等<sup id="fnref-14"><a href="#fn-14" class="footnote-ref">14</a></sup></li></ul><p>啊，对了，我当时参考博文<sup id="fnref-15"><a href="#fn-15" class="footnote-ref">15</a></sup>，用pandas-profiling还跑崩了<sup id="fnref-16"><a href="#fn-16" class="footnote-ref">16</a></sup>，数据太多。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=nUhxVDoR3dybcJ6%2FfeyFQg%3D%3D.yy7S7HkoBsEtNR9G%2FF17f9jxYMNeYD%2B8vo3VSkg%2FXCarcScM6ZJc%2BYOFwinJ9%2B2ZzCh8D4amXX4VehwbHcDGdvdhMOWndxNGOMcUa1srna8%3D" rel="nofollow" target="_blank">第九届“泰迪杯”数据挖掘挑战赛</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=FMN8Ef4EYMUj9u%2BGc07Row%3D%3D.QkLqK1nXx6qtnOTQkRHjXlmkJBAYf3p4yOfTwLRETGVKBIhrNWv7kDffk7R1iW1FJeC0ccmYUYyc8vkL12coNw%3D%3D" rel="nofollow" target="_blank">数据探索分析之全局数据如何看？</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=aSPO32NrwaZlX4h8MXRn3g%3D%3D.O%2F%2BjvYXiRngqA8haG4YqeyRr8Db2w9tMB8YLwCvalL6L%2FH2Q2%2FgTYdF15mZKw5yJnix5acxy7qZIfsHOfqBhWQ%3D%3D" rel="nofollow" target="_blank">数据分析之Pandas缺失数据处理</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=hep57bXDYoY8FCW32TCiSQ%3D%3D.f2SV6wNKo54PaPXIvhsBqR4d8%2FhboIHjJmz7BSvs%2F%2Brru7PfXh6r7D4ZNw8aR9KhS%2FAh68nv%2Bi4Ow1RsV%2BYPVg%3D%3D" rel="nofollow" target="_blank">独家 | 在机器学习中利用统计插补来处理缺失值（附代码）</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=iXLGkpt6oQXDjEjFyTNOjw%3D%3D.LkXqB40O0uONXna0maBGoZS2EJUpI%2BLRZqNq3IpD2Qi8ipY%2Fg8WAuvK%2BB%2ByIGc%2FOv90WrD8WVhP8E1y884B9WQ%3D%3D" rel="nofollow" target="_blank">Kaggle知识点：缺失值处理</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=KYgrU%2BjAHUMu2vvahkPUiw%3D%3D.bN7xIJ1yrmWxjLT7sfcmC2thDRZrA8LmPKUO0TaPWKfd%2BhDc7LmPs1giNkslCOKe5S4IZqajisULUmFjrUE1wImDLPAEByRKf5Fl085j1l8%3D" rel="nofollow" target="_blank">Feature Selection with Null Importances</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=6a%2FOsJ1n1kot78%2BFrDSLAQ%3D%3D.1oEHw%2Bo%2FYZzGETPBqDREiF4Au%2Bxzdbd7bBeFMvo8BsljD6gE9xLuqHg%2B18q3S%2FlMs22y2yYn7C6yi4i%2Fu6rv4Q%3D%3D" rel="nofollow" target="_blank">【数据挖掘比赛】之 Null Importances（特征选择）</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> <a href="https://link.segmentfault.com/?enc=0GZsupJ5mB1Dhbrg2bVCEw%3D%3D.y6FDR3yFcLN6ZvZPosKkCf7xlMD0zicbazJeUCqbvyEU1ZefCiWNF0b7NPUF2t3a" rel="nofollow" target="_blank">特征选择之tree的feature_importance的null importance part2</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"> <a href="https://link.segmentfault.com/?enc=LzH85Luvbu0Q80P0xAwnMg%3D%3D.8uOg5LRzzhOj2%2BdA%2FY7kj0XBj9qZyDPqkBfZ0qamEKBe%2B0QvMfjUfDDaaPE2ChBoi9qqMNky00Kvc9Uh4X0Jdw%3D%3D" rel="nofollow" target="_blank">知识点-如何使用互信息进行单变量特征筛选？</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"> <a href="https://link.segmentfault.com/?enc=S0NwBzL7d11hYy0j3Gbbaw%3D%3D.ZHBGMSuEWvko6%2BfY%2FPL7VRYv8LkDL%2FdtWKLOBym5PjsROM40Tzgt5QyuXtJYNWMzhLRlTwBSQ0HMxkmOOM%2FQWA%3D%3D" rel="nofollow" target="_blank">机器学习 特征选择（过滤法 封装法 嵌入法）</a> <a href="#fnref-10" class="footnote-backref">↩</a></li><li id="fn-11"> <a href="https://link.segmentfault.com/?enc=ELrqiAroFdhQhoBVz0I8Kg%3D%3D.4p3txB%2BSYwhYyrKkfFJQMn7JW0tqaxs7XUnn1T%2F5cwPSvnsL83sDayXV52vtEYWw" rel="nofollow" target="_blank">第八届“泰迪杯”挑战赛A题优秀论文——基于数据挖掘的上市公司高送转预测（1）</a> <a href="#fnref-11" class="footnote-backref">↩</a></li><li id="fn-12"> <a href="https://link.segmentfault.com/?enc=0anzrpYk%2BE4RHh2dtiULSg%3D%3D.OXi1HmL%2F5iJHysyv0hwSbd7BAcSmn5QI6N9FZyaONxcoqb9IO%2BUqrzmebjohPgU9Xqn8cDvtEJE%2F2blBz2OI4Q%3D%3D" rel="nofollow" target="_blank">集成学习中的 stacking 以及python实现</a> <a href="#fnref-12" class="footnote-backref">↩</a></li><li id="fn-13"> <a href="https://link.segmentfault.com/?enc=dgCK7ugU8pPnevZGuFdmbQ%3D%3D.slTMzLQcCAjDY%2BW1y%2BgHqCUtyPoJITMJ%2BtaQX7gsg4VVKnFbYRcs0zRYq3fUuNZOECWfSBm6cmlArSvOGrvWqQ%3D%3D" rel="nofollow" target="_blank">详解 Stacking 的 python 实现</a> <a href="#fnref-13" class="footnote-backref">↩</a></li><li id="fn-14"> <a href="https://link.segmentfault.com/?enc=acf%2FFxk5Jd3BUYCL9bcVQQ%3D%3D.6r0tIgol0QxnYaGdSKQNlyGsEsMCq3odKpiT2k6Sld4aPZR5H5DCKTFKN9VtQQlW" rel="nofollow" target="_blank">对"样本不均衡"一顿操作</a> <a href="#fnref-14" class="footnote-backref">↩</a></li><li id="fn-15"> <a href="https://link.segmentfault.com/?enc=NOdfxRzKu5r%2FS3c1jr8CJw%3D%3D.HdrEhfbBQCQ8%2FiSisAvLWs1Od%2FBSaFGeIqFGz7tLiNQBAKsw4VEsQSe8ceQ1ZrDRvol%2FH6%2FuaI7seNL2jtrkkg%3D%3D" rel="nofollow" target="_blank">2020泰迪杯数据挖掘挑战赛总结（A题）</a> <a href="#fnref-15" class="footnote-backref">↩</a></li><li id="fn-16"> [[未解决]pandas-profiling出现MemoryError](<a href="https://link.segmentfault.com/?enc=htob%2FHwgDGwPUuWJCnHdkw%3D%3D.eMO8EdFvPC%2B1gwF1zwwL8QGpSdsLloOooGGhq1kNh42vLlB6m8e%2F4vmpCNwDr%2Fr2Rn9tJkF8j6dN%2BBj93tPihQ%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/weixin_52202311/article/details/117090739</a>) <a href="#fnref-16" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]2021微信大数据挑战赛_方案 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494163</link>    <guid>https://segmentfault.com/a/1190000047494163</guid>    <pubDate>2025-12-22 18:01:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>@<a href="目录" target="_blank">TOC</a></p><h4>问题概述</h4><p>先来看看这冗长的赛题说明<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup><br/><img width="723" height="1780" referrerpolicy="no-referrer" src="/img/bVdnry0" alt="image.png" title="image.png"/></p><h4>baseline</h4><p>最早是参考<code>麻婆豆腐AI</code><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>的baseline，跑了一遍。</p><p>这份代码用了<code>LabelEncoder</code>、<code>OneHotEncoder</code>，好像没做特征工程，直接丢进去了</p><p>后来在周周星的分享上找了一个baseline<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>，结构相对来说更清晰一点，就按照这个改进。</p><p>先聊一下这份代码的基础特征吧</p><p><img width="723" height="312" referrerpolicy="no-referrer" src="/img/bVdnry1" alt="image.png" title="image.png" loading="lazy"/><br/><img width="723" height="338" referrerpolicy="no-referrer" src="/img/bVdnry2" alt="image.png" title="image.png" loading="lazy"/></p><p>详细描述如下：</p><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>滑窗提取特征：</p><p>首先是滑窗提取特征，即过去N天内，看过多少视频、点赞多少视频等等</p><p>baseline当中Ｎ取的是5，后续参考其他文献等改为7，有提升</p><p>这里的时间粒度都是天，也就是说，如果用户在当天内重复观看某条视频，那他只有一条数据行，只是在<code>is_finish</code>列为<code>1</code>，而在<code>play_times</code>列可能是<code>2</code>或者其他，在<code>play</code>列的时长要超过视频的时长<code>videoplayseconds</code>。但如果用户在另一天重复观看此视频，则会是一条新的数据行。两种情况的数据行在<code>userid</code>、<code>feedid</code>、<code>authorid</code>等列都保持一致，但在<code>date_</code>列则是两个数值。下面的计不计人重复，一般都是对当天内的重复观看的情况而言的。</p><p>然后这些滑窗的特征如下：</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>曝光数据</p><ul><li><p>该用户看过多少个视频</p><p>在<code>天</code>的尺度内，计量单位是<code>个</code>，也就是说当用户在某天内观看某个视频两次，在这里仍旧被记作一次，但如果在另一天再次观看，则被记为两次，下同</p></li><li>该视频被多少个人播放</li><li>该作者被多少个人播放</li><li>该用户看过该作者多少个视频</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>转化数据</p><ul><li><p>该用户看完的视频个数在看过的视频个数当中的占比</p><p>即对标记是否完整观看的<code>is_finish</code>列进行求平均，即为完成率</p></li><li><p>该视频被看完的次数在被看到的次数当中的占比</p><p>此处同样不计入当天内的重复观看，同样对<code>is_finish</code>列进行求平均</p></li><li><p>该作者被看完的次数在被看到次数当中的占比</p><p>同上</p></li><li><p>在该用户观看过的该作者的视频当中，看完的百分比</p><p>同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p></li></ul><ul><li><p>对于每个用户而言</p><ul><li><p>对其在过去N天观看单个视频的次数，<code>play_times</code>列</p><p>此处计入当天内的重复观看的次数，即对同一个视频反复观看</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天对单个视频的观看时长，<code>play</code>列</p><p>此处计入当天内的重复观看的情况，即观看时长超出视频时长的情况</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天在单个视频的停留时长，<code>stay</code>列</p><ul><li>求平均</li><li>求最大</li></ul></li></ul></li><li><p>对于每个视频而言</p><ul><li><p>对其在过去N天被观看的次数，<code>play_times</code>列</p><p>此处计入当天内重复观看的情况，即对同一个视频反复观看</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天被观看的时长，<code>play</code>列</p><p>此处计入当天内重复观看的次数，即观看时长超出视频时长的情况</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天的停留时长，<code>stay</code>列</p><ul><li>求平均</li><li>求最大</li></ul></li></ul></li><li><p>对于每个作者而言</p><p>同上</p></li><li><p>每个观看过的每个作者</p><p>同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>对每个用户而言，在过去N天内</p><ul><li><p>其点赞的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其查看评论的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其点击头像的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其转发的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li></ul></li><li><p>对每个视频而言，在过去N天内</p><ul><li><p>其点赞的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其查看评论的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其点击头像的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其被转发的数量的</p><ul><li>求和</li><li>平均</li></ul></li></ul></li><li><p>对每个作者而言，在过去N天内</p><p>同上</p></li><li><p>每个用户对每个作者，在过去N天内</p><p>同上</p></li></ul><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>全局提取特征</p><p>这里针对的是用户的所有历史数据。由于baseline当中在开头就把测试集和训练集放在一起做特征工程，所以这里其实统计的是过去15天的历史数据，即包含了将要预测的第15天的观看次数等。</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>曝光数据</p><p>此处计入不同日期的重复观看的情况</p><ul><li>用户一共看过几个视频</li><li>该视频一共被多少人看过</li><li>该作者的视频一共被多少个人看过</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>用户与视频</p><ul><li><p>对于每个用户而言，在所有历史数据当中，其看过几个视频</p><p>此处不计入不同日期的重复观看，在所有历史数据意义上的<code>个</code></p></li><li><p>对于每个视频而言，在所有历史数据当中，其被几个用户看过</p><p>此处同样不计入重复，同上</p></li></ul><p>用户与作者</p><ul><li><p>对于每个用户而言，在所有历史数据当中，其看过几个作者</p><p>此处不计入不同日期的重复观看，在所有历史数据意义上的<code>个</code></p></li><li><p>对于每个作者而言，在所有历史数据当中，其被几个用户看过</p><p>此处同样不计入重复，同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>用户与作者</p><ul><li><p>用户在某一天内看某个作者多少次</p><p>即按照<code>userid</code>和<code>authorid</code>进行<code>groupby</code>之后，对<code>date_</code>列进行计数。假设这种情况下，<code>10</code>在<code>date_</code>列出现两次，即意味着用户在第<code>10</code>天看过该用户<code>2</code>次。</p></li><li><p>用户看过的该作者作品在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>用户看过的该作者作品，在该用户看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li>用户观看过视频的平均时长</li><li>作者创作视频的平均时长</li><li>作者创作过几个视频</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p></li></ul><h4>改进-0</h4><p>其实这个不算改进吧，就使用<code>optuna</code><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>对baseline进行了调参，大概上升了<code>0.03%</code>？</p><p>然后后面就没啥用了，可能是我代码的问题，做了新的特征之后调出来的参数竟然和之前一致，没有变化</p><h4>改进-1</h4><p>参考了文章<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup><sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，对特征进行改进</p><p>其实也就是在相应的<code>for</code>循环当中加入了新的特征列名</p><p>baseline当中，滑窗类的特征主要做了以下四个：</p><ul><li>用户</li><li>视频</li><li>作者</li><li>用户对作者</li></ul><p>在上面的基础上追加特征：</p><ul><li>背景音乐</li><li>背景音乐的歌手</li><li>用户对背景音乐</li><li>用户对背景音乐的歌手</li></ul><p>在全局特征当中追加：</p><p>+++++++++++++++++++++++++++++</p><p>曝光数据</p><ul><li>背景音乐被播放几次</li><li>歌手被播放几次</li></ul><p>+++++++++++++++++++++++++++++</p><p>与<code>用户与视频</code>和<code>用户与作者</code>相并列的特征</p><p>用户与音乐</p><ul><li>用户听过多少次这个音乐</li><li>这个音乐被多少个用户听过</li></ul><p>用户与歌手</p><ul><li>用户听过多少次这个歌手</li><li>歌手被多少个用户听过</li></ul><p>+++++++++++++++++++++++++++++</p><p>与第二个<code>用户与作者</code>相并列的特征</p><p>用户与音乐</p><ul><li>用户在某一天内看某个音乐多少次</li><li><p>用户看过的该音乐在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>用户看过的该音乐，在该用户看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>用户与歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><h4>改进-2</h4><p>加入<code>device</code>的特征，感觉这个没啥用</p><p>同时将全局特征的统计限定在前14天，也就是不统计测试集的观看数据</p><hr/><p>后来想想不太对，这个地方其实应该统计全部15天包含测试集的数据</p><p>是这样的，真实情况下，我们不知道第15天的行为对应的标签</p><p>也就是说，当我们去做推荐的时候，用户还没有对这个视频产生交互</p><p>但是在打比赛的时候不一样，打比赛的时候已经有标签了</p><p>那也就意味着，测试集当中的数据，应该是发生过的</p><p>是属于用户画像的一部分的</p><hr/><p>+++++++++++++++++++++++++++++</p><p>在时间滑窗当中加入：</p><ul><li>设备，<code>device</code></li><li>设备对作者</li><li>设备对歌曲</li><li>设备对歌手</li></ul><p>+++++++++++++++++++++++++++++</p><p>在曝光当中加入：</p><ul><li>设备，<code>device</code></li></ul><p>+++++++++++++++++++++++++++++</p><p>相应加入：</p><p>设备对视频</p><ul><li>该型号看过几个个视频</li><li>该视频被几个型号的看过</li></ul><p>设备对作者</p><ul><li>同上</li></ul><p>设备对歌曲</p><ul><li>同上</li></ul><p>设备对歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><p>设备与作者</p><ul><li>设备在某一天内看某个作者多少次</li><li><p>设备看过的该作者作品在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>设备看过的该作者作品，在该设备看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>设备与歌曲</p><ul><li>同上</li></ul><p>设备与歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><h4>改进-3</h4><p>加入<code>embedding</code></p><ul><li>使用<code>word2vec</code>对<code>keyword</code>和<code>tag</code>进行编码，生成<code>32</code>维的向量，作为视频的特征数据</li><li>使用<code>pca</code>将<code>feed_embeddings</code>降维到32</li><li>对以词为单位的<code>description</code>、<code>ocr</code>、<code>asr</code>使用<code>word2vec</code>生成<code>32</code>维的向量，并计算两两之间的差距</li></ul><h4>改进-4</h4><ul><li>对以字为单位的<code>description</code>、<code>ocr</code>、<code>asr</code>使用<code>word2vec</code>生成<code>32</code>维的向量，并计算两两之间的差距</li><li>尝试统计用户点赞过、评论等操作的视频抽取其特征，形成用户的兴趣特征。但没有成功</li></ul><h4>结果</h4><p>A榜当时好像0.659吧？我印象还在0.66挣扎，就差一点，B榜根本没提交</p><p>周周星<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>也用单模的树模型，都能干到0.675</p><h4>参考</h4><ul><li><a href="https://link.segmentfault.com/?enc=0lalaLaX5uyHmXzb44jYsQ%3D%3D.38W%2B1qlsOteYEUvh6I7T5g6ELJa4%2FeuU%2FI8JSQ795BINMeGPRQK8D2n12Q%2F6xamFCoE18E2wwO9Pstf9m%2FKqRw%3D%3D" rel="nofollow" target="_blank">她来了她来了，乘风破浪的微信视频号推荐算法</a><sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup></li><li><a href="https://link.segmentfault.com/?enc=EMx2%2FQe0d6nDPKwoD5BtJQ%3D%3D.odEyLT3kB4TrMDaMsP%2BqQw9XjyCBgVyI%2Br6%2B6euuruCZ8r%2B7Svjo4dbQxHTLLcwFwj3YrOfGEiQteO91txHvFg%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法挑战赛baseline分享</a><sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup></li><li><a href="https://link.segmentfault.com/?enc=6phesTmbdbF3D6W%2F6ma6Vw%3D%3D.05vrg%2FsoMKCzjKqqCjwd%2FuAbBQIvCCJS%2FaCDll4NUqbNo7W5pHd0XYCNgcGt%2FLimWlgB1FF8gCfqjtvpimBfqQ%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛：LightGBM vs DeepFM</a><sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup></li></ul><hr/><h4>更新</h4><p>OTTO队伍的树模型方案，<a href="https://link.segmentfault.com/?enc=coUOJh%2FqafRnSoglDGBSaQ%3D%3D.hlxrkna3ZZZ5TT56WO8sq4EgXROt28fKHDFP7K8P2rdy3i%2FfFtl38i44sjtAxjPPx4x6R2Q%2BcYjHqCxA127XMA%3D%3D" rel="nofollow" target="_blank">https://github.com/juzstu/WBDC2021_Tree_Solution</a></p><p>江离的方案 ，<a href="https://link.segmentfault.com/?enc=sQp3LdN%2FDw%2FJ1fP%2FmKp1aA%3D%3D.GEEMcJyNoUEeTwfhelOfjpGkfPeYXV497%2BSoeicw7tsBN4e7le5lqhRDUH4HDLsM" rel="nofollow" target="_blank">https://github.com/ji1ai1/202105-WEIXIN</a></p><p>关于NetworkX，用图构建用户和视频的关系，做协同过滤</p><p><a href="https://link.segmentfault.com/?enc=en5VL2%2FnNjDS5kJKTm51PA%3D%3D.7FyN3TWCULTIZyw%2Bhpc1v6BOy40KPEgwhkekfwKCjr7fkF7F1LUpLyxihmOXIGpDqZ6jzMHv2BOjeA1GgMy4Nw%3D%3D" rel="nofollow" target="_blank">定量分析方法第17讲：NetworkX基础</a></p><hr/><p><a href="https://link.segmentfault.com/?enc=9w2mA%2BNmRApcWLqQwPy9pg%3D%3D.mOzs4tP7mywYBNbJx3PeEjCbtV0FZoN9Y3sNksw8UGNJhMB%2BRkMFThL8KwLgnXMP6%2FYeMbPKI41gyO1texJnYg%3D%3D" rel="nofollow" target="_blank">特征工程之tag-pooling，以微信大数据比赛数据为例</a></p><p>这个我好像用的是第二种方式，<code>word2vec</code>对每个视频的<code>tag</code>进行<code>embedding</code>之后求平均</p><p><a href="https://link.segmentfault.com/?enc=AoNGyPB3gydx8DolQ%2B1HsA%3D%3D.A6V2uZWajEfnD0EEWS2rlJ7za03KwOvRjNFu6QouENOmNxwYB9sx5QqBap1JTn5Xz%2Bd8PKhXp8XwVi6K%2Bv9SBQ%3D%3D" rel="nofollow" target="_blank">微信大数据竞赛Trick--如何3ID上0.706+</a></p><p>这里好像是按照视频对用户进行<code>groupby</code>，将每个视频的观看用户理解为文档，将视频列表理解为文档列表，对用户进行<code>TFIDF</code>编码，然后使用<code>TruncatedSVD</code>降维。</p><blockquote>该特征的本质就是希望找到基于视频的用户共现性，如果某些视频某些用户总是一起出现，那么就说明这些用户大概率有相同的爱好，就很可能一起转发，一起评论并且同时关注了。</blockquote><p>下面这两个也有使用这个方法</p><p><a href="https://link.segmentfault.com/?enc=GJINf6KLVYIMM3JQRyogTg%3D%3D.9a1qg5ptk%2BXFL0gLuu%2Bo2GZzJxbRuL3mFnEB46La3tCXHtU79ebCiAzBJ2%2BNZIokF8ygXwskw6dXxD8bBv%2BoaQ%3D%3D" rel="nofollow" target="_blank">中科大倪茹：感谢开源，我从入门竞赛到Top 10的经验分享</a></p><p><a href="https://link.segmentfault.com/?enc=T%2Ffrhnxf5FJ7PhcY5d4sMw%3D%3D.cSJExapk9fgV8q7sQw1YbUBke0Z60AFFsQHFkUL35mlAU3m11oJctQaeTtznUcPSQsb4UR8rPIB2%2BTJSlaIBFQ%3D%3D" rel="nofollow" target="_blank">2020DCIC智能算法赛-智慧海洋建设TOP1方案</a></p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=ptrbbP0lGQP6Mb5a8mKvsw%3D%3D.v%2Bx%2FqV28k4yqis2JP37H394QJBsApLe%2FZGVFh%2Bm7qLE%3D" rel="nofollow" target="_blank">微信大数据挑战赛</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=%2Fi972vADzemz2UYOLNvEsg%3D%3D.KJVS8P2SoyYaZGCM7EHYLREpTrwuCRACyAgl5pS37DYJ9mfGYThIlvwUkgUv%2BPrZnECa4sUbsFCKMj6Yv%2F%2BHHg%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=TD3u%2FSbbMyYTLrygUsTlWA%3D%3D.hC0jrbgVYHDaKwGlGWWGWj94hGmonNzPp%2FvFSMPcyZBX4vrOHRbjhQiAVU71T92gx9SGwGGw%2F8ugzFhADKNS2JpHqqaNdVYF3ReHKpHEyOQRIzk3dnUh0IlxL0Cl%2BeLA" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=ibYG6S2vbZrmS%2FDlGcrIAA%3D%3D.Wy5W00HNfnzxxwE%2BIbDP60aCd%2F7Sx%2BlEpq0TeEyC1y5ArX8k6bOXzl%2Fu%2Bppv3bLqWyJfhwpyTTYHpf4OGLkZfQ%3D%3D" rel="nofollow" target="_blank">席卷Kaggle的调参神器，NN和树模型通吃！</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=P8rNZoOB42JZr1tI%2BEIiuQ%3D%3D.nFP%2BoTyhV2ZC69J66uviGwnEYNo7y9E93JN7DjggFBVOPPQf9BUIRgb3MU1jUxJbuIVD5mbDlK4FwZzTVMn9hQ%3D%3D" rel="nofollow" target="_blank">【机器学习】Optuna机器学习模型调参(LightGBM、XGBoost)</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=dwR%2FfgurjvHyuBh7TaPd5w%3D%3D.Y6txoUjrEMv7xKM7RfWXZQ1N7RgWvB429Uesj4Ln%2FK913E4vJ%2BQE6Yu5MbBq31JCAlP41M%2BkgkRntZtRklA48g%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法解题思路</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=YJip%2BVt3pajGLnr%2Fvm4fyQ%3D%3D.23ixoPtUuMNfIqQF0M57rZy1GdPDdZBpYXimWftnKJCREA8anL71zP1X2YsFCGkmh%2FO7Y3qseNRDhsgFZKvWFw%3D%3D" rel="nofollow" target="_blank">微信大数据比赛我用树模型怎么从514到671</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> <a href="https://link.segmentfault.com/?enc=SaNlllod4RbxoVc%2Fhk8bAg%3D%3D.HPCK3y%2FJilj2%2FP0%2BXotrhJG5jxshDEkR0P8L0JXix%2BM6ospuXNexw9%2BHIl%2B6EnijG1FCqH%2FPg76EX%2Byf6hYhaw%3D%3D" rel="nofollow" target="_blank">她来了她来了，乘风破浪的微信视频号推荐算法</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"> <a href="https://link.segmentfault.com/?enc=0JM52a4ha1FBYF3Z4PuNdA%3D%3D.%2BmXX9LRDbbjhXN%2BwDkRIY%2B0jOjCoGJ39W%2FI5U9b1j4JKWzTWNQeupun4ggwEf8Q5VIB%2Ft1%2F1evNXVek8bDeHSg%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法挑战赛baseline分享</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"> <a href="https://link.segmentfault.com/?enc=3UvWV%2BQ5ap0gPk8QDAV%2B8w%3D%3D.5s3vALEkswPHrsetGnMLPoi7FVrOoureD6Jk%2BFER%2FzAzUNml9r9Rkn4qfAuE71bPiWI69KtnM3CI4nTHBBHo4A%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛：LightGBM vs DeepFM</a> <a href="#fnref-10" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]2021微信大数据挑战赛_总结 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494173</link>    <guid>https://segmentfault.com/a/1190000047494173</guid>    <pubDate>2025-12-22 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>确实很早就知道了这个比赛，但似乎一直没有开始。</p><p>之前也就刚打完一个泰迪杯，对于竞赛可以说是一无所知。</p><p>于是就等baseline，等到了麻婆豆腐<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>的那个版本。</p><p>5月20日开赛，我跑baseline大概都快6月中旬。</p><p>那时候一边忙着期末的各种大项目，一边找竞赛的各种指导和资料。</p><p>跑完麻婆豆腐那个，还是一头雾水，不知道这到底是怎么操作的。</p><p>当时为了<code>LabelEncoder</code>、<code>OneHotEncoder</code>都看了好久</p><p>后来找到了天才儿童（好像是这个名字）的baseline<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，结构清晰</p><p>这里面的特征工程几乎都是由<code>for</code>循环控制的，增添特征只需要在循环里面加入列名</p><p>这份代码有用到滑窗法，其详细的信息参考文章<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup></p><p>真正开始做，大概都是6月25日， 先读代码</p><p>弄清楚了结构，就调参、加特征</p><p>A榜基本在常规特征里面折腾，好像稍微加了点<code>embedding</code></p><p>A榜结束的那天晚上我还在疯狂做特征，意图冲击B榜</p><p>然后就出问题了，特征太多，我那会儿都搞到了将近1000+</p><p>从周周星的分享<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>来看，似乎最多500+？</p><p>当时做到用户的兴趣特征，就是把用户看过的那些<code>tag</code>和<code>keyword</code>求平均</p><p>但是贪心，减了一部分，尝试，炸了，再减，再尝试</p><p>由于疯狂做出来的特征没经过验证，质量也不好，所以一致炸</p><p>这也就导致了....我B榜根本就没提交，因为没跑出来</p><p>血泪的教训啊...一定是做加法而不是减法</p><p>应当先跑个基础版本提交，说不好还能中奖</p><p>后来复盘的时候，觉得还是经验不足，好在此次有收获</p><p><strong>首先是，做加法，一定要按照顺序做</strong></p><p>先去挖基础特征，再去做复杂特征</p><p>可以参考这篇文章当中对于往瓶子里面加石头的比喻<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></p><p><strong>然后，做出的特征最好离线保存</strong></p><p>一是方便后面可以直接载入，而不用再做特征</p><p>二是方便调整方案，对不同的特征进行组合验证</p><p>因为训练的时间可能很长，如果能节省做特征的时间，就能充分利用提交次数</p><p>关于特征的离线保存，CSV确实有点慢，可以换别的试试<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup></p><p>对于做完所有的那个好大的数据框</p><p>我当时尝试过<code>pickle</code>、<code>parquet</code>、<code>feather</code>，都炸了</p><p>不仅炸了，还浪费了时间，因为我是在B榜那天做的</p><p>猜测是数据规模的问题，可能他们无法处理？</p><p>但是也不对啊，<code>parquet</code>可是<code>Hadoop</code>那套系统里面的，例子<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>也是<code>2.5GB</code></p><p>当然可能这个总的、所有特征的数据框远超<code>2.5GB</code></p><p>不太清楚发生了啥，我也很疑惑</p><p>后面可以试试分拆数据，然后用其他格式保存，提高<code>I/O</code></p><hr/><p>啊啊啊啊找到原因了！我在本地跑的时候报错，，缺少相关组件。果然还是不熟悉这些工具</p><p>这是<code>parquet</code></p><pre><code>ImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
- Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
- Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.</code></pre><p>这是<code>feather</code></p><pre><code>ImportError: Missing optional dependency 'pyarrow'. Use pip or conda to install pyarrow.</code></pre><p><code>Pandas</code>的<code>Pickle</code>倒是可以直接用，其模块的后缀为<code>.pkl.gzip</code>，在这篇博文当中有说明，不同于<code>Pickle</code>原生格式的文件类型<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup></p><p>但如果是原生的<code>.pkl</code>的后缀，好像是可以直接用<code>Pickle</code>模块，而不用装别的库。</p><p><code>.pkl.gzip</code>的后缀还没有尝试过</p><hr/><p><a href="https://link.segmentfault.com/?enc=zYZf3V4pptZr%2BLw7sHVSXA%3D%3D.jnnKEbxZKNGvuC5Dtpt23OmsjMhcX5WUyvQCXR2IqJSVZJUc8Ch5t6YRPJJULlQ%2FUuz6oUWO5V5vvzgV%2B7mA%2Bg%3D%3D" rel="nofollow" target="_blank">做竞赛别用csv文件了。</a></p><p>这个测评看起来是<code>parquet</code>比较省内存，有压缩文件。不过单纯考虑速度的话，自带的<code>Pickle</code>就够用了。毕竟<code>parquet</code>还要装其他的库</p><hr/><p><strong>还有一个神奇的东西，<code>reduce_mem</code></strong></p><p>好像是对数据格式进行变换，反正很省内存</p><p>具体的代码可以参考<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，两份代码里面都有</p><p><strong>对了，LightGBM也有过一个报错</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=jrtc93jyQPrErDubKpLRMQ%3D%3D.%2BQ5TANZp2MFgJl9nUa06hJo3ivHRXqsOd9sxwGP5rqQPPpqKEB7peBmQO0K0V5HRPIMyDNg3rW06HXD8LjbjgA%3D%3D" rel="nofollow" target="_blank">LightGBM.cv时，feature_pre_filter和min_data_in_leaf相互矛盾</a></li><li><a href="https://link.segmentfault.com/?enc=Jl2GyFxOg%2Fz3g6TzFOpa4Q%3D%3D.NeErAXsXULE5CqoK8rcLM3wfuHDExihH6tUtI08cUEYcu2BWng4pTYQqWpDWforE" rel="nofollow" target="_blank">LightGBMTunerCV stepwise tuning got an error with lightgbm==3.0.0rc1 #1718</a></li></ul><p><strong>word2vec也有报错</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=bF6enjSABbQqRZJLIdmeLQ%3D%3D.HmAQAsEjU4NzqWwPOQNbvdv9VyNLfiGbTlJTWUywWgAxSxvac7qS4AmXoIPr3UxtS4CUnWk6ysP%2FS2dzjADYUw%3D%3D" rel="nofollow" target="_blank">Word2Vec TypeError: __init__() got an unexpected keyword argument ‘size‘</a></li></ul><p><strong>关于保存并实时查看训练过程</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=NzQ3%2Bw2HHfwxltXXi8Efwg%3D%3D.4GVCSZlglo9fExevlGXthcobAmTBAUmQ46DZ5N8HEDjXbDQXmSXuoJ5dILvjxgApvaeNW7pi%2BmFhpyP1F0VKYA%3D%3D" rel="nofollow" target="_blank">Linux保存并实时查看训练日志</a></li></ul><p><strong>这里也放一下相关的比赛的参考吧</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=lqg9cvVt%2FeSe4f7r7hjQfQ%3D%3D.FeqZT84vOKuwNkTLKCD3%2BDQZZMWAkkDSfCUtfcN0Y1l0e4BBClCnZVXoKItpYULIotVgyzug8qUYwX3mVK%2BZIMcb3ukq9giyXR5oB5sYc9W74iJ9b9BifJjc%2FQ64KblS" rel="nofollow" target="_blank">华为digix算法大赛2020机器学习赛道-ctr预估初赛/决赛rank1</a></li><li><a href="https://link.segmentfault.com/?enc=ygdBJ2MmZvARlDZp1jGCKg%3D%3D.Mf4WKEMEFnITsMrHJItBa2%2FBR%2F%2FKDr8wPDWtCIUzreJ%2BwqkzJCwBIcAWkU3Rub7m8DAoITFmd88OxzQhUf5ElQ%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历（一：数据预处理、特征工程）</a></li><li><a href="https://link.segmentfault.com/?enc=jxtn5Kj39CE2gTDLWEAaxQ%3D%3D.6YtBYUpDFEj7owuM44De%2BdHd8VR9A2reG7ujeoIu%2FGkOCcoeqj8o9Fbmlk8QAh28lBfD2HUHsA%2BBBuY4MvItMQ%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历（三：lightgbm调参、ensemble）</a></li><li><a href="https://link.segmentfault.com/?enc=J2DqZQ7ljQUpzJ884IZoGw%3D%3D.UUfL%2FgRQ16ZTkll9VKUn%2FjaVIrlqmWhUxzTMqhvm707r2PZYK9ECZunDWdI7E5KA4eeGxDZerKikxIfW9uqgVA%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测(完整版代码，数据集等总结)</a></li><li><a href="https://link.segmentfault.com/?enc=HwYungiP8MCwxpNR6wSBTg%3D%3D.V4qFnqSQq9p96aKJ0tCvxaoCje8qO0Atiev3kQvuCQW5z2Cz%2B0hoiOCYlPGTSMkXMlVG0pwwTPwHVKV14NYaBw%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历(完：观看决赛答辩感想)</a></li><li><a href="https://link.segmentfault.com/?enc=wl%2B1DuFMoFsnl0qpPup6%2Fg%3D%3D.%2FLrQMCN46ZlcNGq%2B4DYDzmzeQuYNIFNrm6%2Ba21GiilIZkYZepRi4ucnYOUvAjbI2rnDk0CbvuuEHnicoKRZrRA%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛 记录</a></li><li><a href="https://link.segmentfault.com/?enc=W481Y41MgtXRdrSYhQDlPg%3D%3D.8uYTKGtz49Dsm0bkQOsHtk6qPjh18qpgfWYEDVXRAEJhSK3vhbUDLLL7%2FYCnJAVt" rel="nofollow" target="_blank">ijcai-2018 top1 solution-阿里妈妈搜索广告转化预测</a></li><li><a href="https://link.segmentfault.com/?enc=0R4doceIyAYTaRwLLL%2FgcQ%3D%3D.3ikdyCh2M9r0YwSFvnkaa4ltaE%2B8wosUbq5I6y2Xir4vgm8G22PJsriP9DnAeIsvQWBmC2ZgJ6%2FksTnprgiDhw%3D%3D" rel="nofollow" target="_blank">2021-07-01 微信大数据竞赛源码(免费colab/GPU版本）</a></li><li><a href="https://link.segmentfault.com/?enc=RjNXvvgL68aKuPI5Jh6euA%3D%3D.zsb6n2R3RBzAeckYcdHC%2FpkDlOd55bFd3cC6uZGENMmphRt%2BLbzZofMG0irGQsNiBapWBK5rzbxcPMidHTMIfszGmaCDB%2B1FXJ40tngOKqo%3D" rel="nofollow" target="_blank">2018腾讯广告大赛baseline 100行代码带你上0.73</a></li><li><a href="https://link.segmentfault.com/?enc=hqR2q0kiNGJXbiFBRWtFyw%3D%3D.Xowf%2F57PTQfOozXQ%2BVkeEjoOp1GeEpxw6QKg%2FZoKgjnzU8KnCXqSzGadQaAL0HV15VT65k87blrQJHM%2Fre58cg%3D%3D" rel="nofollow" target="_blank">华为digix算法大赛2020机器学习赛道-ctr预估初赛/决赛rank1</a></li><li><a href="https://link.segmentfault.com/?enc=9ImZdZZYmOYemhUWpXG8vA%3D%3D.6JNUjVPBXj1uFEagXUKJdiQOxAnPsYN0Cy%2BCCj00K4nz1pz2MCZ35f3tW23wzKge" rel="nofollow" target="_blank">【IJCAI-18 阿里妈妈搜索广告转化预测】大赛答辩</a></li></ul><div class="footnotes"><hr/><ol><li id="fn-1"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=OrJYM9HenfBKJXnI7BTPug%3D%3D.444adoaOpVfSkEw%2BWD7%2FjiXuCrx9qjnKBKRApiSnlV6reCVm2Ipy2Ps0f6DIWPO%2Bw6F4erEp4nM%2FjmtJiLZqyg%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=7gcvHPmE2dC3x5EXuqdzlw%3D%3D.ZNPRYtgbpJ0Pb%2FqQFWbdfIHkj9ldFLqgHgn%2FhWYAyla7rllbn6n23WOrOXSunWvWH6qk92IzjbCsFGG0BqF6vr3ZKj9qZgbc8V335D3zDPTsuOTexYsJm%2BCEskFGW5o%2F" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=zoUsZ8PvI2KQ5iRqIePgQg%3D%3D.KwxjHVVpckJ%2FB0NV%2F2SD10oAQHdTnHxf6CemlR0Q7%2BowZeh6dDzkwVKrU2EwH3mkWH9fus%2BxFNX2Tu51owuujw%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法解题思路</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=1qYCoaANcPYUDkBq0cDbqw%3D%3D.LiW%2B0tod%2B%2B52cWmJAzr5%2Bv3lkY6LU1qdOxVMRIyy45qEXv3vC9Uez3l0zAkNzozzOYADmdF%2Bv7jvUzi3QoD1GhA5QHJdILsDAvFY6U0Nk%2Fwfzr%2FZf5WmxHVeyzl2%2BPy%2B" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=HOQmC0OJH6lH9e7r2w8cvg%3D%3D.DESa1KvlkcmU9efjsQ%2FEFsMhfDbhWQMGSpddB8xJMZx3IvUdpWmWNR8gp7IysaEgsLi7yodIynJYd2wZhearDg%3D%3D" rel="nofollow" target="_blank">推荐大赛如何在一周时间内打进决赛</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=YQpVPkX2SNDcTbqgxZQ36w%3D%3D.wpkT4tB0EPcGNFFldJ5kVYZ5W3fKYyxzhg1ib5SCmT6FcAV97zmyLenZQSerbja6UXIZRKxi1QmNfNy9Vvj9AA%3D%3D" rel="nofollow" target="_blank">对比不同主流存储格式（csv, feather, jay, h5, parquet, pickle）的读取效率</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=J2hl6QjsJ8V3W%2BsSt9e4Rw%3D%3D.7%2B0aZgbKavBw3Vcp9aJgj6cZzkUykr6YOaoBcDr4q9GEdl3OWLl3%2Fuc6dGrXTcMO" rel="nofollow" target="_blank">Python Pandas to_pickle()压缩文件</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=CHUx56nIUIi1Mzjs4QYyjA%3D%3D.Yl%2Bl6hFUpzykWarpnTj96GUXNFCSKuNLCNL%2FCYcG4rW526RT50Isa8NkI5Rfe1trErSl8yRA5dUJkCr6spWLMQ%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-8" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[烟草专卖执法案卷评查系统：案卷质量效率全域跃升 中烟创新 ]]></title>    <link>https://segmentfault.com/a/1190000047493686</link>    <guid>https://segmentfault.com/a/1190000047493686</guid>    <pubDate>2025-12-22 17:21:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>烟草专卖执法案卷评查工作是保障执法规范性、提升执法公信力的重要环节。随着法治建设的深入推进和案件数量的持续增长，传统人工评查模式已难以适应新形势下的监管需求。北京中烟创新科技有限公司（简称：中烟创新）为此研发了“烟草专卖执法案卷评查系统”，针对烟草行业痛点，提供了提升案卷质量与审查效率的全新解决方案。</p><p>评查系统通过构建AI驱动的全流程防控体系，实现了从事后纠偏向过程控制的范式转移，系统的核心技术突破体现在三维度技术融合架构：动态规则引擎：集成千余项执法标准形成结构化知识库规则，通过自适应学习机制实时吸纳法律法规更新，支持省级差异化监管政策的弹性配置，确保评查基准与最新规范保持同步。多模态分析中枢：结合光学字符识别（OCR）与自然语言处理（NLP）技术，实现对文书格式、证据链逻辑、法律条款引用等多维度的交叉验证。智能决策模型：依托企业级灯塔大模型应用开发平台，具备法律条文解析精度97.9%的语义理解能力，通过模式识别生成风险预警矩阵，为执法人员提供实时决策支持。</p><p>系统采用“人工+AI”双审查模式，融合专业判断与AI效率及一致性优势。支持对接省级监管平台，具备OCR识别能力（纸质/电子文件），推进案卷信息化。显著提升评查效率，达传统模式数倍(如某案例：人工3小时vs系统5分钟），释放90%以上事务性人力负荷，替代了传统逐项核对文书的重负。基于《烟草专卖处罚程序规定》及标准，系统建立精细化评查体系覆盖立案至执行全流程。特别关注程序合规、案件定性、处罚裁量等细节，确保案卷清晰准确记录执法过程。</p><p>自动记录处理阶段并在关键节点发送提醒，保障案件按时按质完成，有效避免人为差错。系统通过六个核心指标评查案卷质量：自由裁量合法性（处罚在法定幅度内）；程序时限合法性（程序在法定期限内）；卷宗形式规范、材料完整；文书内容完整、规范、逻辑一致；法律依据引用准确；文字及多文书信息一致。</p><p>系统还构建了“自查-交叉评查-上级抽查”三级联动评查体系，通过多层级、多视角的检查与复核，确保评查结果客观公正。这种多层次的评查机制有效保障了执法质量的持续提升。系统在三个关键阶段发挥风险防控作用。</p><p>事前预防：通过智能预检自动筛查程序超期、证据链断裂等高发风险，将大部分的常规错误阻断于案卷形成阶段。系统能够实时监测案件处理的各个阶段，对潜在风险进行预警和评估，为执法人员提供决策支持。</p><p>事中干预：建立实时反馈机制，执法人员可在文书制作过程中即时接收格式修正与条款适用建议，显著降低事后整改成本。</p><p>事后治理：通过大数据生成区域性质量热力图，精准定位系统性缺陷，驱动靶向整改。系统自动聚合分析高频错误与典型问题，为持续提升执法规范化水平提供数据支撑和决策参考。</p><p>通过自动化文书生成与智能辅助功能，单个案卷平均制作时间减少约70%，案件整体处理效率提升40%以上。这使得执法人员能够将更多精力投入案件调查与实地核查等核心执法工作。烟草专卖执法案卷评查系统通过技术创新与制度创新的深度融合，树立了传统产业智能化升级的标杆范式，持续输出可复制、可推广的行业级解决方案。</p>]]></description></item><item>    <title><![CDATA[YashanDB的11个功能特性提升数据处理效率 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493689</link>    <guid>https://segmentfault.com/a/1190000047493689</guid>    <pubDate>2025-12-22 17:20:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术领域，随着数据规模和复杂度的不断增加，系统在处理高并发、大容量数据时面临诸多挑战，如性能瓶颈、数据一致性维护困难、存储资源限制等。针对这些通用问题，YashanDB通过其先进的架构设计和功能特性，实现了对海量数据的高效管理与处理，确保数据库系统在各种应用场景下均能保持稳定高性能。本文将系统性剖析YashanDB提供的11项关键功能特性，探讨其在提升数据处理效率方面的技术优势，旨在为数据库开发人员和管理员提供深入的技术理解和实践参考。</p><ol><li>多样化的部署架构支持</li></ol><p>YashanDB提供单机(主备)、分布式集群及共享集群三种部署形态。单机部署适合中小规模应用且对高可用需求适中，主备复制保证数据同步与故障切换。分布式集群采用Shared-Nothing架构，具备线性扩展能力，适合海量数据分析业务。共享集群则基于Shared-Disk架构，利用共享存储和聚合内存技术，实现多实例的强一致性读写和高可用，满足高端核心交易需求。此多样化部署方案为不同业务场景提供灵活配置，优化资源使用，提高整体处理效率。</p><ol start="2"><li>高效的存储引擎及多存储结构支持</li></ol><p>YashanDB支持HEAP、BTREE、MCOL和SCOL四种存储结构，分别针对OLTP、HTAP和OLAP三类典型业务场景进行了优化。HEAP为无序行式存储，适合事务性能需求高的场景;BTREE存储适用于索引结构，提高查询速度;MCOL实现了可变列式存储，通过段页式存储和原地更新技术提高变更效率，支持实时业务需求;SCOL基于对象式切片存储，结合压缩编码提升海量稳态数据查询性能。结合行存表、TAC表和LSC表的灵活运用，满足不同数据操作模式，保障读写平衡和高性能访问。</p><ol start="3"><li>先进的事务机制与多版本并发控制（MVCC）</li></ol><p>全表支持事务的ACID特性，YashanDB通过MVCC技术实现读取一致性，避免读写阻塞。具体实现中，数据库采用事务槽(Xslot)、Undo日志操作数据的历史版本，以SCN为时间视角管理事务可见性。读操作可基于版本快照读取数据，而写操作则通过行锁保证数据一致。支持语句级、事务级一致性读，以及写一致性机制，保障并发事务间的隔离和性能。提供灵活的隔离级别配置，包括读已提交和可串行化，适配不同性能和一致性需求。</p><ol start="4"><li>强大的SQL引擎与基于成本的优化器</li></ol><p>YashanDB的SQL引擎涵盖解析、验证、静态与动态重写、优化与执行等多阶段处理，内置CBO优化模型。优化器基于动态统计信息评估各种执行路径，选择最低成本计划，有效利用索引、计算算子及并行度。支持内置多种执行算子，如扫描、连接、排序和向量化计算算子，利用SIMD技术批量并行处理数据。HINT机制让用户能够微调执行计划，灵活控制SQL执行，进一步提升查询效率。</p><ol start="5"><li>高度并行与分布式SQL执行能力</li></ol><p>支持MPP架构的分布式SQL执行，通过协调节点(CN)、管理节点(MN)和数据节点(DN)分工协作。CN负责生成分布式执行计划，DN并行执行任务，多级并行执行(节点间与节点内)，结合PX并行算子和数据分片交换机制，实现高吞吐量。通过内部互联网络保障高效网络通信，合理分配资源，降低跨节点通信延时，保证海量数据分析的实时性。</p><ol start="6"><li>灵活的存储管理和空间调度机制</li></ol><p>YashanDB逻辑存储层将存储分为块、区和段三层结构，采用段页式和对象式管理。通过多级空闲空间列表与水位线控制，实现精细的空间使用和并发访问优化。支持动态扩展表空间，数据文件和切片文件的并行管理，并利用双写机制防止半写错误，提高数据安全性。合理的空间管理减少碎片，提高I/O效率，促进数据高效访问和修改。</p><ol start="7"><li>先进的索引体系和多样的访问策略</li></ol><p>以BTREE索引为核心，支持唯一索引和非唯一索引，包含叶子块和分支块的平衡结构保证快速检索。支持多种扫描方式，包括全索引扫描、索引快速全扫描、范围扫描、唯一扫描和跳跃扫描，根据查询条件灵活选择。反向索引解决自增列索引倾斜问题。支持函数索引提升复杂表达式查询性能。索引的聚集因子优化存储顺序，降低I/O成本，显著加速数据访问。</p><ol start="8"><li>灵活的PL引擎及过程化编程支持</li></ol><p>内置PL语言引擎提供过程、函数、触发器、自定义类型和定时任务的编译及执行支持。PL语言支持变量、表达式、控制流、异常处理和静态及动态SQL，具备较强的编程能力。支持自治事务提供嵌套独立事务功能，增强事务隔离与异常处理。PL对象可持久化存储，提高复用和执行性能。定时任务管理支持复杂调度，便于实现自动化维护和业务逻辑处理。</p><ol start="9"><li>主备高可用及自动选主机制</li></ol><p>提供基于redo日志的主备复制，支持同步和异步复制模式，保障数据的可靠写入与及时同步。主备切换支持计划内(switchover)和故障切换(failover)，配合redo日志回放和归档修复。自动选主采用Raft协议和yasom仲裁，确保集群一致性和快速故障恢复。支持Quorum配置和多保护模式，兼顾可用性和数据安全性，最大限度提升业务连续性。</p><ol start="10"><li>完善的安全管理体系</li></ol><p>基于角色的访问控制和细粒度的标签访问控制(LBAC)，实现权限隔离和行级数据保护。支持系统特权和对象特权授权，采用多重身份认证方式，包括数据库密码认证和操作系统认证。实现用户管理、密码策略、安全审计及访问日志，保证数据保密性和可审计性。网络层支持SSL/TLS传输加密，保障数据传输安全。数据库层加密支持表空间、表和备份集透明加密，保障数据静态安全。</p><ol start="11"><li>高性能共享集群基础设施</li></ol><p>共享集群基于崖山集群服务(YCS)和崖山文件系统(YFS)，前者负责节点管理、资源调度、高可用和故障仲裁，后者提供高性能并行文件服务和多副本存储。利用全局资源目录(GRC)、全局缓存服务(GCS)和全局锁服务(GLS)实现多实例间资源协调与强一致性访问。YFS支持磁盘组和故障组管理，多副本冗余提高数据可靠性。该基础设施确保多实例数据库环境中高性能、高一致性、高可用性和扩展性。</p><p>总结与建议</p><p>根据应用场景选择合适的部署架构，充分发挥单机、分布式及共享集群的性能优势。</p><p>合理应用存储结构，区分热数据与冷数据，结合HEAP、MCOL及SCOL存储优化读写性能。</p><p>利用MVCC与事务隔离级别，确保数据一致性的同时提升并发处理能力。</p><p>关注SQL语句优化，收集准确统计信息，使用Hint指令辅助优化计划。</p><p>设计合理索引结构，选择合适索引类型和扫描策略，降低I/O开销。</p><p>扩展和利用PL引擎，实现业务逻辑本地化处理，减少网络通信开销。</p><p>部署主备复制与自动选主机制，保障业务的高可用性和数据安全。</p><p>强化安全管理，采取角色、标签访问控制和加密措施，满足合规要求。</p><p>合理配置共享集群资源，提升多实例环境下的协同处理效率。</p><p>定期进行备份与恢复演练，确保数据持久性和业务连续性。</p><p>持续关注数据库性能监控和诊断，快速响应和处理异常事件。</p><p>结论</p><p>随着企业数据量的爆炸式增长与业务对数据实时处理能力的需求日益提升，数据库系统在性能、扩展性和高可用性方面的要求也愈加严苛。YashanDB通过集成多种先进技术和功能特性，包括灵活的部署架构、多样化存储引擎、优化的事务和并发控制、强大的SQL执行引擎、高并发分布式处理能力及完善的安全保障机制，为行业客户提供了高效稳定的数据处理解决方案。未来，面对大数据与云计算的发展趋势，数据库的性能优化与业务适配能力将成为核心竞争力。持续深入掌握并应用YashanDB的技术优势，将助力数据库开发和运维团队有效应对复杂业务挑战，实现数据资源的最大价值化。</p>]]></description></item><item>    <title><![CDATA[YashanDB的版本更新与用户体验改进 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493693</link>    <guid>https://segmentfault.com/a/1190000047493693</guid>    <pubDate>2025-12-22 17:20:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着数据库技术的日益成熟和应用场景的多样化，如何在保证数据一致性和高可用性的同时，提高查询速度和系统稳定性，成为数据库产品竞争的核心指标。对于企业而言，数据库的性能、可靠性和可维护性直接影响业务的连续性和发展速度。YashanDB作为一款功能全面且技术先进的数据库管理系统，通过持续的版本更新和架构优化，不断解决系统瓶颈问题，提升用户体验。</p><p>版本更新的核心技术升级</p><p>多形态部署架构的优化与拓展</p><p>YashanDB支持单机部署、分布式集群部署和共享集群部署三种部署形态，满足不同业务规模和数据访问需求。最新版本在单机部署模式中进一步简化主备复制机制，提升主备同步性能，确保主库故障时快速切换。分布式部署通过更精细的节点管理实现了元数据节点、协调节点与数据节点的高效协同，提升了系统的线性扩展能力。共享集群部署引入聚合内存技术，完善全局资源管理，实现多实例数据访问的强一致性，保证集群多活运行的稳定性和高性能。</p><p>存储引擎和数据结构的增强</p><p>针对在线事务处理(OLTP)及在线分析处理(OLAP)场景，YashanDB支持HEAP、BTREE、MCOL和SCOL四种存储结构。新版优化了MCOL的可变列式存储架构，支持更多维度的原地更新(in-place update)，避免了存储空间膨胀和频繁垃圾回收。SCOL稳态列式存储引入切片式组织和高级压缩技术，提升了海量数据访问的查询速度。存储对象类型的创新，如TAC表与LSC表细分冷热数据存储，大幅提高了系统的在线分析处理能力。</p><p>SQL引擎与优化器的性能提升</p><p>YashanDB数据库的SQL引擎采用基于成本的优化器(CBO)，集成了静态重写与动态重写机制，能够生成高效的执行计划。最新版本增强了统计信息收集与并行统计能力，确保优化器拥有准确的代价模型。支持更丰富的内置函数库和向量化计算框架，利用SIMD技术实现批量数据处理并行，提高CPU的利用率和查询吞吐量。并行执行与分布式SQL计划调度使得复杂查询在大规模集群环境下依然保持高效，满足海量数据分析需求。</p><p>事务管理与高可用性的保障机制</p><p>事务引擎支持ACID特性和多版本并发控制(MVCC)，保障数据的一致性和隔离性。新版本强化了读已提交和可串行化隔离级别的写冲突检测与死锁处理，提升并发事务的稳定性。主备复制架构优化了redo日志传输与回放机制，支持同步和异步复制模式，并引入Quorum机制提升数据保护程度。自动选主功能基于Raft算法，实现了主备故障的快速感知和自动切换，减少了人工干预需求，提高了系统可用率。</p><p>安全管理与数据保护功能完善</p><p>安全体系升级涵盖用户管理、身份认证、访问控制、加密和审计。引入基于角色的访问控制(RBAC)和基于标签的访问控制(LBAC)，实现细粒度权限分配和行级访问控制。支持表空间和表级透明数据加密(TDE)，结合强加密算法保护存储数据安全。网络通信采用标准SSL/TLS加密及X.509证书认证机制，实现数据传输的机密性和完整性保障。审计系统能够灵活配置审计策略并支持异步审计，兼顾审计完整性与系统性能。反入侵机制提供IP黑白名单和连接监听功能，防护外部攻击与异常连接。</p><p>用户体验改进措施</p><p>简化部署与运维管理</p><p>通过完善的运维工具集成及配置管理，新版本支持多种快速部署方案，如单机主备配置、一键分布式集群扩容与共享集群管理。增加了配置参数的灵活调整途径，支持实时和重启生效切换。自动化故障诊断架构集成健康监控线程和自动恢复机制，日志诊断和黑匣子数据帮助快速定位故障。集群服务通过网络和磁盘心跳实现实时异常监控，自动投票仲裁重组集群，保证业务不中断。</p><p>提升查询与事务响应速度</p><p>针对查询体验，提供SQL缓存机制避免重复解析，优化索引策略支持多样索引扫描方式(范围扫描、唯一扫描、跳跃扫描等)，并完善HINT提示语法辅助调整执行计划。加强内存管理结构，优化数据缓存和有界加速缓存，降低I/O延迟。事务并行度调优与写日志线程优化，保证高并发环境下事务吞吐和一致性。</p><p>丰富客户端支持与开发扩展性</p><p>保障各种主流编程语言的客户端驱动稳定支持，包括JDBC、Python DB API、ADO.NET、C和ODBC，提供统一接口调用体验。增强PL引擎编程能力，支持过程、函数、触发器、高级包等多种PL对象的灵活定义与调用，集成动态与静态SQL，满足复杂业务逻辑。支持外置UDF(C/C++及Java)，通过沙箱隔离保障系统稳定性，兼顾安全与性能。</p><p>灵活数据分区与存储管理</p><p>引入多种分区策略支持(范围分区、哈希分区、列表分区、间隔分区及复合分区)，用户可根据业务特点自由组合，减少无效数据扫描，加速定位数据。支持本地分区索引和全局分区索引关联分区管理。存储空间管理上优化段页式和对象式管理接口，提高空间分配效率。完善逻辑与物理存储分离机制，支持云存储和分布式数据空间管理，拓展混合云架构应用。</p><p>技术建议</p><p>基于业务场景合理选择部署形态。中小型应用优先考虑单机主备部署，海量数据分析业务宜采用分布式部署，高可用且多实例数据协同场景选用共享集群。</p><p>根据访问模式选择存储结构。事务性场景采用HEAP行存，实时分析应用首选MCOL列存，需兼顾更新和分析的实时业务采用TAC表，海量稳态数据分析利用LSC表。</p><p>定期维护统计信息，确保优化器准确估算代价，提供最佳执行计划，必要时使用HINT进行精细调优。</p><p>设计合适的索引策略，结合查询特征制定唯一索引、函数索引及多列组合索引，避免过多或滥用索引带来的维护成本。</p><p>合理设定事务隔离级别与锁策略，避免死锁风险，适当利用写一致性与保存点提高事务并发能力。</p><p>启用安全加密与访问控制策略，根据企业合规需求配置数据加密及审计，使用角色和标签实现合理权限划分，保障数据隐私与安全。</p><p>利用自动选主和自动故障检测减少人工干预，实现高可用环境下的快速切换与容灾恢复。</p><p>充分利用PL语言和外置函数增强业务逻辑处理能力，减少网络延迟和客户端复杂度。</p><p>结论</p><p>YashanDB通过持续的技术迭代与版本升级，完善了从存储底层到SQL执行引擎，再到安全管理和高可用架构的整体性能和稳定性。随着数据规模和业务复杂度的不断攀升，数据库系统对查询效率和系统可用性的要求日益严格，YashanDB的多部署形态、多存储结构和高效事务管理架构，为应对未来大数据挑战奠定坚实基础。展望未来，随着云计算与人工智能技术的融合，YashanDB将不断优化并融合更多智能化管理与自动化调优功能，为行业用户提供强大的数据服务支撑。持续关注数据库核心技术提升，是保障业务创新与高效运营的关键。</p>]]></description></item><item>    <title><![CDATA[YashanDB的定制化开发平台与应用生态 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493698</link>    <guid>https://segmentfault.com/a/1190000047493698</guid>    <pubDate>2025-12-22 17:19:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>数据库技术作为支撑现代数据密集型应用的核心技术，面临性能瓶颈、数据一致性保障、灵活扩展性及高可用性等挑战。随着海量数据的增长和多样化业务需求的出现，传统数据库架构难以满足高并发、高性能、实时分析和事务处理等多重需求。YashanDB作为一款新一代适配多类业务场景的数据库产品，通过其定制化开发平台及丰富的应用生态，提供了灵活的部署形态、先进的存储引擎、多层次内存和进程架构、高效的SQL执行引擎和完善的安全体系，全面满足用户的复杂业务需求。本文将系统分析YashanDB的核心技术架构、存储机制、执行引擎、事务控制及生态组件，旨在为数据库开发人员及DBA提供技术指导，推动其对YashanDB潜力的深刻理解。</p><p>多样化部署架构与实例管理</p><p>YashanDB支持单机主备、分布式集群和共享集群三种部署形态，满足不同业务场景对高可用性、性能和可扩展性的需求。在单机主备部署中，通过主库和备库的redo日志同步确保数据一致性，并支持多样的启动关闭模式保障系统的灵活运维。分布式部署采用Shared-Nothing架构，含MN组、CN组及DN组，分别负责元数据管理、查询协调和数据存储，实现线性扩展与高处理能力。共享集群依托共享存储和聚合内存技术，利用全局资源管理实现多实例间并发强一致性访问，满足多活高端交易需求。实例体系区分数据库实体及运行时数据库实例，实例采用多线程架构和分层内存管理，实现资源隔离与高并发支持。部署多样性为用户提供了按需定制的灵活基础。</p><p>多存储结构的存储引擎及对象模型</p><p>YashanDB基于HEAP(堆式存储)、BTREE(B树存储)、MCOL(可变列式存储)及SCOL(稳态列式存储)设计多样化存储引擎，用以满足事务型、高性能分析及混合场景。HEAP存储适合行存表，支持高效随机写入和行内更新;BTREE索引保障索引数据有序访问，提高查询速度。MCOL存储采用段页式管理，实现列数据的原地更新和字典编码，改善实时分析性能;SCOL则面向海量冷数据，通过切片式对象管理及压缩编码技术提升存储利用率及查询效率。表空间采用段页式或对象式管理，支持复杂表结构、索引和外部表等模式。此多样化的存储设计兼顾了读写性能与存储效率，满足业务多元需求。</p><p>高效的SQL引擎与执行优化</p><p>YashanDB的SQL引擎包括解析、验证、优化及执行四大阶段，优化器采用基于成本的CBO模式，基于动态统计信息制定最优执行计划。支持静态及动态语句重写，自动选择最优访问路径及连接顺序。SQL执行引擎兼容传统火山模型与向量化计算，后者利用SIMD技术批处理数据，显著提高计算效率。分布式环境中采用MPP架构，协调节点负责语句解析与计划分发，数据节点并行执行，提高查询吞吐量。并行度调节和HINT提示赋予用户灵活调控能力。此执行体系实现了高效稳定且可扩展的SQL处理能力。</p><p>事务控制与并发一致性保障</p><p>YashanDB支持严格的ACID事务管理和多版本并发控制(MVCC)，通过UNDO数据实现读写不阻塞的快照读，保证读一致性。支持语句级及事务级一致性读隔离。锁机制涵盖表锁与行锁，防止写冲突和死锁，保障并发提交安全。事务隔离支持读已提交及可串行化等级别，解决脏读、不可重复读及幻读问题。在复杂业务下，通过事务保存点与自治事务技术，实现灵活嵌套的事务管理，满足高并发及复杂事务场景需求。</p><p>安全管理与高可用保障体系</p><p>YashanDB具备丰富的安全功能，采用基于角色的访问控制，支持细粒度权限分离及三权分立策略，加强用户身份认证和多级权限监管。提供多层数据加密机制，包括表空间和表级透明加密、备份加密及PL代码加密，保障数据静态和传输安全。支持审计记录和异常监控，强化操作行为追踪。高可用方面，主备复制采用物理Redo日志传输，支持同步及异步复制，结合Quorum机制实现零数据丢失切换。自动选主技术基于Raft协议和仲裁机制，保障故障快速恢复。共享集群服务通过心跳及投票保证集群健壮性，实现多活并发访问与故障自动恢复。</p><p>定制化开发平台与丰富应用生态</p><p>YashanDB内置PL开发语言环境，支持存储过程、自定义函数、触发器及高级包等多种数据库编程模型，实现数据操作与业务逻辑的紧密结合。PL语言提供完整流程控制、异常捕获、绑定参数及动态SQL能力。外置函数支持C/Java扩展，实现复杂运算及定制逻辑。定时任务管理支持任务调度及执行监控，满足后台自动化任务需求。数据库提供多种驱动接口(JDBC、Python、C、ADO.NET、ODBC)，结合开放的插件管理框架，构建完善的开发生态。内嵌的共享集群服务与并行文件系统支持海量存储及高效文件管理，保障底层资源高效利用。完善的故障诊断架构为技术支持提供依据。整体平台为开发者与企业构建了多场景支持、可扩展及安全的应用生态环境。</p><p>技术操作建议</p><p>根据业务需求选择合适的部署架构，单机部署适合小型应用，分布式部署满足大规模数据分析，共享集群则适用于多实例高并发读写场景。</p><p>合理设计存储结构，事务性强场景优先使用行存表HEAP，实时分析建议采用MCOL结构列存表，海量冷数据可采用SCOL格式提高查询性能。</p><p>保证统计信息及时收集，利用优化器的统计数据提升SQL执行计划质量，避免因失准统计导致查询性能下降。</p><p>针对复杂或性能敏感的SQL，加用Hint进行精细优化，必要时利用存储过程及函数封装业务逻辑，减低网络交互及编译开销。</p><p>合理设置事务隔离级别及锁策略，避免不必要的锁等待和死锁，结合多版本并发控制保证事务的一致性与并发性能。</p><p>启用安全机制，包括用户权限管理、数据加密、访问审计及防入侵措施，以符合安全合规要求保障数据安全。</p><p>配置高可用架构中的同步复制及自动选主，确保主库故障时迅速无缝切换，保障业务连续性。</p><p>利用PL开发环境和驱动接口构建定制化应用，实现数据库与业务高度融合，提高开发效率与系统运行性能。</p><p>结论</p><p>随着数据规模和业务场景日益复杂，数据库系统需要具备灵活部署、多样存储、高效执行及全面安全保障能力。YashanDB通过多样化部署架构、完善的存储引擎设计和先进的SQL及事务处理机制，实现了对多类型应用的强力支持。其安全管理和高可用体系保障系统稳定可靠运行，定制化开发平台及开放驱动生态促进业务快速开发与集成。展望未来，随着数据技术的不断演进，进一步提升内存计算能力、智能优化算法和多模数据处理能力将成为数据库核心竞争力。持续深化YashanDB技术栈，融合创新，将推动企业数据资产价值的最大化。</p>]]></description></item><item>    <title><![CDATA[更换DNS服务器多久生效？需要注意哪些事项？ 防火墙后吃泡面 ]]></title>    <link>https://segmentfault.com/a/1190000047493703</link>    <guid>https://segmentfault.com/a/1190000047493703</guid>    <pubDate>2025-12-22 17:18:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在域名管理和网站运营中，更换DNS服务器是一个常见的操作。然而，这一操作并不是即时生效的，而是需要一定的时间来完成，可能会对解析的准确性和及时性造成一定影响。因此，网站管理人员在更换DNS服务器时一定要格外注意。本文国科云针对更换DNS服务器的有关事项做下简单介绍。</p><h2>一、更换DNS服务器是即时生效的吗？</h2><p>更换DNS服务器后，生效时间通常不是即时的，而是需要几分钟到48小时不等。这一时间范围之所以如此宽泛，是因为生效时间受到多种因素的影响，包括TTL（TimetoLive）值、DNS缓存、DNS传播以及网络状况等。</p><h2>二、影响DNS服务器更换生效时间的因素有哪些？</h2><p><strong>TTL值：</strong>TTL是DNS记录中的一个重要参数，它决定了DNS记录在缓存中的存活时间。TTL值设置得越短，DNS记录变更后生效得越快。相反，如果TTL值设置得较长，那么即使修改了DNS记录，缓存中的旧记录也会继续存在一段时间，导致新记录无法立即生效。</p><p><strong>DNS缓存：</strong>DNS缓存是提高解析速度和减轻服务器负载的一种机制。DNS服务器和客户端（如用户的设备）都会缓存DNS记录。当DNS记录修改后，需要等待这些缓存过期或手动刷新缓存，新的解析结果才能生效。本地DNS缓存的生效时间通常较短，可能在几分钟内；而ISP（互联网服务提供商）的DNS缓存则可能更长，可能需要几个小时甚至更长时间。</p><p><strong>DNS传播：</strong>DNS修改需要在全球范围内传播，这个过程称为DNS传播。DNS传播的时间取决于网络的延迟和DNS服务器的更新速度。一般来说，DNS传播的时间可能在几分钟到几小时之间。不同地理位置的用户可能会在不同的时间内看到DNS修改生效。</p><p><strong>网络状况：</strong>实际网络环境也会对DNS修改后的生效时间产生影响。例如，在高峰时段或网络拥堵严重的地区，数据包传输可能会出现延迟甚至丢失的情况，从而延长了DNS更新所需的时间。</p><h2>三、如何加快DNS服务器更换的生效时间？</h2><p><strong>缩短TTL值：</strong>在更换DNS服务器之前，可以将TTL值设置为较短的时间（如几分钟或几小时），以便更快地使旧的解析结果过期。然而，需要注意的是，频繁调整TTL值可能会影响域名解析的稳定性。</p><p><strong>清除DNS缓存：</strong>在更换DNS服务器后，可以尝试清除本地设备上的DNS缓存，以强制获取最新的DNS记录。对于ISP的DNS缓存，则可能需要等待其自然过期或联系ISP进行刷新。</p><p><strong>使用高性能DNS服务器：</strong>选择高性能、高稳定性的DNS服务器可以加快<a href="https://link.segmentfault.com/?enc=9ZJPL9il%2FONolhlICq127g%3D%3D.cAldDowtbwvawf7pHkWQ68CzIExQitrShIsLYiwmmkhlhbrG30L6nR9dgM9P9tXW" rel="nofollow" target="_blank">DNS解析</a>速度，从而在一定程度上缩短生效时间。</p><p><strong>监控生效情况：</strong>在更换DNS服务器后，可以使用工具或命令来监控DNS解析的生效情况。例如，可以使用nslookup或dig命令来查询域名的DNS记录，观察其是否已更新为新的DNS服务器。</p><h2>四、更换DNS服务器需要注意哪些事项？</h2><p><strong>避免频繁更换DNS服务器：</strong>频繁更换DNS服务器可能会导致域名解析不稳定，影响网站的正常访问。因此，在更换DNS服务器之前应充分考虑其必要性和可行性。</p><p><strong>备份原DNS记录：</strong>在更换DNS服务器之前，建议备份原DNS记录，以便在出现问题时可以快速恢复。</p><p><strong>关注TTL值设置：</strong>在设置TTL值时，应平衡解析速度和稳定性之间的关系。过短的TTL值可能会增加DNS查询的频率和负载，而过长的TTL值则可能导致DNS修改生效时间过长。</p>]]></description></item><item>    <title><![CDATA[YashanDB的多租户架构及其业务应用探讨 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493706</link>    <guid>https://segmentfault.com/a/1190000047493706</guid>    <pubDate>2025-12-22 17:18:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在当前数据库技术领域，随着企业业务的多样化和云计算的发展，多租户架构已成为数据库系统设计的重要趋势。多租户环境下，数据库需高效支持多个租户的数据隔离与资源共享，同时解决性能瓶颈和数据一致性问题。YashanDB作为一款面向大规模业务场景的数据库产品，其多租户架构融合了先进的存储、计算与管理技术，旨在满足复杂业务对性能、高可用及安全的需求。本文将深入分析YashanDB的多租户实现架构，详细剖析技术原理，并探讨其在实际业务中的应用价值，面向数据库架构师、系统开发和运维工程师提供技术指导和实践参考。</p><p>YashanDB多租户架构核心技术解析</p><p>多种部署形态支持与多租户资源隔离</p><p>YashanDB支持单机(主备)、分布式集群及共享集群三种部署模式，以适配不同的业务规模和安全隔离需求。单机部署适用于标准多租户场景，通过主备复制保障数据同步和容错。分布式部署通过MN、CN和DN节点分工实现数据水平拆分和计算分布，天然支持租户数据和计算资源的物理隔离，满足海量数据分析和大吞吐量的要求。共享集群部署依赖共享存储和聚合内存技术，实现多实例并发读写单一数据库，为多租户提供同一数据库多活访问能力，确保强一致性和高性能。通过不同部署形态结合角色管理和访问控制，YashanDB实现了多租户在逻辑与物理层面上的资源隔离和安全保障。</p><p>存储引擎与表空间管理实现数据隔离与性能平衡</p><p>YashanDB采用多样的存储结构(HEAP、BTREE、MCOL和SCOL)支持多样业务的数据访问特性。行式HEAP存储面向高频OLTP事务处理，列式MCOL和SCOL存储支持HTAP和OLAP场景，特别是通过活跃切片与稳态切片划分热数据和冷数据，实现写优化与查询性能平衡。逻辑上，数据库通过表空间为数据、索引和大对象提供独立的存储容器，支持数据的物理多租户隔离。每个表空间拥有独立的数据文件，空间管理机制确保数据库对象之间的存储互不干扰。针对多租户，各租户可以在数据库实例内分配独立或共享表空间，结合分区技术进一步细化隔离与访问路径优化。</p><p>多版本并发控制与事务隔离保证数据一致性</p><p>YashanDB在多租户环境中通过MVCC实现事务的读写并发控制。系统基于事务ID与系统变更号(SCN)判定数据版本可见性，保证查询语句读取的一致性快照，避免读写阻塞。同时，支持读已提交和可串行化两级事务隔离，适应不同业务对并发和一致性的权衡需求。写冲突通过行锁顺序执行及死锁检测机制保证资源竞争的安全和高效。事务模块支持自治事务及保存点(SAVEPOINT)，为复杂租户业务逻辑需求提供灵活事务处理能力，确保在多租户并发环境中数据的原子性和完整性。</p><p>SQL引擎与分布式执行，提升多租户多维度性能</p><p>SQL引擎包括解析、校验、优化和执行四阶段，采用基于成本的优化器(CBO)生成最优执行计划。执行过程中利用向量化计算引擎提升批量数据处理效率。多租户资源分配下，多实例协调节点(CN)进行分布式计划制定，将任务并行下发至数据节点(DN)，实现跨租户的弹性伸缩和负载均衡。内部互联总线(DIN/CIN)保证节点间数据交换的可靠与高吞吐。支持丰富的SQL语法及HINT提示，优化器能够基于最新统计信息动态调整执行策略，满足多租户在查询性能和资源有限时的差异化需求。</p><p>安全机制保障多租户数据隔离与访问合规</p><p>YashanDB采用基于角色的访问控制(RBAC)结合行级安全标签(LBAC)实现细粒度权限管理。基于角色管理的权限授权确保租户间访问权限隔离，基于标签的安全策略进一步控制租户访问具体数据行权限。支持多种身份认证方式包括数据库认证、操作系统认证以及SSL/TLS加密保障数据在传输过程中的安全性。多租户架构中，防止恶意行为由连接监听、IP黑白名单、保留连接机制等多维度控制实现。审计功能记录用户操作行为，为多租户合规提供数据支撑。整体安全架构保障数据库服务与业务操作均在权限可控范围内。</p><p>高可用架构与自动选主机制保障多租户业务连续性</p><p>YashanDB通过主备复制架构实现主库数据实时复制至备库，保证故障情况下数据不丢失。支持同步、异步复制及多种保护模式(最大性能、最大可用、最大保护)。支持级联备库实现跨地域容灾。备库通过redo日志回放与主库数据保持一致。主备自动选主采用Raft算法实现主库选举和故障自动切换，保证多租户环境中业务的高可用性和快速恢复。共享集群部署中，YCS服务通过网络心跳和磁盘心跳实现集群实例的监控和自动重组，有效避免单点故障带来的服务中断风险。</p><p>多租户环境下YashanDB的业务应用价值</p><p>弹性扩展支持多业务共享平台</p><p>基于分布式集群和共享集群两种形态，YashanDB能够支持海量租户的并发访问和数据存储需求，通过动态新增节点、调整资源实现弹性扩展，满足业务快速增长需求。合理的分区和表空间策略支持不同租户的数据物理隔离与管理，实现资源按需分配与优化，提升整体硬件利用率及租户体验。</p><p>安全隔离保障业务数据安全合规</p><p>通过权限管理与标签控制结合多级认证机制，YashanDB保障多租户环境下严格的数据访问隔离，防止横向越权访问。同时审计模块全方位记录数据库操作，对于金融、政务等行业合规审计要求提供技术支撑，确保业务数据安全可信。</p><p>高性能保障业务实时响应</p><p>借助多版本并发控制、SQL优化器、列式存储及多级缓存等技术，YashanDB在多租户场景下提供优异的读写性能。多租户业务在共享资源环境中，能够获得最优的执行计划和查询效率，加快业务响应时间，提升用户体验。</p><p>高可用机制保障业务连续性</p><p>主备复制、自动选主和共享集群架构下集群重组机制保证了多租户业务在软硬件故障下可快速恢复。结合多副本数据保护和日志回放，确保数据完整性并最大限度减少故障影响时长，支持企业级业务持续运营。</p><p>技术建议</p><p>合理选择部署形态：根据业务规模和安全隔离需求，单机部署适合中小型业务，分布式部署支撑海量数据及线性扩展，共享集群适合高性能多实例并发场景。</p><p>结合使用多种存储结构及表空间策略：针对在线事务和在线分析制定不同表类型，合理设置表空间及分区实现数据隔离与访问优化。</p><p>强化事务隔离与并发控制配置：针对多租户场景，建议选择读已提交或可串行化隔离级别，根据业务特点调优锁粒度及死锁检测应对策略。</p><p>精细设计访问安全策略：利用RBAC和LBAC策略控制租户数据访问权限，结合身份认证、传输加密技术保障端到端安全。</p><p>构建高可用架构及自动选主流程：根据业务可接受的恢复时间和数据容忍度，配置合适的主备复制模式和保护策略，合理使用自动选主机制确保业务连续性。</p><p>定期维护统计信息和执行计划：保持优化器统计的实时性，适时调整索引和执行计划，保障多租户查询性能。</p><p>结论</p><p>本文系统介绍了YashanDB的多租户架构设计，从部署形态、存储引擎、事务机制、SQL执行、安全及高可用技术等多维度解析其核心技术。结合多租户业务需求，提出具体技术建议以提升资源隔离、安全有效性及性能稳定性。通过合理配置和创新架构，YashanDB为多租户场景提供了高性能、高可用和安全可信的数据库服务。建议数据库架构师和开发运维人员深入理解并应用这些技术原理及最佳实践，助力企业多租户业务平台构建和持续优化。</p>]]></description></item><item>    <title><![CDATA[YashanDB的服务器配置与性能调优建议 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493715</link>    <guid>https://segmentfault.com/a/1190000047493715</guid>    <pubDate>2025-12-22 17:17:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库系统的应用中，性能瓶颈、数据一致性保障和高可用性是关键技术挑战。YashanDB作为国内领先的多形态关系数据库解决方案，支持单机主备、分布式集群以及共享集群三种部署架构，针对不同业务场景提供了丰富且高性能的存储结构与计算引擎。本文将针对YashanDB的服务器配置和性能调优策略进行深入技术分析，旨在为开发人员和数据库管理员提供可操作的规划参考，提升系统稳定性和运行效率。</p><p>一、YashanDB部署形态与服务器资源配置</p><p>YashanDB支持单机部署、分布式集群部署和共享集群部署三种形态，实现从简单应用到海量数据处理的业务覆盖。不同部署形态对服务器资源和网络环境有各自的要求。</p><ol><li>单机主备部署</li></ol><p>典型配置包括至少两台服务器，分别部署主实例和备实例。服务器应配备高速网络连接且优先连接同一交换机，保障主备复制链路的低时延和高稳定性。硬件资源配置应兼顾数据库缓存(例如数据缓存区和共享内存区域)与后台任务线程数目。存储设备建议采用高性能SSD，并预留一定的空间以适应redo日志和数据文件的增长。</p><ol start="2"><li>分布式集群部署</li></ol><p>分布式部署中MN组、CN组、DN组功能分工明确，单台服务器可承载多类型进程。为了实现线性扩展性，集群节点应均衡配置CPU核数、内存容量及存储IO能力。尤其是DN节点，因承担主要的数据存储和查询执行任务，应配置高速SSD，优化数据本地读取效率。网络层面建议采用千兆及以上交换机，减少节点间的通信延迟。负载均衡和容错设计是确保集群稳定运行的基础。</p><ol start="3"><li>共享集群部署</li></ol><p>共享集群部署依赖共享存储及YashanDB自研的崖山文件系统(YFS)和集群服务(YCS)，服务器需支持高速访问共享存储设备且保证高可用。硬件配置应满足多实例读写压力，包括大量CPU核心和充足内存以支撑全局缓存和资源管理。网络侧需保障内部通信组件(CIN)的高吞吐和低延迟。共享存储管理的磁盘组规划和故障组划分，对性能和数据安全至关重要。</p><p>二、存储引擎及缓存配置</p><p>YashanDB支持HEAP、BTREE、MCOL和SCOL四种存储结构，针对不同数据特性优化存储和访问模式。</p><ol><li>数据缓存与有界加速缓存配置</li></ol><p>数据缓存需合理分配内存容量，确保工作集数据块尽可能驻留，显著提升查询响应速度。结合数据库负载特点，调整LRU算法参数以适应高并发场景下的冷热数据访问。</p><p>有界加速缓存主要缓存AC对象，可针对特定业务访问模式优化，减少热点数据访问争夺。</p><ol start="2"><li>缓存池与共享内存</li></ol><p>共享内存区域(SGA)缓存SQL执行计划、数据字典和重做日志缓存，数据库启动时参数如SHARE_POOL_SIZE和DB_CACHE_SIZE需协调配置。</p><p>分布式和共享集群部署中，应根据节点数量和线程规模合理预分配内存池，避免内存争用导致的性能损耗。</p><p>三、多线程架构与后台线程调度</p><p>YashanDB采用多线程架构，充分利用多核资源提升并发处理能力。合理配置后台线程数量和优先级，有助于优化CPU资源分配和IO吞吐量。</p><ol><li>写脏块刷新（DBWR）线程配置</li></ol><p>DBWR线程负责脏页写入磁盘。根据磁盘性能和负载，建议调节DBWR_COUNT参数，避免写入堆积或过度频繁触发写入，平衡延迟和资源占用。</p><ol start="2"><li>日志管理线程</li></ol><p>LGWR和RD_SEND负责重做日志的刷写和传输，网络条件好时可适度提高这些线程的并发级别，降低写延迟。备库侧RD_RECV及日志回放线程应充分利用多核，确保同步速度。</p><ol start="3"><li>读写锁与死锁检测</li></ol><p>根据负载特征设置合理的锁等待超时，避免因长事务或死锁导致资源阻塞。同时监控死锁发生情况，针对热点数据设计合理索引和访问范围，降低锁争用。</p><p>四、SQL引擎优化配置</p><p>SQL引擎性能直接影响业务响应速度和系统吞吐量，合理参数和统计信息管理尤为重要。</p><ol><li>优化器统计信息维护</li></ol><p>定期收集统计信息，启用动态样本收集，保证优化器掌握准确数据分布，提升执行计划质量。采样任务应调度平衡，避免与业务高峰期冲突。</p><ol start="2"><li>执行计划缓存及软解析</li></ol><p>合理设置SQL缓存大小，减少硬解析开销。启用共享线程会话模式时，更应关注缓存命中率，避免过多软解析引发系统负载。</p><ol start="3"><li>并行执行与向量化配置</li></ol><p>使用PX并行执行算子并调节并行度参数MAX_PARALLEL_WORKERS，配合向量化计算技术的启用，发挥多核优势提升查询效率。</p><p>五、数据库实例及内存参数调优</p><p>实例启动参数直接影响数据库运行环境和性能表现。应根据服务器硬件条件，调整以下关键参数：</p><p>SGA_*参数：同步配置共享内存区域各缓存池大小，保证充足缓存容量，减少IO。</p><p>SPA_*参数：根据连接量调整私有内存分配，避免内存浪费。</p><p>REDO_BUFFER_SIZE：根据业务写频率调优日志缓存大小，平衡吞吐和时延。</p><p>MAX_WORKERS及MAX_REACTOR_CHANNELS：根据并发连接数调节，分别适配独占和共享线程会话模式，保障线程资源和响应能力。</p><p>六、性能优化具体建议</p><p>根据业务访问特征合理选择存储表类型：OLTP业务优先使用HEAP行存表，OLAP分析业务首选LSC列存表，HTAP场景采用TAC表，实现事务与分析的均衡。</p><p>配置合适的分区策略，并配合本地分区索引使用，优化大表数据访问效率和维护简易性。</p><p>动态调整PCT_FREE参数，避免更新频繁产生大量行迁移，保障读写效率。</p><p>定期执行数据库统计信息的收集与更新，启用动态统计采样提升执行计划准确性。</p><p>监控和扩展实例DBWR线程数，避免脏页刷盘成为系统瓶颈。</p><p>合理配置共享线程会话模式参数，结合业务连接峰值调节MAX_WORKERS，提高线程利用率。</p><p>根据服务器硬件配置，调整数据库实例内存大小，确保缓存能满足热点数据存储，减少磁盘I/O。</p><p>启用并调优SQL并行执行参数，提高大规模查询处理能力。</p><p>利用优化器Hint对关键SQL进行精细调优，规避不合理执行计划。</p><p>对表空间和文件分配做合理规划，避免频繁扩容和碎片化。</p><p>结论</p><p>随着YashanDB应用规模和业务复杂度的提升，服务器配置与性能调优成为保障数据库稳定高效运行的关键。本文结合YashanDB多种部署形态及底层架构，提出针对不同硬件、业务场景的配置建议与性能优化策略。未来，随着数据规模进一步增长和实时分析需求增加，合理利用分布式资源、高效存储结构及多线程执行模型，将是提升系统竞争力的核心。数据库管理员和开发人员应持续关注YashanDB最新特性及最佳实践，完善配置管理和性能监控，推动业务系统稳定发展。</p>]]></description></item><item>    <title><![CDATA[浅谈计算机如何识别图像2 李子轩xuan ]]></title>    <link>https://segmentfault.com/a/1190000047493718</link>    <guid>https://segmentfault.com/a/1190000047493718</guid>    <pubDate>2025-12-22 17:16:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><p>上篇文章<a href="https://segmentfault.com/a/1190000047432165" target="_blank">浅谈计算机如何识别和存储图像</a>中关于计算机如何“看”图片部分，只是粗糙的一笔带过，而关于<strong>彩色图是如何转换成灰度图</strong>，<strong>不规则的手写图片怎么最终转换成统一像素大小</strong>，<strong>切换为统一背景</strong> 这些原理并未了解，通过以下文章来进行探讨。</p><h2>彩色图转换为灰度图</h2><h3>彩色图为什么要转换为灰度图</h3><table><thead><tr><th>对比维度</th><th>彩色图（RGB）</th><th>灰度图（Gray）</th></tr></thead><tbody><tr><td>通道数</td><td>3个通道（R，G，B）</td><td>1个通道</td></tr><tr><td>每个像素数据</td><td>（R，G，B）三个值</td><td>单一灰度值</td></tr><tr><td>是否包含颜色信息</td><td>✅包含</td><td>❌不包含</td></tr><tr><td>是否包含笔画信息</td><td>✅包含</td><td>✅包含</td></tr><tr><td>计算量</td><td>大（卷积计算多）</td><td>小</td></tr><tr><td>训练速度</td><td>慢</td><td>快</td></tr></tbody></table><blockquote>手写图像识别中将彩色图转换为灰度图，是因为颜色信息对字符识别几乎没有贡献，而灰度图能够完整保留笔画结构信息，同时降低计算复杂度、减少噪声并提升模型泛化能力。</blockquote><h3>彩色图怎么转换为灰度图</h3><p>常用公式进行加权平均：</p><blockquote><strong>Gray=0.299×R+0.587×G+0.114×B</strong></blockquote><p>生理学层面上,人眼的感光细胞分为两种：</p><ol><li>视杆细胞：负责弱光环境（暗视觉），无色彩感知，对蓝绿色波段最敏感，但不参与色觉；</li><li><p>视锥细胞：负责强光环境（明视觉），感知色彩，分为三种类型（红敏、绿敏、蓝敏），其数量和分布直接决定色觉敏感度：</p><ul><li>绿敏视锥细胞：数量最多（约 60%），主要分布在视网膜中央的黄斑区（视觉最清晰的区域）；</li><li>红敏视锥细胞：数量次之（约 30%），与绿敏视锥细胞分布重叠；</li><li>蓝敏视锥细胞：数量最少（约 10%），且大多分布在黄斑区外围，中央区域极少。</li></ul><p>数量和核心区域的分布差异，直接导致绿色信号被大脑接收的强度最高，红色次之，蓝色最弱。</p></li></ol><h2>如何解决手写图片尺寸不统一问题</h2><p>下面以一张400 X 300的图片，转换为28 X 28的尺寸为例子讲解</p><blockquote>首先不能直接resize成目标尺寸:直接调整大小会导致 数字被压扁 / 拉长，笔画比例失真，模型学到“奇怪形状”这些问题<br/>而应该从一张大图片中，把“有意义的内容”提取出来，并用统一规则表达</blockquote><p>原图：<img width="400" height="300" referrerpolicy="no-referrer" src="/img/bVdnriY" alt="0.png" title="0.png"/></p><h3>1. 灰度化</h3><ul><li>彩色图片是 RGB 三通道，每个通道的像素值不同。</li><li>对手写数字识别来说，我们只关心 亮度信息，不需要颜色信息。</li><li>灰度化后，每个像素用 0~255 表示亮度（0=黑，255=白）。</li></ul><p>灰度化公式：</p><pre><code>Gray = 0.299R + 0.587G + 0.114B</code></pre><p>图片结果：<img width="400" height="400" referrerpolicy="no-referrer" src="/img/bVdnrji" alt="Grayscale_Heatmap.png" title="Grayscale_Heatmap.png" loading="lazy"/></p><h3>2. 二值化</h3><p>灰度图后的结果类似于这种： 0, 20, 40, 180, 255...</p><ul><li>灰度图每个像素是 0~255，无法直接表示“笔画 vs 背景”</li><li>通过设定阈值（threshold）把灰度图转换为二值图</li></ul><p>二值化的过程：</p><pre><code>binary = np.where(gray &lt; 128, 1, 0)
</code></pre><blockquote>Gray &lt; 128  →  笔画<br/>Gray ≥ 128  →  背景</blockquote><p>图片结果：<img width="400" height="400" referrerpolicy="no-referrer" src="/img/bVdnrjl" alt="Binarized_Heatmap_0_1.png" title="Binarized_Heatmap_0_1.png" loading="lazy"/></p><h3>3.裁减</h3><p>裁减的原因：</p><ul><li>原图大部分是空白区域，数字只在中间</li><li>如果不裁剪，空白太多，模型学到的是“数字在空白里”，浪费训练信息</li></ul><p>那如何裁减呢？</p><pre><code>top, bottom = rows.min(), rows.max()
left, right = cols.min(), cols.max()</code></pre><p>找到所有值为1的像素，取行列最小值和最大值作为裁剪边界，划分一个矩形。</p><p>图片结果：<img width="400" height="400" referrerpolicy="no-referrer" src="/img/bVdnri6" alt="Resized_Heatmap.png" title="Resized_Heatmap.png" loading="lazy"/></p><h3>4.等比例缩放</h3><ul><li>不同手写数字区域大小不同，需要统一尺寸</li><li>规则：最长边 → 20 像素，短边按比例缩放</li></ul><p>为什么不是直接缩放到 28×28？</p><ul><li>直接缩放会拉长或压扁数字</li><li>会导致笔画比例失真，模型学到“奇怪形状”</li></ul><blockquote>以我的图片为例子，裁减后的尺寸为112 X 134，最长边为134，<br/>那么缩放比率为 20/134 = 0.149...<br/>另一边缩放后的长度为 112 X 0.149 = 16.71 ...</blockquote><p>关于如何处理小数问题，一般存在两种处理方法：</p><ol><li><p>直接省略小数点：</p><ul><li>优点：简单，常用</li><li>缺点：可能会略微缩小图像，使数字边缘丢失 1 个像素</li><li>适合神经网络，误差不大</li></ul></li><li><p>四舍五入</p><ul><li>优点：更接近真实比例</li><li>缺点：仍可能出现 +1 / -1 的偏差</li><li>推荐用于追求精确比例的场景</li></ul></li></ol><p>在这里我采用直接取整的方法：等比例所放后的尺寸为 16 X 20</p><pre><code># binary 是 0/1，把逻辑数组变成可以操作的图像对象，需要转回 0~255 才能 resize
digit_img = Image.fromarray((digit_crop * 255).astype(np.uint8))

w, h = digit_img.size

scale = 20.0 / max(w, h)
new_w = int(w * scale)
new_h = int(h * scale)

digit_resized = digit_img.resize((new_w, new_h), Image.BILINEAR)</code></pre><h3>5.居中填充到 28×28</h3><p>当前是16 X 20的小数字图，但我需要的是28 X 28的比例图</p><p>如果再进行 resize，会造成比例被破坏，数字被横向或者纵向拉伸。</p><p>那么选择的方法是居中填充，也就是左边一半右边一半，上下也同理。</p><pre><code>offset_x = (28 - new_w) // 2
offset_y = (28 - new_h) // 2</code></pre><p>展示图片结果：<img width="400" height="400" referrerpolicy="no-referrer" src="/img/bVdnriT" alt="Final_28x28_Heatmap.png" title="Final_28x28_Heatmap.png" loading="lazy"/></p><h2>背景转换</h2><p>在识别手写图片中，图片可能是五颜六色并且有阴影，然后以我们的MINIST数据集所需要的黑底白字为例进行讲解。</p><p>示例图片： <img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrkS" alt="2.png" title="2.png" loading="lazy"/></p><h3>1.转换为灰度图</h3><p>只是关心笔画结构</p><pre><code>R = img_np[:, :, 0]
G = img_np[:, :, 1]
B = img_np[:, :, 2]

gray = (0.299 * R + 0.587 * G + 0.114 * B).astype(np.uint8)</code></pre><p>图片结果：<br/><img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrl1" alt="2.2.png" title="2.2.png" loading="lazy"/></p><h3>2.背景估计</h3><ul><li>核心思想：把笔画当作“噪声”，用周围像素的平均值估计背景</li><li>结果：得到一张和原图一样大小的背景图</li></ul><pre><code>h, w = gray.shape
// “站在这个像素上，我往上下左右各看 15 个像素，把这一大片的平均亮度当作背景。”
window = 31
pad = window // 2

# 边缘填充
padded = np.pad(gray, pad, mode='edge')
background = np.zeros_like(gray)

for i in range(h):
    for j in range(w):
        region = padded[i:i+window, j:j+window]
        background[i, j] = np.mean(region)

background = background.astype(np.uint8)
</code></pre><p>图片结果:<br/><img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrnG" alt="2.3.png" title="2.3.png" loading="lazy"/></p><h3>3.去阴影</h3><p>如果不去阴影，后续二值化操作（把像素分成黑白）就会出问题：</p><ul><li>阴影深的地方 → 背景误判成笔画</li><li>阴影浅的地方 → 笔画可能被吃掉</li></ul><p>方法：</p><ol><li><p>减去背景</p><ul><li>保留笔画差异</li><li>抹掉阴影和纸张亮度变化</li></ul></li><li><p>拉伸对比度</p><ul><li>减掉背景后，笔画亮度可能还是很接近背景（灰灰的）</li><li>如果直接二值化，阈值不好选</li><li>拉伸对比度 = 把最暗的设为 0，最亮的设为 255，把灰度线性拉开</li></ul></li></ol><pre><code>shadow_removed = np.abs(gray.astype(int) - background.astype(int))

# 拉伸对比度
min_val = shadow_removed.min()
max_val = shadow_removed.max()
shadow_removed = ((shadow_removed - min_val) / (max_val - min_val) * 255).astype(np.uint8)</code></pre><blockquote>数学公式意义：<br/>假设 shadow_removed 的最小值是 10，最大值是 60<br/>用公式：\( new=【（old−10）/（60−10）】×255 \)<br/>结果：10 → 0 ，60 → 255 ， 所有结果均在0-255之间<br/>就像把灰灰的墨水“拉黑拉亮”，让笔画更明显</blockquote><p>图片结果：<br/><img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrpC" alt="2.4.png" title="2.4.png" loading="lazy"/></p><h3>4.二值化</h3><p>像素值：要么0（纯黑），要么255（纯白）</p><blockquote>二值化就是为每个像素设定一个"分数线"，分数高于分数线就变成白色（文字），低于分数线就变成黑色（背景），从而把有256种灰度的图片变成只有纯黑纯白的图片。</blockquote><pre><code>local_window = 15
pad = local_window // 2
padded = np.pad(shadow_removed, pad, mode='edge')

for i in range(h):
    for j in range(w):
        region = padded[i:i+local_window, j:j+local_window]
        threshold = np.mean(region) - 5  # C = 5
        binary[i, j] = 255 if shadow_removed[i, j] &gt; threshold else 0</code></pre><p>图片结果：<img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrrs" alt="2.5.png" title="2.5.png" loading="lazy"/></p><h3>5. 统一为黑底白字</h3><p>目标为黑底白字</p><pre><code>binary = 255 - binary
</code></pre><p>图片结果：<img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrrw" alt="2.6.png" title="2.6.png" loading="lazy"/></p><h3>6.简单去噪</h3><blockquote>核心思想：如果一个白点（文字）周围几乎没邻居，那它很可能是噪声。</blockquote><pre><code>for i in range(1, h-1):          # 遍历每行（跳过最外圈）
    for j in range(1, w-1):      # 遍历每列（跳过最外圈）
        if binary[i, j] == 255:  # 只检查白点（可能是文字的点）
            
            # 获取3×3邻域（包括自己）
            neighbors = binary[i-1:i+2, j-1:j+2]
            # 相当于9个像素：
            # [i-1,j-1] [i-1,j] [i-1,j+1]
            # [i,j-1]   [i,j]   [i,j+1]
            # [i+1,j-1] [i+1,j] [i+1,j+1]
            
            # 计算邻居中有多少白点（==255）
            white_count = np.sum(neighbors == 255)
            
            # 如果白点总数 &lt; 3（包括自己在内）
            if white_count &lt; 3:
                clean[i, j] = 0  # 把这个点设为黑（删除）</code></pre><p>图片结果：<br/><img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnrrC" alt="final_binary.png" title="final_binary.png" loading="lazy"/></p><h2>结语</h2><p>关于背景如何进行转换，规则的手写图片怎么最终转换成统一像素大小，以及彩色图如何转换为灰度图的了解先到这里，通过简单的了解，对原理有了一定的认识。如有错误，请指出！</p>]]></description></item><item>    <title><![CDATA[YashanDB的负载均衡与性能优化技巧一览 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493729</link>    <guid>https://segmentfault.com/a/1190000047493729</guid>    <pubDate>2025-12-22 17:15:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术领域，性能瓶颈和负载不均衡是影响系统可用性和用户体验的关键挑战。合理的负载均衡策略不仅确保系统资源充分利用，还能增强系统的可扩展性和稳定性。YashanDB作为一款支持单机、分布式及共享集群多种部署形态的数据库系统，拥有丰富的架构特色和优化机制。本指南主要针对YashanDB的负载均衡技术和性能优化提供系统化的技术解析，旨在帮助数据库管理员和开发者深入理解并高效运用该数据库的相关功能。</p><p>YashanDB的部署架构与负载均衡基础</p><p>YashanDB支持三种主要部署架构：单机主备部署、分布式集群部署和共享集群部署。每种部署形式具有各自的负载均衡策略和适用场景。</p><p>单机部署： 典型的主备复制模式，负载均衡主要通过将读请求分散到备实例实现，写请求集中于主实例，满足大多数业务的高可用需求。</p><p>分布式部署： 通过MN(管理节点)、CN(协调节点)、DN(数据节点)组的分离，实现多维度负载分摊。CN节点进行SQL请求接收和执行计划分发，DN节点负责数据存储和执行任务，天然支持线性扩展与分布式负载均衡。</p><p>共享集群部署： 基于共享存储和崖山集群内核(YCK)，采用Shared-Disk架构，多实例多活。全局缓存服务(GCS)和资源目录(GRC)协调多实例间数据共享与访问，强保证读写并发的均衡。</p><p>理解各部署架构的负载均衡特性是设计性能优化策略的重要基础。</p><p>核心负载均衡技术解析</p><ol><li>分布式SQL执行负载均衡</li></ol><p>分布式部署中，YashanDB采用MPP架构实现SQL请求的并行和负载均衡：</p><p>协调节点(CN)任务分发：CN节点根据系统的统计信息和数据分布，生成分布式执行计划，合理拆分SQL查询为多个并行执行阶段(stage)，并下发至分布式数据节点(DN)执行，实现任务的水平切分。</p><p>数据节点(DN)负载自治：DN节点依照收到的执行计划并行执行任务，支持节点内垂直和水平数据切分，提升CPU多核利用率及存储IO效率。</p><p>数据交换与网络通信负载：内部互联总线(DIN)采用多路复用和连接池技术，隔离数据与控制消息，减少通信阻塞，提升数据流动效率，实现跨节点数据的高效均衡分发。</p><p>该体系充分利用节点多样化协同，实现跨节点及节点内负载的动态分配，最大化性能。</p><ol start="2"><li>共享集群实例负载均衡</li></ol><p>共享集群部署形态依赖于崖山集群内核(YCK)和崖山文件系统(YFS)实现多实例的多活读写能力：</p><p>全局缓存服务(GCS)：保证多个实例间数据页缓存的有序访问，协调访问权限，实现缓存命中增强，避免热点数据访问瓶颈。</p><p>全局资源目录(GRC)：分布式管理全局资源的状态，提供资源请求排队机制，避免资源争抢导致的阻塞。</p><p>实例级负载均衡：基于YCS集群服务监控实例健康状况，根据实例负载和资源利用率智能调度客户端连接，实现请求的动态分配与迁移。</p><p>该机制保障多实例系统高并发场景下的读写响应均衡，推动系统的扩展性。</p><ol start="3"><li>主备复制与切换中的负载优化</li></ol><p>主备部署通过redo日志复制机制实现高可用，同步与异步复制模式对性能和一致性产生不同影响：</p><p>同步复制下的负载均衡：主库需等待redo日志同步至同步备库后提交事务，压力集中在主库至同步备库的数据传输链路。YashanDB支持设置Quorum参数，平衡安全性要求与性能，动态调整同步备库数量，优化主库提交等待。</p><p>异步复制下的负载均衡：主库提交事务不等待备库确认，提高事务吞吐，但存在数据丢失风险。异步复制场景下，可通过合理配置redo日志缓存与传输线程参数，提升网络传输和备库应用效率，实现负载均衡。</p><p>级联备库架构：减轻主库负载压力，将redo日志流量分散至分层备库，实现远程灾备场景下的负载散列。</p><p>合理调整主备复制参数，提升复制通道整体吞吐，防止主库成为性能瓶颈。</p><p>性能优化技术详解</p><ol><li>存储结构与索引优化</li></ol><p>基于业务场景，选择合理的存储结构与索引策略是提升性能的关键：</p><p>存储结构选择：YashanDB支持HEAP(行存)、MCOL和SCOL(列存)存储结构。事务型业务建议使用HEAP存储，实时分析业务适合MCOL，海量分析推荐SCOL。MCOL支持原地更新提高写并发，SCOL支持列存压缩及稀疏索引，优化读取性能。</p><p>BTree索引策略：采纳前缀索引原则，合理选取索引列，减少回表I/O。启用唯一索引保证数据完整性。函数索引可加速复杂表达式过滤。合理设置PCT FREE预留空间，减少行迁移导致的性能损耗。</p><p>索引维护：定期重建不可用索引，采集准确统计信息支持优化器生成高效执行计划。</p><p>精细化存储策略与索引管理，减少存储层读写压力，释放系统资源。</p><ol start="2"><li>SQL执行优化</li></ol><p>YashanDB通过多阶段SQL执行与优化构建高效查询机制：</p><p>向量化计算：依托SIMD技术实现批量数据处理，减少CPU周期，提高计算吞吐。</p><p>优化器成本模型：基于丰富统计信息，采用基于成本的优化(CBO)，精准评估多种执行计划，选择最优路径执行，提高多表连接及复杂查询效率。</p><p>并行度调优：通过动态调整并行度参数和平衡各节点执行任务量，充分利用多核处理能力，避免单点过载。</p><p>利用HINT与执行计划分析：定向引导优化器选择执行路径，适用于特殊业务场景的调优需求。</p><p>通过SQL层面优化，提升查询和数据操作响应速度。</p><ol start="3"><li>内存管理与线程调度优化</li></ol><p>高效的内存利用与线程调度促进系统整体性能提升：</p><p>共享内存池优化：合理配置共享内存大小，提升SQL缓存、数据缓存、数据字典缓存命中率，减少重复解析和磁盘访问。</p><p>数据缓存策略：依据业务访问特点调整LRU策略和热点块回收机制，保证缓存空间持续有效利用。</p><p>多线程模型调优：根据连接数和请求并发调整工作线程池大小(MAX_WORKERS)、监听线程和异步线程数量，避免线程争用和资源浪费。</p><p>并行执行线程配置：提升数据节点并行度及执行效率，减少查询响应时间。</p><p>合理设置内存参数和线程数量，充分发挥硬件资源优势。</p><ol start="4"><li>负载均衡的网络机制</li></ol><p>网络通信作为数据库节点间及客户端访问的关键环节，是负载均衡与性能调优的重要因素：</p><p>客户端连接模式：支持共享线程与独占线程模式。共享线程降低线程资源占用，适合高连接数场景;独占线程保证单连接响应速度，适合低连接数需求。</p><p>内部互联总线(Internal Communication Service，ICS)：采用多通道资源隔离，保障控制消息和数据消息独立传输，避免网络阻塞。</p><p>连接池管理：多路复用和池化技术减少连接创建开销，提升并发会话的处理能力和网络负载均衡。</p><p>优化网络通讯结构和策略，降低延时，提升整体吞吐量。</p><p>实践建议汇总</p><p>根据业务场景确认合理部署架构，单机部署适用于中小规模，分布式部署适合高并发分析，共享集群满足高端多实例并发。</p><p>启用统计信息自动收集，精准支撑优化器执行计划，提高SQL执行效率。</p><p>选择合适的存储结构(HEAP/MCOL/SCOL)与索引(主键索引、函数索引)，配合合理的PCT FREE值，最大化存储和查询性能。</p><p>灵活调整SQL执行并行度，启用向量化计算，定期基于执行计划及业务负载调优HINT。</p><p>配置共享内存池大小和缓存策略，合理设置线程池规模，避免线程竞态和内存不足。</p><p>基于网络场景选择共享线程或独占线程连接模式，确保连接池配置满足并发需求。</p><p>合理配置主备复制参数，平衡同步备库数量和复制模式，实现写请求负载和数据安全的最优平衡。</p><p>利用YCS和YFS的共享集群内核技术，实现实例间读写协作和缓存共享，提升集群水平扩展能力。</p><p>定时对索引和统计信息进行维护，确保优化器有准确数据。</p><p>结合业务访问特征，合理设计分区策略和分片策略，降低全表扫描压力。</p><p>结论</p><p>YashanDB提供丰富的部署架构和底层技术支持，通过多层次的负载均衡机制实现系统在不同业务场景下的高性能表现。深入理解分布式执行计划、共享集群内核、主备复制机制及网络通信策略，是实现高效数据库运行的基础。结合存储结构优化、索引管理、SQL执行调优与内存线程配置，管理员可精准调整系统资源配置，满足业务对性能和可用性的需求。建议基于本文所述原理和最佳实践，结合实际项目特征进行持续优化，充分发挥YashanDB在负载均衡和性能优化方面的优势。</p>]]></description></item><item>    <title><![CDATA[Apache Doris 4.0.2 版本正式发布 SelectDB技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047493734</link>    <guid>https://segmentfault.com/a/1190000047493734</guid>    <pubDate>2025-12-22 17:15:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>亲爱的社区小伙伴们，<strong>Apache Doris 4.0.2 版本已正式发布。</strong>此版本新增了在 AI &amp; Search、函数、物化视图、Lakehouse 等方面的功能，并同步进行了多项优化改进及问题修复，欢迎下载体验！</p><ul><li>GitHub 下载：<a href="https://link.segmentfault.com/?enc=FVtP843Puzb1JwxxQDfMmw%3D%3D.%2F5Hhwj423u0Pxo0OQMaLtJIi%2BFtlFuM%2BKX%2FNbUQvhdSlUdE6n2r1RVppT1G8K1MR" rel="nofollow" target="_blank">https://github.com/apache/doris/releases</a></li><li>官网下载：<a href="https://link.segmentfault.com/?enc=v%2FxUPZ%2FjxpLR%2FR%2BEDzg5tA%3D%3D.22D1%2BAdjIwpEUj%2FAz23vIhyuTL%2Bejb%2BaKx35Ky3zo%2Fy0y9%2BUtZha1FfZZHFnLD%2FP" rel="nofollow" target="_blank">https://doris.apache.org/download</a></li></ul><h2>新增功能</h2><h3>AI &amp; Search</h3><ul><li>倒排索引支持自定义分析器，包含拼音分词器和拼音过滤器</li><li>倒排索引的搜索函数新增多位置短语查询（PhraseQuery）支持</li><li>新增 ANN 索引仅扫描能力</li></ul><h3>函数</h3><ul><li>新增 <code>sem</code> 聚合函数</li><li>支持源自 Hive 的 <code>factorial</code>简单 SQL 函数</li><li>部分正则表达式函数新增零宽断言支持</li><li>JSON 类型支持 GROUP BY 和 DISTINCT 操作</li><li>新增 add/sub_time 时间函数</li><li>新增 deduplicate_map 函数</li></ul><h3>物化视图</h3><ul><li>非分区基表数据变更时，物化视图仍可参与透明查询重写</li><li>创建 MTMV 支持基于视图创建</li><li>MTMV 刷新支持多 PCT 表</li><li>物化视图包含窗口函数时，支持窗口函数重写</li></ul><h3>Lakehouse</h3><ul><li>新增 Doris Catalog，该功能允许用户通过 Catalog 能力关联多个独立的 Doris 集群并进行高效的联邦数据查询。解决 Doris 集群间数据无法关联查询的问题。文档：<a href="https://link.segmentfault.com/?enc=3jTiBdILJsaAMfDNgtnKjQ%3D%3D.klhZDWKtbx1aZSF6qaV927gTzYauJPEL0tpLl7vctEUmReCtkZbfEDaTsnMZavfKSTFL1loloHka2kFnqfLmBINWbEBA3eai12q971lL7ZM%3D" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/catalogs/doris-ca...</a></li><li>支持通过 rewrite_data_files 方法对 Iceberg 表进行 compaction 操作。该操作允许用户对 Iceberg 小文件进行合并，从而优化读取效率。文档：<a href="https://link.segmentfault.com/?enc=Dc8koaJuwRS%2BoGgAT63zJw%3D%3D.bYV%2FEHlZ0mDwPFV%2FdK2dSMM9qvhOGlhey8W0eVe82MKRe8fKfY6aZBedlOqagEWcZJ9uyvQPHoR0Gh5boPcSj%2FEqeyLrEEmb1XBr%2FbMVY6%2FxIT70mROHI3FQiR%2FbDjaT" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/catalogs/iceberg-...</a></li><li>支持通过 WARM UP 语句对 Hive、Iceberg、Paimon 等外部表数据进行缓存预热。文档：<a href="https://link.segmentfault.com/?enc=dYBGpXZKhWulkDXQlHMP0A%3D%3D.NLtdrs6sjb9fp5zaxN3rjVxdXr%2BPYcd%2B8AKXHFJXXhyGlzuQ176a8u451hhCwJLDsm1EJoySH5fl9D%2ByQaeKfCgyLWT2eJCIOI1Lq5eY29w%3D" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/data-cache#cache-...</a></li><li>支持通过 ALTER 语句对 Iceberg 表进行 Partition Evolution 操作。文档：<a href="https://link.segmentfault.com/?enc=gfY9%2FD4JQExgFwKIXY4UZQ%3D%3D.veFwPLJrg%2BGxTb3ziO9ozBJgKalk48H35i6dHhp%2BHvxKApZXpTlU7khEg4aO%2FeCRgemx0HIt4ykeIx%2Fi4eIZTR2dXPVsjmDUE001McdM900dlbDlLlYIUFlhCjuFJilM" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/catalogs/iceberg-...</a></li><li>支持 HTTP Table Valued Function，支持通过 Table Valued Function 直接读取 HTTP 资源文件。文档：<a href="https://link.segmentfault.com/?enc=QrqbfZl6jbiH4qLLZdCL0A%3D%3D.bV50VUtGVWeIu%2B8UGpJFst77YjQpTGXm2lk8FfaHoBvMomPAdiZB3VzVYu09AuiYH4nH8P%2FT9yivHFFYp38gcYjlEhnNnWHcEuERcFEnI%2Fc%2BAwtECYF50Av0fAE3tVMr" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/sql-manual/sql-functions/ta...</a></li><li>支持直接访问 Huggingface 上的数据集。文档：<a href="https://link.segmentfault.com/?enc=7AXEG0fSpRHQPbapy5fjig%3D%3D.r831oIjS9hUtaFTcduRdjXFGYGMdkDZKjuGZPbiks8MQI5GehujHUcavao%2BRrnqsqJkEW2eYsxjPsww63AW6wA%3D%3D" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/huggingface</a></li><li>支持通过 Iceberg REST Catalog 协议访问 Microsoft OneLake。文档：<a href="https://link.segmentfault.com/?enc=IABnlWcsUBupccsX8TMKsQ%3D%3D.XoidMrMXrEstfIdEZYRIiBotbUVq22C4tHRN1eX1WMUogaTYec8L4ca6TKou4aOIh8L4lf%2FKp7%2FtK8fDwcVjrqqskNO3RTSwoeECDcopf%2FA%3D" rel="nofollow" target="_blank">https://doris.apache.org/docs/4.x/lakehouse/best-practices/do...</a></li><li>支持直接映射 Hive、Iceberg、Paimon、JDBC 外表中的 binary 类型到 Doris 的 varbinary 类型。请参阅各 Catalog 文档的【列映射】小节。</li></ul><h2>优化改进</h2><ul><li>优化 <code>FROM_UNIXTIME</code> 函数性能</li><li>移除 PartitionKey 比较中的 <code>castTo</code>转换操作，提升分区处理效率</li><li>降低 Catalog 中 Column 类的内存占用</li><li>Ann 索引训练前累积多个小批次数据，提升训练效率</li><li>升级 Hadoop 依赖到 3.4.2 版本</li><li>优化 FE 和 BE 的优雅退出机制，降低节点退出对查询的影响</li><li>优化对包含大量分区的 hive 表的写入的效率</li><li>优化 Paimon 表 Split 占用内存过大的问题</li><li>优化对 Parquet RLE_DICTIONARY 编码的读取效率</li><li>优化 FE 和 BE 的优雅退出机制，降低节点退出对查询的影响</li></ul><h2>问题修复</h2><h3>查询</h3><ul><li>修复输入为 null 时 <code>utc_time</code> 函数返回结果错误的问题</li><li>修复 UNION ALL 结合 TVF 时抛出异常的问题</li><li>修复唯一键表创建物化视图时，WHERE 子句包含非键列的问题</li><li>修复 window 函数：LAG/LEAD 偏移参数支持常量表达式计算</li><li>修复聚合函数：可空列投影前下推聚合操作异常；非空列 count 下推聚合问题</li><li>修复时间函数：second/microsecond 函数未处理时间字面量；time_to_sec 处理 null 值时因垃圾值报错</li><li>修复 AI 函数：_exec_plan_fragment_impl 调用 AI 函数时出现未知错误</li><li>修复地理信息：geo 模块内存泄漏</li><li>修复 information_schema：偏移时区格式不兼容</li></ul><h3>物化视图与模式变更</h3><ul><li>修复物化视图包含分组集合和扫描过滤器时重写失败的问题</li><li>修复大流量模式变更时读取单行集非重叠段导致的 coredump 问题</li></ul><h3>存算分离</h3><ul><li>修复 TopN 查询中广播远程读取的问题</li><li>修复云环境下删除 tablet 任务堆积的问题</li><li>修复云环境首次启动时服务上线耗时过长的问题</li></ul><h3>Lakehouse</h3><ul><li>修复某些情况下，Hive 分区变更导致元数据缓存不一致的问题</li><li>修复写入 TIMESTAMP 类型分区的 Iceberg 表错误的问题</li><li>修复 Paimon 表 Incremental Read 行为和 Spark 不一致的问题</li><li>修复某些情况下，外表元数据缓存可能导致的死锁问题</li><li>修复 BE 端 s3 client 线程数不合理导致的 IO 吞吐低的问题</li><li>修复某些情况，写入存储在非 S3 对象存储上的外表时失败的问题</li><li>修复某些情况下，使用 query() 进行 JDBC Catalog SQL 透传失败的问题</li><li>修复 JNI Reader 时间统计导致读取性能下降的问题</li><li>修复 BE 侧 jni.log 无法打印的问题</li></ul><h3>其他</h3><ul><li>修复在非 Master 阶段 UNSET GLOBAL 变量时错误的问题</li><li>修复某些情况下，异常的 export 任务无法取消的问题</li></ul>]]></description></item><item>    <title><![CDATA[YashanDB的高效特性，值得企业关注的6大原因 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493745</link>    <guid>https://segmentfault.com/a/1190000047493745</guid>    <pubDate>2025-12-22 17:14:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代企业信息系统中，数据库技术作为数据管理的核心支撑，面临着性能瓶颈、数据一致性保障、多节点高可用和复杂查询优化等多重挑战。如何实现高性能、高并发、可靠一致的数据服务，成为数据库系统设计的关键问题。YashanDB作为一款支持单机、分布式及共享集群三种部署形态的数据库系统，凭借其先进的架构设计和丰富的技术特性，为企业提供了强大的数据处理能力和灵活的运维管理能力。本文将基于技术事实和行业标准，从六个方面深度解析YashanDB的高效特性，帮助开发人员和数据库管理员理解其在企业级应用中的优势。</p><ol><li>多样化部署架构确保业务适配性和性能</li></ol><p>YashanDB支持单机主备部署、分布式集群部署和共享集群部署三种架构：</p><p>单机部署采用主备复制方式，适用于多数常规业务场景，具备基本的高可用能力，主实例与备实例独立运行，通过redo日志保证数据同步。</p><p>分布式部署基于Shared-Nothing架构，包含管理节点(MN)、协调节点(CN)和数据节点(DN)，支持海量数据线性扩展与高并发处理，适合复杂的分析型业务场景。分布式SQL引擎实现多阶段并行执行，有效降低查询响应时延。</p><p>共享集群部署采用Shared-Disk架构，依赖共享存储、崖山集群内核(YCK)和并行文件系统(YFS)，实现多实例访问同一数据库，保证强一致性、多活读写及在线故障自动切换，满足核心交易等高端应用对高可用和高性能的需求。</p><p>通过灵活选择部署形态，YashanDB能在不同业务负载和资源环境下，提供最优的系统性能和扩展能力。</p><ol start="2"><li>高效的存储引擎及灵活的数据组织</li></ol><p>YashanDB支持多种存储结构以适应不同应用场景：</p><p>行存(HEAP)存储结构采用无序堆式存储，支持高效的插入和事务处理，适合OLTP业务。</p><p>B树(BTREE)索引利用B-Link Tree结构，保证索引的有序性与快速定位，支持多种索引扫描方式如全索引扫描、范围扫描、跳跃扫描等，提升查询效率。</p><p>可变列式存储(MCOL)结合段页式管理和列存储优势，提供对变长列的原地更新能力，兼顾实时写入和分析性能，适合HTAP场景。</p><p>稳态列式存储(SCOL)采用切片式管理、压缩编码及稀疏索引优化查询性能，面向海量冷数据的OLAP分析场景。</p><p>YashanDB支持行存表、TAC列存表和LSC列存表多类型表结构，并通过智能冷热数据划分及后台转换任务自动优化存储，提高存储资源利用率和访问性能。</p><ol start="3"><li>先进的SQL引擎及优化器技术</li></ol><p>YashanDB的SQL引擎具备解析、验证、静态与动态重写、基于成本的优化(CBO)及并行向量化执行等先进功能：</p><p>成本基优化器(CBO)基于准确的统计信息估算数据访问代价，动态生成最优执行计划，选择合理的表连接顺序和访问路径。</p><p>向量化计算基于SIMD技术，实现批量数据并行处理，减少CPU周期消耗，显著提高计算吞吐量。</p><p>分布式SQL执行利用MPP架构，将复杂查询拆分为多个阶段，跨协调节点和数据节点并行执行，充分利用多核、多节点资源。</p><p>丰富的执行算子与Hint机制支持多种查询算子及用户干预优化策略，满足复杂的业务需求和性能调优。</p><p>以上设计保障了YashanDB在海量数据及复杂查询场景下的优异性能表现。</p><ol start="4"><li>完备的事务和并发控制机制保障数据一致性</li></ol><p>YashanDB实现ACID属性的高性能事务管理，采用多版本并发控制(MVCC)实现读写并发无阻塞：</p><p>语句级和事务级读一致性借助系统变更号(SCN)实现，保证查询返回的数据版本统一且隔离。</p><p>写一致性保护针对跨分区更新、并发修改场景，通过事务槽位(Xslot)和写冲突检测确保写入语句近似串行执行，避免漏更新。</p><p>支持读已提交和可串行化两种隔离级别满足不同应用对一致性和性能的权衡需求。</p><p>细粒度锁机制包括表锁和行锁管理，支持死锁检测与解除，确保并发环境下数据一致性与高吞吐。</p><p>自治事务提供嵌套独立事务能力，提高业务逻辑的灵活性和安全性。</p><ol start="5"><li>高可用架构与自动化运维支持</li></ol><p>在高可用设计上，YashanDB提供多层保障：</p><p>主备复制架构基于redo日志的物理复制，支持同步和异步模式，保护模式分为最大性能、最大可用和最大保护级别，满足不同业务对数据安全和主库性能的需求。</p><p>主备切换机制支持计划内切换(Switchover)和故障切换(Failover)，具备日志回退和脑裂检测机制，保障数据库的持续可用和数据一致。</p><p>自动选主技术分布式部署采用Raft算法实现主备自动选主，支持Quorum保证一致性;共享集群引入基于心跳与投票的选主机制，快速响应故障切换。</p><p>崖山集群服务(YCS)与并行文件系统(YFS)为共享集群提供资源管理、故障恢复、多副本冗余及高性能并发访问。</p><p>完善的备份恢复体系支持全量及增量备份，数据归档与基于时间点恢复(PITR)，并可指定备份加密保护数据安全。</p><p>丰富的后台线程体系涵盖崩溃恢复、日志管理、检查点、热块回收、预加载、转换任务调度等，确保系统稳定高效运行。</p><ol start="6"><li>全面的安全机制保障企业数据安全</li></ol><p>YashanDB结合多维度安全特性，满足企业强安全需求：</p><p>身份认证支持数据库密码认证和操作系统认证，采用密码强度策略、账号锁定和过期机制。</p><p>基于角色和标签的访问控制通过RBAC实现系统权限细粒度管理，支持三权分立;基于标签的LBAC提供行级安全控制，保障数据访问合规。</p><p>透明数据加密(TDE)支持表空间及表级AES128/SM4算法加密，备份集加密，确保数据静态存储安全。</p><p>网络传输加密提供基于SSL/TLS的客户端与服务端通信保护，支持X509证书双向认证。</p><p>数据库审计具备权限审计、行为审计和角色审计能力，支持异步审计降低性能影响，方便追溯用户操作和安全监管。</p><p>反入侵及访问控制启用IP黑白名单、连接监听及保留管理员连接，保障数据库服务不受外部攻击影响。</p><p>技术建议</p><p>基于上述YashanDB的高效特性，企业在应用和运维过程中可参考以下技术建议：</p><p>合理选择部署架构：根据业务规模和性能需求，选择单机、分布式或共享集群部署，确保系统具备符合场景的扩展性和高可用能力。</p><p>优化数据存储结构：结合业务数据访问模式，选用合适的行存或列存表结构，合理使用MCOL和SCOL，并利用冷热数据分区提升查询性能。</p><p>完善统计信息管理：定期收集及时更新表、列、索引统计信息，支持优化器做出准确的执行计划选择，提升SQL执行效率。</p><p>合理配置事务隔离级别及锁策略：根据业务准确性需求选择隔离级别，监控锁等待及时诊断死锁，提高并发执行效率。</p><p>建立自动运维机制：启用自动选主、备份恢复、健康监控及故障诊断功能，并结合后台线程资源合理配置，保障系统稳定可靠运行。</p><p>强化安全管控：执行角色权限分离，设置访问控制策略，开启加密和审计功能，持续检测并防御安全威胁，保护数据资产安全。</p><p>结论</p><p>随着企业数据量规模持续增长，数据库系统的性能优化和高可用能力已成为核心竞争力。YashanDB通过多样化的部署架构、先进的存储引擎、高效的SQL引擎、完备的事务机制、成熟的高可用设计以及全面的安全体系，满足了现代企业复杂多变的数据管理需求。未来，随着云原生与大数据技术的进一步融合，YashanDB将持续优化分布式处理能力和智能运维特性，进一步助力企业实现数据资产的价值最大化。数据库专业人员及运维团队应深入理解和掌握YashanDB的技术优势，以推动企业信息系统的稳健发展。</p>]]></description></item><item>    <title><![CDATA[YashanDB的功能实现背后的关键技术分析 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493754</link>    <guid>https://segmentfault.com/a/1190000047493754</guid>    <pubDate>2025-12-22 17:14:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术领域，性能瓶颈、数据一致性保障及高可用性面临严峻挑战。YashanDB作为一款先进的数据库产品，通过多种创新技术和体系架构设计，解决了传统数据库应用中的多项难题。本文面向具有一定数据库基础的开发人员和数据库管理员，详细解析YashanDB实现功能的核心技术，旨在增强对其技术架构的深入理解，并推动数据库性能和可靠性提升的实践。</p><p>多形态部署架构与协同机制</p><p>YashanDB支持单机主备部署、分布式集群及共享集群三种部署形态，以满足不同业务场景的性能、高可用及扩展能力需求。</p><p>单机主备部署：实现主实例与备实例之间的主备复制，保证数据异地同步，支持切换机制以增强容错能力。</p><p>分布式集群部署：核心采用Shared-Nothing架构，分设管理节点(MN)、协调节点(CN)及数据节点(DN)，实现线性扩展和高性能分布式查询执行。协调节点负责SQL解析、执行计划生成及结果汇总，数据节点负责数据持久化与执行，元数据节点管理集群元信息和事务协调。</p><p>共享集群部署：基于Shared-Disk架构，多个实例通过崖山集群内核实现内存聚合(Cohesive Memory)，实现多实例的全局缓存与全局锁管理，保障实例间的强一致性锁和数据访问。依托于崖山文件系统(YFS)的并行文件管理，实现数据文件的高效共享访问。</p><p>多样化存储引擎与数据布局技术</p><p>YashanDB通过支持多种存储结构，以适配不同的应用场景，达到在事务处理、实时分析与海量数据处理间的平衡。</p><p>HEAP存储结构：面向OLTP，将数据行以无序堆式格式存储，提高随机写入性能。采用段区页三级空间管理，并行实现行内In-place Update，结合行迁移及行链接技术优化变长列更新。</p><p>BTREE索引：采用B-Link Tree结构，实现索引的多层次平衡有序存储，支持多种索引扫描方式(全索引扫描、范围扫描、跳跃扫描等)，通过聚集因子降低I/O成本。</p><p>MCOL(Mutable Columnar Storage)：基于段页式列式存储，支持原地更新，使用元数据段和事务管理段保证事务一致性，是HTAP场景的关键支撑，兼顾线上事务与分析要求。</p><p>SCOL(Stable Columnar Storage)：采用对象式管理，进行列数据切片和编码压缩，优化海量数据分析和查询性能。结合后台异步转换任务，实现数据冷热分区与自动转换，显著优化存储与访问效率。</p><p>事务引擎与多版本并发控制（MVCC）</p><p>事务是保障数据库一致性的基础，YashanDB设计了高性能的事务引擎，全面支持ACID属性：</p><p>基于SCN的版本控制：通过系统变更号(SCN)实现查询版本快照的一致性读，并结合Undo表空间管理历史版本，实现读写操作互不阻塞。</p><p>语句级与事务级一致性：默认语句级一致性读可保证每条SQL语句读到快照视角下的可见数据，串行化隔离级别提供事务级一致性，提高数据访问隔离安全性。</p><p>写一致性保障：通过写冲突检测和锁机制防止数据不一致及更新丢失，支持事务的原子提交和滚回。</p><p>锁机制：提供表级共享锁和排他锁、行级排他锁，控制DDL与DML操作间的并发冲突，内置死锁检测机制自动识别并处理死锁状态。</p><p>高效SQL引擎与优化器</p><p>SQL引擎负责SQL语句的解析、优化和执行，YashanDB设计了一套功能丰富且性能优异的SQL处理架构：</p><p>SQL解析器与验证器：完成SQL的词法、语法和语义解析，校验权限和数据类型。</p><p>成本基优化器(CBO)：采用统计信息驱动的成本计算模型，选择最优访问路径与连接顺序，结合静态重写和动态重写生成执行计划。</p><p>执行计划与算子模型：执行计划由多种基础算子构成，包括扫描、连接、排序等，支持多阶段并行分布式执行，通过内部互联总线完成数据交换。</p><p>向量化执行：利用SIMD技术进行批处理算子设计，大幅提高CPU利用率和查询吞吐。</p><p>Hint与并行度控制：提供多样化Hint语法及并行查询能力，实现用户对执行计划的精细控制和性能调优。</p><p>高可用与数据保护技术</p><p>YashanDB提供全面的高可用机制，保证系统持续稳定运行：</p><p>主备复制技术：采用Redo日志同步传输实现主备数据一致性，支撑同步和异步复制模式，多级级联备库部署用于异地容灾。</p><p>自动选主与故障恢复：基于Raft算法和Yasom仲裁机制实现自动主备切换与选主，支持故障切换和计划切换两种模式，确保业务连续性。</p><p>共享集群中的全局资源协调：GRC(全局资源目录)、GCS(全局缓存服务)、GLS(全局锁服务)构建集群资源的一致性管理体系，实现多实例多写的高效协同。</p><p>备份恢复机制：支持全库备份、增量备份和归档备份及基于时间点恢复(PITR)，多级恢复策略保障数据可靠恢复和业务连续。</p><p>双写机制：解决因文件系统缓存产生的数据块半写问题，提高数据完整性。</p><p>丰富的编程与管理接口</p><p>YashanDB内嵌强大的PL引擎和运维生态：</p><p>PL语言支持：支持存储过程、函数、触发器、高级包和自定义数据类型，满足复杂业务逻辑需求，支持自治事务增强程序灵活性。</p><p>多语言数据库驱动：提供JDBC、C、Python、ADO.NET和ODBC驱动，保障应用层多样语言环境的连接互操作。</p><p>集群服务与文件系统：YCS负责集群的节点管理与高可用，YFS实现并行文件操作及共享存储管理，助力共享集群高性能运行。</p><p>安全体系：覆盖用户身份认证、基于角色与标签的访问控制、传输及存储加密、安全审计和反入侵，多层面保障数据安全。</p><p>总结与技术建议</p><p>合理选择部署架构(单机、分布式、共享集群)，结合业务需求安排合理计算和存储资源，以发挥YashanDB性能优势。</p><p>针对业务场景选用合适的存储结构，例如OLTP业务采用HEAP存储，分析场景采用MCOL/SCOL，实现事务与分析的高效融合。</p><p>定期采集和维护统计信息，保证优化器能够生成最优执行计划，结合Hint灵活调整执行逻辑。</p><p>合理规划表分区与索引策略，利用访问约束等特性实现加速大数据查询。</p><p>部署冗余备份和启用主备自动选主机制，确保系统高可用稳定运行。</p><p>使用PL编程提升业务逻辑处理效率，结合定时任务和自定义函数实现灵活运维与业务扩展。</p><p>实施全面安全管理策略，限制权限、加密数据并开展审计与入侵防护，保障数据库环境安全。</p><p>结论</p><p>随着数据规模的迅猛增长和业务复杂度提升，数据库技术不断演进。YashanDB构建在多形态部署、丰富存储引擎、多版本控制及强一致性保障之上，配合高效SQL处理和完备高可用机制，具备适应现代复杂场景需求的能力。未来，随着并行计算、大数据处理及安全合规要求的提升，优化和增强这些核心技术将成为数据库产品竞争的关键。持续深入学习和实践这些技术，对提升数据库性能和保障业务连续性至关重要。</p>]]></description></item><item>    <title><![CDATA[YashanDB的关键数据结构及其优缺点 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493760</link>    <guid>https://segmentfault.com/a/1190000047493760</guid>    <pubDate>2025-12-22 17:13:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术不断发展的背景下，数据库系统在性能优化、数据一致性保障和高可用架构设计中面临诸多挑战。针对线上事务处理、大规模数据分析以及多实例并发访问，数据库需采用高效且灵活的数据结构支撑多样化场景需求。本文深入分析YashanDB的主要关键数据结构，重点介绍其设计原理、功能实现以及所体现的优势和潜在局限性，旨在为开发人员和数据库管理员提供技术参考和实践指导，促进对YashanDB体系架构的理解和应用。</p><p>HEAP存储结构：无序行存数据管理</p><p>HEAP存储是YashanDB中行存表采用的基础数据结构，基于堆式存储实现数据的无序组织。数据以行(Row)为单位，支持变长列，插入操作通过快速定位空闲空间完成，插入效率较高且适合高频数据写入场景。HEAP结构中实现了页面空闲空间的细粒度管理和行迁移机制，对变长字段更新时支持原地更新和跨页行链接，极大程度保证了数据一致性和空间利用率。HEAP存储配合PCT Free参数，在预留更新空间和减少行迁移次数上提供了灵活调优手段。</p><p>优势：适合OLTP场景，插入效率高;支持变长数据和原地更新;灵活的空闲空间管理提高并发性能。</p><p>缺点：由于无序存储，查询性能较BTREE索引低;频繁变量长度更新导致的行迁移可能引起碎片化。</p><p>BTREE存储结构：有序索引管理与访问优化</p><p>YashanDB默认的索引结构采用基于B-Link Tree的多叉平衡树，支持高效的一维数据的有序存储。索引由叶子块和分支块组成，通过多层分支路由实现Log(N)级别查询效率。索引行采用有序排列，叶子块通过双向链表连接，支持顺序扫描和范围查询。为优化I/O访问，BTREE维护索引聚集因子以反映索引对应数据的物理排序程度，影响范围扫描的访问代价。索引支持升序、降序及函数索引，扩展了查询优化空间。</p><p>优势：显著提升索引列相关查询效率;平衡树保证访问深度和查询响应时间;支持多种扫描策略适应不同查询场景。</p><p>缺点：更新索引列涉及删除和重插操作，带来一定写开销;范围查询性能受索引聚集因子影响较大。</p><p>MCOL存储结构：可变列式存储的实时事务与分析平衡</p><p>MCOL(Mutable Columnar Storage)是YashanDB面向HTAP场景设计的列式存储结构，采用段页式管理，每列数据连续存储，最小访问单位为Block。MCOL通过元数据段和事务管理段控制数据版本和一致性，支持原地更新(in-place update)避免传统列存更新中产生的大量墓碑和空间膨胀问题。对于变长列，采用列转行及混合行列存储技术，既保证了更新效率，又充分利用列存查询性能。MCOL组织数据为Batch进行批量处理，配合向量化计算进一步提升查询效率。</p><p>优势：支持实时写入与更新，兼顾事务与分析需求;批量处理加速查询;有效控制存储膨胀和后台清理开销。</p><p>缺点：相较纯行存结构，查询少量列时仍存在一定解码和聚合开销;需要后台转换任务管理数据冷热区间，运维较复杂。</p><p>SCOL存储结构：稳态高压缩列式存储优化大规模分析</p><p>SCOL(Stable Columnar Storage)针对OLAP场景设计，采用对象式管理和切片式存储。数据按切片分块存储，每个切片包含多个列对象，支持压缩编码、稀疏索引及条件下推过滤，显著降低查询I/O。切片划分为活跃切片(实时写入)和稳态切片(压缩存储)两类，通过后台转换任务实现冷热数据自动转换和维护。切片化使得数据可以灵活合并及清理无效记录，优化存储空间和访问性能。</p><p>优势：极大提升大数据分析查询性能;高效存储压缩降低存储成本;灵活冷热数据分离保障数据完整性和分析效率。</p><p>缺点：数据更新开销较高，主要适合冷数据;后台转换任务依赖良好运维策略，维护复杂。</p><p>段页式存储结构：灵活的逻辑存储空间管理与高效块操作</p><p>YashanDB采用段页式逻辑存储结构划分存储空间，将物理数据文件抽象为表空间、段、区和块四级结构。段主要承载数据库对象的数据，区为连续数据块组成的批量空间，减少管理开销。数据块作为逻辑存储最小单元，配合多层次空闲空间管理提高空间的动态使用效率。段空间管理通过多空闲度列表实现快速定位可用空间，增强并发插入性能。段设置的水位线(高水位线和低水位线)标识数据页使用范围，协助全表扫描及数据管理。</p><p>优势：空间管理颗粒度适中，有效支持动态扩展和回收;空闲度列表优化了高并发下的空间复用;支持场景多样存储需求。</p><p>缺点：针对大规模数据和极端高并发仍需持续优化空闲空间管理机制;复杂空间管理略增实现和运维复杂度。</p><p>切片式存储结构及对象式管理：支持云端存储及大规模数据分布式组织</p><p>切片是LSC表数据的物理存储单元，基于对象式管理，每个切片为文件集合，包含数据和元数据文件。通过数据桶(Databucket)支持本地磁盘和云端存储。每列划分成独立对象，文件连续存储，有利于压缩和快速访问。对象式管理通过文件目录组织，提高海量数据管理效率，支持高效冷热数据区分和大规模分布式部署。</p><p>优势：简化数据管理，支持分布式和云存储环境;高压缩率提高存储效率;提升查询性能和弹性扩展能力。</p><p>缺点：文件系统依赖性较强，用户需合理配置存储环境;跨网络访问性能较本地文件系统可能受限。</p><p>事务与MVCC机制支持多版本并发</p><p>YashanDB所有表结构均支持ACID事务和多版本并发控制(MVCC)，确保数据的一致性与隔离性。系统通过事务ID、SCN和undo信息维护数据版本，实现读写不阻塞和快照读。CR Block生成机制利用undo回滚记录，对于不满足读事务可见性的行，实时回滚至可见版本，消除读取冲突。支持强隔离级别(可串行化)及写冲突检测机制，保障事务安全和性能。</p><p>优势：读写并发效率高，保证数据一致性;支持跨实例强一致性的多版本读;完善的事务控制避免数据冲突。</p><p>缺点：undo空间需求较大，需合理配置;高并发写冲突场景需调优。</p><p>结论与建议</p><p>根据应用场景选择存储结构：OLTP场景建议优先使用HEAP行存表配合BTREE索引，OLAP及HTAP场景可结合MCOL和SCOL列存结构，实现读写性能平衡。</p><p>合理设计索引结构，避免过多不必要的索引带来维护开销和写性能退化，充分利用索引扫描优化策略和聚集因子，提升访问效率。</p><p>充分利用段页式空间管理和切片对象式管理的优势，优化存储空间分配与回收，预防碎片化问题，提升存储利用率。</p><p>综合使用事务隔离级别和MVCC策略，合理控制事务粒度和回滚保存点，提升并发能力和数据一致性保障水平。</p><p>完善运维体系，关注后台转换任务、undo空间管理及缓存机制，保障系统稳定高效运行。</p><p>结论</p><p>YashanDB通过多样化且精细设计的数据结构，包括HEAP行存、BTREE索引、MCOL与SCOL列存、段页式及切片式存储等，满足了不同场景对数据访问性能和一致性的严苛要求。各数据结构在其擅长领域展现出优越性能，同时兼顾了系统的灵活性与扩展性。理解和合理应用这些数据结构，是实现数据库高性能管理与优化的关键。技术人员应结合实际业务需求，合理选择、配置和调优底层结构，从而充分发挥YashanDB的存储和计算优势，提升应用系统的整体性能和可靠性。</p>]]></description></item><item>    <title><![CDATA[YashanDB的核心技术架构与实现原理 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493764</link>    <guid>https://segmentfault.com/a/1190000047493764</guid>    <pubDate>2025-12-22 17:12:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库系统中，优化查询速度成为提升整体应用性能的关键因素。查询效率的提升不仅影响业务响应时间，还关系到系统的吞吐量和资源利用率。因此，选择合理的数据库架构和实现高效的存储及执行引擎，成为数据库设计和实现的核心问题。YashanDB作为一款支持多种部署形态的数据库系统，其核心技术架构通过多层次优化方案，实现了高并发、高可用和高性能的目标。本文将系统梳理YashanDB的技术架构和内部实现原理，揭示其在存储管理、事务处理、SQL执行引擎、分布式与集群技术上的技术优势与创新。</p><p>多样的部署架构支持及逻辑架构设计</p><p>YashanDB支持单机部署(主备模式)、分布式集群部署和共享集群部署三种形态，满足从小规模应用到海量数据分析及高性能多活场景的需求。</p><p>单机部署：采用主实例和备实例的主备复制，保证数据的同步和高可用。该模式适用于大部分普通业务场景。</p><p>分布式部署：核心在于MN(元数据管理)、CN(协调)、DN(数据存储)多类型节点的协调及任务分发，适用于海量数据和高并发分析业务，支持线性扩展。</p><p>共享集群部署：通过依赖共享存储和聚合内存技术，实现多实例对数据的强一致并发访问，支持多写和高可用。集群内核包括全局资源管理(GRC)、缓存服务(GCS)和锁服务(GLS)，确保实例间数据和资源的一致性协调。</p><p>逻辑层面，YashanDB构建了包括客户端驱动、SQL引擎、PL引擎和存储引擎的多层架构。SQL引擎负责SQL的解析、优化与执行，PL引擎支持用户自定义的过程式编程，而存储引擎实现了对数据空间和对象的细粒度管理。</p><p>多样化存储引擎与对象管理</p><p>存储引擎是YashanDB数据库核心部件之一，针对不同业务特点提供多样化的存储结构及管理策略。</p><p>存储结构：支持HEAP(无序堆式存储)、BTREE(B树有序存储)、MCOL(可变列式存储)和SCOL(稳态列式存储)。不同结构适配OLTP、HTAP及OLAP场景。</p><p>存储对象：行存表采用HEAP结构，适合高频随机写入的联机事务处理;列存表分为TAC(面向实时分析)和LSC(面向稳态分析)，支持数据分割为活跃切片和稳态切片，实现增量写入与压缩编码。</p><p>空间管理：逻辑空间通过段、区和表空间层次进行精细化管理，其中段空间管理基于多层空闲度列表优化空间利用率并降低并发冲突，采用PCT Free策略预留页面空闲，减少行迁移。</p><p>对象式管理：列存表冷数据切片为独立存储对象，采用文件方式存储，提高压缩编码和数据访问性能。</p><p>高性能事务机制与并发控制</p><p>事务实现ACID特性，是保证数据一致性和完整性的基础。YashanDB通过以下核心机制实现高效且一致的事务处理和并发控制：</p><p>多版本并发控制(MVCC)：采用版本链与UNDO表空间管理历史版本，支持语句级和事务级读一致性，保证查询数据的稳定视图，读写不阻塞。</p><p>事务隔离级别：支持读已提交(Read Committed)和可串行化(Serializable)，兼顾事务隔离和系统性能。</p><p>锁机制：使用行级排他锁和表级锁实现写写冲突控制，防止数据不一致，并支持死锁检测与自动处理以保证系统稳定。</p><p>写一致性：为避免并发写引起的数据遗漏，YashanDB自动触发语句重启机制确保写操作的串行化。</p><p>高效的SQL执行引擎与优化器</p><p>YashanDB的SQL引擎负责SQL语句的处理，涵盖解析、验证、优化和执行全过程。</p><p>解析与验证：将SQL文本解析生成语法树，经过权限和语义检查后转入优化阶段。</p><p>优化器：基于代价的优化器(CBO)使用丰富的统计信息和查询变换(静态、动态重写)，生成最优执行计划，支持HINT指令对执行策略的影响。</p><p>执行算子与向量化：执行计划由扫描、连接、排序等算子组成，支持基于SIMD的向量化计算提高CPU利用率与吞吐性能。</p><p>分布式执行：通过协调节点(CN)分发执行计划至数据节点(DN)，实现MPP式多级并行，支持跨节点数据通信和数据交换。</p><p>多线程高并发架构与实例内存管理</p><p>为充分利用现代多核硬件，YashanDB采用多线程架构：</p><p>线程分类：包含监听线程、逻辑时钟、后台任务(如检查点、日志刷新)、并行执行线程及会话工作线程，动态调配线程数量保证资源利用率。</p><p>内存管理：划分共享内存区域(SGA)和会话私有内存(SPA)，共享内存缓存数据、SQL计划和元数据，加速数据访问;会话私有内存存放查询栈和会话临时数据。</p><p>高可用性设计及主备复制机制</p><p>为应对单点故障与保障业务连续性，YashanDB提供多维度的高可用能力：</p><p>主备复制：采用WAL机制的redo日志传输，支持同步、异步复制及Quorum保护模式，满足不同业务对于数据一致性与可用性的权衡需求。</p><p>自动切换和选主机制：基于Raft协议和yasom仲裁实现自动选主与故障切换，保证主库不可用情况下的快速恢复。</p><p>切换策略：区分计划内切换(Switchover)和故障切换(Failover)，保障最大可用性。</p><p>共享集群核心技术</p><p>共享集群基于Shared-Disk架构，依托于崖山集群内核(YCK)、文件系统(YFS)和集群服务(YCS)，实现多实例多活：</p><p>聚合内存(Cohesive Memory)技术：统一管理多实例的数据页访问，利用GRC、GCS、GLS实现全局资源和缓存协调。</p><p>崖山文件系统：并行文件系统支持共享存储设备管理、高可用和低延迟直接访问，提升大规模数据访问性能。</p><p>集群管理：YCS负责集群监控、资源管理和故障投票仲裁，实现自动故障恢复和集群重组。</p><p>技术实施建议</p><p>合理选择部署架构：根据业务规模和性能需求，选择单机、分布式或共享集群形态，充分利用YashanDB的架构优势。</p><p>针对业务场景选择存储引擎：在联机事务处理场景选择HEAP行存表，复杂分析或HTAP场景采用MCOL和SCOL列存技术，提升数据访问效率。</p><p>设计高效索引策略：利用BTREE索引支持多种扫描方式，结合函数索引和跨表分区索引优化查询性能。</p><p>合理配置事务隔离与锁机制：结合读已提交与可串行化隔离级别及死锁监控，优化事务并发执行效率确保数据一致性。</p><p>利用SQL执行器的向量化和并行能力优化查询表现。</p><p>规划和部署高可用架构，配置合适的保护模式和备库数量，保障业务连续性和数据安全。</p><p>定期收集和更新统计信息，合理利用HINT调整执行计划，提高SQL执行效率。</p><p>应用共享集群技术时，合理规划共享存储和网络，管理好YCS和YFS实例，确保集群的稳定。</p><p>实施安全策略，配置访问控制、加密存储和网络加密，强化数据库系统的安全性。</p><p>结论</p><p>YashanDB凭借其多样化的部署架构、丰富的存储选择、先进的事务管理及高效的SQL引擎，实现了满足多场景需求的数据库解决方案。通过核心技术如多版本并发控制、基于成本的优化器、共享集群核心组件及高可用主备复制机制，保证了系统的高性能、高可用和数据一致性。掌握这些技术原理和最佳实践，有助于工程师在项目中合理设计数据库架构、优化应用性能、提升系统稳定性，充分发挥YashanDB的技术价值。</p>]]></description></item><item>    <title><![CDATA[从“线索内卷”到“用户共生”：汽车企业CRM全链路破局解决方案（2025版） 爱听歌的金针菇 ]]></title>    <link>https://segmentfault.com/a/1190000047493766</link>    <guid>https://segmentfault.com/a/1190000047493766</guid>    <pubDate>2025-12-22 17:11:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在用户主权全面崛起、市场从增量竞争转向存量深耕的2025年，传统汽车CRM以“线索数量”为核心的运营逻辑早已失效。空号率高达70%的低质线索、突破2000元的单条成交成本、数据孤岛造成的运营割裂，让车企陷入“卷而无效”的困境。依托AI技术赋能的CRM系统（如珍客AI CRM），结合行业变革趋势与标杆企业实践，构建一套以“用户全生命周期价值”为核心的一体化解决方案，助力车企实现从“线索狩猎”到“用户深耕”的根本性转型。</p><h2>一、核心变革：AI驱动的三大思维重构</h2><h3>1. 价值导向重构：从“单次成交”到“终身共生”</h3><p>摒弃传统CRM“唯线索量”“唯成交率”的短视逻辑，借助珍客CRM系统的LTV（用户终身价值）核算模型，建立多维度价值衡量体系。汽车作为高价值消费品，用户生命周期长达79.3个月，其价值不仅包括首次购车，更涵盖售后服务、保险续保、二手车置换、增购换购及口碑传播等多个维度。通过系统精准追踪用户全周期行为数据，将运营重心从“短期转化”转向“长期培育”，通过持续价值输出构建品牌与用户的信任纽带。</p><h3>2. 用户认知重构：从“静态线索”到“动态数字身份”</h3><p>打破传统CRM对用户的碎片化认知，依托OneID技术与多维度数据融合能力，构建“动态更新、跨平台统一、全维度覆盖”的用户数字身份体系。系统可打通官网、APP、社交媒体、线下门店等全触点数据，整合用户行为轨迹、偏好特征、社交关系、消费能力等信息，自动生成360°精准用户画像。区别于传统线索分级，支持“意向强度+生命周期+价值潜力”的三维分层体系，实现H/A/B/C级用户的智能识别与标签化管理。</p><h3>3. 协同逻辑重构：从“厂商博弈”到“一体化作战”</h3><p>破解汽车主机厂与经销商“各吹各的号”的协同断层，通过珍客CRM平台搭建“数据互通、责任共担、能力互补、利益共享”的厂商协同体系。汽车主机厂借助系统升级为“线索经营者+赋能者”，负责全域线索获取、智能分发与运营支持；经销商通过系统转型为“体验落地者”，聚焦高价值线索跟进、线下体验优化与售后维护，实现力出一孔的运营效能。</p><h2>二、解决方案核心模块：全链路闭环运营体系</h2><h3>1. 全域线索整合模块：破解“渠道分散+质量低下”痛点</h3><ul><li><strong>全渠道线索归集</strong>：支持垂媒平台、短视频、社交媒体、线下活动、异业合作等10+渠道的线索统一接入，通过标准化API接口实现多平台数据实时同步，彻底解决传统模式下线索分散管理难题，72%经销商可避免因线索混乱导致的跟进延迟。</li><li><strong>AI智能线索筛选</strong>：运用语义分析、行为研判与号码校验技术，自动剔除空号、重复线索及竞品探子，通过留资内容关键词提取、用户行为轨迹权重分析等方式，智能评估用户购车意向度，将线索精准分级为H/A/B/C四级，线索有效率提升至85%以上。</li><li><strong>智能分发机制</strong>：基于动态分发模型，结合经销商服务半径、库存状况、历史转化效率、客户评价等多维度数据，实现高价值线索与最优承接主体的精准匹配。系统支持经销商在线提交调单申请，汽车主机厂可通过后台24小时内完成审核响应，线索分发匹配率提升60%。</li></ul><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnrr5" alt="珍客AI CRM 线索管理" title="珍客AI CRM 线索管理"/></p><h3>2. 用户精细化运营模块：落地“农耕式”培育理念</h3><ul><li><strong>AI驱动分层运营</strong>：根据用户分层标签自动触发差异化运营策略：针对H级高意向用户，系统每日推送现车资源对比、竞品劣势分析等个性化内容；A级中高意向用户每3天收到金融方案、试驾反馈强化等精准触达；B级中意向用户每周获取场景化车型资讯、门店活动预告；C级低意向用户每月收到品牌动态、节日关怀，保持品牌存在感。所有触达时机、渠道、内容均由AI算法优化，提升用户响应率。</li><li><strong>AARRR闭环运营赋能</strong>：依托自动化工作流引擎，基于AARRR模型构建全周期运营闭环：获取阶段根据用户来源渠道自动匹配差异化内容；激活阶段通过AI话术推荐打造“啊哈时刻”（如精准推送用户关注的车型核心卖点）；留存阶段通过智能社群运营、专属服务提醒增强用户粘性；收益阶段挖掘保险、保养、软件订阅等增值服务需求；传播阶段自动触发裂变激励机制。</li><li><strong>场景化内容智能分发</strong>：根据不同渠道用户属性，实现内容差异化适配：小红书用户收到真实车主体验分享，抖音用户获取短平快功能演示视频，知乎用户推送深度技术解析，满足用户多平台交叉验证的决策习惯，内容触达精准度提升52%。</li></ul><h3>3. 汽车厂商协同赋能模块：打破“数据孤岛+能力断层”</h3><ul><li><strong>一体化数据中台</strong>：搭建厂商共用的可视化数据看板，实时同步线索流转状态、跟进记录、转化数据、服务评价等核心信息。主机厂可通过后台监控全链路运营效果，及时发现经销商跟进延迟、线索浪费等问题；经销商可一键获取主机厂提供的赋能资源（如标准化话术、促销政策），实现数据互通透明与资源高效协同。</li><li><strong>AI赋能支持体系</strong>：内置中央呼叫中心模块，AI坐席可承接车型咨询、价格查询等基础需求（响应速度≤3秒），为经销商减负60%；系统搭载智能培训库，为经销商提供线索跟进技巧、异议处理、需求挖掘等分层课程；同时提供标准化工具包，包括AI生成的跟进话术、邀约模板、成交工具等，助力销售顾问快速提升专业能力。</li><li><strong>标准化跟进落地</strong>：通过设定线索跟进SOP，系统自动提醒销售顾问在15分钟内响应高意向线索、24小时内完成首次深度沟通，避免错失最佳跟进时机。经销商可通过系统记录跟进内容、挖掘客户需求，主机厂可实时监控跟进质量，确保服务标准化与体验一致性。</li></ul><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnrsl" alt="珍客AI CRM 客户流转" title="珍客AI CRM 客户流转" loading="lazy"/></p><h3>4. 数据驱动决策模块：实现“运营优化+风险预警”</h3><ul><li><strong>全维度数据分析</strong>：整合线索获取（渠道效果、投放成本、线索质量）、运营转化（响应率、到店率、成交率）、用户价值（LTV、复购率、转介绍率）等核心指标，支持区域、车型、经销商、渠道等多维度交叉分析，生成可视化报表，为厂商决策提供数据支撑。</li><li><strong>智能预警机制</strong>：系统可自定义线索跟进超时、长期未转化、战败线索异常等预警规则，通过短信、APP推送等方式提醒相关责任人及时干预，减少线索浪费。例如，当线索超过30分钟未响应时，系统自动向经销商运营负责人与主机厂区域经理发送预警，避免意向度下降50%以上。</li><li><strong>策略迭代优化</strong>：基于数据分析结果，厂商可持续优化运营策略：根据渠道线索质量调整投放比例，依据用户反馈优化产品配置推荐，结合转化数据升级激励政策，实现运营效率的持续提升。</li></ul><h3>5. 私域运营与裂变模块：复制标杆企业成功经验</h3><ul><li><strong>私域流量沉淀</strong>：支持用户一键添加企业微信、加入品牌社群，系统自动为用户打上私域标签，通过AI算法推送专属福利、用车知识、线下活动邀约等内容，激活私域用户活跃度，构建高粘性用户池。</li><li><strong>裂变激励体系</strong>：借鉴特斯拉推荐模式，内置裂变推荐模块，支持“推荐者-购买者-品牌”三方共赢机制。推荐者可通过系统获取积分（可兑换超充额度、软件升级等），购买者享受购车折扣，系统自动核算奖励并实时到账，助力车企将获客成本降低至传统渠道的1/10。</li><li><strong>KOS私域深耕</strong>：依托内容管理功能，支持销售人员转型为KOS（关键意见销售），系统可生成“车型对比、用车场景、售后服务”等系列内容模板，销售顾问只需简单修改即可在小红书、抖音等平台发布，快速建立个人IP信任，实现从“被动接待”到“主动获客”的转变。</li></ul><p><img width="723" height="449" referrerpolicy="no-referrer" src="/img/bVdnrsd" alt="珍客CRM 私域运营" title="珍客CRM 私域运营" loading="lazy"/></p><h2>三、落地保障：技术、组织与机制三重支撑</h2><h3>1. 技术支撑：一体化平台优势</h3><p>珍客CRM系统具备线索管理、数据分析、协同沟通、赋能支持、预警提醒五大核心模块，支持OneID用户识别、AI智能筛选、动态分发、自动化工作流等关键功能，兼容线上线下多渠道数据接入，保障系统稳定性与扩展性。同时，严格遵循《个人信息保护法》等法规要求，通过数据加密、访问控制、备份恢复等措施，确保用户数据安全与合规。</p><h3>2. 组织变革：建立跨职能协同体系</h3><p>依托珍客CRM系统成立企业级用户运营中心，打破市场、销售、客服、售后部门壁垒，实现信息无缝流转；通过系统明确主机厂与经销商的权责划分，建立“日常沟通、周例会、月度复盘”的三级沟通机制，保障协同效率。</p><h3>3. 考核激励：构建“双向共赢”机制</h3><ul><li>对经销商考核：通过系统采集线索转化率、跟进及时率、协同配合度等指标，与返利、额外线索分配、培训资源挂钩；</li><li>对主机厂考核：基于系统数据评估线索精准度、赋能满意度、问题解决效率等指标，与团队绩效、奖金激励关联；</li><li>设立“利益共享池”：通过系统整合厂商双方资源，对协同考核达标的团队进行额外奖励，强化“利益共同体”意识。</li></ul><h2>四、结语：AI重塑汽车CRM核心价值</h2><p>在用户主权时代，汽车CRM的核心已不再是“管理线索”，而是“经营用户关系”。借助AI CRM的技术赋能，破解了传统模式的七大痛点，实现了从“线索收割”到“用户共生”的转型。特斯拉的裂变推荐、蔚来的KOS私域、理想的场景化运营已经证明，只有真正尊重用户主权、深耕用户价值、打通协同壁垒，才能在存量竞争中实现可持续增长。未来，依托智能运营能力，汽车企业将构建“以用户为中心”的全生命周期运营体系，跳出“内卷”泥潭，构建长期竞争优势。</p>]]></description></item><item>    <title><![CDATA[配置Wireshark抓取https数据包 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047493775</link>    <guid>https://segmentfault.com/a/1190000047493775</guid>    <pubDate>2025-12-22 17:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用Wireshark抓取数据包的过程当中，发现所抓取的包都是加密的数据，其协议类型为TLSv1.3。查询后得知，这属于https的数据包，是加密的数据。于是按照博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>中的方法，配置Wireshark。其大致步骤如下：</p><ul><li>在任意位置建立<strong>ssl_key</strong>文件夹</li><li>在文件夹下新建<strong>sslog.log</strong>文件</li><li>右击<strong>此电脑</strong>的<strong>属性</strong></li><li>选择<strong>设置</strong>窗口右侧的<strong>高级系统设置</strong></li><li>选择<strong>环境变量</strong></li><li>在<strong>系统变量</strong>中新建变量名<strong>SSLKEYLOGFILE</strong></li><li>该变量的值为<strong>sslog.log</strong>文件的绝对路径</li><li>最后配置Wireshark</li></ul><p>但由于软件版本问题，需将配置Wireshark的部分步骤更改如下：<br/><img width="723" height="396" referrerpolicy="no-referrer" src="/img/bVdnqRU" alt="image.png" title="image.png"/><br/>即选择<strong>TLS</strong>协议，给其添加<strong>sslog.log</strong>文件的绝对路径</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=pzTibQHxAuYp09G%2F0aP4BA%3D%3D.9hCpzjvBNdBwySz60oL4af5VCaaNh3IqIP0Sga39qXwCEYbqmRMIruBbvP97gtrGk5uvKwZfLAtTmjmwOw275Q%3D%3D" rel="nofollow" target="_blank">为什么我的 Wireshark 抓不到/抓不全 HTTP 数据包 ？</a> <a href="#fnref-1" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[YashanDB的核心优势：深入了解其性能与应用 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493789</link>    <guid>https://segmentfault.com/a/1190000047493789</guid>    <pubDate>2025-12-22 17:10:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术领域，应用系统面临着诸多挑战，包括性能瓶颈、数据一致性维护、并发控制以及高可用性保障等方面的问题。数据库系统需有效处理大量数据的存储与计算，同时保障系统的稳定性和可扩展性。本文旨在为数据库架构师、开发人员及运维工程师提供YashanDB数据库系统的核心技术解析，全面展示其在性能优化、架构设计及应用实践中的优势。本文内容基于YashanDB的官方技术文档，深入探讨其部署架构、存储引擎、SQL引擎、事务机制、高可用方案及安全管理机制，助力读者全面理解并应用该数据库系统。</p><p>多样化部署架构及扩展能力</p><p>YashanDB支持单机(主备)、分布式集群和共享集群三种部署形态，满足不同规模和业务需求。单机形态通过主备实例实现简单高可用，多数业务场景可适配。分布式部署采纳Shared-Nothing架构，拥有管理节点(MN)、协调节点(CN)以及数据节点(DN)组，支持事务管理、分布式查询及数据分片，适合海量分析及线上混合处理应用。共享集群部署基于Shared-Disk架构，依托聚合内存技术实现多实例间的数据共享与强一致性访问，支持多写多实例并发，适用于对高可用、高性能和弹性扩展要求极高的核心业务场景。</p><p>通过灵活的部署方案，YashanDB能够在单点瓶颈和大规模分布式环境间无缝适配，具备优秀的扩展性和高可用能力。</p><p>多元存储引擎与高效空间管理</p><p>YashanDB的存储引擎支持HEAP(堆式)、BTREE、MCOL(可变列式存储)及SCOL(稳态列式存储)多种存储结构。HEAP适合事务性OLTP场景，支持高速随机写入和原地更新机制。BTREE结构专为索引设计，支持多种索引扫描类型，保证索引有序及快速访问。MCOL通过段页式管理支持在线事务与分析混合场景，核心优势在于列集中存储及支持原位更新，避免存储空间膨胀，提高查询及写入性能。SCOL采用对象式管理，提供高压缩效率与快速访问能力，适合海量稳定数据的OLAP场景，且支持后台透明转换MCOL至SCOL格式以优化查询性能。</p><p>同时，YashanDB通过细粒度的逻辑存储结构划分(包括Block、Extent和Segment)及表空间管理，实现空间灵活分配与高效管理。段页式空间管理的多级空闲度列表优化插入速度及并发，PCT Free设置最大化降低行迁移，提高性能。对象式管理适配切片式存储，便于列式数据压缩及编码。</p><p>成熟的SQL引擎与优化器体系</p><p>YashanDB内置完善的SQL引擎，涵盖解析、验证、静态及动态重写、基于代价模型的优化器及执行模块。优化器借助丰富的统计信息(包括表、列、索引分布与直方图)及CBO算法，精准估算查询代价，选择最优执行计划，显著提升查询效率。支持多种执行算子如扫描、连接、排序和并行执行算子，且通过Hint提供用户干预执行路径的接口。</p><p>值得关注的是，YashanDB支持向量化计算，采用SIMD技术批量处理数据，大幅加速表达式计算与算子执行。分布式SQL执行过程中，协调节点(CN)与数据节点(DN)依托内部分布式互联网络采用MPP架构，支持节点间和节点内并行计算，确保大规模数据的高效处理能力。</p><p>完善的事务及并发控制机制</p><p>数据库事务具备ACID属性，YashanDB基于多版本并发控制(MVCC)实现读写分离，保证语句级和事务级一致性读。查询以一致性快照版本(SCN)执行，避免读写阻塞。写冲突控制结合行锁与事务隔离等级实现，支持读已提交和可串行化两种隔离级别，平衡性能与数据一致性。行级锁粒度最小，排他锁机制保证写操作安全且高效。死锁检测机制自动识别并解除死锁，保障系统稳定运行。</p><p>YashanDB支持自治事务，允许嵌套执行独立的事务单元，在复杂业务编排及日志记录场景中提高灵活性与响应速度。</p><p>完善的高可用与灾备方案</p><p>YashanDB通过主备复制实现故障切换与数据同步。主库异步或同步传输redo日志至多个备库，保障备库与主库数据一致。支持三级主备结构(级联备)，实现跨地域容灾。日志回放优先级高，确保备库查询性能。主备切换包含计划内切换(保证数据无丢失)及故障切换(快速恢复业务但可能存在数据延迟)。自动选主功能支持Raft算法及仲裁机制，结合Quorum机制增强集群稳定性与容错性。</p><p>多维安全体系设计</p><p>安全性方面，YashanDB实现包括用户管理、身份认证、访问控制、数据加密、审计及防入侵等全方位保障。支持基于角色的访问控制(RBAC)、三权分立管理架构，实现职责明确与最小权限原则。加密技术涵盖透明数据加密(表空间级、表级)及备份文件加密，保障存储安全。支持SSL/TLS协议加密传输，确保网络安全。审计功能支持详细事务及权限操作追踪，结合异步审计减轻系统负载。IP黑白名单、连接监听及保留连接机制增强数据库防御能力。</p><p>高性能异构存储与集群文件系统</p><p>YashanDB在共享集群场景中采用自研崖山文件系统(YFS)，支持裸设备管理、多副本冗余及故障组划分，实现高可靠、高性能存储。基于磁盘组与故障组逻辑划分，支持多副本数据分布保障数据安全。YFS采用分配单元管理，提高IO性能及管理效率。配合崖山集群服务(YCS)实现集群资源统一管理、拓扑监控及高可用保障。聚合内存技术实现多实例共享缓存访问，降低延迟，增强多活环境下的访问性能和一致性。</p><p>技术建议</p><p>根据业务规模选择合适的部署架构：中小型应用优先采用单机主备部署，大数据分析及海量业务使用分布式部署，高并发高可用要求采用共享集群。</p><p>依据应用场景合理选择存储引擎：联机事务处理推荐HEAP，混合型分析推荐MCOL，海量分析优先考虑SCOL结构。</p><p>定期收集并维护完整的统计信息，配合CBO优化器，实现SQL执行计划优化，提升查询性能。</p><p>合理配置事务隔离级别，根据应用需求平衡读写一致性与系统吞吐，避免过高隔离级别带来的并发瓶颈。</p><p>启用合适的主备复制保护模式，结合自动选主机制，确保业务连续性与数据安全。</p><p>强化安全策略，实施最小权限原则，合理划分角色与权限，结合数据库加密与审计机制，保障数据安全与合规。</p><p>合理规划存储与表空间布局，调整表空间的空闲比例(PCT Free)、表分区及索引，最大化存储利用率及访问效率。</p><p>充分利用系统提供的存储引擎的原地修改与多版本并发技术，降低锁竞争与存储空间浪费。</p><p>关注共享集群环境下的硬件资源配置及集群配置管理，保障YCS与YFS稳定运行，提高集群一致性与可用性。</p><p>利用异步审计与定时任务等管理机制，降低系统开销，提升数据库整体运行效率。</p><p>结论与未来展望</p><p>本文系统性介绍了YashanDB数据库在架构设计、存储引擎、多版本事务控制、高可用和安全管理等方面的核心优势。随着业务规模和数据复杂度的增长，数据库系统对性能、可靠性和安全性的要求愈发严苛。YashanDB通过多样化部署形态和先进的技术手段，有效满足现代大型分布式应用场景中的需求，具备良好的行业适应性和扩展能力。未来，随着硬件性能的提升与智能化需求的增加，YashanDB将持续优化内核架构和计算模型，推动向更高并发、更低延迟以及自动化管理方向发展，成为关键业务系统的核心竞争力。</p>]]></description></item><item>    <title><![CDATA[YashanDB的结构优化与高效存储设计 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493798</link>    <guid>https://segmentfault.com/a/1190000047493798</guid>    <pubDate>2025-12-22 17:09:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>现代数据库系统在面对日益增长的数据规模和复杂多变的业务需求时，通常面临性能瓶颈、数据一致性保障以及存储资源优化的多重挑战。数据库架构的高效设计不仅关乎系统响应速度，也直接影响到数据的可靠性和可维护性。YashanDB作为一款面向大数据应用的数据库产品，针对这些共性问题进行了系统性的结构优化和存储设计，旨在提高系统的整体处理能力和存储利用率，确保数据一致性与高可用。本文将深入剖析YashanDB的核心架构、数据存储引擎、逻辑存储结构及其他关键技术，帮助数据库管理员和开发者更好地理解其设计原理与实现优势。</p><p>多形态部署架构与实例设计</p><p>YashanDB支持单机主备部署、分布式集群部署和共享集群部署三种主要形态，充分满足不同规模及场景下的应用需求。</p><p>单机部署通过主备复制保证基本的高可用，备库实时同步主库日志实现灾备。分布式部署采用Shared-Nothing架构，节点划分为MN(管理节点)、CN(协调节点)和DN(数据节点)三类，支持海量数据处理与线性扩展。共享集群部署则基于Shared-Disk架构，依赖自研崖山文件系统(YFS)实现共享存储，通过崖山集群内核(YCK)聚合多实例内存，实现多实例间强一致性和高并发读写。</p><p>数据库实例采用多线程架构，包含后台线程及工作线程，分别承担系统管理、I/O操作、事务处理和客户端交互等任务。实例内存空间分为共享内存区域(SGA)和私有内存区域(SPA)，保证多会话高效资源共享与独立数据隔离。</p><p>灵活多样的存储引擎与对象管理</p><p>YashanDB提供四种存储结构满足不同行业场景需求：</p><p>HEAP：用于行式存储，数据无序存放，支持高效的在线事务处理(OLTP)。该结构配合段页式空间管理，使用空闲度列表优化空间利用率及并发插入。</p><p>BTREE：用于索引结构，采用B-Link Tree实现有序索引且支持高效范围扫描。索引管理包含叶子块与分支块，实现平衡树访问，支持多种扫描策略(如范围扫描、唯一扫描等)以优化查询速度。</p><p>MCOL：可变列式存储，支持实时业务的HTAP需求。采用段页式管理，按列集中存储数据，支持原地更新和字典编码，提升变长列的存储及访问效率，改善投影操作表现。</p><p>SCOL：稳态列式存储，适用于海量冷数据场景。采用切片式对象存储，支持高级压缩、编码及条件下推，凭借活跃与稳态切片调度实现冷热数据高效分层存储和后台异步转换，提高查询性能。</p><p>不同存储结构对应的存储对象有行存表、列存表(TAC和LSC)及BTree索引，保证了针对OLTP、HTAP及OLAP多样场景的性能适配。表空间通过分离逻辑和物理结构，利用段、区与块实现细粒度管理和灵活扩展。</p><p>多版本并发控制与事务管理</p><p>YashanDB通过多版本并发控制(MVCC)机制实现高并发读写的一致性保障。每次事务提交时系统变更号(SCN)同步推进，读操作依据SCN决定数据版本可见性，写操作采用行锁控制并发修改，确保事务的原子性、一致性、隔离性与持久性(ACID)。</p><p>支持的事务隔离级别包括读已提交和可串行化。读已提交保证数据不读脏，事务内每条语句利用语句级一致性读;可串行化隔离级别提供事务级一致性，优秀的写冲突检测防止不可串行化现象。事务支持隐式启动与结束，灵活的SAVEPOINT点及自治事务满足复杂业务场景。</p><p>高效持久化与日志管理机制</p><p>为实现数据持久化与故障恢复，YashanDB采用预写日志(WAL)机制，所有修改先记录redo重做日志，再由后台多线程批量写入数据文件。支持双写文件技术避免断裂页和半写问题，利用检查点(Checkpoint)机制定期刷新脏页，保障数据一致性与恢复效率。</p><p>redo日志文件设计包含日志头、redo包和重做记录，支持细粒度日志切换及归档，结合备份机制实现基于时间点的恢复(PITR)。备份支持全量、增量及流式备份，满足多样存储策略需求。</p><p>智能SQL引擎与优化器</p><p>YashanDB内置成本基优化器(CBO)，依据统计信息动态分析数据分布，拓展访问路径，确定连接顺序和语法重写，以生成最低成本执行计划。SQL执行过程包括：解析、验证、静态重写、生成执行计划、动态重写及执行。支持多线程并行执行及向量化计算，利用SIMD技术提升计算效率。</p><p>支持多种执行算子包括扫描算子、连接算子、排序算子、辅助算子及向量化执行算子，灵活组合以满足复杂查询需求。同时支持Hint提示功能，有效辅助优化器制定执行策略。</p><p>主备复制与高可用设计</p><p>通过主备复制机制实现数据库高可用。主库负责业务读写，备库实时接收并回放redo日志，保障数据同步。支持级联备库减少主库负载，提高异地容灾能力。主备角色支持手动切换(Switchover和Failover)及自动选主机制，自动化维护系统健康。</p><p>多重保护模式(最大性能、最大可用、最大保护)满足不同业务容忍度要求，保证从性能优先到零数据丢失的多样配置。</p><p>高效内存与线程架构</p><p>YashanDB采用分层内存架构，划分共享全局区(SGA)及私有会话区(SPA)，分别缓存SQL解析计划、数据缓存、日志缓存及会话私有数据等。采用LRU算法管理数据缓存和有界加速缓存，提高内存利用率。</p><p>多线程架构支持后台管理、I/O处理、监听服务及事务执行。根据会话配置支持独占线程模式和共享线程模式，有效平衡资源使用及响应速度，增强系统稳定性和扩展性。</p><p>安全机制与权限管理</p><p>系统基于角色的访问控制模型，兼顾职责分离，细化权限及角色管理。通过身份认证(数据库认证与操作系统认证)，保障合法访问。支持访问控制策略结合标签实现行级安全隔离。数据透明加密与传输加密保证数据传输及存储过程中的机密性及完整性。提供丰富的审计机制方便安全审计及异常追踪。</p><p>技术建议汇总</p><p>根据业务规模和访问模式选择合适的部署形态，单机主备适用于小规模应用，分布式集群适合海量数据业务，共享集群满足高并发与多写场景。</p><p>合理设计存储结构，针对OLTP选择HEAP行存，针对HTAP和OLAP合理组合MCOL与SCOL列存以降低存储和访问开销。</p><p>充分利用MVCC和事务隔离级别保证数据一致性，在性能允许范围内优先选择可串行化隔离以避免并发问题。</p><p>开启并配置双写和检查点机制，避免脏页及断裂页带来的数据风险，保持日志文件连续性，利于数据库恢复和备份。</p><p>持续收集更新统计信息，利用优化器和Hints优化SQL执行计划，实现查询性能最大化。</p><p>结合业务容忍度合理配置主备复制模式，制定有效备份恢复策略，保证业务连续性和灾难恢复能力。</p><p>开发过程中充分利用PL内置的存储过程、函数及触发器，实现将业务逻辑靠近数据端，提升执行效率。</p><p>增强安全管理，合理配置密码策略、角色权限及访问控制，保证数据库访问安全性和合规性。</p><p>根据实际硬件环境调整内存大小、线程数及并行度，平衡负载，提升系统整体性能。</p><p>结论</p><p>随着数据规模和应用复杂性的不断增长，数据库系统对结构优化与高效存储设计的要求日益提升。YashanDB通过支持多样化部署架构、先进存储引擎、多版本并发机制、智能优化器及完善的安全保障，为现代企业构筑了坚实的数据管理基础。未来，随着技术持续迭代，面向云原生及分布式大数据的适应能力将更为关键，优化技术也必将成为数据库行业的核心竞争力。持续深化对YashanDB技术架构的理解，有助于提升系统部署、维护及性能调优的专业水平，推动企业实现更高效可靠的数据价值挖掘。</p>]]></description></item><item>    <title><![CDATA[YashanDB的可扩展性：如何满足不同企业需求 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493808</link>    <guid>https://segmentfault.com/a/1190000047493808</guid>    <pubDate>2025-12-22 17:08:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在当今信息技术快速发展的环境下，企业对数据库的性能、可用性、以及扩展能力提出了越来越高的要求。随着数据规模的不断扩大和业务复杂性的提升，数据库面临的挑战包括性能瓶颈、数据一致性保障以及高可用性需求等。数据库系统的可扩展性成为衡量其能力的关键指标，直接影响企业业务连续性和数据处理效率。本文聚焦于YashanDB的可扩展性设计，深入剖析其技术架构和功能，通过详尽的技术分析展示其如何满足各类企业在不同场景下的需求，适合数据库管理员、架构师及技术开发人员参考学习。后续内容将分解YashanDB的部署模型、存储引擎设计、事务处理机制及集群架构等关键技术，全面呈现其可扩展能力。</p><p>部署形态多样化实现灵活扩展</p><p>YashanDB支持三种主要部署模式：单机(主备)部署、分布式集群部署和共享集群部署，分别适配不同规模和业务特征的企业需求。单机部署通过主备复制实现基础的高可用，满足资源有限且业务并发较低的场景。分布式部署采用了Shared-Nothing架构，节点间依托于MN、CN、DN三类服务组件协同工作，具备良好的线性扩展特性，尤其适用于海量数据分析业务。共享集群部署则基于Shared-Disk架构和崖山集群内核，利用共享存储和聚合内存技术实现多实例多活、高并发的强一致性访问，满足对高性能及高可用要求极高的核心交易场景。此三种部署形态使YashanDB能根据企业规模和应用类型灵活选择，极大增强了产品的适应力和可扩展性。</p><p>多样化存储引擎优化数据处理效率</p><p>针对不同业务场景，YashanDB设计了多种存储结构以兼顾事务处理和实时分析需求。包括HEAP行存、BTREE索引结构，以及两类列式存储MCOL(可变列式)和SCOL(稳态列式)。HEAP结构提供高效的无序存储，适合事务密集型OLTP场景。BTREE索引保障数据的快速有序访问，提升筛选效率。MCOL存储支持段页式管理和原地更新，既保持了列式存储的查询优势，也确保了实时数据更新的性能，适合混合事务分析场景(HTAP)。SCOL则采用切片式对象管理，结合压缩编码及稀疏索引优化大量冷数据的存储和访问，针对大规模稳态数据的OLAP分析。通过整合多种存储技术，YashanDB提供了弹性的数据处理框架，满足企业对实时性与分析能力的综合需求。</p><p>强健的事务与并发控制确保数据一致性</p><p>YashanDB实现了完善的事务管理体系，支持ACID属性，采用多版本并发控制(MVCC)实现读写并发的高效隔离，确保事务的隔离性和一致性。系统通过基于系统变更号(SCN)的可见性判断实现语句级和事务级读一致性，保障查询数据的准确性。写并发控制采用行锁机制，保障写写冲突得到及时检测和解决。同时，YashanDB支持读已提交和可串行化两种隔离级别，涵盖从常规业务到高严格性应用场景的隔离需求，确保企业在不同风险与性能权衡下做出合理选择。这种健壮的事务及锁机制设计，是数据库可扩展性与数据可靠性的基石。</p><p>高效分布式执行规划提升资源利用率</p><p>在分布式部署形态下，YashanDB协调节点(CN)生成分布式执行计划，将复杂查询拆分为多个并行执行阶段，分发至数据节点(DN)并行处理。系统内部采用异步基于消息的互联网络和多级并行执行策略，提高了查询效率和吞吐能力。通过优化器的成本模型和统计信息动态调整执行策略，结合HINT提示和向量化计算技术，实现批量数据的高效处理。系统支持数据和控制消息分离，保障控制命令的及时响应和数据传输的高吞吐量，合理分配计算资源，满足大规模业务并发需求，有效实现计算资源的线性扩展。</p><p>共享集群架构保障多实例环境下的数据一致性与高可用</p><p>共享集群模式下，YashanDB依托共享存储和崖山集群内核(YCK)通过全局资源管理模块实现了多实例对数据页的协同读写。具体包括全局资源目录(GRC)、全局缓存服务(GCS)、全局锁服务(GLS)，确保各实例对全局资源状态的强一致管理。集群服务(YCS)负责配置、资源监控与故障投票仲裁，保证故障快速感知与自动恢复，提升系统稳定性。崖山文件系统(YFS)作为内嵌文件系统，提供强一致的文件操作和数据冗余，保证存储的高可用性和性能。在多实例写入的并发环境下，YashanDB共享集群架构通过内存聚合与分布式锁，确保业务数据的安全、准确与高效访问。</p><p>操作模型与运维机制支撑弹性扩展</p><p>YashanDB实例支持多线程架构，其中包括日志管理、回滚、检查点等多个后台线程协同工作，保障数据库的持续运行和恢复能力。实例支持配置参数灵活调整，覆盖内存使用、连接线程池大小、并行度控制等多方面，保证系统可根据实际负载自动扩展资源能力。数据库客户端支持多种语言驱动，包括JDBC、C、Python等，满足多样化接入需求。主备机制允许实现多备份同步和级联备份，配合手动或自动选主功能，保障系统的高可用与容灾能力。利用故障诊断与自动修复辅助线程，及时发现和定位问题，减少系统的不可用时间，保障企业业务顺畅运营。</p><p>具体技术建议</p><p>结合实际业务需求合理选择部署形态。小型应用可采用单机主备部署，保证基本高可用;需处理海量数据时推荐分布式部署实现线性扩展;高并发核心业务宜选共享集群部署以保障多实例强一致及高性能。</p><p>针对不同数据访问特性合理选用存储结构。OLTP场景优先采用HEAP行存，HTAP实时分析场景适用MCOL可变列式存储，OLAP离线分析场景可选SCOL稳态列式存储提升查询性能。</p><p>合理配置事务隔离级别和锁机制。常规业务建议使用读已提交隔离，严苛场景则采用可串行化隔离保障数据一致，同时注意事务合理设计，降低死锁概率。</p><p>结合负载调整并行度及内存缓存参数。调整SQL执行并行度和内存共享池大小，提升多核CPU资源利用率，防止资源瓶颈限制系统扩展能力。</p><p>部署高可用主备及备份策略。使用多备库与级联备库配置，确保数据多点备份和容灾能力，实现零数据丢失或最小数据丢失容忍度。</p><p>应用自动选主功能。启用Raft或yasom仲裁机制，实现主备自动故障切换，减少人为介入，提升系统可用性。</p><p>合理规划共享集群的共享存储和容量。根据业务规模评估DiskGroup及故障组设计，选择适当冗余等级，保障数据高可用性和性能弹性。</p><p>按需启用访问控制和加密机制。通过RBAC及LBAC实现精细权限管理，使用表空间或表级TDE保护数据安全，符合企业数据合规要求。</p><p>定期统计与优化SQL执行计划。维护统计信息准确更新，确保优化器生成高效执行计划，配合HINT及索引调优提升查询性能，减小扩展瓶颈。</p><p>完善监控及故障诊断体系。部署健康监控和自动诊断机制，结合诊断存储库和日志分析，及时发现性能瓶颈及故障，保持数据库稳定运行。</p><p>结论</p><p>随着企业数据规模和业务复杂度持续增长，数据库系统在性能扩展性、可靠性及灵活性方面的需求逐渐强化。YashanDB通过多样的部署形态、高效的存储架构、严格的事务控制及完善的集群管理，打造了具备强大可扩展能力的数据库平台。未来，随着云原生技术与大数据分析的发展，进一步优化数据库的弹性伸缩和智能调度能力将成为核心竞争力。企业需要在实际应用中深入了解并结合YashanDB的丰富特性，持续优化数据库架构设计，从而支撑未来业务的快速发展和创新。</p>]]></description></item><item>    <title><![CDATA[YashanDB的跨平台支持：使数据无处不在 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493821</link>    <guid>https://segmentfault.com/a/1190000047493821</guid>    <pubDate>2025-12-22 17:07:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代信息技术领域，数据库系统的性能、稳定性与可扩展性成为保障业务连续性和用户体验的关键因素。数据库在不同的软硬件平台上的移植和运行是企业数字化转型的基础，而跨平台能力直接影响到数据库的部署灵活性与数据的广泛可用性。YashanDB作为国产高性能数据库，其跨平台支持能力涵盖多种部署形态，从单机到分布式再到共享集群，满足不同业务场景对多样化硬件和操作系统环境的需求。本文将基于YashanDB的体系架构与关键技术，详细解析其跨平台支持的核心技术优势，面向数据库管理员与开发人员，深入探讨其如何实现数据在异构环境中的高效一致访问。</p><p>多样化部署架构支持</p><p>YashanDB支持三种主要的部署形态：单机主备部署、分布式集群部署以及共享集群部署，每种形态在硬件依赖和系统架构上具有不同特点，确保数据库能在多样化的硬件平台与操作系统间进行部署和无缝迁移。</p><p>单机部署采用主备复制机制，支持两台服务器分别运行主实例和备实例，以保证高可用性。此种部署形态适合高可用性要求不特别高且资源有限的小规模应用场景，兼容绝大多数常见操作系统和硬件环境。</p><p>分布式部署基于Shared-Nothing架构，包括管理节点(MN)、协调节点(CN)、数据节点(DN)三类不同职责的节点，支持多种操作系统组合和异构硬件的协同服务。各节点可独立运行在不同的平台，应用层通过内部互联总线(DIN)实现高效通信，保证整体的线性扩展能力和负载均衡。</p><p>共享集群部署则依赖共享存储和崖山文件系统(YFS)，实现了多实例多活、数据强一致访问。依托聚合内存技术(Cohesive Memory)协调多实例间访问数据页的同步，此架构适合对性能、高可用和可扩展性均有极高需求的核心业务系统。共享集群部署也支持跨平台运行，提升了硬件利用率和业务连续性。</p><p>统一的存储引擎与多存储结构支持</p><p>YashanDB的存储引擎设计高度灵活，支持多种存储结构包括堆式存储(HEAP)、B树存储(BTREE)、可变列式存储(MCOL)与稳态列式存储(SCOL)，满足不同场景下的性能需求。这种设计不仅提升了系统的跨平台兼容性，还保证了数据访问方式和模型的一致性：</p><p>HEAP存储结构提供无序数据快速写入，适用于联机事务处理(OLTP)场景，写入和更新操作高效。</p><p>BTREE存储结构实现索引数据的有序存储，支持高效的随机读取和范围扫描，跨平台间的数据和索引布局保持一致。</p><p>MCOL存储结构支持实时业务的列式存储，采用段页式管理，可实现原地更新，提升了HTAP场景的查询和写入能力兼顾性。</p><p>SCOL存储结构针对大规模海量数据的稳态分析，采用对象式切片持久化，具备优异的压缩和编码能力，配合分布式体系，实现异构平台上的高性能数据分析。</p><p>基于上述多种存储结构，YashanDB能够在不同平台和操作系统上实现统一而高效的数据访问机制，保证数据结构的一致性并降低跨平台迁移的复杂度。</p><p>统一且高效的SQL引擎与分布式执行框架</p><p>YashanDB的SQL引擎集成了解析、优化和执行的完整流程，以代价基(CBO)优化模式生成最优执行计划。优化器充分利用采集的表、列、索引统计信息，并支持Hint控制与动态调整执行计划，从而充分兼顾多平台环境下的性能需求。</p><p>SQL执行支持分布式多级并行：</p><p>跨节点并行：协调节点(CN)根据数据分布将复杂SQL拆分成多个执行阶段(stage)，由数据节点(DN)并行执行任务，网络通信通过高性能异步互联总线保障。</p><p>节点内并行：数据节点内部支持水平和垂直切分的多线程并行处理，充分利用多核CPU资源，保证各平台CPU架构高效利用。</p><p>该执行框架实现了平台无关的分布式查询调度和数据汇总，支持多样化客户端API(JDBC、Python、C、ADO.NET、ODBC等)，确保用户在不同操作系统和开发环境下均可获得稳定一致的SQL能力。</p><p>多样化客户端驱动与网络通讯支持</p><p>YashanDB提供JDBC、C、Python、ADO.NET及ODBC等多语言驱动，均符合业界协议和接口标准，实现跨语言跨平台连接数据库。数据库驱动层实现了对不同操作系统下内存结构、数据格式和网络协议的兼容适配，确保数据访问和命令交互的稳定。</p><p>网络体系设计上，数据库客户端连接服务采用监听线程接受连接请求，不同连接模式支持独占线程与线程池共享线程实现，满足跨平台多终端、高并发访问需求。内部节点间通讯通过高可靠的异步通信框架ICS实现多实例、分布式节点资源调度和数据交换。</p><p>支持多种操作系统平台的高可用设计</p><p>高可用是数据库稳定性的重要保障。YashanDB支持多种主备部署形式，涵盖一主多备与级联备，主备复制机制基于WAL先写日志(Write Ahead Log)实现数据同步。主库和备库通过独立线程完成日志发送与回放，保证系统在不同服务器、不同操作系统平台上的数据同步无损失。</p><p>自动选主机制基于Raft算法、Quorum投票和Yasom仲裁，实现多样化部署环境下的自动故障切换和主备角色动态调整。共享集群采用网络心跳和磁盘心跳双重检测，一旦感知故障，快速投票选举保持服务连续性。</p><p>跨平台下的统一运维与安全管理</p><p>YashanDB运维管理框架包含基于命令行的yasboot工具和运维服务进程(yasom、yasagent)，结合权限控制、多角色管理实现安全稳健的数据库访问与管理。</p><p>安全体系覆盖用户管理、身份认证、访问控制、透明加密、审计和反入侵等方面，确保数据在多平台环境间的保密性、完整性及可用性，满足政企用户等高安全合规需求。</p><p>实践建议</p><p>针对多平台部署，优先规划数据库实例的资源适配与软硬件协调，确保各部署形态对应平台的稳定运行。</p><p>合理选择存储结构与表组织方式，兼顾业务场景需求，实现数据访问最大化性能和最优跨平台兼容性。</p><p>充分利用分布式执行与并行计算框架，结合优化器统计信息和Hint，优化SQL计划，提升跨平台系统的整体吞吐量。</p><p>使用适配平台的数据库驱动和连接模式，根据并发连接数和资源限制，调整线程模型与网络配置，以保障服务稳定。</p><p>配置完善的高可用方案，选择合适的高可用保护模式和自动选主策略，确保多平台之间的数据一致性和故障快速恢复能力。</p><p>结合YashanDB安全管理能力，制定符合业务和法规要求的权限和审计策略，确保在异构环境中的数据安全和合规。</p><p>结论</p><p>随着企业数字化、多云及异构计算环境的快速发展，数据库系统的跨平台能力日益成为核心竞争力。YashanDB依托其丰富的部署模式、多样化的存储引擎、灵活的SQL执行框架和全面的安全高可用策略，实现了数据在各种操作系统和硬件环境中的高效、稳定访问与管理。展望未来，随着数据规模与应用复杂性的不断提升，进一步增强跨平台兼容性与性能优化将持续成为数据库发展的重要方向。数据库管理员和开发者应深入理解和利用YashanDB跨平台能力，推动业务系统的稳健发展与创新。</p>]]></description></item>  </channel></rss>