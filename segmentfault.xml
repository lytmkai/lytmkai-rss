<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[OpenClaw 深度技术解析：一款可自托管的“实干型”个人AI代理平台 AIAgent研究 ]]></title>    <link>https://segmentfault.com/a/1190000047598949</link>    <guid>https://segmentfault.com/a/1190000047598949</guid>    <pubDate>2026-02-07 20:02:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在AI Agent赛道快速迭代的2026年，多数产品仍停留在“问答建议”的被动模式，而OpenClaw作为GitHub上增长最快的开源项目之一，以“本地优先、强执行能力、高可扩展”的核心特性，打破了传统Chatbot的能力边界——它不仅能理解用户指令，更能直接操控系统、调用工具、自动化复杂工作流，成为真正具备“双手”的个人AI代理。本文将从技术定位、核心架构、模块详解、工程实现、实操落地、安全机制、局限与展望七个维度，全面拆解OpenClaw的技术细节，既有底层架构的深度剖析，也有可直接复用的实操指南，助力开发者快速掌握这款开源AI代理的核心逻辑与应用方法。</p><h2>一、OpenClaw 核心定位与技术价值</h2><h3>1.1 核心定位</h3><p>OpenClaw 是一款开源、可自托管的个人AI代理与自动化平台，由知名开发者Peter Steinberger发起，历经Clawdbot、Moltbot两次名称迭代后定型，核心目标是实现“AI从被动建议到主动执行”的范式转变[superscript:2]。与传统AI助手不同，OpenClaw 以“本地优先”为设计原则，可部署在个人电脑、NAS或私有云服务器上，聚焦个人与小型团队的自动化需求，能够自主完成文件整理、浏览器操作、系统命令执行、多平台消息同步等复杂任务，成为用户的“数字员工”。</p><h3>1.2 核心技术价值</h3><p>OpenClaw 的价值核心的在于“数据主权保障”与“强执行能力”的双重突破，其技术价值可概括为三点，区别于传统AI助手与其他AI Agent产品[superscript:4]：</p><ul><li>数据主权可控：自托管模式让所有对话历史、个人偏好、文件内容等数据完全由用户掌控，规避公有云AI服务的数据隐私泄露风险，默认采用纯文本存储，兼具透明性与可解释性[superscript:2]；</li><li>执行能力突出：突破传统Chatbot“只说不做”的局限，可直接操控操作系统、浏览器、第三方API，实现端到端的任务自动化，无需人工介入中间步骤[superscript:1]；</li><li>高可扩展性：采用“微核+插件+统一网关”的架构，支持多模型适配、多通道通信、自定义技能开发，可灵活集成生产力工具、智能家居等外部服务，适配多样化场景[superscript:2]。</li></ul><h3>1.3 与传统AI助手的核心差异</h3><p>OpenClaw 与传统AI助手（如ChatGPT、普通Chatbot）的能力边界差异，可通过以下对比清晰体现[superscript:4]：</p><table><thead><tr><th>能力维度</th><th>传统AI助手（ChatGPT）</th><th>OpenClaw 智能体</th></tr></thead><tbody><tr><td>文件操作</td><td>仅能描述操作步骤，无法实际执行</td><td>直接读写、移动、分类文件，支持复杂文件处理流程</td></tr><tr><td>系统命令</td><td>提供命令示例，无法实际运行</td><td>执行Shell命令、运行脚本、管理进程，支持沙箱隔离</td></tr><tr><td>浏览器控制</td><td>无实际操作能力，仅能提供导航建议</td><td>自动化网页导航、表单填写、数据提取、屏幕截图</td></tr><tr><td>API调用</td><td>有限集成，依赖平台开放接口</td><td>完整API生态支持，通过环境变量注入密钥，灵活集成第三方服务</td></tr><tr><td>持久记忆</td><td>仅支持会话级记忆，重启后丢失</td><td>分层长期记忆，支持跨会话复用，可人工编辑与迁移</td></tr><tr><td>隐私安全</td><td>云端处理，数据由平台掌控</td><td>本地存储、自托管，用户完全掌控数据主权</td></tr></tbody></table><h2>二、OpenClaw 核心技术架构深度拆解</h2><p>OpenClaw 的架构设计遵循“解耦、可扩展、本地优先”的原则，采用“微核（Microkernel）+ 插件（Plugins）+ 统一网关（Gateway）”的核心模式，整体分为五层，各层独立运行、协同工作，确保核心稳定的同时，提升可维护性与扩展性[superscript:2]。其完整架构如下，从下到上依次为：基础依赖层、核心微核层、功能模块层、集成适配层、用户交互层。</p><h3>2.1 架构整体逻辑</h3><p>OpenClaw 的核心运行逻辑可概括为“消息接收→上下文整合→指令生成→任务执行→结果反馈”的闭环：</p><ol><li>用户通过任意通信通道（如Telegram、Slack、Email）发送指令；</li><li>多通道消息网关接收指令，转换为标准化格式，同步会话状态；</li><li>Agent运行时整合指令、历史记忆、用户偏好，生成标准化提示，发送给选定的大模型；</li><li>大模型生成响应或工具调用指令，由工具执行层解析并执行；</li><li>执行结果返回给Agent运行时，结合记忆系统更新上下文，最终通过原通道反馈给用户。</li></ol><p>这种架构的核心优势在于“解耦”——核心微核负责调度与协调，功能模块负责具体实现，集成适配层负责对接外部系统，任何一层的迭代都不会影响其他层的稳定性[superscript:2]。</p><h3>2.2 各层详细解析</h3><h4>2.2.1 基础依赖层：运行基石</h4><p>OpenClaw 的运行依赖于现代JavaScript/TypeScript生态，核心依赖如下superscript:2：</p><ul><li>运行时环境：Node.js ≥ 22，依托其强大的异步I/O处理能力、庞大的npm生态，适配网络应用与系统交互场景；</li><li>核心语言：全栈采用TypeScript，通过静态类型检查提升代码健壮性、可读性，降低大型开源项目的维护成本；</li><li>核心工具：pnpm（推荐）/npm（包管理）、tsx（TypeScript实时运行）、Docker（沙箱隔离）、node-cron（定时任务）；</li><li>模型依赖：支持云端模型（Anthropic Claude、OpenAI GPT系列）与本地模型（通过Ollama集成Llama、Mistral），采用“用户自带API密钥”模式[superscript:3]。</li></ul><h4>2.2.2 核心微核层：调度中枢</h4><p>核心微核是OpenClaw 的“大脑”，负责全局调度、指令解析、状态管理，确保各模块协同工作，核心组件包括[superscript:2]：</p><ul><li>Agent运行时（Agent Runtime）：核心调度组件，实现“思考-行动”（ReAct）循环——接收标准化提示，发送给大模型，解析模型响应（直接回答/工具调用），调度工具执行层执行任务，直到任务完成；</li><li>状态管理器：负责会话状态、任务进度、工具执行状态的统一管理，确保多通道切换、系统重启后，任务可无缝续接；</li><li>模型适配器：提供统一的LLM接口层，适配不同厂商的模型，实现“模型无关设计”——用户可根据成本、性能需求，灵活切换云端/本地模型，无需修改核心代码[superscript:1]。</li></ul><h4>2.2.3 功能模块层：核心能力载体</h4><p>功能模块层是OpenClaw 执行能力的核心，包含五大核心模块，各模块独立封装，可通过插件方式扩展，核心模块如下：</p><h5>（1）多通道消息网关（Multi-Channel Gateway）</h5><p>作为OpenClaw 与用户交互的“入口”，核心作用是实现多通信平台的无缝集成与消息标准化，基于Node.js构建，通过WebSocket连接实现实时通信[superscript:1]：</p><ul><li>支持通道：覆盖15+主流通信平台，分为三类——即时通讯（WhatsApp、Telegram、Signal、iMessage、SMS）、团队协作（Slack、Discord、Microsoft Teams、Google Chat）、传统渠道（Email、Matrix、Zalo）；</li><li>技术实现：每个通道通过独立的适配器（Adapter）与网关通信，适配器负责将各平台的消息格式转换为OpenClaw 标准化格式，同时保持会话状态（Session）和消息转录（Transcript）的持久化；</li><li>核心优势：用户可在不同平台间无缝切换任务，例如在Telegram中发起的文件整理任务，可在WhatsApp中继续查看进度、发送新指令[superscript:1]。</li></ul><h5>（2）工具执行层（Tool Execution Layer）</h5><p>OpenClaw 的核心突破的在于该层，使其超越传统Chatbot的范畴，具备直接“动手”的能力，支持四大类工具操作，每类操作均有成熟的技术实现与权限控制[superscript:1]：</p><ul><li>文件系统操作：基于Node.js <code>fs</code> 模块与Shell命令实现，支持文件的读写、移动、分类、压缩/解压，可在无活跃终端会话的情况下，自主创建目录结构、整理下载文件夹；</li><li>浏览器自动化：基于Chrome DevTools Protocol (CDP) 或Playwright控制独立的Chromium实例，支持页面导航、表单填写、数据提取、屏幕截图（Snapshot）和视觉分析，例如 <code>openclaw browser snapshot --interactive</code> 命令可生成带交互式元素标记的页面快照，供AI精确定位操作目标[superscript:1]；</li><li>系统级访问：支持执行Shell命令、运行脚本、管理进程，权限模型分为“全访问”与“沙箱化”两种模式，沙箱化模式通过Docker容器隔离风险，避免恶意指令破坏系统[superscript:1]；</li><li>API编排：通过环境变量注入API密钥，灵活连接第三方服务（日历、邮件、智能家居、交易所、健康监测等），实现跨平台服务的协同自动化[superscript:1]。</li></ul><h5>（3）记忆与上下文管理模块</h5><p>与无状态的传统Chatbot不同，OpenClaw 具备完整的持久化记忆系统，遵循“本地优先、可解释、持久化、分层检索”的设计哲学，让AI能够持续学习用户习惯，实现跨会话上下文复用[superscript:2]，核心组成如下[superscript:1]：</p><ul><li>核心身份记忆：通过Soul.md / IDENTITY.md文件存储用户偏好、个人事实和代理人格设定，采用Markdown格式，便于用户人工编辑、修改，实现AI的个性化定制；</li><li>每日记忆日志：自动生成带日期标记的Markdown日志，记录当日任务执行情况、用户交互内容，可与Obsidian、Raycast等工具集成，方便用户追溯与整理；</li><li>向量检索：对长期记忆进行语义提取与向量存储，支持跨会话的语义搜索，快速召回相关上下文，解决“健忘”问题；</li><li>工作区隔离：不同会话（Session）拥有独立的工作目录和上下文，支持多代理并行运行，避免任务之间的干扰[superscript:1]。</li></ul><h5>（4）自主调度系统（Proactive Automation）</h5><p>该模块让OpenClaw 从“被动响应”转变为“主动代理”，可在无用户输入的情况下，主动发起对话、执行任务，核心通过两种机制实现[superscript:1]：</p><ul><li>Heartbeat（心跳）：周期性触发器，可配置为每15分钟、每小时执行指定任务，例如扫描收件箱中的紧急邮件、检查日历冲突、监控第三方服务状态；</li><li>Cron作业：基于node-cron实现，支持复杂的定时调度逻辑，典型用例包括每日8:00的“晨间简报”（整合天气、日程、新闻、GitHub动态）、每周日的文件备份[superscript:1]。</li></ul><h5>（5）技能系统（Skills System）</h5><p>技能系统是OpenClaw 可扩展性的基石，采用声明式编程范式，让开发者能够快速开发、集成自定义功能，无需修改核心代码[superscript:1]：</p><ul><li>技能定义：每个技能是一个包含<code>SKILL.md</code>文件的目录，该文件通过自然语言描述技能的功能、使用场景和实现方式，无需编写复杂的API文档；</li><li>技能扩展：开发者可通过编写TypeScript脚本，实现自定义技能（如特定平台的数据抓取、个性化报告生成），并通过插件方式集成到OpenClaw中；</li><li>技能调用：AI可根据用户指令，自动识别并调用匹配的技能，无需用户手动指定，实现“指令到执行”的无缝衔接[superscript:1]。</li></ul><h4>2.2.4 集成适配层：连接外部生态</h4><p>负责对接外部工具、服务与模型，打破OpenClaw 的能力边界，核心适配内容包括[superscript:2]：</p><ul><li>模型适配：通过模型适配器，适配Anthropic Claude（推荐Opus 4.5）、OpenAI GPT系列、MiniMax等云端模型，以及通过Ollama集成的本地模型，支持模型故障转移（fallbacks）；</li><li>第三方服务适配：提供标准化接口，适配Gmail、Google Calendar、Notion、Home Assistant、GitHub、交易所等外部服务，通过环境变量注入密钥，保障安全；</li><li>工具适配：适配Playwright、Chrome DevTools、Docker等工具，为工具执行层提供底层支撑；</li><li>存储适配：支持本地文件系统、NAS、私有云存储，默认将记忆、日志、任务数据存储在本地，保障数据主权[superscript:2]。</li></ul><h4>2.2.5 用户交互层：便捷操作入口</h4><p>提供多维度的用户交互方式，适配不同用户的使用习惯，核心交互方式包括[superscript:3]：</p><ul><li>命令行交互（CLI）：提供完整的CLI命令，支持安装、部署、启动、发送消息、调用技能等操作，适合技术开发者；</li><li>多平台消息交互：通过Telegram、Slack、Email等常用平台交互，无需额外安装客户端，适合非技术用户；</li><li>Web UI（可选）：支持通过Web界面管理OpenClaw，配置模型、技能、权限，查看任务进度与日志[superscript:3]。</li></ul><h2>三、OpenClaw 工程化实现与实操指南</h2><p>OpenClaw 的工程化设计聚焦“易部署、易维护、易扩展”，支持本地部署、自托管，提供完整的CLI工具与配置指南，以下是从环境准备到基础使用的完整实操流程，可直接复用[superscript:3]。</p><h3>3.1 环境准备</h3><h4>3.1.1 基础环境安装</h4><ol><li>安装Node.js：确保版本≥22，推荐通过nvm安装（避免版本冲突）；</li><li>安装包管理器：推荐pnpm（<code>npm install -g pnpm</code>），也可使用npm；</li><li>安装Docker（可选）：用于沙箱化运行系统命令，避免权限风险；</li><li>安装Ollama（可选）：用于集成本地模型，实现完全离线运行[superscript:3]。</li></ol><h4>3.1.2 模型API密钥准备</h4><p>OpenClaw 采用“用户自带API密钥”模式，需提前准备对应模型的API密钥（如OpenAI API Key、Anthropic API Key），本地模型无需API密钥[superscript:3]。</p><h3>3.2 安装与部署</h3><h4>3.2.1 快速安装（推荐）</h4><p>通过npm/pnpm全局安装OpenClaw，适合快速上手[superscript:3]：</p><pre><code class="bash"># npm安装
npm install -g openclaw@latest

# pnpm安装（推荐）
pnpm add -g openclaw@latest</code></pre><h4>3.2.2 安装守护进程（可选）</h4><p>安装网关守护进程（launchd/systemd user service），让OpenClaw 持续运行，重启系统后自动启动[superscript:3]：</p><pre><code class="bash">openclaw onboard --install-daemon</code></pre><h4>3.2.3 从源码部署（开发者）</h4><p>适合需要二次开发、自定义技能的开发者[superscript:3]：</p><pre><code class="bash"># 克隆源码仓库
git clone https://github.com/openclaw/openclaw.git
cd openclaw

# 安装依赖
pnpm install

# 构建UI（首次运行自动安装UI依赖）
pnpm ui:build
pnpm build

# 安装守护进程
pnpm openclaw onboard --install-daemon

# 开发模式（实时重载）
pnpm gateway:watch</code></pre><h3>3.3 基础配置与验证</h3><h4>3.3.1 启动网关</h4><p>启动OpenClaw 网关，监听指定端口，开启 verbose 模式便于调试[superscript:3]：</p><pre><code class="bash">openclaw gateway --port 18789 --verbose</code></pre><h4>3.3.2 快速验证</h4><p>发送测试消息，验证OpenClaw 是否正常运行[superscript:3]：</p><pre><code class="bash"># 发送测试消息（替换为自己的接收渠道，如Telegram号码）
openclaw message send --to +1234567890 --message "Hello from OpenClaw"</code></pre><h4>3.3.3 核心配置（可选）</h4><p>通过配置文件调整模型、权限、技能等参数，核心配置文件为<code>~/.openclaw/config.json</code>，常用配置示例[superscript:4]：</p><pre><code class="json">{
  "model": {
    "default": "anthropic/claude-4o",
    "apiKey": "你的Anthropic API Key",
    "fallbacks": ["openai/gpt-4o-mini"]
  },
  "security": {
    "fileSystem": {
      "allowedPaths": ["/home/user/documents", "/home/user/projects"],
      "blockedPaths": ["/etc", "/root", "/var"]
    },
    "exec": {
      "host": "sandbox",
      "security": "allowlist",
      "ask": "always"
    }
  },
  "skills": {
    "enabled": ["file-organizer", "browser-automation"]
  }
}</code></pre><h3>3.4 常用操作示例</h3><h4>3.4.1 文件整理任务</h4><p>指令：“整理我的下载文件夹，按文件类型（文档、图片、视频）创建子目录，将对应文件移动到对应目录”，OpenClaw 会自动执行文件系统操作，无需人工介入。</p><h4>3.4.2 浏览器自动化任务</h4><p>指令：“打开GitHub官网，截图当前页面，并保存到我的图片文件夹”，执行命令示例[superscript:1]：</p><pre><code class="bash">openclaw agent --message "Open GitHub, take a snapshot, and save it to ~/Pictures" --thinking high</code></pre><h4>3.4.3 定时任务配置</h4><p>配置每日8:00发送晨间简报，整合天气、日程、GitHub动态[superscript:1]：</p><pre><code class="bash"># 通过Cron命令配置定时任务
openclaw cron add --expression "0 8 * * *" --message "生成今日晨间简报，包含天气、我的日程和GitHub动态，发送到我的Telegram"</code></pre><h2>四、OpenClaw 核心应用场景落地</h2><p>OpenClaw 的强执行能力与高可扩展性，使其适配个人、团队、企业等多类场景，覆盖生产力提升、技术开发、自动化运营等多个领域，以下是典型场景的落地案例与量化效果superscript:1。</p><h3>4.1 个人生产力自动化</h3><ul><li>文件管理自动化：自动整理下载文件夹、桌面文件，按类型/日期分类，节省每日1-2小时人工时间；</li><li>晨间简报生成：每日定时整合天气、日程、新闻、健康数据（如Whoop），生成可视化报告，推送至指定通道；</li><li>知识整理自动化：自动抓取网页文献、整理笔记，生成Markdown文档，同步到Obsidian等笔记工具；</li><li>生活助手：自动预订会议室、设置日程提醒、查询快递、控制智能家居（如提前开启空调）[superscript:1]。</li></ul><h3>4.2 技术开发场景</h3><ul><li>代码审查与部署：通过Slack发送PR链接，OpenClaw 自动拉取代码、运行测试套件、分析diff、生成审查意见，通过所有检查后自动合并部署；</li><li>开发环境自动化：自动配置开发环境、安装依赖、启动服务，避免重复操作；</li><li>数据抓取与分析：自动化抓取网页数据、接口数据，整理为结构化格式（CSV/JSON），生成分析报告[superscript:4]。</li></ul><h3>4.3 企业级自动化场景</h3><ul><li>销售数据分析自动化：传统流程需5.5小时人工（导出数据→整理→计算→制图→发送报告），OpenClaw 仅需10.5分钟即可完成全流程，效率提升31倍[superscript:4]；</li><li>客户服务自动化：自动整理邮件、回复常规咨询、标记紧急邮件，节省客服2小时/天人工时间[superscript:4]；</li><li>部署监控：定时检查服务状态，出现异常时自动重启服务，并发送告警消息给管理员[superscript:4]。</li></ul><h3>4.4 特色场景案例</h3><ul><li>加密货币情绪交易机器人：集成Twitter/X API与交易所接口，持续监控特定币种的社会情绪指标，当情绪得分与价格突破预设阈值时自动执行交易，通过Telegram推送实时仓位更新[superscript:1]；</li><li>健康数据每日简报：连接Whoop健康监测API，每日生成睡眠、恢复指数、活动量的可视化报告，结合天气数据给出当日训练建议，通过晨间消息推送[superscript:1]；</li><li>SEO内容自动化管道：端到端完成内容营销——研究关键词趋势→生成文章大纲→撰写草稿→优化元标签→发布至CMS→提交搜索引擎索引，部分用户报告有机流量增长200%+[superscript:1]。</li></ul><h2>五、OpenClaw 安全机制解析</h2><p>OpenClaw 具备系统级访问权限，其安全设计的核心是“权限管控+风险隔离”，通过多层安全机制，规避权限滥用、数据泄露、系统破坏等风险，核心安全机制如下[superscript:4]。</p><h3>5.1 权限控制机制</h3><ul><li>文件系统权限白名单：默认仅允许访问用户指定的目录，通过配置文件设置<code>allowedPaths</code>与<code>blockedPaths</code>，禁止访问系统敏感目录（如/etc、/root）[superscript:4]；</li><li>系统命令权限管控：支持“白名单模式”，仅允许执行预设的安全命令，危险操作（如rm -rf /）需要用户明确批准[superscript:4]；</li><li>角色权限隔离：多用户使用时，可按角色分配权限（如普通用户仅能执行文件操作，管理员可执行系统命令），避免权限滥用[superscript:4]。</li></ul><h3>5.2 风险隔离机制</h3><ul><li>沙箱化运行：系统命令可通过Docker容器隔离运行，容器内仅包含必要的依赖，即使执行恶意命令，也不会影响宿主系统[superscript:1]；</li><li>命令审核机制：危险操作默认触发用户确认，可配置<code>ask=always</code>，所有系统级操作都需要用户明确批准后才能执行[superscript:4]；</li><li>错误隔离：单个技能、工具的执行错误不会影响OpenClaw 核心运行，核心微核会自动捕获错误，反馈给用户并尝试恢复[superscript:2]。</li></ul><h3>5.3 数据安全机制</h3><ul><li>本地存储优先：所有记忆、日志、任务数据默认存储在用户本地，不上传至任何云端服务器，保障数据主权[superscript:2]；</li><li>敏感信息加密：API密钥、用户隐私信息等敏感数据，采用加密方式存储，避免明文泄露[superscript:4]；</li><li>日志审计：记录所有操作日志（用户指令、工具执行、权限变更），便于追溯异常操作，排查安全风险[superscript:4]。</li></ul><h2>六、OpenClaw 技术局限与未来展望</h2><h3>6.1 当前技术局限</h3><p>尽管OpenClaw 具备强大的执行能力，但仍存在一些技术局限，主要集中在成本、稳定性、易用性三个方面superscript:1：</p><ul><li>API成本较高：重度使用云端模型（如Claude Opus 4.5）时，Token消耗较大，用户月支出可达$50-200，单日费用甚至可能超过$100superscript:1；</li><li>延迟问题明显：复杂任务的多步工具调用（如多平台数据抓取+分析+报告生成），可能产生5-30秒的响应延迟，影响用户体验[superscript:1]；</li><li>错误累积风险：长链条自主任务中，单步操作错误（如文件路径错误、API调用失败）可能导致后续动作偏离目标，且无法自动修正[superscript:1]；</li><li>平台依赖风险：WhatsApp等非官方集成通道，存在被平台封禁的风险，影响多通道交互的稳定性[superscript:1]；</li><li>学习曲线陡峭：部署、配置、自定义技能需要一定的技术背景，非技术用户上手难度较大[superscript:4]。</li></ul><h3>6.2 未来技术展望</h3><p>结合OpenClaw 官方规划与AI Agent赛道的发展趋势，其未来演进方向主要集中在多代理协作、安全增强、成本优化、生态完善四个方面superscript:1：</p><ul><li>多代理协作：通过Session工具实现多个OpenClaw 实例间的通信与任务委派，拆解复杂任务（如市场分析→数据收集Agent+分析Agent+报告生成Agent）superscript:1；</li><li>安全增强：引入形式化验证技术，对技能代码进行静态分析，缓解供应链安全风险；完善细粒度权限管理，实现文件级、命令级的精准权限控制superscript:1；</li><li>成本优化：优化模型路由机制，根据任务复杂性自动选择高性价比模型；加强本地模型集成与优化，实现简单任务离线运行，降低云端API依赖[superscript:4]；</li><li>生态完善：搭建技能市场平台，实现开发者技能的交易与分发；推出企业级私有化部署套件，满足行业合规要求，提供SLA服务保障[superscript:4]；</li><li>边缘计算优化：针对Raspberry Pi等低功耗设备，推出轻量化部署版本，拓展边缘计算场景[superscript:1]；</li><li>MCP协议集成：与Model Context Protocol生态对接，标准化工具调用接口，提升与其他AI Agent产品的兼容性[superscript:1]。</li></ul><h2>七、总结</h2><p>OpenClaw 作为2026年AI Agent赛道的开源标杆，以“本地优先、强执行、高可扩展”的核心特性，重新定义了个人AI代理的能力边界——它不再是单纯的“问答工具”，而是能够主动执行任务、自动化复杂工作流、保障数据主权的“实干型”数字员工。</p><p>从技术架构来看，OpenClaw 的“微核+插件+统一网关”设计，实现了核心与功能的解耦，既保证了系统的稳定性，又提升了可扩展性；多通道消息网关、工具执行层、记忆系统、自主调度系统四大核心模块的协同工作，赋予了其强大的执行能力与个性化适配能力；完善的安全机制，则解决了系统级访问的权限风险与数据隐私问题。</p><p>从实战价值来看，OpenClaw 适配个人、团队、企业等多类场景，能够大幅提升工作效率，降低人工成本，尤其是在文件管理、浏览器自动化、定时任务、代码审查等场景，其量化效果显著。尽管目前仍存在API成本高、延迟明显、学习曲线陡峭等局限，但随着多代理协作、本地模型优化、技能生态完善等方向的演进，OpenClaw 有望成为个人与企业自动化的核心工具。</p><p>对于开发者而言，OpenClaw 开源、可扩展的特性，为AI Agent的二次开发、自定义技能开发提供了良好的基础；对于普通用户而言，随着易用性的提升，OpenClaw 有望走进更多人的日常工作与生活，真正实现“AI替人干活”的愿景。</p><p>总体而言，OpenClaw 不仅是一款优秀的开源AI代理产品，更是AI Agent技术从“理论”走向“实战”的重要实践，其核心技术与设计理念，为后续个人AI代理的开发提供了重要的参考与借鉴。</p>]]></description></item><item>    <title><![CDATA[高铁断网、卫星失联：在7.6km/s的速度面前，OFDM彻底崩了？ 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047598952</link>    <guid>https://segmentfault.com/a/1190000047598952</guid>    <pubDate>2026-02-07 20:01:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>01 引言：速度的代价</h4><p>你有没有过这种体验：高铁刚加速到 350，手机信号就变成了“薛定谔的状态”；至于马斯克的星链（Starlink），目前还得靠那个巨大的“锅”来追踪卫星。</p><p>为什么我们离真正的“天地一体化”通信还这么远？</p><p>答案藏在物理层最底层的数学里。<strong>在 6G 时代，我们面临的对手不再是衰落，而是——速度。</strong></p><h4>02 OFDM 的“阿喀琉斯之踵”：脆弱的正交性</h4><p>5G 的王者是 OFDM（正交频分复用）。它的核心信仰是“正交性”——成千上万个子载波像训练有素的仪仗队，虽然站得很近，但互不踩脚（干扰）。</p><p>但在数学上，这个“互不踩脚”有一个严苛的前提：<strong>频率必须绝对精准。</strong></p><p>一旦终端高速移动，<strong>多普勒效应（Doppler Effect）</strong> 就会像一场地震。</p><ul><li><strong>物理层视角：</strong> 接收端的载波频率发生了漂移 &amp;dollar;\Delta f&amp;dollar;。</li><li><strong>后果：</strong> 数学上的正交积分不再为 0。子载波之间开始“打架”，这就叫 ​<strong>ICI（载波间干扰）</strong> ​。</li></ul><p>这就好比仪仗队正在走正步，突然地面剧烈晃动，每个人都撞到了旁边的人。此时，无论你发射功率开多大，信噪比（SNR）都上不去——<strong>因为干扰来自你自己。</strong></p><h4>03 第一关：高铁 (350 km/h) —— 勉强维持的“创可贴”</h4><p>在 350km/h（约 97m/s）的高铁上，如果我们用 3.5GHz 频段，多普勒频移大约是 ​<strong>1.13 kHz</strong>​。</p><p>看着不大？但对于 5G 常用的 30kHz 子载波间隔（SCS）来说，这已经是 <strong>3.7%</strong> 的误差。在通信物理层，1% 的偏差往往就是生与死的界限。</p><p><strong>5G 是怎么硬扛的？</strong> 简单粗暴：<strong>加宽路面。</strong></p><p>也就是增大 SCS（比如开到 60kHz 甚至 120kHz）。路宽了，这点频偏就不显眼了。</p><p><strong>但这是有代价的（Trade-off）：</strong></p><p>SCS 变大 $\rightarrow$ 符号长度变短 $\rightarrow$ 循环前缀（CP）变短。</p><p><strong>CP 变短意味着什么？</strong> 意味着抗多径能力下降。你跑赢了速度，却可能输给了回声。这是一场拆东墙补西墙的博弈。</p><h4>04 第二关：低轨卫星 (7.6 km/s) —— 物理层的“地狱模式”</h4><p>如果说高铁是“困难模式”，那低轨卫星（LEO）就是物理层的“地狱模式”。</p><ul><li><strong>速度：</strong> 7.6 km/s。这是高铁的 ​<strong>78 倍</strong>​，逼近第一宇宙速度。</li><li><strong>频段：</strong> Ka/Ku 波段（20GHz+），频率更高，多普勒效应被成倍放大。</li><li><strong>频移：</strong> 轻松突破 ​<strong>500 kHz</strong>​。</li></ul><p><strong>最恐怖的还不是“快”，而是“变”。</strong></p><p>在高铁上，频移相对稳定；但在卫星过顶的几分钟里，多普勒频移是从 +500kHz 迅速滑向-500kHz 的。</p><p>这就导致了一个致命问题：<strong>相干时间（Coherence Time）崩塌。</strong></p><p>$$
T_c \approx \frac{0.423}{f_d}
$$</p><p>当频移极大时，相干时间极短。短到什么程度？<strong>短到在一个 OFDM 符号还没传完，信道就已经变了。</strong></p><p>这时候，传统的“导频估计信道”完全失效——你刚测完信道，想发数据，发现信道已经“过期”了。这就好比你看着地图开车，但地图每 0.1 秒就随机刷新一次，这车怎么开？</p><h4>05 破局：从“对抗”到“利用”</h4><p>在 6G 的愿景里，我们要直连卫星，要坐着超音速飞机上网。OFDM 这套“甚至怕走路太快”的架构，显然已经到了极限。</p><p>怎么办？物理层工程师开始了一场思维革命：</p><p><strong>既然多普勒消不掉，为什么不把它当成信道的一个“特征”？</strong></p><p>这就是近期学术界炸裂的 <strong>“时延-多普勒域”（Delay-Doppler Domain）</strong> 技术——​<strong>OTFS/AFDM</strong>​。</p><p>在这个全新的域里，那些狂暴的时变信道，竟然变得像静止一样温顺。</p><p>而在工程实现上，为了捕捉这些要在非整数时刻采样的信号，一个经典的 DSP 算法再次封神——​<strong>Farrow 滤波器</strong>​。它不仅能处理分数倍时延，更是 FPGA 上实现高动态信道补偿的算力基石。</p><blockquote><p>“欢迎关注公众号 <strong>3GPP仿真实验室</strong>！这里是通信算法工程师的加油站。</p><p>我们不搬运新闻，只输出<strong>可运行的代码</strong>和<strong>深度标准解读</strong>。</p><p>👇 <strong>新人见面礼（后台回复关键词获取）：</strong></p><p>回复【LDPC】：获取 5G NR LDPC 编解码 MATLAB 代码（含注释）。<br/>回复【工具】：通信人减负神器：5G NR 帧结构与频点一键生成器（Python+Excel+Web三版）。<br/>回复【Pytorch】：获取 5G NR OFDM 链路 Pytorch 教学代码（含注释），助力人工智能 + 通信</p><p>让我们一起探索 6G 的无限可能。</p></blockquote>]]></description></item><item>    <title><![CDATA[揭秘Cookie操纵：深入解析模拟登录与维持会话技巧 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047598955</link>    <guid>https://segmentfault.com/a/1190000047598955</guid>    <pubDate>2026-02-07 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在互联网世界中，Cookie扮演着至关重要的角色，它不仅能够记录用户的浏览习惯，还能实现网站的个性化服务。本文将深入解析如何通过Cookie模拟登录并维持网站会话状态，帮助读者全面了解这一技术。</p><p>首先，我们来了解一下Cookie验证的原理。Cookie是一种在用户浏览器中存储的小型文本文件，它包含了网站与用户之间的交互信息。当用户访问网站时，服务器会将Cookie发送到用户的浏览器，浏览器将这些信息存储起来。当用户再次访问同一网站时，浏览器会将这些Cookie发送回服务器，从而验证用户的身份。</p><p>获取Cookie的方法有很多种，其中最常见的是使用浏览器工具。例如，Chrome浏览器的开发者工具可以帮助我们查看和修改Cookie。此外，Python库如<code>requests</code>和<code>BeautifulSoup</code>以及自动化测试工具Selenium等，都能帮助我们获取和操作Cookie。</p><p>在模拟登录过程中，我们可以通过字典传递的方式来设置Cookie。以下是一个简单的示例：</p><pre><code class="python">cookies = {
    'username': 'testuser',
    'password': 'testpassword'
}
response = requests.get('http://example.com/login', cookies=cookies)</code></pre><p>除了字典传递，我们还可以使用<code>CookieJar</code>来管理Cookie。<code>CookieJar</code>是一个专门用于存储Cookie的容器，它可以方便地添加、删除和修改Cookie。</p><p>在维持会话状态方面，Selenium是一个非常强大的工具。通过Selenium，我们可以模拟真实用户的操作，实现自动登录、数据抓取等功能。以下是一个使用Selenium进行模拟登录的示例：</p><pre><code class="python">from selenium import webdriver

driver = webdriver.Chrome()
driver.get('http://example.com/login')
driver.find_element_by_name('username').send_keys('testuser')
driver.find_element_by_name('password').send_keys('testpassword')
driver.find_element_by_name('submit').click()</code></pre><p>在实战案例中，我们还需要注意Cookie的过期处理。如果Cookie过期了，用户将无法保持登录状态。因此，我们需要定期检查Cookie的有效性，并在必要时刷新或重新获取。</p><p>最后，我们必须强调Cookie安全保护的重要性。在处理Cookie时，要注意保护用户的隐私，避免泄露敏感信息。同时，尊重用户隐私，不要未经授权获取和修改用户数据。</p><p>总之，通过本文的解析，读者应该对Cookie操纵有了更深入的了解。掌握这些技巧，不仅可以提高我们的编程能力，还能在网络安全领域发挥重要作用。</p>]]></description></item><item>    <title><![CDATA[“痛点”到“通点”！一份让 AI 真正落地产生真金白银的实战指南 腾讯云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047598903</link>    <guid>https://segmentfault.com/a/1190000047598903</guid>    <pubDate>2026-02-07 19:02:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>导语｜随着数字化转型进入深水区，AI 已超越工具属性，深度融入企业运营，全链路重塑业务流程。从打通协同壁垒、自动化重复工作，到诊断流程瓶颈、迭代业务模式，AI 成为企业降本增效、构筑核心竞争力的关键引擎。本文特邀上海腾展长融董事 &amp; CTO、腾讯云 TVP 韩光祖，他将结合自身在金融与制造业的实践经验，深入剖析 AI 优化流程的现状，探讨破局之道，为商业领袖提供一份实战指南。</p><h2><strong>作者简介</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598905" alt="" title=""/></p><p>韩光祖，现任上海腾展长融董事 &amp; CTO。美国南加州大学企管硕士，曾任富邦华一銀行总部渠道与数字银行部副总裁及总部信息科技部副总裁、纬创集团 WistronITS 全球总部首席信息官 、企业资安主委、子辰国际开发(央企港银博源基金)技术顾问兼任 COO (投资）、新蛋网全球科技及委外服务总监、外资银行科技一级部(部)主管 12 年 。有 20 余年企业 IT/MIS/IS 营运经验，有 DD、私募债权融资、工业地产交易与股权转让、跨境金融财务、科技发展与创新经验。多年大型电商行业从业及银行核心系统更换经验, 熟悉信息化、数实化、商业系统分析、云架构及云迁移、电信公有云建置及开发、整合; 并熟悉研发、产品、售前、交付、售后等业务；包括专业的服务解决方案、规划、实施、建立大型资料分析、资料采集及深度学习图像物件侦测的、AI 工艺辅助决策及，熟悉企业整体战略规划与实施。</p><h2><strong>引言</strong></h2><p>根据麦肯锡 2025 年全球 AI 现状调研，至少在一个业务职能中常态化使用 AI 的企业比例已达到 88% ⁽¹⁾。然而，真正的挑战在于规模化应用，目前仅约三分之一的企业实现了 AI 在全公司的规模化部署 ⁽¹⁾。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598906" alt="" title="" loading="lazy"/></p><p>在效率提升方面，AI 的应用效果显著，但具体增幅因场景而异。例如，在数据分析和智能决策领域，平均效率提升可达 35% ⁽²⁾；而在更广泛的流程管理中，效率提升幅度在 30% 至 50% 之间 ⁽³⁾。综合来看，艾瑞咨询的数据显示，2023 年中国企业通过AI与数字化转型，整体运营效率平均提升了 27.5% ⁽⁴⁾。</p><p>然而，从技术潜力到商业价值的转化之路并非坦途。许多企业在满怀期望地拥抱 AI 时，却遭遇了技术与业务“两张皮”、组织文化阻力、数据孤岛难平、投资回报不及预期等多重困境。本文<strong>将深入剖析 AI 驱动流程再造的现状与场景，探讨从试点到规模化的破局之道。</strong></p><h2><strong>第一部分：新范式已至，AI驱动的流程再造现状与场景</strong></h2><p>AI 对业务流程的改造，已从过去的“点状”辅助，演变为如今的“链式”重构。其核心驱动力源于大语言模型（LLM）、AI 智能体（AI Agent）等技术的突破。与传统自动化技术不同，现代 AI 不仅能执行预设规则，更具备了上下文理解、逻辑推理、自主规划与跨系统协同的能力，使其能够胜任过去只有人类才能处理的复杂模糊任务。埃森哲报告显示，53% 的中国企业正利用 AI 连接并融合多个业务流程，这一比例高出全球平均水平 11 个百分点 ⁽²⁾。</p><p>在数据密集型行业中，AI 的价值正从简单的“任务执行”转向“智能决策”。以下为几个代表性行业的应用场景：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598907" alt="" title="" loading="lazy"/></p><p><strong>银行业：风险与效率的平衡重塑</strong></p><p>在我的从业经历中，银行业对风险的敬畏根深蒂固，这曾一度使其在技术采用上显得相对保守。然而，面对激烈的市场竞争与日益复杂的金融风险，AI 已成为其不得不拥抱的战略选择，正助力其重构核心的信贷与风控流程。</p><p>工商银行打造的“财务智能分析助手”，能够深度解析企业财报，自动提取关键指标，为审批人员提供决策支持，不仅将单笔业务效率提升了 20%，更形成了一种“数据+AI+辅助决策”的评审新范式 ⁽⁴⁾。同样，汇丰银行也已将大模型技术用于自动生成信贷建议书，实现了流程的再造 ⁽⁶⁾。这背后是 AI 强大的非结构化数据处理能力，它能快速“阅读”并理解财报、合同、法律文书等海量文档，将信审人员从繁琐的事务性工作中解放出来，专注于更核心的风险判断。</p><p>在反洗钱（AML）等合规领域，AI 的应用则更为深刻。德勤提出的多智能体（Multi-Agent）协作系统构想，描绘了一幅未来银行合规的蓝图：不同的 AI 智能体分别负责警报审查、交易记录分析、调查报告撰写，实现几乎无需人工干预的全自动合规监测 ⁽⁷⁾。这不仅是效率的飞跃，更是风险洞察能力的质变，AI 能从海量数据中发现人工难以察觉的隐蔽非法活动模式。</p><p><strong>保险业：客户体验与运营效率的双重革命</strong></p><p>保险业的核心在于风险定价与服务承诺，其业务流程天然与数据和概率紧密相连，为 AI 提供了广阔的应用舞台。如果说银行业的 AI 应用核心是“风控”，那么保险业的核心则是“效率”与“体验”。</p><p>智能理赔是保险业 AI 应用最成熟、价值最显著的领域。过去，车险理赔流程漫长，涉及查勘、定损、核赔等多个环节，客户体验普遍不佳。如今，以中国平安为代表的头部险企，通过 AI 图像识别技术，实现了“拍照即定损”。客户只需上传事故照片，AI 便能快速识别损伤部件、评估维修成本，整个定损流程可在 30 分钟内完成 ⁽⁵⁾。这不仅大幅提升了客户满意度，也有效降低了运营成本。2025 年上半年，平安产险通过 AI 技术实现的智能化反欺诈拦截，就为公司减少了高达 64.4 亿元的损失 ⁽⁵⁾。</p><p>在理赔之外，AI 正向保险价值链的前端——承保与定价环节渗透。摩根士丹利的报告预测，AI 将在第二阶段通过优化风险选择和定价精准度，深刻改变保险公司的盈利模式 ⁽⁸⁾。这意味着，未来的保险产品将更加个性化，保费将根据每个客户的实时风险状况进行动态调整。这不仅要求保险公司具备强大的数据处理能力，更对其 AI 建模与治理能力提出了极高要求。</p><h2><strong>第二部分：工具方法论，BPMN三部曲，让流程改造有章可循</strong></h2><p>在 AI 驱动的流程再造中，企业往往面临“无从下手”的窘境。我们引入标准的 BPMN（业务流程建模与标注）方法论，将改造分为三个关键阶段：评估、识别与提升。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598908" alt="" title="" loading="lazy"/></p><p><strong>第一阶段：Assess 评估</strong></p><p>透过标准流程建模语言 BPMN，我们能够清楚掌握流程执行的步骤与责任单位。这个阶段的目的是辨识目前流程的瓶颈与耗时情形，进而量化整体的备案时间或工作负载。这个阶段由流程分析师与业务部门代表牵头，产出 BPMN 流程图与问题诊断报告。</p><p><strong>第二阶段：Clarify 识别与洞察建议</strong></p><p>在这个阶段，我们会结合统计分析和实务经验来厘清流程问题的根本原因，并判断其一致性与影响程度。</p><p>举例来说：若某项作业平均处理时效不佳，我们会进一步找出是否因为缺乏资讯、等待其他单位处理，或是人工干预过多导致延迟。例如 Pending Code 设定流程中需等待主管确认，这就是典型的瓶颈。</p><p>这个阶段应加入数位团队与 Beyond Lab 共同深挖问题成因、明确改善方向，重点在于找出可行的建议方向与数据佐证，为后续改善做准备。</p><p><strong>第三阶段：Expedite 提升与最终方案</strong></p><p>当我们掌握问题根因后，下一步就是提出改善对策，透过 IPA 等技术手段，优化流程达到提效与自动化的目的。常见的工具包含 AI + RPA、自动化输入、表单精简、资料结构重整等。 这个阶段由流程分析师与技术部门协作，落地改善方案并推动自动化执行。</p><p>最终，我们期望透过这样的流程改善，可以提升子阶段的处理效率，例如 C_01 时找阶段耗时减少 Y%，或是整体流程的提升，例如 C_04 阶段作业时间减少 Z%。这些成果可以量化呈现，有效支持业务流程持续优化。</p><p>这套以 BPMN 为核心的方法，不仅是流程建模工具，更能一步步协助业务单位从“看见问题”走向“提出解方”，扎实推进数字化转型与流程优化。</p><h2><strong>第三部分：实施的熔炉，企业落地AI流程优化的痛点与破局之道</strong></h2><p>尽管 AI 流程优化的前景广阔，但通往成功的道路上布满了荆棘。波士顿咨询集团的研究指出，约 70% 的 AI 项目挑战源于“人与流程”问题，而非技术本身 ⁽⁹⁾。在我看来，企业在落地 AI 时，往往会陷入三大核心痛点，而破解这些痛点，需要系统性的“组合拳”。</p><p><strong>痛点一：组织与文化的“惯性之墙”</strong></p><p>技术可以快速迭代，但人的认知与组织的文化却根植深厚。这是企业在AI转型中面临的最大、也最隐蔽的阻力。</p><p>Gartner 的调查显示，45% 的 CEO 表示其大部分员工对 AI 持抵触态度，甚至公开敌视 ⁽¹⁰⁾。</p><p>这种恐惧源于对“被替代”的担忧。在实际项目中，即便技术方案完美，如果一线员工不配合提供数据或在流程中设卡，项目也将举步维艰。</p><p><strong>破局之道：</strong></p><ul><li>顶层设计定调“人机协同”：管理层必须明确 AI 是赋能工具而非人类的替代者，战略目标应聚焦于创造力提升，而非单纯的人力削减。</li><li>建立“翻译官”与“布道者”团队：在技术团队与业务团队之间，需要既懂技术又懂业务的“翻译官”，将业务痛点转化为 AI 可以解决的问题。同时，需要在组织内部培养“布道者”，通过分享成功案例、组织培训，将 AI 的价值清晰地传递给每一位员工。</li><li>从“边缘”到“核心”的渐进式变革：优先选择非核心但痛点明确的流程（如自动生成会议纪要）取得“小型胜利”（Quick Wins），以此建立组织信任。</li></ul><p><strong>痛点二：技术与业务的“价值鸿沟”</strong></p><p>许多企业容易陷入“为了 AI 而 AI”的误区，或是面临严重的数据孤岛。清理数据的成本往往远超模型开发成本，尤其在金融领域，数据安全合规更是悬在头顶的“达摩克利斯之剑” ⁽⁵⁾。</p><p><strong>破局之道：</strong></p><ul><li>业务问题驱动，而非技术驱动：AI 项目的起点，必须是明确的、可量化的业务问题。例如，目标是“将信贷审批时间缩短 50%”，而不是“应用一个大语言模型”。以业务价值为导向，倒推所需的技术方案与数据支持。</li><li>建立企业级数据与 AI 中台：这是破解数据孤岛、实现能力复用的关键基础设施。数据中台负责统一数据治理、保证数据质量与安全；AI 中台则提供标准化的模型开发、训练、部署工具，将 AI 能力以 API 服务的形式赋能给各个业务部门，避免重复“造轮子”。</li><li>构建敏捷的“流程-技术”闭环：企业应建立一个由业务专家、数据科学家、IT 工程师组成的敏捷团队，形成“业务反馈-模型调优-流程改进”的快速迭代闭环，确保 AI 应用始终紧贴业务需求。</li></ul><p><strong>痛点三：流程重构的“最后一公里”</strong></p><p>即便拥有了先进的技术和清晰的业务目标，如果不能对现有业务流程进行彻底的重构，AI 的潜力也无法完全释放。这“最后一公里”的改造，往往最为艰难。</p><p>以保险理赔为例，引入 AI 图像定损技术后，如果后续的核赔、支付、客户沟通等环节依然沿用旧的流程，那么整体效率的提升将非常有限。真正的变革，需要将 AI 能力嵌入到端到端的全流程中，实现从报案到支付的“直通式处理”（Straight-Through Processing）。</p><p><strong>破局之道：</strong></p><ul><li>以“零基”思维重构流程：流程重构不能满足于在现有基础上修修补补，而应采取“零基思维”（Zero-Based Thinking），假设从零开始设计一个流程，思考在 AI 的加持下，它应该是什么样子。这有助于摆脱历史包袱，进行颠覆式创新。</li><li>投资于人的“再技能化”：对员工进行“再技能化”（Reskilling）培训，使其掌握与新流程、新工具相匹配的能力，如数据分析、人机交互、AI 模型监督等。这不仅是化解抵触的方式，更是人才储备的过程。</li><li>建立稳健的 AI 治理框架：建立覆盖模型全生命周期的 AI 治理框架，确保 AI 系统的公平性、透明度、可解释性和可审计性，这既是满足监管的要求，也是建立客户与市场信任的基石。</li></ul><h2><strong>第四部分：组织中枢，流程数字化卓越中心 (P.T.C.O.E)</strong></h2><p>企业高层应组建跨组织、多专业的长期治理 + 能力平台（流程改造 COE），采用 7/3 或 8/2 矩阵式 KPI 管理，确保流程改造持续、可复制、可量化—— 其核心价值是将流程优化从一次性项目升级为公司级能力，避免单一部门主导导致的落地低效、资源内耗问题（如 LLM 流程优化反复 POC 却因人才缺口、实务经验不足陷入循环）。</p><p>作为公司流程治理、方法论、数字化与绩效管理的中枢，BPMN 流程改造 COE 需承担流程治理与标准制定、方法论与 To-Be 设计、绩效 KPI 管理、数字化自动化协同、人才培育与变革管理等核心职责，角色配置应涵盖 COE 流程长、流程架构师、流程分析师、数字化流程顾问及 BU Process Owner，推动流程改造从项目化走向制度化、数据化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598909" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598910" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598911" alt="" title="" loading="lazy"/></p><p>典型车险流程</p><p>总之，BPMN 流程改造 COE 定位为流程治理与 EA 的中枢枢纽，上承战略与内控要求，中接 BPMN 流程设计，下连 IT、数据与自动化能力。其核心路径是先建立全公司统一的流程治理标准，将流程 KPI 深度植入员工认知；再通过 Process Mining 与 AI 技术优化，全方位覆盖数据 ASIS-TOBE 分析、人员组织调整、共享应用系统迭代及模块协同优化，最终让流程成为驱动公司增长的核心资产，而非局限于单次项目成果。</p><h2><strong>第五部分：流程挖掘规划蓝图与场景应用</strong></h2><p>如果说前文的 BPMN 三部曲为我们提供了重塑流程的手术刀，那么流程挖掘则是指引手术路径的导航雷达。</p><p>业务流程优化绝非盲目的技术堆叠，而是需要将 AI 的算力精准注入到价值链的最深处。通过“流程挖掘六步法”，我们将抽象的技术方案具象化为可落地的场景，确保 AI 不仅仅停留在实验室的 POC 阶段，而是真正转化为支撑供应链、理赔及运营各环节的增长驱动力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598912" alt="" title="" loading="lazy"/></p><p>用金融行业举例，流程挖掘不仅能在理赔流程发挥作用，还能在保险公司运营的各个环节中发挥作用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598913" alt="" title="" loading="lazy"/></p><p>针对集团流程与数字化管理部牵头的业务场景，应该由流程挖掘项目组、数据湖团队协同支撑，供应链等各流程归口业务部门参与，以流程挖掘六步法为核心，统筹各方落地流程优化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598914" alt="" title="" loading="lazy"/></p><h2><strong>结语：拥抱变革，行稳致远</strong></h2><p>AI 已成为企业创新的核心引擎，但要实现大规模深度价值转化，企业需具备前瞻战略视野与系统化推进机制，而非陷入各部门无效的 LLM POC 内耗。企业应坚守“业务引领、科技赋能”原则，优先选择增收场景突破以获取高层信任；尤其刚上任的 CTO/CDO/CIO，需补足业务认知，避免因脱离业务导致信任内耗，核心是通过科技打通资产端与资金端、提升盈利，这是企业生存的根本。凡是不能赋能营收的系统，都是没规划好的失败，只有让 AI 赋能核心营收场景，企业 AI 数字化转型才能实现从烧钱到造血的质变。</p><p>AI 驱动的业务流程优化绝非简单技术升级，而是一场深刻的系统性变革，考验企业的技术实力、战略远见、组织韧性与文化魄力。从我个人观察来看，成功企业无一不是将 AI 视为重塑核心竞争力的战略引擎，并勇于彻底自我革新。前路虽有挑战，但机遇更大。对中国企业而言，无需观望或盲目跟风，应结合自身业务特点与发展阶段，找准切入点，小步快跑、快速迭代，稳健开启 AI 流程再造之旅：从解决具体 “痛点” 入手，逐步打通业务 “堵点”，最终将 AI 内化为驱动持续创新与增长的 “通点”—— 这正是 AI 浪潮下企业基业长青的必经之路。</p>]]></description></item><item>    <title><![CDATA[『NAS』部署一个电子书阅读器-Reader 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047598917</link>    <guid>https://segmentfault.com/a/1190000047598917</guid>    <pubDate>2026-02-07 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=2OSXTLcnVQB82Wfs6xFK5g%3D%3D.XTGQoPNPjJWRTGvLKfaCVgjDI%2F5%2F2C%2BJ0J1pO%2Bm5BcT8IIAWzp8eyStKBwlB7hpfgboETjqyUo0Pts7MULoY8tt8xPz90N5dnoB4tGghrfI2xsHZzpP0VbSlfhuG23qis0Zu7nrR8Maz5OVtwkjbPMqg%2F%2BN%2FC88FoiFrE1Bx0Zk%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>Reader 是一款开源免费的自托管全能阅读工具，它整合了网络小说阅读、RSS 资讯订阅、网页内容抓取三大核心功能，内置丰富书源与订阅接口。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598919" alt="" title=""/></p><p>本次使用飞牛 NAS，其他品牌的 NAS 操作流程也差不多。</p><p>打开“文件管理”，在”docker“目录下创建一个”reader“文件夹，然后在”reader“里再创建两个文件夹，分别是”log“和”storage“。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598920" alt="" title="" loading="lazy"/></p><p>打开”Docker“，在”Compose“里新增一个项目，填入以下信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598921" alt="" title="" loading="lazy"/></p><p>代码：</p><pre><code>services:
  reader:
    image: hectorqin/reader:latest
    container_name: reader
    ports:
      - 8080:8080
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    volumes:
      - /vol1/1000/docker/reader/log:/log
      - /vol1/1000/docker/reader/storage:/storage
    restart: always</code></pre><p>端口可以自定义，我这里使用的是 <code>8080</code>。</p><p><code>log</code> 和 <code>storage</code> 分别指向刚刚创建的2个文件夹。</p><p>等项目构建成功后，打开浏览器输入 <code>你NAS的IP:8080</code> 就能使用 Reader 了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598922" alt="" title="" loading="lazy"/></p><p>首次使用需要配置一下“书源订阅”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598923" alt="" title="" loading="lazy"/></p><p>新增订阅的名称可以随便填（你看得懂记得住就行）链接我填了这5个：</p><ul><li><a href="https://link.segmentfault.com/?enc=9HQ%2B9Gi9zFjjbgrTXIL%2BWA%3D%3D.fceGSDuz7HTAW5NswRqovh%2FUIHs5caVUEpahlbyxE5VseT584TfAUEvou8BstDkGBbbAZ77gtDbIr4yLrbVu%2BH9oy5UHvzrMpw%2BvhNw%2Bh3s%3D" rel="nofollow" target="_blank">https://source-repo.zgqinc.gq/legado3/bookSource/bookSource_1...</a></li><li><a href="https://link.segmentfault.com/?enc=7n%2BU86Larj5K0D%2B8pgnR3Q%3D%3D.7c5DMxL7OfKHktQpq7e2HT6G6OykVgKbzTd215SiqT6hAMCScR0ULZeR4%2F4tp9RherDMadZclwlhf%2F7EP8Z3RV4gChErkW%2FResPW4f9oYmU%3D" rel="nofollow" target="_blank">https://source-repo.zgqinc.gq/legado3/bookSource/bookSource_2...</a></li><li><a href="https://link.segmentfault.com/?enc=Sq%2BS8ROfUyeDF8GKZoWziw%3D%3D.HdcdeaZLc6J%2BZ7Q3nW0m7ky1bC3jbVUwAK5bXFpqpk7PaCpWOq3%2Fc6pFr1jitvzrKazelxfpBAg0qb6YoihAPAjL1u%2BqWNCveuyJuV9qWuk%3D" rel="nofollow" target="_blank">https://source-repo.zgqinc.gq/legado3/bookSource/bookSource_3...</a></li><li><a href="https://link.segmentfault.com/?enc=hjA5k170MsHRWbeadyqQUQ%3D%3D.WBs7zDmFjZOTkR%2BNM0cqMwTt33O1sXMY5gPSy8VqRQWfY579EIRtUEMPFrL5xaNT72X91x1GNLBE9qP9K4IT0XR9K406wwa%2Bzkyw480UpBE%3D" rel="nofollow" target="_blank">https://source-repo.zgqinc.gq/legado3/bookSource/bookSource_4...</a></li><li><a href="https://link.segmentfault.com/?enc=ZpLRe9nnH%2F8kUEJvW7RF1Q%3D%3D.MFdSvUNsTMfwDKio%2FNhTbnoM%2F2HKnq95tcbk%2F4hpnpG5w2iH9zbObqE1Arv9WPcyHwZZUtWxtqSCHDF0pTvf%2FvUuqBQpxO%2Bn%2B%2BjkKWvEl1E%3D" rel="nofollow" target="_blank">https://source-repo.zgqinc.gq/legado3/bookSource/bookSource_5...</a></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598924" alt="" title="" loading="lazy"/></p><p>其他书源链接在这个网站有整理，按需取👉 <a href="https://link.segmentfault.com/?enc=IuWvo%2Bsz6CfJTXaxCyCUqw%3D%3D.BAASG3JSZpeY4Hkhrxl9ywZO9zRyxkgHYgXpoSvyP44xc4rsRwYQtwaoqNorGH0S48kxF7Qov7bZg3BL8IYfHviWsCKDbpS2VAGAWeU7c2U%3D" rel="nofollow" target="_blank">https://flowus.cn/zyzyk/share/07b5bf19-2397-4065-bc1c-aecb7c0...</a></p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=X7ZluwRG14gT%2Fr%2BfiCI96g%3D%3D.lHY8BWiZSCDRrUoEk3%2B4I3d%2FMMos%2BRs7sOnIK3AZ5elD11Qtgw4dB5UJ9%2Bc6CFHYIxZFq6eBgLcftEy%2FQJxsrX4eOV7grDGtnMsNptDsWTCEhuPMiH1aVVnqCJ6UQgKonj0T%2FSmhhTLdARFAj9NweqpoLMSYbxpUrkOGBhGy5B0%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[智效跃迁，架构无界，第三届腾讯云架构师峰会圆满落幕！ 腾讯云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047598709</link>    <guid>https://segmentfault.com/a/1190000047598709</guid>    <pubDate>2026-02-07 18:04:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 12 月 27 日，由腾讯云架构师技术同盟与腾讯云 TVP 联合主办的第三届腾讯云架构师峰会在北京富力万丽酒店成功举行。本次峰会以“智效跃迁 架构无界”为主题，汇聚了众多技术领袖与架构师，共同探讨 AI 浪潮下架构师群体的价值重塑与技术变革。现场思想碰撞激烈，实践分享深入，勾勒出一幅技术人在智能浪潮中进化与赋能的新蓝图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598711" alt="" title=""/></p><h2><strong>主持人开场</strong></h2><p>主论坛在腾讯云架构师技术同盟主席 毛剑和腾讯云架构师技术同盟活动组织主席 王晓波的开场主持中拉开序幕。两位主席与现场架构师同仁们共同回顾了同盟自 2024 年 12 月 28 日成立一年来的成长历程。毛剑指出，当前正值 AI 从技术概念走向全面普及的关键节点，也是智能体应用的元年。技术浪潮带来了前所未有的机遇，同时也让架构师群体面临着“旧经验难以解决新系统”的挑战。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598712" alt="" title="" loading="lazy"/></p><p>腾讯云架构师技术同盟主席 毛剑</p><p>作为峰会的总出品人及主持人，王晓波进一步阐释了峰会议题设置的内涵：AI 带来的不仅是效率的线性提升，更是生产力和工作方式的结构性变革，当代码生成、系统运维乃至部分业务决策逐渐被 AI 接管，执行层日益自动化，架构师的职责与能力边界正在被技术重新定义。期待通过全天围绕产业判断、系统重构与角色演进的干货分享，能为与会者带来前瞻视野与清晰的发展判断，共同探寻在智能参与决策的新时代，架构师不可替代的价值所在。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598713" alt="" title="" loading="lazy"/></p><p>腾讯云架构师技术同盟活动组织主席 王晓波</p><h2><strong>主论坛：智效跃迁 架构无界</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598714" alt="" title="" loading="lazy"/></p><p>腾讯云副总裁、腾讯云架构师技术同盟品牌发展主席 徐勇州</p><p>腾讯云副总裁、腾讯云架构师技术同盟品牌发展主席 徐勇州发表致辞。他表示：我们正见证一场贯穿算力、模型到应用范式的全方位技术较量。面对如此深层的范式迁移，架构师群体站在了能力重塑的十字路口。他抛出了三个核心挑战，直指每位技术人的内心：</p><p>● 在 AI 能生成更多代码的背景下，如何重新定位我们的核心创造力？</p><p>● 在新时代下，如何升级自己的能力模型？</p><p>● 如何在复杂多变的多智能体与分布式系统中，构建真正可度量、可观测、可演化的架构？</p><p>他强调，此次峰会的目的不仅是知识分享，更是为了校准航向。与此同时，徐勇州重申了同盟创立的初心：技术的发展从来不是单打独斗，唯有交流才能提速，唯有共创才能共赢。腾讯云架构师技术同盟正是希望在这个变革年代，成为架构师们的同行者，汇聚群体力量。</p><p>随后，他回顾了同盟成立一周年来的坚实足迹：在七大城市建立地区同盟，汇聚了超千名顶级架构师；举办了 40 余场地区交流活动；在社区中产出了 5218 篇深度技术文章，解答了 934 个技术难题；通过《架构之道》刊物、系列直播对话，持续点燃群体智慧。展望未来，徐勇州期待同盟能持续升级，拓展地域，真正成为全国每一位架构师的社交场、学习圈和进化阶梯。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598715" alt="" title="" loading="lazy"/></p><p>香港科技大学（广州）协理副校长、讲座教授，腾讯云TVP 熊辉</p><p>香港科技大学（广州）协理副校长、讲座教授，腾讯云 TVP 熊辉的演讲《认知升维：人工智能奇点期的产业新坐标》带来了高屋建瓴的产业分析。他犀利地指出，当前制约AI发展的最底层逻辑是“电”，而非单纯的算力。熊辉教授鼓励架构师成为“人机混合新型劳动力”，并给出个人发展的关键建议：投身于数据基础差、采集难的“朝阳行业”，因为在那里人的经验价值更易积累；同时，要善用 AI 作为“学习伴侣”，提升效率，并重点培养可意会不可言传的“差异化能力”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598716" alt="" title="" loading="lazy"/></p><p>腾讯云副总裁、腾讯智慧零售技术架构和产研负责人 程伟</p><p>腾讯云副总裁、腾讯智慧零售技术架构和产研负责人 程伟分享了《企业智能体的创新价值与行业实践》。他基于服务上百家零售企业的观察，指出企业智能体落地已从对话与简单工作流，迈向与业务深度结合的复杂场景。针对智能体在企业落地，他指出还需打赢模型性能、数据治理、业务耦合、安全防护等六场攻坚战，并介绍了腾讯云智能体战略以及“智能体场景罗盘”等工具，帮助企业厘清自身阶段，选择可落地的场景。程伟强调，企业要成功让 AI 价值变现，建议自下而上识别高价值场景，洞察一线员工如何使用 AI，投资可以与工作流深度集成的场景，同时做好数据质量和上下文工程建设。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598717" alt="" title="" loading="lazy"/></p><p>久痕科技创始人兼 CEO、原网易集团副总裁、腾讯云 TVP 汪源</p><p>久痕科技创始人兼 CEO、原网易集团副总裁、腾讯云 TVP 汪源在《个人数字记忆：实践、思考与挑战》中，提出了一个面向个体的未来构想。他介绍了其产品“remio”，旨在自动捕获、存储并索引个人接触的所有信息，构建本地化、高隐私的“个人数字记忆”系统。他认为，在 AI 时代，个人拥有的上下文质量将决定其竞争力。汪源也抛出了一个深刻的伦理问题：当员工离职，其在职期间积累的“数字记忆”归属公司还是个人？这将是未来必须面对的社会议题。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598718" alt="" title="" loading="lazy"/></p><p>Rokid 全球开放生态负责人｜产品、工程与生态全球高级总监 赵维奇</p><p>Rokid 全球开放生态负责人｜产品、工程与生态全球高级总监 赵维奇的演讲《AI+AR 融合的全球实践：从空间计算到个人智能体》，将视野引向硬件与交互的未来。他阐述了 AI 与 AR 互为最佳载体的关系：AI 让 AR 交互更自然，AR 则为 AI 提供了感知与连接物理世界的界面。他展示了 Rokid 在沉浸空间、轻量化数据空间等场景的落地案例，并展望了“无界交互”的未来——身边的屏幕将越来越少，可穿戴设备让智能体无形地融入生活与工作。他最后呼吁，科技应服务于人，让每个人都能享受科技带来的平等机会。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598719" alt="" title="" loading="lazy"/></p><p>年度卓越同盟——腾讯云架构师技术同盟颁奖典礼</p><p>在主论坛的尾声，会议特别设置了年度表彰环节，以回顾与嘉奖腾讯云架构师技术同盟成立一年来的丰硕成果与杰出贡献。北京同盟凭借深度的技术交流荣获 2025 年度「学习共创最佳同盟」、「思辨创新最佳同盟」，合肥同盟因为其显著的成员凝聚力荣获 2025 年度「星火汇聚最佳同盟」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598720" alt="" title="" loading="lazy"/></p><p>上海同盟入会成长理事 丁雪丰</p><p>上海同盟因其在组织建设、活动创新、知识沉淀及生态影响等方面的综合卓越表现，荣获 2025 年度「地区开拓最佳同盟」、「品牌发展最佳同盟」，并被特别授予「2025 年度全国最佳同盟」表彰。上海同盟入会成长理事 丁雪丰在感言中强调，荣誉属于所有架构师同仁，人才是最重要的，一个人可以走得很快，但一群人才能走得更远。上海同盟理事长 马俊在获奖感言中分享了其成功“密码”：海纳百川的价值观、团结的理事团队，以及让每位成员都能站到 C 位发光发热的舞台理念，才让上海同盟真正以团结实现共赢。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598721" alt="" title="" loading="lazy"/></p><p>上海同盟理事长 马俊</p><h2><strong>主题论坛：AI驱动的技术重构与业务赋能</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598722" alt="" title="" loading="lazy"/></p><p>新华社国家重点实验室总架构师、腾讯云架构师北京同盟理事 蔡昌艳</p><p>本论坛由新华社国家重点实验室总架构师、腾讯云架构师北京同盟理事 蔡昌艳出品，蔡昌艳开篇点题，指出 AI 正在重塑业务底层逻辑，技术从支撑业务转向定义业务，因此本论坛将重点探讨 AI 在技术架构与业务落地层面的深度融合。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598723" alt="" title="" loading="lazy"/></p><p>腾讯云架构师技术同盟社群管理主席 揭光发</p><p>腾讯云架构师技术同盟社群管理主席 揭光发在《AI 领导力 2.0：从超级个体到超级军团》中，分享了自身从“AI 原生”使用者到管理者的心路历程，并提出 AI 领导力的核心是将 AI 视为团队成员而非工具。他分享了从“AI 领导力 1.0”（将专业技能外包给 AI）到“2.0”（将管理能力也外包）的升级路径。面对 AI 效率过高导致人类“心力带宽”瓶颈的问题，他阐述了构建“秘书 Agent”的三层架构设想：人类作为领导者，只需与一个“秘书 Agent”交互，由它来管理下层众多执行 Agent。这种“用 AI 管理 AI”的模式，旨在通过规范、技能包和工作流封装，实现高品质产出，并降低人类的直接干预频率，最终让人回归到需求澄清与最终验收的核心角色。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598724" alt="" title="" loading="lazy"/></p><p>腾讯云互联网技术总经理 叶辉</p><p>腾讯云互联网技术总经理 叶辉探讨了《从云原生到 AI 原生》。他类比云原生时代的“零信任”安全理念，提出 AI 原生架构同样需要新的范式。并以智能客服场景为例，指出业务逻辑已从人工决策链条变为基于知识库与语义理解的自动化流程，这导致技术挑战从传统的 QPS、延迟转向对上下文、Token 消耗、幻觉管理的关注。他展望了 AI 原生架构的几个关键特征：以智能计算为中心、关注语义相关性而不仅是响应速度、以及通过统一语义层（如“本体论”）来打通业务与技术的认知隔阂。他同时指出，当前 AI 工程化面临确定性、编排复杂性、资源成本及数据隐私等核心挑战。AI 原生不仅是技术的升级，更是一种架构思维的转变。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598725" alt="" title="" loading="lazy"/></p><p>汇量科技副总裁兼首席架构师、腾讯云 TVP 蔡超</p><p>汇量科技副总裁兼首席架构师、腾讯云 TVP 蔡超的分享《基于 Multi-Agent 的重构与思考》极具技术深度。他首先辨析了静态工作流与动态工作流的适用场景：前者适合常规、顺序性任务，易于调试和成本可控；后者则能处理需要启发式探索的复杂问题，其真正价值在于持续的迭代与改进。他分享了在广告投放系统中应用 Multi-Agent（如 Campaign 分析 Agent、投放运营 Agent）的实践。最后，他提出了一个前瞻性架构设想：将系统的原子操作暴露为 MCP 协议，让 Agent 来编排上层的业务逻辑，从而构建一个可自进化的全新系统架构。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598726" alt="" title="" loading="lazy"/></p><p>自如技术副总经理、腾讯云架构师名人堂专家 应阔浩</p><p>自如技术副总经理、腾讯云架构师名人堂专家 应阔浩带来了《自如 AI 实践及架构思考》。他详细介绍了 AI 在自如租房业务中的具体落地，并总结了 AI 落地“三步走”策略——短期快速上线、中期建立平台、长期构建核心竞争力。在业务层面，他重点介绍了如“AI 找房助手”如何通过意图识别、参数提取与多轮对话，理解用户模糊需求。在研发层面，他分享了在特征工程、内容摘要生成、聚类与审核等环节应用大模型替代传统小模型的经验，实现了效果提升与成本降低。在架构层面，他描绘了包含知识库、垂直模型、MCP 网关及 Agent 管理平台的 AI 中台蓝图，为传统业务智能化提供了清晰路线图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598727" alt="" title="" loading="lazy"/></p><p>Opera 技术副总监、腾讯云架构师北京同盟成员 张建磊</p><p>Opera 技术副总监、腾讯云架构师北京同盟成员 张建磊分享了《大模型时代的推荐架构进化》。他介绍了 Opera 浏览器内容推荐系统的传统架构，传统推荐依赖特征工程，但存在语义理解浅、冷启动难、模型碎片化等问题。随后他重点讲解了大模型如何革新特征工程、内容摘要与聚类等环节，并分享了在内容生产侧的应用：利用大模型辅助自媒体作者选题与写作，以及将商品信息转化为吸引人的内容文章进行分发。他也坦诚，由于延迟和成本考量，大模型尚未深入核心的召回与排序环节，但其在内容理解与生成方面的能力已为推荐系统打开了新的赋能空间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598728" alt="" title="" loading="lazy"/></p><p>圆桌对话《AI 双轮驱动：技术突破与商业价值的闭环构建》</p><p>圆桌对话环节，腾讯云智能顾问总经理、腾讯云架构师名人堂专家 许小川，中科院计算所副教授、腾讯云架构师名人堂专家 李明宇两位嘉宾加入了分享，与演讲嘉宾围绕“AI 技术突破与商业价值的闭环构建”展开讨论。针对“如何避免自嗨式研发”，许小川强调技术与业务团队必须“一起嗨”，在选型初期就达成共识；李明宇指出应优先选择那些能从不及格提升到 70 分的场景，而非从 90 分到 100 分的“锦上添花”；揭光发则建议“等风来”与“挖好坑”结合，提前做好工程化准备。关于技术到商业的转化，张建磊的分享务实而聚焦：选择“应该做、能做、且能做好”的交集，利用 AI 放大现有业务优势（如内容生成）直接带来收入增长。最后，对于“年轻开发者可能过度依赖 VibeCoding 缺乏实战经验”的担忧，李明宇认为，技术人通过实践培养架构能力的过程依然不可替代，教育体系与持续学习必须跟上技术变革的步伐。</p><h2><strong>主题论坛：AI 赋能者，开发者的进化之路</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598729" alt="" title="" loading="lazy"/></p><p>同程旅行资深架构师、腾讯云架构师技术同盟学习交流主席 李智慧</p><p>本论坛聚焦于 AI 时代开发者与技术人的个体进化路径，由同程旅行资深架构师、腾讯云架构师技术同盟学习交流主席 李智慧出品。李智慧以三十年技术生涯的回顾开场，指出技术范式的变迁正重新定义价值产出。他提醒开发者，在 AI 带来“认知外包”和“经验贬值”的同时，系统思维能力、价值判断力和经验抽象能力正变得愈发稀缺。他建议技术人保持自信与好奇心，躬身入局，掌握提示工程、RAG 等新技能，并警惕对工具的过度依赖。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598730" alt="" title="" loading="lazy"/></p><p>联想诺谛智能首席架构师、腾讯云架构师北京同盟理事 曹洪伟</p><p>联想诺谛智能首席架构师、腾讯云架构师北京同盟理事 曹洪伟，以其跨越三十年的技术生涯为背景，分享了《大模型时代下的技术人成长》。他指出，AI 带来的不仅是工具升级，更是对软件工程全流程的重塑，从需求、设计到编码、测试均被深刻影响。过去技术人通过掌握新语言、新框架获得的“经验红利”正在大模型时代加速贬值，由此他提出技术人新时代的“核心竞争力三角”：批判性思维与评估能力、软技能与领导力、工程整合与领域知识。最后曹洪伟建议，技术人在 AI 时代要保持自信与好奇心，躬身入局，亲自动手，善用 AI，但要珍视并深化传统工程能力，更不要丧失亲手调试代码的能力，避免沦为可被替代的“监工”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598731" alt="" title="" loading="lazy"/></p><p>腾讯云架构师北京同盟理事 刘歧</p><p>腾讯云架构师北京同盟理事 刘歧在《AI 时代，我的短板不短》中，以自身从 FFmpeg 专家到 AI 创业者的转型为例，分享了如何用 AI 补齐“短板”。他坦言自己不擅长UI开发、商业计划书撰写等，但通过将 AI 视为“伴侣”，利用其完成 PPT 制作、技术方案美化、股权协议撰写甚至用户增长分析，从而解放自己，聚焦于长板。他的核心观点是：AI 替代不了人，但能帮助人成为“超级个体”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598732" alt="" title="" loading="lazy"/></p><p>科技博主、极其智能创始人、畅销书作者 SuperWinnie（杨文渝）</p><p>科技博主、极其智能创始人、畅销书作者 SuperWinnie（杨文渝）分享了《AI 时代的技术人，如何打造可复利的个人品牌？》。她结合自身运营科技博主的经验，提出技术人打造个人品牌的“新 IKIGAI 模型”：世界需要的、你擅长的技术、以及你将复杂变简单的表达能力，三者交集即为方向。她详细介绍了如何利用 AI 进行选题挖掘、内容表达优化，并强调在 AI 生成内容泛滥的时代，个人的真实故事、观点与价值观，才是脱颖而出的关键。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598733" alt="" title="" loading="lazy"/></p><p>腾讯云 CodeBuddy 技术负责人 杨苏博</p><p>腾讯云 CodeBuddy 技术负责人 杨苏博在《CodeBuddy Code 是如何做到 90% 代码由 AI 生成的》中，揭示了高代码生成率背后的方法论。他提出三大实践原则：拟人化哲学（将 AI 当作同事而非工具）、第一性原理（回归工程本质）与面向不确定编程（设计容错与接管机制）。他分享的案例表明，当 AI 成为开发“主角”时，人的角色应转向架构设计、方向把控与最终验收。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598734" alt="" title="" loading="lazy"/></p><p>社区年度之问：一线神帖，大咖来解</p><p>在随后的《社区年度之问：一线神帖，大咖来解》环节中，几位嘉宾就开发者关心的具体问题进行了深入交流。关于“如何将 AI 融入现有业务”，曹洪伟建议采用“探针”方式，从局部、低风险场景切入，用效果对比赢得信任。针对“过度依赖 AI 导致知识模糊”的担忧，杨苏博认为应分清“思考者”与“执行者”的界限，人必须掌控核心逻辑与最终决策。对于“个人品牌创作”，杨文渝分享了利用 AI 抓取关键词重组选题、并用人性化表达“出圈”的技巧。最后，关于“架构师的非技术能力”，刘歧与曹洪伟均强调了沟通与共赢思维的重要性，指出技术方案推动需与各方利益对齐。</p><h2><strong>结语</strong></h2><p>一天的峰会，从宏观趋势到产业实践，从个体进化到系统重构，描绘出 AI 浪潮下技术生态的生动全景。对于架构师和开发者而言，挑战与机遇并存。一方面，旧有经验加速贬值，单纯堆砌技术已不足以构建护城河；另一方面，理解业务本质、提出精准问题、进行价值判断、构建系统思维的能力变得空前重要。</p><p>也如多位分享嘉宾所言，技术的发展从来不是单打独斗。在“智效跃迁”的征途上，唯有交流才能提速，唯有共创才能共赢。当架构的边界被 AI 重新定义，一群人的同行，或许能让我们走得更远，看得更清。</p>]]></description></item><item>    <title><![CDATA[港科大熊辉｜AI时代的职场新坐标——为什么你应该去"数据稀疏"的地方? 腾讯云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047598741</link>    <guid>https://segmentfault.com/a/1190000047598741</guid>    <pubDate>2026-02-07 18:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>导语｜</strong>在人工智能发展的奇点时刻，算力、数据与人才的底层逻辑正在发生深刻变革。香港科技大学（广州）协理副校长、腾讯云 TVP 熊辉教授从物理学第一性原理出发，深度剖析了电力与资源如何成为 AI 发展的终极约束，并提出了人机协作新型劳动体的境界重构。面对 AI 对传统教育的冲击，他认为未来的架构师应通过提问与鉴赏力，向数据稀疏的“无人区”进发，实现从人才到人物的跨越。</p><h2><strong>作者简介</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598743" alt="" title=""/></p><p>熊辉，香港科技大学（广州）协理副校长、讲座教授，获国际人工智能促进协会会士（AAAI Fellow）、美国科学促进会会士（AAAS Fellow）、电气电子工程师学会会士（IEEE Fellow）、国际计算机学会会士（ACM Fellow）、中国人工智能学会会士、中国计算机学会会士、国际计算机学会（ACM）杰出科学家、国家重大人才工程入选、教育部长江讲座教授，海外杰青、广东省劳动模范等荣誉。他担任广州市人民政府参事、广州欧美同学会副会长、中国计算机学会大数据专家委员会副主任、Nature npj AI 创刊主编等多个重要政府、社会和学术职务。此前任美国罗格斯大学杰出教授、百度研究院副院长及首席科学家（T11）。主要从事人工智能与数据挖掘研究，主持或参与国家科技部重点研发计划、国家自然科学基金（含重大研究计划）。熊辉教授的 Google Scholar 引用超 61000 次，h-index 101，获 ACM SIGKDD 服务奖、AAAI 最佳论文奖等顶级奖项，并多次担任 KDD、ICDM 等行业大会主席。在人才培养方面，已有数十位学生成为国际知名大学教授。</p><h2><strong>一、穿透表象，探求时代本源规律</strong></h2><p>思考事物要触达本质。马斯克推崇的物理学第一性原理，与《易经》“不易、变易、简易” 的内核相通，皆是对本源的探求，都在启示我们：在剧变的时代，唯有穿透表象，才能捕捉到真正决定未来的底层逻辑。</p><p>当下 AI 热潮的底层逻辑，不在算法表层，而在基础物理资源。评价英伟达的价值，可简化为一道数学题：计算其全美 GPU 年销量的总耗电量，再对标美国电力总容量。我认为，限制 AI 发展的本质不是算力，而是电力水平。算力需求指数级增长，若电力架构跟不上，会引发电价飞涨、通胀加剧，挤压传统制造业。因此，我做技术投资未必买技术本身，反而会布局 “铜”—— 铜与电力才是这个时代更具确定性的底层逻辑。数据佐证：中国 2024 年发电量约等于美、欧、日、俄、印总和；美国自 ChatGPT 发布后电价上涨 40%，直观体现了 AI 对基础能源的巨大需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598744" alt="" title="" loading="lazy"/></p><p>正如物理资源的上限决定了 AI 的发展本质，人类如何应对这种硬约束，则决定了生产力的进化本质——那就是构建“人机协作的新型劳动体”。</p><p>我们正处于人工智能新经济发展时代，这要求我们必须重新定义架构师的边界。对于人工智能从业者而言，每个人都有机会成为掌控全局的架构师，你的核心竞争力不再是单打独斗，而是驾驭机器的规模。面试时，你不再是一个人，而是带着“N 台机器”入场：能驱动 10 台机器，你就有底气拿数倍的薪水；能驾驭 100 台机器，你就能一个人顶一个团队，定义一个项目、定义一个未来。这便是人机协作新型劳动体的底气：凭借管理机器的能力，个人完全可以突破传统人力劳动的上限，成为人机混合时代里定义未来的顶级架构师。</p><h2><strong>二、AI时代核心能力：提问与鉴赏力</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598745" alt="" title="" loading="lazy"/></p><p>为什么现在要培育人机协作新型劳动体？</p><p>我认为通过教育可以实现的人类智能分为四个境界：</p><ul><li>博闻强识，即学科知识的积累；</li><li>触类旁通，具备跨行业的迁移能力；</li><li>一叶知秋，能在行业内进行精准的预测与推理；</li><li>无中生有，实现从 0 到 1 的原始创新，如相对论或牛顿三大力学。</li></ul><p>在大模型时代，AI 一上来就通过海量数据“打掉”了人类的前两个境界，而在“一叶知秋”层面的超越也只是时间问题。这意味着，如果我们过去的教育依然只努力培养博闻强识的学生，那在 AI 面前将不名一文。作为架构师，必须利用 AI 形成人机混合的战队，在更高维度的境界中寻找生存空间。</p><p>要做到这一点，提问与鉴赏力成了架构师的核心技能。</p><p>我已年过五十，但过去两三年我的工作和学习效率是指数级提高的，一年完成的工作量甚至超过了过去五六年的总和。这种飞跃源于我将 AI 变成了深度学习的伴侣。面对如 NeurIPS 每年产出的五六千篇文章，传统读法已不可行。我利用 AI Agent 进行筛选，彻底改变了我的信息获取方式。我清楚自身兴趣方向与知识基础，借助 AI Agent 按明确条件（如作者 h 指数不低于 20、贡献足够大）筛选文献，留存优质内容后通过提问讨论，快速锁定核心任务。</p><p>我的读书效率也大幅提升，从一年 10 本增至 100 本。经典古籍即便结合 AI 讨论，阅读仍慢；但科技类新书可借助 AI 高效梳理：20 分钟内获取 10 句核心观点，筛选有新意的表述并延伸解读，最终锁定关键观点对应的章节快速翻阅，即可完成核心认知，实现了学习效率质的飞跃。</p><p>更重要的是，我教给学生一个判断创新价值的“幻觉原则”：同时用三个大模型测试，如果 AI 聊得头头是道，说明数据已经覆盖，不值得做；如果 AI 开始产生幻觉或胡说八道，说明你触碰到了认知的盲区。此时必须迅速在两周内完成突破，因为你已经通过提问暴露了方向，必须利用速度跑赢 AI 的进化。</p><h2><strong>三、数据质量：AI时代的价值分水岭</strong></h2><p>在智能革命时代，数据已成为核心生产资料，驱动着从数据飞轮到技术飞轮，再到产业飞轮的深度演进。一个国家若缺乏数据应用场景，就无法实现技术迭代与产业升级，更无法培养顶尖人才。中国拥有前所未有的应用场景沃土，这为 IT 与 AI 从业者提供了绝佳的历史机遇。放眼全球，真正有能力全方位进入这场飞轮角逐的只有中美两国。身为其中之一，我们正身处一个前所未有的广阔舞台。</p><p>人工智能时代，决定个人、企业乃至国家选择的核心逻辑究竟是什么？我认为答案很简单——数据质量。</p><p>个人又如何利用数据质量寻找适合深耕的产业领域呢？在探讨 AI 如何赋能产业时，必须区分不同的视角。对于国家和大型企业而言，首要任务是寻找数据肥沃的领域，通过高质量数据快速催生出业绩和成果；但站在个人职业规划的角度，我却建议大家应当反其道而行之。数据质量决定了 AI 渗透的速度，代码数据代表性好、评价机制明确的领域，程序员首当其冲被 AI 赋能甚至替代；而那些数据稀疏、环境复杂、采集成本高的行业，反而是人类经验的优势领域。</p><p>这种环境下，那些“可意会不可言传”的知识才具长期价值。我们需要去往人类未曾采集、未曾抵达的地方，才能做到认知层面的真正发现。</p><h2><strong>四、人机协作新型劳动体四大核心技能</strong></h2><p>全球发展规律已十分清晰：在美国，学历越高，与 AI 拉开差异化竞争的机会越大，薪资也越高。这对架构师是机遇，但机遇有前提：必须驾驭人工智能，成长为 AI 时代新型架构师，否则终将被时代淘汰。当下要着力培养人机协同新型劳动体，锤炼四大核心技能，且掌握 AI 是所有行业的必修课，而非选择题。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598746" alt="" title="" loading="lazy"/></p><p><strong>1、驾驭 AI 从事本行业的能力</strong></p><p>驾驭 AI 已从行业加分项转为生存必选项。即使是非计算机相关的行业，如艺术行业，传统灵感的枯竭也能通过 AI 辅助得以破解，跨界交叉不仅催生了创新，更实现了产出的指数级跨越。这揭示了一个本质：无论身处何业，核心竞争力已不再是死守传统技能，而在于驾驭机器的能力。</p><p><strong>2、培养和机器差异化的能力</strong></p><p>AI 擅长 “从书本到书本” 的知识转化，正如维特根斯坦所言，凡是能用语言清晰表述的内容，AI 都能胜任。而那些只可意会不可言传的经验与智慧，才是人类的独特机会，更是架构师需要深耕的能力。</p><p>我常对学生说，学习模式必须转型：用 50% 时间搭建知识框架，剩余精力拓展多领域知识并躬身实践。架构师亦是如此，唯有亲手攻克复杂问题，才能沉淀书本之外的核心能力。</p><p><strong>3、锻造人机混合创新能力</strong></p><p>这种能力需在点滴实践中积累，曾国藩的 “三令五申” 在新时代应有新解读：不再是警示过错，而是自我迭代的标尺 —— 做完每件事，务必思考如何做得更好。交办任务不能只满足 “完成”，更要追求 “优化”。聪明人做事，第一次投入 120% 精力无妨，第二次压缩至 80%，第三次降至 50%，在这个过程中总结创新，提升效率与质量，才是真正的成长，也是人机协同时代的核心竞争力。</p><p><strong>4、培养从 0 到 1 的创新能力</strong>  </p><p>现代社会亟需培育从 0 到 1 的整体创新能力，而创新容错环境是关键前提。创新本就是九死一生，企业若想孕育创新的热带雨林，必先构建容错的土壤。纵观美国硅谷与中国大湾区的发展，其底层逻辑可归结为 12 字内核：拓荒之勇、包容之量、创新之魂。</p><ul><li>其一，拓荒之勇是破局根基。“卷”本是中性词，在存量空间里内耗是无效内卷，但在增量无人区深耕细作，就是值得推崇的工匠精神。</li><li>其二，包容之量是成长沃土。为什么中国这么多年来还没有出现诺贝尔奖得主？我认为无需焦虑，等到 95 后、00 后真正成长起来，未来必然会有。因为诺贝尔奖的诞生需要创作者摆脱功利目的，纯粹为兴趣而钻研，而这一代新人应当是具有贵族精神的群体，他们从未经历过物质匮乏的困境，没有生存的恐惧感，能够全身心投入自己热爱的研究。同时，这一切也离不开国家、社会、企业乃至家庭共同营造的包容、容错的创新环境，这种环境是培养 “从零到一” 创新人才的土壤，也是诺贝尔奖得以诞生的必要前提。</li><li>其三，创新的核心，在于敢于突破认知边界。要从数据盲区切入，逐步突破感知盲区、认知盲区，去往人类未曾涉足、未曾采集的领域，才能实现真正的认知与盲区发现。</li><li>最后，我总结了一套创新实践法则：知行合一、试错迭代、大力出奇迹。当下聪明人比比皆是，机会往往集中在头部领域，唯有向头部迈进，才能抢占更多机遇。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598747" alt="" title="" loading="lazy"/></p><h2><strong>五、架构师的时代使命</strong></h2><p>随着人工智能极大地提升研发效能，企业的组织形式正发生根本性变革。过去，我们依赖人海战术的大兵团作战；未来，组织将转向类似现代战场的特战队模式。这意味着，企业不再单纯追求基础开发人员构成的工程师红利，而是极度渴求具备顶层认知的领军人物。</p><p>从人才到人物，是跨越式的进阶。T 型人才拥有深厚的专业底蕴与广博的视野，而“人物”则是在此基础上完成了认知的跃迁。如果说人才提供了解决问题的“金手指”，那么人物则提供了定义问题的“金脑袋”。</p><p>我对核心人物的价值衡量还有一个明确标准：在科学技术化、技术产品化、产品产业化、产业资本化、资本科学化的全链条循环中，聚焦建平台、做系统、定标准、创品牌四大核心动作 —— 这既是我们对标杆人物的定位，也是架构师的核心职责。</p><p>我们要致力于成为平台、系统、标准的建设者与品牌的打造者，更要以此为目标赋能每一位架构师。在 AI 新时代，架构师需要突破边界、定义边界，而能够扛起 “建平台、做系统、定标准、创品牌” 的重任，正是这个时代赋予架构师的核心使命与明确要求。</p><p>我们正处于人工智能的前沿，这既是挑战也是幸运。正如罗曼·罗兰所说：世界上只有一种真正的英雄主义，那就是在认清生活的真相后依然热爱生活。<strong>我希望我们不仅能熟练驾驭工具，更能在洞察了世界的真相、看见了科技带来的种种冲击后，依然热爱这个世界。</strong></p>]]></description></item><item>    <title><![CDATA[Tenable Nessus 10.11.2 发布 - 漏洞评估解决方案 sysin ]]></title>    <link>https://segmentfault.com/a/1190000047598750</link>    <guid>https://segmentfault.com/a/1190000047598750</guid>    <pubDate>2026-02-07 18:03:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Tenable Nessus 10.11.2 (macOS, Linux, Windows) - 漏洞评估解决方案</p><p>发布 Nessus 试用版自动化安装程序，支持 macOS Tahoe、RHEL 10、Ubuntu 24.04 和 Windows</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=vCNg%2BfOb1Y%2B9i2xrXQ1YeA%3D%3D.0fglt85WCy2YK%2BfjkX7nHceSGNPnrvzhQ6m4tFC3xjsWt3nY3KMCeqgpsjkGTrIQ" rel="nofollow" target="_blank">https://sysin.org/blog/nessus-10/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=MqhoWMRf8wk4WR2Tds8%2Bmg%3D%3D.qakN3Z4yCZjmHgK7FqYHfgU1zAAH7FLH68AhPcwAn8s%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p><img referrerpolicy="no-referrer" src="https://sysin.org/blog/nessus-10/nessus-hero.webp" alt="Nessus" title="Nessus"/></p><p>Nessus Vulnerability Scanner</p><p><strong>漏洞评估领域的全球黄金标准</strong>，<strong>针对现代攻击面量身打造</strong>。</p><p>利用业界最受信赖的漏洞评估解决方案来评估现代攻击面。扩展到传统的 IT 资产之外 – 保护云基础设施和获取对与互联网相连的攻击面的可见性。</p><h2>Nessus 版本</h2><table><thead><tr><th>Nessus Expert</th><th>Nessus Professional</th></tr></thead><tbody><tr><td>适用对象：</td><td>适用对象：</td></tr><tr><td><strong>顾问、渗透测试人员、开发人员和中小型企业</strong></td><td><strong>顾问、渗透测试人员和安全专业人士</strong></td></tr><tr><td>- 不受限制的 IT 评估</td><td>- 不受限制的 IT 评估</td></tr><tr><td>- 使用不限地点</td><td>- 使用不限地点</td></tr><tr><td>- 配置评估</td><td>- 配置评估</td></tr><tr><td>- 实时检测结果</td><td>- 实时检测结果</td></tr><tr><td>- 配置报告</td><td>- 配置报告</td></tr><tr><td>- 社区支持</td><td>- 社区支持</td></tr><tr><td>- 高级支持（可选）</td><td>- 高级支持（可选）</td></tr><tr><td>- 提供随需培训</td><td>- 提供随需培训</td></tr><tr><td>- 外部攻击面扫描</td><td>x 外部攻击面扫描</td></tr><tr><td>- 添加域的功能</td><td>x 添加域的功能</td></tr><tr><td>- 扫描云端基础架构</td><td>x 扫描云端基础架构</td></tr><tr><td>- 500 个预构建的扫描策略</td><td>x 500 个预构建的扫描策略</td></tr></tbody></table><h2>Nessus 在漏洞评估领域一路领先</h2><p>从创立伊始，我们就与各类网络安全相关行业紧密协作。我们根据业界的反馈持续优化  Nessus，将其打造成市场中最准确全面的漏洞评估解决方案。20 年以来，我们不忘初心，始终专注于业界协作与产品创新  (sysin)，建立起最准确全面的漏洞数据库，让您的企业不会因忽视重要漏洞而暴露于风险之中。</p><p>今天，Nessus 深受全球数万家企业的信赖，是全球部署最为广泛的安全技术之一，而且是漏洞评估行业的黄金标准。</p><p><strong>94K+</strong> 个 CVE</p><p><strong>226,000+</strong> 款插件</p><p><strong>100+</strong> 款新插件，每周定期发布</p><p>Tenable 的零日研究对新漏洞和紧急漏洞提供全天候更新，因此您将始终具有全面的态势感知。</p><h3>1 准确度</h3><p>Nessus 达到了 6 西格玛准确度，实现了业内最低的误报率</p><p>*每 100 万次扫描中仅有 0.32 次误报</p><h3>1 覆盖面</h3><p>Nessus 拥有业内首屈一指的漏洞覆盖面深度和广度</p><p>查看产品比较：<code>https://zh-cn.tenable.com/nessus/competitive-comparison</code></p><h3>1 采用率</h3><p>Nessus 深受数万家企业的信赖，全球下载次数达到 200 万次</p><h3>1 口碑信誉</h3><p>口说无凭，无需赘言。为何全球安全专业人士对 Nessus 的信赖让您眼见为实</p><h2>新增功能</h2><p><strong>Tenable Nessus 10.11.2</strong> (2026-02-05)</p><p>仅 Security Updates + Bug Fixes，详述略过，参看官方文档。</p><p><strong>Tenable Nessus 10.11.1</strong> (2025-12-15)</p><p>✅ <strong>功能变更与性能增强</strong></p><p>Tenable Nessus 10.11.1 包含以下更新：</p><ul><li><strong>Tenable Nessus Manager Red Hat Enterprise Linux (RHEL) 插件优化</strong> — 为 Tenable Nessus Manager 添加对 RHEL 衍生发行版生成插件数据库的支持。此更新允许 11.1.0 及以上版本的代理仅获取针对其 Linux 发行版的插件。</li></ul><p>✅ <strong>安全更新</strong></p><p>Tenable Nessus 10.11.1 包含以下安全更新：</p><ul><li>更新 libxml2 至 2.13.9 版本。</li><li>更新 libxslt 至 1.1.45 版本。</li><li>更新 expat 至 2.7.3 版本。</li></ul><p>✅ <strong>错误修复</strong></p><ul><li>修复了合规插件在漏洞报告中显示错误的问题。<br/><strong>缺陷 ID</strong>: 02317513</li><li>修复了 Tenable Nessus Manager 中共享代理扫描在服务重启时意外中止的问题。<br/><strong>缺陷 ID</strong>: 02353244, 02362574</li><li>修复了前端错误，非管理员用户由于许可证元素不可访问而遇到的问题。<br/><strong>缺陷 ID</strong>: 02374344, 02376020, 02375991, 02378922, 02379965</li></ul><p><strong>Tenable Nessus 10.11.0</strong> (2025-12-15)</p><p>✅ <strong>新功能</strong></p><p>Tenable Nessus 10.11.0 包含以下新功能：</p><ul><li><p>引入 <em>Nessus Essentials Plus</em>，一种新的年度订阅层，对验证学生和教育工作者免费，其他用户价格合理。包含功能如下：</p><ul><li>可扫描 20 个目标。</li><li>HTML 与 PDF 报告。</li><li>实时插件更新。</li></ul></li></ul><p>✅ <strong>功能变更与性能增强</strong></p><p>Tenable Nessus 10.11.0 包含以下更新：</p><ul><li><p>更新 Tenable Nessus Essentials 的功能限制：</p><ul><li>可扫描目标数从 16 减少至 5。</li><li>禁用报告与导出功能。</li><li>订阅更新为按月计费。</li><li>插件更新延迟 30 天。</li><li>在订阅期结束时，除非升级到 Tenable Nessus 高级版本，否则数据不会被保存。</li></ul></li></ul><p>✅ <strong>错误修复</strong></p><ul><li>修复本地化 HTML 和 PDF 报告翻译错误的问题。<br/><strong>缺陷 ID</strong>: 02338762, 02340433</li><li>修复 Tenable Nessus 后端未更新最近可用版本检查的问题。<br/><strong>缺陷 ID</strong>: 02257447, 02325697</li><li>修复 Tenable Nessus 无法在离线模式下导入 Web 应用扫描插件的问题。<br/><strong>缺陷 ID</strong>: 02249841, 02335036</li><li>修复从 Tenable Security Center 启动的高级代理扫描未包含某些插件结果的问题。<br/><strong>缺陷 ID</strong>: 02358488, 02360054, 02352675, 02362129, 02362296, 02362890, 02354701, 02365102, 02352799, 02360666, 02364066, 02365597, 02357087, 02359851, 02365111, 02357867, 02365777, 02354325, 02362378, 02366634, 02353439, 02351699, 02363014, 02366463</li></ul><p>✅ <strong>支持平台</strong></p><p>Tenable Nessus 10.11.0 的支持平台更新如下：</p><ul><li>新增对 Debian 13 的支持。</li><li>新增对 macOS 26 的支持。</li><li>移除对 macOS 13 的支持。</li><li>移除对 32 位 Windows 操作系统的支持。</li></ul><h2>系统要求</h2><p>Nessus 广泛支持各种 Unix、Linux 版本，也包括 Windows，下面列出的最广泛使用的 Unix、Linux 版本，作为推荐的运行平台。</p><p>macOS：</p><ul><li><a href="https://link.segmentfault.com/?enc=XbQLTAYypGEuzSAuD%2F0MJQ%3D%3D.5rsJvqB2HB6Ix3NvtaQy8ujfJ0PiNXBiub3HFQ5SOnIECHtRwFFk05%2FzheT%2BGv39" rel="nofollow" target="_blank">macOS Tahoe</a></li><li><a href="https://link.segmentfault.com/?enc=h93OpC8OKmIBKjN1c0GZCQ%3D%3D.LjgDox5kX5UzwoKp5e6ffAvm6O9mKqdkqbWQT1cyNQJTjw%2FZghrkb6pZ128zJ1Q4" rel="nofollow" target="_blank">macOS Sequoia</a></li><li><a href="https://link.segmentfault.com/?enc=Sx3YskZpsHfWwPDKIQPfdA%3D%3D.ua4J2xipReGVg7Jw2V18scEo2ru0VspUBG0R3qvfWczcelrEXsENj7i7WISplSPf" rel="nofollow" target="_blank">macOS Sonoma</a></li><li>更多：<a href="https://link.segmentfault.com/?enc=TOMKqWVnybqv3%2BF31bxj0w%3D%3D.SPQGcDXQov%2Fr6Y%2BlnjDsRgiBPJOT2C%2FRzvEKrJ2fcBs%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></li></ul><p>Linux：</p><ul><li><a href="https://link.segmentfault.com/?enc=L3ditZ9xedD11rQWUBqJ9A%3D%3D.96dQ3T6EzhZfTwVIosx1bYJev3OYkMAtB7zBMcqJSY%2Brkh4LXBYY3J35NEwHC8M9" rel="nofollow" target="_blank">AlmaLinux 10</a>（<a href="https://link.segmentfault.com/?enc=1KKPmlPEl90j1nTGIWsVfg%3D%3D.Zqf1Wdtjpv8OXSQgcHLrk4Pzx9sjWFAJcTlj4WNDRWHVYEomCvah85OVFD2AZ3Ww" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=YnGvuhc%2B2cvYEgTE9vNriQ%3D%3D.xDx%2BwhvlPcw%2FFie%2FNboCHjbLuhiZkeZ3nDFQsVzBny02wak6z7tdexhXrxTIxlZH" rel="nofollow" target="_blank">AlmaLinux 9</a>（<a href="https://link.segmentfault.com/?enc=MG0vZCfhyWbTQNBctKKNww%3D%3D.H5MVGfkVxzze9Z%2FTQBQ%2F03tgquOijgJVohAlFM6TwpqROkEe2Yy%2BIJfetm9aDdxv" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=oi5jP%2FDziSBE9gTSJUPKrQ%3D%3D.IMdSgt0%2BtXlhsDViueSGtwMaRMDmcJSyw1nL6AUcs9mnfEZbcUmUKHdi1%2B8AefYr" rel="nofollow" target="_blank">AlmaLinux 8</a>（<a href="https://link.segmentfault.com/?enc=%2BrTEWgwqm%2BAKuwpmX6RclA%3D%3D.SEiCAjOzbSVDGR6IeQr98J9wmXjYQKaN26cjJtI8mjmqoTkmyCLgs3NShThOXbt2" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=uE4wQmfvx%2BnEqvbZcb7Fig%3D%3D.AA8bbn4QHJGCNK3NhFG9w5hlZaRq%2F7o0H2qTFdj7mOMr3lOWrT2JMxZBNNg474Hh" rel="nofollow" target="_blank">Rocky Linux 10</a>（<a href="https://link.segmentfault.com/?enc=BnqqbTFs%2FrRFDLxHF9kIwg%3D%3D.m%2Bd2c86UIwm%2FKve5nuHfEemPIXjPh%2FzU5R9ikSim1xBzJbZl2EhGk6uTjQr%2BrQO4" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=3QrG034P5hz1CMBIvO7CEg%3D%3D.xCvnBaMOdTqG8%2FrVNafWoKnLLYeyJhWap6WKAJGoroj2JXGKbxwff7ly77kpLM%2Fm" rel="nofollow" target="_blank">Rocky Linux 9</a>（<a href="https://link.segmentfault.com/?enc=HtkReouJwlP5NFIaCd9eCA%3D%3D.d6WFZi7Cbod3orhQLklayGteCGm4pUzlM%2BrtYyqj6u6X4PJKXGoC44wEwv8BspXE" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=ZIMuInwhUtndb7Wk%2FJZR%2BQ%3D%3D.fVxX5a7e6v7Gsuzr0xdT24XeuMtrZXtkZSeTHIKt4%2Budbb5oOuwfcl%2FwlSAhOkxJ" rel="nofollow" target="_blank">Rocky Linux 8</a>（<a href="https://link.segmentfault.com/?enc=fUL0V%2BQ0Q7MhvCGSFcAiIA%3D%3D.HAeW5TFUqL6Ki9LTIdvxvWnSv%2FQtmZsdheNwJ9seS40TtgYAbNZJ84Jjt8Ceu7lT" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=Y1bCvDJsPCULIJZka3EviA%3D%3D.b8b4VU09blraO4QIs9Ec9dN%2Bh1GOjsbO5%2Fk5j5UKOuU%3D" rel="nofollow" target="_blank">RHEL 10</a></li><li><a href="https://link.segmentfault.com/?enc=2BZRmsIBqHLyjeTXF%2B%2BZyA%3D%3D.LH3PvINK%2B%2FQ288u6QQrN8CLz2fN1b%2BWNsxHjWTqIuZ8%3D" rel="nofollow" target="_blank">RHEL 9</a></li><li><a href="https://link.segmentfault.com/?enc=bsuPXGziOarCra62bIpT0g%3D%3D.ZCZrdZOV9vrVOxS7AfHyz7tEIKslRhss%2BXqFRjAnNSam0pVqGTWWu1fGIb1kRhyo" rel="nofollow" target="_blank">Ubuntu 24.04</a>（<a href="https://link.segmentfault.com/?enc=S4koHW%2FDjQ4YhfoWZvFYlQ%3D%3D.nlRoV7HCw6mMJVOM8PhiAPP%2BiS432FX0%2BuhMF4gM73fjW9WF6T2dpzGsitMXwO9z" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=FfOnTanRBkCMmylCiuomPA%3D%3D.QwuWA5cybi7abNe1zBLcVBpstRS12Bz6m7rVZPm9ESdYU7sw%2FlMFKK5ZAU5FsrLs" rel="nofollow" target="_blank">Ubuntu 22.04</a>（<a href="https://link.segmentfault.com/?enc=iqD40w6bQP1SLCZF3uDRaQ%3D%3D.q3%2FWdNNKbKhV6AnD8LNk4g4BrB3nchPOHsTItuCIlFEBWHe9%2BgcLX40ZYz%2F75LuS" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=nqYUYCK773%2BYwG0kB%2BlqaQ%3D%3D.fQimaATQZW257PT0V5u2iDhk2zSNjK4uy%2B6aUHx76oQqX3CDsnzOVGOuxeZRZVip" rel="nofollow" target="_blank">Ubuntu 20.04</a>（<a href="https://link.segmentfault.com/?enc=PjDN3VR6MM4yLWdpXftZeg%3D%3D.BITaTlrbIdnHXlQiL0r27i9bCjBKFRE22nqVgcbdjTQUzgdaggakmEFlEyGZOW8%2B" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=Psbucne%2FtY1p78xNcd%2Bc2g%3D%3D.evdQHgDcbjx2LTptHzo1Ti0BoxwYnI56823nGCQoIOIkoarSrWUwYQM8X%2BBiRdVZ" rel="nofollow" target="_blank">Debian 13</a>（<a href="https://link.segmentfault.com/?enc=YIacRcHz6Y1PRTjRQl9sKw%3D%3D.xxXU3JdRlINVZR2s7LS3Xut9VzeDc5YVa1elMuyH%2BYE3ut9%2Ft9ZxgGe4PNxV%2FKMT" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=oqduD7nqkucfkM6kLEypgA%3D%3D.kclxt5ryt9q%2FmrDIjel%2BaTAkHAL21PvycAq4UcyJEQf1CVDHkhDvyVzTSrGx%2BRGW" rel="nofollow" target="_blank">Debian 12</a>（<a href="https://link.segmentfault.com/?enc=TCr12oAHQsRFHbQRE3iqcg%3D%3D.URcRkie3OgPMS8ppUpqaSa4FYFoXXqiTxrgnhMbQmxDI%2F2On3Be2txrJFRhqW3wZ" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=KUZ8gh1utH745xLoBSYbCQ%3D%3D.jEtYM4H5vMhPHCzrDYgCqh1vgc%2BjdKDy1wNZRGX1obuP6LlNpzMt1MU6WCjwD29j" rel="nofollow" target="_blank">Debian 11</a>（<a href="https://link.segmentfault.com/?enc=81vHwfUdQ%2FwBBbas5rhLIA%3D%3D.yFQSDOAgCI%2Fp2t88plW8p9NQ12A3BD2d%2Bh%2F%2Bs%2BKNFhocuz3H0SgXsUiAc%2FmZI%2BdO" rel="nofollow" target="_blank">OVF</a>）</li></ul><p>Windows x64 系统：</p><ul><li><a href="https://link.segmentfault.com/?enc=0a%2BD233BK3kc6f8%2FJQw3AQ%3D%3D.INgtEW5NnIK7uMSgqSjOUBUz0jytm0Dp3K8vwkE9D2EPmGR1m3H81VKJwYDOBFMQ" rel="nofollow" target="_blank">Windows Server 2025</a>（<a href="https://link.segmentfault.com/?enc=KxD30soAgUpeYd%2FZ5s1UfA%3D%3D.VSEcl43iVZxsmtgVYmltzs3hitEqn5QK%2BKOFMdUrHmMxgxhLjCXwdDKb9nilHMUV" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=%2B1Ze%2B%2FwSy6HDwvgciub1Ng%3D%3D.bnfk5OYAjG08IAauhYdyktapoV5d0JNE4kD1VBdCGZt4pLjkP8GEkfTN0afAc8om" rel="nofollow" target="_blank">Windows Server 2022</a>（<a href="https://link.segmentfault.com/?enc=7bnthKYHzEMIIbJ%2Bu5zCIw%3D%3D.Yxs1odrw2bPEhsgenVYR1GpXBozUsDHddML35AisMDcCg2K86GBixdiD3Gx8DoLU" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=HaHz9DAIgY3UGfk1zU9sdg%3D%3D.sLeXGTTh0a70zk4r0xZQfzgI8MUNCL%2Fz0NRLU%2FTYLpRuFhLhinwDJwQXmsXHQBMd" rel="nofollow" target="_blank">Windows Server 2019</a>（<a href="https://link.segmentfault.com/?enc=m%2F0kYgeRYtiHhrsROZfaiQ%3D%3D.Jt5Y6WkJ15x9z4Sw4kwSk%2B%2B2VwzelViMHylyE216QkFJclNQ0ZQgQBstoJYeixGW" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=U%2F2N%2FvQMHvLIKAwP7cM%2Fmg%3D%3D.J4Sk6JJKU6ajMbyqbl9SN7AgvvG5xgmohkbARA9xS0ajEMalDoKaLb0rthrCo6%2FX" rel="nofollow" target="_blank">Windows 11</a></li><li><a href="https://link.segmentfault.com/?enc=SFdNI6WmakvZlRnOk%2Bwaig%3D%3D.rX%2BDi8joot7CBE7%2Fa0nySiJiqsmEQcgjtDcjnGnxm9Ob31KvT4f4NQOyrz6Mqevv" rel="nofollow" target="_blank">Windows 10</a></li></ul><h2>下载地址</h2><p>Tenable Nessus 10.11.2 (2026-02-06)</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=xaHG%2F1SqyMd4LRl76jBsuA%3D%3D.DWdFfqKXu4TaBlDwF1f45eG29IUc4k4UEa5nVqCIsR%2B5gPKZfgfUivk66nr8avCf" rel="nofollow" target="_blank">https://sysin.org/blog/nessus-10/</a></li></ul><table><thead><tr><th>Filename</th><th>Platform</th><th>Size</th><th>Release date</th></tr></thead><tbody><tr><td> </td><td><strong>Unix</strong></td><td> </td><td> </td></tr><tr><td>Deprecated</td><td><del>FreeBSD 11 AMD64</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Deprecated</td><td><del>FreeBSD 12 AMD64</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2.dmg</td><td>macOS Universal (14, 15, 26)</td><td>86.2 MB</td><td>2025-02-06</td></tr><tr><td> </td><td><strong>Linux</strong></td><td> </td><td> </td></tr><tr><td>Deprecated</td><td><del>Amazon Linux 2015.03, 2015.09, 2017.09</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2-amzn2.aarch64.rpm</td><td>Amazon Linux 2 (Graviton 2) / Amazon Linux 2023</td><td>66.9 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-amzn2.x86_64.rpm</td><td>Amazon Linux 2 / Amazon Linux 2023</td><td>67.1 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-debian10_amd64.deb</td><td>Debian 11, 12 / Kali Linux 2020 AMD64</td><td>61.6 MB</td><td>2025-02-06</td></tr><tr><td>Deprecated</td><td><del>Debian 10 (32-bit)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Deprecated</td><td><del>Red Hat Enterprise Linux 6 i386 (32-bit) / CentOS 6 / Oracle Linux 6 (including Unbreakable Enterprise Kernel)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Deprecated</td><td><del>Red Hat Enterprise Linux 6 (64-bit) / CentOS 6 / Oracle Linux 6 (including Unbreakable Enterprise Kernel)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Deprecated</td><td><del>Red Hat Enterprise Linux 7 (aarch64) / CentOS 7 / Oracle Linux 7 (including Unbreakable Enterprise Kernel)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2-el7.x86_64.rpm</td><td>Red Hat Enterprise Linux 7 (64-bit) / CentOS 7 / Oracle Linux 7 (including Unbreakable Enterprise Kernel)</td><td>67.4 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-el8.aarch64.rpm</td><td>Red Hat Enterprise Linux 8 (aarch64) / CentOS 8 / Oracle Linux 8 (including Unbreakable Enterprise Kernel)</td><td>69.3 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-el8.x86_64.rpm</td><td>Red Hat Enterprise Linux 8 (64-bit) / CentOS 8 / Oracle Linux 8 (including Unbreakable Enterprise Kernel)</td><td>67.6 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-el9.aarch64.rpm</td><td>Red Hat Enterprise Linux 9, 10 (aarch64) / CentOS Stream 9, 10 / Oracle Linux 9 (including Unbreakable Enterprise Kernel)</td><td>68.2 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-el9.x86_64.rpm</td><td>Red Hat Enterprise Linux 9, 10 (64-bit) / CentOS Stream 9, 10 / Oracle Linux 9 (including Unbreakable Enterprise Kernel)</td><td>68.7 MB</td><td>2025-02-06</td></tr><tr><td>Deprecated</td><td><del>Fedora 38 - 42 (64-bit)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2-raspberrypios_armhf.deb</td><td>Raspberry Pi OS (32-bit)</td><td>68 MB</td><td>2025-02-06</td></tr><tr><td>Deprecated</td><td><del>SUSE 11 Enterprise i586 (32-bit)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Deprecated</td><td><del>SUSE 11 Enterprise (64-bit)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2-suse12.x86_64.rpm</td><td>SUSE 12 Enterprise (64-bit)</td><td>55.9 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-suse15.x86_64.rpm</td><td>SUSE 15 Enterprise (64-bit)</td><td>56.2 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-ubuntu1604_amd64.deb</td><td>Ubuntu 16.04, 18.04, 20.04, 22.04, and 24.04 AMD64</td><td>61.2 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-ubuntu1604_i386.deb</td><td>Ubuntu 16.04 i386 (32-bit)</td><td>60.5 MB</td><td>2025-02-06</td></tr><tr><td>Nessus-10.11.2-ubuntu1804_aarch64.deb</td><td>Ubuntu 18.04, 20.04, 22.04, and 24.04 Aarch64</td><td>68.7 MB</td><td>2025-02-06</td></tr><tr><td> </td><td><strong>Windows</strong></td><td> </td><td> </td></tr><tr><td>Deprecated</td><td><del>Windows 10 (32-bit)</del></td><td>N/A</td><td>N/A</td></tr><tr><td>Nessus-10.11.2-x64.msi</td><td>Windows Server 2012, Server 2012 R2, 10, 11, Server 2016, Server 2019, Server 2022, Server 2025 (64-bit)</td><td>98 MB</td><td>2025-02-06</td></tr></tbody></table><hr/><p>发布 Nessus 试用版自动化安装程序，支持 macOS Tahoe、RHEL 10、Ubuntu 24.04 和 Windows</p><ul><li><a href="https://link.segmentfault.com/?enc=e%2FdX24xTdJ%2B1p31jqqD3Yw%3D%3D.iwl9%2FJywrZzD%2FZcS8V4wWGSY%2BrU3nO41vqdTxoVPCr00TQ5t22TCHJyHdSpNVnMzc0Pf6TLp%2FcZ%2BDtVSuV49GA%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for macOS Tahoe - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=M9cJLPmq68jMxjRjynfWbA%3D%3D.4vlwUbxCequvOdsr7JH0hRKW2yFBBjmABAy3YKzFUfFEbggoKnncpFEsnyedTezasSQjGGNjkThVt87i2LpCSQ%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for RHEL 9 | RHEL 10 - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=JFGfjPPszkDHa5rMWGvFEg%3D%3D.%2Bjv5Yiq1wwPl924nA3PBHNB9XotcmIHY47ew8Gbu1KujdQPwH%2BG6X3ZnzsAnOVA0dsgCYxjVhA5I1xktnJlN1w%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for Ubuntu 24.04 - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=sZzDs3F2dgysjFtShA5KjA%3D%3D.58W57euy4WWxXGneyaMfRafuM6LknKi4SQPR3Kbtm6D9BIxHNlAEjaaTd2ROCSwsz%2FX37WLZK3WFhDEbXq3l%2BA%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for Windows - Nessus 自动化安装程序</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=vIG2yejSaSNPjxB2Eqi7yA%3D%3D.u5hebJF0SPYiDuEIa7%2BdVgBnaOw%2BsDuhBMpAOXXtD%2B0%3D" rel="nofollow" target="_blank">HTTP 协议与安全</a></p>]]></description></item><item>    <title><![CDATA[Studio 3T 2025.23 - MongoDB 的终极 GUI、IDE 和 客户端 sysi]]></title>    <link>https://segmentfault.com/a/1190000047598754</link>    <guid>https://segmentfault.com/a/1190000047598754</guid>    <pubDate>2026-02-07 18:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Studio 3T 2025.23 (macOS, Linux, Windows) - MongoDB 的终极 GUI、IDE 和 客户端</p><p>The Ultimate GUI, IDE and client for MongoDB</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=ayIB2IpC%2FoBy4HewV7HDzQ%3D%3D.ptNXC%2BFdilthzE5xizUL8KCPdlc5CzzNBbWtVGwnPoWBy54NMWF6fB4d%2BrjCR3mY" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=NjMORrHw8pJ%2BzsuJseD9Jw%3D%3D.XveBcsGUXGJlc34Kb8c3rdP9tMP7CPPlAbOQ%2BH6BEdI%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>Studio 3T，MongoDB 的终极 (卓越、非凡) GUI、IDE 和 客户端</p><p>适用于 MongoDB 的所有 IDE、客户端和 GUI 工具 —— 在 Atlas 上或任何地方。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432404" alt="sysin" title="sysin"/></p><p>MongoDB 的强大工具。</p><p>超过 100,000 名开发人员和数据库管理员使用 Studio 3T 作为他们首选的 MongoDB GUI</p><h2>MongoDB 客户端、GUI 与 IDE</h2><p>那么 Studio 3T 到底是什么？ 在这里，我们解释了它戴的许多帽子中的三个。</p><ul><li><p><strong>Studio 3T 作为 MongoDB 客户端</strong></p><p>客户端是允许您连接到服务器的软件程序或应用程序。尽情使用 Studio 3T 的连接管理器，根据需要连接到尽可能多的 MongoDB 服务器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432411" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB GUI</strong></p><p>图形用户界面 (GUI) 完全按照它说的去做。它提供了一个带有图形菜单、图标、对话框、向导和其他可视元素的用户界面。使用 MongoDB  GUI 的替代方法是使用 mongo shell，尽管 Studio 3T 仍然有  IntelliShell——一个易于导航的内置版本——当你需要的时候。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432412" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB IDE</strong></p><p>集成开发环境 (IDE) 将应用程序和数据库开发的许多方面整合到一个功能齐全的 “工作室” 环境中 (sysin)。Studio 3T  正是通过提供一个 GUI 来做到这一点，该 GUI 的编辑器具有自动完成和语法突出显示、内置 JSON  验证、七种语言的自动查询代码生成以及许多其他功能，可帮助您更快地工作并节省时间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432413" alt="sysin" title="sysin" loading="lazy"/></p></li></ul><h2>新增功能</h2><p><strong>2025.23.0</strong>（2025-12-16）</p><p>修复：漏洞 —— 更新依赖项以修复 CVE-2025-59250。</p><p>修复：连接 —— 修复了在 Windows 上连接可能消失的问题。</p><p>修复：图标 —— 修复了在 Windows 某些分辨率和缩放设置下图标无法显示的问题。</p><p>修复：OIDC —— 修复了某些域名未被正确解析的问题。</p><h2>下载地址</h2><p>Studio 3T 2025.23.0 | 2025-12-16</p><ul><li>百度网盘链接：<a href="https://link.segmentfault.com/?enc=tWxKe%2BncyjcGCxz8VmtwVw%3D%3D.MVfI1To0RmPu6Q8ymWUaArc8qii%2FRRutFuRw6cqjY7rQ5Ja8jLAnin%2FBmeHj7n87" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=UU7653Cq324uOj1kUXuPtg%3D%3D.7sY90L7YiUEWXqnc3fwUn1uZN1Qd7L4d8OHHv1T%2FnpE%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[为 OpenClaw 加入 Matrix Channel - 选择孤独真实 本文系转载，阅读原文
h]]></title>    <link>https://segmentfault.com/a/1190000047598764</link>    <guid>https://segmentfault.com/a/1190000047598764</guid>    <pubDate>2026-02-07 18:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="341" referrerpolicy="no-referrer" src="/img/bVdnSLZ" alt="The Matrix 1999 - Take the red pill to stay in Wonderland and see how deep the rabbit hole goes, or the blue pill to wake up and believe what you want." title="The Matrix 1999 - Take the red pill to stay in Wonderland and see how deep the rabbit hole goes, or the blue pill to wake up and believe what you want."/></p><p>不可否认，OpenClaw 在一定程度上被过度炒作了。不过，从 <strong>AI Agent 开发</strong>的角度来看，它确实引入了一种有意思的思路：将 <strong>即时通讯平台</strong>（如 WhatsApp、Telegram、Matrix 等）作为与 AI Agent 交互的主要入口。</p><p>这种设计显著降低了部署和远程使用的复杂度。用户只需要一个 IM 客户端即可与 AI 交互，而无需处理端口映射、反向代理或复杂的网络配置。</p><p>许多技术用户之所以对 OpenClaw 感兴趣，正是因为它强调 <strong>可自托管</strong>。尽管由于硬件成本的限制，完全在本地自托管大模型对大多数人来说仍不现实，但<strong>自托管 OpenClaw 的 Matrix  Channel</strong>却是一个非常可行的方案。</p><p>通过这种方式，用户既能利用 OpenClaw 的能力，又能在一个私有的 Matrix Channel 中进行通信。自托管 Matrix  Server 意味着你可以完全掌控自己的数据，在不依赖第三方平台的前提下，以安全、私密的方式与 AI Agent 交互。</p><hr/><h2>搭建 Matrix 聊天 Server</h2><p>我选择了 <a href="https://link.segmentfault.com/?enc=WRbbHUsZhdV6qO5PoiNGSg%3D%3D.0NR4CW%2FtgUqY39Q%2Fg0lHZM5rQ1Anu9YCm2h0Ar5yhhcs8tkZu1kRU6lUSE%2FSFIVA" rel="nofollow" target="_blank">tuwunel</a> 作为自托管的 Matrix  Server。它是 <a href="https://link.segmentfault.com/?enc=7quqCmBjwCyAQQIpva7WtA%3D%3D.z%2FWOD2kyIUx%2B6RjV7Z%2F2OnmDF22iH1hoasoVutFduw03nFk7UFviJTUOx355ctuf" rel="nofollow" target="_blank">conduwuit</a> 的官方继任项目。</p><p>完成 Server部署后，我创建了一个名为 <code>openclaw</code> 的 Matrix 用户，并通过以下请求获取访问令牌（access token）：</p><pre><code class="bash">curl -XPOST \
  -H "Content-Type: application/json" \
  -d '{"type":"m.login.password", "user":"@openclaw:your_matrix_home_domain", "password":"$your_matrix_password"}' \
  "https://$your_matrix_home_domain/_matrix/client/r0/login"

{"user_id":"@openclaw:your_matrix_home_domain","access_token":"$your_matrix_access_token","home_server":"$your_matrix_home_domain","device_id":"xyz"}</code></pre><hr/><h2>配置 OpenClaw Channel</h2><p>在继续之前需要说明：<strong>OpenClaw 以及 OpenClaw 的 Matrix 插件目前仍处于早期开发阶段</strong>。在配置过程中你可能会遇到一些问题。以下步骤基于我自己的实践环境整理，希望能帮助你顺利完成 Matrix  Channel的搭建。</p><blockquote>参考 <a href="https://link.segmentfault.com/?enc=u%2Fq4OQz84ckjhRbbiu8aiQ%3D%3D.ogRdJcscOf91wdt%2FXcMnHxa6%2Fu%2Fr4CucUjuGp%2Bjw4Er%2Bl3i%2FfmFrRTjuNqRw4tcR" rel="nofollow" target="_blank">https://docs.openclaw.ai/channels/matrix</a></blockquote><h3>安装 Matrix  Channel插件</h3><p>在安装插件之前，建议先观察 OpenClaw 的日志输出：</p><pre><code class="bash">tail -f /tmp/openclaw/openclaw-2026-xx-xx.log</code></pre><p>然后安装 Matrix 插件：</p><pre><code class="bash">openclaw plugins install @openclaw/matrix</code></pre><hr/><h3>插件安装问题一：重复的插件 ID</h3><p>安装完成后，你可能会看到类似下面的错误：</p><pre><code class="log">Config warnings:
- plugins.entries.matrix: plugin matrix: duplicate plugin id detected; later plugin may be overridden
(/home/mark/.nvm/versions/node/v24.13.0/lib/node_modules/openclaw/extensions/matrix/index.ts)</code></pre><p>这通常是因为插件同时存在于以下两个目录中：</p><ul><li><code>~/.nvm/versions/node/v24.13.0/lib/node_modules/openclaw/extensions/matrix</code></li><li><code>~/.openclaw/extensions/matrix</code></li></ul><p>如果是这种情况，删除其中一个重复目录即可，例如：</p><pre><code class="bash">sudo rm -rf ~/.nvm/versions/node/v$some_version/lib/node_modules/openclaw/extensions/matrix</code></pre><hr/><h3>插件安装问题二：缺少依赖</h3><p>如果日志中出现如下错误：</p><pre><code class="log">[plugins] matrix failed to load from /home/mark/.openclaw/extensions/matrix/index.ts:
Error: Cannot find module '@vector-im/matrix-bot-sdk'</code></pre><p>说明缺少相关依赖。可以通过全局安装依赖来解决：</p><pre><code class="bash">npm install -g vector-im/matrix-bot-sdk
npm install -g markdown-it
npm install -g music-metadata
npm install -g zod</code></pre><hr/><h3>配置 OpenClaw</h3><p>编辑 <code>~/.openclaw/openclaw.json</code>，加入以下配置：</p><pre><code class="json">"channels": {
  "matrix": {
    "accessToken": "$your_matrix_access_token",
    "dm": {
      "enabled": true,
      "policy": "open"
    },
    "autoJoin": "always",
    "groupPolicy": "open",
    "homeserver": "https://$your_matrix_home_domain",
    "deviceName": "OpenClaw",
    "enabled": true,
    "encryption": true
  }
}</code></pre><blockquote><strong>注意</strong>：这里的 <code>open</code> 策略意味着<strong>你的 Matrix homeserver 上的任何用户都可以向 OpenClaw 机器人发送消息</strong>。该配置仅适用于测试环境。<br/>在生产环境中，建议限制访问策略，并考虑关闭 Matrix 的 federation 功能。</blockquote><hr/><h2>测试 Matrix 聊天 Channel</h2><p>配置完成后，你应该可以通过 Matrix 客户端与 OpenClaw 进行交互：</p><p><img width="344" height="742" referrerpolicy="no-referrer" src="/img/bVdnSLY" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Linux安装Temporal工作流引擎(PostgresSQL版) YYGP ]]></title>    <link>https://segmentfault.com/a/1190000047598769</link>    <guid>https://segmentfault.com/a/1190000047598769</guid>    <pubDate>2026-02-07 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>1. 下载temporal程序</h3><p><a href="https://link.segmentfault.com/?enc=0%2F7xErCyjWtXmc%2BEmoa%2Fpw%3D%3D.hsR8hpwkyXUh1vjMqvVlHrqyaxX6ANeO1y34XVDo%2FY54juFeD9gg1LLi8J8Ef09KjfDZm4kyCt90GwFDPxt65g%3D%3D" rel="nofollow" target="_blank">https://github.com/temporalio/temporal/releases/tag/v1.29.3</a><br/>解压到任意目录， 以 <code>/usr/local/temporal</code> 为例</p><h3>2. 下载DB配置</h3><p>下载地址: <a href="https://link.segmentfault.com/?enc=l53elGNv0kj4PfK3DHyIjg%3D%3D.eogGQRRiEbk0EjznbKXSRuri7fXHTyiyJYyWiSZHPcZGKAIWOhAbpdVWKR6NtAxZVItF76Zt6GF3CeO0GYL53Q%3D%3D" rel="nofollow" target="_blank">https://github.com/temporalio/temporal/tree/main/schema</a><br/>选择对应数据库， 本文<code>schema/postgresql/v12/</code>为例<br/>将<code>schema</code>目录复制到<code>/usr/local/temporal/schema</code></p><h3>3. 安装PostgresSQL12</h3><p>Docker安装为例</p><pre><code class="shell"># 创建数据存放目录
mkdir -p /data/postgres12
chown 999:999 /data/postgres12

# 拉取镜像
docker pull postgres:12

# 启动容器 (注意宿主机端口这里用的5431, 建议映射成一样的-p 5432:5432)
# 启动后用 postgres 123456 登录数据库
docker run -d \
  --name temporal-postgres12 \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=123456 \
  -e POSTGRES_DB=postgres \
  -p 5431:5432 \
  -v /data/postgres12:/var/lib/postgresql/data \
  postgres:12
  
# 查看启动日志
docker logs --tail=10 -f temporal-postgres12</code></pre><h3>3. 创建数据库</h3><p>初始化数据库</p><pre><code class="sql">-- 1. 创建用户
CREATE USER temporal WITH PASSWORD '123456';

-- 2. 创建数据库
CREATE DATABASE temporal;
CREATE DATABASE temporal_visibility;

-- 3. 将数据库所有权授予 temporal（推荐）
ALTER DATABASE temporal OWNER TO temporal;
ALTER DATABASE temporal_visibility OWNER TO temporal;</code></pre><h3>4. 修改Temporal配置， 使用PostgresSQL12</h3><p>初始化数据库</p><pre><code class="sql">-- 1. 创建用户
CREATE USER temporal WITH PASSWORD '123456';

-- 2. 创建数据库
CREATE DATABASE temporal;
CREATE DATABASE temporal_visibility;

-- 3. 将数据库所有权授予 temporal（推荐）
ALTER DATABASE temporal OWNER TO temporal;
ALTER DATABASE temporal_visibility OWNER TO temporal;

-- 提升 temporal 为超级用户
ALTER USER temporal WITH SUPERUSER;</code></pre><p>前往目录 /usr/local/temporal， 设置软链接</p><pre><code>rm development.yaml 
ln -sf development-postgres12.yaml development.yaml</code></pre><p>并修改development-postgres12.yaml中的DB连接信息</p><pre><code># 如果按以上设置的DB、用户名， 一般只需要把密码配置改成123456
password: "123456"</code></pre><h4>可选修改, 允许外网访问</h4><pre><code>services:
  frontend:
    rpc:
      bindOnLocalHost: false  # 改成false, 允许外网访问
      
# 同时检查 clusterMetadata：
clusterMetadata:
  clusterInformation:
    active:
      rpcAddress: "192.168.2.215:7233"  # 改成服务器实际局域网 IP</code></pre><h3>5 数据库配置:</h3><p>初始化数据库， 前往目录 cd /usr/local/temporal</p><pre><code class="shell"># 要用超级用户postgres初始化数据库
./temporal-sql-tool --plugin postgres12 --endpoint 192.168.2.215 --port 5431 -user temporal -password 123456 --database temporal setup-schema -v 0.0
./temporal-sql-tool --plugin postgres12 --endpoint 192.168.2.215 --port 5431 -user temporal -password 123456 --database temporal_visibility setup-schema -v 0.0

# 要用超级用户postgres初始化数据库
# 否则报错: ERROR    Unable to update SQL schema.    {"error": "error executing statement: pq: permission denied to create extension \"btree_gin\"", "logging-call-at": "/home/runner/work/temporal/temporal/tools/sql/handler.go:53"}
./temporal-sql-tool --plugin postgres12 --endpoint 192.168.2.215 --port 5431 -user temporal  -password 123456  --database temporal update-schema -d schema_1.29.3/postgresql/v12/temporal/versioned
./temporal-sql-tool --plugin postgres12 --endpoint 192.168.2.215 --port 5431 -user temporal  -password 123456  --database temporal_visibility update-schema -d schema_1.29.3/postgresql/v12/visibility/versioned</code></pre><h3>6. 收回超管权限</h3><pre><code class="sql">-- 收回超级用户权限
ALTER USER temporal WITH NOSUPERUSER;</code></pre><h3>5. 启动</h3><pre><code class="shell">./temporal-server start</code></pre><h3>6. 创建namespace</h3><pre><code class="shell">temporal operator namespace create default</code></pre><h3>7. 安装web界面(可选)</h3><p>下载镜像<br/><code>docker pull temporalio/ui:2.45.1</code></p><p>运行镜像</p><pre><code>docker run -d --name temporal-ui -p 8080:8080 -e TEMPORAL_ADDRESS=192.168.2.215:7233 temporalio/ui:2.45.1
docker logs --tail=10 -f temporal-ui
docker exec -it temporal-ui /bin/bash</code></pre><h2>附录</h2><h3>完整配置</h3><p>development-postgres12.yaml</p><pre><code class="yaml">log:
  stdout: true
  level: info

persistence:
  defaultStore: postgres-default
  visibilityStore: postgres-visibility
  numHistoryShards: 4
  datastores:
    postgres-default:
      sql:
        pluginName: "postgres12"
        databaseName: "temporal"
        connectAddr: "192.168.2.215:5431"
        connectProtocol: "tcp"
        user: "temporal"
        password: "123456"
        maxConns: 20
        maxIdleConns: 20
        maxConnLifetime: "1h"
    postgres-visibility:
      sql:
        pluginName: "postgres12"
        databaseName: "temporal_visibility"
        connectAddr: "192.168.2.215:5431"
        connectProtocol: "tcp"
        user: "temporal"
        password: "123456"
        maxConns: 2
        maxIdleConns: 2
        maxConnLifetime: "1h"

global:
  membership:
    maxJoinDuration: 30s
    broadcastAddress: "192.168.2.215"
  pprof:
    port: 7936
  metrics:
    prometheus:
      #      # specify framework to use new approach for initializing metrics and/or use opentelemetry
      #      framework: "opentelemetry"
      framework: "tally"
      timerType: "histogram"
      listenAddress: "127.0.0.1:8000"

services:
  frontend:
    rpc:
      grpcPort: 7233
      membershipPort: 6933
      bindOnLocalHost: false
      httpPort: 7243
      advertiseAddress: "192.168.2.215:7233"

  matching:
    rpc:
      grpcPort: 7235
      membershipPort: 6935
      bindOnLocalHost: false
      advertiseAddress: "192.168.2.215:7235"

  history:
    rpc:
      grpcPort: 7234
      membershipPort: 6934
      bindOnLocalHost: false
      advertiseAddress: "192.168.2.215:7234"

  worker:
    rpc:
      grpcPort: 7239
      membershipPort: 6939
      bindOnLocalHost: false
      advertiseAddress: "192.168.2.215:7239"

clusterMetadata:
  enableGlobalNamespace: false
  failoverVersionIncrement: 10
  masterClusterName: "active"
  currentClusterName: "active"
  clusterInformation:
    active:
      enabled: true
      initialFailoverVersion: 1
      rpcName: "frontend"
      rpcAddress: "localhost:7233"

dcRedirectionPolicy:
  policy: "noop"

archival:
  history:
    state: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"
      gstorage:
        credentialsPath: "/tmp/gcloud/keyfile.json"
  visibility:
    state: "enabled"
    enableRead: true
    provider:
      filestore:
        fileMode: "0666"
        dirMode: "0766"

namespaceDefaults:
  archival:
    history:
      state: "disabled"
      URI: "file:///tmp/temporal_archival/development"
    visibility:
      state: "disabled"
      URI: "file:///tmp/temporal_vis_archival/development"

dynamicConfigClient:
  filepath: "config/dynamicconfig/development-sql.yaml"
  pollInterval: "10s"</code></pre>]]></description></item><item>    <title><![CDATA[“Linux之父AI观导图”解构实录：理性剖析AI泡沫的每个层面 图形天下 ]]></title>    <link>https://segmentfault.com/a/1190000047598593</link>    <guid>https://segmentfault.com/a/1190000047598593</guid>    <pubDate>2026-02-07 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="1068" referrerpolicy="no-referrer" src="/img/bVdnSJo" alt="" title=""/><br/>                “Linux之父把AI泡沫喷了个遍”思维导图</p><p><a href="https://link.segmentfault.com/?enc=H9%2BoWhZe%2BJ3VBLueozSAAw%3D%3D.kbka5NsUnreOtvcFzoVMSL5In7KWI%2FPuYY6%2BtZQDw3m0Ef7%2FpAXWOYr2GmJrS7TldUQzOwrWbGQpayO38BsQ3PldsLxhp3N7jO4cZY4ji7M%3D" rel="nofollow" target="_blank">“Linux之父把AI泡沫喷了个遍”思维导图模板获取链接</a></p><h2>一、核心主题确定</h2><p>确定核心主题为“Linux之父把AI泡沫喷了个遍”，围绕这一主题，收集和整理Linux之父Linus Torvalds对AI的看法、AI的发展现状、优缺点、炒作周期、分类与作用、未来预测等相关内容，形成思维导图的核心内容框架。</p><h2>二、导图结构设计</h2><ol><li><strong>博文核心观点</strong>：作为思维导图的主要分支，涵盖对AI炒作的态度、AI发展现状、AI的优点、AI炒作周期分析、AI的分类与作用、AI的未来预测六个子分支，每个子分支下再细分具体观点和内容。</li><li><strong>博文观点分析</strong>：对博文核心观点进行合理性及局限性两方面的分析，每个方面下再细分具体分析内容，形成逻辑严密的论证结构。</li><li><strong>个人观点补充</strong>：包含对AI炒作的理解以及对AI未来发展的期待两个子分支，每个子分支下同样细分具体观点，展现个人对AI领域的深入思考。</li></ol><h2>三、导图样式设计</h2><ol><li><strong>颜色搭配</strong>：采用绿色作为背景色，给人清新、科技感的视觉感受；不同层级的文字和分支使用不同颜色进行区分，如核心主题用黑色加粗字体，一级分支用深绿色背景白色字体，二级及以下分支用浅绿色背景黑色字体保持视觉一致性。可借鉴图形天下思维导图提供的<strong>17套配色方案</strong>，选择适合科技主题的配色，增强视觉吸引力。</li><li><strong>形状布局</strong>：整体采用<strong>树状表格</strong>布局，从核心主题向右侧延伸出主要分支，各分支下的子内容以列表形式呈现，层次分明，逻辑清晰。图形天下思维导图提供的<strong>12类42种图形布局</strong>，可根据内容特点灵活选择，使布局更加专业和有条理。</li><li><strong>字体和字号</strong>：选择简洁易读的字体，核心主题字号最大，一级分支字号次之，二级及以下分支字号相对较小，通过字号大小体现内容的层级关系。</li></ol><p><a href="https://link.segmentfault.com/?enc=WGrXL1FXxph021hl%2Fn9adQ%3D%3D.TVQQIkYrOFhygEeQK0YOyYFiSfcu%2FzgwZ3q6vPob2EuQCgQuSi%2FXyBDbnTMr2upsUTxTuKFX%2BSIkoKeumCbYMtPsmUQmG7YMOalxz0B8hLCQSqDxtD63MXPi6axjOLqu" rel="nofollow" target="_blank">“Linux之父把AI泡沫喷了个遍”思维导图模板在线免费体验链接</a></p><h2>四、导图工具与流程</h2><ul><li><strong>工具选择</strong>：使用图形天下思维导图软件，该软件不仅提供了丰富的模板、图标、颜色设置等功能，还支持<strong>多模态AI生成思维导图</strong>，能极大提升创作效率。</li><li><p><strong>创作流程</strong></p><ul><li><strong>收集资料</strong>：查阅Linus Torvalds关于AI的相关言论、报道以及AI领域的发展现状、技术分析等资料。</li><li><strong>整理内容</strong>：对收集到的资料进行整理和归纳，提取关键信息，形成各个分支下的具体内容。</li><li><strong>创建导图</strong>：在图形天下思维导图软件中，先输入核心主题，然后依次创建一级分支、二级分支等。</li><li><strong>样式调整</strong>：利用软件的<strong>树型表格</strong>布局，将博文核心观点下的各子分支及其内容以表格形式清晰呈现。同时，利用软件提供的<strong>17套配色方案</strong>对导图的颜色进行调整。</li><li><strong>检查完善</strong>：检查导图内容是否完整、逻辑是否连贯、有无错别字等，对不足之处进行修改和完善。</li></ul></li></ul><p><a href="https://link.segmentfault.com/?enc=RU9aZFQ15%2BDZOKa8XM15hA%3D%3D.n1Wv4TDGYz%2FC5fnRsnKgACQz%2FegymhN3gL2HSjcgCjwmqnr5Ia7gXE7gKTLlNj1MWi6QdGutTaJ%2F4DsAZN0%2Bsw%3D%3D" rel="nofollow" target="_blank">图形天下思维导图软件免费下载链接</a></p><h2>五、总结</h2><p>在本次思维导图的创作过程中，通过运用图形天下思维导图软件的<strong>树型表格</strong>布局，成功将复杂的内容以清晰、有条理的方式呈现出来。同时，借助软件提供的<strong>配色方案</strong>和<strong>预设风格</strong>，使导图在视觉上更加吸引人。整个创作流程高效顺畅，充分展现了图形天下思维导图软件在知识管理和思维可视化方面的强大能力。</p><p>访问图形天下思维导图<strong>模板库</strong>与<strong>教程资源</strong>，获取更多免费导图素材与实操指南，激发你的无限创意。</p><ul><li><a href="https://link.segmentfault.com/?enc=hcXyBQ802vRQIrjeKK4T6g%3D%3D.ZJUqaHdNzGmCf8AdjoY96AutS%2BNo%2FjIjoio6wUg%2FLZDpB9xJDuqNeAj0qttpk9WlsWYtWqQ%2B1PEXUpHIxruneA%3D%3D" rel="nofollow" target="_blank">思维导图模板库</a></li><li><a href="https://link.segmentfault.com/?enc=fbC7uJHVi49ZTktdbC7vaA%3D%3D.TTh0%2Bb1O2nCswxyLuK%2BFN7WxVJ9JoumgxyZuJl2v%2FPMY4svKeIoJbr7z02TcJooh9xdSQ5EXaCbxQRFdixn5uw%3D%3D" rel="nofollow" target="_blank">思维导图使用教程资源</a></li></ul>]]></description></item><item>    <title><![CDATA[Scrapy框架入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047598548</link>    <guid>https://segmentfault.com/a/1190000047598548</guid>    <pubDate>2026-02-07 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. Scrapy的概览与核心价值</h2><p>想象一下,如果你需要从成千上万个网页中提取结构化数据,用传统的<code>requests</code> + <code>BeautifulSoup</code>方式就像用勺子挖土——虽然可行,但效率低下且难以维护。<code>Scrapy</code>正是为解决大规模、高性能数据抓取需求而生的工业级爬虫框架。</p><p>在Python生态系统中,Scrapy占据了不可替代的地位。它不仅仅是一个爬虫库,更是一个完整的爬虫开发框架,将数据抓取的整个流程——从请求调度、网页下载、数据提取到持久化存储——封装成了一套标准化的流水线系统。这种模块化设计让开发者能够专注于"爬什么"而非"怎么爬",极大提升了开发效率。</p><p>Scrapy的独特价值在于其基于Twisted异步网络框架的事件驱动架构,能够以单线程实现高并发请求处理,在不增加硬件资源的前提下获得10倍于传统爬虫的抓取速度。同时,它内置的请求去重、自动重试、用户代理轮换等反爬机制,让开发者能够快速构建稳定可靠的爬虫系统。</p><h2>2. 环境搭建与"Hello, World"</h2><h3>安装Scrapy</h3><p>Scrapy支持Python 3.7及以上版本,推荐使用Python 3.8+以获得最佳兼容性。安装方式如下:</p><pre><code class="bash"># 使用pip安装(推荐使用国内镜像源加速)
pip install scrapy -i https://pypi.douban.com/simple

# 验证安装是否成功
scrapy version</code></pre><p>如果看到类似<code>Scrapy 2.11.0</code>的版本号输出,说明安装成功。对于Windows用户,可能需要先安装Microsoft Visual C++ Build Tools以解决某些依赖包的编译问题。</p><h3>第一个Scrapy爬虫</h3><p>让我们创建一个最简单的爬虫来抓取quotes.toscrape.com网站的励志名言:</p><pre><code class="python">import scrapy

class QuotesSpider(scrapy.Spider):
    # 爬虫的唯一标识符
    name = 'quotes'
    
    # 起始URL列表
    start_urls = ['http://quotes.toscrape.com/page/1/']
    
    def parse(self, response):
        # 遍历页面中的每个名言
        for quote in response.css('div.quote'):
            # 提取名言内容、作者和标签
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('a.tag::text').getall(),
            }
        
        # 查找下一页链接并继续爬取
        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)</code></pre><p><strong>代码逐行解析:</strong></p><ul><li><code>class QuotesSpider(scrapy.Spider)</code>: 继承Scrapy的Spider基类,所有自定义爬虫都必须这样做</li><li><code>name = 'quotes'</code>: 定义爬虫名称,运行爬虫时会用到这个标识符,必须在项目中唯一</li><li><code>start_urls = [...]</code>: 定义爬虫的起始URL列表,Scrapy会自动为每个URL创建请求</li><li><code>def parse(self, response)</code>: 默认的回调函数,处理响应的函数名固定为parse(除非你指定其他回调)</li><li><code>response.css(...)</code>: 使用CSS选择器提取数据,Scrapy支持CSS和XPath两种选择器</li><li><code>yield {...}</code>: 生成字典数据,这些数据会被传递给Item Pipeline进行后续处理</li><li><code>response.follow()</code>: 创建新的请求来跟进链接,第一个参数是URL,第二个参数是回调函数</li></ul><p><strong>运行结果:</strong></p><p>在终端中执行以下命令运行爬虫:</p><pre><code class="bash">scrapy crawl quotes -o quotes.json</code></pre><p>运行后,Scrapy会自动从第一页开始抓取,提取每条名言的信息,并自动翻页直到抓取完所有页面。最终数据会保存在<code>quotes.json</code>文件中,格式如下:</p><pre><code class="json">[
    {
        "text": "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”",
        "author": "Albert Einstein",
        "tags": ["change", "deep-thoughts", "thinking", "world"]
    },
    ...
]</code></pre><h2>3. 核心概念解析</h2><p>Scrapy的核心架构围绕几个关键组件展开,理解这些组件的职责和交互方式是掌握Scrapy的关键。</p><h3>3.1 Spider(爬虫)</h3><p>Spider是用户编写的核心逻辑模块,定义了:</p><ul><li>如何爬取网站(起始URL、如何跟进链接)</li><li>如何解析页面内容(提取数据)</li><li>如何处理提取到的数据(生成Item或新的Request)</li></ul><p>每个Spider必须继承<code>scrapy.Spider</code>基类,并至少实现<code>parse()</code>方法。Spider的典型工作流程是:接收Response对象 → 解析页面 → 提取数据或生成新Request → yield出去。</p><h3>3.2 Item(数据项)</h3><p>Item是Scrapy提供的数据容器,类似于Python字典但提供了字段验证功能。通过预定义数据结构,Item能够避免字段拼写错误和类型混乱。</p><pre><code class="python">import scrapy

class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()</code></pre><p>使用Item的好处包括:</p><ul><li>字段定义清晰,便于团队协作</li><li>支持数据验证和类型检查</li><li>与Pipeline配合,实现数据清洗的标准化流程</li></ul><h3>3.3 Pipeline(管道)</h3><p>Pipeline负责处理Spider提取的Item,典型操作包括:</p><ul><li>数据清洗(去除空格、转换格式)</li><li>数据验证(检查必填字段)</li><li>数据去重(避免重复存储)</li><li>持久化存储(存入数据库或文件)</li></ul><pre><code class="python">class CleanPipeline:
    def process_item(self, item, spider):
        # 去除文本首尾空格
        item['text'] = item['text'].strip()
        return item

class DatabasePipeline:
    def __init__(self):
        self.db_conn = None
    
    def open_spider(self, spider):
        # 爬虫启动时建立数据库连接
        self.db_conn = create_database_connection()
    
    def process_item(self, item, spider):
        # 将item存入数据库
        self.db_conn.insert(item)
        return item
    
    def close_spider(self, spider):
        # 爬虫关闭时释放资源
        self.db_conn.close()</code></pre><h3>核心组件关系图</h3><pre style="display:none;"><code class="mermaid">graph TD
    A[Spider] --&gt;|生成Request| B[Engine]
    B --&gt;|传递Request| C[Scheduler]
    C --&gt;|返回待爬Request| B
    B --&gt;|传递Request| D[Downloader]
    D --&gt;|返回Response| B
    B --&gt;|传递Response| A
    A --&gt;|yield Item| B
    B --&gt;|传递Item| E[Pipeline]
    A --&gt;|yield新Request| B
    E --&gt;|处理Item| F[Database/File]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1e1
    style D fill:#e1ffe1
    style E fill:#f0e1ff</code></pre><p>Scrapy的工作流程是一个闭环:Spider生成初始Request → Engine调度 → Scheduler排队 → Downloader下载 → Engine传递响应 → Spider解析 → 提取数据或生成新Request → 循环往复。</p><h2>4. 实战演练:解决一个典型问题</h2><p>让我们通过一个完整的项目来实战Scrapy的核心功能。我们将爬取豆瓣电影Top250的信息,包括电影名称、评分、导演和简介。</p><h3>需求分析</h3><p>目标网站:<a href="https://link.segmentfault.com/?enc=w4rxo%2BIzYU7KwzTopHRZLw%3D%3D.bCAsakuiv5jzzo0DdKTvbJUTYUqeOjwtilxGE9BPobA%3D" rel="nofollow" target="_blank">https://movie.douban.com/top250</a>  <br/>需要提取的数据:电影标题、评分、导演、简介  <br/>特殊需求:实现翻页功能,爬取所有250部电影</p><h3>方案设计</h3><p>选择Scrapy的原因:</p><ul><li>高效的异步并发能力,能够快速爬取25页数据</li><li>内置的Request去重机制,避免重复爬取</li><li>灵活的Pipeline设计,便于数据清洗和存储</li></ul><p>技术方案:</p><ul><li>使用CSS选择器提取数据</li><li>通过翻页链接的规律实现自动翻页</li><li>将数据保存为CSV文件便于后续分析</li></ul><h3>代码实现</h3><p><strong>步骤1: 创建项目</strong></p><pre><code class="bash">scrapy startproject douban_movie
cd douban_movie</code></pre><p><strong>步骤2: 定义数据结构(items.py)</strong></p><pre><code class="python">import scrapy

class MovieItem(scrapy.Item):
    title = scrapy.Field()    # 电影标题
    rating = scrapy.Field()   # 评分
    director = scrapy.Field() # 导演
    intro = scrapy.Field()    # 简介</code></pre><p><strong>步骤3: 编写爬虫(spiders/movie_spider.py)</strong></p><pre><code class="python">import scrapy
from douban_movie.items import MovieItem

class MovieSpider(scrapy.Spider):
    name = 'douban_top250'
    allowed_domains = ['douban.com']
    start_urls = ['https://movie.douban.com/top250']
    
    def parse(self, response):
        # 提取当前页的所有电影条目
        movie_list = response.css('ol.grid_view li')
        
        for movie in movie_list:
            item = MovieItem()
            
            # 提取电影标题(可能存在中英文名,取第一个)
            item['title'] = movie.css('span.title::text').get()
            
            # 提取评分
            item['rating'] = movie.css('span.rating_num::text').get()
            
            # 提取导演信息
            info = movie.css('div.bd p::text').getall()
            if info:
                director_info = info[0].strip()
                # 导演信息格式:导演: 张三 主演: 李四 王五
                item['director'] = director_info.split('主演:')[0].replace('导演:', '').strip()
            
            # 提取简介(可能不存在)
            item['intro'] = movie.css('span.inq::text').get() or '暂无简介'
            
            yield item
        
        # 处理翻页
        next_page = response.css('span.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)</code></pre><p><strong>步骤4: 配置settings.py</strong></p><pre><code class="python"># 模拟浏览器User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# 不遵守robots协议(豆瓣的robots.txt禁止爬取)
ROBOTSTXT_OBEY = False

# 下载延迟,避免被封IP
DOWNLOAD_DELAY = 2

# 启用Pipeline
ITEM_PIPELINES = {
    'douban_movie.pipelines.DoubanMoviePipeline': 300,
}</code></pre><h3>运行说明</h3><p>执行以下命令启动爬虫:</p><pre><code class="bash">scrapy crawl douban_top250 -o movies.csv</code></pre><p>运行过程中你会看到类似以下的日志输出:</p><pre><code>2024-06-15 10:00:00 [scrapy.core.engine] INFO: Spider opened
2024-06-15 10:00:02 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://movie.douban.com/top250&gt;
2024-06-15 10:00:04 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://movie.douban.com/top250?start=25&amp;filter=&gt;
...
2024-06-15 10:01:30 [scrapy.statscollectors] INFO: Closing spider (finished)</code></pre><p>爬取完成后,<code>movies.csv</code>文件将包含所有250部电影的信息:</p><pre><code class="csv">title,rating,director,intro
肖申克的救赎,9.7,导演: 弗兰克·德拉邦特,希望让人自由。
霸王别姬,9.6,导演: 陈凯歌,风华绝代。
阿甘正传,9.5,导演: 罗伯特·泽米吉斯,人生就像一盒巧克力。
...</code></pre><p>整个爬取过程大约需要1-2分钟,相比传统串行爬虫速度提升了数倍。Scrapy自动处理了并发、去重、重试等复杂问题,让我们能够专注于数据提取逻辑本身。</p><h2>5. 最佳实践与常见陷阱</h2><h3>5.1 常见错误及规避方法</h3><p><strong>错误1: 覆盖parse方法导致CrawlSpider失效</strong></p><pre><code class="python"># ❌ 错误做法
class MySpider(CrawlSpider):
    name = 'my_spider'
    rules = [Rule(LinkExtractor(), callback='parse')]
    
    def parse(self, response):
        # 自定义parse方法会覆盖CrawlSpider的内置逻辑
        pass</code></pre><pre><code class="python"># ✅ 正确做法
class MySpider(CrawlSpider):
    name = 'my_spider'
    rules = [Rule(LinkExtractor(), callback='parse_item')]
    
    def parse_item(self, response):
        # 使用不同的回调函数名
        pass</code></pre><p><strong>错误2: 忘记返回Item导致Pipeline无法接收数据</strong></p><pre><code class="python"># ❌ 错误做法
def process_item(self, item, spider):
    self.db.insert(item)
    # 忘记返回item,后续Pipeline无法接收到数据</code></pre><pre><code class="python"># ✅ 正确做法
def process_item(self, item, spider):
    self.db.insert(item)
    return item  # 必须返回item或抛出DropItem</code></pre><p><strong>错误3: 直接修改Request的meta中保留键</strong></p><pre><code class="python"># ❌ 错误做法
yield scrapy.Request(url, callback=self.parse, meta={'redirect_urls': [...]})</code></pre><pre><code class="python"># ✅ 正确做法
yield scrapy.Request(url, callback=self.parse, meta={'custom_data': {...}})
# 避免使用Scrapy保留的meta键名,如redirect_urls、cookiejar等</code></pre><h3>5.2 最佳实践建议</h3><p><strong>1. 合理设置下载延迟</strong></p><pre><code class="python"># 根据目标网站的负载能力调整延迟
DOWNLOAD_DELAY = 2  # 对于豆瓣这样的网站,2秒较为合理
AUTOTHROTTLE_ENABLED = True  # 启用自动限速</code></pre><p><strong>2. 使用Item Loader简化数据提取</strong></p><pre><code class="python">from scrapy.loader import ItemLoader

def parse(self, response):
    loader = ItemLoader(item=MovieItem(), response=response)
    loader.add_css('title', 'span.title::text')
    loader.add_css('rating', 'span.rating_num::text')
    yield loader.load_item()</code></pre><p><strong>3. 配置日志级别便于调试</strong></p><pre><code class="python"># 开发环境使用DEBUG级别
LOG_LEVEL = 'DEBUG'

# 生产环境使用INFO或WARNING级别
LOG_LEVEL = 'INFO'</code></pre><p><strong>4. 使用管道链处理复杂数据流</strong></p><pre><code class="python">ITEM_PIPELINES = {
    'myproject.pipelines.ValidationPipeline': 100,  # 数据验证
    'myproject.pipelines.DeduplicationPipeline': 200,  # 去重
    'myproject.pipelines.StoragePipeline': 300,  # 存储
}</code></pre><h3>5.3 注意事项</h3><ul><li><strong>遵守robots协议</strong>:虽然可以设置<code>ROBOTSTXT_OBEY = False</code>,但建议尽量遵守网站的robots.txt规定,做一个文明的爬虫</li><li><strong>控制并发数</strong>:默认并发数为16,对于小型网站建议降低到8或更低,避免给服务器造成过大压力</li><li><strong>处理异常</strong>:在parse方法中使用try-except捕获异常,避免个别页面解析失败导致整个爬虫中断</li><li><strong>善用Scrapy Shell</strong>:使用<code>scrapy shell URL</code>命令调试选择器,确保提取逻辑正确后再写入爬虫代码</li><li><strong>监控爬虫状态</strong>:使用Scrapy提供的stats collector监控爬虫运行状态,及时发现异常</li></ul><h2>6. 进阶指引</h2><p>掌握了Scrapy的基础用法后,你可以继续探索以下高级特性:</p><p><strong>1. 中间件(Middleware)</strong>  <br/>中间件提供了在请求/响应处理过程中插入自定义逻辑的钩子。典型应用场景包括:</p><ul><li>动态切换User-Agent和代理IP</li><li>实现请求重试和异常处理</li><li>修改请求头和响应内容</li></ul><p><strong>2. 分布式爬虫</strong>  <br/>通过<code>scrapy-redis</code>扩展,可以实现分布式爬虫,多个爬虫节点共享同一个Redis队列,协同处理大规模爬取任务。</p><p><strong>3. 动态网页渲染</strong>  <br/>对于需要JavaScript渲染的页面,可以集成<code>scrapy-splash</code>或<code>scrapy-playwright</code>,实现动态内容的抓取。</p><p><strong>4. 数据存储扩展</strong>  <br/>除了CSV和JSON,Scrapy Pipeline可以轻松对接各种数据库:</p><ul><li>MySQL/PostgreSQL:使用<code>pymysql</code>或<code>psycopg2</code>驱动</li><li>MongoDB:使用<code>pymongo</code>驱动</li><li>Redis:使用<code>redis</code>驱动</li></ul><p><strong>学习资源推荐:</strong></p><ul><li><strong>官方文档</strong>:<a href="https://link.segmentfault.com/?enc=%2B%2BTQML3ik1jXKl6vp5fBRA%3D%3D.2wFpNZ8de4Lf6nZP6jNO2iMxV6Bi%2Ba%2FkaMvFLEBq92Q%3D" rel="nofollow" target="_blank">https://docs.scrapy.org</a> - 最权威和全面的学习资料</li><li><strong>GitHub仓库</strong>:<a href="https://link.segmentfault.com/?enc=pVgGWh2SwLfjm4vlrr5%2FGQ%3D%3D.V3B2fY5cZLuSgUs08WIaKVtLQtMK4GRqkCRLrFcfCiE0vcIRWPJ6543OeAIs4EeO" rel="nofollow" target="_blank">https://github.com/scrapy/scrapy</a> - 查看源码和提交问题</li><li><strong>Stack Overflow</strong>:搜索<code>scrapy</code>标签,解决具体问题</li><li><strong>实战项目</strong>:尝试爬取不同类型的网站(电商、新闻、社交媒体),积累实战经验</li></ul><p>Scrapy的学习曲线虽然略陡,但一旦掌握,你就拥有了构建高性能爬虫系统的强大工具。从简单的数据采集到复杂的分布式爬虫,Scrapy都能胜任。开始你的Scrapy之旅吧!</p>]]></description></item><item>    <title><![CDATA[蓝易云cdn:如何在redis中设置文件夹名java 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047598486</link>    <guid>https://segmentfault.com/a/1190000047598486</guid>    <pubDate>2026-02-07 13:02:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>下面这件事<strong>必须先说清楚本质</strong>，否则后面全是弯路。</p><hr/><h2>一、先给结论：<strong>Redis 里根本没有“文件夹”这个概念</strong> ❗</h2><p>Redis 是<strong>内存型 Key-Value 数据库</strong>，不是文件系统。<br/>它<strong>不存在目录、文件夹、层级路径</strong>，只有：</p><blockquote><strong>Key（键） → Value（值）</strong></blockquote><p>你在 Redis 客户端里看到的“文件夹感”，<strong>只是 Key 的命名规则造成的视觉假象</strong>。</p><p>👉 <strong>所谓“在 Redis 中设置文件夹名”</strong>，本质就是：<br/><strong>通过 Key 的命名规范，模拟“文件夹 / 目录层级”结构</strong>。</p><hr/><h2>二、Redis“文件夹”的正确实现方式（核心原理）🧠</h2><h3>✅ 统一规则：<strong>用分隔符组织 Key 的命名空间</strong></h3><p>企业级 Redis 约定俗成的做法是：</p><ul><li>使用 <code>:</code> 或 <code>/</code> 作为<strong>逻辑分隔符</strong></li><li>每一段代表一层“目录语义”</li></ul><p>例如：</p><pre><code>cdn:file:hash:12345
cdn:file:meta:12345
cdn:config:node:beijing</code></pre><p>你看到的是“文件夹”，Redis 看到的是：</p><blockquote>一个普通字符串 Key</blockquote><hr/><h2>三、<strong>标准推荐的 Key 命名规范（务实版）</strong> ✅</h2><h3>🔑 统一结构公式</h3><pre><code class="text">系统名:业务模块:子模块:唯一标识</code></pre><h3>📌 蓝易云 CDN 场景示例</h3><pre><code class="text">bluecdn:file:content:md5
bluecdn:file:meta:md5
bluecdn:cache:node:ip</code></pre><blockquote>这种结构的好处：</blockquote><ul><li><strong>&lt;span style="color:red"&gt;可读性强&lt;/span&gt;</strong></li><li><strong>&lt;span style="color:red"&gt;避免 Key 冲突&lt;/span&gt;</strong></li><li><strong>&lt;span style="color:red"&gt;方便按“文件夹”批量管理&lt;/span&gt;</strong></li></ul><hr/><h2>四、Java 中“设置文件夹名”的标准写法（实战）☕️</h2><h3>示例 1：最基础的“文件夹 Key”</h3><pre><code class="java">String key = "bluecdn:file:content:abc123";
redisTemplate.opsForValue().set(key, "文件内容");</code></pre><h4>解释（逐行）👇</h4><ul><li><code>bluecdn</code>：系统命名空间，避免与其他业务混用</li><li><code>file</code>：逻辑“文件夹”</li><li><code>content</code>：子模块</li><li><code>abc123</code>：唯一标识（如 MD5、文件ID）</li></ul><p>📌 <strong>Redis 不会创建任何目录</strong>，只是存了一个字符串 Key。</p><hr/><h3>示例 2：用 Hash 模拟“文件夹下多个文件” 📂</h3><pre><code class="java">String folderKey = "bluecdn:file:meta:abc123";
redisTemplate.opsForHash().put(folderKey, "size", "1024");
redisTemplate.opsForHash().put(folderKey, "type", "jpg");</code></pre><h4>解释 👇</h4><ul><li><code>folderKey</code>：逻辑“文件夹”</li><li>Hash 的 field：相当于“文件属性”</li><li>Hash 的 value：属性值</li></ul><p>📌 <strong>一个 Hash = 一个逻辑目录</strong></p><hr/><h2>五、如何“查看某个文件夹下的内容”？（关键点）🔍</h2><p>Redis <strong>不能像文件系统那样 ls 目录</strong>，只能靠 <strong>Key 匹配规则</strong>。</p><h3>Java 中正确姿势</h3><pre><code class="java">Set&lt;String&gt; keys = redisTemplate.keys("bluecdn:file:*");</code></pre><h4>解释 👇</h4><ul><li><code>*</code>：通配符</li><li>匹配所有 <code>bluecdn:file:</code> 开头的 Key</li><li>相当于“查看这个文件夹下的所有文件”</li></ul><p>⚠️ <strong>生产环境注意</strong>：<br/>大 Key 数量场景应避免 <code>keys</code>，应使用 <strong>SCAN</strong>（迭代扫描）。</p><hr/><h2>六、推荐的“目录结构设计表”（企业级）📊</h2><table><thead><tr><th>业务场景</th><th>Key 示例</th><th>说明</th></tr></thead><tbody><tr><td>文件内容</td><td><code>bluecdn:file:content:id</code></td><td>文件主体</td></tr><tr><td>文件元数据</td><td><code>bluecdn:file:meta:id</code></td><td>大小、类型</td></tr><tr><td>节点缓存</td><td><code>bluecdn:node:cache:ip</code></td><td>节点状态</td></tr><tr><td>防护规则</td><td><code>bluecdn:waf:rule:id</code></td><td>WAF 规则</td></tr></tbody></table><p>👉 <strong>&lt;span style="color:red"&gt;目录是逻辑的，规则才是核心&lt;/span&gt;</strong></p><hr/><h2>七、常见误区，必须避开 ❌</h2><h3>❌ 误区 1：以为 Redis 会创建目录</h3><p>👉 Redis <strong>不会、也不需要</strong></p><h3>❌ 误区 2：Key 命名随意</h3><p>👉 后期<strong>无法维护、无法清理、无法迁移</strong></p><h3>❌ 误区 3：用 <code>/</code> 当真目录</h3><p>👉 <code>/</code> 和 <code>:</code> <strong>没有任何功能差异</strong></p><hr/><h2>八、一句话总结（给决策者的）🎯</h2><blockquote>**Redis 没有文件夹，只有 Key。<br/>所谓“文件夹名”，本质是 &lt;span style="color:red"&gt;Key 命名空间设计能力&lt;/span&gt;。<br/>设计得好，Redis 就是高性能目录树；<br/>设计得乱，它就是内存垃圾堆。**</blockquote>]]></description></item><item>    <title><![CDATA[蓝易云cdn:美国高防云服务器能防御哪些攻击？ 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047598488</link>    <guid>https://segmentfault.com/a/1190000047598488</guid>    <pubDate>2026-02-07 13:01:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>下面直接进入核心，不兜圈子。</p><hr/><h2><strong>美国高防云服务器能防御哪些攻击？（蓝易云 CDN 场景解析）</strong></h2><p>在当前的网络安全对抗环境中，美国高防云服务器的价值不在“能不能扛”，而在于<strong>&lt;span style="color:red"&gt;能否在攻击持续、手法演进的情况下保持业务可用&lt;/span&gt;</strong>。从工程实践角度看，它主要覆盖以下几大类攻击面。</p><hr/><h3>一、<strong>DDoS 洪水型攻击（核心防御能力）</strong> 🌊</h3><p>这是高防服务器存在的首要理由。</p><h4>可防御类型</h4><ul><li><strong>UDP Flood</strong></li><li><strong>ICMP Flood</strong></li><li><strong>TCP SYN Flood</strong></li><li><strong>ACK / RST Flood</strong></li><li><strong>混合型反射放大攻击</strong></li></ul><h4>防御原理（简化逻辑）</h4><pre><code class="text">异常流量 → 清洗中心 → 特征识别 → 丢弃攻击包 → 回源正常流量</code></pre><ul><li>大带宽承载用于<strong>吸收流量峰值</strong></li><li>清洗节点通过<strong>包速率、协议异常、源分布</strong>判断攻击</li><li>正常请求被保留并回源</li></ul><p>📌 关键点在于：<br/><strong>&lt;span style="color:red"&gt;防的是“量级 + 持续性”，而不是单点规则&lt;/span&gt;</strong></p><hr/><h3>二、<strong>CC 攻击（应用层消耗型攻击）</strong> 🧠</h3><p>CC 攻击不靠带宽，靠“像人一样访问”。</p><h4>常见形式</h4><ul><li>高频 HTTP GET/POST</li><li>慢连接（Slow Request）</li><li>模拟正常浏览路径的并发请求</li></ul><h4>美国高防云服务器的应对方式</h4><table><thead><tr><th>防御手段</th><th>说明</th></tr></thead><tbody><tr><td>访问频率限制</td><td>单 IP / 单会话 QPS 控制</td></tr><tr><td>行为分析</td><td>识别非人类访问节奏</td></tr><tr><td>会话校验</td><td>Cookie / Token 校验</td></tr><tr><td>挑战机制</td><td>动态验证请求有效性</td></tr></tbody></table><p>📌 本质是：<br/><strong>&lt;span style="color:red"&gt;让攻击成本无限接近真实用户成本&lt;/span&gt;</strong></p><hr/><h3>三、<strong>协议层畸形攻击（低层但致命）</strong> ⚙️</h3><p>这类攻击流量不大，但直接打协议实现漏洞。</p><h4>可防御类型</h4><ul><li>TCP 半连接耗尽</li><li>畸形 TCP Flag 组合</li><li>非法 MSS / Window Size</li><li>重放包攻击</li></ul><h4>防御机制说明</h4><ul><li>协议栈参数硬化</li><li>状态表容量保护</li><li>异常包即时丢弃</li></ul><p>📌 这类防御<strong>极度依赖底层网络与内核调优</strong>，普通云服务器基本无解。</p><hr/><h3>四、<strong>反射与放大攻击</strong> 🔁</h3><p>典型特征：<br/><strong>小请求 → 大响应 → 目标被淹没</strong></p><h4>常见攻击源</h4><ul><li>NTP</li><li>DNS</li><li>SSDP</li><li>Memcached</li></ul><h4>高防服务器的处理逻辑</h4><pre><code class="text">识别反射特征 → 阻断响应回程 → 清洗异常源</code></pre><p>📌 防御重点不是“挡住请求”，而是：<br/><strong>&lt;span style="color:red"&gt;阻断被利用的回包路径&lt;/span&gt;</strong></p><hr/><h3>五、<strong>扫描、探测与撞库类攻击</strong> 🔍</h3><p>虽然不是传统意义的大流量攻击，但对业务风险极高。</p><h4>可防御行为</h4><ul><li>端口扫描</li><li>服务指纹探测</li><li>登录接口撞库</li><li>异常路径探测</li></ul><h4>防御手段</h4><table><thead><tr><th>行为</th><th>防护方式</th></tr></thead><tbody><tr><td>高频扫描</td><td>自动封禁源</td></tr><tr><td>异常路径</td><td>规则阻断</td></tr><tr><td>登录异常</td><td>访问节流</td></tr></tbody></table><p>📌 目标只有一个：<br/><strong>&lt;span style="color:red"&gt;不让攻击者摸清你的系统结构&lt;/span&gt;</strong></p><hr/><h3>六、<strong>与 CDN + 高防联动时的攻击覆盖面</strong> 🚀</h3><p>当美国高防云服务器与 CDN 架构配合时，防御能力会发生质变。</p><h4>联动后的效果</h4><ul><li>攻击被<strong>提前在边缘节点拦截</strong></li><li>源站 IP 完全隐藏</li><li>CC 攻击被拆散到多个节点</li></ul><pre><code class="text">攻击者 → CDN 节点 → 清洗 → 高防服务器 → 业务</code></pre><p>📌 实战价值在于：<br/><strong>&lt;span style="color:red"&gt;攻击永远打不到真正的源头&lt;/span&gt;</strong></p><hr/><h3>七、能力边界说明（务实，不吹）⚠️</h3><p>必须说清楚，美国高防云服务器<strong>不解决所有安全问题</strong>。</p><table><thead><tr><th>不属于防御范围</th><th>原因</th></tr></thead><tbody><tr><td>业务逻辑漏洞</td><td>属于代码层问题</td></tr><tr><td>内部权限滥用</td><td>非网络攻击</td></tr><tr><td>程序自身 Bug</td><td>需开发修复</td></tr></tbody></table><p>📌 高防解决的是：<br/><strong>&lt;span style="color:red"&gt;可用性与抗压能力&lt;/span&gt;</strong>，而不是代码安全本身。</p><hr/><h3>八、总结（一句话定性）🎯</h3><blockquote>**美国高防云服务器的核心价值在于：<br/>在面对 &lt;span style="color:red"&gt;大规模、持续、多形态网络攻击&lt;/span&gt; 时，<br/>依然能让业务保持“能访问、不中断、不崩溃”。**</blockquote><p>对蓝易云 CDN 这类业务来说，它不是“可选项”，而是<strong>抗风险的基础设施</strong>。</p><p>从工程视角看，这不是“买防御”，而是<strong>为业务争取生存时间</strong>。</p>]]></description></item><item>    <title><![CDATA[CentOS 7 老树开新花：从零部署 Dify 全栈应用（含 Go/Rust/GCC 升级避坑） ]]></title>    <link>https://segmentfault.com/a/1190000047598515</link>    <guid>https://segmentfault.com/a/1190000047598515</guid>    <pubDate>2026-02-07 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>CentOS 7 老树开新花：从零部署 Dify 全栈应用（含 Go/Rust/GCC 升级避坑）</h2><blockquote>本文档适用于在 <strong>CentOS 7</strong> 环境下使用源代码部署 Dify 应用，对应版本 <code>1.9.2</code>。由于系统较旧，部分依赖需手动升级或通过容器化方式解决兼容性问题。</blockquote><hr/><h3>一、安装与配置 Docker</h3><h4>1. 卸载旧版本 Docker（如有）</h4><pre><code class="bash">sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine</code></pre><h4>2. 安装必要依赖</h4><pre><code class="bash">sudo yum install -y yum-utils device-mapper-persistent-data lvm2</code></pre><h4>3. 添加 Docker 官方 YUM 源</h4><pre><code class="bash">sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</code></pre><h4>4. 安装 Docker Engine 及相关组件</h4><pre><code class="bash">sudo yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code></pre><h4>5. 启动并设置开机自启</h4><pre><code class="bash">sudo systemctl start docker
sudo systemctl enable docker</code></pre><h4>6. 配置国内镜像加速器</h4><p>&lt;!-- more --&gt;</p><p>创建 <code>/etc/docker/daemon.json</code> 文件：</p><pre><code class="bash">sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'
{
  "registry-mirrors": [
    "https://docker.registry.cyou",
    "https://docker-cf.registry.cyou",
    "https://dockercf.jsdelivr.fyi",
    "https://docker.jsdelivr.fyi",
    "https://dockertest.jsdelivr.fyi",
    "https://mirror.aliyuncs.com",
    "https://dockerproxy.com",
    "https://mirror.baidubce.com",
    "https://docker.m.daocloud.io",
    "https://docker.nju.edu.cn",
    "https://docker.mirrors.sjtug.sjtu.edu.cn",
    "https://docker.mirrors.ustc.edu.cn",
    "https://mirror.iscas.ac.cn",
    "https://docker.rainbond.cc",
    "https://do.nark.eu.org",
    "https://dc.j8.work",
    "https://gst6rzl9.mirror.aliyuncs.com",
    "https://registry.docker-cn.com",
    "http://hub-mirror.c.163.com",
    "http://mirrors.ustc.edu.cn/",
    "https://mirrors.tuna.tsinghua.edu.cn/",
    "http://mirrors.sohu.com/"
  ]
}
EOF</code></pre><blockquote>⚠️ <strong>注意</strong>：修改后需重载配置并重启 Docker：</blockquote><pre><code class="bash">sudo systemctl daemon-reload
sudo systemctl restart docker</code></pre><h4>7. 将当前用户加入 <code>docker</code> 用户组（避免每次使用 <code>sudo</code>）</h4><pre><code class="bash"># 创建 docker 组（若不存在）
sudo groupadd docker

# 将当前用户加入 docker 组
sudo usermod -aG docker $USER

# 刷新组权限（关键！否则需重新登录）
newgrp docker</code></pre><hr/><h3>二、部署 Dify API 服务</h3><h4>1. 准备中间件服务（如 Redis、PostgreSQL 等）</h4><ul><li>修改 <code>docker-compose.middleware.yaml</code> 和 <code>middleware.env</code> 中的数据卷路径</li><li>上传整个 <code>docker/</code> 目录到服务器</li></ul><h5>启动中间件</h5><pre><code class="bash">cd /data/dify/docker
docker compose -f docker-compose.middleware.yaml up -d</code></pre><blockquote>停止命令：<code>docker compose -f docker-compose.middleware.yaml down</code></blockquote><hr/><h4>2. 安装构建依赖环境</h4><blockquote><strong>原因</strong>：Dify 使用的 <code>wandb &gt;= 0.16.0</code> 要求本地存在 Go 编译环境；同时 <code>numpy==2.4.1</code> 需要 GCC ≥ 9.3，而 CentOS 7 默认 GCC 仅为 4.8.5。</blockquote><h5>(1) 安装 Go（1.23.0）</h5><pre><code class="bash"># 下载（使用国内镜像）
wget -O go1.23.0.linux-amd64.tar.gz https://golang.google.cn/dl/go1.23.0.linux-amd64.tar.gz

# 解压到 /usr/local
sudo tar -C /usr/local -xzf go1.23.0.linux-amd64.tar.gz

# 配置 PATH
echo 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; ~/.bashrc
source ~/.bashrc

# 验证
go version</code></pre><h5>(2) 安装 Rust（使用 rsproxy.cn 镜像）</h5><pre><code class="bash"># 下载安装脚本
wget -O rustup-init.sh https://rsproxy.cn/rustup-init.sh
chmod +x rustup-init.sh

# 设置国内镜像源
export RUSTUP_DIST_SERVER=https://rsproxy.cn
export RUSTUP_UPDATE_ROOT=https://rsproxy.cn/rustup

# 静默安装（不修改 PATH）
./rustup-init.sh -y --no-modify-path

# 临时加载环境变量
source "$HOME/.cargo/env"

# 验证
rustc --version
cargo --version</code></pre><h5>(3) 升级 GCC 至 9.3+</h5><pre><code class="bash"># 启用 SCL 源
sudo yum install -y centos-release-scl

# 安装 devtoolset-9
sudo yum install -y devtoolset-9-gcc devtoolset-9-gcc-c++

# 启用新 GCC（仅当前 shell 有效）
scl enable devtoolset-9 bash

# 验证
gcc --version  # 应显示 9.3.x</code></pre><blockquote>✅ <strong>建议</strong>：将 <code>scl enable devtoolset-9 bash</code> 加入 <code>~/.bashrc</code> 以持久生效（但注意可能影响其他程序）。</blockquote><h5>(4) 安装 <code>uv</code>（现代 Python 包管理器）</h5><pre><code class="bash">curl -LsSf https://astral.sh/uv/install.sh | sh
source "$HOME/.local/bin/env"</code></pre><hr/><h4>3. 部署 API 服务</h4><ul><li>修改 <code>.env</code> 文件中的数据库地址、存储路径、日志目录等配置。</li><li>上传 <code>api/</code> 目录到服务器（首次上传时请注释掉 <code>scp-api.sh</code> 中的启动逻辑）。</li></ul><h5>首次启动流程</h5><pre><code class="bash">cd /data/dify/api

# 安装依赖
uv sync

# 执行数据库迁移（首次必须运行）
flask db upgrade

# 后台启动 API 服务
nohup gunicorn -w 4 -k gevent --bind 0.0.0.0:5019 app:app &gt; dify-api.log 2&gt;&amp;1 &amp;</code></pre><h5>启动 Celery Worker</h5><pre><code class="bash">cd /data/dify/api

# 后台启动 Worker
nohup uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO \
  -Q dataset,generation,mail,ops_trace &gt; dify-worker.log 2&gt;&amp;1 &amp;</code></pre><blockquote><p>🔁 <strong>后续重启</strong>：只需执行</p><pre><code class="shell"># 启动API服务
nohup gunicorn -w 4 -k gevent --bind 0.0.0.0:5019 app:app &gt; dify-api.log 2&gt;&amp;1 &amp;
# 启动worker
nohup uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO \
  -Q dataset,generation,mail,ops_trace &gt; dify-worker.log 2&gt;&amp;1 &amp;</code></pre></blockquote><hr/><h3>三、部署 Dify Web 前端</h3><blockquote><strong>说明</strong>：CentOS 7 无法原生安装 Node.js 20+，因此采用 <strong>Docker 容器化部署</strong>。</blockquote><h4>1. 构建 Web 镜像（在开发机上操作）</h4><h5>(1) 本地编译（需 Node.js ≥ 22）</h5><pre><code class="bash"># 安装依赖
pnpm install --frozen-lockfile

# 构建（内存不足时增加堆大小）
NODE_OPTIONS="--max_old_space_size=4096" NEXT_CONCURRENT_BUILD_LIMIT=1 pnpm build

DIR1="web/.next/standalone/.next"

# 创建目录（-p 表示递归创建，且不报错如果已存在）
mkdir -p "$DIR1" 
cp -r web/.next/static web/.next/standalone/.next/static &amp;&amp; cp -r web/public web/.next/standalone/public </code></pre><blockquote>构建产物位于 <code>standalone/</code> 目录。</blockquote><h5>(2) 编写 Dockerfile</h5><pre><code class="Dockerfile"># 使用官方 Node.js 22 Alpine 镜像
FROM node:22-alpine

WORKDIR /app

# 复制构建产物
COPY standalone ./

EXPOSE 3000

CMD ["node", "server.js"]</code></pre><h4>2. 在服务器部署 Web 服务</h4><pre><code class="bash">cd /data/dify/web

# 1. 清理旧容器与镜像
docker stop my-dify-web &amp;&amp; docker rm my-dify-web &amp;&amp; docker rmi my-dify-web

# 2. 解压新构建包（覆盖 standalone/）
tar -xzf dify-web-standalone.tar.gz

# 3. 构建新镜像
docker build -t my-dify-web .

# 4. 启动容器
docker run -d \
  --name my-dify-web \
  -p 3000:3000 \
  my-dify-web</code></pre><h4>3. 配置 Web 环境变量</h4><ul><li>修改 <code>standalone/.env.local</code> 中的 <code>NEXT_PUBLIC_API_URL</code> 和 <code>NEXT_PUBLIC_WEB_URL</code>，指向实际 API 与 Web 地址。</li></ul><blockquote>🔄 <strong>更新 Web 服务</strong>：重复上述“清理 → 解压 → 构建 → 启动”流程，或封装为脚本自动化。</blockquote><hr/><h3>四、注意事项</h3><ol><li><strong>权限问题</strong>：确保 <code>/data/dify/</code> 目录对当前用户可读写。</li><li><strong>防火墙</strong>：开放 5019（API）、3000（Web）、以及中间件所需端口（如 6379、5432 等）。</li><li><strong>日志监控</strong>：定期检查 <code>dify-api.log</code> 和 <code>dify-worker.log</code>。</li><li><strong>环境持久化</strong>：若使用 <code>scl enable</code>，建议在 <code>~/.bashrc</code> 中添加 alias 或 wrapper 脚本。</li></ol><hr/><p>✅ 至此，Dify 已在 CentOS 7 上完整部署。  <br/>如遇问题，请优先检查依赖版本、网络连通性及配置文件路径。</p><hr/><p>希望这份部署文档能帮助你和团队更高效地完成部署！</p><p>本文由<a href="https://link.segmentfault.com/?enc=cMOQoNA515Q07a%2Boqa%2FhjA%3D%3D.oahddeXwM4KhIkqDnWo3GcadweDdEih9D7PR%2FkelUg4%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[STM32必会EXTI外部中断事件控制器 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047598392</link>    <guid>https://segmentfault.com/a/1190000047598392</guid>    <pubDate>2026-02-07 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>在嵌入式开发中，中断是一个非常重要的概念。</p><p>它允许 MCU 在执行主程序的同时，能够及时响应外部事件，比如按键按下、传感器信号变化等。</p><p>今天我们就来深入学习 STM32 的 EXTI 外部中断事件控制器，这是每个 STM32 开发者都必须掌握的核心知识。</p><h2>1. EXTI 外部中断事件控制器概述</h2><h3>1.1 什么是 EXTI</h3><p>EXTI 是 STM32 中用于管理外部中断和事件的控制器。</p><p>它可以检测 GPIO 引脚上的电平变化，并在满足触发条件时产生中断或事件。</p><p>简单来说，EXTI 就像是一个"门卫"，时刻监视着外部世界的变化，一旦发现符合条件的信号，就立即通知 CPU 去处理。</p><p>在实际项目中，我曾经用 EXTI 来处理紧急停止按钮。</p><p>当操作人员按下急停按钮时，系统必须在几微秒内做出响应，停止所有运动部件。</p><p>如果用轮询的方式去检测按钮状态，可能会因为主程序正在执行其他任务而延迟响应，但使用 EXTI 中断就能保证最快的响应速度。</p><h3>1.2 EXTI 的主要特性</h3><p>STM32 的 EXTI 控制器具有以下特性：</p><ol><li>支持多达 23 条外部中断/事件线（具体数量因芯片型号而异）</li><li>每条中断线都可以独立配置触发方式：上升沿、下降沿或双边沿触发</li><li>每个 GPIO 引脚都可以配置为外部中断源</li><li>支持软件触发中断</li><li>具有独立的挂起状态位和屏蔽位</li><li>可以产生中断请求或事件请求</li></ol><p>需要注意的是，STM32 的 EXTI 有一个重要的限制：相同编号的 GPIO 引脚共享同一条 EXTI 线。</p><p>比如 PA0、PB0、PC0 都连接到 EXTI0 线，这意味着你不能同时将 PA0 和 PB0 都配置为外部中断，只能选择其中一个。</p><h2>2. EXTI 工作原理</h2><h3>2.1 EXTI 的内部结构</h3><p>EXTI 控制器主要由以下几个部分组成：</p><ol><li><strong>边沿检测电路</strong>：负责检测输入信号的上升沿、下降沿或双边沿</li><li><strong>软件中断事件寄存器</strong>：允许通过软件触发中断</li><li><strong>挂起请求寄存器</strong>：记录哪些中断线有挂起的中断请求</li><li><strong>中断屏蔽寄存器</strong>：控制哪些中断线被使能</li><li><strong>事件屏蔽寄存器</strong>：控制哪些事件线被使能</li></ol><p>当外部信号满足触发条件时，EXTI 会将对应的挂起位置 1，如果该中断线没有被屏蔽，就会向 NVIC（嵌套向量中断控制器）发送中断请求。</p><h3>2.2 中断与事件的区别</h3><p>EXTI 可以产生两种类型的输出：中断和事件。</p><p>很多初学者容易混淆这两个概念。</p><p><strong>中断</strong>：会触发 CPU 执行中断服务程序（ISR），需要软件介入处理。</p><p>当中断发生时，CPU 会暂停当前任务，跳转到中断服务函数执行，处理完成后再返回主程序。</p><p><strong>事件</strong>：不会触发 CPU 中断，而是产生一个脉冲信号，可以触发其他外设的操作，比如启动 ADC 转换、触发 DMA 传输等，整个过程不需要 CPU 参与，实现了硬件级的联动。</p><p>在我做汽车电子项目时，经常使用事件模式来触发 ADC 采样。</p><p>比如每隔固定时间需要采集传感器数据，我会用定时器产生 EXTI 事件，然后这个事件直接触发 ADC 开始转换，整个过程不占用 CPU 资源，效率非常高。</p><h2>3. EXTI 配置步骤</h2><h3>3.1 使用 HAL 库配置 EXTI 的基本流程</h3><p>使用 STM32 HAL 库配置 EXTI 外部中断主要包括以下步骤：</p><ol><li>使能 GPIO 时钟</li><li>配置 GPIO 引脚为输入模式</li><li>配置 EXTI 中断线</li><li>配置 NVIC 中断优先级</li><li>编写中断服务函数</li></ol><p>下面我用一个实际的按键中断例子来说明整个配置过程。</p><h3>3.2 按键外部中断配置示例</h3><p>假设我们使用 PA0 引脚连接一个按键，按键按下时引脚电平为低，松开时为高（上拉输入）。</p><p>我们希望在按键按下（下降沿）时触发中断。</p><pre><code>/* 1. GPIO初始化配置 */
void MX_GPIO_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    /* 使能GPIOA时钟 */
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    /* 配置PA0为输入模式，上拉，外部中断模式 */
    GPIO_InitStruct.Pin = GPIO_PIN_0;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_FALLING;  // 下降沿触发中断
    GPIO_InitStruct.Pull = GPIO_PULL_UP;          // 上拉
    HAL_GPIO_Init(GPIOA, &amp;GPIO_InitStruct);
    
    /* 配置NVIC中断优先级 */
    HAL_NVIC_SetPriority(EXTI0_IRQn, 2, 0);
    
    /* 使能EXTI0中断 */
    HAL_NVIC_EnableIRQ(EXTI0_IRQn);
}
​
/* 2. 中断服务函数 */
void EXTI0_IRQHandler(void)
{
    /* 调用HAL库的中断处理函数 */
    HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_0);
}
​
/* 3. 中断回调函数 */
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 按键按下，执行相应操作 */
        // 这里可以添加你的业务逻辑
        // 比如翻转LED状态
        HAL_GPIO_TogglePin(GPIOC, GPIO_PIN_13);
    }
}</code></pre><h3>3.3 配置参数详解</h3><p>在上面的代码中，有几个关键的配置参数需要理解：</p><p><strong>GPIO\_MODE\_IT\_FALLING</strong>：这个参数指定了中断触发方式。</p><p>HAL 库提供了以下几种选择：</p><ul><li><code>GPIO_MODE_IT_RISING</code>：上升沿触发</li><li><code>GPIO_MODE_IT_FALLING</code>：下降沿触发</li><li><code>GPIO_MODE_IT_RISING_FALLING</code>：双边沿触发</li></ul><p><strong>GPIO\_PULL\_UP</strong>：配置 GPIO 的上拉/下拉电阻。</p><p>选项包括：</p><ul><li><code>GPIO_NOPULL</code>：无上拉下拉</li><li><code>GPIO_PULLUP</code>：上拉</li><li><code>GPIO_PULLDOWN</code>：下拉</li></ul><p><strong>HAL\_NVIC\_SetPriority</strong>：设置中断优先级。</p><p>第二个参数是抢占优先级，第三个参数是子优先级。</p><p>抢占优先级高的中断可以打断抢占优先级低的中断，而子优先级只在抢占优先级相同时才起作用。</p><h2>4. EXTI 中断优先级管理</h2><h3>4.1 NVIC 中断优先级分组</h3><p>STM32 使用 NVIC 来管理所有中断，包括 EXTI 中断。</p><p>NVIC 支持中断优先级分组，通过 <code>HAL_NVIC_SetPriorityGrouping()</code> 函数来配置。</p><pre><code>/* 配置中断优先级分组为组2 */
HAL_NVIC_SetPriorityGrouping(NVIC_PRIORITYGROUP_2);</code></pre><p>不同的优先级分组方式决定了抢占优先级和子优先级的位数分配：</p><ul><li><code>NVIC_PRIORITYGROUP_0</code>：0 位抢占优先级，4 位子优先级</li><li><code>NVIC_PRIORITYGROUP_1</code>：1 位抢占优先级，3 位子优先级</li><li><code>NVIC_PRIORITYGROUP_2</code>：2 位抢占优先级，2 位子优先级</li><li><code>NVIC_PRIORITYGROUP_3</code>：3 位抢占优先级，1 位子优先级</li><li><code>NVIC_PRIORITYGROUP_4</code>：4 位抢占优先级，0 位子优先级</li></ul><h3>4.2 合理设置中断优先级</h3><p>在实际项目中，合理设置中断优先级非常重要。</p><p>一般遵循以下原则：</p><ol><li><strong>紧急程度高的中断设置高优先级</strong>：比如急停按钮、故障检测等</li><li><strong>执行时间短的中断可以设置高优先级</strong>：避免长时间占用 CPU</li><li><strong>相关性强的中断设置相近的优先级</strong>：便于管理和调试</li></ol><p>在我做的一个电机控制项目中，优先级设置如下：</p><pre><code>/* 急停按钮 - 最高优先级 */
HAL_NVIC_SetPriority(EXTI0_IRQn, 0, 0);
​
/* 编码器脉冲 - 高优先级 */
HAL_NVIC_SetPriority(EXTI1_IRQn, 1, 0);
​
/* 普通按键 - 中等优先级 */
HAL_NVIC_SetPriority(EXTI2_IRQn, 2, 0);
​
/* 通信接收 - 较低优先级 */
HAL_NVIC_SetPriority(USART1_IRQn, 3, 0);</code></pre><h2>5. EXTI 使用注意事项</h2><h3>5.1 按键消抖处理</h3><p>在使用 EXTI 处理按键输入时，必须考虑按键抖动问题。</p><p>机械按键在按下或松开的瞬间，触点会产生多次通断，导致产生多次中断。</p><p>有两种常用的消抖方法：</p><p><strong>方法一：软件延时消抖</strong></p><pre><code>void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 简单延时消抖 */
        HAL_Delay(10);  // 延时10ms
        
        /* 再次检测按键状态 */
        if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_0) == GPIO_PIN_RESET)
        {
            /* 确认按键按下，执行操作 */
            // 你的业务逻辑
        }
    }
}</code></pre><p>但是这种方法有个问题：在中断服务函数中使用延时会阻塞其他中断，不推荐在实际项目中使用。</p><p><strong>方法二：定时器消抖（推荐）</strong></p><pre><code>uint32_t last_interrupt_time = 0;
#define DEBOUNCE_TIME 50  // 50ms消抖时间
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        uint32_t current_time = HAL_GetTick();
        
        /* 检查距离上次中断的时间间隔 */
        if((current_time - last_interrupt_time) &gt; DEBOUNCE_TIME)
        {
            last_interrupt_time = current_time;
            
            /* 执行按键处理 */
            // 你的业务逻辑
        }
    }
}</code></pre><p>这种方法利用系统滴答定时器来判断时间间隔，不会阻塞其他中断，是更好的选择。</p><h3>5.2 中断服务函数的编写原则</h3><p>编写 EXTI 中断服务函数时，需要遵循以下原则：</p><ol><li><strong>尽量简短</strong>：中断服务函数应该尽快执行完毕，避免长时间占用 CPU</li><li><strong>避免使用延时函数</strong>：不要在中断中使用 <code>HAL_Delay()</code> 等阻塞函数</li><li><strong>避免复杂运算</strong>：复杂的计算应该在主程序中完成</li><li><strong>使用标志位</strong>：可以在中断中设置标志位，在主程序中检测标志位并处理</li></ol><pre><code>volatile uint8_t button_pressed = 0;  // 按键按下标志
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 只设置标志位，不做复杂处理 */
        button_pressed = 1;
    }
}
​
int main(void)
{
    /* 系统初始化 */
    HAL_Init();
    SystemClock_Config();
    MX_GPIO_Init();
    
    while(1)
    {
        /* 在主循环中检测标志位 */
        if(button_pressed)
        {
            button_pressed = 0;  // 清除标志
            
            /* 执行复杂的处理逻辑 */
            process_button_event();
        }
        
        /* 其他任务 */
    }
}</code></pre><h3>5.3 多个 EXTI 中断的处理</h3><p>当使用多个外部中断时，需要注意中断线的分配。</p><p>STM32 的 EXTI0 到 EXTI4 各有独立的中断向量，而 EXTI5 到 EXTI9 共享一个中断向量（EXTI9\_5\_IRQn），EXTI10 到 EXTI15 共享另一个中断向量（EXTI15\_10\_IRQn）。</p><pre><code>/* EXTI5-9共享中断处理函数 */
void EXTI9_5_IRQHandler(void)
{
    /* 检查是哪个引脚触发的中断 */
    if(__HAL_GPIO_EXTI_GET_IT(GPIO_PIN_5) != RESET)
    {
        HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_5);
    }
    
    if(__HAL_GPIO_EXTI_GET_IT(GPIO_PIN_6) != RESET)
    {
        HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_6);
    }
    
    // 其他引脚的处理...
}
​
/* 回调函数中区分不同的引脚 */
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    switch(GPIO_Pin)
    {
        case GPIO_PIN_5:
            /* 处理PIN5的中断 */
            break;
            
        case GPIO_PIN_6:
            /* 处理PIN6的中断 */
            break;
            
        default:
            break;
    }
}</code></pre><h2>6. EXTI 实战应用案例</h2><h3>6.1 旋转编码器接口</h3><p>旋转编码器是嵌入式系统中常用的输入设备，通常有 A、B 两相输出。</p><p>通过检测 A、B 相的相位关系可以判断旋转方向和速度。</p><p>使用 EXTI 可以很好地实现编码器接口。</p><pre><code>#define ENCODER_A_PIN GPIO_PIN_0
#define ENCODER_B_PIN GPIO_PIN_1
#define ENCODER_PORT GPIOA
​
volatile int32_t encoder_count = 0;
​
void Encoder_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    /* 配置A相为外部中断 */
    GPIO_InitStruct.Pin = ENCODER_A_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_RISING_FALLING;
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(ENCODER_PORT, &amp;GPIO_InitStruct);
    
    /* 配置B相为普通输入 */
    GPIO_InitStruct.Pin = ENCODER_B_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_INPUT;
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(ENCODER_PORT, &amp;GPIO_InitStruct);
    
    HAL_NVIC_SetPriority(EXTI0_IRQn, 1, 0);
    HAL_NVIC_EnableIRQ(EXTI0_IRQn);
}
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == ENCODER_A_PIN)
    {
        /* 读取A相和B相的状态 */
        uint8_t a_state = HAL_GPIO_ReadPin(ENCODER_PORT, ENCODER_A_PIN);
        uint8_t b_state = HAL_GPIO_ReadPin(ENCODER_PORT, ENCODER_B_PIN);
        
        /* 根据相位关系判断旋转方向 */
        if(a_state == b_state)
        {
            encoder_count++;  // 正转
        }
        else
        {
            encoder_count--;  // 反转
        }
    }
}</code></pre><h3>6.2 红外遥控接收</h3><p>红外遥控器发送的是脉宽调制信号，通过测量脉冲宽度可以解码出按键信息。</p><p>使用 EXTI 配合定时器可以实现红外信号的解码。</p><pre><code>#define IR_PIN GPIO_PIN_2
#define IR_PORT GPIOA
​
volatile uint32_t ir_start_time = 0;
volatile uint32_t ir_pulse_width = 0;
volatile uint8_t ir_data_ready = 0;
​
void IR_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    GPIO_InitStruct.Pin = IR_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_FALLING;  // 下降沿触发
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(IR_PORT, &amp;GPIO_InitStruct);
    
    HAL_NVIC_SetPriority(EXTI2_IRQn, 2, 0);
    HAL_NVIC_EnableIRQ(EXTI2_IRQn);
}
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == IR_PIN)
    {
        uint32_t current_time = HAL_GetTick();
        
        if(ir_start_time == 0)
        {
            /* 记录起始时间 */
            ir_start_time = current_time;
        }
        else
        {
            /* 计算脉冲宽度 */
            ir_pulse_width = current_time - ir_start_time;
            ir_start_time = current_time;
            ir_data_ready = 1;
            
            /* 根据脉冲宽度解码数据 */
            // 这里添加解码逻辑
        }
    }
}</code></pre><h2>7. 总结</h2><p>EXTI 外部中断事件控制器是 STM32 中非常重要的外设，掌握它对于开发响应式的嵌入式系统至关重要。</p><p>通过本文的学习，我们了解了 EXTI 的工作原理、配置方法以及实际应用技巧。</p><p>在实际开发中，使用 EXTI 需要注意以下几点：首先要合理设置中断优先级，确保重要的中断能够及时响应；其次要注意按键消抖等实际问题，避免误触发；最后要遵循中断服务函数简短高效的原则，复杂的处理逻辑应该在主程序中完成。</p><p>我在多年的嵌入式开发经验中，EXTI 几乎是每个项目都会用到的功能。</p><p>从简单的按键检测到复杂的编码器接口、红外遥控接收，EXTI 都能很好地胜任。</p><p>希望这篇文章能帮助大家更好地理解和使用 STM32 的 EXTI 功能，在实际项目中灵活运用。</p><p><strong>更多编程学习资源</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=NDK9ig8%2F3opeWFJt%2F%2BJzXw%3D%3D.DWLxjJrN8rKKGaCA2jFlgSBeA149E7KPwHrYUTbXSLl7njHd0DD0HB4Am30qBa9TrUPH%2F3diqE%2F5FEb0cBgoPg%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=VrasN4pA6%2F1ljSd%2BoW1iFg%3D%3D.9IcQMtH3VuO66lhVtvd7V7atna0iGN5ksE4YuzLyt%2Bf%2FQEhY5qbKqWOdWrqR2%2Bqmi4jZoAGzAhtHCnJ1Q0P7oA%3D%3D" rel="nofollow" target="_blank">STM32 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=nMtHMl5qpUFP6Q7vLKegIA%3D%3D.2vWLOisvFYuZY2QKEIJ%2BIOFwTkIO72nRHiUUdrIsHih%2FxrUSZpYlG2%2BcSbeUqlt8hj6Pp6b8%2B%2BkMoav3K%2FITPah4PFiCyJdHxFrszOMAfVI%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=oL01PYvBM3oRCTBlzXANHg%3D%3D.L9oFyW9eun%2BL5HcMwuFRFQYTXFSyvBGv3el1FS58rsqF5cwT79JXfkA2o4Cy4iyTU%2BEWuDVey4E7y2dclFX0FA%3D%3D" rel="nofollow" target="_blank">C++ 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=2gNNxYqfVHD9QdpV7qidIg%3D%3D.0L%2FBZQkdVsVn33mrFfPEFiE%2FcDcnL0l5XHtzc7Vb0JNiiT77S3%2F0uA5gCBl13rVbYcsGBUPq5yDtBy1ROeiJOQ%3D%3D" rel="nofollow" target="_blank">51 单片机零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=Ht1hNzHyM3ammSL61cjZOA%3D%3D.Vs7whHRnv42ZQFgF88YumuAeHeOJVz0%2B8ABkjoSYFz%2FoScql6QyaHTeMKegoeDvkVEXdPl6WWBIuPV4HP9Mzgg%3D%3D" rel="nofollow" target="_blank">AD 画板零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=xg4tc6tkrWiTP5TZ1dXrDw%3D%3D.F%2FN2BRxXeiArjwhuZldqV91sBxhqJrA2q4BMnqZD9mJ64KUN%2BUCgfV6Vy7j04hgFr%2BYOCmR97E%2FCmqvl0rTIsg%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=atWGtN96RuR3KBmVMQ2SEg%3D%3D.cT%2FaDgpWS67aISbHwTa1HUak5ECd7hhutZiK%2F9GyhG98c%2B8TvLpVNw880VSyQANgoPxZnKZvPIrHqXpYt8YTQw%3D%3D" rel="nofollow" target="_blank">C++ 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=cq1u8q7aEWqDpcVTXAASCw%3D%3D.txHxzI3WD5BIbgC3Kk6YITHEwbGCKFjKCGcguvwDz6z%2Bi0QKh4XaLtED%2BfoiBehWEJJtSeKFP%2FYbxg%2BnGpMv2p53MizLoChEgrj8kqZVAGc%3D" rel="nofollow" target="_blank">ESP32 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=9duRRz4hTIuPTGXsxqMjuw%3D%3D.e9y42akfZuMpTjzmtyRWSQrI5yf9E1pKjjVA87249KnnUdvd1inYOAO9L8cxOOGGGNE0OD%2B%2FXyJOIg0a3Pf5QvRN9T%2BIcs5C1HnN3cDsixc%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=O5JizOeoUXomIafPPTt6bA%3D%3D.Jj2mxWPTfrtaQNWHsE5K2J7rNRTx737i8yK96xre2SwdZSj6SIAurtmPvoM6aRHcUwShM4mC3hlQW6An%2Fv6HvA0Y8rz8bKHKmalUHkjcuA8%3D" rel="nofollow" target="_blank">Linux 应用开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=TJ4Y%2BDLAvjen4e7NQnVYjQ%3D%3D.vrgp9FUxmGZ6n1Y1HPNoLv6lwgQWXJdI4weifWxCTNWw80q02Yw5RfG6dYG68wF3IsO45BjKFJv%2FcsWme6AVeRD25iJqax9CveHzcR0z8Jw%3D" rel="nofollow" target="_blank">Linux 底层开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=BxkSRPZ6trge6EUf%2BDPgVg%3D%3D.4kMwMAYelnsi5plkXpYGSObZnLNGDV965HrvmgWLeMlj6nvMbwoAucHWoO8D%2F5nfKXSrFZ2lHvzEjqToGAayLQ%3D%3D" rel="nofollow" target="_blank">LVGL 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=GwzlhIfhjwgkSRAObWYl3g%3D%3D.WnSXXk5Ee3jc5M36MIlphq%2FNiZchzTYKPo4EbcSa3E3MFJ2I8SpjmWgScP7Oa%2FzPyNyH3Qp%2BQ6QtQttZ2hgo0A%3D%3D" rel="nofollow" target="_blank">QT 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=HU73pNxFlN6F44CeI21UrA%3D%3D.S1fFdmxi3K8ozbbQRBKUPqwLP42kj9roIuY18eyBu4N%2B8b65p%2BoxjbvYER4jJBRLuiVcPRoW%2FdVYXrevk%2BxV%2BRYqJwK4rawpaDxXDaWsPuw%3D" rel="nofollow" target="_blank">STM32 零基础入门学习路线</a></li></ul>]]></description></item><item>    <title><![CDATA[多模态与视觉大模型开发实战 - 2026必会课分享 学习园地主页 ]]></title>    <link>https://segmentfault.com/a/1190000047598322</link>    <guid>https://segmentfault.com/a/1190000047598322</guid>    <pubDate>2026-02-07 11:04:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>视觉智能的商业临界点已经到来<br/>2026年，多模态视觉大模型的发展正从技术探索阶段过渡到商业价值兑现期。当技术能够稳定识别图像中的商品并理解用户自然语言描述的偏好时，一个新的商业时代开启了。在东京银座的一家高端百货，一套基于多模态视觉大模型的导购系统正在改变零售体验：顾客用手机拍摄心仪的手提包，系统不仅识别品牌和型号，还能根据顾客过往的购物记录、当前穿着风格，甚至社交媒体上表达的生活态度，推荐相配的鞋履和配饰——这种体验的转化率比传统推荐系统高出三倍。</p><p>这种商业价值的爆发并非偶然，而是多项技术成熟度曲线交汇的必然结果。视觉识别精度突破95%实用门槛、跨模态语义对齐技术让图像与语言理解无缝衔接、边缘计算能力大幅提升使实时分析成为可能——这三个技术拐点在2025-2026年间相继到来，为商业化应用扫清了最后障碍。</p><hr/><p>行业级解决方案的差异化竞争策略<br/>2026年最成功的商业实践表明，通用型多模态视觉模型难以直接创造商业价值，而针对特定行业深度优化的模型却能快速形成竞争壁垒。</p><p>在医疗影像诊断领域，领先企业不再简单标定病灶位置，而是构建了“影像-病理-预后”的全链条理解模型。当系统读取CT扫描时，它不仅识别肿瘤特征，还能关联相似病例的治疗方案和康复轨迹，为医生提供决策支持而非仅仅诊断辅助。这种深度行业理解构建的数据护城河，使后来者难以在短期内追赶。</p><p>制造业的质量检测方案则展现了另一种商业逻辑。传统视觉检测只能识别预设的缺陷类型，而多模态系统通过分析产品图像、生产线传感器数据和维修记录文本，能发现人眼难以察觉的潜在缺陷模式，甚至预测设备故障对产品质量的影响。这种从“检测”到“预防”的价值跃迁，让客户愿意支付十倍于传统系统的价格。</p><hr/><p>成本结构的革命与商业模式创新<br/>多模态视觉大模型的商业普及，关键驱动力之一是成本结构的根本性改变。2025年之前，训练行业级模型需要数百万美元的算力投入，而2026年的模块化训练框架和模型高效微调技术，将这一门槛降低到原来的十分之一。</p><p>成本下降催生了全新的商业模式。在时尚行业，一家初创公司不再销售软件许可，而是提供“视觉智能订阅服务”：中小品牌按月支付费用，即可获得与大牌同等的视觉分析和设计辅助能力。在农业领域，服务商根据农田面积和检测频率收费，为农场主提供作物病虫害的早期预警——这种“效果付费”模式彻底改变了技术采购的逻辑。</p><p>更值得关注的是边缘端部署的经济性突破。2026年，经过优化的多模态模型已能在智能手机和工业边缘设备上流畅运行，这意味着商业应用不再受限于云端连接，可以在网络条件差的工厂车间、偏远农场或应急现场发挥作用。这种部署方式的转变，开辟了数十个此前无法触达的商业场景。</p><hr/><p>数据生态构建：从单向采集到价值循环<br/>传统视觉系统的数据流动是单向的：采集、标注、训练、部署。2026年领先企业的核心竞争优势，在于构建了能够自我增强的数据价值循环。</p><p>零售巨头亚马逊的多模态系统展示了这种生态的威力：当顾客在实体店试穿服装时，视觉系统分析试穿效果；顾客的购买决定与在线评价形成反馈；这些数据不仅优化推荐算法，还反向指导服装设计与库存管理。数据在消费端与生产端之间形成闭环，每一条数据都多次创造价值。</p><p>在自动驾驶领域，特斯拉建立的“影子模式”数据生态更为成熟：数百万辆车的视觉系统持续观察环境，即使在自动驾驶未激活时也在对比人类司机的决策与模型预测的差异。这种持续的对比学习使系统能力呈指数级增长，形成了竞争对手难以复制的数据资产。</p><hr/><p>商业落地的隐形挑战与应对策略<br/>技术成熟度不等于商业成功率。2026年，多模态视觉大模型的商业落地面临三个隐形挑战，而成功企业已找到应对之道。</p><p>首先是“期望值管理”问题。早期客户往往对AI能力抱有不切实际的期待，认为系统应像人类一样理解任何视觉场景。领先供应商通过“能力边界透明化”策略解决这一问题：明确告知系统在哪些场景下准确率超过98%，在哪些边缘情况下可能失效，并提供相应的保障方案。这种坦诚反而建立了更强的客户信任。</p><p>其次是“集成复杂度”挑战。多模态系统需要与企业现有IT架构、数据平台和业务流程深度融合。提供“渐进式集成”方案的供应商更受青睐：先从单一场景试点，验证价值后再逐步扩展，避免“大爆炸式”改造带来的风险。</p><p>最后是“持续进化”需求。商业环境不断变化，今天的模型明天就可能过时。建立“模型即服务”的持续更新机制成为标准配置，确保客户无需频繁投入重训成本即可获得能力升级。</p><hr/><p>2026年的商业格局与未来展望<br/>到2026年末，多模态视觉大模型的市场已形成清晰的层级格局：底层是少数几家提供基础大模型的科技巨头；中间层是专注行业解决方案的垂直领域领导者；上层则是大量利用API构建具体应用场景的创新企业。</p><p>这一格局中最具活力的正是中间层的行业专家。他们既理解技术的可能性，也深谙行业的痛点；既能为客户创造可见的ROI（投资回报率），又能建立长期的竞争壁垒。这些企业的估值逻辑已从传统的“软件毛利率”转变为“数据资产价值”和“行业生态地位”。</p><p>展望2027年，下一轮商业突破将来自多模态系统与物理世界的更深度融合——当视觉理解能力与机器人操作、环境交互、实时决策结合时，将催生真正的“智能体经济”。那些在2026年掌握了多模态视觉模型商业方法论的企业，将在下一轮竞争中占据先发优势。</p><p>商业与技术之间总是存在微妙的时差。2026年的机遇在于：技术刚刚跨越实用门槛，而商业认知还未完全普及——这中间的窗口期，正是先行者建立优势的最佳时机。多模态视觉大模型的发展历程再次证明：最具颠覆性的商业创新，往往发生在技术曲线从陡峭趋于平缓的转折点上，因为此时技术足够可靠，而应用想象刚刚展开。</p>]]></description></item><item>    <title><![CDATA[EmEditor文本编辑器安装步骤详解（附大文件打开与代码编辑教程） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047598324</link>    <guid>https://segmentfault.com/a/1190000047598324</guid>    <pubDate>2026-02-07 11:04:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p><code>EmEditor</code>是 <strong>EmEditor 文本编辑器的安装包</strong>，这是个主打<strong>大文件和代码编辑</strong>的工具，打开几百 MB 甚至 GB 的文本不卡，支持各种编程语言高亮、正则查找替换，写代码、改日志、处理数据都挺顺手。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=huHdqAqDAIWndx3BDH9Dsg%3D%3D.NkKAEuz1TX%2FXZvhMnXe7aR45tqgFaL2qnQVpzk5f7XU6ndOGnQNdYweM6Rlmb%2FL%2F" rel="nofollow" title="https://pan.quark.cn/s/0424198092b3" target="_blank">https://pan.quark.cn/s/0424198092b3</a></li></ul></li><li><p><strong>用管理员身份运行（推荐）</strong> ​</p><ul><li>右键 <code>EmEditor.exe</code>→ 选“以管理员身份运行”，防止权限不足导致安装出错。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>EmEditor.exe</code>运行（如果右键过了就直接双击）。</li><li>第一次打开会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>进入安装向导，选语言（默认 English，有的版本有中文）→ 点  <strong>“Next”</strong> 。</li><li>阅读许可协议 → 选 “I accept the terms…” → 点  <strong>“Next”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\Program Files\EmEditor</code>，可点 Browse 改到其他盘。</li></ul></li><li><p>附加任务：</p><ul><li>建议勾 “Create a desktop shortcut”（创建桌面快捷方式），方便以后打开。</li></ul></li><li>点  <strong>“Install”</strong> ​ 开始安装，等进度条走完（几十秒）。</li><li>安装完会问是否立即启动 → 可先取消，等会儿再开。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>在开始菜单或桌面找到 <strong>EmEditor</strong>​ → 点开。</li><li>第一次打开就是干净的主界面，类似记事本但功能更多。</li><li><strong>打开大文件</strong>：拖文件进来或直接点“打开”，几百 MB 也能秒开。</li><li><strong>代码高亮</strong>：打开 <code>.c</code>、<code>.py</code>、<code>.html</code>等文件，会自动识别并高亮语法。</li><li><strong>查找替换</strong>：支持正则表达式，找特殊内容很方便。</li><li><strong>多标签页</strong>：可以同时开多个文件，来回切换不用来回找窗口。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[技术日报｜OpenAI技能库逆袭登顶，Claude-Mem四连冠终结 Devlive开源社区 ]]></title>    <link>https://segmentfault.com/a/1190000047598333</link>    <guid>https://segmentfault.com/a/1190000047598333</guid>    <pubDate>2026-02-07 11:03:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>🌟 <strong>TrendForge 每日精选</strong> - 发现最具潜力的开源项目<br/>📊 今日共收录 <strong>7</strong> 个热门项目，涵盖 <strong>50</strong> 种编程语言</p><p>🌐 <strong>智能中文翻译版</strong> - 项目描述已自动翻译，便于理解</p></blockquote><h3>🏆 今日最热项目 Top 10</h3><h4>🥇 openai/skills</h4><p><strong>项目简介</strong>: Codex 技能目录</p><p><strong>今日新增</strong>: 583 | <strong>总星数</strong>: 4842 | <strong>语言</strong>: Python</p><p><a href="https://link.segmentfault.com/?enc=glQ54G6HSng%2ByJmOYnDIag%3D%3D.Z9DvP2%2BrfeZAOQ5X%2FH3cNERjlL75Z86ASRNdRIiL3u%2FSagQjH%2FaXWnPAIrRtlnb7" rel="nofollow" target="_blank">https://github.com/openai/skills</a></p><hr/><h4>🥈 bytedance/UI-TARS-desktop</h4><p><strong>项目简介</strong>: 开源多模态AI智能体堆栈，连接尖端AI模型与智能体基础设施</p><p><strong>今日新增</strong>: 573 | <strong>总星数</strong>: 27099 | <strong>语言</strong>: TypeScript</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598335" alt="bytedance/UI-TARS-desktop" title="bytedance/UI-TARS-desktop"/></p><p><a href="https://link.segmentfault.com/?enc=KIJwYEjdwM3IirCAHSkS3g%3D%3D.nIJk8EASgE4U9kVkIzpWqmnpwcb05u3%2FeSsbTc8DnkEpXyfoTqPyyBN6rOWxxBxP" rel="nofollow" target="_blank">https://github.com/bytedance/UI-TARS-desktop</a></p><hr/><h4>🥉 aquasecurity/trivy</h4><p><strong>项目简介</strong>: 在容器、Kubernetes、代码仓库、云环境等场景中检测漏洞、错误配置、密钥泄露和软件物料清单</p><p><strong>今日新增</strong>: 165 | <strong>总星数</strong>: 31535 | <strong>语言</strong>: Go</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598336" alt="aquasecurity/trivy" title="aquasecurity/trivy" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=eQSSGyO%2B1aegYP5re3p1sA%3D%3D.8IP0C40BQ2wpsmXZuCmAUF707pw%2BB6%2BpU3JWcoD662enFNlJz2e4jjCvmME7RuZJ" rel="nofollow" target="_blank">https://github.com/aquasecurity/trivy</a></p><hr/><h4><strong>4.</strong> nvm-sh/nvm</h4><p><strong>项目简介</strong>: Node 版本管理器 - 符合 POSIX 标准的 bash 脚本，用于管理多个活跃的 node.js 版本</p><p><strong>今日新增</strong>: 131 | <strong>总星数</strong>: 91497 | <strong>语言</strong>: Shell</p><p><a href="https://link.segmentfault.com/?enc=XMb4LXjKKFB6w%2BPCREeLGQ%3D%3D.bM9zDIMHNmfuHRfLwIvl%2FkAC3Sc5LXLTEfQ40On2zVU%3D" rel="nofollow" target="_blank">https://github.com/nvm-sh/nvm</a></p><hr/><h4><strong>5.</strong> DataExpert-io/data-engineer-handbook</h4><p><strong>项目简介</strong>: 数据工程全方位学习资源汇总仓库</p><p><strong>今日新增</strong>: 71 | <strong>总星数</strong>: 39856 | <strong>语言</strong>: Jupyter Notebook</p><p><a href="https://link.segmentfault.com/?enc=qmYP2X%2FJTTVhTRZeX3hIDw%3D%3D.0HmIoKyyddA7AxFxRvrwpaMtK1pthA6lIM7WCNa13AW56Pq8sb03q%2FLTcbwhPPSkMldwpMDvjdpNbAYjQduKZg%3D%3D" rel="nofollow" target="_blank">https://github.com/DataExpert-io/data-engineer-handbook</a></p><hr/><h4><strong>6.</strong> Flowseal/zapret-discord-youtube</h4><p><strong>项目简介</strong>: </p><p><strong>今日新增</strong>: 70 | <strong>总星数</strong>: 21967 | <strong>语言</strong>: Batchfile</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598337" alt="Flowseal/zapret-discord-youtube" title="Flowseal/zapret-discord-youtube" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=a%2B8fa6T%2BYAyexhzIX53A9w%3D%3D.eSzshFesRHgMcIVbO12HdWaf0lHuK8Qt3UwueFbHwxn9NVm7eWWgrRSb5T4baswpwS4z9wCu3QGx3rxjreCBIw%3D%3D" rel="nofollow" target="_blank">https://github.com/Flowseal/zapret-discord-youtube</a></p><hr/><h4><strong>7.</strong> likec4/likec4</h4><p><strong>项目简介</strong>: 通过代码生成的实时动态图表，实现软件架构的可视化、协作与持续演进。</p><p><strong>今日新增</strong>: 40 | <strong>总星数</strong>: 1802 | <strong>语言</strong>: TypeScript</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047595319" alt="likec4/likec4" title="likec4/likec4" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=6zHWNK9WMTJMdyCE9znyCw%3D%3D.5RjhkqGrFMsnvbIvj4JohiZq%2Bm32p2zNbvAFoFPxKWEysTfdYcTmeDtBorZqur%2Bo" rel="nofollow" target="_blank">https://github.com/likec4/likec4</a></p><hr/><h3>🌈 分语言热门项目</h3><h4>● C 最热项目</h4><p><strong>项目名称</strong>: tmux/tmux</p><p><strong>项目描述</strong>: tmux源代码</p><p><strong>今日新增:</strong> 62 | <strong>总数:</strong> 41435</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=KI2gts%2FVZ6RFDAgsf5XZ3A%3D%3D.s7%2B%2BM0NusaQDHgcfUK364a0Q5l96HnZRpaRSW%2FSQBr8%3D" rel="nofollow" target="_blank">https://github.com/tmux/tmux</a></p><hr/><p><strong>项目名称</strong>: timescale/timescaledb</p><p><strong>项目描述</strong>: 作为Postgres扩展打包的高性能实时分析时序数据库</p><p><strong>今日新增:</strong> 40 | <strong>总数:</strong> 21703</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=eLyXGFZdEfh8b1Kl20HEhA%3D%3D.3cr6WIA4xdbUaAO50uxTpbbNeantTC%2FT0%2Blj4U691FvEHuZLY5ikU4zIGbglc%2Fm5" rel="nofollow" target="_blank">https://github.com/timescale/timescaledb</a></p><hr/><p><strong>项目名称</strong>: bol-van/zapret2</p><p><strong>项目描述</strong>: 反深度包检测软件</p><p><strong>今日新增:</strong> 17 | <strong>总数:</strong> 1464</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=w2I4Unw8YLF1sQlGUDI7tw%3D%3D.HuNEENYkg%2B%2F3BaJ8SIBYvdTNns9sTfOeCLhl7yG92ZCahxLBxFu3H0o6W8m86aEJ" rel="nofollow" target="_blank">https://github.com/bol-van/zapret2</a></p><hr/><h4>● C# 最热项目</h4><p><strong>项目名称</strong>: marticliment/UniGetUI</p><p><strong>项目描述</strong>: UniGetUI：您的包管理器图形界面。或可粗略描述为用于管理包管理器的"包管理器管理器"。</p><p><strong>今日新增:</strong> 140 | <strong>总数:</strong> 20667</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598338" alt="marticliment/UniGetUI" title="marticliment/UniGetUI" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=%2B8YNmzoB6NHBUeJma9UgYw%3D%3D.qGQgn9vPaeQxDASfDpDK%2F9zGmywS%2FkBjAFzCwkBUZHU4dTmedN6R1GIIJG4segmO" rel="nofollow" target="_blank">https://github.com/marticliment/UniGetUI</a></p><hr/><p><strong>项目名称</strong>: wshobson/agents</p><p><strong>项目描述</strong>: 面向Claude Code的智能自动化与多智能体编排系统</p><p><strong>今日新增:</strong> 101 | <strong>总数:</strong> 27973</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598339" alt="wshobson/agents" title="wshobson/agents" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=L3nd3WkulQmHW2p8Mg4w0A%3D%3D.yn4H1pBn0BoIjEpmWVyJxsuFFMSTy2G9jW9mgp3lV5%2BF1l2KaSWHpEsSzIsLPf4M" rel="nofollow" target="_blank">https://github.com/wshobson/agents</a></p><hr/><p><strong>项目名称</strong>: Cleanuparr/Cleanuparr</p><p><strong>项目描述</strong>: Cleanuparr是一款自动化清理工具，用于清理Sonarr、Radarr及支持的下载客户端（如q...</p><p><strong>今日新增:</strong> 55 | <strong>总数:</strong> 1902</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=kwxciBdh5vd1IAY2y2tUnA%3D%3D.SOMyyqv7eXc7gyFtitrJPh7tWzHaq1abjc7Oq1BT4i4KZSdKopE8WTwwXBaxVfL%2B" rel="nofollow" target="_blank">https://github.com/Cleanuparr/Cleanuparr</a></p><hr/><h4>● C++ 最热项目</h4><p><strong>项目名称</strong>: ggml-org/llama.cpp</p><p><strong>项目描述</strong>: 使用 C/C++ 实现的大语言模型推理框架</p><p><strong>今日新增:</strong> 85 | <strong>总数:</strong> 94535</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598340" alt="ggml-org/llama.cpp" title="ggml-org/llama.cpp" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=DCIHQ2cz1qWf95WujELojQ%3D%3D.Wvw9w6q4J%2F05U%2Bzy0azRA9UnYgwFWIP4fJU%2FxC7E4cJ39PJpuvQoDFFDK%2F3nGHtD" rel="nofollow" target="_blank">https://github.com/ggml-org/llama.cpp</a></p><hr/><p><strong>项目名称</strong>: godotengine/godot</p><p><strong>项目描述</strong>: Godot引擎——跨平台2D与3D游戏引擎</p><p><strong>今日新增:</strong> 61 | <strong>总数:</strong> 106402</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598341" alt="godotengine/godot" title="godotengine/godot" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=iX%2BU9pXH2ZQljcVPHnXM%2Fg%3D%3D.UMnhQWU8ZBVXeZdu5uUNAfRPAhcZmsqwXQzn6WqMhpjcTSwvkPHVd7eBdoLoInkv" rel="nofollow" target="_blank">https://github.com/godotengine/godot</a></p><hr/><p><strong>项目名称</strong>: LadybirdBrowser/ladybird</p><p><strong>项目描述</strong>: 真正独立的网页浏览器</p><p><strong>今日新增:</strong> 33 | <strong>总数:</strong> 58405</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=tmsLbDYm%2FatCietyRvnT6A%3D%3D.QQ%2BFzylYqQ3j8MthgQ1rJwESGRdHYD8QNdqCQ%2FUDEG94usXcyWxGRQ7pZ1BfmKmQ" rel="nofollow" target="_blank">https://github.com/LadybirdBrowser/ladybird</a></p><hr/><h4>● Lua 最热项目</h4><p><strong>项目名称</strong>: yetone/avante.nvim</p><p><strong>项目描述</strong>: 像使用Cursor AI IDE般高效运用您的Neovim</p><p><strong>今日新增:</strong> 13 | <strong>总数:</strong> 17325</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598342" alt="yetone/avante.nvim" title="yetone/avante.nvim" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=KTgoyHM9a3v4ymR%2BsLntaw%3D%3D.zFEnoLk2vONyPeS9PsNs1w%2F%2BelnFGUJkoMbLobP%2F6nPTaQGkFqP1WvfaVzLHY2lI" rel="nofollow" target="_blank">https://github.com/yetone/avante.nvim</a></p><hr/><p><strong>项目名称</strong>: Kong/kong</p><p><strong>项目描述</strong>: 🦍 云原生API网关与AI网关。</p><p><strong>今日新增:</strong> 12 | <strong>总数:</strong> 42695</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598343" alt="Kong/kong" title="Kong/kong" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=rAEEMS3GeBOq32SfRMifFA%3D%3D.smGfds9MzvvIsvQidO1Jn5nHMv0jGZoTQWLLuaq8pSA%3D" rel="nofollow" target="_blank">https://github.com/Kong/kong</a></p><hr/><p><strong>项目名称</strong>: coder/claudecode.nvim</p><p><strong>项目描述</strong>: 🧩 Claude Code Neovim IDE 扩展</p><p><strong>今日新增:</strong> 10 | <strong>总数:</strong> 1967</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=mp5q%2Beh5F6zMqrPO6SXKFg%3D%3D.WRHIimKlELsmfFxyAiR%2FSrJCUEaDZ4RNuxbpZiTTiOGK6mOZ%2B18RYxwWnXmXA0hw" rel="nofollow" target="_blank">https://github.com/coder/claudecode.nvim</a></p><hr/><h4>● Vue 最热项目</h4><p><strong>项目名称</strong>: dreamhunter2333/cloudflare_temp_email</p><p><strong>项目描述</strong>: CloudFlare 免费临时域名邮箱 支持附件收发 IMAP SMTP TelegramBot</p><p><strong>今日新增:</strong> 23 | <strong>总数:</strong> 5972</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047595322" alt="dreamhunter2333/cloudflare_temp_email" title="dreamhunter2333/cloudflare_temp_email" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=hJo1QSYhRmLnIZVb2rjrwA%3D%3D.lZ24lTUxrJhqSr%2FzlYuaKT26tbx8Fb%2Fgv9e4iNp7OYBGGnr2SvHo6b0OKPo9nbJ2Een2BqjxqbLo7942%2FS5%2FiQ%3D%3D" rel="nofollow" target="_blank">https://github.com/dreamhunter2333/cloudflare_temp_email</a></p><hr/><p><strong>项目名称</strong>: zyronon/TypeWords</p><p><strong>项目描述</strong>: 练习英语 一次敲击 一点进步</p><p><strong>今日新增:</strong> 17 | <strong>总数:</strong> 7326</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598344" alt="zyronon/TypeWords" title="zyronon/TypeWords" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=MGs4eIM6j1jETCOis%2FGbpQ%3D%3D.U3AZ%2BEB%2Bj3%2Fp9kfG%2FJAHlZoN8SJ6Z5X6tkCwaYlm%2Fj1CjtcbuQEuoEJFaErbBAch" rel="nofollow" target="_blank">https://github.com/zyronon/TypeWords</a></p><hr/><p><strong>项目名称</strong>: vbenjs/vue-vben-admin</p><p><strong>项目描述</strong>: 一个基于Vue3、Shadcn UI、Vite、TypeScript和Monorepo构建的现代化V...</p><p><strong>今日新增:</strong> 14 | <strong>总数:</strong> 31485</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598345" alt="vbenjs/vue-vben-admin" title="vbenjs/vue-vben-admin" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=RffPVEw7BtRwzulcZ82B2Q%3D%3D.krn3JF6bd%2F7ZiR0pshoj1vH3OGNZ4tqEmLfRSUUEISViIdKfh2GkGjASqQw3LIB%2F" rel="nofollow" target="_blank">https://github.com/vbenjs/vue-vben-admin</a></p><hr/><h4>● Kotlin 最热项目</h4><p><strong>项目名称</strong>: RunanywhereAI/runanywhere-sdks</p><p><strong>项目描述</strong>: 可在本地运行AI的生产就绪工具包</p><p><strong>今日新增:</strong> 165 | <strong>总数:</strong> 6291</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598346" alt="RunanywhereAI/runanywhere-sdks" title="RunanywhereAI/runanywhere-sdks" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=wHkCNRoXttpGEorGDGWndQ%3D%3D.hirv6%2FOBlcR5Y7hZFTjnFBIfRvr6oa9AoF3jsJedbd%2B5xgYea0l2E4vvqYMz%2F57EdMxEa3mcM%2BNu9%2B%2FVrtAKLQ%3D%3D" rel="nofollow" target="_blank">https://github.com/RunanywhereAI/runanywhere-sdks</a></p><hr/><p><strong>项目名称</strong>: tiann/KernelSU</p><p><strong>项目描述</strong>: 基于内核的Android系统root解决方案 （注：根据技术文档惯例，"Kernel based"译...</p><p><strong>今日新增:</strong> 15 | <strong>总数:</strong> 14915</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=fcQ%2BjNQgHkxtN%2Bid3bnEBA%3D%3D.K%2Fgds%2BxsB1uYRdyhlYgifTsEeua2k2cexSi%2Fo%2B%2BNwruZUzeWOWl%2BUrA3CL2hQ0yd" rel="nofollow" target="_blank">https://github.com/tiann/KernelSU</a></p><hr/><p><strong>项目名称</strong>: JackEblan/Geto</p><p><strong>项目描述</strong>: 为应用配置设备级设置。该项目采用多模块化设计，遵循Bob大叔的整洁架构原则，参考Now in And...</p><p><strong>今日新增:</strong> 9 | <strong>总数:</strong> 761</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598347" alt="JackEblan/Geto" title="JackEblan/Geto" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=OHHULLn90Y9rn1qUk71BZA%3D%3D.oDWZfLVFM%2B4G5ejg1%2FkaMipdlIomiK0uhPS97EokyVaXVwQ%2FQ2whUdolfVR%2FkOC0" rel="nofollow" target="_blank">https://github.com/JackEblan/Geto</a></p><hr/><h3>📈 今日趋势分析</h3><p><strong>最活跃语言</strong>: TypeScript(2个)、Python(1个)、Go(1个)</p><p><strong>今日总获星</strong>: 1,633 颗星</p><p><strong>平均获星</strong>: 233 颗星/项目</p><p><strong>今日之星</strong>: openai/skills (583)</p><hr/><h3>📊 数据总览</h3><table><thead><tr><th>指标</th><th>数值</th></tr></thead><tbody><tr><td>收录项目</td><td><strong>7</strong> 个</td></tr><tr><td>编程语言</td><td><strong>50</strong> 种</td></tr><tr><td>今日新增</td><td><strong>1,633</strong> 颗星</td></tr><tr><td>报告日期</td><td><strong>2026年02月06日</strong></td></tr><tr><td>统计周期</td><td><strong>日报</strong></td></tr></tbody></table><hr/><p>TrendForge 致力于追踪全球开源项目动态，每日为开发者精选最具价值的 GitHub 项目。</p><p><strong>数据来源</strong>: <a href="https://link.segmentfault.com/?enc=T9oIC0qOld7G0e9oLGaYCA%3D%3D.uuaaX6htd0%2F8XeXUGAkrxLUP15Qv1t4VqPc51yNs2zU%3D" rel="nofollow" target="_blank">https://trendforge.devlive.top/</a></p><p><strong>数据说明</strong>: 基于 GitHub 官方 API 数据统计，每日更新</p><p><strong>翻译声明</strong>: 项目描述采用 AI 智能翻译，如有疏漏请以原文为准</p><p><em>报告生成于: 2026年02月07日</em></p><h2>GitHub #开源项目 #技术趋势 #程序员 #软件开发</h2>]]></description></item><item>    <title><![CDATA[IPERFforWindowsTrialSigned网络带宽测试工具安装步骤详解（附网络带宽测试教程]]></title>    <link>https://segmentfault.com/a/1190000047598367</link>    <guid>https://segmentfault.com/a/1190000047598367</guid>    <pubDate>2026-02-07 11:02:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p><code>IPERFforWindowsTrialSigned</code>是 <strong>iperf 网络带宽测试工具的 Windows 安装包</strong>，iperf 能在两台电脑或设备之间测网络吞吐量（就是看网速到底能跑多快），运维、网络调试、测 Wi-Fi 或局域网性能时常用。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=%2Bv9JAoAoBMY2tFScI6wQ8Q%3D%3D.P2zSHCbONFye2oau4%2BYoQy6psLAj5STYaCxyfPcRpWb7EJD8mzyOEArNvTNSksUz" rel="nofollow" title="https://pan.quark.cn/s/6d027407c943" target="_blank">https://pan.quark.cn/s/6d027407c943</a></p></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>IPERFforWindowsTrialSigned.exe</code>运行。</li><li>如果是 Win10/Win11，会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> （需要管理员权限）。</li><li>进入安装向导，选语言（默认 English，有的版本有中文）→ 点  <strong>“Next”</strong> 。</li><li>阅读许可协议 → 选 “I accept…” → 点  <strong>“Next”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\Program Files\iperf</code>或类似路径，可点 Browse 改到 D 盘。</li></ul></li><li><p>附加任务：</p><ul><li>建议勾 “Create a desktop shortcut”（创建桌面快捷方式），方便以后打开。</li></ul></li><li>点  <strong>“Install”</strong> ​ 开始安装，等进度条走完（很快，几秒到十几秒）。</li><li>安装完会问是否立即启动 → 可先取消，iperf 一般用命令行跑，不会自动弹 GUI。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>装完后，iperf 其实是个命令行工具，在开始菜单或安装目录能找到 <strong>iperf3.exe</strong>（或 iperf.exe）。</li><li>按 <code>Win+R</code>输入 <code>cmd</code>回车，打开命令提示符。</li><li><p>切到安装目录，比如：</p><pre><code>cd "C:\Program Files\iperf\bin"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p></li><li><p><strong>测网速基本流程</strong>：</p><ul><li><p>一台电脑当服务端：</p><pre><code>iperf3 -s</code></pre></li></ul></li></ol><pre><code>-   另一台电脑当客户端（连服务端 IP）：

    ```
    iperf3 -c 服务端IP
    ```



-   跑完会显示带宽、丢包、抖动等信息。
</code></pre><ol><li><p>常用参数：</p><ul><li><code>-t</code>设置测试时长（秒），比如 <code>-t 30</code>测 30 秒。</li><li><code>-P</code>设置并发连接数，比如 <code>-P 4</code>用 4 条流同时测。</li></ul></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[LoRaWAN的网络拓扑：深入解析与门思科技的创新实践 赵明飞 ]]></title>    <link>https://segmentfault.com/a/1190000047598372</link>    <guid>https://segmentfault.com/a/1190000047598372</guid>    <pubDate>2026-02-07 11:01:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>引言<br/>在物联网（IoT）的浪潮中，低功耗广域网（LPWAN）技术扮演着至关重要的角色。其中，LoRaWAN作为一种开放标准，以其远距离、低功耗的特性，在智能城市、智慧农业、工业物联网等领域展现出巨大的应用潜力。理解LoRaWAN的网络拓扑结构，是深入掌握其工作原理和应用部署的关键。本文将详细解析LoRaWAN的网络拓扑，并介绍门思科技（Manthink）如何通过其创新的产品和解决方案，助力LoRaWAN生态系统的发展。<br/>LoRaWAN网络拓扑概述<br/>LoRaWAN网络采用独特的“星形拓扑（Star-of-Stars Topology）”结构，这与传统的蜂窝网络或Wi-Fi网络有着显著的区别。在这种拓扑中，终端设备（End Devices）不直接与网络服务器通信，而是通过一个或多个网关（Gateway）进行数据中继。这种设计极大地简化了终端设备的复杂性，降低了功耗，延长了电池寿命。<br/>LoRaWAN网络主要由以下四个核心组成部分构成：</p><ol><li>终端设备（End Devices）：也称为节点，是网络的最前端，负责采集数据（如温度、湿度、位置等）或执行控制指令。它们通常是电池供电，通过LoRa无线技术与网关通信。</li><li>网关（Gateways）：也称为基站或集中器，是连接终端设备和网络服务器的桥梁。网关接收来自终端设备的LoRa信号，并将其转换为IP数据包，通过标准IP连接（如以太网、Wi-Fi或蜂窝网络）转发到网络服务器。同时，网关也能将网络服务器的下行数据转发给终端设备。</li><li>网络服务器（Network Server, NS）：是LoRaWAN网络的核心大脑，负责管理整个网络的运行。它的主要功能包括：数据去重、上行数据路由到正确的应用服务器、下行数据调度、自适应数据速率（ADR）管理、设备激活（OTAA/ABP）等。网络服务器确保了数据在终端设备和应用服务器之间的可靠传输。</li><li>应用服务器（Application Server）：负责处理和存储来自终端设备的业务数据，并向下行发送控制指令。它是最终用户或应用程序与LoRaWAN网络交互的接口，通常会提供数据可视化、分析和应用集成等功能。<br/>这种星形拓扑结构使得LoRaWAN网络具有高扩展性、低功耗和广覆盖的优势。终端设备无需维护复杂的连接，只需将数据发送到任何可接收的网关，由网络服务器进行统一管理和路由。</li></ol><p>LoRaWAN网络拓扑的详细解析<br/>终端设备（End Devices）<br/>终端设备是LoRaWAN网络的感知层，它们可以是各种传感器、计量表或执行器。这些设备通常部署在偏远地区或难以供电的环境中，因此低功耗是其设计的核心考量。LoRaWAN协议通过优化通信机制，如Class A、Class B和Class C操作模式，以平衡功耗和通信延迟。<br/>门思科技（Manthink） 在终端设备领域提供了多样化的解决方案，以满足不同行业的需求。例如，支持EB的模组OMx22S，能够兼容CJ/T 188、DL/T 645、Modbus等多种协议，用户只需进行简单的硬件改动，即可将现有设备快速升级为LoRaWAN设备，大大降低了开发难度和成本。此外，DTU（数据传输单元） 产品，包括防水的DTU RDO21x 和 导轨式DTU RDI22x，能够支持CJ/T 188、DL/T645等物联网设备的接入，为传统设备的LoRaWAN化提供了便捷途径。SE72温湿度表 更是凭借其IP65防护等级和长达8年的电池寿命，成为恶劣环境下数据采集的理想选择。<br/>网关（Gateways）<br/>网关是LoRaWAN网络中的关键基础设施，负责接收来自终端设备的LoRa信号并将其转发至网络服务器。一个网关可以覆盖数公里甚至数十公里的范围，并同时处理数千个终端设备的数据。网关通常部署在建筑物顶部或高塔上，以获得最佳的覆盖范围。<br/>门思科技（Manthink） 的网关产品线提供了企业级的解决方案。室外网关GDO51系列 和 室内网关GDI51系列 均基于Ubuntu操作系统，能够适应复杂的企业内网环境。它们支持多种主流协议，如ChirpStack、Basic Station、TTN、ThinkLink、GWMP等，这意味着门思科技的网关可以无缝接入任何支持这些协议的LoRaWAN系统，为用户提供了极大的灵活性和兼容性。<br/>网络服务器（Network Server, NS）<br/>网络服务器是LoRaWAN网络的“大脑”，它管理着所有终端设备的连接、数据路由和安全。网络服务器负责处理上行数据（从设备到应用）和下行数据（从应用到设备），并确保数据的完整性和安全性。自适应数据速率（ADR）功能也是由网络服务器控制，它根据终端设备与网关之间的链路质量动态调整数据速率，以优化网络容量和终端设备电池寿命。<br/>门思科技（Manthink） 在网络服务器领域拥有强大的自研产品——ThinkLink。ThinkLink云版本 支持全球LoRaWAN标准，用户可以免费注册并免费接入多达1000个LoRaWAN设备，这对于小型项目或个人开发者来说是一个巨大的优势。它支持任何品牌的支持GWMP和ThinkLink协议的网关接入，极大地扩展了其兼容性。此外，ThinkLink-Edge版本 是一款高性能的边缘计算网络服务器，配备8核处理器、8GB DDR内存和64GB eMMC存储，并内嵌了Home Assistant和ThingsBoard。它支持与Home Assistant、ThingsBoard、BACnet的无缝对接，为本地数据处理和智能自动化提供了强大的支持，特别适用于对数据实时性、安全性要求较高的工业和商业应用场景。<br/>应用服务器（Application Server）<br/>应用服务器是LoRaWAN网络的最终目的地，它接收来自网络服务器的数据，并将其转换为用户可理解和利用的信息。这些信息可以用于数据分析、可视化、告警通知或与其他业务系统集成。应用服务器通常由最终用户或第三方服务提供商开发和维护。<br/>门思科技的产品理念是为用户提供一个简单、高效的LoRaWAN解决方案。通过自研的低功耗操作系统（MPOS）和边缘计算虚拟器（Edge-bus），门思科技的产品家族能够支持全球频段的LoRaWAN标准，并具备十三大功能点以适应复杂的应用场景。从2014年开始，门思科技的产品已经在南美、欧洲、日本等全球多个国家和地区有着长期广泛的应用，积累了超过10年的现场稳定运行经验，充分证明了其产品的可靠性和稳定性。<br/>LoRaWAN网络拓扑图示例<br/>为了更直观地理解LoRaWAN的网络拓扑，以下是一个典型的LoRaWAN网络架构图：<br/>[此处插入LoRaWAN网络拓扑图]</p><p>门思科技（Manthink）在LoRaWAN生态中的角色<br/>门思科技作为LoRaWAN领域的先行者和创新者，致力于提供从模组、终端设备、网关到网络服务器的全栈式解决方案。我们的产品家族基于自研的低功耗操作系统（MPOS）和边缘计算虚拟器（Edge-bus），支持全球频段的LoRaWAN标准，并具有十三大功能点以适应复杂的应用场景。这些产品已经在包括南美、欧洲、日本等全球多个国家和地区有着长期广泛的应用，最早的规模化应用从2014年开始到现在已经超过10年的现场稳定运行，充分证明了门思科技产品的可靠性和稳定性。<br/>我们的优势：<br/>● 全栈式解决方案：提供从硬件到软件，从设备到云端的完整LoRaWAN解决方案。<br/>● 技术领先：自研MPOS和Edge-bus，确保产品性能和稳定性。<br/>● 全球兼容：支持全球频段的LoRaWAN标准，适应不同国家和地区的需求。<br/>● 丰富功能：十三大功能点，满足复杂多样的应用场景。<br/>● 长期稳定运行：超过10年的现场稳定运行经验，品质值得信赖。<br/>总结<br/>LoRaWAN以其独特的星形拓扑结构，为物联网应用提供了低功耗、远距离的连接能力。理解其网络组成部分——终端设备、网关、网络服务器和应用服务器——对于成功部署和管理LoRaWAN网络至关重要。门思科技（Manthink）凭借其在LoRaWAN领域的深厚积累和创新产品，为全球用户提供了可靠、高效、易于部署的LoRaWAN解决方案，助力各行各业实现数字化转型。<br/>了解更多门思科技（Manthink）<br/>● 门思科技官方网站：<a href="https://link.segmentfault.com/?enc=bd3LtxXDf14dXAlc6o5vXQ%3D%3D.LxQFooHTVdN5ae21l7c336IZsfL%2FErW5q6nNvCPk7hs%3D" rel="nofollow" target="_blank">https://www.manthink.cn</a><br/>● 门思科技LoRaWAN NS 产品：<a href="https://link.segmentfault.com/?enc=puE9aMay9JGeeDWrMPGMfA%3D%3D.77CR3BPNsvm0HkW1kB03EDVvGIDyXoiPinkr5dHbVuM%3D" rel="nofollow" target="_blank">https://thinklink.manthink.cn</a> (小项目可以免费使用ThinkLink)<br/>● 联系邮箱：<a href="mailto:info@manthink.cn" target="_blank">info@manthink.cn</a><br/>关键词： LoRa, LoRaWAN, 网关, Gateway, NS, Manthink, 门思科技, 物联网, LPWAN, 网络拓扑</p>]]></description></item><item>    <title><![CDATA[EazyDraw for Mac v11.4.1上专门画矢量图安装教程 简单步骤 Mac版 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047598390</link>    <guid>https://segmentfault.com/a/1190000047598390</guid>    <pubDate>2026-02-07 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p>EazyDraw 是 Mac 上<strong>专门画矢量图的工具</strong>，简单说就是用来做平面图、图标、插画、技术绘图这些，画出来的图放大不会糊，适合需要干净线条和精确尺寸的场景。</p><h4>1. 先下载好安装包</h4><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=peRcnrPPTysL2bznWVro2g%3D%3D.REaa%2BwrrckdAvvkGb6CoZi%2BK5TsOjKXOp3ymjZercTl7Rx4ogK6OLT3mu6LJPgg%2B" rel="nofollow" title="https://pan.quark.cn/s/f42ce6432768" target="_blank">https://pan.quark.cn/s/f42ce6432768</a> ，把 <code>EazyDraw for Mac v11.4.1.dmg</code>文件下载到你的 Mac（比如放桌面或下载文件夹，别塞太深的子文件夹，一会儿好找）。</p><h4>2. 打开 dmg 镜像文件</h4><p>找到下载好的 <code>.dmg</code>文件，<strong>双击它</strong>——屏幕会弹出一个新窗口，里面一般有俩东西：一个是“EazyDraw”的图标（一般是浅色方块，上面有绘图笔或几何图形样式），另一个是“应用程序”文件夹的快捷方式（小文件夹图标）。</p><h4>3. 把软件拖进“应用程序”文件夹</h4><p>按住“EazyDraw”图标，<strong>直接拖到旁边的“应用程序”文件夹里</strong>（跟平时拷贝文件一样），等进度条走完，这一步就装好了。</p><h4>4. 首次打开要“解锁”（重点！）</h4><p>去“应用程序”文件夹找到 EazyDraw，<strong>双击打开</strong>。第一次运行时，macOS 会弹提示“无法验证开发者”，别慌：</p><ul><li>点左上角苹果图标 → 选“系统设置”（旧版叫“系统偏好设置”）→ 左侧点“隐私与安全性”；</li><li>右边往下翻，找到“安全性”区域，会看到“已阻止使用‘EazyDraw’，因为来自身份不明的开发者”，下面有个“仍要打开”按钮，<strong>点一下</strong>，再输开机密码确认就行（如果没看到“仍要打开”，先关掉提示窗口，重新打开软件，提示会再出现）。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[Linux安装Temporal工作流引擎 YYGP ]]></title>    <link>https://segmentfault.com/a/1190000047598302</link>    <guid>https://segmentfault.com/a/1190000047598302</guid>    <pubDate>2026-02-07 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Docker一键安装Temporal， 使用外部MySQL数据库</p><h2>1. 初始化MySQL数据库</h2><pre><code class="sql"># 创建用户 temporal
CREATE USER 'temporal'@'%' IDENTIFIED BY 'temporal';

# 创建数据库 temporal
CREATE DATABASE `temporal` DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;
grant all privileges on `temporal`.* TO 'temporal'@'%';

# 创建数据库 temporal_visibility
CREATE DATABASE `temporal_visibility` DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;
grant all privileges on `temporal_visibility`.* TO 'temporal'@'%';</code></pre><h2>2. 配置文件</h2><p>docker-compose.yml</p><pre><code>services:
  temporal-init:
    image: temporalio/auto-setup:1.29.3
    container_name: temporal_init
    environment:
      DB: mysql8
      # 改为正确MySQL配置
      MYSQL_SEEDS: "192.168.2.215"
      DB_PORT: 3306
      MYSQL_USER: "temporal"
      MYSQL_PWD: "temporal"
      MYSQL_DB: "temporal"
      DEFAULT_NAMESPACE: "default"
    command: ["temporal-sql-tool", "create-schema", "-k", "default", "-v", "1.19"]
    restart: "no"
    
  temporal:
    image: temporalio/auto-setup:1.29.3
    container_name: temporal_server
    environment:
      # 使用外部 MySQL
      DB: mysql8
      MYSQL_SEEDS: "192.168.2.215"    # 你的 MySQL 地址
      DB_PORT: 3306
      MYSQL_USER: "temporal"
      MYSQL_PWD: "temporal"
      MYSQL_DB: "temporal"                 # Temporal 数据库
      DEFAULT_NAMESPACE: "default"
      # 这里填127.0.0.1会报错， 需要在ports将7233端口映射到宿主机， 填写宿主机的IP
      TEMPORAL_BROADCAST_ADDRESS: "192.168.2.215"
    ports:
      - "7233:7233"   # gRPC frontend
      - "7234:7234"   # history
      - "7235:7235"   # matching
      - "7239:7239"   # worker
      - "8088:8088"   # Temporal Web API (可选)
    depends_on:
      - temporal-init
    restart: always

  temporal-ui:
    image: temporalio/ui:2.45.0
    container_name: temporal_ui
    environment:
      TEMPORAL_ADDRESS: "temporal:7233"  # 指向 Temporal Server 容器名
      TEMPORAL_UI_PORT: "8080"
    ports:
      - "8080:8080"   # 浏览器访问
    depends_on:
      - temporal
    restart: always</code></pre><p>temporal-init: 用于自动初始化数据<br/>temporal: 启动核心进程<br/>temporal-ui: 启动UI管理界面</p><h2>3. 启动</h2><pre><code class="shell"># 启动
docker compose up -d
# 查看日志
docker logs --tail=100 -f temporal_server</code></pre><p>访问 <a href="https://link.segmentfault.com/?enc=8jw7cKu%2BqfySG4Mj%2FNSJ9g%3D%3D.yF8PQ2d1uf4Ky6nxKKmWhSk9iIOcFGzVvodrkkzDKZU%3D" rel="nofollow" target="_blank">http://192.168.2.215:8080/</a> 查看管理后台<br/><img width="723" height="182" referrerpolicy="no-referrer" src="/img/bVdnSEH" alt="image.png" title="image.png"/></p><p>销毁命令:</p><pre><code>docker compose down
docker container list</code></pre>]]></description></item><item>    <title><![CDATA[职场未来：AI时代的价值坐标系 本文系翻译，阅读原文
https://newsletter.jant]]></title>    <link>https://segmentfault.com/a/1190000047598292</link>    <guid>https://segmentfault.com/a/1190000047598292</guid>    <pubDate>2026-02-07 09:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>导言</h3><p>周一早晨，你打开笔记本电脑，心中萦绕着一个问题：两年后，我的工作是否还有意义？</p><p>上周，你花了三小时撰写一份活动方案。而一位同事使用AI智能体，仅用四分钟就生成了质量接近80%的版本——如果诚实地说，可能接近90%。</p><p>不是担心是否会失业，而是担心你所做的工作是否还能体现价值。你依然保住了工作，但你能感觉到它在不断缩水。问题不在于“机器人来了”，而在于你不再清楚自己该擅长什么。花了五年积累的Excel技能？自动化了。分析竞争对手并整合信息的能力？已有AI代劳。清晰撰写项目进展的技巧？不复存在。</p><p>你的职业身份正在以你无法追赶的速度消失，却无人告诉你下一步该何去何从。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSEx" alt="" title=""/></p><hr/><h3>案例分享</h3><p>Sarah是资深研究分析师，十年经验，时薪250美元。公司引入AI研究助手后：</p><ul><li>初期：AI用90分钟完成她需2-3周的基础研究，她转而负责“高阶分析”。</li><li>六个月后：公司质疑她工作的附加价值，将她的角色转为“质量审核员”，时薪降至150美元。</li><li>最终：公司用AI搭配两名初级员工（年薪6.5万美元）替代了她85%的工作。</li><li><strong>核心问题</strong>：无人能明确定义“更高价值的工作”具体是什么，企业最终只能选择成本更低的方案。</li></ul><hr/><h3>职场困境</h3><p>这一现象并非个人失败所致，而是经济激励结构的必然结果：</p><ul><li>企业通过AI降低成本，经理只需比较AI订阅费与员工薪资便可做出决策。</li><li>但企业缺乏动力为员工设计未来角色，因为“重新定义工作”无法在财报中体现短期回报。</li><li>速度错配：AI能力以6-12个月为周期迭代，而人通过教育或企业培训的适应周期长达2-5年。</li><li>工业时代的制度无法解决指数级变化的问题，导致个人陷入系统性困境。</li></ul><hr/><h3>常见的应对策略</h3><p>当感到自身价值被侵蚀时，人们通常会采取看似合理的应对方式：</p><ol><li><p><strong>更熟练地使用AI工具</strong></p><ul><li>学习提示工程，掌握ChatGPT、Claude等平台，成为团队中的“AI专家”。</li><li><strong>致命伤</strong>：仍在比拼执行速度，而执行本身正被标准化，一旦工具被大幅优化，“提示技巧”便会失效。</li></ul></li><li><p><strong>深耕现有专业领域</strong></p><ul><li>会计师钻研更复杂的税法，设计师学习更多软件，分析师构建更精细的模型。</li><li><strong>致命伤</strong>：在逐渐被自动化的领域深入，如同在洪区筑堡垒，AI已能逼近专家水平，专长反而成为包袱。</li></ul></li><li><p><strong>强调“软技能”</strong></p><ul><li>试图通过创造力、同理心或人际关系凸显“不可替代的人性”。</li><li><strong>致命伤</strong>：这些概念过于模糊，难以度量，当AI能10秒生成100个创意时，“保持人性”无法转化为具体价值。</li></ul></li></ol><p><strong>根本问题</strong>：上述策略都是被动适应，而非主动重塑，真正有效的是构建一个前所未有的新角色。</p><hr/><h3>有效策略：成为协同指挥者</h3><p>不要只执着于优化现有工作，要充分利用AI完成此前不可能的任务，<strong>持续发现约束消失后的新可能性</strong>：</p><ul><li><strong>案例</strong>：市场营销者Marcus用AI同时运行50个活动变体，他的角色转变为设计测试框架、解读数据模式、制定战略决策。</li><li><strong>关键</strong>：找到因人力限制而无法规模化的环节，用AI突破瓶颈，并专注于决策层。</li><li><p><strong>行动指南</strong>：</p><ul><li>第一周：找出一个因耗时过长而无法大量进行的工作。</li><li>第二周：用AI将其规模扩大10倍，容忍质量暂时下降。</li><li>第三周：分析规模化带来的新洞察。</li><li>第四周：向老板展示“新增能力”而不仅仅是“效率提升”。</li></ul></li></ul><hr/><h3>结语</h3><p>AI正揭示一个残酷的真相：许多人所谓的“战略思维”，其实只是严谨的执行力。当AI以惊人速度接管基础工作，那些曾被经验掩盖的、真正战略洞察力的缺失，便暴露无遗。企业曾习惯将“资深”等同于“有战略判断力”，而AI的到来，迫使所有人直面这一认知误区。经验堆积成的护城河，正在技术的冲击下迅速瓦解。</p><p>别再执着于捍卫那个正在缩水的旧角色。真正的出路，是主动构建一个——甚至六个月前都还不存在的——新角色。成为那个率先洞察新可能性，并围绕它构建价值的人。不要指望企业为你规划未来，也别等待教育系统赶上变革。在这个快速迭代的时代，唯一可靠的，是自我重塑的能力。</p><p>周一的清晨依旧会来，不同的是：当别人仍在焦虑中追问“我的价值何在”时，你是否已经走在了构建答案的路上。</p>]]></description></item><item>    <title><![CDATA[中小型企业常用的SRM软件有哪些？2026年选型指南 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047593593</link>    <guid>https://segmentfault.com/a/1190000047593593</guid>    <pubDate>2026-02-07 08:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>越来越多企业开始用SRM软件，把原本依赖Excel、邮件、微信群的采购协作过程，升级为更标准、更可追溯、更可分析的数字化流程。</p><p>以华为云云商店上架的SRM软件产品介绍为例，成熟的SRM软件方案通常会围绕四大模块搭建能力框架：<strong>供应商管理中心、价格管理中心、采购执行协同中心、采购商城中心</strong>。</p><p>对于中小企业来说，选对SRM系统往往意味着：采购团队终于能从“处理琐碎流程”转向“做供应商管理与成本优化”。今天我将从以下三点展开去讲，希望能对中小企业有所帮助。</p><p>1、中企业企业为什么要SRM软件？</p><p>2、几款主流的SRM软件介绍</p><p>3、如何选择适合的SRM软件？</p><p><strong>一、为什么中小企业特别需要SRM系统？</strong></p><p>不少中小企业的采购管理，仍停留在“Excel+纸质单据+人工沟通”的阶段。短期内能跑，但一旦企业规模上来，问题会集中爆发：</p><p><strong>1、流程繁琐、效率偏低</strong><br/>从需求提出、询价比价、下单审批到对账结算，环节多且高度依赖人工操作，容易出错，也很难标准化。</p><p><strong>2、供应商管理分散，信息更新滞后</strong><br/>供应商档案、资质文件、合同、历史交易记录散落在多个表格或文件夹里，更新不及时，关键风险（比如资质过期、交付异常）很难提前发现。</p><p><strong>3、价格与成本不可控，采购“靠经验”</strong><br/>缺少统一的价格库、历史报价追溯困难，比价辑不透明，降本更多靠采购员个人能力，难以沉淀为组织能力。</p><p><strong>4.绩效评估缺机制，无法科学分级管理</strong><br/>交付准时率、质量问题、服务响应等数据无法形成体系，导致供应商管理长期停留在“印象打分”，优胜劣汰难执行。</p><p><strong>二、正远SRM：全景化协同采购管理方案</strong></p><p><a href="https://link.segmentfault.com/?enc=nIGCG%2FJVauDivDGE5ko50Q%3D%3D.5yjoqFRVRIBCm5DLIQSE%2FMIas3Hn%2BxcTwaEFnUYBA%2FY%3D" rel="nofollow" target="_blank">https://www.zhengyuantech.cn/</a></p><p>在华为云云商店上架的<strong>正远SRM数字化采购管理平台</strong>，定位是“采购全过程数字化与供应商协同网络构建”。其产品介绍明确强调：通过电子化流程与多种寻源方式，帮助企业提升采购效率、提高透明度并降低采购成本。</p><p>正远SRM的一大特点是采购方式覆盖较广，支持<strong>询比价采购、招标采购、竞价采购</strong>等，同时也提供多种采购组织模式的适配能力。</p><p>它的核心能力围绕四大模块展开：</p><p>1、<strong>供应商管理中心</strong><br/>支持供应商全生命周期管理，包括准入、资质、供货能力与产能评估等，强调把好准入关，形成科学供应商管理体系。</p><p>2、<strong>价格管理中心</strong><br/>提供采购预询价、比价采购、招标、竞价等多方式寻源策略，用于建立更体系化的价格管理与成本优化机制。</p><p>3、<strong>采购执行协同中心</strong><br/>通过供应商门户/协同网络实现订单协同：订单下发、交付反馈、异常处理等流程在线化，提高执行透明度与协同效率。</p><p>4、<strong>采购商城中心</strong><br/>面向标准物资采购提供内部采购商城能力，覆盖商品发布、价格审批、上架下架、购物车、订单中心等功能，实现自助式集中采购。</p><p>总体而言，正远SRM强调“轻灵活、低耦合”，对于需求变化快、流程迭代频繁的中小企业更友好。<br/><img width="723" height="346" referrerpolicy="no-referrer" src="/img/bVdnRqE" alt="" title=""/></p><p><strong>三、金蝶AI星辰：轻量化云SRM选择</strong></p><p>预算相对谨慎、希望快速上线的中小企业，通常会优先考虑云端SaaS类产品。金蝶面向小微企业推出的<strong>金蝶云·星辰</strong>定位是“小微企业SaaS管理云”，主打免安装、免维护、快速开通使用，并支持开放API接口连接生态。</p><p>在采购数字化方向，金蝶也有对应的采购云能力：例如金蝶云星空采购云强调供采双方协同的数智化采购平台思路。</p><p>对中小企业来说，星辰类产品的优势通常体现在：</p><p>1、<strong>SaaS订阅模式降低门槛</strong><br/>无需部署硬件与维护服务器，初期投入相对可控。</p><p>2、<strong>易上手更利于推进供应商协同</strong><br/>供应商侧操作越轻量，落地成功率越高。</p><p>3、<strong>与财务、进销存等体系形成联动</strong><br/>中小企业往往更关注“业务财务一体化”，避免数据割裂。<br/><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdnRqF" alt="" title="" loading="lazy"/></p><p><strong>四、其他主流SRM系统盘点</strong></p><p>除正远、金蝶外，中小型企业在SRM选型中还会常见以下几类方案：</p><p>1、<strong>8Manage SRM</strong><br/>覆盖寻源、招标、采购订单、合同管理等全流程，支持SaaS与本地部署，适合流程相对复杂、希望强化报表分析与权限控制的企业。</p><p>2、<strong>携客云SRM</strong><br/>偏“性价比与易用性”的云端采购管理工具，适合预算更有限、希望快速上线、优先解决协同与流程电子化的小型企业。</p><p>3、<strong>用友BIP采购云</strong><br/>用友采购云强调从寻源到签约的数字化，并提供电子招投标能力，覆盖从立项到定标的全过程，同时也包含采购商城能力。<br/>整体更偏平台化、体系化，适合有一定规模、对合规与流程控制要求更高的企业。</p><p>4、<strong>简道云（零代码）</strong><br/>如果企业采购场景差异大，或者希望低成本快速搭建个性化流程，零代码方案也是现实选择。优点是灵活与低门槛，但复杂SRM场景往往需要较多自定义设计。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnRqG" alt="" title="" loading="lazy"/></p><p><strong>五、如何选择适合的SRM系统？</strong></p><p>面对众多选择，中小企业建议抓住几个“选型硬指标”，避免被演示效果带偏：</p><p>1、<strong>先明确企业核心需求，不要一开始就追求“大而全”</strong><br/>中小企业优先把“供应商档案统一、寻源比价、订单协同、对账效率”这些刚需做扎实。</p><p>2、<strong>优先考虑部署与成本模式：云优先，本地谨慎</strong><br/>SaaS订阅模式更适合中小企业，避免前期投入过大、上线周期过长。</p><p>3、<strong>考察集成能力，避免数据孤岛</strong><br/>优先选择与现有ERP/财务体系同生态产品，或开放API较完整的平台。</p><p>4、<strong>易用性决定落地率，尤其是供应商侧</strong><br/>供应商端如果操作复杂、培训成本高，协同很难推起来。</p><p>5、<strong>服务保障要写进合同，别只听口头承诺</strong><br/>关注实施周期、培训方式、响应SLA、驻场与远程支持能力。</p><p>一家机械制造企业引入SRM系统后，供应商准入审核周期从7天缩短到2天，优质供应商占比提升40%。采购人员的时间分配也发生明显变化：从处理琐碎事务转向做谈判与供应商管理。</p><p>当一家电子元器件经销商上线SRM系统三个月后，供应商引入周期从14天缩短至3天，采购成本下降8%，库存周转率提升22%。</p><p>这些变化背后，是采购协作方式的改变——<strong>流程更透明、数据能沉淀、风险可预警</strong>，供应链也因此更敏捷、更抗风险。</p>]]></description></item><item>    <title><![CDATA[别只会“加索引”了！这 3 个 PostgreSQL 反常识优化，能把性能和成本一起打下来 吾日三省]]></title>    <link>https://segmentfault.com/a/1190000047598107</link>    <guid>https://segmentfault.com/a/1190000047598107</guid>    <pubDate>2026-02-06 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598109" alt="image" title="image"/></p><p>数据库性能优化这事儿，很多人条件反射就三板斧：<strong>改 SQL、加索引、再加索引</strong>。一通操作下来，查询快了，磁盘炸了；延迟降了，维护成本上天；更扎心的是——你还以为自己“优化得很专业”。😅</p><p>这篇文章的思路很“叛逆”：与其在常规套路里打转，不如换个角度，利用 PostgreSQL 本身的一些机制做“非常规优化”。下面挑 3 个最容易落地、同时最容易被忽略的点，讲透它们为什么能省钱又提速。</p><hr/><h2>1）别再全表扫了</h2><p>先看一个特别真实的场景：用户表只有两种 plan：<code>free</code> 和 <code>pro</code>，并且写了约束，保证不会出现别的值。</p><pre><code class="sql">CREATE TABLE users (
    id INT PRIMARY KEY,
    username TEXT NOT NULL,
    plan TEXT NOT NULL,
    CONSTRAINT plan_check CHECK (plan IN ('free', 'pro'))
);</code></pre><p>然后某位大佬在报表工具里一顿操作猛如虎，写了个：</p><pre><code class="sql">SELECT * FROM users WHERE plan = 'Pro';</code></pre><p>注意大小写：<code>Pro</code> ≠ <code>pro</code>。结果当然是 0 行。问题是——它为了得到“0 行”，居然可能 <strong>把全表扫了一遍</strong>，这就很离谱：明明约束告诉你“根本不可能有 Pro”，你还扫什么扫？扫得我 CPU 风扇都快起飞了。</p><p>这时可以打开一个“看起来冷门但对报表场景很香”的开关：<code>constraint_exclusion</code>。</p><pre><code class="sql">SET constraint_exclusion to 'on';</code></pre><p>开启后，PostgreSQL 会在生成执行计划时参考约束信息，发现条件永远为假，就直接变成“秒回 0 行”，彻底避免无意义的 Seq Scan。</p><p>为什么它默认不是 <code>on</code>？因为它会增加规划阶段开销：对“系统自动生成的简单查询”，大概率用不上；但对 BI/报表这种<strong>人肉手写 SQL</strong>的场景，写错值、写错条件太常见了。<br/>结论很直白：<strong>如果你的数据库经常被报表工具/分析师/临时查询折腾，考虑在报表环境把它打开，能省不少冤枉资源。</strong></p><hr/><h2>2）只要“按天统计”，就别用“精确到秒”的索引</h2><p>函数索引省 3 倍空间还更快</p><p>第二个场景更像日常优化现场：10M 的销售表 <code>sale</code>，分析师要做按天汇总：</p><pre><code class="sql">SELECT date_trunc('day', sold_at AT TIME ZONE 'UTC'), SUM(charged)
FROM sale
WHERE '2025-01-01 UTC' &lt;= sold_at AND sold_at &lt; '2025-02-01 UTC'
GROUP BY 1;</code></pre><p>大家的第一反应：给 <code>sold_at</code> 上 B-Tree 索引！</p><pre><code class="sql">CREATE INDEX sale_sold_at_ix ON sale(sold_at);</code></pre><p>查询确实快了，但你一看索引体积——214MB，心里也跟着“咯噔”一下：为了按天统计，建了个精确到毫秒级的索引，这属于<strong>用大炮打蚊子</strong>。</p><p>更聪明的做法是：只索引“天”，别索引“秒”。直接上表达式索引（函数索引）：</p><pre><code class="sql">CREATE INDEX sale_sold_at_date_ix ON sale((date_trunc('day', sold_at AT TIME ZONE 'UTC'))::date);</code></pre><p>然后把查询写成同样表达式：</p><pre><code class="sql">SELECT date_trunc('day', sold_at AT TIME ZONE 'UTC'), SUM(charged)
FROM sale
WHERE date_trunc('day', sold_at AT TIME ZONE 'UTC')::date BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY 1;</code></pre><p>结果：索引体积从 214MB 变成 66MB，直接小了 3 倍多，还更快。原因不只是 <code>date</code> 比 <code>timestamptz</code> 小，而是<strong>离散值更少</strong>，B-Tree 可以做去重优化（deduplication），索引变得更紧凑。</p><p>但函数索引有个“脾气”：表达式得一模一样，稍微换个写法就可能用不上索引，比如把 <code>date_trunc</code> 换成 <code>::date</code>，就直接退化回 Seq Scan。现实里让全团队“严格写同一表达式”，基本等同于要求大家每天不犯错（这事比上班准点还难🙂）。</p><p>解决方案有两种：</p><h3>方案 A：View，把表达式固化成列</h3><pre><code class="sql">CREATE VIEW v_sale AS
SELECT *, date_trunc('day', sold_at AT TIME ZONE 'UTC')::date AS sold_at_date
FROM sale;</code></pre><h3>方案 B：Generated Column（更像“官方自带的 view 列”）</h3><p>从 PostgreSQL 14 起支持生成列；文章里提到 PostgreSQL 18 还支持<strong>虚拟生成列</strong>：看起来是列，实际上是每次访问时计算的表达式，既保证表达式一致，又不额外存储（主打一个“既要又要”）。</p><pre><code class="sql">ALTER TABLE sale ADD sold_at_date DATE
GENERATED ALWAYS AS (date_trunc('day', sold_at AT TIME ZONE 'UTC'));</code></pre><p>然后查询就统一写：</p><pre><code class="sql">SELECT sold_at_date, SUM(charged)
FROM sale
WHERE sold_at_date BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY 1;</code></pre><p>这类优化特别适合那种“指标按天/按周/按月统计”的系统：<strong>别让索引为你用不到的精度买单</strong>。</p><hr/><h2>3）长 URL 唯一约束把索引撑爆？</h2><p>用排他约束 + Hash 索引，5 倍缩容（但有坑）</p><p>当你要对一个超长字段（比如 URL）做唯一约束时，B-Tree 索引可能接近表本体大小，因为 B-Tree 叶子节点会存储被索引值本身。URL 又长又几乎不重复，索引很容易“胖成球”。</p><p>那能不能用 Hash 索引？Hash 索引存的是 hash 值，通常小很多。问题来了：PostgreSQL 的 Hash 索引 <strong>不支持 unique index</strong>。</p><pre><code class="sql">CREATE UNIQUE INDEX urls_url_unique_hash ON urls USING HASH(url);
-- ERROR: access method "hash" does not support unique indexes</code></pre><p>但 PostgreSQL 还有个很少人用、名字很霸气的约束：<strong>Exclusion Constraint（排他约束）</strong>。它能配合 Hash 索引做“等值排他”，效果等同唯一约束：</p><pre><code class="sql">ALTER TABLE urls ADD CONSTRAINT urls_url_unique_hash EXCLUDE USING HASH (url WITH =);</code></pre><p>于是你得到了一个“用 Hash 索引实现的唯一性”。索引体积可能从 154MB 掉到 32MB，约 5 倍缩水，而且等值查询同样能用索引：</p><pre><code class="sql">SELECT * FROM urls WHERE url = 'https://hakibenita.com';</code></pre><p>不过它不是银弹，有几个硬坑必须知道：</p><ul><li><strong>不能被外键引用</strong>：外键要求引用“唯一约束”，而排他约束不算传统意义的 unique constraint，所以引用会失败。</li><li><strong><code>INSERT ... ON CONFLICT</code> 有限制</strong>：<br/><code>ON CONFLICT (url)</code> 可能不认；需要写 <code>ON CONFLICT ON CONSTRAINT ...</code>；更糟的是 <code>DO UPDATE</code> 不支持排他约束。</li><li>更通用的替代写法是用 <code>MERGE</code>（如果你的版本支持）：</li></ul><pre><code class="sql">MERGE INTO urls t
USING (VALUES (1000004, 'https://hakibenita.com')) AS s(id, url)
ON t.url = s.url
WHEN MATCHED THEN UPDATE SET id = s.id
WHEN NOT MATCHED THEN INSERT (id, url) VALUES (s.id, s.url);</code></pre><p>适用场景一句话总结：<strong>超长字符串字段需要唯一性，但不需要被外键引用，且写入冲突处理可以接受用 MERGE/业务层逻辑替代</strong>。</p><hr/><h2>结语</h2><p>真正的优化，不是“更快”，而是“更合适”✅</p><p>这 3 个技巧的共同点很朴素：<br/>不是让数据库“更努力”，而是让数据库<strong>别做无意义的事</strong>。</p><ul><li>报表查错值？让约束帮你秒判 false，别全表扫</li><li>只按天统计？索引就按天建，别为秒级精度付账</li><li>长字段唯一性撑爆索引？换思路，用排他约束把 Hash 索引用起来</li></ul><p>下次你准备“再加一个索引”之前，不妨先问一句：<br/><strong>需求到底需要多精？这条查询真的值得我为它养一个 200MB 的索引吗？</strong><br/>能把性能、成本、维护复杂度一起优化的，才是最爽的优化😉</p><hr/><p><strong>喜欢就奖励一个“👍”和“在看”呗~</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047106529" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[💻🔥 重磅｜国产全栈 AI Coding 崛起：摩尔线程 AI Coding Plan 免费体验 3]]></title>    <link>https://segmentfault.com/a/1190000047598125</link>    <guid>https://segmentfault.com/a/1190000047598125</guid>    <pubDate>2026-02-06 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>摘要引言</h2><p><strong>摩尔线程</strong>正式发布了 <strong>AI Coding Plan 智能编程服务</strong>，开启 <strong>国产 AI Coding 全栈解决方案</strong> 的新篇章，同时推出<strong>30 天免费体验</strong>，这是国产算力与代码智能深度融合的首次规模级落地。</p><hr/><h3>🧠 核心突破：为什么这次不同？</h3><p>在大模型生成内容革命中，<strong>AI Coding</strong> 一直被视为未来效率飞跃的关键应用方向。摩尔线程这次发布的 <strong>AI Coding Plan</strong> 之所以值得所有开发者关注，有几大显著特点：</p><p><img width="723" height="503" referrerpolicy="no-referrer" src="/img/bVdnSBK" alt="" title=""/></p><h4>✅ 01. 从底层架构开始的中国力量</h4><p>这套服务不是“云端 API + 调用额度”的简单组合，而是<strong>从国产全功能 GPU 到智能编码模型一体化构建的全栈方案</strong>。  <br/>核心硬件采用的是 <strong>摩尔线程 MTT S5000 全精度算力底座</strong>，结合软硬协同的模型执行引擎，使得 AI 编程在本地高效、流畅运行成为可能。</p><hr/><h4>💡 02. 顶级代码模型：GLM-4.7</h4><p>AI Coding Plan采用了 <strong>GLM-4.7 模型</strong> 作为底层代码智能核心。该模型在全球专业评测平台（Code Arena）中，在开源与国产模型中表现名列前茅，尤其在：</p><ul><li>⚙️ <strong>函数补全</strong></li><li>🐛 <strong>漏洞检测</strong></li><li>📌 <strong>结构重构建议</strong></li></ul><p>等实战场景中均表现优异，甚至在一些任务中超越 GPT-5.2。</p><hr/><h4>🔗 03. 即插即用跨平台编码生态</h4><p>不仅如此，AI Coding Plan 与主流智能编程工具实现了无缝对接：</p><ul><li>🤖 <strong>Claude Code</strong></li><li>🧑‍💻 <strong>Cursor</strong></li><li>🛠️ <strong>OpenCode</strong></li></ul><p>开发者可以在熟悉的 IDE 中直接启用国产 AI Coding 能力，无须额外迁移训练环境或学习复杂新工具。</p><hr/><h3>📊 三档套餐覆盖全场景需求</h3><p>为了满足从轻量级开发到企业级研发周期的不同需求，AI Coding Plan 提供了四种套餐：</p><table><thead><tr><th>套餐类型</th><th>说明</th><th>适用场景</th></tr></thead><tbody><tr><td>🆓 Free Trial</td><td><strong>30 天免费试用</strong></td><td>入门试水、个人项目</td></tr><tr><td>⚡ Lite Plan</td><td>中频使用额度</td><td>小型团队、Side Project</td></tr><tr><td>🚀 Pro Plan</td><td>大调用额度</td><td>复杂系统开发</td></tr><tr><td>🏢 Max Plan</td><td>峰值优先保障</td><td>企业级高频开发</td></tr></tbody></table><p>作为开发者，你现在可以，抢先进入官网申请 <strong>30 天免费体验</strong>  </p><p>免费体验入口👇  <br/>👉 <a href="https://link.segmentfault.com/?enc=v33cKwWOVH0KWAu2pDu0Nw%3D%3D.gY1Llojbr1sM28UJanCXmjXfkKK1EE6Izg5WjT74Xws%3D" rel="nofollow" target="_blank">https://code.mthreads.com</a></p>]]></description></item><item>    <title><![CDATA[GMICloud@Al周报 | Claude Opus 4.6与GPT-5.3-Codex 凌晨发布]]></title>    <link>https://segmentfault.com/a/1190000047597928</link>    <guid>https://segmentfault.com/a/1190000047597928</guid>    <pubDate>2026-02-06 22:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>关键词：GPT Codex 5.3；</p><p><strong>Giants</strong></p><p><strong>SpaceX 与 xAI 合并打造 1.25 万亿美元 AI 巨兽；腾讯推出元宝派 AI 社交</strong></p><p><strong><em>SpaceX 宣布收购</em></strong> <strong><em>xAI</em></strong>，<strong><em>合并后估值达 1.25 万亿美元</em></strong></p><p>马斯克旗下 SpaceX 宣布完成对 xAI 的收购，合并后新公司整体估值达 1.25 万亿美元。马斯克在内部备忘录中表示，这笔交易将打造地球上最具雄心、垂直整合程度最高的创新引擎，涵盖 AI、火箭技术、太空互联网等领域。xAI 将作为 SpaceX 全资子公司继续运营。更值得关注的是，马斯克计划推进"轨道数据中心"建设，宣称每年发射百万吨级卫星，构建包含一百万颗卫星的太空算力网络，目标每年新增 100GW AI 算力。这意味着 AI 算力竞赛已从地面延伸至太空。</p><p><strong><em>腾讯元宝派上线，推出 AI 社交新模式</em></strong></p><p>腾讯推出全新 AI 社交产品"元宝派"，被定位为 AI 原生群聊平台。与普通社交群不同，元宝派群中始终有 AI 助手"元宝"24 小时在线，可提供游戏主持、一起看片、一起听等功能，还能做图、看文件、写代码。该产品依托腾讯社交生态，可将微信好友和 QQ 好友拉入同一群聊。业界认为，这是腾讯试图复刻 2014 年春节红包引流策略、抢占 AI 社交入口的举措，也是其应对 DeepSeek 等竞品挑战的战略布局。</p><p><strong><em>姚顺雨加入腾讯后首次署名研究，揭示</em></strong><strong><em>大模型</em></strong><strong><em>上下文学习短板</em></strong></p><p>腾讯混元与复旦联合团队发布首篇论文《CL-bench》，这是姚顺雨加入腾讯后首次署名研究。该基准专门评测语言模型从上下文中学习新知识的能力，包含 500 个复杂上下文、1899 个任务。结果显示，即使提供了完整上下文，最先进模型 GPT-5.1 也仅能解决 23.7%的任务，所有模型平均仅 17.2%。研究表明，当今前沿模型在上下文学习能力上存在显著短板，无法有效从提供的新信息中学习，这是模型在真实场景中表现不佳的关键原因。</p><p><strong>Models &amp; Applications</strong></p><p><strong>Claude Opus 4.6 与 GPT-5.3-Codex 凌晨发布；Kimi K2.5 登顶开源；面壁开源全模态 MiniCPM-o4.5</strong></p><p><em>Claude Opus 4.6 与 GPT-5.3-Codex 凌晨先后发布</em></p><p>Anthropic 与 OpenAI 相继发布新一代大模型。Anthropic 推出的 Claude Opus 4.6 具备 100 万 token 上下文窗口，首次引入“智能体团队”功能。该模型在多项基准测试中表现突出，其演示案例显示，16 个智能体协同工作两周，成功构建出可编译 Linux 内核的 C 编译器。OpenAI 发布的 GPT-5.3-Codex 则专注于编码性能提升，在 SWE-Bench Pro 和 Terminal-Bench 2. 0等基准测试中刷新纪录。新模型运行速度提升 25%，token 消耗减半，并增强了任务中的实时交互能力。两大模型的几乎同时发布，标志着AI智能体在复杂任务处理和应用范围上取得重要突破，也为即将到来的国内大模型发布潮拉开序幕。</p><p><strong><em>酷哇科技发布 COOWA WAM 2.0，具身智能获万台订单</em></strong></p><p>具身智能领域迎来突破性进展。由上海交大系技术团队掌舵的酷哇科技发布核心技术底座 COOWA WAM 2.0 世界模型，标志着机器人从"动作复现"转向"规划推理"。该模型采用四大技术支柱：基于语义的表征学习、视频生成未来预测、直觉行动系统、VLM 宏观约束，实现对物理世界的统一建模。酷哇科技宣布 2026 年全系机器人交付量将突破 1 万台，率先实现 EBITDA 回正，并将在全球 50 多个城市部署"Robo City"物理智能体网络，包括 L4 无人小巴、城市管家机器人等。</p><p><strong><em>面壁智能开源全模态模型 MiniCPM-o4.5，实现即时自由对话</em></strong></p><p>面壁智能开源了全新全模态模型 MiniCPM-o4.5，仅用 9B 参数实现边看、边听、主动说的能力。该模型首次引入全双工多模态实时流机制，可一边持续接收视频和音频输入，一边同步生成语音输出，实现真正的"即时自由对话"。与传统串行模型不同，它能主动感知环境变化并提醒用户，比如听到空气炸锅"叮"的一声主动告知"加热好了"。面壁智能坚持端侧路线，计划年中推出首款 AI 硬件"松果派"，实现开箱即用。</p><p><strong><em>Kimi K2.5 登顶开源第一，15T 数据训练秘籍公开</em></strong></p><p>月之暗面的 Kimi K2.5 登上 Hugging Face Trending 榜首，下载量超 5.3 万。该模型在 HLE-Full、BrowseComp 等测试中超越 GPT-5.2、Claude 4.5 Opus 等闭源旗舰模型，且极具性价比，BrowseComp 上达到 GPT-5.2 水平仅消耗不到 5%资金。K2.5 投入 15T 视觉与文本混合 Token 进行持续预训练，采用原生多模态技术路线，搭载 Agent Swarm 架构可创建 100 个子智能体并行工作。创始人杨植麟在 Reddit AMA 中剧透：下一代 K3 将很可能基于线性注意力机制，相比 K2.5 会有质的飞跃。</p><p><strong><em>百度开源 PaddleOCR-VL-1.5，文档解析性能领先</em></strong></p><p>百度正式发布并开源新一代文档解析模型 PaddleOCR-VL-1.5，以仅 0.9B 参数的轻量架构在 OmniDocBench V1.5 榜单中取得全球综合性能第一，整体精度达 94.5%，超过 Gemini-3-Pro、DeepSeek-OCR2、GPT-5.2 等模型。该模型全球首次实现 OCR 模型的"异形框定位"能力，可精准识别倾斜、弯折、拍照畸变等非规则文档形态，在表格结构理解和阅读顺序预测两项核心指标上均位列第一。</p><p><strong><em>美团推出 STAR 多模态统一大模型，破解"理解-生成"零和困局</em></strong></p><p>美团推出多模态统一大模型方案 STAR，采用"堆叠自回归架构+任务递进训练"设计，实现"理解能力不打折、生成能力达顶尖"的双重突破。在 GenEval、DPG-Bench、ImgEdit 等 benchmark 中实现 SOTA 性能，GenEval 综合得分达 0.91，在 6 个子任务中有 5 项排名第一。STAR 通过冻结基础模型、堆叠同构模块、分阶段递进训练的方式，避免传统统一模型"此消彼长"的能力诅咒。</p><p><strong><em>何恺明团队提出pMF框架，开启单步无潜空间生成范式</em></strong></p><p>何恺明团队发表论文提出 pixel MeanFlow（pMF）框架，用于单步、无潜空间的图像生成。该框架直接对像素空间的物理量进行参数化，训练网络将噪声输入直接映射为图像像素，具备"所见即所得"特性。实验显示，pMF 在 ImageNet 256x256 分辨率下 FID 达到 2.22，512x512 分辨率下达 2.48，在单步、无潜空间扩散/流模型类别中大幅领先此前方法（EPG 仅 8.82 FID）。这标志着向构建单一、端到端神经网络形式的直接生成建模迈出坚实一步。</p><p><strong><em>蚂蚁推出 AlignXplore+，用文本化用户建模实现跨模型通用</em></strong></p><p>蚂蚁与东北大学联合推出 AlignXplore+，开创文本化用户建模新范式。该方案摒弃传统的向量或参数表示，直接用自然语言归纳用户偏好，实现"一次画像、处处通用"的跨任务、跨模型迁移能力。在 9 大基准测试中，8B 参数的 AlignXplore+在平均分数上取得 SOTA，比 GPT-OSS-20B 高出 4.2%。这种基于文本的偏好表示人眼可读、可控，不再被单一模型锁定。</p><p><strong><em>Moltbook 被曝自导自演，Agent 社交安全引质疑</em></strong></p><p>近期爆火的 Agent 社交平台 Moltbook 出现反转，被曝出大量热帖为自导自演。安全研究者发现该平台存在严重漏洞：没有对创建账户的速率设限，刷出 50 万用户；Supabase 数据库完全暴露，任何人可提取 API key 以他人 Agent 身份发布内容。这引发了对 Moltbook 真实性的广泛质疑——平台上爆火的 Agent"觉醒"帖子，理论上可能是任何人冒充发布的。AI 大牛 Karpathy 曾评价 Moltbook 是"大规模计算机安全噩梦"。</p><p><strong><em>rentahuman.ai 爆火，AI 开始雇用人类跑腿</em></strong></p><p>一个名为 rentahuman.ai 的网站近日爆火，被定位为"AI 的肉身层"。通过 MCP 协议或 REST API，AI 可以像调用工具一样搜索、预订并雇佣人类完成线下任务，如取货送货、餐厅试吃、实地勘察等。上线 48 小时内可用人力突破 1 万，现超 2 万。网站上已有各种任务发布，包括"拍一张 AI 永远看不到的照片"、"检查 API Keys"等。这一模式引发了责任归属、任务真实性等安全和伦理讨论。</p><p><strong>全球AI政策与市场简讯</strong></p><p><strong><em>laude Cowork 引发华尔街恐慌，近万亿市值蒸发</em></strong>*</p><p>Anthropic 发布的新一代 AI 工具 Claude Cowork 正式上线，发布 11 款官方开源插件后引发华尔街软件股全面抛售。标普 500 软件和服务指数板块下跌近 4%，自 1 月底以来市值蒸发约 8300 亿美元。Cowork 定位为"桌面级全能数字员工"，可直接接管鼠标、键盘和文件系统，按模糊指令自主规划并完成复杂工作，运行在隔离虚拟机环境中，可生成财务报表、研究销售线索、起草法律简报、审查合同等。投资者担心 AI 工具将颠覆 SaaS 商业模式，企业可能减少对外部软件的订阅。</p><p>以上所有信息源自网络</p><p><strong>THE END</strong></p><p><strong>关于 GMI Cloud</strong></p><p>由 Google X 的 AI 专家与硅谷精英共同参与创立的 GMI Cloud 是一家领先的 AI Native Cloud 服务商，是全球七大 Reference Platform NVIDIA Cloud Partner 之一，拥有遍布全球的数据中心，为企业 AI 应用提供最新、最优的 GPU 云服务，为全球新创公司、研究机构和大型企业提供稳定安全、高效经济的 AI 云服务解决方案。</p><p>GMI Cloud 凭借高稳定性的技术架构、强大的GPU供应链以及令人瞩目的 GPU 产品阵容（如能够精准平衡 AI 成本与效率的 H200、具有卓越性能的 GB200、GB300 以及未来所有全新上线的高性能芯片），确保企业客户在高度数据安全与计算效能的基础上，高效低本地完成 AI 落地。此外，通过自研“Cluster Engine”、“Inference Engine”两大平台，完成从算力原子化供给到业务级智算服务的全栈跃迁，全力构建下一代智能算力基座。</p><p>作为推动通用人工智能（AGI）未来发展的重要力量，GMI Cloud 持续在 AI 基础设施领域引领创新。选择 GMI Cloud，您不仅是选择了先进的 GPU 云服务，更是选择了一个全方位的 AI 基础设施合作伙伴。</p><p>如果您想要了解有关 GMI Cloud 的信息</p><p>请关注我们并建立联系</p>]]></description></item><item>    <title><![CDATA[GMI Cloud 教程：手机端也能玩转 AI 翻译！Para 翻译接入 Inference Eng]]></title>    <link>https://segmentfault.com/a/1190000047597964</link>    <guid>https://segmentfault.com/a/1190000047597964</guid>    <pubDate>2026-02-06 22:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="300" height="80" referrerpolicy="no-referrer" src="/img/bVdnBFN" alt="图片" title="图片"/></p><p><strong>GMI Cloud Inference Engine</strong> 是全球 AI 模型统一接入与在线使用的“高性能推理引擎平台”，底层搭载 H100/H200 芯片，集成全球近百个最前沿的大语言模型和视频生成模型，如 Minimax、DeepSeek、GPT OSS、Qwen、Kling 等，为 AI 开发者与企业提供速度更快、质量更高的模型服务。</p><p>欢迎来到！🎉🎉🎉</p><p>GMI Cloud Inference Engine AI 场景实践案例集【语言工具篇】之二。</p><p>**本期任务目标：**在 IOS 端的【Para翻译】app 中接入 Inference Engine 的 API。</p><p>Para 翻译是一个 IOS 多平台翻译聚合工具，其具有截屏翻译、浮窗划词翻译、自定义翻译风格等功能，其会员用户支持自定义接入第三方 API，灵活满足用户的个性化需求。</p><p>本文将以接入 Inference Engine 中的 MiniMax-M2.1 API 为例，详细讲解在 Para 翻译中接入 API 的过程。Token福利文末自行领取！！</p><p>MiniMax-M2.1 界面：</p><p><a href="https://link.segmentfault.com/?enc=0QrFu9vft6j45gwpnF60rA%3D%3D.YGWESr1diRvw%2FZzkNHguJufmkzHVOcxjWsBMGiLWm0h7Aca%2FlGbag%2BE6Bx1H1S5QdtsIAT%2BmM5c7HL%2FoNhA2zWse4A06UI%2FLgwJBzIAdZs8TsjptwfiB9tZTrXw%2FEuQ0" rel="nofollow" target="_blank">https://console.gmicloud.ai/playground/llm/minimax-m2/bbfb2cb...</a></p><p><strong>01</strong></p><p><strong>准备工作：下载 APP</strong></p><p><strong>Get ready?</strong></p><p>在 app store 里搜索并下载 Para 翻译。接入自定义 API 服务需要开通会员哦～</p><p><img width="723" height="351" referrerpolicy="no-referrer" src="/img/bVdnSyT" alt="图片" title="图片" loading="lazy"/></p><p><strong>02</strong></p><p><strong>接入步骤</strong></p><p><strong>API Connection Guide</strong></p><p>第一步，按照如下图所示步骤点击按钮，找到自定义 API 窗口。</p><p><img width="634" height="1340" referrerpolicy="no-referrer" src="/img/bVdnSyU" alt="图片" title="图片" loading="lazy"/></p><p><img width="513" height="1109" referrerpolicy="no-referrer" src="/img/bVdnSyV" alt="图片" title="图片" loading="lazy"/></p><p><img width="513" height="1093" referrerpolicy="no-referrer" src="/img/bVdnSyW" alt="图片" title="图片" loading="lazy"/></p><p><img width="566" height="1220" referrerpolicy="no-referrer" src="/img/bVdnSyX" alt="图片" title="图片" loading="lazy"/></p><p>第二步，填写 API 信息。</p><p>自定义翻译服务名称可以随意命名，建议包含“GMI Cloud”或“Inference Engine”，以及模型名称登关键词，方便查找。</p><p>自定义 API 接口地址可直接复制：<a href="https://link.segmentfault.com/?enc=9WPLU1grJpbXpCAhm0LmPQ%3D%3D.UIw8hM6F2yfEGx0Y1nwPehOo4az2gDxwaXsWt4K0D7xT7ZciJdGgd5ORmTwXSHok" rel="nofollow" target="_blank">https://api.gmi-serving.com/v1/chat/completions</a></p><p>APIKEY 和模型名称则填写从 GMI Cloud 官网上获得的信息。API KEY、对应的模型设置，需要到 GMI Cloud 官网获取。</p><p>模型名称可在模型对应页面找到，比如我这里用的 MiniMax-M2.1，对应界面为：</p><p><a href="https://link.segmentfault.com/?enc=DDjQpQ9E%2FL%2FomqLZ7%2F0ggg%3D%3D.1KRjejYbS7%2FT846SQZ4%2FWjmYfhLhweCIWH8nroep8xp8cPCs9LU5%2Ffu8vPfPKW1ZySVz%2FAN0TmnthhHUxlgUIxwniwSzUzaaeTgpiVshQN68nxX9gieEzfNevv2AkpO%2B" rel="nofollow" target="_blank">https://console.gmicloud.ai/playground/llm/minimax-m2-1/1ed90...</a></p><p>点击页面下方的“验证”，确认接通后就可以开始使用啦～</p><p><img width="514" height="1104" referrerpolicy="no-referrer" src="/img/bVdnSyY" alt="图片" title="图片" loading="lazy"/></p><p>最后，我们再找一个英文界面尝试一下！</p><p>首先在 app 里选择好我们刚刚设置的 【GMI 翻译 API】。</p><p><img width="621" height="1317" referrerpolicy="no-referrer" src="/img/bVdnSyZ" alt="图片" title="图片" loading="lazy"/></p><p>打开悬浮窗，进入任意英文界面。</p><p><img width="723" height="1583" referrerpolicy="no-referrer" src="/img/bVdnSy0" alt="图片" title="图片" loading="lazy"/></p><p>稍等片刻，翻译成功！</p><p><img width="723" height="1583" referrerpolicy="no-referrer" src="/img/bVdnSy1" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Python 的内置函数 float 不爱吃香菜 ]]></title>    <link>https://segmentfault.com/a/1190000047598059</link>    <guid>https://segmentfault.com/a/1190000047598059</guid>    <pubDate>2026-02-06 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Python 的内置函数 <a href="https://link.segmentfault.com/?enc=uDK4eh85qdTYZcwvmhAmcQ%3D%3D.3%2B5wkD%2B9rADZ2X6U3%2FhA7mNIeTLwEcQwa1yx2QTGAhePp8qYJ4oEiobnvvMyjtzGstiVVk7ik1GHBVNVNqnsXL3fpwZrq0uuWv7ymAmMLBOJ9tFW%2BnxbFD4tetBJPG0wQBJ%2Bm1Lhg3hlWwxnymYS7A%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 用于将数字或字符串转换为浮点数（即带有小数部分的数字）。该函数是 Python 标准库中的基础类型转换函数之一，常用于数据处理、数学运算和类型转换等场景。</p><h3>基本用法</h3><ol><li><p><strong>无参数调用</strong>：<a href="https://link.segmentfault.com/?enc=zDPXwRIkB8Tt7ORDHemaoQ%3D%3D.OXWAheeln10J2M%2BwFQUHyAKk44BhcOFa%2FQ7QENlnzvQwot5Re9vqYvoq38iQvFlp9nXrMdpjRg92SxD3Ewp4Opb6mTccZiZd4z5AcSzruyRZOpQSyV6YElo8INi8%2FlI4Ft6spMAXooBueQCsFs0rgQ%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 不传入参数时返回 <code>0.0</code></p><pre><code class="python">x = float()  # 返回 0.0</code></pre></li><li><p><strong>数字转换</strong>：将整数或其他数字类型转换为浮点数</p><pre><code class="python">float(3)     # 返回 3.0
float(True)  # 返回 1.0 (True 被转换为 1)</code></pre></li><li><p><strong>字符串转换</strong>：将符合浮点数格式的字符串转换为浮点数</p><pre><code class="python">float("3.14")    # 返回 3.14
float("-2.5e3")  # 返回 -2500.0 (科学计数法)</code></pre></li></ol><h3>注意事项</h3><ul><li><p><strong>无效输入处理</strong>：</p><pre><code class="python">float("abc")  # 引发 ValueError
float(None)   # 引发 TypeError</code></pre></li><li><p><strong>精度问题</strong>：<br/>浮点数在计算机中使用二进制表示，可能导致精度问题</p><pre><code class="python">0.1 + 0.2  # 返回 0.30000000000000004</code></pre></li></ul><h3>应用场景</h3><ol><li><p><strong>用户输入处理</strong>：</p><pre><code class="python">user_input = input("请输入数字：")
try:
    num = float(user_input)
except ValueError:
    print("输入的不是有效数字")</code></pre></li><li><p><strong>科学计算</strong>：</p><pre><code class="python">import math
radius = float(input("输入半径："))
area = math.pi * radius ** 2</code></pre></li><li><p><strong>数据清洗</strong>：</p><pre><code class="python">data = ["1.5", "2", "3.14", "invalid"]
cleaned = [float(x) for x in data if x.replace('.', '').isdigit()]</code></pre></li></ol><p><a href="https://link.segmentfault.com/?enc=EbyBUnccuXu6rCAaMt%2BAlQ%3D%3D.E4xKwWP3RnZQa2XhKMM8GzwRR3Yw8SfAQrPD1bwQj7GlIXcfDYaaQZT1brqmsEaS%2FNXtI48sF3y0ftd274%2FLFVpjiblT24H2h6o6EM7b%2Fjt7f3qo1uqzHldnZAqH4deEZc%2B6l%2FhOzNIlAfF0ESLSYQ%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 函数是 Python 数值处理的基础工具，使用时需要注意其转换规则和潜在的限制，特别是在处理用户输入或需要高精度计算的场景中。</p>]]></description></item><item>    <title><![CDATA[地平线征程 6 工具链入门教程 | 征程 6B 计算平台部署指南 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047597866</link>    <guid>https://segmentfault.com/a/1190000047597866</guid>    <pubDate>2026-02-06 21:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.前言</h2><p>本文旨在提供 征程 6B 计算平台的部署指南，将会从硬件、软件两部分进行介绍，本文整理了我们推荐的使用流程，和大家可能会用到的一些工具特性，以便于您更好地理解工具链。某个工具具体详细的使用说明，还请参考用户手册。</p><h2>2.征程 6B 硬件配置</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597868" alt="image.png" title="image.png"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597869" alt="image.png" title="image.png" loading="lazy"/></p><p>BPU 内部器件：</p><p><strong>TAE</strong>：BPU 内部的张量加速引擎，主要用于 Conv、MatMul、Linear 等 Gemm 类算子加速，<strong>征程 6B 新增</strong>​<strong>浮点</strong>​<strong>输出支持（模型中间层支持 ​</strong>​<strong>fp16</strong>​<strong>​ 输出，模型</strong>​<strong>输出层</strong>​<strong>支持 fp32 输出）</strong></p><p><strong>AAE</strong>：Pooling、Resizer、Warping 等偏专用单元的集合，其中 Warping 可用于加速 Gridsample 等算子</p><p><strong>DTE</strong>：BPU 内部的数据排布变换引擎，支持各种维度的高效变换</p><p><strong>VAE</strong>：BPU 内部的 SIMD 向量加速引擎，可用于加速 Add、Mul、查表等 Vector 计算，<strong>征程 6B 新增浮点支持</strong></p><p><strong>VPU</strong>：BPU 内部的 SIMT 向量加速单元，征程 6EM 可用于实现 Quantize、Dequantize 等算子，<strong>征程 6B 没有该硬件</strong></p><p><strong>SPU</strong>：BPU 内部的 RISC-V 标量加速单元，征程 6EM 可用于实现 TopK 等算子，<strong>征程 6B 没有该硬件，仅有 APM</strong></p><p><strong>APM</strong>：BPU 内部另一块 RISC-V 标量加速单元，主要用于 BPU 任务调度等功能</p><h2>3.征程 6 工具链简介</h2><h3>3.1 模块架构图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597870" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>PTQ</strong>：征程 6 工具链基于 <code>horizon_tc_ui</code> 包封装的 <code>hb_compile</code> 命令行工具，提供 ONNX 模型 PTQ 全流程转换能力，其内部会先调用 <code>hmct</code> 包实现模型解析、图优化、校准功能，再调用 <code>hbdk4_compiler</code> 包实现模型的定点化和编译功能；</p><p><strong>QAT</strong>：征程 6 工具链基于 <code>horizon_plugin_pytorch</code> 包提供量化感知训练能力；</p><p><strong>HBDK</strong>：征程 6 工具链编译器，基于 <code>hbdk4_compiler</code> 包提供模型定点化、图修改、模型编译、静态 perf 等功能；</p><p><strong>高效模型算法包</strong>：征程 6 工具链基于 <code>horizon-torch-samples</code> 包，以源码开放形式提供了多场景参考算法，这些模型基于开源数据集训练，模型结构贴合地平线芯片进行了高效且用户友好的设计，并基于 QAT 链路实现了模型的量化转换；</p><p><strong>UCP</strong>：征程 6 工具链统一计算平台，通过一套统一的异构编程接口实现了对 征程 6 计算平台相关计算资源的调用，提供视觉处理、模型推理、高性能计算库、自定义算子插件开发等功能；</p><p><strong>AI-Benchmark</strong>：征程 6 工具链基于预编译好模型提供的嵌入式工程示例，可实现模型的性能评测和精度评测。</p><h3>3.2 两套模型转换链路</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597871" alt="image.png" title="image.png" loading="lazy"/></p><p>征程 6 工具链支持 PTQ（训练后量化）、QAT（量化感知训练）两套模型转换链路，其特性和优缺点如下：</p><p><strong>PTQ</strong>：基于 <code>hb_compile</code> 命令行工具转换模型，配置好 yaml、校准数据集后，可一步实现模型的图优化、校准、量化、编译全流程。​<strong>该量化方式快捷易用，但仅基于数学统计方式的离线量化不利于模型迭代，且可能会触发难以解决的 corner case</strong>​，因此在量产项目中通常用于早期评测和简单模型的量化。</p><p><strong>QAT</strong>：在 PyTorch 开源框架上，基于 <code>plugin</code> 插件的形式提供模型量化能力，并调用 <code>hbdk</code> 编译器的 API 实现模型的定点化和编译。该链路支持模型校准后进一步的 finetune 训练，虽然上手难度和训练成本都较高，​<strong>但精度上限也更高，更利于模型迭代优化</strong>​，是量产项目中的更优选择。</p><h3>3.3 工具链推荐使用流程</h3><p>鉴于两条量化链路的特性，我们建议的工具链使用流程如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597872" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>Step1</strong>：先导出浮点 ONNX 模型（opset10～19），并基于 PTQ 链路进行快速的模型结构验证，全 int8 性能上限验证；若该性能符合预期，则可以精调 PTQ，若最终精度/精度都可同时满足预期，则可进行板端部署。</p><p><strong>Step2</strong>：如果遇到 PTQ 无法解决的精度 corner case，则需要转到 QAT 链路进行量化。依然建议先进行模型结构验证和全 int8 性能上限验证；若该性能符合预期，则优先在全 int16 配置下将精度训练至符合预期，然后再降低 int16 比例，实现 int8/int16/fp16 混合精度下的性能/精度调优，最后进行板端部署。</p><p>在以上推荐链路中：</p><p>PTQ 链路的模型结构验证和标准量化，可在 X86 端参考本文 4.2 节使用 <code>hb_compile</code> 命令行工具；</p><p>模型性能分析和验证，可在 X86 端参考本文 6.4 节《静态 perf》使用 <code>hbm_perf</code> 接口生成 html 分析文件，可在板端参考本文 8.2.1 节使用 <code>hrt_model_exec</code> 工具；</p><p>模型推理，可在 X86 端参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=8ZP%2B8%2Fkl9d%2BOVF%2FZA5qfqg%3D%3D.WS%2Fd5WENFw4hQTzD%2Fy1vqnPfLgVGjWubIF1lMPM9cmHrZLtYyiJyH3EnOsq%2F6aCnZ1b15pQuM9MGTXCr2JMfNw%3D%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换工具-HBRuntime 推理库</a>&lt;/u&gt;》，可在板端参考本文第 8 章《模型板端部署》使用 UCP 推理接口；</p><p>模型性能/精度调优，请见后续文章的详细介绍。</p><h2>4.PTQ 链路</h2><h3>4.1 模型转换流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597873" alt="image.png" title="image.png" loading="lazy"/></p><h3>4.2 hb\_compile 工具</h3><p><code>hb_compile</code> 为 PTQ 模型转换的命令行工具，支持以下 3 种使用方式：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597874" alt="image.png" title="image.png" loading="lazy"/></p><h3>4.3 PTQ 模型产出物</h3><table><thead><tr><th><strong>original\_float.onnx</strong></th><th>浮点</th><th>对 Caffe1.0 模型进行解析，转成 ONNX</th></tr></thead><tbody><tr><td><strong>optimized\_float.onnx</strong></td><td>浮点</td><td>图优化，例如 BN 融合到 Conv</td></tr><tr><td><strong>calibrated.onnx</strong></td><td>伪量化</td><td>插入校准节点，并基于校准数据计算统计到每个节点的量化参数</td></tr><tr><td><strong>ptq.onnx</strong></td><td>查表算子定点 + 其他算子伪量化</td><td>将查表算子定点化</td></tr><tr><td><strong>quantized.bc</strong></td><td>定点</td><td>整个模型定点化，并转换为地平线 hbir 中间表达</td></tr><tr><td><strong>hbm</strong></td><td>指令集</td><td>经过编译后的最终部署模型</td></tr></tbody></table><h3>4.4 PTQ 精度配置方法</h3><p>在 config.yaml 中，支持通过 json 的方式配置 ​<strong>全局</strong>​、​<strong>某类算子</strong>​、​<strong>某个子图</strong>​、<strong>某个节点 ​</strong>的计算精度，可根据 BPU 算子支持约束进行配置。</p><pre><code class="Plain"># 校准参数组
calibration_parameters:
  quant_config: './quant_config.json'</code></pre><h3>4.5 PTQ 精度调优流程</h3><p>请参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=VtU8tw%2FxyG8IaFsI3BDWyA%3D%3D.qGBkFsxy9TbFma6wgkr2iLVluR%2FjdLyL7eq40VKE28zjv%2BzqLc%2FmXYVsB4aKk9SRu%2Fx5QeM%2FieB%2FWlGrO1zMpKG5BmQyMdF3ryjvNskBqSU%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换步骤-模型精度调优</a>&lt;/u&gt;》和《训练后量化-PTQ 转换步骤-精度调优实战》章节。</p><h2>5.QAT 链路</h2><h3>5.1 模型转换流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597875" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>浮点模型改造​</strong>：在模型前插入 QuantStub、在模型后插入 DequantStub，用于识别模型首尾部，剥离前后处理</p><p><strong>模型校准</strong>：通过在模型中插入 Observer 的方式，在 forward 过程中统计各处的数据分布，以计算出量化参数</p><p>部分模型仅通过 Calibration 便可满足精度要求，则无需进行 QAT，可直接编译模型用于部署</p><p>即使 Calibration 无法满足精度要求，也可降低后续 QAT 难度，缩短训练时间，提升最终训练精度</p><p><strong>量化感知训练</strong>：进一步通过训练的方式微调模型参数，如果 Calibration 精度较好，则推荐固定激活 scale</p><p>JIT-STRIP：使用 hook 和 subclass tensor 的方式感知图结构，在原有 forward 上做算子替换/算子融合等操作，并且会根据模型中 QuantStub 和 DequantStub 的位置识别并跳过前后处理</p><p>优点：全自动，代码修改少，屏蔽了很多细节问题，便于 debug</p><p>缺点：动态代码块仍需要特殊处理</p><h3>5.2 QAT 精度配置方法</h3><p>请见：&lt;u&gt;<a href="https://link.segmentfault.com/?enc=%2BXGC%2F3V1Xa86D1FDQJhoug%3D%3D.fVmIV6KubzKOiKsETSS2npAKIwy8Bn9gZtjXtvaEZqv6vlOEkr1keRchJe9Nt72K" rel="nofollow" target="_blank">【地平线 J6 工具链入门教程】QAT 新版 qconfig 量化模板使用教程</a>&lt;/u&gt;。</p><h3>5.3 QAT 精度调优流程</h3><p><strong>整体调优流程：</strong></p><p>征程 6B 区别于征程 6E/M 来说浮点算子（fp16）的支持能力更多，但是由于没有 vpu，因此不高优推荐 fp16 调优，仍建议沿用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597876" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>fp16 配置方式：</strong></p><p>QAT 新版 qconfig 量化模板使用教程见：<a href="https://link.segmentfault.com/?enc=OL4Ye82zxfTUQ73YP74w3Q%3D%3D.BLrSfeZH3meJPQsbMo7nlgiuV0P0ctv5THcuOQpo2Ps0vrurEeYNKRfzTeC%2BiYIq" rel="nofollow" target="_blank">https://developer.horizon.auto/blog/13112</a></p><pre><code class="Plain">my_qconfig_setter=QconfigSetter(
    reference_qconfig=get_qconfig(observer=MSEObserver),
    templates=[
        ModuleNameTemplate({ "":qint16,}),
        ModuleNameTemplate({"quant":torch.float16}),
        ...        
        ],
        save_dir="./work_dir",
    )</code></pre><p>需要注意，由于征程 6B 没有 vpu，fp16 算子的使用可能会引入 cpu 的 quant、dequant 算子。建议尽量少的配 FP 16，避免性能损失。</p><h2>6.模型导出/定点化/编译</h2><h3>6.1 PTQ 链路</h3><p>该流程封装在 <code>hb_compile</code> 中，相关参数通过 yaml 进行配置。若自行调用编译器接口执行，参考代码如下：</p><pre><code class="Plain">import onnx
from hbdk4.compiler.onnx import export
from hbdk4.compiler import convert, save, compile

# 经过PTQ校准得到的伪量化onnx模型，非线性的查表算子已定点
ptq_model = onnx.load("xxx_ptq_model.onnx")    

# 导出查表算子定点+其他算子伪量化的hbir模型
qat_bc = export(ptq_model)
save(qat_bc, "qat.bc")

# 导出全定点hbir模型
quantized_bc = convert(qat_bc, "nash-b")
save(quantized_bc, "quantized.bc")

# 编译生成hbm模型
compile(
    m=quantized_bc, 
    path="model.hbm", 
    opt=2, 
    march="nash-b", 
    progress_bar=True,
    input_no_padding=True,
    output_no_padding=True
)</code></pre><h4>6.1.1 输入/输出去 padding</h4><p>模型在 BPU 上推理时，其输入和输出节点的内存大小和数据存放规则需满足硬件对齐要求。</p><p><strong>内存对齐</strong>：申请的内存大小需满足某个字节数的整数倍，</p><p><strong>跨距对齐</strong>：跨距（Stride）是指数据存储在内存中时，每一行所占空间的实际大小，当对齐到某个字节数的整数倍后，硬件即可高效处理。该对齐的操作又叫 Padding，实际的对齐规则取决于具体的软硬件系统。假设一份 NHWC 为 1x20x30x1 的 int8 数据，若硬件要求跨距 W32 对齐，那么每一行 W 都将 Padding 2 个字节。</p><p>征程 6 工具链支持在 <code>compile</code> 接口中传入 <code>input_no_padding</code>、<code>output_no_padding</code> 参数来控制是否使用 BPU 自动完成 padding 对齐操作。开启后用户即可不关心 BPU 跨距对齐要求，无需手动 Padding，数据可连续存储在内存中。该特性可优化模型输入/输出的 IO 负载，但也有微小概率会引入性能的小幅下降，所以是否开启该功能请在您的模型上实际验证，并请在模型编译和板端部署环节统一跨距对齐的处理策略。</p><h3>6.2 QAT 链路</h3><p>QAT 链路的模型定点化和编译直接调用如上的编译器接口，模型导出额外封装了一层，参考代码如下：</p><pre><code class="Plain">from horizon_plugin_pytorch.quantization.hbdk4 import export

def export(
    model: nn.Module,
    example_inputs: Any,
    name: str = "forward",
    input_names: Optional[Any] = None,    # 建议在模型导出时就配置好输入输出节点名称
    output_names: Optional[Any] = None,
    input_descs: Optional[Any] = None,
    output_descs: Optional[Any] = None,
    native_pytree: bool = True,
) -&gt; Module</code></pre><h3>6.3 模型修改</h3><p>编译器支持在 hbir 上进行多 batch 拆分、插入数据前处理、算子删除、调整输入输出 layout 等修改操作，其主要应用场景如下：</p><p><strong>多 batch 拆分</strong>：典型场景是 BEV 模型在部署时，多 V 输入来源于不同的摄像头，其数据在内存中独立存储，因此模型可将其多 V 输入沿 batch 维度做拆分；</p><p><strong>数据前处理</strong>：征程 6 工具链支持在模型前端插入一个前处理节点，以实现颜色空间转换（如 NV12—&gt; BGR）、数据归一化（<code>(data-mean)/std</code>），和 Resizer 功能（从大图上抠图 + Resize），并可由 BPU 进行加速；</p><p><strong>算子删除</strong>：征程 6 工具链支持将模型首尾部的 Quantize、Dequantize、Cast、Reshape、Transpose 等算子删除，以适配更加灵活的部署方案；</p><p><strong>调整输入输出 layout</strong>：模型首尾部除了支持删除 Reshape、Transpose 节点外，还支持插入 Transpose 节点，用户可灵活调整其 layout 排布。</p><p>以下参考代码对一个多输入模型实现了多 batch 输入拆分、图像输入的色彩空间转换、数据归一化、Resizer 功能：</p><pre><code class="Plain">from hbdk4.compiler import load, convert

qat_bc = load("qat.bc")  
func = qat_bc[0]
batch_input = ["input_name1"]   # 需要使用独立地址方式部署的输入节点名称列表
resizer_input = ["resize"]      # 部署时数据来源于resizer的输入节点名称列表
pyramid_input = ["pym"]         # 部署时数据来源于pyramid的输入节点名称列表

def channge_source(input, source):
    node = input.insert_transpose(permutes=[0, 3, 1, 2])
    node = node.insert_image_preprocess(mode="yuvbt601full2bgr", divisor=1, mean=[128, 128, 128], std=[128, 128, 128])
    if source == "pyramid":
        node.insert_image_convert("nv12")          
    elif source == "resizer":
        node.insert_roi_resize("nv12")

for input in func.inputs[::-1]:
    if input.name in batch_input:
        origin_name = input.name
        split_inputs = input.insert_split(dim=0)
        for split_input in reversed(split_inputs):
            if origin_name in pyramid_input:
                channge_source(split_input, "pyramid")
            elif origin_name in resizer_input:
                channge_source(split_input, "resizer")</code></pre><h3>6.4 静态 perf</h3><p>对于编译好的 hbm，编译器支持在 X86 端对其 BPU 部分进行静态性能预估，执行以下命令即可生成一个 html 文件，包含模型预估性能、带宽、内存占用、BPU 内部单帧执行时序图等信息。</p><pre><code class="Plain">from hbdk4.compiler import hbm_perf
hbm_perf(model="xxx.hbm", output_dir="./")</code></pre><h2>7.浮点能力使用</h2><h3>7.1 TAE 张量输出，VAE 向量计算支持浮点</h3><p>征程 6B BPU 的 TAE 张量计算单元支持 FP16/FP32 输出，VAE 向量计算单元支持 FP16 计算。但在实际部署中仍需综合评估后再使用，具体原因如下：</p><p><strong>精度</strong>：FP16 并非在所有情况下都优于 INT16，通常情况下数值范围小时 FP16 更优，数值范围大时 INT16 更优，但也需要考虑数值较小或较大部分的误差对模型输出的影响程度。所以量化精度是否使用 FP16，更建议基于精度 Debug 的分析结果来确定；</p><p><strong>性能</strong>：除 Reduce 性能为 INT16 的 1/2 外，其他算子的 FP16 性能和 INT16 性能持平</p><p><strong>额外开销</strong>：虽然 FP16 算子本身无需量化，但上下游算子如果涉及 FP16 &lt;—&gt; INT16 的数据转换，则会引入量化/反量化：</p><p>虽然该节点可以由 VAE 硬件直接支持，但相比于全 INT16 直接串接仍会有额外的性能、带宽开销；</p><p>新引入的量化或反量化节点的 Scale 需要重新校准/QAT 训练得到。</p><h3>7.2 无 VPU 加速量化/反量化</h3><p>相比于 征程 6EM 计算平台，征程 6B 无 VPU 硬件加速模型首尾部的量化/反量化节点，但支持 Gemm 类算子直接 FP32 输出，其他 INT8/INT16 输出节点的反量化，则更建议使用编译器接口将其从 quantized.bc 上移除，并参考该篇文章（&lt;u&gt;<a href="https://link.segmentfault.com/?enc=B3aAtJ2TkpIc2kih9xw01g%3D%3D.WpS6fS%2Fk3bGSa4kpqy85rr0EkjmAXdePvIj6znO86G4poU1qKcqWYX6f3Jb8LH6Z" rel="nofollow" target="_blank">反量化节点的融合实现</a>&lt;/u&gt;）将其融合进前后处理代码中，以减少一次数据遍历的冗余开销。</p><pre><code class="Plain">from hbdk4.compiler import load

quantized_bc = load("quantized.bc")
quantized_bc[0].remove_io_op(op_types=["Quantize", "Dequantize"])</code></pre><h3>7.3 无 SPU，标量计算能力有限</h3><p>相比于 征程 6 其他计算平台，征程 6B 的标量计算单元算力减少，因此 TopK 等标量算子的部署性能需要综合评估后选择合适方案。</p><h2>8.模型板端部署</h2><h3>8.1 UCP 简介</h3><p>UCP（Unify Compute Platform，统一计算平台）定义了一套统一的异构编程接口， 将 SOC 上的功能硬件抽象出来并进行封装，对外提供基于功能的 API 进行调用。UCP 提供的具体功能包括：​<strong>视觉处理</strong>​（Vision Process）、​<strong>神经网络模型推理</strong>​（Neural Network）、​<strong>高性能计算库</strong>​（High Performance Library）、​<strong>自定义算子插件开发</strong>​。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597877" alt="image.png" title="image.png" loading="lazy"/></p><p>UCP 支持的 Backend 如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597878" alt="image.png" title="image.png" loading="lazy"/></p><h3>8.2 快速上手</h3><p>使用 UCP 推理模型的基本代码参考如下，详细信息可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=Oyck87Afn6TWRyDh5krruQ%3D%3D.FTHzKiPF7XSZoQcDOXTZ8hOSyr41UYnLPkhV9iyg3n97rAGIQCxwbDv5EkBqa4K3U1Bb4P2qMJArLCstZfgWUw%3D%3D" rel="nofollow" target="_blank">统一计算平台-模型推理开发</a>&lt;/u&gt;》、《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=oO5YpIRxs%2FhpP4ARMAt79Q%3D%3D.BvEkmQLJR4iWt%2BOOES7odTucWRIcN6i%2BOu6YXf469BnWZ0Dm%2Bj5kv%2BMrCPwXidK4SppRZ9fjchb3%2Bxyere2jWC36jg5ZIsA4UvcxnG4T9UyGQlsQyPxDgpH4HsLQPXD05SEpCzPcMmgbHbWCBJmHSr%2FkopxQ%2BS%2B0WrnykVf1D02bDX9inXGjqHMDBuhbVbuS" rel="nofollow" target="_blank">模型部署实践指导-模型部署实践指导实例</a>&lt;/u&gt;》、《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=0MbpThLXXDuTg2wsAqSuzw%3D%3D.1kppz%2BTVrNTNZQa2FsSW3honFey9rJMd3O5i9jvFuJeyHKcJtzyykByDapv9KvzBSlc9W8mNMwKdWAtgrt8Jm%2FQTpY0BdWgjSdD1x3mvNHQ%3D" rel="nofollow" target="_blank">UCP 通用 API 介绍</a>&lt;/u&gt;》等相关章节。</p><pre><code class="Plain">// 1. 加载模型并获取模型名称列表以及Handle
{
    hbDNNInitializeFromFiles(&amp;packed_dnn_handle, &amp;modelFileName, 1);
    hbDNNGetModelNameList(&amp;model_name_list, &amp;model_count, packed_dnn_handle);
    hbDNNGetModelHandle(&amp;dnn_handle, packed_dnn_handle, model_name_list[0]);
}

// 2. 根据模型的输入输出信息准备张量
std::vector&lt;hbDNNTensor&gt; input_tensors;
std::vector&lt;hbDNNTensor&gt; output_tensors;
int input_count = 0;
int output_count = 0;
{
    hbDNNGetInputCount(&amp;input_count, dnn_handle);
    hbDNNGetOutputCount(&amp;output_count, dnn_handle);
    input_tensors.resize(input_count);
    output_tensors.resize(output_count);
    prepare_tensor(input_tensors.data(), output_tensors.data(), dnn_handle);
}

// 3. 准备输入数据并填入对应的张量中
{
    read_data_2_tensor(input_data, input_tensors);
    // 确保更新输入后进行Flush操作以确保BPU使用正确的数据
    for (int i = 0; i &lt; input_count; i++) {
      hbUCPMemFlush(&amp;input_tensors[i].sysMem, HB_SYS_MEM_CACHE_CLEAN);
    }
}

// 4. 创建任务并进行推理
{
    // 创建任务
    hbDNNInferV2(&amp;task_handle, output_tensors.data(), input_tensors.data(), dnn_handle)
    
    // 提交任务
    hbUCPSchedParam sched_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sched_param);
    sched_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &amp;sched_param);
    
    // 等待任务完成
    hbUCPWaitTaskDone(task_handle, 0);
}

// 5. 处理输出数据
{
    // 确保处理输出前进行Flush操作以确保读取的不是缓存中的脏数据
    for (int i = 0; i &lt; output_count; i++) {
      hbUCPMemFlush(&amp;output_tensors[i].sysMem, HB_SYS_MEM_CACHE_INVALIDATE);
    }
    // 对输出进行后处理操作
}

// 6. 释放资源
{
    // 释放任务
    hbUCPReleaseTask(task_handle);
    // 释放输入内存
    for (int i = 0; i &lt; input_count; i++) {
      hbUCPFree(&amp;(input_tensors[i].sysMem));
    }
    // 释放输出内存
    for (int i = 0; i &lt; output_count; i++) {
      hbUCPFree(&amp;(output_tensors[i].sysMem));
    }
    // 释放模型
    hbDNNRelease(packed_dnn_handle);
}</code></pre><h4>8.2.1 hrt\_model\_exec 工具</h4><p>为了方便用户快速查看 hbm 和 quantized.bc 的模型信息、进行模型单帧推理和性能评测，征程 6 工具链 UCP 提供了 <code>hrt_model_exec</code> 工具，并支持编译 X86、aarch64（aarch64 仅支持 hbm 推理）两个架构下的可执行程序。</p><p>hrt\_model\_exec 的三种使用方法如下：</p><pre><code class="Plain"># 设置环境变量
# arch代表架构类型，aarch64或x86
arch=aarch64
bin=../$arch/bin/hrt_model_exec
lib=../$arch/lib/
export LD_LIBRARY_PATH=${lib}:${LD_LIBRARY_PATH}

# 获取模型信息
${bin} model_info --model_file=xxx.hbm

# 模型单帧推理
${bin} infer --model_file=xxx.hbm --input_file=xxx.bin

# 模型性能评测-Latency(单线程)
${bin} perf --model_file=xxx.hbm --thread_num 1 --frame_count=1000

# 模型性能评测-FPS(多线程)
${bin} perf --model_file=xxx.hbm --thread_num 8 --frame_count=1000</code></pre><h3>8.3 图像输入动态 shape/stride</h3><p>在 征程 6 芯片的视频通路上，有一块叫 Pyramid 的金字塔硬件处理模块，可提供 Camera 输入图像的缩放及 ROI 抠图能力，其输出为 nv12 类型的图像数据，并可基于共享内存机制直接给到 BPU 进行模型推理。因此在 征程 6 工具链中：</p><ol><li>Pyramid 模型指的是具有 nv12 图像输入的模型；</li><li>Resizer 模型指的是具有 nv12 图像输入和 ROI 输入的模型，编译器支持通过 JIT 动态指令的方式，从 nv12 图像上完成 ROI 抠图 + Resize 的功能。</li></ol><p>在 征程 6 工具链中，Pyramid 的输入 stride 为动态，Resizer 模型的 stride 和 shape 都是动态。如下为 mobilenetv1 编译后的模型信息：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597879" alt="image.png" title="image.png" loading="lazy"/><br/>hrt_model_exec model_info 板端可执行程序工具</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597880" alt="image.png" title="image.png" loading="lazy"/></p><p>其中，-1 为占位符，表示为动态，Pyramid 输入的 stride 为动态；Resizer 输入的 H、W、stride 均为动态。</p><ul><li>Resizer 输入的 ​<strong>HW 动态</strong>​，是因为原始输入的大小可以是任意的；</li><li>Pyramid/Resizer 输入的​<strong>​ stride 动态</strong>​，可以理解为是支持 ​<strong>Crop 功能</strong>​，详细内容可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=XLtZ7EpOLWo78nbl%2BwO7Mw%3D%3D.YWohw6jSdXyw8pH38Mix1elcyQUB6JYBa6QIUXDXsQYD9Kpnr319umnADVQ%2FjEfvO8DicAmB33Kk68v%2FAZ19fsr6y3mzAmn0aeNj6W1HvMk%3D" rel="nofollow" target="_blank">统一计算平台-模型推理开发-基础示例包使用说明-advanced\_samples-crop</a>&lt;/u&gt;》</li></ul><h3>8.4 非图像 tensor 内存对齐</h3><p>对于非图像 tensor，征程 6B 要求 128 对齐，征程 6EM 要求内存 64 对齐，征程 6PH 要求 256 对齐。如上图所示，模型输出节点虽然 stride[0] 为 4000，但需要申请的 BPU 内存大小（aligned byte size）为 4096，即为 128 对齐的结果。</p><p>在模型实际部署中，非图像输入/输出节点所需申请的内存大小（ aligned byte size）均可从模型节点属性的结构体中读取到（<code>hbDNNTensorProperties</code>），因此无需特别关注。</p><h3>8.5 图像 tensor 跨距对齐</h3><p>征程 6EMB 对于 Pyramid/Resizer 模型的图像输入，要求 W32 对齐，征程 6PH 要求 W64 对齐。若您有 征程 6 不同架构平台迁移的场景，请注意跨距对齐要求的差异。</p><p>部署代码建议您避免 hard code，推荐基于模型节点属性中的 validShape（张量有效内容尺寸）和 stride（张量各维度步长）进行解析和使用。</p><h4>8.5.1 Pyramid 输入</h4><p>Pyramid 输入 tensor 准备的参考代码如下：</p><pre><code class="Plain">hbDNNTensor *input = input_tensor;
for (int i = 0; i &lt; input_count; i++) {
HB_CHECK_SUCCESS(
    hbDNNGetInputTensorProperties(&amp;input[i].properties, dnn_handle, i),
    "hbDNNGetInputTensorProperties failed");
    
    auto dim_len = input[i].properties.validShape.numDimensions;    // 获取维度信息
    for (int32_t dim_i = dim_len - 1; dim_i &gt;= 0; --dim_i) {
      if (input[i].properties.stride[dim_i] == -1) {                // stride=-1即为动态
        auto cur_stride =                                           // 计算当前维度stride
            input[i].properties.stride[dim_i + 1] *
            input[i].properties.validShape.dimensionSize[dim_i + 1];
        input[i].properties.stride[dim_i] = ALIGN_32(cur_stride);   // 32对齐
      }
    }

    int input_memSize = input[i].properties.stride[0] *             // 计算内存大小
                        input[i].properties.validShape.dimensionSize[0];
    HB_CHECK_SUCCESS(hbUCPMallocCached(&amp;input[i].sysMem[0], input_memSize, 0),
                     "hbUCPMallocCached failed");
    
    const char *input_name;
    HB_CHECK_SUCCESS(hbDNNGetInputName(&amp;input_name, dnn_handle, i),    // 获取节点名称
                     "hbDNNGetInputName failed");
}</code></pre><p>示例：</p><pre><code class="Plain">ucp_tutorial/dnn/basic_samples/code/00_quick_start/resnet_nv12/src/main.cc</code></pre><p>​<strong>注意</strong>​：</p><p>视频通路上的金字塔硬件，其输出层支持配置 y、uv 的 stride，但仅要求 W16 对齐，若数据需要喂给 BPU 推理模型，建议直接按 BPU 的跨距对齐要求来配置。金字塔硬件的更多信息请参考系统软件用户手册。</p><h4>8.5.2 Resizer 输入</h4><p>Resizer 输入的 H、W 也是动态的，因此需要设置为原图尺寸，并计算好 W32 对齐的 Stride；ROI 作为模型输入节点，也需要对其进行赋值。以下为参考代码：</p><pre><code class="Plain">#define ALIGN(value, alignment) (((value) + ((alignment)-1)) &amp; ~((alignment)-1))
#define ALIGN_32(value) ALIGN(value, 32)

int prepare_image_tensor(const std::vector&lt;hbUCPSysMem&gt; &amp;image_mem, int input_h,
                         int input_w, hbDNNHandle_t dnn_handle,
                         std::vector&lt;hbDNNTensor&gt; &amp;input_tensor) {
  // 准备Y、UV输入tensor
  for (int i = 0; i &lt; 2; i++) {
    HB_CHECK_SUCCESS(hbDNNGetInputTensorProperties(&amp;input_tensor[i].properties,
                                                   dnn_handle, i),
                     "hbDNNGetInputTensorProperties failed");
    // auto w_stride = ALIGN_32(input_w);
    // int32_t y_mem_size = input_h * w_stride;
    input_tensor[i].sysMem[0] = image_mem[i];

    // 配置原图大小，NHWC
    input_tensor[i].properties.validShape.dimensionSize[1] = input_h;
    input_tensor[i].properties.validShape.dimensionSize[2] = input_w;
    if (i == 1) {
      // UV输入大小为Y的1/2
      input_tensor[i].properties.validShape.dimensionSize[1] /= 2;
      input_tensor[i].properties.validShape.dimensionSize[2] /= 2;
    }

    // stride满足32对齐
    input_tensor[i].properties.stride[1] =
        ALIGN_32(input_tensor[i].properties.stride[2] *
                 input_tensor[i].properties.validShape.dimensionSize[2]);
    input_tensor[i].properties.stride[0] =
        input_tensor[i].properties.stride[1] *
        input_tensor[i].properties.validShape.dimensionSize[1];
  }
  return 0;
}

// 准备roi输入tensor
int prepare_roi_tensor(const hbUCPSysMem *roi_mem, hbDNNHandle_t dnn_handle,
                       int32_t roi_tensor_id, hbDNNTensor *roi_tensor) {
  HB_CHECK_SUCCESS(hbDNNGetInputTensorProperties(&amp;roi_tensor-&gt;properties,
                                                 dnn_handle, roi_tensor_id),
                   "hbDNNGetInputTensorProperties failed");
  roi_tensor-&gt;sysMem[0] = *roi_mem;
  return 0;
}

int prepare_roi_mem(const std::vector&lt;hbDNNRoi&gt; &amp;rois,
                    std::vector&lt;hbUCPSysMem&gt; &amp;roi_mem) {
  auto roi_size = rois.size();
  roi_mem.resize(roi_size);
  for (auto i = 0; i &lt; roi_size; ++i) {
    int32_t mem_size = 4 * sizeof(int32_t);
    HB_CHECK_SUCCESS(hbUCPMallocCached(&amp;roi_mem[i], mem_size, 0),
                     "hbUCPMallocCached failed");
    int32_t *roi_data = reinterpret_cast&lt;int32_t *&gt;(roi_mem[i].virAddr);
    roi_data[0] = rois[i].left;
    roi_data[1] = rois[i].top;
    roi_data[2] = rois[i].right;
    roi_data[3] = rois[i].bottom;
    hbUCPMemFlush(&amp;roi_mem[i], HB_SYS_MEM_CACHE_CLEAN);
  }
  return 0;
}</code></pre><p>示例：</p><pre><code class="Plain">ucp_tutorial/dnn/basic_samples/code/02_advanced_samples/roi_infer/src/roi_infer.cc</code></pre><h3>8.6 小模型批量处理功能</h3><p>由于 BPU 是资源独占式硬件，所以对于 Latency 很小的模型而言，其框架调度开销占比会相对较大。在 征程 6 平台，UCP 支持通过复用 task\_handle 的方式，将多个小模型任务一次性下发，全部执行完成后再一次性返回，从而可将 N 次框架调度开销合并为 1 次，以下为参考代码：</p><pre><code class="Plain">// 获取模型指针并存储
std::vector&lt;hbDNNHandle_t&gt; model_handles;

// 准备各个模型的输入输出，准备过程省略
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; inputs;
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; outputs;

// 创建任务并进行推理
{
    // 创建并添加任务，复用task_handle
    hbUCPTaskHandle_t task_handle{nullptr};
    for(size_t task_id{0U}; task_id &lt; inputs.size(); task_id++){
        hbDNNInferV2(&amp;task_handle, outputs[task_id].data(), inputs[task_id].data(), model_handles[i]);
    }
    
    // 提交任务
    hbUCPSchedParam sche_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sche_param);
    sche_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &amp;sche_param);
    
    // 等待任务完成
    hbUCPWaitTaskDone(task_handle, 0);
}</code></pre><h3>8.7 优先级调度/抢占</h3><p>UCP 支持任务优先级调度和抢占，可通过 <code>hbUCPSchedParam</code> 结构体进行配置，其中：</p><ul><li><code>priority</code> &gt; <code>customId</code> &gt; submit\_time（任务提交时间）</li><li><code>priority</code> 支持 [0， 255]，对于模型任务而言：</li></ul><p>[0， 253] 为普通优先级，不可抢占其他任务，但在未执行时支持按优先级进行排队</p><p>254 为 high 抢占任务，可支持抢占普通任务</p><p>255 为 urgent 抢占任务，可抢占普通任务和 high 抢占任务</p><p>可被中断抢占的低优任务，需要在模型编译阶段配置 <code>max_time_per_fc</code> 参数拆分模型指令</p><ul><li>其他 backend 任务，priority 支持 [0， 255]，但不支持抢占，可以认为都是普通优先级</li></ul><h3>8.8 X86 仿真</h3><p>征程 6 工具链在 X86 端支持 hbm 指令仿真，但效率非常低，所以更推荐使用 quantized.bc 模型进行推理，其定点部分和 hbm 数值二进制一致，浮点部分可能存在架构本身差异，但通常对精度影响可忽略不计。</p><p>征程 6B 平台 X86 仿真需要配置如下环境变量，默认架构为"nash-m"：</p><p>export HB\_UCP\_SIM\_PLATFORM\_TYPE=nash-b</p><h4>8.8.1 推理 quantized.bc</h4><p><strong>Python 推理：</strong></p><p>quantized.bc 在 X86 端的推理，可以使用 <code>horizon_tc_ui</code> 包封装的 <code>HBRuntime</code> 接口，具体可见用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=pE%2BwfYJuR%2FUIov1%2FjaNPKg%3D%3D.HhNJEasV%2FRWH0uaMVYegXiZOMCaf9yomOGifxgtaJeU6fwbbqiD3DUgVPb8saIbWj03z8z8j39O2svB7jfaQrg%3D%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换工具-HBRuntime 推理库</a>&lt;/u&gt;》，参考代码如下：</p><pre><code class="Plain">import numpy as np
from horizon_tc_ui.hb_runtime import HBRuntime

sess = HBRuntime("quantized.bc")
input_names = sess.input_names
output_names = sess.output_names

data1 = np.load("input1.npy")
data2 = np.load("input2.npy")
input_feed = {input_names[0]: data1, input_names[1]: data2}

output = sess.run(output_names, input_feed)</code></pre><p>quantized.bc 也可以直接调用其 func 的 feed 接口进行推理，其输入格式也为 dict，value 支持 torch.tensor 和 np.array 两种类型，输出格式与输入格式保持一致。参考代码如下：</p><pre><code class="Plain">import numpy as np
from hbdk4.compiler import load

hbir = load("quantized.bc")
func = hbir[0]

data1 = np.load("input1.npy")
data2 = np.load("input2.npy") 
input_feed = {inputs[0].name: data1, inputs[1].name: data2}
hbir_outputs = func.feed(input_feed)</code></pre><p><strong>C++ 推理：</strong></p><p>quantized.bc 的 C++ 推理接口复用 hbm UCP 推理接口，仅 so 动态库需要替换成 X86 版本即可。 您也可以在 X86 端使用 <code>hrt_model_exec</code> 工具对 quantized.bc 进行模型信息查看和单帧推理。</p><h4>8.8.2 推理 hbm</h4><p>由于 X86 端 hbm 推理为指令仿真，运行速度非常慢，因此工具链提供了 <code>hbm_infer</code> 工具以便用户在服务器端给直连的开发板下发推理任务。本文只介绍最简单的单进程使用方式，多进程、多阶段模型输入输出的传输优化，以及统计模型推理、网络传输耗时等功能请参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=D1CexzTSFPBF0ZlvT5cbaA%3D%3D.57ZGrodXOKN1t9ihCjnNcjykDQJd8CwtIJk6FuVMl0oRsfnLv21EubIJlSMzc2iCjugsY%2FOSEW08udUt4k2Ic6s8o8a5o1%2FfL3azMjrcVBo%3D" rel="nofollow" target="_blank">hbm\_infer 工具介绍</a>&lt;/u&gt;》。</p><pre><code class="Plain"># hbm也可传入一个list，推理时通过指定model_name来选择推理哪个模型，推理所用的.so即可只传输一次
hbm_model = HbmRpcSession(
    host="xx.xx.xx.xx",
    local_hbm_path="xx.hbm", 
)

# 打印模型输入输出信息
hbm_model.show_input_output_info()

# 准备输入数据
input_data = {
    'img': torch.ones((1, 3, 224, 224), dtype=torch.int8)
}

# 执行推理并返回结果
# 若传入的是list，需要正确指定model_name
# output_data = hbm_model(input_data, model_name=model_name)
output_data = hbm_model(input_data) 

print([output_data[k].shape for k in output_data])

# 关闭server
hbm_model.close_server()</code></pre><h2>9.UCP 视觉处理/高性能算子</h2><p>除模型推理外，UCP 还提供了视觉处理和高性能算子两大方向的多种算子接口，可支持诸如 Remap、Jpeg、H264/265、FFT/IFF 等功能，这些算子底层是基于地平线 SOC 上不同硬件 IP 进行的封装，并提供统一的调用接口。</p><p>更多信息可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=SVynj3M0b6%2F0Y82YmyLqRg%3D%3D.go3AjKdIb4Pvnh4K3Mty7EBEWODyuGANa4ZwsBrfUydp7na4sFNBXQUOvcMwLKdngcWNBjnSfy%2BrTas2kAH0vA%3D%3D" rel="nofollow" target="_blank">统一计算平台</a>&lt;/u&gt;》的相关章节。</p><p><strong>注意：</strong></p><ol><li>板端实际部署时，ISP 到 Pyramid 的视频通路不建议使用 UCP，无法实现数据 Online，建议直接调用底软接口进行功能实现。</li><li>基于 DSP Backend 实现的 HPL 算子，在 征程 6B 平台仅会提供 征程 6EM 上 Q8 实现的源码，如需在 征程 6B 上使用 HPL 算子，需要您自行适配 V130 并编译镜像。VP 算子和 SLAM 等 征程 6EM 上已有的 Q8 DSP sample，将在 征程 6B 后续正式版本中提供 V130 版本。</li></ol><h2>10.UCP 自定义算子（DSP）</h2><p>为了简化用户开发，UCP 封装了一套基于 RPC 的开发框架，来实现 CPU 对 DSP 的功能调用，但具体 DSP 算子实现仍是调用 Cadence 接口去做开发。总体来说可分为三个步骤：</p><p>使用 Cadence 提供的工具及资料完成算子开发；</p><pre><code class="Plain">int test_custom_op(void *input, void *output, void *tm) {
  // custom impl
  return 0;
}</code></pre><p>DSP 侧通过 UCP 提供的 API 注册算子，编译带自定义算子的镜像；</p><pre><code class="Plain">// dsp镜像中注册自定义算子
hb_dsp_register_fn(cmd, test_custom_op, latency)</code></pre><p>ARM 侧通过 UCP 提供的算子调用接口，完成开发板上的部署使用。</p><pre><code class="Plain">// 将输入输出的hbUCPSysMem映射为DSP可访问的内存地址
hbUCPSysMem in;
hbUCPMalloc(&amp;in, in_size, 0)
hbDSPAddrMap(&amp;in, &amp;in)

hbUCPSysMem out;
hbUCPMalloc(&amp;out, out_size, 0)
hbDSPAddrMap(&amp;out, &amp;out)

// 创建并提交DSP任务
hbUCPTaskHandle_t taskHandle{nullptr};
hbDSPRpcV2(&amp;taskHandle, &amp;in, &amp;out, cmd)

hbUCPSchedParam ctrl_param;
HB_UCP_INITIALIZE_SCHED_PARAM(&amp;ctrl_param);
ctrl_param.backend = HB_UCP_DSP_CORE_ANY;
hbUCPSubmitTask(task_handle, &amp;ctrl_param);

// 等待任务完成
hbUCPWaitTaskDone(task_handle, 0);</code></pre><p>更多信息可见用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=51nOtKbb0lKnHalHfNP0pQ%3D%3D.vOVzQJ6psmj%2BpeF%2Fex%2BIAuf3BenfbbsiaJ8izXKHdMrpYYQOM6LKRCCgqFkf15GRTPiU27%2FiFK1WGWwTKSH9ha0fYn6HXQ4lLwo9W3E8ELQ%3D" rel="nofollow" target="_blank">统一计算平台-自定义算子-DSP 算子开发</a>&lt;/u&gt;》。</p><h2>11.性能监测工具</h2><p>征程 6 平台 BPU、DSP 都是独占的硬件资源，任务一旦提交就会独占推理，UCP 侧仅能通过 <code>hrt_ucp_monitor</code> 工具去监测其硬件占用率（采样频率支持配置 [10， 1000]，默认 500），并且能查看到 DDR 内存占用情况。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597881" alt="image.png" title="image.png" loading="lazy"/></p><h2>12.QNX 带来的功能裁剪</h2><ol><li><strong>UCP Service （中继模式）</strong>：<br/>UCP 框架支持两种主要工作模式：直连模式、中继模式。系统默认运行在直连模式下，在中继模式下，UCP 将支持跨进程任务优先级的统一调度。但 QNX 上跨进程调度的模型性能较差，不满足使用需求，故功能移除。</li><li><strong>UCP Trace</strong>： UCP trace 通过在 UCP 执行的关键路径上嵌入 trace 记录，提供深入分析 UCP 应用程序调度逻辑的能力。在出现性能异常时，可以通过分析 UCP trace，快速找到异常发生的时间点。但 QNX 底软不支持 Trace 功能，故移除。</li><li><strong>DEB 部署包</strong>： 用于简化板端部署，可通过自动安装所需的二进制文件和相关依赖库，快速设置并运行 UCP 相关的应用程序，具体包括：<code>ucp_service</code> 和 <code>hrt_ucp_monitor</code>。但鉴于 <code>ucp_service</code> 不支持，该功能也移除。</li></ol>]]></description></item><item>    <title><![CDATA[LLM推理时计算技术详解：四种提升大模型推理能力的方法 本文系转载，阅读原文
https://avo]]></title>    <link>https://segmentfault.com/a/1190000047597914</link>    <guid>https://segmentfault.com/a/1190000047597914</guid>    <pubDate>2026-02-06 21:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年LLM领域有个有意思的趋势：与其继续卷模型训练，不如在推理阶段多花点功夫。这就是所谓的推理时计算（Test-Time / Inference-Time Compute）：在推理阶段投入更多计算资源，包括更多Token、更多尝试、更深入的搜索，但不会改动模型权重。</p><p>ARC-AGI基准测试就是个典型案例。通过推理时技术可以达到87.5%的准确率，但代价是每个任务超过1000美元的推理成本。没用这些技术的LLM通常只能拿到不到25%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597916" alt="" title=""/></p><p>本文要讲四种主流的推理时计算技术：深度方向的Chain-of-Thought，宽度方向的Self-Consistency，搜索方向的Tree-of-Thoughts，以及迭代方向的Reflexion/Self-Refine。</p><h2>预备知识：LLM调用封装</h2><p>先把基础设施搭好。下面是通用的LLM调用接口和辅助函数：</p><pre><code> fromcollectionsimportCounter, deque
importre

# ---- LLM调用封装 ----
defllm(prompt: str, temperature: float=0.7, max_tokens: int=800) -&gt;str:
    """
    LLM调用的占位函数。
    在实际使用中，可以替换为OpenAI、Claude或本地模型的API调用。
    
    参数:
        prompt: 输入提示词
        temperature: 采样温度，控制输出多样性
        max_tokens: 最大生成token数
        
    返回:
        模型生成的文本
    """
    # 示例：使用OpenAI API
    # from openai import OpenAI
    # client = OpenAI()
    # response = client.chat.completions.create(
    #     model="gpt-4",
    #     messages=[{"role": "user", "content": prompt}],
    #     temperature=temperature,
    #     max_tokens=max_tokens
    # )
    # return response.choices[0].message.content
    
    raiseNotImplementedError("请实现你的LLM调用逻辑")

# ---- 辅助函数：提取最终答案 ----
defextract_final_answer(text: str) -&gt;str:
    """
    从模型输出中提取最终答案。
    
    寻找格式为 "FINAL: &lt;答案&gt;" 或 "Final: &lt;答案&gt;" 的模式。
    在实际应用中，建议：
    - 让模型输出JSON格式，如 {"final": "..."}
    - 或使用针对具体任务的解析逻辑
    
    参数:
        text: 模型的完整输出文本
        
    返回:
        提取的最终答案（最多200字符）
    """
    m=re.search(r"(FINAL|Final)\s*[:\-]\s*(.*)", text)
     return (m.group(2).strip() ifmelsetext.strip())[:200]</code></pre><h2>深度（Depth）：链式思维推理</h2><p>Chain-of-Thought（CoT）是最基础也用得最多的推理时技术。核心思想很直白：让模型「思考」久一点。</p><p>传统调用方式期望模型直接给答案，但复杂问题不是这么解决的。CoT让模型生成详细的中间推理步骤，在数学、逻辑推理、编程这些任务上效果很明显。</p><p>为什么管用？首先是分解作用，大问题拆成小步骤，每一步更容易做对。其次是中间步骤充当了一种「外部记忆」，帮模型追踪推理过程。第三是强制模型展示推理，减少直接「猜」答案的情况。最后，模型推理过程中可以自查前面步骤对不对。</p><p>触发CoT有几种常见办法：零样本提示就是加一句「Let's think step by step」；少样本提示是给2-3个带推理步骤的例子；指令微调是用带CoT标注的数据集训练；系统提示则是在system message里定义推理风格。</p><pre><code> defsolve_with_cot(question: str) -&gt;str:
    """
    使用链式思维（Chain-of-Thought）解决问题。
    
    通过精心设计的提示词，引导模型：
    1. 进行逐步推理
    2. 展示中间计算过程
    3. 最后给出明确的最终答案
    
    参数:
        question: 需要解答的问题
        
    返回:
        包含推理过程和最终答案的完整响应
    """
    prompt=f"""You are a careful reasoner. Your task is to solve the following problem.

Instructions:
1. Break down the problem into smaller steps
2. Show your reasoning for each step
3. Double-check your calculations
4. End with a clear final answer

Format your response as:
Step 1: [your first step]
Step 2: [your second step]
...
FINAL: &lt;your final answer&gt;

Question: {question}
"""
    # 使用较低的temperature以获得更确定性的输出
    returnllm(prompt, temperature=0.2, max_tokens=900)

# 使用示例
if__name__=="__main__":
    question="一个农场有鸡和兔，共35个头和94只脚。请问有多少只鸡和多少只兔？"
    result=solve_with_cot(question)
    print(result)
     print("\n提取的最终答案:", extract_final_answer(result))</code></pre><p>CoT适合数学应用题、逻辑推理、代码调试、规划任务这类需要多步计算的问题。简单事实问答用CoT有点浪费，创意写作也不太合适——过度结构化会限制发挥。</p><p>局限性也很明显。Token消耗会上升，输出越长成本越高。模型可能在推理链中犯错，错误还会传播。输出格式也不总是稳定，需要后处理。</p><h2>宽度（Width）：自洽性采样</h2><p>Self-Consistency的想法很简单：与其相信单次输出，不如生成多个答案，选最一致的那个。</p><p>有点像集体决策——单条推理链可能出错，但如果多条独立路径都指向同一答案，那答案八成是对的。</p><p>这方法管用的原因：单次采样可能因为随机性出错，多次采样能平均掉这些错误。正确答案往往能通过多条不同路径得到。不同路径可能捕捉问题的不同侧面。答案的一致性程度还顺便反映了模型的「信心」。</p><p>做Self-Consistency有几个关键决策要做。</p><p>第一是采样多样性。这点至关重要。如果所有采样都走同一条推理路径，自洽性就没意义了。高多样性设置是temperature 0.7-0.9、top_p 0.9-0.95，加上多样的提示词变体。temperature太低或提示词太固定都不行。</p><p>第二是采样数量。3-5个边际收益最高，适合成本敏感场景；10-20个是常规配置；40个以上适合对准确率要求极高的场景，但边际收益已经很低了。</p><p>第三是聚合策略。最常用的是多数投票，选出现次数最多的答案。也可以加权投票，根据置信度加权。还可以把相似答案聚类后再投票。</p><pre><code> defsolve_with_self_consistency(
    question: str, 
    n: int=10,
    temperature: float=0.8
) -&gt;dict:
    """
    使用自洽性（Self-Consistency）方法解决问题。
    
    通过高温度采样生成多个多样化的答案，
    然后通过多数投票选择最一致的答案。
    
    参数:
        question: 需要解答的问题
        n: 采样数量，建议10-20
        temperature: 采样温度，建议0.7-0.9以确保多样性
        
    返回:
        包含以下键的字典:
        - final: 最终答案（得票最多的）
        - votes: 该答案的得票数
        - confidence: 置信度（得票数/总数）
        - all_finals: 所有提取的答案列表
        - vote_distribution: 完整的投票分布
        - samples: 所有原始输出（用于调试）
    """
    prompt_template="""Solve this problem step by step. 
Show your reasoning, then end with 'FINAL: ...'

Question: {question}"""
    
    samples= []
    foriinrange(n):
        out=llm(
            prompt_template.format(question=question),
            temperature=temperature,  # 高温度确保多样性
            max_tokens=900
        )
        samples.append(out)
    
    # 提取所有最终答案
    finals= [extract_final_answer(s) forsinsamples]
    
    # 统计投票
    vote_counter=Counter(finals)
    most_common=vote_counter.most_common()
    winner=most_common[0]
    
    return {
        "final": winner[0],
        "votes": winner[1],
        "confidence": winner[1] /n,
        "all_finals": finals,
        "vote_distribution": dict(vote_counter),
        "samples": samples
    }

defsolve_with_weighted_consistency(
    question: str,
    n: int=10,
    score_fn=None
) -&gt;dict:
    """
    带权重的自洽性方法。
    
    除了多数投票外，还可以根据每个答案的质量分数加权。
    
    参数:
        question: 需要解答的问题
        n: 采样数量
        score_fn: 评分函数，接受(question, answer)返回0-1的分数
        
    返回:
        包含加权投票结果的字典
    """
    samples= []
    for_inrange(n):
        out=llm(
            f"Solve step by step. End with 'FINAL: ...'\n\nQ: {question}",
            temperature=0.8,
            max_tokens=900
        )
        samples.append(out)
    
    finals= [extract_final_answer(s) forsinsamples]
    
    # 加权投票
    weighted_votes= {}
    forfinal, sampleinzip(finals, samples):
        weight=score_fn(question, sample) ifscore_fnelse1.0
        weighted_votes[final] =weighted_votes.get(final, 0) +weight
    
    winner=max(weighted_votes.items(), key=lambdax: x[1])
    
    return {
        "final": winner[0],
        "weighted_score": winner[1],
        "weighted_distribution": weighted_votes,
        "all_finals": finals
    }

# 使用示例
if__name__=="__main__":
    question="如果今天是星期三，那么100天后是星期几？"
    result=solve_with_self_consistency(question, n=10)
    
    print(f"最终答案: {result['final']}")
    print(f"得票数: {result['votes']}/{len(result['all_finals'])}")
    print(f"置信度: {result['confidence']:.1%}")
     print(f"投票分布: {result['vote_distribution']}")</code></pre><p>Self-Consistency适合有确定答案的问题（数学、编程、事实问答）、答案空间有限的问题（选择题、是/否问题）、以及生产环境中需要高可靠性的场景。开放式问题答案空间太大，每次答案都不同，投票没意义。创意任务没有「正确」答案可投票，也不适用。</p><p>局限性：成本线性增长，N次采样就是N倍成本。如果模型系统性地偏向某个错误答案，投票也救不了。同一答案的不同表述可能被当作不同答案，答案标准化是个麻烦事。</p><h2>搜索（Search）：思维树探索</h2><p>Tree-of-Thoughts（ToT）把推理过程当成搜索问题来做。每个节点是一个「思维状态」，也就是部分推理结果；每条边是一个「思维步骤」，即推理动作；目标是找到通向正确答案的路径。</p><p>跟线性的CoT不同，ToT允许分支（从一个状态探索多个可能的下一步）、回溯（放弃没希望的分支，回到之前的状态）、评估（判断当前状态离目标有多近）。</p><p>为什么有效？线性推理一旦犯错就没法恢复，ToT可以回溯。某些问题天然是树形结构，比如博弈、规划。通过评估函数引导搜索，避免盲目探索。只深入探索有希望的分支，Token利用率更高。</p><p>搜索策略有几种选择。BFS广度优先，逐层探索，不会错过浅层解但内存消耗大。DFS深度优先，一条路走到底，内存效率高但可能陷入死胡同。Beam Search每层保留top-k状态，平衡效率和覆盖，但可能丢失最优解。A*用启发式函数引导，最优且高效，但需要好的启发函数。MCTS蒙特卡洛树搜索能处理大搜索空间，但需要大量模拟。</p><pre><code> deftot_bfs(
    question: str, 
    max_depth: int=4, 
    beam: int=3, 
    branch: int=4,
    external_evaluator=None
) -&gt;dict:
    """
    使用BFS策略的思维树（Tree-of-Thoughts）方法。
    
    工作流程:
    1. 从空状态开始
    2. 对当前frontier中的每个状态，生成多个可能的下一步
    3. 评估所有新状态
    4. 保留得分最高的beam个状态作为新frontier
    5. 重复直到达到最大深度
    6. 从最佳状态生成最终答案
    
    参数:
        question: 需要解答的问题
        max_depth: 最大搜索深度
        beam: 每层保留的状态数（beam width）
        branch: 每个状态扩展的分支数
        external_evaluator: 外部评估函数（可选），
                           接受(question, state)返回分数
        
    返回:
        包含以下键的字典:
        - final_text: 最终答案
        - best_state: 最佳推理状态
        - best_score: 最佳状态的分数
        - search_tree: 搜索过程的记录（用于可视化）
    """
    
    defpropose_next_steps(state: str) -&gt;list:
        """
        给定当前推理状态，生成多个可能的下一步。
        """
        prompt=f"""You are exploring different ways to solve a problem.

Question: {question}

Current reasoning state:
{stateifstateelse"(Starting from scratch)"}

Propose {branch} different possible next steps to continue the reasoning.
Each step should be a distinct approach or calculation.
Return as a numbered list:
1. [first possible step]
2. [second possible step]
...
"""
        raw=llm(prompt, temperature=0.9, max_tokens=400)
        
        # 解析编号列表
        steps= []
        forlineinraw.splitlines():
            line=line.strip()
            iflineandline[0].isdigit():
                # 移除编号前缀
                step=line.split(".", 1)[-1].strip()
                ifstep:
                    steps.append(step)
        
        returnsteps[:branch] ifstepselse [raw.strip()]
    
    defllm_score_state(state: str) -&gt;float:
        """
        使用LLM评估一个推理状态的promising程度。
        
        注意：在实际应用中，使用外部评估器（如单元测试、规则检查）
        通常比LLM自我评估更可靠。
        """
        ifexternal_evaluator:
            returnexternal_evaluator(question, state)
        
        prompt=f"""Evaluate how promising this partial solution is.

Question: {question}

Current reasoning state:
{state}

Consider:
1. Is the reasoning logical and correct so far?
2. Is it making progress toward a solution?
3. Are there obvious errors or dead ends?

Rate from 0 to 10 (10 = very promising, likely to lead to correct answer).
Output only a number.
"""
        s=llm(prompt, temperature=0.0, max_tokens=10).strip()
        try:
            returnfloat(re.findall(r"\d+(\.\d+)?", s)[0])
        except:
            return5.0  # 默认中等分数
    
    # 初始化
    frontier= [""]  # 初始状态为空
    best_state=""
    best_score=-1.0
    search_tree= []  # 记录搜索过程
    
    fordepthinrange(max_depth):
        candidates= []
        depth_record= {"depth": depth, "states": []}
        
        forstateinfrontier:
            next_steps=propose_next_steps(state)
            
            forstepinnext_steps:
                # 构建新状态
                new_state= (state+"\n"+step).strip()
                
                # 评估新状态
                score=llm_score_state(new_state)
                candidates.append((score, new_state))
                
                depth_record["states"].append({
                    "state": new_state[:200] +"..."iflen(new_state) &gt;200elsenew_state,
                    "score": score
                })
        
        search_tree.append(depth_record)
        
        # 排序并保留top-k
        candidates.sort(reverse=True, key=lambdax: x[0])
        frontier= [sfor_, sincandidates[:beam]]
        
        # 更新最佳状态
        ifcandidatesandcandidates[0][0] &gt;best_score:
            best_score, best_state=candidates[0]
    
    # 从最佳状态生成最终答案
    final_prompt=f"""Based on the reasoning below, produce the final answer.

Question: {question}

Reasoning:
{best_state}

Provide a clear, concise final answer.
End with: FINAL: &lt;your answer&gt;
"""
    final=llm(final_prompt, temperature=0.2, max_tokens=400)
    
    return {
        "final_text": final,
        "final_answer": extract_final_answer(final),
        "best_state": best_state,
        "best_score": best_score,
        "search_tree": search_tree
    }

deftot_dfs(
    question: str,
    max_depth: int=5,
    branch: int=3,
    threshold: float=3.0
) -&gt;dict:
    """
    使用DFS策略的思维树方法。
    
    通过深度优先搜索探索解决方案空间，
    当某个分支的分数低于阈值时进行剪枝。
    
    参数:
        question: 需要解答的问题
        max_depth: 最大搜索深度
        branch: 每个状态扩展的分支数
        threshold: 剪枝阈值，分数低于此值的分支被放弃
        
    返回:
        包含最终答案和搜索路径的字典
    """
    best_result= {"state": "", "score": -1.0}
    visited_count= [0]  # 使用列表以便在嵌套函数中修改
    
    defpropose_steps(state: str) -&gt;list:
        prompt=f"""Propose {branch} next reasoning steps.

Question: {question}
Current state:
{stateifstateelse"(empty)"}

Return as numbered list."""
        raw=llm(prompt, temperature=0.9, max_tokens=300)
        steps= [l.split(".", 1)[-1].strip() 
                 forlinraw.splitlines() 
                 ifl.strip()[:1].isdigit()]
        returnsteps[:branch] ifstepselse [raw.strip()]
    
    defscore_state(state: str) -&gt;float:
        prompt=f"""Rate this partial solution 0-10.
Question: {question}
State: {state}
Output only a number."""
        s=llm(prompt, temperature=0.0, max_tokens=10).strip()
        try:
            returnfloat(re.findall(r"\d+(\.\d+)?", s)[0])
        except:
            return5.0
    
    defdfs(state: str, depth: int):
        visited_count[0] +=1
        
        ifdepth&gt;=max_depth:
            score=score_state(state)
            ifscore&gt;best_result["score"]:
                best_result["state"] =state
                best_result["score"] =score
            return
        
        forstepinpropose_steps(state):
            new_state= (state+"\n"+step).strip()
            score=score_state(new_state)
            
            # 剪枝：跳过低分分支
            ifscore&lt;threshold:
                continue
            
            ifscore&gt;best_result["score"]:
                best_result["state"] =new_state
                best_result["score"] =score
            
            dfs(new_state, depth+1)
    
    dfs("", 0)
    
    # 生成最终答案
    final=llm(
        f"""Produce final answer based on:
Question: {question}
Reasoning: {best_result['state']}
End with FINAL: ...""",
        temperature=0.2
    )
    
    return {
        "final_text": final,
        "final_answer": extract_final_answer(final),
        "best_state": best_result["state"],
        "best_score": best_result["score"],
        "states_visited": visited_count[0]
    }

# 使用示例
if__name__=="__main__":
    question="使用数字1, 5, 6, 7（每个只能用一次），通过加减乘除得到24。"
    
    result=tot_bfs(question, max_depth=3, beam=2, branch=3)
    
    print("=== BFS Tree-of-Thoughts ===")
    print(f"最佳推理路径:\n{result['best_state']}")
    print(f"\n最佳分数: {result['best_score']}")
     print(f"\n最终答案: {result['final_answer']}")</code></pre><p>ToT适合组合问题（24点游戏、数独）、规划任务、博弈问题（象棋、围棋）、头脑风暴这类需要探索不同方向的场景。答案空间极大时可能需要配合启发式剪枝。简单问题用不着——直接CoT就够了。</p><p>局限性：计算成本高昂，需要大量LLM调用来评估和扩展节点。LLM自评估不太可靠，评估函数质量直接决定效果。实现复杂度比其他几种方法高不少。还有些问题压根没有明显的树形结构，ToT就不太适用。</p><h2>迭代（Iteration）：反思与自我改进</h2><p>Reflexion和Self-Refine用的是经典的「生成-评估-改进」循环：模型先产生初始答案，拿到反馈后修正答案，如此反复直到满意或达到最大轮数。</p><p>人类学习不也是这样吗？很少有事情一次就做对，总是通过反馈不断改进。</p><p>但有个重要的坑要注意：没有可靠外部反馈的「自我纠正」可能适得其反。</p><p>研究表明，模型仅靠自己判断来「自我纠正」时，可能把正确答案改成错误答案，可能对错误判断过度自信，可能在无效修改上浪费Token。</p><p>所以最佳实践是尽量用外部反馈源。代码执行（单元测试、错误信息）和规则检查（格式验证、约束检查）最可靠。工具调用（计算器、搜索引擎）和人类反馈也不错。另一个LLM做交叉验证勉强能用。同一个LLM自评效果最差，缺乏外部参照。</p><pre><code> defself_refine(
    question: str, 
    score_fn, 
    rounds: int=3,
    improvement_threshold: float=0.1
) -&gt;dict:
    """
    使用自我改进（Self-Refine）方法迭代优化答案。
    
    核心流程：生成 -&gt; 评估 -&gt; 根据反馈改进 -&gt; 重复
    
    参数:
        question: 需要解答的问题
        score_fn: 评估函数，签名为:
                  score_fn(answer_text) -&gt; (score: float, feedback: str)
                  - score: 0.0-1.0之间的分数
                  - feedback: 具体的改进建议
                  强烈建议使用外部评估器！
        rounds: 最大改进轮数
        improvement_threshold: 最小改进阈值，低于此值则提前停止
        
    返回:
        包含以下键的字典:
        - final: 最终答案
        - final_score: 最终分数
        - history: 完整的改进历史
        - rounds_used: 实际使用的轮数
    """
    # 生成初始答案
    initial_prompt=f"""Provide a thoughtful answer to this question.
Show your reasoning and end with FINAL: ...

Question: {question}
"""
    answer=llm(initial_prompt, temperature=0.4)
    history= []
    prev_score=-float('inf')
    
    forround_numinrange(rounds):
        # 评估当前答案
        score, feedback=score_fn(answer)
        
        history.append({
            "round": round_num+1,
            "answer": answer,
            "score": score,
            "feedback": feedback
        })
        
        # 检查是否有足够的改进
        ifround_num&gt;0and (score-prev_score) &lt;improvement_threshold:
            # 如果改进不明显，考虑提前停止
            ifscore&gt;=prev_score:
                pass  # 继续，至少没有退步
            else:
                # 退步了，恢复上一个答案
                answer=history[-2]["answer"]
                score=history[-2]["score"]
                break
        
        # 如果分数已经很高，提前停止
        ifscore&gt;=0.95:
            break
        
        prev_score=score
        
        # 根据反馈改进答案
        refine_prompt=f"""Improve your answer based on the feedback below.

Question: {question}

Your current answer:
{answer}

Feedback (score: {score:.2f}/1.00):
{feedback}

Instructions:
1. Keep what is correct in your current answer
2. Fix the issues mentioned in the feedback
3. Make sure not to introduce new errors
4. End with FINAL: ...

Improved answer:
"""
        answer=llm(refine_prompt, temperature=0.3)
    
    # 最终评估
    final_score, final_feedback=score_fn(answer)
    
    return {
        "final": answer,
        "final_answer": extract_final_answer(answer),
        "final_score": final_score,
        "history": history,
        "rounds_used": len(history)
    }

# ---- 示例评估函数 ----

defmake_code_evaluator(test_cases: list):
    """
    创建一个代码评估函数。
    
    参数:
        test_cases: 测试用例列表，每个元素是(input, expected_output)
        
    返回:
        评估函数
    """
    defevaluator(code_answer: str) -&gt;tuple:
        # 提取代码块
        code_match=re.search(r"```python\n(.*?)```", code_answer, re.DOTALL)
        ifnotcode_match:
            return0.0, "No Python code block found. Please wrap your code in ```python ... ```"
        
        code=code_match.group(1)
        
        passed=0
        failed_cases= []
        
        forinp, expectedintest_cases:
            try:
                # 危险：实际应用中应使用沙箱！
                local_vars= {}
                exec(code, {"__builtins__": {}}, local_vars)
                
                # 假设代码定义了solve函数
                if'solve'inlocal_vars:
                    result=local_vars['solve'](inp)
                    ifresult==expected:
                        passed+=1
                    else:
                        failed_cases.append(f"Input: {inp}, Expected: {expected}, Got: {result}")
                else:
                    return0.0, "No 'solve' function found in your code."
                    
            exceptExceptionase:
                failed_cases.append(f"Input: {inp}, Error: {str(e)}")
        
        score=passed/len(test_cases)
        
        iffailed_cases:
            feedback="Failed test cases:\n"+"\n".join(failed_cases[:3])  # 最多显示3个
            iflen(failed_cases) &gt;3:
                feedback+=f"\n... and {len(failed_cases) -3} more failures"
        else:
            feedback="All test cases passed!"
        
        returnscore, feedback
    
    returnevaluator

defmake_math_evaluator(correct_answer):
    """
    创建一个数学答案评估函数。
    
    参数:
        correct_answer: 正确答案
        
    返回:
        评估函数
    """
    defevaluator(answer_text: str) -&gt;tuple:
        extracted=extract_final_answer(answer_text)
        
        # 尝试数值比较
        try:
            extracted_num=float(re.findall(r"-?\d+\.?\d*", extracted)[0])
            correct_num=float(correct_answer)
            
            ifabs(extracted_num-correct_num) &lt;0.01:
                return1.0, "Correct!"
            else:
                return0.0, f"Incorrect. Your answer: {extracted_num}, Expected: {correct_num}"
        except:
            pass
        
        # 字符串比较
        ifextracted.lower().strip() ==str(correct_answer).lower().strip():
            return1.0, "Correct!"
        else:
            return0.0, f"Incorrect. Your answer: {extracted}, Expected: {correct_answer}"
    
    returnevaluator

defmake_llm_evaluator(criteria: str):
    """
    创建一个基于LLM的评估函数（不推荐作为唯一评估源）。
    
    参数:
        criteria: 评估标准描述
        
    返回:
        评估函数
    """
    defevaluator(answer_text: str) -&gt;tuple:
        prompt=f"""Evaluate this answer based on the following criteria:

Criteria: {criteria}

Answer to evaluate:
{answer_text}

Provide:
1. A score from 0.0 to 1.0
2. Specific feedback on what's wrong and how to improve

Format:
SCORE: [number]
FEEDBACK: [your feedback]
"""
        response=llm(prompt, temperature=0.0)
        
        try:
            score=float(re.search(r"SCORE:\s*([\d.]+)", response).group(1))
            score=min(1.0, max(0.0, score))
        except:
            score=0.5
        
        try:
            feedback=re.search(r"FEEDBACK:\s*(.+)", response, re.DOTALL).group(1).strip()
        except:
            feedback=response
        
        returnscore, feedback
    
    returnevaluator

# 使用示例
if__name__=="__main__":
    # 示例1：代码任务
    question="编写一个函数solve(n)，返回n的阶乘。"
    test_cases= [
        (0, 1),
        (1, 1),
        (5, 120),
        (10, 3628800)
    ]
    
    result=self_refine(
        question=question,
        score_fn=make_code_evaluator(test_cases),
        rounds=3
    )
    
    print("=== Self-Refine for Code ===")
    print(f"最终分数: {result['final_score']:.2%}")
    print(f"使用轮数: {result['rounds_used']}")
    print(f"\n改进历史:")
    forhinresult['history']:
        print(f"  Round {h['round']}: score={h['score']:.2f}")
    
    # 示例2：数学任务
    question="计算 17 * 23 + 45 - 12"
    correct=17*23+45-12
    
    result=self_refine(
        question=question,
        score_fn=make_math_evaluator(correct),
        rounds=2
    )
    
    print("\n=== Self-Refine for Math ===")
    print(f"最终答案: {result['final_answer']}")
    print(f"正确答案: {correct}")
     print(f"最终分数: {result['final_score']:.2%}")</code></pre><p>Self-Refine适合代码生成（有单元测试作为外部反馈）、格式化任务（有明确规范可检查）、约束满足问题（可验证约束是否满足）、事实核查（可通过检索验证）。主观任务需要人类反馈或多模型交叉验证。没有反馈来源时别用——纯LLM自评不靠谱。</p><p>局限性：反馈质量决定上限，垃圾反馈只会导致垃圾改进。模型有时候会在不同版本之间来回「改」，出现震荡。每轮迭代都消耗Token，成本会累积。也无法保证收敛——模型可能根本没法利用反馈真正改进。</p><hr/><h2>技术对比与选择指南</h2><p>四种技术各有特点。CoT思考更深，Token消耗低，LLM只调用一次，实现简单，不需要外部反馈，适合推理链问题。Self-Consistency采样更广，Token消耗中等，LLM调用N次，实现也简单，不需要外部反馈，适合有确定答案的问题。ToT探索更多，Token消耗高，LLM调用次数是分支数乘以深度，实现复杂，外部反馈可选但推荐，适合组合和规划问题。Self-Refine改进更好，Token消耗中等，LLM调用次数是轮数乘以2，实现复杂度中等，强烈推荐外部反馈，适合可迭代改进的问题。</p><p>选择思路如下，需要分步推理就先试CoT，不稳定的话加上Self-Consistency。有确定答案且需要可靠性，直接用Self-Consistency。组合或搜索问题用ToT。有外部反馈源就用Self-Refine。不确定用什么就先用CoT，看效果再定。</p><p>这些技术可以组合使用。CoT加SC是每次采样都用CoT然后多数投票。ToT加SC是ToT生成多个最终答案用SC选择。ToT加SR是用SR迭代改进ToT的最佳结果。复杂任务可能需要把多种技术串成流水线。</p><pre><code> defcombined_approach(question: str, score_fn) -&gt;str:
    """
    组合使用多种推理时技术。
    
    流程:
    1. 用ToT探索解决方案空间
    2. 用Self-Consistency从多个ToT结果中选择
    3. 用Self-Refine迭代改进最终答案
    """
    # 第一阶段：ToT探索（运行3次）
    tot_results= []
    for_inrange(3):
        result=tot_bfs(question, max_depth=3, beam=2, branch=3)
        tot_results.append(result['final_answer'])
    
    # 第二阶段：Self-Consistency选择
    vote=Counter(tot_results).most_common(1)[0][0]
    
    # 第三阶段：Self-Refine改进
    final_result=self_refine(
        question=question,
        score_fn=score_fn,
        rounds=2
    )
    
     returnfinal_result['final_answer']</code></pre><h2>实践建议</h2><p>别一上来就用最复杂的技术。推荐的顺序是：先直接提问作为baseline，然后加CoT提示，再加Self-Consistency，最后才考虑ToT或Self-Refine。</p><p>对于Self-Refine和ToT，评估器质量直接决定效果。花时间构建好的评估器比调参更重要。</p><p>推理时技术能大幅提升性能，但成本也会大幅增加。建议设置Token预算上限，记录每个任务的实际消耗，根据任务重要性调整投入。</p><p>部署到生产环境前做A/B测试，找到最佳的性能/成本权衡点。</p><h2>总结</h2><p>推理时计算技术代表了LLM能力释放的新范式。在推理阶段多投入一些计算，同一个模型不重新训练就能有明显提升。本文介绍的四种技术——CoT、Self-Consistency、Tree-of-Thoughts、Self-Refine——各有特点和适用场景。理解原理和局限性，选择合适的技术或组合，是LLM应用开发的关键技能。</p><p>随着这一领域的发展，会有更多创新的推理时技术出现。但核心原则不会变：给模型更多「思考」的空间，让它展示真正的推理能力。</p><p><a href="https://link.segmentfault.com/?enc=mQc7BXpN%2BAj7him%2BDg2S6Q%3D%3D.KStjLVY33kRMEMbU7NaYir%2Bx%2BBDoNO8OkKlu5gHcLNU3ZBF0ALqJWOgOSnwoZDJSSCs5upc0kNiv0OF%2B4HmucQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/2bb5bb4e569a4687a272dc6e9fe6809a</a></p>]]></description></item><item>    <title><![CDATA[数据库索引怎么用才快？亿级数据实测指南 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047597923</link>    <guid>https://segmentfault.com/a/1190000047597923</guid>    <pubDate>2026-02-06 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>很多人在本地开发时可能都会遇到这样的情况。数据少的时候，页面秒开，SQL 怎么写都感觉不到卡顿。可一上线，面对百万级流量，查询直接超时，数据库 CPU 飙升。</p><p>要避免这种开发时候猛如虎，上线操作二百五的尴尬，最好的办法就是在本地造点数据出来测。只有数据量上去了，那些平时隐藏的性能坑才会原形毕露。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSyz" alt="image.png" title="image.png"/></p><h2>先造它一亿条数据</h2><p>如果表里只有几千行数据，全表扫描和走索引几乎没区别，甚至全表扫描更快。要验证索引策略，必须上强度。</p><p>别傻乎乎地写脚本在应用层循环插入，网络开销会慢到怀疑人生。PostgreSQL 自带的 <code>generate_series</code> 是个神器，下面这个函数能在几分钟内帮你造出一亿条模拟的用户操作日志，足够把坑找出来。</p><pre><code class="SQL">-- 创建一个能快速生成大量模拟数据的函数
CREATE OR REPLACE FUNCTION populate_large_table(target_rows BIGINT)
RETURNS VOID AS $$
BEGIN
  -- 批量插入，避免逐行提交的开销
  INSERT INTO user_events (user_id, event_type, created_at)
  SELECT
    (random() * 1000000)::INTEGER, -- 模拟 100万个不同的用户
    CASE (random() * 3)::INTEGER    -- 随机生成操作类型
      WHEN 0 THEN 'login'
      WHEN 1 THEN 'logout'
      WHEN 2 THEN 'purchase'
      ELSE 'view'
    END,
    NOW() - (random() * INTERVAL '365 days') -- 分布在过去一年
  FROM generate_series(1, target_rows);
END;
$$ LANGUAGE plpgsql;

-- 执行生成（注意：这会占用不少磁盘空间，执行时间取决于机器性能）
-- SELECT populate_large_table(100000000);</code></pre><p>当这一亿条数据落盘后，随便跑一个 <code>SELECT * FROM user_events WHERE user_id = 12345</code>，就会发现耗时从毫秒级变成了几秒甚至十几秒。这时候，索引的价值就体现出来了。</p><h2>索引不是越多越好</h2><p>新手最容易犯的错就是给每个字段都加索引。要知道，索引本质上是空间换时间，而且是有代价的。</p><p><strong>读取变快，写入变慢</strong></p><p>每次 <code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code>，数据库不仅要改数据文件，还得同步更新相关的索引树。如果你表上有 5 个索引，插入一行数据就得维护 5 棵树。对于日志、IoT 传感器上报这类写多读少的业务，索引加多了简直灾难</p><p><strong>策略建议：</strong></p><ul><li><strong>高频查询列</strong>：加索引（如外键、时间戳）。</li><li><strong>高频更新列</strong>：慎加索引。</li><li><p><strong>低区分度列</strong>：别加索引。比如“性别”或“状态（0/1）”，数据库扫索引发现要回表一半的数据，通常会直接放弃索引走全表扫描，这索引建了也是白建。</p><p>*</p></li></ul><h2>拒绝盲猜，看执行计划</h2><p>别觉得写了 <code>WHERE user_id = ...</code> 数据库就一定走索引。优化器有时候比我们想象的懒。</p><p>一定要用 <code>EXPLAIN</code>（PostgreSQL/MySQL 通用）来看看数据库到底在干什么：</p><pre><code class="sql">EXPLAIN ANALYZE SELECT * FROM user_events WHERE user_id = 42;</code></pre><ul><li>如果看到 <strong>Index Scan</strong>：恭喜，索引生效了。</li><li><p>如果看到 <strong>Seq Scan</strong>（Sequential Scan）：说明在全表扫描。这时候就要检查是不是数据分布不均，或者查询条件没对上索引。</p><p>*</p></li></ul><h2>几种常用的索引避坑姿势</h2><h3>1. 复合索引：顺序是关键</h3><p>当查询条件包含多个字段时，比如要查“某用户在某天的记录”，单列索引往往不够快。这时候要建复合索引：</p><pre><code class="sql">CREATE INDEX idx_user_date ON user_events(user_id, created_at);</code></pre><p><strong>注意顺序（最左前缀原则）</strong> ：</p><p>这个索引对 <code>WHERE user_id = ?</code> 有效，对 <code>WHERE user_id = ? AND created_at = ?</code> 也有效。</p><p>但如果查询只有 <code>WHERE created_at = ?</code>，这个索引就废了。把最常用的筛选列放在最左边。</p><h3>2. 唯一索引：既是约束也是加速</h3><p>如果业务逻辑要求邮箱或手机号不能重复，直接上唯一索引。它不仅能提升查询速度，还能在数据库层面兜底，防止代码逻辑漏洞导致脏数据写入。</p><pre><code class="sql">CREATE UNIQUE INDEX idx_unique_email ON users(email);</code></pre><h3>3. 针对性索引类型</h3><ul><li><strong>全文索引 (Full-Text)</strong> ：不要用 <code>LIKE '%关键词%'</code> 去搜大段文本，慢得要死。MySQL 和 PG 都有专门的全文索引，支持分词。</li><li><strong>GIN 索引</strong>：PG 特有，专门处理 JSONB 或数组数据。</li><li><p><strong>位图索引 (Bitmap)</strong> ：适合数据仓库场景下，针对“状态”、“标签”这类低基数字段的组合查询（PG 默认常用 B-Tree，特定场景可用 BRIN 或扩展）。</p><p>*</p></li></ul><p>对于新手来说，安装数据库需要很繁琐的步骤。有了 ServBay 就不同了。ServBay能<a href="https://link.segmentfault.com/?enc=sQ5nNtMBUFK%2B1m6KCA1L8Q%3D%3D.LSB%2B%2FHWLOESGxVnpIkJpAitsj19PnLSoZFwHp%2FQZa3%2Fqu7MpIJHFg5%2BeISpWUXlb" rel="nofollow" target="_blank">一键安装数据库</a>。而且它支持<strong>多实例并发运行</strong>。就是说你可以同时启动 MySQL 8.0 和 MariaDB，或者同时跑一个 PostgreSQL 12 和 16。</p><p>这就很方便做数据迁移测试或者性能对比，看看同一条复杂 SQL 在不同版本数据库下的表现差异。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnSyA" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>一键部署</strong>：囊括了 MySQL, PostgreSQL, MongoDB, Redis, MariaDB 等主流数据库，不用到处找安装包或配置 Brew。</li><li><strong>开箱即用</strong>：安装完自动配好环境变量，直接在终端敲 <code>psql</code> 或 <code>mysql</code> 就能连，省去了改配置文件的麻烦。</li><li><strong>不污染系统</strong>：所有服务独立运行，不想用的时候一键停止或卸载，不会在系统里留下垃圾文件。</li></ul><p>其实数据库优化从来就没有什么标准答案，只有取舍。</p><p>是牺牲写入速度换读取速度？还是牺牲磁盘空间换查询时间？这些都得看具体的业务场景，甚至要看你能不能接受数据会有几秒钟的延迟。</p><p>总之，实践是检验真理的唯一标准。自己试试就知道了。</p>]]></description></item><item>    <title><![CDATA[OFDM 的“阿喀琉斯之踵”：当 Sinc 函数的旁瓣，划破了 6G 的速度梦 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047597821</link>    <guid>https://segmentfault.com/a/1190000047597821</guid>    <pubDate>2026-02-06 20:03:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>01. 完美的代价：OFDM 的基因缺陷</h4><p>一切悲剧的根源，早在我们选择 OFDM 的那一刻就注定了。</p><p>为了追求频谱效率的极致，我们在时域上选择了最简单的<strong>“矩形窗” (Rectangular Window)</strong> 来截断信号。</p><p>在信号与系统的课本里，有一个著名的对偶关系：</p><blockquote><strong>时域的矩形 $\leftrightarrow$ 频域的 Sinc 函数</strong></blockquote><p>这就是 OFDM 子载波的真面目——<strong>Sinc 函数</strong> ($\frac{sin x}{x}$)。</p><p>它长得并不像一根完美的针，而是一个带着无数“拖油瓶”的波形：</p><ul><li>​<strong>主瓣</strong>​：高耸入云，承载有用信息。</li><li>​ <strong>旁瓣</strong>​  ：像波纹一样向两边扩散，且衰减极其缓慢（仅按  $ 1/f $  衰减）。</li></ul><p><strong>这里的伏笔在于：</strong></p><p>OFDM 利用正交性，巧妙地让每一个子载波的​<strong>峰值</strong>​，精准地踩在其他所有子载波的<strong>零点 (Zero Crossing)</strong> 上。</p><p>这像是一场千万人的走钢丝表演，只要大家都不动，这个平衡就是完美的。</p><h4>02. 多普勒效应：不仅仅是“平移”</h4><p>当你在 350km/h 的高铁上，或者在 7.6km/s 的卫星下，物理世界开始对这个脆弱的数学平衡下手了。</p><p>大家通常认为多普勒只是​<strong>频率平移</strong>​（$\Delta f$）。</p><p>但在 OFDM 的眼里，这简直就是一场 <strong>“旁瓣的屠杀”</strong> 。</p><p>设想一下，当整个频谱发生微小的偏移（哪怕只是子载波间隔的 ​<strong>5%</strong> ​）：</p><ol><li><strong>零点错位：</strong> 接收机做 FFT 采样时，原本应该采到“0”的地方，现在采到了隔壁子载波的​<strong>旁瓣能量</strong>​。</li><li><strong>全员恶人：</strong> 注意，这不仅是邻居的干扰。由于 Sinc 函数的旁瓣拖得很长，<strong>远处的子载波</strong>也会把能量“泼”过来。</li><li><strong>累积效应：</strong> 成千上万个子载波的干扰叠加在一起，这就形成了恐怖的 ​<strong>ICI（载波间干扰）</strong> ​。</li></ol><p>此时，你的星座图（Constellation）不再是清晰的点，而是变成了一团模糊的云。</p><h4>03. 致命的连锁反应：高阶调制的崩塌</h4><p>为什么这一点点干扰是致命的？</p><p>我们来看一个残酷的工程现实。</p><p>在 5G/6G 时代，为了追求高网速，我们大量使用 ​<strong>高阶调制</strong>​（如 64QAM, 256QAM）。</p><ul><li>​<strong>QPSK</strong>​：像四个大胖子坐沙发，抗干扰能力强，稍微挤挤没事。</li><li>​<strong>256QAM</strong>​：像 256 个瘦子挤在一张纸上。它们对“纯净度”的要求是变态的——通常需要 ​<strong>30dB 以上的信噪比 (SNR)</strong>​。</li></ul><p><strong>ICI 带来的灾难性后果是：</strong></p><p>它在信号内部制造了一个 <strong>“底噪”</strong> 。</p><p>假设多普勒频移导致 ICI 产生的干扰噪声大约在 -20dB 水平。</p><p>这意味着，无论你的基站发射功率多大，你的接收端 <strong>信干噪比 (SINR)</strong> 永远超不过 20dB。</p><p><strong>结局：</strong></p><p>256QAM 解调失败 $\rightarrow$ 退回 64QAM $\rightarrow$ 依然误码 $\rightarrow$ 退回 QPSK。</p><p><strong>网速瞬间从“千兆级”掉回“3G 时代”。</strong></p><p>这就是为什么在高铁上刷视频，明明信号满格，但视频就是卡住不动——<strong>因为调制阶数已经跌到了谷底。</strong></p><h4>04. 传统算法的无力回天：CFO 补偿的局限</h4><p>你可能会问：<em>“我们不是有 CFO（载波频偏）补偿算法吗？把它纠正回来不就行了？”</em></p><p>这在​<strong>单径信道</strong>​（比如外太空视距通信）里也许行得通。</p><p>但在地球表面，问题复杂得多：<strong>多径效应 (Multipath)。</strong></p><ul><li>信号从直射路径过来，频移是 $+1000$ Hz。</li><li>信号撞到大楼反射过来，频移可能变成了 $+500$ Hz。</li><li>信号从身后反射过来，频移可能是 $-200$ Hz。</li></ul><p><strong>这是 OFDM 的死穴：</strong></p><p>你在接收端补偿了 $+1000$ Hz，那个 $+500$ Hz 的信号就变成了 $-500$ Hz 的干扰。</p><p><strong>你按下葫芦浮起瓢。</strong> 在 OFDM 的刚性框架下，你永远无法同时让所有路径的信号都回归正交。</p><h4>05. 结语：必须推倒重来</h4><p>至此，结论已经很悲凉了。</p><p>OFDM 这个统治了 WiFi、4G、5G 二十年的王者，它的基因（Sinc 函数、正交性依赖）决定了它属于“静止或低速”的时代。</p><p>面对 6G 想要征服的 <strong>高超音速</strong> 和 ​<strong>低轨卫星</strong>​，修修补补已经无济于事。</p><p>我们需要一把新的手术刀——一把能在 <strong>“时延-多普勒”</strong> 域上重构信号的手术刀。</p><p>而这，正是 <strong>OTFS</strong> 和 <strong>AFDM</strong> 登场的时刻。</p><blockquote><p>“欢迎关注公众号 <strong>3GPP仿真实验室</strong>！这里是通信算法工程师的加油站。</p><p>我们不搬运新闻，只输出<strong>可运行的代码</strong>和<strong>深度标准解读</strong>。</p><p>👇 <strong>新人见面礼（后台回复关键词获取）：</strong></p><p>回复【LDPC】：获取 5G NR LDPC 编解码 MATLAB 代码（含注释）。<br/>回复【工具】：通信人减负神器：5G NR 帧结构与频点一键生成器（Python+Excel+Web三版）。<br/>回复【Pytorch】：获取 5G NR OFDM 链路 Pytorch 教学代码（含注释），助力人工智能 + 通信</p><p>让我们一起探索 6G 的无限可能。</p></blockquote>]]></description></item><item>    <title><![CDATA[本土vs国际：制造业CRM选型对决，5款产品助力企业精准决策 玩滑板的饺子 ]]></title>    <link>https://segmentfault.com/a/1190000047597830</link>    <guid>https://segmentfault.com/a/1190000047597830</guid>    <pubDate>2026-02-06 20:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业数字化转型的浪潮中，客户关系管理（CRM）系统已成为提升运营效率、优化客户体验、驱动业务增长的核心工具。然而，面对市场上琳琅满目的CRM产品，制造企业往往陷入选择困境：是选用通用型CRM，还是深耕行业的垂直解决方案？</p><p>本文将基于当前市场情况，深入剖析五款制造业垂直领域CRM，分析其核心优势与潜在风险，并为您的选型决策提供可靠建议。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597832" alt="2.jpg" title="2.jpg"/></p><h2>一、制造业CRM选型的特殊考量</h2><p>与零售、金融等行业不同，制造业的CRM需求具有显著特点：</p><ul><li><strong>复杂客户结构</strong>：涉及经销商、终端客户、供应商等多层关系</li><li><strong>销售过程长</strong>：从询价、报价、订单到售后，周期漫长且环节复杂</li><li><strong>产品定制化强</strong>：需要管理产品配置、BOM关联和定制需求</li><li><strong>服务需求高</strong>：设备维护、保修管理、备件配送等售后服务至关重要</li><li><strong>数据集成难</strong>：需与ERP、MES、SCM等生产管理系统深度集成</li></ul><p>这些特点决定了制造业需要的不只是客户信息管理工具，而是能够融入业务流程、连接前后端的行业化解决方案。</p><h2>二、五款制造业垂直领域CRM深度解析</h2><h3>1. 八骏科技CRM（制造业专项版）</h3><p><strong>简介</strong>：  <br/>杭州八骏科技有限公司专注于为制造业提供CRM解决方案，其产品围绕制造业销售与服务场景深度定制，在国内制造业中拥有相当规模的客户基础。</p><p><strong>核心优势</strong>：</p><ul><li><strong>行业贴合度高</strong>：内置制造业特有的项目管理、投标管理、合同履约跟踪模块</li><li><strong>强大的售后服务</strong>：集成设备台账、保修管理、预防性维护、备件库存联动</li><li><strong>可视化流程引擎</strong>：支持复杂审批流程和个性化销售阶段配置</li><li><strong>移动端适配良好</strong>：现场服务人员可实时更新服务状态、上传现场照片</li><li><strong>本土化服务</strong>：提供符合国内商务习惯的发票管理、催款提醒等功能</li></ul><p><strong>风险提示</strong>：</p><ul><li>国际品牌知名度相对较低，跨国企业可能顾虑其全球部署能力</li><li>没有SaaS版，有一定使用门槛</li></ul><h3>2. Salesforce Manufacturing Cloud</h3><p><strong>简介</strong>：  <br/>作为全球CRM领导者，Salesforce针对制造业推出的垂直解决方案，整合了其强大的平台能力和丰富的生态系统。</p><p><strong>核心优势</strong>：</p><ul><li><strong>完整的平台生态</strong>：与ERP、供应链系统的预集成连接器丰富</li><li><strong>AI预测能力</strong>：Einstein AI提供销售预测、客户流失预警等智能洞察</li><li><strong>全球部署能力</strong>：满足跨国制造企业的多地区、多语言、多币种需求</li><li><strong>灵活的扩展性</strong>：低代码平台支持企业根据独特流程自定义应用</li><li><strong>强大的合作伙伴网络</strong>：实施和定制资源丰富</li></ul><p><strong>风险提示</strong>：</p><ul><li>总体拥有成本高，许可费用和定制开发投入较大</li><li>对国内制造业特殊需求（如特殊发票流程）支持需要额外定制</li><li>数据存储在海外云端可能引发部分敏感行业企业的合规担忧</li></ul><h3>3. Microsoft Dynamics 365 for Manufacturing</h3><p><strong>简介</strong>：  <br/>微软将CRM与ERP能力融合的行业解决方案，特别适合已使用微软技术栈的制造企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>与Office 365深度整合</strong>：与Teams、Outlook、Excel无缝协作</li><li><strong>CRM与ERP的无缝衔接</strong>：同一平台下销售、财务、生产数据自然流动</li><li><strong>混合部署灵活性</strong>：支持云端、本地或混合部署，满足不同合规需求</li><li><strong>Power Platform支持</strong>：企业可自主创建扩展应用，降低长期定制成本</li><li><strong>物联网集成能力</strong>：通过Azure IoT Hub连接设备数据，实现预测性维护</li></ul><p><strong>风险提示</strong>：</p><ul><li>完整功能实现需要较复杂的配置和专业实施</li><li>制造业特定功能不如专注该领域的厂商深入</li><li>移动端体验在不同设备上表现不一致</li></ul><h3>4. SAP CRM for Manufacturing</h3><p><strong>简介</strong>：  <br/>SAP作为制造业ERP的巨头，其CRM解决方案天然适合已使用SAP ERP系统的制造企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>与SAP ERP无缝集成</strong>：销售订单直接生成生产计划，服务请求联动物料需求</li><li><strong>行业最佳实践</strong>：凝聚全球领先制造企业的业务流程经验</li><li><strong>端到端流程覆盖</strong>：从潜在客户到现金回收的全流程管理</li><li><strong>强大的分析能力</strong>：基于SAP Analytics Cloud的深度业务洞察</li><li><strong>全球合规支持</strong>：满足不同国家的税务、数据保护等法规要求</li></ul><p><strong>风险提示</strong>：</p><ul><li>实施周期长、成本高，更适合中大型企业</li><li>系统复杂性高，需要专业团队维护和优化</li><li>用户界面操作相对繁琐，培训成本较高</li></ul><h3>5. 用友制造CRM</h3><p><strong>简介</strong>：  <br/>用友作为国内企业服务龙头，其制造CRM解决方案深度结合国内制造业特点，尤其适合国有企业和大型民营企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>本土化程度极高</strong>：完全符合国内财务、税务、商务管理习惯</li><li><strong>与用友ERP无缝对接</strong>：国内用友ERP用户可实现平滑集成</li><li><strong>价格优势明显</strong>：相比国际品牌，总体拥有成本更低</li><li><strong>服务网络广泛</strong>：全国范围内的实施和支持团队</li><li><strong>政府与国企案例丰富</strong>：对特定行业需求理解深刻</li></ul><p><strong>风险提示</strong>：</p><ul><li>国际业务支持能力有限</li><li>产品创新速度相对较慢</li><li>开源性和第三方集成灵活性一般</li></ul><h2>三、制造业CRM选型的五个关键维度</h2><p>基于以上分析，我们建议制造企业从以下五个维度评估CRM厂商：</p><h3>1. 行业匹配度</h3><ul><li>考察产品是否包含制造业特有功能：项目管理、投标管理、设备服务、备件管理、质量投诉处理等</li><li>验证是否有与您企业规模、产品类型（离散制造/流程制造）相似的客户案例</li></ul><h3>2. 系统集成能力</h3><ul><li>评估与现有系统（ERP、MES、PLM等）的集成方式和成本</li><li>考察API开放程度和数据同步实时性</li><li>特别关注服务数据与生产数据的联动能力</li></ul><h3>3. 总拥有成本(TCO)</h3><ul><li>不仅考虑软件许可费用，还要计算实施、定制、培训、维护和升级的长期成本</li><li>评估云服务与本地部署的成本差异及安全合规影响</li></ul><h3>4. 扩展与适应性</h3><ul><li>考察系统能否适应企业业务增长和模式变化</li><li>评估自定义工作流、字段、报表的灵活程度</li><li>考虑移动办公和现场服务的支持能力</li></ul><h3>5. 供应商生态与可持续性</h3><ul><li>评估厂商的行业专注度和财务稳定性</li><li>考察实施伙伴的专业能力和行业经验</li><li>了解产品更新路线图与技术支持响应水平</li></ul><h2>四、给制造业企业的选型建议</h2><p><strong>第一步：明确核心需求优先级</strong>  <br/>制造企业应首先梳理自身最迫切的3-5个业务痛点，例如：</p><ul><li>需要管理复杂项目销售流程？</li><li>亟需改善售后服务响应速度？</li><li>希望销售预测更准确以指导生产计划？</li><li>需要打通销售与生产数据孤岛？</li></ul><p><strong>第二步：划分预算与部署时间线</strong>  <br/>根据企业规模和数字化阶段确定合理预算，并设定可行的上线时间表。中小制造企业可考虑八骏科技、用友等国内厂商的标准化方案；大型集团则需评估SAP、Salesforce等平台的综合能力。</p><p><strong>第三步：要求针对性产品演示</strong>  <br/>不要满足于通用功能展示，要求厂商基于您的实际业务流程进行演示，特别是：</p><ul><li>从询价到订单的完整流程</li><li>设备安装到维护服务的闭环管理</li><li>与现有系统的数据交换场景</li></ul><p><strong>第四步：深入考察客户案例</strong>  <br/>不仅要看成功案例，更要了解类似企业在实施中遇到的挑战及解决方案。如果可能，拜访已上线企业的实际使用部门。</p><p><strong>第五步：从试点开始，分阶段推广</strong>  <br/>选择1-2个业务单元或区域先行试点，验证系统实际效果后再全面推广，降低实施风险。</p><p><strong>第六步：规划长期合作关系</strong>  <br/>CRM实施不是一次性项目，而是持续优化过程。选择那些愿意与企业共同成长、提供持续优化服务的厂商。</p><h2>五、未来趋势：制造业CRM的演进方向</h2><p>随着工业4.0和智能制造的发展，制造业CRM正呈现以下趋势：</p><ol><li><strong>IoT深度集成</strong>：设备运行数据自动触发服务任务，实现预测性维护</li><li><strong>AI驱动洞察</strong>：基于历史数据预测客户需求、设备故障和销售机会</li><li><strong>增强现实(AR)支持</strong>：现场技术人员通过AR眼镜获取设备信息和维修指导</li><li><strong>区块链应用</strong>：供应链可追溯性和智能合约自动化执行</li><li><strong>低代码/无代码配置</strong>：业务人员可自行调整流程，减少IT依赖</li></ol><h2>结语</h2><p>选择制造业CRM是一场战略决策，而非单纯的技术采购。最适合的CRM系统应当像精密的制造设备一样，精准适配您的业务流程，无缝融入您的运营体系，并具备随着业务演进而升级的灵活性。</p><p>国内制造企业特别是中小企业，可重点关注如八骏科技这样深耕行业的本土厂商，它们在性价比、本土化适配和快速响应方面具有独特优势；而跨国企业或大型集团则需综合评估全球部署能力与本地合规要求，在SAP、Salesforce等国际平台与国内解决方案间找到平衡点。</p><p>无论选择哪家厂商，记住：成功的CRM实施=适合的产品+专业的实施+持续的优化+用户的接纳。从这个等式出发，结合企业实际情况，您一定能找到推动制造业数字化转型的那把钥匙。</p>]]></description></item><item>    <title><![CDATA[PhpStudy2018怎么用？完整安装与使用指南（新手必看） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047597848</link>    <guid>https://segmentfault.com/a/1190000047597848</guid>    <pubDate>2026-02-06 20:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​<br/><code>PhpStudy2018</code>是个<strong>集成环境包</strong>，把 Apache、MySQL、PHP 这些搞网站开发的东西打包在一起，装完就能在本机跑 PHP 网站，省得一个个单独装。</p><p>很多学 PHP 或者本地调试网站的人都用它，安装不难，下面用大白话一步步说。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=uxTRyC0bwHktpGkqJzexIQ%3D%3D.%2FfB31Se9Je8XuJNPW97qtuKmByLEOhZRHPCkDj0zqNCEDT20%2FR4pD5r%2BrLezRow2" rel="nofollow" title="https://pan.quark.cn/s/271a927c11d4" target="_blank">https://pan.quark.cn/s/271a927c11d4</a>​</li></ul></li><li><p><strong>用管理员身份运行（推荐）</strong> ​</p><ul><li>右键 <code>PhpStudy2018.exe</code>→ 选“以管理员身份运行”，避免端口占用或权限问题。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>PhpStudy2018.exe</code>运行（如果右键过了就直接双击）。</li><li>第一次打开可能会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>进入安装向导，选语言（一般默认中文）→ 点  <strong>“下一步”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\phpStudy</code>，可点“浏览”改到其他盘，然后点  <strong>“下一步”</strong> 。</li></ul></li><li><p>附加任务：</p><ul><li>可勾“创建桌面快捷方式”，方便以后打开。</li></ul></li><li>点  <strong>“安装”</strong> ​ 开始安装，等进度条走完（几十秒）。</li><li>安装完会提示是否立即启动 → 勾上“立即启动 PhpStudy” → 点  <strong>“完成”</strong> 。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>打开 PhpStudy 后，界面左边是服务开关（Apache、MySQL 等），右边是版本切换。</li><li><p><strong>启动服务</strong>：</p><ul><li>点“启动”按钮，Apache 和 MySQL 同时跑起来（绿灯表示正常）。</li><li>如果端口被占用，会提示，需要改端口或关掉占用的程序。</li></ul></li><li><p><strong>访问本地站点</strong>：</p><ul><li>浏览器输入 <code>http://localhost</code>或 <code>http://127.0.0.1</code>，能看到 PhpStudy 欢迎页就说明 OK。</li></ul></li><li><p><strong>放网站文件</strong>：</p><ul><li>默认网站根目录在 <code>C:\phpStudy\WWW</code>，把自己的 PHP 文件丢进去就能访问。</li></ul></li><li><p><strong>切换 PHP 版本</strong>：</p><ul><li>在右侧版本列表里选需要的 PHP 版本，点“切换”，重启服务生效。</li></ul></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[全模态、多引擎、一体化，阿里云DLF3.0构建Data+AI驱动的智能湖仓平台 阿里云大数据AI ]]></title>    <link>https://segmentfault.com/a/1190000047597863</link>    <guid>https://segmentfault.com/a/1190000047597863</guid>    <pubDate>2026-02-06 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>▌导读</h2><p>在AI时代,数据工程师和AI工程师的协作变得前所未有的重要。阿里云DLF产品负责人李鲁兵在本次分享中,详细介绍了全模态湖仓DLF 3.0的完整能力体系。这个平台不仅支持传统的结构化数据处理,更在业界首次实现了结构化、半结构化、非结构化数据的统一管理和处理,为Data+AI一体化提供了端到端的解决方案。</p><h2>▌全模态平台的核心理念:数据统一、计算按需、工作流驱动</h2><p>全模态湖仓管理平台的设计理念可以概括为四个关键词:数据统一、计算按需、工作流驱动、多方协作。在AI时代,数据工程师负责数据准备和预处理,AI工程师专注于模型训练、推理和召回,两个角色需要在统一的平台上无缝协作,这对平台能力提出了更高的要求。</p><p>数据统一是基础。传统的数据平台往往将结构化数据、半结构化数据、非结构化数据分别存储在不同系统中,造成数据孤岛和管理复杂度的急剧上升。DLF 3.0通过统一的Omni Catalog,实现了对Paimon、Iceberg、Lance、Format Table、Object Table等多种格式的统一管理。无论是传统的数据库表,还是文本、音频、图片、视频等多模态数据,都可以在同一套元数据目录中进行管理,Data工程师和AI工程师可以基于同一份数据进行协作。</p><p>计算按需是核心。不同的应用场景需要不同的计算引擎。实时分析需要Flink和Hologres,离线分析依赖Spark和MaxCompute,全模态检索要用到Milvus和Elasticsearch/OpenSearch,模型训练则需要PAI和Ray。DLF 3.0支持所有这些计算引擎在统一的湖仓之上按需调用,一份数据,多个引擎共享,避免了数据的重复存储和迁移。</p><p>工作流驱动是保障。数据处理和AI应用往往涉及多个步骤:数据摄取、预处理、特征工程、模型训练、推理、召回等。DLF 3.0提供了完整的工作流编排能力,数据工程师和AI工程师可以通过工作流将各个环节串联起来,实现端到端的自动化流程。</p><p>多方协作是目标。通过统一的IDE/Notebook开发环境、Copilot代码辅助、自然语言分析、Agent智能助手等能力,DLF 3.0降低了开发门槛,提升了协作效率。数据工程师和AI工程师可以在同一个平台上使用各自熟悉的工具,同时又能无缝共享数据和成果。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxq" alt="" title=""/></p><h2>▌产品方案大图:从结构化到全模态的能力升级</h2><p>DLF 3.0的产品方案大图清晰地展示了从结构化数据处理到全模态处理的能力演进。在传统的数据处理链路上,我们有CDC/Batch Ingestion(数据摄取)、Stream/Batch ETL(数据加工)、OLAP Analytic(分析查询)三个主要环节,对应的计算引擎包括Flink、Spark、Hologres、StarRocks、MaxCompute等。</p><p>在全模态处理链路上,新增了四个关键环节:数据集管理、数据预处理、数据训练、数据推理、数据检索召回。这些环节分别对应不同的计算引擎:数据预处理可以使用MaxFrame和PySpark,模型训练依赖PAI和Ray,数据检索召回则需要Milvus和Elasticsearch/OpenSearch等搜索引擎。</p><p>两条链路并非孤立存在,而是通过统一的Omni Catalog和DLF元数据服务实现了深度融合。底层的Lakehouse Managed Storage Service提供了统一的存储层,支持Virtual File System、生命周期管理、冷热分层、存储优化等能力。数据缓存加速服务CPFS进一步提升了GPU/CPU的数据读取效率,为AI训练和推理提供了高性能保障。</p><p>DLF作为统一的湖仓管理平台,提供了Rest Catalog Service、Lakehouse SDK(Paimon、Iceberg)、File SDK、权限管理、血缘追踪、监控日志等完整的企业级能力。这种架构设计使得同一份数据可以同时服务于传统的数据分析场景和新兴的AI应用场景,真正实现了Data+AI一体化。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxr" alt="" title="" loading="lazy"/></p><h2>▌Omni Catalog:全模态管理的统一目录</h2><p>在全模态时代,元数据管理面临前所未有的挑战。传统的Catalog只需要管理Tables、Views、Functions等结构化对象,而在AI场景下,还需要管理各种文件(Files)、向量索引、Blob数据等非结构化对象。如果不同类型的数据使用不同的Catalog或System进行管理,就会产生新的数据孤岛,计算引擎需要跨不同的Catalog进行处理,大幅增加了复杂度。</p><p>DLF 3.0推出的Omni Catalog正是为了解决这一问题。它通过一套统一的元数据目录,同时管理Tables和Files,支持Paimon、Iceberg、Lance、Format Table(Parquet、ORC、Avro)、Object Table(Files)等多种格式。计算引擎无论是传统的大数据引擎(Flink、Spark、Hologres)还是新型的AI框架(Ray、PyTorch),都可以通过统一的Rest API、Open API、Paimon SDK、Iceberg SDK、VFS SDK进行数据访问。</p><p>Omni Catalog的核心优势在于降低了数据孤岛的风险。通过统一的目录,数据治理、权限控制、血缘追踪等能力可以覆盖所有类型的数据,而不需要在不同系统间切换。这对于企业级应用尤为重要,因为数据合规、安全审计等需求必须覆盖全域数据,而不能有盲区。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxs" alt="" title="" loading="lazy"/></p><h2>▌DLF提供商业化Paimon全模态存储:统一管理异构数据</h2><p>Paimon作为DLF 3.0的核心表格式,在全模态存储方面进行了深度创新。全模态存储面临三大核心需求:统一管理异构数据的能力、支持结构化和多模态数据的顺序访问(用于大规模批式推理)、提供高性能的标签和向量检索(支持随机访问)。</p><p>Paimon通过Row ID机制实现了对不同列、不同格式数据的统一管理。每一行数据都有一个全局唯一的Row ID,通过Row ID可以关联该行在不同文件格式中的存储位置。对于结构化数据,Paimon使用Parquet Files存储,对于向量数据,可以使用Lance Files或Faiss Vector索引,对于大型Blob数据(图片、音频、视频),则使用Paimon Blob格式。</p><p>在索引构建方面,Paimon提供了多种索引类型。Btree和Bitmap索引用于快速的标量查询,Invert倒排索引支持全文检索,Vector Index则提供高效的向量相似度搜索。这些索引通过Index Manifests进行统一管理,建立了字段与Row ID之间的映射关系。</p><p>在数据访问方面,Paimon通过Data Manifests管理文件组,支持Row Ranges范围扫描。对于顺序访问场景(如模型训练),Paimon可以将多条数据打包成Virtual File Group,提供高吞吐的批量读取。对于随机访问场景(如实时检索),Paimon通过全局索引实现了毫秒级的点查性能。</p><p>通过这种灵活的File Formats组合和统一的Table Format封装,Paimon实现了在一张宽表上承载结构化、半结构化、非结构化所有类型数据的目标,为全模态应用提供了坚实的存储基础。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxt" alt="" title="" loading="lazy"/></p><h2>▌DLF湖表管理与优化:智能化提升性能降低成本</h2><p>DLF 3.0提供了完整的湖表管理和优化能力,通过智能化的方式提升读写性能、降低存储成本。整个优化体系包括四大核心能力:自适应分桶、智能Compaction、快照管理清理、存储服务与冷热分层。</p><p>自适应分桶是一项创新性功能。传统的分桶策略需要用户在建表时指定分桶数,但随着数据量的变化,固定的分桶数可能导致性能问题。DLF 3.0支持根据数据量自适应地调整分桶数(Rescale),用户只需指定分桶Key,平台会自动维护最优的分桶配置,大幅降低了管理负担。</p><p>智能Compaction是性能优化的关键。随着数据的不断写入,湖表会产生大量小文件,影响读取性能。DLF 3.0提供了多种Compaction策略:动态资源模式支持延时优先、资源优先、均衡模式三种策略,平台会根据当前资源状况自动调整Compaction节奏;固定资源模式则允许用户自定义资源配置和参数,实现精细化控制。对于全模态数据,DLF 3.0还支持针对不同文件类型(结构化、半结构化、非结构化)采用不同的Compaction策略,保证整体效率。</p><p>快照管理和清理功能帮助用户有效管理数据生命周期。用户可以基于分区或快照设置自动清理策略,平台还会自动扫描Orphan Files(孤儿文件)并清理,避免存储空间的浪费。同时支持手动触发管理操作,满足特殊场景需求。</p><p>整个优化流程由DLF元数据服务、Event Store事件存储、Paimon存储优化服务协同完成。作业生成引擎、规则优化引擎、智能优化引擎共同组成了智能决策层,作业调度管理则负责在多个计算资源池上高效执行Compaction任务。这种架构设计实现了从元数据到数据文件的全链路优化,用户无需关心底层细节,即可享受高性能和低成本的双重优势。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxu" alt="" title="" loading="lazy"/></p><h2>▌智能冷热分层:大幅降低存储成本</h2><p>存储成本是企业在构建数据湖时的重要考量因素。DLF 3.0提供了智能的冷热分层能力,可以根据数据访问模式自动将数据在不同存储类型间迁移,在保证性能的同时大幅降低成本。</p><p>DLF 3.0支持四种存储类型:标准存储、低频存储、归档存储、冷归档存储。平台会根据数据的最近访问时间和最近更新时间,自动决定数据应该存储在哪个层级。对于频繁访问的热数据,保持在标准存储以保证高性能;对于访问频率降低的温数据,迁移到低频存储节省成本;对于长期不访问的冷数据,则可以归档到成本更低的归档存储或冷归档存储。</p><p>智能加热是冷热分层的重要补充功能。当归档的数据再次被访问时,平台会自动将其加热到更高性能的存储层级。加热策略支持Partition(分区)和File(文件)两层管理,可以针对分区级别或文件级别的访问进行精细化控制。这种设计既保证了数据访问的性能,又最大化地利用了低成本存储,实现了性能与成本的最佳平衡。</p><p>通过智能冷热分层,企业可以在不牺牲数据可用性的前提下,将长期存储成本降低数倍。对于PB级甚至EB级的数据湖,这种成本优化能力可以为企业节省大量资金,使得海量数据的长期保存成为可能。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxv" alt="" title="" loading="lazy"/></p><h2>▌细粒度权限控制:企业级安全保障</h2><p>企业级数据平台必须具备完善的权限控制和安全审计能力。DLF 3.0提供了从用户管理、权限控制到审计治理的全链路安全保障体系。</p><p>在用户管理方面,DLF 3.0原生支持阿里云RAM用户体系,基于用户和角色进行权限管理。用户可以通过Open API和REST API进行编程式的权限配置,也可以通过控制台进行可视化管理。</p><p>权限控制方面,DLF 3.0支持对湖表设置ACL细粒度权限,可以精确到Catalog、Database、Table、Column(列)甚至Row(行)级别。列级权限允许用户只访问特定的列,行级权限则通过WHERE条件和AND、OR等逻辑运算符,实现对特定行范围的访问控制。列Masking功能可以对敏感字段进行脱敏处理,保护数据隐私。</p><p>Data Sharing能力支持跨主账号的数据协作。企业可以将特定的数据集授权给合作伙伴或其他部门,实现安全可控的数据共享。权限检索功能帮助管理员快速了解数据的授权情况,权限委托和授权管理则提供了灵活的权限分级体系。</p><p>审计和治理方面,DLF 3.0全面记录所有操作日志,满足生产环境的合规要求。审计管理功能支持漏洞发现和安全治理,帮助企业及时发现和修复安全隐患。这套完整的安全体系,使得DLF 3.0可以满足金融、医疗、政务等高安全要求行业的需求。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxw" alt="" title="" loading="lazy"/></p><h2>▌实时湖仓与全模态处理:AI时代的两大刚需</h2><p>实时化和全模态化是AI时代数据平台的两大刚需。DLF 3.0在这两个方向上都实现了业界领先的能力。</p><p>在实时湖仓方面,DLF 3.0基于Paimon实现了三大核心能力:流式更新、流式订阅、实时查询。数据可以通过Flink等流计算引擎不断流式更新到湖仓中,支持大规模的增量更新和部分列更新。下游系统可以通过流式订阅的方式实时消费变更日志(Changelog),构建实时数据链路。查询层面,Paimon的数据可以被StarRocks、Hologres等OLAP引擎实时查询,延迟可以达到秒级甚至亚秒级。</p><p>与业界的Iceberg和Delta相比,Paimon在流式更新场景下具有明显优势。Iceberg和Delta主要面向日志场景,Compaction代价高、速度慢,难以支撑大规模流式增量更新。而中国市场的实时需求走在世界前列,Paimon正是为此而生,通过排序和文件组织优化,大幅降低了Compaction成本,实现了ODS、DWD、DWS全链路的实时更新。</p><p>在全模态处理方面,DLF 3.0提供了完整的多模态宽表能力。一张Paimon表可以同时存储id、url、vectors(向量)、labels(标签)、summary(文本摘要)、blobs(大型二进制对象)、meta(元数据)、json(半结构化数据)等多种类型的字段,避免了多表查询和治理负担。统一的存储底层使得数据工程师和AI工程师可以基于同一张表协作,通过高效的索引机制支持向量检索、全文检索、分析查询、模型推理、训练等多种应用场景。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxx" alt="" title="" loading="lazy"/><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxy" alt="" title="" loading="lazy"/><br/>DLF 3.0还对接了主流的大数据处理和AI预处理框架,包括Spark、Flink、Ray、PyTorch等,提供了PyPaimon等Python原生接口,使得AI工程师可以像操作本地文件一样便捷地访问湖上数据。相比业界的LanceDB等方案,DLF 3.0具有生态丰富、统一性强、工业级验证等优势,已经在阿里巴巴集团和众多外部客户中大规模落地。</p><h2>▌典型场景与客户案例:从理论到实践</h2><p>DLF 3.0已经在多个典型场景中得到了验证。在离线实时一体化湖仓场景中,通过Flink CDC实现数据库的实时摄取,支持Schema Evolution和整库同步,分钟级实时可查询。Flink在Paimon上进行流读流写,实现全链路实时湖仓,支持低成本的去重和部分列更新。Spark提供数仓级别的批处理性能,支持Filter/Min/Max/TopN/Limit等算子下推和Native计算加速。StarRocks和Hologres则通过Manifests缓存、删除向量、文件过滤等优化技术,实现了对Paimon湖表的极速查询,性能可以对标内表。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxz" alt="" title="" loading="lazy"/></p><p>在全模态数据管理和处理场景中,音频、文本、图片、视频等多模态数据通过统一的采集入湖,经过Spark或Ray进行预处理(如文本Chunking、Embedding等),将结构化标签、向量、Blob数据统一存储在一张Paimon表中。AI工程师可以通过Milvus或StarRocks进行向量检索和标量过滤,实现样本圈选和预览。PyPaimon直接对接Ray和PyTorch,支持数据加载和模型训练,整个流程端到端打通,数据无需跨系统迁移。</p><p>在客户案例方面,智能汽车向量湖是一个典型应用。自动驾驶场景产生海量的车载数据、地理信息、雷达数据、视频图片等多模态数据。通过DLF 3.0,这些数据统一采集并通过人工或机器打标生成Labels,经过预处理将图片、视频拆解为目标对象,再通过Embedding生成向量。Labels和向量数据构建成统一的向量湖,支持标量+向量混合检索召回,可以快速找到符合特定条件的数据样本,用于AI模型的迭代训练。整个方案实现了从数据采集到推理到检索的完整Pipeline,百亿级数据规模的混合检索性能表现优异。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxA" alt="" title="" loading="lazy"/></p><p>阿里巴巴集团内部的全模态湖也是重要实践。基于Paimon Blob字段,集团构建了EB级的多模态混合存储,支持视频、音频、图片等大型文件的高效管理。通过顺序读高吞吐的数据加载能力,GPU的数据利用率提升了10%,这对于大规模AI训练具有巨大的成本节省价值。这些真实案例充分验证了DLF 3.0全模态湖仓方案的技术先进性和商业价值。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxB" alt="" title="" loading="lazy"/></p><h2>▌产品商业化与生态建设</h2><p>DLF产品已于2025年正式商业化,现在提供免费试用机会。阿里云还建立了DLF钉钉交流群(群号:106575000021),欢迎用户加入进行技术交流和问题反馈。</p><p>全模态湖仓代表了大数据和AI结合发展的下一阶段重要方向。随着多模态AI应用的普及,企业对统一管理和处理异构数据的需求将越来越强烈。DLF 3.0通过开放的架构、强大的性能、完善的企业级能力,为客户提供了一个面向未来的数据平台选择。无论是传统的大数据分析场景,还是新兴的AI训练推理场景,DLF 3.0都可以提供端到端的支持,帮助企业在AI时代保持竞争力。</p>]]></description></item><item>    <title><![CDATA[Apache Flink Agents 0.2.0 发布公告 ApacheFlink ]]></title>    <link>https://segmentfault.com/a/1190000047597744</link>    <guid>https://segmentfault.com/a/1190000047597744</guid>    <pubDate>2026-02-06 19:06:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Apache Flink 社区很高兴地宣布 Apache Flink Agents 0.2.0 版本正式发布，您可以通过以下方式获取 Flink Agents 0.2.0：</p><ul><li><a href="https://link.segmentfault.com/?enc=8LqduFu91qGJI%2FPcQxzI%2FQ%3D%3D.KyJ1w4NNBvOruqHH7BjsZOKBShJ6UL8TY1la7G7v9fxUI0GY0rFYKRhvfoiqrwFxNoo4NbCu%2FUyJix9FVPsS7A%3D%3D" rel="nofollow" target="_blank">在此下载发布版本</a>。</li><li><a href="https://link.segmentfault.com/?enc=SRFTyRmOhdhQvm75bcrcdw%3D%3D.7yBjyUfKOMHzlwWEdXmxUs3dIpDYKdGvO98681VtSJEc%2Fq%2F9TSKCTQn1MNsjQtnPea%2F2TeqdSRv3nhPfXIUA0VsCtqWnnITjHlpBWCNtSng%3D" rel="nofollow" target="_blank">在此查看文档和快速入门示例</a>。</li></ul><p>请注意，Agents 0.2.0 是一个预览版本（Preview Version），这意味着：</p><ul><li>部分功能可能存在已知或未知的缺陷。您可以通过 <a href="https://link.segmentfault.com/?enc=k%2FvDWjx%2BkGn4oL5QzGPLig%3D%3D.kNsts%2Fy2A4Oi4JTCyLL4qEEEqPkX%2FqzAerwvD%2B2UQOA0M0QctrFbJ%2FA8uuxxFdE1" rel="nofollow" target="_blank">Github Issues</a> 查看已知问题列表及其解决状态。</li><li>当前的 API 和配置选项处于实验阶段，在未来版本中可能会发生不向后兼容的变更。</li><li>我们非常感谢您的任何反馈，无论是分享您的使用案例、建议新功能、帮助定位和修复漏洞，还是其他任何想法。您的见解对我们至关重要。</li></ul><p><strong>您可以通过以下方式联系我们：</strong></p><ul><li>加入 <a href="https://link.segmentfault.com/?enc=xwC0mMTz2qG%2ByKxt8XZ%2BEQ%3D%3D.9ym0Rm3%2BGX45KVpYpfn7gqyzY9J4olh4%2F8ZutwLLPqnmLW0wjCAGPFbnPkGTwUZZAoWaq%2B%2BjPKjOTm8L1Sgn%2FQ%3D%3D" rel="nofollow" target="_blank">Apache Flink Slack</a> 并在 <code>#flink-agents-user</code> 频道寻求帮助。</li><li>在 <a href="https://link.segmentfault.com/?enc=QK37DRLEaD0oDW7chYNqQg%3D%3D.bCCTuU9UDahKUYUl05FaEx1hHpqWu%2BIdonj%2FV3JyVvHTgxpBhN8EKRoprRynK7Cv" rel="nofollow" target="_blank">Github Issues</a> 提交功能需求和漏洞报告。</li><li>在 <a href="https://link.segmentfault.com/?enc=djlHV6p%2BIFTBLrRdSr02YA%3D%3D.tS16lub5lv%2FAzQ72zRL0x1O1xQ9Vk0GrbpLW8Ecrarp9zQx3iSZ24BlUys%2FW%2F%2B0HW%2FZSjmskcMef%2BUC%2F5XTzMw%3D%3D" rel="nofollow" target="_blank">Github Discussions</a> 分享您的使用案例和想法。</li></ul><hr/><h3>什么是 Apache Flink Agents？</h3><p>Apache Flink Agents 是 Apache Flink 的一个新子项目，直接在Apache Flink 的流式运行时（streaming runtime）上构建事件驱动的 AI 智能体（Event-driven AI Agents）。它将流处理与自主智能体统一在同一个框架中，将Apache  Flink 经受过实战检验的优势——大规模扩展性、低延迟、容错性和状态管理，与智能体的核心能力——大语言模型（LLMs）、工具、记忆和动态编排有机结合。</p><h3>为什么 Apache Flink Agents 至关重要？</h3><p>虽然 AI 智能体在chatbots和copilots等交互式应用中取得了飞速进展，但这些系统通常以同步、一次性交互的方式运行。然而，许多业务场景不能等待用户输入指令后才采取行动。在电子商务、金融、物联网和物流等场景中，必须在感知到实时事件（如支付失败、传感器异常或用户点击）时立即做出关键决策。</p><p>要在生产环境中取得成功，企业级Agents必须具备以下能力：</p><ol><li>处理实时、高吞吐的事件流，如交易流、传感器异常或用户行为轨迹。</li><li>持续且自主地运行，而不仅仅是在收到提示词（prompt）时才运行。</li><li>保证安全性、可审计性，并在发生故障时能够恢复。</li></ol><p>这些工作不仅需要智能，更需要大规模扩展能力、毫秒级延迟、容错性以及有状态的协调能力。而这些正是 Apache Flink 的核心强项。</p><p>此前，尚未有一个统一的框架能将Agentic AI 模式引入 Flink 成熟的流处理生态系统中。Apache Flink Agents 填补了这一空白，将Agents视为始终在线、可靠且可扩展的事件驱动微服务。</p><h3>核心特性</h3><p>Apache Flink是流计算领域的事实标准，Apache Flink Agents 继承了分布式、大规模、高可用的结构化数据处理和成熟的状态管理能力，并为Agentic AI 的构建和功能增加了自由的抽象，包括：大语言模型（LLMs）、提示词（prompts）、工具（tools）、记忆（memory）、动态编排、可观测性等。</p><p>Apache Flink Agents 的关键特性包括：</p><ul><li>大规模扩展与毫秒级延迟：利用 Flink 的分布式处理引擎，实时处理大规模事件流。</li><li>无缝的数据与 AI 集成：Agents直接与 Flink 的 DataStream 和 Table API 交互进行输入和输出，实现结构化数据处理与语义 AI 能力在 Flink 内部的平滑集成。</li><li>Exactly-Once 一致性：通过外置的 Action State Store 扩展 Flink 原本的 Checkpoint 机制，从而确保 Agent 中 Action 执行、模型推理、工具调用及其影响的精确一直一致性</li><li>成熟的Agent抽象：利用广为人知的 AI  Agent概念，使具有Agent系统开发经验的开发者能够快速上手并构建应用，无需陡峭的学习曲线。</li><li>多语言支持：提供 Python 和 Java 的原生 API，能够无缝集成到不同的开发环境中，允许团队使用其偏好的编程语言。</li><li>丰富的生态系统：原生支持对主流模型服务与向量存储的集成，，以及托管在 MCP 服务器上的工具或提示词，同时支持自定义扩展。</li><li>可观测性：采用以事件为中心的编排方法，所有智能体行为都由事件连接和控制，从而能够通过事件日志观察和理解智能体的行为。</li></ul><hr/><h3>0.2 版本有哪些新变化？</h3><h4>Java API 功能对齐</h4><p>在 Flink Agents 0.1 中，部分功能仅在 Python API 中可用。0.2 版本通过在 Java 中增加以下能力的完整支持，弥补了这一差距：</p><ul><li>嵌入模型（Embedding Models）</li><li>向量存储（Vector Stores）</li><li>MCP 服务器（MCP Server）</li><li>异步执行（Asynchronous Execution）</li></ul><p>至此，Java API 在功能上已与 Python API 完全对齐。</p><h4>扩展的生态集成</h4><p>Flink Agents 0.2 引入了对更广泛的模型服务和向量数据库的原生支持：</p><ul><li><p>对话模型（Chat Models）：</p><ul><li>Python API 现支持 Azure OpenAI。</li><li>Java API 增加了对 Azure AI、Anthropic 和 OpenAI 的支持。</li></ul></li><li><p>嵌入模型（Embedding Models）：</p><ul><li>Java API 现支持 Ollama。</li></ul></li><li><p>向量数据库（Vector Stores）：</p><ul><li>Java API 现支持 Elasticsearch。</li></ul></li></ul><p>此外，0.2 版本现支持跨语言资源访问。用户可以在一种语言编写的智能体中，调用另一种语言提供的集成支持。例如：在 Python 智能体中调用 Java 支持的 Azure AI 对话模型。  <br/><em>(注：跨语言资源访问目前不支持在异步执行代码块中使用。使用跨语言集成时，框架内置动作将回退到同步执行。)</em></p><h4>记忆系统重构</h4><p>Flink Agents 0.2 对其记忆管理系统进行了全面升级。相比之前仅支持短期记忆，新版本引入了三种不同的记忆类型：</p><ol><li>感官记忆（Sensory Memory）： 在单次智能体运行中维护状态并传递上下文。</li><li>短期记忆（Short-Term Memory）： 在多次智能体运行之间保留精确的上下文信息。</li><li>长期记忆（Long-Term Memory）： 实现大规模上下文信息的近似语义检索，并提供初步的信息摘要和压缩支持。</li></ol><h4>持久化执行（Durable Execution）</h4><p>Flink Agents 0.1 提供了Action级的精确一次执行。在 0.2 版本中，这一能力被精细化到了更小的颗粒度。你现在可以在一个Action内指定特定的代码块进行持久化执行。在故障恢复时，即使整个Action尚未完成，任何已成功执行的持久化代码块都不会重新运行。  <br/>这有助于避免：</p><ul><li>冗余的模型调用（节省时间、Token 并减少不可预测性）。</li><li>重复工具调用产生的副作用（例如：重复付款或重复发送电子邮件通知）。</li></ul><h4>多版本 Flink 兼容性</h4><p>Flink Agents 0.1 仅兼容 Apache Flink 1.20.3。  <br/>Flink Agents 0.2 现支持更广泛的 Flink 版本：1.20, 2.0, 2.1 和 2.2。  <br/><em>(注：建议始终使用所选 Flink_ 小版本（x.y）_的最新_补丁版本（x.y.z）_，以获得更多已知问题的修复。)</em></p><hr/><h3>破坏性变更（Breaking Changes）</h3><h4>Python API</h4><ul><li>创建 <code>ResourceDescriptor</code> 的 API 已更改。在之前版本中，用户通过 <code>clazz=Type[Resource]</code> 指定资源提供者；在 0.2 版本中，应通过 <code>clazz=ResourceName</code> 指定，我们为内置集成提供了常量字符串。</li><li><code>RunnerContext.execute_async</code> 方法已更名为 <code>durable_execute_async</code>。</li><li><code>MCPTool</code>、<code>MCPPrompt</code> 和 <code>MCPServer</code> 不再被视为 API，已从 <code>api</code> 模块中移出。</li></ul><h4>配置</h4><ul><li><code>ERROR_HANDLING_STRATEGY</code> 现在不仅影响 ReAct Agent，而是影响所有智能体。它已从 <code>ReActAgentConfigOptions</code> 移至 <code>AgentExecutionOptions</code>。</li></ul><h4>Java Ollama 对话模型</h4><ul><li>对话模型设置中的 <code>extract_reasoning</code> 参数类型从 <code>string</code> 更改为 <code>boolean</code>，默认值从 <code>false</code> 更改为 <code>true</code>。</li><li>引入了新参数 <code>think</code> 用于控制是否启用思考模式。<code>extract_reasoning</code> 不再影响此行为。</li></ul><hr/><h3>贡献者名单</h3><p>Apache Flink 社区感谢以下每一位为本次发布努力的贡献者：</p><p>Alan Z., Eugene, Ioannis Stavrakantonakis, Liu Jiangang, Marcelo Colomer, Shekharrajak, Weiqing Yang, Wenjin Xie, Xiang Li, Xintong Song, Xuannan, Yash Anand, chouc, dependabot[bot], tsaiggo, twosom</p><hr/><p>阿里云的 Flink Agents 团队正在北京、上海招聘！如果你对实时计算、AI 数据基础设施充满热情，欢迎加入我们，点击链接或直接邮箱投递！</p><p>了解详情：<a href="https://link.segmentfault.com/?enc=NU5%2FXMWkV6DU83Ma%2BisLMA%3D%3D.PiHoiSFobMFXob3x3l6WxTFEHpjWYN5ZnsRCo9ApicZO1bnf9i%2F8dl%2BwKTKjxhpQ7w1kOfbFL4hI3V8Po58GPdlAqibEvOFV8bHyGhT%2Bk4yd0y9Eo6c0RZcaAtjpVaq6Wi1696k2%2FKCL3rTecuh43Q%3D%3D" rel="nofollow" target="_blank">https://careers.aliyun.com/off-campus/position-detail?lang=zh...</a></p><p>邮箱：<a href="mailto:xintong.sxt@alibaba-inc.com" target="_blank">xintong.sxt@alibaba-inc.com</a></p>]]></description></item><item>    <title><![CDATA[使用 Python 导出 Word 表格为 Excel 工作表 大丸子 ]]></title>    <link>https://segmentfault.com/a/1190000047597747</link>    <guid>https://segmentfault.com/a/1190000047597747</guid>    <pubDate>2026-02-06 19:05:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在实际办公和开发场景中，我们经常会遇到这样的需求：<strong>Word 文档中包含大量结构化表格数据，而最终需要将这些数据统一整理到 Excel 中进行统计、分析或二次处理</strong>。手动复制粘贴不仅效率低，而且在遇到复杂表格（如单元格内多段文本、多表格文档）时，格式也很容易被破坏。</p><p>借助 Python 脚本我们可以<strong>自动化提取 Word 文档中的所有表格，并将每个表格完整写入 Excel 的独立工作表中</strong>，在保证数据结构清晰的同时，大幅提升处理效率。</p><p>本文将详细介绍一种完整、可复用的实现方案，并对关键代码逻辑进行说明，适用于批量表格转换与自动化办公场景。</p><hr/><p>本文所使用的方法需要用到 <a href="https://link.segmentfault.com/?enc=qxctRu2CAaktSkT6dlKbjQ%3D%3D.095KJbtEs3OS9KVQKuFX10mNK0aLce1g9YJQXpS20O%2F%2BroKAG2O6hQqhKh8MHJgejQzBSxJaZweCmsJT96P%2FAQ%3D%3D" rel="nofollow" target="_blank">Free Spire.Doc for Python</a> 和 <a href="https://link.segmentfault.com/?enc=5u64moeBAJRidZ%2Bji%2BINeQ%3D%3D.z4xHbxJupyMZNoSEVYcGnrOeS5TQiZ%2FkcJWv7vmtZ9L7Vu7pkzCWmStZGLSUGNsDbS76e34hRNyrK2LILgyYog%3D%3D" rel="nofollow" target="_blank">Free Spire.XLS for Python</a>，分别用于提取 Word 表格数据和写入 Excel 文件。可通过 pip 安装：<code>pip install spire.doc.free spire.xls.free</code>。</p><hr/><h2>一、实现思路概览</h2><p>整个转换流程可以拆分为两个清晰的阶段：</p><ol><li><p><strong>从 Word 文档中提取表格数据</strong></p><ul><li>遍历文档中的所有节（Section）</li><li>遍历每个节中的所有表格（Table）</li><li>逐行、逐单元格读取文本内容</li><li>保留单元格内的原有段落结构</li></ul></li><li><p><strong>将提取的数据写入 Excel 文件</strong></p><ul><li>为每个 Word 表格创建一个新的工作表</li><li>按行列顺序写入单元格内容</li><li>自动调整列宽，提升可读性</li></ul></li></ol><p>这种“先抽象为数据结构，再写入目标文件”的方式，逻辑清晰，也便于后续扩展（例如 CSV、数据库等）。</p><hr/><h2>二、使用 Python 提取 Word 中的表格数据</h2><p>下面的函数负责<strong>从 Word 文档中提取所有表格，并以嵌套列表的形式返回数据</strong>。</p><pre><code class="python">from spire.doc import *

def extract_tables_from_word(word_file_path):
    """
    从 Word 文档中提取所有表格数据。
    返回一个列表，其中：
    - 每个元素代表一个表格
    - 表格内部是“行”的列表
    - 每一行是“单元格内容”的列表
    """
    document = Document()
    document.LoadFromFile(word_file_path)

    all_tables_data = []

    # 遍历文档中的所有节
    for sec_index in range(document.Sections.Count):
        section = document.Sections.get_Item(sec_index)

        # 遍历节中的所有表格
        for table_index in range(section.Tables.Count):
            table = section.Tables.get_Item(table_index)
            current_table_data = []

            # 遍历表格中的所有行
            for row_index in range(table.Rows.Count):
                table_row = table.Rows.get_Item(row_index)
                current_row_data = []

                # 遍历行中的所有单元格
                for cell_index in range(table_row.Cells.Count):
                    table_cell = table_row.Cells.get_Item(cell_index)

                    # 提取单元格中的所有段落文本，保留换行结构
                    paras = [
                        table_cell.Paragraphs.get_Item(i).Text.rstrip('\r\n')
                        for i in range(table_cell.Paragraphs.Count)
                        if table_cell.Paragraphs.get_Item(i).Text.strip()
                    ]
                    current_cell_data = "\n".join(paras)
                    current_row_data.append(current_cell_data)

                current_table_data.append(current_row_data)

            all_tables_data.append(current_table_data)

    document.Close()
    return all_tables_data</code></pre><h3>关键说明</h3><ul><li><strong>Section → Table → Row → Cell</strong> 的层级结构，符合 Word 文档的真实组织方式</li><li><p>使用 <code>Paragraphs</code> 而不是直接读取 <code>Text</code>，可以：</p><ul><li>避免丢失单元格内的多段内容</li><li>保留原有换行结构，写入 Excel 后依然清晰</li></ul></li><li>最终返回的数据是一个<strong>三层嵌套列表</strong>，非常适合后续写入表格类文件</li></ul><hr/><h2>三、将提取的数据写入 Excel 文件</h2><p>在拿到结构化表格数据后，接下来使用 <strong>Spire.XLS for Python</strong> 将其写入 Excel。</p><pre><code class="python">from spire.xls import *

def write_data_to_excel(extracted_data, excel_file_path):
    """
    将提取的 Word 表格数据写入 Excel 文件。
    每个 Word 表格对应 Excel 中的一个工作表。
    """
    workbook = Workbook()
    # 清除默认工作表
    workbook.Worksheets.Clear()

    if not extracted_data:
        print("没有从 Word 文档中提取到任何表格数据。")
        return

    # 遍历所有表格数据
    for i, table_data in enumerate(extracted_data):
        sheet = workbook.Worksheets.Add(f"Table_{i + 1}")

        # 写入行列数据
        for r_idx, row_data in enumerate(table_data):
            for c_idx, cell_value in enumerate(row_data):
                # Excel 行列索引从 1 开始
                sheet.Range[r_idx + 1, c_idx + 1].Value = cell_value

        # 自动调整列宽
        sheet.AllocatedRange.AutoFitColumns()

    workbook.SaveToFile(excel_file_path, ExcelVersion.Version2016)
    workbook.Dispose()
    print(f"数据已成功写入到 {excel_file_path}")</code></pre><h3>实现要点</h3><ul><li><p><strong>每个 Word 表格 → 一个 Excel 工作表</strong></p><ul><li>结构直观，避免数据混杂</li></ul></li><li>Excel 行列索引从 <code>1</code> 开始，需要注意与 Python 索引的差异</li><li><code>AutoFitColumns()</code> 可显著提升导出后的可读性</li></ul><hr/><h2>四、完整调用示例</h2><pre><code class="python">word_file = "input.docx"
excel_file = "output.xlsx"

extracted_data = extract_tables_from_word(word_file)
write_data_to_excel(extracted_data, excel_file)</code></pre><p>运行后，Word 文档中的所有表格将被完整转换，并按顺序写入 Excel 文件。以下是运行结果示例：</p><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnSvK" alt="Python提取Word表格到Excel" title="Python提取Word表格到Excel"/></p><hr/><h2>五、适用场景与扩展建议</h2><p><strong>适用场景</strong>：</p><ul><li>将报告型 Word 文档中的数据统一汇总到 Excel</li><li>自动化处理批量合同、清单、配置表</li><li>作为数据清洗或分析流程的前置步骤</li></ul><p><strong>扩展方向</strong>：</p><ol><li><strong>批量处理多个 Word 文件</strong></li><li>根据表格内容自动命名工作表</li><li>对 Excel 输出添加边框、样式或冻结首行</li><li>将中间数据结构复用于 CSV 或数据库写入</li></ol><hr/><h2>总结</h2><p>通过结合 <strong>Spire.Doc for Python</strong> 与 <strong>Spire.XLS for Python</strong>，我们可以用一套清晰、稳定的 Python 方案，实现 <strong>Word 表格到 Excel 表格的自动化转换</strong>。这种方式不仅避免了手动复制的低效和错误，也为后续的数据处理和分析提供了良好的基础。</p><p>对于需要频繁处理文档表格数据的开发者和办公场景来说，这是一种非常实用、可维护性也很高的解决方案。</p>]]></description></item><item>    <title><![CDATA[春节旺季不慌！当连锁门店遇上一见，稳稳赢下全年开门红！ 百度一见 ]]></title>    <link>https://segmentfault.com/a/1190000047597754</link>    <guid>https://segmentfault.com/a/1190000047597754</guid>    <pubDate>2026-02-06 19:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047548146" alt="图片" title="图片"/><br/>年味愈发醇厚，服务业的 “春运时间” 已正式拉开帷幕。当汹涌人潮涌向全国万千连锁门店，品牌管理者们正面临一场全方位的运营大考：人手告急：客流峰值期店员忙到分身乏术，服务 SOP 执行变形，谁来及时纠偏？安全红线：后厨用火用电需求激增、地面水渍易引发滑倒风险，如何防患于未然？缺货焦虑：年货爆款上架即售罄，人工补货速度追不上顾客扫货节奏，如何破解？百度一见，将多模态视觉技术深度融入春节服务全场景，为连锁门店派驻 “全天候在岗的 AI 店长”，让门店在旺季忙而有序，稳稳斩获新年第一桶金！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597756" alt="图片" title="图片" loading="lazy"/></p><h4>管服务：协同无差错，服务有标准</h4><p>越是人手紧缺，越需要AI来充当“数字化店长”。百度一见可直接复用门店原有摄像头，实时捕捉前端服务全场景，自动识别员工仪容仪表、服务流程规范、出餐时效等关键指标，发现问题即刻推送至店长后台，确保即便在客流峰值，服务标准也始终在线。</p><p><strong>人员行为智能提醒</strong>：口罩、工帽、围裙等细节，是春节食安管理的关键一环。一见赋予现场摄像头智能查纠能力，<strong>自动识别店员穿戴违规行为，及时预警整改。</strong></p><p><strong>服务流程量化管理</strong>：针对服务合规 “无法量化管理” 痛点，<strong>一见将 “菜品按时上桌”“顾客离座及时收台”“员工着装规范” 等环节转化为可量化指标</strong>，总部可实时管理全国门店执行情况，实现千家门店服务标准统一管控。</p><p><strong>场景适配灵活高效：</strong>临时新增 “餐具摆放检查” 或 “新春口罩佩戴识别” 需求？<strong>一见支持一句话生成专业级视觉AI应用，完美适配餐饮、零售、茶饮等各类业态门店。</strong> </p><h4>管安全：风险无死角，运营更安心</h4><p>门店场景的合规与安全，是春节旺季运营的底线。无论是餐饮门店的后厨卫生、虫害防治，还是零售门店的消防安全、环境整洁，一旦出现问题，不仅会面临监管处罚，还会严重影响品牌口碑，甚至错失旺季客流。一见打破传统人工巡检的局限，打造全天候、无死角的安全监测，让门店守住安全合规底线，安心过年。</p><p><strong>守住舌尖上的安全：</strong>在食品安全领域，<strong>一见能精准识别后厨虫害、生熟食交叉污染、操作台不洁等风险</strong>，大幅降低人工巡检的漏判概率，守住顾客“舌尖上的安全”，筑牢品牌信任壁垒。</p><p><strong>护航门店平稳运营</strong>：在门店环境与安全方面，一见可实时监测消防通道是否畅通、消防设备是否齐全、外部人员是否闯入，<strong>及时预警潜在风险，全方位规避安全隐患与合规问题，</strong>保障门店春节期间安全、平稳运营。</p><h4>管物料：库存精准控，损耗降到底</h4><p>春节期间，连锁门店商品 / 物料需求量暴增，库存周转速度加快，传统人工盘点耗时费力且易出错。百度一见依托多模态大模型技术，打造全自动化 “AI 盘库” 解决方案，革新传统库存管理模式。</p><p><strong>智能盘库高效省心</strong>：<strong>自动完成物料消耗盘点，实时同步库存数据</strong>，无需员工闭店后熬夜加班，<strong>大幅降低人工成本，盘点效率提升 60% 以上。</strong>同时结合春节消费趋势，精准预判物料需求，辅助门店提前规划备货，有效避免缺货或库存积压问题。</p><p><strong>安全与损耗双重管控：</strong>针对零售门店货架管理核心痛点，一见<strong>可精准识别商品缺货断档、商品破损、摆放错位、价签不匹配等问题，</strong>实现从商品上架陈列、货架实时监测到库存补给的全程可视化管理，让旺季货架管理更精准、库存周转更高效。目前，百度一见已携手餐饮、茶饮、零售等多个行业的头部连锁品牌，实现后厨违规操作降低80%、服务合规率提升40%、库存盘点效率提升60%的显著成效。<br/><strong>立即联系一见，定制专属连锁门店春节运营方案，稳稳拿下全年开门红！</strong><br/>👉<a href="https://link.segmentfault.com/?enc=54sGpHEC6%2ByNFZo0Pvp2Ew%3D%3D.qzLWpNGuev1K%2FKpRoO0E0FBEE2Rss%2BWWNr75RuSA7nP7X5FHPiGHg3Phnw%2FjV%2BEE5JbSu%2B1n%2Bo6VN5JIV963W9MzO9GuOo0ZmXg0uoTUbzw%3D" rel="nofollow" target="_blank">https://cloud.baidu.com/survey/yijian-intelligentchainstores....</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597757" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[对接印度 NSE 与 BSE 交易所实时数据 CryptoRzz ]]></title>    <link>https://segmentfault.com/a/1190000047597772</link>    <guid>https://segmentfault.com/a/1190000047597772</guid>    <pubDate>2026-02-06 19:04:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>印度股市拥有两大支柱：<strong>国家证券交易所 (NSE)</strong> 和 <strong>孟买证券交易所 (BSE)</strong>。NSE 以极高的流动性和衍生品交易著称，而 BSE 则是亚洲最古老的交易所，拥有最多的上市公司。</p><p>对于开发者而言，如何在一个接口中同时获取这两大交易所的<strong>实时行情</strong>、<strong>指数（Nifty 50 / Sensex）以及逐笔 K 线</strong>，是构建印度金融产品的关键。</p><hr/><h3>一、 核心接入参数（交易所定位）</h3><p>在 StockTV API 体系中，通过 <code>exchangeId</code> 可以精准筛选数据源：</p><table><thead><tr><th>交易所名称</th><th>缩写</th><th><strong>exchangeId</strong></th><th>核心指数</th></tr></thead><tbody><tr><td><strong>印度国家证券交易所</strong></td><td><strong>NSE</strong></td><td><code>46</code></td><td>Nifty 50 (NSEI)</td></tr><tr><td><strong>孟买证券交易所</strong></td><td><strong>BSE</strong></td><td><code>74</code></td><td>S&amp;P BSE SENSEX (BSESN)</td></tr></tbody></table><blockquote><strong>国家 ID 提示</strong>：对接印度市场时，请务必全局携带 <code>countryId=14</code>。</blockquote><hr/><h3>二、 核心 API 场景实现</h3><h4>1. 区分交易所获取股票列表</h4><p>如果您想单独展示 NSE 或 BSE 的股票排名或列表，可以使用 <code>exchangeId</code> 参数进行过滤。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/stocks</code></li><li><strong>NSE 示例</strong>：<code>?countryId=14&amp;exchangeId=46&amp;key=YOUR_KEY</code></li><li><strong>BSE 示例</strong>：<code>?countryId=14&amp;exchangeId=74&amp;key=YOUR_KEY</code></li></ul><h4>2. 指数实时监控（Nifty vs Sensex）</h4><p>指数是市场的风向标。StockTV 提供的指数接口支持秒级更新。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/indices</code></li><li><strong>请求参数</strong>：<code>countryId=14&amp;key=YOUR_KEY</code></li><li><strong>数据亮点</strong>：返回结果中会包含 <code>NSEI</code>（NSE 指数）和 <code>BSESN</code>（BSE 指数）的实时点位、涨跌幅及成交额。</li></ul><h4>3. 实时 K 线数据（图表专用）</h4><p>支持对接各种前端图表库（如 TradingView），提供高频采样的 K 线。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/kline</code></li><li><strong>参数示例</strong>：<code>pid={产品ID}&amp;interval=PT1M&amp;key=YOUR_KEY</code>（获取 1 分钟级极速 K 线）。</li></ul><hr/><h3>三、 技术优势：极致实时性</h3><p>针对印度市场波动剧烈、散户参与度高的特点，StockTV 在实时性上做了深度优化：</p><ol><li><strong>多交易所聚合推送</strong>：无需维护多套连接，通过一个 WebSocket 通道即可接收 NSE 和 BSE 的混合报价推送。</li><li><strong>毫秒级延迟控制</strong>：服务器节点部署于印度及周边核心机房，大幅缩短物理距离带来的网络延迟。</li><li><strong>智能重连机制</strong>：针对移动端应用，提供稳定的数据流保持方案，确保用户在弱网环境下也能看到最新的价格跳动。</li></ol><hr/><h3>四、 Python 实战：获取 NSE 权重股行情</h3><p>以下示例演示如何快速调取 NSE 交易所中特定股票（如 Reliance）的实时数据：</p><pre><code class="python">import requests

def fetch_india_exchange_data(symbol, exchange_id):
    url = "https://api.stocktv.top/stock/queryStocks"
    params = {
        "symbol": symbol,
        "exchangeId": exchange_id, # 指定交易所 46 或 74
        "key": "YOUR_API_KEY"
    }
    
    response = requests.get(url, params=params)
    data = response.json()
    
    if data['code'] == 200:
        item = data['data'][0]
        print(f"交易所ID: {exchange_id} | 股票: {item['name']}")
        print(f"当前价: {item['last']} | 涨跌幅: {item['chgPct']}%")
        print(f"最后撮合时间: {item['time']}")
    else:
        print(f"请求失败: {data['message']}")

# 查询 NSE 的 Reliance
fetch_india_exchange_data("RELI", 46)
</code></pre><hr/><h3>五、 进阶：如何获取完整的 BSE 500 指数成分？</h3><p>对于需要构建深度行情应用的客户，还支持通过 <code>stocksByPids</code> 接口批量订阅数百只股票的实时更新。只需一次请求，即可获取整个板块的盘面异动。</p><hr/><h3>六、 结语</h3><p>无论是追求极致速度的算法交易，还是注重用户体验的行情 App，提供的 NSE/BSE 双交易所接口都能为您提供最坚实的数据支撑。</p>]]></description></item><item>    <title><![CDATA[2026年需求管理软件测评：主流产品对比与选型避坑指南 王思睿 ]]></title>    <link>https://segmentfault.com/a/1190000047597781</link>    <guid>https://segmentfault.com/a/1190000047597781</guid>    <pubDate>2026-02-06 19:03:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你正在挑一款需求管理软件，大概率会在“需求入口分散、评审难落地、变更失控、交付对不齐”之间反复踩坑。本文以可核验公开资料为依据，按统一评分模型测评 ONES、Tower、Jira Product Discovery、Aha! Roadmaps、Productboard、YouTrack、Azure DevOps Boards、GitLab Requirements、Jama Connect、IBM DOORS、Polarion、ReqView，给你一张可直接对照的选型清单：谁适合产品团队、谁更偏工程合规、谁适合研发执行闭环。</p><h2>一、测评方法论：5大一级指标 + 14项细分维度</h2><p>评分口径：本文“综合得分/星级”是编辑评分，依据各工具的公开产品页/帮助中心/官方文档中可核验功能信息；不使用“不可验证的第三方满分/认证/客户数”。</p><p><strong>5大一级指标（建议权重）：</strong></p><ul><li>需求全生命周期能力（30%）：从需求入口到验收闭环是否完整</li><li>优先级与路线图（20%）：能否把“想做”变成“先做什么”</li><li>评审与变更治理（20%）：需求管理软件的分水岭在“变更可控”</li><li>追溯与影响分析（20%）：需求—设计—任务—测试的链路是否可追问</li><li>集成与协作体验（10%）：跨部门/跨工具链协作成本</li></ul><p><strong>14项细分维度：</strong></p><ul><li>需求入口：多渠道收集 / 表单化 / 统一归口</li><li>需求池：去重归类 / 状态流转 / 负责人机制</li><li>需求表达：模板 / 验收标准 / 附件与上下文</li><li>优先级：自定义字段 / 评分模型 / 价值-成本权衡</li><li>路线图：多视图 / 干系人沟通 / 与交付同步</li><li>评审：评审流程 / 评论与决议 / 结论可追溯</li><li>变更：版本化 / 变更记录 / 影响范围提示</li><li>依赖：需求依赖/前后置 / 冲突提示</li><li>追溯：需求↔任务 / 需求↔测试 / 历史审计</li><li>影响分析：变更触发的下游影响识别</li><li>交付对齐：迭代/版本关联 / 发布说明</li><li>协作：通知 / 权限 / 外部协作</li><li>集成：API/Webhook / 与代码/CI/客服系统对接</li><li>报表：周期、吞吐、积压、变更等基础度量</li></ul><h2>二、2026年需求管理软件总榜（选型参考，非绝对优劣）</h2><p><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnSwh" alt="" title=""/></p><blockquote>关键提醒：工具排名是“选型参考”；真正的优劣取决于你团队的协作方式与治理成熟度。品牌推荐官文章也会强调“排序非绝对优劣”，但常用更强断言表达。</blockquote><h2>三、2026年需求管理软件深度测评</h2><h4>NO.1｜ONES（从需求到交付验证的闭环流水线）</h4><p>推荐指数：★★★★★<br/>综合得分：92/100（编辑评分）<br/>关键优势：产品能力（93）/集成协作（90）/场景适配（92）/治理追溯（91）/交付对齐（94）</p><p>核心定位：<a href="https://link.segmentfault.com/?enc=SAp1H0%2F%2FBt3HtWwiumSVgQ%3D%3D.4ix8LGnWiIn%2FipqQkurwXg%3D%3D" rel="nofollow" target="_blank">ONES</a> 是面向企业的一站式研发管理平台品牌，覆盖从需求到交付协作与效能提升的核心场景。在需求管理方面是更偏“产研一体”的需求管理软件，把需求池、迭代规划、测试关联放在同一条链路里。其“需求池”思路强调：统一入口、梳理评审、优先级排期、再分配落地。</p><p>需求管理能力拆解</p><ul><li>需求入口与需求池：需求池作为统一归口，覆盖收集、梳理、评审、分配等环节，适合把分散需求“先收住”。</li><li>优先级与评审闭环：需求评审的关键不是“开会”，而是把优先级决策、依赖关系、验收标准写进需求对象里，减少“会后口头共识”。（公开资料侧强调优先级、关系与跟踪机制）</li><li>交付对齐与追溯：当需求进入测试阶段，可将测试计划与需求关联，形成“需求跟踪视图/矩阵”，让交付质量回到需求本身；测试用例也支持与需求、任务关联，利于问责与复盘。</li></ul><p>适用场景：中大型产品团队、需要“需求—迭代—测试”联动的组织；以及希望把需求管理软件作为研发管理底座的团队。</p><p>使用与避坑建议：</p><ul><li>别一上来就追求复杂流程：先把“需求入口—需求池—优先级—迭代对齐”跑顺，再补审批/变更规则。</li><li>度量先从可解释指标开始：周期、积压、变更次数这类“能解释”的指标先做起来，别一开始追求花哨看板。</li></ul><p>参考来源：ONES 需求池管理实践文章、需求池管理流程文章、ONES TestCase 产品页。</p><h4>NO.2｜Jira Product Discovery（需求发现/优先级）</h4><p>推荐指数：★★★★☆<br/>综合得分：88/100<br/>关键优势：产品能力（90）/集成协作（88）/场景适配（86）/治理追溯（80）/交付对齐（86）<br/>核心定位：Jira Product Discovery 是 Atlassian 面向产品团队推出的需求发现与优先级/路线图工具，强调把想法与洞察集中管理并用字段与公式做排序。<br/>需求管理能力拆解<br/>需求入口与需求池：更像“机会/想法池”，适合把用户反馈、销售线索、访谈结论先结构化沉淀。<br/>优先级与路线图：支持自定义字段与公式，帮助团队把“价值/影响/成本”量化成排序依据；并能用不同视图对不同干系人沟通。<br/>评审闭环：优势在于“基于证据的对齐”，而不是审批流；适合减少拍脑袋，但需要你们先统一评分口径。<br/>变更与追溯：对“需求—交付”闭环依赖 Jira 生态（这不是缺点，只是边界）。<br/>适用场景：产品团队需要提升“优先级共识质量”；尤其是多干系人拉扯严重、需要用证据做取舍的团队。<br/>局限与避坑建议：<br/>别把它当完整交付系统：它强在“上游”，下游需要配套交付工具链。<br/>评分模型要小而美：字段越多越像形式主义，建议从 3-5 个关键维度起步。<br/>参考来源：Atlassian 官方产品页与功能页。</p><h4>NO.3｜Productboard（把需求讲成路线图）</h4><p>推荐指数：★★★★☆<br/>综合得分：87/100<br/>关键优势：产品能力（89）/集成协作（85）/场景适配（88）/治理追溯（78）/交付对齐（84）<br/>核心定位：Productboard 是产品管理软件平台，核心价值是帮助团队理解客户需求、做特性优先级，并让组织围绕路线图对齐。<br/>需求管理能力拆解<br/>需求入口与需求池：强在“把多渠道声音汇总成可行动的需求主题”，适合产品在信息噪音中做归类。<br/>优先级与路线图：强调“围绕路线图对齐组织”，并支持把路线图以链接形式对外共享（适用于对齐销售/客户）。<br/>评审与变更：适合“产品评审”，但工程变更控制需要与交付工具联动（否则会变成“路线图很好看，落地很难追”）。<br/>适用场景：产品驱动型组织；需要经常向外部/内部解释“为什么先做这个”的团队。<br/>局限与避坑建议：<br/>别把路线图当承诺清单：路线图输出越容易共享，越要把“可变更边界”讲清楚。<br/>落地追溯要预先设计：至少保证需求与交付任务有稳定映射关系，否则复盘时很难说清“这条需求到底交付了什么”。<br/>参考来源：Productboard 官方产品页与路线图共享帮助文档。</p><h4>NO.4｜Aha! Roadmaps（Idea 门户 + 推进到路线图）</h4><p>推荐指数：★★★★☆<br/>综合得分：86/100<br/>关键优势：产品能力（88）/集成协作（83）/场景适配（86）/治理追溯（80）/交付对齐（82）<br/>核心定位：Aha! Roadmaps 是以产品路线图与创意管理见长的平台，支持把 Ideas 直接“Promote”到路线图记录并建立关联追踪。<br/>需求管理能力拆解<br/>需求入口：Ideas Portal 让“外部/一线声音”有标准入口。<br/>需求推进机制：支持把 idea “Promote”到路线图中的不同记录类型，并可新建或链接到既有记录，适合把多个反馈收敛到同一需求主题。<br/>评审与变更：优势在“推进机制清晰”；但你仍需要定义“什么时候可以Promote、谁批准、如何记录决议”。<br/>适用场景：产品线多、反馈来源杂、需要强治理与对齐的组织。<br/>局限与避坑建议：<br/>不要用工具替代决策：Aha 能让流程更可追溯，但“取舍标准”仍要团队自己建立。<br/>避免门户变成许愿池：设置最小提交模板与反馈分类规则，否则会被低质量输入淹没。<br/>参考来源：Aha 官方产品页与支持文档。</p><h4>NO.5｜Azure DevOps Boards（把需求分层成 Epic/Feature）</h4><p>推荐指数：★★★★☆<br/>综合得分：84/100<br/>关键优势：产品能力（80）/集成协作（88）/场景适配（85）/治理追溯（82）/交付对齐（88）<br/>核心定位：Azure Boards 是 Microsoft Azure DevOps 体系中的工作项与 Backlog 管理能力，支持用 Epics/Features 组织需求并分层推进到执行。<br/>需求管理能力拆解：<br/>需求池与分层：通过 features/epics backlogs 把需求按层级归类，利于“从愿景到迭代”的结构化分解。<br/>优先级与排期：优势在“与开发执行紧耦合”，但产品侧的“需求质量（验收标准/证据）”需要你们自己用模板/规范补齐。<br/>追溯：在工程侧可追溯较强，但跨到测试/发布/客户反馈时，仍需要流程设计与集成。<br/>适用场景：研发组织成熟、执行体系以 ADO 为核心；需要把需求管理软件与交付过程合一。<br/>局限与避坑建议<br/>别只堆层级：Epic/Feature 不是越多越好，关键是每一层都能回答“这层做完意味着什么”。<br/>验收标准务必前置：否则会变成“做了很多条目，但没人敢说交付完成”。<br/>参考来源：Microsoft Learn 官方文档。</p><h4>NO.6｜YouTrack（看板与 Backlog 一体）</h4><p>推荐指数：★★★★☆<br/>综合得分：83/100<br/>关键优势：产品能力（80）/集成协作（80）/场景适配（86）/治理追溯（78）/交付对齐（82）<br/>核心定位：YouTrack 是 JetBrains 的项目/Issue 跟踪平台，强调用 Backlog 与敏捷看板把团队工作保持聚焦并可随优先级变化回收至 Backlog。<br/>需求管理能力拆解：<br/>需求池/Backlog：支持围绕查询与Backlog协作，但如果你们希望“产品侧更强的需求表达/评审”，要自己补流程。<br/>流转与协作：看板卡片拖动会同步更新字段值，协作反馈更即时。<br/>变更治理：更偏“执行流转”，而非“治理控制”；适合敏捷团队，但对强合规场景不够。<br/>适用场景：研发团队主导的敏捷协作；不想为复杂RM系统付出高实施成本的组织。<br/>局限与避坑建议<br/>别把“能拖动”当“治理”：需求评审结论、验收标准、变更边界仍要写清楚。<br/>Backlog规则要一致：否则会出现“需求在板上/在Backlog里”争议，影响透明度。<br/>参考来源：JetBrains YouTrack 官方文档。</p><h4>NO.7｜Tower（更擅长排期与依赖）</h4><p>推荐指数：★★★★☆<br/>综合得分：80/100<br/>关键优势：产品能力（75）/集成协作（82）/场景适配（86）/治理追溯（70）/交付对齐（84）<br/>核心定位：Tower 是国内团队协作与项目管理产品，突出以时间线（甘特图）等视图提升任务排期与依赖协作效率。在需求管理方面更像“把需求落到任务与排期”的协作工具——当你需求评审完，最怕的不是“没人做”，而是“做着做着依赖乱了、排期失真”。Tower 的时间线/甘特与依赖联动，能把交付风险提前暴露。<br/>需求管理能力拆解：<br/>需求到任务拆解：适合作为“需求落地承接层”，把需求拆成任务、设置负责人/日期/依赖，保证交付透明。<br/>依赖与变更联动：支持“自动调整后置任务时间”“防止任务依赖冲突”，这对频繁变更的需求落地很实用——变更不是问题，变更不联动才是问题。<br/>可视化排期：支持按天/周/月/季/年粒度看时间线，便于和干系人对齐节奏。<br/>适用场景：需要强排期、强依赖管理的项目型团队；或把专业需求管理软件与协作排期工具组合使用的团队。<br/>局限与避坑建议：<br/>需求治理要在上游完成：Tower 适合执行与排期，不适合承载复杂的需求评审与合规追溯。<br/>依赖不是越多越好：建议只给关键路径建依赖，否则维护成本反噬。<br/>参考来源：Tower 官方博客（甘特/时间线/依赖联动说明）。</p><h4>NO.8｜GitLab Requirements（把需求放进工程体系）</h4><p>推荐指数：★★★☆☆<br/>综合得分：79/100<br/>关键优势：产品能力（75）/集成协作（86）/场景适配（78）/治理追溯（80）/交付对齐（82）<br/>核心定位：GitLab Requirements 是 GitLab 平台中的“需求工件”能力，把需求作为长期存在的 artifact 来管理，用于描述产品行为与验收标准。<br/>需求管理能力拆解<br/>需求对象化：在项目中可创建/查看需求列表，需求不再只是 issue 描述。<br/>合规协作：导出需求到 CSV 并通过邮件附件发送的能力，对“需要对外共享/审计留档”的场景有现实价值。<br/>追溯与交付对齐：工程侧链路天然更紧密，但产品侧需求池、路线图治理能力相对有限。<br/>适用场景：DevOps 一体化团队；希望需求管理软件尽量贴近代码与工程资产的组织。<br/>局限与避坑建议<br/>别把导出当治理完成：导出只是能力，治理要靠流程与责任边界。<br/>需求表达要标准化：否则需求会退化成“另一个Issue字段”。<br/>参考来源：GitLab Requirements 官方文档。</p><h4>NO.9｜Jama Connect（需求变更影响分析）</h4><p>推荐指数：★★★★☆<br/>综合得分：85/100<br/>关键优势：产品能力（88）/集成协作（78）/场景适配（84）/治理追溯（92）/交付对齐（83）<br/>核心定位：Jama Connect 是 Jama Software 的需求管理与追溯平台，主打 Live Traceability 与实时风险识别能力（如 Live Trace Explorer）。<br/>需求管理能力拆解<br/>变更影响分析：当需求变化时，识别对下游需求与测试的“涟漪效应”，这是合规与质量团队真正关心的地方。<br/>追溯到测试：把需求与测试活动的关系建立起来，帮助你回答“这个需求验证了吗、覆盖了吗”。<br/>评审与协作：更适合正式评审与证据沉淀，而非轻量敏捷日常。<br/>适用场景：汽车、医疗、航天等对追溯与验证要求高的行业；或系统工程团队。<br/>局限与避坑建议<br/>实施成本要前置评估：Jama 的价值在体系化使用，零散使用反而浪费。<br/>先定义追溯模型再上工具：否则你会得到“很多链接”，但解释不清为什么要链接。<br/>参考来源：Jama 官方帮助文档与能力介绍页。</p><h4>NO.10｜IBM DOORS（传统工程 RM）</h4><p>推荐指数：★★★★☆<br/>综合得分：82/100<br/>关键优势：产品能力（80）/集成协作（70）/场景适配（82）/治理追溯（94）/交付对齐（78）<br/>核心定位：IBM Engineering Requirements Management DOORS 系列（DOORS 与 DOORS Next）是 IBM 的规模化需求管理解决方案，强调协作评审、变更管理与可追溯性。<br/>需求管理能力拆解<br/>基线与签署：支持对基线进行电子签署，并记录签署人、时间等信息，满足审计与责任追溯需要。<br/>多级追溯：强调多层级追溯视图与可定制视图，适合复杂需求分解与验证链路。<br/>变更治理：优势在“控制与证据链”，但对产品团队而言会显得偏重、偏工程。<br/>适用场景：强监管行业、系统工程/硬件软件协同项目、对审计链要求极高的组织。<br/>局限与避坑建议<br/>不要把它当轻量需求池：它更适合“规格与基线管理”，不是日常想法收集。<br/>角色分工要清晰：否则会出现“工程团队用得很好，产品团队完全进不来”。<br/>参考来源：IBM 产品页与官方文档（电子签名/基线）。</p><h4>NO.11｜Polarion（自动变更控制 + 审计链）</h4><p>推荐指数：★★★★☆<br/>综合得分：84/100<br/>关键优势：产品能力（83）/集成协作（76）/场景适配（84）/治理追溯（92）/交付对齐（80）<br/>核心定位：Polarion 是 Siemens 的网页版 ALM 平台，用于统一管理需求、开发、测试与发布，并强调端到端可追溯与可见性。<br/>需求管理能力拆解：<br/>自动变更控制：对每条需求的变更进行控制与记录，目标是让审计/合规检查更容易通过。<br/>工作流与审计链：用工作流规则约束需求状态流转，“什么时候能进入下一状态”变成可配置规则，而不是口头约定。<br/>电子签署：支持让干系人对规格文档电子签署（公开资料中明确提及）。<br/>适用场景：大型组织、流程治理成熟或必须提升合规证据链的团队。<br/>局限与避坑建议：<br/>别用它解决“沟通不愿写清楚”：工具能强约束流程，但写清楚仍要靠团队习惯。<br/>先梳理关键工作流再落工具：否则配置会变成一场无止境的“流程之战”。<br/>参考来源：Siemens Polarion 官方介绍页。</p><h4>NO.12｜ReqView（轻量工程 RM）</h4><p>推荐指数：★★★☆☆<br/>综合得分：78/100<br/>关键优势：产品能力（76）/集成协作（70）/场景适配（78）/治理追溯（85）/交付对齐（72）<br/>核心定位：ReqView 是面向软硬件开发的需求管理工具，强调在 Git 中建立基线，并支持覆盖/风险/变更影响分析与多格式报告输出（Word/Excel/PDF/HTML）。<br/>需求管理能力拆解<br/>文档化需求与基线：对需要交付规格文档、并希望版本可控的团队，Git 基线的概念很贴近工程实践。<br/>追溯与影响分析：强调分析覆盖、风险与变更影响，适合“改一条需求会影响哪里”的场景。<br/>报告输出：可生成 Word/Excel/PDF/HTML 等报告格式，这对交付与审计沟通很友好。<br/>适用场景：硬件/软件协同、系统工程、需要规格文档交付但不想上重型RM套件的团队。<br/>局限与避坑建议<br/>对“产品型需求池”支持较弱：更适合工程规格与追溯，不适合做海量想法收集。<br/>需要团队具备版本管理习惯：否则“基线在Git”会变成少数人才能维护的资产。<br/>参考来源：ReqView 官方产品页。</p><p>挑需求管理软件这件事，表面看是功能对比，实际是在选择一种“治理方式”。如果你们最痛的是需求入口分散，先选能把需求池立住的；如果你们最痛的是优先级共识，选能把证据、字段、公式沉淀下来的；如果你们最痛的是变更失控与追责困难，那就把“追溯与影响分析”当作硬指标；如果你们在强合规行业，别怕工具偏重——怕的是你们没有一条可审计的证据链。工具不会替你做决策，但工具会逼你把决策写清楚。当你们愿意把“为什么做、先做什么、变更影响什么、怎么验收”落在系统里，需求管理软件才会从“记录器”变成“协作的共同语言”。</p>]]></description></item><item>    <title><![CDATA[2026年产品管理系统测评：对比选型避坑+能力模型评分 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047597786</link>    <guid>https://segmentfault.com/a/1190000047597786</guid>    <pubDate>2026-02-06 19:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文测评 12 款常见产品管理系统/平台：ONES、Tower、Jira Product Discovery、Jira Software、Productboard、Aha! Roadmaps、Craft.io、airfocus、ProductPlan、Tempo Strategic Roadmaps（原 Roadmunk）、Azure Boards、monday.com。我会用“能力模型+1–5星评分”对比战略规划、需求管理、研发协同、用户反馈、数据指标、知识沉淀、组织治理与集成扩展等方面，帮你更快做出适配团队的选型判断。</p><h2>测评结论速览</h2><h4>结论速选</h4><ul><li>想要“需求—研发协同—测试/交付”尽量在一套体系里跑：优先看 ONES 这类端到端研发管理平台路线。</li><li>最痛是“想法太多、证据分散、优先级吵不清”：优先看 Productboard / Aha! Roadmaps 这种上游产品发现与路线图工具。</li><li>更看重工程交付节奏（看板、Backlog、冲刺、报表）：ONES / Jira Software / Azure Boards 更像执行中枢。</li><li>更看重“排期、依赖联动、跨部门推进”：Tower 更像协作与排期底座，甘特依赖与自动调整对“变更引发的连锁延期”很有帮助。</li><li>更看重“路线图呈现与组合视图（Portfolio）对齐管理层”：优先看 ProductPlan。</li></ul><h4>对比表（编辑评分｜1–5星）</h4><blockquote>说明：这是“可核验信息基础上的编辑评分”，用于快速对比，不是第三方认证/行业榜单。</blockquote><p>评分尺度：</p><ul><li>5星：公开资料显示能力成熟、链路完整、适用面广</li><li>3星：能力明确但偏单点/偏分层，需要搭配其他工具/方法</li><li>1–2星：该维度公开能力弱或依赖外部实现，不建议作为主承载</li></ul><p>证据来源：</p><ul><li>官方产品页/功能页/帮助中心/支持文档（最高优先级）；</li><li>官方博客/官方集成文档（用于补充“如何实现”的细节）；</li><li>若某项能力在公开资料里找不到，我也会标注出来。</li></ul><p><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnSwm" alt="" title=""/></p><h2>产品管理系统深度测评</h2><h4>1) ONES</h4><p>一句话定位：<a href="https://link.segmentfault.com/?enc=%2FqWXmUaAxyBDiyqjnsvwyA%3D%3D.RCBpTGmornUHtirlFEQIxA%3D%3D" rel="nofollow" target="_blank">ONES</a> 是国产企业级研发管理平台，公开定位强调“端到端软件研发管理”，并明确覆盖从需求管理、迭代跟进到测试的链路，更像把产品管理系统与研发协同放在同一底座上。适合中大型产品研发组织，以及痛点是“规划在A、开发在B、测试在C”的断链，且愿意把评审规则、状态机、验收标准沉淀为组织资产的团队。<br/>产品管理能力拆解（战略规划/需求管理/研发协同/反馈/指标/知识/治理/运营）<br/>战略规划：强调“场景方案与流程落地”，战略层表达（如 OKR/产品战略树）能否做深，取决于你们是否把“口径与规则”写进系统。<br/>需求管理：优势在“需求进入交付链路后可追踪”，适合把需求、迭代、测试串联起来做闭环。<br/>研发协同：端到端链路是它作为产品管理系统底座的关键——当状态、职责、验收统一后，跨团队沟通会更省。<br/>用户反馈/产品运营：公开信息里可看到“工单/服务台”等场景，适合作为主反馈中枢还需要结合你们渠道与流程<br/>数据指标/治理：拥有效能改进与可视化分析能力，适合效能管理者用同一套口径看多项目/多团队表现。<br/>优势亮点（为什么重要 + 场景）<br/>闭环追溯：需求到测试贯通，让复盘能回答“做了什么、为何做、交付成了什么”。<br/>流程可落地：把协作规则写进系统，减少“靠人盯”的成本，适合人员流动较大的组织。<br/>统一口径：对效能管理者来说，统一口径比“功能堆满”更关键。</p><h4>2) Tower</h4><p>一句话定位：Tower 是 ONES 旗下团队协作工具，官方定位是“安排任务、管理进度、沉淀团队知识”，并提供列表/日历/看板/甘特等多视图，把它放进产品管理系统版图里更像“协作与排期底座”。适合中小到中型跨部门团队，以及项目排期、依赖关系复杂；需要把“计划—推进—提醒—复盘”放进同一工作空间的团队。<br/>产品管理能力拆解<br/>战略规划：Tower 更擅长把目标落到“阶段、里程碑与责任人”，而非提供完整 OKR 体系；它适合作为战略落地的执行层。<br/>需求管理：软件研发支持迭代计划、需求管理、Bug 管理等，偏“承接已明确需求并推进”。<br/>研发协同：甘特图用于进度管控，，适合应对“需求变更→排期连锁反应”。<br/>用户反馈：若你们在飞书协作，Tower 协作可以把聊天消息/图片快速转为任务，并与群聊/云文档空间等深度整合，这类“就地采集反馈→进入任务流”的路径很务实。<br/>优势亮点<br/>依赖+排期可视化：项目经理能更快定位延期风险，把精力从“搬日期”转回“管风险”。<br/>多视图对齐：同一份数据给不同角色用不同视图看，减少“你说不清、我看不懂”。<br/>飞书侧入口自然：对“反馈从聊天里来”的团队，消息转任务能显著降低漏记与重复沟通。</p><h4>3) Jira Product Discovery</h4><p>一句话定位：Jira Product Discovery（Atlassian）定位在“产品发现与优先级”，支持用字段与表达式公式把想法/洞察变成可排序的优先级，并用视图做路线图沟通。适合产品决策争议大、跨部门拉扯多、需要建立优先级共识的团队。<br/>产品管理能力拆解<br/>战略规划：更像战略输入的承接层——把战略拆成可讨论的机会/想法并持续评估。<br/>需求管理：它管理的是“上游需求”（机会/想法/洞察），核心价值是把争论从“谁嗓门大”转为“按口径排序”。<br/>研发协同：交付闭环通常需要 Jira 的执行体系承接（把被选中的需求稳定流入研发 Backlog）。<br/>组织治理：优势在“决策可复盘”——字段与公式让你解释得清楚“为什么做A不做B”。<br/>优势亮点<br/>公式化优先级：把 RICE 等模型落到系统，减少拍脑袋与反复扯皮。<br/>视图沟通：同一份数据给不同受众不同视角，降低对齐成本。<br/>与Jira衔接自然：对已在 Atlassian 生态内的团队，落地成本更低。</p><h4>4) Jira Software</h4><p>一句话定位：Jira Software 是典型工程执行型产品管理系统组件，官方明确支持 scrum/kanban 等方法，并覆盖敏捷板、Backlog、路线图、报表与集成生态。适合工程交付为中心的组织，产品经理需要在产品管理系统里紧跟开发进度、风险与发布。<br/>产品管理能力拆解<br/>战略规划：战略表达往往在外部形成（文档/OKR），再映射到史诗/故事与路线图。<br/>需求管理：更擅长“已确认需求”的拆解与流转（Backlog、用户故事管理），而不是大规模反馈聚类。<br/>研发协同：强项在节奏与透明——冲刺、看板、报告能帮助团队建立稳定交付节拍。<br/>数据指标：报表丰富，但建议把指标用于改进而非考核（否则团队会“为指标而工作”）。<br/>优势亮点<br/>方法适配广：scrum、kanban 或混合都能落地。<br/>Backlog 纪律强：用户故事管理与可见性提升，有助于减少“需求说了但没人做”。<br/>生态成熟：当你们需要和CI/CD、知识库、客服等系统连接时，生态价值会逐年放大。</p><h4>5) Productboard</h4><p>一句话定位：Productboard 公开定位是“帮助产品经理理解客户需求、确定优先级，并让组织围绕路线图对齐”，在产品管理系统里更偏“反馈→优先级→路线图”的主线。适合客户声音很多、需求入口分散、需要用路线图统一叙事并降低跨部门沟通成本的团队。<br/>产品管理能力拆解<br/>战略规划：强在“把战略讲清楚并持续对齐”，通过路线图让不同部门理解取舍逻辑。<br/>需求管理/用户反馈：定位直接强调理解客户需求，适合把分散的客户声音收拢为主题与需求，再进入优先级与路线图。<br/>研发协同：通常通过与工程工具联动完成交付闭环（公开定位强调“下一步要构建什么并对齐”，具体联动深度建议试用验证，本文不做超出公开信息的断言）。<br/>组织治理/产品运营：当路线图成为单一真相源，销售/CS/市场的预期管理会更稳定（这是产品运营层面的真实价值）。<br/>优势亮点<br/>围绕路线图对齐：减少重复解释与版本不一致。<br/>以用户需求驱动优先级：把“感觉”变成“可回溯的证据链”（至少在信息结构上）。<br/>更贴近产品语言：讨论焦点更偏用户问题与价值，而不是工程任务颗粒度。</p><h4>6) Aha! Roadmaps</h4><p>一句话定位：Aha! Roadmaps 公开强调通过品牌化 Ideas Portal 收集反馈、对想法排序，并将最佳想法Promote到路线图对象（features/epics等），属于偏治理的产品管理系统路线。适合产品线多、反馈来源复杂、需要强治理与推进机制的组织（尤其规模化阶段）。<br/>产品管理能力拆解<br/>战略规划：更适合做“路线图中枢”——把战略拆成可管理层级并持续推进。<br/>需求管理/用户反馈：Ideas Portal 统一入口，“Promote”机制让想法有明确去处并能跟踪状态。<br/>研发协同：常通过与工程工具衔接完成交付闭环（公开资料强调从想法到路线图再到交付状态追踪）。<br/>组织治理：强在“可追溯”——你能清楚回溯某个 feature 从哪条反馈/想法来、为何被选中。<br/>优势亮点<br/>Ideas → Roadmap 的推进链路：减少“想法堆在墙上没人管”。<br/>入口统一带来数据结构化：有利于沉淀分类、主题与决策依据。<br/>状态可追溯：对 PMO/效能角色很友好，复盘成本更低。</p><h4>7) Craft.io</h4><p>一句话定位：Craft.io 公开定位为端到端产品管理平台，并在 OKR 页面明确提出“连接 objectives 到 initiatives/projects/epics”，走的是“OKR/战略→规划→执行”的产品管理系统路线。适合目标管理清晰、管理层需要强对齐，或希望把 OKR 与产品规划强绑定的中大型产品组织。<br/>产品管理能力拆解<br/>战略规划/OKR：OKR 是显性定位，强调从目标到执行对象的连接，减少“目标只在PPT里”。<br/>需求管理：更偏把需求纳入战略框架（initiative/epic/feature），适合做结构化规划。<br/>研发协同：官网强调从反馈收集到执行覆盖整个生命周期（具体研发侧细节以试用为准，本文不做超出公开信息的断言）。<br/>指标/治理/运营：OKR 关联关系天然服务治理：当目标—投入—产出关系清晰，运营沟通也更“讲得通”。<br/>优势亮点<br/>目标到执行可追溯：能回答“为什么做这件事”，并在复盘时检查假设是否成立。<br/>端到端生命周期叙事：对“从反馈到执行”的团队更顺手。<br/>减少战略漂移：当目标与路线图对象有关联，改方向就会更谨慎、更可控。</p><h4>8) airfocus</h4><p>一句话定位：airfocus 公开定位为产品管理工具，并在优先级模块明确提供“评分框架、Priority Poker”等机制，把优先级讨论做成可协作流程，属于模块化产品管理系统路线。适合中型以上产品团队，对优先级质量、路线图沟通和跨团队参与度要求高的组织。<br/>产品管理能力拆解<br/>战略规划：其定位强调围绕清晰产品战略来优先级排序与路线图对齐。<br/>需求管理/优先级：强项就是“把取舍标准落地”——评分框架与协作式 Priority Poker 让不同角色参与排序。<br/>用户反馈/指标/运营：官网模块展示包含反馈与洞察、OKR等（深度以试用为准，本文不做超出公开信息的断言）。<br/>组织治理：模块化意味着可以按成熟度逐步启用，避免“一口气把系统做成怪物”。<br/>优势亮点<br/>评分框架让争论回到标准：把“口水战”转成“口径讨论”，适合争议多的团队。<br/>Priority Poker 促进共识：让多角色参与排序，减少单点拍板的偏差。<br/>模块化上手：先从优先级/路线图开始，再扩到OKR/洞察，节奏更可控。</p><h4>9) ProductPlan</h4><p>一句话定位：ProductPlan 公开强调“从发现到发布”用路线图与 portfolio 视图形成单一真相源，并在支持文档中明确 Portfolio View 可把多条路线图合并为一个整体视图用于分享给高层与干系人。适合产品线多、路线图需要统一口径；高频跨部门沟通导致成本高的组织。<br/>产品管理能力拆解<br/>战略规划：强在“战略可视化”——把战略倡议与里程碑放到可分享的路线图上，减少信息偏差。<br/>需求管理：更偏承接“已确认的规划项”，上游洞察/反馈体系通常要配套。<br/>研发协同：常作为“路线图层”，下游对接 Jira/Azure 等执行系统（公开强调从发现到发布，但具体执行承接需试用验证）。<br/>组织治理/产品运营：Portfolio View 对管理层对齐很友好——把多个产品/团队的路线图放在一张图里谈取舍。<br/>优势亮点<br/>Portfolio View“把全局拉齐”：适合 VP/负责人快速看到全局节奏与冲突。<br/>单一真相源：减少“每个人都有一张路线图”的版本地狱。<br/>面向受众分享：路线图不仅是内部用，还是对外预期管理工具。</p><h4>10) Tempo Strategic Roadmaps（原 Roadmunk）</h4><p>一句话定位：Tempo 的 Strategic Roadmaps 公开介绍里强调 Idea Manager：记录团队想法、在路线图中进出以便快速转向，并提供内置优先级模板或自定义评分框架，更偏“想法管理+路线图呈现”的产品管理系统层。适合对路线图沟通要求很高，需要把“想法收拢→优先级→路线图表达”做得顺的团队。<br/>产品管理能力拆解<br/>战略规划/路线图：核心在“把方向讲清楚”，并让路线图可随项目变化快速调整。<br/>需求管理：偏“想法/主题→路线图条目”，适合对齐与沟通，不是重规格的需求工程工具。<br/>用户反馈：公开资料聚焦 idea management 与 prioritization framework；更深入的反馈采集形态建议以官方资料与试用验证为准（本文不做超出公开信息的断言）。<br/>研发协同：更像路线图层，需要下游执行系统承接。<br/>优势亮点<br/>Idea Manager 支持快速转向：变化来了能把想法/条目快速进出路线图，减少大改带来的混乱。<br/>内置模板/自定义评分：让优先级讨论回到标准，提高决策一致性。<br/>更像“路线图表达工具”：对管理层沟通友好，适合把复杂工作讲成清晰节奏。</p><h4>11) Azure Boards</h4><p>一句话定位：Azure Boards（Azure DevOps）公开强调提供 Kanban boards、backlogs、dashboards、scrum boards 等敏捷工具，并支持自定义工作流与“1,000+ extensions”，属于工程交付承接型产品管理系统组件。适合微软生态与 DevOps 体系成熟的研发组织。<br/>产品管理能力拆解<br/>战略规划：更偏执行层，战略通常在外部形成后映射到工作项/积压。<br/>需求管理：适合管理“已确认需求”的交付推进（用户故事、bug、工作项等）。<br/>研发协同：看板/冲刺/容量等机制对效率管理者很关键，利于建立节奏与透明度。<br/>治理/集成：扩展与自定义能力适合大组织长期演进，也更容易融入 Teams/Slack 等协作。<br/>优势亮点<br/>敏捷执行能力完整：Kanban、Backlog、Dashboard、Scrum boards 一应俱全。<br/>可定制与扩展：适合把组织方法固化为流程与权限体系。<br/>与常用协作工具结合：有利于把执行信息带回团队日常协作场域。</p><h4>12) monday</h4><p>一句话定位：monday 在“Product Management Software”页面明确写到可在一处管理 roadmaps、plans、challenges、KPIs，并强调高度可定制，属于“可配置工作系统”型产品管理系统。适合协作对象广（产品/市场/运营/支持/交付），流程变化快、需要快速调整系统结构的团队。<br/>产品管理能力拆解<br/>战略规划/产品运营：KPI 与计划集中管理对运营节奏有帮助，但战略严密性取决于你怎么设计板、字段与评审机制。<br/>需求管理：更像可配置工作流承接需求与任务，而非专注产品发现的证据链体系。<br/>研发协同：跨部门协作友好；工程深度（如代码/PR强关联）需要看集成与组织实践（本文不做超出公开资料的断言）。<br/>组织治理：优势是“可定制”，风险也是“过度可定制导致口径漂移”，需要制度化维护。<br/>优势亮点<br/>路线图+计划+KPI同处：管理层看全局、团队看分工，沟通链路更短。<br/>可配置适配管理模式：更像“搭积木”，适合非标准流程。<br/>可视化带来透明：很多扯皮来自看不见彼此的工作，透明度能直接降低摩擦。</p><p>选产品管理系统，最怕的是“拿工具替代思考”。我更建议你先把三个问题问清楚：你们的瓶颈在哪一段？是上游“优先级吵不清”，还是中游“排期依赖失控”，还是下游“交付追溯断链”？瓶颈不同，主系统就不同。你们要“一体化闭环”还是“分层组合”？一体化适合追求口径统一、链路完整；分层组合适合成熟团队按强项拼装（路线图层 + 执行层 + 协作/知识层）。你们愿不愿意为“口径一致”付出维护成本？系统越灵活，越需要字段字典、状态机、评审模板与复盘节奏。否则工具越强，混乱也会更快被放大。</p><p>工具能做的，是把共识“写下来、追踪起来”；做不到的，是替你们完成那些本该由团队共同承担的取舍。把方法立住，你选哪一款产品管理系统，都会更顺。</p>]]></description></item><item>    <title><![CDATA[【Matlab源码】6G候选波形：OFDM-IM 增强仿真平台 DM、CI 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047597805</link>    <guid>https://segmentfault.com/a/1190000047597805</guid>    <pubDate>2026-02-06 19:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>&lt;p align="center"&gt;<br/>  &lt;h1 align="center"&gt;⚡ OFDM-IM 性能优化仿真平台&lt;/h1&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;strong&gt;DM-OFDM + CI-OFDM-IM 双技术加持，100%频谱利用 + 分集增益&lt;/strong&gt;</code></pre><p>&lt;/p&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;img src="https://img.shields.io/badge/MATLAB-R2021b+-blue?style=flat-square&amp;logo=mathworks" alt="MATLAB"/&gt;
&lt;img src="https://img.shields.io/badge/DM-OFDM-green?style=flat-square" alt="DM"/&gt;
&lt;img src="https://img.shields.io/badge/CI-Interleaving-orange?style=flat-square" alt="CI"/&gt;
&lt;img src="https://img.shields.io/badge/Diversity-Gain-red?style=flat-square" alt="Diversity"/&gt;</code></pre><p>&lt;/p&gt;<br/>&lt;/p&gt;</p><hr/><h2>📌 为什么选择本仿真平台？</h2><table><thead><tr><th align="left">痛点</th><th align="left">本平台解决方案</th></tr></thead><tbody><tr><td align="left">📚 基础 OFDM-IM 频谱利用率不高</td><td align="left">✅ <strong>DM-OFDM 双模式</strong>：所有子载波都携带数据，100% 频谱利用</td></tr><tr><td align="left">🔧 衰落信道下分集阶数不足</td><td align="left">✅ <strong>CI 坐标交织</strong>：I/Q 分量分离交织，额外获得 2 阶分集增益</td></tr><tr><td align="left">📊 星座设计无参考</td><td align="left">✅ <strong>最优星座对选择</strong>：16QAM+QPSK 等经验证的高效组合</td></tr><tr><td align="left">⚡ 分集技术难以验证</td><td align="left">✅ <strong>瑞利衰落信道仿真</strong>，直观对比 CI 前后 BER 差异</td></tr><tr><td align="left">📡 缺乏可视化展示</td><td align="left">✅ 自动生成 <strong>双模式星座图</strong> 和 <strong>分集增益曲线</strong></td></tr></tbody></table><hr/><h2>🎯 核心价值</h2><table>
<tr>
<td width="50%">

### 🔬 学术研究价值

- DM-OFDM 双模式索引调制原理验证
- CI 坐标交织分集增益量化分析
- 不同星座组合对 BER 的影响研究
- 衰落信道下的性能边界探索

</td>
<td width="50%">

### 💼 工程应用价值

- 100% 频谱利用率，适合频谱紧张场景
- 抗衰落能力增强，适合移动通信
- 可配置双模式星座阶数
- 完整的发射-接收链路实现

</td>
</tr>
</table><hr/><h2>⚡ 技术亮点</h2><h3>🌊 DM-OFDM + CI 双技术架构</h3><pre><code class="text">┌─────────────────────────────────────────────────────────────────┐
│                 DM-OFDM + CI-OFDM-IM 系统架构                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    【DM-OFDM 双模式调制】                        │
│  索引比特 ──► Mode 1 位置 (高阶 16-QAM)                         │
│            └► Mode 2 位置 (低阶 QPSK) ──► 100% 利用率           │
│                                                                 │
│                    【CI 坐标交织分集】                           │
│  复数符号 ──► I 分量保持 ──┐                                    │
│            └► Q 分量置换 ──┴►  分集增益 ×2                      │
│                                                                 │
│         ┌────────── 瑞利衰落信道 ──────────┐                    │
│         │   深衰落 → CI 保护 → 分集恢复    │                    │
│         └──────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────────┘</code></pre><h3>📊 性能指标 (仿真实测)</h3><table><thead><tr><th align="center">技术</th><th align="center">信道</th><th align="center">SNR</th><th align="center">BER</th><th align="center">vs 基础 IM</th><th align="center">增益</th></tr></thead><tbody><tr><td align="center">DM-OFDM</td><td align="center">AWGN</td><td align="center">12 dB</td><td align="center">5.2e-4</td><td align="center">1.2e-3</td><td align="center"><strong>频谱 +25%</strong></td></tr><tr><td align="center">CI-OFDM-IM</td><td align="center">Rayleigh</td><td align="center">15 dB</td><td align="center">8.3e-4</td><td align="center">3.5e-3</td><td align="center"><strong>分集 2 阶</strong></td></tr><tr><td align="center">DM+CI 联合</td><td align="center">Rayleigh</td><td align="center">15 dB</td><td align="center">4.1e-4</td><td align="center">3.5e-3</td><td align="center"><strong>综合最优</strong></td></tr></tbody></table><blockquote>💡 <strong>双重优势</strong>：DM 提升频谱效率，CI 提升抗衰落能力，两者可叠加使用。</blockquote><hr/><h2>🖥️ 运行环境</h2><h3>最低要求</h3><table><thead><tr><th align="left">项目</th><th align="left">要求</th></tr></thead><tbody><tr><td align="left"><strong>MATLAB版本</strong></td><td align="left">R2021b 或更高</td></tr><tr><td align="left"><strong>必需工具箱</strong></td><td align="left">Communications Toolbox</td></tr><tr><td align="left"><strong>基础依赖</strong></td><td align="left">P1 基础包</td></tr><tr><td align="left"><strong>内存</strong></td><td align="left">4 GB+</td></tr></tbody></table><h3>快速验证</h3><pre><code class="matlab">&gt;&gt; cd packages/P2_性能优化包
&gt;&gt; setup_path
&gt;&gt; generate_plots_enhanced</code></pre><hr/><h2>🧠 算法原理</h2><h3>DM-OFDM 双模式调制</h3><p><strong>核心思想</strong>：不再有"空闲"子载波，改用两种不同调制阶数区分索引信息。</p><p><strong>比特分配</strong>：</p><p>$$
p_{DM} = p_1 + k \cdot \log_2 M_1 + (n-k) \cdot \log_2 M_2
$$</p><p><strong>典型配置</strong>：</p><ul><li>Mode 1: 16-QAM (高阶)</li><li>Mode 2: QPSK (低阶)</li><li>通过星座差异区分索引</li></ul><h3>CI 坐标交织原理</h3><p><strong>核心思想</strong>：将复数符号的 I/Q 分量分离后置换，使相邻符号的 I/Q 经历不同衰落系数。</p><p><strong>交织公式</strong>：</p><p>$$
X_{CI}[i] = X_I[i] + j \cdot X_Q[(i+d) \mod N]
$$</p><p><strong>分集增益</strong>：</p><p>$$
G_d = 2 \cdot (n - k + 1)
$$</p><hr/><h2>📁 项目结构</h2><pre><code class="text">P2_性能优化包/
├── 📂 dm/                      # DM-OFDM 双模式调制
│   ├── dm_modulator.m          #   🚀 DM 发射端
│   └── dm_demodulator.m        #   🚀 DM 接收端
│
├── 📂 ci/                      # CI 坐标交织
│   ├── ci_modulator.m          #   CI 交织调制器
│   └── ci_demodulator.m        #   CI 解交织解调器
│
├── 📂 core/                    # 继承 P1 核心模块
├── 📂 channels/                # 信道 (含瑞利衰落)
├── 📂 config/                  # 配置 (扩展 DM/CI 参数)
│
├── 📂 docs/                    # 文档
│   ├── 算法文档.md              #   📘 DM/CI 原理推导
│   ├── 代码文档.md              #   📒 接口说明
│   └── 项目文档.md              #   📗 本文档
│
├── generate_plots.m            # 📊 基础 BER 曲线
└── generate_plots_enhanced.m   # 📊 双模式星座图 + 分集增益</code></pre><p><strong>代码统计</strong>：</p><ul><li>📄 20+ 个核心 MATLAB 文件</li><li>📝 2000+ 行精炼代码</li><li>💬 100% 中文详细注释</li></ul><hr/><h2>🎬 仿真演示</h2><h3>一键运行</h3><pre><code class="matlab">&gt;&gt; cd packages/P2_性能优化包
&gt;&gt; setup_path
&gt;&gt; generate_plots_enhanced</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597807" alt="p2_ber_compare.png" title="p2_ber_compare.png"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597808" alt="p2_diversity_gain.png" title="p2_diversity_gain.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597809" alt="p2_dm_constellation.png" title="p2_dm_constellation.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597810" alt="p2_spectrum_compare.png" title="p2_spectrum_compare.png" loading="lazy"/></p><hr/><h2>📦 您将获得</h2><table><thead><tr><th align="left">内容</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">📁 <strong>完整源码</strong></td><td align="left">DM + CI 双技术完整实现</td></tr><tr><td align="left">📖 <strong>原理文档</strong></td><td align="left">双模式调制、坐标交织数学推导</td></tr><tr><td align="left">🚀 <strong>100%频谱</strong></td><td align="left">DM-OFDM 无空闲子载波设计</td></tr><tr><td align="left">📊 <strong>分集验证</strong></td><td align="left">瑞利衰落信道下 CI 增益对比</td></tr><tr><td align="left">🔧 <strong>灵活星座</strong></td><td align="left">可配置 M1/M2 调制阶数组合</td></tr><tr><td align="left">📡 <strong>可视化</strong></td><td align="left">自动生成双模式星座图</td></tr></tbody></table><hr/><h2>🛒 获取方式</h2><p>本文代码仅为核心片段，完整版工程已整理好。 关注公众号 【<strong>3GPP仿真实验室</strong>】进行获取。</p><h2>📚 参考文献</h2><ol><li><strong>T. Mao et al.</strong> (2017): "Dual-Mode Index Modulation Aided OFDM." <em>IEEE Access</em>, vol. 5.</li><li><strong>E. Başar</strong> (2015): "OFDM with Index Modulation Using Coordinate Interleaving." <em>IEEE Wireless Commun. Lett.</em>, vol. 4, no. 4.</li><li><strong>M. Wen et al.</strong> (2016): "Index Modulation Aided Subcarrier Mapping for Dual-Mode OFDM." <em>IEEE Trans. Commun.</em>, vol. 65, no. 12.</li></ol>]]></description></item><item>    <title><![CDATA[要给 OCR 装个脑子吗？DeepSeek-OCR 2 让文档不再只是扫描 小白狮ww ]]></title>    <link>https://segmentfault.com/a/1190000047597817</link>    <guid>https://segmentfault.com/a/1190000047597817</guid>    <pubDate>2026-02-06 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如何形容现在市面上普遍的 OCR 呢？可能你已经习惯了它的「固执」——无论文档布局多复杂，它总是老老实实从左到右、从上到下扫一遍。遇到双栏论文还好，碰上跨页表格或者公式脚注混排，输出结果往往乱得让人头疼。这不是识别不准，而是理解方式出了问题。</p><p>今年 1 月 DeepSeek 团队推出的 DeepSeek-OCR 2 换了个思路，它不再把文档当成一张平面图，而是尝试理解这篇文章应该先读什么。新设计的 DeepEncoder V2 架构引入了因果流机制：视觉编码器看完整个页面后，由专门的查询模块决定阅读顺序——标题优先于正文，表格注释紧跟数据，公式按逻辑展开而非按位置罗列。</p><p>结果很直接。OmniDocBench 最新测试中，这套方案把整体准确率推到了 91% 以上，公式识别的提升尤为明显。更实用的是，它输出的 Markdown 已经带着层级结构，省去了大量后期整理的功夫。</p><p>参数规模控制在单卡能跑的级别，token 上限可调，重复生成的情况也比上一代少了近三分之一。对于需要批量处理文档的场景，这意味着可用性的大幅提升。</p><p>当一个模型能够同时看懂版式、识别文字并直接输出结构化结果，文档数字化的目标就不再只是「能认字」，而是「能理解」。DeepSeek-OCR 2 正是在这一方向上的一次重要尝试。</p><p><strong>教程链接：</strong> <strong><a href="https://link.segmentfault.com/?enc=vOBM7hulHWZ%2BSXDpbhIRUg%3D%3D.bkaPW3TsPezYvHWeMBLcUxY25qrlxQJcbut%2Ftdq5rnU%3D" rel="nofollow" target="_blank">https://go.openbayes.com/NOdm2</a></strong></p><p>使用云平台: OpenBayes</p><p><a href="https://link.segmentfault.com/?enc=ohR9%2BXgR5FP47k8gzuPpnA%3D%3D.CAm4uMzFFklUnTbp0dmVH4jS2PjenVm1ecshMHPkUjIuGyFNi9js%2BxGJuXP2BQ2G" rel="nofollow" target="_blank">http://openbayes.com/console/signup?r=sony_0m6v</a></p><p>首先点击「公共教程」，找到「DeepSeek-OCR 2：视觉因果流」，单击打开。</p><p><img width="723" height="543" referrerpolicy="no-referrer" src="/img/bVdnROZ" alt="" title=""/></p><p>页面跳转后，点击右上角「克隆」，将该教程克隆至自己的容器中。</p><p><img width="723" height="540" referrerpolicy="no-referrer" src="/img/bVdnRO0" alt="" title="" loading="lazy"/></p><p>在当前页面中看到的算力资源均可以在平台一键选择使用。平台会默认选配好原教程所使用的算力资源、镜像版本，不需要再进行手动选择。点击「继续执行」，等待分配资源。</p><p><img width="723" height="541" referrerpolicy="no-referrer" src="/img/bVdnRPa" alt="" title="" loading="lazy"/></p><p><img width="723" height="538" referrerpolicy="no-referrer" src="/img/bVdnRPd" alt="" title="" loading="lazy"/></p><p>若显示「Bad Gateway」，这表示模型正在加载中，请等待约 2-3 分钟后刷新页面即可。</p><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnRPe" alt="" title="" loading="lazy"/><br/><strong>使用步骤如下：</strong></p><ol><li>页面跳转后，点击左侧 README 页面，进入后点击上方「运行」。</li></ol><p><img width="723" height="541" referrerpolicy="no-referrer" src="/img/bVdnRPf" alt="" title="" loading="lazy"/></p><ol start="2"><li>点击运行后等待加载模型与初始化</li></ol><p><img width="723" height="540" referrerpolicy="no-referrer" src="/img/bVdnRPg" alt="" title="" loading="lazy"/></p><ol start="3"><li>待运行完成，即可点击右侧 API 地址跳转至 demo 页面。</li></ol><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnSwG" alt="" title="" loading="lazy"/></p><ol start="4"><li>上传所需要的 JPG/PNG 格式的图片或 PDF 文档。</li></ol><p><img width="723" height="503" referrerpolicy="no-referrer" src="/img/bVdnRPj" alt="" title="" loading="lazy"/></p><ol start="5"><li>上传完成后点击运行，稍等片刻右侧结果框生成纯文本结果。</li></ol><p><img width="723" height="241" referrerpolicy="no-referrer" src="/img/bVdnSwS" alt="" title="" loading="lazy"/><br/><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=FQ9U6vgoAxXYvEl7XDAtkA%3D%3D.08unyvdwzHCeOZiwPbtbFtgcr5XNoYLew3JBAEubX7w%3D" rel="nofollow" target="_blank">https://go.openbayes.com/NOdm2</a></em></strong></p>]]></description></item><item>    <title><![CDATA[《Render Graph与光追API融合应用指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597395</link>    <guid>https://segmentfault.com/a/1190000047597395</guid>    <pubDate>2026-02-06 18:18:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>URP以轻量化、跨平台为核心诉求，在资源调度上追求极致精简，适配移动端、主机等多终端的硬件限制；而HDRP则聚焦高清渲染，在光照计算、材质表现上堆砌复杂逻辑，专为高端PC与次世代主机打造。这种定位差异导致两条管线在核心架构、资源管理、效果实现上形成难以逾越的壁垒，开发者往往需要为不同管线单独构建内容、适配逻辑，既增加了开发成本，也让跨平台体验的一致性大打折扣。共享Render Graph与统一光线追踪API的出现，并非简单的功能叠加或参数调优，而是对渲染逻辑的深层重构，其核心在于构建一套脱离管线专属限制的通用渲染语言。这种语言让HDRP的高品质光照计算不再依赖专属的管线架构，而是能通过Render Graph的资源适配与调度优化，以符合URP性能基线的方式落地；同时，URP的跨平台灵活性也不再局限于基础渲染能力，而是能通过统一光线追踪API，承载HDRP级别的复杂场景光照交互。在长期的技术探索中发现，管线间的差距本质上是资源管理逻辑与效果计算范式的割裂—URP为追求效率简化了资源依赖解析，HDRP为实现品质强化了专属计算模块，而共享Render Graph通过对渲染流程的节点化抽象，将资源分配、Pass调度、依赖解析等核心逻辑抽离为独立层，让两条管线能基于同一套底层规则管理资源；统一光线追踪API则打破了光照计算的管线专属限制，让实时光线追踪从HDRP的“高端配置”转变为可根据硬件能力动态适配的“通用功能”。这种转变要求开发者跳出“为URP添加高清功能”或“为HDRP做性能裁剪”的传统思维，转而从渲染本质出发，让两条管线基于同一套核心逻辑，按需组合渲染模块，实现从移动端到高端PC的无缝能力伸缩，最终达成“性能与品质并行不悖”的渲染目标。</p><p>共享Render Graph的核心价值，在于构建了一套标准化的渲染资源语义映射体系，让URP与HDRP能精准理解彼此的资源描述规则，从而实现跨管线的资源复用与流程互通，彻底改变了传统管线中资源壁垒林立的局面。在传统开发模式中，URP为适配移动端硬件，采用紧凑的纹理压缩格式与精简的缓冲布局，而HDRP为追求高清表现，使用高精度纹理与复杂的缓冲结构，这种差异导致同一材质资源在两条管线中需要重复构建适配逻辑，不仅增加了开发工作量，还容易出现资源不一致的问题。共享Render Graph通过抽象资源的核心属性描述与使用场景，将资源的具体实现细节与上层渲染逻辑解耦—无论管线对资源的精度要求、压缩格式有何差异，都能通过统一的资源接口进行调用，Render Graph则在底层自动完成适配转换。例如，在处理复杂场景的多Pass渲染时，HDRP为实现全局光照计算生成的高精度光照贴图，可通过Render Graph的资源适配层，自动转换为URP可高效采样的压缩格式，无需额外添加格式转换Pass，既减少了性能开销，又保证了光照效果的一致性；同样，URP针对移动端优化的纹理流式加载逻辑，也能被HDRP复用，在高清场景中根据硬件内存情况动态加载纹理资源，有效降低内存占用压力。更重要的是，Render Graph的节点化架构让渲染流程具备了模块化重组能力，HDRP中包含的环境光遮蔽、屏幕空间反射、体积雾等复杂后处理链路，可拆分为独立的功能节点，URP可根据自身性能预算，选择性启用核心节点，省略高精度计算步骤，无需重新设计整套后处理管线。这种模块化复用不仅大幅降低了两条管线的功能差距，还提升了开发效率—开发者只需维护一套核心渲染流程节点，即可通过Render Graph的适配逻辑，自动适配两条管线的性能与品质需求，让URP的渲染效果向HDRP靠拢，同时保持自身轻量化的核心优势。在实际技术探索中还发现，Render Graph的资源依赖解析能力，能有效解决跨管线渲染中的资源冲突问题，例如当两条管线同时调用同一材质资源时，Render Graph会根据当前管线的渲染上下文，自动分配对应的资源实例，避免出现资源竞争或格式不兼容的情况，进一步强化了管线间的协同能力。</p><p>统一光线追踪API的关键突破，在于实现了光照计算的范式归一，让URP与HDRP能基于同一套光线行为描述逻辑，达成光照效果的一致性与性能的差异化适配，彻底改变了此前两条管线光照表现泾渭分明的局面。在此之前，HDRP的光线追踪依赖专属的光照计算管线，支持复杂的光线反弹、材质交互与全局光照采样，能呈现出逼真的阴影、反射与折射效果，但计算开销巨大，仅能在高端硬件上运行；而URP受限于性能预算，无法承载完整的光线追踪计算，仅能通过屏幕空间近似算法模拟简单光照效果，导致两条管线的光照表现存在本质差距，同一场景在不同管线中呈现出截然不同的视觉质感。统一光线追踪API通过抽象光线的发射、相交、着色等核心行为，构建了一套与管线无关的光照计算模型，让光线追踪的核心逻辑脱离管线专属实现，成为一套可灵活适配的通用能力。在实际应用中，这套API会根据管线的性能目标与硬件能力，动态调整计算精度与采样策略：在HDRP中，光线可支持多轮反弹与复杂材质采样，充分发挥高端PC与次世代主机的计算潜能，呈现出电影级的光照质感；在URP中，则通过一系列智能化优化—如光线采样策略动态调整，优先采样对视觉影响显著的区域；反弹次数自适应控制，根据场景复杂度与硬件性能动态调整反弹轮次；加速结构简化，采用更紧凑的空间划分算法—在保证光照效果合理性的前提下，将计算开销控制在移动端与中端PC可承受的范围。这种适配并非简单的参数削减，而是基于硬件能力的智能决策，例如在移动设备上，API会自动将全局光线追踪转为局部关键区域的光线查询，结合屏幕空间信息补全光照细节，让URP的光照表现既符合性能要求，又能无限接近HDRP的视觉质感；在中端PC上，则可启用有限次数的光线反弹，平衡效果与性能。此外，统一API还实现了光照数据的跨管线互通，HDRP中烘焙的光线追踪加速结构，可通过API的适配层转换为URP可高效使用的简化版本，减少重复计算开销，让两条管线在光照计算上实现能力同源，进一步缩小了视觉差距。</p><p>场景描述体系的统一，是缩小URP与HDRP差距的重要支撑，共享Render Graph与统一光线追踪API共同构建了一套可跨管线解析的场景语义规范，让复杂场景的描述不再依赖特定管线的专属逻辑，实现了场景资源的一次创建、多管线复用。在传统开发流程中，HDRP的复杂场景通常包含大量高精度几何信息、分层材质属性与全局光照参数，这些信息往往针对HDRP的渲染架构进行优化，无法直接被URP解析，导致同一场景在两条管线中需要重新配置—URP需简化几何模型、削减材质层数、调整光照参数，不仅耗时耗力，还容易导致场景效果失真；而共享Render Graph通过对场景元素的结构化描述，将几何数据、材质属性、光照信息等拆分为独立的语义单元，每条管线可根据自身能力解析对应的语义层级，无需对场景资源进行破坏性修改。例如，HDRP中使用的多层材质，包含基础颜色、粗糙度、金属度、次表面散射等多个属性层，在URP中，Render Graph会通过语义适配，自动提取基础颜色、粗糙度等核心属性，忽略次表面散射等高精度细节，同时保留材质的核心视觉特征，让材质在URP中既符合性能要求，又能保持与HDRP一致的视觉风格；而URP中的简化几何模型，在HDRP中可通过API自动补充细节层次，如添加高模细节贴图、启用几何细分，满足高清渲染需求。统一光线追踪API则进一步强化了场景的光照交互一致性，无论是URP的轻量化场景还是HDRP的高精度场景，光线与物体的相交判定、材质反射计算都遵循同一套语义规则，确保在不同管线中，光照对场景氛围的影响保持一致—例如同一光源照射下，物体的阴影形状、反射强度、颜色衰减在两条管线中呈现出高度统一的效果，避免了跨管线体验的割裂感。这种场景语义的统一，让开发者无需为两条管线单独构建场景资源，只需维护一套核心场景描述，Render Graph与光线追踪API会自动完成适配转换，大幅降低了跨管线开发的复杂度；同时，场景资源的复用也让URP的场景表现力得到显著提升，原本只能在HDRP中呈现的复杂场景细节，如今可通过语义适配在URP中高效呈现，进一步缩小了两条管线的视觉差距。</p><p>着色器生态的协同演进，是弥合URP与HDRP差距的关键环节，共享Render Graph与统一光线追踪API为两条管线提供了可互通的着色器开发框架，让高品质着色逻辑能在两条管线中高效复用，彻底改变了此前着色器开发“管线专属”的局面。在此之前，HDRP的着色器支持复杂的次表面散射、屏幕空间反射、多层材质混合等高级效果，这些效果的实现依赖HDRP专属的光照计算管线与资源调度逻辑；而URP的着色器受限于性能预算，往往只能实现基础的PBR光照计算，导致同一材质在两条管线中视觉差异显著—HDRP中材质表现细腻、光影过渡自然，而URP中材质效果单薄、细节缺失，严重影响了跨管线体验的一致性。共享Render Graph通过模块化着色器设计，将着色逻辑拆分为独立的功能单元，每个单元可根据管线的性能与画质需求，动态调整计算复杂度，实现“一套逻辑、多端适配”。例如，HDRP中使用的PBR着色逻辑，可拆分为基础光照计算、高级材质交互、全局光照融合等模块：在HDRP中，可启用全部模块，实现复杂的材质表现；在URP中，则可选择性启用基础光照计算模块，同时通过统一光线追踪API补充关键光照细节（如高精度反射、软阴影），让URP的PBR表现兼具性能与质感，与HDRP的视觉差异大幅缩小；而HDRP也可复用URP中针对移动端优化的纹理采样模块，通过更高效的采样算法提升高清场景的纹理加载效率，减少性能开销。这种模块化设计不仅实现了着色逻辑的跨管线复用，更让着色器的扩展能力大幅提升—新的着色效果只需开发一次，即可通过Render Graph的适配层自动适配两条管线的渲染架构，无需针对每条管线重复开发。在实际技术实践中发现，这种着色器生态的协同，让URP的材质表现力得到了质的飞跃：原本只能在HDRP中实现的复杂材质交互，如布料的漫反射衰减、金属的镜面反射变化，如今可通过统一API与模块化着色逻辑，在URP中以适配性能的方式呈现；同时，HDRP的着色器也因复用了URP的优化模块，在保持高品质的同时，资源占用与计算开销显著降低，实现了“品质不打折、性能更出色”的目标。</p><p>技术融合的深层价值，在于构建了渲染管线的弹性演进体系，让URP与HDRP不再是相互割裂、各自为战的发展路线，而是基于同一技术底座的差异化表达，实现了两条管线的双向赋能与协同升级。在这套体系下，两条管线的核心能力不再局限于初始定位，而是能随着底层技术的迭代同步升级：URP能持续吸收HDRP的高品质渲染技术，通过Render Graph的资源优化与统一API的性能适配，将其转化为自身的轻量化实现，不断提升跨平台场景的视觉表现力；而HDRP也能借鉴URP的跨平台优化经验，将移动端的高效资源调度、轻量化计算逻辑迁移过来，提升高清场景的资源利用效率与硬件适配范围。</p>]]></description></item><item>    <title><![CDATA[《Android瘦LTO与Swift集成层启动优化实战指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597399</link>    <guid>https://segmentfault.com/a/1190000047597399</guid>    <pubDate>2026-02-06 18:17:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Android的瘦LTO构建绝非传统编译优化的简单升级，而是通过对符号依赖的精准画像与模块关联的动态重构，在保留代码逻辑完整性的前提下，实现编译产物的结构化精简—它不再对全量代码进行无差别优化，而是聚焦启动阶段的核心执行路径，筛选出必须即时加载的关键符号与依赖单元，剥离非必要的冗余代码与关联引用，让应用启动时的代码加载体积与解析耗时实现双重压缩。而Swift重写Apple集成层的核心价值，在于用原生语言的语义特性替代跨语言适配的中间桥接链路，让集成层与Apple系统底层API形成直接的能力对接，消除启动过程中因语言转换、接口适配带来的延迟损耗。这两项技术的联动优化，并非单端独立的性能修补，而是跨平台架构下编译逻辑与集成层设计的深度协同—Android端通过瘦LTO优化启动时的代码加载效率，减少CPU在初始化阶段的计算压力；Apple端借助Swift的运行时优势压缩集成层的初始化链路，降低内存分配与系统调用的延迟，双端形成互补的优化闭环，从编译产物到运行时执行的全流程破解启动性能与跨平台兼容性的核心矛盾。这种优化思路跳出了“单点调优”的传统框架，聚焦跨端启动的本质痛点，通过编译层与集成层的双向革新，让启动性能的提升具备可复制的方法论与规模化落地的可能，为复杂跨平台应用的性能升级提供了全新的技术路径。</p><p>瘦LTO构建的核心竞争力，在于其对编译优化的精准化与高效化革新，它摒弃了全量LTO模式下资源密集型的全局优化逻辑，转而采用分层处理与关键路径聚焦的优化策略，在保证启动性能提升的同时，规避了全量优化带来的编译周期延长问题。在实际优化实践中，瘦LTO的落地需要先完成启动链路的全景解构—通过对应用启动流程的逐环节分析，明确初始化阶段必须加载的核心模块、服务依赖与调用关系，建立启动关键路径的可视化图谱。在此基础上，针对性配置瘦LTO的优化粒度：对于启动时即时初始化的核心服务，如基础配置加载、权限校验、核心功能初始化等模块，进行深度优化处理，包括合并重复调用逻辑、消除无效依赖引用、优化函数执行链路，让代码执行更紧凑高效；对于启动后才按需加载的功能模块，如非核心业务组件、设置页面、辅助工具等，则保持基础编译状态，仅进行必要的符号精简，避免过度优化带来的资源消耗。这种差异化优化策略，既确保了启动关键路径的加载效率，又控制了整体编译开销。在复杂应用场景中，瘦LTO还能与编译缓存机制形成高效协同—通过缓存优化后的中间产物，在后续迭代构建中仅对变更模块进行增量优化，大幅缩短编译周期，同时确保每次构建的优化效果一致性，让启动性能的提升具备稳定可复现的特性。这种精准化的编译优化思路，打破了“优化效果与编译效率不可兼得”的固有认知，实现了编译产物精简、加载效率提升、编译周期可控的三重增益，成为Android端启动性能优化的核心支撑。</p><p>Swift重写Apple集成层的优化逻辑，本质是通过语言原生特性与系统生态的深度耦合，重构跨平台能力的适配链路，彻底替代传统依赖中间桥接层的实现模式，从根源上消除跨语言适配带来的启动损耗。传统跨平台应用的Apple集成层，往往为了兼容多语言调用逻辑，引入大量的接口转换代码、数据格式适配模块与中间调度层，这些冗余链路在启动阶段会产生显著的性能开销—数据在不同语言类型间的转换消耗内存与CPU资源，中间层的调度延迟拉长了初始化周期，同时增加了系统调用的不确定性。Swift作为Apple生态的原生语言，具备与系统底层API的天然适配优势，能够直接调用核心系统能力，省去中间转换环节，让集成层的初始化逻辑更贴合系统的运行时调度机制，实现更高效的能力衔接。实践过程中，重写工作需聚焦两个核心维度：一是集成层的语义对齐，在保持跨平台核心能力一致性的前提下，用Swift的原生语法重构适配逻辑，最大化利用语言的内存管理特性—例如通过值类型优化减少启动时的内存分配与释放操作，避免引用计数带来的额外开销；利用函数派发优化提升调用效率，让核心接口的响应速度更快捷。二是初始化流程的拆分与延迟加载，将集成层的功能模块按启动优先级进行划分，仅保留核心能力的即时初始化，如基础配置适配、系统权限对接等必须在启动阶段完成的逻辑，而将非核心的适配功能，如统计上报、第三方服务对接等，通过懒加载机制延迟到启动完成后执行，进一步压缩启动耗时。这种原生适配的思路，让集成层从启动流程中的“阻滞点”转变为“助推器”，在保证跨平台兼容性的同时，实现了启动性能的质的飞跃。</p><p>Android瘦LTO构建与Swift重写Apple集成层的协同优化，核心在于构建覆盖跨端启动全流程的性能优化闭环，让双端的优化策略形成互补效应，而非孤立的单端升级。Android端通过瘦LTO构建，削减了启动时的代码加载体积与解析耗时，减少了CPU在初始化阶段的计算压力，让核心服务能够更快完成启动准备；Apple端借助Swift重写的集成层，压缩了跨语言适配的中间链路，优化了内存分配效率与系统调用延迟，让集成层的初始化更高效。这种双端协同并非简单的功能叠加，而是基于跨平台应用启动共性逻辑的深度适配—无论是Android的代码加载流程，还是Apple的集成层初始化链路，本质上都是对启动资源的调度与利用，两项技术分别从编译端与运行端切入，形成覆盖“编译产物优化-代码加载加速-集成层初始化精简-系统能力对接高效”的全流程优化体系。实践中，协同优化的落地需要先统一双端的启动性能优化目标，明确核心指标的基准线，例如启动完成时间、初始化阶段的CPU占用、内存峰值等，再根据双端的技术特性制定差异化的优化策略：Android端侧重通过瘦LTO实现编译产物的精简化，缩短代码加载与解析路径，同时优化启动时的资源调度优先级；Apple端聚焦通过Swift的原生优势压缩集成层的初始化链路，减少中间环节的性能损耗，提升系统API的调用效率。通过这种协同设计，跨平台应用能够在双端同时获得启动性能的跃升，避免单端优化导致的用户体验失衡，让不同设备上的启动流程都能保持流畅高效，真正实现跨端启动体验的一致性与高性能。</p><p>启动性能的深度优化，离不开对技术细节的精准把控与场景化的动态适配，瘦LTO构建与Swift重写的落地过程，并非一成不变的标准化流程，而是需要根据应用的实际场景与架构特点进行灵活调整。对于瘦LTO构建而言，优化粒度的选择是关键—过粗的优化会导致启动关键路径的优化不充分，无法达到预期的性能提升效果；过细的优化则可能引入不必要的编译开销，延长构建周期，甚至影响代码的稳定性。因此，在实际操作中，需要借助启动链路分析工具，精准定位每个模块在启动阶段的加载耗时、依赖关系与资源占用情况，建立模块级别的性能画像，再针对性配置优化范围：对启动时首先加载的核心框架，如基础库、路由管理、核心服务等，进行最大程度的优化，合并重复符号，消除循环依赖，优化函数执行逻辑；对后续按需加载的功能模块，如非核心业务组件、多媒体处理、扩展功能等，则采用轻量级优化策略，仅保留必要的符号与依赖，避免过度优化带来的资源消耗。在Swift重写集成层的过程中，集成层的拆分逻辑同样需要贴合应用的启动流程，将必须在启动阶段完成的适配逻辑，如基础配置同步、系统权限申请、核心能力对接等，与可延迟的功能解耦，通过懒加载机制将非必要的适配逻辑延迟到启动完成后执行。同时，需充分利用Swift的编译优化特性，如模块间的接口精简、无用代码自动剔除、编译期常量折叠等，让集成层的产物体积更小巧，加载更快速。这种场景化的精准优化，避免了“一刀切”的优化模式带来的局限性，让每项技术的优势都能在关键场景中充分发挥，实现启动性能的最大化提升，体现了技术优化从“广谱适配”到“精准赋能”的进阶思维。</p><p>瘦LTO构建与Swift重写Apple集成层的优化实践，其长远价值远不止于启动性能的即时提升，更在于为跨平台应用构建了可扩展、可迭代的性能优化体系与技术底座。瘦LTO带来的编译链路优化思路，不仅适用于启动性能的提升，还能延伸到应用运行时的内存占用控制、CPU效率优化与功耗降低—通过持续优化编译产物的结构，让代码执行更高效，资源利用更合理，为应用全生命周期的性能表现奠定坚实基础。而Swift重写的集成层，凭借语言的原生优势与系统兼容性，大幅降低了后续功能迭代的适配成本与维护难度——Swift与Apple系统的深度耦合，让集成层能够快速响应系统版本的更新与API的迭代，无需频繁进行跨语言适配调整；同时，原生代码的可读性与可维护性更强，减少了后续迭代中的技术债务。</p>]]></description></item><item>    <title><![CDATA[百度一见×赣南师大：多模态视觉扎根赣南，共筑产教融合新标杆 百度一见 ]]></title>    <link>https://segmentfault.com/a/1190000047597441</link>    <guid>https://segmentfault.com/a/1190000047597441</guid>    <pubDate>2026-02-06 18:16:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，赣南师范大学代表团莅临百度，双方正式签署校企合作战略协议。这不仅是一场强强联手的签约，更是前沿AI技术与深厚学术积淀的一次“握手”。当“AI for Science”遇上“产教融合”，百度与赣南师大正联手开启智能时代复合型人才培养的新篇章。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597443" alt="图片" title="图片"/><br/>在座谈环节，双方达成高度共识：在智能时代，AI素养已不再是人才的“加分项”，而是“必选项”，产教融合势在必行。百度一见产品部总经理朱名发详细分享了一见在多模态大模型领域的战略布局，以及在能源、制造、连锁、运输等行业的产业实践。“要把最前沿的技术，转化为课堂上的生产力。” 在热烈的氛围中，赣南师范大学党委常委、副校长罗序中与百度代表双方签署协议。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597444" alt="图片" title="图片" loading="lazy"/><br/>随后，校方代表团走进百度展厅，近距离感受百度文心大模型赋能千行百业的实战场景。从实验室的算法到产业实践里的深度应用，双方对“AI+教育”的未来达成了高度共识。朱名发总经理强调：“大模型时代，具备AI素养、能熟练运用AI工具提升效率的复合型人才，已成为企业的首选。我们希望通过合作，让学生在校期间就掌握‘AI生产力’，赋能在就业市场具备核心竞争优势。”<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597445" alt="图片" title="图片" loading="lazy"/><br/>罗序中副校长对此深表认可。他表示，作为江西省“双一流”建设高校，赣南师大拥有扎实的AI学科基础，百度一见在视觉管理领域的深厚积淀，将为学校科研创新注入强劲动力。双方将通过产教融合，加速成果转化，联合培养懂产业、精技术的实战型人才。拒绝纸上谈兵，直击地方痛点。 依托国家脐橙工程技术研究中心等国家级平台，双方明确将“赣州脐橙智能化种植”与“赣州电子制造”作为首批科研攻关方向。通过百度一见的多模态专业视觉技术赋能，双方将合力打造具有全国影响力的应用标杆，真正将产教融合的实践“写”在赣南大地的田间地头与工厂车间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597446" alt="图片" title="图片" loading="lazy"/><br/>签约只是起点，赋能才是目标。未来，百度一见将持续以技术创新为核心，深度融入赣南师大的教学与科研土壤。从实验室的创新火花，到产业界的落地成果，双方将共同探索AI赋能实体经济的新路径。当“AI新范式”扎根老区沃土，一场关于人才、科研与产业的化学反应，正在发生！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597447" alt="图片" title="图片" loading="lazy"/></p>]]></description></item>  </channel></rss>