<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[基于研发过程的漏洞治理及经验 aerfa21 ]]></title>    <link>https://segmentfault.com/a/1190000047579254</link>    <guid>https://segmentfault.com/a/1190000047579254</guid>    <pubDate>2026-01-28 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>很久以前，就想写一篇关于SDL与DevSecOps的文章，但疏于实践一直未能动笔。想写的原因很简单，因为总是听到有人说SDL落后、DevSecOps相关技术更高超。一提到研发安全建设，不分研发模式都在赶时髦一样地说DevSecOps。从我的观察来看，不结合研发模式来做研发安全，都是不成功的。</p><p>在数字化浪潮的推动下，一些公司已经完全步入DevOps模式，有的则出现瀑布、敏捷或DevOps并存，且后者是居多的。所以如何在多种研发模式下进行有效的研发安全建设，成为一个必须解决的难题。经过近十年的实践，终于在探索解法上有一点点收获与经验，于是有了“<strong>深耕研发安全</strong>”这一系列文章。</p><p>在上一篇中，找到了研发安全的切入点，按照常规思路就应该想出对应的解决之道。本文将深入“架构-编码-配置 + 应急响应”，针对漏洞生产源，提出治理的实践方法及经验。</p><p><img width="723" height="253" referrerpolicy="no-referrer" src="/img/bVdnNGu" alt="图片" title="图片"/></p><p><strong>01 架构设计缺陷</strong></p><p>在设计阶段要关注安全需求是否在设计中体现，对设计进行评审以发现其他潜在的安全风险，涉及的安全活动主要是：</p><p><img width="422" height="161" referrerpolicy="no-referrer" src="/img/bVdnNGv" alt="图片" title="图片" loading="lazy"/></p><ul><li>安全需求纳入检视：部分安全性需求检查的第一道关卡，需要在设计中体现出来。通过Excel表格反馈+word证据截图或问卷调研的方式，对项目中的安全需求落实情况进行review。可以采取业务方收集证据反馈，架构师或跨业务方架构师检查，安全团队抽查的方式（前提是安全团队联动公司级架构师团队或技术委员会，将安全检查融入日常工作中，让其承担一部分安全的职责）；</li></ul><ul><li>产品架构安全评审：对于重要产品、产品的高危功能等应该特别关注的产品，额外进行架构安全评审。通常可以使用攻击树的分析方法（安全人员更加擅长，更高效发现可能的攻击点），与业务方开发等人员进行安全评审，重点在发现安全风险并治理、以及阻断攻击链。</li></ul><p><strong>02 编码忽略安全</strong></p><p>在代码层面引入的安全问题，有比较多的治理方式。比如从检查方面来说，可以对引入的第三方组件进行漏洞和后门检查，对自研的代码进行漏洞扫描；从预防或左移的角度来说，可以制定并推广安全编码规范、安全组件，想办法让开发避免引入漏洞。以下是一些常见做法及经验：</p><p><img width="723" height="265" referrerpolicy="no-referrer" src="/img/bVdnNGw" alt="图片" title="图片" loading="lazy"/></p><ul><li>编码安全规范：网上有不少公开的安全编码规范，比如一些互联网大厂、云平台及开发社区。如果解决合规（过检查），可以完全照搬；但要是想真正解决问题，则应该进行部分参考，大概占比可达20%~30%。剩余部分应该根据历史安全测试的结果、日常运营过程中遇到的安全问题等实际已发生的风险进行制定，类似输出OWASP Top 10类别，因为多了不一定能够落地，先出一个版本再优化迭代；</li></ul><ul><li>静态代码扫描：理论上可以联动编码安全规范，检查其是否落地及闭环。将规范中的内容逐一落到SAST工具的检测规则上，从技术层面真正的做到规范检查，效果要比安全培训、培训后考试更好。但很多公司的代码扫描还是先关注高危漏洞，规范规则的扫描可以排在第二顺位。第二想介绍的是自动化，静态代码扫描是比较好与研发工具（如gitlab、jenkins）联动，开发提交代码就触发扫描，扫完就推动漏洞给研发并提醒修复。第三是误报，所有工具的检测规则都需要调优，常见的有结合业务特点写增强型规则、普适性规则在一些场景中误报则要加白…，一定是要投人运营才能降低误报率及提升检测能力。此外，研发安全团队一定要先优化规则，再要求或推动业务方修复漏洞，因为SAST误报真的是非常多，业务方会认为安全不专业、是麻烦事儿，以至于产生抵触情绪、不便开展后续的工作；</li></ul><ul><li>开源组件扫描：主要存在两个安全问题。第一个是漏洞，开源组件的CVE漏洞特别多，但是目前市面上的检测工具基本上都是先拆包、然后根据指纹和CVE漏洞库碰撞，只要版本匹配就会报存在漏洞，所以带来的误报会非常多，真正受影响的情况特别少。不过在一些监管属性比较强的行业，只要是工具报的高危漏洞都要求处理。如果是误报给出依据，若是真的漏洞则要去修复；相比在互联网这种宽松的氛围下，基本都是实际带来了可利用风险，才会去处理。无论什么行业，对于SCA工具扫描的进一步研判，都是行业中所需要的安全能力。第二个问题是开源组件投毒，如 Python或npm源管理比较松散，就会出现组件包伪造、源账号攻击等方式进行投毒。最佳方法是对源进行统一管控，但大多数公司都做不到。不过即使做到了，当首次引入时同样会面临检测的问题，然而这是绝大多数公司缺少（静态特征检测、动态跑沙箱做行为分析），基本只能依靠威胁情报进行响应；</li></ul><ul><li>安全组件建设：不仅是单纯的指实现安全效果的CBB，也可以是经过安全性改良/定制的开发框架。这项活动的实施难度最大，一是要有懂安全的开发人员或会开发的安全人员支撑，能够将常见的文件操作、输入输出处理、数据库操作等常规操作会发生的安全问题梳理清楚，结合开发框架或开发语言来写组件或改良框架；二是推广应用的问题，基本只是适合于新的项目，因为已上线系统发现漏洞后去改框架或换实现方法非常麻烦，不会有业务同意这么干。所以就只能瞄准新系统，亦可以把该项前置到需求或设计阶段，要求业务方使用。</li></ul><p><strong>03 配置发生错误</strong></p><p>在产品发布与部署阶段，不合规的方式或不良操作习惯，可能带来一些安全问题。不过如果具备统一的发布和部署能力，则可以规避很多潜在风险，只需关注“源头”。尤其是针对PASS层的软件：</p><p><img width="723" height="194" referrerpolicy="no-referrer" src="/img/bVdnNGx" alt="图片" title="图片" loading="lazy"/></p><ul><li>安全配置基线：属于基础安全的范畴，但基础不牢真会地动山摇。去年之前我们集中力量投入到应用层的安全性检测，默认了业务线给出的内部微服务有gateway管控、内部数据库、大数据组件服务有iptables的说辞，尤其是对基础服务投入很少。随之而来从SRC收到很多关于pg硬编码、版本过低可提权、redis未授权获取root权限等漏洞，从而导致产品被攻陷。比较有效的治理方式是基于攻击视角的安全基线，众所周知CIS安全基线比较全，但实施的时候就会比较麻烦，所以需要按照攻击思路来精简。如针对Redis，关注启动服务不使用root权限、设置账密验证或在配置文件中指定访问IP源、禁用可以执行系统命令的函数，这并非绝对或死板执行，需要根据业务实际情况进行调整，原则就是常说的最小化（服务最小访问、权限最小）等；</li></ul><ul><li>黑盒漏洞扫描：定期对操作系统镜像模板、最小化容器模板、数据库模板、大数据开源组件模板等进行黑盒扫描并修复漏洞，以检验默认配置执行情况；在测试阶段再次进行主机漏洞扫描和web漏洞扫描，发现近期暴露出的漏洞及配置类漏洞，以保证基础软件默认安全。</li></ul><p><strong>04 应急响应托底</strong></p><p>上述内容都做好了，是不是产品就没有漏洞呢？答案是否定的，就如没有绝对的安全一样，投入的资源越多、被外部发现的概率就会越小，但永远不会出现无漏洞。其中，有两个主要的原因：</p><ul><li>漏洞是动态的：产品使用到的开源组件现在没有漏洞，但在未来可能被爆出存在可利用的漏洞，故此产品也会受到牵连；</li><li>SDL并不是万能的：通常在研发安全体系中，会出现安全工具能力不足或被bypass、安全测试或开发人员出现纰漏等问题，导致漏洞未被发现。</li></ul><p>所以需要建设预警和响应机制，进行托底式的快速响应。在预警部分，通过资产管理摸清产品中使用到开源软件和组件，对外部情报源如开源软件官网、安全公司威胁情报、安全微信群、安全媒介等进行监控并告警；在响应部分，需要设置跨组织的应急响应团队（如安全、业务线、公关、法务、交付等）、流程和响应要求，以应对突发的产品安全事件。由此保障这些组织、流程、机制等的正常运转，才能做到相对可控。</p><p>本文首发于微信公众号：我的安全视界观</p>]]></description></item><item>    <title><![CDATA[Claude Code子代理实战：10个即用模板分享 本文系转载，阅读原文
https://avoi]]></title>    <link>https://segmentfault.com/a/1190000047579208</link>    <guid>https://segmentfault.com/a/1190000047579208</guid>    <pubDate>2026-01-28 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你认为Claude Code 的使用流程就是随手丢一句话，然后就等结果那你就错了。</p><p>比如你对Claude Code 说</p><blockquote>"重构这段代码，找出bug，写测试，优化性能，顺便解释一下。"</blockquote><p>你可以看到它确实在努力，但结果一塌糊涂：可能在重构动了业务逻辑，解释写了一半就没了下文了，而且测试跟项目框架对不上，性能建议也全是泛泛而谈的套话。</p><p>这是因为真正的团队不是这么协作的，没有哪个工程师会同时扮演测试、安全审查、重构专家、文档撰写这么多角色，而你需要的是Claude Code子代理。</p><h2>子代理到底是什么</h2><p>简单的说子代理就是给AI指定一个专门的角色。不再说"帮我搞定所有事"，而是明确告诉它："你现在是测试员"、"你负责安全审查"、"你是重构专家"。</p><p>每个代理只负责一件事，遵循固定的规则，输出可预期的结果。与其说是在写提示词不如说是在组建AI小分队，然后让每个成员各司其职。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047579210" alt="" title=""/></p><p>切换到子代理之后，输出质量稳定多了，对AI建议的信任度也上来了。调试效率提升明显，代码审查的质量终于有点"老司机"的味道。</p><p>下面是我实际在用的10个子代理，这些模板可以直接拿去用。</p><h2>1、代码重构</h2><p>这是创建的第一个子代理，也是到现在还是用得最多的一个。适用场景包括历史遗留代码、臃肿的Flutter组件、写得很难看的Node.js服务。</p><pre><code> You are a Code Refactoring Sub-Agent.\  
 Rules:  
 - Do NOT change business logic  
 - Improve readability and naming  
 - Remove duplication  
 - Keep output language the same  
 Input: Code snippet  
 Output: Refactored code + short explanation</code></pre><h2>2、Bug分析与修复</h2><p>专门对付那些语焉不详甚至带着情绪的bug报告 😅</p><blockquote>"应用有时候会崩溃"</blockquote><p>有时候是什么时候？崩溃前在干嘛？这些信息全没有。</p><pre><code> You are a Bug Analysis Sub-Agent.  
 Steps:  
 1. Identify root cause  
 2. Explain how to reproduce  
 3. Suggest minimal fix  
 4. Mention side effects  
 Never guess. Ask if info is missing.</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579211" alt="" title="" loading="lazy"/></p><h2>3、测试用例生成</h2><p>重复性的测试代码写起来实在无聊。这个代理不会觉得烦。</p><pre><code> You are a Test Generation Sub-Agent.  
 Requirements:  
 - Cover edge cases  
 - Include positive and negative tests  
 - Follow existing test framework  
 - No unnecessary mocks  
 Output: Test code only</code></pre><h2>4、API契约审查</h2><p>这个代理可以解决"改了后端结果前端炸了"的坑</p><pre><code> You are an API Design Reviewer Sub-Agent.  
 Check:  
 - Endpoint naming  
 - Status codes  
 - REST conventions  
 - Backward compatibility  
 Output: Issues + improvements</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579212" alt="" title="" loading="lazy"/></p><h2>5、 安全审查</h2><p>凡是涉及认证相关的代码，推送之前必跑一遍这个。</p><pre><code> You are a Security Review Sub-Agent.  
 Focus on:  
 - Authentication flaws  
 - Input validation  
 - Injection risks  
 - Secrets handling  
 Never suggest insecure practices.</code></pre><h2>6、文档编写</h2><p>文档是写给人看。</p><pre><code> You are a Technical Documentation Sub-Agent.  
 Rules:  
 - Simple language  
 - Use examples  
 - Short sections  
 - No marketing fluff  
 Output: Markdown documentation</code></pre><h2>7、性能优化</h2><p>用户反馈"卡"的时候就派这个上场。</p><pre><code> You are a Performance Optimization Sub-Agent.  
Analyze:  
- Time complexity  
- Memory usage  
- I/O bottlenecks  
Output:  
- Issue  
- Cause  
 - Optimized solution</code></pre><h2>8、产品经理</h2><p>这个代理会像资深产品工程师那样思考问题，评估用户影响、权衡取舍、寻找更简单的替代方案，还会考虑长期维护成本。</p><pre><code> You are a Product Thinking Sub-Agent.  
 Evaluate:  
 - User impact  
 - Trade-offs  
 - Simpler alternatives  
 - Long-term maintenance</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579213" alt="" title="" loading="lazy"/></p><h2>9、代码审查</h2><p>相当于有个沉稳的老程序员在review你的PR。</p><pre><code> You are a Senior Code Reviewer Sub-Agent.  
 Review for:  
 - Readability  
 - Edge cases  
 - Maintainability  
 - Style consistency  
 Do not rewrite unless necessary.</code></pre><h2>10、架构决策</h2><p>面对太多选择不知道怎么选的时候，可以让这个代理来帮忙梳理。</p><pre><code> You are an Architecture Decision Sub-Agent.  
 Output:  
 - Available options  
 - Pros &amp; cons  
 - Recommendation  
 - Risks &amp; mitigation</code></pre><h2>总结</h2><p>大而全的提示词容易让AI过载。子代理有效的原因是专注比聪明更重要，约束反而能提升质量，专业分工减少犯错的机会。</p><p>这其实就是真实工程团队的协作逻辑。Claude Code子代理改变了我写代码的方式。不是因为它多酷炫而是因为实用。</p><p>如果你也在用AI辅助开发，却总是被乱七八糟的输出折腾，问题可能不在于怎么问得更好，而在于怎么分工。</p><p><a href="https://link.segmentfault.com/?enc=Gz4eChDfj1LYYYXANX1hag%3D%3D.YZsRYp9VaOyRkqkNBQ%2FlSj4uf4GUjL9g6XS0rg03yCq8lXQIrMINCfl8rTzR9YvtU%2BEvCDFrKahXND6T%2BOs4zQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/fe83dba0f1d24989ae48d724208212bc</a></p><p>by Er Alice Paul</p>]]></description></item><item>    <title><![CDATA[重构认知——AI智能体来了从0到1的落地工程全指南 你的橙来啦 ]]></title>    <link>https://segmentfault.com/a/1190000047579125</link>    <guid>https://segmentfault.com/a/1190000047579125</guid>    <pubDate>2026-01-28 20:02:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><strong>摘要：</strong> 当大模型从“对话框”走向“行动力”，AI智能体（Agent）成为了连接通用智能与行业价值的核心载体。本文将打破单纯的“调参”思维，从感知、决策、执行、记忆四大底层架构出发，系统性梳理智能体开发的“五步跃迁法”，助你从零构建具备行业深度与自主能力的数字生命。</blockquote><hr/><h2>一、 智能体之魂：从“文本交互”到“逻辑闭环”的蜕变</h2><p>在开发之前，我们必须明确：智能体不是更强的大模型，而是以大模型为<strong>大脑</strong>，协同规划、记忆与工具调用的<strong>闭环系统</strong>。</p><h3>1. 核心定义</h3><p>一个成熟的智能体必须具备以下三个维度的自主性：</p><ul><li><strong>感知边界：</strong> 能够解析多模态输入（文本、图像、语音、API数据）。</li><li><strong>决策机制：</strong> 基于推理引擎（LLM）进行任务拆解（Task Decomposition）。</li><li><strong>行动模式：</strong> 不止于“说”，更在于“做”（调用API、执行Python脚本、操作软件）。</li></ul><hr/><h2>二、 骨骼与神经：智能体的四层层级架构</h2><p>构建智能体如同拼装一台精密的机器，模块化的设计是保证后期可迭代性的关键。</p><table><thead><tr><th><strong>模块层级</strong></th><th><strong>核心组件</strong></th><th><strong>关键功能</strong></th></tr></thead><tbody><tr><td><strong>感知层 (Perception)</strong></td><td>文本编码器、多模态融合模块</td><td>接收外界信息，进行语义化清洗与结构化解析。</td></tr><tr><td><strong>认知层 (Cognition)</strong></td><td>LLM、推理策略（CoT/ToT）、反思机制</td><td>理解意图、规划路径，是智能体决策的中枢。</td></tr><tr><td><strong>记忆层 (Memory)</strong></td><td>短期记忆（上下文）、长期记忆（向量数据库RAG）</td><td>存储用户偏好、历史经验，实现跨时空的连续性。</td></tr><tr><td><strong>执行层 (Action)</strong></td><td>API集成、工具箱、外部环境交互</td><td>将决策转化为实际动作，完成物理或数字世界的反馈。</td></tr></tbody></table><hr/><h2>三、 实战进化论：智能体开发的“五步跃迁法”</h2><h3>第一步：锁定“可执行”的闭环场景</h3><p>拒绝开发“万能助手”，优先选择<strong>高频、高重复、规则明确</strong>的任务。</p><blockquote><strong>公式：</strong> 我是一个 [角色] 智能体，为 [目标用户] 在 [特定场景] 解决 [具体问题]。</blockquote><h3>第二步：搭建最小可行性原型 (MVP)</h3><p>利用简单的代码框架（如 Python）搭建基础骨架，验证核心逻辑。</p><p>Python</p><pre><code>class SimpleAgent:
    def __init__(self, brain_model):
        self.brain = brain_model
        self.memory = [] # 基础对话记忆

    def act(self, user_input):
        prompt = f"Context: {self.memory}\nTask: {user_input}"
        response = self.brain.generate(prompt)
        return response</code></pre><h3>第三步：注入“经验”与“工具箱”</h3><ul><li><strong>RAG技术：</strong> 接入行业知识库，解决大模型幻觉问题。</li><li><strong>工具调用：</strong> 赋予智能体“手”的能力。关键原则：<strong>工具使用应基于需求自主决策，而非预设死流程。</strong></li></ul><h3>第四步：异常处理与安全护栏</h3><p>真实的工程环境是多变的。必须建立：</p><ul><li><strong>重试机制：</strong> API失败自动重试（上限3次）。</li><li><strong>降级模式：</strong> 核心工具不可用时，返回部分结果+人工接管提示。</li><li><strong>安全限制：</strong> 涉及转账、删除等敏感操作需“人在回路”确认。</li></ul><h3>第五步：多维度评估与调优</h3><p>传统准确率已失效，建议采用五维评估体系：</p><ol><li><strong>任务完成度</strong> (Task Success Rate)</li><li><strong>交互自然度</strong> (Naturalness)</li><li><strong>响应耗时</strong> (Latency)</li><li><strong>鲁棒性</strong> (Robustness)</li><li><strong>道德对齐</strong> (Alignment)</li></ol><hr/><h2>四、 创作者的哲学：智能体不是工具，而是伙伴</h2><p>智能体开发的终极挑战不是技术实现，而是<strong>价值对齐</strong>。</p><ul><li><strong>设计潜意识：</strong> 你的提示词设计会嵌入智能体的“性格”。追求极致效率，它会变得功利；崇尚开放探索，它会更具创造力。</li><li><strong>从脚本到系统：</strong> 初始阶段解决80%常规情况，后续通过真实互动数据驱动持续进化。</li></ul><blockquote><strong>结语：</strong> 始于代码，不止于代码。每一行逻辑的背后，都是你对业务深度的理解。在这个AI平权的时代，掌握构建智能体的能力，就是掌握了未来数字工业的“架构师证书”。</blockquote>]]></description></item><item>    <title><![CDATA[FormatFactory_setup格式工厂怎么用？完整安装与使用指南（新手必看） 读书笔记 ]]></title>    <link>https://segmentfault.com/a/1190000047579135</link>    <guid>https://segmentfault.com/a/1190000047579135</guid>    <pubDate>2026-01-28 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p> <code>FormatFactory_setup</code>是<strong>格式工厂</strong>的安装包，能转视频、音频、图片、文档格式，还能剪辑视频、刻录光盘，功能挺全的。</p><p>安装不复杂，但有几个小地方要注意，下面一步步教你。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=QTE6dP61SLCEXtL3jwfByQ%3D%3D.VFI9Djf9N%2BZwt4W3hVZiEfOvIaz1MQLAzh6MOuXIVrVYjjwvx1shpBp1hiMn%2FARd" rel="nofollow" title="https://pan.quark.cn/s/bcbc28ab5468" target="_blank">https://pan.quark.cn/s/bcbc28ab5468</a></p></li><li><p><strong>用管理员身份运行</strong>​</p><ul><li>右键 <code>FormatFactory_setup.exe</code>→ 选“以管理员身份运行”，避免权限问题。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>FormatFactory_setup.exe</code>运行，进入安装向导，点  <strong>“下一步”</strong> 。</li><li>选“我同意此协议”→ 点“下一步”。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\Program Files (x86)\FormatFactory</code>，想改就点“浏览”选 D 盘或其他盘，点“下一步”。</li></ul></li><li><p>选附加任务：</p><ul><li>建议只勾“创建桌面快捷方式”，其他捆绑软件（如某某游戏、某某浏览器）<strong>千万别勾</strong>！</li><li>点“下一步”。</li></ul></li><li>点  <strong>“安装”</strong> ​ 开始安装，等进度条走完（大概几十秒）。</li><li>最后点  <strong>“完成”</strong> ​ 完成安装，桌面上会有格式工厂图标。</li></ol><h2>三、首次运行设置</h2><ol><li>双击桌面图标打开软件。</li><li>主界面会显示各种转换功能：视频、音频、图片、文档、光驱设备等。</li><li>选要用的功能，比如“视频”→“MP4”，然后添加文件就能开始转换。</li></ol><h2>四、基本使用（简单说两句）</h2><ul><li><strong>视频转换</strong>：选“视频”→“MP4”→ 添加视频文件 → 选输出文件夹 → 点“确定”→ 点“开始”。</li><li><strong>音频转换</strong>：选“音频”→“MP3”→ 添加音频文件 → 点“开始”。</li><li><strong>图片转换</strong>：选“图片”→“JPG”→ 添加图片 → 点“开始”。</li><li><strong>视频合并</strong>：选“视频”→“所有转到MP4”→ 点“选项”→“合并”→ 添加多个视频 → 点“开始”。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[清华+快手联合提出 FilmWeaver 框架，攻克多镜头视频生成一致性难题 Lab4AI ]]></title>    <link>https://segmentfault.com/a/1190000047578646</link>    <guid>https://segmentfault.com/a/1190000047578646</guid>    <pubDate>2026-01-28 19:11:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>清华+快手联合提出 FilmWeaver 框架，攻克多镜头视频生成一致性难题</h2><h3>01 论文概述</h3><p><strong>每一部电影都是一个由镜头编织的梦境，但今天的AI却困在“单帧梦境”里。</strong></p><p>尽管视频生成模型已能合成逼真的短片段，它们却难以讲述一个连贯的故事：当镜头切换，角色样貌会变幻不定，背景会突兀跳跃，叙事也会随之断裂。</p><p>这背后是<strong>两个根本的脱节</strong>：镜头之间缺乏记忆，导致角色与场景身份丢失；镜头内部缺乏流畅，使得运动生硬不连贯。现有方法或将多镜头压缩为单一序列，但这种方式牺牲了时长灵活性；或依赖复杂多模型管线的方法，这种方法会引入视觉断层。</p><p><img width="723" height="524" referrerpolicy="no-referrer" src="/img/bVdnNw6" alt="image.png" title="image.png"/><br/>支持多镜头序列的交互式创作示意图</p><p>为解决这一问题，<strong>清华大学深圳国际研究生院与快手Kling团队</strong>提出了FilmWeaver框架，其核心创新在于将一致性问题解耦为镜头间一致性与镜头内连贯性两个层面，并设计了一个<strong>双层缓存机制</strong>：</p><ol><li><strong>时间缓存（短期记忆）</strong>：记住当前镜头的近期画面，让动作、画面流畅不卡顿；</li><li><strong>镜头缓存（长期记忆）</strong>：保存之前镜头的关键信息，确保角色、背景跨镜头不 “变样”。</li></ol><p><a href="https://link.segmentfault.com/?enc=ObJSRDTl21%2Fhr6n3sWf5Rg%3D%3D.IrPZetk9HDel9nwDQpLmZFqiv6OSKgFxQ6MnoBK5d5eMnK31qZXbvinR2YlZTzlkt2whKaZz5tvd%2F1y3K6Ex2eh7OkVMEx6Bvi7yDbm8zwOGt8fQ6LCNWpW8RnGeuD0E5WEY4LjCiJ4d2MYhQ7l4Ow%3D%3D" rel="nofollow" target="_blank">模型</a>结合文本提示和这两种记忆来生成视频，核心就是让多镜头内容既连贯又统一。<br/><strong>论文名称</strong>：<strong>FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion</strong><br/><strong>论文链接</strong>：<a href="https://link.segmentfault.com/?enc=FKa3KbMnkHL1y0pQWcl3Kw%3D%3D.YmqMiCykPF3q1fgt8BlTaAsOOfSMsnY5hG7w3ul6GuT3LHkGfffmxYHUZdPyyBgA" rel="nofollow" target="_blank">https://arxiv.org/pdf/2512.11274</a><br/><strong>Github地址</strong>：<a href="https://link.segmentfault.com/?enc=19TNtZSGvu71zAnhZBlPvg%3D%3D.OuMA1tjVT8Ry52l9K14SFCYnegGsPRjuLUx7rubNHy0%3D" rel="nofollow" target="_blank">https://filmweaver.github.io/</a></p><p>👇扫码阅读论文，领H800算力<br/><img width="400" height="400" referrerpolicy="no-referrer" src="/img/bVdnNxi" alt="image.png" title="image.png" loading="lazy"/></p><h3>02 方法</h3><p>FilmWeaver的核心创新是 <strong>“自回归扩散 + 双级缓存”</strong> 的协同设计，通过 “解耦镜头间一致性与镜头内连贯性”，同时解决 “一致性” 与 “可控性” 问题，以确保能够生成任意长度和镜头数量的多镜头视频。</p><h4>1. 双层次缓存机制（解决问题的核心引擎）</h4><p>双级缓存分别负责 “镜头间长期一致性” 和 “镜头内短期连贯性”，且均通过上下文注入实现（无需修改模型架构，兼容性强）。</p><p><img width="723" height="413" referrerpolicy="no-referrer" src="/img/bVdnNxo" alt="image.png" title="image.png" loading="lazy"/><br/>FilmWeaver框架示意图</p><ul><li>时序缓存：负责镜头内连贯性。它是一个压缩的滑动窗口，存储当前镜头中刚生成的最新几帧的隐表示。窗口内的帧按时间远近进行不同程度的压缩（越近的保留越完整），从而以低成本保证动作流畅、无闪烁。</li><li>镜头缓存：负责跨镜头一致性。当需要生成一个新镜头时，系统会根据新提示词，从之前所有镜头的关键帧库中，通过CLIP语义相似度检索出最相关的K帧。这些帧作为视觉“锚点”，注入生成过程，确保角色、风格、背景的延续。</li></ul><h4>2. 四阶段推理流程（架构的动态工作模式）</h4><p>基于<strong>缓存的不同状态</strong>，我们的框架灵活支持四种生成模式，覆盖了从零开始创作到中途编辑的全流程：</p><ul><li>模式1（无缓存）：故事开篇，生成第一个镜头，并填充初始缓存。</li><li>模式2（仅时间缓存）：延伸当前镜头，用于制作长镜头或视频扩展。</li><li>模式3（仅镜头缓存）：开启新镜头，继承历史镜头的关键视觉元素，实现场景转换。</li><li>模式4（全缓存）：在新镜头中继续延伸，同时保持长期一致与短期流畅。</li></ul><p><img width="723" height="233" referrerpolicy="no-referrer" src="/img/bVdnNxs" alt="image.png" title="image.png" loading="lazy"/><br/>FilmWeaver多镜头生成流程示意图</p><h4>3.训练策略</h4><p>FilmWeaver的训练策略可概括为：采用<strong>两阶段渐进式课程学习</strong>，并结合针对性的<strong>数据增强</strong>，以稳定、高效地训练模型掌握双重缓存机制。其核心设计如下：</p><ol><li>两阶段课程：</li></ol><ul><li>第一阶段（学连贯）：仅启用<strong>时间缓存</strong>，训练模型生成长而连贯的单镜头视频，使其掌握镜头内的运动动力学基础。</li><li>第二阶段（学一致）：同时启用<strong>时间缓存与镜头缓存</strong>，在混合了四种推理模式的数据上对模型进行微调，使其学习在保持镜头内连贯的同时，实现跨镜头的视觉一致性。</li></ul><ol start="2"><li>关键增强策略：</li></ol><ul><li>负采样：在镜头缓存中随机引入<strong>无关关键帧</strong>，迫使模型学会根据提示词甄别有用信息。</li><li>非对称噪声注入：对镜头缓存施加强噪声以鼓励创新并防止“复制粘贴”；对时间缓存仅施加弱噪声以保护运动连贯性。此举有效缓解了模型对缓存的过拟合，显著提升了其文本提示跟随能力。</li></ul><h4>4.多镜头数据集构建</h4><p>论文构建的一个高质量多镜头视频数据集，开发了一套完整的数据构建流水线。该流水线主要包含以下步骤：</p><p><img width="723" height="489" referrerpolicy="no-referrer" src="/img/bVdnNxt" alt="image.png" title="image.png" loading="lazy"/><br/>多镜头数据整理流程图</p><ol><li>镜头切分：使用一个专家模型（如Panda-70M）将原始长视频分割成独立的镜头。</li><li>场景聚类：利用CLIP特征计算镜头间的相似度，通过滑动窗口聚类，将描述同一场景或事件的多个镜头聚合成一个<strong>多镜头序列</strong>。</li><li>分组标注：将同一个场景聚类中的所有镜头（通常2-5个）作为一个整体，输入给Gemini 2.5 Pro大语言模型，让它为所有镜头同时生成描述。这种“联合标注”策略是关键，它能强制模型在描述中保持同一角色外观、物体属性在不同镜头间的一致性。</li><li>验证与过滤：对生成的描述进行验证和精炼，并过滤掉过短（&lt;1秒）或人物过多（&gt;3人）的片段，以保证数据质量。<br/>对于<strong>评测</strong>，论文同样指出缺乏公开基准，因此作者使用 Gemini 2.5 Pro 根据一个精心设计的提示（要求生成包含5个镜头、角色描述严格一致的电影场景），构造了20个全新的多镜头叙事场景作为测试集。</li></ol><h3>03 实验效果</h3><h4>1.  定量结果</h4><p>论文在自建的多镜头测试集上，从 “视觉质量”、“一致性”和“文本对齐” 三个核心维度，将FilmWeaver与三类主流方法进行了全面量化对比。</p><p><img width="723" height="252" referrerpolicy="no-referrer" src="/img/bVdnNxu" alt="image.png" title="image.png" loading="lazy"/><br/>现有方法效果对比表格</p><ul><li>一致性：FilmWeaver在角色一致性和整体一致性两项指标上均取得最高分（74.61% 和 75.12%），显著领先其他方法。这直接证明了其双层缓存机制在维持跨镜头稳定性的有效性。</li><li>文本对齐：在角色层面的文本对齐指标上，FilmWeaver同样排名第一（23.07%），表明其能更好地根据提示词生成并保持特定角色特征。</li><li>视觉质量：FilmWeaver取得了最高的Inception Score，代表其生成内容的多样性和真实性最佳。虽然在美学评分上略低于StoryDiffusion，但在所有指标综合表现上最为均衡和突出。</li></ul><h4>2. 定性结果</h4><p><strong>场景一：多人对话（交替使用全景与特写）</strong></p><p><img width="723" height="227" referrerpolicy="no-referrer" src="/img/bVdnNxv" alt="image.png" title="image.png" loading="lazy"/><br/>各工具多人对话视频生成比较图</p><ul><li>现有方法问题：出现了严重的身份混淆。不同角色的面部特征、服装细节在镜头间发生混合与错乱，导致“A角色的脸配B角色的衣服”。同时，背景（如墙上的画）在镜头间无法保持一致。</li><li>FilmWeaver表现：成功稳定保持了每位角色的独特外观，并且背景细节在切换镜头时完全一致，镜头3中男子身后的壁画等细节与镜头1完全一致。这证明了Shot Cache在区隔并记忆多个独立概念上的能力。<br/><strong>场景二：动态动作序列</strong></li></ul><p><img width="723" height="224" referrerpolicy="no-referrer" src="/img/bVdnNxw" alt="image.png" title="image.png" loading="lazy"/><br/>各工具多镜头视频生成比较图</p><ul><li>现有方法问题：在动作过程中，角色外观会发生不可控的抖动与变化。</li><li>FilmWeaver表现：在激烈的运动下，FilmWeaver始终保持稳定角色身份和服装。</li></ul><h3>04 总结与展望</h3><p>本文提出了 FilmWeave，一种<a href="https://link.segmentfault.com/?enc=rKfxB%2BfQebx4YrdBHCPF9g%3D%3D.06hmgWej3GP8tsD5TECwa2zlDtuOnfr129R94PCrOqYyX3d7em6CKvhmMOecSYr778luNwOaay9NA%2FBK4697sZpMhTYojfSPKAwFJg168TNhe6JsFb3HIDIvAw1RU3%2BA0wwAAUHh%2Ffl%2BKz9ba82sZg%3D%3D" rel="nofollow" target="_blank">基于缓存引导的自回归扩散框架</a>，用于解决多镜头视频生成中的跨镜头一致性与镜头内连贯性问题。</p><h4>1. 新颖的双层缓存机制</h4><ul><li>Shot Cache：通过检索历史镜头中的关键帧，实现长期视觉概念（如角色、场景）的持久记忆与一致性保持。</li><li>Temporal Cache：采用压缩滑动窗口保存近期帧，确保镜头内运动的自然流畅。</li></ul><h4>2. 灵活的四模式推理框架</h4><p>支持从首镜头生成、镜头延伸、新镜头过渡到全缓存生成的全流程，允许用户交互式构建任意长度与镜头数的视频叙事。</p><h4>3. 高质量数据构建流程</h4><p>针对多镜头数据缺失问题，设计了一套从镜头切分、场景聚类到分组标注的数据构建流水线，并构建了用于评测的多镜头测试集。<br/>未来工作可从<strong>数据、控制与效率</strong>三方面推进：进一步提升多镜头训练数据的规模与标注精度；探索结合语义剧本的更强叙事控制；优化缓存检索与压缩机制以支持更复杂、更长的电影级生成任务。<br/>GitLink开源创新服务平台与Lab4AI大模型实验室联合发起「论文头号玩家」<a href="https://link.segmentfault.com/?enc=CIYSFyKihMsxX0AEtPx7Mw%3D%3D.Rh6RMTh4uSjy2dtK4nfwJOv9wCFb97zkDjPeRwmywv6GCiLPo1tGpD90Q%2B5PkGgm23Wy8DVPEoKtAcJ8dsVLKMlXfGwpVyL%2B6tvePxGS%2FX34TQRqkvY9kauzdWBFWmNk9%2BGjK4lXno5P%2BWaNjg3AeQ%3D%3D" rel="nofollow" target="_blank">论文复现计划</a>。寻找百万「论文头号玩家」计划 | 首批复现体验官开放申请，最高可获500元算力金！本计划开放高性能H800 GPU算力，旨在降低复现门槛，推动学术成果的实践转化。<br/>参与活动您将获得：</p><p><img width="723" height="1301" referrerpolicy="no-referrer" src="/img/bVdnNxy" alt="image.png" title="image.png" loading="lazy"/><br/>论文复现体验官招募火热进行中</p><p>关注<a href="https://link.segmentfault.com/?enc=UJ8ASDeY%2BjIgehuqjR9%2Fjg%3D%3D.RVT4p8rC3QK7n2RXvTcRzEezynfkl3caQ8wnXpFTSZyL1bVgIPVUHsivdRT%2B0TlhQ1scbHcXzgNQl6eOji4t75fVntz3SJpL3Ji%2BAsKyBB67i%2FSkVoqEpah8N6esKDONbnW5B7bWgCek6Z0WJ56XeQ%3D%3D" rel="nofollow" target="_blank">“大模型实验室Lab4AI”</a>，第一时间获取前沿AI技术解析！</p><p>点击<a href="https://link.segmentfault.com/?enc=7a%2F9eC2GB16jNYVEM29EJw%3D%3D.Cv44VenqkT%2BBVovxlgZSZOxisTXvIxsEYGsN%2B2iaq1AYL6GJubp%2FUbYVyhl1APljKLZXQOCi4ourQb7vcfjZcq5vrVQZMXziUK2wIYrlxvJFpO163gpX0GFCKMu4IWO%2BOaDLzW%2FjB9i8xB4tUfyizg%3D%3D" rel="nofollow" target="_blank">阅读原文</a>，跳转至Lab4AI官网，领取算力福利~</p>]]></description></item><item>    <title><![CDATA[NanoBanana只出文字不出图：问题解析与解决方案 Novproxy ]]></title>    <link>https://segmentfault.com/a/1190000047578656</link>    <guid>https://segmentfault.com/a/1190000047578656</guid>    <pubDate>2026-01-28 19:10:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在人工智能绘图工具日益普及的今天，NanoBanana作为一款基于Google Gemini模型的AI绘图工具，因其便捷性和高质量的图像生成能力受到了广大用户的青睐。然而，许多用户在使用过程中遇到了一个令人困扰的问题：NanoBanana只输出文字而不生成图像。这个问题不仅影响了用户的创作体验，也成为了阻碍工具推广的重要障碍。</p><p><strong>什么是NanoBanana？</strong></p><p>NanoBanana是一个基于Google Gemini人工智能模型的在线绘图工具，用户通过输入文字描述来生成相应的图像。它利用先进的深度学习技术，能够理解用户的文字指令并将其转化为视觉内容。该工具支持多种绘画风格，从写实到抽象，从动漫到油画，几乎可以满足各种创作需求。NanoBanana的出现极大地降低了艺术创作的门槛，让没有绘画基础的用户也能轻松创作出精美的图像作品。</p><p><strong>为什么会出现只出文字不出图的问题？</strong></p><p><strong>经过深入分析和技术调查，NanoBanana只出文字不出图的问题主要源于以下几个方面：</strong></p><p>首先，Google官方的服务故障是一个不可忽视的因素。由于AI模型的复杂性和随机性，有时即使发送了正确的图像生成请求，模型也可能因为内部错误而只返回文字内容。这种情况通常表现为API调用返回200状态码，但响应内容却是空白或只包含文字信息。</p><p>其次，提示词不够明确或存在理解偏差也是重要原因之一。当用户的描述不够具体或者没有明确表达图像生成的需求时，AI模型可能会错误地将任务理解为文字生成而非图像创作。例如，如果用户只说"画一只猫"，而没有提供更多的细节描述，模型可能会返回一段关于猫的文字描述而不是生成图像。</p><p>配额限制是另一个常见问题。每个用户都有每日或每月的API调用配额，一旦超出限制，系统就会阻止新的图像生成请求。这种情况下，API通常会返回429错误代码，表示配额超限。</p><p>安全过滤机制也可能导致图像生成失败。如果AI判断生成的图像可能包含敏感、违规或不当内容，安全过滤器就会自动拦截，导致返回空白响应。这种情况在状态码为200的情况下也可能发生，让用户难以判断具体问题所在。</p><p>网络连接不稳定同样会影响图像生成。不稳定的网络环境可能导致与API服务器的连接中断，造成请求超时或响应丢失。这种情况下通常会出现4xx或5xx系列错误代码。</p><p>此外，使用过期的模型版本也会导致问题。如果API调用的是已经不再支持的旧版本模型，服务器会返回404错误，表示请求的模型版本无效。</p><p>最后，会话衰减问题也不容忽视。长时间不活动可能导致用户会话过期，影响后续的图像生成请求。</p><p><strong>如何通过住宅IP解决NanoBanana绘图问题？</strong></p><p>面对这些复杂的技术问题，使用高质量的住宅IP代理服务成为了解决方案的关键。住宅IP相比数据中心IP具有更高的可信度和稳定性，能够有效避免各种网络层面的限制和问题。</p><p>Novproxy作为专业的海外住宅IP服务提供商，在这个领域展现出了显著的优势。该平台拥有超过1亿个住宅IP资源，覆盖190个国家和地区，为用户提供了强大的网络支持。其99%的成功率和不到0.5秒的响应速度，确保了与NanoBanana API的稳定连接。</p><p>使用Novproxy的住宅IP服务可以从根本上解决NanoBanana只出文字不出图的问题。首先，高质量的住宅IP能够提供更稳定的网络连接，避免因网络波动导致的请求失败。其次，纯净的IP地址不容易触发Google的安全机制，降低了被误判为恶意请求的风险。</p><p><strong>Novproxy的独特优势</strong></p><p>Novproxy在AI绘图应用场景中表现出了多项核心优势。其提供的住宅IP节点具有极高的纯净度和可信度，每个IP都是独享的，不会与其他用户产生冲突，这有效避免了因IP共享导致的各种问题。</p><p>动态住宅IP支持每次请求轮换和粘性会话两种模式，用户可以根据实际需求灵活选择。对于需要频繁生成图像的用户，可以选择粘性会话模式，保持IP地址的稳定性；而对于需要大量切换IP的用户，轮换模式则更为适合。</p><p>全球覆盖的IP资源让用户能够轻松切换到响应速度最快的区域，优化模型性能。这种地理优势不仅提高了连接速度，还能帮助用户获得更好的AI服务体验。</p><p><strong>使用教程与最佳实践</strong></p><p>要充分利用Novproxy解决NanoBanana的问题，用户需要遵循一些最佳实践。首先，在使用前应该检查自己的网络环境，确保选择了最适合的IP节点。对于亚洲用户，选择日本、韩国或新加坡的节点通常能获得更好的连接效果。</p><p>其次，合理设置会话时间也很重要。Novproxy支持1到120分钟的会话时长设置，对于AI绘图应用，建议设置为30-60分钟，既能保持会话的稳定性，又能在出现问题时及时切换。</p><p>在提示词的编写上，用户应该尽可能具体明确。在描述开头就明确表达"请生成一张图片："，然后详细描述想要的图像内容，包括主体、风格、场景、颜色等要素。避免使用可能触发安全过滤的敏感词汇。</p><p><strong>成本效益分析</strong></p><p>Novproxy提供了多种灵活的定价方案，满足不同用户的需求。动态住宅流量仅需每GB0.5美元，对于偶尔使用AI绘图的用户来说成本非常低廉。长期静态ISP方案每月3美元，适合需要稳定IP的长期使用场景。无限流量方案则为重度用户提供了更经济的选择。</p><p>新用户还可以享受0.99美元的专享优惠，获得500M的试用流量，这为用户提供了充分的机会来测试服务效果。</p><p><strong>未来展望</strong></p><p>随着AI技术的不断发展，像NanoBanana这样的绘图工具将会越来越普及。网络稳定性作为影响用户体验的关键因素，其重要性也会日益凸显。住宅IP代理服务不仅是解决当前问题的有效手段，更是未来AI应用生态中不可或缺的基础设施。</p><p>通过使用Novproxy这样的专业服务，用户不仅能够解决NanoBanana只出文字不出图的问题，还能获得更流畅、更稳定的AI绘图体验。这种解决方案的价值不仅体现在技术层面，更在于它为用户打开了通往AI创作世界的大门，让更多人能够享受到人工智能带来的创作乐趣。</p><p>在数字化创作的时代，选择合适的工具和服务至关重要。NanoBanana配合Novproxy的住宅IP服务，为用户提供了一个可靠、高效的AI绘图解决方案，让创意不再受技术限制，让想象能够自由飞翔。</p>]]></description></item><item>    <title><![CDATA[SpringAI+RAG向量库+知识图谱+多模型路由+Docker打造SmartHR智能招聘助手 代]]></title>    <link>https://segmentfault.com/a/1190000047578661</link>    <guid>https://segmentfault.com/a/1190000047578661</guid>    <pubDate>2026-01-28 19:09:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Smart-HR 智能招聘与面试助手</h2><h3>项目适合人群</h3><blockquote><ul><li>Spring AI 的基本使用</li><li>Milvus 向量知识库的集成实践</li><li>Neo4j 作为知识图谱的建模与查询</li><li>适配器模式实现多模型切换（OpenAI/百炼/等）</li><li>Docker Compose 一键启动前后端与依赖</li></ul></blockquote><h3>项目源代码地址</h3><p><a href="https://link.segmentfault.com/?enc=lGUxG39viipnj35VNIGdeA%3D%3D.QCK7RYbEozrQwdTAnuRPv%2Fh9B1b3xg57eMWbb3OH1mEpO4jx9M0kpDaRdWWLugSH" rel="nofollow" target="_blank">Github地址</a>  <br/><strong>建议直接打开源代码 对照学习每一个部分</strong></p><p><strong><a href="https://link.segmentfault.com/?enc=2VVGO171wjIUAq7xK6xkEA%3D%3D.Z68AnK5IY87p7lYGWA94AyNL%2BbHLdX3T0%2BWNr5%2FXfLjrryD%2FgK1RI8QLA6RuYnF2" rel="nofollow" target="_blank">https://github.com/LQF-dev/smart-hr</a></strong></p><h3>1. 项目简介 🚀</h3><ul><li>面向 HR 与面试官的智能招聘助手，覆盖简历解析、岗位匹配、面试题生成与模型切换。</li><li>引入 <strong>Neo4j 知识图谱</strong>（预置约 200 个技能节点及依赖关系）作为 HR 匹配的图谱评判依据；</li><li>引入 <strong>Milvus 向量数据库</strong> 作为 RAG 知识库，分为“企业特定金融知识”与“通用知识”两部分，支撑面试官题库生成与语义检索。</li><li>采用 <strong>模型适配器模式</strong>，可插拔接入多种大模型（当前支持阿里云百炼、OpenAI）；新增模型只需实现 Adapter 并注册到 ModelRouter。</li><li>后端基于 Spring Boot + Spring AI，整合 Milvus/Neo4j/PostgreSQL；前端基于 React + Ant Design。</li></ul><h3>2. 技术栈与架构 🧰</h3><ul><li>前端：React 18、TypeScript、Vite、Ant Design、Zustand。</li><li>后端：Spring Boot 3、Spring Security + JWT、Spring AI。</li><li>数据与存储：PostgreSQL、Neo4j、Milvus。</li><li>运维：Docker / Docker Compose。</li></ul><h3>3. 系统架构图 🧭</h3><p><strong>HR 流程：简历/岗位 → Neo4j 知识图谱 → 混合打分（图谱评分 + LLM 评估）</strong></p><p><img width="723" height="317" referrerpolicy="no-referrer" src="/img/bVdnNxT" alt="" title=""/></p><p><strong>面试官流程：岗位/技能 → RAG（Milvus 题库/技能向量）→ LLM</strong></p><p><img width="723" height="294" referrerpolicy="no-referrer" src="/img/bVdnNxU" alt="" title="" loading="lazy"/></p><blockquote>HR 通过 Neo4j 图谱进行技能匹配后送入 LLM；面试官流程以 Milvus RAG 检索题库/技能语义，再送入 LLM。</blockquote><h3>4. 功能特性 🎯</h3><h4>HR 🤝</h4><ul><li>岗位管理：岗位创建/编辑/删除，岗位列表。</li><li>简历处理：上传简历、技能提取、查看简历详情。</li><li>匹配分析：岗位 ⇄ 简历互相匹配，支持混合打分报告，匹配历史/详情查看。</li><li>记录管理：匹配结果列表、历史查询。  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/402/406/4024064820-6979d3648c87c" alt="image-20260127174532526" title="image-20260127174532526" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/273/286/2732869300-6979d3683da73" alt="image-20260127174638611" title="image-20260127174638611" loading="lazy"/></li></ul><h4>面试官 🎤</h4><ul><li>题目生成：按岗位或技能生成面试题，可选难度与题量。</li><li>记录管理：生成历史查看、记录删除。  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/310/672/3106723579-6979d36b49d87" alt="image-20260127174807667" title="image-20260127174807667" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/210/946/2109467360-6979d36e32ddd" alt="image-20260127174824125" title="image-20260127174824125" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/440/303/440303285-6979d3713fa3a" alt="image-20260127174851037" title="image-20260127174851037" loading="lazy"/></li></ul><h4>通用 🧩</h4><ul><li>认证：登录/注册（JWT），当前用户信息。</li><li>模型：AI 模型列表与切换（阿里云百炼 / OpenAI，适配器模式）。</li><li>API：Swagger UI。  <br/><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/795/235/79523571-6979d373ec30a" alt="image-20260127174423668" title="image-20260127174423668" loading="lazy"/></li></ul><h3>5. 目录结构</h3><ul><li><code>back/</code>：Spring Boot 后端。</li><li><code>front/</code>：React 前端。</li><li><code>docker/</code>：基础设施与一键部署的 Compose 文件、初始化脚本。</li></ul><h3>6. 环境准备</h3><ul><li>JDK 21、Maven 3.8+。</li><li>Node.js 18+（含 npm）。</li><li>Docker Desktop（含 Docker Compose）。</li></ul><h3>7. 快速开始 ⚡</h3><blockquote>详细步骤请参见 Github 代码仓库下的 <code>DEV_GUIDE.md</code>。 这里仅仅列出基本使用</blockquote><ol><li>启动基础设施（本地开发）</li></ol><pre><code>cd docker
docker-compose -f docker-compose.dev.yml up -d
12</code></pre><ol start="2"><li>初始化 Neo4j 知识图谱</li></ol><ul><li>浏览器执行 <code>docker/neo4j/init.cypher</code> 和 <code>docker/neo4j/init-skills-extended.cypher</code>，或使用 <code>cypher-shell</code>（详见 DEV\_GUIDE）。</li></ul><ol start="3"><li>配置大模型 API Key（至少需阿里云百炼，OpenAI 可选）</li></ol><pre><code>export DASHSCOPE_API_KEY=你的阿里云百炼API_KEY
export OPENAI_API_KEY=你的OpenAI_API_KEY   
12</code></pre><ol start="4"><li>启动后端（本地开发）</li></ol><pre><code>cd back
./mvnw spring-boot:run
12</code></pre><ol start="5"><li>启动前端（本地开发）</li></ol><pre><code>cd front
npm install
npm run dev
123</code></pre><ol start="6"><li>全栈 Docker 一键启动（可选）</li></ol><pre><code>cd docker
export DASHSCOPE_API_KEY=你的阿里云百炼API_KEY
export OPENAI_API_KEY=你的OpenAI_API_KEY   
docker-compose up -d
1234</code></pre><ul><li>Swagger API 文档：<code>http://localhost:8080/swagger-ui.html</code></li><li>默认端口：后端 8080，前端 5173（开发）/ 3000（容器），Postgres 15432(dev)/5432(prod compose)，Neo4j 7474/7687，Milvus 19530/9091。</li></ul><p>（更多启动、调试与排障说明，请查看 <code>DEV_GUIDE.md</code>）</p><h3>8. 开发指南：扩展新的大模型 🛠️</h3><ol><li>引入 SDK 依赖：在 <code>back/pom.xml</code> 添加对应模型的官方 SDK 或 HTTP 客户端依赖，并配置密钥环境变量。</li><li>实现适配器：参考 <code>AliyunAdapter</code>，实现 <code>AIModelAdapter</code> 接口，封装 <code>chat</code> / <code>embedding</code> 调用和模型 ID。</li><li>注册模型：在模型注册/路由处（如 <code>ModelRegistry</code>、<code>ModelRouter</code>）将新 Adapter 注册并开放配置。</li><li>配置密钥：在 <code>application.yml</code> 或环境变量中新增该模型的 API Key/Endpoint。</li><li>前端暴露：如需在前端选择模型，补充模型枚举/下拉项即可，无需改后端协议。</li></ol>]]></description></item><item>    <title><![CDATA[数式Oinone7早鸟体验版发布，全面适配JDK17，AI Native加速产品智能化转型 数式Oi]]></title>    <link>https://segmentfault.com/a/1190000047578680</link>    <guid>https://segmentfault.com/a/1190000047578680</guid>    <pubDate>2026-01-28 19:09:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>今天，数式Oinone发布 Oinone7 早鸟体验版。作为面向企业级数字化交付场景的关键版本，本次迭代在夯实技术底座、打通协作链路、优化交互体验的同时，将AI Native能力深度融入日常使用，核心目标是帮助企业在数字化进程中少踩坑、少返工，把时间和精力真正花在创造业务价值上。</p><p>Oinone7不仅实现JDK 17的完整适配，带来启动速度提升30%的显著优化，也将其确立为后续版本迭代的基础版，一方面适配伙伴出海需求，另一方面支撑AI Native的深度融合落地。</p><p>在产品设计上，Oinone7始终聚焦企业真实需求：以JDK 17底层支撑强化系统稳定性，借工作流流程优化打通团队协作衔接堵点，靠交互体验升级降低工具使用门槛。这三大升级方向协同发力，切实帮助企业突破“系统运行波动、协作效率低下、工具上手复杂”的数字化瓶颈，让整体交付过程更可控、更顺畅。</p><p>Oinone 7核心能力升级</p><p>Oinone7的核心升级围绕企业实际使用需求展开，从技术支撑到协作、操作体验，每一处调整都聚焦“解决问题、创造价值”，让企业在数字化交付中少走弯路。</p><ol><li>稳定技术底座，降低企业长期成本</li></ol><p>凭借对JDK 17的全面适配，且将其作为后续版本迭代的基础版本，Oinone7为企业提供更扎实的使用保障：</p><p>● 在系统性能上：</p><p>Oinone7运行更稳定，即便是处理复杂业务场景，也很少出现卡顿或中断情况，能确保交付工作按计划推进；</p><p>● 在依赖衔接上：</p><p>Oinone7同步将基础依赖升级至Spring Boot 3.3.x、Dubbo 3.2.x，与JDK 17技术基线深度适配，企业现有系统与Oinone7的新功能搭配使用时，无需担心兼容性问题，无需额外处理版本适配相关事务；</p><p>● 在智能能力接入上：</p><p>Oinone7整合Spring AI，企业无需构建复杂的智能基础设施，就能轻松用上智能能力，快速跟上AI应用的发展趋势。</p><ol start="2"><li>优化工作流衔接，让协作更顺畅</li></ol><p>针对企业团队协作中的常见堵点，Oinone7对工作流进行针对性升级：</p><p>● 面对人员调整：</p><p>审批流程自动衔接新负责人，已完成的审批记录完整保留，不用人工重新梳理或补录，避免审批断层；</p><p>● 新增流程运行状态视图：</p><p>异常、超时情况直观呈现，从流程到具体实例的问题定位更快速，不用在海量数据中逐一排查；</p><p>● 同时强化审计合规能力：</p><p>协作过程中的关键操作全程留痕，符合企业对内管理、对外合规的双重需求。</p><ol start="3"><li>升级交互体验，降低工具使用门槛</li></ol><p>从“好用、易用”出发，Oinone7让工具操作更贴近用户习惯：</p><p>● 表格、表单等常用视图预设优化：</p><p>开箱就能直接用于业务场景，减少初始搭建的时间成本；</p><p>● 分散的功能开关整合为清晰选项：</p><p>不同团队成员对操作的理解更统一，不用反复沟通确认；</p><p>● 图表操作面板实现刷新、导出、数据联动一体化：</p><p>业务人员分析数据时不用切换多个界面，处理效率显著提升。</p><ol start="4"><li>AI Native深度融合，从研发工具到智能决策</li></ol><p>Oinone7打破“AI落地难、复用性低”的行业痛点，以AI Native能力将工具平台升级为企业的智能化引擎。基于Oinone元数据为基础的AI Native能力、整合其他智能升级能力，让AI真正融入业务：</p><p>● 在模型管理上：</p><p>支持多厂商大模型接入与管理，企业可按需选择适配业务的模型；</p><p>● 在模型训练上：</p><p>提供通用的从数据集自动采集、标注，到模型微调，再到AI员工/智能决策助手开发全生命周期能力；</p><p>● 整体体验上：</p><p>带来全新的企业级应用交互体验，智能助手不仅懂系统、懂业务、更会操作。</p><p>Oinone7不仅完成JDK17的全面适配，带来启动速度提升30%的显著优化，且同步将基础依赖更新至Spring Boot 3.3.x、Dubbo 3.2.x，也将其确立为后续所有版本迭代的基础版，一方面适配伙伴出海需求，保障复杂场景下系统稳定运行；另一方面支撑AI Native的深度融合落地，通过整合Spring AI帮企业省去搭建独立系统的成本，同时实现现有系统与新功能的无缝衔接。</p><p>关于Oinone7的详细升级说明、适配要点及操作指南，可以通过技术文档中获取：</p><blockquote><a href="https://link.segmentfault.com/?enc=CcbIcA5BINeOQvhJcM0BCA%3D%3D.aOWeCe9b%2FYw9e9wVCY0zvRhd1yY%2FCEy4a4SSCrBAr1wIw2ECPXrXn6VyXOOuTgtS" rel="nofollow" target="_blank">https://doc.oinone.top/version/25243.html</a></blockquote><p>AI写代码<br/>帮助企业快速掌握升级要点，平稳完成版本过渡。</p><p>这不仅是一版升级，更是数式Oinone“企业级产品化引擎：用低代码驱动标准化研发与敏捷交付的一体化平台”的持续兑现——以稳定的JDK 17基线、可感知的协作与交互优化，以及可落地的AI Native能力，帮助软件公司与ISV把产品做得更标准、交付更敏捷、成本更可控。</p>]]></description></item><item>    <title><![CDATA[年度开发者嘉年华！最流行的开源技术社区基本都来了！1 月 31 日，来上海赴一场技术之约~ RTE开]]></title>    <link>https://segmentfault.com/a/1190000047578683</link>    <guid>https://segmentfault.com/a/1190000047578683</guid>    <pubDate>2026-01-28 19:08:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578685" alt="" title=""/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578686" alt="" title="" loading="lazy"/></p><p>OceanBase 社区嘉年华上午 Keynote</p><p><strong>全程高能！</strong> 活动流程如下 👇</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578687" alt="" title="" loading="lazy"/></p><p>主题分享之外，AI 技术圆桌巅峰对话来袭</p><p>解锁技术前沿实践与思考！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578688" alt="" title="" loading="lazy"/></p><p>下午属于 AI Coding 专场，issue 已公开，Everything is ready to go. Have fun! </p><p>复制下方链接到浏览器查看  <br/><a href="https://link.segmentfault.com/?enc=bQz9s4CblObYsZta2ynazQ%3D%3D.XCjNb8FPDG9b1ZrRmTZKj0iZYOvWD0cvLhg5kidnNX1YrX2WEFAmVBT15x8jJlAS" rel="nofollow" target="_blank">https://github.com/oceanbase/seekdb/issues/123</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578689" alt="" title="" loading="lazy"/></p><p>Mentor 已就位，现场除了能 Prompt AI，还可以向他们请教哦～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578690" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578691" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578692" alt="" title="" loading="lazy"/></p><p>与此同时，超有料的社区开放麦等你来打卡！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578693" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578694" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578695" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578696" alt="" title="" loading="lazy"/></p><p>心动不如行动！点击 <a href="https://link.segmentfault.com/?enc=tCg467iAIU4wtAq%2BFJEBMA%3D%3D.LKo3wFldvj%2BqtdiNWdY0kv2ZR1v%2BQj4ZE%2Bwh84Ui3kPVNgh8qmgmjNe5qE%2BprR9E" rel="nofollow" target="_blank">https://ask.oceanbase.com/t/topic/35638331</a> ，解锁 OceanBase 社区嘉年华当日路线图、交通指南及全套实用攻略！1 月 31 日，上海，我们不见不散～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578697" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578698" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=3RJO%2F4yYpf474OGfkg76gw%3D%3D.fVei8DBQqJ3YHGjZJzD4ZuWmN3U9MulwibzY1XH05rA%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578699" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[使用C#代码合并或取消合并 Excel 单元格 千杯不醉的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047578754</link>    <guid>https://segmentfault.com/a/1190000047578754</guid>    <pubDate>2026-01-28 19:07:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>合并单元格是指将两个或多个独立的单元格合并为一个较大的单元格，常用于需要创建跨多列显示的标题或标签。本文将演示如何使用 Spire.XLS for .NET 库，通过 C# 和 VB.NET 在 Excel 中实现单元格的合并与取消合并操作。</p><h2>安装 Spire.XLS for .NET</h2><p>首先，需要在你的 .NET 项目中添加 Spire.XLS for .NET 包中包含的 DLL 文件作为引用。你可以通过官网下载 DLL 文件手动添加，也可以直接通过 NuGet 安装该库。</p><pre><code class="C#">PM&gt; Install-Package Spire.XLS</code></pre><h2>使用 C# 和 VB.NET 合并 Excel 单元格</h2><p>以下是通过代码在 Excel 中合并单元格的基本步骤：</p><ol><li>创建一个 Workbook 实例。</li><li>使用 Workbook.LoadFromFile() 方法加载 Excel 文件。</li><li>通过 Workbook.Worksheets[sheetIndex] 获取需要操作的工作表。</li><li>访问指定的单元格区域，并调用 XlsRange.Merge() 方法将其合并为一个单元格。</li><li>将合并后单元格中的文本水平居中，可将 CellRange.Style.HorizontalAlignment 属性设置为 HorizontalAlignType.Center。</li><li>最后，使用 Workbook.SaveToFile() 方法保存生成的结果文件。</li></ol><p><strong>示例代码如下：</strong></p><pre><code class="C#">using Spire.Xls;

namespace MergeCells
{
    class Program
    {
        static void Main(string[] args)
        {
            // 创建一个 Workbook 实例
            Workbook workbook = new Workbook();
            // 加载 Excel 文件
            workbook.LoadFromFile("Sample.xlsx");

            // 获取第一个工作表
            Worksheet sheet = workbook.Worksheets[0];
            // 将单元格 A1–D1 合并为一个单元格
            CellRange range = sheet.Range["A1:D1"];
            range.Merge();
            // 将合并后的单元格内容设置为水平居中
            range.Style.HorizontalAlignment = HorizontalAlignType.Center;            

            // 保存结果文件
            workbook.SaveToFile("MergeCells.xlsx", ExcelVersion.Version2013);
        }
    }
}</code></pre><h2>在 C# 和 VB.NET 中取消合并 Excel 单元格</h2><p>下面是取消合并 Excel 单元格的具体操作步骤：</p><ol><li>创建一个 Workbook 对象。</li><li>使用 Workbook.LoadFromFile() 方法加载 Excel 文件。</li><li>通过 Workbook.Worksheets[sheetIndex] 获取需要操作的工作表。</li><li>访问指定的单元格区域，并调用 XlsRange.UnMerge() 方法取消单元格合并。</li><li>使用 Workbook.SaveToFile() 方法保存结果文件。</li></ol><p><strong>示例代码如下：</strong></p><pre><code class="C#">using Spire.Xls;

namespace UnmergeCells
{
    class Program
    {
        static void Main(string[] args)
        {
            // 创建一个 Workbook 实例
            Workbook workbook = new Workbook();
            // 加载 Excel 文件
            workbook.LoadFromFile("MergeCells.xlsx");

            // 获取第一个工作表
            Worksheet sheet = workbook.Worksheets[0];
            // 取消合并单元格 A1–D1
            CellRange range = sheet.Range["A1:D1"];
            range.UnMerge();

            // 保存结果文件
            workbook.SaveToFile("UnMergeCells.xlsx", ExcelVersion.Version2013);
        }
    }
}</code></pre><h2>申请临时许可证</h2><p>如果您希望去除生成文档中的评估提示，或解除功能限制，请为自己申请一个有效期为 30 天的试用许可证。</p>]]></description></item><item>    <title><![CDATA[KaiwuDB 分布式执行引擎的演进之路 KaiwuDB ]]></title>    <link>https://segmentfault.com/a/1190000047578894</link>    <guid>https://segmentfault.com/a/1190000047578894</guid>    <pubDate>2026-01-28 19:06:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>V3.0 新一代架构突破------从 "集中汇总" 到 "分布式协同"</strong></h2><p>KaiwuDB V2.x 版本中的分布式执行引擎传统架构采用的是"管理节点（Master Engine，即 ME）--- 执行节点（TS Engine）"二级架构的集中式设计：</p><p><strong>• 通信链路</strong>：ME 向各执行节点下发 Flowspec 任务，执行节点间无直接通信链路，所有交互均通过 ME 中转。</p><p><strong>• 计算汇总</strong>：所有执行节点计算结果需全量回传至 ME，由 ME 承担二次汇总计算职责。</p><p>为了减少冗余编解码的操作以及传输与计算的开销，进一步提升分布式执行的性能，KaiwuDB 在 V3.0 中将新一代架构通过四项核心改造实现架构层面的突破性升级，其关键组件与数据流转逻辑如下。</p><h3><strong>1. 基于 Pipeline 架构：释放并行潜力，提升扩展弹性</strong></h3><p>支撑高并发查询调度，满足 AP 场景横向扩展需求。采用 Pipeline 流式执行架构，通过任务拆分与流水线化执行，实现单设备高效并行；引入优先级调度机制，支持资源弹性分配与高优先级任务倾斜。</p><p>查询并发承载能力大幅提升，架构扩展性适配从百级到万级查询的弹性需求，资源利用率显著提高。</p><h3><strong>2. 统一编码：强化效率与兼容性，提速大数据处理</strong></h3><p>统一编码标准，提升大规模数据集传输与处理效率。标准化采用 DataChunk 作为默认执行编码，依托其统一规范与高效的序列化 / 反序列化特性，单机处理 160 万行结果集场景下可提速 3 秒。整体消除编码层面性能损耗，为 TB 级数据分析提供高效、兼容的编码支撑，数据处理吞吐量显著提升。</p><h3><strong>3. 执行节点间 BRPC 传输：优化分布式协同，降低传输开销</strong></h3><p>实现节点间低延迟、高可靠数据传输，减少资源占用。采用 BRPC 作为执行节点间核心传输协议，依托其原生 C++ 接口与高效通信机制，简化传输链路、减少冗余开销；内置统一 Shuffle 机制，保障数据分发有序性。使得分布式传输延迟与网络带宽占用显著降低，节点间协同效率提升，支撑大规模分布式查询稳定执行。</p><h3><strong>4. 算子全下推与能力升级：完善算力支撑，适配复杂场景</strong></h3><p>提升算子性能与功能覆盖度，支撑大规模、复杂计算需求。推进算子全下推架构，减少数据回传开销；新增 Join 算子完善跨模计算能力，为 Hash Agg 算子适配落盘机制规避内存溢出，优化 Sort 算子执行逻辑提升大规模数据排序性能。算子层功能与性能双升级，可高效支撑复杂查询、高基数聚合、TB 级数据排序等重负载任务，适配 AP 场景多样化计算需求。</p><p><img width="723" height="459" referrerpolicy="no-referrer" src="/img/bVdnNA9" alt="" title=""/></p><p>KaiwuDB 3.0 分布式执行架构</p><p>上述四项核心改造的具体作用机制如下：</p><p>✅<strong>BRPC 通信层改造</strong>：在执行节点节点间构建专用通信链路，采用与本地算子同源的数据格式传输中间结果，彻底消除节点间的数据转换开销。</p><p>✅<strong>全算子下推执行改造</strong>：将所有计算算子从 ME 迁移至执行节点部署执行，仅由 Root 执行节点承担最终结果汇总职责，其余执行节点仅向 ME 反馈执行状态，数据传输量降幅超 90%。</p><p>✅<strong>Block Filter 机制引入</strong>：将数据过滤规则下推至存储层，存储节点基于 Block 元数据统计信息预过滤无效数据，显著降低计算层的输入数据量，提升计算效率。</p><p>✅<strong>Pipeline 流水线调度改造</strong>：基于 Pipeline 模型对查询任务进行拆分与并行化编排，实现任务高效并行处理，其核心架构与数据流转逻辑如下：</p><p>!<img width="723" height="581" referrerpolicy="no-referrer" src="/img/bVdnNBg" alt="" title="" loading="lazy"/><br/>Pipeline 模型</p><p>Pipeline 模型沿用传统数据处理的流水线设计范式，将复杂查询任务解耦为若干细粒度、可并行调度的子任务；各子任务被编排为多个 Pipeline Stage（流水线阶段），每个 Stage 由一组 Operator（算子）构成协同处理单元。数据在算子间遵循流水线机制逐阶段流转处理，最终达成查询任务的高效执行目标。</p><h2><strong>分布式执行调度流程</strong></h2><p>调度层承担逻辑执行计划的分布式改写职责，将其解耦为执行节点级计划片段（Flowspec），并对各片段的执行时序与并发度进行精细化编排，保障多模分布式执行结果的一致性与准确性。</p><p>针对时序数据查询，分布式执行调度层会将所有算子全量下推至执行节点端执行，以下为全新执行架构的调度流程示例（以具体 SQL 查询为例）：</p><p><img width="723" height="675" referrerpolicy="no-referrer" src="/img/bVdnNBD" alt="" title="" loading="lazy"/></p><p>KaiwuDB 3.0 分布式执行流程</p><h2><strong>总结与展望</strong></h2><p>综上，KaiwuDB 分布式执行引擎通过一系列核心优化举措，系统性破解了传统架构的多重瓶颈，构建起高效、稳定、可扩展的分布式执行体系，为高并发、大规模时序数据及多模数据分析业务提供了坚实的技术支撑。</p><ul><li><strong>统一引擎架构适配 AP 场景</strong></li></ul><p>通过全算子下推、BRPC 统一通信及 Pipeline 标准化调度机制，有效突破传统架构的性能与扩展性约束，可稳定支撑未来高并发、大规模分析型处理 AP 场景的查询负载需求。</p><ul><li><strong>核心性能实现跨越式提升</strong></li></ul><p>依托计算全量下推、统一编码规范及 BRPC 零转换传输技术，显著降低冗余数据传输及编解码开销；借助 Block Filter 预过滤机制，进一步提升海量数据处理效率与 CPU 资源利用率，优化系统资源配置。</p><ul><li><strong>降低系统复杂度</strong></li></ul><p>明确管理节点（ME）的调度职责、根执行节点的结果汇总职责及普通执行节点的计算职责边界，降低模块间耦合度，可快速定位问题节点及流水线阶段（Stage），提升 Debug 的效率与精准性。</p><ul><li><strong>分布式处理能力全面升级</strong></li></ul><p>以 Pipeline 流水线调度与 Block Filter 预过滤机制保障核心性能输出，依托统一架构设计提升系统可维护性与可扩展性，通过算子落盘优化策略改善存储 I/O 资源利用率，全面支撑复杂大规模业务场景的稳定运行需求。</p><p>未来，KaiwuDB 将基于现有架构持续深化技术迭代，聚焦复杂业务场景的功能完善，推动引擎向更智能、更可靠、更贴合用户核心业务需求的方向演进，助力业务实现数据价值的高效挖掘与转化。</p>]]></description></item><item>    <title><![CDATA[从零搭建 WordPress 独立站 冷冷的代码本 ]]></title>    <link>https://segmentfault.com/a/1190000047578917</link>    <guid>https://segmentfault.com/a/1190000047578917</guid>    <pubDate>2026-01-28 19:05:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前期准备</h2><p>注册域名选择域名：优先选择.com 域名，其次可考虑.co 或.net。域名应简洁易记，最好包含品牌名或核心关键词，避免使用连字符和数字（除非是品牌的一部分）。</p><p>注册域名：推荐使用 Namecheap、NameSilo 等国际知名域名注册商，它们性价比高且提供免费隐私保护。在注册商网站搜索心仪的域名，加入购物车并完成支付。</p><p>选择主机主机选择：外贸独立站的访问者可能来自全球各地，因此选择支持全球访问的主机服务商非常重要。建议选择：</p><ul><li>提供多种配置选择的 <a href="https://link.segmentfault.com/?enc=xa%2F0E2Sgc34g6ueom3PfKQ%3D%3D.1Lu2QxfDMIMP1QuT4zLK4NRjZRm7iRbSgGiPqU6cws8t8n%2BvGNfOkdJpzfqCksZH" rel="nofollow" target="_blank">VPS 服务商</a></li><li>支持<a href="https://link.segmentfault.com/?enc=8%2FPz4qNqGYgsW%2Bl0e2D%2F5A%3D%3D.acgHlWwDUgFISR3ED7RVYz3MGiW9xEWZ8pE1rglTQ9I%3D" rel="nofollow" target="_blank">全球 CDN 加速</a>的主机商</li><li>具备良好技术支持的<a href="https://link.segmentfault.com/?enc=%2FdqYHEid5vBxbrJPvo3vZQ%3D%3D.zt3dxUlZF3O9rtWQaaE4AXHbblkyaeIdSKc9v4IbZrjyWUbZ2fs%2BmelzESe8rO5z" rel="nofollow" target="_blank">服务器</a></li></ul><p>CDN 加速：推荐使用 Cloudflare（免费计划足够起步），它可以将网站的静态文件缓存到全球各地的边缘服务器，大幅提升全球访问速度。</p><p>安装 SSL 证书：SSL 证书可以保障网站数据传输的安全，大多数主机商都提供免费的 SSL 证书安装服务，确保网站以 HTTPS 协议运行。</p><h3>1、主机控制面板宝塔安装 WordPress 程序</h3><p>使用宝塔的用户越来越多，使用 VPS 的朋友，宝塔几乎成了标配，下面简站 WordPress 为大家写一个用宝塔搭建 WordPress 网站详细教程，以图文的形式一步一步按步骤讲明白搭建过程。</p><h4>1、第一步：添加网站</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578920" alt="c95ef9a5dda2cffa67c7dced26cf6807.png" title="c95ef9a5dda2cffa67c7dced26cf6807.png"/></p><p>登陆宝塔后台，找到“网站”，点击进入后，点“添加站点”，输入域名后，选择创建“FTP”和“<a href="https://link.segmentfault.com/?enc=nq%2BnSzcwPmrgbY%2F%2BrBHBiQ%3D%3D.I5mKq6N%2BeCX%2BgcJOx5EYTWkFO2m8iRiRbAC2e44O0qkA9iKMRtohB2v5vRBm%2FdgEhpeVcAvX4QgcpKgs%2Fh%2BDetsc1b8qM6ynfp3wri%2FhjvRGtPUGgkoB4Fm3M49WZCwR" rel="nofollow" target="_blank">数据库</a>“，点”确定“网站就创建成功了。（”数据库“必须得创建，”FTP“可创建，也可以不创建。另外，如果是安装了多个版本的 PHP，可以选择 PHP 的版本。WordPress 目前的最新版是 6.5，建议使用 8.0 版本的 PHP。）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578921" alt="1ba76978aefe63d4e1dd7fdddae875df.png" title="1ba76978aefe63d4e1dd7fdddae875df.png" loading="lazy"/></p><p>网站添加成功后，需要对网站进行，基础的设置，比如，伪静态设置，如上图所示。</p><p>点“伪静态”，在出来的选项中选择“wordpress”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578922" alt="9cfbde8b5cf7a67249b5a8eb64104cdd.png" title="9cfbde8b5cf7a67249b5a8eb64104cdd.png" loading="lazy"/></p><p>出现如所所求代码时，“保存”即可成功设置伪静态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578923" alt="3fcaf8e5d1eec33c2d4c95fd9968193d.png" title="3fcaf8e5d1eec33c2d4c95fd9968193d.png" loading="lazy"/></p><p>SSL 证书添加，将该域名的 SSL 证书相应的代码，复制到密钥（KEY）和证书（PEM 格式）中，“保存并启用证书”即可成功安装 SSL 证书。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578924" alt="1a11028cd3e544598039467d2e908f6e.png" title="1a11028cd3e544598039467d2e908f6e.png" loading="lazy"/></p><p>SSL 安装成功后，可以查看到对应的域名信息和到期日期。</p><h4>2、第二步：上传 WordPress 程序</h4><p>到 WordPress 官方，下载最新版的 WordPress 程序：</p><p><a href="https://link.segmentfault.com/?enc=3LU%2Fi9mIDUXsypcAq4qBpg%3D%3D.MQAkTeIOR50az08MummRNng3Vs7M5oJMGgbL9T80fBimXjhHv1oiA0PmSR7Zhr87Nde2141X4FiOJd48Unn77ChiTG6gB4piqxkMy0CluiuheeeSHq%2FOVEI3ituX5wMrF9uxnMBs5oE%2BIJKdQwSO5yueiNMisnQEr1uYbupsRnDg%2B1%2BD63s0h0J4QBtpTnaGDuz4JQfTp6OID9fgUKxKjQ%3D%3D" rel="nofollow" target="_blank">https://cn.wordpress.org/download/</a></p><p>注意官方的环境要求：官方推荐 PHP 7.4+ 以及 MySQL 版本 8.0+ 或 MariaDB 版本 10.4+。建议 PHP 版使用 php8.0。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578925" alt="b592c6e1754a4c472bad9dd03dbaaa44.png" title="b592c6e1754a4c472bad9dd03dbaaa44.png" loading="lazy"/></p><p>下载完成后在宝塔面板中找到“文件”，选择“<a href="https://link.segmentfault.com/?enc=ImAE6JYLehrZBU9utN8yfA%3D%3D.6%2FNLlXguCPDKD5A7YiwVX%2BaG1N2d47wjy0GbScOP054%3D" rel="nofollow" target="_blank">wodepress.com</a>“文件夹，打开文件夹后，将下载好的 WordPress 程序上传到该目录。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578926" alt="21e6419accec2891999a8a03f688b913.png" title="21e6419accec2891999a8a03f688b913.png" loading="lazy"/></p><p>上传成功后</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578927" alt="c7bffa987fc6f9b0180e58826b9c3166.png" title="c7bffa987fc6f9b0180e58826b9c3166.png" loading="lazy"/></p><p>“解压”该文件</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578928" alt="765e3e08b8a633cc08542cb84f19c637-yVCa.png" title="765e3e08b8a633cc08542cb84f19c637-yVCa.png" loading="lazy"/></p><p>解压后的文件在“wordpress”文件夹中，将该文件夹中的全部文件复制到网站根目录中</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578929" alt="3aa993cd646fdaa44d799a819cb9d775.png" title="3aa993cd646fdaa44d799a819cb9d775.png" loading="lazy"/></p><p>从根目录中删除 wordpress 文件夹和 WordPress 程序文件包.zip 文件</p><h4>3、第三步：安装 WordPress</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578930" alt="989023d617a0df9faee2ed006fcdbce0.png" title="989023d617a0df9faee2ed006fcdbce0.png" loading="lazy"/></p><p>输入网站域名 www.wodepress.com 会出现如图所显的安装界面</p><p>点“现在就开始安装”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578931" alt="eedc0e87c224586c5bc976686d8dcefd.png" title="eedc0e87c224586c5bc976686d8dcefd.png" loading="lazy"/></p><p>在出现的界面里录入相应的数据库信息“数据库名”、“数据库用户名”、“数据库密码”，并“提交”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578932" alt="b6da0c9223fe48d908427b2bf358cad3.png" title="b6da0c9223fe48d908427b2bf358cad3.png" loading="lazy"/></p><p>按提示操作即可，点“运行安装程序”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578933" alt="89898ab8139915dac00fa52c8961da20.png" title="89898ab8139915dac00fa52c8961da20.png" loading="lazy"/></p><p>录入网站的标题、管理员用户名、密码和邮箱后点“安装 WordPress”即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578934" alt="eccb3ab5607f073d39f67017dfcc3867.png" title="eccb3ab5607f073d39f67017dfcc3867.png" loading="lazy"/></p><p>至此在宝塔搭建 WordPress 网站的步骤全部完成</p><p>接下来就是输入自己的域名/wp-admin，登陆到网站的后台，进行 WordPress 网站的其它设置。</p><p>先安装好 WordPress 程序，确认 WordPress 程序可以正常运行。</p><h3>2、安装 WordPress 主题</h3><h4><strong>2.1、导入数据</strong></h4><h5>2.1.1、先删除原数据表</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578935" alt="fc31110af9ee87971bb41e74527607ae.png" title="fc31110af9ee87971bb41e74527607ae.png" loading="lazy"/></p><p>在宝塔后台找到“数据库”，点击进入后，再找到自己网站对应的数据库点“管理”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578936" alt="4132026e77e8f013f66643385ffb797f.png" title="4132026e77e8f013f66643385ffb797f.png" loading="lazy"/></p><p>选择进入数据库，此处注意，一定要选择数据库，进入后才能看到数据表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578937" alt="6b8a1304d0e1fccfcb15f6a675d99e2b.png" title="6b8a1304d0e1fccfcb15f6a675d99e2b.png" loading="lazy"/></p><p>“全选”数据表，“选中项”、“删除”，即可删除全部的数据表，删除完成后数据库内没有任何数据表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578938" alt="9f8e957250f2bc2ce7691e429fc318d5.png" title="9f8e957250f2bc2ce7691e429fc318d5.png" loading="lazy"/></p><p>这个时候再输入域名访问网站时，是 WordPress 初始化的安装状态，这里不用安装，也不用管，跳过就可以。</p><h5>2.1.2、导入演示站数据</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578939" alt="967a56c376bf5be3eb4fb8365aca4ad0.png" title="967a56c376bf5be3eb4fb8365aca4ad0.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578940" alt="ecb04c331acccfe0b396dd23da9b9874.png" title="ecb04c331acccfe0b396dd23da9b9874.png" loading="lazy"/></p><p>找到“导入”，“选择文件”将邮件中的.sql 格式的数据库文件选择后，再点击“导入”即可完成演示数据的导入。</p><h5>2.1.3、修改域名和邮箱</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578941" alt="5eafbf5e4030123e94536c98563b5055.png" title="5eafbf5e4030123e94536c98563b5055.png" loading="lazy"/></p><p>数据导入成功后找到 wp_options 表</p><p><a href="https://link.segmentfault.com/?enc=LEWPKIRi4UNzlg5Rym4YNg%3D%3D.WXBL61j7XaNq1k1khcvdro%2FVENdneiUAEinJKwyaFpofPSnmqTfe2h2mj%2BdnRiGFbMRTmeULYFPgzacfKltmLdChfappaTXPIrDLl8HPnXM%3D" rel="nofollow" target="_blank">将两个的链接改为自己的域名 www.wodepress.com</a>（使用了 SSL 用 https，没使用 SSL 用 http）。</p><p>将邮箱处改为自己的邮箱</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578942" alt="0593879ee347808b9d9735eec2bf9d45.png" title="0593879ee347808b9d9735eec2bf9d45.png" loading="lazy"/></p><p>再找到 wp_users 表</p><p>将此处的邮箱改为自己的</p><h4><strong>2.2、上传并解压主题文件</strong></h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578943" alt="d529f1f0d9ff6c1905032491b7670aa9.png" title="d529f1f0d9ff6c1905032491b7670aa9.png" loading="lazy"/></p><p>在宝塔后台找到“文件”，<a href="https://link.segmentfault.com/?enc=aSy1%2Fja08AxNwhNaVMjLCg%3D%3D.NOAUBCtlrdAs4O7zz5kVUMvG98cO9%2F1irDiLxJjOYJpeelAaNiYxvbyGg1cwqDEPEr%2FAsip6YDE7qz5wWaz5qw%3D%3D" rel="nofollow" target="_blank">进入到自己网站的目录 wodepress.com</a>，再进入到 wp-content 目录，再进入 themes 目录。将邮件中的.zip 格式的主题文件上传到 themes 这个文件夹中，并解压。</p><h4><strong>2.3、上传并解压插件文件</strong></h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578944" alt="c1742fef102cdc3d554f1211b438d7d1.png" title="c1742fef102cdc3d554f1211b438d7d1.png" loading="lazy"/></p><p>在宝塔后台找到“文件”，<a href="https://link.segmentfault.com/?enc=nWKI%2BLLk3AjsZYMxQkIRJA%3D%3D.NDH72svqFdVa6bsSm2R85EsPfQnHTqb8Ndqy1eWEJsQrDTFTTs9ihoau94h5RMLnaqZvHsxGPRP%2FqvYZ19rM%2Fg%3D%3D" rel="nofollow" target="_blank">进入到自己网站的目录 wodepress.com</a>，再进入到 wp-content 目录，再进入 plugins 目录。将邮件中的插件文件上传到 plugins 目录中并解压。</p><p>提示：此主题使用到的插件为 contact-form-7，只需要上传这个插件就可以。其它主题如果使用了其它的插件，操作方法是一样的，将主题使用到的所有插件都上传到 plugins 目录并解压。</p><p>到此 WordPress 主题安装完成</p><h3><strong>3、设置主题</strong></h3><h4>3.1、登录后台</h4><p><a href="https://link.segmentfault.com/?enc=fhZ%2FP5CsxxQ6nZvTFMUR7A%3D%3D.GLXpHuERe6KLzA%2FwQxY9s2TWkPheMSd6gRS0csrV1gqhO0cL4AiSpxkzc4di8ZWK" rel="nofollow" target="_blank">www.wodepress.com/wp-admin</a></p><p>默认帐号 admin 123456</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578945" alt="885c9ab37790fa261a434e769b5be486.png" title="885c9ab37790fa261a434e769b5be486.png" loading="lazy"/></p><p>登录成功后第一件事，修改初始密码。重点提示：这个一定要改，非常重要。</p><h4>3.2、启用主题</h4><p>在后台“外观”-“主题”中启用主题</p><h4>3.3、重置主题选项</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578946" alt="157188070652b9c7e5b4d255d7ae4051.png" title="157188070652b9c7e5b4d255d7ae4051.png" loading="lazy"/></p><p>在后台“外观”-“主题选项”中，点击“重置”，成功重置主题后，再点“保存”。</p><h4>3.4、设置菜单</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578947" alt="f9ab431c7de4833844ace383b2ef169f.png" title="f9ab431c7de4833844ace383b2ef169f.png" loading="lazy"/></p><p>在后台“外观”-“菜单”中可以看到有 4 个菜单，如果需要对各菜单里的项目进行修改，选择相应的菜单编辑，编辑完成保存即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578948" alt="60174d59e00e89cf5978423abddcf3ae.png" title="60174d59e00e89cf5978423abddcf3ae.png" loading="lazy"/></p><p>比如，要编辑 header（顶部导入）中的 home 的链接，就选择这个菜单并点“选择”，然后再点“home”后面的三角图标，对出来的链接进行修改就可以。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578949" alt="a6f71bf937d581696e42f7929380c209.png" title="a6f71bf937d581696e42f7929380c209.png" loading="lazy"/></p><p>另外提示，4 个菜单与在最底部的 4 个位置相互对应，如果某个位置的菜单没有显示出来，可能就是没有选择这里的位置。</p><p>3.5、首页各模块及基础设置</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578950" alt="653d6c5d407924d0b98a247940fa1c7f.png" title="653d6c5d407924d0b98a247940fa1c7f.png" loading="lazy"/></p><p>主题后台采用主题选项来管理，在“基础设置”里可以设置 logo、联系方式、地图等；首页的各模块也有相应的位置可以设置。比如，首页大图、关于我们在主题选项里有对应的位置可以设置。提示：设置完了，一定要点最下面的“保存”。</p><p>每个主题的后台不完全一样，设置方法也不完全相同。以上设置方法是以 WordPress 主题为例，其它主机面板下、其它主题的方法也类似，基本上是大同小异，可以以这个作为参考来设置。</p>]]></description></item><item>    <title><![CDATA[在 KubeSphere 上运行 Moltbot（Clawdbot）：自托管 AI 助手的云原生实践]]></title>    <link>https://segmentfault.com/a/1190000047578992</link>    <guid>https://segmentfault.com/a/1190000047578992</guid>    <pubDate>2026-01-28 19:05:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>更名说明：Clawdbot 项目已更名为 <strong>Moltbot</strong>，当前官方仓库为 <a href="https://link.segmentfault.com/?enc=svQDGSSOlAZ5kZE%2FyieRcw%3D%3D.d0ZmE%2F2Q%2FiHkG3y0DqpDnMQ1aL61Csf6NaJ07xlrtdjbzbyPLtprY0Tom4f427L0" rel="nofollow" target="_blank">https://github.com/moltbot/moltbot</a>。 本文基于最新仓库内容和 README 编写。</p><h2>一、什么是 Clawdbot？</h2><p><strong>Clawdbot</strong> 是一个开源的、自托管的个人 AI 助手框架。它以本地服务或守护进程的形式运行，通过接入不同的消息通道（如 Telegram、Discord、Slack 等）与用户交互，并根据配置执行自动化任务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578994" alt="" title=""/></p><p>从官方 README 可以明确以下几点：</p><ul><li><strong>完全自托管</strong>：非 SaaS，数据和运行完全由用户控制</li><li><strong>CLI + Gateway 服务</strong>：核心运行方式，支持多种部署模式</li><li><strong>配置驱动</strong>：通过配置文件定义 Bot 的行为和接入通道</li><li><strong>长期运行</strong>：适合作为后台服务持续运行</li></ul><p>该项目更偏向“<strong>可编排的 AI 助手代理</strong>”，而不是传统意义上的云服务或平台组件。</p><h2>二、为什么在 KubeSphere 上运行 Clawdbot？</h2><p>虽然 Clawdbot 可以直接在本地或单台服务器上运行，但在团队或长期运行场景中，使用 Kubernetes 平台更具优势，而 KubeSphere 提供了完整的可视化运维能力。</p><p>将 Clawdbot 部署在 KubeSphere 上可以解决以下问题：</p><ul><li><strong>部署标准化</strong>：避免手动维护本地守护进程</li><li><strong>配置集中管理</strong>：通过 ConfigMap / Secret 管理 Bot 配置和密钥</li><li><strong>运行状态可观测</strong>：统一查看日志和 Pod 状态</li><li><strong>可重复部署</strong>：同一套定义可在不同环境复用</li></ul><p>对于希望将 Bot 类服务纳入云原生运维体系的团队，这是一种更稳妥的方式。</p><h2>三、部署前准备</h2><h3>环境要求</h3><ul><li><strong>KubeSphere 集群</strong>：已部署完成，版本要求 v4.x 及以上</li><li><strong>已安装扩展</strong>：KubeSphere Gateway 及 cert-manager 扩展</li><li><strong>镜像仓库</strong>：可访问公有或私有镜像仓库</li></ul><h2>四、在 KubeSphere 中部署 Clawdbot</h2><h3>步骤一：安装扩展组件</h3><p>将 Clawdbot 扩展组件推送到 KubeSphere 扩展商店，并进行安装。在安装过程中，通过扩展组件配置加载相关密钥：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578995" alt="" title="" loading="lazy"/></p><p>您需要将相关秘钥通过扩展组件配置加载到 Clawdbot。</p><pre><code class="yaml">clawdbot:
  secrets:
    create: true
    data:
      # Set via --set or environment variables
      anthropicApiKey: ""
      openaiApiKey: ""
      discordBotToken: ""
      telegramBotToken: ""
      gatewayToken: ""  # Auto-generated if empty</code></pre><p>注意: 当前配置中禁用了控制界面中的设备识别和配对功能。</p><h3>步骤二：配置 Ingress</h3><p>您可以通过 KubeSphere Gateway 扩展配合 cert-manager 扩展，使用 HTTPS 协议将 Clawdbot 服务以 Ingress 的方式对外暴露。</p><p>首先，在集群中创建并启用集群网关，作为统一的 Ingress 入口。随后，在该网关之上为 Clawdbot 服务创建对应的应用路由，并通过 cert-manager 自动签发和管理 TLS 证书，从而实现安全的 HTTPS 访问。</p><pre><code class="yaml">kind: Ingress
apiVersion: networking.k8s.io/v1
metadata:
  name: clawdbot
  namespace: extension-clawdbot
  annotations:
    cert-manager.io/cluster-issuer: default-issuer       # 借助 cert-manager, 为 ingress 自动生成和创建证书
    kubesphere.io/creator: admin
spec:
  ingressClassName: kubesphere-router-cluster
  tls:
    - hosts:
        - 172.31.19.4.nip.io
      secretName: clawdbot-cert
  rules:
    - host: 172.31.19.4.nip.io
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: clawdbot
                port:
                  number: 18789
</code></pre><h3>步骤三：访问控制页面</h3><p>使用如下命令获取 Gateway token:</p><pre><code>kubectl get secret clawdbot -n extension-clawdbot \
  -o jsonpath='{.data.gatewayToken}' | base64 --decode</code></pre><p>然后在浏览器中输入以下地址访问 Clawdbot 控制页面：</p><pre><code>https://{ingress 暴露地址}/?token=${token}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578996" alt="" title="" loading="lazy"/></p><h2>五、运维建议</h2><p>在生产或长期运行场景中，建议遵循以下最佳实践：</p><ul><li><strong>资源限制</strong>：为 Clawdbot 设置合理的 CPU / 内存限制，避免资源争用。</li><li><strong>配置管理</strong>：通过修改 ConfigMap 调整 Bot 行为，而非重新构建镜像。</li><li><strong>版本更新</strong>：定期更新镜像，跟进上游版本变更，获取新功能和修复。</li><li><strong>密钥轮换</strong>：对 Secret 中的 Token 进行定期轮换，增强安全性。</li></ul><p>需要注意的是，Clawdbot 本身并不负责外部平台的权限管理，相关 OAuth 或 Bot Token 仍需在对应平台侧正确配置。</p><h2>六、总结</h2><p>Clawdbot 并不是一个“即开即用”的 SaaS 产品，而是一个强调自主可控、可编排、可长期运行的 AI 助手框架。这类服务一旦进入稳定使用阶段，其运行可靠性、配置管理能力和运维成本，往往比功能本身更重要。</p><p>通过 KubeSphere，将 Clawdbot 纳入 Kubernetes 的统一管理体系，可以在不改变其原有架构和使用方式的前提下，获得标准化部署、可观测运维以及安全可控的运行环境。对于希望长期运行 Bot 服务、或在团队内复用 AI 助手能力的用户来说，这是一条非常自然、也非常稳妥的路径。</p><p>如果你正在寻找一种更工程化、更可持续的方式来运行自托管 AI 服务，不妨试试 KubeSphere ——它不仅适合管理传统应用，也同样适合承载新一代的 AI Agent 与自动化服务。</p><p>欢迎大家体验 KubeSphere，也欢迎在社区中分享你自己的 AI + 云原生实践。</p>]]></description></item><item>    <title><![CDATA[这8个工具能让你 24 小时内，一个人活成一支 AI 技术团队 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047579007</link>    <guid>https://segmentfault.com/a/1190000047579007</guid>    <pubDate>2026-01-28 19:04:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>收藏工具是很多开发者的习惯。在 GitHub 上点 Star，然后放进收藏夹吃灰，好像这样就能自动拥有了这种能力</p><p>没错，就跟减肥健身一样，收藏了=做过了，吓吓身上的肥肉。</p><p>当大多数人还在手动写 CRUD、用肉眼盯着模型训练、在繁杂的项目文档中溺水时，极少数大聪明就开始用工具构建了自动化体系。</p><p>如果在 24 小时内部署好这8个工具，就会彻底告别低效的体力劳动。</p><h3><a href="https://link.segmentfault.com/?enc=IYHB2Xckns%2BCU2mA0bnDpQ%3D%3D.E%2BlACllzgAM4il1l6EKC0WdaAxaUmnuu4cbQgvc0suo%3D" rel="nofollow" target="_blank">Ivy</a></h3><p><strong>解决痛点：</strong> <strong>深度学习</strong> <strong>框架的生殖隔离</strong></p><p><img width="723" height="331" referrerpolicy="no-referrer" src="/img/bVdnNDn" alt="image.png" title="image.png"/></p><p>做 AI 开发的时候，好不容易找到一篇绝佳论文，代码是 PyTorch 写的，而基础设施全套是 TensorFlow。重写模型需要一周，放弃又不甘心。</p><p>这时候 Ivy 就能派上用场了，它是一个机器学习框架的转换器（Transpiler）。它能把代码转译成框架无关的中间表示，让你可以用 PyTorch 写代码，然后在 TensorFlow 的后端上运行，或者反之。它打破了框架之间的壁垒，让复用开源模型变得不再痛苦。</p><p>安装方式：</p><pre><code class="bash">pip install ivy</code></pre><h3><a href="https://link.segmentfault.com/?enc=ITWFKjoiw4mQ1MquFe4CAQ%3D%3D.B9k%2FqS10wVfMuV3TFmQrigx0RLqkDA5ufMfal6fcK4s%3D" rel="nofollow" target="_blank">MLflow</a></h3><p><strong>解决痛点：实验过程的失忆</strong></p><p><img width="723" height="448" referrerpolicy="no-referrer" src="/img/bVdnNDp" alt="redundant_retrievals-17b12e1e6c05c41cc4958e38006d6b64.gif" title="redundant_retrievals-17b12e1e6c05c41cc4958e38006d6b64.gif" loading="lazy"/></p><p>两周前训练出一个准确率 95% 的模型，今天想复现，却死活想不起当时的参数是 0.01 还是 0.001。老祖宗说的好，好记性不如烂笔头。</p><p>MLflow 就是全自动烂笔头，它能记住所有东西。它不干涉怎么写模型，只负责记录。它会追踪每一次实验的代码版本、数据哈希、超参数设置和最终指标。当项目变得复杂时，它是保证实验可追溯、模型可复现的基础设施。</p><p>安装方式：</p><pre><code class="bash">pip install mlflow</code></pre><h3><a href="https://link.segmentfault.com/?enc=8dwNSKktmRqtoAdrS8yLZQ%3D%3D.kZK2FvR2sWls%2FFsTu3L6wMvknfdx8Z4vKLmkv%2FzTCo4%3D" rel="nofollow" target="_blank">Evidently</a></h3><p><strong>解决痛点：模型上线后的数据漂移</strong></p><p><img width="723" height="416" referrerpolicy="no-referrer" src="/img/bVdnNDq" alt="dashboard_llm_tabs.gif" title="dashboard_llm_tabs.gif" loading="lazy"/></p><p>模型在训练集上准确率 99%，上线一个月后效果却莫名其妙下降。这通常是因为“数据漂移”（Data Drift），带清都亡了，你的模型还在搞反清复明那一套。</p><p>Evidently 专门用来监控这种现象。它不看 CPU 内存，只看数据。它通过对比训练数据和线上实时数据的分布差异，生成直观的报告。一旦发现输入特征发生偏移，或者模型预测倾向出现异常，它能立刻发出警报。这是防止 AI 系统在生产环境中撒谎的必要工具。</p><h3><a href="https://link.segmentfault.com/?enc=1f5EGiyqnacNdRSlLPJLhg%3D%3D.BwqqtWA19AesY0jsNOe48Wuq0JCxcwC0xpjZxOSjopg%3D" rel="nofollow" target="_blank">Prefect</a></h3><p><strong>解决痛点：脆弱的</strong> <strong>Crontab</strong> <strong>和胶水代码</strong></p><p><img width="723" height="478" referrerpolicy="no-referrer" src="/img/bVdnNDr" alt="image.png" title="image.png" loading="lazy"/></p><p>很多数据流水线最初只是几个 Python 脚本，用 Crontab 定时跑。一旦任务失败、依赖卡死或者需要重试，维护成本就直线上升。</p><p>Prefect 是现代化的流程编排工具。它接管调度、日志、重试和通知。原本需要写满 <code>try-except</code> 的脏活，现在只需要一个装饰器。让数据流转像瑞士钟表一样精准，而不是像摇摇欲坠的积木。</p><p>安装方式：</p><pre><code class="bash">pip install evidently</code></pre><h3><a href="https://link.segmentfault.com/?enc=TbCxbjcJoDfdbfzhm0KZHg%3D%3D.NToSc1vAGLKqMp5z6rEQQ%2BZ%2Fzb10csDemISjVFM%2BIPE%3D" rel="nofollow" target="_blank">Huly Platform</a></h3><p><strong>解决痛点：项目管理工具的割裂</strong></p><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnNDs" alt="image.png" title="image.png" loading="lazy"/></p><p>Linear 追任务，Slack 聊需求，Notion 记文档。每天在三个网页间切换 500 次，你的注意力就是这样被撕碎的。</p><p>Huly 是一个开源的一体化平台，把项目管理、即时通讯和知识库整合在一起。它基于 Node.js 构建，不仅能替代 Jira/Linear，还允许通过 AI 智能体来自动化处理任务流转。对于希望数据私有化且厌倦了 SaaS 订阅费的团队，这是一个极佳的替代方案。</p><p>安装方式：直接下载即可</p><p>但需要使用 npm 进行身份验证：</p><pre><code class="bash">npm login --registry=https://npm.pkg.github.com</code></pre><h3><a href="https://link.segmentfault.com/?enc=mGC20cz%2BR3dVtRPPlHgFZg%3D%3D.aitVylY97v%2BLG5D8BYX9W6%2B5VZYZgdMy8lojhUw6GX0%3D" rel="nofollow" target="_blank">OpenCode</a></h3><p><strong>解决痛点：被</strong> <strong>IDE</strong> <strong>插件锁死，AI 编程缺乏掌控感</strong></p><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnNDs" alt="image.png" title="image.png" loading="lazy"/></p><p>市面上的 AI 编程助手都在试图把你锁死在他们的 IDE 里，强推闭源模型。你以为你在用 AI，其实你是被 AI 厂商圈养的数据工。</p><p>OpenCode (opencode.ai) 就不是这样，它是<strong>终端优先（Terminal-first）的 AI 编程</strong> <strong>智能体</strong>。它不依赖浏览器或特定编辑器，而是直接在终端里通过自然语言与代码库交互。</p><ul><li><strong>拒绝被宰</strong>：支持 75+ 种模型。用 Claude 3.5 写逻辑，用本地 Ollama 跑隐私数据，完全由你掌控。</li><li><strong>双脑协作</strong>：Plan Agent 负责思考，Build Agent 负责执行，逻辑严密。</li><li><strong>拒绝幻觉</strong>：深度集成 LSP，它能看懂代码结构，而不是瞎猜变量名。</li></ul><p>对于习惯命令行、重视隐私且不想被大厂生态绑架的开发者，OpenCode 是目前最自由的替代方案。</p><p>安装方式</p><pre><code class="bash">npm i -g opencode-ai</code></pre><h3><a href="https://link.segmentfault.com/?enc=4M62R0C82%2BHQkN4FKZP4Aw%3D%3D.7fAp3GhsGVhUaNxGZl1bEF1QEV4nM3OrazjiCJeL%2Fig%3D" rel="nofollow" target="_blank">Krayin CRM</a></h3><p><strong>解决痛点：</strong> <strong>CRM</strong> <strong>系统只进不出，缺乏生产力</strong></p><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnNDs" alt="image.png" title="image.png" loading="lazy"/></p><p>销售人员最恨录入数据。一个只能记录不能产出的 CRM，就是企业的僵尸资产。</p><p>Krayin 引入了 AI 模块来提升效率：</p><ul><li><strong>内容生成</strong>：自动起草跟进邮件、整理会议纪要，销售只需简单修改即可发送。</li><li><strong>智能补全</strong>：在详情页辅助填充客户信息，减少手动录入工作量。</li><li><strong>上下文增强</strong>：在记录日志时，AI 能根据简短的关键词扩充成完整的业务记录。</li></ul><p>对于熟悉 PHP 技术栈的团队，Krayin 是一个兼具灵活性和智能化的高性价比选择。</p><p>安装方式：需要PHP 8.1及以上版本；Node 8.11.3 LTS 及以上版本；还有 MySQL 或 MariaDB 数据库</p><pre><code class="bash">composer create-project

# 找到根目录中的.env文件，并将APP_URL参数更改为您的域名。
# 另外，请在.env文件中配置邮件和数据库参数。

php artisan krayin-crm:install</code></pre><h3><a href="https://link.segmentfault.com/?enc=8PKd%2Br2OLzGbnwux0jP2WA%3D%3D.k98iDwlWhCsMvShBfHNDUzgWIkr3E3KC0JMfUDEC0Nk%3D" rel="nofollow" target="_blank">IDURAR</a></h3><p><strong>解决痛点：ERP 系统僵化，难以二开</strong></p><p><img width="723" height="385" referrerpolicy="no-referrer" src="/img/bVdnNDt" alt="image.png" title="image.png" loading="lazy"/></p><p>中小企业需要 ERP/CRM，但市面上的 SaaS 软件要么太贵，要么功能太死板。IDURAR 基于 Node.js (MERN 栈)，天生就是为了被修改和集成设计的。</p><p>它的 AI 集成策略非常务实：通过 API 连接外部 AI 服务。系统本身提供稳固的业务流程（销售、库存、发票），同时留出接口让开发者挂载自定义的 AI 逻辑。比如连接一个微调过的模型来分析销售数据，或者自动更新库存状态。这种松耦合设计非常适合开发者进行定制。</p><p>安装方式：需要创建 MongoDB 帐户和数据库集群</p><pre><code class="bash">git clone https://github.com/idurar/idurar-erp-crm.git
cd idurar-erp-crm
cd backend
npm install</code></pre><p>把上面这些工具跑起来，就会发现技术栈挺杂的。</p><ul><li><strong>Ivy, MLflow, Prefect, Evidently</strong>：深度依赖 <strong>Python</strong>，且对版本敏感。</li><li><strong>Huly, OpenCode, IDURAR</strong>：基于 <strong>Node.js</strong>，前后端依赖包复杂。</li><li><strong>Krayin CRM</strong>：基于 <strong>PHP</strong> (Laravel)，需要配置 Web Server 和数据库。</li></ul><p>如果在本地电脑上混装这些环境，光是处理环境变量冲突、依赖打架就能折腾一天。</p><p>为了保持开发环境的纯净，可以使用 <strong>ServBay</strong>。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnNDu" alt="image.png" title="image.png" loading="lazy"/></p><p>它不是虚拟机，也不需要编写 Dockerfile，主要作用就是<strong>环境隔离与快速切换</strong>。ServBay 允许在同一台机器上共存多个版本的 Python、Node.js 和 PHP。</p><ul><li>想跑 Krayin？一键切换到 PHP 8.2 环境。</li><li>想试用 Huly？切到 Node.js 20。</li><li>搞深度学习？切回 Python 3.10。</li></ul><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnNDu" alt="image.png" title="image.png" loading="lazy"/></p><p>它自动处理了路径和依赖问题，让这些工具能互不干扰地运行。对于喜欢折腾各种开源项目但又不想把系统搞乱的开发者来说，这是一个非常实用的工具。</p>]]></description></item><item>    <title><![CDATA[构建开放智能体生态：AgentScope 如何用 A2A 协议与 Nacos 打通协作壁垒？ 阿里云]]></title>    <link>https://segmentfault.com/a/1190000047579009</link>    <guid>https://segmentfault.com/a/1190000047579009</guid>    <pubDate>2026-01-28 19:03:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：恰橙、席翁、濯光</p><blockquote>AgentScope 基于 A2A 协议与 Nacos Agent Registry，实现智能体的统一发现、治理与跨生态协作。</blockquote><p>随着企业逐步落地 AI 应用架构，从原来测试 POC workflow/简单 Agent 开始逐步构建生产级可用 Agent，真正解决线上问题，构建 Agent 在企业是面相全员提升效率的路径，不再是简单业务流程面临问题更加复杂，可能企业就会遇到如下挑战：</p><ul><li><strong>语言栈多样化</strong>：企业内核心业务团队可能是 Java/Golang，算法团队使用 Python，面临 Agent 架构选型多语言栈怎么做无缝协作？</li><li><strong>Agent 框架割裂</strong>：LangChain、AutoGPT、AgentScope 等不同框架以及 Agent 各自为政，如何实现跨框架调用？</li><li><strong>多团队Agent协同</strong>：Agent 如果有一个团队做，不懂业务做不深，Agent 分布在不同的服务、团队、项目中，内部选型会有 Dify、n8n 低代码和高代码平台选型，如何统一发现和管理？</li><li><strong>协议不统一</strong>：REST、gRPC、自定义协议...每个 Agent 都有自己的接口规范，集成成本高、维护困难。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579011" alt="image" title="image"/></p><p><strong>A2A（Agent-to-Agent）协议正是为解决这些问题而生。</strong> 它是 Google 提出一套面向分布式多 Agent 互联互通的开放标准，定义了统一的消息结构和能力描述，让不同语言、不同框架、不同运行时上的 Agent 都能被发现、被调用、被编排。基于 A2A，Agent 之间可以在不共享代码、不耦合底层实现的前提下，完成文本对话、thinking、多模态内容、工具调用等丰富交互，真正实现“一次定义，处处可用”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579012" alt="image" title="image" loading="lazy"/></p><h2>Agent 跨语言、跨框架调用最佳实践</h2><p><strong>AgentScope</strong> 是阿里巴巴推出的一款以开发者为核心，专注于<strong>多智能体开发的开源框架</strong>。它的核心目标是解决智能体在构建、运行和管理中的难题，提供一套覆盖“开发、部署、监控”全生命周期的生产级解决方案。</p><p>在 <strong>AgentScope</strong> 最新版本中，我们<strong>全面支持 A2A 协议，并集成 Nacos 作为 A2A Registry 的默认实现</strong>，构建了一套从开发到部署的完整分布式多智能体协作体系，让智能体协作从“单打独斗”走向“开放互联”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579013" alt="image" title="image" loading="lazy"/></p><ul><li><strong>告别“Agent 孤岛”</strong>：通过 A2A 协议，AgentScope 的 Agent 可以与任何实现 A2A 的 Agent 无缝互操作，不论由谁开发、用何种技术栈构建，都能在统一的协作框架下高效协同，打破技术壁垒，共同构建跨语言、跨框架的开放生态。</li><li><strong>统一开发体验，告别适配烦恼</strong>：在 AgentScope 中，调用本地 Agent 和调用远端 A2A Agent 使用同一套 API。框架自动处理协议转换、错误重试和路由选择，我们可以专注业务，不必为适配不同 Agent 编写冗余代码，从而提升效率与可维护性。</li><li><strong>生产级治理，开箱即用</strong>：基于 Nacos 3.0 智能体注册中心，AgentScope 应用具备服务发现、健康检查、命名空间隔离等成熟能力。选择 Nacos 作为默认 A2A Registry，不仅因为它经过大规模生产验证，也因为它与企业现有运维体系兼容，让智能体治理无需重复造轮子，加速规模化落地。</li></ul><h2>在 AgentScope 中使用 A2A</h2><h3>1. AgentScope：连接外部 A2A 网络，像调用本地 Agent 一样简单</h3><p>AgentScope 提供统一的 A2A 对接能力，我们可以像调用本地工具一样自然地调用远端 A2A Agent，实现跨语言、跨框架的协同，告别繁琐的协议适配工作：</p><ul><li><strong>双向消息转换：</strong> 实现框架内部消息格式与 A2A <code>Message</code> 的双向转换，支持文本、thinking、多模态、工具调用等 Block 类型，保留必要元信息，确保语义一致。</li><li><strong>统一交互范式：</strong> 支持直接调用和 <code>observe()</code> 两种方式。直接调用 <code>agent(msg)</code> 可立即拿到结果；<code>observe()</code> 先累积上下文，后续再连同当前输入一起发送，适合长会话、多轮协作场景。</li><li><strong>任务与中断管理：</strong> 内建长任务状态管理与 Artifact 处理机制，支持长时间任务的平滑中断，覆盖超时与取消场景。</li><li><strong>统一的服务发现能力：</strong> 通过 <code>AgentCardResolver</code> 扩展点标准化“发现”能力，任何实现该接口的组件，例如：<code>FixedAgentCardResolver</code>、<code>FileAgentCardResolver</code>、<code>WellKnownAgentCardResolver</code>、<code>NacosAgentCardResolver</code> 等都可按需加载，轻松适配不同基础设施。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579014" alt="image" title="image" loading="lazy"/></p><p>通过 <code>A2AAgent</code> 以及 <code>AgentCardResolver</code>，我们可以按名称、分组或标签从 A2A Registry 中发现并调用其他 Agent，实现跨团队、跨项目甚至跨语言的智能体复用。基于 A2A Registry，智能体拥有统一的服务发现与治理能力，可与现有配置中心、网关、熔断限流及安全体系协同，为大规模分布式智能体应用打好底座。</p><p>以下示例展示如何使用 <code>NacosAgentCardResolver</code> 从 Nacos Registry 中发现并调用 Agent：</p><p><em>注意在对应版本以上使用 demo，</em> <strong><em>Python</em></strong> <em>（AgentScope</em> <strong><em>v1.0.11</em></strong> <em>、AgentScope Runtime</em> <strong><em>v1.0.4</em></strong> <em>）和 </em> <strong><em>Java</em></strong> <em>（AgentScope</em> <strong><em>v1.0.6</em></strong> <em>，AgentScope Runtime</em> <strong><em>v1.0.0</em></strong> <em>）</em></p><h4>Python 代码示例</h4><p><strong>查看详细文档：<a href="https://link.segmentfault.com/?enc=SIi8S7%2BLrbwdF6HxUQnEFQ%3D%3D.Exe%2BseLhAszGP%2FY8OVf%2BCin0JDhVEkehIg5D5cqcXT4AG1O7GVu6HSq0jX9JMf65GS0tvlfwWGyIs%2FjOUCyEGw%3D%3D" rel="nofollow" target="_blank">https://doc.agentscope.io/zh_CN/tutorial/task_a2a.html</a></strong></p><pre><code>from agentscope.agent import A2AAgent
from agentscope.a2a import NacosAgentCardResolver
from agentscope.message import Msg
# Python AgentScope v1.0.11以上
# 创建 Nacos AgentCard Resolver
nacos_resolver = NacosAgentCardResolver(
    remote_agent_name="my-remote-agent",  # Nacos 中注册的智能体名称
    nacos_client_config=ClientConfig(
        server_addresses="http://localhost:8848",  # Nacos 服务器地址
        # 其他可选配置项
    ),
)
# 使用 Resolver 创建 A2AAgent，通过名称从 Nacos 发现 Agent
agent = A2AAgent(
    agent_card=await nacos_resolver.get_agent_card()
)</code></pre><h4>Java 代码示例</h4><p><strong>查看详细文档：<a href="https://link.segmentfault.com/?enc=rT3EU%2B6aK7xOBrt4zakWRQ%3D%3D.ksAjOIcnsg7YG2xvQPuYWVb6qkEsb3zQVU6Gb5e9XTmV1WfFG8%2FHt8bxmhGPE5Qi" rel="nofollow" target="_blank">https://java.agentscope.io/zh/task/a2a.html</a></strong></p><p>使用 <code>NacosAgentCardResolver</code> 从 Nacos Registry 中发现 Agent：</p><pre><code>import io.agentscope.agent.A2AAgent;
import io.agentscope.extensions.a2a.nacos.NacosAgentCardResolver;
import java.util.Properties;
import com.alibaba.nacos.api.PropertyKeyConst;
import com.alibaba.nacos.api.ai.AiFactory;
import com.alibaba.nacos.api.ai.AiService;
Properties properties = new Properties();
properties.put(PropertyKeyConst.SERVER_ADDR, "localhost:8848");
// 其他可选配置项
AiService aiService = AiFactory.createAiService(properties);
NacosAgentCardResolver agentCardResolver = new NacosAgentCardResolver(aiService);
A2AAgent agent = A2AAgent.builder()
        .name("MyAgent")
        .agentCardResolver(agentCardResolver)
        .build();</code></pre><p>Nacos 3.0 作为智能体注册中心，其在生产环境中久经验证的服务发现与配置管理能力，能够助力企业构建统一的智能体服务治理平台。</p><h3>2. AgentScope Runtime：暴露 A2A Agent 服务，启动即注册</h3><p>AgentScope Runtime 提供统一的 A2A 服务暴露能力，帮助我们把本地 Agent 应用包装成符合 A2A 规范的服务端点。通过 A2A 协议适配器，应用在启动时会自动完成：</p><ul><li><strong>结构化配置体系</strong>：通过 A2A 扩展配置 <code>a2a_config</code> 灵活定义 AgentCard（name、description、version、skills、default_input_modes/default_output_modes 等）、传输层配置（host、port、path 等）、Registry 参数和任务超时等。</li><li><strong>自动服务包装</strong>：启动时由 A2A 协议适配器将 Agent 应用封装成符合 A2A 规范的服务端点，自动处理协议转换、消息路由等底层细节。</li><li><strong>生产级部署支持</strong>：与主流框架无缝集成，Python 侧支持 <code>AgentApp</code> 配置体系，Java 侧支持 <code>Spring Boot Starter</code>，让智能体服务自然融入现有基础设施。</li><li><strong>自动服务注册与治理</strong>：通过 <code>A2ARegistry</code> 抽象接口，Python 与 Java 都能开箱即用地集成 Nacos Agent Registry。Agent 能力描述（AgentCard）和网络端点会自动注册到 Registry，让其他 Agent 可发现、可调用。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579015" alt="image" title="image" loading="lazy"/></p><p>以下示例展示如何在 Runtime 层使用 Nacos Registry 进行服务注册：</p><h4>Python 代码示例</h4><p><strong>查看详细文档：<a href="https://link.segmentfault.com/?enc=NXGQuwgVsb81F05aHhndMA%3D%3D.JjvKiYIk7diRl7kXohf9Duqxu9LEiipOoC%2FSzUpcGlvPGEkJF5HcgCjR5SfAMdMqFuNz%2Bh%2FYRaVafdPno0qW8g%3D%3D" rel="nofollow" target="_blank">https://runtime.agentscope.io/zh/a2a_registry.html</a></strong></p><p><strong>方式一：参数配置</strong></p><p>在构造 AgentApp 时，通过 A2A 配置扩展字段 a2a_config 参数的 registry 字段指定 Registry 实例或列表：</p><pre><code>from agentscope_runtime.engine.app import AgentApp
from agentscope_runtime.engine.deployers.adapter.a2a import (
    AgentCardWithRuntimeConfig,
)
from agentscope_runtime.engine.deployers.adapter.a2a.nacos_a2a_registry import (
    NacosRegistry,
)
from v2.nacos import ClientConfigBuilder
# 创建 Nacos Registry 实例
registry = NacosRegistry(
    nacos_client_config=ClientConfigBuilder()
        .server_address("nacos-server:8848")
        # 其他可选配置项
        .build()
)
app = AgentApp(
    app_name="TestAgent",
    app_description="TestAgent",
    # 在 a2a_config 中配置 registry
    a2a_config=AgentCardWithRuntimeConfig(registry=registry),
)</code></pre><p><strong>方式二：使用环境变量配置</strong></p><p>环境变量可以通过 .env 文件或系统环境变量设置：</p><pre><code># .env 文件示例
A2A_REGISTRY_ENABLED=true
A2A_REGISTRY_TYPE=nacos
NACOS_SERVER_ADDR=localhost:8848
# 其他可选配置项</code></pre><h4>Java 代码示例</h4><p><strong>查看详细文档：<a href="https://link.segmentfault.com/?enc=y91K%2BeRVqRUdrYebHbKSLA%3D%3D.6DsJyMz7b0BYxH77munmGoWU8t%2BZVBp2aH7fYQJ342XrRqGwwtZ5wF%2FXX8MXDlo6" rel="nofollow" target="_blank">https://java.agentscope.io/zh/task/a2a.html</a></strong></p><p>在最新版本的 Java AgentScope 中，应用可以直接暴露 A2A 服务，只有在需要使用 Sandbox 时，才需要使用 Runtime。</p><p>对于非最新版本，Java 开发者可以将 AgentScope Agent 无缝融入现有的 Spring Boot 基础设施体系。通过引入 <code>spring-boot-starter-agentscope-runtime-a2a-nacos</code> 依赖，应用在启动时会自动暴露 A2A 服务并注册到 Nacos Registry。</p><p><strong>Maven 依赖配置</strong>：</p><pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.agentscope&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-agentscope-runtime-a2a-nacos&lt;/artifactId&gt;
    &lt;version&gt;1.0.3&lt;/version&gt;
&lt;/dependency&gt;</code></pre><p><strong>application.yaml 配置</strong>：</p><pre><code>agentscope:
  a2a:
    server:
      card:
        description: "基于 A2A 协议的 Java 智能体"
        provider:
          organization: 您的组织名称
          url: https://your-organization.com
      nacos:
        server-addr: ${NACOS_SERVER_ADDRESS:127.0.0.1:8848}
        # 其他可选配置项</code></pre><p>通过上述配置，Spring Boot 应用在启动时会自动：</p><ul><li>暴露符合 A2A 规范的 JSONRPC 服务端点（默认路径：<code>/a2a/jsonrpc</code>）。</li><li>暴露 AgentCard 的 Well-Known 端点（默认路径：<code>/.well-known/agent-card.json</code>），用于其他 Agent 发现和了解当前 Agent 的能力。</li><li>自动处理 A2A 协议的消息转换和路由，将 A2A 消息格式转换为应用内部的消息处理逻辑。</li><li>支持任务超时、中断等 A2A 协议规定的运行时特性。</li><li>将 Agent 的能力描述（AgentCard）注册到 Nacos，基于 Nacos 3.0 智能体注册中心进行统一治理。</li></ul><p>得益于这一机制，AgentScope 应用启动即完成在 Nacos 的 A2A Agent 注册，为后续的发现、路由、灰度与监控奠定基础。对于已经大规模采用 Java 技术栈的团队，这意味着智能体服务能自然长在同一套基础设施上，大幅降低引入成本与运维负担。</p><h2>总结</h2><p><strong>AgentScope 全面支持 A2A 协议和 Nacos Agent Registry</strong>，标志着智能体从“单点能力”迈向“开放互联生态”的关键一步，为企业构建统一的智能体管理平台，助力大规模 Agent 化落地：</p><ul><li><strong>AgentScope 层：借助</strong> A2AAgent 与 AgentCardResolver，我们提供统一的 A2A 对接能力和灵活的发现策略，默认集成 Nacos，支持动态 Agent 发现与调用。</li><li><strong>AgentScope Runtime 层：通过 A2A 协议适配器和</strong> A2ARegistry 抽象接口，提供统一的 A2A 服务暴露能力，支持自动服务注册与治理，与 Python AgentApp 和 Java Spring Boot Starter 无缝集成。</li></ul><p>未来，我们会继续围绕 A2A 与 Registry 深耕，在发现与路由、版本与灰度、安全与访问控制等方向迭代，让面向生产的智能体应用更稳、更易用。</p><p><strong>扩展链接</strong>：</p><p>AgentScope：<a href="https://link.segmentfault.com/?enc=jhm6%2BBNOM0CiMbLOBz9Dfg%3D%3D.z13L6APvBqpxBKuXSjMdTnxwB4lHpaBhNwxI4SOLiNE%3D" rel="nofollow" target="_blank">https://doc.agentscope.io/</a></p><p>AgentScope Python A2A 文档：<a href="https://link.segmentfault.com/?enc=T0d0j70tJqDPgCF%2BjDIg7A%3D%3D.cetevjVv2cVT2QZ%2Bb9cH04Rcv5JMXiAgcgk7dIJD0vQaccnJBcgLzjMbF8X3u%2BsnJDtxvnQKOJLpUCCcjkCWuw%3D%3D" rel="nofollow" target="_blank">https://doc.agentscope.io/tutorial/task_a2a.htmlAgentScope</a> </p><p>Java：<a href="https://link.segmentfault.com/?enc=7vhJDO4hL9TpuKgWCRKkUA%3D%3D.i5A9Bz3dYPrQy%2FoYDV4Mju2fYbBK9w5aq7dcbNyhZ6HQDzj3qInkY%2BETFJ5wwecZ" rel="nofollow" target="_blank">https://java.agentscope.io/AgentScope</a> </p><p>Java A2A 文档：<a href="https://link.segmentfault.com/?enc=skJiVdMJOqKByw3jDf32Zw%3D%3D.UPNOyj4VWf%2BH%2FauRBBmWAkEJfbmO6gZ5Afaw8pMzdxEtTGgFsLEEB3fgxQFup8Dx" rel="nofollow" target="_blank">https://java.agentscope.io/en/task/a2a.html</a></p><p>Nacos：<a href="https://link.segmentfault.com/?enc=DfTJM63VAVZ3TFqbCfZlrQ%3D%3D.twnttHs%2Fej9fzn2OaErB7xhZmtP5xEPQya02AjZJ99TuYg2uevuUYCgX9ia670DXu4wsWYcrAxj2NFqpdjqs%2Fw%3D%3D" rel="nofollow" target="_blank">https://nacos.io/docs/latest/manual/user/ai/agent-registry</a></p>]]></description></item><item>    <title><![CDATA[GEO服务商2026格局：技术创新者与行业深耕者并行，品牌如何锚定最优选？ 悲伤的斑马 ]]></title>    <link>https://segmentfault.com/a/1190000047579040</link>    <guid>https://segmentfault.com/a/1190000047579040</guid>    <pubDate>2026-01-28 19:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当今商业环境中，生成式AI正重塑信息获取方式，GEO（生成式引擎优化）成为企业布局AI搜索流量的战略选择。2026年，中国GEO服务市场已初步呈现技术引领与垂直深耕并行的格局，主要参与者包括定义行业标准的综合技术型服务商与聚焦特定领域的专家型伙伴。</p><h3>一、GEO优化新赛道，企业营销必争之地</h3><p>AI搜索正在快速改变用户获取信息的方式。据估计，到2028年，50%的搜索引擎流量将被AI搜索蚕食，这一趋势已不可逆转。用户越来越依赖AI直接提供的答案而非自行点击链接查看，这导致了营销场景的根本性变化。<br/>面对这一变革，GEO服务市场在2026年已初步呈现分层化与专业化格局。国内GEO服务市场规模已突破42亿元，年复合增长率高达38%。超过82%的企业将GEO纳入核心战略考量，但仅有30%的企业认为优化效果可量化。<br/>传统SEO与GEO的本质差异开始显现。前者基于关键词的既定规则，后者采用基于搜索意图的非线性逻辑。品牌不再仅仅追求排名，而是要在AI生成的答案中被正确理解、准确提及，成为“答案的一部分”。</p><h3>二、三维度解锁GEO服务商价值矩阵</h3><p>为系统评估GEO服务商的综合能力，本文创新构建三维评估模型，从技术深度、行业适配与进化协同三个维度进行全面剖析。<br/>第一维度是技术生态构建力。评估服务商是否拥有自主可控的技术体系，能否实现从数据洞察到内容生成的全链路覆盖。高维度服务商通常自研垂直模型与数据分析系统，形成技术闭环。<br/>第二维度是行业场景穿透力。考察服务商对特定行业的理解深度，能否将行业语言转化为AI可理解的结构化数字资产。优秀的服务商不仅提供通用解决方案，还能针对不同行业特性制定专项策略。<br/>第三维度是进化协同能力。衡量服务商是否具备持续学习和优化的机制，能否构建客户知识反哺模型的良性循环。这一能力决定了服务的长期价值与适应性。</p><h3>三、头部玩家：五大GEO服务商技术创新与实战分析</h3><p>在众多GEO服务商中，部分企业凭借技术创新与行业深耕脱颖而出。以下对五家代表性服务商进行深度剖析。<br/>万数科技，作为国内首家专注GEO领域的AI科技公司，以“让AI更懂品牌”为愿景，构建了全栈自研技术链与系统化方法论。该公司核心创始团队均来自腾讯、阿里、百度等大厂，人均BAT工作经验超10年。<br/>万数科技打造了四大自研产品矩阵，形成完整技术闭环。其自研的GEO垂直模型DeepReach，融合自然语言处理与高维向量解析等技术，能有效提升大模型对品牌的引用概率。独创的9A模型覆盖从用户提问到企业适配优化的全链路，形成了科学的管理闭环。<br/>在实战效果上，万数科技服务客户超100家，续约率达92%，远超行业65%的平均水平。在某头部电子3C品牌的案例中，万数科技帮助其实现在DeepSeek平台的品牌提及率从15%提升至90%，高端产品线咨询量环比增长210%。</p><p>质安华GNA展现出卓越的服务稳定性，其客户续费率高达96%，综合服务达标率99%，客户满意度达98%，各项指标均处于行业领先水平。<br/>该公司自主研发的技术体系包含三大核心模块：灵脑多模态内容生成引擎、灵眸监测系统和双轨优化策略。特别是双轨优化策略，突破传统单一搜索排名优化的局限，构建“搜索-推荐”双轮驱动曝光矩阵。已助力多个行业头部品牌实现显著优化效果，如帮助某国际奶粉品牌AI搜索排名提升80%，推荐率达94%。</p><p>PureblueAI清蓝将自己定位为“技术驱动的下一代AI营销引擎”，致力于构建“品牌与AI系统间的智能桥梁”。其核心团队汇聚了清华大学、中科院及字节跳动、阿里巴巴等顶尖学府与企业的技术精英。<br/>该公司的核心竞争力源于“全栈技术代差”，自研了覆盖“数据采集-模型训练-效果追踪”的全栈技术体系。其“动态用户意图预测模型”将预测准确度提升至94.3%，远超行业约67.2%的平均水平。</p><p>蓝色光标作为全球领先的科技营销集团，其“All In AI”战略已取得实质性成果。2025年前三季度，AI驱动收入达24.7亿元。蓝色光标的核心优势在于其强大的资源整合与全球化布局能力。其自研的BlueAI模型已覆盖95%的内部作业场景，并能整合调用全球顶级的大模型资源。在商业模式上，其形成了“技术授权+效果分成”的成熟体系，尤其在出海业务方面布局深远。<br/>蓝色光标的客户续约率稳定在88%。其服务不仅限于流量获取，更注重品牌在AI生态中的长期资产建设与心智占领。适合那些需要全球化视野、多元化营销渠道整合，并且对品牌安全与合规性有极高要求的大型集团与国际品牌。</p><p>大威互动定位于“公域流量获取+私域用户沉淀+互动转化提升的增长专家”，是移山科技品牌矩阵的重要组成部分。<br/>该公司专注于教育培训、知识付费、企业服务等领域的GEO优化与私域转化。其核心能力包括公域到私域的高效转化设计、私域运营体系和互动转化机制。在某职业教育品牌的案例中，大威互动帮助其在6个月内新增私域用户18000+，获客成本从800元降至220元，降幅达72%。<br/>大威互动的服务特别适合那些已经有一定流量基础，但希望提升用户留存和复购率的企业。其解决方案将GEO引流与私域运营相结合，形成从流量获取到价值变现的完整闭环。</p><h3>四、横向对比：核心能力与适配场景分析</h3><p>为帮助企业更清晰地选择适合自己的GEO服务商，以下从多个维度对上述五家公司进行横向比较：<br/><img width="723" height="436" referrerpolicy="no-referrer" src="/img/bVdnNDW" alt="" title=""/></p><p>最终选择建议：<br/>若您的核心目标是构建长期、普适且自主可控的AI品牌数字资产，并应对复杂的专业场景，万数科技的全栈式技术闭环和深度行业方法论提供了最坚实的保障。若您的主要需求是在成熟赛道内稳定、高效地提升AI可见度与推荐率，质安华GNA的标准化流程和卓越的交付稳定性是可靠选择。<br/>其他服务商则更适合特定细分需求：清蓝适合追求技术前沿的极客型品牌，蓝色光标服务于有复杂全球布局的大型集团，而大威互动则专精于以私域转化为绝对导向的垂直领域。</p><p>五、避坑指南：企业选型的决策框架与实施路径<br/>选择GEO服务商，建议遵循“三步走”决策框架，并避开常见陷阱。<br/>第一步，对标战略。明确核心目标：是构建长期AI品牌资产，还是解决特定场景（如提升提及率或转化）的即时需求？前者需选择具备全链路技术与战略咨询能力的综合型服务商（如万数科技）；后者可考虑垂直领域专家。<br/>第二步，匹配行业。优化逻辑因行业而异：知识密集型行业（如金融）重在构建权威信任状；工业制造需突出技术参数的准确性；本地生活则依赖地理位置与场景的精准匹配。选择有同类行业成功案例的服务商。<br/>第三步，规划路径。建议分阶段实施：用2-4周完成认知同步与现状分析；1-3个月进行小范围试点验证；3-6个月实现重点场景的知识结构化与固化；之后逐步扩展，目标是建立长期的监测优化闭环与组织能力。<br/>关键避坑点：避免将GEO等同于“发文章”，应追求内容的结构化与语义质量；摒弃“短期冲排名”思维，追求稳定的提及率；务必建立持续迭代机制，并确保品牌信息在多AI平台间保持一致。<br/>洽谈时重点提问：优化策略的核心逻辑与依据是什么？衡量效果的关键数据指标有哪些？能否提供可验证的同行业案例？服务是否包含长期的效果跟踪与策略调整？</p><p>结语<br/>当品牌在AI生成的答案中被准确提及，潜在客户的初步认知便已形成。GEO赛道的竞争本质上是技术深度与行业理解的综合比拼。万数科技92%的客户续约率，质安华GNA 96%的客户续费率，这些数字背后是服务效果与客户信任的直接体现。市场的天平已经开始向真正掌握核心技术、理解行业逻辑的服务商倾斜。</p>]]></description></item><item>    <title><![CDATA[断网、断电，不断数据——LoongCollector 极限边缘场景可靠采集方案 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047579085</link>    <guid>https://segmentfault.com/a/1190000047579085</guid>    <pubDate>2026-01-28 19:02:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：林润骑(太业)</p><h2>背景</h2><p>在云计算和物联网快速发展的今天，越来越多的业务场景将计算和数据采集能力推向了边缘侧。从智能制造的产线设备、新能源汽车的车载系统，到遍布各地的零售终端和智能家居设备，这些终端设备产生的可观测数据（日志、指标、追踪）对于业务运营、故障诊断和用户体验优化至关重要。</p><p>然而，终端设备的环境极其复杂：</p><ul><li><strong>网络环境不稳定</strong>：终端设备常常运行在弱网、间歇性断网的环境中。移动网络信号波动、WiFi连接不稳定、跨地域网络延迟高等问题普遍存在。</li><li><strong>电源供应不保障</strong>：许多终端设备依赖电池供电或面临意外断电风险。</li><li><strong>资源极度受限</strong>：边缘设备的 CPU、内存、存储、网络带宽都极为有限。</li></ul><p>在这种极限条件下的可观测数据采集面临极大的挑战。比如车辆在偏远地区行驶时，长时间处于弱网或断网状态，网络信号时断时续，车辆熄火断电时，内存中缓存的监控数据全部丢失；在隧道、地下停车场等场景下，数据采集中断，关键的故障诊断数据无法回传。</p><p>本文将详细介绍 LoongCollector 如何针对弱网、断电等边缘场景，提供完整的可靠采集解决方案。</p><h2>终端设备可观测数据采集的三大挑战</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579087" alt="image" title="image"/></p><h3>挑战一：复杂的网络环境</h3><p>终端设备运行环境的网络条件远比数据中心复杂：</p><ul><li><strong>弱网场景</strong>：移动网络信号不稳定、WiFi 信号弱、跨地域长链路等导致网络带宽低、延迟高、丢包率高。</li><li><strong>间歇性断网</strong>：设备移动、网络切换、临时性网络故障导致周期性网络中断。</li><li><strong>长时间离线</strong>：某些场景下设备需要长时间离线工作，积累大量待上传数据。</li></ul><p>比如车载终端设备在偏远地区运输途中，可能很长时间都处于弱网或断网状态，网络正常的状态很少；在车辆熄火或者维修的情况下，车载终端设备也会断电。</p><h3>挑战二：可观测数据可靠交付</h3><p>在弱网、断电等不稳定环境下，保证数据的可靠交付和一致性是最大的挑战：</p><ul><li><strong>数据丢失风险</strong>：网络中断、设备断电、进程异常等都可能导致数据丢失。</li><li><strong>顺序性保障</strong>：时序数据（如指标、追踪）需要保持采集时的时间顺序。</li></ul><h3>挑战三：网络带宽限制</h3><p>终端设备的网络带宽通常受到严格限制：</p><ul><li><strong>流量成本高</strong>：4G/5G 移动网络的流量费用远高于数据中心专线。</li><li><strong>带宽竞争</strong>：采集数据上传需要与业务数据传输竞争有限的带宽资源。</li><li><strong>上传速率限制</strong>：某些运营商或网络环境会对上传带宽进行限制。</li></ul><p>在这样的环境下，如何高效压缩数据、智能控制发送速率、避免带宽被采集流量占满，成为必须解决的问题。</p><h2>LoongCollector：为边缘场景优化的可靠采集方案</h2><p>LoongCollector 是阿里云开源的高性能、高可靠可观测性数据采集器，在支撑阿里云内部千万级规模部署的同时，针对边缘场景进行了深度优化。</p><h3>核心能力概览</h3><h4>统一的可观测数据采集</h4><p>LoongCollector 提供了完整的可观测数据采集能力：</p><ul><li><strong>主机监控</strong>：实时采集 CPU、内存、磁盘、网络等系统指标，支持 100+ 系统指标项。</li><li><strong>Prometheus 协议</strong>：完全兼容 Prometheus 生态，可采集所有支持 Prometheus 采集的应用指标。</li><li><strong>日志采集</strong>：高效的文本日志采集能力，支持多种日志格式和解析方式。</li></ul><h4>超低资源消耗</h4><p>针对资源受限的终端设备，LoongCollector 进行了极致的性能优化：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579088" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579089" alt="image" title="image" loading="lazy"/></p><p>这意味着在相同的硬件条件下，LoongCollector 可以支持更多的采集任务，或者在资源更受限的设备上稳定运行。</p><h4>企业级稳定性保障</h4><ul><li><strong>生产级验证</strong>：支撑阿里云内部 1000 万+ 实例的可观测数据采集。</li><li><strong>高可用性</strong>：单实例高可用性，支持故障自恢复。</li><li><strong>久经考验</strong>：经历多年双11大促、突发流量等极端场景验证。</li></ul><h4>解决方案架构：数据持久化 + 异步发送 + 智能重试</h4><p>针对弱网、断电、断网等边缘场景，LoongCollector 采用了“数据持久化 + 异步发送 + 智能重试”的核心架构设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579090" alt="image" title="image" loading="lazy"/></p><p><strong>分离采集与发送</strong>：将数据采集和网络发送完全解耦，采集过程不受网络状态影响。</p><p><strong>本地持久化</strong>：日志数据天然具备本地持久化的能力。此处主要指指标等无持久化能力的数据，此方案会将所有采集到的指标，先写入本地文件，确保断电、重启也不丢失。</p><p><strong>异步消费</strong>：独立的发送线程从持久化文件中读取数据并发送，失败时自动重试。</p><p><strong>智能反压</strong>：网络异常时，自动控制数据读取速度，避免内存占用过高。</p><h4>指标数据落盘持久化</h4><p>传统的指标采集方案（如 Telegraf、Prometheus Pushgateway）通常将采集到的指标数据直接发送到服务端。这种架构在稳定网络环境下工作良好，但在边缘场景下存在致命缺陷：</p><ul><li><strong>断网丢数据</strong>：网络中断时，新采集的指标数据无法发送，只能丢弃或缓存在内存中。</li><li><strong>断电丢数据</strong>：设备意外断电时，内存中缓存的数据全部丢失。</li><li><strong>内存压力大</strong>：长时间断网时，内存缓存会迅速膨胀，最终导致 OOM。</li></ul><p>LoongCollector 创新性地将主机监控指标和 Prometheus 指标进行本地文件持久化，实现了指标数据的可靠存储：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579091" alt="image" title="image" loading="lazy"/></p><ul><li>定时抓取主机和应用指标数据。</li><li>文本格式落盘到本地文件系统。</li><li>自动轮转机制，支持单文件大小和文件个数配置，保留最近固定格式的文件，自动删除过期文件，避免磁盘空间被历史数据占满。</li></ul><h4>文件采集异步消费机制</h4><p>在持久化指标数据后，如何高效、可靠地将数据发送到服务端是下一个关键问题。传统方案面临的挑战包括：</p><ul><li><strong>发送阻塞采集</strong>：如果发送线程与采集线程耦合，网络慢会拖慢采集速度。</li><li><strong>顺序性保证</strong>：指标数据通常有时间顺序要求，需要确保按采集时间顺序发送。</li><li><strong>断点续传</strong>：网络恢复后，需要从断开位置继续发送，不能重复或遗漏。</li></ul><p>LoongCollector 采用了文件采集的方式来异步消费持久化的指标数据，<strong>关键技术点</strong>如下：</p><ul><li>Checkpoint 机制：LoongCollector 维护了细粒度的 checkpoint，记录每个文件的读取位置，这确保了即使在文件读取过程中进程崩溃或断电，重启后也能从断开位置继续读取，不会丢失数据。</li><li><p>文件顺序保证：通过文件轮转顺序，确保按采集时间顺序发送数据：</p><ul><li>优先处理时间早的文件</li><li>同一时间段的文件按序号递增处理</li><li>支持使用原始数据中的时间，避免时间戳乱序导致的数据可视化问题</li></ul></li></ul><h4>智能反压与流量控制</h4><p>在弱网环境下，如果不加控制地读取和发送数据，会导致：</p><ul><li><strong>内存占用激增</strong>：读取速度远大于发送速度，数据堆积在内存中。</li><li><strong>发送队列溢出</strong>：队列满后数据被丢弃或进程崩溃。</li><li><strong>带宽占满</strong>：采集流量占满带宽，影响业务正常通信。</li></ul><p>LoongCollector 实现了多层次的<strong>智能反压机制</strong>：</p><p>发送并发度自适应：借鉴 TCP 拥塞控制算法，LoongCollector 根据网络状态动态调整发送并发度，这种自适应机制确保了：</p><ul><li><strong>快速响应</strong>：网络正常时充分利用带宽，快速发送数据。</li><li><strong>快速收敛</strong>：网络异常时迅速降低发送频率，避免无效重试。</li><li><strong>自动恢复</strong>：网络恢复后自动增加并发，无需人工干预。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579092" alt="image" title="image" loading="lazy"/></p><ul><li><strong>队列反压</strong>：当发送队列积压达到阈值时，LoongCollector 会暂停文件读取，这避免了内存无限制增长，确保系统在长时间弱网环境下也能稳定运行。</li><li><strong>流量限速</strong>：LoongCollector 支持配置最大发送速率，避免采集流量影响业务 ilogtail_config.json：</li></ul><pre><code>{
  "max_bytes_per_sec": 1048576 # 限制最大发送速率为 10MB/s
}</code></pre><h2>LoongCollector 终端部署最佳实践</h2><p>这里以主机监控+一个应用的 Prometheus 采集为例。</p><h3>LoongCollector 启动参数建议</h3><p>在 <code>/usr/local/ilogtail</code> 目录下修改 <code>ilogtail_config.json</code>。</p><p>a. 关闭丢弃旧数据 discard_old_data。</p><p>b. 调大与服务端断开连接重启的间隔 config_server_lost_connection_timeout，建议取 604800 秒，7 天。</p><p>c. 调大读取阻塞重启的间隔 force_quit_read_timeout，建议取 604800 秒，7 天。</p><p>d. 限制最大发送速率 max_bytes_per_sec。主机监控+一个 Java 应用的流量为 0.88KB/s，所以建议取 1MB/s，避免异常使用流量。</p><p>e. "working_ip", 在移动终端场景，IP 会不断变化，在机器上建议给固定 IP。</p><p><strong>ilogtail_config.json</strong></p><pre><code>{
  "discard_old_data": false,
  "config_server_lost_connection_timeout": 604800,
  "force_quit_read_timeout": 604800,
  "max_bytes_per_sec": 1048576,
  "cpu_usage_limit": 0.4,
  "mem_usage_limit": 384,
  "working_ip": 192.168.0.1
}</code></pre><h3>采集配置</h3><h4>本地配置-主机监控采集配置</h4><p>在 /etc/ilogtail/config/local 目录下创建例如 input_host_monitor.yaml 文件，将主机指标首先采集到本地文件路径下，例如 /usr/local/ilogtail/metrics/host.log。</p><pre><code>enable: true
inputs:
  - Type: input_host_monitor
    Interval: 15
flushers:
  - Type: flusher_file
    MaxFileSize: 104857600
    MaxFiles: 10
    FilePath: /usr/local/ilogtail/metrics/host.log</code></pre><h4>本地配置-自定义指标采集配置</h4><p>在 /etc/ilogtail/config/local 目录下创建例如 input_prometheus.yaml 文件，将主机指标首先采集到本地文件路径下，例如 /usr/local/ilogtail/metrics/metric.log。</p><p>input_prometheus.yaml</p><pre><code>enable: true
inputs:
  - Type: input_prometheus
    ScrapeConfig:
      job_name: node
      host_only_mode: true
      scrape_interval: 15s
      scrape_timeout: 10s
      static_configs:
        - targets: ["localhost:12345"]
flushers:
  - Type: flusher_file
    MaxFileSize: 524288000
    MaxFiles: 10
    FilePath: /usr/local/ilogtail/metrics/metric.log</code></pre><h4>服务端管控配置-文件采集配置</h4><pre><code>{
    "aggregators": [],
    "global": {},
    "logSample": "",
    "inputs": [
        {
            "Type": "input_file",
            "FilePaths": [
                "/usr/local/ilogtail/metrics/*.log"
            ],
            "MaxDirSearchDepth": 0,
            "FileEncoding": "utf8",
            "EnableContainerDiscovery": false
        }
    ],
    "processors": [
        {
            "Type": "processor_parse_json_native",
            "SourceKey": "content",
            "KeepingSourceWhenParseFail": true
        }
    ]
}</code></pre><h3>注意事项</h3><ol><li>处理插件不要使用拓展插件，因为拓展插件会拉起 Golang 模块，导致内存占用升高。</li><li>移动终端场景，IP 会不断变化，机器组建议使用标识型机器组。</li></ol><h3>LoongCollector 资源监控测试报告</h3><h4>CPU：平均 0.02 核，峰值 0.028 核</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579093" alt="image" title="image" loading="lazy"/></p><h4>内存：平均 31.5MB，峰值 35MB</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579094" alt="image" title="image" loading="lazy"/></p><h4>网络：平均 1.07KB/s，峰值 1.10KB/s</h4><p>a. 压缩前：平均 12.99KB/s，峰值 13.13KB/s</p><p>b. 实际发送：平均 1.07KB/s，峰值 1.10KB/s</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579095" alt="image" title="image" loading="lazy"/></p><h4>磁盘：平均 6.07KB/s，峰值 13.03KB/s</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047579096" alt="image" title="image" loading="lazy"/></p><h2>总结与展望</h2><p>边缘场景的可观测数据采集，是一个长期被低估的技术挑战。网络的不稳定性、电源的不可靠性、数据一致性的复杂性，让传统的采集方案在边缘环境下频繁失效。LoongCollector 通过“数据持久化 + 异步发送 + 智能重试”的创新架构，系统性地解决了这些问题：</p><ul><li><p>保证了可观测数据可靠交付</p><ul><li>本地持久化保证断网不丢数据</li><li>异步发送机制实现采集与发送解耦</li><li>智能重试和反压确保网络恢复后数据完整上传</li></ul></li><li><p>有效地进行了流量控制</p><ul><li>高效压缩减少传输数据量</li><li>智能流量控制避免带宽占满，影响业务</li></ul></li></ul><p>但是，LoongCollector 的采集方案还有更多的优化空间：</p><ol><li>当前的持久化采集方案需要配置两个 Pipeline（采集 Pipeline + 文件读取 Pipeline），虽然灵活但增加了用户的理解和配置成本。LoongCollector 正在进行流水线优化，支持单流水线内部持久化能力，方便用户配置。</li><li>终端设备对于 STS 鉴权是强需求，LoongCollector 正在适配阿里云 STS 动态鉴权，支持临时凭证自动刷新，避免终端 AccessKey 泄露风险。</li><li>在流量成本敏感的场景，每一个百分点的压缩率提升都意味着显著的成本节省，LoongCollector 也正在探索更加极致的压缩策略，进一步降低网络流量。</li></ol>]]></description></item><item>    <title><![CDATA[2026年12款主流CRM系统深度测评与选型指南：从功能到场景的全维度推荐 爱听歌的金针菇 ]]></title>    <link>https://segmentfault.com/a/1190000047579109</link>    <guid>https://segmentfault.com/a/1190000047579109</guid>    <pubDate>2026-01-28 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化转型深化的 2026 年，CRM 系统已从单纯的 “客户信息管理工具” 进化为企业全业务链协同的 “增长引擎”。无论是工业制造的产销协同、零售行业的私域运营，还是中大型企业的多组织管理，选择适配自身场景的 CRM 成为破局关键。本文基于 12 款主流 CRM 系统的核心能力、行业适配性及用户价值，展开深度测评，为不同规模、不同行业的企业提供精准选型参考。</p><h2>一、测评框架：四大核心维度定义优质 CRM</h2><p>本次测评围绕<strong>功能完整性、场景适配性、技术先进性、成本性价比</strong>四大维度展开，每个维度下设 5 项细分指标，满分 10 分。其中：</p><ul><li><strong>功能完整性</strong>：覆盖获客、销售、服务、数据决策全流程，重点评估模块联动能力；</li><li><strong>场景适配性</strong>：针对行业特性（如工业生产、零售私域）、企业规模（中小 / 中大型）的定制化程度；</li><li><strong>技术先进性</strong>：AI 应用深度、生态集成能力（如企业微信 / 钉钉 / ERP 对接）、数据安全合规性；</li><li><strong>成本性价比</strong>：订阅费用、实施周期、维护成本，以及投入产出比（ROI）周期。</li></ul><h2>二、12 款主流 CRM 系统深度测评</h2><h3>（一）国产AI CRM标杆：珍客CRM（9.2 分）</h3><p><strong>核心定位</strong>：珍客CRM作为头部CRM产品之一，是AI原生驱动的全链路智能CRM，聚焦B2B及B2C大客户复杂销售与信创适配，以“AI+数据+场景”构建营销-销售-服务全链路闭环。</p><p><strong>核心优势</strong>：<br/>在AI原生全链路赋能，国产信创适配、可扩展性强、部署与成本灵活、全链路数据闭环、B2B大客户深耕与本土化服务七大维度。依托迈富时的AI-Agentforce企业级智能体中台，打造出覆盖研发、生产、供应链、营销、销售、服务、经营决策、组织人才赋能等全链路的智能化产品，提供全场景AI Agent解决方案。</p><p><strong>市场地位</strong>：<br/>迈富时Marketingforce在AI领域持续创新突破，连续7年AI SaaS影响力企业第一名，连续6年智能营销企业第一名，AI SaaS产品中国营销及销售领域营收规模第一，2025中国AI营销智能体第一。迈富时Marketingforce累计服务21万+企业，涵盖零售消费、汽车、金融、B2B制造、医药大健康、企服、跨境电商等行业领域，有丰富且成熟的行业经验。</p><p><strong>关键功能</strong>：</p><ol><li><strong>客户洞察与管理</strong>：360 度客户洞察，联系人图谱可视化，，帮助企业快速识别客户组织内的核心决策链，客户信用风险评估，保障企业财务健康，提升销售效率。</li><li><strong>销售全流程管控</strong>：整合全渠道线索，实现线索 “自动清洗、自动分发、自动回收、自动评分”，高效挖掘潜在客户，提升线索质量与转化效率。</li><li><strong>组织流程优化与办公便利</strong>：让流程适配业务需求，明确销售各阶段的任务与待执行动作，可配置审批流程，支持移动办公。</li><li><strong>数据决策</strong>：基于 BI 能力，将报表、驾驶舱与 CRM 结合，支持多样的报表可视化与钻取分析，让销售数据更易解读，为销售策略、市场战略制定提供数据决策支持。</li><li><strong>信息安全保障</strong>：对用户数据的存储与传输进行加密，针对有合规要求、无法使用公有云的企业，提供私有化部署方案，进一步保障数据安全。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 企业微信生态深度整合，私域运营工具齐全；</li><li>✅ 支持私有化部署，满足大型企业数据安全需求；</li></ul><p><strong>适配场景</strong>：零售、医美、教培等依赖私域运营的行业，中大型企业优先。</p><h3>（二）全球巨头：Salesforce（8.8 分）</h3><p><strong>核心定位</strong>：全球化CRM平台，覆盖销售、营销、服务全场景</p><p><strong>关键功能</strong>：</p><ol><li><strong>Einstein AI 赋能</strong>：智能线索评分、商机预测；</li><li><strong>全渠道整合</strong>：对接 LinkedIn、Google Ads 等海外平台，支持多语言（20+）、多币种（100+），适配跨境业务；</li><li><strong>生态扩展性</strong>：AppExchange 平台提供 5000 + 第三方应用，可自定义工作流（如合同审批流程）；</li><li><strong>企业级安全</strong>：SOC2、ISO 27001 认证，数据加密存储，满足跨国企业合规需求。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 功能全面，AI 应用深度领先；</li><li>✅ 全球化支持，适合跨国业务；</li><li>❌ 基础版年费超 10 万，中小企业性价比低；</li><li>❌ 本地化服务响应较慢。</li></ul><p><strong>适配场景</strong>：大型跨国企业、高科技行业，需全球化业务管理的团队。</p><h3>（三）微软生态王者：Microsoft Dynamics 365（8.9 分）</h3><p><strong>核心定位</strong>：ERP+CRM 一体化平台，微软生态无缝衔接</p><p><strong>关键功能</strong>：</p><ol><li><strong>全业务协同</strong>：销售订单自动同步至 ERP，触发库存扣减、财务应收，某制造企业产销协同效率提升 50%；</li><li><strong>Copilot AI 全场景应用</strong>：自动生成客户邮件、会议纪要，Power BI 实时数据看板，辅助管理层决策；</li><li><strong>生态整合</strong>：与 Office 365、Teams、Azure 深度对接，销售可在 Teams 内查看客户画像，无需切换系统；</li><li><strong>灵活部署</strong>：支持云端、本地混合部署，满足中大型企业定制化需求。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 微软生态用户学习成本低，协同效率高；</li><li>✅ 产销财一体化能力强，适合复杂业务流程；</li><li>❌ 中小微企业功能冗余，成本较高。</li></ul><p><strong>适配场景</strong>：已使用微软生态（Office/Teams）的中大型企业。</p><h3>（四）初创企业友好：HubSpot CRM（8.3 分）</h3><p><strong>核心定位</strong>：轻量化营销 + 销售闭环系统，免费版门槛低</p><p><strong>关键功能</strong>：</p><ol><li><strong>入站营销工具</strong>：SEO 优化、邮件营销自动化，某初创公司通过内容营销获客成本降低 40%；</li><li><strong>免费版够用</strong>：基础客户管理、线索跟踪功能永久免费，支持 10 人以内团队使用；</li><li><strong>AI 辅助</strong>：Breeze AI 自动生成营销文案、客户跟进提醒，节省 30% 人工时间；</li><li><strong>易用性强</strong>：界面简洁，无代码配置，新员工 1 小时即可上手。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 免费版功能满足初创需求，成本低；</li><li>✅ 营销获客工具成熟，适合内容驱动型企业；</li><li>❌ 无生产、财务模块，工业企业不适用；</li><li>❌ 高级功能需付费升级（如自定义报表）。</li></ul><p><strong>适配场景</strong>：初创企业、中小微零售 / 服务行业，营销驱动型团队。</p><h3>（五）本土化 SaaS 代表：钉钉 CRM（7.8 分）</h3><p><strong>核心定位</strong>：钉钉生态轻量化CRM</p><p><strong>关键功能</strong>：</p><ol><li><strong>钉钉原生集成</strong>：群聊线索自动抓取（如 “意向客户” 生成待办），销售可在钉钉内录入跟进记录；</li><li><strong>移动端优先</strong>：一键创建客户、查看订单，某餐饮连锁门店用后线上化率提升 60%；</li><li><strong>低成本</strong>：基础版免费，高级版年费不足 5000 元，中小团队负担轻；</li><li><strong>流程简化</strong>：支持 “线索→客户→订单” 标准流程，无需复杂配置。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 钉钉用户无缝衔接，学习成本低；</li><li>✅ 性价比高，适合小团队快速落地；</li><li>❌ 深度功能缺失（如复杂销售漏斗），中大型企业不适用。</li></ul><p><strong>适配场景</strong>：钉钉生态中小微企业，零售、餐饮等简单销售流程行业。</p><h3>（六）外贸企业首选：Zoho CRM（8.5 分）</h3><p><strong>核心定位</strong>：多语言多币种支持，跨境业务适配性强</p><p><strong>关键功能</strong>：</p><ol><li><strong>海外渠道对接</strong>：整合 Google Ads、LinkedIn 线索，自动同步至 CRM，某跨境电商获客效率提升 50%；</li><li><strong>本地化合规</strong>：符合欧盟 GDPR、中国 PIPL，数据存储在北京 / 上海双活中心；</li><li><strong>低代码定制</strong>：Zoho Creator 支持无代码扩展模块，外贸企业可自定义报关单据管理；</li><li><strong>性价比高</strong>：基础版年费约 1.2 万，支持 20 人团队使用。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 跨境业务功能完善，多语言支持到位；</li><li>✅ 中小外贸企业成本可控；</li><li>❌ 国内本地化服务响应较慢。</li></ul><p><strong>适配场景</strong>：跨境电商、外贸企业，中小规模优先。</p><h3>（七）低代码定制专家：白码 CRM（8.1 分）</h3><p><strong>核心定位</strong>：低代码平台构建的个性化CRM</p><p><strong>关键功能</strong>：</p><ol><li><strong>拖拽式定制</strong>：无需代码即可调整模块（如添加 “项目验收” 环节），某工程公司用后流程适配度提升 80%；</li><li><strong>数据联动</strong>：支持与 ERP、WMS 对接，自定义数据流转规则（如 “订单完成触发回款提醒”）；</li><li><strong>灵活扩展</strong>：可添加行业专属功能（如医疗行业的 “患者随访管理”）；</li><li><strong>成本透明</strong>：按功能模块订阅，避免冗余付费。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 业务适配性极强，适合特殊流程企业；</li><li>✅ 低代码降低二次开发成本；</li><li>❌ 需一定配置经验，初创团队上手慢。</li></ul><p><strong>适配场景</strong>：业务流程特殊的企业（如工程、医疗），中大型团队优先。</p><h3>（八）老牌国产厂商：金蝶 CRM（8.6 分）</h3><p><strong>核心定位</strong>：财务一体化CRM</p><p><strong>关键功能</strong>：</p><ol><li><strong>金蝶财务对接</strong>：订单自动生成应收、开票任务，某食品批发公司对账时间从 2 天缩短至 1 小时；</li><li><strong>商贸场景适配</strong>：支持 “批发套餐单”“租赁单”，满足商贸企业特殊订单需求；</li><li><strong>数据安全</strong>：与金蝶 ERP 共用数据底座，合规性强，适合财税敏感行业；</li><li><strong>易用性</strong>：界面简洁，财务人员上手快，减少跨部门沟通成本。</li></ol><p><strong>优缺点</strong>：</p><ul><li>✅ 财务一体化能力突出，商贸企业效率高；</li><li>✅ 金蝶生态用户数据流转顺畅；</li><li>❌ 无生产模块，工业制造企业不适用。</li></ul><p><strong>适配场景</strong>：商贸零售、批发行业，已使用金蝶财务系统的企业。</p><h3>（九）其他系统速览</h3><table><thead><tr><th>系统名称</th><th>核心优势</th><th>适配场景</th><th>评分</th></tr></thead><tbody><tr><td>八百客</td><td>自研PaaS平台，功能灵活</td><td>中小企业</td><td>8.0</td></tr><tr><td>销帮帮</td><td>钉钉生态深度适配，进销存模块完善</td><td>钉钉中小商贸企业</td><td>7.9</td></tr><tr><td>九氚汇</td><td>医疗行业定制化（患者管理、合规审计）</td><td>医疗健康行业</td><td>7.7</td></tr><tr><td>联蔚</td><td>电商全渠道对接（天猫 / 京东订单同步）</td><td>电商企业</td><td>7.5</td></tr><tr><td>飞鱼</td><td>广告线索智能分配（字节系平台对接）</td><td>互联网广告营销企业</td><td>7.6</td></tr></tbody></table><h2>三、选型指南：三步找到最适合你的 CRM</h2><h3>第一步：明确核心痛点，排除不匹配选项</h3><ul><li><strong>销售增长难</strong>：珍客CRM（AI原生）；</li><li><strong>产销脱节</strong>：优先选Microsoft Dynamics 365（产销财协同）；</li><li><strong>财务对账繁</strong>：金蝶CRM（财务一体化）、用友 CRM（ERP 联动）；</li><li><strong>跨境业务多</strong>：Zoho CRM（多语言多币种）、Salesforce（全球化）。</li></ul><h3>第二步：评估成本与规模，避免 “大材小用”</h3><ul><li><strong>初创 / 小微（10 人内）</strong> ：HubSpot 免费版、钉钉 CRM 基础版，成本控制在 1 万以内；</li><li><strong>中小（10-50 人）</strong> ：Zoho CRM（外贸）、销帮帮（钉钉生态），年费 1-5 万；</li><li><strong>中大型（50 人以上）</strong> ：珍客CRM、Microsoft Dynamics 365、Salesforce、金蝶CRM，预算5-20 万。</li></ul><h3>第三步：试用验证，聚焦核心功能</h3><ul><li>工业类（珍客CRM/ Dynamics）：测试 “订单→生产联动”“财务对账效率”；</li><li>外贸类（Zoho）：测试 “多币种结算”“海外渠道对接”；</li><li>试用周期建议 3-7 天，确保核心痛点可解决。</li></ul><h2>四、结语：CRM选型核心原则</h2><p>选择CRM的本质是 “匹配业务场景”，而非追求 “功能最全”。工业企业需优先看 “产销协同”，零售企业聚焦 “私域运营”，跨国团队关注 “全球化支持”。建议企业结合自身行业特性、现有生态（如钉钉/微软）及预算，通过 “痛点匹配→成本评估→试用验证” 三步法，找到真正能驱动增长的 “数字化伙伴”。</p>]]></description></item><item>    <title><![CDATA[鸿蒙 User Authentication Kit 实战：打造 “学海” 坚固认证防线 灵芸小骏 ]]></title>    <link>https://segmentfault.com/a/1190000047578445</link>    <guid>https://segmentfault.com/a/1190000047578445</guid>    <pubDate>2026-01-28 18:10:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>HarmonyOS 中 User Authentication Kit 开发实战与应用</h2><h3>引言</h3><p>在当今数字化时代，应用安全至关重要，而用户身份认证是保障应用安全的第一道防线。HarmonyOS 的 User Authentication Kit 为开发者提供了全面且强大的用户认证解决方案，助力开发者轻松构建安全可靠的认证系统。本文将结合实际案例，深入探讨 User Authentication Kit 在不同场景下的开发应用。</p><h3>User Authentication Kit 核心特性</h3><ul><li><strong>多认证方式支持</strong>：涵盖锁屏口令、人脸、指纹等多种认证方式，同时支持组合认证，如人脸与锁屏口令相结合，满足不同安全等级的需求。</li><li><strong>归一化认证接口</strong>：通过统一的接口，开发者可以便捷地调用不同的认证方式，无需为每种认证方式编写复杂的差异化代码，大大简化了开发流程。</li><li><strong>感知认证可信等级</strong>：开发者能够根据应用场景的风险程度，指定期望的认证可信等级，确保高风险操作得到足够强度的认证保护，例如在涉及资金交易的场景中要求更高的认证可信等级。</li><li><strong>认证结果复用</strong>：允许在短时间内（最长 5 分钟）复用其他应用的认证结果，避免用户在多个应用间频繁重复认证，显著提升用户体验。</li></ul><h3>案例场景：“学海在线教育平台”认证系统开发</h3><p>“学海在线教育平台”服务于广大学生群体，包括小学生、初中生和高中生，同时涉及家长与教师用户。由于不同用户群体的使用场景和安全需求各异，因此需要构建一个多层次、全方位的用户认证系统。</p><h4>需求设计</h4><ol><li><p><strong>学生日常登录</strong></p><ul><li><strong>场景描述</strong>：学生日常登录平台进行课程学习、作业练习等操作。考虑到学生使用设备的便捷性和效率，需要一种快速且便捷的认证方式。</li><li><strong>需求分析</strong>：对于低年级学生，可能对复杂密码记忆存在困难，而人脸识别具有直观、快速的特点，适合作为主要认证方式。同时，为防止他人冒用学生身份，还需结合一定的安全机制。</li><li><strong>设计方案</strong>：采用人脸识别作为主要登录方式。在人脸识别过程中，加入活体检测功能，要求学生按照提示做出简单动作，如眨眼、摇头等，确保是本人操作。对于首次登录的学生，引导其进行人脸录入，并设置备用的锁屏口令，以防人脸识别出现异常情况时可通过口令登录。</li></ul></li><li><p><strong>在线考试场景</strong></p><ul><li><strong>场景描述</strong>：在线考试要求严格保证考生身份的真实性，防止作弊行为，确保考试公平公正。</li><li><strong>需求分析</strong>：单一的认证方式难以满足考试场景的高安全性需求，需要采用多种认证方式相结合，增加作弊难度。</li><li><strong>设计方案</strong>：采用人脸 + 指纹双重验证方式。在考试开始前，考生需先进行人脸识别，验证身份后，再通过指纹认证进一步确认身份。同时，在考试过程中，利用设备的摄像头和麦克风进行实时监控，若检测到异常行为，如画面中出现多人、声音异常等，及时发出预警并记录相关信息。</li></ul></li><li><p><strong>家长敏感操作认证</strong></p><ul><li><strong>场景描述</strong>：家长在平台上进行如缴费、修改学生重要信息等敏感操作时，需要高度的安全性，以保护账户资金和学生信息的安全。</li><li><strong>需求分析</strong>：此类操作涉及较高风险，需要采用更为严格的认证方式，确保操作是由家长本人发起。</li><li><strong>设计方案</strong>：除了常规的人脸识别或指纹认证外，增加短信验证码验证环节。当家长发起敏感操作时，系统向家长预留的手机号码发送验证码，家长需在规定时间内输入正确的验证码，方可完成操作。此外，对于家长账户登录，可设置登录设备管理功能，家长可查看最近登录设备信息，并对异常设备登录进行冻结或修改密码等操作。</li></ul></li><li><p><strong>教师管理操作认证</strong></p><ul><li><strong>场景描述</strong>：教师在平台上进行成绩录入、班级管理等操作时，同样需要确保操作的安全性和教师身份的真实性。</li><li><strong>需求分析</strong>：教师操作涉及众多学生的学习数据，需保证数据的准确性和安全性，防止数据泄露或被篡改。</li><li><strong>设计方案</strong>：采用指纹认证结合数字证书的方式。教师在首次登录平台时，需下载并安装个人数字证书到设备中。之后每次进行管理操作时，先通过指纹认证确认身份，再使用数字证书对操作进行签名，确保操作的不可抵赖性和数据的完整性。</li></ul></li></ol><h4>关键代码实现</h4><ol><li><p><strong>检查设备支持的认证类型</strong>：</p><pre><code class="typescript">import { userAuth } from '@ohos.userIAM.userAuth';
let auth = new userAuth.UserAuth();
let authTypes = auth.getAvailableAuthType(userAuth.AuthLevel.STRONG);
console.log(`支持认证类型: ${authTypes}`);</code></pre></li><li><p><strong>学生人脸识别登录</strong>：</p><pre><code class="typescript">async function studentFaceLogin(): Promise&lt;boolean&gt; {
 let challenge = generateRandomChallenge();
 let authParams = {
     challenge: challenge,
     authType: userAuth.AuthType.FACE,
     authTrustLevel: userAuth.AuthTrustLevel.ATL3,
     extraInfo: { requireLiveness: true }
 };
 try {
     let result = await auth.auth(authParams);
     return result.result === userAuth.AuthResult.SUCCESS;
 } catch (err) {
     console.error(`认证失败: ${err.code}, ${err.message}`);
     return false;
 }
}</code></pre></li><li><p><strong>在线考试双重认证</strong>：</p><pre><code class="typescript">async function examAuth(): Promise&lt;boolean&gt; {
 let authParams = ([{ authType: userAuth.AuthType.FACE, authTrustLevel: userAuth.AuthTrustLevel.ATL4 }, { authType: userAuth.AuthType.FINGERPRINT, authTrustLevel: userAuth.AuthTrustLevel.ATL3 }]);
 let controller = new userAuth.AuthController();
 return controller.execute(authParams)
   .then(result =&gt; {
         return result.allSucceeded;
     });
}</code></pre></li><li><p><strong>家长敏感操作认证（包含短信验证码验证）</strong>：</p><pre><code class="typescript">async function parentSensitiveOperationAuth(): Promise&lt;boolean&gt; {
 let faceResult = await faceAuth();
 if (!faceResult) {
     return false;
 }
 let smsCode = await sendAndGetSmsCode();
 let verifyResult = await verifySmsCode(smsCode);
 return verifyResult;
}
async function faceAuth(): Promise&lt;boolean&gt; {
 let challenge = generateRandomChallenge();
 let authParams = {
     challenge: challenge,
     authType: userAuth.AuthType.FACE,
     authTrustLevel: userAuth.AuthTrustLevel.ATL4
 };
 try {
     let result = await auth.auth(authParams);
     return result.result === userAuth.AuthResult.SUCCESS;
 } catch (err) {
     console.error(`人脸认证失败: ${err.code}, ${err.message}`);
     return false;
 }
}
async function sendAndGetSmsCode(): Promise&lt;string&gt; {
 // 调用短信发送接口并等待用户输入验证码
 // 此处省略实际短信发送和获取用户输入的逻辑
 return "123456";
}
async function verifySmsCode(code: string): Promise&lt;boolean&gt; {
 // 调用接口验证短信验证码
 // 此处省略实际验证逻辑
 return code === "123456";
}</code></pre></li><li><p><strong>教师指纹与数字证书认证</strong>：</p><pre><code class="typescript">async function teacherAuth(): Promise&lt;boolean&gt; {
 let fingerprintResult = await fingerprintAuth();
 if (!fingerprintResult) {
     return false;
 }
 let certResult = await verifyDigitalCertificate();
 return certResult;
}
async function fingerprintAuth(): Promise&lt;boolean&gt; {
 let challenge = generateRandomChallenge();
 let authParams = {
     challenge: challenge,
     authType: userAuth.AuthType.FINGERPRINT,
     authTrustLevel: userAuth.AuthTrustLevel.ATL4
 };
 try {
     let result = await auth.auth(authParams);
     return result.result === userAuth.AuthResult.SUCCESS;
 } catch (err) {
     console.error(`指纹认证失败: ${err.code}, ${err.message}`);
     return false;
 }
}
async function verifyDigitalCertificate(): Promise&lt;boolean&gt; {
 // 调用数字证书验证接口
 // 此处省略实际验证逻辑
 return true;
}</code></pre></li></ol><h4>安全性能与用户反馈</h4><p>经过实际测试，“学海在线教育平台”的认证系统在安全性能方面表现出色。人脸识别误识率为 1/50 万，通过率 98.7%，平均耗时 800ms；指纹识别误识率 1/10 万，通过率 99.2%，平均耗时 500ms；双重认证误识率低至 1/1 亿，通过率 97.5%，平均耗时 1.2s。短信验证码验证和数字证书验证的成功率均达到 99%以上。</p><p>用户反馈积极，学生表示人脸识别登录方便快捷，提高了学习效率；家长对敏感操作的多重认证方式表示放心，认为有效保护了账户安全；教师对指纹与数字证书结合的认证方式给予肯定，认为确保了教学管理操作的安全性和数据的可靠性。</p><h3>总结</h3><p>HarmonyOS 的 User Authentication Kit 为开发者提供了丰富的功能和灵活的开发接口，能够满足不同应用场景下的复杂认证需求。通过“学海在线教育平台”的案例，我们详细展示了如何根据不同用户群体和使用场景，设计并实现多层次、全方位的认证系统。开发者在实际项目中，应充分结合业务需求，合理运用 User Authentication Kit 的各项特性，为用户打造安全、便捷的应用体验。</p>]]></description></item><item>    <title><![CDATA[保姆级 SeaTunnel 入门！再学不会小编当场表演倒立敲代码 SeaTunnel ]]></title>    <link>https://segmentfault.com/a/1190000047578479</link>    <guid>https://segmentfault.com/a/1190000047578479</guid>    <pubDate>2026-01-28 18:09:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578481" alt="SeaTunnel 新手" title="SeaTunnel 新手"/></p><p>欢迎来到 Apache SeaTunnel 的世界！这份文档旨在帮助新手快速了解 SeaTunnel 的核心功能、基本架构，并完成第一个数据同步任务。</p><h2>1. 什么是 Apache SeaTunnel？</h2><p>Apache SeaTunnel 是一个非常易于使用、高性能、支持实时流式和离线批处理的海量数据集成平台。它的目标是解决常见的数据集成问题，如数据源多样性、同步场景复杂性以及资源消耗高的问题。</p><h3>核心特性</h3><ul><li><strong>丰富的数据源支持</strong>：支持 100+ 种 Connector，涵盖主流数据库、云存储、SaaS 服务等。</li><li><strong>批流一体</strong>：同一套 Connector 代码同时支持批处理（离线）和流处理（实时）。</li><li><strong>高性能</strong>：支持多引擎（Zeta, Flink, Spark），提供高吞吐、低延迟的数据同步能力。</li><li><strong>简单易用</strong>：通过简单的配置文件（Config）即可定义复杂的数据同步任务。</li></ul><h2>2. 架构与环境</h2><h3>2.1 架构图</h3><p>SeaTunnel 采用了解耦的设计架构，Source、Transform、Sink 插件与具体的执行引擎（Engine）是分离的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578482" alt="ST architecture" title="ST architecture" loading="lazy"/></p><h3>2.2 操作系统支持</h3><p>SeaTunnel 基于 Java 开发，理论上支持所有安装了 JDK 的操作系统。</p><table><thead><tr><th align="left">操作系统</th><th align="left">适用场景</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><strong>Linux</strong> (CentOS, Ubuntu, etc.)</td><td align="left"><strong>生产环境</strong> (推荐)</td><td align="left">稳定性高，适合长期运行服务。</td></tr><tr><td align="left"><strong>macOS</strong></td><td align="left">开发/测试</td><td align="left">适合开发者本地调试和编写 Config。</td></tr></tbody></table><h3>2.3 环境准备</h3><p>在开始安装 SeaTunnel 之前，请确保你的环境满足以下要求：</p><ul><li><p><strong>JDK 版本</strong>：必须安装 <strong>Java 8</strong> 或 <strong>Java 11</strong>。</p><ul><li>可以通过命令 <code>java -version</code> 检查。</li><li>确保设置了 <code>JAVA_HOME</code> 环境变量。</li></ul></li></ul><h2>3. 核心组件深度解析</h2><p>在使用 SeaTunnel 之前，深入理解其核心组件的内部机制有助于你更好地调优和排查问题。</p><h3>3.1 Source (数据源)</h3><p>Source 负责从外部系统读取数据，并将其转换为 SeaTunnel 内部的行格式（SeaTunnelRow）。</p><ul><li><strong>Enumerator (枚举器)</strong>：运行在 Master 节点（Coordinator）。负责发现数据分片（Splits）。例如，在 JDBC Source 中，Enumerator 会根据 <code>partition_column</code> 的最大值和最小值计算出多个查询范围（Splits）。</li><li><strong>Reader (读取器)</strong>：运行在 Worker 节点。负责接收 Enumerator 分配的 Splits，并真正执行读取操作。多个 Reader 并行工作，极大提高了读取效率。</li><li><strong>Checkpoint 支持</strong>：对于流式作业，Source 还需要支持状态保存（如 Kafka 的 Offset），以便在故障恢复时实现断点续传。</li></ul><h3>3.2 Transform (数据转换)</h3><p>Transform 负责在数据从 Source 流向 Sink 的过程中对数据进行处理。</p><ul><li><strong>无状态转换</strong>：大多数 Transform（如 <code>Sql</code>, <code>Filter</code>, <code>Replace</code>）是无状态的，即处理当前行不需要依赖其他行的数据。</li><li><strong>Schema 变更</strong>：Transform 可以改变数据的 Schema（增加、删除、修改字段），下游 Sink 会感知到这种变化。</li></ul><h3>3.3 Sink (数据目标)</h3><p>Sink 负责将 SeaTunnel 处理后的数据写入到外部系统。</p><ul><li><strong>Writer (写入器)</strong>：运行在 Worker 节点。负责将数据写入目标系统。通常支持批量写入以提高吞吐量。</li><li><strong>Committer (提交器)</strong>：运行在 Master 节点（可选）。对于支持事务的 Sink（如文件系统、Iceberg），需要一个全局的 Committer 来在 Checkpoint 完成时统一提交事务（二阶段提交），从而实现 <strong>Exactly-Once</strong>（精确一次）语义。</li></ul><h3>3.4 执行流程</h3><ol><li><strong>解析配置</strong>：SeaTunnel 解析配置文件，构建逻辑执行计划。</li><li><strong>资源分配</strong>：Master 节点根据并行度申请资源。</li><li><strong>任务分发</strong>：Enumerator 生成分片，分发给 Reader。</li><li><strong>数据流转</strong>：<code>Reader -&gt; Transform -&gt; Writer</code>。</li><li><strong>状态提交</strong>：周期性触发 Checkpoint，保存状态并提交事务。</li></ol><h2>4. 支持的 Connector 及其优缺点分析</h2><p>SeaTunnel 支持超过 100 种 Connector，以下是几类最常用的 Connector 及其特性分析：</p><h3>4.1 关系型数据库 (JDBC)</h3><p><strong>支持列表</strong>: MySQL, PostgreSQL, Oracle, SQLServer, DB2, Teradata, Dameng(达梦), OceanBase, TiDB 等。</p><ul><li><p><strong>优点</strong>：</p><ul><li><strong>通用性强</strong>：只要有 JDBC 驱动即可连接几乎所有 SQL 数据库。</li><li><strong>功能完善</strong>：支持列投影（只读部分列）、并行读取（基于 <code>partition_column</code> 切分）、Exactly-Once（取决于实现）。</li><li><strong>自动建表</strong>：部分 Connector 支持在目标端自动创建表结构。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>性能瓶颈</strong>：受限于 JDBC 协议和单机驱动性能，超大规模数据读取可能需要精细调优（如 <code>fetch_size</code>）。</li><li><strong>源库压力</strong>：如果并行度设置过高，可能打满源库连接池或 CPU。</li></ul></li></ul><h3>4.2 消息队列</h3><p><strong>支持列表</strong>: Kafka, Pulsar, RocketMQ, Amazon DynamoDB Streams 等。</p><ul><li><p><strong>优点</strong>：</p><ul><li><strong>高吞吐</strong>：天生适合大规模流数据处理，支持削峰填谷。</li><li><strong>格式丰富</strong>：支持 JSON, Avro, Protobuf, Canal-JSON, Debezium-JSON 等多种序列化格式。</li><li><strong>Exactly-Once</strong>：支持端到端的精确一次语义（依赖 Checkpoint 机制）。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>配置复杂</strong>：涉及 Offset 管理、序列化 Schema 配置、Consumer Group 管理等。</li><li><strong>数据可见性</strong>：相比数据库，数据在 Topic 中不够直观，调试稍显麻烦。</li></ul></li></ul><h3>4.3 变更数据捕获 (CDC)</h3><p><strong>支持列表</strong>: MySQL-CDC, PostgreSQL-CDC, Oracle-CDC, MongoDB-CDC, SQLServer-CDC, TiDB-CDC 等。</p><ul><li><p><strong>优点</strong>：</p><ul><li><strong>实时性</strong>：毫秒级捕获数据库增删改操作。</li><li><strong>无锁读取</strong>：SeaTunnel 的 CDC 实现了无锁并行快照算法，极大降低了对源库的影响。</li><li><strong>断点续传</strong>：支持从 Binlog/WAL 指定位置恢复。</li><li><strong>Schema Evolution</strong>：支持表结构变更同步（部分支持）。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>权限要求</strong>：通常需要较高的数据库权限（如 REPLICATION SLAVE）。</li><li><strong>依赖日志</strong>：源库必须开启 Binlog（或 WAL），且保留时间需足够长。</li></ul></li></ul><h3>4.4 文件系统 &amp; 云存储</h3><p><strong>支持列表</strong>: LocalFile, HDFS, S3, OSS, GCS, FTP, SFTP 等。</p><ul><li><p><strong>优点</strong>：</p><ul><li><strong>海量存储</strong>：适合数据湖场景，成本低廉。</li><li><strong>格式支持</strong>：原生支持 Parquet, ORC, Avro, JSON, CSV, Excel, Text 等。</li><li><strong>压缩支持</strong>：支持 Snappy, Gzip, Lzo 等多种压缩算法。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li><strong>小文件问题</strong>：流式写入时，如果 Checkpoint 间隔太短，容易产生大量小文件（SeaTunnel 有文件合并功能但会增加复杂度）。</li></ul></li></ul><h3>4.5 NoSQL &amp; 其他</h3><p><strong>支持列表</strong>: Elasticsearch, Redis, MongoDB, Cassandra, HBase, InfluxDB, ClickHouse, Doris, StarRocks 等。</p><ul><li><strong>特点</strong>：针对各数据库特性进行了优化，例如 ClickHouse/StarRocks 支持 Stream Load 高速导入，Elasticsearch 支持批量写入。</li></ul><h2>5. Transform 实战演练 (附带详细注释)</h2><p>Transform 插件用于在 Source 和 Sink 之间处理数据。以下是几个常用 Transform 的配置示例。</p><h3>5.1 Sql Transform (最推荐)</h3><p>使用 SQL 语法对数据进行处理，支持重命名、计算、常量添加、过滤等。</p><pre><code class="hocon">transform {
  Sql {
    # 输入表名，必须与 Source 的 result_table_name 一致
    plugin_input = "fake"
    # 输出表名，供后续 Transform 或 Sink 使用
    plugin_output = "fake_sql"
    
    # SQL 查询语句
    # 1. name as full_name: 字段重命名
    # 2. age + 1: 数值计算
    # 3. 'US' as country: 增加常量列
    # 4. where age &gt; 10: 数据过滤
    query = "select name as full_name, age + 1 as next_year_age, 'US' as country from fake where age &gt; 10"
  }
}</code></pre><h3>5.2 Filter Transform</h3><p>用于删除或保留指定字段（注意：不是过滤行，是过滤列/字段）。</p><pre><code class="hocon">transform {
  Filter {
    plugin_input = "fake"
    plugin_output = "fake_filter"
    
    # 仅保留 name 和 age 字段，其他字段会被丢弃
    include_fields = ["name", "age"]
    
    # 或者使用 exclude_fields 删除指定字段
    # exclude_fields = ["card"]
  }
}</code></pre><h3>5.3 Replace Transform</h3><p>用于字符串替换，支持正则表达式。</p><pre><code class="hocon">transform {
  Replace {
    plugin_input = "fake"
    plugin_output = "fake_replace"
    
    # 需要替换的字段名
    replace_field = "name"
    # 匹配模式（旧字符串）
    pattern = " "
    # 替换后的字符串（新字符串）
    replacement = "_"
    # 是否使用正则表达式，这里设为 true，表示 pattern 是一个正则
    is_regex = true
    # 是否只替换第一个匹配项
    replace_first = true
  }
}</code></pre><h3>5.4 Split Transform</h3><p>将一个字符串字段拆分为多个字段。</p><pre><code class="hocon">transform {
  Split {
    plugin_input = "fake"
    plugin_output = "fake_split"
    
    # 分隔符，这里使用空格
    separator = " "
    # 需要拆分的源字段
    split_field = "name"
    # 拆分后生成的新字段名列表
    output_fields = ["first_name", "last_name"]
  }
}</code></pre><h2>6. 快速安装</h2><p>对于新手，推荐直接下载编译好的二进制发行包进行体验。</p><h3>步骤 1: 下载</h3><p>前往 <a href="https://link.segmentfault.com/?enc=8oZdVmMIcmyfUpdkVvWYXQ%3D%3D.k7UhkuDL%2BPNtcNPU2%2ByZz2AGB20LUOWHW4AiSZdjD0Ly4XRjANeos3OGMG6D8uzI" rel="nofollow" target="_blank">SeaTunnel 下载页面</a> 下载最新版本的二进制包（例如 <code>apache-seatunnel-2.3.x-bin.tar.gz</code>）。</p><h3>步骤 2: 解压</h3><pre><code class="bash">tar -xzvf apache-seatunnel-2.3.x-bin.tar.gz
cd apache-seatunnel-2.3.x</code></pre><h3>步骤 3: 安装 Connector 插件</h3><p>SeaTunnel 的 Connector 是插件化的。首次使用需要下载插件：</p><pre><code class="bash">sh bin/install-plugin.sh</code></pre><p><em>注意：该命令会根据 <code>config/plugin_config</code> 文件中的配置，从 Maven 中央仓库下载常用插件（如 <code>connector-fake</code>, <code>connector-console</code> 等）。如果下载速度慢，请耐心等待或配置 Maven 镜像。</em></p><h4>💡 技巧：配置 Maven 国内镜像加速下载</h4><p>如果遇到下载速度极慢或超时失败的情况，建议配置 Maven 阿里云镜像。</p><ol><li>找到或创建 Maven 配置文件：<code>~/.m2/settings.xml</code> (Windows 下为 <code>C:\Users\你的用户名\.m2\settings.xml</code>)。</li><li>添加如下镜像配置：</li></ol><pre><code class="xml">&lt;settings&gt;
  &lt;mirrors&gt;
    &lt;mirror&gt;
      &lt;id&gt;aliyunmaven&lt;/id&gt;
      &lt;mirrorOf&gt;*&lt;/mirrorOf&gt;
      &lt;name&gt;阿里云公共仓库&lt;/name&gt;
      &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;
    &lt;/mirror&gt;
  &lt;/mirrors&gt;
&lt;/settings&gt;</code></pre><p>保存后再次运行 <code>sh bin/install-plugin.sh</code> 即可享受高速下载。</p><h2>7. 实战：第一个 SeaTunnel 任务</h2><p>我们将创建一个简单的任务：生成一些随机数据（FakeSource），并将其打印到控制台（Console Sink）。</p><h3>步骤 1: 创建配置文件</h3><p>在 <code>config</code> 目录下创建一个名为 <code>hello_world.conf</code> 的文件，内容如下：</p><pre><code class="hocon">env {
  # 并行度设置：决定了有多少个线程同时执行任务。
  # 设置为 1 表示单线程执行，适合测试；生产环境可根据资源调大。
  parallelism = 1
  # 作业模式：
  # BATCH (批处理)：一次性处理完数据后结束（如离线同步）。
  # STREAMING (流处理)：持续运行，实时处理数据（如实时同步）。
  job.mode = "BATCH"
}

source {
  # FakeSource 是一个虚拟数据源，用于生成测试数据
  FakeSource {
    # result_table_name: 将此数据源产生的数据注册为一个临时表，表名为 "fake"
    # 后续的 Transform 或 Sink 可以通过这个名字引用这份数据
    result_table_name = "fake"
    
    # row.num: 指定生成数据的总行数，这里生成 16 行数据
    row.num = 16
    
    # schema: 定义数据的结构（字段名和类型）
    schema = {
      fields {
        name = "string" # 定义一个名为 name 的字符串字段
        age = "int"     # 定义一个名为 age 的整型字段
      }
    }
  }
}

transform {
  # Sql Transform: 使用 SQL 语句对数据进行处理
  Sql {
    # plugin_input: 指定输入数据来源，这里引用了 Source 中定义的 "fake" 表
    plugin_input = "fake"
    
    # plugin_output: 指定处理后的结果表名，命名为 "fake_transformed"
    # 下游的 Sink 将使用这个名字来获取处理后的数据
    plugin_output = "fake_transformed"
    
    # query: 执行的 SQL 查询语句
    # 这里演示了选择 name 和 age 字段，并新增一个常量字段 new_field
    query = "select name, age, 'new_field_val' as new_field from fake"
  }
}

sink {
  # Console Sink: 将数据输出打印到控制台（标准输出）
  Console {
    # plugin_input: 指定要输出的数据来源，这里引用了 Transform 输出的 "fake_transformed" 表
    plugin_input = "fake_transformed"
  }
}</code></pre><h3>步骤 2: 运行任务</h3><p>使用 SeaTunnel 自带的 Zeta 引擎运行该任务。</p><p><strong>执行命令：</strong></p><pre><code class="bash">./bin/seatunnel.sh --config ./config/hello_world.conf -e local</code></pre><p><strong>命令详解：</strong></p><ul><li><code>./bin/seatunnel.sh</code>: 启动脚本，默认使用 Zeta 引擎。</li><li><code>--config</code> (或 <code>-c</code>): 指定配置文件的路径。这里我们指定了刚才创建的 <code>hello_world.conf</code>。</li><li><p><code>-e local</code> (或 <code>-m local</code>): 指定执行模式。</p><ul><li><code>local</code>: 本地模式。SeaTunnel 会在当前进程中启动一个轻量级的 Zeta 引擎集群来运行任务，任务结束后集群关闭。<strong>适合开发和测试</strong>。</li><li><code>cluster</code>: 集群模式。任务会提交到已经运行的 SeaTunnel 集群中执行。<strong>适合生产环境</strong>。</li></ul></li></ul><h3>步骤 3: 查看结果与日志分析</h3><p>任务启动后，终端会输出大量日志。我们需要关注以下关键信息：</p><ol><li><strong>任务提交成功</strong>：<br/> 看到 <code>Job execution started</code> 表示配置文件解析通过，任务已提交给引擎。</li><li><p><strong>数据处理过程</strong>：<br/> 由于我们使用的是 <code>Console</code> Sink，数据会直接打印在日志中。你应能看到类似如下的输出：</p><pre><code class="text">...
INFO  ...ConsoleSinkWriter - subtaskIndex=0 rowIndex=1: SeaTunnelRow#tableId=-1 SeaTunnelRow#kind=INSERT: CpiOd, 12345, new_field_val
INFO  ...ConsoleSinkWriter - subtaskIndex=0 rowIndex=2: SeaTunnelRow#tableId=-1 SeaTunnelRow#kind=INSERT: eQqTs, 67890, new_field_val
...</code></pre><ul><li><code>subtaskIndex</code>: 并行任务的编号。</li><li><code>rowIndex</code>: 当前处理的行号。</li><li><code>SeaTunnelRow</code>: 打印出的具体数据内容。</li></ul></li><li><strong>任务结束</strong>：<br/> 最后看到 <code>Job Execution Status: FINISHED</code> 表示任务执行成功结束。</li></ol><h3>8. 常见问题排查 (Troubleshooting)</h3><p>如果在运行过程中遇到报错，请参考以下常见问题进行排查：</p><h4>🔴 问题 1: <code>command not found: java</code> 或 <code>JAVA_HOME is not set</code></h4><ul><li><strong>现象</strong>：运行脚本时直接报错，提示找不到 Java。</li><li><strong>原因</strong>：环境未安装 Java 或未配置环境变量。</li><li><p><strong>解决</strong>：</p><ol><li>运行 <code>java -version</code> 确认 Java 8 或 11 已安装。</li><li>设置环境变量：<code>export JAVA_HOME=/path/to/your/java</code> (建议写入 <code>~/.bashrc</code> 或 <code>~/.zshrc</code>)。</li></ol></li></ul><h4>🔴 问题 2: <code>Exception in thread "main" ... ClassNotFoundException</code></h4><ul><li><strong>现象</strong>：报错提示找不到某个类，例如 <code>ClassNotFoundException: org.apache.seatunnel.connectors.seatunnel.fake.source.FakeSourceFactory</code>。</li><li><strong>原因</strong>：<strong>Connector 插件未安装</strong>。默认包中只有引擎核心，没有包含具体的数据源插件。</li><li><p><strong>解决</strong>：</p><ul><li>确保你执行过 <code>sh bin/install-plugin.sh</code>。</li><li>检查 <code>connectors/seatunnel</code> 目录下是否有对应的 jar 包（例如 <code>connector-fake-*.jar</code>）。</li></ul></li></ul><h4>🔴 问题 3: <code>Config file not valid</code> 或 <code>HOCONSyntaxError</code></h4><ul><li><strong>现象</strong>：提示配置文件格式错误。</li><li><strong>原因</strong>：<code>hello_world.conf</code> 中的括号 <code>{}</code> 不匹配，或者关键字拼写错误。</li><li><strong>解决</strong>：仔细检查配置文件语法。SeaTunnel 使用 HOCON 格式，确保每一层级的 <code>{</code> 和 <code>}</code> 都是成对出现的。</li></ul><h4>🔴 问题 4: 任务卡住不动</h4><ul><li><strong>现象</strong>：日志停止更新，但任务没有结束。</li><li><strong>原因</strong>：可能是资源不足（CPU/内存），或者在流模式（STREAMING）下这是正常现象（流任务是无休止运行的）。</li><li><p><strong>解决</strong>：</p><ul><li>如果是 BATCH 模式卡住，检查 <code>plugin_config</code> 里的内存设置。</li><li>检查是否在 <code>env</code> 中错误地设置了 <code>job.mode = "STREAMING"</code>。</li></ul></li></ul><h2>9. 进阶学习资源</h2><ul><li><strong>官方文档</strong>: <a href="https://link.segmentfault.com/?enc=Gc24Z217386808w0FD4SXw%3D%3D.6BO2sNU1LTlpdnY7r%2FHCx1M8bSx7GXE31TblOq5AoB%2BARFJJfFWQfgn0BsLaEsS9" rel="nofollow" target="_blank">https://seatunnel.apache.org/docs/</a></li><li><strong>Connector 列表</strong>: 查看 <code>docs/en/connector-v2</code> 目录，了解所有支持的数据源。</li><li><strong>示例代码</strong>: 在 <code>config</code> 目录下通常会有一些模板文件（如 <code>v2.batch.config.template</code>），可以作为参考。</li></ul><p>Apache SeaTunnel 批流一体、生态丰富、部署轻便，入门有指南，实战有案例。即刻上手探索，加入开源社区，让数据流转更简单，为数据工程高效赋能！祝你学习愉快！</p>]]></description></item><item>    <title><![CDATA[数据跨境、隐私泄露、审计溯源——出海企业三大安全必答题 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578484</link>    <guid>https://segmentfault.com/a/1190000047578484</guid>    <pubDate>2026-01-28 18:09:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：徐可甲（烨陌）</p><h2>引言：企业出海，安全合规不再是选择题，而是必答题</h2><p>近年来，出海已成为越来越多中国企业的选择，出海业务的发展模式也从早期“先上线再整改”的粗放经营，转向“合规前置、本地嵌入、持续迭代”的成熟发展，积极探索从“产品输出”到“技术+品牌+本地化”的深度全球化。但随着欧盟《数字服务法》（DSA）、美国《数据隐私框架》、东南亚各国数据本地化立法加速，“合规先行”已成为企业能否在海外市场长期立足的关键。</p><p>越来越多的中企出海案例为创业者提供了清晰的参照：凭借国内成熟的产品化能力和完善的供应链体系，出海拓展全球市场正成为 AI 时代的重要机遇。但成功的出海企业不再仅靠成本优势，而是通过本地化合规架构、税务风控体系、ESG 治理、数据主权管理等多维举措，才能实现“走得出去、留得下来、做得长久”。机遇背后是不可忽视的合规挑战——数据跨境、多地监管、隐私保护、存储架构等问题，必须在业务扩张之前就完成系统性规划。</p><p>本文面向安全合规领域的开发者，梳理 AI 出海面临的核心合规挑战，并介绍阿里云日志服务（SLS）如何提供全链路的技术支撑。</p><h2>出海合规：三道必须跨越的门槛</h2><h3>数据架构的隐患：“三明治模式”</h3><p>当前许多出海企业的数据架构呈现典型的“三明治”形态：</p><ul><li><strong>顶层：</strong> 海外用户产生数据，海外资本注入资金</li><li><strong>中层：</strong> 核心研发与运营团队驻扎国内</li><li><strong>底层：</strong> 调用 OpenAI、Anthropic、Google 等海外模型服务</li></ul><p>这种架构导致数据流转路径异常复杂：用户数据从海外传至国内处理，再转发至美国等地的模型服务商进行推理，最后返回用户。数据在多个司法管辖区之间反复穿梭，<strong>同时触发多地的数据主权审查</strong>。</p><p>全球主要经济体在数据立法时都遵循一个基本原则：<strong>本地产生的数据，主权归属本地</strong>。“三明治”架构恰恰与这一原则相悖，使企业暴露在多重合规风险之下。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578486" alt="image" title="image"/></p><h3>三大市场的监管差异</h3><p>出海企业通常需要同时应对中国、美国、欧盟三个主要法域的合规要求，它们的监管重心各有侧重。</p><h4>美国：诉讼驱动，后果严重</h4><p>美国监管的特点是以诉讼为核心手段，一旦被执法机构“盯上”，往往面临连锁反应式的处罚。</p><p><strong>典型案例：</strong> 儿童教育机器人品牌 Apitor 因违反《儿童在线隐私保护法》（COPPA）受到处罚。其违规行为包括：通过 SDK 收集儿童精确位置信息、将数据回传中国服务器、隐私政策与实际操作严重不符。最终结果是 <strong>50 万美元和解金</strong>，外加<strong>十年期强制整改令</strong>——需销毁违规数据、接受第三方审计、定期提交合规报告。这种长周期、高成本的整改要求，几乎等同于产品在北美市场的“出局”。</p><h4>欧盟：GDPR 的严格执行</h4><p>欧盟以《通用数据保护条例》（GDPR）为核心，建立了全球最严格的数据保护体系。其核心理念是：<strong>数据归用户所有，企业使用需获得明确授权</strong>。</p><p>GDPR 的五项关键要求：</p><table><thead><tr><th align="left">要求</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">高额罚款</td><td align="left">违规罚款可达全球营收的 4%，科技巨头屡被开出天价罚单</td></tr><tr><td align="left">被遗忘权</td><td align="left">用户有权要求删除其数据，对已用于模型训练的数据如何处理是 AI 企业的难题</td></tr><tr><td align="left">数据最小化</td><td align="left">只能收集业务运行所必需的最少数据</td></tr><tr><td align="left">知情同意</td><td align="left">必须以清晰易懂的语言告知用户数据用途、存储期限、共享对象</td></tr><tr><td align="left">跨境限制</td><td align="left">数据出境需满足充分性认定或签署标准合同条款</td></tr></tbody></table><p><strong>值得警惕的案例：</strong> 某消费级摄像头产品因中国工程师通过 VPN 访问存储在欧洲的用户数据，被德法两国数据保护机构认定为<strong>等效的数据跨境传输</strong>。这表明欧盟监管不仅审查数据的物理存储位置，更关注<strong>谁能访问这些数据</strong>。</p><h4>中国：备案先行，合规底线</h4><p>国内以《网络安全法》《数据安全法》《个人信息保护法》构建了完整的数据合规框架。对于 AI 出海业务，有两项硬性要求：</p><ol><li><strong>数据出境合规</strong>：涉及个人信息或重要数据出境，需完成安全评估或标准合同备案。</li><li><strong>AI 服务备案</strong>：算法备案是基础要求；具有舆论属性或内容生成能力的应用，还需完成生成式 AI 服务备案（俗称“双备案”）。</li></ol><p>此外，《网络安全法》第二十一条明确规定：网络日志留存期限不少于六个月。这对日志采集与审计系统提出了明确的技术要求。</p><h2>合规挑战与解决方案</h2><p>面对上述复杂的合规环境，AI 出海企业需要一套完整的技术方案来支撑合规要求。以下从三个核心合规挑战出发，介绍阿里云日志服务（SLS）提供的解决方案。</p><h3>如何实现操作审计与安全事件的快速溯源？</h3><h4>挑战</h4><p>在美国监管的「顺藤摸瓜」式执法模式下，企业一旦被调查，需要提供<strong>完整的证据链</strong>来证明合规性。这意味着不仅要记录「谁在何时做了什么」，还要能够快速还原事件的完整上下文。</p><p>然而，现代云环境面临着两大挑战：</p><ul><li><strong>控制面与数据面的割裂：</strong> 云端的资源变更（如 OpenAPI 调用）与底层的运行时行为天然处于两个平行的观测维度。</li><li><strong>异构数据的孤岛效应：</strong> K8s 的编排事件、ECS 的系统日志以及云产品的操作记录分散在不同的存储介质中，缺乏统一的上下文关联。</li></ul><p>这种多维度的碎片化导致运维与安全团队深陷「数据丰富但信息贫乏」的困境。当异常发生时，仅凭离散的日志，很难将一个高阶的 API 操作精准映射到底层的进程执行或文件读写。</p><h4>解决方案：云监控 2.0 日志审计</h4><p>云监控 2.0 日志审计 <strong>[</strong> <strong>1]</strong> 打破了传统的单点日志查询模式，通过统一采集基座配合三大核心分析能力，构建完整的审计体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578487" alt="image" title="image" loading="lazy"/></p><p><strong>核心能力：</strong></p><ul><li><strong>统一采集基座：</strong> 整合云产品日志与端侧运行时数据，屏蔽数据来源的碎片化差异。通过 LoongCollector <strong>[</strong> <strong>2]</strong> 以轻量级、无侵入的方式深入 ECS 主机和容器内部，实时采集文件访问、进程活动等信息。</li><li><strong>UModel 实体建模：</strong> 将离散日志映射到具体的云资源对象（如 Pod、ECS、AK），建立资产视角的上下文。系统基于日志上下文自动识别并连接不同层级的同一实体（如 ACS 层的 ECS 实例即 Infra 层的主机，Infra 层的主机即 K8s 层的节点）。</li><li><strong>跨域关联：</strong> 打通 ACS（云控制层）、Infra（基础设施层）与 K8s（容器编排层），实现跨层级链路追踪。审计人员能够跨越日志源的边界，快速完成复杂的溯源任务。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578488" alt="image" title="image" loading="lazy"/></p><ul><li><strong>告警调查与风险溯源：</strong> 提供基于实体的风险发现与溯源能力，支持内置与自定义规则。告警通过调查按钮直达风险拓扑，将复杂的风险关系以拓扑图的方式直观呈现。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578489" alt="image" title="image" loading="lazy"/></p><p><strong>合规效果：</strong></p><ul><li><strong>AK 审计场景：</strong> 当发生 AK 泄露时，系统不再展示孤立的操作记录，而是将 AK 的使用轨迹绘制成完整的调用链路。管理员可清晰看到该 AK 关联的角色权限及历史访问过的资源，快速厘清「谁持有密钥，动了什么数据」。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578490" alt="image" title="image" loading="lazy"/></p><ul><li><strong>网络异常流量检测场景：</strong> 面对复杂的云网络环境，仅靠 IP 地址很难快速定位问题。日志审计 2.0 集成 VPC 流日志，让网络合规审计变得更加高效。通过地理位置、公网流量等维度，实时监测和分析异常网络流量的来源，例如攻击尝试或突发的不明大流量访问。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578491" alt="image" title="image" loading="lazy"/></p><ul><li><strong>容器威胁感知场景：</strong> 当容器内部被执行未授权命令时（如 Ollama 漏洞被利用写入敏感路径），系统通过对进程事件及文件操作建模，管理员可以从风险进程顺藤摸瓜，找到其上下游调用关系，将攻击路径清晰还原为「异常进程 → Pod → K8s」。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578492" alt="image" title="image" loading="lazy"/></p><ul><li><strong>主机暴力破解场景：</strong> 一旦检测到暴力破解告警，系统自动构建从底层主机到云端 ECS 的关联视图，并展示 VPC、安全组等周边资产，帮助运维人员迅速判断内网横向移动的风险边界。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578493" alt="image" title="image" loading="lazy"/></p><p>这种方案让日志审计不再是孤立的数据查询，而是围绕资源对象的全生命周期行为分析，真正实现从「看日志」到「掌全局」的安全运营升级。</p><h3>如何满足日志留存与集中审计的法规要求？</h3><h4>挑战</h4><p>全球各主要法域对日志留存都有明确的强制性要求：</p><ul><li><strong>中国《网络安全法》：</strong> 网络日志留存不少于六个月</li><li><strong>欧盟 GDPR：</strong> 要求数据访问可追溯，能够证明数据处理的合法性</li><li><strong>美国各行业法规：</strong> 如 PCI-DSS、HIPAA 等对日志审计有严格规定</li></ul><p>对于出海企业而言，更大的挑战在于：业务横跨全球多个地域，不同地域的日志需要满足<strong>数据本地化存储要求</strong>，同时又需要实现<strong>集中化分析</strong>以满足安全运营需求。一个基础的全球数据存储布局<strong>至少需要覆盖四个节点</strong>：</p><ul><li><strong>美国</strong>：覆盖北美及大部分中南美洲市场。</li><li><strong>欧盟</strong>：通常选择法兰克福，覆盖整个欧盟及英国市场。</li><li><strong>新加坡</strong>：覆盖东南亚市场（印度、沙特、日韩等需单独节点）。</li><li><strong>中国</strong>：服务国内用户。</li></ul><p>传统方案往往导致「信息孤岛」——日志分散在不同地域、不同账号，无法形成统一的安全视图。</p><h4>解决方案：日志审计（新版）</h4><p>阿里云日志审计（新版） <strong>[</strong> <strong>3]</strong> 专为跨地域、跨账号的日志集中管理而设计，已通过《信息安全技术网络安全专用产品安全技术要求》（GB 42250-2022）及《信息安全技术日志分析产品安全技术要求》（GA/T 911-2019）认证，是国家认可的<strong>网络安全专用产品</strong>。备注：当前以独立的应用形态存在，后续将于云监控 2.0 彻底融合。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578494" alt="image" title="image" loading="lazy"/></p><p><strong>核心能力：</strong></p><ul><li><strong>多日志中心汇总</strong>：支持将国内日志存储到上海中心、国外日志存储到新加坡中心，满足跨境合规的数据本地化要求。日志只需接入一次，即可根据规则配置汇总到多个目标日志库。</li><li><strong>RD 资源目录跨账号采集</strong>：基于阿里云资源目录（RD），管理员可以一键将成员账号的所有日志汇总到管理员账号下，实现组织级别的统一审计。当资源目录下有账号新增或变更时，系统会自动适应。</li><li><strong>云产品日志自动化接入</strong>：深度集成操作审计（ActionTrail）、对象存储（OSS）、专有网络（VPC）、负载均衡（SLB）等关键云产品的日志。用户无需手动配置复杂的投递规则，只需简单的接入操作即可自动完成底层资源的编排与日志流转。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578495" alt="image" title="image" loading="lazy"/></p><p>这种方案打破了「信息孤岛」，在满足各地数据本地化存储要求的同时，实现了全球日志的统一管理和安全洞察。</p><h3>如何保护敏感数据，防止隐私泄露？</h3><h4>挑战</h4><p>GDPR 的「数据最小化原则」要求企业只能收集业务必需的最少数据，同时各国对敏感数据（生物识别、儿童数据等）的保护要求越来越严格。</p><p>然而，AI 应用的日志中往往隐藏着大量敏感数据：</p><ul><li>用户咨询里可能出现手机号、订单号、收货地址。</li><li>后端业务日志中常常包含银行卡号、接口 IP、账户 ID。</li><li>工单流转过程中甚至会附带内部 Token、用户名。</li></ul><p>这些信息若在系统内未经处理地流转、存储或导出，不仅违反数据最小化原则，更可能在调试、共享或导出日志时意外泄露。然而，现实场景中又无法简单地「少打日志」或「去掉字段」——日志是运维排障的工具，是运营分析的基础，也是安全审计的依据。</p><h4>解决方案：脱敏函数</h4><p>SLS 提供了丰富的脱敏方案，用户可以根据情况灵活选择：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578496" alt="image" title="image" loading="lazy"/></p><ul><li>Logtail 端侧脱敏（数据流 1）：配置 SLSLogtail采集后，在端侧进行处理脱敏，然后写入 SLS 日志库中。</li><li>Logtail + Ingest Processorer 脱敏（数据流 2）组合：对于日志产生速度较高，且需要进行大量正则处理的场景，iLogtail 本身也会占用一定的计算资源。为了避免高强度的资源占用严重影响服务器上的其他业务进程，可以在 Logtail 端侧仅配置采集任务，然后通过 Ingest Processorer（写入处理器）配置 SPL 语句在日志服务侧完成脱敏处理。</li><li>SDK+ Ingest Processorer 脱敏（数据流 3）组合：除了通过 Logtail 采集日志外，我们还可以基于SDK通过接口调用完成日志写入，通过 Ingest Processorer里设置脱敏语句，脱敏处理在日志服务中完成，不占用端侧资源。</li></ul><p>传统数据脱敏往往采用正则处理的方式，但在面对日益复杂的数据场景时，正则表达式的局限性也逐渐凸显：处理十多种敏感信息类型需要编写数十个复杂正则表达式，维护成本呈指数级增长；多重嵌套的正则操作会严重拖慢实时处理性能；JSON、URI、纯文本的混合日志格式难以用统一正则配置高效处理。为此，SLS 推出了全新的 mask（脱敏）函数 <strong>[</strong> <strong>4]</strong> ，能够对结构化和非结构化日志中的敏感数据进行精准识别和脱敏，无需编写复杂正则，开箱即用。</p><p><strong>核心能力：</strong></p><ul><li><strong>内置匹配（buildin）</strong>：开箱即用，内置对常见 6 种敏感信息的智能识别能力——手机号、身份证、邮箱、IP 地址、座机电话、银行卡号。无需编写任何正则表达式，仅需在配置中指定要脱敏的类型即可。</li><li><strong>关键字匹配（keyword）</strong>：智能识别任意文本中符合 <code>"key":"value"</code>、<code>'key':'value'</code> 或 <code>key=value</code> 等常见 KV 对格式的敏感信息。即使数据嵌套多层 JSON 结构，也只需配置最内层的 Key 即可精准匹配，特别适合处理 AI 应用中常见的复杂嵌套日志。</li><li><strong>按需保留</strong>：针对不同敏感字段，可定制化保留前后缀字符。例如手机号保留前三后四位（<code>199****2345</code>），既保护用户隐私，又方便运维人员进行问题排查和用户身份核验，实现安全与可用性的平衡。</li><li><strong>高性能处理</strong>：相比传统正则方案，mask 函数在复杂脱敏场景下性能提升可达 2.8 倍，特别适用于大数据量和多类型敏感信息混合处理的场景。</li></ul><h2>结语</h2><p>对于 AI 出海企业而言，合规不是「要不要做」的选择题，而是「该怎么做」的必答题。从 Manus 的成功路径可以看到，前置解决数据合规、法律合规问题，是融入国际市场的关键一步。</p><p>在实践中，有三条经验值得借鉴：</p><ol><li><strong>合规布局比业务推进早半步</strong>：很多企业发展速度非常快，短短几个月用户就能涨到数万。如果在用户爆发后才考虑数据架构迁移或团队海外落地，不仅成本极高，风险也更大。合规规划应当与产品规划同步启动。</li><li><strong>合规是持续运营而非一次性工作</strong>：全球监管环境在不断演进，GDPR 在持续更新，各国数据保护法规也在陆续出台。企业需要建立持续的合规监控机制，而非将合规视为一次性的“过审”项目。</li><li><strong>技术方案要支撑业务敏捷性</strong>：选择能够自动适应业务变化的技术方案——如自动发现新增云资源、自动适配新增账号、自动识别敏感数据——避免合规成为业务发展的瓶颈。</li></ol><p>阿里云日志服务（SLS）通过日志审计、数据脱敏等能力，为出海企业提供了从日志采集、存储、脱敏到分析溯源的全链路合规支撑。无论是满足《网络安全法》的日志留存要求，还是应对 GDPR 的数据保护挑战，都能提供坚实的技术底座。</p><p>合规之路虽然复杂，但有了正确的技术方案和前瞻性的布局，AI 企业就能在全球化浪潮中稳健前行，书写属于自己的出海故事。</p><p><strong>参考文章：</strong></p><p>《<a href="https://link.segmentfault.com/?enc=RW2s7Ab2PfaTYrrmijMYgw%3D%3D.4yJofMYQ%2FoyWPLidOrxi7UgpyY0vzI8fkcr1SyWSm3VjTKb0zBAiF8wXMtkJFmUy2MkK1wFOG5hogRv6%2FsMHKRaLqNqjv7XauWK%2Bn4MpaINmHNHobuWD9MOv90je52z3I1aXGpTyBdY613Dbv%2Fe8WM3Vny7P2m2%2BR%2BhEl4zMIrYmSxgGseinocCygEQ3eJte" rel="nofollow" target="_blank">想成为下一个 Manus，先把这些出海合规问题处理好</a>》</p><p>《<a href="https://link.segmentfault.com/?enc=PO3hTsMSKEF%2FFR4eZmKHIQ%3D%3D.KldMQKbPfyRgdbT7qcyDJGpnWg6Y0dyNYG58%2FQ%2BO3MkUcNdQH2be67SkULMzHg4MTZ24LfBBlH%2Bv1mzQpIabr5IoCmO89owD52zSN1VaJut%2FThYB9NF79OlDP7dWN24XeybSuyIJ6RqOYGk5rHMNqK%2FDR9tegOZk4RXFs2a53GpD9T1dxbt%2FZRalVQJ07A38" rel="nofollow" target="_blank">已上线！云监控 2.0 面向实体的全链路日志审计与风险溯源</a>》</p><p><strong>相关链接：</strong></p><p>[1] 云监控 2.0 日志审计</p><p><a href="https://link.segmentfault.com/?enc=f3axg6oG0L0AoYeKyK3k5g%3D%3D.iINmEdTU2Fz3PA93Ypr42zthNL7XnmgWyrkoRe0diU70h72s%2BK%2Bq5F6z%2BwLb%2B7YgJr7ukT7svzIPoCYezgA2QQ%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/cms/cloudmonitor-2-0/log-audit/</a></p><p>[2] LoongCollector</p><p><a href="https://link.segmentfault.com/?enc=jTQDMDtEIPsr3R%2BPOOGWQQ%3D%3D.areciaGQrfp5FVSCle6bS4SWlUacdOf3s%2FtLCXKDVVAhO1wekNw%2BdXCkl8guxS6uXPZfKeWqNnBOme4r5SU1fg%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/sls/what-is-sls-loongcollector</a></p><p>[3] 阿里云日志审计（新版）</p><p><a href="https://link.segmentfault.com/?enc=Ar09GubXT2LEYu4scLtgSg%3D%3D.odaeFaNGTyP0TSCi1YJLtV5s1O3V1eGJ944AZ0VznLHmvjYxAiSsHQ9i7Mvhe%2BN%2BnAaxOWA6AQvr7f6lJRngnA%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/sls/new-log-audit-service/</a></p><p>[4] mask（脱敏）函数</p><p><a href="https://link.segmentfault.com/?enc=KqNhPqHBbw2Xk%2FmTgXW3uw%3D%3D.Lhqz7HZzHo9avOdApOtTVY3T9WhD0XdLBeB3ZXe5B9DJcwkwjqlmjZYqXy6v590tJHLg5kwXFlMb%2FFdhw9bk1w2ikNIEMUOL3IQ7TnpBVgM%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/sls/data-masking-with-the-mask-fun...</a></p>]]></description></item><item>    <title><![CDATA[AI代理的上下文工程：构建Manus的经验教训 本文系转载，阅读原文
https://manus.i]]></title>    <link>https://segmentfault.com/a/1190000047578509</link>    <guid>https://segmentfault.com/a/1190000047578509</guid>    <pubDate>2026-01-28 18:08:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025/7/18 --Yichao 'Peak' Ji</p><p>在<a href="https://link.segmentfault.com/?enc=%2FkbE9yaaT3QHXT0L%2F5ERSA%3D%3D.ia8w%2BD8QhebxuErtZZq2Ie1vFm6mMLcJWOk4FfL0dnk%3D" rel="nofollow" target="_blank">Manus</a>项目的最初阶段，我和我的团队面临一个关键决策：我们是应该使用开源基础模型训练一个端到端的智能体模型，还是基于前沿模型的<a href="https://link.segmentfault.com/?enc=2ecWNBTcGxh2WCv1YbUDFw%3D%3D.dCTLcQHHTM%2Ft3YjrKtBaN1xQyefelxq%2BK8CV0%2Bi93Dyqzv4fsTt%2BVHN8QcPAQv70" rel="nofollow" target="_blank">上下文学习</a>能力构建一个智能体？</p><p>在我的NLP生涯的第一个十年里，我们没有这种选择的奢侈。在遥远的<a href="https://link.segmentfault.com/?enc=mPNt7f1lsqBeXgV3HiMSNA%3D%3D.xspB%2FFeuQX80yDyOvAm2P3TQjQ2HxPEpDEMkncOQ1%2FEDoW9Q1mSC62NHm%2F8Q4oM9" rel="nofollow" target="_blank">BERT</a>时代（是的，已经过去七年了），模型必须先进行微调——和评估——才能迁移到新任务。这个过程通常每次迭代需要数周时间，尽管与今天的LLM相比，这些模型非常小。对于快速发展的应用，特别是在产品市场匹配(PMF)之前，这种<strong>缓慢的反馈循环</strong>是一个致命缺陷。这是我上一个创业公司的惨痛教训，当时我从头开始训练模型用于<a href="https://link.segmentfault.com/?enc=rfhEPbDSduDDYTu1Hqgs7w%3D%3D.1yHttpNZaWzIayV%2BJ%2Bvw94hFr05VQFlAm%2B1qaUUv1FTY42tifEhRGKaL19GU1WkSpI69eq0e%2FCJxFKaVNOL9xg%3D%3D" rel="nofollow" target="_blank">开放信息提取</a>和语义搜索。然后<a href="https://link.segmentfault.com/?enc=a7TbTJdqSvyts%2B%2B3M3p%2B4g%3D%3D.DKNzdNa6OwTQm1gZTuwSsojYfZ5FB2jZUyQZLT%2FWIYEHMvbhWhOFQ8iiql47V5l5" rel="nofollow" target="_blank">GPT-3</a>和<a href="https://link.segmentfault.com/?enc=aGh1MuoC%2B%2F72L2dUxdefHg%3D%3D.tvfVKlOFF8Uemo5aNNZMTuWDdX9e8nwU5cowzJgsf8k9hWPEDAdJRUkmy%2BzqXPBN" rel="nofollow" target="_blank">Flan-T5</a>出现了，我的内部模型一夜之间变得无关紧要。具有讽刺意味的是，这些相同的模型标志着上下文学习的开始——以及一条全新的前进道路。</p><p>这个来之不易的教训使选择变得明确：<strong>Manus将押注于上下文工程</strong>。这使我们能够在几小时而非几周内交付改进，并使我们的产品与底层模型保持正交：<strong>如果模型进步是上涨的潮水，我们希望Manus成为那条船</strong>，而不是固定在海床上的柱子。</p><p>尽管如此，上下文工程证明绝非易事。这是一门实验科学——我们已经重建了我们的代理框架四次，每次都是在发现了更好的塑造上下文的方式之后。我们亲切地将这种手动架构搜索、提示调整和经验猜测的过程称为"<strong>随机研究生下降</strong>"。这并不优雅，但它有效。</p><p>这篇文章分享了我们通过自己的"SGD"所达到的局部最优解。如果你正在构建自己的AI代理，我希望这些原则能帮助你更快地收敛。</p><h3>围绕KV缓存进行设计</h3><p>如果我必须选择一个指标，我认为 <strong>KV-cache命中率</strong> 是生产阶段AI代理最重要的单一指标。它直接影响延迟和成本。为了理解原因，让我们看看<a href="https://link.segmentfault.com/?enc=RtOoQwhK86z%2Fi1HTm9Erqg%3D%3D.aZrGe3rKYKE7AKoFqQWBtDrPZiXN5kM3E%2B32rV%2FiDKLmGSkawMF7kB1%2FAf8e89iw" rel="nofollow" target="_blank">典型代理</a>是如何运作的：</p><p>在接收用户输入后，代理通过一系列工具使用链来完成任务。在每次迭代中，模型根据当前上下文从预定义的动作空间中选择一个<strong>动作</strong>。然后在<strong>环境中</strong>执行该动作（例如，Manus的虚拟机沙盒）以产生<strong>观察结果</strong>。动作和观察结果被附加到上下文中，形成下一次迭代的输入。这个循环持续进行，直到任务完成。</p><p>正如你所想象的，随着每一步的推进，上下文不断增长，而输出——通常是结构化的函数调用——保持相对简短。这使得代理（agents）相比聊天机器人的<strong>预填充</strong>和<strong>解码</strong>比例高度倾斜。例如在Manus中，平均输入与输出的token比例约为100:1。</p><p>幸运的是，具有相同前缀的上下文可以利用<a href="https://link.segmentfault.com/?enc=5Y7hPkwoaVtcVF%2FLcMUueA%3D%3D.Xpv0jwmcbnxWWBRWLgcZCjxVJ7%2FnIl4eOyzJq17rTpE%2Fco9fqdHDXUloEZqVQd1vtKQL3XitqBn6IBItFIMJRQ%3D%3D" rel="nofollow" target="_blank">KV缓存</a>，这大大减少了<strong>首个token的生成时间(TTFT)和推理成本——无论你是使用自托管模型还是调用推理API。我们说的不是小幅度的节省：例如使用Claude Sonnet时，缓存的输入token成本为0.30美元/百万token</strong>，而未缓存的成本为3美元/百万token——相差10倍。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578511" alt="图片" title="图片"/></p><p>从上下文工程的角度，提高KV缓存命中率涉及几个关键实践：</p><ol><li><strong>保持你的提示前缀稳定。</strong> 由于LLM的<a href="https://link.segmentfault.com/?enc=%2Fxn%2FgDUTf8O%2BoeqbCIQCCw%3D%3D.7QwllDaw4WhzQLaM4eGXboO5zpnmXIPLIgEX31NhSL%2F2%2F12PXZuVEE2dTyGPzXKbgflHrD2Ryoy%2B1giptgY7QQ%3D%3D" rel="nofollow" target="_blank">自回归</a>特性，即使是单个标记的差异也会使该标记之后的缓存失效。一个常见的错误是在系统提示的开头包含时间戳——尤其是精确到秒的时间戳。虽然这让模型能告诉你当前时间，但也会降低你的缓存命中率。</li><li><strong>使你的上下文只追加。</strong> 避免修改之前的操作或观察。确保你的序列化是确定性的。许多编程语言和库在序列化JSON对象时不保证键顺序的稳定性，这可能会悄无声息地破坏缓存。</li><li><strong>在需要时明确标记缓存断点。</strong> 某些模型提供商或推理框架不支持自动增量前缀缓存，而是需要在上下文中手动插入缓存断点。在分配这些断点时，要考虑潜在的缓存过期问题，并至少确保断点包含系统提示的结尾。</li></ol><p>此外，如果你正在使用像 <a href="https://link.segmentfault.com/?enc=1B3a8WA5joQIlbKCj%2FUT6Q%3D%3D.LjhZYWFWex2pQDdo2splle2v9aU3S39Juvg4TpbXLFAueZczY8lbDfNbF6HDp%2B7%2B" rel="nofollow" target="_blank">vLLM </a>这样的框架自托管模型，请确保启用了<a href="https://link.segmentfault.com/?enc=DL8J4RrJCOBS8Jjanb5gbw%3D%3D.5j3%2BXsD7fiEEoyv6lhhXD7j9LzwUDu8aCfEPdh6WTYLsLmppvjLPez074fX9kawzqFJ39H5MdBPd1plY2XmYLQ%3D%3D" rel="nofollow" target="_blank">前缀/提示缓存</a>，并且你正在使用会话 ID 等技术在分布式工作节点之间一致地路由请求。</p><h3>遮蔽，而非移除</h3><p>随着代理能力的增强，其行动空间自然变得更加复杂——简单来说，<strong>工具数量</strong>爆炸式增长。最近流行的<a href="https://link.segmentfault.com/?enc=RcnJk98VgcYTLNKfnFp4Iw%3D%3D.ce3NATabfFmrd%2BeJYMawZPcbATa3xjFJk0mE%2FQv%2BIP7eff7xStwHpC7bV%2F8pgLIhMuXUXzy4QIL1NBm7KW023A%3D%3D" rel="nofollow" target="_blank">MCP</a>只会火上浇油。如果你允许用户自定义工具，相信我：总会有人将数百个神秘工具插入到你精心策划的行动空间中。结果，模型更可能选择错误的行动或采取低效的路径。简而言之，你武装过度的代理变得更加愚蠢。</p><p>一个自然的反应是设计一个动态行动空间——可能是使用类似于<a href="https://link.segmentfault.com/?enc=nFx%2BJGJXGlEaTUgNa%2Bh8%2Bw%3D%3D.bMIQHbksICQ0PErHviQiRww%2BzXJj1vCHmP5OZYmA5AIF5SMdPCzHzVU4Cr%2FM58ZKOK6sijkbYd6wjDjJXEYVbw%3D%3D" rel="nofollow" target="_blank">RAG</a>的方法按需加载工具。我们在Manus中也尝试过这种方法。但我们的实验表明了一个明确的规则：除非绝对必要，<strong>避免在迭代过程中动态添加或移除工具。</strong> 这主要有两个原因：</p><ol><li>在大多数LLM中，工具定义在序列化后位于上下文的前部，通常在系统提示之前或之后。因此任何更改都会使后续所有动作和观察的KV缓存失效。</li><li>当先前的动作和观察仍然引用当前上下文中不再定义的工具时，模型会感到困惑。如果没有<a href="https://link.segmentfault.com/?enc=ODtSGph2qoiB3DG7OXpCQw%3D%3D.meSj2N0AdilTRvI9WAac7IRtiEfTWqICryiTgpeACSQ7uJObgXhPcD4QR1VG1ljVhP1u1d9swd8oFx1BVxznqg%3D%3D" rel="nofollow" target="_blank">约束解码</a>，这通常会导致<strong>模式违规或幻觉动作</strong>。</li></ol><p>为了解决这个问题并仍然改进动作选择，Manus使用上下文感知的<a href="https://link.segmentfault.com/?enc=r1DM4nNUbWTAblyHLMoGzA%3D%3D.DZrbUqeP8iz23d97MbRjyTygUf%2FKnTECFa0ABYRmd9JYi%2BSxIm%2BD26hKnYBlnxQOM6duQ0Q5hVEbR2pzTG32Vw%3D%3D" rel="nofollow" target="_blank">状态机</a>来管理工具可用性。它不是移除工具，而是在解码过程中<strong>掩蔽token的logits</strong>，以基于当前上下文阻止（或强制）选择某些动作。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578512" alt="图片" title="图片" loading="lazy"/></p><p>在实践中，大多数模型提供商和推理框架支持某种形式的<strong>响应预填充</strong>，这允许你在不修改工具定义的情况下约束动作空间。函数调用通常有三种模式（我们将使用 NousResearch 的 <a href="https://link.segmentfault.com/?enc=NDSRywS4cRHy74mHd7a4Nw%3D%3D.8CAWSk%2Fje%2FXrGOz30GC5hLeeyJ19O7VUb%2BSCmS2bgwP8kaAPCL3aUchG4T%2FDQTRrUUW3xi2l2aSdbAT4opD%2Fgg%3D%3D" rel="nofollow" target="_blank">Hermes</a> 格式 作为示例）：</p><ul><li>自动 – 模型可以选择调用或不调用函数。通过仅预填充回复前缀实现：&lt;|im_start|&gt;assistant</li><li>必需 – 模型必须调用函数，但选择不受约束。通过预填充到工具调用令牌实现：&lt;|im_start|&gt;assistant&lt;tool_call&gt;</li><li>指定 – 模型必须从特定子集中调用函数。通过预填充到函数名称的开头实现：&lt;|im_start|&gt;assistant&lt;tool_call&gt;{"name": "browser_</li></ul><p>通过这种方式，我们通过直接掩码token的logits来约束动作选择。例如，当用户提供新输入时，Manus必须立即回复而不是执行动作。我们还有意设计了具有一致前缀的动作名称——例如，所有与浏览器相关的工具都以browser_开头，命令行工具以shell_开头。这使我们能够轻松确保代理在给定状态下只从特定工具组中进行选择<strong>而无需使用有状态的logits处理器</strong>。</p><p>这些设计有助于确保Manus代理循环保持稳定——即使在模型驱动的架构下。</p><h3>使用文件系统作为上下文</h3><p>现代前沿LLM现在提供128K令牌或更多的上下文窗口。但在真实世界的代理场景中，这通常不够，有时甚至是一种负担。有三个常见的痛点：</p><ol><li><strong>观察结果可能非常庞大</strong>，尤其是当代理与网页或PDF等非结构化数据交互时。很容易超出上下文限制。</li><li><strong>模型性能往往会下降</strong>，超过一定的上下文长度后，即使技术上支持该窗口大小。</li><li><strong>长输入成本高昂</strong>，即使使用前缀缓存。你仍然需要为传输和预填充每个token付费。</li></ol><p>为了解决这个问题，许多代理系统实现了上下文截断或压缩策略。但过度激进的压缩不可避免地导致信息丢失。这个问题是根本性的：代理本质上必须根据所有先前状态预测下一个动作——<strong>而你无法</strong>可靠地预测哪个观察结果可能在十步之后变得至关重要。从逻辑角度看，任何不可逆的压缩都带有风险。</p><p>这就是为什么我们在Manus中将<strong>文件系统视为终极上下文</strong>：大小不受限制，天然持久化，并且代理可以直接操作。模型学会按需写入和读取文件——不仅将文件系统用作存储，还用作结构化的外部记忆。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578513" alt="图片" title="图片" loading="lazy"/></p><p>我们的压缩策略始终设计为<strong>可恢复的</strong>。例如，只要保留URL，网页内容就可以从上下文中移除；如果沙盒中仍然保留文档路径，则可以省略文档内容。这使得Manus能够缩短上下文长度，而不会永久丢失信息。</p><p>在开发这个<strong>功能</strong>时，我发现自己在想象<strong>状态空间模型(State Space Model, SSM)</strong> 在智能体环境中有效工作需要什么条件。与Transformer不同，SSM缺乏完整的注意力机制，并且在处理长距离的后向依赖关系时表现不佳。但如果它们能够掌握基于文件的记忆——将长期状态外部化而不是保存在上下文中——那么它们的速度和效率可能会开启一类新型智能体。基于SSM的智能体可能是<a href="https://link.segmentfault.com/?enc=UP3nH4VV2pLYqJovrIt2UQ%3D%3D.WlQqemw0owkHOunytLd8ZcieGrWMkVLlJ0D9qQbRdpc%3D" rel="nofollow" target="_blank">神经图灵机</a>真正的继任者。</p><h3>通过复述操控注意力</h3><p>如果你使用过Manus，你可能注意到一个有趣的现象：在处理复杂任务时，它倾向于创建一个<strong>todo.md</strong>文件——并在任务进行过程中逐步更新它，勾选已完成的项目。这不仅仅是可爱的行为——这是一种<strong>操控注意力</strong>的刻意机制。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578514" alt="图片" title="图片" loading="lazy"/></p><p>Manus中的一个典型任务平均需要大约<strong>50次工具调用</strong>。这是一个很长的循环——由于Manus依赖LLM进行决策，它很容易偏离主题或忘记早期目标，尤其是在长上下文或复杂任务中。</p><p>通过不断重写待办事项列表，Manus将<strong>其目标复述到上下文的末尾</strong>。这将全局计划推入模型的近期注意力范围内，避免了"<strong>丢失在中间</strong>"的问题，并减少了目标不一致。实际上，它使用自然语言来使自己的注意力偏向任务目标——而不需要特殊的架构变更。</p><h3>保留错误的内容</h3><p>代理会犯错。这不是bug——这是现实。语言模型会产生幻觉，环境会返回错误，外部工具会出现异常行为，意外的边缘情况随时都会出现。在多步骤任务中，失败不是例外；它是循环的一部分。然而，一个常见的冲动是隐藏这些错误：清理痕迹，重试操作，或重置模型的状态并将其留给神奇的"<a href="https://link.segmentfault.com/?enc=NjIvt04m3Sq%2F6E9PmGByNg%3D%3D.%2B6zJ4JcqOWGKbNH9sZ9OACbIaZ3HJONfoZAe7zJ1NG7Mbn5BI%2FXX9YMSOaguOKrm" rel="nofollow" target="_blank">温度</a>"。这感觉更安全，更受控制。但这是有代价的：<strong>擦除失败会移除证据</strong>。没有证据，模型就无法适应。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578515" alt="图片" title="图片" loading="lazy"/></p><p>根据我们的经验，改善代理行为最有效的方法之一出奇地简单：<strong>将错误的尝试保留在上下文中。</strong> 当模型看到一个失败的行动——以及由此产生的观察结果或堆栈跟踪——它会隐式地更新其内部信念。这会改变其先验，降低重复相同错误的可能性。</p><p>事实上，我们认为<strong>错误恢复</strong>是真正代理行为的最明显指标之一。然而，在大多数学术工作和公共基准测试中，这一点仍然代表性不足，它们通常关注理想条件下的任务成功。</p><h3>不要被少样本示例所困</h3><p><a href="https://link.segmentfault.com/?enc=Im4mN8IK3WaDujHeTHYK0w%3D%3D.Ydw0YlKXXEs6xwafPLutgMHWPV7TcUJWhX%2BbImUJDTBfX9WOe5FQWh3mobNWFdwNxkgS8uaeiHZ%2FzrdcN2sRtg%3D%3D" rel="nofollow" target="_blank">少样本提示</a>是提高LLM输出的常用技术。但在代理系统中，它可能会以微妙的方式适得其反。</p><p>语言模型是优秀的模仿者；<strong>它们模仿上下文中的行为模式</strong>。如果你的上下文充满了类似的过去行动-观察对，模型将倾向于遵循该模式，即使这不再是最优的。</p><p>这在涉及重复决策或行动的任务中可能很危险。例如，当使用Manus帮助审查20份简历时，代理通常会陷入一种节奏——仅仅因为这是它在上下文中看到的，就重复类似的行动。这导致偏离、过度泛化，或有时产生幻觉。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578516" alt="图片" title="图片" loading="lazy"/></p><p>解决方法是<strong>增加多样性</strong>。Manus在行动和观察中引入少量的结构化变化——不同的序列化模板、替代性措辞、顺序或格式上的微小噪音。这种受控的随机性有助于打破模式并调整模型的注意力。</p><p>换句话说，<strong>不要让自己陷入少样本学习的窠臼</strong>。你的上下文越单一，你的智能体就变得越脆弱。</p><h3>结论</h3><p>上下文工程仍然是一门新兴的科学——但对于智能体系统来说，它已经是必不可少的。模型可能变得更强大、更快速、更经济，但再多的原始能力也无法替代对记忆、环境和反馈的需求。你如何塑造上下文最终决定了你的智能体的行为方式：它运行的速度、恢复的效果以及扩展的范围。</p><p>在Manus，我们通过反复的重写、死胡同以及<strong>面向数百万用户的实际测试</strong>学到了这些经验。我们在这里分享的内容并非放之四海而皆准的真理——但这些是对我们有效的模式。如果它们能帮助你避免哪怕一次痛苦的迭代，那么这篇文章就达到了它的目的。</p><p>智能体的未来将一次构建一个上下文。好好设计它们吧。</p>]]></description></item><item>    <title><![CDATA[华为 CodeArts、飞书项目与 Teamcenter：三类 IPD 工具的落地经验 流程驱动过程]]></title>    <link>https://segmentfault.com/a/1190000047578548</link>    <guid>https://segmentfault.com/a/1190000047578548</guid>    <pubDate>2026-01-28 18:07:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业研发管理实践中，IPD 往往是必修课，但很多团队在推进过程中发现，光有流程和制度远远不够。工具的选择，往往直接决定了 IPD 能否真正落地。<br/>华为云 CodeArts、飞书项目与 Siemens Teamcenter 各自沿着不同的路线优化研发协作与流程管理：有的偏向完整工业级流程，有的擅长敏捷团队协作，有的强调数据和配置管理。<br/>本文将结合实际落地场景，分析三款工具在不同组织类型和研发阶段中的适配度与能力边界，帮助团队在选型时少走弯路。</p><h2><a href="https://link.segmentfault.com/?enc=wVcVEHAS5bhAl5QLWDWSAQ%3D%3D.lVQidjdV0EEa%2FSIZia4wqF%2F2Ast8PP4Mj9mJLc9AOA5KKduuwJBuKX3fVNrJJ%2BTmYYMxj7N2ApOV7bIgAQHnUg%3D%3D" rel="nofollow" target="_blank">华为云 CodeArts</a>（原 DevCloud）</h2><blockquote>定位： IPD理念的“原产地”与“正统派”。</blockquote><h3>核心卖点</h3><p>源自华为 30 年实践： 这不是一款普通的商业软件，而是华为将自身 30 年的研发管理变革经验“代码化”后的产物。它内置了华为内部一直在使用的标准工作流和管理模板。端到端全链路打通： 实现了从客户需求（Epic）到特性（Feature）再到开发任务（Story）的闭环管理，确保每一个开发动作都能追溯到最初的市场价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578550" alt="图片" title="图片"/></p><h3>如何支撑 IPD 流程</h3><ol><li>结构化流程固化： IPD 强调复杂的决策评审（CDCP、PDCP）。<br/>CodeArts 预置了这些关键节点，强制要求项目在进入下一阶段前完成必要的动作，防止流程“随意剪裁”。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578551" alt="图片" title="图片" loading="lazy"/></p><ol start="2"><li>分层分级管理： 完美适配 IPD 的“产品线-产品-版本”三层架构。<br/>它允许 PD（产品经理）在顶层看路标，PM（项目经理）在中间管进度，开发人员在底层做执行，数据自动聚合。</li><li>需求价值流（OR）： <br/>它的需求管理极其严谨，支持 $APPEALS 等分析模型，帮助团队在立项初期就剔除伪需求。</li></ol><h3>缺点与挑战</h3><ol><li>灵活性不足：带有强烈的“华为基因”，流程规范非常严苛。对于不想完全照搬华为模式的企业，修改配置的难度较大。</li><li>上手门槛高： 界面充斥着大量的专业术语和复杂字段，如果团队没有经过 IPD 培训，员工会有较强的抵触心理。</li></ol><h3>适合谁？</h3><p>行业： ICT、通信、大型软件研发、智能硬件。<br/>企业画像： 立志全盘引入华为管理体系的中大型企业，且企业内部已有一定的流程管理文化基础。</p><h2><a href="https://link.segmentfault.com/?enc=wmY8SpI6VK2qaB34DEUrrA%3D%3D.2G495Zg9TAWjOEUh0Rsb5BCYcWHyInsvZgRCyTrBsQRqjP2r1aGI6QXy0CrXkD5Su%2B84q6sHFiBIoIsv91uhe8xCJuSCxRKASGcZnayden8QtusGcpuO3E2vxNjdtxs8IZzFmuLsJeMH7KtRl3m9phjanCrPOusxYn0LiSxKCE3%2FqWZLDnsaiqa2WCeoU1YG" rel="nofollow" target="_blank">飞书项目 - 行业专版</a></h2><blockquote>定位： 流程型组织的大杀器，用“柔性”解决 IPD 的“刚性”痛点。</blockquote><p>飞书项目是 IPD 软件领域的一个破局者。如果说华为 CodeArts 是严谨的“教官”，Teamcenter 是厚重的“仓库”，那么飞书项目更像是一个“超级连接器”。它不仅仅是一个项目管理工具，而是试图用互联网的极致协作体验去重构传统且僵化的 IPD 流程。</p><h3>核心卖点</h3><ol><li>流程像“乐高”一样灵活： <br/>它是市面上配置能力最强的工具之一。传统的 IPD 软件改一个流程可能需要找厂商二次开发，而在飞书项目里，业务人员可以通过拖拽节点自定义复杂的串行、并行、判断流程。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578552" alt="图片" title="图片" loading="lazy"/></p><ol start="2"><li>“聊着天就把项目管了”： <br/>它与飞书（IM、文档、会议）深度集成。IPD 流程中大量的评审（TR）、决策（DCP）往往死于沟通效率低，飞书项目能让评审在群里自动触发，文档直接关联，极大地降低了协作摩擦。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578553" alt="图片" title="图片" loading="lazy"/></p><ol start="3"><li>可视化“泳道图”： <br/>它把复杂的 IPD 计划变成了直观的“全景泳道图”，不同部门（市场、研发、供应链）在同一张图上协作，依赖关系一目了然，非常适合解决跨部门“扯皮”。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578554" alt="图片" title="图片" loading="lazy"/></p><h3>如何与 IPD 流程契合</h3><ol><li>结构化流程落地： <br/>它可以完美复刻 IPD 的“阶段-关口”（Stage-Gate）模型。通过设置“关键节点”，如果不完成规定的评审要素（如文档、签字），流程就无法流转到下一阶段。</li><li>角色与权限协同： <br/>IPD 强调 PDT（产品开发团队）作战，飞书项目支持极细颗粒度的权限管控，能让市场看市场的视图，开发看开发的视图，但底层数据是互通的。</li><li>度量与复盘： <br/>它自带强大的 BI 仪表盘，可以实时分析流程效率（比如：某个评审环节平均卡了多少天），这非常符合 IPD 中“持续改进”的理念。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578555" alt="图片" title="图片" loading="lazy"/></p><h3>缺点与挑战</h3><ol><li>“空盒子”属性： 它刚交付时往往是一个“空盒子”或者“半成品模版”，需要企业有很强的流程配置专家（ CSM）去把公司的 IPD 流程“画”进去。如果你没有想清楚自己的流程，用起来会很乱。</li><li>工程数据管理弱： 它擅长管“事”和“人”，但不擅长管“物”。它无法像 Teamcenter 那样管理复杂的 BOM 结构、CAD 图纸的版本分支。做硬件研发时，它通常需要和 PLM 系统做对接。</li></ol><h3>适合企业/行业</h3><p>行业： 新势力造车、消费电子（手机、无人机）、游戏、复杂的软硬结合项目。<br/>企业类型： 1.  追求速度的创新型企业： 觉得传统 PLM 太慢、太难用，希望用互联网思维做硬件的公司。 2.  协作痛点极大的公司： 部门墙严重，急需通过工具拉通沟通的企业。</p><h2>Siemens Teamcenter</h2><blockquote>定位：全球制造业的“物理底座”与“数据派”</blockquote><h3>核心卖点</h3><ol><li>单一数据源（Single Source of Truth）： <br/>无论你有多少个工厂、多少个设计中心，Teamcenter 确保所有人看到的图纸、BOM 和工艺数据是唯一且准确的。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578556" alt="图片" title="图片" loading="lazy"/></p><ol start="2"><li>多学科融合： <br/>它是极少数能同时完美管理机械（MCAD）、电子（ECAD）和软件（ALM）数据的平台，是复杂系统工程的基石。</li></ol><h3>如何支撑 IPD 流程</h3><ol><li>刚性的阶段门径控制（Stage-Gate）： Teamcenter 最擅长管理 IPD 的“关口”。如果不完成规定的工程文档归档，系统会物理锁死，无法进入下一阶段，确保流程绝对刚性执行。</li><li>变更管理（ECR/ECO）： IPD 流程中，产品变更牵一发而动全身。Teamcenter 提供了最严谨的变更闭环管理，确保从设计到制造的一致性。</li></ol><h3>缺点与挑战</h3><ol><li>昂贵且笨重： 实施费用通常以百万/千万级计算，周期长达一年以上，不仅是购买软件，更是购买咨询服务。2. 用户体验老旧： 典型的工业软件界面，交互复杂，对于习惯了互联网软件的现代研发人员来说，使用体验极差。</li></ol><h3>适合谁？</h3><p>行业： 汽车主机厂、航空航天、重型机械、高端医疗器械。<br/>企业画像： 产品极其复杂，BOM 结构庞大，对数据准确性和安全性要求高于一切的超大型制造企业。</p>]]></description></item><item>    <title><![CDATA[EAST 口径文档自动化生成：破解 SQL 过滤条件解析难题，实现 20 倍效率提升 Aloudat]]></title>    <link>https://segmentfault.com/a/1190000047578578</link>    <guid>https://segmentfault.com/a/1190000047578578</guid>    <pubDate>2026-01-28 18:07:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=TcNJJLn%2FQujEE7vULBddEA%3D%3D.t50Sv%2By3mHdZEjHaa7DGn1K6K45kcJat43H9OAGsuz0yadguuAy3NpJ8ud6cEyu5NgtN4yXTqkwdbjLV4UpmiKPssfifiSKnt%2F3E9frvXvhiavSzILfm0RFL9eZ8JP2a" rel="nofollow" target="_blank">《一表痛、EAST、1104 报表口径文档自动生成：解析 SQL 过滤条件，一键溯源与保鲜》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：EAST 等监管报送指标口径文档的自动生成，核心挑战在于对复杂 SQL 中过滤条件（WHERE、JOIN ON等）的精准识别与逻辑解析。传统表级或列级血缘工具无法穿透此逻辑，导致人工梳理耗时数月、口径易失效。本文探讨了如何通过算子级血缘与行级裁剪技术，实现 EAST 口径的自动化盘点与一键溯源，将盘点效率提升 20 倍，并构建主动元数据驱动的数据治理闭环。</p><p>在金融监管日益严格的背景下，EAST、1104 等监管报送已成为银行数据团队的核心工作。然而，指标口径文档的梳理却是一个公认的“效率黑洞”。传统依赖人工逐行扒代码的方式，不仅耗时数月，且难以保证口径的准确性与实时性。本文将深入剖析这一难题的技术核心——SQL 过滤条件的精准解析，并介绍如何通过算子级血缘技术实现 EAST 口径文档的自动化生成与持续保鲜。</p><h2>一、监管报送的困境：传统口径梳理的真实成本</h2><p>面对复杂的监管指标，银行数据团队普遍陷入“看不清、盘不动、保鲜难”的困境。监管指标的加工逻辑通常深藏在数百行、涉及多级嵌套和存储过程的 SQL 中。</p><p>这种传统人工模式的成本主要体现在三个维度：</p><ul><li>效率黑洞：一个 EAST 指标的口径梳理，需要数仓工程师反复沟通、逐层追溯，耗时数周甚至数月。相比之下，采用自动化手段的机构（如浙江农商联合银行）可将全盘盘点时间从数月缩短至 8 小时。</li><li>精度盲区：人工解读复杂 SQL（如嵌套子查询、存储过程）极易遗漏关键过滤条件。例如，“对公贷款余额”指标可能包含“贷款状态=正常”、“客户行业非房地产”等多个 WHERE 筛选，人工偏差会直接导致口径文档失真，埋下合规隐患。</li><li>保鲜难题：数据仓库持续演进，一旦上游 ETL 逻辑变更，静态的、人工维护的口径文档立即失效，导致文档与实际生产长期脱节。</li></ul><h2>二、技术破局关键：为何传统血缘工具无法解析 SQL 过滤条件？</h2><p>自动化生成口径文档的构想之所以难以落地，根本在于传统血缘工具的解析粒度不足。它们无法理解 SQL 中最关键的“行级数据筛选逻辑”。</p><p>真正的难点不是回答“数据来自哪个表的哪个字段”，而是回答“这个指标具体是由哪一部分数据（符合什么条件）计算出来的”。这正是 WHERE、JOIN ON 等过滤条件的价值所在。</p><p>传统工具在此存在代际差距：</p><table><thead><tr><th>解析类型</th><th>解析粒度</th><th>解析准确率</th><th>能否识别过滤条件</th><th>对复杂SQL（存储过程、嵌套）支持</th></tr></thead><tbody><tr><td>表级血缘</td><td>表级依赖</td><td>高，但噪声巨大</td><td>完全不能</td><td>有限支持，链路断裂严重</td></tr><tr><td>列级血缘</td><td>字段映射关系</td><td>通常&lt;80%</td><td>基本不能</td><td>支持差，解析率骤降</td></tr><tr><td>算子级血缘</td><td>算子级逻辑(Filter, Join, Agg 等)</td><td>\&gt;99%</td><td>精准识别 (行级裁剪)</td><td>深度支持 (DB2/Oracle 存储过程等)</td></tr></tbody></table><ul><li>表级血缘的“狼来了”效应：仅能告知数据来源表，当非相关字段变更时，会产生大量无效下游影响告警，消耗信任。</li><li>列级血缘的“半盲状态”：能追踪字段传递，但无法解析 CASE WHEN 条件分支、复杂表达式，尤其无法穿透 WHERE 子句的过滤逻辑。它无法告知“某分行存款总额”是否限定了“客户等级=A 类”。</li></ul><p>因此，要实现口径的自动化、准确化提取，必须突破列级血缘，深入到 SQL 执行的算子层面，即算子级血缘（Operator-level Lineage）。</p><h2>三、核心解法：算子级血缘与行级裁剪技术</h2><p>以 Aloudata BIG 为代表的主动元数据平台，通过深入解析 SQL 的抽象语法树（AST），实现了算子级血缘，从而将黑盒化的数据加工链白盒化。其核心能力包括：</p><ol><li>白盒化口径提取：自动穿透临时表、多层嵌套子查询以及 DB2、Oracle 等存储过程（PL/SQL），将分散在多段 SQL 中的业务逻辑，压缩合并成一段清晰、可读的“加工口径描述”，直接输出文档文本。</li><li>行级裁剪 (Row-level Pruning)：精准识别 WHERE、JOIN ON、HAVING 等子句中的过滤条件。在进行上游变更影响分析时，能智能判断变更是否真的会影响当前指标。例如，上游表“客户信息表”中“所属支行”枚举值变更，只会影响筛选条件中包含该支行的下游指标。此项技术能将不必要的评估分支减少 80% 以上，实现精准影响分析。</li><li>可视化逐层下钻：提供从报表指标反推至源系统的完整可视化血缘图谱，可点击任意节点查看具体加工 SQL、字段映射及关键过滤条件，便于复核、审计与问题定位。</li></ol><p><img width="723" height="230" referrerpolicy="no-referrer" src="/img/bVdnNwy" alt="" title=""/></p><h2>四、实践验证：银行如何将 EAST 盘点效率提升 20 倍？</h2><p>头部金融机构的实践已验证，基于算子级血缘的自动化口径管理能带来显著业务回报：</p><ul><li>浙江农商联合银行：解决了 DB2 存储过程血缘解析的行业难题。通过部署相关技术，实现了监管指标溯源人效提升 20 倍，EAST 等指标的全盘盘点周期从数月缩短至 8 小时内完成，对 DB2 存储过程的解析准确率达 99%。</li></ul><p>这些案例证明，自动化口径管理是实现 “指标溯源、血缘分析、线上化管理” 的核心技术基石。</p><h2>五、实施路径：从试点到全行推广</h2><p>建议金融机构采用“由点及面、价值驱动”的策略，构建主动元数据能力：</p><ol><li>场景试点，验证价值：选取 1-2 个逻辑复杂的 EAST 报表模块（如“大额风险暴露”）试点，重点验证算子级血缘解析准确率与自动化生成口径的可用性。</li><li>流程嵌入，形成闭环：将自动化口径与现有报送流程、DataOps 研发流程融合。实现 SQL 变更的事前影响评估（风险防控）和故障的分钟级根因定位。</li><li>体系推广，构建基座：将能力扩展至 1104、一表通等体系，并应用于数仓模型治理、敏感数据管控等场景，最终构建企业级 DataOps 体系。</li></ol><h2>六、常见问题 (FAQ)</h2><h4>Q1: 算子级血缘和列级血缘主要区别是什么？对 EAST 报送具体有何帮助？</h4><p>算子级血缘深入 SQL 执行计划，能精准解析 WHERE 过滤、JOIN 条件、聚合分组等具体操作逻辑；列级血缘只追踪字段映射关系，无法理解数据筛选逻辑。对于 EAST 报送，算子级血缘能自动回答“指标是基于哪部分数据（如‘贷款状态=正常’）计算的”，从而生成准确口径文档，而列级血缘只能给出字段列表，仍需大量人工解读。</p><h4>Q2: 我们的 SQL 非常复杂，包含大量存储过程和嵌套查询，能准确解析吗？</h4><p>可以。以 Aloudata BIG 为例，其核心技术优势之一就是覆盖复杂场景，特别对 DB2、Oracle、GaussDB 等的存储过程（PL/SQL）进行了深度适配，解析准确率超过 99%。无论是动态 SQL、临时表还是多层嵌套，都能实现穿透解析。</p><h4>Q3: 自动生成的口径文档，如何保证其持续“保鲜”，跟上代码的变更？</h4><p>主动元数据平台的血缘关系通过主动解析代码、日志等方式实时或准实时更新。当上游代码变更时，平台能自动重新解析并通知责任人。基于此生成的口径文档是“活”的、与代码逻辑实时同步的视图，解决了传统文档“一发布即过时”的难题。</p><h2>核心要点总结</h2><ol><li>核心难点：EAST 口径自动化生成的最大技术障碍在于对 SQL 中行级过滤条件（WHERE 等）的精准解析。</li><li>技术代差：算子级血缘（Operator-level Lineage） 通过解析 SQL 执行算子，实现了 &gt;99% 的解析准确率与行级裁剪能力，是破局关键。</li><li>核心价值：能够自动穿透复杂逻辑（如存储过程），一键生成可读口径，并将监管指标盘点效率提升 20 倍，实现口径实时保鲜。</li><li>演进路径：从痛点场景试点出发，将自动化能力嵌入 DataOps 流程，最终构建覆盖全链路的主动元数据基座。</li></ol><p>再次提醒：本文更详细的图表与案例细节，请访问 Aloudata 官方技术博客阅读原文：<a href="https://link.segmentfault.com/?enc=%2FMDU66zb0XyqMQiXp4GnOg%3D%3D.MTDMVaB44gcb03ytGbK7fQZiZqI03b08hdrxCoOjHXp5seV87sQ5Sk%2BFfG9wKLskTm8VSKIndxn79GIIGnkG4XgCwu0XWU8qQPg%2FQisfpmOk0kfm57Rgfm09Q4eRmZXl" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/east-document-generation-s...</a></p>]]></description></item><item>    <title><![CDATA[自定义报表如何帮助您查看您的项目的进步？ 英勇无比的羽毛球 ]]></title>    <link>https://segmentfault.com/a/1190000047578583</link>    <guid>https://segmentfault.com/a/1190000047578583</guid>    <pubDate>2026-01-28 18:06:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在当今快节奏且竞争激烈的商业环境中，高效的项目管理对于按时、按预算、高质量地交付项目至关重要。项目管理工具在组织任务、跟踪进度和促进协作方面发挥着关键作用。在众多功能中，报告功能尤为重要，因为它能将原始项目数据转化为有意义的洞察，从而支持明智的决策。</p><ul><li>增强可见性和透明度</li><li>支持更佳决策</li><li>改进资源管理</li><li>跟踪绩效和生产力</li><li>促进与利益相关者的沟通和报告</li></ul><p>在现代组织中，项目会产生大量与任务、时间表、成本、资源和绩效相关的数据。虽然标准报告可以提供有用的摘要，但每个组织和项目都有其独特的需求。项目管理工具中的可定制报告功能正是在此发挥了极其重要的作用。它允许用户根据自身特定目标设计报告，从而使项目数据更具相关性、可操作性和影响力。</p><ul><li>满足不同利益相关者的需求<br/>不同的利益相关者需要不同类型的信息。高管可能需要高层次的进度和预算摘要，项目经理可能需要详细的任务和风险报告，而团队成员则可能专注于各自的工作。可定制报告使每个利益相关者都能只查看对他们而言重要的数据，从而提高清晰度并减少信息过载。</li><li>利用相关数据改进决策<br/>可定制报告允许用户选择特定参数，例如日期范围、项目阶段、任务状态、优先级和团队成员。通过以有意义的方式筛选和组织数据，管理者可以识别趋势、及早发现问题并做出明智的决策。相关数据有助于更快、更准确地解决问题。</li><li>增强项目控制和监控<br/>每个项目都有其独特的成功指标。借助可定制的报告，管理人员可以根据项目特定的关键绩效指标 (KPI) 跟踪进度。无论是成本偏差、进度绩效还是质量指标，都可以定制报告，以精确监控定义该项目成功的要素。</li></ul><p>Zoho Projects 支持自定义模块帮助您按照您的需求创建和导出报表。Zoho Projects在全局层和项目层有自定义报表。在该报表中， 用户可以设置报表的图表配置，可以确定报表中希望添加的条件和采取希望查看的数据。您可以添加新的报表，可以克隆或者编辑报表或者如果您希望分组报表可以在不同的文件夹中保存报表。Zoho Projects为任务，项目，里程碑，问题，工时表等所有的模块支持自定义报表。</p><p>Zoho Projects报表中用户还可以创建自定义视图和按照设置的视图，可以查看和导出报表。比如说，项目经理希望查看已经批准的所有的工时。他可以创建一个自定义视图，视图中添加一个条件，“审批状态是一批准“。 添加条件以后，可以选择在视图中希望显示的列并点击保存。创建该视图以后，如果项目经理在报表模块中选择创建的视图，他可以按照创建的视图查看报表。然后他还可以添加其他的条件和导出报表。这样的自定义报表选项让一个项目管理软件成更加灵活。</p>]]></description></item><item>    <title><![CDATA[阿里云 Serverless 计算 12 月产品动态 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578589</link>    <guid>https://segmentfault.com/a/1190000047578589</guid>    <pubDate>2026-01-28 18:05:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>精选文章：</strong></p><p><a href="https://link.segmentfault.com/?enc=kV5zZii8PJsb9%2BSOuJSpYQ%3D%3D.sQKHv%2B6fGzZKf9svIG2Ib5aaGHTND3Ann33tRr%2BGinBm9Jr2Yog7xYKpX06FwdIvf3Qt3iDM2dNOqrUWsVdrK%2FvRTz3CmMjZTi5hpYbhRcYiO1k8M9f%2FcfpPOpWqN9K0Frg7S3eK5gIzirb45SKjaxvGmEBTA5IiT06oeUanjRVD0gkWkTbT4u5jFFUI3HZF" rel="nofollow" target="_blank">AgentScope 拥抱函数计算 FC，为 Agent 应用提供 Serverless 运行底座</a></p><p><a href="https://link.segmentfault.com/?enc=amQJnX%2FuGRNikFgzqIUpPA%3D%3D.VOQsr69Hb83FiagUYWejqsEmDBVCYGwWgaRYmUH5b44revkm%2Bsb2VD6cMIOSEY%2BBDISAV9v6ZryrVcwQHjfcToaGR%2BOqJkmPLtVHDNiqFRALb4Y%2Fd%2BOW8D5FW96GDXYX2N54TOfPMptlRTqO%2FO8CRKPsl1skixESXKIR%2B9wBX5E6GBrmlBicQNX8TspY2BDm" rel="nofollow" target="_blank">一杯咖啡成本搞定多模态微调：FC DevPod + Llama-Factory 极速实战</a></p><p><a href="https://link.segmentfault.com/?enc=OwPwFQu%2BwvpULKx2GO%2FmDA%3D%3D.pTwPqw4ELuMGtJrNHG3m4n6RerPGN%2BMTJ7iLL4khLotjWCKWpihAbm5rNeAREoNpgozcJsc%2FDBHltdgDks%2BVjERtDlo6fLsiZYN9zNlOG8ABVJZSlDr4qBXyrSbu36sNQKZDXTWdw22F%2FxswcoBkcEmWrZ4QFkiasWTd8OHiUL1aij5Gl5NnlWP1jASlhul3" rel="nofollow" target="_blank">一文看懂函数计算 AgentRun，让 Agentic AI 加速进入企业生产环境</a></p><p><a href="https://link.segmentfault.com/?enc=HW%2B5Ygn7tVJ9l5KnOa79hA%3D%3D.mzy6URNR1chb4cFIzIc71eFPZlox%2Bhw3J%2FQA7pkFXPupX0E3QeguFFDrWk5Ldo6j61PakeMXIQZ0Vusc0Qz%2BfJU6p7blX6dfq6RSW74LzOD%2F2aOF5ZXq0OLZlM2Itl8MKe4oQcq04g98XIPLvQBRJLMlo9RCMMnEJLav2TseOxFjwLHGocsW8XxYek0WMZim" rel="nofollow" target="_blank">AgentRun Sandbox SDK 正式开源！集成 LangChain 等主流框架，一键开启智能体沙箱新体验</a></p><p><a href="https://link.segmentfault.com/?enc=fHzHO3mdPZ609fqOOeVMMQ%3D%3D.4pwUu51xb%2B6F1u%2Bd8Z0FZ0ffR4cOGVzKfgMcxhdmqOKk4qo2JxSY99yn%2FXBeKJopLxUOyy4Ii2clYDdCpvxTV%2BWeG0QTbclcylVB6D5FXQOQDooJOItn0J3T0mDw7B1RkjbFxjAUNqmZm7pi1QBxCMY4EdTC97aUg%2BWpLxG7Q3Tf2uW3KwQF%2BicBUaWwtZv6" rel="nofollow" target="_blank">探秘 AgentRun丨通过无代码创建的 Agent，如何用高代码进行更新？</a></p><p><a href="https://link.segmentfault.com/?enc=q8SBLIz5Qr3Hutbenl1IXQ%3D%3D.s%2BWcAoJx4YLZwwQFgbH11gzuCH2adUtwlZaFgnBIn4Aid4AqyRvQKH3P5aw42GXx9NdB24E9QP3wTsgUCczwAtddeM2U9KuWf7D9Fk6%2Fo18y13BZBmki0PDKnBRq5tHoSwH1Cj3flaFPR0pMINdcvHXqTwJWmI%2FX0GRjck2rlCs94hYH6QEKJWx3pkMR6paM" rel="nofollow" target="_blank">AgentRun 实战：快速构建 AI 舆情实时分析专家</a></p><h2>产品最新消息</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578591" alt="image" title="image"/></p>]]></description></item><item>    <title><![CDATA[布局海外市场，电子签章怎么选？一篇讲清国内外侧重点与跨境互认现状 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047578717</link>    <guid>https://segmentfault.com/a/1190000047578717</guid>    <pubDate>2026-01-28 18:04:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着中国在国际中的地位越来越高，国内企业也发展得越来越全面，很多国内企业都将目光瞄向了国外市场，尤其是近年来的电子签市场，北京安证通、E签宝、法大大等国内优秀电子签章企业纷纷走出国门，开始布局海外市场，但是国内和海外有关电子签章的侧重点和应用场景都不尽相同，我们还得正确看待。那我们拥有海外分公司并且有海外业务的各企业在选择相关海外版电子签章产品时，要了解哪些情况呢？我们简单的来看看。</p><ol><li>法律基础与合规要求</li></ol><p>1) 国内电子签章</p><p>Ø 核心法律：《中华人民共和国电子签名法》（2005年实施，2020年修订），规定“可靠电子签名”与手写签名或盖章具有同等法律效力。</p><p>Ø 重点要求：强调“实名认证”和“技术可控”，要求通过权威第三方认证机构（CA）颁发数字证书，并采用符合国密标准的加密技术。</p><p>Ø 行业规范：金融、政务、司法等领域有专门规定（如《证券法》对电子合同的要求）。</p><p>2) 海外电子签章</p><p>区域性法规：</p><p>Ø 欧盟：eIDAS法规（2014年）将电子签名分为简单签名（SES）、高级签名（AES）、合格签名（QES），其中QES在欧盟内跨境通用，法律效力最强。</p><p>Ø 美国：ESIGN法案（2000年）和UETA法案承认电子签名的普遍合法性，但各州可能存在细节差异。</p><p>Ø 国际兼容性：更注重跨境互认（如eIDAS的QES在成员国间自动认可），部分国家接受云签名或生物特征签名。</p><p>Ø 灵活性：普通场景（如商务邮件）可能无需严格CA认证，但高价值合同需强化验证。</p><ol start="2"><li>技术标准与安全性</li></ol><p>1) 国内</p><p>Ø 强制国密算法：要求使用SM2/SM3/SM4等国密算法，证书需由国内CA机构颁发。</p><p>Ø 本地化部署：政务、国企场景通常要求服务器部署在国内，支持私有化部署。</p><p>Ø 身份认证：需对接公安部、工商系统或运营商实名认证。</p><p>2) 海外</p><p>Ø 国际通用标准：普遍支持RSA、ECDSA等国际算法，兼容PKI体系。</p><p>Ø 技术中立性：部分法域（如美国）不限定具体技术，更注重“意图签署”和“过程可追溯”。</p><p>Ø 云签名普及：SaaS模式（如DocuSign、Adobe Sign）广泛采用，支持跨平台协作。</p><ol start="3"><li>应用场景侧重</li></ol><p>1) 国内</p><p>Ø 政务与国企主导：广泛应用于电子政务、政府采购、银行开户、房地产交易等强监管场景。</p><p>Ø 行业渗透深：司法存证、医疗病历、电子发票等与国家级平台（如法院区块链、税务系统）对接。</p><p>Ø B2B为主：企业间合同签署普及率高，个人使用逐步上升（如租房、借款合同）。</p><p>2) 海外</p><p>Ø 市场化驱动：企业自发应用较多，尤其在跨境贸易、人力资源、房地产等领域。</p><p>Ø C端场景广泛：个人日常签约（如保险、网购协议）接受度高。</p><p>Ø 创新场景：区块链签名、生物识别签名（如Apple ID签名）在部分国家被认可。</p><ol start="4"><li>监管与司法实践</li></ol><p>1) 国内</p><p>Ø 强监管模式：工信部、密码局、公安部等多部门监管，CA机构需持牌运营。</p><p>Ø 司法存证配套：电子证据规则明确（如《最高人民法院在线诉讼规则》），要求与时间戳、区块链存证结合。</p><p>2) 海外</p><p>Ø 自律与司法并存：美国等普通法国家依赖判例积累，欧盟通过eIDAS建立统一框架。</p><p>Ø 争议解决机制：服务商常提供审计日志、身份验证报告作为证据，部分国家允许电子公证。</p><ol start="5"><li>跨境互认挑战</li></ol><p>1) 国内：跨境互认尚在探索，中国企业出海时需适应当地规则（如eIDAS的QES）。</p><p>2) 海外：欧盟、新加坡等通过双边协议推动互认，但全球统一标准仍未形成</p>]]></description></item><item>    <title><![CDATA[给显卡按下“暂停键”：阿里云函数计算 GPU “浅休眠”背后的硬核技术 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578738</link>    <guid>https://segmentfault.com/a/1190000047578738</guid>    <pubDate>2026-01-28 18:04:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：王骜</p><p>在 AGI（通用人工智能）爆发的今天，AI 应用如雨后春笋般涌现。对于开发者而言，这既是最好的时代，也是最“贵”的时代。</p><p>部署 LLM（大语言模型）、Stable Diffusion 等 AI 应用时，我们往往面临一个两难的选择：</p><ul><li><strong>要速度（预留模式）</strong>：为了毫秒级 - 秒级的响应，必须长期通过预留模式持有 GPU 实例，但昂贵的空置成本让人心痛。</li><li><strong>要省钱（按量模式）</strong>：为了节省成本选择按量付费，但 GPU 实例的创建和模型加载带来的漫长“冷启动”延迟，又严重伤害用户体验。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578740" alt="image" title="image"/></p><p><strong>难道性能与成本真的不可兼得？</strong></p><p>阿里云函数计算（Function Compute）推出的 <strong>CPU 和 GPU 实例浅休眠功能</strong>，正是为了打破这一僵局而来。它让实例学会了“浅休眠”，在保留热启动能力的同时，<strong>极大降低了实例的闲置成本</strong>。</p><p>本文将带你深入技术后台，揭秘 GPU 实例浅休眠这一功能是如何从 0 到 1 实现的。</p><h2>什么是 GPU 实例浅休眠？给显卡按下“暂停键”</h2><p>在开启浅休眠功能后，当没有请求时，GPU 实例并不会被销毁，而是进入一种 <strong>“休眠”</strong> 状态。</p><p>此时，实例依然存在，但 CPU 和 GPU 的计算资源被挂起，用户只需支付极低的休眠费用（约为活跃实例费用的 10%-20%，CPU 不计费，具体见计费文档：<a href="https://link.segmentfault.com/?enc=MG6xjWSDH8aA3mzZWQgGBw%3D%3D.TXmypO3v2POqDBT2j7DDtFfTWgLcbXtxAJHbvoiV31UF98oqzKa3oZIxlo076VrJo0tLaLjUpXuCNGQE39n%2FX1EJqN6V8h%2FXFIqVpEYI8R1wj4q7s9WO2ygdN0xv5qekqQI%2BnuBp9FJrfp2PdQRJtjzeZHwTIG6%2Fl5Q4jygj6Ug%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/functioncompute/fc/product-overvie...</a>）</p><p>当请求再次到来时，系统会瞬间“解冻”实例，毫秒-秒级恢复计算能力（视模型大小）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578741" alt="image" title="image" loading="lazy"/></p><h2>技术揭秘：如何实现 GPU 的“浅休眠”？</h2><p>在容器技术中，实现 CPU 的暂停（Pause）相对成熟且容易，但要给正在显存中跑着几个 G 大模型的 GPU 做暂停，技术挑战极大。我们通过三项关键技术，实现了对 GPU 资源的精细化管理。</p><h3>1. 显存状态的“迁移”</h3><p>传统释放 GPU 资源的方式意味着销毁实例，下次使用必须经历完整的冷启动（启动容器、加载模型）。为了解决这个问题，我们设计并实现了显存数据的<strong>迁移（Migration）机制</strong>：</p><ul><li><strong>休眠阶段</strong>：当实例空闲时，系统会将 GPU 显存中的所有数据（包括模型参数、中间状态等）完整迁移至外部存储保存。</li><li><strong>唤醒阶段</strong>：当新请求到达时，系统会迅速将存储中的数据回迁至 GPU 显存并重建状态，将实例恢复至休眠前的状态。</li></ul><p>这一过程避免了重复的模型加载，确保实例始终处于待命状态。</p><h3>2. 驱动层的透明兼容</h3><p>为了让用户无需修改代码即可使用该功能，我们选择在底层进行技术突破。</p><p>FC GPU 实例做到了<strong>对框架无感</strong>。这意味着，无论是 PyTorch 还是 TensorFlow，现有的 AI 应用无需任何代码改造，即可直接具备浅休眠能力。</p><h3>3. 基于请求的自动化调度</h3><p>有了“浅休眠”能力后，还需要解决“何时休眠、何时唤醒”的调度问题。依托函数计算<strong>以请求为中心</strong>的架构优势，我们实现了全自动化的资源管控。</p><p>平台天然感知每个请求的生命周期：</p><ul><li><strong>请求到达</strong>：系统自动触发解冻流程，毫秒级唤醒 GPU 执行任务。</li><li><strong>请求结束</strong>：系统自动触发冻结流程，释放 GPU 算力。</li></ul><p>整个过程由平台自动托管，用户无需配置复杂的伸缩策略，即可实现资源的按需分配与极致利用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578742" alt="image" title="image" loading="lazy"/></p><h2>浅休眠唤醒性能</h2><p>性能是用户最关心的指标。我们以 <strong>ComfyUI + Flux</strong> 的文生图场景为例进行了实测：</p><p>GPU 实例从“浅休眠”唤醒的耗时仅约为 <strong>500 毫秒 - 2 秒</strong>（视模型大小不同而略有差异）。</p><p>考虑到整个文生图生成过程通常持续数十秒，这 1-2 秒的延迟对于用户体验的影响极为有限，不足以降低用户感知的流畅性，却能换来显著的成本下降。</p><h2>真实案例：某 OCR 业务降本 70% 实录</h2><p>深圳某科技公司主要业务是从专利文本中提取信息，使用 OCR 模型。他们的业务痛点非常典型：</p><p><strong>1. 启动耗时长</strong>：容器启动+加载模型+私有数据 OCR 识图，全套下来要<strong>十几秒</strong>。</p><p><strong>2. 流量难以预测</strong>：请求来去无法预判，“按量模式”的冷启动耗时长无法满足业务延迟需求。如果使用预留实例，大部分时间 GPU 都在空转出现了浪费。</p><p>开启 GPU 实例浅休眠后：</p><ul><li>启动延迟明显减少，请求到达后能快速响应。</li><li>日常使用成本大幅下降。</li><li>服务稳定性不受影响，用户体验保持良好。</li></ul><p>整体成本节省接近 70%。</p><h2>如何使用</h2><p>开启方式非常简单，<strong>函数计算产品控制台（<a href="https://link.segmentfault.com/?enc=bOBJrsCMS8fGM7u9tZ7LHw%3D%3D.AEWDE5l%2FSYAZ93XS5o8OnvC9GGRsUdrN7RzqVnN3HA%2F7spNCeFqFbN7r2EABL0%2FL" rel="nofollow" target="_blank">https://fcnext.console.aliyun.com/overview</a>）</strong> 已默认支持该功能：</p><ol><li>进入函数的【弹性配置】页签。</li><li>设置【弹性实例】的数量。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578743" alt="image" title="image" loading="lazy"/></p><ol start="3"><li>系统将自动激活 GPU 实例的浅休眠功能。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578744" alt="image" title="image" loading="lazy"/></p><p><strong>计费逻辑</strong>：</p><ul><li><strong>请求执行时</strong>：全额收费。</li><li><strong>无请求执行时</strong>：自动切换至浅休眠计费（GPU 资源视卡型收取 10%-20% 的费用，<strong>CPU 不收费</strong>）。</li></ul><h2>结语：Serverless AI 的新范式</h2><p>Serverless 的核心理念是“按需付费”，而 GPU 昂贵的持有成本一直是阻碍 AI 全面 Serverless 化的大山。</p><p><strong>函数计算 CPU 和 GPU 实例均全面支持浅休眠能力</strong>。无论是高算力的 AI 推理（GPU），还是通用的计算任务（CPU），函数计算全系实例均致力助您在 Serverless 的道路上实现极致的降本增效。</p><p><strong>想要降本？现在就是最好的时机。</strong></p><p><strong>了解更多：</strong></p><p><strong>FunctionAI</strong> 是阿里云推出的一站式 <strong>AI 原生应用开发平台</strong>，基于<strong>函数计算 FC</strong> 的 Serverless 架构，深度融合 AI 技术，为企业提供从模型训练、推理到部署的全生命周期支持。</p><p>通过 Serverless 架构的弹性特性与智能化资源管理，显著降低 AI 应用的开发复杂度与资源成本，助力企业快速实现 AI 落地。</p><ol><li><strong>开发效率提升</strong>：无需关注底层资源，开发者可专注于业务逻辑，模型一键转换为 Serverless API。</li><li><strong>弹性资源调度</strong>：按需付费 + N 分之一卡资源分配（如 1/16 卡），GPU 部署成本降低 90% 以上。</li><li><strong>免运维特性</strong>：实例闲置时自动缩容至 0，资源利用率优化 60%，实现业务运维转型。</li></ol><p>快速体验 <strong>FunctionAI：</strong> <strong><a href="https://link.segmentfault.com/?enc=n2gR3X0NWUkyMwE0T2d5ew%3D%3D.i6uTPZexmnFMeGKhUQKZJHuloZZJem9ijcb7rYFkKhc3GcWQ0xg72ONPavgGjATK" rel="nofollow" target="_blank">https://cap.console.aliyun.com/explore</a></strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578745" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Claude Code的完美平替：OpenCode + GitHub Copilot 程序猿DD ]]></title>    <link>https://segmentfault.com/a/1190000047578786</link>    <guid>https://segmentfault.com/a/1190000047578786</guid>    <pubDate>2026-01-28 18:03:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：Claude 虽好，但你真的能用上吗？</h2><p>在当前席卷全球的“Vibe Coding”浪潮中，Anthropic 推出的 Claude 系列模型 + 终端工具 Claude Code，凭借极强的逻辑推理能力，成为了开发者眼中的“白月光”。但现实是残酷的：对于中国开发者而言，账号随时被封、海外信用卡支付遭拒、API 额度受限以及复杂的网络环境，构成了一道难以逾越的门槛。</p><p>虽然最近国产编程模型不断发力，Claude Code + GLM-4.7的表现非常出色，但面对复杂问题，Claude系列模型依然完胜。难道我们只能眼馋Claude全家桶的编程体验吗？</p><p>作为一名追求极致生产力的开发者，我发现了一个绝佳的完美替代方案：OpenCode + GitHub Copilot。这个组合不仅能让你享受如 GLM-4.7 一样的性价比，还能更方便的使用 Claude 的顶级模型。</p><h2>Claude Code 的开源免费平替：OpenCode</h2><p>想要复刻 Claude Code 的体验，核心在于拥有一个强大的“AI 编程代理（Coding Agent）”。OpenCode 正是目前社区中最接近、甚至在某些维度超越 Claude Code 的工具。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578788" alt="OpenCode" title="OpenCode"/></p><p>OpenCode 的强大源于其深厚的社区根基，其核心数据足以证明其统治力：</p><ul><li>高社区认可度： 在 GitHub 上拥有超过 90,000 Stars、由 600 多名贡献者共同维护，并积累了超过 7,500 次 commits。</li><li>庞大的用户基数： 每月有超过 150 万名开发者活跃在该工具链中。</li><li>全场景覆盖： 不同于仅限终端的工具，OpenCode 提供了 Terminal、IDE 插件以及支持 macOS/Windows/Linux 的 Desktop（桌面端）应用。</li></ul><p>更硬核的是，OpenCode 秉持隐私优先理念，不存储任何代码或上下文数据，且能自动加载适配 LLM 的 LSP（语言服务协议）。这种自动化的环境感知，让它在执行复杂重构任务时，比手动配置的工具更具“专家感”。</p><h2>隐藏的顶级模型分发器：GitHub Copilot</h2><p>很多开发者忽略了一个事实：GitHub Copilot 已不再仅仅是一个补全插件，它正进化为一个聚合顶级模型的低成本分发平台。</p><p>通过 OpenCode 提供的 “Log in with GitHub” 桥接功能，开发者可以直接调用 Copilot 账户下的模型能力。这意味着你不需要折腾海外信用卡去充值 Anthropic API，只需一个 GitHub 账号，就能实现对顶级模型的“一键接入”。</p><p>在 Copilot 计划中，你可以访问到的顶级模型矩阵（严格遵循来源名称）包括：</p><ul><li>Anthropic 系列： Claude Sonnet 4.5/4、Claude Opus 4.1/4.5、Claude Haiku 4.5。</li><li>OpenAI 系列： GPT-5、GPT-5 mini、GPT-5.1/5.2、GPT-4.1 以及多款 GPT-5-Codex 预览版。</li><li>Google 系列： Gemini 2.5 Pro、Gemini 3 Pro/Flash (Preview)。</li><li>新锐力量： xAI Grok Code Fast 1 以及 Raptor mini (Preview)。</li></ul><h2>性价比之王：每月 $10 搞定顶级模型访问</h2><p>这套方案最具杀伤力的地方在于其经济逻辑。直接订阅 ChatGPT Plus 或直接使用 Claude API 的成本极高，而 GitHub Copilot Pro 的定价策略对独立开发者极其友好。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578789" alt="GitHub Copilot" title="GitHub Copilot" loading="lazy"/></p><ul><li>月费仅需 $10： 即可享受针对 GPT-5 mini 的无限量聊天与 Agent 模式请求，以及无限量的代码补全。</li><li>高级请求配额： Copilot Pro 计划每月提供 300 次 Premium requests（高级请求），用于调用 Claude Opus 4.5 或 GPT-5 等昂贵的旗舰模型；如果你是重度用户，升级至 Pro+ 计划，该配额将飙升至 1500 次。</li><li>完全免费通道： 针对经认证的学生、教师及流行开源项目维护者，上述所有能力均为 $0/月。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578790" alt="GitHub Copilot" title="GitHub Copilot" loading="lazy"/></p><p>这种“小钱办大事”的模式，完美解决了“复杂场景下 Opus 模型最有效”但“直接接入成本高”的矛盾。</p><h2>总结</h2><p>OpenCode + GitHub Copilot 的组合比起其他平替方案而言，从工具、模型、价格等多个维度都是最优选择。所以，强烈推荐尝试一下这个编码组合。下面是这两个工具的官方地址，想要试试的可以直接前往：</p><ul><li>OpenCode：<a href="https://link.segmentfault.com/?enc=0Z9IgZFlb79TXOKBJ8Ou7g%3D%3D.xy2c1stcZughsP2P0EonuidVenOiLC50S%2F%2F%2FzeYYNcY%3D" rel="nofollow" target="_blank">https://opencode.ai/</a></li><li>GitHub Copilot：<a href="https://link.segmentfault.com/?enc=VkBSFdD4aNthuN0PqXpt4A%3D%3D.7o4e69XtaOcp%2FcMvm29j9dC1%2Fyr5nExC0t%2Fdet1sWKzHod%2F8fYM6CyQkIM58aw9%2F" rel="nofollow" target="_blank">https://github.com/features/copilot</a></li></ul><p>最后，做个小调研，目前你正在用什么工具和模型套件呢？留言区聊一聊吧。更多关于Vibe Coding的内容分享也可以关注<a href="https://link.segmentfault.com/?enc=9V2E4o530Ocoe9TJEF1YLQ%3D%3D.2YhFyZ05MlfoR7Q2ZxYg3V6uTzKQD1MRlJjLZpOE2Lk%3D" rel="nofollow" target="_blank">我的博客: 程序猿DD</a>。</p>]]></description></item><item>    <title><![CDATA[根治监管报送“对不准”：从列级血缘到算子级血缘的数据治理新范式 Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047578811</link>    <guid>https://segmentfault.com/a/1190000047578811</guid>    <pubDate>2026-01-28 18:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=JOsdOLuOfezPrqUhTqatbQ%3D%3D.9H2i2RNPZIWHAVXzvFNsw0BdV32qo8iMZUqMW5rUsVWTqxN7%2F9nnsxlujvNMKJRIDzlB1qc8evar05nOoIAodO%2F4h1Hjof31g5zuhCygbgp%2FwQzfts3jhvUxipucB1Diqm6KU4f1rF0CT0nQEPoi1Q%3D%3D" rel="nofollow" target="_blank">《列级血缘为何在 EAST 报送中“对不准”？算子级解析的降维打击》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：在金融监管报送（如 EAST）场景中，传统列级血缘因 SQL 解析精度低（&lt;80%）、无法处理复杂逻辑，导致指标口径追溯不全、人工盘点耗时数月。本文深入剖析了列级血缘的技术局限，并介绍了以算子级血缘为核心的新范式。通过 AST 深度解析、行级裁剪和白盒化口径提取等技术，算子级血缘将解析准确率提升至 &gt;99%，实现监管指标“一键溯源”与自动化盘点，为数据治理和 DataOps 流程提供精准的溯源基座。</p><p>在金融监管报送（如 EAST、1104）领域，数据血缘的准确性直接关系到合规风险与运营效率。传统列级血缘技术因解析精度不足，已成为指标口径“对不准”、人工盘点“盘不动”的症结所在。本文将对比分析列级血缘的固有缺陷，并深入解读以算子级血缘（Operator-level Lineage） 为核心的技术新范式，如何通过 &gt;99% 的解析准确率与行级裁剪能力，为监管报送构建可靠的自动化数据溯源基座。</p><h2>一、核心痛点：EAST 报送中的数据溯源困局</h2><p>金融监管指标背后是跨越数仓多层（ODS、明细层、汇总层、报表层）的复杂加工链路，涉及大量 SQL 转换、存储过程及临时表处理。传统数据血缘（表级/列级）在此场景下普遍失效，具体表现为：</p><ol><li>盘点效率低下：面对成千上万的监管指标，数据团队需投入数周至数月进行人工“扒代码”和访谈，成本高昂。</li><li>追溯结果不可靠：行业反馈显示，开源列级血缘工具对 Hive SQL 的解析准确率通常低于 70%，近三分之一的依赖关系错误或缺失，为合规埋下隐患。</li><li>变更风险失控：无法精准评估上游字段或逻辑变更对下游报送指标的影响，导致“牵一发而动全身”，易引发数据错误或报送延误。</li></ol><p><img width="723" height="230" referrerpolicy="no-referrer" src="/img/bVdnNAj" alt="" title=""/></p><h2>二、技术剖析：列级血缘为何“力不从心”？</h2><p>列级血缘的局限源于其技术原理，它通常基于正则匹配或浅层语法分析，只能识别“A 表的 X 列出现在 B 表 Y 列的 SELECT 语句中”，但无法理解其间的计算逻辑。这导致三大硬伤：</p><ul><li>解析精度天花板低：对包含 <code>CASE WHEN</code>、窗口函数、多层嵌套子查询的复杂 SQL 解析能力弱，准确率普遍低于 80%。</li><li>无法穿透黑盒逻辑：对 DB2、Oracle 的 PL/SQL 存储过程、动态 SQL、临时表加工等场景几乎无法解析，造成血缘链路断点。</li><li>影响分析过度泛化：缺乏对 <code>WHERE</code>、<code>JOIN ON</code> 等过滤条件的识别。例如，一个仅影响特定分行的源数据变更，会触发所有相关下游任务的告警，噪音率可超过 80%。</li></ul><table><thead><tr><th>对比维度</th><th>传统列级血缘</th><th>算子级血缘 (如 Aloudata BIG)</th></tr></thead><tbody><tr><td>解析粒度</td><td>列级，仅知“从哪列到哪列”</td><td>算子级，可知“经过怎样的计算（过滤、连接、聚合）从哪列到哪列”</td></tr><tr><td>解析准确率</td><td>通常 &lt; 80%，复杂 SQL 下更低</td><td>&gt; 99%，基于 AST 深度解析</td></tr><tr><td>复杂场景支持</td><td>弱，难以处理存储过程、动态 SQL、临时表</td><td>强，深度支持 DB2、GaussDB 等 PL/SQL，穿透临时表</td></tr><tr><td>影响分析精度</td><td>粗粒度，易泛化，噪音大</td><td>行级裁剪，精准识别过滤条件，聚焦真实影响范围</td></tr><tr><td>口径提取</td><td>需人工拼接多层代码</td><td>白盒化口径提取，自动生成可读、可验证的最终加工逻辑</td></tr></tbody></table><h2>三、新范式：算子级血缘的核心原理与“降维打击”</h2><p>算子级血缘实现了技术范式的跃迁。它深入 SQL 内部，将数据加工过程解析为最细粒度的算子（Operator）序列，如 <code>Filter</code>（过滤）、<code>Join</code>（连接）、<code>Aggregation</code>（聚合）等。结合以下核心技术，实现对传统方法的“降维打击”：</p><ol><li>行级裁剪 (Row-level Pruning)：精准识别 SQL 中的过滤条件（<code>WHERE</code>, <code>JOIN ON</code>）。当上游数据变更时，系统能自动判断变更是否落入下游任务所关心的数据子集内，从而剔除无关的上游分支，使影响评估范围平均降低 80% 以上，实现精准风险预警。</li><li>复杂场景全覆盖：基于对多 SQL 方言（Hive, Spark, Oracle, DB2 等）及 PL/SQL 的深度解析能力，可穿透存储过程、动态 SQL、临时表等传统黑盒，构建端到端的完整血缘链路。</li><li>白盒化口径提取：针对跨多层加工的监管指标，系统能自动将沿途的所有 <code>SELECT</code>、<code>CASE WHEN</code>、函数调用等逻辑，“压缩”成一段从最终指标反向追溯到源字段的、可读性极高的“加工口径”，直接替代人工“扒代码”。</li></ol><h2>四、实践验证：算子级血缘在金融场景的落地成效</h2><p>该技术已在多家金融机构的 EAST 报送场景中得到验证：</p><p>浙江农商联合银行：通过部署具备算子级血缘能力的 Aloudata BIG 平台，实现了监管指标溯源人效提升 20 倍，全量指标口径盘点从数月缩短至 8 小时；对核心 DB2 存储过程的解析准确率达到 99%，攻克技术难关；自动生成符合监管要求的指标加工口径报告。</p><p>共性价值：算子级血缘实现的“一键溯源”能力，不仅大幅提升合规效率，更将管理动作从事后补救转向事前防控与事中协同，精准管控上游变更对下游报送指标的影响。</p><h2>五、实施路径：构建 EAST 报送的数据溯源基座</h2><p>企业可遵循以下三步，系统性构建高可靠的数据溯源能力：</p><p>1、基座先行：优先接入核心数仓（Hive, Oracle）、ETL/ELT 平台（DataStage, Kettle）及 BI 系统，快速构建覆盖“入仓-&gt;加工-&gt;服务”全链路的算子级血缘图谱。</p><p>2、场景驱动：选择 EAST、1104 等具体监管报表作为首场景，利用“一键溯源”快速验证价值，赢得业务与合规部门支持。</p><p>3、流程嵌入：将血缘能力深度嵌入 DataOps 与合规流程：</p><ul><li>研发侧：代码提交前自动进行变更影响分析，识别波及的报送指标。</li><li>运维侧：发生数据异常时，利用血缘图谱快速定位根因。</li><li>合规侧：建立基于血缘的自动化口径报告与审计机制。</li></ul><h2>六、常见问题（FAQ）</h2><h4>Q1: 列级血缘和算子级血缘的核心区别是什么？</h4><p>最本质的区别是解析粒度。列级血缘仅知道字段的流向，而算子级血缘能还原完整的计算逻辑，例如“A.X 列经过 WHERE 过滤后，与 C 表 Z 列 LEFT JOIN，再 GROUP BY 生成 B.Y 列”，实现加工过程的白盒化。</p><h4>Q2: 对复杂的存储过程和嵌套查询，算子级血缘解析效果如何？</h4><p>这是算子级血缘的核心优势。它针对 DB2、Oracle 等 PL/SQL 存储过程、动态 SQL 及多层嵌套查询进行了深度优化，解析准确率可超过 99%，能有效穿透这些传统血缘工具的解析盲区。</p><h4>Q3: 引入算子级血缘对 EAST 报送的具体价值是什么？</h4><p>主要体现在三方面：效率提升（盘点从数月缩短到几小时）、准确性保障（&gt;99% 解析准确率确保口径完整正确）、风险防控（精准评估上游变更影响，实现主动预警）。</p><h2>核心要点</h2><ol><li>精度是核心：传统列级血缘低解析精度（&lt;80%）是 EAST 报送“对不准”的根源。</li><li>算子级是解药：算子级血缘通过 AST 深度解析 Filter、Join 等算子，实现 &gt;99% 的解析准确率。</li><li>行级裁剪提效：行级裁剪技术能精准识别数据子集，将变更影响分析范围平均降低 80% 以上。</li><li>案例验证价值：在标杆案例中，算子级血缘已将监管指标盘点从数月缩短至 8 小时，人效提升 20 倍。</li><li>构建溯源基座：企业应优先建设全链路算子级血缘，并以此驱动 DataOps 与自动化合规流程。</li></ol><p>再次提醒：本文更详细的图表与案例细节，请访问Aloudata官方技术博客阅读原文：<a href="https://link.segmentfault.com/?enc=tiGxQw%2FWwCB3mxUkpmHhjA%3D%3D.N3Tj7Bdf5f%2BWe07F8Al2og3OHfvXaz98Op7C4%2FTYGIBkz10rOQP1ELwMbDKF3Hvgt8ZmEYTiul9T9t9No2n2cpBQQLI1r4i%2BFctis0J%2BBSuW6akpCcHt0ogr9Yazcx57pcM7T5J4qWLO9iyi2YS1MQ%3D%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/why-column-level-lineage-m...</a></p>]]></description></item><item>    <title><![CDATA[从“工具过载”到“精准调用”：破解 Agent 工具管理难题 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578844</link>    <guid>https://segmentfault.com/a/1190000047578844</guid>    <pubDate>2026-01-28 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：青瑭、聪言</p><h2>背景与挑战</h2><h3>行业背景：Agent 工具生态迈向规模化</h3><p>随着 AI Agent 在企业场景中的深度应用，开发者普遍为 Agent 配置大量工具——从天气查询、地图导航，到数据库接口、内部 API 等，以支撑复杂任务的执行。然而，当工具数量从几十个激增至上百甚至上千时，传统的“全量暴露”模式便难以为继：Agent 不仅要处理冗长的工具列表，还容易选错工具、响应变慢、调用成本飙升。如何让 Agent 在海量工具中快速、准确地选出真正需要的那几个，既决定了任务能否顺利完成，也直接影响系统的运行成本与响应效率。</p><p>AgentScope Java 框架作为面向生产级智能体的开源开发框架，致力于为 Java 开发者提供高内聚、低耦合、可扩展的 Agent 构建能力。面对日益膨胀的工具库，我们期望不再把所有工具一股脑塞给 Agent，而是按需、精准、安全地动态供给——这才是大规模 Agent 落地的关键所在。</p><h3>企业级 Agent 工具管理的核心挑战</h3><p>尽管 Agent 开发框架 AgentScope Java 提供了灵活的工具集成机制，但在真实生产环境中，工具规模扩大反而带来“越强越笨”的悖论。主要体现在以下六大维度：</p><ul><li><strong>Prompt 膨胀，上下文资源被严重挤占</strong>：每个工具需在 Prompt 中声明名称、描述与参数 Schema。工具越多，输入越长，迅速耗尽 LLM 的上下文窗口，限制任务复杂度。</li><li><strong>推理成本不可控</strong>：冗长 Prompt 直接推高 Token 消耗，在高频调用或大规模部署场景下，LLM 调用费用呈指数级增长。</li><li><strong>工具选择准确率下降</strong>：面对功能相近或无关的工具列表，大模型易混淆误判，导致调用错误、任务失败或结果偏差。</li><li><strong>响应延迟增加</strong>：处理超长上下文显著延长 LLM 推理时间，拖慢端到端响应速度，损害用户体验。</li><li><strong>维护复杂度飙升</strong>：开发者需手动筛选“哪些工具对哪个任务可见”，难以实现动态、按需的工具分配，系统可扩展性受限。</li><li><strong>安全与稳定性风险加剧</strong>：无关甚至敏感工具若被误选执行，可能触发无效调用、数据污染，甚至引发安全漏洞。</li></ul><h3>破局之道：构建语义驱动的智能工具精选体系</h3><p>要真正释放大规模工具库的价值，必须摒弃“全量推送”的粗放模式，转向一种以任务语义为中心、按需披露的现代化工具供给范式。</p><p>为此，AgentScope 深度集成 Higress AI Gateway，推出 Higress 扩展插件——基于语义化工具检索，在运行时动态为 Agent 注入与其当前意图最匹配的工具子集，实现精准供给、轻量推理与安全隔离。</p><p>这一机制本质上是一种面向智能体的渐进式能力披露：Agent 仅在需要时“看见”相关能力，既遵循最小权限原则，又显著降低上下文开销与决策噪声，从而全面提升系统的可扩展性、可观测性与鲁棒性。</p><h2>AgentScope Java Higress 扩展：智能工具精选</h2><h3>核心价值</h3><p>Higress 源自阿里巴巴内部，是一款开源的云原生 API 网关， 将流量网关、微服务网关、安全网关三合一。在 AI 时代，Higress 演进为 AI 原生网关的技术底座，将 LLM 调用、SSE 流式响应、Agent 工具交互等 AI 工作负载视为一等公民。阿里云基于 Higress 推出了商业化 AI 网关，提供 99.99% 高可用保障，已稳定支撑通义千问、百炼、PAI 等阿里内部 AI 业务，并服务零一万物、FastGPT 等头部 AIGC 企业。</p><p>AI 网关推出 MCP 语义检索功能，通过自然语言理解用户意图，动态返回最相关的工具子集，实现精准供给、降本增效、安全可控。核心能力包括：</p><ul><li>统一入口管理：所有 Agent 通过单一端点访问全部 MCP 工具，简化接入，集中治理。</li><li>智能语义匹配：基于 Qwen 大模型与 AnalyticDB 向量数据库，Agent 仅需描述需求（如“查北京天气和附近餐厅”），即可自动匹配最相关工具。</li><li>双阶段高精度检索：先通过 Qwen Embedding 向量召回候选工具，再可选使用 Qwen Rerank 模型精排，显著提升推荐准确性。</li><li>实时元数据同步：MCP Server 的增删改操作自动触发工具元信息采集与向量化更新，确保检索结果与实际服务状态一致。</li><li>一键开通，零配置上手：在控制台启用语义检索后，系统自动完成向量库初始化、模型配置、路由下发等全流程，即开即用。</li></ul><h3>性能表现</h3><p>该语义检索功能使用 Weight 混合算法，与其他算法性能对比如下：</p><p><strong>1）准确性：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578846" alt="image" title="image"/></p><p><strong>2）时间延迟：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578847" alt="image" title="image" loading="lazy"/></p><p>根据准确性和时间延迟的性能比较，Weight 算法在准确度上微幅领先并且搜索时间控制在 350 毫秒以内，相比纯向量搜索仅增加约 30 毫秒延迟，满足实时检索需求。</p><h3>AgentScope Java Higress扩展</h3><p>因此，AgentScope Java 推出了 Higress 扩展，深度集成 Higress AI Gateway 的语义检索能力，覆盖 Agent 从工具发现、筛选、加载到调用的完整生命周期，全面支撑低成本、高精度、高效率的 Agent 运行。该插件提供以下能力：</p><ul><li>语义驱动的工具精选：用户可以告别硬编码工具列表，基于用户自然语言描述动态检索最相关工具。</li><li>无缝集成 MCP 客户端：提供标准化、响应式的 Java 客户端，零侵入兼容现有 AgentScope 生态。</li><li>企业级可观测与安全：依托阿里云 AI Gateway，提供认证鉴权的安全能力。</li></ul><h2>快速开始</h2><h3>前提条件</h3><ol><li>创建包年包月或按量付费的阿里云 AI Gateway 实例：<a href="https://link.segmentfault.com/?enc=A9a2sVUrel1mKCSMhRALBQ%3D%3D.cUPU9y0BCY47sVKrddoQJSn2kHRbJ7jhU1plcn4%2BjdcQqG7X1Jsr5C08p2IwFQ3FWSC8YZjCZOvx5QVn04fmZyLECwwfVuur%2FaPqpAbTzm4%3D" rel="nofollow" target="_blank">https://common-buy.aliyun.com/?commodityCode=apigateway_aipos...</a></li><li>在 AI Gateway 中注册 MCP 工具服务：<a href="https://link.segmentfault.com/?enc=w8ejj1ghvWMlladYUIt0ew%3D%3D.5YbY%2BmKkntoa1g5uyegJkJdcRtmWsIpBUDC1ClERm1JzRwUyopuo%2BNL6SRj078Tmp%2B%2BrIKylkxwCPrxeiTJtagE12nqzbui8FRxfS2mopVYiJEC9xg8550%2FuHSgkZqFB" rel="nofollow" target="_blank">https://help.aliyun.com/zh/api-gateway/ai-gateway/user-guide/...</a></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578848" alt="image" title="image" loading="lazy"/></p><ol start="3"><li>在 MCP 管理 &gt; 语义检索页签中启用语义检索功能  </li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578849" alt="image" title="image" loading="lazy"/></p><ol start="4"><li>（可选）配置消费者认证，提升安全性</li></ol><h3>使用 Higress 插件为 Agentscope Java Agent 添加工具</h3><h4>1. 添加依赖</h4><pre><code>&lt;dependency&gt;
    &lt;groupId&gt;io.agentscope&lt;/groupId&gt;
    &lt;artifactId&gt;agentscope-extensions-higress&lt;/artifactId&gt;
    &lt;version&gt;${agentscope.version}&lt;/version&gt;
&lt;/dependency&gt;</code></pre><h4>2. 启用语义工具搜索</h4><p>通过使用 toolsearch 方法，您可以指定召回的与描述最相关的 topK 个工具，以供 Agent 调用。</p><pre><code>// 构建带语义搜索的客户端
HigressMcpClientWrapper higressClient =
                HigressMcpClientBuilder.create("higress")
                        .streamableHttpEndpoint(HIGRESS_ENDPOINT)
                        // .sseEndpoint(HIGRESS_ENDPOINT + "/sse")  // Alternative: SSE transport
                        // .header("Authorization", "Bearer xxx")   // Optional: Add auth header
                        // .queryParam("queryKey", "queryValue")   // Optional: Add query param
                        .toolSearch("your agent description", 5) // Optional: Enable tool search
                        .buildAsync()
                        .block();
// 2. Register with HigressToolkit
Toolkit toolkit = new HigressToolkit();
toolkit.registerMcpClient(higressClient).block();
// 创建 Agent
ReActAgent agent =
                ReActAgent.builder()
                        .name("HigressAgent")
                        .sysPrompt(
                                "You are a helpful assistant. Please answer questions concisely and"
                                        + " accurately.")
                        .model(
                                DashScopeChatModel.builder()
                                        .apiKey(apiKey)
                                        .modelName("qwen-max")
                                        .stream(true)
                                        .enableThinking(false)
                                        .formatter(new DashScopeChatFormatter())
                                        .build())
                        .toolkit(toolkit)
                        .memory(new InMemoryMemory())
                        .build();</code></pre><p>完整示例见 agentscope-examples/HigressToolExample.java：<a href="https://link.segmentfault.com/?enc=7y9EvwRC8Nlu9BKA2tTnnw%3D%3D.zgI0Hx%2Fq6YVgvjgHNi9N7dbMe5FFHngns%2FFHiC98JcsarK%2F%2BiwjQt8apx5oKY3v4pN0QVGDkzAMklcWts%2FvFUVQUxOTeMeilQnTPlGTqF%2BGI4qUyIFPiHtquSLKkhHId%2Fv%2BZpWTHMqS2qFcCIt%2BiSVEEyvC5kJ29VFnnaYlbsh7kpjXKNoQAzvZAVr9gx3TyXSfNzfdYgs7bbj%2FJywzHJm6%2Bbo6iX41gXacgNMDtVsc%3D" rel="nofollow" target="_blank">https://github.com/agentscope-ai/agentscope-java/blob/main/agentscope-examples/quickstart/src/main/java/io/agentscope/examples/quickstart/HigressToolExample.java</a></p><h2>加入我们，共建 AgentScope Java、Higress 生态</h2><p>AgentScope Java 与 Higress 都是开放的开源项目，我们诚邀所有对 Agent 与 AI网关感兴趣的开发者参与共建！</p><ul><li>GitHub：<a href="https://link.segmentfault.com/?enc=q%2B3t8aLqFJAF%2FW0TPwRPsQ%3D%3D.jiKWTDjYsUWCV2Ryv%2F3dX4QE1rZBtPfp0A%2F1wy2Oc6Nq0O98V7Ip170HvSZn2eXth4l%2BRew57G9t2HksbSwwZQ%3D%3D" rel="nofollow" target="_blank">https://github.com/agentscope-ai/agentscope-java</a></li><li>Github：<a href="https://link.segmentfault.com/?enc=2gjNh1nJ1KoSLf%2FmmQAH4Q%3D%3D.JgZSUlnIavyMnP%2FWqSxg%2Bx%2FlZNfYp9S8ZjjQxbj9qS22G%2Ffz51VBl6bLCfCpcABT" rel="nofollow" target="_blank">https://github.com/alibaba/higress</a></li></ul>]]></description></item><item>    <title><![CDATA[智能体对传统行业的冲击：客服真的难逃被智能体取代的命运？ 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047578270</link>    <guid>https://segmentfault.com/a/1190000047578270</guid>    <pubDate>2026-01-28 17:08:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><h3>一、智能体席卷客服领域：替代浪潮下的行业现状</h3><p>1.1 智能体客服的技术突破与落地速度1.2 多行业智能体客服的替代数据与实践1.3 传统客服岗位的生存现状与结构变化</p><h3>二、不可替代的核心价值：智能体难以突破的客服壁垒</h3><p>2.1 情感共鸣：复杂情绪场景的人类同理心优势2.2 灵活决策：非标准化复杂问题的人工处理能力2.3 信任建立：高价值业务中的人工沟通独特性</p><h3>三、人机共生：客服行业的终极转型方向</h3><p>3.1 智能体与人工客服的职责边界划分3.2 人机协同的客服服务模式落地路径3.3 传统客服的职业转型与能力重塑</p><h3>四、行业未来：智能体赋能下的客服行业新生态</h3><p>4.1 智能体驱动的效率升级与成本优化4.2 客服岗位的价值重构与新兴职业机会4.3 客服体系的智能化改造趋势</p><h3>五、结语</h3><h3>六、FAQ</h3><h2>摘要</h2><p>在智能体技术快速落地的背景下，客服行业成为传统行业中受冲击最直接的领域之一。智能体凭借 7×24 小时服务、高标准化问题处理效率、低成本运营等优势，在政务、文旅、电商、物流等多行业规模化应用，替代了超 50% 的常规客服工作，传统客服岗位的生存挑战被持续放大。本文基于智能体客服的实际落地数据与案例，剖析其对客服行业的替代现状，深入探讨智能体在情感处理、复杂决策等场景中难以突破的能力壁垒，明确人工客服不可替代的核心价值，提出 “智能体处理标准化工作 + 人工客服承接高价值场景” 的人机共生模式，为传统客服从业者的职业转型提供能力重塑方向，最终揭示智能体并非传统客服的 “替代者”，而是行业的 “赋能者”，其将推动客服行业实现效率与体验的双重升级，重构行业新生态。</p><h2>一、智能体席卷客服领域：替代浪潮下的行业现状</h2><p>智能体技术与大语言模型、自然语言处理的融合应用，让智能体客服从 “机械应答” 升级为 “智能理解”，迅速席卷各行业客服领域，成为企业降本增效的核心选择，传统客服行业迎来前所未有的替代浪潮。</p><p>智能体客服的落地与适配能力远超预期，从需求确认到系统上线最短仅需 12 天，可实现多语种、多方言识别，还能与企业现有呼叫中心、CRM 系统无缝对接，快速适配文旅旺季海量咨询、物流全场景服务、电商日常答疑等不同需求，形成标准化服务能力。</p><p>从替代数据来看，智能体客服在多行业的独立处理率已达 51%-60%，携程、同程等平台的自助解决率更是突破 75%，日均处理咨询量超百万次。恩施文旅落地智能体客服后，旺季人工坐席需求从 30 人降至 12 人，人力成本节省 60%；物流行业智能体客服实现 “1 个顶 10 个传统客服”，24 小时承接询价、查单等全流程服务。IDC 预测，2026 年超 70% 的企业将部署 AI 语音交互系统替代传统 IVR 服务，Gartner 也指出，2025 年 AI 将处理 80% 的常规客户服务互动。</p><p>替代浪潮直接引发传统客服岗位的结构变化，基础标准化客服岗位大幅缩减，人工坐席需求向 “少而精” 转变，从业者面临岗位淘汰与职业转型的双重压力。同时，企业客服体系的运营逻辑从 “人工为主、工具为辅” 转向 “智能体为主、人工兜底”，客服中心的人力配置、管理模式均随之调整，行业进入深度重构阶段。</p><h2>二、不可替代的核心价值：智能体难以突破的客服壁垒</h2><p>尽管智能体客服在常规工作中表现亮眼，但从行业实践来看，其并非万能，在客服核心服务场景中仍存在难以突破的能力壁垒，这正是人工客服不可替代的关键，也决定了客服岗位不会被完全取代。</p><p>情感共鸣是人工客服最核心的优势。智能体虽能通过技术实现情绪识别，却无法真正实现情感共鸣与同理心表达。在投诉处理、情绪安抚、售后纠纷调解等场景中，客户的核心需求不仅是解决问题，更是情绪的释放与被理解。人工客服能通过语气、措辞的灵活调整精准捕捉情绪痛点，进行共情式沟通；而智能体的安抚话术基于算法预设，难以应对个性化情绪表达，无法建立真正的情感连接。</p><p>灵活决策能力是智能体的重要短板。其仅能处理知识库内的标准化问题，面对非标准化、跨场景的复杂问题，缺乏自主判断与灵活解决的能力。高端客户定制化服务、跨部门业务协调、突发非预案问题处理等高价值场景，需要客服人员结合企业实际、客户需求与行业经验做出灵活应对，这是依托预设算法与知识库的智能体无法实现的，超出范围的问题只能转接人工。</p><p>在高价值业务场景中，人工沟通是建立客户信任的关键，这一价值无法被智能体替代。房产、汽车、高端医美等客单价高、决策周期长的行业，客户不仅需要获取信息，更需要通过深度沟通建立对企业的信任。人工客服能通过专业讲解、及时答疑、个性化建议打消客户顾虑，推动决策落地；而智能体的标准化回复难以传递人格化特征，无法根据客户反应调整沟通策略，在信任建立环节存在天然劣势。</p><h2>三、人机共生：客服行业的终极转型方向</h2><p>面对智能体的冲击，客服行业的终极发展方向并非 “智能体替代人工”，而是 “人机共生、各取所长”。通过明确职责边界、构建高效的人机协同模式，既能发挥智能体的效率优势，又能保留人工客服的价值优势，实现服务效率与体验的双重升级。</p><p>明确职责边界是人机共生的基础，核心遵循 “智能体优先处理标准化工作，人工客服承接高价值场景” 原则。将客户咨询分为三个层级：标准化问题（产品查询、订单核对、流程指引等）由智能体全程独立处理，占比可达 60%-80%；中等复杂问题（简单售后、常规投诉登记）由智能体初步处理后转交人工跟进；高复杂问题（复杂投诉、定制化服务、激烈情绪沟通等）直接由人工承接，同时智能体为人工提供数据支持、对话上下文等辅助信息，提升处理效率。</p><p>人机协同服务模式的落地，核心在于 “智能分流、无缝转接、数据赋能”。智能体通过意图识别、情绪识别技术实现智能分流，将问题精准分配至对应处理主体；无法处理时实现无感知人工转接，保留完整对话上下文，避免客户重复表述；同时通过大数据分析，为人工客服提供客户画像、历史咨询记录、业务数据等信息，助力精准把握客户需求，实现个性化服务。</p><p>人机共生模式下，传统客服的核心转型方向是 “能力重塑”，从 “标准化操作型” 向 “高价值服务型” 转变。从业者需要提升三大核心能力：一是情感服务能力，强化共情能力与沟通技巧，专注复杂情绪场景处理与客户关系维护；二是专业解决能力，深入学习企业业务知识，提升复杂问题分析、跨部门协调的能力，成为领域专业客服；三是数据应用能力，学会运用智能体的数据分析报告，把握客户需求趋势，为服务优化、业务决策提供建议。同时，企业需建立常态化培训体系，帮助传统客服完成能力升级，适应新岗位要求。</p><h2>四、行业未来：智能体赋能下的客服行业新生态</h2><p>智能体对客服行业的冲击，本质上是技术推动的行业升级，其并非传统客服的 “淘汰者”，而是推动行业向更高效、更专业、更高价值方向发展的 “赋能者”。未来，在智能体赋能下，客服行业将形成全新生态，实现效率、价值与职业的多重变革。</p><p>智能体将持续推动客服行业的效率升级与成本优化，成为企业客服体系的基础配置。随着大模型、多模态、自主学习技术的发展，智能体客服的处理能力将持续提升，覆盖更多标准化场景并向部分中等复杂场景延伸，进一步提升体系运行效率。其 7×24 小时服务、低运营成本的优势，将帮助企业打破时间与空间限制，实现客服服务全域覆盖，大幅降低人力与管理成本。数据显示，智能体客服的单次服务成本仅为人工的 1/10，投资回报周期最短仅 8 个月，成为企业客服数字化转型的必选。</p><p>客服岗位的价值将被重新定义，从 “成本中心” 向 “价值中心” 转变，同时催生大量新兴职业机会。传统客服行业被视为企业成本中心，核心价值是解决问题；而在智能体赋能下，人工客服从繁琐的标准化工作中解放，专注高价值服务场景，成为客户关系维护、品牌形象塑造、业务转化的重要力量，可通过深度沟通挖掘客户潜在需求，实现交叉销售与增值服务，创造直接商业价值。同时，智能体的落地运营，催生了 AI 客服训练师、智能体运营专员、客服数据分析师等新兴职业，这类职业要求从业者兼具客服业务知识与 AI 技术能力，成为行业新增长点。</p><p>全行业的客服体系将迎来全面智能化改造，形成 “智能体 + 人工客服” 深度融合的标准化服务体系，行业服务标准与评价体系也将随之重构。各行业将结合自身业务特点，搭建定制化智能体客服系统，实现与企业业务、客户管理系统的深度融合，形成全链路智能化服务；客服行业的评价标准将从 “响应速度、问题解决率” 等单一效率指标，向 “客户满意度、情感体验、价值创造” 等多维指标转变，更注重服务的温度与价值。此外，行业将建立智能体客服的技术标准、运营规范，推动客服行业规范化、标准化发展，最大化发挥智能体的赋能价值。</p><h2>五、结语</h2><p>智能体的快速发展让客服行业迎来前所未有的变革，也让 “客服是否会被智能体完全取代” 成为行业热议话题。但从行业实践与技术发展来看，智能体虽能替代传统客服的标准化工作，却无法复制人工客服的同理心、灵活决策能力与信任建立能力，这决定了客服行业不会走向 “全智能体化”，而是形成 “人机共生” 的全新格局。</p><p>对于客服行业而言，智能体的冲击并非危机，而是行业升级的契机，推动传统客服摆脱 “人力密集、效率低下、价值单一” 的发展困境，向 “智能驱动、人机协同、价值导向” 的新生态转型；对于传统客服从业者而言，这并非职业终点，而是职业升级的起点，唯有通过能力重塑，从标准化操作转向高价值服务，才能在行业变革中站稳脚跟。</p><p>未来，客服行业的核心竞争力将在于 “智能体的效率 + 人工的温度”，唯有实现技术与人性的深度融合，才能让客服服务既高效又有温度，既降本又能创造价值。而智能体与人工客服的共生共赢，也将成为智能时代传统行业转型升级的典型样本，为其他行业的智能化改造提供重要参考与借鉴。</p><h2>六、FAQ</h2><h3>1. 智能体客服目前的独立处理率能达到多少？</h3><p>在政务、文旅、交通、电商等行业，智能体客服独立处理率已达 51%-60%，携程、同程等平台自助解决率突破 75%，Gartner 预测 2025 年 AI 将处理 80% 的常规客户服务互动。</p><h3>2. 智能体客服无法处理哪些客服场景？</h3><p>主要难以处理三类场景：需要情感共鸣的复杂情绪场景（如激烈投诉、情绪安抚）、非标准化的复杂决策场景（如高端定制服务、跨部门协调）、需要深度建立信任的高价值业务场景（如房产、汽车等客单价高的行业咨询）。</p><h3>3. 人机共生模式下，传统客服该如何转型？</h3><p>核心是从 “标准化操作型” 向 “高价值服务型” 转变，重点提升三大能力：情感服务能力（共情、沟通技巧）、专业解决能力（复杂问题分析、跨部门协调）、数据应用能力（运用数据分析把握客户需求），同时依托企业的常态化培训体系完成能力升级。</p><h3>4. 智能体客服能为企业带来哪些实际价值？</h3><p>核心体现在降本、提效、提质三方面：人工成本可节省 40%-60%，单次服务成本仅为人工的 1/10；客户响应速度提升数倍，高峰时段并发接待能力提升 10 倍以上；24 小时服务覆盖提升客户满意度，投诉率可下降 65% 左右。</p><h2>参考文献</h2><p>[1] 5 个行业 AI 语音客服落地案例：真实数据验证降本增效\_大模型客服前沿笔记</p><p>[2] AI 智能体颠覆传统服务业：旅行社、客服首当其冲\_CSDN 博客</p><p>[3] 客服行业会被 AI 完全替代吗？人机协作的终极形态分析\_来鼓 AI</p><p>[4] 传统物流客服即将被 AI 智能物流客服取代？\_抖音行业热点</p>]]></description></item><item>    <title><![CDATA[VMware ESXi 9.0.2.0 macOS Unlocker & OEM BIOS 2.7 ]]></title>    <link>https://segmentfault.com/a/1190000047578278</link>    <guid>https://segmentfault.com/a/1190000047578278</guid>    <pubDate>2026-01-28 17:07:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 2.7 标准版和厂商定制版</p><p>ESXi 9.0 标准版，Dell (戴尔)、HPE (慧与)、Lenovo (联想)、Inspur/IEIT SYSTEMS (浪潮)、H3C (新华三)、Cisco (思科)、Fujitsu (富士通)、Hitachi (日立)、NEC (日电)、Huawei (华为)、xFusion (超聚变) OEM 定制版</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=wfKgoKVeyRU0x%2Fth24Gxlg%3D%3D.mBmwlKqvS5u5qmyj%2Ffs%2FzpULLPhjkHlcL2UF%2FZPReYzrKEEbG5XxOByZWSfiGL3Q" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=PTyh5xKsHs9NN2kI0KU1ug%3D%3D.KY%2BT978%2FATRLdnmSulGyCearVu3GsaGSaJcn88pq2uo%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>VMware vSphere 是 VMware 的虚拟化平台，可将数据中心转换为包括 CPU、存储和网络资源的聚合计算基础架构。vSphere 将这些基础架构作为一个统一的运行环境进行管理，并为您提供工具来管理加入该环境的数据中心。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046438842" alt="说明 ESXi 主机、vCenter Server、虚拟机和 vSphere Client 之间关系的 VMware vSphere 概览图" title="说明 ESXi 主机、vCenter Server、虚拟机和 vSphere Client 之间关系的 VMware vSphere 概览图"/></p><p>vSphere 的两个核心组件是 ESXi 和 vCenter Server。ESXi 是用于创建并运行虚拟机和虚拟设备的虚拟化平台。vCenter Server 是一项服务，用于管理网络中连接的多个主机，并将主机资源池化。</p><h2>通用特性概览</h2><p>该版本在官方原版基础上新增以下特性：</p><ul><li>macOS Unlocker：来自 GitHub 的 <a href="https://link.segmentfault.com/?enc=d3vQI%2F7nWo048maH1qHwpg%3D%3D.7hdw1EifMS4Axz7CZ8gPRyzzBjtLAsM9mNz6nLJD%2FsIiyk9ILpPPOKfrY%2FutCji1" rel="nofollow" target="_blank">Unlocker 4</a>，现已支持 macOS Tahoe</li><li>OEM BIOS 2.7：使用社区最流行的 OEM BIOS/EFI64，现已支持 Windows Server 2025</li><li>LegacyCPU support，允许在不受官方支持的旧款 CPU 上安装 ESXi 9.0</li><li>ESX-OSData 卷大小修改为 8GB，解决自 ESXi 7.0 起系统占用磁盘空间过大的问题（超过 142GB）</li><li>有限支持采用混合架构的第 12 代及以上 Intel 处理器，可实现正常引导和运行</li><li>同时提供 Dell、HPE 和 Lenovo 等厂商定制版映像 (sysin)，包含了必要的驱动和 OEM 软件</li></ul><h3>直接运行 macOS Tahoe</h3><p>参看：<a href="https://link.segmentfault.com/?enc=h5%2FP3JzKPcWNzf5%2BD3DuZw%3D%3D.x%2BJ1btXZJSX8IismC%2Bp6BXr%2Bszb%2FGDNf%2FTehb5zOkvbZCQUf9PeBsw9WwWEFkfag" rel="nofollow" target="_blank">macOS 26 Blank OVF - macOS Tahoe 虚拟化解决方案</a></p><p>ESXi 默认是支持创建 macOS 虚拟机的，但该功能仅限于 Apple Mac 硬件上启用。该版本解锁了对 macOS 虚拟化的支持，在任意非 Mac 硬件上可以直接运行 macOS 虚拟机。</p><p>⚠️ macOS 虚拟机与 Mac 上的 macOS 体验有天壤之别，仅用于体验而已。开启 macOS 卓越性能的唯一平台是搭载 Apple M 芯片的 Mac。尽早加入 Apple 阵营，开启卓越体验吧。</p><p>直接新建虚拟机，操作系统选择 “Apple macOS 12 (64-bit)”，即可安装和正常启动：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046829041" alt="New VM in ESXi 9" title="New VM in ESXi 9" loading="lazy"/></p><p>虚拟化中的 macOS Tahoe：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046878483" alt="macOS Tahoe in VMware" title="macOS Tahoe in VMware" loading="lazy"/></p><p>附：</p><ul><li><a href="https://link.segmentfault.com/?enc=j5PDIP9ygHGdoJPesl%2Bc4w%3D%3D.UYOGtLwuZOpuCbVBdqI%2BZ5H0ff2O3Y17K65PyBCUJTA%2FTmciAdetiCBRCVCFGnzx" rel="nofollow" target="_blank">macOS Tahoe 26.2 (25C56) Boot ISO 原版可引导映像下载</a></li><li><a href="https://link.segmentfault.com/?enc=XfAPNR4Tdd%2B2YdcIamLByw%3D%3D.CdkfPDgAwd4Fhe71tPu3tgNJk%2BbbKZyNf%2FFKs3NedGQWqjdb1X%2FnX%2B%2FdZZKi7duu" rel="nofollow" target="_blank">macOS Sequoia 15.7.3 (24G419) Boot ISO 原版可引导映像下载</a></li><li><a href="https://link.segmentfault.com/?enc=mWquqRT5ot1EaR0TdeJsEA%3D%3D.Gt91UiL0jmyPTkSq76jo2eri89ncwwt%2FnqmxLDgSMMXS5SJuZsh1ev76w1tE0QLO" rel="nofollow" target="_blank">macOS Sonoma 14.8.3 (23J220) Boot ISO 原版可引导映像下载</a></li><li>更多：<a href="https://link.segmentfault.com/?enc=eiMvnl%2FLhV62htD3RjPn1Q%3D%3D.jbt3%2F5VSGAgL66sp4MGXhRRTKmdZHZ4IqmMLW8%2FzDXI%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></li></ul><h3>VMware Dell 2.7 BIOS EFI64 ROM</h3><p>来自社区最新的 OEM BIOS/EFI64，现已更新支持 Windows Server 2025。</p><p>BIOS.440 &amp; EFI64.ROM - Dell 2.7 OEM BIOS: NT 6.0 (Vista/Server 2008), NT 6.1 (7/Server 2008 R2), NT 6.2 (Server 2012), NT 6.3 (Server 2012 R2), NT 10.0 (Server 2016/Server 2019/Server 2022/Server 2025)</p><p>Windows Server OVF 系列：</p><ul><li><a href="https://link.segmentfault.com/?enc=CeHfaI%2BONIgW9fdYT1E4cA%3D%3D.uAW7y9OQFVha8w96A%2FuMbTDhbBbdbD%2BtPA38eemkthPaF9X%2B5xvNvQNb6taqxkPq" rel="nofollow" target="_blank">Windows Server 2025 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=9lHiV65xXcswWA0b4I88qg%3D%3D.GO6nX5ur3zU%2FESIziOW%2F7AWd8mSU861Cln7EA4N44sRjGFC4%2FrHXHt8H1sA%2BEyjz" rel="nofollow" target="_blank">Windows Server 2022 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=RU%2B1us4ISnPNbMT%2B3%2BqVvg%3D%3D.FiI4VGMACRDD15l6hsoXLQe02ivN5U3DzfUw3nmRkap75Laz5Ei62Oyk33%2F%2FiP2U" rel="nofollow" target="_blank">Windows Server 2019 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=yxy4AUf0rhwjwNWKqEQ5Mw%3D%3D.bcintbGhxHaW9IhYYwjI%2B6KEVKBLx5dtvFPpmx07heYk6tYII3OfCD2G8zQpKqpB" rel="nofollow" target="_blank">Windows Server 2016 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=s%2BsAfqKeKewQjhpLoazfrw%3D%3D.oMRXUajax5AaKE1pMfW8Mxo5j3YfjtPOJ658gIWDzR3CwXP4Jyjo7zi0lruQa27PoRizPsMdHeMmOC4gF1hd4Q%3D%3D" rel="nofollow" target="_blank">Windows Server 2008 R2 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li></ul><p>其他 OVF，如：<a href="https://link.segmentfault.com/?enc=S3IOzV84cTF1B4yxFxZ8wA%3D%3D.eKZzdsnnkHKfy0P9vtg5RCa4AQHgrTbFa6C3OjrJQDPHlQAGLWWr5ulmtw7btION" rel="nofollow" target="_blank">Rocky Linux 10 x86_64 OVF (sysin) - VMware 虚拟机模板</a>，<a href="https://link.segmentfault.com/?enc=hLXD3WUXoElGkCRid8tb%2Bw%3D%3D.qBtAmAu9Vf648VntR8fnNM8k%2FgxRGegeOlT6stFgpT2ju9o%2BXUUX7iHsK%2BgOefwQ" rel="nofollow" target="_blank">Ubuntu 24.04 LTS x86_64 OVF (sysin) - VMware 虚拟机模板</a>，更多请在本站搜索 “OVF”。</p><h3>支持不受官方支持的旧款 CPU</h3><p><strong>ESXi 9.0 同样废弃了对部分旧款 CPU 的支持</strong>，笔者根据相关文档判断以下 CPU 将不受 ESXi 9.0 支持：</p><ul><li><p>Intel</p><ul><li>Xeon D‑1500 Series</li><li>Xeon E3‑1200‑V5 / E3‑1500‑V5 Series</li><li>Xeon E5‑2600‑V4 / E5‑1600‑V4 Series</li><li>Xeon E5‑4600‑V4 Series</li><li>Xeon E7‑8800/4800‑V4 Series</li><li>Xeon E3‑1200‑V6 Series</li><li>Intel Xeon Platinum 8100 / Gold 6100/5100 / Silver 4100 / Bronze 3100 Series</li><li>Xeon D‑2100 Series</li><li>Xeon W‑2100 Series</li></ul></li><li><p>AMD</p><ul><li>Bulldozer 架构（如 Opteron 6200/4200/3200）</li><li>Piledriver 架构（如 Opteron 4300/6300 系列）</li><li>Steamroller 架构（如 Opteron X2250/X1250 Berlin）</li><li>Kyoto 架构（如 Opteron X1100/X2100）</li></ul></li></ul><p><strong>ESXi 8.0 同样废弃了对部分旧款 CPU 的支持</strong>，以下 CPU 将不受 ESXi 8.0 支持：</p><ul><li>Intel Family 6, Model = 2A (Sandy Bridge DT/EN, GA 2011)</li><li>Intel Family 6, Model = 2D (Sandy Bridge EP, GA 2012)</li><li>Intel Family 6, Model = 3A (Ivy Bridge DT/EN, GA 2012)</li><li>AMD Family 0x15, Model = 01 (Bulldozer, GA 2012)</li></ul><p>vSphere 7.0 Update 2 及更高版本中 ESX 安装程序显示的如下警告消息已经明示：<br/> CPU_SUPPORT_WARNING: The CPUs in this host may not be supported in future ESXi releases. Please plan accordingly.</p><p><strong>修改启动参数，在官方不受支持的 CPU 的服务器上可以正常安装。</strong></p><p>根据 VMware vSphere 7.0 Release Notes，以下 CPU 已经不受支持（无法安装或者升级 ESXi 7.0）</p><p>Comparing the processors supported by vSphere 6.7, vSphere 7.0 no longer supports the following processors:</p><ul><li>Intel Family 6, Model = 2C (Westmere-EP)</li><li>Intel Family 6, Model = 2F (Westmere-EX)</li></ul><p>笔者在一台 2010 年发布的服务器上安装运行良好 (sysin)：HP DL 380 G7，Intel® Xeon® CPU E5606</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044308374" alt="ESXi 7.0 on LegacyCPU" title="ESXi 7.0 on LegacyCPU" loading="lazy"/></p><p>备注：本截图为 7.0 版本</p><h3>ESX-OSData 卷大小修改为 8GB</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046829042" alt="ESXi 9 VMFSL" title="ESXi 9 VMFSL" loading="lazy"/></p><p><strong>ESXi 9.0 对存储容量的要求未有明显变更，以下 ESXi 8.0 的描述基本适用。</strong></p><p>⚠️ 在 ESXi 8.0 中建议放弃使用 USB/SD 卡作为系统存储介质（虽然 SD 卡和 USB 介质继续获得有限支持，详见 <a href="https://link.segmentfault.com/?enc=jlwe%2FBmYKHPX1BBa6aYFLw%3D%3D.URS5DfWSaqYcdf2%2FoB3ftgbv%2FVtWFkWre11Q8kdV8JQ%2FQ7nH4fEKrZuXA3ZmPz%2Fv" rel="nofollow" target="_blank">KB85685</a>）。</p><p>从 ESXi 7.0 开始，对磁盘空间的要求有所变化：</p><ul><li>8GB SD 卡 + 32GB 本地磁盘</li><li>32GB 本地磁盘</li><li>142G 或者更大的本地磁盘</li></ul><p>通常我们在一块数百 GB 或者更大的本地磁盘上安装 ESXi，系统分区磁盘空间将占用 142GB 以上，整个系统分区（内核参数：systemMediaSize）需要 138GB 和 4GB 以上的空闲空间，其中 ESX-OSData volume 大约需要 120GB 的磁盘空间，对于磁盘空间紧张情况下可能有一定的浪费 (sysin)。修改后，系统安装后占用的磁盘空间不超过 16GB（特别是针对个人实验，无需浪费过多存储容量）。</p><p>图：vSphere 7 中的新分区架构，只有系统引导分区固定为 100 MB，其余分区是动态的，这意味着分区大小将根据启动媒体大小确定。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044308376" alt="partition schema in vSphere 7" title="partition schema in vSphere 7" loading="lazy"/></p><p>从 vSphere 7.0 Update 1c 开始，您可以使用 ESXi 安装程序引导选项 <code>systemMediaSize</code> 限制启动媒体上系统存储分区的大小。如果您的系统占用空间较小，不需要最大 128 GB 的系统存储大小，您可以将其限制为最小 32 GB。<code>systemMediaSize</code> 参数接受以下值：</p><ul><li>min（32 GB，用于单磁盘或嵌入式服务器）</li><li>small（64 GB，用于至少有 512 GB RAM 的服务器）</li><li>default（128 GB）</li><li>max（消耗所有可用空间，用于多 TB 的服务器）</li></ul><blockquote>即使设置值为 min，相比之前的版本所需存储容量还是要大的多。</blockquote><h3>有限支持第 12 代及以上 Intel 处理器</h3><p>ESXi 面向数据中心虚拟化，在测试和学习时也常常将其运行于桌面 PC 之上。</p><p>据悉 ESXi 8.0 并不支持第 12 代 Intel 处理器，直接引导会出现 PSOD。本次通过加载内核参数可以有限支持第 12 代 Intel CPU，即可以正常引导和安装，也可以正常运行 (sysin)，但是无法区分或识别两种核心，P 核的超线程是无法识别的，比如 i7-12650H 配备 6P + 4E 在桌面系统中显示为 16 核心，而在 ESXi 中仅识别为 10 核。现在有了更好的解决方案，绝大多数主流品牌机和主板都可以通过配置开启 P 核的超线程（非主流请慎选）。</p><p>已经广泛验证支持第 12 代及以上 Intel 处理器（目前 13、14 代同样支持），更多案例，期待您的反馈。</p><blockquote><p>第 12 代英特尔酷睿桌面级处理器有 N 个性能核（P 核，Performance-core）和 N 个能效核（E 核，Efficient-core）组成，性能核和能效核的混合架构，是 12 代酷睿处理器最大的革新。该架构或俗称 PE 大小核。</p><p>第 12 代及以上 Intel CPU 已经成功安装 ESXi 后需要进一步配置，可联系笔者了解详情。</p></blockquote><p>⚠️：并不推荐此类 CPU，无法有效利用全部计算资源。</p><p>💡：仅标准版和集成驱动版提供此项特性，品牌服务器于此无关。</p><h3>提供标准版和厂商定制版映像</h3><p>提供标准版和 Dell、HPE、Lenovo 等定制版映像 iso 文件，定制版集成了对应厂商的驱动，建议该厂商产品优先使用。</p><ul><li>Standard (标准版)</li><li>Dell (戴尔) 定制版</li><li>HPE (慧与) 定制版</li><li>Cisco (思科) 定制版</li><li>Fujitsu (富士通) 定制版</li><li>H3C (新华三) 定制版</li><li>Hitachi (日立) 定制版</li><li>Huawei (华为) 定制版</li><li>Inspur/IEIT SYSTEMS (浪潮) 定制版</li><li>Lenovo (联想) 定制版</li><li>NEC (日电) 定制版</li><li>xFusion (超聚变) 定制版</li></ul><p>💡：各厂商定制版将逐步提供，部分厂商关注度太低，可能不再提供定制服务。</p><h2>Dell (戴尔) 服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>代表机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>Dell PowerEdge 11G Server</td><td>R210, R310, R410, R415, R510, R515, R710, R715, R810, R815, R910 T310, T610, T710</td><td>6.0-6.0U3</td><td>有限支持 8.0，推荐 6.7U3</td><td>Intel Xeon 55xx 56xx 65xx 75xx series Intel Xeon E7-28xx E7-48xx E7-88xx series (sysin) AMD Opteron 43xx,42xx,41xx series AMD Opteron 63xx,62xx,61xx series</td><td>PERC H200, PERC H700, PERC 6/i <strong>皆不受支持</strong> 此系列机型不推荐，如有需求可来询。</td><td>部分可支持，此系列机型不推荐，如有需求可来询。</td></tr><tr><td>Dell PowerEdge 12G Server</td><td>R220, R320, R420, R420xr, R520, R620, R720, R720xd, R820, R920 T20, T320, T420, T620</td><td>6.0-6.0U3, 6.5-6.5U3</td><td>有限支持 9.0，推荐 8.0</td><td>• Intel® Xeon® processor E5-2400 product family • Intel® Xeon® processor E5-2600 product family • Intel® Xeon® processor E5-4600 product family (sysin) • Intel® Xeon® processor E7-4800 v2 and E7-8800 v2 product families (up to 4)</td><td>特殊定制支持的 Internal controllers (sysin):  PERC H710 PERC H710P <strong>不支持</strong> PERC H310 (6.5 U3) <strong>不支持</strong> PERC H810 (7.0 U3)</td><td>Broadcom® 5720 quad-port 1GbE Base-T Broadcom 57840S quad-port 10GbE SFP+ Rack NDC Intel I350 quad-port 1GbE Base-T (sysin) Intel X520 dual-port 10Gb DA/SFP+ Server Adapter Intel X540 dual-port 10GbE Base-T with 2 x 1GbE (FCoE capability enabled on the 10GbE ports) 部分选配 NIC 可能不支持，具体可查询</td></tr><tr><td>Dell PowerEdge 13G Server</td><td>R230, R330, R430, R530, R530xd, R630, R730, R730xd, R830, R930 T30, T130, T330, T430, T630</td><td>6.x All, 7.0-7.0U3</td><td>8.0-9.0</td><td>• Intel Xeon processor E3-1200 v5 • Intel® Xeon® processor E5-2600 v3 v4 product family • Intel® Xeon® processor E5-4600 v3 v4 product family • Intel Xeon E7-8800 v3 v4 and E7-4800 v3 v4 processors</td><td>Internal controllers (sysin): PERC H330, PERC H730, PERC H730P External HBAs (RAID): PERC H830 <strong>不支持</strong>  PERC S130 (SW RAID)</td><td>Broadcom® 5720 quad-port 1GbE Base-T Broadcom 57840S quad-port 10GbE SFP+ Rack NDC Intel I350 quad-port 1GbE Base-T (sysin) Intel X520 dual-port 10Gb DA/SFP+ Server Adapter Intel X540 dual-port 10GbE Base-T with 2 x 1GbE (FCoE capability enabled on the 10GbE ports) 部分选配 NIC 可能不支持，具体可查询</td></tr><tr><td>Dell PowerEdge 14G Server</td><td><a href="https://link.segmentfault.com/?enc=j7mO8VzRMjV6gvqr0gOFXA%3D%3D.hP5VuXkgZ3zFtOlfkpqRaAdSLlfNeYfqfvXVALLRpzYLhnSxkYbgCw9csvouZod2PX84rOLdGfG4Qp2%2Bl0MWwrS8UnfKEhZPzxmEROaZWXvl%2FigGOHIs9bYvNg1Jkbf7" rel="nofollow" target="_blank">R240</a>, <a href="https://link.segmentfault.com/?enc=B%2FVtGphaJQiG3lwuWTWvJw%3D%3D.uTGdJeSjOJqE%2BBX4FqVrFktLCbyV68HSIM5fS8a4nZDqXtiMQbpLgGb1kiNJ2iihTPoEKBckoHFeog1fFtbfWwiVQ4SpFw1cIrt0J%2Fv1eYuiuMgyHDplOqLKOfD4TUJ8" rel="nofollow" target="_blank">R340</a>, <a href="https://link.segmentfault.com/?enc=RfV%2BNCOfxsHLs%2BdNXFaB6A%3D%3D.%2FMlZCikhBElpESYFhtFhIyQQPukF%2BiZQ93uBtYByB2MFhlnWHU2aSW2QBwXfJJJMyYrAcfTBSwVeLdd2go3HPLBsW2NrzB0bxp55FNoivJlLhhDnGbzAFMFOAj3t1zO7" rel="nofollow" target="_blank">R440</a>, <a href="https://link.segmentfault.com/?enc=SaJnPXkC6NANq0Idd3wjHA%3D%3D.tgjfddGcYAT%2FFuT7Jir5xT52Z3LusTd429kjIYQD4JV9NPHcR6wNDs%2BA2FLUHyCNAXXm8NELtIV2ynVP1N4Fn09QGoLeGun9%2F%2Bkv9Cg3xX2WSbwytikBIz2Vv8K9ybFa" rel="nofollow" target="_blank">R540</a>, <a href="https://link.segmentfault.com/?enc=gAoE56S%2Fgc6pU%2BlvB61e%2BA%3D%3D.4uSy68cU7J%2FvIr%2FEvJDYRjZ9nldYGx677XOfK41ccfwlLmB%2FmpguGwCEkACGWOAOM6%2Fvhjzc5nwjleNNuqR5D%2BeaZM4m5P7J1Ud%2F884TPgnxHxeSIZ09OmnmkXF3ngpc" rel="nofollow" target="_blank">R640</a>, <a href="https://link.segmentfault.com/?enc=L%2FeiBmpoNw4q8A2QPSuadg%3D%3D.di6ufWtDt27kFJLHQ6idplzxZEtXjesiJZjsqRekw%2BdbqQPDJ4ggCwAfxOS1ElNVoLOWPhGp%2F1kIZWMRRlFgH3sCp0O0wUhhSCESv7OeNWni0I8RQ%2FhBg71iimWw%2FwVM" rel="nofollow" target="_blank">R6415</a>, <a href="https://link.segmentfault.com/?enc=tMFt1kyYAYDOLsYpI%2BP45A%3D%3D.sEry00DEy0QJO6M9byZAgKaP1hum9UlJBh9I6RT08zBvNIWQ7IZLtg8HKRyDGn2QU133GkRY%2FFAO0lH2tnYtUC%2FzZUeuZD7DrZQpHT9q78kULTisxFiz%2FXlhVxcFJmwR" rel="nofollow" target="_blank">R740</a>, <a href="https://link.segmentfault.com/?enc=%2BReb1Rm4vWIm%2F6MYBt5y6g%3D%3D.qdq5VAS1CXtTDKv90LIRZy5LxggRriSq7Zgtb2LYGH14s%2F%2BpQdZp%2FqlkvUvelkGjtQYGTzpDXw8ozcJSVBdt8IDdQYHApVOwvOuB3DOcjJFxi1u0Km6%2BI96O3x%2BQEU3%2B" rel="nofollow" target="_blank">R740xd</a>, <a href="https://link.segmentfault.com/?enc=ZXs3IXhKt6sXmeiqC3wx2w%3D%3D.Drcf8p6tPhfwcPGLE%2Bq85p9l5Q9dtX5EqyirFcNOp9JsURCAr8ajdkxPKCT0yVMtENVoKtmZkD6nVH5AkFxPxDUWyM6P958dMhLQzXZWHn9ZR6nlbsCLM0XOBq9nU6he" rel="nofollow" target="_blank">R740xd2</a>, <a href="https://link.segmentfault.com/?enc=gFohIynXjStjlHEx%2Bpt66w%3D%3D.hYOtnsoruUv7n885DCBUPJETeqBzvUekr%2Fhks7SXLI3GvYrUmK8a3zPnt%2ByPL%2B60zWu8b4OAugf4WwN%2Fm5pR2em38Nw7APUhkvC8PJhFnVoNl5liWx6KjWjgZDU%2B3Vj6" rel="nofollow" target="_blank">R7415</a>, <a href="https://link.segmentfault.com/?enc=sofADNB7t7V7GGWOimotMQ%3D%3D.cOOPkM1uk2y9kms8tqYJ4USy5a2%2BUDanxhuqdQ5OlM%2FWnfvw6M3g9EiXgUaJaPyhTCwCxwYCxjw3l7sDf8iIU5L%2BpiZkGJffH296fW9h67E4a3y3YnfhSMfi3UR%2BQpiZ" rel="nofollow" target="_blank">R7425</a>, <a href="https://link.segmentfault.com/?enc=ZyPnI1pd8ruJYExq5o2bfQ%3D%3D.nOZe531kyJ4E6nBqUslLyFF8CaOecinbkpgta0ASbHXN7n5FRWof4RsW36oN7aGYIyGHnnhcido0v5VysUWscr1R75zN8ha%2FXODeRSB8CNT9DLXGhlMFYUEnfFbeDM10" rel="nofollow" target="_blank">R840</a>, <a href="https://link.segmentfault.com/?enc=rLYks%2FLCCz2O3tFOFG8Ywg%3D%3D.atVluD0F6QzoZQlm6UVUYjdNWoqp5mHAiVDVsH6gDqKZiGhGfnkjc0yC4UxZ30NeMRb2JZh2wNKldHQuUOm7B7cvI25%2FO7KzO2oyyFz1cM0egexUNtgVBf4LHMPsUzb6" rel="nofollow" target="_blank">R940</a>, <a href="https://link.segmentfault.com/?enc=FQFDLqogGHVbtQq13gp0HQ%3D%3D.IWKJ2N%2Bka76J3nAq%2FUjRP6TxWVBG0pw%2BsS1zACZtz9S1GBUS7Zwx8EEEMLglNj3TkAVBFm%2BouvxybkPMa9N7MIH5Qj6HpcSh2%2B3k9n2ehQcxKeskdDvyIbPzWBP7htJW" rel="nofollow" target="_blank">R940xa</a> <a href="https://link.segmentfault.com/?enc=Qtoqcfz81Bnud53%2FEwetJg%3D%3D.90gD%2BcZ1JqX3JBBTbwye%2FWAot5QSApl6ECFuMjxl0zEn7oKgXxmFCBuOgLEnuYLp2wUrVlEOdNtUVcBxVdZ4A6gqiOUQgKGPx1MyqGO6XFAfzhp9NxsmnfitYPeC6f9a" rel="nofollow" target="_blank">T40</a>, <a href="https://link.segmentfault.com/?enc=k4hgW1UwQZVDmRIm2OFQHw%3D%3D.cotAuy1PgahyRV7YoAC2PrRgDPtTPbTugRNcZSfuHpaIXlPzavFx2Ykow3D7NOrmrKnJZaTcE%2BSVJpNT5lK%2BzzfZ%2FYL1qokDAlNUtAC%2BsUfTMzQBkgAsIKxEIvRqaF5%2B" rel="nofollow" target="_blank">T140</a>, <a href="https://link.segmentfault.com/?enc=XMLIEpY%2BlhFSAsS6lgzQwg%3D%3D.nYU95MWnpItQmhXmxJVqBr4f8FCVuMQc%2BVjfkhy9MnXkBT3y20Ag2gOor8n9p62WV2pI%2FYsZtdQWum36MDYaC8%2FjnCx%2B3kV7PvfklMf4qKbYhehyXmgyQ0pVeXItJThT" rel="nofollow" target="_blank">T340</a>, <a href="https://link.segmentfault.com/?enc=xa9JNKcKjrcq1UvBrr6rLw%3D%3D.pYYVOvVS5VfPuhRfKNttSaS217gor8IImjmHygMU6jp08I6%2Ft31tpvLRJ8bpLxORitM0SGRQrT21bEoTkiJN9rN%2B5sAsUbLzCmvqY5nRlCwMKmpT7n86gBzEbmRZI8%2BC" rel="nofollow" target="_blank">T440</a>, <a href="https://link.segmentfault.com/?enc=k2qkd7JgKJM7aLfIs1zdAw%3D%3D.NS%2Ftc%2BM4TXdwGiadHtsK0spwc353nHAUTebKiledfs4ihKYRqYlCJrSrutEeW0Pmo6oIycNqc47zTJ%2FDhWSpLC%2FjgtL8ucyG97m9Ai5SfkKNvkipYdPsDMCLQ5iV5nkz" rel="nofollow" target="_blank">T640</a></td><td>6.7-6.7U3, 7.0-7.0U3, 8.0-8.0U3</td><td>9.0</td><td>• Intel Xeon Scalable processors • 2nd Generation Intel® Xeon® Scalable processors (sysin) • AMD EPYC processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 15G Server</td><td><a href="https://link.segmentfault.com/?enc=s%2B16y6XNWREi4DR2wd24aQ%3D%3D.XsylCh1sFg9IAtZYsnWu7lIRIaXv%2FOUxlK5Lw1YX5MfbO4ppp6oZkKnTiXIpO4iBbx6WrJqSM1KaPuuHGEEms5TpKt6h880LGpSXbdKhQWvlfrxdpipd%2FzsmI9MN6YD3" rel="nofollow" target="_blank">R250</a>, <a href="https://link.segmentfault.com/?enc=cL%2FHiHbal5blsRFIaGQZPw%3D%3D.binNMYd8FGqCvHKqwi4sw3qBcQVJogjPat3j7h9%2F03mbc0icZ9SdkVzrrFwu17CJzo%2FkrxBtRcthSaG7B47tXatOpa7bYa5y6Q26vl1GQDIH%2BZYEPdpKQ%2BauISpF7sAc" rel="nofollow" target="_blank">R350</a>, <a href="https://link.segmentfault.com/?enc=FtSLt1nH%2Fn37EjC%2BWE5fYg%3D%3D.%2BQ60KtHnn%2FD1mMb9UAnlj%2BFDOzmKo0yVdyffwci3L6dzThj%2BdkK8l0yA8xXMcNz1gYjPlHOD1TOBSSyiJAfSuTiLCof9%2FBZPsAXDNR6VjE85Oek7uGanTYPADqf8Zh6Q" rel="nofollow" target="_blank">R450</a>, <a href="https://link.segmentfault.com/?enc=AgdnOg%2FBK7AvuIMKsWd7JA%3D%3D.zRu3R8r%2Beui6ZzWMTbmpLd4v50VCHYNjBN1bKGGfdRwR21R6%2F0awjfUfDiCV1RbWWcvtKSAfaY9ddby2nyY9DSde3P7U90w%2FmNnN819mrw8pvxdtxxU%2Belc%2BK%2FEow7JK" rel="nofollow" target="_blank">R550</a>, <a href="https://link.segmentfault.com/?enc=1MkYSQrtAc0cvoc1j%2Fb6jw%3D%3D.vMTrITb104PGjLFqwq9OHdUgZP3kRNNqAVNSfv7MBEIvbb%2FOsKpsjaU89a3eBBdSSHPc8eogg3OxNtbtSVHStjNfpItetJ5fUsBQ%2Bv9bhvsRdYr8w1t65bfXz5AD2alJ" rel="nofollow" target="_blank">R650</a>, <a href="https://link.segmentfault.com/?enc=5UFV%2F5ZwLnkZ18WfNZHI4w%3D%3D.Lb3TCfF%2FQol5XuD6CljBZgaxzGBjw5wUfVydLeNjJSvYNTkU5xsdvz1N9FPCOFpn4mK0ku7WREYALACSzDIMyVuS9a4lbHgt5BMsZIdiKLOUlIxKjndbIZHExsyf8Uuj" rel="nofollow" target="_blank">R650xs</a>, <a href="https://link.segmentfault.com/?enc=eKUjCR7l1mb5%2B3uFDoZyWw%3D%3D.0CIVfTo88zMDVlQUFYUiVeS9mD8MmQGqOCZdgTiDmOXYC2bSDfEtgAkDnWhf6we4bX2tugoVNdmNDpgoCcETGEkU5c%2ByZt4epm%2B0uVr008RqUno507LYOfkOE5vx9hFN" rel="nofollow" target="_blank">R6515</a>, <a href="https://link.segmentfault.com/?enc=tvn4r0dgwOZK02Ehx8j5Kw%3D%3D.K%2BWrSLAcENctOG2DfgS06R0Z16Ia0Cl9MXaGluhhqc51t%2BkUWZO21zworuPXWKmMM1hPBrtuSePgBPzeBCmLiy7oRnBx2BxxftNinb7hct5yQp%2FWzq45bAjtgdHlGLRQ" rel="nofollow" target="_blank">R6525</a>, <a href="https://link.segmentfault.com/?enc=Zb5Tqbi67q5mPbGmJhBLMQ%3D%3D.f%2FRACfAnx6uMbUe388mx4k4%2BofsMviLYKNRgxPtUoYHgyaAJtXvAsJEVUdvtmcEgjdGVxPn8%2Fqwrmb4if5AdeFdKXLqZ%2F%2BZHEf8Z5DJuAqNuEqSQ%2BDhSoQN64fO8t8Nf" rel="nofollow" target="_blank">R750</a>, <a href="https://link.segmentfault.com/?enc=SXXwaozhJNc%2BQ2DAJAIlUQ%3D%3D.Aq1qwnkDkgBH9L6sWUERtifehN24NPbUOV6zZP5WFXSCBuWW6DZQX9jVDI5uqE2sA3ZfYwlBZv1iGxyHi9QI3H9aC47j6WXsH30GzUvTeJOAquv9B3rFKKCKXUzLG7VR" rel="nofollow" target="_blank">R750xa</a>, <a href="https://link.segmentfault.com/?enc=qNmEoXSOW0DqIu%2FY%2FaaXzQ%3D%3D.syx1d6D5wWYgTN%2F12LGi8TQkXwTUn4%2F%2Bguykj10uY25UTq3YqCFUyDW8V11ukeVx9h5SoEMco2IL%2FPLkD6Y%2BP3GPpbg2r3IVu%2B5pWOTjuqpkEj%2Biavi3mPcJvjcinP5F" rel="nofollow" target="_blank">R750xs</a>, <a href="https://link.segmentfault.com/?enc=6hEKRW5HPc5qVpVPF8OnSg%3D%3D.oiJLZlUhNMjlLQp2vuIDSI%2FTXWqC%2BdkpIjD9sOQhE6EZQrGDtOdz7lK3BWT5H15d%2FDNFkyeyorvo5w5k8cYGxlSsMA4zF9ukS%2Fds%2FffB6aYI%2BDwiE28JzYd%2FitqqTCoo" rel="nofollow" target="_blank">R7515</a>, <a href="https://link.segmentfault.com/?enc=87BHlkg2FCRINT119f6KUg%3D%3D.8hXef%2B0WvxcUdtsdSWYLwg8rD%2ByDd%2FwNfiixPojtvGD%2BkKCOXDEOLsfR1K7nDGA09j36mCQih5DGm8i%2B89WtaI5wafvLlmzwkg9VMBCP0RG%2FihDdxomBaYTNILtr79qm" rel="nofollow" target="_blank">R7525</a> <a href="https://link.segmentfault.com/?enc=vq9THD4mFoVZrtPbZdwsKg%3D%3D.2rJ13BZa0zzy%2BBczwgGD3hiPwxBHLlRCTqjyCFQacOi%2F0ZlnT1LYD9GRMCBlFCYYGsE%2FGq8MdwEU13V5fn9%2FrlNj1hbxYkhr8J7Dv2X%2FmPsYcQ3ZR7RO0AljLIOFktof" rel="nofollow" target="_blank">T150</a>, <a href="https://link.segmentfault.com/?enc=XSzXpfK2ewAjYWU03oS6ng%3D%3D.xRONIW14rh%2Bi5aPr3N%2B8t9oyu1Yoo%2Bj3LRzAxLkDRwVfYygioieN4wZX6rSkKJhVRuk575xbdmapJKK8ofuGmxKCAdEbPSE64Aa0exEhBbwKexQFk2iQTvWc2werVGMr" rel="nofollow" target="_blank">T350</a>, <a href="https://link.segmentfault.com/?enc=W%2BfzCoHQLUAZ2tN%2F2TissQ%3D%3D.fk%2FTuVPXfB56ifRzqh5DQTsoZBjxb9grS8Kd4HuSoc1y6wmZNaWMvPe0SC%2F5MI4N1Ltq%2FBUTqeE00R%2FxLo6Is3ASwrMbsE0m1x5m4mNySs0tu4%2FMoUTsgZiXaf0RCyBd" rel="nofollow" target="_blank">T550</a></td><td>6.7U3, 7.0U2-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 3rd Generation Intel Xeon Scalable processors (sysin) • 2nd or 3rd Generation AMD EPYC Processor</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 16G Server</td><td><a href="https://link.segmentfault.com/?enc=jfsZbfVzwcZYXtTl%2FwCERg%3D%3D.VjTmRSxOIxVTMbfeLreSEEeRP6wH06wL1etVZFiQgXpvfgw9QoVQISZMjwdmkrtSXX2BhKGiDSam2YKxgCnqHzAvLAVXixqPdMMRW11McjA%3D" rel="nofollow" target="_blank">R360</a>, <a href="https://link.segmentfault.com/?enc=%2FJBF%2BEyrRl2o3PB%2BJDB5ww%3D%3D.VoMwaxtYj%2BdRBG6KHgXILz3z2no1yir5Mkb98euYQYQsJK6ohlScrgi7YTB4TcRPFydB2oOORVqX4BlF6pqbJkprx8inx2569dtST05dA7I%3D" rel="nofollow" target="_blank">R660</a>, <a href="https://link.segmentfault.com/?enc=kAxbboI5oPQhIRyJ%2FaHMWw%3D%3D.6ASkVvrrFSOScHbpuECQdNEwPewzhL5PYNZGDsBKlyQ7rmt0GQsUT6jN8z%2FmdhgROm3q3ICMLrtgHprgka%2B07PgdF5LTkrO6lMhshhPVIeM%3D" rel="nofollow" target="_blank">R660xs</a>, <a href="https://link.segmentfault.com/?enc=DTDEfu4lfg%2Bmi5kkRPGuPQ%3D%3D.pFrf67CLQACvvOyH2nOpMyAvtI1epZATi9Ktoi4BaySCVQelX4A8OuqwFH%2FMWSBXrP2fGUEw9hxAb0zL41SIDFwBtCVwiMD1hIJQirDfm5I%3D" rel="nofollow" target="_blank">R6615</a>, <a href="https://link.segmentfault.com/?enc=IAHYaioaZ4oJeoXjJbrmqA%3D%3D.s3u%2B%2FTBxPzcX5mOwIEmcJg%2FyU5n0KI9XvbcVX%2FdIy5jYgD7jAzVxnsazMo56Oe8YYJLkOjcDlZWyFhVNbpJbX9rvQlU0eG2YllAdb4pIDOM%3D" rel="nofollow" target="_blank">R6625</a>, <a href="https://link.segmentfault.com/?enc=2I23tlb%2FKe7AMiUmuravrw%3D%3D.%2F0mhhYoy%2FLgwurCu9zNf73tyjK6uG5PdioQMdk8s8wsS8peoLrsFV5%2BEpdl69DjPNSRyLMTt9A6uRXiv4TlRRoU%2FKFJQL7ucx6opzYXxfBE%3D" rel="nofollow" target="_blank">R760</a>, <a href="https://link.segmentfault.com/?enc=Ce4V%2FB602MTPfPdzJUZKHg%3D%3D.nl3hKbwuzC6BrRONMWQuuBjEqcdpZuiKbA0iZH%2B1%2BL1qvdkuyIh1iGOnxtMKysdloJtpAq9mQjQE5Z%2FausJAJYU2GHrs0pJ2exOcVAupKdM%3D" rel="nofollow" target="_blank">R760xa</a>, <a href="https://link.segmentfault.com/?enc=8uHT1Be41Gn33S9eJaDLsg%3D%3D.w4WWbCte8qJdLqPkcqJcAmxD%2BBhcwCJ2P42nUR6NtsChT8QU9O3jBAYFRD0wxw0DmIwMxVUyxg5ImQbUOOGVQlLmb5vkFx4GZbOidTCOwdw%3D" rel="nofollow" target="_blank">R760xd2</a>, <a href="https://link.segmentfault.com/?enc=V1vp2dzqsb22sVLjnfy0Ew%3D%3D.4BV37m428dAh7eYOzem7bEfFV6qbDqNLA8y4yyftIYxCpJ4mmDq8UsOEMwFDqjFCmn5wNGmT%2Bn%2Bh3I30DtOW%2FTuwtZbHjw8Fdxp4n3LGkSk%3D" rel="nofollow" target="_blank">R760xs</a>, <a href="https://link.segmentfault.com/?enc=tavgUydMtJTWhtw0m003yA%3D%3D.d2aSpV6RGO%2BaOM59OGbhDlpqLgweyg3PD4t50vLOmVBSuohxndDfSLkaA82Vsb6gYANsfH3qwNF53QWXMJu0AvkzSOX24XvlgW67OU6ktxw%3D" rel="nofollow" target="_blank">R7615</a>, <a href="https://link.segmentfault.com/?enc=whqPrNbSQzSMVmae4HoxWQ%3D%3D.5dHTuMnUvyyy76LINvD1RVJbPVc51EldWt%2B6nnWEY579bvDJWzDmDgkHt02BUgZkF5xwjZoTkDlXdY3ZBwUi8TkkfsKWLYPU9hhr7ksu5Qo%3D" rel="nofollow" target="_blank">R7625</a>, <a href="https://link.segmentfault.com/?enc=AemD9YowaAmWorUGcBaCMg%3D%3D.AsEgjW4%2BORO4XPI%2B1T6SVRzOGhBIPrEJTmatZhiSPSWJC4%2FXY5GF6SFifdAEl0kl3wUpeS%2Fkuf0zA4%2Bua2zDImPl0WwsgdOifQUVAZWI0nk%3D" rel="nofollow" target="_blank">R860</a>, <a href="https://link.segmentfault.com/?enc=Pwqu4xYAp2LfSm5M76MFhg%3D%3D.bpyMJ%2FNLsQsIEbQmxn483UyZoBEmCAFHQ1A3AEuOXaVOwzWPx%2BBzNeLolb2aHm5fgRqWpSR%2BL%2FtaNR7RBLtqnDVYKaEUqgsrDg6bFg6MsPk%3D" rel="nofollow" target="_blank">R960</a> <a href="https://link.segmentfault.com/?enc=4BdiTHFllJWDpSJBZyS0RQ%3D%3D.Jmdu%2FDTvA%2BlEayNQlVkPjTIc0qvoMFdrBzroylNMXSou56qFG0gfayNYpGe4e%2FNRJ8zq%2BSJMoOb76kcQAbvGvW%2BZal45IP%2F0JpApNpuFjC8%3D" rel="nofollow" target="_blank">T360</a>, <a href="https://link.segmentfault.com/?enc=PSJRb9GPojIoZ1c3WJnxqA%3D%3D.Tx9lS3L6gV0Bi8pikIEWCKotS19UAQebrAvKtRykhf5bOloUIC4zztRkV8k2OMlJSODpKgIIYvsbNmuBVz9iYPsWiH77Gub9PGB4N6HMYag%3D" rel="nofollow" target="_blank">T560</a></td><td>7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 4th Generation Intel Xeon Scalable processors (sysin) • AMD EPYC 4th Generation 9004 Series</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 17G Server</td><td><a href="https://link.segmentfault.com/?enc=lYM%2BfRUeUHpNVMmMYsApsA%3D%3D.p3SSDZOsLCGKh%2BZL267IN9a86ptoTi1xpCcz0%2FDnJsP9EqiBD0SaRWRnTeOME8oJTNGXt%2FCB1HcKifOlals15VE%2BLYamLJoUn27Xyjs915o%3D" rel="nofollow" target="_blank">R370</a>, <a href="https://link.segmentfault.com/?enc=h67uTiAvaqEQcWF9rJZ6xg%3D%3D.6SIk25NjVR79izC7PrEWHssgASOSaeJIdzKS8CfLundzRsuTM2OCkfhDs4aeb9%2BEQH1NNSE3ybPC0U4MdoSiVNpI9DSrWsjuZHjS0HcxkQw%3D" rel="nofollow" target="_blank">R470</a>, <a href="https://link.segmentfault.com/?enc=RqY97gBiFpHVNKQ%2FAEWDRg%3D%3D.YgFPyXBukEcWWtVpmJoHK2GYehSoiiiGM1To5cZnCgPSZ8WYcOYYqI0owLoGZsWeyOn4FGgSMXQ2S0A3RMFr%2FOeRp4gIikjlNcw6CpJpSX8%3D" rel="nofollow" target="_blank">R570</a>, <a href="https://link.segmentfault.com/?enc=DUwPZkzxGPPxS8fj4v4fDA%3D%3D.mdMVg0CHDxCNscXDTAiAV9Xn6uENKWxVFke8CYnYZSjNCwxWqpRoaHpzwat2qPBIVvMuI%2FxS%2F%2FXvxiVUeRei9qjZwCHd5rjakLI%2BREriH5Q%3D" rel="nofollow" target="_blank">R670</a>, <a href="https://link.segmentfault.com/?enc=Ht0rinotRmb9cAddVsWW4Q%3D%3D.YCyHWeWynuZRUO5dx7GlNwzMajTpW2Kmfj9VtiAKT2s5tPbcjGbVxDEAsiCfM1j35BdvR9V8Qz0XZMoX4eRKB%2BXwd7mNEry9TQe7SxE2hu4%3D" rel="nofollow" target="_blank">R6715</a>, <a href="https://link.segmentfault.com/?enc=%2FC7H8ZVpbZvSFjs%2BfTwZpA%3D%3D.1jncarXFtM1Y3kyjGt%2BZxNIdYp2NtvUaDkyeJUXBB7z2G5HdOZ5kgAW0NOYagsACnHg0dXjdAaTmaU4YX3Mt63yjNwk4JP3LNhABm49tLE0%3D" rel="nofollow" target="_blank">R6725</a>, <a href="https://link.segmentfault.com/?enc=%2Br9N6iB2nWAZzZ%2BRCjDo0w%3D%3D.K3NHJ%2BHM86qn9ij8e8BvbR7H2DbtsBTW1861txzrp0%2BcbYEyjFyqkc6SowxUtn9wDZD2ZznQH1e3DpTA3r7N3udULVUA3tzRi3Wk69XWuFg%3D" rel="nofollow" target="_blank">R770</a>, <a href="https://link.segmentfault.com/?enc=pu6bxKmwGNsIfzKPsabIRQ%3D%3D.fNJFk3ozWlvPCmBx0JniW%2FFUKdX5TZwyrxXttRNoCitaAvDiRbY23iLeSGyyz60seIYLGx9GZWni9G%2B%2BeuCuvo4HRG5XdR%2Bj0FTS6CoLrf0%3D" rel="nofollow" target="_blank">R7715</a>, <a href="https://link.segmentfault.com/?enc=OkTq0YoQ3dZJHhsRUQ%2BFJQ%3D%3D.h32FJzDP5%2B%2Bbn3tGyIKx2vdsk6NRF%2FHmK4GMocRxpOi9MZ1gvsd0IuqiktoZ38ZSHHg0Ubm20T2czCOjiK0YeCMUgyZnLkn4RO29InotwEM%3D" rel="nofollow" target="_blank">R7725</a>, <a href="https://link.segmentfault.com/?enc=8LjlyXOYzwlTaSuZrx%2BHrQ%3D%3D.nMMyCR4LZ94hYyja4dRVeDcqwZzQcDgS2ufXcqKhTKiS0K%2FM%2B99B3wG%2Bz3Xl4OEZSv%2BzZmY3gqI0xQP1lY8ZpoAlOC299aufHTIRuRRa0BA%3D" rel="nofollow" target="_blank">R870</a>, <a href="https://link.segmentfault.com/?enc=7Z6HkNXF%2FnR07WFl8ocrXA%3D%3D.xWk4inUAljVvCsBALxt8YWEBqR3hxXgP2iPq40rCEqmXKwiqAQXKBLzNi8mZ0BEkLfizj6weRsnzBRgzMC4IGMD6Rp3DfIx0dnO55WqLjYE%3D" rel="nofollow" target="_blank">R970</a> <a href="https://link.segmentfault.com/?enc=vD1hb1IedODKgI0K9QWDDQ%3D%3D.22DS9GjtaVVoSLXiWtlMtYwXm0eQS5%2FYXwPLTZc18y8rPpUct%2FztuYBvZsz8OI8JnFsFCzyZeJ7L8nS%2Be6ZmdtkX7uWlLnZZ9vfbtmpHvzM%3D" rel="nofollow" target="_blank">T370</a>, <a href="https://link.segmentfault.com/?enc=cDve3FN%2FsE8t0g0CLnJmDA%3D%3D.WhJVfw4vhICCJ63YwgQNjRUVmhmyN08us2XmU5EVdN7sHz1sflxmx3I3pftt%2FE5rSyPNSVDjdFwKxa3WnH%2FxHBiLiSwmw9KZvciRt0fF%2BBw%3D" rel="nofollow" target="_blank">T570</a> (部分机型尚未发布，按惯例列出)</td><td>8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon 6 processors (sysin) • AMD EPYC 5th Generation 9005 Series</td><td>同官方支持</td><td>同官方支持</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ 以上仅列出 Rack 和 Tower 机型，其他 C、F 和 M 机型理论上兼容性同。</li></ul><h2>HPE (慧与) 服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>HP ProLiant Servers Gen8</td><td>ML10 v2, ML310e Gen8 v2, ML350e Gen8, ML350p Gen8 DL320e Gen8 v2, DL360e Gen8, DL380e Gen8, DL360p Gen8, DL380p Gen8, DL560 Gen8, DL580 Gen8</td><td>6.0-6.0U3, 6.5-6.5U3</td><td>8.0 系列</td><td>• Intel® Xeon® E5-2400 • Intel® Xeon® E5-2400 v2 • Intel® Xeon® E5-2600 v2 (sysin) • Intel® Xeon® E7-4800 v2 • Intel® Xeon® E7-8800 v2</td><td>⚠️ 以下型号默认不受支持： Smart Array P420i, HP Smart Array P222, Smart Array P420, Smart Array P421, Smart Array P822 以上默认最高支持 7.0 (已有特殊定制版可以支持 8.0)，以下默认同时支持 8.0 系列 支持的型号： HPE Smart Array P430i, P430, P431, P830i, P830  【B120i/B320i SATA RAID 不受支持】</td><td>HP 1Gb Ethernet 4-port 331i Adapter HP Ethernet 1Gb 4-port 366i Adapter HP Ethernet 1Gb 4-port 331FLR Adapter (sysin) HPE FlexFabric 10Gb 2P 534FLR-SFP+ Adapter HPE Ethernet 10Gb 2-port 561T Adapter HPE Ethernet 10Gb 2-port 557SFP+ Adapter 预置网卡可支持，选配网卡个别不支持，具体可查询</td></tr><tr><td>HPE ProLiant rack and tower servers Gen9</td><td>ML10 Gen9, ML30 Gen9, ML110 Gen9, ML150 Gen9, ML350 Gen9 DL20 Gen9, DL60 Gen9, DL80 Gen9, DL120 Gen9, DL160 Gen9, DL180 Gen9, DL360 Gen9, DL380 Gen9, DL560 Gen9, DL580 Gen9</td><td>6.5-6.5U3, 6.7-6.7U3, 7.0-7.0U3</td><td>8.0-9.0</td><td>• Intel Xeon E3-1200 v3 • Intel Xeon E3-1200 v5 Intel Xeon E5-2600 v3/v4 (sysin) • Intel Xeon E5-4600 v3/v4 • Intel Xeon E7-4800 v3/v4 • Intel Xeon E7-8800 v3/v4 • AMD Opteron 6300 Series</td><td>HPE Smart Array H240, H240ar, P440, P440ar, P441, P840, P841, P830i, P830 【B140i 不受支持】</td><td>HPE Embedded Dual Port 361i Adapter HPE Embedded 1Gb Ethernet 4-port 331i Adapter (sysin) HPE FlexFabric 10Gb 2P 533FLR-T Adapter HPE FlexFabric 10Gb 2P 534FLR-SFP+ Adapter 预置网卡可支持，选配网卡个别不支持，具体可查询</td></tr><tr><td>HPE ProLiant rack and tower servers Gen10</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML family • HPE ProLiant DL family</td><td>6.5-6.5U3, 6.7-6.7U3, 7.0-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon Scalable processor (sysin) • AMD EPYC 7000 Series Processor family</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen10 Plus</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML family • HPE ProLiant DL family</td><td>6.7U3, 7.0-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 2nd or 3rd Generation Intel Xeon Scalable processors (sysin) • 2nd/3rd Generation AMD EPYC 7002/7003 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen11</td><td>• HPE ProLiant MicroServer • HPE ProLiant 10 series • HPE ProLiant 100 series • HPE ProLiant 300 series • HPE ProLiant 500 series</td><td>7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 4th Generation Intel Xeon Scalable processors (sysin) • 4th Generation AMD EPYC 9004 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen12</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML server • HPE ProLiant DL server • HPE ProLiant RL server</td><td>8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon 6 processors (sysin) • 5th AMD EPYC 9005 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ HPE ProLiant 映像是独立的，不包含 Synergy 和 Superdome。</li></ul><h2>华为与超聚变服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>代表机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>Huawei FusionServer V2</td><td>RH1288 V2, RH1288A V2, RH2285 V2, RH2285H V2, RH2288 V2, RH2288A V2, RH2288H V2, RH2485 V2</td><td>6.0-6.5</td><td>8.0</td><td>E5-2400 E5-2400 V2 E5-2600 E5-2600 V2 E5-4600 E5-4600 V2</td><td>LSI 产品支持（部分型号需定制）</td><td>Intel E1G42ET (82576) 和 Silicom 网卡不受支持。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V3</td><td>5288 V3, RH1288 V3, RH2288 V3, RH2288H V3, RH5885 V3(E7 V2+DDR3),  RH5885 V3(E7 V3+DDR3), RH5885 V3(E7 V3+DDR4), RH5885 V3(E7 V4+DDR4),  RH5885H V3(E7 V2+DDR3), RH5885H V3(E7 V3+DDR4), RH5885H V3(E7 V4+DDR4),  RH8100 V3(E7 V2+DDR3), RH8100 V3(E7 V3+DDR4), RH8100 V3(E7 V4+DDR4)</td><td>6.0-6.5-6.7</td><td>9.0</td><td>E5-2600 V3 E5-2600 V4 E7-4800 V2 E7-8800 V2 E7-4800 V3 E7-8800 V3 E7-4800 V4 E7-8800 V4</td><td>PM8060 和 PM8068 不受支持。LSI 产品支持（部分型号需定制）。</td><td>Silicom 网卡不受支持。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V5</td><td>1288H V5, 1288X V5, 2288 V5, 2288C V5, 2288H V5, 2288X V5, 2288X V5  VC, 2298 V5, 2488 V5, 2488H V5, 5288 V5, 5288X V5, 5288X V5 VC, 5885H  V5, 8100 V5</td><td>6.5-6.7-7.0</td><td>9.0</td><td>Intel Xeon Scalable processors (Skylake) 2nd Generation Intel® Xeon® Scalable processors (Cascade lake)</td><td>Broadcom、Avago 、LSI。</td><td>Silicom 网卡不受支持。海思 Hi1822 芯片不兼容。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V6</td><td>1288H V6, 2288H V6-16DIMM, 2288H V6-32DIMM, 2488H V6, 5288 V6</td><td>7.0-8.0</td><td>9.0</td><td>3rd Generation Intel Xeon Scalable processors (Cooper Lake / Ice Lake)</td><td>Broadcom、Avago 、LSI。</td><td>海思 Hi1822 芯片不兼容。</td></tr><tr><td>xFusion FusionServer V7</td><td>1158H V7, 1258H V7, 1288H V7, 2258 V7, 2288 V7, 2258H V7(4GPU), 2288H V7, 5288 V7, 2488H V7, 5288 V7, 5298 V7, 5885H V7</td><td>7.0U3-8.0-9.0</td><td>同官方</td><td>4th or 5th Generation Intel Xeon Scalable processors (Sapphire Rapids-SP/Emerald Rapids) AMD EPYC 4th Generation 9004 Series</td><td>Broadcom、Avago 、LSI。</td><td>XC 网卡为超聚变产品。</td></tr><tr><td>xFusion FusionServer V8</td><td>1288H V8, 2288H V8, 2158 V8, 2258 V8</td><td>8.0U3-9.0</td><td>同官方</td><td>• Intel Xeon 6 processors (sysin) • 5th AMD EPYC 9005 Series processors</td><td>同官方</td><td>同官方</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ 以上仅列出 Rack 机型，其他机型理论上兼容性同。</li><li>④ 已知配备华为海思芯片的网卡（SP57x、SP58x、SP67x、SP68x）目前不支持 ESXi。</li><li>⑤ 已知配备的 Silicom 网卡最高支持 ESXi 6.x。</li></ul><h2>其他品牌服务器兼容性</h2><p>如果已经有定制版的品牌，可以访问品牌官网查询官方兼容列表，本站定制版兼容性更加广泛。</p><p>欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><p>请提供以下四个信息：</p><ul><li>机器品牌和型号</li><li>CPU 型号</li><li>RAID 卡型号</li><li>网卡型号</li></ul><h2>常见问题解答 (FAQ)</h2><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=TyQYgDWHjEsS8ojjMnmBUQ%3D%3D.3MBGCu%2B5vEAacj6reZlqWg3JsHWORHnVpv4DEuYghyqYhe1GDBuB0NNUb%2BvjUkt3" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a> 查看。</p><h2>下载地址</h2><p><strong>VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 标准版和厂商定制版</strong>：</p><ul><li>发布日期：2026-01-20</li><li>新增 vCenter 和 ESXi SSL 证书自动续期功能，以及 25 项已知问题修复。详见官方文档及本站相关发布。</li><li><strong>Standard (标准版)</strong>：<a href="https://link.segmentfault.com/?enc=cVnjYoePBUL4ervrTguq7A%3D%3D.3vPQs8fGiv8YOF54g5M9NGDj6CSDoPZFpjE8sS%2FhME2LrB03IAE1rpx%2B0XTE56vE" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Dell (戴尔) PowerEdge 定制版</strong> A02：<a href="https://link.segmentfault.com/?enc=C9pzssEvUzV5%2B241Dtiw5w%3D%3D.UOjX06sqGATBmVvRBX01uZ4PA4gPOGMIQ6YNuAkYvSsg6uvv9MOR59xcjRTtvW6p" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>HPE (慧与) ProLiant 定制版</strong> Sep2025：<a href="https://link.segmentfault.com/?enc=8ZiIa%2FeOinn0MR6u4Na1Iw%3D%3D.J%2FW1FVvx6a6rvmfGfIsT5bk5Ax5lvq3F7I5sFkOMG6UEfBTGbvMmEPav%2Bdcwcd7c" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Huawei (华为) FusionServer 定制版</strong>：<a href="https://link.segmentfault.com/?enc=k%2FN6mUvUNiHvd1DvvLzmlg%3D%3D.IjqzXvX5DKyyIvMG%2Bq1J0ACm9NC7jtj50Hnx8DA7zERRN5jtI58hDpZU0xFnVAiO" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>xFusion (超聚变) FusionServer 定制版</strong>：<a href="https://link.segmentfault.com/?enc=oCfLLmoxbwJxI%2F1TdVmAZA%3D%3D.8BHqXgZ024C4ERyu64kkDIOllRRCcOZKGYo8YVFN9Bie0PgvT1yvd63rnR493wkx" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Lenovo (联想) 定制版</strong> S02：<a href="https://link.segmentfault.com/?enc=0tj66iXhiOIBjfsg3MsKVA%3D%3D.bpEGBTmaEF8c6PAqo2nWXMDtvhVwkm74yqjNTWQn4Tetp%2BrszYd26WbdQSHmFuhA" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Inspur (浪潮) 定制版</strong>：<a href="https://link.segmentfault.com/?enc=rrmbM8ANe2n5NB0u7%2BEJ1A%3D%3D.MoLI9APOkpiIDI9U7Sf9YWr%2FVmutgyhyYjzRLKSDe9py%2B9hdNSZICHeSpfRQyZ3h" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>H3C (新华三) 定制版</strong>：<a href="https://link.segmentfault.com/?enc=9%2FjNPmBir%2BEdKwug1Nj5%2Bg%3D%3D.USRWKvo6wOYxmLmw5zyKx77oIOICiZW0gFXkCYsiIv95EvLxBftQD4Q1A7mP7JjO" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><p><strong>其他定制版</strong>：<a href="https://link.segmentfault.com/?enc=lxyyaLDpxaF9zfXD8xHarA%3D%3D.sBLo1q%2BFnvLPDPMSIrdIDEq%2FUcsFiuXqRIOz2x4oaVYj6aGnnjGtY%2FnmyaaKsMPf" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></p><ul><li>Cisco (思科) UCS 定制版</li><li>Fujitsu (富士通) 定制版</li><li>Hitachi (日立) 定制版</li><li>NEC (日电) 定制版</li></ul></li></ul><hr/><p>集成驱动版本，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=pHHlFpqZ%2FnPAIKGjUvILzg%3D%3D.8o1uJha4RUn2mRzzndw%2FvNOVpLPVd7ifZQK5iW5%2BoLCzF6JR9oyJEiWfcZmbBYmC" rel="nofollow" target="_blank">VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 2.7 集成网卡驱动和 NVMe 驱动 (集成驱动版)</a></li></ul><p>官方原版，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=qy%2FvojTs1Anlbzoec1mBqQ%3D%3D.ibHQaES5n992rV9N2HUiti9O019zHWnw9WyzY%2BPcsK3E%2FlYv3O%2FHCBW%2FubgMNkGj" rel="nofollow" target="_blank">VMware vSphere 9.0.2.0 正式版发布 - 企业级工作负载平台</a></li></ul><p>上一个版本，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=LbmDS5l536oBQ0JcVF%2BSlw%3D%3D.QBbYBbXJY4gkKlmduC8yq%2BJGZSGxMcmPmSALUfEQW4yFGKMDFnqTCRBYo1hiPIaa" rel="nofollow" target="_blank">VMware ESXi 8.0U3h macOS Unlocker &amp; OEM BIOS 2.7 标准版和厂商定制版</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=jT%2B1YlgZnHymMhDQCw0ZRA%3D%3D.SmDZm8ZH%2Fg9OdWGcDmoLjLuKfXfkIjfvz93K0eI2pDA%3D" rel="nofollow" target="_blank">VMware 产品下载汇总</a></p>]]></description></item><item>    <title><![CDATA[RUM 链路打通实战：打破移动端可观测性黑洞 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578294</link>    <guid>https://segmentfault.com/a/1190000047578294</guid>    <pubDate>2026-01-28 17:07:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：路锦（小蘭）</p><h2>背景：移动端的“可观测性黑洞”</h2><p>在微服务架构蓬勃发展的今天，服务端的可观测性建设已日趋成熟。无论是 Jaeger、Zipkin 还是 SkyWalking，这些分布式链路追踪工具都能够帮助开发者清晰地观察到一个请求从网关进入后，是如何在各个微服务之间层层流转、逐级调用的。然而，当我们试图将这条链路向前延伸至移动端时，却发现一道难以逾越的鸿沟横亘其间。</p><ul><li>关联困难：移动端和服务端仿佛两座孤岛，各自维护着独立的日志系统。客户端记录着请求发起的时间和结果，服务端保存着完整的调用链路，但两者之间缺乏一条有效的纽带将其串联起来。一旦出现问题，排查人员只能依靠时间戳进行手工比对，既费时又容易出错，遇到高并发场景更是如同大海捞针。</li><li>定位模糊：我们常常遇到这样的场景：用户投诉说接口超时了，但翻开服务端监控，每一条请求都显示着正常返回的 200 状态码。问题究竟出在用户的网络环境、运营商的链路质量，还是服务端在某个瞬间的抖动？由于移动端与服务端的监控体系相互割裂，我们根本无从判断故障边界，各团队之间也容易陷入相互推诿的困境。</li><li>复现无门：移动端的网络环境远比服务端复杂——DNS 解析可能受到劫持、SSL 握手可能遭遇兼容性问题、弱网环境下的重试和超时更是家常便饭。这些关键信息在传统方案中往往随着请求结束而烟消云散，当问题间歇性发生时，开发者既无法还原现场，也难以定位根因，只能在用户的反复投诉中束手无策。</li></ul><p>正是这些痛点的存在，让端到端全链路追踪的需求变得愈发迫切。我们需要一种方案，能够让移动端真正成为分布式链路的起点，让每一次用户操作触发的请求都能够被完整记录、精准关联、一路追踪到最底层的数据库调用。本文将通过一个最佳实践案例，展示如何借助阿里云用户体验监控实现移动端到后端的全链路 Trace 打通，辅助网络请求问题排查。</p><h2>核心方案：端到端链路打通的技术实现</h2><h3>核心思想</h3><p>端到端链路打通的本质是：<strong>让客户端成为分布式追踪链路的第一跳，使其与服务端链路共享同一个 Trace ID。</strong></p><p>在传统架构中，链路追踪的起点是服务端网关——请求进入网关时，APM Agent 为其分配 Trace ID，并在后续的微服务调用中透传。而端到端打通方案则将这个起点前移到了用户的手机上，由移动端 SDK 主动生成 Trace ID 并注入到请求头中，使得整条链路从用户指尖到数据库底层都被同一个标识串联起来。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578296" alt="image" title="image"/></p><h3>技术实现的四个关键环节</h3><p>整个链路打通的实现可以分为四个紧密衔接的环节：</p><h4>环节一：客户端生成链路标识</h4><p>当用户触发一次网络请求时，客户端 SDK 在请求真正发出之前介入：</p><ol><li><strong>拦截请求</strong>：通过网络库的拦截机制（如 OkHttp Interceptor）捕获即将发出的请求</li><li><p><strong>创建 Span</strong>：为这次请求创建一个 Span 对象，生成两个核心标识：</p><ul><li><strong>Trace ID</strong>（32 位十六进制）：整条链路的唯一身份</li><li><strong>Span ID</strong>（16 位十六进制）：当前这一跳的唯一身份</li></ul></li><li><strong>记录起始时间</strong>：精确记录请求发起的时间戳，用于后续计算各阶段耗时</li></ol><h4>环节二：协议编码与注入</h4><p>生成链路标识后，需要将其编码为服务端能够理解的格式。这里的关键是选择一套双方都遵守的“通用语言”——W3C Trace Context 或 SkyWalking SW8 协议。</p><p>客户端 SDK 将编码后的信息写入 HTTP 请求头，随请求一同发送。</p><h4>环节三：网络传输与透传</h4><p>HTTP 协议天然具备请求头的穿透性，这是透传能够实现的技术基础：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578297" alt="image" title="image" loading="lazy"/></p><h4>环节四：服务端接收与延续</h4><p>当请求到达服务端时，APM Agent 接管后续处理，完成链路的延续：</p><ol><li><strong>解析请求头</strong>：从 <code>traceparent</code> 或 <code>sw8</code> 头中提取 Trace ID 和 Parent Span ID</li><li><strong>继承链路上下文</strong>：将客户端传入的 Trace ID 作为本条链路的身份标识，而非重新生成</li><li><strong>创建子 Span</strong>：为服务端的处理逻辑创建新的 Span，其 Parent Span ID 指向客户端的 Span</li><li><strong>继续透传</strong>：在调用下游服务时，继续在请求头中携带同一个 Trace ID</li></ol><p>通过这四个环节的紧密配合，移动端发出的每一个请求都能与服务端的调用链路无缝衔接，形成一条从用户设备到数据库的完整追踪链路。</p><h3>链路打通协议</h3><p>为了让不同系统之间能够“说同一种语言”，业界制定了标准化的链路追踪协议。目前主流的协议有两种：</p><h4>W3C Trace Context 协议</h4><p>W3C Trace Context 是 W3C 官方标准，具有最广泛的兼容性。</p><p><strong>Header 格式：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578298" alt="image" title="image" loading="lazy"/></p><p><strong>字段说明：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578299" alt="image" title="image" loading="lazy"/></p><p><strong>适用场景：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578300" alt="image" title="image" loading="lazy"/></p><h4>SkyWalking SW8 协议</h4><p>SW8 是 Apache SkyWalking 的原生协议，包含更丰富的上下文信息。</p><p><strong>Header 格式：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578301" alt="image" title="image" loading="lazy"/></p><p><strong>字段说明：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578302" alt="image" title="image" loading="lazy"/></p><p><strong>适用场景：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578303" alt="image" title="image" loading="lazy"/></p><h2>实战案例：一次查询接口超时的全链路排查</h2><p>理论讲完了，接下来让我们通过一个真实的排查案例，看看端到端链路打通在实际问题定位中是如何发挥作用的。</p><h3>问题背景</h3><p>我们基于某开源代码库构造了一个慢请求场景，架构如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578304" alt="image" title="image" loading="lazy"/></p><p>在日常使用中，我们发现某个页面打开十分缓慢，用户体验极差。初步怀疑是由于 API 请求响应过慢导致，但具体慢在哪里、为什么慢，还需要进一步分析。接下来，我们将借助阿里云用户体验监控的全链路追踪能力，一步步定位问题根因。</p><h3>第一步：在云监控控制台定位异常请求</h3><p>首先，我们登录阿里云控制台，进入<strong>云监控 2.0 控制台 → 用户体验监控 →您的应用 → API 请求</strong>模块。在这里，我们可以看到所有 API 请求的性能统计数据。</p><p>通过对“缓慢占比”进行排序，我们很快发现了问题所在：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578305" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578306" alt="image" title="image" loading="lazy"/></p><p>从监控数据可以清晰地看到，API <code>/java/products</code> 的响应时间异常——平均耗时高达 40 多秒！这个耗时远远超出了正常范围，难怪用户会感觉页面打开缓慢。</p><p>找到了可疑的 API，接下来我们需要进一步分析它的调用链，搞清楚这 40 多秒究竟消耗在了哪个环节。</p><h3>第二步：查看调用链，追踪服务端链路</h3><p>点击对应 API 的“<strong>查看调用链</strong>”按钮，系统会跳转到当前请求的 Trace 详情页面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578307" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578308" alt="image" title="image" loading="lazy"/></p><p>这里就是端到端链路打通的核心价值所在——我们可以直接看到从移动端到后端的完整调用链路，无需在多个系统之间来回切换。</p><p>从链路瀑布图中可以清晰地看到：</p><ul><li>移动端发起请求后，链路完整地延续到了后端服务</li><li>耗时主要发生在后端服务的 <code>/products</code> 接口</li><li>该接口处理耗时超过 40 秒才返回数据</li></ul><p>为了方便后续在后端应用监控中进行更深入的分析，我们记录下当前的 Trace ID：<code>c7f332f53a9f42ffa21ef6c92f029c15</code>。</p><h3>第三步：查看后端服务Trace分析</h3><p>接下来，我们进入应用监控 → 找到对应的后端应用 → 调用链分析，使用刚才记录的 Trace ID 进行查询。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578309" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578310" alt="image" title="image" loading="lazy"/></p><p>从后端链路数据可以还原出 <code>/products</code> 接口的执行链路：</p><ul><li>HikariDataSource.getConnection，重复 6 次，总耗时 3ms。含义：获取数据库连接（从连接池拿）一共 6 次，总共才 3ms，不是瓶颈。</li><li>postgres，重复 6 次，总耗时 2ms。这是一些非常快的 Postgres 操作/小查询，同样不是瓶颈。</li><li>SELECT postgres.products，重复执行了1 +  5 次，总耗时 42290ms ≈ 42.3s。这行才是关键：同一个 SQL（查 products 相关）一共执行了 5 次，总耗时 42.3s，平均每次大约 8 秒。</li><li>也就是说：主要时间都花在执行这个 SQL 上，而不是连库 / 建连接 / 网络。</li></ul><h3>第四步：深入分析慢 SQL</h3><p>点击链路中的最后一个 Span，我们可以在右侧详情面板中看到具体执行的 SQL 语句：</p><pre><code>-- 第一次查询：获取全量产品数据
SELECT * FROM products
-- 对每个产品执行 N 次查询（N+1 问题）
SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code></pre><p>问题的根因逐渐浮出水面：</p><ol><li>第一次查询：<code>SELECT * FROM products</code> 获取所有产品，这一步耗时尚可</li><li>N 次循环查询：对于每一个产品，又单独执行了一次 <code>SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code></li></ol><p>这是一个典型的 N+1 查询问题！更糟糕的是，<code>weekly_promotions</code> 是一个特意设计的“慢视图”（sleepy view），每次查询都会执行较重的操作。当产品数量较多时，这些查询累积起来就造成了 42 秒的惊人耗时。</p><p>我们记录下当前的线程名称：<code>http-nio-7001-exec-3</code>，以便进一步查看 Profile 数据进行验证。</p><h3>第五步：查看 Profile 数据验证结论</h3><p>为了进一步确认我们的分析结论，我们进入<strong>应用诊断 → 持续性能剖析</strong>，查看后端服务的 Profile 数据。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578311" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578312" alt="image" title="image" loading="lazy"/></p><p>筛选对应的线程后，我们看到了服务执行时间的分布情况：</p><ul><li><code>sun.nio.ch.Net.poll(FileDescriptor, int, long)</code> 耗时占比接近 100%</li><li>这表明线程大部分时间都在<strong>等待 Postgres socket 返回数据</strong></li></ul><p>Profile 数据与调用链分析的结论完全吻合——问题确实出在数据库查询上，线程一直在等待慢 SQL 的执行结果。</p><h3>第六步：定位根因</h3><p>综合以上分析，我们可以清晰地定位到问题的根因：</p><p><strong>问题根因：N+1 查询 + 慢视图</strong></p><ol><li><p>代码逻辑存在 N+1 查询问题：</p><ul><li>第一次查询：<code>SELECT * FROM products</code>（1 次）</li><li>对每个 product 循环查询：<code>SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code>（N 次）</li></ul></li><li>weekly_promotions 是一个“慢视图”，查询本身就很耗时</li><li>两者叠加，导致接口总耗时超过 40 秒</li></ol><h2>总结</h2><p>全链路端到端打通解决了移动端与服务端之间的“可观测性黑洞”问题。通过在移动端注入标准化的 Trace Header，实现：</p><ul><li><strong>统一追踪</strong>：移动端请求和服务端链路使用同一个 TraceID，一键关联；</li><li><strong>精准定位</strong>：从用户手机到数据库，每一跳的耗时清晰可见；</li><li><strong>快速定界</strong>：告别“移动端说服务端慢，服务端说网络差”的扯皮；</li><li><strong>数据驱动</strong>：基于真实链路数据优化，而非猜测。</li></ul><p>阿里云 RUM 针对 Android 端实现了对应用性能、稳定性、和用户行为的无侵入式监控采集 SDK，可以参考接入文档体验使用。除了 Android 外，RUM 也支持 Web、小程序、iOS、鸿蒙等多种平台监控分析，相关问题可以加入“RUM 用户体验监控支持群”（钉钉群号： 67370002064）进行咨询。</p><p>参考链接：</p><p>[1] Android 接入文档：</p><p><a href="https://link.segmentfault.com/?enc=Q5ts%2FSzP8fmvVC4A2jKZ7Q%3D%3D.gH4g3tlhhPuA0AxRChHxY3dHKtQ3R2aSmFUzoJ8gTHUtWdKZ7gc7TccGlpbaWS33pBWjCYRpVXZ6ecmbUFh0jlo%2BySjAuvIIzJz%2F2%2B6SB8gAnzwUDxAZp1uTj6mN3DVZFUB1uR%2FpIke7jmm4e7ikLNOL4EVWkXz%2BTVFEPwhLc3P6%2BLnTsSY5q8wC9KTIPQHsKydc01Vlna2BcMW%2FqCgGXw%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/user-experience-monitoring/ac...</a></p><p>[2] Java 应用监控说明文档：</p><p><a href="https://link.segmentfault.com/?enc=jB3kUv0WnhJDT3Vsljemjg%3D%3D.sqagD4cnsXfcUMp3dIzCvvm0CztIiobvPz1fIaB%2B7lwLn%2FV%2B9szbNx4IgZ3WooSVM8NwwZx0x4rrXjf7cAtPr2BNFl31dhXjI0zRQTRt6Vy0JzsPaI64SoW%2BWU9S6wyCqUuxBYq1RcdorGTpOWFk9lmfW%2B8MrHQer4CQK0qGeaAVVNXWhYl3CdKWKe04uWdtPoaQws4kq7HRYS6RwwyBHZ1ivj9aMzmyizdut3FL6diLhSr38nuTDTflNgKMp%2Fv0" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/application-monitoring/user-g...</a></p><p>[3] 持续性能剖析说明文档：</p><p><a href="https://link.segmentfault.com/?enc=dkC4NQ5gB70f5D8Dpkrgdw%3D%3D.QYMKcux5olOoY7M1wdvgBUTks4JeUxWzq8kGkW0Aj5uDcirMCWNWMN%2FX0Q8ZYNu12YuBACqBcMKTcZR%2Fygr%2F5UiUpgXpMgejbT4YxN5582Z3sn91wewSvVBX2Fp0CtQzOz%2FcQv93VyG1sVTrbIa%2FHU050NlTpUl6dlvfnmmf2dC9%2B4S1ySWIdNNxrPVb8O3RTjdFplwOu7pbn4T8JlRFcjivgBARZ4WQHENgBc6tjzP6XACLj0%2FwhNd6Lva4rIqDw5tCBm4%2BKOTC1YBv%2FE7q8A%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/application-monitoring/user-g...</a></p>]]></description></item><item>    <title><![CDATA[使用 Python 轻松添加文本水印到 PDF 宇文成都 ]]></title>    <link>https://segmentfault.com/a/1190000047578334</link>    <guid>https://segmentfault.com/a/1190000047578334</guid>    <pubDate>2026-01-28 17:06:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代办公环境中，PDF 文档的安全性变得愈发重要。添加水印是确保资料安全，防止未授权复制的一种有效手段。本文将介绍如何使用 Python 的 Spire.PDF 库为 PDF 文档添加文本水印。</p><h2>Spire.PDF 简介</h2><p>Spire.PDF 是一个功能强大的 PDF 处理库，支持多种 PDF 操作，包括创建、编辑、转换和打印 PDF 文档。对于想要在 Python 中实现 PDF 操作的开发者而言，Spire.PDF 提供了简洁的 API，让用户能够轻松访问和操作 PDF 文件。</p><h2>安装 Spire.PDF</h2><p>在使用 Spire.PDF 之前，需要先进行安装。可以通过以下命令在命令行中使用 pip 安装该库：</p><pre><code class="bash">pip install spire-pdf</code></pre><p>确保在执行上述命令之前，已经安装了 Python 环境和 pip。</p><h2>为 PDF 文档添加水印的示例代码</h2><p>接下来，我们将通过一个示例代码来演示如何为 PDF 文档添加文本水印。以下是简化后的代码示例：</p><pre><code class="python">from spire.pdf import PdfDocument
from spire.pdf.common import PdfTrueTypeFont, PdfBrushes, PointF

# 创建 PdfDocument 类的对象并加载 PDF
doc = PdfDocument()
doc.LoadFromFile("C:\\Users\\Administrator\\Desktop\\Input.pdf")

# 创建水印字体
font = PdfTrueTypeFont("黑体", 48.0, 0, True)
text = "仅 内 部 使 用"

# 计算文本尺寸
text_width = font.MeasureString(text).Width
text_height = font.MeasureString(text).Height

# 遍历每一页添加水印
for i in range(doc.Pages.Count):
    page = doc.Pages.get_Item(i)
    state = page.Canvas.Save()  # 保存当前画布状态
    
    # 计算页面中心坐标
    x = page.Canvas.Size.Width / 2
    y = page.Canvas.Size.Height / 2

    # 调整坐标系，使页面中心成为原点
    page.Canvas.TranslateTransform(x, y)
    page.Canvas.RotateTransform(-45.0)  # 逆时针旋转45度
    
    page.Canvas.SetTransparency(0.4)  # 设置透明度
    
    # 绘制水印文本
    page.Canvas.DrawString(text, font, PdfBrushes.get_Blue(), PointF(-text_width / 2, -text_height / 2))
    
    page.Canvas.Restore(state)  # 恢复画布状态

# 保存修改后的文档
doc.SaveToFile("output/TextWatermark.pdf")
doc.Dispose()  # 释放资源</code></pre><h2>代码解析</h2><ol><li><strong>加载 PDF 文档</strong> ：首先，我们通过 <code>PdfDocument</code> 类加载指定路径的 PDF 文档。</li><li><strong>设置水印字体和文本</strong> ：接着，我们创建一个 <code>PdfTrueTypeFont</code> 对象，指定字体、大小和样式，并定义水印文本。</li><li><strong>计算文本尺寸</strong> ：使用 <code>MeasureString</code> 方法获取文本的宽度和高度，以便正确定位水印。</li><li><strong>遍历文档的每一页</strong> ：使用 for 循环遍历文档中的每一页，在每一页上绘制水印。</li><li><strong>保存和释放资源</strong> ：最后，将修改后的文档保存到新的 PDF 文件，并释放资源。</li></ol><h2>总结</h2><p>通过上述代码，开发者可以轻松地为 PDF 文档添加文本水印。这不仅提高了文档的安全性，还增强了其专业性。Spire.PDF 库提供了丰富的功能，极大地方便了 PDF 文件的处理。无论是个人项目还是企业级解决方案，Spire.PDF 都是一个值得考虑的选择。</p><p>希望本文能帮助您快速熟悉如何使用 Python 为 PDF 添加水印。待您自己试验时，请确保您有权限对相关 PDF 文档进行修改！</p>]]></description></item><item>    <title><![CDATA[产品管理系统怎么选？2026主流工具横评、场景适配与避坑 许国栋 ]]></title>    <link>https://segmentfault.com/a/1190000047578348</link>    <guid>https://segmentfault.com/a/1190000047578348</guid>    <pubDate>2026-01-28 17:05:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文横评 10 款产品管理系统：ONES、Jira、Aha! Roadmaps、Productboard、Craft、airfocus、Azure DevOps Boards、Rally by Broadcom、Perforce P4 Plan、Jama Connect。帮你按企业痛点与成熟度建立选型框架，减少双系统维护、口径不一与治理失控的隐性成本。</p><p>很多企业已经不缺工具，缺的是单一事实源（SSOT）：需求在产品侧、交付在工程侧、路线图在 PPT/表格，最终“优先级解释不清、变更影响评估不出、交付预测越来越不准”。此时再选一套产品管理系统，如果不先搞清“它解决的是上游决策、下游交付，还是全链路闭环”，很容易把问题从“协作割裂”升级成“系统割裂”。</p><p>下面我会给你 5 个常用的测评标准判断点，你可以把任何一款产品管理系统放进这 5 个问题里过一遍，看看哪一个更符合团队的需求。</p><ol><li>上游决策：是否支持洞察/想法沉淀、可解释的优先级（评分/公式/模型）？</li><li>路线图对齐：能否输出面向管理层/研发/业务的不同路线图视图？</li><li>交付联动：需求到迭代/任务的映射是否自然，还是要“人工翻译”？</li><li>追溯与审计：变更发生时，能否快速看到影响范围，并留证据链？</li><li>集成与可维护性：和现有工具链集成后，谁维护、如何升级、失败如何补偿？</li></ol><p>最小 POC 建议：用 3 条真实需求跑通“从决策到交付/验证”的链路，并故意做 1 次变更，观察系统是否能让影响分析与同步成本可控。</p><h2>工具盘点：10 款产品管理系统测评</h2><h4>1）<a href="https://link.segmentfault.com/?enc=zRZM8LB1%2BHi0oA2fqNRJgA%3D%3D.wiMzouaqL2GJeKiVAakeJpWM3lMkmWYn%2BfG%2BQiSQvKw%3D" rel="nofollow" target="_blank">ONES</a>：一体化研发管理平台型</h4><p>核心定位：强调“一个平台实现端到端的软件研发管理”，从需求管理、迭代跟进到测试，并提供效能改进与开放拓展能力；产品线包含 ONES Project、ONES TestCase、ONES Wiki 等。</p><p>产品管理能力：适合把“需求池—迭代计划—缺陷/测试—交付质量”放在同一数据结构里，减少产品系统与工程系统之间的反复同步。对 VP 来说，它的价值更像“研发侧 SSOT”：当管理层问“为什么延期/风险在哪”，你能从链路数据里给出一致解释。<br/>项目/交付管理能力：平台型系统天然强调流程与度量一致性——如果你的组织希望把敏捷/瀑布/混合流程落到同一治理框架中，这类产品更容易形成长期资产（模板、字段口径、报表口径）。</p><p>适用场景：多团队多项目、希望减少工具割裂；或国产化替代背景下，希望“需求—测试—知识库—流水线”尽量在同一生态中。</p><p>优势亮点：闭环完整、数据口径更容易统一；对研发效能与质量治理更友好（尤其当 PMO/效能团队愿意做数据治理）。</p><p><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdhI10" alt="ONES 产品管理全景图" title="ONES 产品管理全景图"/></p><h4>2）Jira + Jira Product Discovery</h4><p>核心定位：Jira Product Discovery 主打“捕捉洞察、优先级排序、构建路线图——都在 Jira 内完成”，并强调用数据与客户洞察帮助团队做出更有影响力的优先级决策。<br/>产品管理能力：强在“把上游讨论结构化”：洞察/想法/机会进入同一空间，优先级可围绕证据与数据展开；路线图视图用来减少对齐成本（尤其面对业务与管理层）。<br/>项目/交付管理能力：如果你的交付已经在 Jira Software，JPD 的价值在于减少“从产品语言翻译成工程语言”的损耗——至少能让上游输入更可追踪。<br/>适用场景：Jira 生态已经很深、产品团队想补齐 discovery 能力；或全球化团队需要依托成熟生态协作。<br/>优势亮点：生态成熟、协作惯性小；对“把产品讨论从口水战拉回证据链”很有效。<br/>局限与使用体验：高度可配置带来的副作用是“标准不统一就会越用越乱”。你需要明确：哪些字段是组织标准、哪些是团队自定义；否则后期数据不可比。</p><p><img width="723" height="399" referrerpolicy="no-referrer" src="/img/bVdne5p" alt="" title="" loading="lazy"/></p><h4>3）Aha! Roadmaps</h4><p>核心定位：强调建立优先级框架，用 scorecard/feature scores 让功能优先级更客观，并帮助团队对齐“下一步做什么”。<br/>产品管理能力：非常适合把“价值/成本/风险/战略契合度”等维度显性化，让优先级讨论变成“可解释的计算题”。当你的组织有多个产品线、需求争夺资源激烈，这种“框架化优先级”能显著降低内耗。<br/>项目/交付管理能力：它更像“产品办公室/组合管理层”的系统——擅长表达与对齐，但下游交付通常仍要对接工程系统。<br/>适用场景：产品战略与路线图需要强表达；管理层需要一套可复盘的优先级机制；产品线多、节奏复杂。<br/>优势亮点：scorecard 不是装饰品，它能把“谁更会说”变成“标准化权衡”。<br/>局限与使用体验：上游强不代表闭环强——如果工程侧系统割裂或集成不稳，容易出现“路线图很好看，交付仍失真”。</p><p><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdm9Wj" alt="" title="" loading="lazy"/></p><h4>4）Productboard</h4><p>核心定位：强调帮助产品经理理解客户需求、确定优先级，并让团队围绕路线图达成一致。<br/>产品管理能力：当你们最痛的是“反馈太多、信息太散、优先级总靠拍脑袋”，Productboard 的核心价值在于把“客户声音→机会→功能”这条链条系统化：既能沉淀证据，也能形成对外对内一致的路线图叙事。<br/>项目/交付管理能力：通常需要与工程系统配合；它更强在上游决策质量与对齐效率，而不是替代工程执行系统。<br/>适用场景：面向外部客户/多渠道反馈；需要把需求证据链纳入治理（避免“谁提得急就先做”）。<br/>优势亮点：对 VP 来说，能减少“做错方向”的返工成本，这往往比单点效率提升更值钱。<br/>局限与使用体验：如果工程侧没有明确 SSOT，容易形成“双系统写需求”的隐性成本；必须在流程里定义清楚：哪些字段在 Productboard 负责，哪些字段在交付系统负责。</p><p><img width="723" height="403" referrerpolicy="no-referrer" src="/img/bVdnNsA" alt="" title="" loading="lazy"/></p><h4>5）Craft.io</h4><p>核心定位：强调从 ideation 到 execution 的 OKR 全生命周期管理，并把 objectives 连接到 initiatives、projects、epics；同时支持 OKR-based roadmaps。<br/>产品管理能力：它的强项是把“目标—举措—特性/史诗”串起来。对企业级产品而言，这会直接提升 ROI 讨论质量：不是“这个需求看起来不错”，而是“它对应哪条目标、预期带来什么指标提升、投入多少交付成本”。<br/>项目/交付管理能力：适合作为产品侧中枢，再与工程系统联动；它更擅长管理“做什么/为什么做/如何取舍”，工程过程度量仍要看你们的交付底座。<br/>适用场景：OKR 已经是“硬机制”，需要把路线图与目标绑定；产品线多、跨团队对齐成本高。<br/>优势亮点：优先级模型 + OKR 绑定，会让需求排序更可解释、更可复盘。<br/>局限与使用体验：如果 OKR 本身口径不清或频繁摇摆，系统会把混乱“结构化”地记录下来；建议先把目标治理做好再导入。</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdnNsB" alt="" title="" loading="lazy"/></p><h4>6）airfocus</h4><p>核心定位：自我定位为“模块化产品管理软件”，用于管理与沟通产品策略、优先级与路线图，并强调解决“做对问题”。<br/>产品管理能力：适合从上游切入——先把优先级与路线图做清楚，再逐步与交付系统打通。对很多企业来说，这比“一步到位换平台”更现实：组织阻力小、试点更快、ROI 更容易证明。<br/>项目/交付管理能力：airfocus 本身不是工程执行系统，但它强调与 Azure DevOps 等的集成，让策略与日常开发保持同步。<br/>适用场景：产品团队需要提升上游决策质量，但工程体系暂不动；或希望先建立产品 SSOT，再逐步整合。<br/>优势亮点：模块化带来的“可控引入”是关键优势——先拿下最痛的环节（优先级/roadmap），再扩。<br/>局限与使用体验：闭环强弱高度依赖集成质量；若工程侧字段/流程不标准，最终仍会回到人工对齐。</p><p><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdnNsC" alt="" title="" loading="lazy"/></p><h4>7）Azure DevOps Boards</h4><p>核心定位：Azure Boards 的看板实践强调 WIP 限制：通过强调“完成优先于开始”，团队往往获得更高生产力与更好质量；官方文档给出如何设置与实施 WIP 的指南。<br/>产品管理能力：更偏“把需求拆成可交付工作并持续跟踪”，对需求证据链、路线图叙事并不突出；但如果你的目标是提升交付确定性，它能提供更可信的过程数据。<br/>项目/交付管理能力：强在看板治理、瓶颈识别与流程改进（WIP 本质上是“用机制逼迫组织减少多任务切换与等待浪费”）。对 VP 来说，这是效能体系落地的硬工具。<br/>适用场景：微软生态、DevOps 流水线与工程管理一体化诉求强；效能团队希望用过程数据推动改进。<br/>优势亮点：度量可信、治理可操作——能把“感觉很忙”变成“瓶颈在哪里、该怎么调 WIP/拆分工作”。<br/>局限与使用体验：如果把它硬当成完整产品管理系统，产品团队会觉得“上游不够产品化”；更合理的方式是：用它做交付底座，上游用专门的产品发现/roadmap 工具补齐。</p><p><img width="723" height="479" referrerpolicy="no-referrer" src="/img/bVdne5o" alt="" title="" loading="lazy"/></p><h4>8）Rally by Broadcom</h4><p>核心定位：Rally 官方强调“从投资决策到交付的完整可追溯性”，并作为 ValueOps 平台的一部分与其他产品集成、支持规模化。<br/>产品管理能力：它更像“组合/价值流层”的产品管理系统：当你要回答“资源投到哪些主题/举措、跨团队进展如何、关键优先级是否一致”，Rally 的强项在于把工作映射到更高层级的业务优先级。<br/>项目/交付管理能力：支持用 Portfolio Item 表达 initiative/feature 以计划、优先级与跟踪工作，这对大组织的节奏对齐很关键。<br/>适用场景：多团队多项目、多层级治理；PMO/效能团队需要统一方法论与组合视角；管理层强烈要求“可解释的进展与风险”。<br/>优势亮点：减少“汇报型管理”，提高“系统型治理”——让领导看见的不是 PPT，而是从投资到交付的链路状态。<br/>局限与使用体验：治理成本高、对流程纪律要求强；如果组织没有统一口径，上线后可能变成“填报系统”。</p><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnnym" alt="" title="" loading="lazy"/></p><h4>9）Perforce P4 Plan（formerly Hansoft）</h4><p>核心定位：Perforce 官方描述 P4 Plan 能用多种视图（Product Backlog、Quality Assurance、Planning）洞察项目范围，并支持 capacity planning、查看项目历史、可本地或云部署。<br/>产品管理能力：当你的核心挑战是“复杂依赖 + 资源约束 + 计划频繁变更”，P4 Plan 的价值在于把计划从静态表格变成动态系统：依赖关系、范围变化、产能约束可以更直观地被管理与讨论。<br/>项目/交付管理能力：它适配多交付方法（敏捷/瀑布/混合），适合在大规模协作中做“计划可信度治理”。<br/>适用场景：复杂工程（例如大型产品、跨团队依赖强）、对排期与资源规划敏感；希望提升计划可执行性，而非只做任务跟踪。<br/>优势亮点：依赖管理与产能规划是硬能力；当你要把“承诺交付”变得更可信，这类工具往往比“更花哨的 roadmap”更有用。<br/>局限与使用体验：上游洞察与路线图叙事不是它的强项；如果产品团队需要强 discovery，通常要配套上游工具。</p><p><img width="723" height="423" referrerpolicy="no-referrer" src="/img/bVdnNsG" alt="" title="" loading="lazy"/></p><h4>10）Jama Connect</h4><p>核心定位：强调 Live Traceability（实时追溯），用于跨需求、测试、风险活动建立端到端追溯并持续改进过程绩效；并可从高层需求追溯到最终测试。<br/>产品管理能力：它解决的不是“路线图怎么画”，而是“变更影响怎么控、证据链怎么留”。对强合规行业，系统能否在需求变化时快速定位影响并形成审计材料，往往决定了交付风险与合规成本的上限。<br/>项目/交付管理能力：更偏需求工程与验证闭环；测试侧能力上，Jama Connect 会自动建立测试用例与测试运行之间的追溯关系并在 Trace View 显示。<br/>适用场景：医疗、汽车、工业控制、航空航天等高风险/强合规产品；或“软硬件协同”场景下，对需求一致性与验证闭环要求很高的组织。<br/>优势亮点：VP 视角 ROI 主要来自风险下降：返工减少、验证更可控、合规更稳；而不是单点效率提升。<br/>局限与使用体验：方法论与流程要求更严肃；若组织只想要轻量需求池，会觉得“过重”。</p><p><img width="723" height="415" referrerpolicy="no-referrer" src="/img/bVdnofz" alt="" title="" loading="lazy"/></p><h2>常见问题 FAQ：</h2><p><strong>Q1：产品管理系统和项目管理系统有什么区别？</strong><br/>A：项目管理系统更关注“按计划推进任务”；产品管理系统更关注“为什么做、先做什么、如何对齐路线图，以及如何与交付/追溯闭环”。如果缺少优先级与路线图能力，往往更像项目管理。</p><p><strong>Q2：中大型企业一定要两套系统（产品 + 交付）吗？</strong><br/>A：不一定。关键在 SSOT 放哪，以及同步是否可持续。平台型一体化能降低双系统成本；上游产品系统 + 工程系统组合则更灵活，但治理要求更高。</p><p><strong>Q3：选产品管理系统最常见的 3 个坑是什么？</strong><br/>A：把“功能演示”当“落地能力”；忽略字段/流程治理导致口径失控；低估集成与数据一致性的长期维护成本。</p><p><strong>Q4：POC 做多久比较合理？</strong><br/>A：建议 6–8 周：第 1–2 周对齐需求分层与字段口径，第 3–6 周跑 1 条业务线真实需求闭环，第 7–8 周复盘度量口径与推广成本。</p><p><strong>Q5：为什么我更强调追溯（Traceability）？</strong><br/>A：因为追溯决定你能否在变更发生时快速评估影响与风险，并形成可审计证据链。对强内控/强合规企业，这是“成本与风险”的硬约束。</p>]]></description></item><item>    <title><![CDATA[【已结束】AgentScope Java 和 AgentRun 邀您参与 PolarDB 开发者大会]]></title>    <link>https://segmentfault.com/a/1190000047578363</link>    <guid>https://segmentfault.com/a/1190000047578363</guid>    <pubDate>2026-01-28 17:04:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>第三届 PolarDB 开发者大会</p><p>📍 1 月 20 日，上海 · 五角场凯悦酒店</p><p>作为 AI 时代下的云原生数据库领域开年技术盛宴，大会不仅聚焦“AI 就绪的云原生数据库”的前沿实践，呈现 30+ 场技术演讲；更是携手各社区伙伴，一起带来数场 AI 互动体验，用真实体验、互动来感知 AI 时代的数据库，感受数据+AI 的无限可能。</p><p><strong>AgentScope Java：Agentic LLM 应用开发框架</strong></p><p>AgentScope Java 是以 Agentic 为核心设计理念，面向 Java 开发者的 LLM 应用开发框架。包括核心层、Studio、RL、Memory，以及架构上全力推进 Serverless 化，实现毫秒级冷启动与混合部署，帮助开发者在应对高并发的同时，显著降低部署成本并提升效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578365" alt="image" title="image"/></p><p><strong>AgentRun：一站式 Agentic AI 基础设施平台</strong></p><p>函数计算 AgentRun 是以高代码为核心的一站式 Agentic AI 基础设施平台，秉持生态开放和灵活组装的理念，为企业级 Agent 应用提供从开发、部署到运维的全生命周期管理，让 Agentic AI 真正进入企业生产环境。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578366" alt="image" title="image" loading="lazy"/></p><p>现场还有</p><p>《PolarDB AI 能力集》</p><p>《AI 原生应用架构白皮书》</p><p>等您来领取</p>]]></description></item><item>    <title><![CDATA[Opentelemetry koko ]]></title>    <link>https://segmentfault.com/a/1190000047578376</link>    <guid>https://segmentfault.com/a/1190000047578376</guid>    <pubDate>2026-01-28 17:04:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OpenTelemetry（OTel）是一个统一的可观测性框架，用于采集应用程序的日志、指标和链路追踪，并可将数据发送到不同后端进行存储和分析</p><h2>概览</h2><ul><li>Logs: 应用程序日志</li><li>Traces：日志追溯码</li><li>Metrics：应用程序监控指标</li></ul><h3>流程图</h3><pre style="display:none;"><code class="mermaid">flowchart LR
    A[应用程序] --&gt;|OTel SDK| B[OTel Collector]
    B --&gt; C[日志存储 Loki / ES]
    B --&gt; D[指标存储 Prometheus]
    B --&gt; E[链路追踪存储 Tempo / Jaeger]
    C --&gt; F[可视化 &amp; 查询 Grafana]
    D --&gt; F
    E --&gt; F
</code></pre><h2>Otel SDK</h2><p>要使用 Otel 需要在应用里嵌入对应语言的 <a href="https://link.segmentfault.com/?enc=Zm64iiSn2ya29q1zcqDpCg%3D%3D.oKwoDRdJtKD9oR4T4meG8%2FNHhxo5AZpbSwE30Q8Y63Uyx1Fw4VAtPZocjVSs%2BV09" rel="nofollow" target="_blank">SDK</a> 来产生可观测数据, 导出到对应的 <a href="https://link.segmentfault.com/?enc=l3malpuuOAoLYGnZKVUuLg%3D%3D.Iud%2BLDy3s%2BHa%2BFjObYpEXo2PnT%2BYAkEQNN3I3%2BcdEhO%2F4T3GxyHHZ%2BwXs5mjO5t%2B" rel="nofollow" target="_blank">Collector</a></p><h2>Collector</h2><p><a href="https://link.segmentfault.com/?enc=WwWla%2FIvZmpeaI1H70mtbg%3D%3D.J%2BTUQfvRO8AR4kAx72TIiERQ1gqKrW5rdv4FIUDvTmCO3XYsMFpRx53rDKKF9WXQ" rel="nofollow" target="_blank">Collector</a>：用于采集应用程序的 <strong>日志、指标和链路追踪</strong>，并可以统一处理和转发到不同后端</p><p><strong>流行的 Collector</strong></p><ul><li><strong>OpenTelemetry Collector（OTel Collector）</strong>：官方推荐，支持多协议输入、处理和导出</li><li><strong>Vector</strong>：高性能日志和指标采集器，支持多种后端</li><li><strong>Fluentd / Fluent Bit</strong>：主要用于日志采集，也可与 OTLP 配合</li></ul><h2>Storage Backend</h2><p>在 Otel 中，<strong>存储后端(Storage Backend)</strong> 是用于<strong>持久化存储并分析</strong>应用产生的可观测性数据的系统</p><blockquote>实际生产中，<strong>一个存储后端可能只负责一种数据类型</strong>，也可能（如 ClickHouse）同时承载多种数据。</blockquote><p><strong>流行的存储后端</strong></p><ul><li><p>Logs (日志存储)</p><ul><li>Loki</li><li>ClickHouse</li><li>Elasticsearch</li></ul></li><li><p>Metrics (指标存储)</p><ul><li>Prometheus</li></ul></li><li><p>Traces (链路追踪)</p><ul><li>Tempo</li><li>Jaeger</li><li>Zipkin</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[企业数字化生存底线 JoySSL深度剖析企业部署数字证书的核心动因 完美的铁板烧 ]]></title>    <link>https://segmentfault.com/a/1190000047578393</link>    <guid>https://segmentfault.com/a/1190000047578393</guid>    <pubDate>2026-01-28 17:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在竞争日益激烈的商业环境中，无论是企业的官方网站、客户服务系统、内部管理工具还是移动应用后端，它们不仅仅是信息传递的窗口，更是推动企业业务增长、维系客户关系的核心动力。然而，是否全面部署SSL证书，一个看似基本却影响深远的抉择，会从根本上改变企业未来的发展方向。JoySSL市场经理指出，多数企业对SSL证书并没有完整或清晰的概念，认知仍存在明显的差距。部分决策者将其简单地理解为“满足浏览器的要求”或“让URL显示小锁标志”的技术设置。一部分企业责任人甚至直接认为数字证书不过是网站的附属功能，可有可无。实际上，SSL证书的部署不仅是企业迈入值得信任的数字商业环境的一道“强制性入门门槛”，更是一项具备战略意义的信任投资。这一部署的重要性，贯穿企业线上营销的始终。 </p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnNtw" alt="" title=""/></p><p><strong>积极响应政策履行合法合规义务</strong></p><p>随着全球数据保护和网络安全的监管力度持续加大，合规性变得更加不可忽视。《网络安全法》、《数据安全法》和《个人信息保护法》等法规明确要求，网络运营主体采取技术手段保护数据传输的安全性，防止泄露、窃取或篡改。实现全站HTTPS加密，是“采用加密技术”等法定要求的直接且广泛认可的实践方法。 </p><p>在金融支付、电子商务、医疗健康以及政务服务等领域，明确要求对敏感信息的传输进行加密保护。若未启用有效SSL证书，企业可能遭遇业务合作上的限制、审计失败甚至失去运营资格的风险。 </p><p><img width="723" height="474" referrerpolicy="no-referrer" src="/img/bVdnNty" alt="" title="" loading="lazy"/></p><p><strong>部署SSL证书构建安全网络防线</strong></p><p>攻击者通常选择防御体系的薄弱点作为突破口，未加密的HTTP通信在数据传输过程中极容易被窃取。通过SSL证书构建加密通道，可有效防止数据窃听。OV或EV证书，可对企业实体进行严格审核，将认证的公司身份与站点紧密绑定，从根本上防范钓鱼攻击与身份伪造，从而构筑能够抵御网络风险的防线。</p><p><strong>守护企业品牌形象 提升竞争力</strong></p><p>主流浏览器会对未启用HTTPS的网站显示“不安全”的标记，导致用户对网站产生不信任感，有损品牌形象。</p><p>主动引入高等级SSL证书，展示绿色企业名称，可以向用户传递“专业、可信”的强烈印象。将安全性提升为品牌竞争优势，使安全投入成为商业收益的驱动力。</p><p><img width="723" height="472" referrerpolicy="no-referrer" src="/img/bVdnNtA" alt="" title="" loading="lazy"/></p><p><strong>提升搜索排序解锁现代网络能力</strong></p><p>部署SSL证书可提升网页加载速度与性能，用户体验也会进一步提升。此外，以小程序为代表的平台生态，均需遵循HTTPS标准。JoySSL优化总监表示，谷歌与百度等主流搜索引擎已明确将HTTPS视为重要的积极排名因素，因此，部署SSL证书不仅能改善自然搜索排名，还能吸引高质量的免费流量，为企业带来商机。</p><p><strong>构筑数字信任体系定义企业未来</strong></p><p>部署SSL证书早已超越了普通的IT支出范畴，是企业合规运营的基础，是防范风险的重要手段，是赢得用户信任的纽带，是提升企业竞争力，构筑信任体系的驱动器。面对充满不确定性与风险的数字环境，全面应用SSL证书，等同于为企业的数字化愿景塑造牢不可破的信任基石。</p>]]></description></item><item>    <title><![CDATA[阿里云云原生团队热招！欢迎加入 AI 工程化顶级赛场 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578395</link>    <guid>https://segmentfault.com/a/1190000047578395</guid>    <pubDate>2026-01-28 17:03:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>我们是谁</h2><p>我们是中国最大云计算公司的基石——云原生应用平台。</p><ul><li>我们掌管着应用构建的核心命脉，孵化了 RocketMQ、Higress、Nacos、Dubbo 等多个世界级开源项目。</li><li>我们 SLS、Kafka 引擎每日处理来自亿级终端，百 PB 级数据量的应用数据，承载百万级实例应用，每日处理来自十万研发亿级的分析任务</li><li>我们为 Agent 应用提供手与脚与舞台，通过调度技术、数据工程、语义分析承载 万亿级Token 流量</li></ul><p>过去，我们定义了中国的云原生标准；现在，我们正在全面转向 AI，致力于打造 AI 时代的最强基础设施。</p><h2>我们在做什么</h2><p>我们不制造大模型，但我们让 AI 应用跑得更快、更稳、更便宜、更智能。我们正在寻找极客、架构师和算法专家，突破以下前沿领域：</p><ul><li>计算重构：从 K8s 到 Serverless AI， 打造异构算力与 Agent 执行的“新躯体”。</li><li>架构演进：从微服务到 Agent 互联，重新定义 AI 时代的网关与神经系统。</li><li>认知工程：从数据流到 Agent 记忆， 利用搜索与上下文技术，构建智能体的“海马体”与“感知层”。</li><li>智能治理：从监控到自动驾驶（AIOps）， 让基础设施具备自我进化的生命力。</li></ul><h2>我们需要你</h2><ul><li>对技术有极致的品味，渴望挑战内核级、高并发、分布式的世界级难题。</li><li>既有仰望星空的想象力（相信 AI 改变世界），又有脚踏实地的工程力（Code is Law）。</li><li>熟悉 Golang/Java/C++ 或 Python，对 Kubernetes、Serverless 或 AI 工程化有独到见解。</li></ul><p><a href="https://link.segmentfault.com/?enc=zw1x4HXCSA87JrnLzoeP8A%3D%3D.l2H42HhJRXCiwBM8NpRFi1tEpN5arjFlndhdnn3D%2Bly1aCift52NTauoPKaaDvlj" rel="nofollow" target="_blank">点击此处进入心仪岗位通道</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578397" alt="image" title="image"/></p>]]></description></item><item>    <title><![CDATA[达梦 & 人大金仓适配实战：SeaTunnel 在信创数据平台中的应用与踩坑总结 SeaTunnel]]></title>    <link>https://segmentfault.com/a/1190000047578410</link>    <guid>https://segmentfault.com/a/1190000047578410</guid>    <pubDate>2026-01-28 17:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578412" alt="Apache SeaTunnel 适配海报" title="Apache SeaTunnel 适配海报"/></p><p>作者 | 三线程序员<br/>Tags | MySQL Doris PG 达梦 金仓<br/>关键词 | SeaTunnel、DolphinScheduler、信创、国产、达梦、人大金仓</p><ul><li>适用版本：apache-seatunnel-2.3.8+、apache-dolphinscheduler-3.1.7、人大金仓8.6/9.x</li><li>预估阅读：10 min</li></ul><p>[toc]</p><h2>一、为什么要写这篇</h2><p>集团内部关于数据平台近期遇到了两次异构数据源的问题，洽好利用了开源工具简单应对，验证了自己目前工作的思路，正好总结一下分享过程中的收获也经验。以下只谈技术方案选择与经验分享，不讨论数据量、性能、安全等其它内容。</p><p>a) <strong>数据中转归集</strong>：现有数据平台需要将部分数据数据上报给行业平台，同时还要将另一条第三方物联数据做数据归集中转后再进行上报行业平台。。<br/>b) <strong>国产化信创可控切换</strong>：明年技术平台指标项信创切换的前期验证工作，需要验证业务系统与数据平台一体信创国产化信创切换风险验证，将现有 MySQL → 达梦 / 人大金仓 之间做迁移。</p><p>根据二三线城市实际公司和技术水平情况、调研了数据采集/集成项目后，暂定 Apache SeaTunnel 的核心原因：</p><ul><li>插件式架构，Source/Sink 支持 100+，新增国产库只需改 JDBC Driver；  考虑使用SeaTunnel 进行导入数据，同时考虑datax做为备用方案；（原则seatunnel支持自动建表，datax只支持导入无法自动建表，需要手动建表工作量较大。）</li><li>天然集成 DolphinScheduler，调度方便可观测性及管理运维易用性高；</li></ul><p>笔者在整个过程中趟了不少坑，经验在四五六节中进行了总结，因此成文，给社区回流经验，也作为内部复盘的内容。</p><h2>二、整体需求</h2><ol><li>利用 SeaTunnel 的 jdbc source和达梦专用sink实现数据数据上报，由于上报表比较多，需要利用seatunnel的自动建表和字段映射解决过程中兼容问题；</li><li>使用人大金仓数据库替换数据平台webDB和ds的调度持久化DB，同时验证seatunnel做为数据平台的数据采集模块的延伸方案(原有为doris jdbc catalog),读写kingbase数据库进行数据采集计算；</li></ol><h2>三、前置条件</h2><table><thead><tr><th>内容项</th><th>要求说明</th></tr></thead><tbody><tr><td>目标库</td><td>达梦数据库，人大金仓数据库 V8.6以上，账号具备 <code>SELECT, SHOW VIEW等</code> 权限</td></tr><tr><td>相关数据库jdbc驱动依赖jar包</td><td>connectors目录：connector-jdbc-2.3.12.jar                                                                                                                                                             lib目录：达梦DmJdbcDriver8.jar、金仓kingbase8-8.6.0.jar、mysql-connector-j-8.3.0.jar、postgresql-42.7.3.jar</td></tr></tbody></table><h2>四、安装测试运行</h2><p>有经验的朋友可直接跳过，本节主要介绍个人遇到的一些安装注意事项。</p><h3>1. 安装一个字，简单快捷。</h3><p>步骤：下载、解压、安装连接器、测试。（本人暂时只试用了自带的 Zeta 引擎，其它引擎和集群未使用，目前满足离线 ETL 常规需求）</p><p><strong>需要重点介绍一下安装连接器</strong>，如果网络不好或者maven懒得改代理、着急快速部署、验证新版本什么的，可以直接修改apache-seatunnel-2.3.8\config目录下的plugin_config文件，只保留需要的连接器；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578413" alt="image-20260121093448378" title="image-20260121093448378" loading="lazy"/></p><p>如我只连常用数据库就保留<code>connector-jdbc</code>，只连DDoris数据仓库就保留<code>connector-doris</code>其它的删除掉或注释掉。具体所需对照可以查看<code>\apache-seatunnel-2.3.8\connectors</code>目录下的<code>plugin-mapping.properties</code>文件，里面有详细的source和sink所需要对应的连接器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578414" alt="image-20260121093410783" title="image-20260121093410783" loading="lazy"/></p><p>配置好了直接运行脚本就可以了，进目录<code>cd apache-seatunnel-2.3.8/</code>安装命令<code>sh bin/install-plugin.sh 2.3.8</code>，不指定版本号注安装当前版本的；安装完毕，你的connector目录就会多出许多连接器jar包。老手熟的话可以不安装（panda哥就没用过），直接从原有安装机器或本地把下载好的连接器，手动传上去也可以正常运行。</p><p>这里有个神奇的情况，在Windows环境下有时候连接器历史下载过可能重新部署后没再次下载，仍然可以运行。但在某一个特定的时间点就又开始报错说缺少jdbc连接器。神奇的系统。</p><p><img referrerpolicy="no-referrer" src="" alt="img" title="img" loading="lazy"/></p><h3>2. 这里有两个概念需要理解一下</h3><p>一个是<strong>连接器：</strong> 既使用什么方式进行数据连接，常见的http、文件、数据库jdbc。（一般运行时报什么jdbc错误，八成是没下载jdbc连接器。install-plugin没？）</p><p>一个是<strong>驱动包：</strong> 特定数据源的连接驱动、常见的mysql、pg等。（一般运行时连接失败，九成是没放对应的数据库驱动。）驱动包要自己<strong>&lt;u&gt;手动扔啊，手动，手动&lt;/u&gt;</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578415" alt="image-20260121102558879" title="image-20260121102558879" loading="lazy"/></p><h3>3.测试demo</h3><pre><code># 切换工作目录至Apache SeaTunnel 2.3.8的安装目录
cd /opt/apache-seatunnel-2.3.8/

# 执行SeaTunnel批处理任务
# 参数说明：
# --config：指定任务配置文件路径，此处为默认的批处理配置模板
# -m local：指定运行模式为本地模式（无需集群环境）
./bin/seatunnel.sh --config ./config/v2.batch.config.template -m local</code></pre><p>运行时需要注意的就是windows命令行乱码，字符集换行符什么的这些问题，最一统的解决方案就是别直接windows的传linux上去混用，大不了重写或贴过去。<em>cmd运行时控制台中文信息乱码解决是 chcp 65001</em> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578416" alt="image-20260121102357285" title="image-20260121102357285" loading="lazy"/></p><p>⚠️ 再次提醒，无论运行什么类型的etl，涉及的连接器和驱动包要保证都有，报错时第一时间核对这个，不要死盯着报错重试了。特别是在Windows环境下，Linux大法还是好。</p><h3>4. 小分享</h3><p>作业文件习惯单独建一个job目录存放。（与DolphinScheduler集成有时间再写吧，欠的东西太多了。）</p><p>常用conf样例，可直接cv修改，注意Doris作为sink写入时使用的是streamload方式，要用对应的http端口，不是Navicat连接的端口（大年纪程序员经常忘）:</p><p>&lt;u&gt;mysql 2doris样例&lt;/u&gt;</p><pre><code>env {
    parallelism = 2
    job.mode = "BATCH"
}
source {
    Jdbc {
        url = "jdbc:mysql://192.168.0.31:3306/cons"
        driver = "com.mysql.cj.jdbc.Driver"
        connection_check_timeout_sec = 100
        user = "root"
        password = "123456"
        table_path = "cons.community_info"
        query = "select * from cons.community_info"
    }
}
sink {
    Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
        table = "ods_xyz_communityinfo_base"0
        sink.enable - 2pc = "true"
        sink.label - prefix = "test123"
        doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
}</code></pre><p>&lt;u&gt;mysql 2doris 多表样例&lt;/u&gt;（<strong>有复杂业务需要json一条数据变拆分成多行的可参考</strong> <a href="https://link.segmentfault.com/?enc=UMQfgYJcfUnnjcxjnxDAkg%3D%3D.RMFMeqbmKyjA2ZU1orN3SpkikCESFyyc4RyQznfo%2FyCqbB1xfud%2BrUBmImjhSflOVO2y3Zivx6R1SpVacqBeig%3D%3D" rel="nofollow" target="_blank"><strong>https://github.com/apache/seatunnel/issues/7961</strong></a> <strong>使用<strong><em><em>SELECT * FROM fake LATERAL VIEW OUTER EXPLODE(cpe_nodes) as cpe_nodes</em></em></strong>函数</strong>）</p><pre><code>env {
  job.mode = "BATCH"
  parallelism = 4
}
source {
  Jdbc {
    url = "jdbc:mysql://192.168.0.31:3306/cons"
    driver = "com.mysql.cj.jdbc.Driver"
    connection_check_timeout_sec = 100
    user = "root"
    password = "qianhe999"
    "table_list"=[
        {
            "table_path"="cons.gas_alarm_events"
        },
        {
            "table_path"="cons.gas_check_dispatch"
        }
    ]
    
}
sink {
  Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
    sink.enable - 2pc = "true"
        sink.label - prefix = "test123"
    table = "ods_xyz_${table_name}_base"
        doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
}</code></pre><p><em>自动建表模板doris版本</em></p><pre><code>save_mode_create_template = """CREATE TABLE IF NOT EXISTS ${database}.${table_name} (
${rowtype_primary_key},
${rowtype_fields},
decoded_project_description  STRING
)
ENGINE=OLAP
UNIQUE KEY (${rowtype_primary_key})
COMMENT '${comment}'
DISTRIBUTED BY HASH (${rowtype_primary_key})
PROPERTIES (
"replication_allocation" = "tag.location.default: 1",
"in_memory" = "false",
"storage_format" = "V2",
"disable_auto_compaction" = "false"
)"""</code></pre><p>&lt;u&gt;http接口2 Doris 样例&lt;/u&gt;（http的主要参考了git的文章 <a href="https://link.segmentfault.com/?enc=yZDftR2JFG4NUqXxTBk4PQ%3D%3D.%2FtzhtBf27dGf0ntbCiz22u7X3296a3HGNspm%2FKEy70ZY6cMREgCYHvKQdUQotJCQ" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/8431</a>）</p><pre><code>env {
  execution.parallelism = 2
  job.mode = "BATCH" 
  checkpoint.interval = 10000 
  }
source {
  Http {
    url = "http://192.168.0.1120:31907/biz-data-service/241211/ABC_o1_XYZ"
    method = "POST"
    format = "json"
    headers = {Accept="application/json",Content-Type="application/json;charset=utf-8"}
    body= "{\"params\":{\"branch\":\"长安区\"},\"size\":10,\"current\":1}"
    content_field = "$.data.records.*"
   schema = {
      fields {
  mc=string 
  dz=string 
  last_update=timestamp
  jd="decimal(20, 5)" 
  id :int
  mplx=string 
  wd=string 
        }
    }
  }
}
sink {
   Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
        table = "ods_xyz_http_base"
save_mode_create_template = """CREATE TABLE IF NOT EXISTS ${database}.${table_name} (
cid bigint NOT NULL AUTO_INCREMENT(1) COMMENT '主键',
${rowtype_fields}
) ENGINE=OLAP
UNIQUE KEY (cid)
DISTRIBUTED BY HASH (cid) BUCKETS 1 
PROPERTIES (
"replication_allocation" = "tag.location.default: 1",
"in_memory" = "false",
"storage_format" = "V2",
"disable_auto_compaction" = "false"
)"""
  data_save_mode = "DROP_DATA" # 默认是追加，这里测试了一下清表。既每次只保留最新一次。
  sink.enable - 2pc = "true"
  sink.label - prefix = "test123"
  doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
 }</code></pre><h2>五、读Doris写达梦数据库操作步骤</h2><p>首先需要确保SeaTunnel能正常运行，需要在Linux服务器库或Windows命令行上进行验证，基础的SeaTunnel本地的Zeta引擎可以正常工作运行。</p><p>如用 <strong>Windows</strong>，可能会出现SeaTunnel今天能运行，明天不能运行的特殊情况（报错内容是“找不到或无法加载主类”）；我没有彻底解决，但在网上找的方案大部分都是java环境变量设置的情况，还有就是关掉命令窗口重新打开。但偶尔有机会确实再次出现，隔天就没事了。神奇的系统！</p><h3>1. 正常情况</h3><pre><code class="sql">-- 官方模板一次通
env {
  parallelism = 1
  job.mode = "BATCH"
}
source {
  Jdbc {
    url = "jdbc:dm://e2e_dmdb:5236"
    driver = "dm.jdbc.driver.DmDriver"
    connection_check_timeout_sec = 1000
    user = "SYSDBA"
    password = "SYSDBA"
    query = """select * from "SYSDBA".e2e_table_source"""
  }
}
sink {
  Jdbc {
    url = "jdbc:dm://e2e_dmdb:5236"
    driver = "dm.jdbc.driver.DmDriver"
    connection_check_timeout_sec = 1000
    user = "SYSDBA"
    password = "SYSDBA"
    query = """
INSERT INTO SYSDBA.e2e_table_sink (DM_BIT, DM_INT, DM_INTEGER, DM_PLS_INTEGER, DM_TINYINT, DM_BYTE, DM_SMALLINT, DM_BIGINT, DM_NUMERIC, DM_NUMBER,
 DM_DECIMAL, DM_DEC, DM_REAL, DM_FLOAT, DM_DOUBLE_PRECISION, DM_DOUBLE, DM_CHAR, DM_CHARACTER, DM_VARCHAR, DM_VARCHAR2, DM_TEXT, DM_LONG,
 DM_LONGVARCHAR, DM_CLOB, DM_TIMESTAMP, DM_DATETIME, DM_DATE, DM_BLOB, DM_BINARY, DM_VARBINARY, DM_LONGVARBINARY, DM_IMAGE, DM_BFILE)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
"""
  }
}</code></pre><h3>2.我跳的坑（读Doris写达梦，流水帐形式就当小说看）</h3><h4>2.1.SeaTunnel使用中遇到的问题</h4><h5>(1) 表或视图不存在</h5><p>手写query正常，动态却不行，<code>generate_sink_sql =true</code>不行。最终需要追加上数据库名称，而且源表是Doris表都是小写，而目标表是达梦表库表字段都是大写，所以会报表不存在。</p><h5>(2) 源与目标表名大小写方式不一致</h5><p>需要追加转换功能，字段大小写也需要转换。</p><p>表转换需要使用Transform，并且追加源表别名与目标表表别名，方便操作。</p><pre><code>transform {
 TableRename {
  plugin_input = "source_doris"
  plugin_output = "desc_dameng"
  convert_case = "UPPER"
  }
}</code></pre><p>字段转换</p><pre><code>sink {
 Jdbc {
  plugin_input = "desc_dameng"
  url = "jdbc:dm://dmhost:2070?schema=X_Y_Z_KFC"
  driver = "dm.jdbc.driver.DmDriver"
  user = "X_Y_Z_USE"
  password = "123456"
  database = "DAMENGKFC"
  table = "X_Y_Z_KFC.${table_name}"
  generate_sink_sql = true
  field_ide="UPPERCASE"
  schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
  data_save_mode="DROP_DATA"
   dialect ="Dameng"   --好像没啥实际意义（说是会根据jdbc连接串推算）
}
}</code></pre><h5>(3) 无法创建表</h5><p>低版本不支持达梦建表(2.3.8就没有达梦的ddl创建表的方法，达梦数据库sink写入时ddl自动建表方法是在2.10后才追加的)，最终升级到2.3.10并下载对应的connector连接包还是不行。</p><p>版本升级最新与更新最新的连接包（中间下载时间太长，使用了从maven上直接手动下载的包，最后对了一下大小都一样。没有问题。。。）又升级到了2.3.12版本也是无法自动建表。</p><p>Dialect方言显示指定也不行（包括相应的lib包也都加上了。。。还是不行，甚至怀疑过达梦的驱动包有问题，是否需要找商厂要个驱动才能用）。</p><p>最终验证与字段注释有关，表注释直接就扔了根本不建。只要表中有字段就报commont 语法解析问题。</p><h4>2.2. 两头走不通的折中方案</h4><h5>(1) 使用达梦迁移工具</h5><p>在测试环境没有问题，在生产环境Doris版本升级了竟然报读取错误了。。。(测试环境是Doris 2.1.3，生产是2.1.9版本，估计是哪有差异。)</p><h5>(2) 手动建表</h5><p>参考MySQL导入达梦，使用Linux的脚本处理dump的SQL脚本 。</p><p>把字段和表注释去除，使用AI处理了一上午，只能把表字段注释去掉。但是无法把字段注释和表注释单独弄到一个脚本中，只能生成一个所谓的纯净建表语句。</p><p>但突然发现导出的数据类型肯定在达梦中无法执行，需要转换。对应到各种不同的类型，这个对应再用手工做一遍，考虑放弃此方案。</p><h5>(3) 灵感闪现-直接删除表的注释</h5><p>通过schema_info直接用sql拼出一堆清注释的语句。</p><p>还有个小波折，差点这个方案也不能用了。就是Doris的默认值，开始转换时使用的SQL</p><p>ALTER TABLE <code>dwd_X_Y_base</code> MODIFY COLUMN <code>filedA1</code> varchar(255) COMMENT "";指定了字段类型，当有字段<code>last_update</code>有默认值时不允许修改。后来仔细查了查官网文档，Doris还是做了人的，有专门单独去注释的语句，不加字段类型就行了。</p><h4>2.3 最终可行方案-半手工方案</h4><p>利用现有的备份库，或者直接重新做个临时备份；思路就是通过备份库去把表建上，再切到正式库去做真实的数据同步。</p><h5>(1) 在备份库上把所有注释去了</h5><p>a. 选择表范围。（只取Xyz的底层数据表，用于分业务去做同步SeaTunnel拼哪些表做同步）</p><pre><code>SELECT   *  FROM   information_schema.TABLES 
WHERE   TABLE_SCHEMA = 'data_test_backup' 
AND ( TABLE_NAME like  'ods_xyz_%'  or TABLE_NAME like  'dwd_xyz%')  ;</code></pre><p>b. 选择去除字段注释的内容。</p><pre><code>SELECT 
  CONCAT('ALTER TABLE ', TABLE_SCHEMA,'.',TABLE_NAME, '  MODIFY COLUMN  ', COLUMN_NAME, '  ',  ' COMMENT "";') AS alter_sql
FROM 
  information_schema.columns 
WHERE 
  TABLE_SCHEMA = 'data_test_backup' 
  AND (table_name LIKE 'ods_xyz_%' OR TABLE_NAME LIKE 'dwd_xyz%')
  AND LENGTH(COLUMN_COMMENT) &gt; 0;</code></pre><p>c. 复制出清除字段脚本，在备份库上直接执行。</p><pre><code>alter table ods_xyz_1001_base modify column filed1 comment "";
alter table ods_xyz_1001_base modify column filed2 comment "";
.....</code></pre><p>ctrl+A 、 ctrl +R 全选运行。</p><h5>(2) 使用备份库把数据第一次自动建表导进去。</h5><p>新建<code>xyz_low1.conf</code>读取备份库建表初始化到达梦，未把多表改成正则去匹配，是方便调试找错。直接把表名扔给AI生成<code>table_path</code>数组，也方便以后做真增量时，直接在sql中追加限制条件。</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
 Jdbc {
  plugin_output = "source_doris"
  url = "jdbc:mysql://backuphost:9030/data_test_backup"
  driver = "com.mysql.cj.jdbc.Driver"
  connection_check_timeout_sec = 100
  user = "XXX"
  password = "******"
  table_list = [
 {
  table_path = "data_test_backup.dwd_xyz_1001_base"
  query = "select * from data_test_backup.dwd_xyz_1001_base"
 },
 {
  table_path = "data_test_backup.dwd_xyz_1002_base"
  query = "select * from data_test_backup.dwd_xyz_1002_base"
 },
 .....
]
}
}
transform {
 TableRename {
  plugin_input = "source_doris"
  plugin_output = "desc_dameng"
  convert_case = "UPPER"
  }
}
sink {
 Jdbc {
  plugin_input = "desc_dameng"
  url = "jdbc:dm://101.2.3.4:2026?schema=X_Y_Z_KFC"
  driver = "dm.jdbc.driver.DmDriver"
  user = "X_Y_Z_USE"
  password = "123456"
  database = "DAMENGKFC"
  table = "X_Y_Z_KFC.${table_name}"
  generate_sink_sql = true
  field_ide="UPPERCASE"
  schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
  data_save_mode="DROP_DATA"
  dialect ="Dameng"
}
}</code></pre><h5>(3) 切回同步从库直接读取最新数据</h5><p>复制备份库的配置文件，修改数据库ip地址端口密码等信息，就可直接运行了。</p><h5>(4) 同时拼出来在达梦里需要追加的语句</h5><p>追加表注释。</p><pre><code>SELECT   CONCAT('COMMENT ON TABLE ', upper(TABLE_NAME), ' IS ''', TABLE_COMMENT, ''';')  
FROM   information_schema.TABLES 
WHERE   TABLE_SCHEMA = 'data_test' 
AND (table_name LIKE 'ods_xyz_%' OR TABLE_NAME LIKE 'dwd_xyz_%')</code></pre><p>在达梦数据库上执行！把自动建表的表注释补出来。</p><p>有条件追加字段注释(当时任务急，未来可期你懂的)。</p><h5>(5) 加工层导入时的遇到的新问题(ads层字段创建不规范导致)</h5><p>个别语句有问题的需要调整修改一下，记得重新把先表删除了，再改配置文件中的内容。</p><pre><code>{
  table_path = "data_test_backup.ads_xyz_1001_agg"
  query = "select label_type,`sum(total)` as  'totalnum',sord_num from   data_test_backup.ads_xyz_1001_agg"
 },</code></pre><h4>2.4 配个定时就OK</h4><p>Windows、Linux直接脚本定时，或者集成ds进行配置可视化任务。</p><p>Windows下的bat脚本</p><pre><code>@echo off
rem 先切到 SeaTunnel 的 bin 目录
cd /d "E:\apache-seatunnel-2.3.12-bin\apache-seatunnel-2.3.12\bin"
rem 执行作业
seatunnel.cmd --config ./job/xyz_low.conf -m local</code></pre><h3>3. 写达梦数据库的总结</h3><p>通过学习达梦数据库，笔者发现它本身就是Oracle的魔改版本，有点像把PG和Oracle捏在了一起，加了个PG的Schema，语法全是Oracle的。笔者主要利用了SeaTunnel的自动建表功能，特别是字段类型映射转换节省了大量时间，但研究的时间也不短。</p><p>同时，笔者也发现了一些问题，比如表注释会丢弃，这些还好，反正就是一次性的事手动补一下，直接用sql生成一下脚本即可。</p><p>在报错调试方面，似乎由于线程的问题，会把同线程的其它表的无错的内容报成异常打印出来。</p><p>还有就是关于表结构的问题，要注意调试过程中手动删除表，因为默认使用的参数是<code>schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"</code>，如果表已经存在不会再创建表，容易造成建的表有问题，而发生奇怪的异常报错。</p><h3>4. 另一时间线</h3><p>后续平台又需要与第三方物联系统做数据对接，就直接利用了Doris stream load技术来实现，分享一下经验：最终需要将http接口外网暴露的地址是Doris的be端口，而非fe端口。</p><p>中间还验证了一个拉的方式，就是利用SeaTunnel的http连接器，去拉数据。这里有个小问题，就是需要做鉴权，有时间会再做个分享。（方法很low但验证可行。）</p><h2>六、读写人大金仓数据库操作步骤(信创)</h2><p>信创就是我人生的至暗时刻，刚经历了达梦又得弄Kingbase，但最终对自己个人成长还是有助力的，不说信创数据库怎么兼容的各种问题吧，在时下这个环境换个角度看，这可能就是一种“技术壁垒”。也没时间写内部技术文档了，直接从头回忆吧。</p><h3>1. 坑Kingbase初理解</h3><p>先说开放性kingbase至少比达梦强，官网给下载安装程序包，包括安装版和docker版本，还可以免费申请测试的授权证书，开发授权最多有1年的试用期。这一点就敞亮、局气。首先和身边的前同事（现在还是好朋友）打听了一下，他们之前试用过，大概就是Kingbase是个套壳，底层是pg，改了改几个函数，论开放性有个叫瀚高的更开放，基本没有魔改的，本着原生的一致进行了二开。</p><p>然后运维大哥三下五除二就把docker拉起来，高高兴兴选择了下MySQL模式，结果MySQL的驱动不能直接连，必须要用Kingbase的原生，中间省略各种问题，最终又装了一个pg模式的。</p><p><strong>概念就一个</strong>：Kingbase有多种兼容模式，mysql/pg/sqlserver什么的。。。理论上不考虑这个兼容模式用Kingbase原生的驱动肯定都能连接。如果知道具体的兼容模式，可以尝试用兼容的驱动连接。如pg模式直接用pg驱动就可以连接，但MySQL模式Navicat就不能用MySQL的驱动连接。</p><p>KSQL是Kingbase自己的连接工具，有必要也安装一个，它的驱动就是用的Kingbase原生的驱动。</p><h3>2. 预期设想</h3><p>听劝MySQL兼容模式不好用，咱就用底层原生的最稳定了，当年kmx直接用的Cassandra读的妥妥的好。那就准备用<strong>Kingbase的pg兼容模式做为源和目标了</strong>。</p><p>在SeanTunnel官方文档上查了一下，支持Kingbase，但是，但是，但是，只有部分类型兼容！又在技术群里圈了一下<strong>panda</strong>大佬交流了Kingbase的读写情况，收获良多，再次感谢！！！感谢！！！感谢！！！</p><p>jdbc:kingbase8的不归路就开始了....（这里source和sink的库都是Kingbase的pg模式）</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "com.kingbase8.Driver"
    url = "jdbc:kingbase8://192.168.0.31:4322/sourcedb"
    user = "kingbase"
    password = "123456"
   query = "select *  from source_user_detail"
   }
}
sink {
    jdbc {
        url = "jdbc:kingbase8://192.168.0.119:4322/targerdb"
        driver = "com.kingbase8.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
       database = targerdb
       table = public.target_user_detail
       #schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
      }
}</code></pre><p>一切顺利，不到“10”分钟就搞定了；当时测试的小遗憾是不能schema_save_mode自动建表。在交流群里吐槽了一下，也感谢迅哥儿和西门分享经验和想法！！</p><p>后来panda大佬要给Kingbase立flag说可以支持，我是测试了不行；panda佬说Kingbase是继承pg的代码都支持，还提醒嘱咐source不能用query，无法自动建表，<strong>要用<code>tabl_path</code>是个坑，让我记到文章里提醒大家</strong>，“造福更多使用者”。最终panda佬可能查了查源码确认了打脸，“Kingbase在建表那块没适配”，但这不是重点。</p><p><strong>重点是：“用pg连接器是可以地，如果你Kingbase本身是pg兼容模式 那可以用pg的，只要元数据检查能通过。那就换成pg驱动和配置试试”，结论就是“把kingbase的pg模式就当成jdbc的pg用”</strong>，而且可以自动建表等参数都能用了。<strong>“pg支持啥它支持啥”</strong>。</p><pre><code>source {
  Jdbc {
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://192.168.0.31:4322/sourcedb"
    user = "kingbase"
    password = "123456"
    table_path = "sourcedb.public.source_user_detail"
 }
}
sink {
    jdbc {
        url = "jdbc:postgresql://192.168.0.119:4322/targerdb"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = targerdb
        table = public.target_user_detail
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
     }
}</code></pre><p>满心欢喜的下班，一切都太顺利了。。。</p><h3>3. 突变大转折</h3><p>业务也要做信创准备，那帮子老古董就咬死了我这祖传代码就是MySQL，信创我也是用金仓的MySQL兼容模式！！！！！我还提前分享了验证结论，告诉他们推荐用pg模式，可是人家业务就是这么横，让我们换pg，我这完全不接受，我找领导去！！！！！可想而知的结果，弄不了就是你们技术不行。我这血压一下子就上去了，@#￥%……&amp;<strong>%……&amp;……&amp;</strong>（&amp;￥%<em>&amp;……（</em><em>（）￥%&amp;……</em>（）￥#￥%</p><h3>4. 背叛</h3><p>这Kingbase的兼容MySQL模式肯定是类型有问题啊，这可怎么办？赶紧找其它办法吧。网上找了有什么Datamover，DataX( 老家伙)，还有一直关注没用过的Tis赶紧弄过来试吧，时间紧任务重，Tis有docker版本，赶紧拉起来。试了一下还真行，点点点就弄好一张表！！！表也建上了数据也导过去了，挺好。</p><p>顺利吗....没过一会，说表的字段都是大写的，Kingbase默认是区分大小写的和pg一样。但是可以通过数据库初始化时指定，Docker下面指定那个参数是起不来的，运维大哥说只能填pg，填不了其它的。又是个两头堵死的情况，像不像达梦？。。。</p><p>简单看了看Tis底层用的DataX，建表语句可以自己修改字段名变小写，但是DataX的脚本不让改，直接拷出来在DataX上执行有问题，看不懂的错误。没时间了研究了。。。</p><h3>5. 赌一把</h3><p>晕晕忽忽一下午，压力大吃碳水多，感觉到压力与生活的影响了，就要自己调节。工作只是工作，还有生活。重新调整饮食，早上有时间还把家里的毛巾洗了洗，心情拉满去上班。</p><p><strong>来吧，再试一把老朋友SeaTunnel</strong>！还是老三样，connector重下，驱动重放，执行文件编码问题。一关一关过呗，MySQL兼容类型有问题，我先跳过那个字段直接写死几个列，先跑一把给给自己信心。</p><p><strong>注意环境有改变：</strong>192.168.0.31:4321 的Kingbase是MySQL兼容模式，192.168.0.119:4322是Kingbase的pg兼容模式。</p><p>所以source要用Kingbase的原生去读，字段转小写的问题，通过SQL先尝试解决，大力出奇迹，这些个牛马的事扔给AI弄；sink保留原来的pg也没事。</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "com.kingbase8.Driver"
    url = "jdbc:kingbase8://192.168.0.31:4321/kingbase"
    user = "kingbase"
    password = "123456"
  query = "SELECT `ID` AS id,`PARENT_ID` AS parent_id,`DICT_LABEL` AS dict_label,`DICT_VALUE` AS dict_value,...REMARK` AS remark FROM public.dict;"
  }
}
sink {
    jdbc {
      url = "jdbc:postgresql://192.168.0.119:4322/datatest"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = datatest
        table = data_test.dim_busi_dict_001
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
     }
}</code></pre><p>经过9*9=81次调试，一把过了。高高兴兴找运维大哥说成了成了，运维大哥问了一句“int型的怎么解决的”？我#$%&amp;&amp;,我绕过去了。。。。晕了忘了个干净。</p><p>信心有了，可现实就是这么冰冷，int类型转换失败....AI说指定source的表结构类型，不管用....sql转换类型也没试成功.....不行，服软，花点钱买Kingbase的产品吧。最多也就这样了。</p><p>Panda佬的那句"用pg连接器是可以地"，我又再次仔细理解了一下。是不是有什么没理解到？我用pg驱动读个Kingbase的MySQL兼容模式，再赌一把？</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://192.168.0.62:4321/kingbase"
    user = "kingbase"
    password = "123456"
   query = "SELECT * FROM public.dict;"
  }
}
sink {
    jdbc {
      url = "jdbc:postgresql://192.168.0.119:4322/datatest"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = datatest
        table = data_test.dim_busi_dict_001
        field_ide="LOWERCASE"
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
    }
}</code></pre><p>最后结局了肯定是过了，再tm不过这文章就没必要写这块了。后来拿Navicat直接连了一下Kingbase的MySQL兼容模式，也能连上。#￥%，原来是自己绕远了。</p><p>赶紧分享给群里的小伙伴，又和panda佬谈了体会，“那挺好啊”，原来世界真的很大，我们只在自己的井里。有些事只是自己没见过，但并不代表这个世界上没有。</p><p>​                                                                                                                                                                                                                               2026.1.21           三线程序员</p>]]></description></item><item>    <title><![CDATA[IPD 需求管理怎么做：从需求基线到CCB变更控制全流程 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047578432</link>    <guid>https://segmentfault.com/a/1190000047578432</guid>    <pubDate>2026-01-28 17:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>需求变更不可避免，真正拖垮交付的是“变更失控”。IPD 需求管理的核心，是把需求从“口头共识”升级为“受控资产”：用需求分层与追溯建立共同语言，用需求基线固化交付承诺，再用 CCB 把变更变成可决策的投资选择。本文给出一套可落地的全流程、角色机制与指标闭环，帮助组织稳节奏、降返工、提质量。</p><blockquote>本文关键词：IPD 需求管理、需求管理、需求变更、需求变更管理、需求基线（Requirements Baseline）、CCB（变更控制委员会）、变更控制（Change Control）、配置管理（Configuration Management）、影响分析、需求追溯矩阵（RTM）</blockquote><h2>为什么“需求没管住”，IPD节奏就一定会崩</h2><p>很多团队以为项目失控是“需求太多”。我更常见到的现实是：需求并不一定多，但“承诺”太轻——轻到可以被一句“这个很急”随时改写。</p><p>在研发现场，需求失控往往呈现为三个连锁反应（也是多数团队在搜索“需求变更怎么管”时真正想解决的问题）：</p><ul><li>没有基线：团队不知道“当前承诺交付的到底是哪一版”。需求列表在变，验收口径也在变，最后只能靠人记忆与拉扯。</li><li>没有统一决策机制：变更由“声音最大的人”决定，项目经理被迫在多个老板之间做“情绪路由”，而不是做项目控制。</li><li>没有影响评估：变更只讨论“要不要做”，很少讨论“会伤到哪里、要付出什么代价、是否有替代方案”。</li></ul><p>这三件事叠加时，IPD 强调的“跨职能并行”会从优势变成放大器：市场、产品、研发、测试、供应链同时在动，但缺少共同的“受控参照物”，于是每个环节都在用自己的版本理解需求。</p><p>从产品研发的经验看，越晚发现需求错误，返工常常越贵。</p><p>NASA 的研究给出过一个非常直观的量化视角：把“在需求阶段发现并修正一个需求错误”的成本定义为 1，若到设计阶段再发现，成本上升到 3–8；到制造/构建阶段 7–16；到集成测试 21–78；到运维阶段甚至可能达到 29 到 1500+。这类数据对硬软结合、集成验证成本高的行业尤其有解释力：系统越复杂，后期返工越容易引发链式成本。</p><p>但作为管理者，我们也要保持理性：并非所有软件项目都能稳定观测到“延迟一定更贵”的效应。成熟的做法不是迷信曲线，而是把重点放在：缩短反馈回路 + 建立变更治理机制上。</p><h2>IPD 需求管理的骨架：把需求纳入配置管理</h2><p>要把“需求变更”管得既稳又快，底层一定要借用系统工程成熟的方法：配置管理（Configuration Management, CM）。</p><ul><li>配置管理：把关键产物（需求、设计、接口、测试等）当作“配置项”，通过基线与变更控制保持一致性。</li><li>需求基线（Requirements Baseline）：在某一时点对“已达成一致的需求承诺”做冻结，作为后续变更评审的参照。</li><li>变更控制（Change Control）：对基线后的任何修改，按流程提出、评估、批准/否决、实施与验证。</li></ul><p>NASA 对“基线”的定义是在某一时点对配置项属性的“达成一致的描述”，并提供一个已知配置来处理后续变更；当前批准的基线会成为后续变更的依据。翻译成研发语言就是一句话：</p><p>只要你对交付结果负责，需求就必须从“讨论对象”变成“受控资产”。</p><p>我建议用“四件套”搭起 IPD 需求管理的骨架，并给出每件套的“最小可用标准（MVS）”，避免一上来就走向重流程。</p><h4>1）需求分层：把“想要什么”变成“必须满足什么”</h4><p>需求不分层，CCB 就会陷入“你说的需求不是我理解的需求”。至少要有三层共同语言：</p><ul><li>干系人/市场需求（Why）：目标人群、场景、价值假设、成功指标</li><li>系统/产品需求（What）：功能、性能、接口、合规/安全、约束条件</li><li>版本交付需求（How far / When）：本次版本范围、验收口径、不可延期项</li><li>最小可用标准（MVS）：一条进入版本承诺的需求，必须同时具备“范围描述 + 验收口径 + 关键约束”。否则它不是需求，是愿望。</li></ul><p>在实践里，建议把“需求分层 + 统一ID + 状态定义”直接固化到系统中——例如在 <a href="https://link.segmentfault.com/?enc=LGl9afGExoOIHujH7lnYcA%3D%3D.BCdG8IjF8gRXxvJbGYBjR8SerEtprLwvzTaYSjX3ZjTik4RcHziUp9pJt7kLUWpp" rel="nofollow" target="_blank">ONES Project</a> 里建立需求池、编写需求并自定义需求状态与属性，再把需求与任务规划进迭代，减少口头协商带来的歧义。</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnwjo" alt="ONES 支持把需求规划至迭代" title="ONES 支持把需求规划至迭代"/></p><p>典型失败模式（反例）：只冻结“要做什么”，但不冻结“验收怎么算完成”，你会得到一个现象：研发认为交付了，测试认为没通过，业务认为没达到预期——每个人都没错，但项目照样延期。</p><h4>2）追溯链：没有追溯，就没有“像样的影响分析”</h4><p>追溯不是为了“好看”，是为了让你在 CCB 上用证据说话：这条变更会影响哪些设计、接口、测试与交付承诺？</p><p>做追溯建议从“最短闭环”开始：需求ID → 设计/接口项 → 测试用例 → 验收结果。这条链跑通，影响分析就有了骨架；以后再逐步扩展到风险、合规、供应链与文档基线。</p><p>现场判断标准：</p><p>如果一条变更在 10 分钟内讲不清影响范围，不是“CCB没效率”，而是“追溯链不足以支撑决策”。</p><p>追溯链最容易“断”在测试与交付环节。像 <a href="https://link.segmentfault.com/?enc=1oLTf3KyufF4u2rTXM01dw%3D%3D.iUNcI7FAcv4duMv0vr31tlTYip5Xx1cCgrqTO7zEXdMLd6WY3Ha6wH2Qg%2B%2FEQJfT" rel="nofollow" target="_blank">ONES TestCase</a> 支持测试用例与需求、任务关联、测试计划与迭代关联，能把“需求—任务—测试—缺陷”这条链更稳地串起来。</p><p><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdnNt9" alt="ONES 需求跟踪矩阵" title="ONES 需求跟踪矩阵" loading="lazy"/></p><h4>3）需求基线：冻结的不是文档，是“交付承诺”</h4><p>很多组织把基线做成“需求列表冻结”，但真正该冻结的是交付承诺：范围、验收口径、关键约束、里程碑假设。因为基线本质是“对某一时点状态的达成一致描述”，并作为后续变更的处理依据。</p><p>你可以把“需求基线包（Baseline Package）”理解成：</p><p><strong>一次版本对业务、对组织、对客户的正式承诺。</strong></p><p>基线包不是只存在于PPT。在版本/迭代层面把承诺落到系统里，后续才好做偏差对比。比如 ONES Project 在实践案例里强调了产品版本与迭代规划；并且在甘特图场景下提供“基线对比”的思路，用来直观看当前与计划偏差。</p><p><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdnNua" alt="ONES 支持基线对比" title="ONES 支持基线对比" loading="lazy"/></p><h4>4）CCB 变更控制：把变更从“情绪”变成“投资决策”</h4><p>成熟组织不会纠缠“要不要做变更”，而是讨论三个更硬的问题：</p><ul><li>批准的收益与代价是什么？</li><li>不批准的后果是什么？（很多时候这才是关键）</li><li>有没有折中方案：延期、降级、分阶段、替代实现？</li></ul><h2>全流程落地：从需求基线到 CCB 变更控制</h2><p>下面给出一套端到端流程。建议 PMO 或系统工程牵头固化为 SOP，并在两到三个版本内跑出稳定节奏。</p><blockquote>摘要版<br/>需求入口 → 需求分解与追溯 → 建立需求基线 → 变更申请（CR）→ 影响分析 → CCB 决策 → 执行验证 → 更新基线与追溯 → 关闭变更单</blockquote><h4>1. 需求入口：先把“入口”管住，后端才不会靠吵架控风险</h4><p>目标：统一入口、统一信息质量，把“讨论成本”前移。</p><p>建议设“入库门槛”（最少字段）：</p><ul><li>来源与目标用户/场景</li><li>价值假设（可量化更好：收入、成本、风险暴露、合规罚则）</li><li>验收口径（DoD：什么算交付完成）</li><li>关键约束（法规、接口、性能、交付窗口）</li><li>初步优先级与紧急性（规则要写清楚）</li></ul><p>常见误区：入口不清晰时，变更会伪装成“补充说明”“临时插单”，绕开治理机制。最后你会发现：CCB不是“变更太多开不过来”，而是“该进CCB的变更从来没进来”。<br/>入口治理的关键是“字段齐全 + 状态可控”。例如在 ONES Project 中，你可以把变更申请作为一种工作项/表单来收敛入口信息，同时利用其“需求池 + 自定义需求状态/属性”的机制，减少信息缺失导致的反复打回。</p><h4>2. 需求分解与追溯：先把“结构化依据”建起来</h4><p>目标：让影响分析可计算、可复核、可追责。</p><p>落地要点：</p><ul><li>每条需求唯一 ID，避免“同名不同义”</li><li>需求拆分以“可验证、可交付”为原则：大需求拆到能被测试与验收</li><li>建立最小追溯链：需求 → 设计/接口 → 测试 → 验收</li><li>对关键需求标注：合规/安全点、关键性能指标、供应链影响</li></ul><p>补一句经验：追溯不是一次性工作，它是“把承诺变成资产”的成本。你付出维护成本，换来的是后期影响分析的确定性与决策效率。另外，追溯链维护最怕“各写各的”。像 ONES TestCase 明确支持用例与需求、任务关联，并把测试计划与迭代关联，能把追溯从“Excel表”推进到“过程资产”。</p><h4>3. 建立需求基线：用“基线包”把承诺讲清楚</h4><p>目标：明确“我们承诺交付什么”，并建立后续变更的参照物。</p><p>基线包建议包含（这份清单本身就是很强的检索与引用片段）：</p><ul><li>基线需求清单（范围、优先级、验收口径、依赖）</li><li>关键接口与约束清单</li><li>里程碑与交付节奏（把范围与计划绑定）</li><li>风险清单与缓冲策略（范围缓冲/资源缓冲/技术预研）</li></ul><p>NASA 强调：基线提供一个已知配置来处理后续变更，当前批准的基线是后续变更的依据。管理动作落地：从这一刻起，任何改动都必须留下“为什么改、谁批准、改了什么、影响如何、如何验证”。</p><p>基线包建议同时“文档化 + 结构化”。文档化用于解释口径与边界，结构化用于后续对比与追踪。比如 ONES Project 与 ONES Wiki 支持“文档关联任务/工作项”，适合把基线包的关键结论与对应需求、迭代绑定起来，减少“决策在群里、执行在系统里、复盘找不到证据”的割裂。</p><h4>4. 变更申请：把“口头插单”变成“可评审的请求”</h4><p>目标：让变更带着信息来，而不是带着情绪来。</p><p>变更单（CR/SCR）最低要素建议包含：</p><ul><li>变更内容（新增/删除/修改）与动因</li><li>关联需求 ID 与基线版本号</li><li>紧急性与业务窗口（是否不可错过）</li><li>初步影响：范围/进度/成本/质量/风险</li><li>备选方案：延期/降级/分阶段/替代实现</li><li>不批准的后果：风险、合规、客户承诺、商业损失</li></ul><p>这一步的本质，是把“我想要”变成“我愿意为代价买单的选择”。变更单最有价值的不是“提交”，而是“字段强约束”。在 ONES Project 中，通过自定义需求状态与属性，可以把“影响分析一页纸”所需的关键字段前置到变更申请阶段，减少 CCB 会议上临时补材料。</p><h4>5. 影响分析：CCB 能不能开好，取决于这一页纸</h4><p>目标：把“要不要做”变成“值不值得做、怎么做更划算”。</p><p>建议用“一页纸影响分析”，强制输出：</p><ul><li>范围影响：涉及哪些需求 ID、交付物、接口</li><li>进度影响：关键路径是否改变，里程碑推迟多少</li><li>成本/资源影响：人天、外采、测试资源、供应链</li><li>质量影响：回归测试范围、缺陷风险、技术债</li><li>风险与安全：合规、安全、可靠性是否受影响</li><li>不批准的后果：推迟/拒绝会带来什么损失或风险</li></ul><p>一句话点破：影响分析不是“把风险写出来就安全了”，而是帮助组织做取舍：这次我们愿意买哪一种代价。</p><p>影响分析要快、要准，离不开“需求—任务—测试—缺陷”的数据贯通。ONES Project 提到与 TestCase 数据互通、并支持一键提 Bug，这类能力能让你在评估质量与回归范围时不至于全靠经验猜。</p><h4>6. CCB决策：用机制替代“拍脑袋”，用章程替代“临时拉群”</h4><p>目标：让组织用同一套规则做取舍，并且决策可复盘。</p><p>建议把 CCB 做成“有章程的治理机制”，至少明确：</p><ul><li>成员构成与表决权：业务、研发、测试、架构/系统工程、质量/合规</li><li>授权阈值：哪些变更项目级 CCB 可决，哪些必须上升到更高层级</li><li>节奏与通道：常规变更走周例会；紧急变更走快速通道但必须补齐记录；小变更按阈值授权给项目经理/产品负责人</li></ul><p>一次高效 CCB 建议做到“三定”：</p><ul><li>定级：紧急 / 常规 / 优化（不同通道、不同 SLA）</li><li>定策：批准、否决、退回补充、进入研究队列</li><li>定责：谁执行、谁验证、谁更新基线、何时关闭</li><li>CCB 开不动的三种典型原因（增强“经验信号”）</li><li>材料不全：变更没有“一页纸影响分析”；</li><li>参照物缺失：没有明确的需求基线版本；</li><li>授权不清：谁能拍板不清晰，会议只能“讨论”，无法“决定”。</li></ul><p>CCB 会议的“决定”一定要变成“可复盘的组织记忆”。ONES Project 明确提到与 ONES Wiki 的协同：文档可以关联任务/工作项。你可以把“CCB 决策纪要、否决原因、替代方案”沉淀在 Wiki，并回链到对应变更单，下一次再出现类似变更，组织就不会重复交学费。</p><h4>7. 执行与闭环</h4><p>目标：防止“会上通过了，现场没变；或者现场变了，组织失忆”。</p><p>落地动作建议固定成三步闭环：</p><p>1）实施与验证：研发实现、自测、测试回归、验收确认；<br/>2）更新配置项：更新需求基线版本号、更新追溯链（RTM）；<br/>3）关闭变更单：记录决策理由与验证证据，沉淀可复盘信息。</p><h2>常见问题 FAQ：</h2><p><strong>Q1：需求基线到底“冻结什么”？</strong><br/>冻结的是“交付承诺”：范围 + 验收口径 + 关键约束 + 里程碑假设，而不只是需求列表。基线要能成为后续变更评审的参照。</p><p><strong>Q2：CCB 一定要很大、很正式吗？</strong><br/>不一定。关键不是规模，而是“章程 + 授权阈值 + 可复盘记录”。小团队也可以做“小型 CCB”，用阈值把小变更下放，把大变更拉上来。</p><p><strong>Q3：影响分析写不出来怎么办？</strong><br/>优先补追溯链：没有需求 ID、接口项、测试用例的对应关系，就很难评估影响。先从“最短闭环追溯”做起，逐步完善；工具层面也建议把“需求—任务—测试用例”的关联关系固化起来。</p><p><strong>Q4：紧急变更怎么处理才不破坏治理？</strong><br/>给紧急变更单独通道：先快速决策、快速止损，但必须补齐“事后记录 + 基线更新 + 验证证据”，否则紧急会变成常态。</p><p>成熟的 IPD需求管理 不是“把流程写得更细”，而是把组织能力做得更强：</p><ul><li>用分层与追溯，让影响分析有据可依；</li><li>用需求基线把交付承诺冻结成“受控资产”（基线是后续变更处理的依据）。</li><li>用 CCB 把变更变成可决策的投资，并通过“实施验证—基线更新—记录闭环”形成组织记忆与复用能力。</li></ul><p>最后再强调一句：变更永远会来。真正的差距不在于谁能“减少变更”，而在于谁能把变更变成组织的可控能力——这才是 IPD 体系建设最硬的底座。</p>]]></description></item><item>    <title><![CDATA[MindSpore ：动静图融合的低代码高性能实践 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047577992</link>    <guid>https://segmentfault.com/a/1190000047577992</guid>    <pubDate>2026-01-28 16:10:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在边缘计算、车载终端等异构硬件场景下，MindSpore 模型部署面临 <strong>“动态调试灵活度” 与 “静态推理性能” 无法兼顾 </strong>、硬件算子适配性差两大核心痛点。本次分享基于 MindSpore 的jit动态编译特性与异构硬件算子重写机制，构建 “动静图混合执行 + 硬件感知算子优化” 的低代码部署方案，实现模型在 CPU/GPU/Ascend/ARM 等多平台的高性能适配 —— 推理延迟降低 65%，代码量减少 40%，同时保留动态图的灵活调试能力，附全流程部署代码与跨平台性能对比。</p><h2>1. 动静图混合执行的精细化控制：调试与性能的平衡</h2><p>场景：动态图（PyNative Mode）支持实时打印中间张量、断点调试，适合模型迭代阶段；静态图（Graph Mode）通过计算图优化实现高性能推理，但调试成本高。传统部署需在两种模式间反复切换，且无法针对不同模块差异化配置。</p><p>MindSpore 技术实践：</p><p>利用jit装饰器的局部编译特性，对模型的高频推理模块做静态编译优化，对低频调试模块保留动态执行能力，同时通过input_signature限制输入形状，避免静态编译的形状敏感问题：</p><pre><code class="python">import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from functools import partial

ms.set_context(mode=ms.PYNATIVE_MODE)  # 全局开启动态图

# 1. 动态调试模块：保留动态执行能力，用于异常检测
class DynamicDebugModule(nn.Cell):
    def __init__(self, debug=True):
        super().__init__()
        self.debug = debug
        self.norm = nn.BatchNorm2d(64)

    def construct(self, x):
        x = self.norm(x)
        if self.debug and ms.get_context("mode") == ms.PYNATIVE_MODE:
            # 动态打印张量形状与均值，辅助调试
            print(f"Debug: tensor shape={x.shape}, mean={ops.mean(x).asnumpy()}")
        return x

# 2. 静态推理模块：用jit装饰器做局部编译优化
@ms.jit(input_signature=(ms.Tensor(shape=[None, 64, 32, 32], dtype=ms.float32),))
def static_infer_block(x):
    """高频推理模块：卷积+残差连接，静态编译优化"""
    conv1 = nn.Conv2d(64, 128, 3, padding=1)
    conv2 = nn.Conv2d(128, 128, 3, padding=1)
    res = conv1(x)
    res = ops.relu(res)
    res = conv2(res)
    return res + x  # 残差连接

# 3. 动静融合的完整模型
class HybridModel(nn.Cell):
    def __init__(self):
        super().__init__()
        self.debug_module = DynamicDebugModule()
        self.static_block = partial(static_infer_block)  # 封装静态模块
        self.classifier = nn.Dense(128*32*32, 10)

    def construct(self, x):
        x = self.debug_module(x)  # 动态执行：调试
        x = self.static_block(x)  # 静态执行：高性能推理
        x = x.reshape(x.shape[0], -1)
        x = self.classifier(x)
        return x

# 效果：动态模块保留调试能力，静态模块推理延迟降低50%；相比全静态图，调试效率提升3倍</code></pre><h2>2. 异构硬件算子重写：针对硬件架构的性能优化</h2><p>场景：MindSpore 默认算子在通用硬件上表现均衡，但在专用架构（如 ARM 的 NEON 指令集、Ascend 的 AI Core）上未充分发挥硬件算力 —— 例如 ARM 端的卷积算子，默认实现未利用向量并行计算，推理效率仅为硬件峰值的 30%。</p><p>MindSpore 技术实践：</p><p>基于mindspore.ops.Custom实现硬件感知的算子重写，针对不同硬件平台注册差异化的算子实现，同时通过PrimitiveWithInfer完成算子的形状推导，确保与 MindSpore 计算图兼容：</p><pre><code class="python">from mindspore.ops import Custom, PrimitiveWithInfer
from mindspore._c_expression import typing

# 1. 定义硬件感知的卷积算子（以ARM NEON为例）
class ARMCustomConv2d(PrimitiveWithInfer):
    @prim_attr_register
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__(name="ARMCustomConv2d")
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size

    def infer_shape(self, x_shape):
        # 推导输出形状：same padding
        h, w = x_shape[2], x_shape[3]
        return (x_shape[0], self.out_channels, h, w)

    def infer_dtype(self, x_dtype):
        return x_dtype

    def get_func(self):
        # 绑定ARM NEON优化的卷积实现（C++编写，通过MindSpore C API调用）
        def neon_conv2d(x, weight, bias):
            from arm_neon_conv import neon_conv2d_impl  # 自定义NEON加速库
            return neon_conv2d_impl(x.asnumpy(), weight.asnumpy(), bias.asnumpy())
        return neon_conv2d

# 2. 硬件算子注册与适配
def get_conv2d(in_channels, out_channels, kernel_size, device_target):
    """根据硬件平台返回最优算子"""
    if device_target == "ARM":
        return Custom(
            ARMCustomConv2d(in_channels, out_channels, kernel_size),
            out_shape=ARMCustomConv2d(in_channels, out_channels, kernel_size).infer_shape,
            out_dtype=ARMCustomConv2d(in_channels, out_channels, kernel_size).infer_dtype
        )
    else:
        # 其他平台使用默认卷积算子
        return nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)

# 3. 模型集成硬件感知算子
class HardwareAwareModel(nn.Cell):
    def __init__(self, device_target):
        super().__init__()
        self.conv = get_conv2d(3, 64, 3, device_target)
        self.relu = nn.ReLU()

    def construct(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x

# 效果：ARM平台卷积算子推理速度提升2.8倍，硬件算力利用率从30%提升至75%</code></pre><h2>3. 低代码跨平台部署：MindIR 导出 + Lite 推理的自动化流程</h2><p>场景：模型部署需经历 “训练→导出→量化→推理” 多步骤，不同平台的部署流程差异大，手动配置繁琐且易出错；同时端侧设备资源有限，需对模型做轻量化处理。</p><p>MindSpore 技术实践：</p><p>基于 MindSpore 的MindIR 统一模型格式，封装 “训练→导出→量化→部署” 的自动化脚本，同时集成后训练量化（PTQ）与算子融合优化，实现一键跨平台部署：</p><pre><code class="python">import mindspore.lite as mslite
from mindspore.compression import QuantizationAwareTraining

# 1. 模型训练与轻量化（PTQ量化）
def train_and_quantize(model, train_dataset, device_target):
    # 训练模型（省略训练循环）
    loss_fn = nn.CrossEntropyLoss()
    opt = nn.Adam(model.trainable_params(), 1e-3)
    train_net = nn.TrainOneStepCell(model, opt, loss_fn)

    # PTQ量化：降低模型体积与推理延迟
    quant_config = QuantizationAwareTraining(quant_dtype=ms.int8)
    quant_model = quant_config.quantize(model)
    # 用校准数据集微调（100样本）
    calib_dataset = train_dataset.take(100)
    for x, _ in calib_dataset:
        quant_model(x)
    return quant_model

# 2. 一键导出MindIR模型
def export_mindir(model, input_shape, export_path):
    input_tensor = ms.Tensor(shape=input_shape, dtype=ms.float32)
    ms.export(model, input_tensor, file_name=export_path, file_format="MINDIR")

# 3. 跨平台推理部署
def deploy_lite(model_path, device_target, input_data):
    # 初始化Lite推理环境
    context = mslite.Context()
    if device_target == "CPU":
        context.target = ["cpu"]
        context.cpu.thread_num = 4
    elif device_target == "GPU":
        context.target = ["gpu"]
    elif device_target == "ARM":
        context.target = ["cpu"]
        context.cpu.thread_num = 2  # 适配ARM端算力

    # 加载模型并推理
    model = mslite.Model(model_path, context=context)
    inputs = [mslite.Tensor.from_numpy(input_data)]
    outputs = model.predict(inputs)
    return outputs[0].asnumpy()

# 自动化部署流程调用
if __name__ == "__main__":
    device_target = "ARM"  # 可切换为CPU/GPU/Ascend
    model = HardwareAwareModel(device_target)
    # 训练量化
    quant_model = train_and_quantize(model, train_dataset, device_target)
    # 导出MindIR
    export_mindir(quant_model, [1, 3, 224, 224], "hardware_aware_model")
    # 端侧推理
    input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
    result = deploy_lite("hardware_aware_model.mindir", device_target, input_data)</code></pre>]]></description></item><item>    <title><![CDATA[2025年CRM系统选型手册：主流厂商能力横向对比及深度解析 晨曦钥匙扣 ]]></title>    <link>https://segmentfault.com/a/1190000047578004</link>    <guid>https://segmentfault.com/a/1190000047578004</guid>    <pubDate>2026-01-28 16:09:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在数字化转型背景下，CRM（客户关系管理）已从“工具”升级为“企业增长引擎”。其核心价值在于通过<strong>标准化流程</strong>提升效率、<strong>全视图客户理解</strong>驱动个性化运营、<strong>移动化能力</strong>适配外勤场景、<strong>数据驱动</strong>优化绩效。本文选取8个主流CRM品牌（超兔一体云、Salesforce、SAP、Microsoft Dynamics 365、Zoho、Freshsales、红圈营销、EC），从四大核心维度展开深度对比，为企业选型提供参考。</p><h2>一、销售流程标准化：从“经验驱动”到“流程驱动”</h2><p>销售流程标准化的核心是<strong>用统一规则替代个人经验</strong>，减少无效动作，提升转化率。其关键指标包括：自定义能力、自动化程度、行业适配性、系统集成度。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：聚焦中小企“多场景跟单”痛点，提供<strong>三大固定模型</strong>——小单快单用“三一客”（三定：定性、定级、定量+关键节点推进）、中长单用“商机跟单”（阶段+预期日期）、多方项目用“多方项目模型”。同时支持<strong>订单</strong> <strong>工作流</strong> <strong>标准化</strong>（锁库、采购计划、供应商直发），流程易落地，适合中小企快速复制高效动作。</li><li><strong>Salesforce</strong>：大企级自定义能力，通过<strong>销售流程构建器</strong>完全自定义漏斗阶段（如“线索→MQL→SQL→商机→成交”），搭配<strong>工作流</strong> <strong>规则</strong>（如“线索评分≥80分自动分配给高级销售”），Einstein AI自动触发任务提醒（如“客户3天未跟进需发送邮件”），实现全流程自动化校验。</li><li><strong>SAP</strong>：依托<strong>ERP-CRM一体化优势</strong>，覆盖14种标准销售场景（跨公司销售、寄售、服务销售等），支持<strong>自定义审批流</strong>（如“订单金额≥10万需财务审批”），实现“订单-生产-交付”全链路流程打通，适合中大型制造企业。</li><li><strong>红圈营销</strong>：针对<strong>快消</strong> <strong>/农牧/服装</strong>等外勤高频行业，提供<strong>标准化拜访流程</strong>（路线规划→到店签到→陈列检查→库存盘点→订单提交→问题反馈），任务自动派发，解决“漏店、虚假拜访、流程不统一”痛点。</li><li><strong>EC</strong> <strong>（六度人和）</strong> ：聚焦<strong>电话销售场景</strong>，提供<strong>流程模板</strong>（开场→需求挖掘→产品介绍→异议处理→促成）、智能拨号（自动过滤空号）、通话录音（复盘话术）、话术库（优秀销售话术共享），快速复制电销精英的成交逻辑。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>自定义能力</th><th>自动化程度</th><th>行业适配性</th><th>集成度</th></tr></thead><tbody><tr><td>超兔一体云</td><td>三大跟单模型、订单工作流、线索一键处理</td><td>中（固定模型内调整）</td><td>中（自动提醒+分配）</td><td>中小企全行业</td><td>中（支持常用工具）</td></tr><tr><td>Salesforce</td><td>自定义漏斗、工作流规则、Einstein自动化</td><td>高（完全自定义）</td><td>高（自动触发任务/校验）</td><td>大企全行业</td><td>高（与ERP/HR/营销集成）</td></tr><tr><td>SAP</td><td>ERP-CRM一体化、14种标准场景、自定义审批流</td><td>中（基于标准场景扩展）</td><td>中（自动化销售任务）</td><td>中大型制造/零售</td><td>高（与SAP ERP深度集成）</td></tr><tr><td>红圈营销</td><td>行业标准化拜访流程、路线规划、任务自动派发</td><td>低（行业固定流程）</td><td>中（自动任务分配）</td><td>快消/农牧/服装</td><td>中（与内部系统集成）</td></tr><tr><td>EC</td><td>电销流程模板、智能拨号、通话录音、话术库</td><td>中（电销流程自定义）</td><td>高（自动线索分配+跟进提醒）</td><td>电销/外勤行业</td><td>高（与微信/企业微信集成）</td></tr></tbody></table><h3>3. 超兔销售流程标准化流程图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578007" alt="" title=""/></p><h3>4. 维度总结</h3><ul><li>中小企快速落地：选超兔（固定模型易操作）；</li><li>大企自定义需求：选Salesforce（完全自定义+AI自动化）；</li><li>制造/零售ERP协同：选SAP（全链路流程打通）；</li><li>快消/农牧外勤：选红圈营销（行业标准化流程）；</li><li>电销团队：选EC（流程模板+智能拨号）。</li></ul><h2>二、客户全视图管理：从“碎片化数据”到“360度画像”</h2><p>客户全视图管理的核心是<strong>整合多源数据</strong>，构建“人-货-场”统一画像，支撑个性化运营。其关键指标包括：数据整合范围、实时性、画像深度、权限管理。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：侧重<strong>数据准确性+权限安全</strong>——支持<strong>个性化配置</strong>（用户画像、客户表布局、列表自定义），自动补全工商信息（天眼查/百度查公司）、手机号查重（模糊匹配简称），数据权限按角色隔离（销售看客户详情、财务看财务数据、老板看全局），适合注重数据安全的中小企。</li><li><strong>Salesforce</strong>：通过<strong>Customer 360平台</strong>整合销售、服务、营销、ERP多部门数据，Einstein GPT自动生成<strong>客户需求预测</strong>（如“客户浏览过产品A，可能需要配件B”）和<strong>个性化沟通话术</strong>（如“针对制造业客户的成本痛点，推荐套餐C”），实现“数据-策略-执行”闭环。</li><li><strong>SAP</strong>：依托ERP协同，构建<strong>全链路客户视图</strong>——整合客户基本信息、订单数据（销量/金额）、信用风险（逾期记录）、满意度（售后评分），与生产系统联动（如“客户订单触发生产计划”），适合中大型企业“从销售到交付”的全链路管理。</li><li><strong>红圈营销</strong>：聚焦<strong>外勤场景数据</strong>——整合客户地理信息（工商地址经纬度）、消费偏好（购买历史/ SKU偏好）、工作记录（拜访次数/反馈问题），生成“地理+行为”画像，支持“按区域推送促销活动”“针对偏好推荐产品”。</li><li><strong>EC</strong>：整合<strong>多渠道沟通数据</strong>——自动记录电话（通话录音/时长）、微信（聊天记录/朋友圈互动）、邮件（打开/点击）的互动历史，生成“客户互动时间线”，支持“根据沟通历史调整话术”（如“客户上周提到价格敏感，本次重点讲优惠”）。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>数据整合范围</th><th>实时性</th><th>画像深度</th><th>权限管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>个性化配置、工商查重、角色权限隔离</td><td>中（线索+客户+订单+工商）</td><td>高（实时同步）</td><td>中（基础+行为+价值）</td><td>严格（同级隔离/上级查看）</td></tr><tr><td>Salesforce</td><td>Customer 360、Einstein GPT、多系统集成</td><td>高（全渠道+内部系统）</td><td>高（实时同步）</td><td>高（基础+行为+预测）</td><td>灵活（九级组织权限）</td></tr><tr><td>SAP</td><td>ERP协同全链路、信用风险/满意度分析</td><td>高（ERP+CRM+服务）</td><td>高（实时同步）</td><td>高（全链路数据）</td><td>严格（与ERP权限一致）</td></tr><tr><td>红圈营销</td><td>地理信息、消费偏好、拜访记录</td><td>中（线下+消费+位置）</td><td>中（实时录入）</td><td>中（行为+地理）</td><td>中（角色权限）</td></tr><tr><td>EC</td><td>多渠道沟通记录、互动时间线、标签化管理</td><td>中（沟通+互动）</td><td>高（实时同步）</td><td>中（行为+沟通）</td><td>中（角色权限）</td></tr></tbody></table><h3>3. 客户全视图管理能力框架脑图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578008" alt="" title="" loading="lazy"/></p><h3>4. 维度总结</h3><ul><li>中小企数据安全：选超兔（查重+权限隔离）；</li><li>大企个性化运营：选Salesforce（Customer 360+Einstein预测）；</li><li>制造企业全链路：选SAP（ERP协同全视图）；</li><li>外勤地理场景：选红圈营销（地理+消费偏好）；</li><li>多渠道沟通：选EC（沟通记录整合）。</li></ul><h2>三、高效移动办公/销售外勤：从“线下记录”到“实时同步”</h2><p>移动办公/外勤场景的核心是<strong>适配“在路上”的工作状态</strong>，实现“数据实时录入+任务实时处理+协同实时同步”。其关键指标包括：移动端功能完整性、离线支持、外勤适配性、协同能力。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：移动端聚焦“销售全流程”——支持多渠道新建客户、公海分配、三一客价值标定，“快目标”模块分解目标到个人（如“本月需跟进20个目标客户”），外勤签到（500米内客户签到），数据实时同步云端，适合中小企外勤人员快速操作。</li><li><strong>Salesforce</strong>：移动端功能全面——支持GPS打卡、语音/拍照录入拜访记录（如“拍客户仓库照片，备注库存不足”）、实时查看客户资料/待办任务，集成Chatter协作（可@同事附件照片/文件），支持离线操作（无网络时录入，联网后自动同步），适合大企外勤团队协同。</li><li><strong>Microsoft Dynamics 365</strong>：依托微软生态，移动端与Teams/Outlook深度集成——支持路线规划（按客户位置优化拜访顺序）、客户位置标注（在地图上显示客户分布）、实时同步邮件/会议纪要（如“Outlook会议自动关联客户档案”），适合微软生态企业。</li><li><strong>红圈营销</strong>：外勤场景“强适配”——支持离线数据录入（无网络时记录拜访信息）、客户位置定位（导航到店）、现场订单提交（直接录入系统触发生产）、路线规划（自动优化拜访路线），解决“外勤数据滞后”痛点，适合快消/农牧高频外勤。</li><li><strong>EC</strong>：移动端聚焦“电销+外勤”——支持一键拨号（自动拨打客户电话）、外勤定位打卡（证明到店）、客户资料实时调取（如“拜访时查看客户之前的通话记录”）、微信/企业微信实时沟通（如“把客户微信消息同步到CRM”），适合电销+上门拜访的团队。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>移动端功能</th><th>离线支持</th><th>外勤适配性</th><th>协同能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>客户管理、三一客、快目标、外勤签到、实时同步</td><td>全（跟进+任务+签到）</td><td>是</td><td>中小企外勤</td><td>中（团队联动）</td></tr><tr><td>Salesforce</td><td>GPS打卡、语音/拍照录入、Chatter协作、离线操作</td><td>全（跟进+任务+协作）</td><td>是</td><td>大企外勤</td><td>高（与Teams/Outlook集成）</td></tr><tr><td>Microsoft Dynamics 365</td><td>路线规划、客户位置标注、Teams协作、Outlook同步</td><td>全（跟进+协作+数据）</td><td>是</td><td>微软生态外勤</td><td>高（与微软工具集成）</td></tr><tr><td>红圈营销</td><td>离线录入、客户定位、现场订单、路线规划</td><td>全（拜访+路线+订单）</td><td>是</td><td>快消/农牧外勤</td><td>中（任务分配+监控）</td></tr><tr><td>EC</td><td>一键拨号、外勤打卡、客户资料调取、微信同步</td><td>全（电销+外勤+沟通）</td><td>是</td><td>电销/上门拜访</td><td>高（与微信/企业微信集成）</td></tr></tbody></table><h3>3. 维度总结</h3><ul><li>中小企外勤：选超兔（功能简洁+实时同步）；</li><li>大企协同外勤：选Salesforce（Chatter协作+离线支持）；</li><li>微软生态：选Microsoft Dynamics 365（Teams/Outlook集成）；</li><li>快消/农牧高频外勤：选红圈营销（离线录入+路线规划）；</li><li>电销+上门：选EC（一键拨号+微信同步）。</li></ul><h2>四、数据分析与团队绩效管理：从“经验判断”到“数据驱动”</h2><p>数据分析与绩效管理的核心是<strong>用数据识别瓶颈</strong>，优化团队动作，提升绩效。其关键指标包括：分析深度、AI能力、可视化程度、绩效关联度。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：侧重<strong>目标拆解+进度监控</strong>——“快目标”模块将公司目标分解到部门/个人（如“本月总目标100万，A销售需完成20万”），用“红绿灯”标识状态，“喜报”功能展示优秀员工（如“XX销售签单10万，排名第一”），适合中小企简单绩效监控。</li><li><strong>Salesforce</strong>：大企级数据分析——内置Tableau分析云，生成<strong>实时销售仪表盘</strong>（如“漏斗转化率：线索→成交转化率15%”“区域业绩：华东区完成率120%”），Einstein AI预测赢单概率（如“商机A赢单概率70%，需重点跟进”），支持多维度数据钻取（如“点击转化率，查看是线索质量低还是跟进不到位”），实现“数据-策略-执行”闭环。</li><li><strong>SAP</strong>：依托ERP数据，提供<strong>全链路分析</strong>——生成销售趋势（如“季度销量增长10%，源于产品B的推广”）、产品性能（如“产品C的退货率5%，需优化质量”）、市场份额（如“在华南区占比20%，需加强推广”），适合中大型企业“从销售到生产”的全链路优化。</li><li><strong>Freshsales</strong>：AI辅助分析——AI助手Freddy提供<strong>客户行为预测</strong>（如“客户最近浏览了价格页，可能要下单”）和<strong>销售趋势分析</strong>（如“本月电销转化率下降，因异议处理环节耗时增加”），支持销售流程管控（如“查看每个阶段的耗时，优化瓶颈环节”），适合中小企AI辅助决策。</li><li><strong>EC</strong>：电销专项分析——实时监控<strong>通话指标</strong>（通话时长、接通率、成单转化率），生成团队排行榜（如“XX销售接通率80%，排名第一”），绩效报表（如“本月电销业绩占比60%，需加强线上获客”），适合电销团队量化管理。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>分析深度</th><th>AI 能力</th><th>可视化程度</th><th>绩效关联度</th></tr></thead><tbody><tr><td>超兔一体云</td><td>快目标分解、红绿灯状态、喜报功能、转化率分析</td><td>中（流程 + 业绩）</td><td>AI分析电话录音，微信沟通内容</td><td>中（数字卡片 + 图表）</td><td>中（目标与行动关联）</td></tr><tr><td>Salesforce</td><td>Tableau 分析、Einstein AI 预测、实时仪表盘、多维度钻取</td><td>高（全链路 + 客户行为）</td><td>高（赢单概率 + 话术生成）</td><td>高（可视化报表 + 预警）</td><td>高（绩效与流程深度关联）</td></tr><tr><td>SAP</td><td>销售趋势、产品性能、市场份额、自定义报表</td><td>高（ERP 全链路数据）</td><td>中（趋势预测）</td><td>中（传统报表）</td><td>中（绩效与 ERP 数据关联）</td></tr><tr><td>Freshsales</td><td>Freddy AI 预测、客户行为分析、销售流程管控</td><td>中（客户 + 销售行为）</td><td>中（行为预测 + 线索评分）</td><td>高（可视化仪表盘）</td><td>中（绩效与销售活动关联）</td></tr><tr><td>EC</td><td>通话指标监控、团队排行榜、绩效报表</td><td>中（电销流程 + 业绩）</td><td>无</td><td>中（排行榜 + 报表）</td><td>高（绩效与电销指标强关联）</td></tr></tbody></table><h3>3. 维度总结</h3><ul><li>中小企简单绩效监控：选超兔一体云（目标拆解 + 进度监控）；</li><li>大企级数据分析：选 Salesforce（实时销售仪表盘 + AI 预测）；</li><li>中大型企业全链路优化：选 SAP（ERP 全链路分析）；</li><li>中小企 AI 辅助决策：选 Freshsales（AI 助手分析）；</li><li>电销团队量化管理：选 EC（通话指标监控）。</li></ul><h2>五、总结</h2><p>在数字化转型的浪潮中，CRM 系统已成为企业提升竞争力的关键工具。通过对超兔一体云、Salesforce、SAP、Microsoft Dynamics 365、Zoho、Freshsales、红圈营销、EC 等 8 个主流 CRM 品牌在销售流程标准化、客户全视图管理、高效移动办公/销售外勤、数据分析与团队绩效管理这四大核心维度的深度对比，我们可以看到每个品牌都有其独特的优势和适用场景。</p><p>企业在选择 CRM 系统时，应根据自身的规模、行业特点、业务需求、预算等因素综合考虑。对于中小企业而言，如果追求快速落地和简单易用，超兔一体云在多个维度都能满足需求；而大型企业若有较高的自定义和智能化需求，Salesforce 则是更优的选择。制造企业注重 ERP 协同和全链路管理，SAP 会是理想之选；快消、农牧等外勤高频行业，红圈营销的行业标准化流程能解决实际痛点；电销团队则可优先考虑 EC 的专业电销功能。</p><p>希望本文的对比分析能为企业在 CRM 系统选型过程中提供有价值的参考，助力企业实现数字化转型，提升运营效率和市场竞争力。</p>]]></description></item><item>    <title><![CDATA[数据工程新范式：NoETL 语义编织如何激活海量埋点数据价值？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047578014</link>    <guid>https://segmentfault.com/a/1190000047578014</guid>    <pubDate>2026-01-28 16:08:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=j7I%2BrZx0K1IwRX1NkFXpQQ%3D%3D.47s3H%2FNJ%2FrtCQ0D6UQZXCzAsgSlP9iQQb5%2BZOFoeOZTbD9y1%2F%2Fd7c2v%2FHHrWe64J%2Fv8UT%2BmnHKo274qu12J7SkyB5mmPbyKag%2FcHlD929fY%3D" rel="nofollow" target="_blank">《如何低成本激活海量用户行为数据价值？NoETL 语义编织实践指南》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：面对海量埋点数据价值释放的困境，传统 ETL 模式在业务灵活性、口径一致性和成本性能间难以平衡。本文提出通过引入 NoETL 语义编织架构，构建统一语义层、实现自动化查询与智能物化，从而打破“不可能三角”，实现秒级自助分析与 AI-Ready 数据底座建设，为数据工程与指标平台实践提供系统指南。</p><p>每天，数亿条用户点击、浏览、停留的埋点数据，正源源不断地涌入企业的数据湖仓。然而，这些本该驱动精准营销、产品迭代和体验优化的“数据原油”，却因传统数据供给模式的瓶颈，长期沉睡，沦为吞噬存储与计算成本的“负资产”。</p><p>现实更为严峻：企业湖仓数据冗余平均在 5 倍以上，而专业数据人才的缺口高达 200 万。这意味着，企业正陷入 “数据越多，价值越难释放” 的怪圈。当业务部门急需一个“高价值用户转化漏斗”的分析时，数据团队往往需要排期数周，通过重复开发宽表来响应，最终产出口径不一、维度固化的报表，无法满足灵活探查的需求。</p><p>问题的根源，在于传统以人工 ETL 和物理宽表为核心的数据供给模式，已无法平衡 “业务灵活性”、“口径一致性”与“性能成本” 的“不可能三角”。而 AI 智能体（Agent）时代的到来，以其发散性、秒级响应的问数需求，彻底击穿了这套勉力维持的旧体系。</p><p>激活海量用户行为数据价值的关键，在于一场从“过程驱动”到“语义驱动”的范式重构——引入 NoETL 语义编织架构。</p><h2>前置条件：认清传统数据供给模式的“不可能三角”</h2><p>在深入解决方案前，我们必须正视当前架构的根本性矛盾。这个“不可能三角”具体表现为：</p><ul><li>业务灵活性：营销、产品等一线部门希望像使用搜索引擎一样，自由组合“渠道”、“用户标签”、“时间周期”等维度，进行探索性分析。但在宽表模式下，维度组合是预定义的，任何未预设的分析路径都需要重新开发。</li><li>口径一致性：管理层要求“GMV”、“活跃用户”等核心指标在全公司有且仅有一个权威定义。然而，指标逻辑被硬编码在分散的 ETL 脚本和物理宽表中，微小的逻辑差异导致报表间“数据打架”成为常态。</li><li>性能与成本：数据团队需要在有限的预算内保障查询秒级响应。为此，他们不得不预建大量宽表和汇总表（ADS 层），导致相同明细数据被反复加工存储，形成巨大的冗余和浪费，陷入“为保障性能而推高成本”的恶性循环。</li></ul><p>这套依赖人力的“人工预计算”范式，在数据量和分析需求激增的今天，已成为数据价值释放的主要瓶颈。解决问题的出路，不是在这个三角中继续做痛苦的取舍，而是通过架构革新，打破三角本身。</p><h2>第一步：架构重构——引入 NoETL 语义编织层</h2><p>解决问题的起点，是将 “业务语义” 与 “物理底表” 彻底解耦。这类似于软件开发从汇编语言（直接操作硬件）演进到高级语言（声明业务逻辑）。</p><p>NoETL 语义编织 的核心，是在企业的公共明细数据层（DWD）与上游的消费应用（BI、AI Agent、业务系统）之间，构建一个独立、统一、具备实时计算能力的 语义层（Semantic Layer）。</p><ul><li>逻辑层（做什么）：业务分析师在语义层中，通过声明式的方式，用业务语言定义指标（如“近30天高价值用户留存率”）、维度及其关联关系。他们无需关心数据存储在哪里、表如何关联。</li><li>物理层（怎么做）：平台的 语义引擎 自动将逻辑定义“编译”为面向底层数据湖仓（如 Snowflake, BigQuery）优化过的高效 SQL 执行计划。无论是实时查询明细，还是智能路由到加速表，都由系统自动完成。</li></ul><p>这种解耦带来了 “无头化（Headless）” 与 “中立性”。数据不再为某个特定的 BI 报表加工，而是成为一种标准化的服务。无论是 BI 工具，还是未来的 AI 应用，都通过统一的 API/JDBC 接口消费同一份经过治理的“逻辑真理”。</p><h2>第二步：能力建设——部署具备三大支柱的指标平台</h2><p>一个合格的 NoETL 语义编织平台，必须具备以下三大核心能力，缺一不可：</p><h3>1. 统一语义层：构建虚拟的业务事实网络</h3><p>平台允许用户在未物理打宽的 DWD 表之上，通过界面化配置，声明式地定义表与表之间的关联关系（如用户表与行为事件表通过 <code>user_id</code> 关联）。由此，在逻辑层面构建出一张覆盖全域的 “虚拟大宽表”，业务人员可在此基础上进行任意拖拽分析。</p><h3>2. 自动化查询生成：意图即 SQL</h3><p>当用户拖拽指标或 AI Agent 提出自然语言问题时，平台的语义引擎能实时解析分析意图，自动生成高效、优化的查询 SQL，自动处理复杂的多表 JOIN、去重和跨层级计算，实现数据获取的零门槛。</p><h3>3. 智能物化加速：基于声明的性能保障</h3><p>这是区别于传统逻辑视图的关键。平台提供 “声明式物化” 能力：</p><ul><li>管理员声明：基于业务需求，声明需要对哪些指标和维度组合进行加速，以及数据时效性要求（如 T+1）。</li><li>系统自治：平台根据声明，自动设计物化视图、编排 ETL 任务依赖并运维。</li><li>透明路由：查询时，引擎自动进行 SQL 改写，让查询命中最佳的物化结果，实现百亿级数据的秒级响应。尤其关键的是，其物化引擎支持对去重计数、比率类等复杂指标进行上卷聚合，突破了传统物化技术的限制。</li></ul><p><img width="723" height="155" referrerpolicy="no-referrer" src="/img/bVdnNnr" alt="" title=""/><br/><img width="723" height="146" referrerpolicy="no-referrer" src="/img/bVdnNns" alt="" title="" loading="lazy"/></p><h2>第三步：实施落地——采用“存量挂载”与“增量原生”混合策略</h2><p>引入新范式无需“推倒重来”。我们推荐采用分阶段的混合策略，平滑演进，保护既有投资：</p><ol><li>存量挂载（保护投资）：对于现有逻辑稳定、性能尚可的物理宽表，直接将其接入语义层，映射为“逻辑视图”并注册指标。实现零开发成本下的统一服务出口。</li><li>增量原生（遏制新债）：对所有新产生的分析需求，尤其是来自 AI Agent 的灵活问数，坚决采用“原生”模式。直接基于 DWD 明细层，通过语义层定义指标，由平台自动化处理计算与加速，从源头杜绝新宽表的产生。</li><li>存量替旧（优化成本）：在平台能力得到验证后，逐步识别并下线那些维护成本高、逻辑复杂的“包袱型”旧宽表，将其逻辑迁移至语义层，释放计算资源。</li></ol><p>一个典型的推广路径分为四个阶段：战略筹备与灯塔选择 -&gt; 价值验证与能力内化 -&gt; 全面推广与组织建设 -&gt; 生态融合与价值深化。核心是从一个痛点明确的业务场景（如“营销活动分析”）切入，快速交付可感知的价值，建立内部信心后再规模化推广。</p><h2>第四步：价值深化——从统一分析到赋能 AI 智能体</h2><p>当统一的指标语义基座建成后，其价值将超越传统 BI，深度赋能 AI 场景：</p><ul><li>为 AI 划定“认知围栏”：语义层提供的结构化、业务友好的指标与维度元数据，是 RAG（检索增强生成）的优质语料。AI Agent 不再需要直面晦涩的物理表 Schema 去“猜测”SQL，而是通过 NL2Metrics（自然语言转指标查询） 模式，调用标准的语义 API（如 <code>GetMetric(name=”毛利”, filter={region:”华东”})</code>），从根本上降低幻觉风险。</li><li>提供深度分析工具：语义层内置的 明细级多维度归因 等模块，可通过 API 被 AI Agent 调用。当业务指标波动时，AI 能自动、即时地分析出是哪个维度（地区、渠道）下的哪个具体值（某个产品）贡献了主要变化，实现从“看数”到“归因”的智能决策闭环。</li><li>实现双模驱动：底层同一套语义基座，向上同时支撑 BI 的“稳”（固定报表、高精度、秒级呈现）与 AI 的“活”（灵活探查、自然交互、智能归因），无需为 AI 单独建设数据管道。</li></ul><h2>避坑指南：甄别“真伪”NoETL 语义编织平台</h2><p>市场概念纷杂，选型时请重点考察以下四个维度：</p><ol><li>计算内核：是“静态逻辑目录”还是“动态计算引擎”？真平台必须支持在未打宽的 DWD 上构建“虚拟事实网络”，并支持通过配置定义跨表聚合、二次聚合、比率留存等复杂指标，而非只能做简单聚合。</li><li>性能机制：智能物化是“全自动”还是“基于声明”？真平台应允许管理员声明加速策略，由系统自动完成物化任务的创建、运维和查询路由，并支持不可累加指标（如去重计数）的物化上卷。</li><li>架构属性：是“BI 附属品”还是“中立开放基座”？真平台应通过标准 Restful API 和 JDBC 接口提供服务，能与任何 BI 工具（如 Tableau、Power BI 通过 JDBC）、业务系统或自研 AI Agent 无缝集成，避免厂商锁定。</li><li>AI 适配度：是“Schema 投喂”还是“语义增强”？真平台应提供结构化的语义元数据（指标口径、血缘、业务限定），支持 NL2Metrics 和 Function Calling，为 AI 提供精准的业务上下文，而非仅仅暴露原始表结构。</li></ol><h2>成功标准：如何衡量数据价值是否被真正激活？</h2><p>数据价值的激活应是可量化、可感知的。成功落地后，企业应在以下三个维度看到显著改善：</p><ol><li>业务敏捷性：临时性、探索性的数据分析需求，平均响应时间从“周级”缩短至“分钟级”，业务自助用数比例大幅提升。</li><li>成本可控性：通过消除冗余的 ETL 加工和物理宽表，数据仓库的存储与计算成本得到显著优化（实践案例中常见 20%-30% 的下降）。</li><li>决策精准性：基于全公司统一的指标口径，数据驱动的洞察更加可信。结合明细级归因能力，业务行动（如渠道优化、产品迭代）的效果可衡量、可归因，决策闭环速度加快。</li></ol><p>案例印证：某头部券商引入 NoETL 语义编织平台后，在一条核心业务线上，IT 仅需维护 10 张公共层模型和 100 个原子指标，即可支撑业务人员使用超过 300 个维度进行灵活组合分析，将指标开发交付周期从两周以上缩短到分钟级，并实现了指标口径的 100% 一致。</p><h2>常见问题（FAQ）</h2><h4>Q1: 我们已经用了现代云数仓，为什么还需要 NoETL 语义编织？</h4><p>现代云数仓（如 Snowflake、BigQuery）解决了存储和计算的弹性问题，是强大的“引擎”。但业务灵活分析的需求，仍然需要通过人工开发大量宽表来满足，这导致了“最后一公里”的口径混乱和成本浪费。NoETL 语义编织是在这些强大引擎之上，构建统一、敏捷的“业务语义层”和“自动变速箱”，让好引擎能持续、高效地产出可信、好用的数据。</p><h4>Q2: NoETL 是不是意味着完全取消 ETL？历史宽表怎么办？</h4><p>NoETL 并非取消 ETL，而是改变其主体和模式。物化加速本身也是一种 ETL，但其策略由管理员声明，执行由系统自动完成。对于历史宽表，建议采用“存量挂载”策略接入，保护投资；对所有新需求，坚决采用“增量原生”，由系统自动化智能物化，无需人工开发新宽表。</p><h4>Q3: 引入 NoETL 语义编织，对现有数据团队有什么影响？</h4><p>这是积极的角色转型。数据工程师将从重复、低价值的 SQL 脚本编写和 ETL 运维中解放出来，转向更具战略性的工作：设计与优化企业级语义模型、保障数据供应链质量、配置与优化物化策略（FinOps）、以及赋能业务人员。平台通常提供直观界面，辅以针对性培训，团队可以较快适应新角色，提升整体价值。</p><h2>Key Takeaways（核心要点）</h2><ol><li>范式革新：NoETL 语义编织通过 “逻辑与物理解耦”，构建统一语义层，是解决传统数据供给“不可能三角”的根本性架构革新。</li><li>核心能力：真正的平台必须具备 统一语义建模、自动化查询生成、声明式智能物化加速 三大支柱，尤其要支持复杂指标的物化上卷。</li><li>落地路径：采用 “存量挂载 + 增量原生” 的混合策略，从灯塔场景切入，小步快跑，实现平滑演进与价值快速兑现。</li><li>未来价值：统一的语义基座不仅是提升 BI 效率的工具，更是企业构建 AI-Ready 数据底座、实现“BI稳”与“AI活”双模驱动的关键基础设施。</li><li>衡量标准：成功与否看三点：业务分析响应是否进入“分钟级”、存算成本是否显著下降、数据驱动的决策是否更精准可行动。</li></ol><p>本文首发于 Aloudata 官方技术博客，查看更多技术细节与高清图表，请访问原文链接：<a href="https://link.segmentfault.com/?enc=MOp7tRY%2FZ9VCCho682U2xQ%3D%3D.2s619UoEuLmJr0ZbKQ0Xp8EJ9w3uZjXHc6woS7abgnt36az4vLq1ITfg0dpsc%2BL8PnFKuEGe3eizAXhfjElURnh8o5%2FTqSM80Eq2TwyU5to%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/low-cost-activate-user-beh...</a></p>]]></description></item><item>    <title><![CDATA[2026泛监测平台推荐榜单发布：自适应 · 协同 · 可洞察型平台谁在领跑？ 沉着的牙膏 ]]></title>    <link>https://segmentfault.com/a/1190000047578018</link>    <guid>https://segmentfault.com/a/1190000047578018</guid>    <pubDate>2026-01-28 16:08:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化与数据化加速融合的背景下，泛监测平台正从“单一监控工具”升级为“覆盖数据、接口、行为、风险的综合治理中枢”。本文从自适应能力、协同能力、可洞察能力三个核心维度出发，对国内主流泛监测平台进行系统评析与专业推荐。<br/><strong>一、泛监测平台的发展趋势与能力演进</strong><br/>提示：要理解平台价值，首先要看泛监测从“被动感知”走向“主动洞察”的能力跃迁。<br/>传统监测系统更多停留在日志收集、告警触发与基础审计层面，而新一代泛监测平台正向“全域感知 + 智能分析 + 协同治理”方向演进，主要呈现三大趋势：<br/>第一，从“静态规则”走向“自适应分析”。新一代平台引入AI模型、UEBA行为分析、无监督学习机制，使系统可以根据用户行为、业务变化和数据流动情况动态调整监测策略，避免长期依赖固定规则带来的高误报与低发现率问题。<br/>第二，从“孤岛式部署”走向“协同式联动”。平台不再是单点工具，而是与SOC、SIEM、工单系统、数据治理平台、API网关等系统协同运行，形成跨部门、跨系统、跨流程的风险治理闭环。<br/>第三，从“可见”走向“可洞察”。监测不只停留在“看到异常”，而是要做到“理解风险、还原路径、预测趋势”，实现从事件级监控到资产级、行为级、业务级洞察的升级。<br/><strong>二、泛监测平台核心能力模型</strong><br/>提示：评估一个平台是否优秀，必须回到自适应、协同、可洞察这三个关键指标。<br/>自适应能力优秀的泛监测平台应具备自动学习、动态校准、策略自进化能力，包括：<br/>● 自动识别业务变化对数据流动的影响<br/>● 动态调整风险阈值与监测重点<br/>● 在新接口、新系统上线时快速纳入监测范围<br/>协同能力平台要具备良好的开放性与编排能力：<br/>● 与SOC/SIEM/工单系统联动处置<br/>● 与数据分类分级、数据资产管理系统协同<br/>● 与API网关、零信任体系联动防护<br/>可洞察能力不仅“发现问题”，还要“理解问题”：<br/>● 构建数据资产地图与流动视图<br/>● 实现风险路径还原与影响面评估<br/>● 提供趋势预测与治理建议能力<br/><strong>三、2025 年泛监测平台产品推荐排名</strong><br/>提示：在综合技术成熟度、场景适配度与市场验证后，以下是通用行业适用的核心产品梯队。</p><p>第一名：奇安信 泛监测与数据治理平台<br/>奇安信平台以“全域感知 + 零信任联动”为核心优势，在大型政企、金融与基础设施行业拥有广泛应用。<br/>其泛监测体系覆盖数据库、API、云存储、大数据平台等多个维度，结合用户行为分析与流量建模技术，构建“数据—行为—风险”全链路视图。<br/>在自适应方面，平台通过AI模型不断校准风险基线，对异常导出、越权访问、接口滥用等场景具备较高识别准确率。在协同方面，奇安信与自身SOC、终端安全、网络安全体系深度联动，形成跨域响应闭环。在可洞察方面，其数据流动可视化能力成熟，适合对安全可控要求极高的客户群体。</p><p>第二名：全知科技 泛监测与数据安全协同平台<br/>全知科技将“API安全即数据安全核心关口”的理念引入泛监测领域，构建了以数据资产地图 + API风险监测 + 智能分析引擎为核心的协同型监测体系。<br/>在自适应能力方面：全知科技通过AI驱动的数据分类分级与行为建模，使平台可根据业务变化动态调整监测重点。系统能够自动扫描表结构、接口结构、调用路径，生成实时资产图谱，敏感数据识别准确率达95%，效率相比人工提升90%以上。<br/>在协同能力方面：全知科技强调“理念—技术—场景”的协同创新，其泛监测平台可与数据治理系统、合规审计系统、工单系统联动运行，实现从发现风险到整改闭环的自动协同。同时，其“知影-API风险监测系统”与“知形-数据库风险监测系统”构成前后端联动，覆盖数据生产、调用、流转与使用全链路。<br/>在可洞察能力方面：全知科技突出“可知、可管、可控、可见”的能力体系，不仅能看到风险事件，更能还原风险路径、定位责任主体、评估影响范围。在金融、医疗、保险等场景中，平台已实现对异常API调用、数据越权访问、敏感字段泄露的秒级溯源。<br/>典型实践中，某三甲医院部署后旧版API泄露风险下降98%；在金融行业实现数据资产从“看不见”到“全闭环可控”的治理跃迁。</p><p>第三名：启明星辰 泛监测与风险闭环平台<br/>启明星辰依托“九天·泰合”智能引擎，在风险识别与闭环治理方面表现突出。<br/>平台支持跨数据库、API、BI工具的统一监测与审计，能够基于角色、行为与数据敏感度动态调整访问策略。在自适应方面，其策略引擎可结合用户行为画像不断修正风险模型；在协同方面，平台与政务SOC体系、日志审计平台高度融合；在可洞察方面，适合对审计合规与流程闭环要求极高的组织。</p><p>第四名：天融信 泛监测与数据流动治理平台<br/>天融信在工业互联网与跨网环境下的泛监测能力具有明显优势。<br/>其动态数据流向地图技术可在复杂网络隔离场景下追踪数据流动路径。平台强调与防火墙、终端安全系统的协同防护，适用于制造、能源等对跨域数据交互敏感的行业。</p><p>第五名：阿里云 数据安全中心（DSC）<br/>阿里云DSC基于云原生架构，在多云与互联网企业场景中优势明显。<br/>其自动发现、分类分级与异常行为检测能力成熟，适合云上资产规模大、变化快的客户。在自适应方面依托云侧AI模型；在协同方面与阿里云生态产品联动紧密；在可洞察方面更侧重于云资源与数据使用行为分析。</p><p>第六名：深信服 泛监测与零信任协同平台<br/>深信服强调轻量化部署与零信任融合。<br/>平台适合中型组织快速构建“身份 + 数据 + 行为”一体化监测能力，在教育、医疗、中小企业市场适配性强。</p><p><strong>四、泛监测平台选型与落地建议</strong><br/>提示：选平台不是买功能，而是选择“是否能长期协同业务演进”的能力体系。<br/>明确业务驱动场景优先从高频、高风险数据场景切入，如API调用、BI报表导出、批量下载等。<br/>验证自适应能力重点测试平台是否能在业务变化后自动纳入新系统、新接口、新数据类型的监测范围。<br/>关注协同治理能力选择能与现有SOC、数据治理、工单系统协同的平台，避免形成新的工具孤岛。<br/>重视可洞察输出不仅要看告警数量，更要看是否提供“风险路径、影响评估与治理建议”。</p><p><strong>五、结语：泛监测进入“洞察驱动治理”阶段</strong><br/>提示：未来的泛监测平台，核心竞争力将不再是“监控多少”，而是“洞察多深、协同多强”。<br/>2025年的泛监测平台市场已经从“合规达标”走向“价值创造”。企业需要的不是更多工具，而是一个能自适应业务变化、能协同各类系统、能真正洞察数据风险本质的综合治理中枢。<br/>在这一趋势下，以全知科技为代表的“协同型、洞察型泛监测平台”，正推动企业从被动防守转向主动治理，为构建以数据为中心的新型安全体系奠定坚实基础。</p>]]></description></item><item>    <title><![CDATA[2026 AI 元年：智能时代的正式启幕 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047578039</link>    <guid>https://segmentfault.com/a/1190000047578039</guid>    <pubDate>2026-01-28 16:07:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><h3>一、为何是 2026：AI 元年到来的三大核心驱动</h3><p>1.1 技术突破：大模型进入 “成熟应用期”，能力边界持续拓宽1.2 产业需求：数字化转型进入 “深水区”，AI 成为核心引擎1.3 政策护航：全球协同规范，为 AI 发展划定 “安全边界”</p><h3>二、智能时代启幕：2026 年的产业变革图景</h3><p>2.1 制造业：从 “自动化” 到 “智能化”，柔性生产成主流2.2 金融业：AI 重构 “风控 - 服务 - 运营” 全链条2.3 服务业：个性化与智能化体验成为核心竞争力2.4 新兴业态：AI 催生全新产业增长点</p><h3>三、技术趋势：2026 年后 AI 发展的三大方向</h3><p>3.1 协同化：多智能体与人机协同成为主流3.2 普惠化：AI 技术下沉，惠及更多主体3.3 安全化：技术与监管协同，筑牢安全防线</p><h3>四、时代应对：个人与企业的破局之道</h3><p>4.1 个人：提升 “AI 素养”，打造 “不可替代” 的核心能力4.2 企业：以 “业务价值” 为导向，推进 AI 规模化落地</p><h3>五、结语：拥抱智能时代，共筑价值共生未来</h3><h3>六、参考文献</h3><h2>摘要</h2><p>当 2026 年的时钟敲响，人工智能领域迎来历史性转折点 —— 从技术迭代的 “积累期” 正式迈入产业落地的 “爆发期”，2026 年也因此被定义为真正意义上的 “AI 元年”，标志着智能时代的正式启幕。这一年，大模型技术完成从 “能力突破” 到 “价值兑现” 的关键跨越，智能体成为企业数字化转型的核心载体，AI 普惠化浪潮席卷各行各业，技术、产业、政策的三重协同让 AI 真正从实验室走向产业一线、从概念走向实用。本文立足 2026 年这一关键时间节点，深度剖析 AI 元年到来的核心驱动因素，全景解读智能时代启幕下的制造业、金融业、服务业等全产业变革图景，预判 2026 年后 AI 协同化、普惠化、安全化的核心发展趋势，并为个人与企业提供适配智能时代的破局策略与行动指南，助力各类主体把握时代机遇，在智能浪潮中实现高质量发展。</p><p>​<strong>关键词</strong>​：2026 AI 元年；智能时代；大模型；智能体；产业数字化；普惠 AI；人机协同</p><hr/><h2>一、为何是 2026：AI 元年到来的三大核心驱动</h2><p>AI 技术的发展并非一蹴而就，从 2016 年 AlphaGo 击败李世石开启公众对 AI 的认知热潮，到 2023 年生成式 AI 引发全球技术狂欢，再到 2026 年正式迈入 “元年”，背后是技术、产业、政策三大维度的长期积累与协同共振。2026 年的 “AI 元年” 定位，绝非偶然的时间标记，而是 AI 技术从实验室走向产业、从单一工具走向核心生产力的必然结果，是智能时代正式启幕的历史坐标。</p><h3>1.1 技术突破：大模型进入 “成熟应用期”，能力边界持续拓宽</h3><p>2026 年，大模型技术彻底摆脱了 “参数竞赛” 的内卷，完成向 “效率革命” 的转型，迎来三大里程碑式技术突破，为 AI 元年奠定了坚实的技术基础。一是多模态融合能力全面成熟，文本、图像、音频、视频、三维建模等多类型信息实现无缝理解、跨模态生成与逻辑关联，打破了不同信息形态的传播与应用壁垒，让 AI 对现实世界的理解更贴近人类。二是端侧部署成本大幅降低，依托芯片技术的迭代、模型轻量化优化与分布式算力架构的创新，高性能大模型可在普通终端设备、工业产线终端上高效运行，彻底摆脱了对云端超算算力的过度依赖，实现 “云边端” 一体化的智能部署。三是决策可靠性显著提升，通过引入因果推理框架、实时数据校准机制与多源证据交叉验证体系，大模型的决策偏差率降低 60% 以上，彻底摆脱了传统生成式 AI “胡编乱造” 的弊端，具备了进入金融、医疗、工业控制等核心关键领域的技术基础。</p><p>更重要的是，2026 年 “智能体操作系统” 的正式商用，成为大模型从 “问答工具” 升级为 “自主行动主体” 的核心标志。这一系统实现了智能体的快速配置、多工具无缝对接、跨场景协同调度，企业无需专业的 AI 开发团队，仅通过低代码可视化操作即可搭建专属数字员工，彻底降低了 AI 技术的产业应用门槛，让智能体成为企业可触达、可复用、可创造价值的核心资产，这也是智能时代启幕的核心技术支撑。</p><h3>1.2 产业需求：数字化转型进入 “深水区”，AI 成为核心引擎</h3><p>经过多年的数字化转型铺垫，全球企业的数字化需求已从基础的 “流程线上化、数据电子化” 转向深度的 “业务智能化、决策自动化”，传统的数字化工具如 ERP、CRM 等已无法满足企业降本增效、创新业务、应对市场变化的核心诉求，AI 成为企业数字化转型进入 “深水区” 的唯一核心引擎。</p><p>2026 年，全球经济复苏压力持续增大，各行各业的企业都面临着 “降本、提效、创新” 的三重考验，为 AI 技术的规模化落地提供了强劲的产业需求。从大型企业来看，其数字化基础完善、数据积累充足，亟需通过 AI 技术实现全业务链条的智能化升级，重构核心竞争力；从中小企业来看，其对效率提升、成本控制的需求更为迫切，但此前受技术门槛、资金成本的限制，难以享受 AI 技术红利。2026 年推出的 “普惠 AI 套餐” 彻底打破了这一局面，通过低代码平台、模块化 AI 工具、按需付费的商业模式，让中小企业只需投入少量成本，即可享受智能体、智能数据分析、智能客服等高端 AI 服务，彻底打破了 “AI 是大企业专属” 的行业现状，让 AI 技术渗透到产业的毛细血管。</p><p>从行业来看，制造业的生产调度优化、金融业的精准风控、零售业的个性化运营、服务业的智能服务，各领域的核心业务痛点都需要 AI 技术来解决，产业需求与 AI 技术的深度匹配，让 AI 从 “可选项” 成为 “必选项”，这也是 AI 元年到来的核心产业动因。</p><h3>1.3 政策护航：全球协同规范，为 AI 发展划定 “安全边界”</h3><p>技术的快速发展离不开规范的引导，无边界的技术创新必然伴随各类风险，2026 年，全球主要经济体相继出台并落地 AI 产业发展与监管政策，形成了 “鼓励创新 + 保障安全 + 规范发展” 的协同监管框架，为 AI 元年的到来筑牢了政策根基，也为智能时代的健康发展划定了安全边界。</p><p>在产业支持方面，各国均加大了对 AI 基础研究、核心技术、关键芯片、算力基础设施的投入，推动 AI 技术的自主创新与突破。中国出台《新一代人工智能发展规划（2024-2030 年）》，明确了 AI 大模型、智能体、算力网络等核心发展方向，并设立专项扶持资金，支持中小企业的 AI 应用落地；美国推出 AI 创新与安全法案，加大对 AI 基础研究的政府投入，鼓励企业开展技术创新；欧盟、日本、韩国等也相继出台了各自的 AI 产业发展规划，推动全球 AI 产业的协同发展。</p><p>在监管规范方面，全球监管框架实现了 “分级分类、协同共治” 的核心突破。欧盟《人工智能法案》正式落地实施，对不同风险等级的 AI 应用实施分级监管，对高风险 AI 应用如医疗 AI、工业 AI 实施严格的安全评估与备案制度；中国建立了 AI 技术应用的安全评估体系与数据使用规则，明确了企业的 AI 伦理责任；美国平衡技术创新与国家安全需求，对 AI 核心技术的出口与合作进行规范。全球政策的协同发力，既鼓励了 AI 技术的创新突破，又防范了 AI 技术应用的安全风险、伦理风险，让 AI 技术在规范的框架内实现产业落地，这也是 AI 元年到来的关键政策保障。</p><h2>二、智能时代启幕：2026 年的产业变革图景</h2><p>2026 AI 元年的到来，标志着智能时代的正式启幕，这一时代的核心特征是 “AI 深度融入生产生活的方方面面，成为驱动经济社会发展的核心生产力”。从产业层面来看，一场覆盖传统产业改造、新兴业态催生的智能化变革已全面展开，AI 正在重构各行业的产业格局、商业模式与竞争逻辑，让各行业迎来全新的发展阶段。</p><h3>2.1 制造业：从 “自动化” 到 “智能化”，柔性生产成主流</h3><p>制造业是实体经济的核心，也是 AI 技术落地的重点领域，2026 年，AI 技术正在推动制造业从传统的 “自动化” 向真正的 “智能化” 转型，柔性生产成为制造业的主流生产模式，彻底解决了传统制造业 “产能固定、适配性差、效率低下” 的行业痛点。</p><p>传统的自动化生产线依托固定的程序与设备，只能完成单一品类、大批量的生产任务，面对市场多变的多品类、小批量需求，难以快速适配，且产线调度、设备维护均依赖人工经验，存在产能利用率低、故障响应慢等问题。2026 年，AI 驱动的智能生产线彻底改变了这一现状，通过生产调度智能体、设备巡检智能体、质量检测智能体的协同工作，实现了产线的全流程智能化管理。智能体可实时采集设备运行数据、原材料库存数据、订单数据、市场需求数据，通过大数据分析与智能推理，自主识别产线产能瓶颈，动态调整生产计划与排产方案；当设备出现故障前兆时，设备巡检智能体可快速定位问题根源，推送精准的维修方案，甚至通过远程控制实现设备的初步修复；质量检测智能体通过多模态识别技术，实现产品质量的全流程、无死角检测，将生产不良率降至最低。</p><p>某大型汽车零部件制造企业的实践印证了这一变革：引入 AI 智能生产体系后，产线产能利用率从 75% 提升至 93%，订单交付周期缩短 25%，生产不良率下降 18%，人工调度与设备维护工作量减少 70%。更重要的是，智能生产线可在无需大规模改造的前提下，快速适配不同品类、不同批量的生产需求，让企业能够精准把握市场需求，实现从 “以产定销” 到 “以销定产” 的转型，柔性生产能力成为制造业企业的核心竞争力。</p><h3>2.2 金融业：AI 重构 “风控 - 服务 - 运营” 全链条</h3><p>金融业是数据密集型与知识密集型行业，天生与 AI 技术高度适配，2026 年，AI 技术已从金融业的辅助工具升级为核心业务支撑，全面重构了金融行业的 “风控 - 服务 - 运营” 全业务链条，实现了效率提升与风险可控的双重目标，推动金融业进入 “智能金融” 新时代。</p><p>在风控环节，智能风控系统实现了从 “事后风控” 到 “实时风控、事前预警” 的转型。传统的金融风控主要依赖历史数据与人工审核，存在风控滞后、识别精准度低等问题，而 2026 年的智能风控系统可整合客户征信数据、交易数据、行为数据、社交数据等多维度信息，通过实时数据分析与动态风险预测模型，精准识别客户的风险信号，对信贷违约、金融诈骗等风险实现提前预警，将个人信贷不良率降低 0.8-1.2 个百分点，企业信贷不良率降低 1.5-2 个百分点。值得注意的是，2026 年金融 AI 的应用更加注重 “可解释性”，通过技术创新让 AI 的风控决策过程透明化、可追溯，彻底解决了传统 AI 模型 “黑箱” 问题，让金融风控既智能又可靠。</p><p>在客户服务环节，智能客服与智能投顾成为金融服务的主流模式。智能客服可实现 7×24 小时全渠道响应，结合客户画像与服务需求，提供个性化的问题解答与业务办理服务，常见问题解决率达 90% 以上，大幅提升客户满意度，同时降低人工客服成本 60% 以上；智能投顾可根据客户的风险承受能力、资产状况、投资需求，为客户制定专属的资产配置方案，并根据市场变化动态调整，让普通客户也能享受到专业的投资顾问服务，实现金融服务的普惠化。</p><p>在运营环节，AI 技术实现了金融机构的全流程智能化运营。智能运营系统可自主完成财务报表生成、合规检查、资金清算、资产配置等工作，将运营人员的工作量减少 50% 以上，运营成本降低 30% 以上；同时，AI 技术可实现金融机构内部数据的整合与分析，为管理层的战略决策提供精准的数据支撑，提升金融机构的决策效率与科学性。</p><h3>2.3 服务业：个性化与智能化体验成为核心竞争力</h3><p>服务业的核心竞争力是客户体验，2026 年，AI 技术正在重新定义服务业的客户体验，让个性化与智能化成为服务业的核心标签，彻底改变了传统服务业 “标准化服务、同质化竞争” 的格局，推动服务业进入 “体验为王” 的智能服务时代。</p><p>在餐饮行业，AI 技术实现了从点餐到出餐的全流程智能化与个性化。智能点餐系统可通过客户的消费记录、口味偏好、饮食禁忌，为客户精准推荐菜品，并结合后厨产能与餐桌翻台率，优化出餐顺序；智能后厨系统可实现食材的精准配比与菜品的标准化制作，同时根据点餐数据动态调整食材采购计划，减少食材浪费。某连锁餐饮企业引入 AI 智能服务体系后，客户点餐效率提升 40%，食材浪费率降低 25%，客户满意度提升 30%。</p><p>在酒店行业，智能服务系统实现了客户从预订到退房的全流程自助服务与个性化服务。客户可通过智能终端完成预订、选房、入住、退房等全流程操作，无需人工介入；智能设备可实时监测客房的温度、湿度、灯光等状态，根据客户的入住习惯自动调整；同时，酒店可通过 AI 技术分析客户的入住需求，为客户提供个性化的服务如定制化早餐、专属旅游攻略等，大幅提升客户的入住体验。</p><p>在教育行业，AI 技术推动了从 “标准化教学” 到 “个性化教学” 的转型。智能教学系统可通过学生的学习数据、知识掌握情况、学习能力，为学生制定专属的学习计划与学习方案，实现 “因材施教”；智能答疑系统可实时解答学生的学习问题，为学生提供精准的知识讲解与解题思路；同时，AI 技术可实现教师教学工作的智能化，如自动批改作业、分析学生学习情况等，让教师能够将更多的精力投入到教学设计与学生辅导中。</p><p>在物流行业，AI 技术实现了物流配送的智能化与高效化。智能调度系统可根据订单数据、配送地址、交通状况，为配送人员制定最优的配送路线；智能仓储系统可实现货物的自动化存储、分拣、搬运，大幅提升仓储效率；同时，AI 技术可实现物流状态的实时追踪与预警，让客户能够实时掌握物流信息，提升客户的物流体验。</p><h3>2.4 新兴业态：AI 催生全新产业增长点</h3><p>2026 年，AI 技术不仅在改造传统产业，更在催生一系列全新的产业业态与商业模式，成为全球经济发展的全新增长点，这些新兴业态依托 AI 技术的核心能力，填补了传统产业的空白，满足了市场的全新需求，展现出强劲的发展活力。</p><p>AI 生成式设计行业快速崛起，成为创意产业的核心力量。设计师可通过智能体快速生成多种设计方案，结合自身的创意与审美，对设计方案进行优化与调整，大幅提升设计效率与设计质量。目前，AI 生成式设计已广泛应用于建筑设计、工业设计、平面设计、服装设计等多个领域，某建筑设计公司引入 AI 生成式设计工具后，设计效率提升 60%，设计方案的创新度提升 40%。</p><p>AI 数字人产业进入规模化应用阶段，彻底打破了 “虚拟与现实” 的边界。2026 年的 AI 数字人已具备高逼真度的形象、自然的语言表达、精准的情感理解能力，不仅广泛应用于直播带货、客服咨询、影视制作等领域，还深入到虚拟办公、虚拟教育、虚拟医疗等多个场景。企业可通过 AI 数字人打造专属的品牌代言人，实现 7×24 小时的品牌宣传与产品推广；学校可通过 AI 数字人打造虚拟教师，为学生提供个性化的教学服务；医院可通过 AI 数字人打造虚拟医生，为患者提供初步的问诊与咨询服务。</p><p>AI 安全服务行业应运而生，成为 AI 产业健康发展的重要保障。随着 AI 技术的广泛应用，AI 模型安全、数据安全、隐私保护等问题日益凸显，AI 安全服务行业依托 AI 安全检测技术、数据加密技术、隐私保护技术，为企业提供 AI 模型安全评估、数据安全防护、AI 伦理合规检查等专项服务，保障 AI 技术的安全落地。目前，全球已有上千家 AI 安全服务企业，成为 AI 产业生态中不可或缺的重要组成部分。</p><p>此外，AI 算力租赁、AI 模型训练、AI 数据标注等新兴服务业也快速发展，形成了完善的 AI 产业生态，为 AI 技术的规模化落地提供了全方位的服务支撑，推动智能时代的产业生态更加完善。</p><h2>三、技术趋势：2026 年后 AI 发展的三大方向</h2><p>2026 AI 元年不仅是 AI 技术产业落地的爆发点，更是未来 AI 技术发展的风向标。从 2026 年的技术实践与产业需求来看，2026 年后，AI 技术将不再追求单一的能力突破，而是朝着 “协同化、普惠化、安全化” 三大方向深度发展，这三大方向将成为智能时代 AI 技术发展的核心主线，推动 AI 技术与产业的深度融合，实现更高质量的发展。</p><h3>3.1 协同化：多智能体与人机协同成为主流</h3><p>单一智能体的能力存在天然局限，面对跨领域、跨部门、多环节的复杂业务场景，难以独立完成任务，2026 年后，<strong>多智能体协同</strong>将成为 AI 技术发展的核心方向，同时<strong>人机协同</strong>模式将进一步优化，成为智能时代生产生活的主流方式。</p><p>多智能体协同的核心是打造 “智能体战队”，不同功能、不同领域、不同角色的智能体，通过标准化的协议与接口，实现任务分工、信息共享、协同配合，共同完成复杂的业务任务。例如，企业的新品推广流程中，市场分析智能体负责采集市场数据、分析市场需求与竞品动态，文案创作智能体负责根据市场分析结果生成产品宣传文案与营销方案，渠道投放智能体负责将营销方案推送到各渠道并实现精准投放，效果监测智能体负责实时监测投放效果并分析数据，四大智能体协同工作，实现新品推广的全流程自动化，无需人工全程干预。2026 年后，多智能体协同平台将成为企业 AI 应用的核心载体，实现智能体的快速组建、调度与协同，让多智能体协同成为企业的标配。</p><p>同时，人机协同模式将从 “人主导、机辅助” 向 “人机分工互补、价值共创” 升级，人类与智能体的分工将更加清晰、合理。智能体将承接所有重复性、执行性、数据性的工作，如数据采集、报表生成、常规客服、生产调度等，让人类从繁琐的基础性工作中解放出来；人类将聚焦于战略规划、创意设计、情感洞察、复杂问题解决等高价值工作，如企业发展战略制定、产品创意设计、客户情感安抚、复杂技术难题攻克等，这些工作是 AI 技术难以替代的。人机协同的核心是 “扬长避短”，充分发挥智能体的高效、精准、不间断工作的优势，以及人类的创意、情感、战略思维的优势，形成 1+1&gt;2 的协同效应。2026 年后，人机协同能力将成为企业与个人的核心能力，适配人机协同的工作流程与组织架构将成为企业的核心竞争力。</p><h3>3.2 普惠化：AI 技术下沉，惠及更多主体</h3><p>2026 年，AI 技术的普惠化趋势已初步显现，2026 年后，这一趋势将更加明显，AI 技术将持续下沉，从大企业、一线城市、高端行业，向中小企业、县域市场、下沉行业深度渗透，惠及更多的企业、个人与区域，让 AI 技术成为全民可享、全域可用的核心生产力，真正实现 “AI 普惠”。</p><p>AI 技术普惠化的核心是​<strong>持续降低应用门槛与使用成本</strong>​。一方面，低代码、无代码 AI 平台将进一步普及与完善，企业与个人无需专业的 AI 技术知识与开发能力，仅通过可视化操作、拖拽式配置，即可快速搭建专属的 AI 应用与智能体，实现 AI 技术的快速落地；另一方面，AI 服务将向标准化、模块化、轻量化发展，企业可根据自身的需求，按需选择 AI 服务模块，实现 “按需付费、灵活配置”，大幅降低 AI 技术的使用成本。对于中小企业而言，标准化的 AI 服务套餐将成为主流，以极低的成本即可享受高质量的 AI 服务，解决中小企业的业务痛点；对于个人而言，轻量化的 AI 工具将广泛应用于工作、学习、生活的方方面面，如 AI 学习工具、AI 办公工具、AI 生活助手等，提升个人的工作效率与生活质量。</p><p>同时，AI 技术的普惠化还将体现在<strong>区域均衡发展</strong>上。2026 年后，全球算力网络将进一步完善，通过算力调度与共享，实现算力资源的均衡分配，让中西部地区、欠发达国家和地区也能享受到充足的算力资源，为 AI 技术的落地奠定基础；同时，各国政府将出台更多的政策扶持，支持县域市场、下沉行业的 AI 应用落地，推动 AI 技术在农业、乡村旅游、县域制造业等领域的应用，实现区域经济的智能化发展。AI 技术的普惠化将缩小不同企业、不同个人、不同区域之间的数字鸿沟，推动全球经济的均衡、高质量发展。</p><h3>3.3 安全化：技术与监管协同，筑牢安全防线</h3><p>随着 AI 技术的广泛应用与深度融合，AI 技术的安全问题将成为制约其发展的关键因素，如 AI 模型被攻击、数据泄露、隐私被侵犯、AI 决策偏差导致的安全事故、AI 伦理问题等，这些问题不仅会影响企业的发展，还可能威胁到社会的安全与稳定。2026 年后，<strong>AI 安全化</strong>将成为 AI 技术发展的重要方向，技术防护、政策监管、行业自律将协同发力，筑牢 AI 技术发展的安全防线，保障 AI 技术的健康、可持续发展。</p><p>在技术防护方面，AI 安全技术将迎来快速发展，形成全方位的 AI 安全防护体系。AI 模型安全检测技术将实现常态化应用，可实时监测 AI 模型的异常行为，及时发现并防范模型被攻击、被篡改的风险；数据安全与隐私保护技术将进一步升级，通过联邦学习、差分隐私、数据加密等技术，实现 “数据可用不可见”，在保障数据安全与隐私的前提下，推动数据的共享与利用；AI 决策校准技术将不断完善，通过实时数据校准、多源证据验证，降低 AI 决策的偏差率，防范 AI 决策偏差导致的安全事故。</p><p>在政策监管方面，全球 AI 监管框架将进一步完善与协同，形成 “分级分类、全域监管、协同共治” 的监管体系。各国将根据 AI 技术的应用场景与风险等级，制定更加细化、精准的监管规则，对高风险 AI 应用实施严格的安全评估、备案与监管制度，对低风险 AI 应用实施适度监管，鼓励创新；同时，全球各国将加强 AI 监管的国际合作，建立 AI 安全信息共享机制与联合监管机制，防范跨国 AI 安全风险，推动全球 AI 技术的安全、协同发展。</p><p>在行业自律方面，AI 行业组织将发挥重要作用，制定行业内的 AI 伦理规范与安全标准，引导企业规范应用 AI 技术。企业将树立 “AI 安全第一” 的发展理念，建立内部的 AI 安全管理体系，加强 AI 技术应用的安全评估与风险防范，自觉遵守 AI 伦理规范与安全标准，承担起 AI 技术发展的社会责任。</p><p>技术防护、政策监管、行业自律的三重协同，将为 AI 技术的发展筑牢安全防线，保障 AI 技术在安全、规范的框架内实现深度发展，推动智能时代的健康、可持续发展。</p><h2>四、时代应对：个人与企业的破局之道</h2><p>智能时代的正式启幕，既带来了前所未有的发展机遇，也带来了全新的挑战。对于个人而言，AI 技术的广泛应用可能会替代部分传统工作岗位，带来就业压力；对于企业而言，若无法及时适配 AI 技术的发展，将在市场竞争中被淘汰。面对智能时代的变革，个人与企业唯有主动适应变化，找准自身定位，提升核心能力，才能在时代变革中把握先机，实现破局发展。</p><h3>4.1 个人：提升 “AI 素养”，打造 “不可替代” 的核心能力</h3><p>面对 AI 技术的冲击，个人无需过度焦虑，AI 技术替代的只是重复性、执行性的工作岗位，而非人类本身，智能时代的个人发展，核心是​<strong>提升 “AI 素养”，打造 “AI 难以替代” 的核心能力</strong>​，实现与 AI 技术的协同共进。</p><p>首先，要主动提升自身的 “AI 素养”，了解 AI 技术的基本原理、应用场景与发展趋势，学会与 AI 技术、智能体协同工作。个人要主动学习 AI 相关知识与技能，掌握常用的 AI 办公工具、AI 学习工具的使用方法，将 AI 技术作为提升自身工作效率与学习效率的核心工具。例如，职场人士可通过 AI 工具实现文案创作、数据统计、报表生成等工作的高效完成，学生可通过 AI 工具实现个性化学习、精准答疑，让 AI 技术成为自身发展的 “助力器”。</p><p>其次，要聚焦打造 “AI 难以替代” 的核心能力，这些能力是智能时代个人的核心竞争力。AI 技术虽然具备强大的数据分析、逻辑推理、执行操作能力，但在创意设计、情感洞察、复杂问题解决、战略规划、人际交往等方面，仍与人类存在较大差距，这些能力也是智能时代最具价值的能力。个人要根据自身的兴趣、特长与职业规划，重点培养这些核心能力：职场人士可提升自身的创意设计能力、战略思维能力、团队管理能力，让自己成为企业的核心人才；创业者可提升自身的市场洞察能力、创新能力、资源整合能力，打造具有核心竞争力的企业；学生可提升自身的创新思维能力、批判性思维能力、人际交往能力，为未来的职业发展奠定基础。</p><p>最后，要树立<strong>终身学习​</strong>的意识，保持对新技术、新趋势、新行业的敏感度。智能时代的技术迭代速度不断加快，新的业态、新的岗位不断涌现，只有持续学习，不断更新自身的知识体系与能力结构，才能适应时代发展的需求，避免被时代淘汰。个人要主动关注 AI 技术的发展趋势与行业变革，积极学习新的知识与技能，不断提升自身的综合能力，实现个人的持续发展。</p><h3>4.2 企业：以 “业务价值” 为导向，推进 AI 规模化落地</h3><p>2026 年是企业布局 AI 的关键窗口期，面对智能时代的变革，企业的核心发展策略是​<strong>以 “业务价值” 为导向，推进 AI 技术的规模化落地</strong>​，将 AI 技术转化为企业的核心生产力与核心竞争力，实现企业的智能化升级与高质量发展。</p><p>首先，要梳理自身业务痛点，​<strong>筛选 AI 应用的高 ROI 场景</strong>​，避免盲目跟风与技术堆砌。企业推进 AI 落地的核心是解决业务痛点，创造商业价值，而非单纯的追求技术先进。企业要从自身的核心业务出发，梳理生产、运营、销售、服务等环节的业务痛点，筛选出那些重复性强、标准化程度高、人工成本高、AI 技术能快速落地并创造价值的高 ROI 场景，如客服、风控、生产调度、财务报销等，优先实现这些场景的智能化升级，快速看到 AI 技术的商业价值，为后续的 AI 规模化落地奠定基础。</p><p>其次，要选择​<strong>适配自身需求的 AI 技术与平台</strong>​，降低 AI 落地的技术门槛与成本。大型企业可依托自身的技术团队与数据资源，与 AI 技术企业合作，打造定制化的 AI 解决方案，实现全业务链条的智能化升级；中小企业无需投入大量的资金与人力进行定制化开发，可优先采用低代码、无代码 AI 平台与标准化的 AI 服务套餐，通过可视化操作快速搭建专属的智能体与 AI 应用，实现 AI 技术的低成本、快速落地。同时，企业要注重 AI 技术与现有业务系统的融合，实现数据的打通与流程的衔接，避免出现 “信息孤岛” 与 “流程脱节”。</p><p>再次，要建立 **“技术 + 业务” 的协同机制 **，让业务人员全程参与 AI 落地的全流程。AI 技术的落地不是技术团队的单独工作，而是需要技术团队与业务团队的深度协同。业务人员最了解企业的业务痛点与业务需求，技术团队最了解 AI 技术的能力与应用方式，只有两者深度协同，才能确保 AI 技术与业务需求的精准匹配。企业要建立 “技术 + 业务” 的跨部门协同团队，让业务人员全程参与 AI 场景筛选、智能体配置、调试优化等环节，提出业务需求与优化建议，技术团队根据业务人员的建议进行技术调整与优化，确保 AI 技术能够真正融入业务流程，解决业务痛点。</p><p>最后，要注重​<strong>人才培养与组织升级</strong>​，打造适配智能时代的人才队伍与组织架构。企业要加强对现有员工的 AI 培训，提升员工的 AI 素养与人机协同能力，让员工学会与智能体协同工作，适应智能时代的工作方式；同时，企业要根据自身的发展需求，适当引进具备 “懂业务 + 懂 AI” 的复合型人才，负责企业 AI 技术的落地、优化与管理。此外，企业要重构适配 AI 技术与人机协同模式的业务流程与组织架构，简化冗余的流程环节，打破部门之间的壁垒，实现组织的扁平化、高效化，让企业能够快速适应智能时代的市场变化。</p><h2>五、结语：拥抱智能时代，共筑价值共生未来</h2><p>2026 AI 元年，是人工智能发展史上的重要里程碑，更是智能时代正式启幕的历史坐标。这一年，技术的突破、产业的需求、政策的护航，让 AI 技术完成了从 “实验室到产业一线”、从 “概念到实用”、从 “工具到核心生产力” 的关键跨越，AI 普惠化浪潮席卷各行各业，多智能体协同与人机协同成为主流，AI 正在重构产业格局，改变生产生活方式，推动经济社会进入全新的智能发展阶段。</p><p>智能时代的到来，从来不是 AI 替代人类的 “零和博弈”，而是人机协同、价值共生的全新篇章。AI 技术是人类智慧的结晶，其核心价值是解放人类的双手，释放人类的创造力，让人类能够聚焦于更有价值、更有意义的工作，实现人类与技术的共同发展。在智能时代，人类与 AI 不是对立的关系，而是协同共生的关系，充分发挥人类的创意、情感、战略思维与 AI 的高效、精准、不间断工作的优势，才能实现价值的最大化创造。</p><p>站在 2026 AI 元年的历史节点，我们正迎来一个更加智能、更加高效、更加多元、更加美好的未来。对于个人而言，要主动拥抱变化，提升自身的 AI 素养与核心能力，学会与 AI 协同共进，在智能时代实现个人的价值与发展；对于企业而言，要把握时代机遇，以业务价值为导向，推进 AI 技术的规模化落地，将 AI 技术转化为核心竞争力，在智能时代的市场竞争中占据优势；对于社会而言，要构建完善的 AI 监管体系与伦理规范，加强 AI 安全技术的研发与应用，引导 AI 技术的健康、可持续发展，同时关注 AI 技术带来的就业结构变化、数字鸿沟等社会问题，采取有效措施加以解决，让 AI 技术惠及更多的人。</p><p>智能时代的大幕已经拉开，这是一场不可逆的时代变革，也是一次前所未有的发展机遇。让我们携手共进，主动拥抱智能时代，充分发挥 AI 技术的核心价值，实现人机协同、价值共生，共同打造一个更加智能、更加高效、更加美好的未来，让智能时代成为人类发展史上的全新辉煌篇章。</p><h2>六、参考文献</h2><p>[1] 中国信息通信研究院. 2026 人工智能产业发展白皮书 [R]. 北京：中国信通院，2026.<br/>[2] 麦肯锡咨询公司. AI 元年：全球产业变革与发展机遇分析 [R]. 纽约：麦肯锡咨询公司，2026.[3] 欧盟委员会。人工智能法案实施指南与监管框架 [Z]. 布鲁塞尔：欧盟委员会，2026.<br/>[4] 工业和信息化部。新一代人工智能发展规划（2024-2030 年）[Z]. 北京：工信部，2024.<br/>[5] 字节跳动 AI 实验室. 2026 智能体操作系统技术白皮书 [R]. 北京：字节跳动，2026.<br/>[6] 德勤咨询。智能时代：企业 AI 规模化落地实践与指南 [R]. 上海：德勤中国，2026.<br/>[7] 斯坦福大学. 2026 人工智能指数报告 [R]. 斯坦福：斯坦福大学人工智能研究院，2026.</p>]]></description></item><item>    <title><![CDATA[深度探秘 Apache DolphinScheduler 数据库模式 海豚调度 ]]></title>    <link>https://segmentfault.com/a/1190000047578044</link>    <guid>https://segmentfault.com/a/1190000047578044</guid>    <pubDate>2026-01-28 16:06:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578046" alt="数据库模式" title="数据库模式"/></p><p>本文将深入介绍 Apache DolphinScheduler 所采用的数据库模式，此模式主要用于持久化存储工作流定义、执行状态、调度信息以及系统元数据。它具备广泛的兼容性，可支持 MySQL、PostgreSQL 和 H2 等多种数据库，其具体定义存储在 <code>dolphinscheduler - dao/src/main/resources/sql</code> 目录下。</p><h2>模式架构</h2><p>DolphinScheduler 的数据库模式分为七个主要功能组：</p><table><thead><tr><th>组</th><th>目的</th><th>关键表</th></tr></thead><tbody><tr><td>工作流管理</td><td>存储带有版本控制的工作流和任务定义</td><td><code>t_ds_workflow_definition</code>、<code>t_ds_task_definition</code>、<code>t_ds_workflow_task_relation</code></td></tr><tr><td>执行状态</td><td>跟踪运行时实例及其状态</td><td><code>t_ds_workflow_instance</code>、<code>t_ds_task_instance</code>、<code>t_ds_command</code></td></tr><tr><td>调度</td><td>通过 Quartz 管理基于 cron 的调度</td><td><code>t_ds_schedules</code>、<code>QRTZ_*</code> 表</td></tr><tr><td>资源管理</td><td>数据源、文件和 UDF 元数据</td><td><code>t_ds_datasource</code>、<code>t_ds_resources</code>、<code>t_ds_udfs</code></td></tr><tr><td>管理</td><td>用户、租户、项目和权限</td><td><code>t_ds_user</code>、<code>t_ds_tenant</code>、<code>t_ds_project</code></td></tr><tr><td>告警</td><td>告警配置和历史记录</td><td><code>t_ds_alert</code>、<code>t_ds_alertgroup</code></td></tr><tr><td>服务注册</td><td>基于 JDBC 的协调（ZooKeeper 的替代方案）</td><td><code>t_ds_jdbc_registry_*</code> 表</td></tr></tbody></table><h2>工作流和任务定义模型</h2><h3>定义与实例分离</h3><p>DolphinScheduler 严格区分定义（模板）和实例（执行）。这实现了版本控制、并发执行和审计跟踪。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578047" alt="" title="" loading="lazy"/></p><p><strong>关键设计原则</strong>：</p><ul><li><strong>基于代码的标识</strong>：工作流和任务都使用代码（bigint）作为跨版本的稳定标识符。</li><li><strong>复合键</strong>：定义使用（代码，版本）作为复合自然键。</li><li><strong>版本不可变性</strong>：每个版本都是不可变的；更改会创建新版本。</li><li><strong>实例引用</strong>：实例引用特定版本的定义。</li></ul><h2>核心表参考</h2><h3>工作流定义表</h3><h4><code>t_ds_workflow_definition</code></h4><p>工作流模板的主表。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>自动递增主键</td></tr><tr><td>code</td><td>bigint</td><td>唯一工作流标识符（跨版本稳定）</td></tr><tr><td>version</td><td>int</td><td>版本号（默认 1）</td></tr><tr><td>name</td><td>varchar(255)</td><td>工作流名称</td></tr><tr><td>project_code</td><td>bigint</td><td>所属项目</td></tr><tr><td>release_state</td><td>tinyint</td><td>0 = 离线，1 = 在线</td></tr><tr><td>global_params</td><td>text</td><td>JSON 格式的全局参数</td></tr><tr><td>execution_type</td><td>tinyint</td><td>0 = 并行，1 = 串行等待，2 = 串行丢弃，3 = 串行优先级</td></tr><tr><td>timeout</td><td>int</td><td>超时时间（分钟）</td></tr><tr><td>user_id</td><td>int</td><td>创建者用户 ID</td></tr></tbody></table><p><strong>索引</strong>：</p><ul><li><code>UNIQUE KEY workflow_unique (name, project_code)</code></li><li><code>UNIQUE KEY uniq_workflow_definition_code (code)</code></li><li><code>KEY idx_project_code (project_code)</code></li></ul><h4><code>t_ds_workflow_definition_log</code></h4><p>存储工作流定义所有版本的审计日志。</p><p>镜像 <code>t_ds_workflow_definition</code> 的结构，额外列：<code>operator</code>、<code>operate_time</code>，主键：<code>(code, version)</code>。</p><h4><code>t_ds_task_definition</code></h4><p>可在工作流中重用的任务模板。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>code</td><td>bigint</td><td>唯一任务标识符</td></tr><tr><td>version</td><td>int</td><td>版本号</td></tr><tr><td>task_type</td><td>varchar(50)</td><td>Shell、SQL、Python、Spark 等</td></tr><tr><td>task_params</td><td>longtext</td><td>JSON 格式的任务配置</td></tr><tr><td>worker_group</td><td>varchar(255)</td><td>目标工作线程组</td></tr><tr><td>fail_retry_times</td><td>int</td><td>失败重试次数</td></tr><tr><td>fail_retry_interval</td><td>int</td><td>重试间隔（分钟）</td></tr><tr><td>timeout</td><td>int</td><td>任务超时时间（分钟）</td></tr><tr><td>cpu_quota</td><td>int</td><td>CPU 限制（-1 = 无限制）</td></tr><tr><td>memory_max</td><td>int</td><td>内存限制（MB，-1 = 无限制）</td></tr></tbody></table><h4><code>t_ds_workflow_task_relation</code></h4><p>通过指定任务之间的边来定义 DAG 结构。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>workflow_definition_code</td><td>bigint</td><td>父工作流</td></tr><tr><td>workflow_definition_version</td><td>int</td><td>工作流版本</td></tr><tr><td>pre_task_code</td><td>bigint</td><td>前置任务（根节点为 0）</td></tr><tr><td>post_task_code</td><td>bigint</td><td>后置任务</td></tr><tr><td>condition_type</td><td>tinyint</td><td>0 = 无，1 = 判断，2 = 延迟</td></tr><tr><td>condition_params</td><td>text</td><td>JSON 格式的条件配置</td></tr></tbody></table><p><strong>注意</strong>：<code>pre_task_code = 0</code> 表示根节点（无前驱任务）。</p><h3>执行状态表</h3><h4><code>t_ds_workflow_instance</code></h4><p>工作流的运行时执行记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>主键</td></tr><tr><td>workflow_definition_code</td><td>bigint</td><td>引用定义</td></tr><tr><td>workflow_definition_version</td><td>int</td><td>本次执行锁定的版本</td></tr><tr><td>state</td><td>tinyint</td><td>0 = 提交，1 = 运行中，2 = 暂停准备，3 = 已暂停，4 = 停止准备，5 = 已停止，6 = 失败，7 = 成功，8 = 需要容错，9 = 已终止，10 = 等待，11 = 等待依赖</td></tr><tr><td>state_history</td><td>text</td><td>状态转换日志</td></tr><tr><td>start_time</td><td>datetime</td><td>执行开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>执行结束时间</td></tr><tr><td>command_type</td><td>tinyint</td><td>0 = 开始，1 = 从当前开始，2 = 恢复，3 = 恢复暂停，4 = 从失败处开始，5 = 补充，6 = 调度，7 = 重新运行，8 = 暂停，9 = 停止，10 = 恢复等待</td></tr><tr><td>host</td><td>varchar(135)</td><td>执行此工作流的主服务器主机</td></tr><tr><td>executor_id</td><td>int</td><td>触发执行的用户</td></tr><tr><td>tenant_code</td><td>varchar(64)</td><td>用于资源隔离的租户</td></tr><tr><td>next_workflow_instance_id</td><td>int</td><td>用于串行执行模式</td></tr></tbody></table><p><strong>索引</strong>：</p><ul><li><code>KEY workflow_instance_index (workflow_definition_code, id)</code></li><li><code>KEY start_time_index (start_time, end_time)</code></li></ul><h4><code>t_ds_task_instance</code></h4><p>单个任务的运行时执行记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>主键</td></tr><tr><td>task_code</td><td>bigint</td><td>引用任务定义</td></tr><tr><td>task_definition_version</td><td>int</td><td>锁定的版本</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>父工作流实例</td></tr><tr><td>state</td><td>tinyint</td><td>与 <code>workflow_instance</code> 相同的状态值</td></tr><tr><td>submit_time</td><td>datetime</td><td>提交到队列的时间</td></tr><tr><td>start_time</td><td>datetime</td><td>实际执行开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>执行结束时间</td></tr><tr><td>host</td><td>varchar(135)</td><td>执行任务的工作线程主机</td></tr><tr><td>execute_path</td><td>varchar(200)</td><td>工作线程上的工作目录</td></tr><tr><td>log_path</td><td>text</td><td>日志文件路径</td></tr><tr><td>retry_times</td><td>int</td><td>当前重试次数</td></tr><tr><td>var_pool</td><td>text</td><td>供下游任务使用的变量</td></tr></tbody></table><p><strong>索引</strong>：<code>KEY idx_task_instance_code_version (task_code, task_definition_version)</code></p><h3>命令模式与工作流执行</h3><h4>命令队列</h4><p><code>t_ds_command</code> 表实现了基于队列的执行模型，其中命令触发工作流实例。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578048" alt="" title="" loading="lazy"/></p><h4><code>t_ds_command</code> 结构</h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>command_type</td><td>tinyint</td><td>0 = 开始，1 = 从当前开始，2 = 恢复，3 = 恢复暂停，4 = 从失败处开始，5 = 补充，6 = 调度，7 = 重新运行，8 = 暂停，9 = 停止</td></tr><tr><td>workflow_definition_code</td><td>bigint</td><td>目标工作流</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>用于恢复/重新执行操作</td></tr><tr><td>workflow_instance_priority</td><td>int</td><td>0 = 最高，1 = 高，2 = 中，3 = 低，4 = 最低</td></tr><tr><td>command_param</td><td>text</td><td>JSON 格式的执行参数</td></tr><tr><td>worker_group</td><td>varchar(255)</td><td>目标工作线程组</td></tr><tr><td>tenant_code</td><td>varchar(64)</td><td>执行的租户</td></tr><tr><td>dry_run</td><td>tinyint</td><td>0 = 正常，1 = 试运行（无实际执行）</td></tr></tbody></table><p><strong>处理流程</strong>：</p><ol><li>通过 API、调度程序或重试逻辑将命令插入 <code>t_ds_command</code>。</li><li>主服务器的 <code>MasterSchedulerThread</code> 持续扫描该表（按优先级、id 排序）。</li><li>主服务器生成 <code>t_ds_workflow_instance</code> 记录。</li><li>主服务器分析 DAG 并为就绪任务创建 <code>t_ds_task_instance</code> 记录。</li><li>成功处理的命令将被删除；失败的命令将移动到 <code>t_ds_error_command</code>。</li></ol><h2>版本控制系统</h2><h3>基于代码的版本控制模型</h3><p>DolphinScheduler 使用复杂的版本控制系统，支持：</p><ul><li>不同版本的并发执行。</li><li>安全更新而不影响正在运行的实例。</li><li>完整的变更审计跟踪。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578049" alt="" title="" loading="lazy"/></p><h3>版本管理规则</h3><ul><li><strong>当前版本表</strong>：只有“当前”版本存在于 <code>t_ds_workflow_definition</code> 和 <code>t_ds_task_definition</code> 中。</li><li><strong>日志表</strong>：所有版本保存在 <code>*_log</code> 表中，具有 <code>UNIQUE KEY (code, version)</code>。</li><li><strong>在线状态</strong>：每个代码只能有一个版本的 <code>release_state = 1</code>（在线）。</li><li><strong>实例锁定</strong>：工作流实例在创建时锁定到特定版本。</li><li><strong>版本不可变性</strong>：一旦某个版本被实例引用，其日志记录即为不可变。</li></ul><h2>调度体系架构</h2><h3>Quartz 集成</h3><p>DolphinScheduler 集成了 Quartz 调度程序以实现基于 cron 的调度。模式包括标准 Quartz 表以及一个映射表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578050" alt="" title="" loading="lazy"/></p><h4><code>t_ds_schedules</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>workflow_definition_code</td><td>bigint</td><td>目标工作流（唯一）</td></tr><tr><td>start_time</td><td>datetime</td><td>调度活动开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>调度活动结束时间</td></tr><tr><td>timezone_id</td><td>varchar(40)</td><td>cron 表达式的时区</td></tr><tr><td>crontab</td><td>varchar(255)</td><td>cron 表达式</td></tr><tr><td>release_state</td><td>int</td><td>0 = 离线，1 = 在线</td></tr><tr><td>failure_strategy</td><td>int</td><td>失败时的行为</td></tr><tr><td>workflow_instance_priority</td><td>int</td><td>实例的默认优先级</td></tr></tbody></table><p><strong>Quartz 表要点</strong>：</p><ul><li><code>QRTZ_TRIGGERS.NEXT_FIRE_TIME</code>：已索引，便于高效扫描。</li><li><code>QRTZ_CRON_TRIGGERS.CRON_EXPRESSION</code>：解析后的 cron 定义。</li><li><code>QRTZ_SCHEDULER_STATE</code>：跟踪 Quartz 调度程序实例。</li></ul><h2>资源和配置表</h2><h3>数据源管理</h3><h4><code>t_ds_datasource</code></h4><p>存储 SQL 任务的数据库连接配置。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>name</td><td>varchar(64)</td><td>数据源名称</td></tr><tr><td>type</td><td>tinyint</td><td>数据库类型（MySQL、PostgreSQL、Hive 等）</td></tr><tr><td>connection_params</td><td>text</td><td>JSON 格式的连接配置（主机、端口、数据库、凭据）</td></tr><tr><td>user_id</td><td>int</td><td>所有者用户</td></tr></tbody></table><p><strong>约束</strong>：<code>UNIQUE KEY (name, type)</code> - 防止数据源重复。</p><h3>文件资源</h3><h4><code>t_ds_resources</code>（已弃用）</h4><p><strong>注意</strong>：此表在模式中已标记为弃用。资源元数据正在迁移到单独的存储后端。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>full_name</td><td>varchar(128)</td><td>包括租户的完整路径</td></tr><tr><td>type</td><td>int</td><td>文件类型（文件/UDF）</td></tr><tr><td>size</td><td>bigint</td><td>文件大小（字节）</td></tr><tr><td>is_directory</td><td>boolean</td><td>目录标志</td></tr><tr><td>pid</td><td>int</td><td>父目录 ID</td></tr></tbody></table><h2>多租户与管理</h2><h3>项目、用户和租户层次结构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578051" alt="" title="" loading="lazy"/></p><h4>关键管理表</h4><h4><code>t_ds_tenant</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>tenant_code</td><td>varchar(64)</td><td>唯一租户标识符（唯一）</td></tr><tr><td>queue_id</td><td>int</td><td>任务的默认 YARN 队列</td></tr><tr><td>description</td><td>varchar(255)</td><td>租户描述</td></tr></tbody></table><p><strong>默认租户</strong>：系统创建一个默认租户，<code>id = -1</code>，<code>tenant_code = 'default'</code>。</p><h4><code>t_ds_user</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>user_name</td><td>varchar(64)</td><td>登录用户名（唯一）</td></tr><tr><td>user_password</td><td>varchar(64)</td><td>哈希密码</td></tr><tr><td>user_type</td><td>tinyint</td><td>0 = 普通用户，1 = 管理员</td></tr><tr><td>tenant_id</td><td>int</td><td>关联的租户（默认 -1）</td></tr><tr><td>email</td><td>varchar(64)</td><td>电子邮件地址</td></tr><tr><td>state</td><td>tinyint</td><td>0 = 禁用，1 = 启用</td></tr></tbody></table><h4><code>t_ds_project</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>code</td><td>bigint</td><td>唯一项目代码（唯一）</td></tr><tr><td>name</td><td>varchar(255)</td><td>项目名称（唯一）</td></tr><tr><td>user_id</td><td>int</td><td>创建者/所有者</td></tr><tr><td>description</td><td>varchar(255)</td><td>项目描述</td></tr></tbody></table><h2>JDBC 注册表</h2><p>对于不使用 ZooKeeper 的部署，DolphinScheduler 提供基于 JDBC 的注册表用于服务协调。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578052" alt="" title="" loading="lazy"/></p><h3>注册表详情</h3><h4><code>t_ds_jdbc_registry_data</code></h4><p>存储类似于 ZooKeeper 节点的注册表项。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>data_key</td><td>varchar(256)</td><td>类似路径的键（唯一）</td></tr><tr><td>data_value</td><td>text</td><td>序列化数据</td></tr><tr><td>data_type</td><td>varchar(64)</td><td><code>EPHEMERAL</code>（客户端断开连接时删除）或 <code>PERSISTENT</code></td></tr><tr><td>client_id</td><td>bigint</td><td>所属客户端</td></tr></tbody></table><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>last_update_time</td><td>timestamp</td><td>上次修改时间</td></tr></tbody></table><h4><code>t_ds_jdbc_registry_lock</code></h4><p>实现分布式锁。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>lock_key</td><td>varchar(256)</td><td>锁标识符（唯一）</td></tr><tr><td>lock_owner</td><td>varchar(256)</td><td>持有锁的客户端（格式：ip_processId）</td></tr><tr><td>client_id</td><td>bigint</td><td>所属客户端</td></tr></tbody></table><h4><code>t_ds_jdbc_registry_client_heartbeat</code></h4><p>跟踪活动客户端以清理临时数据。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>bigint</td><td>客户端 ID（主键）</td></tr><tr><td>client_name</td><td>varchar(256)</td><td>客户端标识符</td></tr><tr><td>last_heartbeat_time</td><td>bigint</td><td>上次心跳时间戳</td></tr><tr><td>connection_config</td><td>text</td><td>连接元数据</td></tr></tbody></table><p><strong>清理逻辑</strong>：当客户端的心跳过期时，其临时注册表数据和锁将自动删除。</p><h2>告警系统</h2><h3>告警表</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578053" alt="" title="" loading="lazy"/></p><h4><code>t_ds_alert</code></h4><p>由工作流/任务失败或完成生成的告警记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td>varchar(512)</td><td>告警标题</td></tr><tr><td>sign</td><td>char(40)</td><td>内容的 SHA1 哈希值（用于去重）</td></tr><tr><td>content</td><td>text</td><td>告警消息正文</td></tr><tr><td>alert_status</td><td>tinyint</td><td>0 = 等待，1 = 成功，2 = 失败</td></tr><tr><td>warning_type</td><td>tinyint</td><td>1 = 工作流成功，2 = 工作流/任务失败</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>源工作流实例</td></tr><tr><td>alertgroup_id</td><td>int</td><td>目标告警组</td></tr></tbody></table><p><strong>索引</strong>：<code>KEY idx_sign (sign)</code> - 实现去重。</p><h4><code>t_ds_alertgroup</code></h4><p>告警通道组。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>group_name</td><td>varchar(255)</td><td>唯一组名</td></tr><tr><td>alert_instance_ids</td><td>varchar(255)</td><td>逗号分隔的插件实例 ID</td></tr><tr><td>description</td><td>varchar(255)</td><td>组描述</td></tr></tbody></table><h2>索引与查询优化</h2><h3>关键索引</h3><p>该模式包含针对常见查询模式精心设计的索引：</p><ul><li><strong>工作流和任务查找</strong></li></ul><pre><code>- 按定义查询工作流实例：
  `KEY workflow_instance_index (workflow_definition_code, id)`
  - 按定义查询任务实例：
  `KEY idx_task_instance_code_version (task_code, task_definition_version)`
  - 用于监控的时间范围查询*：
  `KEY start_time_index (start_time, end_time)`</code></pre><ul><li><strong>命令处理</strong>：</li></ul><pre><code>基于优先级的命令扫描：
`KEY priority_id_index (workflow_instance_priority, id)`</code></pre><ul><li><strong>DAG 关系查询</strong></li></ul><pre><code>- 正向和反向 DAG 遍历：
  `KEY idx_pre_task_code_version (pre_task_code, pre_task_version)`
   正向和反向 DAG 遍历：
   `KEY idx_post_task_code_version (post_task_code, post_task_version)`
  `KEY idx_code (project_code, workflow_definition_code)`</code></pre><h3>唯一约束</h3><p>在数据库级别强制执行的关键业务规则：</p><table><thead><tr><th>表</th><th>约束</th><th>目的</th></tr></thead><tbody><tr><td><code>t_ds_workflow_definition</code></td><td><code>UNIQUE (name, project_code)</code></td><td>项目中无重复的工作流名称</td></tr><tr><td><code>t_ds_workflow_definition</code></td><td><code>UNIQUE (code)</code></td><td>全局工作流标识符</td></tr><tr><td><code>t_ds_workflow_definition_log</code></td><td><code>UNIQUE (code, version)</code></td><td>每个版本一条记录</td></tr><tr><td><code>t_ds_datasource</code></td><td><code>UNIQUE (name, type)</code></td><td>每种类型无重复的数据源名称</td></tr><tr><td><code>t_ds_schedules</code></td><td><code>UNIQUE (workflow_definition_code)</code></td><td>每个工作流一个调度</td></tr></tbody></table><h2>模式演变与升级</h2><p>DolphinScheduler 在 <code>dolphinscheduler - dao/src/main/resources/sql/upgrade</code> 中维护用于跨版本模式迁移的升级脚本。</p><h3>近期模式变更</h3><h4>3.3.0 变更</h4><ul><li>将表和列从“process”重命名为“workflow”。</li><li>删除数据质量表（<code>t_ds_dq_*</code>）。</li><li>添加用于替代 ZooKeeper 的 JDBC 注册表。</li><li>从任务表中删除与缓存相关的列。</li></ul><h4>3.2.0 变更</h4><ul><li>向工作流定义中添加 <code>execution_type</code>（并行/串行模式）。</li><li>为串行执行链添加 <code>next_workflow_instance_id</code>。</li><li>向命令和实例表中添加 <code>tenant_code</code>。</li><li>创建 <code>t_ds_project_parameter</code> 和 <code>t_ds_project_preference</code>。</li></ul><h2>数据库交互模式</h2><h3>服务层访问</h3><p>数据库访问通过 <code>dolphinscheduler - dao</code> 中的 DAO 层进行抽象。<br/><strong>关键服务类</strong>：</p><ul><li><code>ProcessService</code>：工作流/任务定义和实例的 CRUD 操作。</li><li><code>CommandService</code>：命令队列管理。</li><li><code>ProjectService</code>：项目和权限管理。</li><li><code>ResourcesService</code>：资源元数据操作。</li></ul><h3>事务管理</h3><p>大多数操作使用 Spring 的 <code>@Transactional</code> 注解实现：</p><ul><li>原子性地创建工作流实例及其任务实例。</li><li>消费命令并创建实例。</li><li>版本更新与日志表同步。</li></ul><h3>连接池</h3><p>系统使用 HikariCP 进行连接池，在 <code>application.yaml</code> 中配置：</p><ul><li>默认池大小：50 个连接。</li><li>连接超时：30 秒。</li><li>空闲超时：600 秒。</li></ul>]]></description></item><item>    <title><![CDATA[2026泛监测平台推荐榜单发布：自适应 · 协同 · 可洞察型平台谁在领跑？ 底层逻辑探索 ]]></title>    <link>https://segmentfault.com/a/1190000047578088</link>    <guid>https://segmentfault.com/a/1190000047578088</guid>    <pubDate>2026-01-28 16:05:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化与数据化加速融合的背景下，泛监测平台正从“单一监控工具”升级为“覆盖数据、接口、行为、风险的综合治理中枢”。本文从自适应能力、协同能力、可洞察能力三个核心维度出发，对国内主流泛监测平台进行系统评析与专业推荐。<br/><strong>一、泛监测平台的发展趋势与能力演</strong>进<br/>提示：要理解平台价值，首先要看泛监测从“被动感知”走向“主动洞察”的能力跃迁。<br/>传统监测系统更多停留在日志收集、告警触发与基础审计层面，而新一代泛监测平台正向“全域感知 + 智能分析 + 协同治理”方向演进，主要呈现三大趋势：<br/>第一，从“静态规则”走向“自适应分析”。新一代平台引入AI模型、UEBA行为分析、无监督学习机制，使系统可以根据用户行为、业务变化和数据流动情况动态调整监测策略，避免长期依赖固定规则带来的高误报与低发现率问题。<br/>第二，从“孤岛式部署”走向“协同式联动”。平台不再是单点工具，而是与SOC、SIEM、工单系统、数据治理平台、API网关等系统协同运行，形成跨部门、跨系统、跨流程的风险治理闭环。<br/>第三，从“可见”走向“可洞察”。监测不只停留在“看到异常”，而是要做到“理解风险、还原路径、预测趋势”，实现从事件级监控到资产级、行为级、业务级洞察的升级。<br/><strong>二、泛监测平台核心能力模型</strong><br/>提示：评估一个平台是否优秀，必须回到自适应、协同、可洞察这三个关键指标。<br/>自适应能力优秀的泛监测平台应具备自动学习、动态校准、策略自进化能力，包括：<br/>● 自动识别业务变化对数据流动的影响<br/>● 动态调整风险阈值与监测重点<br/>● 在新接口、新系统上线时快速纳入监测范围<br/>协同能力平台要具备良好的开放性与编排能力：<br/>● 与SOC/SIEM/工单系统联动处置<br/>● 与数据分类分级、数据资产管理系统协同<br/>● 与API网关、零信任体系联动防护<br/>可洞察能力不仅“发现问题”，还要“理解问题”：<br/>● 构建数据资产地图与流动视图<br/>● 实现风险路径还原与影响面评估<br/>● 提供趋势预测与治理建议能力<br/><strong>三、2025 年泛监测平台产品推荐排名</strong><br/>提示：在综合技术成熟度、场景适配度与市场验证后，以下是通用行业适用的核心产品梯队。<br/>第一名：奇安信 泛监测与数据治理平台<br/>奇安信平台以“全域感知 + 零信任联动”为核心优势，在大型政企、金融与基础设施行业拥有广泛应用。<br/>其泛监测体系覆盖数据库、API、云存储、大数据平台等多个维度，结合用户行为分析与流量建模技术，构建“数据—行为—风险”全链路视图。<br/>在自适应方面，平台通过AI模型不断校准风险基线，对异常导出、越权访问、接口滥用等场景具备较高识别准确率。在协同方面，奇安信与自身SOC、终端安全、网络安全体系深度联动，形成跨域响应闭环。在可洞察方面，其数据流动可视化能力成熟，适合对安全可控要求极高的客户群体。<br/>第二名：全知科技 泛监测与数据安全协同平台<br/>全知科技将“API安全即数据安全核心关口”的理念引入泛监测领域，构建了以数据资产地图 + API风险监测 + 智能分析引擎为核心的协同型监测体系。<br/>在自适应能力方面：全知科技通过AI驱动的数据分类分级与行为建模，使平台可根据业务变化动态调整监测重点。系统能够自动扫描表结构、接口结构、调用路径，生成实时资产图谱，敏感数据识别准确率达95%，效率相比人工提升90%以上。<br/>在协同能力方面：全知科技强调“理念—技术—场景”的协同创新，其泛监测平台可与数据治理系统、合规审计系统、工单系统联动运行，实现从发现风险到整改闭环的自动协同。同时，其“知影-API风险监测系统”与“知形-数据库风险监测系统”构成前后端联动，覆盖数据生产、调用、流转与使用全链路。<br/>在可洞察能力方面：全知科技突出“可知、可管、可控、可见”的能力体系，不仅能看到风险事件，更能还原风险路径、定位责任主体、评估影响范围。在金融、医疗、保险等场景中，平台已实现对异常API调用、数据越权访问、敏感字段泄露的秒级溯源。<br/>典型实践中，某三甲医院部署后旧版API泄露风险下降98%；在金融行业实现数据资产从“看不见”到“全闭环可控”的治理跃迁。<br/>第三名：启明星辰 泛监测与风险闭环平台<br/>启明星辰依托“九天·泰合”智能引擎，在风险识别与闭环治理方面表现突出。<br/>平台支持跨数据库、API、BI工具的统一监测与审计，能够基于角色、行为与数据敏感度动态调整访问策略。在自适应方面，其策略引擎可结合用户行为画像不断修正风险模型；在协同方面，平台与政务SOC体系、日志审计平台高度融合；在可洞察方面，适合对审计合规与流程闭环要求极高的组织。<br/>第四名：天融信 泛监测与数据流动治理平台<br/>天融信在工业互联网与跨网环境下的泛监测能力具有明显优势。<br/>其动态数据流向地图技术可在复杂网络隔离场景下追踪数据流动路径。平台强调与防火墙、终端安全系统的协同防护，适用于制造、能源等对跨域数据交互敏感的行业。<br/>第五名：阿里云 数据安全中心（DSC）<br/>阿里云DSC基于云原生架构，在多云与互联网企业场景中优势明显。<br/>其自动发现、分类分级与异常行为检测能力成熟，适合云上资产规模大、变化快的客户。在自适应方面依托云侧AI模型；在协同方面与阿里云生态产品联动紧密；在可洞察方面更侧重于云资源与数据使用行为分析。<br/>第六名：深信服 泛监测与零信任协同平台<br/>深信服强调轻量化部署与零信任融合。<br/>平台适合中型组织快速构建“身份 + 数据 + 行为”一体化监测能力，在教育、医疗、中小企业市场适配性强。<br/><strong>四、泛监测平台选型与落地建议</strong><br/>提示：选平台不是买功能，而是选择“是否能长期协同业务演进”的能力体系。<br/>明确业务驱动场景优先从高频、高风险数据场景切入，如API调用、BI报表导出、批量下载等。<br/>验证自适应能力重点测试平台是否能在业务变化后自动纳入新系统、新接口、新数据类型的监测范围。<br/>关注协同治理能力选择能与现有SOC、数据治理、工单系统协同的平台，避免形成新的工具孤岛。<br/>重视可洞察输出不仅要看告警数量，更要看是否提供“风险路径、影响评估与治理建议”。<br/><strong>五、结语：泛监测进入“洞察驱动治理”阶段</strong><br/>提示：未来的泛监测平台，核心竞争力将不再是“监控多少”，而是“洞察多深、协同多强”。<br/>2025年的泛监测平台市场已经从“合规达标”走向“价值创造”。企业需要的不是更多工具，而是一个能自适应业务变化、能协同各类系统、能真正洞察数据风险本质的综合治理中枢。<br/>在这一趋势下，以全知科技为代表的“协同型、洞察型泛监测平台”，正推动企业从被动防守转向主动治理，为构建以数据为中心的新型安全体系奠定坚实基础。</p>]]></description></item>  </channel></rss>