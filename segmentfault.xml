<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[智能体对传统行业的冲击：客服真的难逃被智能体取代的命运？ 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047578270</link>    <guid>https://segmentfault.com/a/1190000047578270</guid>    <pubDate>2026-01-28 17:08:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><h3>一、智能体席卷客服领域：替代浪潮下的行业现状</h3><p>1.1 智能体客服的技术突破与落地速度1.2 多行业智能体客服的替代数据与实践1.3 传统客服岗位的生存现状与结构变化</p><h3>二、不可替代的核心价值：智能体难以突破的客服壁垒</h3><p>2.1 情感共鸣：复杂情绪场景的人类同理心优势2.2 灵活决策：非标准化复杂问题的人工处理能力2.3 信任建立：高价值业务中的人工沟通独特性</p><h3>三、人机共生：客服行业的终极转型方向</h3><p>3.1 智能体与人工客服的职责边界划分3.2 人机协同的客服服务模式落地路径3.3 传统客服的职业转型与能力重塑</p><h3>四、行业未来：智能体赋能下的客服行业新生态</h3><p>4.1 智能体驱动的效率升级与成本优化4.2 客服岗位的价值重构与新兴职业机会4.3 客服体系的智能化改造趋势</p><h3>五、结语</h3><h3>六、FAQ</h3><h2>摘要</h2><p>在智能体技术快速落地的背景下，客服行业成为传统行业中受冲击最直接的领域之一。智能体凭借 7×24 小时服务、高标准化问题处理效率、低成本运营等优势，在政务、文旅、电商、物流等多行业规模化应用，替代了超 50% 的常规客服工作，传统客服岗位的生存挑战被持续放大。本文基于智能体客服的实际落地数据与案例，剖析其对客服行业的替代现状，深入探讨智能体在情感处理、复杂决策等场景中难以突破的能力壁垒，明确人工客服不可替代的核心价值，提出 “智能体处理标准化工作 + 人工客服承接高价值场景” 的人机共生模式，为传统客服从业者的职业转型提供能力重塑方向，最终揭示智能体并非传统客服的 “替代者”，而是行业的 “赋能者”，其将推动客服行业实现效率与体验的双重升级，重构行业新生态。</p><h2>一、智能体席卷客服领域：替代浪潮下的行业现状</h2><p>智能体技术与大语言模型、自然语言处理的融合应用，让智能体客服从 “机械应答” 升级为 “智能理解”，迅速席卷各行业客服领域，成为企业降本增效的核心选择，传统客服行业迎来前所未有的替代浪潮。</p><p>智能体客服的落地与适配能力远超预期，从需求确认到系统上线最短仅需 12 天，可实现多语种、多方言识别，还能与企业现有呼叫中心、CRM 系统无缝对接，快速适配文旅旺季海量咨询、物流全场景服务、电商日常答疑等不同需求，形成标准化服务能力。</p><p>从替代数据来看，智能体客服在多行业的独立处理率已达 51%-60%，携程、同程等平台的自助解决率更是突破 75%，日均处理咨询量超百万次。恩施文旅落地智能体客服后，旺季人工坐席需求从 30 人降至 12 人，人力成本节省 60%；物流行业智能体客服实现 “1 个顶 10 个传统客服”，24 小时承接询价、查单等全流程服务。IDC 预测，2026 年超 70% 的企业将部署 AI 语音交互系统替代传统 IVR 服务，Gartner 也指出，2025 年 AI 将处理 80% 的常规客户服务互动。</p><p>替代浪潮直接引发传统客服岗位的结构变化，基础标准化客服岗位大幅缩减，人工坐席需求向 “少而精” 转变，从业者面临岗位淘汰与职业转型的双重压力。同时，企业客服体系的运营逻辑从 “人工为主、工具为辅” 转向 “智能体为主、人工兜底”，客服中心的人力配置、管理模式均随之调整，行业进入深度重构阶段。</p><h2>二、不可替代的核心价值：智能体难以突破的客服壁垒</h2><p>尽管智能体客服在常规工作中表现亮眼，但从行业实践来看，其并非万能，在客服核心服务场景中仍存在难以突破的能力壁垒，这正是人工客服不可替代的关键，也决定了客服岗位不会被完全取代。</p><p>情感共鸣是人工客服最核心的优势。智能体虽能通过技术实现情绪识别，却无法真正实现情感共鸣与同理心表达。在投诉处理、情绪安抚、售后纠纷调解等场景中，客户的核心需求不仅是解决问题，更是情绪的释放与被理解。人工客服能通过语气、措辞的灵活调整精准捕捉情绪痛点，进行共情式沟通；而智能体的安抚话术基于算法预设，难以应对个性化情绪表达，无法建立真正的情感连接。</p><p>灵活决策能力是智能体的重要短板。其仅能处理知识库内的标准化问题，面对非标准化、跨场景的复杂问题，缺乏自主判断与灵活解决的能力。高端客户定制化服务、跨部门业务协调、突发非预案问题处理等高价值场景，需要客服人员结合企业实际、客户需求与行业经验做出灵活应对，这是依托预设算法与知识库的智能体无法实现的，超出范围的问题只能转接人工。</p><p>在高价值业务场景中，人工沟通是建立客户信任的关键，这一价值无法被智能体替代。房产、汽车、高端医美等客单价高、决策周期长的行业，客户不仅需要获取信息，更需要通过深度沟通建立对企业的信任。人工客服能通过专业讲解、及时答疑、个性化建议打消客户顾虑，推动决策落地；而智能体的标准化回复难以传递人格化特征，无法根据客户反应调整沟通策略，在信任建立环节存在天然劣势。</p><h2>三、人机共生：客服行业的终极转型方向</h2><p>面对智能体的冲击，客服行业的终极发展方向并非 “智能体替代人工”，而是 “人机共生、各取所长”。通过明确职责边界、构建高效的人机协同模式，既能发挥智能体的效率优势，又能保留人工客服的价值优势，实现服务效率与体验的双重升级。</p><p>明确职责边界是人机共生的基础，核心遵循 “智能体优先处理标准化工作，人工客服承接高价值场景” 原则。将客户咨询分为三个层级：标准化问题（产品查询、订单核对、流程指引等）由智能体全程独立处理，占比可达 60%-80%；中等复杂问题（简单售后、常规投诉登记）由智能体初步处理后转交人工跟进；高复杂问题（复杂投诉、定制化服务、激烈情绪沟通等）直接由人工承接，同时智能体为人工提供数据支持、对话上下文等辅助信息，提升处理效率。</p><p>人机协同服务模式的落地，核心在于 “智能分流、无缝转接、数据赋能”。智能体通过意图识别、情绪识别技术实现智能分流，将问题精准分配至对应处理主体；无法处理时实现无感知人工转接，保留完整对话上下文，避免客户重复表述；同时通过大数据分析，为人工客服提供客户画像、历史咨询记录、业务数据等信息，助力精准把握客户需求，实现个性化服务。</p><p>人机共生模式下，传统客服的核心转型方向是 “能力重塑”，从 “标准化操作型” 向 “高价值服务型” 转变。从业者需要提升三大核心能力：一是情感服务能力，强化共情能力与沟通技巧，专注复杂情绪场景处理与客户关系维护；二是专业解决能力，深入学习企业业务知识，提升复杂问题分析、跨部门协调的能力，成为领域专业客服；三是数据应用能力，学会运用智能体的数据分析报告，把握客户需求趋势，为服务优化、业务决策提供建议。同时，企业需建立常态化培训体系，帮助传统客服完成能力升级，适应新岗位要求。</p><h2>四、行业未来：智能体赋能下的客服行业新生态</h2><p>智能体对客服行业的冲击，本质上是技术推动的行业升级，其并非传统客服的 “淘汰者”，而是推动行业向更高效、更专业、更高价值方向发展的 “赋能者”。未来，在智能体赋能下，客服行业将形成全新生态，实现效率、价值与职业的多重变革。</p><p>智能体将持续推动客服行业的效率升级与成本优化，成为企业客服体系的基础配置。随着大模型、多模态、自主学习技术的发展，智能体客服的处理能力将持续提升，覆盖更多标准化场景并向部分中等复杂场景延伸，进一步提升体系运行效率。其 7×24 小时服务、低运营成本的优势，将帮助企业打破时间与空间限制，实现客服服务全域覆盖，大幅降低人力与管理成本。数据显示，智能体客服的单次服务成本仅为人工的 1/10，投资回报周期最短仅 8 个月，成为企业客服数字化转型的必选。</p><p>客服岗位的价值将被重新定义，从 “成本中心” 向 “价值中心” 转变，同时催生大量新兴职业机会。传统客服行业被视为企业成本中心，核心价值是解决问题；而在智能体赋能下，人工客服从繁琐的标准化工作中解放，专注高价值服务场景，成为客户关系维护、品牌形象塑造、业务转化的重要力量，可通过深度沟通挖掘客户潜在需求，实现交叉销售与增值服务，创造直接商业价值。同时，智能体的落地运营，催生了 AI 客服训练师、智能体运营专员、客服数据分析师等新兴职业，这类职业要求从业者兼具客服业务知识与 AI 技术能力，成为行业新增长点。</p><p>全行业的客服体系将迎来全面智能化改造，形成 “智能体 + 人工客服” 深度融合的标准化服务体系，行业服务标准与评价体系也将随之重构。各行业将结合自身业务特点，搭建定制化智能体客服系统，实现与企业业务、客户管理系统的深度融合，形成全链路智能化服务；客服行业的评价标准将从 “响应速度、问题解决率” 等单一效率指标，向 “客户满意度、情感体验、价值创造” 等多维指标转变，更注重服务的温度与价值。此外，行业将建立智能体客服的技术标准、运营规范，推动客服行业规范化、标准化发展，最大化发挥智能体的赋能价值。</p><h2>五、结语</h2><p>智能体的快速发展让客服行业迎来前所未有的变革，也让 “客服是否会被智能体完全取代” 成为行业热议话题。但从行业实践与技术发展来看，智能体虽能替代传统客服的标准化工作，却无法复制人工客服的同理心、灵活决策能力与信任建立能力，这决定了客服行业不会走向 “全智能体化”，而是形成 “人机共生” 的全新格局。</p><p>对于客服行业而言，智能体的冲击并非危机，而是行业升级的契机，推动传统客服摆脱 “人力密集、效率低下、价值单一” 的发展困境，向 “智能驱动、人机协同、价值导向” 的新生态转型；对于传统客服从业者而言，这并非职业终点，而是职业升级的起点，唯有通过能力重塑，从标准化操作转向高价值服务，才能在行业变革中站稳脚跟。</p><p>未来，客服行业的核心竞争力将在于 “智能体的效率 + 人工的温度”，唯有实现技术与人性的深度融合，才能让客服服务既高效又有温度，既降本又能创造价值。而智能体与人工客服的共生共赢，也将成为智能时代传统行业转型升级的典型样本，为其他行业的智能化改造提供重要参考与借鉴。</p><h2>六、FAQ</h2><h3>1. 智能体客服目前的独立处理率能达到多少？</h3><p>在政务、文旅、交通、电商等行业，智能体客服独立处理率已达 51%-60%，携程、同程等平台自助解决率突破 75%，Gartner 预测 2025 年 AI 将处理 80% 的常规客户服务互动。</p><h3>2. 智能体客服无法处理哪些客服场景？</h3><p>主要难以处理三类场景：需要情感共鸣的复杂情绪场景（如激烈投诉、情绪安抚）、非标准化的复杂决策场景（如高端定制服务、跨部门协调）、需要深度建立信任的高价值业务场景（如房产、汽车等客单价高的行业咨询）。</p><h3>3. 人机共生模式下，传统客服该如何转型？</h3><p>核心是从 “标准化操作型” 向 “高价值服务型” 转变，重点提升三大能力：情感服务能力（共情、沟通技巧）、专业解决能力（复杂问题分析、跨部门协调）、数据应用能力（运用数据分析把握客户需求），同时依托企业的常态化培训体系完成能力升级。</p><h3>4. 智能体客服能为企业带来哪些实际价值？</h3><p>核心体现在降本、提效、提质三方面：人工成本可节省 40%-60%，单次服务成本仅为人工的 1/10；客户响应速度提升数倍，高峰时段并发接待能力提升 10 倍以上；24 小时服务覆盖提升客户满意度，投诉率可下降 65% 左右。</p><h2>参考文献</h2><p>[1] 5 个行业 AI 语音客服落地案例：真实数据验证降本增效\_大模型客服前沿笔记</p><p>[2] AI 智能体颠覆传统服务业：旅行社、客服首当其冲\_CSDN 博客</p><p>[3] 客服行业会被 AI 完全替代吗？人机协作的终极形态分析\_来鼓 AI</p><p>[4] 传统物流客服即将被 AI 智能物流客服取代？\_抖音行业热点</p>]]></description></item><item>    <title><![CDATA[VMware ESXi 9.0.2.0 macOS Unlocker & OEM BIOS 2.7 ]]></title>    <link>https://segmentfault.com/a/1190000047578278</link>    <guid>https://segmentfault.com/a/1190000047578278</guid>    <pubDate>2026-01-28 17:07:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 2.7 标准版和厂商定制版</p><p>ESXi 9.0 标准版，Dell (戴尔)、HPE (慧与)、Lenovo (联想)、Inspur/IEIT SYSTEMS (浪潮)、H3C (新华三)、Cisco (思科)、Fujitsu (富士通)、Hitachi (日立)、NEC (日电)、Huawei (华为)、xFusion (超聚变) OEM 定制版</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=wfKgoKVeyRU0x%2Fth24Gxlg%3D%3D.mBmwlKqvS5u5qmyj%2Ffs%2FzpULLPhjkHlcL2UF%2FZPReYzrKEEbG5XxOByZWSfiGL3Q" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=PTyh5xKsHs9NN2kI0KU1ug%3D%3D.KY%2BT978%2FATRLdnmSulGyCearVu3GsaGSaJcn88pq2uo%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>VMware vSphere 是 VMware 的虚拟化平台，可将数据中心转换为包括 CPU、存储和网络资源的聚合计算基础架构。vSphere 将这些基础架构作为一个统一的运行环境进行管理，并为您提供工具来管理加入该环境的数据中心。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046438842" alt="说明 ESXi 主机、vCenter Server、虚拟机和 vSphere Client 之间关系的 VMware vSphere 概览图" title="说明 ESXi 主机、vCenter Server、虚拟机和 vSphere Client 之间关系的 VMware vSphere 概览图"/></p><p>vSphere 的两个核心组件是 ESXi 和 vCenter Server。ESXi 是用于创建并运行虚拟机和虚拟设备的虚拟化平台。vCenter Server 是一项服务，用于管理网络中连接的多个主机，并将主机资源池化。</p><h2>通用特性概览</h2><p>该版本在官方原版基础上新增以下特性：</p><ul><li>macOS Unlocker：来自 GitHub 的 <a href="https://link.segmentfault.com/?enc=d3vQI%2F7nWo048maH1qHwpg%3D%3D.7hdw1EifMS4Axz7CZ8gPRyzzBjtLAsM9mNz6nLJD%2FsIiyk9ILpPPOKfrY%2FutCji1" rel="nofollow" target="_blank">Unlocker 4</a>，现已支持 macOS Tahoe</li><li>OEM BIOS 2.7：使用社区最流行的 OEM BIOS/EFI64，现已支持 Windows Server 2025</li><li>LegacyCPU support，允许在不受官方支持的旧款 CPU 上安装 ESXi 9.0</li><li>ESX-OSData 卷大小修改为 8GB，解决自 ESXi 7.0 起系统占用磁盘空间过大的问题（超过 142GB）</li><li>有限支持采用混合架构的第 12 代及以上 Intel 处理器，可实现正常引导和运行</li><li>同时提供 Dell、HPE 和 Lenovo 等厂商定制版映像 (sysin)，包含了必要的驱动和 OEM 软件</li></ul><h3>直接运行 macOS Tahoe</h3><p>参看：<a href="https://link.segmentfault.com/?enc=h5%2FP3JzKPcWNzf5%2BD3DuZw%3D%3D.x%2BJ1btXZJSX8IismC%2Bp6BXr%2Bszb%2FGDNf%2FTehb5zOkvbZCQUf9PeBsw9WwWEFkfag" rel="nofollow" target="_blank">macOS 26 Blank OVF - macOS Tahoe 虚拟化解决方案</a></p><p>ESXi 默认是支持创建 macOS 虚拟机的，但该功能仅限于 Apple Mac 硬件上启用。该版本解锁了对 macOS 虚拟化的支持，在任意非 Mac 硬件上可以直接运行 macOS 虚拟机。</p><p>⚠️ macOS 虚拟机与 Mac 上的 macOS 体验有天壤之别，仅用于体验而已。开启 macOS 卓越性能的唯一平台是搭载 Apple M 芯片的 Mac。尽早加入 Apple 阵营，开启卓越体验吧。</p><p>直接新建虚拟机，操作系统选择 “Apple macOS 12 (64-bit)”，即可安装和正常启动：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046829041" alt="New VM in ESXi 9" title="New VM in ESXi 9" loading="lazy"/></p><p>虚拟化中的 macOS Tahoe：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046878483" alt="macOS Tahoe in VMware" title="macOS Tahoe in VMware" loading="lazy"/></p><p>附：</p><ul><li><a href="https://link.segmentfault.com/?enc=j5PDIP9ygHGdoJPesl%2Bc4w%3D%3D.UYOGtLwuZOpuCbVBdqI%2BZ5H0ff2O3Y17K65PyBCUJTA%2FTmciAdetiCBRCVCFGnzx" rel="nofollow" target="_blank">macOS Tahoe 26.2 (25C56) Boot ISO 原版可引导映像下载</a></li><li><a href="https://link.segmentfault.com/?enc=XfAPNR4Tdd%2B2YdcIamLByw%3D%3D.CdkfPDgAwd4Fhe71tPu3tgNJk%2BbbKZyNf%2FFKs3NedGQWqjdb1X%2FnX%2B%2FdZZKi7duu" rel="nofollow" target="_blank">macOS Sequoia 15.7.3 (24G419) Boot ISO 原版可引导映像下载</a></li><li><a href="https://link.segmentfault.com/?enc=mWquqRT5ot1EaR0TdeJsEA%3D%3D.Gt91UiL0jmyPTkSq76jo2eri89ncwwt%2FnqmxLDgSMMXS5SJuZsh1ev76w1tE0QLO" rel="nofollow" target="_blank">macOS Sonoma 14.8.3 (23J220) Boot ISO 原版可引导映像下载</a></li><li>更多：<a href="https://link.segmentfault.com/?enc=eiMvnl%2FLhV62htD3RjPn1Q%3D%3D.jbt3%2F5VSGAgL66sp4MGXhRRTKmdZHZ4IqmMLW8%2FzDXI%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></li></ul><h3>VMware Dell 2.7 BIOS EFI64 ROM</h3><p>来自社区最新的 OEM BIOS/EFI64，现已更新支持 Windows Server 2025。</p><p>BIOS.440 &amp; EFI64.ROM - Dell 2.7 OEM BIOS: NT 6.0 (Vista/Server 2008), NT 6.1 (7/Server 2008 R2), NT 6.2 (Server 2012), NT 6.3 (Server 2012 R2), NT 10.0 (Server 2016/Server 2019/Server 2022/Server 2025)</p><p>Windows Server OVF 系列：</p><ul><li><a href="https://link.segmentfault.com/?enc=CeHfaI%2BONIgW9fdYT1E4cA%3D%3D.uAW7y9OQFVha8w96A%2FuMbTDhbBbdbD%2BtPA38eemkthPaF9X%2B5xvNvQNb6taqxkPq" rel="nofollow" target="_blank">Windows Server 2025 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=9lHiV65xXcswWA0b4I88qg%3D%3D.GO6nX5ur3zU%2FESIziOW%2F7AWd8mSU861Cln7EA4N44sRjGFC4%2FrHXHt8H1sA%2BEyjz" rel="nofollow" target="_blank">Windows Server 2022 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=RU%2B1us4ISnPNbMT%2B3%2BqVvg%3D%3D.FiI4VGMACRDD15l6hsoXLQe02ivN5U3DzfUw3nmRkap75Laz5Ei62Oyk33%2F%2FiP2U" rel="nofollow" target="_blank">Windows Server 2019 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=yxy4AUf0rhwjwNWKqEQ5Mw%3D%3D.bcintbGhxHaW9IhYYwjI%2B6KEVKBLx5dtvFPpmx07heYk6tYII3OfCD2G8zQpKqpB" rel="nofollow" target="_blank">Windows Server 2016 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li><li><a href="https://link.segmentfault.com/?enc=s%2BsAfqKeKewQjhpLoazfrw%3D%3D.oMRXUajax5AaKE1pMfW8Mxo5j3YfjtPOJ658gIWDzR3CwXP4Jyjo7zi0lruQa27PoRizPsMdHeMmOC4gF1hd4Q%3D%3D" rel="nofollow" target="_blank">Windows Server 2008 R2 OVF (2025 年 12 月更新) - VMware 虚拟机模板</a></li></ul><p>其他 OVF，如：<a href="https://link.segmentfault.com/?enc=S3IOzV84cTF1B4yxFxZ8wA%3D%3D.eKZzdsnnkHKfy0P9vtg5RCa4AQHgrTbFa6C3OjrJQDPHlQAGLWWr5ulmtw7btION" rel="nofollow" target="_blank">Rocky Linux 10 x86_64 OVF (sysin) - VMware 虚拟机模板</a>，<a href="https://link.segmentfault.com/?enc=hLXD3WUXoElGkCRid8tb%2Bw%3D%3D.qBtAmAu9Vf648VntR8fnNM8k%2FgxRGegeOlT6stFgpT2ju9o%2BXUUX7iHsK%2BgOefwQ" rel="nofollow" target="_blank">Ubuntu 24.04 LTS x86_64 OVF (sysin) - VMware 虚拟机模板</a>，更多请在本站搜索 “OVF”。</p><h3>支持不受官方支持的旧款 CPU</h3><p><strong>ESXi 9.0 同样废弃了对部分旧款 CPU 的支持</strong>，笔者根据相关文档判断以下 CPU 将不受 ESXi 9.0 支持：</p><ul><li><p>Intel</p><ul><li>Xeon D‑1500 Series</li><li>Xeon E3‑1200‑V5 / E3‑1500‑V5 Series</li><li>Xeon E5‑2600‑V4 / E5‑1600‑V4 Series</li><li>Xeon E5‑4600‑V4 Series</li><li>Xeon E7‑8800/4800‑V4 Series</li><li>Xeon E3‑1200‑V6 Series</li><li>Intel Xeon Platinum 8100 / Gold 6100/5100 / Silver 4100 / Bronze 3100 Series</li><li>Xeon D‑2100 Series</li><li>Xeon W‑2100 Series</li></ul></li><li><p>AMD</p><ul><li>Bulldozer 架构（如 Opteron 6200/4200/3200）</li><li>Piledriver 架构（如 Opteron 4300/6300 系列）</li><li>Steamroller 架构（如 Opteron X2250/X1250 Berlin）</li><li>Kyoto 架构（如 Opteron X1100/X2100）</li></ul></li></ul><p><strong>ESXi 8.0 同样废弃了对部分旧款 CPU 的支持</strong>，以下 CPU 将不受 ESXi 8.0 支持：</p><ul><li>Intel Family 6, Model = 2A (Sandy Bridge DT/EN, GA 2011)</li><li>Intel Family 6, Model = 2D (Sandy Bridge EP, GA 2012)</li><li>Intel Family 6, Model = 3A (Ivy Bridge DT/EN, GA 2012)</li><li>AMD Family 0x15, Model = 01 (Bulldozer, GA 2012)</li></ul><p>vSphere 7.0 Update 2 及更高版本中 ESX 安装程序显示的如下警告消息已经明示：<br/> CPU_SUPPORT_WARNING: The CPUs in this host may not be supported in future ESXi releases. Please plan accordingly.</p><p><strong>修改启动参数，在官方不受支持的 CPU 的服务器上可以正常安装。</strong></p><p>根据 VMware vSphere 7.0 Release Notes，以下 CPU 已经不受支持（无法安装或者升级 ESXi 7.0）</p><p>Comparing the processors supported by vSphere 6.7, vSphere 7.0 no longer supports the following processors:</p><ul><li>Intel Family 6, Model = 2C (Westmere-EP)</li><li>Intel Family 6, Model = 2F (Westmere-EX)</li></ul><p>笔者在一台 2010 年发布的服务器上安装运行良好 (sysin)：HP DL 380 G7，Intel® Xeon® CPU E5606</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044308374" alt="ESXi 7.0 on LegacyCPU" title="ESXi 7.0 on LegacyCPU" loading="lazy"/></p><p>备注：本截图为 7.0 版本</p><h3>ESX-OSData 卷大小修改为 8GB</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046829042" alt="ESXi 9 VMFSL" title="ESXi 9 VMFSL" loading="lazy"/></p><p><strong>ESXi 9.0 对存储容量的要求未有明显变更，以下 ESXi 8.0 的描述基本适用。</strong></p><p>⚠️ 在 ESXi 8.0 中建议放弃使用 USB/SD 卡作为系统存储介质（虽然 SD 卡和 USB 介质继续获得有限支持，详见 <a href="https://link.segmentfault.com/?enc=jlwe%2FBmYKHPX1BBa6aYFLw%3D%3D.URS5DfWSaqYcdf2%2FoB3ftgbv%2FVtWFkWre11Q8kdV8JQ%2FQ7nH4fEKrZuXA3ZmPz%2Fv" rel="nofollow" target="_blank">KB85685</a>）。</p><p>从 ESXi 7.0 开始，对磁盘空间的要求有所变化：</p><ul><li>8GB SD 卡 + 32GB 本地磁盘</li><li>32GB 本地磁盘</li><li>142G 或者更大的本地磁盘</li></ul><p>通常我们在一块数百 GB 或者更大的本地磁盘上安装 ESXi，系统分区磁盘空间将占用 142GB 以上，整个系统分区（内核参数：systemMediaSize）需要 138GB 和 4GB 以上的空闲空间，其中 ESX-OSData volume 大约需要 120GB 的磁盘空间，对于磁盘空间紧张情况下可能有一定的浪费 (sysin)。修改后，系统安装后占用的磁盘空间不超过 16GB（特别是针对个人实验，无需浪费过多存储容量）。</p><p>图：vSphere 7 中的新分区架构，只有系统引导分区固定为 100 MB，其余分区是动态的，这意味着分区大小将根据启动媒体大小确定。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044308376" alt="partition schema in vSphere 7" title="partition schema in vSphere 7" loading="lazy"/></p><p>从 vSphere 7.0 Update 1c 开始，您可以使用 ESXi 安装程序引导选项 <code>systemMediaSize</code> 限制启动媒体上系统存储分区的大小。如果您的系统占用空间较小，不需要最大 128 GB 的系统存储大小，您可以将其限制为最小 32 GB。<code>systemMediaSize</code> 参数接受以下值：</p><ul><li>min（32 GB，用于单磁盘或嵌入式服务器）</li><li>small（64 GB，用于至少有 512 GB RAM 的服务器）</li><li>default（128 GB）</li><li>max（消耗所有可用空间，用于多 TB 的服务器）</li></ul><blockquote>即使设置值为 min，相比之前的版本所需存储容量还是要大的多。</blockquote><h3>有限支持第 12 代及以上 Intel 处理器</h3><p>ESXi 面向数据中心虚拟化，在测试和学习时也常常将其运行于桌面 PC 之上。</p><p>据悉 ESXi 8.0 并不支持第 12 代 Intel 处理器，直接引导会出现 PSOD。本次通过加载内核参数可以有限支持第 12 代 Intel CPU，即可以正常引导和安装，也可以正常运行 (sysin)，但是无法区分或识别两种核心，P 核的超线程是无法识别的，比如 i7-12650H 配备 6P + 4E 在桌面系统中显示为 16 核心，而在 ESXi 中仅识别为 10 核。现在有了更好的解决方案，绝大多数主流品牌机和主板都可以通过配置开启 P 核的超线程（非主流请慎选）。</p><p>已经广泛验证支持第 12 代及以上 Intel 处理器（目前 13、14 代同样支持），更多案例，期待您的反馈。</p><blockquote><p>第 12 代英特尔酷睿桌面级处理器有 N 个性能核（P 核，Performance-core）和 N 个能效核（E 核，Efficient-core）组成，性能核和能效核的混合架构，是 12 代酷睿处理器最大的革新。该架构或俗称 PE 大小核。</p><p>第 12 代及以上 Intel CPU 已经成功安装 ESXi 后需要进一步配置，可联系笔者了解详情。</p></blockquote><p>⚠️：并不推荐此类 CPU，无法有效利用全部计算资源。</p><p>💡：仅标准版和集成驱动版提供此项特性，品牌服务器于此无关。</p><h3>提供标准版和厂商定制版映像</h3><p>提供标准版和 Dell、HPE、Lenovo 等定制版映像 iso 文件，定制版集成了对应厂商的驱动，建议该厂商产品优先使用。</p><ul><li>Standard (标准版)</li><li>Dell (戴尔) 定制版</li><li>HPE (慧与) 定制版</li><li>Cisco (思科) 定制版</li><li>Fujitsu (富士通) 定制版</li><li>H3C (新华三) 定制版</li><li>Hitachi (日立) 定制版</li><li>Huawei (华为) 定制版</li><li>Inspur/IEIT SYSTEMS (浪潮) 定制版</li><li>Lenovo (联想) 定制版</li><li>NEC (日电) 定制版</li><li>xFusion (超聚变) 定制版</li></ul><p>💡：各厂商定制版将逐步提供，部分厂商关注度太低，可能不再提供定制服务。</p><h2>Dell (戴尔) 服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>代表机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>Dell PowerEdge 11G Server</td><td>R210, R310, R410, R415, R510, R515, R710, R715, R810, R815, R910 T310, T610, T710</td><td>6.0-6.0U3</td><td>有限支持 8.0，推荐 6.7U3</td><td>Intel Xeon 55xx 56xx 65xx 75xx series Intel Xeon E7-28xx E7-48xx E7-88xx series (sysin) AMD Opteron 43xx,42xx,41xx series AMD Opteron 63xx,62xx,61xx series</td><td>PERC H200, PERC H700, PERC 6/i <strong>皆不受支持</strong> 此系列机型不推荐，如有需求可来询。</td><td>部分可支持，此系列机型不推荐，如有需求可来询。</td></tr><tr><td>Dell PowerEdge 12G Server</td><td>R220, R320, R420, R420xr, R520, R620, R720, R720xd, R820, R920 T20, T320, T420, T620</td><td>6.0-6.0U3, 6.5-6.5U3</td><td>有限支持 9.0，推荐 8.0</td><td>• Intel® Xeon® processor E5-2400 product family • Intel® Xeon® processor E5-2600 product family • Intel® Xeon® processor E5-4600 product family (sysin) • Intel® Xeon® processor E7-4800 v2 and E7-8800 v2 product families (up to 4)</td><td>特殊定制支持的 Internal controllers (sysin):  PERC H710 PERC H710P <strong>不支持</strong> PERC H310 (6.5 U3) <strong>不支持</strong> PERC H810 (7.0 U3)</td><td>Broadcom® 5720 quad-port 1GbE Base-T Broadcom 57840S quad-port 10GbE SFP+ Rack NDC Intel I350 quad-port 1GbE Base-T (sysin) Intel X520 dual-port 10Gb DA/SFP+ Server Adapter Intel X540 dual-port 10GbE Base-T with 2 x 1GbE (FCoE capability enabled on the 10GbE ports) 部分选配 NIC 可能不支持，具体可查询</td></tr><tr><td>Dell PowerEdge 13G Server</td><td>R230, R330, R430, R530, R530xd, R630, R730, R730xd, R830, R930 T30, T130, T330, T430, T630</td><td>6.x All, 7.0-7.0U3</td><td>8.0-9.0</td><td>• Intel Xeon processor E3-1200 v5 • Intel® Xeon® processor E5-2600 v3 v4 product family • Intel® Xeon® processor E5-4600 v3 v4 product family • Intel Xeon E7-8800 v3 v4 and E7-4800 v3 v4 processors</td><td>Internal controllers (sysin): PERC H330, PERC H730, PERC H730P External HBAs (RAID): PERC H830 <strong>不支持</strong>  PERC S130 (SW RAID)</td><td>Broadcom® 5720 quad-port 1GbE Base-T Broadcom 57840S quad-port 10GbE SFP+ Rack NDC Intel I350 quad-port 1GbE Base-T (sysin) Intel X520 dual-port 10Gb DA/SFP+ Server Adapter Intel X540 dual-port 10GbE Base-T with 2 x 1GbE (FCoE capability enabled on the 10GbE ports) 部分选配 NIC 可能不支持，具体可查询</td></tr><tr><td>Dell PowerEdge 14G Server</td><td><a href="https://link.segmentfault.com/?enc=j7mO8VzRMjV6gvqr0gOFXA%3D%3D.hP5VuXkgZ3zFtOlfkpqRaAdSLlfNeYfqfvXVALLRpzYLhnSxkYbgCw9csvouZod2PX84rOLdGfG4Qp2%2Bl0MWwrS8UnfKEhZPzxmEROaZWXvl%2FigGOHIs9bYvNg1Jkbf7" rel="nofollow" target="_blank">R240</a>, <a href="https://link.segmentfault.com/?enc=B%2FVtGphaJQiG3lwuWTWvJw%3D%3D.uTGdJeSjOJqE%2BBX4FqVrFktLCbyV68HSIM5fS8a4nZDqXtiMQbpLgGb1kiNJ2iihTPoEKBckoHFeog1fFtbfWwiVQ4SpFw1cIrt0J%2Fv1eYuiuMgyHDplOqLKOfD4TUJ8" rel="nofollow" target="_blank">R340</a>, <a href="https://link.segmentfault.com/?enc=RfV%2BNCOfxsHLs%2BdNXFaB6A%3D%3D.%2FMlZCikhBElpESYFhtFhIyQQPukF%2BiZQ93uBtYByB2MFhlnWHU2aSW2QBwXfJJJMyYrAcfTBSwVeLdd2go3HPLBsW2NrzB0bxp55FNoivJlLhhDnGbzAFMFOAj3t1zO7" rel="nofollow" target="_blank">R440</a>, <a href="https://link.segmentfault.com/?enc=SaJnPXkC6NANq0Idd3wjHA%3D%3D.tgjfddGcYAT%2FFuT7Jir5xT52Z3LusTd429kjIYQD4JV9NPHcR6wNDs%2BA2FLUHyCNAXXm8NELtIV2ynVP1N4Fn09QGoLeGun9%2F%2Bkv9Cg3xX2WSbwytikBIz2Vv8K9ybFa" rel="nofollow" target="_blank">R540</a>, <a href="https://link.segmentfault.com/?enc=gAoE56S%2Fgc6pU%2BlvB61e%2BA%3D%3D.4uSy68cU7J%2FvIr%2FEvJDYRjZ9nldYGx677XOfK41ccfwlLmB%2FmpguGwCEkACGWOAOM6%2Fvhjzc5nwjleNNuqR5D%2BeaZM4m5P7J1Ud%2F884TPgnxHxeSIZ09OmnmkXF3ngpc" rel="nofollow" target="_blank">R640</a>, <a href="https://link.segmentfault.com/?enc=L%2FeiBmpoNw4q8A2QPSuadg%3D%3D.di6ufWtDt27kFJLHQ6idplzxZEtXjesiJZjsqRekw%2BdbqQPDJ4ggCwAfxOS1ElNVoLOWPhGp%2F1kIZWMRRlFgH3sCp0O0wUhhSCESv7OeNWni0I8RQ%2FhBg71iimWw%2FwVM" rel="nofollow" target="_blank">R6415</a>, <a href="https://link.segmentfault.com/?enc=tMFt1kyYAYDOLsYpI%2BP45A%3D%3D.sEry00DEy0QJO6M9byZAgKaP1hum9UlJBh9I6RT08zBvNIWQ7IZLtg8HKRyDGn2QU133GkRY%2FFAO0lH2tnYtUC%2FzZUeuZD7DrZQpHT9q78kULTisxFiz%2FXlhVxcFJmwR" rel="nofollow" target="_blank">R740</a>, <a href="https://link.segmentfault.com/?enc=%2BReb1Rm4vWIm%2F6MYBt5y6g%3D%3D.qdq5VAS1CXtTDKv90LIRZy5LxggRriSq7Zgtb2LYGH14s%2F%2BpQdZp%2FqlkvUvelkGjtQYGTzpDXw8ozcJSVBdt8IDdQYHApVOwvOuB3DOcjJFxi1u0Km6%2BI96O3x%2BQEU3%2B" rel="nofollow" target="_blank">R740xd</a>, <a href="https://link.segmentfault.com/?enc=ZXs3IXhKt6sXmeiqC3wx2w%3D%3D.Drcf8p6tPhfwcPGLE%2Bq85p9l5Q9dtX5EqyirFcNOp9JsURCAr8ajdkxPKCT0yVMtENVoKtmZkD6nVH5AkFxPxDUWyM6P958dMhLQzXZWHn9ZR6nlbsCLM0XOBq9nU6he" rel="nofollow" target="_blank">R740xd2</a>, <a href="https://link.segmentfault.com/?enc=gFohIynXjStjlHEx%2Bpt66w%3D%3D.hYOtnsoruUv7n885DCBUPJETeqBzvUekr%2Fhks7SXLI3GvYrUmK8a3zPnt%2ByPL%2B60zWu8b4OAugf4WwN%2Fm5pR2em38Nw7APUhkvC8PJhFnVoNl5liWx6KjWjgZDU%2B3Vj6" rel="nofollow" target="_blank">R7415</a>, <a href="https://link.segmentfault.com/?enc=sofADNB7t7V7GGWOimotMQ%3D%3D.cOOPkM1uk2y9kms8tqYJ4USy5a2%2BUDanxhuqdQ5OlM%2FWnfvw6M3g9EiXgUaJaPyhTCwCxwYCxjw3l7sDf8iIU5L%2BpiZkGJffH296fW9h67E4a3y3YnfhSMfi3UR%2BQpiZ" rel="nofollow" target="_blank">R7425</a>, <a href="https://link.segmentfault.com/?enc=ZyPnI1pd8ruJYExq5o2bfQ%3D%3D.nOZe531kyJ4E6nBqUslLyFF8CaOecinbkpgta0ASbHXN7n5FRWof4RsW36oN7aGYIyGHnnhcido0v5VysUWscr1R75zN8ha%2FXODeRSB8CNT9DLXGhlMFYUEnfFbeDM10" rel="nofollow" target="_blank">R840</a>, <a href="https://link.segmentfault.com/?enc=rLYks%2FLCCz2O3tFOFG8Ywg%3D%3D.atVluD0F6QzoZQlm6UVUYjdNWoqp5mHAiVDVsH6gDqKZiGhGfnkjc0yC4UxZ30NeMRb2JZh2wNKldHQuUOm7B7cvI25%2FO7KzO2oyyFz1cM0egexUNtgVBf4LHMPsUzb6" rel="nofollow" target="_blank">R940</a>, <a href="https://link.segmentfault.com/?enc=FQFDLqogGHVbtQq13gp0HQ%3D%3D.IWKJ2N%2Bka76J3nAq%2FUjRP6TxWVBG0pw%2BsS1zACZtz9S1GBUS7Zwx8EEEMLglNj3TkAVBFm%2BouvxybkPMa9N7MIH5Qj6HpcSh2%2B3k9n2ehQcxKeskdDvyIbPzWBP7htJW" rel="nofollow" target="_blank">R940xa</a> <a href="https://link.segmentfault.com/?enc=Qtoqcfz81Bnud53%2FEwetJg%3D%3D.90gD%2BcZ1JqX3JBBTbwye%2FWAot5QSApl6ECFuMjxl0zEn7oKgXxmFCBuOgLEnuYLp2wUrVlEOdNtUVcBxVdZ4A6gqiOUQgKGPx1MyqGO6XFAfzhp9NxsmnfitYPeC6f9a" rel="nofollow" target="_blank">T40</a>, <a href="https://link.segmentfault.com/?enc=k4hgW1UwQZVDmRIm2OFQHw%3D%3D.cotAuy1PgahyRV7YoAC2PrRgDPtTPbTugRNcZSfuHpaIXlPzavFx2Ykow3D7NOrmrKnJZaTcE%2BSVJpNT5lK%2BzzfZ%2FYL1qokDAlNUtAC%2BsUfTMzQBkgAsIKxEIvRqaF5%2B" rel="nofollow" target="_blank">T140</a>, <a href="https://link.segmentfault.com/?enc=XMLIEpY%2BlhFSAsS6lgzQwg%3D%3D.nYU95MWnpItQmhXmxJVqBr4f8FCVuMQc%2BVjfkhy9MnXkBT3y20Ag2gOor8n9p62WV2pI%2FYsZtdQWum36MDYaC8%2FjnCx%2B3kV7PvfklMf4qKbYhehyXmgyQ0pVeXItJThT" rel="nofollow" target="_blank">T340</a>, <a href="https://link.segmentfault.com/?enc=xa9JNKcKjrcq1UvBrr6rLw%3D%3D.pYYVOvVS5VfPuhRfKNttSaS217gor8IImjmHygMU6jp08I6%2Ft31tpvLRJ8bpLxORitM0SGRQrT21bEoTkiJN9rN%2B5sAsUbLzCmvqY5nRlCwMKmpT7n86gBzEbmRZI8%2BC" rel="nofollow" target="_blank">T440</a>, <a href="https://link.segmentfault.com/?enc=k2qkd7JgKJM7aLfIs1zdAw%3D%3D.NS%2Ftc%2BM4TXdwGiadHtsK0spwc353nHAUTebKiledfs4ihKYRqYlCJrSrutEeW0Pmo6oIycNqc47zTJ%2FDhWSpLC%2FjgtL8ucyG97m9Ai5SfkKNvkipYdPsDMCLQ5iV5nkz" rel="nofollow" target="_blank">T640</a></td><td>6.7-6.7U3, 7.0-7.0U3, 8.0-8.0U3</td><td>9.0</td><td>• Intel Xeon Scalable processors • 2nd Generation Intel® Xeon® Scalable processors (sysin) • AMD EPYC processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 15G Server</td><td><a href="https://link.segmentfault.com/?enc=s%2B16y6XNWREi4DR2wd24aQ%3D%3D.XsylCh1sFg9IAtZYsnWu7lIRIaXv%2FOUxlK5Lw1YX5MfbO4ppp6oZkKnTiXIpO4iBbx6WrJqSM1KaPuuHGEEms5TpKt6h880LGpSXbdKhQWvlfrxdpipd%2FzsmI9MN6YD3" rel="nofollow" target="_blank">R250</a>, <a href="https://link.segmentfault.com/?enc=cL%2FHiHbal5blsRFIaGQZPw%3D%3D.binNMYd8FGqCvHKqwi4sw3qBcQVJogjPat3j7h9%2F03mbc0icZ9SdkVzrrFwu17CJzo%2FkrxBtRcthSaG7B47tXatOpa7bYa5y6Q26vl1GQDIH%2BZYEPdpKQ%2BauISpF7sAc" rel="nofollow" target="_blank">R350</a>, <a href="https://link.segmentfault.com/?enc=FtSLt1nH%2Fn37EjC%2BWE5fYg%3D%3D.%2BQ60KtHnn%2FD1mMb9UAnlj%2BFDOzmKo0yVdyffwci3L6dzThj%2BdkK8l0yA8xXMcNz1gYjPlHOD1TOBSSyiJAfSuTiLCof9%2FBZPsAXDNR6VjE85Oek7uGanTYPADqf8Zh6Q" rel="nofollow" target="_blank">R450</a>, <a href="https://link.segmentfault.com/?enc=AgdnOg%2FBK7AvuIMKsWd7JA%3D%3D.zRu3R8r%2Beui6ZzWMTbmpLd4v50VCHYNjBN1bKGGfdRwR21R6%2F0awjfUfDiCV1RbWWcvtKSAfaY9ddby2nyY9DSde3P7U90w%2FmNnN819mrw8pvxdtxxU%2Belc%2BK%2FEow7JK" rel="nofollow" target="_blank">R550</a>, <a href="https://link.segmentfault.com/?enc=1MkYSQrtAc0cvoc1j%2Fb6jw%3D%3D.vMTrITb104PGjLFqwq9OHdUgZP3kRNNqAVNSfv7MBEIvbb%2FOsKpsjaU89a3eBBdSSHPc8eogg3OxNtbtSVHStjNfpItetJ5fUsBQ%2Bv9bhvsRdYr8w1t65bfXz5AD2alJ" rel="nofollow" target="_blank">R650</a>, <a href="https://link.segmentfault.com/?enc=5UFV%2F5ZwLnkZ18WfNZHI4w%3D%3D.Lb3TCfF%2FQol5XuD6CljBZgaxzGBjw5wUfVydLeNjJSvYNTkU5xsdvz1N9FPCOFpn4mK0ku7WREYALACSzDIMyVuS9a4lbHgt5BMsZIdiKLOUlIxKjndbIZHExsyf8Uuj" rel="nofollow" target="_blank">R650xs</a>, <a href="https://link.segmentfault.com/?enc=eKUjCR7l1mb5%2B3uFDoZyWw%3D%3D.0CIVfTo88zMDVlQUFYUiVeS9mD8MmQGqOCZdgTiDmOXYC2bSDfEtgAkDnWhf6we4bX2tugoVNdmNDpgoCcETGEkU5c%2ByZt4epm%2B0uVr008RqUno507LYOfkOE5vx9hFN" rel="nofollow" target="_blank">R6515</a>, <a href="https://link.segmentfault.com/?enc=tvn4r0dgwOZK02Ehx8j5Kw%3D%3D.K%2BWrSLAcENctOG2DfgS06R0Z16Ia0Cl9MXaGluhhqc51t%2BkUWZO21zworuPXWKmMM1hPBrtuSePgBPzeBCmLiy7oRnBx2BxxftNinb7hct5yQp%2FWzq45bAjtgdHlGLRQ" rel="nofollow" target="_blank">R6525</a>, <a href="https://link.segmentfault.com/?enc=Zb5Tqbi67q5mPbGmJhBLMQ%3D%3D.f%2FRACfAnx6uMbUe388mx4k4%2BofsMviLYKNRgxPtUoYHgyaAJtXvAsJEVUdvtmcEgjdGVxPn8%2Fqwrmb4if5AdeFdKXLqZ%2F%2BZHEf8Z5DJuAqNuEqSQ%2BDhSoQN64fO8t8Nf" rel="nofollow" target="_blank">R750</a>, <a href="https://link.segmentfault.com/?enc=SXXwaozhJNc%2BQ2DAJAIlUQ%3D%3D.Aq1qwnkDkgBH9L6sWUERtifehN24NPbUOV6zZP5WFXSCBuWW6DZQX9jVDI5uqE2sA3ZfYwlBZv1iGxyHi9QI3H9aC47j6WXsH30GzUvTeJOAquv9B3rFKKCKXUzLG7VR" rel="nofollow" target="_blank">R750xa</a>, <a href="https://link.segmentfault.com/?enc=qNmEoXSOW0DqIu%2FY%2FaaXzQ%3D%3D.syx1d6D5wWYgTN%2F12LGi8TQkXwTUn4%2F%2Bguykj10uY25UTq3YqCFUyDW8V11ukeVx9h5SoEMco2IL%2FPLkD6Y%2BP3GPpbg2r3IVu%2B5pWOTjuqpkEj%2Biavi3mPcJvjcinP5F" rel="nofollow" target="_blank">R750xs</a>, <a href="https://link.segmentfault.com/?enc=6hEKRW5HPc5qVpVPF8OnSg%3D%3D.oiJLZlUhNMjlLQp2vuIDSI%2FTXWqC%2BdkpIjD9sOQhE6EZQrGDtOdz7lK3BWT5H15d%2FDNFkyeyorvo5w5k8cYGxlSsMA4zF9ukS%2Fds%2FffB6aYI%2BDwiE28JzYd%2FitqqTCoo" rel="nofollow" target="_blank">R7515</a>, <a href="https://link.segmentfault.com/?enc=87BHlkg2FCRINT119f6KUg%3D%3D.8hXef%2B0WvxcUdtsdSWYLwg8rD%2ByDd%2FwNfiixPojtvGD%2BkKCOXDEOLsfR1K7nDGA09j36mCQih5DGm8i%2B89WtaI5wafvLlmzwkg9VMBCP0RG%2FihDdxomBaYTNILtr79qm" rel="nofollow" target="_blank">R7525</a> <a href="https://link.segmentfault.com/?enc=vq9THD4mFoVZrtPbZdwsKg%3D%3D.2rJ13BZa0zzy%2BBczwgGD3hiPwxBHLlRCTqjyCFQacOi%2F0ZlnT1LYD9GRMCBlFCYYGsE%2FGq8MdwEU13V5fn9%2FrlNj1hbxYkhr8J7Dv2X%2FmPsYcQ3ZR7RO0AljLIOFktof" rel="nofollow" target="_blank">T150</a>, <a href="https://link.segmentfault.com/?enc=XSzXpfK2ewAjYWU03oS6ng%3D%3D.xRONIW14rh%2Bi5aPr3N%2B8t9oyu1Yoo%2Bj3LRzAxLkDRwVfYygioieN4wZX6rSkKJhVRuk575xbdmapJKK8ofuGmxKCAdEbPSE64Aa0exEhBbwKexQFk2iQTvWc2werVGMr" rel="nofollow" target="_blank">T350</a>, <a href="https://link.segmentfault.com/?enc=W%2BfzCoHQLUAZ2tN%2F2TissQ%3D%3D.fk%2FTuVPXfB56ifRzqh5DQTsoZBjxb9grS8Kd4HuSoc1y6wmZNaWMvPe0SC%2F5MI4N1Ltq%2FBUTqeE00R%2FxLo6Is3ASwrMbsE0m1x5m4mNySs0tu4%2FMoUTsgZiXaf0RCyBd" rel="nofollow" target="_blank">T550</a></td><td>6.7U3, 7.0U2-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 3rd Generation Intel Xeon Scalable processors (sysin) • 2nd or 3rd Generation AMD EPYC Processor</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 16G Server</td><td><a href="https://link.segmentfault.com/?enc=jfsZbfVzwcZYXtTl%2FwCERg%3D%3D.VjTmRSxOIxVTMbfeLreSEEeRP6wH06wL1etVZFiQgXpvfgw9QoVQISZMjwdmkrtSXX2BhKGiDSam2YKxgCnqHzAvLAVXixqPdMMRW11McjA%3D" rel="nofollow" target="_blank">R360</a>, <a href="https://link.segmentfault.com/?enc=%2FJBF%2BEyrRl2o3PB%2BJDB5ww%3D%3D.VoMwaxtYj%2BdRBG6KHgXILz3z2no1yir5Mkb98euYQYQsJK6ohlScrgi7YTB4TcRPFydB2oOORVqX4BlF6pqbJkprx8inx2569dtST05dA7I%3D" rel="nofollow" target="_blank">R660</a>, <a href="https://link.segmentfault.com/?enc=kAxbboI5oPQhIRyJ%2FaHMWw%3D%3D.6ASkVvrrFSOScHbpuECQdNEwPewzhL5PYNZGDsBKlyQ7rmt0GQsUT6jN8z%2FmdhgROm3q3ICMLrtgHprgka%2B07PgdF5LTkrO6lMhshhPVIeM%3D" rel="nofollow" target="_blank">R660xs</a>, <a href="https://link.segmentfault.com/?enc=DTDEfu4lfg%2Bmi5kkRPGuPQ%3D%3D.pFrf67CLQACvvOyH2nOpMyAvtI1epZATi9Ktoi4BaySCVQelX4A8OuqwFH%2FMWSBXrP2fGUEw9hxAb0zL41SIDFwBtCVwiMD1hIJQirDfm5I%3D" rel="nofollow" target="_blank">R6615</a>, <a href="https://link.segmentfault.com/?enc=IAHYaioaZ4oJeoXjJbrmqA%3D%3D.s3u%2B%2FTBxPzcX5mOwIEmcJg%2FyU5n0KI9XvbcVX%2FdIy5jYgD7jAzVxnsazMo56Oe8YYJLkOjcDlZWyFhVNbpJbX9rvQlU0eG2YllAdb4pIDOM%3D" rel="nofollow" target="_blank">R6625</a>, <a href="https://link.segmentfault.com/?enc=2I23tlb%2FKe7AMiUmuravrw%3D%3D.%2F0mhhYoy%2FLgwurCu9zNf73tyjK6uG5PdioQMdk8s8wsS8peoLrsFV5%2BEpdl69DjPNSRyLMTt9A6uRXiv4TlRRoU%2FKFJQL7ucx6opzYXxfBE%3D" rel="nofollow" target="_blank">R760</a>, <a href="https://link.segmentfault.com/?enc=Ce4V%2FB602MTPfPdzJUZKHg%3D%3D.nl3hKbwuzC6BrRONMWQuuBjEqcdpZuiKbA0iZH%2B1%2BL1qvdkuyIh1iGOnxtMKysdloJtpAq9mQjQE5Z%2FausJAJYU2GHrs0pJ2exOcVAupKdM%3D" rel="nofollow" target="_blank">R760xa</a>, <a href="https://link.segmentfault.com/?enc=8uHT1Be41Gn33S9eJaDLsg%3D%3D.w4WWbCte8qJdLqPkcqJcAmxD%2BBhcwCJ2P42nUR6NtsChT8QU9O3jBAYFRD0wxw0DmIwMxVUyxg5ImQbUOOGVQlLmb5vkFx4GZbOidTCOwdw%3D" rel="nofollow" target="_blank">R760xd2</a>, <a href="https://link.segmentfault.com/?enc=V1vp2dzqsb22sVLjnfy0Ew%3D%3D.4BV37m428dAh7eYOzem7bEfFV6qbDqNLA8y4yyftIYxCpJ4mmDq8UsOEMwFDqjFCmn5wNGmT%2Bn%2Bh3I30DtOW%2FTuwtZbHjw8Fdxp4n3LGkSk%3D" rel="nofollow" target="_blank">R760xs</a>, <a href="https://link.segmentfault.com/?enc=tavgUydMtJTWhtw0m003yA%3D%3D.d2aSpV6RGO%2BaOM59OGbhDlpqLgweyg3PD4t50vLOmVBSuohxndDfSLkaA82Vsb6gYANsfH3qwNF53QWXMJu0AvkzSOX24XvlgW67OU6ktxw%3D" rel="nofollow" target="_blank">R7615</a>, <a href="https://link.segmentfault.com/?enc=whqPrNbSQzSMVmae4HoxWQ%3D%3D.5dHTuMnUvyyy76LINvD1RVJbPVc51EldWt%2B6nnWEY579bvDJWzDmDgkHt02BUgZkF5xwjZoTkDlXdY3ZBwUi8TkkfsKWLYPU9hhr7ksu5Qo%3D" rel="nofollow" target="_blank">R7625</a>, <a href="https://link.segmentfault.com/?enc=AemD9YowaAmWorUGcBaCMg%3D%3D.AsEgjW4%2BORO4XPI%2B1T6SVRzOGhBIPrEJTmatZhiSPSWJC4%2FXY5GF6SFifdAEl0kl3wUpeS%2Fkuf0zA4%2Bua2zDImPl0WwsgdOifQUVAZWI0nk%3D" rel="nofollow" target="_blank">R860</a>, <a href="https://link.segmentfault.com/?enc=Pwqu4xYAp2LfSm5M76MFhg%3D%3D.bpyMJ%2FNLsQsIEbQmxn483UyZoBEmCAFHQ1A3AEuOXaVOwzWPx%2BBzNeLolb2aHm5fgRqWpSR%2BL%2FtaNR7RBLtqnDVYKaEUqgsrDg6bFg6MsPk%3D" rel="nofollow" target="_blank">R960</a> <a href="https://link.segmentfault.com/?enc=4BdiTHFllJWDpSJBZyS0RQ%3D%3D.Jmdu%2FDTvA%2BlEayNQlVkPjTIc0qvoMFdrBzroylNMXSou56qFG0gfayNYpGe4e%2FNRJ8zq%2BSJMoOb76kcQAbvGvW%2BZal45IP%2F0JpApNpuFjC8%3D" rel="nofollow" target="_blank">T360</a>, <a href="https://link.segmentfault.com/?enc=PSJRb9GPojIoZ1c3WJnxqA%3D%3D.Tx9lS3L6gV0Bi8pikIEWCKotS19UAQebrAvKtRykhf5bOloUIC4zztRkV8k2OMlJSODpKgIIYvsbNmuBVz9iYPsWiH77Gub9PGB4N6HMYag%3D" rel="nofollow" target="_blank">T560</a></td><td>7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 4th Generation Intel Xeon Scalable processors (sysin) • AMD EPYC 4th Generation 9004 Series</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>Dell PowerEdge 17G Server</td><td><a href="https://link.segmentfault.com/?enc=lYM%2BfRUeUHpNVMmMYsApsA%3D%3D.p3SSDZOsLCGKh%2BZL267IN9a86ptoTi1xpCcz0%2FDnJsP9EqiBD0SaRWRnTeOME8oJTNGXt%2FCB1HcKifOlals15VE%2BLYamLJoUn27Xyjs915o%3D" rel="nofollow" target="_blank">R370</a>, <a href="https://link.segmentfault.com/?enc=h67uTiAvaqEQcWF9rJZ6xg%3D%3D.6SIk25NjVR79izC7PrEWHssgASOSaeJIdzKS8CfLundzRsuTM2OCkfhDs4aeb9%2BEQH1NNSE3ybPC0U4MdoSiVNpI9DSrWsjuZHjS0HcxkQw%3D" rel="nofollow" target="_blank">R470</a>, <a href="https://link.segmentfault.com/?enc=RqY97gBiFpHVNKQ%2FAEWDRg%3D%3D.YgFPyXBukEcWWtVpmJoHK2GYehSoiiiGM1To5cZnCgPSZ8WYcOYYqI0owLoGZsWeyOn4FGgSMXQ2S0A3RMFr%2FOeRp4gIikjlNcw6CpJpSX8%3D" rel="nofollow" target="_blank">R570</a>, <a href="https://link.segmentfault.com/?enc=DUwPZkzxGPPxS8fj4v4fDA%3D%3D.mdMVg0CHDxCNscXDTAiAV9Xn6uENKWxVFke8CYnYZSjNCwxWqpRoaHpzwat2qPBIVvMuI%2FxS%2F%2FXvxiVUeRei9qjZwCHd5rjakLI%2BREriH5Q%3D" rel="nofollow" target="_blank">R670</a>, <a href="https://link.segmentfault.com/?enc=Ht0rinotRmb9cAddVsWW4Q%3D%3D.YCyHWeWynuZRUO5dx7GlNwzMajTpW2Kmfj9VtiAKT2s5tPbcjGbVxDEAsiCfM1j35BdvR9V8Qz0XZMoX4eRKB%2BXwd7mNEry9TQe7SxE2hu4%3D" rel="nofollow" target="_blank">R6715</a>, <a href="https://link.segmentfault.com/?enc=%2FC7H8ZVpbZvSFjs%2BfTwZpA%3D%3D.1jncarXFtM1Y3kyjGt%2BZxNIdYp2NtvUaDkyeJUXBB7z2G5HdOZ5kgAW0NOYagsACnHg0dXjdAaTmaU4YX3Mt63yjNwk4JP3LNhABm49tLE0%3D" rel="nofollow" target="_blank">R6725</a>, <a href="https://link.segmentfault.com/?enc=%2Br9N6iB2nWAZzZ%2BRCjDo0w%3D%3D.K3NHJ%2BHM86qn9ij8e8BvbR7H2DbtsBTW1861txzrp0%2BcbYEyjFyqkc6SowxUtn9wDZD2ZznQH1e3DpTA3r7N3udULVUA3tzRi3Wk69XWuFg%3D" rel="nofollow" target="_blank">R770</a>, <a href="https://link.segmentfault.com/?enc=pu6bxKmwGNsIfzKPsabIRQ%3D%3D.fNJFk3ozWlvPCmBx0JniW%2FFUKdX5TZwyrxXttRNoCitaAvDiRbY23iLeSGyyz60seIYLGx9GZWni9G%2B%2BeuCuvo4HRG5XdR%2Bj0FTS6CoLrf0%3D" rel="nofollow" target="_blank">R7715</a>, <a href="https://link.segmentfault.com/?enc=OkTq0YoQ3dZJHhsRUQ%2BFJQ%3D%3D.h32FJzDP5%2B%2Bbn3tGyIKx2vdsk6NRF%2FHmK4GMocRxpOi9MZ1gvsd0IuqiktoZ38ZSHHg0Ubm20T2czCOjiK0YeCMUgyZnLkn4RO29InotwEM%3D" rel="nofollow" target="_blank">R7725</a>, <a href="https://link.segmentfault.com/?enc=8LjlyXOYzwlTaSuZrx%2BHrQ%3D%3D.nMMyCR4LZ94hYyja4dRVeDcqwZzQcDgS2ufXcqKhTKiS0K%2FM%2B99B3wG%2Bz3Xl4OEZSv%2BzZmY3gqI0xQP1lY8ZpoAlOC299aufHTIRuRRa0BA%3D" rel="nofollow" target="_blank">R870</a>, <a href="https://link.segmentfault.com/?enc=7Z6HkNXF%2FnR07WFl8ocrXA%3D%3D.xWk4inUAljVvCsBALxt8YWEBqR3hxXgP2iPq40rCEqmXKwiqAQXKBLzNi8mZ0BEkLfizj6weRsnzBRgzMC4IGMD6Rp3DfIx0dnO55WqLjYE%3D" rel="nofollow" target="_blank">R970</a> <a href="https://link.segmentfault.com/?enc=vD1hb1IedODKgI0K9QWDDQ%3D%3D.22DS9GjtaVVoSLXiWtlMtYwXm0eQS5%2FYXwPLTZc18y8rPpUct%2FztuYBvZsz8OI8JnFsFCzyZeJ7L8nS%2Be6ZmdtkX7uWlLnZZ9vfbtmpHvzM%3D" rel="nofollow" target="_blank">T370</a>, <a href="https://link.segmentfault.com/?enc=cDve3FN%2FsE8t0g0CLnJmDA%3D%3D.WhJVfw4vhICCJ63YwgQNjRUVmhmyN08us2XmU5EVdN7sHz1sflxmx3I3pftt%2FE5rSyPNSVDjdFwKxa3WnH%2FxHBiLiSwmw9KZvciRt0fF%2BBw%3D" rel="nofollow" target="_blank">T570</a> (部分机型尚未发布，按惯例列出)</td><td>8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon 6 processors (sysin) • AMD EPYC 5th Generation 9005 Series</td><td>同官方支持</td><td>同官方支持</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ 以上仅列出 Rack 和 Tower 机型，其他 C、F 和 M 机型理论上兼容性同。</li></ul><h2>HPE (慧与) 服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>HP ProLiant Servers Gen8</td><td>ML10 v2, ML310e Gen8 v2, ML350e Gen8, ML350p Gen8 DL320e Gen8 v2, DL360e Gen8, DL380e Gen8, DL360p Gen8, DL380p Gen8, DL560 Gen8, DL580 Gen8</td><td>6.0-6.0U3, 6.5-6.5U3</td><td>8.0 系列</td><td>• Intel® Xeon® E5-2400 • Intel® Xeon® E5-2400 v2 • Intel® Xeon® E5-2600 v2 (sysin) • Intel® Xeon® E7-4800 v2 • Intel® Xeon® E7-8800 v2</td><td>⚠️ 以下型号默认不受支持： Smart Array P420i, HP Smart Array P222, Smart Array P420, Smart Array P421, Smart Array P822 以上默认最高支持 7.0 (已有特殊定制版可以支持 8.0)，以下默认同时支持 8.0 系列 支持的型号： HPE Smart Array P430i, P430, P431, P830i, P830  【B120i/B320i SATA RAID 不受支持】</td><td>HP 1Gb Ethernet 4-port 331i Adapter HP Ethernet 1Gb 4-port 366i Adapter HP Ethernet 1Gb 4-port 331FLR Adapter (sysin) HPE FlexFabric 10Gb 2P 534FLR-SFP+ Adapter HPE Ethernet 10Gb 2-port 561T Adapter HPE Ethernet 10Gb 2-port 557SFP+ Adapter 预置网卡可支持，选配网卡个别不支持，具体可查询</td></tr><tr><td>HPE ProLiant rack and tower servers Gen9</td><td>ML10 Gen9, ML30 Gen9, ML110 Gen9, ML150 Gen9, ML350 Gen9 DL20 Gen9, DL60 Gen9, DL80 Gen9, DL120 Gen9, DL160 Gen9, DL180 Gen9, DL360 Gen9, DL380 Gen9, DL560 Gen9, DL580 Gen9</td><td>6.5-6.5U3, 6.7-6.7U3, 7.0-7.0U3</td><td>8.0-9.0</td><td>• Intel Xeon E3-1200 v3 • Intel Xeon E3-1200 v5 Intel Xeon E5-2600 v3/v4 (sysin) • Intel Xeon E5-4600 v3/v4 • Intel Xeon E7-4800 v3/v4 • Intel Xeon E7-8800 v3/v4 • AMD Opteron 6300 Series</td><td>HPE Smart Array H240, H240ar, P440, P440ar, P441, P840, P841, P830i, P830 【B140i 不受支持】</td><td>HPE Embedded Dual Port 361i Adapter HPE Embedded 1Gb Ethernet 4-port 331i Adapter (sysin) HPE FlexFabric 10Gb 2P 533FLR-T Adapter HPE FlexFabric 10Gb 2P 534FLR-SFP+ Adapter 预置网卡可支持，选配网卡个别不支持，具体可查询</td></tr><tr><td>HPE ProLiant rack and tower servers Gen10</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML family • HPE ProLiant DL family</td><td>6.5-6.5U3, 6.7-6.7U3, 7.0-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon Scalable processor (sysin) • AMD EPYC 7000 Series Processor family</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen10 Plus</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML family • HPE ProLiant DL family</td><td>6.7U3, 7.0-7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 2nd or 3rd Generation Intel Xeon Scalable processors (sysin) • 2nd/3rd Generation AMD EPYC 7002/7003 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen11</td><td>• HPE ProLiant MicroServer • HPE ProLiant 10 series • HPE ProLiant 100 series • HPE ProLiant 300 series • HPE ProLiant 500 series</td><td>7.0U3, 8.0-8.0U3, 9.0</td><td>同官方支持</td><td>• 4th Generation Intel Xeon Scalable processors (sysin) • 4th Generation AMD EPYC 9004 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr><tr><td>HPE ProLiant rack and tower servers Gen12</td><td>• HPE ProLiant MicroServer • HPE ProLiant ML server • HPE ProLiant DL server • HPE ProLiant RL server</td><td>8.0U3, 9.0</td><td>同官方支持</td><td>• Intel Xeon 6 processors (sysin) • 5th AMD EPYC 9005 Series processors</td><td>同官方支持</td><td>同官方支持</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ HPE ProLiant 映像是独立的，不包含 Synergy 和 Superdome。</li></ul><h2>华为与超聚变服务器兼容性</h2><p>因新产品刚刚发布，将及时更新相关内容。</p><p>以下如未列出，欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><table><thead><tr><th>产品线</th><th>代表机型</th><th>官方兼容版本</th><th>定制版兼容性</th><th>CPU</th><th>RAID controllers</th><th>NIC</th></tr></thead><tbody><tr><td>Huawei FusionServer V2</td><td>RH1288 V2, RH1288A V2, RH2285 V2, RH2285H V2, RH2288 V2, RH2288A V2, RH2288H V2, RH2485 V2</td><td>6.0-6.5</td><td>8.0</td><td>E5-2400 E5-2400 V2 E5-2600 E5-2600 V2 E5-4600 E5-4600 V2</td><td>LSI 产品支持（部分型号需定制）</td><td>Intel E1G42ET (82576) 和 Silicom 网卡不受支持。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V3</td><td>5288 V3, RH1288 V3, RH2288 V3, RH2288H V3, RH5885 V3(E7 V2+DDR3),  RH5885 V3(E7 V3+DDR3), RH5885 V3(E7 V3+DDR4), RH5885 V3(E7 V4+DDR4),  RH5885H V3(E7 V2+DDR3), RH5885H V3(E7 V3+DDR4), RH5885H V3(E7 V4+DDR4),  RH8100 V3(E7 V2+DDR3), RH8100 V3(E7 V3+DDR4), RH8100 V3(E7 V4+DDR4)</td><td>6.0-6.5-6.7</td><td>9.0</td><td>E5-2600 V3 E5-2600 V4 E7-4800 V2 E7-8800 V2 E7-4800 V3 E7-8800 V3 E7-4800 V4 E7-8800 V4</td><td>PM8060 和 PM8068 不受支持。LSI 产品支持（部分型号需定制）。</td><td>Silicom 网卡不受支持。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V5</td><td>1288H V5, 1288X V5, 2288 V5, 2288C V5, 2288H V5, 2288X V5, 2288X V5  VC, 2298 V5, 2488 V5, 2488H V5, 5288 V5, 5288X V5, 5288X V5 VC, 5885H  V5, 8100 V5</td><td>6.5-6.7-7.0</td><td>9.0</td><td>Intel Xeon Scalable processors (Skylake) 2nd Generation Intel® Xeon® Scalable processors (Cascade lake)</td><td>Broadcom、Avago 、LSI。</td><td>Silicom 网卡不受支持。海思 Hi1822 芯片不兼容。 部分适用 C3 定制版。</td></tr><tr><td>Huawei/xFusion FusionServer V6</td><td>1288H V6, 2288H V6-16DIMM, 2288H V6-32DIMM, 2488H V6, 5288 V6</td><td>7.0-8.0</td><td>9.0</td><td>3rd Generation Intel Xeon Scalable processors (Cooper Lake / Ice Lake)</td><td>Broadcom、Avago 、LSI。</td><td>海思 Hi1822 芯片不兼容。</td></tr><tr><td>xFusion FusionServer V7</td><td>1158H V7, 1258H V7, 1288H V7, 2258 V7, 2288 V7, 2258H V7(4GPU), 2288H V7, 5288 V7, 2488H V7, 5288 V7, 5298 V7, 5885H V7</td><td>7.0U3-8.0-9.0</td><td>同官方</td><td>4th or 5th Generation Intel Xeon Scalable processors (Sapphire Rapids-SP/Emerald Rapids) AMD EPYC 4th Generation 9004 Series</td><td>Broadcom、Avago 、LSI。</td><td>XC 网卡为超聚变产品。</td></tr><tr><td>xFusion FusionServer V8</td><td>1288H V8, 2288H V8, 2158 V8, 2258 V8</td><td>8.0U3-9.0</td><td>同官方</td><td>• Intel Xeon 6 processors (sysin) • 5th AMD EPYC 9005 Series processors</td><td>同官方</td><td>同官方</td></tr></tbody></table><p>💡 提示：</p><ul><li>① 以上机型、CPU 等参数未全部列出，详尽信息可在官网查询。</li><li>② ESXi 不支持 SW RAID（仅 Intel VROC 等少数产品支持）。</li><li>③ 以上仅列出 Rack 机型，其他机型理论上兼容性同。</li><li>④ 已知配备华为海思芯片的网卡（SP57x、SP58x、SP67x、SP68x）目前不支持 ESXi。</li><li>⑤ 已知配备的 Silicom 网卡最高支持 ESXi 6.x。</li></ul><h2>其他品牌服务器兼容性</h2><p>如果已经有定制版的品牌，可以访问品牌官网查询官方兼容列表，本站定制版兼容性更加广泛。</p><p>欢迎提供机型和配置（CPU、RAID Controller、网卡）来询。</p><p>请提供以下四个信息：</p><ul><li>机器品牌和型号</li><li>CPU 型号</li><li>RAID 卡型号</li><li>网卡型号</li></ul><h2>常见问题解答 (FAQ)</h2><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=TyQYgDWHjEsS8ojjMnmBUQ%3D%3D.3MBGCu%2B5vEAacj6reZlqWg3JsHWORHnVpv4DEuYghyqYhe1GDBuB0NNUb%2BvjUkt3" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a> 查看。</p><h2>下载地址</h2><p><strong>VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 标准版和厂商定制版</strong>：</p><ul><li>发布日期：2026-01-20</li><li>新增 vCenter 和 ESXi SSL 证书自动续期功能，以及 25 项已知问题修复。详见官方文档及本站相关发布。</li><li><strong>Standard (标准版)</strong>：<a href="https://link.segmentfault.com/?enc=cVnjYoePBUL4ervrTguq7A%3D%3D.3vPQs8fGiv8YOF54g5M9NGDj6CSDoPZFpjE8sS%2FhME2LrB03IAE1rpx%2B0XTE56vE" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Dell (戴尔) PowerEdge 定制版</strong> A02：<a href="https://link.segmentfault.com/?enc=C9pzssEvUzV5%2B241Dtiw5w%3D%3D.UOjX06sqGATBmVvRBX01uZ4PA4gPOGMIQ6YNuAkYvSsg6uvv9MOR59xcjRTtvW6p" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>HPE (慧与) ProLiant 定制版</strong> Sep2025：<a href="https://link.segmentfault.com/?enc=8ZiIa%2FeOinn0MR6u4Na1Iw%3D%3D.J%2FW1FVvx6a6rvmfGfIsT5bk5Ax5lvq3F7I5sFkOMG6UEfBTGbvMmEPav%2Bdcwcd7c" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Huawei (华为) FusionServer 定制版</strong>：<a href="https://link.segmentfault.com/?enc=k%2FN6mUvUNiHvd1DvvLzmlg%3D%3D.IjqzXvX5DKyyIvMG%2Bq1J0ACm9NC7jtj50Hnx8DA7zERRN5jtI58hDpZU0xFnVAiO" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>xFusion (超聚变) FusionServer 定制版</strong>：<a href="https://link.segmentfault.com/?enc=oCfLLmoxbwJxI%2F1TdVmAZA%3D%3D.8BHqXgZ024C4ERyu64kkDIOllRRCcOZKGYo8YVFN9Bie0PgvT1yvd63rnR493wkx" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Lenovo (联想) 定制版</strong> S02：<a href="https://link.segmentfault.com/?enc=0tj66iXhiOIBjfsg3MsKVA%3D%3D.bpEGBTmaEF8c6PAqo2nWXMDtvhVwkm74yqjNTWQn4Tetp%2BrszYd26WbdQSHmFuhA" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>Inspur (浪潮) 定制版</strong>：<a href="https://link.segmentfault.com/?enc=rrmbM8ANe2n5NB0u7%2BEJ1A%3D%3D.MoLI9APOkpiIDI9U7Sf9YWr%2FVmutgyhyYjzRLKSDe9py%2B9hdNSZICHeSpfRQyZ3h" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><strong>H3C (新华三) 定制版</strong>：<a href="https://link.segmentfault.com/?enc=9%2FjNPmBir%2BEdKwug1Nj5%2Bg%3D%3D.USRWKvo6wOYxmLmw5zyKx77oIOICiZW0gFXkCYsiIv95EvLxBftQD4Q1A7mP7JjO" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></li><li><p><strong>其他定制版</strong>：<a href="https://link.segmentfault.com/?enc=lxyyaLDpxaF9zfXD8xHarA%3D%3D.sBLo1q%2BFnvLPDPMSIrdIDEq%2FUcsFiuXqRIOz2x4oaVYj6aGnnjGtY%2FnmyaaKsMPf" rel="nofollow" target="_blank">https://sysin.org/blog/vmware-esxi-9-oem/</a></p><ul><li>Cisco (思科) UCS 定制版</li><li>Fujitsu (富士通) 定制版</li><li>Hitachi (日立) 定制版</li><li>NEC (日电) 定制版</li></ul></li></ul><hr/><p>集成驱动版本，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=pHHlFpqZ%2FnPAIKGjUvILzg%3D%3D.8o1uJha4RUn2mRzzndw%2FvNOVpLPVd7ifZQK5iW5%2BoLCzF6JR9oyJEiWfcZmbBYmC" rel="nofollow" target="_blank">VMware ESXi 9.0.2.0 macOS Unlocker &amp; OEM BIOS 2.7 集成网卡驱动和 NVMe 驱动 (集成驱动版)</a></li></ul><p>官方原版，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=qy%2FvojTs1Anlbzoec1mBqQ%3D%3D.ibHQaES5n992rV9N2HUiti9O019zHWnw9WyzY%2BPcsK3E%2FlYv3O%2FHCBW%2FubgMNkGj" rel="nofollow" target="_blank">VMware vSphere 9.0.2.0 正式版发布 - 企业级工作负载平台</a></li></ul><p>上一个版本，请访问：</p><ul><li><a href="https://link.segmentfault.com/?enc=LbmDS5l536oBQ0JcVF%2BSlw%3D%3D.QBbYBbXJY4gkKlmduC8yq%2BJGZSGxMcmPmSALUfEQW4yFGKMDFnqTCRBYo1hiPIaa" rel="nofollow" target="_blank">VMware ESXi 8.0U3h macOS Unlocker &amp; OEM BIOS 2.7 标准版和厂商定制版</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=jT%2B1YlgZnHymMhDQCw0ZRA%3D%3D.SmDZm8ZH%2Fg9OdWGcDmoLjLuKfXfkIjfvz93K0eI2pDA%3D" rel="nofollow" target="_blank">VMware 产品下载汇总</a></p>]]></description></item><item>    <title><![CDATA[RUM 链路打通实战：打破移动端可观测性黑洞 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578294</link>    <guid>https://segmentfault.com/a/1190000047578294</guid>    <pubDate>2026-01-28 17:07:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：路锦（小蘭）</p><h2>背景：移动端的“可观测性黑洞”</h2><p>在微服务架构蓬勃发展的今天，服务端的可观测性建设已日趋成熟。无论是 Jaeger、Zipkin 还是 SkyWalking，这些分布式链路追踪工具都能够帮助开发者清晰地观察到一个请求从网关进入后，是如何在各个微服务之间层层流转、逐级调用的。然而，当我们试图将这条链路向前延伸至移动端时，却发现一道难以逾越的鸿沟横亘其间。</p><ul><li>关联困难：移动端和服务端仿佛两座孤岛，各自维护着独立的日志系统。客户端记录着请求发起的时间和结果，服务端保存着完整的调用链路，但两者之间缺乏一条有效的纽带将其串联起来。一旦出现问题，排查人员只能依靠时间戳进行手工比对，既费时又容易出错，遇到高并发场景更是如同大海捞针。</li><li>定位模糊：我们常常遇到这样的场景：用户投诉说接口超时了，但翻开服务端监控，每一条请求都显示着正常返回的 200 状态码。问题究竟出在用户的网络环境、运营商的链路质量，还是服务端在某个瞬间的抖动？由于移动端与服务端的监控体系相互割裂，我们根本无从判断故障边界，各团队之间也容易陷入相互推诿的困境。</li><li>复现无门：移动端的网络环境远比服务端复杂——DNS 解析可能受到劫持、SSL 握手可能遭遇兼容性问题、弱网环境下的重试和超时更是家常便饭。这些关键信息在传统方案中往往随着请求结束而烟消云散，当问题间歇性发生时，开发者既无法还原现场，也难以定位根因，只能在用户的反复投诉中束手无策。</li></ul><p>正是这些痛点的存在，让端到端全链路追踪的需求变得愈发迫切。我们需要一种方案，能够让移动端真正成为分布式链路的起点，让每一次用户操作触发的请求都能够被完整记录、精准关联、一路追踪到最底层的数据库调用。本文将通过一个最佳实践案例，展示如何借助阿里云用户体验监控实现移动端到后端的全链路 Trace 打通，辅助网络请求问题排查。</p><h2>核心方案：端到端链路打通的技术实现</h2><h3>核心思想</h3><p>端到端链路打通的本质是：<strong>让客户端成为分布式追踪链路的第一跳，使其与服务端链路共享同一个 Trace ID。</strong></p><p>在传统架构中，链路追踪的起点是服务端网关——请求进入网关时，APM Agent 为其分配 Trace ID，并在后续的微服务调用中透传。而端到端打通方案则将这个起点前移到了用户的手机上，由移动端 SDK 主动生成 Trace ID 并注入到请求头中，使得整条链路从用户指尖到数据库底层都被同一个标识串联起来。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578296" alt="image" title="image"/></p><h3>技术实现的四个关键环节</h3><p>整个链路打通的实现可以分为四个紧密衔接的环节：</p><h4>环节一：客户端生成链路标识</h4><p>当用户触发一次网络请求时，客户端 SDK 在请求真正发出之前介入：</p><ol><li><strong>拦截请求</strong>：通过网络库的拦截机制（如 OkHttp Interceptor）捕获即将发出的请求</li><li><p><strong>创建 Span</strong>：为这次请求创建一个 Span 对象，生成两个核心标识：</p><ul><li><strong>Trace ID</strong>（32 位十六进制）：整条链路的唯一身份</li><li><strong>Span ID</strong>（16 位十六进制）：当前这一跳的唯一身份</li></ul></li><li><strong>记录起始时间</strong>：精确记录请求发起的时间戳，用于后续计算各阶段耗时</li></ol><h4>环节二：协议编码与注入</h4><p>生成链路标识后，需要将其编码为服务端能够理解的格式。这里的关键是选择一套双方都遵守的“通用语言”——W3C Trace Context 或 SkyWalking SW8 协议。</p><p>客户端 SDK 将编码后的信息写入 HTTP 请求头，随请求一同发送。</p><h4>环节三：网络传输与透传</h4><p>HTTP 协议天然具备请求头的穿透性，这是透传能够实现的技术基础：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578297" alt="image" title="image" loading="lazy"/></p><h4>环节四：服务端接收与延续</h4><p>当请求到达服务端时，APM Agent 接管后续处理，完成链路的延续：</p><ol><li><strong>解析请求头</strong>：从 <code>traceparent</code> 或 <code>sw8</code> 头中提取 Trace ID 和 Parent Span ID</li><li><strong>继承链路上下文</strong>：将客户端传入的 Trace ID 作为本条链路的身份标识，而非重新生成</li><li><strong>创建子 Span</strong>：为服务端的处理逻辑创建新的 Span，其 Parent Span ID 指向客户端的 Span</li><li><strong>继续透传</strong>：在调用下游服务时，继续在请求头中携带同一个 Trace ID</li></ol><p>通过这四个环节的紧密配合，移动端发出的每一个请求都能与服务端的调用链路无缝衔接，形成一条从用户设备到数据库的完整追踪链路。</p><h3>链路打通协议</h3><p>为了让不同系统之间能够“说同一种语言”，业界制定了标准化的链路追踪协议。目前主流的协议有两种：</p><h4>W3C Trace Context 协议</h4><p>W3C Trace Context 是 W3C 官方标准，具有最广泛的兼容性。</p><p><strong>Header 格式：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578298" alt="image" title="image" loading="lazy"/></p><p><strong>字段说明：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578299" alt="image" title="image" loading="lazy"/></p><p><strong>适用场景：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578300" alt="image" title="image" loading="lazy"/></p><h4>SkyWalking SW8 协议</h4><p>SW8 是 Apache SkyWalking 的原生协议，包含更丰富的上下文信息。</p><p><strong>Header 格式：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578301" alt="image" title="image" loading="lazy"/></p><p><strong>字段说明：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578302" alt="image" title="image" loading="lazy"/></p><p><strong>适用场景：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578303" alt="image" title="image" loading="lazy"/></p><h2>实战案例：一次查询接口超时的全链路排查</h2><p>理论讲完了，接下来让我们通过一个真实的排查案例，看看端到端链路打通在实际问题定位中是如何发挥作用的。</p><h3>问题背景</h3><p>我们基于某开源代码库构造了一个慢请求场景，架构如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578304" alt="image" title="image" loading="lazy"/></p><p>在日常使用中，我们发现某个页面打开十分缓慢，用户体验极差。初步怀疑是由于 API 请求响应过慢导致，但具体慢在哪里、为什么慢，还需要进一步分析。接下来，我们将借助阿里云用户体验监控的全链路追踪能力，一步步定位问题根因。</p><h3>第一步：在云监控控制台定位异常请求</h3><p>首先，我们登录阿里云控制台，进入<strong>云监控 2.0 控制台 → 用户体验监控 →您的应用 → API 请求</strong>模块。在这里，我们可以看到所有 API 请求的性能统计数据。</p><p>通过对“缓慢占比”进行排序，我们很快发现了问题所在：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578305" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578306" alt="image" title="image" loading="lazy"/></p><p>从监控数据可以清晰地看到，API <code>/java/products</code> 的响应时间异常——平均耗时高达 40 多秒！这个耗时远远超出了正常范围，难怪用户会感觉页面打开缓慢。</p><p>找到了可疑的 API，接下来我们需要进一步分析它的调用链，搞清楚这 40 多秒究竟消耗在了哪个环节。</p><h3>第二步：查看调用链，追踪服务端链路</h3><p>点击对应 API 的“<strong>查看调用链</strong>”按钮，系统会跳转到当前请求的 Trace 详情页面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578307" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578308" alt="image" title="image" loading="lazy"/></p><p>这里就是端到端链路打通的核心价值所在——我们可以直接看到从移动端到后端的完整调用链路，无需在多个系统之间来回切换。</p><p>从链路瀑布图中可以清晰地看到：</p><ul><li>移动端发起请求后，链路完整地延续到了后端服务</li><li>耗时主要发生在后端服务的 <code>/products</code> 接口</li><li>该接口处理耗时超过 40 秒才返回数据</li></ul><p>为了方便后续在后端应用监控中进行更深入的分析，我们记录下当前的 Trace ID：<code>c7f332f53a9f42ffa21ef6c92f029c15</code>。</p><h3>第三步：查看后端服务Trace分析</h3><p>接下来，我们进入应用监控 → 找到对应的后端应用 → 调用链分析，使用刚才记录的 Trace ID 进行查询。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578309" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578310" alt="image" title="image" loading="lazy"/></p><p>从后端链路数据可以还原出 <code>/products</code> 接口的执行链路：</p><ul><li>HikariDataSource.getConnection，重复 6 次，总耗时 3ms。含义：获取数据库连接（从连接池拿）一共 6 次，总共才 3ms，不是瓶颈。</li><li>postgres，重复 6 次，总耗时 2ms。这是一些非常快的 Postgres 操作/小查询，同样不是瓶颈。</li><li>SELECT postgres.products，重复执行了1 +  5 次，总耗时 42290ms ≈ 42.3s。这行才是关键：同一个 SQL（查 products 相关）一共执行了 5 次，总耗时 42.3s，平均每次大约 8 秒。</li><li>也就是说：主要时间都花在执行这个 SQL 上，而不是连库 / 建连接 / 网络。</li></ul><h3>第四步：深入分析慢 SQL</h3><p>点击链路中的最后一个 Span，我们可以在右侧详情面板中看到具体执行的 SQL 语句：</p><pre><code>-- 第一次查询：获取全量产品数据
SELECT * FROM products
-- 对每个产品执行 N 次查询（N+1 问题）
SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code></pre><p>问题的根因逐渐浮出水面：</p><ol><li>第一次查询：<code>SELECT * FROM products</code> 获取所有产品，这一步耗时尚可</li><li>N 次循环查询：对于每一个产品，又单独执行了一次 <code>SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code></li></ol><p>这是一个典型的 N+1 查询问题！更糟糕的是，<code>weekly_promotions</code> 是一个特意设计的“慢视图”（sleepy view），每次查询都会执行较重的操作。当产品数量较多时，这些查询累积起来就造成了 42 秒的惊人耗时。</p><p>我们记录下当前的线程名称：<code>http-nio-7001-exec-3</code>，以便进一步查看 Profile 数据进行验证。</p><h3>第五步：查看 Profile 数据验证结论</h3><p>为了进一步确认我们的分析结论，我们进入<strong>应用诊断 → 持续性能剖析</strong>，查看后端服务的 Profile 数据。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578311" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578312" alt="image" title="image" loading="lazy"/></p><p>筛选对应的线程后，我们看到了服务执行时间的分布情况：</p><ul><li><code>sun.nio.ch.Net.poll(FileDescriptor, int, long)</code> 耗时占比接近 100%</li><li>这表明线程大部分时间都在<strong>等待 Postgres socket 返回数据</strong></li></ul><p>Profile 数据与调用链分析的结论完全吻合——问题确实出在数据库查询上，线程一直在等待慢 SQL 的执行结果。</p><h3>第六步：定位根因</h3><p>综合以上分析，我们可以清晰地定位到问题的根因：</p><p><strong>问题根因：N+1 查询 + 慢视图</strong></p><ol><li><p>代码逻辑存在 N+1 查询问题：</p><ul><li>第一次查询：<code>SELECT * FROM products</code>（1 次）</li><li>对每个 product 循环查询：<code>SELECT * FROM reviews, weekly_promotions WHERE productId = ?</code>（N 次）</li></ul></li><li>weekly_promotions 是一个“慢视图”，查询本身就很耗时</li><li>两者叠加，导致接口总耗时超过 40 秒</li></ol><h2>总结</h2><p>全链路端到端打通解决了移动端与服务端之间的“可观测性黑洞”问题。通过在移动端注入标准化的 Trace Header，实现：</p><ul><li><strong>统一追踪</strong>：移动端请求和服务端链路使用同一个 TraceID，一键关联；</li><li><strong>精准定位</strong>：从用户手机到数据库，每一跳的耗时清晰可见；</li><li><strong>快速定界</strong>：告别“移动端说服务端慢，服务端说网络差”的扯皮；</li><li><strong>数据驱动</strong>：基于真实链路数据优化，而非猜测。</li></ul><p>阿里云 RUM 针对 Android 端实现了对应用性能、稳定性、和用户行为的无侵入式监控采集 SDK，可以参考接入文档体验使用。除了 Android 外，RUM 也支持 Web、小程序、iOS、鸿蒙等多种平台监控分析，相关问题可以加入“RUM 用户体验监控支持群”（钉钉群号： 67370002064）进行咨询。</p><p>参考链接：</p><p>[1] Android 接入文档：</p><p><a href="https://link.segmentfault.com/?enc=Q5ts%2FSzP8fmvVC4A2jKZ7Q%3D%3D.gH4g3tlhhPuA0AxRChHxY3dHKtQ3R2aSmFUzoJ8gTHUtWdKZ7gc7TccGlpbaWS33pBWjCYRpVXZ6ecmbUFh0jlo%2BySjAuvIIzJz%2F2%2B6SB8gAnzwUDxAZp1uTj6mN3DVZFUB1uR%2FpIke7jmm4e7ikLNOL4EVWkXz%2BTVFEPwhLc3P6%2BLnTsSY5q8wC9KTIPQHsKydc01Vlna2BcMW%2FqCgGXw%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/user-experience-monitoring/ac...</a></p><p>[2] Java 应用监控说明文档：</p><p><a href="https://link.segmentfault.com/?enc=jB3kUv0WnhJDT3Vsljemjg%3D%3D.sqagD4cnsXfcUMp3dIzCvvm0CztIiobvPz1fIaB%2B7lwLn%2FV%2B9szbNx4IgZ3WooSVM8NwwZx0x4rrXjf7cAtPr2BNFl31dhXjI0zRQTRt6Vy0JzsPaI64SoW%2BWU9S6wyCqUuxBYq1RcdorGTpOWFk9lmfW%2B8MrHQer4CQK0qGeaAVVNXWhYl3CdKWKe04uWdtPoaQws4kq7HRYS6RwwyBHZ1ivj9aMzmyizdut3FL6diLhSr38nuTDTflNgKMp%2Fv0" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/application-monitoring/user-g...</a></p><p>[3] 持续性能剖析说明文档：</p><p><a href="https://link.segmentfault.com/?enc=dkC4NQ5gB70f5D8Dpkrgdw%3D%3D.QYMKcux5olOoY7M1wdvgBUTks4JeUxWzq8kGkW0Aj5uDcirMCWNWMN%2FX0Q8ZYNu12YuBACqBcMKTcZR%2Fygr%2F5UiUpgXpMgejbT4YxN5582Z3sn91wewSvVBX2Fp0CtQzOz%2FcQv93VyG1sVTrbIa%2FHU050NlTpUl6dlvfnmmf2dC9%2B4S1ySWIdNNxrPVb8O3RTjdFplwOu7pbn4T8JlRFcjivgBARZ4WQHENgBc6tjzP6XACLj0%2FwhNd6Lva4rIqDw5tCBm4%2BKOTC1YBv%2FE7q8A%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/arms/application-monitoring/user-g...</a></p>]]></description></item><item>    <title><![CDATA[使用 Python 轻松添加文本水印到 PDF 宇文成都 ]]></title>    <link>https://segmentfault.com/a/1190000047578334</link>    <guid>https://segmentfault.com/a/1190000047578334</guid>    <pubDate>2026-01-28 17:06:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代办公环境中，PDF 文档的安全性变得愈发重要。添加水印是确保资料安全，防止未授权复制的一种有效手段。本文将介绍如何使用 Python 的 Spire.PDF 库为 PDF 文档添加文本水印。</p><h2>Spire.PDF 简介</h2><p>Spire.PDF 是一个功能强大的 PDF 处理库，支持多种 PDF 操作，包括创建、编辑、转换和打印 PDF 文档。对于想要在 Python 中实现 PDF 操作的开发者而言，Spire.PDF 提供了简洁的 API，让用户能够轻松访问和操作 PDF 文件。</p><h2>安装 Spire.PDF</h2><p>在使用 Spire.PDF 之前，需要先进行安装。可以通过以下命令在命令行中使用 pip 安装该库：</p><pre><code class="bash">pip install spire-pdf</code></pre><p>确保在执行上述命令之前，已经安装了 Python 环境和 pip。</p><h2>为 PDF 文档添加水印的示例代码</h2><p>接下来，我们将通过一个示例代码来演示如何为 PDF 文档添加文本水印。以下是简化后的代码示例：</p><pre><code class="python">from spire.pdf import PdfDocument
from spire.pdf.common import PdfTrueTypeFont, PdfBrushes, PointF

# 创建 PdfDocument 类的对象并加载 PDF
doc = PdfDocument()
doc.LoadFromFile("C:\\Users\\Administrator\\Desktop\\Input.pdf")

# 创建水印字体
font = PdfTrueTypeFont("黑体", 48.0, 0, True)
text = "仅 内 部 使 用"

# 计算文本尺寸
text_width = font.MeasureString(text).Width
text_height = font.MeasureString(text).Height

# 遍历每一页添加水印
for i in range(doc.Pages.Count):
    page = doc.Pages.get_Item(i)
    state = page.Canvas.Save()  # 保存当前画布状态
    
    # 计算页面中心坐标
    x = page.Canvas.Size.Width / 2
    y = page.Canvas.Size.Height / 2

    # 调整坐标系，使页面中心成为原点
    page.Canvas.TranslateTransform(x, y)
    page.Canvas.RotateTransform(-45.0)  # 逆时针旋转45度
    
    page.Canvas.SetTransparency(0.4)  # 设置透明度
    
    # 绘制水印文本
    page.Canvas.DrawString(text, font, PdfBrushes.get_Blue(), PointF(-text_width / 2, -text_height / 2))
    
    page.Canvas.Restore(state)  # 恢复画布状态

# 保存修改后的文档
doc.SaveToFile("output/TextWatermark.pdf")
doc.Dispose()  # 释放资源</code></pre><h2>代码解析</h2><ol><li><strong>加载 PDF 文档</strong> ：首先，我们通过 <code>PdfDocument</code> 类加载指定路径的 PDF 文档。</li><li><strong>设置水印字体和文本</strong> ：接着，我们创建一个 <code>PdfTrueTypeFont</code> 对象，指定字体、大小和样式，并定义水印文本。</li><li><strong>计算文本尺寸</strong> ：使用 <code>MeasureString</code> 方法获取文本的宽度和高度，以便正确定位水印。</li><li><strong>遍历文档的每一页</strong> ：使用 for 循环遍历文档中的每一页，在每一页上绘制水印。</li><li><strong>保存和释放资源</strong> ：最后，将修改后的文档保存到新的 PDF 文件，并释放资源。</li></ol><h2>总结</h2><p>通过上述代码，开发者可以轻松地为 PDF 文档添加文本水印。这不仅提高了文档的安全性，还增强了其专业性。Spire.PDF 库提供了丰富的功能，极大地方便了 PDF 文件的处理。无论是个人项目还是企业级解决方案，Spire.PDF 都是一个值得考虑的选择。</p><p>希望本文能帮助您快速熟悉如何使用 Python 为 PDF 添加水印。待您自己试验时，请确保您有权限对相关 PDF 文档进行修改！</p>]]></description></item><item>    <title><![CDATA[产品管理系统怎么选？2026主流工具横评、场景适配与避坑 许国栋 ]]></title>    <link>https://segmentfault.com/a/1190000047578348</link>    <guid>https://segmentfault.com/a/1190000047578348</guid>    <pubDate>2026-01-28 17:05:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文横评 10 款产品管理系统：ONES、Jira、Aha! Roadmaps、Productboard、Craft、airfocus、Azure DevOps Boards、Rally by Broadcom、Perforce P4 Plan、Jama Connect。帮你按企业痛点与成熟度建立选型框架，减少双系统维护、口径不一与治理失控的隐性成本。</p><p>很多企业已经不缺工具，缺的是单一事实源（SSOT）：需求在产品侧、交付在工程侧、路线图在 PPT/表格，最终“优先级解释不清、变更影响评估不出、交付预测越来越不准”。此时再选一套产品管理系统，如果不先搞清“它解决的是上游决策、下游交付，还是全链路闭环”，很容易把问题从“协作割裂”升级成“系统割裂”。</p><p>下面我会给你 5 个常用的测评标准判断点，你可以把任何一款产品管理系统放进这 5 个问题里过一遍，看看哪一个更符合团队的需求。</p><ol><li>上游决策：是否支持洞察/想法沉淀、可解释的优先级（评分/公式/模型）？</li><li>路线图对齐：能否输出面向管理层/研发/业务的不同路线图视图？</li><li>交付联动：需求到迭代/任务的映射是否自然，还是要“人工翻译”？</li><li>追溯与审计：变更发生时，能否快速看到影响范围，并留证据链？</li><li>集成与可维护性：和现有工具链集成后，谁维护、如何升级、失败如何补偿？</li></ol><p>最小 POC 建议：用 3 条真实需求跑通“从决策到交付/验证”的链路，并故意做 1 次变更，观察系统是否能让影响分析与同步成本可控。</p><h2>工具盘点：10 款产品管理系统测评</h2><h4>1）<a href="https://link.segmentfault.com/?enc=zRZM8LB1%2BHi0oA2fqNRJgA%3D%3D.wiMzouaqL2GJeKiVAakeJpWM3lMkmWYn%2BfG%2BQiSQvKw%3D" rel="nofollow" target="_blank">ONES</a>：一体化研发管理平台型</h4><p>核心定位：强调“一个平台实现端到端的软件研发管理”，从需求管理、迭代跟进到测试，并提供效能改进与开放拓展能力；产品线包含 ONES Project、ONES TestCase、ONES Wiki 等。</p><p>产品管理能力：适合把“需求池—迭代计划—缺陷/测试—交付质量”放在同一数据结构里，减少产品系统与工程系统之间的反复同步。对 VP 来说，它的价值更像“研发侧 SSOT”：当管理层问“为什么延期/风险在哪”，你能从链路数据里给出一致解释。<br/>项目/交付管理能力：平台型系统天然强调流程与度量一致性——如果你的组织希望把敏捷/瀑布/混合流程落到同一治理框架中，这类产品更容易形成长期资产（模板、字段口径、报表口径）。</p><p>适用场景：多团队多项目、希望减少工具割裂；或国产化替代背景下，希望“需求—测试—知识库—流水线”尽量在同一生态中。</p><p>优势亮点：闭环完整、数据口径更容易统一；对研发效能与质量治理更友好（尤其当 PMO/效能团队愿意做数据治理）。</p><p><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdhI10" alt="ONES 产品管理全景图" title="ONES 产品管理全景图"/></p><h4>2）Jira + Jira Product Discovery</h4><p>核心定位：Jira Product Discovery 主打“捕捉洞察、优先级排序、构建路线图——都在 Jira 内完成”，并强调用数据与客户洞察帮助团队做出更有影响力的优先级决策。<br/>产品管理能力：强在“把上游讨论结构化”：洞察/想法/机会进入同一空间，优先级可围绕证据与数据展开；路线图视图用来减少对齐成本（尤其面对业务与管理层）。<br/>项目/交付管理能力：如果你的交付已经在 Jira Software，JPD 的价值在于减少“从产品语言翻译成工程语言”的损耗——至少能让上游输入更可追踪。<br/>适用场景：Jira 生态已经很深、产品团队想补齐 discovery 能力；或全球化团队需要依托成熟生态协作。<br/>优势亮点：生态成熟、协作惯性小；对“把产品讨论从口水战拉回证据链”很有效。<br/>局限与使用体验：高度可配置带来的副作用是“标准不统一就会越用越乱”。你需要明确：哪些字段是组织标准、哪些是团队自定义；否则后期数据不可比。</p><p><img width="723" height="399" referrerpolicy="no-referrer" src="/img/bVdne5p" alt="" title="" loading="lazy"/></p><h4>3）Aha! Roadmaps</h4><p>核心定位：强调建立优先级框架，用 scorecard/feature scores 让功能优先级更客观，并帮助团队对齐“下一步做什么”。<br/>产品管理能力：非常适合把“价值/成本/风险/战略契合度”等维度显性化，让优先级讨论变成“可解释的计算题”。当你的组织有多个产品线、需求争夺资源激烈，这种“框架化优先级”能显著降低内耗。<br/>项目/交付管理能力：它更像“产品办公室/组合管理层”的系统——擅长表达与对齐，但下游交付通常仍要对接工程系统。<br/>适用场景：产品战略与路线图需要强表达；管理层需要一套可复盘的优先级机制；产品线多、节奏复杂。<br/>优势亮点：scorecard 不是装饰品，它能把“谁更会说”变成“标准化权衡”。<br/>局限与使用体验：上游强不代表闭环强——如果工程侧系统割裂或集成不稳，容易出现“路线图很好看，交付仍失真”。</p><p><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdm9Wj" alt="" title="" loading="lazy"/></p><h4>4）Productboard</h4><p>核心定位：强调帮助产品经理理解客户需求、确定优先级，并让团队围绕路线图达成一致。<br/>产品管理能力：当你们最痛的是“反馈太多、信息太散、优先级总靠拍脑袋”，Productboard 的核心价值在于把“客户声音→机会→功能”这条链条系统化：既能沉淀证据，也能形成对外对内一致的路线图叙事。<br/>项目/交付管理能力：通常需要与工程系统配合；它更强在上游决策质量与对齐效率，而不是替代工程执行系统。<br/>适用场景：面向外部客户/多渠道反馈；需要把需求证据链纳入治理（避免“谁提得急就先做”）。<br/>优势亮点：对 VP 来说，能减少“做错方向”的返工成本，这往往比单点效率提升更值钱。<br/>局限与使用体验：如果工程侧没有明确 SSOT，容易形成“双系统写需求”的隐性成本；必须在流程里定义清楚：哪些字段在 Productboard 负责，哪些字段在交付系统负责。</p><p><img width="723" height="403" referrerpolicy="no-referrer" src="/img/bVdnNsA" alt="" title="" loading="lazy"/></p><h4>5）Craft.io</h4><p>核心定位：强调从 ideation 到 execution 的 OKR 全生命周期管理，并把 objectives 连接到 initiatives、projects、epics；同时支持 OKR-based roadmaps。<br/>产品管理能力：它的强项是把“目标—举措—特性/史诗”串起来。对企业级产品而言，这会直接提升 ROI 讨论质量：不是“这个需求看起来不错”，而是“它对应哪条目标、预期带来什么指标提升、投入多少交付成本”。<br/>项目/交付管理能力：适合作为产品侧中枢，再与工程系统联动；它更擅长管理“做什么/为什么做/如何取舍”，工程过程度量仍要看你们的交付底座。<br/>适用场景：OKR 已经是“硬机制”，需要把路线图与目标绑定；产品线多、跨团队对齐成本高。<br/>优势亮点：优先级模型 + OKR 绑定，会让需求排序更可解释、更可复盘。<br/>局限与使用体验：如果 OKR 本身口径不清或频繁摇摆，系统会把混乱“结构化”地记录下来；建议先把目标治理做好再导入。</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdnNsB" alt="" title="" loading="lazy"/></p><h4>6）airfocus</h4><p>核心定位：自我定位为“模块化产品管理软件”，用于管理与沟通产品策略、优先级与路线图，并强调解决“做对问题”。<br/>产品管理能力：适合从上游切入——先把优先级与路线图做清楚，再逐步与交付系统打通。对很多企业来说，这比“一步到位换平台”更现实：组织阻力小、试点更快、ROI 更容易证明。<br/>项目/交付管理能力：airfocus 本身不是工程执行系统，但它强调与 Azure DevOps 等的集成，让策略与日常开发保持同步。<br/>适用场景：产品团队需要提升上游决策质量，但工程体系暂不动；或希望先建立产品 SSOT，再逐步整合。<br/>优势亮点：模块化带来的“可控引入”是关键优势——先拿下最痛的环节（优先级/roadmap），再扩。<br/>局限与使用体验：闭环强弱高度依赖集成质量；若工程侧字段/流程不标准，最终仍会回到人工对齐。</p><p><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdnNsC" alt="" title="" loading="lazy"/></p><h4>7）Azure DevOps Boards</h4><p>核心定位：Azure Boards 的看板实践强调 WIP 限制：通过强调“完成优先于开始”，团队往往获得更高生产力与更好质量；官方文档给出如何设置与实施 WIP 的指南。<br/>产品管理能力：更偏“把需求拆成可交付工作并持续跟踪”，对需求证据链、路线图叙事并不突出；但如果你的目标是提升交付确定性，它能提供更可信的过程数据。<br/>项目/交付管理能力：强在看板治理、瓶颈识别与流程改进（WIP 本质上是“用机制逼迫组织减少多任务切换与等待浪费”）。对 VP 来说，这是效能体系落地的硬工具。<br/>适用场景：微软生态、DevOps 流水线与工程管理一体化诉求强；效能团队希望用过程数据推动改进。<br/>优势亮点：度量可信、治理可操作——能把“感觉很忙”变成“瓶颈在哪里、该怎么调 WIP/拆分工作”。<br/>局限与使用体验：如果把它硬当成完整产品管理系统，产品团队会觉得“上游不够产品化”；更合理的方式是：用它做交付底座，上游用专门的产品发现/roadmap 工具补齐。</p><p><img width="723" height="479" referrerpolicy="no-referrer" src="/img/bVdne5o" alt="" title="" loading="lazy"/></p><h4>8）Rally by Broadcom</h4><p>核心定位：Rally 官方强调“从投资决策到交付的完整可追溯性”，并作为 ValueOps 平台的一部分与其他产品集成、支持规模化。<br/>产品管理能力：它更像“组合/价值流层”的产品管理系统：当你要回答“资源投到哪些主题/举措、跨团队进展如何、关键优先级是否一致”，Rally 的强项在于把工作映射到更高层级的业务优先级。<br/>项目/交付管理能力：支持用 Portfolio Item 表达 initiative/feature 以计划、优先级与跟踪工作，这对大组织的节奏对齐很关键。<br/>适用场景：多团队多项目、多层级治理；PMO/效能团队需要统一方法论与组合视角；管理层强烈要求“可解释的进展与风险”。<br/>优势亮点：减少“汇报型管理”，提高“系统型治理”——让领导看见的不是 PPT，而是从投资到交付的链路状态。<br/>局限与使用体验：治理成本高、对流程纪律要求强；如果组织没有统一口径，上线后可能变成“填报系统”。</p><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnnym" alt="" title="" loading="lazy"/></p><h4>9）Perforce P4 Plan（formerly Hansoft）</h4><p>核心定位：Perforce 官方描述 P4 Plan 能用多种视图（Product Backlog、Quality Assurance、Planning）洞察项目范围，并支持 capacity planning、查看项目历史、可本地或云部署。<br/>产品管理能力：当你的核心挑战是“复杂依赖 + 资源约束 + 计划频繁变更”，P4 Plan 的价值在于把计划从静态表格变成动态系统：依赖关系、范围变化、产能约束可以更直观地被管理与讨论。<br/>项目/交付管理能力：它适配多交付方法（敏捷/瀑布/混合），适合在大规模协作中做“计划可信度治理”。<br/>适用场景：复杂工程（例如大型产品、跨团队依赖强）、对排期与资源规划敏感；希望提升计划可执行性，而非只做任务跟踪。<br/>优势亮点：依赖管理与产能规划是硬能力；当你要把“承诺交付”变得更可信，这类工具往往比“更花哨的 roadmap”更有用。<br/>局限与使用体验：上游洞察与路线图叙事不是它的强项；如果产品团队需要强 discovery，通常要配套上游工具。</p><p><img width="723" height="423" referrerpolicy="no-referrer" src="/img/bVdnNsG" alt="" title="" loading="lazy"/></p><h4>10）Jama Connect</h4><p>核心定位：强调 Live Traceability（实时追溯），用于跨需求、测试、风险活动建立端到端追溯并持续改进过程绩效；并可从高层需求追溯到最终测试。<br/>产品管理能力：它解决的不是“路线图怎么画”，而是“变更影响怎么控、证据链怎么留”。对强合规行业，系统能否在需求变化时快速定位影响并形成审计材料，往往决定了交付风险与合规成本的上限。<br/>项目/交付管理能力：更偏需求工程与验证闭环；测试侧能力上，Jama Connect 会自动建立测试用例与测试运行之间的追溯关系并在 Trace View 显示。<br/>适用场景：医疗、汽车、工业控制、航空航天等高风险/强合规产品；或“软硬件协同”场景下，对需求一致性与验证闭环要求很高的组织。<br/>优势亮点：VP 视角 ROI 主要来自风险下降：返工减少、验证更可控、合规更稳；而不是单点效率提升。<br/>局限与使用体验：方法论与流程要求更严肃；若组织只想要轻量需求池，会觉得“过重”。</p><p><img width="723" height="415" referrerpolicy="no-referrer" src="/img/bVdnofz" alt="" title="" loading="lazy"/></p><h2>常见问题 FAQ：</h2><p><strong>Q1：产品管理系统和项目管理系统有什么区别？</strong><br/>A：项目管理系统更关注“按计划推进任务”；产品管理系统更关注“为什么做、先做什么、如何对齐路线图，以及如何与交付/追溯闭环”。如果缺少优先级与路线图能力，往往更像项目管理。</p><p><strong>Q2：中大型企业一定要两套系统（产品 + 交付）吗？</strong><br/>A：不一定。关键在 SSOT 放哪，以及同步是否可持续。平台型一体化能降低双系统成本；上游产品系统 + 工程系统组合则更灵活，但治理要求更高。</p><p><strong>Q3：选产品管理系统最常见的 3 个坑是什么？</strong><br/>A：把“功能演示”当“落地能力”；忽略字段/流程治理导致口径失控；低估集成与数据一致性的长期维护成本。</p><p><strong>Q4：POC 做多久比较合理？</strong><br/>A：建议 6–8 周：第 1–2 周对齐需求分层与字段口径，第 3–6 周跑 1 条业务线真实需求闭环，第 7–8 周复盘度量口径与推广成本。</p><p><strong>Q5：为什么我更强调追溯（Traceability）？</strong><br/>A：因为追溯决定你能否在变更发生时快速评估影响与风险，并形成可审计证据链。对强内控/强合规企业，这是“成本与风险”的硬约束。</p>]]></description></item><item>    <title><![CDATA[【已结束】AgentScope Java 和 AgentRun 邀您参与 PolarDB 开发者大会]]></title>    <link>https://segmentfault.com/a/1190000047578363</link>    <guid>https://segmentfault.com/a/1190000047578363</guid>    <pubDate>2026-01-28 17:04:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>第三届 PolarDB 开发者大会</p><p>📍 1 月 20 日，上海 · 五角场凯悦酒店</p><p>作为 AI 时代下的云原生数据库领域开年技术盛宴，大会不仅聚焦“AI 就绪的云原生数据库”的前沿实践，呈现 30+ 场技术演讲；更是携手各社区伙伴，一起带来数场 AI 互动体验，用真实体验、互动来感知 AI 时代的数据库，感受数据+AI 的无限可能。</p><p><strong>AgentScope Java：Agentic LLM 应用开发框架</strong></p><p>AgentScope Java 是以 Agentic 为核心设计理念，面向 Java 开发者的 LLM 应用开发框架。包括核心层、Studio、RL、Memory，以及架构上全力推进 Serverless 化，实现毫秒级冷启动与混合部署，帮助开发者在应对高并发的同时，显著降低部署成本并提升效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578365" alt="image" title="image"/></p><p><strong>AgentRun：一站式 Agentic AI 基础设施平台</strong></p><p>函数计算 AgentRun 是以高代码为核心的一站式 Agentic AI 基础设施平台，秉持生态开放和灵活组装的理念，为企业级 Agent 应用提供从开发、部署到运维的全生命周期管理，让 Agentic AI 真正进入企业生产环境。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578366" alt="image" title="image" loading="lazy"/></p><p>现场还有</p><p>《PolarDB AI 能力集》</p><p>《AI 原生应用架构白皮书》</p><p>等您来领取</p>]]></description></item><item>    <title><![CDATA[Opentelemetry koko ]]></title>    <link>https://segmentfault.com/a/1190000047578376</link>    <guid>https://segmentfault.com/a/1190000047578376</guid>    <pubDate>2026-01-28 17:04:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OpenTelemetry（OTel）是一个统一的可观测性框架，用于采集应用程序的日志、指标和链路追踪，并可将数据发送到不同后端进行存储和分析</p><h2>概览</h2><ul><li>Logs: 应用程序日志</li><li>Traces：日志追溯码</li><li>Metrics：应用程序监控指标</li></ul><h3>流程图</h3><pre style="display:none;"><code class="mermaid">flowchart LR
    A[应用程序] --&gt;|OTel SDK| B[OTel Collector]
    B --&gt; C[日志存储 Loki / ES]
    B --&gt; D[指标存储 Prometheus]
    B --&gt; E[链路追踪存储 Tempo / Jaeger]
    C --&gt; F[可视化 &amp; 查询 Grafana]
    D --&gt; F
    E --&gt; F
</code></pre><h2>Otel SDK</h2><p>要使用 Otel 需要在应用里嵌入对应语言的 <a href="https://link.segmentfault.com/?enc=Zm64iiSn2ya29q1zcqDpCg%3D%3D.oKwoDRdJtKD9oR4T4meG8%2FNHhxo5AZpbSwE30Q8Y63Uyx1Fw4VAtPZocjVSs%2BV09" rel="nofollow" target="_blank">SDK</a> 来产生可观测数据, 导出到对应的 <a href="https://link.segmentfault.com/?enc=l3malpuuOAoLYGnZKVUuLg%3D%3D.Iud%2BLDy3s%2BHa%2BFjObYpEXo2PnT%2BYAkEQNN3I3%2BcdEhO%2F4T3GxyHHZ%2BwXs5mjO5t%2B" rel="nofollow" target="_blank">Collector</a></p><h2>Collector</h2><p><a href="https://link.segmentfault.com/?enc=WwWla%2FIvZmpeaI1H70mtbg%3D%3D.J%2BTUQfvRO8AR4kAx72TIiERQ1gqKrW5rdv4FIUDvTmCO3XYsMFpRx53rDKKF9WXQ" rel="nofollow" target="_blank">Collector</a>：用于采集应用程序的 <strong>日志、指标和链路追踪</strong>，并可以统一处理和转发到不同后端</p><p><strong>流行的 Collector</strong></p><ul><li><strong>OpenTelemetry Collector（OTel Collector）</strong>：官方推荐，支持多协议输入、处理和导出</li><li><strong>Vector</strong>：高性能日志和指标采集器，支持多种后端</li><li><strong>Fluentd / Fluent Bit</strong>：主要用于日志采集，也可与 OTLP 配合</li></ul><h2>Storage Backend</h2><p>在 Otel 中，<strong>存储后端(Storage Backend)</strong> 是用于<strong>持久化存储并分析</strong>应用产生的可观测性数据的系统</p><blockquote>实际生产中，<strong>一个存储后端可能只负责一种数据类型</strong>，也可能（如 ClickHouse）同时承载多种数据。</blockquote><p><strong>流行的存储后端</strong></p><ul><li><p>Logs (日志存储)</p><ul><li>Loki</li><li>ClickHouse</li><li>Elasticsearch</li></ul></li><li><p>Metrics (指标存储)</p><ul><li>Prometheus</li></ul></li><li><p>Traces (链路追踪)</p><ul><li>Tempo</li><li>Jaeger</li><li>Zipkin</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[企业数字化生存底线 JoySSL深度剖析企业部署数字证书的核心动因 完美的铁板烧 ]]></title>    <link>https://segmentfault.com/a/1190000047578393</link>    <guid>https://segmentfault.com/a/1190000047578393</guid>    <pubDate>2026-01-28 17:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在竞争日益激烈的商业环境中，无论是企业的官方网站、客户服务系统、内部管理工具还是移动应用后端，它们不仅仅是信息传递的窗口，更是推动企业业务增长、维系客户关系的核心动力。然而，是否全面部署SSL证书，一个看似基本却影响深远的抉择，会从根本上改变企业未来的发展方向。JoySSL市场经理指出，多数企业对SSL证书并没有完整或清晰的概念，认知仍存在明显的差距。部分决策者将其简单地理解为“满足浏览器的要求”或“让URL显示小锁标志”的技术设置。一部分企业责任人甚至直接认为数字证书不过是网站的附属功能，可有可无。实际上，SSL证书的部署不仅是企业迈入值得信任的数字商业环境的一道“强制性入门门槛”，更是一项具备战略意义的信任投资。这一部署的重要性，贯穿企业线上营销的始终。 </p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnNtw" alt="" title=""/></p><p><strong>积极响应政策履行合法合规义务</strong></p><p>随着全球数据保护和网络安全的监管力度持续加大，合规性变得更加不可忽视。《网络安全法》、《数据安全法》和《个人信息保护法》等法规明确要求，网络运营主体采取技术手段保护数据传输的安全性，防止泄露、窃取或篡改。实现全站HTTPS加密，是“采用加密技术”等法定要求的直接且广泛认可的实践方法。 </p><p>在金融支付、电子商务、医疗健康以及政务服务等领域，明确要求对敏感信息的传输进行加密保护。若未启用有效SSL证书，企业可能遭遇业务合作上的限制、审计失败甚至失去运营资格的风险。 </p><p><img width="723" height="474" referrerpolicy="no-referrer" src="/img/bVdnNty" alt="" title="" loading="lazy"/></p><p><strong>部署SSL证书构建安全网络防线</strong></p><p>攻击者通常选择防御体系的薄弱点作为突破口，未加密的HTTP通信在数据传输过程中极容易被窃取。通过SSL证书构建加密通道，可有效防止数据窃听。OV或EV证书，可对企业实体进行严格审核，将认证的公司身份与站点紧密绑定，从根本上防范钓鱼攻击与身份伪造，从而构筑能够抵御网络风险的防线。</p><p><strong>守护企业品牌形象 提升竞争力</strong></p><p>主流浏览器会对未启用HTTPS的网站显示“不安全”的标记，导致用户对网站产生不信任感，有损品牌形象。</p><p>主动引入高等级SSL证书，展示绿色企业名称，可以向用户传递“专业、可信”的强烈印象。将安全性提升为品牌竞争优势，使安全投入成为商业收益的驱动力。</p><p><img width="723" height="472" referrerpolicy="no-referrer" src="/img/bVdnNtA" alt="" title="" loading="lazy"/></p><p><strong>提升搜索排序解锁现代网络能力</strong></p><p>部署SSL证书可提升网页加载速度与性能，用户体验也会进一步提升。此外，以小程序为代表的平台生态，均需遵循HTTPS标准。JoySSL优化总监表示，谷歌与百度等主流搜索引擎已明确将HTTPS视为重要的积极排名因素，因此，部署SSL证书不仅能改善自然搜索排名，还能吸引高质量的免费流量，为企业带来商机。</p><p><strong>构筑数字信任体系定义企业未来</strong></p><p>部署SSL证书早已超越了普通的IT支出范畴，是企业合规运营的基础，是防范风险的重要手段，是赢得用户信任的纽带，是提升企业竞争力，构筑信任体系的驱动器。面对充满不确定性与风险的数字环境，全面应用SSL证书，等同于为企业的数字化愿景塑造牢不可破的信任基石。</p>]]></description></item><item>    <title><![CDATA[阿里云云原生团队热招！欢迎加入 AI 工程化顶级赛场 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047578395</link>    <guid>https://segmentfault.com/a/1190000047578395</guid>    <pubDate>2026-01-28 17:03:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>我们是谁</h2><p>我们是中国最大云计算公司的基石——云原生应用平台。</p><ul><li>我们掌管着应用构建的核心命脉，孵化了 RocketMQ、Higress、Nacos、Dubbo 等多个世界级开源项目。</li><li>我们 SLS、Kafka 引擎每日处理来自亿级终端，百 PB 级数据量的应用数据，承载百万级实例应用，每日处理来自十万研发亿级的分析任务</li><li>我们为 Agent 应用提供手与脚与舞台，通过调度技术、数据工程、语义分析承载 万亿级Token 流量</li></ul><p>过去，我们定义了中国的云原生标准；现在，我们正在全面转向 AI，致力于打造 AI 时代的最强基础设施。</p><h2>我们在做什么</h2><p>我们不制造大模型，但我们让 AI 应用跑得更快、更稳、更便宜、更智能。我们正在寻找极客、架构师和算法专家，突破以下前沿领域：</p><ul><li>计算重构：从 K8s 到 Serverless AI， 打造异构算力与 Agent 执行的“新躯体”。</li><li>架构演进：从微服务到 Agent 互联，重新定义 AI 时代的网关与神经系统。</li><li>认知工程：从数据流到 Agent 记忆， 利用搜索与上下文技术，构建智能体的“海马体”与“感知层”。</li><li>智能治理：从监控到自动驾驶（AIOps）， 让基础设施具备自我进化的生命力。</li></ul><h2>我们需要你</h2><ul><li>对技术有极致的品味，渴望挑战内核级、高并发、分布式的世界级难题。</li><li>既有仰望星空的想象力（相信 AI 改变世界），又有脚踏实地的工程力（Code is Law）。</li><li>熟悉 Golang/Java/C++ 或 Python，对 Kubernetes、Serverless 或 AI 工程化有独到见解。</li></ul><p><a href="https://link.segmentfault.com/?enc=zw1x4HXCSA87JrnLzoeP8A%3D%3D.l2H42HhJRXCiwBM8NpRFi1tEpN5arjFlndhdnn3D%2Bly1aCift52NTauoPKaaDvlj" rel="nofollow" target="_blank">点击此处进入心仪岗位通道</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578397" alt="image" title="image"/></p>]]></description></item><item>    <title><![CDATA[达梦 & 人大金仓适配实战：SeaTunnel 在信创数据平台中的应用与踩坑总结 SeaTunnel]]></title>    <link>https://segmentfault.com/a/1190000047578410</link>    <guid>https://segmentfault.com/a/1190000047578410</guid>    <pubDate>2026-01-28 17:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578412" alt="Apache SeaTunnel 适配海报" title="Apache SeaTunnel 适配海报"/></p><p>作者 | 三线程序员<br/>Tags | MySQL Doris PG 达梦 金仓<br/>关键词 | SeaTunnel、DolphinScheduler、信创、国产、达梦、人大金仓</p><ul><li>适用版本：apache-seatunnel-2.3.8+、apache-dolphinscheduler-3.1.7、人大金仓8.6/9.x</li><li>预估阅读：10 min</li></ul><p>[toc]</p><h2>一、为什么要写这篇</h2><p>集团内部关于数据平台近期遇到了两次异构数据源的问题，洽好利用了开源工具简单应对，验证了自己目前工作的思路，正好总结一下分享过程中的收获也经验。以下只谈技术方案选择与经验分享，不讨论数据量、性能、安全等其它内容。</p><p>a) <strong>数据中转归集</strong>：现有数据平台需要将部分数据数据上报给行业平台，同时还要将另一条第三方物联数据做数据归集中转后再进行上报行业平台。。<br/>b) <strong>国产化信创可控切换</strong>：明年技术平台指标项信创切换的前期验证工作，需要验证业务系统与数据平台一体信创国产化信创切换风险验证，将现有 MySQL → 达梦 / 人大金仓 之间做迁移。</p><p>根据二三线城市实际公司和技术水平情况、调研了数据采集/集成项目后，暂定 Apache SeaTunnel 的核心原因：</p><ul><li>插件式架构，Source/Sink 支持 100+，新增国产库只需改 JDBC Driver；  考虑使用SeaTunnel 进行导入数据，同时考虑datax做为备用方案；（原则seatunnel支持自动建表，datax只支持导入无法自动建表，需要手动建表工作量较大。）</li><li>天然集成 DolphinScheduler，调度方便可观测性及管理运维易用性高；</li></ul><p>笔者在整个过程中趟了不少坑，经验在四五六节中进行了总结，因此成文，给社区回流经验，也作为内部复盘的内容。</p><h2>二、整体需求</h2><ol><li>利用 SeaTunnel 的 jdbc source和达梦专用sink实现数据数据上报，由于上报表比较多，需要利用seatunnel的自动建表和字段映射解决过程中兼容问题；</li><li>使用人大金仓数据库替换数据平台webDB和ds的调度持久化DB，同时验证seatunnel做为数据平台的数据采集模块的延伸方案(原有为doris jdbc catalog),读写kingbase数据库进行数据采集计算；</li></ol><h2>三、前置条件</h2><table><thead><tr><th>内容项</th><th>要求说明</th></tr></thead><tbody><tr><td>目标库</td><td>达梦数据库，人大金仓数据库 V8.6以上，账号具备 <code>SELECT, SHOW VIEW等</code> 权限</td></tr><tr><td>相关数据库jdbc驱动依赖jar包</td><td>connectors目录：connector-jdbc-2.3.12.jar                                                                                                                                                             lib目录：达梦DmJdbcDriver8.jar、金仓kingbase8-8.6.0.jar、mysql-connector-j-8.3.0.jar、postgresql-42.7.3.jar</td></tr></tbody></table><h2>四、安装测试运行</h2><p>有经验的朋友可直接跳过，本节主要介绍个人遇到的一些安装注意事项。</p><h3>1. 安装一个字，简单快捷。</h3><p>步骤：下载、解压、安装连接器、测试。（本人暂时只试用了自带的 Zeta 引擎，其它引擎和集群未使用，目前满足离线 ETL 常规需求）</p><p><strong>需要重点介绍一下安装连接器</strong>，如果网络不好或者maven懒得改代理、着急快速部署、验证新版本什么的，可以直接修改apache-seatunnel-2.3.8\config目录下的plugin_config文件，只保留需要的连接器；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578413" alt="image-20260121093448378" title="image-20260121093448378" loading="lazy"/></p><p>如我只连常用数据库就保留<code>connector-jdbc</code>，只连DDoris数据仓库就保留<code>connector-doris</code>其它的删除掉或注释掉。具体所需对照可以查看<code>\apache-seatunnel-2.3.8\connectors</code>目录下的<code>plugin-mapping.properties</code>文件，里面有详细的source和sink所需要对应的连接器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578414" alt="image-20260121093410783" title="image-20260121093410783" loading="lazy"/></p><p>配置好了直接运行脚本就可以了，进目录<code>cd apache-seatunnel-2.3.8/</code>安装命令<code>sh bin/install-plugin.sh 2.3.8</code>，不指定版本号注安装当前版本的；安装完毕，你的connector目录就会多出许多连接器jar包。老手熟的话可以不安装（panda哥就没用过），直接从原有安装机器或本地把下载好的连接器，手动传上去也可以正常运行。</p><p>这里有个神奇的情况，在Windows环境下有时候连接器历史下载过可能重新部署后没再次下载，仍然可以运行。但在某一个特定的时间点就又开始报错说缺少jdbc连接器。神奇的系统。</p><p><img referrerpolicy="no-referrer" src="" alt="img" title="img" loading="lazy"/></p><h3>2. 这里有两个概念需要理解一下</h3><p>一个是<strong>连接器：</strong> 既使用什么方式进行数据连接，常见的http、文件、数据库jdbc。（一般运行时报什么jdbc错误，八成是没下载jdbc连接器。install-plugin没？）</p><p>一个是<strong>驱动包：</strong> 特定数据源的连接驱动、常见的mysql、pg等。（一般运行时连接失败，九成是没放对应的数据库驱动。）驱动包要自己<strong>&lt;u&gt;手动扔啊，手动，手动&lt;/u&gt;</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578415" alt="image-20260121102558879" title="image-20260121102558879" loading="lazy"/></p><h3>3.测试demo</h3><pre><code># 切换工作目录至Apache SeaTunnel 2.3.8的安装目录
cd /opt/apache-seatunnel-2.3.8/

# 执行SeaTunnel批处理任务
# 参数说明：
# --config：指定任务配置文件路径，此处为默认的批处理配置模板
# -m local：指定运行模式为本地模式（无需集群环境）
./bin/seatunnel.sh --config ./config/v2.batch.config.template -m local</code></pre><p>运行时需要注意的就是windows命令行乱码，字符集换行符什么的这些问题，最一统的解决方案就是别直接windows的传linux上去混用，大不了重写或贴过去。<em>cmd运行时控制台中文信息乱码解决是 chcp 65001</em> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578416" alt="image-20260121102357285" title="image-20260121102357285" loading="lazy"/></p><p>⚠️ 再次提醒，无论运行什么类型的etl，涉及的连接器和驱动包要保证都有，报错时第一时间核对这个，不要死盯着报错重试了。特别是在Windows环境下，Linux大法还是好。</p><h3>4. 小分享</h3><p>作业文件习惯单独建一个job目录存放。（与DolphinScheduler集成有时间再写吧，欠的东西太多了。）</p><p>常用conf样例，可直接cv修改，注意Doris作为sink写入时使用的是streamload方式，要用对应的http端口，不是Navicat连接的端口（大年纪程序员经常忘）:</p><p>&lt;u&gt;mysql 2doris样例&lt;/u&gt;</p><pre><code>env {
    parallelism = 2
    job.mode = "BATCH"
}
source {
    Jdbc {
        url = "jdbc:mysql://192.168.0.31:3306/cons"
        driver = "com.mysql.cj.jdbc.Driver"
        connection_check_timeout_sec = 100
        user = "root"
        password = "123456"
        table_path = "cons.community_info"
        query = "select * from cons.community_info"
    }
}
sink {
    Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
        table = "ods_xyz_communityinfo_base"0
        sink.enable - 2pc = "true"
        sink.label - prefix = "test123"
        doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
}</code></pre><p>&lt;u&gt;mysql 2doris 多表样例&lt;/u&gt;（<strong>有复杂业务需要json一条数据变拆分成多行的可参考</strong> <a href="https://link.segmentfault.com/?enc=UMQfgYJcfUnnjcxjnxDAkg%3D%3D.RMFMeqbmKyjA2ZU1orN3SpkikCESFyyc4RyQznfo%2FyCqbB1xfud%2BrUBmImjhSflOVO2y3Zivx6R1SpVacqBeig%3D%3D" rel="nofollow" target="_blank"><strong>https://github.com/apache/seatunnel/issues/7961</strong></a> <strong>使用<strong><em><em>SELECT * FROM fake LATERAL VIEW OUTER EXPLODE(cpe_nodes) as cpe_nodes</em></em></strong>函数</strong>）</p><pre><code>env {
  job.mode = "BATCH"
  parallelism = 4
}
source {
  Jdbc {
    url = "jdbc:mysql://192.168.0.31:3306/cons"
    driver = "com.mysql.cj.jdbc.Driver"
    connection_check_timeout_sec = 100
    user = "root"
    password = "qianhe999"
    "table_list"=[
        {
            "table_path"="cons.gas_alarm_events"
        },
        {
            "table_path"="cons.gas_check_dispatch"
        }
    ]
    
}
sink {
  Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
    sink.enable - 2pc = "true"
        sink.label - prefix = "test123"
    table = "ods_xyz_${table_name}_base"
        doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
}</code></pre><p><em>自动建表模板doris版本</em></p><pre><code>save_mode_create_template = """CREATE TABLE IF NOT EXISTS ${database}.${table_name} (
${rowtype_primary_key},
${rowtype_fields},
decoded_project_description  STRING
)
ENGINE=OLAP
UNIQUE KEY (${rowtype_primary_key})
COMMENT '${comment}'
DISTRIBUTED BY HASH (${rowtype_primary_key})
PROPERTIES (
"replication_allocation" = "tag.location.default: 1",
"in_memory" = "false",
"storage_format" = "V2",
"disable_auto_compaction" = "false"
)"""</code></pre><p>&lt;u&gt;http接口2 Doris 样例&lt;/u&gt;（http的主要参考了git的文章 <a href="https://link.segmentfault.com/?enc=yZDftR2JFG4NUqXxTBk4PQ%3D%3D.%2FtzhtBf27dGf0ntbCiz22u7X3296a3HGNspm%2FKEy70ZY6cMREgCYHvKQdUQotJCQ" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/8431</a>）</p><pre><code>env {
  execution.parallelism = 2
  job.mode = "BATCH" 
  checkpoint.interval = 10000 
  }
source {
  Http {
    url = "http://192.168.0.1120:31907/biz-data-service/241211/ABC_o1_XYZ"
    method = "POST"
    format = "json"
    headers = {Accept="application/json",Content-Type="application/json;charset=utf-8"}
    body= "{\"params\":{\"branch\":\"长安区\"},\"size\":10,\"current\":1}"
    content_field = "$.data.records.*"
   schema = {
      fields {
  mc=string 
  dz=string 
  last_update=timestamp
  jd="decimal(20, 5)" 
  id :int
  mplx=string 
  wd=string 
        }
    }
  }
}
sink {
   Doris {
        fenodes = "192.168.0.110:8030"
        username = "root"
        password = "123456"
        database = "data_test"
        table = "ods_xyz_http_base"
save_mode_create_template = """CREATE TABLE IF NOT EXISTS ${database}.${table_name} (
cid bigint NOT NULL AUTO_INCREMENT(1) COMMENT '主键',
${rowtype_fields}
) ENGINE=OLAP
UNIQUE KEY (cid)
DISTRIBUTED BY HASH (cid) BUCKETS 1 
PROPERTIES (
"replication_allocation" = "tag.location.default: 1",
"in_memory" = "false",
"storage_format" = "V2",
"disable_auto_compaction" = "false"
)"""
  data_save_mode = "DROP_DATA" # 默认是追加，这里测试了一下清表。既每次只保留最新一次。
  sink.enable - 2pc = "true"
  sink.label - prefix = "test123"
  doris.config = {
            format = "json"
            read_json_by_line = "true"
        }
    }
 }</code></pre><h2>五、读Doris写达梦数据库操作步骤</h2><p>首先需要确保SeaTunnel能正常运行，需要在Linux服务器库或Windows命令行上进行验证，基础的SeaTunnel本地的Zeta引擎可以正常工作运行。</p><p>如用 <strong>Windows</strong>，可能会出现SeaTunnel今天能运行，明天不能运行的特殊情况（报错内容是“找不到或无法加载主类”）；我没有彻底解决，但在网上找的方案大部分都是java环境变量设置的情况，还有就是关掉命令窗口重新打开。但偶尔有机会确实再次出现，隔天就没事了。神奇的系统！</p><h3>1. 正常情况</h3><pre><code class="sql">-- 官方模板一次通
env {
  parallelism = 1
  job.mode = "BATCH"
}
source {
  Jdbc {
    url = "jdbc:dm://e2e_dmdb:5236"
    driver = "dm.jdbc.driver.DmDriver"
    connection_check_timeout_sec = 1000
    user = "SYSDBA"
    password = "SYSDBA"
    query = """select * from "SYSDBA".e2e_table_source"""
  }
}
sink {
  Jdbc {
    url = "jdbc:dm://e2e_dmdb:5236"
    driver = "dm.jdbc.driver.DmDriver"
    connection_check_timeout_sec = 1000
    user = "SYSDBA"
    password = "SYSDBA"
    query = """
INSERT INTO SYSDBA.e2e_table_sink (DM_BIT, DM_INT, DM_INTEGER, DM_PLS_INTEGER, DM_TINYINT, DM_BYTE, DM_SMALLINT, DM_BIGINT, DM_NUMERIC, DM_NUMBER,
 DM_DECIMAL, DM_DEC, DM_REAL, DM_FLOAT, DM_DOUBLE_PRECISION, DM_DOUBLE, DM_CHAR, DM_CHARACTER, DM_VARCHAR, DM_VARCHAR2, DM_TEXT, DM_LONG,
 DM_LONGVARCHAR, DM_CLOB, DM_TIMESTAMP, DM_DATETIME, DM_DATE, DM_BLOB, DM_BINARY, DM_VARBINARY, DM_LONGVARBINARY, DM_IMAGE, DM_BFILE)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
"""
  }
}</code></pre><h3>2.我跳的坑（读Doris写达梦，流水帐形式就当小说看）</h3><h4>2.1.SeaTunnel使用中遇到的问题</h4><h5>(1) 表或视图不存在</h5><p>手写query正常，动态却不行，<code>generate_sink_sql =true</code>不行。最终需要追加上数据库名称，而且源表是Doris表都是小写，而目标表是达梦表库表字段都是大写，所以会报表不存在。</p><h5>(2) 源与目标表名大小写方式不一致</h5><p>需要追加转换功能，字段大小写也需要转换。</p><p>表转换需要使用Transform，并且追加源表别名与目标表表别名，方便操作。</p><pre><code>transform {
 TableRename {
  plugin_input = "source_doris"
  plugin_output = "desc_dameng"
  convert_case = "UPPER"
  }
}</code></pre><p>字段转换</p><pre><code>sink {
 Jdbc {
  plugin_input = "desc_dameng"
  url = "jdbc:dm://dmhost:2070?schema=X_Y_Z_KFC"
  driver = "dm.jdbc.driver.DmDriver"
  user = "X_Y_Z_USE"
  password = "123456"
  database = "DAMENGKFC"
  table = "X_Y_Z_KFC.${table_name}"
  generate_sink_sql = true
  field_ide="UPPERCASE"
  schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
  data_save_mode="DROP_DATA"
   dialect ="Dameng"   --好像没啥实际意义（说是会根据jdbc连接串推算）
}
}</code></pre><h5>(3) 无法创建表</h5><p>低版本不支持达梦建表(2.3.8就没有达梦的ddl创建表的方法，达梦数据库sink写入时ddl自动建表方法是在2.10后才追加的)，最终升级到2.3.10并下载对应的connector连接包还是不行。</p><p>版本升级最新与更新最新的连接包（中间下载时间太长，使用了从maven上直接手动下载的包，最后对了一下大小都一样。没有问题。。。）又升级到了2.3.12版本也是无法自动建表。</p><p>Dialect方言显示指定也不行（包括相应的lib包也都加上了。。。还是不行，甚至怀疑过达梦的驱动包有问题，是否需要找商厂要个驱动才能用）。</p><p>最终验证与字段注释有关，表注释直接就扔了根本不建。只要表中有字段就报commont 语法解析问题。</p><h4>2.2. 两头走不通的折中方案</h4><h5>(1) 使用达梦迁移工具</h5><p>在测试环境没有问题，在生产环境Doris版本升级了竟然报读取错误了。。。(测试环境是Doris 2.1.3，生产是2.1.9版本，估计是哪有差异。)</p><h5>(2) 手动建表</h5><p>参考MySQL导入达梦，使用Linux的脚本处理dump的SQL脚本 。</p><p>把字段和表注释去除，使用AI处理了一上午，只能把表字段注释去掉。但是无法把字段注释和表注释单独弄到一个脚本中，只能生成一个所谓的纯净建表语句。</p><p>但突然发现导出的数据类型肯定在达梦中无法执行，需要转换。对应到各种不同的类型，这个对应再用手工做一遍，考虑放弃此方案。</p><h5>(3) 灵感闪现-直接删除表的注释</h5><p>通过schema_info直接用sql拼出一堆清注释的语句。</p><p>还有个小波折，差点这个方案也不能用了。就是Doris的默认值，开始转换时使用的SQL</p><p>ALTER TABLE <code>dwd_X_Y_base</code> MODIFY COLUMN <code>filedA1</code> varchar(255) COMMENT "";指定了字段类型，当有字段<code>last_update</code>有默认值时不允许修改。后来仔细查了查官网文档，Doris还是做了人的，有专门单独去注释的语句，不加字段类型就行了。</p><h4>2.3 最终可行方案-半手工方案</h4><p>利用现有的备份库，或者直接重新做个临时备份；思路就是通过备份库去把表建上，再切到正式库去做真实的数据同步。</p><h5>(1) 在备份库上把所有注释去了</h5><p>a. 选择表范围。（只取Xyz的底层数据表，用于分业务去做同步SeaTunnel拼哪些表做同步）</p><pre><code>SELECT   *  FROM   information_schema.TABLES 
WHERE   TABLE_SCHEMA = 'data_test_backup' 
AND ( TABLE_NAME like  'ods_xyz_%'  or TABLE_NAME like  'dwd_xyz%')  ;</code></pre><p>b. 选择去除字段注释的内容。</p><pre><code>SELECT 
  CONCAT('ALTER TABLE ', TABLE_SCHEMA,'.',TABLE_NAME, '  MODIFY COLUMN  ', COLUMN_NAME, '  ',  ' COMMENT "";') AS alter_sql
FROM 
  information_schema.columns 
WHERE 
  TABLE_SCHEMA = 'data_test_backup' 
  AND (table_name LIKE 'ods_xyz_%' OR TABLE_NAME LIKE 'dwd_xyz%')
  AND LENGTH(COLUMN_COMMENT) &gt; 0;</code></pre><p>c. 复制出清除字段脚本，在备份库上直接执行。</p><pre><code>alter table ods_xyz_1001_base modify column filed1 comment "";
alter table ods_xyz_1001_base modify column filed2 comment "";
.....</code></pre><p>ctrl+A 、 ctrl +R 全选运行。</p><h5>(2) 使用备份库把数据第一次自动建表导进去。</h5><p>新建<code>xyz_low1.conf</code>读取备份库建表初始化到达梦，未把多表改成正则去匹配，是方便调试找错。直接把表名扔给AI生成<code>table_path</code>数组，也方便以后做真增量时，直接在sql中追加限制条件。</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
 Jdbc {
  plugin_output = "source_doris"
  url = "jdbc:mysql://backuphost:9030/data_test_backup"
  driver = "com.mysql.cj.jdbc.Driver"
  connection_check_timeout_sec = 100
  user = "XXX"
  password = "******"
  table_list = [
 {
  table_path = "data_test_backup.dwd_xyz_1001_base"
  query = "select * from data_test_backup.dwd_xyz_1001_base"
 },
 {
  table_path = "data_test_backup.dwd_xyz_1002_base"
  query = "select * from data_test_backup.dwd_xyz_1002_base"
 },
 .....
]
}
}
transform {
 TableRename {
  plugin_input = "source_doris"
  plugin_output = "desc_dameng"
  convert_case = "UPPER"
  }
}
sink {
 Jdbc {
  plugin_input = "desc_dameng"
  url = "jdbc:dm://101.2.3.4:2026?schema=X_Y_Z_KFC"
  driver = "dm.jdbc.driver.DmDriver"
  user = "X_Y_Z_USE"
  password = "123456"
  database = "DAMENGKFC"
  table = "X_Y_Z_KFC.${table_name}"
  generate_sink_sql = true
  field_ide="UPPERCASE"
  schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
  data_save_mode="DROP_DATA"
  dialect ="Dameng"
}
}</code></pre><h5>(3) 切回同步从库直接读取最新数据</h5><p>复制备份库的配置文件，修改数据库ip地址端口密码等信息，就可直接运行了。</p><h5>(4) 同时拼出来在达梦里需要追加的语句</h5><p>追加表注释。</p><pre><code>SELECT   CONCAT('COMMENT ON TABLE ', upper(TABLE_NAME), ' IS ''', TABLE_COMMENT, ''';')  
FROM   information_schema.TABLES 
WHERE   TABLE_SCHEMA = 'data_test' 
AND (table_name LIKE 'ods_xyz_%' OR TABLE_NAME LIKE 'dwd_xyz_%')</code></pre><p>在达梦数据库上执行！把自动建表的表注释补出来。</p><p>有条件追加字段注释(当时任务急，未来可期你懂的)。</p><h5>(5) 加工层导入时的遇到的新问题(ads层字段创建不规范导致)</h5><p>个别语句有问题的需要调整修改一下，记得重新把先表删除了，再改配置文件中的内容。</p><pre><code>{
  table_path = "data_test_backup.ads_xyz_1001_agg"
  query = "select label_type,`sum(total)` as  'totalnum',sord_num from   data_test_backup.ads_xyz_1001_agg"
 },</code></pre><h4>2.4 配个定时就OK</h4><p>Windows、Linux直接脚本定时，或者集成ds进行配置可视化任务。</p><p>Windows下的bat脚本</p><pre><code>@echo off
rem 先切到 SeaTunnel 的 bin 目录
cd /d "E:\apache-seatunnel-2.3.12-bin\apache-seatunnel-2.3.12\bin"
rem 执行作业
seatunnel.cmd --config ./job/xyz_low.conf -m local</code></pre><h3>3. 写达梦数据库的总结</h3><p>通过学习达梦数据库，笔者发现它本身就是Oracle的魔改版本，有点像把PG和Oracle捏在了一起，加了个PG的Schema，语法全是Oracle的。笔者主要利用了SeaTunnel的自动建表功能，特别是字段类型映射转换节省了大量时间，但研究的时间也不短。</p><p>同时，笔者也发现了一些问题，比如表注释会丢弃，这些还好，反正就是一次性的事手动补一下，直接用sql生成一下脚本即可。</p><p>在报错调试方面，似乎由于线程的问题，会把同线程的其它表的无错的内容报成异常打印出来。</p><p>还有就是关于表结构的问题，要注意调试过程中手动删除表，因为默认使用的参数是<code>schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"</code>，如果表已经存在不会再创建表，容易造成建的表有问题，而发生奇怪的异常报错。</p><h3>4. 另一时间线</h3><p>后续平台又需要与第三方物联系统做数据对接，就直接利用了Doris stream load技术来实现，分享一下经验：最终需要将http接口外网暴露的地址是Doris的be端口，而非fe端口。</p><p>中间还验证了一个拉的方式，就是利用SeaTunnel的http连接器，去拉数据。这里有个小问题，就是需要做鉴权，有时间会再做个分享。（方法很low但验证可行。）</p><h2>六、读写人大金仓数据库操作步骤(信创)</h2><p>信创就是我人生的至暗时刻，刚经历了达梦又得弄Kingbase，但最终对自己个人成长还是有助力的，不说信创数据库怎么兼容的各种问题吧，在时下这个环境换个角度看，这可能就是一种“技术壁垒”。也没时间写内部技术文档了，直接从头回忆吧。</p><h3>1. 坑Kingbase初理解</h3><p>先说开放性kingbase至少比达梦强，官网给下载安装程序包，包括安装版和docker版本，还可以免费申请测试的授权证书，开发授权最多有1年的试用期。这一点就敞亮、局气。首先和身边的前同事（现在还是好朋友）打听了一下，他们之前试用过，大概就是Kingbase是个套壳，底层是pg，改了改几个函数，论开放性有个叫瀚高的更开放，基本没有魔改的，本着原生的一致进行了二开。</p><p>然后运维大哥三下五除二就把docker拉起来，高高兴兴选择了下MySQL模式，结果MySQL的驱动不能直接连，必须要用Kingbase的原生，中间省略各种问题，最终又装了一个pg模式的。</p><p><strong>概念就一个</strong>：Kingbase有多种兼容模式，mysql/pg/sqlserver什么的。。。理论上不考虑这个兼容模式用Kingbase原生的驱动肯定都能连接。如果知道具体的兼容模式，可以尝试用兼容的驱动连接。如pg模式直接用pg驱动就可以连接，但MySQL模式Navicat就不能用MySQL的驱动连接。</p><p>KSQL是Kingbase自己的连接工具，有必要也安装一个，它的驱动就是用的Kingbase原生的驱动。</p><h3>2. 预期设想</h3><p>听劝MySQL兼容模式不好用，咱就用底层原生的最稳定了，当年kmx直接用的Cassandra读的妥妥的好。那就准备用<strong>Kingbase的pg兼容模式做为源和目标了</strong>。</p><p>在SeanTunnel官方文档上查了一下，支持Kingbase，但是，但是，但是，只有部分类型兼容！又在技术群里圈了一下<strong>panda</strong>大佬交流了Kingbase的读写情况，收获良多，再次感谢！！！感谢！！！感谢！！！</p><p>jdbc:kingbase8的不归路就开始了....（这里source和sink的库都是Kingbase的pg模式）</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "com.kingbase8.Driver"
    url = "jdbc:kingbase8://192.168.0.31:4322/sourcedb"
    user = "kingbase"
    password = "123456"
   query = "select *  from source_user_detail"
   }
}
sink {
    jdbc {
        url = "jdbc:kingbase8://192.168.0.119:4322/targerdb"
        driver = "com.kingbase8.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
       database = targerdb
       table = public.target_user_detail
       #schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
      }
}</code></pre><p>一切顺利，不到“10”分钟就搞定了；当时测试的小遗憾是不能schema_save_mode自动建表。在交流群里吐槽了一下，也感谢迅哥儿和西门分享经验和想法！！</p><p>后来panda大佬要给Kingbase立flag说可以支持，我是测试了不行；panda佬说Kingbase是继承pg的代码都支持，还提醒嘱咐source不能用query，无法自动建表，<strong>要用<code>tabl_path</code>是个坑，让我记到文章里提醒大家</strong>，“造福更多使用者”。最终panda佬可能查了查源码确认了打脸，“Kingbase在建表那块没适配”，但这不是重点。</p><p><strong>重点是：“用pg连接器是可以地，如果你Kingbase本身是pg兼容模式 那可以用pg的，只要元数据检查能通过。那就换成pg驱动和配置试试”，结论就是“把kingbase的pg模式就当成jdbc的pg用”</strong>，而且可以自动建表等参数都能用了。<strong>“pg支持啥它支持啥”</strong>。</p><pre><code>source {
  Jdbc {
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://192.168.0.31:4322/sourcedb"
    user = "kingbase"
    password = "123456"
    table_path = "sourcedb.public.source_user_detail"
 }
}
sink {
    jdbc {
        url = "jdbc:postgresql://192.168.0.119:4322/targerdb"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = targerdb
        table = public.target_user_detail
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
     }
}</code></pre><p>满心欢喜的下班，一切都太顺利了。。。</p><h3>3. 突变大转折</h3><p>业务也要做信创准备，那帮子老古董就咬死了我这祖传代码就是MySQL，信创我也是用金仓的MySQL兼容模式！！！！！我还提前分享了验证结论，告诉他们推荐用pg模式，可是人家业务就是这么横，让我们换pg，我这完全不接受，我找领导去！！！！！可想而知的结果，弄不了就是你们技术不行。我这血压一下子就上去了，@#￥%……&amp;<strong>%……&amp;……&amp;</strong>（&amp;￥%<em>&amp;……（</em><em>（）￥%&amp;……</em>（）￥#￥%</p><h3>4. 背叛</h3><p>这Kingbase的兼容MySQL模式肯定是类型有问题啊，这可怎么办？赶紧找其它办法吧。网上找了有什么Datamover，DataX( 老家伙)，还有一直关注没用过的Tis赶紧弄过来试吧，时间紧任务重，Tis有docker版本，赶紧拉起来。试了一下还真行，点点点就弄好一张表！！！表也建上了数据也导过去了，挺好。</p><p>顺利吗....没过一会，说表的字段都是大写的，Kingbase默认是区分大小写的和pg一样。但是可以通过数据库初始化时指定，Docker下面指定那个参数是起不来的，运维大哥说只能填pg，填不了其它的。又是个两头堵死的情况，像不像达梦？。。。</p><p>简单看了看Tis底层用的DataX，建表语句可以自己修改字段名变小写，但是DataX的脚本不让改，直接拷出来在DataX上执行有问题，看不懂的错误。没时间了研究了。。。</p><h3>5. 赌一把</h3><p>晕晕忽忽一下午，压力大吃碳水多，感觉到压力与生活的影响了，就要自己调节。工作只是工作，还有生活。重新调整饮食，早上有时间还把家里的毛巾洗了洗，心情拉满去上班。</p><p><strong>来吧，再试一把老朋友SeaTunnel</strong>！还是老三样，connector重下，驱动重放，执行文件编码问题。一关一关过呗，MySQL兼容类型有问题，我先跳过那个字段直接写死几个列，先跑一把给给自己信心。</p><p><strong>注意环境有改变：</strong>192.168.0.31:4321 的Kingbase是MySQL兼容模式，192.168.0.119:4322是Kingbase的pg兼容模式。</p><p>所以source要用Kingbase的原生去读，字段转小写的问题，通过SQL先尝试解决，大力出奇迹，这些个牛马的事扔给AI弄；sink保留原来的pg也没事。</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "com.kingbase8.Driver"
    url = "jdbc:kingbase8://192.168.0.31:4321/kingbase"
    user = "kingbase"
    password = "123456"
  query = "SELECT `ID` AS id,`PARENT_ID` AS parent_id,`DICT_LABEL` AS dict_label,`DICT_VALUE` AS dict_value,...REMARK` AS remark FROM public.dict;"
  }
}
sink {
    jdbc {
      url = "jdbc:postgresql://192.168.0.119:4322/datatest"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = datatest
        table = data_test.dim_busi_dict_001
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
     }
}</code></pre><p>经过9*9=81次调试，一把过了。高高兴兴找运维大哥说成了成了，运维大哥问了一句“int型的怎么解决的”？我#$%&amp;&amp;,我绕过去了。。。。晕了忘了个干净。</p><p>信心有了，可现实就是这么冰冷，int类型转换失败....AI说指定source的表结构类型，不管用....sql转换类型也没试成功.....不行，服软，花点钱买Kingbase的产品吧。最多也就这样了。</p><p>Panda佬的那句"用pg连接器是可以地"，我又再次仔细理解了一下。是不是有什么没理解到？我用pg驱动读个Kingbase的MySQL兼容模式，再赌一把？</p><pre><code>env {
  parallelism = 2
  job.mode = "BATCH"
}
source {
  Jdbc {
    driver = "org.postgresql.Driver"
    url = "jdbc:postgresql://192.168.0.62:4321/kingbase"
    user = "kingbase"
    password = "123456"
   query = "SELECT * FROM public.dict;"
  }
}
sink {
    jdbc {
      url = "jdbc:postgresql://192.168.0.119:4322/datatest"
        driver = "org.postgresql.Driver"
        user = "kingbase"
        password = "123456"
        generate_sink_sql = true
        database = datatest
        table = data_test.dim_busi_dict_001
        field_ide="LOWERCASE"
        schema_save_mode = "CREATE_SCHEMA_WHEN_NOT_EXIST"
        data_save_mode="APPEND_DATA"
    }
}</code></pre><p>最后结局了肯定是过了，再tm不过这文章就没必要写这块了。后来拿Navicat直接连了一下Kingbase的MySQL兼容模式，也能连上。#￥%，原来是自己绕远了。</p><p>赶紧分享给群里的小伙伴，又和panda佬谈了体会，“那挺好啊”，原来世界真的很大，我们只在自己的井里。有些事只是自己没见过，但并不代表这个世界上没有。</p><p>​                                                                                                                                                                                                                               2026.1.21           三线程序员</p>]]></description></item><item>    <title><![CDATA[IPD 需求管理怎么做：从需求基线到CCB变更控制全流程 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047578432</link>    <guid>https://segmentfault.com/a/1190000047578432</guid>    <pubDate>2026-01-28 17:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>需求变更不可避免，真正拖垮交付的是“变更失控”。IPD 需求管理的核心，是把需求从“口头共识”升级为“受控资产”：用需求分层与追溯建立共同语言，用需求基线固化交付承诺，再用 CCB 把变更变成可决策的投资选择。本文给出一套可落地的全流程、角色机制与指标闭环，帮助组织稳节奏、降返工、提质量。</p><blockquote>本文关键词：IPD 需求管理、需求管理、需求变更、需求变更管理、需求基线（Requirements Baseline）、CCB（变更控制委员会）、变更控制（Change Control）、配置管理（Configuration Management）、影响分析、需求追溯矩阵（RTM）</blockquote><h2>为什么“需求没管住”，IPD节奏就一定会崩</h2><p>很多团队以为项目失控是“需求太多”。我更常见到的现实是：需求并不一定多，但“承诺”太轻——轻到可以被一句“这个很急”随时改写。</p><p>在研发现场，需求失控往往呈现为三个连锁反应（也是多数团队在搜索“需求变更怎么管”时真正想解决的问题）：</p><ul><li>没有基线：团队不知道“当前承诺交付的到底是哪一版”。需求列表在变，验收口径也在变，最后只能靠人记忆与拉扯。</li><li>没有统一决策机制：变更由“声音最大的人”决定，项目经理被迫在多个老板之间做“情绪路由”，而不是做项目控制。</li><li>没有影响评估：变更只讨论“要不要做”，很少讨论“会伤到哪里、要付出什么代价、是否有替代方案”。</li></ul><p>这三件事叠加时，IPD 强调的“跨职能并行”会从优势变成放大器：市场、产品、研发、测试、供应链同时在动，但缺少共同的“受控参照物”，于是每个环节都在用自己的版本理解需求。</p><p>从产品研发的经验看，越晚发现需求错误，返工常常越贵。</p><p>NASA 的研究给出过一个非常直观的量化视角：把“在需求阶段发现并修正一个需求错误”的成本定义为 1，若到设计阶段再发现，成本上升到 3–8；到制造/构建阶段 7–16；到集成测试 21–78；到运维阶段甚至可能达到 29 到 1500+。这类数据对硬软结合、集成验证成本高的行业尤其有解释力：系统越复杂，后期返工越容易引发链式成本。</p><p>但作为管理者，我们也要保持理性：并非所有软件项目都能稳定观测到“延迟一定更贵”的效应。成熟的做法不是迷信曲线，而是把重点放在：缩短反馈回路 + 建立变更治理机制上。</p><h2>IPD 需求管理的骨架：把需求纳入配置管理</h2><p>要把“需求变更”管得既稳又快，底层一定要借用系统工程成熟的方法：配置管理（Configuration Management, CM）。</p><ul><li>配置管理：把关键产物（需求、设计、接口、测试等）当作“配置项”，通过基线与变更控制保持一致性。</li><li>需求基线（Requirements Baseline）：在某一时点对“已达成一致的需求承诺”做冻结，作为后续变更评审的参照。</li><li>变更控制（Change Control）：对基线后的任何修改，按流程提出、评估、批准/否决、实施与验证。</li></ul><p>NASA 对“基线”的定义是在某一时点对配置项属性的“达成一致的描述”，并提供一个已知配置来处理后续变更；当前批准的基线会成为后续变更的依据。翻译成研发语言就是一句话：</p><p>只要你对交付结果负责，需求就必须从“讨论对象”变成“受控资产”。</p><p>我建议用“四件套”搭起 IPD 需求管理的骨架，并给出每件套的“最小可用标准（MVS）”，避免一上来就走向重流程。</p><h4>1）需求分层：把“想要什么”变成“必须满足什么”</h4><p>需求不分层，CCB 就会陷入“你说的需求不是我理解的需求”。至少要有三层共同语言：</p><ul><li>干系人/市场需求（Why）：目标人群、场景、价值假设、成功指标</li><li>系统/产品需求（What）：功能、性能、接口、合规/安全、约束条件</li><li>版本交付需求（How far / When）：本次版本范围、验收口径、不可延期项</li><li>最小可用标准（MVS）：一条进入版本承诺的需求，必须同时具备“范围描述 + 验收口径 + 关键约束”。否则它不是需求，是愿望。</li></ul><p>在实践里，建议把“需求分层 + 统一ID + 状态定义”直接固化到系统中——例如在 <a href="https://link.segmentfault.com/?enc=LGl9afGExoOIHujH7lnYcA%3D%3D.BCdG8IjF8gRXxvJbGYBjR8SerEtprLwvzTaYSjX3ZjTik4RcHziUp9pJt7kLUWpp" rel="nofollow" target="_blank">ONES Project</a> 里建立需求池、编写需求并自定义需求状态与属性，再把需求与任务规划进迭代，减少口头协商带来的歧义。</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnwjo" alt="ONES 支持把需求规划至迭代" title="ONES 支持把需求规划至迭代"/></p><p>典型失败模式（反例）：只冻结“要做什么”，但不冻结“验收怎么算完成”，你会得到一个现象：研发认为交付了，测试认为没通过，业务认为没达到预期——每个人都没错，但项目照样延期。</p><h4>2）追溯链：没有追溯，就没有“像样的影响分析”</h4><p>追溯不是为了“好看”，是为了让你在 CCB 上用证据说话：这条变更会影响哪些设计、接口、测试与交付承诺？</p><p>做追溯建议从“最短闭环”开始：需求ID → 设计/接口项 → 测试用例 → 验收结果。这条链跑通，影响分析就有了骨架；以后再逐步扩展到风险、合规、供应链与文档基线。</p><p>现场判断标准：</p><p>如果一条变更在 10 分钟内讲不清影响范围，不是“CCB没效率”，而是“追溯链不足以支撑决策”。</p><p>追溯链最容易“断”在测试与交付环节。像 <a href="https://link.segmentfault.com/?enc=1oLTf3KyufF4u2rTXM01dw%3D%3D.iUNcI7FAcv4duMv0vr31tlTYip5Xx1cCgrqTO7zEXdMLd6WY3Ha6wH2Qg%2B%2FEQJfT" rel="nofollow" target="_blank">ONES TestCase</a> 支持测试用例与需求、任务关联、测试计划与迭代关联，能把“需求—任务—测试—缺陷”这条链更稳地串起来。</p><p><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdnNt9" alt="ONES 需求跟踪矩阵" title="ONES 需求跟踪矩阵" loading="lazy"/></p><h4>3）需求基线：冻结的不是文档，是“交付承诺”</h4><p>很多组织把基线做成“需求列表冻结”，但真正该冻结的是交付承诺：范围、验收口径、关键约束、里程碑假设。因为基线本质是“对某一时点状态的达成一致描述”，并作为后续变更的处理依据。</p><p>你可以把“需求基线包（Baseline Package）”理解成：</p><p><strong>一次版本对业务、对组织、对客户的正式承诺。</strong></p><p>基线包不是只存在于PPT。在版本/迭代层面把承诺落到系统里，后续才好做偏差对比。比如 ONES Project 在实践案例里强调了产品版本与迭代规划；并且在甘特图场景下提供“基线对比”的思路，用来直观看当前与计划偏差。</p><p><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdnNua" alt="ONES 支持基线对比" title="ONES 支持基线对比" loading="lazy"/></p><h4>4）CCB 变更控制：把变更从“情绪”变成“投资决策”</h4><p>成熟组织不会纠缠“要不要做变更”，而是讨论三个更硬的问题：</p><ul><li>批准的收益与代价是什么？</li><li>不批准的后果是什么？（很多时候这才是关键）</li><li>有没有折中方案：延期、降级、分阶段、替代实现？</li></ul><h2>全流程落地：从需求基线到 CCB 变更控制</h2><p>下面给出一套端到端流程。建议 PMO 或系统工程牵头固化为 SOP，并在两到三个版本内跑出稳定节奏。</p><blockquote>摘要版<br/>需求入口 → 需求分解与追溯 → 建立需求基线 → 变更申请（CR）→ 影响分析 → CCB 决策 → 执行验证 → 更新基线与追溯 → 关闭变更单</blockquote><h4>1. 需求入口：先把“入口”管住，后端才不会靠吵架控风险</h4><p>目标：统一入口、统一信息质量，把“讨论成本”前移。</p><p>建议设“入库门槛”（最少字段）：</p><ul><li>来源与目标用户/场景</li><li>价值假设（可量化更好：收入、成本、风险暴露、合规罚则）</li><li>验收口径（DoD：什么算交付完成）</li><li>关键约束（法规、接口、性能、交付窗口）</li><li>初步优先级与紧急性（规则要写清楚）</li></ul><p>常见误区：入口不清晰时，变更会伪装成“补充说明”“临时插单”，绕开治理机制。最后你会发现：CCB不是“变更太多开不过来”，而是“该进CCB的变更从来没进来”。<br/>入口治理的关键是“字段齐全 + 状态可控”。例如在 ONES Project 中，你可以把变更申请作为一种工作项/表单来收敛入口信息，同时利用其“需求池 + 自定义需求状态/属性”的机制，减少信息缺失导致的反复打回。</p><h4>2. 需求分解与追溯：先把“结构化依据”建起来</h4><p>目标：让影响分析可计算、可复核、可追责。</p><p>落地要点：</p><ul><li>每条需求唯一 ID，避免“同名不同义”</li><li>需求拆分以“可验证、可交付”为原则：大需求拆到能被测试与验收</li><li>建立最小追溯链：需求 → 设计/接口 → 测试 → 验收</li><li>对关键需求标注：合规/安全点、关键性能指标、供应链影响</li></ul><p>补一句经验：追溯不是一次性工作，它是“把承诺变成资产”的成本。你付出维护成本，换来的是后期影响分析的确定性与决策效率。另外，追溯链维护最怕“各写各的”。像 ONES TestCase 明确支持用例与需求、任务关联，并把测试计划与迭代关联，能把追溯从“Excel表”推进到“过程资产”。</p><h4>3. 建立需求基线：用“基线包”把承诺讲清楚</h4><p>目标：明确“我们承诺交付什么”，并建立后续变更的参照物。</p><p>基线包建议包含（这份清单本身就是很强的检索与引用片段）：</p><ul><li>基线需求清单（范围、优先级、验收口径、依赖）</li><li>关键接口与约束清单</li><li>里程碑与交付节奏（把范围与计划绑定）</li><li>风险清单与缓冲策略（范围缓冲/资源缓冲/技术预研）</li></ul><p>NASA 强调：基线提供一个已知配置来处理后续变更，当前批准的基线是后续变更的依据。管理动作落地：从这一刻起，任何改动都必须留下“为什么改、谁批准、改了什么、影响如何、如何验证”。</p><p>基线包建议同时“文档化 + 结构化”。文档化用于解释口径与边界，结构化用于后续对比与追踪。比如 ONES Project 与 ONES Wiki 支持“文档关联任务/工作项”，适合把基线包的关键结论与对应需求、迭代绑定起来，减少“决策在群里、执行在系统里、复盘找不到证据”的割裂。</p><h4>4. 变更申请：把“口头插单”变成“可评审的请求”</h4><p>目标：让变更带着信息来，而不是带着情绪来。</p><p>变更单（CR/SCR）最低要素建议包含：</p><ul><li>变更内容（新增/删除/修改）与动因</li><li>关联需求 ID 与基线版本号</li><li>紧急性与业务窗口（是否不可错过）</li><li>初步影响：范围/进度/成本/质量/风险</li><li>备选方案：延期/降级/分阶段/替代实现</li><li>不批准的后果：风险、合规、客户承诺、商业损失</li></ul><p>这一步的本质，是把“我想要”变成“我愿意为代价买单的选择”。变更单最有价值的不是“提交”，而是“字段强约束”。在 ONES Project 中，通过自定义需求状态与属性，可以把“影响分析一页纸”所需的关键字段前置到变更申请阶段，减少 CCB 会议上临时补材料。</p><h4>5. 影响分析：CCB 能不能开好，取决于这一页纸</h4><p>目标：把“要不要做”变成“值不值得做、怎么做更划算”。</p><p>建议用“一页纸影响分析”，强制输出：</p><ul><li>范围影响：涉及哪些需求 ID、交付物、接口</li><li>进度影响：关键路径是否改变，里程碑推迟多少</li><li>成本/资源影响：人天、外采、测试资源、供应链</li><li>质量影响：回归测试范围、缺陷风险、技术债</li><li>风险与安全：合规、安全、可靠性是否受影响</li><li>不批准的后果：推迟/拒绝会带来什么损失或风险</li></ul><p>一句话点破：影响分析不是“把风险写出来就安全了”，而是帮助组织做取舍：这次我们愿意买哪一种代价。</p><p>影响分析要快、要准，离不开“需求—任务—测试—缺陷”的数据贯通。ONES Project 提到与 TestCase 数据互通、并支持一键提 Bug，这类能力能让你在评估质量与回归范围时不至于全靠经验猜。</p><h4>6. CCB决策：用机制替代“拍脑袋”，用章程替代“临时拉群”</h4><p>目标：让组织用同一套规则做取舍，并且决策可复盘。</p><p>建议把 CCB 做成“有章程的治理机制”，至少明确：</p><ul><li>成员构成与表决权：业务、研发、测试、架构/系统工程、质量/合规</li><li>授权阈值：哪些变更项目级 CCB 可决，哪些必须上升到更高层级</li><li>节奏与通道：常规变更走周例会；紧急变更走快速通道但必须补齐记录；小变更按阈值授权给项目经理/产品负责人</li></ul><p>一次高效 CCB 建议做到“三定”：</p><ul><li>定级：紧急 / 常规 / 优化（不同通道、不同 SLA）</li><li>定策：批准、否决、退回补充、进入研究队列</li><li>定责：谁执行、谁验证、谁更新基线、何时关闭</li><li>CCB 开不动的三种典型原因（增强“经验信号”）</li><li>材料不全：变更没有“一页纸影响分析”；</li><li>参照物缺失：没有明确的需求基线版本；</li><li>授权不清：谁能拍板不清晰，会议只能“讨论”，无法“决定”。</li></ul><p>CCB 会议的“决定”一定要变成“可复盘的组织记忆”。ONES Project 明确提到与 ONES Wiki 的协同：文档可以关联任务/工作项。你可以把“CCB 决策纪要、否决原因、替代方案”沉淀在 Wiki，并回链到对应变更单，下一次再出现类似变更，组织就不会重复交学费。</p><h4>7. 执行与闭环</h4><p>目标：防止“会上通过了，现场没变；或者现场变了，组织失忆”。</p><p>落地动作建议固定成三步闭环：</p><p>1）实施与验证：研发实现、自测、测试回归、验收确认；<br/>2）更新配置项：更新需求基线版本号、更新追溯链（RTM）；<br/>3）关闭变更单：记录决策理由与验证证据，沉淀可复盘信息。</p><h2>常见问题 FAQ：</h2><p><strong>Q1：需求基线到底“冻结什么”？</strong><br/>冻结的是“交付承诺”：范围 + 验收口径 + 关键约束 + 里程碑假设，而不只是需求列表。基线要能成为后续变更评审的参照。</p><p><strong>Q2：CCB 一定要很大、很正式吗？</strong><br/>不一定。关键不是规模，而是“章程 + 授权阈值 + 可复盘记录”。小团队也可以做“小型 CCB”，用阈值把小变更下放，把大变更拉上来。</p><p><strong>Q3：影响分析写不出来怎么办？</strong><br/>优先补追溯链：没有需求 ID、接口项、测试用例的对应关系，就很难评估影响。先从“最短闭环追溯”做起，逐步完善；工具层面也建议把“需求—任务—测试用例”的关联关系固化起来。</p><p><strong>Q4：紧急变更怎么处理才不破坏治理？</strong><br/>给紧急变更单独通道：先快速决策、快速止损，但必须补齐“事后记录 + 基线更新 + 验证证据”，否则紧急会变成常态。</p><p>成熟的 IPD需求管理 不是“把流程写得更细”，而是把组织能力做得更强：</p><ul><li>用分层与追溯，让影响分析有据可依；</li><li>用需求基线把交付承诺冻结成“受控资产”（基线是后续变更处理的依据）。</li><li>用 CCB 把变更变成可决策的投资，并通过“实施验证—基线更新—记录闭环”形成组织记忆与复用能力。</li></ul><p>最后再强调一句：变更永远会来。真正的差距不在于谁能“减少变更”，而在于谁能把变更变成组织的可控能力——这才是 IPD 体系建设最硬的底座。</p>]]></description></item><item>    <title><![CDATA[MindSpore ：动静图融合的低代码高性能实践 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047577992</link>    <guid>https://segmentfault.com/a/1190000047577992</guid>    <pubDate>2026-01-28 16:10:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在边缘计算、车载终端等异构硬件场景下，MindSpore 模型部署面临 <strong>“动态调试灵活度” 与 “静态推理性能” 无法兼顾 </strong>、硬件算子适配性差两大核心痛点。本次分享基于 MindSpore 的jit动态编译特性与异构硬件算子重写机制，构建 “动静图混合执行 + 硬件感知算子优化” 的低代码部署方案，实现模型在 CPU/GPU/Ascend/ARM 等多平台的高性能适配 —— 推理延迟降低 65%，代码量减少 40%，同时保留动态图的灵活调试能力，附全流程部署代码与跨平台性能对比。</p><h2>1. 动静图混合执行的精细化控制：调试与性能的平衡</h2><p>场景：动态图（PyNative Mode）支持实时打印中间张量、断点调试，适合模型迭代阶段；静态图（Graph Mode）通过计算图优化实现高性能推理，但调试成本高。传统部署需在两种模式间反复切换，且无法针对不同模块差异化配置。</p><p>MindSpore 技术实践：</p><p>利用jit装饰器的局部编译特性，对模型的高频推理模块做静态编译优化，对低频调试模块保留动态执行能力，同时通过input_signature限制输入形状，避免静态编译的形状敏感问题：</p><pre><code class="python">import mindspore as ms
import mindspore.nn as nn
import mindspore.ops as ops
from functools import partial

ms.set_context(mode=ms.PYNATIVE_MODE)  # 全局开启动态图

# 1. 动态调试模块：保留动态执行能力，用于异常检测
class DynamicDebugModule(nn.Cell):
    def __init__(self, debug=True):
        super().__init__()
        self.debug = debug
        self.norm = nn.BatchNorm2d(64)

    def construct(self, x):
        x = self.norm(x)
        if self.debug and ms.get_context("mode") == ms.PYNATIVE_MODE:
            # 动态打印张量形状与均值，辅助调试
            print(f"Debug: tensor shape={x.shape}, mean={ops.mean(x).asnumpy()}")
        return x

# 2. 静态推理模块：用jit装饰器做局部编译优化
@ms.jit(input_signature=(ms.Tensor(shape=[None, 64, 32, 32], dtype=ms.float32),))
def static_infer_block(x):
    """高频推理模块：卷积+残差连接，静态编译优化"""
    conv1 = nn.Conv2d(64, 128, 3, padding=1)
    conv2 = nn.Conv2d(128, 128, 3, padding=1)
    res = conv1(x)
    res = ops.relu(res)
    res = conv2(res)
    return res + x  # 残差连接

# 3. 动静融合的完整模型
class HybridModel(nn.Cell):
    def __init__(self):
        super().__init__()
        self.debug_module = DynamicDebugModule()
        self.static_block = partial(static_infer_block)  # 封装静态模块
        self.classifier = nn.Dense(128*32*32, 10)

    def construct(self, x):
        x = self.debug_module(x)  # 动态执行：调试
        x = self.static_block(x)  # 静态执行：高性能推理
        x = x.reshape(x.shape[0], -1)
        x = self.classifier(x)
        return x

# 效果：动态模块保留调试能力，静态模块推理延迟降低50%；相比全静态图，调试效率提升3倍</code></pre><h2>2. 异构硬件算子重写：针对硬件架构的性能优化</h2><p>场景：MindSpore 默认算子在通用硬件上表现均衡，但在专用架构（如 ARM 的 NEON 指令集、Ascend 的 AI Core）上未充分发挥硬件算力 —— 例如 ARM 端的卷积算子，默认实现未利用向量并行计算，推理效率仅为硬件峰值的 30%。</p><p>MindSpore 技术实践：</p><p>基于mindspore.ops.Custom实现硬件感知的算子重写，针对不同硬件平台注册差异化的算子实现，同时通过PrimitiveWithInfer完成算子的形状推导，确保与 MindSpore 计算图兼容：</p><pre><code class="python">from mindspore.ops import Custom, PrimitiveWithInfer
from mindspore._c_expression import typing

# 1. 定义硬件感知的卷积算子（以ARM NEON为例）
class ARMCustomConv2d(PrimitiveWithInfer):
    @prim_attr_register
    def __init__(self, in_channels, out_channels, kernel_size):
        super().__init__(name="ARMCustomConv2d")
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size

    def infer_shape(self, x_shape):
        # 推导输出形状：same padding
        h, w = x_shape[2], x_shape[3]
        return (x_shape[0], self.out_channels, h, w)

    def infer_dtype(self, x_dtype):
        return x_dtype

    def get_func(self):
        # 绑定ARM NEON优化的卷积实现（C++编写，通过MindSpore C API调用）
        def neon_conv2d(x, weight, bias):
            from arm_neon_conv import neon_conv2d_impl  # 自定义NEON加速库
            return neon_conv2d_impl(x.asnumpy(), weight.asnumpy(), bias.asnumpy())
        return neon_conv2d

# 2. 硬件算子注册与适配
def get_conv2d(in_channels, out_channels, kernel_size, device_target):
    """根据硬件平台返回最优算子"""
    if device_target == "ARM":
        return Custom(
            ARMCustomConv2d(in_channels, out_channels, kernel_size),
            out_shape=ARMCustomConv2d(in_channels, out_channels, kernel_size).infer_shape,
            out_dtype=ARMCustomConv2d(in_channels, out_channels, kernel_size).infer_dtype
        )
    else:
        # 其他平台使用默认卷积算子
        return nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)

# 3. 模型集成硬件感知算子
class HardwareAwareModel(nn.Cell):
    def __init__(self, device_target):
        super().__init__()
        self.conv = get_conv2d(3, 64, 3, device_target)
        self.relu = nn.ReLU()

    def construct(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x

# 效果：ARM平台卷积算子推理速度提升2.8倍，硬件算力利用率从30%提升至75%</code></pre><h2>3. 低代码跨平台部署：MindIR 导出 + Lite 推理的自动化流程</h2><p>场景：模型部署需经历 “训练→导出→量化→推理” 多步骤，不同平台的部署流程差异大，手动配置繁琐且易出错；同时端侧设备资源有限，需对模型做轻量化处理。</p><p>MindSpore 技术实践：</p><p>基于 MindSpore 的MindIR 统一模型格式，封装 “训练→导出→量化→部署” 的自动化脚本，同时集成后训练量化（PTQ）与算子融合优化，实现一键跨平台部署：</p><pre><code class="python">import mindspore.lite as mslite
from mindspore.compression import QuantizationAwareTraining

# 1. 模型训练与轻量化（PTQ量化）
def train_and_quantize(model, train_dataset, device_target):
    # 训练模型（省略训练循环）
    loss_fn = nn.CrossEntropyLoss()
    opt = nn.Adam(model.trainable_params(), 1e-3)
    train_net = nn.TrainOneStepCell(model, opt, loss_fn)

    # PTQ量化：降低模型体积与推理延迟
    quant_config = QuantizationAwareTraining(quant_dtype=ms.int8)
    quant_model = quant_config.quantize(model)
    # 用校准数据集微调（100样本）
    calib_dataset = train_dataset.take(100)
    for x, _ in calib_dataset:
        quant_model(x)
    return quant_model

# 2. 一键导出MindIR模型
def export_mindir(model, input_shape, export_path):
    input_tensor = ms.Tensor(shape=input_shape, dtype=ms.float32)
    ms.export(model, input_tensor, file_name=export_path, file_format="MINDIR")

# 3. 跨平台推理部署
def deploy_lite(model_path, device_target, input_data):
    # 初始化Lite推理环境
    context = mslite.Context()
    if device_target == "CPU":
        context.target = ["cpu"]
        context.cpu.thread_num = 4
    elif device_target == "GPU":
        context.target = ["gpu"]
    elif device_target == "ARM":
        context.target = ["cpu"]
        context.cpu.thread_num = 2  # 适配ARM端算力

    # 加载模型并推理
    model = mslite.Model(model_path, context=context)
    inputs = [mslite.Tensor.from_numpy(input_data)]
    outputs = model.predict(inputs)
    return outputs[0].asnumpy()

# 自动化部署流程调用
if __name__ == "__main__":
    device_target = "ARM"  # 可切换为CPU/GPU/Ascend
    model = HardwareAwareModel(device_target)
    # 训练量化
    quant_model = train_and_quantize(model, train_dataset, device_target)
    # 导出MindIR
    export_mindir(quant_model, [1, 3, 224, 224], "hardware_aware_model")
    # 端侧推理
    input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
    result = deploy_lite("hardware_aware_model.mindir", device_target, input_data)</code></pre>]]></description></item><item>    <title><![CDATA[2025年CRM系统选型手册：主流厂商能力横向对比及深度解析 晨曦钥匙扣 ]]></title>    <link>https://segmentfault.com/a/1190000047578004</link>    <guid>https://segmentfault.com/a/1190000047578004</guid>    <pubDate>2026-01-28 16:09:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在数字化转型背景下，CRM（客户关系管理）已从“工具”升级为“企业增长引擎”。其核心价值在于通过<strong>标准化流程</strong>提升效率、<strong>全视图客户理解</strong>驱动个性化运营、<strong>移动化能力</strong>适配外勤场景、<strong>数据驱动</strong>优化绩效。本文选取8个主流CRM品牌（超兔一体云、Salesforce、SAP、Microsoft Dynamics 365、Zoho、Freshsales、红圈营销、EC），从四大核心维度展开深度对比，为企业选型提供参考。</p><h2>一、销售流程标准化：从“经验驱动”到“流程驱动”</h2><p>销售流程标准化的核心是<strong>用统一规则替代个人经验</strong>，减少无效动作，提升转化率。其关键指标包括：自定义能力、自动化程度、行业适配性、系统集成度。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：聚焦中小企“多场景跟单”痛点，提供<strong>三大固定模型</strong>——小单快单用“三一客”（三定：定性、定级、定量+关键节点推进）、中长单用“商机跟单”（阶段+预期日期）、多方项目用“多方项目模型”。同时支持<strong>订单</strong> <strong>工作流</strong> <strong>标准化</strong>（锁库、采购计划、供应商直发），流程易落地，适合中小企快速复制高效动作。</li><li><strong>Salesforce</strong>：大企级自定义能力，通过<strong>销售流程构建器</strong>完全自定义漏斗阶段（如“线索→MQL→SQL→商机→成交”），搭配<strong>工作流</strong> <strong>规则</strong>（如“线索评分≥80分自动分配给高级销售”），Einstein AI自动触发任务提醒（如“客户3天未跟进需发送邮件”），实现全流程自动化校验。</li><li><strong>SAP</strong>：依托<strong>ERP-CRM一体化优势</strong>，覆盖14种标准销售场景（跨公司销售、寄售、服务销售等），支持<strong>自定义审批流</strong>（如“订单金额≥10万需财务审批”），实现“订单-生产-交付”全链路流程打通，适合中大型制造企业。</li><li><strong>红圈营销</strong>：针对<strong>快消</strong> <strong>/农牧/服装</strong>等外勤高频行业，提供<strong>标准化拜访流程</strong>（路线规划→到店签到→陈列检查→库存盘点→订单提交→问题反馈），任务自动派发，解决“漏店、虚假拜访、流程不统一”痛点。</li><li><strong>EC</strong> <strong>（六度人和）</strong> ：聚焦<strong>电话销售场景</strong>，提供<strong>流程模板</strong>（开场→需求挖掘→产品介绍→异议处理→促成）、智能拨号（自动过滤空号）、通话录音（复盘话术）、话术库（优秀销售话术共享），快速复制电销精英的成交逻辑。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>自定义能力</th><th>自动化程度</th><th>行业适配性</th><th>集成度</th></tr></thead><tbody><tr><td>超兔一体云</td><td>三大跟单模型、订单工作流、线索一键处理</td><td>中（固定模型内调整）</td><td>中（自动提醒+分配）</td><td>中小企全行业</td><td>中（支持常用工具）</td></tr><tr><td>Salesforce</td><td>自定义漏斗、工作流规则、Einstein自动化</td><td>高（完全自定义）</td><td>高（自动触发任务/校验）</td><td>大企全行业</td><td>高（与ERP/HR/营销集成）</td></tr><tr><td>SAP</td><td>ERP-CRM一体化、14种标准场景、自定义审批流</td><td>中（基于标准场景扩展）</td><td>中（自动化销售任务）</td><td>中大型制造/零售</td><td>高（与SAP ERP深度集成）</td></tr><tr><td>红圈营销</td><td>行业标准化拜访流程、路线规划、任务自动派发</td><td>低（行业固定流程）</td><td>中（自动任务分配）</td><td>快消/农牧/服装</td><td>中（与内部系统集成）</td></tr><tr><td>EC</td><td>电销流程模板、智能拨号、通话录音、话术库</td><td>中（电销流程自定义）</td><td>高（自动线索分配+跟进提醒）</td><td>电销/外勤行业</td><td>高（与微信/企业微信集成）</td></tr></tbody></table><h3>3. 超兔销售流程标准化流程图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578007" alt="" title=""/></p><h3>4. 维度总结</h3><ul><li>中小企快速落地：选超兔（固定模型易操作）；</li><li>大企自定义需求：选Salesforce（完全自定义+AI自动化）；</li><li>制造/零售ERP协同：选SAP（全链路流程打通）；</li><li>快消/农牧外勤：选红圈营销（行业标准化流程）；</li><li>电销团队：选EC（流程模板+智能拨号）。</li></ul><h2>二、客户全视图管理：从“碎片化数据”到“360度画像”</h2><p>客户全视图管理的核心是<strong>整合多源数据</strong>，构建“人-货-场”统一画像，支撑个性化运营。其关键指标包括：数据整合范围、实时性、画像深度、权限管理。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：侧重<strong>数据准确性+权限安全</strong>——支持<strong>个性化配置</strong>（用户画像、客户表布局、列表自定义），自动补全工商信息（天眼查/百度查公司）、手机号查重（模糊匹配简称），数据权限按角色隔离（销售看客户详情、财务看财务数据、老板看全局），适合注重数据安全的中小企。</li><li><strong>Salesforce</strong>：通过<strong>Customer 360平台</strong>整合销售、服务、营销、ERP多部门数据，Einstein GPT自动生成<strong>客户需求预测</strong>（如“客户浏览过产品A，可能需要配件B”）和<strong>个性化沟通话术</strong>（如“针对制造业客户的成本痛点，推荐套餐C”），实现“数据-策略-执行”闭环。</li><li><strong>SAP</strong>：依托ERP协同，构建<strong>全链路客户视图</strong>——整合客户基本信息、订单数据（销量/金额）、信用风险（逾期记录）、满意度（售后评分），与生产系统联动（如“客户订单触发生产计划”），适合中大型企业“从销售到交付”的全链路管理。</li><li><strong>红圈营销</strong>：聚焦<strong>外勤场景数据</strong>——整合客户地理信息（工商地址经纬度）、消费偏好（购买历史/ SKU偏好）、工作记录（拜访次数/反馈问题），生成“地理+行为”画像，支持“按区域推送促销活动”“针对偏好推荐产品”。</li><li><strong>EC</strong>：整合<strong>多渠道沟通数据</strong>——自动记录电话（通话录音/时长）、微信（聊天记录/朋友圈互动）、邮件（打开/点击）的互动历史，生成“客户互动时间线”，支持“根据沟通历史调整话术”（如“客户上周提到价格敏感，本次重点讲优惠”）。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>数据整合范围</th><th>实时性</th><th>画像深度</th><th>权限管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>个性化配置、工商查重、角色权限隔离</td><td>中（线索+客户+订单+工商）</td><td>高（实时同步）</td><td>中（基础+行为+价值）</td><td>严格（同级隔离/上级查看）</td></tr><tr><td>Salesforce</td><td>Customer 360、Einstein GPT、多系统集成</td><td>高（全渠道+内部系统）</td><td>高（实时同步）</td><td>高（基础+行为+预测）</td><td>灵活（九级组织权限）</td></tr><tr><td>SAP</td><td>ERP协同全链路、信用风险/满意度分析</td><td>高（ERP+CRM+服务）</td><td>高（实时同步）</td><td>高（全链路数据）</td><td>严格（与ERP权限一致）</td></tr><tr><td>红圈营销</td><td>地理信息、消费偏好、拜访记录</td><td>中（线下+消费+位置）</td><td>中（实时录入）</td><td>中（行为+地理）</td><td>中（角色权限）</td></tr><tr><td>EC</td><td>多渠道沟通记录、互动时间线、标签化管理</td><td>中（沟通+互动）</td><td>高（实时同步）</td><td>中（行为+沟通）</td><td>中（角色权限）</td></tr></tbody></table><h3>3. 客户全视图管理能力框架脑图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578008" alt="" title="" loading="lazy"/></p><h3>4. 维度总结</h3><ul><li>中小企数据安全：选超兔（查重+权限隔离）；</li><li>大企个性化运营：选Salesforce（Customer 360+Einstein预测）；</li><li>制造企业全链路：选SAP（ERP协同全视图）；</li><li>外勤地理场景：选红圈营销（地理+消费偏好）；</li><li>多渠道沟通：选EC（沟通记录整合）。</li></ul><h2>三、高效移动办公/销售外勤：从“线下记录”到“实时同步”</h2><p>移动办公/外勤场景的核心是<strong>适配“在路上”的工作状态</strong>，实现“数据实时录入+任务实时处理+协同实时同步”。其关键指标包括：移动端功能完整性、离线支持、外勤适配性、协同能力。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：移动端聚焦“销售全流程”——支持多渠道新建客户、公海分配、三一客价值标定，“快目标”模块分解目标到个人（如“本月需跟进20个目标客户”），外勤签到（500米内客户签到），数据实时同步云端，适合中小企外勤人员快速操作。</li><li><strong>Salesforce</strong>：移动端功能全面——支持GPS打卡、语音/拍照录入拜访记录（如“拍客户仓库照片，备注库存不足”）、实时查看客户资料/待办任务，集成Chatter协作（可@同事附件照片/文件），支持离线操作（无网络时录入，联网后自动同步），适合大企外勤团队协同。</li><li><strong>Microsoft Dynamics 365</strong>：依托微软生态，移动端与Teams/Outlook深度集成——支持路线规划（按客户位置优化拜访顺序）、客户位置标注（在地图上显示客户分布）、实时同步邮件/会议纪要（如“Outlook会议自动关联客户档案”），适合微软生态企业。</li><li><strong>红圈营销</strong>：外勤场景“强适配”——支持离线数据录入（无网络时记录拜访信息）、客户位置定位（导航到店）、现场订单提交（直接录入系统触发生产）、路线规划（自动优化拜访路线），解决“外勤数据滞后”痛点，适合快消/农牧高频外勤。</li><li><strong>EC</strong>：移动端聚焦“电销+外勤”——支持一键拨号（自动拨打客户电话）、外勤定位打卡（证明到店）、客户资料实时调取（如“拜访时查看客户之前的通话记录”）、微信/企业微信实时沟通（如“把客户微信消息同步到CRM”），适合电销+上门拜访的团队。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>移动端功能</th><th>离线支持</th><th>外勤适配性</th><th>协同能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>客户管理、三一客、快目标、外勤签到、实时同步</td><td>全（跟进+任务+签到）</td><td>是</td><td>中小企外勤</td><td>中（团队联动）</td></tr><tr><td>Salesforce</td><td>GPS打卡、语音/拍照录入、Chatter协作、离线操作</td><td>全（跟进+任务+协作）</td><td>是</td><td>大企外勤</td><td>高（与Teams/Outlook集成）</td></tr><tr><td>Microsoft Dynamics 365</td><td>路线规划、客户位置标注、Teams协作、Outlook同步</td><td>全（跟进+协作+数据）</td><td>是</td><td>微软生态外勤</td><td>高（与微软工具集成）</td></tr><tr><td>红圈营销</td><td>离线录入、客户定位、现场订单、路线规划</td><td>全（拜访+路线+订单）</td><td>是</td><td>快消/农牧外勤</td><td>中（任务分配+监控）</td></tr><tr><td>EC</td><td>一键拨号、外勤打卡、客户资料调取、微信同步</td><td>全（电销+外勤+沟通）</td><td>是</td><td>电销/上门拜访</td><td>高（与微信/企业微信集成）</td></tr></tbody></table><h3>3. 维度总结</h3><ul><li>中小企外勤：选超兔（功能简洁+实时同步）；</li><li>大企协同外勤：选Salesforce（Chatter协作+离线支持）；</li><li>微软生态：选Microsoft Dynamics 365（Teams/Outlook集成）；</li><li>快消/农牧高频外勤：选红圈营销（离线录入+路线规划）；</li><li>电销+上门：选EC（一键拨号+微信同步）。</li></ul><h2>四、数据分析与团队绩效管理：从“经验判断”到“数据驱动”</h2><p>数据分析与绩效管理的核心是<strong>用数据识别瓶颈</strong>，优化团队动作，提升绩效。其关键指标包括：分析深度、AI能力、可视化程度、绩效关联度。</p><h3>1. 各品牌核心功能展开</h3><ul><li><strong>超兔一体云</strong>：侧重<strong>目标拆解+进度监控</strong>——“快目标”模块将公司目标分解到部门/个人（如“本月总目标100万，A销售需完成20万”），用“红绿灯”标识状态，“喜报”功能展示优秀员工（如“XX销售签单10万，排名第一”），适合中小企简单绩效监控。</li><li><strong>Salesforce</strong>：大企级数据分析——内置Tableau分析云，生成<strong>实时销售仪表盘</strong>（如“漏斗转化率：线索→成交转化率15%”“区域业绩：华东区完成率120%”），Einstein AI预测赢单概率（如“商机A赢单概率70%，需重点跟进”），支持多维度数据钻取（如“点击转化率，查看是线索质量低还是跟进不到位”），实现“数据-策略-执行”闭环。</li><li><strong>SAP</strong>：依托ERP数据，提供<strong>全链路分析</strong>——生成销售趋势（如“季度销量增长10%，源于产品B的推广”）、产品性能（如“产品C的退货率5%，需优化质量”）、市场份额（如“在华南区占比20%，需加强推广”），适合中大型企业“从销售到生产”的全链路优化。</li><li><strong>Freshsales</strong>：AI辅助分析——AI助手Freddy提供<strong>客户行为预测</strong>（如“客户最近浏览了价格页，可能要下单”）和<strong>销售趋势分析</strong>（如“本月电销转化率下降，因异议处理环节耗时增加”），支持销售流程管控（如“查看每个阶段的耗时，优化瓶颈环节”），适合中小企AI辅助决策。</li><li><strong>EC</strong>：电销专项分析——实时监控<strong>通话指标</strong>（通话时长、接通率、成单转化率），生成团队排行榜（如“XX销售接通率80%，排名第一”），绩效报表（如“本月电销业绩占比60%，需加强线上获客”），适合电销团队量化管理。</li></ul><h3>2. 横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心功能</th><th>分析深度</th><th>AI 能力</th><th>可视化程度</th><th>绩效关联度</th></tr></thead><tbody><tr><td>超兔一体云</td><td>快目标分解、红绿灯状态、喜报功能、转化率分析</td><td>中（流程 + 业绩）</td><td>AI分析电话录音，微信沟通内容</td><td>中（数字卡片 + 图表）</td><td>中（目标与行动关联）</td></tr><tr><td>Salesforce</td><td>Tableau 分析、Einstein AI 预测、实时仪表盘、多维度钻取</td><td>高（全链路 + 客户行为）</td><td>高（赢单概率 + 话术生成）</td><td>高（可视化报表 + 预警）</td><td>高（绩效与流程深度关联）</td></tr><tr><td>SAP</td><td>销售趋势、产品性能、市场份额、自定义报表</td><td>高（ERP 全链路数据）</td><td>中（趋势预测）</td><td>中（传统报表）</td><td>中（绩效与 ERP 数据关联）</td></tr><tr><td>Freshsales</td><td>Freddy AI 预测、客户行为分析、销售流程管控</td><td>中（客户 + 销售行为）</td><td>中（行为预测 + 线索评分）</td><td>高（可视化仪表盘）</td><td>中（绩效与销售活动关联）</td></tr><tr><td>EC</td><td>通话指标监控、团队排行榜、绩效报表</td><td>中（电销流程 + 业绩）</td><td>无</td><td>中（排行榜 + 报表）</td><td>高（绩效与电销指标强关联）</td></tr></tbody></table><h3>3. 维度总结</h3><ul><li>中小企简单绩效监控：选超兔一体云（目标拆解 + 进度监控）；</li><li>大企级数据分析：选 Salesforce（实时销售仪表盘 + AI 预测）；</li><li>中大型企业全链路优化：选 SAP（ERP 全链路分析）；</li><li>中小企 AI 辅助决策：选 Freshsales（AI 助手分析）；</li><li>电销团队量化管理：选 EC（通话指标监控）。</li></ul><h2>五、总结</h2><p>在数字化转型的浪潮中，CRM 系统已成为企业提升竞争力的关键工具。通过对超兔一体云、Salesforce、SAP、Microsoft Dynamics 365、Zoho、Freshsales、红圈营销、EC 等 8 个主流 CRM 品牌在销售流程标准化、客户全视图管理、高效移动办公/销售外勤、数据分析与团队绩效管理这四大核心维度的深度对比，我们可以看到每个品牌都有其独特的优势和适用场景。</p><p>企业在选择 CRM 系统时，应根据自身的规模、行业特点、业务需求、预算等因素综合考虑。对于中小企业而言，如果追求快速落地和简单易用，超兔一体云在多个维度都能满足需求；而大型企业若有较高的自定义和智能化需求，Salesforce 则是更优的选择。制造企业注重 ERP 协同和全链路管理，SAP 会是理想之选；快消、农牧等外勤高频行业，红圈营销的行业标准化流程能解决实际痛点；电销团队则可优先考虑 EC 的专业电销功能。</p><p>希望本文的对比分析能为企业在 CRM 系统选型过程中提供有价值的参考，助力企业实现数字化转型，提升运营效率和市场竞争力。</p>]]></description></item><item>    <title><![CDATA[数据工程新范式：NoETL 语义编织如何激活海量埋点数据价值？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047578014</link>    <guid>https://segmentfault.com/a/1190000047578014</guid>    <pubDate>2026-01-28 16:08:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=j7I%2BrZx0K1IwRX1NkFXpQQ%3D%3D.47s3H%2FNJ%2FrtCQ0D6UQZXCzAsgSlP9iQQb5%2BZOFoeOZTbD9y1%2F%2Fd7c2v%2FHHrWe64J%2Fv8UT%2BmnHKo274qu12J7SkyB5mmPbyKag%2FcHlD929fY%3D" rel="nofollow" target="_blank">《如何低成本激活海量用户行为数据价值？NoETL 语义编织实践指南》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：面对海量埋点数据价值释放的困境，传统 ETL 模式在业务灵活性、口径一致性和成本性能间难以平衡。本文提出通过引入 NoETL 语义编织架构，构建统一语义层、实现自动化查询与智能物化，从而打破“不可能三角”，实现秒级自助分析与 AI-Ready 数据底座建设，为数据工程与指标平台实践提供系统指南。</p><p>每天，数亿条用户点击、浏览、停留的埋点数据，正源源不断地涌入企业的数据湖仓。然而，这些本该驱动精准营销、产品迭代和体验优化的“数据原油”，却因传统数据供给模式的瓶颈，长期沉睡，沦为吞噬存储与计算成本的“负资产”。</p><p>现实更为严峻：企业湖仓数据冗余平均在 5 倍以上，而专业数据人才的缺口高达 200 万。这意味着，企业正陷入 “数据越多，价值越难释放” 的怪圈。当业务部门急需一个“高价值用户转化漏斗”的分析时，数据团队往往需要排期数周，通过重复开发宽表来响应，最终产出口径不一、维度固化的报表，无法满足灵活探查的需求。</p><p>问题的根源，在于传统以人工 ETL 和物理宽表为核心的数据供给模式，已无法平衡 “业务灵活性”、“口径一致性”与“性能成本” 的“不可能三角”。而 AI 智能体（Agent）时代的到来，以其发散性、秒级响应的问数需求，彻底击穿了这套勉力维持的旧体系。</p><p>激活海量用户行为数据价值的关键，在于一场从“过程驱动”到“语义驱动”的范式重构——引入 NoETL 语义编织架构。</p><h2>前置条件：认清传统数据供给模式的“不可能三角”</h2><p>在深入解决方案前，我们必须正视当前架构的根本性矛盾。这个“不可能三角”具体表现为：</p><ul><li>业务灵活性：营销、产品等一线部门希望像使用搜索引擎一样，自由组合“渠道”、“用户标签”、“时间周期”等维度，进行探索性分析。但在宽表模式下，维度组合是预定义的，任何未预设的分析路径都需要重新开发。</li><li>口径一致性：管理层要求“GMV”、“活跃用户”等核心指标在全公司有且仅有一个权威定义。然而，指标逻辑被硬编码在分散的 ETL 脚本和物理宽表中，微小的逻辑差异导致报表间“数据打架”成为常态。</li><li>性能与成本：数据团队需要在有限的预算内保障查询秒级响应。为此，他们不得不预建大量宽表和汇总表（ADS 层），导致相同明细数据被反复加工存储，形成巨大的冗余和浪费，陷入“为保障性能而推高成本”的恶性循环。</li></ul><p>这套依赖人力的“人工预计算”范式，在数据量和分析需求激增的今天，已成为数据价值释放的主要瓶颈。解决问题的出路，不是在这个三角中继续做痛苦的取舍，而是通过架构革新，打破三角本身。</p><h2>第一步：架构重构——引入 NoETL 语义编织层</h2><p>解决问题的起点，是将 “业务语义” 与 “物理底表” 彻底解耦。这类似于软件开发从汇编语言（直接操作硬件）演进到高级语言（声明业务逻辑）。</p><p>NoETL 语义编织 的核心，是在企业的公共明细数据层（DWD）与上游的消费应用（BI、AI Agent、业务系统）之间，构建一个独立、统一、具备实时计算能力的 语义层（Semantic Layer）。</p><ul><li>逻辑层（做什么）：业务分析师在语义层中，通过声明式的方式，用业务语言定义指标（如“近30天高价值用户留存率”）、维度及其关联关系。他们无需关心数据存储在哪里、表如何关联。</li><li>物理层（怎么做）：平台的 语义引擎 自动将逻辑定义“编译”为面向底层数据湖仓（如 Snowflake, BigQuery）优化过的高效 SQL 执行计划。无论是实时查询明细，还是智能路由到加速表，都由系统自动完成。</li></ul><p>这种解耦带来了 “无头化（Headless）” 与 “中立性”。数据不再为某个特定的 BI 报表加工，而是成为一种标准化的服务。无论是 BI 工具，还是未来的 AI 应用，都通过统一的 API/JDBC 接口消费同一份经过治理的“逻辑真理”。</p><h2>第二步：能力建设——部署具备三大支柱的指标平台</h2><p>一个合格的 NoETL 语义编织平台，必须具备以下三大核心能力，缺一不可：</p><h3>1. 统一语义层：构建虚拟的业务事实网络</h3><p>平台允许用户在未物理打宽的 DWD 表之上，通过界面化配置，声明式地定义表与表之间的关联关系（如用户表与行为事件表通过 <code>user_id</code> 关联）。由此，在逻辑层面构建出一张覆盖全域的 “虚拟大宽表”，业务人员可在此基础上进行任意拖拽分析。</p><h3>2. 自动化查询生成：意图即 SQL</h3><p>当用户拖拽指标或 AI Agent 提出自然语言问题时，平台的语义引擎能实时解析分析意图，自动生成高效、优化的查询 SQL，自动处理复杂的多表 JOIN、去重和跨层级计算，实现数据获取的零门槛。</p><h3>3. 智能物化加速：基于声明的性能保障</h3><p>这是区别于传统逻辑视图的关键。平台提供 “声明式物化” 能力：</p><ul><li>管理员声明：基于业务需求，声明需要对哪些指标和维度组合进行加速，以及数据时效性要求（如 T+1）。</li><li>系统自治：平台根据声明，自动设计物化视图、编排 ETL 任务依赖并运维。</li><li>透明路由：查询时，引擎自动进行 SQL 改写，让查询命中最佳的物化结果，实现百亿级数据的秒级响应。尤其关键的是，其物化引擎支持对去重计数、比率类等复杂指标进行上卷聚合，突破了传统物化技术的限制。</li></ul><p><img width="723" height="155" referrerpolicy="no-referrer" src="/img/bVdnNnr" alt="" title=""/><br/><img width="723" height="146" referrerpolicy="no-referrer" src="/img/bVdnNns" alt="" title="" loading="lazy"/></p><h2>第三步：实施落地——采用“存量挂载”与“增量原生”混合策略</h2><p>引入新范式无需“推倒重来”。我们推荐采用分阶段的混合策略，平滑演进，保护既有投资：</p><ol><li>存量挂载（保护投资）：对于现有逻辑稳定、性能尚可的物理宽表，直接将其接入语义层，映射为“逻辑视图”并注册指标。实现零开发成本下的统一服务出口。</li><li>增量原生（遏制新债）：对所有新产生的分析需求，尤其是来自 AI Agent 的灵活问数，坚决采用“原生”模式。直接基于 DWD 明细层，通过语义层定义指标，由平台自动化处理计算与加速，从源头杜绝新宽表的产生。</li><li>存量替旧（优化成本）：在平台能力得到验证后，逐步识别并下线那些维护成本高、逻辑复杂的“包袱型”旧宽表，将其逻辑迁移至语义层，释放计算资源。</li></ol><p>一个典型的推广路径分为四个阶段：战略筹备与灯塔选择 -&gt; 价值验证与能力内化 -&gt; 全面推广与组织建设 -&gt; 生态融合与价值深化。核心是从一个痛点明确的业务场景（如“营销活动分析”）切入，快速交付可感知的价值，建立内部信心后再规模化推广。</p><h2>第四步：价值深化——从统一分析到赋能 AI 智能体</h2><p>当统一的指标语义基座建成后，其价值将超越传统 BI，深度赋能 AI 场景：</p><ul><li>为 AI 划定“认知围栏”：语义层提供的结构化、业务友好的指标与维度元数据，是 RAG（检索增强生成）的优质语料。AI Agent 不再需要直面晦涩的物理表 Schema 去“猜测”SQL，而是通过 NL2Metrics（自然语言转指标查询） 模式，调用标准的语义 API（如 <code>GetMetric(name=”毛利”, filter={region:”华东”})</code>），从根本上降低幻觉风险。</li><li>提供深度分析工具：语义层内置的 明细级多维度归因 等模块，可通过 API 被 AI Agent 调用。当业务指标波动时，AI 能自动、即时地分析出是哪个维度（地区、渠道）下的哪个具体值（某个产品）贡献了主要变化，实现从“看数”到“归因”的智能决策闭环。</li><li>实现双模驱动：底层同一套语义基座，向上同时支撑 BI 的“稳”（固定报表、高精度、秒级呈现）与 AI 的“活”（灵活探查、自然交互、智能归因），无需为 AI 单独建设数据管道。</li></ul><h2>避坑指南：甄别“真伪”NoETL 语义编织平台</h2><p>市场概念纷杂，选型时请重点考察以下四个维度：</p><ol><li>计算内核：是“静态逻辑目录”还是“动态计算引擎”？真平台必须支持在未打宽的 DWD 上构建“虚拟事实网络”，并支持通过配置定义跨表聚合、二次聚合、比率留存等复杂指标，而非只能做简单聚合。</li><li>性能机制：智能物化是“全自动”还是“基于声明”？真平台应允许管理员声明加速策略，由系统自动完成物化任务的创建、运维和查询路由，并支持不可累加指标（如去重计数）的物化上卷。</li><li>架构属性：是“BI 附属品”还是“中立开放基座”？真平台应通过标准 Restful API 和 JDBC 接口提供服务，能与任何 BI 工具（如 Tableau、Power BI 通过 JDBC）、业务系统或自研 AI Agent 无缝集成，避免厂商锁定。</li><li>AI 适配度：是“Schema 投喂”还是“语义增强”？真平台应提供结构化的语义元数据（指标口径、血缘、业务限定），支持 NL2Metrics 和 Function Calling，为 AI 提供精准的业务上下文，而非仅仅暴露原始表结构。</li></ol><h2>成功标准：如何衡量数据价值是否被真正激活？</h2><p>数据价值的激活应是可量化、可感知的。成功落地后，企业应在以下三个维度看到显著改善：</p><ol><li>业务敏捷性：临时性、探索性的数据分析需求，平均响应时间从“周级”缩短至“分钟级”，业务自助用数比例大幅提升。</li><li>成本可控性：通过消除冗余的 ETL 加工和物理宽表，数据仓库的存储与计算成本得到显著优化（实践案例中常见 20%-30% 的下降）。</li><li>决策精准性：基于全公司统一的指标口径，数据驱动的洞察更加可信。结合明细级归因能力，业务行动（如渠道优化、产品迭代）的效果可衡量、可归因，决策闭环速度加快。</li></ol><p>案例印证：某头部券商引入 NoETL 语义编织平台后，在一条核心业务线上，IT 仅需维护 10 张公共层模型和 100 个原子指标，即可支撑业务人员使用超过 300 个维度进行灵活组合分析，将指标开发交付周期从两周以上缩短到分钟级，并实现了指标口径的 100% 一致。</p><h2>常见问题（FAQ）</h2><h4>Q1: 我们已经用了现代云数仓，为什么还需要 NoETL 语义编织？</h4><p>现代云数仓（如 Snowflake、BigQuery）解决了存储和计算的弹性问题，是强大的“引擎”。但业务灵活分析的需求，仍然需要通过人工开发大量宽表来满足，这导致了“最后一公里”的口径混乱和成本浪费。NoETL 语义编织是在这些强大引擎之上，构建统一、敏捷的“业务语义层”和“自动变速箱”，让好引擎能持续、高效地产出可信、好用的数据。</p><h4>Q2: NoETL 是不是意味着完全取消 ETL？历史宽表怎么办？</h4><p>NoETL 并非取消 ETL，而是改变其主体和模式。物化加速本身也是一种 ETL，但其策略由管理员声明，执行由系统自动完成。对于历史宽表，建议采用“存量挂载”策略接入，保护投资；对所有新需求，坚决采用“增量原生”，由系统自动化智能物化，无需人工开发新宽表。</p><h4>Q3: 引入 NoETL 语义编织，对现有数据团队有什么影响？</h4><p>这是积极的角色转型。数据工程师将从重复、低价值的 SQL 脚本编写和 ETL 运维中解放出来，转向更具战略性的工作：设计与优化企业级语义模型、保障数据供应链质量、配置与优化物化策略（FinOps）、以及赋能业务人员。平台通常提供直观界面，辅以针对性培训，团队可以较快适应新角色，提升整体价值。</p><h2>Key Takeaways（核心要点）</h2><ol><li>范式革新：NoETL 语义编织通过 “逻辑与物理解耦”，构建统一语义层，是解决传统数据供给“不可能三角”的根本性架构革新。</li><li>核心能力：真正的平台必须具备 统一语义建模、自动化查询生成、声明式智能物化加速 三大支柱，尤其要支持复杂指标的物化上卷。</li><li>落地路径：采用 “存量挂载 + 增量原生” 的混合策略，从灯塔场景切入，小步快跑，实现平滑演进与价值快速兑现。</li><li>未来价值：统一的语义基座不仅是提升 BI 效率的工具，更是企业构建 AI-Ready 数据底座、实现“BI稳”与“AI活”双模驱动的关键基础设施。</li><li>衡量标准：成功与否看三点：业务分析响应是否进入“分钟级”、存算成本是否显著下降、数据驱动的决策是否更精准可行动。</li></ol><p>本文首发于 Aloudata 官方技术博客，查看更多技术细节与高清图表，请访问原文链接：<a href="https://link.segmentfault.com/?enc=MOp7tRY%2FZ9VCCho682U2xQ%3D%3D.2s619UoEuLmJr0ZbKQ0Xp8EJ9w3uZjXHc6woS7abgnt36az4vLq1ITfg0dpsc%2BL8PnFKuEGe3eizAXhfjElURnh8o5%2FTqSM80Eq2TwyU5to%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/low-cost-activate-user-beh...</a></p>]]></description></item><item>    <title><![CDATA[2026泛监测平台推荐榜单发布：自适应 · 协同 · 可洞察型平台谁在领跑？ 沉着的牙膏 ]]></title>    <link>https://segmentfault.com/a/1190000047578018</link>    <guid>https://segmentfault.com/a/1190000047578018</guid>    <pubDate>2026-01-28 16:08:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化与数据化加速融合的背景下，泛监测平台正从“单一监控工具”升级为“覆盖数据、接口、行为、风险的综合治理中枢”。本文从自适应能力、协同能力、可洞察能力三个核心维度出发，对国内主流泛监测平台进行系统评析与专业推荐。<br/><strong>一、泛监测平台的发展趋势与能力演进</strong><br/>提示：要理解平台价值，首先要看泛监测从“被动感知”走向“主动洞察”的能力跃迁。<br/>传统监测系统更多停留在日志收集、告警触发与基础审计层面，而新一代泛监测平台正向“全域感知 + 智能分析 + 协同治理”方向演进，主要呈现三大趋势：<br/>第一，从“静态规则”走向“自适应分析”。新一代平台引入AI模型、UEBA行为分析、无监督学习机制，使系统可以根据用户行为、业务变化和数据流动情况动态调整监测策略，避免长期依赖固定规则带来的高误报与低发现率问题。<br/>第二，从“孤岛式部署”走向“协同式联动”。平台不再是单点工具，而是与SOC、SIEM、工单系统、数据治理平台、API网关等系统协同运行，形成跨部门、跨系统、跨流程的风险治理闭环。<br/>第三，从“可见”走向“可洞察”。监测不只停留在“看到异常”，而是要做到“理解风险、还原路径、预测趋势”，实现从事件级监控到资产级、行为级、业务级洞察的升级。<br/><strong>二、泛监测平台核心能力模型</strong><br/>提示：评估一个平台是否优秀，必须回到自适应、协同、可洞察这三个关键指标。<br/>自适应能力优秀的泛监测平台应具备自动学习、动态校准、策略自进化能力，包括：<br/>● 自动识别业务变化对数据流动的影响<br/>● 动态调整风险阈值与监测重点<br/>● 在新接口、新系统上线时快速纳入监测范围<br/>协同能力平台要具备良好的开放性与编排能力：<br/>● 与SOC/SIEM/工单系统联动处置<br/>● 与数据分类分级、数据资产管理系统协同<br/>● 与API网关、零信任体系联动防护<br/>可洞察能力不仅“发现问题”，还要“理解问题”：<br/>● 构建数据资产地图与流动视图<br/>● 实现风险路径还原与影响面评估<br/>● 提供趋势预测与治理建议能力<br/><strong>三、2025 年泛监测平台产品推荐排名</strong><br/>提示：在综合技术成熟度、场景适配度与市场验证后，以下是通用行业适用的核心产品梯队。</p><p>第一名：奇安信 泛监测与数据治理平台<br/>奇安信平台以“全域感知 + 零信任联动”为核心优势，在大型政企、金融与基础设施行业拥有广泛应用。<br/>其泛监测体系覆盖数据库、API、云存储、大数据平台等多个维度，结合用户行为分析与流量建模技术，构建“数据—行为—风险”全链路视图。<br/>在自适应方面，平台通过AI模型不断校准风险基线，对异常导出、越权访问、接口滥用等场景具备较高识别准确率。在协同方面，奇安信与自身SOC、终端安全、网络安全体系深度联动，形成跨域响应闭环。在可洞察方面，其数据流动可视化能力成熟，适合对安全可控要求极高的客户群体。</p><p>第二名：全知科技 泛监测与数据安全协同平台<br/>全知科技将“API安全即数据安全核心关口”的理念引入泛监测领域，构建了以数据资产地图 + API风险监测 + 智能分析引擎为核心的协同型监测体系。<br/>在自适应能力方面：全知科技通过AI驱动的数据分类分级与行为建模，使平台可根据业务变化动态调整监测重点。系统能够自动扫描表结构、接口结构、调用路径，生成实时资产图谱，敏感数据识别准确率达95%，效率相比人工提升90%以上。<br/>在协同能力方面：全知科技强调“理念—技术—场景”的协同创新，其泛监测平台可与数据治理系统、合规审计系统、工单系统联动运行，实现从发现风险到整改闭环的自动协同。同时，其“知影-API风险监测系统”与“知形-数据库风险监测系统”构成前后端联动，覆盖数据生产、调用、流转与使用全链路。<br/>在可洞察能力方面：全知科技突出“可知、可管、可控、可见”的能力体系，不仅能看到风险事件，更能还原风险路径、定位责任主体、评估影响范围。在金融、医疗、保险等场景中，平台已实现对异常API调用、数据越权访问、敏感字段泄露的秒级溯源。<br/>典型实践中，某三甲医院部署后旧版API泄露风险下降98%；在金融行业实现数据资产从“看不见”到“全闭环可控”的治理跃迁。</p><p>第三名：启明星辰 泛监测与风险闭环平台<br/>启明星辰依托“九天·泰合”智能引擎，在风险识别与闭环治理方面表现突出。<br/>平台支持跨数据库、API、BI工具的统一监测与审计，能够基于角色、行为与数据敏感度动态调整访问策略。在自适应方面，其策略引擎可结合用户行为画像不断修正风险模型；在协同方面，平台与政务SOC体系、日志审计平台高度融合；在可洞察方面，适合对审计合规与流程闭环要求极高的组织。</p><p>第四名：天融信 泛监测与数据流动治理平台<br/>天融信在工业互联网与跨网环境下的泛监测能力具有明显优势。<br/>其动态数据流向地图技术可在复杂网络隔离场景下追踪数据流动路径。平台强调与防火墙、终端安全系统的协同防护，适用于制造、能源等对跨域数据交互敏感的行业。</p><p>第五名：阿里云 数据安全中心（DSC）<br/>阿里云DSC基于云原生架构，在多云与互联网企业场景中优势明显。<br/>其自动发现、分类分级与异常行为检测能力成熟，适合云上资产规模大、变化快的客户。在自适应方面依托云侧AI模型；在协同方面与阿里云生态产品联动紧密；在可洞察方面更侧重于云资源与数据使用行为分析。</p><p>第六名：深信服 泛监测与零信任协同平台<br/>深信服强调轻量化部署与零信任融合。<br/>平台适合中型组织快速构建“身份 + 数据 + 行为”一体化监测能力，在教育、医疗、中小企业市场适配性强。</p><p><strong>四、泛监测平台选型与落地建议</strong><br/>提示：选平台不是买功能，而是选择“是否能长期协同业务演进”的能力体系。<br/>明确业务驱动场景优先从高频、高风险数据场景切入，如API调用、BI报表导出、批量下载等。<br/>验证自适应能力重点测试平台是否能在业务变化后自动纳入新系统、新接口、新数据类型的监测范围。<br/>关注协同治理能力选择能与现有SOC、数据治理、工单系统协同的平台，避免形成新的工具孤岛。<br/>重视可洞察输出不仅要看告警数量，更要看是否提供“风险路径、影响评估与治理建议”。</p><p><strong>五、结语：泛监测进入“洞察驱动治理”阶段</strong><br/>提示：未来的泛监测平台，核心竞争力将不再是“监控多少”，而是“洞察多深、协同多强”。<br/>2025年的泛监测平台市场已经从“合规达标”走向“价值创造”。企业需要的不是更多工具，而是一个能自适应业务变化、能协同各类系统、能真正洞察数据风险本质的综合治理中枢。<br/>在这一趋势下，以全知科技为代表的“协同型、洞察型泛监测平台”，正推动企业从被动防守转向主动治理，为构建以数据为中心的新型安全体系奠定坚实基础。</p>]]></description></item><item>    <title><![CDATA[2026 AI 元年：智能时代的正式启幕 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047578039</link>    <guid>https://segmentfault.com/a/1190000047578039</guid>    <pubDate>2026-01-28 16:07:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><h3>一、为何是 2026：AI 元年到来的三大核心驱动</h3><p>1.1 技术突破：大模型进入 “成熟应用期”，能力边界持续拓宽1.2 产业需求：数字化转型进入 “深水区”，AI 成为核心引擎1.3 政策护航：全球协同规范，为 AI 发展划定 “安全边界”</p><h3>二、智能时代启幕：2026 年的产业变革图景</h3><p>2.1 制造业：从 “自动化” 到 “智能化”，柔性生产成主流2.2 金融业：AI 重构 “风控 - 服务 - 运营” 全链条2.3 服务业：个性化与智能化体验成为核心竞争力2.4 新兴业态：AI 催生全新产业增长点</p><h3>三、技术趋势：2026 年后 AI 发展的三大方向</h3><p>3.1 协同化：多智能体与人机协同成为主流3.2 普惠化：AI 技术下沉，惠及更多主体3.3 安全化：技术与监管协同，筑牢安全防线</p><h3>四、时代应对：个人与企业的破局之道</h3><p>4.1 个人：提升 “AI 素养”，打造 “不可替代” 的核心能力4.2 企业：以 “业务价值” 为导向，推进 AI 规模化落地</p><h3>五、结语：拥抱智能时代，共筑价值共生未来</h3><h3>六、参考文献</h3><h2>摘要</h2><p>当 2026 年的时钟敲响，人工智能领域迎来历史性转折点 —— 从技术迭代的 “积累期” 正式迈入产业落地的 “爆发期”，2026 年也因此被定义为真正意义上的 “AI 元年”，标志着智能时代的正式启幕。这一年，大模型技术完成从 “能力突破” 到 “价值兑现” 的关键跨越，智能体成为企业数字化转型的核心载体，AI 普惠化浪潮席卷各行各业，技术、产业、政策的三重协同让 AI 真正从实验室走向产业一线、从概念走向实用。本文立足 2026 年这一关键时间节点，深度剖析 AI 元年到来的核心驱动因素，全景解读智能时代启幕下的制造业、金融业、服务业等全产业变革图景，预判 2026 年后 AI 协同化、普惠化、安全化的核心发展趋势，并为个人与企业提供适配智能时代的破局策略与行动指南，助力各类主体把握时代机遇，在智能浪潮中实现高质量发展。</p><p>​<strong>关键词</strong>​：2026 AI 元年；智能时代；大模型；智能体；产业数字化；普惠 AI；人机协同</p><hr/><h2>一、为何是 2026：AI 元年到来的三大核心驱动</h2><p>AI 技术的发展并非一蹴而就，从 2016 年 AlphaGo 击败李世石开启公众对 AI 的认知热潮，到 2023 年生成式 AI 引发全球技术狂欢，再到 2026 年正式迈入 “元年”，背后是技术、产业、政策三大维度的长期积累与协同共振。2026 年的 “AI 元年” 定位，绝非偶然的时间标记，而是 AI 技术从实验室走向产业、从单一工具走向核心生产力的必然结果，是智能时代正式启幕的历史坐标。</p><h3>1.1 技术突破：大模型进入 “成熟应用期”，能力边界持续拓宽</h3><p>2026 年，大模型技术彻底摆脱了 “参数竞赛” 的内卷，完成向 “效率革命” 的转型，迎来三大里程碑式技术突破，为 AI 元年奠定了坚实的技术基础。一是多模态融合能力全面成熟，文本、图像、音频、视频、三维建模等多类型信息实现无缝理解、跨模态生成与逻辑关联，打破了不同信息形态的传播与应用壁垒，让 AI 对现实世界的理解更贴近人类。二是端侧部署成本大幅降低，依托芯片技术的迭代、模型轻量化优化与分布式算力架构的创新，高性能大模型可在普通终端设备、工业产线终端上高效运行，彻底摆脱了对云端超算算力的过度依赖，实现 “云边端” 一体化的智能部署。三是决策可靠性显著提升，通过引入因果推理框架、实时数据校准机制与多源证据交叉验证体系，大模型的决策偏差率降低 60% 以上，彻底摆脱了传统生成式 AI “胡编乱造” 的弊端，具备了进入金融、医疗、工业控制等核心关键领域的技术基础。</p><p>更重要的是，2026 年 “智能体操作系统” 的正式商用，成为大模型从 “问答工具” 升级为 “自主行动主体” 的核心标志。这一系统实现了智能体的快速配置、多工具无缝对接、跨场景协同调度，企业无需专业的 AI 开发团队，仅通过低代码可视化操作即可搭建专属数字员工，彻底降低了 AI 技术的产业应用门槛，让智能体成为企业可触达、可复用、可创造价值的核心资产，这也是智能时代启幕的核心技术支撑。</p><h3>1.2 产业需求：数字化转型进入 “深水区”，AI 成为核心引擎</h3><p>经过多年的数字化转型铺垫，全球企业的数字化需求已从基础的 “流程线上化、数据电子化” 转向深度的 “业务智能化、决策自动化”，传统的数字化工具如 ERP、CRM 等已无法满足企业降本增效、创新业务、应对市场变化的核心诉求，AI 成为企业数字化转型进入 “深水区” 的唯一核心引擎。</p><p>2026 年，全球经济复苏压力持续增大，各行各业的企业都面临着 “降本、提效、创新” 的三重考验，为 AI 技术的规模化落地提供了强劲的产业需求。从大型企业来看，其数字化基础完善、数据积累充足，亟需通过 AI 技术实现全业务链条的智能化升级，重构核心竞争力；从中小企业来看，其对效率提升、成本控制的需求更为迫切，但此前受技术门槛、资金成本的限制，难以享受 AI 技术红利。2026 年推出的 “普惠 AI 套餐” 彻底打破了这一局面，通过低代码平台、模块化 AI 工具、按需付费的商业模式，让中小企业只需投入少量成本，即可享受智能体、智能数据分析、智能客服等高端 AI 服务，彻底打破了 “AI 是大企业专属” 的行业现状，让 AI 技术渗透到产业的毛细血管。</p><p>从行业来看，制造业的生产调度优化、金融业的精准风控、零售业的个性化运营、服务业的智能服务，各领域的核心业务痛点都需要 AI 技术来解决，产业需求与 AI 技术的深度匹配，让 AI 从 “可选项” 成为 “必选项”，这也是 AI 元年到来的核心产业动因。</p><h3>1.3 政策护航：全球协同规范，为 AI 发展划定 “安全边界”</h3><p>技术的快速发展离不开规范的引导，无边界的技术创新必然伴随各类风险，2026 年，全球主要经济体相继出台并落地 AI 产业发展与监管政策，形成了 “鼓励创新 + 保障安全 + 规范发展” 的协同监管框架，为 AI 元年的到来筑牢了政策根基，也为智能时代的健康发展划定了安全边界。</p><p>在产业支持方面，各国均加大了对 AI 基础研究、核心技术、关键芯片、算力基础设施的投入，推动 AI 技术的自主创新与突破。中国出台《新一代人工智能发展规划（2024-2030 年）》，明确了 AI 大模型、智能体、算力网络等核心发展方向，并设立专项扶持资金，支持中小企业的 AI 应用落地；美国推出 AI 创新与安全法案，加大对 AI 基础研究的政府投入，鼓励企业开展技术创新；欧盟、日本、韩国等也相继出台了各自的 AI 产业发展规划，推动全球 AI 产业的协同发展。</p><p>在监管规范方面，全球监管框架实现了 “分级分类、协同共治” 的核心突破。欧盟《人工智能法案》正式落地实施，对不同风险等级的 AI 应用实施分级监管，对高风险 AI 应用如医疗 AI、工业 AI 实施严格的安全评估与备案制度；中国建立了 AI 技术应用的安全评估体系与数据使用规则，明确了企业的 AI 伦理责任；美国平衡技术创新与国家安全需求，对 AI 核心技术的出口与合作进行规范。全球政策的协同发力，既鼓励了 AI 技术的创新突破，又防范了 AI 技术应用的安全风险、伦理风险，让 AI 技术在规范的框架内实现产业落地，这也是 AI 元年到来的关键政策保障。</p><h2>二、智能时代启幕：2026 年的产业变革图景</h2><p>2026 AI 元年的到来，标志着智能时代的正式启幕，这一时代的核心特征是 “AI 深度融入生产生活的方方面面，成为驱动经济社会发展的核心生产力”。从产业层面来看，一场覆盖传统产业改造、新兴业态催生的智能化变革已全面展开，AI 正在重构各行业的产业格局、商业模式与竞争逻辑，让各行业迎来全新的发展阶段。</p><h3>2.1 制造业：从 “自动化” 到 “智能化”，柔性生产成主流</h3><p>制造业是实体经济的核心，也是 AI 技术落地的重点领域，2026 年，AI 技术正在推动制造业从传统的 “自动化” 向真正的 “智能化” 转型，柔性生产成为制造业的主流生产模式，彻底解决了传统制造业 “产能固定、适配性差、效率低下” 的行业痛点。</p><p>传统的自动化生产线依托固定的程序与设备，只能完成单一品类、大批量的生产任务，面对市场多变的多品类、小批量需求，难以快速适配，且产线调度、设备维护均依赖人工经验，存在产能利用率低、故障响应慢等问题。2026 年，AI 驱动的智能生产线彻底改变了这一现状，通过生产调度智能体、设备巡检智能体、质量检测智能体的协同工作，实现了产线的全流程智能化管理。智能体可实时采集设备运行数据、原材料库存数据、订单数据、市场需求数据，通过大数据分析与智能推理，自主识别产线产能瓶颈，动态调整生产计划与排产方案；当设备出现故障前兆时，设备巡检智能体可快速定位问题根源，推送精准的维修方案，甚至通过远程控制实现设备的初步修复；质量检测智能体通过多模态识别技术，实现产品质量的全流程、无死角检测，将生产不良率降至最低。</p><p>某大型汽车零部件制造企业的实践印证了这一变革：引入 AI 智能生产体系后，产线产能利用率从 75% 提升至 93%，订单交付周期缩短 25%，生产不良率下降 18%，人工调度与设备维护工作量减少 70%。更重要的是，智能生产线可在无需大规模改造的前提下，快速适配不同品类、不同批量的生产需求，让企业能够精准把握市场需求，实现从 “以产定销” 到 “以销定产” 的转型，柔性生产能力成为制造业企业的核心竞争力。</p><h3>2.2 金融业：AI 重构 “风控 - 服务 - 运营” 全链条</h3><p>金融业是数据密集型与知识密集型行业，天生与 AI 技术高度适配，2026 年，AI 技术已从金融业的辅助工具升级为核心业务支撑，全面重构了金融行业的 “风控 - 服务 - 运营” 全业务链条，实现了效率提升与风险可控的双重目标，推动金融业进入 “智能金融” 新时代。</p><p>在风控环节，智能风控系统实现了从 “事后风控” 到 “实时风控、事前预警” 的转型。传统的金融风控主要依赖历史数据与人工审核，存在风控滞后、识别精准度低等问题，而 2026 年的智能风控系统可整合客户征信数据、交易数据、行为数据、社交数据等多维度信息，通过实时数据分析与动态风险预测模型，精准识别客户的风险信号，对信贷违约、金融诈骗等风险实现提前预警，将个人信贷不良率降低 0.8-1.2 个百分点，企业信贷不良率降低 1.5-2 个百分点。值得注意的是，2026 年金融 AI 的应用更加注重 “可解释性”，通过技术创新让 AI 的风控决策过程透明化、可追溯，彻底解决了传统 AI 模型 “黑箱” 问题，让金融风控既智能又可靠。</p><p>在客户服务环节，智能客服与智能投顾成为金融服务的主流模式。智能客服可实现 7×24 小时全渠道响应，结合客户画像与服务需求，提供个性化的问题解答与业务办理服务，常见问题解决率达 90% 以上，大幅提升客户满意度，同时降低人工客服成本 60% 以上；智能投顾可根据客户的风险承受能力、资产状况、投资需求，为客户制定专属的资产配置方案，并根据市场变化动态调整，让普通客户也能享受到专业的投资顾问服务，实现金融服务的普惠化。</p><p>在运营环节，AI 技术实现了金融机构的全流程智能化运营。智能运营系统可自主完成财务报表生成、合规检查、资金清算、资产配置等工作，将运营人员的工作量减少 50% 以上，运营成本降低 30% 以上；同时，AI 技术可实现金融机构内部数据的整合与分析，为管理层的战略决策提供精准的数据支撑，提升金融机构的决策效率与科学性。</p><h3>2.3 服务业：个性化与智能化体验成为核心竞争力</h3><p>服务业的核心竞争力是客户体验，2026 年，AI 技术正在重新定义服务业的客户体验，让个性化与智能化成为服务业的核心标签，彻底改变了传统服务业 “标准化服务、同质化竞争” 的格局，推动服务业进入 “体验为王” 的智能服务时代。</p><p>在餐饮行业，AI 技术实现了从点餐到出餐的全流程智能化与个性化。智能点餐系统可通过客户的消费记录、口味偏好、饮食禁忌，为客户精准推荐菜品，并结合后厨产能与餐桌翻台率，优化出餐顺序；智能后厨系统可实现食材的精准配比与菜品的标准化制作，同时根据点餐数据动态调整食材采购计划，减少食材浪费。某连锁餐饮企业引入 AI 智能服务体系后，客户点餐效率提升 40%，食材浪费率降低 25%，客户满意度提升 30%。</p><p>在酒店行业，智能服务系统实现了客户从预订到退房的全流程自助服务与个性化服务。客户可通过智能终端完成预订、选房、入住、退房等全流程操作，无需人工介入；智能设备可实时监测客房的温度、湿度、灯光等状态，根据客户的入住习惯自动调整；同时，酒店可通过 AI 技术分析客户的入住需求，为客户提供个性化的服务如定制化早餐、专属旅游攻略等，大幅提升客户的入住体验。</p><p>在教育行业，AI 技术推动了从 “标准化教学” 到 “个性化教学” 的转型。智能教学系统可通过学生的学习数据、知识掌握情况、学习能力，为学生制定专属的学习计划与学习方案，实现 “因材施教”；智能答疑系统可实时解答学生的学习问题，为学生提供精准的知识讲解与解题思路；同时，AI 技术可实现教师教学工作的智能化，如自动批改作业、分析学生学习情况等，让教师能够将更多的精力投入到教学设计与学生辅导中。</p><p>在物流行业，AI 技术实现了物流配送的智能化与高效化。智能调度系统可根据订单数据、配送地址、交通状况，为配送人员制定最优的配送路线；智能仓储系统可实现货物的自动化存储、分拣、搬运，大幅提升仓储效率；同时，AI 技术可实现物流状态的实时追踪与预警，让客户能够实时掌握物流信息，提升客户的物流体验。</p><h3>2.4 新兴业态：AI 催生全新产业增长点</h3><p>2026 年，AI 技术不仅在改造传统产业，更在催生一系列全新的产业业态与商业模式，成为全球经济发展的全新增长点，这些新兴业态依托 AI 技术的核心能力，填补了传统产业的空白，满足了市场的全新需求，展现出强劲的发展活力。</p><p>AI 生成式设计行业快速崛起，成为创意产业的核心力量。设计师可通过智能体快速生成多种设计方案，结合自身的创意与审美，对设计方案进行优化与调整，大幅提升设计效率与设计质量。目前，AI 生成式设计已广泛应用于建筑设计、工业设计、平面设计、服装设计等多个领域，某建筑设计公司引入 AI 生成式设计工具后，设计效率提升 60%，设计方案的创新度提升 40%。</p><p>AI 数字人产业进入规模化应用阶段，彻底打破了 “虚拟与现实” 的边界。2026 年的 AI 数字人已具备高逼真度的形象、自然的语言表达、精准的情感理解能力，不仅广泛应用于直播带货、客服咨询、影视制作等领域，还深入到虚拟办公、虚拟教育、虚拟医疗等多个场景。企业可通过 AI 数字人打造专属的品牌代言人，实现 7×24 小时的品牌宣传与产品推广；学校可通过 AI 数字人打造虚拟教师，为学生提供个性化的教学服务；医院可通过 AI 数字人打造虚拟医生，为患者提供初步的问诊与咨询服务。</p><p>AI 安全服务行业应运而生，成为 AI 产业健康发展的重要保障。随着 AI 技术的广泛应用，AI 模型安全、数据安全、隐私保护等问题日益凸显，AI 安全服务行业依托 AI 安全检测技术、数据加密技术、隐私保护技术，为企业提供 AI 模型安全评估、数据安全防护、AI 伦理合规检查等专项服务，保障 AI 技术的安全落地。目前，全球已有上千家 AI 安全服务企业，成为 AI 产业生态中不可或缺的重要组成部分。</p><p>此外，AI 算力租赁、AI 模型训练、AI 数据标注等新兴服务业也快速发展，形成了完善的 AI 产业生态，为 AI 技术的规模化落地提供了全方位的服务支撑，推动智能时代的产业生态更加完善。</p><h2>三、技术趋势：2026 年后 AI 发展的三大方向</h2><p>2026 AI 元年不仅是 AI 技术产业落地的爆发点，更是未来 AI 技术发展的风向标。从 2026 年的技术实践与产业需求来看，2026 年后，AI 技术将不再追求单一的能力突破，而是朝着 “协同化、普惠化、安全化” 三大方向深度发展，这三大方向将成为智能时代 AI 技术发展的核心主线，推动 AI 技术与产业的深度融合，实现更高质量的发展。</p><h3>3.1 协同化：多智能体与人机协同成为主流</h3><p>单一智能体的能力存在天然局限，面对跨领域、跨部门、多环节的复杂业务场景，难以独立完成任务，2026 年后，<strong>多智能体协同</strong>将成为 AI 技术发展的核心方向，同时<strong>人机协同</strong>模式将进一步优化，成为智能时代生产生活的主流方式。</p><p>多智能体协同的核心是打造 “智能体战队”，不同功能、不同领域、不同角色的智能体，通过标准化的协议与接口，实现任务分工、信息共享、协同配合，共同完成复杂的业务任务。例如，企业的新品推广流程中，市场分析智能体负责采集市场数据、分析市场需求与竞品动态，文案创作智能体负责根据市场分析结果生成产品宣传文案与营销方案，渠道投放智能体负责将营销方案推送到各渠道并实现精准投放，效果监测智能体负责实时监测投放效果并分析数据，四大智能体协同工作，实现新品推广的全流程自动化，无需人工全程干预。2026 年后，多智能体协同平台将成为企业 AI 应用的核心载体，实现智能体的快速组建、调度与协同，让多智能体协同成为企业的标配。</p><p>同时，人机协同模式将从 “人主导、机辅助” 向 “人机分工互补、价值共创” 升级，人类与智能体的分工将更加清晰、合理。智能体将承接所有重复性、执行性、数据性的工作，如数据采集、报表生成、常规客服、生产调度等，让人类从繁琐的基础性工作中解放出来；人类将聚焦于战略规划、创意设计、情感洞察、复杂问题解决等高价值工作，如企业发展战略制定、产品创意设计、客户情感安抚、复杂技术难题攻克等，这些工作是 AI 技术难以替代的。人机协同的核心是 “扬长避短”，充分发挥智能体的高效、精准、不间断工作的优势，以及人类的创意、情感、战略思维的优势，形成 1+1&gt;2 的协同效应。2026 年后，人机协同能力将成为企业与个人的核心能力，适配人机协同的工作流程与组织架构将成为企业的核心竞争力。</p><h3>3.2 普惠化：AI 技术下沉，惠及更多主体</h3><p>2026 年，AI 技术的普惠化趋势已初步显现，2026 年后，这一趋势将更加明显，AI 技术将持续下沉，从大企业、一线城市、高端行业，向中小企业、县域市场、下沉行业深度渗透，惠及更多的企业、个人与区域，让 AI 技术成为全民可享、全域可用的核心生产力，真正实现 “AI 普惠”。</p><p>AI 技术普惠化的核心是​<strong>持续降低应用门槛与使用成本</strong>​。一方面，低代码、无代码 AI 平台将进一步普及与完善，企业与个人无需专业的 AI 技术知识与开发能力，仅通过可视化操作、拖拽式配置，即可快速搭建专属的 AI 应用与智能体，实现 AI 技术的快速落地；另一方面，AI 服务将向标准化、模块化、轻量化发展，企业可根据自身的需求，按需选择 AI 服务模块，实现 “按需付费、灵活配置”，大幅降低 AI 技术的使用成本。对于中小企业而言，标准化的 AI 服务套餐将成为主流，以极低的成本即可享受高质量的 AI 服务，解决中小企业的业务痛点；对于个人而言，轻量化的 AI 工具将广泛应用于工作、学习、生活的方方面面，如 AI 学习工具、AI 办公工具、AI 生活助手等，提升个人的工作效率与生活质量。</p><p>同时，AI 技术的普惠化还将体现在<strong>区域均衡发展</strong>上。2026 年后，全球算力网络将进一步完善，通过算力调度与共享，实现算力资源的均衡分配，让中西部地区、欠发达国家和地区也能享受到充足的算力资源，为 AI 技术的落地奠定基础；同时，各国政府将出台更多的政策扶持，支持县域市场、下沉行业的 AI 应用落地，推动 AI 技术在农业、乡村旅游、县域制造业等领域的应用，实现区域经济的智能化发展。AI 技术的普惠化将缩小不同企业、不同个人、不同区域之间的数字鸿沟，推动全球经济的均衡、高质量发展。</p><h3>3.3 安全化：技术与监管协同，筑牢安全防线</h3><p>随着 AI 技术的广泛应用与深度融合，AI 技术的安全问题将成为制约其发展的关键因素，如 AI 模型被攻击、数据泄露、隐私被侵犯、AI 决策偏差导致的安全事故、AI 伦理问题等，这些问题不仅会影响企业的发展，还可能威胁到社会的安全与稳定。2026 年后，<strong>AI 安全化</strong>将成为 AI 技术发展的重要方向，技术防护、政策监管、行业自律将协同发力，筑牢 AI 技术发展的安全防线，保障 AI 技术的健康、可持续发展。</p><p>在技术防护方面，AI 安全技术将迎来快速发展，形成全方位的 AI 安全防护体系。AI 模型安全检测技术将实现常态化应用，可实时监测 AI 模型的异常行为，及时发现并防范模型被攻击、被篡改的风险；数据安全与隐私保护技术将进一步升级，通过联邦学习、差分隐私、数据加密等技术，实现 “数据可用不可见”，在保障数据安全与隐私的前提下，推动数据的共享与利用；AI 决策校准技术将不断完善，通过实时数据校准、多源证据验证，降低 AI 决策的偏差率，防范 AI 决策偏差导致的安全事故。</p><p>在政策监管方面，全球 AI 监管框架将进一步完善与协同，形成 “分级分类、全域监管、协同共治” 的监管体系。各国将根据 AI 技术的应用场景与风险等级，制定更加细化、精准的监管规则，对高风险 AI 应用实施严格的安全评估、备案与监管制度，对低风险 AI 应用实施适度监管，鼓励创新；同时，全球各国将加强 AI 监管的国际合作，建立 AI 安全信息共享机制与联合监管机制，防范跨国 AI 安全风险，推动全球 AI 技术的安全、协同发展。</p><p>在行业自律方面，AI 行业组织将发挥重要作用，制定行业内的 AI 伦理规范与安全标准，引导企业规范应用 AI 技术。企业将树立 “AI 安全第一” 的发展理念，建立内部的 AI 安全管理体系，加强 AI 技术应用的安全评估与风险防范，自觉遵守 AI 伦理规范与安全标准，承担起 AI 技术发展的社会责任。</p><p>技术防护、政策监管、行业自律的三重协同，将为 AI 技术的发展筑牢安全防线，保障 AI 技术在安全、规范的框架内实现深度发展，推动智能时代的健康、可持续发展。</p><h2>四、时代应对：个人与企业的破局之道</h2><p>智能时代的正式启幕，既带来了前所未有的发展机遇，也带来了全新的挑战。对于个人而言，AI 技术的广泛应用可能会替代部分传统工作岗位，带来就业压力；对于企业而言，若无法及时适配 AI 技术的发展，将在市场竞争中被淘汰。面对智能时代的变革，个人与企业唯有主动适应变化，找准自身定位，提升核心能力，才能在时代变革中把握先机，实现破局发展。</p><h3>4.1 个人：提升 “AI 素养”，打造 “不可替代” 的核心能力</h3><p>面对 AI 技术的冲击，个人无需过度焦虑，AI 技术替代的只是重复性、执行性的工作岗位，而非人类本身，智能时代的个人发展，核心是​<strong>提升 “AI 素养”，打造 “AI 难以替代” 的核心能力</strong>​，实现与 AI 技术的协同共进。</p><p>首先，要主动提升自身的 “AI 素养”，了解 AI 技术的基本原理、应用场景与发展趋势，学会与 AI 技术、智能体协同工作。个人要主动学习 AI 相关知识与技能，掌握常用的 AI 办公工具、AI 学习工具的使用方法，将 AI 技术作为提升自身工作效率与学习效率的核心工具。例如，职场人士可通过 AI 工具实现文案创作、数据统计、报表生成等工作的高效完成，学生可通过 AI 工具实现个性化学习、精准答疑，让 AI 技术成为自身发展的 “助力器”。</p><p>其次，要聚焦打造 “AI 难以替代” 的核心能力，这些能力是智能时代个人的核心竞争力。AI 技术虽然具备强大的数据分析、逻辑推理、执行操作能力，但在创意设计、情感洞察、复杂问题解决、战略规划、人际交往等方面，仍与人类存在较大差距，这些能力也是智能时代最具价值的能力。个人要根据自身的兴趣、特长与职业规划，重点培养这些核心能力：职场人士可提升自身的创意设计能力、战略思维能力、团队管理能力，让自己成为企业的核心人才；创业者可提升自身的市场洞察能力、创新能力、资源整合能力，打造具有核心竞争力的企业；学生可提升自身的创新思维能力、批判性思维能力、人际交往能力，为未来的职业发展奠定基础。</p><p>最后，要树立<strong>终身学习​</strong>的意识，保持对新技术、新趋势、新行业的敏感度。智能时代的技术迭代速度不断加快，新的业态、新的岗位不断涌现，只有持续学习，不断更新自身的知识体系与能力结构，才能适应时代发展的需求，避免被时代淘汰。个人要主动关注 AI 技术的发展趋势与行业变革，积极学习新的知识与技能，不断提升自身的综合能力，实现个人的持续发展。</p><h3>4.2 企业：以 “业务价值” 为导向，推进 AI 规模化落地</h3><p>2026 年是企业布局 AI 的关键窗口期，面对智能时代的变革，企业的核心发展策略是​<strong>以 “业务价值” 为导向，推进 AI 技术的规模化落地</strong>​，将 AI 技术转化为企业的核心生产力与核心竞争力，实现企业的智能化升级与高质量发展。</p><p>首先，要梳理自身业务痛点，​<strong>筛选 AI 应用的高 ROI 场景</strong>​，避免盲目跟风与技术堆砌。企业推进 AI 落地的核心是解决业务痛点，创造商业价值，而非单纯的追求技术先进。企业要从自身的核心业务出发，梳理生产、运营、销售、服务等环节的业务痛点，筛选出那些重复性强、标准化程度高、人工成本高、AI 技术能快速落地并创造价值的高 ROI 场景，如客服、风控、生产调度、财务报销等，优先实现这些场景的智能化升级，快速看到 AI 技术的商业价值，为后续的 AI 规模化落地奠定基础。</p><p>其次，要选择​<strong>适配自身需求的 AI 技术与平台</strong>​，降低 AI 落地的技术门槛与成本。大型企业可依托自身的技术团队与数据资源，与 AI 技术企业合作，打造定制化的 AI 解决方案，实现全业务链条的智能化升级；中小企业无需投入大量的资金与人力进行定制化开发，可优先采用低代码、无代码 AI 平台与标准化的 AI 服务套餐，通过可视化操作快速搭建专属的智能体与 AI 应用，实现 AI 技术的低成本、快速落地。同时，企业要注重 AI 技术与现有业务系统的融合，实现数据的打通与流程的衔接，避免出现 “信息孤岛” 与 “流程脱节”。</p><p>再次，要建立 **“技术 + 业务” 的协同机制 **，让业务人员全程参与 AI 落地的全流程。AI 技术的落地不是技术团队的单独工作，而是需要技术团队与业务团队的深度协同。业务人员最了解企业的业务痛点与业务需求，技术团队最了解 AI 技术的能力与应用方式，只有两者深度协同，才能确保 AI 技术与业务需求的精准匹配。企业要建立 “技术 + 业务” 的跨部门协同团队，让业务人员全程参与 AI 场景筛选、智能体配置、调试优化等环节，提出业务需求与优化建议，技术团队根据业务人员的建议进行技术调整与优化，确保 AI 技术能够真正融入业务流程，解决业务痛点。</p><p>最后，要注重​<strong>人才培养与组织升级</strong>​，打造适配智能时代的人才队伍与组织架构。企业要加强对现有员工的 AI 培训，提升员工的 AI 素养与人机协同能力，让员工学会与智能体协同工作，适应智能时代的工作方式；同时，企业要根据自身的发展需求，适当引进具备 “懂业务 + 懂 AI” 的复合型人才，负责企业 AI 技术的落地、优化与管理。此外，企业要重构适配 AI 技术与人机协同模式的业务流程与组织架构，简化冗余的流程环节，打破部门之间的壁垒，实现组织的扁平化、高效化，让企业能够快速适应智能时代的市场变化。</p><h2>五、结语：拥抱智能时代，共筑价值共生未来</h2><p>2026 AI 元年，是人工智能发展史上的重要里程碑，更是智能时代正式启幕的历史坐标。这一年，技术的突破、产业的需求、政策的护航，让 AI 技术完成了从 “实验室到产业一线”、从 “概念到实用”、从 “工具到核心生产力” 的关键跨越，AI 普惠化浪潮席卷各行各业，多智能体协同与人机协同成为主流，AI 正在重构产业格局，改变生产生活方式，推动经济社会进入全新的智能发展阶段。</p><p>智能时代的到来，从来不是 AI 替代人类的 “零和博弈”，而是人机协同、价值共生的全新篇章。AI 技术是人类智慧的结晶，其核心价值是解放人类的双手，释放人类的创造力，让人类能够聚焦于更有价值、更有意义的工作，实现人类与技术的共同发展。在智能时代，人类与 AI 不是对立的关系，而是协同共生的关系，充分发挥人类的创意、情感、战略思维与 AI 的高效、精准、不间断工作的优势，才能实现价值的最大化创造。</p><p>站在 2026 AI 元年的历史节点，我们正迎来一个更加智能、更加高效、更加多元、更加美好的未来。对于个人而言，要主动拥抱变化，提升自身的 AI 素养与核心能力，学会与 AI 协同共进，在智能时代实现个人的价值与发展；对于企业而言，要把握时代机遇，以业务价值为导向，推进 AI 技术的规模化落地，将 AI 技术转化为核心竞争力，在智能时代的市场竞争中占据优势；对于社会而言，要构建完善的 AI 监管体系与伦理规范，加强 AI 安全技术的研发与应用，引导 AI 技术的健康、可持续发展，同时关注 AI 技术带来的就业结构变化、数字鸿沟等社会问题，采取有效措施加以解决，让 AI 技术惠及更多的人。</p><p>智能时代的大幕已经拉开，这是一场不可逆的时代变革，也是一次前所未有的发展机遇。让我们携手共进，主动拥抱智能时代，充分发挥 AI 技术的核心价值，实现人机协同、价值共生，共同打造一个更加智能、更加高效、更加美好的未来，让智能时代成为人类发展史上的全新辉煌篇章。</p><h2>六、参考文献</h2><p>[1] 中国信息通信研究院. 2026 人工智能产业发展白皮书 [R]. 北京：中国信通院，2026.<br/>[2] 麦肯锡咨询公司. AI 元年：全球产业变革与发展机遇分析 [R]. 纽约：麦肯锡咨询公司，2026.[3] 欧盟委员会。人工智能法案实施指南与监管框架 [Z]. 布鲁塞尔：欧盟委员会，2026.<br/>[4] 工业和信息化部。新一代人工智能发展规划（2024-2030 年）[Z]. 北京：工信部，2024.<br/>[5] 字节跳动 AI 实验室. 2026 智能体操作系统技术白皮书 [R]. 北京：字节跳动，2026.<br/>[6] 德勤咨询。智能时代：企业 AI 规模化落地实践与指南 [R]. 上海：德勤中国，2026.<br/>[7] 斯坦福大学. 2026 人工智能指数报告 [R]. 斯坦福：斯坦福大学人工智能研究院，2026.</p>]]></description></item><item>    <title><![CDATA[深度探秘 Apache DolphinScheduler 数据库模式 海豚调度 ]]></title>    <link>https://segmentfault.com/a/1190000047578044</link>    <guid>https://segmentfault.com/a/1190000047578044</guid>    <pubDate>2026-01-28 16:06:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578046" alt="数据库模式" title="数据库模式"/></p><p>本文将深入介绍 Apache DolphinScheduler 所采用的数据库模式，此模式主要用于持久化存储工作流定义、执行状态、调度信息以及系统元数据。它具备广泛的兼容性，可支持 MySQL、PostgreSQL 和 H2 等多种数据库，其具体定义存储在 <code>dolphinscheduler - dao/src/main/resources/sql</code> 目录下。</p><h2>模式架构</h2><p>DolphinScheduler 的数据库模式分为七个主要功能组：</p><table><thead><tr><th>组</th><th>目的</th><th>关键表</th></tr></thead><tbody><tr><td>工作流管理</td><td>存储带有版本控制的工作流和任务定义</td><td><code>t_ds_workflow_definition</code>、<code>t_ds_task_definition</code>、<code>t_ds_workflow_task_relation</code></td></tr><tr><td>执行状态</td><td>跟踪运行时实例及其状态</td><td><code>t_ds_workflow_instance</code>、<code>t_ds_task_instance</code>、<code>t_ds_command</code></td></tr><tr><td>调度</td><td>通过 Quartz 管理基于 cron 的调度</td><td><code>t_ds_schedules</code>、<code>QRTZ_*</code> 表</td></tr><tr><td>资源管理</td><td>数据源、文件和 UDF 元数据</td><td><code>t_ds_datasource</code>、<code>t_ds_resources</code>、<code>t_ds_udfs</code></td></tr><tr><td>管理</td><td>用户、租户、项目和权限</td><td><code>t_ds_user</code>、<code>t_ds_tenant</code>、<code>t_ds_project</code></td></tr><tr><td>告警</td><td>告警配置和历史记录</td><td><code>t_ds_alert</code>、<code>t_ds_alertgroup</code></td></tr><tr><td>服务注册</td><td>基于 JDBC 的协调（ZooKeeper 的替代方案）</td><td><code>t_ds_jdbc_registry_*</code> 表</td></tr></tbody></table><h2>工作流和任务定义模型</h2><h3>定义与实例分离</h3><p>DolphinScheduler 严格区分定义（模板）和实例（执行）。这实现了版本控制、并发执行和审计跟踪。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578047" alt="" title="" loading="lazy"/></p><p><strong>关键设计原则</strong>：</p><ul><li><strong>基于代码的标识</strong>：工作流和任务都使用代码（bigint）作为跨版本的稳定标识符。</li><li><strong>复合键</strong>：定义使用（代码，版本）作为复合自然键。</li><li><strong>版本不可变性</strong>：每个版本都是不可变的；更改会创建新版本。</li><li><strong>实例引用</strong>：实例引用特定版本的定义。</li></ul><h2>核心表参考</h2><h3>工作流定义表</h3><h4><code>t_ds_workflow_definition</code></h4><p>工作流模板的主表。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>自动递增主键</td></tr><tr><td>code</td><td>bigint</td><td>唯一工作流标识符（跨版本稳定）</td></tr><tr><td>version</td><td>int</td><td>版本号（默认 1）</td></tr><tr><td>name</td><td>varchar(255)</td><td>工作流名称</td></tr><tr><td>project_code</td><td>bigint</td><td>所属项目</td></tr><tr><td>release_state</td><td>tinyint</td><td>0 = 离线，1 = 在线</td></tr><tr><td>global_params</td><td>text</td><td>JSON 格式的全局参数</td></tr><tr><td>execution_type</td><td>tinyint</td><td>0 = 并行，1 = 串行等待，2 = 串行丢弃，3 = 串行优先级</td></tr><tr><td>timeout</td><td>int</td><td>超时时间（分钟）</td></tr><tr><td>user_id</td><td>int</td><td>创建者用户 ID</td></tr></tbody></table><p><strong>索引</strong>：</p><ul><li><code>UNIQUE KEY workflow_unique (name, project_code)</code></li><li><code>UNIQUE KEY uniq_workflow_definition_code (code)</code></li><li><code>KEY idx_project_code (project_code)</code></li></ul><h4><code>t_ds_workflow_definition_log</code></h4><p>存储工作流定义所有版本的审计日志。</p><p>镜像 <code>t_ds_workflow_definition</code> 的结构，额外列：<code>operator</code>、<code>operate_time</code>，主键：<code>(code, version)</code>。</p><h4><code>t_ds_task_definition</code></h4><p>可在工作流中重用的任务模板。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>code</td><td>bigint</td><td>唯一任务标识符</td></tr><tr><td>version</td><td>int</td><td>版本号</td></tr><tr><td>task_type</td><td>varchar(50)</td><td>Shell、SQL、Python、Spark 等</td></tr><tr><td>task_params</td><td>longtext</td><td>JSON 格式的任务配置</td></tr><tr><td>worker_group</td><td>varchar(255)</td><td>目标工作线程组</td></tr><tr><td>fail_retry_times</td><td>int</td><td>失败重试次数</td></tr><tr><td>fail_retry_interval</td><td>int</td><td>重试间隔（分钟）</td></tr><tr><td>timeout</td><td>int</td><td>任务超时时间（分钟）</td></tr><tr><td>cpu_quota</td><td>int</td><td>CPU 限制（-1 = 无限制）</td></tr><tr><td>memory_max</td><td>int</td><td>内存限制（MB，-1 = 无限制）</td></tr></tbody></table><h4><code>t_ds_workflow_task_relation</code></h4><p>通过指定任务之间的边来定义 DAG 结构。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>workflow_definition_code</td><td>bigint</td><td>父工作流</td></tr><tr><td>workflow_definition_version</td><td>int</td><td>工作流版本</td></tr><tr><td>pre_task_code</td><td>bigint</td><td>前置任务（根节点为 0）</td></tr><tr><td>post_task_code</td><td>bigint</td><td>后置任务</td></tr><tr><td>condition_type</td><td>tinyint</td><td>0 = 无，1 = 判断，2 = 延迟</td></tr><tr><td>condition_params</td><td>text</td><td>JSON 格式的条件配置</td></tr></tbody></table><p><strong>注意</strong>：<code>pre_task_code = 0</code> 表示根节点（无前驱任务）。</p><h3>执行状态表</h3><h4><code>t_ds_workflow_instance</code></h4><p>工作流的运行时执行记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>主键</td></tr><tr><td>workflow_definition_code</td><td>bigint</td><td>引用定义</td></tr><tr><td>workflow_definition_version</td><td>int</td><td>本次执行锁定的版本</td></tr><tr><td>state</td><td>tinyint</td><td>0 = 提交，1 = 运行中，2 = 暂停准备，3 = 已暂停，4 = 停止准备，5 = 已停止，6 = 失败，7 = 成功，8 = 需要容错，9 = 已终止，10 = 等待，11 = 等待依赖</td></tr><tr><td>state_history</td><td>text</td><td>状态转换日志</td></tr><tr><td>start_time</td><td>datetime</td><td>执行开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>执行结束时间</td></tr><tr><td>command_type</td><td>tinyint</td><td>0 = 开始，1 = 从当前开始，2 = 恢复，3 = 恢复暂停，4 = 从失败处开始，5 = 补充，6 = 调度，7 = 重新运行，8 = 暂停，9 = 停止，10 = 恢复等待</td></tr><tr><td>host</td><td>varchar(135)</td><td>执行此工作流的主服务器主机</td></tr><tr><td>executor_id</td><td>int</td><td>触发执行的用户</td></tr><tr><td>tenant_code</td><td>varchar(64)</td><td>用于资源隔离的租户</td></tr><tr><td>next_workflow_instance_id</td><td>int</td><td>用于串行执行模式</td></tr></tbody></table><p><strong>索引</strong>：</p><ul><li><code>KEY workflow_instance_index (workflow_definition_code, id)</code></li><li><code>KEY start_time_index (start_time, end_time)</code></li></ul><h4><code>t_ds_task_instance</code></h4><p>单个任务的运行时执行记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>int</td><td>主键</td></tr><tr><td>task_code</td><td>bigint</td><td>引用任务定义</td></tr><tr><td>task_definition_version</td><td>int</td><td>锁定的版本</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>父工作流实例</td></tr><tr><td>state</td><td>tinyint</td><td>与 <code>workflow_instance</code> 相同的状态值</td></tr><tr><td>submit_time</td><td>datetime</td><td>提交到队列的时间</td></tr><tr><td>start_time</td><td>datetime</td><td>实际执行开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>执行结束时间</td></tr><tr><td>host</td><td>varchar(135)</td><td>执行任务的工作线程主机</td></tr><tr><td>execute_path</td><td>varchar(200)</td><td>工作线程上的工作目录</td></tr><tr><td>log_path</td><td>text</td><td>日志文件路径</td></tr><tr><td>retry_times</td><td>int</td><td>当前重试次数</td></tr><tr><td>var_pool</td><td>text</td><td>供下游任务使用的变量</td></tr></tbody></table><p><strong>索引</strong>：<code>KEY idx_task_instance_code_version (task_code, task_definition_version)</code></p><h3>命令模式与工作流执行</h3><h4>命令队列</h4><p><code>t_ds_command</code> 表实现了基于队列的执行模型，其中命令触发工作流实例。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578048" alt="" title="" loading="lazy"/></p><h4><code>t_ds_command</code> 结构</h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>command_type</td><td>tinyint</td><td>0 = 开始，1 = 从当前开始，2 = 恢复，3 = 恢复暂停，4 = 从失败处开始，5 = 补充，6 = 调度，7 = 重新运行，8 = 暂停，9 = 停止</td></tr><tr><td>workflow_definition_code</td><td>bigint</td><td>目标工作流</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>用于恢复/重新执行操作</td></tr><tr><td>workflow_instance_priority</td><td>int</td><td>0 = 最高，1 = 高，2 = 中，3 = 低，4 = 最低</td></tr><tr><td>command_param</td><td>text</td><td>JSON 格式的执行参数</td></tr><tr><td>worker_group</td><td>varchar(255)</td><td>目标工作线程组</td></tr><tr><td>tenant_code</td><td>varchar(64)</td><td>执行的租户</td></tr><tr><td>dry_run</td><td>tinyint</td><td>0 = 正常，1 = 试运行（无实际执行）</td></tr></tbody></table><p><strong>处理流程</strong>：</p><ol><li>通过 API、调度程序或重试逻辑将命令插入 <code>t_ds_command</code>。</li><li>主服务器的 <code>MasterSchedulerThread</code> 持续扫描该表（按优先级、id 排序）。</li><li>主服务器生成 <code>t_ds_workflow_instance</code> 记录。</li><li>主服务器分析 DAG 并为就绪任务创建 <code>t_ds_task_instance</code> 记录。</li><li>成功处理的命令将被删除；失败的命令将移动到 <code>t_ds_error_command</code>。</li></ol><h2>版本控制系统</h2><h3>基于代码的版本控制模型</h3><p>DolphinScheduler 使用复杂的版本控制系统，支持：</p><ul><li>不同版本的并发执行。</li><li>安全更新而不影响正在运行的实例。</li><li>完整的变更审计跟踪。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578049" alt="" title="" loading="lazy"/></p><h3>版本管理规则</h3><ul><li><strong>当前版本表</strong>：只有“当前”版本存在于 <code>t_ds_workflow_definition</code> 和 <code>t_ds_task_definition</code> 中。</li><li><strong>日志表</strong>：所有版本保存在 <code>*_log</code> 表中，具有 <code>UNIQUE KEY (code, version)</code>。</li><li><strong>在线状态</strong>：每个代码只能有一个版本的 <code>release_state = 1</code>（在线）。</li><li><strong>实例锁定</strong>：工作流实例在创建时锁定到特定版本。</li><li><strong>版本不可变性</strong>：一旦某个版本被实例引用，其日志记录即为不可变。</li></ul><h2>调度体系架构</h2><h3>Quartz 集成</h3><p>DolphinScheduler 集成了 Quartz 调度程序以实现基于 cron 的调度。模式包括标准 Quartz 表以及一个映射表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578050" alt="" title="" loading="lazy"/></p><h4><code>t_ds_schedules</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>workflow_definition_code</td><td>bigint</td><td>目标工作流（唯一）</td></tr><tr><td>start_time</td><td>datetime</td><td>调度活动开始时间</td></tr><tr><td>end_time</td><td>datetime</td><td>调度活动结束时间</td></tr><tr><td>timezone_id</td><td>varchar(40)</td><td>cron 表达式的时区</td></tr><tr><td>crontab</td><td>varchar(255)</td><td>cron 表达式</td></tr><tr><td>release_state</td><td>int</td><td>0 = 离线，1 = 在线</td></tr><tr><td>failure_strategy</td><td>int</td><td>失败时的行为</td></tr><tr><td>workflow_instance_priority</td><td>int</td><td>实例的默认优先级</td></tr></tbody></table><p><strong>Quartz 表要点</strong>：</p><ul><li><code>QRTZ_TRIGGERS.NEXT_FIRE_TIME</code>：已索引，便于高效扫描。</li><li><code>QRTZ_CRON_TRIGGERS.CRON_EXPRESSION</code>：解析后的 cron 定义。</li><li><code>QRTZ_SCHEDULER_STATE</code>：跟踪 Quartz 调度程序实例。</li></ul><h2>资源和配置表</h2><h3>数据源管理</h3><h4><code>t_ds_datasource</code></h4><p>存储 SQL 任务的数据库连接配置。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>name</td><td>varchar(64)</td><td>数据源名称</td></tr><tr><td>type</td><td>tinyint</td><td>数据库类型（MySQL、PostgreSQL、Hive 等）</td></tr><tr><td>connection_params</td><td>text</td><td>JSON 格式的连接配置（主机、端口、数据库、凭据）</td></tr><tr><td>user_id</td><td>int</td><td>所有者用户</td></tr></tbody></table><p><strong>约束</strong>：<code>UNIQUE KEY (name, type)</code> - 防止数据源重复。</p><h3>文件资源</h3><h4><code>t_ds_resources</code>（已弃用）</h4><p><strong>注意</strong>：此表在模式中已标记为弃用。资源元数据正在迁移到单独的存储后端。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>full_name</td><td>varchar(128)</td><td>包括租户的完整路径</td></tr><tr><td>type</td><td>int</td><td>文件类型（文件/UDF）</td></tr><tr><td>size</td><td>bigint</td><td>文件大小（字节）</td></tr><tr><td>is_directory</td><td>boolean</td><td>目录标志</td></tr><tr><td>pid</td><td>int</td><td>父目录 ID</td></tr></tbody></table><h2>多租户与管理</h2><h3>项目、用户和租户层次结构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578051" alt="" title="" loading="lazy"/></p><h4>关键管理表</h4><h4><code>t_ds_tenant</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>tenant_code</td><td>varchar(64)</td><td>唯一租户标识符（唯一）</td></tr><tr><td>queue_id</td><td>int</td><td>任务的默认 YARN 队列</td></tr><tr><td>description</td><td>varchar(255)</td><td>租户描述</td></tr></tbody></table><p><strong>默认租户</strong>：系统创建一个默认租户，<code>id = -1</code>，<code>tenant_code = 'default'</code>。</p><h4><code>t_ds_user</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>user_name</td><td>varchar(64)</td><td>登录用户名（唯一）</td></tr><tr><td>user_password</td><td>varchar(64)</td><td>哈希密码</td></tr><tr><td>user_type</td><td>tinyint</td><td>0 = 普通用户，1 = 管理员</td></tr><tr><td>tenant_id</td><td>int</td><td>关联的租户（默认 -1）</td></tr><tr><td>email</td><td>varchar(64)</td><td>电子邮件地址</td></tr><tr><td>state</td><td>tinyint</td><td>0 = 禁用，1 = 启用</td></tr></tbody></table><h4><code>t_ds_project</code></h4><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>code</td><td>bigint</td><td>唯一项目代码（唯一）</td></tr><tr><td>name</td><td>varchar(255)</td><td>项目名称（唯一）</td></tr><tr><td>user_id</td><td>int</td><td>创建者/所有者</td></tr><tr><td>description</td><td>varchar(255)</td><td>项目描述</td></tr></tbody></table><h2>JDBC 注册表</h2><p>对于不使用 ZooKeeper 的部署，DolphinScheduler 提供基于 JDBC 的注册表用于服务协调。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578052" alt="" title="" loading="lazy"/></p><h3>注册表详情</h3><h4><code>t_ds_jdbc_registry_data</code></h4><p>存储类似于 ZooKeeper 节点的注册表项。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>data_key</td><td>varchar(256)</td><td>类似路径的键（唯一）</td></tr><tr><td>data_value</td><td>text</td><td>序列化数据</td></tr><tr><td>data_type</td><td>varchar(64)</td><td><code>EPHEMERAL</code>（客户端断开连接时删除）或 <code>PERSISTENT</code></td></tr><tr><td>client_id</td><td>bigint</td><td>所属客户端</td></tr></tbody></table><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>last_update_time</td><td>timestamp</td><td>上次修改时间</td></tr></tbody></table><h4><code>t_ds_jdbc_registry_lock</code></h4><p>实现分布式锁。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>lock_key</td><td>varchar(256)</td><td>锁标识符（唯一）</td></tr><tr><td>lock_owner</td><td>varchar(256)</td><td>持有锁的客户端（格式：ip_processId）</td></tr><tr><td>client_id</td><td>bigint</td><td>所属客户端</td></tr></tbody></table><h4><code>t_ds_jdbc_registry_client_heartbeat</code></h4><p>跟踪活动客户端以清理临时数据。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>id</td><td>bigint</td><td>客户端 ID（主键）</td></tr><tr><td>client_name</td><td>varchar(256)</td><td>客户端标识符</td></tr><tr><td>last_heartbeat_time</td><td>bigint</td><td>上次心跳时间戳</td></tr><tr><td>connection_config</td><td>text</td><td>连接元数据</td></tr></tbody></table><p><strong>清理逻辑</strong>：当客户端的心跳过期时，其临时注册表数据和锁将自动删除。</p><h2>告警系统</h2><h3>告警表</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578053" alt="" title="" loading="lazy"/></p><h4><code>t_ds_alert</code></h4><p>由工作流/任务失败或完成生成的告警记录。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>title</td><td>varchar(512)</td><td>告警标题</td></tr><tr><td>sign</td><td>char(40)</td><td>内容的 SHA1 哈希值（用于去重）</td></tr><tr><td>content</td><td>text</td><td>告警消息正文</td></tr><tr><td>alert_status</td><td>tinyint</td><td>0 = 等待，1 = 成功，2 = 失败</td></tr><tr><td>warning_type</td><td>tinyint</td><td>1 = 工作流成功，2 = 工作流/任务失败</td></tr><tr><td>workflow_instance_id</td><td>int</td><td>源工作流实例</td></tr><tr><td>alertgroup_id</td><td>int</td><td>目标告警组</td></tr></tbody></table><p><strong>索引</strong>：<code>KEY idx_sign (sign)</code> - 实现去重。</p><h4><code>t_ds_alertgroup</code></h4><p>告警通道组。</p><table><thead><tr><th>列</th><th>类型</th><th>描述</th></tr></thead><tbody><tr><td>group_name</td><td>varchar(255)</td><td>唯一组名</td></tr><tr><td>alert_instance_ids</td><td>varchar(255)</td><td>逗号分隔的插件实例 ID</td></tr><tr><td>description</td><td>varchar(255)</td><td>组描述</td></tr></tbody></table><h2>索引与查询优化</h2><h3>关键索引</h3><p>该模式包含针对常见查询模式精心设计的索引：</p><ul><li><strong>工作流和任务查找</strong></li></ul><pre><code>- 按定义查询工作流实例：
  `KEY workflow_instance_index (workflow_definition_code, id)`
  - 按定义查询任务实例：
  `KEY idx_task_instance_code_version (task_code, task_definition_version)`
  - 用于监控的时间范围查询*：
  `KEY start_time_index (start_time, end_time)`</code></pre><ul><li><strong>命令处理</strong>：</li></ul><pre><code>基于优先级的命令扫描：
`KEY priority_id_index (workflow_instance_priority, id)`</code></pre><ul><li><strong>DAG 关系查询</strong></li></ul><pre><code>- 正向和反向 DAG 遍历：
  `KEY idx_pre_task_code_version (pre_task_code, pre_task_version)`
   正向和反向 DAG 遍历：
   `KEY idx_post_task_code_version (post_task_code, post_task_version)`
  `KEY idx_code (project_code, workflow_definition_code)`</code></pre><h3>唯一约束</h3><p>在数据库级别强制执行的关键业务规则：</p><table><thead><tr><th>表</th><th>约束</th><th>目的</th></tr></thead><tbody><tr><td><code>t_ds_workflow_definition</code></td><td><code>UNIQUE (name, project_code)</code></td><td>项目中无重复的工作流名称</td></tr><tr><td><code>t_ds_workflow_definition</code></td><td><code>UNIQUE (code)</code></td><td>全局工作流标识符</td></tr><tr><td><code>t_ds_workflow_definition_log</code></td><td><code>UNIQUE (code, version)</code></td><td>每个版本一条记录</td></tr><tr><td><code>t_ds_datasource</code></td><td><code>UNIQUE (name, type)</code></td><td>每种类型无重复的数据源名称</td></tr><tr><td><code>t_ds_schedules</code></td><td><code>UNIQUE (workflow_definition_code)</code></td><td>每个工作流一个调度</td></tr></tbody></table><h2>模式演变与升级</h2><p>DolphinScheduler 在 <code>dolphinscheduler - dao/src/main/resources/sql/upgrade</code> 中维护用于跨版本模式迁移的升级脚本。</p><h3>近期模式变更</h3><h4>3.3.0 变更</h4><ul><li>将表和列从“process”重命名为“workflow”。</li><li>删除数据质量表（<code>t_ds_dq_*</code>）。</li><li>添加用于替代 ZooKeeper 的 JDBC 注册表。</li><li>从任务表中删除与缓存相关的列。</li></ul><h4>3.2.0 变更</h4><ul><li>向工作流定义中添加 <code>execution_type</code>（并行/串行模式）。</li><li>为串行执行链添加 <code>next_workflow_instance_id</code>。</li><li>向命令和实例表中添加 <code>tenant_code</code>。</li><li>创建 <code>t_ds_project_parameter</code> 和 <code>t_ds_project_preference</code>。</li></ul><h2>数据库交互模式</h2><h3>服务层访问</h3><p>数据库访问通过 <code>dolphinscheduler - dao</code> 中的 DAO 层进行抽象。<br/><strong>关键服务类</strong>：</p><ul><li><code>ProcessService</code>：工作流/任务定义和实例的 CRUD 操作。</li><li><code>CommandService</code>：命令队列管理。</li><li><code>ProjectService</code>：项目和权限管理。</li><li><code>ResourcesService</code>：资源元数据操作。</li></ul><h3>事务管理</h3><p>大多数操作使用 Spring 的 <code>@Transactional</code> 注解实现：</p><ul><li>原子性地创建工作流实例及其任务实例。</li><li>消费命令并创建实例。</li><li>版本更新与日志表同步。</li></ul><h3>连接池</h3><p>系统使用 HikariCP 进行连接池，在 <code>application.yaml</code> 中配置：</p><ul><li>默认池大小：50 个连接。</li><li>连接超时：30 秒。</li><li>空闲超时：600 秒。</li></ul>]]></description></item><item>    <title><![CDATA[2026泛监测平台推荐榜单发布：自适应 · 协同 · 可洞察型平台谁在领跑？ 底层逻辑探索 ]]></title>    <link>https://segmentfault.com/a/1190000047578088</link>    <guid>https://segmentfault.com/a/1190000047578088</guid>    <pubDate>2026-01-28 16:05:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化与数据化加速融合的背景下，泛监测平台正从“单一监控工具”升级为“覆盖数据、接口、行为、风险的综合治理中枢”。本文从自适应能力、协同能力、可洞察能力三个核心维度出发，对国内主流泛监测平台进行系统评析与专业推荐。<br/><strong>一、泛监测平台的发展趋势与能力演</strong>进<br/>提示：要理解平台价值，首先要看泛监测从“被动感知”走向“主动洞察”的能力跃迁。<br/>传统监测系统更多停留在日志收集、告警触发与基础审计层面，而新一代泛监测平台正向“全域感知 + 智能分析 + 协同治理”方向演进，主要呈现三大趋势：<br/>第一，从“静态规则”走向“自适应分析”。新一代平台引入AI模型、UEBA行为分析、无监督学习机制，使系统可以根据用户行为、业务变化和数据流动情况动态调整监测策略，避免长期依赖固定规则带来的高误报与低发现率问题。<br/>第二，从“孤岛式部署”走向“协同式联动”。平台不再是单点工具，而是与SOC、SIEM、工单系统、数据治理平台、API网关等系统协同运行，形成跨部门、跨系统、跨流程的风险治理闭环。<br/>第三，从“可见”走向“可洞察”。监测不只停留在“看到异常”，而是要做到“理解风险、还原路径、预测趋势”，实现从事件级监控到资产级、行为级、业务级洞察的升级。<br/><strong>二、泛监测平台核心能力模型</strong><br/>提示：评估一个平台是否优秀，必须回到自适应、协同、可洞察这三个关键指标。<br/>自适应能力优秀的泛监测平台应具备自动学习、动态校准、策略自进化能力，包括：<br/>● 自动识别业务变化对数据流动的影响<br/>● 动态调整风险阈值与监测重点<br/>● 在新接口、新系统上线时快速纳入监测范围<br/>协同能力平台要具备良好的开放性与编排能力：<br/>● 与SOC/SIEM/工单系统联动处置<br/>● 与数据分类分级、数据资产管理系统协同<br/>● 与API网关、零信任体系联动防护<br/>可洞察能力不仅“发现问题”，还要“理解问题”：<br/>● 构建数据资产地图与流动视图<br/>● 实现风险路径还原与影响面评估<br/>● 提供趋势预测与治理建议能力<br/><strong>三、2025 年泛监测平台产品推荐排名</strong><br/>提示：在综合技术成熟度、场景适配度与市场验证后，以下是通用行业适用的核心产品梯队。<br/>第一名：奇安信 泛监测与数据治理平台<br/>奇安信平台以“全域感知 + 零信任联动”为核心优势，在大型政企、金融与基础设施行业拥有广泛应用。<br/>其泛监测体系覆盖数据库、API、云存储、大数据平台等多个维度，结合用户行为分析与流量建模技术，构建“数据—行为—风险”全链路视图。<br/>在自适应方面，平台通过AI模型不断校准风险基线，对异常导出、越权访问、接口滥用等场景具备较高识别准确率。在协同方面，奇安信与自身SOC、终端安全、网络安全体系深度联动，形成跨域响应闭环。在可洞察方面，其数据流动可视化能力成熟，适合对安全可控要求极高的客户群体。<br/>第二名：全知科技 泛监测与数据安全协同平台<br/>全知科技将“API安全即数据安全核心关口”的理念引入泛监测领域，构建了以数据资产地图 + API风险监测 + 智能分析引擎为核心的协同型监测体系。<br/>在自适应能力方面：全知科技通过AI驱动的数据分类分级与行为建模，使平台可根据业务变化动态调整监测重点。系统能够自动扫描表结构、接口结构、调用路径，生成实时资产图谱，敏感数据识别准确率达95%，效率相比人工提升90%以上。<br/>在协同能力方面：全知科技强调“理念—技术—场景”的协同创新，其泛监测平台可与数据治理系统、合规审计系统、工单系统联动运行，实现从发现风险到整改闭环的自动协同。同时，其“知影-API风险监测系统”与“知形-数据库风险监测系统”构成前后端联动，覆盖数据生产、调用、流转与使用全链路。<br/>在可洞察能力方面：全知科技突出“可知、可管、可控、可见”的能力体系，不仅能看到风险事件，更能还原风险路径、定位责任主体、评估影响范围。在金融、医疗、保险等场景中，平台已实现对异常API调用、数据越权访问、敏感字段泄露的秒级溯源。<br/>典型实践中，某三甲医院部署后旧版API泄露风险下降98%；在金融行业实现数据资产从“看不见”到“全闭环可控”的治理跃迁。<br/>第三名：启明星辰 泛监测与风险闭环平台<br/>启明星辰依托“九天·泰合”智能引擎，在风险识别与闭环治理方面表现突出。<br/>平台支持跨数据库、API、BI工具的统一监测与审计，能够基于角色、行为与数据敏感度动态调整访问策略。在自适应方面，其策略引擎可结合用户行为画像不断修正风险模型；在协同方面，平台与政务SOC体系、日志审计平台高度融合；在可洞察方面，适合对审计合规与流程闭环要求极高的组织。<br/>第四名：天融信 泛监测与数据流动治理平台<br/>天融信在工业互联网与跨网环境下的泛监测能力具有明显优势。<br/>其动态数据流向地图技术可在复杂网络隔离场景下追踪数据流动路径。平台强调与防火墙、终端安全系统的协同防护，适用于制造、能源等对跨域数据交互敏感的行业。<br/>第五名：阿里云 数据安全中心（DSC）<br/>阿里云DSC基于云原生架构，在多云与互联网企业场景中优势明显。<br/>其自动发现、分类分级与异常行为检测能力成熟，适合云上资产规模大、变化快的客户。在自适应方面依托云侧AI模型；在协同方面与阿里云生态产品联动紧密；在可洞察方面更侧重于云资源与数据使用行为分析。<br/>第六名：深信服 泛监测与零信任协同平台<br/>深信服强调轻量化部署与零信任融合。<br/>平台适合中型组织快速构建“身份 + 数据 + 行为”一体化监测能力，在教育、医疗、中小企业市场适配性强。<br/><strong>四、泛监测平台选型与落地建议</strong><br/>提示：选平台不是买功能，而是选择“是否能长期协同业务演进”的能力体系。<br/>明确业务驱动场景优先从高频、高风险数据场景切入，如API调用、BI报表导出、批量下载等。<br/>验证自适应能力重点测试平台是否能在业务变化后自动纳入新系统、新接口、新数据类型的监测范围。<br/>关注协同治理能力选择能与现有SOC、数据治理、工单系统协同的平台，避免形成新的工具孤岛。<br/>重视可洞察输出不仅要看告警数量，更要看是否提供“风险路径、影响评估与治理建议”。<br/><strong>五、结语：泛监测进入“洞察驱动治理”阶段</strong><br/>提示：未来的泛监测平台，核心竞争力将不再是“监控多少”，而是“洞察多深、协同多强”。<br/>2025年的泛监测平台市场已经从“合规达标”走向“价值创造”。企业需要的不是更多工具，而是一个能自适应业务变化、能协同各类系统、能真正洞察数据风险本质的综合治理中枢。<br/>在这一趋势下，以全知科技为代表的“协同型、洞察型泛监测平台”，正推动企业从被动防守转向主动治理，为构建以数据为中心的新型安全体系奠定坚实基础。</p>]]></description></item><item>    <title><![CDATA[湖流一体：基于  Fluss+ Paimon 的实时湖仓数据底座 ApacheFlink ]]></title>    <link>https://segmentfault.com/a/1190000047578091</link>    <guid>https://segmentfault.com/a/1190000047578091</guid>    <pubDate>2026-01-28 16:05:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>摘要：本文整理自阿里云高级技术专家、Apache Flink PMC 成员、Apache Fluss PPMC 成员 伍翀老师，在 Flink Forward Asia 2025 城市巡回深圳站中的分享。</p><p>Tips：关注「Apache Flink公众号」回复 FFA 2025 查看会后资料～</p></blockquote><h2>一、问题起点：分析型流处理系统的缺失</h2><p>在大数据处理领域，我们通常将系统划分为四个象限：</p><ul><li><strong>纵轴</strong>：批处理 vs 流处理 </li><li><strong>横轴</strong>：业务型 vs 分析型</li></ul><p>会得到四个象限：</p><ul><li>MySQL、PostgreSQL：<strong>业务型 + 批处理</strong></li><li>Kafka、Pulsar：<strong>业务型 + 流处理</strong></li><li>Snowflake、Iceberg：<strong>分析型 + 批处理</strong> </li></ul><p>但你会发现——<strong>分析型 + 流处理</strong> 这一块，几乎是空白的。</p><p>因此，<strong>Fluss 的定位非常明确：填补这个空白，做一个面向分析型场景的实时流存储系统</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578093" alt="" title=""/></p><h2>二、Fluss 是什么？</h2><h3>Fluss核心架构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578094" alt="" title="" loading="lazy"/></p><p>Fluss 的整体架构和传统的 Kafka 比较类似，本质上是一个<strong>带服务的存储系统</strong>。数据会在 Fluss 的 Server 之间进行三副本、高可用、持久化存储。同时，系统会结合远程的 HDFS 或对象存储实现数据分层，将数据按冷热与生命周期进行合理划分。</p><p>在数据分层方面，Fluss 会将长周期的历史数据持续下沉到数据湖格式中，用于更长周期的数据存储与各类分析型场景。同时，这几层不同形态、不同介质上的分层数据，可以进行联合查询，我们称之为 Union Read。用户无需关注底层的存储细节，依然通过同一套 SQL API，即可对最底层的多层数据进行数据合并，并保证数据的一致性，在上层看到的仍是一张表的统一视图。</p><p>此外，Fluss 还提供了流式读取、流式写入、实时更新、实时写入、点查以及维表查询等能力。</p><h3>核心应用场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578095" alt="" title="" loading="lazy"/></p><p>Fluss 在阿里内部、阿里云以及部分业界的核心业务场景中已经有了较多应用。当前主要有两个新的核心应用方向：</p><p>一方面是 Fluss + Flink，用来替代传统的 Kafka，构建实时数仓，形成一种新的实时数仓范式；</p><p>另一方面是 Fluss + Paimon，用来构建流批一体、秒级响应的湖仓架构，我们将这一架构称为<strong>湖流一体</strong>。</p><p>本次议题的重点主要在于介绍湖流一体的架构及其应用场景。不过在进入该部分之前，会先快速介绍 Fluss + Flink 替代 Kafka 构建实时数仓时，所提供的一些核心能力及其解决的问题。</p><h2>三、 Fluss + Flink 实时数仓场景</h2><p>整体梳理下来，Fluss 与 Flink 配合用于实时数仓建设，主要具备四个核心特性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578096" alt="" title="" loading="lazy"/></p><p>Fluss 的第一个核心能力是「<strong>流式查询下推</strong>」。与 Kafka 提供行式数据流（如 JSON、Avro）不同，Fluss 基于 Apache Arrow 构建列式流式日志系统，在磁盘侧即以列存格式组织数据。当下游仅需部分列时，可直接读取并传输所需列，端到端采用 Zero Copy，避免中间序列化/反序列化，显著降低网络、CPU 与解析开销。列裁剪之外，Fluss 还支持分区裁剪与条件下推：查询条件（如“双11当天”或特定业务分区）可下推至服务端，跳过无关数据。</p><p>第二个能力是「<strong>实时数仓的分层化</strong>」。借助毫秒级读写、实时更新及完整 Changelog 能力，Fluss 可贯通 ODS、DWD、DWS 等层级，构建分层清晰的端到端实时数据管道，弥补传统 Kafka 架构在数仓分层建设上的不足。</p><p>第三个能力是「<strong>实时宽表构建</strong>」。基于 Fluss + Flink，通过 Delta Join 等新范式替代传统双流 Join，简化状态管理，提升链路可维护性与版本升级体验，并支持 Partial Update 等多表实时拼接方式。同时，Fluss 提供面向大数据场景优化的「异步维表」能力，作为高吞吐外部维表被 Flink 查询，通过异步化、批量化、流水线化等优化，显著提升维表查询吞吐性能。</p><p>第四个能力是「<strong>MergeEngine 合并机制</strong>」。Fluss 在服务端提供或规划了类似 Paimon 的合并语义，包括 去重合并引擎  FistRow/LastRow/Versioned MergeEngine，也正在支持聚合合并引擎 Aggregate Merge Engine，已支撑实时长周期聚合指标和用户画像等场景。</p><h2>四、“湖流一体”：Fluss 与 Paimon 的协同架构</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578097" alt="" title="" loading="lazy"/></p><p>这是一个 Fluss + Paimon 的湖流一体 High Level 架构图。整个体系中，Fluss 能够与 Paimon 或者类似的湖仓框架（如 Iceberg）做无缝集成，对用户来说几乎就像在使用一个「统一的数据库」，只是底层会根据不同的数据特性和时效性需求做冷热分层：</p><ul><li><strong>热数据存放在高性能介质上；</strong></li><li><strong>冷数据以更高压缩率存放在更低成本的介质上。</strong></li></ul><p>这一整套冷热分层和数据移动的过程，都由系统自动完成，无需用户干预。用户在读取「这张表」时，不需要关心数据具体位于哪一层存储，系统会自动将多层数据进行拼接，对外呈现为一份完整结果。这个跨分层拼接并统一查询的能力，在 Fluss 中称为 Union Read。</p><p>Fluss 将数据自动落到 Paimon 等湖仓时，严格遵循 Paimon / Iceberg 等系统原生的开放协议。因此，现有的湖仓生态和查询引擎可以无缝对接与访问 Paimon 中的数据，不会破坏或影响已有的离线链路与计算体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578098" alt="" title="" loading="lazy"/></p><p>在展开介绍这个湖流一体架构之前，先简单聊一下业界在湖流融合方向上的一些趋势。这条路并不是只有我们在走，业界很多流存储厂商其实都在向这个方向演进。</p><p>在 2023 年，我们启动了 Fluss 项目，并首次提出「湖流一体」的概念。随后在 2024 年，Kafka 背后的商业公司 Confluent 推出了 Tableflow 产品。Tableflow 的核心目标，就是把 Kafka 中的数据无缝同步到 Iceberg 上。此后一两年内，市面上流存储相关的厂商也陆续推出了类似产品，比如 Redpanda、StreamNative、Upstash 等，都开始提供类似的「流到湖」的数据打通方案。</p><p>从这些公司的产品设计上，可以看到两个共同点：</p><ol><li><strong>都是从 Kafka 到 Iceberg</strong>  <br/>也就是做「流到湖」的数据通道，解决的是：Kafka 里的数据如何高效落到 Iceberg。  <br/>但反向的问题——Iceberg 里的数据如何真正被流系统复用、为流计算所用——他们还没有去做，或者至少没有给出清晰的产品化方案。</li><li><strong>都是围绕 Kafka 生态的公司</strong>  <br/>这些公司本质上都是做 Kafka 或 Kafka 兼容服务的厂商，提供的是 Kafka API 兼容的消息队列 / 流存储服务。所以它们的设计天然是「以 Kafka 为中心」，在 Kafka 外挂一个往湖仓（如 Iceberg）同步数据的组件或服务，所以也会受到 Kafka API 在与湖仓集成时的各种限制。</li></ol><p>那这种「流到湖」的单向模式和 Fluss 的「湖流一体」之间，在架构理念和能力边界上有什么差异呢？</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578099" alt="" title="" loading="lazy"/></p><p>业界此类产品可分为两类：一类是以 Confluent Tableflow 为代表的「流式入湖」服务，另一类是以 Fluss 为代表的「湖流一体」架构。</p><p>「流式入湖」本质上是单向的数据同步通道，仅解决“如何将流数据从 Kafka 等源搬入数据湖”的问题；而「湖流一体」则聚焦于流与湖的双向数据共享——既让流端数据为湖端所用，也让湖端数据反哺流端，这是设计理念的根本差异。</p><p>在数仓分层上，流式入湖主要服务于 ODS 层的数据接入，后续 DWD、DWS 等层级仍需依赖独立批流作业构建，无法形成闭环；而湖流一体面向全链路实时数仓，旨在弥补 Iceberg、Paimon 等湖仓在秒级数据新鲜度上的不足，覆盖从 ODS 到 DWS 的端到端时效性需求。</p><p>理念层面，流式入湖属于 ETL 接入层能力，关注 Kafka 数据如何写入湖；湖流一体则是「流批一体」战略下的具体落地，以统一存储承载流与批的双重语义。</p><p>成本方面，流式入湖因 Kafka 与湖中同时保留数据副本，导致双份存储开销及潜在一致性风险；湖流一体则通过单一数据拷贝实现流湖共享，仅需一份存储成本。</p><p>开发成本上，流式入湖需为每个 Topic 单独配置复杂参数，接入成本高；而 Fluss 作为与湖仓在数据模型层原生对齐的流存储，开启湖流一体能力仅需一个配置开关，显著降低开发与运维负担。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578100" alt="" title="" loading="lazy"/></p><p>在探讨为何不基于 Kafka 或其 API 来实现湖流一体时，核心原因在于：Kafka 是为消息系统设计的，而非为分析场景设计。这导致在尝试构建湖流一体架构时，会遇到四个基础且难以绕过的问题。</p><ol><li><strong>Kafka 内部没有 Schema</strong></li></ol><p>由于 Kafka 本身是「无 Schema」的，在将其与「有 Schema」的数据湖 / 湖仓体系对接时，会产生大量额外工作，例如：</p><ul><li>需要手动配置每个 Topic 对应的表；</li><li>每张表的 Schema 定义、字段类型和映射关系等都需要手工填写；</li><li>并且这些配置对于每一个 Topic 或表都要单独进行。</li></ul><p>相反，Fluss 作为「有 Schema 的流存储」，只需在目标表上打开一个配置开关，后续的映射和元数据同步工作即可自动完成，大幅降低了使用成本和接入复杂度。</p><ol><li> <strong>数据模型不匹配</strong></li></ol><p>Kafka 的数据模型主要是为微服务和消息队列场景设计的，在对接大数据 / 数仓体系时会出现明显割裂，例如：</p><ul><li>在数据湖 / 数仓中，普遍存在数据库、数据表、分区、分桶等高层数据抽象；</li><li>Kafka 仅提供 Topic 概念，缺乏与上述模型一一对应的元数据体系。</li></ul><p>相比之下，Fluss 从一开始就按「面向数据湖 / 数仓」的方式进行对齐，支持数据库、表、分区、桶，以及变更日志、主键、更新语义等，使得在实施湖流一体时能够无缝融合，无需大量额外的适配逻辑。</p><ol><li><p><strong>不支持更新语义</strong></p><p>数据湖 / 湖仓（如 Iceberg、Paimon）通常支持更新与删除操作，并具备完整的 Changelog / Merge 语义。而 Kafka 的核心模型是追加写日志，不具备真正的记录级更新能力。  <br/>将一个「不支持更新」的系统与「支持更新」的系统深度融合，势必需要处理状态重建、补写、回刷等复杂逻辑，增加系统复杂性与维护难度。</p><p>Fluss 则原生支持更新及 Changelog 语义，可以生成完整的变更日志供下游订阅，从而与湖仓的更新语义自然对齐。</p></li><li><strong>业务场景与 API 语义的矛盾</strong></li></ol><p>Kafka 提供的 API 主要围绕消息语义展开，例如按 Topic + Offset 顺序消费。若要实现真正的湖流一体，不仅需要让流数据写入湖仓，还需要让湖仓中的数据能够反向为流所用，这就要求：</p><ul><li>流系统能够原生地访问湖仓中的数据，且没有额外的转换开销；</li><li>支持按表、分区甚至按条件的灵活读取方式。</li></ul><p>在 Kafka 现有 API 框架下，要实现这种反向能力，意味着需要在服务端执行一系列复杂转换：</p><ul><li>从远程数据湖中读取 Parquet / ORC 等列存文件；</li><li>将其转换回 Kafka 的行式消息格式；</li><li>再通过消息 API 以流的形式回放给消费者。</li></ul><p>这种做法与 Kafka 当前的业务模型存在明显冲突，会使存储与计算路径异常复杂，并引入大量并非消息队列范畴的工作负载。因此，在 Kafka 现有架构和 API 语义下，很难自然地将湖仓数据转变为流的一部分，供流计算直接复用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578101" alt="" title="" loading="lazy"/></p><h2>五、为什么我们选择基于 Fluss 重新构建湖流一体架构？</h2><p>在设计 Fluss 之初，我们就明确了一个核心理念：<strong>不能在 Kafka 的基础上修修补补，而必须从分析型场景的原生需求出发，重新定义流存储</strong>。这背后有三个关键设计：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578102" alt="" title="" loading="lazy"/></p><ol><li><strong>从 Topic 到 Table 的范式转变</strong>  <br/>Kafka 是面向消息系统的，其核心抽象是无 Schema 的 Topic；而 Fluss 以“表（Table）”为第一公民，天然携带 Schema。这使得 Fluss 能与 Paimon、Iceberg 等 Lakehouse 表的 Schema 类型无缝对齐，避免了传统方案中手动维护 Schema 映射的复杂性和出错风险。</li><li><strong>支持完整的数据更新语义</strong>  <br/>湖仓系统普遍支持行级更新（如主键 Upsert），但 Kafka 仅支持追加写入。Fluss 原生支持实时更新，并能生成完整的 Changelog，为下游提供一致的变更数据流，这是实现湖流双向融合的基础。</li><li><strong>列式存储格式的深度优化</strong>  <br/>Fluss 基于 Apache Arrow 构建流式列存日志，不仅支持高效的列裁剪和条件下推，还能与 Lakehouse 的列式文件格式（如 Parquet、ORC）高效对接，极大降低 I/O 和计算开销。</li></ol><h3>内置 Tiering Service：实现湖流自动同步</h3><p>Fluss 内置一个名为 <strong>Tiering Service</strong> 的后台服务（当前基于 Flink 实现，未来可扩展至其他运行时），它会自动将开启了“湖流一体”特性的表数据，持续地从 Fluss 转换为 Paimon 等 Lakehouse 格式，并<strong>精确记录 Lakehouse 快照与 Fluss Log Offset 之间的对应关系</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578103" alt="" title="" loading="lazy"/></p><p>这个 Offset 位点是实现 <strong>Union Read</strong>（统一读取）的关键——它确保了从 Lakehouse 读取的历史数据与从 Fluss 读取的实时数据之间<strong>严格的一致性边界</strong>，从而实现“不多一条、不少一条”的端到端 Exactly-Once 语义。</p><p>更重要的是，一旦数据被成功分层到 Lakehouse，Fluss 便可安全清理旧数据，仅保留短周期（如 6 小时）的热数据。这显著降低了实时存储层的成本，同时不影响全量历史回溯能力。</p><h2>六、 Fluss + Paimon：湖流一体架构的六大核心优势</h2><h3>流存储成本降低 10 倍以上</h3><p>在传统 Lambda 架构中，实时链路（Kafka + Flink）和离线链路（Hive + Spark）各自独立，数据需双份存储。Kafka 通常只能保留 7 天数据，但业务往往需要数月甚至数年的回溯能力——这导致要么牺牲回溯能力，要么承担高昂的 Kafka 存储成本。</p><p>而在 Fluss + Paimon 的湖流一体架构中：</p><ul><li><strong>Lakehouse 存储长期历史数据（月级、年级）</strong></li><li><strong>Fluss 仅保留超短期热数据（如 6 小时）</strong></li></ul><p>用户仍可从几个月前开始完整回溯，且实时消费延迟保持在毫秒级。存储成本可从“7天”降至“6小时”，节省高达 20 倍的存储开销。更重要的是，流批在存储层真正统一，开发者只需面对“一张表”，无需在流/批之间切换逻辑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578104" alt="" title="" loading="lazy"/></p><h3>高效、一致的数据回溯（Backfill）</h3><p>当业务逻辑变更需要重跑过去 30 天的数据时，传统方案需手动拼接离线表与 Kafka 流，一致性难以保障。</p><p>Fluss 的 Union Read 机制自动完成这一过程：</p><ul><li>获取 Paimon 最新快照及其对应的 Fluss Log Offset；</li><li>从 Paimon 并行读取历史数据（支持列裁剪、谓词下推，性能接近批处理）；</li><li>在精确的 Offset 位点无缝切换至 Fluss 流读。</li></ul><p>整个过程自动、高效、强一致，大幅简化数据回填作业。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578105" alt="" title="" loading="lazy"/></p><h3>批查询获得秒级新鲜度</h3><p>传统 Lakehouse 表的新鲜度受限于 Checkpoint 或 Commit 频率（通常为分钟级）。但在 Fluss + Paimon 架构下，批查询可通过 Union Read 同时读取：</p><ul><li><strong>Paimon 中的分钟级历史数据</strong></li><li><strong>Fluss 中的秒级实时数据</strong></li></ul><p>最终结果具备秒级端到端新鲜度，满足实时报表、运营看板等高时效性场景需求。目前 StarRocks、Flink 等引擎均已支持此类 Union 查询。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578106" alt="" title="" loading="lazy"/></p><h3>分层数仓的新鲜度不受层级影响</h3><p>在传统流数仓中，每经过一层（ODS → DWD → DWS），数据可见性都依赖一次 Flink Checkpoint，导致端到端延迟累积（如 5 分钟 × 3 层 = 15 分钟）。</p><p>而 Fluss + Paimon 的湖流一体架构中，<strong>层间数据流动与 Checkpoint 解耦</strong>：</p><ul><li>数据在 Fluss 表之间以毫秒级延迟流动；</li><li>每层 Fluss 表按固定频率（如 3 分钟）同步到 Paimon；</li><li>用户看到的 Paimon 表始终具有<strong>稳定、可预测的新鲜度</strong>。</li></ul><p>这确保了数仓各层级的时效性可控，有效消除了业务开发中“每增加一层就带来额外延迟”的心智负担。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578107" alt="" title="" loading="lazy"/></p><h3>更高效的 CDC 与 Changelog 生成</h3><p>Paimon 原生支持两种 Changelog 生成方式：</p><ul><li><strong>Lookup 模式：资源消耗大；</strong></li><li><strong>Full Compaction 模式：延迟高。</strong></li></ul><p>而 Fluss 本身已维护热数据的索引状态，可在写入时<strong>直接生成高质量的 Changelog</strong>。该 Changelog 一方面用于驱动 Paimon 主表的 Upsert，另一方面可直接 Append 到 Paimon 的 Changelog 表中，<strong>避免重复计算</strong>，实现低延迟、低成本的变更数据捕获。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578108" alt="" title="" loading="lazy"/></p><h3>湖仓的轻量级实时接入层</h3><p>Lakehouse 客户端通常较重，对写入端要求高。Fluss 作为带服务的存储系统，将复杂写入逻辑下沉至服务端，提供轻量 SDK（Java、Python、Rust 等），支持多种写入场景：</p><ul><li>大数据引擎（Flink、Spark）</li><li>IoT 设备</li><li>AI 训练/推理系统（如向量 Embedding 写入）</li></ul><p>尤其在 AI 场景中，Fluss 可作为<strong>高速缓冲层</strong>：</p><ul><li>避免 GPU 计算被对象存储写入阻塞；</li><li>平滑应对数据写入的波峰波谷（削峰填谷）；</li><li>后台持续将数据分层至 Lakehouse。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578109" alt="" title="" loading="lazy"/></p><h2>七、总结</h2><h3>无缝集成，平滑演进</h3><p>Fluss 的设计理念是<strong>不颠覆现有湖仓架构，而是增强其实时能力</strong>。用户只需在现有 Paimon 表上开启“湖流一体”开关，并配置 Fluss endpoint，即可将一张普通表升级为支持毫秒级流读的实时表，<strong>原有链路完全不受影响</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578110" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578111" alt="" title="" loading="lazy"/></p><p>目前，阿里云上的 Fluss 已与 DLF、Paimon 深度集成，提供开箱即用的湖流一体、Union Read 等能力，并可申请免费试用。更多详情可访问：<a href="https://link.segmentfault.com/?enc=IW05EqtY9ZXOXyo10slNcA%3D%3D.aB2nMAmIvlkKVX9JIaiDIooPM0VRQ9fc3boe%2Fp7EcMc5y7WGxlXwfWYrfFNorOAd" rel="nofollow" target="_blank">https://www.aliyun.com/product/flink/fluss</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578112" alt="" title="" loading="lazy"/></p><h3>未来规划</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578113" alt="" title="" loading="lazy"/></p><ol><li><strong>更广泛的查询引擎支持</strong>：StarRocks、Trino、Spark 等已内部对接或社区推进中；</li><li><strong>元数据统一</strong>：支持 Paimon 表一键升级为湖流一体表（<a href="https://link.segmentfault.com/?enc=SLBK14Iz34%2FNx8zl5esMpw%3D%3D.0C%2BSeZ9FLDqSE7IgrfiTsKaFwChm3VAE7Hyj5ELtDEQMs6J%2BPL40pnx7O816Yxv3sh%2F8c91nXb2wJ%2F0Q%2FnYimnk1zjKucsrytOnQlhDePoK10w%2FLccPkTtamPZ6MyuTnS22U92yHp%2Btf9M1ydnFfg6t%2BWoUuqE%2FLwgrbXnyCljdgv91zBaZjFz6JlkN6DsnY" rel="nofollow" target="_blank">PIP-39</a>）；</li><li><strong>高性能 Union Read</strong>：对接 Paimon Deletion Vector，提升主键表的批查性能；</li></ol><p>Fluss 不是另一个 Kafka，也不是简单的“Kafka + Lakehouse 同步工具”。它是面向分析型场景、为湖流一体而生的新一代流存储。通过重新思考流与湖的关系，Fluss 正在推动实时数仓进入“一份存储、统一视图、秒级新鲜、低成本回溯”的新时代。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578114" alt="" title="" loading="lazy"/></p><p>Fluss 团队正在杭州、上海招聘！  <br/>如果你对实时计算、湖仓一体、AI 数据基础设施充满热情，欢迎加入我们，一起改变世界！</p><blockquote><strong>Bring better analytics to data streams, and better data freshness to data lakehouses.</strong></blockquote><h2>阿里云流存储 Fluss 于 2026 年 1 月 13 日 正式开启免费公测</h2><p>基于 Apache Fluss 打造的高性能列式流存储系统，具备毫秒级读写响应、实时数据更新及部分字段更新能力，可替换Kafka构建面向分析的流式存储，结合DLF（Paimon）等数据湖产品构建湖流一体架构。</p><p>公测活动： 公测期间单用户可免费使用2个集群，单个集群上限80 Core，如果您在使用过程中向我们提出改进建议或评测报告，我们将依据反馈内容的深度与质量，向优质测评者赠送定制Fluss周边礼品。</p><p><a href="https://link.segmentfault.com/?enc=1z4tWWT2r3Ro%2F254mI9kDw%3D%3D.ACbHH3lBb5rATlHUjaiZdwrlBnbiv%2B6vsv2K4lCGVm6bCBN5L1zGW5LvlJ5Ejv6LHbE14kjkW07HAd014QyBEkE2YU4MOemmfL3K19CzmiI5b1yqQLUImEK4rdLjVbI1WYwDJJwm%2F%2FBx6jSctQZKjw%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/flink/realtime-fluss/product-overview/join-the-public-preview-of-fluss</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578115" alt="image.png" title="image.png" loading="lazy"/></p><h3>更多内容</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578116" alt="" title="" loading="lazy"/></p><hr/><h3>活动推荐</h3><p>复制下方链接或者扫描左边二维码</p><p>即可免费试用阿里云 <strong>Serverless Flink</strong>，体验新一代实时计算平台的强大能力！</p><p>了解试用详情：<a href="https://link.segmentfault.com/?enc=J6tbqThL9EjzWxDq0CsGCg%3D%3D.hdgGvA79NwvJDmIHQErzb%2B0PvIhbwKmUcKz%2Fq5P28oLijIZS13Ka4JsOHeF4g3Mi" rel="nofollow" target="_blank">https://free.aliyun.com/?productCode=sc</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578117" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[非侵入式·智能化·实时——金融行业数据库审计与监测方案 底层逻辑探索 ]]></title>    <link>https://segmentfault.com/a/1190000047578156</link>    <guid>https://segmentfault.com/a/1190000047578156</guid>    <pubDate>2026-01-28 16:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>一、以“数据驱动与落地成效”为核心的整体概要</strong><br/>提示：本段将从战略高度概括金融行业数据库审计的价值与成效。在金融数字化转型不断深化的背景下，数据库已成为承载核心业务数据与客户敏感信息的关键基础设施。围绕“非侵入式、智能化、实时”三大特性，全知科技推出面向金融行业的数据库审计与监测方案，通过旁路部署、AI分析与实时感知能力，实现对数据库访问行为的全量记录、智能识别与动态预警。方案在不影响业务系统性能的前提下，构建覆盖“采集—分析—处置—审计”的闭环体系，不仅满足监管合规要求，也在实际落地中显著提升了风险发现效率、审计自动化水平与安全运营能力，真正实现数据安全治理从“被动合规”向“主动防御”的升级。<br/><strong>二、在政策与技术双重驱动下的行业背景与挑战</strong><br/>提示：本段将从政策环境和技术发展层面引出数据库审计的必要性。近年来，《数据安全法》《个人信息保护法》《银行业信息科技风险管理指引》《等保2.0》等法规密集出台，对金融机构的数据安全治理提出了更高标准。与此同时，云计算、大数据与分布式架构在金融行业广泛应用，数据库环境呈现出多类型、多实例、多地域并存的复杂态势。传统依赖人工审计或单点日志工具的方式，难以及时发现异常访问、越权操作和批量数据导出等高风险行为。监管趋严与技术演进的叠加，使金融行业必须建设一套具备非侵入式部署、智能化分析和实时监测能力的数据库审计体系。<br/><strong>三、聚焦“可见、可控、可追溯”的行业痛点分析</strong><br/>提示：本段将系统梳理金融机构在数据库安全管理中的核心痛点。首先，外部攻击手段日益隐蔽，黑客通过SQL注入、弱口令、权限提升等方式绕过传统防护层，直接对数据库发起攻击。其次，内部人员违规操作具有高隐蔽性，批量查询、导出或篡改数据往往难以及时被发现。再次，数据库类型多样、部署环境复杂，传统审计工具难以做到统一管理与全量覆盖。最后，事后追溯困难，零散日志无法快速还原事件全过程，影响责任界定与合规取证。以上痛点迫切需要通过“非侵入式、智能化、实时”的数据库审计能力来系统解决。<br/><strong><a href="https://link.segmentfault.com/?enc=tTwQzKk02%2BeAGJwIrytluw%3D%3D.LogsfGSeRd1maUA%2FQ4CrFjCmskFfreVC%2FWaOFT2MXcU%3D" rel="nofollow" target="_blank">四、以“非侵入式+智能化+实时”为核心的整体解决方案</a></strong><br/>提示：本段将介绍方案的总体设计理念与技术路线。全知科技数据库审计方案采用旁路流量镜像与日志采集相结合的方式，实现对数据库访问行为的非侵入式获取，避免在业务系统中部署代理，确保核心交易系统稳定运行。系统通过深度协议解析技术还原SQL语句和参数，并结合AI智能分析引擎构建动态行为基线，实现对异常访问、越权操作、批量导出等风险行为的实时识别。通过统一管理平台，将采集、分析、告警与审计证据留存整合为一体，形成完整的数据库安全治理闭环。<br/><strong>五、以“全量留痕与智能分析”为核心的功能模块设计</strong><br/>提示：本段将从功能层面拆解数据库审计系统的关键能力。在采集层，系统通过旁路镜像、日志文件及云数据库接口实现全量数据获取；在解析层，支持多种主流及国产数据库协议的深度解析；在分析层，利用AI模型与规则库对访问行为进行语义分析与风险分级；在告警层，系统对高危行为进行实时告警并支持多渠道推送；在审计层，系统对DDL、DML、DCL等操作进行完整记录，支持多维检索与合规报表自动生成。通过可视化态势大屏，安全人员可以直观掌握数据库安全运行状态。<br/><strong>六、围绕“真实场景”的应用落地实践</strong><br/>提示：本段将结合金融机构实际应用场景说明方案的落地效果。在大型银行与证券机构的实践中，全知科技数据库审计系统通过两周内完成部署，实现对多地机房与云环境数据库的统一监控。系统上线后，异常访问识别准确率显著提升，误报率大幅降低；合规审计报表由人工整理转为自动生成，审计周期从数天缩短至数小时；安全运维团队能够在分钟级定位风险源头，数据库安全从“事后追责”转向“事中阻断”。<br/><strong>七、体现“安全、合规、效率”三重价值的推广意义</strong><br/>提示：本段将总结方案在行业推广层面的综合价值。在安全层面，方案实现对外部攻击与内部违规的双重防护；在合规层面，满足等保2.0与金融监管对日志审计和证据留存的要求；在效率层面，通过智能化手段降低人工运维和审计成本。该方案具备高度可复制性，适用于银行、证券、保险等多类金融机构，为行业构建统一、可持续演进的数据库安全治理体系提供了范式。<br/><strong>八、围绕方案的常见问题解答（Q&amp;A）</strong><br/>提示：本段将通过问答形式强化读者理解。<br/>Q1：数据库审计系统是否影响数据库性能？<br/>A：采用旁路非侵入式部署，不对业务系统产生性能影响。<br/>Q2：是否支持国产数据库？<br/>A：支持达梦、人大金仓、南大通用等多种国产数据库。<br/>Q3：告警是否会过多干扰运维？<br/>A：通过AI基线模型有效降低误报率，仅对高风险行为告警。<br/>Q4：是否满足监管审计要求？<br/>A：内置合规模板，可自动生成监管报表与审计证据。<br/><strong>九、来自用户的真实评价</strong><br/>提示：本段将从客户视角呈现方案价值。“全知科技的数据库审计系统帮助我们实现了对核心数据的可视化管理，既满足了监管要求，又显著提升了内部安全运营效率，是我们数字化安全体系的重要支撑。”——某股份制银行信息安全负责人。<br/>作为新一代数据安全引领者，全知科技凭借丰富的市场实践经验及技术支撑实力，充分发挥了数据安全领域标杆企业的领头作用，为《数据安全技术 数据接口安全风险监测方法》的顺利编制、发布提供了重要支持。此次牵头编制数据接口安全国标，是业界对全知科技技术权威性与业界影响力的高度认可，也标志着全知科技在数据安全标准化建设领域迈出了坚实的一步。面向未来，全知科技将持续深化“非侵入式、智能化、实时”的技术路线，推动金融行业数据库审计与监测能力向更高水平演进，为金融数字经济发展筑牢坚实的数据安全底座。</p>]]></description></item><item>    <title><![CDATA[全链路、可参考、AI降噪的运营商API安全解决方案 底层逻辑探索 ]]></title>    <link>https://segmentfault.com/a/1190000047578181</link>    <guid>https://segmentfault.com/a/1190000047578181</guid>    <pubDate>2026-01-28 16:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概要<br/>（提示：本节回答“为什么要做、做到了什么、结果是否可量化”。）</p><pre><code>   在运营商数字化转型全面加速的背景下，API 已从技术接口升级为连接用户数据、政企业务与网络能力的关键基础设施，其安全性直接决定数据合规水平与业务连续性。围绕“接口全可视、风险全可控、责任可追溯”的行业目标，全知科技基于运营商真实业务场景，提出一套覆盖 API 全生命周期的风险监测与治理系统。该系统以“全链路风险治理”为核心，从资产发现、风险识别、动态防护到审计溯源形成闭环；以“可参考”为导向，将监管要求、集团考核指标转化为可执行的技术路径；以“AI 降噪”为突破点，在保障业务连续性的前提下，将 API 安全告警误报率稳定控制在 5% 以下。在多家省级运营商的实践中，该方案实现 API 资产可视率 100%、高危风险闭环率 100%，为运营商行业提供了一套可复制、可推广的 API 安全治理样本。</code></pre><p>二、多业务并行下，API 成为运营商新的高风险承载点<br/>（提示：本节聚焦“环境变化带来了哪些新的安全压力”。）</p><pre><code>   随着“数字中国”战略推进，运营商加速布局 5G 专网、政企云、智慧家庭与物联网生态，业务系统之间的协同高度依赖 API 进行数据交换与能力调用。API 承载的数据类型高度敏感，既包括用户身份证号、手机号、通话详单等个人信息，也涵盖政企客户核心业务数据与网络运行数据。与此同时，国家层面已形成“法律法规—行业标准—集团考核”三重约束机制。《数据安全法》《个人信息保护法》明确运营商数据安全主体责任，《电信行业数据分类分级方法》等文件进一步细化 API 管控要求，集团层面则将 API 风险监测纳入年度考核指标，要求实现接口资产可视、风险可控、事件可追溯。在现实落地中，多数运营商仍面临三类共性问题：一是 API 分散于多系统、多协议，资产底数不清；二是敏感数据在接口中的流转路径不可视；三是传统防护手段误报率高，风险响应滞后，难以支撑集团级考核与监管审计。</code></pre><p>三、从“看得见的漏洞”到“看不见的业务逻辑风险”<br/>（提示：本节回答“真正的风险在哪里”。）</p><pre><code>   运营商 API 风险并不局限于传统漏洞，而更多隐藏于复杂的业务逻辑与跨系统调用关系中。一方面，未鉴权、弱鉴权、明文传输等显性问题依然存在，直接威胁用户隐私与政企业务安全；另一方面，更具破坏性的风险往往来自业务逻辑层，如异常账号跨地市批量拉取用户数据、物联网设备被频繁重配置等。此外，运营商 API 调用规模巨大，日均千万级请求使得传统基于规则的监测机制极易产生误报。一旦防护策略过于激进，极有可能影响正常通信服务或政企业务连续性，反而放大运营风险。这使得 API 风险治理必须在“安全强度”与“业务稳定”之间找到平衡点。</code></pre><p>四、以全链路设计实现 API 风险的闭环治理<br/>（提示：本节说明“方案如何设计、如何落地”。）</p><pre><code>   “[知影-API 风险监测系统](https://jsj.top/f/CuRr3f)”的部署阶段采用轻量化旁路接入方式，无需改造 BOSS、CRM、核心网与物联网平台，即可对接省分出口、地市专网及边缘节点。在运营层面，方案通过“中心—分布式”架构，将地市与区县 API 流量统一汇聚至省分中心，实现资产盘点与策略统一下发，避免防护标准碎片化。运行过程中形成“四步闭环”：第一步，资产梳理。通过 7×24 小时流量解析，自动识别 RESTful、GRPC、Diameter 等接口，输出包含影子 API 的资产清单；第二步，风险评估。结合自动化检测与业务建模，按“用户影响+业务影响”双维度排序风险；第三步，动态防护。基于行为基线实时拦截异常调用，并通过 AI 降噪引擎控制误报；第四步，合规审计。自动生成符合监管要求的审计报告，实现长期留痕与快速回溯。</code></pre><p>五、从“能监测”到“真正用得起来”<br/>（提示：本节聚焦“数据化成果与实际变化”。）</p><pre><code>   在某省级运营商的实践中，系统在一周内完成 4.5 万余个 API 的全量梳理，识别出 6 万余个未登记接口并全部纳入统一管理。上线三个月内，累计捕获 API 安全事件 156 起，其中高危事件 23 起，告警准确率提升至 94%，误报率降至 4.8%。更重要的是，风险整改周期由原来的 72 小时缩短至 12 小时，所有高危问题实现闭环处置，并顺利通过工信部专项检查。两起真实数据泄露事件均在 4 小时内完成定位与阻断，未造成监管问责。</code></pre><p>六、为运营商行业提供可复制的治理模板<br/>（提示：本节回答“是否具备行业参考意义”。）<br/>该系统的价值不仅体现在单点防护能力，更在于形成了一套可复用的 API 安全治理方法论：一是将监管要求转化为可执行的技术指标，降低合规落地难度；二是以 AI 降噪技术解决大规模 API 场景下的误报难题；三是通过全链路设计，打通风险监测、整改与审计，支撑长期治理。<br/>七、五个关键问答<br/>1.为什么运营商需要专属的 API 风险监测？<br/>因为通用安全产品无法识别电信专用协议与业务逻辑风险。<br/>2.AI 降噪解决了什么问题？解决了高并发场景下误报过多、影响业务的问题。<br/>3.是否会影响核心业务运行？旁路部署与动态策略确保业务零中断。<br/>4.能否支撑监管审计？系统内置合规模板与长期留痕能力。<br/>5.是否具备推广价值？已在多省运营商验证，具备高度可复制性。<br/>八、呈现一线用户的真实反馈<br/>（提示：本节从用户角度验证方案有效性。）</p><pre><code>   多家运营商反馈， “知影-API 风险监测系统”显著提升了 API 资产透明度与风险响应效率，使安全部门首次能够以“数据化方式”掌握全省 API 风险态势。在不增加运维负担的前提下，实现了集团考核指标的稳定达标，并为后续数据治理与业务创新奠定了安全基础。
   随着移动互联网、云计算和AI的普及，企业不再单打独斗，而是通过API将自身能力以“服务”的方式输出，进而融入更大的生态。但与此同时，API接口的暴露面也在不断扩大，成为黑客攻击和数据泄露的高风险入口。全知科技作为国内领先的API安全厂商，凭借知影-API风险监测系统在安全领域的突出表现，不仅在国内市场屡获认可，还在国际舞台上赢得权威肯定。公司作为牵头单位主导制定《数据安全技术 数据接口安全风险监测方法》国家标准，并多次入选 Gartner 《Market Guide for API Management, China》、IDC 相关研究报告以及《中国API解决方案代表厂商名录》。在《2025年中国ICT技术成熟度曲线》（Hype Cycle for ICT in China, 2025）等前瞻性研究中，全知科技亦被列为代表供应商，彰显了其在技术创新与行业规范建设上的领先地位。</code></pre>]]></description></item><item>    <title><![CDATA[基于Web Component的React与Vue跨栈系统融合实践 Frank ]]></title>    <link>https://segmentfault.com/a/1190000047578188</link>    <guid>https://segmentfault.com/a/1190000047578188</guid>    <pubDate>2026-01-28 16:02:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、背景与需求</h2><p>最近一直会有一些这样的需求, 两套完全独立的前端系统，分别基于React和Vue框架开发，用户体系及鉴权体系独立,本次测试将尝试把Vue系统嵌入React中，实现核心交互逻辑：点击切换至React系统时，侧边栏（Aside）渲染React菜单，内容区（Content）加载React组件；切换至Vue系统时，侧边栏与内容区同步渲染Vue对应的菜单及组件，形成视觉与功能统一的集成体验,基础UI如下图:<br/><img width="723" height="503" referrerpolicy="no-referrer" src="/img/bVdnNp9" alt="image.png" title="image.png"/></p><h2>二、技术环境</h2><ul><li><strong>Vue技术栈</strong>：Vue3 + Vite.js + UnoCss + TypeScript (Vue项目用的是开源的)</li><li><strong>React技术栈</strong>：React17 + Webpack + Sass + TypeScript (React项目是自有的)</li><li><strong>后端及部署</strong>：Spring Boot + JAVA17 + Docker + MySQL + Redis (Vue项目后台)</li></ul><h2>三、方案选型</h2><p>目前微前端领域已有qiankun.js、MicroApp等成熟方案，但也又一定的局限性,本次实践旨在探索更轻量化的浏览器原生方案——Web Component。作为W3C制定的浏览器原生组件化标准，Web Component具备跨框架UI复用与封装能力，无需依赖第三方框架，可天然实现不同技术栈的融合。</p><h2>四、工程改造实现</h2><h3>4.1 Vue工程改造（Web Component打包）</h3><p>核心目标是将Vue项目打包为可被React调用的Web Component自定义元素，需新增专属入口文件并配置打包规则。</p><h4>4.1.1 新增Web Component入口文件</h4><p>创建<code>src/web-component-entry.ts</code>作为打包入口，封装Vue应用为自定义元素，实现组件的挂载、卸载与属性监听,以下是伪代码:</p><pre><code class="typescript">
// src/web-component-entry.ts
import App from './App.vue'
import { createApp, h } from 'vue'

class VueWebComponentElement extends HTMLElement {
  private _app: any = null
  private _reactToken: string = ''

  // 定义需要监听的属性
  static get observedAttributes() {
    return ['mode']
  }

  constructor() {
    super()
    // 监听来自React的事件
    this.addEventListener('app-changed', (e: CustomEvent) =&gt; {
      const { token } = e.detail
      this._reactToken = token
    })
  }

  async connectedCallback() {
    if (this._app) return
    // 创建挂载容器并设置样式
    const rootNode = document.createElement('div')
    rootNode.setAttribute('id', 'app-vue')
    rootNode.style.height = '100%'
    this.appendChild(rootNode)

    // 获取属性并初始化Vue应用
    const mode = this.getAttribute('mode') || 'full'
    const app = createApp({
      render() {
        return h(App, { mode })
      }
    })

    // 比如挂载Vue生态依赖（权限、指令、全局组件、Store、Router等）
    app.mount(rootNode)
    this._app = app
  }

  // 属性变化回调
  attributeChangedCallback(name: string, oldValue: string, newValue: string) {
    // 可根据属性变化执行对应逻辑（如样式切换、数据更新）
  }

  // 组件卸载回调
  disconnectedCallback() {
    if (this._app) {
      this._app.unmount()
      delete this._app
    }
  }
}

// 定义自定义元素（避免重复定义）
if (!customElements.get('wc-pvue')) {
  customElements.define('wc-pvue', VueWebComponentElement)
}

export default VueWebComponentElement</code></pre><h4>4.1.2 Vite打包配置调整</h4><p>在<code>vite.config.ts</code>中新增Web Component打包模式，指定输出格式、入口文件及资源命名规则：</p><pre><code class="typescript">// vite.config.ts部分配置
import { defineConfig, loadEnv, resolve } from 'vite'
import vue from '@vitejs/plugin-vue'

export default defineConfig(({ mode }) =&gt; {
  const env = loadEnv(mode, process.cwd())
  const isWebComponent = env.VITE_BUILD_MODE === 'webcomponent'

  return {
    plugins: [vue()],
    build: {
      minify: 'terser',
      // 区分Web Component打包目录
      outDir: env.VITE_OUT_DIR &amp;&amp; isWebComponent 
        ? `${env.VITE_OUT_DIR}/web-component` 
        : env.VITE_OUT_DIR || 'dist',
      sourcemap: env.VITE_SOURCEMAP === 'true' ? 'inline' : false,
      terserOptions: {
        compress: {
          drop_debugger: env.VITE_DROP_DEBUGGER === 'true',
          drop_console: env.VITE_DROP_CONSOLE === 'true'
        }
      },
      // Web Component专属打包配置
      ...(isWebComponent ? {
        lib: {
          entry: resolve(__dirname, 'src/web-component-entry.ts'),
          name: 'PVue',
          fileName: 'pvue',
          formats: ['umd'] // 输出UMD格式，兼容浏览器环境
        },
        rollupOptions: {
          output: {
            entryFileNames: 'pvue.js',
            assetFileNames: 'pvue.[ext]'
          }
        }
      } : {})
    }
  }
})</code></pre><p>注：为简化测试，当前配置未分离Vue运行时依赖，导致最终UMD文件体积偏大。若需优化体积，可通过<code>external</code>配置排除Vue核心依赖，但需在React项目中同步引入对应依赖，确保Vue应用运行环境完整。</p><h3>4.2 React工程改造（集成Web Component）</h3><p>React端需通过布局组件控制系统切换逻辑，同时引入Vue打包后的资源文件。</p><h4>4.2.1 布局组件改造</h4><p>在<code>layout.tsx</code>中通过状态控制渲染逻辑，切换至Vue系统时加载自定义元素<code>&lt;wc-pvue /&gt;</code>：</p><pre><code class="tsx">
import React, { useState } from 'react'
import { Layout } from 'antd' // 假设使用Ant Design布局组件
import SiderMenu from './SiderMenu'
import Header from './Header'
import styles from './layout.module.sass'

const AppLayout = ({ children }: { children: React.ReactNode }) =&gt; {
  const [app, setApp] = useState&lt;'react' | 'vue'&gt;('react')

  // 系统切换回调
  const onAppChanged = (targetApp: 'react' | 'vue') =&gt; {
    setApp(targetApp)
    // 延迟发送事件，确保Vue组件已渲染
    setTimeout(() =&gt; {
      const wcEl = document.querySelector('wc-pvue')
      wcEl?.dispatchEvent(
        new CustomEvent('app-changed', {
          detail: {
            token: (cache.getCache('accessInfo', 'session') as any)?.accessToken,
          },
          bubbles: true,
          composed: true, // 允许事件穿透Shadow DOM
        })
      )
    }, 500)
  }

  return (
    &lt;Layout className={styles['app-layout-wrapper']}&gt;
      &lt;Header onAppChanged={onAppChanged} /&gt;
      {app === 'react' ? (&lt;Layout className={styles['app-content-wrapper']}&gt;
          &lt;SiderMenu /&gt;
          &lt;Layout&gt;{children}&lt;/Layout&gt;
        &lt;/Layout&gt;
      ) : (
        // 加载Vue对应的Web Component
        &lt;wc-pvue /&gt;
      )}
    &lt;/Layout&gt;
  )
}

export default AppLayout</code></pre><h4>4.2.2 引入Vue资源</h4><p>在React项目的<code>index.html</code>中引入Vue打包后的CSS与JS文件，确保自定义元素可正常渲染：</p><pre><code class="html">
&lt;!-- 引入Vue Web Component样式 --&gt;
&lt;link rel="stylesheet" href="vue/pvue.css" /&lt;!-- 引入Vue Web Component脚本 --&gt;
</code></pre><p>至此，基础嵌入功能实现完成，可通过切换菜单验证两侧系统的渲染效果。</p><h2>五、关键技术点突破</h2><h3>5.1 样式隔离与覆盖</h3><p>Web Component天然支持Shadow DOM，可构建独立DOM树实现样式隔离，避免与React主系统样式冲突；Vue端也可通过Scoped CSS限定样式作用域。但实际业务中常需覆盖子系统样式，结合本次Vue项目使用UnoCSS及CSS变量的特性，采用变量覆盖方案实现样式定制：</p><pre><code class="css">
wc-pvue {
  height: 100%;
  /* 覆盖Vue项目内部CSS变量 */
  --app-footer-height: 0px;
  --tags-view-height: 0px;
  --top-tool-height: 0px;

  /* 隐藏Vue项目中不需要的元素 */
  #v-tool-header,
  #v-tags-view {
    display: none;
  }
}</code></pre><p>样式覆盖需结合项目实际场景调整：若无法通过CSS变量或选择器覆盖，需修改Vue项目源码；若涉及主题切换等动态需求，可通过自定义元素属性传递状态，在Vue端监听属性变化同步更新样式。</p><h3>5.2 跨框架消息通讯</h3><p>UI层嵌入仅完成视觉整合，跨框架逻辑协同的核心在于消息通讯。常用方案包括全局状态共享（挂载至window）、属性传递、事件驱动等，本次实践采用浏览器原生<code>CustomEvent</code>实现解耦式通讯。</p><p>前文实现了React向Vue发送事件传递Token，但通过<code>setTimeout</code>规避渲染时机问题的方案存在不稳定性。更优实践为Vue主动发起通讯：在Vue组件的<code>connectedCallback</code>生命周期中发送就绪事件，React监听该事件后再传递数据，确保渲染与通讯时序一致：</p><pre><code class="typescript">
// Vue端：web-component-entry.ts 中修改connectedCallback
async connectedCallback() {
  // 省略原有挂载逻辑...
  // 组件挂载完成后通知React
  this.dispatchEvent(
    new CustomEvent('vue-ready', {
      bubbles: true,
      composed: true
    })
  )
}

// React端：layout.tsx 中监听事件
useEffect(() =&gt; {
  const handleVueReady = () =&gt; {
    const wcEl = document.querySelector('wc-pvue')
    wcEl?.dispatchEvent(
      new CustomEvent('app-changed', {
        detail: { token: (cache.getCache('accessInfo', 'session') as any)?.accessToken },
        bubbles: true,
        composed: true
      })
    )
  }
  document.addEventListener('vue-ready', handleVueReady)
  return () =&gt; document.removeEventListener('vue-ready', handleVueReady)
}, [])</code></pre><h2>六、实践总结与待解决问题</h2><p>基于Web Component可实现React与Vue跨栈系统的基础融合，通过自定义元素封装、原生事件通讯、CSS变量覆盖等手段，满足核心交互与样式适配需求。但本次实践仍存在诸多待优化点：</p><ol><li><strong>路由兼容性</strong>：React采用BrowserRouter（HTML5 History模式），Vue采用HashRouter，两者路由规则冲突，且页面切换时HTML标题同步、路由守卫协同等问题未解决。可通过统一路由模式（如均采用History模式）、主应用接管路由分发实现兼容。</li><li><strong>统一认证体系</strong>：两套系统原有独立登录权限机制，目前仅实现Token传递，未完成身份态同步、权限统一校验等功能，需设计跨系统认证中心或共享令牌机制。</li><li><strong>第三方系统改造限制</strong>：本次实践基于可自由修改的开源Vue项目，若需嵌入第三方不可控Vue系统，无法进行源码改造，需探索无侵入式封装方案。</li></ol><p>相较于qiankun等成熟微前端框架，Web Component也是一种更轻量化的选择方案, 具体实践依然要根据具体的项目情况来选择和评估。当然,后续抽空还会分享一种基于类似门户系统的iframe融合方案,但不会在浏览器打开新页签,大家还有哪些方案可以分享呢,欢迎留言讨论!</p>]]></description></item><item>    <title><![CDATA[设计模式:不再手动 set DTO，采用 Builder 模式 代码丰 ]]></title>    <link>https://segmentfault.com/a/1190000047578192</link>    <guid>https://segmentfault.com/a/1190000047578192</guid>    <pubDate>2026-01-28 16:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、背景</h2><p>在实际项目中，我们经常需要构造一些字段很多的 <a href="https://link.segmentfault.com/?enc=oaSSpJvfRIW39VbPw9cQTw%3D%3D.SicP1fIviMARylXxlUfw5qVIuvZZ1zylj7UjkWzSIt8IEjcSbY8UT0ky2jC1FHccCyGXsJSkPCnOVMVNVmoW3Q%3D%3D" rel="nofollow" target="_blank">DTO</a>、请求对象或结果对象。  <br/>一开始，最自然的写法，往往就是 <code>new</code> 一个对象，然后一行一行 <code>set</code>。</p><p>但当对象逐渐变复杂，这种写法会很快暴露问题。</p><p>这篇文章通过一个非常典型的对比，讲清楚：  <br/><strong>为什么在复杂对象构建场景下，Builder 模式会比手动 set 更合适。</strong></p><h2>二、手动set</h2><p>你一定见过这样的代码：</p><pre><code>MatchResult result = new MatchResult();
result.setResumeId(resumeId);
result.setPositionId(positionId);
result.setFinalScore(finalScore);
result.setRagScore(ragScore);
result.setGraphScore(graphScore);
result.setLlmScore(llmScore);
result.setMatchedSkills(matchedSkills);
result.setMissingSkills(missingSkills);
result.setExtraSkills(extraSkills);
result.setRecommendLevel(recommendLevel);
result.setMatchGrade(matchGrade);
123456789101112</code></pre><p><strong>手动 set 写法的几个问题</strong></p><ol><li>代码冗余，可读性差</li><li>容易构造出“半成品对象”</li><li>必填字段只能靠约定</li><li>扩展成本高，容易漏改</li></ol><h2>三、使用builder模式</h2><pre><code>@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class MatchResult {

    
    private String resumeId;

    
    private String positionId;

    
    private float finalScore;

    
    private float ragScore;

    
    private float graphScore;

    
    private float llmScore;

    
    private List&lt;String&gt; matchedSkills;

    
    private List&lt;String&gt; missingSkills;

    
    private List&lt;String&gt; extraSkills;

    
    private String llmReport;

    
    private Map&lt;String, Object&gt; scoreDetails;

    
    private int recommendLevel;

    
    private String matchGrade;
}


</code></pre><pre><code>MatchResult result = MatchResult.builder()
        .resumeId(resumeId)
        .positionId(positionId)
        .finalScore(finalScore)
        .ragScore(ragScore)
        .graphScore(graphScore)
        .llmScore(llmScore)
        .matchedSkills(matchedSkills)
        .missingSkills(missingSkills)
        .extraSkills(extraSkills)
        .recommendLevel(recommendLevel)
        .matchGrade(matchGrade)
        .build();
</code></pre><h2>四、使用builder模式的好处</h2><p>1、 可读性明显更好</p><pre><code>builder()
  .xxx()
  .yyy()
  .zzz()
  .build()
</code></pre><p>2、对象构建是<a href="https://link.segmentfault.com/?enc=FK%2F8GALbzNL4N8XsoYD%2Brw%3D%3D.vea8xZ4dcW7%2BSECNKNaPauk%2Br5T22jVJ4TP8TQZz2Cy5DXo472NGW0bDtwsJ%2FO56fgZI%2BxOdLG3nH6lGeimRtErvhsmsqIJuz2OvYDV0DfehwPmFju2YbvRGuWy6RrRU" rel="nofollow" target="_blank">原子操作</a></p><pre><code>MatchResult result = MatchResult.builder()
        ...
        .build();</code></pre><p>要么构建成功，  <br/>要么直接失败。</p><p>不会再出现“半成品对象”。</p><p>3、对扩展更加友好  <br/>当新增字段时：</p><blockquote><p>Builder 增加一个方法</p><p>旧代码不需要改</p><p>需要使用新字段的地方再补  <br/>不需要更改之前的原始代码</p></blockquote>]]></description></item><item>    <title><![CDATA[蚂蚁正式开源 LingBot-Depth，基于掩码深度建模的新一代空间感知模型 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047578222</link>    <guid>https://segmentfault.com/a/1190000047578222</guid>    <pubDate>2026-01-28 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1月27日，我们正式开源了 LingBot-Depth 空间感知模型。</strong></p><p>不同于数字世界，具身智能的落地高度依赖物理空间信息，空间智能是其在现实场景落地应用的核心关键，而视觉维度下支撑空间智能的重要桥梁正是距离与尺度（Metric Depth）。基于这一核心需求，空间感知模型 LingBot-Depth 应运而生。</p><p>LingBot-Depth 是一种面向真实场景的深度补全模型，依托奥比中光 Gemini 330 系列双目 3D 相机进行 RGB-Depth 数据采集与效果验证，并基于深度引擎芯片直出的深度数据进行训练与优化，旨在将不完整且受噪声干扰的深度传感器数据转化为高质量、具备真实尺度的三维测量结果，提升环境深度感知与三维空间理解能力，为机器人、自动驾驶汽车等智能终端赋予更精准、更可靠的三维视觉。</p><p>实验结果表明，<strong>本模型在深度精度与像素覆盖率两项核心指标上均超越业界顶级工业级深度相机</strong>。在 NYUv2、ETH3D 等多个基准测试中，LingBot-Depth 在深度补全、单目深度估计及双目匹配任务上均达到当前最优水平，并在无需显式时序建模的情况下保持视频级时间一致性。LingBot-Depth 模型也已通过奥比中光深度视觉实验室的专业认证，在精度、稳定性及复杂场景适应性方面均达到行业领先水平。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578224" alt="图片" title="图片"/><br/>注解：在最具挑战的稀疏深度补全任务中，LingBot-Depth 性能整体优于现有多种主流模型。（图中数值越低代表性能越好。）</p><p>下游任务验证进一步表明，模型能够在 RGB 与深度两种模态之间学习到对齐的潜在空间表征，从而实现对透明及反光物体的稳定机器人抓取。<br/><a href="https://www.bilibili.com/video/BV1ZW6TBnEdn/?aid=115964569979323&amp;cid=35635200804" target="_blank">https://www.bilibili.com/video/BV1ZW6TBnEdn/?aid=115964569979...</a></p><h2>技术架构：创新的掩码深度建模范式</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578225" alt="图片" title="图片" loading="lazy"/><br/>在家庭和工业环境中，玻璃器皿、镜面、不锈钢设备等透明和反光物体物体十分常见，但却是机器空间感知的难点。传统深度相机受制于光学物理特性，在面对透明或高反光材质时，往往无法接收有效回波。针对这一行业共性难题，我们研发了<strong>“掩码深度建模”（Masked Depth Modeling，MDM）技术</strong>。训练过程中，我们使用海量 RGB–深度图像对，但刻意遮挡其中一部分深度区域，让模型仅根据 RGB 图像去预测缺失的深度值。随着训练进行，模型逐渐学会建立“外观—几何”之间的对应关系，也就是从“物体看起来像什么”推断“它大概有多远”。</p><p>在涵盖家庭、办公环境、健身房及户外场景的上千万张图像数据上完成训练后，当深度相机传回的数据出现缺失或异常时，LingBot-Depth 模型已能够融合彩色图像（RGB）中的纹理、轮廓及环境上下文信息，对缺失区域进行推断与补全，输出更完整、致密、边缘更清晰的三维深度图。</p><h2>核心亮点</h2><h3>精准且稳定的相机深度感知</h3><p>LingBot-Depth 在传统深度传感器易失效的复杂场景中，仍可输出具备真实尺度的高精度深度结果，包括透明物体、玻璃表面以及高反光材质等极具挑战性的环境。不同于依赖硬件改进的方案，本模型从视觉理解层面弥补传感器缺陷，实现对真实三维结构的可靠恢复。</p><p>除单帧精度优势外，LingBot-Depth 还表现出优异的时间一致性。在无需显式时序建模的情况下，模型即可为视频输入生成稳定、连贯的深度序列，有效避免闪烁与结构跳变问题，为机器人操作、AR/VR 以及动态场景感知等应用提供可靠的连续空间理解能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578226" alt="图片" title="图片" loading="lazy"/></p><h3>卓越的 3D 和 4D 环境感知能力</h3><p>LingBot-Depth 为下游空间感知任务提供了坚实而通用的基础能力。通过将含噪且不完整的传感器深度优化为干净、稠密且具备真实尺度的三维测量结果，模型显著提升了多种高层视觉任务的稳定性与精度。具体而言，LingBot-Depth 支持：</p><ol><li>更加准确的结构化室内场景建图，并有效提升相机位姿与运动轨迹估计的精度；</li><li>面向机器人学习的可靠 4D 点跟踪能力，在统一的真实尺度空间中同时刻画静态场景几何结构与动态物体运动。这使得系统能够在复杂真实环境中建立一致、连续且可用于决策与交互的空间理解表征。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578227" alt="图片" title="图片" loading="lazy"/></li></ol><h3>灵巧抓取操作适用于透明与反光物体</h3><p>通过在统一潜在空间中联合对齐 RGB 外观信息与深度几何结构，LingBot-Depth 使机器人在以往难以处理的复杂场景中实现稳定可靠的操作能力。基于模型优化后的高质量深度结果及跨模态对齐特征，我们进一步训练了一种基于扩散模型的抓取位姿生成策略，在透明杯、反光金属容器等具有挑战性的物体上取得了较高的抓取成功率。在真实机器人测试中，在透明储物盒等传统传感器难以处理的场景中，LingBot-Depth 通过生成合理的深度估计，成功实现了 50% 的抓握率，突破了技术瓶颈。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578228" alt="图片" title="图片" loading="lazy"/></p><h2>从实验室到落地应用：显著提升消费级深度相机对高难物体的处理效果</h2><p>LingBot-Depth 展现出与现有硬件设备的良好适配性。在不更换更高成本传感器的情况下，模型可提升可靠性并降低系统部署门槛。LingBot-Depth 模型依托奥比中光 Gemini330 系列双目 3D 相机进行效果测试，结果显示：面对透明玻璃、高反射镜面、强逆光以及复杂曲面等极具挑战性的光学场景，搭载 LingBot-Depth 后输出的深度图变得平滑、完整，且物体的轮廓边缘非常锐利，效果优于业内领先 3D 视觉公司 Stereolabs 推出的 ZED Stereo Depth 深度相机。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047578229" alt="图片" title="图片" loading="lazy"/><br/>注解：搭载 LingBot-Depth 后，奥比中光 Gemini 330 系列在透明及反光场景下深度图的完整性和边缘清晰度明显提升</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047578230" alt="图片" title="图片" loading="lazy"/><br/>注解：奥比中光 Gemini 330 系列相机搭载 LingBot-Depth 后输出的深度图效果优于业界领先的 ZED 深度相机</p><p>这意味着在不更换传感器硬件的前提下，LingBot-Depth 可显著提升消费级深度相机对高难物体的处理效果，降低机器人因深度缺失与噪声引发的抓取失败与碰撞风险。在具身智能、自动驾驶等领域都有一定应用价值，能够极大程度提升具身操作的精准度。</p><p>目前，我们已与奥比中光达成战略合作伙伴关系，将基于 LingBot-Depth 模型推出新一代深度相机，依托 Gemini 330 系列相机提供的芯片级 3D 数据，进一步通过技术协同、生态共建，为机器人处理各行各业极端场景、走向真正落地提供强大的技术支撑。</p><p>LingBot-Depth 已成功实现模型轻量化与端侧部署，具备在边缘计算设备上高效运行的能力。未来，我们期待通过开源开放与生态合作，和广大合作伙伴一起加速具身智能在家庭、工业、物流等复杂场景的大规模应用落地。</p><p>目前我们的模型、代码、技术报告已全部开源，欢迎大家访问我们的开源仓库。<br/>Website：<a href="https://link.segmentfault.com/?enc=7LUHi%2BFPhK4l9YvmE0%2Byng%3D%3D.adWSuf76IB8poB8zy4uyabHXHb9I7Dh1nFRSk8v56L2a0Z6ESf6%2BQrcSTXo%2Fu6np" rel="nofollow" target="_blank">https://technology.robbyant.com/lingbot-depth</a><br/>Model：<a href="https://link.segmentfault.com/?enc=kAvKNeyzsCRURs5qwTkiAw%3D%3D.0ZTRQQi4B1490jgDthX8JzZuAi%2BXIAdDhxTt4TlGGYW%2BXRS0FX1KV96eH%2F%2BNjcBh" rel="nofollow" target="_blank">https://huggingface.co/robbyant/lingbot-depth</a><br/>Code：<a href="https://link.segmentfault.com/?enc=n20xXgcIZCw%2FXvB5ZZ0RAA%3D%3D.QtSNNFNbew98e25lTuyGAumTKE2kfnlp10bPaAha1ZVOfkylU5zBUPndK7a4kgF5" rel="nofollow" target="_blank">https://github.com/Robbyant/lingbot-depth</a><br/>Tech Report：<a href="https://link.segmentfault.com/?enc=BQpTxnyy%2BMKAKmP3PU7LEQ%3D%3D.TkuodPU2jWNSdJ1rVqtsF8kLYAJS0YP22arYhjBtVz98st5gubaeIoDMpgeRKcX1ROMfg9EH%2BwCe4amJ4mpEefYM3OaWANBRraJ1gsjmrys%3D" rel="nofollow" target="_blank">https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf</a></p><p>后续我们还将开源 300 万对精心标注的 RGB-深度数据，包括 200 万对实拍 RGB-D 样本，和 100 万对渲染样本，推动空间感知技术的开源生态建设和技术创新。</p><p>LingBot-Depth 的开源标志着我们在空间智能领域迈出的第一步。本周，我们还将陆续为大家带来我们在具身智能领域智能基座方向的更多成果，我们期待与全球开发者、研究者、产业伙伴一起，共同探索具身智能的上限。</p>]]></description></item><item>    <title><![CDATA[面试官：既然 JWT 这么好，为什么大厂还在用 Session？ 王中阳讲编程 ]]></title>    <link>https://segmentfault.com/a/1190000047577612</link>    <guid>https://segmentfault.com/a/1190000047577612</guid>    <pubDate>2026-01-28 15:14:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>面试官问："现在都 2026 年了，登录鉴权是不是该全切到 JWT 了？"</p><p>很多人会不假思索地点头："当然！JWT 无状态、可扩展、跨域方便，Session 早该被淘汰了。"</p><p>如果你这么回答，恭喜你，掉坑里了。</p><p><strong>这时候面试官通常会补一刀：</strong><br/>"那如果用户手机丢了，或者改了密码，你怎么把旧的 JWT 立即作废？"</p><p>这一问，往往能把 90% 的候选人问懵。</p><p>这篇文章就来聊聊：为什么被吹上天的 JWT，在很多大厂的核心业务里，反而不如"老土"的 Session？</p><h2>看懂本质差异</h2><p>在撕逼之前，先对齐一下概念。</p><p><strong>Session 方案</strong>（类似于"会员卡 + 账本"）：</p><ol><li>用户登录，服务端给一个 <code>sessionId</code>（会员卡号）。</li><li>服务端在 Redis 或内存里存一份记录（账本）：<code>sessionId_123 =&gt; { user_id: 1, role: 'admin' }</code>。</li><li>每次请求，服务端查账本，确认有效才放行。</li><li><strong>核心特点：服务端有状态（Stateful），控制权在服务端。</strong></li></ol><p><strong>JWT 方案</strong>（类似于"现金"）：</p><ol><li>用户登录，服务端根据用户信息生成一串加密字符串（钞票）。</li><li>钞票上写着：<code>{ user_id: 1, role: 'admin', expire: '2027-01-01' }</code>。</li><li>服务端<strong>不存记录</strong>，只负责发钱和验钞。</li><li>每次请求，服务端解密验钞，没过期就是真的。</li><li><strong>核心特点：服务端无状态（Stateless），控制权在客户端（只要没过期，就能用）。</strong></li></ol><h2>JWT 的致命死穴：我想封杀你，但做不到</h2><p>回到开头的面试题："怎么把旧的 JWT 立即作废？"</p><p>在 Session 方案里，这太简单了。<br/>你手机丢了？客服后台点一下"下线"，服务端把 Redis 里的 <code>sessionId</code> 删了。下次那个手机再发请求，查不到记录，直接拒绝。<strong>秒级生效。</strong></p><p>但在 JWT 方案里，服务器是不存状态的。<br/>Token 发出去了，就像泼出去的水。只要还在有效期内（比如 2 小时），哪怕你把服务器重启了、把用户密码改了，拿着旧 Token 的黑客依然能畅通无阻。</p><p><strong>这时候你会想各种补救办法，但你会发现，每个办法都很尴尬：</strong></p><h3>1. "那我把过期时间设短点？比如 5 分钟？"</h3><p>那用户每 5 分钟就得重新登录一次？体验爆炸。<br/>你说搞个 Refresh Token 自动续期？那 Refresh Token 也是 Token，它不需要作废吗？如果 Refresh Token 被偷了，黑客能无限续杯，岂不是更危险？</p><h3>2. "那搞个黑名单（Blacklist）？"</h3><p>用户注销时，把这个 Token 记到 Redis 黑名单里。每次请求都查一下是不是在黑名单。<br/><strong>打脸时刻</strong>：兄弟，你既然都要查 Redis 了，为什么不直接用 Session？<br/>JWT 的最大优势就是"无状态、不查库"，你现在每秒几万次请求都要查黑名单，那 JWT 的性能优势还在哪？</p><h2>为什么大厂（特别是金融/支付）偏爱 Session？</h2><p>除了"无法废止"这个硬伤，JWT 还有几个隐性成本，大厂算得很精：</p><h3>1. 续签（Renewal）问题</h3><p>Session 续签是无感的。只要你一直在操作，服务端就在 Redis 里顺手把你的过期时间往后延。<br/>JWT 里的过期时间是写死在 Payload 里的。想续签？必须发一个新的 JWT 给你。前端得写一堆拦截器逻辑：发现快过期了 -&gt; 拿着旧 Token 换新 Token -&gt; 重发请求。<strong>复杂度的天平，从后端倾斜到了前端。</strong></p><h3>2. 带宽占用</h3><p>Session ID 只有 32 个字节。<br/>一个包含基本信息的 JWT，动不动就几百个字节。如果你的 Token 放在 Header 里，每次 HTTP 请求都要多带几百字节的数据。对于像淘宝、微信这种亿级流量的入口，光是这多出来的流量成本就是一笔巨款。</p><h3>3. 数据实时性</h3><p>JWT 里的信息是"快照"。<br/>你刚登录时是普通会员，生成了 JWT。下一秒你充钱成了 VIP。<br/>但你手里的 JWT 写的还是"普通会员"。除非你重新登录，或者服务端在验证 Token 后再查一次库（又回到了查库的老路），否则你的 VIP 权益无法即时生效。<br/>Session 每次都查 Redis，天然保证数据是最新的。</p><h2>那 JWT 到底有什么用？</h2><p>把 JWT 贬得一文不值也不对。存在即合理，JWT 在以下场景是<strong>绝杀</strong>：</p><h3>1. 微服务/服务间调用（Machine-to-Machine）</h3><p>A 服务调 B 服务，不用维持长连接会话。发一个短期的 JWT，B 服务解密验证签名就知道是谁调的，效率极高。</p><h3>2. 单次授权 Token</h3><p>比如"重置密码链接"、"邮箱验证链接"。<br/>发一个 JWT 放在 URL 里，有效期 10 分钟。用户点开，验签通过，准许改密码。用完即废，不需要维持状态。</p><h3>3. 不想/不能做服务端存储</h3><p>比如一些简单的工具类网站，没钱买 Redis，只想撸个单机版 Node.js，那 JWT 是真香。</p><h2>面试怎么答？</h2><p><strong>简洁版</strong>（30 秒）：</p><blockquote><p>JWT 最大的优势是无状态，但也正是它的劣势。因为无状态，所以服务端无法主动废止 Token（比如用户改密、被盗号场景）。要解决这个问题通常需要引入 Redis 做黑名单，这就违背了 JWT 无状态的初衷。</p><p>相比之下，Session + Redis 方案虽然有状态，但能做到精细化的权限控制和实时踢人下线。对于复杂的 C 端业务，Session 的安全性和控制力更好；而 JWT 更适合微服务间的授权或一次性验证。</p></blockquote><p><strong>进阶版</strong>（1 分钟，带架构思考）：</p><blockquote><p>技术选型没有银弹，只有取舍。</p><p><strong>Session 的本质是"控制"</strong> ：服务端掌握绝对控制权，适合对安全性要求高、需要实时管理用户状态的场景（如电商、银行）。缺点是需要维护存储组件（Redis），有扩容成本。</p><p><strong>JWT 的本质是"交换"</strong> ：用计算（CPU 验签）换存储（内存/Redis）。适合服务间通信，或者对即时性要求不高的应用。</p><p>如果很多大厂还在用 Session，往往是因为他们的基础设施（Redis 集群）已经足够强大，相比于 JWT 带来的"无法废止"风险，他们更愿意承担存储成本来换取绝对的安全控制权。</p></blockquote><p><strong>最后总结一句：</strong><br/>不要为了用新技术而用新技术。如果你的业务需要"此时此刻把这个讨厌的用户踢出去"，请老老实实拥抱 Session。</p><blockquote><p><strong>⚡️ 别把时间浪费在低效复习上</strong></p><p>很多人复习抓不住重点。作为过来人，我分析了100+份大厂面试记录，将 <strong>Go/Java/AI 的核心考察点、高频题、易错点</strong> 浓缩进了一份 PDF。</p><p><strong>不搞虚的，全是干货。</strong></p><p><strong>加我微信：wangzhongyang2025</strong>，备注 <strong>【面经】</strong> 免费发你，立即纠正你的复习方向，把时间用在刀刃上。</p></blockquote>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-为什么 IT项目看起来成功，但业务不满意？ ServiceDeskPl]]></title>    <link>https://segmentfault.com/a/1190000047577802</link>    <guid>https://segmentfault.com/a/1190000047577802</guid>    <pubDate>2026-01-28 15:13:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在许多企业中，ITSM 系统、IT 工单管理系统 以及 ITIL 流程 的落地，往往被视为一项阶段性成功：系统上线了，流程跑起来了，指标也能在仪表板上“交差”。</p><p><img width="507" height="351" referrerpolicy="no-referrer" src="/img/bVdnNj3" alt="" title=""/></p><p>然而，另一种声音却在业务侧反复出现——“流程是规范了，但事情并没有更好办”“找 IT 还是慢”“体验反而更复杂了”。这种割裂感，几乎贯穿了所有规模的组织。</p><p>问题并不在于 ITSM 是否有价值，而在于：ITSM 的“成功标准”，往往只在 IT 视角成立。</p><p><strong>当“项目成功”不等于“服务成功”</strong></p><p>在 IT 团队内部，ITSM 项目通常围绕一组清晰、可量化的目标推进：</p><p>-工单是否全部纳入系统</p><p>-事件、问题、变更流程是否符合 ITIL 要求</p><p>-SLA 是否达标</p><p>-报表是否可视化</p><p>从项目管理角度看，这些目标完全合理。但问题在于，它们更多衡量的是系统运行是否“合规”，而不是服务交付是否“有效”。</p><p>这正是 ITSM 项目最常见的第一个断层：指标完成 ≠ 服务被认可。</p><p><strong>ITSM 成功的最大误区：把“管理”当成“服务”</strong></p><p>从根本上说，ITSM 失败的原因并不是工具能力不足，而是视角错位：</p><p>IT 关注的是可控性，而业务关注的是可用性。</p><p>当 ITSM 只用于“规范 IT 行为”，而未用于“优化业务体验”，即便系统再先进，业务满意度也难以提升。</p><p><strong>从“IT 视角成功”到“业务视角成功”的转化模型</strong></p><p>要真正解决“ITSM 看起来成功，但业务依然不满意”的问题，关键并不在于增加流程或工具功能，而在于重新定义什么才是成功。</p><p>成熟组织通常会采用一种“双层指标模型”，例如 <strong><a href="https://link.segmentfault.com/?enc=pancEqx9LmzQ43R2Dw8s6Q%3D%3D.jnldxb9b%2Fz8tbP2tNcYK0QXBM9BFKMvijjDtqh9NhHGHweLpCSDbl8qIpobpSKmHMbvsPysPV%2F56fCsWEXPY3flyVgpr53bs4OXxagWMzCw%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a></strong> ServiceDesk Plus将 ITSM 的技术指标映射到业务结果上。</p><p><strong>Q1：为什么 ITSM 上线后业务满意度反而下降？</strong></p><p>通常是流程复杂度上升、体验未同步优化，导致业务感知成本提高。</p><p><strong>Q2：SLA 达标是否还能作为核心指标？</strong></p><p>可以，但必须与业务影响指标结合，否则容易产生误导。</p><p><strong>Q3：ITSM 如何支撑跨部门服务？</strong></p><p>关键在于统一入口、共享上下文以及可编排的工作流能力。</p><p><strong>Q4：中小企业是否需要这么复杂的 ITSM？</strong></p><p>不是复杂，而是适配。规模越小，越需要避免过度设计。</p>]]></description></item><item>    <title><![CDATA[中小微到大型企业的CRM选型指南：4大核心维度的10款主流品牌深度横评 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047577805</link>    <guid>https://segmentfault.com/a/1190000047577805</guid>    <pubDate>2026-01-28 15:12:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化转型浪潮中，<strong>CRM（客户关系管理）系统</strong>已从“销售工具”升级为“企业全域增长引擎”——不仅要解决“获客 - 销售”的基础流程，更要串联“上下游协作 - 生产交付”的全链路闭环。本文选取<strong>超兔一体云、Oracle CX、Capsule CRM、Bitrix24、Brevo、励销云、探马SCRM、Odoo CRM、YetiForce、Dolibarr</strong>10款主流CRM/ERP产品，从<strong>获客/市场、销售管理、上下游管理、MES生产管理</strong>四大核心维度展开深度对比，为不同规模、不同行业的企业提供选型参考。</p><h2>一、核心能力框架：4大维度的底层逻辑</h2><p>在对比前，先明确4大维度的<strong>底层价值逻辑</strong>——企业的增长需要“从获客到交付”的全链路闭环，每个维度都对应着闭环中的关键环节：</p><ul><li><strong>获客/市场</strong>：解决“流量从哪来、线索怎么转”的问题，核心是“精准触达 + 高效转化”；</li><li><strong>销售管理</strong>：解决“线索如何变成订单”的问题，核心是“流程标准化 + 效率提升”；</li><li><strong>上下游管理</strong>：解决“订单如何落地”的问题，核心是“生态协同 + 数据打通”；</li><li><strong>MES生产管理</strong>：解决“产品如何交付”的问题，核心是“销售需求与生产的联动”。</li></ul><h2>二、核心维度深度对比</h2><h3>（一）获客/市场：从“流量覆盖”到“精准转化”的能力分层</h3><h4>1. 各品牌能力拆解</h4><table><thead><tr><th>品牌</th><th>获客/市场核心能力</th><th>优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>多渠道集客（百度/巨量、官网/微信、地推/会销、工商搜客）；线索一键处理 + 分配提醒；营销物料（话术/文件/竞品）</td><td>toB/toC混合场景、需要全渠道覆盖的中小微企业</td></tr><tr><td><strong>Oracle CX</strong></td><td>数据驱动（CDP整合多渠道线索）；AI个性化营销（跨渠道触达）；营销自动化（活动编排 + 效果优化）</td><td>大型企业、需要精准营销 + 数据沉淀的高科技/制造行业</td></tr><tr><td><strong>Capsule CRM</strong></td><td>无明确获客功能（仅官网提“赢更多交易”）</td><td>小型企业、无需复杂获客工具，聚焦销售转化</td></tr><tr><td><strong>Bitrix24</strong></td><td>线索获取（邮件营销、表单生成器）；多渠道线索整合</td><td>团队协作型企业、需要基础营销工具的中小微企业</td></tr><tr><td><strong>Brevo</strong></td><td>强营销自动化（邮件/短信触达、客户分群）；多渠道效果评估</td><td>依赖线上营销的企业、需要批量触达 + 转化追踪的电商/ SaaS行业</td></tr><tr><td><strong>励销云</strong></td><td>AI电话机器人（日呼千次）；LBS定位筛选高意向客户；线索清洗 + 外呼</td><td>电销型企业、需要高效获客的toB行业（如金融/教育）</td></tr><tr><td><strong>探马SCRM</strong></td><td>微信生态深度集成（社群裂变、客户标签/行为轨迹）；社交化营销</td><td>依赖微信获客的企业、需要私域运营的零售/服务行业</td></tr><tr><td><strong>Odoo CRM</strong></td><td>营销自动化（活动编排）；线索管理（自定义字段/报表）；与ERP集成</td><td>技术型企业、需要开源定制 + 一体化管理的制造/贸易行业</td></tr><tr><td><strong>YetiForce</strong></td><td>营销活动管理；线索追踪（自定义字段）</td><td>有技术团队的企业、需要基础营销功能的中小微企业</td></tr><tr><td><strong>Dolibarr</strong></td><td>线索管理（邮件营销、基础表单）；与ERP集成</td><td>小型制造/贸易企业、需要基础获客工具的低成本需求</td></tr></tbody></table><h4>2. 关键流程可视化：超兔一体云获客流程</h4><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/8b02d62016ca440a88c71b8bf2619189~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5YWU6ICz5py1:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTMwMzM1Mjg4NDg1NzEzMiJ9&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769667044&amp;x-orig-sign=FkOmKiU3LunabhfC7S0fRH0aSYs%3D" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><h4>3. 雷达图评分（10分制）</h4><table><thead><tr><th>品牌</th><th>获客/市场</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td></tr><tr><td>Oracle CX</td><td>8</td></tr><tr><td>Brevo</td><td>7</td></tr><tr><td>励销云</td><td>8</td></tr><tr><td>探马SCRM</td><td>7</td></tr><tr><td>Odoo CRM</td><td>7</td></tr><tr><td>Bitrix24</td><td>6</td></tr><tr><td>YetiForce</td><td>6</td></tr><tr><td>Dolibarr</td><td>5</td></tr><tr><td>Capsule CRM</td><td>3</td></tr></tbody></table><h3>（二）销售管理：从“流程标准化”到“效率提升”的能力差异</h3><h4>1. 各品牌能力拆解</h4><table><thead><tr><th>品牌</th><th>销售管理核心能力</th><th>优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>客户中心（个性化配置 + 生命周期 + 查重）；多种跟单模型（小单快单/商机/多方项目）；合同订单（多模型 + 财务管控）</td><td>中小微企业、需要适配不同业务场景（小单/长单/项目）的制造/服务行业</td></tr><tr><td><strong>Oracle CX</strong></td><td>销售流程自动化（线索→商机→CPQ→合同）；AI定价/订单优化；销售绩效（预测 + 目标管理）</td><td>大型企业、需要复杂流程 + 绩效管控的高科技/制造行业</td></tr><tr><td><strong>Capsule CRM</strong></td><td>极简易用（联系人/机会跟踪、任务提醒）；单一客户视图（整合互动记录）</td><td>小型企业、无需复杂功能，聚焦销售跟进的零售/服务行业</td></tr><tr><td><strong>Bitrix24</strong></td><td>销售漏斗可视化；商机跟踪；任务提醒</td><td>团队协作型企业、需要基础销售工具的中小微企业</td></tr><tr><td><strong>Brevo</strong></td><td>基础销售流程（线索→商机→订单）；客户管理</td><td>依赖线上销售的企业、需要简单流程的电商/ SaaS行业</td></tr><tr><td><strong>励销云</strong></td><td>客户查重（防撞单）；SCRM（客户标签/行为）；销售流程自动化</td><td>电销型企业、需要避免撞单 + 客户分层的金融/教育行业</td></tr><tr><td><strong>探马SCRM</strong></td><td>销售漏斗（社交化机会跟踪）；客户生命周期（微信互动记录）；任务提醒</td><td>依赖微信销售的企业、需要私域转化的零售/服务行业</td></tr><tr><td><strong>Odoo CRM</strong></td><td>销售管道（可视化跟踪）；CPQ报价管理；与ERP/财务集成</td><td>技术型企业、需要一体化管理的制造/贸易行业</td></tr><tr><td><strong>YetiForce</strong></td><td>销售漏斗；合同管理；客户服务工单</td><td>有技术团队的企业、需要基础销售功能的中小微企业</td></tr><tr><td><strong>Dolibarr</strong></td><td>客户订单管理；与库存/财务联动</td><td>小型制造/贸易企业、需要基础销售 + 库存协同的低成本需求</td></tr></tbody></table><h4>2. 关键流程可视化：Oracle CX销售流程</h4><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/17245d29d59b42b2b969075018a7b47c~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5YWU6ICz5py1:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTMwMzM1Mjg4NDg1NzEzMiJ9&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769667045&amp;x-orig-sign=Fs62ntEIX6bS65kB6uNruQEuQdk%3D" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h4>3. 雷达图评分（10分制）</h4><table><thead><tr><th>品牌</th><th>销售管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td></tr><tr><td>Oracle CX</td><td>9</td></tr><tr><td>Odoo CRM</td><td>8</td></tr><tr><td>探马SCRM</td><td>8</td></tr><tr><td>励销云</td><td>7</td></tr><tr><td>Bitrix24</td><td>7</td></tr><tr><td>YetiForce</td><td>7</td></tr><tr><td>Capsule CRM</td><td>6</td></tr><tr><td>Dolibarr</td><td>6</td></tr><tr><td>Brevo</td><td>5</td></tr></tbody></table><h3>（三）上下游管理：从“内部管控”到“生态协同”的能力进阶</h3><h4>1. 各品牌能力拆解</h4><table><thead><tr><th>品牌</th><th>上下游管理核心能力</th><th>优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>OpenCRM平台（连接内部CRM与上下游）；上下游协作（报价/订单/对账/物流）；三流合一</td><td>需要供应链协同的中小微企业、toB项目型业务（如设备制造/工程）</td></tr><tr><td><strong>Oracle CX</strong></td><td>PRM（合作伙伴关系管理）；与Oracle ERP深度集成（库存/订单/交付）</td><td>大型企业、需要复杂生态协同的制造/零售行业</td></tr><tr><td><strong>Capsule CRM</strong></td><td>无</td><td>小型企业、无需上下游协作</td></tr><tr><td><strong>Bitrix24</strong></td><td>项目协作模块（间接管理外部合作）</td><td>团队协作型企业、需要基础协作的中小微企业</td></tr><tr><td><strong>Brevo</strong></td><td>无</td><td>依赖线上销售的企业、无需上下游协作</td></tr><tr><td><strong>励销云</strong></td><td>无</td><td>电销型企业、无需上下游协作</td></tr><tr><td><strong>探马SCRM</strong></td><td>无</td><td>依赖微信销售的企业、无需上下游协作</td></tr><tr><td><strong>Odoo CRM</strong></td><td>通过ERP模块扩展（供应商管理、采购流程）</td><td>技术型企业、需要一体化供应链管理的制造/贸易行业</td></tr><tr><td><strong>YetiForce</strong></td><td>集成第三方工具（如ERP）</td><td>有技术团队的企业、需要基础协作的中小微企业</td></tr><tr><td><strong>Dolibarr</strong></td><td>ERP模块（供应商管理、采购流程）</td><td>小型制造/贸易企业、需要基础供应链协同的低成本需求</td></tr></tbody></table><h4>2. 关键流程可视化：超兔OpenCRM上下游协作流程</h4><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/0750c451b9f54737a3768074b4b6a416~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5YWU6ICz5py1:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTMwMzM1Mjg4NDg1NzEzMiJ9&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769667045&amp;x-orig-sign=%2FpOOP3Lv0qAcGtwKg3qA5KcGdFs%3D" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h4>3. 雷达图评分（10分制）</h4><table><thead><tr><th>品牌</th><th>上下游管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>8</td></tr><tr><td>Oracle CX</td><td>7</td></tr><tr><td>Odoo CRM</td><td>6</td></tr><tr><td>Dolibarr</td><td>5</td></tr><tr><td>Bitrix24</td><td>4</td></tr><tr><td>YetiForce</td><td>3</td></tr><tr><td>Brevo</td><td>3</td></tr><tr><td>励销云</td><td>3</td></tr><tr><td>探马SCRM</td><td>3</td></tr><tr><td>Capsule CRM</td><td>2</td></tr></tbody></table><h3>（四）MES生产管理：从“销售驱动”到“生产协同”的能力闭环</h3><h4>1. 各品牌能力拆解</h4><table><thead><tr><th>品牌</th><th>MES生产管理核心能力</th><th>优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>轻量化MES（排程/报工/质检/入库）；与CRM联动（销售订单→生产排产）；MRP物料计算</td><td>中小微生产企业、需要销售 - 生产一体化的制造/装配行业</td></tr><tr><td><strong>Oracle CX</strong></td><td>集成第三方MES/ERP（销售订单同步生产）；生产进度反馈客户服务</td><td>大型企业、需要生产 - 客户联动的高科技/制造行业</td></tr><tr><td><strong>Capsule CRM</strong></td><td>无</td><td>小型企业、无需生产管理</td></tr><tr><td><strong>Bitrix24</strong></td><td>无</td><td>团队协作型企业、无需生产管理</td></tr><tr><td><strong>Brevo</strong></td><td>无</td><td>依赖线上销售的企业、无需生产管理</td></tr><tr><td><strong>励销云</strong></td><td>无</td><td>电销型企业、无需生产管理</td></tr><tr><td><strong>探马SCRM</strong></td><td>无</td><td>依赖微信销售的企业、无需生产管理</td></tr><tr><td><strong>Odoo CRM</strong></td><td>安装MES模块（生产计划/工单/设备监控）；与ERP集成</td><td>技术型企业、需要开源定制的制造/装配行业</td></tr><tr><td><strong>YetiForce</strong></td><td>无</td><td>有技术团队的企业、无需生产管理</td></tr><tr><td><strong>Dolibarr</strong></td><td>插件扩展（社区支持有限）</td><td>小型制造企业、需要基础生产功能的低成本需求</td></tr></tbody></table><h4>2. 关键流程可视化：超兔MES - CRM联动流程</h4><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/b86860c8ccb545b1be02a3d29b926e5f~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5YWU6ICz5py1:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMTMwMzM1Mjg4NDg1NzEzMiJ9&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769667045&amp;x-orig-sign=8t0XkCf8uKR9363J%2Fv8mujbYCGM%3D" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h4>3. 雷达图评分（10分制）</h4><table><thead><tr><th>品牌</th><th>MES生产管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td></tr><tr><td>Odoo CRM</td><td>7</td></tr><tr><td>Oracle CX</td><td>6</td></tr><tr><td>Dolibarr</td><td>4</td></tr><tr><td>YetiForce</td><td>2</td></tr><tr><td>Bitrix24</td><td>2</td></tr><tr><td>其他品牌</td><td>1</td></tr></tbody></table><h2>三、综合能力雷达图：各品牌的“长短板”</h2><p>基于4大维度的评分，各品牌的综合能力可通过雷达图直观呈现（10分制，维度：获客/市场、销售管理、上下游管理、MES生产管理）：</p><table><thead><tr><th>品牌</th><th>获客/市场</th><th>销售管理</th><th>上下游管理</th><th>MES生产管理</th><th>综合定位</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>9</td><td>9</td><td>8</td><td>9</td><td>中小微企业“一体化增长引擎”，覆盖全链路闭环</td></tr><tr><td><strong>Oracle CX</strong></td><td>8</td><td>9</td><td>7</td><td>6</td><td>大型企业“数据驱动型CRM”，聚焦精准营销 + 流程自动化</td></tr><tr><td><strong>Odoo CRM</strong></td><td>7</td><td>8</td><td>6</td><td>7</td><td>技术型企业“开源定制平台”，适合需要一体化管理的制造/贸易行业</td></tr><tr><td><strong>探马SCRM</strong></td><td>7</td><td>8</td><td>3</td><td>1</td><td>微信生态“私域运营工具”，适合依赖微信获客的零售/服务行业</td></tr><tr><td><strong>励销云</strong></td><td>8</td><td>7</td><td>3</td><td>1</td><td>电销型企业“高效获客工具”，适合需要批量触达的toB行业</td></tr><tr><td><strong>Brevo</strong></td><td>7</td><td>5</td><td>3</td><td>1</td><td>线上营销“自动化工具”，适合依赖邮件/短信的电商/ SaaS行业</td></tr><tr><td><strong>Bitrix24</strong></td><td>6</td><td>7</td><td>4</td><td>2</td><td>团队协作“基础CRM”，适合需要简单工具的中小微企业</td></tr><tr><td><strong>Dolibarr</strong></td><td>5</td><td>6</td><td>5</td><td>4</td><td>小型企业“低成本ERP + CRM”</td></tr></tbody></table><h2>四、总结与建议</h2><p>在企业数字化转型的进程中，选择适合自身的CRM系统至关重要。不同品牌的CRM系统在获客/市场、销售管理、上下游管理和MES生产管理等核心维度上各有优劣。</p><p>对于中小微企业而言，如果希望实现全链路闭环管理，超兔一体云是一个不错的选择，它在各个维度都有出色的表现，能够为企业提供一体化的解决方案，助力企业全面提升运营效率。大型企业若追求精准营销和复杂流程的自动化管理，Oracle CX则凭借其强大的数据驱动能力和完善的流程管控体系，成为理想之选。技术型企业可考虑Odoo CRM，其开源定制的特性能够满足企业对一体化管理的个性化需求。</p><p>依赖微信生态获客的零售/服务行业，探马SCRM的私域运营功能可以帮助企业更好地管理客户关系；电销型企业使用励销云的高效获客工具，能够提高销售效率；而依赖线上营销的电商/SaaS行业，Brevo的营销自动化功能则能发挥重要作用。团队协作型中小微企业可选择Bitrix24作为基础的CRM工具，小型制造/贸易企业对于低成本的基础获客和销售管理需求，Dolibarr是一个合适的选择；小型企业若仅需要简单的销售跟进功能，Capsule CRM的极简易用特性能够满足其需求。</p><p>企业在选型时，应充分评估自身的规模、行业特点、业务需求以及数字化转型的目标，综合考虑各品牌的“长短板”，做出最适合自己的决策，从而让CRM系统真正成为企业全域增长的强大引擎。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[为什么开源OCR在Demo阶段很好，用到项目就开始出问题？ 合合技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047577824</link>    <guid>https://segmentfault.com/a/1190000047577824</guid>    <pubDate>2026-01-28 15:11:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​1分钟速览</p><blockquote>开源 OCR / 文档解析在 demo 阶段表现良好，是因为你验证的是“算法是否可行”； 而在真实项目中出问题，是因为你真正需要的是“一个可长期运行的工程系统”。</blockquote><p>这不是你当初判断失误，而是项目进入了<strong>必须升级文档底座的阶段</strong>。</p><p>当你开始在解析层遇到不可控问题时，真正要问的已经不是</p><p>“还能不能再调一调”，</p><p>而是：</p><blockquote>这个能力，是否已经到了必须交给生产级系统来承担的时候。</blockquote><hr/><p>当我们构建一个需要处理文档的AI系统时，选择技术栈的第一个决策点往往是文档解析。许多团队的开局惊人相似：选择一个流行的开源OCR工具，快速搭建演示原型，看着它流畅地识别测试文档中的文字和表格，然后满怀信心地推进项目。</p><p>然而，当项目真正进入生产阶段，面对成千上万的真实文档时，最初的信心往往开始动摇。</p><p>如果你正在推进下面这类项目：</p><ul><li>集团级 <strong>知识库 / AI 中台</strong></li><li>面向业务的 <strong>RAG / 文档 Agent</strong></li><li>审计、法务、科研等 <strong>文档密集型系统</strong></li></ul><p>那你很可能遇到过一个相同的现象：</p><blockquote>开源 OCR / 文档解析在 demo 阶段表现不错，但一进入真实项目，问题就开始集中暴露。</blockquote><p>这并不罕见，也并不意味着你当初的技术判断是错误的。</p><p><strong>这不是某个工具的问题，而是一个“阶段错配”的问题</strong>。</p><h2>一、为什么在 demo 阶段，开源方案是“合理选择”？</h2><p>在项目早期，也就是概念验证阶段，大多数团队的验证目标非常清晰且有限：</p><ul><li>能不能识别文字？</li><li>表格结构大致对不对？</li><li>能不能接到下游模型里跑通一条链路？</li></ul><p>此时的文档样本通常经过挑选，它们是清晰的扫描件、结构简单的表格，其特征也较为明显：</p><ul><li>样本量小</li><li>文档相对干净</li><li>格式单一、可控</li><li>人工肉眼校验即可</li></ul><p>在这个阶段，开源OCR或文档解析工具往往表现良好，<strong>完全可以满足需求</strong>：</p><ul><li>成本优势明显（零直接成本）</li><li>快速集成能力</li><li>社区支持与可定制性</li><li>满足“看起来有效”的演示需求</li></ul><p><strong>从技术决策角度看，这个选择是理性的</strong>。</p><p>问题不出在这里，但也埋下了一个种子：团队验证的是“算法是否工作”，而非“系统能否稳定运行”。</p><h2>二、什么时候问题开始出现？不是“用久了”，而是“换阶段了”</h2><p>真正的问题不随时间线性出现，而是在项目跨越某个临界点时集中爆发。这个分水岭通常出现在项目进入以下状态之一时：</p><ul><li><p>文档规模开始上量（成千上万页）</p><ul><li>从几十个样本文档到数万页的实际业务文档，处理压力从算法层面转移到工程层面。</li></ul></li><li><p>文档类型开始混杂</p><ul><li>不同年代的扫描件（从高清到低分辨率）</li><li>多语言混合文档</li><li>复杂表格（尤其是跨页表格）</li><li>手写注释与印刷体混合</li></ul></li><li><p>解析结果被多个下游系统依赖</p><ul><li>RAG</li><li>信息抽取</li><li>审核、比对</li><li>数据入库</li></ul></li></ul><p>在一些科研、法务、审计类项目中，单个文件就可能是上千页，而且对准确率有明确业务责任。<br/>这时，团队往往会发现：</p><blockquote>demo 阶段没暴露的问题，开始以“不可预测”的方式集中出现。</blockquote><h2>三、问题为什么不是“识别率不够高”，而是“系统开始不稳定”？</h2><p>进入项目阶段后，问题的表现形式通常不是“完全不可用”，而是：</p><ul><li>表格偶尔错位</li><li>标题层级不稳定</li><li>阅读顺序偶发错误</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577826" alt="图片" title="图片"/></p><pre><code>                               复杂表格结构出错
</code></pre><p>生产环境中最棘手的问题不是“识别率从95%降到85%”，而是无法预测的失败模式。这些问题单看一次，似乎都不严重。</p><p>在真实系统中，它们会被<strong>下游能力放大</strong>：</p><ul><li>错位的表格 → 抽取字段整体偏移</li><li>错乱的结构 → RAG 召回范围失真</li><li>顺序错误 → 模型给出“看起来合理但不可信”的答案</li></ul><p>这也是为什么很多团队会产生错觉：</p><blockquote>“是不是模型还需要再调一调？”</blockquote><h2>四、为什么这是工程级问题，而不是参数或模型问题？</h2><p>许多团队最初的应对策略是增加后处理规则。然而，他们很快发现一个事实：</p><p><strong>一旦信息在解析阶段丢失，后续几乎无法可靠恢复。</strong></p><h4>为什么后处理救不了？</h4><ul><li>跨页表格一旦在解析阶段被拆断，后处理无法稳定还原结构</li><li>标题层级丢失，本质是上下文关系消失</li><li>这类错误不是“规则没写够”，而是信息已经丢失</li></ul><h4>为什么模型背不了这个锅？</h4><ul><li>模型只能基于输入推理</li><li>输入结构不稳定，模型只会稳定地产生不稳定结果</li></ul><p>在一些审计和数据处理项目中，团队尝试直接用多模态模型做文档抽取，但很快遇到两个现实限制：</p><ul><li><strong>吞吐和延迟无法支撑批量处理</strong></li><li>泛化能力不足，格式一变就失效</li></ul><p>最终结论往往是：</p><blockquote>问题不在模型能力，而在缺少一个稳定、可控的解析层。</blockquote><h2>五、成熟团队是如何看待“文档解析”的？</h2><p>在已经跑过真实项目的团队里，会出现一个明显的认知转变：</p><blockquote>文档解析不是一个功能，而是基础设施。</blockquote><p>成熟方案通常具备几个共性：</p><ul><li><p>优先保证结构稳定</p><ul><li>表格连续性（尤其是跨页）</li><li>标题层级一致</li><li>阅读顺序可预期</li></ul></li><li><p>以工程系统形态存在</p><ul><li>支持批量、异步处理</li><li>有失败重试和状态追踪</li><li>上量后性能可预测</li></ul></li><li><p>能被长期复用</p><ul><li>同时服务 RAG、抽取、审核、入库</li><li>而不是一次性脚本或 Demo 工具</li></ul></li></ul><p>这正是面向生产的解析系统——如TextIn xParse——所采用的方法论：不追求单一的“最智能”算法，而是构建可预测、可监控、可维护的工程系统。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577827" alt="图片" title="图片" loading="lazy"/></p><p>例如，面对复杂表格，TextIn xParse更注重表格结构还原、标题/注释与表格的语义关联，而不仅仅是字符识别率。</p><h2>六、在真实项目中，解析通常处在什么位置？</h2><p>在生产系统中，解析能力通常处在一个非常明确的位置：</p><p>文档输入 </p><p>↓ </p><p>解析（结构化/去噪/表格/层级/顺序） </p><p>↓ </p><p>标准化输出 </p><p>↓ </p><p>RAG/抽取/审核/数据处理</p><p>换句话说：</p><blockquote>解析层决定了后面所有 AI 能力的上限和稳定性。</blockquote><h2>七、为什么生产级文档解析，不能只靠开源工具补出来？</h2><p>这不是“开源好不好”的问题，而是<strong>阶段是否匹配</strong>的问题。</p><p>开源OCR工具的设计目标通常是解决广泛的通用识别问题，提供算法实现参考，以及满足研究和轻量级应用需求。</p><p>而当你的系统开始具备以下特征：</p><ul><li>长期运行</li><li>批量处理</li><li>多业务依赖</li><li>对准确率和可追溯性有责任</li></ul><p>那你需要的已经不是一个“能跑的工具”，而是一个能长期运行的工程级能力。</p><p>当团队选择基于开源工具自建解析系统时，往往低估了：</p><ol><li>维护成本：持续适应新文档格式、修复边缘案例</li><li>集成成本：与下游系统深度整合的复杂性</li><li>机会成本：团队时间从核心业务逻辑转移到基础设施维护</li><li>风险成本：解析错误导致的业务决策风险</li></ol><p>这也是为什么在科研、法律、审计等对精度、稳定性、本地化高度敏感的项目中，文档解析会被当作生产级底座来选型，而不是临时方案——正是由于隐性成本往往远超采用专业解决方案的直接成本。</p><h2>八、一个国家实验室的知识库建设历程</h2><p>一个国家级科研机构的项目演进过程清晰验证了文档解析应用可能面对的阶段与问题。该实验室最初的目标是构建一个覆盖其核心领域科研成果的内部知识库，用于辅助研究人员快速检索相关文献、实验数据和报告。</p><p><strong>第一阶段：快速原型验证</strong></p><p>项目初期，团队选择了流行的开源OCR和文档解析工具包。在有限的演示数据集上——几十份清晰扫描的论文和报告——系统表现令人满意。文字识别准确，基本表格结构得以保留，与初步搭建的检索系统对接顺利。这一阶段成功证明了“技术路径可行”，项目如期进入全面开发。</p><p><strong>第二阶段：规模化遭遇瓶颈</strong></p><p>当系统开始导入真实的库存文档时，问题开始暴露。这些文档包括：</p><ul><li>年代较久远的研究文件（部分为低质量复印件）</li><li>包含复杂跨页数据表格的年度报告</li><li>多语言混合的国际合作论文</li><li>带有大量手写批注的实验文件</li></ul><p>在数千份文档的批量处理中，团队观察到：</p><ol><li>性能不可预测：处理时间波动极大，从数秒到数分钟不等，无法预估整体完成时间</li><li>错误模式随机：同一份文档两次处理可能得到不同结果，特别是复杂表格的结构</li><li>维护负担沉重：每出现一种新文档格式，就需要编写新的后处理规则</li></ol><p><strong>第三阶段：基础设施升级</strong></p><p>面对上线期限和准确性要求的双重压力，团队重新评估了解析层的定位。他们需要的不是“另一个更聪明的算法”，而是一个能够提供：</p><ul><li><strong>稳定结构输出：</strong>确保相同文档类型获得一致解析结果</li><li><strong>可预测性能：</strong>支持大规模批量处理，有明确的时间预估</li><li><strong>专业格式支持：</strong>专门优化对科研文档中复杂表格、公式、图表注释的处理能力</li></ul><p>基于这些标准，实验室最终选择了 TextIn xParse 作为生产环境的解析引擎。切换后最显著的改善不仅仅是准确率的提升，更是：</p><ul><li>处理速度变得可预测，万页级文档库的解析时间从不可预估降至可控范围</li><li>跨页表格的连贯性得到保障，数据完整性不再依赖运气</li><li>系统维护工作量大幅降低，团队重新聚焦于上层知识库应用逻辑的开发</li></ul><p>这个案例的启示在于：当项目从“验证可能性”进入“保障可靠性”阶段时，对基础设施的要求发生了质的变化。该国家实验室的经验表明，<strong>解析能力的升级不是一种“优化”，而是在特定阶段必须完成的“切换”</strong>——从实验性工具切换到生产级系统<em>*</em>*。这种切换带来的价值，往往不在于单项指标的提升，而在于整个系统从“可能出错”到“可信赖”的状态转变。</p><h2>结论：阶段的正确匹配</h2><p>开源OCR在demo阶段表现出色，是因为它完美匹配了该阶段的需求：快速验证、低成本、灵活性。但当项目进入生产阶段，需求发生了根本变化：</p><p>从验证“是否可行”转变为保障“始终可用”。</p><p>这种转变需要的是：</p><ul><li>工程级的稳定性而非算法级的新颖性</li><li>可预测的性能而非偶尔的卓越表现</li><li>完整的生态系统而非孤立的工具</li></ul><p>当你的项目开始出现无法通过调整参数解决的解析问题时，真正需要问的不是“如何修补这个工具”，而是：</p><p>我们的文档解析需求是否已经跨越了从“实验工具”到“生产系统”的临界点？</p><p>对于已经达到这一临界点的团队，专业解析解决方案提供的不仅仅是更好的识别算法，更是一个完整的工程体系——这是从演示原型到生产系统必须跨越的鸿沟。</p><p>选择何时跨越这一鸿沟，取决于项目的规模、复杂度和风险容忍度。但一旦决定跨越，就需要相应的工程思维和工具支持，因为在这个阶段，可靠性不再是可选项，而是必需品。</p>]]></description></item><item>    <title><![CDATA[数据工程决策：自研 vs 采购 NoETL 自动化指标平台的深度分析 Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047577839</link>    <guid>https://segmentfault.com/a/1190000047577839</guid>    <pubDate>2026-01-28 15:11:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=L%2B3f20gY64xzT0FyPxhZGQ%3D%3D.%2FhrcA0k0WtNai23CefCEKElY4VVA01zdTNC4LtEphYZm1hWT4t%2FNMLlbcKUackEXDmN42jARtltSb3LjAZ%2FmkU9JjZ5nhkiAdPmWp20LFRM%3D" rel="nofollow" target="_blank">《自研指标平台是大坑？80%企业选择采购NoETL自动化指标平台》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：本文深入剖析了企业自研指标平台面临的三大核心技术挑战：统一语义层构建、智能物化加速与开放生态适配。通过对比传统静态指标字典与 NoETL 动态语义引擎的架构差异，并结合总拥有成本（TCO）分析，论证了对于绝大多数企业而言，采购成熟的 NoETL 自动化指标平台是实现数据敏捷、降低长期成本、规避技术风险的理性选择。</p><h2>认知误区：你以为在做“字典”，实际需要打造“引擎”</h2><p>企业启动自研指标平台的初衷通常是解决“口径乱”的问题，希望建立一个统一的指标目录或“字典”。然而，在 AI 驱动的数智化运营时代，业务对数据的灵活性要求呈指数级增长。一个简单的指标目录，无法支撑业务人员“任意维度、任意筛选”的自助分析，更无法让 AI 智能体（Agent）理解并调用。</p><p>“传统 ETL 通过宽表和汇总表交付指标的模式，导致了大量指标的重复开发，造成企业在存储和计算上的巨大浪费。” —— Aloudata CAN 产品白皮书</p><p>问题的本质在于，支撑现代数据分析的并非一个静态的“字典”，而是一个能实时工作的“引擎”。这个引擎需要具备：</p><ul><li>语义解析能力：理解业务术语（如“有效销售额”）背后的复杂计算逻辑（SUM(订单金额) - SUM(退款金额)）。</li><li>动态计算能力：在不预建物理宽表的前提下，实时关联多张明细表，生成“虚拟业务事实网络”。</li><li>性能保障能力：通过智能化的物化加速，确保对海量明细数据的查询也能获得秒级响应。</li></ul><p>自研项目往往始于对“统一口径”的朴素追求，却最终陷入构建一个企业级“语义计算引擎”的深水区，其技术复杂度和资源需求远超初期规划。</p><p><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdnNkD" alt="" title=""/></p><h2>挑战一：语义解析——从“静态表”到“动态虚拟宽表”</h2><p>这是自研面临的第一道技术鸿沟。传统方式通过 ETL 工程师编写 SQL，将业务逻辑固化在物理宽表（DWS/ADS）中。而 NoETL 语义编织要求平台构建一个“统一语义层”，在不进行物理打宽的前提下，通过声明式建模，让系统能实时理解并关联跨多张明细表（DWD）的业务逻辑，形成“虚拟业务事实网络”。</p><p>这要求自研团队具备编译原理、查询优化和复杂业务抽象能力，而非简单的 SQL 封装。具体挑战包括：</p><ol><li>逻辑关联的动态解析：如何让系统理解“订单表”的“客户 ID”与“客户维度表”的“客户 ID”在业务上等价，并能处理多通路等复杂场景。这需要设计一套元数据模型来声明和管理表间关联关系。</li><li>复杂指标的函数化封装：如何将“近 30 天消费金额 &gt;5000 的客户数”这类业务需求，配置化为可复用的语义函数（如跨表限定、指标维度化、二次聚合），而无需为每个需求手写数百行 SQL。这本质上是构建一个面向业务人员的“高级查询语言”及其编译器。</li><li>NL2Metrics 的意图理解：若想对接 AI，还需构建让大模型能理解的“语义知识图谱”，实现从自然语言到指标调用的精准转换（NL2Metrics），从根源上根治数据幻觉。这需要将业务指标、维度、限定条件等语义元数据结构化，并提供标准的 Function Calling 接口。</li></ol><p>自研团队需要从“SQL 脚本执行者”转变为“语义编译器设计者”，这是一个质的飞跃。</p><h2>挑战二：智能物化——从“人工运维”到“系统自治”</h2><p>即使解决了语义解析，面对企业百亿级的明细数据，如何保障查询的秒级响应？传统做法是数据工程师基于经验，手动创建和维护大量的物化视图（加速表）。但这种方式成本高昂、响应滞后，且极易形成新的数据冗余。</p><p>NoETL 平台的智能物化加速，其核心并非取消 ETL，而是将其升级为一种由“声明式策略”驱动的自动化性能服务。自研实现这一能力的难点在于：</p><ol><li>物化策略的自动生成与优化：如何基于用户对指标和维度的“加速声明”，结合数据分布和查询历史，自动设计出存储成本与查询性能最优的物化方案，并支持去重计数、比率类等复杂指标的上卷。</li><li>查询的透明改写与路由：如何让用户的查询请求（无论是来自 BI 拖拽还是 AI 调用）无感知地自动路由到最优的物化结果上，并完成底层 SQL 的透明改写，这对查询优化器的要求极高。</li><li>口径变更影响的全面分析：如何在指标口径变更时自动识别并提示所有下游影响，辅助用户根据变更影响告警进行物化任务重建和数据回刷操作，这对数据血缘解析有着极高的要求。</li></ol><p>“通过智能物化加速确保十亿、百亿级明细数据的秒级查询响应。” —— NoETL 指标平台白皮书</p><p>这要求自研团队不仅精通数据库内核优化，还需具备平台级的资源调度与成本管控（FinOps）能力。</p><h2>挑战三：生态适配——从“孤岛工具”到“中立基座”</h2><p>指标平台的终极价值在于被消费。企业内往往存在多种 BI 工具（如 Tableau、Power BI）、业务系统和新兴的 AI 应用。自研平台极易陷入为某个特定前端（如某个自研报表系统）深度定制的陷阱，成为一个新的“数据孤岛”。</p><p>真正的指标平台必须是中立的“数据中枢”，其挑战在于：</p><ol><li>标准化接口设计与实现：提供稳定、高性能的 Restful API 和成熟的 JDBC 驱动，确保下游各类应用能无差别、高性能地调用指标服务。</li><li>治理规则的内嵌与强制执行：将企业的数据安全策略（行列级权限）、审批流程等治理要求，平台化、内嵌化到指标的生产和消费链路中，从技术上保障“One Truth”的落地，而非依赖人工监督。</li><li>与现有数据湖仓的平滑集成：无需推翻重来，能通过标准连接器对接企业已有的各类数据湖仓，实现对存量宽表的“挂载”与新需求的“原生”建模混合策略，保护既有投资。</li></ol><p>生态适配能力决定了平台是企业长期演进的“基石”还是又一个短命的“项目”。</p><h2>TCO分析：自研与采购的总拥有成本对比</h2><p>决策必须超越初始采购费用，基于总拥有成本（TCO）进行理性分析。自研的初始开发成本只是冰山一角，后续高昂的持续维护、升级、扩容成本，以及因效率低下导致的业务机会成本，构成了“隐形高利贷”。</p><table><thead><tr><th>成本维度</th><th>自研模式 (典型问题)</th><th>采购 NoETL 平台 (典型收益)</th></tr></thead><tbody><tr><td>人力成本</td><td>组建并长期供养一支精通数据架构、编译原理、分布式系统的顶尖团队，招聘难、流失风险高。</td><td>将数据工程师从重复 ETL 开发中解放，转向高价值的语义建模与业务赋能，人力结构优化。</td></tr><tr><td>开发与运维成本</td><td>语义解析能力、动态查询能力、只能物化能力、查询命中与上卷等复杂功能需人工持续设计、开发、调试、运维，复杂度线性攀升。</td><td>成熟平台实现自动化指标生产、智能物化与查询路由，运维复杂度大幅降低，实现“以销定产”。</td></tr><tr><td>机会成本</td><td>需求响应慢（周/天级），压抑业务探索，错失市场机会；数据口径混乱，引发决策风险。传统方案探索性分析准确率仅 40%。</td><td>需求分钟级响应，激活业务自助分析；口径 100% 一致，构建决策信任基石。复杂任务准确率可达 98.75%。</td></tr></tbody></table><p>根据第三方测试数据，采用成熟的 NoETL 架构平台，可实现 3 年 TCO 降低 45%，需求平均响应时间缩短 90.71%，从“成本中心”转变为“效率引擎”。（来源：相关技术评测报告）</p><p><img width="723" height="307" referrerpolicy="no-referrer" src="/img/bVdnNky" alt="" title="" loading="lazy"/></p><h2>决策矩阵：何时该自研，何时该果断采购？</h2><p>企业不应一概而论。通过以下决策矩阵，可以清晰判断自身情况：</p><p>应果断采购，若:</p><ul><li>核心目标是快速实现业务数据化运营与敏捷决策。</li><li>缺乏构建并长期维护复杂数据计算引擎（语义引擎、智能物化）的核心技术团队。</li><li>需要对接多种 BI 工具和 AI 应用，避免厂商锁定。</li><li>希望控制长期 TCO，避免技术债务失控，追求确定性回报。</li></ul><p>可谨慎评估自研，若:</p><ul><li>拥有极其特殊、封闭且稳定的业务场景，市面产品完全无法满足。</li><li>具备世界级的数据系统工程团队，且将自研平台作为核心战略产品投入。</li><li>不计较时间与金钱成本，旨在技术积累。</li></ul><p>对于绝大多数追求数据敏捷、希望快速获得业务价值的企业，采购成熟的 NoETL 自动化指标平台是明确的最优解。</p><h2>常见问题 (FAQ)</h2><h4>Q1: 自研指标平台，初期投入大概需要多少人和多长时间？</h4><p>初期投入严重低估是常见陷阱。要打造一个具备基本语义解析和查询能力的原型，至少需要一个 5-8 人的资深团队（含架构、前后端、数据开发），耗时 6-12 个月。而这仅能达到“可用”水平，距离支撑企业级复杂分析、智能物化和 AI 对接的“好用”阶段，还需持续投入 2-3 年及更多资源进行迭代和运维，总成本远超预期。</p><h4>Q2: 采购 NoETL 指标平台，如何与我们现有的数据仓库集成？</h4><p>成熟的 NoETL 平台设计为中立的数据基座。它通过标准连接器直接读取您现有数据仓库的公共明细层（DWD）数据，无需数据搬迁。平台在逻辑层构建语义模型和虚拟宽表，对下游提供统一 API 服务。现有 BI 报表和 ETL 任务可以逐步迁移至新平台消费，实现平滑演进，保护既有投资。</p><h4>Q3: 如果未来业务变化很大，采购的平台会不会不够灵活？</h4><p>这正是 NoETL 平台的核心优势——应对变化。其“语义模型驱动”的架构，将易变的业务逻辑（指标口径、维度关联）上浮至可配置的语义层，而将稳定的物理存储与计算下放。当业务变化时，只需在语义层修改或新增配置，无需改动底层 ETL 和物理表。这种解耦设计使平台天生具备极强的业务适应性。</p><h4>Q4: 如何验证平台真能解决“口径不一致”和“响应慢”的问题？</h4><p>要求在 POC（概念验证）中设置真实业务场景：1) 口径验证：在平台中统一定义一个核心指标（如“有效销售额”），并确保通过 API 在不同测试报表中调用结果完全一致。2) 性能验证：针对一个涉及多表关联和复杂筛选的灵活分析需求，测试从发起查询到获取结果的端到端响应时间，要求达到秒级。同时，核查厂商提供的同类客户案例中的量化收益数据。</p><h2>核心要点</h2><ol><li>本质是引擎，而非字典：自研指标平台的核心挑战是构建具备实时语义解析与智能物化能力的“动态计算引擎”，技术复杂度远超一个静态的指标目录。</li><li>三大挑战难以逾越：语义解析（构建虚拟宽表）、智能物化（系统自治的性能服务）、生态适配（中立的数据中枢）是自研工程实现上的核心难点，需要顶尖的架构与工程团队。</li><li>TCO 揭示真实成本：自研的隐性成本（长期维护、机会成本）极高，而采购成熟平台能获得开发提效 10 倍、存算成本降 70%、分钟级响应的确定性回报。</li><li>采购是理性决策：对于绝大多数追求数据敏捷与业务价值的企业，采购经过大规模复杂场景验证的 NoETL 自动化指标平台，是规避风险、加速见效的最优路径。</li></ol><p>本文首发于 Aloudata 官方技术博客，查看更多技术细节与高清架构图，请访问原文链接：<a href="https://link.segmentfault.com/?enc=8kmjUpzXaJqlGzT6ENPKuQ%3D%3D.Ins%2FJhVOBj4JbpTpzt3ki%2FVAjvaZgHyW3zm%2BTgthjEn2xexU6AldW6vgL34SFZ6EO4yXtZURv33595C1pX%2BGkOOqC19EjjsjQlORjQvzTAA%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/noetl-automation-metric-pl...</a></p>]]></description></item><item>    <title><![CDATA[不想上班，所以我写了个能搞钱的工具 凌览 ]]></title>    <link>https://segmentfault.com/a/1190000047577856</link>    <guid>https://segmentfault.com/a/1190000047577856</guid>    <pubDate>2026-01-28 15:10:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是凌览。</p><ul><li>个人网站：<a href="https://link.segmentfault.com/?enc=4Xru7pZZeSfu5vMa3oN1wg%3D%3D.lboc5xso6Qdv1tpa7mgcmMDZMaO9ivmfrSlI5URrKDw%3D" rel="nofollow" target="_blank">blog.code24.top</a></li><li>去水印下载鸭：<a href="https://link.segmentfault.com/?enc=AGEd2yPC4qSKC9p5q9j%2Big%3D%3D.vz2Uaz%2FRjWNJBCOZXTxIw30Cz3udMTlTVLpK80Zoc6Q%3D" rel="nofollow" target="_blank">nologo.code24.top</a></li></ul><p>如果本文能给你提供启发或帮助，欢迎动动小手指，一键三连（<code>点赞</code>、<code>评论</code>、<code>转发</code>），给我一些支持和鼓励谢谢。</p><hr/><p>还记得钉钉提示音让心脏漏跳半拍的感觉？还记得周五火锅吃到一半，弹出那句"不急，周一给我"的绝望？</p><p>每个月总有那么几个瞬间，手指悬在"离职申请"上，想把那些"抓手赋能""链路闭环"的黑话统统砸回老板脸上，拉着姑娘直奔海边，让海风把KPI、OKR统统吹散。</p><p>但深夜算完银行卡、账单和退休政策后，现实很骨感：按现在的攒钱速度，我得打工打到退休。</p><p>盯着满桌演算草稿，我突然开窍：我是个开发者啊！ 既然距离自由还有十万八千里，为什么不开发个产品赚睡后收入，让代码替我加速跑路？</p><p>之前做自媒体剪视频，最烦的就是找素材——好不容易从平台扒下来的视频，中间总挂着碍眼的水印，关键帧根本没法用。</p><p>与其到处求无水印资源，或是开着 PS 一帧帧抠图，我干脆一拍键盘：不如自己写个工具，一键把水印抹干净。</p><h2>调研</h2><p>有了想法肯定得调研可不可行，找找竞品有哪些问题。</p><p>总结下来,竞品蛮多,当然问题也多。</p><p>调研完发现竞品问题很集中:</p><ol><li>个人开发居多，打开就是输入框+按钮，用户不知道怎么用</li><li>企业级产品强制开会员，成本就上来了</li><li>大部分为小程序，对于PC端并不支持</li><li>扎堆传统平台，AI视频/图片的新平台完全空白</li></ol><p>这四点全是机会，不同质化竞争，做差异化。</p><h2>设计+开发</h2><p>首先，我不是设计师，UI 这块没什么专业见解。具体做法是先扒一遍竞品，在巨人肩膀上修修补补。</p><p>技术栈没折腾，直接上熟练工：Vue3 + Tailwind + UniApp + Node.js。本职前端，后端找了个 24 小时在线的「赛博同事」——AI 负责写，我负责 Review 和改 Bug（毕竟底子还在，只是生锈了）。</p><p>坚持能简则简,什么高并发、什么数据库等选择能省则省。比如其中有个 IP 限流的功能，直接本地 JSON 文件临时存储，足够用了。</p><p>主页面 UI 如下：</p><p><img width="723" height="1109" referrerpolicy="no-referrer" src="/img/bVdnNkT" alt="" title=""/></p><h2>上线与收益</h2><p>因 wx 平台审核导致小程序名称与 PC 端品牌名被迫不一致,更繁琐的是整个上线前的各种审核——备案、公安备案、企业认证，层层审核确实消耗了大量耐心，这步不详细描述跳过。</p><p>上线后，我把它分享给了我的朋友，也得到了很多人的认可。</p><p><img width="258" height="258" referrerpolicy="no-referrer" src="/img/bVdnNkU" alt="" title="" loading="lazy"/></p><p>聊聊收益，其实我有三条来钱路子。今天先聊最「躺」的那个——流量主。</p><p>流量主有个「新手村」门槛：500 个累计用户。跨过去之后倒是省心，平台把广告组件都打包好了，接起来贼方便。<br/>PC 端本来也想挂 Google Ads 赚点美刀，但一看要填表、申请、过审……算了，拖延症犯了，至今没搞。主要是嫌烦，先放着吧。</p><p><img width="723" height="255" referrerpolicy="no-referrer" src="/img/bVdnNkV" alt="" title="" loading="lazy"/></p><p>收入不多，但每天一根火腿肠是没问题的。推广到位单靠流量主一个月也能有300，每年只需要支付认证费用30再加服务器成本，对于我来说还是赚的。</p><h2>最后</h2><p>当然，这个工具还不足以让我实现不想上班的愿望,还在努力中。</p><p>至少现在我每天能多吃一根火腿肠了！！！</p>]]></description></item><item>    <title><![CDATA[工程资料软件厂商怎么联系 聪明的拐杖 ]]></title>    <link>https://segmentfault.com/a/1190000047577874</link>    <guid>https://segmentfault.com/a/1190000047577874</guid>    <pubDate>2026-01-28 15:09:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工程建设领域，选择合适的工程资料软件对项目资料管理至关重要。当您确定心仪的软件厂商后，了解如何联系他们就成了关键一步。以下介绍几种常见且有效的联系工程资料软件厂商的途径。<br/>官方网站渠道<br/>几乎所有正规的工程资料软件厂商都拥有自己的官方网站。以筑业软件为例，您只需在搜索引擎中输入 “筑业软件官网”，即可找到其官方网站。在官网首页，通常会有 “联系我们” 的板块，点击进入后能看到详细的联系方式，包括公司地址、联系电话、电子邮箱等。通过电话，您可以直接与厂商的销售团队或技术支持人员沟通，咨询软件功能、价格、售后服务等问题；若问题较为复杂，需要详细阐述，发电子邮件也是不错的选择，厂商一般会在工作日内及时回复。<br/>在线客服咨询<br/>许多工程资料软件厂商在其官网设置了在线客服功能。比如品茗软件，官网页面上会有一个醒目的在线客服图标，可能是一个小机器人或者 “在线咨询” 按钮。点击后，会弹出聊天窗口，您可以随时与在线客服人员交流。在线客服能够快速解答一些常见问题，如软件基本功能介绍、试用版获取方式等。如果遇到技术难题，客服还会及时将问题转接给相关技术部门，并跟进处理进度，及时向您反馈。<br/>线下展会及活动<br/>行业展会、研讨会等线下活动是与工程资料软件厂商直接面对面交流的绝佳机会。每年各地都会举办各类建筑行业展会，众多软件厂商会参展并设置展位。您可以前往展位，与厂商的工作人员深入沟通，亲身体验软件的操作演示，更直观地了解软件功能。同时，在活动现场还能结识其他使用该软件的工程人员，交流使用心得和经验。此外，一些厂商还会在活动现场举办讲座或培训课程，分享行业最新动态以及软件的新功能和应用技巧。<br/>社交媒体平台<br/>如今，不少工程资料软件厂商也活跃在社交媒体平台上。像微信公众号、微博、抖音等，您可以通过搜索软件厂商的官方账号进行关注。在这些平台上，厂商会发布软件的最新资讯、功能更新、使用教程等内容。您还可以通过留言、私信等方式与厂商互动，提出您的疑问和需求。一些厂商会定期收集用户反馈，并在后续的产品优化中予以考虑。<br/>通过以上多种途径，您可以轻松与工程资料软件厂商取得联系，获取所需信息，为选择适合自己工程项目的资料软件做好充分准备。</p>]]></description></item><item>    <title><![CDATA[2026 年 1 月最新排行榜：国内 AI 能力最强的 BI 工具有哪些 看点 ]]></title>    <link>https://segmentfault.com/a/1190000047577883</link>    <guid>https://segmentfault.com/a/1190000047577883</guid>    <pubDate>2026-01-28 15:08:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、行业背景：AI 重塑 BI，企业数据价值释放的新拐点</p><p>随着生成式 AI 技术与 BI 深度融合，企业数字化转型进入 “数据智能” 新阶段。据 IDC《2024 年中国商业智能 (BI) 市场跟踪报告》显示，2024 年中国 BI 市场规模达到 78.6 亿元，同比增长 18.2%，预计到 2028 年将突破 170 亿元，年复合增长率超 15%。</p><p>然而，企业仍面临三大核心痛点：</p><p>•  数据割裂——68% 的企业数据分散在 ERP、CRM 等 10 + 系统中，跨工具整合耗时耗力；</p><p>•  工具低效—— 仅 32% 的企业实现 “数据接入→分析→可视化” 全流程闭环，多数仍依赖传统人工模式；</p><p>•  AI 落地难——45% 的企业 AI 应用停留在 “生成报表” 阶段，无法真正实现 “数据变知识、知识促决策”。</p><p>本次测评聚焦 BI 工具的 AI 核心能力，从理解精度、分析可信度、智能洞察等维度，盘点 2026 年国内 10 款 AI 能力领先的 BI 工具，为企业选型提供权威参考。</p><p>二、测评体系说明</p><p>本次测评围绕 AI 驱动 BI 的核心价值构建五大维度评估体系：</p><p>1、自然语言理解能力：意图识别准确率、模糊问题处理、专业术语适配度</p><p>2、AI 分析可信度：过程透明度、结果可干预性、数据溯源能力</p><p>3、智能洞察能力：异常检测、归因分析、预测性分析深度</p><p>4、AI 协作效率：多轮上下文对话、团队共享编辑、智能建议推送</p><p>5、企业级 AI 适配性：数据安全合规、现有系统集成、国产化支持</p><p>三、2026 年国内 AI 能力最强的 BI 工具 TOP10</p><ol><li>FineBI（帆软）（综合评分：4.8/5.0）</li></ol><p>产品定位：帆软旗下一站式数据分析平台，依托 20 年 BI 技术积累，打造 “可信、高效、全栈” 的 AI 驱动 BI 解决方案。帆软是 Gartner 全球 ABI 魔力象限唯一入选中国独立 BI 厂商，IDC 报告显示，帆软已连续八年（2017–2024）蝉联中国 BI 市场占有率第一。</p><p>核心优势：</p><p>•  可信 AI 分析：采用 Text2DSL 技术，将自然语言提问转化为可理解、可干预的结构化指令，彻底消除 AI “黑盒子” 顾虑，过程可控、结果可信</p><p>•  全链路 AI 闭环：覆盖输入联想与意图解析、多轮上下文对话、异常检测与归因分析、一键生成仪表盘、智能预测、大模型生成报告全流程，实现从提问到决策落地的完整闭环</p><p>•  企业级 AI 底座：兼容全行业复杂业务场景，支持 100 + 数据源接入，无缝对接企业现有数据系统，确保数据安全合规</p><p>•  全民 AI 分析：大幅降低数据分析门槛，业务人员无需技术背景，通过自然语言交互即可获取深度洞察，将数据获取效率提升 90% 以上</p><p>适用场景：</p><p>•  高管决策：实时获取核心指标汇总与异常归因，快速响应市场变化</p><p>•  业务分析：自助完成多维度数据探索，精准定位业务增长点</p><p>•  运营监控：实时监控业务数据，智能预警异常波动</p><p>•  数据团队：高效构建企业级分析模型，提升数据资产复用率</p><p>真实案例：</p><p>“交个朋友” 是多账号矩阵、多角色协作的直播电商企业，面临业务系统化流程化线上化难、绩效管理需牵引合力、直播不确定性大（如冷品风险）、实时数据反馈慢影响决策等问题。</p><p>•  解决方案：与帆软合作，利用 FineBI 工具打造直播数字化全流程管理体系，实现二十个关键环节全流程线上化及多平台多场景管理；构建以直播场次为单元的绩效管理体系，结合财务数据实时反馈激励；通过内外部数据整合、商品机制对比、冷品数据模型辅助选品决策；利用实时数据大屏、15 分钟数据播报、复播指数看板等实现直播过程数据快反与优化。</p><p>•  成效：形成覆盖抖音淘宝多平台的直播业务全流程线上化管理；主播和运营可实时查看业绩完成等数据并调整策略，主播等级月度迭代更新；通过冷品模型降低直播冷品概率；实现直播数据实时反馈，如早于抖音后台开发商品每十秒销售及增速数据功能，助力打造 “爆款” 直播间，提升直播转化。</p><ol start="2"><li>观远数据（综合评分：4.6/5.0）</li></ol><p>产品定位：AI 增强型云原生 BI 平台，专注为企业提供从数据接入到智能决策的全链路 AI 驱动数据分析解决方案，助力企业实现数据智能落地。</p><p>核心优势：云原生架构支持弹性扩展，AI 智能洞察引擎自动识别业务异常并完成根因分析，支持自然语言生成多维度分析报告，具备数据安全合规与国产化适配能力，适配零售、制造等行业复杂场景。</p><p>适用场景：零售智能供应链分析、制造生产质量监控、企业经营指标实时预警。</p><ol start="3"><li>奥威 BI（Power-BI）（综合评分：4.5/5.0）</li></ol><p>产品定位：专注财务与供应链场景的 AI BI 工具，为企业提供财务智能分析与供应链数字化决策支持。</p><p>核心优势：AI 生成复杂财务报表，供应链智能预测与异常预警，支持多维度数据溯源与结果干预，适配财务、制造等行业特殊需求。</p><p>适用场景：财务报表自动化、供应链库存优化、零售业绩分析。</p><ol start="4"><li>数林 BI（Shulin BI）（综合评分：4.4/5.0）</li></ol><p>产品定位：专注集团财务数据分析的 AI BI 工具，通过自然语言交互实现集团财务智能合并与多维度分析。</p><p>核心优势：集团财务智能合并报表，多维度财务 AI 洞察，异常指标自动预警与归因，支持跨子公司数据统一分析。</p><p>适用场景：集团企业财务分析、连锁企业业绩监控、多公司数据合并。</p><ol start="5"><li>网易有数 ChatBI（综合评分：4.3/5.0）</li></ol><p>产品定位：敏捷型 AI BI 工具，面向互联网、零售行业提供轻量级自然语言分析服务。</p><p>核心优势：中文理解精度高，实时数据 AI 处理，轻量化部署，支持可视化快速生成。</p><p>适用场景：零售实时销售分析、互联网运营监控、敏捷业务数据探索。</p><ol start="6"><li>海致 BDP 智能问答（综合评分：4.2/5.0）</li></ol><p>产品定位：云端自助分析平台的 AI 对话功能，为中小企业提供轻量化协作式 AI 分析服务。</p><p>核心优势：云端一站式 AI 分析，多源数据整合，团队协作共享，AI 智能洞察推送。</p><p>适用场景：中小企业销售分析、团队数据协作、轻量化业务监控。</p><ol start="7"><li>明略科技认知智能 BI（综合评分：4.1/5.0）</li></ol><p>产品定位：知识图谱 + NLP 驱动的 AI BI 工具，专注复杂关系数据智能分析。</p><p>核心优势：知识图谱 AI 构建，复杂语义理解，行业预训练模型，数据安全合规管控。</p><p>适用场景：金融客户关联风险分析、公安情报挖掘、供应链关系洞察。</p><ol start="8"><li>百分点科技对话式 BI（综合评分：4.0/5.0）</li></ol><p>产品定位：智能决策 AI BI 工具，依托大数据与 AI 技术提供端到端分析支持。</p><p>核心优势：AI 决策闭环，多源数据整合，行业解决方案内置，实时业务预警。</p><p>适用场景：零售销量预测、金融风险预警、制造生产优化。</p><ol start="9"><li>星环科技 Sophon Chat（综合评分：3.9/5.0）</li></ol><p>产品定位：大数据平台的 AI 对话分析模块，主打海量复杂数据自然语言交互。</p><p>核心优势：PB 级数据 AI 秒查，多模态分析，云原生架构，数据安全加密。</p><p>适用场景：制造海量设备数据分析、金融大规模交易监控、政务大数据处理。</p><ol start="10"><li>国双科技对话式 BI（综合评分：3.8/5.0）</li></ol><p>产品定位：营销与运营场景 AI BI 工具，主打营销数据自然语言分析。</p><p>核心优势：营销场景 AI 适配，多渠道数据整合，ROI 智能分析，归因效果评估。</p><p>适用场景：营销活动效果分析、广告投放优化、用户转化路径洞察。</p><p>四、综合对比表格<br/>产品名称    平台定位    核心技术优势    国产化适配    适用人群    协作效率    性价比<br/>FineBI（帆软）    全栈企业级 AI BI 平台    Text2DSL 可信 AI、全链路 AI 闭环、全民分析    ⭐⭐⭐⭐⭐    全规模企业、全行业    ⭐⭐⭐⭐⭐    ⭐⭐⭐⭐⭐<br/>观远数据    AI 增强型云原生 BI 平台    云原生弹性扩展、AI 根因分析、自然语言报告    ⭐⭐⭐⭐⭐    零售、制造、互联网企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>奥威 BI    财务供应链 AI BI 工具    财务报表 AI 生成、供应链智能预测    ⭐⭐⭐⭐⭐    财务、制造、零售企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>数林 BI    集团财务 AI BI 工具    集团财务智能合并、财务 AI 预警    ⭐⭐⭐⭐⭐    集团企业、连锁企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>网易有数    敏捷型 AI BI 工具    中文理解精度高、实时数据处理    ⭐⭐⭐⭐⭐    互联网、零售中小企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>海致 BDP    云端协作 AI BI 平台    云端一站式服务、团队共享    ⭐⭐⭐⭐    中小企业、创业公司    ⭐⭐⭐⭐⭐    ⭐⭐⭐⭐⭐<br/>明略科技    认知智能 AI BI 工具    知识图谱构建、复杂语义理解    ⭐⭐⭐⭐⭐    金融、公安、制造企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>百分点科技    智能决策 AI BI 工具    AI 决策闭环、行业解决方案内置    ⭐⭐⭐⭐⭐    零售、金融、制造企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>星环科技    海量数据 AI BI 平台    PB 级数据秒查、多模态分析    ⭐⭐⭐⭐⭐    大型制造、金融企业    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>国双科技    营销场景 AI BI 工具    营销 AI 适配、ROI 智能分析    ⭐⭐⭐⭐    零售、互联网营销部门    ⭐⭐⭐⭐    ⭐⭐⭐⭐<br/>五、选型指南</p><p>五步选型法<br/>1、明确 AI 核心需求：根据企业规模与业务场景，区分日常快速查询、复杂分析、预测决策等不同 AI 需求优先级</p><p>2、验证 AI 分析可信度：重点考察工具是否支持 AI 分析过程可视化、结果可干预，避免 “黑盒子” 式 AI 带来的决策风险</p><p>3、评估系统适配能力：验证工具与现有 BI 系统、数据仓库的集成度，确保数据资产复用与安全合规</p><p>4、测试全民分析能力：评估工具对非技术人员的友好度，确保业务人员能自主通过 AI 获取数据洞察</p><p>5、考察 AI 服务体系：选择具备成熟 AI 实施培训、售后响应能力的厂商，保障 AI 工具快速落地与持续优化</p><p>首推方案：FineBI（帆软）</p><p>FineBI 凭借 “可信 AI 分析 + 全链路 AI 闭环 + 企业级适配 + 全民分析” 的核心优势，成为不同规模企业的首选。无论是中小企业快速降低数据分析门槛，还是大型企业实现复杂业务场景的智能决策，FineBI 都能提供端到端的 AI 驱动 BI 解决方案，覆盖全行业、全场景需求。</p><p>六、本文相关 FAQs</p><p>问题 1：对话式 BI 的 AI 可信度如何保障？</p><p>对话式 BI 的 AI 可信度主要通过三个层面保障：首先是技术层面，采用可解释 AI 技术，将自然语言提问转化为结构化指令，让用户清晰看到 AI 分析的逻辑与过程；其次是数据层面，建立严格的数据溯源机制，确保分析结果可追溯至原始数据，避免数据失真；最后是合规层面，通过权限管控、数据脱敏等手段，确保 AI 分析符合企业数据规范与行业监管要求。</p><p>企业在选型时，应优先选择支持 AI 过程可视化、结果可干预的工具，同时要求厂商提供明确的安全合规方案，避免 “黑盒子” AI 带来的决策风险。此外，可通过试点测试，验证 AI 分析结果与人工分析的一致性，进一步提升可信度。</p><p>问题 2：企业如何快速落地 AI 驱动的数据分析？</p><p>企业快速落地 AI 驱动数据分析需要三步：第一步是数据基础建设，统一数据标准与指标体系，消除数据孤岛，确保 AI 分析有高质量的数据支撑；第二步是工具选型与试点，选择适配企业场景的 AI BI 工具，从核心业务场景入手进行试点，让业务人员快速体验 AI 带来的效率提升；第三步是组织能力建设，通过培训与案例分享，提升业务人员 AI 分析能力，建立数据驱动的企业文化。</p><p>此外，企业应避免盲目追求 “高大上” 的 AI 功能，而是聚焦业务痛点，先解决 “数据获取效率低”“报表制作耗时” 等基础问题，逐步深化 AI 应用，实现从 “用数据” 到 “用活数据” 的转变。</p><p>问题 3：AI BI 工具如何适配复杂业务场景？</p><p>AI BI 工具适配复杂业务场景需要具备三个核心能力：一是强大的自然语言理解能力，能准确识别专业术语与模糊问题，适配行业特殊需求；二是灵活的 AI 分析配置能力，支持用户自定义分析逻辑与指标，满足个性化业务需求；三是深度的行业场景沉淀，内置行业分析模型与最佳实践，降低场景化分析门槛。</p><p>企业在选型时，应优先选择具备行业解决方案积累的厂商，同时验证工具对复杂业务逻辑的支持能力，比如多表关联、复杂指标计算、异常归因分析等。此外，可通过定制开发与二次扩展，让 AI BI 工具更好地适配企业独特的业务场景。</p><p>七、总结</p><p>AI 正成为 BI 工具的核心竞争力，可信、高效、全栈的 AI 驱动 BI 解决方案，是企业释放数据价值、提升决策效率的关键。2026 年，国内 BI 市场将继续保持高速增长，AI 技术的深度融合将推动 BI 从 “工具” 向 “智能平台” 转变。企业在选型时，应结合自身业务需求，重点考察 AI 分析的可信度与适配性，选择真正能为业务创造价值的 AI BI 解决方案，实现数据驱动的数字化转型。</p>]]></description></item><item>    <title><![CDATA[深度解析：如何通过颗粒化职责切分工具实现精准定岗、定责与优先级排布？ NAVI_s1mple ]]></title>    <link>https://segmentfault.com/a/1190000047577888</link>    <guid>https://segmentfault.com/a/1190000047577888</guid>    <pubDate>2026-01-28 15:07:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在分布式协作与高并发业务的数字化浪潮中，企业面临的核心挑战已不再是“人力的堆砌”，而是“责任的模糊”。颗粒化职责切分工具不仅是权限的分配媒介，更是通过原子级的职责解构模型，将庞杂的业务流程转化为可观测、可追踪、可即时响应的组织级执行引擎。</p><h3><strong>一、 为什么现代组织必须重视“颗粒化”职责切分？</strong></h3><p>传统的粗放型职能模式往往导致“责任空档”：宽泛的角色定义与重叠的职能边界使关键任务在执行终端发生推诿或遗漏。颗粒化职责切分工具的核心价值在于：</p><ul><li><strong>打破责任衰减</strong>：通过颗粒化的职责清单，确保每一个执行点都能精准触达特定责任人，消除多头管理导致的信息失真。</li><li><strong>支撑深度权责穿透</strong>：支持在复杂的业务结构中横向拉通协作链条，纵向穿透职责深度，实现权责边界的全局统一。</li><li><strong>实现动态执行校准</strong>：通过各职责单元间的实时状态与交付反馈，自动捕捉职责错配风险，确保团队在快速迭代中保持高效。</li><li><strong>管理标准资产化</strong>：将验证有效的颗粒化职责模板沉淀，实现跨项目、跨团队的成熟管理模式迁移与复用。</li></ul><hr/><p><strong>二、 颗粒化职责切分的技术路径：三维解构架构</strong></p><p>构建颗粒化职责切分体系需要遵循“单元定义”与“权责绑定”的逻辑：</p><ol><li><strong>原子单元层（Atomic Unit Layer）</strong>：定义职责切分的最小原子单位，包含具体动作描述、交付标准及核心考核维度。</li><li><strong>权责映射层（Authority Mapping）</strong>：将分散的职责单元通过逻辑链路（如前置、决策、审核）连接，记录责任形成的闭环路径。</li><li><strong>效能预警层（Performance Warning）</strong>：位于架构顶端，通过状态标记、响应时效展示职责单元的饱和度与执行健康度，实现风险的主动预警。</li></ol><hr/><p><strong>三、 核心技术实现与算法示例</strong></p><p>颗粒化职责切分工具的底层逻辑涉及权责图谱、偏离度检测及协作效率模型。</p><h4><strong>1. 基于图论的职责影响力与负荷权重评估</strong></h4><p>在网状协作中，关键职责单元的承载质量决定了项目的一致性。以下为 JavaScript 实现的职责权重计算逻辑：</p><p>JavaScript</p><p>/**  <br/> * 递归计算职责单元的影响力权重及其执行压力  <br/> * @param {Object} unit 职责单元（包含关联下游职责数组）  <br/> * @returns {number} 该单元的综合压力得分  <br/> */  <br/>function calculateUnitResponsibility(unit) {</p><pre><code>// 基准情况：如果是末端执行单元，返回其基础复杂度评分  
if (\!unit.dependents || unit.dependents.length \=== 0) {  
    return unit.baseComplexity || 0;  
}

// 汇总下游关联职责的加权压力  
const totalPressure \= unit.dependents.reduce((acc, target) \=\&gt; {  
    // 根据权责连接的紧密程度进行计算  
    const dependencyStrength \= target.linkWeight || (1 / unit.dependents.length);  
    return acc \+ (calculateUnitResponsibility(target) \* dependencyStrength);  
}, 0);

// 更新该职责核心单元的全局压力评分  
unit.globalPressure \= Math.round(totalPressure);  
return unit.globalPressure;  </code></pre><p>}</p><h4><strong>2. Python：职责偏离度的动态熵减审计引擎</strong></h4><p>利用颗粒化模型，自动检测各成员“实际产出”与“标准职责路径”的熵增差异，识别执行脱节风险：</p><p>Python</p><p>class ResponsibilityAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设职责基准：岗位类型 \-\&gt; 职责切分粒度与偏差阈值  
    self.benchmarks \= {  
        "Product\_RD": {  
            "Spec": {"granularity": 0.9, "threshold": 95},  
            "Code": {"granularity": 0.8, "threshold": 90},  
            "Test": {"granularity": 0.85, "threshold": 92}  
        }  
    }

def verify\_granularity\_alignment(self, current\_assignment, job\_type):  
    """对比实际职责切分与标准基准，识别管理薄弱点"""  
    base\_std \= self.benchmarks.get(job\_type)  
    if not base\_std:  
        return "缺失匹配的职责切分标准"

    for unit\_type, data in current\_assignment.items():  
        std \= base\_std.get(unit\_type)  
        if std:  
            gap \= (data\['clarity\_rate'\] \- std\['threshold'\]) / std\['threshold'\]  
            if gap \&lt; \-0.10:  
                print(f"\[Responsibility Alert\] '{unit\_type}' 单元职责模糊，存在推诿风险")  
                \# 触发职责再切分引导机制  
                self.\_trigger\_repartition(unit\_type)
</code></pre><hr/><p><strong>四、 工具分类与选型思路</strong></p><p>实施颗粒化职责切分时，工具的选择应基于对“颗粒度控制能力”的需求：</p><ul><li><strong>结构化看板类（如板栗看板）</strong>：核心优势在于<strong>任务单元的深度切分与责任人明确绑定</strong>，支持将职责细节与执行卡片深度关联，适合需要“颗粒化分工”的研发与运营团队。</li><li><strong>多维管理类（如 ClickUp）</strong>：通过自定义字段与多层子任务结构，适合大规模复杂项目的职责层层穿透与拆解。</li><li><strong>职责文档类（如 Notion）</strong>：利用数据库模板定义标准职责单元，适合流程驱动型组织进行职责边界的文字化定义与索引。</li></ul><hr/><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“颗粒度过细导致的协作摩擦”</strong>：应在工具中通过合理的层级视图，确保成员在关注细节时仍能理解全局目标，避免陷入过度微观管理的陷阱。</li><li><strong>激活职责的动态反馈</strong>：职责切分不是静态的说明书，应根据执行结果动态修正切分粒度，实现“定义-执行-优化”的闭环。</li><li><strong>定期进行管理“减负”</strong>：随着流程成熟，应精简冗余的审批环节与过度切分的职责节点，保持组织的高敏捷执行力。</li></ul><hr/><p><strong>六、 结语</strong></p><p><strong>颗粒化切分是构建确定性组织的底层逻辑。</strong> 颗粒化职责切分工具不仅解决了“谁负责”的问题，更通过严密的原子级架构，将企业的每一次分工转化为可视化、可度量、可复用的管理资产。当组织的职责能以颗粒化形式精准对齐时，团队才能在复杂多变的环境中实现“个体精准触发”与“集体敏捷协同”的完美统一。</p>]]></description></item><item>    <title><![CDATA[算力的去中心化重构：简析Codigger分布式计算生态 codigger ]]></title>    <link>https://segmentfault.com/a/1190000047577891</link>    <guid>https://segmentfault.com/a/1190000047577891</guid>    <pubDate>2026-01-28 15:07:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>今天我们一起来聊聊Codigger分布式计算生态——它正在悄悄推动传统操作系统，完成一次向全球分布式节点网格的跨越。这绝不是一套简单的系统架构，本质上，它是一套全新的算力互联网运行体系。核心逻辑很简单：让计算资源打破物理设备的限制，就像我们日常使用的电力一样，在网络中自由流转、按需调用。<br/><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnNk5" alt="image.png" title="image.png"/><br/>这套生态最核心的突破，是实现了从单机持有算力到网格接入算力的根本性转变，最终目标是让计算资源成为像水电一样的公用事业。依托分布式编译和按需计算两大核心模块，Codigger打破了单机硬件的束缚，让我们不再受限于本地终端的性能瓶颈，轻松接入一个庞大的共享算力池。在这里，我们既能灵活调用高性能节点，加速重型计算任务——比如编译效率能提升300%，这就是实实在在的效能提升；同时，也能把自己设备的闲置算力利用起来，通过变现转化为实际价值。这种双向流动的模式，真正盘活了存量算力，让每一份计算资源都能发挥最大效用。<br/><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnNlp" alt="image.png" title="image.png" loading="lazy"/><br/>而支撑这一切的安全基石，在于物理隔离的数据主权理念，说到底就是让数据真正归用户自主掌控。和传统云服务集中托管数据的模式不同，Codigger采用加密分片技术，把数据分散存储在私有节点中。这种设计相当于在逻辑层面，搭建了一道去中心化的安全屏障，从根源上保障数据主权完全属于用户。与此同时，借助数据市场模块，我们还能在确保安全的前提下，释放数据的价值，比如为AI训练提供数据支持，实现价值变现。</p><p>说到这里，就不得不提旗舰应用SIDE——它是整个分布式网格的交互枢纽，更是生态统合的核心。大家可别把它只当成一款普通的代码编辑器，它真正的价值在于具备强大的计算编排能力，能无缝集成分布式任务调度和多端实时编辑功能。这种设计最巧妙的地方，就是实现了操作体验和底层能力的解耦：开发者明明是在本地操作，背后调用的却是全球的分布式算力资源，大大降低了分布式计算的使用门槛。</p><p>总的来说，Codigger正以分布式架构为核心，重新定义传统操作系统的边界。它致力于打造高弹性、高安全、高效率的业务连续性体系，为未来的去中心化协作场景，筑牢基础设施的根基。这不仅是一次技术革新，更在为我们构建一种全新的算力使用方式。</p>]]></description></item><item>    <title><![CDATA[Linux目录结构有哪些？每个目录的作用是什么？ 码云笔记 ]]></title>    <link>https://segmentfault.com/a/1190000047577895</link>    <guid>https://segmentfault.com/a/1190000047577895</guid>    <pubDate>2026-01-28 15:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文介绍下 Linux 系统中各个目录都起到一个什么样的作用。对于初次接触 Linux 系统的时候，打开终端输入 ls /，面对满屏的目录名一脸茫然：/bin、/boot、/etc……这些名字像密码一样，让人摸不着头脑。</p><p>其实 Linux 的目录结构就像一棵倒挂的大树，根目录/是树干，其他目录是树枝和树叶。每个用户的家目录（比如/home/你的用户名）则是树上的一个‘鸟巢’，你的私人文件、照片、代码都在这里安家。而系统文件则像树的‘根系’，藏在/usr、/bin 等目录中，默默支撑着整个系统的运行。</p><p>Linux 文件系统采用层次化的结构来组织文件和目录，其中每个目录都有特定的用途。下面是由<a href="https://link.segmentfault.com/?enc=D1HUSo56VJwytwmzaEplbA%3D%3D.PCZh3kixN4Ngop1lTcDYj0QM0ig3UEd%2BWpDQl0vJgOc%3D" rel="nofollow" title="码云笔记" target="_blank">码云笔记</a> Linux 文件系统中各个主要目录及其详细用途的讲解：</p><h2>根目录 /</h2><ul><li>描述：根目录是整个文件系统的起始点，所有其他文件和目录都是从这个目录派生出来的。</li><li><p>用途：作为系统的基础，所有文件和目录都在此目录下形成树状结构。</p><h2>/bin</h2></li><li>描述：这个目录包含用户在系统启动和运行过程中需要的基本命令的可执行文件。</li><li><p>用途：存放常用的用户命令，例如：</p><pre><code>ls：列出目录内容。
cp：复制文件。
mv：移动或重命名文件。
rm：删除文件。</code></pre><h2>/sbin</h2></li><li>描述：与 /bin 类似，但包含系统管理命令，通常只有超级用户（root）可以使用。</li><li><p>用途：存放用于系统管理的命令，例如：</p><pre><code>shutdown：关机命令。
reboot：重启命令。
ifconfig：网络接口配置命令。</code></pre><h2>/etc</h2></li><li>描述：这个目录包含系统的全局配置文件。</li><li><p>用途：存放各种程序和服务的配置文件，例如：</p><pre><code>/etc/passwd：存储用户账户信息。
/etc/fstab：定义文件系统的挂载点。
/etc/hosts：本地主机名解析配置。
/etc/network/interfaces：网络接口配置。</code></pre><h2>/dev</h2></li><li>描述：设备文件目录，包含对系统中硬件设备的访问接口。</li><li><p>用途：存放设备文件，这些文件表示内存、硬盘、USB 设备等。例如：</p><pre><code>/dev/sda：第一个 SATA 硬盘。
/dev/null：空设备，任何写入其中的数据都会被丢弃。</code></pre><h2>/proc</h2></li><li>描述：一个虚拟文件系统，它提供了关于系统和内核运行时状态的信息。</li><li><p>用途：存放进程和系统信息的接口，包括：</p><pre><code>/proc/cpuinfo：CPU 信息。
/proc/meminfo：内存使用情况。
/proc/[pid]：特定进程的相关信息，其中[pid]是进程 ID。</code></pre><h2>/sys</h2></li><li>描述：另一个虚拟文件系统，提供内核及其设备的详细信息和管理接口。</li><li><p>用途：主要用于内核空间和用户空间之间的交互，提供有关设备驱动和硬件信息。例如：</p><pre><code>/sys/class：设备类别。
/sys/block：块设备信息。</code></pre><h2>/usr</h2></li><li>描述：包含用户程序和只读数据，是系统中大多数用户应用和工具的存放位置。</li><li><p>用途：存放更高级别的用户命令和库，包含多个子目录：</p><pre><code>/usr/bin：大多数用户命令的可执行文件。
/usr/sbin：系统管理员命令，不同于/sbin，该目录中的命令通常不用于正常操作。
/usr/lib：用户程序的共享库。
/usr/share：共享数据和文档，如帮助文件和图标。</code></pre><h2>/var</h2></li><li>描述：可变数据文件目录，包含不断变化的数据。</li><li><p>用途：存放日志文件、邮件队列、缓存等，例如：</p><pre><code>/var/log：系统和服务的日志文件。
/var/tmp：临时文件，可以跨重启保存。
/var/spool：邮件和打印任务的存储位置。</code></pre><h2>/tmp</h2></li><li>描述：临时文件存放目录，通常系统重启后会清空。</li><li><p>用途：用于存放短期使用的临时文件，所有用户都可以访问。</p><h2>/home</h2></li><li>描述：普通用户的主目录，每个用户在此目录下有自己的子目录。</li><li><p>用途：存储用户的个人文件和设置，例如：</p><pre><code>/home/user1：用户 user1 的主目录。
用户的文档、下载、桌面等文件都存放在其主目录下。</code></pre><h2>/root</h2></li><li>描述：超级用户（root）的主目录。</li><li><p>用途：存放 root 用户的个人文件和配置，类似于普通用户的/home 目录。</p><h2>/media</h2></li><li>描述：临时挂载点，用于自动挂载可移动媒体，如 USB 闪存驱动器和 CD/DVD。</li><li><p>用途：当插入 USB 或光盘时，系统通常会在此目录下创建相应的子目录来访问这些媒体。</p><h2>/mnt</h2></li><li>描述：通常用于临时挂载文件系统的目录。</li><li><p>用途：系统管理员可以手动在该目录下挂载其他文件系统。</p><h2>/lib</h2></li><li>描述：/lib 目录包含系统运行所需的共享库文件和内核模块。</li><li>用途：</li><li>存放由 /bin 和 /sbin 中的可执行文件所依赖的共享库（例如 .so 文件）。</li><li>在 32 位系统中，通常会有一个子目录 /lib/i386 或 /lib/x86_64 用于存放特定架构的库文件。</li><li>动态链接库（如标准 C 库 libc.so）在这里提供给其他程序调用，确保程序可以正确运行。</li><li><p>除了共享库外，某些设备驱动模块也会存放在 /lib/modules 下。</p><h2>/boot</h2></li><li>描述：/boot 目录用于存放引导加载程序和内核文件。</li><li>用途：</li><li>包含用于启动操作系统的重要文件，如 Linux 内核 (vmlinuz) 和初始 RAM 磁盘镜像 (initrd 或 initramfs)，这些文件是系统启动时所需的。</li><li>引导加载器（如 GRUB）配置文件也存放在此目录下，通常为 grub/ 子目录。</li><li><p>config-*文件则保存了内核的配置信息，便于用户查看。</p><h2>/opt</h2></li><li>描述：/opt 目录用于安装附加的第三方应用程序。</li><li>用途：</li><li>适用于那些不属于系统标准软件包管理的巨型应用或商业软件。</li><li>每个应用程序通常会在此目录下有一个独立的子目录，例如/opt/mysql或/opt/google/chrome，以便于管理和维护。</li><li><p>这种结构使得不同软件之间的依赖关系更加清晰，并且方便卸载。</p><h2>/lost+found</h2></li><li>描述：/lost+found是用于存放丢失文件的特殊目录。</li><li>用途：</li><li>在文件系统检查（如运行 fsck 命令）时，如果发现一些文件系统的结构损坏或者文件丢失，系统会将这些文件恢复到 /lost+found 目录中。</li><li>丢失的文件会被重命名为数字（代表其 inode 号），用户可以根据需要尝试恢复这些文件。</li><li><p>这个目录通常是空的，但在文件系统遭遇问题时，对数据恢复具有重要意义。<br/>除了上述目录，还有一些其他常见的目录：</p><h2>/srv</h2></li><li>描述：该目录用于存放服务数据，特定于某个服务的数据。</li><li><p>用途：例如，Web 服务（如 Apache 或 Nginx）可能会在/srv/www下存放网站文件。FTP 服务可能在/srv/ftp下存放文件。</p><h2>/run</h2></li><li>描述：/run是一个临时文件系统，存放运行时数据。</li><li>用途：包含当前运行的服务和系统状态的信息，例如 PID 文件、锁文件等。</li><li><p>在系统启动时创建，系统关闭时会被清空。</p><h2>/snap</h2></li><li>描述：用于存放通过 Snaps 安装的应用程序。</li><li>用途：Snap 是一种软件包管理系统，允许用户从 Snap Store 下载和安装应用程序。每个 Snap 包会在此目录下有自己的子目录。</li></ul><p>在 Linux 的世界里，目录不仅是文件的容器，更是逻辑的起点。掌握它，你就握住了通往系统深处的钥匙。<a href="https://link.segmentfault.com/?enc=9HGoiHUS9t4OL7%2BmSmT%2B2g%3D%3D.V1Edjv3LtCEMRr9TL96EB27RmFoBKNJCZ28XtqRFs7E%3D" rel="nofollow" target="_blank">https://mybj123.com/28670.html</a></p>]]></description></item><item>    <title><![CDATA[团队协作聚焦指南：如何用颗粒化职责切分工具消除推诿、明确执行边界 Ord1naryLife ]]></title>    <link>https://segmentfault.com/a/1190000047577906</link>    <guid>https://segmentfault.com/a/1190000047577906</guid>    <pubDate>2026-01-28 15:05:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在分布式协作与高并发业务的数字化浪潮中，企业面临的核心挑战已不再是“人力的堆砌”，而是“责任的模糊”。颗粒化职责切分工具不仅是权限的分配媒介，更是通过原子级的职责解构模型，将庞杂的业务流程转化为可观测、可追踪、可即时响应的组织级执行引擎。</p><h3><strong>一、 为什么现代组织必须重视“颗粒化”职责切分？</strong></h3><p>传统的粗放型职能模式往往导致“责任空档”：宽泛的角色定义与重叠的职能边界使关键任务在执行终端发生推诿或遗漏。颗粒化职责切分工具的核心价值在于：</p><ul><li><strong>打破责任衰减</strong>：通过颗粒化的职责清单，确保每一个执行点都能精准触达特定责任人，消除多头管理导致的信息失真。</li><li><strong>支撑深度权责穿透</strong>：支持在复杂的业务结构中横向拉通协作链条，纵向穿透职责深度，实现权责边界的全局统一。</li><li><strong>实现动态执行校准</strong>：通过各职责单元间的实时状态与交付反馈，自动捕捉职责错配风险，确保团队在快速迭代中保持高效。</li><li><strong>管理标准资产化</strong>：将验证有效的颗粒化职责模板沉淀，实现跨项目、跨团队的成熟管理模式迁移与复用。</li></ul><hr/><p><strong>二、 颗粒化职责切分的技术路径：三维解构架构</strong></p><p>构建颗粒化职责切分体系需要遵循“单元定义”与“权责绑定”的逻辑：</p><ol><li><strong>原子单元层（Atomic Unit Layer）</strong>：定义职责切分的最小原子单位，包含具体动作描述、交付标准及核心考核维度。</li><li><strong>权责映射层（Authority Mapping）</strong>：将分散的职责单元通过逻辑链路（如前置、决策、审核）连接，记录责任形成的闭环路径。</li><li><strong>效能预警层（Performance Warning）</strong>：位于架构顶端，通过状态标记、响应时效展示职责单元的饱和度与执行健康度，实现风险的主动预警。</li></ol><hr/><p><strong>三、 核心技术实现与算法示例</strong></p><p>颗粒化职责切分工具的底层逻辑涉及权责图谱、偏离度检测及协作效率模型。</p><h4><strong>1. 基于图论的职责影响力与负荷权重评估</strong></h4><p>在网状协作中，关键职责单元的承载质量决定了项目的一致性。以下为 JavaScript 实现的职责权重计算逻辑：</p><p>JavaScript</p><p>/**  <br/> * 递归计算职责单元的影响力权重及其执行压力  <br/> * @param {Object} unit 职责单元（包含关联下游职责数组）  <br/> * @returns {number} 该单元的综合压力得分  <br/> */  <br/>function calculateUnitResponsibility(unit) {</p><pre><code>// 基准情况：如果是末端执行单元，返回其基础复杂度评分  
if (\!unit.dependents || unit.dependents.length \=== 0) {  
    return unit.baseComplexity || 0;  
}

// 汇总下游关联职责的加权压力  
const totalPressure \= unit.dependents.reduce((acc, target) \=\&gt; {  
    // 根据权责连接的紧密程度进行计算  
    const dependencyStrength \= target.linkWeight || (1 / unit.dependents.length);  
    return acc \+ (calculateUnitResponsibility(target) \* dependencyStrength);  
}, 0);

// 更新该职责核心单元的全局压力评分  
unit.globalPressure \= Math.round(totalPressure);  
return unit.globalPressure;  </code></pre><p>}</p><h4><strong>2. Python：职责偏离度的动态熵减审计引擎</strong></h4><p>利用颗粒化模型，自动检测各成员“实际产出”与“标准职责路径”的熵增差异，识别执行脱节风险：</p><p>Python</p><p>class ResponsibilityAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设职责基准：岗位类型 \-\&gt; 职责切分粒度与偏差阈值  
    self.benchmarks \= {  
        "Product\_RD": {  
            "Spec": {"granularity": 0.9, "threshold": 95},  
            "Code": {"granularity": 0.8, "threshold": 90},  
            "Test": {"granularity": 0.85, "threshold": 92}  
        }  
    }

def verify\_granularity\_alignment(self, current\_assignment, job\_type):  
    """对比实际职责切分与标准基准，识别管理薄弱点"""  
    base\_std \= self.benchmarks.get(job\_type)  
    if not base\_std:  
        return "缺失匹配的职责切分标准"

    for unit\_type, data in current\_assignment.items():  
        std \= base\_std.get(unit\_type)  
        if std:  
            gap \= (data\['clarity\_rate'\] \- std\['threshold'\]) / std\['threshold'\]  
            if gap \&lt; \-0.10:  
                print(f"\[Responsibility Alert\] '{unit\_type}' 单元职责模糊，存在推诿风险")  
                \# 触发职责再切分引导机制  
                self.\_trigger\_repartition(unit\_type)
</code></pre><hr/><p><strong>四、 工具分类与选型思路</strong></p><p>实施颗粒化职责切分时，工具的选择应基于对“颗粒度控制能力”的需求：</p><ul><li><strong>结构化看板类（如板栗看板）</strong>：核心优势在于<strong>任务单元的深度切分与责任人明确绑定</strong>，支持将职责细节与执行卡片深度关联，适合需要“颗粒化分工”的研发与运营团队。</li><li><strong>多维管理类（如 ClickUp）</strong>：通过自定义字段与多层子任务结构，适合大规模复杂项目的职责层层穿透与拆解。</li><li><strong>职责文档类（如 Notion）</strong>：利用数据库模板定义标准职责单元，适合流程驱动型组织进行职责边界的文字化定义与索引。</li></ul><hr/><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“颗粒度过细导致的协作摩擦”</strong>：应在工具中通过合理的层级视图，确保成员在关注细节时仍能理解全局目标，避免陷入过度微观管理的陷阱。</li><li><strong>激活职责的动态反馈</strong>：职责切分不是静态的说明书，应根据执行结果动态修正切分粒度，实现“定义-执行-优化”的闭环。</li><li><strong>定期进行管理“减负”</strong>：随着流程成熟，应精简冗余的审批环节与过度切分的职责节点，保持组织的高敏捷执行力。</li></ul><hr/><p><strong>六、 结语</strong></p><p><strong>颗粒化切分是构建确定性组织的底层逻辑。</strong> 颗粒化职责切分工具不仅解决了“谁负责”的问题，更通过严密的原子级架构，将企业的每一次分工转化为可视化、可度量、可复用的管理资产。当组织的职责能以颗粒化形式精准对齐时，团队才能在复杂多变的环境中实现“个体精准触发”与“集体敏捷协同”的完美统一。</p>]]></description></item><item>    <title><![CDATA[分享两款完全本地的聊天记录查看、分析与导出工具 Jason ]]></title>    <link>https://segmentfault.com/a/1190000047577919</link>    <guid>https://segmentfault.com/a/1190000047577919</guid>    <pubDate>2026-01-28 15:04:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>仅作为技术交流分享，不可用于其它用途</strong></p><h2>一 WeFlow</h2><p>持续维护中。。。</p><p>测试wx版本：4.1.6.46</p><p>项目地址，备用下载：<a href="https://link.segmentfault.com/?enc=jKqrVmYYCvh2EQ%2Fq3gSgoA%3D%3D.SDkAVelxo3fdCk%2Fha8aBJXA3UT1hlxaj1JiNHdsL61%2BEsw0h%2FkU0TSI2YNodfyL3" rel="nofollow" target="_blank">https://wwbxo.lanzoue.com/iGAs03h6zsoj</a>（资源受限，看标题无需下载）</p><p>主要功能：</p><ul><li>本地实时查看聊天记录</li><li>统计分析与群聊画像</li><li>年度报告与可视化概览</li><li>导出聊天记录为 HTML 等格式</li><li>本地解密与数据库管理</li></ul><p>⚠️ 本工具仅适配微信 <strong>4.0 及以上</strong>版本，请确保你的微信版本符合要求<br/><img width="723" height="470" referrerpolicy="no-referrer" src="/img/bVdnNlT" alt="weflow1.png" title="weflow1.png"/><br/><img width="723" height="470" referrerpolicy="no-referrer" src="/img/bVdnNlU" alt="weflow2.png" title="weflow2.png" loading="lazy"/></p><h2>二 <strong>EchoTrace</strong></h2><p>已停止维护。</p><p>测试wx版本：4.1.6.46</p><p>项目地址，备用下载：<a href="https://link.segmentfault.com/?enc=1IIOum%2FjSOB3LNj7K0X42Q%3D%3D.ysbahDJ67RCHZpijvvPbYLDLvVwNVSakR%2B4EyCpSqkDnUFkN1XEVL%2FOkPonDze7h" rel="nofollow" target="_blank">https://wwbxo.lanzoue.com/iGAs03h6zsoj</a>（资源受限，看标题无需下载）</p><p>EchoTrace 是一个<strong>完全本地</strong>的微信聊天记录导出、分析与年度报告生成工具。它可以解密你的微信聊天记录并保存在本地离线查看，也可以将其导出为HTML等与朋友分享，还可以根据你的聊天记录为你生成独一无二的分析报告</p><p><img width="723" height="394" referrerpolicy="no-referrer" src="/img/bVdnNlV" alt="echotrace2.png" title="echotrace2.png" loading="lazy"/><br/><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdnNlW" alt="echotrace.png" title="echotrace.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Async Asset Upload下Time Slice和Buffer Size在移动端分别如何设]]></title>    <link>https://segmentfault.com/a/1190000047577929</link>    <guid>https://segmentfault.com/a/1190000047577929</guid>    <pubDate>2026-01-28 15:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>1）Async Asset Upload下Time Slice和Buffer Size在移动端分别如何设置<br/>2）UE LLM中观察到Physics/ChaosAcceleration内存占用代表什么</p><hr/><p>这是第462篇UWA技术知识分享的推送，精选了UWA社区的热门话题，涵盖了UWA问答、社区帖子等技术知识点，助力大家更全面地掌握和学习。</p><p>UWA社区主页：<a href="https://link.segmentfault.com/?enc=VJYbvdnprl%2FtHmTeJzjO4g%3D%3D.gUlqyXjyAJADua%2BX0BGiWvosCKlPs%2F45%2BXXy26LfZkk%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA QQ群：793972859</p><p><strong>From UWA AI 问答</strong></p><p><strong>Q：Async Asset Upload下Time Slice和Buffer Size设置有什么用，移动端一般怎么设置？</strong></p><blockquote><p>A：在Unity的异步资源上传管线（Async Upload Pipeline，AUP）中，Time Slice和Buffer Size是两个核心可控参数，直接影响纹理/网格上传的CPU占用、GPU上传吞吐、内存开销与帧稳定性，尤其在移动端资源加载密集场景下至关重要。&lt;br/&gt;<br/>一、核心作用解析</p><ul><li><strong>asyncUploadTimeSlice（时间片）</strong><br/>控制<strong>每帧在渲染线程上用于上传数据的最大毫秒数</strong>（范围 1～33ms）。值越小，单帧上传压力越低、帧率更稳定；值越大，上传更快、但可能造成偶发卡顿。本质是「CPU时间配额」，防止上传阻塞主线程或渲染线程。</li><li><strong>asyncUploadBufferSize（环形缓冲区大小）</strong><br/>设定用于暂存待上传数据的环形缓冲区内存上限（2～2047MB）。该缓冲区复用于解压、读取与GPU上传。设置过小会频繁重分配（触发额外GC/系统调用）；过大则浪费RAM，尤其在内存受限的Android/iOS设备上易引发OOM或系统杀进程。</li></ul><p>二、移动端推荐配置（基于实测与平台约束）</p><p><img referrerpolicy="no-referrer" src="http://uwa-ducument-img.oss-cn-beijing.aliyuncs.com/Blog/TechSharing_462/1.png" alt="" title=""/></p><p>三、协同优化要点</p><ul><li>必须<strong>配合LZ4压缩+Streaming Mipmaps启用</strong>，否则异步上传无法生效；</li><li>在Quality Settings中统一配置，<strong>避免运行时频繁修改</strong>（引发缓冲区重分配）。</li></ul><p>⚠️ 注意：Editor中行为与真机不同（全量加载），所有调优必须在真机Profile（如Android Systrace/iOS Metal GPU Capture）下验证。</p></blockquote><hr/><p><strong>From UWA AI 问答</strong></p><p><strong>Q：UE LLM中观察到Physics/ChaosAcceleration内存占用代表什么？</strong></p><blockquote><p>A：Physics/ChaosAcceleration内存占用（常见于Unreal Engine 5的Chaos物理系统性能报告中）<strong>代表Chaos物理求解器在运行时为加速度（Acceleration）数据所分配和维护的内存总量</strong>，具体包括：</p><ul><li><strong>刚体/软体粒子加速度向量缓存</strong>：每个动态物理对象（如TRigidParticles、TDynamicParticles、TSoftsSolverParticlesRange）均需存储当前帧的加速度（FSolverVec3或TVector&lt;float,3&gt;类型），用于积分、约束求解与碰撞响应；</li><li><strong>并行计算结构开销</strong>：Chaos使用多线程+SIMD（如ISPC）加速，加速度数据常以AoS或SoA方式组织，并伴随对齐填充、任务队列缓冲区、临时中间数组等内存开销；</li><li><strong>加速结构关联元数据</strong>：若启用AccelerationStructureUseDynamicTree或AccelerationStructureCacheOverlappingLeaves，加速度数据可能与空间索引结构（BVH、Grid）耦合，产生额外引用或缓存内存。</li></ul><p>⚠️ 注意：该指标<strong>不等于“每帧实时计算的加速度值大小”</strong>（单个加速度仅12字节），而是整个生命周期内被活跃管理的加速度数据集合 —— 尤其在高密度物理对象（如布料、碎裂、大量RigidBody）、未及时休眠或未启用ParticlePool优化时，其内存会显著增长。</p><p>✅ 优化建议：</p><ul><li>启用p.Chaos.Solver.UseParticlePool true，复用已休眠粒子内存；</li><li>调整休眠阈值（p.Chaos.Solver.Sleep.*Threshold）加速非活跃对象进入休眠态，减少持续更新的加速度数据量；</li><li>检查是否误启用了p.Chaos.Solver.ShrinkArrays false（默认） —— 若设为true可在空闲时收缩数组，但需权衡频繁重分配开销；</li><li>对非关键物理对象（如远景碎石）启用p.Chaos.Solver.OneWayPairCollisionMode 0或降低p.Chaos.Solver.Joint.TransferCollisionsStiffnessClamp，减少约束激活导致的加速度更新频次。</li></ul><p>该内存项通常与Active Constraints、Physics.ProcessReports CPU耗时协同分析 —— 若二者同步激增，大概率指向物理对象过载或约束配置不当。</p></blockquote><p><strong>分享链接：</strong><br/><a href="https://link.segmentfault.com/?enc=GAkbDx5RkCCt0Z2UExjjNA%3D%3D.J1wF%2FKW5loeDWlt%2BqVnAHX9RPJKBlb9Xu4Kt%2FFR9RQl%2BCPHMxfPM1IWEYvhSmaqlQcJGTUpnyKmf6n7zjeCSEYidu1FJIvZkDCtT5m7Wq5lQ2zQWFOCgoZOm%2Fc0YFTal" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=hE3%2FyjAP9rbU2NAqwKWD2Q%3D%3D.igxrqb4W59vN57s12hshug8kA2sjPnBaRdWFJ%2BVDKYVyRnjkZwQBSMy8vDoyomc%2BNnm%2FuLZAo9qlrwT9VqoB5dqjn7RpCrzAtTumcCXxqJW5N%2B7lhtYFRVhOsbvsFT9g" rel="nofollow" target="_blank">https://www.uwa4d.com/main/uwa-shared.html?shareId=59dd2fd7-8...</a></p><p><strong>无论是社区里开发者们的互助讨论，还是AI基于知识沉淀的快速反馈，核心都是为了让每一个技术难题都有解、每一次踩坑都有回响。本期分享分别来自UWA AI问答和UWA问答社区，希望这些从真实开发场景中提炼的经验，能直接帮你解决当下的技术卡点，也让你在遇到同类问题时，能更高效地找到破局方向。</strong></p><p>封面图来源于网络</p><hr/><p>今天的分享就到这里。生有涯而知无涯，在漫漫的开发周期中，我们遇到的问题只是冰山一角，UWA社区愿伴你同行，一起探索分享。欢迎更多的开发者加入UWA社区。</p><p>UWA官网：<a href="https://link.segmentfault.com/?enc=2DX%2B8zP%2BpV5AvUEutG6cKQ%3D%3D.%2FL8EMpb4kuX6sRuSM6yw7uWgqqbTczFeELqx9miy2fE%3D" rel="nofollow" target="_blank">www.uwa4d.com</a><br/>UWA社区：<a href="https://link.segmentfault.com/?enc=Js26af%2BIlwdBBhHkyTZsQQ%3D%3D.uJYrgugtU3GgOqFUQQ7kVbw1ZPzD1ey%2FsKutHQ1m0ZQ%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA学堂：<a href="https://link.segmentfault.com/?enc=OG%2FNZ7sNn2g%2BzGpheqh1cA%3D%3D.we1qV3XHGiKbuYgSNbY7SH0DvBD5oFRsuo%2FNOVgH8S4%3D" rel="nofollow" target="_blank">edu.uwa4d.com</a><br/>官方技术QQ群：793972859</p>]]></description></item><item>    <title><![CDATA[使用 DockerSlim 优化/专业 Docker 容器镜像 landonVM ]]></title>    <link>https://segmentfault.com/a/1190000047577938</link>    <guid>https://segmentfault.com/a/1190000047577938</guid>    <pubDate>2026-01-28 15:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>前言</h3><p>在日常 DevOps 与容器运维中，镜像臃肿始终是一个令人头疼的顽疾。受限于网络环境，在国内或内网<a href="https://link.segmentfault.com/?enc=FL2kECVY4TCFvS%2FZf6s5ag%3D%3D.K%2BrRZR%2FW0zuhLsERRHKATnKmj%2Bh%2F1IBp4CiSv8ilx1PkCZjp%2FVhGU%2F%2B8nMwXasOy" rel="nofollow" target="_blank">服务器</a>上分发镜像往往异常痛苦：传统的“国外中转-手动导出-内网传输”流程不仅低效，且动辄数 GB 的镜像体积极大地拖慢了部署效率，也带来了不必要的安全隐患。<br/>针对这一痛点，开源利器 SlimToolkit/slim（原 DockerSlim） 脱颖而出。它通过自动化静态与动态分析，精准剔除镜像中的冗余内容。在保证应用功能完好无损的前提下，它能构建出体积更小、安全性更高、性能更优的精简镜像。官方数据显示，其压缩比最高可达 30 倍，让“巨型镜像”瞬间瘦身。</p><h3>一、SlimToolkit 的核心优势</h3><p>SlimToolkit（原 DockerSlim）不仅仅是一个压缩工具，它通过底层的动态分析与静态探测，实现了容器镜像的本质进化。</p><h4>1. 极致的瘦身效能 (Extreme Slimming)</h4><ul><li>压缩比率： 在不损失任何功能的前提下，可将镜像体积降低 10 至 30 倍。</li><li>性能提升： 显著减少镜像拉取（Pull）时间，加速 CI/CD 流水线启动，降低存储与带宽成本。</li></ul><h4>2. 零侵入式自动化 (Zero-Config Automation)</h4><ul><li>智能剖析： 自动分析镜像内容，无需手动修改 Dockerfile 或优化源码。</li><li>动态监控： 通过运行临时容器来观察应用的实际行为，精准识别并保留必要的系统调用与文件依赖。</li></ul><h4>3. 深度安全加固 (Hardened Security)</h4><ul><li>最小化攻击面： 自动剔除镜像中未使用的二进制文件、 shell 工具、库文件及敏感配置。</li><li>防御强化： 通过精简组件，天然地防御了基于预装工具的潜在漏洞利用（RCE 等）。</li></ul><h4>4. 全栈技术栈支持 (Polyglot Support)</h4><ul><li>广泛兼容： 深度适配 Python, Node.js, Go, Java, .NET 以及 PHP、Ruby 等主流语言环境。</li><li>环境一致性： 无论是微服务还是复杂的单体应用，瘦身后依然保持运行时的行为一致性。</li></ul><h3>二、安装DockerSlim</h3><p>DockerSlim 提供了预编译的二进制文件，也可以使用 Docker 运行。推荐方式如下：<br/>方式一：使用主机二进制（适用于Linux/macOS）</p><pre><code>curl -sL https://downloads.dockerslim.com/releases/1.40.0/dist_linux.tar.gz | tar -xz 
sudo mv dist_linux/docker-slim /usr/local/bin/
</code></pre><p>也可以使用官方一键脚本：</p><pre><code>curl -sL https://raw.githubusercontent.com/slimtoolkit/slim/master/scripts/install-slim.sh | sudo -E bash -</code></pre><p>方式二：通过 Docker 启动</p><pre><code>docker run -it --rm \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v $(pwd):/work \
  slimtoolkit/slim</code></pre><h3>三、压缩Python 应用镜像</h3><p>假设我们有一个基于 Flask 的 Python 应用，Dockerfile 如下：</p><pre><code>FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
</code></pre><p>构建镜像：</p><p><code>docker build -t flask-app .</code></p><p>现在我们用 DockerSlim 来“瘦身”：</p><p><code>docker-slim build flask-app</code></p><p>执行完后，你会看到输出中提示生成了一个名为 flask-app.slim 的镜像。我们来对比一下大小：</p><pre><code>docker images | grep flask-app
REPOSITORY        TAG       IMAGE ID       CREATED         SIZE
flask-app.slim    latest    4d2e0cc78      5 minutes ago   25MB
flask-app         latest    1c1f86e6a      10 minutes ago  300MB
</code></pre><p>同样的方式，我们可以对nginx镜像进行瘦身测试：</p><pre><code>docker pull nginx
docker-slim build nginx
docker images|grep nginx
nginx.slim                latest               0e2f8367fca4   17 seconds ago   13.4MB
nginx                     latest               1e5f3c5b981a   2 months ago     192M
</code></pre><h3>四、 核心原理与总结</h3><p>SlimToolkit 之所以能够实现极致压缩，核心在于其独特的“动态探测与自举”机制。</p><h4>1. 动态分析技术 (Dynamic Analysis)</h4><p>不同于传统的静态扫描，SlimToolkit 会在扫描阶段真正运行你的镜像。它通过实时收集应用在运行状态下调用的二进制指令、共享库文件、配置文件以及环境变量，精准勾勒出应用的“生命线”。</p><h4>2. 自动化“自举”探测 (Self-Bootstrapping)</h4><p>SlimToolkit 会模拟真实的生产环境，通过以下链路实现依赖集的识别：</p><ul><li>模拟启动： 自动启动容器并监控其初始化进程。</li><li>主动探测： 自动扫描暴露端口，执行 HTTP 健康检查，触发应用的业务逻辑。</li><li>精准裁剪： 在识别出最小依赖集后，自动剔除所有未被触达的冗余文件，实现“按需保留”。</li></ul><h4>3. 透明化与安全审查</h4><p>瘦身并不意味着进入“黑盒”。SlimToolkit 在构建过程中会同步生成：</p><ul><li>精细化瘦身报告： 详细记录了哪些文件被保留，哪些被剔除。</li><li>文件变更分析： 提供清晰的元数据对比，确保安全团队可以对精简后的镜像进行审计。</li><li>Seccomp/AppArmor 配置： 自动为镜像生成配套的安全配置文件，进一步收紧运行时的权限控制。</li></ul><p>官方项目地址：<a href="https://link.segmentfault.com/?enc=55LK39NxHyP98CD7KUji0Q%3D%3D.gNWcZ7W%2FSh7e0I3xQNNXdkkOfy0gNUhGmeyyk6WnsYz7sfCaxLMk1dedWAu0I2nQ" rel="nofollow" target="_blank">https://github.com/slimtoolkit/slim</a></p><p>本文原发于我的博客：<a href="https://link.segmentfault.com/?enc=mV%2F3R7z9oMGIAuNGUvGglQ%3D%3D.1yiQDA4y5SpR%2BCPcdrWuI1hbYgH6y0xhyrCVVHTJax0%3D" rel="nofollow" target="_blank">landonVPS</a><br/>​</p>]]></description></item><item>    <title><![CDATA[MindSpore 大模型训练进阶：高效显存管理 + 增量式断点续训的实践 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047577946</link>    <guid>https://segmentfault.com/a/1190000047577946</guid>    <pubDate>2026-01-28 15:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在千亿参数大模型（如 LLaMA-7B/13B）的训练场景中，显存瓶颈与训练中断恢复是两大核心痛点 —— 前者直接限制模型规模，后者会导致工业级训练的时间与算力成本翻倍。本次分享基于 MindSpore 的高阶训练特性，构建 “分层显存优化 + 增量式断点续训” 的工业级大模型训练方案，实现单卡支持 7B 模型全量训练、断点恢复耗时从小时级降至分钟级，同时通过算子级优化将训练吞吐量提升 35%。方案附全流程代码与显存利用率量化分析。</p><h3>1. 大模型分层显存优化：混合精度 + 张量重计算 + 显存分片</h3><p>场景：训练 LLaMA-7B 模型时，单卡（A100 80G）直接加载全量参数会导致显存占用超 90%，训练中极易触发 OOM；传统混合精度训练仅优化数据类型，无法解决大模型的中间激活值显存占用问题。</p><p>MindSpore 技术实践：</p><p>采用三级显存优化策略，结合 MindSpore 的AMP混合精度、Recompute张量重计算、TensorSlicer显存分片能力，分层降低参数、激活值、梯度的显存开销：</p><pre><code class="python">import mindspore as ms
from mindspore import nn, ops
from mindspore.nn.transformer import RecomputeConfig
from mindspore.train import amp

# 1. 混合精度训练配置（FP16+BF16混合）
amp_level = "O3"  # 最高级混合精度优化
cast_type = ms.float16  # 权重与激活值用FP16，梯度用BF16
loss_scaler = amp.DynamicLossScaler(scale_value=2**16, scale_factor=2, scale_window=1000)

# 2. 张量重计算配置（仅保存关键层梯度，中间激活值按需重计算）
recompute_config = RecomputeConfig()
recompute_config.recompute = True
recompute_config.recompute_slice_activation = True  # 激活值分片存储
# 仅对Transformer的FeedForward层开启重计算（注意力层保留激活值提升效率）
recompute_layers = ["feed_forward"]

# 3. 显存分片策略（按维度拆分大张量，降低单张量显存占用）
class TensorSlicer:
    def __init__(self, slice_dim=1, slice_num=4):
        self.slice_dim = slice_dim
        self.slice_num = slice_num
        self.slice_op = ops.Split(axis=slice_dim, output_num=slice_num)
    
    def slice(self, tensor):
        return self.slice_op(tensor)
    
    def concat(self, tensor_list):
        return ops.Concat(axis=self.slice_dim)(tensor_list)

# 4. 集成到LLaMA模型训练
class LLaMATrainNetwork(nn.Cell):
    def __init__(self, llama_model):
        super().__init__()
        self.model = llama_model
        self.slicer = TensorSlicer()
        self.loss_fn = nn.CrossEntropyLoss()
    
    def construct(self, input_ids, labels):
        # 输入张量分片，降低显存峰值
        input_ids_slices = self.slicer.slice(input_ids)
        logits_slices = []
        for slice_ in input_ids_slices:
            logits = self.model(slice_)
            logits_slices.append(logits)
        logits = self.slicer.concat(logits_slices)
        loss = self.loss_fn(logits.reshape(-1, logits.shape[-1]), labels.reshape(-1))
        return loss

# 构建训练网络
llama_model = nn.TransformerDecoder(num_layers=32, hidden_size=4096)  # LLaMA-7B等效结构
train_net = LLaMATrainNetwork(llama_model)
train_net = amp.build_train_network(
    train_net, optimizer=nn.AdamWeightDecay(train_net.trainable_params(), 1e-4),
    loss_scale_manager=loss_scaler, amp_level=amp_level, cast_type=cast_type
)

# 效果：LLaMA-7B单卡训练显存占用从75G降至45G，激活值显存占比从40%降至15%</code></pre><h3>2. 增量式断点续训：全状态保存与精准恢复</h3><p>场景：大模型训练周期长达数周，断电、硬件故障等中断事件频发；传统断点续训仅保存模型参数，重启后需重新初始化优化器、重置数据迭代器，导致重复训练 10%~20% 的 epoch，算力浪费严重。</p><p>MindSpore 技术实践：</p><p>基于 MindSpore 的CheckpointManager实现增量式全状态保存—— 除模型参数外，额外保存优化器状态、数据迭代器位置、训练超参、epoch/step 进度，恢复时精准接续训练：</p><pre><code class="python">from mindspore.train import CheckpointManager, CheckpointConfig
from mindspore.dataset import GeneratorDataset

# 1. 自定义全状态数据集（记录迭代器位置）
class ResumableDataset:
    def __init__(self, data, start_step=0):
        self.data = data
        self.start_step = start_step
        self.total_steps = len(data)
    
    def __getitem__(self, idx):
        return self.data[idx]
    
    def __len__(self):
        return self.total_steps - self.start_step

# 2. 配置增量式断点保存
ckpt_config = CheckpointConfig(
    save_checkpoint_steps=1000,  # 每1000步保存一次
    keep_checkpoint_max=5,  # 保留最新5个断点
    integrated_save=True  # 集成保存模型+优化器状态
)

# 自定义CheckpointManager，额外保存训练状态
class IncrementalCheckpointManager(CheckpointManager):
    def __init__(self, config, ckpt_dir):
        super().__init__(config, ckpt_dir)
        self.train_state = {"epoch": 0, "step": 0, "start_step": 0}
    
    def save_train_state(self, epoch, step):
        self.train_state["epoch"] = epoch
        self.train_state["step"] = step
        # 保存到JSON文件，与ckpt文件一一对应
        import json
        with open(f"{self.ckpt_dir}/train_state_{epoch}_{step}.json", "w") as f:
            json.dump(self.train_state, f)
    
    def load_train_state(self, ckpt_path):
        import json
        state_path = ckpt_path.replace(".ckpt", ".json")
        with open(state_path, "r") as f:
            self.train_state = json.load(f)
        return self.train_state

# 3. 断点恢复逻辑
ckpt_manager = IncrementalCheckpointManager(ckpt_config, "./llama_ckpt")
resume_ckpt = "./llama_ckpt/ckpt_0_10000.ckpt"  # 待恢复的断点文件

if resume_ckpt:
    # 加载模型+优化器参数
    param_dict = ms.load_checkpoint(resume_ckpt)
    ms.load_param_into_net(train_net, param_dict)
    # 加载训练状态
    train_state = ckpt_manager.load_train_state(resume_ckpt)
    start_epoch = train_state["epoch"]
    start_step = train_state["step"]
    # 恢复数据集迭代器位置
    dataset = ResumableDataset(raw_data, start_step=start_step)
else:
    start_epoch = 0
    start_step = 0
    dataset = GeneratorDataset(raw_data, column_names=["input_ids", "labels"])

# 4. 训练循环（含断点保存）
for epoch in range(start_epoch, 100):
    for step, (input_ids, labels) in enumerate(dataset):
        loss = train_net(input_ids, labels)
        current_step = start_step + step + 1
        # 每1000步保存断点（含训练状态）
        if current_step % 1000 == 0:
            ckpt_manager.save_checkpoint(train_net, epoch=epoch, step_num=current_step)
            ckpt_manager.save_train_state(epoch, current_step)

# 效果：断点恢复时间从2小时降至10分钟，无重复训练步骤，算力利用率提升20%</code></pre><h3>3. 显存动态监控与自适应调整</h3><p>场景：大模型训练过程中，显存占用会随数据分布、模型迭代波动，固定的 batch size 与重计算策略无法适配动态显存变化，仍存在 OOM 风险。</p><p>MindSpore 技术实践：</p><p>利用 MindSpore 的Profiler实现显存实时监控，结合预设阈值动态调整 batch size 与重计算层数，确保显存占用稳定在安全区间：</p><pre><code class="python">from mindspore.profiler import Profiler
import psutil

# 1. 显存监控函数
def monitor_gpu_memory(threshold=0.85):
    """监控GPU显存占用，超过阈值返回True"""
    profiler = Profiler(output_path="./profiler")
    mem_info = profiler.get_memory_info()
    used_ratio = mem_info["used"] / mem_info["total"]
    profiler.analyse()
    return used_ratio &gt; threshold

# 2. 自适应调整策略
class AdaptiveTrainer:
    def __init__(self, train_net, init_batch_size=8):
        self.train_net = train_net
        self.batch_size = init_batch_size
        self.max_batch_size = 16
        self.min_batch_size = 4
    
    def adjust_batch_size(self, is_over_threshold):
        if is_over_threshold and self.batch_size &gt; self.min_batch_size:
            self.batch_size -= 2
            print(f"显存超限，batch size调整为{self.batch_size}")
        elif not is_over_threshold and self.batch_size &lt; self.max_batch_size:
            self.batch_size += 2
            print(f"显存充足，batch size调整为{self.batch_size}")
        return self.batch_size

# 3. 集成到训练循环
adaptive_trainer = AdaptiveTrainer(train_net)

for epoch in range(start_epoch, 100):
    dataset = dataset.batch(adaptive_trainer.batch_size)
    for step, (input_ids, labels) in enumerate(dataset):
        loss = train_net(input_ids, labels)
        # 每500步监控显存并调整
        if step % 500 == 0:
            is_over = monitor_gpu_memory()
            adaptive_trainer.adjust_batch_size(is_over)

# 优化前后对比
| 指标                | 优化前 | 优化后 |
|---------------------|--------|--------|
| 单卡7B模型显存占用 | 75G    | 45G    |
| 断点恢复耗时        | 120min | 10min  |
| OOM发生率           | 15%    | 0%     |
| 训练吞吐量          | 22样本/秒 | 30样本/秒 |</code></pre>]]></description></item><item>    <title><![CDATA[基于1百万物种的百亿级基因数据，英伟达等构建EDEN系列模型，基因组与蛋白质预测能力达 SOTA 超]]></title>    <link>https://segmentfault.com/a/1190000047577983</link>    <guid>https://segmentfault.com/a/1190000047577983</guid>    <pubDate>2026-01-28 15:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>可编程生物学的根本目标在于对生命系统实现理性设计与精准调控，从而为复杂疾病带来革命性疗法。然而，这一进程长期受限于生物系统的内在复杂性。跨尺度的调控网络、隐藏的长程序列依赖关系，以及生物应对环境变化的多样适应性，都使得传统「试错式」研发陷入定制化、低通量、高成本的困境。</p><p>究其根本，当前计算模型所依赖的训练数据，无论规模还是多样性，都远未覆盖生命在数十亿年进化中形成的浩瀚设计空间。这些模型因此难以捕捉到通用设计法则，在面对多模态、跨尺度的创新疗法设计时，泛化能力严重不足。</p><p>为突破这一根本性限制，Basecamp Research、英伟达及多所顶尖学术机构共同开发了 EDEN 系列宏基因组基础模型，通过从海量跨物种、关联环境信息的自然进化数据中学习，首次系统性地提炼出生物设计的深层「语法」与通用原则。该模型参数规模达 280 亿，在多项基准测试中取得了 SOTA，其核心突破在于获得了卓越的跨物种序列理解与生成能力，从而将生物工程从「筛选」推进到「可预测编程」的新阶段。</p><p>为验证 EDEN 作为统一生物设计引擎的能力，研究团队在多种治疗模式上进行了系统测试。在基因治疗中，仅凭目标位点 30 个碱基的提示，EDEN 即可从头设计出能在人类基因组中精准整合大片段的活性重组酶。在抗菌肽设计方面，同一模型生成的肽库对多重耐药病原体活性高达 97%，且具备微摩尔级效价。在生态系统层面，EDEN 成功构建出包含数万个人工基因组、代谢途径准确且物种关联性合理的合成微生物组。</p><p>相关研究成果以「Designing AI-programmable therapeutics with the EDEN family of foundation models」为题，已发表预印本于 bioRxiv。</p><p>研究亮点：</p><ul><li>开创了从进化历史中直接学习通用设计原则的新范式，利用覆盖全球生物多样性的宏基因组数据库BaseData 进行训练，获得了卓越的跨物种序列理解与生成能力</li><li>验证了单一基础模型驱动多尺度、多模态疗法设计的强大通用性，证明一个模型即可统一应对从分子到生态系统的复杂设计挑战。</li><li>EDEN 仅凭 DNA 提示，就能为多种疾病相关位点设计出功能性的重组酶，在未经训练的靶点上实现了 63.2% 的功能命中率</li></ul><p><img width="723" height="302" referrerpolicy="no-referrer" src="/img/bVdnNmc" alt="" title=""/></p><p><em>论文地址：</em></p><p><em><a href="https://link.segmentfault.com/?enc=2iEp0NxSAEQCBzP76KvClA%3D%3D.mbbpIQLKGbw8XDJbqOnRglluW7wv2r6rTWho1SXL8gW1O%2FylPm2j%2FCQEEDyKI4au" rel="nofollow" target="_blank">https://doi.org/10.64898/2026.01.12.699009</a></em>\<br/>关注公众号，后台回复「EDEN」获取完整 PDF</p><p>更多 AI 前沿论文：\<br/><a href="https://link.segmentfault.com/?enc=apiTP5J6DXng3hmw1sp%2F4A%3D%3D.zirvDNWKQkmRr3X1%2BChhFEw3m6TqJQ3FDMIJAdGjQVg%3D" rel="nofollow" target="_blank">https://hyper.ai/papers</a></p><h2>BaseData 数据集：以高质量长序列重塑生物 AI 数据基准</h2><p>该研究使用的 BaseData 数据集从根本上突破了传统生物数据库的局限。传统数据库通常依赖有限的参考基因组和碎片化的短序列，而 BaseData 旨在系统性地捕捉完整进化信号，构建了一个覆盖全球生物多样性的进化基因组数据供应链。</p><p>BaseData 的核心价值首先体现在规模与战略性构成上。如下图所示，其包含 9.7 万亿个用于训练的核苷酸标记，涵盖超过 1 百万个新物种和 1 千亿个新基因。更重要的是，其内容并非随机采集，而是刻意富集了环境宏基因组、噬菌体及可移动遗传元件等高信息密度的序列。这些数据天然记录了噬菌体-宿主互作、水平基因转移等关键进化动力，为模型学习跨物种的通用功能规则提供了核心素材。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdnNmd" alt="" title="" loading="lazy"/><br/><em>BaseData 与 OG2 对比、基于生物群落起源、基于 pH 值的 UMAP 图</em></p><p>在数据质量方面，BaseData 实现了质的提升，关键体现在序列上下文的完整性。与广泛使用的 OpenGenome-2（OG2）相比，其连续序列片段（重叠群）的长度中位数达到 18.6 kbp（OG2 为 4.0 kbp），每个组装体包含的基因数量也显著更多。更长的连续背景对模型理解基因间的调控与代谢通路至关重要。</p><p><img width="503" height="314" referrerpolicy="no-referrer" src="/img/bVdnNme" alt="" title="" loading="lazy"/><br/>BaseData 与 OG2 在宏基因组数据库中的片段长度分布</p><p>为量化这种质量优势，研究团队开展了对照实验：在 BaseData 和 OG2 的等规模数据上训练了系列模型。结果清晰地验证了「质量感知缩放定律」。在同等计算开销下，基于 BaseData 训练的模型测试困惑度下降更快。一个重要发现是，大型模型（如 70 亿参数）能充分利用 BaseData 的长序列信息，性能最终超越在 OG2 上训练的同类模型，直接证明了长程上下文对模型性能的决定性影响。</p><p><img width="516" height="407" referrerpolicy="no-referrer" src="/img/bVdnNml" alt="" title="" loading="lazy"/><br/>在不同参数情况下，EDEN 模型家族的困惑度测试与浮点运算次数之间的关系</p><p>基于这一规律，研究团队使用完整 BaseData 训练了参数量达 280 亿的 EDEN-28B 模型。该模型不仅达到了最低测试困惑度，其性能提升轨迹也与从小规模模型推导的缩放预测完美吻合。在下游任务监控中，模型在预训练期间生成蛋白质的结构置信度指标随训练进程持续单调上升，证明高质量数据直接且稳定地提升了面向实际治疗的生成能力。</p><p>此外，所有数据均通过覆盖 28 个国家、208 项许可的规范化法律协议获取，建立了从源头到使用的可追溯与利益共享框架，为大规模生物 AI 研究设立了必要的伦理与治理标准。</p><p><img width="723" height="321" referrerpolicy="no-referrer" src="/img/bVdnNmt" alt="" title="" loading="lazy"/></p><p>EDEN-28B 模型生成的大丝氨酸重组酶 pLDDT 的分布情况</p><h2>通用生物设计引擎 EDEN</h2><p>EDEN 模型家族以「规模化、通用性与可扩展性」为核心设计原则，模型参数规模跨越 1 亿至 280 亿。其中，作为核心工作模型的 EDEN-28B，其架构与训练策略均深度适配宏基因组数据的独特性质。</p><p>在模型架构上，EDEN 采用了经过大规模语言模型验证的仅解码器 Transformer 架构，具体基于 Llama 3.1 的设计风格。这一选择得益于 Transformer 对长程依赖关系卓越的建模能力。EDEN-28B 包含 48 层网络，隐藏层维度为 6,144，配备 48 个注意力头，使用 SwiGLU 激活函数与 RoPE 位置编码。模型使用单核苷酸分辨率的标记化方法，词汇表大小为 512，从而能够以最基础的「字母」级别理解和生成 DNA 序列。</p><p>一项关键技术亮点在于其长序列生成能力。尽管模型的上下文窗口设定为 8,192 个标记，但在实际应用中，它能够稳定生成并准确拼接超过 13,000 个碱基对的连贯基因组序列，且保持正确的基因顺序、阅读框与调控元件结构。这表明模型所学到的远非局部模式匹配，而是能够推断并应用一套超越物理窗口长度的、更深层的基因组组织「语法」。整个训练在 1,008 个 H100 GPU 上完成，通过大规模分布式计算实现了对海量进化数据的高效学习。</p><p><img width="366" height="329" referrerpolicy="no-referrer" src="/img/bVdnNmu" alt="" title="" loading="lazy"/><br/>用于EDEN训练的类似 Llama3.1 的架构</p><p>EDEN 的核心设计哲学遵循「预训练‑微调」范式。在第一阶段，模型在涵盖跨物种进化历史的 BaseData 上进行大规模预训练，从而内化了关于蛋白质折叠、代谢通路组装等生物设计的通用原则。</p><p>在此坚实基础上，针对特定治疗设计任务——例如设计靶向特定 DNA 位点的重组酶或生成新型抗菌肽——仅需使用少量高质量的任务配对数据进行轻量级微调，即可使模型快速掌握该任务的「方言」。这种设计使单一的 EDEN 模型能够作为一个通用的「生物序列引擎」，灵活适配并驱动从基因插入、多肽设计到微生物组工程等截然不同的治疗模式，真正实现了「一个模型，多重能力」的可编程生物学愿景。</p><h2>驱动从分子、细胞到生态系统级别的治疗创新</h2><p>为系统验证 EDEN 模型在实际治疗设计中的通用性与有效性，研究团队选择了在尺度、模式与生物复杂性上截然不同的四个关键方向进行实验验证。</p><p>在 AI 可编程基因插入（aiPGI）领域，团队重点攻克了「大片段 DNA 精准整合」这一长期瓶颈。传统 CRISPR 技术依赖造成双链断裂，而天然的大型丝氨酸重组酶无法识别人类基因组序列。如下图所示，EDEN 的解决方案是，通过对模型中蕴含的数百万 LSR‑附着位点配对关系进行微调，构建出能够理解「目标 DNA 序列→对应重组酶」映射关系的 EDEN‑LSR 模型。</p><p><img width="720" height="525" referrerpolicy="no-referrer" src="/img/bVdnNmw" alt="" title="" loading="lazy"/></p><p>大型丝氨酸重组酶（LSR）机制的示意图</p><p>实验结果表明，该方案成功为 10 个不同的疾病相关基因位点及 4 个潜在「安全港」位点生成了具有活性的 LSR，总体功能命中率达到 53.6%。更关键的是，其中 50% 的设计酶能在原代人类 T 细胞中实现治疗相关水平的 CAR 基因插入，且部分变体在细胞系中实现了高达 40% 的整合效率，证明了其临床应用潜力。</p><p><img width="723" height="520" referrerpolicy="no-referrer" src="/img/bVdnNmz" alt="" title="" loading="lazy"/></p><p>利用 EDEN 实现人工智能可编程基因插入（aiPGI）</p><p>在新型桥接重组酶（BRs）领域，EDEN 模型的能力进一步拓展至一个更具编程灵活性的基因编辑系统——桥接重组酶。如下图所示，为优化设计，团队通过在数百万个含 BR 的基因组区域上对模型进行微调，构建了 EDEN‑BR 专门模型。</p><p><img width="723" height="403" referrerpolicy="no-referrer" src="/img/bVdnNmB" alt="" title="" loading="lazy"/></p><p>桥重组酶系统的示意图</p><p>关键的生化实验验证了这一设计流程的可行性。如下图所示，在初步的无细胞测试中，由 EDEN‑BR 生成的 49 个候选序列里，有两个被证实具有明确的重组酶活性。这两个名为 DF3843 和 DF3881 的人工设计蛋白，与已知任何天然 BR 序列相似性最高仅分别为 85% 和 65.8%，与一个已被深入研究的参照蛋白 ISCro4 的序列相似性甚至低于 35%，但在三维结构上却高度相似。这证明 EDEN 并非进行简单序列模仿，而是掌握了决定蛋白质功能与折叠的核心结构逻辑。</p><p><img width="723" height="314" referrerpolicy="no-referrer" src="/img/bVdnNmC" alt="" title="" loading="lazy"/></p><p>EDEN 生成的和野生型 BR 的 IVTT 测验结果</p><p>在新型抗菌肽（AMP）领域，研究团队验证了 EDEN 设计新型抗菌肽的能力。如下图所示，通过采用包含基因组上下文信息的微调策略，模型能够生成结构新颖的抗菌肽序列。</p><p><img width="563" height="551" referrerpolicy="no-referrer" src="/img/bVdnNmD" alt="" title="" loading="lazy"/></p><p>抗菌肽生成的微调和提示策略</p><p>实验验证取得了突破性成果。如下图所示，在一个由 33 条生成肽构成的 AMP 库中，高达 97% 的序列显示出抗菌活性。其中，针对多重耐药的革兰氏阴性菌（如鲍曼不动杆菌），顶级设计候选物的抑菌浓度达到微摩尔级别，展现了强大的穿透外膜能力。这些生成序列与已知数据库的相似度普遍很低，证实了模型能够突破传统同源性限制、实现真正的「从头设计」。</p><p><img width="723" height="417" referrerpolicy="no-referrer" src="/img/bVdnNmF" alt="" title="" loading="lazy"/></p><p>EDEN 生成的肽对病原菌株的抗菌活性验证测定结果</p><p>最后，在最复杂的生态系统层面，研究挑战了「合成微生物组」的设计。传统方法难以协调多物种间的代谢互作与生态平衡。如下图所示，EDEN 通过消化系统微生物组数据进行微调后，仅根据功能基因或生态位提示，便成功生成了一个包含超过 9 万个物种、规模达千兆碱基的合成宏基因组。</p><p><img width="723" height="278" referrerpolicy="no-referrer" src="/img/bVdnNmJ" alt="" title="" loading="lazy"/></p><p>合成微生物组的生成策略</p><p>生成结果显示出高度的生态真实性：99% 的物种被正确归类于消化系统相关的生物群，并完整保留了跨物种的代谢通路。此外，模型甚至能准确生成整合在宿主基因组中的原噬菌体结构，证明了其已捕捉到宿主与病毒间精细的互作逻辑。</p><p><img width="723" height="467" referrerpolicy="no-referrer" src="/img/bVdnNmK" alt="" title="" loading="lazy"/></p><p>合成微生物组的 UMAP 分析和 16 种显著富集的代谢途径概览</p><p>这四大跨尺度实验共同表明，基于统一进化数据预训练的 EDEN 模型，能够作为一个通用的生物设计引擎，仅需极少的任务特异性数据引导，即可快速、可靠地驱动从分子、细胞到生态系统级别的治疗创新，为可编程生物学奠定了坚实的实践基础。</p><h2>AI 与合成生物学的融合创新</h2><p>近年来，可编程生物学领域在学术界与工业界的融合创新步伐显著加快，一系列重磅进展正在重新定义生物设计的边界。</p><p>全球顶尖学术机构正以前所未有的规模和精度，将进化智慧转化为可计算的模型。例如，2024 年初，由DeepMind、Isomorphic Labs 与多所大学组成的联合团队，发布了能同时预测蛋白质结构、相互作用并生成具有特定功能全新蛋白质的 AlphaFold 3 模型。该模型首次将生物分子的复杂共舞纳入统一框架进行高精度模拟，被 Nature 杂志评价为「在绘制生命的分子机器内部运作方式上实现了飞跃」。</p><p>产业界则加速将这些突破转化为平台与疗法。在 AI 制药领域，NVIDIA 与 Recursion Pharmaceuticals 发布了生物化学 AI 模型库 BioNeMo，旨在使药物发现从「大海捞针」转向「按图索骥」。合成生物学公司 Ginkgo Bioworks 则利用其自动化平台，系统性设计用于碳捕获和化学品生产的微生物群落，推动「合成生态系统」的工程化。</p><p>这股由数据和算法驱动的新浪潮，正推动生物学从观察描述的科学，转变为一门可编写、可调试、可预测的工程学科，不仅意味着我们能够更精准地编写生命代码来攻克疾病，更预示着我们将有能力系统地设计生物系统，以应对资源、环境与健康领域的全球性挑战。</p><p>参考链接：\<br/>1.<a href="https://link.segmentfault.com/?enc=YCJ%2FuTWLPWeTDYF3xrf3Wg%3D%3D.OoMqwRE4N9mFECb0%2BUsEP3srS0s86DTiTyPNHPlyoY%2FJfMOZVSr3xuMW8xElCPqoFA8lJ2DhxU3SSnyONFUk%2B1diCGGngzJZwOxITzBEufsGroCWcNGpoi7BXU9WJhhi" rel="nofollow" target="_blank">https://nvidianews.nvidia.com/news/nvidia-announces-broad-exp...</a>\<br/>2.<a href="https://link.segmentfault.com/?enc=TTZmMHKeIPbMv1NJsrWHAw%3D%3D.F1OqcHbDLJjvfpnjG3Zwldhgo%2F4uf6KBXnfqILvqXtp%2BEloIiVPPDie59tgitLJyxOoNv1JhelQ5ainZoTMF%2BbHh2SzmBFX3foZImBzCGS8%2F7Wnri1tMdO9fmxGPXCG%2BdgC3ItFng%2F1QkgLRiZkwkdkUnH%2Bv1zO2U4TA8zh6wIE%3D" rel="nofollow" target="_blank">https://www.ginkgobioworks.com/2024/01/04/ginkgo-bioworks-and...</a></p>]]></description></item><item>    <title><![CDATA[阿里发布 Qwen3-Max-Thinking；阶跃星辰获超 50 亿融资，加速推进「AI 进入物理]]></title>    <link>https://segmentfault.com/a/1190000047577649</link>    <guid>https://segmentfault.com/a/1190000047577649</guid>    <pubDate>2026-01-28 14:04:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577651" alt="" title=""/></p><p>开发者朋友们大家好：</p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的 <strong>技术</strong> 」、「有亮点的 <strong>产品</strong> 」、「有思考的 <strong>文章</strong> 」、「有态度的 <strong>观点</strong> 」、「有看点的 <strong>活动</strong> 」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2><strong>01 有话题的技术</strong></h2><h6><strong>1、阿里发布万亿参数模型 Qwen3-Max-Thinking，性能对标 GPT-5.2</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577652" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577653" alt="" title="" loading="lazy"/></p><p>昨天，阿里正式发布千问旗舰推理模型 Qwen3-Max-Thinking。该模型<strong>总参数量超万亿（1T）</strong>，在多项权威评测中刷新全球纪录，官方宣称其性能媲美 GPT-5.2、Gemini 3 Pro，是迄今为止最接近国际顶尖水平的国产 AI 大模型。</p><p>Qwen3-Max-Thinking 的预训练数据量高达 36T Tokens，并在预览版基础上进行了更大规模的强化学习后训练。在涵盖事实知识、复杂推理、指令遵循等 19 个基准测试中，该模型刷新了数项最佳表现（SOTA）纪录。</p><p>根据官方公布的评测数据，Qwen3-Max-Thinking 在启用 TTS（Test-time Scaling）机制后，在科学知识（GPQA Diamond）测试中得分 92.8，略高于 GPT-5.2 的 92.4；</p><p>在数学推理（IMO-AnswerBench）和代码编程（LiveCodeBench 2025.02-2025.05）中分别取得 91.5 和 91.4 的高分，均优于 GPT-5.2、Claude Opus 4.5 和 Gemini 3 Pro。</p><p>特别是在启用工具的「人类最后的测试」（Humanity's Last Exam with Search）中，该模型得分为 58.3，大幅领先 GPT-5.2-Thinking 的 45.5 分，录得当前所有模型的最高分。</p><p><strong>技术层面，阿里表示 Qwen3-Max-Thinking 采用了一种全新的测试时扩展机制。</strong> 与业界普遍的简单增加并行推理路径不同，新机制能对此前推理结果进行「经验提取」式的提炼，通过多轮自我迭代在相同上下文中实现更高效的推理计算。</p><p><strong>此外，模型大幅增强了自主调用工具的原生 Agent 能力。</strong> 经过基于规则奖励与模型奖励的联合强化学习训练，模型可自适应选用搜索、个性化记忆和代码解释器等核心工具，不仅回答更流畅，还大幅降低了模型幻觉。</p><p>目前，普通用户可通过千问 PC 端和网页端免费试用新模型，千问 App 也即将接入；企业开发者则可通过阿里云百炼获取 API 服务。</p><p>体验链接</p><p>Qwen Chat: https\://chat.qwen.ai/</p><p>阿里云百炼：</p><p>https\://bailian.console.aliyun.com/cn-beijing/?tab=model#/model-market/detail/qwen3-max-2026-01-23</p><p>( @APPSO)</p><h6><strong>2、打通感知、交互与执行：讯飞星辰升级多模态全栈能力，加速智能体规模化落地</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577654" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577655" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577656" alt="" title="" loading="lazy"/></p><p>1 月 26 日，讯飞星辰智能体平台官宣重大升级，实现了讯飞星辰智能体平台和 AIUI 开放平台完全打通、升级超拟人交互技术、支持快速定制音色、RPA 升级，提供<strong>一套全面且完整的多模交互解决方案</strong>，让智能体拥有更全面的类人化交互能力、全场景执行能力。</p><ul><li><strong>AIUI 开放平台接口打通</strong> ：支持在「讯飞星辰」创建智能体并一键发布至 AIUI，实现语音交互与机器人动作规划（如桌面机器人绘本生成、运动轨迹）的同步调用与快速集成。</li><li><strong>秒级「一句话声音复刻」</strong> ：利用超拟人交互技术，支持通过自然语言描述声线并在几秒内合成 4 个候选音色；支持中英日韩粤等多语种、方言及多风格（新闻、交谈、绘本）音色生成。</li><li><strong>单图构建多模态数字分身</strong> ：支持通过一张照片快速生成数字人，其口型、表情及动作由大模型自动驱动；结合多模态视觉理解，支持智能体实现主动迎宾与环境感知的交互闭环。</li><li><strong>RPA 执行能力组件化</strong> ：升级网页自动化智能组件，支持非专业开发人员通过低代码配置参数进行流程编排；提供开源可视化数据表格功能，实现数据提取与处理过程的透明化。</li></ul><p>最直观的一个例子就是，将 <strong>为智能体定制声音的时间压缩到了几秒钟</strong> 。</p><p>发布会的实际演示中，操作人员在讯飞星辰智能体平台生成了曹操人格的智能体后，通过自然语言描述想要的音色声线、输入试听文本、点击生成，就在几秒内合成 4 个候选音色。接着选择保存、应用音色后，用户就能与刚刚的曹操人格智能体进行语音聊天。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577657" alt="" title="" loading="lazy"/></p><p>这是讯飞星辰智能体平台此次升级的一个缩影，而智能体的未来形态，将从单一工具，升级为兼具感知、交互能力，拥有专属声音、形象与性格人设，还能自主完成操作执行的全能型智能体，驱动这一切进化的核心，正是<strong>多模交互技术</strong>。</p><p>当前海内外大厂与科创企业均在智能体平台赛道加速布局、密集发力，但行业仍普遍面临技术落地难、场景适配不深的核心痛点。</p><p>讯飞星辰智能体平台此次实现<strong>感知、交互、执行</strong>三大核心能力的一体化整合，从底层打破智能体落地过程中的技术协同壁垒，直面其场景适配难题，为智能体技术的规模化落地扫清关键障碍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577658" alt="" title="" loading="lazy"/></p><p>简言之，讯飞星辰智能体平台此次升级，核心便是瞄准降低智能体开发门槛、丰富其可落地的能力边界两大核心目标，在扩展服务能力的基础上，还提供了低代码、一键接入、快速接入等快速开发部署工具。</p><p>总的来看，当前智能体产业技术成熟度足够支撑场景落地，市场需求旺盛，但落地效率与成本仍是核心瓶颈，而打通场景适配、能力集成、生态协同的全栈能力，将成为智能体产业竞争的核心壁垒。</p><p>相关链接：</p><p>https\://agent.xfyun.cn</p><p>（@智东西、@讯飞开放平台）</p><h6><strong>3、Google 支付 6800 万美元和解金，解决语音助手「监视」用户的指控</strong></h6><p>据路透社报道，Google 已同意支付 6800 万美元，以解决一项指控其语音助手非法监视用户、并利用相关数据投放广告的索赔诉讼。</p><p>Google 在这项集体诉讼的和解协议中并未承认存在任何不当行为。该诉讼指控 Google「在未经个人同意的情况下，非法且故意地拦截并录制个人的机密通信，并随后将这些通信未经授权地披露给第三方。」诉讼进一步声称，「从这些录音中收集的信息被错误地传输给了第三方，用于定向广告及其他目的。」</p><p><strong>该案件的核心争议集中在「错误唤醒」上，即指控 Google Assistant 即使在用户未通过唤醒词有意触发的情况下，也会自动激活并录制用户的通信内容。TechCrunch 已就此联系 Google 寻求置评。</strong></p><hr/><p>长期以来，美国民众一直怀疑电子设备在不适当地监视他们，这些怀疑正日益转化为法律诉讼。2021 年，苹果公司曾同意支付 9500 万美元，以解决关于其语音助手 Siri 在未获用户提示的情况下录制对话的类似指控。</p><p>与其他科技巨头一样，Google 近年来也面临着多起隐私相关的诉讼。去年，该公司同意向得克萨斯州支付 14 亿美元，以解决两起指控其违反该州数据隐私法的诉讼。</p><p>( @TechCrunch)</p><hr/><h2><strong>02 有亮点的产品</strong></h2><h6><strong>1、249 元起，苹果推出升级版 AirTag，精确查找范围扩大 50%</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577659" alt="" title="" loading="lazy"/></p><p>昨天，苹果突然官宣，正式推出新款 AirTag，采用与 iPhone 17 系列、iPhone Air、Apple Watch Ultra 3 及 Apple Watch Series 11 相同的第二代超宽带芯片，在连接范围、精确查找能力与扬声器音量方面均进行了大幅升级：</p><ul><li>精确查找范围最高提升 50%，定位更快更准</li><li>蓝牙连接范围扩大，远距离也能找到</li><li>扬声器音量提升 50%，提示音更响亮</li><li>支持 Apple Watch 精确查找，查找场景更丰富</li><li>「查找」网络升级，脱离配对设备也能回传位置</li><li>防追踪机制强化，跨平台警报更可靠</li><li>支持共享物品位置，协助航空公司找回延误行李</li><li>外壳与磁铁采用高比例再生材料，更环保</li></ul><p>新款 AirTag 已正式开售。售价方面，单件装售价 249 元，四件装售价 849 元，并提供免费镌刻服务。零售店将于本周晚些时候陆续上架。</p><p>与此同时，苹果今天还推送了 iOS、iPadOS 和 watchOS 26.2.1，主要更新内容是新增对 AirTag 2 的支持。</p><p>( @APPSO)</p><h6><strong>2、京东「抢跑」淘宝，首款智能眼镜购物应用落地乐奇 Rokid</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577660" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577661" alt="" title="" loading="lazy"/></p><p>1 月 26 日消息，京东科技购物智能体 JoyGlance 正式登录智能眼镜品牌乐奇 Rokid，标志着行业首款智能眼镜购物应用正式落地，是京东布局「具身智能消费场景」的关键一步。</p><p>用户只需将 Rokid 眼镜系统更新至最新版本，应用由京东自研大模型 JoyAI 驱动，深度融合 Rokid 在光波导显示、远场语音交互与自研操作系统上的硬件能力，将传统网购流程<strong>从「搜索—浏览—比价—下单—支付」五步</strong>，压缩为极简的 <strong>「说、看、付」三步</strong> 。</p><p>据悉，2025 年 10 月，Rokid 乐奇与京东科技就达成战略协议。此次携手，不仅是技术突破，更是消费入口的迁移，开启全球首个「所见即购买」的智能眼镜全链路购物入口，<strong>实现「目光所及、皆可购买」</strong> 。</p><p>当购物从「指尖滑动」转向「目光注视」，智能眼镜正从可穿戴设备升级为下一代空间计算与消费交互终端。用户不再依赖搜索框或直播链接，而是将物理世界直接转化为购物入口，或为电商行业开辟了全新的场景。</p><p>（@即智 Ultra）</p><h6><strong>3、LiveTok 发布「LiveTok Avatars」：支持单张照片生成实时交互式 AI 数字孪生</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577662" alt="" title="" loading="lazy"/></p><p>LiveTok 推出基于 AI 的虚拟助手平台「LiveTok Avatars」。该产品支持通过单张静态照片构建具备实时音视频交互能力的数字分身，旨在通过拟人化的「数字孪生」替代传统文字客服，实现 24/7 的实时客户互动。</p><ul><li><strong>单图驱动数字孪生</strong> ：用户仅需上传单张人物照片，AI 即可生成具备面部动态的克隆形象，无需复杂的视频采集。</li><li><strong>行为与语调克隆</strong> ：AI 模型通过学习可复刻特定个体的说话风格、语速及特定动作习惯，提供具备自然停顿的类人语音响应。</li><li><strong>低代码 Web 集成</strong> ：支持通过嵌入数行代码直接在网站部署，无需复杂的后端环境配置。</li><li><strong>实时音视频同步</strong> ：提供低延迟的实时语音对话环境，演示版本目前支持单次最高 2 分钟的交互。</li></ul><p>目前处于 Beta 测试阶段，提供免费起步版，特定「数字孪生」功能需申请加入 Waitlist。</p><p>相关链接：</p><p>https\://www.livetok.ai/products/avatars</p><p>( @LiveTok)</p><h6><strong>4、阶跃星辰获超 50 亿人民币融资，印奇出任董事长</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577663" alt="" title="" loading="lazy"/></p><p>昨天，<strong>大模型创业公司阶跃星辰（StepFun）完成超 50 亿人民币 B+ 轮融资</strong>，创下过去 12 个月大模型赛道单笔最高融资纪录。上国投先导基金、国寿股权、浦东创投、徐汇资本、无锡梁溪基金、厦门国贸、华勤技术等产业投资方参与本轮融资，腾讯、启明、五源等老股东继续加码。本轮资金将主要用于基础模型研发，并加速「AI + 终端」战略落地。</p><p><strong>同日，阶跃星辰宣布千里科技董事长印奇正式出任公司董事长，全面负责公司战略节奏与技术方向。</strong> 印奇此前已深度参与阶跃星辰的战略规划，其加入被视为公司在大模型「季后赛」阶段强化产业落地能力的关键一步。</p><p>这笔融资规模不仅超过月之暗面此前宣布的 5 亿美元 C 轮，也高于智谱与 MiniMax IPO 募资额，成为近期 AI 资本市场最受关注的事件之一。</p><p>过去两年间，该团队在「百模大战」中突围，跻身国内大模型第一梯队，并持续坚持预训练路线，构建了覆盖语言、多模态、音频、动作等方向的完整模型矩阵。</p><p>印奇的加入补足了阶跃星辰在产业落地上的关键能力。作为旷视科技联合创始人，印奇在 AIoT、城市级物联网系统等领域拥有丰富经验，其长期关注的「AI+终端」路径也与阶跃星辰的战略方向高度一致。</p><ul><li>在商业化方面，阶跃星辰已与国内六成头部智能手机品牌达成深度合作，模型装机量突破 4200 万台，覆盖 OPPO、荣耀、中兴等品牌，日均服务用户达 2000 万人次；</li><li>在汽车领域，公司与千里科技、吉利合作，将端到端语音模型集成至智能座舱系统，吉利银河 M9 上市 3 个月销量接近 4 万辆，阶跃星辰今年的车载模型装车目标为百万级；</li><li>在技术路线方面，阶跃星辰<strong>坚持「原生多模态」策略</strong>，直接从图文交错语料进行端到端训练，以提升模型对物理世界的理解能力。其音频模型 Step-Audio-R1.1 通过 MGRD 技术在权威榜单 Artificial Analysis 上取得全球第一。</li></ul><p>印奇的加入意味着阶跃星辰将加速推进「AI 进入物理世界」的战略，并在手机、汽车等消费终端形成更具确定性的商业闭环。</p><p>( @APPSO)</p><hr/><h2>03 有态度的观点</h2><h6><strong>1、俞敏洪：AI 或消灭大量教师岗位，中小学教师「一大半是不合格的」</strong></h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577664" alt="" title="" loading="lazy"/></p><p>据快科技报道，新东方创始人俞敏洪近日在今年崇礼论坛上围绕互联网与人工智能对教育行业的影响发表最新观点。</p><p>他指出，技术变革正推动教育从「一张嘴一块黑板」到「互联网 + 教育」，再迈向「AI + 教育」，并强调这一趋势将深刻改变教师岗位结构。</p><p>俞敏洪表示，互联网仍在人类可控范围内，但其带来的舆论放大效应已深刻影响个人生活。他提到，过去三年遭遇的网暴与互联网环境密切相关。</p><p>相比之下，人工智能的影响更具结构性，其在教育、医疗、生物等领域的应用将持续扩大。</p><p>在教育场景中，他认为 AI 已能完成接近 100% 的英语交流与作业批改，不仅提升效率，也减轻学生面对老师时的心理压力。他指出，AI 的普及可能会「消灭大量老师岗位」，因为基础知识传递正被技术快速替代。</p><p>他进一步强调，<strong>未来教师的核心价值将转向激发学生潜能、塑造人格与引导成长，这些能力无法被技术替代。</strong></p><hr/><p>按照这一标准，他直言目前国内中小学教师「一大半不合格」，部分教师面对学生提问时因无法回答而迁怒学生的现象亟需改善。</p><p>俞敏洪还回顾新东方在「互联网 + 教育」时代的结构性变化：互联网放大名师影响力，使大量优秀教师离开线下课堂，包括他本人也不再走进教室授课。</p><p>他认为，AI 的到来将带来更深层次的行业重塑，对教师提出更高要求，而这些要求比以往更难达到。</p><p>他强调，人工智能的最终走向取决于使用者，而非技术本身，教育行业需要在技术变革中重新定义教师角色与价值。</p><p>( @APPSO)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577665" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577666" alt="" title="" loading="lazy"/></p><hr/><p><a href="https://link.segmentfault.com/?enc=ynL5P%2B%2BHolYdbBIP2IG1SQ%3D%3D.L6AZaR63HRDz8fqpWCSmy7A6svyOnggKxj6KZCkN3wY%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047577667" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考​</p>]]></description></item><item>    <title><![CDATA[数据服务器进行数据备份时的注意事项汇总 德迅云安全_珍珍 ]]></title>    <link>https://segmentfault.com/a/1190000047577702</link>    <guid>https://segmentfault.com/a/1190000047577702</guid>    <pubDate>2026-01-28 14:04:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>数据服务器在日常运行中承担着业务数据存储与处理的重要任务，任何数据的丢失都可能导致无法挽回的损失。因此，数据备份是保障数据安全和业务连续性不可或缺的环节。无论是企业级服务器还是中小型业务服务器，在进行数据备份时都需要充分考虑到完整性、可恢复性、效率以及安全性等多方面的因素，避免在关键时刻因备份不当而失去保障。备份工作表面上看起来只是复制和保存数据，但在实际操作中涉及存储介质选择、备份策略设计、恢复测试、安全防护等多个细节环节，任何一个环节处理不当都可能让备份形同虚设。</p><p>　　在数据备份过程中，首先需要明确的是备份目标和范围。数据服务器往往承载着数据库、日志文件、应用程序文件、配置文件等不同类别的数据，其中有些数据属于关键业务数据，必须做到实时或准实时备份，而有些数据则可以周期性备份。因此，备份前应对业务系统进行分类，明确哪些是核心数据、哪些是次要数据，确保资源合理分配。核心数据库应当采用增量备份或日志备份，保证在最短时间内能够恢复到最新状态，而一些不经常更新的存档数据则可以安排定期全量备份。备份范围明确后，才能设计出合理的方案，避免过度备份带来的资源浪费，或因遗漏关键数据而导致恢复失败。</p><p>　　在选择备份方式时，也要结合业务需求与存储条件。常见的方式包括全量备份、增量备份和差异备份。全量备份虽然最直观，但占用时间和存储空间较大，适合在首次备份或关键节点进行。增量备份只记录自上次备份以来的变动，节省存储，但在恢复时需要依赖前置的备份链条，操作复杂。差异备份则记录自上次全量备份以来的所有变化，相比增量恢复速度快，但占用空间更大。实际部署中，往往采用全量加增量或全量加差异的组合策略，在效率和恢复速度之间取得平衡。对于数据库类服务器，还可以利用数据库自身的备份机制，例如MySQL的mysqldump、xtrabackup，或者Oracle、SQL Server等自带的日志备份机制，确保一致性。</p><p>　　存储介质的选择同样是重点。传统的磁带机、光盘、机械硬盘仍然在部分行业中使用，但随着数据量的不断增长，这些介质的速度和可靠性逐渐无法满足需求。如今常见的方案包括本地磁盘阵列、NAS、SAN存储、对象存储以及云存储服务。本地存储的优势在于恢复速度快，适合短期和频繁恢复的场景，但若发生硬件故障或自然灾害则存在风险。云端存储因具备分布式冗余能力和灵活的扩展性，成为越来越多企业的选择，但需要考虑网络带宽与成本因素。在部署时，最佳实践是采用本地与远程相结合的方式，即所谓的异地备份和多副本策略，确保即使在灾难性故障下仍然可以找到可用的备份数据。</p><p>　　在备份过程中必须关注数据一致性。特别是数据库和业务系统在运行中会不断更新，如果备份时没有锁定数据或采用热备技术，可能导致备份文件出现逻辑错误，恢复后数据不完整甚至不可用。为此，可以使用快照技术或应用层级的备份工具来保证一致性，例如利用LVM快照、ZFS快照、VM快照等，在瞬间冻结数据状态，再进行备份复制。同时，事务型数据库需要考虑在备份过程中开启一致性选项，以保证数据逻辑关系完整。</p><p>　　除了备份的执行，还要重视备份的验证。很多服务器虽然定期执行了备份任务，但管理员从未进行过恢复测试，等到真正需要恢复时才发现备份文件损坏、格式不兼容或缺少关键数据。为避免这种情况，应该在日常维护中定期进行备份恢复演练，验证备份数据的可用性。恢复演练不仅能确保备份的完整性，还能帮助运维团队熟悉恢复流程，在紧急情况下能够快速反应，降低停机损失。</p><p>　　安全性是数据备份中的另一个关键问题。备份文件本身同样包含敏感信息，如果存储或传输过程中缺乏加密和访问控制，就可能成为攻击者的突破口。因此在备份设计中需要采用加密机制，对备份数据进行传输加密和存储加密，防止数据被窃取。同时要做好权限管理，确保只有经过授权的人员才能访问和恢复备份数据。对于使用云存储的备份方案，应特别关注服务提供商的安全机制和合规性，避免因为第三方平台漏洞而泄露企业数据。</p><p>　　备份调度和自动化也是不可忽视的方面。手动备份容易因操作失误或遗忘而失效，自动化调度能够保证备份按计划执行。利用脚本、任务调度器或专业的备份软件，可以设定周期性任务并生成日志，方便事后审计和问题追踪。结合监控告警系统，还能在备份失败时及时通知管理员，避免长时间处于无保护状态。</p><p>　　数据备份不仅仅是一次性的任务，而是一个持续的过程。在数据服务器生命周期中，业务需求和数据规模都会发生变化，原有的备份策略可能逐渐不再适用，因此需要定期评估与调整。通过监控存储空间使用情况、分析恢复速度、评估成本投入，不断优化备份方案，才能长期保证备份系统的有效性。</p><p>　　此外，还要结合企业整体的灾备规划进行部署。单纯的数据备份虽然能保障数据层面的安全，但如果服务器或数据中心遭遇严重事故，单一的备份手段仍可能不足。将数据备份与灾难恢复方案结合，建立容灾备份中心，确保业务在极端情况下也能快速切换到备用系统，这才是真正的业务连续性保障。</p><p>　　总而言之，数据服务器在进行数据备份时需要注意备份目标和范围的明确、备份方式的合理选择、存储介质的多样化和安全性、一致性保障、备份验证、恢复演练、权限控制、自动化调度以及与灾备体系的结合。备份的核心价值不在于拥有多少份数据拷贝，而在于当意外发生时能否高效、完整、可靠地恢复系统与业务。只有在备份全流程中将细节落实到位，才能真正做到数据无忧，确保企业信息资产的长期安全和业务系统的稳定运行。</p>]]></description></item><item>    <title><![CDATA[给网站选域名怎么选？要考虑哪些方面 德迅云安全_珍珍 ]]></title>    <link>https://segmentfault.com/a/1190000047577712</link>    <guid>https://segmentfault.com/a/1190000047577712</guid>    <pubDate>2026-01-28 14:03:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>为网站选择一个合适的域名是网站成功的关键之一。域名不仅是网站的“门牌号”，也是品牌的代表，直接影响用户的记忆、访问体验以及搜索引擎的优化。下面我会详细讲解如何挑选一个合适的域名，确保它能够为网站带来更多流量和用户。</p><ol><li>选择简短易记的域名</li></ol><p>选择一个简短的域名有助于用户记忆和输入。短域名不仅便于记住，输入时也不容易出错。理想长度推荐域名长度控制在 6-15 个字符之间。避免数字和特殊符号：数字容易混淆(例如 1 和 l)，下划线和连字符也会给用户带来困扰。</p><p>域名应该容易拼写和发音。避免使用难以拼写或多音字的词语。一个简单明了的域名可以帮助你吸引更多的访问者，并且减少因拼写错误导致的流量损失。</p><ol start="2"><li>与品牌或内容相关</li></ol><p>选择与网站的主题、业务或品牌相关的域名。域名应该能准确传达你网站的核心内容或服务，让用户通过域名就能大致了解网站的定位。</p><p>选择一个独特且富有辨识度的域名，避免和其他品牌或网站名字过于相似，以免产生混淆并损害品牌形象。</p><ol start="3"><li>选择合适的域名后缀(TLD)</li></ol><p>域名后缀(TLD)是网址中“dot”后面的部分，最常见的是 .com，它适合全球网站使用，因为它已经是最广为人知的后缀。</p><p>.com：最常用，适合全球任何类型的网站。</p><p>.org：适用于组织和非营利机构。</p><p>.net：通常用于网络服务公司，但如今已经非常普遍。</p><p>.co：适合创始人和创业公司，比较新颖且短小。</p><p>.io：流行于科技公司，尤其是初创公司。</p><p>如果目标用户群体是特定国家或地区，可以选择国家级的域名后缀(如 .cn、.uk、.us 等)。</p><ol start="4"><li>考虑 SEO 优化</li></ol><p>包含目标关键词的域名对 SEO(搜索引擎优化)是有利的，特别是对于新网站，域名中的关键词可以帮助搜索引擎更好地理解网站内容。</p><p>然而，域名中关键词的匹配并不是唯一的SEO排名因素，但合理选择依然有助于提高网站的排名和曝光度。</p><ol start="5"><li>避免版权问题</li></ol><p>在选择域名时，务必确保你所选的域名不会侵犯他人的商标或品牌。如果选择的域名与已有商标过于相似，可能会面临法律诉讼风险。</p><p>为了避免侵权，使用商标搜索工具检查所选域名是否已被注册为商标。</p><ol start="6"><li>检查域名的可用性</li></ol><p>在选定域名后，使用域名注册平台检查该域名是否已被注册。如果该域名已经被他人注册，考虑修改域名或选择一个新的。</p><p>确保你的域名在社交媒体平台上的用户名也是可用的。如果域名与社交媒体账户的用户名一致，可以提高品牌的一致性和知名度。</p><ol start="7"><li>长期考虑和品牌保护</li></ol><p>选择一个长期可用的域名，避免频繁更换域名，这样可以避免影响现有用户和搜索引擎的排名。</p><p>为了保护品牌，可以考虑注册多个相关的域名，并将它们重定向到主网站。这样可以防止竞争对手或恶意用户注册类似的域名。</p><p>选定一个好的域名是成功的网站运营的关键因素之一。</p><p>通过这些方法，你可以为网站选择一个既能代表品牌，又便于用户记忆和访问的好域名。</p>]]></description></item><item>    <title><![CDATA[三大跃迁重塑格局：2026数据治理厂商品牌综合排名与选型攻略 数据工坊 ]]></title>    <link>https://segmentfault.com/a/1190000047577716</link>    <guid>https://segmentfault.com/a/1190000047577716</guid>    <pubDate>2026-01-28 14:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当前，数据治理领域正处于战略升级与价值重塑的关键阶段。一方面，“数字中国”建设的深入推进与数据资产“入表”政策落地，推动数据治理平台从过去的合规工具，快速转向支撑企业数字化转型与价值创造的核心引擎；另一方面，行业整体迈向规模化、智能化与国产化并重的发展格局，市场持续快速增长。据IDC预测，2026年中国数据治理平台市场规模将突破860亿元，年复合增长率保持在29.7%左右。<br/>行业演进集中体现为三大趋势：一是AI深度赋能，自然语言与机器学习技术贯穿治理全流程，实现质量自动监控与智能修复，大幅降低使用门槛；二是信创适配成为刚需，国产软硬件体系在关键行业加速落地，本土厂商凭借生态理解与服务能力占据主导；三是运营向资产化转型，数据从管理对象转向可运营资产，治理平台逐步承担起价值发现、资产入表与数据服务化的重要角色。<br/>在平台选型过程中，构建清晰的评估框架尤为关键。目前多家权威机构从不同角度提供了参考：IDC注重技术底座与AI融合能力；赛迪顾问侧重信创生态适配与合规体系建设；Gartner强调自动化与全生命周期管理；中国软件评测中心则从八大功能维度提供可落地的性能标准。综合来看，企业应立足自身所处行业、数据现状与战略目标，在技术适配性、场景贴合度、安全可控性与价值转化力等维度进行系统评估，选择真正符合长远发展需要的治理平台。<br/>核心厂商竞争力深度解析</p><ol><li>百分点科技百思数据治理平台（AI-DG）<br/>百分点科技作为数据智能领域的领先企业，通过创新的百思数据治理平台（AI-DG）和百思数据治理大模型成功将理念落地，助力众多政企客户激活数据要素潜能，在数字化竞争中构建核心优势。基于对行业场景的深度理解，百分点科技将AI与大模型深度融合，构建了全栈国产化适配、场景驱动的数据治理架构，实现从“治理数据”到“智能数据”的跃迁：<br/>百思数据治理平台（AI-DG）是百分点科技面向AI时代的新一代智能治理平台，以自研的百思数据治理大模型为核心引擎，实现三大核心突破：基于领域专家知识的智能决策体系，实现从数据标准到数据应用的端到端智能治理；创新的对话式交互模式，通过自然语言驱动多智能体协同，完成从业务需求到技术实现的全链路、全流程自动化开发；具备多模态数据治理能力，深度融合文本、图像、音视频等异构数据的理解与分析能力。平台致力于构建智能、高效、可信的数据资产体系，成为推动政企智能化转型的战略级数字基础设施。</li><li>华为云数据治理中心<br/>华为云数据治理中心最大的特色在于其 "安全优先" 的设计理念，从芯片到应用层构建了全栈可信体系。支持国密三级加密、数据脱敏等 23 项安全功能，通过了等保 2.0、ISO27701 等多项认证。<br/>在技术架构上，采用 "存算分离" 模式，与华为 FusionInsight 大数据平台深度协同，特别适合对数据主权有严格要求的政府部门。但其治理功能相对基础，在数据建模、指标管理等方面不如专业工具完善，更多作为华为生态的补充组件存在。</li><li>阿里云数据治理中心<br/>依托阿里云的基础设施优势，该产品在弹性扩展和成本控制方面表现亮眼。其 Serverless 架构可实现资源秒级启停，使中小客户的 IT 投入降低 30%-50%。功能上侧重 "轻量化治理"，通过数据地图、质量监控等模块化设计，降低了操作门槛。但在复杂场景下暴露出局限性：血缘分析仅支持到表级，无法满足高精度追溯需求；数据安全模块缺乏国密算法支持，在政府、金融行业的应用受限。<br/>某电商企业案例显示，其在处理双 11 峰值数据时，需额外采购计算资源才能避免性能瓶颈，这反映出纯云原生架构在极端负载下的韧性不足。</li><li>腾讯云数据治理平台<br/>整合元数据管理、数据质量监控、数据安全管控等核心功能，与腾讯云 TDSQL、COS 等产品深度适配。核心优势在于 “数据安全”，支持细粒度权限管控与数据脱敏，弹性扩展能力强。在互联网服务、游戏、政务等腾讯生态辐射领域具备天然优势，适合需要兼顾安全合规与弹性扩展的企业，尤其适配云上混合部署场景。</li><li>年数据治理的竞争维度已全面升级，单纯的功能堆砌不再是核心竞争力，“技术适配性、场景贴合度、价值转化力” 成为企业选型的关键考量。企业唯有立足自身技术架构、业务需求与长期发展战略，精准匹配平台特色，才能让数据治理真正脱离 “成本中心” 属性，成为驱动业务增长的核心资产。</li><li>联通数科智慧数据治理平台：运营商的网络协同能力<br/>依托联通的通信网络优势，该平台在边缘计算场景中表现独特。支持 5G 边缘节点的数据预处理，特别适合工业物联网、智慧交通等场景。其 "一点接入、全网调度" 的能力，可实现跨地域数据治理的协同管理。<br/>但作为行业解决方案延伸出的产品，其通用性稍弱，在金融、电商等非通信相关领域的案例较少，生态适配性有待提升。</li><li>字节跳动数据治理与开发平台<br/>字节跳动凭借其超大规模数据实践与前沿技术积累，推出了企业级数据治理与开发平台 DataLeap。该平台植根于字节内部日均百万级任务调度、EB级数据处理的实际场景，具备高并发、高可靠、高弹性的平台特性。其核心亮点包括全链路数据治理与开发一体化、智能血缘与影响分析、云原生与多引擎兼容、数据安全与合规增强和协作与知识沉淀。<br/>DataLeap 已服务于字节内部及多个外部行业客户，尤其在应对高并发数据处理、复杂数据链路治理与敏捷数据开发场景中表现突出，适用于中大型企业、互联网公司及正在进行数据中台建设的组织。</li></ol><p>相关问题解答（FAQ）</p><ol><li>数据治理平台主要解决哪些问题？<br/>数据治理平台帮助企业系统化管理数据资源，确保数据的准确性、一致性、安全性与可用性，支持数据标准落地、质量提升、资产梳理与合规管控，为数据分析、业务创新与决策支持奠定可靠基础。</li><li>AI如何提升数据治理的效率和效果？<br/>通过机器学习自动识别数据异常与重复记录，利用自然语言处理解析数据标签与业务含义，实现治理规则的智能推荐与执行，大幅减少人工干预，提升响应速度与治理覆盖度。</li><li>数据治理供应商选型时应优先考虑哪些因素？<br/>需结合自身信息化基础、行业监管要求与发展阶段，重点考察平台的国产化适配能力、AI治理成熟度、数据安全机制、资产运营支持水平以及厂商的行业案例与持续服务能力。</li><li>什么是数据资产化？治理平台在其中起什么作用？<br/>数据资产化是指将数据视为可计量、可运营、可增值的经济资源。治理平台通过确权管理、质量评估、价值计量、分级授权等功能，为数据资源转化为会计资产和可交易标的提供技术与管理支撑。</li><li>对于非技术部门，数据治理平台能带来哪些直接帮助？<br/>业务人员可通过自然语言查询数据、了解数据含义与来源；系统自动监控数据质量，减少因数据问题导致的决策偏差；此外，平台支持的数据服务化输出，能让业务部门更便捷、安全地获取所需数据，推动数据在业务场景中的直接应用。</li></ol>]]></description></item><item>    <title><![CDATA[没有域名可以申请SSL证书吗 逼格高的仙人掌 ]]></title>    <link>https://segmentfault.com/a/1190000047577753</link>    <guid>https://segmentfault.com/a/1190000047577753</guid>    <pubDate>2026-01-28 14:01:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>众所周知，我们可以为域名申请SSL证书，那么没有域名可以申请SSL证书吗?使用IP地址的网站可以申请SSL证书实现HTTPS加密吗？下面我们将详细介绍。</p><h4><strong>没有域名可以申请SSL证书吗?</strong></h4><p>当然可以。没有域名，只有IP地址，可以通过IP地址来申请SSL证书。我们平时访问域名，实质上是在通过访问域名背后的服务器IP地址进入一个网站。因此，对于那些没有域名只有IP地址的网站，可以通过IP地址来申请SSL证书，通常这类证书我们称之为IP SSL证书.IP SSL证书为只能通过IP地址访问的企业解决了其数据传输安全问题，还可帮助用户识别企业网站身份真伪。<br/><img width="723" height="414" referrerpolicy="no-referrer" src="/img/bVdmRTx" alt="" title=""/></p><h4><a href="https://link.segmentfault.com/?enc=xTf4ttJg%2FibVm07Z1KelgQ%3D%3D.5wrSYOl5RE0fEkiXif%2B%2B40cdRXz%2F%2FyTqYwEpb3QJ8Nv1GTQoZblQryl1XRHGu11Z3gjLE%2FoXBzDsLiyCqRr2QR484k2D1dt0SDwwUz6iqzM%3D" rel="nofollow" target="_blank"><strong>IP证书申请流程</strong></a></h4><p>一、打开JoySSL的官方网站，注册一个账号。在注册过程中只需填写基本信息即可。重要的是最后一栏注册码务必填写<strong>230970</strong>才可以获取免费测试公网、内网IP地址HTTPS的资格。</p><p>二、选择IP地址SSL证书并试用，填写相关申请信息，包括IP地址、联系人姓名、联系电话和电子邮箱等。</p><p>三、提交申请后，JoySSL会自动生成CSR，并按照系统提示选择服务器文件验证IP地址所有权。</p><p>四、一旦您的申请通过验证，10分钟左右，JoySSL会生成并签发HTTPS证书。签发后，您在JoySSL的证书管理页面上下载已签发的证书文件。</p><p>五、根据您使用的服务器软件（如Apache、Nginx、IIS等），按照相应的配置指南将证书文件和私钥文件配置到服务器上。</p><p>六、使用浏览器访问您申请证书的IP地址，检查浏览器是否显示绿色的安全锁图标，并且地址栏以“https://”开头。如果一切正常，您应该能够安全地访问该IP地址提供的服务。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-平台化 IT 服务管理 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047577790</link>    <guid>https://segmentfault.com/a/1190000047577790</guid>    <pubDate>2026-01-28 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在大多数企业的数字化进程中，ITSM 系统 与 IT 工单管理系统 的引入，往往始于一个非常现实的需求： 统一入口、减少混乱、提升响应效率。 然而，当组织规模扩大、业务复杂度上升、系统数量激增之后， 单一工具所能承载的价值很快触及上限。<br/><img width="522" height="339" referrerpolicy="no-referrer" src="/img/bVdnNjO" alt="" title=""/></p><p>此时，IT 服务管理面临的已不再是“有没有系统”的问题， 而是系统是否具备平台化能力： 能否整合多类服务、统一服务体验、沉淀治理规则， 并最终演进为支撑全组织运行的服务中台。</p><p><strong><a href="https://link.segmentfault.com/?enc=yVJDrjhOq4dSWAgpdgkf%2FA%3D%3D.VmyzICi89sKT%2BWy7DJvlP4uOVOH3P5TDRivFH0%2FgauJFTEn5EI2OmD8SXltszHgSwdFcV2p8X1K0jLZOec0fevlQ3fgCPnjJ2BsbYnKaSJ0%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a></strong> 将围绕“平台化 IT 服务管理”这一主题， 系统拆解企业从工具堆叠走向服务中台的演进逻辑， 并结合实践经验，解析这一转型过程中常见的误区、关键能力与落地路径。</p><p><strong>为什么“多工具并存”终将走向失控</strong></p><p>在 IT 服务管理早期阶段，工具堆叠几乎是不可避免的结果。 服务台、资产管理、监控、权限管理、协作工具各自独立建设， 在短期内确实能够解决局部问题。</p><p><strong>平台化 IT 服务管理的本质是什么</strong></p><p>平台化并不意味着“一个系统替代所有系统”， 而是通过统一的服务抽象层， 将分散的能力整合为一致、可治理、可扩展的服务体系。</p><p>在平台化 ITSM 模式下，服务不再以“系统”为中心， 而是以“服务对象”和“服务结果”为核心进行组织。 用户无需关心背后涉及多少工具， 只需通过统一入口发起请求并获得结果。</p><p><strong>平台化 IT 服务管理的典型应用场景</strong></p><p>当 ITSM 演进为服务中台，其价值将体现在多个高频场景中。</p><p>例如，在员工入职场景下，服务中台能够自动编排账号创建、 设备配置、权限分配与安全校验， 避免传统人工交接带来的延误与风险。</p><p>在变更管理场景中，平台化 ITSM 可以基于历史数据与风险规则， 动态调整审批路径与控制策略， 而非依赖固定模板。</p><p>这些能力的共同特点在于：将复杂的跨系统操作封装为标准化、可复用的服务能力。</p><p><strong>平台化 IT 服务管理背后的组织与治理模型</strong></p><p>当 IT 服务管理完成从“工具集合”向“平台能力”的转型后， 真正的挑战往往不再来自技术本身，而是组织与治理方式是否能够同步进化。 如果仍然沿用传统的职能割裂式管理模式， 即便拥有再先进的平台，也难以释放其长期价值。</p><p>平台化 IT 服务管理强调的是服务视角下的责任重构。 这意味着 IT 不再只是被动响应请求的执行者， 而是以“服务能力提供者”的身份参与业务运行。</p><p><strong>ServiceDesk Plus 如何支撑 IT 服务中台化建设</strong></p><p>在众多 ITSM 工具中， ServiceDesk Plus 之所以被广泛应用于中大型组织， 正是因为其设计理念并未局限于“工单工具”， 而是围绕平台化与扩展性进行构建。</p><p>通过统一的服务目录、灵活的流程引擎、 低代码业务规则以及丰富的集成能力， ServiceDesk Plus 能够将分散的 IT 能力 逐步整合为一致的服务体验。</p><p>更重要的是，该平台支持在不破坏既有流程的前提下， 逐步引入自动化、治理规则与数据洞察， 非常适合作为服务中台建设的核心枢纽。</p><p><strong>平台化 IT 服务管理是否适合中小企业？</strong></p><p>平台化并非规模专属。 中小企业同样可以从统一入口、流程整合和自动化中受益， 关键在于循序渐进，而非一次性重构。</p><p><strong>平台化 ITSM 是否会增加系统复杂度？</strong></p><p>短期内可能需要一定规划成本， 但长期来看，平台化恰恰是为了解决工具堆叠带来的复杂性问题。</p><p><strong>服务中台是否意味着完全自动化？</strong></p><p>并非如此。 服务中台的目标是“合理自动化”， 在关键节点保留人工决策能力。</p><p><strong>如何判断组织是否已具备平台化条件？</strong></p><p>当组织开始关注服务一致性、 跨系统协同与治理能力时， 通常已经站在平台化转型的起点。</p>]]></description></item><item>    <title><![CDATA[蚂蚁灵波科技正式开源 LingBot-Depth ：让机器人“看清”物理世界 本文系转载，阅读原文
]]></title>    <link>https://segmentfault.com/a/1190000047577560</link>    <guid>https://segmentfault.com/a/1190000047577560</guid>    <pubDate>2026-01-28 13:03:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>具身智能的"视觉突破"：当AI终于学会"看懂"玻璃和镜子</strong></p><p>在人工智能向物理世界迈进的征途中，我们正在见证一个关键瓶颈的突破。</p><p>长期以来，机器人在面对透明玻璃杯、不锈钢器皿、镜面等日常物体时，常常表现得"笨拙"——不是抓取失败，就是直接碰撞。这并非算法不够智能，而是它们的"眼睛"——深度相机，在物理层面就无法准确感知这些材质的距离信息。这一光学物理特性的限制，成为制约具身智能落地的核心痛点之一。</p><p>LingBot-Depth 的开源，代表了业界在解决这一问题上的新思路：<strong>不是单纯依赖硬件升级，而是通过AI模型弥补传感器的先天缺陷</strong>。其核心创新"掩码深度建模"（MDM）技术，让模型学会了从RGB图像的纹理、轮廓等视觉线索中，推断出物体的真实距离——这类似于人类即使闭上一只眼睛，也能凭借经验判断物体远近的能力。</p><p>值得关注的几个技术亮点：</p><ol><li><strong>性能突破</strong>：在深度精度和像素覆盖率上超越业界顶级工业相机，这意味着软件算法首次在某些维度上超越了硬件极限。</li><li><strong>成本优势</strong>：无需更换昂贵传感器，在消费级深度相机上即可实现工业级效果，这对机器人大规模商业化至关重要。</li><li><strong>生态协同</strong>：与奥比中光的战略合作，展现了"芯片级数据+AI算法"的协同路径，这可能成为深度视觉领域的新范式。</li><li><strong>开源策略</strong>：模型、代码、技术报告全部开源，后续还将释放300万对标注数据，这种开放姿态有望加速整个行业的技术迭代。</li></ol><p>从更宏观的视角看，LingBot-Depth 的意义不仅在于解决了一个具体的技术问题，更在于它验证了一条路径：<strong>通过大规模数据训练和多模态融合，AI可以突破传统传感器的物理限制，为具身智能提供更可靠的空间理解能力</strong>。这与当前大模型从语言智能向多模态、具身智能演进的趋势高度契合。</p><p>当然，从实验室到真实世界的应用，仍有诸多挑战：模型的泛化性、实时性、边缘部署的资源消耗等。但至少，我们看到了让机器人真正"看清"物理世界的曙光。</p><hr/><p><em>SegmentFault 思否编辑部</em>  <br/><em>2026年1月</em></p><hr/><p><em>以下内容转载自蚂蚁灵波科技官方公众号。</em></p><p><strong>今天，我们正式开源了 LingBot-Depth 空间感知模型。</strong></p><p><a href="https://link.segmentfault.com/?enc=1Co70JZj%2BKq5b9LtIBcxWw%3D%3D.SfZ7PtB2IZkyFXogprumX2%2F9j3Mry0CCPt3xBEIxCka9HThxvJdbw6TBZOgu136qtzNb0BC91bJCDp%2B47KfdgA%3D%3D" rel="nofollow" target="_blank">点击查看视频</a></p><p>不同于数字世界，具身智能的落地高度依赖物理空间信息，空间智能是其在现实场景落地应用的核心关键，而视觉维度下支撑空间智能的重要桥梁正是距离与尺度（Metric Depth）。基于这一核心需求，空间感知模型 LingBot-Depth 应运而生。</p><p>LingBot-Depth 是一种面向真实场景的深度补全模型，依托奥比中光 Gemini 330 系列双目 3D 相机进行 RGB-Depth 数据采集与效果验证，并基于深度引擎芯片直出的深度数据进行训练与优化，旨在将不完整且受噪声干扰的深度传感器数据转化为高质量、具备真实尺度的三维测量结果，提升环境深度感知与三维空间理解能力，为机器人、自动驾驶汽车等智能终端赋予更精准、更可靠的三维视觉。</p><p>实验结果表明，<strong>本模型在深度精度与像素覆盖率两项核心指标上均超越业界顶级工业级深度相机。</strong>在 NYUv2、ETH3D 等多个基准测试中，LingBot-Depth 在深度补全、单目深度估计及双目匹配任务上均达到当前最优水平，并在无需显式时序建模的情况下保持视频级时间一致性。LingBot-Depth 模型也已通过奥比中光深度视觉实验室的专业认证，在精度、稳定性及复杂场景适应性方面均达到行业领先水平。<br/><img width="723" height="282" referrerpolicy="no-referrer" src="/img/bVdnNfz" alt="640.webp" title="640.webp"/><br/>注解：在最具挑战的稀疏深度补全任务中，LingBot-Depth 性能整体优于现有多种主流模型。（图中数值越低代表性能越好。）</p><p>下游任务验证进一步表明，模型能够在 RGB 与深度两种模态之间学习到对齐的潜在空间表征，从而实现对透明及反光物体的稳定机器人抓取。</p><h4>01技术架构：创新的掩码深度建模范式</h4><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnNfA" alt="640 (1).webp" title="640 (1).webp" loading="lazy"/><br/>在家庭和工业环境中，玻璃器皿、镜面、不锈钢设备等透明和反光物体物体十分常见，但却是机器空间感知的难点。传统深度相机受制于光学物理特性，在面对透明或高反光材质时，往往无法接收有效回波。针对这一行业共性难题，我们研发了<strong>“掩码深度建模”（Masked Depth Modeling，MDM）技术。</strong>训练过程中，我们使用海量 RGB–深度图像对，但刻意遮挡其中一部分深度区域，让模型仅根据 RGB 图像去预测缺失的深度值。随着训练进行，模型逐渐学会建立“外观—几何”之间的对应关系，也就是从“物体看起来像什么”推断“它大概有多远”。</p><p>在涵盖家庭、办公环境、健身房及户外场景的上千万张图像数据上完成训练后，当深度相机传回的数据出现缺失或异常时，LingBot-Depth 模型已能够融合彩色图像（RGB）中的纹理、轮廓及环境上下文信息，对缺失区域进行推断与补全，输出更完整、致密、边缘更清晰的三维深度图。</p><h4>02 核心亮点</h4><p><strong>精准且稳定的相机深度感知</strong></p><p>LingBot-Depth 在传统深度传感器易失效的复杂场景中，仍可输出具备真实尺度的高精度深度结果，包括透明物体、玻璃表面以及高反光材质等极具挑战性的环境。不同于依赖硬件改进的方案，本模型从视觉理解层面弥补传感器缺陷，实现对真实三维结构的可靠恢复。</p><p>除单帧精度优势外，LingBot-Depth 还表现出优异的时间一致性。在无需显式时序建模的情况下，模型即可为视频输入生成稳定、连贯的深度序列，有效避免闪烁与结构跳变问题，为机器人操作、AR/VR 以及动态场景感知等应用提供可靠的连续空间理解能力。<br/><img width="723" height="382" referrerpolicy="no-referrer" src="/img/bVdnNf6" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>卓越的 3D 和 4D 环境感知能力</strong><br/>LingBot-Depth 为下游空间感知任务提供了坚实而通用的基础能力。通过将含噪且不完整的传感器深度优化为干净、稠密且具备真实尺度的三维测量结果，模型显著提升了多种高层视觉任务的稳定性与精度。具体而言，LingBot-Depth 支持：</p><p>更加准确的结构化室内场景建图，并有效提升相机位姿与运动轨迹估计的精度；</p><p>面向机器人学习的可靠 4D 点跟踪能力，在统一的真实尺度空间中同时刻画静态场景几何结构与动态物体运动。这使得系统能够在复杂真实环境中建立一致、连续且可用于决策与交互的空间理解表征。<br/><img width="640" height="355" referrerpolicy="no-referrer" src="/img/bVdnNf7" alt="11.jpg" title="11.jpg" loading="lazy"/></p><p><strong>灵巧抓取操作适用于透明与反光物体</strong><br/>通过在统一潜在空间中联合对齐 RGB 外观信息与深度几何结构，LingBot-Depth 使机器人在以往难以处理的复杂场景中实现稳定可靠的操作能力。基于模型优化后的高质量深度结果及跨模态对齐特征，我们进一步训练了一种基于扩散模型的抓取位姿生成策略，在透明杯、反光金属容器等具有挑战性的物体上取得了较高的抓取成功率。在真实机器人测试中，在透明储物盒等传统传感器难以处理的场景中，LingBot-Depth 通过生成合理的深度估计，成功实现了 50% 的抓握率，突破了技术瓶颈。<br/><img width="723" height="283" referrerpolicy="no-referrer" src="/img/bVdnNfH" alt="640 (2).webp" title="640 (2).webp" loading="lazy"/><br/><a href="https://link.segmentfault.com/?enc=jLMSYVIxHd%2BQc%2F2YgE%2B3ew%3D%3D.5wF3odfb%2Fza2lmd9qh1CZ3FgN4AGh%2FFHRXSVJdAWXJ0c3bOWOeXYBzzhOxuVNUREplCSYsqzOaPnOGeE7VlgLQ%3D%3D" rel="nofollow" target="_blank">点击查看视频</a></p><h4>03 从实验室到落地应用：显著提升消费级深度相机对高难物体的处理效果</h4><p>LingBot-Depth 展现出与现有硬件设备的良好适配性。在不更换更高成本传感器的情况下，模型可提升可靠性并降低系统部署门槛。LingBot-Depth 模型依托奥比中光 Gemini330 系列双目 3D 相机进行效果测试，结果显示：面对透明玻璃、高反射镜面、强逆光以及复杂曲面等极具挑战性的光学场景，搭载 LingBot-Depth 后输出的深度图变得平滑、完整，且物体的轮廓边缘非常锐利，效果优于业内领先 3D 视觉公司 Stereolabs 推出的 ZED Stereo Depth 深度相机。<br/>!<a href="" target="_blank">上传中...</a><img width="723" height="429" referrerpolicy="no-referrer" src="/img/bVdnNfI" alt="640 (3).webp" title="640 (3).webp" loading="lazy"/><br/>注解：搭载 LingBot-Depth 后，奥比中光 Gemini 330 系列在透明及反光场景下深度图的完整性和边缘清晰度明显提升<br/><img width="723" height="448" referrerpolicy="no-referrer" src="/img/bVdnNfJ" alt="640 (4).webp" title="640 (4).webp" loading="lazy"/><br/>注解：奥比中光 Gemini 330 系列相机搭载 LingBot-Depth 后输出的深度图效果优于业界领先的 ZED 深度相机</p><p>这意味着在不更换传感器硬件的前提下，LingBot-Depth 可显著提升消费级深度相机对高难物体的处理效果，降低机器人因深度缺失与噪声引发的抓取失败与碰撞风险。在具身智能、自动驾驶等领域都有一定应用价值，能够极大程度提升具身操作的精准度。</p><p>目前，我们已与奥比中光达成战略合作伙伴关系，将基于 LingBot-Depth 模型推出新一代深度相机，依托 Gemini 330 系列相机提供的芯片级 3D 数据，进一步通过技术协同、生态共建，为机器人处理各行各业极端场景、走向真正落地提供强大的技术支撑。</p><p>LingBot-Depth 已成功实现模型轻量化与端侧部署，具备在边缘计算设备上高效运行的能力。未来，我们期待通过开源开放与生态合作，和广大合作伙伴一起加速具身智能在家庭、工业、物流等复杂场景的大规模应用落地。</p><p>目前我们的模型、代码、技术报告已全部开源，欢迎大家访问我们的开源仓库。</p><pre><code>Website：
https://technology.robbyant.com/lingbot-depth

Model：
https://huggingface.co/robbyant/lingbot-depth

Code：
https://github.com/Robbyant/lingbot-depth

Tech Report：
https://github.com/Robbyant/lingbot-depth/blob/main/tech-report.pdf</code></pre><p>后续我们还将开源 300 万对精心标注的 RGB-深度数据，包括 200 万对实拍 RGB-D 样本，和 100 万对渲染样本，推动空间感知技术的开源生态建设和技术创新。</p><p>LingBot-Depth 的开源标志着我们在空间智能领域迈出的第一步。本周，我们还将陆续为大家带来我们在具身智能领域智能基座方向的更多成果，我们期待与全球开发者、研究者、产业伙伴一起，共同探索具身智能的上限。<br/><img width="723" height="693" referrerpolicy="no-referrer" src="/img/bVdnNf9" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[蚂蚁灵波科技全面开源 LingBot-VLA 具身大模型 本文系转载，阅读原文
https://mp]]></title>    <link>https://segmentfault.com/a/1190000047577568</link>    <guid>https://segmentfault.com/a/1190000047577568</guid>    <pubDate>2026-01-28 13:03:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>思否编辑部观察</strong></p><p>具身智能正在经历从实验室走向产业化的关键转折点。长期以来,机器人操控模型面临着"一机一训"的困境——每换一个机器人本体、每增加一个新任务,都需要重新采集数据、重新训练模型,这种高昂的迁移成本严重制约了具身智能的规模化落地。</p><p>此次蚂蚁集团开源的 LingBot-VLA 具身大模型,为行业带来了三个重要突破:</p><p><strong>1. 首次验证了具身智能领域的 Scaling Law</strong>  <br/>通过 20,000 小时真实机器人数据的预训练,系统性证明了 VLA 模型性能随数据规模持续提升的规律。这一发现意义重大——它表明具身智能可以像大语言模型一样,通过"大数据+大模型"的范式实现能力跃迁,为行业指明了清晰的技术路线。</p><p><strong>2. 解决了跨本体泛化的核心难题</strong>  <br/>通过涵盖 9 种主流双臂机器人构型的大规模预训练,LingBot-VLA 实现了"一个大脑,多种身体"的愿景。在 GM-100 真机评测中,其跨本体泛化成功率达到 17.3%,这意味着同一个模型可以快速适配不同厂商的机器人硬件,大幅降低了商业化部署的门槛。</p><p><strong>3. 打造了真正实用的开源生态</strong>  <br/>不同于许多"只开源权重"的项目,LingBot-VLA 同步开放了数据处理、高效微调、自动化评估的全套工具链,训练效率达到主流框架的 1.5~2.8 倍。这种"开箱即用"的完整方案,将帮助开发者以更低成本快速落地自己的具身智能应用。</p><p>特别值得关注的是,LingBot-VLA 引入深度信息后的性能提升,体现了空间感知能力对机器人操控的重要性。结合昨日开源的 LingBot-Depth 模型,我们看到了一个清晰的技术演进路径:从精准的空间感知到智能的操控决策,具身智能正在构建起完整的"感知-认知-执行"闭环。</p><p>随着蚂蚁集团承诺未来几天将陆续开源更多具身智能成果,我们有理由相信,2026 年将成为具身智能从"能用"到"好用"、从"实验室"到"生产线"的关键转折年。</p><p><em>SegmentFault 思否编辑部</em>  <br/><em>2026年1月</em></p><hr/><p><em>以下内容转载自蚂蚁灵波科技官方公众号。</em></p><p><strong>继昨日开源高精度空间感知模型 LingBot-Depth 后，今天，我们为大家带来了具身大模型 LingBot-VLA。</strong></p><p><a href="https://link.segmentfault.com/?enc=fOQG9lDk6H%2FI0IYwEOKkvQ%3D%3D.VOPaUGQQtQQlFFSdSIHYCwu2R6vCa1JrU2FNa9oFyfyvDE3EWcccn9gmfwJVP0Q67Ogn5Wg41i09kCtTR9%2BpxA%3D%3D" rel="nofollow" target="_blank">LingBot-VLA 具身大模型全面开源</a></p><p>在上海交通大学开源的具身评测基准 GM-100（包含 100 项真实操作任务）测试中，LingBot-VLA 在 3 个不同的真实机器人平台上，跨本体泛化平均成功率相较于 Pi0.5 的 13.0% 提升至 15.7%（w/o Depth）。引入深度信息（w/ Depth）后，空间感知能力增强，平均成功率进一步攀升至 17.3%，展现了 LingBot-VLA 强大的准确性和泛化性。</p><p><img width="723" height="585" referrerpolicy="no-referrer" src="/img/bVdnNge" alt="640.webp" title="640.webp"/></p><p>在 GM-100 真机评测中，LingBot-VLA 跨本体泛化性能领先</p><p>在 RoboTwin 2.0 仿真基准（包含50项任务）评测中，面对高强度的环境随机化干扰（如光照、杂物、高度扰动），LingBot-VLA 凭借独特的可学习查询对齐机制，高度融合深度信息，操作成功率比 Pi0.5 提升了 9.92%，实现了从虚拟仿真到真实落地的全方位性能领跑。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnNgf" alt="640 (1).webp" title="640 (1).webp" loading="lazy"/></p><p>在 RoboTwin 2.0 仿真评测中，LingBot-VLA 跨任务泛化性能领先</p><h4>01 Scaling Law 下的大规模真机数据预训练</h4><p>长期以来，由于本体差异、任务差异、环境差异等，具身智能模型落地面临严重的泛化性挑战。开发者往往需要针对不同硬件和不同任务重复采集大量数据进行后训练，直接抬高了落地成本，也使行业难以形成可规模化复制的交付路径。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047577570" alt="图片" title="图片" loading="lazy"/><br/>针对上述问题，我们基于在海量真实世界数据上的预训练，第一次系统研究了 VLA 模型在真实机器人任务性能上随着数据规模增长时的 Scaling Law。项目发现随着预训练数据规模从 3,000 小时扩展到 6,000、13,000、18,000，最终至 20,000 小时，模型在下游任务的成功率获得持续且显著的提升。值得注意的是，预训练数据量达到 20,000 小时时，模型性能仍呈现上升趋势，表明 VLA 的性能仍然能够随着数据量的增加而提升。这些实验结果证明了 VLA 模型在用真实数据预训练时呈现了良好的可扩展性，为未来的 VLA 开发和大规模数据挖掘提供了重要启示。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047577571" alt="图片" title="图片" loading="lazy"/><br/>依此研究结果，我们仔细构造了 20,000 小时的真实机器人训练数据，涵盖了 9 种主流的双臂机器人构型（包括 AgileX Cobot Magic，Galaxea R1Pro、R1Lite 、AgiBot G1等）。为了进行精确的数据标注，数据里的视频由人工标注者按原子动作进行切分，并用大模型标注视频对应任务和子任务。在 codebase 的开发中，适配了 Fully Sharded Data Parallel (FSDP) 分布式、混合精度、算子融合等优化，从而让同一个“大脑”可以快速迁移至不同形态的机器人上，并在任务变化、环境变化时保持可用的成功率与鲁棒性。</p><h4>02 深度信息辅助的机器人操控性能提升</h4><p><img width="723" height="204" referrerpolicy="no-referrer" src="/img/bVdnNgg" alt="640 (2).webp" title="640 (2).webp" loading="lazy"/><br/>仿真实验结果</p><p>为了显式捕捉操控环境中的空间感知能力，并进一步提升机器人执行的鲁棒性，我们采用了一种基于查询向量（query）的深度蒸馏方法。具体而言，我们引入了与三视角操作图像相对应的可学习 queries，这些 queries 经 VLM 处理后，与 LingBot-Depth 输出的 depth embeddings 进行对齐。这种对齐机制在维持模型训练与推理的效率的同时，有效将深度信息集成到 LingBot-VLA 中。在真实机器人平台和仿真环境下进行的广泛实验证明，深度信息的融入提升了 LingBot-VLA 的操控性能。</p><h4>03 后训练成本低、效率高、代码全开源，真正实用的 VLA 模型</h4><p>得益于涵盖主流构型和详尽任务的大规模预训练，LingBot-VLA 具备强大的通用操控能力，并且能够将其高效迁移到多样的下游机器人任务中。实验表明，LingBot-VLA 在下游任务中能够使用更少的数据，达到超越 π0.5 的性能；并且性能优势会随着数据量的增加而持续扩大。目前，LingBot-VLA 已与星海图、松灵、乐聚等知名机器人厂商完成适配，验证了模型在不同构型机器人上的跨本体迁移能力。<br/><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdnNgr" alt="640 (3).webp" title="640 (3).webp" loading="lazy"/></p><p>与此同时，我们构建了一套高效的后训练工具链，在 8 卡 GPU 配置下实现了单卡每秒 261 个样本的吞吐量，其训练效率达到 StarVLA、OpenPI 等主流框架的 1.5~2.8 倍，实现了数据与算力成本的双重降低。此次开源，我们不仅提供了模型权重，还同步开放了包含数据处理、高效微调及自动化评估在内的全套代码库。我们希望这一举措可以大幅压缩模型训练周期，降低商业化落地的算力与时间门槛，助力开发者以更低成本快速适配自有场景，提升模型实用性。目前我们的模型、后训练代码、技术报告、以及我们和上海交大共同打造的 GM-100 Benchmark 已全部开源，欢迎大家访问我们的开源仓库。</p><pre><code>Website：
https://technology.robbyant.com/lingbot-vla

Model：
https://huggingface.co/collections/robbyant/lingbot-vla
https://www.modelscope.cn/collections/Robbyant/LingBot-VLA

Datasets:
https://huggingface.co/datasets/robbyant/lingbot-GM-100

Code:
https://github.com/Robbyant/lingbot-vla

Tech Report:
https://arxiv.org/abs/2601.18692</code></pre><p>具身智能的大规模应用依赖高效的具身大模型，这直接决定了模型是否可用以及能否用得起。我们希望通过 LingBot-VLA 的开源，积极探索具身智能上限，推进具身智能研发早日进入可复用、可验证、可规模化落地的新阶段。</p><p>本周，我们已相继开源 LingBot-Depth 和 LingBot-VLA 两款模型，未来几天，我们还将陆续为大家带来我们在具身智能领域智能基座方向的更多成果。我们期待与全球开发者、研究者、产业伙伴一起，加速具身智能技术的迭代与规模化应用，助力 AGI 更快到来。</p><p><img width="723" height="702" referrerpolicy="no-referrer" src="/img/bVdnNgh" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item>  </channel></rss>