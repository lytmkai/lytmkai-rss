<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[Cherry Studio API完整参考手册（实操版） 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047610615</link>    <guid>https://segmentfault.com/a/1190000047610615</guid>    <pubDate>2026-02-14 11:06:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Cherry Studio API完整参考手册（实操版）</h2><p>Cherry Studio 是一款功能强大的桌面客户端，核心优势在于支持多类大语言模型（LLM）提供商接入，为开发者提供统一的API接口，无需单独适配不同AI服务的接口规范，大幅降低集成成本。本文基于Cherry Studio最新版本，整合完整API参考、安装配置、实操示例、错误处理及最佳实践，全程贴合开发者实际使用场景，既是入门指南，也是日常开发的实用参考手册。</p><p>本文档适配所有支持Cherry Studio的操作系统（Windows、Mac、Linux），所有代码示例可直接复制使用，仅需替换对应密钥及参数，适合各类技术水平的开发者快速上手。</p><h3>一、概述与核心功能特性</h3><h4>1.1 核心定位</h4><p>Cherry Studio 作为桌面客户端，核心价值是“统一AI服务入口”——开发者通过其提供的标准化API，可快速访问DeepSeek、OpenAI、Anthropic等多家厂商的大语言模型，无需关注不同厂商接口的差异，实现“一次集成，多模型复用”。</p><p>项目地址：<a href="https://link.segmentfault.com/?enc=IKUk%2Fs4ZYBKLapZEZYZUqA%3D%3D.zPzzwoccX8ssY2D3wM67QuEhp1LX6I9uc%2FfsgubMydMP0opEJoCHH7%2BanPMM5713tpHobSW1xTiYknfwKI%2F%2Bfw%3D%3D" rel="nofollow" target="_blank">https://gitcode.com/GitHub_Trending/ch/cherry-studio</a>，可从该地址获取最新版本客户端及源码。</p><h4>1.2 核心功能特性（附支持状态）</h4><p>Cherry Studio 的功能模块按“已支持”“开发中”分类，开发者可根据需求规划集成方案，具体如下表所示：</p><p>| 功能模块 | 支持状态 | 详细描述 |</p><table/><p>| 多LLM提供商集成 | ✅ 已支持 | 提供统一API接口，可接入DeepSeek、OpenAI、Anthropic等主流厂商，无需单独适配 |</p><p>| DeepSeek-R1支持 | ✅ 已支持 | 深度集成DeepSeek-R1模型，可直接通过API调用该模型完成对话生成等操作 |</p><p>| 对话管理 | ✅ 已支持 | 自动维护多轮对话上下文，无需开发者手动处理会话记忆，提升开发效率 |</p><p>| 流式响应 | ✅ 已支持 | 支持实时流式文本生成，可实现“边生成边展示”的交互效果，优化用户体验 |</p><p>| 文件处理 | 🔄 开发中 | 后续将支持文档上传、解析与分析功能，目前暂不开放相关API |</p><p>| 插件系统 | 🔄 开发中 | 将支持插件扩展，开发者可通过插件新增功能，目前暂不支持自定义插件集成 |</p><h3>二、快速开始：安装与基础配置（3步上手）</h3><p>Cherry Studio 的安装与配置流程极简，无需复杂环境依赖，完成以下3步即可启动服务并调用API，适合新手快速入门。</p><h4>2.1 步骤1：安装Cherry Studio桌面客户端</h4><ol><li>访问Cherry Studio官方项目地址或官方网站，下载对应操作系统的最新版本客户端（Windows可下载exe安装包，Mac可下载dmg镜像）；</li><li>双击安装包，按照引导完成安装（默认安装路径即可，无需手动修改）；</li><li>安装完成后，桌面将生成快捷方式，双击启动客户端，进入服务配置环节。</li></ol><h4>2.2 步骤2：启动服务（核心指令）</h4><p>启动Cherry Studio服务需通过命令行执行指令，指定端口和API密钥（自定义密钥，用于后续API认证），具体指令如下：</p><pre><code class="bash">
cherry-studio start --port 8080 --api-key your-api-key
</code></pre><p>参数说明：</p><ul><li>--port 8080：指定服务运行端口，默认端口为8080，若该端口被占用，可替换为其他空闲端口（如8081、9000）；</li><li>--api-key your-api-key：自定义API认证密钥，后续调用所有API时需携带该密钥，建议设置复杂密钥，提升安全性；</li><li>补充：若启动失败，可检查端口是否被占用，或重新安装客户端重试。</li></ul><h4>2.3 步骤3：基础请求示例（JavaScript）</h4><p>服务启动后，可通过简单的API请求测试服务可用性，以下是JavaScript语言的基础聊天请求示例，可直接复制使用（替换your-api-key即可）：</p><pre><code class="javascript">
// JavaScript示例：调用聊天补全接口

const API_BASE = 'http://localhost:8080/api/v1';



async function chatWithAI(message) {

  const response = await fetch(`${API_BASE}/chat/completions`, {

    method: 'POST',

    headers: {

      'Content-Type': 'application/json',

      'Authorization': 'Bearer your-api-key' // 替换为启动服务时设置的API密钥

    },

    body: JSON.stringify({

      model: 'deepseek-r1', // 指定调用的模型，此处以DeepSeek-R1为例

      messages: [{ role: 'user', content: message }], // 对话内容，user表示用户输入

      stream: false // 是否开启流式响应，false表示同步响应

    })

  });



  return await response.json(); // 返回AI响应结果

}



// 调用函数测试

chatWithAI('Hello, Cherry Studio!')

  .then(result =&gt; console.log('AI响应：', result))

  .catch(error =&gt; console.error('请求失败：', error));
</code></pre><p>运行说明：将上述代码保存为.js文件，通过node命令运行，若控制台输出AI响应结果，说明服务启动正常，API可正常调用。</p><h3>三、核心API端点参考（重点必看）</h3><p>Cherry Studio 提供标准化API端点，所有请求均需携带认证信息，核心支持3类常用端点：聊天补全、模型列表、流式聊天，以下是详细参考（含请求参数、响应结构）。</p><h4>3.1 通用认证方式</h4><p>所有API请求都必须在请求头（Header）中包含认证信息，否则将返回401认证失败错误，认证格式如下：</p><pre><code class="http">
Authorization: Bearer your-api-key
</code></pre><p>说明：your-api-key 即为启动服务时设置的API密钥，大小写敏感，请勿泄露或误写。</p><h4>3.2 端点1：聊天补全接口（最常用）</h4><p>用于调用指定模型生成对话响应，支持多轮对话，适用于大部分聊天、问答类场景。</p><ul><li>端点地址：POST /api/v1/chat/completions</li><li>请求方式：POST</li><li>请求参数（JSON格式，必填参数标注）：</li></ul><pre><code class="json">
{

  "model": "string (必填)", // 模型ID，如deepseek-r1、gpt-4

  "messages": [ // 对话历史，必填，支持多轮

    {

      "role": "system|user|assistant", // 角色，system（系统提示）、user（用户）、assistant（AI）

      "content": "string" // 角色对应的内容

    }

  ],

  "temperature": 0.7, // 可选，生成多样性，0-1之间，值越大越随机

  "max_tokens": 2048, // 可选，最大生成 tokens 数，默认2048

  "top_p": 1.0, // 可选，采样阈值，0-1之间，无需修改默认值即可

  "stream": false, // 可选，是否开启流式响应，默认false

  "provider": "deepseek|openai|anthropic" // 可选，指定模型提供商，默认自动匹配模型所属厂商

}
</code></pre><ul><li>响应结构（JSON格式，成功返回）：</li></ul><pre><code class="json">
{

  "id": "chatcmpl-123", // 对话ID，唯一标识

  "object": "chat.completion", // 对象类型，固定值

  "created": 1677652288, // 创建时间戳（秒）

  "model": "deepseek-r1", // 调用的模型ID

  "choices": [ // 响应结果列表，默认返回1个结果

    {

      "index": 0, // 索引，默认0

      "message": {

        "role": "assistant", // 角色，固定为assistant

        "content": "Hello! How can I help you today?" // AI生成的响应内容

      },

      "finish_reason": "stop" // 结束原因，stop表示正常结束

    }

  ],

  "usage": { // tokens 消耗统计

    "prompt_tokens": 9, // 提示词 tokens 数

    "completion_tokens": 12, // 生成内容 tokens 数

    "total_tokens": 21 // 总 tokens 数

  }

}
</code></pre><h4>3.3 端点2：模型列表接口</h4><p>用于查询当前Cherry Studio支持的所有模型列表，可快速获取模型ID、所属厂商等信息，方便开发者选择合适的模型。</p><ul><li>端点地址：GET /api/v1/models</li><li>请求方式：GET</li><li>请求参数：无（仅需携带认证头）</li><li>响应示例（JSON格式）：</li></ul><pre><code class="json">
{

  "object": "list", // 对象类型，固定为list

  "data": [

    {

      "id": "deepseek-r1", // 模型ID，调用时需填写该值

      "object": "model", // 子对象类型，固定为model

      "created": 1677652288, // 模型添加时间戳

      "owned_by": "deepseek", // 模型所属厂商

      "permissions": [], // 权限列表，默认空

      "root": "deepseek-r1", // 根模型ID，与id一致

      "parent": null // 父模型，无父模型则为null

    },

    {

      "id": "gpt-4",

      "object": "model",

      "created": 1677652288,

      "owned_by": "openai",

      "permissions": [],

      "root": "gpt-4",

      "parent": null

    }

  ]

}
</code></pre><h4>3.4 端点3：流式聊天接口</h4><p>与普通聊天补全接口功能一致，核心区别在于开启流式响应（stream=true），可实现实时返回生成内容，适用于需要即时交互的场景（如在线聊天界面）。</p><ul><li>端点地址：POST /api/v1/chat/completions（与普通聊天接口一致，通过stream参数区分）</li><li>请求方式：POST</li><li>核心差异：请求参数中stream设为true，其他参数与普通聊天接口一致；</li><li>使用示例（JavaScript）：</li></ul><pre><code class="javascript">
async function streamChat(message) {

  const response = await fetch(`${API_BASE}/chat/completions`, {

    method: 'POST',

    headers: {

      'Content-Type': 'application/json',

      'Authorization': 'Bearer your-api-key'

    },

    body: JSON.stringify({

      model: 'deepseek-r1',

      messages: [{ role: 'user', content: message }],

      stream: true // 开启流式响应

    })

  });



  const reader = response.body.getReader();

  const decoder = new TextDecoder();



  while (true) {

    const { done, value } = await reader.read();

    if (done) break; // 流式响应结束，退出循环



    const chunk = decoder.decode(value);

    const lines = chunk.split('\n'); // 按行解析流式数据



    for (const line of lines) {

      if (line.startsWith('data: ')) {

        const data = line.slice(6); // 截取data: 后面的内容

        if (data === '[DONE]') break; // 接收完毕，退出循环



 const parsed = JSON.parse(data);

        // 打印实时生成的内容（可替换为页面渲染逻辑）

        console.log(parsed.choices[0].delta.content || '');

      }

    }

  }

}



// 调用流式聊天函数

streamChat('请详细介绍Cherry Studio的核心功能');
</code></pre><h3>四、配置管理（config.yaml + 环境变量）</h3><p>Cherry Studio 支持通过配置文件和环境变量两种方式管理服务参数，可根据实际部署需求灵活配置，以下是详细说明。</p><h4>4.1 配置文件结构（config.yaml）</h4><p>配置文件位于Cherry Studio安装目录下，名为config.yaml，可手动修改参数，重启服务后生效，核心结构如下（含注释说明）：</p><pre><code class="yaml">
# config.yaml 核心配置

api:

  port: 8080 # 服务端口，与启动指令中的--port一致，指令优先级高于配置文件

  cors_origins: ["http://localhost:3000"] # 允许跨域的地址，前端项目需添加对应地址

  rate_limit: 1000 # 接口请求频率限制，每分钟最多1000次请求

providers: # 模型提供商配置，需填写对应厂商的API密钥

  deepseek:

    api_key: ${DEEPSEEK_API_KEY} # 环境变量引用，也可直接填写密钥字符串

    base_url: "https://api.deepseek.com" # DeepSeek接口地址，无需修改



  openai:

    api_key: ${OPENAI_API_KEY} # OpenAI API密钥，需自行申请



  anthropic:

    api_key: ${ANTHROPIC_API_KEY} # Anthropic API密钥，需自行申请

logging: # 日志配置

  level: "info" # 日志级别，可选info、warn、error、debug

  file: "cherry-studio.log" # 日志文件名称，默认存储在安装目录下
</code></pre><h4>4.2 环境变量配置</h4><p>为避免密钥明文暴露在配置文件中，推荐通过环境变量配置各类密钥，Cherry Studio支持的环境变量如下，可根据需要配置：</p><p>| 环境变量名称 | 详细描述 | 默认值 |</p><table/><p>| CHERRY_API_KEY | Cherry Studio服务认证密钥（启动服务时的--api-key） | 无（必填） |</p><p>| DEEPSEEK_API_KEY | DeepSeek模型提供商的API密钥，需自行申请 | 无 |</p><p>| OPENAI_API_KEY | OpenAI模型提供商的API密钥，需自行申请 | 无 |</p><p>| ANTHROPIC_API_KEY | Anthropic模型提供商的API密钥，需自行申请 | 无 |</p><p>| CHERRY_PORT | Cherry Studio服务运行端口，与启动指令--port一致 | 8080 |</p><p>| CHERRY_LOG_LEVEL | 日志级别，可选info、warn、error、debug | info |</p><h3>五、错误处理与故障排除（避坑必看）</h3><p>开发过程中调用API可能出现各类错误，本节整理了常见错误代码、响应格式及故障排查方法，帮助开发者快速定位并解决问题。</p><h4>5.1 错误响应格式</h4><p>所有API错误均返回统一格式的JSON响应，包含错误代码、错误信息和错误类型，便于解析处理：</p><pre><code class="json">
{

  "error": {

    "code": "invalid_api_key", // 错误代码，关键定位依据

    "message": "Invalid API key provided", // 错误详细信息

    "type": "invalid_request_error" // 错误类型

  }

}
</code></pre><h4>5.2 常见错误代码及解决方案</h4><p>| 错误代码 | HTTP状态码 | 错误描述 | 解决方案 |</p><table/><p>| invalid_api_key | 401 | API密钥无效或未携带 | 1. 检查请求头中Authorization是否携带正确密钥；2. 确认密钥与启动服务时的--api-key一致；3. 检查密钥大小写是否正确 |</p><p>| rate_limit_exceeded | 429 | 请求频率超过限制 | 1. 优化代码，减少请求频率；2. 调整config.yaml中的rate_limit参数（需重启服务）；3. 实现请求重试机制，避免集中请求 |</p><p>| model_not_found | 404 | 指定的模型不存在 | 1. 调用GET /api/v1/models接口，查询支持的模型ID；2. 确认模型ID填写正确，无拼写错误；3. 检查模型提供商是否已配置API密钥 |</p><p>| provider_unavailable | 503 | 模型提供商服务不可用 | 1. 检查对应厂商的API密钥是否有效；2. 访问厂商官网，确认服务是否正常；3. 切换其他可用模型 |</p><p>| invalid_parameters | 400 | 请求参数格式错误或缺失必填参数 | 1. 检查请求参数是否完整（如model、messages是否填写）；2. 确认参数格式符合JSON规范；3. 检查参数值是否符合要求（如temperature范围0-1） |</p><h4>5.3 常见故障排查方法</h4><h5>故障1：服务启动失败</h5><p>可能原因：端口被占用、客户端安装不完整、命令行指令错误；</p><p>解决方案：1. 更换服务端口（--port 8081）；2. 重新安装Cherry Studio客户端；3. 检查指令拼写，确保指令格式正确。</p><h5>故障2：API请求连接超时</h5><p>可能原因：服务未启动、网络连接异常、防火墙拦截端口；</p><p>解决方案：1. 确认Cherry Studio服务已正常启动；2. 检查本地网络连接，确保可访问localhost；3. 关闭防火墙或开放对应服务端口。</p><h5>故障3：流式响应无返回</h5><p>可能原因：stream参数未设为true、模型响应过慢、代码解析逻辑错误；</p><p>解决方案：1. 确认请求参数中stream: true；2. 更换响应速度较快的模型（如deepseek-r1）；3. 检查流式解析代码，确保未遗漏data: 前缀的处理。</p><h5>故障4：日志分析方法</h5><p>可通过日志文件定位详细错误信息，常用日志操作指令（bash）：</p><pre><code class="bash">
# 查看实时日志（实时监控服务运行状态）

tail -f cherry-studio.log



# 搜索错误日志（定位具体错误原因）

grep "ERROR" cherry-studio.log



# 监控API性能（查看API调用耗时）

grep "API_CALL" cherry-studio.log | awk '{print $NF}'
</code></pre><h3>六、最佳实践（提升开发效率与稳定性）</h3><p>结合实际开发场景，整理3个核心最佳实践，帮助开发者优化API调用逻辑，提升系统稳定性和开发效率。</p><h4>6.1 实践1：连接池管理（避免频繁创建连接）</h4><p>频繁创建和关闭HTTP连接会降低性能，建议使用连接池管理请求连接，以下是Node.js环境的连接池示例：</p><pre><code class="javascript">
// 建议使用连接池避免频繁创建连接

const { Pool } = require('pg');



const apiPool = new Pool({

  connectionString: process.env.DATABASE_URL,

  max: 20, // 最大连接数

  idleTimeoutMillis: 30000, // 空闲连接超时时间（30秒）

  connectionTimeoutMillis: 2000, // 连接超时时间（2秒）

});
</code></pre><h4>6.2 实践2：请求重试机制（提升接口稳定性）</h4><p>面对网络波动、服务临时不可用等场景，实现重试机制可提升接口调用成功率，以下是通用重试函数示例：</p><pre><code class="javascript">
async function withRetry(fn, maxRetries = 3) {

  for (let i = 0; i &lt; maxRetries; i++) {

    try {

      return await fn(); // 执行API请求函数

    } catch (error) {

      if (i === maxRetries - 1) throw error; // 最后一次重试失败，抛出错误

      // 指数退避重试，每次重试间隔翻倍（1秒、2秒、4秒...）

      await new Promise(resolve =&gt; setTimeout(resolve, 1000 * Math.pow(2, i)));

    }

  }

}



// 使用示例：调用聊天接口并添加重试机制

withRetry(() =&gt; chatWithAI('查询今天的天气'))

  .then(result =&gt; console.log(result))

  .catch(error =&gt; console.error('最终请求失败：', error));
</code></pre><h4>6.3 实践3：性能监控（实时掌握API运行状态）</h4><p>添加API调用性能监控，可实时掌握接口响应时间、成功率等指标，便于及时优化，示例如下：</p><pre><code class="javascript">
// 添加性能监控

const startTime = Date.now();

try {

  const result = await chatWithAI(message);

  const duration = Date.now() - startTime; // 计算API调用耗时



  console.log(`API调用耗时: ${duration}ms`);

  monitor.recordApiCall('success', duration); // 记录成功调用（需自行实现monitor）

} catch (error) {

  monitor.recordApiCall('error', Date.now() - startTime); // 记录失败调用

  throw error;

}
</code></pre><h3>七、扩展功能：WebSocket实时通信与自定义提供商</h3><h4>7.1 WebSocket实时通信</h4><p>Cherry Studio 支持WebSocket连接，可实现更高效的实时通信（比流式响应更轻量），适用于实时聊天、消息推送等场景，核心使用示例如下：</p><pre><code class="javascript">
// WebSocket连接建立

const ws = new WebSocket('ws://localhost:8080/ws/chat');



ws.onopen = () =&gt; {

  console.log('WebSocket连接已建立');

  // 连接成功后，发送认证信息

  ws.send(JSON.stringify({

    type: 'auth',

    api_key: 'your-api-key'

  }));

};



ws.onmessage = (event) =&gt; {

  const data = JSON.parse(event.data);

  // 接收AI返回的消息

  if (data.type === 'message') {

    console.log('收到消息:', data.content);

  }

};



// 发送聊天消息

ws.send(JSON.stringify({

  type: 'message',

  model: 'deepseek-r1',

  content: 'Hello, WebSocket!'}

));
</code></pre><p>WebSocket消息格式（JSON）：</p><pre><code class="json">
{

  "type": "message|error|ping|pong", // 消息类型

  "content": "消息内容", // 消息主体

  "timestamp": 1677652288, // 时间戳

 "message_id": "msg_123" // 消息唯一ID

}
</code></pre><h4>7.2 自定义提供商集成</h4><p>若需接入Cherry Studio未默认支持的模型提供商，可通过自定义提供商功能实现，核心示例如下（JavaScript）：</p><pre><code class="javascript">
// 自定义模型提供商类

class CustomProvider {

  constructor(config) {

    this.config = config; // 配置信息（如API密钥、接口地址）

  }



  // 实现聊天补全逻辑

  async chatCompletions(params) {

    // 自定义调用逻辑（对接第三方模型接口）

    return {

      id: `chatcmpl-${Date.now()}`,

      choices: [{

        message: {

          role: 'assistant',

          content: 'Custom response' // 自定义响应内容

        }

      }]

    };

  }

}



// 注册自定义提供商到Cherry Studio

cherryStudio.registerProvider('custom', CustomProvider);
</code></pre><p>说明：自定义提供商需实现chatCompletions方法，确保返回格式与Cherry Studio标准响应一致，注册后即可通过provider: "custom"调用该提供商的模型。</p><h3>八、版本历史与技术支持</h3><h4>8.1 版本历史</h4><p>| 版本号 | 发布日期 | 主要变更 |</p><table/><p>| v1.0.0 | 2024-01-15 | 初始版本发布，支持基础聊天补全、模型列表接口 |</p><p>| v1.1.0 | 2024-02-01 | 新增流式响应支持，优化API响应速度 |</p><p>| v1.2.0 | 2024-03-15 | 多提供商集成优化，新增WebSocket实时通信 |</p><h4>8.2 技术支持</h4><p>如需技术支持或报告问题，请提供以下信息，以便快速定位并解决问题：</p><ul><li>Cherry Studio 版本号（可通过客户端关于页面查看）；</li><li>操作系统和运行环境信息（如Windows 11、Node.js 16.x）；</li><li>详细的错误日志（可从cherry-studio.log中提取）；</li><li>问题复现步骤（清晰描述如何操作出现的问题）。</li></ul><p>注意：本文档基于Cherry Studio最新版本编写，API接口可能随版本更新而变化。建议定期查看官方项目地址，获取最新文档和版本更新信息。</p><h3>九、总结</h3><p>Cherry Studio 的核心价值在于“统一AI接口入口”，通过标准化的API设计，帮助开发者快速集成多厂商大语言模型，无需关注不同厂商接口的差异，大幅降低开发成本。本文档覆盖了从安装配置、API调用、错误处理到最佳实践的全流程内容，所有代码示例可直接复制使用，适合各类开发者快速上手。</p><p>随着Cherry Studio的不断更新，后续将支持文件处理、插件系统等更多功能，开发者可持续关注官方动态，充分发挥其在AI集成开发中的优势，提升开发效率和产品体验。</p><p>本文由<a href="https://link.segmentfault.com/?enc=UWiXHL9M3v0dH2CMhJbwHg%3D%3D.KswCzyVJqytYbXNnuGQ%2Bfh7u59tqTKgTi4P8l7t4Of0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Cherry Studio对接OpenClaw实操资料（含配置+启动+成本控制） 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047610671</link>    <guid>https://segmentfault.com/a/1190000047610671</guid>    <pubDate>2026-02-14 11:05:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Cherry Studio对接OpenClaw实操资料（含配置+启动+成本控制）</p><h2>Cherry Studio对接OpenClaw实操资料（含配置+启动+成本控制）</h2><p>本文档聚焦Cherry Studio与OpenClaw的完整对接流程，整合环境准备、配置步骤、启动排查、模型适配及Token成本控制核心要点，全程贴合开发者实际操作场景，所有代码示例、配置片段可直接复制使用，兼顾新手入门与进阶避坑，助力快速实现两者联动，高效调用第三方模型。</p><p>适配环境：Windows/Mac/Linux、Cherry Studio v1.2.0+、OpenClaw最新稳定版，文档内容同步结合OpenClaw Token省流技巧，规避“会话累积导致成本暴涨”的常见问题。</p><h3>一、对接核心概述</h3><h4>1.1 对接意义</h4><p>Cherry Studio 作为统一AI接口入口，可通过自定义提供商功能对接OpenClaw，实现“一次集成，多模型复用”——无需单独适配OpenClaw的接口规范，即可通过Cherry Studio的标准化API，调用OpenClaw中配置的各类第三方模型（如Claude Opus 4.5、GLM-4.7等），同时借助Cherry Studio的会话管理、流式响应等功能，提升开发效率。</p><h4>1.2 核心前提（必做）</h4><ol><li>已安装Cherry Studio桌面客户端（参考Cherry Studio API手册2.1章节），并能正常启动服务；</li><li>已安装OpenClaw，获取其配置文件（通常为config.json）及API相关信息；</li><li>确保两者网络互通（本地部署需关闭防火墙拦截，远程部署需配置对应端口开放）；</li><li>提前准备第三方模型的API密钥（如Anthropic、ModelScope密钥），用于OpenClaw模型配置。</li></ol><h3>二、前期准备（3步到位）</h3><h4>2.1 环境检查与依赖安装</h4><h5>2.1.1 基础依赖</h5><ul><li>安装Node.js（v16.x+）：用于运行OpenClaw插件及Cherry Studio自定义提供商；</li><li>安装npm/yarn：用于安装OpenClaw所需插件（如@ai-sdk/openai-compatible）；</li><li>安装Python（可选）：若OpenClaw启用fetch服务，需安装Python及uvx工具（执行<code>pip install uvx</code>）。</li></ul><h5>2.1.2 核心依赖安装（终端执行）</h5><pre><code class="bash">
# 安装Cherry Studio对接OpenClaw所需依赖

npm install @ai-sdk/openai-compatible

# 安装OpenClaw常用插件（贴合常见配置）

npm install oh-my-opencode@3.1.10 opencode-pty opencode-supermemory@latest
</code></pre><h4>2.2 关键信息收集</h4><p>提前收集以下信息，避免配置时反复查找：</p><ol><li>Cherry Studio服务信息：启动端口（默认8080）、API密钥（启动时设置的--api-key）；</li><li>OpenClaw配置信息：基础地址（baseURL，如<a href="https://link.segmentfault.com/?enc=kuycM9DwgsKpOFg3qWHh4Q%3D%3D.9QX8QwWn1K9rEIs05bPeTfNIbe4g5gk8rcV1xsB3l5o%3D" rel="nofollow" target="_blank">http://127.0.0.1:23333/api/v1</a>）、模型ID（注意：不支持冒号“:”，需替换为下划线“_”或转义）；</li><li>第三方模型信息：模型所属提供商（Anthropic/ModelScope等）、API密钥、模型ID（与OpenClaw配置一致）。</li></ol><h4>2.3 版本兼容性检查</h4><ul><li>Cherry Studio版本需≥v1.1.0（支持自定义提供商集成）；</li><li>OpenClaw插件版本需与核心版本匹配，避免插件冲突（推荐使用配置中指定的插件版本）；</li><li>若出现版本不兼容，可降级Cherry Studio至v1.2.0（稳定版），或更新OpenClaw至最新版。</li></ul><h3>三、完整对接配置步骤（核心环节）</h3><p>对接流程分为3步：OpenClaw配置优化 → Cherry Studio自定义提供商配置 → 联调测试，每一步均提供可复制的配置示例，规避常见错误。</p><h4>3.1 第一步：OpenClaw配置优化（关键避坑）</h4><p>修改OpenClaw的config.json配置文件，适配Cherry Studio对接，同时优化Token成本控制（结合上传的省流指南），核心配置如下（替换占位符为实际信息）：</p><pre><code class="json">
{

  "$schema": "https://opencode.ai/config.json",

  "plugin": [

    "oh-my-opencode@3.1.10",

    "opencode-pty",

 "opencode-supermemory@latest",

    "opencode-browser"

  ],

  "disabled_providers": [],

  "provider": {

    "cherry": { // Cherry Studio对接专属配置节点

      "baseUrl": "http://127.0.0.1:8080/api/v1", // Cherry Studio服务地址（含端口）

      "apiKey": "your-cherry-api-key", // Cherry Studio启动时设置的API密钥

      "api": "openai-completions",

      "models": [ // 模型配置（重点：ID不支持冒号，替换为下划线）

 {

          "id": "30182ea9-dad7-437c-aa6e-2f5c8c823174_xopglm5", // 冒号替换为下划线

          "name": "xunfeixincheng/xopglm5",

          "reasoning": false,

          "input": ["text"],

          "cost": {

            "input": 0,

            "output": 0,

            "cacheRead": 0,

            "cacheWrite": 0

          },

 "contextWindow": 200000,

          "maxTokens": 8192

        }

      ]

    },

    "modelscope": { // 其他第三方模型配置（可选）

      "name": "ModelScope",

      "npm": "@ai-sdk/openai-compatible",

      "models": {

        "ZhipuAI/GLM-4.7": {

          "name": "GLM-4.7"

        }

      },

 "options": {

        "apiKey": "your-modelscope-api-key",

        "baseURL": "https://api-inference.modelscope.cn/v1"

      }

    }

  },

  "mcp": { // 临时禁用不必要的MCP服务，避免启动失败（可后续按需启用）

    "chrome-devtools": { "type": "local", "command": ["npx", "-y", "chrome-devtools-mcp@latest"], "enabled": false },

    "context7": { "type": "local", "command": ["cmd", "/c", "npx", "-y", "@upstash/context7-mcp"], "enabled": false },

    "fetch": { "type": "local", "command": ["uvx", "mcp-server-fetch"], "enabled": false },

    "memory": { "type": "local", "command": ["cmd", "/c", "npx", "-y", "@modelcontextprotocol/server-memory"], "enabled": false }

  },

  "compaction": { "auto": true }, // 自动压缩会话，控制Token消耗

  "session": { // 自动重置会话，从根源省成本（核心配置）

    "reset": { "dailyTime": "04:00", "idleMinutes": 60 }

  },

  "cache": { "ttl": "1h", "pruneOnExpiry": true } // 缓存优化，减少重复Token消耗

}
</code></pre><h5>关键配置说明（避坑重点）</h5><ol><li>模型ID处理：OpenClaw模型ID不支持冒号“:”，需替换为下划线“_”（如示例中修改的ID），否则会导致配置解析失败；</li><li>MCP服务：默认禁用所有MCP服务，避免因依赖缺失（如uvx、特定npm包）导致OpenClaw启动失败，后续可逐个启用排查；</li><li>成本控制配置：添加session自动重置、cache缓存、compaction自动压缩，对应上传文档中的省流策略，避免会话累积导致Token暴涨；</li><li>Cherry对接节点：baseUrl、apiKey需与Cherry Studio启动信息完全一致，否则无法建立连接。</li></ol><h4>3.2 第二步：Cherry Studio配置（自定义提供商）</h4><p>Cherry Studio需通过自定义提供商集成OpenClaw，配置分为2部分：config.yaml配置 + 自定义提供商代码，均贴合Cherry Studio API手册扩展功能章节。</p><h5>3.2.1 Cherry Studio config.yaml配置（修改安装目录下的config.yaml）</h5><pre><code class="yaml">
# config.yaml 核心配置（新增OpenClaw自定义提供商）

api:

  port: 8080 # 与OpenClaw配置中的baseUrl端口一致

  cors_origins: ["http://127.0.0.1:23333"] # 允许OpenClaw地址跨域（替换为OpenClaw实际地址）

  rate_limit: 1000

providers:

  deepseek: # 原有提供商保留

    api_key: ${DEEPSEEK_API_KEY}

    base_url: "https://api.deepseek.com"

  anthropic: # 原有提供商保留

    api_key: ${ANTHROPIC_API_KEY}

  openclaw: # 新增OpenClaw自定义提供商节点

    api_key: ${OPENCLAW_API_KEY} # OpenClaw的API密钥（可选，按需配置）

    base_url: "http://127.0.0.1:23333/api/v1" # OpenClaw基础地址（与OpenClaw config.json一致）

logging:

  level: "info"

  file: "cherry-studio.log"
</code></pre><h5>3.2.2 自定义提供商代码（对接OpenClaw）</h5><p>创建自定义提供商文件（如openclaw-provider.js），放在Cherry Studio安装目录的providers文件夹下，代码示例如下（可直接复制使用）：</p><pre><code class="javascript">
// OpenClaw自定义提供商，适配Cherry Studio标准化API

class OpenClawProvider {

 constructor(config) {

    this.config = config; // 读取Cherry Studio config.yaml中的openclaw节点配置

    this.baseUrl = config.base_url; // OpenClaw基础地址

    this.apiKey = config.api_key; // OpenClaw API密钥

    // 模型ID映射（若OpenClaw模型ID有特殊字符，可在此做映射）

    this.idMap = {

      "30182ea9-dad7-437c-aa6e-2f5c8c823174_xopglm5": "30182ea9-dad7-437c-aa6e-2f5c8c823174_xopglm5"

    };

  }



  // 核心方法：实现聊天补全接口，对接OpenClaw

  async chatCompletions(params) {

    // 替换模型ID（适配OpenClaw ID规范）

    const modelId = this.idMap[params.model] || params.model;

    // 构造OpenClaw请求参数（贴合OpenClaw接口规范）

    const requestParams = {

      model: modelId,

      messages: params.messages,

      temperature: params.temperature || 0.2, // 默认0.2，减少重试，控制Token

      max_tokens: params.max_tokens || 8192,

      stream: params.stream || false

    };



    // 发送请求到OpenClaw

    const response = await fetch(`${this.baseUrl}/chat/completions`, {

 method: 'POST',

      headers: {

        'Content-Type': 'application/json',

        'Authorization': `Bearer ${this.apiKey}` // 认证信息

      },

      body: JSON.stringify(requestParams)

    });



    // 返回响应（适配Cherry Studio标准化响应格式）

    if (params.stream) {

      return response.body; // 流式响应，直接返回

    } else {

      const result = await response.json();

      return {

        id: result.id || `chatcmpl-${Date.now()}`,

        object: "chat.completion",

        created: Date.now() / 1000 | 0,

        model: modelId,

        choices: result.choices || [],

        usage: result.usage || { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }

      };

    }

  }

}



// 注册自定义提供商到Cherry Studio

module.exports = {

  register: (cherryStudio) =&gt; {

    cherryStudio.registerProvider('openclaw', OpenClawProvider);

  }

};
</code></pre><h4>3.3 第三步：启动服务与联调测试</h4><p>完成配置后，按以下顺序启动服务，逐步测试对接是否成功，避免因启动顺序错误导致失败。</p><h5>3.3.1 启动顺序（必按此顺序）</h5><ol><li>启动OpenClaw：打开终端，进入OpenClaw安装目录，执行启动指令（具体指令参考OpenClaw官方文档）；</li><li>启动Cherry Studio：打开终端，执行启动指令（指定端口和API密钥，与配置一致）：</li></ol><pre><code class="bash">
cherry-studio start --port 8080 --api-key your-cherry-api-key
</code></pre><ol start="3"><li>验证服务启动：分别访问Cherry Studio（<a href="https://link.segmentfault.com/?enc=LzPIJQxrAApG6mICkz6%2Faw%3D%3D.ytLNLW1WcpuUGM3BUS5cNfOcNyx6eLeRXe5C4VhVG08%3D" rel="nofollow" target="_blank">http://localhost:8080</a>）和OpenClaw（<a href="https://link.segmentfault.com/?enc=GAq73f4e40VeQWKH9uVbUw%3D%3D.wg0c6Cz7LxY8bsDrm2ndNdkgf6hkYJoaANQocUy1nX4%3D" rel="nofollow" target="_blank">http://127.0.0.1:23333</a>），确认服务正常运行。</li></ol><h5>3.3.2 联调测试（JavaScript示例，可直接复制）</h5><p>通过Cherry Studio的API调用OpenClaw中的模型，测试对接是否成功，同时验证Token消耗是否正常：</p><pre><code class="javascript">
// 测试Cherry Studio对接OpenClaw

const API_BASE = 'http://localhost:8080/api/v1';

const CHERRY_API_KEY = 'your-cherry-api-key'; // Cherry Studio API密钥



async function testOpenClawChat() {

  try {

    const response = await fetch(`${API_BASE}/chat/completions`, {

      method: 'POST',

 headers: {

        'Content-Type': 'application/json',

        'Authorization': `Bearer ${CHERRY_API_KEY}`

      },

      body: JSON.stringify({

        provider: 'openclaw', // 指定调用OpenClaw自定义提供商

        model: '30182ea9-dad7-437c-aa6e-2f5c8c823174_xopglm5', // OpenClaw模型ID（替换后）

        messages: [{ role: 'user', content: '测试Cherry Studio对接OpenClaw，返回1句话即可' }],

        temperature: 0.2,

        stream: false

      })

    });



    const result = await response.json();

    console.log('对接成功，AI响应：', result.choices[0].message.content);

    console.log('Token消耗：', result.usage);

  } catch (error) {

    console.error('对接失败，错误信息：', error.message);

  }

}



// 执行测试

testOpenClawChat();
</code></pre><h5>测试成功标准</h5><ol><li>终端正常输出AI响应内容和Token消耗信息；</li><li>OpenClaw日志中无错误信息（查看openclaw.log）；</li><li>Cherry Studio日志中无401/404/503等错误（查看cherry-studio.log）。</li></ol><h3>四、常见问题排查（避坑重点）</h3><p>结合之前遇到的启动失败、配置错误、Token暴涨等问题，整理高频故障及解决方案，快速定位问题。</p><h4>4.1 故障1：OpenClaw无法启动</h4><h5>常见原因</h5><ol><li>模型ID包含冒号“:”，导致配置解析失败；</li><li>MCP服务依赖缺失（如uvx未安装、npm包拉取失败）；</li><li>插件版本冲突（如@latest版本插件与核心版本不兼容）。</li></ol><h5>解决方案</h5><ol><li>检查所有模型ID，将冒号替换为下划线；</li><li>保持MCP服务禁用，启动成功后再逐个启用排查；</li><li>精简插件，仅保留核心插件（oh-my-opencode@3.1.10、opencode-pty），重新安装插件。</li></ol><h4>4.2 故障2：Cherry Studio无法调用OpenClaw模型（返回404/503）</h4><h5>常见原因</h5><ol><li>Cherry Studio config.yaml中未配置OpenClaw跨域（cors_origins未添加OpenClaw地址）；</li><li>自定义提供商未注册成功（代码路径错误、方法缺失）；</li><li>OpenClaw baseUrl配置错误，或OpenClaw未正常启动。</li></ol><h5>解决方案</h5><ol><li>修改Cherry Studio config.yaml，添加OpenClaw地址到cors_origins；</li><li>确认自定义提供商文件放在providers文件夹下，代码中register方法正确；</li><li>重启OpenClaw，确认baseUrl与Cherry Studio配置一致。</li></ol><h4>4.3 故障3：Token消耗过快（类似两小时烧光100美元）</h4><h5>常见原因</h5><ol><li>未配置会话自动重置，对话历史持续累积；</li><li>未启用会话压缩和缓存，重复请求消耗大量Token；</li><li>高价模型（如Claude Opus 4.5）用于日常杂活，未按需选择模型。</li></ol><h5>解决方案</h5><ol><li>确保OpenClaw配置中添加session自动重置（dailyTime+idleMinutes）；</li><li>启用compaction自动压缩和cache缓存（参考3.1节配置）；</li><li>核心任务用高价模型，日常测试用Sonnet/GPT-4o-mini等低成本模型；</li><li>手动执行命令控成本：/new（新建会话）、/reset（重置会话）、/compact（压缩会话）。</li></ol><h4>4.4 故障4：对接成功但响应缓慢</h4><h5>解决方案</h5><ol><li>启用流式响应（stream: true），提升交互体验；</li><li>优化Cherry Studio连接池配置（参考API手册6.1节）；</li><li>清理OpenClaw旧会话文件（删除~/.openclaw/agents.main/sessions/*.jsonl）。</li></ol><h3>五、Token成本控制进阶策略（补充上传文档核心要点）</h3><p>结合上传的OpenClaw省流指南，补充适配Cherry Studio对接场景的成本控制技巧，进一步降低Token消耗。</p><h4>5.1 会话管理（核心省流）</h4><ol><li>手动控制：在Cherry Studio调用接口时，可通过传递clear: true参数，手动清空会话历史；</li><li>自动控制：保持OpenClaw中session配置（每日04:00自动重置+60分钟空闲重置），高价模型可将idleMinutes改为30；</li><li>定期清理：每周清理OpenClaw旧会话文件，避免历史会话占用资源、消耗Token。</li></ol><h4>5.2 模型与参数优化</h4><ol><li>按需选模型：在Cherry Studio调用时，通过provider和model参数灵活切换模型，避免“大材小用”；</li><li>参数调整：将temperature设为0.2-0.5（减少生成多样性，降低重试率），max_tokens按需设置（避免过度生成）；</li><li>启用预留Token：在OpenClaw配置中设置reserveTokens: 20000，避免单次请求Token耗尽。</li></ol><h4>5.3 实时监控与预警</h4><ol><li>Cherry Studio侧：添加性能监控，记录每次调用的Token消耗（参考API手册6.3节）；</li><li>OpenClaw侧：使用命令监控Token消耗：</li></ol><pre><code class="bash">
/status  # 查看会话状态、当前Token使用量

/usage full  # 查看详细用量记录

/usage cost  # 查看成本统计
</code></pre><ol start="3"><li>预警设置：在模型后台设置用量限额，或使用APIYI平台余额提醒，防止费用暴涨。</li></ol><h3>六、完整配置抄作业（直接复制使用）</h3><p>整合以上所有配置，提供可直接复制的完整配置片段，替换占位符即可快速完成对接与优化。</p><h4>6.1 OpenClaw config.json（核心片段）</h4><pre><code class="json">
{

  "provider": {

    "cherry": {

      "baseUrl": "http://127.0.0.1:8080/api/v1",

      "apiKey": "your-cherry-api-key",

      "api": "openai-completions",

      "models": [

        {

          "id": "30182ea9-dad7-437c-aa6e-2f5c8c823174_xopglm5",

          "name": "xunfeixincheng/xopglm5",

          "reasoning": false,

          "input": ["text"],

          "cost": { "input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0 },

          "contextWindow": 200000,

          "maxTokens": 8192

        }

      ]

    }

  },

  "mcp": {

    "chrome-devtools": { "enabled": false },

    "context7": { "enabled": false },

    "fetch": { "enabled": false },

    "memory": { "enabled": false }

  },

  "compaction": { "auto": true },

  "session": { "reset": { "dailyTime": "04:00", "idleMinutes": 60 } },

  "cache": { "ttl": "1h", "pruneOnExpiry": true }

}
</code></pre><h4>6.2 Cherry Studio config.yaml（核心片段）</h4><pre><code class="yaml">
api:

  port: 8080

  cors_origins: ["http://127.0.0.1:23333"]

providers:

  openclaw:

    api_key: "your-openclaw-api-key"

    base_url: "http://127.0.0.1:23333/api/v1"
</code></pre><h3>七、总结</h3><p>Cherry Studio对接OpenClaw的核心是“自定义提供商+配置互通”，关键避坑点在于OpenClaw模型ID规范、MCP服务依赖、跨域配置，而成本控制的核心是“会话重置+缓存压缩+按需选模型”。</p><p>本文档整合了对接全流程、常见故障、成本优化等核心内容，所有代码和配置均可直接复制使用，兼顾新手入门与进阶需求。对接成功后，可充分发挥Cherry Studio的统一接口优势和OpenClaw的多模型适配能力，同时通过省流策略，实现“高效开发+低成本使用”的双重目标。</p><p>后续若遇到版本更新、模型适配等问题，可参考Cherry Studio官方项目仓库和OpenClaw官方文档，或通过Cherry Studio技术支持渠道反馈。</p><p>本文由<a href="https://link.segmentfault.com/?enc=TH5MTUMnM724lKxCfwBdZA%3D%3D.TsLHx0ezvNgowNZg2QIXfcdVOCMSdv66XlYd6KeeJ3g%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[删掉 40 行代码，性能飙 400 倍？OpenJDK 这次把“慢如 /proc”按在地上摩擦！ 吾]]></title>    <link>https://segmentfault.com/a/1190000047610685</link>    <guid>https://segmentfault.com/a/1190000047610685</guid>    <pubDate>2026-02-14 11:04:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>有些人刷短视频解压，有些人刷八卦上头，而有的人（懂的都懂）会定期翻 OpenJDK 的 commit log——属于那种“看别人修 bug 也能获得快乐”的硬核爱好。最近就有这么一个提交，能让人滑着滑着突然停住：<strong>一个几十行的小修，直接把 30x–400x 的性能差距抹平了。</strong></p><p>提交信息长这样（链接转纯文本）：<br/><a href="https://link.segmentfault.com/?enc=r9uwDQB21GLn44MFOgB05g%3D%3D.KsSQi5StWIpH1xtfPIctLidwrg8m0%2BXFg2W0XIMSVtBNxVjtR0bbLMdVrmFJFV3ij7kKikR%2Fhmy%2BjrVOcwOiFQ%3D%3D" rel="nofollow" target="_blank">https://github.com/openjdk/jdk/commit/858d2e434dd</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610688" alt="image" title="image"/></p><p>关键点一句话：<strong>Linux 下 <code>ThreadMXBean.getCurrentThreadUserTime()</code> 以前走 <code>/proc</code> 读文件+解析，现在改成 <code>clock_gettime()</code> 一把梭。</strong><br/>听着朴实无华，但效果非常“离谱地好”。</p><hr/><h2>这锅到底有多大</h2><p>同门两兄弟，一个像跑车，一个像拖拉机</p><p>先认识两位主角：</p><ul><li><code>ThreadMXBean.getCurrentThreadCpuTime()</code>：拿“当前线程总 CPU 时间”（user + system）</li><li><code>ThreadMXBean.getCurrentThreadUserTime()</code>：只拿“当前线程 user 时间”</li></ul><p>你可能会以为：两者应该差不多吧？都是“线程 CPU 时间”嘛。<br/>结果历史实现告诉你：<strong>想多了。</strong></p><p>原始 bug 报告直接给出差距：<br/><a href="https://link.segmentfault.com/?enc=86oB3nTvi3SZm1ewkI130g%3D%3D.CdvM4QtZmEXeWG51LwpxO5X%2BsktlV7pXM2zlAjohb1N0v7Lpw83jkgnDH6mQGLZ3" rel="nofollow" target="_blank">https://bugs.openjdk.org/browse/JDK-8210452</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610689" alt="image" title="image" loading="lazy"/></p><p>里面量化得很狠：</p><blockquote>getCurrentThreadUserTime 比 getCurrentThreadCpuTime 慢 30x–400x</blockquote><p>并发一上来，差距还会被放大，属于“越忙越慢，越慢越崩”。</p><hr/><h2>旧实现</h2><p>为了拿个 user time，JVM 竟然去读 <code>/proc</code></p><p>旧版 Linux 实现藏在 <code>os_linux.cpp</code> 里，本质流程是：</p><ol><li>拼路径 <code>/proc/self/task/&lt;tid&gt;/stat</code></li><li><code>open()</code> 打开文件</li><li><code>read()</code> 读进 buffer</li><li>处理一个“恶意格式”：线程名里可能有括号，所以要找最后一个 <code>)</code></li><li><code>sscanf()</code> 抽第 13/14 个字段（user/sys tick）</li><li>tick 换算成纳秒</li></ol><p>那段被删掉的代码大概长这样</p><pre><code>static jlong user_thread_cpu_time(Thread *thread) {
  pid_t  tid = thread-&gt;osthread()-&gt;thread_id();
  char *s;
  char stat[2048];
  size_t statlen;
  char proc_name[64];
  int count;
  long sys_time, user_time;
  char cdummy;
  int idummy;
  long ldummy;
  FILE *fp;


  os::snprintf_checked(proc_name, 64, "/proc/self/task/%d/stat", tid);
  fp = os::fopen(proc_name, "r");
  if (fp == nullptr) return -1;
  statlen = fread(stat, 1, 2047, fp);
  stat[statlen] = '0';
  fclose(fp);


  // Skip pid and the command string. Note that we could be dealing with
  // weird command names, e.g. user could decide to rename java launcher
  // to "java 1.4.2 :)", then the stat file would look like
  //                1234 (java 1.4.2 :)) R ... ...
  // We don't really need to know the command string, just find the last
  // occurrence of ")" and then start parsing from there. See bug 4726580.
  s = strrchr(stat, ')');
  if (s == nullptr) return -1;


  // Skip blank chars
  do { s++; } while (s &amp;&amp; isspace((unsigned char) *s));


  count = sscanf(s,"%c %d %d %d %d %d %lu %lu %lu %lu %lu %lu %lu",
                 &amp;cdummy, &amp;idummy, &amp;idummy, &amp;idummy, &amp;idummy, &amp;idummy,
                 &amp;ldummy, &amp;ldummy, &amp;ldummy, &amp;ldummy, &amp;ldummy,
                 &amp;user_time, &amp;sys_time);
  if (count != 13) return -1;


  return (jlong)user_time * (1000000000 / os::Posix::clock_tics_per_second());
}</code></pre><p>看到这里你大概已经开始皱眉：<strong>为了拿一个时间，居然要走文件系统、内核拼字符串、用户态 sscanf 解析……</strong><br/>这不是性能敏感路径里最忌讳的“豪华套餐”吗？</p><hr/><h2>对比：CPU time 那位“亲兄弟”一直就很优雅</h2><p>同样是拿时间，<code>getCurrentThreadCpuTime()</code> 从古至今都干净得像刚洗完的盘子：</p><pre><code>jlong os::current_thread_cpu_time() {
  return os::Linux::thread_cpu_time(CLOCK_THREAD_CPUTIME_ID);
}


jlong os::Linux::thread_cpu_time(clockid_t clockid) {
  struct timespec tp;
  clock_gettime(clockid, &amp;tp);
  return (jlong)(tp.tv_sec * NANOSECS_PER_SEC + tp.tv_nsec);
}</code></pre><p>一句 <code>clock_gettime()</code>，没有文件 IO、没有解析、没有一堆 syscalls 组合拳。</p><hr/><h2>为什么差这么多：一次 syscall vs 一串 syscall + VFS + 字符串 + 解析</h2><p>旧 <code>/proc</code> 路径大概干了这些活：</p><ul><li><code>open()</code> syscall</li><li>VFS 分发 + dentry 查找</li><li>procfs 动态生成内容</li><li>内核把数字格式化成字符串</li><li><code>read()</code> syscall，把内容拷回用户态</li><li>用户态 <code>sscanf()</code> 解析</li><li><code>close()</code> syscall（还可能牵扯锁、futex 等）</li></ul><p>而 <code>clock_gettime(CLOCK_THREAD_CPUTIME_ID)</code> 路径基本是：</p><ul><li><strong>单次 syscall</strong> → 直接走内核里一条更短的函数链，读调度实体里的时间信息</li></ul><p>所以差距不在“要不要进内核”（两者都要），而在<strong>进内核之后做了多少额外工作</strong>。</p><hr/><h2>那问题来了：当年为啥不用 clock_gettime 拿 user time？</h2><p>答案很“标准化”：POSIX 规定 <code>CLOCK_THREAD_CPUTIME_ID</code> 返回的是<strong>总 CPU 时间（user + system）</strong>。<br/>“只要 user 时间”这件事，在 POSIX 意义上并没有一个通用开关。</p><p>但 Linux 有自己的“私房菜”：<strong>clockid_t 的位编码</strong>。这个编码在 Linux 内核里稳定很多年，但你在 man page 里不一定能看到它的完整说明——要想懂，得看内核源码那种“祖传注释”。</p><hr/><h2>新实现</h2><p>用 pthread_getcpuclockid 拿到 clockid，再把类型位翻成 VIRT（user-only）</p><p>Linux 从 2.6.12（2005 年）开始，就在 <code>clockid_t</code> 里编码了“时钟类型/线程或进程”等信息。<code>pthread_getcpuclockid()</code> 会给你一个 POSIX 合规的 clockid（通常是 SCHED：user+system）。<br/>接着只要把低位类型从 <code>10</code> 翻成 <code>01</code>（VIRT：user-only），再交给 <code>clock_gettime()</code>，就能得到 user time。</p><p>新代码核心如下：</p><pre><code>static bool get_thread_clockid(Thread* thread, clockid_t* clockid, bool total) {
  constexpr clockid_t CLOCK_TYPE_MASK = 3;
  constexpr clockid_t CPUCLOCK_VIRT = 1;


  int rc = pthread_getcpuclockid(thread-&gt;osthread()-&gt;pthread_id(), clockid);
  if (rc != 0) {
    // Thread may have terminated
    assert_status(rc == ESRCH, rc, "pthread_getcpuclockid failed");
    return false;
  }


  if (!total) {
    // Flip to CPUCLOCK_VIRT for user-time-only
    *clockid = (*clockid &amp; ~CLOCK_TYPE_MASK) | CPUCLOCK_VIRT;
  }


  return true;
}

static jlong user_thread_cpu_time(Thread *thread) {
  clockid_t clockid;
  bool success = get_thread_clockid(thread, &amp;clockid, false);
  return success ? os::Linux::thread_cpu_time(clockid) : -1;
}</code></pre><p>对比旧实现：</p><ul><li>没有 <code>/proc</code></li><li>没有 fread buffer</li><li>没有 <code>sscanf</code></li><li>syscalls 数量也从“一串”变成“基本一个”</li></ul><p>简直是“删代码删出性能”。</p><hr/><h2>真实性能：从 11 微秒降到 279 纳秒，直接 40 倍起飞</h2><p>为了量化差距，修复里还顺手带了 JMH benchmark（这点很加分：<strong>没有基准的优化，容易变成自我感动</strong>）。</p><p>原 benchmark 示例：</p><pre><code>@State(Scope.Benchmark)
@Warmup(iterations = 2, time = 5)
@Measurement(iterations = 5, time = 5)
@BenchmarkMode(Mode.SampleTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Threads(16)
@Fork(value = 1)
public class ThreadMXBeanBench {
    static final ThreadMXBean mxThreadBean = ManagementFactory.getThreadMXBean();
    static long user; // To avoid dead-code elimination

    @Benchmark
    public void getCurrentThreadUserTime() throws Throwable {
        user = mxThreadBean.getCurrentThreadUserTime();
    }


    public static void main(String[] args) throws RunnerException {
        Options opt = new OptionsBuilder()
                .include(ThreadMXBeanBench.class.getSimpleName())
                .build();
        new Runner(opt).run();
    }
}</code></pre><p>旧版本（/proc 路径）结果大意是：<strong>平均 ~11 微秒</strong>。<br/>修复后结果：<strong>平均 ~0.279 微秒（也就是 279 纳秒）</strong>。<br/>算下来差不多 <strong>40x</strong> 改善（在 30x–400x 区间内，符合历史报告范围）。</p><p>配套 profile 也很直观：旧版像“syscall 自助餐”，新版像“只吃一道菜”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610690" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610691" alt="image" title="image" loading="lazy"/></p><hr/><h2>彩蛋：还能再榨出 13%？内核 fast-path：PID=0 时跳过 radix tree 查找</h2><p>性能到了纳秒级还继续抠？这就很“工程师”。<br/>在修复后的 profile 里还能看到一个小热点：内核里会做一次 <strong>radix tree lookup</strong> 去定位目标线程的 pid 结构。原因是：<code>pthread_getcpuclockid()</code> 返回的 clockid 编码了具体 TID，内核拿到后要去查。</p><p>但内核有个更快的分支：<strong>如果 clockid 里编码的 PID/TID 是 0，内核把它解释成“当前线程”</strong>，直接走 current task，跳过查找。</p><pre><code>/*
 * Functions for validating access to tasks.
 */
static struct pid *pid_for_clock(const clockid_t clock, bool gettime)
{
[...]


  /*
  * If the encoded PID is 0, then the timer is targeted at current
  * or the process to which current belongs.
  */
  if (upid == 0)
      // the fast path: current task lookup, cheap
      return thread ? task_pid(current) : task_tgid(current);


  // the generalized path: radix tree lookup, more expensive
  pid = find_vpid(upid);
  [...]</code></pre><p>于是有人提出一个“更野但更快”的想法：<strong>既然 OpenJDK 已经在改 clockid 的低位类型了，那干脆自己构造一个 PID=0 的 clockid 走 fast-path。</strong></p><p>clockid 编码示意：</p><pre><code>clockid (原理示意)：


// Linux Kernel internal bit encoding for dynamic CPU clocks:
// [31:3] : Bitwise NOT of the PID or TID (~0 for current thread)
// [2]    : 1 = Per-thread clock, 0 = Per-process clock
// [1:0]  : Clock type (0 = PROF, 1 = VIRT/User-only, 2 = SCHED)
static_assert(sizeof(clockid_t) == 4, "Linux clockid_t must be 32-bit");
constexpr clockid_t CLOCK_CURRENT_THREAD_USERTIME = static_cast&lt;clockid_t&gt;(~0u &lt;&lt; 3 | 4 | 1);</code></pre><p>然后把 <code>getCurrentThreadUserTime()</code> 直接改为用这个 <code>CLOCK_CURRENT_THREAD_USERTIME</code> 调 <code>clock_gettime()</code>。</p><p>结果呢？benchmark 从 <strong>81.7ns 降到 70.8ns</strong>，约 <strong>13%</strong> 提升。<br/>绝对值不大，但属于“白给的快”。</p><p>当然，这也带来一个工程上的灵魂拷问：<strong>为了这点收益，值不值得在 JVM 里依赖更多 Linux 内核 ABI 细节？</strong><br/>这类优化就很像把车轮胎气压从 2.3 调到 2.35：能快，但你得接受“更挑环境”的风险感。</p><hr/><h2>三条硬核经验：写性能代码时，别只信标准，别只信直觉</h2><p>这次修复能这么漂亮，背后有三条特别“值钱”的经验：</p><p>1）<strong>别只读 POSIX，要敢读内核源码</strong><br/>标准告诉你“可移植的下限”，内核源码告诉你“可用的上限”。两者之间有时差着 400 倍。</p><p>2）<strong>别迷信老实现里的假设</strong><br/>当年的 <code>/proc</code> 解析可能是合理折中，但假设会“固化”成代码，一固化就是十几年。隔段时间回头看，往往能捡到大便宜。</p><p>3）<strong>优化要带基准，要留证据</strong><br/>这次提交很关键的一点：带了 JMH benchmark。否则这种改动很容易被质疑为“玄学改法”。</p><hr/><h2>结尾：JDK 26 可能给你“免费加速包”</h2><p>这个改动在 2025-12-03 落地，距离 JDK 26 冻结只差一天。文章里也给了时间点：JDK 26 预计 2026 年 3 月发布。<br/>如果你的系统里确实用到了 <code>ThreadMXBean.getCurrentThreadUserTime()</code>（比如 profiling、监控、诊断工具链），那这波升级基本等于：<strong>白捡一个数量级的延迟下降</strong>。</p><hr/><p><strong>喜欢就奖励一个“👍”和“在看”呗~</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047106529" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[感知无界·创造有形：百灵全模态 Ming-flash-omni-2.0 焕新生活想象 RTE开发者社]]></title>    <link>https://segmentfault.com/a/1190000047610826</link>    <guid>https://segmentfault.com/a/1190000047610826</guid>    <pubDate>2026-02-14 11:04:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>马年将至，百灵 Ming-flash-omni-2.0 正式焕新登场！在这个辞旧迎新的时刻，让我们先请出 Ming-flash-omni-2.0 为大家送上一份特别的“马年祝福”！</p><h2>01 Ming-flash-omni-2.0 速览</h2><p>本次发布的<strong>百灵全模态大模型 Ming-flash-omni-2.0</strong>，基于 <strong>Ling-2.0</strong>（MoE 架构，100B-A6B）架构训练。相比之前发布的 Preview 版本，Ming-flash-omni-2.0 实现了全模态能力的代际跃迁，无论是在复杂的视觉理解、充满情感的语音交互，还是极具创意的图像编辑上，Ming-flash-omni-2.0 的实测表现均已跻身<strong>开源领先</strong>水准。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610828" alt="" title=""/></p><p>长期以来，多模态大模型领域存在一个难题：通用的“全模态大模型”（Omni-MLLMs）往往在特定领域的表现不如“模态专用大模型”（Specialist MLLMs）。Ming-omni 系列的研发初衷，正是为了填补这道鸿沟。从 Lite 版本到 Flash Preview，我们验证了模型规模对性能的提升作用；而从 Preview 到如今的 2.0 版本，我们通过海量数据的精细化打磨，进一步触达了性能的天花板。Ming-flash-omni-2.0 的诞生证明了：一个统一架构的全模态模型，完全可以既是博学的通才，又是特定模态的专家。</p><h2>02 特色能力</h2><p>Ming-flash-omni-2.0 兼具领先的通用泛化性能与深度的领域专长，特别是在视觉百科知识力、沉浸式语音生成及高动态图像创作领域，展现出极强的专业竞争力。</p><p><strong>视觉百科：看懂万物，更懂你所见</strong></p><p>Ming-flash-omni-2.0 不仅仅是看见图像，更能调动背后的专家级知识库，实现“所见即所知”。它能：</p><ul><li>懂自然：精准识别花草鸟兽，从珍稀植物的品种溯源、濒危动物的特征识别，科普知识随手可得；</li><li>懂生活：从解析地方名菜风味到全球地标的精准匹配，满足好奇心与实用性；</li><li>懂专业：文物古玩精准辨识，识别年代、器型与工艺细节，成为工作中的高效助手。</li></ul><p>当博学的“百科全书”叠加了极致的“视觉捕捉”，Ming-flash-omni-2.0 展现出了极强的时空语义理解能力:</p><p><strong>可控语音生成：有情绪，有温度，声临其境</strong></p><p>告别机械的电子音，Ming-flash-omni-2.0 让声音充满了表现力。它不仅能说话，还能根据你的指令调整情绪、语调甚至背景氛围。</p><ul><li><strong>让文字拥有温度与情绪</strong>：你可以通过指令控制方言、语速、情感，同时支持普通话、粤语、四川话的自然切换。</li><li><strong>千人千面的声音定制</strong>：支持基于自然语言描述的音色定义（涵盖年龄、性别、情感质感等维度），想要特定的音色？只需一段自然语言描述即可生成对应风格的音色，或者从内置的 100+ 精品音色与经典角色音色中挑选，它都能精准还原、自然演绎。</li><li><strong>全能的声音艺术家</strong>：Ming-flash-omni-2.0 作为业界首个将语音、音效和背景音乐生成融为一体的模型，实现了三类声学信号统一自回归 + 连续音频表征来生成，营造出声临其境的听觉体验。</li></ul><p><strong>图像创作：所想即所见，光影随心变</strong></p><p>Ming-flash-omni-2.0 实现全能型图像处理能力，大幅提升生图、改图及分割的性能表现，赋予了你对画面的绝对掌控权。</p><ul><li><strong>氛围感重构</strong>：拒绝千篇一律的游客照。一句话，就能把平平无奇的照片变成“节日大片”或“故事感写真”，只需一句简单的指令——如烟花、海鸥、日出日落、花瓣雨、落叶纷飞、毛毛细雨或漫天飞雪，模型便能在<strong>完美保持人物与场景特征一致性</strong>的同时，为画面自然注入沉浸式的环境氛围。</li><li><strong>“任意门”般的场景合成</strong>：想去阿尔卑斯滑雪？无需P图高手，模型能精准理解你的指令，将人物无缝融入全新的背景中。</li><li><strong>智能的“橡皮擦”</strong>：无论是杂乱的人群还是多余的物体，它都能精准移除，并自动补全背景细节，还原照片最纯净的美。</li></ul><p><strong>通过融合 Ming-flash-omni-2.0 的语音与图像生成能力，还可以实现“音画一体”的创作体验。所见有形，所感有声，让视觉的张力与听觉的温情在此刻深度交织。</strong></p><h2>03 技术深解：Ming-flash-omni-2.0 如何实现突破？</h2><p>我们整理了驱动 Ming-flash-omni-2.0 性能飞跃的核心技术细节。</p><p><strong>全模态感知的强化</strong></p><ul><li><strong>像素级细粒度感知</strong>: 针对易混淆的图像（如珍稀动植物），我们引入了亿级高质量数据，并采用“难例挖掘”策略，通过将相似样本拼接为多图布局进行对比学习，促进模型在对比学习中学会分辨微小的特征差异。</li><li><strong>音频细粒度感知增强</strong>: 引入高质音频-文本数据，对语音的年龄、性格、风格、语速、语调、职业、情绪、方言等维度进行精细标注，强化 Ming-flash-omni-2.0 对人声和音色的感知和可控生成能力。</li><li><strong>结构化知识对齐</strong>: 通过引入知识图谱，将图像实体、音频描述与结构化的专家知识对齐，确保模型不仅“看到”，更能“懂得”。</li><li><strong>视频时序建模</strong>: 引入 Time-Interleaved VideoRoPE 机制，就像给视频帧打上了精准的时间戳，显著增强了模型对动态事件的捕捉能力。</li></ul><p><strong>泛音频统一生成框架</strong></p><p>Ming-flash-omni-2.0 作为业界首个全场景音频统一生成模型，可在同一条音轨中同时生成语音（Speech）、环境音效（Audio）与音乐（Music）。针对语音、音效与音乐在频带分布及序列长度上的显著差异的难题，我们提出了异构音频信号联合建模方案：</p><ul><li><strong>低帧率/高保真连续表征</strong>：自研 <strong>12.5Hz 超低帧率连续语音 Tokenizer</strong>，实现了对高频 Audio/Music 信号的高保真重构。该机制不仅降低了特征冗余，更在统一的潜在空间内实现了异构音频信号的标准化表征。</li><li><strong>Patch-based 压缩与曝光偏差缓解</strong>：引入<strong> Patch-by-Patch 四帧压缩策略</strong>，将生成序列长度进一步缩减。这一设计有效缩短了自回归建模的路径，显著缓解了超长音频生成任务中常见的曝光偏差累积问题，通过非对称的 DiT head condition 和 patch size 解决多种类型音频统一建模。</li><li><strong>极低频推理优化</strong>：在推理阶段，模型实现了 <strong>3.1Hz 的业界极低推理帧率</strong>。这不仅极大降低了计算开销，而且使模型在保持高音质输出的同时，具备了实时的生成速度与极致的计算效率。</li></ul><p><strong>视觉生成、编辑和分割的深度融合</strong></p><p>Ming-flash-omni-2.0 首创将<strong>生成、编辑、分割</strong>融入单一原生模型，实现架构级深度统一的同时，模型在生成、编辑及分割的典型指标上均达领先水平，并兼顾了生成图像的视觉真实感。</p><ul><li><strong>原生单流与动态感知</strong>：采用单流设计，在统一 Token 空间内利用<strong>全量注意力机制</strong>打通三大任务，并引入基于<strong>动作标签的平衡采样</strong>策略，针对高动态场景（如旅拍）实现任务间深度对齐。这一融合有效消除了复杂动作生成的僵硬感，确保了人物体态的自然与画面的动态张力。</li><li><p><strong>扩散模型强化学习鲁棒性优化</strong>： 针对强化学习易出现的“奖励欺骗”问题，构建<strong>三重稳健机制。</strong></p><p><strong>1）冷启动</strong>：利用确定性的“编辑式分割”任务建立模型的基础<strong>空间认知与定位能力</strong>；</p><p><strong>2）统一奖励空间建模</strong>：集成多维度评价指标，防止模型因过度优化单一奖励而陷入<strong>过拟合或退化解</strong>；</p><p><strong>3）离线分布正则化</strong>：通过引入约束项，确保生成内容始终锚定在真实图像分布内，大幅提升结果的视觉保真度。</p></li></ul><h2>04 后续规划</h2><p>Ming-flash-omni-2.0 代表了我们在全模态模型探索上的阶段性进展，在多项核心指标上取得了突破。但与大模型普遍存在的幻觉挑战类似，当前版本在知识准确性、特定 IP 内容的识别与生成，以及英文音色克隆的逼真度方面仍有提升空间。此外，指令遵循能力也需进一步优化，以更好地支持复杂任务的精准执行。未来我们将持续优化 Ming-Omni 系列，向全模态智能的深水区挺进，在多任务融合中实现新的智能涌现。</p><h2><strong>05 开源相关信息</strong></h2><p><strong>Ming-flash-omni-2.0 模型权重和推理代码已开源：</strong></p><p>🤗<strong>Hugging Face</strong>：</p><p><a href="https://link.segmentfault.com/?enc=ec9geKnLGX7wELR83ipYug%3D%3D.EI3DAoaKSbBBMq57bB86Xjmuuwc3xR6HfynoyoPSz9Q5hGsWA29P8W%2BFwQENnBPu9rj8%2FhVCqU59%2B2OXS2u5pA%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/inclusionAI/Ming-flash-omni-2.0</a></p><p>🤖<strong>ModelScope</strong>：<br/><a href="https://link.segmentfault.com/?enc=K5fMp%2FX4OTxwDCRL4JMhZg%3D%3D.x1dhAj4M35INTTsdDYQUR3PJLhZwJ6ruE%2FgT682XweNGYSNMHeSs%2B5TJ8sf50FnLOfnBB1OXYA5O4EPZFTzIjFa%2B4WIvbsC7KRa%2FMLSw8VQ%3D" rel="nofollow" target="_blank">https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-2.0</a></p><p>📦<strong>GitHub</strong>：<br/><a href="https://link.segmentfault.com/?enc=NDywUsAN9DVTQ9Dd7phQmg%3D%3D.fKv2sIbGQr162g5Hkx0I3q9uabXVPGYh%2BwDKRWM%2BRBgfVw%2Bn%2BwAYYxlz5ibUuTj3" rel="nofollow" target="_blank">https://github.com/inclusionAI/Ming</a></p><p><strong>欢迎大家试用反馈，共同推进开源全模态模型的发展。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610829" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610830" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=YcpTYdOgBBZuc8J3zXRdMA%3D%3D.k6JzA8OZNf2pJg5IY3trK8t8GuIdTWECefEbzNwEMpM%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610831" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026年6款主流CRM横评：从客户管理到AI赋能全方位较量 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047610846</link>    <guid>https://segmentfault.com/a/1190000047610846</guid>    <pubDate>2026-02-14 11:03:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型浪潮中，CRM系统已从单一客户管理工具升级为覆盖全业务链路的数字化中枢。本文选取<strong>超兔一体云、Creatio、Keap、简道云、销帮帮CRM、快启</strong>六款市场主流产品，从<strong>客户管理、销售管理、AI智能、自定义能力、</strong> <strong>API</strong> <strong>对接</strong>五大核心维度展开专业横向对比，为不同规模、不同需求的企业提供选型参考。</p><h2>一、核心定位与适配客户总览</h2><p>先通过全局表格快速建立认知：</p><table><thead><tr><th>品牌</th><th>核心定位</th><th>适配客户群体</th><th>核心优势关键词</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全业务一体化数字化平台</td><td>成长型→中大型企业（全业务协同）</td><td>多跟单模型、AI工作流、RPA集成</td></tr><tr><td>Creatio</td><td>无代码定制化CRM平台</td><td>中大型企业（复杂流程需求）</td><td>全流程无代码定制、多AI模型选择</td></tr><tr><td>Keap</td><td>轻量化CRM+营销自动化平台</td><td>中小微企业（基础获客需求）</td><td>低成本、轻量化营销自动化</td></tr><tr><td>简道云</td><td>零代码低代码应用搭建平台（含CRM）</td><td>全规模企业（平台化需求）</td><td>拖拽式搭建、跨办公平台集成</td></tr><tr><td>销帮帮CRM</td><td>垂直行业深耕型CRM</td><td>成长型企业（垂直赛道）</td><td>AI客户评分、云叩低代码支撑</td></tr><tr><td>快启</td><td>SCRM+AI智能获客平台</td><td>中小企业（获客驱动型）</td><td>微信生态整合、AI获客机器人</td></tr></tbody></table><h2>二、客户管理维度：全生命周期价值挖掘能力对比</h2><h3>核心价值</h3><p>实现客户数据统一归集、精准画像、生命周期管控与权限安全，最终提升客户留存与复购率。</p><h3>横向对比表格</h3><table><thead><tr><th>品牌</th><th>多渠道数据整合</th><th>客户画像构建</th><th>生命周期管理</th><th>权限管控</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>多渠道+自动查重（含企业简称模糊匹配）</td><td>工商/天眼查/社交数据自动补全</td><td>客池自动分类+工作流驱动</td><td>岗位细分权限（财务仅看数据）</td><td>AI生成跟进工作流、经纬度标记</td></tr><tr><td>Creatio</td><td>360°视图统一归集</td><td>自定义字段+流失客户智能分析</td><td>关键节点自动提示</td><td>精细化授权访问/修改</td><td>AI智能体嵌入Outlook/Teams</td></tr><tr><td>Keap</td><td>获客蓝图+表单采集</td><td>行为数据分层精准触达</td><td>基础跟进提醒</td><td>轻量化角色权限</td><td>条件触发式内容推送</td></tr><tr><td>简道云</td><td>表单+第三方集成</td><td>标签化管理+场景化自动化维护</td><td>规则触发客户维护</td><td>角色数据权限</td><td>生日关怀自动化流程</td></tr><tr><td>销帮帮CRM</td><td>公海池分配+自动归集</td><td>AI评分+360°立体画像</td><td>阶段化跟进管控</td><td>垂直行业数据权限</td><td>自定义标签适配赛道需求</td></tr><tr><td>快启</td><td>微信生态+多方式录入</td><td>标签管理+行为轨迹记录</td><td>智能公海流转+签约客户分库</td><td>异常操作实时提醒</td><td>个性化自动丢公海规则</td></tr></tbody></table><h3>品牌亮点可视化（超兔客户管理核心逻辑）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610848" alt="" title=""/></p><pre><code>mindmap
  root((超兔客户管理核心逻辑))
    多渠道数据整合
      广告/官网/微信/工商搜客
      自动查重(客户名/手机号/企业简称)
    客户画像构建
      工商/天眼查数据补全
      社交头像/经纬度自动标记
      自定义字段与列表布局
    生命周期管控
      客池自动分类(需求培养/成交等)
      AI生成工作流驱动跟进
    权限安全
      岗位细分权限(财务/销售隔离)</code></pre><h2>三、销售管理维度：跟单效率与流程可控性对比</h2><h3>核心价值</h3><p>适配不同业务场景的跟单模型，通过自动化与可视化提升销售转化率与团队管控能力。</p><h3>雷达图分值对比（满分10）</h3><ul><li>超兔一体云：9分 | Creatio：8.5分 | 销帮帮CRM：8分 | 快启：7.5分 | 简道云：7分 | Keap：6分</li></ul><h3>横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心跟单模型</th><th>销售自动化能力</th><th>数据可视化</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>三一客小单/商机跟单/多方项目</td><td>工作流驱动+待办提醒+自动日报</td><td>行动记录分析+业绩拆解</td><td>三一客“三定”推进、多方项目收支管控</td></tr><tr><td>Creatio</td><td>无代码自定义全流程</td><td>预定义模板+自定义自动化规则</td><td>定制化销售漏斗</td><td>全流程节点精准配置</td></tr><tr><td>Keap</td><td>轻量化跟进模型</td><td>跟进提醒+邮件自动化推送</td><td>基础漏斗可视化</td><td>营销获客流程自动化</td></tr><tr><td>简道云</td><td>自定义流程搭建</td><td>规则触发任务+数据联动</td><td>实时业绩仪表盘</td><td>多维度销售数据监控</td></tr><tr><td>销帮帮CRM</td><td>全流程阶段化跟进</td><td>拜访记录自动同步+智能提醒</td><td>AI辅助业绩报表</td><td>高价值客户精准识别</td></tr><tr><td>快启</td><td>线索→签约→售后全流程</td><td>自动跟进任务+外勤签到</td><td>多维度工作报表</td><td>销售人员外勤行为管理</td></tr></tbody></table><h3>品牌亮点可视化（超兔“三一客”小单快单流程）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610849" alt="" title="" loading="lazy"/></p><pre><code>sequenceDiagram
  销售-&gt;&gt;超兔系统: 录入小单线索
  超兔系统-&gt;&gt;销售: 生成三定任务(定人=销售A, 定时=24h内, 定动作=首次触达)
  销售-&gt;&gt;超兔系统: 完成触达并记录客户需求
  超兔系统-&gt;&gt;销售: 触发下一节点(发送报价, 限时12h)
  销售-&gt;&gt;超兔系统: 客户确认报价, 生成订单
  超兔系统-&gt;&gt;销售: 自动标记至“成交”客池, 同步财务数据</code></pre><h2>四、AI智能维度：业务场景的深度赋能对比</h2><h3>核心价值</h3><p>将AI能力嵌入业务全流程，降低人工成本、提升决策精准度与流程效率。</p><h3>横向对比表格</h3><table><thead><tr><th>品牌</th><th>AI核心能力类型</th><th>大模型支持情况</th><th>核心应用场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>自然语言生成工作流+多场景AI助手</td><td>内置AI+Coze工作流调用</td><td>AI待办/AI日报/AI话术/沟通内容意向分析</td></tr><tr><td>Creatio</td><td>跨流程推理智能体</td><td>OpenAI/Anthropic/Gemini自主选择</td><td>客户互动辅助/流程自动化推理</td></tr><tr><td>Keap</td><td>基础营销自动化</td><td>无明确大模型支持</td><td>条件触发内容推送/跟进提醒</td></tr><tr><td>简道云</td><td>智能助手+数据预警</td><td>内置AI模型</td><td>生日关怀触发/数据异常预警</td></tr><tr><td>销帮帮CRM</td><td>AI客户评分+智能工作流</td><td>内置AI模型</td><td>高价值客户识别/售后报销自动化</td></tr><tr><td>快启</td><td>AI获客机器人+大数据分析</td><td>内置AI+大数据模型</td><td>精准线索挖掘/客户意向分析</td></tr></tbody></table><h3>品牌亮点可视化（超兔AI工作流生成逻辑）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610850" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
  A[销售输入自然语言需求: 新线索24h内触达, 未完成提醒主管]
  B[AI解析生成工作流草稿]
  C[用户确认/调整节点权限与限时]
  D[工作流引擎触发执行]
  E{任务完成?}
  F[触发下一节点任务]
  G[自动提醒主管]
  
  A--&gt;B--&gt;C--&gt;D--&gt;E
  E--是--&gt;F
  E--否--&gt;G</code></pre><h2>五、自定义能力维度：业务适配灵活度对比</h2><h3>核心价值</h3><p>根据企业个性化业务需求快速调整系统，适配不同行业与发展阶段的变化。</p><h3>雷达图分值对比（满分10）</h3><ul><li>Creatio：9分 | 超兔一体云：8.5分 | 销帮帮CRM：8分 | 简道云：7.5分 | 快启：7分 | Keap：5分</li></ul><h3>横向对比表格</h3><table><thead><tr><th>品牌</th><th>定制方式</th><th>可定制范围</th><th>代码依赖</th><th>适配场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>功能订阅+可视化配置</td><td>菜单/工作台/业务表/工作流/多表聚合</td><td>无</td><td>全业务一体化定制</td></tr><tr><td>Creatio</td><td>无代码拖拽+设计器</td><td>销售流程/客户档案/业务规则/报表</td><td>无</td><td>中大型企业复杂流程定制</td></tr><tr><td>Keap</td><td>模板化调整</td><td>客户分层/邮件内容/表单</td><td>无</td><td>中小微企业基础需求</td></tr><tr><td>简道云</td><td>零代码拖拽</td><td>表单/流程/仪表盘/行业模板修改</td><td>无</td><td>全规模企业平台化搭建</td></tr><tr><td>销帮帮CRM</td><td>零代码+云叩低代码</td><td>表单/流程/插件/轻量代码编写</td><td>可选低代码</td><td>垂直行业复杂场景</td></tr><tr><td>快启</td><td>可视化配置</td><td>字段/流程/报表/公海规则</td><td>无</td><td>中小企业获客流程定制</td></tr></tbody></table><h2>六、API对接维度：系统集成与生态拓展对比</h2><h3>核心价值</h3><p>打通企业现有系统数据孤岛，实现全业务流程协同与数据共享。</p><h3>横向对比表格</h3><table><thead><tr><th>品牌</th><th>核心集成对象</th><th>特殊集成能力</th><th>API开放程度</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>用友/金蝶ERP、WMS</td><td>RPA对接电商(京东/淘宝)、国税开票</td><td>全开放+官方文档</td><td>RPA机器人解决非标准化系统对接</td></tr><tr><td>Creatio</td><td>遗留系统、Office365、第三方应用</td><td>单一数据库低复杂度集成</td><td>开放API</td><td>跨系统数据无缝流转</td></tr><tr><td>Keap</td><td>邮件系统、日历、支付工具</td><td>基础工具集成</td><td>基础API</td><td>提升流程一致性</td></tr><tr><td>简道云</td><td>企业微信/钉钉/飞书、第三方系统</td><td>Webhook+开放API</td><td>全开放+文档</td><td>办公平台深度整合</td></tr><tr><td>销帮帮CRM</td><td>第三方系统、办公平台</td><td>端到端数据互通</td><td>开放API</td><td>打通数据孤岛</td></tr><tr><td>快启</td><td>ERP、邮件系统、AD域服务器</td><td>标准化API接口(口袋助理)</td><td>开放API</td><td>企业账号信息同步</td></tr></tbody></table><h2>七、最终选型建议</h2><ol><li><strong>中大型全业务一体化需求</strong>：优先选<strong>超兔一体云</strong>，全业务模块整合、多跟单模型、AI工作流驱动及RPA集成能力，支撑企业数字化转型全链路。</li><li><strong>中大型复杂流程定制需求</strong>：选<strong>Creatio</strong>，无代码全定制+多AI模型自主选择+低复杂度集成，满足中大型企业个性化流程与数据主权要求。</li><li><strong>中小微轻量化获客与管理</strong>：选<strong>Keap</strong>，低成本、轻量化自动化+营销获客能力，快速上手解决基础CRM需求。</li><li><strong>零代码平台化搭建需求</strong>：选<strong>简道云</strong>，拖拽式操作+200+行业模板，覆盖CRM及其他业务系统搭建，实现全场景数字化。</li><li><strong>垂直行业</strong> <strong>CRM</strong> <strong>深耕需求</strong>：选<strong>销帮帮CRM</strong>，AI客户评分+公海池管理+云叩低代码平台，适配垂直行业个性化业务规则。</li><li><strong>获客驱动型中小企业</strong>：选<strong>快启</strong>，微信生态深度整合+AI获客机器人+智能公海管理，精准挖掘线索提升获客效率。</li></ol><h2>八、总结</h2><p>企业在选择CRM或数字化平台时，需紧密结合自身规模、业务特性、数字化转型阶段及核心需求优先级决策：中大型企业更需关注全业务协同、复杂流程定制与跨系统集成能力；中小微企业则侧重轻量化部署、低成本获客与快速上手的工具属性。上述六款产品各有核心优势，通过精准匹配自身需求，可有效借助数字化工具提升客户运营效率、销售转化能力与企业整体竞争力。</p>]]></description></item><item>    <title><![CDATA[『n8n』让大模型识别图片内容 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047610852</link>    <guid>https://segmentfault.com/a/1190000047610852</guid>    <pubDate>2026-02-14 11:02:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个n8n小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=Pvg4yrhzL%2FINadKy5gSCfw%3D%3D.rksBdnpMTorIAme0hwYjxcDZbNtFGIGAcnwYTTXym0hjAl4qo94RsPW1z8sjxSANr4%2FC%2F8BSPZxrDxy3fl7i5ttGtZbGuSuYbe8qn9APUGBuzxjD8UP6xV5W4w%2BO574KNSoip9203gFKiuCgPYMRARRXZtoEU6WJ%2BlJKYgNplzs%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a></blockquote><p>有没有发现 n8n 的聊天窗只有一个输入框，好像没有上传文件的功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610854" alt="" title=""/></p><p>那是因为没开启“Allow File Uploads”。双击「When chat message received」节点，在“Parameters”里找到“Options”，点击“Add Field”开启“Allow File Uploads”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610855" alt="" title="" loading="lazy"/></p><p>回到工作流面板，打开聊天窗口就能看到这里有一个上传文件的按钮。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610856" alt="" title="" loading="lazy"/></p><p>但需要你用多模态大模型，才能识别图片内容。我用 Kimi-k2.5 演示一下。</p><p>上传一张小恐龙图片。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610857" alt="" title="" loading="lazy"/></p><p>问它“这张图里的动物是什么？”。</p><p>Kimi-k2.5 很快就识别出来了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610858" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，想了解更多n8n玩法欢迎关注<a href="https://link.segmentfault.com/?enc=%2B4ACNS7fFuTAhVr2H5XLLQ%3D%3D.q%2F5JS%2FFZaxiSUB3JqEQ9o3%2Fj2VOJZWJcYcD25%2BJ5IbUz4NmWaohg3iSyv%2FRKeidv0o1hBE%2BB54yp0LEDISTMInGBR1%2FXGosJ3sEAVqLC9vyXbyJSXYfQLYyezEnJe2wvwbL52hqWSHsZa1ErOBPcbQo%2BGN0r0gA6lyouu8zabUg%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a>👏</p><p>如果你有 NAS，我非常建议你在 NAS 上部署一套 n8n，搞搞副业也好，帮你完成工作任务也好  <a href="https://link.segmentfault.com/?enc=tibJ5z4JDD4LYgmwTKajgw%3D%3D.E6MVRQBedAJiUipaNL2aoZtW6yJpyrEywNstuolHecKavOHiCtsyt%2F0ZQHDXlvyyGlRnq%2B3gDF24ejjm5wr5dQ%3D%3D" rel="nofollow" target="_blank">《『NAS』不止娱乐，NAS也是生产力，在绿联部署AI工作流工具-n8n》</a></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[12C总线和协议 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047610873</link>    <guid>https://segmentfault.com/a/1190000047610873</guid>    <pubDate>2026-02-14 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>在嵌入式开发中，我们经常需要让主控芯片与各种外设进行通信，比如读取温湿度传感器的数据、控制 OLED 显示屏显示内容、读写 EEPROM 存储器等等。</p><p>这时候就需要用到各种通信协议，而 I2C 总线就是其中最常用的一种。</p><p>我刚入行做单片机开发的时候，第一个接触的通信协议就是 I2C，当时用 STM32 读取一个温度传感器，虽然代码不多，但理解其工作原理还是花了不少时间。</p><p>今天就和大家详细聊聊 I2C 总线的方方面面。</p><h2>1. I2C 总线基础知识</h2><h3>1.1 什么是 I2C 总线</h3><p>I2C 总线是由飞利浦公司（现在的 NXP）在 1980 年代开发的一种串行通信总线。</p><p>它最大的特点就是只需要两根信号线就能实现多个设备之间的通信，这两根线分别是 SDA（Serial Data Line，串行数据线）和 SCL（Serial Clock Line，串行时钟线）。</p><p>相比于并行总线需要 8 根、16 根甚至更多的数据线，I2C 总线大大节省了芯片的引脚资源和 PCB 板的布线空间。</p><p>I2C 总线采用主从模式（Master-Slave），通信过程中必须有一个主设备来控制总线，从设备只能被动响应。</p><p>主设备负责产生时钟信号和发起通信，从设备则根据自己的地址来判断是否需要响应。</p><p>一条 I2C 总线上可以挂载多个主设备和多个从设备，理论上最多可以连接 128 个设备（7 位地址模式）或 1024 个设备（10 位地址模式）。</p><h3>1.2 I2C 总线的硬件连接</h3><p>I2C 总线的硬件连接非常简单。</p><p>SDA 和 SCL 都是开漏输出（Open-Drain）或开集输出（Open-Collector），需要外接上拉电阻到电源。</p><p>典型的上拉电阻阻值在 1kΩ 到 10kΩ 之间，具体取值要根据总线电容负载和通信速率来确定。</p><p>总线电容越大、速率越高，上拉电阻就要选得越小。</p><p>开漏输出的特点是只能主动拉低电平，不能主动拉高电平，释放总线后依靠上拉电阻将电平拉高。</p><p>这种设计有两个好处：一是允许多个设备连接到同一条总线上而不会发生电气冲突，二是可以实现不同电压等级设备之间的通信（通过选择合适的上拉电压）。</p><p>在实际项目中，我曾经遇到过一个问题：I2C 通信时好时坏，波形也不太正常。</p><p>后来发现是上拉电阻选得太大了（用的是 10kΩ），总线电容负载比较大，导致信号上升沿太慢。</p><p>换成 2.2kΩ 的电阻后问题就解决了。</p><p>所以硬件设计时一定要注意这个细节。</p><h3>1.3 I2C 总线的速率模式</h3><p>I2C 总线定义了几种不同的速率模式：</p><p>标准模式（Standard Mode）：时钟频率最高 100kHz，这是最早的 I2C 标准，现在很多低速外设仍然使用这个速率。</p><p>快速模式（Fast Mode）：时钟频率最高 400kHz，是目前最常用的速率模式，能够满足大部分应用场景的需求。</p><p>快速模式增强版（Fast Mode Plus）：时钟频率最高 1MHz，用于对速度要求较高的场合。</p><p>高速模式（High Speed Mode）：时钟频率最高 3.4MHz，需要特殊的硬件支持，实际应用中比较少见。</p><p>超快速模式（Ultra Fast Mode）：时钟频率最高 5MHz，这是最新的标准，目前支持的设备还不多。</p><p>在实际开发中，我们最常用的是标准模式和快速模式。</p><p>选择哪种速率主要看外设芯片的支持情况和实际需求。</p><p>如果只是读取一个温度传感器，标准模式完全够用；如果要驱动一个 OLED 显示屏，可能就需要用到快速模式来提高刷新速度。</p><h2>2. I2C 通信协议详解</h2><h3>2.1 起始和停止条件</h3><p>I2C 通信的开始和结束都有特定的信号标志。</p><p>起始条件（Start Condition）是指在 SCL 为高电平期间，SDA 由高电平变为低电平。</p><p>停止条件（Stop Condition）是指在 SCL 为高电平期间，SDA 由低电平变为高电平。</p><p>这两个条件非常重要，它们定义了一次完整通信的边界。</p><p>主设备在发起通信前必须先发送起始条件，通信结束后必须发送停止条件。</p><p>从设备通过检测起始条件来知道通信开始了，通过检测停止条件来知道通信结束了。</p><p>还有一种特殊情况叫做重复起始条件（Repeated Start），就是在一次通信过程中，不发送停止条件，直接再发送一个起始条件。</p><p>这样可以在不释放总线的情况下改变通信方向或切换从设备，常用于连续读写操作。</p><h3>2.1.1 数据传输格式</h3><p>I2C 总线上的数据传输以字节为单位，每个字节都是 8 位。</p><p>数据在 SCL 为低电平期间准备好，在 SCL 为高电平期间被采样。</p><p>数据传输遵循高位在前（MSB First）的原则，也就是先传输最高位。</p><p>每传输完一个字节，接收方都要发送一个应答位（ACK）或非应答位（NACK）。</p><p>应答位是在第 9 个时钟周期内，接收方将 SDA 拉低表示应答，如果 SDA 保持高电平则表示非应答。</p><p>主设备作为接收方时，通常在接收到最后一个字节后发送非应答位，告诉从设备数据传输结束了。</p><h3>2.2 设备地址</h3><p>I2C 总线上的每个从设备都有一个唯一的地址，主设备通过这个地址来选择要通信的从设备。</p><p>标准的 I2C 地址是 7 位，加上 1 位读写位，总共占用一个字节。</p><p>读写位为 0 表示写操作，为 1 表示读操作。</p><p>比如一个 EEPROM 芯片的地址是 0x50（二进制 1010000），当主设备要写数据时，发送的地址字节就是 0xA0（1010000 + 0）；当主设备要读数据时，发送的地址字节就是 0xA1（1010000 + 1）。</p><p>有些 I2C 设备支持 10 位地址模式，这样可以在同一条总线上连接更多设备。</p><p>10 位地址的传输需要两个字节，第一个字节的高 5 位是 11110，后面跟着 10 位地址的最高 2 位和读写位；第二个字节是 10 位地址的低 8 位。</p><p>不过实际项目中，10 位地址模式用得比较少。</p><h3>2.2.1 地址冲突问题</h3><p>在设计系统时，必须确保总线上的每个从设备地址都不相同。</p><p>但有时候会遇到地址冲突的情况，比如需要在同一条总线上连接两个相同型号的传感器，而这两个传感器的地址是固定的。</p><p>解决办法有几种：一是选择支持地址配置的芯片，很多 I2C 设备都有几个地址选择引脚，通过接高电平或低电平可以改变设备地址。</p><p>二是使用 I2C 总线扩展器或多路复用器，将一条总线扩展成多条独立的总线。</p><p>三是如果可能的话，使用软件模拟 I2C，用不同的 GPIO 引脚来连接不同的设备。</p><h3>2.3 完整的通信时序</h3><p>一次完整的 I2C 写操作时序如下：</p><ol><li>主设备发送起始条件</li><li>主设备发送从设备地址和写标志（地址字节的最低位为 0）</li><li>从设备发送应答位</li><li>主设备发送寄存器地址或数据</li><li>从设备发送应答位</li><li>重复步骤 4 和 5，直到所有数据发送完毕</li><li>主设备发送停止条件</li></ol><p>一次完整的 I2C 读操作时序如下：</p><ol><li>主设备发送起始条件</li><li>主设备发送从设备地址和写标志</li><li>从设备发送应答位</li><li>主设备发送要读取的寄存器地址</li><li>从设备发送应答位</li><li>主设备发送重复起始条件</li><li>主设备发送从设备地址和读标志（地址字节的最低位为 1）</li><li>从设备发送应答位</li><li>从设备发送数据</li><li>主设备发送应答位（如果还要继续读）或非应答位（如果这是最后一个字节）</li><li>重复步骤 9 和 10，直到所有数据读取完毕</li><li>主设备发送停止条件</li></ol><p>这个时序看起来比较复杂，但实际使用时，STM32 的 HAL 库已经把这些细节都封装好了，我们只需要调用几个简单的函数就可以完成通信。</p><h2>3. STM32 的 I2C 编程实战</h2><h3>3.1 硬件 I2C 的配置</h3><p>STM32 芯片内部集成了硬件 I2C 控制器，可以自动处理时序、应答等细节，大大简化了编程工作。</p><p>使用 STM32CubeMX 配置 I2C 非常方便，只需要几个步骤：</p><ol><li>在 Pinout &amp; Configuration 页面，找到 I2C 外设（比如 I2C1），点击 Mode，选择 I2C 模式</li><li>系统会自动分配 SDA 和 SCL 引脚，也可以手动修改</li><li>在 Configuration 页面，设置 I2C 参数，主要是时钟速率（比如 100kHz 或 400kHz）</li><li>生成代码</li></ol><p>生成的代码中会有一个初始化函数，类似这样：</p><pre><code>void MX_I2C1_Init(void)
{
  hi2c1.Instance = I2C1;
  hi2c1.Init.ClockSpeed = 100000;  // 时钟速率100kHz
  hi2c1.Init.DutyCycle = I2C_DUTYCYCLE_2;
  hi2c1.Init.OwnAddress1 = 0;
  hi2c1.Init.AddressingMode = I2C_ADDRESSINGMODE_7BIT;
  hi2c1.Init.DualAddressMode = I2C_DUALADDRESS_DISABLE;
  hi2c1.Init.OwnAddress2 = 0;
  hi2c1.Init.GeneralCallMode = I2C_GENERALCALL_DISABLE;
  hi2c1.Init.NoStretchMode = I2C_NOSTRETCH_DISABLE;
  
  if (HAL_I2C_Init(&amp;hi2c1) != HAL_OK)
  {
    Error_Handler();
  }
}</code></pre><h3>3.2 I2C 读写函数</h3><p>HAL 库提供了多个 I2C 通信函数，最常用的有以下几个：</p><pre><code>// 主设备发送数据
HAL_StatusTypeDef HAL_I2C_Master_Transmit(I2C_HandleTypeDef *hi2c, 
                                          uint16_t DevAddress, 
                                          uint8_t *pData, 
                                          uint16_t Size, 
                                          uint32_t Timeout);
​
// 主设备接收数据
HAL_StatusTypeDef HAL_I2C_Master_Receive(I2C_HandleTypeDef *hi2c, 
                                         uint16_t DevAddress, 
                                         uint8_t *pData, 
                                         uint16_t Size, 
                                         uint32_t Timeout);
​
// 向指定寄存器写数据
HAL_StatusTypeDef HAL_I2C_Mem_Write(I2C_HandleTypeDef *hi2c, 
                                    uint16_t DevAddress, 
                                    uint16_t MemAddress, 
                                    uint16_t MemAddSize, 
                                    uint8_t *pData, 
                                    uint16_t Size, 
                                    uint32_t Timeout);
​
// 从指定寄存器读数据
HAL_StatusTypeDef HAL_I2C_Mem_Read(I2C_HandleTypeDef *hi2c, 
                                   uint16_t DevAddress, 
                                   uint16_t MemAddress, 
                                   uint16_t MemAddSize, 
                                   uint8_t *pData, 
                                   uint16_t Size, 
                                   uint32_t Timeout);</code></pre><h3>3.2.1 实战案例：读取 MPU6050 传感器</h3><p>MPU6050 是一个常用的六轴姿态传感器，内部集成了三轴陀螺仪和三轴加速度计，通过 I2C 接口与主控芯片通信。</p><p>它的 I2C 地址是 0x68 或 0x69（取决于 AD0 引脚的电平）。</p><p>下面是一个读取 MPU6050 数据的完整例程：</p><pre><code>#define MPU6050_ADDR 0xD0  // MPU6050地址左移1位（0x68 &lt;&lt; 1）
#define WHO_AM_I_REG 0x75  // WHO_AM_I寄存器地址
#define PWR_MGMT_1_REG 0x6B  // 电源管理寄存器
#define ACCEL_XOUT_H 0x3B  // 加速度X轴高字节寄存器
​
// 初始化MPU6050
uint8_t MPU6050_Init(void)
{
    uint8_t check;
    uint8_t data;
    
    // 读取WHO_AM_I寄存器，检查设备是否存在
    HAL_I2C_Mem_Read(&amp;hi2c1, MPU6050_ADDR, WHO_AM_I_REG, 1, &amp;check, 1, 1000);
    
    if(check == 0x68)  // MPU6050的WHO_AM_I值是0x68
    {
        // 唤醒MPU6050（默认是睡眠模式）
        data = 0;
        HAL_I2C_Mem_Write(&amp;hi2c1, MPU6050_ADDR, PWR_MGMT_1_REG, 1, &amp;data, 1, 1000);
        
        // 设置加速度计量程为±2g
        data = 0x00;
        HAL_I2C_Mem_Write(&amp;hi2c1, MPU6050_ADDR, 0x1C, 1, &amp;data, 1, 1000);
        
        // 设置陀螺仪量程为±250°/s
        data = 0x00;
        HAL_I2C_Mem_Write(&amp;hi2c1, MPU6050_ADDR, 0x1B, 1, &amp;data, 1, 1000);
        
        return 0;
    }
    return 1;
}
​
// 读取加速度数据
void MPU6050_Read_Accel(int16_t *AccelX, int16_t *AccelY, int16_t *AccelZ)
{
    uint8_t data[6];
    
    // 从ACCEL_XOUT_H开始连续读取6个字节
    HAL_I2C_Mem_Read(&amp;hi2c1, MPU6050_ADDR, ACCEL_XOUT_H, 1, data, 6, 1000);
    
    // 组合高低字节
    *AccelX = (int16_t)(data[0] &lt;&lt; 8 | data[1]);
    *AccelY = (int16_t)(data[2] &lt;&lt; 8 | data[3]);
    *AccelZ = (int16_t)(data[4] &lt;&lt; 8 | data[5]);
}
​
// 主函数中的使用示例
int main(void)
{
    HAL_Init();
    SystemClock_Config();
    MX_GPIO_Init();
    MX_I2C1_Init();
    
    int16_t accel_x, accel_y, accel_z;
    
    if(MPU6050_Init() == 0)
    {
        while(1)
        {
            MPU6050_Read_Accel(&amp;accel_x, &amp;accel_y, &amp;accel_z);
            
            // 这里可以对数据进行处理或显示
            // printf("X: %d, Y: %d, Z: %d\r\n", accel_x, accel_y, accel_z);
            
            HAL_Delay(100);  // 延时100ms
        }
    }
    else
    {
        // MPU6050初始化失败
        while(1)
        {
            // 错误处理
        }
    }
}</code></pre><p>这个例程展示了 I2C 通信的典型流程：先初始化设备，然后循环读取数据。</p><p>需要注意的是，HAL 库的 I2C 地址参数需要左移 1 位，因为库函数会自动添加读写位。</p><h3>3.3 软件模拟 I2C</h3><p>有时候硬件 I2C 引脚被占用了，或者需要在任意 GPIO 上实现 I2C 通信，这时候可以用软件模拟 I2C。</p><p>虽然软件模拟的效率不如硬件 I2C，但胜在灵活性高，而且对于低速设备来说完全够用。</p><p>软件模拟 I2C 的核心是用 GPIO 来产生 I2C 时序。下面是一个简单的实现：</p><pre><code>// 定义SDA和SCL引脚
#define I2C_SCL_PIN GPIO_PIN_6
#define I2C_SCL_PORT GPIOB
#define I2C_SDA_PIN GPIO_PIN_7
#define I2C_SDA_PORT GPIOB
​
// 延时函数（用于控制时钟速率）
void I2C_Delay(void)
{
    uint8_t i = 10;  // 调整这个值可以改变速率
    while(i--);
}
​
// 设置SDA为输出模式
void SDA_OUT(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    GPIO_InitStruct.Pin = I2C_SDA_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_OUTPUT_OD;
    GPIO_InitStruct.Pull = GPIO_NOPULL;
    GPIO_InitStruct.Speed = GPIO_SPEED_FREQ_HIGH;
    HAL_GPIO_Init(I2C_SDA_PORT, &amp;GPIO_InitStruct);
}
​
// 设置SDA为输入模式
void SDA_IN(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    GPIO_InitStruct.Pin = I2C_SDA_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_INPUT;
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(I2C_SDA_PORT, &amp;GPIO_InitStruct);
}
​
// 产生起始条件
void I2C_Start(void)
{
    SDA_OUT();
    HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_SET);
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_SET);
    I2C_Delay();
    HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_RESET);
    I2C_Delay();
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);
}
​
// 产生停止条件
void I2C_Stop(void)
{
    SDA_OUT();
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);
    HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_RESET);
    I2C_Delay();
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_SET);
    I2C_Delay();
    HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_SET);
    I2C_Delay();
}
​
// 发送一个字节
void I2C_Send_Byte(uint8_t byte)
{
    uint8_t i;
    SDA_OUT();
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);
    
    for(i = 0; i &lt; 8; i++)
    {
        if(byte &amp; 0x80)
            HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_SET);
        else
            HAL_GPIO_WritePin(I2C_SDA_PORT, I2C_SDA_PIN, GPIO_PIN_RESET);
        
        byte &lt;&lt;= 1;
        I2C_Delay();
        HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_SET);
        I2C_Delay();
        HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);
    }
}
​
// 等待应答
uint8_t I2C_Wait_Ack(void)
{
    uint8_t ack;
    SDA_IN();
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_SET);
    I2C_Delay();
    
    if(HAL_GPIO_ReadPin(I2C_SDA_PORT, I2C_SDA_PIN))
        ack = 1;  // 无应答
    else
        ack = 0;  // 有应答
    
    HAL_GPIO_WritePin(I2C_SCL_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);
    return ack;
}</code></pre><p>软件模拟 I2C 的代码比较长，这里只列出了部分关键函数。</p><p>完整的实现还需要接收字节、发送应答等函数。</p><p>虽然代码量比较大，但原理很清晰，就是严格按照 I2C 时序来操作 GPIO 引脚。</p><h2>4. I2C 使用中的常见问题</h2><h3>4.1 通信失败的排查</h3><p>在实际项目中，I2C 通信失败是很常见的问题。</p><p>遇到这种情况，可以按照以下步骤排查：</p><p>第一步，检查硬件连接。</p><p>用万用表测量 SDA 和 SCL 是否正常，静态时应该是高电平（上拉电阻的作用）。</p><p>如果是低电平，可能是某个设备把总线拉低了，或者上拉电阻没接好。</p><p>第二步，检查设备地址。</p><p>很多初学者会忘记地址要左移 1 位，或者把读写位搞混了。</p><p>可以用逻辑分析仪抓取波形，看看实际发送的地址是否正确。</p><p>第三步，检查时序。</p><p>有些 I2C 设备对时序要求比较严格，如果时钟速率太高或者延时不够，可能导致通信失败。</p><p>可以尝试降低时钟速率，或者在关键位置增加延时。</p><p>第四步，检查设备状态。</p><p>有些设备需要先初始化才能正常工作，比如 MPU6050 默认是睡眠模式，必须先写电源管理寄存器唤醒它。</p><p>仔细阅读设备的数据手册，按照要求进行初始化。</p><h3>4.2 总线冲突和仲裁</h3><p>当多个主设备同时发起通信时，可能会发生总线冲突。</p><p>I2C 协议定义了仲裁机制来解决这个问题：每个主设备在发送数据的同时监测总线状态，如果发现总线电平与自己发送的不一致，就说明有其他设备也在发送数据，这时候要立即停止发送，让出总线。</p><p>仲裁过程是按位进行的。</p><p>由于 I2C 是开漏输出，低电平会覆盖高电平，所以发送低电平的设备会赢得仲裁。</p><p>比如设备 A 发送地址 0x50（01010000），设备 B 发送地址 0x48（01001000），在第 5 位时，A 发送 1 但检测到 0，就知道自己输掉了仲裁，会停止发送。</p><p>不过在实际应用中，多主设备的情况比较少见。</p><p>如果确实需要多个主设备，要做好软件设计，避免同时发起通信，或者使用仲裁机制来处理冲突。</p><h3>4.3 时钟延展</h3><p>I2C 协议允许从设备在需要更多时间处理数据时，通过拉低 SCL 来延长时钟周期，这叫做时钟延展（Clock Stretching）。</p><p>主设备在拉高 SCL 后，必须检测 SCL 是否真的变成高电平，如果 SCL 被从设备拉低了，就要等待从设备释放 SCL。</p><p>有些 STM32 的硬件 I2C 控制器支持时钟延展，有些不支持。</p><p>如果不支持，遇到需要时钟延展的从设备就可能出现问题。</p><p>这时候可以尝试用软件模拟 I2C，或者在软件中实现时钟延展检测。</p><h3>4.4 电磁干扰</h3><p>I2C 总线的信号频率不高，但在强电磁干扰环境下仍然可能出现通信错误。</p><p>我之前做过一个项目，设备在实验室测试时一切正常，但到了工业现场就频繁出现通信失败。</p><p>后来发现是附近有大功率电机，产生了很强的电磁干扰。</p><p>解决电磁干扰问题的方法有：缩短 I2C 总线长度，理想情况下不要超过 1 米。</p><p>在 SDA 和 SCL 上串联小电阻（比如 100Ω），可以抑制高频干扰。</p><p>使用屏蔽线或双绞线。</p><p>在软件中增加重试机制，检测到通信错误时自动重试。</p><h2>5. 总结</h2><p>I2C 总线是嵌入式系统中最常用的通信协议之一，它结构简单、使用方便、节省引脚，非常适合连接各种低速外设。</p><p>掌握 I2C 的工作原理和编程方法，是每个嵌入式工程师的必备技能。</p><p>在实际开发中，我们既要理解 I2C 的底层时序，也要会使用 HAL 库等高层接口。</p><p>遇到问题时，要善于用逻辑分析仪等工具来分析波形，结合数据手册来排查原因。</p><p>只要多实践、多总结，很快就能熟练掌握 I2C 通信。</p><p>希望这篇文章能帮助大家更好地理解和使用 I2C 总线。</p><p>如果你在项目中遇到了 I2C 相关的问题，欢迎留言交流讨论。</p><p><strong>更多编程学习资源</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=CcSPK%2BTpOAL5UMXqgar7tA%3D%3D.EKrOrr4%2BtQCLVIi1%2FttxcPnhAV57nm3ycgszBj%2BpTCS4aU6llMVgkBgzTV323BkUCGmQFu%2Bb%2FuQVLc2JAnHzag%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=eYV4%2B%2FklTKiOosRByB1KvA%3D%3D.bv2oLfecl0spwtYRlBOOBoeTIZir5hbJl9CkICmjHiSRqhOpLDm%2BrPSo%2BlFszyqA6sEgiLrrR4igbumuBKtaUw%3D%3D" rel="nofollow" target="_blank">STM32 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=1wEvodf65sogBLfNKhEz2w%3D%3D.f9u166sphCj8tArID13vre%2B%2BbHopZvn2jZdfW2H%2FTbx7eZLeb3BuEWrRw6R3rH83ZIrSrFLEJawm2HmGXcL6xN2cWtC9O687fawNsfZSQO8%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=I%2FLUiXRZx64Brxtjmo8Z%2BQ%3D%3D.9hujXqoFmDcQ7oBVoFI006K7GpKmQs%2F3CHyFSD3jkZONEjBgzB2nCl7DfME81nucBaD%2BqbU3j2YELl9XMY0AsA%3D%3D" rel="nofollow" target="_blank">C++ 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=Iukx1wlFR52ssVtciwy1LA%3D%3D.CnoxE%2BKI1LoqLhD%2BM4vt7I1r0fNPzSKeAkdhEr4mt88seRnswovksJ4AERUAGB%2F1ulhOaDXN6W3bAozXGC7Tlg%3D%3D" rel="nofollow" target="_blank">51 单片机零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=TZ1CH96eX%2F0y6zyTRsxjpA%3D%3D.8JsQ%2BD60183H1PEdZ513gd2jnLWgzbJ0K%2FlzaP%2BS7T5WNTCGg%2FxcjRBGqYHNfCQvaFT4cTosuqDpzBb2bd6JfQ%3D%3D" rel="nofollow" target="_blank">AD 画板零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=8tblNrG5tqKn0U9RkHvdzQ%3D%3D.PenquXLH5zTqlDG7hd0AtiFx9mvqui084QlhTRa7a866yapKmYDbvqsYD4AKITURP6e64a3SIpOLxMkV5Ofi5Q%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=G8xw6jeonSab6BxvbnMrcw%3D%3D.iiDMRbU%2BTnX9FHMMOUm28bGEjENPKLcmJ1OUch9JtyE3W6dsKxACiHa%2BecHhe9qtJoaGffyb2cLrPA4DKgKO5Q%3D%3D" rel="nofollow" target="_blank">C++ 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=KznOqBvZ4HTQqnvrLyoT6Q%3D%3D.tI5PT7oWiQ8X8awp6DmDpIXWw12SxlMQYm8WczObHSwxNeWuwyU3yhy6Z4YUiwIvx%2FLjv5HNH8GxQI9xg2HeSQRBE5O6pfFVRb%2F31PGYb8Q%3D" rel="nofollow" target="_blank">ESP32 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=shXD9WbZ%2Bfrs7%2BY80OkCiw%3D%3D.dZKsXGe4xGGD803VLiin6bj6pyKcD4zQ1ZxtCCGSLmIHe7yldE9y7ObI%2BJq4aucBVc8%2BvTenqunEepl6arGT9YLIrWI9GvbXHHCYXgEd%2FFI%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=XAme4uqgEZSicgKno5tYOg%3D%3D.5%2FtuUn5B3aJkDsAJNYWU%2FMxppIPRslmAE8%2FR274B3Ck4IOCOX%2BaPCsmd4Mti2cI72%2F%2FZcMmYVVybq%2F9E1Zv%2BbdFH%2FOhx5RYGtgGx8a7o6m4%3D" rel="nofollow" target="_blank">Linux 应用开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=Bbj4%2FqpsofP4Xm42dW2s%2Fw%3D%3D.neTIJLWvpRApvP6AbxYkYYGQvPX%2Bi%2FXXAf9RZEtUeg%2BlELuIYkqR0k7Zb9afoNAQeVMWKKB1verzb36VaCxonJxiQYeUsJv%2B%2BM3f7ysZkNo%3D" rel="nofollow" target="_blank">Linux 底层开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=VUd0TAn49902%2FZ1i5LBPjA%3D%3D.1m5O298SjONkP9NReXrFvsokdUHBgB1H25v%2BG07cCrMwj8%2F8PRPczbjEqAipoN8uxfMal1WeWh1Gcq6fHjvfQw%3D%3D" rel="nofollow" target="_blank">LVGL 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=Nr4Iqsq%2BQJ%2FsHtOe2pjKzQ%3D%3D.wp6m3wuE0RyWSI1X0srzjlJBKFPG0Ye3A3yjoa9fuVgZ7FL4ZFkfhKR8%2FSE%2B21m17pEkvMCInUdjDa9twJzx6g%3D%3D" rel="nofollow" target="_blank">QT 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=XZgzWpHDnByyGCflUSQ53g%3D%3D.3b97xsUHk1UwFnVLPMqr1uK1L93XE%2FROMKIqyelY2%2Blynf3O%2FV1LUjBqMzSAlaewfiWiM2mviD1DU7zX4BQAbhcCg7bzDQTXDVB5kdNfKIE%3D" rel="nofollow" target="_blank">STM32 零基础入门学习路线</a></li></ul>]]></description></item><item>    <title><![CDATA[【节点】[CustomDiffuse节点]原理解析与实际应用 SmalBox ]]></title>    <link>https://segmentfault.com/a/1190000047610759</link>    <guid>https://segmentfault.com/a/1190000047610759</guid>    <pubDate>2026-02-14 10:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=WYzTD1UwAS4izdwOMWMx8Q%3D%3D.qaGZyuEai%2BisDtsu%2Fd%2FlYMbEhyje4JC9zXZ%2B%2BY5maXS2WA1XjjAhiHJIBv%2Ba20pimX%2BYtqz6pJwto2ao616p6Uo0gWSwlo5pqJ7PDQ0l9uX2KuDyVfWiglFND4%2BRV5l2aeJIYzLx3PsE5KmnjMd%2F1xF%2FW%2FxoCVlTrg7rYncWCqCYAbnZvAMfv3ae3ZGPsQGjlSOL2r9pGw6aWlhH661JuPY%2FUCTkkPFWAbaawNZz774%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong></blockquote><h2><strong>描述</strong></h2><p>CustomDiffuse节点是Unity URP Shader Graph中一个功能强大的光照计算节点，专门用于实现用户自定义的固有色光照效果。该节点为着色器开发者提供了高度灵活的光照控制能力，允许用户基于物理的渲染原则或艺术化的视觉需求来定义材质的漫反射行为。在实时渲染管线中，漫反射光照是表面着色的基础组成部分，它决定了材质在直接光照下的基本外观特征。</p><p>CustomDiffuse节点的核心价值在于其可定制性。与标准的Lambert或Oren-Nayar漫反射模型不同，这个节点不强制使用特定的光照算法，而是将光照计算的各个要素作为输入端口开放给用户。这种设计理念使得开发者能够根据项目特定的视觉风格或性能要求，实现从简单的N·L点积计算到复杂的自定义BRDF模型。</p><p>在实际应用场景中，CustomDiffuse节点特别适合那些需要特殊材质表现的场合。比如在风格化渲染中，艺术家可能希望实现非真实感的漫反射过渡，或者在特定类型的表面（如丝绸、绒毛等）上实现物理准确的散射效果。通过组合不同的输入数据和自定义计算逻辑，开发者可以精确控制光线与材质表面的交互方式。</p><p>该节点的另一个重要特性是其与URP渲染管线的深度集成。它能够正确处理URP中的多光源设置、光照衰减和阴影信息，确保自定义的漫反射计算能够与引擎的其他渲染组件协同工作。这种集成保证了即使在复杂的场景光照条件下，自定义的漫反射效果也能保持视觉一致性和性能稳定性。</p><h2><strong>端口</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610761" alt="" title=""/></p><h3>输入端口详解</h3><p>Diffuse输入端口接收Vector 3类型的数据，代表材质的基础固有色信息。这个端口通常连接到材质的Albedo纹理或基础颜色属性。在物理渲染上下文中，Diffuse输入应该表示材质表面对漫反射光的反射率系数，其数值范围通常在0到1之间。对于高质量的渲染结果，建议使用线性空间颜色值，并确保颜色值符合能量守恒原则。</p><p>Light Color输入端口提供灯光本身的颜色信息，这是实现准确色彩再现的关键要素。在URP中，不同类型的灯光（方向光、点光源、聚光灯）都会提供其颜色和强度信息。开发者可以利用这个端口实现各种创意效果，比如通过修改灯光颜色来模拟特殊的光照环境，或者根据表面特性对灯光颜色进行过滤处理。</p><p>Light Attenuation端口处理光照的衰减和阴影信息，这是实现真实光照效果的重要组成部分。该输入通常来自Shader Graph中的光照衰减节点，包含了距离衰减、角度衰减以及实时阴影数据。对于高级用法，开发者可以结合Shadowmask和光照探针数据来实现更复杂的光照交互效果。</p><p>Normal WS端口要求世界空间下的法线向量输入，这是计算光照方向性的基础。正确的法线数据对于任何基于物理的光照模型都至关重要。在实际使用中，法线信息可以来自顶点法线、法线贴图，或者是通过自定义计算生成的修改法线。确保法线向量为单位长度是获得准确光照结果的必要前提。</p><p>Light Direction WS端口提供从表面点到光源的方向向量，同样在世界空间下表示。这个向量通常通过标准化处理，并且指向光源的方向。在多点光源场景中，需要为每个光源分别计算其方向向量。对于方向光，这个方向是恒定的；而对于点光源和聚光灯，则需要基于片元位置实时计算。</p><h3>输出端口特性</h3><p>Out输出端口生成最终的自定义漫反射照明结果，以Vector 3形式表示RGB颜色值。这个输出可以直接用于后续的光照计算，或者与其他光照组件（如高光反射、环境光等）进行混合。输出的颜色值应该保持在合理的范围内，避免出现HDR效果，除非后续有适当的色调映射处理。</p><h3>端口交互与数据流</h3><p>理解这些端口之间的数据流关系对于有效使用CustomDiffuse节点至关重要。典型的数据处理流程开始于Diffuse和Light Color的乘法组合，这建立了基础的色彩响应。接着通过法线和光照方向的点积计算获得基础的漫反射强度，再结合光照衰减因子来模拟距离和阴影的影响。</p><p>在实际的着色器构建过程中，这些端口的连接顺序和数据处理方式可以根据需求灵活调整。例如，在某些卡通渲染风格中，可能会在计算N·L点积后添加一个步进函数来创建硬边缘的阴影过渡。而在追求物理准确性的场景中，则可能使用更复杂的函数来模拟表面粗糙度对漫反射的影响。</p><h2><strong>核心算法原理</strong></h2><h3>基础光照模型</h3><p>CustomDiffuse节点的默认行为基于经典的Lambertian漫反射模型，这是计算机图形学中最基础且广泛应用的光照模型之一。Lambert模型的核心理念是表面反射的光线强度与入射光线方向和表面法线夹角的余弦值成正比。数学表达式为：Diffuse = Albedo × LightColor × max(0, N·L)，其中N·L表示法向量与光照方向向量的点积。</p><p>这个简单的模型虽然物理上不够精确，但在实时渲染中因其计算效率和直观性而被广泛使用。它假设表面是理想的漫反射体，在各个观察方向上呈现相同的亮度。在实际实现中，max(0, N·L)操作确保了当光线从表面后方照射时不会产生负值光照，这是符合物理直觉的约束。</p><h3>高级漫反射模型</h3><p>对于需要更高质量渲染效果的项目，CustomDiffuse节点可以扩展实现更先进的漫反射模型。Oren-Nayar模型是一个著名的改进，它考虑了表面粗糙度对漫反射的影响。与Lambert模型不同，Oren-Nayar不假设表面是完美漫反射体，而是通过粗糙度参数模拟微表面细节对光线的散射效应。</p><p>另一个值得关注的模型是Disney principled BRDF中的漫反射组件，它结合了多种散射效应以提供更加物理准确的结果。这种模型通常包含次表面散射的近似模拟，能够更好地表现诸如布料、皮肤等特殊材质的视觉特性。</p><h3>能量守恒考虑</h3><p>在实现自定义漫反射模型时，能量守恒是一个重要的物理原则。它要求表面反射的光线总能量不能超过入射光线的能量。在着色器设计中，这意味着漫反射、镜面反射和其他光能传输组件的总和应当合理约束。通过CustomDiffuse节点，开发者可以精确控制漫反射组件的能量分配，确保渲染结果的物理合理性。</p><h2><strong>实际应用示例</strong></h2><h3>基础Lambert漫反射实现</h3><p>创建一个基础的Lambert漫反射效果是理解CustomDiffuse节点用法的理想起点。首先需要在Shader Graph中创建相应的节点网络：</p><ul><li>将Albedo纹理或颜色属性连接到Diffuse输入端口</li><li>使用URP中的Main Light节点获取主光源的颜色和方向信息</li><li>通过Transform节点将物体空间法线转换到世界空间</li><li>计算法线与光照方向的点积，并使用Saturate节点限制结果在0-1范围内</li><li>将点积结果与光源颜色和Albedo颜色相乘，得到基础的漫反射输出</li></ul><p>这种实现方式虽然简单，但已经能够为大多数实体材质提供可信的漫反射效果。它是许多游戏和交互应用中漫反射计算的基础。</p><h3>风格化卡通渲染</h3><p>在非真实感渲染中，CustomDiffuse节点可以创造出各种艺术化的光照效果。卡通渲染通常特征化地使用硬阴影边界和有限的颜色过渡。实现这种效果的关键在于对N·L点积结果进行离散化处理：</p><ul><li>使用Remap节点调整点积的范围和分布</li><li>通过Posterize节点或自定义的步进函数创建离散的光照级别</li><li>可以添加边缘光效果，通过在法线与视角方向接近垂直时添加额外的光照项</li><li>结合阴影色阶，使用多个CustomDiffuse节点分别处理不同光照区域的颜色</li></ul><p>这种技术广泛应用于动漫风格的游戏和媒体作品中，能够创造出鲜明、富有表现力的视觉风格。</p><h3>布料和毛发特殊材质</h3><p>某些材质类型需要特殊的漫反射处理来准确表现其视觉特性。布料材质通常表现出逆向的反射特性——当光照方向与观察方向相反时反而显得更亮。这种效果可以通过在CustomDiffuse节点中实现Wrap Lighting模型来实现：</p><ul><li>修改标准的N·L计算，添加一个偏移量：diffuse = saturate((N·L + w) / (1 + w))</li><li>其中w参数控制包裹效果的强度，典型值在0到1之间</li><li>对于绒毛材质，可以使用sheen项模拟边缘处的背光散射效果</li></ul><p>这些高级用法展示了CustomDiffuse节点在实现特定材质特性时的灵活性和强大功能。</p><h2><strong>性能优化建议</strong></h2><h3>计算复杂度管理</h3><p>在使用CustomDiffuse节点实现复杂光照模型时，需要注意计算性能的平衡。实时渲染对着色器的计算效率有严格要求，特别是在移动平台或VR应用中。以下是一些优化建议：</p><ul><li>尽可能使用最简单的光照模型满足视觉需求</li><li>避免在CustomDiffuse计算中使用复杂的数学函数如sin、pow等</li><li>考虑使用近似计算代替精确但昂贵的运算</li><li>对于静态物体，可以考虑将部分光照信息烘焙到光照贴图中</li></ul><h3>平台特定优化</h3><p>不同硬件平台对着色器计算的能力和限制各不相同。在针对多平台开发时，需要特别关注：</p><ul><li>移动平台通常对分支语句和复杂纹理查询更加敏感</li><li>在性能受限的情况下，可以考虑使用更低的计算精度（half代替float）</li><li>某些平台可能对特定类型的数学运算有硬件加速，可以优先使用这些运算</li></ul><h3>光照模型简化策略</h3><p>当项目面临性能压力时，可以考虑以下简化策略：</p><ul><li>使用预计算的查找纹理（LUT）替代实时复杂计算</li><li>将部分每像素计算转移到每顶点计算</li><li>在远距离或小尺寸物体上使用简化的光照模型</li><li>利用URP的着色器变体功能，为不同质量设置提供不同复杂度的实现</li></ul><h2><strong>常见问题与解决方案</strong></h2><h3>光照不一致问题</h3><p>在使用CustomDiffuse节点时，可能会遇到不同光源条件下光照效果不一致的问题。这通常是由于没有正确处理多光源环境或光照空间转换错误导致的：</p><ul><li>确保所有向量计算在相同的坐标空间中进行（通常推荐世界空间）</li><li>检查法线向量的长度是否为单位长度，非单位法线会导致错误的光照计算</li><li>验证光照方向向量是否正确指向光源，对于点光源需要基于片元位置计算方向</li></ul><h3>阴影衔接问题</h3><p>自定义漫反射模型与URP阴影系统的集成可能会产生视觉瑕疵，特别是在阴影边界处：</p><ul><li>确保Light Attenuation输入正确包含了阴影信息</li><li>在自定义模型中考虑阴影柔和度与漫反射过渡的协调性</li><li>可以使用阴影颜色调制来改善阴影区域的艺术表现</li></ul><h3>HDR和颜色管理</h3><p>在高动态范围渲染中，CustomDiffuse节点的输出可能需要特殊处理：</p><ul><li>注意颜色值范围，避免在未经色调映射的情况下输出HDR值</li><li>在线性颜色空间下进行所有光照计算，确保物理准确性</li><li>对于特别明亮的光源，可能需要单独处理以避免颜色过饱和</li></ul><h2><strong>高级技巧与创意应用</strong></h2><h3>动态材质效果</h3><p>CustomDiffuse节点不仅可以处理静态光照计算，还可以实现各种动态效果：</p><ul><li>基于时间或顶点位置调制漫反射颜色，创建动态变化的表面外观</li><li>结合噪声纹理模拟表面污染、磨损等随时间变化的效果</li><li>使用世界空间坐标实现与场景位置相关的材质变化</li></ul><h3>非真实感渲染技术</h3><p>除了传统的真实感渲染，CustomDiffuse节点在NPR领域也有广泛应用：</p><ul><li>实现水墨画风格的渐变控制，通过自定义的过渡函数</li><li>创建素描效果，使用hatching纹理基于光照强度进行混合</li><li>模拟油画笔触，结合噪声和方向性光照响应</li></ul><h3>特殊场景应用</h3><p>在某些特定类型的场景中，CustomDiffuse节点可以提供针对性的解决方案：</p><ul><li>在水下环境中模拟光线的吸收和散射效应</li><li>在雾霭场景中实现距离相关的颜色衰减</li><li>为雪地或沙漠等高反射环境创建特殊的光照响应</li></ul><hr/><blockquote><a href="https://link.segmentfault.com/?enc=nWdVBVK1QoXujEjtq%2FQSzA%3D%3D.3%2BjyIXcslFZzbCUPnkMCeBYQcQs0zHCawK8TMKR6fU1M%2F4s5qITLoCZkdPPg%2FpOs3t8BfdOoMiziBe6Wstk1fEw3iClq6zxqE9%2BtlI6BPDdNpjlLoAfnPr4BCbtkoMiJLNMHJDCwLdldSbVbRRqdeBfPqvlgog6FnQ7l4CPRDdeKG%2F5Qw5WVlyOAkx47aQgkSm8%2B9viH4op0TIdsoQymncePQcattipa7cOfevEPYdc%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[如何在 Linux 中将文件复制到多个目录 ？ 本文系转载，阅读原文
https://www.koo]]></title>    <link>https://segmentfault.com/a/1190000047610765</link>    <guid>https://segmentfault.com/a/1190000047610765</guid>    <pubDate>2026-02-14 10:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610767" alt=" Copy a File to Multiple Directories in Linux" title=" Copy a File to Multiple Directories in Linux"/></p><p>要将文件复制到 Linux 中的多个目录，可以使用 <code>cp</code> 和 <code>xargs</code> 命令。所有目标目录都将作为标准输入管道连接到 <code>xargs</code> 命令，示例如下：</p><pre><code>echo dir1 dir2 dir3 | xargs -n 1 cp -v file.txt</code></pre><p>这将复制文件 file.txt 到 dir1，dir2 和 dir3 目录。</p><p>或者，使用 for 循环将文件复制到多个目录，示例如下：</p><pre><code>for dir in dir1 dir2 dir3; do
    cp file.txt $dir
done</code></pre><p>也可以使用 find 命令将文件复制到多个目录，示例如下：</p><pre><code>find dir1 dir2 dir3 -type d -exec cp file.txt {} \;</code></pre><p><strong>注意：</strong> 确保您具有将文件复制到目标目录的必要权限。</p><h3>我的开源项目</h3><p><a href="https://link.segmentfault.com/?enc=OSsAEFjZNqe5KCnSueujUg%3D%3D.v%2BhCrrGGYXv55jEBpiZ0XTg4W7JjL9l%2BumQ0snDUMN4%3D" rel="nofollow" target="_blank"><img referrerpolicy="no-referrer" src="/img/remote/1460000043426502" alt="酷瓜云课堂-开源知识付费解决方案" title="酷瓜云课堂-开源知识付费解决方案" loading="lazy"/></a></p><ul><li><a href="https://link.segmentfault.com/?enc=u9nY0e6kVQEelyqJKotjuw%3D%3D.we6e40RZK17VXAsek3DMVlO1oFl55lEOTROJ8cew%2FCo223c9ZIuYO7JBjGsQkueL" rel="nofollow" target="_blank">course-tencent-cloud（酷瓜云课堂 - gitee仓库）</a></li><li><a href="https://link.segmentfault.com/?enc=nYL%2Fo%2FXJmrSko2D%2FrGoV1Q%3D%3D.xSAY9zB3Diaa0tflEF2qWuFK%2Bf2OJd3MsQ3ybsT%2FbT5f%2FIiNlecnn2X2LfQ7LqGmpcudpsWazqiBpCPkba2E6g%3D%3D" rel="nofollow" target="_blank">course-tencent-cloud（酷瓜云课堂 - github仓库）</a></li></ul>]]></description></item><item>    <title><![CDATA[Fatal error: require(): Failed opening required 以及]]></title>    <link>https://segmentfault.com/a/1190000047610770</link>    <guid>https://segmentfault.com/a/1190000047610770</guid>    <pubDate>2026-02-14 10:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>“Fatal error: require(): Failed opening required...” 以及如何彻底避免它再次出现</h2><p>凌晨两点，值班告警响了。生产环境 API 开始报 500，而且只出现在新扩容的节点上。你打开日志，熟悉又刺眼的报错跳了出来：</p><p>本地一切正常，测试环境也没问题。但在云原生部署这种“环境随时变化”的现实里，一个看起来不起眼的路径差异，就足以把服务直接打趴。</p><p>这并不是什么“新手失误”，而是很多人对 PHP 最基础能力——文件加载机制——理解不够深入导致的系统性问题。</p><p>早期 PHP 时代，我们把 <code>include</code> 和 <code>require</code> 当积木用来拼页面。到了 PHP 8.2+、Composer、容器化微服务的今天，这组函数仍然在引擎核心位置。但现实中，很多开发者依旧把它们当成“设完就不用管”的工具。</p><p>如果你想从“写脚本”走向“做稳定系统”，就必须搞清楚：当一个文件被加载进另一个文件时，底层到底发生了什么。</p><p>这篇文章会从运行机制、线上常见坑和工程实践三层，讲清楚怎样把 PHP 文件加载写到足够稳。</p><h3>底层到底在发生什么？</h3><p>当你执行 <code>include 'file.php'</code>，并不是“复制粘贴代码”这么简单。PHP 实际上会让当前执行流程暂停，切换到目标文件，把它编译为操作码，再在当前作用域里执行。</p><h4>文件加载的四种形式</h4><p>PHP 有四种主加载方式，它们不是语法糖，而是行为差异：</p><ul><li><code>include</code>：温和模式。文件不存在时抛 <code>Warning</code>，脚本继续执行。</li><li><code>require</code>：强制模式。文件不存在时直接致命错误并中断执行。</li><li><code>include_once</code> / <code>require_once</code>：在前两者基础上增加“是否已加载”检查，避免重复声明。</li></ul><p>理解这个差异非常关键：在现代业务系统里，很多核心依赖一旦缺失，不应该“带伤继续跑”。</p><h4>一个更实用的心智模型：作用域注入器</h4><p>可以把文件加载理解成“作用域注入器”：</p><ul><li>在函数内部 <code>include</code>，被加载文件里定义的变量只在该函数作用域可见。</li><li>在脚本顶层 <code>include</code>，变量会进入全局作用域。</li></ul><p>另外，很多人误判性能瓶颈。真正重的通常不是代码执行本身，而是文件状态检查（stat 调用）：</p><p>每次 <code>include</code>，PHP 都要向操作系统确认：文件是否存在、权限是否可读、最后修改时间等。在高并发 API 中，这个动作每秒成千上万次时，开销会非常明显。</p><h3>PHP 是如何解析路径的</h3><p>当你写 <code>include 'utils.php';</code> 这种相对路径时，PHP 会依次尝试：</p><ul><li>当前脚本目录</li><li><code>php.ini</code> 中 <code>include_path</code> 指定的目录</li><li>当前工作目录（cwd）</li></ul><p>问题就出在这里：它有环境依赖。</p><p>比如你的命令行任务进程工作目录是 <code>/var/www/</code>，而 Web 进程工作目录是 <code>/var/www/public/</code>，同一行相对路径代码可能一个能跑、一个直接崩。</p><h3>最容易把线上搞崩的 5 类错误</h3><p>这些是我在遗留项目重构里反复见到的高频问题。</p><h4>相对路径陷阱</h4><p><strong>错误写法</strong>：<code>include 'includes/header.php';</code></p><p><strong>为什么会发生</strong>：本地启动目录刚好是项目根目录，所以一直“看起来正常”。</p><p><strong>线上后果</strong>：一旦被子目录调用、被定时任务调用，或者入口目录变了，路径上下文就变了。这是“我本地没问题”类事故的头号来源。</p><h4><code>_once</code> 的性能税</h4><p><strong>错误写法</strong>：在高频循环里大量使用 <code>require_once</code>。</p><p><strong>为什么会发生</strong>：担心 <code>Cannot redeclare class</code> 之类的重复声明。</p><p><strong>线上后果</strong>：每次 <code>_once</code> 都会触发已加载表检查。PHP 8 虽然优化了很多，但它依然比直 <code>require</code> 慢。依赖关系清晰的模块化系统，不该长期依赖引擎“二次确认”。</p><h4>用 <code>@</code> 把报错静音</h4><p><strong>错误写法</strong>：<code>@include 'optional_config.php';</code></p><p><strong>为什么会发生</strong>：想省掉 <code>if (file_exists(...))</code> 的显式判断。</p><p><strong>线上后果</strong>：你把真正问题藏起来了。文件读取失败可能不是“文件不存在”，而是权限不对（如 <code>chmod</code>）。报错被吃掉后，排障时间会从 5 分钟拉到几小时。</p><h4>动态 include 引发路径穿越</h4><p><strong>错误写法</strong>：<code>include $_GET['page'] . '.php';</code></p><p><strong>为什么会发生</strong>：图省事做“动态路由”。</p><p><strong>线上后果</strong>：严重安全风险。攻击者可构造 <code>../../../../etc/passwd</code>，或利用 <code>php://filter/...</code> 读取敏感配置。即使关闭远程 URL 加载，本地文件同样会被攻击。</p><h4>加载带副作用的文件</h4><p><strong>错误写法</strong>：一个文件既定义类，又直接执行逻辑（输出 HTML、连数据库等）。</p><p><strong>为什么会发生</strong>：历史代码里职责边界没分清。</p><p><strong>线上后果</strong>：测试几乎没法写。你只是想测试类定义，却被迫触发数据库连接和页面输出。</p><h3>正确做法（PHP 8+）</h3><p>在现代项目里，类加载通常由 Composer + PSR-4 自动加载处理，<code>include</code>/<code>require</code> 更多用于配置、模板和少量模块逻辑。</p><p>但即便如此，也建议守住下面三条。</p><h4>始终使用绝对锚点路径</h4><p>把路径固定在已知根上。<code>__DIR__</code> 永远指向“当前文件所在目录”，不会随工作目录变化。</p><p><strong>错误示例（脆弱）</strong></p><pre><code class="php">&lt;?php
// 如果从 public/ 目录启动，这里可能失败
require 'config/settings.php';</code></pre><p><strong>正确示例（稳定）</strong></p><pre><code class="php">&lt;?php
// 无论从哪里调用，都能稳定解析
require __DIR__ . '/config/settings.php';</code></pre><h4>善用加载返回值</h4><p>这是 PHP 里经常被忽略但非常实用的能力：被加载文件可以 <code>return</code> 值。</p><p><code>config.php</code></p><pre><code class="php">&lt;?php
return [
    'db' =&gt; [
        'host' =&gt; '127.0.0.1',
        'pass' =&gt; $_ENV['DB_PASS'] ?? 'root',
    ],
    'debug' =&gt; false,
];</code></pre><p><code>app.php</code></p><pre><code class="php">&lt;?php
$config = require __DIR__ . '/config.php';
// $config 是局部变量，不污染全局</code></pre><h4>关键组件要做防御式加载</h4><p>对于必须存在的文件，不要依赖默认报错，自己把预期写清楚。</p><pre><code class="php">&lt;?php
$templatePath = __DIR__ . '/views/header.php';
if (!file_exists($templatePath)) {
    throw new \RuntimeException("关键视图组件缺失: {$templatePath}");
}
require $templatePath;</code></pre><h3>生产环境注意点：扩缩容与安全</h3><p>当系统从单机走到容器集群或函数计算，文件加载不再只是代码细节，而是基础设施问题。</p><h4>安全：路径穿越防护</h4><p>很多“PHP 不安全”的印象，本质是加载策略不安全。</p><ul><li><strong>白名单（Allow-list）</strong>：绝不直接信任用户输入拼路径。</li><li><strong><code>basename()</code></strong>：确实需要用输入值时，先做路径片段清洗，拦截 <code>../</code> 穿越。</li><li><strong><code>open_basedir</code></strong>：在 <code>php.ini</code> 限制 PHP 可访问路径范围，防止越界读取。</li></ul><h4>性能：OPcache 是基础设施而不是可选项</h4><p>生产环境应开启 OPcache。它会把预编译后的字节码放内存，避免每次请求重复解析文件。</p><p><strong>部署提示</strong>：在高并发集群中可以考虑 <code>opcache.validate_timestamps=0</code>，换取更快加载速度；但这意味着每次发布都必须做平滑重载，否则代码更新不会生效。</p><h4>可观测性：失败必须可追踪</h4><p>文件加载失败不应只留下一个“白屏”或 500。</p><ul><li><strong>可追踪信息</strong>：日志至少要包含 <code>include_path</code> 与 <code>cwd</code>。</li><li><strong>监控策略</strong>：对 <code>E_COMPILE_ERROR</code> 做专门告警，这类问题通常与发布或环境差异有关，需优先回滚。</li></ul><h4>部署形态差异（容器 vs 函数计算）</h4><p>容器镜像里文件路径通常固定可预测；函数计算环境常见只读文件系统、目录映射变化。统一使用 <code>__DIR__</code> 能显著降低环境差异带来的路径问题。</p><h3>真实事故："空配置"幽灵</h3><p>我曾参与排查过一个支付业务事故：后台任务随机失败。问题根因是他们用 <code>include</code> 加载环境配置。</p><p>某次发布脚本漏拷了生产配置文件。因为是 <code>include</code>，进程没有崩，业务继续跑，只是拿到一个空的 <code>$config</code>。</p><p>结果是任务带着空 API 密钥连续运行了 6 小时，造成大量交易失败。</p><p>如果当时使用的是 <code>require</code>，任务会第一时间中断并触发告警，损失会小得多。</p><p>一句话：<strong>没有它系统就不能活，那就必须 <code>require</code>。</strong></p><h3>排障清单（看到 Failed opening required 时直接照做）</h3><ol><li><strong>打印绝对路径</strong>：<br/><code>var_dump(realpath(__DIR__ . '/your-file.php'));</code><br/>若返回 <code>false</code>，说明文件根本不在你以为的位置。</li><li><strong>确认运行身份</strong>：<br/><code>echo exec('whoami');</code><br/>看当前系统用户是否有读权限。</li><li><strong>排查隐藏语法错误</strong>：<br/>某些文件不是“不存在”，而是语法错误导致加载失败。<br/>用命令行执行：<code>php -l filename.php</code>。</li><li><strong>检查 PHP 开始标签</strong>：<br/>文件应以 <code>&lt;?php</code> 开头。若短标签关闭而你写了 <code>&lt;?</code>，后续可能出现各种诡异问题（如 header 已发送）。</li></ol><h3>更专业的加载封装示例</h3><p>不要长期依赖裸 <code>var_dump</code>。建议用结构化日志和统一包装。</p><pre><code class="php">&lt;?php
/**
 * 带可观测性的文件加载器
 * 开发环境要“响亮失败”，生产环境可控降级。
 */
function load_component(string $filePath, array $context = []): mixed
{
    $absolutePath = realpath($filePath);
    if (!$absolutePath || !file_exists($absolutePath)) {
        error_log(sprintf(
            "[FileLoader] Failure: %s | CWD: %s | User: %s",
            $filePath,
            getcwd(),
            get_current_user()
        ));

        if (getenv('APP_DEBUG') === 'true') {
            throw new \Exception("组件不存在: {$filePath}");
        }

        return null; // 生产环境按约定降级
    }

    extract($context);
    return require $absolutePath;
}</code></pre><h3>常见问题</h3><h4>Q：<code>require_once</code> 一定比 <code>require</code> 更好吗？</h4><p>不一定。<code>require_once</code> 更像是组织不清晰时的安全网。依赖关系明确、自动加载健全时，<code>require</code> 更直接、性能更好。</p><h4>Q：可以根据数据库值动态 include 文件吗？</h4><p>可以，但必须非常谨慎。推荐白名单映射：数据库只存 ID，代码里把 ID 映射到固定路径，不要把路径原文存进数据库后直接加载。</p><h4>Q：加载大文件会拖慢应用吗？</h4><p>开启 OPcache 后，首次之后基本没有“解析”成本；但文件中的业务逻辑仍要执行，依旧消耗 CPU 和内存。文件内容要聚焦，避免把大量无关逻辑塞在一起。</p><h4>Q：模板文件适合用 <code>include</code> 吗？</h4><p>小项目可以。中大型系统建议使用成熟模板方案，能在安全性和复用性上更稳。</p><h3>结语</h3><p>把 <code>include</code> 和 <code>require</code> 用好，不只是语法问题，而是工程能力问题。</p><p>你的代码运行在操作系统、权限模型、缓存机制和部署流水线共同构成的环境里。只理解“本地能跑”，远远不够。</p><h4>最佳实践小结</h4><ul><li><strong>快速失败</strong>：关键依赖统一使用 <code>require</code>。</li><li><strong>路径绝对化</strong>：避免相对路径，优先 <code>__DIR__</code>。</li><li><strong>作用域收敛</strong>：用 <code>return</code> 返回配置，避免全局变量污染。</li><li><strong>失败可观测</strong>：把加载失败当成一类关键系统事件处理。</li></ul><h4>你的下一步</h4><p>现在就打开项目，全局搜索 <code>include</code> / <code>require</code>：</p><p>凡是不以 <code>__DIR__</code> 或统一根路径常量开头的，今天就改。</p><p>这一步做完，你的生产环境就会少一类高概率事故。<br/><a href="https://link.segmentfault.com/?enc=ItiJi0IgQLYkcm47Dmoqaw%3D%3D.fhYrveGclDFxVEs9FKhCDhxRkQq%2F6b0fin51kGvdxwpwYG1XBBatp7BH7%2BSzZGZhsruxBPlzx07tadbc1fqNSPRfEhFTcIx%2BL63xA%2B35enc%3D" rel="nofollow" target="_blank">Fatal error: require(): Failed opening required...”—以及如何彻底避免它再次出现</a></p>]]></description></item><item>    <title><![CDATA[融资 1 亿美元李飞飞参投，斯坦福小镇论文作者创立数字孪生公司 Simile 丨日报 RTE开发者社]]></title>    <link>https://segmentfault.com/a/1190000047610797</link>    <guid>https://segmentfault.com/a/1190000047610797</guid>    <pubDate>2026-02-14 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610799" alt="" title=""/></p><p>开发者朋友们大家好：</p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、谷歌发布 Gemini 3 Deep Think，编程水平排名世界第八</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610800" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610801" alt="" title="" loading="lazy"/></p><p>今天凌晨，谷歌发布了 Gemini 3 Deep Think 的重大升级。作为<strong>专用于复杂任务的推理模式</strong>，该版本试图解决科学与工程领域的诸多挑战。据悉，去年 9 月加入 Google DeepMind 的清华物理系校友姚顺宇也参与了此次研发。</p><p><strong>在编程领域</strong>，Gemini 3 Deep Think 在 Codeforces 平台上取得了 3455 的 Elo 分数，<strong>位列世界第八</strong>。这意味着全球仅有 7 名人类选手能在此类比赛中击败它，而此前最佳模型 OpenAI o3（约一年前数据）的排名仅为第 175 位。</p><p>该模型在多项学术基准测试中刷新了纪录：</p><ul><li><strong>通用与抽象推理</strong>：在「人类的最后考试」基准测试中，不使用工具取得了 48.4% 的 SOTA 成绩；在 ARC-AGI-2 中达到 84.6%。值得注意的是，其在 ARC-AGI-1 上的每任务成本仅为 7.17 美元，相比 OpenAI o3-preview 「高计算」版本降低了数百倍。</li><li><strong>科学竞赛</strong>：在 2025 年国际数学、物理和化学奥林匹克竞赛笔试中均获金牌水平，并在高等理论物理 CMT-Benchmark 测试中得分 50.5%。</li></ul><p>谷歌同时展示了 Deep Think 在科研中的实际应用。罗格斯大学数学家 Lisa Carbone 利用其识别出一篇专业论文中人工评审未发现的逻辑缺陷；杜克大学 Haozhe Wang 的实验室则利用其优化半导体工艺，实现了厚度大于 100 微米薄膜的精确生长目标。<strong>此外，该模型还能将草图转化为可 3D 打印的实体模型。</strong></p><p>目前，全新 Deep Think 已面向 Google AI Ultra 订阅用户及部分 API 合作伙伴开放。</p><p>（@机器之心）</p><p><strong>2、小红书开源工业级语音系统 FireRedASR2S：集成四大核心组件</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610802" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610803" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610804" alt="" title="" loading="lazy"/></p><p>2026 年 2 月 12 日，<strong>小红书正式发布并开源了工业级一体化语音识别系统 FireRedASR2S</strong>。该项目基于 Apache-2.0 许可协议，相关的模型权重与推理代码目前已在 Hugging Face 和 ModelScope 等平台开放下载。</p><p>FireRedASR2S 将单点语音能力扩展为了完整的处理生态，系统内部集成了<strong> ASR（自动语音识别）、VAD（语音活动检测）、LID（语种识别）和 Punc（标点预测）四个核心组件</strong>。这些模块在架构设计上保持自包含与独立性，开发者既可以将其整合为端到端的工作流，也能脱离主系统单独调用任意单个模块。</p><p>根据官方公布的基准测试数据，各核心组件的具体能力表现如下：</p><ul><li><strong>FireRedASR2</strong>：支持普通话、20 多种方言与口音、中英文语码转换以及歌词识别。该模块提供 LLM（结合大语言模型以优化无缝交互）与 AED（平衡性能与效率，支持词级时间戳）两个版本。评测显示，其普通话平均字符错误率（CER）低至 2.89%，方言平均 CER 为 11.55%，整体表现优于 Doubao-ASR、Qwen3-ASR-1.7B 与 Fun-ASR 等竞品。</li><li><strong>FireRedVAD</strong>：支持超百种语言的非流式与流式语音活动检测，涵盖语音、歌声及音乐，并具备音频事件检测能力。其 F1 分数高达 97.57%，领先其他开源基准。</li><li><strong>FireRedLID</strong>：覆盖 100 多种语言及 20 多种中文方言，语种检测准确率达到 97.18%，客观数据超越了 Whisper 与 SpeechBrain-LID。</li><li><strong>FireRedPunc</strong>：提供多领域的中英文标点预测服务，平均 F1 分数达到 78.90%，显著优于 FunASR-Punc。</li></ul><p>在实际应用与部署环节，系统要求输入 16kHz 16 位单声道 PCM 格式音频。对于输入长度，AED 版本最高支持 60 秒的音频，而 LLM 版本目前支持最长 30 秒的输入。后续，开发团队还将陆续公开技术报告与微调代码。</p><p>GitHub: <br/><a href="https://link.segmentfault.com/?enc=4lOB3eFt2xLRi0di3L8VwQ%3D%3D.%2BuNd8SaW%2B0LwEwP0JzYwxHcO9UTOajh5lsDP3Fw8oRZX%2F3wkg8aZ9wfZN10RlUFM" rel="nofollow" target="_blank">https://github.com/FireRedTeam/FireRedASR2S</a></p><p>HuggingFace: <br/><a href="https://link.segmentfault.com/?enc=gucYuAIvD1E9xzdZMf51lw%3D%3D.2runaO%2FrBkw9cdIJ24Rb2NrB0w%2BAtMfs%2B0ScZdgF23%2FbZySoootziVf4GQECHtj66zL6%2BCz800J5B8FiEiZ7eA%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/FireRedTeam/FireRedASR2-AED</a></p><p>( @GitHub)</p><p><strong>3、涉嫌侵犯开源项目 FFmpeg 的版权，瑞芯微被 GitHub 冻结代码库</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610805" alt="" title="" loading="lazy"/></p><p>2026 年 2 月，国内芯片设计企业瑞芯微（Rockchip）<strong>因涉嫌侵犯开源项目 FFmpeg 的版权</strong>，其相关代码库被 GitHub 平台冻结。这一事件再次引发行业对开源软件合规使用的关注。</p><p>经查，瑞芯微在产品开发过程中使用了 FFmpeg 的核心组件 libavcodec 代码，但在使用过程中存在多项违规操作：</p><ul><li><strong>删除版权信息</strong>：删除了代码原作者信息及版权声明；</li><li><strong>篡改许可证</strong>：擅自将原代码的 LGPL 许可证更改为 Apache 协议。</li></ul><p><strong>尽管 LGPL 协议允许商业场景使用，但明确要求使用者必须保留原始版权声明、按需提供源代码，并保持许可证的一致性。</strong>瑞芯微的操作直接违反了这些条款。事实上，该违规行为早在 2024 年初就已被发现。当时，瑞芯微工程师 HermanChen 曾公开道歉，称对许可证冲突缺乏了解，并承诺整改。然而，在随后的近两年时间里，<strong>瑞芯微并未采取实质性整改措施</strong>。最终，FFmpeg 项目方依据《数字千年版权法案》（DMCA）向 GitHub 发起正式投诉，导致瑞芯微相关项目库被冻结。</p><p>数据显示，目前 97% 的代码库包含开源组件，其中 63% 存在许可证冲突。业内专家指出，许多企业开发者对 GPL、MIT、Apache、LGPL 等主流许可证的区别认知不足，错误地认为开源代码可随意修改分发，从而埋下法律风险。</p><p>不同许可证规则差异显著。<strong>以此次涉事的 LGPL 为例，它允许闭源软件动态链接使用，仅要求修改库本身代码时开源修改部分；而 Apache 协议虽支持商业闭源，但更侧重专利保护，且与 GPL 系列协议存在兼容性冲突，二者不可随意替换。</strong>此次事件表明，开源合规管理已成为企业发展的必修课，企业需建立完善的审查机制，明确协议边界，规避版权风险。</p><p>（@人人极客社区）</p><p><strong>4、蚂蚁百灵开源发布万亿参数思考模型 Ring-2.5-1T ，主打深度思考与长程智能体执行</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610806" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610807" alt="" title="" loading="lazy"/></p><p>今天中午，蚂蚁百灵正式发布并开源了<strong>首个混合线性架构的万亿参数思考模型 Ring-2.5-1T</strong>。作为迈向通用智能体时代的关键一步，该模型在<strong>预训练</strong>和<strong>强化学习</strong>层面均进行了大规模扩展。</p><p>相比前代产品 Ring-1T，Ring-2.5-1T 在三个核心维度实现了大幅提升：</p><ul><li><strong>高效生成</strong>：基于高效的 1:7 MLA + Lightning Linear Attention 架构，在超过 32K 的生成长度下，访存规模降低 10 倍以上，生成吞吐提升 3 倍以上。</li><li><strong>深度思考</strong>：在 RLVR 基础上引入 dense reward 反馈机制。自测结果显示，其在 IMO 2025（获 35 分）和 CMO 2025（获 105 分）中均达到金牌水平。</li><li><strong>长程执行</strong>：通过大规模全异步智能体强化学习（fully-async agentic RL）训练，显著增强了复杂任务的长程自主执行能力，可适配 Claude Code 及 OpenClaw 等框架。</li></ul><p>在架构层面，Ling 2.5 采用增量训练方式，将 Ling 2.0 的 GQA 升级为混合线性注意力结构。改造后，尽管激活参数量从 51B 增至 63B，但推理效率仍大幅提升。测试显示，无论在单机 8 卡 H20-3e 还是 H200 环境下，其长程推理的吞吐优势均十分显著。</p><p>为验证其长程执行能力，开发团队将 Ring-2.5-1T 接入 Claude Code，仅用两小时便自动完成了一个微型版操作系统（TinyOS）的开发，并能进一步实现 bash 功能。此外，该模型在数学、代码、逻辑等<strong>高难推理任务</strong>以及智能体搜索（如 GAIA2-search）等<strong>长程任务执行上，均达到了开源领域的领先水平</strong>。</p><p>目前，Ring-2.5-1T 仍存在 token efficiency 和指令遵循方面的局限性。其模型权重已在 Hugging Face 和 ModelScope 开源，相关体验页及 API 服务也将在 Ling Studio 与 ZenMux 陆续上线。</p><p>HuggingFace: </p><p><a href="https://link.segmentfault.com/?enc=RKWeJSGchTC7XxhjxvlfUA%3D%3D.4hckt6JGvtFPIJMvLZZdKIVWqoteVRCLmYuuy3oN03YvGvWt1BKVhvONZPHNtNYf" rel="nofollow" target="_blank">https://huggingface.co/inclusionAI/Ring-2.5-1T</a></p><p>（@百灵大模型）</p><h2>02 有亮点的产品</h2><p><strong>1、自然语言几分钟构建 AI 智能体：VM0 正式开启公开测试</strong></p><p>2026 年 2 月 6 日，VM0 宣布正式开启<strong>公开测试</strong>。在经历了约两个月的内部构建与私密测试后，该平台现已向更多开发者开放。</p><p><strong>VM0 是一款基于自然语言构建 AI 智能体的工具，并配备了支持智能体全天候（24/7）运行的沙盒环境。</strong> 用户只需用自然语言描述具体需求，VM0 便会自动处理运行时、执行操作及环境配置。即使用户关闭了电脑，其部署的智能体应用也会保持持续运行状态。</p><p>在构建体验上，该平台试图同时满足不同类型用户的需求：</p><ul><li><strong>面向 Vibe Coder 和快速实验</strong>：对于刚接触 AI 智能体或希望快速测试想法的用户，平台提供了几分钟即可上手的体验。无需繁重的环境设置或提前阅读长篇文档，仅需执行一条简单的初始命令（<code>npm install -g @vm0/cli &amp;&amp; vm0 onboard</code>），即可运行首个智能体。</li><li><strong>面向专业开发者</strong>：如果需要获取更多控制权，VM0 提供了一套完整的开发工具包。开发者可以将 VM0 接入现有的基础设施中，并在实际需要时进行规模化扩展。</li></ul><p>由于目前产品仍处于测试阶段，官方正积极向早期用户征集错误报告、细节打磨建议以及功能反馈，这些反馈将直接塑造产品的下一步走向。此外，VM0 正在建设一个由社区驱动的 Cookbook，鼓励开发者分享其实际构建的案例，例如客户支持智能体、数据分析工作流或内部工具等。</p><p>根据公布的路线图，VM0 下一步的发展计划包括：<strong>推出自托管运行程序、简化的智能体分享功能、支持更多模型提供商、VM0 平台智能体构建器、VM0 Slack 集成以及 VM0 连接器</strong>。</p><p>相关链接：<br/><a href="https://link.segmentfault.com/?enc=BEccPUu27Dpvaja4R3k57w%3D%3D.KxK%2FlYwKNa7KmoM%2FHghpmKRexemDGK46kaQD201Aws8%3D" rel="nofollow" target="_blank">https://docs.vm0.ai/docs</a></p><p>( @VM0 Blog)</p><p><strong>2、AI 数字孪生初创公司 Simile 获 1 亿美元融资，用 AI 模拟真实用户反馈</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610808" alt="" title="" loading="lazy"/></p><p>AI 数字孪生初创公司 Simile 宣布完成 1 亿美元融资。本轮融资由 Index Ventures 领投，Bain Capital Ventures 等机构投资者跟投，AI 先驱李飞飞与 OpenAI 联合创始人 Andrej Karpathy 也参与了投资。</p><p>企业在推出新产品前，通常需要收集潜在客户的反馈，但这类调研往往耗时费力，且难以触及特定目标受众（如世界 500 强高管）。为此，Simile 构建了一个 AI 模型，<strong>利用个人数据来模拟人们对新产品、功能变更等商业动态的反应</strong>。目前，其首批客户包括 CVS Health Corp。 和澳大利亚最大的移动互联网提供商 Telstra Group。</p><p>该 AI 模型主要帮助企业简化以下测试流程：</p><ul><li><strong>用户界面更新评估</strong>：开发者可在向真实客户全面推出更新前，观察模拟用户对界面变更的反应。</li><li><strong>财报电话会议准备</strong>：据首席执行官 Joon Sung Park 透露，在一次模拟电话会议中，该模型准确预测了分析师 10 个问题中的 8 个，能够帮助上市公司高管提前做好准备。</li></ul><p>Simile 由 Joon Sung Park、Michael Bernstein 和 Percy Liang 共同创立。这三位计算机科学家此前曾开发过模拟环境 Smallville，证明了 AI 智能体不仅能模拟个体行为，还能模拟群体行为。据报道，Simile 耗时七个月开发该模型，其训练数据来源于对数百人的采访记录、交易日志以及科学期刊文本。</p><p>Index Ventures 合伙人 Shardul Shah 表示，Simile <strong>建立了高保真模型来解答真实人类会做什么以及为什么这样做，这在各类组织中有着广泛的应用需求</strong>。除了模拟买家行为，AI 生成模拟正被广泛应用于更多领域，例如 Simile 的投资人李飞飞曾于 2024 年创办 World Labs，用于生成三维虚拟环境以训练工业机器人。</p><p>( @SiliconAngle)</p><h2>03 有态度的观点</h2><p><strong>1、DeepMind CEO：AI 在未来十五年会解决人类棘手难题</strong></p><p>近日，Google DeepMind CEO 德米斯 · 哈萨比斯接受《财富》杂志的采访时，其提到：人类正站在「科学发现新黄金时代」的边缘，尽管未来 10 到 15 年将经历剧烈的行业洗牌与阵痛，但最终将迎来一场足以媲美「文艺复兴」的技术变革。</p><p>哈萨比斯在访谈中提出了「激进富足」的概念。他预言 AI 将通过对科学方法的深度内化，解决人类最棘手的难题：</p><ul><li>医疗革命： 未来 15 年内，AI 将使个性化医疗成为常态，攻克重大疾病。</li><li>能源突破： AI 将加速核聚变与太阳能新材料的研发，彻底解决能源危机。</li><li>宇宙探索： 算力的突破将最终支持人类「在星际间穿梭，探索银河系」。</li></ul><p>采访中，哈萨比斯也提到了 Google 在当今 AI 圈的一些风险以及挑战。</p><p>面对 OpenAI 等竞争对手的崛起，哈萨比斯坦言谷歌必须面临「创新者困境」。他强调：「<strong>如果我们不进行自我颠覆，别人就会动手。你最好按自己的节奏来。</strong>」</p><p>据悉，随着 Gemini 系列模型及 Nano Banana 图像生成模型的发布，Alphabet（Google 母公司）股价在去年飙升约 65%，哈萨比斯认为公司已跨越了 AI 助手辅助高阶研究的「分水岭」。</p><p>( @APPSO)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610809" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610810" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=jzJBZxvBvq541vmOJ13qjw%3D%3D.zKmm5jelow1a4Gjx%2FpQfdPDPjPNf2TxSd8Ihz4O28YE%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610811" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[RAG 文本分块：七种主流策略的原理与适用场景 本文系转载，阅读原文
https://avoid.o]]></title>    <link>https://segmentfault.com/a/1190000047610619</link>    <guid>https://segmentfault.com/a/1190000047610619</guid>    <pubDate>2026-02-13 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>检索是 RAG 系统的搜索引擎，分块则是这个搜索引擎的基础。分块太长、太短、有噪声、切错了位置——随便犯哪个错LLM 都会有问题。行业里有句话流传很广："分块决定了 RAG 质量的 70%。"</p><p>这个说法不夸张：好的分块让检索器拿到完整、有上下文、真正相关的信息；差的分块把文档打成碎片，上下文断裂，LLM 只能靠"编"来填补空白。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610621" alt="" title=""/></p><h2>什么是分块？</h2><p>RAG 的起点是文档收集与摄取：把所有原始材料（文档、文章、知识库条目）汇聚到一起。在进入检索环节之前，这些文档要经过文本分块处理也就是切分成更小的、有意义的片段。</p><p>每个分块应当是连贯且自包含的，这样检索器才能在面对查询时快速定位、排序，并返回最相关的信息。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610622" alt="" title="" loading="lazy"/></p><p>分块就是在生成 Embedding 之前，把大段文本拆成更小语义单元的过程。检索器真正搜索的对象而不是整篇文档就是这些分块。</p><p>分块做得好，文档中的内容就能被干净地捕获，上下文得以保留LLM 能做出有意义的推理。分块做得差，语义被割裂检索充满噪声。向量存储、Embedding 模型、Reranker——这些统统排在分块之后，分块才是真正的起点。</p><h2>固定大小分块</h2><p>这是最简单的方式。按预设的字符数或 Token 数直接切分，比如每 500 个 Token 一块完全不管句子和段落的边界在哪。</p><p>速度快，行为可预测，处理大规模、结构混乱的数据集时很实用。但缺点也很明显——语义经常被拦腰切断。一个句子在这个分块里开了头，到下一个分块才结束，Embedding 的语义表达力就会打折扣。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610623" alt="" title="" loading="lazy"/></p><p>实践中一般会在相邻分块之间设置一定的重叠来缓解这个问题：</p><pre><code> from langchain.text_splitter import RecursiveCharacterTextSplitter  

splitter = RecursiveCharacterTextSplitter(  
    chunk_size=500,  
    chunk_overlap=50  
)  

 chunks = splitter.split_text(long_text)</code></pre><p>切分文本时，连续的分块之间通常会加入一小段重叠区域来维持上下文的连贯。所谓重叠，就是前一个分块的尾部几句话，在下一个分块的开头再出现一次。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610624" alt="" title="" loading="lazy"/></p><p>这么做是为了防止跨越分块边界的关键信息丢失。没有重叠的话，检索器可能只拿到部分内容LLM 因此漏掉了关键上下文，给出残缺甚至误导性的回答。重叠量一般控制在分块长度的 10% 到 20%，在冗余和效率之间找一个平衡点。</p><p>固定大小分块适合的场景包括日志文件、邮件、代码仓库，以及结构参差不齐的大型语料库。</p><h2>基于句子的分块</h2><p>这种方式按完整句子来划分文本，而不是按任意长度一刀切。每个分块至少包含一个或多个完整的句子，语法完整，语义连贯。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610625" alt="" title="" loading="lazy"/></p><p>好处是每个分块都是一个有意义的思想单元。检索器向 LLM 返回的信息更精确、更易理解，碎片化回答的风险降低不少。实际使用中通常也会搭配小幅重叠，进一步保证分块之间的衔接。</p><h2>基于段落的分块</h2><p>以完整段落为单位切分，不再拘泥于单个句子或固定 Token 数。这种方式天然保留了文档的结构和行文节奏，检索器更容易抓到完整的想法。</p><p>每个分块往往对应一个独立的主题或子主题，LLM 处理起来更从容，也更容易给出准确的回答。对长篇文档、研究论文、综述类文章来说，段落级分块效果不错。和句子级分块一样，也可以加重叠来保持连贯。</p><h2>语义分块</h2><p>语义分块的切入点不是长度，而是语义本身。它利用 Embedding 或相似度分数来识别文本中天然的断裂点——主题切换、上下文转折、章节边界。</p><p>产出的分块语义清晰度更高，边界和语义对齐，检索质量有明显提升，尤其在知识库、技术文档、结构化文章这类内容上效果突出。代价是计算开销更大而且分块长度不一致，后续处理需要额外考虑。</p><pre><code> from langchain_experimental.text_splitter import SemanticChunker  
 from sentence_transformers import SentenceTransformer  
   
 model = SentenceTransformer("all-MiniLM-L6-v2")  
 chunker = SemanticChunker(model, breakpoint_threshold=0.4)  
   
 chunks = chunker.split_text(long_text)</code></pre><p>如果文档质量高、主题流转有明确脉络，语义分块往往是精度最高的选择。</p><h2>递归分割</h2><p>递归分割是固定大小和语义分块之间的一个折中方案。核心思路是优先尊重文档结构，只有在必要时才进一步拆分。</p><p>具体做法是先尝试按标题切分。如果某个章节还是太长，就按段落切。段落还不够就按句子。句子仍然超限，最后才按字符兜底。这样得到的分块既保有语义完整性，尺寸也在可控范围内。</p><pre><code> recursive_splitter = RecursiveCharacterTextSplitter(  
     separators=["\n## ", "\n### ", "\n", ". ", ""],  
     chunk_size=600,  
     chunk_overlap=80  
 )  
   
 chunks = recursive_splitter.split_text(long_doc)</code></pre><p>开发者文档、技术手册、学术论文、研究报告——凡是层级结构明确的内容，递归分割都很适合。</p><h2>滑动窗口分块</h2><p>有些文本的语义天然是跨句分布的。法律合同、科学论文、长段论证，一个完整的意思可能横跨好几个句子。滑动窗口就是为这种场景设计的。</p><p>它不生成彼此独立的分块，而是创建相互重叠的窗口。比如窗口大小 400 Token，每次滑动 200 Token，这样相邻的分块之间有一半的内容是共享的，语义在边界处不会断裂。</p><p>上下文保持得很好，但分块数量会膨胀，存储和检索的成本都会上升。</p><p>法律 RAG、金融分析、医学文献检索、合规审查——这些领域用滑动窗口的比较多。</p><h2>层次化分块</h2><p>层次化分块是一个多层级的架构：小分块负责细粒度精确检索，中等分块支撑平衡的推理，大分块维持全局上下文。</p><p>检索时，系统先用小分块锁定精确位置，再把关联的大分块拉进来补充完整上下文。这种组合能有效压制幻觉，提升推理的深度。</p><p>企业级 RAG 系统和 LlamaIndex 这类多粒度检索框架，背后都有层次化分块的影子。</p><h2>实践中常见的分块失误</h2><p>多数 RAG 项目翻车根源都是分块层面的问题。分块过大模型被不相关的细节淹没。分块过小语义丧失殆尽。句子被拦腰切断、不相关的段落被混到一个分块里，Embedding 质量直接垮掉。没有重叠，上下文断裂。没有元数据，检索器找不到方向。还有一个常见错误——所有类型的文档套用同一种分块策略。</p><p>分块没有万能方案。政策文件和教科书不一样，通话记录和研究论文不一样。策略必须跟着文档类型和检索任务走。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047610626" alt="" title="" loading="lazy"/></p><h2>总结</h2><p>分块不是一个可有可无的预处理环节，它是 RAG 管道的脊梁。好的分块是一个有意义的、自包含的知识单元；差的分块是一个孤零零的碎片，把 LLM 带向歧途。</p><p>检索是引擎分块是燃料。燃料的质量决定了整个系统是输出干净、可靠的结果，还是不断产出噪声和幻觉。LLM 本身再好，也救不了烂分块。</p><p><a href="https://link.segmentfault.com/?enc=mLbki017nch6q2DBNVwvaQ%3D%3D.VbbcuN20Y6RMy8smm0MawC8TU7F1GrUd8NAe6jQi3VYw1g111et4Zka65HzXH1frho6JxsAEP6iUODtLempF8A%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/e6520bd283254415ae61cfa28fb2ef32</a></p><p>作者：Abinaya Subramaniam</p>]]></description></item><item>    <title><![CDATA[OpenClaw + 智能家居 + 家庭服务器 = 自由的家庭 AI Agent MarkZhu ]]></title>    <link>https://segmentfault.com/a/1190000047610542</link>    <guid>https://segmentfault.com/a/1190000047610542</guid>    <pubDate>2026-02-13 21:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000044262194" alt="logo" title="logo"/></p><p>本文展示了如何通过自托管的 OpenClaw Skills，对 Home Lab（家庭服务器群）与智能家居进行真 AI 智能化控制。而不受限于小米米家等平台的各种智能限制。通过将家庭服务器群环境、监控系统以及几种 API 封装为结构化的 Skills，AI Agent 可以完成服务器故障排查、以有趣的智能家居灯光方式可视化运维操作，并通过智能音箱汇总报播网站实时新闻。</p><hr/><h2>背景</h2><p>这些年，我一直在运行和维护自己的 Home Lab（家庭服务器群） 和一套小型智能家居系统。随着设备和服务越来越多，维护和管理也变得越来越复杂。</p><p>同时，我也逐渐意识到，传统的“智能家居”其实并不真正智能。运营方出于各种原因，没有把最近的 AI 科技更新到智能家居平台上。很多问题的存在，往往只是因为我懒得去做自动化。如今，现代 AI 技术或许可以帮助解决这些问题。</p><p>我的 Home Lab 已经运行了 5 年多，承载着多个 Linux 主机上的各种服务。同时，我还运行了一套 Home Assistant 实例，用来管理家中的 IoT 智能家居设备。</p><p>在可观测性方面，我部署了 Prometheus 和 Grafana，并为硬件和软件异常配置了告警机制。</p><p>但问题是：当手机收到告警后，我仍然需要手动调查和排查问题。如果我不在家，用手机远程排障就会变得低效又麻烦。</p><p>这正是 OpenClaw 发挥作用的地方。理想情况下，我只需要给它一些目标，它就应该能替我完成后续操作。</p><p>作为 Home Lab 和智能家居的管理员，OpenClaw 需要具备以下三类“技能 (Skills) ”：</p><ul><li><strong>Home Lab 环境知识</strong> —— 包括机器清单、服务部署情况、监控架构等。</li><li><a href="https://link.segmentfault.com/?enc=d%2BenAKopKP3YHaI6WcsyxQ%3D%3D.YzueESxnGdsg8gTyGH6GPwImsbBc00UF%2BCdHxC1tuF1rnT0B3gq8tV9TUy9wC9Qt" rel="nofollow" target="_blank">Home Assistant</a></li><li><a href="https://link.segmentfault.com/?enc=zeiDSR9iq4P5qcjLGlGCTA%3D%3D.wNUzYwnEzbiZgyYmyqzWANjO0ZSMWNSxDol8hUGpVb9PVgkngWk2lhnujuWVdA2IXELlATKk8uFI43Nmmin2gABiYft%2F9xRbEskU%2BwsDYAorAh3EOzzeyF5ozWoqOajD" rel="nofollow" target="_blank">Prometheus API</a></li></ul><hr/><h2>使用场景</h2><h3>1. Home Lab 故障排查与根因分析</h3><p>查找某台服务器上 CPU 占用最高的服务，并通过智能音箱播报结果。</p><p>当收到“CPU 使用率过高”或“温度过高”告警时，这种方式尤其方便。</p><p><img width="723" height="748" referrerpolicy="no-referrer" src="/img/bVdnVP5" alt="image.png" title="image.png" loading="lazy"/><br/><em>（配图：root cause 分析示意图)</em></p><hr/><h3>2. Home Lab “氛围灯光秀大师”</h3><p>重启某一台服务器。重启开始时，立刻调暗书房灯光；当机器启动完成、所有服务初始化完毕后，再把灯光恢复。</p><p>这样一来，当我人在书房时，就可以通过灯光变化 “可视化” 服务器的启动进度。</p><p><img width="433" height="940" referrerpolicy="no-referrer" src="/img/bVdnVP6" alt="image.png" title="image.png" loading="lazy"/><br/><em>（配图：重启灯光联动示意图)</em></p><hr/><h3>3. AI 驱动的智能家居控制</h3><p>将 Hacker News (<a href="https://link.segmentfault.com/?enc=jhcNu%2B17BL9c%2BlFHZHwtxQ%3D%3D.8B%2Fu9c78zHilzU%2FooBvtTsV8IOnZZl2ZmtIp100KyDw%3D" rel="nofollow" target="_blank">https://news.ycombinator.com/</a>) 的内容摘要和翻译为中文，并通过智能音箱朗读出来。</p><p>当我在晨起或做家务时，也可以通过智能音箱播报了解最新任意网站的动态摘要。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047610544" alt="use-case-speak-news.png" title="use-case-speak-news.png" loading="lazy"/><br/>（配图：语音播报示意图）</p><hr/><h2>Skills 设计</h2><p>为了实现这些功能，我首先需要向 OpenClaw 介绍我的 Home Lab 环境，包括：</p><ul><li>所有 Linux 主机列表</li><li>每台机器上运行的服务</li><li>监控系统细节</li><li>Agent 必须遵守的运维规则</li></ul><p>我不希望把这些信息直接塞进普通的 LLM 上下文里——那样会迅速消耗上下文窗口，并浪费大量 token。</p><p>Anthropic 的 Skills 规范提供了一个清晰的解决方案：我可以将 Home Lab 环境封装成一个 Skill，让 OpenClaw 在需要时按需加载。</p><blockquote>注意：以下 Skill 代码片段为了便于阅读，做了简化处理。</blockquote><hr/><h3>Home Lab 环境 Skill</h3><p>核心思想：</p><ul><li>通过 Bash 和 SSH 控制 Home Lab 机器</li><li>所有 Linux 主机使用同一个用户名</li><li>SSH 无需密码登录</li><li>Prometheus 运行在 192.168.1.74:9090</li><li>优先从 Prometheus 查询指标，而非 SSH 到机器</li><li>仅在无法获取数据时才通过 SSH 获取实时信息</li><li>某些关键机器（如网关）默认只读访问</li><li>执行任何会修改系统状态的命令前必须请求确认</li></ul><p>特别强调了两条运维规范：</p><ol><li>执行前必须先解释行动计划</li><li>所有可能改变系统状态的操作必须征求用户确认</li></ol><p>这相当于为 AI Agent 定义了运维行为准则（Operational Guardrails）。</p><p>路径：</p><pre><code>~/openclaw/workspace/skills/home-lab/SKILL.md</code></pre><p>Skill 片段：</p><pre><code class="markdown">---
name: home-lab
description: Control the home lab where you(personal assistant running inside OpenClaw) are running on
---

# Home Lab

Control home lab Linux machines via bash and SSH commands.

## Conduct of Code

Always provide a clear, high-level explanation of your action plan before executing the first step. Let the user understand and confirm the plan before proceeding.

Always request confirmation before executing any command that may change system state, such as rebooting the OS, modifying configuration files, or restarting services. Clearly explain what the command does and why it is necessary before asking for confirmation.

## Home Lab Environment

All Linux machines use the same username: `mark`. SSH login does not require a password.

### Linux Machines

- 192.168.1.58 : Raspberry Pi, always on
- 192.168.1.68 : Raspberry Pi, always on
- 192.168.1.108 : Raspberry Pi, always on
- 192.168.1.14 : X86_64 server

### Observability and Monitoring

A Prometheus instance runs on 192.168.1.74:9090. Metrics are collected by Node Exporter on each machine.

When users request metrics (temperature, CPU usage, memory usage, disk usage), first query Prometheus. Only execute remote SSH commands if the required metrics cannot be found in Prometheus.

If you are unfamiliar with the Prometheus API, load the `prometheus-api` Skill.

Note that Prometheus metrics may not always be real-time. If real-time data is required, execute a remote SSH command.

### Services running on the machines

#### 192.168.1.68
This machine acts as the main Internet gateway of the home lab. 

Access this machine in read-only mode. Do not change anything without explicit confirmation. 

##### Home Assistant
The home automation system. Access by http://192.168.1.68:8123

##### OpenClaw
An OpenClaw instance runs at http://127.0.0.1:18789. It may be you or may be another AI assistant.

#### 192.168.1.74

##### Prometheus

Base URL: `http://192.168.1.74:9090`  

All machines are monitored via Node exporter, with metrics collected by this Prometheus server. 

## Remote wake up

Wake up 192.168.1.14 using:
```bash
ssh mark@192.168.1.108 o
```</code></pre><hr/><h3>Home Assistant Skill</h3><p>基于 Home Assistant API 文档，我创建了一个 webhook，用于向小米智能音箱发送通知。</p><p>然后在 Skill 文档中告诉 Agent 如何调用该 API，例如：</p><pre><code class="bash">curl -s -X POST $HA_URL/api/webhook/ai-speak \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $HA_TOKEN" \
  -d '{"msg": "需要播报的内容"}'</code></pre><p>此外，还封装了：</p><ul><li>列出智能设备</li><li>控制开关</li><li>控制灯光亮度</li></ul><p>这些都通过标准的 Home Assistant REST API 实现。</p><pre><code class="markdown">---
name: homeassistant
description: Control Home Assistant - smart plugs, lights, scenes, automations.
homepage: https://www.home-assistant.io/
metadata: {"clawdis":{"emoji":"🏠","requires":{"bins":["curl"],"env":["HA_TOKEN"]},"primaryEnv":"HA_TOKEN"}}
---

# Home Assistant

Control smart home devices via Home Assistant API.

## Setup

Set environment variables:
- `HA_URL`: Your Home Assistant URL (e.g., `http://192.168.1.100:8123`)
- `HA_TOKEN`: Long-lived access token (create in HA → Profile → Long-Lived Access Tokens)

## Quick Commands

### Smart Speaker Integration 🔊 
There is a smart speaker you can control. You can control it to speech any text to the user. You can use it to send notification to user at home.

```bash
curl -s -X POST $HA_URL/api/webhook/ai-speak -H "Content-Type: application/json" -H "Authorization: Bearer $HA_TOKEN" -d '{"msg": "The message you want to speech to user"}'
```

### List entities by domain
```bash
curl -s "$HA_URL/api/states" -H "Authorization: Bearer $HA_TOKEN" | \
  jq -r '.[] | select(.entity_id | startswith("switch.")) | .entity_id'
```

### Turn on/off
```bash
# Turn on
curl -s -X POST "$HA_URL/api/services/switch/turn_on" \
  -H "Authorization: Bearer $HA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"entity_id": "switch.office_lamp"}'
```

### Control lights
```bash
# Turn on with brightness
curl -s -X POST "$HA_URL/api/services/light/turn_on" \
  -H "Authorization: Bearer $HA_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"entity_id": "light.living_room", "brightness_pct": 80}'
```</code></pre><hr/><h3>Prometheus API Skill</h3><p>基于 Prometheus API 文档封装。</p><p>这样 Agent 可以：</p><ul><li>查询 CPU 使用率</li><li>查询设备温度</li><li>查询内存、磁盘使用情况</li><li>结合 Alertmanager 做进一步自动化处理</li></ul><hr/><h2>未来使用场景</h2><h3>主动型 Agent</h3><p>下一步，我希望 Agent 不只是“被动响应”，而是主动监控并处理 Home Lab 和智能家居状态。</p><p>用户只需要用自然语言描述期望行为，Agent 可以自动转换为：</p><ul><li>监控规则</li><li>告警规则</li><li>自动化规则</li></ul><p>例如：</p><p>通过集成 OpenClaw Webhook 到 Prometheus Alertmanager，实现：</p><ul><li>自动触发故障排查</li><li>自动调查分析</li><li>自动进行根因定位</li></ul><p>也就是说，当告警触发时，Agent 不只是发通知，而是直接开始分析问题。</p><h3>管理我的家居物品</h3><p>用自然语言管理我的 <a href="https://link.segmentfault.com/?enc=9Tzi0FnA8LJSs%2B8DnN%2FaXw%3D%3D.MBs8WKzWGNs5uOPLRtzgbpE2GmEKEnaMLPaDa8RdhDYBQacCPLXTD88JPYvLZTbF" rel="nofollow" target="_blank">Homebox</a>，这是一个自托管的家庭物品管理系统。例如，当我购买新设备时，我可以告诉 Agent: “我买了一部新的 iPhone 15 Pro Max，请将其添加到我的家庭物品清单中”。</p><hr/><h2>总结</h2><p>通过将 Home Lab 环境知识、Prometheus 监控系统和 Home Assistant API 封装为结构化 Skills，OpenClaw 可以将传统“手动运维 + 被动智能家居”升级为“AI 编排驱动”的自动化系统。</p><p>它不仅可以：</p><ul><li>分析服务器故障</li><li>有趣化运维过程</li><li>语音播报任意网站信息</li></ul><p>更重要的是，它为“主动式 AI 运维与智能家居控制”提供了一个可扩展的架构基础。</p><h2>安全风险警告</h2><p>赋予人工智能 Agent 在您的 Home Lab 中执行命令并控制智能家居设备的权限会带来重大的安全风险。攻击者可能利用该 Agent 获取未经授权的访问权限、造成损害或窃取敏感信息。请仅从可信来源下载和安装任何 Skill 。安装任何 Skill 之前，务必先查看其代码。考虑在权限受限的沙箱环境中运行 Agent，以降低潜在风险。</p><p>本文发布于我的博客：<a href="本文发布于我的博客：https://blog.mygraphql.com/zh/posts/ai/ai-personal-assistant/openclaw-as-home-lab-admin/" target="_blank">https://blog.mygraphql.com/zh/posts/ai/ai-personal-assistant/openclaw-as-home-lab-admin/</a></p>]]></description></item><item>    <title><![CDATA[find-skills技能全解析：一键解决AI Agent技能搜索、安装与管理痛点 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047610458</link>    <guid>https://segmentfault.com/a/1190000047610458</guid>    <pubDate>2026-02-13 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>find-skills技能全解析：一键解决AI Agent技能搜索、安装与管理痛点<br/>在AI Agent使用过程中，“找技能、装技能、管技能”是多数用户面临的核心难题——要么四处搜罗技能资源，要么切换平台搜索打断工作流，要么安装后难以统一管理更新。此前在Skills蓝皮书分享过的Skills.sh资源库中，一款名为find-skills的技能异军突起，不仅登顶24h安装榜榜首，长期稳居总榜第二且持续上升，日均安装量突破10k+，与第二名拉开显著差距。</p><p>这款由Vercel官方发布的技能，之所以能快速走红，核心在于它完美解决了技能获取与管理的全流程痛点，无需切换平台、无需复杂操作，仅需在单个Agent中运行，就能完成技能搜索、安装、检查、更新的闭环。本文将从核心优势、详细操作步骤、注意事项三个维度，全方位解析find-skills的使用方法，帮助用户高效利用AI Agent技能，提升工作效率。</p><p>一、find-skills核心优势：为什么它能成为“技能神器”？</p><p>在find-skills出现之前，用户获取技能的方式普遍存在诸多弊端，而它的出现的实现了技能管理的“一站式闭环”，具体优势对比及核心亮点如下：</p><p>1.1 解决传统技能获取的核心痛点</p><p>传统技能获取主要有两种方式，均存在明显局限：</p><ul><li>人工分享/仓库搜索：依赖他人分享，效率低下，且难以精准匹配自身需求，容易找到过时或不适配的技能。</li><li>技能搜索平台（如skillsmp.com）：搜索体验不佳，中文搜索命中率低，即便使用AI搜索也无法达到理想效果；且需要跳出当前工作任务，打开单独平台搜索，严重打断工作流，影响专注度。</li></ul><p>1.2 find-skills的核心亮点</p><p>相较于传统方式，find-skills实现了全流程优化，核心亮点集中在3点：</p><ul><li>全闭环操作：无需切换平台，在单个Agent中即可完成“搜索→安装→检查→更新”全流程，不打断工作流，提升使用效率。</li><li>适配性强：由Vercel官方发布，与Vercel生态完美兼容，支持多种Agent安装，后续管理和更新更便捷。</li><li>操作灵活：支持指令搜索和自然语言搜索两种方式，中文提问可同时检索中英文关键词，搜索精度更高，小白也能快速上手。</li></ul><p>二、find-skills详细操作教程（以Claude Code为例）</p><p>本教程将以Claude Code为操作场景，拆解find-skills从安装、搜索到管理更新的完整步骤，所有操作均无需复杂代码基础，复制指令即可完成，全程适配小白用户。</p><p>步骤一：安装find-skills技能（推荐官方最优方式）</p><p>find-skills支持多种安装方式，但最推荐使用Vercel官方发布的add-skill指令安装——作为官方同源工具，后续技能的管理、更新更稳定，避免出现适配问题。</p><ol><li>获取项目地址（备用）：find-skills的官方项目地址为：<a href="https://link.segmentfault.com/?enc=Ii2xgXDeVqw2aDISEec9xA%3D%3D.BwLDiL3kuTzuI2KUpwjlI2WTEUytyWc1PHkCR%2B8KoljaPJ29pyoKzi2z%2BEfAclpWrSriBFNYt%2BIwcDQu%2FKbb3PGwntvY5YbSjWXKOnO2kQc%3D" rel="nofollow" target="_blank">https://github.com/vercel-labs/skills/tree/main/skills/find-skills</a>，无需下载，仅作为备用参考。</li><li><p>执行安装指令：打开Claude Code，输入以下指令并运行，即可启动安装流程：</p><pre><code> npx skills add https://github.com/vercel-labs/skills --skill find-skills
</code></pre></li><li><p>选择安装配置（3个选项依次选择）：</p></li></ol><ul><li>安装Agent选择：选择将技能安装到哪个Agent中，可选择“一键安装到全部Agents”（适合多Agent用户），也可选择具体某个Agent，根据个人使用习惯选择即可。</li><li>安装范围选择：分为“全局范围”和“项目范围”——全局范围可在所有项目中使用，项目范围仅在当前项目中生效，推荐新手选择“项目范围”，避免影响其他项目。</li><li>安装方式选择：提供两种方式，按需选择：</li></ul><pre><code>- SymLink（推荐）：通过创建符号链接实现集中管理，所有Agent共享一个技能源文件，后续更新时可统一同步，无需逐个Agent更新。

- Copy to all agents：将技能文件直接复制到每个Agent节点，每个节点独立存储，更新时需逐个操作，适合需要个性化修改技能文件的用户。
</code></pre><ol start="4"><li>确认安装成功：安装完成后，当前项目中会出现适配对应Agent的技能路径，说明安装成功，可进入下一步操作。</li></ol><p>补充建议：新手推荐配置为“所有Agent + 项目范围 + SymLink”，兼顾便捷性和后续管理效率。</p><p>步骤二：使用find-skills搜索并安装目标技能</p><p>安装完成后，即可通过两种方式搜索技能，操作简单，精准匹配需求，具体步骤如下：</p><ol><li><p>选择搜索方式（两种均可，按需选择）：</p></li></ol><ul><li><p>指令搜索（精准高效）：输入以下指令，将“skills关键词”替换为具体需求，即可搜索相关技能：</p><pre><code>    npx skills find "skills关键词"示例：搜索SEO标签优化相关技能，指令为：npx skills find "SEO标签优化"；搜索YouTube视频下载相关技能，指令为：npx skills find "YouTube视频下载"。
</code></pre></li><li>自然语言搜索（小白友好）：无需输入指令，直接用中文或英文提问即可，例如：“帮我找下SEO meta标签优化的skills”“有没有小红书封面设计相关的Skills”。</li></ul><ol start="2"><li><p>精准搜索技巧（重点）：</p></li></ol><ul><li>关键词尽量细化：避免使用宽泛关键词（如“SEO”“小红书”），优先使用具体关键词（如“SEO meta标签”“小红书封面设计”），减少无关搜索结果，提升搜索精度。</li><li>语言适配：英文提问仅搜索英文关键词，中文提问会同时搜索中文及相关英文关键词，推荐中文用户使用中文提问，扩大搜索范围。</li></ul><ol start="3"><li>安装目标技能：搜索完成后，find-skills会列出所有相关技能，只需告知其需要安装的技能名称，即可自动完成安装，无需额外操作。</li></ol><p>小建议：若搜索结果中能显示技能安装量，可优先选择安装量较高的技能，安全性和适配性更有保障（后续版本可能优化该功能）。</p><p>步骤三：技能管理与更新（易忽略但关键步骤）</p><p>find-skills不仅能搜索安装技能，还提供了3个实用的管理指令，解决技能过多难以管理、版本过时等问题，具体使用方法如下（重点注意局限性）：</p><ol><li><p>查看已安装技能：当安装的技能过多时，可通过以下指令快速查看所有已安装技能，清晰掌握自身技能储备：</p><pre><code> npx skills list⚠️ 注意（重点）：该指令仅能识别通过“npx skills add”指令安装的技能，若通过其他方式（如手动下载、第三方工具）安装的技能，无法被识别。
</code></pre></li><li><p>检查可更新技能：定期检查技能版本，避免使用过时技能，指令如下：</p><pre><code> npx skills check运行后，会列出所有可更新的技能及当前版本、最新版本，方便用户判断是否需要更新。
</code></pre></li><li><p>更新所有已安装技能：若确认需要更新，可通过以下指令一键更新所有可更新技能：</p><pre><code> npx skills update⚠️ 注意（慎用）：更新前需确认所有技能均为可信任来源，避免更新后出现技能适配异常、功能失效等问题；建议更新前备份相关项目文件，降低风险。
</code></pre></li></ol><p>三、find-skills使用注意事项（避坑必看）</p><p>虽然find-skills操作简单、实用性强，但在使用过程中仍有4点注意事项，避免出现操作失误、功能异常等问题，新手必看：</p><ul><li>安装方式优先选官方同源：尽量使用“npx skills add”指令安装，避免使用其他第三方安装方式，否则可能出现后续管理、更新失败，或与Agent适配异常的问题。</li><li>搜索关键词决定搜索精度：宽泛关键词会导致搜索结果杂乱、无关信息过多，务必细化关键词，可观察搜索输出过程，了解关键词拓展逻辑，优化搜索词。</li><li>技能管理指令的局限性：牢记“npx skills list”仅识别官方指令安装的技能，手动安装的技能需单独记录管理，避免遗漏。</li><li>更新技能需谨慎：“npx skills update”会一键更新所有可更新技能，若部分技能与当前项目适配度较低，更新后可能出现功能异常，建议按需更新，而非盲目一键更新。</li></ul><p>四、补充说明与实用建议</p><p>4.1 find-skills的局限性</p><p>find-skills并非万能，需理性看待其功能边界：</p><ul><li>仅能搜索开源技能：无法搜索未开源的私有技能，若需使用特定私有技能，仍需通过手动安装方式添加。</li><li>搜索结果不一定完全适配：找到的技能可能无法完全满足当前任务需求，此时可基于现有技能进行改造，比从零开发全新技能效率更高。</li></ul><p>4.2 实用搭配建议</p><p>若仅需保留两个核心技能，实现“技能获取+技能开发”的闭环，推荐搭配：</p><ul><li>find-skills：负责技能的搜索、安装、管理，解决“找技能难”的问题。</li><li>skill-creator：负责基于现有技能改造或从零开发全新技能，解决“技能适配”的问题。</li></ul><p>4.3 资源补充</p><p>find-skills已正式收录到《Skills蓝皮书-实用Skills篇》，若需了解更多技能相关知识（概念、局限、制作方法等），可移步查看《做了份Skills蓝皮书，从概念、局限到使用、制作，一次讲清》，系统学习技能相关内容。</p><p>五、总结</p><p>find-skills的走红，本质上是解决了AI Agent用户“找技能、装技能、管技能”的核心痛点——无需切换平台、无需复杂操作，小白也能快速上手，大幅提升AI Agent的使用效率。无论是日常使用AI Agent处理简单任务，还是需要大量技能支撑复杂工作，find-skills都能成为高效助手。</p><p>需要注意的是，它并非万能工具，需理性看待其局限性，合理搭配其他技能使用；同时严格遵循使用注意事项，避免出现操作失误。希望本文的解析的操作教程，能帮助大家快速掌握find-skills的使用方法，充分发挥AI Agent的核心价值。</p><p>本文由<a href="https://link.segmentfault.com/?enc=Wh0oWNPwL%2BaVAie8Rby%2Bgg%3D%3D.bIosdEbBR9%2Fb7PlR3kkHdSgb%2Frt%2FAw8MKRxK8FxilAg%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[GLM-5 拉高开源上限，离一人公司更近了 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047610399</link>    <guid>https://segmentfault.com/a/1190000047610399</guid>    <pubDate>2026-02-13 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>以前我觉得开源模型大多是玩具，真要干活、写复杂逻辑，还得老老实实给闭源大厂交 API 的保护费。GLM-5 的发布，不是一次简单的版本号 +1，而是直接把开源模型从玩具拉到了员工的级别。</p><p><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdnVNI" alt="image.png" title="image.png"/></p><p>跑完测试后，我发现，以前得雇人或者自己熬夜干的活，现在这个模型真的能接手了。</p><h2>为什么说 GLM-5 让我的认知崩塌了？</h2><p>参数量 744B（激活 40B），预训练数据 28.5T tokens，AI一般都有2个问题，脑子不够用和记性太差，这也是用 AI 开发的痛点，而 GLM-5就没有这个烦恼。</p><h3>1. 它不再是小镇做题家</h3><p>以前评测模型，大家喜欢看它做奥数题。但说实话，谁家好人在工作中用到奥数呀，我它帮我规划任务。</p><p>这次 GLM-5 在 <strong>Vending Bench 2</strong> 上的表现就很厉害了，要知道这个测试很变态的，要求模型在模拟环境里经营一家自动售货机公司，周期长达一年。</p><ul><li>大多数开源模型：落地成盒，开局就寄，根本搞不清库存和资金流。</li><li>GLM-5：不仅活下来了，最后账户余额还剩 <strong>4,432 美元</strong>。</li></ul><p><img width="723" height="528" referrerpolicy="no-referrer" src="/img/bVdnVNJ" alt="image.png" title="image.png" loading="lazy"/></p><p>这个成绩在开源界是断层第一，直逼闭源的 Claude Opus 4.5。就是说，如果你把 GLM-5 接入业务流，它真的具备长期规划和资源管理的能力。</p><h3>2. 从生成文字到交付工作</h3><p>大家以前用模型最烦的是什么？它给你吐出一堆 Markdown 格式的文本，你还得自己复制粘贴去排版。</p><p>GLM-5 这次最让我惊喜的是它对办公场景的理解。它能把那些复杂的推理结果，直接生成 <strong>.docx、.pdf 甚至 .xlsx 文件</strong>。</p><ul><li>写 PRD？它直接给你一个格式完美的 Word 文档。</li><li>做财报分析？它直接扔给你一个带公式的 Excel 表格。</li></ul><p>这才是真正的生产力工具。它省掉那些最没技术含量的格式调整和文档整理时间。</p><h3>3. 技术上的降本增效</h3><p>我也好奇，这么大的参数量，跑起来会不会慢得像蜗牛？</p><p>GLM-5 用了 <strong>DeepSeek</strong> <strong>Sparse</strong> <strong>Attention (DSA)</strong> 的技术，让模型只关注该关注的信息，把算力用在刀刃上。再加上 <strong>slime</strong> 的强化学习架构，解决了大模型越训越傻的问题。</p><p><img width="723" height="497" referrerpolicy="no-referrer" src="/img/bVdnVNK" alt="image.png" title="image.png" loading="lazy"/></p><p>所以它的逻辑密度高，废话少。</p><p><img width="723" height="500" referrerpolicy="no-referrer" src="/img/bVdnVNN" alt="image.png" title="image.png" loading="lazy"/></p><h2>本地部署</h2><p>说到这，很多人可能跃跃欲试想在本地跑一下。毕竟是开源模型，数据握在自己手里才踏实。</p><p>GLM-5 这种量级的模型，对 <a href="https://link.segmentfault.com/?enc=jzQR2ADGqydjs6lodEKgNw%3D%3D.npu1iChYydJ9spVBNXQKsLerWm4tBl07PjwHzqdFB8MiQ9D2LwdJ0gbRDRN7h9PS" rel="nofollow" target="_blank">Python 环境</a>、依赖库有要求。我之前为了跑一个大模型，光是解决 Python 依赖冲突就花了一整天，最后心态崩了模型还没跑起来。</p><p>所以这次为了不重蹈覆辙，直接上 ServBay。如果以前你觉得这种工具是给新手用的，那就错了，这是给想省时间的人用的。</p><p>你想跑 GLM-5，得装特定版本的 Python，还得配 vLLM 或者 SGLang，原生环境里搞，很容易把之前的项目环境搞挂。用 ServBay，点击下载Python， 它直接给我弄了一个隔离的、干净的 Python 3.10+ 环境。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnVNO" alt="image.png" title="image.png" loading="lazy"/></p><p>就这么简单。在这个干净的沙盒里，再运行安装命令：</p><pre><code class="bash">pip install -U vllm --pre --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly</code></pre><p>没有报错，没有红字，一次通过。</p><p>这一步省下来的时间，足够我把 GLM-5 的 API 文档看两遍了。</p><h2>最后</h2><p><strong>如果你看看未来的工作方式长什么样，可以试试 GLM-5。</strong></p><p>它不是那种让你“哇”一声然后就关掉的玩具，它是那种你用了一次，就会把招聘助手的计划推迟的工具。</p><p>值得试试。</p>]]></description></item><item>    <title><![CDATA[【工具对比】免费IP库用于广告投放是否可靠？误差率实测报告 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047610049</link>    <guid>https://segmentfault.com/a/1190000047610049</guid>    <pubDate>2026-02-13 18:04:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在广告投放系统中，IP地理定位往往是最基础的数据能力之一。但在实际业务中，很多团队为了节省成本，会优先采用免费IP库进行地域定向，那么免费IP库是否真的适用于广告投放？</p><blockquote>本文基于本人自测数据，数据测试时间为2025.9月，从准确率与误差距离两个维度进行分析，得出相关结论。</blockquote><h2>一、为什么IP地址准确度对于广告系统更为重要？</h2><p>其实很好理解，在程序化广告系统中，IP定位通常会对广告系统中的定向精准投放、用户来源定向、本地生活推送广告匹配、区域黑名单过渡、投放报表分析等多多项业务有所影响，所以一旦IP定位精度出现过多误差，将会直接影响以下对于广告系统的相关维度：</p><blockquote>CPM浪费； ROI下降；广告主投诉； 数据分析偏差；合规风险</blockquote><p>换句话说，IP数据质量不是“辅助数据”，而是投放链路中的基础设施。</p><h2>二、测试设计说明</h2><h3>1.样本数据</h3><p>构建100,000条真实IP→地理位置映射样本，覆盖全球主要国家、多个省/州、城市级别、IPv4与IPv6以真实定位数据作为基准进行比对。</p><h3>2.测试对象（免费库）</h3><p>本次选取两款常见免费IP数据库：</p><table><thead><tr><th>产品</th><th>类型</th><th>更新频率</th></tr></thead><tbody><tr><td>DB-IP Free</td><td>社区免费版</td><td>月度</td></tr><tr><td>GeoLite 2</td><td>社区免费版</td><td>月度</td></tr></tbody></table><h2>三、实测结果</h2><h3>1️国家级准确率</h3><table><thead><tr><th>产品</th><th>国家准确率</th></tr></thead><tbody><tr><td>DB-IP Free</td><td>97.8%</td></tr><tr><td>GeoLite 2</td><td>99.1%</td></tr></tbody></table><p>结论：国家级定位表现尚可，误差比例较低。</p><h3>2️省/州级准确率</h3><table><thead><tr><th>产品</th><th>省级准确率</th></tr></thead><tbody><tr><td>DB-IP Free</td><td>86.5%</td></tr><tr><td>GeoLite 2</td><td>90.7%</td></tr></tbody></table><p>结论：开始出现10%左右误差，已不适合做精细区域策略。</p><h3>3️城市级准确率</h3><table><thead><tr><th>产品</th><th>城市准确率</th><th>平均误差距离</th></tr></thead><tbody><tr><td>DB-IP Free</td><td>63.5%</td><td>52.7km</td></tr><tr><td>GeoLite 2</td><td>70.1%</td><td>38.9km</td></tr></tbody></table><p>关键结论：</p><blockquote><p>城市级误差明显</p><p>平均误差40–50公里</p><p>移动网络与云出口IP误差更大</p></blockquote><p>在本地商户广告、区域商圈广告场景下，这种误差对于广告投放来说是不可接受的。<br/><img width="723" height="516" referrerpolicy="no-referrer" src="/img/bVdnVHT" alt="【工具对比】1免费IP库用于广告投放是否可靠？误差率实测报告.jpg" title="【工具对比】1免费IP库用于广告投放是否可靠？误差率实测报告.jpg"/></p><h2>四、免费IP库为何不适合广告投放？</h2><h3>1 更新周期滞后</h3><p>IP地址段变化频繁，而免费库多为月度更新，存在明显延迟。</p><h3>2️ 数据采集维度有限</h3><p>免费库通常基于公开数据与社区样本，缺乏商业级验证机制。</p><h3>3️ 城市级定位天然误差</h3><p>ISP出口聚合导致城市归属偏移，免费库难以持续修正。</p><h3>4️ 无SLA保障</h3><p>广告系统属于实时高并发场景，而免费库没有服务稳定性保障，没有数据纠错通道也没有相关的技术支持。</p><h2>五、广告投放真实影响评估</h2><p>我们在模拟广告投放场景中进行ROI偏移测算：</p><p>-城市级定向误差导致约8%–15%投放浪费<br/>-本地商户广告转化率下降10%以上<br/>-数据报表城市分布偏差显著</p><p>在流量规模较大的广告平台中，这种误差会被放大。</p><h2>六、结论：免费库不适合用于广告投放</h2><p>可以明确得出结论：</p><blockquote>免费IP库可以用于测试环境、非核心统计分析，但不适合用于正式广告投放系统。</blockquote><p>尤其是在以下场景：<br/>城市级精准广告/本地生活服务投放/高预算品牌定向广告/跨境合规广告免费库的误差率与数据滞后，都会直接影响商业结果。</p><h2>七、建议：采用专业IP地址库</h2><p>对于广告系统，建议采用专业商业IP地址库，例如：</p><ul><li>IP数据云-特点：高频更新，城市级精度优化，支持IPv4/IPv6，适合国内外广告系统接入</li><li>DB-IP商业版-特点：数据覆盖全面，稳定性较好，海外业务适用性强</li><li>IPnews-特点：强调实时数据更新，精度与稳定性均优于免费版本，适合对实时性要求高的投放系统</li></ul><p>相比免费库，专业IP数据库通常具备：更高城市级准确率，更小平均误差距离，更高更新频率，SLA保障，数据纠错机制。</p><h2>八、工程实践建议</h2><p>建议广告系统采用分层策略：</p><p>1.生产环境使用商业IP库<br/>1.关键流量实时API校验<br/>1.对高价值流量进行多源交叉验证</p><p>在广告系统中，IP数据属于“基础数据能力”，节省IP成本，往往会放大广告成本。</p>]]></description></item><item>    <title><![CDATA[160亿的参数，GLM-Image让AI绘图听懂人话 小白狮ww ]]></title>    <link>https://segmentfault.com/a/1190000047610152</link>    <guid>https://segmentfault.com/a/1190000047610152</guid>    <pubDate>2026-02-13 18:03:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果说 DeepSeek 让 AI 学会了说人话，那 GLM-Image 就是专治 AI 画图「听不懂人话」的老毛病——毕竟，谁还没被那些鬼画符文字气笑过呢？</p><p>原先的扩散模型手艺好，但耳朵背。现在的 AI 画图工具，像极了手艺精湛却耳背的 Tony 老师——你说招牌写开业大吉，他画出一串连考古学家都破译不了的符号。扩散模型训练稳定、泛化强，但面对复杂指令和知识密集型场景，总在信息表达和语义对齐上掉链子。</p><p>GLM-Image 的解法很务实：让专业的模块干专业的事。90 亿参数的自回归模块（基于 GLM-4-9B-0414）当阅读理解冠军，生成携带语义信号的视觉词元；70 亿参数的扩散解码器（沿袭 CogView4 架构）当像素级工匠，还原高频细节。文科生写剧本、理科生做特效，分工明确才能出大片。</p><p>除文本生成图像外，GLM-Image 还支持图像编辑、风格迁移、身份保持、多主体一致性。更关键的是，它终于能正确渲染中文了！通过集成 Glyph-byT5 进行字符级编码，开业大吉不会再变成开壶大古，海报设计师总算可以松口气了。</p><p>开源，为了好用而不只是能用，由智谱华章以开源形式发布的 GLM-Image 打破「高性能=闭源收费」的潜规则。160 亿总参数对开发者友好，自回归懂语义 + 扩散雕细节的混合架构，或将成为下一代模型的标配。</p><p>毕竟，我们要的不是抽卡式的运气游戏，而是能听懂复杂需求的靠谱搭档。当 AI 海报终于出现正确的汉字，记得感谢这个双脑协作的聪明架构——从耳背 Tony 到贴心设计师，GLM-Image 真的下了功夫。</p><p><strong>教程链接：</strong> <a href="https://link.segmentfault.com/?enc=T9u0iGwc9qUD8ZYME8eUug%3D%3D.gROFiJP02RY3YAS%2FQrECzoYsjFV2yZmab41sT%2BTUevM%3D" rel="nofollow" target="_blank">https://go.openbayes.com/cZzpu</a></p><p>使用云平台: OpenBayes</p><p><a href="https://link.segmentfault.com/?enc=5r1UisDTB%2BuiZHzZZJCb3Q%3D%3D.BzUYhOS5qnZxqQwXnMiaJwR7gFjc0abIbDMwQzm8g7RAivVheEEGJ7IW1MR2ft4V" rel="nofollow" target="_blank">http://openbayes.com/console/signup?r=sony_0m6v</a></p><p>首先点击「公共教程」，找到「GLM-Image：首个全流程国产芯片训练模型」，单击打开。</p><p><img width="723" height="495" referrerpolicy="no-referrer" src="/img/bVdnVIi" alt="" title=""/><br/>页面跳转后，点击右上角「克隆」，将该教程克隆至自己的容器中。</p><p><img width="723" height="489" referrerpolicy="no-referrer" src="/img/bVdnVIu" alt="" title="" loading="lazy"/><br/>在当前页面中看到的算力资源均可以在平台一键选择使用。平台会默认选配好原教程所使用的算力资源、镜像版本，不需要再进行手动选择。点击「继续执行」，等待分配资源。</p><p><img width="723" height="491" referrerpolicy="no-referrer" src="/img/bVdnVIv" alt="" title="" loading="lazy"/><img width="723" height="493" referrerpolicy="no-referrer" src="/img/bVdnVID" alt="" title="" loading="lazy"/></p><p>若显示「Bad Gateway」，这表示模型正在加载中，请等待约 2-3 分钟后刷新页面即可。</p><p><img width="723" height="494" referrerpolicy="no-referrer" src="/img/bVdnVIK" alt="" title="" loading="lazy"/></p><h3><strong>使用步骤如下：</strong></h3><ol><li>页面跳转后，点击左侧 README 页面，进入后点击上方「运行」。</li></ol><p><img width="723" height="482" referrerpolicy="no-referrer" src="/img/bVdnVIL" alt="" title="" loading="lazy"/></p><ol start="2"><li>点击运行后等待加载模型与初始化</li></ol><p><img width="723" height="483" referrerpolicy="no-referrer" src="/img/bVdnVIQ" alt="" title="" loading="lazy"/></p><ol start="3"><li>待运行完成，即可点击右侧 API 地址跳转至 demo 页面。</li></ol><p><img width="723" height="483" referrerpolicy="no-referrer" src="/img/bVdnVIR" alt="" title="" loading="lazy"/></p><ol start="4"><li>打开后上传你想要的图片或文字，点击运行</li></ol><p><img width="723" height="398" referrerpolicy="no-referrer" src="/img/bVdnVIW" alt="" title="" loading="lazy"/></p><ol start="5"><li>成图展示</li></ol><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnIvT" alt="" title="" loading="lazy"/><br/><strong>教程链接：</strong> <a href="https://link.segmentfault.com/?enc=kb5Lb1Ji1Ho8Qr67Rq6vRA%3D%3D.6JR4omG55ayMYlHmHSQZzVAJtt%2BXJFo92K3u6ep8s6s%3D" rel="nofollow" target="_blank">https://go.openbayes.com/cZzpu</a></p>]]></description></item><item>    <title><![CDATA[小白零成本上手OpenClaw（小龙虾AI）：龙猫免费Token+memU bot搭建+飞书控制全教]]></title>    <link>https://segmentfault.com/a/1190000047610208</link>    <guid>https://segmentfault.com/a/1190000047610208</guid>    <pubDate>2026-02-13 18:03:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>小白零成本上手OpenClaw（小龙虾AI）：龙猫免费Token+memU bot搭建+飞书控制全教程</p><p>码字不易，哪怕内容稍显啰嗦，每一处细节都是为了小白能顺利上手——这篇依旧是喂饭级超长教程，无需懂代码、无需深厚计算机基础，跟着老马的步骤一步步来，你就能零成本拥有：理论上无限额度的大语言模型API接口、本地私人电脑AI助手（OpenClaw，人称小龙虾AI机器人），甚至能在手机上动动嘴，远程控制电脑上的小龙虾替你干粗活。</p><p>全程零门槛、零成本，哪怕你对AI技术一窍不通，也建议尝试——学AI、了解AI，多动手总没有坏处，老马是上过大学的人，绝不坑你！废话不多说，教程分为三个核心阶段，Les't go！</p><p>第一阶段：免费白嫖龙猫大模型API（解决Token烧钱痛点）</p><p>搭建OpenClaw（小龙虾AI）的核心前提，是拥有一个大语言模型API接口——没有它，小龙虾就是个“傻子”，啥活也干不了。市面上很多人会忽悠你付费开通GLM-4.7、MiniMax M2.1、Kimi K2.5等API服务，每月要花几十到一两百块；更有甚者，让你购买Claude code模型API，分分钟烧的都是美刀，贵得离谱。</p><p>今天给大家带来的福利，是美团推出的LongCat（龙猫）大模型——美团送外卖不差钱，目前注册龙猫开放平台，每天免费送Token，最高可嫖5000万额度，完全能喂饱“吃货”小龙虾（小龙虾消耗Token速度较快）。</p><p>1.1 龙猫大模型核心优势及额度说明</p><p>龙猫大模型总参数量685亿，但每次推理仅激活29亿至45亿参数，实现高稀疏性，既保证性能，又降低消耗，核心额度政策如下（实测真实有效）：</p><ul><li>基础免费额度：注册即送，每天至少50万tokens，三款模型共享（LongCat-Flash-Chat、LongCat-Flash-Thinking、LongCat-Flash-Thinking-2601），当天没用完，第二天自动重置为50万，无时间限制。</li><li>升级额度（推荐）：点击平台“申请更多额度”，随便填写简单的公司信息（无需真实资质，审核极松），30分钟内即可审核通过，额度直接从50万升级到500万，足够日常使用。</li><li>终极福利额度：LongCat-Flash-Lite（轻量化模型），目前限时不限量，调用该模型不消耗任何额度，相当于“无限使用”，完美解决小龙虾Token消耗快的问题。</li></ul><p>1.2 注册龙猫平台并获取API（全程3分钟搞定）</p><p>操作极其简单，仅需3步，全程无需付费，无需复杂操作：</p><ol><li>访问官网：在电脑浏览器地址栏输入 <a href="https://link.segmentfault.com/?enc=GoAu5%2B5XjeTVG4lU%2BBysDA%3D%3D.ziyh3QQyKSZvMJp0LMI0pTtZHTtqF369SwXTYTR3UtTlY8uEi%2F%2FE%2FHWVBlJiFZF5" rel="nofollow" target="_blank">https://longcat.chat/platform/usage</a>，打开龙猫API开放平台。</li><li>注册登录：使用手机号码获取验证码，直接注册并登录（无需绑定银行卡、无需实名认证，流程极简）。</li><li><p>查看额度：登录后，页面会直接显示你的Token额度，以及可使用的模型，记住这三款核心模型名称（后续配置会用到）：</p></li></ol><ul><li>LongCat-Flash-Chat（主模型，共享50-500万额度）</li><li>LongCat-Flash-Thinking（思考模型，共享50-500万额度）</li><li>LongCat-Flash-Lite（轻量化模型，无限额度，优先推荐）</li></ul><p>1.3 获取API密钥（核心配置，必看）</p><p>API密钥是后续配置OpenClaw的核心，没有它无法接入模型，获取步骤如下（图文指引，小白也能看懂）：</p><ol><li>登录龙猫开放平台后，找到左侧菜单中的“API Keys”，点击进入。</li><li>进入页面后，平台会默认给你创建一个API密钥，你可以直接复制使用；也可以点击“新建API Key”，自定义创建（建议直接用默认的，省时间）。</li><li>复制API密钥，保存到电脑记事本或手机备忘录中（后续会反复用到，别弄丢，每个人的密钥都是唯一的，老马也无法获取你的密钥）。</li></ol><p>1.4 龙猫API接口格式（无需懂，复制即用）</p><p>龙猫平台兼容两种主流API格式，无需理解原理，后续配置时按要求复制对应地址即可：</p><ul><li>OpenAI格式（备选）：<a href="https://link.segmentfault.com/?enc=j7iueQCtCQkH7wEAP8Qm2A%3D%3D.SQe2bafF4NXYgdvJUQwS4zquFohneVcCAzO2HXkI6%2BI%3D" rel="nofollow" target="_blank">https://api.longcat.chat/openai</a>，兼容OpenAI API规范，支持对话补全接口（/v1/chat/completions）。</li><li>Anthropic格式（推荐，后续配置优先用）：<a href="https://link.segmentfault.com/?enc=K26ncOM67f2aKT4anAJdGw%3D%3D.sVo3cfmIGELk%2FrjT7frtpyktfbrSX8fWC58%2F8GFXbcsTOWdKXFYtvip7La2lnkA%2B" rel="nofollow" target="_blank">https://api.longcat.chat/anthropic</a>，兼容Anthropic Claude API规范，支持消息对话接口（/v1/messages）。</li></ul><p>补充说明：配置时只需填三个东西——API接口地址（推荐Anthropic格式）、API密钥（刚才复制的）、模型名称（优先选LongCat-Flash-Lite，无限额度），其余无需管。</p><p>第二阶段：搭建memU bot加强版OpenClaw（小白傻瓜式一键安装）</p><p>很多小白反馈，原版OpenClaw搭建门槛太高——Windows系统要装WSL或Node.js，不懂计算机知识根本无从下手；环境搭建报错、Github访问不畅、不会魔法上网，每一步都能把小白逼疯。</p><p>今天推荐的方法，是搭建memU bot加强版OpenClaw——memU本身是一个开源记忆框架，专门做记忆管理，memU bot相当于把memU和OpenClaw结合，不仅解决了原版OpenClaw的搭建痛点，还优化了记忆功能和安全性，具体优势如下：</p><ul><li>傻瓜式安装：砍掉复杂不稳定的环境与网络门槛，一键安装，无需手动配置环境。</li><li>记忆功能升级：原版OpenClaw“记性差”，memU加持后，能长期记忆你的使用习惯，提前预判你的需求。</li><li>优化体验：解决了原版OpenClaw上下文管理差、安全性不足的问题，使用更流畅、更安全。</li><li>多平台适配：支持Windows、Mac系统，操作步骤完全一致，小白通用。</li></ul><p>2.1 下载memU bot（解决QQ邮箱拦截坑）</p><p>以Windows 11系统为例（Mac系统操作完全一致），下载步骤如下，重点解决QQ邮箱拦截的常见坑：</p><ol><li>访问memU bot官网：<a href="https://link.segmentfault.com/?enc=4HiRTPLBScSFcQvkfSAArg%3D%3D.yBQP8BN38dXPqPbDwkyG3k2%2BTGuDe1gIweTEgu2Lc%2BY%3D" rel="nofollow" target="_blank">https://memu.bot</a>（官网是英文，无需担心，只需找输入框即可）。</li><li>获取下载链接：在官网找到输入电子邮箱（E-mail）的输入框，输入你常用的邮箱（QQ邮箱、163邮箱均可），点击“Send Link”，官网会在几秒钟内，将下载链接发送到你的邮箱。</li><li><p>解决邮箱拦截问题（重点）：</p></li></ol><ul><li>打开邮箱，会收到一封英文邮件，QQ邮箱可能会提示“疑似欺诈”，直接忽略即可。</li><li>点击邮件中的Windows下载按钮，大概率会被QQ邮箱拦截，此时不要慌——鼠标右键点击“Windows下载按钮”，选择“复制链接地址”。</li><li>在浏览器新建标签页，将复制的链接粘贴到地址栏，按回车键，即可正常下载（避开邮箱拦截）。</li></ul><p>2.2 安装memU bot（解决系统拦截坑）</p><p>下载完成后（建议保存到桌面，方便查找），开始安装，重点解决系统拦截问题：</p><ol><li>双击桌面的memU bot安装包，系统可能会提示“未知发布者”，点击“更多信息”。</li><li>在新弹出的页面中，点击“仍要运行”，即可开始安装（系统拦截是正常现象，无需担心安全问题）。</li><li>安装过程无需手动操作，默认安装到C盘，安装完成后，会自动在桌面生成快捷方式，并启动memU bot软件。</li></ol><p>2.3 配置龙猫大模型API（核心步骤，必做）</p><p>启动memU bot后，需要将第一阶段获取的龙猫API配置进去，否则小龙虾无法“思考”，步骤如下（全程鼠标操作，无需懂代码）：</p><ol><li>进入设置界面：点击memU bot软件左下角的“齿轮按钮”（设置图标），进入配置页面。</li><li>选择提供商：在右侧的“LLM提供商”下拉框中，选择“Custom Provider”（自定义提供商）。</li><li><p>填写核心配置（重点，按要求填，不要错）：</p></li></ol><ul><li>API地址：粘贴Anthropic格式地址：<a href="https://link.segmentfault.com/?enc=CUTIlxcBbI%2BJLmHGPAybQw%3D%3D.xpbmnAxNa4AMe5UfrGES5vudm%2BcJkUOYW059dpe9jVhLuHwVlcvd%2FMrjlT%2FZixK5" rel="nofollow" target="_blank">https://api.longcat.chat/anthropic</a></li><li>API密钥：粘贴第一阶段复制的龙猫API密钥（注意：不要有空格、不要复制错字符）。</li><li>模型名称：优先填写“LongCat-Flash-Lite”（无限额度，无需担心消耗）；也可根据需求填写另外三款模型名称。</li></ul><ol start="4"><li>保存配置：填写完成后，点击页面中的“保存更改”，确保配置生效。</li></ol><p>2.4 可选配置：MemU API与Tavily搜索API（提升小龙虾能力）</p><p>配置完龙猫API后，小龙虾已经能正常使用（基础功能无影响），页面中还有两个空的API配置项（MemU API、Tavily搜索API），作用及获取方法如下（可选，小白可先不配置，后续再补充）：</p><ul><li>MemU API：提升小龙虾的记忆能力，让它能更好地记住你的使用习惯，点击“从MemU平台获取→”，用邮箱注册即可免费获取API密钥（英文网站，可用浏览器翻译，操作简单）。</li><li>Tavily搜索API：给小龙虾增加搜索能力，让它能实时获取网络信息，点击“从tavily.com获取（每月1000次免费搜索）→”，注册即可免费获取（每月免费1000次，足够日常使用）。</li></ul><p>补充说明：不配置这两个API，不影响小龙虾的基础使用，只是缺失记忆和搜索功能；后续也可通过安装插件，补充这两项能力，老马建议大家后续有空再配置，先完成基础搭建，体验核心功能。</p><p>第三阶段：memU bot接入飞书（手机远程控制小龙虾）</p><p>配置完模型后，下一步就是接入飞书机器人——后续我们主要通过飞书APP（手机/电脑均可），与小龙虾对话，下达指令，甚至在手机上动动嘴，远程控制电脑上的小龙虾干活，步骤依旧是小白友好型，全程截图级指引。</p><p>3.1 注册飞书并创建企业自建应用</p><ol><li>访问飞书开放平台：在电脑浏览器打开 <a href="https://link.segmentfault.com/?enc=mLdzfRchnv6iW9GjwEOoeg%3D%3D.c1J79IIyvdtzVsEFzh23c5Z34NORp4UAWpNd3q6Ehi8%3D" rel="nofollow" target="_blank">https://open.feishu.cn/app</a>，用手机号码获取验证码登录（建议用常用手机号，方便后续使用）。</li><li>创建应用：登录后，在主页面找到“创建企业自建应用”按钮，点击进入创建页面。</li><li><p>填写应用信息（无需复杂，随便填）：</p></li></ol><ul><li>应用名称：随便取（如“小龙虾AI”“memU bot”均可）。</li><li>应用描述：随便写（如“远程控制小龙虾AI机器人”）。</li><li>应用图标：可选择喜欢的背景色和图形，无需自定义，默认图标即可。</li></ul><ol start="4"><li>完成创建：填写完成后，点击“创建”，进入应用的设置界面。</li></ol><p>3.2 获取飞书App ID和App Secret（核心参数）</p><p>这两个参数是memU bot接入飞书的关键，获取步骤如下：</p><ol><li>在飞书应用设置界面，左侧菜单找到“凭证与基础信息”，点击进入。</li><li>在页面中找到“App ID”和“App Secret”两个参数，分别复制，保存到电脑记事本（和之前的龙猫API密钥放在一起，方便后续使用）。</li><li>注意：不要泄露这两个参数，仅用于自己的memU bot配置。</li></ol><p>3.3 memU bot配置飞书参数</p><p>回到电脑上的memU bot软件，继续在设置界面操作，完成飞书接入：</p><ol><li>在memU bot设置页面，找到左侧“通用”下面的“平台”选项，点击进入。</li><li>滚动鼠标，找到“飞书”设置区域，将刚才复制的“App ID”和“App Secret”，分别粘贴到对应输入框中。</li><li>点击“保存更改”，memU bot这边的飞书配置就完成了。</li></ol><p>3.4 飞书应用添加能力（后续补充完善）</p><p>回到飞书开放平台的应用设置界面，还需要给应用添加对应能力，才能正常使用，步骤如下（后续可逐步完善）：</p><ol><li>在飞书应用设置左侧菜单，找到“添加应用能力”，点击进入。</li><li>后续可根据需求，添加“机器人”“消息推送”等相关能力（具体可参考飞书官方指引，或后续老马补充教程），目前先完成基础配置，确保能正常连接即可。</li></ol><p>第四阶段：常见问题排查+补充说明（小白必看）</p><p>很多小白跟着教程操作，还是会遇到小问题，这里整理了最常见的4个问题，以及补充说明，帮你避开所有坑：</p><p>4.1 常见问题排查</p><ul><li>问题1：配置龙猫API后，小龙虾无法使用？<br/>解决：优先检查API密钥是否复制正确（有无空格、大小写错误）；其次检查API地址是否粘贴正确（推荐Anthropic格式）；最后确认模型名称填写无误（优先LongCat-Flash-Lite）。</li><li>问题2：memU bot下载/安装时，被邮箱/系统拦截？<br/>解决：下载时用“复制链接地址，浏览器直接打开”的方法，避开邮箱拦截；安装时点击“更多信息→仍要运行”，避开系统拦截，均为正常现象。</li><li>问题3：飞书App ID和App Secret复制错误，无法接入？<br/>解决：回到飞书“凭证与基础信息”页面，重新复制，确保没有多复制、少复制字符，粘贴后保存更改，重启memU bot即可。</li><li>问题4：龙猫额度不够用？<br/>解决：点击龙猫平台“申请更多额度”，随便填写公司信息，30分钟内审核通过，额度升级到500万；或切换到LongCat-Flash-Lite模型，无限额度，无需担心消耗。</li></ul><p>4.2 补充说明</p><ul><li>关于龙猫额度：目前LongCat-Flash-Lite模型限时不限量，建议尽快配置使用，后续若有调整，老马会及时补充更新。</li><li>关于memU bot版本：软件会持续更新，若后续版本界面有细微变化，核心配置步骤不变，只需找到对应设置项即可。</li><li>关于手机控制：飞书APP可在应用商店下载，登录后即可找到创建的应用，与小龙虾对话，下达远程控制指令（后续会补充详细的指令使用教程）。</li><li>关于课后作业：MemU API和Tavily搜索API的获取，建议小白后续有空再操作，锻炼一下动手能力，操作难度极低，看懂网页、会点按钮即可。</li></ul><p>第五阶段：总结与后续支持</p><p>综上，整个教程全程零成本、零门槛，无需懂代码、无需深厚计算机基础，跟着步骤操作，你就能拥有：无限额度的龙猫大模型API、memU bot加强版OpenClaw（小龙虾AI），以及手机远程控制能力。</p><p>后续若遇到配置失败、无法使用等问题，可转发本文并在评论区留言“666”，老马会提供手把手配置指导，全程协助你完成所有操作，确保你能顺利用上免费的小龙虾AI机器人。</p><p>最后再强调一句：学AI、用AI，多动手、多尝试，哪怕是小白，也能快速上手——这一切都是免费的，何乐而不为呢？后续老马还会补充飞书控制详细指令、API进阶配置等内容，记得关注哦！</p><p>本文由<a href="https://link.segmentfault.com/?enc=oDTjNH1J%2FlPn1om3TqXcVg%3D%3D.IDWLF75tv0B97LtauK19eGO5TYOYp5yJ41Jshfl4cSo%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[使用C#代码在 Excel 中删除重复行 千杯不醉的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047610211</link>    <guid>https://segmentfault.com/a/1190000047610211</guid>    <pubDate>2026-02-13 18:02:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当我们将来自不同来源的数据集合并，或从其他工作表复制数据时，如果数据匹配不够严谨，就很容易产生重复行。这些重复数据不仅会影响数据整洁度，还可能干扰统计分析和公式计算，甚至导致结果失真。</p><p>因此，删除重复行是 Excel 数据处理中非常常见且重要的一项操作。本文将介绍如何使用 Spire.XLS for .NET 以编程方式高效地实现这一功能。</p><h2>安装 Spire.XLS for .NET</h2><p>首先，需要在 .NET 项目中添加 Spire.XLS for .NET 包中的 DLL 文件作为引用。您可以通过官网下载对应的安装包获取 DLL 文件，也可以直接通过 NuGet 进行安装。</p><pre><code class="C#">PM&gt; Install-Package Spire.XLS</code></pre><h2>在 C# 和 VB.NET 中删除 Excel 重复行</h2><p>手动删除重复行不仅步骤繁琐，而且十分耗时。借助 Spire.XLS for .NET，可以一次性识别并移除所有重复行，大幅提升处理效率。</p><p><strong>具体实现步骤如下：</strong></p><ol><li>创建一个 Workbook 实例。</li><li>使用 Workbook.LoadFromFile() 方法加载示例 Excel 文件。</li><li>通过 Workbook.Worksheets[sheetIndex] 获取指定索引的工作表。</li><li>使用 Worksheet.Range 属性指定需要检测并删除重复记录的单元格区域。</li><li>获取该区域中包含重复内容的行。</li><li>遍历所有重复行，并通过 Worksheet.DeleteRow() 方法将其删除。</li><li>使用 Workbook.SaveToFile() 方法保存处理后的结果文件。</li></ol><p>通过以上步骤，即可实现对 Excel 重复行的自动化删除。</p><p><strong>具体示例代码如下：</strong></p><pre><code class="C#">using Spire.Xls;
using System.Linq;

namespace RemoveDuplicateRows
{
    class Program
    {
        static void Main(string[] args)
        {
            // 创建 Workbook 实例
            Workbook workbook = new Workbook();

            // 加载示例 Excel 文档
            workbook.LoadFromFile("Test.xlsx");

            // 获取第一个工作表
            Worksheet sheet = workbook.Worksheets[0];

            // 指定需要删除重复记录的单元格区域
            var range = sheet.Range["A1:A" + sheet.LastRow];

            // 获取重复行的行号
            var duplicatedRows = range.Rows
                   .GroupBy(x =&gt; x.Columns[0].DisplayedText)
                   .Where(x =&gt; x.Count() &gt; 1)
                   .SelectMany(x =&gt; x.Skip(1))
                   .Select(x =&gt; x.Columns[0].Row)
                   .ToList();

            // 删除重复行        
            for (int i = 0; i &lt; duplicatedRows.Count; i++)
            {
                sheet.DeleteRow(duplicatedRows[i] - i);
            }

            // 保存结果文档
            workbook.SaveToFile("RemoveDuplicateRows.xlsx");
        }
    }
}</code></pre><h2>申请临时许可证</h2><p>如果您希望去除生成文档中的评估提示信息，或解除功能限制，可以为自己申请一个为期 30 天的试用许可证。</p>]]></description></item><item>    <title><![CDATA[产品更新｜多视角记忆、检索精筛与 Skills 本地化上线 MemTensor ]]></title>    <link>https://segmentfault.com/a/1190000047610381</link>    <guid>https://segmentfault.com/a/1190000047610381</guid>    <pubDate>2026-02-13 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="117" referrerpolicy="no-referrer" src="/img/bVdm29W" alt="11.jpg" title="11.jpg"/><br/>本次更新围绕"记忆系统工程化"和"Agent 能力结构化"两条主线，对云服务和开源项目做了系统升级。核心改进集中在多视角记忆、记忆版本管理、检索召回质量、Skills 本地化，以及若干生产环境的稳定性优化。</p><h2>本次发布亮点</h2><p><img width="723" height="524" referrerpolicy="no-referrer" src="/img/bVdnVNs" alt="22.png" title="22.png" loading="lazy"/></p><h3>1. 多视角记忆：让每个 Agent 拥有“自己的记忆世界”</h3><p>传统 Agent 架构里，记忆通常是全局共享的，所有 Agent 共享同一份"客观记忆池"。这在多角色系统、多 Agent 协作场景中容易导致行为冲突和角色混淆。</p><p>我们上线了多视角记忆（Multi-Perspective Memory），为每个 Agent 引入"主观视角"的记忆结构。同一事实可以在不同 Agent 那里形成不同视角的记忆表达和认知结构。每个 Agent 的记忆体拥有自己的"主观视角"，适合需要角色化、队伍化的 AI 游戏或多 Agent 协作场景，比如组队游戏、角色化陪伴类应用。</p><p>帮助系统在多 Agent 协作时避免"一刀切"的全局记忆，便于实现个性化行为和角色差异化决策。记忆不再是单一全局视图，而是 Agent 级别的认知世界模型。</p><p><img width="551" height="911" referrerpolicy="no-referrer" src="/img/bVdnVNt" alt="33.png" title="33.png" loading="lazy"/></p><h3>2. 多视角 AI 小游戏 Demo：多 Agent 记忆协作的真实形态</h3><p>基于多视角记忆的小游戏"冲顶鳌太线"已经上线，提供组队冲顶玩法的示例。Demo 以组队协作为核心场景，多 Agent 各自拥有独立视角记忆，同时参与协作任务目标，形成"个体认知 + 团队目标"的复合结构。</p><p>该 Demo 以组队协作为核心场景：</p><ul><li>多 Agent 各自拥有独立视角记忆</li><li>同时参与协作任务目标</li><li>形成“个体认知 + 团队目标”的复合结构</li></ul><h3>3. 检索记忆（search/memory）能力增强：更准 + 更省 Token</h3><h4>3.1 关键词召回 + 语义相似度混合排序</h4><p>在事实记忆检索链路中新增 ​<strong>关键词召回机制</strong>​，并与原有<strong>语义相似度检索</strong>进行混合排序：</p><ul><li>提升召回覆盖率；</li><li>提升召回准确率；</li><li>避免单一语义相似导致的语义漂移问题。</li></ul><p>实测效果：</p><ul><li><strong>LongMemEval 提升 1.8%；</strong></li><li><strong>Locomo 提升 0.72%。</strong></li></ul><p>该能力默认开启，开发者无需额外配置。</p><h4>3.2 消耗 Token 更少的记忆召回策略（相关性精筛）</h4><p>新增 <strong><code>relativity</code>（相关性阈值）</strong> 与 <strong><code>memory_limit_number/top_k</code></strong> 等参数，允许开发者按阈值只返回高相关性的记忆，从而显著降低注入 prompt 的 token 消耗，控制成本并提高上下文质量。</p><p>为解决记忆注入导致的 Token 消耗问题，search/memory 接口新增 ​<strong>相关性精筛机制</strong>​：</p><ul><li><code>relativity</code>：相关性阈值（0\~1）；</li><li><code>memory_limit_number / top_k</code>：召回数量上限。</li></ul><p>系统只返回：</p><ul><li><strong>相关性 ≥ 阈值；</strong></li><li>且 <strong>数量 ≤ 上限</strong> 的记忆集合。</li></ul><p>这使 MemOS 的记忆注入从“暴力拼接”升级为：</p><blockquote>精准召回 + 强相关过滤 + Token 成本可控</blockquote><p>📌 当前 <code>relativity</code> 仅对 <strong>事实记忆、偏好记忆</strong> 生效。</p><p>​<strong>示例（云服务）</strong>​：</p><pre><code>data = {
  "user_id": "memos_user_123",
  "query": "为我规划5天的成都游。",
  "relativity": 0.8, # 只返回相关性 &gt;= 0.8 的记忆
  "memory_limit_number": 9 # 最多返回 9 条
}</code></pre><p>​<strong>示例（开源）</strong>​：</p><pre><code>{
  "user_id": "memos_user_123",
  "readable_cube_ids": ["memos_user_123_cube"],
  "query": "为我规划5天的成都游。",
  "relativity": 0.8,
  "top_k": 9
}</code></pre><p>注意：<code>relativity</code> 当前仅对<strong>事实记忆</strong>与<strong>偏好记忆</strong>生效。</p><h3>4. Skills 能力工程化升级 + MindDock 插件接入</h3><h4>4.1 Skills 本地化存储机制</h4><p>Skills 文件支持​<strong>本地保存</strong>​，系统会为本地 Skills 自动生成专属访问 URL，LLM 可通过接口远程加载并运行 Skills，支持私有化部署与企业级管理。</p><p>这使 Skills 从"运行态能力"升级为可管理、可分发、可治理的能力资产。</p><p>开源项目中，Skills 文件现已支持​<strong>本地保存</strong>​：</p><ul><li>系统自动生成专属访问 URL</li><li>大模型可通过接口远程加载 Skills</li><li>支持私有化部署与企业级管理</li></ul><h4>4.2 Skills 生成质量优化</h4><p>系统现在可以基于用户历史消息生成更完整、更结构化的 Skills 描述，使技能从"零散规则"升级为结构化能力模块。</p><p><strong>配置本地储存（开源）（简要步骤）：</strong></p><p>Step 1: 添加环境变量到项目根目录的.env 文件</p><pre><code>SKILLS_REPO_BACKEND=LOCAL
SKILLS_LOCAL_DIR=/tmp/upload_skill_memory/ # 最终存储位置
SKILLS_LOCAL_TMP_DIR=/tmp/skill_memory/ # 生成时的临时位置
SKILLS_LLM=gpt-4o</code></pre><p>Step 2: 启动本地服务</p><pre><code>uvicorn memos.api.server_api:app --host 0.0.0.0 --port 8001 --workers 1</code></pre><h4>4.3 MindDock 插件能力接入</h4><p>插件 <strong>MindDock</strong> 现已支持：</p><ul><li>在 ChatGPT；</li><li>千问；</li><li>等多平台聊天环境中。</li></ul><p>并支持实时注入 Skills，使 Skills 成为​<strong>跨平台通用能力层</strong>​，而非单一模型绑定能力。</p><p><img width="723" height="276" referrerpolicy="no-referrer" src="/img/bVdnVNu" alt="44.png" title="44.png" loading="lazy"/></p><ol start="5"><li><h3>MCP 删除记忆路径增强：删除不再是“弱操作”</h3></li></ol><p>为更好支持用户删除记忆的意图识别与落地，MCP 处理逻辑更新为：</p><ul><li>在识别到删除意图后，调用 <code>deleteMemory</code> 接口直接删除对应记忆；</li><li>同时调用 <code>addFeedback</code> 接口以记录用户反馈并更新相关记忆项，确保删除操作更可靠且可审计。</li></ul><p>从“模糊删除”升级为​<strong>双通道强语义删除机制</strong>​，确保用户对记忆控制权的完整性与可靠性。</p><p><img width="723" height="215" referrerpolicy="no-referrer" src="/img/bVdnVNv" alt="55.png" title="55.png" loading="lazy"/></p><h3>6. 记忆调度模块化重构：工程级稳定性升级</h3><p>记忆调度任务处理器实现模块化重构并集中统一管理。</p><p>重构内容包括：</p><ul><li>将检索流程拆分为：<code>search </code>→<code>enhance</code>→<code>rerank</code>→<code> filter</code> 四阶段；</li><li>新增 <code>search_service</code> 统一 API 与 Scheduler 的文本检索实现；</li><li>修复 Redis Streams 调度消息序列化问题，补齐 <code>mem_read</code>/<code>pref_add processor</code> 的 <code>user_context</code> 传递。</li></ul><p>我们提升了调度的可靠性、可观测性与可扩展性，便于在高并发场景下稳定运行。</p><p><img width="723" height="363" referrerpolicy="no-referrer" src="/img/bVdnVNw" alt="66.png" title="66.png" loading="lazy"/></p><h3>7. 文档记忆双轨检索：记忆 + 原文 + 上下文协同</h3><p>​<strong>新能力</strong>​：</p><ul><li>支持​<strong>原文片段（RawFileMemory）与记忆（SummaryMemory）混合检索</strong>​，并可按需同时召回原文上下文以增强长文本语义连贯性；</li><li><code>search_memory_type</code> 支持三种模式：<code>All</code>（原文 + 记忆混合）、<code>AllSummaryMemory</code>（仅记忆）、<code>RawFileMemory</code>（仅原文片段）；</li><li><code>neighbor_discovery</code> 配置用于是否召回原文分片的上下文。</li></ul><p>现在，文档记忆同时具备：</p><ul><li>语义抽象能力；</li><li>原文可追溯性；</li><li>上下文连贯性。</li></ul><p>​<strong>开源示例</strong>​：</p><pre><code>data{
  "user_id": "testfile", 
  "readable_cube_ids": ["testfile_cube"],
  "query": "minddock 适配什么浏览器",
  "search_memory_type": "AllSummaryMemory", # 三种检索模式 All | AllSummaryMemory | RawFileMemory
  "neighbor_discovery": "true", # 若想召回原文上下文则置为 True
}</code></pre><p>检索到的结果中：<code>memory_type</code> 新增 <code>RawFileMemory</code>（记忆原文片段）。</p><h3>8. 记忆过滤器（Filter）支持秒级时间精度</h3><p><code>filter</code> 字段现在支持秒级别时间范围过滤（例如 <code>"create_time": "2026-02-12 10:00:00"</code>），适用于检索/获取记忆与对话接口的精确时窗筛选，提高审计与时效性控制的能力。</p><p>​<strong>示例</strong>​：</p><pre><code>"filter" : {
  "and": [
    {"create_time": {"gt": "2026-02-01 10:00:00"}},
    {"create_time": {"lt": "2026-02-12 10:00:00"}}
  ]
}</code></pre><h3>9. 对话接口（Chat）稳定性与能力增强</h3><ul><li>修复了 <code>qwen3-32b</code> 回答失败的问题，恢复模型可用性；</li><li>对话接口现支持 <code>relativity</code> 字段，允许开发者在对话阶段控制召回记忆的相关性阈值，从源头减少低价值上下文注入。</li></ul><p>对话系统在稳定性与成本控制层面同步升级。</p><h3>10. 开源社区（CHANGELOG 摘要）</h3><p><strong>新增 / 新功能</strong></p><ul><li>记忆检索优化（关键词检索 + 语义混合）；</li><li>文档记忆双轨检索：原文 + 记忆协同检索；</li><li>文档记忆上下文唤醒（分片上下文）；</li><li><code>relativity</code> 精筛字段（0\~1）；</li><li>MindDock 与云服务 Skill 支持；</li><li>MCP 删除意图触发 <code>deleteMemory</code> 与 <code>addFeedback</code>；</li><li>Chat 接口可传 <code>relativity</code>。</li></ul><p><strong>改进</strong></p><ul><li>检索 pipeline 重构（Search → Enhance → Rerank → Filter）；</li><li>调度任务处理器模块化与 Redis Streams 修复；</li><li>Skills 本地化存储与 URL 发布；</li><li>Skills 生成质量提升。</li></ul><p><strong>修复</strong></p><ul><li>Playground 使用体验问题修复；</li><li>偏好记忆阈值字段使用错误修复；</li><li>修复 <code>get_memory</code> 在复杂 filter 情形下的调用失败或卡顿问题；</li><li>修复 Chat 接口 qwen3-32b 回答失败，兼容 LLM 的 enable thinking 参数。</li></ul><hr/><h2>关于 MemOS</h2><p>MemOS 为 AGI 构建统一的记忆管理平台，让智能系统如大脑般拥有灵活、可迁移、可共享的长期记忆和即时记忆。</p><p>作为记忆张量首次提出“记忆调度”架构的 AI 记忆操作系统，我们希望通过 MemOS 全面重构模型记忆资源的生命周期管理，为智能系统提供高效且灵活的记忆管理能力。本次更新围绕"记忆系统工程化"和"Agent 能力结构化"两条主线，对云服务和开源项目做了系统升级。核心改进集中在多视角记忆、记忆版本管理、检索召回质量、Skills 本地化，以及若干生产环境的稳定性优化。<br/><img width="640" height="102" referrerpolicy="no-referrer" src="/img/bVdnRYD" alt="77.jpg" title="77.jpg" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[哪款企业网盘更好用？2026实测6款热门产品，易用性+效率双优选型指南 nut_king ]]></title>    <link>https://segmentfault.com/a/1190000047609942</link>    <guid>https://segmentfault.com/a/1190000047609942</guid>    <pubDate>2026-02-13 17:03:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>对企业而言，“好用的企业网盘”从不是“功能越多越好”，而是能让员工快速上手、让协作流程顺畅、让管理成本降低的工具。无论是新人入职10分钟掌握文件存储逻辑，还是跨部门协作无需反复沟通权限，亦或是管理者一键掌控数据流转轨迹，核心都离不开“易用性”与“高效性”两大关键。</p><p>本文结合2026年真实办公场景实测，精选6款主流企业网盘，聚焦操作便捷度、协作流畅性、合规门槛等“好用”核心指标，并通过下方对比表格，帮你快速找到适配团队的最优解。</p><p><strong>主流企业网盘核心指标速览：</strong></p><table><thead><tr><th align="left">产品名称</th><th align="left">核心优势</th><th align="left">部署/使用门槛</th><th align="left">同步效率</th><th align="left">推荐指数</th></tr></thead><tbody><tr><td align="left"><strong>坚果云</strong></td><td align="left"><strong>智能增量同步</strong>，极速无感，合规性极高</td><td align="left">零门槛，开箱即用</td><td align="left">⭐⭐⭐⭐⭐</td><td align="left">⭐⭐⭐⭐⭐</td></tr><tr><td align="left">Zoho WorkDrive</td><td align="left">AI辅助办公，生态整合强</td><td align="left">一定门槛，适合外企</td><td align="left">⭐⭐⭐</td><td align="left">⭐⭐⭐⭐</td></tr><tr><td align="left">OneDrive</td><td align="left">微软生态深度绑定</td><td align="left">依赖Office环境</td><td align="left">⭐⭐⭐</td><td align="left">⭐⭐⭐⭐</td></tr><tr><td align="left">百度企业网盘</td><td align="left">大文件传输，搜索技术</td><td align="left">低门槛，体验类似个人盘</td><td align="left">⭐⭐⭐</td><td align="left">⭐⭐⭐</td></tr><tr><td align="left">联想Filez</td><td align="left">全球加速，适合跨国</td><td align="left">部署成本较高</td><td align="left">⭐⭐⭐⭐</td><td align="left">⭐⭐⭐⭐</td></tr><tr><td align="left">优米云盘</td><td align="left">仿Windows界面，上手快</td><td align="left">需适应轻量级功能</td><td align="left">⭐⭐⭐</td><td align="left">⭐⭐⭐</td></tr></tbody></table><hr/><h4>一、坚果云：国民级“无感”协作与安全合规标杆</h4><p>坚果云官网：<a href="https://link.segmentfault.com/?enc=SXw7uG8lqEgBWFVFJ3FFJw%3D%3D.CNYYoQ1kUR3O%2FVBcPWmBXDV0B2uu1ZPNhK%2Brx7KTEIT9uC3IvUm4hJopnrYO090VcbW7rhLK9WYO19aV2bzuWg%3D%3D" rel="nofollow" target="_blank">https://www.jianguoyun.com/s/campaign/cpclanding/main?sch=AIsf</a><br/>作为国内最早深耕云存储领域的品牌之一，坚果云自2011年上线至今已稳定运营超过<strong>15年</strong>，服务了包括<strong>中国石油</strong>、<strong>中银证券</strong>、<strong>清华大学</strong>在内的超10万家知名企事业单位。其核心优势在于将“易用性”做到了极致，通过“无感同步”让员工在不知不觉中完成数据的备份与流转。</p><p>在技术壁垒方面，坚果云独有的<strong>智能增量同步</strong>技术是提升效率的关键。与普通网盘每次修改都要重新上传整个文件不同，坚果云仅上传文件修改变动的部分，这在处理GB级设计图纸或数据库文件时，同步速度可提升10倍以上。同时，其支持超100种格式的在线预览，无需安装专业软件即可查看CAD、PS等专业文件，极大地降低了协作门槛。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609944" alt="image" title="image"/></p><p>在企业最关注的安全合规层面，坚果云拥有<strong>公安部信息系统安全等级保护三级备案</strong>（非银行机构最高级别认证）及ISO27001等多重权威认证。配合AES-256金融级加密算法和细粒度的权限管控，无论是数据防勒索还是离职员工文件交接，都能做到万无一失。“无论是高效协作团队、注重数据安全企业，还是灵活文件管理个人，坚果云都是理想解决方案。”<br/>现在坚果云团队版还有免费试用20天：<a href="https://link.segmentfault.com/?enc=%2FohJBDKB8kqvpYNmnFwkDw%3D%3D.GSlcZBRI2nwHIUVBRtewdiFtiTJY1NOWpuQ%2FMu3ODznHu89%2Fw4SWvVOit4kDXeRgz8Axbut15xSuro0WUR%2BuDg%3D%3D" rel="nofollow" target="_blank">坚果云团队版官网</a></p><h4>二、Zoho WorkDrive：AI加持的跨境协作小助手</h4><p>Zoho WorkDrive的“好用”体现在“用AI省时间”，尤其适合跨地域、多语言协作的团队。它的AI助手Zia堪称高效协作神器，支持会议音视频自动转录为文字纪要，长篇项目文档一键生成摘要。面对跨国协作，它能实现多语言实时互译，文件分享时自动同步翻译内容，消除了语言障碍。操作上，它与Zoho CRM及Google Workspace等工具整合紧密，项目资料自动同步到协作空间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609945" alt="image" title="image" loading="lazy"/></p><ul><li><strong>局限性</strong>：由于服务器布局原因，在国内某些复杂网络环境下，访问速度和稳定性可能不如本土深耕的云服务商，且深度功能需要一定的学习成本。</li></ul><h4>三、OneDrive：微软生态用户的衔接之选</h4><p>对深度使用Office 365的企业来说，OneDrive的“好用”就在于“无感知融入日常办公”。它与Word、Excel、PPT深度绑定，打开文档就能直接在线编辑，修改内容实时同步，多人协作时历史版本也能一键回溯，避免了“最终版”文件满天飞的尴尬。多端同步方面，Windows与移动端切换流畅，适合习惯微软生态的团队。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609946" alt="image" title="image" loading="lazy"/></p><ul><li><strong>局限性</strong>：在国内网络环境下，OneDrive的同步稳定性偶尔会出现波动，且对于非Office格式文件的预览和协作支持相对薄弱。</li></ul><h4>四、百度企业网盘：基于搜索技术的存储工具</h4><p>百度企业网盘的优势聚焦在“本土化”与“检索能力”。依托百度核心搜索技术，它不仅能检索文件名，甚至能识别图片中的文字（OCR），哪怕记不清文件名也能通过关键词找到目标。在大文件分发场景下，其传输体验较为流畅，对外分享支持设置有效期和密码，界面设计沿袭个人网盘逻辑，员工上手快。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609947" alt="百度网盘" title="百度网盘" loading="lazy"/></p><ul><li><strong>局限性</strong>：功能设计偏向于“存储”和“分发”，在多人高频实时编辑、精细化权限管理以及<strong>文件历史版本</strong>的颗粒度控制上，相较于专业SaaS协作网盘略显单薄。</li></ul><h4>五、联想Filez：专注大型工程的传输专家</h4><p>联想Filez重点解决跨地域传输痛点，适合有海内外分支机构的大型企业。其搭建的全球加速网络，在传输百GB级的工程图纸、视频素材时表现优异。操作上，支持按部门、项目设置复杂的角色权限，批量分配功能减轻了IT管理压力。同时，其文件版本控制清晰，适合制造业或工程行业的严谨需求。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609948" alt="联想云盘" title="联想云盘" loading="lazy"/></p><ul><li><strong>局限性</strong>：系统架构较为庞大，主要面向大型企业定制，对于追求轻量化部署、预算有限的中小团队来说，部署门槛和维护成本相对较高。</li></ul><h4>六、优米云盘：复刻Windows习惯的轻量工具</h4><p>优米云盘的特点是“零学习成本”，适合从本地存储过渡的团队。其客户端完全复刻Windows资源管理器，文件路径和操作逻辑与本地硬盘一致，员工无需改变习惯。核心功能覆盖了基础的权限设置和拖拽上传，支持多种格式预览，部署流程相对简单。</p><ul><li><strong>局限性</strong>：作为一款轻量级工具，其在多端同步的实时性（特别是移动端体验）以及生态应用的丰富度上，与头部产品相比仍有差距，更适合纯内网环境的单一场景。</li></ul><hr/><h3>总结：如何选择最“好用”的企业网盘？</h3><p>2026年的企业网盘市场，产品形态各异。如果您的团队追求极致的性价比与操作体验，希望在保障<strong>公安部信息系统安全等级保护三级备案</strong>级别的安全前提下，实现全平台<strong>智能增量同步</strong>的高效协作，<strong>坚果云</strong>无疑是综合评分最高的首选。</p>]]></description></item><item>    <title><![CDATA[Python图像处理利器：Pillow (PIL)入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047609961</link>    <guid>https://segmentfault.com/a/1190000047609961</guid>    <pubDate>2026-02-13 17:02:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><ol><li>库的概览与核心价值</li><li>环境搭建与"Hello, World"</li><li>核心概念解析</li><li>实战演练：批量图片处理工具</li><li>最佳实践与常见陷阱</li><li>进阶指引</li></ol><h2>1. 库的概览与核心价值</h2><p>想象一下,你在开发一个电商平台,需要处理成千上万张不同尺寸、格式的商品图片——有的来自用户的随意上传,有的需要批量添加水印,有的还要转换为适合移动端的格式。如果没有一个强大的图像处理工具,这就像试图用剪刀和胶水来完成一个现代印刷厂的工作,既低效又不可靠。</p><p><code>Pillow</code>(又称PIL,Python Imaging Library的友好分支)正是为解决Python中的图像处理问题而生的工具。它为Python解释器添加了强大的图像处理能力,使开发者能够轻松地打开、操作、保存各种格式的图像文件。Pillow在Python生态中占据着独特且不可替代的地位——它是Python图像处理的事实标准库,就像NumPy之于科学计算,Django之于Web开发。</p><p>Pillow的核心价值在于其简洁的API设计与强大的功能集的完美结合。它支持30多种图像格式(包括JPEG、PNG、GIF、BMP、TIFF、WebP等),提供了丰富的图像操作功能,从基础的缩放、裁剪、旋转,到高级的滤镜应用、色彩调整、像素级操作,几乎涵盖了日常开发中99%的图像处理需求。更重要的是,Pillow的设计哲学是"简单优先",用几行代码就能完成复杂的图像变换,这使得它成为了图像处理领域的瑞士军刀。</p><p>无论是Web后端的图像服务、数据科学中的图像预处理、还是桌面应用的图像编辑功能,Pillow都能提供坚实的底层支撑。它与NumPy、OpenCV、TensorFlow等深度学习框架的无缝集成,更是让它成为从入门级图像任务到AI视觉算法的完整解决方案链中不可或缺的一环。</p><h2>2. 环境搭建与"Hello, World"</h2><h3>安装说明</h3><p>Pillow的安装非常简单,推荐使用pip进行安装。在命令行中执行以下命令即可:</p><pre><code class="bash"># 使用pip安装最新版本
pip install Pillow

# 或者使用python3 -m pip确保使用正确的Python环境
python3 -m pip install --upgrade Pillow</code></pre><p><strong>注意事项</strong>:</p><ul><li>Pillow和旧的PIL库不能共存,如果之前安装过PIL,请先卸载</li><li>Pillow ≥ 1.0版本不再支持<code>import Image</code>,必须使用<code>from PIL import Image</code></li><li>对于Linux用户,某些发行版可能需要先安装系统级的依赖库(如libjpeg、zlib)</li></ul><p><strong>可选依赖</strong>:<br/>如果需要处理XMP元数据,可以额外安装:</p><pre><code class="bash">pip install defusedxml olefile</code></pre><h3>Hello, World示例</h3><p>让我们从一个最简单的示例开始,学习如何使用Pillow打开一张图片、获取基本信息并保存为新格式:</p><pre><code class="python">from PIL import Image

# 1. 打开一张图片(假设当前目录下有test.jpg)
img = Image.open('test.jpg')

# 2. 获取图片基本信息
print(f"图片格式: {img.format}")        # 输出: JPEG
print(f"图片尺寸: {img.size}")          # 输出: (1920, 1080) - (宽度, 高度)
print(f"图片模式: {img.mode}")          # 输出: RGB - 颜色模式
print(f"文件名: {img.filename}")        # 输出: test.jpg

# 3. 显示图片(使用系统默认图片查看器)
img.show()

# 4. 保存为PNG格式(自动根据扩展名确定格式)
img.save('test.png')

# 5. 保存为压缩后的JPEG(quality参数控制质量,1-95)
img.save('test_compressed.jpg', quality=70, optimize=True)</code></pre><h3>逐行解释</h3><ul><li><code>from PIL import Image</code>: 从Pillow库中导入Image类,这是最核心的类,用于表示和操作图像对象。注意包名是<code>PIL</code>而不是<code>Pillow</code>,这是历史原因。</li><li><code>img = Image.open('test.jpg')</code>: <code>open()</code>函数是工厂方法,用于从文件中加载图像并返回Image对象。它会根据文件内容自动识别格式,而不是依赖文件扩展名。返回的<code>img</code>对象包含了图像的所有信息和操作方法。</li><li><code>img.format</code>: 属性,返回图像的原始格式(如JPEG、PNG等)。如果图像不是从文件加载的(如新建的图像),这个值为None。</li><li><code>img.size</code>: 属性,返回一个包含(宽度,高度)的元组,单位是像素。这是图像的基本维度信息。</li><li><code>img.mode</code>: 属性,返回图像的颜色模式,如RGB(真彩色)、L(灰度)、RGBA(带透明通道的RGB)、CMYK(印刷模式)等。</li><li><code>img.show()</code>: 方法,使用操作系统的默认图片查看器显示图像。这个方法会先将图像保存为临时文件,然后调用系统程序打开它,主要用于调试和快速预览。</li><li><code>img.save('test.png')</code>: 方法,将图像保存到文件。Pillow会根据文件扩展名自动确定输出格式(PNG)。这个方法支持各种格式特定的参数,比如JPEG的<code>quality</code>(质量)、<code>optimize</code>(优化)等。</li><li><code>img.save('test_compressed.jpg', quality=70, optimize=True)</code>: 保存时指定格式参数。<code>quality=70</code>表示压缩质量为70(范围1-95,数值越小压缩率越高但质量越差),<code>optimize=True</code>启用优化算法进一步减小文件体积。</li></ul><h3>运行结果</h3><p>运行上述代码后,你将在终端看到图片的基本信息,同时会:</p><ol><li>弹出一个图片查看器窗口显示原图</li><li>在当前目录下生成<code>test.png</code>(无损PNG格式)</li><li>生成<code>test_compressed.jpg</code>(压缩后的JPEG,文件体积会比原JPEG更小)</li></ol><p><strong>常见安装失败及解决</strong>:</p><ul><li>如果提示"ModuleNotFoundError: No module named 'PIL'",说明安装失败,重新运行<code>pip install Pillow</code></li><li>Windows用户如果遇到编译错误,尝试使用预编译的wheel包:<code>pip install --only-binary=:all: Pillow</code></li><li>Linux用户如果遇到某些格式不支持,需要安装系统依赖(如Ubuntu:<code>sudo apt-get install libjpeg-dev zlib1g-dev</code>)</li></ul><h2>3. 核心概念解析</h2><p>Pillow的核心设计围绕几个关键概念展开,理解这些概念是熟练使用Pillow的基础。本节重点介绍Image对象、图像模式和坐标系统这三大核心概念。</p><h3>3.1 Image对象</h3><p><code>Image</code>类是Pillow中最核心的对象,代表一个图像实例。你可以通过多种方式创建Image对象:</p><pre><code class="python">from PIL import Image

# 方式1: 从文件加载
img1 = Image.open('photo.jpg')

# 方式2: 创建空白图像
# 参数: 颜色模式, 尺寸(宽,高), 背景色(可选)
img2 = Image.new('RGB', (800, 600), color='white')

# 方式3: 从其他图像操作得到
img3 = img1.resize((400, 300))

# 方式4: 从颜色数据创建
img4 = Image.new('L', (100, 100), color=128)  # 创建灰色图像</code></pre><p>Image对象是不可变的——大多数操作方法(如<code>resize()</code>, <code>rotate()</code>)都会返回新的Image对象,而不会修改原对象。这种设计符合函数式编程的理念,让代码更安全、可预测。</p><h3>3.2 图像模式(Mode)</h3><p>图像模式定义了像素的存储方式和颜色表示。Pillow支持多种模式,最常用的包括:</p><table><thead><tr><th>模式</th><th>描述</th><th>典型用途</th></tr></thead><tbody><tr><td>1</td><td>1位像素,黑白</td><td>二值图像、文字图像</td></tr><tr><td>L</td><td>8位像素,灰度</td><td>灰度照片、医学图像</td></tr><tr><td>P</td><td>8位像素,使用调色板</td><td>索引颜色图像(GIF)</td></tr><tr><td>RGB</td><td>3x8位像素,真彩色</td><td>普通照片、屏幕显示</td></tr><tr><td>RGBA</td><td>4x8位像素,带透明通道</td><td>需要透明效果的图像</td></tr><tr><td>CMYK</td><td>4x8位像素,分色</td><td>印刷品、出版物</td></tr><tr><td>LAB</td><td>3x8位像素,LAB颜色空间</td><td>色彩科学应用</td></tr></tbody></table><p><strong>模式转换示例</strong>:</p><pre><code class="python">from PIL import Image

img_rgb = Image.open('photo.jpg')  # 默认是RGB模式

# 转换为灰度图
img_gray = img_rgb.convert('L')

# 转换为带透明通道的RGB
img_rgba = img_rgb.convert('RGBA')

# 转换为CMYK(印刷用)
img_cmyk = img_rgb.convert('CMYK')

# 查看转换前后
print(f"原始: {img_rgb.mode} -&gt; 转换后: {img_rgba.mode}")</code></pre><p>理解图像模式非常重要,因为不同的操作对模式有特定要求。例如,<code>ImageFilter</code>模块的某些滤镜只适用于RGB和L模式图像。</p><h3>3.3 坐标系统</h3><p>Pillow使用笛卡尔坐标系统,原点(0,0)位于图像的左上角:</p><ul><li>X轴向右为正</li><li>Y轴向下为正</li><li>坐标值以像素为单位</li></ul><p><strong>关键点</strong>:</p><ul><li><code>img.size</code>返回(width, height)元组</li><li><code>img.crop(box)</code>中的box是4元组:(left, upper, right, lower)</li><li>注意:坐标是像素"之间"的位置,所以crop(0,0,100,100)裁剪出的区域正好是100x100像素</li></ul><p><strong>坐标系统可视化</strong>:</p><pre style="display:none;"><code class="mermaid">graph TD
    A[图像坐标系统] --&gt; B[原点 0,0 - 左上角]
    A --&gt; C[X轴 - 向右为正]
    A --&gt; D[Y轴 - 向下为正]
    A --&gt; E[size - width, height]
    E --&gt; F[width - X轴最大值]
    E --&gt; G[height - Y轴最大值]
    A --&gt; H[crop box - left, upper, right, lower]
    H --&gt; I[left - 左边界x坐标]
    H --&gt; J[upper - 上边界y坐标]
    H --&gt; K[right - 右边界x坐标]
    H --&gt; L[lower - 下边界y坐标]</code></pre><h3>3.4 核心概念关系图</h3><p>Image对象、模式和坐标系统这三个核心概念相互关联,共同构成了Pillow图像处理的基础架构:</p><pre style="display:none;"><code class="mermaid">graph LR
    A[Image对象] --&gt; B[图像模式 mode]
    A --&gt; C[坐标系统]
    A --&gt; D[像素数据]
    
    B --&gt; E[RGB]
    B --&gt; F[L - 灰度]
    B --&gt; G[RGBA - 透明]
    B --&gt; H[CMYK - 印刷]
    
    C --&gt; I[原点: 左上角0,0]
    C --&gt; J[尺寸: width x height]
    C --&gt; K[裁剪: box - left,upper,right,lower]
    
    D --&gt; L[getpixel x,y]
    D --&gt; M[putpixel x,y,value]
    D --&gt; N[split - 分离通道]
    D --&gt; O[merge - 合并通道]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#f0e1ff
    style D fill:#e1ffe1</code></pre><h3>3.5 其他重要概念</h3><p><strong>图像通道(Bands)</strong>:</p><ul><li>RGB图像有3个通道:R(红)、G(绿)、B(蓝)</li><li>RGBA图像有4个通道:R、G、B、A(透明度)</li><li>可以使用<code>split()</code>方法分离通道,<code>merge()</code>方法合并通道</li></ul><p><strong>懒加载(Lazy Loading)</strong>:</p><ul><li><code>Image.open()</code>不会立即加载整个图像数据</li><li>只有在访问像素数据或执行操作时才会真正加载</li><li>这使得打开大文件的速度很快,不会立即消耗大量内存</li></ul><p><strong>过滤器(Filters)</strong>:</p><ul><li>Pillow提供内置滤镜(模糊、锐化、边缘检测等)</li><li>使用<code>img.filter(ImageFilter.BLUR)</code>等应用滤镜</li><li>自定义滤镜需要理解卷积操作</li></ul><p>掌握这些核心概念后,你就能理解Pillow的大部分操作逻辑,并能够高效地解决各种图像处理问题。</p><h2>4. 实战演练：批量图片处理工具</h2><p>让我们通过一个实际项目来综合运用Pillow的核心功能。假设你需要开发一个电商平台的图片处理工具,需要批量完成以下任务:</p><ol><li>统一调整商品图片尺寸</li><li>添加水印</li><li>优化压缩</li><li>生成缩略图</li><li>生成统计报告</li></ol><h3>需求分析</h3><p>电商平台的商品图片来源多样,尺寸不一,存储格式各异。为了提升用户体验和节省带宽,我们需要将所有图片处理成统一的标准:</p><ul><li>主图:800x800像素,JPEG格式,质量85%</li><li>缩略图:200x200像素,保持比例</li><li>添加半透明的品牌水印</li><li>生成处理报告</li></ul><h3>方案设计</h3><p>我们将使用以下Pillow功能:</p><ul><li><code>Image.open()</code> - 读取原始图片</li><li><code>Image.resize()</code> - 调整尺寸</li><li><code>Image.thumbnail()</code> - 生成缩略图(保持比例)</li><li><code>ImageDraw</code>和<code>ImageFont</code> - 绘制水印</li><li><code>Image.save()</code> - 保存优化后的图片</li><li><code>os</code>模块 - 批量文件处理</li></ul><h3>代码实现</h3><pre><code class="python">import os
from PIL import Image, ImageDraw, ImageFont
from datetime import datetime

class ImageProcessor:
    """电商图片批量处理工具"""
    
    def __init__(self, input_dir, output_dir, watermark_text="MyBrand"):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.watermark_text = watermark_text
        self.main_size = (800, 800)      # 主图尺寸
        self.thumb_size = (200, 200)     # 缩略图尺寸
        self.report = []                  # 处理报告
        
        # 创建输出目录
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'main'), exist_ok=True)
        os.makedirs(os.path.join(output_dir, 'thumb'), exist_ok=True)
    
    def add_watermark(self, img):
        """添加半透明水印"""
        # 确保图片有alpha通道
        if img.mode != 'RGBA':
            img = img.convert('RGBA')
        
        # 创建水印图层
        watermark = Image.new('RGBA', img.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(watermark)
        
        # 尝试加载字体,失败则使用默认字体
        try:
            font = ImageFont.truetype("arial.ttf", 40)
        except:
            font = ImageFont.load_default()
        
        # 计算水印位置(右下角)
        text_bbox = draw.textbbox((0, 0), self.watermark_text, font=font)
        text_width = text_bbox[2] - text_bbox[0]
        text_height = text_bbox[3] - text_bbox[1]
        
        x = img.width - text_width - 20
        y = img.height - text_height - 20
        
        # 绘制半透明文字
        draw.text((x, y), self.watermark_text, fill=(255, 255, 255, 128), font=font)
        
        # 合并水印到原图
        watermarked = Image.alpha_composite(img, watermark)
        return watermarked
    
    def process_image(self, filename):
        """处理单张图片"""
        input_path = os.path.join(self.input_dir, filename)
        
        try:
            # 1. 读取原始图片
            img = Image.open(input_path)
            original_format = img.format
            original_size = img.size
            
            # 2. 调整主图尺寸(居中裁剪到正方形)
            # 先缩放使短边达到目标尺寸
            ratio = max(self.main_size[0] / img.width, self.main_size[1] / img.height)
            new_size = (int(img.width * ratio), int(img.height * ratio))
            img_resized = img.resize(new_size, Image.Resampling.LANCZOS)
            
            # 居中裁剪到目标尺寸
            left = (new_size[0] - self.main_size[0]) // 2
            top = (new_size[1] - self.main_size[1]) // 2
            img_cropped = img_resized.crop((
                left, top,
                left + self.main_size[0],
                top + self.main_size[1]
            ))
            
            # 3. 添加水印
            img_watermarked = self.add_watermark(img_cropped)
            
            # 4. 保存主图(JPEG格式,优化压缩)
            main_filename = os.path.splitext(filename)[0] + '.jpg'
            main_path = os.path.join(self.output_dir, 'main', main_filename)
            img_watermarked.save(main_path, 'JPEG', quality=85, optimize=True)
            
            # 5. 生成缩略图(保持比例)
            img_thumb = img.copy()
            img_thumb.thumbnail(self.thumb_size, Image.Resampling.LANCZOS)
            thumb_path = os.path.join(self.output_dir, 'thumb', main_filename)
            img_thumb.save(thumb_path, 'JPEG', quality=75)
            
            # 6. 记录处理信息
            main_filesize = os.path.getsize(main_path) / 1024  # KB
            thumb_filesize = os.path.getsize(thumb_path) / 1024
            
            self.report.append({
                'filename': filename,
                'original_format': original_format,
                'original_size': original_size,
                'main_size': self.main_size,
                'main_filesize': round(main_filesize, 2),
                'thumb_size': img_thumb.size,
                'thumb_filesize': round(thumb_filesize, 2),
                'status': 'success'
            })
            
            return True
            
        except Exception as e:
            self.report.append({
                'filename': filename,
                'error': str(e),
                'status': 'failed'
            })
            return False
    
    def process_all(self):
        """批量处理所有图片"""
        # 支持的图片格式
        supported_formats = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')
        
        # 获取所有图片文件
        image_files = [
            f for f in os.listdir(self.input_dir)
            if f.lower().endswith(supported_formats)
        ]
        
        if not image_files:
            print(f"错误: 在 {self.input_dir} 中没有找到支持的图片格式")
            return
        
        print(f"开始处理 {len(image_files)} 张图片...")
        print("-" * 60)
        
        # 处理每张图片
        success_count = 0
        for i, filename in enumerate(image_files, 1):
            print(f"[{i}/{len(image_files)}] 处理: {filename}", end=" ")
            if self.process_image(filename):
                print("✓")
                success_count += 1
            else:
                print("✗")
        
        print("-" * 60)
        print(f"处理完成! 成功: {success_count}/{len(image_files)}")
        self.generate_report()
    
    def generate_report(self):
        """生成处理报告"""
        print("\n" + "=" * 60)
        print("处理报告")
        print("=" * 60)
        
        # 统计信息
        success = [r for r in self.report if r['status'] == 'success']
        failed = [r for r in self.report if r['status'] == 'failed']
        
        if success:
            total_main_size = sum(r['main_filesize'] for r in success)
            total_thumb_size = sum(r['thumb_filesize'] for r in success)
            avg_main_size = total_main_size / len(success)
            avg_thumb_size = total_thumb_size / len(success)
            
            print(f"\n成功处理: {len(success)} 张")
            print(f"主图总大小: {total_main_size:.2f} KB (平均 {avg_main_size:.2f} KB)")
            print(f"缩略图总大小: {total_thumb_size:.2f} KB (平均 {avg_thumb_size:.2f} KB)")
            print(f"总输出大小: {total_main_size + total_thumb_size:.2f} KB")
        
        if failed:
            print(f"\n失败: {len(failed)} 张")
            for item in failed:
                print(f"  - {item['filename']}: {item.get('error', '未知错误')}")
        
        # 详细列表(前10张)
        if success:
            print("\n处理详情(前10张):")
            print("-" * 60)
            print(f"{'文件名':&lt;20} {'原始尺寸':&lt;12} {'主图大小(KB)':&lt;12} {'缩略图大小(KB)':&lt;14}")
            print("-" * 60)
            for item in success[:10]:
                print(f"{item['filename']:&lt;20} {str(item['original_size']):&lt;12} "
                      f"{item['main_filesize']:&lt;12.2f} {item['thumb_filesize']:&lt;14.2f}")
        
        print("\n输出目录:")
        print(f"  主图: {os.path.join(self.output_dir, 'main')}")
        print(f"  缩略图: {os.path.join(self.output_dir, 'thumb')}")


# 使用示例
if __name__ == "__main__":
    # 创建测试环境(如果需要测试)
    # 假设有一个input_images目录包含待处理图片
    
    processor = ImageProcessor(
        input_dir="input_images",      # 输入目录
        output_dir="output_images",    # 输出目录
        watermark_text="MyShop©"       # 水印文字
    )
    
    # 开始批量处理
    processor.process_all()</code></pre><h3>运行说明</h3><ol><li><p><strong>准备工作</strong>:</p><ul><li>在当前目录下创建<code>input_images</code>文件夹</li><li>放入一些待处理的图片(支持JPG、PNG、BMP、GIF格式)</li></ul></li><li><p><strong>运行程序</strong>:</p><pre><code class="bash">python image_processor.py</code></pre></li><li><p><strong>输出结果</strong>:</p><ul><li><p>程序会在<code>output_images</code>目录下生成两个子目录:</p><ul><li><code>main/</code>: 存放800x800的主图,带水印</li><li><code>thumb/</code>: 存放200x200的缩略图</li></ul></li><li>控制台输出详细的处理报告</li></ul></li></ol><h3>结果展示</h3><p>程序运行后,你将看到类似的输出:</p><pre><code>开始处理 15 张图片...
------------------------------------------------------------
[1/15] 处理: product1.jpg ✓
[2/15] 处理: product2.png ✓
[3/15] 处理: product3.jpg ✓
...
------------------------------------------------------------
处理完成! 成功: 14/15

============================================================
处理报告
============================================================

成功处理: 14 张
主图总大小: 845.32 KB (平均 60.38 KB)
缩略图总大小: 128.76 KB (平均 9.20 KB)
总输出大小: 974.08 KB

处理详情(前10张):
------------------------------------------------------------
文件名              原始尺寸      主图大小(KB)   缩略图大小(KB)  
------------------------------------------------------------
product1.jpg       (1920, 1080)  65.23          9.45           
product2.png       (1200, 800)   58.76          8.92           
product3.jpg       (800, 800)    52.34          8.15           
...

输出目录:
  主图: output_images/main
  缩略图: output_images/thumb</code></pre><p>这个综合项目展示了Pillow的多个核心功能:</p><ul><li><strong>文件I/O</strong>: <code>open()</code>和<code>save()</code>处理多种格式</li><li><strong>几何变换</strong>: <code>resize()</code>和<code>crop()</code>调整尺寸和裁剪</li><li><strong>图像合成</strong>: <code>Image.alpha_composite()</code>添加透明水印</li><li><strong>绘图功能</strong>: <code>ImageDraw</code>和<code>ImageFont</code>绘制文字</li><li><strong>缩略图生成</strong>: <code>thumbnail()</code>保持比例缩放</li><li><strong>批量处理</strong>: 结合<code>os</code>模块实现自动化</li></ul><p>通过这个项目,你可以看到Pillow如何优雅地将复杂的图像处理任务简化为清晰、可维护的代码。</p><h2>5. 最佳实践与常见陷阱</h2><p>在使用Pillow进行图像处理时,掌握一些最佳实践和避免常见陷阱可以让你的代码更高效、更可靠。</p><h3>5.1 常见错误及规避方法</h3><h4>错误1:忘记关闭文件或内存泄漏</h4><pre><code class="python"># ❌ 错误做法 - 文件未关闭
for filename in os.listdir('images'):
    img = Image.open(os.path.join('images', filename))
    process(img)  # 处理图像
    # 文件句柄未关闭,可能导致资源泄漏

# ✅ 正确做法 - 使用上下文管理器
for filename in os.listdir('images'):
    filepath = os.path.join('images', filename)
    with Image.open(filepath) as img:
        process(img)  # 自动关闭文件</code></pre><p><strong>为什么</strong>: 虽然<code>Image.open()</code>的文件句柄会在Image对象被垃圾回收时自动关闭,但在批量处理大文件时,最好显式使用<code>with</code>语句或调用<code>img.close()</code>来及时释放资源。</p><h4>错误2:直接修改原图对象</h4><pre><code class="python"># ❌ 错误做法 - 可能意外修改原图
img = Image.open('original.jpg')
img.resize((400, 300))  # 返回新对象,但未赋值!
img.save('resized.jpg')  # 保存的还是原图

# ✅ 正确做法 - 赋值返回的新对象
img = Image.open('original.jpg')
img_resized = img.resize((400, 300))
img_resized.save('resized.jpg')</code></pre><p><strong>为什么</strong>: Pillow的大部分操作方法(如<code>resize()</code>, <code>rotate()</code>, <code>crop()</code>)都返回新的Image对象,而不是原地修改。如果不赋值,操作就无效了。</p><h4>错误3:忽略图像模式不匹配</h4><pre><code class="python"># ❌ 错误做法 - 直接粘贴不同模式的图片
img_rgb = Image.new('RGB', (400, 300), 'red')
img_rgba = Image.new('RGBA', (100, 100), (0, 0, 255, 128))
img_rgb.paste(img_rgba, (50, 50))  # 可能出错或效果不符合预期

# ✅ 正确做法 - 先转换模式
img_rgb = Image.new('RGB', (400, 300), 'red')
img_rgba = Image.new('RGBA', (100, 100), (0, 0, 255, 128))

# 方法1: 将RGBA转RGB
img_rgb_to_paste = img_rgba.convert('RGB')
img_rgb.paste(img_rgb_to_paste, (50, 50))

# 方法2: 使用蒙版保持透明度
img_rgb.paste(img_rgba, (50, 50), img_rgba.split()[3])  # 使用alpha通道作为蒙版</code></pre><p><strong>为什么</strong>: 不同模式的图像不能直接粘贴。要么转换模式,要么使用蒙版来处理透明通道。</p><h3>5.2 最佳实践</h3><h4>实践1:选择合适的重采样算法</h4><pre><code class="python"># 缩小图像 - 使用LANCZOS
small_img = large_img.resize((400, 300), Image.Resampling.LANCZOS)

# 放大图像 - 使用BICUBIC
large_img = small_img.resize((800, 600), Image.Resampling.BICUBIC)

# 快速处理(质量要求不高时) - 使用NEAREST
thumbnail = img.resize((100, 100), Image.Resampling.NEAREST)</code></pre><p><strong>为什么</strong>: 不同的重采样算法在质量和速度上有差异:</p><ul><li><code>LANCZOS</code>: 最佳质量,适合缩小图像</li><li><code>BICUBIC</code>: 平衡,适合放大图像</li><li><code>NEAREST</code>: 最快,但会产生锯齿</li></ul><h4>实践2:优化JPEG保存质量</h4><pre><code class="python"># 网页用图片 - 平衡质量和大小
img.save('web.jpg', 'JPEG', quality=85, optimize=True)

# 存档图片 - 最高质量
img.save('archive.jpg', 'JPEG', quality=95, progressive=True)

# 缩略图 - 更小文件
img.save('thumb.jpg', 'JPEG', quality=70, optimize=True)</code></pre><p><strong>为什么</strong>:</p><ul><li><code>quality</code>: 1-95,值越大质量越好但文件越大</li><li><code>optimize=True</code>: 启用额外优化,减小文件体积</li><li><code>progressive=True</code>: 渐进式JPEG,加载时先显示低质量预览</li></ul><h4>实践3:处理大图像时的内存优化</h4><pre><code class="python"># ❌ 错误做法 - 加载超大图像到内存
huge_img = Image.open('huge.tif')  # 可能导致内存溢出

# ✅ 正确做法 - 使用懒加载和分块处理
# Pillow默认使用懒加载,只有在需要时才读取像素
img = Image.open('huge.tif')
print(img.size)  # 快速获取尺寸,不加载完整图像

# 分块处理大图像
def process_in_chunks(img_path, chunk_size=1000):
    img = Image.open(img_path)
    width, height = img.size
    
    for y in range(0, height, chunk_size):
        for x in range(0, width, chunk_size):
            box = (x, y, min(x+chunk_size, width), min(y+chunk_size, height))
            chunk = img.crop(box)
            process_chunk(chunk)</code></pre><p><strong>为什么</strong>: 处理大图像(如5000x5000以上的TIFF)时,一次性加载到内存可能超出可用内存。利用Pillow的懒加载特性和分块处理可以有效降低内存使用。</p><h4>实践4:批量处理时使用多线程</h4><pre><code class="python">from concurrent.futures import ThreadPoolExecutor

def process_single_image(filepath):
    with Image.open(filepath) as img:
        # 处理图像
        processed = img.resize((800, 600))
        processed.save(filepath.replace('.jpg', '_processed.jpg'))
        return filepath

# 多线程批量处理
def batch_process(image_dir, max_workers=4):
    filepaths = [
        os.path.join(image_dir, f) 
        for f in os.listdir(image_dir) 
        if f.endswith('.jpg')
    ]
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(process_single_image, filepaths))
    
    return results</code></pre><p><strong>为什么</strong>: I/O密集型任务(如文件读写)和CPU密集型任务(如图像处理)可以并行化,显著提升批量处理速度。<code>ThreadPoolExecutor</code>提供了简洁的多线程接口。</p><h4>实践5:正确处理颜色空间转换</h4><pre><code class="python"># 显示用图片 - 转换为sRGB
img_display = img.convert('RGB')
img_display.save('display.jpg')

# 印刷用图片 - 转换为CMYK
img_print = img.convert('CMYK')
img_print.save('print.jpg')

# 处理用图片 - 先转换为LAB色彩空间
img_lab = img.convert('LAB')
# 在LAB空间进行亮度、对比度调整...
img_result = img_lab.convert('RGB')  # 转回RGB保存</code></pre><p><strong>为什么</strong>: 不同用途需要不同的颜色空间:</p><ul><li>RGB: 屏幕显示,网页</li><li>CMYK: 印刷品</li><li>LAB: 色彩科学,图像处理(亮度与色度分离)</li></ul><h3>5.3 注意事项</h3><ol><li><strong>DecompressionBombWarning</strong>: 打开非常大的图像时,Pillow会发出警告。如果确实需要处理,可以先<code>Image.MAX_IMAGE_PIXELS = None</code>关闭限制,但要小心内存溢出。</li><li><strong>文件扩展名不一定准确</strong>: <code>Image.open()</code>根据文件内容识别格式,而不是扩展名。所以<code>Image.open('photo.png')</code>可能实际打开的是JPEG文件。</li><li><strong>GIF动画处理</strong>: Pillow可以读取GIF动画,但只能保存单帧或创建新动画。处理多帧GIF需要遍历<code>seek()</code>方法。</li><li><strong>字体依赖</strong>: <code>ImageFont</code>使用系统字体,不同操作系统上可能不一致。打包字体文件到项目中可以确保跨平台一致性。</li><li><strong>透明度处理</strong>: PNG的透明度在转换为JPEG时会被丢弃,因为JPEG不支持透明通道。需要先用<code>convert('RGB')</code>去除alpha通道。</li></ol><p>掌握这些最佳实践和陷阱,你的Pillow代码将更加健壮、高效和专业。</p><h2>6. 进阶指引</h2><p>当你掌握了Pillow的基础功能后,还有更多高级特性和生态工具值得探索,这将极大扩展你的图像处理能力。</p><h3>6.1 高级功能</h3><h4>像素级操作与NumPy集成</h4><p>Pillow可以与NumPy无缝协作,这对科学计算和图像算法开发非常重要:</p><pre><code class="python">import numpy as np
from PIL import Image

# Pillow图像转NumPy数组
img = Image.open('photo.jpg')
arr = np.array(img)  # 形状: (height, width, channels)

# 使用NumPy进行批量像素操作
inverted_arr = 255 - arr  # 反转所有像素
arr[:,:,0] = 0  # 将红色通道设为0

# NumPy数组转回Pillow图像
img_processed = Image.fromarray(arr)
img_processed.save('numpy_processed.jpg')</code></pre><p>这种集成使得Pillow成为连接Python科学计算生态和图像处理的桥梁,你可以利用NumPy的向量化运算加速图像处理。</p><h4>自定义滤镜与图像算法</h4><p>Pillow支持创建自定义滤镜:</p><pre><code class="python">from PIL import ImageFilter

# 创建自定义锐化滤镜
class SharpenFilter(ImageFilter.BuiltinFilter):
    name = "Sharpen"
    
    # 3x3卷积核
    filterargs = (3, 3), (
        0, -1,  0,
       -1,  5, -1,
        0, -1,  0
    ), 1.0, 0

# 应用自定义滤镜
img = Image.open('blur.jpg')
sharpened = img.filter(SharpenFilter())
sharpened.save('sharpened.jpg')</code></pre><p>你可以设计各种卷积核实现边缘检测、浮雕、锐化等效果。</p><h4>高级图像合成</h4><pre><code class="python">from PIL import Image, ImageDraw, ImageFont

# 创建复杂的合成图像
bg = Image.new('RGBA', (800, 600), (240, 248, 255))
fg = Image.open('logo.png').convert('RGBA')

# 调整透明度
fg_alpha = fg.copy()
alpha = fg_alpha.split()[3]
alpha = alpha.point(lambda p: p * 0.7)  # 70%透明度
fg_alpha.putalpha(alpha)

# 居中粘贴
x = (bg.width - fg.width) // 2
y = (bg.height - fg.height) // 2
bg.paste(fg_alpha, (x, y), fg_alpha)

# 添加文字
draw = ImageDraw.Draw(bg)
draw.text((20, 20), "Professional Design", fill=(50, 50, 80))
bg.save('composite.png')</code></pre><h3>6.2 生态扩展</h3><p>Pillow是Python图像处理生态的核心组件,与其他库配合可以构建强大的应用:</p><table><thead><tr><th>库名</th><th>用途</th><th>配合场景</th></tr></thead><tbody><tr><td>OpenCV</td><td>计算机视觉</td><td>视频处理、人脸识别、目标检测</td></tr><tr><td>scikit-image</td><td>科学图像处理</td><td>医学图像、卫星图像分析</td></tr><tr><td>matplotlib</td><td>数据可视化</td><td>图像显示、数据可视化</td></tr><tr><td>TensorFlow/PyTorch</td><td>深度学习</td><td>图像分类、目标检测、图像生成</td></tr></tbody></table><p><strong>示例:与OpenCV互操作</strong>:</p><pre><code class="python">import cv2
from PIL import Image

# Pillow -&gt; OpenCV
img_pil = Image.open('photo.jpg')
img_cv = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

# OpenCV处理
gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
edges = cv2.Canny(gray, 100, 200)

# OpenCV -&gt; Pillow
edges_pil = Image.fromarray(edges)
edges_pil.save('edges.jpg')</code></pre><h3>6.3 学习路径</h3><p><strong>初学者</strong>:</p><ol><li>熟练掌握<code>Image</code>类的基本方法</li><li>理解图像模式和坐标系统</li><li>实践常见的图像操作(缩放、裁剪、旋转)</li><li>参考:官方文档的Tutorial章节</li></ol><p><strong>进阶开发者</strong>:</p><ol><li>深入学习<code>ImageFilter</code>和<code>ImageEnhance</code></li><li>掌握<code>ImageDraw</code>和<code>ImageFont</code>绘图</li><li>学习批量处理和性能优化</li><li>参考:官方文档的Handbook章节</li></ol><p><strong>高级用户</strong>:</p><ol><li>探索与NumPy、OpenCV的集成</li><li>学习自定义滤镜和图像算法</li><li>研究源码理解底层实现</li><li>参考:GitHub上的Pillow源码和Issue讨论</li></ol><h3>6.4 学习资源</h3><ul><li><strong>官方文档</strong>: <a href="https://link.segmentfault.com/?enc=mYc7hMjCT2ibgagF1MmNJQ%3D%3D.kjUgMGdquvslvwI03lnoHn1ce2R7%2BvPWtMQ7Y4jABf0%3D" rel="nofollow" target="_blank">https://pillow.readthedocs.io/</a> (最权威的参考资料)</li><li><strong>GitHub仓库</strong>: <a href="https://link.segmentfault.com/?enc=fOgWjoAp95vkpOXzMVE02A%3D%3D.BotdnmG81KH05cOOrE%2BVIUINEsl5Upc4zxAzSYsgWsxQqxoYFRAvGK9%2F79%2BD3t2o" rel="nofollow" target="_blank">https://github.com/python-pillow/Pillow</a> (源码、Issue、PR)</li><li><strong>Stack Overflow</strong>: 搜索[pillow]标签获取社区解决方案</li><li><strong>Real Python</strong>: 有多篇Pillow教程,适合实战学习</li></ul><p>Pillow的世界远比这篇文档介绍的要广阔。随着你探索的深入,你会发现它不仅是图像处理的工具,更是连接创意与技术的桥梁。无论是构建专业图像处理应用,还是实现创意可视化,Pillow都能成为你可靠的伙伴。</p>]]></description></item><item>    <title><![CDATA[打造云端数字员工：OpenClaw 的 SAE 弹性托管实践 Serverless ]]></title>    <link>https://segmentfault.com/a/1190000047609981</link>    <guid>https://segmentfault.com/a/1190000047609981</guid>    <pubDate>2026-02-13 17:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>开源项目 OpenClaw（原名 Clawdbot / Moltbot）在 GitHub 上的星标数突破 14 万，揭示了 AI 技术栈的显著演进：人工智能正从被动生成的“对话框”，迈向具备自主规划能力的“智能代理（Autonomous Agents）”。OpenClaw 正是这一概念的工程化落地——它以轻量级 CLI 工具的形式，在用户设备上启动了一个本地网关服务，为 Agent 提供了一个安全、持久且可扩展的运行时环境。</p><p>在这个环境中，Agent 是决策核心，Skills 是能力边界。网关则作为运行时，负责协调交互、记忆与执行三大子系统。它依据 Skills 的标准化接口定义，将大模型的模糊意图映射为精准的系统指令，从而驱动整套智能体生命周期的运转：</p><ul><li>交互与感知：它通过插件化适配器统一接入 WhatsApp、Telegram，并利用 Webhook 对接钉钉、飞书等国内平台；同时通过心跳机制与 Cron 调度器，实现 7×24 小时的任务值守与主动触发。</li><li>决策与记忆：内置的 Memory 子系统利用本地向量数据库，为 Agent 提供了持久化的长短期记忆，使其能记住用户偏好与历史决策；配合 Skills 注册表，Agent 可按需加载外部工具（如邮件收发、日历管理），不断扩展能力边界。</li><li>安全执行：它不依赖脆弱的本地环境，而是直接调度宿主机的 Docker Daemon，为每个任务动态创建临时沙箱容器来隔离运行代码；同时集成 Headless Chromium，利用 CDP 协议实现像素级的浏览器自动化。</li></ul><p>这种架构让 AI 从“聊天窗口”真正走入“生产环境”，升级为能交付结果的“数字员工”。</p><h2>为什么选择在 SAE 上托管 OpenClaw？</h2><p>OpenClaw 的执行力依赖于对 Docker 运行时和系统资源的深度调用。阿里云 SAE 凭借全功能的容器环境与Serverless 化的资源调度，为 OpenClaw 提供了一个既能完整运行其所有高级功能，又能避免资源闲置与运维复杂的理想托管平台。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609983" alt="" title=""/></p><h4>零门槛释放 Agent 全量能力</h4><p>OpenClaw 的核心能力在于能动态创建“沙箱”来执行代码，这要求宿主环境具备完整的 Docker 运行时权限。</p><p>SAE 原生支持 Docker-in-Docker (DinD) 模式，允许 OpenClaw 在实例内部独立运行一套完整的 Docker Daemon。这意味着无论是启动临时的 Python 执行环境，还是运行 Headless 浏览器进行网页操作，都能在云端顺畅执行，开发者无需关心底层的环境搭建，即可获得与本地部署一致的完整功能体验。</p><h4><strong>极致弹性实现算力取用自由</strong></h4><p>OpenClaw 的工作负载往往具有显著的潮汐效应与脉冲特征，固定规格的部署方式必然无法兼顾性能和成本。</p><p>SAE 提供了秒级的水平扩缩与垂直规格调整能力，能够精准跟随 Agent 的实际负载动态分配资源。配合秒级冷启动机制，以及精准的按量付费模式，开发者可以真正实现“用多少付多少”，以最优的成本结构支撑 Agent 的全天候运行。</p><h4>全托管架构保障服务高可用</h4><p>作为你的“数字员工”，OpenClaw 需要具备生产级的稳定性。</p><p>SAE 提供了全托管的运行环境，内置了跨可用区容灾、健康检查与故障自愈能力。开发者无需关注服务器的补丁更新或宕机恢复，只需专注于 Agent 的 Skills 开发与业务逻辑构建，即可获得 7×24 小时 的企业级服务保障。</p><h2><strong>部署与配置步骤指引</strong></h2><h4>前置准备</h4><p>在开始部署前，请确保已完成以下准备工作：</p><ul><li>已开通并授权<a href="https://link.segmentfault.com/?enc=4jBQmM6NSzvShVW%2BW9n4eQ%3D%3D.UUOltsddr9HrixIgE0DumvMn9ouuwz4e5ZDcOimOcx14TXjNnwxftCwAi1IWuBs6pw2DaV5OMwaOGlfE6SqTRC9V62dNiL2wwoV2PVxfEpY%3D" rel="nofollow" target="_blank">Serverless应用引擎</a>，详见<a href="https://link.segmentfault.com/?enc=8GY2oC5xyt%2FTsWfoA%2Fsr2w%3D%3D.lsRXSYQdhvEX95yINkyZjY8k5lULcHre7cep2SVQGUMq2zN3uvWl%2BpZj1FgslgnjfSh1WdyKoagbHIitT4yPUA%3D%3D" rel="nofollow" target="_blank">准备工作</a></li><li>已安装并配置 <code>saectl</code> 命令行工具<br/>用于远程访问 OpenClaw 实例。安装与配置方法详见<a href="https://link.segmentfault.com/?enc=9QNeoVq5LvHLNySamJnbSA%3D%3D.R0InPZWsx%2BqBQS4Ck0ZGxvMEL%2FkBSrlSYrMBMsdf79Kjw4WFToYgK3gWb6kIolgFaeDbfc4AhNAc9DLImKlrsqlPzMuE9rB7rkrGN4uOHfu%2F1Gf9Ky5ZQchjJf3t15U5GboMM2P1pLZRowwto9Csnw%3D%3D" rel="nofollow" target="_blank">Saectl 命令行工具</a></li><li>专有网络（VPC）中已配置公网 NAT 网关并绑定 EIP<br/>用于沙箱容器访问公网（如模型 API、网页抓取等）</li></ul><h4>Step1：应用中心一键部署</h4><ol><li>登录 SAE 控制台，进入「应用中心」。</li><li>搜索并点击模板 「OpenClaw — Serverless 部署」，进入服务创建页面。</li><li><p>在表单中填写以下必要信息：</p><ul><li>服务实例名称：自定义，如 openclaw-test</li><li>专有网络（VPC）：选择已配置 NAT 网关的 VPC</li><li>交换机（vSwitch）：选择对应可用区的交换机</li></ul></li><li>其余参数保持默认，点击「创建」。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609984" alt="" title="" loading="lazy"/></p><p>服务创建通常需要 2–3 分钟。创建完成后，在 SAE 应用列表中将看到名为 openclaw-gateway 的应用。</p><h4>Step2：登陆应用实例并初始化配置</h4><p>OpenClaw 的 CLI 命令需在 Gateway 容器内部执行。您可通过以下任一方式登录：ont&gt;</p><p><strong>方式 A：通过 SAE 控制台 WebShell</strong></p><p>1.在 SAE 控制台找到 openclaw-gateway 应用。<br/>2.进入「实例列表」，点击任意实例右侧的 「WebShell」 按钮，即可进入容器终端。</p><p><strong>方式 B：通过 saectl 命令行工具（推荐）</strong></p><pre><code class="powershell">saectl exec -it -n &lt;namespace&gt; &lt;pod-name&gt;</code></pre><p>详见<a href="https://link.segmentfault.com/?enc=l8ZUtneovpHDRW8YMOKvdw%3D%3D.TRCYV3rucRhWsSgb2sE0HJUEBycj9Etw5%2BVYwtI%2B%2BfmMvGljQ2N1tEwQhBTF%2Bj5hDKP1MbAT0WBY%2FMCnBn3XL%2BHLPfil6rV%2F9J1khhTDEr4%3D" rel="nofollow" target="_blank">使用 Saectl 工具管理应用实例 Pod</a></p><blockquote>后续所有命令均在容器实例内执行。</blockquote><p><strong>初始化 OpenClaw 运行环境</strong></p><p>1.设置终端逻辑尺寸（避免 TUI 渲染异常）</p><pre><code class="powershell">stty rows 40 cols 120</code></pre><p>2.执行初始化命令</p><pre><code class="powershell">openclaw onboard --install-daemon</code></pre><p>此命令将通过交互形式引导您完成基础配置，并安装后台守护进程。</p><blockquote>过程中若提示 “Systemd user services are unavailable.”，属正常现象。OpenClaw 在容器环境中使用轻量级进程管理器 supervisord 替代 systemd。</blockquote><p><strong>启动 Gateway 服务</strong></p><p>在容器内使用 supervisord 管理服务生命周期：</p><ul><li>首次部署后启动服务：</li></ul><pre><code class="powershell">supervisorctl start openclaw</code></pre><ul><li>后续修改配置后重启服务：</li></ul><pre><code class="powershell">supervisorctl restart openclaw</code></pre><h4>Step3：配置百炼为模型提供商</h4><ol><li>将阿里云百炼接入为兼容 OpenAI 协议的模型后端。</li></ol><pre><code class="python">openclaw config set models.providers.dashscope '{
  "baseUrl": "https://dashscope.aliyuncs.com/compatible-mode/v1",
  "api": "openai-completions",
  "apiKey": "your-api-key-here",
  "models": [
    {
      "id": "qwen3-max-2026-01-23",
      "name": "qwen3-max-2026-01-23",
      "reasoning": false,
      "input": ["text"],
      "cost": {
        "input": 0,
        "output": 0,
        "cacheRead": 0,
        "cacheWrite": 0
      },
      "contextWindow": 262144,
      "maxTokens": 65536
    }
  ]
}'</code></pre><blockquote>请将<code>your-api-key-here</code> 替换为有效的百炼 API Key。</blockquote><ol start="2"><li>指定该模型为默认推理模型（需与上述 id 一致）：</li></ol><pre><code class="powershell">openclaw config set agents.defaults.model.primary "dashscope/qwen3-max-2026-01-23"</code></pre><ol start="3"><li>重启 Gateway 使配置生效</li></ol><h4>Step4：启用并配置沙箱环境</h4><p>OpenClaw 的沙箱机制用于隔离 AI 代理的代码执行、文件操作和浏览器自动化行为。</p><pre><code class="python"># 1. 启用全功能沙箱模式
openclaw config set agents.defaults.sandbox.mode "all"

# 2. 指定代码执行沙箱的基础镜像
openclaw config set agents.defaults.sandbox.docker.image "openclaw-sandbox:bookworm-slim"

# 3. 设置代码沙箱的网络模式（bridge 允许外网访问；若无需联网可设为 "none"）
openclaw config set agents.defaults.sandbox.docker.network "bridge"

# 4. 启用浏览器自动化沙箱
openclaw config set agents.defaults.sandbox.browser.enabled true

# 5. 指定浏览器沙箱镜像
openclaw config set agents.defaults.sandbox.browser.image "openclaw-sandbox-browser:bookworm-slim"

# 6. 设置浏览器沙箱的网络模式（同上，按需选择 "bridge" 或 "none"）
openclaw config set agents.defaults.sandbox.browser.network "bridge"</code></pre><h4>Step5：访问 OpenClaw 控制界面</h4><p>OpenClaw 支持两种交互方式：终端 TUI 和 Web Control UI。</p><p><strong>方式A：命令行 TUI</strong></p><pre><code class="powershell">openclaw tui</code></pre><p>默认进入 main Agent 的 main Session，可直接开始对话。</p><p><strong>方式B：Web Control UI</strong></p><ol><li>确认 Gateway 绑定地址</li></ol><pre><code class="bash"># 查看配置
openclaw config get gateway.port
openclaw config get gateway.bind

# 应该是：
# port: 18789
# bind: "lan"</code></pre><p>若 gateway.bind 为 loopback，则无法从外部访问，需要设置为 lan</p><pre><code class="python"># 修改为 lan（允许外部访问）
openclaw config set gateway.bind "lan"

# 重启 Gateway
supervisorctl restart openclaw</code></pre><ol start="2"><li>配置公网访问入口</li></ol><p>在 SAE 控制台为应用<a href="https://link.segmentfault.com/?enc=R6Qj8qlH%2BkPXiQsmTiTqOw%3D%3D.%2FremDAfo%2FFeRzJ79tHb2DPY3X4ZiqlhzT3lJh2MkCXsew7hanlMVdrIvc3j59zXcniXf4ljQK%2B6Sz3NLYIdwPA%3D%3D" rel="nofollow" target="_blank">绑定 CLB 并生成公网访问 IP</a>，并配置 HTTPS 监听器，容器端口为 18789（OpenClaw Gateway 监听端口）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609985" alt="" title="" loading="lazy"/></p><ol start="3"><li>设备配对</li></ol><p>获取认证凭据：</p><pre><code class="bash"># 获取 Gateway 认证 Token
openclaw config get gateway.auth</code></pre><p>在浏览器中打开：</p><pre><code class="bash">https://&lt;CLB_PUBLIC_IP&gt;:18789?token=&lt;GATEWAY_AUTH_TOKEN&gt;</code></pre><p>首次访问将显示 “Pairing required”，表示需授权当前设备。</p><p>批准设备配对请求</p><pre><code class="powershell"># 列出待处理的配对请求
openclaw devices list --token "&lt;GATEWAY_AUTH_TOKEN&gt;"

# 找到状态为 "pending" 的请求 ID，并批准
openclaw devices approve &lt;requestId&gt;</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609986" alt="" title="" loading="lazy"/></p><p>批准后刷新页面，即可正常使用 Web 控制台。</p><h2>构建钉钉 AI 助理</h2><h4>Step1：创建钉钉应用</h4><p>创建钉钉应用需要您的钉钉账号有开发者权限。您可以联系您的组织管理员获取钉钉开放平台的开发权限，具体操作请参见<a href="https://link.segmentfault.com/?enc=Cz2PgNuw34OdfThaDWu1zw%3D%3D.f8hT91%2B%2FNHYMwdncEQK0pKZhcyuKEQpfa%2F5DdR42hlaof207JPjy8qhhBHhAvLd9cJYawLuoW75k6SKWK5oUSt8J51nDCpD7geRxtrgt1u8%3D" rel="nofollow" target="_blank">获取开发者权限</a>。</p><ol><li>创建应用</li></ol><p>a. 访问<a href="https://link.segmentfault.com/?enc=Q5MDk7MZuoZVi3mgtbJYaA%3D%3D.fly%2F2t%2FXdeaGv9uJoOXTpaij3GGNPF4u4Xh5%2FkTrAQ0%3D" rel="nofollow" target="_blank">钉钉开放平台</a>，点击创建。如果创建过应用但未展示应用开发指引，点击立即开始进入钉钉应用页面。</p><p>b. 在应用开发的左侧导航栏中，点击钉钉应用，在钉钉应用页面右上角点击创建应用。<br/>c. 在创建应用面板，填写应用名称和应用描述，在应用图标上传图标，完成后点击保存。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609987" alt="" title="" loading="lazy"/></p><ol start="2"><li>查看应用 Client ID 和 Client Secret</li></ol><p>在左侧菜单选择凭证与基础信息，复制Client ID和Client Secret，用于下一步创建连接流。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609988" alt="" title="" loading="lazy"/></p><ol start="3"><li>创建消息卡片<br/>a. 访问<a href="https://link.segmentfault.com/?enc=s4inpPdIimYDwwHn14fnMw%3D%3D.9eznNYxkzwogqFqZjDQXD518%2Bg%2F%2FrRe8x6qn8POh%2F3LEHmfyEgHcEQ4n2KG8QuQ1" rel="nofollow" target="_blank">卡片平台</a>，点击新建模板。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609989" alt="" title="" loading="lazy"/></p><p>b. 在创建模板输入框，填入模板信息，单击创建。</p><ul><li>卡片类型：选择消息卡片。</li><li>卡片模板场景：选择AI 卡片。</li><li>关联应用：关联<a href="https://link.segmentfault.com/?enc=09EBODspe9eop2uNBg9Ihw%3D%3D.71QUUZQlG9dJA9TVDhgP5U14L0R%2FOxxhDHA6d2ZuyOGmdKVENpM6U%2By%2FsVaarGRiZpT8nRLqwvqvQhXXrfkky1%2BWdHzyUo8DIML0vdlhIJXUyznY9mhfz4ZYtNhASLStUNV%2Fi%2FJE0MPgCQkQBLNNimUZLI%2F1A8mOivO%2B0WIYV0wRwHjRmsoOFbEEr6M6DgFS9T1vw30AuH%2FPVCPAFGraMoPtKqIBono3vDdQbCwJuQV5M2KvGOBZssAz1mgsHiOa9AQY0EAiJto1FpaiQTYg4u%2FEq6tDUoUp4VHRBIMOqPo%3D" rel="nofollow" target="_blank">应用创建步骤中的应用</a>。</li></ul><p>c. 在模拟编辑页面，不要使用预设模板，不需要进行任何额外操作，直接保存并发布模板。然后点击返回模板列表页面。</p><p>d. 直接保存并发布模板。然后点击返回模板列表页面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609990" alt="" title="" loading="lazy"/></p><p>e. 返回模板列表，复制模板 ID，用于创建钉钉连接流使用</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609991" alt="" title="" loading="lazy"/></p><ol start="4"><li>授予应用发送卡片消息权限</li></ol><p>创建卡片后，您需要给应用授予发送卡片消息的权限。</p><p>a.访问<a href="https://link.segmentfault.com/?enc=rCW7KeKV%2FRRnoin0Sjmmsg%3D%3D.W0rBOvPRXD5Wad7tfQXgJBYE2HrrYTW6EazrhMjYCJMy8RaGG0ZncCk8ZFvlyBnj" rel="nofollow" target="_blank">钉钉应用列表</a>。找到刚刚创建的应用，点击应用名称进入详情页面。</p><p>b.在左侧菜单选择开发配置 &gt; 权限管理，在左侧搜索框分别输入Card.Streaming.Write和Card.Instance.Write，并在操作列点击申请权限。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609992" alt="" title="" loading="lazy"/></p><h4>Step2：创建 AppFlow 连接流</h4><ol><li>使用 <a href="https://link.segmentfault.com/?enc=v7uv4RUkNskkecZA2lMmbw%3D%3D.RUOu0qc8dFADAGYJx5gm1dy3JOT0yMAbrGFJ9NgJeGFEsTwZsMArarVZomKDZ0w9q4%2BWKfHcryiZEf9ChKlqeIlV6CsZ%2BYd4TKVwTLRn%2FymtVbF0dvhJsLAJhQz7%2F4CH4cXwl5xBvUno6Zv9DZWqXg%3D%3D" rel="nofollow" target="_blank">AppFlow模板</a> 创建连接流，单击立即使用进入创建流程。</li><li>在连接流账号授权配置向导页，点击钉钉应用机器人下的添加新凭证，填入创建的应用的 Client ID 和 Client Secret，并设置一个自定义凭证名称。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609993" alt="" title="" loading="lazy"/></p><ol start="3"><li>在连接流的账户授权配置向导页，点击 moltbot 下的添加新凭证。输入之前通过以下命令获取的 token。</li></ol><pre><code class="powershell">openclaw config get gateway.auth</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609994" alt="" title="" loading="lazy"/></p><ol start="4"><li><p>在执行动作配置向导页按照页面提示配置完成后点击下一步。</p><ul><li>公网地址：填写 SAE 应用访问配置中的公网访问地址<code>https://&lt;CLB_PUBLIC_IP&gt;:18789</code>。</li><li>模板ID：填写保存的AI卡片模板ID。</li></ul></li><li>在基本信息配置向导页，填写连接流名称和连接流描述（保持默认），完成后点击下一步。</li><li>界面提示流程配置成功，复制 WebhookUrl，点击发布。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609995" alt="" title="" loading="lazy"/></p><h4>Step3：配置钉钉机器人</h4><ol><li><p>添加并配置机器人</p><ul><li>进入钉钉开发者后台，找到您的应用，点击进入详情页。</li><li>在「应用能力」中点击「添加能力」，选择「机器人」。</li><li>开启机器人开关，消息接收模式选择 HTTP，并将消息接收地址填为 OpenClaw 生成的 Webhook URL，完成后点击「发布」。</li></ul></li><li><p>发布应用版本</p><ul><li>在应用开发页面，进入「版本管理与发布」。</li><li>点击「创建新版本」，填写版本号和描述，设置可见范围后保存，并在弹窗中点击「直接发布」。</li></ul></li><li><p>在钉钉群中使用机器人</p><ul><li>进入目标钉钉群 → 群设置 → 智能群助手 → 添加机器人。</li><li>搜索并选择您刚创建的机器人，完成添加。</li><li><p>在群聊或私聊中 @该机器人，即可开始对话。</p><p><em>本实践需加白使用，如果您有任何疑惑，欢迎加入“Serverless应用引擎（SAE）用户群”，钉钉群号：23198618。</em></p></li></ul></li></ol>]]></description></item><item>    <title><![CDATA[SpringBoot中结合MySQL、Redis，实现异步落库的评论点赞/拉踩功能 Personal]]></title>    <link>https://segmentfault.com/a/1190000047610018</link>    <guid>https://segmentfault.com/a/1190000047610018</guid>    <pubDate>2026-02-13 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>概要</h2><p>该方案是一个典型的<strong>用空间（Redis缓存）和异步化换取时间（响应速度）和系统稳定性（数据库抗压）</strong> 的架构设计。它非常适合点赞这类<strong>写多读多、对实时一致性要求稍低</strong>的业务场景。</p><h2>前言</h2><p>学习java过程中的心得，如有错误请提醒作者纠正，感谢不尽！！！</p><p>如果有更好的实现，欢迎分享！！！</p><h3>当前实现优点</h3><ol><li><p><strong>高性能与低延迟</strong>：</p><ul><li><strong>写操作快</strong>：用户点赞/拉踩请求直接操作内存数据库Redis，响应速度极快，用户体验好。</li><li><strong>数据库在定时任务触发前压力小</strong>：高频的写操作被Redis承接，避免了直接冲击MySQL</li></ul></li><li><p><strong>数据一致性保障</strong>：</p><ul><li><strong>原子性操作</strong>：使用<strong>Lua脚本</strong>在Redis内完成状态切换（点赞-&gt;拉踩-&gt;无状态），保证了“一个评论同一时刻只能有一种状态”的业务逻辑的原子性，防止并发请求导致的数据错乱。</li><li><strong>分布式锁</strong>：对单个用户的操作加锁（<code>RLock</code>），防止同一用户极短时间内的重复提交造成缓存数据问题。</li></ul></li><li><p><strong>批量处理效率高</strong>：</p><ul><li><strong>异步落库</strong>：定时任务将缓存中的大量变更集中起来，通过<strong>批量插入/更新</strong>（<code>ON DUPLICATE KEY UPDATE</code>）和<strong>批量更新统计</strong>（<code>CASE WHEN</code>）的方式与数据库交互，极大地减少了网络I/O和SQL执行次数，数据库处理效率高。</li></ul></li></ol><h3><strong>当前实现存在问题</strong></h3><ol><li><p>定时任务引发的数据库峰值：</p><ol><li><strong>可通过使用消息队列削峰填谷</strong></li><li><strong>将定时任务从处理全部数据改为处理部分数据，定时任务的周期调低</strong></li></ol></li></ol><h2>主要实现细节</h2><p>Redis + Redis分布式锁 + 原子操作 + 异步落库 + SpringBoot定时任务</p><h2>流程图</h2><p><img width="723" height="692" referrerpolicy="no-referrer" src="/img/bVdnVHF" alt="" title=""/></p><h2>请求方法设计</h2><p><strong>请求URL</strong>：<code>/api/article/v1/{commentId}/vote</code></p><p><strong>请求方法</strong>：PUT</p><p><strong>请求参数</strong>：type(Integer);(type=0表示修改成无状态，type=1表示修改成点赞状态，type=-1表示修改成拉踩状态)</p><p><strong>URL示例</strong>：/api/article/v1/{commentId}/vote?type = 1</p><h2>Redis表设计</h2><p>下述三表都用来记录用户对评论的状态(点赞、拉踩、无状态)</p><h3>articles:comments:likes:users:{userId}</h3><p>{userId}为动态键名</p><p><strong>Redis Set</strong>数据结构；</p><p>存储的值：{commentId}</p><h3>articles:comments:dislikes:users:{userId}</h3><p>{userId}为动态键名</p><p><strong>Redis Set</strong>数据结构；</p><p>存储的值：{commentId}</p><h3>articles:comments:stateless:users:{userId}</h3><p>{userId}为动态键名</p><p><strong>Redis Set</strong>数据结构；</p><p>存储的值：{commentId}</p><h2>MySQL表设计</h2><p>该博客聚焦实现点赞/拉踩功能，涉及x_article_comments表不多，故不做x_article_comments表的字段介绍</p><ul><li><p>x_article_comments_votes</p><ul><li>联合主键(comment_id, user_id)</li><li>vote字段表用户对评论的状态，1代表点赞，-1代表拉踩，0代表无状态，即不处于点赞状态或拉踩状态</li></ul></li></ul><pre><code class="sql">-- 文章评论表
CREATE TABLE x_article_comments (
  id BIGINT AUTO_INCREMENT PRIMARY KEY,
  article_id BIGINT UNSIGNED NOT NULL,            -- 关联的文章
  parent_id BIGINT UNSIGNED NULL,               -- NULL 表示顶级评论；否则是回复
  root_id BIGINT UNSIGNED NULL,                 -- 同一Thread的Root评论 id（便于查询整个Thread）
  user_id BIGINT UNSIGNED NOT NULL,             -- 评论作者
  content TEXT NOT NULL,
  status TINYINT DEFAULT 1,            -- 1=显示,0=已删除/隐藏,2=待审核 等
  like_count INT DEFAULT 0,
  dislike_count INT DEFAULT 0,
  reply_count INT DEFAULT 0, -- 该层孩子的数量
  reply_descendant_count INT DEFAULT 0, -- 该层后代的数量
  version INT DEFAULT 0,               -- 乐观锁（更新时校验）
  create_time DATETIME DEFAULT CURRENT_TIMESTAMP,
  update_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

  PRIMARY KEY (id)
) ENGINE=InnoDB
  DEFAULT CHARSET=utf8mb4
  COLLATE=utf8mb4_unicode_ci;

-- 点赞/踩表（每个用户对某条评论的动作）
CREATE TABLE x_article_comments_votes (
  comment_id BIGINT NOT NULL,
  user_id BIGINT NOT NULL,
  vote TINYINT NOT NULL, -- 1=like, -1=dislike, 0=none
  create_time DATETIME DEFAULT CURRENT_TIMESTAMP,
  update_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  PRIMARY KEY (comment_id, user_id)
) ENGINE=InnoDB
  DEFAULT CHARSET=utf8mb4
  COLLATE=utf8mb4_unicode_ci;</code></pre><h2>后端实现细节</h2><h3>处理用户发起的点赞/拉踩/无状态请求</h3><h4>Controller层</h4><h5>校验参数</h5><p>先校验当前用户是否登录，userReadApi.getCurrentUserId()是我封装好的方法，用于获取发起当前请求的用户ID</p><pre><code class="java">        // 获取当前用户ID
        Long userId = userReadApi.getCurrentUserId();
        if (userId == null) {
            return AjaxResult.error(HttpStatus.UNAUTHORIZED, "用户未登录或获取用户ID失败");
        }</code></pre><h5>完整代码</h5><pre><code class="java">    /**
     * 点赞/拉踩/无状态评论
     */
    @PutMapping("/{commentId}/vote")
    public AjaxResult voteComment(
            @PathVariable("commentId") Long commentId,
            @RequestParam("type") Integer type
    ) {
        log.info("进入请求 /api/article/v1/{}/vote -&gt; 点赞/取消点赞评论 type={}", commentId, type);

        // 获取当前用户ID
        Long userId = userReadApi.getCurrentUserId();
        if (userId == null) {
            return AjaxResult.error(HttpStatus.UNAUTHORIZED, "用户未登录或获取用户ID失败");
        }


        articleService.voteComment(userId, commentId, type);

        log.info("返回结果 /api/article/v1/{}/vote -&gt; OK", commentId);
        return AjaxResult.success();
    }</code></pre><h4>Service层</h4><h5>校验参数</h5><pre><code class="java">        // 1. 参数校验
        if (commentId == null || type == null) {
            throw new ClientException(HttpStatus.BAD_REQUEST, "参数不完整");
        }
        if (type != ArticleCommentVoteConstants.LIKE &amp;&amp; type != ArticleCommentVoteConstants.DISLIKE &amp;&amp; type != ArticleCommentVoteConstants.STATELESS) {
            throw new ClientException(HttpStatus.BAD_REQUEST, "投票类型非法");
        }

        // 2. 校验评论是否存在
        ArticleComment comment = articleCommentMapper.selectArticleCommentById(commentId);
        if (comment == null) {
            throw new ClientException(HttpStatus.NOT_FOUND, "评论不存在");
        }</code></pre><h5>分布式锁 + lua脚本</h5><p>使用了redisson实现加锁，并使用了lua脚本保证原子性，从而防止用户频繁发起请求导致在redis缓存的数据出现错误的情况</p><h6><strong>分布式锁相关代码</strong></h6><pre><code class="java">        // 5. 尝试获取分布式锁
        RLock lock = redisson.getLock(RedisKeyConstants.ARTICLES_COMMENTS_VOTES_LOCK_KEY + userId);
        boolean isLocked = false;
        try {
            isLocked = lock.tryLock(0, RedisKeyConstants.ARTICLES_COMMENTS_VOTES_LOCK_KEY_EXPIRE_TIME, TimeUnit.SECONDS);
            if (!isLocked) {
                throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "系统繁忙，请稍后再试");
            } else {
                // 获取到锁，执行投票逻辑，此处省略
                // ...
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new ClientException(HttpStatus.ERROR, "系统繁忙，请稍后再试");
        } finally {
            if (isLocked &amp;&amp; lock.isHeldByCurrentThread()) {
                lock.unlock();
                log.info("用户 {} 的锁已释放", userId);
            }
        }</code></pre><h6>lua脚本相关代码</h6><p><strong>预先准备好redis键和lua脚本</strong></p><pre><code class="java">        // 3. 构建 Redis 键
        String likeKey = RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + userId;
        String dislikeKey = RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + userId;
        String statelessKey = RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + userId;
        // 4. 根据投票类型处理逻辑
        DefaultRedisScript&lt;Long&gt; deleteScript = new DefaultRedisScript&lt;&gt;();
        deleteScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(RedisLuaConstants.ARTICLE_COMMENTS_VOTE_LUA_SCRIPT_PATH)));
        deleteScript.setResultType(Long.class);
        Long execute = null;</code></pre><p><strong>lua脚本文件</strong></p><p>lua脚本能够<strong>保证原子性</strong>，若用户频繁发起请求也能保证<strong>以下的要求</strong></p><p>三个键对应的set集合里的值应<strong>保持互斥</strong>，只允许一个值如3只能出现在一个键，例如：共有三个键like:{userId}、dislike:{userId}、stateless:{userId}，键like现在持有3，用户对commentId = 3发起了点赞/拉踩/无状态请求，修改为拉踩，此时要去除like:{userId}键和stateless:{userId}键的set集合中值为3的元素，执行完后只有键dislike:{userId}存在值3</p><pre><code class="lua">-- Lua 脚本：处理评论投票逻辑
local mainKey = KEYS[1]       -- 进行add的键
local srem1Key = KEYS[2]    -- 进行srem的键
local srem2Key = KEYS[3]  -- 进行srem的键
local commentId = ARGV[1]     -- 评论 ID

-- 添加到main集合
redis.call('SADD', mainKey, commentId)

-- 从两个集合中移除
redis.call('SREM', srem1Key, commentId)
redis.call('SREM', srem2Key, commentId)

-- 返回操作结果（可选）
return 1  -- 表示成功执行
</code></pre><p><strong>获取到锁后，为实现存进redis，执行的操作如下</strong></p><pre><code class="java">                switch (type) {
                    case ArticleCommentVoteConstants.LIKE:
                        // 4. 处理点赞逻辑
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(likeKey, dislikeKey, statelessKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "处理点踩逻辑失败");
                        }
                        break;
                    case ArticleCommentVoteConstants.DISLIKE:
                        // 5. 处理点踩逻辑
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(dislikeKey, likeKey, statelessKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "处理点踩逻辑失败");
                        }
                        break;
                    case ArticleCommentVoteConstants.STATELESS:
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(statelessKey, dislikeKey, likeKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.ERROR, "处理无状态逻辑失败");
                        }
                        break;
                    default:
                        throw new ClientException(HttpStatus.BAD_REQUEST, "未知的投票类型");
                }</code></pre><h5>完整代码</h5><pre><code class="java">    @Override
    public void voteComment(Long userId, Long commentId, Integer type) {
        // 1. 参数校验
        if (commentId == null || type == null) {
            throw new ClientException(HttpStatus.BAD_REQUEST, "参数不完整");
        }
        if (type != ArticleCommentVoteConstants.LIKE &amp;&amp; type != ArticleCommentVoteConstants.DISLIKE &amp;&amp; type != ArticleCommentVoteConstants.STATELESS) {
            throw new ClientException(HttpStatus.BAD_REQUEST, "投票类型非法");
        }

        // 2. 校验评论是否存在
        ArticleComment comment = articleCommentMapper.selectArticleCommentById(commentId);
        if (comment == null) {
            throw new ClientException(HttpStatus.NOT_FOUND, "评论不存在");
        }

        // 3. 构建 Redis 键
        String likeKey = RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + userId;
        String dislikeKey = RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + userId;
        String statelessKey = RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + userId;
        // 4. 根据投票类型处理逻辑
        DefaultRedisScript&lt;Long&gt; deleteScript = new DefaultRedisScript&lt;&gt;();
        deleteScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(RedisLuaConstants.ARTICLE_COMMENTS_VOTE_LUA_SCRIPT_PATH)));
        deleteScript.setResultType(Long.class);
        Long execute = null;
        // 5. 尝试获取分布式锁
        RLock lock = redisson.getLock(RedisKeyConstants.ARTICLES_COMMENTS_VOTES_LOCK_KEY + userId);
        boolean isLocked = false;
        try {
            isLocked = lock.tryLock(0, RedisKeyConstants.ARTICLES_COMMENTS_VOTES_LOCK_KEY_EXPIRE_TIME, TimeUnit.SECONDS);
            if (!isLocked) {
                throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "系统繁忙，请稍后再试");
            } else {
                // 获取到锁，执行投票逻辑
                switch (type) {
                    case ArticleCommentVoteConstants.LIKE:
                        // 4. 处理点赞逻辑
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(likeKey, dislikeKey, statelessKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "处理点踩逻辑失败");
                        }
                        break;
                    case ArticleCommentVoteConstants.DISLIKE:
                        // 5. 处理点踩逻辑
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(dislikeKey, likeKey, statelessKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.SERVICE_UNAVAILABLE, "处理点踩逻辑失败");
                        }
                        break;
                    case ArticleCommentVoteConstants.STATELESS:
                        execute = stringRedisTemplate.execute(deleteScript, Arrays.asList(statelessKey, dislikeKey, likeKey), commentId.toString());
                        if (execute == null) {
                            throw new ClientException(HttpStatus.ERROR, "处理无状态逻辑失败");
                        }
                        break;
                    default:
                        throw new ClientException(HttpStatus.BAD_REQUEST, "未知的投票类型");
                }
            }
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new ClientException(HttpStatus.ERROR, "系统繁忙，请稍后再试");
        } finally {
            if (isLocked &amp;&amp; lock.isHeldByCurrentThread()) {
                lock.unlock();
                log.info("用户 {} 的锁已释放", userId);
            }
        }
    }</code></pre><h3>定时任务</h3><p>定时任务要实现落库到MySQL中，并且清空redis对应的缓存</p><h4>创建定时任务</h4><pre><code class="java">@Component
@Slf4j
public class VoteSyncScheduler {

    private final ArticleService articleService;
    public VoteSyncScheduler(ArticleService articleService) {
        this.articleService = articleService;
    }

    @Scheduled(fixedRate = 5 * 60 * 1000) // 每五分钟执行一次
    public void syncVote() {
        log.info("开始执行点赞同步任务...");
        Boolean result = articleService.syncVote();
        log.info("点赞同步任务执行完成。");
    }
}</code></pre><h4>syncVote实现</h4><p>MySQL涉及操作多张表，所以需要在方法上加上注解@Transactional</p><p><a href="##流程图" target="_blank">参考流程图</a></p><ol><li>我们首先要从redis中获取数据，并将数据封装到两个集合中。</li><li>落库到MySQL</li><li>根据x_article_comments_votes表，通过SQL语句统计出like_count, dislike_count</li><li>将统计出的数据更新到x_article_comments</li><li>清除已在该定时任务处理完的redis缓存数据</li></ol><h5>变量说明</h5><ul><li>List&lt;ArticleCommentVote&gt; votes = new ArrayList&lt;&gt;();      // 保存所有投票数据</li><li>Set&lt;Long&gt; TotalcommentId = new HashSet&lt;&gt;();     // 保存所有评论id</li><li>likesUserIdToCommentIdsMap、dislikesUserIdToCommentIdsMap、statelessUserIdToCommentIdsMap——存储的值为经过逻辑后键中已被处理的值，在最后一步清除redis缓存发挥作用</li></ul><h5>从redis中获取数据，并将数据封装到两个集合中</h5><pre><code class="java">        Set&lt;String&gt; userLikesKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + "*");
        Set&lt;String&gt; userDislikesKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + "*");
        Set&lt;String&gt; userStatelessKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + "*");
        if (userLikesKeys.isEmpty() &amp;&amp; userDislikesKeys.isEmpty() &amp;&amp; userStatelessKeys.isEmpty()){
            log.info("没有需要同步的数据");
            return true;
        }
        List&lt;ArticleCommentVote&gt; votes = new ArrayList&lt;&gt;();      // 保存所有投票数据
        Set&lt;Long&gt; TotalcommentId = new HashSet&lt;&gt;();     // 保存所有评论id
        Map&lt;Long, Set&lt;String&gt;&gt; likesUserIdToCommentIdsMap = new HashMap&lt;&gt;();
        Map&lt;Long, Set&lt;String&gt;&gt; dislikesUserIdToCommentIdsMap = new HashMap&lt;&gt;();
        Map&lt;Long, Set&lt;String&gt;&gt; statelessUserIdToCommentIdsMap = new HashMap&lt;&gt;();
        for (String userLikesKey : userLikesKeys) {
            String userId = userLikesKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY, "");
            Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userLikesKey);
            for (String commentId : commentIds) {
                votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.LIKE, null, null));
                TotalcommentId.add(Long.parseLong(commentId));
                // 更新或新增likesUserIdToCommentIdsMap键值对，值的set集合新增commentId
                likesUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
            }
        }
        for (String userDislikesKey : userDislikesKeys) {
            String userId = userDislikesKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY, "");
            Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userDislikesKey);
            for (String commentId : commentIds) {
                votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.DISLIKE, null, null));
                TotalcommentId.add(Long.parseLong(commentId));
                // 更新或新增dislikesUserIdToCommentIdsMap键值对，值的set集合新增commentId
                dislikesUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
            }
        }
        for (String userStatelessKey : userStatelessKeys) {
            String userId = userStatelessKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY, "");
            Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userStatelessKey);
            for (String commentId : commentIds) {
                votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.STATELESS, null, null));
                TotalcommentId.add(Long.parseLong(commentId));
                // 添加到statelessUserIdToCommentIdsMap键值对，值的set集合新增commentId
                statelessUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
            }
        }</code></pre><h5>落库到MySQL</h5><h6>Service层相关代码</h6><pre><code class="java">        // 根据votes执行落库逻辑（包含插入/更新）
        if (!votes.isEmpty()){
            int i = articleCommentVoteMapper.batchInsertOrUpdate(votes);
            if (i &lt; 0){
                throw new ClientException(HttpStatus.ERROR, "批量插入或更新数据失败");
            }
            log.info("批量插入或更新数据成功，数量为：{}", i);
        } else {
            log.info("没有需要落库的数据");
            return true;
        }</code></pre><h6>SQL语句相关实现</h6><pre><code class="xml">    &lt;!-- 原有方法 --&gt;
    &lt;insert id="batchInsertOrUpdate" parameterType="java.util.List"&gt;
        INSERT INTO x_article_comments_votes (
            comment_id,
            user_id,
            vote,
            create_time,
            update_time
        )
        VALUES
        &lt;foreach collection="list" item="item" separator=","&gt;
            (
                #{item.commentId},
                #{item.userId},
                #{item.vote},
                NOW(),
                NOW()
            )
        &lt;/foreach&gt;
        ON DUPLICATE KEY UPDATE
            vote = VALUES(vote),
            update_time = VALUES(update_time)
    &lt;/insert&gt;</code></pre><h5>根据x_article_comments_votes表，通过SQL语句统计出like_count, dislike_count</h5><h6>Service层相关代码</h6><pre><code class="java">        List&lt;UpdateCommentVoteCountDTO&gt; updateCommentVoteCountDTOList = getCommentLikeCountAndDislikeCountByListOfCommentId(TotalcommentId);</code></pre><h6>SQL语句相关实现</h6><pre><code class="xml">&lt;!-- 新增方法: 聚合查询点赞/点踩数量 --&gt;
&lt;select id="selectCommentLikeCountAndDislikeCountByListOfCommentId" resultType="com.anon.spaceblogserver.modules.article.POJO.DTO.UpdateCommentVoteCountDTO"&gt;
    SELECT
        comment_id AS commentId,
        SUM(CASE WHEN vote = 1 THEN 1 ELSE 0 END) AS likeCount,
        SUM(CASE WHEN vote = -1 THEN 1 ELSE 0 END) AS dislikeCount
    FROM x_article_comments_votes
    WHERE comment_id IN
    &lt;foreach collection="commentIds" item="commentId" open="(" separator="," close=")"&gt;
        #{commentId}
    &lt;/foreach&gt;
    GROUP BY comment_id
&lt;/select&gt;</code></pre><h5>将统计出的数据批量更新到x_article_comments</h5><h6>Service层相关代码</h6><pre><code class="java">int updated = batchUpdateCommentLikeCountAndDislikeCount(updateCommentVoteCountDTOList, MySQLBatchSizeConstants.DEFAULT_UPDATE_BATCH_SIZE);
if (updated &lt; 0){
    throw new ClientException(HttpStatus.ERROR, "批量更新数据失败");
}
log.info("批量更新数据成功，更新数量为：{}", updated);</code></pre><h6>限制单条SQL语句长度，批量更新操作具体实现</h6><p>因为<strong>SQL语句</strong>采用了<code>case when</code>来<strong>减少网络往返</strong>，为<strong>限制SQL语句长度防止溢出</strong>以及<strong>影响性能</strong>，采用的策略如下</p><p>若检测到参数updateCommentVoteCountDTOList超过指定个数，将会拆分成多条SQL语句与MySQL数据库进行交互</p><pre><code class="java">/**
 * 批量更新评论的点赞数和点踩数
 * @param updateCommentVoteCountDTOList     需要更新的评论点赞数和点踩数列表
 * @param batchSize                   批次大小，建议根据实际情况调整，过大可能导致单次更新过慢，过小可能导致更新次数过多
 * @return      成功更新的记录数
 */
public int batchUpdateCommentLikeCountAndDislikeCount(List&lt;UpdateCommentVoteCountDTO&gt; updateCommentVoteCountDTOList, Integer batchSize) {
    int totalUpdated = 0;
    for (int i = 0; i &lt; updateCommentVoteCountDTOList.size(); i += batchSize) {
        // 截取当前批次的数据
        List&lt;UpdateCommentVoteCountDTO&gt; batch = updateCommentVoteCountDTOList.subList(
                i,
                Math.min(i + batchSize, updateCommentVoteCountDTOList.size())
        );
        // 执行当前批次的更新
        int updated = articleCommentMapper.batchUpdateCommentLikeCountAndDislikeCount(batch);
        if (updated &lt; 0) {
            log.error("批量更新数据失败，当前批次更新数量: {}", updated);
            return -1;
        }
        totalUpdated += updated;

        log.info("批量更新评论点赞/点踩数，当前批次更新数量: {}, 累计更新数量: {}", updated, totalUpdated);
    }
    return totalUpdated;
}</code></pre><h6>SQL语句相关实现</h6><pre><code class="xml">&lt;update id="batchUpdateCommentLikeCountAndDislikeCount"&gt;
    UPDATE x_article_comments
    SET like_count = CASE id
    &lt;foreach collection="list" item="item"&gt;
        WHEN #{item.commentId} THEN #{item.likeCount}
    &lt;/foreach&gt;
    END,
    dislike_count = CASE id
    &lt;foreach collection="list" item="item"&gt;
        WHEN #{item.commentId} THEN #{item.dislikeCount}
    &lt;/foreach&gt;
    END,
    update_time = NOW()
    WHERE id IN
    &lt;foreach collection="list" item="item" open="(" separator="," close=")"&gt;
        #{item.commentId}
    &lt;/foreach&gt;
&lt;/update&gt;</code></pre><h5>清除已在该定时任务处理完的redis缓存数据</h5><p>前面提到的变量——likesUserIdToCommentIdsMap、dislikesUserIdToCommentIdsMap、statelessUserIdToCommentIdsMap。存储的值为经过逻辑后键中已被处理的值，在最后一步清除redis缓存发挥作用</p><p>上述变量类型形式为<strong>Map&lt;Long, Set&lt;String&gt;&gt;</strong>，满足了userId -&gt; 评论id集合，这可以精准且方便的清除已处理后的redis缓存数据</p><p>构建上述三个Map集合的过程在<a href="####从redis中获取数据，并将数据封装到两个集合中" target="_blank">第一步操作</a>中经历三个for循环已经构建完毕</p><p>该操作同样用到<strong>lua脚本</strong>保证<strong>原子性</strong></p><h6>lua脚本</h6><p>该脚本实现<strong>安全批量的移除</strong>key中<strong>要删除的元素列表</strong>，<strong>为什么不直接把键删除的原因</strong> -&gt; 假如在该定时任务执行过程中以及该脚本执行之前<strong>用户又发起点赞/拉踩/无状态请求</strong>，若与定时任务触发后<strong>从redis查到的用户对评论的状态相等，则会移除</strong>，这并没有什么问题，<strong>若不相等，不会移除新请求中用户设置的点赞/拉踩/无状态，留到下一次定时任务触发后处理</strong>。</p><pre><code class="lua">-- 安全批量删除，包含key存在性检查
-- KEYS[1]: Set的key
-- ARGV[1..n]: 要删除的元素列表
--
local key = KEYS[1]
local key_type = redis.call('TYPE', key).ok

-- 检查key是否存在且类型为set
if key_type == 'none' then
    return 0
elseif key_type ~= 'set' then
    return redis.error_reply('WRONGTYPE Operation against a key holding the wrong kind of value')
end

-- 兼容 Lua 5.1 和 Lua 5.2+
local unpack = unpack or table.unpack

local result = 0

-- 如果 ARGV 不为空，则执行批量删除
if #ARGV &gt; 0 then
    result = redis.call('SREM', key, unpack(ARGV))
end

return result</code></pre><h6>Service层相关代码</h6><pre><code class="java">// 清空Redis中的数据
DefaultRedisScript&lt;Long&gt; deleteScript = new DefaultRedisScript&lt;&gt;();
deleteScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(RedisLuaConstants.BATCH_REMOVE_MEMBERS_FROM_SET_LUA_SCRIPT_PATH)));
deleteScript.setResultType(Long.class);
likesUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
    Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + userId.toString()), commentIdSet.toArray());
    if (result == null || result &lt; 0){
        log.error("批量删除用户 {} 缓存点赞的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
    }
});
dislikesUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
    Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + userId.toString()), commentIdSet.toArray());
    if (result == null || result &lt; 0){
        log.error("批量删除用户 {} 缓存点踩的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
    }
});
statelessUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
    Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + userId.toString()), commentIdSet.toArray());
    if (result == null || result &lt; 0){
        log.error("批量删除用户 {} 缓存无状态的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
    }
});</code></pre><h4>SyncVote完整代码</h4><pre><code class="java">@Transactional
@Override
public Boolean syncVote() {
    //TODO 发散思维，该段逻辑似乎在收藏、投币等功能有相似之处，该段之所以有点复杂是因为有三种状态————点赞、点踩、无状态，且三者之间是互斥的，但前面的收藏、投币功能没有这个问题，并且好像只有两个状态，日后可以抽象出一个通用方法来处理这类功能，减少代码重复度
    Set&lt;String&gt; userLikesKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + "*");
    Set&lt;String&gt; userDislikesKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + "*");
    Set&lt;String&gt; userStatelessKeys = stringRedisTemplate.keys(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + "*");
    if (userLikesKeys.isEmpty() &amp;&amp; userDislikesKeys.isEmpty() &amp;&amp; userStatelessKeys.isEmpty()){
        log.info("没有需要同步的数据");
        return true;
    }
    List&lt;ArticleCommentVote&gt; votes = new ArrayList&lt;&gt;();      // 保存所有投票数据
    Set&lt;Long&gt; TotalcommentId = new HashSet&lt;&gt;();     // 保存所有评论id
    Map&lt;Long, Set&lt;String&gt;&gt; likesUserIdToCommentIdsMap = new HashMap&lt;&gt;();
    Map&lt;Long, Set&lt;String&gt;&gt; dislikesUserIdToCommentIdsMap = new HashMap&lt;&gt;();
    Map&lt;Long, Set&lt;String&gt;&gt; statelessUserIdToCommentIdsMap = new HashMap&lt;&gt;();
    for (String userLikesKey : userLikesKeys) {
        String userId = userLikesKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY, "");
        Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userLikesKey);
        for (String commentId : commentIds) {
            votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.LIKE, null, null));
            TotalcommentId.add(Long.parseLong(commentId));
            // 更新或新增likesUserIdToCommentIdsMap键值对，值的set集合新增commentId
            likesUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
        }
    }
    for (String userDislikesKey : userDislikesKeys) {
        String userId = userDislikesKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY, "");
        Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userDislikesKey);
        for (String commentId : commentIds) {
            votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.DISLIKE, null, null));
            TotalcommentId.add(Long.parseLong(commentId));
            // 更新或新增dislikesUserIdToCommentIdsMap键值对，值的set集合新增commentId
            dislikesUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
        }
    }
    for (String userStatelessKey : userStatelessKeys) {
        String userId = userStatelessKey.replace(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY, "");
        Set&lt;String&gt; commentIds = stringRedisTemplate.opsForSet().members(userStatelessKey);
        for (String commentId : commentIds) {
            votes.add(new ArticleCommentVote(Long.parseLong(commentId), Long.parseLong(userId), ArticleCommentVoteConstants.STATELESS, null, null));
            TotalcommentId.add(Long.parseLong(commentId));
            // 添加到statelessUserIdToCommentIdsMap键值对，值的set集合新增commentId
            statelessUserIdToCommentIdsMap.computeIfAbsent(Long.parseLong(userId), k -&gt; new HashSet&lt;&gt;()).add(commentId);
        }
    }
    // 根据votes执行落库逻辑（包含插入/更新）
    if (!votes.isEmpty()){
        int i = articleCommentVoteMapper.batchInsertOrUpdate(votes);
        if (i &lt; 0){
            throw new ClientException(HttpStatus.ERROR, "批量插入或更新数据失败");
        }
        log.info("批量插入或更新数据成功，数量为：{}", i);
    } else {
        log.info("没有需要落库的数据");
        return true;
    }
    // 根据TotalcommentId集合中的commentId，利用聚合函数获取对应表中对应评论的likeCount和dislikeCount，封装成List&lt;updateCommentVoteCountDTO&gt; updateCommentVoteCountDTOList
    List&lt;UpdateCommentVoteCountDTO&gt; updateCommentVoteCountDTOList = getCommentLikeCountAndDislikeCountByListOfCommentId(TotalcommentId);
    // 根据updateCommentVoteCountDTOList执行批量更新的逻辑，作用的表为x_article_comments，使用case when then语法更新likeCount和dislikeCount
    int updated = batchUpdateCommentLikeCountAndDislikeCount(updateCommentVoteCountDTOList, MySQLBatchSizeConstants.DEFAULT_UPDATE_BATCH_SIZE);
    if (updated &lt; 0){
        throw new ClientException(HttpStatus.ERROR, "批量更新数据失败");
    }
    log.info("批量更新数据成功，更新数量为：{}", updated);
    // 清空Redis中的数据
    DefaultRedisScript&lt;Long&gt; deleteScript = new DefaultRedisScript&lt;&gt;();
    deleteScript.setScriptSource(new ResourceScriptSource(new ClassPathResource(RedisLuaConstants.BATCH_REMOVE_MEMBERS_FROM_SET_LUA_SCRIPT_PATH)));
    deleteScript.setResultType(Long.class);
    likesUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
        Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_LIKES_USERS_KEY + userId.toString()), commentIdSet.toArray());
        if (result == null || result &lt; 0){
            log.error("批量删除用户 {} 缓存点赞的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
        }
    });
    dislikesUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
        Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_DISLIKES_USERS_KEY + userId.toString()), commentIdSet.toArray());
        if (result == null || result &lt; 0){
            log.error("批量删除用户 {} 缓存点踩的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
        }
    });
    statelessUserIdToCommentIdsMap.forEach((userId, commentIdSet) -&gt; {
        Long result = stringRedisTemplate.execute(deleteScript, List.of(RedisKeyConstants.ARTICLES_COMMENTS_STATELESS_USERS_KEY + userId.toString()), commentIdSet.toArray());
        if (result == null || result &lt; 0){
            log.error("批量删除用户 {} 缓存无状态的评论失败，欲删除的commentId -&gt; {}", userId, commentIdSet);
        }
    });
    return true;
}</code></pre>]]></description></item><item>    <title><![CDATA[100类中药材图像识别数据集分享（适用于目标检测任务） 风筝 ]]></title>    <link>https://segmentfault.com/a/1190000047609816</link>    <guid>https://segmentfault.com/a/1190000047609816</guid>    <pubDate>2026-02-13 16:06:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>100类中药材图像识别数据集分享（适用于目标检测任务）</h2><h3>数据集分享</h3><p>如需下载该数据集，可通过以下方式获取：</p><ul><li><p>💾 数据集打包为 ZIP 文件，解压后即用。</p><pre><code class="bash">https://pan.baidu.com/s/1zyL7C7byFj3VYeYnLGM2Gg?pwd=jsw8</code></pre></li></ul><h3>引言</h3><p>在中医药现代化的浪潮中，如何利用人工智能技术实现中药材的快速、准确识别，成为了中医药信息化领域的重要研究方向。传统的中药材识别主要依赖于专家经验和人工比对，这种方法不仅效率低下，而且在面对种类繁多、外观相似度高的中药材时，容易产生误判。随着深度学习技术的迅猛发展，特别是基于YOLO等目标检测模型的图像识别技术在多个领域取得显著成效，将其应用于中药图像识别已展现出广阔的前景。</p><p>为推动中药材智能识别的研究与落地，我们整理并发布了一套高质量的中药材图像识别数据集。该数据集涵盖100类常见中药材图像，共计9200张样本，并完成了标准YOLO格式的标注和训练/验证集划分，可直接用于模型训练和算法测试。本文将对该数据集进行详细介绍，包括数据集概述、结构详情、适用场景等内容，旨在为相关研究和应用提供参考。</p><h3>数据集概述</h3><p>本数据集收录了来自中药材识别实际场景中的100个类别图像，总计9200张高质量样本图。这些图像已按照<code>train/val</code>分组格式进行整理，适用于主流深度学习框架（如PyTorch、TensorFlow、YOLO等）的训练与验证流程。图像分辨率清晰，涵盖了不同拍摄角度、光照条件和背景下的实物图像，既体现了真实场景的复杂性，又保证了语义的代表性。</p><h4>数据集基本信息</h4><ul><li><strong>图像总数</strong>：9200张</li><li><strong>训练集</strong>：8000张</li><li><strong>验证集</strong>：1200张</li><li><strong>类别数量</strong>：100种中药材</li><li><strong>命名规范</strong>：统一使用简体中文命名，便于中文语义处理任务</li></ul><h3>数据集结构</h3><p>本数据集采用标准的文件夹结构进行组织，具体如下：</p><pre><code class="plaintext">/train/
    └── 安息香/
    └── 白扁豆/
    ...
/val/
    └── 安息香/
    └── 白扁豆/
    ...</code></pre><p>文件命名规则为自动生成，确保不重名，例如<code>安息香_001.jpg</code>。这种结构设计不仅便于数据的管理和浏览，也符合主流深度学习框架的数据加载要求。</p><h4>类别配置</h4><p>以下是数据集的类别配置（YOLO格式）：</p><pre><code class="yaml">nc: 100
names: ['安息香', '白扁豆', '白矾', '白蔹', '白茅根', '白前', '白芍', '白芷', '柏子仁', '北沙参',
        '荜拨', '荜澄茄', '鳖甲', '槟榔', '苍术', '草豆蔻', '沉香', '川楝子', '川木香', '川牛膝',
        '大腹皮', '淡豆豉', '稻芽', '地龙', '冬虫夏草', '防风', '番泻叶', '蜂房', '甘草', '干姜',
        '甘松', '藁本', '硅石脂', '枸杞子', '桂枝', '谷精草', '谷芽', '海龙', '海螵蛸', '合欢皮',
        '黄柏', '黄芪', '黄芩', '湖北贝母', '僵蚕', '芥子', '鸡冠花', '金灯笼', '鸡内金', '荆芥穗',
        '金果榄', '金钱白花蛇', '九香虫', '橘核', '苦地丁', '莱菔子', '莲房', '莲须', '莲子',
        '莲子心', '灵芝', '荔枝核', '龙眼肉', '芦根', '路路通', '麦冬', '木丁香', '羌活',
        '千年健', '秦皮', '全蝎', '忍冬藤', '人参', '肉豆蔻', '桑寄生', '桑螵蛸', '桑椹',
        '山慈菇', '山奈', '山茱萸', '沙苑子', '石榴皮', '丝瓜络', '酸枣仁', '苏木',
        '太子参', '天花粉', '天麻', '土荆皮', '瓦楞子', '五加皮', '细辛', '银柴胡',
        '薏苡仁', '郁金', '浙贝母', '枳壳', '竹茹', '诃子', '自然铜']</code></pre><h3>数据处理流程</h3><p>为确保数据集的质量和可用性，我们在构建过程中遵循了严格的数据处理流程，具体步骤如下：</p><pre style="display:none;"><code class="mermaid">flowchart TD
    A[数据收集] --&gt; B[图像预处理]
    B --&gt; C[类别标注]
    C --&gt; D[数据划分]
    D --&gt; E[格式转换]
    E --&gt; F[质量检测]
    F --&gt; G[最终发布]</code></pre><ol><li><strong>数据收集</strong>：从多个来源收集中药材图像，确保覆盖不同角度、光照和背景</li><li><strong>图像预处理</strong>：对收集到的图像进行清洗、去噪和标准化处理</li><li><strong>类别标注</strong>：采用人工标注的方式，确保类别归属的准确性</li><li><strong>数据划分</strong>：按照7:3的比例划分为训练集和验证集</li><li><strong>格式转换</strong>：将标注结果转换为YOLO标准格式</li><li><strong>质量检测</strong>：对处理后的数据进行质量检查，确保数据的一致性和完整性</li><li><strong>最终发布</strong>：打包发布数据集，提供下载链接</li></ol><h3>数据集特点</h3><p>本数据集具有以下显著特点：</p><ol><li><strong>类别丰富</strong>：涵盖100种常见中药材，基本覆盖了临床常用品种</li><li><strong>样本充足</strong>：总计9200张图像，每个类别均有足够的样本量</li><li><strong>标注规范</strong>：采用标准YOLO格式标注，可直接用于模型训练</li><li><strong>场景多样</strong>：图像拍摄场景多样，包括不同角度、光照和背景</li><li><strong>中文命名</strong>：统一使用简体中文命名，便于中文语义处理任务</li><li><strong>结构清晰</strong>：采用标准文件夹结构，易于管理和使用</li></ol><h3>适用场景</h3><p>本数据集可广泛应用于以下人工智能与中医药交叉领域：</p><h4>1. 中药识别图像分类任务</h4><p>可用于训练ResNet、ViT、YOLO等模型，实现中药材的自动分类和识别。通过深度学习模型的训练，可以提高中药材识别的准确率和效率，减少人工干预。</p><h4>2. 中药拍照识别App研发</h4><p>作为图像识别后端训练数据，可支持开发中药拍照识别App，用户只需拍摄中药材照片，即可快速获取药材名称、功效等信息，便于中药辅助查询和科普应用。</p><h4>3. 医学辅助系统训练数据</h4><p>可结合图文信息进行知识联动识别，为医生提供中药材识别的辅助工具，减少用药错误的发生。</p><h4>4. 深度学习模型迁移学习训练</h4><p>可用于预训练或微调模型，增强模型对自然图像中药材的理解能力，为其他相关任务提供基础。</p><h4>5. 中药材跨模态研究</h4><p>可用于中文名称—图像联合建模、图文检索、图像标注等跨模态研究，推动中医药信息化的发展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046860383" alt="image-20250712145211757" title="image-20250712145211757"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046860384" alt="image-20250712145359020" title="image-20250712145359020" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046860385" alt="image-20250712145509920" title="image-20250712145509920" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046860386" alt="image-20250712145520147" title="image-20250712145520147" loading="lazy"/></p><h3>模型训练建议</h3><p>针对本数据集的特点，我们提出以下模型训练建议：</p><ol><li><strong>模型选择</strong>：对于分类任务，可选择ResNet50、EfficientNet等模型；对于检测任务，建议使用YOLOv8、RT-DETR等最新模型。</li><li><strong>数据增强</strong>：建议使用随机裁剪、翻转、旋转、亮度调整等数据增强技术，提高模型的泛化能力。</li><li><strong>训练策略</strong>：采用小批量梯度下降法，初始学习率设置为0.001，根据验证集性能动态调整学习率。</li><li><strong>评估指标</strong>：使用准确率、精确率、召回率和F1-score等指标评估模型性能。</li></ol><h3>应用案例</h3><h4>案例一：中药识别App开发</h4><p>基于本数据集训练的模型，开发了一款中药识别App，用户只需拍摄中药材照片，即可快速获取药材名称、功效、用法等信息。该App已在多家中医院和药店试用，取得了良好的效果。</p><h4>案例二：中医药教学辅助系统</h4><p>将训练好的模型集成到中医药教学辅助系统中，学生可以通过系统识别中药材，加深对中药材的认识和理解，提高学习效率。</p><h4>案例三：中药材质量检测</h4><p>结合其他传感器数据，利用训练好的模型对中药材质量进行检测，识别药材的真伪和品质等级，为中药材的质量控制提供技术支持。</p><h3>结语</h3><p>中药文化源远流长，是中华民族的瑰宝。随着人工智能技术的不断发展，将其应用于中医药领域，实现中药材的智能识别，对于推动中医药现代化具有重要意义。本数据集立足实际拍摄与分类标准，旨在为研究者、开发者和中医药爱好者提供一份结构清晰、数据质量可靠、类别丰富的中药图像数据集，为中药AI识别迈出坚实一步。</p><p>我们希望通过本数据集的发布，能够促进中医药与人工智能的深度融合，推动中药材智能识别技术的发展和应用，为中医药现代化做出贡献。如需生成配套训练代码（如YOLOv8格式训练脚本）、中药图像识别模型部署方案，可以参考相关资源。</p><h3>参考资源</h3><ul><li><a href="https://link.segmentfault.com/?enc=zXzo%2BZs2viCXVlw4YLszFw%3D%3D.P70OY6pjvMwo6Z5%2FKPP0bVeqIaQ6Ql66YhCW1wZF59Y%3D" rel="nofollow" target="_blank">YOLOv8官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=H4fHTfRe87i8dOUG1dfjpg%3D%3D.hFKsJE8wYb1T83QbSR8Wna3EF%2BV5GvySXvy8Nfceh2iCQ5gxFVSXIcmFCwRNiYVk" rel="nofollow" target="_blank">PyTorch官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=kcwO9jm4IXv4ib1XiT2x%2FQ%3D%3D.UfQDwJ9BCASoX9VCBh9bFwzUzxyc2aGyHMV9T7ERKlw%3D" rel="nofollow" target="_blank">TensorFlow官方文档</a></li></ul><p>通过本数据集的使用和相关技术的应用，我们相信中药材智能识别技术将会取得更大的突破，为中医药事业的发展注入新的活力。</p>]]></description></item><item>    <title><![CDATA[金属材料表面六种缺陷类型数据集：工业视觉检测的优质训练资源 风筝 ]]></title>    <link>https://segmentfault.com/a/1190000047609826</link>    <guid>https://segmentfault.com/a/1190000047609826</guid>    <pubDate>2026-02-13 16:05:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>金属材料表面六种缺陷类型数据集：工业视觉检测的优质训练资源</h2><h3>数据集分享</h3><p>如需下载该数据集，可通过以下方式获取：</p><pre><code class="bash">https://pan.baidu.com/s/1eltE8ewS4V1ONDGubsYJ4g?pwd=skr8</code></pre><h3>引言</h3><p>在现代工业制造中，金属材料的表面质量直接影响产品的外观、性能和安全性。金属材料在轧制、热处理、运输及长期使用过程中，常会产生各类表面缺陷，如裂纹、划痕、氧化皮等。这些缺陷不仅降低产品的外观质量，更可能影响其强度、疲劳寿命甚至安全性能。因此，及时、准确地检测金属表面缺陷，对于保证产品质量、提高生产效率具有重要意义。</p><p>随着工业制造向自动化与智能化演进，基于深度学习的表面缺陷检测成为提升质量控制的重要手段。然而，高质量、标注规范的数据集一直是算法研究和应用落地中的瓶颈。为推动智能检测系统在实际场景中的应用，我们构建了一套面向学术与工业的金属缺陷数据集，包含6类典型缺陷，1800张图像，标注完整，已按<code>train/val/test</code>划分，并使用YOLO项目格式进行标注，适用于目标检测、缺陷分类与工业视觉相关任务。</p><h3>数据集概述</h3><p>本数据集聚焦于金属表面质量检测，涵盖了6类典型的金属表面缺陷，总计1800张高质量图像。所有图像均已完成标注，并按照训练集、验证集和测试集进行了合理划分，可直接用于深度学习模型的训练、验证和测试。</p><h4>基本信息</h4><ul><li><strong>图像总数</strong>：1800张（已完成标注）</li><li><strong>标注格式</strong>：YOLO格式（可与COCO格式相互转化）</li><li><strong>图像尺寸</strong>：统一为640×640（可自定义缩放）</li><li><p><strong>数据划分</strong>：</p><ul><li>训练集: 1260张</li><li>验证集: 360张</li><li>测试集: 180张</li></ul></li><li><strong>类别数量</strong>：6类</li></ul><h4>类别配置</h4><p>以下是数据集的类别配置（YOLO格式）：</p><pre><code class="yaml">nc: 6
names:
  0: crazing
  1: inclusion
  2: patches
  3: pitted_surface
  4: rolled-in_scale
  5: scratches</code></pre><h3>数据集结构</h3><p>本数据集采用标准的文件夹结构进行组织，具体如下：</p><pre><code class="plaintext">/train/
    └── images/
    └── labels/
/val/
    └── images/
    └── labels/
/test/
    └── images/
    └── labels/</code></pre><p>其中，<code>images</code>文件夹存放原始图像，<code>labels</code>文件夹存放对应的标注文件。标注文件采用YOLO格式，记录了缺陷的类别和位置信息。</p><h3>缺陷类型详情</h3><p>本数据集包含6类典型的金属表面缺陷，每类缺陷均有其独特的特征和形成原因。以下是各类缺陷的详细说明：</p><table><thead><tr><th>类别编号</th><th>类别名称</th><th>中文释义</th><th>特征描述</th></tr></thead><tbody><tr><td>0</td><td>crazing</td><td>裂纹/龟裂</td><td>表面微裂纹，形似龟壳裂纹，多因材料老化或热处理不均导致</td></tr><tr><td>1</td><td>inclusion</td><td>杂质夹杂</td><td>材料中混入非金属杂质，外观呈点状或条状暗斑，影响材料纯度</td></tr><tr><td>2</td><td>patches</td><td>表面块状斑痕</td><td>局部表面区域发生变色或质地异常，可能与氧化或油污有关</td></tr><tr><td>3</td><td>pitted_surface</td><td>凹坑/腐蚀点</td><td>表面形成小孔或点蚀，通常是腐蚀或加工缺陷的结果</td></tr><tr><td>4</td><td>rolled-in_scale</td><td>轧入氧化皮</td><td>热轧过程中氧化皮卷入表层形成异色斑块，边缘不规则</td></tr><tr><td>5</td><td>scratches</td><td>划痕</td><td>线性划痕，由硬物刮擦形成，深浅不一，走向基本一致</td></tr></tbody></table><p>所有缺陷都已使用边界框（bounding box）形式手动标注，标注精度高，适合用于YOLO全系列、Faster R-CNN、RT-DETR等检测模型的训练和评估。</p><h3>数据处理流程</h3><p>为确保数据集的质量和可用性，我们在构建过程中遵循了严格的数据处理流程，具体步骤如下：</p><pre style="display:none;"><code class="mermaid">flowchart TD
    A[数据采集] --&gt; B[图像预处理]
    B --&gt; C[缺陷标注]
    C --&gt; D[数据划分]
    D --&gt; E[格式转换]
    E --&gt; F[质量验证]
    F --&gt; G[数据集发布]</code></pre><ol><li><strong>数据采集</strong>：从工业生产现场采集金属表面缺陷图像，确保覆盖不同类型、不同严重程度的缺陷</li><li><strong>图像预处理</strong>：对采集到的图像进行清洗、去噪、尺寸统一等处理</li><li><strong>缺陷标注</strong>：采用人工标注的方式，使用边界框标记缺陷的位置和类别</li><li><strong>数据划分</strong>：按照7:2:1的比例划分为训练集、验证集和测试集</li><li><strong>格式转换</strong>：将标注结果转换为YOLO标准格式</li><li><strong>质量验证</strong>：对处理后的数据进行质量检查，确保标注的准确性和一致性</li><li><strong>数据集发布</strong>：打包发布数据集，提供下载链接</li></ol><h3>数据集特点</h3><p>本数据集具有以下显著特点：</p><ol><li><strong>标注规范</strong>：所有图像均采用人工标注，标注精度高，格式统一</li><li><strong>数据划分合理</strong>：按照7:2:1的比例划分为训练集、验证集和测试集，符合深度学习模型训练的常规要求</li><li><strong>缺陷类型典型</strong>：涵盖了6类典型的金属表面缺陷，基本覆盖了工业生产中常见的缺陷类型</li><li><strong>图像质量高</strong>：所有图像均为高质量采集，分辨率统一为640×640，便于模型训练</li><li><strong>格式标准</strong>：采用YOLO标准格式标注，可直接用于主流深度学习框架</li><li><strong>场景真实</strong>：图像均来自实际工业生产场景，具有较高的真实感和代表性</li></ol><h3>适用场景</h3><p>本数据集广泛适用于以下研究与工业应用：</p><h4>1. 工业缺陷检测模型训练</h4><p>可直接用于训练YOLOv5、YOLOv8、RT-DETR等检测模型，用于实际部署或研究验证。通过在本数据集上训练模型，可以实现对金属表面缺陷的自动检测和分类，提高检测效率和准确性。</p><h4>2. 缺陷分类与分割任务</h4><p>可对图像中心区域裁剪生成分类任务数据，或与语义分割工具配合进一步扩展。例如，可以将缺陷区域裁剪出来，构建分类数据集，用于训练专门的缺陷分类模型；也可以将边界框标注转换为像素级标注，用于语义分割任务。</p><h4>3. 算法对比与论文验证</h4><p>适合用于不同检测网络的性能评估，支持标准化训练流程，有利于模型泛化性对比。研究人员可以在本数据集上测试不同算法的性能，进行公平的比较和分析。</p><h4>4. 图像增强与合成学习研究</h4><p>图像背景多样、缺陷类型复杂，适合作为生成对抗网络（GAN）或图像增强算法的输入。通过对数据集进行图像增强，可以扩展数据集规模，提高模型的泛化能力；也可以用于研究缺陷图像的合成方法，进一步丰富数据集。</p><h4>5. 工业自动化质检系统开发</h4><p>可集成至边缘计算设备，实现对流水线上的金属件在线检测与报警。通过将训练好的模型部署到边缘设备，可以实现实时、高效的缺陷检测，减少人工干预，提高生产效率和产品质量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046856498" alt="image-20250710185629776" title="image-20250710185629776"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046597730" alt="image-20250530103542576" title="image-20250530103542576" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046597733" alt="image-20250530102805805" title="image-20250530102805805" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046597734" alt="img" title="img" loading="lazy"/></p><h3>模型训练建议</h3><p>针对本数据集的特点，我们提出以下模型训练建议：</p><ol><li><strong>模型选择</strong>：对于目标检测任务，建议使用YOLOv8、RT-DETR等最新模型，这些模型在精度和速度上都有较好的表现。</li><li><strong>数据增强</strong>：建议使用随机裁剪、翻转、旋转、亮度调整、对比度调整等数据增强技术，提高模型的泛化能力。</li><li><strong>训练策略</strong>：采用小批量梯度下降法，初始学习率设置为0.001，使用余弦退火策略调整学习率。</li><li><strong>评估指标</strong>：使用精确率、召回率、F1-score和mAP等指标评估模型性能，综合考虑模型的检测效果。</li><li><strong>模型优化</strong>：可以采用模型剪枝、量化等技术，减少模型大小，提高推理速度，便于在边缘设备上部署。</li></ol><h3>应用案例</h3><h4>案例一：钢铁生产线上的缺陷检测</h4><p>某钢铁企业将基于本数据集训练的模型部署到生产线上，实现了对钢板表面缺陷的实时检测。系统能够在钢板生产过程中自动检测出裂纹、划痕等缺陷，并及时报警，大大提高了检测效率和准确性，减少了人工成本。</p><h4>案例二：汽车零部件质量控制</h4><p>某汽车零部件制造商使用本数据集训练的模型，对汽车车身钢板的表面缺陷进行检测。通过在生产线上安装摄像头和边缘计算设备，实现了对零部件表面缺陷的自动检测，确保了产品质量，降低了不合格品率。</p><h4>案例三：研究算法性能对比</h4><p>某研究机构使用本数据集对多种目标检测算法进行了性能对比，包括YOLOv5、YOLOv8、Faster R-CNN等。通过实验分析，他们发现YOLOv8在检测精度和速度上都有较好的表现，适合用于实时检测场景。</p><h3>数据集扩展与未来规划</h3><p>本数据集是我们在金属表面缺陷检测领域的初步尝试，未来我们计划从以下几个方面对数据集进行扩展和完善：</p><ol><li><strong>增加缺陷类型</strong>：进一步扩展缺陷类别，涵盖更多工业生产中常见的金属表面缺陷</li><li><strong>扩大数据集规模</strong>：增加图像数量，提高数据集的多样性和代表性</li><li><strong>添加多模态标注</strong>：加入语义分割、实例分割等多模态标注形式，支持更复杂的检测与识别任务</li><li><strong>引入更多场景</strong>：收集不同材质、不同工艺、不同环境下的金属表面缺陷图像，提高模型的泛化能力</li><li><strong>提供预训练模型</strong>：基于扩展后的数据集，训练并发布预训练模型，方便用户直接使用</li></ol><h3>结语</h3><p>本数据集通过系统性地收集、整理和标注金属材料表面六类典型缺陷，填补了工业视觉领域在金属表面缺陷检测方向公开数据资源的空白。其在样本多样性、标注精度和场景适配性方面具有显著优势，不仅可作为深度学习算法的训练基准，也适用于真实工业质检系统的部署验证。</p><p>我们希望通过本数据集的发布，能够促进工业视觉检测技术的发展，推动智能制造与视觉质检技术的落地应用。我们诚邀学术界与工业界的研究者在此基础上深入探索，共同推动金属表面缺陷检测技术的进步，为工业制造的高质量发展做出贡献。</p><p>通过本数据集的使用和相关技术的应用，我们相信金属表面缺陷检测技术将会取得更大的突破，为工业制造的质量控制提供更加强有力的支持。</p>]]></description></item><item>    <title><![CDATA[国内企业使用最多的SRM系统是哪几个品牌？首席技术官深度选型指南 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047609829</link>    <guid>https://segmentfault.com/a/1190000047609829</guid>    <pubDate>2026-02-13 16:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业数字化转型的深水区，SRM的选型早已超出简单采购工具的范畴。作为在SRM与数字化采购领域深耕20年的从业者，我见证了企业采购从人工方式走向ERP系统，再到今天的独立平台化的演进过程。很多CTO在选型时会被品牌知名度牵着走，从而忽略了底层架构与企业业务演进的匹配度。<strong>尤其在供应链风险频发的背景下，一套成熟的SRM应当成为企业的供应协同引擎，而不是一个漂亮但封闭僵化的预制套件。</strong><br/>基于艾瑞咨询《2024年中国采购数字化平台行业研究报告》等第三方机构的行业洞察，采购数字化平台正向“自动化、智能化”演进。报告详细探讨了“低代码/零代码”、“iPaaS”以及“AI赋能”在SRM领域的关键作用，这些技术正成为衡量厂商底座能力的核心指标。下面先从CTO最关心的维度做直观对比，再逐一展开分析。</p><h3>一、一个表格读懂三大类SRM服务商</h3><p>在深入分析各品牌前，我们需要从CTO关心的维度对这三大类做直观对比。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609831" alt="图片" title="图片"/></p><h3>二、平台定制型：追求架构底座的生命力</h3><p>这类厂商适合业务逻辑复杂、希望系统能持续进化的巨型企业。推荐对象是那种愿意把SRM当作长期数智化底座的公司。</p><h4>1、<a href="https://link.segmentfault.com/?enc=%2F3iCjXFmWsOdmnvJ934%2B3w%3D%3D.xv21Ch%2ByoqKv9EpGIJ9BW0Rqw2C2W%2FJexg%2BfMn0xCMU%3D" rel="nofollow" target="_blank">正远科技</a></h4><p>成立于2002年，是低代码驱动的平台化专家。它的核心优势在于把业务逻辑和系统实现解耦，允许可视化建模和快速迭代。由此带来的好处很直观：开发周期大幅缩短，业务方能拿到的是一个可持续生长的系统，而不是上线就过时的产品。正远还在AI能力和信创适配上投入很多。比如发票识别、供应商风险侦测和合同合规比对等，都已经工具化。<br/>对于注重数据主权并要求私有化部署的企业，正远对信创生态的支持是显著优势。其客户包括魏桥创业、南山集团和威高集团，能在复杂场景下体现出底层理解能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609832" alt="图片" title="图片" loading="lazy"/></p><h3>三、ERP延伸型：强调生态一致性与合规</h3><p>这类产品更适合业务流程比较标准、并且已经深度绑定某套ERP的企业。它们的价值在于和现有财务、供应链系统天然打通，降低合规与对账的摩擦。</p><h4>1、用友</h4><p>在大型集团中普及度极高，它的优势在于业财一体化，适合把财务合规和供应链管理放在同一治理框架下的企业。用友在电子招投标和合规实践方面有丰富经验，适合把合规作为首要诉求的公司。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609833" alt="图片" title="图片" loading="lazy"/></p><h4>2、金蝶</h4><p>侧重协同与轻量应用，近年通过AI产品线在制造业展现出竞争力。金蝶强调社交化协同与低门槛供应商接入，利用微信等渠道提高供应商响应效率。在需要管理大量中小供应商，或研发变更频繁的电子制造企业，金蝶的PLM与SRM结合模式表现稳健。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609834" alt="图片" title="图片" loading="lazy"/></p><h3>四、专业垂直型：以速度优先，解决当下问题</h3><p>如果企业的采购并不直接影响核心生产，只是希望先把流程跑顺，或者需要在较短时间内看到效果，那么专业垂直型SRM往往更现实。这类厂商主打标准化SaaS，优势不在“复杂能力”，而在于交付速度。对互联网、快消，以及正在试点数字化采购的企业来说，这是一个低风险的切入口。</p><h4>1、甄云科技</h4><p>甄云是国内较早将SRM做成标准化SaaS的厂商之一。它在界面设计和流程完整度上比较成熟，上手成本低。甄云的一个典型特点是引入了采购商城模式，能够直接对接京东、苏宁等第三方平台。对于间接物料和行政类采购，这种模式效率很高，往往能在较短周期内看到管理成效。当然，它更适合通用场景，而不是深度定制。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609835" alt="图片" title="图片" loading="lazy"/></p><h4>2、企企通</h4><p>企企通的产品思路更偏向互联网化，强调协同体验和移动端使用感受。它围绕采购人的日常操作习惯做了不少优化，在审批流转和跨部门协同上比较顺畅。同时，企企通在供应链金融等延展能力上投入较多，适合追求敏捷管理、组织结构相对扁平的企业。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609836" alt="图片" title="图片" loading="lazy"/></p><h3>五、CTO在选型时关注的三个问题</h3><p>当选型进入后半程，技术团队往往已经看过大量方案。这时困扰CTO的，通常不是功能多少，而是一些更底层、更现实的取舍问题。</p><h4>Q1：如何平衡快速上线和长期可扩展性？</h4><p>如果企业的采购流程还在摸索阶段，主要覆盖办公用品或非生产物资，那么专业垂直型SaaS是性价比最高的选择，三个月内看到ROI并不罕见。但如果SRM管理的是核心生产物料，涉及BOM频繁调整、供应商分级和绩效考核，这类系统往往很快触顶。从长期看，平台定制型虽然前期投入更高，但低代码架构能支撑持续迭代，避免几年后被迫推倒重来。</p><h4>Q2：已有SAP或Oracle，集成难度该怎么看？</h4><p>信息孤岛是CTO普遍焦虑的问题。ERP延伸型产品在自家体系内集成顺畅，但一旦涉及多套异构系统，灵活性反而受限。独立的平台型厂商在这方面通常更有经验，比如通过集成适配器处理复杂的主数据和业务同步。建议在选型阶段要求厂商做真实接口演示，现场拉通旧系统，而不是只看方案文档。</p><h4>Q3：信创私有化部署是否意味着运维压力陡增？</h4><p>在当前合规环境下，私有化部署已经成为不少中大型制造企业的标配。私有化部署虽然需要额外的算力和运维投入，但在现代架构的帮助下复杂度已经显著降低。基于容器和流程引擎的系统，运维已经可以做到模块化和精细化管理。对技术团队来说，这种投入换来的，是对核心数据和业务逻辑的完全掌控。</p><h3>六、CTO在选型时绕不开的三条逻辑</h3><p>从行业发展趋势来看，SRM正从单一流程工具，演进为企业级数智化平台。CTO在做最终决策时，建议回到以下三个核心判断。</p><h4>1、部署模式的取舍</h4><p>对于中大型制造企业，私有化部署仍然更稳妥。SaaS 上线速度快，但在复杂集成和安全控制方面弹性有限。私有化部署虽然前期投入更高，却能为长期扩展留出空间。</p><h4>2、集成能力是否经得起实战考验</h4><p>SRM的价值在于连接，而不是孤立存在。是否具备对接SAP、Oracle等主流ERP的真实案例，比功能列表更重要。选型时一定要看实操，而不是听承诺。</p><h4>3、低代码是否真正可用</h4><p>采购流程不会一成不变。如果每次调整都需要厂商改底层代码，系统的长期成本会迅速失控。成熟的低代码底座，能把变化留在企业内部消化，这是很多CTO后期才意识到的关键点。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609837" alt="图片" title="图片" loading="lazy"/></p><h3>七、我们的建议</h3><p>整体来看，国内SRM市场已经形成清晰分层。希望系统具备持续演进能力，并对信创和复杂制造场景有要求的企业，可以重点关注正远科技。强调合规和业财一体化的集团型企业，用友和金蝶依然是稳妥选择。而在轻量化采购、商城联动和快速交付方面，甄云和企企通具备明显优势。<br/>SRM的选型，本质上是在为企业选择一个可以陪跑多年的数字化底座。从CTO的角度看，能随着业务一起成长的系统，才是真正值得长期投入的技术资产。</p>]]></description></item><item>    <title><![CDATA[中国版“龙虾”重磅发布！立即领取限免“枫清龙虾新春码” Fabarta ]]></title>    <link>https://segmentfault.com/a/1190000047609845</link>    <guid>https://segmentfault.com/a/1190000047609845</guid>    <pubDate>2026-02-13 16:03:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609847" alt="微信图片_20260213092724_2808_3.jpg" title="微信图片_20260213092724_2808_3.jpg"/><br/>最近，当红AI 助手 OpenClaw “龙虾”（原名 MoltBot、ClawdBot）以燎原之势席卷全球开发者社区，开启了全新的 “全职 AI 员工” 时代。然而，当自动化能力的获取不再是高门槛，如何让 AI 在高效执行任务的同时，始终将控制权交还给人，成为行业新的挑战 —— 理想的智能体，应当既能深入本地系统流畅操作，又能在每一个关键节点等待人工确认，让数据主权与操作可控成为默认配置。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609848" alt="微信图片_20260213103134_2850_3.jpg" title="微信图片_20260213103134_2850_3.jpg" loading="lazy"/></p><h3>枫清科技重磅推出Fabarta 个人专属智能体龙虾版</h3><p>当海外用户纷纷尝试OpenClaw 本地部署方法，以此保障数据与操作的可控性时，枫清科技正式推出 Fabarta 个人专属智能体龙虾版，让每个人都能拥有“更懂你、更安全” 的专属 AI 助手。</p><h3>扎根本地的高效办公超级助手</h3><p>这款面向办公与个人生产力的本地智能体，支持文件与应用协作、可审计的本地工具调用，并能通过本地记忆能力持续适配用户的使用习惯。与此同时，个人专属智能体龙虾版的本地知识库能力，沿用了深度打磨的企业级解析器，同时搭载经过链路调优的技术架构，让知识检索更精准、更全面。</p><p>而这些高效办公能力的落地，均依托其底层架构确立的本地执行核心原则。Fabarta 个人专属智能体龙虾版并非一个简单的聊天助手，而是真正在用户设备上运行的超级助手。基于 OpenClaw 沉淀的本地执行框架，它能够直接操作用户的文件系统、浏览器和各类应用，自动完成文件整理、流程任务执行等复杂操作。</p><h3>本地执行+ 人工终审 掌控终极决策权</h3><p>更重要的是，所有数据处理均在本地完成，配合白名单权限管理和全程可审计机制，每一次操作都留有痕迹，关键步骤需人工确认，用户既能享受AI 自动化带来的高效体验，又能牢牢掌握人工的终极决策权。</p><h3>多办公场景的自动化实践</h3><p>这种“本地执行 + 人工终审” 的设计理念，在真实办公场景中有着直观且丰富的体现。当用户面对复杂的项目资料，只需一句话指令，Fabarta 个人专属智能体龙虾版就能在本机自动完成分类归档，按项目和资产类型建立清晰的文件库，生成总索引和资产盘点表，整个过程仅写入指定的 Outbox 文件夹，每一步操作都可审计、可追溯。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609849" alt="图片2.png" title="图片2.png" loading="lazy"/></p><p>当用户面对繁杂的邮件回复需求，个人专属智能体龙虾版能自动读取收件箱、拟写回复，甚至自动打开邮箱客户端，但会明确停在发送前的最后一步，等待用户检查确认。用户无需担心AI 越权操作，因为所有动作全程留痕，截图、时间线、操作轨迹（trace）均可回放，形成完整的操作证据包。</p><p>对于需要运营社媒账号的用户，个人专属智能体龙虾版能将本地素材自动转化为小红书、公众号等平台的图文草稿，自动填充内容和配图，且仅保存草稿而不进行群发，让用户牢牢掌握内容最终发布权。</p><h3>越用越懂的个性化私人助理</h3><p>除了具备强大的自动化执行能力，个人专属智能体龙虾版更像一位越用越懂你的专属私人助理。它内置本地个人记忆库，能够持续理解用户的使用习惯，基于企业级知识库解析能力，对个人文件资料形成长期语义记忆。它会针对用户的常用术语、项目背景、文件路径及个人偏好，在本机逐步构建起可控的长期记忆体系，实现更精准、更全面的知识检索与复用。</p><p>这份“越用越懂” 的使用体验，得益于枫清科技将企业级 RAG 能力下沉至个人场景，配合可配置的私有模型部署方案与云边端协同架构，既保证了个人数据的隐私安全，又能根据用户需求灵活调用云端能力。</p><h3>全系统适配 + 多场景平台拓展</h3><p>值得一提的是，这一架构已深度融入主流操作系统生态—— 通过与 macOS、麒麟操作系统的深度适配，Fabarta 个人专属智能体龙虾版将企业级安全管控与本地化处理能力成功延伸至个人终端。其中，Fabarta 携手 Mac 生态推出的企业级 AI 解决方案，以 Mac Studio为核心构建企业知识中台（EKC），协同终端Mac mini上的个人专属智能体龙虾版，既释放 “开箱即用” 的本地 AI 生产力价值，又实现 “数据不出域” 的精细权限控制；联合麒麟生态打造的信创桌面个人智能体解决方案，更是依托麒麟 KART 系统级 AI 底座承载端侧模型，让敏感数据全程在本地闭环处理。</p><p>除了完成主流操作系统的深度适配，让本地执行能力落地各类终端，个人专属智能体龙虾版还在模型部署与平台搭建层面实现了全维度拓展。除云端大模型外，枫清科技还提供本地模型一体机方案及企业智能体平台，实现从个人助手到集团系统的全场景覆盖。</p><h3>枫清龙虾新春码限时免费申领</h3><p>目前，枫清科技官网已开启“枫清龙虾新春码” 限时发放活动，春节期间每天上午 10:00-10:30 限量释放免费体验名额。用户领取 “新春码” 后即可激活使用，体验期结束后可购买权益包继续享受服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609850" alt="图片4.png" title="图片4.png" loading="lazy"/></p><p>即刻登录枫清科技官网（<a href="https://link.segmentfault.com/?enc=duY2YGLUt5%2BHO60Ac5PnnA%3D%3D.4oHSMrIm5x%2FPNS97f8J6U%2B0KaIEgoO%2FQDXOat9Ltcgg%3D" rel="nofollow" target="_blank">https://fabarta.com/my-agent</a>）下载Fabarta 个人专属智能体，把工作放心交给你的 “个人超级智能体”！</p>]]></description></item><item>    <title><![CDATA[赋能某跨境智慧物流：基于 AWS Graviton 的全栈数据平台实现 25% 成本节省 数新智能 ]]></title>    <link>https://segmentfault.com/a/1190000047609878</link>    <guid>https://segmentfault.com/a/1190000047609878</guid>    <pubDate>2026-02-13 16:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>某跨境智慧物流集团是跨境物流与供应链数字化解决方案的行业领导者。为应对海量物流数据实时处理、全球化部署与成本效益持续优化等挑战，该客户携手数新智能，在亚马逊云科技（AWS）上完成核心数据平台的战略性重构。</p><p>本次项目的核心亮点为：全栈采用基于 AWS 自研芯片 Graviton 的实例，并部署数新云智能原生数据底座 CyberEngine，旨在打造兼具极致性价比、卓越性能与全球敏捷性的下一代数据基础设施。</p><p><strong>关于客户</strong></p><p><strong>在海量数据中寻求效率与成本的平衡</strong></p><p>该客户全球化业务每天产生并处理 TB 级的物流轨迹、仓储库存与交易数据。原有架构面临三大核心挑战：</p><ul><li><strong>计算成本高：</strong> 数据处理资源消耗巨大，传统 x86 计算实例的高昂成本成为业务扩张的沉重负担。</li><li><strong>实时分析瓶颈：</strong> 物流状态追踪、智能调度等场景对实时性要求严苛，原有系统难以支撑毫秒级响应的数据服务与高并发分析。</li><li><strong>架构敏捷性不足：</strong> 随着业务在全球快速布局，数据平台需要在多区域实现快速部署、一致体验与弹性伸缩，同时保持技术栈的先进性与开放性。</li></ul><p>客户需要的不只是一次简单的云迁移，更是一次从底层芯片到顶层应用、旨在获得长期竞争优势的架构革新。</p><p><strong>客户挑战</strong></p><h4><strong>基于 AWS Graviton 的全栈深度优化</strong></h4><p>数新智能的解决方案核心，是将 AWS Graviton 处理器的原生优势 与 CyberEngine 数据底座的云原生能力进行深度耦合，实现从硬件到软件的全栈协同优化。</p><p><strong>数新智能的全栈实施路径</strong></p><ul><li><strong>全栈 Graviton 化：</strong> 将该客户数据平台的所有计算节点，包括 CyberData 平台应用层、CyberEngine 底座的 Spark、Flink 计算集群，以及 StarRocks 实时分析引擎，全部部署在 AWS Graviton3/Graviton4 实例上。这为整个平台奠定了高性价比的基石。</li><li><strong>云原生数据底座落地：</strong> 部署数新智能 CyberEngine云原生数据底座。该底座并非简单集成开源组件，而是针对 Graviton 环境深度优化 Spark（批处理）、Flink（流计算）与 StarRocks（实时分析）的运行时与调度策略，充分释放 ARM 架构每瓦特性能优势。</li><li><strong>智能混合调度与优化：</strong> 通过数新智能统一任务调度引擎，结合物流数据管线的特性（实时事件流、离线批量报表、即时交互查询），智能将任务分发至不同 Graviton 实例类型支撑的最优计算集群中，实现资源利用率最大化。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609880" alt="图片" title="图片"/></p><p><strong>解决方案</strong></p><p><strong>建立全链路数据血缘与质量标准</strong></p><p>我们为客户构建的架构，充分利用了 Graviton 实例家族（如计算优化型 C7g/C6g、内存优化型 R8g/R7g）的特性，形成了高效、弹性的数据处理流水线。</p><p><strong>核心 AWS 技术特性的场景化落地</strong></p><p>我们深度结合亚马逊云科技的原生服务能力，精准解决客户的业务痛点，实现技术价值最大化：</p><ul><li><strong>统一接入与实时计算：</strong> 全球物流事件流通过 统一数据集成引擎实时摄入。Flink on Graviton 集群充分发挥 Graviton 高内存带宽、低延迟的优势，对订单状态、车辆位置等流数据进行毫秒级处理与关联分析，为实时追踪看板提供支撑。</li><li><strong>批量计算与数据湖加工：</strong> 海量历史日志与事务数据存储在 Amazon S3 中。Spark on Graviton 集群执行复杂的 ETL 与数据建模任务。借助Graviton 实例高核心密度及大缓存特性（如 Graviton5 提供 5 倍于前代的 L3 缓存），大规模数据扫描与聚合作业效率显著提升。</li><li><strong>实时分析与数据服务：</strong> 处理后的聚合结果与特征数据同步至 StarRocks on Graviton 构建的实时数仓。依托 Graviton 处理器优化的单核性能与整体吞吐量，复杂多表关联查询、多维分析均实现亚秒级响应，高效赋能运营人员即席分析与决策。</li><li><strong>全局智能化治理：</strong> 统一元数据服务贯穿数据全生命周期，基于 Graviton 实例的高效计算能力，快速构建并维护全链路数据血缘与资产目录，保障数据质量与安全合规。</li></ul><p><strong>项目价值</strong></p><p>通过全栈部署 AWS Graviton 与数新智能 CyberEngine 的深度融合优化，该跨境智慧物流集团的新数据平台取得了远超预期的核心成效：</p><p><strong>成本效益显著优化</strong></p><p>整体计算成本降低 25% 以上。这得益于 Graviton 实例卓越的性价比优势，以及 CyberEngine 弹性伸缩能力对资源的精细化管控，实现成本与效率的平衡。</p><p><strong>处理性能全面跃升</strong></p><ul><li>实时计算延迟从分钟级降至秒内，精准满足全球化物流事件实时监控需求；</li><li>大型夜间批处理作业窗口时间平均缩短 30%，为业务预留更充足的分析缓冲期；</li><li>运营分析平台复杂查询响应速度提升数倍，用户决策效率与使用体验同步改善。</li></ul><p><strong>架构敏捷性与可持续性双赢</strong></p><p>云原生架构与 Graviton 的深度结合，使新区域数据平台部署周期缩短 70%，大幅提升全球业务扩张效率。同时，Graviton 的高能效特性，助力客户降低单位计算任务的碳排放，在技术创新中践行企业社会责任。</p><p>该客户的实践清晰地证明，在数据驱动决策的时代，基础设施的先进性是业务创新的关键引擎。数新智能通过将 自研的云原生数据底座CyberEngine 与 业界领先的 AWS Graviton 自研芯片 进行全栈深度集成，不仅解决了客户在成本与性能上的燃眉之急，更为其构建了面向未来的数据核心竞争力。</p><p>我们深信，真正的技术价值，在于将底层硬件的强大潜力，通过领先的软件平台转化为切实的业务成果。数新智能愿与更多的全球化企业携手，从芯片到架构，重塑数据生产力，驭“数”前行，智领全球。</p>]]></description></item><item>    <title><![CDATA[2026AI医疗行业专题报告：智能医疗器械、手术机器人、脑机接口、可穿戴设备|附240+份报告PDF]]></title>    <link>https://segmentfault.com/a/1190000047609893</link>    <guid>https://segmentfault.com/a/1190000047609893</guid>    <pubDate>2026-02-13 16:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>原文链接：<a href="https://link.segmentfault.com/?enc=OdWYc5iBcH969YpO2al61Q%3D%3D.wcD9rPYWeX9Hrc6fy7GDhaXJxZY8muqCdNGtkUoHQYg%3D" rel="nofollow" title="https://tecdat.cn/?p=44979" target="_blank">https://tecdat.cn/?p=44979</a>  <br/>原文出处：拓端抖音号@拓端tecdat</p><h2><a name="t1" target="_blank"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609895" alt="封面" title="封面"/></h2><h3><a name="t2" target="_blank"/>引言</h3><p>医疗健康行业正经历由AI与智能化技术驱动的系统性革新，手术机器人的毫米级精准操作、脑机接口的神经功能调控、可穿戴设备的全周期健康监测、AI辅助诊断的高效赋能，正从诊断、治疗、康复等全链条重构医疗服务模式。本报告洞察基于《浙商证券：医疗器械创新系列行业报告（一）：手术机器人五问五答》《国信证券：人工智能行业专题：OpenAI发布医疗健康Gpt，开启AI医疗新时代》《中国信通院：智能化医疗装备产业蓝皮书（2025年）》《华创证券：脑机接口行业：政策加码，临床加速，产业化进入关键阶段》等多份行业研究报告及数据，系统梳理全球及中国智能医疗领域的市场规模、核心赛道、技术趋势与商业化路径。</p><p>报告聚焦手术机器人、脑机接口、可穿戴医疗设备、AI医疗应用四大核心领域，深度拆解高增长背后的驱动逻辑，为创业者、投资者、医疗机构从业者、医疗器械企业从业者提供可落地的决策参考。文末<strong>240+份</strong>AI医疗与智能医疗器械行业研究报告及数据，本文完整报告数据图表和文末最新参考报告合集已分享在交流群，阅读原文查看、进群咨询，定制数据、报告和800+行业人士共同交流和成长。</p><h3><a name="t3" target="_blank"/>一、智能医疗：从技术萌芽到规模化爆发的进化之路</h3><h4><a name="t4" target="_blank"/>1.1 行业演进脉络</h4><p>医疗智能化的发展并非一蹴而就，而是经历“工具普及-数字化升级-智能化生态”的三阶进化：</p><ul><li>萌芽期（基础电子阶段）：以水银血压计、玻璃体温计为代表，首次将健康检测场景从医院延伸至家庭，但产品功能单一、数据孤立，仅能满足基础测量需求；</li><li>成长期（数字化阶段）：传感器与移动互联网技术突破，蓝牙、Wi-Fi实现健康数据实时同步，产品从“单点测量”升级为“数据记录”，为健康管理数字化奠定基础；</li><li>爆发期（智能化生态阶段）：AI、物联网、大数据技术成熟，设备升级为“数据采集-分析预警-远程协同”的综合健康终端，手术机器人、脑机接口等复杂装备从实验室走向临床，AI医疗应用渗透诊断、治疗、康复全场景，行业价值链持续拉长。</li></ul><h4><a name="t5" target="_blank"/>1.2 核心驱动因素</h4><ul><li>需求端：人口老龄化与慢病高发催生刚性需求。2024年中国65岁及以上人口达2.20亿，成人高血压患者约2.45亿、糖尿病患者1.48亿，庞大的慢病人群推动院外监测与居家治疗市场持续扩容；</li><li>政策端：“健康中国2030”“人工智能+”行动等政策持续加码，医保支付改革推动家用器械深度融入医疗服务体系，为行业规模化落地提供政策保障；</li><li>技术端：AI算法、高精度传感器、柔性电子等核心技术突破，使设备更精准、便携、智能，破解了传统医疗设备“精准度不足、场景适配性弱”的痛点。</li></ul><h4><a name="t6" target="_blank"/>1.3 核心市场规模：高增长赛道的量化图景</h4><p>中国智能医疗领域呈现“低渗透率+高成长性”的双重特征，多个细分赛道增速领跑全球：</p><h5>1.3.1 健康监测领域：存量渗透+增量创新双轮驱动</h5><p>健康监测作为居家健康管理的核心入口，涵盖血压计、血糖仪、可穿戴设备等产品。其中，全球可穿戴设备市场规模达286亿美元，中国市场规模45.3亿美元；而高血压患者家庭血压计拥有率仅45.3%，存量设备渗透空间广阔；连续血糖监测（CGM）作为创新品类，中国市场规模达17.3亿元，正推动血糖管理从“点状测量”向“连续监测”升级。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609896" alt="" title="" loading="lazy"/>  <br/>健康监测市场关键指标横向条形图表1数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：健康监测市场呈现“存量设备渗透不足，增量技术爆发”的格局，CGM等创新产品是未来增长核心。  <br/>行动建议：对创业者，可聚焦CGM等高增长细分领域，布局低成本、高精准度的产品；对医疗机构，可引入智能监测设备构建慢病管理闭环。</p><h5>1.3.2 治疗科技前沿领域：国产替代+出海加速共振</h5><ul><li>手术机器人：全球市场规模达212亿美元，中国市场72亿元，虽仅占全球5%但增速迅猛，2024-2032E CAGR约34%；</li><li>脑机接口：作为新兴赛道，全球市场规模19.8亿美元，中国17.3亿元，2023-2029E CAGR35.1%，政策加码与临床加速推动产业化进入关键阶段。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609897" alt="" title="" loading="lazy"/>  <br/>脑机接口与手术机器人市场对比灰底比例条形图表2数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国在手术机器人、脑机接口领域与全球差距逐步缩小，本土化应用潜力巨大。  <br/>行动建议：投资者可关注具备技术壁垒的国产龙头企业；医疗机构可试点引入成熟手术机器人，提升诊疗精准度。</li></ul><h5>1.3.3 行业规范化：注册数量爆发印证规模化趋势</h5><p>政策与技术的共振，推动中国智能医疗装备注册数量迎来爆发式增长。2020-2024年第三类AI医疗装备年注册数量从9项增至32项，累计上市产品超百款，与可穿戴设备20%以上的高增长率形成呼应，印证行业已进入规模化、规范化发展的快车道。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609898" alt="" title="" loading="lazy"/>  <br/>中国AI医疗装备注册数量增长折线图表3数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：AI医疗装备注册门槛逐步明晰，行业从野蛮生长转向规范发展。  <br/>行动建议：企业需加快核心产品的注册申报，抢占市场先机；监管机构可进一步优化审评流程，平衡创新与安全。</p><h5>1.3.4 出海表现：高端化拓展成效显著</h5><p>中国医疗产品出海呈现“基础品类稳增+高端设备突破”的特征。2025年1-2月巴西对中国主要医疗产品进口同比增长迅猛，维生素及衍生物增长率达88.20%，医用仪器及器具达14.50%，表明中国医疗供应链在满足新兴市场基础需求的同时，正向高附加值产品拓展。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609899" alt="" title="" loading="lazy"/>  <br/>巴西进口中国医疗产品同比增长率横向条形图表4数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国医疗产品出海呈现“原料药与高端设备双增长”特征，新兴市场需求旺盛。  <br/>行动建议：出口企业可重点布局巴西等新兴市场，优化维生素、医用仪器等优势产品的供应链；同时关注当地法规与认证要求，降低出海风险。</p><h5>1.3.5 专利布局：数量领先但全球化不足</h5><p>中国已成全球医疗健康创新核心，2019-2025年医疗健康专利占比达52.4%，但域外专利占比仅4.2%；美国PCT国际专利占比34.9%，域外专利占比35.4%，显示美国创新主体更擅长全球化专利布局，中国企业在国际专利保护上仍需加强。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609900" alt="" title="" loading="lazy"/>  <br/>中美医疗健康专利布局对比灰底比例条形图表5数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国医疗健康专利数量全球领先，但全球化布局不足，出海面临专利风险。  <br/>行动建议：企业出海前应完善目标市场专利布局，尤其是PCT国际专利申请；政府可加大对国际专利申请的资金支持与政策引导。</p><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047609901" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>专题：2025年游戏科技的AI革新研究报告：全球市场趋势研究报告|附130+份报告PDF、数据仪表盘汇总下载</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=UZXYq16Z%2FHkOB2zPnhREUA%3D%3D.j4zMHZMjw1udoblE3HDBYavSO9dk7M3XKmWt7Ey0qpo%3D" rel="nofollow" title="https://tecdat.cn/?p=44082" target="_blank">https://tecdat.cn/?p=44082</a></p><hr/><h3><a name="t8" target="_blank"/>二、核心赛道深度解析：技术突破与商业化路径</h3><h4><a name="t9" target="_blank"/>（一）AI医疗：千亿市场的场景渗透与支付逻辑</h4><p>中国医疗AI市场规模已突破千亿，其中基层CDSS（临床决策支持系统）市场规模17.41亿元，院内AI应用市场规模224.4亿元，未来十年均将保持20%以上复合高增速。</p><h5>1. 场景渗透特征：诊断优先，多场景协同</h5><p>AI技术已深度渗透智能问诊、医学图像处理、健康监测、康养养老四大场景，其中智能问诊专利渗透率67.2%，医学图像处理55.1%，成为技术应用高地。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609902" alt="" title="" loading="lazy"/>  <br/>AI医疗应用场景专利渗透率灰底比例条形图表6数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：AI在医疗场景的渗透呈现“诊断优先、多场景协同”特征，智能问诊与影像处理是核心突破口。  <br/>行动建议：软件企业可聚焦高渗透率场景迭代产品，提升算法准确率；医疗机构可先在影像科、问诊中心试点AI工具，降本增效。</p><h5>2. 企业专利布局：平台型vs传统巨头差异化竞争</h5><p>全球主要企业在专利布局上呈现分化：平安集团在医学图像、康养养老、智能问诊等多场景均占据首位，展现平台化布局野心；飞利浦、西门子等传统医疗巨头则固守健康监测、医学影像等优势领域，构筑技术护城河。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609903" alt="" title="" loading="lazy"/>  <br/>全球主要企业医疗AI专利持有量热图表7数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：头部企业专利布局分化，平台型企业全场景覆盖，传统巨头聚焦优势赛道。  <br/>行动建议：创业者可选择巨头布局薄弱的细分场景切入；投资者可重点关注全场景布局的平台型企业与细分赛道隐形冠军。</p><h5>3. 投融资趋势：中国市场复苏弹性领先</h5><p>2025年全球医疗健康一级市场温和复苏，融资总额604亿美元，融资事件数2353起；中国市场反弹强劲，融资总额96亿美元，融资事件数861起，同比大幅上涨32%，显示市场信心持续恢复。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609904" alt="" title="" loading="lazy"/>  <br/>全球及中国医疗健康投融资规模横向条形图表8数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：全球医疗健康投融资企稳回升，中国市场复苏弹性更强，资本信心回暖。  <br/>行动建议：创业者可抓住融资窗口期，重点对接关注AI医疗、手术机器人赛道的资本；投资者可加大对具备商业化能力的企业布局。</p><h5>4. 细分融资结构：技术驱动型赛道成资本焦点</h5><p>细分领域融资呈现结构性分化：生物医药依旧是吸金主力（236亿美元），数字健康赛道因AI驱动实现爆发式增长（145亿美元，+77%），器械与耗材稳健增长（130亿美元），医疗服务和医药商业则备受冷落，反映资本对技术驱动型创新的明确偏好。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609905" alt="" title="" loading="lazy"/>  <br/>全球医疗健康细分领域融资总额横向条形图表9数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：资本聚焦技术驱动型赛道，生物医药、数字健康、器械耗材成三大核心投资方向。  <br/>行动建议：企业可重点布局AI+药物研发、数字健康解决方案等资本偏好领域；医疗机构可与创新企业合作，试点新技术应用。</p><h5>5. 中国细分市场：院内+基层双引擎增长</h5><p>中国医疗AI细分市场呈现“整体千亿、细分分化”特征，基层CDSS与院内AI应用成为核心增长引擎，二者均保持20%以上复合增速。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609906" alt="" title="" loading="lazy"/>  <br/>中国医疗AI细分市场规模横向条形图表10数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国医疗AI市场已迈入千亿量级，院内应用与基层CDSS同步高增。  <br/>行动建议：企业可针对院内场景开发高精度AI工具，针对基层场景推出高性价比解决方案；政府可加大对基层CDSS的采购与推广力度。</p><h5>6. 商业化支付逻辑：B端为核心，C端待培育</h5><p>AI医疗商业化的核心在于支付方明确：药企为最强支付方（5星），因加速研发降本需求强烈；医院（4星）为提效评级有较强动力；保险机构（4星）控费需求明确但模式尚在探索；C端患者（3星）付费习惯仍需培育。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609907" alt="" title="" loading="lazy"/>  <br/>AI医疗支付方付费动力星级雷达图表11数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：B端（药企、医院、保险）是当前AI医疗商业化的核心买单方，C端市场仍需教育。  <br/>行动建议：企业可优先对接药企、医院需求，开发针对性解决方案；同时通过科普提升C端用户付费意愿。</p><h4><a name="t10" target="_blank"/>（二）手术机器人：国产替代与出海加速的双重机遇</h4><h5>1. 市场规模与增长潜力</h5><p>中国手术机器人市场2024-2032E CAGR约34%，从72亿元增长至767亿元：</p><ul><li>腔镜手术机器人：占比58%，2024-2032E CAGR29%，配置证放开与收费目录落地为核心催化；</li><li>骨科手术机器人：渗透率持续提升，2024-2032E CAGR41%，国产替代空间广阔。</li></ul><h5>2. 出海进展：国产龙头突破海外市场</h5><p>国产企业在出海方面已取得实质性突破：</p><ul><li>微创机器人：腔镜手术机器人图迈（获CE认证）全球商业化订单突破160台，覆盖40多个国家；骨科机器人鸿鹄（获中国NMPA、美国FDA、欧盟CE等认证）2025年H1全球累计订单超过55台；</li><li>精锋医疗：腔镜手术机器人MP1000（2025年3月获CE认证）、SP1000（2025年10月获CE认证），截至2025年10月末签订72台海外订单。  <br/>国产头部企业依托产品力、性价比、5G远程手术等优势，正打开海外广阔市场。</li></ul><h5>3. 盈利模式：对标海外龙头，构建“设备+耗材+服务”闭环</h5><ul><li>腔镜手术机器人：对标全球龙头直觉外科，采用“系统+耗材+服务”模式，2024年直觉外科耗材与服务占比达76%，国内企业盈利能力有望随耗材与服务占比提升而增强；</li><li>骨科手术机器人：参考史赛克等海外巨头经验，植入物创新产品与骨科机器人协同推广将形成更强成长拉动。</li></ul><h4><a name="t11" target="_blank"/>（三）脑机接口：政策+临床驱动的前沿赛道</h4><h5>1. 市场规模与产品形态</h5><ul><li>市场规模：2023年中国市场规模17.3亿元，2023-2029E CAGR35.1%，预计2029年达105亿元；</li><li>产品形态：分为侵入式、半侵入式、非侵入式，其中侵入式信号质量优势显著，是产业趋势；非侵入式因安全性高可作为补充。</li></ul><h5>2. 核心驱动因素</h5><ul><li>政策端：国家将脑机接口纳入前瞻布局的未来产业，2025年政策密集释放，北京、上海、重庆等地方出台配套政策；</li><li>临床端：2025年中国脑机接口各细分领域均取得突破性进展，阶梯医疗完成国内首例侵入式系统人体长期埋植临床试验，博睿康的脑机接口系统NEO在多中心注册临床试验中取得显著成果。</li></ul><h5>3. 应用场景：医疗为主，向非医疗延伸</h5><p>当前集中在医疗领域，覆盖肢体运动障碍诊疗、癫痫与神经发育障碍诊疗、意识与认知障碍诊疗等；未来有望向工业安全、航空航天、娱乐游戏等非医疗领域延伸。</p><h4><a name="t12" target="_blank"/>（四）可穿戴医疗设备：健康监测的大众化普及</h4><h5>1. 市场规模与增长</h5><p>中国可穿戴医疗设备市场规模持续增长，2014-2018年CAGR67.6%，2018-2023E CAGR19.8%，2023年预计达189.2亿美元。</p><h5>2. 产品分类与技术支撑</h5><ul><li>产品分类：分为监测型（心率、血压、血糖监测等）和治疗型（植入式心脏起搏器、胰岛素泵等），其中监测型设备占据最大份额；</li><li>核心技术：高精度传感器、生物信号处理、无线通讯、低功耗设计等是基础支撑，AI、机器学习、5G通信等新兴技术推动设备智能化水平快速提升。</li></ul><h5>3. 市场竞争格局</h5><p>华为、迈瑞医疗、联想健康等国内企业，以及苹果、Fitbit、Garmin等国际品牌竞争激烈，市场呈现多极化趋势：头部企业市场份额不断扩大，细分市场仍有中小企业发展空间。</p><h3><a name="t13" target="_blank"/>三、企业案例与市场格局</h3><h4><a name="t14" target="_blank"/>（一）讯飞医疗：AI医疗领军企业的业务布局与增长潜力</h4><h5>1. 业务结构：G端打底，BC端突破</h5><p>讯飞医疗业务覆盖GBC全场景：传统优势的G端业务收入占比过半，构筑基本盘；B端和C端业务增速显著更高，尤其是患者服务业务复合增速超87%，成为收入结构优化核心引擎。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609908" alt="" title="" loading="lazy"/>  <br/>讯飞医疗业务收入占比及增速双轴图表12数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：讯飞医疗G端业务稳固，BC端业务成为增长核心，收入结构持续优化。  <br/>行动建议：同行企业可参考其“G端打底、BC端突破”的业务模式；投资者可重点关注其BC端业务落地进展与盈利能力改善。</p><h5>2. 市场格局：竞争分散，新进入者有机会</h5><p>医疗AI市场集中度较低，讯飞医疗暂居榜首但市场份额仅为5.9%，大量长尾企业合计占据超过四分之三的市场，表明行业技术门槛虽高，但应用场景多样，尚未形成垄断。  </p><p>2023年中国医疗人工智能市场份额圆环图表13数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：医疗AI市场竞争分散，头部企业优势不明显，新进入者仍有机会。  <br/>行动建议：新进入者可选择细分场景深耕，打造差异化优势；头部企业可通过并购整合扩大市场份额。</p><h5>3. 财务表现：营收高增，盈利拐点临近</h5><p>受益于BC端业务快速放量，讯飞医疗营业收入保持30%左右的年增速；随着规模效应显现和运营效率提升，公司归母净利润亏损大幅收窄，券商预测其将在2026年实现净利润转正。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609909" alt="" title="" loading="lazy"/>  <br/>讯飞医疗营收与净利润折线图表14数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：讯飞医疗营收高增，净利润持续改善，盈利拐点即将到来。  <br/>行动建议：投资者可长期关注其盈利转正进度；企业可借鉴其规模化降本的运营策略，提升盈利能力。</p><h4><a name="t15" target="_blank"/>（二）技术层面：机器学习模型与算力支撑</h4><h5>1. 机器学习模型：树模型准确率领先</h5><p>在医疗预测任务中，集成树模型（如随机森林、XGBoost）表现最优，其处理非线性关系和特征交互的能力更强，为AI辅助诊断提供核心技术支撑。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609910" alt="" title="" loading="lazy"/>  <br/>机器学习模型准确率对比横向条形图表15数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：随机森林等树模型在医疗预测任务中准确率最高，是AI辅助诊断的核心算法选择。  <br/>行动建议：技术企业可优先采用随机森林等高性能模型开发产品；医疗机构在选择AI工具时，可重点关注算法类型与准确率指标。</p><h5>2. 数据与算力：产业发展的核心基石</h5><p>医疗AI模型的训练高度依赖高质量标注数据和强大算力：2025年全球医疗数据标注需求同比激增217%，同期中国企业采购特定AI芯片的金额高达160亿美元，反映行业在数据基础设施和算力储备上的巨大投入。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609911" alt="" title="" loading="lazy"/>  <br/>医疗AI数据与算力需求增长横向条形图表16数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：数据标注与算力是AI医疗发展的核心支撑，行业投入持续加码。  <br/>行动建议：企业可布局医疗数据标注服务或算力租赁业务；政府可加大对医疗数据共享平台与算力基础设施的投入。</p><h3><a name="t16" target="_blank"/>四、用户需求场景与行动清单</h3><h4><a name="t17" target="_blank"/>（一）核心用户类型与痛点关联</h4><ol><li><strong>创业者</strong>：痛点集中在“赛道选择难、技术壁垒高、商业化路径不清晰”，报告价值在于明确高增长细分赛道（CGM、手术机器人出海、脑机接口医疗应用），提供盈利模式参考（设备+耗材+服务、CRO服务）；</li><li><strong>投资者</strong>：痛点是“项目估值难、风险判断不准”，报告通过市场规模、增速、竞争格局数据，筛选出具备技术壁垒与商业化能力的企业类型（平台型手术机器人企业、上游核心零部件厂商、AI医疗头部企业）；</li><li><strong>医疗机构从业者</strong>：痛点是“设备选型难、技术落地效果不确定”，报告梳理了各细分领域成熟产品（如微创机器人图迈、精锋医疗MP1000），提供了临床应用案例与效果数据；</li><li><strong>医疗器械企业从业者</strong>：痛点是“技术迭代慢、出海受阻”，报告分析了技术发展趋势（AI+多模态融合、大小模型协同）与出海成功案例，给出专利布局与国际认证建议。</li></ol><h4><a name="t18" target="_blank"/>（二）可落地的3件事</h4><ol><li>调研本地三甲医院与基层医疗机构的设备需求差异，重点关注骨科、腔镜手术机器人的入院进展，结合收费目录政策，筛选适配的合作或投资方向；</li><li>分析所在区域慢病（高血压、糖尿病）人群分布数据，对接可穿戴设备企业，探索“设备+社区医疗+医保”的慢病管理合作模式；</li><li>跟踪脑机接口临床进展，重点关注侵入式产品的安全性与有效性数据，评估在神经康复领域的试点应用可行性。</li></ol><h4><a name="t19" target="_blank"/>（三）风险提示与应对方案</h4><ol><li><strong>政策变动风险</strong>：收费目录落地不及预期、医保报销政策调整。应对方案：密切关注医保局、药监局政策动态，选择政策支持力度大的细分领域（如基层医疗AI、国产手术机器人）；社群将实时更新政策解读，提供政策应对咨询；</li><li><strong>技术迭代风险</strong>：AI算法、传感器技术更新快，产品面临淘汰。应对方案：加大研发投入，聚焦核心技术（如AI算法优化、高精准传感器），与高校、科研机构建立合作；社群提供技术趋势周报，对接技术资源；</li><li><strong>数据安全风险</strong>：医疗数据泄露、隐私保护合规问题。应对方案：遵循《通用数据保护条例》等法规，建立数据加密与隔离机制；社群分享数据安全合规指南，对接合规咨询机构。</li></ol><h3><a name="t20" target="_blank"/>五、核心数据表格与图表列表</h3><h4><a name="t21" target="_blank"/>（一）核心数据表格</h4><table><thead><tr><th>细分领域</th><th>中国市场规模（2024/2023年）</th><th>全球市场规模（2024/2023年）</th><th>2024-2032E/2023-2029E CAGR</th><th>核心驱动因素</th></tr></thead><tbody><tr><td>手术机器人</td><td>72亿元</td><td>212亿美元</td><td>34%</td><td>收费目录落地、出海加速</td></tr><tr><td>脑机接口</td><td>17.3亿元</td><td>19.8亿美元</td><td>35.1%</td><td>政策支持、临床突破</td></tr><tr><td>可穿戴医疗设备</td><td>189.2亿美元（2023E）</td><td>286亿美元</td><td>19.8%（2018-2023E）</td><td>健康意识提升、技术创新</td></tr><tr><td>医疗AI</td><td>1000亿元</td><td>-</td><td>20%+</td><td>场景渗透、支付方明确</td></tr><tr><td>CGM</td><td>17.3亿元</td><td>-</td><td>-</td><td>糖尿病管理需求、技术升级</td></tr><tr><td>医疗健康投融资（中国）</td><td>96亿美元</td><td>604亿美元</td><td>-</td><td>市场复苏、技术驱动</td></tr></tbody></table><h4><a name="t22" target="_blank"/>（二）图表列表</h4><ol><li>健康监测市场关键指标横向条形图表1</li><li>脑机接口与手术机器人市场对比灰底比例条形图表2</li><li>中国AI医疗装备注册数量增长折线图表3</li><li>巴西进口中国医疗产品同比增长率横向条形图表4</li><li>中美医疗健康专利布局对比灰底比例条形图表5</li><li>AI医疗应用场景专利渗透率灰底比例条形图表6</li><li>全球主要企业医疗AI专利持有量热图表7</li><li>全球及中国医疗健康投融资规模横向条形图表8</li><li>全球医疗健康细分领域融资总额横向条形图表9</li><li>中国医疗AI细分市场规模横向条形图表10</li><li>AI医疗支付方付费动力星级雷达图表11</li><li>讯飞医疗业务收入占比及增速双轴图表12</li><li>2023年中国医疗人工智能市场份额圆环图表13</li><li>讯飞医疗营收与净利润折线图表14</li><li>机器学习模型准确率对比横向条形图表15</li><li>医疗AI数据与算力需求增长横向条形图表16</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609895" alt="封面" title="封面" loading="lazy"/></p><h4><a name="t23" target="_blank"/>本专题内的参考报告（PDF）目录</h4><ul><li>中国信通院：智能化医疗装备产业蓝皮书（2025年）.pdf</li><li>2026-02-12 14:28</li><li>医药生物行业从设备招投标看2026年行业投资机遇——设备拐点向上趋势明确，医疗新科技蓬勃发展.pdf</li><li>2026-02-12 14:21</li><li>医疗保健行业创新链系列——中国创新药研发景气度渐趋改善，早研产业链或显著受益.pdf</li><li>2026-02-12 14:21</li><li>医疗耗材&amp;线下药店行业深度报告——在分化中寻找确定性.pdf</li><li>2026-02-12 14:21</li><li>医疗器械创新系列行业报告（一）：手术机器人五问五答.pdf</li><li>2026-02-12 14:21</li><li>2026年医疗健康与生命科学行业职场展望Final.pdf</li><li>2026-02-11 15:32</li><li>医疗卫生行业：新冠肺炎全球风险评估-第9版.pdf</li><li>2026-02-11 15:26</li><li>创新医疗器械盘点系列（4）：肿瘤基因检测的“勇敢者游戏”（上篇）.pdf</li><li>2026-02-09 14:20</li><li>家用医疗器械专题报告（一）：健康监测&amp;呼吸治疗篇.pdf</li><li>2026-02-08 09:56</li><li>华西证券-小核酸药物行业深度研究报告：RNA精准医疗时代的崛起与挑战.pdf</li><li>2026-02-08 09:56</li><li>中国民营医疗服务：穿越寒冬，静待春生.pdf</li><li>2026-02-06 16:42</li><li>2025年全球医疗健康产业资本报告.pdf</li><li>2026-02-04 16:40</li><li>第139期-叶彦辛&amp;宋立恒-《智启 Al 新程从 FastGPT 实战到医疗模型可解释性探索》.pdf</li><li>2026-02-04 16:37</li><li>口腔医疗机构广告合规指南（2025）.pdf</li><li>2026-02-01 13:30</li><li>AI医疗行业专题报告——AI重构医疗，从场景落地到变现讨论.pdf</li><li>2026-02-01 13:27</li><li>“通往再平衡之路”系列之二：从医疗服务涨价看稳通胀路径.pdf</li><li>2026-02-01 13:26</li><li>长江证券：医疗器械出海深度（二）复盘希森美康——海外深耕，属地筑基.pdf</li><li>2026-02-01 13:25</li><li>光大证券：AI医疗行业专题报告——AI重构医疗，从场景落地到变现讨论.pdf</li><li>2026-02-01 13:25</li><li>医疗器械出海深度（二）复盘希森美康——海外深耕，属地筑基.pdf</li><li>2026-01-30 15:56</li><li>知识产权出版社：医疗健康行业2025年专利分析白皮书.pdf</li><li>2026-01-30 15:54</li><li>未来健康7：未来的医疗体系.pdf</li><li>2026-01-29 14:29</li><li>2025年智能体时代：重塑企业未来报告-医疗保健和生命科学行业.pdf</li><li>2026-01-28 16:00</li><li>华创医疗器械求索系列11：脑机接口行业：政策加码，临床加速，产业化进入关键阶段.pdf</li><li>2026-01-28 15:51</li><li>人工智能行业专题：OpenAI发布医疗健康Gpt，开启AI医疗新时代.pdf</li><li>2026-01-27 15:48</li><li>思宇MedTech：2025医疗器械BD白皮书.pdf</li><li>2026-01-25 12:39</li><li>上海社会科学院：AI医疗治理白皮书（2026版）.pdf</li><li>2026-01-24 17:41</li><li>中国生物制药、医疗设备及医用耗材出口及重点进口国市场分析.pdf</li><li>2026-01-22 12:06</li><li>医疗保障、气象服务领域“数据要素×”典型场景指引.pdf</li><li>2026-01-21 17:32</li><li>口腔医疗机构广告合规指南（2025） .pdf</li><li>2026-01-21 16:17</li><li>讯飞医疗科技-2506.HK-医疗AI领军企业，大模型技术领先，BC端场景加速落地.pdf</li><li>2026-01-21 15:37</li><li>硕远咨询：2025年中国母婴医疗服务行业市场研究报告.pdf</li><li>2026-01-19 16:57</li><li>2025年中国可穿戴医疗设备行业市场研究报告.pdf</li><li>2026-01-19 16:47</li><li>财信证券：医疗器械行业深度——时代变革下，创新与出海仍是投资主线.pdf</li><li>2026-01-15 15:33</li><li>贝恩公司：2026年全球医疗健康行业私募股权报告（英文版）.pdf</li><li>2026-01-14 16:12</li><li>医疗保障法律法规及政策汇编(2026年版）.pdf</li><li>2026-01-14 16:09</li><li>全球医药、医疗行业——2026年-关注慢病迭代，经营质量和现金流.pdf</li><li>2026-01-12 15:12</li><li>全球医药、医疗行业——2026年-关注慢病迭代，经营质量和现金流.pdf</li><li>2026-01-11 09:25</li><li>中国科技产业化促进会：2025年中国健康医疗数据要素应用案例集.pdf</li><li>2026-01-06 16:03</li><li>StartUs Insights：2026年全球医疗行业趋势研究报告（英文版）.pdf</li><li>2026-01-06 15:25</li><li>医疗科技跨年展望暨近期热点综述.pdf</li><li>2026-01-06 15:15</li><li>医疗彩超行业：临床诊断的基石与智能化升级核心.pdf</li><li>2026-01-06 15:15</li><li>2025年量子技术：健康与医疗保健领导者的战略要务报告.pdf</li><li>2026-01-03 10:50</li><li>动脉智库：2025年数字医疗年度创新白皮书.pdf</li><li>2025-12-31 15:38</li><li>段涛教授团队：2025 妇儿医疗健康科普白皮书.pdf</li><li>2025-12-30 14:53</li><li>从医疗科技到健康科技：赋能未来健康护理生态.pdf</li><li>2025-12-30 14:48</li><li>IVD体外诊断相关医疗器械行业报告——IVD国内短期承压，头部企业积极出海.pdf</li><li>2025-12-30 14:40</li><li>耐用消费产业行业研究：宠物医疗系列之一：黄金增长期叠加连锁化率提升，宠物医院板块机会在即.pdf</li><li>2025-12-29 15:52</li><li>医药行业报告：数说德国医疗医保系统，医保商保协调发展.pdf</li><li>2025-12-29 15:51</li><li>动脉智库：2025年医疗器械及供应链年度创新白皮书.pdf</li><li>2025-12-27 16:59</li><li>2025医疗人工智能产业报告：价值计量&amp;支付探索，突破医疗AI困境.pdf</li><li>2025-12-26 16:07</li><li>医药生物行业：AI医疗应用商业化加速，重视AI医疗底部机会.pdf</li><li>2025-12-26 15:59</li><li>CIC工信安全：医疗器械行业数字化转型发展报告（2025）.pdf</li><li>2025-12-25 16:53</li><li>CIC工信安全：医疗装备行业数字化转型场景图谱（2025）.pdf</li><li>2025-12-25 16:53</li><li>CIC工信安全：医疗装备行业数字化转型场景需求清单（2025）.pdf</li><li>2025-12-25 16:53</li><li>丁香园：2024医疗机构最佳雇主洞察报告.pdf</li><li>2025-12-25 16:52</li><li>动脉智库：2025年医疗服务年度创新白皮书.pdf</li><li>2025-12-25 16:43</li><li>全球医药、医疗行业——2026年医疗科技行业展望，AI提效、资本开支复苏与医疗器械政策趋稳.pdf</li><li>2025-12-24 15:30</li><li>2025年中国口腔医疗行业市场研究报告-硕远咨询.pdf</li><li>2025-12-22 15:02</li><li>蛋壳研究院：2025年医疗人工智能产业报告.pdf</li><li>2025-12-18 14:47</li><li>药品和医疗器械警戒领域的前瞻性监管情报.pdf</li><li>2025-12-16 16:22</li><li>易观分析：2025年AI精准医疗市场专题分析报告.pdf</li><li>2025-12-15 16:10</li><li>2024年中德比较视野下的中国基层医疗守门模式-一个多层次的分析框架.pdf</li><li>2025-12-14 08:43</li><li>医药魔方：2025年中国医疗器械投融资趋势与国产替代机遇报告.pdf</li><li>2025-12-14 08:30</li><li>PitchBook：2026年医疗保健展望报告（英文版）.pdf</li><li>2025-12-11 16:27</li><li>医疗实践：美国医疗系统改善女性医疗保健的500亿美元机遇.pdf</li><li>2025-12-10 16:59</li><li>商业医疗险报告三——探索受益于商业医疗险发展的细分赛道.pdf</li><li>2025-12-09 16:09</li><li>商业医疗险报告二：他山之石，辩证看待美国健康险管理医疗模式.pdf</li><li>2025-12-09 16:09</li><li>2026年医疗器械年度投资策略：支付优化，创新出海.pdf</li><li>2025-12-08 16:07</li><li>商业医疗险报告三-探索受益于商业医疗险发展的细分赛道.pdf</li><li>2025-12-07 10:18</li><li>保持领先地位——药品和医疗器械警戒领域的前瞻性监管情报.pdf</li><li>2025-12-05 16:51</li><li>国家医疗保障局：长期护理保险服务管理文书（2026年版.pdf</li><li>2025-12-05 16:51</li><li>沙利文：2025年中国医疗器械国际化现状与趋势蓝皮书.pdf</li><li>2025-11-26 15:49</li><li>2025年美国医疗服务可负担性及价值评估追踪报告.pdf</li><li>2025-11-22 16:34</li><li>全球医药、医疗行业——代谢新药研发系列（四），PCSK9Lp(a)心血管新药黄金时代.pdf</li><li>2025-11-22 16:26</li><li>医疗保健设备与服务行业——当医疗遇上AI，技术突破或重构诊疗逻辑.pdf</li><li>2025-11-15 15:03</li><li>2025未来健康指数报告：构筑医疗Al信任基石-医患双重视角下的医疗健康未来.pdf</li><li>2025-11-13 15:31</li><li>2025年智启新质生产力之三 ——生成式人工智能 （AIGC）在医疗器械 的潜在应用.pdf</li><li>2025-11-10 13:50</li><li>让GCCs适用于中端医疗技术.pdf</li><li>2025-11-10 13:40</li><li>Salesforce：2025年中国医疗健康和生命科学行业报告.pdf</li><li>2025-11-08 17:46</li><li>医疗器械专题：脑机接口行业深度专题二：三个维度看脑机接口行业发展趋势.pdf</li><li>2025-11-08 17:40</li><li>克劳锐：2025健康医疗内容消费趋势洞察报告.pdf</li><li>2025-11-07 16:31</li><li>TempusAI启示：用数据构筑AI+医疗行业领先优势-中邮证券.pdf</li><li>2025-11-05 16:40</li><li>SVB：2025年医疗科技行业未来展望报告（英文版）.pdf</li><li>2025-10-31 15:12</li><li>全球医药、医疗行业——全球健康产业进入新拐点-长期韧性显现，创新动能积聚.pdf</li><li>2025-10-28 16:18</li><li>欧盟人工智能法案如何重塑emea的医疗器械产业.pdf</li><li>2025-10-27 16:12</li><li>医疗器械海外深度（三）：中美对比，创新出海.pdf</li><li>2025-10-24 14:06</li><li>北欧可持续医疗中心：2025年可持续医疗趋势报告.pdf</li><li>2025-10-20 14:55</li><li>探索医疗保健领域的塑料循环利用机会.pdf</li><li>2025-10-20 14:53</li><li>跨越信任鸿沟：AI在科研与医疗领域深度应用的核心挑战.pdf</li><li>2025-10-18 17:14</li><li>2025中国医疗健康保障体系转型发展报告：应对老龄化挑战与推动商业健康保险创新.pdf</li><li>2025-10-17 16:00</li><li>2025年医疗服务报告：基于对30个国家的调研（英文版）.pdf</li><li>2025-10-17 15:59</li><li>医疗保健：医疗器械2025.pdf</li><li>2025-10-16 15:19</li><li>AI 时代的医疗保健业：科技注入，赋能 医疗创新与患者关怀-IBM.pdf</li><li>2025-10-14 15:25</li><li>医药生物行业专题报告：AI职能蜕变，医疗行业变革蓄势待发.pdf</li><li>2025-10-14 15:09</li><li>动脉橙：2025年9月全球医疗健康领域投融资月报.pdf</li><li>2025-10-13 09:51</li><li>西心血管疾病相关医疗器械行业报告——心血管行业空间广阔，集采助力国产替代.pdf</li><li>2025-09-30 16:37</li><li>2025年中国医疗美容市场洞察报告：轻医美如何用“生活化场景”开拓新增长极？.pdf</li><li>2025-09-29 15:55</li><li>2025年Q3医疗器械行业薪酬报告.pdf</li><li>2025-09-29 15:54</li><li>2025年Q3医疗美容行业薪酬报告.pdf</li><li>2025-09-29 15:54</li><li>2025年AI应用与行业转型：对医疗、金融服务、气候与能源及交通领域的影响报告（英文版）.pdf</li><li>2025-09-26 14:23</li><li>美国医疗行业系列研究（三）——美国药品支付体系拆解-美国高药价的成因？特朗普药价政策的影响？.pdf</li><li>2025-09-25 16:00</li><li>生物医药行业——商业医疗险报告一-见微知著，医保承压下商保或为破局之法.pdf</li><li>2025-09-25 16:00</li><li>_印孚瑟斯Infosys：2025年医疗保健市场前景报告（英文版）.pdf</li><li>2025-09-23 16:36</li><li>可负担医疗的未来：释放人工智能的潜力，以改造东南亚的卫生系统.pdf</li><li>2025-09-22 16:20</li><li>农林牧渔行业：宠物医疗空间广阔，全国连锁模式最优.pdf</li><li>2025-09-21 17:13</li><li>2025热电偶导线在医疗器械中的应用场景白皮书.pdf</li><li>2025-09-20 16:56</li><li>2025年智能医疗健康：人工智能驱动转型与价值重塑报告.pdf</li><li>2025-09-14 19:32</li><li>2025年未来医生白皮书：医疗行业持续发展的关键洞察（英文版）.pdf</li><li>2025-09-14 19:30</li><li>美国卫生与公众服务部发布 “医疗卫生行业人工智能发展战略计划”.pdf</li><li>2025-09-12 16:35</li><li>2025年未来AI与劳动力：生成式AI对医疗行业岗位的影响研究报告（英文版）.pdf</li><li>2025-09-12 16:33</li><li>医疗保健行业GLP_1受体激动剂行业深度报告：GLP_1RAs引领降糖减重市场，更多适应症有待开发.pdf</li><li>2025-09-11 15:12</li><li>浙江省基本医疗保险医疗服务项目目录（2025年）.pdf</li><li>2025-09-09 15:21</li><li>上海喜美医疗美容品牌升级规划方案.pdf</li><li>2025-09-06 19:21</li><li>ITIF：2025 AR&amp;VR在医疗领域中的应用潜力研究报告（英文版）.pdf</li><li>2025-08-27 16:52</li><li>信任與創新：提升遙距醫療管治.pdf</li><li>2025-08-26 17:02</li><li>中国医疗器械出海东南亚白皮书 - 天册律师事务所.pdf</li><li>2025-08-26 17:02</li><li>2025年信任与创新：提升遥距医疗管治研究报告（繁体版）.pdf</li><li>2025-08-21 17:01</li><li>顺为人和：2025年医疗器械标杆企业组织效能报告.pdf</li><li>2025-08-21 16:57</li><li>全球医药、医疗行业：GenAI前沿实践更新，Agent化落地成主线.pdf</li><li>2025-08-15 16:00</li><li>医疗器械行业深度（R3）：神经介入行业，大空间，新机遇.pdf</li><li>2025-08-15 15:59</li><li>2025年医疗保健预算执行-从瓶颈到解决方案报告.pdf</li><li>2025-08-14 16:55</li><li>智慧健康医疗体系概述.pdf</li><li>2025-08-14 16:48</li><li>AI医疗行业深度：驱动因素、重点方向、产业链及相关公司深度梳理.pdf</li><li>2025-08-14 16:47</li><li>医疗保健预算执行 从瓶颈到解决方案.pdf</li><li>2025-08-12 16:08</li><li>医疗保健服务公共比较表和估值指南.pdf</li><li>2025-08-12 16:07</li><li>2025年emea医疗保健市场快照：欧洲、中东和非洲地区医疗保健私营市场活动概述.pdf</li><li>2025-08-11 15:46</li><li>2025年信心与价值：提升医疗价格透明度研究报告（繁体简版）.pdf</li><li>2025-08-10 18:40</li><li>艾社康：2024-2025多层次医疗保障创新案例集.pdf</li><li>2025-08-06 16:18</li><li>2025年马来西亚医疗器械评估优化白皮书：价值导向型综合性方法（英文版）.pdf</li><li>2025-08-06 16:16</li><li>2025年AI科技勾勒医疗未来蓝图-AI for 医疗健康系列报告“智” 愈未来.pdf</li><li>2025-08-05 15:30</li><li>医疗健康大模型伦理与安全白皮书.pdf</li><li>2025-08-05 15:27</li><li>2025人工智能大模型在医疗领域发展态势研究报告.pdf</li><li>2025-08-02 16:20</li><li>中国医疗保健：银发经济崛起-高盛.pdf</li><li>2025-08-01 16:47</li><li>亿欧智库 _ 2025中国人工智能医疗健康研究报告.pdf</li><li>2025-07-29 17:10</li><li>2025年医疗耗材数字化领用白皮书-以低值耗材为切入口的AI智能仓储实践.pdf</li><li>2025-07-29 17:09</li><li>医药行业2025年中期投资策略——BD加速创新药重估，后续持续看好创新药及产业链、AI医疗、脑机接口等结构性机会.pdf</li><li>2025-07-23 16:22</li><li>2025中国宠物医疗行业现状报告-嘉世咨询.pdf</li><li>2025-07-20 20:06</li><li>2025“人工智能 ”医疗健康行业应用白皮书-阿里云.pdf</li><li>2025-07-20 20:04</li><li>医药生物行业专题报告：“AI+医疗”商业化进程有望加快.pdf</li><li>2025-07-20 19:59</li><li>健闻咨询：2025年Z世代个性化消费医疗洞察报告.pdf</li><li>2025-07-18 16:43</li><li>汇银林泰：2025高端医疗发展白皮书.pdf</li><li>2025-07-18 16:43</li><li>动脉智库：2025年H1全球医疗健康产业资本报告.pdf</li><li>2025-07-17 15:51</li><li>2025商业健康保险与医药产业高质量 协同发展——团体补充医疗保险改革新视角.pdf</li><li>2025-07-17 15:48</li><li>医药生物-AI医疗行业系列二暨GenAI系列深度之62：AI医药，智愈未来，技术变革下的生态重塑.pdf</li><li>2025-07-16 16:02</li><li>2024年塑造美国医疗经济的八大趋势研究报告（英文）.pdf</li><li>2025-07-15 16:24</li><li>医疗器械行业2025H2投资策略：国内不利因素逐渐消退，海外市场进展迅速.pdf</li><li>2025-07-15 16:23</li><li>宠物医疗行业系列2-宠物医院分散格局谋突破，连锁专科领未来.pdf</li><li>2025-07-11 15:57</li><li>2025年未来医疗调查报告（英文）.pdf</li><li>2025-07-09 16:23</li><li>2025年医疗美容行业白皮书-薪智.pdf</li><li>2025-07-07 16:50</li><li>保健品品牌 × 小红书“疗愈式营销”品效双赢【医疗保健】【医药保健】【种草营销】.pdf</li><li>2025-07-06 08:30</li><li>薪智：2025年Q2薪智医疗美容行业薪酬报告.pdf</li><li>2025-06-30 15:06</li><li>德勤：2025年中国智慧医疗行业白皮书.pdf</li><li>2025-06-28 17:12</li><li>2025年关于最有价值和最强大的制药、医疗器械和服务品牌的年度报告（英文版）.pdf</li><li>2025-06-28 17:08</li><li>香2024年优化跨境就医应对医疗需求报告（繁体版）.pdf</li><li>2025-06-28 17:03</li><li>2025年医疗保健品牌榜.pdf</li><li>2025-06-26 16:55</li><li>2025制药、医疗科技与生物技术领域AI应用 ：解锁商业成功之道（英文）.pdf</li><li>2025-06-25 16:34</li><li>2024年全球医疗科技行业状况及2025年展望报告（英文版）-Vamstar.pdf</li><li>2025-06-23 15:39</li><li>2025“面向未来的医疗”调研报告（英文）.pdf</li><li>2025-06-19 16:03</li><li>嘉世咨询：2025年宠物医疗行业简析报告.pdf</li><li>2025-06-17 15:21</li><li>荣续ESG智库：2025年医疗器械行业ESG白皮书.pdf</li><li>2025-06-16 09:49</li><li>荣续ESG智库：2025年医疗卫生行业ESG白皮书.pdf</li><li>2025-06-16 09:49</li><li>IDC：2025年医疗行业智慧文印解决方案白皮书.pdf</li><li>2025-06-14 16:43</li><li>医药生物行业深度报告：引领医疗革命，CGT成长空间广阔.pdf</li><li>2025-06-13 16:08</li><li>沙利文：2025年中国医疗器械出海现状与趋势蓝皮书.pdf</li><li>2025-06-12 15:39</li><li>阿里云：2025医疗健康行业AI应用白皮书.pdf</li><li>2025-06-11 16:38</li><li>兰州市基本医疗保障政策指南.pdf</li><li>2025-06-09 13:31</li><li>2025年易凯资本中国健康产业白皮书-医疗技术与器械篇.pdf</li><li>2025-06-07 16:42</li><li>2025年易凯资本中国健康产业白皮书-医疗与健康服务篇.pdf</li><li>2025-06-06 15:36</li><li>2025 医疗健康新质生产力 “创变引擎” 系列洞察 创新医疗科技篇.pdf</li><li>2025-06-04 16:26</li><li>智药局：2025年AI Agent+医疗行业研究报告.pdf</li><li>2025-06-02 08:58</li><li>2025年AI医疗行业发展现状、趋势、主要应用领域及相关标的分析报告.pdf</li><li>2025-05-22 15:55</li><li>医疗器械行业深度：AI医疗重构诊疗流程，效率与市场增长下的投资机会.pdf</li><li>2025-05-16 16:45</li><li>2025年人工智能与机器学习在医疗科技领域的崛起研究报告（英文版）.pdf</li><li>2025-05-13 16:24</li><li>2025年第一季度欧洲和美国远程医疗报告.pdf</li><li>2025-05-12 15:49</li><li>医疗行业分布式数据库解决方案白皮书 .pdf</li><li>2025-05-12 15:43</li><li>南京大学（高阳）：2024年健康医疗数据的确权与流通报告.pdf</li><li>2025-05-10 15:44</li><li>2025年医疗大模型研究报告-新质生产力大模型在各医疗场景的赋能实践.pdf</li><li>2025-05-09 16:27</li><li>2025年迈向全民医疗保障的中国经验研究报告（英文版）.pdf</li><li>2025-05-09 16:25</li><li>国际劳工组织（ILO）：2025年迈向全民医疗保障的中国经验研究报告.pdf</li><li>2025-05-08 15:59</li><li>罗氏医疗（梁莉）：融合创新技术团队适应医疗行业的敏捷转型之路.pdf</li><li>2025-05-03 10:35</li><li>中国LSHC生命科学与医疗行业调查报告.pdf</li><li>2025-04-30 17:14</li><li>智慧医疗专题-智慧养老整体解决方案（22页 ）.pdf</li><li>2025-04-26 14:23</li><li>艾昆纬：降低医疗科技行业的风险与干扰.pdf</li><li>2025-04-24 15:54</li><li>中国软件评测中心：2024年广东省医疗诊断、监护及治疗设备产业调研报告.pdf</li><li>2025-04-22 15:41</li><li>2024年高性能医疗器械创新发展报告-国家高性能医疗器械创新中心.pdf</li><li>2025-04-21 10:04</li><li>人工智能在医疗场景中的应用分享.pdf</li><li>2025-04-17 16:46</li><li>AI医疗专题：从AIGC角度看医药产业图谱.pdf</li><li>2025-04-17 16:36</li><li>AI 医疗：提质增效，全面赋能.pdf</li><li>2025-04-17 16:36</li><li>沙利文：2025年放疗医疗器械市场行业研究报告.pdf</li><li>2025-04-16 15:38</li><li>中国AI医疗行业白皮书：精准医疗，智能未来.pdf</li><li>2025-04-16 15:28</li><li>湖北数据集团：2025年医疗数据合规白皮书.pdf</li><li>2025-04-15 16:19</li><li>EY安永：2025年中企出海白皮书：医药和医疗器械篇.pdf</li><li>2025-04-15 16:17</li><li>医疗保健行业ESG管理策略研究报告-北京ESG研究院.pdf</li><li>2025-04-14 11:00</li><li>腾讯&amp;罗兰贝格：2025年医疗大健康行业全渠道营销报告.pdf</li><li>2025-04-12 16:40</li><li>2025年人工智能赋能医疗行业的未来白皮书：AI引领智能新征程（英文版）.pdf</li><li>2025-04-12 16:37</li><li>传媒行业GenAI系列之五十：国内云价值重估，AI游戏、AI社区、AI医疗、AI教育仍有低估.pdf</li><li>2025-04-12 16:29</li><li>浙江大学（姚畅）：2025年AI大模型如何破局传统医疗报告.pdf</li><li>2025-04-03 15:43</li><li>摩熵咨询：2025年中国AI医疗健康企业创新发展百强榜单报告.pdf</li><li>2025-04-01 15:39</li><li>2025商业健康险医药行业与医疗机构协同创新案例研究报告.pdf</li><li>2025-03-29 16:28</li><li>生物医药行业：AI心脏大模型发布，医疗AI商业化进程加速.pdf</li><li>2025-03-28 16:27</li><li>大健康医疗信息流投放.pdf</li><li>2025-03-25 15:55</li><li>罗兰贝格：2025年全球医疗器械报告-创新与效率平衡之道.pdf</li><li>2025-03-22 17:06</li><li>2024年医疗保健行业网络安全调查.pdf</li><li>2025-03-21 15:53</li><li>医疗AI专题报告（三）：设备篇：AI时代下的智能医疗设备革命.pdf</li><li>2025-03-21 15:41</li><li>手术机器人：高端医疗器械领域的“明珠”，重构现代外科手术体系.pdf</li><li>2025-03-20 14:48</li><li>2025年Q1医疗美容行业薪酬报告.pdf</li><li>2025-03-17 14:49</li><li>动脉橙：2025年2月全球医疗健康领域投融资月报.pdf</li><li>2025-03-16 17:08</li><li>医疗AI专题报告（二）：多组学篇：AI技术驱动精准诊断实现重要突破.pdf</li><li>2025-03-16 17:07</li><li>2025年企业高端健康福利调研报告-医疗保险和体检的优化之道.pdf</li><li>2025-03-15 15:37</li><li>猎聘：2025年医疗器械行业人才供需洞察报告.pdf</li><li>2025-03-14 15:50</li><li>计算机行业深度报告：AI+医疗：大模型重塑医疗生态.pdf</li><li>2025-03-13 17:05</li><li>中国生命科学与医疗行业-调研结果：2025年行业现状与展望报告.pdf</li><li>2025-03-12 15:41</li><li>中国生命科学与医疗行业-调研结果：2025年行业现状与展望报告（英文版）.pdf</li><li>2025-03-12 15:41</li><li>预训练大模型与医疗：从算法研究到应用.pdf</li><li>2025-03-11 16:25</li><li>挖掘亚太地区人工智能在医疗科技领域的价值（2025年.pdf</li><li>2025-03-10 09:29</li><li>大模型平民化开启“AI+医疗”新纪元-国联民生证券.pdf</li><li>2025-03-10 09:22</li><li>隐形眼镜品牌日抛产品抖音品牌营销策略案【医疗个护】【抖音营销】【种草营销】.pdf</li><li>2025-03-10 09:17</li><li>2025年智启原新：医药和医疗器械企业AI原生转型报告.pdf</li><li>2025-03-07 16:27</li><li>AI+医疗投资框架：AI平权赋能医疗数据价值重估.pdf</li><li>2025-03-07 16:18</li><li>知识产权出版社：医疗健康行业2024年专利分析白皮书.pdf</li><li>2025-03-05 15:21</li><li>医疗器械专题之脑机接口：中国脑机接口行业现状与展望.pdf</li><li>2025-03-05 15:13</li><li>医药生物行业深度报告：AI与医疗产业深度融合，有望为医疗带来产业变革.pdf</li><li>2025-02-27 14:48</li><li>医药生物行业报告：“AI+医疗”高景气度有望持续，创新药利好政策持续加码.pdf</li><li>2025-02-25 14:32</li><li>医疗器械专题之基因测序：分子诊断掌上明珠，四代测序开启规模化应用时代.pdf</li><li>2025-02-25 14:32</li><li>东吴证券-AI+医疗：提质增效，全面赋能.pdf</li><li>2025-02-25 14:32</li><li>AI医疗专题系列二：从DEEPSEEK的崛起看AI医疗发展方向及投资机会.pdf</li><li>2025-02-24 15:38</li><li>医疗AI专题报告-一-：制药篇：大鹏一日同风起，AI医疗启新篇.pdf</li><li>2025-02-24 15:38</li><li>医药生物行业行业深度报告：Deepseek冲击波系列报告-医疗AI赋能，大数据价值深度挖掘.pdf</li><li>2025-02-23 16:20</li><li>AI医疗行业专题报告：模型平权下的AI医疗大时代，梳理海内外AI+医疗投资机会.pdf</li><li>2025-02-23 16:13</li><li>计算机行业深度报告：DeepSeek系列报告之AI+医疗.pdf</li><li>2025-02-20 14:51</li><li>动脉橙：2025年1月全球医疗健康领域投融资月报.pdf</li><li>2025-02-20 14:47</li><li>AI+医疗行业深度：AI+医药：势不可挡，未来已至.pdf</li><li>2025-02-20 14:41</li><li>洞察宠物医疗保险市场.pdf</li><li>2025-02-18 15:44</li><li>医药生物：再论AI医疗如何选：海外映射+寻找高壁垒赛道.pdf</li><li>2025-02-18 15:35</li><li>医药生物行业：医疗器械行业全景图：发展趋势及投资机会展望.pdf</li><li>2025-02-18 15:35</li><li>国信证券-人工智能行业专题：第一大应用-海内外医疗AI梳理.pdf</li><li>2025-02-12 14:22</li><li>拉丁美洲医疗保健和生命科学部门市场准入快速指南.pdf</li><li>2025-02-10 16:20</li><li>摩熵咨询：2024年反腐整风运动下医疗及药企产业变局分析报告.pdf</li><li>2025-02-09 17:36</li><li>AON怡安智库：2025年全球医疗趋势报告.pdf</li><li>2025-02-08 15:13</li><li>医药生物行业2025年年度策略：政策拐点愈发明确，布局创新药+医疗设备+服务.pdf</li><li>2025-02-08 15:04</li></ul>]]></description></item><item>    <title><![CDATA[不共享数据，也能联合训练！UCL团队用联邦学习重塑血液形态学检查 超神经HyperAI ]]></title>    <link>https://segmentfault.com/a/1190000047609936</link>    <guid>https://segmentfault.com/a/1190000047609936</guid>    <pubDate>2026-02-13 16:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>血液形态学检查是临床诊断血液疾病的重要环节，通过观察外周血涂片（PBS）或骨髓穿刺（BMA）中的细胞形态，医生可以判断白血病、贫血、感染及遗传性血液疾病的类型。然而，这一过程不仅劳动强度大，而且高度依赖经验丰富的专业人员。尤其在低收入和中等收入国家（LMICs），技能专家稀缺，使得快速、可靠且可扩展的血液学诊断成为急需解决的问题。</p><p>近年来，人工智能和深度学习的发展为血液形态分析提供了新的解决方案。AI 模型能够自动识别不同类型的白细胞，并辅助医生进行快速诊断。研究表明，深度学习在自动化血液学诊断中具备显著潜力，但现实应用中仍面临重要挑战——模型训练对数据的依赖性极强，而临床数据通常分布在不同医院，且存在染色方法差异、成像设备差异以及少数罕见细胞类型的问题。这种数据异质性会导致模型在新机构或新患者群体中泛化能力下降。</p><p>更重要的是，医疗数据涉及患者隐私，跨机构共享数据受到严格限制。传统集中式训练方法通常需要汇集大量敏感医疗数据并依赖高性能计算资源，在很多机构难以实现。如何在保护隐私的前提下，实现多机构协作训练，成为医疗 AI 领域亟待解决的关键问题。</p><p>在此背景下，来自伦敦大学学院（UCL）计算机科学系的研究团队提出了一种用于白细胞形态分析的联邦学习框架，使各机构能够在不交换训练数据的情况下进行协同训练。利用来自多个临床站点的血液涂片，该联邦模型在保证完全数据隐私的同时，学习到稳健且域不变的特征表示。在卷积网络和基于 Transformer 的架构上的评估表明，与集中式训练相比，联邦训练在跨站点性能和对未知机构的泛化能力上表现出色。</p><p>相关研究成果以「MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis」为题，已发布预印本于 arXiv。</p><p>研究亮点：</p><ul><li>与集中式训练相比，联邦训练在跨站点性能和对未知机构的泛化能力上表现出色</li><li>该方法能够在不共享原始数据的情况下，实现跨机构模型协作训练，为资源有限的医疗环境提供了一种可行的解决方案。</li></ul><p><img width="723" height="396" referrerpolicy="no-referrer" src="/img/bVdnVGf" alt="" title=""/><br/><em>论文地址：</em>\<br/><em><a href="https://link.segmentfault.com/?enc=lYdxyQsPxmOG2%2BDdSlpbvA%3D%3D.1c%2Bbfyu%2FLHgY9IxZJ7HVFZldYhv130U8RP1%2B2piUOa0ypOHOPIUOzR2nX5tubCV3" rel="nofollow" target="_blank">https://arxiv.org/abs/2601.04121</a></em>\<br/>关注公众号，后台回复「MORPHFED」获取完整 PDF</p><h2>数据集：反映现实临床中的异质性</h2><p>本研究使用了来自多个医疗机构的血液涂片数据，确保训练数据既能覆盖不同细胞类型，又能反映现实临床中的异质性。</p><p>具体而言，研究使用了来自两个中心的独立数据集，这两个数据集包含 11 种共同细胞类型（如中性粒细胞、嗜酸性粒细胞、嗜碱性粒细胞、早幼粒细胞等），保证分类目标一致，同时保留了染色和成像的差异，用于测试联邦学习在真实异质环境下的泛化能力。</p><p>下图显示了不同客户端的类别分布情况</p><p><img width="732" height="542" referrerpolicy="no-referrer" src="/img/bVdnVGg" alt="" title="" loading="lazy"/><br/>联邦客户端中的类别分布</p><p>下图则展示了两个训练数据集中部分细胞类型的示例，可以明显观察到染色风格的差异，这正是模型需要克服的数据偏移。</p><p><img width="716" height="378" referrerpolicy="no-referrer" src="/img/bVdnVGh" alt="" title="" loading="lazy"/><br/>两个训练数据集中样本细胞类型</p><p>此外，为了独立评估模型在完全未见过机构数据上的表现，研究保留了来自巴塞罗那临床医院（Client 3）的 12,992 张图像，作为外部验证集。该数据集具有不同的成像设备、染色方法及患者群体，用于测试模型在真实跨机构场景下的泛化能力。</p><h2>两类深度学习架构和四种联邦聚合策略</h2><p>本研究采用了两类深度学习架构：</p><ul><li>ResNet-34：基于卷积神经网络（CNN）的经典架构，使用 ImageNet 预训练权重。</li><li>DINOv2-Small：基于自监督视觉Transformer（Vision Transformer, ViT），通过自监督学习捕捉图像全局特征。</li></ul><p>训练遵循统一协议：联邦模型进行了 5 轮全局通信，每轮每个客户端进行 5 个本地训练周期，总计 25 个训练周期；集中式基线模型使用 25 个训练周期，并进行 4 折交叉验证，如下图所示。数据划分为 60% 训练集、13.33% 验证集、13.33% 本地测试集和 13.33% 全局测试集；所有图像均调整为 224×224 像素，并采用保守的数据增强策略（平移 ±10%，旋转 ±5°）以保持诊断形态信息。</p><p><img width="723" height="204" referrerpolicy="no-referrer" src="/img/bVdnVGi" alt="" title="" loading="lazy"/>*(A) 联邦学习框架展示了隐私保护的协作训练过程，其中 Client 1 和 Client 2 在本地进行模型训练，参数在中央服务器进行聚合。\<br/>(B) 集中式训练范式，完全访问合并数据集，并使用 4 折交叉验证。*</p><p>两种架构均采用选择性微调：ResNet-34 冻结早期层，仅训练最后三个残差块（约 11M 参数）；DINOv2-Small 冻结前 8 个 Transformer 块（0-7），训练第 8 至 11 块（约 9M 参数）。Client 3 的数据在所有训练过程中保持隔离，仅用于评估最终模型对新机构数据的泛化能力。</p><p>在联邦学习框架中，中央服务器负责协调训练并分发全局参数，但不访问原始数据；客户端在本地训练，仅返回参数更新。</p><p>研究采用了四种联邦聚合策略：</p><ul><li>FedAvg：计算客户端参数的加权平均，对极端类别分布敏感。</li><li>FedMedian：逐坐标取中值，对异常客户端和拜占庭错误具有稳健性，但可能抑制少数类信号。</li><li>FedProx：在本地目标函数中加入近端约束，增强非IID数据下的收敛稳定性。</li><li>FedOpt：在聚合梯度上使用自适应优化（Adam），动态调整学习率以应对客户端异质性，并加快收敛。</li></ul><p>此外，为解决严重类别不平衡问题，研究结合了 Focal Loss、加权随机采样以及梯度累积策略，保证少数类细胞的训练信号不被忽略。梯度裁剪（最大范数 1.0）确保训练过程稳定收敛。</p><p>模型性能通过平衡准确率（balanced accuracy）进行评估，重点关注跨机构泛化能力，以测试模型在遇到不同成像协议和患者群体的数据时的稳健性。</p><h2>联邦训练在跨站点性能和对未知机构的泛化能力上表现出色</h2><p>为了验证联邦学习框架的有效性，研究人员分别进行了联合测试集评估和外部分布数据泛化评估。</p><p>①联合测试集评估</p><p>模型在包含两个客户端数据的联合数据集上进行评估，结果如下表所示，不同聚合方法在不同架构上的表现存在显著差异。</p><p><img width="723" height="402" referrerpolicy="no-referrer" src="/img/bVdnVGj" alt="" title="" loading="lazy"/><br/>联邦学习聚合方法在 ResNet-34 和 DINOv2-Small 架构上的性能比较，涵盖四种联邦策略</p><p>值得注意的是，FedOpt 表现出极大的波动性：在 ResNet-34 上表现极差（平衡准确率 0.3638），而在 DINOv2-S 上保持了有竞争力的性能（平衡准确率 0.5594）；相比之下，FedAvg 和 FedProx 在两种模型上表现相对稳定；FedMedian 在两种架构上表现最一致，分别达到 ResNet-34 的平衡准确率 0.5738 和 DINOv2-S 的 0.5797。</p><p>结果表明，联邦学习显著提升了性能，相比仅使用单个机构数据训练的模型（58% vs 52% 平衡准确率），证明了无需共享数据即可进行协同训练的优势。尽管联邦模型的性能略低于对所有数据进行集中训练的模型，但它们在保持完整数据隐私的同时，仍能达到可比精度。</p><p>②外部分布数据泛化评估</p><p>对来自巴塞罗那的 Client 3 外部验证数据集的评估显示，两种联邦方法（FedMedian 和 FedOpt）在完全未见过的机构数据上的泛化能力均优于集中式训练（平衡准确率 67% vs 64%），如下表。这表明，在联邦训练过程中接触到异质的机构特征（如成像设备、患者群体和染色方法）有助于模型学习更具泛化性的形态特征。</p><p><img width="723" height="528" referrerpolicy="no-referrer" src="/img/bVdnVGk" alt="" title="" loading="lazy"/><br/>Client 3 外部验证的类别级 F1 分数</p><p>FedMedian 在少数类细胞上表现出特别显著的提升：带状中性粒细胞（Band neutrophils）F1: 0.62 vs 集中式 0.30（提升 107%），早幼粒细胞（Promyelocytes）F1: 0.61 vs 0.35（提升 74%），显示在不同机构协议下诊断相关特征得到了有效保留。然而，对中幼粒细胞（Metamyelocytes）的识别对所有方法仍然具有挑战性（F1: 0.02-0.30），反映出从极其罕见类别学习稳健表征的根本困难</p><p>③架构-聚合策略相互作用规律</p><p>研究人员还进一步识别出关键的架构-聚合策略相互作用规律：FedMedian 提供跨架构稳健性，但对罕见类别不利；FedOpt 在少数类细胞信号保真上表现更好，但对架构敏感。DINOv2-S 的预训练 Transformer 架构对非IID数据分布表现出更高鲁棒性，而 ResNet-34 对梯度冲突更敏感。</p><p>总体而言，这些发现将联邦学习定位为稳健、隐私保护且具泛化能力的血液学影像分析框架。</p><h2>联邦学习成为破解医疗「数据孤岛」的关键</h2><p>联邦学习是一种面向分布式数据环境的协同机器学习范式，其核心理念是在不集中原始数据的前提下完成模型联合训练。在联邦学习框架中，各参与机构（如医院、实验室或研究中心）在本地进行模型训练，仅向中央服务器上传模型参数或梯度更新，服务器负责对这些更新进行聚合并生成全局模型，再将模型下发至各节点继续迭代训练。通过这种「数据不出域、模型可协作」的机制，联邦学习在实现跨机构知识共享的同时，能够有效保护数据隐私并满足严格的数据合规要求。</p><p>过去几年，已有不少机构在推进如何用联邦学习赋能医疗行业，典型的比如端到端人工智能生物技术公司 Owkin——该公司曾获得法国 20 家值得关注的人工智能初创企业、2023 年最值得关注的医疗和技术初创公司之一、最佳医疗技术大奖、福布斯 AI 50 强。</p><p>让 AI 技术在多模态患者数据中识别不同的生物标志物，并对患者进行亚群分类，将每类患者与最佳治疗靶点匹配，推动靶点药物研发、优化疾病诊断工具，实现真正意义上的个性化医疗，是 Owkin 公司正在走的路。而实现以上目标的关键在于——如何既能进行数据共享，又能保证患者的数据隐私？针对此，Owkin 采用联邦学习来解决。为了推动相关技术的普及，Owkin 开源了联邦学习软件 Substra ，可用于临床研究、药物研发等。\<br/>开源地址：</p><p><a href="https://link.segmentfault.com/?enc=%2F3oacSj744LInelpoTybng%3D%3D.q7dmbOHCHcy1RQMnobU%2FPP2tCofU5GL4ek9Fc1BIfdo%3D" rel="nofollow" target="_blank">https://github.com/substra</a></p><p>而在医疗影像领域，联邦学习同样被视为破解「数据孤岛」和隐私合规难题的关键技术路径。医疗影像数据高度敏感，涉及患者隐私与严格监管（如 GDPR、HIPAA 等），传统集中式训练往往面临伦理审批、法律风险和数据跨境传输限制等现实障碍。联邦学习使得不同医院能够在不共享原始影像数据的情况下联合训练模型，从而提升模型对不同设备、不同染色协议、不同患者群体的泛化能力。已有研究表明，联邦学习在放射影像、数字病理、超声影像等领域可实现接近甚至超过集中式训练的跨机构泛化性能，尤其在外部数据测试中表现出更强的鲁棒性。</p><p>从更宏观的角度看，联邦学习所代表的「分布式协同智能」模式，正在成为未来医疗 AI 规模化部署的重要基础设施。它不仅为隐私保护型医学大模型的训练提供了可行路径，也为跨机构临床决策支持系统和全球协作医学研究平台奠定了技术基础。在血液形态分析等细分领域，联邦学习有望推动 AI 从单机构实验室应用走向跨区域、跨体系的临床级智能诊断服务，为精准医学和数字化医疗提供关键支撑。</p><p>参考文献：<br/>\<br/>1.<a href="https://link.segmentfault.com/?enc=sQ8rxQ7Q1PZ7ww9d0gdXMQ%3D%3D.kBvb%2FlmM5SzR0TMsHa6QLJZEU7T2kXIr2MfCkdpoUEV5m9Tolbz8FljakWyNIb9R" rel="nofollow" target="_blank">https://arxiv.org/abs/2601.04121</a><br/>\<br/>2.<a href="https://link.segmentfault.com/?enc=d1aqQaxxoekg9fsiN438PA%3D%3D.perrXz4Fb1BKfhw0gH%2BhsdBhL0uaED6kVpt3xfItNP%2BnM6UrOA8c9vg55DP7aG5l63phHb6DWb7N4pMzO%2BNXhQ%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Lf6N7EUHlhibLNc9YXWjTQ</a><br/></p>]]></description></item><item>    <title><![CDATA[随心项目管理公众号系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047609773</link>    <guid>https://segmentfault.com/a/1190000047609773</guid>    <pubDate>2026-02-13 15:04:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>随心项目管理系统是一款专为外包团队、设计团队及项目驱动型组织打造的轻量级项目管理工具。该系统由开发者基于自身实际需求开发，旨在解决项目分类管理、进度跟踪及团队协作中的常见痛点，以低成本、免开发的方式帮助中小团队快速实现项目数字化管理。</p><p>核心定位：聚焦项目全生命周期管理，提供从项目创建、任务分配到进度查询的一站式解决方案，特别适合需要精细化流程管理的外包服务团队。</p><hr/><p>二、功能介绍</p><ol><li>项目分类管理</li></ol><ul><li>支持多维度项目分类，便于外包团队按客户、类型、优先级等维度管理项目</li><li>灵活的项目归档与检索功能，历史项目数据可追溯</li></ul><ol start="2"><li>流程查询与进度跟踪</li></ol><ul><li>可视化项目进度查询，团队成员可实时掌握项目状态</li><li>清晰的任务流转记录，减少沟通成本</li></ul><ol start="3"><li>团队协作功能</li></ol><ul><li>适配美工、UI设计师等创意团队的工作流程</li><li>支持团队成员间的任务分配与协同</li></ul><ol start="4"><li>系统配置</li></ol><ul><li>演示环境：<a href="https://link.segmentfault.com/?enc=hv%2FJffg5QdzkJPHjo%2F8fBg%3D%3D.7ro5E%2BXftk3Jg2PSXFtlFw9gD99vOYace1uLLlgQkLE%3D" rel="nofollow" target="_blank">http://project.xqzbk.top/</a>（账号：admin / 123456）</li><li>支持微信公众号端接入</li><li>基于PHP7.1开发，源码加密交付</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><p>场景类型 具体应用</p><p>外包服务团队 软件外包、设计外包项目全流程管理</p><p>创意工作室 美工、UI/UX设计项目进度管控</p><p>小型技术团队 内部项目分类与任务分配</p><p>自由职业者联盟 多人协作项目的进度同步</p><p>行业价值</p><ol><li>降低管理成本：无需自建开发团队，即买即用，节省90%以上开发投入</li><li>提升协作效率：通过系统化的项目分类和进度查询，减少50%以上的沟通时间</li><li>规范流程管理：为外包团队建立标准化的项目交付流程，提升客户满意度</li><li>快速部署上线：基于微擎平台，支持微信公众号快速接入，触达10亿+用户生态</li></ol><hr/><p>四、产品参数与购买信息</p><ul><li>交付方式：微擎系统在线交付，源码已加密</li><li>开发者信誉：5.00分（实名+企业双认证）</li></ul><hr/><p>问答环节（Q&amp;A）</p><p>Q1：这个系统适合多大的团队使用？</p><p>A：随心项目管理系统主要面向中小型外包团队、设计工作室及10-50人的项目驱动型组织。系统轻量易用，无需专职IT人员维护，特别适合没有技术开发能力但急需项目管理工具的团队。</p><p>Q2：购买后如何部署使用？</p><p>A：系统基于微擎平台交付，购买后可直接在您的微擎系统中安装。支持微信公众号接入，需确保服务器环境满足PHP7.1要求。具体部署可参考微擎官方文档或联系卖家客服。</p><p>Q3：系统数据安全性如何保障？</p><p>A：系统部署在您自己的服务器上，数据自主可控。微擎平台提供官方正品保障，开发者已通过实名认证和企业认证，信誉指数5.00分，可放心购买。</p><p>Q4：是否支持多项目管理？</p><p>A：是的，系统核心功能就是项目分类管理，支持同时管理多个项目，按客户、类型、状态等多维度分类，满足外包团队多项目并行的管理需求。</p><p>Q5：与普通项目管理工具（如Teambition、Tower）相比有什么优势？</p><p>A：随心项目管理系统深度适配外包团队工作流程，特别是针对美工、UI等创意岗位优化了项目分类和进度查询方式。同时基于微信生态，无需额外安装APP，在移动端使用更便捷，且一次购买永久使用，长期使用成本更低。</p><hr/><p>温馨提示：请勿线下交易！90%的欺诈、纠纷、资金盗取均由线下交易导致。请通过微擎官方平台完成购买，享受消费保障服务。</p>]]></description></item><item>    <title><![CDATA[飞创证书查询公众号管理系统 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047609776</link>    <guid>https://segmentfault.com/a/1190000047609776</guid>    <pubDate>2026-02-13 15:04:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>飞创证书查询系统是微擎应用市场的一款专业证书管理工具，由KaijingStudio开发，该系统是一款高度自定义的证书管理与查询解决方案，适用于各行各业，旨在帮助企业和机构实现证书信息的数字化管理、快速查询与批量生成。</p><p>系统核心优势在于"零技术门槛"——用户无需编程基础，只需填写相应信息即可自动生成证书，大幅降低证书制作与管理成本。目前已获得14家企业采用，在微擎平台拥有5.0分的信誉评分和应用评分。</p><hr/><p>二、功能介绍</p><p>核心功能模块：</p><ol><li><p>高度自定义字段系统</p><ul><li>支持根据行业需求灵活配置证书字段</li><li>可自定义证书内容结构，满足不同证书类型要求</li></ul></li></ol><ol start="2"><li><p>高度自定义模板引擎</p><ul><li>提供可视化模板设计功能</li><li>支持多种证书版式自定义，打造品牌专属证书样式</li></ul></li></ol><ol start="3"><li><p>智能查询系统</p><ul><li>自定义手机端搜索条件</li><li>用户可通过微信快速查询证书真伪与详情</li><li>支持多维度检索，提升查询效率</li></ul></li></ol><ol start="4"><li><p>批量打印模板</p><ul><li>内置批量打印功能</li><li>支持证书批量生成与打印，大幅提升工作效率</li></ul></li></ol><ol start="5"><li><p>多平台适配</p><ul><li>支持微信公众号端部署</li><li>可扩展至微信小程序、抖音小程序等多平台</li></ul></li></ol><p>技术特性：</p><ul><li>交付方式：在线交付，微擎系统一键部署</li><li>源码加密：已加密，保障系统安全性</li><li>运行环境：支持PHP7.1+</li><li>数据获取：需获取用户基本信息（昵称、头像、性别、地区）、位置信息及相册权限</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景：</p><p>场景类型 具体应用</p><p>教育培训 学员结业证书、培训合格证、在线课程证书</p><p>企业认证 员工资质证书、产品认证证书、授权经销商证书</p><p>行业协会 会员资格证书、技能等级证书、荣誉奖项证书</p><p>政府机构 行政许可证书、资质认定证书、电子证照</p><p>赛事活动 参赛证书、获奖证书、志愿者服务证书</p><p>行业价值：</p><ol><li>降本增效：替代传统纸质证书制作流程，节省设计、印刷、邮寄成本</li><li>防伪溯源：数字化证书+查询系统，有效防范证书造假</li><li>品牌升级：自定义模板打造专业证书形象，提升机构公信力</li><li>数据管理：集中化证书数据管理，支持批量操作与统计分析</li><li>用户体验：手机端即时查询，提升用户满意度与信任度</li></ol><hr/><p>四、常见问题解答（Q&amp;A）</p><p>Q1：这个系统需要技术背景才能使用吗？</p><p>A：不需要。系统设计初衷就是"简单易操作"，普通工作人员只需填写信息即可自动生成证书，无需编程或设计基础。</p><p>Q2：证书模板可以自定义设计吗？</p><p>A：完全可以。系统支持高度自定义模板，您可以根据品牌VI设计专属证书样式，包括LOGO、配色、版式等。</p><p>Q3：用户如何查询证书？</p><p>A：用户可通过微信公众号或小程序，输入指定信息（如姓名、证书编号等）进行查询。查询条件支持后台自定义设置。</p><p>Q4：是否支持批量生成证书？</p><p>A：支持。系统提供批量打印模板功能，可一次性生成大量证书，适合毕业季、培训结业等批量场景。</p><p>Q5：数据安全性如何保障？</p><p>A：系统源码已加密，部署在微擎平台，数据存储安全可靠。同时支持权限管理，确保敏感信息不被泄露。</p><p>Q6：可以部署到抖音小程序吗？</p><p>A：系统本身支持多平台扩展，具体抖音小程序定制开发需求可联系开发者KaijingStudio进行商谈。</p><p>Q7：系统对服务器有什么要求？</p><p>A：需要支持PHP7.1+的运行环境，建议使用微擎官方推荐的服务器配置以确保最佳性能。</p>]]></description></item><item>    <title><![CDATA[防伪溯源红包微信公众号系统 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047609781</link>    <guid>https://segmentfault.com/a/1190000047609781</guid>    <pubDate>2026-02-13 15:03:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>防伪溯源红包微信小程序系统是微擎平台上一款集防伪验证、产品溯源、红包营销于一体的数字化营销工具。该系统基于"一物一码"核心技术，为每个商品赋予唯一数字身份，实现"识别-互动-留存"的完整闭环。</p><p>核心定位：不仅是防伪查询工具，更是连接品牌与消费者的智能营销入口，帮助企业实现产品数字化、用户数据化、营销精准化。</p><hr/><p>二、功能介绍</p><ol><li>一物一码防伪溯源</li></ol><ul><li>唯一身份标识：为每个产品生成独一无二的二维码，绑定防伪码、溯源码、营销码</li><li>真伪即时验证：消费者扫码即可验证产品真伪，防止假冒伪劣</li><li>全生命周期追溯：展示加工原料、生产日期、流通渠道、质检报告等完整信息</li><li>安全验证码机制：配合物理防伪标签，双重保障品牌安全</li></ul><ol start="2"><li>智能红包营销系统</li></ol><ul><li>多样化奖励形式：支持现金红包、实物奖品、积分、优惠券、抽奖等多种奖励</li><li>灵活发放规则：可设置固定/随机金额、扫码即得、关注公众号领红包、首次验证奖励等</li><li>裂变营销玩法：支持邀请好友助力、拼团扫码、连续扫码任务等社交裂变模式</li><li>精准投放策略：基于LBS地理位置、用户标签、扫码次数等维度精准发放</li></ul><ol start="3"><li>数据统计与分析</li></ol><ul><li>实时数据看板：可视化展示扫码人数、时间、区域、红包领取金额等核心指标</li><li>用户画像构建：收集用户微信昵称、头像、性别、地区等信息，建立精准用户档案</li><li>渠道效果分析：追踪不同渠道、批次的扫码转化效果，优化投放策略</li><li>ROI智能核算：自动计算营销投入产出比，辅助预算决策</li></ul><ol start="4"><li>系统管理功能</li></ol><ul><li>多开模式支持：支持多商家、多活动无限多开，满足连锁品牌需求</li><li>批次管理：防伪码可定义前缀、位数，支持按批次统计管理</li><li>防窜货追踪：记录每个二维码的扫描地点，自动预警窜货行为</li><li>权限分级：支持管理员、代理商、销售商、溯源员等多角色权限设置</li><li>自定义配置：商品参数、溯源信息、页面风格、菜单等均可自定义</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><p>行业领域 典型应用 核心价值</p><p>快消行业 饮料、零食、日用品开盖/开袋扫码 提升终端动销率，促进复购</p><p>酒类行业 白酒、啤酒瓶盖二维码 防伪溯源+红包激励双效合一</p><p>美妆护肤 中小品牌洗护产品包装码 低成本获客，吸引年轻消费群体</p><p>母婴行业 奶粉罐码、纸尿裤袋码 一罐一码，增强消费者信任</p><p>农资行业 化肥、种子包装袋码 针对农村市场简化操作，扩大覆盖</p><p>建材行业 涂料、管材产品码 小工返利码，激励渠道推荐</p><p>汽配行业 润滑油、配件产品码 汽修师傅专属红包，锁定专业客群</p><p>行业价值</p><ol><li>降低营销成本：红包直达消费者，省去中间环节，费用精准可控</li><li>提升用户粘性：互动式扫码体验增强参与感，重复领取机制促进复购</li><li>沉淀数据资产：构建品牌私域流量池，为精准营销提供数据支撑</li><li>强化品牌保护：一物一码+区块链溯源，有效打击假冒伪劣</li><li>赋能渠道管理：通过扫码数据追踪货物流向，防止窜货乱价</li><li>促进社交裂变：分享领红包机制实现低成本口碑传播</li></ol><hr/><p>四、问答环节</p><p>Q1：这个系统如何确保防伪码不被复制盗刷？</p><p>A： 系统采用"一物一码"技术，每个二维码都是唯一且一次性的。首次扫码验证后即失效，后台会记录扫码时间、地点、设备等信息。若发现异常扫码行为（如短时间内同一码被多次扫描），系统会自动预警。同时支持启用"炮灰域名"功能，隐藏主域名，降低被封风险。</p><p>Q2：红包发放支持哪些形式？能否设置领取条件？</p><p>A： 支持现金红包（直达微信零钱）、积分、优惠券、实物奖品、抽奖机会等多种形式。领取条件可灵活设置，包括：扫码即得、关注公众号后领取、输入验证码领取、地理位置限制、首次验证奖励、连续扫码奖励等，满足不同营销场景需求。</p><p>Q3：系统是否支持多门店、多品牌管理？</p><p>A： 支持。系统采用多开模式设计，可无限创建不同活动、不同品牌、不同门店的独立管理后台。每个活动可单独设置红包金额、数量、有效期、适用地区等参数，数据独立统计，满足连锁品牌和集团化运营需求。</p><p>Q4：如何防止代理商或门店恶意刷单？</p><p>A： 系统内置多重风控机制：①LBS地理位置限制，超出范围无法领取；②微信个人资料区域限制；③单设备/单微信号领取次数限制；④异常扫码行为监测（如同一地点短时间内大量扫码）；⑤人工审核机制。同时支持黑名单功能，可将可疑账号加入黑名单禁止参与。</p><p>Q5：溯源信息如何录入？是否支持前端添加？</p><p>A： 支持两种方式：①管理员在后台统一录入产品溯源信息；②授权前台溯源员通过移动端扫描添加，适合生产现场实时录入。溯源信息可包括原料来源、生产批次、质检报告、物流轨迹等，消费者扫码后完整展示，增强信任感。</p><p>Q6：系统对服务器和PHP版本有什么要求？</p><p>A： 系统兼容PHP5.3至PHP7.1版本，支持所有微擎版本，无需额外安装插件。建议服务器配置：Linux系统、Nginx/Apache、MySQL5.5+，以确保高并发扫码场景的稳定性。同时支持自动和手工两种红包发放模式，适应不同运营节奏。</p><p>Q7：购买后是否提供技术支持和二次开发？</p><p>A： 作为官方正品模块，提供标准售后服务。如需深度定制（如对接ERP系统、定制特殊营销玩法、UI界面深度改造等），可联系开发者进行付费定制开发。源码已加密，但提供丰富的配置选项，一般无需改动代码即可满足常规需求。</p>]]></description></item><item>    <title><![CDATA[B2B行业平台小程序详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047609785</link>    <guid>https://segmentfault.com/a/1190000047609785</guid>    <pubDate>2026-02-13 15:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>B2B行业平台小程序是一款专为B2B业务场景打造的轻量化数字化解决方案。基于微信小程序生态开发，无需下载安装即可使用，帮助企业快速搭建集求购信息发布、产品展示、供应商管理、企业资料展示于一体的线上业务平台。该系统源码已加密，支持在线交付，适用于各类垂直行业的B2B交易撮合场景，助力企业实现业务数字化转型。</p><hr/><p>二、功能介绍</p><ol><li>求购信息管理</li></ol><ul><li>支持采购商在线发布求购需求</li><li>实时推送求购信息给匹配供应商</li><li>求购状态跟踪与历史记录查询</li></ul><ol start="2"><li>产品管理系统</li></ol><ul><li>供应商可自主上传产品信息</li><li>支持产品分类、标签、多图展示</li><li>产品上下架与库存状态管理</li></ul><ol start="3"><li>供应商信息管理</li></ol><ul><li>供应商资质审核与认证</li><li>企业信息展示与信用评级</li><li>供应商分类标签化管理</li></ul><ol start="4"><li>发布信息管理</li></ol><ul><li>信息审核机制，确保内容合规</li><li>信息置顶、推荐等运营功能</li><li>数据统计与曝光量分析</li></ul><ol start="5"><li>企业资料模块</li></ol><ul><li>企业简介、联系方式展示</li><li>企业资质证书上传</li><li>企业动态与新闻发布</li></ul><ol start="6"><li>平台运营工具</li></ol><ul><li>红包营销功能（霸榜红包）</li><li>用户收藏与分享机制</li><li>消息通知系统</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><ul><li>垂直行业B2B平台：建材、机械、电子元器件、化工原料等行业</li><li>供应链协同平台：连接上游供应商与下游采购商</li><li>产业带数字化：为产业集群提供线上展示与交易撮合服务</li><li>展会线上化：线下展会配套线上供需对接平台</li></ul><p>行业价值</p><ol><li>降低获客成本：通过小程序轻量化入口，减少企业营销投入</li><li>提升交易效率：信息实时同步，缩短供需匹配周期</li><li>数据资产沉淀：积累行业交易数据，辅助商业决策</li><li>增强客户粘性：便捷的交互体验提升用户留存率</li><li>快速部署上线：基于成熟源码，大幅降低开发周期与成本</li></ol><hr/><p>四、常见问题解答（Q&amp;A）</p><p>Q1：这款小程序是否需要额外开发？</p><p>A：系统为成品源码交付，已包含核心B2B功能模块，可直接部署使用。如需个性化定制，可联系开发者进行二次开发。</p><p>Q2：小程序是否支持多平台？</p><p>A：当前版本基于微信小程序生态开发，主要适用于微信端。如需抖音小程序等其他平台版本，可咨询定制开发服务。</p><p>Q3：源码加密是否影响二次开发？</p><p>A：源码已加密，但提供标准接口文档。基础功能配置可通过后台完成，深度定制需开发者协助。</p><p>Q4：平台如何保证交易安全？</p><p>A：系统内置信息发布审核机制，支持企业资质认证。建议结合平台担保交易或线下验货等模式，确保交易安全。</p><p>Q5：是否提供售后服务？</p><p>A：购买后可享受平台基础技术支持。建议开通微擎VIP，可获得30天无售后急速退款保障及优先技术支持。</p><p>Q6：适合什么规模的企业使用？</p><p>A：适用于中小型企业快速搭建B2B平台，也适合行业协会、产业园区构建垂直领域供需对接平台。</p><p>Q7：如何提升平台用户活跃度？</p><p>A：系统内置红包营销、信息收藏、分享裂变等功能，运营方可结合行业活动、精准推送等策略提升活跃度。</p><p>交付方式：微擎系统在线交付</p>]]></description></item><item>    <title><![CDATA[疯狂社群裂变系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047609788</link>    <guid>https://segmentfault.com/a/1190000047609788</guid>    <pubDate>2026-02-13 15:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>疯狂社群裂变是一款专注于微信公众号平台的社群营销工具，由艺霖科技开发。该系统以"红包裂变"为核心机制，通过现金奖励驱动用户主动分享传播，帮助商家实现低成本、高效率的社群引流与用户增长。</p><p>系统采用Swoole加密技术，需安装swoole_loader扩展运行。它整合了红包激励、付费入群、卡密系统、资源发放等多种变现与裂变手段，形成完整的社群运营闭环。</p><hr/><p>二、适用场景与行业价值</p><ol><li>核心适用场景</li></ol><ul><li>知识付费社群：通过付费入群+卡密系统，实现课程、资料的有偿分享</li><li>资源引流：利用网盘资源自动发放，吸引精准用户群体</li><li>活动裂变：红包激励驱动用户分享，快速扩大活动影响力</li><li>私域流量池建设：将公域流量转化为可控的社群资产</li></ul><ol start="2"><li>重点服务行业</li></ol><ul><li>教育培训：在线课程推广、学习资料分发、学员社群运营</li><li>电商零售：产品推广、优惠券发放、粉丝群维护</li><li>内容创作：自媒体涨粉、付费专栏、资源变现</li><li>本地生活：商家联盟、社区团购、同城服务推广</li></ul><ol start="3"><li>行业价值</li></ol><ul><li>降低获客成本：相比传统广告投放，红包裂变成本更低、效果更精准</li><li>提升用户粘性：通过社群运营建立长期用户关系，提高复购率</li><li>实现自动化变现：付费入群+资源发放形成无人值守的变现闭环</li><li>数据资产沉淀：自定义表单收集用户数据，构建私域流量池</li></ul><hr/><p>三、常见问题解答（Q&amp;A）</p><p>Q1：这个系统支持哪些平台？</p><p>A：目前主要支持微信公众号平台，需配合微擎系统使用。</p><p>Q2：系统运行有什么技术要求？</p><p>A：系统已进行Swoole加密，需要服务器安装swoole_loader扩展。如不会安装，可联系客服免费协助。</p><p>Q3：红包裂变是否安全合规？</p><p>A：系统提供"炮灰域名"功能，可有效保护主域名安全。但使用时仍需遵守微信平台规则，避免过度营销。</p><p>Q4：能否同时开展多个活动？</p><p>A：支持多活动管理，您可以同时运营多个裂变活动，每个活动独立配置，互不干扰。</p><p>Q5：付费入群的资金如何结算？</p><p>A：资金通过微信支付接口直接进入商户账户，系统不介入资金流转，保障资金安全。</p><p>Q6：是否支持定制化开发？</p><p>A：系统提供自定义表单等灵活配置，如需深度定制，可联系开发商艺霖科技咨询。</p>]]></description></item><item>    <title><![CDATA[外贸邮件开发信不封号攻略:避开3个坑,选对方法高效获客 旅途中的围巾_d7edGc ]]></title>    <link>https://segmentfault.com/a/1190000047609799</link>    <guid>https://segmentfault.com/a/1190000047609799</guid>    <pubDate>2026-02-13 15:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在外贸获客的诸多渠道中，电子邮件营销始终凭借成本低、覆盖面广、可沉淀客户资产的核心优势，成为企业开发海外客户的重要手段。但很多外贸人都有过这样的困扰：刚群发完开邮件发信，邮箱就被封，前期的客户名单和沟通努力全部白费。其实，群发邮件本身并非封号的根源，真正的问题在于选错了发送工具、用错了发送方式。本文就为外贸企业拆解群发邮件的避坑要点，教你如何安全、高效地做海外邮件群发。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609801" alt="图片" title="图片"/></p><p>坑1：用个人邮箱群发，直接触发平台风控<br/>Gmail、Outlook、QQ邮箱、163邮箱等个人邮箱，核心定位是个人一对一日常沟通，从产品设计上就不支持营销类邮件的大规模发送，这也是外贸人用个人邮箱群发最易被封的根本原因。<br/>当个人邮箱出现以下行为时，平台风控系统会立即预警，轻则限制发送，重则直接封号且恢复难度极高：<br/>1、短时间内向大量陌生海外邮箱发送内容高度相似的开发信；<br/>2、收件人分散在不同国家、不同域名，发送行为偏离个人沟通逻辑；<br/>3、邮件多次被海外收件人标记为垃圾邮件，负面反馈累积；<br/>4、发送列表中包含大量无效、不存在的邮箱地址，退信率过高。个人邮箱仅适合少量熟客的日常沟通，绝对不能用于外贸开发信的群发。<br/>坑2：迷信企业邮箱，忽略其核心定位局限<br/>不少外贸企业为了规避个人邮箱的问题，使用企业邮箱后便直接用来群发邮件开发信，结果依然遭遇封号、送达率暴跌的问题，甚至影响公司正常业务沟通。这是因为企业邮箱的设计目标是企业日常业务沟通，而非规模化营销群发，其天然存在三大短板：</p><ol><li>有严格的单日发送量限制，无法满足外贸批量开发的需求；</li><li>高频群发会快速拉低企业邮箱的IP信誉，导致后续邮件易被归为垃圾邮件；</li><li>单一个IP/域名的信誉受损，会牵连公司其他正常使用的业务邮箱，影响日常办公、客户对接。当发送规模达到每天数千甚至上万封时，企业邮箱的风控风险会急剧上升，完全无法适配外贸规模化获客的需求。企业邮箱适合公司内部沟通、熟客对接，而非外贸冷启动的批量邮件开发信发送。<br/>坑3：忽视工具专业性，用“通用工具”做“专业事”<br/>无论是个人邮箱还是企业邮箱，封号的核心问题都是工具与使用场景不匹配。如果外贸企业已经进入持续、高频、规模化的邮件获客阶段，使用专业的外贸邮件群发工具，是规避封号风险、提升投递效率的唯一可行方案。以深耕邮件营销领域的U-Mail邮件群发平台为例，其核心设计逻辑围绕外贸行业的群发需求打造，从根源上解决了封号和送达率问题，这也是专业工具与普通邮箱的核心区别。<br/>选对专业工具，实现群发“零封号+高送达”专业的外贸邮件群发工具，并非简单的“批量发送”，而是通过技术手段实现合规发送、智能投递、信誉维护，以U-Mail为例，其核心优势体现在这5点：</li><li>专用通道+高信誉IP，从工具层面规避封号只要邮件内容正规合法、符合外贸沟通场景，U-Mail不会因发送量大而封号。平台为外贸客户配备专用群发通道，搭建了专属的投递风控模型，同时持续维护海内外高信誉IP资源，将封号风险完全控制在工具层面，无需用户承担账号被封的损失。</li><li>超大发送量，适配外贸规模化获客U-Mail单日最高可发送30万封邮件，完美匹配外贸企业的批量开发需求，无论是新客户开发信的全域投放、展会后客户的集中跟进，还是多国家、多市场的同步布局，都能轻松承接；同时采用一对一独立投递机制，每封邮件单独发送，有效降低被海外邮箱系统识别为垃圾邮件的概率。</li><li>海内外分离通道，针对性提升送达率外贸邮件的收件人遍布全球，不同国家、不同邮箱系统（Gmail、Outlook、Yahoo等）的投递规则差异极大。U-Mail打造海内外分离的专用发送通道，针对主流海外邮箱做了专属的投递优化，搭配独立维护的海外高信誉通道资源，大幅提升邮件的实际进箱率，避免邮件石沉大海。</li><li>自动化运营，节省团队时间成本平台支持定时发送、批量任务自动执行，外贸团队只需提前准备好邮件内容和客户名单，设置好发送时间，系统即可全程自动化完成投递，无需人工值守，让团队把精力集中在客户跟进和转化上，提升整体工作效率。</li><li>全维度数据追踪+免费邮箱清洗，从源头优化发送效果U-Mail提供完整的数据统计分析功能，邮件的送达情况、打开率、点击量、退信原因、无效地址等数据一目了然，帮助外贸团队快速优化邮件内容和发送策略；同时支持免费清洗无效邮箱地址，从源头降低退信率，进一步维护发送信誉，提升后续邮件的投递稳定性。<br/>补充：海外邮件群发工具怎么选？<br/>除了U-Mail，市场上也有一些海外邮件群发平台，外贸企业可根据自身业务规模和需求选择，以下是几款主流工具的核心特点，供大家参考：<br/>Mailchimp：品牌知名度高，功能成熟，适合做内容型邮件营销，但对海外冷邮件、外贸开发信的审核和限制较多，不太适配外贸冷启动场景；<br/>Brevo（原Sendinblue）：价格亲民，操作门槛低，适合中小规模的营销邮件发送，单日发送量有限，适合客群相对固定的外贸企业；<br/>GetResponse：自动化营销流程完善，适合做长期的客户生命周期运营，对开发信的适配性一般，更适合有成熟内容体系的外贸企业。<br/>核心提醒：多数海外平台对“冷邮件”“外贸邮件开发信”的审核规则严格，部分平台甚至禁止此类邮件发送，使用前务必充分了解平台规则，避免因违规导致账号被封。外贸邮件群发被封号，本质上是工具与发送场景不匹配的结果，选对工具，才能从根源上解决问题。<br/>1、少量熟客沟通、日常业务对接：个人邮箱/企业邮箱完全够用；<br/>2、批量海外客户开发、规模化获客：必须使用专业的外贸邮件群发平台。对于已经进入规模化外贸获客阶段的企业而言，选择U-Mail邮件群发平台，不仅能彻底规避封号风险，更能通过专用通道、智能投递、数据化运营，实现邮件营销的提效、提质、提转化，让海外客户开发更高效。</li></ol>]]></description></item><item>    <title><![CDATA[讯飞星辰免费Glm5计划及OpenClaw配置全指南 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047609505</link>    <guid>https://segmentfault.com/a/1190000047609505</guid>    <pubDate>2026-02-13 14:08:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用OpenClaw的过程中，很多用户都会面临Token消耗过快的问题，目前各大厂商纷纷推出相关Code Plan计划以优化Token使用体验，但多数需要付费购买。对于追求低成本、高实用性的用户而言，一款免费且可用的Token供应方案就显得尤为重要。本文将详细介绍讯飞星辰推出的春节免费Token计划，以及如何将其配置到OpenClaw中，帮助大家零成本畅享大模型应用。</p><p>一、讯飞星辰免费Token计划核心优势</p><p>讯飞星辰MaaS平台推出的春节免费Token计划，是目前验证可行的免费Token解决方案，核心优势集中在以下几点，适配OpenClaw用户的实际需求：</p><ul><li>完全免费：经实际测试，该计划真正实现0元获取Token，无需支付任何费用即可使用，有效解决OpenClaw Token消耗过快、成本过高的痛点。</li><li>适配性强：明确支持OpenClaw运行，无需额外修改工具核心设置，配置流程简单，新手也能快速上手。</li><li>官方合规：Token供应来自讯飞星辰官方平台，稳定性有保障，无需担心非正规渠道Token带来的账号安全或使用异常问题。</li></ul><p>补充说明：官方宣传该Token使用无速度限制，但实际使用过程中会存在轻微卡顿，整体流畅度可满足日常使用需求，属于可接受范围。</p><p>二、前期准备：获取讯飞星辰Token及相关授权</p><p>在进行OpenClaw配置前，需先前往讯飞星辰MaaS平台获取模型API授权、API Key等关键信息，具体步骤如下：</p><ol><li>访问讯飞星辰MaaS平台官方地址：<a href="https://link.segmentfault.com/?enc=iXqPgdeB2dpYSazH4tFZBg%3D%3D.tpWuL9tOOBipiFWAswntpiabnAVkmnHgBo6OCl7L5Xg%3D" rel="nofollow" target="_blank">https://maas.xfyun.cn/</a>，完成平台注册及登录（若已有账号可直接登录）。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609507" alt="" title=""/></p><ol start="2"><li>进入平台模型集市：访问<a href="https://link.segmentfault.com/?enc=QVuy4hOYMHKrdCK%2Bie7Xjw%3D%3D.fg%2BkQyzfxeo3Hy9MUnXc6pWuvhY0RzuoEAoN0IaO7rcOHaG0qALCboGVOiaVNJws" rel="nofollow" target="_blank">https://maas.xfyun.cn/modelSquare</a>，找到对应模型卡片，点击“API调用”按钮，即可获取模型API授权及现金礼品卡（礼品卡可用于后续相关服务拓展，非配置必需）。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609508" alt="" title="" loading="lazy"/></p><ol start="3"><li>获取核心配置信息：登录后进入推理服务控制台（地址：<a href="https://link.segmentfault.com/?enc=sOgblZMhL05YJ%2F21ugnmsw%3D%3D.8LRsJP50LqqmnI7oxDtIAehhv9ufS3T89w8hub%2BokZNAQZRhlTf3lhOakrFBhdm2" rel="nofollow" target="_blank">https://maas.xfyun.cn/modelService</a>），该页面将展示所有模型服务配置所需的关键信息，其中开发者专属API Key是后续配置的核心，需重点记录。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609509" alt="" title="" loading="lazy"/></p><p>三、OpenClaw详细配置步骤（附可直接复制模板）</p><p>讯飞星辰MaaS平台提供OpenAI兼容的接口形态，因此在OpenClaw中可直接按“OpenAI / OpenAI-Compatible”模式配置，具体操作如下，全程无需修改复杂参数：</p><p>步骤1：找到OpenClaw配置文件</p><p>打开OpenClaw工具，定位到配置文件（通常可在工具设置中找到“配置文件”入口，或按工具指引找到对应文件路径），将以下模板复制粘贴到配置文件中，替换原有相关配置（若配置文件为空，可直接粘贴）。</p><p>步骤2：填充核心配置信息</p><p>将前期在讯飞星辰推理服务控制台获取的API Key，替换模板中“YOUR_API_KEY”位置，其余参数无需修改（模板已包含常用模型及最优基础配置）。</p><p>可直接复制的配置模板</p><p>{<br/>  "meta": {</p><pre><code>"lastTouchedVersion": "2026.2.1",
"lastTouchedAt": "2026-02-04T12:14:10.945Z"</code></pre><p>},<br/>  "models": {</p><pre><code>"mode": "merge",
"providers": {
  "ds": {
    "baseUrl": "https://maas-api.cn-huabei-1.xf-yun.com/v2",
    "apiKey": "YOUR_API_KEY",
    "api": "openai-completions",
    "models": [
      {
        "id": "xopdeepseekv32",
        "name": "DeepSeek-V3.2",
        "reasoning": false,
        "input": [
          "text"
        ],
        "cost": {
          "input": 0.0025,
          "output": 0.01,
          "cacheRead": 0,
          "cacheWrite": 0
        },
        "contextWindow": 32768,
        "maxTokens": 32768
      }
    ]
  }
}</code></pre><p>},<br/>  "agents": {</p><pre><code>"defaults": {
  "model": {
    "primary": "ds/xopdeepseekv32"
  },
  "models": {
    "ds/xopdeepseekv32": {
      "alias": "xopdeepseekv32"
    }
  },
  "compaction": {
    "mode": "safeguard"
  },
  "maxConcurrent": 4,
  "subagents": {
    "maxConcurrent": 8
  }
}</code></pre><p>},<br/>  "messages": {</p><pre><code>"ackReactionScope": "group-mentions"</code></pre><p>},<br/>  "commands": {</p><pre><code>"native": "auto",
"nativeSkills": "auto"</code></pre><p>},<br/>  "channels": {<br/>  },<br/>  "gateway": {</p><pre><code>"mode": "local",
"tailscale": {
  "mode": "off"
}</code></pre><p>},<br/>  "plugins": {</p><pre><code>"entries": {
},
"installs": {
}</code></pre><p>}<br/>}</p><p>步骤3：验证配置是否成功</p><p>配置完成后，保存配置文件并重启OpenClaw（部分版本无需重启，直接生效）。在工具的聊天窗口中发送一条简单的测试消息（如“你好”），若能正常收到返回结果，即表示已成功调用讯飞星辰MaaS平台的模型服务，Token可正常使用。</p><p>四、常见问题及补充说明</p><ol><li>配置后无法正常使用？</li></ol><p>优先检查API Key是否填写正确（注意大小写、空格），若API Key无误，可重新登录讯飞星辰平台确认API授权是否有效，或刷新推理服务控制台后重新获取API Key再次配置。</p><ol start="2"><li>使用过程中速度过慢？</li></ol><p>如前文所述，实际使用中会存在轻微卡顿，属于正常现象，不影响日常使用；若卡顿严重，可检查网络连接，或重启OpenClaw及网络设备尝试优化。</p><ol start="3"><li>免费Token有使用期限或额度限制吗？</li></ol><p>该计划为讯飞星辰春节专属免费活动，具体使用期限及额度以平台官方通知为准，建议获取Token后及时配置使用，避免过期。</p><p>五、后续支持</p><p>若在配置过程中遇到其他问题，无法独立解决，可通过以下方式获取协助：点赞、关注本文，转发一次并在评论区留言“666”，即可获取手把手配置指导，全程协助完成所有操作，确保顺利使用免费Token畅享OpenClaw服务。</p><p>综上，讯飞星辰春节免费Token计划是OpenClaw用户的高性价比选择，零成本、易配置、可实用，无需额外付费即可解决Token消耗过快的问题，适合各类OpenClaw用户尝试使用。按照本文步骤操作，即可快速完成配置，开启流畅的大模型应用体验。</p><p>本公众号提供有偿搭建 openclaw 和 opencode 等服务，并提供免费AI 模型 token方案，让大家可以畅快使用，免费续杯。</p><p>有需要的加V mapleCx330</p><p>本文由<a href="https://link.segmentfault.com/?enc=I61LKVTVFJN%2FAvETBTZEvw%3D%3D.ObAL4k2Ihkd41k4%2Bb06K97ylRQcbEoddIWX%2FS0usmfY%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Prompt caching 技术是如何实现 1 折的推理成本优化的？ Baihai_IDP ]]></title>    <link>https://segmentfault.com/a/1190000047609587</link>    <guid>https://segmentfault.com/a/1190000047609587</guid>    <pubDate>2026-02-13 14:07:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p><strong>编者按：</strong> 你是否曾好奇过，那些声称能将长文本输入成本降低90%、延迟减少85%的"Prompt Caching"技术，背后究竟缓存了什么？是简单的文本复用，还是某种更深层的计算优化？</p><p>我们今天为大家带来的文章，作者的核心观点是：Prompt Caching的本质并非简单的文本字符串缓存，而是对Transformer注意力机制中Key-Value（KV）矩阵计算结果的复用，通过避免重复计算注意力权重来实现成本削减与性能提升。</p><p>文章的重点内容包括：第一，从Tokenizer到Embedding再到Transformer的完整技术拆解，帮助读者建立对LLM内部数据流的直觉认知；第二，对注意力机制（Attention）的数学原理进行深入浅出的阐释，详细展示了Query、Key、Value矩阵的计算过程以及Softmax权重分配机制；第三，揭示了"KV Caching"的核心实现逻辑 —— 通过缓存历史token的K、V投影矩阵，使模型在增量生成时只需计算最新token，而非重新处理整个上下文；第四，对OpenAI与Anthropic两种缓存策略的对比分析，指出自动路由与显式控制之间的权衡，以及Temperature等采样参数对缓存机制的零影响。</p></blockquote><p><strong>作者 | Sam Rose</strong></p><p><strong>编译 | 岳扬</strong></p><p>撰写本文时，OpenAI 和 Anthropic 的 API 中，缓存的 input token 单价仅为普通 input token 的十分之一。</p><p>Anthropic 甚至声称[1]，prompt caching 能将长 prompt 的延迟“最高降低 85%”。而在实际测试中，我发现对于足够长的 prompt，这一说法确实成立。我向 Anthropic 和 OpenAI 各发送了数百次请求，注意到在所有 input token 均被缓存的情况下，首 token 延迟（time-to-first-token latency）出现了明显下降。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609589" alt="" title=""/></p><p>缓存 token（cached token）到底是什么玩意儿？ </p><p>这背后究竟发生了什么，让服务商能给 input token 打出 1 折的超低折扣？他们在各次请求之间到底保存了什么？这可不是简单地把响应结果存下来，等收到相同 prompt 时再复用 —— 通过 API 就能很容易地验证这一点并未发生。<strong>随便写个 prompt，连续发送十几次，你会发现即使使用情况栏（usage 部分）显示 input token 已被缓存，每次得到的回复仍然各不相同。</strong></p><p>我对大模型厂商文档中的解释[2-3]并不满意 —— 它们虽能很好地说明如何使用 prompt caching，却巧妙地避开了“究竟缓存了什么”这个核心问题。于是我决定深入探究，一头扎进 LLM 工作原理的“兔子洞”，直到彻底搞明白服务商究竟缓存了哪些精确的数据、这些数据的用途，以及它们如何让每个人的 LLM 请求都变得更快速、更便宜。</p><p>读完本文，你将……</p><ul><li>在更深层次上理解 LLM 的工作原理</li><li>对“LLM 的运作方式”建立新的直觉认知</li><li>弄明白究竟哪些二进制数据被缓存了，以及它们如何降低你的 LLM 请求成本</li></ul><h2><strong>01 LLM 架构</strong></h2><p>本质上，LLM 就是一个巨大的数学函数：输入一串数字，并输出一个数字。在 LLM 内部，存在着一个由数十亿个精心设计的运算构成的巨型图结构，负责将这些输入数字转化为输出数字。</p><p>这个由海量数学运算构成的巨型图结构大致可分为 4 个部分。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609590" alt="" title="" loading="lazy"/></p><p><strong>图中的每个节点都可以看作一个函数，接收输入并产生输出。输入会以循环方式不断馈入 LLM，直到遇到某个特殊的输出值指示其停止。</strong> 用伪代码表示大致如下：</p><pre><code>prompt ="What is the meaning of life?";

tokens = tokenizer(prompt);
while(true){
 embeddings = embed(tokens);
for([attention, feedforward] of transformers){
 embeddings = attention(embeddings);
 embeddings = feedforward(embeddings);
}
 output_token = output(embeddings);
if(output_token === END_TOKEN){
break;
}
 tokens.push(output_token);
}

print(decode(tokens));</code></pre><p>尽管以上描述已大幅简化，但现代 LLM 的核心代码行数之少仍让我感到意外。  </p><p>Sebastian Raschka[4] 用 PyTorch 从零实现了多个开源模型，还产出了大量高质量的教学材料 —— 如果你喜欢本文，大概率也会喜欢他的内容。以当前领先的开源模型之一 Olmo 3 为例，其核心代码仅数百行[5]。</p><p>Prompt caching 发生在 Transformer 的“attention（注意力机制）”中。接下来我们将按顺序逐步拆解 LLM 的工作原理，直到抵达这一环节。这意味着，我们的旅程得从 tokens 说起。</p><h2><strong>02 Tokenizer（分词器）</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609591" alt="" title="" loading="lazy"/></p><p>在 LLM 处理你的 prompt（提示词）之前，必须先将其转换为它能理解的表示形式。这个过程分为两步，由 tokenizer 和 embedding 共同完成。为什么要这么做，要到讲 embedding 时才能完全明晰，现在请先耐心了解 tokenizer 的作用。</p><p>Tokenizer 会将你的 prompt 拆成多个小片段，并为每个唯一的片段分配一个整数 ID，称为"token"。例如，GPT-5 对 prompt "Check out ngrok.ai" 的分词结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609592" alt="" title="" loading="lazy"/></p><p>该 prompt 已被拆分为数组 [“Check”, " out", " ng", “rok”, “.ai”]，并转换为 tokens [4383, 842, 1657, 17690, 75584]。相同的 prompt 始终生成相同的 tokens。tokens 也是区分大小写的 —— 因为大小写能传递语义信息。例如，首字母大写的 "Will" 更可能是人名，而小写的 "will" 则更可能是助动词。</p><p>为什么不直接按空格或字符分割？</p><p>这其实是个相当深刻的问题，细讲起来足以让本文篇幅翻倍。简短而不尽兴的答案是：这是一种权衡。若想深入理解，Andrej Karpathy 有一期从零实现 tokenizer 的精彩视频（<a href="https://link.segmentfault.com/?enc=KM%2FJYcshRUNZOdRqnP%2FD2w%3D%3D.GNMb1EZBQNh1TizSj3tudh5u8rSekPrLOBmJbcNqDWMOe%2F4t8%2BI3Ue684DGWro%2BY" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=zduSFxRajkE</a>） 。<strong>对于 prompt caching 而言，只需知道：tokenization 的作用就是把文本变成数字。</strong></p><p>Tokens 是 LLM 输入与输出的基本单位。当你向 ChatGPT 提问时，回复会随着每次 LLM 迭代完成而逐个 token 流式返回。服务商这么做，是因为生成完整回复可能需要数十秒，而一旦 token 生成就立即返回，能让交互体验更流畅自然。</p><p>我们来问一个 LLM 领域的经典问题，亲眼看看这个过程：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609593" alt="" title="" loading="lazy"/></p><p>Prompt tokens 输入，✨ AI 魔法发生 ✨，输出一个 token，循环往复。这个过程称为“inference（推理）”。注意：每个输出 token 都会在下一轮迭代前被追加到 input prompt 中。LLM 需要全部上下文才能给出高质量回答 —— 如果只输入原始 prompt，它会反复尝试生成答案的第一个 token。如果只输入已生成的回答部分，它会立刻忘记问题本身。因此，每一轮迭代都必须将完整的 prompt 加上已生成的回答内容重新输入 LLM。</p><p><strong>那个 199999 &lt;END&gt; token 是什么？</strong></p><p>这个推理过程总得有个终点。<strong>LLM 拥有多种“特殊”token，其中之一就是标志着响应结束的 token。</strong> 在 GPT-5 的分词器中，这就是 token 199999。这只是 LLM 终止生成过程的多种方式之一：<strong>你也可以通过 API 指定最大生成 token 数，服务商还可能基于安全策略设定其他终止规则。</strong></p><p>此外还有用于标记对话消息起止的特殊 token —— 正是这些 token 让 ChatGPT、Claude 等聊天模型能分辨一条消息何时结束、下一条何时开始。</p><p>关于 tokenizer（分词器）的最后一点：它们种类繁多！ChatGPT 使用的 tokenizer 与 Claude 不同，甚至 OpenAI 自家的不同模型也使用不同的 tokenizer。每种 tokenizer 都有自己独特的文本切分规则。如果你想直观比较不同 tokenizer 的分词效果，可以试试 tiktokenizer[6]。</p><p>认识了 tokens 之后，接下来我们聊聊 embeddings。</p><h2><strong>03 Embedding</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609594" alt="" title="" loading="lazy"/></p><p>经过 tokenizer 处理后的 tokens，现在进入 embedding 阶段。要理解 embedding，不妨先思考模型的目标是什么。</p><p>人类用代码解决问题时，会编写接收输入、产生输出的函数，比如华氏转摄氏：</p><pre><code>function fahrenheitToCelsius(fahrenheit){
return((fahrenheit -32)*5)/9;
}</code></pre><p>我们可以把任意数字传入 fahrenheitToCelsius，并能获得正确结果。但假如我们面对一个问题，却不知道背后的公式呢？假如我们只有下面这张神秘的输入-输出对照表：  </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609595" alt="" title="" loading="lazy"/></p><p>（我并不指望你能认出这个函数 —— 不过，如果你把截图贴进 ChatGPT，它能立刻识别出来。）</p><p>当我们知道每个输入对应的正确输出，却不知道产生这种对应关系的函数时，就可以“训练”一个模型来学习这个函数。做法是：给模型提供一块“画布” —— 那个由海量数学运算构成的巨型图结构，然后不断调整这个图结构，直到模型收敛到正确的函数。每次更新图结构后，我们都将输入数据喂进去，观察输出数据与目标的差距。反复迭代，直到结果足够接近目标。这就是训练的本质。</p><p>事实证明，在训练文本生成模型时，能够识别两个句子是否“相似”会很有帮助。但“相似”具体指什么？它们可能同样悲伤、幽默或发人深省；也可能在长度、节奏、语气、语言、词汇或结构上相近。描述句子相似性的方式有无数维度，而两个句子可能在某些维度上相似，在另一些维度上则不然。</p><p><strong>Tokens 本身只是简单的整数编号，没有任何“维度”信息；而 embeddings 则是高维向量，承载了丰富的语义和结构信息。</strong></p><p>Embedding 是一个长度为 n 的数组，代表 n 维空间中的一个位置。如果 n=3，embedding 可能是 [10, 4, 2]，表示三维空间中 x=10、y=4、z=2 的坐标点。在 LLM 训练过程中，每个 token 会被随机分配一个起始位置，随后训练过程会不断微调所有 token 的位置，直到找到能产生最佳输出的排列方式。</p><p>Embedding 阶段的第一步，就是查表获取每个 token 对应的 embedding。用伪代码表示大概是这样：</p><pre><code>// Created during training, never changes during inference.
const EMBEDDINGS = [...];
 
function embed(tokens) {
 return tokens.map(token =&gt; {
 return EMBEDDINGS[token];
 });
}</code></pre><p>于是，我们把 tokens（整数数组）转换成了 embeddings（数组的数组，即“矩阵”）。  </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609596" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609597" alt="" title="" loading="lazy"/></p><p>tokens [75, 305, 284, 887] 被转换为一个由 3 维 embeddings 构成的矩阵。</p><p><strong>Embedding 的维度越多，模型可用于比较句子的“角度”就越多。</strong> 我们刚才一直在用 3 维 embeddings 举例，但当前主流模型的 embedding 维度通常是几千维，最大的甚至超过 10,000 维。</p><p>为了说明更高维度的价值，下面我展示了 8 组彩色形状，它们最初位于一维空间中 —— 挤在一条直线上，杂乱无章，难以理解。但随着维度增加，你就能清楚地看到存在 8 个不同的、相关的组别。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609598" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609599" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609600" alt="" title="" loading="lazy"/></p><p>三维是我这里能提供的视觉示例的极限，至于几千维的空间能表达什么，就得靠你发挥想象力了。</p><p>Embedding 阶段还有最后一件事要做。<strong>在获取 token 的 embedding 后，会将该 token 在 prompt 中的位置信息编码进 embedding 中。</strong> 我没有深入研究这一机制的具体实现方式，只知道它对 prompt caching 的工作方式影响不大，但如果没有这一步，LLM 就无法判断 prompt 中 tokens 的先后顺序。</p><p>更新一下前面的伪代码，假设存在一个叫 encodePosition 的函数，它接收 embeddings 和位置信息，并返回嵌入了位置编码的新 embeddings。</p><pre><code>const EMBEDDINGS =[...];
 
// Input: array of tokens (integers)
function embed(tokens){
// Output: array of n-dimensional embedding arrays
return tokens.map((token, i)=&gt;{
 const embeddings = EMBEDDINGS[token];
return encodePosition(embeddings, i);
});
}</code></pre><p>总而言之，embeddings 是 n 维空间中的点，你可以将其视为它们所代表文本的语义含义。<strong>在训练过程中，每个 token 都会在该空间中移动，靠近其他语义相似的 token。维度越多，LLM 对每个 token 的表示就越复杂、越细腻。</strong>  </p><p>至此，tokenizer 和 embedding 阶段所做的全部工作，都是为了把原始文本转换成 LLM 能处理的形式。接下来，我们来看看这些数据进入 transformer 阶段后会发生什么。</p><h2><strong>04 Transformer</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609601" alt="" title="" loading="lazy"/></p><p>Transformer 阶段的核心任务，就是接收 embeddings 作为输入，并在 n 维空间中对它们进行调整。它通过两种方式实现这一点，而我们只关注第一种：attention（注意力机制）。我们暂不讨论 “Feedforward” 层或输出阶段（至少在这篇文章中👀）。</p><p><strong>Attention 机制的作用，是帮助 LLM 理解 prompt 中各个 token 之间的关系 —— 具体做法是让每个 token 能够影响其他 token 在 n 维空间中的位置。</strong> 它通过加权组合 prompt 中所有 token 的 embeddings 来实现这一点。输入是整个 prompt 的 embeddings，输出则是一个新的 embedding，它是所有输入 embeddings 的加权组合。</p><p>举个例子，如果 prompt 是 “Mary had a little”，被分词为四个 token：Mary、had、a、little，那么 attention 机制可能会决定，在生成下一个 token 时，模型会认为：</p><ul><li>“Mary” 最重要（63%）（译者注：因为整个句子的主语是 Mary，后续内容很可能围绕她展开）</li><li>“had” 和 “a” 次之（16% 和 12%）（译者注：它们是语法结构的一部分，但语义信息较弱）</li><li>“little” 也有一定作用（9%）（译者注：它修饰后面的名词）</li></ul><p>然后，它会把所有 token 的 embeddings 分别乘以对应的权重，然后把结果加在一起，得到一个融合后的向量。这正是 LLM 判断“在当前上下文中，每个 token 应该被关注多少”的方式。</p><p>这是目前为止整个流程中最复杂、最抽象的部分。我会先用伪代码展示它，然后再看看 embeddings 在经过这一过程时是如何被变换的。我本想让这一部分的数学内容少一些，但这里很难避免一些数学运算。别担心，你能行的，我相信你。</p><p>Attention 中的大部分计算都是矩阵乘法。对于本文而言，你只需知道：输出矩阵的形状由两个输入矩阵的形状决定，输出的行数等于第一个输入矩阵的行数，列数等于第二个输入矩阵的列数。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609602" alt="" title="" loading="lazy"/></p><p>理解了这一点，我们来看一个简化版的注意力机制如何计算分配给每个 token 的权重。在以下代码中，我用 * 表示矩阵乘法。</p><pre><code>// Similar to EMBEDDINGS from the pseudocode
// earlier, WQ and WK are learned during 
// training and do not change during inference.
//
// These are both n*n matrices, where n is the
// number of embedding dimensions. In our example
// above, n =3.
const WQ =[[...],[...],[...]];
const WK =[[...],[...],[...]];

// The input embeddings look like this:
//[
//[-0.1,0.1,-0.3],// Mary
//[1.0,-0.5,-0.6],// had
//[0.0,0.8,0.6],// a
//[0.5,-0.7,1.0]// little
//]
function attentionWeights(embeddings){
 const Q = embeddings * WQ;
 const K = embeddings * WK;
 const scores = Q * transpose(K);
 const masked = mask(scores);
return softmax(masked);
}</code></pre><p>接下来，让我们看看 embedding 在流经这个函数时是如何变化的。  </p><p>等等，WQ 和 WK 变量到底是什么？</p><p>还记得我之前说过，每个 token 的 embedding 最初都被随机分配了一个位置，然后在训练过程中不断微调，直到模型找到一个良好的排列状态吗？</p><p>WQ 和 WK 也是类似的。它们是 n×n 的矩阵（n 即 embedding 维度），在训练开始时被赋予随机值，随后也在训练中被不断调整，以帮助模型收敛到一个更优的解。</p><p>任何在训练过程中被调整的数，都被称为“模型参数”。embedding 向量中的每个浮点数，以及 WQ、WK 矩阵中的每个数值，都是一个参数。当你听说某个模型有“1750 亿参数”时，指的就是这些数字。</p><p><strong>至于 WQ 和 WK 到底代表什么，我们其实并不完全清楚。随着模型训练收敛，它们最终会变成某种对 embedding 的变换方式，有助于模型生成更好的输出。</strong> 它们内部可能在做任何事情 —— 而如何解释这些矩阵的含义，目前仍是一个开放且活跃的研究方向。</p><p>要得到 Q 和 K，我们分别将 embeddings 与 WQ 和 WK 相乘。WQ 和 WK 的行数和列数始终等于 embedding 的维度（本例中为 3）。这里我为 WQ 和 WK 选取了随机值，并将结果四舍五入到小数点后两位以便阅读。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609603" alt="" title="" loading="lazy"/></p><p>得到的 Q 矩阵有 4 行 3 列。4 行是因为 embeddings 矩阵有 4 行（每个 token 一行），3 列是因为 WQ 有 3 列（每个 embedding 维度一列）。</p><p>K 的计算完全相同，只是将 WQ 换成 WK。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609604" alt="" title="" loading="lazy"/></p><p>Q 和 K 都是输入 embedding 到新的 n 维空间的"投影"。它们不是原始的 embedding，但由原始 embeddings 推导而来。</p><p>然后，我们将 Q 和 K 相乘。我们对 K 进行“转置”，也就是沿对角线翻转，使得得到的矩阵是一个方阵，其行数和列数都等于输入提示词中的 token 数量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609605" alt="" title="" loading="lazy"/></p><p><strong>这些 scores 表示每个 token 对下一个生成 token 的重要程度。</strong> 左上角的数值 -0.08，代表 “Mary” 对 “had” 的重要性。再往下一行的 -0.10，则代表 “Mary” 对 “a” 的重要性。在展示完矩阵运算后，我会用图示更直观地说明这一点。接下来的所有操作，都是为了将这些 scores 转换为可用于混合 embeddings 的权重。</p><p>这个 score 矩阵的第一个问题是：它允许未来的 token 影响过去的 token。在第一行，我们唯一知道的词是"Mary"，所以它应该是唯一对生成"had"有贡献的词。第二行也是如此，我们知道"Mary"和"had"，所以只有这两个词应该对生成"a"有贡献，依此类推。</p><p>为了解决这个问题，我们对矩阵应用一个三角形掩码（triangular mask），将未来 token 对应的位置置零。不过，我们并不是真的设为 0，而是设为负无穷（negative infinity） —— 原因稍后解释。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609606" alt="" title="" loading="lazy"/></p><p>第二个问题是，这些 scores 是任意的数值。如果它们能变成一个每行之和等于 1 的概率分布，对我们来说会更有用。这正是 softmax 函数的作用。softmax 具体如何运作的细节并不重要 —— 它比简单的“将每个数字除以该行总和”稍复杂一点，但结果是一样的：每行之和为 1，且每个数字都在 0 和 1 之间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609607" alt="" title="" loading="lazy"/></p><p>为了解释为什么用负无穷，下面是一个 softmax 的代码实现：</p><pre><code>function softmax(matrix){
return matrix.map(row =&gt;{
 const exps = row.map(x =&gt; Math.exp(x));
 const sumExps = exps.reduce((a, b)=&gt; a + b,0);
return exps.map(exp =&gt; exp / sumExps);
});
}</code></pre><p>它并不是简单地把每个数加起来再除以总和，而是先对每个数值取 Math.exp，也就是计算 e^x。如果我们用 0 代替负无穷，Math.exp(0) === 1，这些被屏蔽的位置仍然会产生非零权重。而 Math.exp(-Infinity) 是 0，这正是我们想要的。  </p><p>下面的图片展示了提示词"Mary had a little"的 attention 权重示例。</p><p>这些权重与上面的计算结果不匹配，因为我是从 Transformer Explained 网站[7]上运行的 GPT-2 模型中提取的。所以这些是一个真实模型（尽管是老模型）的真实权重。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609608" alt="" title="" loading="lazy"/></p><p>第一行只有"Mary"，因此Mary对"had"的生成的贡献是100%。然后在第二行，"Mary"贡献了79%，而"had"贡献了21%用于生成"a"，以此类推。LLM 认为这个句子中最重要的词是 “Mary”，这一点并不意外——从每一行中 “Mary” 都拥有最高权重就能看出。如果我让你补全"Jessica had a little"这个句子，你不太可能选择"lamb"。</p><p>接下来就只剩下对 token embeddings 进行加权混合了，谢天谢地，这一步比计算权重要简单得多。</p><pre><code>// Learned during training, doesn't change 
// during inference. This is also an n*n matrix,
// where n is the number of embedding dimensions.
const WV =[[...],[...],...];
 
function attention(embeddings){
 const V = embeddings * WV;
// This is the `attentionWeights` function from
// the section above. We're wrapping it in
// this `attention` function.
 const weights = attentionWeights(embeddings);
return weights * V;
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609609" alt="" title="" loading="lazy"/></p><p>为什么不直接混合原始 embeddings？  </p><p>当我们通过 Q 和 K 相乘得到 attention 权重时，我们完全是在衡量 token 之间的相关性。Embeddings 编码了 token 的各种语义信息 —— 某一维可能表示“颜色”，另一维表示“大小”，再一维表示“礼貌/粗鲁程度”，等等。而权重是通过相似度来判断哪些 token 更相关。</p><p>WV 的作用，则是让模型决定在混合时保留哪些维度的信息。</p><p>以句子 “Mary had a little” 为例，这里关于 “Mary” 最重要的信息是“人名”。模型在训练中可能也学到了很多关于 “Bloody Mary（血腥玛丽鸡尾酒）” 或 “Mary Queen of Scots（苏格兰女王玛丽）” 的知识，但这些与这首童谣无关，如果带入后续计算反而会引入噪声。因此，WV 允许模型在混合 embeddings 之前，先过滤掉不相关的特征。</p><p>接着，我们将生成的权重与 V 相乘，输出一组新的 embeddings：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609610" alt="" title="" loading="lazy"/></p><p><strong>Attention 机制的最终输出，就是这个输出矩阵的最后一行。</strong> 通过 attention 过程，前面所有 token 的上下文信息都被融合进了这一行。但要注意：为了得到最后一行，前面所有行都必须被计算出来。</p><p>总而言之，输入是一组 embeddings，输出是一个新的 embedding。Attention 机制通过大量精细的数学运算，按照训练中学到的 WQ、WK 和 WV 矩阵所决定的重要性比例，将各个 token 的信息进行了加权融合。正是这一机制，让 LLM 能够理解在其上下文窗口中“什么内容重要，以及为什么重要”。</p><p>现在，我们终于掌握了讨论 caching 所需的一切知识。</p><p>当然，Attention 还有更多技术细节</p><p>我在本文展示的是一个简化版的 attention，目的是突出与 prompt caching 最相关的核心部分。实际中的 attention 机制更为复杂。如果你希望深入了解更多技术细节，我推荐 3blue1brown 关于 attention 的视频[8]。</p><h2><strong>05 Prompt caching</strong></h2><p>我们再来看一遍上面的网格，但这次会展示在推理循环中每生成一个新 token 时，它是如何逐步填充的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609611" alt="" title="" loading="lazy"/></p><p>每次生成新 token 时，都会将其追加到输入中，并重新完整处理整个 prompt。但仔细观察：之前计算出的权重从未改变。第二行始终是 0.79 和 0.21，第三行始终是 0.81、0.13、0.06。我们其实在不断重复大量不必要的计算。如果你刚刚才处理完 “Mary had a”，那么在生成下一个 token 时，对 “Mary had a little” 中前三个 token 的大部分矩阵运算其实是冗余的 —— 而这正是 LLM 推理循环的默认行为。</p><p>通过以下两个改动，就能避免这些重复计算：</p><ul><li><strong>在每次迭代中缓存 K 和 V 矩阵。</strong></li><li><strong>只将最新 token 的 embeddings 输入模型，而不是整个 prompt。</strong></li></ul><p>现在我们再次走一遍矩阵运算过程，但这一次：前 4 个 token 的 K 和 V 矩阵已被缓存，我们只传入一个新 token 的 embeddings。</p><p>是的，又要面对矩阵运算了，抱歉！不过内容和之前基本一致，我们会快速过一遍。</p><p>计算新的 Q 时，输出只有一行。WQ 和之前一样，没有变化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609612" alt="" title="" loading="lazy"/></p><p>接着，计算新的 K 也同样只输出一行，而 WK 也和之前一样保持不变。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609613" alt="" title="" loading="lazy"/></p><p>但随后我们将这一新行追加到前一次迭代缓存的 4 行 K 矩阵之后：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609614" alt="" title="" loading="lazy"/></p><p>于是现在我们拥有了提示词中所有 token 的 K 矩阵，但我们只需要计算它的最后一行。</p><p>我们继续以这种方式来获取新的 score：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609615" alt="" title="" loading="lazy"/></p><p>以及新的的 weights：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609616" alt="" title="" loading="lazy"/></p><p>全程我们只计算必需的部分，完全不需要对旧值进行任何重新计算。获取 V 的新一行时也是同样的做法：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609617" alt="" title="" loading="lazy"/></p><p>然后将其追加到我们缓存的 V 中：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609618" alt="" title="" loading="lazy"/></p><p>最后，我们将新的权重与新的 V 相乘，得到最终的新 embeddings：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609619" alt="" title="" loading="lazy"/></p><p>我们只需要这单独一行新的 embedding。得益于缓存的 K 和 V，先前所有 token 的上下文信息都已被融入其中。</p><p><strong>被缓存的数据是 embeddings <em> WK 和 embeddings </em> WV 的结果，也就是 K 和 V。</strong> 因此，提示词缓存通常被称为"KV caching"。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609620" alt="" title="" loading="lazy"/></p><p>就是这样，上面那些 K 和 V 矩阵，就是服务提供商保存在他们巨大数据中心里的 1 和 0，用来给我们提供一折的 token 成本和更快的响应。</p><p>服务提供商在请求发出后，会将每个提示词的这些矩阵保留 5-10 分钟，如果你发送一个以相同提示词开头的新请求，他们就会复用缓存的 K 和 V，而不是重新计算它们。<strong>缓存匹配不需要完全一致 —— 即使新 prompt 只和缓存中的某一部分开头相同，也可以复用那部分已缓存的计算结果，而不必整个 prompt 完全匹配。</strong></p><p>OpenAI 和 Anthropic 的缓存机制截然不同。<strong>OpenAI 完全自动处理，会尽可能尝试将请求路由到缓存条目。</strong> 在我的实验中，通过发送请求然后立即重发，缓存命中率约为 50%。考虑到长上下文窗口的首字节延迟（time-to-first-byte）可能很长，这种自动缓存可能导致性能表现不稳定。</p><p><strong>Anthropic 则赋予你更多控制权，让你决定何时缓存以及缓存多久。</strong> 你需要为这项特权付费，但在我进行的实验中，当我们要求 Anthropic 缓存某个提示词时，他们会 100% 地将请求路由到缓存条目。因此，如果你的应用涉及长上下文窗口，并且需要可预测的延迟，Anthropic 可能是更合适的选择。</p><p>等等，那 temperature 这些参数会影响提示词缓存吗？</p><p>LLM 提供商提供了多种参数来控制模型输出的随机性，常见的有 temperature、top_p 和 top_k。这些参数都作用于推理循环的最后一步，即模型根据它为词表中每个 token 分配的概率来选取 token。这发生在 attention 机制产生最终 embedding 之后，因此提示词缓存不受这些参数影响。你可以随意调整它们，而不用担心导致缓存的提示词失效。</p><h2><strong>致谢</strong></h2><p>为了学习撰写本文所需的全部知识，我如饥似渴地阅读了大量优质内容，以下是我认为对我最有帮助的：</p><ul><li>Build a Large Language Model (From Scratch)[9] by Sebastian Raschka[10].</li><li>Neural Networks: Zero to Hero[11] by Andrej Karpathy[12].</li><li>Neural Networks video course[13] by 3blue1brown[14].</li><li>Transformer Explainer[15] by Aeree Cho[16] et al.</li></ul><p>如果你喜欢这篇文章，你一定会喜欢这些资源。</p><p><strong>END</strong></p><p><strong>本期互动内容 🍻</strong></p><p><strong>❓按照文中逻辑，缓存本质是拿内存换计算。当你处理10万Token以上的超长上下文时，有没有估算过KV Cache的内存占用成本 vs 重新计算的API成本？在什么临界点你会选择放弃缓存？</strong></p><p><strong>文中链接</strong></p><p>[1]<a href="https://link.segmentfault.com/?enc=fvaBUJarK2UnSEr9IFrh6Q%3D%3D.ATvM0W5lTysFtJL2W4r8tAAV5yAXEkAGrLcx5r3EDTPLNW0P4sKySRU9wXDlYdcg" rel="nofollow" target="_blank">https://claude.com/blog/prompt-caching</a></p><p>[2]<a href="https://link.segmentfault.com/?enc=h04XyC8Te%2FJ5gaIrJ%2BE5zQ%3D%3D.15UOqKajPMrlqGm%2B1urDwhUlE9EHo1ResJDOv%2FPuk2ZpMBAt5MPNjAhIw7fZEQB%2BUxzvW93GelUBWzC1wt94FFrI5XefhdNWcZxOx6R5FXM%3D" rel="nofollow" target="_blank">https://docs.claude.com/en/docs/build-with-claude/prompt-caching</a></p><p>[3]<a href="https://link.segmentfault.com/?enc=efGC3MRdX7zdVpuH2Ihosg%3D%3D.BIe%2FBBp5X8MHHQa8JrvXLYLsT%2B1kPTqvjYZkeBeXBVSUsa%2B6jT15TJNMTz5x0yNfG1AyyOzmqZJkb5mub3MGZQ%3D%3D" rel="nofollow" target="_blank">https://platform.openai.com/docs/guides/prompt-caching</a></p><p>[4]<a href="https://link.segmentfault.com/?enc=z9rXaGaF9D3vc3MCPIyqww%3D%3D.6SIRscUNvQpAfy9059eoG%2Bo4HD%2F91siM%2FNGKY5jFmFd1X1%2B8uLMyBbYWQ14gNjZS" rel="nofollow" target="_blank">https://magazine.sebastianraschka.com/</a></p><p>[5]<a href="https://link.segmentfault.com/?enc=nyOOCJNKQ2dvs1RGM2yZIg%3D%3D.cxc7mhEh7Q%2BJoaSOSTBDYcqC%2B34qq2TDjXJYrfixHqs%2B7uzTTpXx0ZcTve3n%2BgFC8v%2FAj9hnaeAG3jwKCVN6KZ20S89uuF9lVsCrpP%2FFPdpE6g7w82PS4Q31FhZnfZz1" rel="nofollow" target="_blank">https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/13_olmo3/standalone-olmo3.ipynb</a></p><p>[6]<a href="https://link.segmentfault.com/?enc=o5KzEGoIH0dzB6aptmY5wQ%3D%3D.tP88zR%2FOGaP1U8nSsMh5ITpiHVH6RvWOJefxkZ2HZWcrWZbS4CTk6drZd75RSoIL" rel="nofollow" target="_blank">https://tiktokenizer.vercel.app/</a></p><p>[7]<a href="https://link.segmentfault.com/?enc=1NkkN4DkAkXx8njswC%2BAFg%3D%3D.hLP5XIJ%2F%2FNANEDxGsih3EI7DL%2Fp169g1xDFee%2BeW26JPVlMPJ0TjmXug4LK%2FAcOVqWGaq%2BbGl0Vh2Sh8ZstUCg%3D%3D" rel="nofollow" target="_blank">https://poloclub.github.io/transformer-explainer/</a></p><p>[8]<a href="https://link.segmentfault.com/?enc=1oIaozjqaHtDvtQGB%2Fd3wA%3D%3D.025ocRtxVcdH%2F1TDN5U%2BoAZOrMdiIAkmkZtdu9R%2F2bbPG9FgCnAV0sXDWXoNJDTF" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=eMlx5fFNoYc</a></p><p>[9]<a href="https://link.segmentfault.com/?enc=Mdgejk8s5tR0o27HqiNj9w%3D%3D.WvicHQYE0bcoed7XWJmnbmjfGhSIIQ8K%2FAm93FX8SrAoJwOpzyqF0pLF8hW32%2B60tzrUM42ZKif3gW9HcSplcklYQSP9lQ4pXQQyqPpdhcc%3D" rel="nofollow" target="_blank">https://www.oreilly.com/library/view/build-a-large/9781633437...</a></p><p>[10]<a href="https://link.segmentfault.com/?enc=J%2FrCitbVoUSZd50HQniuog%3D%3D.7OT1ZeHID0n1wkMUvqZwTW65h4GwoPSjW5zD2pNOEu4%3D" rel="nofollow" target="_blank">https://sebastianraschka.com/</a></p><p>[11]<a href="https://link.segmentfault.com/?enc=5l3DLmIzTJ4XdfyYvnxFoA%3D%3D.l%2B9u9G3zUCeiI0jUHzA6YJNXIUdBz%2BhLxjbcojCYX2dlrITa0eIPVRB52dGeW8Q6Hvf2oHBq0iXTeiqrHTc%2FUiDzPE2u%2BUZ%2B1B9Wu2hxcfuXGM4tdXJJE6eXQzYNse2c" rel="nofollow" target="_blank">https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbu...</a></p><p>[12]<a href="https://link.segmentfault.com/?enc=R1Rf8HpG72I6XD2%2B4uLRQA%3D%3D.n7p4C887o6oRY0bDEzBK63d%2FisKbzeef4pRWshj3nDo%3D" rel="nofollow" target="_blank">https://karpathy.ai/</a></p><p>[13]<a href="https://link.segmentfault.com/?enc=dcd1WWcW394LILaM303lZw%3D%3D.cC693gY%2BlNqSzgangWG9vjfus0%2F3%2Bvh9dmV2lG4qJMcrM7HPkIQ25xktRqfd5dtRUkAGknMxkSX1TCLPj9FmSHoPJAIfOpuFOAsQsaIoWwk%3D" rel="nofollow" target="_blank">https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000...</a></p><p>[14]<a href="https://link.segmentfault.com/?enc=52wcwUNKpXx2TtZASrLAaQ%3D%3D.6c1jDvIO1m6PEyhOvBVsJ%2F9oCmDMXNRC2bks9nAPnM84eD3m0pSvGe4Fpg%2F9kR%2FN" rel="nofollow" target="_blank">https://www.youtube.com/@3blue1brown</a></p><p>[15]<a href="https://link.segmentfault.com/?enc=ctdErqgGBMGCVUgEmeJ8hQ%3D%3D.CHcM35tD5ZOcIXx6jMoHR%2BHPzojPlPAeTXdd6cJdIbNb6ZPpvmR9nGw2D%2BQVYkWCerwcIkG24kmd6kYoMsQ70Q%3D%3D" rel="nofollow" target="_blank">https://poloclub.github.io/transformer-explainer/</a></p><p>[16]<a href="https://link.segmentfault.com/?enc=C5pNIbZXcM2ibTUeLPPyIQ%3D%3D.XW4b8zaDEPq8LUMTzH0%2B8gxV%2B6ndf2TQsM16DCDLdYk%3D" rel="nofollow" target="_blank">https://aereeeee.github.io/</a></p><p><strong>本文经原作者授权，由</strong> <strong>Baihai IDP</strong> <strong>编译。如需转载译文，请联系获取授权。</strong></p><p><strong>原文链接：</strong>  </p><p><a href="https://link.segmentfault.com/?enc=s6tsfHxLVc2PGfdOOZkZ6w%3D%3D.6sGy7d%2FWTIeZxq2sxhr%2FTbEtwe%2BPiwH60FQKWopxQsrSKUlghNDqzMXNqgnhX9LY" rel="nofollow" target="_blank">https://ngrok.com/blog/prompt-caching/</a></p>]]></description></item><item>    <title><![CDATA[博睿大使｜推荐Bonree ONE 有礼活动正式启幕！ 博睿数据 ]]></title>    <link>https://segmentfault.com/a/1190000047609671</link>    <guid>https://segmentfault.com/a/1190000047609671</guid>    <pubDate>2026-02-13 14:06:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>博睿大使｜推荐Bonree ONE 有礼活动正式启幕！原创 一体化智能可观测 博睿宏远 2026年2月12日 16:00 北京<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609673" alt="图片" title="图片"/><br/>博睿大使【推荐Bonree ONE有礼】活动正式启幕！即日起至2026年12月31日诚邀各位伙伴成为 Bonree ONE 的引荐者向博睿数据推荐新客户、新商机，解锁丰厚奖励！即刻点击下方海报或扫描海报二维码参与活动吧！具体活动规则详见下方海报👇<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609674" alt="图片" title="图片" loading="lazy"/><br/>— 精彩资料推荐 —<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609675" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609676" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609677" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609678" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609679" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609680" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609681" alt="图片" title="图片" loading="lazy"/><br/>往期推荐_● 博睿数据持续领跑中国APMO市场！► 点击阅读_● 扬帆奋楫 再攀高峰！博睿数据2025年度精彩回顾！► 点击阅读_● 新起点·新视觉｜博睿数据全球品牌VI系统全新升级！► 点击阅读_● 《智能体协同矩阵重塑自主运维新范式》白皮书重磅发布！► 点击阅读</p>]]></description></item><item>    <title><![CDATA[Python网络编程实战：利用WebSocket实现金融级实时数据推送 EmilyLi ]]></title>    <link>https://segmentfault.com/a/1190000047609719</link>    <guid>https://segmentfault.com/a/1190000047609719</guid>    <pubDate>2026-02-13 14:06:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>引言</strong><br/>在Web开发中，我们习惯了RESTful API。但在金融量化（FinTech）领域，RESTful往往是性能瓶颈的代名词。<br/>本文将从后端工程的角度，详细拆解如何使用Python的websocket-client库，对接第三方行情服务商（以AllTick为例），实现一个高可用、低延迟的港股行情接入模块。</p><p><strong>技术栈选择</strong></p><p>Language: Python 3.9+</p><p>Protocol: WebSocket (RFC 6455)</p><p>Library: websocket-client (同步阻塞模式，适合独立进程)</p><p><strong>模块实现细节</strong></p><ol><li>连接管理类（Connection Manager）<br/>为了保持代码的整洁，建议将WebSocket操作封装在一个类中。我们需要处理Socket生命周期的四个关键事件：Open, Message, Error, Close。<br/>在on_open回调中，我们执行订阅操作。这是一种典型的异步编程思想——连接建立是事件，订阅是响应。</li></ol><pre><code>import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    print(data)  # 输出实时行情数据

def on_open(ws):
    # 订阅港股代码为HK.0005（汇丰控股）的实时数据
    ws.send(json.dumps({
        "event": "subscribe",
        "symbol": "HK.0005",  # 港股代码
        "channel": "market_data"
    }))

if __name__ == "__main__":
    websocket.enableTrace(True)
    ws = websocket.WebSocketApp("wss://api.alltick.co/market_data",  # 使用AllTick的WebSocket URL
                                on_message=on_message,
                                on_open=on_open)
    ws.run_forever()</code></pre><ol start="2"><li>消息反序列化与路由（Deserialization &amp; Routing）<br/>服务端推送的数据是Byte流或String。我们需要做两件事：</li></ol><p>JSON反序列化：将字符串转为Dict。</p><p>业务路由：根据symbol字段，将数据分发给不同的策略回调函数。<br/>注意：这里的异常处理至关重要，格式错误的包不应导致进程崩溃。</p><pre><code>response = '{"symbol": "HK.0005", "price": 123.45, "volume": 10000}'
data = json.loads(response)

price = data['price']
volume = data['volume']

print(f"汇丰控股当前价格: {price}, 成交量: {volume}")</code></pre><ol start="3"><li>订阅协议的构造<br/>根据API文档，订阅请求通常是一个包含Event Type和Channel的JSON对象。这里演示了如何构造一个标准的订阅Payload。</li><li>弹性设计（Resilience Engineering）<br/>在分布式系统中，"Design for Failure"是核心准则。我们利用while True循环配合try...except块，实现了一个简易但有效的守护进程（Daemon）。如果Socket意外断开，程序会休眠数秒后尝试重连，实现无人值守运行。</li></ol><pre><code>import time

def fetch_data_with_retry():
    retries = 3
    for _ in range(retries):
        try:
            data = fetch_data_from_api()
            return data
        except Exception as e:
            print(f"请求失败: {e}, 正在重试...")
            time.sleep(2)  # 等待2秒后重试
    print("重试次数已用完，无法获取数据")</code></pre><p><strong>总结</strong><br/>通过WebSocket，我们成功将网络开销分摊到了连接建立的一次性成本上，后续的数据传输几乎没有额外Header开销。这对于高频数据处理是非常必要的优化。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnVCQ" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[HarmonyOS 6.0 图书馆管理系统（ArkTS）开发实战 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047609721</link>    <guid>https://segmentfault.com/a/1190000047609721</guid>    <pubDate>2026-02-13 14:05:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>HarmonyOS 6.0 图书馆管理系统（ArkTS）开发实战</h2><h3>一、项目概述</h3><p>你想要开发的是基于HarmonyOS 6.0、使用ArkTS语言构建的图书馆管理系统，该系统面向图书馆管理员和读者，核心实现图书查询、借阅/归还、图书管理等基础功能，采用HarmonyOS 6.0的最新特性（如Stage模型、ArkUI组件化）开发，适配多设备形态，兼顾易用性和性能。<br/><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/a97c7d88ad7043649fd3d89218c7884f.png" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>二、技术栈与环境准备</h3><h4>1. 核心技术</h4><ul><li>开发语言：ArkTS（TypeScript超集，HarmonyOS原生开发语言）</li><li>应用模型：Stage模型（HarmonyOS 6.0推荐的主流应用模型）</li><li>UI框架：ArkUI（基于TSX的声明式UI）</li><li>数据存储：Preferences（轻量级键值存储）+ RelationalStore（关系型数据库）</li><li>设备适配：自适应布局（Flex/Grid）</li></ul><h4>2. 环境要求</h4><ul><li>DevEco Studio：4.2及以上版本</li><li>HarmonyOS SDK：6.0（API Version 11）</li><li>模拟器/真机：HarmonyOS 6.0及以上设备</li></ul><h3>三、核心功能设计</h3><p>本系统聚焦3个核心模块，满足基础图书馆管理需求：</p><ol><li>图书查询（读者端）：按书名/作者/分类检索图书，查看图书状态（可借/已借出）</li><li>借阅/归还（管理员端）：扫描/输入图书编号，完成借阅、归还操作</li><li>图书管理（管理员端）：新增、编辑、删除图书信息</li></ol><h3>四、代码实现</h3><h4>1. 项目结构（Stage模型）</h4><pre><code>library-system/
├── entry/
│   ├── src/main/ets/
│   │   ├── entryability/       # 应用入口
│   │   ├── pages/              # 页面（图书列表、借阅页、管理页）
│   │   ├── model/              # 数据模型
│   │   ├── util/               # 工具类（数据库、存储）
│   │   └── resources/          # 资源（字符串、样式）</code></pre><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/b4fa1af80e2a4c42bacb39e9ef4b4ba9.png" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>2. 数据模型定义（model/BookModel.ets）</h4><p>定义图书和借阅记录的数据结构，作为全局数据模型：</p><pre><code class="typescript">/**
 * 图书数据模型
 */
export interface Book {
  id: string;        // 图书编号（唯一标识）
  name: string;      // 书名
  author: string;    // 作者
  category: string;  // 分类（如计算机、文学）
  status: boolean;   // 状态：true-可借，false-已借出
  borrowTime?: string;// 借阅时间（可选）
  borrower?: string;  // 借阅人（可选）
}

/**
 * 全局状态管理（简化版）
 */
export class BookManager {
  private static instance: BookManager;
  private books: Book[] = [];

  private constructor() {
    // 初始化测试数据
    this.books = [
      { id: "001", name: "ArkTS开发实战", author: "鸿蒙开发者", category: "计算机", status: true },
      { id: "002", name: "HarmonyOS 6.0进阶", author: "华为技术团队", category: "计算机", status: false, borrowTime: "2026-02-01", borrower: "张三" },
      { id: "003", name: "百年孤独", author: "加西亚·马尔克斯", category: "文学", status: true }
    ];
  }

  // 单例模式，保证全局唯一实例
  public static getInstance(): BookManager {
    if (!BookManager.instance) {
      BookManager.instance = new BookManager();
    }
    return BookManager.instance;
  }

  // 获取所有图书
  getBooks(): Book[] {
    return this.books;
  }

  // 按关键词查询图书
  searchBooks(keyword: string): Book[] {
    return this.books.filter(book =&gt; 
      book.name.includes(keyword) || 
      book.author.includes(keyword) || 
      book.category.includes(keyword)
    );
  }

  // 借阅图书
  borrowBook(bookId: string, borrower: string): boolean {
    const book = this.books.find(b =&gt; b.id === bookId);
    if (book &amp;&amp; book.status) {
      book.status = false;
      book.borrowTime = new Date().toLocaleDateString();
      book.borrower = borrower;
      return true;
    }
    return false;
  }

  // 归还图书
  returnBook(bookId: string): boolean {
    const book = this.books.find(b =&gt; b.id === bookId);
    if (book &amp;&amp; !book.status) {
      book.status = true;
      book.borrowTime = undefined;
      book.borrower = undefined;
      return true;
    }
    return false;
  }

  // 新增图书
  addBook(book: Book): void {
    this.books.push(book);
  }

  // 删除图书
  deleteBook(bookId: string): boolean {
    const index = this.books.findIndex(b =&gt; b.id === bookId);
    if (index !== -1) {
      this.books.splice(index, 1);
      return true;
    }
    return false;
  }
}</code></pre><h4>3. 图书列表/查询页面（pages/BookListPage.ets）</h4><p>实现图书列表展示和关键词查询功能，采用ArkUI声明式UI：</p><pre><code class="tsx">@Entry
@Component
struct BookListPage {
  // 状态变量：搜索关键词、图书列表
  @State searchKeyword: string = "";
  @State bookList: Book[] = [];
  private bookManager = BookManager.getInstance();

  // 页面初始化时加载数据
  aboutToAppear() {
    this.bookList = this.bookManager.getBooks();
  }

  // 搜索图书
  onSearch() {
    this.bookList = this.bookManager.searchBooks(this.searchKeyword);
  }

  build() {
    Column() {
      // 搜索栏
      Row({ space: 10 }) {
        TextField({ placeholder: "输入书名/作者/分类查询" })
          .width("70%")
          .height(40)
          .border({ width: 1, radius: 8 })
          .padding(8)
          .onChange((value) =&gt; {
            this.searchKeyword = value;
          })
        Button("搜索")
          .width("20%")
          .height(40)
          .backgroundColor("#007DFF")
          .onClick(() =&gt; this.onSearch())
      }
      .padding(10)
      .width("100%")

      // 图书列表
      List() {
        ForEach(this.bookList, (book: Book) =&gt; {
          ListItem() {
            Column() {
              Row({ space: 15 }) {
                Text(`编号：${book.id}`)
                  .fontSize(14)
                  .fontColor("#666")
                Text(`书名：${book.name}`)
                  .fontSize(16)
                  .fontWeight(FontWeight.Bold)
                Text(book.status ? "可借" : "已借出")
                  .fontSize(14)
                  .fontColor(book.status ? "#00C800" : "#FF4D4F")
              }
              .width("100%")
              .padding(5)

              Row({ space: 15 }) {
                Text(`作者：${book.author}`)
                  .fontSize(14)
                Text(`分类：${book.category}`)
                  .fontSize(14)
              }
              .width("100%")
              .padding(5)

              // 已借出图书显示借阅信息
              if (!book.status) {
                Row() {
                  Text(`借阅人：${book.borrower}`)
                    .fontSize(12)
                    .fontColor("#999")
                  Text(`借阅时间：${book.borrowTime}`)
                    .fontSize(12)
                    .fontColor("#999")
                }
                .width("100%")
                .padding(5)
              }
            }
            .width("100%")
            .padding(10)
            .borderBottom({ width: 0.5, color: "#EEEEEE" })
          }
        })
      }
      .width("100%")
      .flexGrow(1)
    }
    .width("100%")
    .height("100%")
    .padding(5)
  }
}</code></pre><h4>4. 借阅/归还页面（pages/BorrowReturnPage.ets）</h4><p>实现图书借阅和归还的核心操作：</p><pre><code class="tsx">@Entry
@Component
struct BorrowReturnPage {
  @State bookId: string = "";
  @State borrower: string = "";
  @State tipText: string = "";
  @State tipColor: string = "#333";
  private bookManager = BookManager.getInstance();

  // 借阅操作
  borrowBook() {
    if (!this.bookId || !this.borrower) {
      this.tipText = "图书编号和借阅人不能为空！";
      this.tipColor = "#FF4D4F";
      return;
    }
    const result = this.bookManager.borrowBook(this.bookId, this.borrower);
    if (result) {
      this.tipText = `借阅成功！图书${this.bookId}已借出`;
      this.tipColor = "#00C800";
    } else {
      this.tipText = "借阅失败！图书不存在或已借出";
      this.tipColor = "#FF4D4F";
    }
    // 清空输入框
    this.bookId = "";
    this.borrower = "";
  }

  // 归还操作
  returnBook() {
    if (!this.bookId) {
      this.tipText = "图书编号不能为空！";
      this.tipColor = "#FF4D4F";
      return;
    }
    const result = this.bookManager.returnBook(this.bookId);
    if (result) {
      this.tipText = `归还成功！图书${this.bookId}已入库`;
      this.tipColor = "#00C800";
    } else {
      this.tipText = "归还失败！图书不存在或未借出";
      this.tipColor = "#FF4D4F";
    }
    // 清空输入框
    this.bookId = "";
  }

  build() {
    Column({ space: 20 }) {
      // 借阅模块
      Column({ space: 10 }) {
        Text("图书借阅")
          .fontSize(18)
          .fontWeight(FontWeight.Bold)
          .alignSelf(ItemAlign.Start)
        TextField({ placeholder: "输入图书编号" })
          .width("100%")
          .height(40)
          .border({ width: 1, radius: 8 })
          .padding(8)
          .onChange((value) =&gt; this.bookId = value)
        TextField({ placeholder: "输入借阅人姓名" })
          .width("100%")
          .height(40)
          .border({ width: 1, radius: 8 })
          .padding(8)
          .onChange((value) =&gt; this.borrower = value)
        Button("确认借阅")
          .width("100%")
          .height(40)
          .backgroundColor("#007DFF")
          .onClick(() =&gt; this.borrowBook())
      }
      .width("90%")
      .padding(15)
      .backgroundColor("#F5F7FA")
      .borderRadius(10)

      // 归还模块
      Column({ space: 10 }) {
        Text("图书归还")
          .fontSize(18)
          .fontWeight(FontWeight.Bold)
          .alignSelf(ItemAlign.Start)
        TextField({ placeholder: "输入图书编号" })
          .width("100%")
          .height(40)
          .border({ width: 1, radius: 8 })
          .padding(8)
          .onChange((value) =&gt; this.bookId = value)
        Button("确认归还")
          .width("100%")
          .height(40)
          .backgroundColor("#00C800")
          .onClick(() =&gt; this.returnBook())
      }
      .width("90%")
      .padding(15)
      .backgroundColor("#F5F7FA")
      .borderRadius(10)

      // 提示信息
      Text(this.tipText)
        .fontSize(14)
        .fontColor(this.tipColor)
    }
    .width("100%")
    .height("100%")
    .padding(20)
    .justifyContent(FlexAlign.Center)
  }
}</code></pre><h3>五、功能扩展与优化建议</h3><ol><li><strong>持久化存储</strong>：当前数据仅存在于内存中，可集成RelationalStore将图书数据存入本地数据库，保证应用重启后数据不丢失；</li><li><strong>权限管理</strong>：新增登录模块，区分管理员/读者权限（管理员可操作借阅/归还，读者仅可查询）；</li><li><strong>扫码功能</strong>：集成HarmonyOS的扫码API，通过扫描图书条形码/二维码快速获取图书编号；</li><li><strong>多设备适配</strong>：使用MediaQuery适配手机、平板、智慧屏等不同尺寸设备，优化大屏布局；</li><li><strong>网络同步</strong>：对接后端接口（如SpringBoot），实现多设备数据同步、远程图书管理。</li></ol><h3>六、运行效果</h3><ol><li>图书列表页：可输入关键词搜索图书，列表展示图书基本信息和状态；</li><li>借阅/归还页：输入图书编号和借阅人信息，完成借阅/归还操作，实时提示操作结果；</li><li>所有操作实时同步到内存中的图书数据，刷新列表可看到状态变化。</li></ol><h4>总结</h4><ol><li>本图书馆管理系统基于HarmonyOS 6.0 + ArkTS开发，采用Stage模型和声明式UI，核心实现了图书查询、借阅/归还、图书管理等基础功能；</li><li>代码采用单例模式管理图书数据，保证全局数据一致性，同时通过ArkUI组件实现了简洁易用的交互界面；</li><li>可基于本基础版本扩展持久化存储、权限管理、扫码、网络同步等功能，适配更复杂的图书馆业务场景。</li></ol><p>该系统充分利用了HarmonyOS 6.0的ArkTS特性，代码结构清晰、易扩展，适合作为HarmonyOS应用开发的入门实战项目。</p>]]></description></item><item>    <title><![CDATA[纯原生适配！ArkTS 开发 DormMate新生系统欢迎界面全解析 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047609728</link>    <guid>https://segmentfault.com/a/1190000047609728</guid>    <pubDate>2026-02-13 14:04:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>纯原生适配！ArkTS 开发 DormMate新生系统欢迎界面全解析</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609730" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>前言</h3><p>随着高校信息化建设的推进，传统的宿舍管理模式存在效率低、信息孤岛多、交互体验差等问题。新生入住宿舍是学校管理中非常关键的环节，从分配床位、办理入住手续，到查询宿舍信息，管理流程繁杂。</p><p>本篇文章以 <strong>HarmonyOS 6.0 原生开发</strong> 为基础，分享 <strong>DormMate 新生宿舍管理系统</strong>中“欢迎区域”模块的实现方法。重点解析 ArkTS 声明式 UI 构建、多端适配以及鸿蒙原生组件使用技巧，为想基于 HarmonyOS 6.0 进行原生应用开发的读者提供参考。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609731" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>背景</h3><ol><li><p><strong>传统管理痛点</strong>：</p><ul><li>手工登记信息，易出错</li><li>新生对流程不熟悉，需人工指导</li><li>信息更新慢，难以实时共享</li></ul></li><li><p><strong>系统设计目标</strong>：</p><ul><li><strong>简洁友好的欢迎界面</strong>：让新生第一眼就感受到服务功能</li><li><strong>高可扩展性</strong>：欢迎区域可以轻松添加活动信息、公告、快捷入口</li><li><strong>跨端统一体验</strong>：手机、平板、桌面端界面一致</li></ul></li><li><p><strong>技术选型</strong>：</p><ul><li><strong>HarmonyOS 6.0</strong>：原生分布式操作系统，提供多端统一的应用开发框架</li><li><strong>ArkTS</strong>：鸿蒙原生声明式开发语言，支持跨设备 UI 一致性渲染</li><li><strong>ArkUI</strong>：鸿蒙原生 UI 框架，提供丰富的组件库和布局能力</li></ul></li></ol><hr/><h3>HarmonyOS 6.0 原生开发介绍</h3><p>HarmonyOS 6.0 基于“一次开发，多端部署”的核心理念，提供了 <strong>分布式软总线</strong>、<strong>分布式数据管理</strong> 和 <strong>统一的 ArkUI 框架</strong>。ArkTS 作为其原生开发语言，具备以下优势：</p><table><thead><tr><th>特性</th><th>HarmonyOS 6.0 原生开发</th></tr></thead><tbody><tr><td>跨端开发</td><td>✅ 天然支持手机、平板、智慧屏、桌面端等多终端部署</td></tr><tr><td>UI 构建</td><td>✅ 声明式 UI 语法，与 相近但更贴合鸿蒙系统</td></tr><tr><td>性能</td><td>✅ 系统级深度优化，原生渲染性能更佳</td></tr><tr><td>系统能力</td><td>✅ 全面调用 HarmonyOS 分布式能力、系统服务</td></tr></tbody></table><p>在 DormMate 系统中，我们将利用 ArkTS + ArkUI 构建原生界面，充分发挥 HarmonyOS 6.0 的分布式特性，实现多端统一的欢迎页面。</p><hr/><h3>开发核心代码：欢迎区域实现</h3><p>下面是“欢迎区域”的核心实现代码，以及逐行解析。该模块的功能包括：</p><ul><li>欢迎新生文字提示</li><li>简介功能</li><li>当季入住信息</li><li>图标装饰</li></ul><h4>完整代码</h4><pre><code class="typescript">@Entry
@Component
struct WelcomeSection {
  // 获取系统主题
  @State theme: ThemeConstants = getThemeConstants();

  build() {
    Column() {
      this.buildWelcomeCard()
    }
    .padding(16)
    .width('100%')
    .backgroundColor(this.theme.backgroundColor)
  }

  /**
   * 构建欢迎区域卡片
   */
  @Builder
  buildWelcomeCard() {
    Row() {
      // 文字内容区域
      Column() {
        // 欢迎标题
        Text('欢迎使用新生宿舍管理系统')
          .fontSize(this.theme.headlineSmall.fontSize)
          .fontWeight(FontWeight.Bold)
          .fontColor(this.theme.onSurface)
          .margin({ bottom: 8 })

        // 功能描述
        Text('为新生提供便捷的宿舍分配、入住流程管理和宿舍信息查询服务')
          .fontSize(this.theme.bodyMedium.fontSize)
          .fontColor(this.theme.onSurfaceVariant)
          .margin({ bottom: 16 })
          .maxLines(2)
          .textOverflow({ overflow: TextOverflow.Ellipsis })

        // 入住季标签
        Text('2024届新生入住季')
          .fontSize(this.theme.labelLarge.fontSize)
          .fontWeight(FontWeight.Bold)
          .fontColor(this.theme.primary)
          .backgroundColor(this.theme.primaryContainer)
          .padding({ left: 16, right: 16, top: 8, bottom: 8 })
          .borderRadius(20)
      }
      .alignItems(ItemAlign.Start)
      .flexGrow(1) // 占据剩余空间，适配多端

      // 装饰图标区域
      Stack() {
        Text('宿')
          .fontSize(this.theme.displayLarge.fontSize)
          .fontWeight(FontWeight.Bold)
          .fontColor(this.theme.primary)
      }
      .width(100)
      .height(100)
      .backgroundColor(this.theme.primaryContainer)
      .borderRadius(20)
      .justifyContent(FlexAlign.Center)
      .margin({ left: 16 })
    }
    .width('100%')
    .padding(24)
    // 渐变背景
    .backgroundImage(
      LinearGradient.createLinearGradient(
        { x: 0, y: 0 }, // 起始点
        { x: 1, y: 0 }, // 结束点
        [
          this.theme.surfaceVariant + '80', // 带透明度的表面变体色
          this.theme.surface + 'CC'         // 带透明度的表面色
        ]
      )
    )
    .borderRadius(16)
  }
}

/**
 * 主题常量定义（模拟系统主题，实际开发可通过AbilityStage获取）
 */
interface ThemeConstants {
  backgroundColor: string;
  surface: string;
  surfaceVariant: string;
  onSurface: string;
  onSurfaceVariant: string;
  primary: string;
  primaryContainer: string;
  headlineSmall: { fontSize: number };
  bodyMedium: { fontSize: number };
  labelLarge: { fontSize: number };
  displayLarge: { fontSize: number };
}

/**
 * 获取主题常量（简化实现，实际项目建议使用主题管理）
 */
function getThemeConstants(): ThemeConstants {
  // 亮色主题示例，实际可根据系统设置动态切换
  return {
    backgroundColor: '#f9f9f9',
    surface: '#ffffff',
    surfaceVariant: '#f0f0f0',
    onSurface: '#1d1d1f',
    onSurfaceVariant: '#6e6e73',
    primary: '#007aff', // 鸿蒙系统蓝色
    primaryContainer: '#007aff1a', // 主色透明变体
    headlineSmall: { fontSize: 24 },
    bodyMedium: { fontSize: 16 },
    labelLarge: { fontSize: 14 },
    displayLarge: { fontSize: 64 }
  };
}</code></pre><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609732" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>逐行解析</h4><h5>1. 组件结构与入口</h5><pre><code class="typescript">@Entry
@Component
struct WelcomeSection {
  @State theme: ThemeConstants = getThemeConstants();

  build() {
    Column() {
      this.buildWelcomeCard()
    }
    .padding(16)
    .width('100%')
    .backgroundColor(this.theme.backgroundColor)
  }</code></pre><ul><li><code>@Entry</code>：标记该组件为应用入口组件</li><li><code>@Component</code>：声明这是一个 ArkUI 组件</li><li><code>@State</code>：状态装饰器，用于管理组件内部状态（此处存储主题信息）</li><li><code>build()</code>：组件的构建方法，返回 UI 结构</li><li>外层 <code>Column</code> 作为根布局，提供基础的页面边距和背景色</li></ul><h5>2. 欢迎卡片构建器</h5><pre><code class="typescript">@Builder
buildWelcomeCard() {
  Row() {
    // 文字内容区域
    Column() { ... }
    .flexGrow(1)
    
    // 装饰图标区域
    Stack() { ... }
    ...
  }
  .width('100%')
  .padding(24)
  ...
}</code></pre><ul><li><code>@Builder</code>：构建器装饰器，用于封装可复用的 UI 片段</li><li><code>Row</code>：水平布局容器，对应 Row 组件</li><li><code>flexGrow(1)</code>：让文字区域占据剩余空间，实现自适应布局</li><li><code>Stack</code>：堆叠容器，用于实现装饰图标区域（ Container + Center）</li></ul><h5>3. 文字内容区域</h5><pre><code class="typescript">Column() {
  // 欢迎标题
  Text('欢迎使用新生宿舍管理系统')
    .fontSize(this.theme.headlineSmall.fontSize)
    .fontWeight(FontWeight.Bold)
    .fontColor(this.theme.onSurface)
    .margin({ bottom: 8 })

  // 功能描述
  Text('为新生提供便捷的宿舍分配、入住流程管理和宿舍信息查询服务')
    .fontSize(this.theme.bodyMedium.fontSize)
    .fontColor(this.theme.onSurfaceVariant)
    .margin({ bottom: 16 })
    .maxLines(2)
    .textOverflow({ overflow: TextOverflow.Ellipsis })

  // 入住季标签
  Text('2024届新生入住季')
    .fontSize(this.theme.labelLarge.fontSize)
    .fontWeight(FontWeight.Bold)
    .fontColor(this.theme.primary)
    .backgroundColor(this.theme.primaryContainer)
    .padding({ left: 16, right: 16, top: 8, bottom: 8 })
    .borderRadius(20)
}
.alignItems(ItemAlign.Start)
.flexGrow(1)</code></pre><ul><li><code>Column</code>：垂直布局容器，对应 Column 组件</li><li><code>Text</code>：文本组件，支持 fontSize、fontWeight、fontColor 等样式配置</li><li><code>maxLines + textOverflow</code>：实现文本超出两行时的省略号效果</li><li>所有样式均基于主题常量，保证多端风格统一</li></ul><h5>4. 装饰图标区域</h5><pre><code class="typescript">Stack() {
  Text('宿')
    .fontSize(this.theme.displayLarge.fontSize)
    .fontWeight(FontWeight.Bold)
    .fontColor(this.theme.primary)
}
.width(100)
.height(100)
.backgroundColor(this.theme.primaryContainer)
.borderRadius(20)
.justifyContent(FlexAlign.Center)
.margin({ left: 16 })</code></pre><ul><li><code>Stack</code> 配合 <code>justifyContent(FlexAlign.Center)</code> 实现文字居中效果</li><li>直接通过链式调用设置宽高、背景色、圆角等样式，语法更简洁</li><li><code>margin({ left: 16 })</code> 实现与文字区域的间距</li></ul><h5>5. 渐变背景实现</h5><pre><code class="typescript">.backgroundImage(
  LinearGradient.createLinearGradient(
    { x: 0, y: 0 }, // 起始点
    { x: 1, y: 0 }, // 结束点
    [
      this.theme.surfaceVariant + '80', // 80对应16进制的透明度(0.5)
      this.theme.surface + 'CC'         // CC对应16进制的透明度(0.8)
    ]
  )
)</code></pre><ul><li>使用 <code>LinearGradient</code> 创建线性渐变背景</li><li>鸿蒙中通过 16 进制后缀表示透明度（80=0.5，CC=0.8）</li><li>渐变方向从左到右（x从0到1）</li></ul><h5>6. 主题管理</h5><pre><code class="typescript">interface ThemeConstants { ... }

function getThemeConstants(): ThemeConstants {
  return {
    backgroundColor: '#f9f9f9',
    surface: '#ffffff',
    surfaceVariant: '#f0f0f0',
    onSurface: '#1d1d1f',
    onSurfaceVariant: '#6e6e73',
    primary: '#007aff',
    primaryContainer: '#007aff1a',
    headlineSmall: { fontSize: 24 },
    bodyMedium: { fontSize: 16 },
    labelLarge: { fontSize: 14 },
    displayLarge: { fontSize: 64 }
  };
}</code></pre><ul><li>通过接口定义主题常量结构，保证类型安全</li><li>实际项目中可结合 <code>AbilityStage</code> 和 <code>Configuration</code> 实现深色/浅色主题动态切换</li><li>主色使用鸿蒙系统默认蓝色（#007aff），符合系统设计规范</li></ul><hr/><h4>多端适配说明</h4><p>在 HarmonyOS 6.0 中，该组件可通过以下方式实现多端自适应：</p><ol><li><strong>尺寸适配</strong>：使用百分比宽度（<code>width('100%')</code>）和 <code>flexGrow</code> 实现不同屏幕尺寸适配</li><li><strong>字体适配</strong>：可结合 <code>vp</code> 单位（虚拟像素）替代固定像素值，自动适配不同屏幕密度</li><li><p><strong>布局适配</strong>：通过媒体查询（<code>@Media</code>）为不同设备类型定制布局：</p><pre><code class="typescript">// 平板/桌面端适配示例
@Media(minWidth: 800) {
  .buildWelcomeCard() {
 Row() {
   // 平板端可调整布局比例
   Column() { ... }.flexGrow(2)
   Stack() { ... }.width(120).height(120)
 }
  }
}</code></pre></li></ol><hr/><h3>心得</h3><ol><li><p><strong>HarmonyOS 6.0 原生开发优势</strong>：</p><ul><li>原生 API 直接调用鸿蒙系统能力，无需中间层适配</li><li>多端部署能力更原生，无需额外插件支持</li></ul></li><li><p><strong>UI 设计技巧</strong>：</p><ul><li>使用主题常量统一管理颜色和字体，保证多端风格一致</li><li>链式调用语法让样式配置更简洁直观</li></ul></li><li><p><strong>开发效率</strong>：</p><ul><li>ArkTS 支持热重载，开发调试效率高</li><li>原生组件性能更优，尤其在鸿蒙设备上表现更佳</li></ul></li></ol><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609733" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>总结</h3><p>本文介绍了基于 <strong>HarmonyOS 6.0 原生开发</strong> 的 <strong>DormMate 新生宿舍管理系统</strong>欢迎区域模块实现思路。通过 ArkTS + ArkUI 构建的原生界面，充分利用了鸿蒙系统的分布式能力和原生渲染优势，为新生提供了一个简洁、易读、现代化的入口界面。</p><p>HarmonyOS 原生开发在系统集成度、性能表现和多端适配方面更具优势，尤其适合深度适配鸿蒙生态的应用。该欢迎区域组件具备良好的可扩展性，可快速添加公告、快捷入口等功能，并天然支持在手机、平板、桌面端等多设备上统一呈现。</p><p><strong>DormMate</strong> 的设计理念是：<strong>原生、高效、跨端统一</strong>，为学校宿舍管理系统提供了一套深度适配 HarmonyOS 生态的前端解决方案。</p><h4>关键点回顾</h4><ol><li><strong>核心实现</strong>：使用 ArkTS 声明式语法，通过 Row/Column/Stack 布局组合 + LinearGradient 渐变背景实现欢迎区域 UI</li><li><strong>主题管理</strong>：通过主题常量统一管理颜色和字体样式，支持深色/浅色模式适配</li><li><strong>多端适配</strong>：利用 flex 布局、百分比宽度和媒体查询，实现手机/平板/桌面端的自适应展示</li></ol>]]></description></item><item>    <title><![CDATA[OpenClaw模型接入全指南：免费Token+新模型适配（含Higress解决方案） 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047609742</link>    <guid>https://segmentfault.com/a/1190000047609742</guid>    <pubDate>2026-02-13 14:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OpenClaw模型接入全指南：免费Token+新模型适配（含Higress解决方案）</p><p>当前AI圈迭代速度迅猛，智谱GLM-5、MiniMax M2.5等新模型接连发布，在性能和性价比上实现大幅突破，但OpenClaw用户普遍面临两大痛点：一是Token消耗过快、付费成本高，二是新模型无法及时适配，需等待官方发版升级。本文将整合两大核心解决方案——讯飞星辰免费Token计划（解决成本问题）与Higress AI网关（解决新模型适配问题），搭配详细操作步骤，帮助OpenClaw用户零成本、高效适配各类前沿模型，畅享大模型生产力。</p><p>一、基础保障：讯飞星辰免费Token计划（零成本用模型）</p><p>对于追求低成本使用OpenClaw的用户，讯飞星辰MaaS平台推出的春节免费Token计划，是经实测可行的优选方案，可有效解决Token消耗过快的核心痛点，适配各类OpenClaw基础使用场景。</p><p>1.1 核心优势</p><ul><li>完全免费：0元获取Token，无需支付任何费用，彻底解决OpenClaw Token使用成本高的问题，日常使用无经济压力。</li><li>适配性强：明确支持OpenClaw运行，无需修改工具核心设置，配置流程简单易懂，新手用户也能快速完成操作。</li><li>官方合规：Token由讯飞星辰官方平台提供，稳定性有保障，规避非正规渠道Token带来的账号安全、使用异常等风险。</li></ul><p>补充说明：官方宣传Token使用无速度限制，实际使用中会存在轻微卡顿，整体流畅度可满足日常文本生成、简单编程等需求，属于可接受范围。</p><p>1.2 前期准备：获取讯飞星辰Token及API授权</p><p>配置前需先前往讯飞星辰MaaS平台，获取模型API授权、API Key等关键信息，具体步骤如下：</p><ol><li>访问讯飞星辰MaaS平台官方地址：<a href="https://link.segmentfault.com/?enc=Nqo1ajIXAoMzWduVXk22%2Bg%3D%3D.y9yBQanKlV78I4XvnbiHmbStoLc68lo8iVwEkht1hps%3D" rel="nofollow" target="_blank">https://maas.xfyun.cn/</a>，完成注册及登录（已有账号可直接登录）。</li><li>进入模型集市：访问<a href="https://link.segmentfault.com/?enc=uAk2Pw2R3adtZy9rl73zqA%3D%3D.IyQKIUbUU6v%2FdmEcp4xQTF0aWYHz2VnmQi4j0jw%2BUPANTu7yxXlbBB%2F1EGwyZpN7" rel="nofollow" target="_blank">https://maas.xfyun.cn/modelSquare</a>，找到对应模型卡片，点击“API调用”，即可获取模型API授权及现金礼品卡（礼品卡非配置必需，可用于后续服务拓展）。</li><li>获取核心配置信息：登录后进入推理服务控制台（地址：<a href="https://link.segmentfault.com/?enc=Y6UthHLgQPskJ5%2FdaBUmmA%3D%3D.hS4f%2BNEjve%2BG97wErt4dHpRSTZpSj%2BtldWITmKZxO2I5Jjp31LJlEadmMi6gs9vW" rel="nofollow" target="_blank">https://maas.xfyun.cn/modelService</a>），页面将展示所有模型服务所需关键信息，其中开发者专属API Key是后续配置的核心，需重点记录。</li></ol><p>1.3 OpenClaw配置步骤（附可直接复制模板）</p><p>讯飞星辰MaaS平台提供OpenAI兼容接口，可在OpenClaw中按“OpenAI / OpenAI-Compatible”模式直接配置，全程无需修改复杂参数：</p><ol><li>找到配置文件：打开OpenClaw工具，在设置中找到“配置文件”入口（或按工具指引查找文件路径），将下方模板复制粘贴，替换原有相关配置（配置文件为空可直接粘贴）。</li><li>填充核心信息：将前期获取的API Key，替换模板中“YOUR_API_KEY”位置，其余参数无需修改（模板已包含常用模型及最优基础配置）。</li><li>验证配置：保存配置文件并重启OpenClaw（部分版本无需重启可直接生效），在聊天窗口发送测试消息（如“你好”），若能正常收到返回结果，即表示配置成功，Token可正常使用。</li></ol><p>可直接复制的配置模板</p><p>{<br/>  "meta": {</p><pre><code>"lastTouchedVersion": "2026.2.1",
"lastTouchedAt": "2026-02-04T12:14:10.945Z"</code></pre><p>},<br/>  "models": {</p><pre><code>"mode": "merge",
"providers": {
  "ds": {
    "baseUrl": "https://maas-api.cn-huabei-1.xf-yun.com/v2",
    "apiKey": "YOUR_API_KEY",
    "api": "openai-completions",
    "models": [
      {
        "id": "xopdeepseekv32",
        "name": "DeepSeek-V3.2",
        "reasoning": false,
        "input": [
          "text"
        ],
        "cost": {
          "input": 0.0025,
          "output": 0.01,
          "cacheRead": 0,
          "cacheWrite": 0
        },
        "contextWindow": 32768,
        "maxTokens": 32768
      }
    ]
  }
}</code></pre><p>},<br/>  "agents": {</p><pre><code>"defaults": {
  "model": {
    "primary": "ds/xopdeepseekv32"
  },
  "models": {
    "ds/xopdeepseekv32": {
      "alias": "xopdeepseekv32"
    }
  },
  "compaction": {
    "mode": "safeguard"
  },
  "maxConcurrent": 4,
  "subagents": {
    "maxConcurrent": 8
  }
}</code></pre><p>},<br/>  "messages": {</p><pre><code>"ackReactionScope": "group-mentions"</code></pre><p>},<br/>  "commands": {</p><pre><code>"native": "auto",
"nativeSkills": "auto"</code></pre><p>},<br/>  "channels": {<br/>  },<br/>  "gateway": {</p><pre><code>"mode": "local",
"tailscale": {
  "mode": "off"
}</code></pre><p>},<br/>  "plugins": {</p><pre><code>"entries": {
},
"installs": {
}</code></pre><p>}<br/>}</p><p>二、进阶解决方案：Higress AI网关（适配各类新模型）</p><p>随着智谱GLM-5、MiniMax M2.5等新模型密集发布，OpenClaw原生存在“模型硬编码”问题——新模型无法通过配置直接接入，需等待官方发版升级，严重滞后于模型迭代速度。而Higress AI网关通过“模型配置与网关解耦”的设计，可彻底解决这一痛点，让OpenClaw用户即时适配各类前沿模型。</p><p>2.1 OpenClaw原生模型支持困境</p><p>目前OpenClaw的各个provider默认模型均为硬编码，新模型发布后无法通过配置支持，需等待维护者处理相关issue并发版升级，具体痛点如下：</p><ul><li>新模型适配滞后：智谱GLM-5、MiniMax M2.5均无法直接接入，后续Qwen/DeepSeek发布新模型，大概率仍需等待官方适配。</li><li>迭代节奏不匹配：MiniMax 108天内连发3个版本，智谱、DeepSeek等厂商迭代速度相近，而OpenClaw官方适配速度难以跟上，导致用户无法及时使用新模型的核心能力。</li></ul><p>2.2 Higress AI网关核心优势</p><p>Higress采用与OpenClaw原生完全不同的设计思路，将模型配置与网关解耦，新增模型无需升级，热更新即时生效，核心优势如下：</p><ul><li>热更新支持：新增模型、更换供应商后，配置热加载，无需重启网关，即时生效。</li><li>任意模型兼容：只要模型提供OpenAI兼容API，即可接入OpenClaw，无需担心适配问题。</li><li>预配置常用供应商：插件内置智谱、MiniMax、Kimi、DeepSeek、Qwen等主流厂商，无需手动配置基础参数。</li><li>操作极简：通过Higress OpenClaw Integration Skill，一句话即可完成全部配置，无需修改复杂代码。</li></ul><p>2.3 Higress接入OpenClaw详细步骤</p><p>Higress通过专属Integration Skill简化配置流程，全程无需手动修改配置文件，仅需通过OpenClaw对话即可完成：</p><ol><li>安装Integration Skill：在OpenClaw聊天窗口发送指令：“帮我下载并安装这个技能：<a href="https://link.segmentfault.com/?enc=bX%2B3cV4Ou4piloUsCu4Fyg%3D%3D.XjHAVL3vpIODS%2BQzZrhtPL7xYcmTCpYn0INqxRRfF6GhbzJWRhdLocuO%2BsN3drqWY9C1VJFILOMlVGOGxmuAcw%3D%3D" rel="nofollow" target="_blank">https://higress.cn/skills/higress-openclaw-integration.zip</a>”。</li><li>自动配置网关：发送指令：“使用这个技能帮我配置Higress AI Gateway”，OpenClaw将自动完成以下操作：</li></ol><ul><li>下载并安装Higress Integration Skill；</li><li>部署Higress AI Gateway；</li><li>配置指定的模型供应商和API Key；</li><li>安装并启用OpenClaw插件。</li></ul><ol start="3"><li><p>使用新模型：配置完成后，直接在OpenClaw中指定模型即可使用，示例如下：</p></li></ol><ul><li>使用GLM-5：model: "higress/glm-5"</li><li>使用MiniMax M2.5：model: "higress/minimax-m25"</li><li>自动路由（智能选模型）：model: "higress/auto"</li></ul><p>2.4 后续新增模型：一句话快速适配</p><p>若后续DeepSeek发布V4、Qwen推出新版本等，无需重启网关、无需升级组件，仅需在OpenClaw中发送简单指令即可完成适配：</p><ul><li>添加新供应商API Key：“帮我添加DeepSeek的API Key：sk-xxx”</li><li>切换默认模型：“帮我把默认模型切换到deepseek-v4”</li></ul><p>指令发送后，配置热加载即时生效，真正实现“模型迭代无滞后”。</p><p>三、重点关注：GLM-5与MiniMax M2.5核心能力解析</p><p>当前AI圈最值得关注的两大新模型——GLM-5（开源标杆）与MiniMax M2.5（性价比之王），通过Higress可直接接入OpenClaw，其核心能力如下，方便用户根据需求选择使用：</p><p>3.1 GLM-5：开源界的“系统架构师”</p><p>智谱2月11日发布的GLM-5，采用MoE架构，744B总参数中每次仅激活44B，配合DeepSeek稀疏注意力机制，在保持高性能的同时大幅降低部署成本，核心亮点如下：</p><ul><li>参数与性能：744B总参数、202K上下文窗口，Coding与Agent能力达到开源SOTA，官方定位为“Opus 4.6与GPT-5.3的国产开源平替”。</li><li>核心优势：擅长复杂系统工程与长程Agent任务，在真实编程场景的体感逼近Claude Opus 4.5，适合需要深度架构设计的场景。</li></ul><p>3.2 MiniMax M2.5：Agent时代的性价比之王</p><p>MiniMax在GLM-5发布一天后推出的M2.5，主打“真实世界生产力”，性能与成本优势突出，核心亮点如下：</p><ul><li>性能领先：SWE-Bench Verified跑分80.2%，Multi-SWE-Bench跑分51.3%拿下第一，编程能力出众。</li><li>成本极低：使用成本仅为Opus的1/10，100 TPS连续工作一小时仅需1美金，1万美金可支持4个Agent连续工作一年。</li><li>适配广泛：支持Go、C、C++、TypeScript等10+语言，覆盖Web、Android、iOS等全平台，适合各类编程场景。</li><li>智能特性：动手写代码前，会主动拆解功能、结构和UI设计，具备“架构师级”思考能力。</li></ul><p>四、高效使用技巧：Higress自动路由功能</p><p>GLM-5与MiniMax M2.5定位不同（GLM-5架构能力强，M2.5性价比高），Higress的自动路由功能可根据任务类型智能调度模型，无需手动切换，提升使用效率：</p><p>4.1 配置自动路由规则</p><p>在OpenClaw中发送指令，即可配置自动路由规则，示例如下：</p><ul><li>遇到“深入思考”“复杂问题”“架构设计”时，使用glm-5；</li><li>遇到“简单”“快速”“翻译”时，使用minimax-m25-lite；</li><li>日常代码任务，使用minimax-m25（便宜又能打）。</li></ul><p>4.2 使用方法</p><p>配置完成后，仅需在OpenClaw中指定模型为“higress/auto”，系统将根据消息内容自动选择最合适的模型进行推理，兼顾性能与成本。</p><p>五、常见问题及补充说明</p><p>5.1 讯飞星辰Token相关问题</p><ul><li>配置后无法使用：优先检查API Key是否填写正确（注意大小写、空格），若无误，可重新登录讯飞星辰平台确认API授权有效，或刷新控制台重新获取API Key。</li><li>使用速度过慢：轻微卡顿属于正常现象，不影响日常使用；卡顿严重时，可检查网络连接，或重启OpenClaw及网络设备。</li><li>Token期限与额度：该计划为讯飞星辰春节专属免费活动，具体期限及额度以官方通知为准，建议及时配置使用。</li></ul><p>5.2 Higress网关相关问题</p><ul><li>无法自动配置：若当前使用的模型能力较弱，无法自动完成配置，可访问Higress Integration Skill说明文档（相关链接见文末），按步骤手动配置。</li><li>新模型接入失败：确认模型提供OpenAI兼容API，若仍失败，可发送指令重新添加供应商API Key，或检查网关配置是否生效。</li></ul><p>六、总结与后续支持</p><p>6.1 核心方案对比</p><p>对比项</p><p>OpenClaw原生</p><p>OpenClaw+讯飞星辰Token</p><p>OpenClaw+Higress</p><p>使用成本</p><p>较高（需付费Token）</p><p>零成本</p><p>按需选择（可搭配免费/付费Token）</p><p>新模型支持</p><p>需等待官方发版</p><p>仅支持讯飞星辰相关模型</p><p>一句话配置，即时适配</p><p>操作难度</p><p>中等</p><p>简单（模板复制）</p><p>极简（对话指令）</p><p>维护成本</p><p>高（等官方更新）</p><p>低（官方保障）</p><p>低（自主可控，即时响应）</p><p>6.2 后续支持</p><ol><li>讯飞星辰Token配置问题：若遇到配置失败、Token无法使用等问题，可转发本文并在评论区留言“666”，获取手把手配置指导。</li><li>Higress网关配置问题：可访问Higress OpenClaw Integration Skill官方链接，查看详细说明文档，或参考文档中的手动配置步骤。</li></ol><p>6.3 相关链接</p><ul><li>讯飞星辰MaaS平台：<a href="https://link.segmentfault.com/?enc=Cb7QEz1aksyPegMp2gk3Uw%3D%3D.FURKTYdb2UTjChzAGMb3O5ABHqQLqjPfBWgmJKQMclo%3D" rel="nofollow" target="_blank">https://maas.xfyun.cn/</a></li><li>Higress OpenClaw Integration Skill：<a href="https://link.segmentfault.com/?enc=L3zJH%2BwIE3Zu%2Fe32W9NO9w%3D%3D.Eb%2BUDxEF5qZSZPveyoz1mk%2Fi7gRfBbGVUp32Gur5bfMmVvBSKuKrxL6oour%2BIHoJ4JzxabPRqbl%2FQA9LTZQSEUGPkhvK%2FCdXFzz9ozNV%2B9uWm%2BvvMkpq6Z7X0%2FZPbfeg" rel="nofollow" target="_blank">https://github.com/alibaba/higress/tree/main/.claude/skills/higress-openclaw-integration</a></li></ul><p>综上，讯飞星辰免费Token计划解决了OpenClaw用户的成本痛点，Higress AI网关解决了新模型适配痛点，两者结合可让用户零成本、高效使用各类前沿AI模型。无论是追求低成本的普通用户，还是需要使用新模型提升生产力的进阶用户，均可按照本文步骤操作，快速实现模型接入与高效使用。</p><p>本文由<a href="https://link.segmentfault.com/?enc=Ba69GfkayQL8%2F06a9AXmYg%3D%3D.t6w1jJl9e4tUzCOB6dqcmjU%2B%2BctI0EfEMZ4mWF7T6qA%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[从零开始开发HarmonyOS 6.0 TodoList应用（ArkTS版） 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047609746</link>    <guid>https://segmentfault.com/a/1190000047609746</guid>    <pubDate>2026-02-13 14:03:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>从零开始开发HarmonyOS 6.0 TodoList应用（ArkTS版）</h2><p>你想要基于HarmonyOS 6.0和ArkTS语言开发一个TodoList（待办清单）应用，这篇文章会从项目搭建、核心功能实现到界面美化，一步步带你完成一个可运行、功能完整的TodoList应用，适合HarmonyOS开发新手学习和实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047609748" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>一、开发环境准备</h3><p>在开始编码前，确保你已完成以下准备：</p><ol><li>安装最新版DevEco Studio（建议4.1及以上版本，适配HarmonyOS 6.0）</li><li>配置HarmonyOS SDK 6.0</li><li>了解ArkTS基础语法（声明式UI、状态管理、组件生命周期）</li></ol><h3>二、项目创建</h3><ol><li>打开DevEco Studio，新建“Empty Ability”项目</li><li><p>配置项目信息：</p><ul><li>Project name: TodoListDemo</li><li>Bundle name: 自定义（如com.example.todolist）</li><li>Compile SDK: 6.0 (API 12)</li><li>Model: Stage</li><li>Language: ArkTS</li></ul></li></ol><h3>三、核心功能实现</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609749" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>3.1 数据模型定义</h4><p>首先定义待办事项的数据结构，创建<code>model/TodoItem.ets</code>文件：</p><pre><code class="typescript">/**
 * 待办事项数据模型
 */
export interface TodoItem {
  // 唯一标识
  id: string;
  // 待办内容
  content: string;
  // 是否完成
  isCompleted: boolean;
  // 创建时间
  createTime: string;
}

/**
 * 生成唯一ID
 */
export function generateId(): string {
  return Date.now().toString() + Math.random().toString(36).substr(2, 9);
}

/**
 * 格式化时间
 */
export function formatTime(time: number): string {
  const date = new Date(time);
  return `${date.getFullYear()}-${(date.getMonth() + 1).toString().padStart(2, '0')}-${date.getDate().toString().padStart(2, '0')} ${date.getHours().toString().padStart(2, '0')}:${date.getMinutes().toString().padStart(2, '0')}`;
}</code></pre><h4>3.2 主页面实现（核心功能）</h4><p>修改<code>pages/Index.ets</code>，实现待办事项的添加、删除、状态切换、清空功能：</p><pre><code class="typescript">@Entry
@Component
struct TodoListPage {
  // 待办事项列表（状态管理）
  @State private todoList: TodoItem[] = [];
  // 输入框内容
  @State private inputContent: string = '';
  // 页面标题
  private title: string = '我的待办清单';

  build() {
    Column() {
      // 标题区域
      Text(this.title)
        .fontSize(24)
        .fontWeight(FontWeight.Bold)
        .margin({ top: 20, bottom: 15 })
        .alignSelf(ItemAlign.Center);

      // 输入和添加区域
      Row({ space: 10 }) {
        TextField(this.inputContent, (value: string) =&gt; {
          this.inputContent = value;
        })
          .placeholder('请输入待办事项...')
          .width('70%')
          .height(40)
          .border({ width: 1, radius: 8, color: '#E5E5E5' })
          .padding({ left: 10 });

        Button('添加')
          .width('20%')
          .height(40)
          .backgroundColor('#007DFF')
          .fontColor(Color.White)
          .borderRadius(8)
          .onClick(() =&gt; this.addTodoItem());
      }
      .margin({ bottom: 20 })
      .padding({ left: 15, right: 15 });

      // 待办事项列表区域
      List({ space: 10 }) {
        ForEach(this.todoList, (item: TodoItem) =&gt; {
          ListItem() {
            Row({ space: 10 }) {
              // 完成状态切换复选框
              Checkbox()
                .select(item.isCompleted)
                .onChange((isChecked: boolean) =&gt; {
                  this.toggleTodoStatus(item.id);
                })
                .width(20)
                .height(20);

              // 待办内容（完成时加删除线）
              Text(item.content)
                .fontSize(16)
                .decoration({ type: item.isCompleted ? TextDecorationType.LineThrough : TextDecorationType.None })
                .fontColor(item.isCompleted ? '#999999' : '#333333')
                .flexGrow(1);

              // 创建时间
              Text(item.createTime)
                .fontSize(12)
                .fontColor('#999999')
                .width(100);

              // 删除按钮
              Button('删除')
                .width(60)
                .height(30)
                .backgroundColor('#FF4D4F')
                .fontColor(Color.White)
                .borderRadius(6)
                .fontSize(12)
                .onClick(() =&gt; this.deleteTodoItem(item.id));
            }
            .padding(10)
            .backgroundColor(Color.White)
            .borderRadius(8)
            .shadow({ radius: 2, color: '#00000010', offsetX: 0, offsetY: 2 });
          }
        })
      }
      .width('100%')
      .flexGrow(1)
      .padding({ left: 15, right: 15 });

      // 清空按钮（有数据时显示）
      if (this.todoList.length &gt; 0) {
        Button('清空所有待办')
          .width('90%')
          .height(40)
          .backgroundColor('#F5F5F5')
          .fontColor('#666666')
          .borderRadius(8)
          .margin({ top: 10, bottom: 20 })
          .onClick(() =&gt; this.clearAllTodos());
      }
    }
    .width('100%')
    .height('100%')
    .backgroundColor('#F8F8F8');
  }

  /**
   * 添加待办事项
   */
  private addTodoItem(): void {
    // 空内容校验
    if (this.inputContent.trim() === '') {
      prompt.showToast({ message: '待办内容不能为空！' });
      return;
    }

    // 创建新待办项
    const newTodo: TodoItem = {
      id: generateId(),
      content: this.inputContent.trim(),
      isCompleted: false,
      createTime: formatTime(Date.now())
    };

    // 添加到列表
    this.todoList.push(newTodo);
    // 清空输入框
    this.inputContent = '';
  }

  /**
   * 切换待办事项完成状态
   * @param id 待办项ID
   */
  private toggleTodoStatus(id: string): void {
    const index = this.todoList.findIndex(item =&gt; item.id === id);
    if (index !== -1) {
      this.todoList[index].isCompleted = !this.todoList[index].isCompleted;
    }
  }

  /**
   * 删除待办事项
   * @param id 待办项ID
   */
  private deleteTodoItem(id: string): void {
    this.todoList = this.todoList.filter(item =&gt; item.id !== id);
  }

  /**
   * 清空所有待办事项
   */
  private clearAllTodos(): void {
    this.todoList = [];
  }
}

// 导入数据模型
import { TodoItem, generateId, formatTime } from '../model/TodoItem';
// 导入提示框
import prompt from '@ohos.promptAction';</code></pre><h4>3.3 代码核心解释</h4><ol><li><strong>状态管理</strong>：使用<code>@State</code>装饰器管理待办列表（<code>todoList</code>）和输入框内容（<code>inputContent</code>），状态变化会自动触发UI刷新。</li><li><p><strong>核心方法</strong>：</p><ul><li><code>addTodoItem()</code>：校验输入内容，创建新待办项并添加到列表，清空输入框；</li><li><code>toggleTodoStatus()</code>：根据ID切换待办项的完成状态；</li><li><code>deleteTodoItem()</code>：根据ID过滤删除指定待办项；</li><li><code>clearAllTodos()</code>：清空整个待办列表。</li></ul></li><li><p><strong>UI组件</strong>：</p><ul><li><code>TextField</code>：用于输入待办内容；</li><li><code>Checkbox</code>：标记待办项是否完成；</li><li><code>List + ForEach</code>：循环渲染待办列表；</li><li><code>Button</code>：实现添加、删除、清空操作。</li></ul></li><li><p><strong>交互优化</strong>：</p><ul><li>输入空内容时弹出Toast提示；</li><li>完成的待办项显示删除线和灰色字体；</li><li>列表项添加阴影和圆角，提升视觉效果；</li><li>无待办项时隐藏“清空”按钮。</li></ul></li></ol><h3>四、运行效果</h3><ol><li>启动模拟器（选择HarmonyOS 6.0版本的设备）或连接真机；</li><li><p>点击“运行”按钮，应用启动后：</p><ul><li>在输入框输入待办内容，点击“添加”可新增待办项；</li><li>勾选复选框可标记待办为“已完成”；</li><li>点击“删除”可移除指定待办项；</li><li>点击“清空所有待办”可删除全部待办。</li></ul></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609750" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>五、功能扩展建议（可选）</h3><p>你可以基于此基础版本扩展更多实用功能：</p><ol><li><strong>本地持久化</strong>：使用<code>@ohos.data.preferences</code>将待办数据保存到本地，重启应用不丢失；</li><li><strong>分类管理</strong>：添加待办分类（工作/生活/学习），支持筛选；</li><li><strong>编辑功能</strong>：允许修改已添加的待办内容；</li><li><strong>优先级标记</strong>：为待办项添加高/中/低优先级标签；</li><li><strong>滑动删除</strong>：实现列表项左滑删除的交互效果。</li></ol><h4>总结</h4><ol><li>本次TodoList应用基于HarmonyOS 6.0和ArkTS开发，核心使用<code>@State</code>状态管理实现UI与数据的双向绑定，通过List+ForEach渲染动态列表；</li><li>实现了待办事项的<strong>添加、状态切换、删除、清空</strong>四大核心功能，同时做了输入校验、视觉美化等交互优化；</li><li>代码结构清晰，数据模型与UI逻辑分离，符合HarmonyOS应用开发的最佳实践，可在此基础上快速扩展更多功能。</li></ol><p>这个TodoList应用覆盖了ArkTS开发的核心知识点（状态管理、组件使用、事件处理），是HarmonyOS新手入门的经典练手项目，你可以直接复制代码运行，也可以根据自己的需求调整界面和功能。</p>]]></description></item><item>    <title><![CDATA[如何利用IP风险情报保障跨境业务的网络安全 科技块儿 ]]></title>    <link>https://segmentfault.com/a/1190000047609755</link>    <guid>https://segmentfault.com/a/1190000047609755</guid>    <pubDate>2026-02-13 14:02:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、跨境业务面临的网络安全挑战</h2><p>跨境业务的快速发展为全球经济带来了新的机遇，但随之而来的网络安全挑战也愈加严峻。不同地区的法规、文化和网络环境使得跨境业务面临独特的安全威胁。尤其是来自不同国家和地区的IP地址，可能涉及到各种类型的网络攻击和欺诈活动。</p><h3>常见的网络安全风险包括：</h3><ul><li>恶意IP攻击：恶意IP来源可能会进行各种形式的网络攻击，如分布式拒绝服务（DDoS）攻击、SQL注入、跨站脚本（XSS）攻击等。</li><li>代理和VPN伪装：黑客常常利用代理服务器和VPN隐藏真实IP地址，以规避检测进行非法操作，如网络入侵、数据盗窃、欺诈等。</li><li>钓鱼攻击：通过伪装成可信的IP地址，攻击者进行钓鱼攻击，诱使用户泄露敏感信息。</li><li>IP地理位置伪造：黑客利用IP地理位置伪造技术，误导目标系统相信攻击来自合法地区，逃避安全防护。</li></ul><p>这些问题不仅给企业的网络安全带来巨大威胁，还可能对企业的声誉、财务安全和合规性造成长期影响。因此，跨境业务需要借助有效的IP风险情报服务，提前识别并应对这些潜在的安全威胁。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnVDq" alt="如何利用IP风险情报保障跨境业务的网络安全" title="如何利用IP风险情报保障跨境业务的网络安全"/></p><h2>二、IP风险情报如何帮助识别恶意IP、代理和VPN</h2><p>IP风险情报是通过对IP地址及其相关数据进行分析，从而识别和预测可能的网络安全威胁。它能够提供以下几个重要功能：</p><h3>1. 恶意IP识别：</h3><p>IP风险情报服务通过对全球范围内的IP地址进行实时监控和更新，识别出被标记为恶意的IP地址。这些IP地址可能与网络攻击、数据泄露或欺诈活动相关联。通过对恶意IP的实时警报，跨境业务能够有效阻止攻击的发生。</p><h3>2. 代理和VPN检测：</h3><p>攻击者往往使用代理或VPN来隐藏真实IP地址，从而规避安全系统的检测。IP风险情报服务可以通过IP地址的特殊模式、历史数据和地理位置信息，识别是否存在代理或VPN的使用，帮助企业判断其是否为合法用户。</p><h3>3. IP信誉评分：</h3><p>每个IP地址都有一个信誉评分，反映其历史行为和安全性。通过对这些评分的分析，企业能够判断某个IP是否属于高风险区域。对那些信誉较低的IP进行封锁或限制访问，可以有效减少安全隐患。</p><h3>4. 地区性风险分析：</h3><p>在跨境业务中，某些地区可能会存在较高的网络攻击频率和欺诈风险。通过IP风险情报服务，企业能够对不同地区的IP进行风险分析，制定更为精准的防护策略。</p><h2>三、实际案例：IP风险情报在跨境电商中的应用</h2><p>某跨境电商公司在拓展国际市场的过程中，发现其网站常常遭遇恶意流量攻击，尤其是来自某些特定国家和地区的IP地址。为了保障网络安全，该公司决定引入IP风险情报服务，并根据以下几个步骤进行了安全优化：</p><h3>1. 识别恶意流量：</h3><p>通过IP风险情报，该公司能够实时识别来自恶意IP的流量，并快速阻止其进入系统。这些恶意IP通常与DDoS攻击、数据盗窃和账户滥用等行为有关。</p><h3>2. 代理与VPN检测：</h3><p>在进行订单处理和用户身份验证时，IP风险情报帮助该公司识别了大量使用VPN的IP地址，许多伪装成来自合法地区的攻击者被成功识别并封锁。</p><h3>3. 风险评分优化：</h3><p>利用IP信誉评分，企业能够有效评估每个IP的安全性，并采取相应的防护措施。例如，对于来自高风险国家的IP地址，增加了多重身份验证措施，减少了欺诈行为。</p><p>通过这些措施，该电商公司成功降低了欺诈风险，提升了交易安全性，也有效保障了客户的个人信息和资金安全。</p><h2>四、如何选择适合的IP风险情报服务</h2><p>在选择IP风险情报服务时，企业需要考虑以下几个因素：</p><ul><li>数据的实时性和准确性：选择能够提供实时更新和精准数据的IP情报服务，确保及时发现新的风险IP。</li><li>全球范围的IP覆盖：跨境业务往往涉及多个国家和地区，因此选择一个全球覆盖广泛的IP风险情报服务至关重要。</li><li>代理和VPN识别能力：确保所选择的IP情报服务具备强大的代理和VPN检测能力，避免潜在的伪装攻击。</li><li>API接口与集成能力：为确保IP情报服务能与现有的安全系统和业务流程无缝对接，选择提供易于集成的API接口的服务。</li></ul><p>例如，<a href="https://link.segmentfault.com/?enc=Mps8WLL3Teyax6FoFgqGrg%3D%3D.ftS%2Birh0Y1vZt1vWbI0biM7dB5%2FYwfsao5HgnvSPiBw%3D" rel="nofollow" target="_blank"><strong>IP数据云</strong></a>提供全球范围内的IP地址风险情报服务，能够识别各类恶意IP、代理和VPN，并提供精准的IP信誉评分，帮助跨境企业加强网络安全防护。</p><h2>五、总结</h2><p>随着跨境业务的不断扩展，网络安全已成为企业面临的重要挑战。通过有效利用IP风险情报，企业可以实时识别恶意IP、代理和VPN，保护自身免受网络攻击和数据泄露等风险。IP数据云等专业的IP情报服务，不仅能提升跨境业务的安全性，还能帮助企业优化全球运营策略，减少网络威胁对业务的影响。因此，选择一个可靠的IP风险情报服务，并将其应用于日常的安全防护中，已成为跨境企业保障网络安全的必然之举。</p>]]></description></item><item>    <title><![CDATA[从零开发HarmonyOS 6.0 天气应用（ArkTS版） 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047609758</link>    <guid>https://segmentfault.com/a/1190000047609758</guid>    <pubDate>2026-02-13 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>从零开发HarmonyOS 6.0 天气应用（ArkTS版）</h2><p>你需要一篇基于HarmonyOS 6.0和ArkTS语言的其他应用开发教程，这篇文章会带你从零构建一个功能完整的天气应用，涵盖网络请求、数据解析、UI适配、状态管理等核心知识点，适合有基础ArkTS认知的开发者学习实践。</p><h3>一、应用核心功能</h3><p>本次开发的天气应用包含以下核心能力：</p><ol><li>基于和风天气API获取实时天气数据（温度、天气状况、风力、湿度）</li><li>展示今日天气概览和未来3天预报</li><li>支持手动输入城市查询天气</li><li>适配不同屏幕尺寸，优化视觉体验</li></ol><h3>二、开发前准备</h3><h4>2.1 环境要求</h4><ul><li>DevEco Studio 4.1+</li><li>HarmonyOS SDK 6.0 (API 12)</li><li>基础网络权限配置</li><li>和风天气API Key</li></ul><h4>2.2 权限配置</h4><p>在<code>module.json5</code>中添加网络访问权限：</p><pre><code class="json">{
  "module": {
    "requestPermissions": [
      {
        "name": "ohos.permission.INTERNET"
      }
    ]
  }
}</code></pre><h3>三、核心代码实现</h3><h4>3.1 数据模型定义</h4><p>创建<code>model/WeatherModel.ets</code>，定义天气数据结构：</p><pre><code class="typescript">/**
 * 实时天气数据模型
 */
export interface RealTimeWeather {
  temp: string; // 温度
  text: string; // 天气状况（晴/雨/多云）
  windDir: string; // 风向
  windScale: string; // 风力等级
  humidity: string; // 湿度
  updateTime: string; // 更新时间
}

/**
 * 未来预报数据模型
 */
export interface DailyForecast {
  fxDate: string; // 日期
  tempMax: string; // 最高温
  tempMin: string; // 最低温
  textDay: string; // 白天天气状况
}

/**
 * 天气响应体模型
 */
export interface WeatherResponse {
  realTime: RealTimeWeather;
  dailyForecasts: DailyForecast[];
}</code></pre><h4>3.2 网络请求工具类</h4><p>创建<code>utils/HttpUtil.ets</code>，封装网络请求方法：</p><pre><code class="typescript">/**
 * 网络请求工具类
 * @param url 请求地址
 * @returns 响应数据
 */
export async function request&lt;T&gt;(url: string): Promise&lt;T&gt; {
  try {
    const response = await fetch.fetch(url);
    if (response.responseCode !== 200) {
      throw new Error(`请求失败，状态码：${response.responseCode}`);
    }
    const result = await response.text();
    return JSON.parse(result) as T;
  } catch (error) {
    console.error('网络请求异常：', error);
    throw error;
  }
}</code></pre><h4>3.3 天气服务类</h4><p>创建<code>service/WeatherService.ets</code>，封装API调用逻辑（替换<code>YOUR_API_KEY</code>为你的真实Key）：</p><pre><code class="typescript">import { request } from '../utils/HttpUtil';
import { RealTimeWeather, DailyForecast } from '../model/WeatherModel';

// 和风天气API配置
const API_KEY = 'YOUR_API_KEY';
const BASE_URL = 'https://devapi.qweather.com/v7';

/**
 * 根据城市获取天气数据
 * @param city 城市名称
 */
export async function getWeatherByCity(city: string): Promise&lt;{
  realTime: RealTimeWeather,
  dailyForecasts: DailyForecast[]
}&gt; {
  // 1. 获取城市Location ID
  const locationUrl = `${BASE_URL}/location/search?key=${API_KEY}&amp;location=${city}`;
  const locationRes: any = await request(locationUrl);
  
  if (!locationRes.location || locationRes.location.length === 0) {
    throw new Error('未找到该城市的天气数据');
  }
  
  const locationId = locationRes.location[0].id;
  
  // 2. 获取实时天气
  const realTimeUrl = `${BASE_URL}/weather/now?key=${API_KEY}&amp;location=${locationId}`;
  const realTimeRes: any = await request(realTimeUrl);
  const realTime: RealTimeWeather = {
    temp: realTimeRes.now.temp,
    text: realTimeRes.now.text,
    windDir: realTimeRes.now.windDir,
    windScale: realTimeRes.now.windScale,
    humidity: realTimeRes.now.humidity,
    updateTime: realTimeRes.updateTime
  };
  
  // 3. 获取未来3天预报
  const dailyUrl = `${BASE_URL}/weather/3d?key=${API_KEY}&amp;location=${locationId}`;
  const dailyRes: any = await request(dailyUrl);
  const dailyForecasts: DailyForecast[] = dailyRes.daily.map((item: any) =&gt; ({
    fxDate: item.fxDate,
    tempMax: item.tempMax,
    tempMin: item.tempMin,
    textDay: item.textDay
  }));
  
  return { realTime, dailyForecasts };
}</code></pre><h4>3.4 主页面实现</h4><p>修改<code>pages/Index.ets</code>，实现天气查询和展示核心逻辑：</p><pre><code class="typescript">@Entry
@Component
struct WeatherPage {
  // 状态管理
  @State private cityName: string = '北京'; // 默认查询北京
  @State private inputCity: string = '';
  @State private realTimeWeather: RealTimeWeather | null = null;
  @State private dailyForecasts: DailyForecast[] = [];
  @State private isLoading: boolean = false;
  @State private errorMsg: string = '';

  // 页面加载时初始化数据
  aboutToAppear() {
    this.fetchWeatherData(this.cityName);
  }

  build() {
    Column() {
      // 标题区域
      Text('鸿蒙天气')
        .fontSize(28)
        .fontWeight(FontWeight.Bold)
        .margin({ top: 20, bottom: 15 })
        .alignSelf(ItemAlign.Center);

      // 城市查询区域
      Row({ space: 10 }) {
        TextField(this.inputCity, (value: string) =&gt; {
          this.inputCity = value;
        })
          .placeholder('请输入城市名称...')
          .width('70%')
          .height(45)
          .border({ width: 1, radius: 8, color: '#E5E5E5' })
          .padding({ left: 10 });

        Button('查询')
          .width('20%')
          .height(45)
          .backgroundColor('#007DFF')
          .fontColor(Color.White)
          .borderRadius(8)
          .onClick(() =&gt; {
            if (this.inputCity.trim()) {
              this.fetchWeatherData(this.inputCity.trim());
            }
          });
      }
      .margin({ bottom: 20 })
      .padding({ left: 15, right: 15 });

      // 加载状态提示
      if (this.isLoading) {
        LoadingProgress()
          .width(40)
          .height(40)
          .margin({ bottom: 20 })
          .alignSelf(ItemAlign.Center);
      }

      // 错误提示
      if (this.errorMsg) {
        Text(this.errorMsg)
          .fontSize(14)
          .fontColor('#FF4D4F')
          .margin({ bottom: 20 })
          .alignSelf(ItemAlign.Center);
      }

      // 实时天气展示
      if (this.realTimeWeather) {
        Column() {
          Text(`${this.cityName} 实时天气`)
            .fontSize(20)
            .fontWeight(FontWeight.Medium)
            .margin({ bottom: 10 });

          Row({ space: 20 }) {
            Text(`${this.realTimeWeather.temp}°C`)
              .fontSize(48)
              .fontWeight(FontWeight.Bold);

            Column() {
              Text(this.realTimeWeather.text)
                .fontSize(18)
                .margin({ bottom: 5 });
              Text(`更新时间：${this.formatTime(this.realTimeWeather.updateTime)}`)
                .fontSize(12)
                .fontColor('#999');
            }
          }
          .margin({ bottom: 15 });

          // 天气详情
          Grid() {
            GridItem() {
              this.buildWeatherInfoItem('风向', `${this.realTimeWeather.windDir}`);
            }
            GridItem() {
              this.buildWeatherInfoItem('风力', `${this.realTimeWeather.windScale}级`);
            }
            GridItem() {
              this.buildWeatherInfoItem('湿度', `${this.realTimeWeather.humidity}%`);
            }
          }
          .columnsTemplate('1fr 1fr 1fr')
          .width('90%')
          .margin({ bottom: 30 });
        }
        .padding(20)
        .backgroundColor(Color.White)
        .borderRadius(12)
        .shadow({ radius: 4, color: '#00000010', offsetX: 0, offsetY: 2 })
        .width('90%')
        .alignSelf(ItemAlign.Center);
      }

      // 未来预报展示
      if (this.dailyForecasts.length &gt; 0) {
        Text('未来3天预报')
          .fontSize(18)
          .fontWeight(FontWeight.Medium)
          .margin({ bottom: 10, top: 20 })
          .alignSelf(ItemAlign.Start)
          .padding({ left: 15 });

        List({ space: 10 }) {
          ForEach(this.dailyForecasts, (item: DailyForecast) =&gt; {
            ListItem() {
              Row({ space: 10 }) {
                Text(item.fxDate)
                  .width(80)
                  .fontSize(14);
                Text(item.textDay)
                  .width(60)
                  .fontSize(14);
                Text(`↑${item.tempMax}°C`)
                  .width(50)
                  .fontSize(14)
                  .fontColor('#FF4D4F');
                Text(`↓${item.tempMin}°C`)
                  .width(50)
                  .fontSize(14)
                  .fontColor('#007DFF');
              }
              .padding(15)
              .backgroundColor(Color.White)
              .borderRadius(8)
              .width('100%');
            }
          })
        }
        .padding({ left: 15, right: 15 })
        .width('100%');
      }
    }
    .width('100%')
    .height('100%')
    .backgroundColor('#F8F8F8');
  }

  /**
   * 构建天气信息子项
   */
  @Builder
  private buildWeatherInfoItem(title: string, value: string) {
    Column() {
      Text(title)
        .fontSize(14)
        .fontColor('#999')
        .margin({ bottom: 5 });
      Text(value)
        .fontSize(16)
        .fontWeight(FontWeight.Medium);
    }
    .alignItems(ItemAlign.Center);
  }

  /**
   * 格式化时间
   */
  private formatTime(timeStr: string): string {
    return timeStr.replace('T', ' ').substring(0, 16);
  }

  /**
   * 获取天气数据
   */
  private async fetchWeatherData(city: string) {
    this.isLoading = true;
    this.errorMsg = '';
    try {
      const weatherData = await getWeatherByCity(city);
      this.cityName = city;
      this.realTimeWeather = weatherData.realTime;
      this.dailyForecasts = weatherData.dailyForecasts;
    } catch (error: any) {
      this.errorMsg = error.message || '获取天气数据失败，请重试';
      console.error('获取天气失败：', error);
    } finally {
      this.isLoading = false;
    }
  }
}

// 导入依赖
import { RealTimeWeather, DailyForecast } from '../model/WeatherModel';
import { getWeatherByCity } from '../service/WeatherService';
import prompt from '@ohos.promptAction';</code></pre><h3>四、核心代码解释</h3><h4>4.1 网络请求封装</h4><ul><li><code>HttpUtil.ets</code>封装了通用的<code>fetch</code>请求方法，处理了状态码校验和异常捕获，简化后续API调用逻辑；</li><li>所有网络请求使用<code>async/await</code>异步语法，避免回调地狱，代码更易读。</li></ul><h4>4.2 状态管理</h4><ul><li>使用<code>@State</code>装饰器管理核心数据（实时天气、预报数据、加载状态、错误信息），状态变化自动触发UI刷新；</li><li><code>aboutToAppear</code>生命周期钩子实现页面初始化时自动加载默认城市（北京）的天气数据。</li></ul><h4>4.3 UI设计</h4><ul><li>采用“卡片式”设计风格，通过<code>backgroundColor</code>、<code>borderRadius</code>、<code>shadow</code>实现拟物效果；</li><li>使用<code>Grid</code>和<code>List</code>组件实现数据的规整展示，适配不同屏幕宽度；</li><li>增加加载中（<code>LoadingProgress</code>）和错误提示状态，提升用户体验。</li></ul><h4>4.4 数据处理</h4><ul><li>对和风天气API返回的原始数据进行结构化解析，映射到自定义数据模型，降低耦合；</li><li>时间格式化方法<code>formatTime</code>处理API返回的ISO格式时间，提升可读性。</li></ul><h3>五、运行与调试</h3><ol><li>替换<code>WeatherService.ets</code>中的<code>API_KEY</code>为你从和风天气官网获取的真实Key；</li><li>启动HarmonyOS 6.0模拟器或连接真机；</li><li>运行应用，默认展示北京天气，输入其他城市（如上海、广州）可查询对应天气数据。</li></ol><h3>六、功能扩展建议</h3><ol><li><strong>定位功能</strong>：集成HarmonyOS定位API，自动获取当前城市天气；</li><li><strong>缓存优化</strong>：使用<code>@ohos.data.preferences</code>缓存已查询城市的天气数据，减少网络请求；</li><li><strong>主题切换</strong>：支持浅色/深色模式，适配系统主题；</li><li><strong>更多数据展示</strong>：添加空气质量、日出日落、紫外线指数等信息；</li><li><strong>动画效果</strong>：为天气卡片添加加载动画、数据刷新过渡动画。</li></ol><h4>总结</h4><ol><li>本次天气应用基于HarmonyOS 6.0和ArkTS开发，核心实现了<strong>网络请求、数据解析、状态管理、UI组件封装</strong>四大核心能力；</li><li>采用“分层设计”思想（数据模型-工具类-服务类-UI页面），符合鸿蒙应用开发的最佳实践；</li><li>代码包含完整的异常处理、加载状态管理和用户体验优化，可直接作为基础模板扩展更多天气相关功能。</li></ol><p>该天气应用覆盖了ArkTS开发中网络交互、复杂数据渲染、UI组件封装等高频场景，是提升鸿蒙应用开发能力的优质练手项目，你可以基于此代码进一步优化和扩展。</p>]]></description></item><item>    <title><![CDATA[外汇量化开发：为什么实时 API 是短线策略的必备基建？ 我不是股神ber ]]></title>    <link>https://segmentfault.com/a/1190000047609286</link>    <guid>https://segmentfault.com/a/1190000047609286</guid>    <pubDate>2026-02-13 13:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>做外汇量化策略开发这么久，我一直被一个问题困扰：<strong>为什么回测很稳的策略，一上实盘就容易跟不上行情？</strong><br/>后来慢慢发现，问题往往不是算法不行，而是<strong>数据链路太弱</strong>。<br/>外汇市场 24 小时不间断波动，尤其做短线、波段、震荡策略时，对数据实时性要求极高。以前我也用过爬虫、手动刷新网页、Excel 同步等方式，不仅效率极低，还经常漏数据、错格式，真正想抓住关键波动时，机会早就没了。</p><p><strong>一、传统数据方式，到底坑在哪？</strong><br/>只要你做过外汇程序开发，基本都会遇到这三个共性痛点：</p><ul><li>延迟不可控<br/>网页刷新、第三方数据更新都有固定间隔，几分钟的延迟，在快节奏行情里就是致命差距。</li><li>格式乱七八糟<br/>不同平台数据结构不统一，解析、清洗、对齐要花大量时间，还容易出 Bug。</li><li>很难接入自动化系统<br/>没有标准接口，数据没法直接喂给策略、回测框架、监控面板，自动化基本是空谈。<br/>这些问题不解决，策略再漂亮也跑不起来。</li></ul><p><strong>二、稳定的外汇 API，究竟解决了什么？</strong><br/>对量化开发者来说，<strong>实时汇率 API 不是锦上添花，而是底层基建</strong>。<br/>它能提供低延迟、连续推送、格式统一的行情数据，拿来就能直接在代码里使用，不用再做多余处理。<br/>我在实际项目里会用 <strong>AllTick API</strong> 来订阅主流货币对实时行情，接入简单、稳定性也够用。</p><pre><code>import websocket
import json

url = "wss://realtime.alltick.co/forex?symbols=USDCNY"

def on_message(ws, message):
    data = json.loads(message)
    print(f"USD/CNY 当前汇率: {data['price']} 时间: {data['time']}")

def on_error(ws, error):
    print(f"连接错误: {error}")

def on_close(ws):
    print("连接已关闭")

def on_open(ws):
    print("实时数据连接成功，开始接收数据...")

ws = websocket.WebSocketApp(url,
                            on_message=on_message,
                            on_error=on_error,
                            on_close=on_close)
ws.on_open = on_open
ws.run_forever()
</code></pre><p>像这样通过 WebSocket 订阅后，数据可以直接进入你的逻辑：分析、计算、可视化、触发信号都没问题。<br/>对比手动爬取、表格整理，效率提升是数量级的。</p><p><strong>三、实战开发中，这些细节直接影响效果</strong><br/>在长期写策略、接数据的过程中，我总结了几个非常实用的关键点：</p><ul><li>合理设置波动阈值<br/>网络抖动、微小波动很常见，不要对每一次价格变动都响应，设置阈值能大幅减少无效计算。</li><li>按需订阅货币对<br/>一次性订阅太多品种会增加程序压力，只选策略真正用到的就行。</li><li>精简数据存储<br/>实时数据量巨大，只存价格、时间戳、货币对这些关键字段，能显著降低数据库压力。<br/>把实时数据 + 历史数据结合，还能做回测、预警、自定义看板，比单纯看 K 线更贴近真实市场。</li></ul><p><strong>四、对量化策略与自动化的真实价值</strong><br/>API 解决的不只是 “看行情”，而是让策略从理论变成可运行的系统。<br/>有了稳定实时数据，你可以：</p><ul><li>实时监控汇率，突破 / 跌破自动报警</li><li>记录关键支撑阻力位</li><li>快速验证轻量级量化策略</li><li>实现半自动甚至全自动交易逻辑<br/>没有实时数据入口，再好的策略也只能停留在回测阶段。<br/>把数据层交给稳定接口，我们才能真正专注在策略逻辑、风控和算法优化上。</li></ul><p>五、给开发者的实用建议<br/>如果你也在做外汇相关开发，这几点可以直接参考：</p><ul><li>先明确自己的策略覆盖范围，不要盲目全量订阅</li><li>实时数据最好配合持久化或可视化，分析效率更高</li><li>优先选择低延迟、格式标准、长期稳定的接口，减少后期重构</li></ul><p>一句话总结：<br/>在外汇量化开发里，<strong>稳定的实时 API = 策略落地的基础效率底座</strong>，它能让你在快速波动的市场里，更稳、更准地抓住真正有价值的机会。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnVvQ" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[2026年11款国内外主流CRM核心能力横向深度评测 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047609290</link>    <guid>https://segmentfault.com/a/1190000047609290</guid>    <pubDate>2026-02-13 13:02:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在数字化转型浪潮中，CRM（客户关系管理）系统已成为企业打通销售全链路、提升客户运营效率的核心工具。本次评测选取11款国内外主流CRM产品，围绕<strong>客户管理、SFA（</strong> <strong>销售自动化</strong> <strong>）、团队协同、统计分析、自定义能力</strong>五大核心维度展开深度横向对比，为不同规模、不同业务场景的企业选型提供专业参考。</p><p>评测对象包括：超兔一体云、Pipedrive、Nimble、Insightly、Streak、Infor CRM、Zendesk Sell、快启CRM、金现代CRM、管家婆、飞书CRM。</p><ul><li><ul><li>*</li></ul></li></ul><h2>一、客户管理：从线索到复购的全生命周期闭环</h2><h3>核心价值</h3><p>客户管理的核心是实现线索-客户-成交-复购的全流程可控，通过精准画像、数据查重和权限隔离提升客户运营效率与数据安全性。</p><h3>关键能力横向对比</h3><table><thead><tr><th>品牌</th><th>全生命周期覆盖</th><th>客户画像能力</th><th>查重机制</th><th>数据权限管理</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全流程覆盖</td><td>工商补全/社交头像/经纬度标记</td><td>多字段查重/模糊简称查重/自定义</td><td>角色分级/财务数据隔离</td><td>自动标记工商地址经纬度、微信支付宝头像获取</td></tr><tr><td>Pipedrive</td><td>全流程覆盖</td><td>可定制字段/移动端同步</td><td>基础字段查重</td><td>角色级权限</td><td>移动端实时数据同步</td></tr><tr><td>Nimble</td><td>全流程覆盖</td><td>社交数据自动更新</td><td>基础字段查重</td><td>团队级权限</td><td>Twitter/LinkedIn数据整合</td></tr><tr><td>Infor CRM</td><td>全流程覆盖</td><td>行业化客户细分</td><td>行业适配查重</td><td>角色+部门权限</td><td>汽车/零售垂直行业客户管理模板</td></tr><tr><td>快启CRM</td><td>全流程覆盖</td><td>基础画像/分类管理</td><td>基础字段查重/公海规则关联</td><td>角色分级/签约客户分库</td><td>智能公海推荐、来电弹屏</td></tr><tr><td>金现代CRM</td><td>全流程覆盖</td><td>360°视图/流失预警</td><td>基础字段查重</td><td>角色分级/跨部门权限</td><td>AI客户画像、智慧商城联动</td></tr></tbody></table><h3>典型流程可视化（超兔一体云）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609292" alt="" title=""/></p><pre><code>flowchart LR
    A[多渠道集客&lt;br&gt;百度/抖音/官网/微信/工商搜客] --&gt; B[线索一键处理&lt;br&gt;新客户/老客户待办/订单]
    B --&gt; C[客池分类&lt;br&gt;需求培养/有需求/上首屏/目标/成功]
    C --&gt; D[客户画像与背景调查&lt;br&gt;工商补全/天眼查/微信头像]
    D --&gt; E[客户维护&lt;br&gt;跟单/订单/财务记录]
    E --&gt; F[客户复购/流失预警]
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#9f9,stroke:#333,stroke-width:2px</code></pre><h2>二、SFA（销售自动化）：适配多场景的跟单效率引擎</h2><h3>核心价值</h3><p>SFA通过标准化跟单模型、自动化任务触发和订单财务管控，降低销售手动操作成本，提升成单转化率。</p><h3>关键能力横向对比</h3><table><thead><tr><th>品牌</th><th>跟单模型数量</th><th>自动化场景</th><th>订单/财务管控</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>3种（三一客/商机/多方项目）</td><td>待办提醒/订单锁库/采购计划生成</td><td>应收三角联动/账期信用管理</td><td>三一客小单快单模型、多方项目全周期管控</td></tr><tr><td>Pipedrive</td><td>1种（通用）</td><td>任务提醒/AI销售助理/互动追踪</td><td>交易阶段预测</td><td>AI销售助理、交易进度预测</td></tr><tr><td>Zendesk Sell</td><td>1种（通用）</td><td>邮件序列/自动拨号</td><td>基础订单跟踪</td><td>批量个性化邮件模板、原生拨号功能</td></tr><tr><td>金现代CRM</td><td>1种（通用）</td><td>跟进任务触发/AI开单</td><td>库存价格实时同步</td><td>语音/图片AI开单、智能漏斗分析</td></tr><tr><td>快启CRM</td><td>1种（通用）</td><td>跟进任务触发/日程同步</td><td>基础订单跟踪</td><td>可视化销售漏斗、跟单转日程</td></tr></tbody></table><h3>特色模型可视化（超兔三一客小单快单）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609293" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((超兔三一客小单快单模型))
        三定规则
            定性&lt;br&gt;判断客户需求真假
            定级&lt;br&gt;评估客户购买力等级
            定量&lt;br&gt;明确成单时间与金额
        关键节点推进
            首次触达&lt;br&gt;需求确认
            方案发送&lt;br&gt;异议处理
            报价跟进&lt;br&gt;逼单成交
        效率提升
            一键生成待办
            自动同步跟单时间线
            数据自动汇总报表</code></pre><h2>三、团队协同：跨角色跨链路的信息协同网络</h2><h3>核心价值</h3><p>通过角色权限隔离、跨部门工单流转和供应链协同，打破信息孤岛，实现销售、财务、采购、客户的全链路协同。</p><h3>关键能力横向对比</h3><table><thead><tr><th>品牌</th><th>角色权限管理</th><th>跨部门协同能力</th><th>供应链协同能力</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>双重指挥系统/角色分级</td><td>跨岗位数据隔离/待办同步</td><td>OpenCRM平台全流程协同</td><td>华为式行政+业务双重指挥系统、上下游对账</td></tr><tr><td>飞书CRM</td><td>飞书原生角色权限</td><td>飞书会议/文档/即时通讯联动</td><td>无原生供应链协同</td><td>依托飞书生态全场景协同</td></tr><tr><td>Infor CRM</td><td>角色+部门权限</td><td>跨系统数据同步</td><td>Infor SCM套件联动</td><td>制造业零部件采购进度同步</td></tr><tr><td>金现代CRM</td><td>飞书原生角色权限</td><td>工单流转/SLA管理</td><td>无原生供应链协同</td><td>飞书即时沟通+工单流转闭环</td></tr><tr><td>快启CRM</td><td>角色分级管理</td><td>跨部门短消息沟通</td><td>无原生供应链协同</td><td>异常操作实时提醒、项目考核闭环</td></tr></tbody></table><h3>供应链协同可视化（超兔一体云）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047609294" alt="" title="" loading="lazy"/></p><pre><code>flowchart TD
    subgraph 企业内部系统
        A[超兔一体云CRM] --&gt; B[订单管理]
        A --&gt; C[采购管理]
        A --&gt; D[财务管理]
    end
    subgraph 上游供应商
        B --&gt; E[询价响应]
        C --&gt; F[采购执行&lt;br&gt;发货/物流]
        D --&gt; G[付款发票/对账]
        E &amp; F &amp; G --&gt; H[供应商评分]
    end
    subgraph 下游客户
        B --&gt; I[报价确认/订单确认]
        B --&gt; J[物流订阅/收货确认]
        D --&gt; K[款项发票/投诉处理]
        I &amp; J &amp; K --&gt; L[客户满意度反馈]
    end
    H --&gt; A
    L --&gt; A</code></pre><ul><li><ul><li>*</li></ul></li></ul><h2>四、统计分析：数据驱动的决策支撑体系</h2><h3>核心价值</h3><p>通过多维度数据聚合、可视化报表和AI预测，为企业提供销售效能、库存管理、财务状况的全景洞察。</p><h3>关键能力横向对比</h3><table><thead><tr><th>品牌</th><th>分析引擎类型</th><th>数据覆盖维度</th><th>可视化能力</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>多表聚合/同比环比引擎</td><td>销售/财务/库存/客户全维度</td><td>自定义大屏/驾驶舱</td><td>单日KPI引擎、复杂多表关联BI分析</td></tr><tr><td>Pipedrive</td><td>AI预测分析引擎</td><td>销售全维度</td><td>销售漏斗可视化</td><td>AI驱动交易预测、实时销售报告</td></tr><tr><td>金现代CRM</td><td>对话式BI引擎</td><td>销售/财务/库存/服务多维度</td><td>多维可视化报表</td><td>AI对话生成报表、经营预警</td></tr><tr><td>快启CRM</td><td>效能分析引擎</td><td>销售过程/结果/人员维度</td><td>效能报表可视化</td><td>薪酬激励体系支撑报表</td></tr><tr><td>Infor CRM</td><td>行业化分析引擎</td><td>销售/供应链维度</td><td>行业化报表</td><td>汽车/零售行业定制报表</td></tr></tbody></table><h2>五、自定义能力：适配企业个性化需求的柔性框架</h2><h3>核心价值</h3><p>通过按需订阅、自定义配置和低代码开发，让CRM系统快速适配企业独特业务流程，降低落地成本。</p><h3>关键能力横向对比</h3><table><thead><tr><th>品牌</th><th>功能订阅模式</th><th>菜单/工作台自定义</th><th>业务表/工作流自定义</th><th>特色功能</th></tr></thead><tbody><tr><td>超兔一体云</td><td>功能白名单按需订阅</td><td>三级菜单/多岗位驾驶舱</td><td>自定义业务表/复合工作流</td><td>自定义多表聚合BI分析</td></tr><tr><td>飞书CRM</td><td>生态内按需开通</td><td>飞书工作台自定义</td><td>低代码Apaas定制</td><td>基于飞书应用引擎深度定制</td></tr><tr><td>金现代CRM</td><td>模块按需订阅</td><td>角色专属工作台</td><td>低代码平台全流程定制</td><td>20+行业模板适配</td></tr><tr><td>快启CRM</td><td>模块按需订阅</td><td>基础菜单自定义</td><td>低代码字段/报表定制</td><td>跟进字段/效能报表个性化配置</td></tr><tr><td>Infor CRM</td><td>行业套件订阅</td><td>行业模板固定菜单</td><td>行业化工作流定制</td><td>垂直行业智能补货系统定制</td></tr></tbody></table><h2>六、综合能力雷达图与选型推荐</h2><h3>综合能力评分（满分100）</h3><table><thead><tr><th>品牌</th><th>客户管理</th><th>SFA</th><th>团队协同</th><th>统计分析</th><th>自定义能力</th><th>综合得分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>90</td><td>92</td><td>88</td><td>89</td><td>91</td><td>90</td></tr><tr><td>Pipedrive</td><td>85</td><td>88</td><td>82</td><td>86</td><td>84</td><td>85</td></tr><tr><td>Nimble</td><td>82</td><td>78</td><td>80</td><td>75</td><td>76</td><td>78</td></tr><tr><td>Insightly</td><td>83</td><td>81</td><td>84</td><td>80</td><td>82</td><td>82</td></tr><tr><td>Streak</td><td>75</td><td>72</td><td>70</td><td>68</td><td>70</td><td>71</td></tr><tr><td>Infor CRM</td><td>88</td><td>85</td><td>86</td><td>83</td><td>79</td><td>84</td></tr><tr><td>Zendesk Sell</td><td>80</td><td>83</td><td>78</td><td>77</td><td>75</td><td>79</td></tr><tr><td>快启CRM</td><td>86</td><td>84</td><td>81</td><td>85</td><td>87</td><td>85</td></tr><tr><td>金现代CRM</td><td>87</td><td>86</td><td>85</td><td>88</td><td>89</td><td>87</td></tr><tr><td>管家婆</td><td>84</td><td>82</td><td>83</td><td>82</td><td>80</td><td>82</td></tr><tr><td>飞书CRM</td><td>81</td><td>79</td><td>90</td><td>84</td><td>88</td><td>84</td></tr></tbody></table><h3>选型推荐</h3><ol><li><strong>全流程一体化需求</strong>：超兔一体云，优势是全业务打通、多场景跟单模型、供应链协同、高自定义能力，适合中小微企业低成本实现数字化转型。</li><li><strong>国际业务</strong> <strong>销售自动化</strong>：Pipedrive，优势是AI销售助理、交易进度预测、移动端实时同步，适合有海外业务的企业。</li><li><strong>社交获客导向</strong>：Nimble，优势是社交数据自动整合、客户社交画像，适合依赖社交媒体获客的企业。</li><li><strong>中大型垂直行业</strong>：Infor CRM，优势是行业化模板、SCM套件联动，适合汽车、零售等垂直领域的中大型企业。</li><li><strong>飞书</strong> <strong>生态深度用户</strong>：飞书CRM，优势是原生即时沟通、会议、文档协同，适合已落地飞书办公系统的企业。</li><li><strong>本土</strong> <strong>低代码</strong> <strong>高定制</strong>：金现代CRM/快启CRM，优势是低代码平台、对话式BI、行业适配，适合需要高度定制化的本土企业。</li></ol><h2>结语</h2><p>本次评测显示，不同CRM系统的核心能力差异显著：国际品牌侧重通用销售自动化，本土品牌更贴合国内企业的全流程协同与低代码定制需求，而超兔一体云凭借全业务打通的一体云架构、多场景跟单模型和高自定义能力，在中小微企业CRM选型中具备明显优势。企业需根据自身业务规模、行业属性和数字化阶段，选择最适配的CRM系统。</p>]]></description></item>  </channel></rss>