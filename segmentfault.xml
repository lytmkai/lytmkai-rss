<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[给 Claude 装个仪表盘，时刻监测Token消耗跟任务进度 BugShare ]]></title>    <link>https://segmentfault.com/a/1190000047606908</link>    <guid>https://segmentfault.com/a/1190000047606908</guid>    <pubDate>2026-02-12 11:07:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>最近，Anthropic 推出的命令行工具 <strong>Claude Code</strong> 简直火得一塌糊涂。很多程序员朋友都说，那种在终端里直接指挥 AI 改代码、跑测试的感觉，确实比网页端反复“复制粘贴”要爽得多。</p><p>但用久了，大家普遍发现一个痛点：<strong>“看不见”</strong>。</p><p>你不知道 Claude 现在到底处理了多少 Token，不知道它背地里偷偷读了多少文件，更不知道它那个“大脑”任务列表进行到哪一步了。</p><p>这种感觉就像开夜车没仪表盘，心里总有点没底。</p><p>直到我发现了 <strong>Claude HUD</strong>（作者：Jarrod Watts）。它给 Claude Code 穿上了一层“外骨架”，让它从一个简单的黑框对话框，瞬间变成了科幻感十足的<strong>开发者工作站</strong>。</p><p>今天，咱们就聊聊这个让无数极客直呼“真香”的神器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606911" alt="claude-hud-preview-5-2.png" title="claude-hud-preview-5-2.png"/></p><hr/><h2>一、 什么是 Claude HUD？</h2><p>HUD 原意是“平视显示器”，通常出现在战斗机飞行员的头盔或高端汽车的挡风玻璃上。</p><p><strong>Claude HUD</strong> 干的也是这件事。它是一个专门为 Claude Code 设计的插件，会在你的终端底部常驻一个<strong>状态栏</strong>。</p><p>有了它，你不再需要通过翻看长长的聊天记录去确认进度。它把 Claude 的运行状态、Token 消耗、正在使用的工具、甚至当前的 Git 分支，全都浓缩在屏幕最下方。</p><p><strong>一句话总结：它让 Claude 从一个“黑盒”，变成了一个“透明盒”。</strong></p><hr/><h2>二、 为什么它比原版好用？</h2><p>如果你还在犹豫要不要装，看这三个功能就够了：</p><ol><li><strong>Context 进度条（防“宕机”神器）</strong><br/>Claude 虽然强，但上下文（Context Window）是有上限的。很多时候聊着聊着，AI 开始胡言乱语，往往是因为 Token 满了。<br/>HUD 直接在底部给你一个<strong>电量条一样的视觉反馈</strong>。看到变红了？赶紧重启会话或者清理上下文，再也不用盲目猜测。</li><li><strong>实时“动作监控”</strong><br/>当 Claude 在执行复杂任务（比如重构整个文件夹）时，它会频繁调用 <code>read_file</code>、<code>grep</code>、<code>edit_file</code> 等工具。<br/>在 HUD 里，你可以看到这些动作像流水灯一样闪过。它在读哪行代码？改了哪个文件？你一眼就能掌握全局，这种<strong>掌控感</strong>对开发者来说太重要了。</li><li><strong>任务进度（Todo List）可视化</strong><br/>给 Claude 下达一个大任务时，它会自动拆解成好几个步骤。HUD 会把这些步骤实时显示出来：<br/><code>▸ Fix auth bug (2/5)</code><br/>这就好比进度条，让你知道它现在是卡住了，还是正在稳步推进。</li></ol><hr/><h2>三、 手把手安装教程（三步搞定）</h2><p>安装过程非常顺滑，前提是你已经安装了 <code>claude-code</code>。</p><h3>第一步：添加插件市场</h3><p>在你的 Claude 会话中输入：</p><pre><code class="bash"># 这一步可能需要FQ
/plugin marketplace add jarrodwatts/claude-hud</code></pre><h3>第二步：安装插件</h3><p>接着输入：</p><pre><code class="bash">/plugin install claude-hud</code></pre><p><em>（Linux 用户如果遇到报错，记得先设置一下临时目录权限，官方文档里有贴心提示）</em></p><pre><code class="bash">#⚠️ 在 Linux 系统中，/tmp通常会使用单独的文件系统 (tmpfs)，这会导致插件安装失败，并出现以下错误：
#EXDEV: cross-device link not permitted
#解决方法：安装前设置 TMPDIR：
mkdir -p ~/.cache/tmp &amp;&amp; TMPDIR=~/.cache/tmp claude</code></pre><h3>第三步：初始化配置</h3><p>运行设置命令：</p><pre><code class="bash">/claude-hud:setup
# 执行后可以不进行额外配置（按ESC键取消），后续再配置</code></pre><p>搞定！你会发现终端底部立刻亮起了一排整齐的状态栏，帅气程度瞬间提升几个档次。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606912" alt="image-20260211162341964.png" title="image-20260211162341964.png" loading="lazy"/></p><hr/><h2>四、你的仪表盘，你说了算！</h2><p>很多插件装完就那样了，但 Claude HUD 最骚的地方在于它的<strong>高度自定义</strong>。你只需要在 Claude 会话中输入一行神奇的命令：</p><pre><code class="bash">/claude-hud:configure</code></pre><p>输入这个命令后，你会进入一个“图形化”的配置菜单（就在终端里），完全不需要你去手改代码或 JSON 文件。</p><h3>1. 选择喜欢的“装修风格”</h3><ul><li><strong>Full（全能模式）：</strong> 所有的信息全开，适合那种喜欢“掌控一切”的硬核玩家。</li><li><strong>Essential（极简模式）：</strong> 只保留最核心的活动状态和 Git 信息，清爽不打扰。</li><li><strong>Minimal（迷你模式）：</strong> 只有一个窄窄的 Model 名称和 Token 条，存在感极低。</li></ul><pre><code class="bash"># 默认值（2 行）
[Opus | Max] │ my-project git:(main*)
Context █████░░░░░ 45% │ Usage ██░░░░░░░░ 25% (1h 30m / 5h)
# 第 1 行— 模型、计划名称（或Bedrock）、项目路径、Git 分支
# 第 2 行— 上下文栏（绿色 → 黄色 → 红色）和使用速率限制

# 可选行（通过以下方式启用/claude-hud:configure）
◐ Edit: auth.ts | ✓ Read ×3 | ✓ Grep ×2        ← Tools activity
◐ explore [haiku]: Finding auth code (2m 15s)    ← Agent status
▸ Fix authentication bug (2/5)                   ← Todo progress</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606913" alt="PixPin_2026-02-11_16-26-36.png" title="PixPin_2026-02-11_16-26-36.png" loading="lazy"/></p><h3>2. 细节控的福音</h3><p>您也可以直接在以下位置编辑配置文件<code>~/.claude/plugins/claude-hud/config.json</code>：</p><ul><li><strong>想看 Git 变动？</strong> 开启 <code>showFileStats</code>，连改了几个文件、删了几行都能直接看到。</li><li><strong>嫌路径太长占地方？</strong> 调一下 <code>pathLevels</code>，只显示最后 1-2 级目录。</li><li><strong>想监控 Token 消耗？</strong> 开启 <code>showUsage</code>，实时盯着你的 Pro/Max 会员限额还剩多少。</li></ul><p><strong>配置完后，甚至不需要重启，仪表盘会根据你的选择实时变幻，这种丝滑感真的会上瘾。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606914" alt="PixPin_2026-02-11_16-12-24.png" title="PixPin_2026-02-11_16-12-24.png" loading="lazy"/></p><hr/><h2>五、 真实使用场景：它能帮你省多少事？</h2><ul><li><strong>场景 A：大规模重构代码</strong><br/>当你让 AI 把一个旧项目的 CommonJS 全改成 ESM 时，你会看到 HUD 上的“工具活动”疯狂跳动。如果它读了不该读的 <code>.env</code> 或备份文件，你可以立刻中断，修正指令，避免浪费 Token 和时间。</li><li><strong>场景 B：深陷 Debug 泥潭</strong><br/>当你和 Claude 缠斗了半小时还没修好 Bug 时，看一眼 HUD 的 <strong>Context Health</strong>。如果进度条已经 90% 了，说明对话太长，AI 已经变笨了。这时候果断 <code>/clear</code>，重新开始，往往能秒解。</li><li><strong>场景 C：多任务并行</strong><br/>如果你同时在几个分支上反复横跳，HUD 的 <strong>Git Status</strong> 功能会提醒你当前在哪。配合它显示的 <code>pathLevels</code>，你绝不会在复杂的 Monorepo（大仓库）里迷路。</li></ul><hr/><h2>写在最后</h2><p>在这个 AI 辅助开发的时代，工具的边界就是你能力的边界。</p><p>Claude HUD 并不是改变了 Claude 的智商，它改变的是<strong>你与 AI 协作的交互体验</strong>。从“被动等待结果”到“主动监控过程”，这种转变带来的不仅是效率的提升，更是心智负担的减轻。</p><p>如果你已经用上了 Claude Code，听我一句劝：<strong>这个 HUD 插件，必须安排上！</strong></p><p><strong>如果您觉得这篇文章有帮助，欢迎点赞、转发，让更多小伙伴告别“盲打”时代！）</strong></p>]]></description></item><item>    <title><![CDATA[有奖活动丨首套语音 AI 盲盒邀你来拆！对话式 AI「黑话」周边空降，谁能看懂这些梗？ RTE开发者]]></title>    <link>https://segmentfault.com/a/1190000047606919</link>    <guid>https://segmentfault.com/a/1190000047606919</guid>    <pubDate>2026-02-12 11:06:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606921" alt="" title=""/></p><p>各位 RTE 开发者社区的小伙伴们，这一年，我们聊 ASR、TTS、LLM，在 TEN Framework 的各种模块里反复跳跃。在代码世界里，我们习惯了将 ASR、TTS、LLM 像积木一样拼装成强大的 Voice Agent。</p><p>最近，社区偷偷搞了一件大事，我们把这些「模块」给实体化做成了新春周边盲盒大礼包（你就说这些够不够 Physical 吧？），这不只是一份新年礼包，更是一次 Voice Agent 社区内部的暗号对接。</p><p>快来猜猜里面都有些什么吧～只要你的脑洞够大或者直觉够准，这整套诚意满满的新春周边大礼包，我们就包邮送到家！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606922" alt="" title="" loading="lazy"/></p><p>礼包贺卡写着一段开发者会秒懂的真诚祝愿。</p><h2>🚨深度剧透：这盒子里到底装了啥？</h2><p>看到这个充满科技感的「九宫格」了吗？每一个缩写方盒里，都藏着一件根据「技术梗」定制的实体物件。</p><p>在这里：</p><ul><li><strong>TTS</strong> 不负责合成，只负责让你<strong>入睡</strong>。</li><li><strong>VAD</strong> 不检测语音，只负责让你<strong>清静</strong>。</li><li><strong>S2S</strong> 依然是全场最贵，贵到我们<strong>只能送你一个～</strong>。</li></ul><p>为了让大家更好地发挥脑洞，我们先来打个样！</p><p><strong>WebSocket </strong>➡<strong> WebSock<del>et</del>s</strong></p><p>盒内真身： 袜子（socks）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606923" alt="" title="" loading="lazy"/></p><p><strong>TTS ➡</strong> Time to Sleep 语音合成再忙，也要准时入眠</p><p>盒内真身： 睡眠真丝眼罩</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606924" alt="" title="" loading="lazy"/></p><p><strong>零丢包包 ➡</strong> 不仅是致敬「沈阳站站」的趣味梗，更是对 Voice Agent 稳定交互的终极祝福</p><p>真身：镭射透明包</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606925" alt="" title="" loading="lazy"/></p><p>剩下的 7 个盒子，就看大家的「梗力值」了！</p><h2>👇请听题，猜猜都是什么东西？（谜面在此）：</h2><ol><li><strong>RTC：</strong> Roast the Coffee（会有什么好物？)</li><li><strong>TEN：</strong> Take a SIP（它是 TEN 的拓展能力，也是某种闲适状态？)</li><li><strong>VAD：</strong> Very Anti Dialogue（如果想屏蔽一切对话，你需要什么？)</li><li><strong>S2S：</strong> 某样「太贵了简直买不起」的神秘之物</li><li><strong>STT：</strong> Stick to Task（来自獭獭的职场叮嘱）</li><li><strong>TTD：</strong> Time to Drink（除了酒，还能是什么？)</li><li><strong>LLM：</strong> Long Last Mint（这个提示够明显了吧？)</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606926" alt="" title="" loading="lazy"/></p><h2>🎁怎么玩？（奖品超厚！)</h2><p><strong>1-7 号分别都代表什么物品？</strong> 请在公众号【RTE开发者社区】本文下方评论区留下你的脑洞，我们将送出 <strong>9 套</strong> 价值不菲的新春周边盲盒大礼包！</p><ul><li><strong>【神算奖 x 3】</strong> 猜得最准、最快的技术锦鲤。</li><li><strong>【脑洞奖 x 3】</strong> 虽然你猜错了，但我觉得你说的比我们做的更有趣！</li><li><strong>【阳光普照奖 x 3】</strong> 不想动脑子？只要留言送上纯粹的新年祝福，我们随机抽人「送福气」！</li></ul><h2>⏳活动须知</h2><ul><li><strong>活动时间：</strong> 即日起至春节假期结束。</li><li><strong>揭晓方式：</strong> 年后返工第一天（2 月 24 日）置顶评论区见！</li><li><strong>参与方式：</strong> 关注【RTE开发者社区】公众号，在本文下方评论区留言即可。</li></ul><p>新的一年，愿大家的 Agent 永远不丢包。快去评论区开猜吧！👇</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606927" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606928" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=pI0AP2baISPxq2iSrwMntw%3D%3D.cBoTf1gU6YJ2w8dwRoRR5SQBmFFxbSjZ6Ox2SClJVgU%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606929" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[[Python] 玩转金融API：WebSocket客户端的封装与异常处理实战 EmilyLi ]]></title>    <link>https://segmentfault.com/a/1190000047606967</link>    <guid>https://segmentfault.com/a/1190000047606967</guid>    <pubDate>2026-02-12 11:05:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在开发金融数据采集器或交易机器人时，WebSocket 是绕不开的技术栈。今天想通过一个外汇行情接入的实战案例，和大家深入聊聊 Python 客户端的设计模式与避坑指南。</p><p>一、 技术背景与选型 外汇市场（Forex）具有数据量大（High Volume）、更新频次高（High Frequency）的特点。使用 Python 的 websocket-client 库可以快速搭建客户端，但要做到“生产级稳定”，光会写 ws.run_forever() 是远远不够的。 我们需要解决以下工程问题：</p><p>网络不稳定：如何实现优雅的断线重连？</p><p>粘包与拆包：虽然 WebSocket 协议层面解决了 TCP 的粘包问题，但在逻辑层，我们需要处理 JSON 解析的异常。</p><p>阻塞问题：WebSocket 的接收线程不能阻塞主线程的业务逻辑。</p><p>二、 客户端设计模式 下面的代码展示了一个标准的“订阅-接收”模型。 我们在 on_open 回调中发送鉴权 Token 和订阅指令（Payload），在 on_message 中处理异步推送的数据。这里参考了 AllTick API 的参数设计，其文档中关于 Command ID 和 Sequence ID 的设计是典型的金融协议风格。</p><p>完整代码实现</p><pre><code>import json
import websocket

# 请将下面的 testtoken 替换为你自己的 API Token
WS_URL = "wss://quote.alltick.co/quote-b-ws-api?token=testtoken"

def on_message(ws, message):
    """
    收到行情推送后的回调函数
    """
    data = json.loads(message)
    # 推送消息中通常包含 symbol, price 等字段
    print(f"[行情推送] {data.get('symbol')} 最新价格：{data.get('price')}")

def on_open(ws):
    """
    WebSocket 连接建立后执行订阅
    """
    print("[WebSocket 已连接]")
    # 构造订阅请求
    # cmd_id/seq_id/trace/data 等字段可根据具体文档调整
    subscribe_request = {
        "cmd_id": 22002,
        "seq_id": 1,
        "trace": "subscribe_forex_001",
        "data": {
            "symbol_list": [
                {"code": "EURUSD"},
                {"code": "USDJPY"},
                {"code": "GBPUSD"}
            ]
        }
    }
    ws.send(json.dumps(subscribe_request))

# 创建 WebSocket 应用
ws_app = websocket.WebSocketApp(
    WS_URL,
    on_open=on_open,
    on_message=on_message
)

# 开始运行
ws_app.run_forever()</code></pre><p>三、 进阶：数据流的下游处理 为了演示数据的可用性，我们结合 pandas 将接收到的 JSON 转换为 DataFrame。 这里有一个性能优化的小技巧：不要每来一条数据就创建一个 DataFrame，因为创建对象的开销很大。建议使用一个 list 暂存数据，每积累 100 条或者每隔 1 秒，再批量转换为 DataFrame 进行分析。</p><pre><code>import pandas as pd

# 假设有一批 tick 数据
tick_samples = [
    {"symbol":"EURUSD", "price":1.1035, "timestamp":1670001234},
    {"symbol":"EURUSD", "price":1.1037, "timestamp":1670001240},
]

df = pd.DataFrame(tick_samples)
df["datetime"] = pd.to_datetime(df["timestamp"], unit="s")
print(df)
</code></pre><p>四、 避坑指南（经验之谈）</p><p>Keep-Alive 心跳：很多新手代码跑着跑着就断了，没有任何报错。这往往是因为没有处理 Ping/Pong 心跳，被防火墙或负载均衡器（LB）判定为死连接而切断。务必在 run_forever 中配置 ping_interval 和 ping_timeout。</p><p>SSL 证书验证：在某些内网或测试环境下，如果遇到 SSL 报错，可能需要设置 sslopt={"cert_reqs": ssl.CERT_NONE}，但在生产环境请务必开启验证。</p><p>异常捕获：在 on_message 里一定要加 try...except。因为金融数据偶尔会出现格式错误或缺失字段，如果这里抛出未捕获的异常，整个连接都会断开，导致程序崩溃。</p><p>掌握这些细节，你的爬虫和数据采集程序才算真正入门了金融工程领域。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnUT8" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[堆内存对象的Managed Size具体是如何计算的 侑虎科技 ]]></title>    <link>https://segmentfault.com/a/1190000047606971</link>    <guid>https://segmentfault.com/a/1190000047606971</guid>    <pubDate>2026-02-12 11:05:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>1）堆内存对象的Managed Size具体是如何计算的<br/>2）微信小游戏项目，跑图过程场景加载比较慢如何优化</p><hr/><p>这是第464篇UWA技术知识分享的推送，精选了UWA社区的热门话题，涵盖了UWA问答、社区帖子等技术知识点，助力大家更全面地掌握和学习。</p><p>UWA社区主页：<a href="https://link.segmentfault.com/?enc=eZzd1RGnQcYLpvMEvFRepA%3D%3D.nMlWEbEdeJecCMNUu8kB0l3f1Zn50WvEU3DFmVEcvpw%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA QQ群：793972859</p><p><strong>From 问答社区</strong></p><p><strong>Q：在Memory Profiler观察到一个长度为9的字符串对象Managed Size为40B，显然不止每个字符2字节。请问这个应该是怎么算的？</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606973" alt="" title=""/></p><blockquote><p>A：堆内存对象的Managed Size可以认为是以下四个部分：</p><ol><li>类型对象指针；</li><li>同步块索引；</li><li>专属属性（比如数组和String就有一个int32 length 4B）；</li><li>实例字段。</li></ol><p>针对每种对象这四个部分可能都有不同，最好的办法就是直接看下源码里这个类的构造。比如String：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606974" alt="" title="" loading="lazy"/></p><p>它就是8 (*monotor) + 8(union) + 4(length) + 2( N+1(\0) )，特殊之处就是多了个\0，相当于长度要算N+1。所以总共是22+2*N字节。</p><p>其中为什么每个对象都要额外分配类型对象指针和同步块索引，导致一定的基础占用，原理上可以参考一些官方文档或者社区文章。<br/><a href="https://link.segmentfault.com/?enc=e9DH3LeXUII5FnhaSlIJew%3D%3D.U8ILyMMoePq0gQ3gzhEkRWsdYD0qT2oQVZzEgJGyLQsDg5WB%2BjxurloZz%2FESXlbBwebX4ETs4sL9o8L7ktY%2BQF4KVk93vC8lRDaILzrV7k5hOMBRopd98qD5vAXt4IPUdTwb99qpiwBzc6swNNjOKmKHBROdtvpENhls4alOoDg%3D" rel="nofollow" target="_blank">《.NET 框架内部结构：CLR 如何创建运行时对象 |Microsoft Learn》</a></p></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=ymrJhjuFG%2Fr6jD%2BR%2FXqVNg%3D%3D.2CK%2F2i86JB%2FMtkBEiNfYKY5dKT1e2IuZ18n8iavzbvev2YhDBrqzfUHwXx%2FkVn4XYzohe1uuYbVFu80xaa1yCQ%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=LhLX7AJC53fp37FXohfKxQ%3D%3D.20voPiwYB3wK3%2FEv0%2BWY1C9vEv%2BenIzPbXmAFAprGJ0c8Jj4EcAOGVNhBg7esYoGeu7MqJ0QQC%2BU5Lw%2FHnyWhg%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/69858fdcabed2e338a7dac5e</a></p><hr/><p><strong>From 问答社区</strong></p><p><strong>Q1：目前有个点比较陌生，就是小游戏的每秒资源后台最大同时下载数量，和下载文件大小的指标比较缺失， 这块会影响玩家跑图的过程中场景加载比较慢的问题。  请问有没有官方大数据指标？</strong></p><blockquote><p>A：官方说法是小游戏资源下载并发数为10，超过时底层自动排队；单个请求文件最大不超过100MB（理论最大值，但考虑到带宽，建议单文件2~5MB以内）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606975" alt="" title="" loading="lazy"/></p></blockquote><p><strong>Q2：由于我们是横屏地图，真机上持续控制角色移动10秒以上，停下后，附近场景资源下载大概要等10秒才加载完整。 大概200个资源，请问有什么解决的方法？</strong></p><blockquote>A：这种情况看起来是没有用WXAssetBundle的接口进行AssetBundle加载，一般更推荐用这个，而不是原生的AssetBundle加载接口。WXAssetBundle的加载接口会把网上下载的东西缓存到本地，下次直接从本地加载，而且这个接口会有自动卸载的功能，总体内存占用也会更小一些。其他的优化方法就是资源设置以及打包策略的问题了。</blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=EqMMcMHOQaCAdY7bHivu7A%3D%3D.3XSINDIwWI1n9a9zzLd2KbsgKGsXVOCH5inxVF%2FCdiNcS%2FLMOe2sFelR4N00tPm7uaJTffFO3FymJ1Ez%2BJZVuA%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=L8xeT2CmCG%2B3MaR015pO0g%3D%3D.ndAW%2BJr%2B6flIqoBHc85KzT%2B%2F%2B7lGuHAljs6kkQuSjumvOODVaeFGguFcTo%2BBOjf2S4nMMo7CiMsX2rPlwnNMhw%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/6985999eabed2e338a7dac60</a></p><p><strong>无论是社区里开发者们的互助讨论，还是AI基于知识沉淀的快速反馈，核心都是为了让每一个技术难题都有解、每一次踩坑都有回响。本期分享分别来自UWA AI问答和UWA问答社区，希望这些从真实开发场景中提炼的经验，能直接帮你解决当下的技术卡点，也让你在遇到同类问题时，能更高效地找到破局方向。</strong></p><p>封面图来源于网络</p><hr/><p>今天的分享就到这里。生有涯而知无涯，在漫漫的开发周期中，我们遇到的问题只是冰山一角，UWA社区愿伴你同行，一起探索分享。欢迎更多的开发者加入UWA社区。</p><p>UWA官网：<a href="https://link.segmentfault.com/?enc=eyPfAcjlAuQsPd%2F1zcVAPQ%3D%3D.PF660ov8qh%2Bpn5gE%2F1VCjGFklhoxuysZ6Hd2azjHf8E%3D" rel="nofollow" target="_blank">www.uwa4d.com</a><br/>UWA社区：<a href="https://link.segmentfault.com/?enc=ql59gmSkKwH9Rs%2FEZ2lFyg%3D%3D.XIAZ1i4Roryuj0pjinFkMzHCwSy5tQzCVKKbFTBm4zE%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA学堂：<a href="https://link.segmentfault.com/?enc=pILlSppoiQW37V%2BamkLLbg%3D%3D.s2CNjzgrqbQwgwjwtGg8tvetSQPHNhNRKQ8QUJdY%2FDw%3D" rel="nofollow" target="_blank">edu.uwa4d.com</a><br/>官方技术QQ群：793972859</p>]]></description></item><item>    <title><![CDATA[Tavus 发布视听感知模型 Raven-1，捕捉用户语气、表情及语境；「雷格斯」获投数千万，探索「]]></title>    <link>https://segmentfault.com/a/1190000047606985</link>    <guid>https://segmentfault.com/a/1190000047606985</guid>    <pubDate>2026-02-12 11:04:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606987" alt="" title=""/></p><p>开发者朋友们大家好：</p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、Tavus 发布实时视听感知模型 Raven-1：能读懂讽刺与犹豫，赋予 AI 真实「情商」</strong></p><p><strong>Tavus 公司近日发布了专为实时 AI 打造的视听感知模型 Raven-1。</strong> 该模型旨在解决当前对话式 AI 仅能理解文字而无法感知人类真实意图的痛点。不同于依赖转录文本的传统系统，Raven-1 通过<strong>原生多模态感知系统</strong>，将音频、视觉和时间动态融合为统一的理解框架，从而捕捉用户说话时的语气、表情、犹豫及语境。</p><p>Raven-1 在前代产品 Raven-0 视觉理解的基础上，<strong>实现了音视频流的实时对齐</strong>。其核心能力包括：</p><ul><li><strong>视听融合</strong>：将语调、韵律、面部表情、姿势和注视方向整合为单一感知表征，能准确区分真诚的微笑与讽刺的假笑。</li><li><strong>句子级时间建模</strong>：追踪对话中的情绪和注意力演变，捕捉如挫败感累积或怀疑消退等细微叙事弧线。</li><li><strong>自然语言输出</strong>：生成可解释的自然语言描述而非离散标签，使下游 LLM 能直接理解复杂的情感状态。</li><li><strong>实时响应</strong>：总流水线延迟低于 600 毫秒，且上下文新鲜度（context freshness）保持在 300 毫秒以内，确保 AI 能在恰当时机做出反应。</li></ul><p>该系统还支持通过 OpenAI 兼容模式调用自定义工具，允许开发者定义特定事件（如大笑或注意力转移）以触发相应操作。在 Tavus 的技术栈中，Raven-1 与对话流程模型 Sparrow-1 及情感渲染系统 Phoenix-4 协同工作，形成「感知-响应」闭环，显著<strong>提升了对话的深度与自然度</strong>。</p><p>Raven-1 的应用前景广阔，特别是在医疗健康、教育培训及招聘面试等高风险场景中，它能帮助 AI 实时识别患者不适、学员参与度或求职者的非语言信号。目前，该模型已在 Tavus 平台上线。</p><p>Demo: </p><p><a href="https://link.segmentfault.com/?enc=fIHpXeBwH8oRLqb6QKco1w%3D%3D.xtgRoRdsJ0%2Fkb0jfuXGgp2iil2fHcZ%2BdtOH8YWnQqfM%3D" rel="nofollow" target="_blank">https://raven.tavuslabs.org/</a></p><p>Blog: </p><p><a href="https://link.segmentfault.com/?enc=VAjSYoW%2FL161pQIllkwhag%3D%3D.1%2FSajWyBc6Q5er0PUpJNwupSVfZlHGTXTWFv9xqLH%2BhJI1oDX6UxMNMO%2Bjr27hxrFwOIwvwyFwEirJWryc6OHZDLpvBDNW8RexW8NMhEBAZhxSNbhz32DvdTuUH2Hugh" rel="nofollow" target="_blank">https://www.tavus.io/post/raven-1-bringing-emotional-intellig...</a></p><p>( @tavus@X、@Tavus Blog)</p><p><strong>2、智谱新模型架构曝光：DeepSeek 同款稀疏注意力</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606988" alt="" title="" loading="lazy"/></p><p>日前，据海外博主「Chetaslua」消息，智谱下一代模型（或为 GLM-5）将采用 DeepSeek 同款架构。</p><p>据 Chetaslua 分析，<strong>GLM-5 将采用了 DeepSeek-V3/V3.2 架构，其中包含稀疏注意力机制（DSA）和多 Token 预测（MTP）；模型总参数量达 745B，将会是上一代 GLM-4.7 的 2 倍。</strong></p><p>值得一提的是，近期有一个名为「Pony Alpha」的神秘模型上线全球模型服务平台 OpenRouter，并且引发较高热度。其中不乏有人分析指出，该模型或为智谱新的模型。</p><p>而据第一财经消息，智谱目前有相关保密项目在推进中，该神秘模型，是智谱即将发布新一代模型 GLM-5。</p><p>据悉，OpenRouter 合作方 Kilo Code 曾透露，Pony Alpha 是「某个全球实验室最受欢迎的开源模型的专项进化版」。</p><p>对此，报道指出，Pony Alpha 更有可能是 DeepSeek-V4 或者智谱即将发布的新一代模型 GLM-5。</p><p>( @APPSO)</p><p><strong>3、vLLM 推出流式输入与 Realtime API：打破批处理限制，解锁低延迟实时推理</strong></p><p>vLLM 联合 Meta 与 Mistral AI 推出流式输入功能及 WebSocket 「Realtime API」。该更新打破了先接收完整请求、再开始推理的传统范式，<strong>允许模型在用户说话或数据传输过程中同步处理</strong>，为语音助手、实时转录及机器人控制等低延迟场景提供了原生支持。</p><ul><li><strong>「StreamingInput」增量接口</strong>：核心输入对象从静态 Prompt 升级为异步生成器。开发者可以像「喂料」一样，将数据碎块随时间 yield 给引擎，实现边输入边处理。</li><li><strong>「Anchor Request」锚定会话模式</strong>：会话启动时建立一个长期存在的「锚定请求」。后续到达的数据块直接进入队列，并强行复用已计算好的 KV Cache（中间计算状态），彻底避免了传统模式下每增加一段话就要重算整句前缀的计算浪费。</li><li><strong>智能缓存衔接策略</strong>：在处理新输入块时，引擎会保留之前生成的大部分 Token 缓存。系统会自动丢弃最后一个尚未生成 Cache 状态的 Token 并进行重算，确保新生成的回答能完美衔接最新的输入上下文，且无需用户手动管理状态。</li><li><strong>兼容 OpenAI 标准的 Realtime API</strong>：通过 /v1/realtime 端点提供 WebSocket 双向通信。支持 16kHz 的 PCM16 原生音频流输入，服务端可实时返回 transcription.delta（转录增量）和文本/音频响应，支持「听」与「说」同时并发。</li><li><strong>模型架构适配要求</strong>：该特性需配合具备「因果注意力」机制的模型（如「Voxtral」）。这类模型在处理当前信息时无需参考后续未到达的内容，结合滑动窗口注意力可实现无限长度流式推理。</li></ul><p>功能已在 vLLM 最新版本中开源。支持 vllm serve 一键启动，配合 Mistral AI 的 Voxtral-Mini-4B-Realtime-2602 等模型即可实现亚秒级语音交互。</p><p>相关链接：</p><p><a href="https://link.segmentfault.com/?enc=%2F4CNErTW4emPnNkQwVLCgw%3D%3D.Zcr%2FET13ErFOq7G0IuXw95lndkG3k9zCznKxPZXHm37H12zwyMRhQnIiKBTJXtqhTrbRHRBD33itXfTl9Tm%2Fdw%3D%3D" rel="nofollow" target="_blank">https://blog.vllm.ai/2026/01/31/streaming-realtime.html</a></p><p>GitHub: </p><p><a href="https://link.segmentfault.com/?enc=lbQpIEAEkF9X1a33LLpIzQ%3D%3D.H8L%2F%2BQnZEjlyvMvUiDQAthDF%2FNqI2vcZnbnFy5H47L%2Bc4i6dcJw3PEpN6Jo8qi50" rel="nofollow" target="_blank">https://github.com/vllm-project/vllm</a></p><p>( @vLLM)</p><h2>02有亮点的产品</h2><p><strong>1、DuckDuckGo AI 语音聊天上线，承诺不存储音频</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606989" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606990" alt="" title="" loading="lazy"/></p><p>DuckDuckGo 昨日发布公告，<strong>宣布其 AI 聊天机器人平台 Duck.ai 新增实时语音聊天功能，主打极致隐私保护。</strong></p><p>与市面上其他语音助手不同，该功能的核心卖点在于 <strong>「隐私优先」</strong> 的架构设计。用户通过加密通道与大语言模型（LLM）进行自然对话，无需担心语音数据被后台监听或二次利用。</p><p>为了兼顾智能体验与数据安全，DuckDuckGo 采用了独特的「中间人」模式。虽然语音聊天的底层智能由 OpenAI 提供支持，但 DuckDuckGo 在用户与 OpenAI 之间建立了一道防火墙。</p><p>官方强调，双方均受严格合同限制：DuckDuckGo 匿名化处理音频，OpenAI 仅负责处理请求，严禁保留数据。这意味着，<strong>该平台不会存储用户的聊天音频，也不会调用内容用于训练 AI 模型</strong>。</p><p>为消除用户疑虑，DuckDuckGo 公布了具体的隐私保护细节：</p><ul><li>临时处理：音频流仅在说话时传输，会话结束后即刻销毁；</li><li>零训练：用户的声音和 AI 的回复均不会喂给算法模型；</li><li>加密传输：全程通过 WebRTC 和中继服务器进行高强度加密；</li><li>零留存：无论是 DuckDuckGo 还是 OpenAI，在通话结束后都不会保留任何记录。</li></ul><p>在使用门槛上，Duck.ai 保持了开放策略：用户无需注册账号即可免费体验（受每日额度限制）。对于重度用户，DuckDuckGo 推出了每月 10 美元（现汇率约合 69.3 元人民币）的订阅服务，不仅大幅提升了使用限额，还附带了个人信息移除服务以及身份盗窃恢复服务等。</p><p>（@IT 之家）</p><p><strong>2、奇妙拉比获投数千万，探索「硬件+IP+AI」新生态</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606991" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606992" alt="" title="" loading="lazy"/></p><p>奇妙拉比日前完成数千万人民币天使轮融资，由锦秋基金领投，首程控股联合投资，沧澜资本担任独家财务顾问。该品牌隶属于银屿趣玩（四川）人工智能科技有限公司，于 2025 年 3 月正式诞生，试图以潮流审美与可玩性定义 AI 潮玩新范式。</p><p>区别于传统 AI 玩具或陪伴型产品，奇妙拉比强调潮流审美、收藏价值与强 IP 人格，<strong>构建了「本体硬件 + 角色分支 + 配件生态 + 周边收藏 + 内容更新」的产品体系</strong>。</p><p>其首个核心 IP 雷格斯（RAGUS &amp; WHITE）于 2025 年 6 月推出，<strong>坚持「潮玩优先，AI 后置」逻辑</strong>，通过 AI 赋予角色稳定人格与长期记忆，使其能随用户互动而「生长」。</p><p>市场数据验证了这一品类价值：雷格斯在近乎零推广下，预售当日引发小程序崩溃，下单量数千台；在线下，其在北京陶朱新造局等空间长期稳居销量前三。新品「阴阳双生」系列也于近日亮相，基于多元宇宙设定展示了同一人格的不同演化可能。</p><p>本轮融资将重点投入两大方向：</p><ul><li><strong>内容生态建设</strong>：持续完善 AI 潮玩宇宙的世界观与角色体系，深化用户与角色的长期互动。</li><li><strong>SKU 矩阵扩展</strong>：推进多形态产品与玩法创新，探索与成熟 IP 及艺术家的授权合作。</li></ul><p>联合创始人景林彦认为，传统潮玩体验峰值集中在拆箱瞬间，<strong>而 AI 潮玩通过角色成长驱动复购</strong>，是行业的下半场。</p><p>投资方锦秋基金与首程控股均看好 AI 潮玩作为新品类的潜力，认为其结合了 AI 与 IP 的优势，具备极大的想象空间。首程控股联席总裁叶芊特别指出，奇妙拉比团队在资源匮乏下展现出的极强战斗力与创新力，是其投资的核心原因之一。</p><p>（@IPO 早知道）</p><p><strong>3、YC 孵化生产力工具 VoiceOS 上线：支持跨应用语音指令与 Prompt 自动优化</strong></p><p>由 YC 投资的语音生产力工具 VoiceOS 正式上线。该产品被定义为一款<strong>通用的语音操作系统</strong>，试图通过语音交互将工作效率提升至新的层级，解决传统键盘输入带来的效率瓶颈。</p><p>VoiceOS 团队认为，尽管键盘是目前主流的输入工具，但它往往成为连接大脑与数字世界之间的阻碍。用户在将想法转化为屏幕文字的过程中，面临着精神负担重、纠错耗时以及应用切换导致思路中断等问题。很多时候，当用户完成了打字、重组语言、修正错别字和调整格式后，最初的灵感火花已经消逝。</p><p>针对这一痛点，VoiceOS 并未止步于传统的语音转文字功能，而是构建了一个能理解用户意图的通用语音界面。<strong>它能够即时将口述的想法转化为经过润色的输出，并自动处理格式、语气、语法和语境。</strong>其核心功能包括：</p><ul><li><strong>即时回复</strong>：用户无需打字或过度思考，只需口述意图（如「要求明天重新安排时间」），系统即可自动生成完整回复。</li><li><strong>优化提示词</strong>：能够轻松地将用户杂乱的思维碎片，转化为适用于 AI 工具的精准提示词。</li><li><strong>全平台兼容</strong>：支持 Slack、Gmail、Notion、ChatGPT、Cursor 等任意应用程序，且无需进行额外设置。</li></ul><p>该项目的创始人 Kai 和 Jonah 在过去 7 年中积累了丰富的语音 AI 开发经验，涵盖从消费级产品到世界 500 强企业的部署。他们指出，此前语音技术的发展瓶颈并非在于模型能力，而在于交互界面。在通用人工智能（AGI）逐渐成为现实的背景下，<strong>键盘可能不再是人类与技术交互的主要方式，语音将取而代之成为新的操作系统</strong>。</p><p>( @ycombinator@X、@VoiceOS Blog)</p><p><strong>4、被迫改名、发货推迟至 2027：奥特曼与 Jony Ive 的 AI 硬件项目遇阻</strong></p><p>据 Gizmodo 援引 Wired 的报道，OpenAI 首席执行官 Sam Altman 与前苹果设计师 Jony Ive 合作开发的 AI 硬件项目正面临多重阻碍，问题主要集中在<strong>品牌命名、发货时间表以及技术研发</strong>三个方面。</p><p>首先是品牌命名问题。法庭文件显示，这家新成立的公司在尝试以「io」命名时遭遇了法律障碍。OpenAI 副总裁 Peter Welinder 在文件中称，经过对产品命名策略的评估，公司已决定不在任何 AI 硬件产品的命名、营销或销售中使用「io」一词。尽管官方表述为「决定」，但鉴于该公司在去年 6 月曾因商标索赔遭到起诉并收到法院命令，这一更名举动被外界视为并非完全自愿。</p><p>其次，产品的上市时间表大幅推迟。尽管此前有《The Information》和 Axios 的报道称 OpenAI 最快可能在今年揭晓其设备，但最新消息显示，<strong>这家目前暂无名称的公司要等到 2027 年 2 月才会开始正式发货</strong>。这使得原定于今年下半年的产品展示充满了不确定性。</p><p>此外，据《金融时报》此前报道，该项目的研发过程也面临着具体的软硬件挑战：</p><ul><li><strong>算力瓶颈</strong>：团队在整合足够的计算能力以支持设备运行方面遇到了困难。</li><li><strong>交互缺陷</strong>：设备核心的语音助手功能尚不完善。该助手被设计为「全天候聆听」，但在实际测试中，难以精准区分何时该介入聆听用户的指令，以及何时该保持静默，这直接影响了设备的基础可用性。</li></ul><p>目前的 AI 硬件市场环境并不乐观，Humane 的 AI Pin 和 Rabbit R1 等先发产品均因未能兑现功能承诺而遭遇挫折。Altman 和 Ive 的团队不仅需要解决上述技术与法律难题，还需面对智能手机这一成熟形态的强势竞争。</p><p>( @Gizmodo )</p><h2>03有态度的观点</h2><p><strong>1、Anthropic：AI 智能体将重塑开发全流程</strong></p><p>Anthropic 近日发布了一份名为《2026 Agentic Coding Trends Report》的重磅报告。</p><p>报告指出，<strong>随着 AI 编程能力从「实验性工具」向「生产力系统」的演进</strong>，软件开发行业正站在一场「地壳运动般」变革的边缘。</p><p>报告预测，到 2026 年，软件开发将不再局限于人类编写代码，而是转向由人类编排 AI 智能体团队来完成。这一转变将导致传统的软件开发生命周期发生剧烈坍缩，项目交付时间将从数周缩短至数小时。</p><p>报告中引人注目的技术趋势是 AI 智能体架构的演进。目前的单体智能体受限于上下文窗口和单线程处理能力，往往只能处理线性任务。但 Anthropic 指出，<strong>2026 年将是「多智能体协同」爆发的一年。</strong></p><p>有趣的是，报告中还提到，尽管 AI 承担了更多执行层面的工作，但报告揭示了一个关键的「协作悖论」：AI 使用率高，但完全放权率低。</p><p>Anthropic 的内部研究显示，虽然工程师在 60% 的工作中都会使用 AI，但他们表示能够「完全通过」的任务比例仅为 0-20%。这表明，AI 目前更像是一个需要持续监督的合作伙伴，而非完全自动化的替代品。</p><p>报告指出，随着 AI 能力的提升，人类的监督方式也将发生质变——从「逐行审查」转向「基于智能体的质量控制」，即利用 AI 智能体来审查其他 AI 生成的大规模代码，人类仅需关注高风险和战略性的部分。</p><p>( @APPSO)</p><h2>04 Real-Time AI Demo</h2><p><strong>1、AI 界的 WWE？Agent Wars 上线 Beta 版：围观 AI 实时编程对决，支持 SOL 下注</strong></p><p>开发者 Joaki 近日推出了名为 Agent Wars 的平台，目前处于 Beta 测试阶段。该项目主打 AI 智能体之间的实时编程对决，并允许观众使用 SOL 代币对比赛结果进行下注。</p><p>在该平台上，<strong>核心互动机制分为人类观众与 AI 智能体两端</strong>。对于观众而言，他们可以观看 AI 智能体实时解决编程挑战，并在比赛开始前投入 SOL 押注获胜方。赔率根据资金池实时更新，采用彩池投注模式：若某智能体占据资金池的绝大比例，押注该智能体将获得较低的赔率回报；反之，押注冷门方则可能获得高额回报。<strong>获胜的投注者将按比例瓜分输家资金池的 95%</strong>。</p><p>对于 AI 智能体开发者，参与流程包括通过 API 注册并创建智能体档案。智能体需设置「心跳」机制，每 30 分钟检查一次战斗匹配。一旦匹配成功，智能体便会接收到涵盖编程、算法实现或调试等不同难度（从简单到专家级）的挑战任务。</p><p>每场对决遵循一套标准化的流程：</p><ul><li><strong>准备阶段</strong>：智能体匹配完成，投注通道开启，观众在战斗开始前下注。</li><li><strong>实战阶段</strong>：智能体接收题目并现场编写代码，此时投注通道关闭。</li><li><strong>裁判阶段</strong>：由 AI 裁判根据正确性、代码质量和效率对解决方案进行评估；若评分相同，响应速度更快者胜出。</li><li><strong>结果公示</strong>：系统宣布获胜者，并自动分发奖励。</li></ul><p>值得注意的是，为了激励开发者参与，<strong>获胜智能体的所有者将直接获得总投注池的 5% 作为奖励</strong>。在 Beta 测试期间，平台暂不收取任何服务费。开发者若需提取收益，可通过个人资料页面将 SOL 直接转入钱包。目前，任何 AI 智能体均可通过公开的技术文档申请加入这场竞技。</p><p>体验链接：</p><p><a href="https://link.segmentfault.com/?enc=ph6%2F49502x9CNfP9NWnAXw%3D%3D.%2F9iY8o6whfklserI0%2BK%2BeGFbczQh29hYmTgZwIy99EQ%3D" rel="nofollow" target="_blank">https://www.agentwars.gg/</a></p><p>（@itsjoaki@X、@Agent Wars）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606993" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606994" alt="" title="" loading="lazy"/></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606995" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[APS排产的拓展属性管理，轻松应对企业的复杂生产需求 软件部长 ]]></title>    <link>https://segmentfault.com/a/1190000047607005</link>    <guid>https://segmentfault.com/a/1190000047607005</guid>    <pubDate>2026-02-12 11:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代制造业中，同一产品往往有多种规格配置，比如汽车的颜色、配置等级，电器的不同型号等。这些多样化需求如何体现在生产计划中？<br/>JVS-APS排产系统的拓展属性管理模块负责对这些产品特征进行统一管理和分类，使生产计划能够基于产品多维度特性进行精准排产，满足现代制造业个性化定制的需求。<br/>以下解读所用到的是开源的JVS智能排产系统。<br/>JVS-APS系统是由软开企服开源的一款智能排产系统，系统聚焦于离散制造行业（如汽车、电子、机械、航空航天等）及流程制造行业（如化工、食品、医药等），面向中大型企业客户，通过AI驱动的智能算法，实现生产计划与排程的高效性、准确性、敏捷性，帮助企业提升设备利用率、降低库存成本、缩短交付周期，实现精益生产与数智化转型。<br/>拓展属性是指在生产一个产品时，该产品所带有的一系列特征或标识。好比汽车有不同系列、颜色、尺寸大小等，这些都可称为汽车的属性。所以属性管理模块即是对生产产品的自带特征或标识作为属性进行管理。</p><h2>功能说明</h2><p>• 属性新增<br/>通过系统对产品自带属性进行新增。<br/>• 属性查看<br/>将所有属性集中放于一个列表之中，便于查看与管理。</p><h2>操作步骤</h2><p>1、点击【基础数据】下面的【属性管理】，进入属性管理页面。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607007" alt="图片" title="图片"/><br/>2、点击【新增属性】，即可进入新增页面。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607008" alt="图片" title="图片" loading="lazy"/><br/>3、新增属性页面需输入对应的属性名(需用户手动输入，可自定义属性名称)、属性key（默认为属性名的拼音，也可自定义key值）、属性校验（是否必填校验）、属性类型（可选择 文本框、数字框、下拉框、单选、多选 这几种类型）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607009" alt="图片" title="图片" loading="lazy"/><br/>4、填写完相关信息后点击提交即会出现在列表页中，可对数据进行二次编辑或删除。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607010" alt="图片" title="图片" loading="lazy"/><br/>5、此外还有查询功能，可选择输入属性名或属性key或选择属性类型这几种方式，查出其中对应一条或相同类型的多条数据。输入完成后点击查询即可查到相关数据信息。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607011" alt="图片" title="图片" loading="lazy"/><br/>拓展属性管理看似是很简单的功能，其实无论是离散制造还是流程制造行业，合理利用，不仅解决了多品种小批量生产的复杂性问题，还可以帮助企业实现从“人治”到“数智”的转型升级。</p>]]></description></item><item>    <title><![CDATA[文本差异对比器 在线工具分享 兔子昂 ]]></title>    <link>https://segmentfault.com/a/1190000047607063</link>    <guid>https://segmentfault.com/a/1190000047607063</guid>    <pubDate>2026-02-12 11:03:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在日常工作和学习中，我们经常需要对比两段文字或代码及其差异。</p><p>比如程序员在排查 Bug 时，需要对比修改前后的代码片段；文案编辑在校对文章时，需要找出修订的具体位置；或者是运维人员在对比两个配置文件的细微不同。</p><p>如果仅仅依靠肉眼去逐行扫描，不仅效率极低，而且非常容易看走眼，漏掉关键的差异点。</p><p>今天给大家分享一个我开发的在线工具——<strong>文本差异对比器 (Text Diff Checker)</strong>，它可以帮你一键快速找出两段文本的所有不同之处。</p><blockquote>在线工具网址：<a href="https://link.segmentfault.com/?enc=qIEV1EvO8PXMKZ1zb8Plpw%3D%3D.bscNXYfbO8GSyBKsINUyrQvA1%2B8oJvT7hw8LwoutStsSzT2pAFxOs1AcMpaOBnBb" rel="nofollow" target="_blank">https://see-tool.com/diff-checker</a>  <br/>工具截图：  <br/><img width="723" height="419" referrerpolicy="no-referrer" src="/img/bVdnUVZ" alt="" title=""/></blockquote><h2>工具核心功能</h2><p><strong>1. 多种对比模式</strong><br/>工具支持三种精度的对比模式，满足不同场景的需求：</p><ul><li><strong>按行对比 (Line)</strong>：适合代码或长文章，快速定位哪些行发生了变化。</li><li><strong>按词对比 (Word)</strong>：适合英文文章或短语，精确到单词级别的差异。</li><li><strong>按字符对比 (Char)</strong>：适合对比密钥、哈希值或细节要求极高的文本，精确到每一个字符。</li></ul><p><strong>2. 直观的高亮显示</strong><br/>采用经典的“红删绿增”配色方案：</p><ul><li><strong>红色背景</strong>：表示左侧文本中被删除或修改的内容。</li><li><strong>绿色背景</strong>：表示右侧文本中新增或修改后的内容。<br/>差异一目了然，不需要任何学习成本。</li></ul><p><strong>3. 灵活的辅助选项</strong></p><ul><li><strong>忽略空格</strong>：有时候仅仅是缩进不同，勾选此项可以忽略空白字符的差异。</li><li><strong>忽略大小写</strong>：不区分大小写的比对。</li><li><strong>显示行号</strong>：方便定位具体位置。</li></ul><h2>技术实现 (Vue 3)</h2><p>作为一个前端开发者，我使用目前流行的 <strong>Vue 3 (Composition API)</strong> 框架配合 <strong>Tailwind CSS</strong> 编写了这个工具。</p><p><strong>纯前端处理，安全无忧</strong><br/>值得一提的是，这个工具的所有计算逻辑完全在你的本地浏览器中运行。这意味着：<strong>你输入的任何文本（无论是机密代码还是私人文章）都不会被上传到服务器</strong>。你可以放心大胆地使用它来对比敏感数据，隐私安全有绝对保障。</p><p><strong>响应式设计</strong><br/>工具对移动端和桌面端都做了适配。无论你是在电脑前工作，还是临时用手机查看差异，都能获得良好的使用体验。</p><h2>如何使用</h2><ol><li>打开工具页面，你会看到左右两个输入框。</li><li>在左侧输入框粘贴“原始内容”，在右侧输入框粘贴“修改后的内容”。</li><li>根据需要选择对比模式（默认是“按行对比”）。</li><li>点击中间的 <strong>“开始对比”</strong> 按钮。</li><li>页面下方会立即生成对比报告，显示增加/删除的行数统计，并高亮展示差异详情。</li></ol><h2>结语</h2><p>这个小工具虽然功能单一，但在关键时刻能帮我们节省大量的时间和精力。如果你也受够了用肉眼“且”不同，不妨试试这个文本差异对比器。</p><p>欢迎大家使用并提出宝贵意见！</p>]]></description></item><item>    <title><![CDATA[当轻帆云 ITSM 遇上 OpenClaw：一种“去 UI 化”的生产实践 云智慧 ]]></title>    <link>https://segmentfault.com/a/1190000047607066</link>    <guid>https://segmentfault.com/a/1190000047607066</guid>    <pubDate>2026-02-12 11:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026年开年， 一个名为 OpenClaw 的开源 AI 智能体项目在开发者社区引发广泛关注。不同于云端对话式助手，OpenClaw 直接在本地运行，能够操作软件、收发邮件、执行代码，甚至可以自动完成一系列跨应用的任务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607068" alt="图片" title="图片"/></p><p>这种“本地执行”能力能否在企业 ITSM 场景中找到用武之地？<a href="https://link.segmentfault.com/?enc=2JIQNNs1O8KuSdtF9AI82g%3D%3D.Vm0S2VydRAGz%2F%2FUUr%2B7NeEFbG1yqxbZ%2BXL%2B03TkLeE64oRK0%2BMFhyE7xhY4smG8NgXlqK5mSFZISseXI%2FLJ%2BbA%3D%3D" rel="nofollow" target="_blank">轻帆云 ITSM</a> 团队尝试将这一能力引入服务管理流程，探索一种减少界面交互、提升服务效率的新可能。</p><h2>从图形界面到自然语言：轻帆云的“去 UI 化”探索“</h2><p>最好的 UI，就是没有 UI。”——当服务足够智能，用户便无需与界面打交道。</p><p>在传统 ITSM 模式中，用户需主动登录系统、浏览服务目录、填写表单、提交工单，并在后续反复登录以跟踪进度、处理驳回、催办超时、完成评价——整个过程高度依赖界面操作与人工干预，使用门槛高、体验割裂。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607069" alt="图片" title="图片" loading="lazy"/></p><p>在已跑通飞书/Telegram 端的实验室环境中，轻帆云通过集成 OpenClaw，实现了 IT 服务流程的“去 UI 化”重构：用户只需在对话中自然表达需求，例如 “帮我开下 CRM 权限，我要处理上个月的财务报表”，系统即可自动解析意图、创建工单，并全程驱动后续流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607070" alt="图片" title="图片" loading="lazy"/></p><p>工单临近超时时，系统会自动向处理人发起催办；若被驳回，则尝试分析原因、补充必要信息并重新提交。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607071" alt="图片" title="图片" loading="lazy"/></p><p>任务完成后，结果将即时推送至用户，满意度评价也依预设规则自动完成。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607072" alt="图片" title="图片" loading="lazy"/></p><p>用户无需再登录后台，也无需记忆流程节点。这一变革将服务入口从“系统后台”迁移至“日常对话”，把被动查询转变为主动服务，让智能 IT 服务真正走向“无感、无扰、无界面”。</p><h2>Opencalw驱动的轻帆云ITSM：“去 UI 化”探索的实现路径</h2><p>“去 UI 化”的本质，并非消除界面，而是让用户在发起和跟踪 IT 服务时，无需主动打开或操作 ITSM 系统界面。这一体验的背后，是一套端到端的自动化执行链路。</p><p>用户在 Telegram 中的一句话，如何驱动复杂的 ITSM 流程？其核心在于以下四个环节的紧密协同：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607073" alt="图片" title="图片" loading="lazy"/></p><h3>1、语义解析 —— OpenClaw 的“大脑”</h3><p>用户输入“帮我申请 CRM 权限”，OpenClaw 即时识别意图（如 dosm_create），并将模糊请求拆解为结构化参数（例如 system: CRM），替代了传统流程中手动浏览目录、填写表单的操作。</p><h3>2、身份穿梭 —— 合规的安全桥梁</h3><p>系统自动捕获用户的 Telegram ID，并精准映射为轻帆云内部真实的 userId（如 19733）。所有操作均以真实身份执行，满足企业审计与权限管控要求，实现安全可信的“AI 代用户操作”。</p><h3>3、协议封包 —— AI 的“手脚”</h3><p>OpenClaw 调用预定义 Skill，将参数封装为符合轻帆云接口规范的 JSON 表单（含必要转义与认证字段），并通过 POST /orderCreate 请求直接创建工单，全程绕过前端界面，实现无感系统交互。</p><h3>4、感知反馈 —— 即时的服务闭环</h3><p>工单状态变更后，轻帆云通过 Webhook 实时回传数据，OpenClaw 将系统响应转化为自然语言消息（如“单号 WO123 已生成，处理人正在审批中”），主动推送至用户对话窗口，无需用户主动查询。</p><p>正是这套链路，让用户无需登录、无需填表、无需查进度，即可完成 IT 服务主流程——界面依然存在，但已退居幕后，由 AI 代理完成交互。</p><h2>轻帆云的 AI 能力中台：让“去 UI 化”快速落地“</h2><p>去 UI 化”体验的快速实现，源于轻帆云早已构建的 AI 能力中台——一个面向<a href="https://link.segmentfault.com/?enc=Jtg%2BwH0Dg6uidc6HNpp7kw%3D%3D.096j3qq%2BkakoLtzdBRehf3J6Uet7pNA41xX25lBWFPy1HZYId3lkViCyUeYTSh%2BI" rel="nofollow" target="_blank">企业级 ITSM</a> 场景设计的开放智能架构。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607074" alt="图片" title="图片" loading="lazy"/></p><p>该中台底层兼容主流大模型服务（如 DeepSeek、通义、讯飞星火等），中层集成了自研的 RAG 引擎、长短时记忆机制、工作流编排系统与多 Agent 协同框架，完整覆盖从自然语言理解到跨系统执行的全链路需求。</p><p>正因具备这一标准化、可扩展的智能底座，<a href="https://link.segmentfault.com/?enc=6kAmAWFfwQkq%2FlOSoC0qvA%3D%3D.nX4Ev0%2Fm0Loy0cI%2FguqWXfPC7xD5k4tIWYu3ul9azOtq1lhq9oixKD5l8QQHfjQN" rel="nofollow" target="_blank">好用的轻帆云IT服务管理平台</a>无需为 OpenClaw 进行定制开发，仅通过配置即可将其作为外部智能体接入，驱动工单创建、状态追踪等核心流程。整个过程不改造原有 ITSM 系统，不影响现有业务运行，真正实现了“去 UI 化”能力的敏捷交付。</p><h2>前沿探索·驱动轻帆云ITSM持续进化</h2><p>从大模型DeepSeek到智能体OpenClaw，<a href="https://link.segmentfault.com/?enc=XmboMbx5tQCF1at2neimPQ%3D%3D.W8QxyreofphYv3yR2D32pG4t7jHGfp7DlAw2r8v0Wi2Lf0K%2FIITNzUTVyjMWGD7L" rel="nofollow" target="_blank">国内主流企业级智能IT服务管理平台——轻帆云</a>始终致力于将前沿 AI 能力转化为可落地的产品价值。正是这种对新技术的高效集成能力，让“去 UI 化”等创新体验得以快速实现，并持续推动 ITSM 服务向更智能、更无感的方向演进。</p><p>详询热线：400-666-1332</p>]]></description></item><item>    <title><![CDATA[从分库分表到原生分布式：高德基于 OceanBase 的数据底座演进之路 OceanBase技术站 ]]></title>    <link>https://segmentfault.com/a/1190000047607099</link>    <guid>https://segmentfault.com/a/1190000047607099</guid>    <pubDate>2026-02-12 11:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><strong><em>高德面对数据存储扩展和成本效率、极致的性能要求、容灾高可用保障三大挑战，传统分库分表架构已无法支撑海量业务数据。其选择 OceanBase 原生分布式数据库升级核心业务，依托其高压缩、异地多活、MySQL 兼容能力，实现零感知平滑迁移。升级后存储成本、响应时间、研发效率均得到显著优化，容灾能力增强，完成从分库分表到原生分布式的数据底座演进。</em></strong></p><p>提起高德，许多人首先想到的是其精准的数字地图。但如今的高德，早已不仅仅是一款导航工具，而是发展成为覆盖衣食住行等全场景的“国民级”出行服务平台。</p><p>数据显示，高德的日活跃用户数最高已突破 3.6 亿人次（截至 2025 年 10 月 1 日），日均活跃用户也稳定在 1.5 亿人次以上。如此庞大的用户体量，为高德的业务系统带来了海量数据存储与实时响应的双重挑战，尤其在五一、国庆等出行高峰期间，后台系统承受的压力更为显著。</p><p>面对持续增长的业务压力，传统基于分库分表的数据库架构已无力支撑。在这一背景下，高德开始探索将数据库升级至新一代分布式数据库 OceanBase 的可行路径。其中，“足迹”与“云同步”等核心业务，成为此次升级的先行者。</p><h2>高德地图在线服务应对海量存储三大核心挑战与思考</h2><p>作为高德地图出行服务技术负责人，李岩的核心职责之一是应对各业务快速发展带来的高并发与海量存储挑战。</p><p>“用户每一次路线规划、每一次地点搜索的背后，都是对系统稳定性与体验感的极致考验。我们必须确保每一次请求都稳如磐石，用户体验如丝般顺滑。”李岩表示。</p><p>高德的技术体系构建在云原生基础设施之上，依托云的弹性伸缩能力，可根据业务需求快速实现资源的扩缩容，从而为上层应用的开发与部署提供极大的灵活性。高德强大的云底座也让李岩能更专注于应对大并发、低延迟的海量存储带来的技术压力。</p><p>李岩将高德应对海量数据所面临的核心挑战归纳为三方面：</p><p><strong>一是数据存储扩展和成本效率，即如何合理部署与分配业务数据。</strong>这不仅关乎存储成本，更直接影响业务模型能否实现低成本的扩展与迭代，进而影响整体业务效率。</p><p><strong>二是极致的性能要求。</strong>高德的在线服务必须在高 QPS/TPS 下保持低耗时的响应，并能在需要时实现平滑的弹性伸缩。同时，系统还需具备故障快速发现、诊断与恢复的能力。</p><p><strong>三是容灾高可用保障。</strong>系统需支持在出现故障时实现“一键”逃逸能力与逃逸的力度，并尽可能让用户对此过程无感知。</p><p>实际上，无论是“足迹”还是“云同步”，这些核心业务系统数据库升级的目标正是为了应对上述三个核心挑战。</p><h2>选择 OceanBase：解决千亿级数据规模的分库分表架构瓶颈</h2><p>高德的“足迹”服务是一项自动记录用户历史出行与停留地点的位置服务。它通过持续收集定位数据，生成个人专属的出行时间轴，帮助用户回顾、管理和分享自己的移动路线。目前，数据规模已超过 7000 亿条，存储规模突破 360TB，读写并发峰值平均每秒超过 27 万次请求。</p><p>“‘足迹’需要在 10 毫秒内响应用户请求，是一种典型的海量数据、高并发、低延迟业务场景，对底层数据库系统要求极高。”李岩表示。</p><p>“足迹”原先采用分库分表架构。虽在一定程度上缓解了数据量的问题，但随着业务不断迭代、数据量不断增长，其弊端也逐渐凸显：例如大表以及大表在做索引变更时会引发抖动，进而影响系统稳定性；其次，一些模型拓展以及数据归档成本都相对较高，在成本上持续面临压力。</p><p>鉴于业务的快速增长，李岩和团队选择从底层存储层入手，做一次彻底的治理和升级，彻底解决分库分表架构带来的性能瓶颈，满足业务快速发展对数据底座的全新要求。</p><p>从业务角度而言，此次治理和升级，高德地图在数据库选型上需满足三大方面的要求：高可用、业务效率和成本。经过测试和对比以后，最终选择 OceanBase 来服务高德地图。</p><p>李岩表示，之所以选择 OceanBase，是因为其具有原生分布式和单机分布式一体化架构，具备弹性扩展、高可用和多活容灾能力。同时，除了 OceanBase 的性能稳定、产品成熟度高，已经过市场充分验证外，还有三点能力也是他看中的：</p><p>1.极致的存储压缩：OceanBase 提供多种压缩方式，显著降低存储规模；<br/>2.高度兼容 MySQL：减少应用层代码修改量，助力业务平滑升级。<br/>3.原生多活架构支持：可以根据业务需求灵活地选择。</p><h2>高德足迹大规模在线升级的考量与实践：零故障、零问题、用户零感知的平滑跃迁</h2><p>在确定采用 OceanBase 后，高德随即启动了系统化的升级工作。在千亿级别数据规模下，要做在线的升级，风险非常高。最终，凭借充分的准备与 OceanBase 团队的有力支持，整个升级过程实现了零故障，而且终端用户完全零感知，实现了业务的平滑跃迁。</p><p>在李岩看来，能够顺利完成此次“足迹”业务的升级，重点在于做好以下几方面的工作：</p><p>第一，站在全域、全链路的视角，放量回切控制点保持强统一、强同步、强一致，特别是涉及多层级的服务调用依赖时，这点尤其关键，否则就容易出现读写错位、放量错位的问题。</p><p>第二，进行多维度的数据校验。在“足迹”数据升级过程中，从最底层到最上层，分别进行了数据级、SQL级以及业务级的对比。只有三个校验都没有问题，才是真的没有问题。</p><p>第三，全量数据同步吞吐量高。这次升级过程中，OMS 的吞吐达到了每秒 1000 万行的并发写入，大大缩短了迁移周期。</p><p>升级后的系统平稳，已历经今年“双十一”等流量高峰期的考验，运行稳定、流畅，整体表现优异。对于此次数据库升级，李岩认为非常成功，为高德带来了多方面的实质性收益：</p><p>成本显著优化：OceanBase 通过高效的压缩算法，压缩比达到 2.23:1，数据规模得到降低，存储成本节省 55.2%。</p><p>性能持续提升：系统平均响应时间由原来的 5.95 毫秒降至 4.42 毫秒，提升了 25.7%，用户体验进一步改善。</p><p>研发效率提高：业务开发人员无需再关注分片、路由、弹性扩缩容等底层细节，“如同操作单机 MySQL 一样使用分布式数据库”。</p><h2>OceanBase 异地多活架构在高德的落地实践</h2><p>“云同步”是高德的另一项核心业务。“云同步”主要完成用户多端设备（如多个手机之间、手机与车机之间）的数据同步。</p><p>该业务的显著特点是数据量极为庞大，仅 3 个单元的数据总量就高达 5000 亿条。这一在线服务场景，对数据的实时性和高可用性都提出了极高的要求。</p><p>为了保证用户体验，同时也为了容灾，“云同步”在具体的业务场景下联合 OceanBase 采用了异地多活架构，在上海、张北、深圳三个数据中心分别进行部署。业务应用层提供多点写入的能力，基于用户维度进行单元化的路由和单元化的纠偏；中间的底层存储则依赖 OceanBase 的三地五中心金融级无损容灾能力部署，最下层的数据通路通过 OMS 来实现。</p><p>李岩介绍，由于高德需要实现不同单元间的双向同步，整个同步链路相对复杂，最终形成了三地六项的同步架构。“多活的目的，是为了就近接入以降低延迟，均衡各地域的资源分布和利用率，同时提升服务的 SLA，实现完全的高可用。同时，‘云同步’性能也得到了显著提升，实现了读写 QPS/TPS 22 万，读写平均响应时间 10 毫秒以内。升级后性能表现平稳，并且无论是水位还是 OMS 三地间同步链路也非常平滑。”</p><p>“如果没有 OceanBase 原生的多活能力，数据同步逻辑就必须在应用层实现，这不仅会大幅提升系统复杂度，也会显著增加开发和维护成本。”李岩说。</p><p>同时，系统的容灾能力也得到明显增强：当某个业务中心发生故障时，可在分钟级完成流量切换至其他数据中心，实现真正意义上的跨地域容灾逃逸。</p><h2>结语</h2><p>回看 OceanBase 在高德多个核心业务系统的应用，李岩认为，收益主要体现在以下三方面：</p><p>简单：提高了研发效率，业务研发像使用单机一样去使用分布式存储，不用关心分区、路由等等细节问题；</p><p>降本：因为有多种数据压缩手段以及数据和日志的分离，带来较高压缩比，数据规模越大，效果越明显；</p><p>多活和高可用：能保障数据的强一致性，提供多种多活部署方案，业务可按需选用。</p><p>“更重要的是，从分库分表到原生分布式数据库，高德完成的不仅是一次数据库升级，更是一场面向未来的架构演进，还为未来的业务发展预留了充足空间。”李岩总结说。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=wqQVoybnAzI%2BkbCENgBImw%3D%3D.6c39pACBwTwXlct9BVtL7Z7KyE7ddntGDNc5QkPI5hE%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[使用C#代码在 PowerPoint 演示文稿中插入表格 千杯不醉的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047606354</link>    <guid>https://segmentfault.com/a/1190000047606354</guid>    <pubDate>2026-02-12 10:05:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在 PowerPoint 中，表格是一种非常实用的工具，可以帮助你以清晰、简洁且直观的方式展示和整理数据。通过使用表格，你能够更有效地传达复杂信息，让观众更容易理解并记住重点内容。</p><p>本文将介绍如何使用 Spire.Presentation for .NET，在 C# 和 VB.NET 中向 PowerPoint 演示文稿插入表格。</p><h2>安装 Spire.Presentation for .NET</h2><p>在开始之前，您需要将 Spire.Presentation for .NET 安装包中的 DLL 文件添加为 .NET 项目的引用。您可以通过官方下载链接获取 DLL 文件，或直接通过 NuGet 进行安装。</p><pre><code class="C#">PM&gt; Install-Package Spire.Presentation</code></pre><h2>在 C# 和 VB.NET 中向 PowerPoint 演示文稿插入表格</h2><p>您可以使用 ISlide.Shapes.AppendTable(float x, float y, double[] widths, double[] heights) 方法，在指定幻灯片上添加表格。具体步骤如下：</p><ol><li>创建 <code>Presentation</code> 类的实例。</li><li>通过 <code>Presentation.LoadFromFile(string file)</code> 方法加载 PowerPoint 文件。</li><li>使用 <code>Presentation.Slides[int index]</code> 属性获取指定的幻灯片。</li><li>定义两个 double 数组（widths 和 heights），分别用于设置表格列数、列宽以及行数、行高。</li><li>调用 <code>ISlide.Shapes.AppendTable(float x, float y, double[] widths, double[] heights)</code> 方法，在幻灯片指定位置添加具有指定行列数量和尺寸的表格。</li><li>使用一个二维字符串数组存储表格数据。</li><li>遍历该二维数组，并通过 <code>ITable[int columnIndex, int rowIndex].TextFrame.Text</code> 属性，将数据填充到对应的单元格中。</li><li>将表格首行内容设置为居中对齐。</li><li>通过 <code>ITable.StylePreset</code> 属性为表格应用内置样式。</li><li>最后，使用 <code>Presentation.SaveToFile(string file, FileFormat fileFormat)</code> 方法保存演示文稿。</li></ol><p><strong>完整示例代码如下：</strong></p><pre><code class="C#">using Spire.Presentation;

namespace InsertTable
{
    internal class Program
    {
        static void Main(string[] args)
        {
            //初始化 Presentation 类的实例
            Presentation presentation = new Presentation();
            //加载 PowerPoint 演示文稿
            presentation.LoadFromFile(@"Input.pptx");

            //获取第一张幻灯片
            ISlide slide = presentation.Slides[0];

            //定义两个 double 数组 widths 和 heights，用于指定表格的列数、列宽以及行数、行高
            double[] widths = new double[] { 100, 100, 150, 100, 100 };
            double[] heights = new double[] { 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15 };

            //在幻灯片的指定位置添加具有指定行列数量和尺寸的表格
            ITable table = slide.Shapes.AppendTable(presentation.SlideSize.Size.Width / 2 - 275, 90, widths, heights);

            //使用二维字符串数组存储表格数据
            string[,] data = new string[,]{
            {"Name", "Capital", "Continent", "Area", "Population"},
            {"Venezuela", "Caracas", "South America", "912047", "19700000"},
            {"Bolivia", "La Paz", "South America", "1098575", "7300000"},
            {"Brazil", "Brasilia", "South America", "8511196", "150400000"},
            {"Canada", "Ottawa", "North America", "9976147", "26500000"},
            {"Chile", "Santiago", "South America", "756943", "13200000"},
            {"Colombia", "Bogota", "South America", "1138907", "33000000"},
            {"Cuba", "Havana", "North America", "114524", "10600000"},
            {"Ecuador", "Quito", "South America", "455502", "10600000"},
            {"Paraguay", "Asuncion", "South America", "406576", "4660000"},
            {"Peru", "Lima", "South America", "1285215", "21600000"},
            {"Jamaica", "Kingston", "North America", "11424", "2500000"},
            {"Mexico", "Mexico City", "North America", "1967180", "88600000"}
            };

            //遍历字符串数组，并将数据填充到表格的每个单元格中
            for (int i = 0; i &lt; 13; i++)
                for (int j = 0; j &lt; 5; j++)
                {
                    //为表格的每个单元格赋值
                    table[j, i].TextFrame.Text = data[i, j];
                    //设置字体名称和字体大小
                    table[j, i].TextFrame.Paragraphs[0].TextRanges[0].LatinFont = new TextFont("Times New Roman");
                    table[j, i].TextFrame.Paragraphs[0].TextRanges[0].FontHeight = 16;
                }

            //将表格第一行的内容设置为居中对齐
            for (int i = 0; i &lt; 5; i++)
            {
                table[i, 0].TextFrame.Paragraphs[0].Alignment = TextAlignmentType.Center;
            }

            //为表格应用内置样式
            table.StylePreset = TableStylePreset.MediumStyle2Accent6;

            //将演示文稿保存到文件
            presentation.SaveToFile("InsertTable.pptx", FileFormat.Pptx2013);
            presentation.Dispose();
        }
    }
}</code></pre><h2>申请临时许可证</h2><p>如果您希望去除生成文档中的评估提示信息，或解除功能限制，可以申请为期 30 天的试用许可证。</p>]]></description></item><item>    <title><![CDATA[麒麟桌面系统【如何连接隐藏wifi】 沙鱼 ]]></title>    <link>https://segmentfault.com/a/1190000047606402</link>    <guid>https://segmentfault.com/a/1190000047606402</guid>    <pubDate>2026-02-12 10:04:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h5>麒麟桌面系统老v10和v10-sp1两个系统版本的连接有些不一样，下面分别对这两个系统进行说明</h5><h2>一、V10-SP1版本</h2><ol><li>打开“<code>加入其他网络</code>”<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606405" alt="file" title="file"/><br/>或者从“开始” - “设置” - “无线局域网”进入也可以<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606406" alt="file" title="file" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606407" alt="file" title="file" loading="lazy"/></li><li>在弹出的“查找并加入无线局域网络”窗口，配置好ssid和密钥，点击加入。(<code>这两个必须输入，密码必须大于等于8位</code>)<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606408" alt="file" title="file" loading="lazy"/></li></ol><hr/><h2>二、老V10版本</h2><ol><li>鼠标点击桌面任务栏右下角网络连接图标，再点击“编辑连接”，进入网络连接页面。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606409" alt="file" title="file" loading="lazy"/></li><li>在网络连接页面，点击“增加”按钮。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606410" alt="file" title="file" loading="lazy"/></li><li>再选择连接类型，选择“Wi-Fi”，然后点击“新建”按钮。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606411" alt="file" title="file" loading="lazy"/></li><li>在正在编辑Wi-Fi连接1-&gt;Wi-Fi页面，在连接名称处和SSID处输入隐藏无线网络的名称，设备处选择对应的无线网卡硬件设备。<br/>例如：此处的无线网络名称为“ChinaNet-8312”，无线网卡硬件设备为“wlp6s0”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606412" alt="file" title="file" loading="lazy"/></li><li>在正在编辑Wi-Fi连接1（此处是正在编辑ChinaNet-8312）-&gt;Wi-Fi安全性页面，在安全处选择“WPA及WPA2个人”选项，在密码处输入无线网络的密码，然后点击“<code>保存</code>”按钮，即可连接无线网络。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606413" alt="file" title="file" loading="lazy"/></li><li><p>如果点击“保存”按钮后，无法正常使用无线网络，则在桌面空白处鼠标右键，选择“在终端中打开”选项，打开终端，执行以下操作。</p><ol><li>在终端输入<code>nmcil connection show</code>命令后回车查看所以网卡的连接信息。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606414" alt="file" title="file" loading="lazy"/><br/>从上图可知，刚才添加的无线网络“ChinNet-8312”没有连接激活。</p><ol start="2"><li>再在终端输入<code>nmcli connection up [无线网络名称]</code>，即此处是<code>nmcli connection up ChinNet-8312</code>命令后回车激活无线网络连接，会显示连接已成功激活。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606415" alt="file" title="file" loading="lazy"/></p><ol start="3"><li>无线网络连接激活成功后，再输入<code>nmcli connection show</code>命令后回车查看所有网卡的连接信息。正常使用的网卡，连接信息显示为绿色。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606416" alt="file" title="file" loading="lazy"/></p></li></ol><h2>三、命令行连接隐藏wifi</h2><ol><li>打开终端：<code>win + T</code></li><li><p>查看网卡，我这里是wlp2s0，后面连接需要用到</p><pre><code class="bash">nmcli  device</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606417" alt="file" title="file" loading="lazy"/></p></li><li><p>查看可用的wifi设备</p><pre><code class="bash">nmcli device wifi list</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606418" alt="file" title="file" loading="lazy"/></p></li><li><p>连接隐藏 Wi-Fi</p><pre><code class="bash">nmcli connection add \
 type wifi \
 con-name "kylintest" \
 ifname wlan0 \
 ssid "HiddenSSID" \
 wifi-sec.key-mgmt wpa-psk \
 wifi-sec.psk "YourPassword"</code></pre><h4>上面命令的参数说明：</h4></li><li>con-name：连接名称（自定义，如 MyHiddenWiFi）。</li><li>ifname：无线网卡接口（如 wlan0，可用 ip a 查看）。</li><li>ssid：隐藏 Wi-Fi 的 SSID（必须正确）。</li><li>wifi-sec.key-mgmt wpa-psk：使用 WPA-PSK 加密方式。</li><li>wifi-sec.psk：Wi-Fi 密码。</li></ol><h2>四、查询系统之前连接过的WiFi密码</h2><p>打开终端执行以下命令：</p><pre><code class="bash">sudo  grep  -r  "psk="  /etc/NetworkManager/system-connections/</code></pre><p>如下图所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606419" alt="file" title="file" loading="lazy"/></p><p>本文由<a href="https://link.segmentfault.com/?enc=Zq%2BR8845%2FV01QQUf6xLX4g%3D%3D.h2jHs9pXnpHGWG7%2BLtKDwxHk1Q9xj9knWoI%2BZVPH2wM%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[麒麟桌面系统【修改默认NTP服务器】 沙鱼 ]]></title>    <link>https://segmentfault.com/a/1190000047606489</link>    <guid>https://segmentfault.com/a/1190000047606489</guid>    <pubDate>2026-02-12 10:04:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、现象</h2><p>麒麟V10桌面系统，开机重启之后，发现系统时间不对，与实际时间相差较大。而且单位里多台电脑出现了同样的情况，但是每一台电脑的时间都是不一样的。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606492" alt="file" title="file"/></p><h2>二、处理方法</h2><h3>方法1：手动修改时间</h3><pre><code class="bash">sudo date -s "2025-05-30 10:20:32"</code></pre><h3>方法2：修改默认的可访问的NTP服务器</h3><p>执行以下命令，修改NTP服务器为<code>ntp2.aliyun.com</code>，也可以根据需要修改成单位内部的NTP服务器的域名或者ip地址。</p><pre><code class="bash">sudo  sed  -i  's|#*NTP=.*|NTP=ntp2.aliyun.com|g'   /etc/systemd/timesyncd.conf</code></pre><p>重启一下时间同步服务<code>systemd-timesyncd</code></p><pre><code class="bash">systemctl  restart systemd-timesyncd.service</code></pre><h3>方法3：安装补丁包</h3><p>(<code>原理同方法2，修改默认的NTP服务器为阿里云服务器</code>)</p><ol><li>下载补丁包 [<em>点击以下图片下载</em>]<br/><a href="https://link.segmentfault.com/?enc=nPVZUizckdnEzsRjrS3Uzw%3D%3D.CYzmjIQX2dsBJN62%2BPVlmWsz9rCnqmo6s4vR3z942mhWAejrshB849ZEGvIpP5pW626UoPmXvSoffao%2Fh80I4dtkgPmpafn6HqqhrM6jKpo%3D" rel="nofollow" target="_blank">&lt;img src="https://gxxc.wiki/wp-content/uploads/2025/05/image-1748580746389.png" alt="描述文字"&gt;</a><br/><code>如果以上无法下载，可以访问这里下载https://gxxc.wiki/kd/6642.html</code></li><li><p>安装补丁包<br/>解压补丁包后，进入deb包所在目录，双击deb包安装，<code>或者</code>使用以下命令安装：</p><pre><code class="bash">sudo  dpkg  -i  fix-timesync_1.0.0_all.deb</code></pre></li><li>安装完补丁包后，再查看一下时间<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606493" alt="file" title="file" loading="lazy"/></li><li><p>建议将当前对的时间，同步到硬件RTC</p><pre><code class="bash">sudo  hwclock  -w</code></pre></li></ol><h2>三、原因分析</h2><p>日志显示无法连接到ntp服务器，读取硬件RTC时间<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606494" alt="file" title="file" loading="lazy"/><br/>所以显示出不同的时间。而systemd-timesyncd 这个服务本身不会将当前系统时间时间写入硬件RTC，所以时间过去越久，硬件时间与实际时间相差越多，当系统无法从默认的ntp服务器获取到正确时间的时候，就会用硬件时间，从而时间有差异。</p><h2>四、其他</h2><p>systemd-timesyncd 这个服务本身不会将当前系统时间时间写入硬件RTC。所以，建议用户每过一段时间后手动将当前时间同步到硬件上，可以执行命令<code>timedatectl</code>查看当前硬件时间<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606495" alt="file" title="file" loading="lazy"/><br/>如果硬件时间和当前系统时间相差较大，可以先将系统时间设置正确后，执行以下命令将当前时间同步到硬件RTC</p><pre><code class="bash">sudo  hwclock  -w</code></pre><p>本文由<a href="https://link.segmentfault.com/?enc=yEFAdK92wWX45DKLLb8xBA%3D%3D.0G4rsjSzI36QFd%2BXQPC8umZ0ZX46zc728yEUywgspYA%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[麒麟服务器【无法识别U盘的处理】 沙鱼 ]]></title>    <link>https://segmentfault.com/a/1190000047606708</link>    <guid>https://segmentfault.com/a/1190000047606708</guid>    <pubDate>2026-02-12 10:03:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>问题描述</h2><p>将 U 盘插入电脑后，系统没有自动挂载，并且在文件管理器中看不到该 U 盘设备。经常在用ventoy做了系统启动盘的U盘上，常常遇到这个问题，请看以下解决方法。</p><h2>解决办法</h2><h3>方法1</h3><p>将U盘格式化成ext4格式后使用，注意格式化前备份U盘里的重要数据。</p><h3>方法2</h3><p>系统默认无法识别到exfat格式的硬盘，需要安装fuse-exfat这个软件包解决对exfat格式u盘的支持，安装命令如下：</p><pre><code class="bash">yum  install  -y  fuse-exfat</code></pre><p>如果服务器不联外网，可以下载安装包后，拷贝到系统上离线安装，下面是x86架构下的路径<code>(arm的把x86_64修改成aarch64即可)</code>：</p><blockquote>V10-SP1: <a href="https://link.segmentfault.com/?enc=i65SoJaESCWJKCz4tAYxMA%3D%3D.20tcX4dvYYuFziY6fCtPYmnzFgNWu8pTtvZVgHT%2BEJVw1A%2BIHZuHbFQWj1HEtvMtNr00KmBaoR4qP57KcQ4EXzeBL3Iuz%2Fbj6BWlwGxHfIc%3D" rel="nofollow" target="_blank">https://update.cs2c.com.cn/NS/V10/V10SP1/os/adv/lic/base/x86_...</a><br/>V10-SP2: <a href="https://link.segmentfault.com/?enc=wdu%2FdUGlgtKOU1cfXCTFUw%3D%3D.Vf6QrIpwQJ2MBWRjkvfSIW%2BAqlqWFCjHwCd0PK2SXU2sbFdTefTfSh0mI5fIFEPMM6cWDxXZ%2BJ0Oc6nF6cMUxOsEAUw%2FPRcBn0otVp5yrtg%3D" rel="nofollow" target="_blank">https://update.cs2c.com.cn/NS/V10/V10SP2/os/adv/lic/base/x86_...</a><br/>V10-SP3: <a href="https://link.segmentfault.com/?enc=7k4C6gixYuKvSO9U23t9iQ%3D%3D.27dk0LxzbNYZH1b8rQZH9VqqM9mKK8WNsuo5FuiiLjFk%2FqgYgN%2BCPaMn8ZXfL7opkDbNGQg%2FxEfYJ5y3KmCCfVTQgAaZsZhOZevslgHQX48%3D" rel="nofollow" target="_blank">https://update.cs2c.com.cn/NS/V10/V10SP3/os/adv/lic/base/x86_...</a></blockquote><p>`有客户又说了，我的U盘都读不出来，下载后，怎么拷贝到服务器里？<br/>  这里咱们建议可以格式化U盘成其他格式拷贝，也可以通过刻录进CD盘再拷贝到服务器，或者也可以挂iso配置本地源进行安装`<br/> </p><h4>方法3</h4><p>执行 “lsblk” 命令查看是否能检测到 U 盘设备，<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606711" alt="file" title="file"/><br/>如果能检测到但未挂载，如上图所示，可以执行以下命令，将u盘挂载到/mnt目录下，用命令访问/mnt看看是否能够访问。</p><pre><code class="bash">sudo  mount  /dev/sdb1  /mnt</code></pre><p>这里假设 U 盘设备名为 <code>sdb1</code>，请根据实际情况修改；</p><blockquote>如果以上方法还是检测不到，可能是 U 盘故障或者其他原因，可尝试更换其他 U 盘。</blockquote><p>本文由<a href="https://link.segmentfault.com/?enc=ihiRMCNJjZsMLyk28X%2B%2F4Q%3D%3D.HVIEMgbNp0xNBoUCGlg8pO0Ro74qAoteG78zznp83Lg%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[麒麟桌面系统【笔记本电池容量和损耗】 沙鱼 ]]></title>    <link>https://segmentfault.com/a/1190000047606715</link>    <guid>https://segmentfault.com/a/1190000047606715</guid>    <pubDate>2026-02-12 10:02:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>麒麟桌面操作系统，如何通过命令查看笔记本电池使用情况，我们可以通过upower命令，默认安装有，如果没有该命令，可以通过以下命令安装</p><pre><code class="bash">sudo  apt  install  upower</code></pre><h3>查看命令：</h3><pre><code class="bash">upower -i `upower -e | grep 'BAT'`</code></pre><h3>输出如下：</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606718" alt="" title=""/></p><p>本文由<a href="https://link.segmentfault.com/?enc=BewFWOlI2pwRbp59vwpJQA%3D%3D.oGPgkRvUKoQpJ4ZWNI2%2B2UHGrzyazf%2Fp3OrRJuvnFX8%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[秒杀活动时系统在干什么 PHP 高并发场景优化指南 JaguarJack ]]></title>    <link>https://segmentfault.com/a/1190000047606819</link>    <guid>https://segmentfault.com/a/1190000047606819</guid>    <pubDate>2026-02-12 10:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>秒杀活动时系统在干什么 PHP 高并发场景优化指南</h2><p>秒杀活动是电商平台的关键战役，往往会带来流量和订单的剧烈飙升。秒杀期间，每一毫秒都很关键，后端需要同时扛住海量请求。对 PHP 应用来说，这尤其有挑战性，但只要优化到位，即使流量洪峰来了，用户体验也能稳住。</p><p>这篇文章会拆解 PHP 后端在秒杀期间需要做哪些事情：从数据库查询优化，到缓存管理，再到应用扩容。</p><h3>用负载均衡应对高并发</h3><p>秒杀期间，PHP 应用需要动态扩容来承接激增的流量。负载均衡是把请求分散到多台服务器的核心手段。</p><h4>负载均衡 + 自动扩容</h4><p>流量暴涨时，PHP 应用应该部署在负载均衡器（比如 AWS ELB 或 NGINX）后面，由负载均衡器把请求均匀分发到多台应用服务器。</p><p>工作原理：</p><ul><li><strong>PHP-FPM 工作进程</strong>：每台 PHP 服务器通过 PHP-FPM（FastCGI 进程管理器）处理请求。负载均衡器确保请求被分散到多台服务器，避免单台服务器被打垮。</li><li><strong>自动扩容</strong>：流量上来后，AWS EC2 Auto Scaling 或 Google Cloud Compute Engine 等云服务会自动拉起更多 PHP 实例来承接负载。</li></ul><p><strong>配置自动扩容</strong>：当 CPU 使用率或请求量超过阈值时触发扩容。</p><pre><code class="bash">aws autoscaling create-auto-scaling-group \
  --auto-scaling-group-name php-flash-sale-group \
  --min-size 2 --max-size 50 --desired-capacity 10 \
  --vpc-zone-identifier subnet-xyz</code></pre><p><strong>负载均衡器配置</strong>：确保请求被高效地分发到所有 PHP 服务器。</p><pre><code class="nginx">upstream php_backend {
    server php-server-1;
    server php-server-2;
    server php-server-3;
}
server {
    location / {
        proxy_pass http://php_backend;
    }
}</code></pre><p>通过负载均衡加自动扩容，PHP 后端可以平滑地应对秒杀期间的流量洪峰。</p><h3>缓存策略：减轻数据库压力</h3><p>秒杀期间最大的挑战之一，就是防止数据库因为大量读写操作变成瓶颈。最有效的手段是用缓存来分担数据库查询，同时提升响应速度。</p><h4>缓存静态内容和数据库查询</h4><p><strong>CDN 缓存静态资源</strong></p><p>图片、CSS、JavaScript 这类静态资源应该通过 CDN（比如 Cloudflare 或 AWS CloudFront）在边缘节点缓存，保证用户能快速加载。在 PHP 中设置合适的缓存控制头：</p><pre><code class="php">header("Cache-Control: public, max-age=3600");  // Cache static assets for 1 hour</code></pre><p><strong>内存缓存热点数据</strong></p><p>用 Redis 或 Memcached 缓存频繁查询的数据，比如商品库存和价格，减少数据库压力。</p><p>秒杀期间，把商品库存状态存到 Redis 里，每次查库存就不用打数据库了：</p><pre><code class="php">$redis = new Redis();
$redis-&gt;connect('localhost', 6379);
// Check if product availability is cached
$productId = 123;
$productAvailability = $redis-&gt;get("product:{$productId}:availability");
if (!$productAvailability) {
    // Cache miss, fetch from database
    $productAvailability = fetchProductAvailabilityFromDb($productId);
    $redis-&gt;set("product:{$productId}:availability", $productAvailability, 3600);  // Cache for 1 hour
}</code></pre><p>这样可以大幅减少秒杀期间的数据库查询次数，用户的响应速度也更快。</p><h3>优化数据库性能</h3><p>数据库性能往往是秒杀场景的瓶颈所在，特别是大量请求同时读写数据库的时候。优化查询、确保 PHP 应用高效处理数据库操作至关重要。</p><h4>分库分表</h4><p>分库分表是把数据库拆分成更小、更易管理的部分，每个部分只处理一部分数据，从而把查询分散到多个数据库实例上。</p><p>比如可以按用户地区分库（北美用户和欧洲用户各用一套数据库），以此均衡负载。</p><h4>连接池</h4><p>每次请求都开关数据库连接会带来很大的开销。通过连接池复用数据库连接，可以显著降低这部分消耗。在 PHP 中，可以配置持久连接：</p><pre><code class="php">$mysqli = new mysqli("p:localhost", "username", "password", "database");</code></pre><h4>读写分离</h4><p>如果用了数据库主从复制（比如 MySQL Replication），可以配置 PHP 应用把读查询发到从库，写查询发到主库：</p><pre><code class="php">$readDb = new mysqli('read-replica-host', 'username', 'password', 'database');
$writeDb = new mysqli('primary-db-host', 'username', 'password', 'database');</code></pre><h4>查询优化</h4><p>秒杀期间要确保数据库查询经过优化：对高频查询字段（比如商品 ID、分类等）建好索引。在 PHP 中使用预处理语句可以提升查询执行效率：</p><pre><code class="php">$stmt = $mysqli-&gt;prepare("SELECT * FROM products WHERE id = ?");
$stmt-&gt;bind_param("i", $productId);
$stmt-&gt;execute();
$result = $stmt-&gt;get_result();
$product = $result-&gt;fetch_assoc();</code></pre><p>通过分库分表、连接池、读写分离和查询优化，可以防止数据库成为瓶颈，保证 PHP 应用在秒杀这种高并发场景下依然跑得动。</p><h3>会话管理和用户认证</h3><p>秒杀期间，用户能不能顺利加购、结账、登录，直接决定了转化率。会话管理必须针对高并发做优化。</p><h4>用 Redis 做会话持久化</h4><p>用 Redis 存储会话数据，这样即使请求被负载均衡器分发到不同的 PHP 服务器上，会话也不会丢失：</p><pre><code class="php">// Store session data in Redis
session_set_save_handler(new RedisSessionHandler($redis), true);
session_start();</code></pre><h4>用 JWT 做无状态认证</h4><p>用户登录和认证环节，可以用 JWT（JSON Web Token）来减轻会话存储的压力，实现无状态认证：</p><pre><code class="php">// Example of generating JWT token
$payload = ['user_id' =&gt; $userId, 'exp' =&gt; time() + 3600];  // Expires in 1 hour
$jwt = JWT::encode($payload, $secretKey);</code></pre><p>把会话数据交给 Redis，认证环节用 JWT，就能保证秒杀期间的登录和会话管理又快又稳。</p><h3>实时库存管理</h3><p>秒杀期间，库存必须随着商品售出实时更新。PHP 需要确保库存数据在多台服务器之间保持同步，一旦有人下单，库存立刻扣减。</p><h4>事件驱动架构处理库存更新</h4><p>通过 Apache Kafka 或 RabbitMQ 实现事件驱动架构，实时处理库存变更：</p><pre><code class="php">// Kafka Producer: Send product purchase events
$producer-&gt;produce('product-purchased-topic', 0, json_encode(['product_id' =&gt; 123, 'quantity' =&gt; 1]));</code></pre><p>库存服务订阅这些事件，实时更新数据库中的商品库存。用户下单后，购买事件发送到 Kafka，库存服务收到事件后立即扣减库存，其他用户就不会再买到已经卖完的商品。</p><h3>总结</h3><p>秒杀期间保证 PHP 应用的性能，需要多管齐下：负载均衡、缓存、数据库优化、实时库存管理，缺一不可。通过自动扩容、Redis 内存缓存、高效的数据库查询和事件驱动架构，PHP 应用完全有能力扛住流量洪峰，给用户提供流畅的体验。</p><p>把这些手段用好，你的电商平台就能顶住秒杀的压力，不宕机、不卡顿，把转化率拉到最高。</p><p><a href="https://link.segmentfault.com/?enc=NH2dV4vsgaWiL8P358Nd5g%3D%3D.B7vTFWaeZ2%2BuQYeKJruA5zEI4XA5Ags8%2B%2FqahwxIXnvKoYkc5sJ2VS7yysIJrKtQfTdW%2BKNqv0tpGbovPX%2FXxA%3D%3D" rel="nofollow" target="_blank">秒杀活动时系统在干什么 PHP 高并发场景优化指南</a></p>]]></description></item><item>    <title><![CDATA[酷监控！一款高颜值的监控工具！ Java陈序员 ]]></title>    <link>https://segmentfault.com/a/1190000047606859</link>    <guid>https://segmentfault.com/a/1190000047606859</guid>    <pubDate>2026-02-12 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是 <code>Java陈序员</code>。</p><p>在如今数字化运营时代，服务的稳定性直接决定用户体验。但搭建一套完善的服务监控体系往往门槛不低：要么是专业监控工具配置复杂、学习成本高，要么是轻量工具功能单一，难以覆盖全场景需求。</p><p>今天，给大家推荐一款高颜值的监控系统工具，轻量易部署！</p><blockquote>关注微信公众号：【Java陈序员】，获取<strong>开源项目分享、AI副业分享、超200本经典计算机电子书籍等。</strong></blockquote><h2>项目介绍</h2><p><code>coolmonitor</code> —— 酷监控，一个高颜值的监控工具，支持网站监控、接口监控、HTTPS 证书监控等多种监控类型，帮助开发者及运维人员实时掌握网站、接口运行状态。</p><p><strong>功能特色</strong>：</p><ul><li><strong>多维度监控覆盖</strong>：支持 HTTP、HTTPS 网站、API 接口、HTTPS 证书过期、TCP 端口、MySQL、Redis 数据库等多种监控</li><li><strong>多渠道通知配置</strong>：支持邮件、Webhook、微信、钉钉、企业微信等多类型通知渠道</li><li><strong>便捷的操作体验</strong>：响应式布局，适配桌面、平板、移动端，支持深色、浅色主题切换</li><li><strong>数据可视化</strong>：监控数据支持可视化展示，通过 ECharts 生成响应时间趋势图，支持按小时、天维度查看</li><li><strong>持久化存储</strong>：采用 SQLite 轻量数据库，监控配置、运行数据持久化存储，轻量级部署无需额外依赖</li></ul><h2>快速上手</h2><p><code>coolmonitor</code> 支持 Docker 部署，可通过 Docker 快速部署。</p><p>1、拉取镜像</p><pre><code class="bash">docker pull star7th/coolmonitor:latest</code></pre><p>2、创建挂载目录</p><pre><code class="bash">mkdir -p /data/software/coolmonitor</code></pre><p>3、运行容器</p><pre><code class="bash">docker run -d \
    --name coolmonitor \
    -p 3333:3333 \
    -v /data/software/coolmonitor:/app/data \
    star7th/coolmonitor:latest</code></pre><p>4、容器运行成功后，浏览器访问</p><pre><code class="bash">http://{IP/域名}:3333</code></pre><p>5、根据引导，设置管理员账号密码，完成系统初始化</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606861" alt="" title=""/></p><h2>功能体验</h2><ul><li><strong>主面板</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606862" alt="" title="" loading="lazy"/></p><ul><li><strong>监控详情页</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606863" alt="" title="" loading="lazy"/></p><ul><li><strong>添加监控项</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606864" alt="" title="" loading="lazy"/></p><ul><li><strong>添加通知方式</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606865" alt="" title="" loading="lazy"/></p><ul><li><strong>状态页</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606866" alt="" title="" loading="lazy"/></p><p><code>coolmonitor</code> 没有复杂的配置项，能覆盖日常监控的核心需求，颜值高、易部署、易维护，不管是个人开发者监控自己的小网站，还是中小企业监控内部服务，都非常适用。快去部署体验吧~</p><pre><code class="bash">项目地址：https://github.com/star7th/coolmonitor</code></pre><h2>最后</h2><p>推荐的开源项目已经收录到 <code>GitHub</code> 项目，欢迎 <code>Star</code>：</p><pre><code>https://github.com/chenyl8848/great-open-source-project</code></pre><p>或者访问网站，进行在线浏览：</p><pre><code>https://chencoding.top:8090/#/</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046659706" alt="" title="" loading="lazy"/></p><p><strong>我创建了一个开源项目交流群，方便大家在群里交流、讨论开源项目</strong>。</p><p><strong>但是任何人在群里打任何广告，都会被 T 掉</strong>。</p><p><strong>如果你对这个交流群感兴趣或者在使用开源项目中遇到问题，可以通过如下方式进群</strong>：</p><p><strong>关注微信公众号：【Java陈序员】，回复【开源项目交流群】进群，或者通过公众号下方的菜单添加个人微信，并备注【开源项目交流群】，通过后拉你进群</strong>。</p><blockquote>大家的点赞、收藏和评论都是对作者的支持，如文章对你有帮助还请点赞转发支持下，谢谢！</blockquote><hr/>]]></description></item><item>    <title><![CDATA[剑指offer-76、删除链表的节点 SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047598564</link>    <guid>https://segmentfault.com/a/1190000047598564</guid>    <pubDate>2026-02-12 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>题⽬描述</h2><p>给定单向链表的头指针和⼀个要删除的节点的值，定义⼀个函数删除该节点。返回删除后的链表的头节点。</p><ol><li>此题对⽐原题有改动</li><li>题⽬保证链表中节点的值互不相同</li><li>该题只会输出返回的链表和结果做对⽐，所以若使⽤ C 或 C++ 语⾔，你不需要 free 或 delete 被删除的节点</li></ol><p>数据范围:</p><ul><li>0&lt;=链表节点值&lt;=10000</li><li>0&lt;=链表⻓度&lt;=10000</li></ul><p>示例1</p><pre><code class="txt">输⼊：{2,5,1,9},5
返回值：{2,1,9}
说明：给定你链表中值为 5 的第⼆个节点，那么在调⽤了你的函数之后，该链表应变为 2 -&gt; 1 -&gt; 9</code></pre><p>示例2</p><pre><code class="txt">输⼊：{2,5,1,9},1
返回值：{2,5,9}
说明：给定你链表中值为 1 的第三个节点，那么在调⽤了你的函数之后，该链表应变为 2 -&gt; 5 -&gt; 9</code></pre><h2>思路及解答</h2><h3>虚拟头节点</h3><p>如果要删除链表⾥⾯的⼀个节点，其实就是将前置节点的next 直接指向当前节点的后置节点，这样在链表中再也找不到该节点了，也就是相当于删除了。</p><p>假设有⼀个链表，我们需要删除⾥⾯的 5 :</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598566" alt="" title=""/></p><p>⾸先需要判断链表头结点是不是为空，如果为空，那么就直接返回NULL ，如果等于我们要找的，那么直接返回下⼀个节点引⽤即可。</p><p>如果不符合以上说的，那么我们需要新建⼀个前置节点pre ,与现在的链表连接在⼀起：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598567" alt="" title="" loading="lazy"/></p><p>然后初始化⼀个 cur 节点表示当前节点，指向 head 节点：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598568" alt="" title="" loading="lazy"/></p><p>cur 不为空， cur 和 pre 后移：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598569" alt="" title="" loading="lazy"/></p><p>发现 cur 正是我们需要查找的 5 ，那么记录下 5 的下⼀个节点 1 ,也就是next :</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598570" alt="" title="" loading="lazy"/></p><p>cur 的 next 指向 NULL ,使⽤ pre 的 next 指向刚刚记录的 next :</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598571" alt="" title="" loading="lazy"/></p><p>简化链表也就是：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598572" alt="" title="" loading="lazy"/></p><p>取之前虚拟的头结点的后⼀个节点，就是删除掉之后的新链表：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598573" alt="" title="" loading="lazy"/></p><pre><code class="java">class ListNode {
    int val;
    ListNode next = null;
    public ListNode(int val) {
        this.val = val;
    }
}

public class Solution13 {
    public ListNode deleteNode(ListNode head, int val) {
        if (head == null) {
            return null;
        }
        
        if (head.val == val) {
            return head.next;
        }
        
        // ⽤⼀个节点将头结点链接起来
        ListNode pre = new ListNode(-1);
        pre.next = head;
        ListNode cur = head;
        ListNode next = null;
        while (cur != null) {
            if (cur.val == val) {
                // 将前置节点直接连接后⼀个节点，相当于删除掉了该节点
                pre.next = cur.next;
                break;
            }
            cur = cur.next;
            pre = pre.next;
        }
        return head;
    }
}</code></pre><h3>迭代</h3><p>通过遍历链表找到目标节点并修改指针，维护前驱指针，当找到目标节点时修改指针跳过该节点</p><pre><code class="java">public class Solution {
    public ListNode deleteNode(ListNode head, int val) {
        // 处理头节点就是要删除的节点的情况
        if (head != null &amp;&amp; head.val == val) {
            return head.next;
        }
        
        ListNode prev = null;
        ListNode curr = head;
        
        // 遍历查找目标节点
        while (curr != null &amp;&amp; curr.val != val) {
            prev = curr;
            curr = curr.next;
        }
        
        // 找到目标节点后跳过它
        if (curr != null) {
            prev.next = curr.next;
        }
        
        return head;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，最坏情况下需要遍历整个链表</li><li><strong>空间复杂度</strong>：O(1)，只使用常数空间</li></ul><h3>递归</h3><p>当前节点是要删除的节点则返回next，否则递归处理剩余链表</p><pre><code class="java">public class Solution {
    public ListNode deleteNode(ListNode head, int val) {
        // 递归终止条件
        if (head == null) {
            return null;
        }
        
        // 当前节点是要删除的节点
        if (head.val == val) {
            return head.next;
        }
        
        // 递归处理剩余链表
        head.next = deleteNode(head.next, val);
        return head;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，需要处理每个节点</li><li><strong>空间复杂度</strong>：O(n)，递归调用栈的深度</li></ul>]]></description></item><item>    <title><![CDATA[零售行业 SRM 系统推荐榜单（2026版）——供应商管理全景指南 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047600121</link>    <guid>https://segmentfault.com/a/1190000047600121</guid>    <pubDate>2026-02-12 08:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>做零售的都懂，供应商管理往往是企业运营中最繁杂、最耗时间的那一块——品类多、供应商多、对账频繁，还得实时盯供货节奏。尤其在当前线上线下融合加速的趋势下，靠 Excel 表格管供应商管理早已跟不上节奏。为了帮助零售企业快速建立科学、高效的供应商管理体系，本文整理了一份<strong>2026年适配零售行业的 SRM 系统推荐榜单</strong>，为企业选型提供实用参考。</p><p><strong>一、什么是 SRM 系统？为何对零售如此关键</strong></p><p>在进入推荐之前，先明确一个核心概念：<strong>SRM</strong> 系统是用来管理供应商全生命周期的一套软件解决方案。它的目标是帮助企业从供应商准入、绩效管理、风险监控，到日常协同、合同与对账等业务流程，实现数字化、标准化和可视化管理。</p><p>在零售行业，这类系统尤为重要，因为：</p><p>（1）零售企业的采购品类极其杂乱，SKU 数量庞大，供应商类型多样，从品牌方、大型经销商到小型供应商与加工厂，管理复杂度高；</p><p>（2）线上线下库存变动快，促销、季节性业务强，供货节奏必须准确把握；</p><p>（3）对账频率高、数据核对工作量巨大，若没有系统支持，很难避免账目不清、对账延迟等问题；</p><p>（4）供应商绩效评估和风险预警体系不足会直接影响供货稳定性。</p><p><strong>SRM 系统的价值不仅是“管数据”，更是帮助企业提升供应链协同效率、降低风险、提升供应商价值贡献、实现战略采购的长期平台。</strong></p><p><strong>二、零售行业 SRM 系统推荐榜单</strong></p><p>在众多 SRM 平台中，我们结合零售行业的典型需求（如多门店协同、库存节奏与采购链路紧密联动、促销物资临时采购、大宗物料管理等）进行了评估，推荐了以下几款系统：</p><p><strong>1 正远科技 — </strong><strong>零售全场景低代码定制 SRM 方案</strong></p><p><strong>推荐指数：★★★★★</strong><br/><strong>适用规模：大型连锁 / 区域连锁 / 新零售集团</strong></p><p>正远科技 SRM 的最大特色是<strong>低代码可编排架构</strong>，支持业务人员通过拖拽可视化方式快速调整流程，而无需 IT 二次开发，适合零售企业业务变化频繁的特性。</p><p>在零售行业的典型场景中：</p><p>（1）促销临时采购流程、临时物料变更、赠品采购等审批流可快速调整；</p><p>（2）多区域采购需求与多门店执行协同无缝衔接；</p><p>（3）门店数据、仓储、ERP、对账系统之间实现实时协同。</p><p>在供应商生命周期管理上，它支持从供应商准入、评级、评级调整到淘汰全流程闭环控制，同时具备实时风险监控能力，有助于减少因供应商突发状况导致的断货风险。在订单与执行层面，系统支持 VMI 库存管理和条码收货，这对门店稀散、收货点多的零售企业尤为重要。真正实现采购订单、物流路径、对账流水在一个协同大平台中的柔性联动。</p><p><strong>亮点功能：</strong></p><p>（1）低代码可视化流程自定义；</p><p>（2）支持类电商化采购商城；</p><p>（3）与 ERP、OA、企业微信、工作流无缝集成；</p><p>（4）提供实时供货节奏监控与预警。</p><p>该系统在实际落地案例中，能有效缩短采购周期、提升对账效率，在部分零售企业中采购周期平均缩短约 40% 以上，供应链协同效率显著提高。<br/><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdnS7Y" alt="" title=""/></p><p><strong>2 甄云科技 — </strong><strong>AI 驱动智能采购 SRM</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：大型 / 规模化零售企业</strong></p><p>甄云科技的 SRM 最大亮点在于<strong>AI 与大数据的深度嵌入</strong>。在零售行业常见的“品类杂，价格波动快”的场景下，甄云的智能比价和风控引擎能够帮助企业在采购阶段快速筛选、比价与评估供应商。</p><p>例如，它的 AI 比价工具能在几秒内完成多平台的价格比对，并提供趋势分析，这对于价格敏感的零售采购极具价值，同时还能直接提升成本控制能力。系统还能对接大量外部监控数据，进行供应商风险预警。</p><p><strong>优势特点：</strong></p><p>（1）跨平台智能比价与采购成本预警；</p><p>（2）实时风险监测与供应商健康得分；</p><p>（3）支持多语言、多币种，适合跨境采购；</p><p>（4）与 ERP/WMS 等多生态系统集成。</p><p>甄云科技 SRM 的标准化程度较高，对于流程规范、企业规模较大的零售集团尤为适合。但对于需要深度业务定制的场景，其灵活度略逊与更开放的低代码平台。<br/><img width="723" height="383" referrerpolicy="no-referrer" src="/img/bVdnS7Z" alt="" title="" loading="lazy"/></p><p><strong>3 鲸采云 — </strong><strong>中小零售轻量化 SaaS 优选方案</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：中小零售 / 区域连锁</strong></p><p>鲸采云定位轻量化 SaaS SRM，其操作门槛低、部署迅速，非常适合中小型零售企业。系统覆盖了从供应商准入、绩效评估到采购执行的基本 SRM 模块，同时内置了标准的供应商数据管理能力，支持扫码收货、智能补货、分门店采购协同等功能。</p><p>针对中小企业常见的“预算有限、没有 IT 团队”的情况，该系统提供了丰富的第三方插件接口，可以与金蝶、用友等主流财务系统无缝对接，并支持与钉钉、企业微信等移动办公系统联动。</p><p><strong>适配亮点：</strong></p><p>（1）快速上手、低门槛 SaaS 方案；</p><p>（2）支持采购商城式下单体验；</p><p>（3）集成主流办公和财务系统插件；</p><p>（4）门店与总部协同集中管理功能完整。</p><p>对于采购流程相对稳定、供应商数量适中、对高级定制需求不高的中小零售企业，这款系统性价比较高。<br/><img width="723" height="362" referrerpolicy="no-referrer" src="/img/bVdnS70" alt="" title="" loading="lazy"/></p><p><strong>4 用友采购云 — </strong><strong>ERP 生态深度联动 SRM</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：已部署用友 ERP 的零售企业</strong></p><p>用友采购云的优势在于与其 ERP 大生态产品深度集成，能将采购流程、生意账务和库存管理紧密联动，从而避免信息孤岛和重复录入工作。通过自动化审批与财务联动，可显著提升合规性与对账效率。</p><p>特别是在跨区域、跨业务单元的零售企业，用友采购云对不同业务模式的支持能力很强，例如多税制处理、合同自动生成、应付账款自动管理等。</p><p><strong>主要优势：</strong></p><p>（1）深度 ERP / 财务系统集成；</p><p>（2）自动化审批与合规管理；</p><p>（3）支持全球采购与跨币种场景；</p><p>（4）对账流程全链路自动化。</p><p>对于已经在用友系统中的零售企业来说，该系统可以<strong>最大限度减少 ERP 与 SRM 之间的数据不一致情况</strong>。<br/><img width="723" height="376" referrerpolicy="no-referrer" src="/img/bVdnS71" alt="" title="" loading="lazy"/></p><p><strong>5 企企通 — </strong><strong>SRM + 供应链金融结合的特定场景解决方案</strong></p><p><strong>推荐指数：★★★☆☆</strong><br/><strong>适用规模：资金链压力大的中小企业</strong></p><p>企企通的特色在于其 SRM 平台与供应链金融服务的结合，可以为合作供应商提供融资支持，从而在一定程度上稳定供应链。对于一些资金压力较大、对现金流敏感的中小零售企业，这种功能尤其有价值。</p><p>该系统在供应商管理、订单协同、物流跟踪等基础功能之外，加强了资金服务联动，例如供应商融资额度展示、资金通道对接等。对于希望通过供应链金融提升供货稳定性和合作紧密度的企业是一大加分项。</p><p>但在行业专属功能方面，例如临时促销物资管理、多门店库存节奏优化等方面不如前几款完善，因此更适合<strong>采购流程相对简单、希望补贴供应链现金流的中小企业</strong>。<br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnS72" alt="" title="" loading="lazy"/></p><p><strong>五、选型小贴士与实施建议</strong></p><p>（1）<strong>先从业务优先级出发，不要盲目追求花哨功能</strong>：比如对账自动化、风险预警、门店端补货支持，这些是零售企业最基础的痛点。</p><p>（2）<strong>试用是必须的</strong>：任何系统的卖点再好，若实际操作繁琐、不贴合业务流程，那么 ROI 很难体现。</p><p>（3）<strong>数据迁移与集成成本要提前评估</strong>：老系统迁移、新系统上线与现有库存、财务数据的集成成本往往被低估。</p><p>（4）<strong>长期战略要考虑扩展性</strong>：是否支持未来供应链升级，这是构建企业竞争力的关键。</p><p><strong>六、结语</strong></p><p>供应商管理是零售企业运营效率的核心组成部分，也是企业实现精细化运营和供应链协同的战略基础。<strong>选对 SRM 系统等于为企业供应链打下坚实的数字化基础</strong>。<br/>本文整理的榜单结合了国内 SRM 供应商和国际通行的优秀方案，同时结合零售行业具体特征，为不同规模和发展阶段的零售企业提供实用选型参考。</p>]]></description></item><item>    <title><![CDATA[Copilot 2026 完全指南：2026 年了，它凭什么还是月活第一的 AI 编程助手？ 卡尔A]]></title>    <link>https://segmentfault.com/a/1190000047606576</link>    <guid>https://segmentfault.com/a/1190000047606576</guid>    <pubDate>2026-02-11 22:04:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文共 5200 字，阅读预计需要 6 分钟。</p><p>编程 IDE 赛道卷成红海，Cursor、Claude Code、Google AI Studio 各有拥趸。但 Copilot 在 2026 年依然稳坐月活第一的位置，它凭什么？</p><p>这篇文章，从我个人深度使用的体验出发，<strong>拆解 Copilot 的三个核心优势、三个明显劣势、以及六个核心特性的实战用法</strong>——帮你判断，它到底适不适合你。</p><p><img width="474" height="474" referrerpolicy="no-referrer" src="/img/bVdnUNB" alt="" title=""/></p><p><strong>一次 Cursor 账单，让我彻底想明白了</strong></p><p>先说一件真事。</p><p>前几天，我在 Cursor Max 模式下，开了 Plan 模式，用 Claude Opus 4.6 重构一个项目的页面样式。</p><p>一次 Plan 制定，一次 Plan 修改，再加上按照 Plan 执行工作。</p><p><strong>三个步骤，花掉了十几刀的 token 费用。</strong></p><p><img width="723" height="145" referrerpolicy="no-referrer" src="/img/bVdnUNC" alt="" title="" loading="lazy"/></p><p>然后我算了一下，这三个步骤如果在 Copilot 里做，就是三次独立的调用。再乘以 Copilot 对 Opus 4.6 设置的系数 3，也就是消耗 9 次 premium request。</p><p><strong>而 Copilot 每月 10 美金的订阅套餐里，有 300 次这样的调用额度。</strong></p><p>换句话说，Cursor 里花十几刀干的活，Copilot 只扣了 9 次调用——连月度额度的 3% 都不到。</p><p>这个差距，让我彻底想明白了一件事：<strong>在长期、复杂项目使用的场景下，成本结构比单次能力更重要。</strong></p><p>说实在的，我不是说 Cursor 不好。Cursor 在很多方面确实更强，后面会讲到。但作为一个每天都要写代码、调研资料、做内容创作的人，我需要一个成本可控的主力工具，而不是每次点「执行」后都要关注token用量。</p><p><strong>三个让我留下来的理由</strong></p><p><strong>按次计费：O(1) 复杂度的成本控制</strong></p><p>Copilot 最核心的竞争力，就是它的按次计费逻辑。</p><p>每月 10 美金的 Pro 套餐，包含 300 次 premium request。即使超额，每次也只收 4 美分。</p><p>这意味着什么？</p><p>它不会因为你用了最贵的 Claude Opus 4.6，就让你反复盯着 token 用量看。它只是出于成本考虑，让一次调用消耗 3 次 premium request 的额度——但费用不会随 token 用量上涨而上涨。</p><p><img width="723" height="454" referrerpolicy="no-referrer" src="/img/bVdnUND" alt="" title="" loading="lazy"/></p><p>我喜欢用一个程序员都懂的类比：<strong>这就像 O(1) 复杂度的算法。</strong></p><p>不管你输入多大，成本是固定的。</p><p>相比之下，Cursor 的计费更接近 O(n)——输入越大，token 越多，费用越高。Cursor Pro 每月 20 刀，Pro+ 每月 60 刀，Ultra 每月 200 刀，而且这些套餐背后还是基于用量的逻辑。</p><p>在大型项目的重构修改、大量资料调研，或者长上下文的内容创作中，<strong>Copilot 的开销可能只有按 token 计费的几十分之一。</strong></p><p>对于经常要调用强模型做大型任务的开发者来说，这个差距是实打实的。</p><p><strong>新模型第一时间支持 + 无限 Tab 补全</strong></p><p>Copilot 对新模型的支持速度一直很快——只要厂商开放了 API，基本第一时间就能用上。</p><p><img width="669" height="1077" referrerpolicy="no-referrer" src="/img/bVdnUNE" alt="" title="" loading="lazy"/></p><p>而且 GPT-5 mini 、Gpt-4o等的调用和 Tab 代码补全，在 Pro 套餐里都是无限次数的。</p><p>这个"无限"很重要。Tab 补全可能是你写代码时每分钟都在用的功能，随手做点修改，增添个函数，如果这个也限次数，那体验会非常割裂。Copilot 在这一点上没有扣扣搜搜。</p><p>当然也有例外。比如 GPT-5.3 Codex并没有支持，但是这个的原因是 OpenAI 延迟开放 API 的老传统，现在三方 IDE 都用不了。</p><p><strong>GitHub 生态的原生力量</strong></p><p>这一点经常被忽略，但其实很关键。</p><p>Copilot 背靠微软和 GitHub，如果你平常接触开源比较多，它的生态整合是相当完整的。GitHub 的 issue、PR、仓库索引，都是原生集成的，<strong>不需要任何额外配置。</strong></p><p>Copilot 的 coding agent 在上线后的前 5 个月里，开发者用它合并了超过 100 万个 Pull Request。这个数字本身就说明了生态粘性。</p><p>另外，Copilot 对 Plan 模式和 Skills 的支持也做得不错。Plan 模式可以让模型先规划再执行，Skills 则是一个可扩展的工具提示词集合——比如 Anthropic 官方出的前端设计 Skill，可以显著提升 Copilot 做前端的效果。这些在后面的特性拆解里会详细讲。</p><p><strong>三个不可忽视的缺点</strong></p><p>说完优势，聊聊让我不太舒服的地方。</p><p><strong>前端样式：Copilot 的"审美盲区"</strong></p><p>这是最明显的短板。</p><p>Copilot 在前端样式、UI 设计这些方面，和 Cursor、Google AI Studio 的差距比较大。尤其是用 GPT 系列模型的时候，尤其是Codex，出来的页面效果。。。说实在的特别难评。</p><p>以下是我在copilot里，用GPT-5.1-Codex-Max，做的火柴人小游戏：</p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnUNG" alt="" title="" loading="lazy"/></p><p>这个页面的设计审美真的有点过分了。。。</p><p>后来我接了 Anthropic 官方发的前端设计 Skill 之后，效果能好一些，但如果你真的要用 Copilot 做前端样式类的工作，<strong>最好还是切到 Claude Opus 来搞。包括写作，我个人体感更好的也是Claude的模型。</strong></p><p><strong>超大型任务：和 Cursor Max 的差距</strong></p><p>Copilot 的 Agent 模式在超大型任务上的执行力，和 Cursor 有一点差距。</p><p>虽然我个人体感这个差距很小，日常使用几乎感觉不到，但是，Cursor 开启 Max 模式后，是能感觉出来的——同样的任务，同样的模型，Cursor 对<strong>复杂项目、复杂任务的精确执行和精确理解</strong>做得要好一些。</p><p>而且，除了Gpt-5.2 Codex，<strong>copilot对其他模型的上下文limit基本都是128K</strong>，这也是限制它超长任务执行能力的关键。</p><p><img width="723" height="129" referrerpolicy="no-referrer" src="/img/bVdnUNH" alt="" title="" loading="lazy"/></p><p>甚至 Cursor 还有多 Agent 竞赛的功能，可以让多个 Agent 同时尝试不同方案，然后你选最好的那个。</p><p>这个"更强"的代价，也许是几十倍的成本，但在一些场景下是真的有对应的收益的。</p><p>所以这事得看你怎么算账：是为了 5% 的精度提升花数倍甚至数十倍的钱，还是用 Copilot 把 90% 的活先干了，剩下多去迭代几轮或者自己上？</p><p><strong>自定义规则和记忆：灵活度不够</strong></p><p>Cursor 有 rules 这样的系统，能做到项目级的规则配置，还有记忆功能来记住用户的编程偏好——比如你喜欢用什么命名规范、偏好什么代码风格，它都能记住。基本接一个cursor-memory-bank，就能很方便的实现。</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnUNI" alt="" title="" loading="lazy"/></p><p>Copilot 虽然也有 .github/copilot-instructions.md，但说实话，灵活度和粒度上差不少。</p><p>这就好比一个能记住你口味的老厨师，和一个每次都要重新告诉他"少盐少油"的新厨师。做出来的菜可能差不多，但沟通成本差很多。</p><p><strong>六个核心特性实战手册</strong></p><p>说了这么多宏观的优劣势，我们来看看 Copilot 各个核心特性的具体用法。<strong>这部分比较实操，建议收藏。</strong></p><p><strong>1. Tab 补全：最成熟，也最容易被低估</strong></p><p>Tab 补全是 Copilot 最早出名、也是最成熟的功能。订阅 Pro 之后无限次使用。</p><p>你在写代码的时候，它会实时预测你接下来要写的内容，按 Tab 就能接受建议。对于逻辑简单的函数，你甚至可以只写一行注释，然后靠 Tab 补全快速把代码写完。</p><p><img width="723" height="212" referrerpolicy="no-referrer" src="/img/bVdnUNJ" alt="" title="" loading="lazy"/></p><p>这个功能有几个小技巧，很多人不知道：</p><p><strong>第一，写好注释再写代码。</strong> 注释越清晰，补全质量越高。这其实就是在给模型做上下文工程——你的注释就是 prompt。</p><p><strong>第二，打开相关文件放在旁边的 Tab 里。</strong> Copilot 会自动把打开的文件当作上下文来参考。所以如果你在写一个调用其他模块的函数，把那个模块文件打开放旁边就行。</p><p><img width="723" height="215" referrerpolicy="no-referrer" src="/img/bVdnUNK" alt="" title="" loading="lazy"/></p><p><strong>第三，Ctrl+右箭头逐词接受。</strong> 如果补全的内容只对了一半，不用全盘接受或全盘拒绝，按 Ctrl+右箭头可以一个词一个词地接受。这个技巧能省很多手动修改的时间。</p><p><strong>2. Inline Chat：小范围精修利器</strong></p><p><img width="645" height="294" referrerpolicy="no-referrer" src="/img/bVdnUNL" alt="" title="" loading="lazy"/></p><p>在代码里按 Ctrl+I（Mac 上是 Cmd+I），可以直接在当前位置发起一次对话。</p><p>适合小范围的修改，比如"给这个函数加上错误处理"、"把这段逻辑重构成异步的"之类的。</p><p>它的好处是改完直接有 diff 预览，你可以逐行审查，不满意就撤销。比在聊天窗口里来回复制粘贴高效得多。</p><p><img width="597" height="327" referrerpolicy="no-referrer" src="/img/bVdnUNM" alt="" title="" loading="lazy"/></p><p>很多人常用 Chat 面板做小修改，但其实<strong>微调切到 Inline Chat 效率更高也更精准</strong>。</p><p><strong>3. Ask 模式：纯对话，不动代码</strong></p><p><img width="684" height="333" referrerpolicy="no-referrer" src="/img/bVdnUNN" alt="" title="" loading="lazy"/></p><p>Ask 模式就是侧边栏的对话窗口。在这里可以选模型、问问题、讨论方案。</p><p>它的特点是"纯对话，不动代码"——不会帮你直接改文件，<strong>但你可以引用工作区的文件给它</strong>。</p><p>我的习惯是，把需要的文件直接选中后，鼠标拖到对话框里，省掉复制粘贴。</p><p><img width="569" height="243" referrerpolicy="no-referrer" src="/img/bVdnUNO" alt="" title="" loading="lazy"/></p><p>不过，Copilot 对工作区的文件都有访问权限，显式引用只是提醒模型重点关注。多个项目的话，从左上角把文件夹添加到工作区即可。</p><p><img width="489" height="486" referrerpolicy="no-referrer" src="/img/bVdnUNP" alt="" title="" loading="lazy"/></p><p><strong>4. Agent 模式：核心中的核心</strong></p><p>Agent 模式是各个编程 IDE 最核心的功能，也是 Copilot 用得最多的模式。</p><p>在 Agent 模式下，Copilot 可以自主地读文件、写文件、跑终端命令、分析报错，然后迭代修复，直到任务完成。</p><p><strong>按次计费的爽感就体现在这里：即使迭代了特别多轮，它还是只收一次的费用。</strong></p><p>另外一个很实用的细节：Agent 模式里可以<strong>控制模型可用的工具</strong>。</p><p><img width="444" height="211" referrerpolicy="no-referrer" src="/img/bVdnUNQ" alt="" title="" loading="lazy"/></p><p>比如你只想让它帮你解决部署问题但不修改任何文件，可以把文件编辑的工具禁用掉。</p><p>这在生产环境排查问题的时候特别有用，<strong>相当于强制禁止文件修改。</strong></p><p><img width="700" height="366" referrerpolicy="no-referrer" src="/img/bVdnUNR" alt="" title="" loading="lazy"/></p><p><strong>5. Plan 模式：复杂任务专属</strong></p><p><img width="723" height="266" referrerpolicy="no-referrer" src="/img/bVdnUNS" alt="" title="" loading="lazy"/></p><p><strong>Plan 模式</strong>是专门用来做复杂任务的。</p><p>它会强制模型输出完整的执行计划，并且从执行来看会启动一些子 agent 来做信息收集，防止主 Agent 的上下文过长。</p><p>关键是：在你点击「Start Implementation」之前，你可以和模型反复对话来修改计划。</p><p><strong>即使你告诉它"现在开始执行任务"，只要还在 Plan 模式下，它还是只做计划生成。</strong></p><p>所以，「Start Implementation」本质上就是点击后，</p><p>1、帮你切换到了 Agent 模式，</p><p>2、把「Start Implementation」输入到输入框中</p><p>因此如果你读计划读得差不多了，<strong>自己手动切到 Agent 模式让它执行，效果是一样的。</strong></p><p>我的习惯是：<strong>涉及复杂的、大量文件改动的任务，都先让它出 Plan，确认没问题了再放手让它跑。</strong></p><p><img width="723" height="645" referrerpolicy="no-referrer" src="/img/bVdnUNT" alt="" title="" loading="lazy"/></p><p><strong>6. Skills：各IDE的“杀手级”功能</strong></p><p><strong>Skills</strong> 是近两个月，copilot刚支持的功能。</p><p><img width="723" height="347" referrerpolicy="no-referrer" src="/img/bVdnUNU" alt="" title="" loading="lazy"/></p><p>在设置中打开 userAgentSkills 的开关，把 Skills 文件下载到指定路径下，模型就能读取和使用这些 Skills 了。</p><p><img width="684" height="579" referrerpolicy="no-referrer" src="/img/bVdnUNV" alt="" title="" loading="lazy"/></p><p>比如 Anthropic 官方出的前端设计 Skill，能显著提升 Copilot 生成前端代码的质量。这其实就是一套精心设计的系统提示词，告诉模型在做前端任务时应该遵循哪些设计原则和最佳实践。</p><p><strong>写在最后：适合你的，才是最好的</strong></p><p>总结一下这一期的内容。</p><p>Copilot 的三个核心优势：<strong>按次计费成本可控、新模型支持快、GitHub 生态整合强。</strong></p><p>三个主要劣势：<strong>前端样式效果不好、超大型任务的执行力略逊、自定义规则和记忆能力不够灵活。</strong></p><p>六个核心功能的使用方法：<strong>Tab 补全、Inline Chat、Chat 面板里的 Ask、Agent 和 Plan 三种模式，以及 Skills。</strong></p><p>这些优缺点都很明显，没有哪个工具是完美的。关键是匹配你的场景。</p><p>我个人的建议是这样的：</p><p><strong>1、如果你经常做大型的后端项目，且频繁调用比较强的模型来做重构、修改之类的工作</strong> → Copilot 的按次计费逻辑会帮你省非常多的钱。这是它最核心的优势。</p><p><strong>2、如果你追求极致的执行精度，不太在乎 token 开销</strong> → Cursor 开 Max 模式确实体感更强，甚至可以开启多 Agent 竞赛的功能。但月均开支要做好心理准备。</p><p><strong>3、如果你更多是做小的演示 demo 或者 UI 设计</strong> → Google AI Studio 也许更适合你。免费额度够用，从想法到可运行的小项目特别快。</p><p>**总之，我的建议是组合着用。**Copilot 当主力省成本，Cursor Max 当精度补刀，Google AI Studio 做快速验证。</p><p>这套组合拳打下来，性价比是最高的。</p><p><strong>既然看到这了，如果觉得不错，随手点个赞、收藏、转发三连吧～</strong></p><p><strong>我是Carl，大厂研发裸辞的AI创业者，只讲能落地的AI干货。</strong></p><p><strong>关注我，更多AI趋势与实战，我们下期再见！</strong></p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdnUis" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Agent Lightning：微软开源的框架无关 Agent 训练方案，LangChain/Aut]]></title>    <link>https://segmentfault.com/a/1190000047606596</link>    <guid>https://segmentfault.com/a/1190000047606596</guid>    <pubDate>2026-02-11 22:03:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Agent 搭建起来之后怎么让它真正变得越来越好？搭建完成后的优化就很少有人认真说过。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606598" alt="" title=""/><br/>Agent Lightning 号称能把任何 AI Agent 变成"可优化的猛兽"，而且几乎不用改代码。那问题来了，市面上 Agent 框架满天飞这个凭什么就不一样呢？</p><h2>training gap</h2><p>做过 Agent 部署的人大概都有同感：把 Agent 跑起来其实没那么难，真正难的是让它持续进步。</p><p>OpenAI 的 Agent SDK、LangChain 这类编排框架，原型设计和快速部署确实很拿手。几个小时就能让一个能用的 Agent 上线。但到了优化这一步，用真实场景的反馈去训练 Agent、提升它的表现基本就只能靠自己摸索了。</p><p>微软的研究人员给这个问题起了个名字叫"training gap"。开发环境里跑得好好的 Agent一碰到真实用户、边缘场景和领域特有的问题性能就打折扣。传统框架能给你的帮助很有限：手动调 prompt，手动改参数，然后顺带祈祷别有问题。</p><p>而Agent Lightning 的切入点就在这里，它把 Agent 框架和优化基础设施做了解耦。微软的说法是这套方案"可以无缝地为任何现有 Agent 启用模型训练，无需对 Agent 代码做任何修改。"</p><h2>Agent Lightning 的工作原理</h2><p>Agent Lightning 在现有 Agent 代码和微软的 verl 训练基础设施之间插入了一层客户端-服务器架构。可以理解为一个翻译层：把 Agent 的交互记录转化成训练数据，优化完参数再塞回去。</p><p>具体流程是：Agent 照常运行，什么都不用改，但每一次交互都会被 Lightning 客户端截获。数据会传到 Lightning 服务器，服务器端跑强化学习、自动 prompt 优化、监督微调这些手段，再把改进后的参数推回到 Agent 里。</p><p>特别值得说的是框架的兼容性：LangChain、AutoGen、CrewAI、微软自家的 Agent Framework都能接。团队管它叫"Lightning AI Agent 的终极训练器"。</p><p>安装也是直接一个pip命令：</p><pre><code>pip install agentlightning</code></pre><p>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606599" alt="" title="" loading="lazy"/></p><h2>实际应用和用例</h2><p>最有说服力的场景是 Agent 需要适配私有数据或者特定行业需求的情况。通用预训练模型处理常规任务还行，碰到公司内部流程、行业“黑话”、独特的业务逻辑，就容易出问题。</p><p>拿客服 Agent 举例：它得学会你公司特有的工单升级流程、产品的各种坑、跟客户打交道的语气和方式。传统做法是手写 prompt 然后盼着它能泛化到各种情况。换成 Agent Lightning系统能直接从真实客户对话中学习，拿解决率、满意度评分、各项业务指标来自动优化响应策略。</p><p>代码生成也是个很适合的场景：Agent 在跟你的代码库、编码规范、开发流程不断交互的过程中，Agent Lightning 能持续微调模型，让它越来越贴合团队的具体要求。</p><p>搜索和检索类应用也一样，Agent 需要弄清楚哪些信息源对哪类查询最有价值、怎么按用户偏好排序结果、什么时候该转人工，这些都可以在实际使用中不断优化。</p><h2>竞争格局</h2><p>Agent Lightning 进入的赛道已经很拥挤了，但定位上有明确的差异化。别人在卷 Agent 编排和模型服务，而微软选择切入的是一个几乎没人认真做过的方向：优化。</p><p>Agent 优化可以说是平台策略的自然延伸，通过解决那些单纯做模型或做编排的玩家解决不了的问题，把开发者留在微软的生态里。</p><p>而且Agent Lightning 没有被包装成 Azure 的专属服务而是直接开源，这既展现了微软对自身平台能力的信心，也说明他们对推动这个领域发展有诚意。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606600" alt="" title="" loading="lazy"/></p><h2>总结</h2><p>AI Agent 行业一直在解决"怎么搭"，却没认真回答"搭完之后怎么办"。而Agent Lightning 把开发和优化解耦这个思路填补了从 LangChain 到 AutoGen 这一批框架都没覆盖到的空白。</p><p>但是从版本能看得出来，0.1.2 版离生产级还有距离。但方向本身没问题，当 AI Agent 越来越多地承担关键业务，能持续从真实反馈中学习的 Agent 和不能的之间差距只会越拉越大。谁先跑通这条优化闭环，谁就拿到了下一阶段的门票。</p><p><a href="https://link.segmentfault.com/?enc=RYzO1TPOgfo084XpPgGTKQ%3D%3D.J4w%2FzlaM5Wz%2BnS7HGpUKd%2BHQWO1wniz7ZrLEraxpvu6XMYnTkyokx7C6kK3k%2FLeEvr6WIwphr5OjxdJZGg6sQQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/eea592726e5940c29d80fadf9908b2e6</a></p><p>by Mandar Karhade</p>]]></description></item><item>    <title><![CDATA[《GraphQL批处理与全局缓存共享的底层逻辑》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047606613</link>    <guid>https://segmentfault.com/a/1190000047606613</guid>    <pubDate>2026-02-11 22:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>微前端架构在分布式前端体系的深度落地过程中，跨应用数据请求的冗余分发已然成为制约前端整体效能提升的核心桎梏，传统碎片化的请求发起模式下，彼此解耦的微应用针对同源基础元数据的重复拉取行为，不仅持续加剧网络传输层的资源损耗与带宽占用，更会间接引发页面渲染时序的紊乱、前端状态的不同步等隐性架构问题，GraphQL批处理与缓存共享的融合落地方案，绝非对现有请求机制的表层修补与局部优化，而是从数据调度逻辑与状态共识构建的底层内核出发，彻底重构微前端体系内数据流转的核心范式。批处理机制的核心价值在于将离散化、碎片化的独立请求，转化为具备语义关联的聚合调度单元，彻底打破应用物理边界对请求链路的人为割裂，缓存共享则致力于搭建跨应用的全局数据共识层，让同源数据在整个微应用集群中实现一次采集、全域复用的理想状态。这一技术思路的本质是把数据请求从单一应用的独立行为，升级为整个架构层面的协同动作，从根源上消解重复请求生成的土壤，让微前端的数据交互回归高效、统一、可控的理想状态，也让前端架构从被动适配业务需求的底层形态，转向主动治理数据流转的高阶形态，在分布式前端体系中建立起数据流转的秩序感与稳定性，让每一次数据交互都能贴合架构的整体设计逻辑，而非无序消耗系统资源。</p><p>GraphQL批处理在微前端复杂场景中的落地，核心依托于对请求依赖的拓扑化深度分析与聚合粒度的精细化动态调控，在多微应用并行初始化的典型业务场景中，各类基础配置信息、核心主体元数据、全局权限规则等非业务独占型数据，成为跨应用重复请求的高发区域，批处理机制并非简单将多条独立请求机械合并为单一传输链路，而是先完成请求语义的精准归类与依赖关系的逐层拆解，严格区分实时性要求较高的动态数据与稳定性较强的静态数据，仅对同数据域、同执行优先级、同生命周期的请求执行聚合调度操作，同时完整保留每一条请求的独立响应解析能力，从机制上避免单一请求异常引发整体链路的响应故障。实践过程中通过精准界定请求的共享域标识，让批处理引擎能够精准识别可聚合的请求单元，既最大化保障请求合并带来的效能收益，又不破坏每一个微应用的数据独立性与业务自治性，让批处理成为适配微前端弹性架构的轻量化调度能力，也让请求聚合从机械合并的初级形态升级为语义驱动的智能调度形态，大幅提升数据交互的精准度与稳定性，让每一次网络传输都能实现资源利用的最大化，彻底规避无效请求与重复传输带来的资源内耗，让请求调度逻辑贴合微前端架构的解耦核心诉求。</p><p>微前端缓存共享体系的构建，核心是打造分层可控、逻辑清晰的跨应用状态共识体系，而非粗暴的全局数据拷贝与无差别共享，基于微前端基座的中继调度能力，将缓存体系划分为全局公共缓存、跨应用共享缓存、应用私有缓存三个逻辑层级，其中公共缓存专门承载全应用复用的基础元数据，共享缓存适配多应用协同的业务关联数据，私有缓存则全力保障单应用的业务隔离性与数据私密性。缓存共享的核心价值不在于存储本身，而在于跨应用的数据同步与一致性保障，通过语义化的缓存版本标识与轻量化订阅分发机制，实现缓存更新的全域无感同步，同时设计精细化的失效触发规则，结合数据更新事件与应用生命周期节点，主动清理过期缓存数据，避免脏数据在整个微应用集群中扩散。实践中通过基座的缓存代理层，统一管控缓存的读写操作与数据分发流程，让微应用无需感知底层缓存的实现细节，仅通过标准化接口即可获取共享数据，大幅降低跨应用数据协同的适配成本，也让缓存管理从分散失控的初级形态转向集中可控的架构级能力，在多应用共存的复杂环境中持续维持数据状态的统一与可信，为微前端的数据协同提供稳定的底层支撑。</p><p>GraphQL批处理与缓存共享的协同运转，构建起请求调度、数据缓存、全域分发的闭环治理体系，二者的耦合逻辑并非简单的功能叠加，而是相互赋能、深度融合的有机整体，批处理引擎为缓存层提供高质量、归一化的标准数据源，彻底避免碎片化请求带来的数据格式差异与字段冲突问题，缓存层则为批处理提供前置的命中校验能力，从源头大幅减少重复请求的触发频次。在多微应用并行加载的实际业务场景中，系统先通过共享缓存层完成同源数据的原子性命中判定，缓存命中时直接向各依赖微应用分发标准化数据，未命中时则由批处理引擎聚合所有待请求单元，生成单一高效的调度链路执行数据拉取操作，响应结果经归一化处理后存入共享缓存，再同步至所有依赖该数据的微应用。这一过程中，缓存命中的原子性判定与批处理的防重触发机制，成为保障协同稳定性的核心节点，让数据请求与缓存复用的衔接实现无间隙、无冗余，也让整个数据流转链路形成自驱式的效能优化闭环，持续降低系统的网络负载与计算消耗，让数据交互的每一个环节都能实现最优效率，彻底解决微前端架构中重复请求的行业痛点。</p><p>该技术方案在微前端架构中的规模化落地，需聚焦非功能维度的精细化优化与架构适配，批处理的效能发挥依赖合理的性能阈值动态设定，结合实时网络环境与页面渲染时序要求，动态调整请求聚合的等待窗口与最大聚合粒度，避免过度聚合引发的响应延迟问题，缓存共享则注重内存资源的轻量化管控，通过数据过期策略与懒加载机制，精准控制缓存存储的资源占用规模，同时将批处理拦截与缓存代理逻辑，无缝融入微前端的应用加载与全生命周期流程，不侵入微应用的核心业务代码，保持各应用的技术栈无关性与业务自治能力。实践中通过架构层的统一封装，让数据协同能力成为微前端基座的原生能力，无需各微应用单独适配改造，同时构建数据流转的全链路感知体系，让请求调度、缓存命中、数据分发的全流程可观测、可追溯，为后续优化与迭代提供精准的数据依据，保障方案长期迭代的可扩展性，也让微前端的数据治理能力具备持续演进的底层支撑，完美适配业务规模与架构形态的动态变化，为大型分布式前端体系的稳定运行保驾护航。</p><p>从技术实践的长期视角来看，GraphQL批处理与缓存共享的深度融合，并非临时性的性能优化手段，而是微前端数据治理体系的核心基础能力，这一方案解决的不仅是重复请求的表层问题，更构建了跨应用数据协同的标准化技术范式，让微前端架构从应用拼装的初级形态，升级为数据一体化的成熟形态。通过彻底消解数据孤岛与请求冗余，大幅降低系统的网络开销与渲染阻塞风险，全面提升整体用户体验与系统运行稳定性，其核心价值在于用极简的架构逻辑，解决分布式前端的复杂数据问题，回归技术服务于业务的本质。</p>]]></description></item><item>    <title><![CDATA[《GraphQL状态图建模与低时延控制能力解析》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047606616</link>    <guid>https://segmentfault.com/a/1190000047606616</guid>    <pubDate>2026-02-11 22:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>物联网设备态联拓扑的规模化落地进程中，设备状态图的高效查询与控制指令的低时延调度，已然成为构筑全域物联交互体系的核心命题，传统物联查询接口的刚性范式，始终难以适配异构设备的态数据柔性获取需求，固定字段与固定接口的设计逻辑，无法匹配设备状态图动态变化的拓扑结构，更难以满足多场景下差异化的态数据拾取诉求，GraphQL以态联查询的独特技术特性切入设备状态图交互场景，彻底打破了固定接口与设备态拓扑的适配壁垒，其在设备状态图查询中的优劣势深度博弈，本质是态粒度定制化能力与物联链路客观约束性的动态平衡，而实时订阅机制对设备控制指令低延迟需求的实际适配能力，更是直接决定物联控制层整体交互效能的关键标尺。设备状态图并非单一的态数据简单罗列，而是涵盖设备本体运行态、集群协同联动态、环境感知关联态的三维拓扑化态联结构，GraphQL对这一复杂结构的解构与精准查询能力，从底层重构了物联态数据的交互逻辑，也让传统物联查询从被动适配场景转向主动建模需求，这一技术选型的深层思考，必须扎根物联场景的链路传输特性、终端算力边界、业务交互核心诉求，而非单纯依托技术表层特性做浅层次落地应用。态查询的柔性价值与物联场景的客观约束，共同构成了GraphQL在物联网场景中落地的核心考量维度，也让设备态查询与指令控制的协同交互，拥有了全新的技术探索方向，这种从技术本质到场景深度适配的全维度思考，也是物联网前端交互技术迭代升级的核心逻辑，更是区分技术炫技与工程落地的关键标尺。</p><p>GraphQL在物联网设备状态图查询中的核心优势，完全根植于态粒度的定制化拾取与态联拓扑的柔性解析能力，设备状态图本身承载着多维度、多层级的态数据信息，从设备基础运行态、功能模块工作态，到深层集群联动状态、环境关联响应态，不同业务场景对态数据的拾取需求存在极其显著的差异性，传统查询模式需要依托多接口拆分适配不同需求场景，极易产生态数据冗余传输、链路资源无效消耗、终端解析压力过载等一系列问题，GraphQL可依据实际交互需求，精准拾取设备状态图中的目标态字段，完全无需传输冗余无效数据，完美适配物联网终端带宽有限、算力薄弱、续航敏感的客观特性，同时其态联查询能力可深度解析设备状态图的拓扑关联逻辑，实现跨设备、跨集群、跨区域的态数据联动查询，统一异构设备的态查询口径，大幅降低多类型终端接入的适配成本与开发周期。设备状态图的态元数据自描述特性，还能让前端交互层快速感知态数据结构与关联关系，简化设备态可视化的开发流程，让设备状态图的查询从固定范式转向柔性建模，大幅提升物联态数据的传输、解析与渲染全链路效能，也为物联网设备态的精细化管理、全域化监控提供了核心技术支撑，这种按需适配、精准获取的查询特性，让物联网多终端、多场景、多协议的态数据交互拥有了更灵活、更高效的实现路径，也让物联感知层的数据采集效率实现了质的飞跃。</p><p>GraphQL应用于物联网设备状态图查询的显性短板，集中体现在复杂态联拓扑的解析开销与场景化适配的多重约束层面，设备状态图的拓扑关联越复杂、层级越丰富，GraphQL的态查询解析单元需要处理的关联逻辑就越繁杂，这一过程会持续消耗服务端与边缘节点的运算资源，在边缘算力受限、供电紧张的物联网场景中，解析开销会直接转化为态查询的响应延迟，进而影响物联交互的实时性与稳定性。定制化的态查询需求需要后端构建精细化的态联解析逻辑，每一次设备状态图的拓扑迭代、态字段新增，都需要同步调整解析规则，大幅提升了设备状态图的维护与迭代成本，不同物联网终端的算力差异、存储差异、适配能力差异，也让轻量级传感设备、低功耗终端难以适配复杂的态查询解析流程，形成柔性查询与终端适配性的核心矛盾。同时跨域态联查询的协同约束，会让设备状态图的跨节点查询面临链路损耗、节点跳转延迟等问题，进一步放大技术特性带来的性能短板，这些劣势并非技术本身的固有缺陷，而是GraphQL的柔性特性与物联网场景客观约束碰撞产生的适配问题，也是落地过程中需要重点攻克、分层优化的核心难点，这种技术特性与场景约束的天然冲突，也是物联网技术选型中必须直面、理性权衡的现实问题，无法通过简单的参数调整实现完全消解。</p><p>GraphQL实时订阅机制为物联网设备控制指令的交互提供了全新的技术实现路径，其依托持久化连接构建的态推送体系，彻底摒弃了传统轮询模式的资源浪费与时延损耗，成为适配设备控制指令低延迟需求的核心支撑能力，实时订阅可精准绑定设备状态图与控制指令的关联关系，当控制指令下发或设备态发生变更时，通过增量推送机制仅传输核心指令与变更态数据，大幅缩短数据传输的链路时长与载荷体积。在物联控制场景中，边缘节点可作为订阅中继节点，承接云端与终端的指令中转任务，进一步压缩指令传输的物理路径，降低端到端的响应延迟，订阅会话的轻量化管理机制，可支撑多设备、多集群并发的指令订阅需求，避免会话冗余带来的资源抢占与链路拥堵，同时指令与态数据的双向订阅交互，能让控制指令的下发与设备态的反馈形成完整闭环，保障物联控制的精准性与实时性。这一机制的核心价值，在于将传统的被动查询转为主动推送，让设备控制指令的交互逻辑完全贴合物联场景的低延迟诉求，也让物联控制层的交互效率实现了质的提升，边缘侧的本地化订阅处理，还能进一步降低云端依赖，提升指令响应的稳定性，即便在弱网、断网边缘场景中，也能保障核心控制指令的本地执行与状态同步，让物联控制的可靠性得到全方位保障。</p><p>GraphQL实时订阅对设备控制指令低延迟需求的满足能力，存在明确的场景化适配边界，并非能够全场景覆盖物联控制的严苛时延要求，在高密度设备集群的集中控制场景中，大量并发订阅会话会挤占传输带宽与运算资源，导致指令推送的链路拥堵、排队延迟，直接放大整体响应延迟。物联网场景的网络波动性、不稳定性，会直接影响持久化连接的稳定性，连接抖动、中断会直接打破实时订阅的低延迟保障，边缘节点的算力分配若偏向设备状态图的解析处理，会挤占控制指令的调度资源，形成查询与订阅的资源抢占矛盾，进一步加剧时延问题。不同协议物联网设备的指令转换环节，会产生额外的时延损耗，让高要求的低延迟需求难以落地，同时订阅机制的保活逻辑需要持续消耗链路资源与终端算力，在弱网、窄带环境中，保活机制的失效会直接中断指令推送，影响控制指令的实时传递与执行。这些适配边界的存在，要求实时订阅机制必须结合物联场景特性做定制化优化，而非盲目套用通用化的订阅逻辑，这种场景化的适配思考、差异化的策略调整，也是物联网技术落地的核心准则，更是保障控制指令低延迟需求落地的关键前提。</p><p>物联网场景中GraphQL的落地应用，需要依托场景特性制定差异化的选型策略与全维度优化方案，平衡设备状态图查询的优劣势，精准适配控制指令的低延迟需求，针对设备状态图查询，可采用分层态联建模的方式，拆解复杂拓扑的关联逻辑，简化解析流程，降低服务端与边缘节点的运算开销，针对轻量级终端、低功耗设备，简化态查询的解析流程，裁剪非核心功能，保障终端的适配性与运行稳定性。</p>]]></description></item><item>    <title><![CDATA[大模型榜单周报（2026/02/08） KAI智习 ]]></title>    <link>https://segmentfault.com/a/1190000047606621</link>    <guid>https://segmentfault.com/a/1190000047606621</guid>    <pubDate>2026-02-11 22:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 本周概览</h2><p>本周大模型行业呈现多维度竞争格局，模型调用量榜单出现显著变化，Google Gemini 3 Flash Preview强势登顶，Kimi K2.5爆发式增长。各大厂商密集发布新模型，OpenAI推出GPT-5.3-Codex编码模型，Anthropic发布Claude Opus 4.6，美团推出多模态统一大模型方案STAR，快手可灵AI发布3.0版本，上海AI实验室发布书生Intern-S1-Pro。编程能力榜单中，Kimi K2.5-thinking成为国产编程模型榜首。前沿数学能力榜单出现重大调整，Claude Opus 4.5 (no thinking)成绩暴增跃居前三。</p><h2>2. 重点关注事件</h2><ul><li><strong>OpenAI发布GPT-5.3-Codex编码模型</strong>（2.6）：融合GPT-5.2推理能力与GPT-5.2-Codex编码性能，运行速度提升25%，支持终端操作与长期任务。该模型曾参与自身训练调试，被定为首个"高"网络安全风险等级。</li><li><strong>Anthropic发布Claude Opus 4.6</strong>（2.6）：显著提升编码、推理与代理任务能力，首创百万token上下文窗口。Terminal-Bench 2.0等评测领先，GDPval-AA超GPT-5.2达144 Elo分，定价维持$5/$25每百万token不变。</li><li><strong>美团推出多模态统一大模型方案STAR</strong>（2.4）：凭借创新的"堆叠自回归架构 + 任务递进训练"双核心设计，GenEval突破0.91，实现了"理解能力不打折、生成能力达顶尖"的双重突破。</li><li><strong>快手可灵AI发布3.0版本</strong>（2.4）：推出视频3.0与Omni模型，支持智能分镜、图生视频+主体参考、多语种对口型、15秒长视频生成。</li><li><strong>上海AI实验室发布书生Intern-S1-Pro</strong>（2.4）：核心科学能力实现跃升，高难度综合学科评测稳居AI4S领域国际领先水平，复杂数理逻辑推理能力达奥赛金牌水平，面向真实科研流程的智能体能力位居开源模型第一梯队。</li></ul><h2>3. 榜单变化</h2><h3>OpenRouter模型调用量排名</h3><ul><li><strong>整体调用量</strong>：Google Gemini 3 Flash Preview强势登顶，从上周第2位（580B tokens，14%增长）跃升至本周第1位（791B tokens，36%增长），反超Claude Sonnet 4.5成为榜首；Claude Sonnet 4.5退居次席，从上周第1位（766B tokens，15%增长）降至本周第2位（727B tokens，5%增长），环比调用量绝对值减少39B tokens；Kimi K2.5爆发式增长新入前三，本周以673B tokens和350%的增长率位列第3，而上周未进入前十榜单；Grok Code Fast 1大幅下滑，从上周第3位（477B tokens，12%增长）骤降至本周第8位（336B tokens，下降30%），排名下跌5位；MiniMax M2.1高速增长新入榜，本周以371B tokens和115%的增长率位列第7，上周未在榜单中。</li><li><strong>模型市占率</strong>：MoonshotAI爆发式攀升，从上周203B tokens（3.5%，第7位）暴涨至本周606B tokens（8.8%，第5位），份额增长5.3个百分点，排名上升2位；x-ai大幅下滑，从上周719B tokens（12.3%，第4位）骤降至本周587B tokens（8.6%，第6位），份额减少3.7个百分点；MiniMax强势入榜，本周以323B tokens（4.7%）新进入前十榜单第7位；三大巨头份额齐降，Google保持第1但份额从24%降至23%，Anthropic保持第2但份额从17.1%降至15.4%，OpenAI保持第3但份额从14%降至13.4%；DeepSeek稳中有进，从上周553B tokens（9.4%，第5位）增至本周651B tokens（9.5%，第4位），超越x-ai上升1位。</li><li><strong>模型吞吐量</strong>：gpt-oss-120b速度大幅回落，从上周第2位（836 tok/s）骤降至本周第4位（447 tok/s），速度下降46%；Llama 3.1 8B Instruct性价比跃升，从上周第9位（Cerebras提供，203 tok/s，0.10/M）升至本周第6位（Groq提供，306tok/s，0.05/M），速度提升51%且价格降低50%；两款模型跌出前十，上周第5位的Llama 3.3 70B Instruct（265 tok/s）和第8位的Qwen3 Next 80B（233 tok/s）本周退出榜单；两款模型入榜，Llama 4 Maverick（第8位，181 tok/s）和Mistral Small Creative（第9位，180 tok/s）新进入前十；Gemini 2.5 Flash Lite Preview持续提速，从上周第10位（169 tok/s）升至本周第7位（221 tok/s），速度提升31%。</li><li><strong>编程调用量</strong>：Kimi K2.5爆发式增长登顶，从上周第4位（139B tokens，8.9%）暴涨至本周第1位（463B tokens，25.2%），份额激增16.3个百分点；Grok Code Fast 1大幅下滑，从上周榜首（255B tokens，16.4%）骤降至本周第3位（173B tokens，9.4%），份额减少7个百分点；MiniMax M2.1快速攀升，从上周第6位（115B tokens，7.4%）跃升至本周第2位（226B tokens，12.3%），份额增长4.9个百分点；Claude双模型份额齐降，Claude Sonnet 4.5从第2位（12.3%）降至第5位（7.9%），Claude Opus 4.5从第3位（10.0%）降至第4位（8.7%）；GPT-5.2持续收缩，从第8位（61.4B tokens，3.9%）降至第9位（38.7B tokens，2.1%），同时<a href="https://link.segmentfault.com/?enc=YtoYq3Hre6jO1UiT0rJK%2FA%3D%3D.AyFNM5R6Zw%2BUEsNttfzXDYfvfwp2GXtItlB832H%2FqZw%3D" rel="nofollow" target="_blank">https://www.arcee.ai/</a>发布的400B参数稀疏MoE开源模型Trinity Large Preview (free)新进入前十榜单，排名第7位。</li></ul><h3>各领域能力榜单</h3><ul><li><strong>编程能力榜单（Code Arena）</strong>：Kimi K2.5-thinking新晋榜单第5位，仅次于御三家的模型，成为国产编程模型榜首。</li><li><strong>文生图能力榜单（Artificial Analysis Text to Image Leaderboard）</strong>：FLUX.2 [dev] Turbo分数超过Nano Banana，二者排名易位，分别排名9、10。</li><li><strong>理科能力榜单（GPQA LLM Stats）</strong>：Claude Opus 4.6以91.3%的得分排名第4位，仅次于GPT-5.2 Pro、GPT 5.2和Gemini 3 Pro。</li><li><strong>前沿数学能力榜单（EPOCH AI FrontierMath）</strong>：Claude Opus 4.5 (no thinking)成绩暴增跃居前三，从上周五第16位（准确率20.7%，60/290）飙升至本周第3位（38.3%，111/290），准确率提升17.6个百分点；其次是Kimi K2.5 (Fireworks)新进入前十榜单，以27.9%（81/290）排名第10，取代了同系列的Kimi K2 Thinking（21.4%，第15位）。</li><li><strong>GAIA测试集榜单</strong>：LR AILab of Lenovo CTO Org发布的Lemon agent登顶首位。</li></ul><h2>4. 排行榜</h2><table><thead><tr><th>测评类型</th><th>第一名</th><th>第二名</th><th>第三名</th></tr></thead><tbody><tr><td>模型调用量</td><td>Gemini 3 Flash Preview</td><td>Claude Sonnet 4.5</td><td>Kimi K 2.5</td></tr><tr><td>公司市占率</td><td>Google</td><td>Anthropic</td><td>OpenAI</td></tr><tr><td>模型速度</td><td>gpt-oss-safeguard-20b</td><td>Qwen3 32B</td><td>gpt-oss-20b</td></tr><tr><td>编程模型调用量</td><td>Kimi K 2.5</td><td>MiniMax M2.1</td><td>Grok Code Fast 1</td></tr></tbody></table><h3>各公司按不同能力领域排名汇总</h3><table><thead><tr><th>测评类型</th><th>领先公司</th></tr></thead><tbody><tr><td>大语言模型 Text Arena</td><td>Google、xAI、Anthropic、百度、OpenAI、智谱、阿里巴巴、月之暗面</td></tr><tr><td>编程能力 Code Arena</td><td>Anthropic、OpenAI、Google、智谱、MiniMax</td></tr><tr><td>编程能力 LiveCodeBench</td><td>OpenAI、Anthropic、Google</td></tr><tr><td>代码工程任务能力 SWE-benchLite</td><td>基于Claude、Gemini、GPT、Qwen、DeepSeek开发的开源系统</td></tr><tr><td>图像编辑和生成能力 Image Edit Arena</td><td>OpenAI、Google、字节、腾讯、Black Forest Labs、Reve</td></tr><tr><td>文生图能力 Text-to-Image Arena</td><td>OpenAI、Google、Black Forest Labs、腾讯</td></tr><tr><td>图像编辑和生成能力 Image Editing Leaderboard</td><td>OpenAI、Google、字节、Black Forest Labs、阿里巴巴、Reve</td></tr><tr><td>文生图能力 Text to Image Leaderboard</td><td>OpenAI、Google、Black Forest Labs、字节、Fal</td></tr><tr><td>GPQA</td><td>OpenAI、Google、Anthropic、xAI、阿里巴巴</td></tr><tr><td>FrontierMath</td><td>OpenAI、Google、Anthropic、DeepSeek、月之暗面、xAI</td></tr><tr><td>Humanity's Last Exam</td><td>Google、OpenAI、Anthropic</td></tr><tr><td>GAIA</td><td>LR AILab of Lenovo CTO Org、JoinAI、Nvidia、Suzhou AI Lab&amp;Shuqian Tech、Microsoft AI Asia -Ads、ShawnAgent、ZTE-AICloud</td></tr></tbody></table><hr/><p>关注我，第一时间掌握更多AI前沿资讯！</p>]]></description></item><item>    <title><![CDATA[JDK22安装教程 Windows版：详细步骤+验证方法（含下载地址） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047606503</link>    <guid>https://segmentfault.com/a/1190000047606503</guid>    <pubDate>2026-02-11 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><h4><strong>一、准备工作</strong>​</h4><ol><li><strong>获取安装包</strong>：从指定链接下载JDK 22安装包至电脑（链接：<a href="https://link.segmentfault.com/?enc=p%2BshW50iFE2X688RUJDs%2Fg%3D%3D.K9881napwecNUUx6sgp71C1Ta813wRxXC6kMn5VJDH5jg2hD8nuTqmdnnOhDV5zs" rel="nofollow" title="https://pan.quark.cn/s/09ba1b00f415" target="_blank">https://pan.quark.cn/s/09ba1b00f415</a></li></ol><h4><strong>二、安装步骤</strong>​</h4><ol><li><strong>解压安装包</strong>：右键点击下载的安装包文件，选择【解压到当前文件夹】（建议解压至非系统盘，如D盘，避免C盘空间不足）。</li><li><strong>进入安装目录</strong>：打开解压后的【JDK22】文件夹（可通过资源管理器直接双击进入）。</li><li><strong>启动安装程序</strong>：找到【JDK-22.0.2_windows-x64_bin.exe】文件，右键选择【以管理员身份运行】（管理员权限可避免安装路径写入失败等问题）。</li><li><p><strong>确认安装路径</strong>：</p><ul><li>弹出安装向导后，点击【下一步】；</li><li>保持默认安装路径（或自定义路径，建议路径不含中文/空格），再次点击【下一步】。</li></ul></li><li><strong>等待组件安装</strong>：安装过程中会显示“正在更新组件”，耐心等待进度完成（约1-3分钟，无需额外操作）。</li><li><strong>完成安装</strong>：组件更新完毕后，点击【关闭】退出安装向导。</li></ol><h4><strong>三、验证安装成功</strong>​</h4><ol><li><strong>打开命令提示符</strong>：按下快捷键 <code>Win + R</code>调出“运行”窗口，输入 <code>cmd</code>并点击【确定】（或按回车）。</li><li><p><strong>检查JDK版本</strong>：在命令提示符窗口中输入 <code>java -version</code>，按下回车键。若显示类似以下信息，说明安装成功：</p><pre><code>java version "22.0.2" 2022-07-19  
Java(TM) SE Runtime Environment (build 18.0.2+9-61)  
Java HotSpot(TM) 64-Bit Server VM (build 18.0.2+9-61, mixed mode, sharing)</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p></li></ol><h4><strong>注意事项</strong>​</h4><ul><li>若需配置环境变量（如<code>JAVA_HOME</code>、<code>Path</code>），可根据实际需求补充设置（JDK 18默认可能已自动配置基础环境，通过<code>java -version</code>能识别即无需额外操作）。</li><li>安装路径建议简洁（如 <code>D:\Program Files\Java\jdk-22.0.2</code>），避免后续开发工具引用时出错。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[SuperScan4单文件扫描安装步骤详解（附端口扫描与主机存活检测教程） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047606486</link>    <guid>https://segmentfault.com/a/1190000047606486</guid>    <pubDate>2026-02-11 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p> </p><p><code>SuperScan4</code>是 <strong>SuperScan 4</strong>​ 的单文件扫描工具，主要用来做<strong>端口扫描、主机存活检测</strong>，网管、搞安全的、测试网络连通性的人常用它快速扫一批 IP，看哪些机器开着、哪些端口开着。</p><p>它是绿色单文件，所谓的“安装”其实就是准备好环境、直接运行，下面用大白话一步步说。</p><h2>一、准备工作</h2><ol><li><p><strong>下载 SuperScan4.exe</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=j7uO3HFDrfi%2FRSVybS3ATg%3D%3D.9noLlXI4FEpknIY3WE7Bv2ac7LBkt7GYSBstAcogJ8H4tKLY%2Fq5OQU1i9irKfYwY" rel="nofollow" title="https://pan.quark.cn/s/56173e5006cb" target="_blank">https://pan.quark.cn/s/56173e5006cb</a></p></li><li><p><strong>确认系统版本</strong>​</p><ul><li>支持 Win7/Win10/Win11 等常见 Windows 系统，老版本在 Win10/Win11 可能需要右键“以管理员身份运行”才能正常扫。</li></ul></li></ol><h2>二、“安装”步骤（其实就是运行准备）</h2><p>SuperScan4 是绿色单文件，<strong>不用像普通软件那样一步步装</strong>，只要保证能打开并正常使用：</p><ol><li>把下载好的 <code>SuperScan4.exe</code>放到一个固定文件夹，比如 <code>D:\Tools\SuperScan</code>，别放桌面容易误删或丢失。</li><li>右键 <code>SuperScan4.exe</code>→ 选“以管理员身份运行”（有些系统不提权会出现权限错误或扫不到结果）。</li><li>如果是 Win10/Win11，会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>第一次打开可能界面比较简单，直接就能用，不需要额外装插件。</li></ol><h2>三、基本使用方法（简单说两句）</h2><ol><li>打开 SuperScan4.exe → 在 “IP 范围” 里填要扫的地址段，比如 <code>192.168.1.1-192.168.1.254</code>。</li><li><p>选端口范围：</p><ul><li>默认会扫一些常用端口，也可以自己填，比如 <code>1-1000</code>或单独 <code>80,443,3389</code>。</li></ul></li><li>点  <strong>“Start”</strong> （开始）按钮 → 等扫描结果出来，会列出存活的主机和开放端口。</li><li>可以导出结果或复制到记事本保存。</li><li>扫的时候别一次性扫太大范围，尤其是外网，容易被认为攻击，还可能被防火墙拦截。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[汽车行业如何选研发管理平台？看看行业标杆客户怎么说 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047606365</link>    <guid>https://segmentfault.com/a/1190000047606365</guid>    <pubDate>2026-02-11 19:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在汽车行业，研发管理平台的选型面临独特挑战，尤其是在“智能座舱、自动驾驶、车载电子”快速发展的背景下。汽车研发不仅涉及复杂的软硬件协同，还需要应对严格的合规要求、长周期项目管理、跨部门协同等难题。选择合适的研发管理平台，不仅关乎项目管理的高效性，还决定了产品的创新能力与市场响应速度。</p><h2>一、汽车行业的研发管理痛点</h2><p><strong>1）跨部门协同难：软硬件、算法与系统工程的协作壁垒</strong></p><p>汽车行业的研发项目通常涉及多个部门，包括硬件与软件开发、算法开发与嵌入式系统、汽车电子与机械工程等。研发过程中也经常出现协作壁垒：</p><ul><li>不同部门使用不同的工具，信息割裂</li><li>项目进度、资源和风险很难实时同步</li><li>没有统一的协作平台，沟通对接耗时，误差频发</li></ul><p><strong>2）研发流程冗长，难以管控项目进度与交付质量</strong></p><p>汽车项目周期长、涉及面广，管理难度大，极易出现：</p><ul><li>项目阶段多、环节复杂，进度不易把控</li><li>各环节之间信息不流通，导致反复修改与返工</li><li>团队间依赖性强，缺乏透明的资源与任务管理体系</li></ul><p><strong>3）高合规要求：标准化与流程化必须强制执行</strong></p><p>汽车行业对质量与合规要求高，选型时特别看重以下几点：</p><ul><li>是否能满足汽车行业标准（如 ASPICE、ISO 26262 等）</li><li>是否支持流程自动化与标准化</li><li>是否支持全生命周期管理，从概念设计到产品交付</li></ul><p><strong>4）技术创新的需求：研发测试一体化、缺陷追溯与数据驱动决策</strong></p><p>随着自动驾驶、智能座舱等技术的发展，研发过程需要快速迭代和持续创新：</p><ol><li>迭代周期短，需求变更频繁</li><li>测试与研发不能“割裂”，需要紧密对接</li><li>项目进展与质量评估要依赖数据驱动，避免“盲目决策”</li></ol><h2>二、ONES 在汽车行业的解题思路</h2><p><strong>1）支持跨部门、跨团队的协同与资源透明化</strong></p><p>汽车研发涉及的跨部门、跨团队协作多，平台要做到：</p><ul><li>跨部门数据和任务的统一管理与协作</li><li>项目任务、工时、进度实时可视化</li><li>支持多团队的灵活配置，避免信息割裂与重复劳动</li></ul><p><strong>2）流程与标准化：支持 ASPICE 和 ISO 26262 等标准的执行与追溯</strong></p><p>汽车行业的研发管理平台，必须支撑行业标准：</p><ul><li>ASPICE、ISO 26262等流程标准的数字化落地</li><li>通过平台实现流程自动化与标准化管理</li><li>支持从设计、研发到交付的全生命周期管控，确保质量与合规</li></ul><p><strong>3）支撑研发与测试一体化：缺陷回溯与测试用例关联</strong></p><p>平台需要支持：</p><ul><li>研发任务与测试用例的无缝对接</li><li>缺陷管理与需求、任务、测试的闭环回溯</li><li>数据驱动决策，减少人工依赖和项目误差</li></ul><p><strong>4）技术创新与数据化管理：从需求到交付的全程追溯与可视化</strong></p><p>随着技术快速发展，平台应支撑：</p><ul><li>需求变更的高效管理与追溯</li><li>项目全生命周期的实时跟踪与透明化</li><li>数据分析和决策支持功能，帮助团队在快速变化的技术环境中保持竞争力</li></ul><h2>三、汽车行业客户证言</h2><p>以下内容来自 ONES 汽车行业客户证言。若你正在评估研发管理平台，可将这些证言作为参考样本，对照自身的跨部门协作、流程标准化、缺陷管理与数据决策需求进行判断。</p><p><strong>四维智联：中国智能汽车产业链的核心技术供应商</strong></p><p>ONES 系统助力我们完成项目、需求、缺陷以及质量管理的标准化、线上化管理，规范了从需求、研发到交付的全流程，提升了团队间的协作效率，协助建立了统一指标管理体系。</p><p><strong>新阳荣乐：服务一汽、东风、长安、北汽制造、北汽新能源等龙头厂</strong></p><p>ONES 助力新阳荣乐落地 ASPICE 认证的项目管理过程，提供对应的项目管理指导、优化内部业务流程管理，构建项目与业务一体化平台。</p><p><strong>易捷特：由东风汽车、雷诺、日产合资成立的新能源汽车企业</strong></p><p>易捷特通过引入 ONES 研发项目管理系统，实现了跨部门、跨地域的工单统一管理与流程自动化，显著提升协同效率和项目管理专业性，支撑高质量交付与客户满意度提升。</p><p><strong>众鸿科技：中国智能网联汽车创新 TOP 50</strong></p><p>技术总监 林先生：与 ONES 合作后，我们的智能座舱、舱泊一体系统研发流程实现标准化管控，完美适配 ASPICE 体系与功能安全要求，跨研发中心协作效率提升 40%；其全流程管理能力助力我们加速国产芯片适配与技术创新，产品迭代周期缩短 30%，为深耕汽车电子赛道、实现平台化量产提供了坚实支撑。</p><p><strong>佳因特：超 15 万台充电桩销往全球 60 个国家</strong></p><p>ONES 支撑佳因特全产品线研发项目管理，确保项目资源合理分配，团队高效协同。</p>]]></description></item><item>    <title><![CDATA[AI产品需求分析入门 AIAgent研究 ]]></title>    <link>https://segmentfault.com/a/1190000047606376</link>    <guid>https://segmentfault.com/a/1190000047606376</guid>    <pubDate>2026-02-11 19:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在AI技术快速渗透各行各业的今天，AI产品早已走出实验室，成为解决实际问题、提升效率的核心载体——从日常使用的AI聊天机器人、图片生成工具，到企业级的智能风控、数据分析系统，背后都离不开专业的需求分析。不同于传统互联网产品，AI产品需求分析既要兼顾“用户价值”，更要适配“技术可行性”，是连接用户需求、业务目标与AI技术的核心桥梁。对于新手而言，无需一开始陷入复杂的算法细节，先掌握核心逻辑与基础步骤，就能快速入门AI产品需求分析。</p><h2>一、先搞懂：什么是AI产品需求分析？</h2><p>简单来说，AI产品需求分析的核心是：<strong>明确“用AI技术解决什么问题”“解决给谁看”“怎么用技术落地”“落地后如何衡量效果”</strong> ，最终将模糊的用户痛点、业务诉求，转化为可落地、可衡量、符合AI技术特性的产品需求。</p><p>与传统互联网产品需求分析相比，它有两个核心差异，也是新手最需要注意的点：</p><ul><li>传统产品侧重“功能实现”，比如“做一个一键付款功能”，技术路径清晰；AI产品侧重“效果达成”，比如“做一个能识别垃圾类别的AI工具”，核心是让AI的识别准确率、响应速度达到可用标准，技术迭代空间更大。</li><li>传统产品需求可“一步到位”，功能上线后基本满足需求；AI产品需求是“迭代演进”的，比如AI聊天机器人的话术流畅度、理解准确率，需要通过数据反馈、模型优化，逐步逼近理想效果，无法一蹴而就。</li></ul><p>一句话总结：AI产品需求分析，是“以问题为核心，以技术为支撑，以效果为目标”的系统性思考过程，而非单纯罗列功能。</p><h2>二、入门核心：AI产品需求分析的3个基本原则</h2><p>新手入门，无需追求复杂方法，先守住3个基本原则，就能避开80%的坑，确保需求不偏离方向。</p><h3>1. 可行性优先：拒绝“技术空想”</h3><p>AI产品的核心是“技术落地”，再美好的需求，若当前技术无法实现，就是无效需求。新手最容易犯的错误，就是过度追求“炫酷功能”，比如“做一个能完全替代人类的AI客服”，忽略了当前AI的理解能力、情感表达能力的局限性。</p><p>正确的做法是：先判断需求的技术可行性——比如，当前AI能否实现核心功能（如识别、生成、预测）？需要多少数据支撑？落地成本（人力、算力）是否可控？若暂时无法完全实现，可拆解为“最小可行需求”，比如先实现“AI识别常见垃圾类别（准确率≥80%）”，再逐步扩展类别、提升准确率。</p><h3>2. 价值导向：AI是“工具”，不是“噱头”</h3><p>所有AI产品的需求，都必须围绕“解决实际问题、创造价值”展开，要么提升效率，要么降低成本，要么优化体验，拒绝为了“AI”而“AI”。</p><p>比如，同样是“AI图片生成工具”，针对普通用户的需求是“简单输入文字，就能生成好看的图片，无需专业设计能力”（优化体验）；针对电商商家的需求是“快速生成商品主图，降低设计成本”（降低成本）。明确核心价值，才能让需求更聚焦，避免功能冗余。</p><h3>3. 数据驱动：AI的“燃料”的是数据</h3><p>AI模型的训练、优化，离不开大量高质量数据——比如AI识别垃圾，需要收集成千上万张不同垃圾的图片，标注清楚类别，才能训练出可用的模型。因此，在需求分析阶段，就要考虑“数据来源”：数据从哪里来？是否合规？数据质量是否达标？</p><p>比如，做一个“AI识别手写文字”的需求，若无法获取足够多、覆盖不同字体、不同书写场景的手写文字数据，即使技术路径可行，最终的识别效果也会很差，需求落地后也无法满足用户需求。</p><h2>三、新手实操：AI产品需求分析的5个基础步骤</h2><p>掌握原则后，跟着这5个步骤走，就能快速完成一次基础的AI产品需求分析，从“空想”走向“落地”。</p><h3>步骤1：明确场景与用户，找准核心痛点</h3><p>任何产品需求的起点，都是“谁在什么场景下，遇到了什么问题”，AI产品也不例外。新手要避免“泛泛而谈”，比如不要说“做一个AI工具”，而要具体到场景和用户。</p><p>举例：用户是“中小电商商家”，场景是“每天需要生成10张商品主图，自己不会设计，找设计师成本高、周期长”，核心痛点是“商品主图生成效率低、成本高”。</p><p>这一步的关键是：聚焦“具体场景、具体用户”，拒绝模糊化描述，只有找准痛点，后续的AI需求才能有的放矢。</p><h3>步骤2：拆解需求，明确AI的核心作用</h3><p>找到痛点后，不要直接想“用AI怎么做”，而是先拆解需求，区分“哪些部分需要AI实现，哪些部分用传统功能实现即可”——AI只负责解决“传统技术无法高效解决”的问题，比如识别、生成、预测等，无需所有功能都依赖AI。</p><p>继续上面的例子，需求拆解为：① 生成商品主图；② 支持自定义商品类别、背景风格；③ 生成后可简单编辑；④ 快速导出。其中，“生成商品主图”是核心，需要AI实现（文生图、图生图）；“自定义风格、简单编辑、导出”是辅助功能，用传统产品功能即可实现。</p><p>这一步的关键是：聚焦“AI的核心价值”，不要过度依赖AI，避免增加技术复杂度和落地成本。</p><h3>步骤3：明确效果指标，让需求可衡量</h3><p>AI产品的需求，必须有“可衡量的效果指标”，否则无法判断需求是否落地、是否满足用户需求。新手最容易忽略这一点，只说“做一个AI识别工具”，却不说“识别准确率要达到多少”“响应速度要多久”。</p><p>常见的AI效果指标有：准确率（比如垃圾识别准确率≥80%）、响应速度（比如AI生成图片≤10秒/张）、召回率（比如智能推荐的召回率≥70%）、用户满意度（比如AI客服的用户满意度≥85%）。</p><p>继续举例，明确效果指标：AI生成商品主图，准确率≥85%（与商品实际外观匹配），响应速度≤8秒/张，支持3种以上背景风格，用户可直接使用的图片占比≥70%。</p><p>这一步的关键是：指标要具体、可量化，避免“大概”“差不多”，这样后续技术开发、测试才有明确的标准。</p><h3>步骤4：评估技术可行性与落地成本</h3><p>这是AI产品需求分析的核心步骤，也是区别于传统产品的关键。新手可以从3个维度评估，无需深入了解算法细节，只需和技术同学简单沟通即可：</p><ul><li>技术路径：当前AI技术能否实现核心需求？比如，商品主图生成，可用成熟的文生图模型（如Stable Diffusion、即梦AI的生成模型），技术路径可行。</li><li>数据支撑：是否有足够的高质量数据？比如，商品主图生成，需要收集不同类别的商品图片、背景图片，标注清楚类别、风格，若数据不足，可考虑使用公开数据集、外包标注。</li><li>落地成本：人力（算法工程师、数据标注师）、算力（模型训练、推理需要的服务器资源）、时间（开发周期）是否可控？比如，中小团队做商品主图生成工具，可基于成熟模型微调，降低开发成本和周期。</li></ul><p>若评估后发现不可行，可调整需求，比如降低效果指标、拆解为更小的需求，避免盲目推进。</p><h3>步骤5：输出需求文档，明确边界与迭代计划</h3><p>需求分析完成后，需要将思考的结果整理为需求文档（PRD），传递给技术、测试等团队，明确需求的边界、优先级和迭代计划。新手的需求文档无需过于复杂，核心包含3部分内容：</p><ul><li>需求概述：明确场景、用户、核心痛点和需求目标，让团队快速了解需求背景。</li><li>核心需求与效果指标：详细说明AI核心功能、效果指标、辅助功能，明确需求的优先级（哪些必须实现，哪些可后续迭代）。</li><li>迭代计划：AI产品无法一步到位，需明确迭代节奏，比如V1版本实现核心功能（准确率≥85%），V2版本提升准确率（≥90%）、增加更多风格，V3版本优化编辑功能。</li></ul><p>这一步的关键是：文档清晰、简洁，明确“做什么、不做什么、做到什么程度、分几步做”，避免团队理解偏差。</p><h2>四、新手避坑：AI产品需求分析的4个常见误区</h2><p>入门阶段，只要避开这4个误区，就能少走很多弯路，让需求更具落地性。</p><ul><li>误区1：过度追求技术炫酷，忽视用户需求。比如，盲目追求“多模态生成”“大模型应用”，却没考虑用户是否真的需要，导致产品上线后无人使用。记住：AI是工具，用户需要的是“解决问题”，不是“炫酷技术”。</li><li>误区2：忽视数据问题，认为“技术能解决一切”。比如，做AI识别需求，却没考虑数据来源、数据质量，导致模型训练效果差，无法落地。记住：数据是AI的燃料，没有高质量数据，再强的算法也无用。</li><li>误区3：需求太模糊，没有可衡量的指标。比如，只说“做一个AI客服，能回答用户问题”，却不说“回答准确率、响应速度”，导致技术开发没有标准，测试无法判断效果。</li><li>误区4：期望一步到位，不考虑迭代。比如，要求AI产品上线就达到“完美效果”，忽视了AI模型需要数据反馈、持续优化的特性，导致需求落地周期过长，甚至失败。</li></ul><h2>五、入门建议：新手如何快速提升AI产品需求分析能力？</h2><p>AI产品需求分析能力，不是一蹴而就的，新手可以从3个方面入手，快速提升，循序渐进。</p><ul><li>多体验：多使用各类AI产品（如即梦AI、ChatGPT、Midjourney、剪映AI），思考它们的需求场景、核心功能、效果指标，拆解它们的需求逻辑——比如，使用即梦AI的视频生成功能，思考“它的用户是谁？核心痛点是什么？效果指标如何设计？”。</li><li>多实践：从小需求入手，尝试完成一次完整的需求分析，比如“做一个AI识别宠物类别的工具”，按照前面的5个步骤，拆解需求、明确指标、评估可行性、输出需求文档，哪怕是简单的练习，也能快速积累经验。</li><li>多沟通：多和技术同学沟通，了解AI技术的基本逻辑、落地难点（比如数据标注、模型微调的成本），避免提出不可行的需求；多和用户沟通，了解真实痛点，避免“自嗨式需求”。</li></ul><h2>六、总结</h2><p>AI产品需求分析入门，核心不是掌握复杂的算法知识，而是建立“以问题为核心、以技术为支撑、以效果为目标”的思考方式——先找准具体场景和用户痛点，再拆解需求、明确效果指标，评估技术可行性，最后通过迭代逐步落地。</p><p>对于新手而言，不要急于求成，先守住“可行性、价值导向、数据驱动”3个原则，避开常见误区，多体验、多实践、多沟通，就能快速掌握AI产品需求分析的基础逻辑，逐步成长为合格的AI产品需求分析师。</p><p>记住：AI产品的核心是“解决问题”，需求分析的核心是“让技术落地，创造价值”，这也是所有AI产品需求分析的底层逻辑。</p>]]></description></item><item>    <title><![CDATA[Flask 入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047606382</link>    <guid>https://segmentfault.com/a/1190000047606382</guid>    <pubDate>2026-02-11 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 库的概览与核心价值</h2><p>想象一下,在搭建一个 Web 应用时,如果需要同时处理路由、模板、数据库、表单验证、用户认证等数十个复杂功能,就像试图在一天内盖好一栋摩天大楼——不仅容易迷失方向,还可能因为过度设计而拖垮开发效率。<code>Flask</code>正是为解决这个"选择困难症"而生的轻量级框架。</p><p>Flask被称为"微框架"(Microframework),它的核心哲学是"保持简单,按需扩展"。与Django这样自带全套装备的"全栈框架"不同,Flask只提供Web开发最基础的功能:路由分发和模板渲染,其他功能则通过丰富的扩展生态系统来实现。这种设计让开发者能够根据项目需求自主选择工具链,就像搭积木一样灵活组装自己的技术栈。</p><p>Flask的不可替代性体现在三个方面:极低的学习曲线让初学者能快速上手,高度的扩展性支持项目从原型到生产环境的平滑演进,而简洁的代码结构则为团队协作和代码维护提供了良好基础。无论是构建简单的API服务、个人博客,还是复杂的企业级应用,Flask都能提供一个优雅而高效的起点。</p><h2>2. 环境搭建与 "Hello, World"</h2><h3>安装说明</h3><p>安装Flask前,强烈建议先创建虚拟环境以隔离项目依赖:</p><pre><code class="bash"># 创建虚拟环境
python3 -m venv venv

# 激活虚拟环境
# macOS/Linux:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 安装Flask
pip install Flask</code></pre><p>Flask会自动安装以下核心依赖:</p><ul><li><code>Werkzeug</code>: WSGI工具包,处理HTTP请求和响应</li><li><code>Jinja2</code>: 模板引擎,用于生成动态HTML</li><li><code>Click</code>: 命令行工具,提供<code>flask</code>命令</li><li><code>MarkupSafe</code>: 自动转义HTML,防止XSS攻击</li><li><code>ItsDangerous</code>: 数据签名工具,保护session安全</li></ul><h3>最简示例</h3><p>创建一个<code>app.py</code>文件,写入以下代码:</p><pre><code class="python">from flask import Flask

# 创建Flask应用实例
app = Flask(__name__)

# 使用装饰器定义路由
@app.route('/')
def hello_world():
    return '&lt;p&gt;Hello, World!&lt;/p&gt;'

if __name__ == '__main__':
    app.run(debug=True)</code></pre><h3>逐行解释</h3><ul><li><code>from flask import Flask</code>: 导入Flask核心类,这是构建应用的起点</li><li><code>app = Flask(__name__)</code>: 创建应用实例。<code>__name__</code>参数帮助Flask定位模板和静态文件目录</li><li><code>@app.route('/')</code>: 路由装饰器,告诉Flask当用户访问根路径(<code>/</code>)时调用下面的函数</li><li><code>def hello_world():</code>: 视图函数,处理请求并返回响应内容</li><li><code>return '&lt;p&gt;Hello, World!&lt;/p&gt;'</code>: 返回HTML字符串,Flask会自动将其转换为HTTP响应</li><li><code>if __name__ == '__main__':</code>: 确保只有在直接运行脚本时才启动服务器</li><li><code>app.run(debug=True)</code>: 启动开发服务器。<code>debug=True</code>开启调试模式,代码修改后自动重载,并提供错误调试页面</li></ul><h3>运行结果</h3><p>在终端执行:</p><pre><code class="bash">flask --app app run
# 或者
python app.py</code></pre><p>服务器启动后,访问 <a href="https://link.segmentfault.com/?enc=hk7%2BUgtW7uvLend9USZFwQ%3D%3D.W7rltI4Hpti6ClGPaPcVUygAlfqNhM7bvP6dDHiHQqI%3D" rel="nofollow" target="_blank">http://127.0.0.1:5000/</a> 即可看到 "Hello, World!" 页面。</p><h2>3. 核心概念解析</h2><p>Flask的三大核心概念:应用实例、路由系统和请求上下文,它们共同构成了Web应用的骨架。</p><h3>应用实例(Application Instance)</h3><p>应用实例(<code>app = Flask(__name__)</code>)是Flask应用的中心,负责管理路由、配置和扩展。它通过<code>__name__</code>参数确定模块位置,以便正确查找<code>templates</code>和<code>static</code>目录。可以将应用实例理解为一个"中央指挥官",协调所有组件协同工作。</p><h3>路由系统(Routing)</h3><p>路由使用装饰器<code>@app.route()</code>将URL路径映射到视图函数:</p><pre><code class="python"># 基础路由
@app.route('/about')
def about():
    return 'About Page'

# 动态路由
@app.route('/user/&lt;username&gt;')
def show_user(username):
    return f'User: {username}'

# 类型约束路由
@app.route('/post/&lt;int:post_id&gt;')
def show_post(post_id):
    return f'Post ID: {post_id}'

# 多HTTP方法支持
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        return 'Processing login...'
    return 'Login form'</code></pre><p>动态路由中的<code>&lt;username&gt;</code>和<code>&lt;int:post_id&gt;</code>是URL转换器,前者匹配任意字符串,后者只匹配整数。Flask还支持<code>float</code>、<code>path</code>(包含斜杠)、<code>uuid</code>等转换器。</p><h3>请求上下文(Request Context)</h3><p>请求上下文包含两个关键代理对象:<code>request</code>和<code>session</code>。它们允许在视图函数中访问请求数据和会话信息,无需显式传递参数。</p><pre><code class="python">from flask import request, session

# 获取查询参数: /search?q=keyword
@app.route('/search')
def search():
    keyword = request.args.get('q', '')
    return f'Searching for: {keyword}'

# 获取表单数据
@app.route('/submit', methods=['POST'])
def submit():
    username = request.form.get('username')
    return f'Username: {username}'

# 获取JSON数据
@app.route('/api/data', methods=['POST'])
def api_data():
    data = request.get_json()
    return jsonify(data)

# 使用session存储用户状态
@app.route('/set_session')
def set_session():
    session['user_id'] = 123
    return 'Session set'</code></pre><h3>概念关系图</h3><pre style="display:none;"><code class="mermaid">graph TD
    A[Flask应用实例] --&gt; B[路由系统]
    A --&gt; C[配置管理]
    A --&gt; D[扩展注册]
    B --&gt; E[视图函数]
    E --&gt; F[请求上下文]
    F --&gt; G[request对象]
    F --&gt; H[session对象]
    E --&gt; I[响应生成]
    I --&gt; J[字符串/JSON/模板]
    D --&gt; K[数据库扩展]
    D --&gt; L[表单验证扩展]
    D --&gt; M[认证扩展]</code></pre><h2>4. 实战演练:构建一个待办事项API</h2><p>让我们通过一个完整的迷你项目来掌握Flask的核心功能。我们将构建一个简单的待办事项管理API,支持增删改查(CRUD)操作。</p><h3>需求分析</h3><p>我们需要创建一个RESTful API,允许用户:</p><ol><li>获取所有待办事项</li><li>创建新待办事项</li><li>更新待办事项状态</li><li>删除待办事项</li></ol><p>数据存储在内存中(列表),适合快速原型开发。</p><h3>方案设计</h3><p>选择Flask的以下功能:</p><ul><li>路由系统:定义API端点</li><li><code>request</code>对象:解析JSON请求体</li><li><code>jsonify</code>:返回JSON格式响应</li><li>动态路由:处理特定ID的待办事项</li><li>HTTP方法:GET/POST/PUT/DELETE对应CRUD操作</li></ul><h3>代码实现</h3><p>创建<code>todo_api.py</code>:</p><pre><code class="python">from flask import Flask, request, jsonify

app = Flask(__name__)

# 内存数据库
todos = [
    {'id': 1, 'title': 'Learn Flask', 'completed': False},
    {'id': 2, 'title': 'Build API', 'completed': False}
]
next_id = 3

# 获取所有待办事项
@app.route('/api/todos', methods=['GET'])
def get_todos():
    return jsonify(todos)

# 创建新待办事项
@app.route('/api/todos', methods=['POST'])
def create_todo():
    global next_id
    data = request.get_json()
    
    if not data or 'title' not in data:
        return jsonify({'error': 'Title is required'}), 400
    
    todo = {
        'id': next_id,
        'title': data['title'],
        'completed': data.get('completed', False)
    }
    todos.append(todo)
    next_id += 1
    
    return jsonify(todo), 201

# 更新待办事项
@app.route('/api/todos/&lt;int:todo_id&gt;', methods=['PUT'])
def update_todo(todo_id):
    todo = next((t for t in todos if t['id'] == todo_id), None)
    
    if not todo:
        return jsonify({'error': 'Todo not found'}), 404
    
    data = request.get_json()
    todo['title'] = data.get('title', todo['title'])
    todo['completed'] = data.get('completed', todo['completed'])
    
    return jsonify(todo)

# 删除待办事项
@app.route('/api/todos/&lt;int:todo_id&gt;', methods=['DELETE'])
def delete_todo(todo_id):
    global todos
    todo = next((t for t in todos if t['id'] == todo_id), None)
    
    if not todo:
        return jsonify({'error': 'Todo not found'}), 404
    
    todos = [t for t in todos if t['id'] != todo_id]
    return jsonify({'message': 'Todo deleted'})

if __name__ == '__main__':
    app.run(debug=True)</code></pre><h3>运行说明</h3><ol><li><p>启动服务器:</p><pre><code class="bash">python todo_api.py</code></pre></li><li>使用curl或Postman测试API:</li></ol><pre><code class="bash"># 获取所有待办事项
curl http://127.0.0.1:5000/api/todos

# 创建新待办事项
curl -X POST http://127.0.0.1:5000/api/todos \
  -H "Content-Type: application/json" \
  -d '{"title": "Deploy to production"}'

# 更新待办事项
curl -X PUT http://127.0.0.1:5000/api/todos/1 \
  -H "Content-Type: application/json" \
  -d '{"completed": true}'

# 删除待办事项
curl -X DELETE http://127.0.0.1:5000/api/todos/1</code></pre><h3>结果展示</h3><p>这个API完美展示了Flask的核心能力:</p><ul><li>清晰的路由定义(<code>/api/todos</code>, <code>/api/todos/&lt;id&gt;</code>)</li><li>HTTP方法处理(GET/POST/PUT/DELETE)</li><li>JSON请求解析(<code>request.get_json()</code>)</li><li>错误处理和状态码返回(400/404)</li><li>动态路由参数(<code>&lt;int:todo_id&gt;</code>)</li></ul><h2>5. 最佳实践与常见陷阱</h2><h3>常见错误及规避方法</h3><h4>错误1: 直接使用<code>app.run()</code>部署到生产环境</h4><pre><code class="python"># ❌ 错误做法
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)  # 仅适合开发环境</code></pre><p>Flask内置服务器性能有限且不安全,生产环境应使用Gunicorn或uWSGI:</p><pre><code class="bash"># ✅ 正确做法: 使用Gunicorn部署
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app</code></pre><h4>错误2: 硬编码敏感信息</h4><pre><code class="python"># ❌ 错误做法
app.config['SECRET_KEY'] = 'my-secret-key-123'
app.config['DATABASE_URI'] = 'postgresql://user:password@localhost/db'</code></pre><p>使用环境变量或配置文件:</p><pre><code class="python"># ✅ 正确做法
import os

app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY') or 'dev-key'
app.config['DATABASE_URI'] = os.environ.get('DATABASE_URI')

# 或者使用配置文件
# config.py
class Config:
    SECRET_KEY = os.environ.get('SECRET_KEY')
    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URI')

# app.py
from config import Config
app.config.from_object(Config)</code></pre><h4>错误3: 忘记设置<code>SECRET_KEY</code>导致session无法使用</h4><pre><code class="python"># ❌ 错误做法
@app.route('/login')
def login():
    session['user_id'] = 1  # 会报错: RuntimeError: The session is unavailable
    return 'Logged in'</code></pre><pre><code class="python"># ✅ 正确做法
app = Flask(__name__)
app.secret_key = 'your-secret-key-here'  # 生产环境应从环境变量读取

@app.route('/login')
def login():
    session['user_id'] = 1
    return 'Logged in'</code></pre><h3>最佳实践建议</h3><p><strong>1. 使用虚拟环境隔离依赖</strong></p><pre><code class="bash"># 创建并激活虚拟环境
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt</code></pre><p><strong>2. 生成依赖清单</strong></p><pre><code class="bash">pip freeze &gt; requirements.txt</code></pre><p><code>requirements.txt</code>文件示例:</p><pre><code>Flask==3.0.0
Werkzeug==3.0.1
Jinja2==3.1.2</code></pre><p><strong>3. 项目结构组织</strong></p><p>对于小型项目,建议采用以下结构:</p><pre><code>myproject/
├── app.py              # 主应用文件
├── requirements.txt     # 依赖清单
├── config.py           # 配置文件
├── templates/          # 模板目录
│   └── index.html
└── static/             # 静态文件
    ├── css/
    └── js/</code></pre><p>对于大型项目,使用蓝图(Blueprint)模块化:</p><pre><code>myproject/
├── app.py
├── requirements.txt
├── blueprints/
│   ├── auth.py
│   ├── api.py
│   └── main.py
└── templates/</code></pre><p><strong>4. 启用调试模式注意事项</strong></p><p>开发环境可启用调试模式:</p><pre><code class="python">app.run(debug=True)</code></pre><p>但生产环境必须关闭:</p><pre><code class="python">app.run(debug=False)  # 或不指定,默认为False</code></pre><p>调试模式会暴露敏感信息并允许在浏览器中执行任意Python代码,存在严重安全风险。</p><h2>6. 进阶指引</h2><p>Flask的简洁性不仅体现在核心功能上,更体现在其强大的扩展能力。当你的项目需要更复杂的功能时,以下扩展值得关注:</p><p><strong>数据库集成</strong></p><ul><li><code>Flask-SQLAlchemy</code>: 提供ORM功能,简化数据库操作</li><li><code>Flask-Migrate</code>: 数据库迁移工具,管理表结构变更</li></ul><p><strong>表单处理与验证</strong></p><ul><li><code>Flask-WTF</code>: 集成WTForms,提供表单验证和CSRF保护</li></ul><p><strong>用户认证与授权</strong></p><ul><li><code>Flask-Login</code>: 管理用户会话和认证状态</li><li><code>Flask-Security</code>: 提供完整的认证、角色管理和密码加密</li></ul><p><strong>API开发</strong></p><ul><li><code>Flask-RESTful</code>: 快速构建RESTful API</li><li><code>Flask-Marshmallow</code>: 序列化/反序列化数据</li></ul><p><strong>任务队列与异步处理</strong></p><ul><li><code>Celery</code>: 处理耗时任务(如发送邮件、图片处理)</li><li><code>Flask-Celery-Helper</code>: 简化Celery与Flask的集成</li></ul><h3>学习路径建议</h3><ol><li><strong>掌握基础</strong>(当前阶段): 理解路由、请求/响应、模板渲染</li><li><strong>扩展技能</strong>: 学习3-5个常用扩展,构建功能完整的应用</li><li><strong>深入原理</strong>: 研究Flask的上下文机制、信号系统、中间件</li><li><strong>生产部署</strong>: 掌握Gunicorn/Nginx部署、Docker容器化</li><li><strong>性能优化</strong>: 了解缓存策略、数据库优化、异步处理</li></ol><h3>学习资源</h3><ul><li><strong>官方文档</strong>: <a href="https://link.segmentfault.com/?enc=DaN0cAGBi3abTuNq%2FsFX7A%3D%3D.9aViwMiXJ9BQ4cs5Ru5Zy8NvzlYVLEhRFlr%2BAk9FZefv7thgQNK6Lb6SHzbc%2FEFN" rel="nofollow" target="_blank">https://flask.palletsprojects.com/</a> (最权威的学习资源)</li><li><strong>中文文档</strong>: <a href="https://link.segmentfault.com/?enc=Wxaifp%2FmdVrbtP8xEUoAPA%3D%3D.kQCDZzGvbErCXyAYwAdXMuQgJ9F7YDEr%2BVWxxNPtIsw%3D" rel="nofollow" target="_blank">https://flask.github.net.cn/</a> (适合中文读者)</li><li><strong>GitHub仓库</strong>: <a href="https://link.segmentfault.com/?enc=ZwBpTWlZbWaIQtsF68Msbw%3D%3D.fmTH4EzoYmKNZy66tYmJuuE3EPFvuN6V%2B2%2B2V3Ub6QOA%2FAsE6MmfkVpQDvZ9lzU9" rel="nofollow" target="_blank">https://github.com/pallets/flask</a> (源码和Issue讨论)</li><li><strong>Stack Overflow</strong>: 使用<code>flask</code>标签搜索问题和解决方案</li></ul><p>Flask的学习曲线平缓,但要精通它需要实践和耐心。建议从简单项目开始,逐步引入新功能和技术,在实践中深化理解。记住,Flask的力量不在于它提供了什么,而在于它不限制你做什么——这正是"微框架"哲学的精髓所在。</p>]]></description></item><item>    <title><![CDATA[AI推理：如何实现吞吐翻倍、时延降90%与GPU资源节省26%？ 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606039</link>    <guid>https://segmentfault.com/a/1190000047606039</guid>    <pubDate>2026-02-11 18:09:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：马国忠</p><h2>引言：AI规模化落地，推理系统面临全新挑战</h2><p>﻿</p><p>全球领先的市场研究和咨询公司IDC预测，到2028年，75%的新 AI 工作负载将实现容器化，从而显著提升模型与工作负载更新的速度、一致性与安全性。容器化技术将成为 AI 推理时代的“默认交付形态”。当前随着大模型技术快速演进与业务场景的深度融合，<strong>AI业务对推理基础设施的需求呈现爆发式增长</strong>。在早期小流量场景下，手动部署与定制化方案尚可应对；然而当模型规模、并发请求与业务复杂度攀升至新高度时，传统推理系统在以下四个主要方面逐渐暴露出瓶颈。</p><ol><li><strong>稳定性不足</strong>：</li></ol><p>◦<strong>单点故障风险</strong>：手动部署的静态架构缺乏多副本与故障自愈机制，单节点宕机易引发服务中断；</p><p>◦<strong>负载不均衡</strong>：缺乏智能流量调度，高并发时部分节点过载导致响应延迟，低负载时资源闲置；</p><p>◦<strong>故障恢复滞后</strong>：依赖人工排查与重启，恢复周期长，影响业务连续性。</p><p>2.<strong>资源利用率低下</strong>：</p><p>◦<strong>静态资源分配</strong>：固定规模的GPU集群无法适应流量波动，高峰时段资源争抢，低谷期GPU闲置率超40%；</p><p>◦<strong>缺乏弹性机制</strong>：无法根据请求队列长度、KV缓存利用率等指标动态扩缩容，导致周级别GPU卡时浪费超5000+。</p><p>3.<strong>推理性能瓶颈</strong>：</p><p>◦<strong>混合请求排队</strong>：长、短文请求混合处理时，短文首字时延（TTFT）因排队激增90%以上；</p><p>◦<strong>缓存复用率低</strong>：多副本场景下相同前缀请求随机调度，重复计算导致KV缓存命中率不足60%；</p><p>◦<strong>硬件拓扑未优化</strong>：跨交换机部署引发传输延迟，人工调整拓扑亲和性成本高且易出错。</p><p>4.<strong>定制成本高昂</strong>：</p><p>◦<strong>多引擎适配复杂</strong>：vLLM、SGLang等引擎需独立开发接入层，版本迭代与兼容性维护成本攀升；</p><p>◦<strong>运维依赖人工</strong>：从部署、监控到故障修复全流程手动操作，人力成本占比超30%，且易引入人为错误。</p><p>﻿</p><p>为此，京东云结合实际业务需求与技术趋势，全面拥抱云原生技术栈，积累了丰富的云原生与高性能推理经验。自主研发了新一代<strong>云原生AI推理框架。</strong> 推动推理系统完成了一次体系化升级，实现了从手动部署到全场景AutoScale，从资源浪费到GPU利用率最大化。</p><p>•<strong>流量高峰自动扩容、低谷自动缩容</strong>，GPU卡时节省高达26%；</p><p>•<strong>智能流量调度与KV缓存复用</strong>，最高提升吞吐124%，首次生成时延TTFT降低90%；</p><p>•<strong>具备多级高可用能力</strong>，支持流量隔离、故障自愈与深度可观测；</p><p>•<strong>模型量化、引擎调优、算子开发</strong>等多项优化，推理引擎单点性能呈现局部领先优势。</p><p>﻿</p><p>详细了解一套生产级分布式AI训练推理平台（JoyBuilder）的云原生改造全纪实。京东云<strong>云原生AI推理框架。</strong></p><p>﻿</p><h2>一、系统架构设计：面向生产级的高性能云原生推理平台</h2><h3>设计理念：</h3><p>我们遵循三大核心设计原则，确保系统长期迭代的灵活性：</p><p>1.<strong>解耦与组合</strong> 各模块尽量松耦合，优先复用开源成熟组件，同时避免被社区绑定，保留核心模块的可替换能力。</p><p>2.<strong>扩展性优先</strong> 支持以插件化方式集成智能调度算法（流量调度、扩缩容决策、Prefix Cache打分等）；容器编排能力可扩展，目前已支持跨机部署与基于角色的调度策略。</p><p>3.<strong>引擎无感接入</strong> 目前可同时支持vLLM、SGLang等主流推理引擎，最终实现任意推理引擎的低成本接入。</p><h3>系统架构：</h3><p>﻿</p><h3>模块详解：</h3><h4><strong>1. 智能流量调度网关</strong></h4><p>基于云原生Gateway API与Inference Extension框架，我们构建了支持多引擎、高可用、高扩展的智能推理网关，支持多层次调度策略：</p><p>﻿</p><table><thead><tr><th><strong>核心能力</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>长短文分桶推理 流量调度</strong></td><td>网关基于高效的<strong>长短文分桶算法</strong>，构建跨模型集群的分流调度，显著<strong>降低短文TTFT（首字生成时延）；</strong></td></tr><tr><td><strong>前缀缓存感知KV复用流量调度</strong></td><td>面向不同模型上下文特征，基于 <strong>HashTrie 算法</strong>构建集群内全局pod的近似前缀缓存画像，支持prefix cache的亲和调度，<strong>有效降低推理 TTFT(首字生成时延)；</strong></td></tr><tr><td><strong>多维负载均衡流量调度</strong></td><td>毫秒级实时采集KV Cache Utilization、Waiting Queue等server load指标，支持<strong>load aware 亲和调度；</strong></td></tr><tr><td><strong>交换机拓扑感知流量调度</strong></td><td>为减少PD group组内KV cache传输的耗时，构建网络拓扑感知，<strong>支持全局最优prefill + 局部最优decode的网络亲和调度</strong>；</td></tr><tr><td><strong>多引擎PD分离流量编排调度</strong></td><td>已支持<strong>vLLM</strong>(PD串行)、<strong>SGLang</strong>(PD异步并行) 异构引擎无差别流量调度</td></tr><tr><td><strong>多LoRA动态流量调度、模型切换的流量调度</strong></td><td>实现不同引擎多LoRA的动态装、卸载，集成LoRA-aware 的动态流量感知调度能力；</td></tr><tr><td><strong>精确的前缀感知Cache-aware流量调度</strong></td><td>实时订阅引擎侧KV Events Metrics，构建精确的前缀缓存画像，实现更高效的prefix cache亲和调度，进一步降低推理TTFT；</td></tr><tr><td><strong>基于时延预测的SLO-aware 流量优先级感知调度</strong></td><td>利用延迟预测来估算每个请求在每个可用节点上的首次生成时间（TTFT）和每个输出令牌时间（TPOT），实现基于时延预测的SLO-aware智能调度；</td></tr></tbody></table><h4><strong>2. 容器编排与资源调度</strong></h4><p>﻿</p><p>•<strong>部署灵活</strong>：PD分离部署，具有Group和Pool两种模式，实现弹性扩缩容与拓扑感知调度。</p><p>•<strong>高可用机制</strong>：多副本部署，避免单点故障。同时支持故障时自动摘流与容器自愈，保障服务持续可用，用户无感知。</p><p>﻿</p><table><thead><tr><th><strong>核心能力</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>容器编排</strong></td><td>根据推理引擎工作特点，基于容器之间的协作关系（Kimi多容器跨机推理、PD分离架构等），将各个推理引擎容器一定的组织方式部署成一组Pods，并联动服务发现、重启策略。</td></tr><tr><td><strong>GPU资源调度</strong></td><td>自动将各个新创建的Pod调度到具有足够GPU资源的机器节点。</td></tr><tr><td><strong>拓扑感知调度</strong></td><td>Kimi跨机推理， TP16部署的2台机器保证在同一个交换机下；PD分离部署，协作关系的P和D在同一个交换机下。</td></tr><tr><td><strong>优先级调度和抢占</strong></td><td>支持在线服务和离线任务的混合调度，高优的在线服务可以抢占低优任务的GPU资源。</td></tr></tbody></table><h4><strong>3. 系统稳定性与可观测</strong></h4><p>•集成流量镜像、全链路告警与主备值班协同机制。</p><p>•通过网关大盘、调度模块监控、模型性能面板等多层次观测体系，实现问题快速发现与定位。</p><p>﻿</p><p>﻿</p><h4>4. 引擎优化与性能突破</h4><p>针对MoE、多模态等模型特点，通过算子优化、引擎调优与量化等手段，在多项关键性能指标上实现行业领先。</p><p>﻿</p><h2>二、关键场景落地与收益量化</h2><h4>1. 长短文混合调度</h4><p><strong>问题</strong>：长、短文请求混合排队时，短文TTFT急剧上升，集群吞吐下降。 <strong>方案</strong>：通过长短文分桶与跨集群调度，实现长短文分离处理。</p><p><strong>收益</strong>（以Kimi-K2与DeepSeek-V3压测为例）：</p><p>•<strong>Kimi-K2</strong>：短文TTFT降低90.97%，吞吐提升124.46%；长文吞吐提升33.89%，集群整体吞吐提升67%。</p><p>•<strong>DeepSeek-V3</strong>：短文TTFT降低79.09%，吞吐提升36.7%；长文吞吐提升14.34%，集群整体吞吐提升21.82%。</p><p>﻿</p><h4>2. KV Cache全局感知的流量调度</h4><p><strong>问题</strong>：多副本场景下相同前缀请求被随机调度，导致每个实例都重复计算并缓存相同前缀。 <strong>方案</strong>：持续刻画更新集群级KV Cache缓存画像，实现前缀匹配的智能路由，KV Cache高效复用。</p><p><strong>收益</strong>：</p><p>•DeepSeek-V3场景下，集群吞吐提升29.9%，首Token时延TTFT降低28.7%；</p><p>•Kimi-K2场景下，KV Cache命中率整体提升20%\~30%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606041" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h4>3. 全场景自动弹性伸缩</h4><p><strong>问题</strong>：夜间或周末的流量低谷期GPU资源闲置严重。 <strong>方案</strong>：通过多种弹性部署模式并基于排队长度与KV使用率等多项指标，实现全场景自动扩缩容。</p><p><strong>收益</strong>：</p><p>•周级别节省GPU卡时5000+，资源利用率提升26%；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606042" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>4. 硬件拓扑亲和调度</h4><p><strong>问题</strong>：跨交换机部署导致性能下降；人工修正部署成本高，维护压力大。 <strong>方案</strong>：</p><p>•通过节点标签与亲和性规则，实现交换机级自动拓扑亲和调度；</p><p>•Router实现按组进行PD配对流量调度。</p><p><strong>收益</strong>：</p><p>•组容器间通信不跨交换机，数据高效传输，全程自动化，无需人工干预，保证服务SLA。<br/>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606043" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4><strong>5. 稳定性与业务连续性</strong></h4><p><strong>问题：</strong> 容器故障后，因分发机制导致持续的客户影响。故障恢复强依赖人工，导致故障时间长，修复难度大。</p><p><strong>方案：</strong> 通过实时健康监测，快速感知故障容器，进行隔离。启动新副本，实现故障自愈。</p><p><strong>收益：</strong></p><p>•实现自动隔离，自动自愈，无需人工干预，节点人力成本，提高用户体验。</p><p>﻿</p><p>﻿</p><h4><strong>6.推理引擎无感接入</strong></h4><p><strong>问题</strong>：多引擎支持成本高，定制化开发量大，维护成本高。 <strong>方案</strong>：构建统一推理引擎调度接入层，支持vLLM、SGLang等不同推理引擎一键接入。</p><p><strong>收益：</strong></p><p>•推理引擎无感快速接入。</p><p>•降低开发与维护成本。</p><p>﻿</p><p>﻿</p><h2>三、收益总结</h2><p>京东云云原生AI推理框架通过多维度调度与系统级优化，显著提升了推理效率与资源利用率。短文与长文吞吐均有大幅增长，首 token 延迟明显降低，并结合自动弹性扩缩容与 KV Cache 感知调度，进一步提升集群吞吐与缓存命中率，同时节省可观的 GPU 卡时成本。在此基础上，引入硬件拓扑亲和调度，实现更高效的自动化部署与调度，降低大规模集群运维压力；配合故障自愈、高可用机制与更精细的可观测体系，使系统运行更加稳定、可控、易排障。通过针对引擎瓶颈的持续优化，不同模型场景下的吞吐能力均得到明显增强。</p><table><thead><tr><th><strong>能力</strong></th><th><strong>量化结果与效益</strong></th></tr></thead><tbody><tr><td>长短文调度</td><td>吞吐：短文提升120%+，长文提升30%+ TTFT：短文降低90%</td></tr><tr><td>自动弹性扩缩容</td><td>GPU卡时：节省GPU卡时约26%</td></tr><tr><td>KV Cache感知调度</td><td>提升KV Cache命中率：增长约20%\~30% TTFT：降低29% 集群吞吐：增长30%</td></tr><tr><td>硬件拓扑亲和调度</td><td>实现自动化部署与调度，降低大规模集群运维成本</td></tr><tr><td>故障自愈与高可用</td><td>自动检测故障、自动恢复故障，减少对人工的依赖，更具可控性</td></tr><tr><td>可观测性</td><td>具备更细致的监控告警体系、提升故障发现和排查效率</td></tr><tr><td>引擎瓶颈优化</td><td>DS-MoE模型吞吐提升9%，多模态模型吞吐最高提升39%</td></tr></tbody></table><h2>四、客户案例</h2><h3>客户背景</h3><p>客户原系统面临AI规模化落地的挑战，在推理系统的稳定性、性能和资源利用率方面遇到了明显瓶颈。京东云通过帮助客户升级至云原生架构，成功改造了其推理系统，实现显著的性能提升和资源节约。见证了新系统如何带来切实的业务效益。</p><h3>解决方案</h3><p>京东云通过<strong>云原生AI推理框架</strong>对客户原78台节点进行逐步云原生改造，在不到一个月时间内从最初的2%切流比率提升到达到40%，实现对用户AI推理系统的云原生重构，助力企业实现高效、稳定、低成本的AI规模化落地。<strong>核心方案</strong>包括：采用<strong>智能流量调度</strong>技术，通过长短文分桶、KV缓存复用及拓扑感知调度；基于流量波动的<strong>弹性扩缩容</strong>机制；<strong>高可用架构</strong>通过多副本部署与故障自愈保障服务连续性；支持vLLM、SGLang等主流引擎的<strong>无感接入</strong>；<strong>硬件拓扑优化</strong>实现跨交换机亲和调度，减少传输延迟。<br/>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606044" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>客户收益</h3><p>﻿</p><p>•<strong>GPU吞吐能力</strong>：切换云原生系统后，GPU吞吐提升幅度达74%。这一增强使客户在高负载情况下依然能够维持高效的模型推理速度。</p><p>•<strong>限流数量</strong>：云原生AI推理框架系统将需要限流的请求显著减少82%，这意味着更多的客户请求在高峰时段得到及时响应，提高了用户体验和满意度。</p><table><thead><tr><th>﻿</th><th><strong>整体</strong></th><th><strong>旧版系统</strong></th><th><strong>云原生系统</strong></th><th><strong>收益</strong></th></tr></thead><tbody><tr><td><strong>机器规模</strong></td><td>78 (100%)</td><td><strong>50 (64%)</strong></td><td><strong>28 (36%)</strong></td><td><strong>-</strong></td></tr><tr><td><strong>请求数量</strong></td><td>36671 (100%)</td><td><strong>17091 (47%)</strong></td><td><strong>19580 (53%)</strong></td><td><strong>-</strong></td></tr><tr><td><strong>GPU吞吐</strong> <strong>(TGS)</strong></td><td>-</td><td><strong>183</strong></td><td><strong>319</strong></td><td><strong>提升74%</strong></td></tr><tr><td><strong>限流数量</strong></td><td>687 ( 1.87%)</td><td><strong>570 (3.3%)</strong></td><td><strong>117 (0.59%)</strong></td><td><strong>减少82%</strong></td></tr><tr><td>备注： 1、数据来源基于Kimi-K2-instruct-0905模型。</td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table><p>客户对于系统的云原生改造表示高度认可：“云原生AI系统的导入，让我们不仅在资源利用上实现了显著的性价比提升，同时在关键业务高峰期的响应能力也大大增强，显著减少了因限流带来的服务瓶颈问题。”</p><p>﻿</p><p>﻿</p><h2>五、未来展望</h2><p>京东云将继续优化<strong>云原生AI推理框架</strong>，致力于为客户提供更智能、高效、稳定的AI基础设施。通过在各个行业和应用场景中的深化应用，我们的客户可以持续依赖这一平台，实现业务的长期可持续发展。</p><p>这个成功案例不仅展示了京东云<strong>云原生AI推理框架</strong>系统的技术优势，也为其他企业提供了一个可借鉴的成功模型，期待更多客户从中获益。</p><p>京东云云原生AI推理框架的研发升级并非一蹴而就。从架构设计、配置调试再到全量上线，每一步都围绕着<strong>业务价值、性能提升与运维提效</strong>展开。我们相信，只有将稳定性、性能、成本三者统筹兼顾的基础设施，才能真正支撑AI业务规模化、可持续地落地与增长。如您有类似场景或技术交流需求，欢迎随时联系我们。</p>]]></description></item><item>    <title><![CDATA[Agent Skills与MCP：一场被误解的"替代战争" 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606050</link>    <guid>https://segmentfault.com/a/1190000047606050</guid>    <pubDate>2026-02-11 18:08:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：牛潇</p><p>在 Agentic AI 快速演进的今天，“Agent Skills 会取代 MCP”或“MCP 已经过时”的声音不绝于耳。这种二元对立的叙事，看似犀利，实则掩盖了智能体架构设计中最本质的真相：<strong>Agent Skills 与 MCP 并非对手，而是搭档</strong>。</p><p>﻿</p><p>前者是人类智慧的结晶——将业务规则、决策逻辑与合规要求，以声明式、可读、可维护的方式注入 AI 代理；后者是技术能力的桥梁——让 AI 能安全、可靠地触达现实世界的数据、工具与系统。一个回答“怎么做才对”，一个解决“能不能做”；一个由产品经理和业务专家驱动，一个由工程师和 SRE 构建。</p><p>﻿</p><p>本文旨在彻底厘清二者的核心差异、适用边界与协同模式。我们将从概念定义出发，深入实现机制，通过多维对比与真实场景剖析，最终给出一套可落地的选择策略与高阶架构范式。无论你是智能体开发者、平台架构师，还是正在规划 AI 原生应用的产品负责人，都能从中获得清晰的判断框架与实践指引。</p><p>﻿</p><p>更重要的是，我们将证明：<strong>真正的智能，不在于选择某一种工具，而在于知道何时用哪一种，以及如何让它们共舞</strong>。</p><p>﻿</p><h2>一、核心概念定义：超越实现的标准</h2><h3>1.1 Model Context Protocol (MCP) - 模型上下文协议</h3><p><strong>本质定义</strong>：MCP是一种<strong>标准化通信协议</strong>，定义了AI模型如何与外部系统建立安全、高效、可审计的双向连接。</p><p><strong>核心特性</strong>：</p><p>•<strong>能力扩展协议</strong>：为AI代理提供访问实时数据、专业工具和企业系统的标准化接口</p><p>•<strong>安全沙箱规范</strong>：在协议层面定义权限控制、数据隔离和操作审计机制</p><p>•<strong>上下文同步机制</strong>：解决模型内部状态与外部世界状态的一致性问题</p><p>•<strong>标准化接口</strong>：通过JSON-RPC或其他标准协议定义请求/响应格式、认证机制和错误处理</p><p>•<strong>服务化架构</strong>：协议设计支持独立部署的服务进程，实现高可用和水平扩展</p><p><strong>架构定位</strong>：MCP解决了 <strong>"能不能做"</strong> 的问题 (Capability)，为AI代理扩展其原始训练数据范围之外的能力边界。</p><h3>1.2 Agent Skills - 代理技能标准</h3><p><strong>本质定义</strong>：Agent Skills是一种<strong>模块化能力封装标准</strong>，通过声明式、配置化的方式定义AI代理在特定场景下的行为规范、决策逻辑和工作流程。</p><p><strong>核心特性</strong>：</p><p>•<strong>流程编排标准</strong>：定义如何将原子操作组合成完整业务流程的标准</p><p>•<strong>上下文感知能力</strong>：标准定义了技能如何根据对话历史和环境动态调整行为</p><p>•<strong>透明可解释性</strong>：决策路径对人类可见，便于理解和修改</p><p>•<strong>轻量级集成</strong>：标准设计支持无服务部署，修改配置即可生效</p><p>•<strong>组合式架构</strong>：支持技能的嵌套、组合和复用</p><p><strong>架构定位</strong>：Agent Skills解决了 <strong>"怎么做才对"</strong> 的问题 (Orchestration)，为AI代理编写符合业务标准和人类期望的行为规范。</p><p>﻿</p><h2>二、设计哲学与架构差异</h2><h3>2.1 MCP：能力导向的设计</h3><p>MCP的核心设计哲学是<strong>能力扩展</strong>。它关注：</p><p>•<strong>原子操作</strong>：如何安全地执行单一、精确的操作</p><p>•<strong>连接管理</strong>：如何高效管理与外部系统的连接</p><p>•<strong>权限边界</strong>：如何在协议层面实现细粒度权限控制</p><p>•<strong>数据标准化</strong>：如何统一不同数据源的格式和语义</p><p>MCP的架构本质上是<strong>服务化</strong>的：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606052" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>2.2 Agent Skills：流程导向的设计</h3><p>Agent Skills的核心设计哲学是<strong>业务价值</strong>。它关注：</p><p>•<strong>决策逻辑</strong>：在特定情境下如何做出正确决策</p><p>•<strong>流程规范</strong>：如何将多个操作组合成符合业务标准的流程</p><p>•<strong>上下文适应</strong>：如何根据环境变化动态调整行为</p><p>•<strong>人类协作</strong>：如何使AI行为可理解、可预测、可修正</p><p>Agent Skills的架构本质上是<strong>声明式</strong>的：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606053" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>三、多维度对比分析</h2><table><thead><tr><th>维度</th><th>Agent Skills</th><th>MCP</th></tr></thead><tbody><tr><td><strong>协议/标准类型</strong></td><td>声明式配置标准 (Declarative)</td><td>通信协议标准 (Imperative)</td></tr><tr><td><strong>核心关注点</strong></td><td>业务流程与决策逻辑 (What &amp; Why)</td><td>能力执行与数据获取 (How &amp; Can)</td></tr><tr><td><strong>抽象层级</strong></td><td>业务逻辑层 (Business Logic Layer)</td><td>能力扩展层 (Capability Layer)</td></tr><tr><td><strong>数据流向</strong></td><td>自顶向下 (决策驱动)</td><td>自底向上 (能力驱动)</td></tr><tr><td><strong>变更频率</strong></td><td>高 (业务规则经常变化)</td><td>低 (接口相对稳定)</td></tr><tr><td><strong>维护主体</strong></td><td>业务专家、领域专家</td><td>工程师、系统管理员</td></tr><tr><td><strong>安全模型</strong></td><td>软约束 (依赖执行引擎的策略)</td><td>硬约束 (协议层强制控制)</td></tr><tr><td><strong>性能特性</strong></td><td>低延迟 (纯逻辑决策)</td><td>可变延迟 (依赖外部系统)</td></tr><tr><td><strong>复用模式</strong></td><td>领域特定复用</td><td>跨领域通用复用</td></tr><tr><td><strong>标准化程度</strong></td><td>高 (结构化配置)</td><td>高 (协议规范)</td></tr></tbody></table><h2>﻿</h2><h2>四、核心区别总结</h2><h3>4.1 本质差异</h3><blockquote><strong>Agent Skills定义业务价值路径，MCP实现技术能力扩展</strong></blockquote><p>•<strong>Agent Skills关注"为什么"和"如何"</strong> ：</p><p>◦为什么这个任务对业务有价值？</p><p>◦如何确保任务按照业务标准和合规要求完成？</p><p>◦如何在不同业务情境下动态调整决策逻辑？</p><p>◦其设计哲学是以业务为中心，将人类专业知识编码为AI可执行的规范</p><p>•<strong>MCP关注"什么"和"能否"</strong> ：</p><p>◦需要访问什么外部数据或工具？</p><p>◦AI能否安全、可靠地执行这个具体操作？</p><p>◦如何在协议层面实现权限控制和数据隔离？</p><p>◦其设计哲学是以能力为中心，解决AI与现实世界连接的技术问题</p><h3>4.2 架构隐喻</h3><p>•<strong>Agent Skills如同"企业SOP手册"</strong> ：</p><p>◦详细描述每个业务流程的标准步骤</p><p>◦规定在特定情境下的决策规则</p><p>◦可由非技术人员编写和维护</p><p>◦随业务需求灵活调整</p><p>•<strong>MCP如同"企业IT基础设施"</strong> ：</p><p>◦提供基础数据访问和计算能力</p><p>◦确保系统安全性和可靠性</p><p>◦需要专业技术团队维护</p><p>◦变更需要严格测试和审批流程</p><h2>﻿</h2><h2>五、场景示例对比：标准应用与实际落地</h2><h3>5.1 场景一：客户服务工单处理</h3><p><strong>需求</strong>：自动处理客户支持工单，需要理解客户意图、查询相关数据、生成适当回复。</p><p><strong>Agent Skills标准应用</strong>（业务流程编排）：</p><pre><code>skill:
  name: "customer_ticket_processing"
  trigger: 
    event: "new_ticket_created"
  workflow:
    steps:
      - name: "intent_classification"
        description: "识别客户工单类型"
        rules:
          - "如果包含'退款'、'钱'等关键词，标记为财务类"
          - "如果包含'无法登录'、'错误'，标记为技术类"
          - "如果包含'多久'、'什么时候'，标记为咨询类"
      
      - name: "data_requirements"
        description: "确定需要查询的数据"
        conditional:
          if: "ticket_type == 'financial'"
          then: ["mcp_order_history", "mcp_payment_records"]
          elif: "ticket_type == 'technical'"
          then: ["mcp_user_activity", "mcp_system_logs"]
      
      - name: "response_generation"
        description: "生成符合品牌标准的回复"
        prompt_template: |
          你是一个专业客服代表，遵循以下规则：
          1. 使用友好、专业的语气
          2. 对于财务问题，必须提供具体金额和时间
          3. 对于技术问题，提供具体解决方案而不是模糊建议
          4. 如无法解决，明确升级路径
        constraints:
          - "不得承诺无法确认的信息"
          - "必须引用数据支持你的结论"
</code></pre><p><strong>为什么适用Agent Skills</strong>：</p><p>•✅<strong>业务规则复杂</strong>：需要根据多种条件动态调整处理流程</p><p>•✅<strong>合规要求高</strong>：必须遵循特定的沟通标准和数据使用规范</p><p>•✅<strong>频繁变更</strong>：客户政策和响应标准经常变化</p><p>•❌<strong>不适合MCP</strong>：这不是原子操作，而是需要上下文感知的决策流程</p><hr/><p>﻿</p><p><strong>MCP标准应用</strong>（能力提供）：</p><pre><code>class CustomerDataMCP:
    @mcp_tool(permission="read_only")
    def get_order_history(self, customer_id, limit=10):
        """安全获取客户订单历史"""
        # 从订单数据库获取数据
        orders = self.order_db.query(
            "SELECT order_id, amount, status, created_at FROM orders WHERE customer_id=? ORDER BY created_at DESC LIMIT?",
            [customer_id, limit]
        )
        return {
            "orders": orders,
            "total_count": self.order_db.count("orders", {"customer_id": customer_id})
        }
    
    @mcp_tool(permission="read_only")
    def get_system_status(self):
        """获取系统当前状态"""
        # 从监控系统获取实时状态
        return self.monitoring_api.get_current_status()
    
    @mcp_tool(permission="write")
    def create_support_note(self, ticket_id, note_content, agent_id):
        """创建客服备注，需要写权限"""
        if not self.auth.has_permission(agent_id, "support_write"):
            raise PermissionError("Insufficient permissions")
        return self.support_db.insert_note(ticket_id, note_content, agent_id)
</code></pre><p><strong>为什么适用MCP</strong>：</p><p>•✅<strong>数据敏感性</strong>：涉及客户个人数据，需要严格的权限控制</p><p>•✅<strong>跨系统集成</strong>：需要连接订单系统、监控系统和客服系统</p><p>•✅<strong>结构化输出</strong>：需要统一的数据格式，避免文本解析歧义</p><p>•❌<strong>不适合Agent Skills</strong>：这不是业务决策，而是需要安全控制的原子操作</p><h3>5.2 场景二：金融风险评估</h3><p><strong>需求</strong>：为贷款申请提供风险评估，需要综合多源数据、应用复杂模型、生成合规报告。</p><p><strong>Agent Skills标准应用</strong>（决策规则与合规性）：</p><pre><code>skill:
  name: "loan_risk_assessment"
  trigger: 
    event: "loan_application_received"
  workflow:
    compliance_rules:
      - "必须检查申请者年龄是否≥18岁"
      - "必须验证收入证明真实性"
      - "禁止基于种族、性别等因素做决策"
      - "超过$100,000的贷款必须人工审核"
    
    assessment_steps:
      1. "data_collection":
           tools: ["mcp_credit_report", "mcp_income_verification", "mcp_employment_history"]
      
      2. "risk_calculation":
           description: "应用公司标准风险模型"
           rules:
             - "信用分&lt;600：高风险"
             - "负债收入比&gt;50%：中高风险"
             - "就业历史&lt;2年：中风险"
      
      3. "decision_logic":
           rules:
             - "如果高风险因素≥2，拒绝贷款"
             - "如果中风险因素≥3，要求额外担保"
             - "否则，批准贷款但限制额度"
      
      4. "report_generation":
           template: |
             # 贷款风险评估报告
             **申请人**: {applicant_name}
             **申请金额**: ${loan_amount}
             
             ## 风险因素分析
             {risk_factors_section}
             
             ## 决策依据
             {decision_rationale}
             
             ## 合规声明
             本评估严格遵循[相关法规]，未考虑受保护特征。
</code></pre><p><strong>为什么适用Agent Skills</strong>：</p><p>•✅<strong>合规驱动</strong>：需要严格遵循金融法规和内部政策</p><p>•✅<strong>决策复杂</strong>：需要权衡多个风险因素并应用业务规则</p><p>•✅<strong>审计要求</strong>：需要完整的决策路径记录和解释</p><p>•❌<strong>不适合MCP</strong>：这不是技术实现问题，而是业务决策逻辑</p><hr/><p>﻿</p><p><strong>MCP标准应用</strong>（数据获取与模型执行）：</p><pre><code>class FinancialRiskMCP:
    @mcp_tool(permission="sensitive_data")
    def get_credit_report(self, applicant_id):
        """获取信用报告，处理敏感数据"""
        # 通过安全通道调用外部信用机构API
        report = self.credit_api.get_report(applicant_id)
        # 数据脱敏处理
        return self._sanitize_sensitive_data(report)
    
    @mcp_tool(permission="model_execution")
    def run_risk_model(self, features):
        """执行风险评估模型"""
        # 加载预训练的风险评估模型
        model = self.model_registry.get("risk-assessment-v3")
        # 特征工程和预测
        processed_features = self._preprocess_features(features)
        prediction = model.predict(processed_features)
        # 生成可解释的模型输出
        explanation = self.explainer.generate_explanation(model, processed_features)
        return {
            "risk_score": prediction,
            "confidence": model.confidence_score,
            "key_factors": explanation.top_factors
        }
    
    @mcp_tool(permission="document_generation")
    def generate_compliance_report(self, assessment_data):
        """生成合规的审计报告"""
        # 应用合规模板
        report = self.report_template.render(assessment_data)
        # 添加数字签名
        signed_report = self.crypto.sign_document(report)
        # 存档到审计系统
        self.audit_system.archive(signed_report)
        return signed_report
</code></pre><p><strong>为什么适用MCP</strong>：</p><p>•✅<strong>数据安全</strong>：涉及敏感金融数据，需要严格的访问控制</p><p>•✅<strong>专业模型</strong>：需要调用专门的风险评估模型</p><p>•✅<strong>审计追踪</strong>：需要完整的操作日志和数字签名</p><p>•❌<strong>不适合Agent Skills</strong>：这不是业务规则，而是需要安全控制的技术操作</p><h3>5.3 场景三：实际落地案例 - Claude Code 中的自动化部署</h3><p><strong>需求</strong>：在软件开发项目中，实现自动化部署流程，包括运行测试、构建和部署到生产环境。</p><p>这是一个已在<strong>Claude Code</strong>中实际落地的场景，完美展现了 MCP 与 Skills 的协同工作模式。</p><h4>MCP 层实现：提供原子能力</h4><pre><code># mcp_deployment_server.py
from mcp_server import MCPServer, mcp_tool

class DeploymentMCP(MCPServer):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self._setup_connections()
    
    def _setup_connections(self):
        """建立必要的系统连接"""
        self.ci_connection = self._connect_to_ci_system()
        self.s3_client = self._setup_s3_client()
    
    @mcp_tool()
    def run_tests(self):
        """
        运行项目测试套件
        返回结构化的测试结果
        """
        # 通过CI系统API触发测试
        test_result = self.ci_connection.run_pipeline("test")
        
        # 返回结构化结果
        return {
            "success": test_result["status"] == "passed",
            "total_tests": test_result["total"],
            "failed_tests": test_result["failed"],
            "duration_ms": test_result["duration"]
        }
    
    @mcp_tool(permission="deployment_write")
    def upload_to_s3(self, environment="production"):
        """
        将构建产物上传到S3
        需要部署权限
        """
        # 验证环境
        if environment not in ["staging", "production"]:
            raise ValueError("Invalid environment")
        
        # 获取最新构建产物
        build_artifact = self.ci_connection.get_latest_build_artifact()
        
        # 上传到S3
        bucket_name = f"myapp-{environment}-bucket"
        key = f"builds/{build_artifact['version']}/{build_artifact['filename']}"
        
        self.s3_client.upload_file(
            build_artifact['local_path'],
            bucket_name,
            key
        )
        
        return {
            "status": "success",
            "bucket": bucket_name,
            "key": key,
            "url": f"https://{bucket_name}.s3.amazonaws.com/{key}"
        }
</code></pre><p><strong>MCP 层关键价值</strong>：</p><p>•✅<strong>安全封装</strong>：敏感凭证（S3密钥、CI系统令牌）完全封装在服务内部</p><p>•✅<strong>标准化接口</strong>：提供统一的输入/输出格式，便于调用</p><p>•✅<strong>错误处理</strong>：在服务层处理网络错误、超时等异常情况</p><p>•✅<strong>权限控制</strong>：通过<code>@mcp_tool(permission="deployment_write")</code>严格控制部署权限</p><h4>Skills 层实现：定义业务流程</h4><pre><code># CLAUDE.md (项目根目录)

## Skill: 代码部署 (Deploy)

 **触发条件** ：用户要求"deploy"、"部署"、"上线"或相关操作

 **执行流程** ：
1.  **运行测试** ：
   - 首先调用 MCP 工具 `run_tests`
   - 如果返回失败，立即停止并报错："测试未通过，无法部署"
   - 不要继续执行后续步骤
   - 具体失败原因：{failed_tests} 个测试失败

2.  **执行上传** ：
   - 如果测试通过，调用 MCP 工具 `upload_to_s3`
   - 参数设置：
     - environment: "production"（生产环境）
   - 等待上传完成确认
   - 验证返回的S3 URL是否可访问

3.  **验证部署** ：
   - 访问部署后的URL进行健康检查
   - 确认关键功能是否正常工作
   - 如果验证失败，触发回滚流程

4.  **报告结果** ：
   - 用简洁的语言总结部署结果
   - 包含关键信息：部署时间、版本号、S3 URL
   - 如果有问题，提供具体的错误信息和建议

 **安全规则** ：
- 永远不要直接在生产环境执行未经测试的代码
- 任何破坏性操作（如数据库迁移）必须先询问用户确认
- 部署前必须备份当前版本
- 生产环境部署必须获得至少一名资深工程师的批准
</code></pre><p><strong>Skills 层关键价值</strong>：</p><p>•✅<strong>业务逻辑清晰</strong>：用自然语言描述完整的部署流程，易于理解和修改</p><p>•✅<strong>灵活调整</strong>：业务规则变化时（如添加预发布环境），只需修改配置</p><p>•✅<strong>团队协作</strong>：非工程师（如产品经理、QA）也能理解并参与优化流程</p><p>•✅<strong>透明决策</strong>：用户可以看到完整的推理过程，增强信任</p><h4>协同工作流程</h4><pre><code>用户请求："部署最新版本到生产环境"

1. Claude 检测到"部署"关键词，激活 "代码部署 (Deploy)" Skill
2. Skill 定义第一步：运行测试
   → 调用 MCP 工具 `run_tests`
   ← MCP 返回：{"success": true, "total_tests": 125, "failed_tests": 0}
3. Skill 检查测试结果，判定通过
4. Skill 定义第二步：执行上传
   → 调用 MCP 工具 `upload_to_s3` with {"environment": "production"}
   ← MCP 返回：{
        "status": "success", 
        "bucket": "myapp-production-bucket",
        "key": "builds/v2.3.1/app-bundle.zip",
        "url": "https://myapp-production-bucket.s3.amazonaws.com/builds/v2.3.1/app-bundle.zip"
      }
5. Skill 进行验证和报告
6. 最终输出："✅ 部署成功！版本 v2.3.1 已部署到生产环境，S3 URL: https://..."
</code></pre><p><strong>为什么这种分层架构最优</strong>：</p><p>•✅<strong>关注点分离</strong>：MCP 专注"如何安全执行"，Skills 专注"如何正确流程"</p><p>•✅<strong>变更独立性</strong>：修改部署流程不需要修改 MCP 服务，反之亦然</p><p>•✅<strong>复用性</strong>：<code>run_tests</code>和<code>upload_to_s3</code>工具可被其他 Skill 复用</p><p>•✅<strong>安全与灵活性平衡</strong>：敏感操作受控，业务逻辑灵活可变</p><p><strong>实际工程价值</strong>：</p><p>•<strong>开发效率</strong>：新团队成员通过阅读<code>CLAUDE.md</code>即可理解部署流程</p><p>•<strong>运维可靠性</strong>：MCP 层的错误处理和重试机制提高了系统稳定性</p><p>•<strong>合规保证</strong>：所有部署操作都有完整审计日志</p><p>•<strong>快速迭代</strong>：业务流程调整只需修改配置，无需重新部署服务</p><h2>﻿</h2><h2>六、选择建议与高阶策略</h2><h3>6.1 基础决策框架</h3><p><strong>优先选择Agent Skills当</strong>： ✅ 业务规则复杂且经常变化：当决策逻辑依赖于业务策略而非技术实现时 ✅ 需要人类可读的规范：当非技术人员需要理解和修改行为规则时 ✅ 涉及主观判断：当任务需要权衡多个因素且没有明确的算法时 ✅ 强调一致性和合规性：当需要确保AI行为符合公司政策或法规要求时 ✅ 快速原型和迭代：当需要快速验证想法而不想投入大量工程资源时</p><p><strong>优先选择MCP当</strong>： ✅ 需要访问外部数据源：当任务依赖实时数据、专有系统或敏感信息时 ✅ 性能要求严格：当需要高效处理大量数据或低延迟响应时 ✅ 安全性至关重要：当涉及财务交易、个人隐私或系统关键操作时 ✅ 需要精确控制：当任务要求精确的输入/输出格式或复杂的状态管理时 ✅ 跨系统集成：当需要连接多个不兼容的系统或协议时</p><h3>6.2 高阶决策树</h3><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606054" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>6.3 经典协同模式</h3><p><strong>模式1：分层架构（最常见）</strong></p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606055" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p><strong>最佳实践</strong>：</p><p>•<strong>Agent Skills负责"为什么"和"做什么"</strong> ：定义业务目标和流程</p><p>•<strong>MCP负责"怎么做"</strong> ：提供具体的执行能力</p><p>•<strong>严格分离关注点</strong>：避免在Skills中硬编码技术细节，避免在MCP中包含业务规则</p><p><strong>模式2：技能驱动型MCP</strong></p><pre><code>skill:
  name: "dynamic_mcp_selection"
  description: "根据上下文动态选择最合适的MCP工具"
  logic:
    - if: "data_freshness_requirement == 'real-time'"
      then: "use mcp_live_data_feed"
    - if: "data_volume &gt; 1GB"
      then: "use mcp_batch_processing"
    - if: "security_classification == 'sensitive'"
      then: "use mcp_encrypted_channel"
</code></pre><p><strong>优势</strong>：最大化灵活性，适应复杂多变的业务需求</p><p><strong>模式3：MCP增强型技能</strong></p><pre><code>class SkillEnhancementMCP:
    @mcp_tool
    def get_optimal_workflow(self, task_type, context):
        """基于历史数据推荐最佳工作流程"""
        # 分析历史任务完成数据
        historical_data = self.analytics_db.get_task_metrics(task_type)
        # 应用机器学习模型推荐最优流程
        recommended_workflow = self.recommender.predict_optimal_workflow(
            task_features=context,
            historical_performance=historical_data
        )
        return recommended_workflow
</code></pre><p><strong>优势</strong>：利用数据驱动优化技能定义，形成闭环学习系统</p><p>﻿</p><h2>七、总结与技术展望</h2><h3>7.1 核心原则重申</h3><p>1.<strong>MCP = 能力扩展 (Capability Extension)</strong> ：解决"能不能做"的问题</p><p>2.<strong>Agent Skills = 业务编排 (Business Orchestration)</strong> ：解决"怎么做才对"的问题</p><p>3.<strong>协同而非替代</strong>：两者在智能体架构中互补共存，创造最大价值</p><h3>7.2 真实工程经验教训</h3><p>在多个大型AI系统中，我们观察到以下关键点：</p><p><strong>误区1：用MCP实现所有功能</strong></p><p>•<strong>症状</strong>：每个小功能都实现为MCP工具</p><p>•<strong>后果</strong>：过度工程化，维护成本高，业务逻辑与技术实现耦合</p><p>•<strong>解法</strong>：优先评估是否需要外部系统访问或安全控制</p><p><strong>误区2：在Agent Skills中硬编码复杂逻辑</strong></p><p>•<strong>症状</strong>：Skills配置超过2000行，包含大量条件判断</p><p>•<strong>后果</strong>：决策逻辑难以理解和维护，执行不可靠</p><p>•<strong>解法</strong>：将复杂逻辑拆分为MCP工具，Skills只负责业务编排</p><p><strong>误区3：忽视安全边界</strong></p><p>•<strong>症状</strong>：在Skills中暴露敏感操作，在MCP中缺少输入校验</p><p>•<strong>后果</strong>：安全漏洞，数据泄露风险</p><p>•<strong>解法</strong>：敏感操作始终通过MCP，Skills只包含公开的业务规则</p><h3>7.3 未来展望</h3><p>随着AI智能体架构的发展，我们观察到以下趋势：</p><p>1.<strong>标准化融合</strong>：</p><p>◦MCP协议将支持技能描述标准，使能力发现和组合更加自动化</p><p>◦Agent Skills标准将内置对MCP能力的语义描述，提升互操作性</p><p>2.<strong>动态协同</strong>：</p><p>◦智能体将能够根据任务复杂度自动决定使用Skills还是MCP</p><p>◦运行时将动态平衡配置驱动和代码驱动的执行路径</p><p>3.<strong>开发者体验优化</strong>：</p><p>◦统一的开发框架将无缝集成Skills和MCP</p><p>◦低代码平台将使业务专家能够定义技能，自动映射到合适的MCP能力</p><h2>﻿</h2><h2>八、结语：协同共生，而非零和博弈</h2><p>在AI技术社区中，经常出现"Skills将取代MCP"或"MCP是过时的技术"等论调。这些观点源于对两者本质的误解，忽视了它们在智能体架构中互补共存的价值。</p><p><strong>MCP和Agent Skills不是竞争关系，而是共生关系</strong>：</p><p>•MCP扩展了AI的感知和行动能力，使其能够连接现实世界</p><p>•Agent Skills定义了AI的思维和决策模式，使其能够按照人类期望的方式行动</p><p>将AI智能体视为一个完整的系统：</p><p>•<strong>MCP是感官和肢体</strong>：眼睛（数据获取）、耳朵（事件监听）、手（工具执行）</p><p>•<strong>Agent Skills是大脑和神经系统</strong>：决策逻辑、行为规范、学习能力</p><p>没有感官和肢体，大脑无法感知世界；没有大脑和神经系统，肢体无法协调行动。两者缺一不可。</p><p>﻿</p><h3><strong>终极建议</strong>：</h3><p>不要陷入"二选一"的思维陷阱。最强大的AI代理系统往往是Skills和MCP精心设计的协同体：</p><p>•<strong>用Skills定义业务价值</strong>：什么是对用户真正有用的？</p><p>•<strong>用MCP实现技术可能</strong>：如何最安全、高效地交付这些价值？</p><p>•<strong>持续优化两者的边界</strong>：随着业务演进和技术进步，重新评估职责分配</p><p>记住：<strong>技术的目的是解决问题，而不是创造新的复杂性</strong>。Skills和MCP都是工具，明智的工程师会根据具体问题选择最合适的工具，或将多个工具创造性地组合，以交付最大价值。</p><p>在AI代理的未来，我们不会看到Skills取代MCP，或MCP淘汰Skills。相反，我们将见证一个融合的生态系统，其中配置驱动的灵活性与代码实现的强大性和谐共存，共同推动AI代理走向更智能、更可靠、更有价值的未来。</p><p>﻿</p><hr/><p>﻿</p><p>﻿</p><h2>参考资料与延伸阅读</h2><p>本文引用的核心概念与技术规范基于以下行业权威文档，推荐读者深入阅读以获取更多技术细节：</p><ol><li>Model Context Protocol Specification</li></ol><p>•来源: Anthropic &amp; MCP Community</p><p>•地址: <a href="https://link.segmentfault.com/?enc=ZV%2FqXQWcq7JQGyKu9Z00zw%3D%3D.f7%2Bx%2B%2Bd3X7mUQRmp703xfrU2a71VZGn3gPM9FC5imGwssjkqiMdIGhHx1u5DPPHX" rel="nofollow" target="_blank">modelcontextprotocol.io/introduction</a>﻿</p><p>•对应内容: 支持文中第一章与第二章关于 MCP 作为“标准化连接协议”、“安全沙箱”及“JSON-RPC 消息规范”的技术定义。</p><ol start="2"><li>Building Effective Agents</li></ol><p>•来源: Anthropic Research Team (2024)</p><p>•地址: <a href="https://link.segmentfault.com/?enc=7SqYuemm8BiscPdRQM7yvA%3D%3D.xwTpEpEiDc9Nn5ZtUd2tGcdVb6JjXWSsGwERTJRqPrBlQ5YtIfvvlVFnE3GdcQFi1ZVvWNsdZ3En7HkRYPZIYA%3D%3D" rel="nofollow" target="_blank">anthropic.com/research/building-effective-agents</a>﻿</p><p>•对应内容: 支持文中关于“Workflows（工作流） vs Agents（自主体）”的区分，以及为何在生产环境中应优先采用确定性较高的 Agent Skills（即文中提到的 Orchestration）。</p><ol start="3"><li>The Future of Agentic AI &amp; Design Patterns</li></ol><p>•来源: Andrew Ng, DeepLearning.AI (The Batch, Issue 242)</p><p>•地址: <a href="https://link.segmentfault.com/?enc=R0id3t7zJUqne9L8aiwylQ%3D%3D.ESZ117ZAWg%2Fx%2FXjDWOEnOQ6BL5lrIFj%2BaNaK3xAcIWP9NEpYd1az2ZiYelK8BlpakIRLy1hxXXNF0iNRIeyt1w%3D%3D" rel="nofollow" target="_blank">deeplearning.ai/the-batch/issue-242</a>﻿</p><p>•对应内容: 支持文中关于“SOP 即智能”的观点，详细阐述了通过结构化流程（Agentic Workflows）来提升 AI 产出质量的设计模式。</p><ol start="4"><li>Semantic Kernel Overview</li></ol><p>•来源: Microsoft Learn</p><p>•地址: <a href="https://link.segmentfault.com/?enc=0Nf0IHEknFZ8x3qzW9xvsw%3D%3D.s138dWfYk1H4VaeLcpCTEvSVY7UTorkMXJWTj56CzEg8K4vphlC8xKj%2BlpRckXOPMfsV7yW%2Bb5iIfwiEoA%2BTJg%3D%3D" rel="nofollow" target="_blank">learn.microsoft.com/en-us/semantic-kernel/overview</a>﻿</p><p>•对应内容: 提供了文中“声明式技能”的工程实现参考，展示了如何将自然语言 Prompt 封装为可调用的 Skills/Plugins。</p>]]></description></item><item>    <title><![CDATA[突破传统限制：OxygenREC--一个基于指令跟随的“快慢思考“电商生成式推荐框架 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606061</link>    <guid>https://segmentfault.com/a/1190000047606061</guid>    <pubDate>2026-02-11 18:07:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：李卿阳</p><p>在电商推荐系统中，推荐模型长期面临着两个核心矛盾：一方面，传统的多阶段级联推荐系统存在目标不一致和误差累积的问题；另一方面，直接引入大型语言模型LLM虽然能带来强大的推理能力，但其高昂的延迟和计算成本在工业级应用中难以承受。更重要的是，现有的生成式推荐方法在多场景扩展性上面临巨大瓶颈--每个场景都需要独立训练和部署，导致资源利用率低下、维护成本高昂。</p><p>京东零售OxygenREC团队在论文《OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation》中提出了一种全新的解决方案：<strong>OxygenREC</strong>。这是一个基于“快慢思考”的指令跟随生成式推荐框架，不仅解决了推理能力与延迟之间的矛盾，更实现了“一次训练，多处部署”的多场景统一高效解决方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606063" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>一、 关键挑战</h2><p>OxygenREC 旨在解决当前推荐系统，特别是生成式推荐范式下的三大核心难题：</p><p>1.<strong>有限的演绎推理能力</strong>：现有的生成式推荐方法主要从用户海量行为中进行归纳学习，但在需要结合现实世界知识进行<strong>深度演绎推理</strong>的场景下表现不佳。比如下边两个例子：</p><p>1.当推荐的时空背景和用户画像是“成都冬至时的年轻宝妈”时，传统模型可能只是推荐“冬季外套”这样的商品，而无法深度推理出此时成都是“冷湿环境”，这位年轻母亲潜在的需求可能是“婴儿排汗睡衣”。</p><p>2.有个户外运动vlogger在购物行为中反复对比华为Mate 70和iPhone 16 Pro两款手机，传统系统因为用户频繁的交互历史，只会不断加强重复推荐这两款商品进行比价，而无法推理出其真正诉求可能是“高质量的移动影像”，从而模型未能精准推荐‘华为Pura’系列这一真正符合用户诉求的目标商品。</p><p>2.<strong>多场景适应与资源效率的矛盾</strong>：大部分推荐平台拥有首页、频道流、购物车、搜索等多种推荐场景。现有生成式推荐模型如果为每个场景训练独立模型，会带来巨大的运营和计算成本，而使用简单的统一模型又会面临“负迁移”问题--不同场景间的知识相互干扰，导致性能下降。</p><p>3.<strong>工业级部署的工程挑战</strong>：将<strong>LLM的深度推理能力</strong>与推荐系统的大规模稀疏特征、<strong>严格延迟</strong>要求相结合，是一个巨大的<strong>系统工程</strong>挑战。它需要同时处理推荐系统典型的TB级稀疏嵌入和LLM典型的十亿级稠密参数，这对训练框架和推理引擎都提出了极高要求。</p><h2>二、 核心贡献</h2><p>面对这些挑战，京东零售OxygenREC团队提出了一个基于指令跟随的生成式推荐框架-OxygenREC，首次把LLM中的“快慢思考”模式引入到生成式推荐中来。在OxygenREC框架中，通过基于Transformer 的Encoder-Decoder 作为骨干网络，能够根据特定指令生成语义化物品序列，来执行推荐场景的”快思考"方式。在“慢思考”模式中，引入上下文推理指令--由近线LLM pipeline 生成，将用户行为与上下文合成为可解释的指令。同时多场景对齐中，通过场景指令与基于强化学习的对齐机制，实现“一次训练，多场景部署”。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606064" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3><strong>1. “快慢思考”架构：知识注入与低延迟的平衡</strong></h3><p>这是整个OxygenREC的基础，其核心思想是将复杂的推理过程“离线化”，保证在线服务的<strong>低延迟</strong>。</p><p>•<strong>慢思考</strong>：一个近线的LLM pipeline，综合分析用户的时空上下文、个性化特征和历史行为，生成高质量的 <strong>“上下文推理指令”</strong> 。这个过程融合了世界知识，能进行深度演绎推理，但因其是近线批量处理，不增加在线请求的延迟。</p><p>•<strong>快思考</strong>：一个高效的编码器-解码器骨干网络。它接收“慢思考”生成的指令，结合实时用户信号，在严格的延迟限制下生成推荐序列。该骨干网络本身轻量、高效，专为实时推理优化。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606065" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿</p><h3><strong>2. 语义对齐的指令控制机制：让指令真正发挥作用</strong></h3><p>仅仅生成指令是不够的，还必须确保模型能够准确理解并遵循指令。OxygenREC通过两项关键技术实现精准指令控制：</p><p>•<strong>查询到物品的对齐损失</strong>：在训练阶段，通过一个辅助的<strong>Query-to-Item</strong> (Q2I) 损失函数，将指令嵌入与目标物品嵌入在同一个语义空间中对齐。这使得指令能够“理解”物品，并用于检索：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606066" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•<strong>指令引导检索(IGR)</strong> ：在生成推荐时，利用对齐后的指令作为查询，从用户长期历史行为中检索出最相关的部分，过滤掉无关的噪声。这确保了模型生成时专注在与当前指令意图最相关的历史信息上，大大提升了可控性和准确性。</p><p>﻿</p><h3><strong>3. 基于指令与强化学习的多场景统一对齐：Train-Once-Deploy-Everywhere</strong></h3><p>这是解决多场景扩展性的关键。OxygenREC摒弃了为每个场景独立建模的思路。</p><p>•<strong>场景指令化</strong>：将不同的场景信息（如首页、购物车）和可选的触发物品（如用户点击的入口商品）统一编码为 <strong>“场景指令”</strong> ，作为模型的条件输入。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606067" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•<strong>统一奖励映射与策略优化</strong>：设计了一个统一的奖励映射服务，将不同场景、不同业务目标（如GMV，转化率，合法性，多样性）的奖励信号归一化。在此基础上，提出了<strong>Soft Adaptive Group Clip Policy Optimization</strong> (SA-GCPO) ****算法进行强化学习训练:</p><p>﻿<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606068" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•该算法用自适应门控函数替代传统基于GRPO的硬截断方式(hard clip):</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606069" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>•并以基于用户真实反馈的奖励分数作为阈值区分正负advantage样本，显著提升了多任务、多场景下策略学习的稳定性和效率：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606070" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿</p><h3><strong>4. 大规模生产级系统实现</strong></h3><p>为了支撑以上创新，团队构建了完整的工程体系：</p><p>•﻿<a href="https://link.segmentfault.com/?enc=hWISJX5dFN4zXHr8Vxa24A%3D%3D.r9hqxJ3zX4XIREQ2omZ0k1t5ldDl6k1CXHGFzW0NK14QU%2Br44wh%2BQHyXaVayGK5a1BIWLktSaPOTgL7vv%2FEHRBv0MF%2BKOyqd2xcw1zV9h6R4m1qE1giFujHVXQzNLV1mL6XgZxCQWfCmEgxyZWASk6W2BYU5RJei%2FY1SaX0kurv9PWfHptgY3VATOdcZKqJWtNB2jyeaflOGKL0WCoHOrnEV9DlNyAAt%2BMD9%2Fql86g%2F6E0zcRbXJ9Jt4bicAhRX1M9Ye3zsjV5VtuRU4hpZbCcVExxOwBuYFoQSckSbSTGI%3D" rel="nofollow" target="_blank">统一训练框架：基于PyTorch，深度融合了工业级稀疏嵌入引擎和LLM稠密训练引擎，在128张H800 GPU集群上实现了40%的模型FLOPs利用率。</a>﻿</p><p>•<strong>高性能推理引擎xLLM</strong>：针对生成式推荐长上下文、大候选集的特点，定制开发了<a href="https://link.segmentfault.com/?enc=KLyAaUZAINdRK%2BQfiHjlJw%3D%3D.7M%2BPFtCwy4EqcdcK%2B7qEoO4H8WL7lstw94A4ZTHf9ju1NwNFAS5RzSHxIOxH4EX%2F" rel="nofollow" target="_blank">xLLM</a>推理框架，通过xSchedule（系统调度）、xAttention（算子优化）、xBeam（束搜索优化）三级优化，满足线上严格的服务级别目标。</p><p>•<strong>近线指令服务</strong>：推理指令通过近线服务批量生成并存入KV数据库，线上推荐模型直接读取，实现了零在线LLM调用，兼顾了语义丰富性和低延迟。</p><p>﻿</p><h2>三、 实验成果</h2><p>OxygenREC在京东几个核心场景的大量离线实验和在线A/B测试中取得了显著效果，证明OxygenREC 基于生成式推荐的方法在大规模工业级推荐系统中的有效性。</p><h3>1. 基于快慢思考的生成式框架有效性验证</h3><p>•<strong>语义ID</strong>：通过多源对比学习（文本、图像、行为关联）构建的层次化语义ID，在保持高类别纯度（92.8%）的同时，实现了极低的ID碰撞，证明了其强大的表达和区分能力。</p><p>•<strong>指令跟随</strong>：消融实验证明，在BOS右侧插入指令的方式为最佳；融合了场景ID和触发物品ID的指令效果显著优于单一组件；IGR和Q2I对齐机制共同作用带来了显著的性能提升。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606071" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>•<strong>统一模型 vs. 独立模型</strong>：在六个核心场景的对比中，统一的OxygenREC模型全面超越了为每个场景独立微调的基线模型，验证了OxygenREC框架在场景间正向迁移的有效性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606072" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><h3>2. 基于SA-GCPO后训练的有效性验证</h3><p>在后续训练阶段，提出的SA-GCPO算法在合成数据比例变化时表现更稳定，且性能显著优于传统的GRPO及其变体GSPO。例如，在33%合成数据比例下，SA-GCPO在HR\@1和HR\@10上有显著提升。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606073" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3. 电商场景在线A/B测试的商业效果</h3><p>OxygenREC已在京东App上形成覆盖用户购物全链路的部署闭环：首页导流（场景1、2）-&gt; 频道浏览（场景3、4）-&gt; 商品结算转化（场景5、6）。在线测试结果表明，该模型在<strong>所有关键业务指标</strong>上均带来显著提升：</p><p>•<strong>首页场景</strong>：GMV提升4.52%-8.40%。</p><p>•<strong>频道流场景</strong>：其中一个场景的订单量提升了<strong>8.03%</strong> ，显示出模型精准匹配购买意图的能力。</p><p>•<strong>结算路径场景</strong>：在用户强购买意图下，GMV提升高达11.80%。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606074" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>与行业上其他生成式推荐方式对比:</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606075" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>OxygenREC 在几个关键维度上进行了生成式推荐的范式革新：</p><p>•架构上，用“快慢思考”破解了推理与延迟的死结。</p><p>•效率上，用“统一指令模型”破解了多场景训练的困局。</p><p>•控制上，用“语义对齐与引导检索”构建了生成式推荐模型的指令跟随能力。</p><p>•优化上，用“SA-GCPO”和全栈系统优化，确保了技术在工业巨量流量下的可行性、稳定性和卓越性能。</p><p>﻿</p><h2>总结与展望</h2><p>OxygenREC的成功，标志着生成式推荐在工业落地上迈出了关键一步。它通过“快慢思考”巧妙平衡了深度推理与低延迟，通过“指令跟随”实现了对推荐过程的精准可控，并通过统一的奖励与策略学习破解了多场景扩展的难题，真正实现了“一次训练，多场景部署”的pipeline。</p><p>未来，京东零售OxygenREC团队计划从两个方向继续探索：</p><p>•一是向基于语言扩散模型的<strong>非自回归生成</strong>范式演进，从根本上突破序列生成延迟与列表长度的线性关系，满足更高吞吐需求；</p><p>•二是开展<strong>跨场景用户轨迹建模</strong>，从用户在首页、搜索、购物车、结算等多场景的连贯行为中挖掘更深层的用户意图，实现更长周期的价值推荐。</p><p>OxygenREC不仅是一个高效的推荐系统，更为工业级生成式AI应用的大模型设计提供了宝贵范式--如何将大模型的“脑”与小模型的“身手”结合，如何在复杂多目标任务中实现稳定高效的学习，这其中的思想值得广泛借鉴。</p>]]></description></item><item>    <title><![CDATA[2026年自有充电运营平台选型10大关键指标 发怒的皮带 ]]></title>    <link>https://segmentfault.com/a/1190000047606109</link>    <guid>https://segmentfault.com/a/1190000047606109</guid>    <pubDate>2026-02-11 18:06:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>想选到适配业务、能支撑长期发展的充电桩运营平台，可重点关注这 10 个关键指标：</p><table><thead><tr><th>指标</th><th>核心价值</th></tr></thead><tbody><tr><td>1. 私有化部署与源码交付</td><td>拥有底层代码，数据与业务自主权 100% 可控</td></tr><tr><td>2. 定制开发灵活性</td><td>贴合自己的差异化运营场景</td></tr><tr><td>3. 多终端兼容能力</td><td>全场景操作更高效</td></tr><tr><td>4. 核心功能完备性</td><td>支撑日常运营，减少额外成本</td></tr><tr><td>5. 数据可视化分析能力</td><td>用数据做运营决策，更精准</td></tr><tr><td>6. 接口开放性与拓展性</td><td>对接第三方，实现功能扩容</td></tr><tr><td>7. 系统稳定性与并发承载</td><td>高峰时段运营不卡顿、不中断</td></tr><tr><td>8. 合规性与政策适配性</td><td>避开监管和运营风险</td></tr><tr><td>9. 运维与技术支持体系</td><td>降低后期维护成本，问题解决快</td></tr><tr><td>10. 成本与性价比模型</td><td>匹配运营阶段，控制长期投入</td></tr></tbody></table><p>这些指标能帮你精准匹配平台方案、避开选型坑，给独立部署的充电桩运营平台把好合规、功能、成本的关。</p><p>建议选型先把核心指标（比如私有化部署、定制开发）验证清楚，再逐一核对其他维度，这样能选出真正贴合业务的平台。</p><h2>1. 私有化部署与源码交付</h2><p>私有化部署是独立运营的基础，而<strong>源码交付</strong>则是自主权的终极保障。对于追求长期发展的运营商，不仅要数据在自己手里，更要系统架构在自己控制下。</p><p>评估私有化部署能力，可分为以下 5 点：</p><ol><li>​<strong>云环境适配</strong>​：基于云原生设计，重点确认厂商能否快速在你的阿里云、腾讯云或私有服务器上完成一键部署。</li><li>​<strong>源码完整性</strong>​：要求厂商明确提供​<strong>全套前后端源码</strong>​。只有拿到代码，才能避免因厂商倒闭或涨价导致的“卡脖子”风险。</li><li>​<strong>落地细节</strong>​：明确云服务器配置（如：4 核 8G 起步）、部署周期，以及后续版本升级时，源码如何进行平滑的 Git 合并或增量更新。</li><li>​<strong>资金自主权</strong>​：确认平台是否支持​<strong>资金直达运营商自有支付商户号</strong>​，不经过中转平台，确保资金链安全合规。</li><li>​<strong>成本</strong>​：对比 SaaS 按月/按桩付费模式，计算“一次性买断 + 自有云成本”在 3-5 年内的总投入。</li></ol><p>​<strong>注意</strong>​：私有化部署的核心是“资产化”。如果厂商不给源码只给安装包，你依然无法根据业务逻辑进行深度修改。</p><blockquote>某运营商早期使用 SaaS，后期想增加‘物流车队预充值优惠’功能被平台方拒绝。后来部署了自有平台，技术人员经过两周开发就上线了，这就是源码的威力。</blockquote><h2>2. 定制开发灵活性</h2><p>充电桩运营场景差别大，比如公共快充、小区慢充、物流园专用桩，定制开发能力直接决定平台能不能贴合自己的业务。</p><p>评估定制开发能力，重点看这几点原因：</p><ul><li>​<strong>适配自己的场景</strong>​：园区桩需要企业白名单充电功能，物流园桩需要车队充电排班，灵活的定制能避免平台功能和实际运营脱节。</li><li>​<strong>控制成本</strong>​：要是平台是微服务这类模块化架构，能按需加、减功能，不用为用不上的通用模块花钱，减少初期投入。</li><li>​<strong>方便业务升级</strong>​：有完善的二次开发文档和标准化接口，后期想加充电套餐、改分账规则，能快速响应。</li><li>​<strong>落地快</strong>​：厂商能明确定制的计价方式，比如按模块算、按人天算，能确定开发周期，不会无限期拖。</li><li>​<strong>沟通成本低</strong>​：厂商有成熟的定制经验，能减少需求沟通、方案调整的时间。</li></ul><p>以下是定制开发评估的核心维度，供你参考：</p><table><thead><tr><th>评估维度</th><th>核心内容</th><th>适用场景</th></tr></thead><tbody><tr><td>架构设计</td><td>是否是微服务模块化</td><td>有任何定制需求都适用</td></tr><tr><td>接口文档</td><td>是不是完善、标准化</td><td>自己做二次开发或对接第三方时</td></tr><tr><td>响应周期</td><td>评估需求、落地功能要多久</td><td>有紧急定制需求时</td></tr><tr><td>计价方式</td><td>按模块/人天/总价收费</td><td>需要管控定制成本时</td></tr></tbody></table><blockquote>某物流园运营商，因为平台定制灵活性不够，单开发车队充电排班功能就额外花了 10 万，落地还花了 2 个月，错过了运营的好时机。</blockquote><h2>3. 多终端兼容</h2><p>多终端兼容能让运营提效，毕竟运营、运维、用户各有操作场景，对应的终端都得适配。</p><p>评估多终端兼容能力，这么做就对了：</p><ol><li><p>​<strong>覆盖全场景终端</strong>​：</p><ul><li>​<strong>管理端</strong>​：web 电脑端，负责设备管理、订单审计。</li><li>​<strong>运营端</strong>​：移动端 APP/小程序，支持远程监控、快速处理。</li><li>​<strong>用户端</strong>​：微信/支付宝小程序，追求“扫码即充”的极致体验。</li></ul></li><li>​<strong>测数据同步</strong>​：在 web 端修改电价，观察小程序端是否在 1 秒内同步更新。</li><li>​<strong>试 Uniapp 跨端能力</strong>​：确认用户端是否基于 Uniapp 开发，这样一套代码可以同时发布到微信、支付宝等多个平台，覆盖更多流量入口。</li><li>​<strong>操作逻辑统一</strong>​：不同终端的报警推送、工单处理流程应保持闭环，避免运营信息断档。</li></ol><p><strong>多终端兼容评估指南</strong></p><table><thead><tr><th>终端类型</th><th>适用场景</th><th>核心验证点</th></tr></thead><tbody><tr><td>web 管理端</td><td>批量管订单、管设备</td><td>数据看板加载快不快，批量操作顺不顺</td></tr><tr><td>移动端 APP/小程序</td><td>随时监控运营情况</td><td>能否快速处置，故障预警会不会及时推</td></tr><tr><td>微信/支付宝小程序</td><td>用户充电操作</td><td>扫码充电、付订单流不流畅</td></tr><tr><td>充电桩触控屏</td><td>现场充电操作</td><td>本地计费准不准，故障提示清不清晰</td></tr></tbody></table><h2>4. 功能完备性</h2><p>核心功能完不完善，直接决定平台能不能支撑日常运营，避免后期额外开发、增加成本。</p><p>全面核对核心功能，按这些方法来：</p><ol><li><strong>盯紧运营核心需求</strong><br/>重点看设备、订单、用户、财务、运维这五大核心模块，比如设备管理要能远程监控、故障预警，财务管理要可审核可追溯，多运营方要可支持自动分账。</li><li><strong>多维度验证功能</strong><br/>别只看功能清单，还可以这么做：模拟真实的运营场景，比如用户充电到财务对账；测试峰谷电价调价、退款核销这些边缘场景；看看不同角色的操作权限是不是分开的，避免权限混乱。<br/>多维度测，能发现功能漏洞，避免上线后出问题。</li><li><p><strong>用实际案例验证</strong><br/>用真实运营案例测试功能，比只看清单靠谱。<br/>某区域充电运营商，靠平台的自动分账功能，直接适配了桩主、物业、运营商的收益分成，不用额外定制， 节省了三个月的开发时间与开发成本。</p><blockquote>我认识的同行，站点上线后才发现自动分账不支持多角色比例配置，只能手动对账，花费了很多人工与精力。</blockquote></li><li><p><strong>按主题核验功能</strong><br/>给功能核验定几个主题，能覆盖全流程，还不会漏核心点：</p><table><thead><tr><th>核验主题</th><th>主题说明</th><th>核验示例</th></tr></thead><tbody><tr><td>全流程闭环</td><td>从用户充电到财务对账都覆盖</td><td>模拟用户扫码充电，核对订单、计费、分账全流程</td></tr><tr><td>异常场景处理</td><td>测试故障、退款等异常情况</td><td>设备故障时，订单会不会自动暂停，退款能不能秒到账</td></tr><tr><td>批量操作效率</td><td>测试批量管理的能力</td><td>批量改多个站点的计费规则，看看要花多久，最好不超过 1 分钟</td></tr></tbody></table></li><li><strong>做一份可落地的功能清单</strong><br/>制定功能核验清单，关键在这几点：结合自己的运营场景，比如物流园、小区、公共桩；标清楚功能优先级，哪些是必须的，哪些是可选定制的；明确验收标准，比如分账准确率要 100%。</li></ol><p>记住：核心功能完善，能减少初期定制成本，贴合自己场景的功能，才是真正有用的功能。</p><h2>5. 数据可视化与分析能力</h2><p>数据是充电桩运营决策的核心，好的可视化和分析能力，能让运营更精细，提升单桩收益。</p><p>评估数据可视化与分析能力，这些技巧很实用：</p><ol><li><strong>聚焦核心分析维度</strong><br/>别贪多，重点看和收益相关的细节维度，比如别只看整体运营数据，重点看单桩在不同时间段的利用率、站点峰谷时段的收益，维度越精准，对决策的参考价值越大。</li><li><strong>数据看板要服务运营</strong><br/>数据看板不是堆数据，要能指导实际运营。可以用“数据-决策-动作”的逻辑验证，比如单桩利用率低于 30% 时，看板能不能提示调整定价或推广策略，而不是只展示数据。</li><li><strong>优化数据展示形式</strong><br/>数据看板好不好读，决定了用起来顺不顺，一定要打磨好：核心指标比如单桩日收益、设备故障率，要突出展示；用合适的图表，趋势用折线图、占比用饼图；能一键按站点、时间段、桩类型筛选；加预警提醒，比如收益低于阈值时弹窗。</li><li><strong>看重数据的落地性</strong><br/>数据的价值在于落地，想确认平台的数据分析能力能不能用，就这么做：让厂商演示用数据制定运营策略的案例，比如通过充电需求预测调整充电桩布局；验证能不能自定义生成报表，能不能兼容 Excel、BI 工具；测试数据能不能导出、分享，方便团队协作。</li><li><strong>关注数据实时性</strong><br/>优先选数据实时可用的平台，重点看这几点：数据更新延迟不超过 5 分钟；异常数据能不能快速预警；能不能查历史数据，回溯运营情况。</li></ol><p>数据可视化不是选完就完事，而是要长期用数据驱动运营，才能看到收益提升。</p><p>​<strong>小技巧</strong>​：如果平台已有基础数据看板，别浪费，只要让厂商优化展示维度、加上预警功能，就能快速提升数据的价值。</p><table><thead><tr><th>数据能力优化要素</th><th>重要性</th><th>快速优化技巧</th></tr></thead><tbody><tr><td>维度拆分</td><td>匹配运营决策需求</td><td>按站点、单桩、时间段拆分核心指标</td></tr><tr><td>实时性</td><td>支撑即时决策</td><td>要求数据更新延迟 ≤5 分钟</td></tr><tr><td>预警功能</td><td>及时发现问题</td><td>设置单桩收益、故障率的阈值提醒</td></tr><tr><td>导出能力</td><td>方便二次分析</td><td>验证能导出 Excel/BI 工具兼容的格式</td></tr></tbody></table><p>某充电站运营主管建议：</p><blockquote>如果是新手选数据能力，最简单的方法，就是让厂商按你的运营场景演示看板，好不好用一眼就能看出来。</blockquote><p>多验证几次，平台的数据分析能力就能真正帮到运营决策。</p><h2>6. 评估接口开放性与拓展性</h2><p>接口开不开放，决定了平台能不能融入新能源生态，对接流量平台、支付渠道、道闸这些第三方系统，对运营商来说性价比很高。</p><p>原因很简单：能直接对接生态伙伴，不受单一系统限制；低成本就能实现功能扩容，不用重复开发；还能和产业链伙伴建立长期合作。</p><p>让平台的接口能力发挥价值，这么做：</p><ol><li>​<strong>尽早梳理对接需求</strong>​：别拖，现在就列出要对接的核心方，比如小桔、星星等流量平台、道闸系统、监管平台，明确对接的场景。</li><li>​<strong>明确对接标准</strong>​：想让厂商适配对接需求，就要说清要求，比如要有标准化的 API/SDK 接口文档、能查接口调用日志、提供对接测试环境。</li><li>​<strong>验证接口兼容性</strong>​：核对接口能不能对接主流第三方系统，比如国家电网系统、微信/支付宝支付、高德/百度地图，确保能顺利对接。</li><li><p><strong>多测试不同类型接口</strong></p><table><thead><tr><th>接口类型</th><th>测试目的</th><th>测试示例</th></tr></thead><tbody><tr><td>设备对接</td><td>验证桩体数据能不能正常上传</td><td>桩体故障信息能不能实时同步到平台</td></tr><tr><td>支付对接</td><td>验证订单能不能正常结算</td><td>微信支付的订单能不能自动核销</td></tr><tr><td>数据对接</td><td>验证能不能和生态互通</td><td>能不能通过车企 API 获取充电需求数据</td></tr></tbody></table></li><li>​<strong>活用接口文档</strong>​：可以让厂商提供接口调用示例，但别只看文档，一定要实际测试，看接口调用的成功率。</li><li>​<strong>跟踪对接效果并优化</strong>​：关注接口调用的成功率、数据同步的延迟，测试不同的对接配置，找到最稳定的方式。</li></ol><blockquote>我的平台最初只对接了微信支付，后来对接了电网 API，能自动同步峰谷电价，单桩收益直接提了 15%。关键是要把接口对接当成生态布局的核心。——某充电运营商创始人</blockquote><p>接口拓展的核心是建立生态，只要专注对接核心伙伴，运营能力自然会提升。从小的开始，先对接 1-2 个核心系统，平台就能慢慢融入新能源生态。</p><h2>7. 验证系统稳定性与并发承载</h2><p>系统稳定性是充电桩高峰期运营的生命线，晚高峰、节假日充电的人多，系统稳不稳，直接影响用户体验和运营收益。</p><p>评估系统稳定性与并发承载，这么做：</p><ol><li><strong>选对测试方式</strong><br/>选和自己运营场景匹配的测试方法，比如：模拟晚高峰 1000+ 同时充电的并发订单测试；验证集群部署下，故障能不能自动切换的故障切换测试；测试设备掉线后，数据能不能重连补传的数据补传测试。</li><li>​<strong>说清测试需求</strong>​：准备一段简洁的测试需求，比如“模拟节假日 1000 个桩同时充电，验证订单结算准不准”，反复和厂商确认，确保测试场景贴合实际。</li><li>​<strong>全程参与测试</strong>​：别只当旁观者，全程跟着测试，看看订单处理速度、数据准不准，记录故障恢复要多久。</li><li>​<strong>索要测试报告</strong>​：尽量让厂商提供测试报告，哪怕是简化版的，也能了解平台的真实稳定性。</li><li>​<strong>参考同行经验</strong>​：看看其他运营商的测试案例，把有用的方法用到自己的选型中。</li></ol><p>一位充电运营商负责人分享了自己的经历：</p><blockquote>验证系统稳定性是我选型最正确的决定，模拟高峰并发后，发现某平台订单结算异常率达 3%，还好没选，不然上线后要损失几十万。</blockquote><p><strong>系统稳定性测试小贴士</strong></p><table><thead><tr><th>操作建议</th><th>建议原因</th></tr></thead><tbody><tr><td>模拟真实高峰场景</td><td>避免测试和实际运营脱节</td></tr><tr><td>测试时长 ≥24 小时</td><td>验证平台长期运行的稳定性</td></tr><tr><td>记录故障恢复时长</td><td>评估厂商的应急响应能力</td></tr></tbody></table><p>​<strong>跟进测试结果</strong>​：测试结束后，整理发现的问题，让厂商提供优化方案，确认后续的稳定性保障措施。</p><h2>8. 核查合规性与政策适配性</h2><p>合规是充电桩运营的底线，各地的能源、消防、数据监管政策不一样，平台能不能适配，直接关系到会不会被停运。</p><p>充分做好合规性评估，这么做：</p><ol><li><strong>主动核对政策要求</strong><br/>大部分运营商会忽略地方政策差异，但只有少数人会主动核对。一定要收集当地发改委、能源局、消防部门的监管文件，一条一条核对平台能不能适配。就这一个简单的动作，能避开 90% 的合规风险。</li><li><strong>索要合规证明</strong><br/>别只听厂商口头说合规，要让他们提供实际的合规证明，比如互联互通认证、数据安全备案，把这些放到选型清单里。这不是走形式，而是关键的风险保障，还能让物业、桩主这些合作方更信任你。</li><li><strong>按政策优化平台配置</strong><br/>认真读政策要求，然后让厂商落地到平台配置中。某运营商就是这么做的，顺利通过了地方能源局的合规检查，避免了停运整改。</li><li><strong>把合规要求纳入选型标准</strong><br/>政策要求的合规点，比厂商的任何宣传都靠谱。把这些要求当成核心选型标准，能快速筛选出合规的厂商，比如某运营商就把“数据上传至地方监管平台”作为硬指标，省了很多筛选时间。</li><li><p><strong>建立合规核查闭环</strong></p><table><thead><tr><th>步骤</th><th>具体行动</th><th>行动结果</th></tr></thead><tbody><tr><td>1</td><td>收集地方的政策文件</td><td>明确具体的合规要求</td></tr><tr><td>2</td><td>拆解合规的核心点</td><td>先核对风险高的条款</td></tr><tr><td>3</td><td>验证平台能不能适配</td><td>确认合规功能能落地</td></tr><tr><td>4</td><td>索要合规证明文件</td><td>留存好合规的依据</td></tr><tr><td>5</td><td>重复以上步骤</td><td>适配后续的政策更新</td></tr></tbody></table></li><li><strong>妥善处理合规争议点</strong><br/>有些政策的解读会有差异，遇到这种情况，要快速和厂商沟通，明确优化方案，等方案落地后，再重新验证合规性。</li><li><strong>细化合规适配场景</strong><br/>政策要求能帮你理清，平台到底要适配哪些合规场景。用这些信息完善合规评估清单，然后针对性核对平台能力。</li></ol><p>​<strong>重要提示</strong>​：合规性评估不只是核对条款，更是运营的风险防火墙，用好它，平台才能一直合规运营。</p><h2>9. 评估运维与技术支持体系</h2><p>运维和技术支持是平台长期稳定运行的保障，能降低后期的维护成本，让问题解决得更快。</p><p>评估运维与技术支持体系，这么做：</p><ol><li>​<strong>定核心评估维度</strong>​：比如响应速度、有没有本地化团队、版本怎么升级，不用贪多，专注评估 1-2 个核心维度就行。</li><li>​<strong>多次验证</strong>​：通过多次沟通、模拟故障报修，看看厂商的实际支持能力，真实的体验才最靠谱。</li><li>​<strong>用实操案例验证</strong>​：让厂商分享同类项目的运维案例，比如故障响应要多久、问题解决率多少，用案例评估更直观。</li><li>​<strong>了解服务短板</strong>​：不光要问厂商的服务优势，也要敢问短板，比如夜间响应会不会延迟，真实的服务能力才能帮你做出准确判断。</li><li>​<strong>模拟报修测服务</strong>​：假装设备出故障报修，看看厂商的解决方案怎么样，和客服沟通一下，实际感受服务质量。</li></ol><p>某物流园充电桩运营商就这么做的，模拟故障报修后，发现某厂商说的 7×24 小时支持，其实只覆盖工作时间，及时排除了这个选型风险。</p><p><strong>运维支持评估框架参考</strong></p><table><thead><tr><th>评估内容</th><th>评估目的</th></tr></thead><tbody><tr><td>响应时效</td><td>看看故障处理速度快不快</td></tr><tr><td>本地化团队</td><td>评估有没有现场支持的能力</td></tr><tr><td>升级机制</td><td>确认版本升级能不能灰度更新，不中断运营</td></tr><tr><td>运维培训</td><td>看看厂商会不会提供培训，降低自己的维护成本</td></tr></tbody></table><p>某运维经理提醒：</p><blockquote>如果运维支持只写在合同里，根本没用。你能明显感受到响应速度的差异，更想看到实际解决故障的能力，而不是漂亮的话术。</blockquote><p><strong>真实案例：运维支持的价值</strong><br/>一位运营商通过严格评估运维支持体系，得到了这些实实在在的好处：</p><ul><li>设备故障的响应时间缩短 60%，从 4 小时降到了 1.6 小时</li><li>运维成本降低 25%，通过厂商的培训，自己的团队就能排查基础故障</li><li>平台升级的中断时间从 8 小时降到 1 小时，因为有灰度更新机制</li></ul><p>精准评估，才能选到能长期支撑运营的运维支持体系。</p><h2>10. 核算成本与性价比模型</h2><p>成本和性价比决定了选型的投入产出比，核心是匹配自己的运营阶段，控制好长期投入。</p><p>和厂商核算成本，想实现性价比最大化，这么做：</p><p><strong>明确成本核算的要求</strong><br/>理想的成本模型，要满足这几点：报价清单清晰，把部署费、定制费、运维费分开；按模块收费，能按需选核心功能；后期升级、定制的计价方式透明；没有隐藏成本，比如服务器授权、接口调用费。</p><p><strong>选合适的核算方式</strong></p><table><thead><tr><th>核算模式</th><th>操作方式</th></tr></thead><tbody><tr><td>阶段成本核算</td><td>按 1 年、3 年、5 年，核算全周期的成本</td></tr><tr><td>模块成本核算</td><td>把核心模块和可选模块的成本分开算</td></tr><tr><td>对比性价比</td><td>按“功能-成本”，对比不同厂商的方案</td></tr></tbody></table><p><strong>实现高性价比的关键</strong><br/>提前明确成本核算的目标，和厂商协商好这些事：各阶段的成本构成，比如初期部署、中期定制、后期运维；成本上涨的条件，比如定制功能的数量、运维的次数；性价比的衡量指标，比如单个功能模块的成本、每年的运维成本。</p><p><strong>成本核算案例参考</strong><br/>2023 年，某充电桩运营商对比了 3 家厂商的方案，按“3 年全周期 + 核心模块”核算，最终选的厂商，比初始报价最低的那家，实际省了 18% 的成本，核心原因就是没有隐藏的接口调用费和升级费。</p><p>​<strong>打破成本误区</strong>​：某运营商创始人分享，初期只看报价高低，忽略了后期定制的计价方式，最后总成本超预算 40%；而按“阶段 + 模块”核算成本，能精准控制投入。</p><p>建议从核心模块的成本核算开始，比如先算设备管理、订单管理的成本，慢慢了解厂商后，再把定制、运维成本加进来核算。</p><blockquote>对于想控制成本的运营商来说，按阶段 + 模块核算性价比，能省不少钱。”——某充电运营公司财务负责人</blockquote><h2>总结</h2><p>指标已经给你了，现在就行动起来。</p><p>这 10 个指标不是空理论，而是经过验证的实战标准，已经帮很多运营商选到了适配的充电桩运营平台。</p><p>快速回顾核心要点：</p><ol><li>私有化部署与源码交付：一切的基础</li><li>定制开发灵活性：个性化需求的前提</li><li>多终端兼容能力：好用的全平台系统</li><li>核心功能完备性：支撑日常全流程运营</li><li>数据可视化分析能力：用数据做精细化决策</li><li>接口开放性与拓展性：未来生态发展空间</li><li>系统稳定性与并发承载：规模扩大的保障</li><li>合规性与政策适配性：避开监管和运营风险</li><li>运维与技术支持体系：降低维护成本</li><li>成本与性价比模型：合理的成本控制</li></ol><p>选充电桩运营平台，不只是选一个系统，更是选一个能支撑业务长期发展的伙伴。</p><p>现在就开始行动，选型先验证核心指标，再逐一核对其他维度，很快就能打造出适合自己的平台选型标准。</p><p>别等了，选充电桩运营平台，最好的时间是昨天，其次就是现在。</p><p>你的充电运营业务，值得配一个最契合的平台，赶紧去选出能支撑你发展的方案吧！</p>]]></description></item><item>    <title><![CDATA[电子签章在数字化进程中的占比重吗？ 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047606113</link>    <guid>https://segmentfault.com/a/1190000047606113</guid>    <pubDate>2026-02-11 18:05:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化发展的宏大图景中，电子签章并非占据最大“数量”份额的技术，但它扮演着至关重要的“关键路径和基石”角色。我们可以从多个维度来解析它的“占比”：</p><ol><li>从数字化进程的构成维度看：核心“赋能工具”和“最后一公里”</li></ol><p>1) 数字化发展可以分解为：</p><p>Ø 基础设施层（云计算、网络、数据中心）：占比最大，是“土壤”。</p><p>Ø 平台与数据层（中台、数据库、大数据、AI平台）：是“引擎”。</p><p>Ø 应用与业务层（各类软件、系统、流程）：是“果实”。</p><p>Ø 信任与合规层（身份认证、电子签章、数据安全、区块链）：是“规则和保障”。</p><p>2) 电子签章的定位：它横跨“信任与合规层”和“应用与业务层”。</p><p>Ø 占比特点：它的代码体量或直接产值在整体IT投资中可能只占很小的百分比（通常低于5%），但它赋能和撬动的业务流程数字化比例却极高。没有它，很多核心业务（如合同、订单、人事、贷款）的线上化就无法形成“闭环”，数字化进程会在“最后一公里”卡住。因此，它的战略价值占比远高于其经济成本占比。</p><ol start="2"><li>从市场渗透率和应用广度看：极高渗透的“关键节点”</li></ol><p>在几乎所有涉及签名、盖章的数字化场景中，电子签章都已成为标配或首选方案：</p><p>1) ToB（企业服务）领域：渗透率极高，接近必需。合同签署、订单确认、供应链协同、内部审批（如请假、报销）等，电子签章是核心节点。特别是在金融、房地产、人力资源、电商平台等行业。</p><p>2) ToC（消费者）领域：渗透率快速增长。在线开户、保险购买、租房合同、教育协议、政务办理等，用户已越来越习惯使用电子签名。</p><p>3) ToG（政务）领域：政策驱动，成为“数字政府”核心组件。“一网通办”、“不见面审批”等改革中，电子签章/签名是实现全流程在线、具有法律效力的必要条件，其在政务服务线上化流程中的占比几乎是100%（凡是需要签字盖章的环节）。</p><ol start="3"><li>从价值和影响看：具有“杠杆效应”的催化剂</li></ol><p>电子签章带来的价值放大了整体数字化的效益：</p><p>1) 效率杠杆：将数天甚至数周的签署周期缩短至几分钟，释放了巨大的人力和时间成本。</p><p>2) 成本杠杆：消除纸质文档的打印、邮寄、仓储和管理成本。</p><p>3) 风控与合规杠杆：提供完整的签署证据链，增强审计追踪能力，符合国内外多种法规要求。</p><p>4) 体验与协同杠杆：实现跨地域、全天候的无缝协作，提升客户和伙伴体验。</p><p>它的“占比”体现在：在已经数字化的业务流程中，它往往是提升最显著、投资回报率最高的环节之一。</p><p>5) 总结：</p><p>一个恰当的比喻，如果把数字化发展比作 “修建遍布全国的高速公路网”：云计算、网络是路基和路面（最大投资）。各种应用软件是行驶在路上的车辆。数据是运输的货物。而电子签章就像是高速收费站、交通规则和电子眼系统。</p><p>它的“占比”：从建设成本看，收费站和监控系统的造价在整个公路网投资中占比不高。但从功能性、必要性和对通行效率、安全秩序的保障作用来看，它是不可或缺的核心枢纽。没有它，高速公路就无法实现高效、合法、可追溯的运营。</p><ol start="4"><li>结论：</li></ol><p>电子签章在数字化发展的直接经济投入占比不大，但作为数字信任的基础设施和业务流程数字化的关键闭环工具，其战略重要性、场景渗透率和价值杠杆效应的占比极高。它已经从一项“可选项”转变为数字化深入发展的 “必选项”和“加速器” ，是衡量一个组织或社会数字化成熟度的重要标尺。目前市场电子签章产品比较全面、服务比较热诚、安全性比较高的厂商主要有：北京安证通、E签宝、法大大、契约锁等可给予大家选择。</p>]]></description></item><item>    <title><![CDATA[富滇银行基于 OceanBase 实现从TP到HTAP，百年“老字号”炼就数字引擎 OceanBas]]></title>    <link>https://segmentfault.com/a/1190000047606133</link>    <guid>https://segmentfault.com/a/1190000047606133</guid>    <pubDate>2026-02-11 18:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><strong><em>富滇银行面临竞争不利、数据库类型多而DBA稀缺的挑战，将数字化转型列为战略。其通过“滇峰计划”选用 OceanBase 作为核心数据底座，依托其 HTAP 能力实现 TP/AP 场景覆盖与技术收敛，兼顾稳定性、灵活性与成本效益。该合作达成零故障运行、交易性能大幅提升、运维简化的成效。</em></strong></p><p>富滇银行是一家拥有百年历史的“老字号”银行品牌，成立于 1912 年。2007 年 12 月，富滇银行在原昆明市商业银行基础上重组成立，成为云南省唯一的省属城市商业银行，资产总额 4300 亿元左右（2025 年上半年数据）。</p><p>作为一家中小银行，富滇银行深知自身在与大型银行的竞争中处于不利地位，因此很早就将数字化转型提升至战略高度。</p><p>2021年，富滇银行启动“滇峰计划”，全面推进数字化转型基础平台建设。在这一过程中，OceanBase 作为关键数据底座加入，为这家百年老字号银行的业务创新、服务拓展提供坚实支撑，助力其走出云南、迈向国际，积极融入“一带一路”建设。</p><h2>OceanBase 入选：滇峰计划的数字底座</h2><p>2025年10月18日，富滇银行新一代核心业务系统正式上线。不出意外，该系统继续选用 OceanBase 作为核心数据库。</p><p>这也是富滇银行对OceanBase过去几年表现的高度认可。</p><p>“我们从2022年开始用OceanBase，几年来，OceanBase一直保持着零故障的纪录。”富滇银行数据库负责人郝仕东表示。</p><p>这份信任也同步延伸至富滇银行的国际化布局中。由富滇银行与老挝外贸大众银行共同投资设立的合资银行老中银行，同样选择OceanBase：以OceanBase 2F1A+同城虚拟机备集容灾，4 台机器另加一台虚机实现了老中银行所有 IT 系统上线。这一跨境项目的成功，与国内新一代核心系统的平稳上线，共同印证了 OceanBase 作为富滇银行数字底座的坚实可靠性。</p><p>富滇银行的这份信心源于 OceanBase 的良好体验。2022 年初，作为“滇峰计划”的成果之一的新一代互联网核心系统上线，项目引入了包括 OceanBase 数据库在内的多项互联网主流技术，并将关键系统和业务逐步迁移至新一代分布式架构。</p><p>郝仕东表示，当时选择 OceanBase 是综合考量的结果。除了其在稳定性、高可用性及银行丰富场景中的成熟验证外，OceanBase 在性能、可靠性与成本效益上的综合优势，能显著提升银行的运营效率与服务能力。此外，OceanBase 还符合富滇银行对数据库“中立性”的要求。</p><p>郝仕东解释道：“数据库应是一个中立的产品。要做到中立，一方面要求行方具备自主掌控的能力，即能够自主运维、灵活部署，不依赖原厂；另一方面也要求产品方具备技术兜底能力，在出现问题时能快速响应。”</p><p>“滇峰计划”数字化平台上线后，OceanBase 表现亮眼。该平台每分钟交易量峰值达 50 万笔，提升 80 倍，核心交易平均处理时间低于 200 毫秒，单笔交易响应时间缩短一半以上。例如，用户画像处理从 T+1 提升至实时分析，业务响应效率显著增强。</p><h2>从 TP 到 AP：富滇银行能力再拓展</h2><p>富滇银行并未满足于 OceanBase 在事务处理（TP）领域的稳定表现，而是进一步挖掘其在分析处理（AP）领域的潜力，以此驱动业务创新。当然，这一探索也与该行统一的“技术收敛”战略紧密相连。</p><p>“我们前几年统计过，行里大大小小有接近 30 款数据库软件，但专业的 DBA 团队仅 3 人左右，想把每一款数据库搞深搞透非常困难，这给业务连续性带来了不确定性。”郝仕东道出了当时的困境。</p><p>为此，富滇银行制定了明确的技术路线图：将 TP、AP、实时数仓等场景，逐步集中到 OceanBase 这一款 HTAP 数据库上。</p><p>这一计划正稳步推进。据郝仕东介绍，目前富滇银行约 50% 的系统已运行在 OceanBase 之上。“我们的目标是，到 2027 年，全行 169 个系统中将有 85% 的数据库升级至 OceanBase。” </p><p>这一宏大的计划，彰显了富滇银行对技术栈进行统一治理的决心，也体现了对 OceanBase HTAP 能力的全面认可。</p><p>在 AP 场景的实践中，OceanBase 的 HTAP 能力的确发挥了关键作用。郝仕东指出，数字化转型后，银行的业务更注重数据驱动，比如，风控、审计、一表通等强监管系统都偏向 AP 处理。</p><p>以一表通业务为例。一表通要在可信区建设一套独立的分布式数据库，这个业务除了普通的 SQL 外，还有一些即席查询的 SQL，业务应用以 AP 为主同时兼顾 TP。富滇银行在测试对比了 SPARK 技术方案和 OceanBase 之后采用了后者，借助其 HTAP 能力，从而避免了专门为一表通系统建设一个分布式集群，节约了投资。</p><p>而且，OceanBase HTAP 支持行列混存对业务开发非常友好。他举例道，在总账系统中，多数查询是列存场景，但偶尔需要快速点查单条数据，行列混存模式就能兼顾两者需求。</p><p>另外，OceanBase 所具备的向量、文档等“多模”能力，也为未来进一步收敛其他特型数据库（如图数据库、向量数据库）奠定了基础，有望进一步降低运维成本与复杂性。</p><p>目前，富滇银行正将实时数仓作为 AP 能力建设的重点，已将审计、一表通、监管报送等系统纳入升级计划。郝仕东相信，随着技术收敛的推进，OceanBase 将以其统一的平台能力，为富滇银行的“数智化”升级提供更简洁、高效的数据支撑。</p><h2>AI 时代：数据底座的新使命</h2><p>面对 AI 时代的到来，郝仕东认为，数据库不仅要支持传统业务，更应成为 AI 系统的“燃料库”。</p><p>“即便未来代码全部由 AI 生成，数据仍是 AI 的‘燃料’，而 DBA 就是燃料的主理人。”他表示，富滇银行正积极推进 AI 布局，包括大模型建设、知识库构建与协同办公平台开发。新建系统普遍会加入“智慧”模块，依赖数据分析与智能决策，这些都离不开高性能、多模态的数据库支持。</p><p>目前，富滇银行在大模型场景中多采用私有化部署，服务商自带向量数据库。但郝仕东透露，未来有计划将向量、JSON 等能力集中至 OceanBase，进一步统一技术栈。</p><p>回顾合作历程，郝仕东总结了 OceanBase 带来的三大核心价值：</p><p>成本节约：OceanBase 支持通用服务器，具备 4:1 至 5:1 的高压缩比，显著降低存储与采购成本。同时，多模能力也减少其他类型数据库的采购与运维投入。</p><p>开发灵活：行列混存架构让业务开发可根据场景选择存储模式，兼顾点查与批量分析，提高了开发灵活性。</p><p>运维简化：统一的 OCP 运维平台集成监控、诊断、优化与恢复功能，提升系统可用性与运维效率。</p><p>郝仕东透露，接下来富滇银行将继续围绕 OceanBase 推进国产升级，将数据应用层（ADS）与服务层（DWS）全面升级至 OceanBase。</p><p>“OceanBase 每年推出一个大版本、数个小版本，迭代速度快，解决问题的能力令人惊喜。他们不怕事、不躲事，这种态度让我们对长期合作充满信心。”郝仕东表示。</p><p>对于 OceanBase 的未来，他期待其在 AI 技术加持下，进一步强化数据库的自适应与自治能力，成为更智能、更柔性的“数据工厂”，输出令用户惊艳的数据产品。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=umnP%2F4n8xDFlB%2BlTB2rVrQ%3D%3D.bcIe0u0lEKp%2BnoC46uluODS19mZbkmZ8wIrM9UUVS%2FM%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[IP数据云这类IP查询工具有什么特点？ 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047606138</link>    <guid>https://segmentfault.com/a/1190000047606138</guid>    <pubDate>2026-02-11 18:04:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>互联网业务中IP地址数据服务是构建精准定位、风险控制、运营优化等能力的重要基础。随着大数据的扩散，市场对IP数据应用的要求不断提高，IP数据云等专注于IP地址数据解析的平台正逐步成为开发者与企业的必备工具。本文将从<strong>IP地址定位精准度、数据维度、服务稳定性、更新频率等关键维度</strong>出发，全面分析这类工具有何特点。</p><h2>一、IP定位的精准度过渡到街道级</h2><p>IP数据云全球IP归属地产品在定位精准度不断进行提升。其归属地服务不仅涵盖国家/省市/区县/街道等多层级位置，还覆盖纬度/经度、时区、邮政编码及气象站代码等细化维度，能够支持街道级的精准定位查询。</p><h2>二、丰富的数据维度过渡到多场景数据支持</h2><p>目前来看单一的地理位置数据已无法满足现代业务需求。IP数据云在数据维度层面不仅覆盖传统的<strong>归属地信息</strong>，还提供包括：</p><ol><li>运营商/ASN号与行业类型信息；</li><li>IP宿主信息（所属机构/行业/商圈定位）；</li><li>IP真人标签、代理识别、风险画像与行为分析等扩展能力；</li><li>网络应用场景识别（例如企业专线、数据中心或普通家庭宽带区分）。</li></ol><p>通过“多维度+多业务面”的数据结构使IP地址完成地理标签、风险评估、精细营销定位、反作弊与用户分析等复杂业务逻辑构建。相比一些传统的地理定位服务仅返回国家/城市等基本信息，IP数据云的数据输出更偏向于满足企业级应用需求，这也是其针对政企安全、金融风控等场景常被选用的原因。</p><h2>三、更新频率——日更、周更、月更可自定义</h2><p>数据的时效性直接影响定位准确与业务效果。IP数据云支持日更、周更、月更的更新策略，并可根据业务需求定制更新频率，确保IP映射与属性数据与最新互联网环境相匹配。在行业层面，一些主流IP数据库通常也会采取定期更新策略。例如部分国际服务提供每日、每周或月度更新，以应对IP地址分配与地理变更的频繁动态。</p><h2>四、服务稳定性与可用性——高性能API与离线方案</h2><p>在性能与稳定性上，IP数据云提供1000次/秒级别的API响应能力，并支持多种语言SDK与离线库接入（包括CSV、TXT、定制格式等），为大量并发查询场景提供良好的响应保障。</p><p>与此类工具对比，一些开源或轻量级IP数据库虽支持本地部署，但是在高并发负载、扩展性与稳定性保障上往往需要开发者自行搭建缓存、负载等技术栈来支撑。因此，从企业级应用角度看，IP数据云的稳定性与易集成特性具有明显优势。<br/><img width="723" height="517" referrerpolicy="no-referrer" src="/img/bVdnUG4" alt="IP数据云这类IP查询工具有什么特点？.png" title="IP数据云这类IP查询工具有什么特点？.png"/></p><h2>五、适配多种业务场景</h2><p>单纯的地理位置查询已经不能满足当前互联网应用的多元需求。IP数据云通过扩展数据服务能力，使得IP数据在广告定向投放、交易风险评估、网络安全分析、用户画像构建等场景中都能直接作为核心数据源供业务调用。这一点也区别于部分只聚焦IP→基础位置映射的传统服务。</p><h2>总结</h2><p>整体来看，IP数据云这类IP数据服务工具在以下几个维度具备明显特点和竞争力：</p><ol><li><strong>精准度更高</strong>：细粒度地理定位能力支持街道级别解析；</li><li><strong>数据维度丰富</strong>：融合运营商、行业标签、风险属性及行为洞察；</li><li><strong>更新机制灵活</strong>：支持敏捷更新机制，提升数据时效；</li><li><strong>稳定性强</strong>：高并发API+离线部署满足不同业务规模；</li><li><strong>业务适配广泛</strong>：不仅限于定位，还可用于反欺诈、资产洞察等多场景。</li></ol><p>相比传统IP定位库，IP数据云更聚焦企业级复杂场景与数据洞察能力的构建，为决策、风控、运营优化等提供了一套完整的IP数据引擎解决方案。</p>]]></description></item><item>    <title><![CDATA[工业4.0转型中，哪些AI平台真正被国际认可？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047606144</link>    <guid>https://segmentfault.com/a/1190000047606144</guid>    <pubDate>2026-02-11 18:03:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、工业4.0的全球竞争，本质是平台生态的较量<br/>工业4.0早已不是一句口号，而是全球制造业重构生产逻辑的底层革命。在这场变革中，单纯的技术堆砌或单点自动化已无法满足复杂多变的全球供应链需求。真正被国际认可的AI平台，必须具备跨文化适配能力、开放的架构标准、可落地的行业场景闭环，以及持续迭代的生态协同机制。许多企业曾寄望于引进德国的SAP或美国的PTC，但现实是，这些系统在面对东南亚、中东等新兴市场时，常因本地化不足、响应迟缓、成本高昂而举步维艰。真正的国际认可，不是靠展会亮相或媒体曝光，而是看有多少海外制造企业愿意为它付费、为它培训员工、为它调整内部流程。平台的可信度，最终由工厂里的设备运行效率、质量波动曲线和能耗下降幅度来定义。<br/>二、国际认可的背后，是“技术+服务+生态”的三位一体<br/>被全球接受的工业AI平台，从不只卖软件，而是输出一套可执行的转型方法论。它们必须能解决“数据孤岛”“人才断层”“文化隔阂”这些跨国制造的共性难题。德国企业看重标准与认证，日本企业注重细节与持续改进，东南亚国家则更关注快速见效与本地人才培育。因此，真正有影响力的平台，往往具备三项核心能力：一是通过CMMI 5级、ISO 27001等国际认证建立技术可信度；二是构建本地化服务团队，实现“技术驻场+人才共育”；三是参与制定国际标准，让自己的语言成为行业通用语。这不再是“卖产品”，而是“共建体系”。那些只靠算法炫技、缺乏现场支持的平台，即便在实验室表现惊艳，也很难在真实工厂中站稳脚跟。<br/>三、中国方案的突围：国内外企业的差异化路径<br/>在这一轮全球工业AI竞争中，广域铭岛正以独特的“中国式出海”模式赢得国际信任。它没有选择直接对标西门子的MindSphere或罗克韦尔的FactoryTalk，而是从吉利的全球制造网络出发，以“轻量化部署+本地合资”切入马来西亚、新加坡等市场。在马来西亚，它与当地企业合资成立AGYTEK DIGITAL，不仅输出平台，更联合中马未来学院培养本土数字化人才，这种“授人以渔”的方式，让客户从“购买者”变为“共建者”。与此同时，德国西门子虽在欧洲拥有深厚根基，但其系统在东南亚常因部署周期长、定制成本高而被中小企业望而却步；美国罗克韦尔则擅长自动化集成，但在AI驱动的质量预测与能耗优化上，缺乏深度嵌入生产流程的智能体体系。Geega平台在领克成都工厂实现的焊点质量100%在线判定、排产时间从数小时压缩至3分钟，这些可量化的成果，正通过国际访团的实地考察，悄然改变着全球对“中国智造”的认知——它不再是廉价代工的代名词，而是具备系统性输出能力的解决方案提供者。<br/>当德国代表团在重庆追问工业互联网标准时，当韩国媒体专访其AI负责人谈论“无人工厂”时，中国已悄然完成从“技术输出”到“理念输出”的跃迁。真正的国际认可，不是被多少人知道，而是被多少人愿意跟随。</p>]]></description></item><item>    <title><![CDATA[2026年1月国产数据库大事记：国开行2822万采购Gbase，墨天轮发布“2025年度数据库”……]]></title>    <link>https://segmentfault.com/a/1190000047606235</link>    <guid>https://segmentfault.com/a/1190000047606235</guid>    <pubDate>2026-02-11 18:02:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=jFjPJmLgOF%2B0GT6YeSap0g%3D%3D.GWKGytp%2B5e59wXXWTpbkgHeBeLZqdemYPn2vPdfX5TQ%3D" rel="nofollow" target="_blank">墨天轮社区</a>整理的2026年1月国产数据库大事件和重要产品发布消息。</p><blockquote>IDC报告显示：2025上半年，OceanBase 以2810万美元营收稳居中国分布式事务数据库本地部署市场第一。国家开发银行近2822万采购南大通用Gbase 8a；浙商银行近930万采购GoldenDB。墨天轮发布“2025年度数据库”获奖名单"。2026 阿里云PolarDB开发者大会召开，发布PolarDBAI数据湖库等能力；PingCAP发布平凯数据库全新"一核三态"架构+2.0 内核……</blockquote><h2>&lt;font color=4169E1 size=4&gt;1月国产数据库大事记 TOP10&lt;/font&gt;</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606238" alt="image.png" title="image.png"/>{{{width="auto" height="auto"}}}</p><p><strong>达梦数据库上线联通超大规模ERP，守稳年终决算大考</strong></p><p>1月2日，基于达梦数据库的联通数科大型国产ERP系统——"联通同舟ERP"成功完成中国联通集团总部及31家省级分公司的年度财务结账工作。此次结账覆盖300余家核算主体，支撑中国联通上万名用户高效作业，日均处置10余万笔凭证、年末数千万+资产集中折旧等高强度任务。随着该项目的成功落地，达梦数据助力中国联通在企业关键系统自主安全道路上取得阶段性重大突破。</p><blockquote>目前，在达梦数据库的支撑下，中国联通全栈国产化"同舟ERP"已经覆盖了中国联通集团和31省分公司100%的业务场景，平稳承接接近20年全量历史数据资产，在年度结账、海量业务处理等关键考验中实现稳定运行。</blockquote><p><strong>能源首例！金仓助力中煤生产运营智控平台裸金属多租户数据库国产化落地</strong></p><p>1月3日消息，近日，中国中煤能源集团有限公司（简称“中煤”）在金仓企业级统一智控平台KEMCC的支撑下，成功上线了生产运营管控体系，涉及智控平台及其支撑的50+生产运营系统，成为能源行业首例裸金属多数据库实例多租户部署的国产化替换项目，拉通中煤的煤炭、电力、化工、销售等业务链条，为“煤与煤电”“煤电与新能源”联营提供数据支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606239" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>海量数据、达梦数据、万里数据库获评信创世界 “最佳信创数据库厂商”</strong></p><p>1月5日，“2025 XCWA信创世界年终大奖”正式揭晓，本届评选被誉为信创产业"奥斯卡"，吸引上百家企业逾200份申报材料，经30位权威专家评审产生最终名单。在<strong><em><em>数据库领域</em></em></strong>，<strong><em><em>海量数据、达梦数据库、万里数据库</em></em></strong>获评"最佳信创数据库厂商"，<strong><em><em>YMatrix超融合数据库、OceanBase</em></em></strong>获评"年度最佳信创分布式数据库厂商"。</p><p><strong>万里数据库与山石网科达成战略合作</strong></p><p>1月5日消息，近日，万里数据库与山石网科正式签署战略合作协议。双方将基于各自在数据库与网络安全领域的技术积累与市场优势，围绕数据库安全测试、安全能力共建、解决方案融合、市场协同等多个维度展开深度合作，携手打造更安全、更可靠的信创数据基础设施，助力我国信创产业实现高质量发展。</p><p><strong>Apache Doris 官网 Ask AI 智能问答已上线</strong></p><p>1月6日消息，Apache Doris 官网现已正式推出 Ask AI 功能。您可以直接输入问题，它将基于官方文档快速定位答案，并关联相关的内容片段，为您提供贴合场景的说明。此外，所有回答均附有准确的文档链接，方便您随时查阅与验证，可称之为学习和使用 Doris 的得力助手。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606240" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>IDC报告：OceanBase蝉联中国分布式数据库本地部署市场第一</strong></p><p>1月7日消息，近日，<a href="https://link.segmentfault.com/?enc=AM1mM7QGksLZ0aXlckAKLg%3D%3D.Isvj%2BwrE4LNx0FBHpJga%2BzlI3iXwUOQ9pdpgWK2fclIX4ki7IQ0lImyb7AP2ou90rdm9hoLo3wT3PCj1qFopBQ%3D%3D" rel="nofollow" target="_blank">全球权威机构 IDC 发布的《IDC中国分布式事务数据库市场追踪，2025H1》报告</a>显示，2025 上半年，<strong>OceanBase</strong> 以 2810 万美元营收，稳居<strong>中国分布式事务数据库本地部署市场第一</strong>。这是继 2024 年下半年后，OceanBase 连续两次在该细分市场拔得头筹。同时，在包含公有云的整体市场中，OceanBase 以 4060 万美元营收位列独立厂商第一、整体第四，持续领跑国产数据库阵营。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606241" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>IDC 统计，2025 上半年，中国分布式事务数据库市场规模达 4.2 亿美元，同比增长 19.6%。其中，本地部署市场增速高达 24.9%，显著高于公有云部署模式，预计 2024-2029 年复合增长率将达 24.2%。分布式数据库正加速向金融、政务等核心系统渗透，在性能、稳定性与综合成本上比肩甚至超越国际产品。IDC 同时认为，当前市场集中度持续提升，前五大厂商已占据 82.5% 的市场份额。</blockquote><p><a href="https://link.segmentfault.com/?enc=i8ja%2FEbYH%2FZUGGR3Ly9a2w%3D%3D.G2lqDGzjHcJzcIylUjAn%2FvBJanJbbID2ewKFTaUN6QUXQhcEvwlMgsQ79lJ3n%2BFhfzSdfhWX%2B5L771mrpGlrtA%3D%3D" rel="nofollow" target="_blank"><strong>墨天轮发布“2025年度数据库”获奖名单</strong></a></p><p>1月7日消息，墨天轮社区发布“2025年度数据库”获奖名单，<strong><em><em>OceanBase、阿里云PolarDB、达梦数据库</em></em></strong>荣获"<strong>最具影响力数据库奖</strong>"，其中OceanBase连续蝉联金融行业本地部署市场第一并发布AI原生数据库seekdb，PolarDB以TPC-C测试20.55亿tpmC性能及0.8元/tpmC性价比刷新世界纪录，达梦数据库在国产替代领域持续领跑；<strong><em><em>GoldenDB、腾讯云TDSQL、华为云GaussDB</em></em></strong>获"<strong>卓越表现数据库奖</strong>"，展现金融级分布式数据库的成熟商用能力；<strong><em><em>YashanDB、移动云He3DB</em></em></strong>获"<strong>最具潜力数据库奖</strong>"，代表国产数据库技术创新新势力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606242" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>国产芯 × 数据库，TimechoDB全球性能夺冠</strong></p><p>1月8日消息，近日，天谋科技基于 Apache IoTDB 开发的时序数据库 TimechoDB 与海光 C86 国产芯片、KeyarchOS 操作系统组合，在国际权威 TPCx-IoT 物联网数据处理性能基准测试中以 2465 万 IoTps 的速率夺冠，创下世界纪录。此次“国产 CPU+数据库”性能登顶，既彰显了国产算力在物联网时序数据处理领域从跟跑到领跑的历史性跨越，也呈现出 C86 与 TimechoDB 实现生态协同的创新成果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606243" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>TPCx-IoT 基准测试是全球公认的物联网数据处理能力权威标准，该测试主要模拟真实场景下大规模设备数据的采集、存储、查询与分析，对于算力性能和上层应用适配性要求严苛，其测试结果也被视为全球科技企业与行业用户的重要选型依据。</blockquote><p><strong>达梦数据入选“2025上市公司高质量发展优秀实践范例”</strong></p><p>1月8日消息，近期，由《大众证券报》主办的2025上市公司高质量发展优秀实践案例评选结果揭晓，武汉达梦数据库股份有限公司获评2025上市公司高质量发展优秀案例实践典范·创新发展优秀案例及投资者关系优秀典范双奖。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606244" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>国家开发银行2821.69万采购380个节点Gbase 8a MPP数据库+1年维保</strong></p><p>1月9日，国家开发银行发布《国家开发银行 一表通暨大数据服务平台建设项目（Gbase8a MPP数据库产品采购）结果公告》，由北京宇信科技集团股份有限公司中标，中标价2821.69万元。本项目需采购380个节点的Gbase 8a信创版本MPP数据库。供应商需具备Gbase 8a MPP数据库产品原厂授权资质，提供原厂服务完成各环境安装部署调试并配合提供开发支持,在系统全部上线后提供一年的原厂维保服务。</p><p><strong>金篆数据库GoldenDB助力广发证券科技柜台核心系统上线</strong></p><p>1月9日消息，近日，金篆数据库GoldenDB助力广发证券新一代经纪业务运营平台（以下统称为“NBOP”系统）完成全栈自主可控。NBOP系统是支撑全经纪业务运营的核心平台，服务全经纪业务条线超6000名业务人员的日常操作、超2000万名经纪客户的高频业务需求。新系统支持5000TPS下系统的平滑运行，整体性能较替换前显著提升！</p><p><strong>OceanBase DataPilot 获 Hugging Face DABstep 最高分！</strong></p><p>1月11日消息，OceanBase DataPilot 在被誉为“数据智能时代新基准”的 HuggingFace DABstep 基准测试 Hard 级别中获得全球最高分，并已连续 1 个月大幅领先第二名，位居全球第一。该⼯具旨在评估最先进的语⾔模型和 AI 代理在多步骤推理中的能⼒，特别是在数据分析领域的表现。</p><blockquote>DABstep 全球实时榜单：<a href="https://link.segmentfault.com/?enc=R%2FGAMJyg%2FsH3KrHc3yJpgg%3D%3D.qRA2GmiLMPvg7RQr2p7mJQ8GrLFi%2FAQSib9b%2B0DzEXxSgwMnnm1F50j0L4x7s2Kd" rel="nofollow" target="_blank">https://huggingface.co/spaces/adyen/DABstep</a></blockquote><p><strong>《向量数据库管理系统技术要求》团体标准正式发布</strong></p><p>1月13日，中国电子工业标准化技术协会正式发布T/CESA 1485-2026《向量数据库管理系统技术要求》团体标准，该标准将于2026年2月13日正式实施。该标准填补了向量数据库在向量数据类型、向量检索、向量数据查询和向量存储管理等方面的标准空白，为行业产品研发、评估选型与生态建设提供了关键依据。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606245" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>该标准由全国信标委数据库标准工作组（SAC/TC28/WG31）组织，星环信息科技（上海）股份有限公司、清华四川能源互联网研究院等单位参与制定。该标准界定了向量数据库管理系统的技术参考结构，规定了向量数据库管理系统的功能要求、存储管理、接口要求、系统管理和性能要求。适用于向量数据库管理系统的设计、开发、选型与检测。</blockquote><p><strong>YashanDB 2026城市行落地成都，与云和恩墨共推国产数据库一体机创新实践方案</strong></p><p>1月13日，由深圳计算科学研究院、崖山科技主办的“2026 YashanDB数据库城市行”新年首站在成都举办。现场，YashanDB与云和恩墨联合发布了“zData X for YashanDB数据库一体机解决方案”，该全栈信创方案以卓越性能与简化运维，为关键业务提供了高性价比承载选择。此外，YashanDB与海光信息、佰思杰等伙伴的联合解决方案也一同发布，展现了其在多元技术生态中的融合能力。同时，YashanDB为西南地区一批新晋生态伙伴举行了授牌仪式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606246" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>zData X for YashanDB数据库一体机解决方案"基于云和恩墨自研的数据库运行基础平台zData X，深度适配YashanDB V23版本，实现了从服务器、操作系统、数据库到管理平台软件的全栈信创兼容，涵盖海光、鲲鹏等国产芯片，麒麟、统信、openEuler等国产操作系统，构建了自主可控的技术底座。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606247" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}<br/><em>全栈信创的zData X for YashanDB一体机方案</em></p><p><strong>近930万！浙商银行GoldenDB数据库大单揭晓，索远电子脱颖而出</strong></p><p>1月14日消息，浙商银行发布《关于浙商银行2025年GoldenDB数据库软件授权与驻场服务采购的成交结果公告》，第一成交候选人为江苏索远电子科技有限公司，中标金额为9,288,596.40元。采购内容为GoldenDB数据库软件永久授权（除许可数量外，无其他限制，含一年原厂维保服务）。</p><p><strong>连获工信部赛迪认可！OceanBase 再入选“中国高质量软件及服务先锋榜”</strong></p><p>1月14日消息，近日，工信部赛迪顾问发布的《品质革命：2025中国高质量软件及服务系列研究》报告中，OceanBase 荣登“2025 中国高质量软件及服务先锋榜”，成为唯一上榜的国产分布式数据库厂商；同时，中国航信基于OceanBase打造的民航领域首批全栈国产离港系统也作为标杆案例入选报告，标志着OceanBase在金融、民航等关键领域的国产化替代能力获得权威认可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606248" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>赛迪顾问于2025年11月发布的《央国企软件应用市场研究报告》指出，从 2024 年的市场格局来看，央国企数据库市场各厂商不断突破发展，格局逐渐清晰。其中，海扬数据库 OceanBase 在发展能力和市场地位层面位列数据库市场本土厂商第一。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606249" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>2 家金融核心搭载 OceanBase 及一体机斩获“鼎信杯”大奖</strong></p><p>1月14日消息，中邮证券和金谷国际信托联合OceanBase申报的两个项目在第四届"鼎信杯"大赛金融赛道中双双斩获"金鼎实践奖"：中邮证券通过部署OceanBase数据库一体机（ODM）构建"三域一体"融合集群，实现TB级数据1小时极速切换、10:1数据压缩比及全栈极简运维；金谷信托则基于OceanBase分布式架构打造新一代核心业务平台，实现事务处理效率提升30%、每秒1000+笔高并发处理能力及99.99%核心服务可用性。</p><blockquote>截至目前，OceanBase已服务全部政策性银行、5/6国有大行及超100家千亿级银行，支撑190余个核心系统，连续两年位居金融行业本地部署市场第一。</blockquote><p><strong>第八届金猿奖-2025中国大数据产业「年度国产化优秀代表厂商」榜单/奖项发布</strong></p><p>1月14日，第八届金猿大数据产业发展论坛在上海举行，会上首次公布了“2025中国大数据产业年度国产化优秀代表厂商”榜单，宝兰德、传神语联、东方通、<strong>电科金仓、海量数据、极限科技、浪潮KaiwuDB、四维纵横、天谋科技、腾讯云、易捷行云、智臾科技（DolphinDB）</strong>、曙光存储等13家企业上榜，涵盖中间件、数据库、云计算、存储、AI大模型等基础软件领域，全面展现了国产软硬件在核心技术突破、全栈自主创新及关键行业替代中的最新成果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606250" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>DolphinDB荣获2025中国大数据产业年度「AI Infra领先企业」和「国产化优秀代表厂商」两项大奖，这些奖项不仅肯定了 DolphinDB 在 AI 基础设施建设中的技术领先性，也彰显了其在国产化替代与行业落地实践中的独特价值。</li></ul><p><strong>Milvus 2.6云上GA：三层存储降本85% 、速度快ES 4-7 倍</strong></p><p>1月15日消息，Milvus 2.6.x正式在Zilliz Cloud云上GA，通过三层分层存储架构（内存+本地SSD+对象存储）通过智能LRU预测将存储成本降低87%、计算支出减少25%，整体TCO接近S3水平；Index Build Level索引策略实现精度与成本自动平衡；新增地理空间、时区时间戳、INT8向量等数据类型；JSON Shredding与JSON Path索引让元数据过滤提速100倍；BM25全文搜索速度较Elasticsearch快4-7倍且支持关键词+向量混合检索……覆盖AWS、GCP、Azure、阿里云、腾讯云五大平台，成为全托管、生产就绪的AI应用开发平台。</p><p><strong>云和恩墨荣获超聚变2025行业贡献伙伴奖</strong></p><p>1月16日，2026超聚变四川伙伴大会在成都召开，近190家核心生态伙伴齐聚，共探智能体时代产业机遇，并对2025年做出突出贡献的渠道合作伙伴予以表彰。云和恩墨凭借与超聚变的深度协同创新，以及在多行业软硬件一体化方案的落地实践，荣获“2025行业贡献伙伴奖”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606251" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>方案中，超聚变FusionServer系列机架服务器作为“硬件基石”提供算力支撑；云和恩墨zData X多元数据库一体化承载平台则作为“软件大脑”，以全栈管理能力激活硬件潜能、运行数据库工作负载。双方联合打造的一体化解决方案，已在医疗、贸易、金融、能源等多行业核心业务场景落地。例如山东省某医院HIS系统、上海某贸易企业BI系统、贵州某证券公司OA/HR系统及某省电力营销2.0系统等项目。</blockquote><p><strong>正式获批！清华大学联合海量数据、清华工研院共建“数据智能北京市重点实验室”</strong></p><p>1月21日消息，清华大学联合海量数据、清华工研院共建的"数据智能北京市重点实验室"正式获批，由清华大学计算机系李国良教授担任主任。该实验室聚焦AI原生数据库、自主数据科学系统、可信数据空间三大方向，致力于构建安全可信、智能高效的新一代数据基础设施。</p><p><strong>时序数据库 Apache IoTDB 入选国家重点研发计划高新技术成果产业化试点</strong></p><p>1月22日，工业和信息化部正式公布《2025 年度国家重点研发计划高新技术成果产业化试点名单》，分布式时序数据管理系统 Apache IoTDB 作为相关技术成果之一入选。此次入选，反映了该技术成果在基础软件领域的持续研发积累，以及其在工程化与产业化应用方面形成的实践经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606252" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>国家重点研发计划重点资助事关国计民生的重大社会公益性研究,工业和信息化部最终确定 67 个试点成果与 108 个试点单位，为高新技术从“实验室”走向“产业化”铺就高速路。IoTDB 项目自 2011 年起由清华大学软件学院团队研发，2018 年开源加入 Apache 软件基金会，并于 2020 年毕业成为 Apache 顶级项目。2021年天谋科技成立，围绕 IoTDB 构建企业级国产信创产品与工程化交付体系，推动技术成果实际落地。</blockquote><p><strong>DolphinDB与DSG 达成生态合作，异构数据同步再添新选择</strong></p><p>1月22日消息，浙江智臾科技有限公司（简称：DolphinDB）和迪思杰（北京）数据管理技术有限公司（简称：DSG）近日达成深度合作，依托双方核心技术优势联合推出 Oracle、MySQL 等数据源到 DolphinDB 的专属实时数据同步方案，构建“数据高效流转-高性能分析”一体化能力，为金融、能源、工业制造等关键行业提供数据驱动决策新支撑。</p><p><strong>达梦数据与梦石科技达成战略合作，赋能自主医疗新生态</strong></p><p>1月22日消息，近日，达梦数据与梦石科技近日正式签署战略合作协议，双方将在产品整合、技术研发、市场拓展等领域深度合作，共同探索智慧医疗领域的创新应用场景，打造国产自主创新产业生态新标杆，助力医疗行业数字化、智能化升级。</p><p><strong>虚谷伟业荣获成都市信创密码 “2025 年度优秀合作伙伴”称号</strong></p><p>1月26日消息，虚谷伟业在成都市信创密码适配服务中心2025年度工作总结暨优秀合作伙伴表彰会上，凭借在信创密码产业生态建设中的深度协作与卓越表现，获评“2025年度优秀合作伙伴”，与华为、海光信息、麒麟软件等企业共同上榜，彰显了其在信创数据库领域的核心实力。</p><p><strong>赛迪顾问：达梦数据再获金融集中式国产第一，南大通用GBASE在金融行业云数仓市场占有率第一</strong></p><p>1月28日消息，近日，赛迪顾问发布《中国金融业数据库市场研究报告（2025）》。达梦再获中国金融行业集中式数据库国内厂商第一，并在银行、保险、证券三大子市场竞争象限中分别位列第一。这已是达梦连续2年斩获该领域桂冠。GBASE南大通用是唯一一家分布式与集中式数据库均位居金融业（包含银行业、保险业）领导者象限，同时取得金融业用户渗透率第一、云数仓市场占有率第一的“双第一”成绩。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606253" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>截至2025年底，达梦数据已服务超过260家金融机构客户，累计支撑超过2500套金融业务系统稳定运行，其中包括：银行业务系统1700余套，证券业务系统350余套以及保险业务系统450余套。目前，GBase数据库已覆盖金融主管单位、政策性银行、国有大行、股份制银行、城商行、农商行、农信社及保险、证券等各类机构270余家。</blockquote><p><strong>云和恩墨与崖山科技战略携手，多维协同共筑国产数据库创新生态</strong></p><p>1月28日，云和恩墨与崖山科技在北京正式签署战略合作协议，双方将在产品研发、市场开拓、客户服务及生态赋能等多维度展开全面协同，联合打造"Data+AI"融合解决方案，重点突破金融、政务、制造等关键行业核心场景，共筑国产数据库创新生态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606254" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><h2>&lt;font color=4169E1 size=4&gt;1月产品/版本发布&lt;/font&gt;</h2><p><strong>2026 阿里云PolarDB开发者大会召开，PolarDB发布AI数据湖库等产品能力</strong></p><p>1月20日，2026阿里云PolarDB开发者大会盛大召开，阿里云旗下云原生数据库PolarDB正式发布系列全新产品能力，包括AI数据湖库（Lakebase）、模型算子化以及面向Agent应用开发的托管能力等。与此同时，阿里云PolarDB首次阐释了“AI就绪数据库”的四大核心支柱，包括多模态AI数据湖库、高效融合搜索能力、模型算子化服务以及面向Agent应用开发的后端服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606255" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>目前，阿里云PolarDB海内外用户规模已超2万，部署规模超300万核，覆盖全球86个可用区。</blockquote><p><strong>2026 平凯数据库新品分享会举办，一核三态，重塑 AI 时代数据底座</strong></p><p>1月22日，平凯星辰举办"一源·三生·共进化"新品分享会，正式发布平凯数据库（TiDB企业版）全新"一核三态"架构——基于同一内核衍生出敏捷模式（存算聚合）、标准模式（3~∞节点存算分离）、聚能模式（存算聚合+亲和调度）三种部署形态，破解数据库选型"水平扩展、业务透明、极致性能"难以兼得的"不可能三角"，实现数据分布"可聚可散"的自适应能力。同时发布新一代内核及平凯数据库云服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606256" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>敏捷模式：专为 TB 级以下数据量及创新业务设计。敏捷模式仅需 1-3 个节点即可起步，不仅读写性能大幅优于 MySQL，压缩率更提升 3 倍以上，提供了优于单机主从架构的高可用能力，极大地降低了客户的试错成本与使用门槛。</li><li>标准模式：延续经典的存算分离架构，在水平扩展与业务透明性上保持业界标杆水准，完美适配数据量快速增长的成长型与核心业务场景。</li><li>聚能模式：专为对延迟极度敏感的场景打造。通过内存直连与亲和性调度等技术创新，将延迟降低至原来的 1/4，吞吐提升 2-3 倍，让客户无需牺牲分布式弹性即可享受单机般的极致性能。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606257" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>PingCAP新一代内核通过存算分离 2.0 架构，实现了对数据库内部模块的深度解耦与抽象。这一技术突破使得在线任务、离线计算与 AI 引擎（如向量、全文索引）之间能够实现“零干扰”的资源隔离。基于该内核的平凯数据库云服务将于 2026 年上半年正式推出。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606258" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><h2>相关资料</h2><ul><li><a href="https://link.segmentfault.com/?enc=C3DK1YxtNvHYiaR%2FFvbyxQ%3D%3D.N8z0%2BT%2BTKIAU9QtF7wt4a5pqw6j58vPzAeXHgAzo7e5a2SylYVnJ18nOV0WDrwdr" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜-2026年2月已更新</a></li><li><a href="https://link.segmentfault.com/?enc=2iNf2sHuGIIJjnUh5rcELA%3D%3D.Rsj7T7fbN2%2FR%2FS41gTB1D9xTEGdznQge0ELyAsLALbo%2FL9L5rrHG%2BHqTc4rMZvpj" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜规则解读</a></li><li><a href="https://link.segmentfault.com/?enc=Ep%2FCPBFFJWRhXq30q%2BILag%3D%3D.jBGmrdoszEbxPmKYxUUrvDcLS%2FM5NOW7lj7aAUlim2D9bC3Idx8mOCGw4W8Munm%2F" rel="nofollow" target="_blank">月度国产数据库大事记合辑</a></li><li><a href="https://link.segmentfault.com/?enc=ThaHqap63rIv%2FWmEmWGDtQ%3D%3D.s3JbSs3JMuLNrPRoyLylUzBpVxfzboKGpK%2BhqMoneaLIsV4%2BNV6VWexJcm4jOViJ" rel="nofollow" target="_blank">中国数据库排行榜 - 月度解读</a></li><li><a href="https://link.segmentfault.com/?enc=elyXRW1WLKD0ZckztSRvVg%3D%3D.pLD%2BbSIvy8FEy%2F3ZAlQGwBUo%2BngYQg2VOjZPE7phxnQ%3D" rel="nofollow" target="_blank">国产数据库招投标信息汇总</a></li><li><a href="https://link.segmentfault.com/?enc=xGPrqq1KveQ%2BwKc2zjTASg%3D%3D.5S2u0du7gWxPWJ1dGMwdtoYvJuXsJpVthErkPZVAC%2BnKX91LiS9XrOJKek2PXGTd" rel="nofollow" target="_blank">【合辑】2025年数据库厂商年终总结</a></li></ul><p>点击阅读原文：<a href="https://link.segmentfault.com/?enc=8XY%2FHlGGk%2BUvHevglPpiCQ%3D%3D.81LpGI1%2F%2F9wYAgaJC3MxWwl%2FuaT0ayKHCR2F7bOq1pZukH1Iu%2Fl1zDKPL81loJW4f5usDCOLbr4ymFGsFMt2%2Bg%3D%3D" rel="nofollow" target="_blank">https://www.modb.pro/db/2019292973163438080</a></p><hr/><p>欲了解更多可浏览<a href="https://link.segmentfault.com/?enc=EOOvQ50so86DoGKHaazaCA%3D%3D.3J0BST9xn2PuYD3aSzmxiLf1AfO11FeA60r8RlGRFWQ%3D" rel="nofollow" target="_blank">墨天轮社区</a>，围绕数据人的学习成长提供一站式的全面服务，打造集新闻资讯、在线问答、活动直播、在线课程、文档阅览、资源下载、知识分享及在线运维为一体的统一平台，持续促进数据领域的知识传播和技术创新。</p><p>关注官方公众号： 墨天轮、 墨天轮平台、墨天轮成长营、数据库国产化 、数据库资讯</p>]]></description></item><item>    <title><![CDATA[先建“语义基座”，再谈运维智能！阿里云以 Operation Intelligence 定义 AIO]]></title>    <link>https://segmentfault.com/a/1190000047606326</link>    <guid>https://segmentfault.com/a/1190000047606326</guid>    <pubDate>2026-02-11 18:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：鸢玮</p><p>大模型的出现，给许多行业带来了颠覆性的改变，运维这个向来被视为稳定、保守的领域也不例外。虽然“AIOps”这个概念早在 2016 年由 Gartner 提出，但早期的智能运维更多是利用大数据和机器学习对传统运维流程进行效率上的提升。十年后的今天，大模型的强大能力，正推动着 AIOps 从辅助工具，演进为数智化转型中不可或缺的核心基础设施，让运维真正迈入智能化的深水区。</p><p><strong>阿里云云原生应用平台事业部总经理、资深技术专家周琦</strong>作为这一变革的深度参与者，对 AIOps 的本质有着深刻洞察。“AIOps 这个词已经被广泛使用，但我更倾向于用 Operation Intelligence 来定义它。”周琦在采访中强调，“它的核心是发现与沉淀运维操作中的智慧，让工程师从重复繁琐的劳动中解放出来，聚焦于更高价值的创造。”</p><h2>十年演进，重塑 AIOps 底层逻辑</h2><p>在传统的运维时代，更多依赖人工被动处理故障，效率低下；而后进入到自动化运维时代，借助工具实现任务自动化，缩短了故障恢复时间；到了小模型运维时代，通过机器学习实现异常检测与根因分析，运维也初步具备智能化特征；如今进入到大模型时代，运维才真正开始走向真正的智能化。</p><p>回顾 AIOps 过去十年的发展，周琦认为有两个关键转折点重塑了其底层逻辑。<strong>第一个转折点是通用大模型的到来。</strong> 在此之前，所谓的智能运维更多是通过垂类 AI 模型来解决告警治理、异常检测等单一、点状的问题。这种方式虽然有用，但难以规模化。大模型的通用特性，像是一个巨大的杠杆，将 AIOps 的能力从“点状解决”扩展到“面状全域覆盖”，凭借其强大的泛化能力可以应对千变万化的碎片化运维任务。</p><p><strong>第二个转折点则在于数据整合技术的突破。</strong> 过去，运维工作呈现高度碎片化特征，数据和引擎往往由不同供应商提供，形成了天然的数据孤岛。周琦表示，想要建设统一的 AIOps 体系，首先就要跨过这道鸿沟。如今，存储、计算与分析技术的进步，实现了异构数据的关联与串联，将分散在各个系统中的数据整合在一起，为全域智能运维奠定了坚实基础。</p><p>技术的演进也推动了企业对 AIOps 认知的转变。周琦观察到，早期，企业引入 AIOps 的核心诉求只是保障系统的稳定性，关注的焦点集中在故障修复、告警处理等基础功能方面。但现在，企业的需求维度大大拓宽了，安全性、可扩展性、延时、用户体验等这些过去容易被忽略的“隐性成本”，正受到前所未有的关注。这种认知的升级带来需求的延伸，AIOps 不再仅是运维工程师的工具，还需要满足企业管理者对系统成熟度、跨模块依赖关系等深层因素的考量，真正覆盖多角色、多维度的运营需求。<strong>真正的 AIOps，不是让人去适应工具，而是让工具主动理解人、服务人、成就人。</strong></p><h2>能力跃迁，让系统“能感知、会思考、可行动”</h2><p>大模型时代的到来，让 AIOps 具备了前所未有的智能化能力。那么，大模型究竟为运维领域带来了哪些质变？周琦用一个生动的比喻来解释，给 AI 装上“摄像头”。传统运维在很大程度上依赖于工程师的个体经验，一位经验丰富的老师傅心中通常有一张无形的系统拓扑图，知道哪里容易出问题、该如何分析。但这种宝贵的经验附着于个体，难以沉淀、复制和规模化。大模型的出现，结合阿里云构建的实时数据采集与分析引擎，相当于为 AI 赋予了感知能力，使其能够真正能“看懂”系统、“理解”故障、“思考”方案。</p><p>这带来了运维能力的根本性跃迁。机器不再是机械地匹配预设规则、触发阈值告警，而是开始能够“读懂”告警信息背后的语义，“理解”系统当前真实的运行状态，甚至能“归纳”历史故障的复杂模式，并主动生成可供执行的修复建议。为此，<strong>阿里云提出 Operation Intelligence 理念</strong>，把人的经验变成系统的智慧，把个体的直觉转化为组织的资产，让系统具备“类人决策”能力，周琦将阿里云践行的 Operation Intelligence 理念概括为三个层面的能力进化。</p><ul><li><strong>在感知层面</strong>，目标是突破传统监控中常见的“数据孤岛”，构建从终端设备到业务流程的全链路感知网络。</li><li><strong>在认知层面</strong>，关键在于融合大模型的通用理解能力与专用领域算法，将海量、原始的观测数据转化为可解释、可推理的系统关系图谱。</li><li>最终，<strong>在行动层面</strong>，通过模型与算法的协同驱动，实现自动化的处置闭环，推动运维从“人工救火”向“系统自愈”转变，通过高效的人机协同大幅提升整体运营效能。</li></ul><p>当然，大模型并非万能，针对大模型“幻觉”问题，阿里云设计了一套双重保障机制。周琦介绍说，在技术层面，通过强化多源数据的交叉验证，将数据采集、清洗、预处理等基础但繁重的工作交由传统工具完成，让大模型聚焦在最核心的推理环节，从源头减少幻觉产生的可能性。在应用层面，系统支持企业外挂自身的私有知识库，利用行业或企业特有的领域知识来补充和修正通用大模型可能存在的认知盲区，确保建议的准确性与合规性。</p><h2>构建智能运维新范式，解放人力聚焦高价值</h2><p>理想与现实之间总是存在挑战。周琦坦言，阿里云在自身的大规模实践中深刻体会到两大核心难题。其一是数据层面的挑战，包括异构系统形成的数据孤岛、数据洪流带来的存储与算力压力。其二是认知层面的挑战，不同团队、不同系统之间存在的“语义鸿沟”，以及对系统拓扑、故障根因逻辑链的理解不一致问题。</p><p>为了系统性地解决这些问题，<strong>阿里云将内部的实践经验产品化，形成了一套帮助企业在大模型时代构建智能运维新范式，并且在可观测产品中落地。</strong></p><p>这套架构分为三层，底层是以<strong>日志服务 SLS</strong> 为核心引擎构建的统一可观测数据平台，实现日志、指标、链路、事件等多类型数据的统一接入与存储。该引擎具备 EB 级存储规模和秒级千亿行查询能力，能轻松应对每天数百 PB 数据，在保障数据完整性的同时，综合成本较自建方案降低 50% 以上。更重要的是，它支持全栈、实时、无侵入的数据接入，覆盖从移动端到基础设施的 200 多种组件，让企业无需重构现有系统即可完成数据整合。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606328" alt="image" title="image"/></p><p>中层通过 <strong>UModel 统一模型</strong>构建 IT 系统的 “数字孪生”，这是阿里云可观测性产品的核心建模框架。UModel 基于本体论，提供了一套观测实体及实体关系的定义，覆盖从用户体验、应用服务、容器到底层基础设施的每一层表征。UModel 就像给整个 IT 系统建立一套通用语言词典，让应用、容器、网络等不同组件能用同一套语义对话，彻底告别“你说你的指标，我说我的日志”的沟通困境。周琦表示，这套标准化建模彻底消除了语义歧义，让不同部门、不同系统之间的协作更高效，也让运维人员的经验得以沉淀为可复用的组织资产，而非随人员流动流失。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606329" alt="image" title="image" loading="lazy"/></p><p>上层则是以 AI Agent 为智能核心，实现“工具适应人”的新范式。Agent 采用自然语言交互方式，支持全场景上下文感知，用户可在任意界面随时召唤，直接通过自然语言提问，无需掌握复杂的查询指令。AIOps Agent 基于阿里云可观测平台的多源数据采集、存储、分析能力，采用“统一数据平台 + UModel + 传统算法 + 生成式 AI”的混合处理架构，能够自主规划、调用工具、执行分析并反思优化，可以提供从自然语言交互到自动化巡检的全流程运维辅助能力，解决各类开放和未知的运维难题，将运维人员从重复的查询、分析工作中解放出来。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606330" alt="image" title="image" loading="lazy"/></p><blockquote><strong>周琦形象地说，</strong> “希望运维未来可以高度自动化，让 AIOps 把那些又脏又累的活儿做了。”这意味着，企业客户无需再投入大量宝贵的人力资源去完成数据采集、清洗、对齐等基础且繁琐的工程工作，阿里云的平台已经将这些“隐形工程”承担下来。</blockquote><p>如今，阿里云 AIOps Agent 已在 6000 多家企业落地，帮助大型企业客户实现故障 MTTR 从小时级降至小于 15 分钟。</p><p>对于企业而言，部署 AIOps 的终极价值远不止于减轻运维团队的负担，而是它能释放出宝贵的研发与创新资源，让技术人才能够专注于业务价值创造。同时，它也能帮助企业系统性地管理那些以往容易被忽视的隐性成本与合规风险，从长远角度优化 IT 投资的整体回报。</p><h2>开源引领生态共建，推动“技术平权”愿景</h2><p>阿里云深知，“语义基座”的价值在于普及，而开源与生态建设是实现“技术平权”的关键，更能让全行业运维人员共同成长。为此，阿里云在开源布局、标准建设和生态协同上持续发力，推动 AIOps 行业整体进步。</p><p><strong>在开源布局方面</strong>，阿里云计划将 UModel 统一语义语言开源至社区，并向 OpenTelemetry 社区贡献了探针、采集器等核心工具。这些工具已被滴滴等公司开发人员广泛采用，大幅降低了行业重复开发成本。其中，无侵入探针的代码已开源在 GitHub 上，经过众多企业实战验证，在安全性和稳定性上备受认可，让中小企业无需自行研发即可获得高质量的数据采集能力。</p><p><strong>在标准建设方面</strong>，阿里云正在构建 AIOps 成熟度 Benchmark 榜单，构建了从数据分析到复杂异常检测的分级标准，涵盖基础任务处理、异常发现、根因分析、隐形问题挖掘、自主修复等不同阶段，让企业能够清晰评估自身能力水平，找到明确的进阶路径。周琦表示，希望可以和业界一起共创，攻克智能运维领域的难题，推动 AIOps 标准落地，促进整个可观测性领域的快速发展。</p><p><strong>在生态协同方面</strong>，阿里云通过大赛联动高校、企业，将工业界高频问题转化为赛题，促进产学研深度融合。通过大赛的方式，阿里云将标准 Benchmark 和真实场景赛题提供给参赛者，让高校学生、企业开发者都能在实战中提升能力，同时为行业贡献创新方案。</p><p>周琦表示，阿里云通过开放共建的模式，打破技术壁垒，让不同规模、不同行业的企业都可以落地 AIOps，实现“技术平权”，让中小企业也能调用顶级“隐形工程师团队”，让每个运维人员都能借助智能工具发挥更大价值，向“智能运营专家”演进。</p><h2>未来趋势：自主 Agent 协同，运维能力重构</h2><p>展望未来，周琦从不同时间维度来做出判断。短期来看，低风险任务将实现全自动化闭环，如 IP 封禁、简单扩容等操作可由 AI 自主完成，而重要操作仍保留人机协同决策模式，确保系统安全。同时，多角色 Agent 协同雏形将逐步显现，运维、安全、成本控制等不同领域的 Agent 将共享统一数据视图，提升跨域运营效率。</p><p>中长期来看，AIOps 将与 AI Coding、测试等环节深度打通，最终形成开发、测试到运维的全生命周期智能闭环。周琦解释道，AI Coding 目前在开发态做的非常有效，但从一个演示应用到企业级系统，部署后能稳定运行，还需要很长时间。“我们希望能够将 AI Coding 和 AIOps 串联，实现全局优化。让应用系统不光能跑起来，还能跑得更好、更稳，把运行态的状况实时反馈给 AI Coding。”</p><p>技术的演进必然带来运维人员角色与能力的重构。周琦表示，过去，运维人员是“救火队员”，整天忙于处理各类故障；未来，他们将转变为“系统教练”，而他们的核心能力不再是重复的操作经验，而是架构设计、业务理解、多维度决策等高阶能力。未来的运维人员需要平衡安全、成本、合规、可扩展性等多重诉求，专注于系统长期价值的优化。</p><h2>结语</h2><p>在阿里云可观测团队的定义中，智能运维是一场深刻的范式转移。它以大模型为驱动，基于统一的数据平台与领域知识模型，实现了从“人适应工具”到“将人类创造力注入系统智能之中”的本质转变，最终构建起数据、认知与行动闭环融合的智能体系。</p><p>纵观这场由 Operation Intelligence 引领的变革，其核心在于将运维智慧从依赖个人的隐性经验，沉淀为可复制、可迭代的组织数字资产，推动工程师从重复劳作中解放，实现价值的创造性升维。</p><p>阿里云始终致力于通过自身实践与生态共建，让任何规模的企业都能获得顶级“隐形工程师”团队的支持，在数智化浪潮中聚焦核心创造，实现个人与企业的共同成长。</p><p>正如周琦所言，“未来的运维竞争，将不再是工具的竞争，而是人的创造力与战略眼光的竞争”。当统一语言打通系统与智能的鸿沟，技术真正服务于人的价值释放，这场变革便不止于运维效率的提升，更将成为企业创新加速、行业持续进步的核心动力。</p>]]></description></item><item>    <title><![CDATA[指标平台选型的关键——无宽表下的查询性能如何保障？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047606360</link>    <guid>https://segmentfault.com/a/1190000047606360</guid>    <pubDate>2026-02-11 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=d5QFwngLGJ4RAE55Ruxo3g%3D%3D.GX5W52w1rsVRJvds98Fvm4USHWsf4SvQ6VmT5LLCTbsDnGU9UDPlj9NAahJ2nfkV2EoDBbIv75sE7O03nsDm7TLPHjfp1IUWd%2BQ2A8y7NcJGU8ja4Wh7cU1BDqzdIlgSLnHIoH%2BPnUCNu0dDphuivQ%3D%3D" rel="nofollow" target="_blank">《指标平台选型关键：Aloudata CAN 如何保障无宽表下的查询性能》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：本文深入探讨在数据工程中，企业进行指标平台选型时面临的核心挑战——如何在摒弃物理宽表、保障业务灵活性的同时，实现海量数据下的高性能查询。文章系统分析了自研高性能指标平台必须跨越的三大技术难关：统一语义解析、智能物化加速与开放生态适配，并对比了基于 NoETL 语义编织技术的 Aloudata CAN 如何通过声明式策略与自动化引擎，实现百亿级数据秒级响应的成熟方案，为技术决策者提供清晰的 Build vs Buy 评估框架。</p><p>许多企业在指标平台选型时，往往陷入一个根本性的认知误区：认为这仅仅是选择一个“指标字典”或元数据目录，用于记录和查询指标定义。这种认知导致许多自研项目停留在构建一个静态的 CRUD 系统层面。</p><p>然而，一个真正能够支撑企业级数据分析的指标平台，其核心是一个 动态智能计算引擎。它不仅要“记住”指标的定义，更要能“理解”复杂的业务语义，并“自动执行”从 DWD 明细数据层到最终指标结果的复杂计算过程。性能保障，尤其是高并发、复杂查询下的秒级响应能力，是这一引擎的基石，而非附属功能。</p><p>“在高并发、高吞吐量的数据分析场景下，简单的事情往往变得不那么简单。一个业务逻辑简单的指标大盘，一旦面临大促或年终数据汇总等高峰期，就会出现卡顿甚至崩溃的情况。” —— 从传统 Cube 到现代化指标体系：物化视图驱动的指标平台升级之路</p><h2>技术难关一：统一语义解析——性能的“第一道坎”</h2><p>自研平台面临的第一道坎，是如何将业务人员定义的指标（如“华东区上月高价值用户的日均消费金额”），精准、高效地解析为底层数据引擎可执行的查询计划。这一步直接决定了查询性能的基线。</p><ul><li>静态解析的局限：传统模式依赖预建的物理宽表或 Cube。查询路径被固化，一旦业务需求超出预建模型的范围（例如，需要按新的维度组合下钻），要么无法响应，要么需要 DBA 重新开发宽表，周期以周计，完全丧失灵活性。</li><li>动态解析的挑战：要摆脱宽表束缚，必须构建强大的 语义引擎 (Semantic Engine)。它需要在逻辑层构建一个“虚拟业务事实网络”，通过声明式策略定义表间关联。当用户组合指标与维度时，引擎必须实时进行语义推导，生成最优的、包含正确关联和聚合逻辑的 SQL。这涉及到复杂的查询优化、谓词下推、公共子表达式识别等数据库核心技术，工程复杂度极高。</li></ul><h2>技术难关二：智能物化加速——性能保障的“工程黑洞”</h2><p>即使解决了语义解析，面对百亿级明细数据的即席查询，性能依然难以达标。此时，物化加速（预计算）成为必选项。然而，这正是自研与采购成熟方案的核心分水岭，一个深不见底的“工程黑洞”。</p><ol><li>手动模式的困境  <br/>传统做法是 DBA 手动创建和维护物化视图或汇总表。但这带来两个致命问题：</li></ol><ul><li>灵活性丧失：为每个可能的查询组合创建物化视图，会导致“物化视图爆炸”，存储成本失控。</li><li>运维成本爆炸：需要手动管理刷新策略（全量/增量）、处理数据一致性、监控任务失败。当源表数据发生更新或删除时，增量计算的逻辑复杂性呈指数级上升。</li></ul><p>“影响增量计算性能的因素极为复杂，包括查询算子组合复杂度、源表数据变化模式（Append Only vs Update/Delete）、变化频率等。能应对各种业务场景的多方面因素是一个极具挑战的工程难题。” —— 破解千亿数据处理痛点：快手基于增量计算解决时效、成本</p><ol start="2"><li>智能物化加速引擎：从“人治”到“自治”  <br/>真正的性能保障，需要一套 声明式策略驱动的智能物化加速引擎。这正是 Aloudata CAN 的核心壁垒。</li></ol><ul><li>声明式策略：用户无需关心具体物理表，只需在界面声明对哪些高频查询的“指标+维度”组合进行加速，以及期望的刷新时效（如 T+1 或准实时）。</li><li>自动化执行与运维：系统根据策略自动编排物化任务，智能选择生成明细加速、汇总加速或结果加速表，并负责全生命周期的调度、监控、失败重试和血缘管理。</li><li>智能路由与透明加速：查询发生时，语义引擎会自动进行 SQL 改写，将查询智能路由到已存在的最优物化结果上，对用户完全透明。</li></ul><p>权威背书：某全球连锁餐饮巨头（麦当劳中国）基于 Aloudata CAN，在 百亿级数据规模 下，实现了核心业务查询 P90 &lt; 1s 的性能，日均支撑百万级 API 调用，覆盖 30+ 业务场景。</p><h2>技术难关三：开放生态适配——性能服务的“最后一公里”</h2><p>即使计算引擎性能卓越，若无法被业务系统便捷消费，就会形成新的数据孤岛。自研平台需要设计一套标准、稳定、高性能的服务接口，并确保在所有消费端指标口径绝对一致，这同样是巨大的工程投入。</p><ul><li>接口标准化：需要提供标准的 REST API 和 JDBC 接口，以适配企业内部多样的 BI 工具（如 Tableau、Power BI）、AI 应用及自建业务系统。</li><li>性能与稳定性：接口层本身不能成为性能瓶颈，需要处理高并发、连接池管理、查询超时与熔断。</li><li>生态集成：与主流 BI 工具（如 FineBI、Quick BI）的深度集成，以及提供像 WPS 插件这样的办公场景嵌入能力，都需要长期的研发和合作投入。</li></ul><h2>TCO 分析：自研性能保障的“隐形高利贷”</h2><p>当企业决定自研以攻克上述“鬼门关”时，必须算清一笔总拥有成本（TCO）的“隐形账”。</p><table><thead><tr><th>维度</th><th>自研路径</th><th>采购 Aloudata CAN</th></tr></thead><tbody><tr><td>初期投入</td><td>高。组建专项团队（架构、研发、测试），至少 6-12 个月起。</td><td>低。主要为软件许可和实施服务成本。</td></tr><tr><td>三年 TCO</td><td>极高。持续研发、性能调优、运维人力成本叠加，且存在项目失败风险。</td><td>可控。固定许可费+运维服务费，无额外研发人力负担。</td></tr><tr><td>性能达成时间</td><td>长。从零到一构建稳定、高性能的引擎，通常以“年”为单位。</td><td>短。开箱即用，在客户环境中已验证的架构，可快速达到 SLA。</td></tr><tr><td>团队技能要求</td><td>极高。需要顶尖的数据库内核研发、查询优化、分布式系统人才。</td><td>中。侧重业务建模与配置，无需深入引擎底层。</td></tr><tr><td>业务风险</td><td>高。研发周期长，可能错失市场时机；系统不稳定直接影响业务决策。</td><td>低。基于成熟产品，风险可控，可快速赋能业务。</td></tr></tbody></table><p>核心结论：自研高性能指标平台是一笔长期的“技术高利贷”，其隐形成本（机会成本、人才成本、时间成本）往往远超采购成熟方案。作为 Gartner 中国数据编织代表厂商，Aloudata CAN 将经过验证的语义编织与智能物化加速能力产品化，让企业能将稀缺的研发资源聚焦于业务创新，而非重复造轮子。</p><h2>决策矩阵：何时该自研，何时该采购？</h2><p>企业应根据自身情况，参考以下框架决策：</p><p>1、坚定选择自研：</p><ul><li>拥有顶尖的数据库内核研发团队，且将“高性能计算引擎”作为核心战略产品。</li><li>业务场景极为特殊，市面上没有任何方案能满足其定制化需求。</li><li>对技术掌控有绝对要求，且不计较长期投入和成本。</li></ul><p>2、强烈建议采购（如 Aloudata CAN）：</p><ul><li>核心目标是快速解决业务的数据分析需求，提升决策效率。</li><li>数据规模已达亿/十亿级，且对查询性能（秒级响应）有明确要求。</li><li>缺乏或难以组建具备数据库内核研发能力的团队。</li><li>希望统一企业指标口径，并面向多种 BI 工具和 AI 应用提供一致服务。</li><li>关注总体拥有成本（TCO），希望将 IT 投入转化为明确的业务价值。</li></ul><h2>常见问题（FAQ）</h2><h4>Q1: 不建物理宽表，如何保证复杂查询的秒级响应？</h4><p>Aloudata CAN 通过 NoETL 语义编织 技术，在逻辑层构建“虚拟业务事实网络”。同时，其 智能物化加速引擎 支持声明式物化策略，自动为高频查询组合生成并维护最优的物化结果（明细加速、汇总加速）。查询时，语义引擎自动进行 SQL 改写和智能路由，透明命中物化结果，从而实现百亿级数据 P90 &lt; 1s 的查询性能。</p><h4>Q2: 与传统数据库的物化视图相比，Aloudata CAN 的物化加速有何不同？</h4><p>传统数据库物化视图需要 DBA 手动创建、维护和选择刷新策略，是“手动优化”。Aloudata CAN 的物化加速是 声明式、自动化 的。用户只需关注业务逻辑（定义指标），系统根据查询模式自动决策物化什么、如何物化、何时刷新，并负责全生命周期运维，实现了从“人治”到“自治”的跃升，大幅降低使用和维护门槛。</p><h4>Q3: 选择指标平台时，除了查询性能，还应重点评估哪些方面？</h4><p>除了查询性能，还需重点评估：1) 指标定义与管理能力：是否支持复杂业务逻辑（如指标转标签、自定义周期）；2) 口径一致性：是否为企业提供唯一可信指标源；3) 开放性与集成能力：是否提供标准 API/JDBC，支持各类 BI 工具和 AI 应用；4) AI 原生适配：是否具备 NL2MQL2SQL 等能力，根治 AI 问数幻觉；5) 总拥有成本(TCO)：包括采购成本、实施效率与长期运维开销。</p><h4>Q4: 我们的数据量不大，也需要考虑这种无宽表的指标平台吗？</h4><p>即使当前数据量不大，采用无宽表的现代化指标平台（如 Aloudata CAN）也具有战略价值。它能帮助企业在数据增长初期就建立 统一的指标语义层和治理规范，避免未来因口径混乱、宽表泛滥而导致的“先乱后治”高成本重构。同时，其敏捷的配置化开发模式，能极大提升数据响应速度，赋能业务创新，是一种面向未来的“弯道超车”架构选择。</p><h2>核心要点</h2><ol><li>性能是引擎，不是功能：现代指标平台的核心是一个动态智能计算引擎，性能保障是其基石，选型时需首先评估其在高并发、复杂查询下的能力。</li><li>智能物化加速是分水岭：手动创建和维护物化视图/宽表无法平衡灵活性与性能。真正的解决方案是基于声明式策略的智能物化加速引擎，实现自动化运维与透明路由。</li><li>开放生态是价值出口：平台必须通过标准接口（API/JDBC）无缝集成现有 BI 与 AI 生态，避免成为新的数据孤岛，确保性能价值直达业务。</li><li>自研 TCO 远超想象：自研高性能指标平台涉及长期的研发、调优和运维投入，其总拥有成本（TCO）和机会成本往往被严重低估。</li><li>采购成熟方案是高效路径：对于绝大多数企业，采购像 Aloudata CAN 这样经过大规模实践验证的成熟方案，是快速获得高性能、统一语义层并控制总成本的最优路径。</li></ol>]]></description></item><item>    <title><![CDATA[快速上手：Chrome/Firefox/Edge 浏览器 Canvas 指纹防护实战 ToDetec]]></title>    <link>https://segmentfault.com/a/1190000047605695</link>    <guid>https://segmentfault.com/a/1190000047605695</guid>    <pubDate>2026-02-11 17:13:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在日常上网过程中，你可能听说过“浏览器指纹检测”这个概念，但具体它是怎么工作的，尤其是“浏览器Canvas 指纹”，很多人还是一头雾水。</p><p>今天，就给大家聊聊什么是 Canvas 指纹，它对隐私的威胁，以及如何在 Chrome、Edge、Firefox 上有效防护，顺便分享一些实用工具和操作技巧。</p><h3>什么是浏览器Canvas 指纹？</h3><p>简单来说，浏览器Canvas 指纹是浏览器指纹检测的一种方式。它通过浏览器渲染一段隐藏的图形（Canvas），然后读取渲染结果的像素信息。每台电脑、每个浏览器的硬件和软件环境不同，比如显卡型号、操作系统字体、驱动版本等等，这些细微差异会让 Canvas 渲染出的图像“唯一化”，从而生成一个“指纹”，可以用来追踪你的上网行为。</p><p>也就是说，即便你关闭了 Cookie，网站仍然可以通过 Canvas 指纹识别你是不是同一位访问者。听起来有点吓人对吧？这也是很多隐私保护爱好者特别关注的点。</p><h3>浏览器指纹检测的危害</h3><p>浏览器指纹检测不仅用于广告追踪，还可能被用来：</p><p>精准识别用户身份，即便切换 IP 或隐身模式也能追踪</p><p>个性化广告投放，让你的隐私被过度利用</p><p>防止访问某些网站或限制功能，比如一些服务会根据指纹限制访问次数</p><p>所以，了解 Canvas 指纹的工作原理，并学会防护，确实很必要。<br/><img width="723" height="485" referrerpolicy="no-referrer" src="/img/bVdnUzU" alt="" title=""/></p><h3>如何在 Chrome/Edge/Firefox 上防护 Canvas 指纹</h3><ol><li>使用隐私浏览器或增强隐私插件</li></ol><p>Firefox：Firefox 对隐私保护比较友好，可以在 about:config 中开启 privacy.resistFingerprinting，它会主动对 Canvas 指纹进行干扰，降低被唯一识别的风险。</p><p>Chrome / Edge：可以安装类似 uBlock Origin、Privacy Badger 的插件，有些插件提供 Canvas 指纹保护功能，会在 Canvas 渲染请求时提示你是否允许。</p><p>温馨提示：完全屏蔽 Canvas 指纹可能会导致某些网站功能异常，比如图形验证码或绘图功能。建议按需开启。</p><ol start="2"><li>修改浏览器 Canvas 行为</li></ol><p>部分浏览器插件可以随机化 Canvas 指纹，或者在读取 Canvas 数据时注入“噪声”，从而干扰指纹生成。例如：</p><p>CanvasBlocker（Firefox / Chrome）：这是一个专门防护 Canvas 指纹的插件，可以随机化你的 Canvas 输出，阻止网站准确识别你的浏览器。</p><p>Trace（Chrome / Edge）：提供多种防护选项，包括 Canvas、WebGL 和字体指纹保护。</p><p>通过这些插件，你可以在保证上网体验的前提下，有效降低 Canvas 指纹被利用的风险。</p><ol start="3"><li>使用隐身或隔离浏览模式</li></ol><p>虽然隐身模式不能完全防止 Canvas 指纹，但结合插件使用，可以大幅降低追踪成功率。此外，多账户浏览器或容器插件（Firefox 的 Multi-Account Containers）也可以隔离网站数据，避免跨站点追踪。</p><ol start="4"><li>检测你的浏览器指纹安全性</li></ol><p>防护前，最好先知道自己的浏览器有多“容易被识别”。ToDetect指纹查询：</p><p>查看自己浏览器的 Canvas 指纹信息</p><p>检测是否存在其他指纹威胁（WebGL、字体、插件等）</p><p>评估当前防护措施的效果</p><p>操作也很简单，只需访问网站，点击检测即可生成报告。这样你可以直观了解防护是否成功。</p><h3>总结</h3><p>Canvas 指纹是现代浏览器指纹检测中一个比较精准的手段，它能在无 Cookie 情况下追踪用户。想要在 Chrome、Edge、Firefox 上防护 Canvas 指纹，主要方法就是：</p><p>使用隐私浏览器或增强隐私插件</p><p>随机化或干扰 Canvas 输出</p><p>使用隔离浏览或容器模式</p><p>借助 ToDetect指纹查询工具 评估防护效果</p><p>如果你平时比较注重隐私，上述方法结合起来使用，会显著降低浏览器被跟踪的概率。毕竟，保护自己的数字足迹，是每一个现代网民都该掌握的技能。</p>]]></description></item><item>    <title><![CDATA[别再折腾配置了！OpenCloudOS推出OpenClaw“极速版”脚本 OpenCloudOS ]]></title>    <link>https://segmentfault.com/a/1190000047605697</link>    <guid>https://segmentfault.com/a/1190000047605697</guid>    <pubDate>2026-02-11 17:12:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在上篇文章（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）中，我们介绍了如何基于OpenCloudOS 9 安装配置OpenClaw，并接入企业微信等IM，让你最终拥有一位 7x24 的“AI全能助理”。一些用户看完文章后跃跃欲试，但一上手实操，却被繁琐的配置劝退了：</p><p>● “Node.js 版本不对，报错了……”</p><p>● “GitHub 连不上，插件装不下来……”</p><p>● “我想接企业微信，怎么还得手动改配置文件？”</p><p>大家的痛点，OpenCloudOS社区听到了！</p><p>为了让大家把精力从“装环境”转移到“用 AI”上，带来了 OpenClaw x OpenCloudOS 2.0 部署方案 。这一次，我们推出了一键安装脚本，用户只要一条命令就能极速体验OpenClaw。</p><h2><strong>一、 新版“极速部署脚本”做了什么？</strong></h2><p>1. 环境全自动适配 ：自动检测系统环境，帮你搞定 Node.js 24 等所有底层依赖，不再担心版本冲突。</p><p>2. 国内 IM 原生支持 ：不再需要满世界找插件。 企业微信、QQ、飞书、钉钉 ，你想用哪个，脚本直接帮你装好。</p><p>3. 网络与源优化 ：针对国内网络环境做了深度适配，下载更稳、速度更快。</p><p>简单来说：以前需要 30 分钟的手工操作，现在只需要 1 分钟等待。</p><h2><strong>二、极速版上手指南</strong></h2><h4><strong>2.1 安装 Openclaw 及IM相关插件</strong></h4><h5><strong>场景A：我全都要（推荐）</strong></h5><p>如果你希望 OpenClaw 能连接企业微信、QQ、飞书、钉钉等所有国内主流 IM，可直接运行如下脚本。</p><pre><code class="auto"># 默认完整安装所有国内 IM 插件
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash</code></pre><p>备注：因一键安装脚本会执行较多依赖并启动OpenClaw安装，所以整个安装过程大概耗时15-20min左右，中途请不要终止或推出。</p><p>一键安装脚本代码请见：<a href="https://link.segmentfault.com/?enc=onCDwfn1UjcIq9qOH7y77A%3D%3D.xblJS0304C9yxtL%2BL40pBCjtwJbentI9eUkRk%2BE4ACam%2BYv7MeEFtuWqqG%2FUQiy1mdjnL%2FdiUMWT0NPsdyUxadAsi5g2%2FO898BjgFonMhZY%3D" rel="nofollow" target="_blank">scripts/deploy\_opneclaw.sh · OpenCloudOS/web-extra - Gitee.com</a></p><h5><strong>场景B：我只需要特定渠道</strong></h5><p>如果你只想安装某个指定IM（如企业微信和QQ），不想安装多余插件，可直接运行如下脚本。安装耗时：5-10min</p><pre><code class="auto"># 安装指定IM（下方以同时安装企业微信和QQ为例）：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --plugins wecom,qq</code></pre><h5><strong>场景C：纯净版安装</strong></h5><p>如果你只需要 OpenClaw 的核心功能（比如只在终端使用，或者通过 Web 界面交互），不需要连接任何国内IM，可直接运行如下脚本。安装耗时：2-3min</p><pre><code class="auto"># 跳过所有 IM 插件安装：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --skip-plugins</code></pre><pre><code class="auto"># 手动打开交互命令
openclaw onboard</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605701" alt="image.png" title="image.png"/></p><h4><strong>2.2 配置 OpenClaw</strong></h4><p>OpenClaw配置流程较多，OpenCloudOS已在上篇内容（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）进行了详细展示，这篇不再赘述。唯一不同的是，如您在2.1章节中执行了指定IM安装（如企业微信、飞书等），则会在IM配置环节的列表中，看见该可选项。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605702" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>2.3 OpenClaw 运行状态确认</strong></h4><pre><code class="auto"># 查看openclaw是否在后台运行
openclaw health
# 查看模型状态，是否连上了大模型
openclaw models list
# 查看聊天通道,比如qq，企业微信等
openclaw channels list</code></pre><h2><strong>三、接入IM及实际应用演示</strong></h2><p>接下来，我们将详解如何配置企业微信、QQ、飞书、钉钉.</p><h4><strong>3.1 接入企业微信</strong></h4><p>OpenClaw 原生基本只支持国外社交软件，可以通过插件的方式来支持国内的社交软件。这里我们以企业微信为例，演示接入教程。</p><p>注意要接入企业微信有两个条件，首先你的clawdbot安装在有公网ip的机器上，2.你是企业管理员能创建APP或者机器人</p><pre><code class="auto"># 查看企业微信插件运行是否加载
openclaw plugins list | grep -i wecom</code></pre><p>企业微信插件使用目录</p><p><a href="https://link.segmentfault.com/?enc=M4unXfzxKC1wlWK0ec%2BXtA%3D%3D.bkn4FrxKLgQfkD83StSJsO9DYhe6Qa8Nv5sAwEOTSj%2FqAJYNKGq8XjjAieQedzYRJqDoQES%2FUTAQiyWh1imRfg%3D%3D" rel="nofollow" target="_blank">@marshulll/openclaw-wecom - npm</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605703" alt="image.png" title="image.png" loading="lazy"/><br/>接下来需要在企业微信里创建一个一个应用，这一步需要<a href="https://link.segmentfault.com/?enc=w24hTWU05IaZHoxFDYy7jA%3D%3D.1BTne%2FRQWyv5XVwdV2Tfkp8T4vhsgSPJgrDJNELX5u1SnYu7xQEoFMAKep0bUhkJ" rel="nofollow" target="_blank">首页 - 企业微信开发者中心</a>先在这里创建一个应用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605704" alt="image.png" title="image.png" loading="lazy"/><br/>选择个人<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605705" alt="image.png" title="image.png" loading="lazy"/><br/>配置企业微信应用相关信息</p><p>首先获取如下信息</p><p>1.  登录 <a href="https://link.segmentfault.com/?enc=Ka0Sfy1zvkOThQOkLrExXA%3D%3D.hYiLdHJNfU3Oo%2FNXMunamF56dyNJnhnjvp6PkmpefJI2%2F8tDMMrJfBxObQ1NgRp7" rel="nofollow" target="_blank">企业微信管理员后台</a></p><p>2.  在"我的企业"中查看 企业ID (CorpID)<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605706" alt="image.png" title="image.png" loading="lazy"/></p><p>3.  进入"应用管理" → 选择或创建应用</p><p>4.  在应用详情页获取：AgentId：应用ID</p><p>Secret(corpsecret)：点击"查看Secret"获取<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605707" alt="image.png" title="image.png" loading="lazy"/></p><p>5.  在"接收消息"设置中获取：Token：点击"随机获取"</p><p>EncodingAESKey：点击"随机获取"<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605708" alt="image.png" title="image.png" loading="lazy"/><br/><strong>然后在部署了openclaw的服务器上输入如下命令：</strong></p><pre><code class="auto"># 企业微信应用配置（必需）
# 这里配置的是 app 模式，可以参考插件使用指南换成bot或者both模式
openclaw config set channels.wecom.mode "app"
openclaw config set channels.wecom.defaultAccount "app"
openclaw config set channels.wecom.accounts.app.mode "app"
openclaw config set channels.wecom.accounts.app.webhookPath "/wecom/app"
openclaw config set channels.wecom.accounts.app.corpId "你企业ID"
openclaw config set channels.wecom.accounts.app.corpSecret "应用secret"
openclaw config set channels.wecom.accounts.app.agentId "你的应用ID"
openclaw config set channels.wecom.accounts.app.callbackToken "你设置的应用的token"
openclaw config set channels.wecom.accounts.app.callbackAesKey "你设置的应用的aes-key"
openclaw config set channels.wecom.enabled true
 
# 设置openclaw链接公网
openclaw config set gateway.bind lan
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p>配置应该是这样的，我只配置了app，如果你配置了bot会更丰富一点<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605709" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605710" alt="image.png" title="image.png" loading="lazy"/><br/>如上执行后，回到企业微信app管理界面，点击保存，企业微信会回发送token和AESKey去和openclaw服务器进行匹配<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605711" alt="image.png" title="image.png" loading="lazy"/></p><p>如果匹配成功界面如下<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605712" alt="image.png" title="image.png" loading="lazy"/><br/>在企业微信里找到相关应用，直接和他聊天<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605713" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到 Clawdbot 确实识别到了相关的用户和请求<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605714" alt="image.png" title="image.png" loading="lazy"/><br/>让 ClawdBot 创建一个定时任务：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605715" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到确实创建完成了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605716" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.2 接入QQ</strong></h4><p>QQ更方便个人用户使用，OpenCloudOS也提供一个接入QQ的场景。</p><p>QQ插件地址<a href="https://link.segmentfault.com/?enc=SJ6GDjY%2FbiFNnizbhJjIow%3D%3D.wpMB9sbXKjNGQvEyZZyoBwtWRX8g3VJKFWb0Rruk34K5fyxqK25EpOVq%2FPnxNuYR" rel="nofollow" target="_blank">https://github.com/sliverp/qqbot#</a></p><pre><code class="auto"># 一件安装脚本已经安装了qq相关插件
# 查看当前的脚本
openclaw plugins list | grep qq
 
# 如果你开始没安装qq插件可以执行如下命令安装
openclaw plugins install https://github.com/sliverp/qqbot.git</code></pre><p>创建QQ机器人：</p><p>访问 <a href="https://link.segmentfault.com/?enc=jjyNDI%2Fqn%2F5s3fR8naIHYw%3D%3D.RiV9oCLjjuUluVMSUBPRbV9dFZX5J4z2OrqiewlgD30%3D" rel="nofollow" target="_blank">QQ 开放平台</a></p><p>创建机器人应用</p><p>获取 AppID 和 AppSecret（ClientSecret）</p><p>Token 格式为 AppID:AppSecret，例如 102146862:Xjv7JVhu7KXkxANbp3HVjxCRgvAPeuAQ<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605717" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605718" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">#方式一：交互式配置,选择 qqbot，按提示输入 Token
openclaw channels add
#方式二：命令行配置
openclaw channels add --channel qqbot --token "AppID:AppSecret"
# 示例
openclaw channels add --channel qqbot --token "102146862:xxxxxxxx"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605719" alt="image.png" title="image.png" loading="lazy"/><br/>配置好后在qq开发平台里的，沙箱配置里先点击添加成员再扫描二维码就能和ClawdBot沟通，并安排他工作了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605720" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605721" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.3 接入飞书</strong></h4><p>飞书插件地址</p><p><a href="https://link.segmentfault.com/?enc=34Ee5DR7U2AFtU9R3bTG1Q%3D%3D.k0YXbPbj4ccvLi8igNVRuZa1vm2keV7XD93FagP5O%2BFU2SeOcuV2JBuKPfLsZ%2B2v" rel="nofollow" target="_blank">GitHub - m1heng/clawdbot-feishu</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable feishu
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install @m1heng-clawd/feishu</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605722" alt="image.png" title="image.png" loading="lazy"/><br/>飞书应用（机器人）配置</p><p>进入飞书应用中心：<a href="https://link.segmentfault.com/?enc=2s4OZ%2BC23MertWnPl7gzHg%3D%3D.JmoZSJtUuLQQSDFj6Te%2BfphdTp%2FsyvEEjTuyp64PhF4%3D" rel="nofollow" target="_blank">开发者后台 - 飞书开放平台</a></p><p>创建企业自建应用</p><p>路径： 创建应用 → 企业自建应用</p><p>  基础信息按提示填写即可（名称、描述等），完成后进入应用详情页。</p><p>配置应用权限</p><p>进入 权限管理，添加以下权限（按插件文档要求）：</p><p>必要权限</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im:message</td><td>消息</td><td>发送和接收消息</td></tr><tr><td>im:message.p2p_msg:readonly</td><td>私聊</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im:message.group_at_msg:readonly</td><td>群聊</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im:message:send_as_bot</td><td>发送</td><td>以机器人身份发送消息</td></tr><tr><td>im:resource</td><td>媒体</td><td>上传和下载图片/文件</td></tr></tbody></table><p>可直接复制如下配置直接导入</p><pre><code class="auto">{
  "scopes": {
    "tenant": [
      "bitable:app:readonly",
      "contact:user.base:readonly",
      "docx:document",
      "docx:document.block:convert",
      "docx:document:create",
      "docx:document:readonly",
      "drive:drive",
      "drive:drive:readonly",
      "im:chat:readonly",
      "im:message",
      "im:message.group_at_msg:readonly",
      "im:message.group_msg",
      "im:message.p2p_msg:readonly",
      "im:message:send_as_bot",
      "im:resource",
      "wiki:wiki:readonly"
    ],
    "user": [
      "contact:contact.base:readonly"
    ]
  }
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605723" alt="image.png" title="image.png" loading="lazy"/><br/>进入凭证与基础信息 页面，记录 App ID / App Secret 同步更新到 openclaw 配置中<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605724" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">openclaw config set channels.feishu.appId "你的appid"
openclaw config set channels.feishu.appSecret "你的app_secret"
openclaw config set channels.feishu.enabled true
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605725" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605726" alt="image.png" title="image.png" loading="lazy"/><br/>然后进入事件与回调界面，订阅方式选择长链接<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605727" alt="image.png" title="image.png" loading="lazy"/><br/>之后点击右下角的添加事件</p><p>添加如下事件</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im.message.receive_v1</td><td>接收消息（必需）</td><td>发送和接收消息</td></tr><tr><td>im.message.message_read_v1</td><td>消息已读回执</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im.chat.member.bot.added_v1</td><td>机器人进群</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im.chat.member.bot.deleted_v1</td><td>机器人被移出群</td><td>以机器人身份发送消息</td></tr></tbody></table><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605728" alt="image.png" title="image.png" loading="lazy"/></p><p>之后点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605729" alt="image.png" title="image.png" loading="lazy"/><br/>在飞书里直接搜索 你机器人的名字就能和他聊天了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605730" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.4 接入钉钉</strong></h4><p>钉钉插件地址</p><p><a href="https://link.segmentfault.com/?enc=mgfhDzZJ7kmqByBVMBhYUA%3D%3D.jJIjBFGmfKFbLPY%2BG8bZIswSWbfroMwQyn5CZfwtR61nGTFNxTnNnVGSmmePWMJSDmSucGhxYZCp2KzRdUkh6Q%3D%3D" rel="nofollow" target="_blank">GitHub - soimy/openclaw-channel-dingtalk: A dingtalk bot channel plugin for clawdbot</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable dingtalk
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install https://github.com/soimy/clawdbot-channel-dingtalk.git</code></pre><p>前往<a href="https://link.segmentfault.com/?enc=5Tq1vpHrgRs8uYSokC0G5w%3D%3D.5EcpqxB%2FvjaCw26FCO90oUK0%2BNw9gI3fjYDoRmRy0a8%3D" rel="nofollow" target="_blank">钉钉开发者后台</a>，使用具有管理员权限的账号进行登录。选择</p><p>创建应用-&gt;填写应用名称 和 描述 -&gt; 再点击左侧“添加应用能力” -&gt; 选择 “机器人"-&gt;完善机器人配置 -&gt; 消息接受模式选择 stream模式 -&gt; 点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605731" alt="image.png" title="image.png" loading="lazy"/><br/>然后点击坐上角的 "凭证与基础信息" 找到Client ID与Client Secret两个参数<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605732" alt="image.png" title="image.png" loading="lazy"/><br/>然后再进入发布界面，点击保存<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605733" alt="image.png" title="image.png" loading="lazy"/><br/>然后在服务器上输入</p><pre><code class="auto">openclaw config set channels.dingtalk.clientId "你的ClientID"
openclaw config set channels.dingtalk.clientSecret "你的Client Secret"
openclaw config set channels.dingtalk.enabled true
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels.dingtalk</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605734" alt="image.png" title="image.png" loading="lazy"/><br/>接着你就可以在钉钉里搜索到你的应用并让他给你干活了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605735" alt="image.png" title="image.png" loading="lazy"/></p><p>参考链接</p><p><a href="https://link.segmentfault.com/?enc=3J%2BLGc0g4i3s9rOIm7gvAA%3D%3D.fpPlpSbELX1EFKt8LdhwEjpo6g7pf7Kx3UYnPGAszlZNEM%2FcOZofpa9Hw9aq9eXL" rel="nofollow" target="_blank">Node.js — 下载 Node.js®</a></p><p><a href="https://link.segmentfault.com/?enc=ClwCQ9mfBGQ%2BiwlZrGLRrQ%3D%3D.dFjgb0mKz6hxzaoEkHv60qC1XRaG7WNC7OrNUIIrl%2B4%3D" rel="nofollow" target="_blank">Moltbot — Personal AI Assistant</a></p><p><a href="https://link.segmentfault.com/?enc=QTKRY%2FAt0me0xOKdVE0%2F%2Fw%3D%3D.DRh5uGmND4NkEP59iOmxvO6CX8NB6hOYJdNtOn5nHWSro37GR9dS7jMUzu5vgFVM2rPjx8szL%2Fs1VMFmXLq%2BWQ%3D%3D" rel="nofollow" target="_blank">openclaw企业微信插件</a></p><p><a href="https://link.segmentfault.com/?enc=GK1N58jdxtA7rus14uH8VQ%3D%3D.LVPjTp%2B535z5rWMPTQdSx2wu4yNB7s64%2BxkLJiLZ1XA%3D" rel="nofollow" target="_blank">MoltHub</a></p><p><a href="https://link.segmentfault.com/?enc=%2FPy2DIg98xZcZDy9cT50qg%3D%3D.MlelPgfy3upjMHOwA%2FAdW%2B2xtIczDDhqlWQCarmNDDzbIXe7egJ1W1ZI1GPAebq%2F" rel="nofollow" target="_blank">https://linux.do/t/topic/1518570</a></p><p><a href="https://link.segmentfault.com/?enc=mketfBKmN7BOaXbI7k1FLw%3D%3D.RLQL0IOI2wHdPR1%2FGjFfP1aZZvXYUIz5y%2F%2FeE7t8ziRe%2BSbdC1pO8oSMDFBbRC9FCEy0u9DcpXGWKv422ESKfw%3D%3D" rel="nofollow" target="_blank">🚀 云上Moltbot（原Clawdbot）最全实践指南合辑-腾讯云开发者社区-腾讯云</a></p><p><a href="https://link.segmentfault.com/?enc=2Cf9uABs%2BjPGPIYdupiHqg%3D%3D.P7bbZOBVuc7Jn1Ek3LffY6TJoJX0gouBub%2FxR8S8adnBtBw5chqBtRHlQ2hsJnTw" rel="nofollow" target="_blank">openclaw的QQ机器人插件</a></p><p><a href="https://link.segmentfault.com/?enc=U5v7oM84FdPCDcgQK2UjyA%3D%3D.ekOuV%2BmnJn30a8VV1hF8r5XgHYnscHBoAu%2FNt0jUufkFhRjPEpsiWsrgJOh2Sr5gx7LSeYUwCTpRgKPy17m8PA%3D%3D" rel="nofollow" target="_blank">Clawdbot 全面指南 - 汇智网</a></p>]]></description></item><item>    <title><![CDATA[推理速度 10 倍提升，蚂蚁集团开源业内首个高性能扩散语言模型推理框架 dInfer 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047605777</link>    <guid>https://segmentfault.com/a/1190000047605777</guid>    <pubDate>2026-02-11 17:11:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>蚂蚁集团开源业界首个高性能扩散语言模型（Diffusion Large Language Model，dLLM）推理框架 dInfer。<br/>在基准测试中，dInfer 将 dLLM 的推理速度相比于 Fast-dLLM 提升了 10 倍以上，并在关键的单批次（batch size=1）推理场景下，作为首个开源框架实现了大幅超越经过高度优化的自回归（AR）模型的性能里程碑，在 HumanEval 上达到 1011 tokens / 秒的吞吐量。dInfer 通过一系列算法与系统协同创新，攻克了 dLLM 的推理瓶颈，兑现了其内生并行生成带来的推理效率潜力。<br/>这不仅为开发者提供了即刻可用的高效推理框架，更标志着扩散语言模型这一全新的范式迈出了走向成熟的坚实一步。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605779" alt="图片" title="图片"/></p><ul><li>论文链接: <a href="https://link.segmentfault.com/?enc=Bd9ZlJwd0CVaI3YPoYKltA%3D%3D.ylZYHI2SwHdO%2BC9%2Ft7yOEo9Q%2BUzzS9bIT36fiMq4aMxytTi%2F2eUzW7YazI7s3ym2" rel="nofollow" target="_blank">https://arxiv.org/abs/2510.08666</a></li><li><p>项目地址: <a href="https://link.segmentfault.com/?enc=oPjzL0zsP5o5XRvZmWPwIw%3D%3D.CRR099b3Pbx6BNoxHARCwd%2FQWvLOS%2FfpPoHJAjZSPv4VutrEaNtPo7OB7oe5P%2FDH" rel="nofollow" target="_blank">https://github.com/inclusionAI/dInfer</a> </p><h2>理论的「翅膀」，现实的「枷锁」：扩散语言模型的推理困境</h2><p>近年来，以自回归（Autoregressive，AR）范式为核心的大语言模型（Large Language Models）已经取得了巨大的成功，推动了智能问答、代码生成、智能体助手等领域的重大进步。然而，AR 生成范式也存在其固有瓶颈：生成过程完全依赖前序结果，必须逐词串行生成，这导致推理延时难以降低，即使 GPU 的并行计算能力强大也无用武之地。<br/>作为一种全新的范式，扩散语言模型（dLLM）应运而生。它将文本生成视为一个 「从随机噪声中逐步恢复完整序列」的去噪过程。这种模式天然具备三大优势：</p></li><li>高度并行：理论上可以在单次迭代中，并行地预测和更新序列中的多个 token</li><li>全局视野：模型的每一步决策都基于对整个序列的全局上下文理解，而非仅依赖于已生成的部分</li><li>结构灵活：更易于适应多模态、代码生成等需要复杂结构和长程依赖的任务<br/>凭借这些优势，以 LLaDA-MoE 为代表的 dLLM 已在多个基准测试中，展现出与顶尖 AR 模型相媲美的准确性 。然而在推理效率方面，dLLM 理论上的强大潜能，却长期被残酷的现实「枷锁」所束缚。dLLM 的高效推理面临三大核心挑战：<br/>1.高昂的计算成本：多步迭代去噪的特性，意味着模型需要反复对整个序列进行计算，这带来了巨大的算力开销<br/>2.KV 缓存的失效：dLLM 中的双向注意力机制，使得 token 对应的 KV 值在每次迭代中都会改变。这导致 AR 模型中「一次计算、永久复用」的 KV 缓存技术直接失效，使得推理过程异常昂贵<br/>3.并行解码的双刃剑：尽管理论上可以并行生成序列中的所有 token，但在难以精准刻画其联合概率分布的情况下一次性解码太多 token，极易引发彼此间的语义错配，导致「并行越多，质量越差」的窘境</li></ul><p>这些瓶颈使得 dLLM 的推理速度一直不尽人意，其并行生成带来的效率沦为「纸上谈兵」。如何打破枷锁，释放 dLLM 在推理效率的潜能，成为整个领域亟待解决的难题。</p><h2>dInfer：人人可上手的扩散语言模型高效推理框架</h2><p>为彻底突破上述瓶颈，蚂蚁集团推出了 dInfer—— 一个专为 dLLM 设计的、算法与系统深度协同的高性能推理框架，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p><p>dInfer 的设计哲学是模块化与可扩展性，以系统性集成算法与系统优化。如下图所示，dInfer 包含四大核心模块：模型接入（Model）、KV 缓存管理器（KV-Cache Manager），扩散迭代管理器（Iteration Manager），和解码策略（Decoder）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605780" alt="图片" title="图片" loading="lazy"/><br/>图 1. dInfer 架构</p><p>这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测。更重要的是，dInfer 针对上述三大挑战，在每个模块中都集成了针对性的解决方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605781" alt="图片" title="图片" loading="lazy"/><br/>表 1. dInfer 组件</p><h2>dInfer 如何「快」起来？</h2><p>1.削减计算成本，控制生成质量：邻近 KV 缓存刷新 (Vicinity KV-Cache Refresh)<br/>dLLM 使用双向注意力机制让模型获得更全局的视野，代价是每次解码会影响所有的 token 的 KV 值，导致 AR 模型依赖的 KV 缓存技术不能直接应用到 dLLM 上。如果不使用任何 KV 缓存，在一个 sequence 上的一次 diffusion 迭代会导致大量的计算。</p><p>为了削减计算成本，Fast-dLLM 提出的将 sequence 划分为 block，然后再逐个对 block 进行解码，并在当前解码 block 之外进行 KV 缓存的方法，可以有效降低 diffusion 迭代的计算成本。然而虽然利用上了 KV 缓存，但在大部分情况下，缓存中的 KV 实际上是过时的，因此会导致生成质量的下降。</p><p>为了缓解这一问题，dInfer 采取了一种邻近刷新的策略：KV 缓存过时的原因是 dLLM 中一个新 token 的确定，会影响全局所有 token 的 KV 表示。而 dInfer 基于「语义局部性」原理（ 一个词的更新，对其近邻词的影响最大），在每次迭代解码一个 block 时，dInfer 只选择性地重新计算该区块及其邻近一小片区域的 KV，而让远处的缓存保持不变。这好比修改文档中的一句话，你只需检查上下文是否通顺，而无需重读整篇文章。<br/>这种策略结合 dInfer 的其它优化，在计算开销和生成质量之间取得了平衡，首次让 KV 缓存机制在 dLLM 上高效、可靠地运作起来。</p><p>2.系统优化：让 dLLM 的前向运算速度追上 AR<br/>在利用上 KV 缓存之后，dInfer 选择了合适的 block 大小和 Vicinity KV-Cache Refresh 的范围，并做了一系列的系统优化，以使 dLLM 一次迭代的速度能追上运行在 SOTA 的推理服务框架如 vLLM 上的 AR 模型，包括：</p><ul><li>多卡并行：结合了张量并行 (TP) 与专家并行 (EP)，即使在 batch size=1 的条件下，也能充分利用 GPU 的算力，效率提升超 100%。</li><li>编译优化：通过 torch.compile 进行内核融合并编译为 CUDA Graph 执行，消除了 PyTorch 框架的执行开销，结合上述的多卡并行，可让效率提升 200%。</li><li>消除迭代之间的气泡：采用循环展开 (Loop Unrolling) 技术，让 Python 可以连续不断地启动 CUDA 内核，消除了迭代间的 GPU 空闲气泡，带来 5-10% 的性能提升。</li><li>早停：在生成 EOS token 后，跳过后续 block 的推理过程，可以减少 5-40% 不必要的开销。</li></ul><p>3.并行解码：层级解码 (Hierarchical) 与信用解码 (Credit)<br/>为了在保证生成质量的前提下，最大化并行解码的 token 数量，dInfer 提出了两种无需额外训练的解码算法 ：</p><ul><li>层级解码 (Hierarchical Decoding)：该算法借鉴了「分治」思想，将待解码的区域不断递归地一分为二，并优先在每个子区域的中心位置解码 token 。这种方式自然地拉开了新生 token 间的距离，减少了它们之间的语义干扰 。在理想情况下，它能以近似对数级的复杂度完成多点并行生成，既快又稳 </li><li>信用解码 (Credit Decoding)：在多轮迭代中，有些正确的 token 可能很早就被模型稳定地预测出来，但因其单次置信度未能「达标」而被反复重算 。dInfer 为此引入了「累积信用」机制，持续追踪并累积每个 token 在历史迭代中的置信表现 。一个长期被稳定预测的 token，即使当前置信度稍低，也能凭借高累积信用被「破格」解码，从而有效避免了大量冗余计算</li></ul><p>4.压榨每步迭代价值：迭代平滑 (Iteration Smoothing)<br/>传统 dLLM 在每轮迭代中，只利用了置信度最高的 token 信息，而将其他位置的概率分布整个丢弃。dInfer 的迭代平滑算法，旨在回收这些被浪费的信息。</p><p>它基于未解码位置的 logits 分布得到该位置的加权 Embedding，并将其作为宝贵先验知识，平滑地融入下一轮迭代的 Embedding 中。这极大地丰富了上下文信息，使得单次迭代解码的 token 数量平均提升了 30-40%。</p><p>此外，由于 dInfer 可以无障碍地接入多种扩散语言模型，此次率先支持了基于轨迹蒸馏（Trajectory Distillation）加速 diffusion 去噪过程的 LLaDA-MoE-TD 模型，推理性能更强。</p><h2>实测数据：里程碑式的性能飞跃</h2><p>在配备 8 块 NVIDIA H800 GPU 的节点上，dInfer 的性能表现令人瞩目。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605782" alt="图片" title="图片" loading="lazy"/><br/>图 2. 评测数据</p><ul><li>10 倍性能提升：在与先前的 dLLM 推理方案 Fast-dLLM 的对比中，dInfer 在模型效果持平的情况下，平均推理速度（avg TPS）实现了 10.7 倍的巨大提升（681 vs 63.6）</li><li>超越自回归：与在业界顶尖的推理服务框架 vLLM 上运行的、参数量和性能相当的 AR 模型 Qwen2.5-3B 相比，dInfer 的平均推理速度是其 2.5 倍（681 vs 277） </li><li>突破推理极速：在代码生成任务 HumanEval 上，dInfer 在单批次推理中创造了 1011 tokens / 秒的纪录 。这是开源社区首次见证，扩散语言模型在延迟敏感的单批次推理场景下，速度显著超越经过高度优化的自回归模型</li></ul><p>更进一步，当结合轨迹蒸馏（Trajectory Distillation）技术（一种让模型学会 「跳跃式」去噪的后训练优化方法）后，dInfer 的平均推理速度飙升至 847 TPS，实现了超过 3 倍于 AR 模型的性能。</p><h2>开源开放：共建下一代 AI 推理新生态</h2><p>dInfer 的诞生，不仅是一个工具的发布，更是一次 LLM 范式的试炼：它证明了扩散语言模型的效率潜力并非空中楼阁，而是可以通过系统性的创新工程兑现，使其成为 AGI 道路上极具竞争力的选项。<br/>目前，dInfer v0.1 的全部代码、技术报告与实验配置已开源。<br/>蚂蚁希望 dInfer 能成为：</p><ul><li>研究者的标准平台：为 dLLM 领域的算法创新提供一个公平、高效的试验场 。</li><li>开发者的加速引擎：助力社区将强大的 dLLM 轻松部署到实际应用中，享受极致性能 。<br/>dInfer 连接了前沿研究与产业落地，标志着扩散语言模型从「理论可行」迈向「实践高效」的关键一步。我们诚邀全球的开发者与研究者一同加入，共同探索扩散语言模型的广阔未来，构建更加高效、开放的 AI 新生态。</li></ul>]]></description></item><item>    <title><![CDATA[新年小惊喜！龙蜥之旅五周年特辑上线啦，解锁你的社区足迹 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605785</link>    <guid>https://segmentfault.com/a/1190000047605785</guid>    <pubDate>2026-02-11 17:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>开源的世界里，没有微不足道的参与，只有共同成就的未来。</p><p>回望 2025，龙蜥社区始终向上突破——从技术演进到解决方案规模化落地，从高校开源教育到生态拓展，龙蜥社区交出了一份扎实而令人自豪的答卷。</p><p>而你，或许曾提交过代码、参与过活动，又或许只是默默关注、静静使用——无论以何种方式同行，你都是这段旅程中不可或缺的一份力量。正是因为你的每一次关注与信任，都为龙蜥注入了前行的动力。</p><p>值此龙蜥社区成立五周年&amp; 2026 新年来临之际，为感谢大家的一路相伴，我们特别准备了上千份精美礼品，包括龙蜥定制保温杯、限定款龙蜥卫衣、萌趣小龙抱枕、猫超卡、B 站/腾讯视频月卡等。诚挚邀请每一位关注、使用或贡献过龙蜥的朋友，打开龙蜥社区官网（openanolis.cn），一起回望 2025 年那些你在社区留下的珍贵“足迹”，重温属于你与龙蜥的共同记忆，还有精美周边领取哦。</p><p>活动时间：2026 年 2 月 10 日-3 月 31 日</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605787" alt="图片" title="图片"/><br/>（图/龙蜥开源足迹活动礼品图集）</p><h3>开启你的“龙蜥开源足迹”</h3><p>活动期间，首次登录龙蜥官网，会自动弹出“龙蜥开源足迹”，一键点击开启。若手滑退出或再次进入龙蜥官网，不用担心，可在官网顶部点击“龙蜥开源足迹”图片或官网右侧图标，都可开启活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605788" alt="图片" title="图片" loading="lazy"/></p><h3>兑换礼品</h3><p>活动截止时间：2026 年 3 月 31 日。</p><p>📌 兑换流程：开启龙蜥之旅，回顾年度开源足迹，点击抽盲盒。<br/>🎁 礼品邮寄：根据中奖提示，填写收件信息。工作人员会在 3 月起陆续安排邮寄。</p><h3>彩蛋</h3><p>除领取盲盒外，还可以下载“龙蜥开源足迹”图片，并在龙蜥公众号（搜 OpenAnolis 龙蜥）发布的这篇文章评论区留言，注意必须带“龙蜥开源足迹”图 + 评论（图片示意图如下所示；评论格式为“2026，我祝龙蜥社区...”内容不限，祝福或需求都可）。我们将按照规则送出龙蜥定制双肩包。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605789" alt="图片" title="图片" loading="lazy"/><br/>（图/“龙蜥开源足迹”示意图）</p><p>✨ <strong>送礼规则</strong></p><p>高赞有礼！我们将分别在 2 月 27 日（统计 2 月 10-26 日 23:59 的点赞数）、4月 1 日（统计 2 月 27 日-3 月 31 日 月 1- 30 日 23:59 的点赞数）各公布评论点赞排名前 10 的用户，送出龙蜥定制双肩包一个。</p><p>届时请大家及时关注龙蜥公众号（OpenAnolis龙蜥），获奖名单将在以上两个开奖日通过公众号文章的形式公布，请及时填写邮寄地址。</p><p>注意：评论须为原创，禁止发广告、拉踩等无意义内容，同一用户多条评论仅取点赞最高的一条参与当期评选。2 月已获奖的用户将不再参与 3 月的点赞排名评选。</p><p>快来一键开启你的龙蜥开源足迹吧～</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[2025 年度回顾｜龙蜥这一年：AI 领航，生态共荣 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605796</link>    <guid>https://segmentfault.com/a/1190000047605796</guid>    <pubDate>2026-02-11 17:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605798" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[如何通过设计研发协同平台实现制造业的高效创新？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047605803</link>    <guid>https://segmentfault.com/a/1190000047605803</guid>    <pubDate>2026-02-11 17:09:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的竞争格局中，产品研发的效率与质量已成为企业核心竞争力的关键。然而，传统的研发管理模式往往面临信息割裂、流程冗长、协作低效等痛点。设计图纸版本混乱、跨部门沟通成本高、质量风险难以提前识别等问题，不仅拖慢了产品上市速度，也增加了研发过程中的隐性成本。面对这些挑战，越来越多的企业开始将目光投向设计研发协同平台，希望通过数字化手段重构研发流程，实现数据驱动的协同创新。<br/>设计研发协同平台的核心价值<br/>设计研发协同平台并非单一的工具叠加，而是一种系统性的研发管理理念的落地。它通过整合产品生命周期中的需求、设计、仿真、试制、质量等环节，构建起一个统一的数据底座和协作环境。在这个平台上，三维模型、技术文档、物料清单（BOM）等核心数据得以结构化存储和动态关联，任何变更都能实时传递到所有相关环节，从而避免因信息滞后导致的错误和返工。更重要的是，平台打破了部门壁垒，使得设计、工艺、生产、质量等团队能够在同一语境下协作。例如，设计人员发起的模型修改，工艺人员可以即时反馈可制造性意见，质量人员则能同步更新FMEA（失效模式与影响分析）中的风险控制措施。这种“设计-工艺-质量”的一体化协同，不仅加速了决策流程，也从根本上提升了产品的可制造性和可靠性。<br/>技术实现与流程重构<br/>从技术层面看，现代设计研发协同平台通常基于云原生架构，支持分布式协同和轻量化应用。通过模型轻量化技术，非设计人员（如采购或质量工程师）无需安装专业CAD软件，即可通过浏览器查看、批注甚至参与评审复杂的三维模型。同时，平台内置的流程引擎将传统的纸质审批、邮件沟通转变为自动化的工作流，任务推送、节点提醒、权限控制等功能大幅减少了人为延误。在质量管控方面，平台通过集成FMEA管理模块，将历史故障库、行业标准与实时项目数据打通。系统能够基于相似产品或设计特征，自动推荐潜在的失效模式及改进措施，从而帮助工程师在研发早期识别风险，而非事后补救。这种预防性的质量保障机制，使得平台不再是简单的文档管理系统，而是贯穿产品创新全过程的智能决策支持系统。<br/>实践案例<br/>在国内，广域铭岛旗下的Geega（际嘉）工业互联网平台已成为研发协同领域的代表性解决方案。在某汽车零部件企业的实践中，Geega平台通过统一数据源和流程集成，实现了BOM准确率提升至98%，设计变更审批周期缩短50%，零部件复用率提高30%。其特点在于深度契合中国制造业的需求，注重轻量化部署和低成本适配，尤其擅长与现有ERP、MES系统的集成，帮助企业以较低门槛实现研发数字化。相比之下，国际厂商如PTC的Windchill和西门子的Teamcenter则代表了另一种路径。PTC通过强化AR/VR与数字孪生技术的融合，使研发协同不再局限于桌面屏幕，而是延伸至车间现场和远程运维场景。例如，工程师可通过AR设备在物理原型上叠加虚拟设计模型，直接进行偏差比对和装配验证。西门子Teamcenter则依托其完整的PLM（产品生命周期管理）生态，实现了从设计、仿真到制造执行的全链条数据闭环，特别适用于大型跨国企业的多地点、多学科协同需求。尽管路径不同，这些平台都在试图解决同一问题：如何让研发更高效、更可靠、更贴近市场需求。<br/>设计研发协同平台的崛起，标志着制造业研发模式从“粗放式管理”向“精细化运营”的转变。它不仅仅是一种技术工具，更是企业重塑研发体系、构建数字驱动文化的重要契机。当数据成为新的生产要素，协同成为新的创新范式，那些率先拥抱这一变革的企业，无疑将在未来的市场竞争中占据先机。</p>]]></description></item><item>    <title><![CDATA[让 AI Agent 安全“跑”在云端：基于函数计算打造 Agent 代码沙箱 Serverless]]></title>    <link>https://segmentfault.com/a/1190000047605809</link>    <guid>https://segmentfault.com/a/1190000047605809</guid>    <pubDate>2026-02-11 17:09:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：安全沙箱与 Serverless 的技术交汇</h2><p>随着大语言模型（LLM）从“对话框”走向“行动体（Agent）”，其能力边界正在迅速扩张。现代 AI Agent 不再是文字的搬运工，而是能够自主思考、调用工具、甚至编写并运行代码以解决复杂问题的智能助手。然而开发者始终面临一个根本性挑战：如何在保证执行效率的同时，实现资源强隔离与资源可控性？<br/>阿里云函数计算 FC 为这一难题提供了全新的解题思路。其底层基于轻量级安全沙箱，天然具备进程级隔离、资源极致伸缩、按需付费等特性。这种架构与 Agent 对代码执行环境的需求高度契合，使得构建高密度、低成本、安全可靠的 Agent 运行时成为可能。</p><h2>为什么需要 Agent 代码沙箱？</h2><p>Agent 的核心价值在于其“自主执行”能力，而代码执行是实现这一能力的关键路径。在工具调用、动态数据分析、自动化任务处理等典型场景中，Agent 生成的代码往往来自不可信的推理过程，若缺乏有效的沙箱保护，开发者将面临多重风险，为此 AI 开发者对运行时有着如下多个核心诉求：</p><ul><li>安全与隔离特性：必须确保不同用户的 Agent 代码在文件系统、网络访问上完全隔离，严防恶意指令注入导致的越权操作。</li><li>资源管理控制：代码缺陷或恶意行为可能导致 CPU/内存耗尽。系统需要能够对单个执行任务进行精细化的资源配额限制。</li><li>生命周期管理：Agent 任务存在短时型突发、长周期会话等多种任务模型，需提供灵活生命周期管理能力。</li><li>按资源消耗计费：若简单按实例运行时长计费，在长周期交互场景下，用户将为大量的“等待时间”支付不必要的费用。需在用户成本控制与平台资源利用率之间寻找平衡点。</li></ul><p>由此可见，构建一个强隔离、可管控、即开即用且按需回收的代码执行环境——Agent 代码沙箱，已成为 AI 应用架构中的刚需。</p><h2>为什么是 Serverless？函数计算的核心优势</h2><p>在众多技术路线中，Serverless 函数计算凭借其天然的“沙箱基因”，成为了构建 Agent 运行时的理想底座：</p><ol><li>底层安全隔离：主流云厂商的函数计算服务普遍采用 MicroVM 或强化容器技术作为执行单元。每个函数实例运行在一个轻量级、启动迅速的 MicroVM 中，具备完整的内核隔离。这种架构从进程、内存、文件系统等多维度实现安全保障。</li><li>极致的弹性伸缩：Agent 的请求模式具有高度不确定性。函数计算的毫秒级扩缩容能力，让开发者无需担心容量规划，轻松应对从零到万级并发的波动。</li><li>按量付费的经济性：传统常驻服务无论是否处理请求，均持续产生费用。而函数计算采用“用多少付多少”的计费模式，极大降低用户成本。（下文也将介绍 AI 场景下如何实现经济计费）</li><li>简化的运维体验：函数计算将基础设施管理完全托管给云平台，开发者只需关注代码逻辑，这种“代码即服务”的模式，极大加速了 AI 业务的迭代与上线周期。</li><li>异构算力支持：针对图像处理、音视频编解码等高性能场景，函数计算成熟的 GPU 实例支持，为 Agent 提供了更广阔的技能空间。</li></ol><h2>产品化实践：基于函数计算构建沙箱能力</h2><p>为了将通用的函数计算转化为专业的 Agent 运行时，我们不仅需要底层的隔离，更需要在协议层、会话层和调度层进行深度重构。</p><h3>协议扩展：定义多元化业务的接入标准</h3><p>Agent 的交互模式远比传统 Web 应用复杂。为了让 Agent 沙箱能够无缝嵌入现有的 AI 生态，我们针对不同场景实现了协议适配：</p><ol><li>针对工具生态：支持 MCP SSE 与 Streamable 协议<br/>随着 Model Context Protocol (MCP) 成为 Agent 工具调用的事实标准，函数计算在网关层实现了兼容标准的 MCP 协议，这意味着可以在函数计算平台实现一键托管 MCP 服务。</li><li>针对 Web/Browser Agent：支持标准 Cookie 协议<br/>Browser Agent 需要模拟登录状态或维持持久化的 Web 会话。函数计算的接入层通过实现兼容标准 Cookie 协议，使得沙箱环境能够保持与目标网站的交互状态，支持复杂的自动化操作。在用户首请求时，服务端将生成全局唯一的 CookieID 并通过 Response 中的 Set-Cookie 字段返回，后续请求用户仅需携带相同 CookieID 便实现定向路由。</li><li>针对灵活接入：定义统一 Header Field 协议<br/>在基于 Header Field 的会话亲和机制中，仅需客户端通过在 HTTP Header 中注入特定的元数据。函数计算系统网关会解析请求头中的会话 ID，并将其作为路由键，确保携带相同会话 ID 的后续请求被精准路由到同一函数实例。这种方式不依赖客户端状态（如 Cookie），可以应用在任何客户端以 HTTP 协议交互的业务场景中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605811" alt="" title=""/></p><h3>底座能力：构建有状态的会话管理</h3><p>在解决了协议层“如何接入”后，接下来的挑战是如何在无状态的 FaaS 架构上，构建“有状态”的会话体验。</p><h4>会话生命周期管理</h4><p>Agent 的执行往往不是一次性的，而是多轮对话，为此需要赋予会话生命周期管理能力，如下图所示，系统提供用户主动、系统自动两种能力实现灵活、完整的管理机制：</p><ol><li>用户主动管理</li></ol><ul><li>续期：面对 Agent 执行逻辑的不确定性，在生命周期配置上通常很难做到“一次性设对”。期间为延续状态的连续性，避免任务中断，可通过 Update API 实现对 Session TTL/IdleTimeout 的续期，主动延长沙箱寿命，续期后会话仍处于活跃状态且继续可用。</li><li>销毁：显式通过 Delete API 删除会话，实现提前销毁释放资源。</li></ul><ol start="2"><li>系统自动管理</li></ol><ul><li>Session TTL：会话达到 TTL（最大存活时长上限）后，无论是否仍在使用，平台都会自动回收资源。</li><li>Session IdleTimeout：会话在 IdleTimeout 规定时间内没有活动，平台判定为空闲并自动回收。</li></ul><p>两类方式最终都会走到生命周期结束 → 会话销毁 → 关联资源释放。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605812" alt="" title="" loading="lazy"/></p><h4>会话亲和能力</h4><p>这是将 FaaS 转化为“AI 运行时”的关键。通过会话亲和，我们保证了 Agent 上一轮生成的中间变量、本地文件在下一轮交互中依然可用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605813" alt="" title="" loading="lazy"/></p><p>整个流程分为用户首请求和非首请求，以 HeaderField 为例：</p><p><strong>会话初始化流程（首请求）</strong></p><ol><li>发起请求：Client（客户端）向 Gateway（网关）发送请求，并在 Header 中携带特定的 x-fc-session-id，用于标识该请求属于哪个 Agent 会话。</li><li>生成内部 ID：Gateway 接收请求后，对 session_key 进行哈希处理，生成一个系统内部使用的 internal_session_id。</li><li>查询会话状态：Gateway 向 MetaDB（元数据库）发起查询，核实该 session_id 是否已经存在（即是否已经有对应的运行实例）。</li><li>未命中处理：MetaDB 未搜到到相关信息，表明这是一个新会话，或者之前的会话已失效，需要重新分配资源。</li><li>触发调度：由于是新会话，Gateway 随机选择一个 Scheduler（调度器）节点，请求为该会话分配计算资源。</li><li>分配实例：Scheduler 根据当前资源情况，从资源池中分配一个可用的 VM实例（即沙箱环境）。</li><li>持久化映射关系：Scheduler 将 session_id 与分配到的 instance（实例）的对应关系写入 MetaDB。这样后续携带相同 ID 的请求就能实现“会话亲和性”，直接路由到该实例。</li><li>路由响应：Scheduler 将实例的路由信息返回给 Gateway。</li><li>返回首包：Gateway 完成链路建立，将处理后的首包数据返回给 Client。至此，该 Agent 会话正式建立，后续交互将直接复用此路径。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605814" alt="" title="" loading="lazy"/></p><p><strong>热请求数据流程</strong></p><ol><li>发起请求：Client（客户端）发起请求，并在 Header 中携带已有的 x-fc-session-id。</li><li>查询会话记录：Gateway（网关）接收请求后，前往 MetaDB（元数据库）查询该 Session ID 对应的记录。</li><li>返回映射信息：MetaDB 返回该会话之前绑定的 Instance（实例）信息以及负责管理该实例的 Target Scheduler（目标调度节点）。</li><li>直连调度节点：Gateway 根据返回的信息，直接联系对应的 Target Scheduler。</li><li>确认路由实例：Target Scheduler 告知 Gateway 该实例有效，可以进行数据转发。</li><li>转发请求：Gateway 将客户端的业务请求转发给对应的 Instance。</li><li>处理并响应：Instance（Agent 沙箱）执行代码逻辑处理请求，并将结果返回给 Gateway。</li><li>返回业务数据：Gateway 将最终的执行结果回传给 Client，完成一次有状态的会话交互。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605815" alt="" title="" loading="lazy"/></p><h4>会话隔离能力</h4><p>为了极致的安全，我们引入了“一会话一实例”的隔离模型。每个 Agent Session 独占一个底层的运行实例。一旦会话结束，实例立即销毁并擦除数据。通过会话配额控制，可以有效防止单个用户创建过多沙箱导致资源过载。</p><h3>扩展配套能力，强化 Agent 底座</h3><p>除了核心的调度与协议，针对生产环境中的性能与成本挑战，我们进一步扩展了配套能力：</p><ol><li>预热能力<br/>冷启动是 Serverless 的天敌。针对 Agent 实时交互的要求，我们支持 <code>CreateSession</code> 主动预热。在用户刚进入对话页面时，系统提前准备好预留实例。将沙箱的就绪时间压缩至极低延时。</li><li>会话级存储隔离<br/>Agent 经常需要读写文件。我们实现了会话维度的动态存储挂载。每个沙箱可以根据 Session ID 动态挂载独立的 NAS 或 OSS 路径。这样既保证了数据在会话内的持久化，又确保了不同会话间的文件系统是物理隔离的。同时满足沙箱异常 Crash 后数据的可恢复。</li><li><p>计费升级模型进化：从 FAAS 的“按请求”到“按资源消耗”<br/>FaaS按请求计费模式，在AI场景下会产生巨大的“保活成本”。会话计费模型必须与资源的实际使用强挂钩，因此系统针对会话函数的计费模式升级到Serverless AI 计费模式。</p><ul><li>活跃期：当会话实例正在处理用户请求时，按照活跃单价计费。</li><li>空闲期：当会话处于空闲、仅维持连接和上下文状态时，系统切换到一个极低的“保活”费率。仅收取内存、磁盘的费用，不再收取相对较高的CPU费用。</li></ul></li></ol><p>这个模式对客户而言，相对传统常驻实例完整生命周期计费模式成本大幅降低。</p><h2>总结与展望</h2><p>Serverless 函数计算凭借其安全隔离、弹性伸缩、按需付费等基因，正成为构建 Agent 运行时的理想选择。通过协议生态扩展、会话管理能力增强、配套能力完善，我们已实现从“单一函数执行”到“复杂 Agent 托管平台”的跨越。未来，我们也将持续聚焦启动优化、更长会话支持等等核心能力，做好AI原生时代坚实的护航者。</p>]]></description></item><item>    <title><![CDATA[『n8n』不用写SQL，了解一下内置的Datatable 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605823</link>    <guid>https://segmentfault.com/a/1190000047605823</guid>    <pubDate>2026-02-11 17:08:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个n8n小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=1%2Bs8biC3qr0KLLd3oT0OKg%3D%3D.K9PJ%2BnLxPIDKwvN%2BXNTNsmnuLoIf4qRX1Ory2Gu6QfEUzcN6ZQdy9dfRy466MR%2FtjrkSDS5uQdmXsK8%2FRT666WmP2tRDzyUwi5C7eOiG1hTyhi5BU%2F3A%2FaviR6hzHzPNBAp%2BOEKOURZN%2FBntxrmP1ee87sjqO7VJ%2FuZ3M5pNhl4%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a></blockquote><p>非技术出身的工友在使用 n8n 时是否会遇到这样的困惑：爬取了一堆数据不知道存哪里，总不能每次都导出到Excel再来回导入？想让多个工作流共用一套数据，却找不到简单的方法？不想折腾MySQL、PostgreSQL这些复杂的外部数据库，也看不懂晦涩的SQL语句？</p><p>如果有以上困惑，而且你的数据结构不是那么复杂的话，可以试试 n8n 内置的 Data tables。</p><p>我用一个简单的例子介绍一下 Data tables 的用法，顺便讲讲不同格式的字段该如何转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605825" alt="" title=""/></p><p>我们可以在 Data tables 面板管理各个数据表，点击右上角的“Create data table”创建一个数据表。</p><p>我以“员工信息表”作为演示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605826" alt="" title="" loading="lazy"/></p><p>点击加号新增 <code>name</code> 和 <code>married</code>。</p><p><code>name</code> 是“员工姓名”，Type 选择 string。</p><p><code>married</code> 是“是否结婚”，Type 选择 boolean（这个类型只有“是”和“否”两个选项）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605827" alt="" title="" loading="lazy"/></p><p>创建一个工作流，添加要给表单节点。</p><p>只有2个字段，表单的“姓名”对应数据表里的 <code>name</code>；表单的“是否结婚”对应数据表里的 <code>married</code>。</p><p>但是，数据表的 <code>married</code> 的类型是布尔型，也就是 <code>true</code> 表示已结婚，<code>false</code> 表示未结婚。但表单的“是否结婚”的选项却是字符串的“已结”和“未结”。这里要做一下转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605828" alt="" title="" loading="lazy"/></p><p>在表单节点后面添加一个「Insert row」节点，搜 table 就能找到它。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605829" alt="" title="" loading="lazy"/></p><p>给「Insert row」节点做以下配置。</p><p>「From list」选择刚刚创建的“员工信息”表。</p><p>「Values to insert」填入 <code>name</code> 和 <code>married</code> 这两个字段。</p><p><code>name</code> 这项填入 <code>{{ $json['姓名'] }}</code> 比较好理解，我不讲解了。</p><p><code>married</code> 这项填入 <code>{{ $json['是否结婚'] === '已结' ? true : false }}</code>，这是 JS 的三元运算符，判断上一个节点传入的“是否结婚”这项的值是否为“已结”，如果是的话就存入 <code>true</code> ，否则存入 <code>false</code>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605830" alt="" title="" loading="lazy"/></p><p>测试一下整个工作流。</p><p>我提交了2次表单：</p><ul><li>雷猴，已结</li><li>鲨鱼辣椒，未结</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605831" alt="" title="" loading="lazy"/></p><p>来到数据表就能看到这两项了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605832" alt="" title="" loading="lazy"/></p><p>鲨鱼辣椒突然说他今天要结婚了，作为 HR 也应该更新一下数据。</p><p>此时再提交一次表单：鲨鱼辣椒，已结。你会发现工作流又多了一条数据，这并不是我们想要的结果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605833" alt="" title="" loading="lazy"/></p><p>在 Data tables 里先手动删除第3项。</p><p>回到工作流这边，新增了一些节点。</p><ul><li><p>上面那条：</p><ul><li>「If row exists」：如果传入的“姓名”<strong>已在</strong>数据表里，就走这条。</li><li>「Update row(s)」：更新表中的数据。</li></ul></li><li><p>下面那条：</p><ul><li>「If row does not exist」：如果传入的“姓名”<strong>不在</strong>数据表里，就走这条。</li><li>「Insert row」：新增一条数据。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605834" alt="" title="" loading="lazy"/></p><p>「If row exists」和「If row does not exist」的判断条件都是一样的，如下图所示。只不过这两个节点的功能不同而已。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605835" alt="" title="" loading="lazy"/></p><p>在来看「Update row(s)」这个节点，通过 <code>name</code> 这个字段找到要修改的那行数据，找到后就修改 <code>married</code> 这列的值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605836" alt="" title="" loading="lazy"/></p><p>「Insert row」节点的配置不需要改变。</p><p>试试～</p><p>提交一项：鲨鱼辣椒，已婚</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605837" alt="" title="" loading="lazy"/></p><p>回到数据表这边就能看到鲨鱼辣椒的婚姻状态变成“true”了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605838" alt="" title="" loading="lazy"/></p><p>再提交一项：蝎子莱莱，未婚。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605839" alt="" title="" loading="lazy"/></p><p>由于“蝎子莱莱”不在数据表里，所以走的是“新增”路线。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605840" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，想了解更多n8n玩法欢迎关注<a href="https://link.segmentfault.com/?enc=2EhXDv9wyvkBho6%2FOzptww%3D%3D.XC0vk1%2BC0zi%2F7TeobnIwX3BUmsXNSSsEeV47RXU7wvrwNn%2B7n8hZczeTSVEEpnjXAdWypa81v24m5FjigFqo7qX0eoZOiSkXe585dtAOjQXMP2jtmI3a10ncAmnVjMt0nmefilHkTZDBFTr1h528wdCNEJzFV2jMSRVw2OAkEk8%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a>👏</p><p>如果你有 NAS，我非常建议你在 NAS 上部署一套 n8n，搞搞副业也好，帮你完成工作任务也好  <a href="https://link.segmentfault.com/?enc=QVefuQl2ghqxx1XEmEAPpA%3D%3D.%2FKSidRKzhzuCFS8Ds4OXoVWqLkiYB6SnS2zSaDtwdCoPXlCK0Nc5aJzTmMxxQlvxJ26EOegoV9Ni7OqGSHB5KA%3D%3D" rel="nofollow" target="_blank">《『NAS』不止娱乐，NAS也是生产力，在绿联部署AI工作流工具-n8n》</a></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[『NAS』在绿联部署隐私优先的媒体文件格式工具-VERT.sh 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605897</link>    <guid>https://segmentfault.com/a/1190000047605897</guid>    <pubDate>2026-02-11 17:07:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=VIgaLS5CWZ2ZY%2F3%2BJZLtYQ%3D%3D.h5NIi8HPRwSyOjatedO%2BCrl45vRNof7NaSYHOUWfWBMrcWDs7kPK2aCN7AXOuPqwy3UVhH%2Bofgx8qvP%2FNopPOYMzQwyUvwaZN00QCQV5JrHuaxnoqrzqZmlSDasbMuC26GHq2aa4EmgQTUC3SAwpW3HgllkUw%2FlZ28ZYVXCddNQ%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>VERT.sh 是一款开源文件转换工具，它所有转换都在本地浏览器完成，文件（除视频外）永不上传云端，支持 n 种格式（文档、图片、音频、视频），无广告、无文件大小限制，开箱即用！</p><p>这次我使用绿联的 NAS 部署 VERT.sh，其他品牌的 NAS 只要支持 Docker 的，部署步骤都是大同小异。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605899" alt="" title=""/></p><p>首先在“文件管理”应用里找到“docker”文件夹，在里面创建一个“vert”文件夹。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605900" alt="" title="" loading="lazy"/></p><p>打开“Docker”应用，点击左侧菜单的“项目”，创建一个新项目。</p><ul><li>项目名称：vert。</li><li>存放路径：选择上一步创建的 <code>xxx/docker/vert</code> 文件夹。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605901" alt="" title="" loading="lazy"/></p><p>Compose配置 输入以下代码：</p><pre><code>services:
  vert:
    image: ghcr.io/vert-sh/vert:latest
    container_name: vert
    ports:
      - 2340:80 
    restart: unless-stopped</code></pre><p>这里我配置了 <code>2340</code> 这个端口，你根据你实际情况配置吧，只要不跟其他项目的端口冲突就行。</p><p>然后等 Docker 下载镜像和构建项目，等就行，速度取决于你的网速。</p><p>项目构建完成后，点击左侧菜单”容器“这项，找到 vert 这项，点击它右侧的小箭头会弹出端口号，点击端口这个按钮。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605902" alt="" title="" loading="lazy"/></p><p>点击端口按钮后，浏览器会新开一个窗口运行 VERT.sh。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605903" alt="" title="" loading="lazy"/></p><p>它默认界面是英文的，点击顶部菜单的“Settings”项，找到“Language”，切换成中文就行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605904" alt="" title="" loading="lazy"/></p><p>回到首页可以看到 VERT.sh 支持图片、音频、文档和视频等文件的格式转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605905" alt="" title="" loading="lazy"/></p><p>测试了一下，图片是可以转格式的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605906" alt="" title="" loading="lazy"/></p><p>但视频要传到人家的服务器转，而且还不一定成功😮‍💨</p><p>就当没转视频格式这个功能吧😤</p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=fFgXTmv6QthMvvdoNFf8BA%3D%3D.ipHZudRMwvb5anEVIdEdTS7DK3PmG48paxrNVICj443e8uvMbXBjYc98bGUlSS86GaK0Oe1SH86WUuma6ks0k3e%2BJ5hPa%2FzZaqdN2bJcRhX8qCVbhCm%2BrGV65pEDtPnDdyeD3gxnW5gTYqBsjM26bwQBWozsa3zXlvDkvGAFkzM%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[R语言优化沪深股票投资组合：粒子群优化算法PSO、重要性采样、均值-方差模型、梯度下降法|附代码数据]]></title>    <link>https://segmentfault.com/a/1190000047605916</link>    <guid>https://segmentfault.com/a/1190000047605916</guid>    <pubDate>2026-02-11 17:06:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=G%2BafYjCBdcOD3o7U3NjxBw%3D%3D.GKQqcOekisfPcApG8oLZUnecuKhHq9DGp%2BmkO2bkon0%3D" rel="nofollow" title="https://tecdat.cn/?p=44965" target="_blank">https://tecdat.cn/?p=44965</a>  <br/>原文出处：拓端数据部落公众号  <br/><strong>关于分析师</strong>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605918" alt="" title=""/></p><p>在此对 Hongxuan Liu 对本文所作的贡献表示诚挚感谢，他完成了应用统计专业的硕士学位，专注机器学习、风险管控领域。擅长 R 语言、Python，在机器学习、风险管控领域具备扎实的技术功底，可熟练运用相关软件开展数据分析、建模及风险管控相关工作。Hongxuan Liu 曾在中国农业银行从事数据分析工作，深耕金融领域数据分析与风险管控相关业务，负责银行各类数据的整理、分析与建模，为银行的风险防控、投资决策等核心业务提供数据支撑与实操建议，积累了丰富的金融行业数据分析实战经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t1" target="_blank"/>专题：2025年智能优化算法在A股投资组合配置中的实践与创新</h3><h4><a name="t2" target="_blank"/>引言</h4><p>在国内A股市场的投资实践中，普通投资者和中小机构始终面临一个核心难题：如何在多只股票间分配资金，既能控制波动风险，又能实现资产稳健增值。早年间，多数投资者依赖经验或“等权重均分”的方式配置资产，这种缺乏量化支撑的策略，在2020年后市场波动加剧的背景下，资产波动幅度比科学配置方案高出30%以上。  <br/>马科维茨的均值-方差模型为量化配置提供了理论基础，但该模型对应的优化问题存在非凸性，传统梯度下降算法极易陷入局部最优，无法找到真正的全局最优配置。粒子群优化算法（PSO）凭借全局搜索能力强的优势成为解决这类问题的有效工具，但传统PSO初始粒子随机分布，导致收敛速度慢、无效计算多。基于此，我们结合为金融机构提供投资组合优化咨询项目的实战经验，提出将重要性采样（IS）与PSO融合的IS-PSO算法，通过定向生成高质量初始粒子群，解决传统PSO“盲目搜索”的痛点。本文将拆解该算法的设计逻辑、落地步骤及在沪深A股样本上的应用效果，让读者既能掌握实操方法，也能理解背后的核心原理。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，该<strong>项目完整代码与数据已</strong>分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂 怎么做，也懂 为什么这么做；遇代码运行问题，更能享24小时调试支持。</p><h4><a name="t3" target="_blank"/> 项目文件目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605920" alt="" title="" loading="lazy"/></p><h4><a name="t4" target="_blank"/>整体流程脉络</h4><p>&lt;pre data-index="0" name="code" style="color: rgb(0, 0, 0); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"&gt;&lt;img alt="" src="https://i-blog.csdnimg.cn/direct/fa519e9285cd4756b5c826972a17bbba.png" style="border: 0px; max-width: 650px;"&gt;<br/>&lt;/pre&gt;</p><h4><a name="t5" target="_blank"/>1 投资组合优化的行业痛点与技术基础</h4><p>对于A股投资者而言，“分散投资却没分散风险”是普遍痛点——不少投资者将资金分散到多只股票后，要么收益跑不赢大盘，要么遇到市场调整时亏损远超预期。这背后的核心原因，是没有科学量化不同股票的收益和风险特征，仅依靠经验或简单的行业均分策略，无法适配复杂的市场环境。  <br/>马科维茨均值-方差模型为解决这一问题提供了核心框架：将投资组合的收益定义为各资产预期收益率的加权平均值，风险用收益率的标准差衡量，核心目标是找到“有效前沿”——即在给定风险下收益最高，或给定收益下风险最低的资产组合。这一目标转化为数学问题后，核心是最大化效用函数：max (ω^Tμ - λ/2*sqrt(ω^TΣω))。其中ω是资产权重向量（各股票配置比例），μ是资产预期收益率向量，λ是风险厌恶系数（数值越大代表投资者越不愿承担风险），Σ是资产收益率的协方差矩阵。  <br/>但这个效用函数具有非凸性，传统梯度下降算法依赖目标函数的梯度信息迭代，很容易停在局部最优解，无法找到真正的全局最优权重配置，这也是传统算法在实际投资中效果不佳的关键原因。</p><h4><a name="t6" target="_blank"/>2 IS-PSO算法的创新设计与实现</h4><p>粒子群优化算法（PSO）是模拟鸟群觅食行为的智能优化算法，每个“粒子”对应一组资产权重，通过迭代更新粒子的位置（权重）和速度，跟踪个体最优位置（pbest）和全局最优位置（gbest），最终找到最优权重配置。但传统PSO的初始粒子是随机生成的，大量粒子落在无效解区域，导致算法收敛慢、计算效率低。  <br/>我们的核心创新点在于，用重要性采样（IS）优化初始粒子群的生成逻辑：先随机生成大量权重样本，筛选出目标函数值前20%的“优质样本”，再基于这些样本的分布生成80%的初始粒子，剩余20%粒子随机生成（保证群体多样性）。这种方式让初始粒子聚焦在最优解附近，大幅减少无效迭代，提升收敛效率。</p><h5>2.1 IS-PSO算法的R语言核心实现</h5><p>以下是修改后的核心代码（变量名、代码结构均做调整，英文注释已翻译为中文，省略部分通用迭代逻辑）：</p><pre><code># ===== 融合重要性采样的改进粒子群优化算法（IS-PSO） =====enhanced_pso_with_is &lt;- function(converge_threshold = 1e-6, min_diversity = 1e-4, rand_seed = NULL) { # 设置随机种子，保证结果可重复 if (!is.null(rand_seed)) { set.seed(rand_seed) }# 步骤1：重要性采样预生成大量权重样本 pre_sample_total &lt;- 500 # 预生成500个随机权重样本 pre_weight_matrix &lt;- matrix(runif(pre_sample_total * asset_num), pre_sample_total, asset_num) # 权重归一化处理（确保所有资产权重和为1） pre_weight_matrix &lt;- t(apply(pre_weight_matrix, 1, function(x) x / sum(x))) # 计算每个预生成样本的目标函数值 pre_obj_scores &lt;- apply(pre_weight_matrix, 1, calc_objective_func)# 步骤2：筛选前20%的优质权重样本 top_sample_indexes &lt;- order(pre_obj_scores, decreasing = F)[1:(pre_sample_total * 0.2)] top_weight_samples &lt;- pre_weight_matrix[top_sample_indexes, ] sample_central &lt;- colMeans(top_weight_samples) # 优质样本的中心位置 sample_cov_matrix &lt;- cov(top_weight_samples) # 优质样本的协方差矩阵# 步骤3：生成初始粒子群（80%来自优质区域，20%随机生成） particle_positions &lt;- matrix(0, particle_total, asset_num) for (i in 1:particle_total) { if (i &lt;= particle_total * 0.8) { # 80%粒子从优质样本区域生成，考虑资产间相关性 asset_dim &lt;- asset_num # 尝试对协方差矩阵做Cholesky分解（添加小值保证矩阵正定） chol_matrix &lt;- try(chol(sample_cov_matrix + 1e-6 * diag(asset_dim)), silent = TRUE) if (inherits(chol_matrix, "try-error")) { # 分解失败时，使用对角协方差生成扰动值 disturbance_val &lt;- sqrt(diag(sample_cov_matrix)) * rnorm(asset_dim) } else { # 分解成功则生成符合协方差分布的扰动值 disturbance_val &lt;- chol_matrix %*% rnorm(asset_dim) } # 调整扰动幅度（0.2为实战验证的经验系数） temp_weight &lt;- sample_central + 0.2 * disturbance_val temp_weight &lt;- pmax(temp_weight, 0) # 保证权重非负（不允许卖空） } else { # 20%粒子随机生成，维持粒子群的多样性 temp_weight &lt;- runif(asset_num) } # 对生成的权重做归一化处理 particle_positions[i, ] &lt;- temp_weight / sum(temp_weight) }# 省略：粒子速度初始化、个体最优/全局最优参数初始化代码 ......# 省略：粒子位置/速度迭代更新、收敛条件判断的核心循环代码 ......# 返回算法最优结果 return(list( best_weight_config = global_best_pos, best_objective_val = -(global_best_score), portfolio_annual_return = sum(global_best_pos * return_vector), portfolio_annual_risk = sqrt(t(global_best_pos) %*% cov_mat %*% global_best_pos), converge_iterations = iter_count ))}</code></pre><p><strong>代码说明</strong>：</p><ul><li>核心逻辑是通过预采样筛选优质权重样本，让80%的初始粒子聚焦在最优解附近，减少无效搜索；</li><li>变量名如<code>n_pre_samples</code>改为<code>pre_sample_total</code>、<code>n_assets</code>改为<code>asset_num</code>，更贴合中文使用习惯；</li><li>省略部分为PSO算法通用的迭代更新逻辑（如粒子速度调整、收敛判断循环），可参考常规PSO实现补充。</li></ul><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047605921" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>专题：2025年游戏科技的AI革新研究报告</h3><h3><a name="t8" target="_blank"/>原文链接：<a href="https://link.segmentfault.com/?enc=Myp5h5WC0B%2BASmWAPSACAw%3D%3D.imfeGLiOhCySBr7wnb1yRHJh7AFbDE9pZEoqJn2NKbA%3D" rel="nofollow" title="https://tecdat.cn/?p=44082" target="_blank">https://tecdat.cn/?p=44082</a></h3><hr/><h4><a name="t9" target="_blank"/>3 IS-PSO算法在A股中的实际应用效果</h4><p>我们选取沪深A股10只不同行业的股票（覆盖装备制造、信息技术、环保、医药等领域），以2020年7月至2025年6月共1198个交易日的收盘价为基础数据，先计算日对数收益率（Rt = ln(Pt/Pt-1)），再年化处理得到预期收益率和协方差矩阵，对比等权重配置、梯度下降算法、传统PSO、IS-PSO四种方式的应用效果。</p><h5>3.1 收敛效率大幅提升</h5><p>传统PSO算法平均需要120次迭代才能收敛，而IS-PSO仅需31次迭代，收敛速度提升74.4%。在计算效率上，IS-PSO平均运行时间为66.789毫秒，远低于传统PSO的232.732毫秒，这意味着在实际应用中，IS-PSO能更快给出最优配置方案，降低计算资源消耗。</p><h5>3.2 收益风险比更优</h5><p>在高风险厌恶场景（λ=0.9）下，IS-PSO配置的投资组合平均收益达到0.165，高于传统PSO的0.157；且IS-PSO生成的权重呈现“稀疏性”——仅聚焦2只核心股票，却实现了更高的收益风险比。这对中小投资者而言，大幅降低了选股和资金配置的门槛，无需分散到多只股票就能实现风险与收益的平衡。</p><h5>3.3 权重配置结果可视化</h5><p>（空行）  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605922" alt="" title="" loading="lazy"/>  <br/>（空行）  <br/>上图清晰展示了不同算法的权重配置差异：梯度下降算法配置的资产数量多，但收益表现不如PSO类算法；IS-PSO与传统PSO均聚焦少数核心资产，但IS-PSO在收敛速度和高风险场景下的收益表现更优，更适合普通投资者使用。</p><h4><a name="t10" target="_blank"/>4 应急修复服务与落地建议</h4><p>针对算法落地过程中可能出现的代码运行异常、结果不符合预期等问题，我们提供<strong>24小时响应的应急修复服务</strong>，相比投资者自行调试，问题解决效率提升40%，能快速定位并解决代码报错、参数设置不当、数据适配异常等问题。  <br/>在实际落地层面，IS-PSO算法可直接适配中小投资者的需求：只需导入股票收盘价数据，设置风险厌恶系数，算法就能自动输出最优权重配置。后续可进一步优化方向包括：纳入债券、基金等多元资产，考虑交易成本、流动性等实际交易约束，通过贝叶斯优化实现算法参数的自动调整。</p><h4><a name="t12" target="_blank"/>总结</h4><ol><li>针对A股投资组合优化的非凸性问题，融合重要性采样的PSO算法（IS-PSO）通过优化初始粒子群分布，将收敛速度提升74.4%，大幅提升计算效率；</li><li>IS-PSO在高风险厌恶场景下收益表现更优，且生成的稀疏权重配置降低了中小投资者的实操门槛；</li><li>该算法已通过实际咨询项目验证，配套的24小时应急修复服务可保障算法稳定落地应用。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p>]]></description></item>  </channel></rss>