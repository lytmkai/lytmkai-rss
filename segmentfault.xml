<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[从0到1到100：中小学学科答题小程序的设计与实现 CC同学呀 ]]></title>    <link>https://segmentfault.com/a/1190000047608128</link>    <guid>https://segmentfault.com/a/1190000047608128</guid>    <pubDate>2026-02-12 19:03:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>研究背景</h2><p>在教育信息化 2.0 行动计划的推动下，数字化学习工具逐渐融入中小学的日常教学与课后辅导中。中小学生的注意力持续时间较短，传统的刷题模式形式单一、趣味性不足，难以调动学生的学习积极性，而游戏化学习模式将学习与互动、竞赛相结合，能够有效提升学生的参与度和学习效率。<br/>微信作为国内用户量最大的社交平台，其小程序生态日趋完善，具有无需下载安装、即用即走、开发成本低、传播便捷等特点，非常适合开发轻量化的教育类应用。同时，腾讯微信云开发技术为小程序提供了一站式的云端开发解决方案，无需开发者搭建独立的服务器和域名，降低了小程序的开发和部署门槛，为教育类小程序的快速开发提供了技术支撑。在此背景下，设计并实现一款基于微信云开发的中小学学科答题小程序，能够满足校方、教师的教学辅助需求和家长的课后辅导需求，助力中小学教育的数字化转型。</p><h2>主要研究内容</h2><p>（1）通过调研中小学教学和课后辅导的实际需求，结合游戏化学习理论，完成小程序的功能性和非功能性需求分析，并进行技术、经济、操作可行性分析；<br/>（2）确定小程序的设计原则，设计基于微信云开发的系统架构，将小程序划分为前端用户模块和后台管理模块，并对各模块的功能进行详细设计；<br/>（3）根据需求分析和总体设计，完成小程序的数据库设计，设计用户表、题库表、答题记录表、竞赛表等核心数据表的结构；<br/>（4）选择微信小程序平台和腾讯云开发技术作为核心技术栈，完成小程序前端页面的开发、云函数的编写和后台管理功能的实现；<br/>（5）制定详细的系统部署步骤，完成小程序的本地部署和云端发布，并设计功能测试用例，对小程序的各项功能进行测试，验证系统的稳定性和实用性；<br/>（6）总结本次研究的成果和不足，对小程序的后续优化和功能拓展进行展望。</p><h2>系统需求分析</h2><p>需求分析是系统开发的基础，通过调研校方、教师、家长和学生的实际需求，明确系统的功能和性能要求，为后续的系统设计和实现提供依据。本次调研采用问卷调查和访谈相结合的方式，调研对象包括中小学教师、家长和学生，共发放问卷 300 份，回收有效问卷 286 份，访谈中小学教师 20 名、家长 30 名。</p><h2>功能性需求</h2><p>根据调研结果，将中小学学科答题小程序的用户分为普通用户（学生）、<strong> 管理用户（教师 / 家长 / 校方管理员）</strong> 两类，不同用户的功能性需求不同，同时小程序需满足基础的系统管理需求，具体功能性需求如下：<br/>2.1.1 普通用户（学生）需求<br/>普通用户为中小学学生，核心需求是通过小程序进行学科知识点的练习和答题竞赛，具体需求包括：<br/>（1）用户登录：支持通过微信授权快速登录，无需单独注册账号，登录后可查看个人信息和答题记录；<br/>（2）题库选择：支持按照学科（数学、语文、英语等）和知识点对题库进行分类筛选，方便学生选择针对性的知识点进行练习；<br/>（3）随机抽题：支持选择特定学科或知识点，由系统随机生成题目进行练习，抽题数量可灵活选择；<br/>（4）答题练习：答题过程中显示题目序号、题干和答题选项，支持选择题、判断题等常见题型，答题完成后即时显示答题结果；<br/>（5）解析详解：每道题目答题完成后，提供详细的解答和解题思路解析，帮助学生理解错题原因，巩固知识点；<br/>（6）答题竞赛：支持参与模拟竞赛，竞赛前可查看竞赛规则（答题时间、题目数量），竞赛过程中倒计时显示，答题完成后即时显示竞赛成绩；<br/>（7）排行榜查看：支持查看答题竞赛的积分排行榜，按积分从高到低排序，显示用户昵称、积分和排名，提升学习的竞争性；<br/>（8）个人中心：支持查看个人答题记录（答题次数、正确率、错题集）、竞赛记录（竞赛次数、最高成绩、平均成绩），可修改个人昵称等基础信息。</p><h2>系统基础需求</h2><p>（1）权限管理：实现超级管理员和普通管理员的权限分离，超级管理员拥有所有操作权限，普通管理员仅拥有题库管理、答题参数设置等部分权限；<br/>（2）数据备份与恢复：支持对题库、用户信息等核心数据进行备份，防止数据丢失；支持在数据异常时进行数据恢复；<br/>（3）缓存清理：支持清理小程序的本地缓存，提升小程序的运行速度。</p><h2>性能需求</h2><p>（1）响应速度：小程序前端页面的加载时间不超过 3 秒，随机抽题、答题结果展示、排行榜查看等操作的响应时间不超过 1 秒；<br/>（2）并发处理：支持至少 100 名用户同时参与答题竞赛，系统无卡顿、无延迟；<br/>（3）数据处理：支持 Excel 文件批量导入题库，5000 条数据的导入时间不超过 30 秒。</p><h2>技术可行性</h2><p>本小程序基于微信小程序平台和腾讯微信云开发技术进行开发，相关技术均为腾讯官方推出，技术文档完善、社区支持活跃，开发门槛较低。<br/>（1）微信小程序的开发框架提供了丰富的组件和 API，支持前端页面的快速开发，且兼容 iOS 和 Android 两大移动操作系统；<br/>（2）微信云开发技术提供了云函数、云数据库、云存储、定时器等一站式云端服务，无需开发者搭建独立的服务器和域名，降低了后端开发的复杂度；<br/>（3）开发所需的辅助技术如 Node.js、NPM 均为开源技术，学习资源丰富，开发者可快速掌握；<br/>（4）微信开发者工具为小程序的开发、调试、预览、部署提供了一体化的支持，支持本地调试和真机测试，方便开发过程中的问题排查。</p><h2>经济可行性</h2><p>本项目的开发和部署成本较低，后期维护成本几乎为零，具有良好的经济可行性：<br/>（1）开发成本：项目基于开源技术和腾讯免费的云开发基础配额进行开发，开发过程中无需支付软件授权费、服务器租赁费、域名注册费等费用；<br/>（2）部署成本：微信云开发的资源配额价格低廉，基础版配额可满足中小学校方的日常使用需求，即使后续需要提升资源配额，费用也远低于传统的服务器部署；<br/>（3）维护成本：微信云开发由腾讯官方进行维护，无需开发者进行服务器的运维、升级、安全防护等工作，节省了大量的维护人力和物力成本；<br/>（4）使用成本：小程序对用户完全免费，用户无需支付任何费用即可使用所有功能，易于推广和使用。</p><h2>系统总体设计</h2><p>模块化设计原则：将系统划分为多个独立的功能模块，每个模块实现特定的功能，模块之间通过统一的接口进行交互，降低模块之间的耦合度，方便后续的功能扩展和 bug 修复；<br/>用户体验优先原则：结合中小学生和管理员的使用习惯，进行前端界面和操作流程的设计，保证界面友好、操作简单，提升用户的使用体验；<br/>轻量化设计原则：基于微信小程序 “即用即走” 的特性，简化前端页面的代码和资源，降低小程序的代码包大小，提升页面的加载速度；<br/>云原生设计原则：充分利用微信云开发的技术优势，将业务逻辑、数据存储、文件存储均部署在云端，实现前后端的解耦，降低本地开发的复杂度；<br/>权限最小化原则：针对不同的管理员角色分配最小的操作权限，超级管理员拥有所有权限，普通管理员仅拥有必要的操作权限，保证系统的数据安全；<br/>可扩展设计原则：在系统架构和数据库设计中，预留扩展接口和字段，方便后续新增功能和扩展数据类型，适应不同用户的个性化需求。</p><h2>数据库设计</h2><p>本小程序采用微信云开发的云数据库进行数据存储，云数据库是一种非关系型数据库（NoSQL），以集合（Collection）为单位存储数据，每个集合包含多个文档（Document），文档采用 JSON 格式存储数据，具有灵活的结构，适合小程序的轻量化开发需求。根据系统的功能需求，设计用户集合、题库集合、答题记录集合、竞赛记录集合、管理员集合、答题参数集合、系统日志集合七个核心集合<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608130" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>前端功能实现</h2><p>小程序前端是用户与系统的交互界面，采用 WXML、WXSS、JavaScript 进行开发，遵循微信小程序的页面 - 逻辑 - 配置开发模式，每个页面由.wxml（页面结构）、.wxss（页面样式）、.js（页面逻辑）、.json（页面配置）四个文件组成。前端功能实现的核心是通过微信云开发的 API 与云端进行交互，将用户的操作请求发送至云函数，接收云函数返回的处理结果，并完成页面的动态渲染。</p><h2>云函数实现</h2><p>云函数是本小程序的业务逻辑处理核心，基于 Node.js 开发，核心云函数为mcloud，包含用户管理、题库管理、答题处理、竞赛管理、参数设置、数据统计六大业务模块的处理逻辑。云函数的开发遵循模块化原则，将不同的业务逻辑拆分为不同的函数，通过action参数区分前端的请求类型，实现对不同请求的处理。</p><pre><code class="c">// 得分统计
    async statAnswer(userId) { 
        let where = {
            ANSWER_USER_ID: userId,
            ANSWER_TYPE: 1
        }
        let cnt = await AnswerModel.count(where);
        let score = await AnswerModel.sum(where, 'ANSWER_SCORE');

        let data = {
            USER_ANSWER_CNT: cnt,
            USER_ANSWER_SCORE: score
        }
        await UserModel.edit({ USER_MINI_OPENID: userId }, data);
    }

    // 每日可答题次数校验
    async isAnswerTimes(userId, cateId) {
        let dayCnt = 100;
        let setup = await setupUtil.get('answer');
        if (setup) {
            setup = dataUtil.dbForms2Obj(setup);
            dayCnt = Number(setup.daycnt);

            if (setup.open != true) {
                return '竞赛尚未开始!';
            }
        }

        let where = {
            ANSWER_CATE_ID: String(cateId),
            ANSWER_USER_ID: userId,
            ANSWER_TYPE: 1,
            ANSWER_DAY: timeUtil.time('Y-M-D')
        }
        let cnt = await AnswerModel.count(where);
        if (cnt &gt;= dayCnt) {
            return '每日竞赛答题最多' + dayCnt + '次，请明日再来！';
        }

        return '';
    }

    async saveMyAnswer(userId,
     ) { 
     
    }

    // 随机N条记录，生成本次题库
    async genQuestion(userId, type, cateId) { 

        return { questionList: [], maxTime:10 };
    }


    async getMyAnswerList(userId, {
        search, // 搜索条件
        sortType, // 搜索菜单
        sortVal, // 搜索菜单
        orderBy, // 排序 
        page,
        size,
        isTotal = true,
        oldTotal
    }) {

        orderBy = orderBy || {
            'ANSWER_ADD_TIME': 'desc'
        };
        let fields = 'ANSWER_SCORE,ANSWER_CATE_NAME,ANSWER_TYPE,ANSWER_ADD_TIME,ANSWER_CNT,ANSWER_PER,ANSWER_SUCC_CNT,ANSWER_DURATION,ANSWER_START,ANSWER_END';

        let where = {};
        where.and = {
            ANSWER_USER_ID: userId,
            _pid: this.getProjectId() //复杂的查询在此处标注PID
        };

        if (util.isDefined(search) &amp;&amp; search) {
            where.or = [

            ];
        } else if (sortType &amp;&amp; util.isDefined(sortVal)) {
            // 搜索菜单
            switch (sortType) {
                case 'type': {
                    where.and.ANSWER_TYPE = Number(sortVal);
                    break;
                }
                case 'cateId': {
                    where.and.ANSWER_CATE_ID = String(sortVal);
                    break;
                }
                case 'sort': {
                    orderBy = this.fmtOrderBySort(sortVal, 'ANSWER_ADD_TIME');
                    break;
                }
            }
        }

        return await AnswerModel.getList(where, fields, orderBy, page, size, isTotal, oldTotal);
    }</code></pre><h2>git代码</h2><p><a href="https://link.segmentfault.com/?enc=V92CkP8G%2BWcBS%2BKMkoFfWQ%3D%3D.bR%2F3FP4EO%2FSx0mVt%2FH6HqcCGaWYeUerfvHoyxrcOQbcldhPjTBy0Wp%2FVUOi7%2B3FL" rel="nofollow" target="_blank">点击下载</a></p>]]></description></item><item>    <title><![CDATA[函数计算 AgentRun 重磅上线知识库功能，赋能智能体更“懂”你 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047608137</link>    <guid>https://segmentfault.com/a/1190000047608137</guid>    <pubDate>2026-02-12 19:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：靖苏</p><p>阿里云函数计算 <strong>AgentRun</strong> 正式推出全新<strong>知识库功能</strong>，为智能体（Agent）注入更强的语义理解与上下文感知能力。通过深度集成<strong>百炼知识库</strong>与 <strong>RAGFlow 知识库</strong>，AgentRun 让开发者能够轻松构建具备“知识”的智能应用，真正实现“更懂用户、更贴场景、更高效响应”。</p><h2>为什么需要知识库？</h2><p>在传统智能体开发中，模型往往依赖通用训练数据，缺乏对特定业务、私有文档或实时信息的理解能力。这导致其在面对专业领域问题、企业内部知识或个性化需求时表现受限。</p><p>AgentRun 的知识库功能正是为解决这一痛点而生——它将外部知识源无缝接入智能体运行流程，通过<strong>检索增强生成（RAG）</strong> 技术，让智能体在回答问题、执行任务时，能动态调用相关知识，大幅提升准确性、专业性与可信度。</p><h2>双引擎支持：百炼+RAGFlow，覆盖多元知识形态</h2><h3>百炼知识库绑定</h3><p>函数计算 AgentRun 可以绑定您账号下已经创建好的阿里云百炼知识库 <strong>[</strong> <strong>1]</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608139" alt="image" title="image"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“百炼”，可以多选绑定您账号下已经在阿里云百炼控制台创建好的多个知识库。填写检索配置后，点击创建知识库，即可将您的阿里云百炼知识库绑定至 AgentRun 平台。</p><h3>RAGFlow 知识库绑定</h3><p>函数计算 AgentRun 可以绑定您账号下已经创建好的 RAGFlow 知识库。如果您没有 RAGFlow 知识库，可以点击此链接（<a href="https://link.segmentfault.com/?enc=iUpFBqMSTnskNUwGWsrSsA%3D%3D.MW%2FyadxC%2FEOH8RAW%2FuQVEiMezpwMwpRlk6e2IkyMBe%2Fwed60OUcYytUyJEAvtKXrPNc2eLtUIFawWFKSPGJZaU7xtxvcyssOjz7kM8PZ2DYM75KE5QVyBGFl83ayIn6HtEXK2ZTA95wV1GNFSywgPObaruyWmaoWvqUMA1a10qa0a8u%2BUh8%2F6dw712oYEoowXMtzvqEuM%2B34Tj379bCtS7SKi8SL36auZUtCUAnS%2FZM4LZQ0qfcedr9mD4UDP9A93mEVgbEOdtpPP935BbyOLg%3D%3D" rel="nofollow" target="_blank">https://saenext.console.aliyun.com/cn-hangzhou/scene-market/m...</a> ），一键在 SAE 上创建 RAGFlow。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608140" alt="image" title="image" loading="lazy"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“RAGFlow”，填写您已部署的 RAGFlow 的 BaseURL、Dataset IDs 和 API-KEY（将其保存在凭证中）。填写检索配置后，点击创建知识库，即可将您自建的 RAGFlow 知识库绑定至 AgentRun 平台。</p><p>RAGFlow 知识库详细配置获取方式，可参考此文档：<a href="https://link.segmentfault.com/?enc=fv9Df6vGBIXkxrsEY0iTXQ%3D%3D.l7hIce0MNnwe%2F8pM0kIvM39A7xVmJ92YWj139WmYbfxKpWPzWmQLlBLr7Twqs4N41df1CbSOs5uWZO5tcAVlihXGSQmscvYPrnb1wIJPwhw%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/functioncompute/fc/knowledge-base-...</a>。</p><h2>三大集成方式，灵活适配各类开发场景</h2><p>函数计算 AgentRun 知识库功能支持快速创建集成、代码集成和 MCP 集成三种方式，满足不同技术栈和开发习惯。</p><h3>快速创建Agent集成知识库功能</h3><p>对于希望快速验证想法或加速产品迭代的团队，AgentRun 提供了<strong>低代码、可视化</strong>的知识库绑定能力。开发者只需登录 AgentRun 控制台，选择已创建的百炼或 RAGFlow 知识库，将其关联到目标智能体，并配置简单的检索参数（如返回结果数量、相似度阈值等），即可完成集成——全程无需编写一行代码。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608141" alt="image" title="image" loading="lazy"/></p><p>这一模式极大降低了技术门槛，让产品经理、运营人员甚至非技术背景的创新者也能参与智能体的构建与优化。无论是搭建内部知识问答机器人、客户自助服务助手，还是快速验证某个垂直领域的 AI 应用场景，都能在<strong>几分钟内完成部署并上线试用</strong>。</p><p><strong>代码集成知识库查询能力</strong>对于追求极致灵活性与控制力的开发者，AgentRun 提供了<strong>原生代码级知识库接入能力</strong>。您可以在代码逻辑中，调用 AgentRun SDK 的知识库检索接口，根据业务上下文动态发起检索请求，精准筛选并注入最相关的信息片段到智能体的推理流程中。您可以使用 AgentRun SDK，调用以下封装的接口，进行单知识库查询或多知识库查询。</p><pre><code>fromagentrun.knowledgebaseimportKnowledgeBase
## 获取单知识库，进行查询
knowledgebase=KnowledgeBase.get_by_name("ragflow-test")
single_kb_retrieve_result=knowledgebase.retrieve("&lt;your-query&gt;")
print(single_kb_retrieve_result)
## 获取多知识库，进行查询，支持跨供应商知识库类型检索
multi_kb_retrieve_result=KnowledgeBase.multi_retrieve(
    query="&lt;your-query&gt;",
    knowledge_base_names=["ragflow-test","&lt;your-knowledge-base-name-2&gt;"],
)
print(multi_kb_retrieve_result)</code></pre><p>同样，您可以集成 LangChain 框架，将知识库的查询能力集成在工具或上下文中。</p><pre><code>"""AgentRun 知识库智能体集成代码示例
使用前，请参考https://docs.agent.run/docs/tutorial/quick-start 配置好相应认证信息和环境变量
curl http://127.0.0.1:9000/openai/v1/chat/completions -X POST \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "什么是Serverless?"}], "stream": true}'
"""
import json
import os
from typing import Any
from langchain.agents import create_agent
import pydash
from agentrun import Config
from agentrun.integration.langchain import model
from agentrun.integration.langchain import knowledgebase_toolset
from agentrun.integration.langgraph.agent_converter import AgentRunConverter
from agentrun.knowledgebase import KnowledgeBase
from agentrun.server import AgentRequest, AgentRunServer
from agentrun.server.model import ServerConfig
from agentrun.utils.log import logger
# 请替换为您已经创建的 模型 名称
AGENTRUN_MODEL_SERVICE = os.getenv("AGENTRUN_MODEL_SERVICE", "&lt;your-model-service&gt;")
AGENTRUN_MODEL_NAME = os.getenv("AGENTRUN_MODEL_NAME", "&lt;your-model-name&gt;")
KNOWLEDGE_BASES = os.getenv("AGENTRUN_KNOWLEDGE_BASES", "ragflow-test").split(",")
if AGENTRUN_MODEL_NAME.startswith("&lt;") or not AGENTRUN_MODEL_NAME:
    raise ValueError("请将 MODEL_NAME 替换为您已经创建的模型名称")
## 加载知识库工具，知识库可以以工具的方式供Agent进行调用
knowledgebase_tools = []
if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
    knowledgebase_tools = knowledgebase_toolset(
        knowledge_base_names=KNOWLEDGE_BASES,
    )
else:
    logger.warning("KNOWLEDGE_BASES 未设置或未替换，跳过加载知识库工具。")
agent = create_agent(
    model=model(AGENTRUN_MODEL_SERVICE, model=AGENTRUN_MODEL_NAME, config=Config(timeout=180)),
    tools=[
        *knowledgebase_tools,   ## 通过工具集成知识库查询能力
    ],
    system_prompt="你是一个 AgentRun 的 AI 专家，可以通过查询知识库文档来回答用户的问题。",
)
async def invoke_agent(request: AgentRequest):
    messages = [
        {"role": msg.role, "content": msg.content}
        for msg in request.messages
    ]
    # 如果配置了知识库，查询知识库并将结果添加到上下文
    if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
        # 获取用户最新的消息内容作为查询
        user_query = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break
        if user_query:
            try:
                retrieve_result = await KnowledgeBase.multi_retrieve_async(
                    query=user_query,
                    knowledge_base_names=KNOWLEDGE_BASES,
                )
                # 直接将检索结果添加到上下文
                if retrieve_result:
                    messages.append({
                        "role": "assistant",
                        "content": json.dumps(retrieve_result, ensure_ascii=False),
                    })
            except Exception as e:
                logger.warning(f"知识库检索失败: {e}")
    input: Any = {"messages": messages}
    converter = AgentRunConverter()
    if request.stream:
        async def async_generator():
            async for event in agent.astream(input, stream_mode="updates"):
                for item in converter.convert(event):
                    yield item
        return async_generator()
    else:
        result = await agent.ainvoke(input)
        return pydash.get(result, "messages[-1].content", "")
AgentRunServer(
    invoke_agent=invoke_agent,
    config=ServerConfig(
        cors_origins=[
            "*"
        ]
    ),
).start()</code></pre><p>注意⚠️：如果您选择了 RAGFlow 的知识库，<strong>需要确保您的 Agent 运行环境和 RAGFlow 的 BaseURL 的地址处于同一网络环境下，否则 AgentRun SDK 将无法调用 RAGFlow 的 API 实现查询能力。</strong></p><p>通过代码集成，AgentRun 赋予开发者“全栈可控”的能力——既享受函数计算的弹性与免运维优势，又保留对智能体认知过程的深度掌控，真正实现“知识为我所用，逻辑由我定义”。</p><h3>MCP 集成：将知识库检索作为 Agent 的工具调用</h3><p>AgentRun 知识库率先实现“Agentic RAG”（智能体 RAG）模式——将传统静态检索升级为动态、可编程的智能体工具调用。具体而言，用户可一键将知识库发布为 MCP，使其成为大语言模型（LLM）可主动调用的工具之一。在此模式下，LLM 不再被动接收上下文，而是具备“工具使用能力”，在推理过程中自主判断何时调用 RAG、数据库查询、库存检查等工具，并基于返回结果进行多步推理与任务分解。这种机制使 RAG 从单一检索功能转变为智能体工具箱中的灵活组件，与其他工具并列协作，显著提升复杂任务的处理能力。其工作方式更贴近人类“思考—行动—反思”的认知流程：模型先分析问题，制定计划，再按需调用多个工具获取信息，最终整合结果生成答案。</p><p>进入其他 &gt;&gt; 工具管理 &gt;&gt; 工具市场，可以搜索到 <strong>“AgentRun 知识库 MCP”</strong> 工具模板，点击安装后，填写知识库名称和类型，即可将知识库的查询能力一件发布成 MCP 工具给大模型进行调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608142" alt="image" title="image" loading="lazy"/></p><p>创建完毕后，点击工具详情，即可看到集成调用的工具地址：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608143" alt="image" title="image" loading="lazy"/></p><p>基于 MCP 工具标准协议，AgentRun 支持以标准化方式对接知识库服务，实现跨平台、跨模型的上下文注入能力，保障架构的开放性与可扩展性。</p><h2>结语：从“能回答”到“真理解”，智能体正在拥有“知识之眼”</h2><p>AgentRun 知识库功能的上线，不仅是一次技术能力的升级，更标志着智能体发展迈入新阶段——从依赖通用语料的“泛化应答”，转向基于专属知识的“情境理解”。当智能体能够随时调用企业文档、行业规范、用户历史甚至实时数据，它便不再只是一个语言模型的接口，而成为一个<strong>具备领域认知、上下文记忆与决策依据的数字协作者</strong>。</p><p>未来，随着知识库的持续进化——支持多模态内容、动态更新、跨源推理——AgentRun 将进一步降低构建“有知识、有逻辑、有温度”智能体的门槛。</p><p>我们相信，真正的智能，不在于模型有多大，而在于是否“懂你所需、知你所问、信你所依”。</p><p><strong>AgentRun，正让每一个智能体，学会思考，更学会理解。</strong></p><p><strong>相关链接：</strong></p><p>[1] 阿里云百炼知识库</p><p><a href="https://link.segmentfault.com/?enc=lxliN5coN6z41szdnERlCg%3D%3D.Oa26KHsEek%2F7AykJ%2BdQYc%2BzRNh9MYL2yzdaln%2B%2FEFUnV5wEw5o6uOgeGHZiPCSNsZGlw9DB3AgrnNY1UDN4fQBnLMwYUVeU9glN%2BBwM7Ep4%3D" rel="nofollow" target="_blank">https://bailian.console.aliyun.com/cn-beijing/?admin=1&amp;tab=ap...</a></p>]]></description></item><item>    <title><![CDATA[巨人网络《超自然行动组》携手阿里云打造云原生游戏新范式 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047608151</link>    <guid>https://segmentfault.com/a/1190000047608151</guid>    <pubDate>2026-02-12 19:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>从开服第一天起，就跑在云上；</p><p>上线一年，DAU 已经突破 1000 万；</p><p>高峰期百万玩家同时在线，零重大故障。</p><p>这不是科幻，而是巨人网络与阿里云共书写的云原生实战。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608153" alt="image" title="image"/></p><h2>《超自然行动组》的云原生架构先行战略</h2><p>2025 年 1 月，巨人网络推出多人组队欢乐冒险游戏《超自然行动组》，凭借创新的“中式微恐+多人合作"的独特玩法，迅速成为现象级产品。最近，《超自然行动组》宣布 DAU 突破 1000 万，更攀升至 iOS 游戏畅销榜第四。尤为值得一提的是，自开服第一天起，<strong>这款游戏从未部署在任何物理机或传统虚拟机上——它从第一天起，就运行在云原生架构之上</strong> <strong>。</strong></p><p>对于大多数游戏公司而言，“上线即爆款” 是甜蜜的烦恼——流量洪峰来得快、退得慢，而传统架构却“笨重”：</p><ul><li>游戏服（如战斗服、房间服）部署在固定服务器，扩容需数天；</li><li>为应对峰值需长期预留资源，空闲时浪费严重；</li><li>版本更新靠脚本，灰度发布难，一出错就“全服回滚”；</li><li>日志分散、监控割裂，故障定位动辄几小时；</li><li>安全防护薄弱，易受 DDoS 攻击；</li><li>数据层瓶颈突出：战斗结算延迟、排行榜卡顿、玩家数据丢失等问题频发。</li></ul><p>《超自然行动组》团队深知：若沿用旧模式，很可能“倒在成功的路上”。</p><p>于是，他们选择了一条更难但更远的路——<strong>全面拥抱云原生</strong>。</p><p>通过 ACK（容器服务）、ESS（弹性伸缩）、网络型负载均衡 NLB、OpenKruiseGame（OKG）、SLS（日志服务）、ARMS（应用实时监控服务）、阿里云原生防护（Native Protection），以及云原生数据库 polardb 和 Redis 的深度协同，巨人网络构建了一套高弹性、高可用、低成本、智能化、高安全且高性能数据处理能力的新一代游戏基础设施，为行业树立了云原生落地的标杆。如今，随着日活跃用户（DAU）突破千万大关，这套技术体系，已经成为游戏行业“云原生转型”的标杆案例。</p><h2>高弹性×低延迟×零故障：解码&lt;超自然行动组&gt;的云原生底座</h2><p>《超自然行动组》基于阿里云 ACK 与 OpenKruiseGame（OKG）构建了业界领先的云原生游戏服架构：通过蓝绿发布与原地升级实现零停机、无感交付；通过 OKG+多 NLB 资源池，全面覆盖 BGP、电信、联通、移动等主流线路，实现多运营商网络自动化映射。结合 HPA 智能扩缩容与 OKG 优雅下线机制，在成本与用户体验间取得平衡；通过 ACK Koordinator 组件，实现 CPU Burst 与 QoS 精细化调度，显著提升集群资源利用率；并通过基础设施与业务状态的双向感知，构建起“业务语义驱动”的自动化运维闭环——真正实现了高弹性、高可用、高性能、高安全的新一代游戏后端体系。在显著降低运维压力的同时，实现了机制化、可持续的成本优化。</p><p>在网络层面，作为一款对延迟极度敏感的竞技手游，《超自然行动组》依托阿里云打造了“云边协同、三网通吃、弹性集约”的新一代云网络架构：通过 OKG 与 NLB 实现电信、联通、移动、BGP 四线并发接入，全国玩家自动匹配最优链路，并以“静态网络+动态计算”创新模式达成 50 节点/分钟的极速扩容，15 分钟内可拉起数千战斗服，彻底告别排队；同时，借助阿里云高速通道，将本地机房的账号、支付等核心系统与上海 VPC 内网直连，构建毫秒级同步、金融级安全的混合云中枢；并通过共享带宽包统一聚合公网出口，在简化运维的同时显著降本，为玩家交互与高频状态同步提供弹性“带宽蓄水池”，真正实现千万玩家同场竞技零卡顿、零等待的极致体验。</p><p>在数据层面，云原生 polardb 和 Tair（兼容 Redis）构建了弹性，稳定的玩家存档方案，支持千万级玩家高并发登录和读写，基于 polardb 云原生数据库的存算分离和弹性能力，支持游戏在活动期间自动扩展弹性，并且支持玩家数据的秒级备份和回档，大幅降低了数据库的运维成本，并且 PolarDB Serverless 支持自动扩容和缩容，能够根据用户访问量的实时变化，秒级调整计算资源。在高峰时期自动增加资源，低谷时期自动减少资源，确保社区始终运行在最佳状态。基于阿里云 Tair（兼容 Redis）支持玩家超高并发的访问，作为实时排行榜、战斗状态缓存和匹配池的核心，依托多线程与持久内存优化，单实例 QPS 超百万，实现毫秒级排名刷新、瞬时结算与断线无缝恢复。</p><p>当数百万玩家涌入《超自然行动组》，DDoS 攻击成为影响体验的关键风险。为此，巨人网络联合阿里云，基于云原生安全架构打造了一套高性能、智能化的防护体系。该方案依托阿里云原生高防能力，无需架构改造，一键接入即可实现 TB 级 DDoS 攻击的毫秒级识别与精准清洗，防护能力行业领先。即便在版本更新或大型赛事等高并发场景下，系统仍保障 99.99% 以上服务可用性，真正做到“攻击零感知、切换无中断”。面对突发流量洪峰，系统支持防御带宽自动弹性伸缩，动态调配资源，避免因容量不足导致服务中断。同时，通过集成安全事件中心，运营团队可实时监控攻击事件，分析攻击类型与特征，并结合 AI 驱动的策略建议，快速部署定制化游戏协议防护规则，显著提升响应效率与防御精准度。从高效清洗到智能决策，阿里云以“稳定、高效、安全”为核心，为《超自然行动组》构筑起坚不可摧的数字护盾，在保障千万玩家流畅竞技的同时，也为游戏行业树立了云原生安全新标杆。</p><p>对于《超自然行动组》这款主打实时互动的竞技游戏，“能跑” 只是起点，“看得清、查得准” 才是保障千万玩家流畅体验的关键。运维团队摒弃传统分散监控工具，基于阿里云日志服务 SLS 、云监控 CMS 的 Prometheus 服务、Grafana 服务，搭建起轻量、标准、深度集成的可观测体系：</p><ul><li>依托 Prometheus 实时采集百万级 PCU 下的资源水位与在线人数、匹配时长等核心业务指标，确保高并发下监控精准不丢点；</li><li>通过 SLS 统一汇聚全链路日志，支持按 RequestID / 玩家 ID 秒级还原行为路径，结合 SQL 分析与自定义规则，实现地图报错统计、异常操作追踪；</li><li>借助 Grafana 打造统一全景大盘，融合展示指标与日志数据，告警时可一键跳转 SLS 查看关联日志，实现 “指标发现问题、日志定位根因” 的闭环，将故障响应时间从小时级压缩至分钟级，充分发挥云原生可观测与协同优势。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608154" alt="image" title="image" loading="lazy"/></p><p><em>超自然云原生架构</em></p><h2>从“能跑”到“跑赢”：OKG 重塑游戏后端新范式</h2><p>当一款游戏从“能跑”走向“跑得快、跑得省、跑得稳”，背后一定有一套先进的技术底座在支撑。《超自然行动组》的故事，源于巨人网络，也属于所有正在思考“如何用云原生重构游戏后端”的开发者。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608155" alt="image" title="image" loading="lazy"/></p><p>面对全球游戏市场对高并发、低延迟及快速迭代的极致追求，OpenKruiseGame (OKG) 作为阿里云打造的“为游戏而生”的云原生游戏服管理方案，正成为推动行业架构平滑升级的核心引擎。针对游戏业务特有的异构性管理难题，OKG 提供了从精细化配置、自动化网络接入到业务状态感知的一站式管理体系。它不仅极大降低了游戏厂商的云原生转型门槛，更通过全球多地域一致性交付能力，助力开发者突破地域限制，实现业务的快速敏捷部署与全球化扩张。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608156" alt="image" title="image" loading="lazy"/></p><p>云原生，已不再是互联网应用的专属，而是下一代游戏基础设施的必然选择。</p>]]></description></item><item>    <title><![CDATA[Apache Doris 4.0.3 版本正式发布 SelectDB技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047608164</link>    <guid>https://segmentfault.com/a/1190000047608164</guid>    <pubDate>2026-02-12 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>亲爱的社区小伙伴们，<strong>Apache Doris 4.0.3 版本已正式发布。</strong>此版本新增了在 AI &amp; Search、湖仓一体、查询引擎等方面的能力，并同步进行了多项优化改进及问题修复，欢迎下载体验！</p><ul><li>GitHub 下载：<a href="https://link.segmentfault.com/?enc=x%2B76iEQfzUlDK4tMwlVjkw%3D%3D.qQenp6u67c1y9pgPJRwosQEuTX07HU%2B83Aa8171D1QLbHozOYMtKzTXZZt%2FEtlNg" rel="nofollow" target="_blank">https://github.com/apache/doris/releases</a></li><li>官网下载：<a href="https://link.segmentfault.com/?enc=yvZWOmjIS8NWVT55MOku7A%3D%3D.lISNSqnU8TiXI7c2uxFSUySg8ShLeUKgX6NSPm4HCY6GNZw6RQyb2ehZOTTnfT1b" rel="nofollow" target="_blank">https://doris.apache.org/download</a></li></ul><h2>新增功能</h2><h3>AI &amp; Search</h3><ul><li>添加倒排索引 NORMALIZER 支持</li><li>实现类似 ES 的布尔查询</li><li>为搜索函数引入 lucene 布尔模式</li></ul><h3>湖仓一体</h3><ul><li>支持通过 AwsCredentialsProviderChain 加载 Catalog 凭证</li><li>支持使用 OSSHDFS 存储的 Paimon DLF Catalog</li><li>为 Iceberg 表添加 manifest 级别缓存</li></ul><h3>查询引擎</h3><ul><li>支持 INTERVAL 函数并修复 EXPORT_SET</li><li>支持 TIME_FORMAT 函数</li><li>支持 QUANTILE_STATE_TO/FROM_BASE64 函数</li></ul><h2>优化改进</h2><ul><li>引入加载作业系统表</li><li>使视图、物化视图、生成列和别名函数能够持久化会话变量</li><li>将表查询计划操作接收的 SQL 添加到审计日志</li><li>启用流式加载记录到审计日志系统表</li><li>通过列裁剪优化复杂类型列读取</li><li>兼容 MySQL MOD 语法</li><li>为 sql_digest 生成添加动态配置</li><li>使用 Youngs-Cramer 算法实现 REGR_SLOPE/INTERCEPT 以与 PG 对齐</li></ul><h2>问题修复</h2><ul><li>修复 JdbcConnector 关闭时的 JNI 全局引用泄漏</li><li>修复由于 BE 统计信息上传不及时导致 CBO 无法稳定选择同步物化视图的问题</li><li>用默认的 JSONB null 值替换无效的 JSONB</li><li>修复由于并发删除后端导致的 OlapTableSink.createPaloNodesInfo 空指针异常</li><li>修复 FROM DUAL 错误匹配以 dual 开头的表名</li><li>修复 BE 宕机时预热取消失败的问题</li><li>修复当物化视图被 LimitAggToTopNAgg 重写但查询未被重写时物化视图重写失败的问题</li><li>修复刷新时 lastUpdateTime 未更新的问题并添加定时刷新日志</li><li>修复 hll_from_base64 输入无效时的崩溃问题</li><li>修复带表达式的加载列映射的敏感性问题</li><li>修复删除表时未删除约束相关信息的问题</li><li>修复 parquet topn 延迟物化复杂数据错误结果</li><li>始终创建数据和索引页缓存以避免空指针</li><li>修改 tablet cooldownConfLock 以减少内存占用</li><li>修复读取 parquet footer 时缺失 profile 的问题</li><li>修复 Exception::to_string 中潜在的释放后使用问题</li><li>修复浮点字段 to_string 问题</li><li>修复读取 hudi parquet 导致 BE 崩溃的问题</li><li>修复 Kerberos 认证配置检测</li><li>修复空表下的同步失败问题</li><li>修复 parquet 类型未处理 float16 的问题</li><li>修复 BM25 LENGTH_TABLE 范数解码问题</li><li>避免某些日期类函数的误报</li></ul>]]></description></item><item>    <title><![CDATA[拒绝“Demo 级”架构：基于 SAE × SLS 构建 Dify 高可用生产底座 Serverle]]></title>    <link>https://segmentfault.com/a/1190000047608003</link>    <guid>https://segmentfault.com/a/1190000047608003</guid>    <pubDate>2026-02-12 18:04:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>导读：</p><blockquote><p>在上一篇<a href="https://link.segmentfault.com/?enc=aBoL1B0q0hVMJzjmzQLP4A%3D%3D.YO79WjNtq%2FguM227YmdpHW%2FS3CdfesVxha3l%2Bj1IpzEWjMz%2B2ESlwzVboIMBwQPeHMMYnfI40P%2BMF9hnxBlUWci59DtjuHEsv3Ina%2BMwRIZ2wL1EfMNyaG3BPzdYpeOIXr%2FAcqUfqAv%2BwrtO6xKwJkiXOQGJ0Cmzftah%2BzVK7r%2B1tO02l92vK899Uv3JcuBEYcYPnxiboLET9jtXqDWduzRijJ5A4rfuvtqTHOvNzL4%3D" rel="nofollow" target="_blank">《告别数据库“膨胀”：Dify x SLS 构建高可用生产级 AI 架构》</a>中，我们深度剖析了 Dify 在大规模生产场景下数据库因日志写入而面临的性能瓶颈，并介绍了通过 SLS 插件实现“存算分离”的硬核改造方案。</p><p>然而，解决“数据存储”只是跨过了生产落地的第一道坎。面对复杂的微服务运维、波动的 AI 潮汐流量，如何构建一个弹性、免运维的“计算底座”同样关键。本文作为系列的第二篇，将视野从单一的数据架构扩展至全栈基础设施，为您介绍基于 阿里云 SAE × SLS 的终极生产级解决方案。</p><p>随着 LLM 应用的爆发式增长，Dify 以其强大的工作流编排与友好的可视化界面，正成为企业构建 AI 应用的首选。然而，当应用从本地 Demo 迈向大规模生产时，开发者常会遭遇两大“隐形”挑战：运维复杂度的剧增与数据架构的性能瓶颈。</p><p>本文将深度解析这一架构瓶颈，并介绍基于阿里云 <strong>SAE（Serverless 应用引擎）</strong> 与 <strong>SLS（日志服务）</strong> 的联合解决方案。通过“全托管算力”与“存算分离”的双轮驱动，打造一个高弹性、低成本、且具备深度数据洞察力的生产级 Dify 环境。</p></blockquote><h2>一、现状与挑战：Dify 规模化落地的架构瓶颈</h2><p>在单机 Demo 阶段，使用 Docker Compose 部署配合默认的 PostgreSQL 存储方案完全够用。但一旦进入生产环境，这两项基础设施往往最先成为性能与扩展性的瓶颈。</p><h4>运维管理复杂</h4><p>Dify 是一个由 API 服务、Worker、Web 前端、KV缓存、关系型数据库、向量数据库等多个组件构成的微服务架构。在生产环境中，这种架构给运维带来了很大挑战：</p><ul><li><strong>资源缺乏弹性</strong>：AI 应用通常具有明显的流量波峰波谷特征。若采用自建 Kubernetes 或 ECS 集群，扩容响应滞后，高峰期用户排队等待，低谷期又造成大量资源闲置，推高成本。</li><li><strong>维护成本高昂</strong>：保障高可用、配置负载均衡、处理节点故障、执行蓝绿/灰度发布等基础设施工作，不仅技术门槛高，还会大量挤占开发团队本应用于业务创新的精力。</li><li><strong>性能瓶颈明显</strong>：默认部署方式下的 QPS 能力有限，难以支撑高并发场景，尤其在推理密集型任务下容易成为系统瓶颈。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608005" alt="" title=""/></p><h4>数据库容量爆炸</h4><p>Dify 默认将所有数据（包括业务元数据和运行日志）存储在 PostgreSQL 中。随着业务量增长，“数据特征”与“存储引擎”的错配问题日益凸显：</p><ul><li><strong>日志“撑爆”数据库</strong>：Workflow 的每一次节点执行，都会完整记录输入输出、Prompt、推理过程及 Token 统计等详细信息。在生产级高并发场景下，这些数据占据了数据库绝大部分资源，导致表空间迅速膨胀。</li><li><strong>拖慢核心业务：</strong> 高频、高吞吐的日志写入会大量占用数据库连接池和 I/O 资源，严重干扰核心业务操作（如创建应用、知识库检索、对话上下文管理等），导致响应延迟、超时甚至服务不可用。</li></ul><h2><strong>二、协同赋能：SAE 与 SLS 的核心优势</strong></h2><p>为解决上述瓶颈，SAE 与 SLS 协同发力——SAE 聚焦弹性算力调度，SLS 专攻海量日志存储，共同构建高性能、高可用的 Dify 运行底座。</p><h4>SAE：极致弹性的 Dify 全托管运行环境</h4><p>SAE 不仅负责 Dify 核心微服务（API、Worker、Sandbox）的编排，更通过一键化模板集成了 Dify 运行所需的完整云生态。</p><ul><li><strong>一键全栈交付：</strong> 开发者无需手动搭建复杂环境。通过预置模板，可一键部署完整的微服务集群，并自动创建和集成连通日志服务SLS（工作流日志存储）、表格存储Tablestore（向量存储）、云数据库 Redis 版（缓存）及 RDS for PostgreSQL（元数据存储）等阿里云服务，无需逐个购买和配置，实现“开箱即是生产级”的交付体验。</li><li><strong>企业级高可用保障：</strong> 实例自动分布于多可用区，配合健康检查与自愈机制规避单点故障。支持金丝雀发布，确保在工作流频繁迭代时，流量切换平滑无感。</li><li><strong>秒级算力弹性</strong>：完美适配 AI 业务的“潮汐特征”。SAE 支持按 CPU/内存利用率或 QPS 指标进行自动扩缩容。在推理高峰期，秒级拉起 Worker 实例抗压；在业务低谷期，自动释放闲置资源，将算力成本严格控制在“有效使用”范围内。</li><li><strong>深度性能调优</strong>：SAE 对 Dify 实施了穿透代码与架构的“立体调优”，不仅在底层修补了 Redis 集群适配与慢 SQL 短板，更精准调优了运行参数并对齐了资源规格。这一全链路改造驱动吞吐量实现从 10 QPS 到 500 QPS 的 50 倍跃迁，确保 AI 响应如丝般顺滑。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608006" alt="" title="" loading="lazy"/></p><h4>SLS：支撑海量数据的“存算分离”方案</h4><p>SLS 并非简单的数据库替代品，而是专为日志场景设计的云原生基础设施。相比于 PostgreSQL，SLS 在 Dify 场景下实现了四个维度的架构升级：</p><ul><li><strong>极致存储弹性：</strong> 不同于数据库需按“峰值”预置资源，SLS 作为 Saas 化服务，天然支持秒级弹性伸缩。无论是深夜的低谷还是突发的推理洪峰，都能自适应承载，无需关心分片或容量上限。</li><li><strong>架构解耦负载隔离：</strong> 相利用追加写入特性，避免了数据库常见的随机 I/O 与锁竞争，轻松支撑万级 TPS 吞吐。同时通过将日志负载彻底剥离至云端，确保海量日志写入不影响 Dify 核心业务的响应速度。</li><li><strong>分层存储低成本留存</strong>：依托高压缩比技术，热数据实时分析，冷数据自动沉降至归档存储，可以远低于数据库 SSD 的成本满足长周期的审计与回溯需求。</li><li><strong>开箱即用的业务洞察：</strong> 内置的 OLAP 分析引擎支持 SQL 实时查询、可视化报表与告警监控，帮助开发者将沉睡的日志数据转化为直观的业务洞察。</li></ul><h2><strong>三、极简部署：1 分钟定义生产级底座</strong></h2><p>SAE 应用中心内置了深度优化的 Dify 生产级模板，通过简单的参数配置，即可一键交付一套高可用就绪的运行环境，告别繁琐的 YAML 编写与环境调试。</p><p><strong>Step 1：选择部署模板</strong></p><p>登录 SAE 控制台，进入应用中心，选择 <strong>“Dify 社区版 - Serverless 部署”</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608007" alt="" title="" loading="lazy"/></p><p><strong>Step 2：配置参数与规格选型</strong></p><p>目前提供了 Dify 高性能版、Dify 高可用版、Dify 测试版 三种模板。</p><p>如果是应对高并发生产场景，建议优先选择 <strong>Dify 高性能版</strong>，该版本专门针对 <code>api</code> 镜像以及 <code>plugin-daemon</code> 镜像做了深度优化，运行效率更高。配置过程极为精简，只需手动填写各云服务的密码并选定所属的 VPC 与子网（VSW），系统便会针对选定的云资源给出一份总预估价格，确保成本清晰透明。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608008" alt="" title="" loading="lazy"/></p><p><strong>Step 3：提交并访问服务</strong></p><p>点击提交后，系统会自动完成核心服务的部署与云资源关联。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608009" alt="" title="" loading="lazy"/></p><p>部署完成后，直接在浏览器输入控制台提供的服务地址 <code>${EXTERNAL-IP}:${PORT}</code>，即可开始您的 Dify 应用编排之旅。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608010" alt="" title="" loading="lazy"/></p><blockquote>注：当Dify启动并运行之后，SLS插件会自动创建相关的logstore和索引配置。无须手动干预，直接从SLS控制台进入对应的project，即可工作流日志进行实时的查询和分析。</blockquote><h2>四、50 倍性能跃迁：SAE 从 10 QPS 到 500 QPS 的突破之路</h2><p>Dify 社区版的默认配置仅能支撑 10 QPS，但这仅仅是起步。从“尝鲜”到 500 QPS 的生产级扩容，并非简单的堆砌服务器资源，而是一场步步惊心的“闯关游戏”。每当你试图提升吞吐量时，总会撞上新的隐形天花板——从基础的参数限制到深层的架构瓶颈。SAE 团队通过全链路压测，为您提前探明并攻克了这条晋级之路上的两大核心关卡，让高性能部署变得有迹可循。</p><h4>瓶颈一：突破 10 QPS 限制——组件并发与数据库连接的协同调优</h4><ol><li><strong>为什么默认配置只有 10 QPS？</strong></li></ol><p>Dify 社区版默认配置更多是为了方便开发者快速试用，而非为大规模生产环境设计。其核心组件 dify-api 的默认参数极为保守：</p><pre><code class="plain">SERVER_WORKER_AMOUNT（工作进程数）：1
SERVER_WORKER_CONNECTIONS（单进程最大连接数）：10</code></pre><p>这两个参数直接锁死了单节点的吞吐上限。但在生产环境中，我们不能简单地将这些参数“调大十倍”，因为应用层的并发能力提升，立即会引发下游数据库的连锁反应。</p><ol start="2"><li><strong>牵一发而动全身的“连接池”难题</strong></li></ol><p>随着 QPS 的增长，dify-api 和 dify-plugin-daemon 等组件会向 PostgreSQL 发起海量连接。如果缺乏全链路的参数协同，系统极易陷入瘫痪：</p><ul><li>连接数被打满：PostgreSQL 的总连接数是有限的，盲目增加组件并发，会导致数据库连接迅速耗尽，后续请求直接报错。</li><li>组件间的连接争抢：SQLAlchemy 连接池有“懒加载”机制，且空闲连接在过期前不会释放。如果配置不当，会出现非核心组件占用大量空闲连接，而核心组件因拿不到连接而“饥饿”的情况。</li></ul><p><strong>解决方案：经过实战验证的“生产级配置矩阵”</strong></p><p>为了避免用户陷入繁琐的参数试错循环，SAE 团队在真实生产环境下进行了多轮全链路压测。 摸索出了不同流量档位下API 并发度、数据库连接池大小与各组件资源规格之间的<strong>生产级配置清单</strong>。用户无需关心具体的参数计算，只需根据预估流量选择对应的规格档位，确保每一份算力都能转化为实际的业务吞吐量。</p><blockquote>注：压测场景并不包含代码执行（Code Sandbox）链路。dify-sandbox 组件的规格与数量请根据实际业务中代码运行的复杂度自行评估调整。</blockquote><p><a href="https://link.segmentfault.com/?enc=AMoF2WRCiWPjx57lrQK8pw%3D%3D.OzN3KaTtTh0OwyvOexYn70RMcRhRlrZgTu6dttllPaZ6j3VDCrsZKASyTIKYBpJIJl3KML1kqssxaesJuSgOJA%3D%3D" rel="nofollow" target="_blank">Dify性能优化</a></p><h4>瓶颈二：从 200 QPS 到 500 QPS —— Redis 单点瓶颈与读写分离</h4><ol><li><strong>集成 ARMS 链路追踪定位性能瓶颈</strong></li></ol><p>在将数据库连接优化、QPS 稳定提升至 200 后，系统吞吐量难以进一步提高。为定位瓶颈，SAE 团队通过 SAE 平台深度集成的 ARMS 应用监控，对 dify-plugin-daemon 组件进行链路分析——在 SAE 控制台的应用详情页点击“应用监控”，即可查看耗时最长的调用链路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608011" alt="" title="" loading="lazy"/></p><p>Trace 数据显示，下游 Redis 的 SET/DEL 操作频繁失败。SAE 团队尝试将 Redis 实例垂直扩容至最大规格（64 核），但效果甚微：QPS 仅小幅提升，SET/DEL 操作延迟却急剧升高，CPU 利用率迅速打满。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608012" alt="" title="" loading="lazy"/></p><ol start="2"><li><strong>Dify-plugin-daemon 高频读写 Redis 引发单点拥堵</strong></li></ol><p>通过代码分析发现，这是 Dify 业务逻辑与 Redis 单点架构冲突的结果：</p><ul><li>dify-plugin-daemon 在处理每次数据链路请求时，都会生成一个新的 Session ID 并写入 Redis。这种高频的写入逻辑导致 Redis 请求量居高不下。</li><li>原生架构中，所有的 Session 读写请求都全部集中在同一个 Redis 节点上。在 200+ QPS 的高并发冲击下，Redis 单线程处理能力达到极限，导致 set 和 del 等基础操作的耗时急剧增大，从而阻塞了整个请求链路。</li></ul><p><strong>解决方案：集群化改造实现读写分离</strong></p><p>为了突破单机架构限制，SAE 团队深入组件底层，对 dify-plugin-daemon 进行了集群化适配：</p><ul><li>补全集群协议：针对原生组件不支持 Redis Cluster 的问题，SAE 团队修改了底层代码，使其完整支持 Redis Cluster 协议。</li><li>实现读写分离：通过架构升级，将原本集中在单机上的海量请求分发到集群。利用集群的多节点特性，实现了流量的负载分担与读写分离。</li></ul><p>这一改造彻底解决了单点瓶颈，成功支撑业务吞吐量从 200 QPS 平滑提升至 500 QPS。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608013" alt="" title="" loading="lazy"/></p><h2>五、激活全链路数据价值：SLS 从“黑盒运行”到“深度洞察”的透视之眼</h2><p>Dify 上线后，如何评估模型的成本和性能？如何分析业务的走势？依托 SLS 强大的 OLAP 分析引擎，我们无需预先定义表结构，即可对 Dify 的工作流日志进行深度挖掘，构建覆盖“技术指标”与“业务指标”的全景仪表盘。</p><h4><strong>基础设施视角：透视 LLM 成本与性能</strong></h4><p>对于Dify的LLM节点，workflow_node_execution日志中的process_data字段中详细记录了模型的调用情况，可以用来对模型调用情况进行秒级多维分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608014" alt="" title="" loading="lazy"/></p><p><strong>场景 A：Token 消耗与成本审计</strong> 实时监控 Token 的消耗趋势，是控制 AI 成本的关键。我们可以统计输入（prompt_tokens）、输出（completion_tokens）及总 Token 数随时间的变化曲线，精准识别异常流量。</p><p>分析 SQL 示例：</p><pre><code class="plain">node_type:llm | select
  sum(
    json_extract_long(process_data, '$.usage.prompt_tokens')
  ) prompt_tokens,
  sum("process_data.usage.completion_tokens") completion_tokens,
  sum("process_data.usage.total_tokens") total_tokens,
  date_trunc('minute', __time__) t
group by
  t
order by
  t
limit
  all</code></pre><p>注：json中的字段可以在SQL中直接用json_extract_xxx函数进行提取分析，如<code>json_extract_long(process_data, '$.usage.prompt_tokens')</code>。对于常用的字段建议额外建立json子索引，然后在SQL中就可以引用对应的列名，如<code>"process_data.usage.completion_tokens"</code>，便于进行更高效的统计分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608015" alt="" title="" loading="lazy"/></p><p><strong>场景 B：首字延迟（TTFT）性能分位分析</strong> LLM 的响应速度直接影响用户体验。通过分析 <code>time_to_first_token</code> 的 P50、P90、P99 分位值，可以客观评估模型在不同负载下的响应稳定性，为模型路由或推理加速提供数据支撑。</p><p>分析 SQL 示例：</p><pre><code class="plain">node_type:llm | select
  date_format(__time__-__time__ % 60, '%m-%d %H:%i') as time,
  approx_percentile("process_data.usage.time_to_first_token", 0.25) as Latency_p25,
  approx_percentile("process_data.usage.time_to_first_token", 0.50) as Latency_p50,
  approx_percentile("process_data.usage.time_to_first_token", 0.75) as Latency_p75,
  approx_percentile("process_data.usage.time_to_first_token", 0.99) as Latency_p99,
  min("process_data.usage.time_to_first_token") as Latency_min
group by
  time
order by
  time
limit
  all</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608016" alt="" title="" loading="lazy"/></p><h4>业务运营视角：洞察用户意图与转化</h4><p>除了底层的模型指标，SLS 还能帮助我们深入理解业务逻辑。以一个“电商智能客服助手”的Dify应用为例，我们可以利用 SQL 拆解工作流节点的输入输出，辅助运营决策。</p><p><strong>场景 A：用户意图分布趋势</strong> 通过分析工作流中“意图识别”节点的输出结果，我们可以量化统计用户咨询的高频类目（如：退换货、物流查询、优惠券），并观察这些需求随时间的变化趋势，从而指导知识库的优化方向。</p><p>分析 SQL 示例：</p><pre><code class="plain">* and title: 用户意图识别 | select
  json_extract(outputs, '$.text') as "用户意图",
  count(1) as pv
group by
  "用户意图"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608017" alt="" title="" loading="lazy"/></p><p><strong>场景 B：异常诊断与漏斗分析</strong> 通过统计特定节点的错误率或特定意图的后续流转情况，构建漏斗图，快速定位导致用户流失的节点。例如，分析“商品检索”节点的空结果率，以判断是否需要扩充商品知识库。</p><p>可以通过漏斗图，分析观察工作流哪些中间节点出现异常失败的比率较高。</p><p>分析 SQL 示例：</p><pre><code class="plain">status:succeeded | select
  title,
  count(distinct workflow_run_id) cnt
group by
  title
order by
  cnt desc</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608018" alt="" title="" loading="lazy"/></p><h2>六、结语：让 AI 应用回归业务本质</h2><p>从“能用”到“好用”，Dify 的生产级落地需要坚实的基础设施支撑。SAE 与 SLS 的联合方案，不仅仅是两个云产品的简单叠加，而是通过“算力托管”与“存储解耦”的深度协同，为 Dify 带来了全栈 Serverless 化的架构质变：</p><ul><li><strong>全栈弹性：</strong> 计算层随流量秒级伸缩，存储层无惧突发吞吐，完美适配 AI 业务的“潮汐特征”。</li><li><strong>结构性降本：</strong> 彻底消除闲置资源浪费，用低成本的分层存储替代昂贵的数据库扩容，最大化 ROI。</li><li><strong>极致稳定：</strong> 全托管免运维底座配合 I/O 物理隔离，彻底消除单点故障风险与数据库性能黑洞。</li><li><strong>深度洞察：</strong> 打通从基础设施监控到业务数据分析的“黑盒”，用 Token 成本与用户意图数据反哺业务进化。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608019" alt="" title="" loading="lazy"/></p><p>通过 SAE 联合 SLS 发布的这一解决方案，Dify开发者无需再为底层的资源和架构操心，只需一次简单的配置，即可拥有一个高可用、高性能、低成本的 AI 应用环境，从而真正专注于业务创新与 Prompt 调优。</p><p><strong>立即体验：</strong> 欢迎登录<a href="https://link.segmentfault.com/?enc=kOOHvCzGOPUs0ue0w0B9jQ%3D%3D.pm6EwFMHAJgqsayWjn1MIWvpfNLL7H6jtHAnUNmvSirNg6xLn4VoM5WgrKirAjPh" rel="nofollow" target="_blank">阿里云 SAE 控制台</a>，进入应用中心，搜索 Dify 模板，勾选Dify高性能版，开启您的一键托管之旅。</p><h3>了解 Serverless 应用引擎 SAE</h3><p>阿里云 Serverless 应用引擎 SAE 是面向 AI 时代的一站式容器化应用托管平台，以“托底传统应用、加速 AI 创新”为核心理念。它简化运维、保障稳定、闲置特性降低 75% 成本，并通过 AI 智能助手提升运维效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047415441" alt="" title="" loading="lazy"/>面向 AI，SAE 集成 Dify 等主流框架，支持一键部署与弹性伸缩，在 Dify 场景中实现性能<strong>提升 50 倍、成本优化 30% 以上</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047415442" alt="" title="" loading="lazy"/></p><h4>产品优势</h4><p>凭借八年技术沉淀，SAE 入选 2025 年 Gartner 云原生魔力象限全球领导者，亚洲第一，助力企业零节点管理、专注业务创新。SAE 既是传统应用现代化的“托举平台”，也是 AI 应用规模化落地的“加速引擎”。</p><ol><li><strong>传统应用运维的“简、稳、省”优化之道</strong></li></ol><ul><li>简：零运维心智，专注业务创新</li><li>稳：企业级高可用，内置全方位保障</li><li>省：极致弹性，将成本降至可度量</li></ul><p><strong>2.加速 AI 创新：从快速探索到高效落地</strong></p><ul><li>快探索：内置 Dify、RAGFlow、OpenManus 等 热门 AI 应用模板，开箱即用，分钟级启动 POC；</li><li>稳落地：提供生产级 AI 运行时，性能优化（如 Dify 性能提升 50 倍）、无感升级、多版本管理，确保企业级可靠交付；</li><li>易集成：深度打通网关、ARMS、计量、审计等能力，助力传统应用智能化升级。</li></ul><h4>适合谁？</h4><p>✅ 创业团队：没有专职运维，需要快速上线<br/>✅ 中小企业：想降本增效，拥抱云原生<br/>✅ 大型企业：需要企业级稳定性和合规性<br/>✅ 出海企业：需要中国区 + 全球部署<br/>✅ AI创新团队：想快速落地AI应用</p><h4>了解更多</h4><p>产品详情页地址：<a href="https://link.segmentfault.com/?enc=92A0EY7XQghGuN%2Bwj8NO4w%3D%3D.i0UBIhU6hSZfiKBKnG84NvySTapuGEUTtUygB3w4zwOcOANJx5pOQK8vwY%2BJNIrB" rel="nofollow" target="_blank">https://www.aliyun.com/product/sae</a></p>]]></description></item><item>    <title><![CDATA[3天工作量压缩至30分钟，重构我的Go后端开发逻辑 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047608041</link>    <guid>https://segmentfault.com/a/1190000047608041</guid>    <pubDate>2026-02-12 18:04:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>不会吧，你不会现在写代码还是靠拼时长吧，不会吧？</p><p>我曾经也认为，优秀的后端工程师就是写得快、写得多。直到我发现，我把大量时间浪费在了配置环境、用Print查Bug、以及手动排查内存泄漏上。</p><p>真正的效率提升，不仅仅是手速变快了，今天分享8个彻底改变我开发逻辑的工具，它们把我的焦虑变成了生产力。</p><h2>ServBay：我再也不想在本地环境上浪费一秒钟</h2><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnVbJ" alt="image.png" title="image.png"/></p><p>说实话，每次接手新项目或者维护老项目，最让我头疼的不是代码逻辑，而是 <a href="https://link.segmentfault.com/?enc=8JZCaPwZS9E36xiS%2FmXG8w%3D%3D.RUgTilgmolDJ5z6TXjhDMsGBQjGkzULF75x0PSKxDwNI1B6gRFJBUdZQwHvBmKoa" rel="nofollow" target="_blank">Go 环境配置</a>。</p><p>以前为了跑一个老项目，我得去改 <code>.bash_profile</code>，去整 <code>GOPATH</code>，甚至因为版本冲突把本地环境搞得一团糟。如果是混合技术栈，比如还得跑个Java服务，那简直是灾难。</p><p>而 ServBay，它就是拯救我于水深火热中的。它能够了一键安装，多版本共存。我可以同时保留 Go 1.11 和最新的 Go 1.24。</p><p>现在，环境配置对我来说就是点一下鼠标的事。这种隔离且并存的能力，让我工作效率蹭蹭的。</p><h2>Delve：求求大家，别再用Print调试了</h2><p>曾几何时，我也是Print党。遇到Bug，就在代码里疯狂塞 <code>fmt.Println("111")</code>、<code>fmt.Println("here")</code>。</p><p>但在Go的高并发场景下，这种做法是一时爽，事后火葬场。Goroutine一多，控制台输出乱成一锅粥，根本看不出谁先谁后。</p><p><strong>Delve</strong> 能够仔细剖析正在运行的程序。</p><p>不需要修改代码，直接启动调试：</p><pre><code class="bash">dlv debug main.go</code></pre><p>遇到死锁或者逻辑诡异的地方，打个断点，直接看内存里的变量状态。它能清晰地展示出每一个Goroutine停在哪里。自从用了Delve，我解决并发Bug的时间从半天缩短到了几分钟。</p><h2>Cobra：写出让人想用的CLI工具</h2><p><img width="723" height="307" referrerpolicy="no-referrer" src="/img/bVdnVbK" alt="image.png" title="image.png" loading="lazy"/></p><p>以前写内部脚本，我总是偷懒直接解析 <code>os.Args</code>。结果就是，过了一个月，连我自己都忘了参数顺序是怎样的，同事用起来更是怨声载道。</p><p>后来我强制自己用 <strong>Cobra，</strong> 它是Kubernetes都在用的库。用它写出来的工具，天生就带有规范的帮助文档（--help）和子命令结构。</p><p>看看这个架子，写出来就显得很专业：</p><pre><code class="go">package main

import (
        "fmt"
        "github.com/spf13/cobra"
)

func main() {
        var rootCmd = &amp;cobra.Command{
                Use:   "deploy",
                Short: "一键部署工具",
                Run: func(cmd *cobra.Command, args []string) {
                        fmt.Println("正在执行部署逻辑...")
                },
        }
        // 哪怕只是个内部工具，也要像模像样
        rootCmd.Execute()
}</code></pre><p>把烂脚本变成正规军，Cobra是门槛最低的选择。</p><h2>GoVet：编译通过不代表逻辑正确</h2><p>编译器只能告诉我们语法没错，但它不管逻辑是不是弱智。</p><p>我有一次在 <code>if</code> 条件里把 <code>==</code> 写成了 <code>=</code>（虽然Go通常会报错，但在某些特定构造下容易混淆），或者在循环里错误地使用了闭包变量，导致线上数据全错。</p><p><strong>GoVet</strong> 就是为了拦截这种低级但致命的错误存在的。</p><pre><code class="bash">go vet ./...</code></pre><p>它专门扫描那些“看起来对，但执行起来会炸”的代码。比如 <code>Printf</code> 的参数类型不对，或者不可达的代码块。现在我把它做进了提交前的钩子（Pre-commit hook），不通过Vet检查的代码，根本不允许提交。</p><h2>Golangci-lint：我的全自动代码洁癖管家</h2><p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnVbL" alt="image.png" title="image.png" loading="lazy"/></p><p>团队协作最怕什么？怕每个人的代码都有自己的想法。</p><p>与其在Code Review时因为“花括号换不换行”或者“变量名太短”吵架，不如直接上 <strong>Golangci-lint</strong>。</p><p>它不是一个工具，它是一个聚合器，并行跑了50多个检查器。</p><pre><code class="bash">golangci-lint run</code></pre><p>配置好 <code>.golangci.yml</code> 后，它就是就是一个无情的检查机器。未使用的变量、过高的圈复杂度、拼写错误，它全能抓出来。它让Code Review回归到了关注业务逻辑本身，而不是纠结语法细节。</p><h2>Pprof：甚至能看清内存的毛细血管</h2><p>服务上线后CPU突然飙高，或者内存缓慢泄漏，这时候看日志是没用的。以前我只能靠猜，现在我靠 <strong>Pprof</strong>。</p><p>只需要在代码里加一行副作用引入：</p><pre><code class="go">import _ "net/http/pprof"</code></pre><p>然后启动个HTTP服务，就能通过浏览器看到程序的X光片。</p><p>我也经常用命令行来生成火焰图：</p><pre><code class="bash">go tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile</code></pre><p>哪一行代码占用了最多的CPU，哪个对象在这个瞬间分配了最多的内存，一目了然。不夸张地说，Pprof 给了我一种“上帝视角”。</p><h2>Godotenv：别把秘密写在代码里</h2><p>有些初级事故是因为把数据库密码或者AWS Key直接硬编码在代码里，然后推到了Git仓库。</p><p><strong>Godotenv</strong> 是我所有项目的标配。</p><p>开发时，我只需要在本地建一个 <code>.env</code> 文件：</p><pre><code class="plain">DB_SECRET=123456
DEBUG_MODE=true</code></pre><p>代码里直接读：</p><pre><code class="go">import "github.com/joho/godotenv"

func init() {
    // 自动加载，从此告别硬编码
    _ = godotenv.Load() 
}</code></pre><p>这样既方便本地调试，又彻底杜绝了泄密风险。</p><h2>Gosec：上线前的最后一道防线</h2><p><img width="723" height="262" referrerpolicy="no-referrer" src="/img/bVdnVbM" alt="image.png" title="image.png" loading="lazy"/></p><p>即便有了前面的工具，安全漏洞依然防不胜防。比如随机数生成器用得不安全，或者TLS配置太弱。</p><p>人工审查很难发现这些隐患，但 <strong>Gosec</strong> 可以。</p><p>它会扫描代码的抽象语法树（AST），专门寻找安全漏洞。</p><pre><code class="bash">gosec ./...</code></pre><p>它会直接甩一份报告给我，告诉我哪一行代码可能导致SQL注入，哪里的文件权限设置太宽泛。对于金融类或者对安全性要求高的项目，这是必须要跑的流程。</p><hr/><h3>低效是一种选择，而你本可以拒绝</h3><p>开发者的黄金时间极其有限。是用这仅有的精力去和环境配置搏斗、去肉眼查错，还是把它们交给工具，自己专注于构建复杂的系统逻辑？</p><p>这不只是工具的差异，这是职业生涯的加速度差异。</p><p>从今天开始，选两个装上，别让重复劳动毁了你的创造力。</p>]]></description></item><item>    <title><![CDATA[ArkUI框架运行原理与常见性能优化方案 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047608043</link>    <guid>https://segmentfault.com/a/1190000047608043</guid>    <pubDate>2026-02-12 18:03:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、ArkUI框架概述</h2><p>ArkUI是OpenHarmony生态中核心的UI渲染框架，采用声明式开发范式，支持多设备（手机、平板、PC等）多端统一开发。开发者通过ArkTS语言描述界面，框架负责组件树构建、布局测量、渲染绘制及事件处理。底层由方舟运行时引擎驱动，协同无障碍、国际化等系统能力，保障高性能与良好用户体验。</p><h2>二、开发范式与执行机制</h2><ol><li>开发范式<br/>当前ArkUI中主流的开发范式采用ArkTS声明式范式，支持多端统一UI描述。在一些需要更高性能的场景下，可以采用Native API进行开发。</li><li><p>代码执行流程<br/>ETS源码经IDE编译生成ABC中间指令文件，打包成HAP安装包。应用启动时，原能力子系统启动对应应用进程，ArkUI子系统负责组件创建与渲染，最终由图形侧执行渲染指令，完成界面展示。<br/><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdnVaP" alt="image.png" title="image.png"/></p><h2>三、渲染核心流程与状态管理</h2></li><li>组件树构建<br/>框架在运行时维护组件树的压栈与出栈，动态构建UI组件树结构。<br/><img width="208" height="312" referrerpolicy="no-referrer" src="/img/bVdnVaQ" alt="image.png" title="image.png" loading="lazy"/></li><li>布局测量与渲染绘制<br/>父节点传递约束条件，子节点自底向上计算尺寸和位置，完成布局测量。随后根据信息发送渲染指令，执行绘制操作，生成最终界面。<br/><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdnVaR" alt="image.png" title="image.png" loading="lazy"/></li><li>差异更新机制<br/>通过装饰器（如@State、@Provider）实现状态观察，观察过程识别“脏”组件，即需要更新的组件。<br/>ArkUI区分两类“脏”状态：</li><li>布局脏：影响尺寸和位置，需重新测量布局，以及判定影响范围。</li><li><p>绘制脏：仅影响样式，重绘但不重新布局。<br/><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdnVaS" alt="image.png" title="image.png" loading="lazy"/><br/>状态变更触发依赖收集，精准标记相关组件为脏，在布局过程只更新需要刷新的组件，避免造成组件树的重建。</p><h2>四、ArkUI应用开发性能优化方案</h2><p>1、创建过程优化<br/>方案1：使用组件懒加载机制，减少创建数量，提升响应速度。在滚动过程中进行数据读取和加载，使用LazyForEach仅渲染可视区域项，避免一次性数据加载过多，解决页面加载耗时长问题，关于长列表优化可以参考<a href="https://link.segmentfault.com/?enc=zWEXykSOrJ1MMbeDHplFKw%3D%3D.67%2FCyeohJ1Zpv3lE1l4ILTnSlZaOLiGGO2%2FPzLs4BBBnRGaBH14DkcthT4Jdjg0EQtVHekRCGpn1gKFTf9fT82%2FLI16%2BGI7GY4Z95oece5htSEUZ95MsLbwFWN5iyozfM7U%2Fg08j7KBBNh9D5hzku0N1hLpV8Hxy9vLhAS7%2BRhA%3D" rel="nofollow" target="_blank">长列表加载丢帧优化。</a><br/><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdnVaT" alt="image.png" title="image.png" loading="lazy"/><br/>方案2：高负载场景分帧渲染，将本来一帧内加载的数据分成多帧加载，但是分帧渲染需要开发者计算每帧中加载多少数据，操作复杂，因此在必要的情况下才推荐使用。<a href="https://link.segmentfault.com/?enc=%2FNOnEvHq229sFGBknGSvGw%3D%3D.78mVGRmEWfk9mL7N%2B3DjrNsbLGSjPWN2JpgA2pZ%2Fyoi0Pqf8Q%2FeUS0l%2BN1rS7YcaPkgyudPzQatwdTETvKnsjxl2PtwOnTAdoBwxOh6y1SOjNrjAe%2BrndKwlJ8Ls%2Bfa96RJ6HHpKhNRbMFlLweBqPAtvCovA%2B4pwD8S7rstq7zCc11HXnn%2BOSSJAjkDO53TA" rel="nofollow" target="_blank">详请可点击查看</a>。<br/><img width="723" height="519" referrerpolicy="no-referrer" src="/img/bVdnVaU" alt="image.png" title="image.png" loading="lazy"/></p></li></ol><p>2、布局过程优化<br/>方案1：精简组件数量，使用扁平化布局组件（如RelativeContainer、Grid）替代多层Column/Row嵌套，减少中间节点数量。<br/><img width="723" height="95" referrerpolicy="no-referrer" src="/img/bVdnVaV" alt="image.png" title="image.png" loading="lazy"/><br/>方案2：利用布局边界减少布局计算<br/>①对固定尺寸组件设置具体宽高，限制布局影响范围。<br/>②优先使用无状态组件@Builder替代@Component，减少状态依赖。<br/><img width="723" height="450" referrerpolicy="no-referrer" src="/img/bVdnVaW" alt="image.png" title="image.png" loading="lazy"/></p><p>3、更新过程优化<br/>复用替代重建, 利用组件复用机制，减少滑动过程中组件创建、布局开销，提升帧率。<br/><img width="408" height="352" referrerpolicy="no-referrer" src="/img/bVdnVaX" alt="image.png" title="image.png" loading="lazy"/></p><p>4、状态管理优化<br/>可以采用状态管理V2进行开发，状态管理V2相对于状态管理V1优化了更新方式，由V1的对象级观察，优化为属性级观察，可以降低状态更新时带来的开销。详细内容参考：<a href="https://link.segmentfault.com/?enc=qyA230qJNl4NzOAgD8B9hg%3D%3D.YTX30JTm%2FZmmuIkL8FyRMTp9VVsdjDBsA%2BOfwoA4eC517fM03X90xEEAdr19Q1u3bdyJUgT9474NMATOs%2BC%2FQgT5Z8uOOc48L8cZeSf1JMtdOTEQg%2B0Uiro6vHw8oHRTRr2ZS3SlEU8y1XgovgY5d%2F13G9fCeWmdlizyiWDYQ8c%3D" rel="nofollow" target="_blank">状态管理V2。</a></p><h2>五、工具链支持与性能分析</h2><p>推荐使用DevEco Studio内置工具：</p><ul><li>AppAnalyzer：实现“体检-报告-修复”一体化流程，快速定位布局耗时及性能瓶颈。</li><li>通过工具量化指标，结合业务场景，精准实施优化策略。<br/><img width="723" height="128" referrerpolicy="no-referrer" src="/img/bVdnVaY" alt="image.png" title="image.png" loading="lazy"/></li><li>ArkUI Inspector：用于可视化的展示UI组件树，分析UI的布局层次和参数。使用方法可以参考ArkUI Inspector使用说明</li><li>CPU Profiler：Profiler：用于在运行过程中抓取trace和调用栈对耗时点进行分析，使用方法可以参考CPU Profiler的使用指导分析的思路可以参考常用Trace的含义。</li></ul><h2>六、性能标准与实践建议</h2><ul><li>帧率要求：120fps设备单帧耗时≤8ms，90fps设备单帧耗时≤12ms。</li><li>响应速度：页面跳转及交互反馈延迟需低于用户感知阈值，保证流畅体验。<br/>实践中应结合懒加载、分帧渲染、组件复用、扁平化布局及状态管理优化等多种手段，综合提升应用性能和用户体验。</li></ul><h2>七、总结</h2><p>ArkUI框架通过声明式开发范式和高效的状态管理机制，实现了灵活且高性能的UI渲染。性能优化需基于框架运行机制，结合具体业务场景，重点控制组件数量、优化更新粒度、合理利用复用与懒加载策略。借助DevEco Studio提供的丰富工具链，开发者可快速定位性能瓶颈，持续提升鸿蒙应用的流畅度和响应速度。</p><h2>八、更多参考</h2><p>1、界面渲染性能优化<br/>2、<a href="https://link.segmentfault.com/?enc=32fu7o%2B4u6Hy5b0ktVem8A%3D%3D.2G1xYHP4pg4WCEb4uqIkhP6qdllpGyu0yVyz%2FFBiwXgOgQInDok7PPUjjMWr%2Btuxh4D6tyl4L2%2B1Z9zR%2Bn%2BM3cqF1Jp04g6y3Hqo0mGZoY51Rcwmkc2shoX%2ByfRRoFuMI6ypdm5fHbqo%2BukzSbvZPjwcQcJS0P3M2UuRy2mIi%2Fw%3D" rel="nofollow" target="_blank">AppAnalyzer</a></p><p>所有人【华为专家面对面01期】ArkUI框架运行原理与常见性能优化方案 </p><p>了解ArkUI渲染的基本流程，探索通过节点优化、懒加载、预加载、组件复用等技术手段,提升列表场景下应用的流畅度，打造极致流畅的界面体验。<br/>➡️ <a href="https://link.segmentfault.com/?enc=Li57llIRbSSPSrRIEoisrw%3D%3D.se3J789EsNUFhQ%2BimpDDetFagzrn0tIz4bl7XkG%2F3l0nFYghNvnu2jXCtl84jyGm7kuLl279TuGBJKE4nS%2B65Bi3911UHhlQ0W8GqkxAllWPxYqE6SRYNUsj8D3Jo5Fy8y%2Bb98oVDfQpMoPcUn1Lgw%3D%3D" rel="nofollow" target="_blank">详情点击</a></p>]]></description></item><item>    <title><![CDATA[函数计算AgentRun重磅上线知识库功能，赋能智能体更“懂”你 Serverless ]]></title>    <link>https://segmentfault.com/a/1190000047608089</link>    <guid>https://segmentfault.com/a/1190000047608089</guid>    <pubDate>2026-02-12 18:02:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>阿里云函数计算 <strong>AgentRun</strong> 正式推出全新 <strong>知识库功能</strong>，为智能体（Agent）注入更强的语义理解与上下文感知能力。通过深度集成 <strong>百炼知识库</strong> 与 <strong>RAGFlow 知识库</strong>，AgentRun 让开发者能够轻松构建具备“知识”的智能应用，真正实现“更懂用户、更贴场景、更高效响应”。</p><h2>为什么需要知识库？</h2><p>在传统智能体开发中，模型往往依赖通用训练数据，缺乏对特定业务、私有文档或实时信息的理解能力。这导致其在面对专业领域问题、企业内部知识或个性化需求时表现受限。</p><p>AgentRun 的知识库功能正是为解决这一痛点而生——它将外部知识源无缝接入智能体运行流程，通过 <strong>检索增强生成（RAG）</strong> 技术，让智能体在回答问题、执行任务时，能动态调用相关知识，大幅提升准确性、专业性与可信度。</p><h2>双引擎支持：百炼 + RAGFlow，覆盖多元知识形态</h2><h3>百炼知识库绑定</h3><p>函数计算AgentRun可以绑定您账号下已经创建好的<a href="https://link.segmentfault.com/?enc=6GpFDXaDlCfD6eE3L2fTOA%3D%3D.MZ8cb3Qd38PjjJTpvAU6%2FVzjzPggSs2mU957a%2BVh9IR3LEZ9oXJU%2B09dxqjPb%2B8yJUyAnPn0YuM8HZkLjTuiAALOfuLpiZMP6vxtcb3JZyg%3D" rel="nofollow" target="_blank">阿里云百炼知识库</a>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608091" alt="" title=""/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“百炼”，可以多选绑定您账号下已经在阿里云百炼控制台创建好的多个知识库。填写检索配置后，点击创建知识库，即可将您的阿里云百炼知识库绑定至AgentRun平台。</p><h3>RAGFlow知识库绑定</h3><p>函数计算AgentRun可以绑定您账号下已经创建好的RAGFlow知识库。如果您没有RAGFlow知识库，可以点击<a href="https://link.segmentfault.com/?enc=T%2F8vBWl5yupaSXzBytGr8A%3D%3D.2NcMueMjf2QCDf%2BdSRN%2FFnn0OpNJt935Uucr95Kw11VzZVOJtlZ7tOWBMXyTrD5kAE2rSC5aUtO4UG%2Foor7jQuM6PpsH9Upj2Mgh28XSJLm2Xe5z2RKYBLSZ07tsvTsJY4sofyHCyvIThye8mvTMtBEmq7Cwp9K8TKFgTaXIZuJ6bJCnQYpp%2Bc025ORVbDCO8vxPVhtXUykByZ4Ka4ZTtuCD1XlUqvWMhtp1kgZwY%2FiFWgOcV9iOUrjUtNakQlXisszI2%2BocBhFyhPO3C%2BtjQQ%3D%3D" rel="nofollow" target="_blank">此链接</a>，一键在SAE上创建RAGFlow。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608092" alt="" title="" loading="lazy"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“RAGFlow”，填写您已部署的RAGFlow的BaseURL、Dataset IDs和API-KEY（将其保存在凭证中）。填写检索配置后，点击创建知识库，即可将您自建的RAGFlow知识库绑定至AgentRun平台。</p><blockquote>RAGFlow知识库详细配置获取方式，可参考<a href="https://link.segmentfault.com/?enc=Wo4ewzBVe%2FaNyL839deYZw%3D%3D.QXRqhftrXzqbmowvHzcWYe1Eh2RixUoTE8%2BHzNFWtd74LJiTr7cM6QaVqJCyCtMsVoXsYsRnTYs9waQY0g4iX%2FchkJvTFvpsXEzU4n5gXCakOo%2BwkJtbI8A8OawN30326ekqaqwIFKYpekfBu9VDdg8QxG7RrCYo%2BM%2Fc55vtEdprFupvvou%2Fw%2Bg7J2oBdkfEdIMuilA4W0ns1aiA8jdjIA%3D%3D" rel="nofollow" target="_blank">此文档</a>。</blockquote><h2>三大集成方式，灵活适配各类开发场景</h2><p>函数计算AgentRun 知识库功能支持快速创建集成、代码集成和MCP集成三种方式，满足不同技术栈和开发习惯。</p><h3>快速创建Agent集成知识库功能</h3><p>对于希望快速验证想法或加速产品迭代的团队，AgentRun 提供了<strong>低代码、可视化</strong>的知识库绑定能力。开发者只需登录 AgentRun 控制台，选择已创建的百炼或 RAGFlow 知识库，将其关联到目标智能体，并配置简单的检索参数（如返回结果数量、相似度阈值等），即可完成集成——全程无需编写一行代码。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608093" alt="" title="" loading="lazy"/></p><p>这一模式极大降低了技术门槛，让产品经理、运营人员甚至非技术背景的创新者也能参与智能体的构建与优化。无论是搭建内部知识问答机器人、客户自助服务助手，还是快速验证某个垂直领域的 AI 应用场景，都能在<strong>几分钟内完成部署并上线试用</strong>。</p><p><strong>代码集成知识库查询能力</strong>对于追求极致灵活性与控制力的开发者，AgentRun 提供了<strong>原生代码级知识库接入能力</strong>。您可以在代码逻辑中，调用<a href="https://link.segmentfault.com/?enc=E4g0Wh71iKkmsCISQgOuFA%3D%3D.hS04OIGCDwEamgFjv6%2B6VAGdzt1Hqu72Bcwkw1DE62DmsTbm7yVwGqmTszaq98Z4B85IcK%2FTIAYMgxZzHfxAgQ%3D%3D" rel="nofollow" target="_blank">AgentRun SDK</a>的知识库检索接口，根据业务上下文动态发起检索请求，精准筛选并注入最相关的信息片段到智能体的推理流程中。您可以使用<a href="https://link.segmentfault.com/?enc=auaNfMU51K%2B%2BFzfSGTYWjA%3D%3D.%2FeQMs6w2HfP7cGPmR6siFs3jAFoa5fgx9%2Fm1BIS6O7tbKV6v1ojieoGO3qSFpiy%2B1dJvh%2BIRdyxGSM%2FceR%2FWLA%3D%3D" rel="nofollow" target="_blank">AgentRun SDK</a>，调用以下封装的接口，进行单知识库查询或多知识库查询。</p><pre><code class="python">fromagentrun.knowledgebaseimportKnowledgeBase
## 获取单知识库，进行查询
knowledgebase=KnowledgeBase.get_by_name("ragflow-test")
single_kb_retrieve_result=knowledgebase.retrieve("&lt;your-query&gt;")
print(single_kb_retrieve_result)
## 获取多知识库，进行查询，支持跨供应商知识库类型检索
multi_kb_retrieve_result=KnowledgeBase.multi_retrieve(
    query="&lt;your-query&gt;",
    knowledge_base_names=["ragflow-test","&lt;your-knowledge-base-name-2&gt;"],
)
print(multi_kb_retrieve_result)</code></pre><p>同样，您可以集成LangChain框架，将知识库的查询能力集成在工具或上下文中。</p><pre><code class="python">"""AgentRun 知识库智能体集成代码示例

使用前，请参考https://docs.agent.run/docs/tutorial/quick-start 配置好相应认证信息和环境变量

curl http://127.0.0.1:9000/openai/v1/chat/completions -X POST \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "什么是Serverless?"}], "stream": true}'
"""

import json
import os
from typing import Any

from langchain.agents import create_agent
import pydash

from agentrun import Config
from agentrun.integration.langchain import model
from agentrun.integration.langchain import knowledgebase_toolset
from agentrun.integration.langgraph.agent_converter import AgentRunConverter
from agentrun.knowledgebase import KnowledgeBase
from agentrun.server import AgentRequest, AgentRunServer
from agentrun.server.model import ServerConfig
from agentrun.utils.log import logger

# 请替换为您已经创建的 模型 名称
AGENTRUN_MODEL_SERVICE = os.getenv("AGENTRUN_MODEL_SERVICE", "&lt;your-model-service&gt;")
AGENTRUN_MODEL_NAME = os.getenv("AGENTRUN_MODEL_NAME", "&lt;your-model-name&gt;")
KNOWLEDGE_BASES = os.getenv("AGENTRUN_KNOWLEDGE_BASES", "ragflow-test").split(",")

if AGENTRUN_MODEL_NAME.startswith("&lt;") or not AGENTRUN_MODEL_NAME:
    raise ValueError("请将 MODEL_NAME 替换为您已经创建的模型名称")

## 加载知识库工具，知识库可以以工具的方式供Agent进行调用
knowledgebase_tools = []
if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
    knowledgebase_tools = knowledgebase_toolset(
        knowledge_base_names=KNOWLEDGE_BASES,
    )
else:
    logger.warning("KNOWLEDGE_BASES 未设置或未替换，跳过加载知识库工具。")

agent = create_agent(
    model=model(AGENTRUN_MODEL_SERVICE, model=AGENTRUN_MODEL_NAME, config=Config(timeout=180)),
    tools=[
        *knowledgebase_tools,   ## 通过工具集成知识库查询能力
    ],
    system_prompt="你是一个 AgentRun 的 AI 专家，可以通过查询知识库文档来回答用户的问题。",
)


async def invoke_agent(request: AgentRequest):
    messages = [
        {"role": msg.role, "content": msg.content}
        for msg in request.messages
    ]

    # 如果配置了知识库，查询知识库并将结果添加到上下文
    if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
        # 获取用户最新的消息内容作为查询
        user_query = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break

        if user_query:
            try:
                retrieve_result = await KnowledgeBase.multi_retrieve_async(
                    query=user_query,
                    knowledge_base_names=KNOWLEDGE_BASES,
                )
                # 直接将检索结果添加到上下文
                if retrieve_result:
                    messages.append({
                        "role": "assistant",
                        "content": json.dumps(retrieve_result, ensure_ascii=False),
                    })
            except Exception as e:
                logger.warning(f"知识库检索失败: {e}")

    input: Any = {"messages": messages}

    converter = AgentRunConverter()
    if request.stream:

        async def async_generator():
            async for event in agent.astream(input, stream_mode="updates"):
                for item in converter.convert(event):
                    yield item

        return async_generator()
    else:
        result = await agent.ainvoke(input)
        return pydash.get(result, "messages[-1].content", "")


AgentRunServer(
    invoke_agent=invoke_agent,
    config=ServerConfig(
        cors_origins=[
            "*"
        ]
    ),
).start()</code></pre><blockquote>注意⚠️：如果您选择了RAGFlow的知识库，<strong>需要确保您的Agent运行环境和RAGFlow的BaseURL的地址处于同一网络环境下，否则AgentRun SDK将无法调用RAGFlow的API实现查询能力。</strong></blockquote><p>通过代码集成，AgentRun 赋予开发者“全栈可控”的能力——既享受函数计算的弹性与免运维优势，又保留对智能体认知过程的深度掌控，真正实现“知识为我所用，逻辑由我定义”。</p><h3>MCP集成：将知识库检索作为Agent的工具调用</h3><p>AgentRun知识库率先实现“Agentic RAG”（智能体RAG）模式——将传统静态检索升级为动态、可编程的智能体工具调用。具体而言，用户可一键将知识库发布为MCP，使其成为大语言模型（LLM）可主动调用的工具之一。在此模式下，LLM不再被动接收上下文，而是具备“工具使用能力”，在推理过程中自主判断何时调用RAG、数据库查询、库存检查等工具，并基于返回结果进行多步推理与任务分解。这种机制使RAG从单一检索功能转变为智能体工具箱中的灵活组件，与其他工具并列协作，显著提升复杂任务的处理能力。其工作方式更贴近人类“思考—行动—反思”的认知流程：模型先分析问题，制定计划，再按需调用多个工具获取信息，最终整合结果生成答案。</p><p>进入其他 &gt;&gt; 工具管理 &gt;&gt; 工具市场，可以搜索到“<strong>AgentRun知识库MCP</strong>” 工具模板，点击安装后，填写知识库名称和类型，即可将知识库的查询能力一件发布成MCP工具给大模型进行调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608094" alt="" title="" loading="lazy"/></p><p>创建完毕后，点击工具详情，即可看到集成调用的工具地址：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608095" alt="" title="" loading="lazy"/></p><p>基于MCP工具标准协议，AgentRun 支持以标准化方式对接知识库服务，实现跨平台、跨模型的上下文注入能力，保障架构的开放性与可扩展性。</p><h2>结语：从“能回答”到“真理解”，智能体正在拥有“知识之眼”</h2><p>AgentRun 知识库功能的上线，不仅是一次技术能力的升级，更标志着智能体发展迈入新阶段——从依赖通用语料的“泛化应答”，转向基于专属知识的“情境理解”。当智能体能够随时调用企业文档、行业规范、用户历史甚至实时数据，它便不再只是一个语言模型的接口，而成为一个<strong>具备领域认知、上下文记忆与决策依据的数字协作者</strong>。</p><p>未来，随着知识库的持续进化——支持多模态内容、动态更新、跨源推理——AgentRun 将进一步降低构建“有知识、有逻辑、有温度”智能体的门槛。</p><p>我们相信，真正的智能，不在于模型有多大，而在于是否“懂你所需、知你所问、信你所依”。</p><p><strong>AgentRun，正让每一个智能体，学会思考，更学会理解。</strong></p>]]></description></item><item>    <title><![CDATA[日志成本降低 83%：云上 Elasticsearch 和 SelectDB 的基准测试及成本分析 ]]></title>    <link>https://segmentfault.com/a/1190000047608105</link>    <guid>https://segmentfault.com/a/1190000047608105</guid>    <pubDate>2026-02-12 18:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在可观测性场景中，Elasticsearch 常受限于写入性能与高昂成本。在《可观测性方案怎么选？SelectDB vs Elasticsearch vs ClickHouse》一文中提到， 在云上日志服务中，SelectDB 相比 Elasticsearch 展现出明显的性能和成本优势。为进一步探索，本文通过基准测试对比二者表现，验证 SelectDB 在日志场景下性能与成本上的显著优势。1、基准目标和方法本次测试的目的是在可观测性场景下公平比较 SelectDB 和 Elasticsearch 的实际性能和成本，并为用户提供参考数据。为尽可能做到真实和公平，我们设计了如下对比测试：测试环境：使用 腾讯云 Elasticsearch 和 SelectDB Cloud  进行测试，未进行任何针对性调优。测试数据：使用 Elasticsearch 的官方性能测试集http logs，以确保测试中立性（实际更偏向 Elasticsearch）。测试内容：写入性能、查询性能、存储空间和成本的比较，这些是可观测性场景中用户最关心的指标。测试方法：第一阶段比较相同资源下的性能第二阶段比较支持相同负载所需的成本。第二阶段超越了传统的性能测试，以验证性能优势是否能在实际用户需求中转化为成本优势，而不仅仅是一种推断。2、相同资源下的性能比较在测试的第一阶段，比较相同配置下 Elasticsearch 和 SelectDB 的性能和成本。第一步：Elasticsearch 和 SelectDB 分别购买具有相同配置（48vCPU、348GB RAM）的集群，成本分别为 18.83 元/小时 和  16.95 元/小时。（1）腾讯云 Elasticsearch（48c） 费用：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608107" alt="图片" title="图片"/><br/>（2）SelectDB Cloud（48c） 费用：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608108" alt="图片" title="图片" loading="lazy"/><br/>第二步：在 Elasticsearch 中创建索引，并在 SelectDB Cloud 中创建表。为确保公平性，两个系统使用相同的模式，包括字段类型、索引类型、共享/分片数量等。需要注意的是，Elasticsearch 的索引大致对应于 SelectDB 的表。第三步：将相同的 HTTP 日志数据集导入到 Elasticsearch 和 SelectDB Cloud。Elasticsearch 耗时 225 秒，而 SelectDB Cloud 仅需 69 秒。SelectDB Cloud 比 Elasticsearch 快 3.3 倍。第四步：分别在 Elasticsearch 和 SelectDB Cloud 中运行 HTTP 日志测试集的查询。Elasticsearch 中的首次运行（冷查询）耗时 2.049 秒，第二次运行（热启动）耗时 1.691 秒，SelectDB Cloud 中的首次运行（冷查询）耗时 0.599 秒，第二次运行（热启动）耗时 0.52 秒。SelectDB Cloud 在冷查询和热启动时的速度均比 Elasticsearch 快 3  倍以上。第五步：分别获取 Elasticsearch 和 SelectDB Cloud 的存储空间使用情况。Elasticsearch 的存储空间使用量为 12.8GB，而 SelectDB Cloud 的存储空间使用量为 3.3GB。与 Elasticsearch 相比，SelectDB Cloud 的存储空间减少了 75%。通过本次测试可以看出，在相同配置下，SelectDB Cloud 的数据导入性能比 Elasticsearch 快 3.3 倍，查询性能快 3 倍以上，存储空间减少 75%。这意味着，在相同配置下，SelectDB Cloud 的用户将比使用 Elasticsearch 的用户获得数倍的性能提升。在可观测性场景下，用户更关心相同负载和性能下能否真正降低成本。因此，接下来的测试将验证 SelectDB Cloud 的性能优势能转化为多大的实际成本优势。3、成本突破：从性能领先到真正的成本降低在测试的第二阶段，SelectDB Cloud 将缩小至其原始规模的 1/6，与使用 6 倍资源的 Elasticsearch 进行性能比较。第一步：将 SelectDB Cloud 48vCPU 的集群规模缩减至 8vCPU，成本也大幅降低至 2.93 元/小时。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608109" alt="图片" title="图片" loading="lazy"/><br/>第二步：在仅有 8vCPU 的 SelectDB Cloud 集群中创建相同的表。第三步：将相同的 HTTP 日志数据集导入到具有 8vCPU 的 SelectDB Cloud 集群中。这一过程耗时 140 秒，速度仍比 48vCPU 的 Elasticsearch 云集群快 1.6 倍。第四步：在 8vCPU 的 SelectDB Cloud 集群中运行来自 HTTP 日志测试集的查询。第一次运行（冷查询）耗时 1.389 秒，第二次运行（热启动）耗时 1.246 秒。8 vCPU 的 SelectDB Cloud 在冷查询时比 48vCPU 的 Elasticsearch 还快 47.5%。在第五步中，获取 8vCPU SelectDB Cloud 集群中的存储空间使用情况。SelectDB Cloud 的存储空间使用量仍为 3.3GB，比 Elasticsearch 低 75%。通过本次测试可以看出，在将 SelectDB Cloud 的资源缩减至 Elasticsearch 的 1/6 后，成本仅为 2.93 元/小时，比 Elasticsearch 的 18.83 元/小时 节省了 85% 的费用。尽管成本大幅降低，但性能仍保持显著优势：数据导入性能快 1.6 倍，冷查询性能快 47.5%，存储空间减少 75%。这意味着，对于从 Elasticsearch 切换到 SelectDB Cloud 以支持相同负载的用户来说，SelectDB Cloud 将实现实打实的 83% 成本降低，并提供更好的性能。4、为什么 SelectDB 能如此显著地降低成本SelectDB Cloud 出色的性能和成本优势得益于针对可观测性场景进行的广泛优化。SelectDB 针对日志场景优化倒排索引降低空间占用，数据和索引均采用列式存储，并使用 ZSTD 压缩算法，实现了高压缩率，可大幅减少存储空间。此外，SelectDB 将所有数据存储在低成本的对象存储中，热数据在 SSD 等本地磁盘上进行缓存和加速，利用可观测性数据冷热分层的特点降低存储空间单价。这些特性使 SelectDB 的存储成本比 Elasticsearch 降低了接近一个数量级。 SelectDB 采用存储与计算分离的架构。在写入数据时，计算层仅存在一次计算消耗，避免了 Elasticsearch 存储与计算一体化架构所需的多副本。此外，SelectDB 为日志和追踪等时间序列数据设计了时间序列压缩策略，将后台数据合并的写入放大从 3 降低到 1，大幅节省计算和 IO 资源消耗。SelectDB 专为实时分析而设计，这意味着它支持高性能聚合操作，这些操作常用于可观测性领域。在搜索查询方面，SelectDB 以一种针对日志搜索和topn查询（如SELECT * FROM log WHERE message MATCH 'error' ORDER BY time DESC LIMIT 100）进行优化的方式实现了倒排索引。结果是，SelectDB 在搜索查询方面速度快 2 倍，在聚合查询方面速度快 10 倍。结论在 HTTP 日志基准工作负载下，SelectDB Cloud 与 Elasticsearch 相比实现了 83%的成本降低。在实际生产环境中，许多用户已经在 PB 规模下用 SelectDB 或 Apache Doris 取代了 Elasticsearch，实现了显著的成本节约。您可以阅读来自网易、MiniMax、领创集团、中信信用卡中心的用户故事来了解更多。我们建议您根据实际业务场景设计测试，亲自验证 SelectDB 在成本与性能上的表现。欢迎申请 SelectDB Cloud 试用验证。</p>]]></description></item><item>    <title><![CDATA[【鲲苍提效】应用链路全景透视，让性能问题无处可藏 汉得数字平台 ]]></title>    <link>https://segmentfault.com/a/1190000047607766</link>    <guid>https://segmentfault.com/a/1190000047607766</guid>    <pubDate>2026-02-12 17:05:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607769" alt="" title=""/><br/>汉得鲲苍基础架构管理平台的核心目标是为企业的异构系统提供简单高效的一站式统一闭环管理能力，包括统一资源（集群、主机、存储等）管理、统一应用及部署管理、统一监控管理、统一服务治理，帮助企业实现更快、更好、更全面的异构系统管理。</p><p>接下来我们将会提供一系列推文，介绍鲲苍平台的使用，帮助您快速了解本平台，给您更好的使用体验。</p><p>本文为系列推文的第三十一讲，将介绍如何通过鲲苍监控应用性能，在分布式系统中快速定位性能问题，大大缩短故障排查时间，高效解决性能问题！</p><h2>本篇概述</h2><p>在分布式架构时代，一次用户请求的背后，可能历经数十个服务的流转，如何快速洞察系统性能、精准定位性能瓶颈？鲲苍平台「应用性能监控（APM）」能力，为您提供从全局拓扑到代码堆栈的全链路可观测方案，让应用性能问题无处遁形。</p><h2>功能亮点：应用性能监控接入</h2><h3>1. 新建数据源</h3><p>服务可观测性/监控数据源配置：新增Skywalking类型数据源。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607770" alt="" title="" loading="lazy"/></p><h3>2. 新建应用性能监控集群</h3><p>应用性能监控/应用性能监控集群配置：关联数据源。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607771" alt="" title="" loading="lazy"/></p><h3>3. 应用性能监控接入</h3><p>查看 接入指南 ，按步骤操作：<img referrerpolicy="no-referrer" src="/img/remote/1460000047607772" alt="" title="" loading="lazy"/><br/>部署前端应用时，开启 isTrace ，例如：</p><pre><code class="bash">`ClientMonitor.register({
  accessTokenUrl： http://1.2.3.4:8080/oauth/oauth/token,
  collector:${HOPS_CLUSTER_URL}/v3/segments,
  isAjax: true,
  isTrace: true,
  namespace: '',
  clientId: '';
  clientSecret: '';
})`</code></pre><p>部署后端应用时，通过 javaagent 接入应用性能监控，例如：</p><pre><code class="bash">## 应用启动需要添加以下启动参数
-Xms1024m -Xmx1536m -javaagent:agent_path/skywalking-agent.jar -Dskywalking.agent.namespace=hops-dev -Dskywalking.agent.service_name=hops-dev:hzero-product -Dskywalking.collector.backend_service=127.0.0.1:11800</code></pre><h2>应用性能监控分析</h2><h3>1. 全景拓扑，一眼看懂服务关系</h3><p>基于真实的调用链路数据，自动绘制实时服务依赖关系图。节点颜色动态反映服务健康状态，直观呈现系统架构全貌，依赖关系一目了然。</p><ul><li>边上可查看服务间平均响应延迟，点击可查看详细的平均响应时间、平均吞吐量、平均SLA、响应时间分布等指标</li><li><p>服务实例上点击可查看服务应用性能指数（APDEX）、响应时间、吞吐量、SLA、响应时间分布等指标<img referrerpolicy="no-referrer" src="/img/remote/1460000047607773" alt="" title="" loading="lazy"/></p><h3>2. 链路追踪，穿透每一个调用环节</h3><p>从入口到数据库，完整记录请求在分布式系统中的流转路径。支持查看每个环节的耗时、状态、异常详情、SQL语句，支持多种视图灵活切换，轻松定位慢调用与异常节点。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607774" alt="" title="" loading="lazy"/></p><h3>3. 多维监控，关键指标实时掌控</h3><p><strong>全局概览</strong>：掌握集群整体响应延迟分布、吞吐量排行、慢服务/慢端点排行等。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607775" alt="" title="" loading="lazy"/></p></li></ul><p><strong>服务维度</strong>：深入查看单服务响应时间、吞吐量、SLA、Apdex满意度指数等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607776" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607777" alt="" title="" loading="lazy"/></p><p><strong>服务端点及数据库分析</strong>：分析接口性能与数据库慢查询，全面覆盖应用层到数据层。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607778" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607779" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607780" alt="" title="" loading="lazy"/></p><h3>4. 深度剖析，直击性能根源</h3><p><strong>JVM&amp;实例级深度分析</strong></p><p>针对 Java 服务，鲲苍提供实例级 JVM 健康洞察，从“现象”到“根因”，不再依赖经验猜测：</p><ul><li>CPU 使用率、GC 耗时与次数、线程状态、线程堆栈</li><li>堆内存使用情况与对象分布</li><li>MBean 详情、系统属性与运行环境信息等<img referrerpolicy="no-referrer" src="/img/remote/1460000047607781" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607782" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607783" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607784" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607785" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607786" alt="" title="" loading="lazy"/></li></ul><p><strong>服务链路性能剖析</strong><br/>通过采样跟踪与性能剖析任务，鲲苍可对指定 API 在一段时间内进行方法级堆栈分析，并以火焰图形式呈现调用链。宽而平的“平顶”函数，往往就是性能瓶颈所在，问题定位更直接、更高效。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607787" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607788" alt="" title="" loading="lazy"/></p><h3>5. 应用性能告警，防患于未然</h3><p>基于响应时间、成功率、吞吐量等核心指标，灵活配置告警规则与生效范围，实现应用性能的主动感知与提前预警，助您提前发现风险，保障系统持续稳定运行。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607789" alt="" title="" loading="lazy"/></p><h2>联系我们：</h2><ul><li>如果您想了解鲲苍更详细的功能介绍和产品信息，请登录开放平台查阅我们的产品文档</li><li>如果您有疑问，可以通过开放平台进行工单反馈，问题分类请选择【产品/汉得基础架构管理平台】</li><li>相关产品咨询或更多信息了解，欢迎联系我们。<br/>邮箱：<a href="mailto:openhand@vip.hand" target="_blank">openhand@vip.hand</a>-china.com</li></ul>]]></description></item><item>    <title><![CDATA[[开源] myclaw：2000 行 Go 平替 43 万行的 OpenClaw 荀彧9527 ]]></title>    <link>https://segmentfault.com/a/1190000047607856</link>    <guid>https://segmentfault.com/a/1190000047607856</guid>    <pubDate>2026-02-12 17:04:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>推荐API服务：<a href="https://link.segmentfault.com/?enc=hv9gh9CmlWCICWLvRHyKjQ%3D%3D.dvsRT0uATnTz2P1W4XOU0%2BXHIOfGCYkmpGm1hjvSFfU%3D" rel="nofollow" target="_blank">https://nicecode.cc/</a></p><h3>AI Agent Gateway 赛道的现状</h3><p>2026 年初，AI Agent 领域最火的项目非 OpenClaw 莫属。这个前身为 Clawdbot 🦞（后改名 Moltbot，最终定名 OpenClaw）的项目，在 GitHub 上已经积累了超过 17 万 Star。它的核心理念很直接：给 LLM 一双"手"，让 AI 能操作你的本地系统——执行命令、读写文件、控制浏览器。</p><p>OpenClaw 的架构确实强大：</p><p>• Gateway + Pi Agent：Gateway 是 Node.js WebSocket 服务（默认绑 ws://127.0.0.1:18789），内嵌 Pi（Mario Zechner 写的开源 Coding Agent）通过 JSON-RPC over stdio 做推理和工具调用<br/>• 多模型支持：通过 Pi 的统一 LLM API 接 Anthropic、OpenAI、Google、Ollama 等多家 Provider<br/>• 支持 WhatsApp、Telegram、Discord、iMessage、Slack、Signal 等消息通道<br/>• 沙箱模式、设备配对审批、加密凭据存储<br/>但它也有明显的代价：43 万行 TypeScript 代码，Node.js 运行时，以及相当复杂的依赖链。</p><p>对于只想自托管一个 AI 助手的个人开发者来说，这个体量太重了。myclaw 想做的事情很简单——用 Go 写一个够用的轻量替代。</p><h3>myclaw 是什么</h3><p>myclaw 是一个 Go 编写的自托管 AI Agent Gateway。设计目标三条：</p><ol><li>轻量：核心代码约 2000 行，单二进制部署，无运行时依赖</li><li>实用：覆盖日常场景——Telegram 和飞书双通道、定时任务、记忆持久化</li><li>可扩展：模块化架构，Channel 接口抽象清晰，加新通道写一个 struct 就行<br/>架构上借鉴了 OpenClaw 的 Gateway 模式，但实现上砍掉了所有我用不到的东西。</li></ol><p>myclaw 的整体架构可以用一句话概括：消息总线驱动的服务编排。<br/><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdnU8F" alt="image.png" title="image.png"/><br/>核心组件包括：</p><ol><li>Message Bus（消息总线）<br/>消息总线是 myclaw 的中枢。两种消息类型：</li></ol><p>• InboundMessage：从通道流入，携带 Channel、SenderID、ChatID、Content、Timestamp 等字段<br/>• OutboundMessage：从 Agent 流出，携带 Channel、ChatID、Content、ReplyTo 等字段<br/>通过 Pub/Sub 模式（SubscribeOutbound / DispatchOutbound），各服务之间实现松耦合的事件路由。缓冲区默认 100 条消息，Goroutine 安全。</p><ol start="2"><li>Gateway（网关编排器）<br/>Gateway 是顶层编排器，负责：</li></ol><p>• 组装系统 Prompt（从 AGENTS.md + SOUL.md + 记忆上下文拼接）<br/>• 处理入站消息，调用 Agent 运行时（支持 Anthropic 和 OpenAI 两种 Provider）<br/>• 将 Agent 输出路由到对应的消息通道<br/>• 处理 SIGINT / SIGTERM 优雅关闭<br/>Provider 切换的逻辑很直接——配置里 provider.type 写 openai 就走 OpenAI，其他情况默认 Anthropic。不搞什么抽象工厂，一个 switch 解决。</p><ol start="3"><li>Channel（消息通道）<br/>Channel 接口定义了四个方法：Name()、Start()、Stop()、Send()。目前实现了两个通道：</li></ol><p>Telegram 通道：</p><p>• 基于 telegram-bot-api/v5 长轮询<br/>• Markdown → Telegram HTML 格式转换<br/>• 消息分片（4096 字符限制）<br/>• 发送者白名单过滤<br/>• 代理配置支持（方便国内网络环境）<br/>飞书通道：</p><p>• Webhook 模式，启动一个 HTTP Server 监听 /feishu/webhook（默认端口 9876）<br/>• Tenant Access Token 管理，带缓存和双重检查锁<br/>• URL Verification Challenge 自动应答<br/>• 事件驱动的消息接收（im.message.receive_v1）<br/>• 发送者白名单过滤（基于 open_id）<br/>• Verification Token 校验<br/>飞书通道需要一个公网可达的 Webhook URL。本地开发可以用 Cloudflared 临时隧道，生产环境建议配 DNS。</p><ol start="4"><li>Memory（记忆系统）<br/>记忆系统分为两层：</li></ol><p>• 长期记忆（MEMORY.md）：持久化的知识库<br/>• 每日日记（YYYY-MM-DD.md）：按日期归档的交互记录<br/>提供 ReadLongTerm()、WriteLongTerm()、ReadToday()、AppendToday() 和 GetRecentMemories(days) 方法。默认取最近 7 天的日记，和长期记忆一起组装进 LLM 的系统 Prompt。</p><p>文件就是 Markdown，想手动改也行。</p><ol start="5"><li>Cron（定时任务）<br/>支持三种调度模式：</li></ol><p>• cron：标准 Cron 表达式（基于 robfig/cron/v3）<br/>• every：固定间隔（毫秒级）<br/>• at：一次性定时执行<br/>任务持久化为 JSON（存在 ~/.myclaw/data/cron/jobs.json），支持状态追踪（LastRunAtMs、LastStatus、LastError）和执行后自动删除。任务的执行结果可以通过 deliver 字段指定是否推送到某个消息通道。</p><ol start="6"><li><p>Heartbeat（心跳服务）<br/>定期读取 HEARTBEAT.md 文件内容，触发 Agent 处理。Agent 返回 HEARTBEAT_OK 表示无需进一步操作。默认间隔 30 分钟，适合做周期性自检或主动提醒。</p><h3>为什么用 Go</h3><p>选 Go 不是为了赶时髦，是几个实际的考量：</p></li><li>单二进制部署：go build 产出一个可执行文件，不需要 Node.js 运行时或 Python 虚拟环境。scp 到服务器直接跑</li><li>并发原语：Goroutine + Channel 天然适合消息总线架构。每个通道、每个定时任务、Webhook Server 都是独立的 Goroutine，代码写起来比 async/await 回调链清爽</li><li>内存占用：Go 运行时的内存开销远低于 Node.js / Python，一个长期驻留的 Gateway 进程，这点差别会累积</li><li>交叉编译：GOOS=linux GOARCH=arm64 go build 一行命令编译到任意平台</li></ol><h3>快速开始</h3><p>安装</p><pre><code>go install github.com/stellarlinkco/myclaw/cmd/myclaw@latest</code></pre><p>初始化<br/><code>myclaw onboard</code><br/>这会在 ~/.myclaw/ 下创建配置文件和工作空间：</p><pre><code>~/.myclaw/
├── config.json          # 主配置
├── workspace/
│   ├── AGENTS.md        # Agent 角色定义
│   ├── SOUL.md          # 人格特质
│   ├── HEARTBEAT.md     # 心跳任务提示词
│   └── memory/
│       └── MEMORY.md    # 长期记忆
└── data/
    └── cron/
        └── jobs.json    # 定时任务持久化</code></pre><p>配置<br/>编辑 ~/.myclaw/config.json：</p><pre><code>{
  "agent": {
    "model": "claude-sonnet-4-5-20250929",
    "maxTokens": 8192,
    "temperature": 0.7,
    "maxToolIterations": 20
  },
  "provider": {
    "type": "anthropic",
    "apiKey": "sk-ant-..."
  },
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "your-bot-token",
      "allowFrom": ["123456789"],
      "proxy": ""
    },
    "feishu": {
      "enabled": true,
      "appId": "cli_xxx",
      "appSecret": "xxx",
      "verificationToken": "xxx",
      "port": 9876,
      "allowFrom": ["ou_xxx"]
    }
  },
  "gateway": {
    "host": "0.0.0.0",
    "port": 18790
  }
}</code></pre><p>想用 OpenAI 兼容的 API？把 provider.type 改成 "openai"，填上对应的 Key 和 Base URL 就行。</p><p>也支持环境变量覆盖：<br/><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdnU8H" alt="image.png" title="image.png" loading="lazy"/><br/>一个细节：如果只设了 OPENAI_API_KEY 而没有配 provider.type，myclaw 会自动把 Provider 切到 OpenAI。少一步配置。<br/><strong>运行</strong></p><pre><code># REPL 模式（命令行交互）
myclaw agent

# 单条消息模式
myclaw agent -m "今天的任务清单"

# 完整 Gateway 模式（启动所有服务）
myclaw gateway

# 查看状态
myclaw status</code></pre><h3>部署</h3><p><strong>Docker</strong><br/>myclaw 提供了多阶段 Dockerfile（golang:1.24-alpine 构建，alpine:3.21 运行），编译产物约 10MB。</p><pre><code># 构建并启动
docker compose up -d --build

# 如果需要飞书 Webhook 的公网隧道
docker compose --profile tunnel up -d --build</code></pre><p>Docker Compose 里包含一个可选的 Cloudflared 隧道服务，通过 --profile tunnel 激活。它会自动把飞书 Webhook 端口暴露到公网，省去自己配 Nginx 反向代理的麻烦。</p><p>本地开发也可以直接用 Make：</p><p><code>make tunnel  # 启动 cloudflared 临时隧道</code><br/>拿到 *.trycloudflare.com 的 URL 后填到飞书开放平台的事件订阅里就行。<br/><strong>裸机部署</strong></p><pre><code># 交叉编译
GOOS=linux GOARCH=amd64 go build -ldflags="-s -w" -o myclaw ./cmd/myclaw

# 丢到服务器上
scp myclaw user@server:/usr/local/bin/
ssh user@server "myclaw onboard &amp;&amp; myclaw gateway"</code></pre><h3>人格定制</h3><p>myclaw 的一个有趣设计是通过 Markdown 文件定义 Agent 的"灵魂"。</p><p>AGENTS.md 定义角色和行为准则——你是谁、你能做什么、你的边界在哪里。SOUL.md 定义人格特质——语气、偏好、思维方式。这两个文件会被 Gateway 拼接到系统 Prompt 中。</p><p>这意味着你可以通过编辑两个 Markdown 文件来完全自定义 AI 助手的行为，不需要改任何代码。想要一个严肃的工作助手？改 SOUL.md。想要一个幽默的聊天伙伴？也是改 SOUL.md。</p><h3>与 OpenClaw 的对比</h3><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnU8I" alt="image.png" title="image.png" loading="lazy"/><br/>myclaw 不试图替代 OpenClaw。如果你需要多平台消息通道、完整的沙箱安全模型、Pi Agent 的 Skills 扩展体系，OpenClaw 是更好的选择。myclaw 的定位是：你只需要一个能通过 Telegram 或飞书控制的、带记忆的、能跑定时任务的 AI 助手，并且希望它是一个 10MB 的二进制文件而不是一个 Node.js 项目。</p><h3>测试</h3><p>myclaw 的测试覆盖率在 82%-100% 之间，核心模块都有单元测试：</p><p>• bus_test.go：消息总线的发布/订阅<br/>• channel_test.go：通道接口、Telegram 适配和飞书 Webhook 处理<br/>• config_test.go：配置加载和环境变量覆盖<br/>• cron_test.go：三种调度模式<br/>• gateway_test.go：服务编排和优雅关闭（90.2% 覆盖）<br/>• heartbeat_test.go：心跳触发逻辑<br/>• memory_test.go：记忆读写和上下文组装<br/>• main_test.go：CLI 命令注册<br/>使用依赖注入的 Factory 模式，测试时替换外部依赖。RuntimeFactory、BotFactory、FeishuClientFactory 这些接口让你不需要真实的 Telegram Bot 或 Anthropic API 也能跑完所有测试。</p><pre><code>make test          # 跑全部测试
make test-race     # 带竞态检测
make test-cover    # 生成覆盖率报告</code></pre><h3>安全考量</h3><p>AI Agent Gateway 的安全性不容忽视。OpenClaw 社区已经多次讨论过"投毒网页"导致的 Prompt 注入攻击问题。myclaw 采取了几个基本措施：</p><p>• 发送者白名单：Telegram 和飞书通道都支持 allowFrom 配置，只有白名单中的用户才能触发 Agent<br/>• 工具迭代上限：maxToolIterations 限制单次对话中的工具调用次数，防止 Agent 失控循环<br/>• 工作空间隔离：tools.restrictToWorkspace 默认开启，Agent 的文件操作限制在工作空间目录内<br/>• Webhook 验证：飞书通道支持 Verification Token 校验，防止伪造请求<br/>对于生产环境，建议配合 Docker 容器运行以提供进程级隔离。</p><h3>关键依赖</h3><p>myclaw 的外部依赖保持精简，直接依赖只有 4 个：</p><p>• agentsdk-go（v0.8.0）：Agent 运行时，底层包了 Anthropic SDK 和 OpenAI SDK，处理 ReAct 循环和工具调用<br/>• telegram-bot-api/v5：Telegram Bot API 客户端<br/>• robfig/cron/v3：Cron 表达式解析和调度<br/>• spf13/cobra：CLI 框架<br/>间接依赖包括 anthropic-sdk-go、openai-go、go-sdk（MCP）和 OpenTelemetry 相关的 Tracing 库。go.sum 里条目不少，但运行时真正加载的东西不多。</p><h3>我的看法</h3><p>myclaw 证明了一件事：构建一个实用的 AI Agent Gateway 不需要 43 万行代码。2000 行 Go，两个消息通道，一套记忆系统，一个 Cron 调度器——日常够用了。</p><p>当然它也有明显的不足。没有 Web UI，没有多用户会话隔离，飞书通道目前只支持纯文本消息。如果你的场景需要这些，OpenClaw 或者自己加功能。</p><p>Go 的单二进制部署和低内存占用让它特别适合丢在一台小 VPS 上长期跑着。如果你认同"能用 2000 行解决的问题不要用 43 万行"这个理念，可以试试。</p>]]></description></item><item>    <title><![CDATA[Dragonfly 论文入选 IEEE TON：AI 领域海量镜像与大模型分发的解决方案 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047607891</link>    <guid>https://segmentfault.com/a/1190000047607891</guid>    <pubDate>2026-02-12 17:04:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着生成式人工智能（AIGC）等技术不断演进，海量镜像与大模型的分发成为 AI 领域的一项关键挑战。这些挑战包括：海量分片（数百万个）、高并发拉取需求、严格的延迟要求，以及动态的网络环境等。如何在兼容 OCI 等主流格式，并且无需侵入性的实现动态、高效、可扩展的大规模镜像与模型文件分发系统，已是云原生应用与 AI 服务的迫切需求。</p><p>为了解决这些问题，蚂蚁集团与大连理工大学合作设计了一套动态、高效、可扩展的大规模镜像与模型文件分发系统。近日，由蚂蚁集团与大连理工大学共同撰写关于该系统的论文被 IEEE Transactions on Networking (TON) 期刊录用。TON 是由 IEEE 认可的高影响力学术期刊，在网络与系统领域具有重要影响力。本论文的录用标志着研究成果对行业发展具有前瞻性和创新性。 </p><h2>论文简介</h2><p>论文设计构建了一个高效、可扩展的 P2P 模型分发系统，该系统是对 CNCF 孵化项目 Dragonfly 的增强，通过多层次设计实现了资源优化与数据同步的有机结合，旨在解决传统 P2P 文件分发系统在面对 AI 大模型（如千亿参数模型）分发的特定挑战时表现不佳的问题。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607893" alt="图片" title="图片"/><br/>论文链接：<a href="https://link.segmentfault.com/?enc=3pheekG8exl8sQ59UwJ3uw%3D%3D.6wJ7RJ3%2BiKHd3EUeAWF81npkxJdMSa98hLYZRx4uYvlvTkMNfgkMSVJL3GYLEa9c" rel="nofollow" target="_blank">https://ieeexplore.ieee.org/document/11152005</a><br/>项目官网：<a href="https://link.segmentfault.com/?enc=t9OBZ%2FjMZomC2VmNyY7hxA%3D%3D.3ZQ11q3NvjcKIzn%2F18T08g%3D%3D" rel="nofollow" target="_blank">https://d7y.io</a></p><h2>技术方案与创新方法</h2><p>传统的集中式镜像/模型中心（Container/Model Registry）在并发下载高峰期常遭遇单点带宽瓶颈，导致拉取速度下降、任务延迟增加。另一方面，单纯依赖内容分发网络（CDN）或私有链路虽能缓解部分热点问题，却无法充分利用集群内部节点的空闲带宽资源，同时引入额外的成本开销。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607894" alt="图片" title="图片" loading="lazy"/><br/> 图 1: 文件分发系统架构图</p><p>应对这些问题，本方案引入了该方案引入了三个关键设计：首先，引入轻量级的网络测量机制，通过主动探测网络延迟和推断带宽，实时预测网络信息。其次，设计了可扩展的调度框架，通过将推理与调度解耦，提升了调度系统的资源利用率和响应速度。最后，Trainer 模块采用异步模型训练与推理方法，结合图学习算法，实现了基于突发性任务的增量学习。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607895" alt="图片" title="图片" loading="lazy"/><br/>图 2: 三个关键设计的调度算法</p><p>如图 2 所示，轻量级的网络测量机制确保在有限的可用网络资源下对集群中的每个节点进行高效探测。可扩展的调度框架确保足够的可用资源执行调度任务。异步模型训练和推理方法让算法结合节点特性参数进行聚合，以捕捉集群内的相似性，从而提升带宽预测效果。</p><h2>性能成果</h2><p>性能评估表明，相较于主流系统和算法，本系统在总加载完成时间上实现了至少 10% 的缩减，同时将节点平均带宽利用率提升约 20%。此外，所提出的轻量级探测机制通过减少探测频率和计算复杂度，相比现有网络探测方法有效降低了资源开销。该系统不仅能满足 AI 对大规模模型分发的高并发、低延迟需求，还能更高效地利用集群资源，希望可以为行业提供参考。</p><h2>关于我们</h2><p>蚂蚁集团容器镜像与存储团队，主要参与<br/>Dragonfly(<a href="https://link.segmentfault.com/?enc=mSRreybcrae6rT8iRQ1clg%3D%3D.4LllGiyZVV%2F%2BAzApiUvTsSQK3UOWlVrGW4E5SqTyJBitfh6MK7GMB%2F410NQgSs9J" rel="nofollow" target="_blank">https://github.com/dragonflyoss/dragonfly</a>)、<br/>Nydus(<a href="https://link.segmentfault.com/?enc=2MDMSkZNtNr2onK3oxRz3g%3D%3D.ai7%2FOpjHFygnmPdyP%2B4ZkfGgEVDx%2Bp9VxBWZ4XhJWLQGjaRcPm7%2FtBQTOTzK%2BUNq" rel="nofollow" target="_blank">https://github.com/dragonflyoss/nydus</a>)、<br/>Harbor(<a href="https://link.segmentfault.com/?enc=a%2B5SDMzVo%2B2syWBcavu6QA%3D%3D.JSplnywtnDvnfPdeoIfuz8UMOMa2%2FjI%2BKJGW66zAeKdmH7ApVo4L1pFOZTOsup%2BD" rel="nofollow" target="_blank">https://github.com/goharbor/harbor</a>)和 <br/>ModelPack(<a href="https://link.segmentfault.com/?enc=NmTuv1ZwCYMXQyaeC%2FdbTw%3D%3D.wSWqIhPRw6sXtjFq3Y1UiHRu479W9IDanaokarqMeASPnO0u5hmFVflG9mlNP%2BZS" rel="nofollow" target="_blank">https://github.com/modelpack/model-spec</a>) 等开源项目在内部的开发落地和上游项目的维护。我们致力于打造业内顶尖的容器镜像服务，并推动云原生场景下 AI 模型和镜像分发的社区标准化。</p>]]></description></item><item>    <title><![CDATA[向量数据库选型指南：Pinecone vs Weaviate vs Chroma 深度对比 ꯭꯭听꯭]]></title>    <link>https://segmentfault.com/a/1190000047607897</link>    <guid>https://segmentfault.com/a/1190000047607897</guid>    <pubDate>2026-02-12 17:03:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在AI应用开发的浪潮中，向量数据库已经成为构建智能检索、推荐系统和RAG（检索增强生成）应用的基础设施。当你的应用需要处理嵌入向量、进行语义搜索时，选择合适的向量数据库就显得至关重要。今天我们就来深入对比三款主流向量数据库：Pinecone、Weaviate和Chroma，帮助你做出明智的技术选型。</p><h2>什么是向量数据库？为什么需要它？</h2><p>在传统数据库中，我们通过精确匹配来查询数据——比如查找"张三"的订单记录。但在AI时代，我们面临的是另一个挑战：如何找到"语义相似"的内容？当用户问"如何提升工作效率"时，系统需要找到所有关于时间管理、任务规划、工具使用的相关文档，即便这些文档中并没有出现"工作效率"这四个字。</p><p>这就是向量数据库的用武之地。它将文本、图像等数据转换为高维向量（通常由嵌入模型生成），然后通过计算向量之间的距离（相似度）来实现语义搜索。想象一下，每个概念都是空间中的一个点，相似的概念会聚集在一起，向量数据库就是帮你在这个高维空间中快速找到最近邻居的工具。</p><h2>三款数据库的基因与定位</h2><h3>Pinecone：云原生的性能之选</h3><p>Pinecone诞生于2019年，是一个完全托管的云服务，它的核心理念是"让开发者专注于应用，而不是基础设施"。Pinecone团队在设计之初就瞄准了企业级性能和规模化需求，它采用了专有的优化算法，在处理数十亿级别的向量检索时依然能保持毫秒级的响应速度。</p><p>如果你正在构建一个需要处理海量数据的生产系统，比如全网商品的相似推荐、大规模文档库的智能检索，Pinecone的稳定性和性能表现会让你印象深刻。不过，这种高性能是有代价的——它是一个纯云服务，你无法在本地部署，而且定价相对较高。</p><h3>Weaviate：开源的全能战士</h3><p>Weaviate从2019年开始开源，它的野心更大——不仅仅是一个向量数据库，而是一个完整的AI原生数据库。除了向量搜索，Weaviate还支持传统的CRUD操作、复杂的过滤条件、多模态数据（文本、图像等）的混合查询。</p><p>这款数据库的架构非常灵活，你可以选择云托管，也可以在自己的服务器上部署。它内置了多种向量化模型，甚至支持在查询时实时生成嵌入向量，这对于快速原型开发非常友好。如果你的应用场景复杂，需要结合传统数据库功能和向量搜索，Weaviate会是一个理想的选择。</p><h3>Chroma：轻量级的开发者最爱</h3><p>Chroma是三者中最年轻的，2022年才推出，但它迅速在开发者社区中走红。原因很简单：它足够轻量、足够简单。Chroma的设计哲学是"嵌入即数据库"——你可以用几行Python代码就启动一个向量数据库，无需复杂的配置和部署。</p><p>对于AI应用的原型开发、小规模项目或者本地实验，Chroma简直是完美的工具。它默认使用本地持久化，也支持客户端-服务器模式。虽然在企业级功能和性能上不如前两者，但它的简洁性和易用性让初学者和独立开发者爱不释手。</p><h2>核心能力对比：谁更适合你的场景？</h2><h3>性能与规模</h3><p>在处理亿级向量的场景下，Pinecone展现出了明显的优势。它采用的近似最近邻（ANN）算法经过深度优化，查询延迟可以控制在10-50毫秒之间。Weaviate的性能也相当不错，特别是在合理配置HNSW索引参数后，可以达到相近的水平。Chroma在小规模数据（百万级）下表现良好，但当数据量突破千万级别时，性能会有明显下降。</p><p>一个典型的例子：如果你在构建一个服务百万用户的推荐系统，每天处理上亿次查询，Pinecone的稳定性和性能会给你更多信心。但如果是一个企业内部的知识库检索系统，用户基数有限，Weaviate或Chroma都能胜任。</p><h3>功能丰富度</h3><p>Weaviate在功能上是最全面的。它支持混合搜索（结合关键词和向量）、多租户隔离、复杂的GraphQL查询、自动向量化等。这意味着你可以在一个系统中同时满足传统数据库和向量数据库的需求。</p><p>Pinecone则专注于向量搜索这一核心功能，提供了元数据过滤、命名空间隔离等实用特性，但不会有传统数据库的CRUD操作。Chroma介于两者之间，提供了基础的元数据过滤和简单的查询接口，足够日常使用但不够企业级。</p><h3>部署与运维</h3><p>这是三者差异最大的地方。Pinecone是纯云服务，你无需关心任何基础设施，一个API密钥就能开始使用。这既是优势也是限制——优势在于零运维成本，限制在于你必须依赖外部服务，数据存储在第三方云上。</p><p>Weaviate提供了最大的灵活性：你可以使用官方云服务Weaviate Cloud Services（WCS），也可以通过Docker、Kubernetes自行部署。对于有数据主权要求或需要定制化配置的企业，这种灵活性至关重要。</p><p>Chroma默认是一个嵌入式数据库，可以直接在应用中启动，也支持独立的服务器模式。它的部署极其简单，甚至不需要Docker，一个Python环境就够了。</p><h3>成本考量</h3><p>成本不仅是金钱，还包括学习成本和维护成本。Pinecone按照向量存储量和查询次数计费，对于中大型应用，月度费用可能从数百到数千美元不等。但你节省了运维时间和基础设施成本。</p><p>Weaviate和Chroma都是开源的，如果自行部署，只需要承担服务器成本。Weaviate的云服务定价比Pinecone略低，而且有免费额度。Chroma完全免费，但你需要自己处理扩展性和高可用问题。</p><h2>实际应用场景建议</h2><h3>选择Pinecone，如果你：</h3><ul><li>需要处理数千万甚至数亿级别的向量数据</li><li>对查询延迟有严格要求（如实时推荐系统）</li><li>希望最小化运维工作，专注于业务逻辑</li><li>有充足的预算，愿意为性能和稳定性付费</li></ul><h3>选择Weaviate，如果你：</h3><ul><li>需要结合传统数据库功能和向量搜索</li><li>有数据隐私或本地化部署的要求</li><li>应用场景复杂，需要灵活的查询能力</li><li>希望在开源生态和企业支持之间取得平衡</li></ul><h3>选择Chroma，如果你：</h3><ul><li>正在进行原型开发或概念验证</li><li>数据规模在百万级别以内</li><li>团队规模较小，需要快速上手</li><li>预算有限，或者偏好简单的技术栈</li></ul><h2>技术演进与未来趋势</h2><p>向量数据库领域还很年轻，技术演进非常迅速。Pinecone最近推出了Serverless架构，进一步降低了使用门槛。Weaviate在多模态搜索和AI集成方面持续发力，最新版本已经支持了生成式AI模块。Chroma则在不断优化性能，缩小与成熟产品的差距。</p><p>值得注意的是，传统数据库巨头也在入场。PostgreSQL的pgvector插件、Elasticsearch的向量搜索功能都在快速成熟。选型时也可以考虑这些"混合型"方案，特别是当你已经在使用这些数据库时。</p><h2>结语</h2><p>向量数据库的选型没有绝对的对错，关键是匹配你的实际需求。如果你追求极致性能且预算充足，Pinecone是最省心的选择；如果需要功能全面且部署灵活，Weaviate值得深入研究；如果想要快速启动或控制成本，Chroma会是理想的起点。</p><p>技术选型永远是一个权衡的过程——性能、成本、灵活性、易用性，你需要在这些维度之间找到最适合自己的平衡点。建议在做最终决定前，针对你的真实数据和查询模式做一些基准测试，数据会给你最直观的答案。</p><p>记住，最好的数据库不是功能最多的，也不是性能最强的，而是最适合你的业务场景、团队能力和发展阶段的那一个。</p><hr/><h2>快速对比表格</h2><table><thead><tr><th>特性</th><th>Pinecone</th><th>Weaviate</th><th>Chroma</th></tr></thead><tbody><tr><td><strong>类型</strong></td><td>托管云服务</td><td>开源（可托管）</td><td>开源嵌入式</td></tr><tr><td><strong>推出时间</strong></td><td>2019</td><td>2019</td><td>2022</td></tr><tr><td><strong>适用规模</strong></td><td>亿级+</td><td>千万-亿级</td><td>百万-千万级</td></tr><tr><td><strong>部署方式</strong></td><td>仅云端</td><td>云端/自托管</td><td>嵌入式/服务器</td></tr><tr><td><strong>查询延迟</strong></td><td>10-50ms</td><td>20-100ms</td><td>50-200ms</td></tr><tr><td><strong>功能丰富度</strong></td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>易用性</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td><strong>成本</strong></td><td>高</td><td>中</td><td>低/免费</td></tr><tr><td><strong>企业支持</strong></td><td>是</td><td>是</td><td>社区</td></tr></tbody></table><hr/><p><strong>作者注：</strong> 本文基于2026年2月的技术现状撰写，向量数据库技术发展迅速，建议查阅各产品官方文档获取最新信息。</p><p><strong>相关资源：</strong></p><ul><li>Pinecone官网：<a href="https://link.segmentfault.com/?enc=NmB%2Bvdp%2BdQSuESM0Inq7Mg%3D%3D.j5KX%2Fqd5SXdjQRTkrgMd2OqtLlpFlqtzTLZ8BOy2Ms8%3D" rel="nofollow" target="_blank">https://www.pinecone.io</a></li><li>Weaviate官网：<a href="https://link.segmentfault.com/?enc=b8XBqqsZTteiw5%2FZ%2BCpieg%3D%3D.eouo8sv%2BtQ1UtTpTgonpGgx5sm1RGwi0lJDTo3IlyjA%3D" rel="nofollow" target="_blank">https://weaviate.io</a></li><li>Chroma官网：<a href="https://link.segmentfault.com/?enc=ul0CUjIW%2B6dBdiMKj9BitQ%3D%3D.GeCutvxvlVi3SwQR%2FbsqBzmv7ZnLfn3f8dO0ushq1G8%3D" rel="nofollow" target="_blank">https://www.trychroma.com</a></li></ul>]]></description></item><item>    <title><![CDATA[『NAS』将魂斗罗马里奥塞进NAS里 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047607908</link>    <guid>https://segmentfault.com/a/1190000047607908</guid>    <pubDate>2026-02-12 17:02:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=OIbxR326Hl964XzLpvB60A%3D%3D.MPdd3tnoyNZC3vzjW04vnrP9keghPCQZzcwb2%2Bb7WPZJS2pF%2BpeYk4IP25ZdS4rEqIJeaMHr5f2s2wF3lxqEGiBx79OncNeq6BLdv5zCvzluM0OJuXmeNZAhW4nBL2dU2Yw9iYIJXwMBjHRNz8IE3C9z7QVou2kh4JzWXfI26vU%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>JSNES 是一款怀旧游戏模拟器，无需安装任何客户端，仅通过浏览器即可运行，支持超级马里奥、魂斗罗等海量经典游戏。可部署到 NAS、服务器等设备打造本地怀旧游戏中心，完全免费无广告，轻松重温童年游戏乐趣。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607910" alt="" title=""/></p><p>本次使用飞牛 NAS 部署 JSNES，其他品牌的 NAS 部署流程也是差不多的。</p><p>在“文件管理”找到“docker”文件夹，在里面创建一个“jsnes”文件夹。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607911" alt="" title="" loading="lazy"/></p><p>打开“Docker”，切换到「Compose」面板，创建一个项目。</p><p>项目名称填 <code>jsnes</code>。</p><p>路径选择刚刚在“文件管理”里创建的 <code>/docker/jsnes</code>，具体目录根据你的 NAS 情况来填。</p><p>来源选择“创建docker-compose.yml”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607912" alt="" title="" loading="lazy"/></p><p>输入以下代码：</p><pre><code>services:
  jsnes:
    image: docker.1ms.run/wangz2019/jsnes:1.0.0
    container_name: jsnes
    ports:
      - 3456:80
    restart: always</code></pre><p>我给它配置了 <code>3456</code> 端口，你可以自定义。</p><p>等 jsnes 下载并构建完成后，切换到「容器」面板，找到 jsnes 点击这个“链接”按钮就可以在浏览器打开 jsnes 了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607913" alt="" title="" loading="lazy"/></p><p>支持键盘按键操作。 </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607914" alt="" title="" loading="lazy"/></p><p>在手机也可以玩的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607915" alt="" title="" loading="lazy"/></p><p>除了马里奥和魂斗罗之外，还有淘金者、功夫、坦克大战等众多经典游戏。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607916" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=X8nFGUn4xlbeQ0GIpX0fjQ%3D%3D.SYtNdgI8Y6CHkXVMScGnv2f4yAnjuNnyY5Xd9bbeyxrCdhXM%2FomYx254Lz6ydLkuZ7SYd7kyrmZAbPsmnaGdBs3vao7B3dI2h6VqkN%2FgNs16m%2FRW5Olz2wX0xry6b1rMs68jqhq5h0ZAFguwjMzjp6NfBk%2BsVyRJGOu77Sn%2FS20%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[拆开一看才明白：Codex 这种“本地AI写代码”到底怎么跑起来的？ 吾日三省吾码 ]]></title>    <link>https://segmentfault.com/a/1190000047607960</link>    <guid>https://segmentfault.com/a/1190000047607960</guid>    <pubDate>2026-02-12 17:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你也在折腾“让大模型在本地改代码”的 Agent，八成遇到过这种崩溃瞬间：模型说要执行命令、工具跑完回传一大坨、上下文越滚越长、性能忽高忽低、最后还把权限问题搞成“你敢让我删库我就敢执行”……😅</p><p>OpenAI 的 Codex CLI（本地跨平台软件代理）把这套流程摊开讲得很直白：核心就是 <strong>agent loop（代理循环）</strong>——一个负责“用户 ↔ 模型 ↔ 工具”编排的 harness。把 loop 设计对了，Agent 才不会像一只喝了三杯咖啡的无头苍蝇。</p><p>下面就用更接地气（但依然严谨）的方式，把 Codex 的 agent loop 彻底“拆开看看”，顺便给做 Java/Python 工程化的同学一些能直接落地的套路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607962" alt="image" title="image"/></p><h2>1）所谓 Agent Loop：其实就是“反复问、反复干、反复追加”的流水线</h2><p>Codex 这套 loop 的节奏非常规律：</p><ol><li><strong>收用户输入</strong>：把用户的话塞进要发给模型的 prompt（注意：真实 prompt 不是一段字符串，而是“多条消息/多种 item 的列表”）。</li><li><strong>模型推理（inference）</strong>：把 prompt 送到模型，让模型输出。</li><li><p><strong>分支</strong>：模型输出要么是</p><ul><li><strong>最终回复（assistant message）</strong>：这回合结束；</li><li><strong>工具调用（tool call）</strong>：比如让 agent 执行 <code>ls</code>、读文件、跑测试等。</li></ul></li><li><strong>执行工具 + 追加结果</strong>：agent 执行工具，把工具输出追加回 prompt，再请求模型下一轮推理。</li><li><strong>循环直到停止调用工具</strong>：最后必须以 assistant message 收尾（哪怕主要产出是“本地代码改动”）。</li></ol><p>一回合（turn）里可能有很多次“推理↔工具”迭代；多回合（multi-turn）则会把历史对话都带上，prompt 越滚越长：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607963" alt="image" title="image" loading="lazy"/></p><p>这也解释了为什么 Agent 工程化最容易踩的坑，永远是这两座大山：</p><ul><li><strong>性能</strong>（请求体越来越大，推理越来越贵，缓存还经常失效）</li><li><strong>上下文窗口</strong>（context window）不够用（尤其单回合工具调用特别多的时候）</li></ul><h2>2）Codex 如何“组装 prompt”：不是你以为的一段文本，而是一串分角色的 item</h2><p>Codex CLI 用的是 <a href="https://link.segmentfault.com/?enc=efFWgMQPCLDWE7U8UzagkA%3D%3D.RBZKltXaBq195AM5UHHz5wr2%2FF1QbYfClVSlE%2FZTUrLuwF7%2FOHjPywIENzkCcteX7HvyOCYjKD%2FKDAPxFMl15w%3D%3D" rel="nofollow" target="_blank">Responses API</a>，而不是让用户直接手搓 prompt。用户提交的 JSON 里最关键的三块是：</p><ul><li><code>instructions</code>：系统/开发者指令（Codex 既支持用户配置，也有模型内置 base instructions）</li><li><code>tools</code>：可调用的工具定义列表（Codex 内置 shell、plan 等，也可接 MCP 工具，甚至用 web_search）</li><li><code>input</code>：多条 item 的数组（消息、文件、图片、推理结果、工具调用/输出等都在这里）</li></ul><p>Codex 会先往 <code>input</code> 里插入一堆“铺垫项”，再追加用户的真实提问。典型的插入顺序包含：</p><ul><li><strong>developer 消息：权限/沙箱说明</strong>（只约束 Codex 自带的 shell 工具）</li><li><strong>developer 消息：用户自定义 developer_instructions（可选）</strong></li><li><strong>user 消息：用户指令聚合（可选）</strong>（例如 AGENTS.md/AGENTS.override.md、skills 等）</li><li><strong>user 消息：环境上下文</strong>（cwd、shell 等）</li></ul><p>示例（权限/沙箱说明）：</p><pre><code class="txt">&lt;permissions instructions&gt;
  - description of the sandbox explaining file permissions and network access
  - instructions for when to ask the user for permissions to run a shell command
  - list of folders writable by Codex, if any
&lt;/permissions instructions&gt;</code></pre><p>示例（环境上下文）：</p><pre><code class="txt">&lt;environment_context&gt;
  &lt;cwd&gt;/Users/mbolin/code/codex5&lt;/cwd&gt;
  &lt;shell&gt;zsh&lt;/shell&gt;
&lt;/environment_context&gt;</code></pre><p>而真正发到 Responses API 的 <code>input</code> item，是这种结构：</p><pre><code class="json">{
  "type": "message",
  "role": "user",
  "content": [
    {
      "type": "input_text",
      "text": "Add an architecture diagram to the README.md"
    }
  ]
}</code></pre><p>另外，Codex 还会把工具定义塞到 <code>tools</code> 里，长得大概这样（里面同时包含 shell、plan、web_search、以及 MCP 工具）：</p><pre><code class="javascript">[
  // Codex's default shell tool for spawning new processes locally.
  {
    "type": "function",
    "name": "shell",
    "description": "Runs a shell command and returns its output...",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {
        "command": {"type": "array", "description": "The command to execute"},
        "workdir": {"description": "The working directory..."},
        "timeout_ms": {"description": "The timeout for the command..."}
      },
      "required": ["command"]
    }
  },

  // Codex's built-in plan tool.
  {
    "type": "function",
    "name": "update_plan",
    "description": "Updates the task plan...",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {"plan": "...", "explanation": "..."},
      "required": ["plan"]
    }
  },

  // Web search tool provided by the Responses API.
  { "type": "web_search", "external_web_access": false },

  // MCP server tool (example).
  {
    "type": "function",
    "name": "mcp__weather__get-forecast",
    "description": "Get weather alerts for a US state",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {"latitude": {}, "longitude": {}},
      "required": ["latitude", "longitude"]
    }
  }
]</code></pre><p>Codex 文章里还有几张“prompt 快照”的图，直观看出服务器会把 system、tools、instructions、input 组装成最终 prompt：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607964" alt="image" title="image" loading="lazy"/></p><h2>3）流式推理 + 工具回填：SSE 事件才是“真实对话记录”</h2><p>Codex 发起一次推理，Responses API 会用 SSE（Server-Sent Events）流式返回事件；这些事件不仅用来 UI 实时输出，还会被 Codex 转成内部对象，并追加回 <code>input</code>，供下一轮推理继续使用。</p><p>示例（SSE 事件流片段）：</p><pre><code class="txt">data: {"type":"response.reasoning_summary_text.delta","delta":"ah ", ...}
data: {"type":"response.reasoning_summary_text.delta","delta":"ha!", ...}
data: {"type":"response.reasoning_summary_text.done", "item_id":...}
data: {"type":"response.output_item.added", "item":{...}}
data: {"type":"response.output_text.delta", "delta":"forty-", ...}
data: {"type":"response.output_text.delta", "delta":"two!", ...}
data: {"type":"response.completed","response":{...}}</code></pre><p>如果模型输出了 <code>function_call</code>，Codex 执行工具后会把 <strong>推理摘要、函数调用、函数输出</strong> 一股脑追加回下一次请求的 <code>input</code>，示例：</p><pre><code class="javascript">[
  /* ... original items ... */
  {
    "type": "reasoning",
    "summary": [
      {"type": "summary_text", "text": "**Adding an architecture diagram for README.md**\n\nI need to..."}
    ],
    "encrypted_content": "gAAAAABpaDWNMxMeLw..."
  },
  {
    "type": "function_call",
    "name": "shell",
    "arguments": "{\"command\":\"cat README.md\",\"workdir\":\"/Users/mbolin/code/codex5\"}",
    "call_id": "call_8675309..."
  },
  {
    "type": "function_call_output",
    "call_id": "call_8675309...",
    "output": "&lt;p align=\"center\"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;..."
  }
]</code></pre><p>对应的第二张快照图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607965" alt="image" title="image" loading="lazy"/></p><p>当模型终于输出 assistant message（不再请求工具）时，这个 turn 才算结束：</p><pre><code class="txt">data: {"type":"response.output_text.done","text":"I added a diagram to explain...", ...}
data: {"type":"response.completed","response":{...}}</code></pre><p>用户再发一句话，就进入下一 turn：需要把上一次 assistant message 和本次 user message 一起追加进去：</p><pre><code class="javascript">[
  /* ... all items from the last request ... */
  {
    "type": "message",
    "role": "assistant",
    "content": [{ "type": "output_text", "text": "I added a diagram to explain the client/server architecture." }]
  },
  {
    "type": "message",
    "role": "user",
    "content": [{ "type": "input_text", "text": "That's not bad, but the diagram is missing the bike shed." }]
  }
]</code></pre><p>第三张快照图更能说明：这玩意儿会一直长一直长……</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607966" alt="image" title="image" loading="lazy"/></p><h2>4）性能：请求体“看似二次方”，但缓存命中能救命</h2><p>很多同学看到“每次把所有历史 input 都带上”第一反应：<strong>这不就是网络传输二次方增长吗？</strong> 没错。</p><p>Responses API 支持 <code>previous_response_id</code> 来减少重复传输但Codex 当前选择不用它，主要为了两点：</p><ul><li><strong>保持请求无状态</strong>：对 API 提供方更友好。</li><li><strong>支持 ZDR（Zero Data Retention）</strong>：不在服务端存历史数据，避免与“零数据保留”冲突；同时 reasoning 的 <code>encrypted_content</code> 允许服务端解密以保留“模型理解”，但不持久化用户数据本身。</li></ul><p>真正的性能关键在于：<a href="https://link.segmentfault.com/?enc=p%2FOe5ynECiM%2B385ByZthSw%3D%3D.f3hqcwGx1q3dt61B4ETyC2%2F7s84stjSJXxFsbfdAPWv4FjfR3F33VNkTLkEVddT%2FPjAPMwPxF5Vx2KU4BvjJ9qXZ89mxW72pTwwc%2FANvRSQ%3D" rel="nofollow" target="_blank"><strong>prompt caching</strong></a>。缓存命中要求非常苛刻：<strong>必须是精确的前缀匹配</strong>。因此缓存命中要求非常苛刻：<strong>必须是精确的前缀匹配</strong>。因此) Codex 特别强调“旧 prompt 是新 prompt 的 exact prefix”，这不是强迫症，是省钱省到骨子里了😂。</p><p>哪些变化会导致 cache miss？</p><ul><li>中途改 <code>tools</code>（甚至工具顺序不稳定都会炸）</li><li>改 <code>model</code>（模型内置指令变了）</li><li>改沙箱配置、审批策略、cwd 等</li></ul><p>Codex 的一个经典教训：早期接入 MCP 工具时，因为 <strong>工具枚举顺序不一致</strong> 导致缓存频繁失效（工具数量一多，直接“性能心电图”📉）。</p><p>为了尽量保住前缀，Codex 对“中途配置变更”的处理是：<strong>追加新消息</strong>，而不是修改旧消息：</p><ul><li>沙箱配置/审批模式变化：追加新的 developer <code>&lt;permissions instructions&gt;</code></li><li>cwd 变化：追加新的 user <code>&lt;environment_context&gt;</code></li></ul><p>这招非常值得抄作业：你改的是“环境状态”，但你不能动 prompt 的“历史前缀”。</p><h2>5）上下文窗口：不够用怎么办？压缩（compaction）才是正解</h2><p>Agent 聊着聊着就爆 context window，这事不是“会不会发生”，而是“什么时候发生”。Codex 的策略是超过阈值就 <strong>compact</strong>：把冗长的 <code>input</code> 替换成一个更短、但能代表历史的 item 列表，让后续推理还能“记得发生过啥”。</p><p>早期需要手动 <code>/compact</code>；后来 Responses API 提供了专门的 compaction endpoint：<br/><a href="https://link.segmentfault.com/?enc=YF0ywUSlzB%2F283ua1y8Wfw%3D%3D.XVgEZ4niHzRNynK7Z6k7GJMJjLNK8ptYB8bgajWz3aToT60dq1J8PWKrQhO5rqCS2xj3Ss2Ya8g%2BhjRAtexnMBQ0O6iE78y6FCD%2Bd7ftM8E%3D" rel="nofollow" target="_blank">https://platform.openai.com/docs/guides/conversation-state#co...</a></p><p>它会返回一组可直接替代原 <code>input</code> 的 items，其中包含 <code>type=compaction</code> 以及不透明的 <code>encrypted_content</code>，用于保留模型的“潜在理解”。Codex 现在会在超过 <code>auto_compact_limit</code> 后自动触发这件事。</p><hr/><h2>6）给做 Java/Python 工程化 Agent 的“抄作业清单”✅</h2><p>把 Codex 的设计拆完，可以总结出几条非常硬核、非常工程的结论：</p><ol><li><strong>把 prompt 当“不可变日志”来设计</strong><br/>追加新事件，不修改旧事件。你改旧的，缓存就没了；你改多了，定位就疯了。</li><li><strong>工具列表要稳定排序 + 版本可控</strong><br/>工具顺序不稳定 ≈ 主动放弃缓存。生产环境别玩“每次启动动态扫描全部工具再随机排列”的刺激游戏。</li><li><strong>权限/沙箱必须写进 prompt</strong><br/>这不是文档，这是模型行为约束的一部分。尤其是 shell 这类高危工具，不写清楚“哪些目录可写、何时需要用户确认”，迟早出事故。</li><li><strong>流式事件要落地成结构化记录</strong><br/>SSE 不只是展示给用户看的“打字效果”，而是下一轮推理的输入材料。要能回放、能审计、能复现。</li><li><strong>上下文管理要有自动化机制</strong><br/>compaction 不是锦上添花，是续命针。阈值触发、摘要策略、加密理解保留，都是工程必须项。</li></ol><hr/><p>当 Agent loop 真正做对了，你会发现“让模型写代码”不再是玄学，更像一套可控的流水线：<strong>可追溯、可审计、可缓存、可压缩、可扩展</strong>。</p><p>而 Codex 把这套“写代码的 AI 代理”最核心的一环——loop——摊开讲清楚了：所谓智能体，很多时候并不是模型多聪明，而是 harness 多靠谱。</p><p>（Codex 开源仓库： <a href="https://link.segmentfault.com/?enc=94A8hvv6OvvNV9c9IvyDfw%3D%3D.pxrN4EG3eqtWKpGlku%2F14O8affAlxU8d7I1NO%2BL1wLw%3D" rel="nofollow" target="_blank">https://github.com/openai/codex</a> ）</p><hr/><p><strong>喜欢就奖励一个“👍”和“在看”呗~</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047106529" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[数据库数据恢复—ASM故障后Oracle数据如何起死回生？ 北亚数据恢复 ]]></title>    <link>https://segmentfault.com/a/1190000047607992</link>    <guid>https://segmentfault.com/a/1190000047607992</guid>    <pubDate>2026-02-12 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>一、Oracle数据库故障描述</strong><br/>一个Oracle数据库故障表现为ASM磁盘组掉线，ASM实例无法挂载（mount）。数据库管理员自行进行简单修复，未能成功，随后联系北亚数据恢复中心恢复数据。</p><p><strong>二、Oracle数据库故障分析方法</strong><br/>北亚企安数据恢复工程师先对底层磁盘展开分析，从组成ASM磁盘组的磁盘中提取ASM元数据作进一步研究。经分析发现，ASM存储元数据已损坏，这就是diskgroup无法挂载的原因。接着，北亚企安数据恢复工程师重组ASM存储空间，导出其中的数据库文件，再对导出的文件进行检测与恢复。若检测显示数据文件完整，后续可直接用其启动数据库；若文件也损坏，则需对底层文件进行解析和恢复。</p><p><strong>三、Oracle数据库数据恢复过程</strong><br/>1、按上述方法分析和提取底层数据，得到ASM元数据，借助其重组出ASM存储空间。<br/>2、得到ASM存储空间后，使用北亚自主开发的ASM解析工具（也可用其他常见工具或自编脚本）解析ASM结构，目的是获取ASM中的数据文件。<br/><img width="723" height="445" referrerpolicy="no-referrer" src="/img/bVc6FHj" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复"/><br/>3、对提取的Oracle数据库文件进行检测。<br/>检测结果：<br/><img width="723" height="417" referrerpolicy="no-referrer" src="/img/bVc6FHk" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/><br/>4、利用北亚自主开发的oracle数据库解析工具，解析所有数据文件中的数据记录，然后按用户需求导入到新数据库中。<br/><img width="718" height="590" referrerpolicy="no-referrer" src="/img/bVc6FHl" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/></p><p><strong>四、Oracle数据库数据恢复成功</strong><br/>通过重组ASM存储空间、对ASM磁盘底层解析，导出恢复后的数据库文件，并进一步对这些文件进行底层解析，再按用户要求将数据导入新数据库。北亚企安数据恢复工程师抽查数据表验证恢复数据，未发现异常，随后通知用户方进行全面数据验证，结果显示数据恢复完整，本次Oracle数据库数据恢复成功。<br/><img width="723" height="323" referrerpolicy="no-referrer" src="/img/bVc6FHm" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[首款 NL2GeoSQL 的测试基准和数据集来了！ 爱可生开源社区 ]]></title>    <link>https://segmentfault.com/a/1190000047607608</link>    <guid>https://segmentfault.com/a/1190000047607608</guid>    <pubDate>2026-02-12 16:09:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>专题介绍</h2><p>当我们对 AI4SQL/AI4DB/DB4AI 类产品进行研究时，我们发现 SQL 领域应用能力的提升很大程度上依赖于高质量的数据集。</p><p>还需要在此基础上进行数据合成，生成针对特定问题的训练集和评估集。为了帮助更多开发者快速获取资源，我们将近年来公开的 Text2SQL/NL2SQL 数据集进行了整理清单，持续分享给大家！</p><p>本期为系列文章的第六期，将介绍 <strong>大模型在地理空间查询 SQL 生成</strong> 和 <strong>提高 NL2SQL 精准度</strong> 方面的两款数据集：<a href="https://link.segmentfault.com/?enc=qNudtB%2BvK2GrN%2Fev2yN8RA%3D%3D.Wj2NtBKfA7Y1CowgWB45CtalYW8xZfQPN%2FVIZhSXEdGVsGSnNpf0F13op%2B4yJf75" rel="nofollow" title="GeoSQL-Eval 论文" target="_blank">GeoSQL-Eval</a> 与 <a href="https://link.segmentfault.com/?enc=5vHWvc99jJW0bFZG7gdxrg%3D%3D.Ytvwh%2BmKn8RXWhM4LTILBOi22X%2BspJZSM7aiNF2zHlOXu3SiOt4zeTDdfq0AePddCoxgBg67V233vTDBKQ6xzA%3D%3D" rel="nofollow" title="DeKeyNLU 论文" target="_blank">DeKeyNLU</a>。</p><h2>GeoSQL-Eval / GeoSQL-Bench</h2><p><a href="https://link.segmentfault.com/?enc=Nirdu%2FV188ots%2BZMMQ%2F2qw%3D%3D.xK8jU%2F4eKMwsaka5aSlMrNHVNe15YGs894xm9326nmuxJ0JBrQQVEAeOP1tR1r5rNlwbzaU%2FbljEODof7OrkuA%3D%3D" rel="nofollow" title="GeoSQL-Eval 排行榜" target="_blank">GeoSQL-Eval</a> 是首个面向 PostGIS 环境的端到端自动化评估框架，旨在衡量大型语言模型在 <strong>地理空间</strong> 数据库查询生成（GeoSQL）方面的性能。</p><p>该研究还包括发布 <strong>GeoSQL-Bench 基准测试数据集</strong>，其中包含 14,178 个实例、340 个 PostGIS 函数和 82 个专题数据库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607610" alt="image2026-2-2 10_41_56.png" title="image2026-2-2 10_41_56.png"/></p><h3>论文意图</h3><p>本文主要针对现有大型语言模型在生成 PostGIS 空间查询（GeoSQL）方面的性能评估难题，探讨如何系统地衡量这些模型的性能，因为目前 <strong>缺乏专门的评估基准和框架</strong>。传统的 NL2SQL 基准测试无法涵盖空间数据类型、函数和坐标系等复杂元素，导致在实际应用场景中出现函数错觉和参数误用等错误。</p><p>为了解决这一问题，论文提出了：</p><ul><li><strong>GeoSQL-Bench 基准测试</strong></li><li><strong>GeoSQL-Eval 评估框架</strong></li></ul><p>这些框架旨在为 <strong>NL2GeoSQL</strong> 任务建立一个标准化、多层次且可执行的评估系统，支持模型能力诊断和优化，并降低不同领域用户使用空间数据库的门槛。</p><h3>数据集分析</h3><p><strong>GeoSQL-Bench 数据集</strong> 采用多源结构化方法构建，涵盖三种类型的任务：</p><ol><li><strong>多项选择题和判断题</strong>（2380 道），基于 PostGIS 3.5 官方手册，测试函数功能、参数顺序、返回类型以及是否符合规范；</li><li><strong>语法级 SQL 生成题</strong>（3744 道），源自手册示例，包含显式提示和欠规范提示，验证模型生成可执行查询的能力；</li><li><strong>表结构检索题</strong>（2155 道），基于使用联合国全球地理信息管理 (UN GGIM) 主题和 ISO 19115 分类构建的包含 82 个真实场景的空间数据库，要求模型使用表结构生成复杂查询。</li></ol><p>所有任务均在 GPT-4o 的辅助下生成，并经过领域专家的三重审核，以确保准确性、多样性和真实性。</p><h2>小结</h2><p>本研究使用 <strong>GeoSQL-Eval 框架</strong> 系统地评估了六大类共 24 个主流模型。</p><p>实验表明，推理增强型模型（例如 GPT-5 和 o4-mini）在复杂的空间查询和多轮查询生成方面表现出色，尤其是在几何任务中展现出显著的准确率优势。通用非推理模型（例如 Claude3.7-Sonnet）在执行效率和语法正确性方面表现更佳。然而，函数调用和参数匹配错误仍然是核心瓶颈，约占 70%，而表结构检索任务由于多表连接逻辑的复杂性而面临最大挑战。</p><p>这项工作建立了首个针对 NL2GeoSQL 任务的标准化评估系统，为自然语言与空间数据库的交互提供了关键的基准和优化方向。</p><h2>DeKeyNLU</h2><p><a href="https://link.segmentfault.com/?enc=aRdRLtcbNQ4l0%2FmLlkvhhw%3D%3D.kDLiI7aS1VOPf51mwdw%2BJb%2BKMZ8Zqt3RcwVqYG5L3Yat215hrgcEl%2Bv30Eb7%2Fv8q" rel="nofollow" title="DeKeyNLU 数据集" target="_blank">DeKeyNLU</a> 通过三层人工交叉验证，实现了任务分解和关键词提取的联合细粒度标注。在此基础上，DeKeySQL 框架创新性地将一个专门的理解模块深度集成到 RAG（结果生成）过程中，建立了一种 “<strong>优先考虑精确语义解析</strong>” 的新范式，<strong>显著提高了复杂查询 SQL 生成的准确性和领域适应性。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607611" alt="image2026-2-3 14_41_56 (1).png" title="image2026-2-3 14_41_56 (1).png" loading="lazy"/></p><h3>论文意图</h3><p>本文旨在解决当前 RAG（检索增强生成）和 CoT（思维链）技术在 NL2SQL（自然语言 SQL 生成）任务中遇到的主要瓶颈：</p><p><strong>通用大模型在任务分解和关键词提取方面的准确性不足。</strong></p><p>现有的数据集在任务分解方面往往过于碎片化，且缺乏特定领域的关键词标注。为了解决这些问题，作者提出了 <strong>DeKeyNLU 数据集</strong> 和 <strong>DeKeySQL 流程</strong>（包含三个模块：用户问题理解、实体检索和生成）。通过对模型进行微调以优化问题理解阶段，最终生成的 SQL 语句的准确性得到了提升。</p><h3>数据集分析</h3><p><strong>DeKeyNLU 数据集</strong> 包含 1500 个高质量标注的问答对，数据来源于 BIRD 基准数据集，涵盖金融、教育等多个领域的真实数据库场景，数据集按 <strong>7:2:1</strong> 的比例划分为训练集、验证集和测试集。</p><p>数据合成采用 “<strong>LLM 预标注 + 人工润色</strong>” 的混合工作流程：</p><ul><li>第一步：使用 GPT-4o 自动生成每个问题的初步任务分解（主任务/子任务）和关键词提取（对象/实现）；</li><li>第二步：三位专家标注员进行三轮交叉验证和修订确保标注质量。</li></ul><h3>小结</h3><p>论文通过引入 <strong>DeKeyNLU 数据集</strong> 和 <strong>DeKeySQL 框架</strong>，证明了 <strong>针对性的任务分解和关键词提取训练能够有效提升 NL2SQL 的性能。</strong></p><p>实验结果表明，利用 DeKeyNLU 对 “用户问题理解” 模块进行微调后，模型在 BIRD 开发集上的准确率从 62.31% 提升至 69.10%，在 Spider 开发集上的准确率从 84.2% 提升至 88.7%。</p><p>在 NL2SQL 流程中，实体检索被认为是影响整体准确率的最关键环节，其次是用户问题理解和修正机制。这些发现凸显了以数据集为中心的方法和精心设计的流程对于提升 NL2SQL 系统能力的重要价值，并为用户实现直观、准确的数据交互铺平了道路。</p>]]></description></item><item>    <title><![CDATA[我把大模型装进了电脑里：Ollama 本地部署全攻略 程序员小崔日记 ]]></title>    <link>https://segmentfault.com/a/1190000047607635</link>    <guid>https://segmentfault.com/a/1190000047607635</guid>    <pubDate>2026-02-12 16:08:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>本地大模型神器来了！Ollama 一键部署 30B 模型实战指南</h2><hr/><h3>一、认识这只"羊驼"</h3><p>如果你最近在研究本地大模型，那你一定绕不开它。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607637" alt="" title=""/></p><p>它叫 <strong>Ollama</strong>。</p><p>官网地址：\<br/><a href="https://link.segmentfault.com/?enc=WB9UYYkvxDztgn%2FUzt5aEQ%3D%3D.IlXYgBhFuFicAXQrZOr21tBmAV9LmipXmZqQbjNzbVc%3D" rel="nofollow" target="_blank">https://ollama.com</a></p><p>一句话总结：</p><blockquote><strong>Ollama = 本地大模型运行与管理工具</strong></blockquote><p>它的核心目标非常简单：</p><p>让你在自己的电脑上，像用 Docker 一样管理和运行大语言模型。</p><hr/><h3>二、为什么 Ollama 这么受欢迎？</h3><p>以前部署大模型通常有三种方式：</p><ul><li>调用 API（长期成本高）</li><li>自己编译部署（流程复杂）</li><li>各种依赖冲突（容易踩坑）</li></ul><p>Ollama 做了一件非常关键的事情：</p><blockquote>把复杂的模型部署，变成一行命令。</blockquote><p>例如：</p><pre><code class="bash">ollama run qwen3:8b</code></pre><p>自动下载\<br/>自动加载\<br/>直接进入对话</p><p>对开发者来说，体验非常流畅。</p><hr/><h3>三、安装与使用</h3><h4>1. 下载安装</h4><p>访问官网下载安装即可。</p><p>支持系统：</p><ul><li>Windows</li><li>macOS</li><li>Linux</li></ul><p>安装完成后即可开始运行模型。</p><hr/><h4>2. 第一次下载模型的注意事项</h4><p>首次运行模型时会自动下载。</p><p>强烈建议：</p><blockquote>在设置中将模型下载目录改到 D 盘或其他大容量磁盘。</blockquote><p>原因：</p><ul><li><code>qwen3:30b</code> 等模型体积较大</li><li>下载后可能占用十几 G 甚至几十 G 空间</li><li>默认路径在 C 盘容易导致磁盘爆满</li></ul><p>提前规划好存储路径非常重要。</p><hr/><h3>四、模型区别与推荐</h3><h4>1. GPT-OSS 系列</h4><p>包含：</p><ul><li>gpt-oss:120b</li><li>gpt-oss:20b</li></ul><p>特点：</p><ul><li>通用对话模型</li><li>适合写作、问答、知识整理</li></ul><p>推荐建议：</p><ul><li>16GB 内存以下建议选择 20b</li><li>高性能设备可以尝试 120b</li></ul><hr/><h4>2. DeepSeek 系列</h4><p>包含：</p><ul><li>deepseek-v3.1:671b-cloud</li><li>deepseek-r1:8b</li></ul><p>特点：</p><ul><li>推理能力较强</li><li>数学与逻辑能力表现不错</li></ul><p>说明：</p><ul><li>671b 为云端模型</li><li>本地可选择 r1:8b 体验推理能力</li></ul><p>适合对逻辑思考要求较高的场景。</p><hr/><h4>3. Qwen3 系列（当前主流推荐）</h4><p>包含：</p><ul><li>qwen3:4b / 8b / 30b</li><li>qwen3-coder:30b / 480b-cloud</li><li>qwen3-vl:4b / 8b / 30b / 235b-cloud</li></ul><h5>（1）qwen3 ------ 通用模型</h5><p>适合：</p><ul><li>日常聊天</li><li>写文章</li><li>知识问答</li><li>代码辅助</li></ul><p>推荐配置参考：</p><ul><li>8GB 内存 → 4b</li><li>16GB 内存 → 8b</li><li>32GB 内存以上 → 30b</li></ul><hr/><h5>（2）qwen3-coder ------ 专业代码模型</h5><p>专为程序员优化：</p><ul><li>代码生成</li><li>代码补全</li><li>Bug 修复</li><li>项目结构生成</li></ul><p>推荐：</p><ul><li>本地优先选择 30b</li><li>480b 为云端版本</li></ul><p>如果你是开发者，这个系列非常值得长期使用。</p><hr/><h5>（3）qwen3-vl ------ 视觉语言模型</h5><p>VL = Vision + Language</p><p>可以实现：</p><ul><li>图片识别</li><li>图文问答</li><li>图片分析</li></ul><p>推荐：</p><ul><li>8b 起步</li><li>追求更好效果可选择 30b</li></ul><hr/><h4>4. Gemma3 系列（Google 系）</h4><p>包含：</p><ul><li>gemma3:1b / 4b / 12b / 27b</li></ul><p>特点：</p><ul><li>体积小</li><li>运行速度快</li><li>资源占用较低</li></ul><p>适合：</p><ul><li>轻量电脑</li><li>老设备</li><li>快速测试</li></ul><p>推荐：</p><ul><li>4b 或 12b 更均衡</li></ul><hr/><h3>五、如果只推荐三个模型</h3><p>综合考虑性能与实用性，建议优先尝试：</p><ul><li>日常聊天：qwen3:8b</li><li>写代码：qwen3-coder:30b</li><li>轻量体验：gemma3:4b</li></ul><p>如果你的机器配置较高：</p><blockquote>可以直接尝试 qwen3:30b。</blockquote><hr/><h3>六、一个必须说明的事实</h3><p>蒸馏模型并不是满血模型。</p><p>参数规模不等于能力等同于顶级闭源模型。</p><p>实际表现取决于：</p><ul><li>CPU / GPU 性能</li><li>显存大小</li><li>内存容量</li><li>是否开启量化</li></ul><p>同一个模型，在不同设备上的表现差距可能非常明显。</p><p>因此建议多尝试不同模型，找到最适合自己机器的版本。</p><hr/><h3>七、本地部署真正的意义</h3><p>本地运行大模型，并不是为了与顶级闭源模型直接竞争。</p><p>它的真正价值在于：</p><ul><li>数据隐私</li><li>零 API 成本</li><li>企业内网部署</li><li>本地知识库整合</li><li>可深度定制</li></ul><p>对于开发者而言，这是可控、可扩展的能力。</p><hr/><h3>结语</h3><p>当你第一次在本地成功运行一个 30B 模型时，那种掌控感非常真实。</p><p>Ollama 的出现，让本地大模型真正进入"普通开发者可用"阶段。</p><p>如果你正在探索 AI 工具链，本地部署值得认真体验一次。</p><hr/><p><strong>作者：程序员小崔日记</strong></p><p>本文由<a href="https://link.segmentfault.com/?enc=pW58eKHO8eOIwvWpTmbzAA%3D%3D.eQovV5FjCz%2FFu%2F5ja5Cej84zHiUThHenNFotzu5Opyw%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[只有243多行的AI训练代码，中文注释版 CRStudio ]]></title>    <link>https://segmentfault.com/a/1190000047607639</link>    <guid>https://segmentfault.com/a/1190000047607639</guid>    <pubDate>2026-02-12 16:07:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Andrej Karpathy刚发布了一个仅用约 250 行纯 Python 代码就实现了 GPT 训练和推理全过程的演示，非常适合用来理解大型语言模型底层的数学原理。</p><blockquote><p>Andrej Karpathy：“新的艺术项目。<br/>用243行纯粹的、无依赖的Python代码实现GPT的训练与推理。这包含了所需内容的完整算法部分，其余的一切都只是为了提升效率。我已无法再进一步简化。</p><p>其工作原理是将完整的LLM架构和损失函数彻底分解为构成它的最基本数学运算（+、<em>、</em>*、log、exp），然后通过一个微小的标量自动求导引擎（micrograd）来计算梯度，优化器使用Adam。<br/>”</p></blockquote><p><img width="690" height="553" referrerpolicy="no-referrer" src="/img/bVdnU4F" alt="image.png" title="image.png"/></p><p>源文件在这里：<a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank">https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95</a></p><h4>我用AI添加了注释并手动整理了一下：</h4><pre><code class="python"># 导入所需依赖库
import torch # PyTorch核心库，用于张量计算和神经网络构建
import torch.nn as nn # PyTorch的神经网络模块，包含层、损失函数等
from torch.nn import functional as F # PyTorch的优化器模块，用于模型参数更新
import torch.optim as optim # GPT2的分词器，用于文本的编码和解码
from transformers import GPT2Tokenizer # 从huggingface/transformers导入GPT2分词器，适配英文文本

# 定义全局超参数，控制模型训练和结构
batch_size = 16 # 每次训练的样本数，小批量梯度下降用
block_size = 32 # 上下文窗口大小，模型能看到的最大文本长度
max_iters = 5000 # 训练的最大迭代次数
eval_interval = 100 # 每多少轮迭代评估一次模型性能
learning_rate = 1e-3 # 优化器的学习率，控制参数更新步长
device = 'cuda' if torch.cuda.is_available() else 'cpu' # 模型运行的设备，优先使用GPU(cuda)，无则用CPU
eval_iters = 200 # 每次评估时的迭代次数，取平均减少波动
n_embd = 64 # 嵌入层的维度，模型中隐藏层的特征维度
n_head = 4 # 注意力头的数量，实现多头自注意力
n_layer = 4 # Transformer解码器的层数

# 固定随机种子，保证实验结果可复现
torch.manual_seed(1337)

# 加载GPT2分词器，设置为不使用填充（padding）和未知词（unk）的特殊处理
# GPT2分词器基于BPE(字节对编码)，适配英文自然语言，词表大小约50k
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# 禁用填充token，因为本微型GPT不做批量填充对齐
tokenizer.pad_token = None
# 禁用未知token，遇到未登录词时直接拆分为子词
tokenizer.unk_token = None

# 加载训练数据：这里使用经典的莎士比亚文本作为训练语料
# 从github拉取原始文本文件，读取为字符串格式
with open('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# 对原始文本进行分词编码，将字符串转换为模型可处理的整数张量
# return_tensors='pt'：返回PyTorch张量；truncation=True：超长文本截断
# max_length=None：不限制单条文本长度，后续按block_size切分
encoded_text = tokenizer(text, return_tensors='pt', truncation=True, max_length=None)
# 提取编码后的输入id，展平为一维张量（shape: [总词数]）
data = encoded_text.input_ids.flatten()

# 划分训练集和验证集：90%数据用于训练，10%用于验证
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

# 数据加载函数：随机生成一批训练/验证样本
# split：指定数据集（'train'/'val'），控制加载训练集还是验证集
def get_batch(split):
    # 根据split选择对应的数据集
    data = train_data if split == 'train' else val_data
    # 随机生成batch_size个起始索引，范围：[0, 数据长度-block_size)，保证能取到连续的block_size个词
    ix = torch.randint(len(data) - block_size, (batch_size,))
    # 构造输入张量x：取每个起始索引后连续的block_size个词，shape: [batch_size, block_size]
    x = torch.stack([data[i:i+block_size] for i in ix])
    # 构造目标张量y：取每个起始索引后偏移1的block_size个词（语言模型的预测目标是下一个词），shape: [batch_size, block_size]
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    # 将张量移到指定设备（GPU/CPU）
    x, y = x.to(device), y.to(device)
    return x, y

# 定义评估函数：计算模型在训练/验证集上的平均损失（无梯度计算，提升效率）
# model：待评估的模型实例
@torch.no_grad()  # 装饰器，禁用梯度计算，减少内存占用
def estimate_loss(model):
    # 初始化损失字典，存储训练集和验证集的损失
    out = {}
    # 将模型设为评估模式，关闭Dropout等训练特有的层
    model.eval()
    # 遍历训练集和验证集
    for split in ['train', 'val']:
        # 初始化损失数组，存储每次迭代的损失
        losses = torch.zeros(eval_iters)
        # 循环eval_iters次，计算平均损失
        for k in range(eval_iters):
            # 获取一批样本
            X, Y = get_batch(split)
            # 前向传播，得到模型输出和损失
            logits, loss = model(X, Y)
            # 记录当前迭代的损失
            losses[k] = loss.item()
        # 计算该数据集的平均损失，存入字典
        out[split] = losses.mean()
    # 将模型恢复为训练模式，开启Dropout等层
    model.train()
    return out

# 定义单头自注意力层：实现自注意力的核心逻辑（缩放点积注意力）
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        # 定义查询（q）、键（k）、值（v）的线性投影层，将n_embd维映射到head_size维
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        # 注册三角掩码张量，用于遮蔽未来的词（自回归语言模型，不能看到未来信息）
        # tril：下三角矩阵，upper三角部分填充为-∞，后续softmax后为0
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        # 定义Dropout层，防止过拟合，随机失活20%的神经元
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x：输入张量，shape: [batch_size, block_size, n_embd]
        B, T, C = x.shape
        # 线性投影得到q、k、v，shape均为[batch_size, block_size, head_size]
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)

        # 计算注意力权重：q @ k.T / sqrt(head_size)（缩放点积）
        # wei shape: [batch_size, block_size, block_size]
        wei = q @ k.transpose(-2, -1) * C**-0.5
        # 应用掩码：将上三角部分设为-∞，遮蔽未来的词
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        # softmax归一化，得到注意力权重（每行和为1）
        wei = F.softmax(wei, dim=-1)
        # 应用Dropout，随机失活部分注意力权重
        wei = self.dropout(wei)
        # 注意力加权求和：权重 @ v，得到输出，shape: [batch_size, block_size, head_size]
        out = wei @ v
        return out

# 定义多头自注意力层：将多个单头注意力的输出拼接，实现多维度的特征提取
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        # 创建num_heads个单头注意力层，存入nn.ModuleList（可被PyTorch识别的层列表）
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        # 线性投影层，将拼接后的特征映射回n_embd维
        self.proj = nn.Linear(head_size * num_heads, n_embd)
        # Dropout层，防止过拟合
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # 拼接所有单头注意力的输出，dim=-1表示在最后一维拼接
        # 输出shape: [batch_size, block_size, head_size*num_heads]
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        # 线性投影+Dropout，完成多头注意力的输出变换
        out = self.dropout(self.proj(out))
        return out

# 定义前馈网络层：Transformer解码器中的全连接层，实现特征的非线性变换
class FeedFoward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        # 两层线性层+ReLU激活+Dropout，隐藏层维度设为n_embd*4（Transformer原论文设定）
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # 升维
            nn.ReLU(),  # 非线性激活
            nn.Linear(4 * n_embd, n_embd),  # 降维回原维度
            nn.Dropout(0.2),  # 随机失活，防止过拟合
        )

    def forward(self, x):
        # 前向传播，输入输出shape均为[batch_size, block_size, n_embd]
        return self.net(x)

# 定义Transformer解码器块：由「多头自注意力 + 前馈网络」组成，带残差连接和层归一化
class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        # 计算每个注意力头的维度：总嵌入维 / 头数
        head_size = n_embd // n_head
        # 多头自注意力层
        self.sa = MultiHeadAttention(n_head, head_size)
        # 前馈网络层
        self.ffwd = FeedFoward(n_embd)
        # 层归一化层（Pre-LN架构，Transformer原论文是Post-LN，Pre-LN更易训练）
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        # 残差连接 + 层归一化 + 多头自注意力：x = x + sa(ln1(x))
        # 残差连接缓解深度网络的梯度消失问题
        x = x + self.sa(self.ln1(x))
        # 残差连接 + 层归一化 + 前馈网络：x = x + ffwd(ln2(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# 定义微型GPT模型核心类，继承自nn.Module（PyTorch所有网络的基类）
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # 词嵌入层：将词的整数ID映射为n_embd维的向量，vocab_size为词表大小
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        # 位置嵌入层：将位置索引映射为n_embd维的向量，捕捉文本的位置信息
        # 因为Transformer是并行计算，无内置位置信息，需手动加入
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        # Transformer解码器块序列：n_layer个Block堆叠
        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
        # 最后的层归一化层
        self.ln_f = nn.LayerNorm(n_embd)
        # 输出线性层：将n_embd维的特征映射回词表大小，用于预测下一个词的概率
        self.lm_head = nn.Linear(n_embd, vocab_size)

        # 初始化模型参数：使用自定义的初始化方式，提升训练稳定性
        self.apply(self._init_weights)

    # 模型参数初始化函数
    def _init_weights(self, module):
        # 如果是线性层，初始化权重为正态分布（均值0，标准差0.02），偏置为0
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        # 如果是嵌入层，初始化权重为正态分布（均值0，标准差0.02）
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    # 模型前向传播函数：输入x（词ID），y（目标词ID，可选），返回预测logits和损失
    def forward(self, idx, targets=None):
        # idx shape: [batch_size, block_size]
        # targets shape: [batch_size, block_size]
        B, T = idx.shape

        # 词嵌入 + 位置嵌入，shape均为[batch_size, block_size, n_embd]
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))
        # 嵌入层输出：词嵌入+位置嵌入，捕捉词的语义和位置信息
        x = tok_emb + pos_emb

        # 经过Transformer解码器块序列，输出shape不变：[batch_size, block_size, n_embd]
        x = self.blocks(x)
        # 最后的层归一化
        x = self.ln_f(x)
        # 输出线性层，得到logits（未归一化的概率），shape: [batch_size, block_size, vocab_size]
        logits = self.lm_head(x)

        # 如果没有目标值，仅返回logits（用于生成文本）
        if targets is None:
            loss = None
        else:
            # 重塑logits和targets，适配交叉熵损失的输入格式
            # cross_entropy要求输入为[B*T, vocab_size]，目标为[B*T]
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            # 计算交叉熵损失（语言模型的核心损失，预测下一个词的概率）
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    # 文本生成函数：基于当前输入idx，生成后续max_new_tokens个词
    # 自回归生成：每次预测一个词，拼接到输入后，继续预测下一个
    def generate(self, idx, max_new_tokens):
        # idx: 初始输入张量，shape: [batch_size, block_size]
        for _ in range(max_new_tokens):
            # 截取最后block_size个词，保证输入长度不超过模型的上下文窗口
            idx_cond = idx[:, -block_size:]
            # 前向传播，得到logits（无目标值，loss=None）
            logits, loss = self(idx_cond)
            # 取最后一个时间步的logits（预测下一个词的logits），shape: [batch_size, vocab_size]
            logits = logits[:, -1, :]
            # softmax归一化，得到下一个词的概率分布
            probs = F.softmax(logits, dim=-1)
            # 根据概率分布随机采样一个词ID（也可以用argmax取最可能的词，即贪心生成）
            idx_next = torch.multinomial(probs, num_samples=1)
            # 将采样的词ID拼接到输入后，更新输入张量
            idx = torch.cat((idx, idx_next), dim=1)
        # 返回生成后的完整词ID张量
        return idx

# 主程序入口：实例化模型、优化器，开始训练和生成
if __name__ == "__main__":
    # 获取GPT2分词器的词表大小，作为模型的vocab_size
    vocab_size = tokenizer.vocab_size
    # 实例化微型GPT模型，移到指定设备
    model = GPTLanguageModel(vocab_size).to(device)
    # 打印模型参数量，查看模型规模
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

    # 定义优化器：使用AdamW（Adam的改进版，带权重衰减，防止过拟合）
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    # 训练循环：迭代max_iters次
    for iter in range(max_iters):
        # 每隔eval_interval次迭代，评估模型损失并打印
        if iter % eval_interval == 0:
            losses = estimate_loss(model)
            print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

        # 获取一批训练样本
        xb, yb = get_batch('train')
        # 前向传播，得到logits和损失
        logits, loss = model(xb, yb)
        # 梯度清零：PyTorch梯度会累加，每次迭代前需清零
        optimizer.zero_grad(set_to_none=True)
        # 反向传播：计算损失对模型参数的梯度
        loss.backward()
        # 优化器步骤：更新模型参数
        optimizer.step()

    # 训练完成后，进行文本生成
    # 初始化输入：&lt;|endoftext|&gt;是GPT2的特殊起始token，编码为张量并移到设备
    # shape: [1, 1]（batch_size=1, block_size=1）
    start_idx = tokenizer.encode("&lt;|endoftext|&gt;", return_tensors='pt').to(device)
    # 生成max_new_tokens=500个词
    generated_ids = model.generate(start_idx, max_new_tokens=500)
    # 将生成的词ID解码为字符串，跳过特殊token
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    # 打印生成的文本
    print("\nGenerated text:\n")
    print(generated_text)</code></pre>]]></description></item><item>    <title><![CDATA[【划重点】HarmonyOS 应用市场审核 3.5 驳回“十大高频问题”全解析 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047607645</link>    <guid>https://segmentfault.com/a/1190000047607645</guid>    <pubDate>2026-02-12 16:07:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在 HarmonyOS 生态蓬勃发展的今天，应用上架是开发者面临的关键一环。不少作品因触碰 3.5 条款（应用价值与独特性） 被拒，常见的困惑包括：为何被判“功能单一”或“缺乏实质服务”？<br/>本专题复盘近期审核数据，深度解析 3.5 条款 Top 10 驳回问题，为您提供：</p><ul><li>问题预警： 详解高频被拒问题，提前预警</li><li>调优路径： 给出应用从纯展示向强交互的实操路径</li><li>通关范本： 参考标杆案例，让优化有据可依<br/>读懂 3.5，上架更有数。优化应用实用性，助您的作品顺利开启鸿蒙之旅。</li></ul><p><strong>问题1：不收录应用功能与手机系统自带的功能重复，缺乏独特价值。如简易计算器、桌面时钟、加密备忘录、天气、手电筒、指南针、镜子、日历、计时类等。</strong></p><p><strong>【改进建议】</strong><br/>明确应用的独特价值，确保其核心功能与系统功能非完全重叠，强化在垂直领域的专业能力；交互设计与用户体验优化，通过清晰的界面布局与突出的重点功能，降低用户认知与操作成本，确保其价值能被用户快速感知和顺畅使用。</p><p><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVi" alt="image.png" title="image.png"/></p><p><strong>【优化后成功上架案例】</strong><br/>优化前：与系统备忘录功能相似，只能简单记录。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVk" alt="image.png" title="image.png" loading="lazy"/><br/>优化后：丰富应用核心功能，优化首页打卡统计展示，及新增“膳食食材、养生知识”等养生内容及“组队打卡、隔空传送”等趣味内容。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVl" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题2：应用为有限的信息内容罗列不收录，如信息介绍，生活指南、法律案例展示、书籍推荐、诗词罗列等。</strong><br/><strong>【改进建议】</strong><br/>聚焦应用核心功能，避免信息的简单堆砌与罗列，需打造差异化内容，增强交互设计与闭环体验，提升用户使用体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVm" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>【优化后成功上架案例】</strong><br/>优化前：仅少量菜谱信息罗列，UX设计略显欠缺。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVn" alt="image.png" title="image.png" loading="lazy"/></p><p>优化后：丰富应用核心功能，新增“做饭计划、上传菜谱、菜篮子”等模块，满足多种用户使用场景，优化整体界面设计展示，凸显菜品，使应用具有实用性的同时更加美观。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVp" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题3：单一界面的恶作剧恶搞类应用不收录，如屏幕破裂、屏幕鬼魂、模拟打嗝等。</strong><br/><strong>【改进建议】</strong><br/>通过整合多种恶作剧类型，增强应用场景适配性与趣味性。结合创意互动与交互创新，在丰富功能的同时保持界面简洁，提升娱乐性。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVq" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题4：单一H5/Web页面应用不收录，如一张图片，一首音乐、一本书、一个主题、单一影视剧集类、单一非官方游戏攻略类等。</strong><br/><strong>【改进建议】</strong><br/>聚焦应用核心功能，构建复合型功能体系，避免功能单一化；深化内容价值，丰富交互场景与功能闭环，持续提升用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVr" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题5：不收录企业黄页类应用，仅展示文字信息、图片介绍，不提供用户服务。</strong><br/><strong>【改进建议】</strong><br/>强化功能深度，增加功能实用性，提供实际可使用有价值的功能，不能仅企业文字信息、图片介绍，未提供实际的用户服务。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVI" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题6：不收录应用内容均为各种广告推广。</strong><br/><strong>【改进建议】</strong><br/>删除广告推广内容<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVU" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题7：不收录应用页面仅提供简单的预约功能，但未对预约内容进行详细说明，未体现功能的实质价值，未给到用户清晰、准确的服务预期。</strong><br/><strong>【改进建议】</strong><br/>优化功能设计，确保功能闭环完整，消除使用断点，提供清晰明确的功能内容。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVV" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题8：不收录应用无实质功能，仅记录想对自己说的话。</strong><br/><strong>【改进建议】</strong><br/>围绕核心功能深化与拓展实用场景，突出差异化设计理念，强化技术业务协同能力，优化数据存储机制，提升操作便捷性与界面友好度，打造流畅舒适的用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVW" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题9：不收录应用无实质功能和真实用户使用场景，如纯虚构内容。</strong><br/><strong>【改进建议】</strong><br/>围绕真实用户的实际使用场景进行功能规划与设计，提供满足实际使用需求的可用功能；深度优化应用的核心使用场景体验，打磨并升级用户界面的设计质感与交互流畅度。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVX" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题10：不收录资讯类应用内容老旧、过时，最新资讯模块展示过时的新闻报道。</strong><br/><strong>【改进建议】</strong><br/>建立资讯内容定期更新机制，定期对资讯内容进行动态更新与优化，确保信息时效性，保持内容新鲜度与吸引力，避免陈旧内容影响用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUV0" alt="image.png" title="image.png" loading="lazy"/></p><p>➡️ <a href="https://link.segmentfault.com/?enc=PCPVXYhvZhx3xqG2LS05%2Bg%3D%3D.FdWwQ0am8T22aSsdcOICRTF20ZJi1ahPSsgXPPH9new5CLCyfyjCFlU8hwx6EKVSiXcym5pVPajTBpwm39JxVodg1qa6l7V7Cqdi6iNeNI5P3rIUfLoSKvMaRFsLC2k%2FzMCKa9aeBEuqz258YIM0sA%3D%3D" rel="nofollow" target="_blank">原贴指路</a></p>]]></description></item><item>    <title><![CDATA[sizeof与strlen BlackQid ]]></title>    <link>https://segmentfault.com/a/1190000047607650</link>    <guid>https://segmentfault.com/a/1190000047607650</guid>    <pubDate>2026-02-12 16:06:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><code>sizeof</code>计算变量所占内存空间大小，单位是字节，如果操作数是类型，计算的是使用类型创建的变量所占内存空间的大小。<code>sizeof</code>只关注占用内存空间的大小，不在乎内存中存放什么数据。</p><p><code>strlen</code>是C语言库函数，功能是求字符串长度。函数原型如下：</p><pre><code class="c">size_t strlen ( const char * str );</code></pre><p>统计的是从<code>strlen</code>函数的参数<code>str</code>中这个地址开始向后，<code>\0</code>之前字符串中字符的个数。<code>strlen</code>函数会一直向后找<code>\0</code>字符，直到找到为止，所以可能存在越界查找。 <code>strlen</code>关注到了字符串中具体的内容。</p><table><thead><tr><th>sizeof</th><th>strlen</th></tr></thead><tbody><tr><td>1. 是操作符 &lt;br/&gt;2. 计算操作数所占内存的大小，单位是字节&lt;br/&gt;3. 不关注内存中存放什么数据</td><td>1. 是库函数，使用需要包含头文件string.h&lt;br/&gt;2. 是求字符串长度的，统计的是\0之前字符的个数&lt;br/&gt;3. 关注内存中是否有\0，如果没有\0，就会持续往后找，可能会越界</td></tr></tbody></table><p>数组名的意义：</p><ol><li><code>sizeof(数组名)</code>，这里的数组名表示整个数组，计算的是整个数组的大小。</li><li><code>&amp;数组名</code>，这里的数组名表示整个数组，取出的是整个数组的地址。</li><li>除此之外所有的数组名都表示首元素的地址。（二维数组的首元素是一个一维数组）</li></ol><hr/><p>小试牛刀：</p><pre><code class="c">#define _CRT_SECURE_NO_WARNINGS 1
#include &lt;stdio.h&gt;
int main()
{
    char* c[] = { "ENTER","NEW","POINT","FIRST" };
    char** cp[] = { c + 3,c + 2,c + 1,c };
    char*** cpp = cp;
    printf("%s\n", **++cpp);
    printf("%s\n", *--*++cpp+3);
    printf("%s\n", *cpp[-2]+3);
    printf("%s\n", cpp[-1][-1]+1);
    return 0;
}</code></pre>]]></description></item><item>    <title><![CDATA[无需修改内核即可为 PostgreSQL 数据库对象添加自定义属性 IvorySQL ]]></title>    <link>https://segmentfault.com/a/1190000047607656</link>    <guid>https://segmentfault.com/a/1190000047607656</guid>    <pubDate>2026-02-12 16:05:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在开发实践中，经常会遇到一个问题：如何在不修改 PostgreSQL 内核代码的前提下，为数据库对象附加自定义元数据。本文展示了一种基于 PostgreSQL SECURITY LABELS 机制的可行方案，用于实现自定义属性。这种方式具备事务性、与数据库对象强关联，并且能够与标准 PostgreSQL 操作良好协同。</p><h2>问题背景：复制冲突的管理</h2><p>在一个典型的两主节点向第三节点复制数据的架构中，UPDATE/UPDATE 冲突较为常见。复制冲突的处理本身较为复杂，且在多数场景下并无通用解法，但某些列类型可以采用更简单的处理思路。</p><p>例如，在银行业务场景中，账户余额字段通常只允许增减操作。此时可以采用基于增量（delta）的方式：不在冲突时选择某个绝对值，而是分别计算两次更新的变化量并进行叠加。该方法仅需进行基础校验（如溢出检查、余额不小于 0），即可确保所有更新均被正确计入。</p><p>难点在于：如果 PostgreSQL 本身不支持此类机制，如何标记特定列以启用基于 delta 的冲突解决策略？理想状态下，可以直接通过类似以下语法完成：</p><pre><code>ALTER TABLE accounts ALTER COLUMN balance SET delta_apply = 'true';</code></pre><p>但这种深度集成 SQL 语法的方式实现难度较高，且对可移植性意义有限。更现实的需求是通过扩展接口完成设置，例如：</p><pre><code>SELECT my_extension.set_delta_apply('accounts', 'balance', true);</code></pre><p>社区中曾多次讨论为数据库对象引入自定义属性的提案，也曾提交过内核补丁，但实现代码规模较大，而应用场景相对有限，合入内核的可行性存疑。此外，有时需要为索引、大对象或数据类型等非表对象附加属性，这进一步增加了复杂度。更重要的是，即便补丁被接受，也只能影响未来版本，而现实需求往往是“当下可用”。</p><h2>功能需求</h2><p>实现该功能前需明确相关需求：</p><ul><li><strong>对象生命周期绑定</strong>：属性必须与数据库对象建立内部依赖关系。例如，在执行 <code>DROP … CASCADE</code> 删除父对象时，属性也应随之自动删除。</li><li><strong>扩展关联性</strong>：属性需要与扩展建立明确关联，以便在扩展被卸载时由数据库系统正确处理。</li><li><strong>事务性行为</strong>：对象属性需满足事务规则与可见性约束，并行事务修改时，新属性值仅在当前事务内可见，提交前回滚则恢复原值。</li><li><strong>升级与迁移支持</strong>：在 <code>pg_upgrade</code> 以及 dump/restore 过程中，属性应随数据库对象一并正确迁移。</li></ul><p>理想情况下可实现类似 PostgreSQL GUC 的会话级特性，但实现难度显著提升。</p><h2>方案评析</h2><p>以对象 OID 为键的简易全局哈希表无法满足需求，属性值可为变长类型（如字符串），对象与属性的关联实现复杂，且无法保障事务特性与 MVCC 机制。</p><p>另一种思路是在扩展中创建一张 &lt;<code>key</code>, <code>value</code>&gt; 表存储属性，由扩展在运行时查询该表。这在理论上可行，但实践中问题较多：涉及升级、dump/restore、复制一致性等一系列复杂问题，同时还需持续校验对象是否存在，并引入额外的查询开销，整体可靠性较差。</p><h2>实现思路</h2><p>具体目标是为任意表的列定义一个 <code>delta_apply</code> 属性，用于逻辑复制场景下的 UPDATE/UPDATE 冲突处理。当该属性启用时，订阅端不采用传统冲突解决策略，而是计算新旧值之间的差量，并将其累加到订阅端当前值中。</p><p>在 PostgreSQL 中，唯一同时满足上述全部需求的机制是 SECURITY LABELS。尽管该机制最初用于安全模块，但并未限制其仅用于安全相关元数据。需要注意以下事项：</p><ul><li>面向安全的工具可能会检查或校验标签内容。</li><li>需要使用独立且唯一的 provider 名称，以避免冲突。</li></ul><p>它是如何工作的？让我们看看这张图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607658" alt="extension_side.jpg" title="extension_side.jpg"/></p><p>实现的核心依赖于系统表 pg_seclabel。该表中的记录与数据库对象天然绑定，对其的增删改通过 SECURITY LABEL … 工具命令完成。扩展可以通过 utility hook 监听这一过程。</p><p>SECURITY LABEL 支持多种对象类型，包括视图、函数等，基本覆盖常见需求。每条标签记录包含对象的 OID 及对象类型（表、列、函数等），并通过文本字段存储任意自定义数据，同时通过 provider 字段区分不同模块生成的标签。</p><p>由于每次访问都直接查询 pg_seclabel 成本较高，且目前尚无系统级缓存，引入本地哈希缓存以降低开销：</p><pre><code>typedef struct PropertyCacheEntry {
    Oid classId;
    Oid objectId;
    char *propertyValue;
    bool valid;
} PropertyCacheEntry;
static HTAB *property_cache = NULL;</code></pre><p>通过注册 RelcacheCallback 实现缓存失效管理，对象执行 ALTER TABLE 操作时标记对应缓存条目无效。缓存填充策略可根据使用场景调整。例如，在核心 hook 中访问对象时加载，或在扩展提供的用户接口函数中主动加载。</p><h2>属性增删的实现细节</h2><p>为了保持各后端缓存一致性，每次属性变更都需要向其他进程发送失效通知。对象本身发生变化时，可通过 RelcacheCallback 处理；但属性变更本质上只是对 <code>pg_seclabel</code> 的 DML 操作，如何通知其他后端成为问题。</p><p>PostgreSQL 提供了 <a href="https://link.segmentfault.com/?enc=jpN6mfe%2FqbX98U3AvVuYeA%3D%3D.uxhUdfTNyJrTl9gfcAjGohsUKlVboRdomgKVZ2OyV%2BZx3jZazfayyKJyb0Vq92M2emDJ9MjAmSV8TK6M4sOPapHTl4FBEmvKsG%2FRCn1fIphT%2BlZCmaiIaGKxS%2BfnWEIe0DxGoDQAdYDXm4TuxQm6GQ%3D%3D" rel="nofollow" target="_blank">CacheInvalidateRelcacheByRelid</a>，但仅适用于 <code>pg_class</code> 中的对象，对于数据类型等对象无效。因此，在实际实现中，属性变更时会触发一次对象自身的“无实质变更更新”，以借此触发相应的失效回调，从而刷新扩展内部缓存。</p><p>扩展通过 <code>set_property()</code> 接口向用户暴露能力，用于为指定对象设置 SECURITY LABEL。标签文本中描述属性值，例如 <code>delta_apply: true</code>。在扩展中实现的 <code>seclabel_provider</code> 回调负责校验对象类型及属性合法性。</p><p>标签文本字段具备高度灵活性，允许存储复杂结构，例如以 JSON 形式描述属性逻辑。</p><p>通过该机制，客户端与扩展之间建立了一种相对原生的通信方式。扩展可在运行时判断属性是否存在，并据此调整行为。</p><p>在 <code>delta_apply</code> 的具体实现中，逻辑复制订阅端在处理 UPDATE 记录时，会同时查询对应表的属性缓存。若存在标记为增量属性的列，则计算新旧值差量并累加到订阅端当前值。即便在冲突解决策略（如 last-update-wins）下决定拒绝整条更新，增量列的变更仍会被应用，从而降低冲突概率并确保增量更新不丢失。</p><h2>外部干扰处理</h2><p>pg_seclabel 仍然是系统目录表，具备足够权限的用户（如 DBA）可以直接修改其内容。为降低风险，可在扩展中引入内部 GUC，例如 <code>myextension.call_guard</code>。在扩展 UI 函数执行前将其置为 true，结束后重置为 <code>false</code>，并在关键路径中校验其状态是否符合预期。</p><p>理论上，超级用户仍可能通过手段绕过该限制。虽然可以进一步为该 GUC 设置 hook 进行防护，但实现复杂度显著提高，容易演变为过度设计。</p><h2>总结</h2><p>PostgreSQL SECURITY LABELS 机制提供了一种可靠、事务安全的方式，用于在不修改内核的情况下为数据库对象添加自定义属性。尽管该机制最初面向安全模块，但同样适用于扩展级元数据管理，且具备良好的生命周期管理与 MVCC 支持。</p><p>该方案支持多种对象类型，具备事务一致性，并可正确参与 dump/restore 与升级流程。</p><p>原文链接：</p><p><a href="https://link.segmentfault.com/?enc=8z0pNgTWbVKGk08cYoYSAQ%3D%3D.eXB7s3jpsiRxiXqcmAWKa1ohSV2TJeiEVhP3LAKsleIKw%2Fl1302vZ0SNObLsRBJxARZScIwm5KUBy0ehyQ6nr8wIldChdBvRbR2%2FZTtVg7ha92wIANUbIpazi9E%2Bm32fOZ5QKWejaOJ%2FMgOwwBRasA%3D%3D" rel="nofollow" target="_blank">https://www.pgedge.com/blog/custom-properties-for-postgresql-...</a></p><p>作者：Andrei Lepikhov</p><hr/><h2><a href="https://link.segmentfault.com/?enc=%2BOFpBSEcm9rv1T8AMmVgbg%3D%3D.2KBCHSpNCB1Toqj%2FmB2IPAUZelHFhYK5P1J0K0wPKFs%3D" rel="nofollow" target="_blank">HOW 2026 议题招募中</a></h2><p>2026 年 4 月 27-28 日，由 IvorySQL 社区联合 PGEU（欧洲 PG 社区）、PGAsia（亚洲 PG 社区）共同打造的 HOW 2026（IvorySQL &amp; PostgreSQL 技术峰会） 将再度落地济南。届时，PostgreSQL 联合创始人 Bruce Momjian 等顶级大师将亲临现场。</p><p>自开启征集以来，HOW 2026 筹备组已感受到来自全球 PostgreSQL 爱好者的澎湃热情。为了确保大会议题的深度与广度，我们诚邀您在 2026 年 2 月 27 日截止日期前，提交您的技术见解。</p><p>投递链接：<a href="https://link.segmentfault.com/?enc=ohAI8pG9ezBwcQ2OU%2BRvoQ%3D%3D.5JbJDzO1etB%2BruJdZhPuC74k6zbB3BJUUeyXTUqUR%2Bk%3D" rel="nofollow" target="_blank">https://jsj.top/f/uebqBc</a></p>]]></description></item><item>    <title><![CDATA[企业级指标中台 API/JDBC 架构选型四步法 Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047607662</link>    <guid>https://segmentfault.com/a/1190000047607662</guid>    <pubDate>2026-02-12 16:04:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=eIVXIJmKksG7oeyLIEadiA%3D%3D.IkewO0FaHKxJ6H62ywWK%2FbIX5l%2BnNDAovvjz3WTKZtOPhLifYDwkJSclIllKDlESePGgW%2Fj8CBJ0edrjWHedhs2rHUfejXQxzgCcXtjZoTVXPfpYRyYxeyidNZb6T7qz" rel="nofollow" target="_blank">《指标中台全场景消费选型：Aloudata CAN API/JDBC 架构的适配性与扩展性》</a>转载请注明出处。</blockquote><p>摘要：本文面向数据工程团队，提供一套四步评估框架，用于选型指标中台的 API/JDBC 架构。核心聚焦于技术适配性、性能扩展性、生态治理扩展性及安全运维扩展性，并以 Aloudata CAN NoETL 指标平台为例，详解如何通过语义层、计算存储解耦与智能物化加速，构建高并发、全场景的统一指标服务出口。</p><p>在 BI 报表、AI 分析、业务系统等多端消费场景下，企业数据服务接口面临适配性与扩展性的双重挑战。传统 API 架构常因计算存储紧耦合而引发性能瓶颈与安全风险。本文面向数据架构师与工程团队，提供一套聚焦“适配性”与“扩展性”的四步选型评估框架，并以 Aloudata CAN NoETL 指标平台为例，详解如何通过其语义引擎、智能物化加速与开放化服务架构，构建稳定、高效且面向未来的统一指标服务出口。</p><h2>引言：全场景指标消费对 API/JDBC 架构的严苛挑战</h2><p>企业数据消费场景正以前所未有的速度演进：管理层需要通过 BI 报表实时监控业务健康度，分析师需要灵活下钻探查数据波动原因，业务系统需要调用精准的指标数据进行自动化决策，AI 大模型则需要安全、准确地“理解”企业数据以提供智能洞察。这些多样化的消费端，无一例外地依赖于底层数据服务接口——通常是 RESTful API 或 JDBC 连接。</p><p>然而，传统的“数仓+ETL+宽表”模式下的数据服务接口，正面临严峻考验：</p><ul><li>扩展瓶颈：查询负载直接冲击承载宽表的数据库，高并发下 CPU、内存迅速耗尽，导致查询超时甚至服务雪崩。</li><li>安全风险：直接暴露数据库或宽表接口，权限管控粗放，存在数据泄露风险，尤其在对接 AI 应用时更为突出。</li><li>适配困难：新消费端（如新 BI 工具、AI 应用）接入需要复杂的定制开发，形成“架构孤岛”。</li></ul><p>因此，为指标中台选择一套具备强大适配性与扩展性的 API/JDBC 底层架构，已成为企业数据工程团队的核心决策之一。以下四步评估框架，旨在提供一套清晰、可执行的选型方法论。</p><h2>第一步：技术适配性——能否无缝融入现有技术栈？</h2><p>评估一套 API/JDBC 架构的首要标准是其“开箱即用”的适配能力。它必须能够无缝融入企业现有的技术生态，避免产生高昂的改造成本和新的“架构孤岛”。正如行业专家在选型实践中指出的，开放性与灵活性是关键考量点。</p><h3>向下适配：对接现有数据湖仓，无需大改造</h3><p>优秀的架构不应要求企业推翻重来。其核心在于能够直接对接企业现有的 DWD（明细数据层），通过声明式策略在逻辑层面构建虚拟业务事实网络，而非强制要求建设或改造大量的物理宽表（DWS/ADS 层）。</p><ul><li>理想特征：平台作为语义层与指标计算引擎，构建在现有数据湖仓之上。通过配置化的方式声明业务实体间的逻辑关联（Join），形成“虚拟明细大宽表”，从而直接基于明细数据定义和计算指标。</li><li>价值体现：这实现了 “做轻数仓” ，保护了历史投资，避免了因引入新平台而引发的数仓大改造。企业可以采用 “存量挂载、增量原生、存量替旧” 的三步走策略，实现架构的平滑演进。</li></ul><h3>向上适配：标准接口，支持多前端消费</h3><p>架构必须提供广泛兼容的标准接口，确保各类消费端能够“即插即用”。</p><p>理想特征：</p><ul><li>标准 API/JDBC：提供完善的 RESTful API 和 JDBC 驱动。</li><li>生态深度融合：与 FineBI、Quick BI 等主流 BI 工具实现无缝集成；通过 JDBC 支持 Tableau、Power BI 等其他工具。</li><li>多元消费支持：除 BI 工具外，API 应能直接支持自研业务系统、AI 大模型调用，以及通过 WPS 插件在办公表格中直接分析。</li></ul><p>权威背书：在某全球连锁餐饮巨头的实践中，该架构日均支撑百万级 API 调用，验证了其标准接口在高并发、多场景下的稳定服务能力。</p><h2>第二步：性能扩展性——如何支撑高并发与大数据量？</h2><p>当“双十一”大促或月度财报日来临，指标查询量可能瞬间激增数十倍。架构的横向扩展能力是应对此类业务峰值的生命线。其核心在于计算与存储的解耦以及智能的预计算加速机制。</p><h3>计算存储解耦与无状态横向扩展</h3><p>紧耦合的架构（查询直接在存储数据的数据库上执行）是性能扩展的天花板。现代指标平台应采用计算层与底层数据存储分离的架构。</p><ul><li>理想特征：计算节点设计为无状态，可以像云原生应用一样，根据查询并发压力快速进行弹性伸缩（Scale-Out）。流量高峰时快速扩容实例，低谷时自动缩容，从而高效利用资源并确保服务 SLA。</li><li>价值体现：从根本上避免了因单个数据库实例性能瓶颈导致的整个数据服务集群雪崩的风险。</li></ul><h3>智能物化加速：以空间换时间，保障查询稳定性</h3><p>应对高并发和大数据量查询，不能仅依赖底层数据库的“硬扛”，更需要智能的“空间换时间”策略。</p><ul><li>理想特征：基于声明式物化策略。用户可配置需要对哪些指标和维度组合进行预计算及更新时效，系统据此自动编排和维护多层级的物化加速表（明细加速、汇总加速、结果加速）。查询时，语义引擎 自动进行 SQL 改写和智能路由，透明地命中最优的物化结果。</li><li>权威背书：同样在上述餐饮巨头案例中，面对 百亿级数据规模，该平台实现了 P90 查询响应时间小于 1 秒 的极致性能。这证明了智能物化加速引擎在将不可预测的复杂查询，转化为可预测的快速数据检索方面的核心价值。</li></ul><h2>第三步：生态与治理扩展性——能否支持未来演进？</h2><p>选型不仅要满足当下，更要具备面向未来的扩展性。这包括对 AI 等新技术的原生适配能力，以及在指标体系膨胀过程中内嵌的治理能力。</p><h3>AI-Ready：提供根治幻觉的语义层，而非裸数据接口</h3><p>直接向 AI 大模型开放数据库查询权限，是极其危险的做法。理想的架构应充当安全、语义化的代理。</p><ul><li>理想特征：采用 NL2MQL2SQL 架构。AI 负责将自然语言问题转换为结构化的指标查询语言（MQL），然后由平台的语义引擎将其翻译为优化、安全的 SQL。这相当于将“写代码”的开放题，变成了“选指标”的选择题，极大收敛搜索空间，从根源上杜绝“幻觉”。</li><li>安全增强：所有 AI 发起的查询，必须经过语义层的统一鉴权（行列级权限），实现 “先安检，后执行” ，确保每一次 AI 交互都是合规、可控的。</li></ul><h3>治理内嵌：定义即治理，保障指标资产持续健康</h3><p>随着业务发展，企业指标数量可能呈指数级增长。缺乏治理的指标体系将迅速失控，重回“口径混乱”的老路。</p><ul><li>理想特征：实现 “定义即治理” 。在指标定义环节，系统自动进行全局判重、逻辑校验和影响分析。结合完整的指标生命周期管理（创建、发布、变更、下线）、版本控制和权责体系，确保每一个进入指标库的资产都是规范、可信的。</li><li>价值体现：将治理流程内嵌于生产流程，变被动的事后治理为主动的事前预防，保障了指标体系在持续扩展过程中的健康度与一致性。</li></ul><h2>第四步：安全与运维扩展性——如何降低长期 TCO？</h2><p>总拥有成本（TCO）是选型的关键经济指标。优秀的架构应通过内置的安全稳定机制和低摩擦的运维模式，有效降低长期投入。</p><h3>内置安全与稳定性机制</h3><p>安全与稳定不应是事后补救的功能，而应是架构的固有属性。</p><p>理想特征：</p><ul><li>精细化权限：支持基于用户、角色、组织的行列级数据权限管控。</li><li>熔断与隔离：具备查询级熔断机制，异常慢查询或恶意请求会被自动隔离，防止其耗尽资源、拖垮整个集群，有效防范“链式雪崩”。</li></ul><p>价值体现：从架构层面为企业数据资产建立了主动防御体系，降低了数据泄露和系统宕机的风险与成本。</p><h3>运维复杂度与平滑演进路径</h3><p>平台落地和升级的复杂度直接关系到项目成败与 ROI。</p><p>理想特征：支持 渐进式落地策略。企业无需一次性迁移所有历史资产，而是可以：</p><ol><li>存量挂载：将现有稳定宽表逻辑接入，统一服务出口。</li><li>增量原生：所有新需求直接基于平台 NoETL 方式开发。</li><li>存量替旧：逐步优化或下线维护成本高的旧宽表。</li></ol><p>价值体现：极大降低了实施阻力，保护了既有 IT 投资，使企业能够在业务连续的前提下，平稳、可控地向现代化数据架构演进。</p><h2>综合评估清单与行动建议</h2><p>将上述四个维度转化为可执行的评估清单，有助于在选型过程中进行客观对比。</p><table><thead><tr><th>评估维度</th><th>关键问题</th><th>理想特征 (以 Aloudata CAN 为例)</th></tr></thead><tbody><tr><td>技术适配性</td><td>是否需要改造现有数仓？能否连接现有 BI 工具？</td><td>直接对接 DWD，标准 API/JDBC，与 FineBI/Quick BI 等深度集成，开箱即用。</td></tr><tr><td>性能扩展性</td><td>如何应对突发高并发？大数据量查询能否稳定在秒级？</td><td>计算存储解耦，无状态横向扩展；声明式智能物化加速，百亿数据 P90&lt;1s。</td></tr><tr><td>生态扩展性</td><td>是否支持 AI 智能问数？能否表达复杂业务指标？</td><td>NL2MQL2SQL 架构根治幻觉；支持指标转标签、跨表聚合等复杂业务逻辑。</td></tr><tr><td>安全运维扩展性</td><td>权限管控是否精细？架构升级是否必须推翻重来？</td><td>行列级权限，内置查询熔断机制；支持“存量挂载、增量原生、存量替旧”平滑演进。</td></tr></tbody></table><p>行动建议：</p><ul><li>初创/快速发展期企业：应优先考虑技术适配性和平滑演进路径，选择能够快速上线、不绑架技术栈的平台，为未来留足扩展空间。</li><li>成熟/高并发企业：需重点评估性能扩展性和内置安全机制，通过 POC 严格测试其在高并发压力下的稳定性（P99 延迟）和资源消耗。</li><li>普遍原则：要求供应商提供与自身业务场景相近的客户案例验证数据（如日均 API 调用量、查询性能指标），并评估其产品在行业标准制定中的参与度（如信通院标准起草单位），作为技术先进性与可靠性的重要佐证。</li></ul><h2>常见问题（FAQ）</h2><h4>Q1: Aloudata CAN 的 API 和 JDBC 接口，与直接查询数据库有什么区别？</h4><p>本质区别在于“语义层”。直接查库暴露的是原始表字段和复杂 SQL 逻辑，而 CAN 的接口提供的是经过治理的、业务友好的“指标”和“维度”，屏蔽底层复杂性，保障口径一致性与查询安全，并通过智能物化加速获得更优性能。</p><h4>Q2: 如果我们已经有很多基于宽表开发的 FineBI 报表，迁移到 Aloudata CAN 的 API/JDBC 接口工作量有多大？</h4><p>工作量可控。首先，无需重做报表，只需将 BI 工具的数据源切换至 Aloudata CAN 的 JDBC。其次，通过“存量挂载”功能将现有宽表逻辑接入，统一口径。后续新需求采用“增量原生”方式开发，逐步优化底层架构，实现平滑演进。</p><h4>Q3: 在高并发场景下，Aloudata CAN 的 API 服务如何保证稳定性和不超时？</h4><p>主要通过三层机制：1) 无状态计算层支持横向扩展应对流量峰值；2) 智能物化路由使查询自动命中预计算结果，避免冲击源库；3) 内置熔断与隔离防止异常查询拖垮集群。这在某餐饮巨头日均百万级 API 调用的场景中已得到验证。</p><h4>Q4: 想要让大模型使用我们的指标数据，通过 Aloudata CAN 的 API 接入是否安全？</h4><p>相比直接开放数据库更安全。Aloudata CAN 充当“安全代理”和“语义翻译”。所有查询需经语义层行列级权限校验。AI 通过 NL2MQL2SQL 调用标准指标接口，而非编写任意 SQL，极大限制操作范围，杜绝越权访问和数据泄露。</p><h2>核心要点</h2><ol><li>适配性是基础：优秀的指标中台 API 架构必须能 开箱即用，无缝对接企业现有数据湖仓和 BI 工具，避免产生“架构孤岛”。</li><li>解耦是扩展的关键：计算与存储解耦 是实现无状态横向扩展、应对高并发流量的前提，而 智能物化加速 是保障大数据量下稳定秒级响应的核心引擎。</li><li>治理必须内嵌：通过 “定义即治理” 和 NL2MQL2SQL 架构，在指标生产源头和 AI 消费入口嵌入管控，确保指标体系在扩展中的健康度与安全性。</li><li>平滑演进降低 TCO：支持 “存量挂载、增量原生、存量替旧” 的渐进式策略，能最大程度保护历史投资，降低项目风险与长期运维成本，实现架构的可持续升级。</li></ol><ul><li><ul><li>*</li></ul></li></ul><p>本文详细内容及高清架构图，请访问 Aloudata 官方技术博客原文：<a href="https://link.segmentfault.com/?enc=NKfgmQ1sNSofNfAvIQM88A%3D%3D.1v5wmxVA1Cdbvv2wAdfBzRNljCPmu%2FxocoNNgkJm2RUf%2F38n6hSjjPlrot%2FJ2aZ3PGip%2BUIn0wjdDjOIGGaerooy0ofFq%2B1oUulhCaAVbNc1jPSYYMSydmGTWPpjRgug" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/metric-middleware-aloudata...</a></p>]]></description></item><item>    <title><![CDATA[“马”住这份攻略：1个智能体，N种专属用法！ Smartbi ]]></title>    <link>https://segmentfault.com/a/1190000047607664</link>    <guid>https://segmentfault.com/a/1190000047607664</guid>    <pubDate>2026-02-12 16:03:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>年关将至，此时此刻，不同部门的战壕里，大家都在为了“圆满收尾”而全力冲刺：</p><p>财务部需要严谨核算：查凭证、平账目、出年报，容不得半点差错；</p><p>市场部正在灵活复盘：看转化、析渠道、定策略，急需洞察明年的先机。</p><p>如何让一个智能体，既能满足财务的严谨，又能适应市场的灵活？白泽SmartBI近期上线的“智能体快捷方式”功能，为您提供了一种极简解法。快来“马”住这份攻略，让您新的一年“马”力全开！</p><h2>01 业务痛点：不同部门个性化需求如何满足？</h2><p>在实际业务中，不同角色的关注点截然不同，通用配置的智能体往往难以解决个性化的提效需求：</p><h4>财务部门：</h4><p>日常聚焦于查凭证、看余额、审报表，高频使用“财务会计模型”和“资金管理模型”，需要一个专注财务领域的纯净入口，告别手动切换的繁琐。</p><h4>市场部门：</h4><p>核心关注客户价值与渠道转化，依赖“客户生命周期价值模型”和“渠道引流模型”来支撑决策，渴望一个能够直达业务核心、快速获得答案的智能体。</p><p>业务人员都希望能对智能体做轻量化调整，以满足自己个性化需求（如预设常用模型、简化交互流程），又需要一直保持智能体最新版本，享受升级带来的新功能。</p><p>如何能够同时满足统一维护与个性体验？白泽SmartBI给出了解决方案。</p><h2>02 解决方案：快捷方式 + 参数配置</h2><p>现在，通过“快捷方式 + 参数配置”的组合，您可以将一个核心智能体，快速为不同部门“分发”专属入口。它既保留了“开箱即用”的便捷，又能实时同步底层的每一次能力升级。真正实现了 “千人千面，常用常新”。</p><h4>快捷方式</h4><p>低成本实现一个智能体、N个“分身”</p><ul><li>它是指向原智能体的链接，不是复制粘贴，低成本实现千人千面；</li><li>原智能体有任何升级优化，所有快捷方式自动同步，功能永远保持最新；</li><li>支持基于同一智能体创建多个不同快捷方式，实现“一对多”个性化配置。</li></ul><h4>参数配置</h4><p>给每个“分身”设定专属性格</p><ul><li>每个快捷方式都可以独立配置运行参数（如限定数据模型范围、调整反问规则、联网等设置）。</li><li>通过提前预设，就能让同一个智能体在不同部门面前，展现出完全不同的“专属性格”，同时满足企业权限管控的需求。</li></ul><p>接下来我们从实际业务场景出发，三步带您解锁智能体专属用法。</p><h2>03 实操流程：3步打造专属智能体入口</h2><h4>Step1：创建快捷方式</h4><p>在“智能助理”下，创建两个快捷方式，命名为：智能助理_财务、智能助理_市场。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607666" alt="图片" title="图片"/></p><h4>Step2：配置专属参数</h4><p>智能助理_财务：数据模型列表设为财务常用的【财务会计模型】和【资金管理模型】，并开启自动选择数据模型。</p><p>智能助理_市场：数据模型列表设为市场常用的【客户生命周期价值模型】和【渠道引流模型】，并开启自动选择数据模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607667" alt="图片" title="图片" loading="lazy"/></p><p>同时还可以对联网、文件上传、反问等多项参数进行自定义设置。</p><h4>Step3：设置权限</h4><p>完成配置后即刻启用，将“智能助理_财务”权限开放给财务部，“智能助理_市场”权限开放给市场部。</p><h4>效果：开箱即用，马上见效！</h4><p>财务小王和市场小林登录后，只需选择自己的专属入口，直接输入问题即可。所有模型切换自动完成，他们得到的是最精准、最直接的答案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607668" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607669" alt="图片" title="图片" loading="lazy"/></p><h2>04 体验升维：提效立竿见影</h2><p>通过“快捷方式+参数配置”，我们解决的不仅是模型切换的操作痛点，更构建了一种可持续的个性化能力：各业务部门能获得量身定制的智能体验，而所有定制入口都能随原智能体同步进化，保障了企业整体迭代的效率与一致性，效果立竿见影。</p><p>SmartBI在不断拓展智能体能力边界的同时，也从未停止对“交互体验”的微调与打磨。这不仅是产品从可用迈向易用的关键，更是SmartBI能够承载大型企业复杂需求、交付可靠服务的最佳证明。</p><p>欢迎免费试用白泽SmartBI！开启您的专属智能之旅！</p><p>​</p>]]></description></item><item>    <title><![CDATA[鸿蒙架构师修炼之道-如何成为团队的架构师 waylau ]]></title>    <link>https://segmentfault.com/a/1190000047607851</link>    <guid>https://segmentfault.com/a/1190000047607851</guid>    <pubDate>2026-02-12 16:03:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>要成为鸿蒙开发团队的架构师，需要从知识储备、技能提升、经验积累、职业素养培养等多个方面进行努力，以下是具体的建议。</p><h3>扎实的知识储备</h3><p>鸿蒙开发团队的架构师需要具备扎实的知识储备。</p><ul><li><strong>操作系统知识</strong>：深入掌握操作系统的基本原理，包括进程管理、内存管理、文件系统、网络协议栈等。了解Linux内核的相关知识也很有帮助，因为鸿蒙系统与Linux有一定的渊源。</li><li><strong>鸿蒙系统知识</strong>：全面学习鸿蒙系统的架构、特性、开发框架和工具。熟悉鸿蒙的分布式技术、HarmonyOS应用开发语言（如ArkTS、仓颉编程语言等）、应用开发框架以及系统服务等内容。</li><li><strong>编程语言</strong>：熟练掌握至少一种鸿蒙应用开发语言，如ArkTS、C++、仓颉等，同时要对JavaScript、TypeScript等前端语言有一定的了解，以便进行跨平台开发和与Web技术的交互。</li><li><strong>硬件知识</strong>：了解硬件体系结构、芯片原理、传感器原理等硬件基础知识，有助于更好地理解鸿蒙系统与硬件的交互，以及在不同硬件平台上进行系统优化。</li></ul><h3>丰富的提升技能</h3><p>鸿蒙开发团队的架构师需要具备丰富的提升技能。</p><ul><li><strong>架构设计能力</strong>：通过学习架构设计模式和原则，如微服务架构、分层架构等，提升系统架构设计能力。能够根据业务需求，设计出合理、高效、可扩展的鸿蒙系统架构方案。</li><li><strong>开发与调试能力</strong>：具备熟练的鸿蒙应用开发能力，能够独立完成应用的编码、调试和测试工作。掌握调试工具和技巧，能够快速定位和解决开发过程中出现的问题。</li><li><strong>性能优化能力</strong>：学习性能优化的方法和技术，如代码优化、算法优化、资源管理优化等。能够对鸿蒙系统和应用进行性能分析和调优，提高系统的运行效率和响应速度。</li><li><strong>安全与隐私保护能力</strong>：了解安全与隐私保护的相关知识和技术，如数据加密、身份认证、访问控制等。能够在鸿蒙系统和应用的设计和开发中，充分考虑安全与隐私问题，确保用户数据的安全。</li></ul><h3>经验的积累</h3><p>鸿蒙开发团队的架构师需要具备丰富的项目经验。</p><ul><li><strong>项目实践</strong>：积极参与鸿蒙相关的项目开发，从简单的应用项目开始，逐步积累经验。在项目中，承担不同的角色和任务，如模块开发、架构设计、项目管理等，全面提升自己的能力。</li><li><strong>社区贡献</strong>：参与开源鸿蒙社区的开发和维护工作，贡献自己的代码和技术方案。通过与社区中的其他开发者交流和合作，学习先进的技术和经验，提高自己的知名度和影响力。</li><li><strong>技术分享与交流</strong>：积极参加鸿蒙技术相关的研讨会、讲座、线上论坛等活动，与同行进行技术分享和交流。了解行业的最新动态和技术趋势，拓宽自己的技术视野。</li></ul><h3>职业素养的培养</h3><p>鸿蒙开发团队的架构师需要注重职业素养的培养。</p><ul><li><strong>学习能力</strong>：鸿蒙技术在不断发展和更新，需要具备良好的学习能力，能够快速掌握新的技术和知识。保持学习的热情和好奇心，不断提升自己的技术水平。</li><li><strong>沟通能力</strong>：作为架构师，需要与团队成员、产品经理、其他部门等进行频繁的沟通和协作。具备良好的沟通能力，能够清晰地表达自己的想法和观点，倾听他人的意见和建议，推动项目的顺利进行。</li><li><strong>团队合作精神</strong>：在团队中，要能够与不同背景和专业的人员合作，发挥自己的技术优势，共同完成项目目标。具备团队合作精神，能够关心和帮助团队成员，营造良好的团队氛围。</li><li><strong>问题解决能力</strong>：在项目开发过程中，会遇到各种各样的问题和挑战。具备较强的问题解决能力，能够迅速分析问题的本质，提出有效的解决方案，确保项目的顺利进行。</li></ul><p>综上，要成为团队的架构师，“打铁还需自身硬”，除了下苦功夫，还需要针对性的对自身能力进行不断打磨。</p><p>这里推荐 <a href="https://link.segmentfault.com/?enc=9cdJbWmq565u9HULOCCriQ%3D%3D.nPFPM2P84cUwgaDvjK6dCgfO9Oxt8wJAvy8T4iSsmV9k6ldK5HTtBBYGiiZvXz1KiXoK1EZ7r%2Faz9cmR%2FaThITizkSjzHUlANTfxJ6%2FZIqk%3D" rel="nofollow" target="_blank">《鸿蒙架构师修炼之道》</a>（北京大学出版社）这本书。本书不但通过真实案例讲解架构设计流程和经验，还总结了丰富的鸿蒙架构师工作原则和技巧，读者可以对照本书内容进行查漏补缺，提升自身能力，早日踏上鸿蒙架构师修炼之道。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607853" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[LLaDA2.1 正式开源，可纠错编辑机制让 100B 扩散模型突破 892 TPS 速度极限 蚂蚁]]></title>    <link>https://segmentfault.com/a/1190000047607860</link>    <guid>https://segmentfault.com/a/1190000047607860</guid>    <pubDate>2026-02-12 16:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在大语言模型的技术版图中，自回归（AR）架构长期占据主导地位，而扩散模型则被视作一条充满挑战的“非共识”路线。LLaDA2.0 已经成功证明了扩散语言模型（dLLM）规模化至 100B 参数的可行性，但生成速度与生成质量的平衡始终是横亘在扩散模型面前的核心难题。</p><p>2月11日，我们正式发布 LLaDA2.1，通过可纠错编辑机制，首次让扩散语言模型在保持高质量的同时，将推理速度推至 892 TPS的新高度，让扩散语言模型从“研究探索”向“真正可用”迈进了一大步。</p><ul><li>模型：<br/><a href="https://link.segmentfault.com/?enc=F6METJ5skJDDSvadfWvScw%3D%3D.o7EhjYTjm6X%2FpQ96oOfIdGe9AmBFuw1TrzINvStSnhQ%2FLK7n3JT%2BgL6YWI2XHbJMPSir0Lvm9WBqc6DSYtBnyQ%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/collections/inclusionAI/llada21</a>；<a href="https://link.segmentfault.com/?enc=RC7PyZOO4msgxH9YrXsBfA%3D%3D.EUpNFZHi3rWWzqRoyDg2AOPvBzV%2FkBbaOsP03Ti7hVe9EXwwtOIiUjo0IFbACkHl9VsvAE6zL6r%2FrElstG4UDQ%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/collections/inclusionAI/LLaDA21</a></li><li>GitHub：<br/><a href="https://link.segmentfault.com/?enc=5IWZLcT5e5%2FxKadYkGMwbQ%3D%3D.0Lmd0HzFwlmUx5ov8sSLIV86gRAVwsTQvixxmpuX0JCkQX9SzKF2DUS6q7uprfnt" rel="nofollow" target="_blank">https://github.com/inclusionAI/LLaDA2.X</a></li><li>技术报告：<br/><a href="https://link.segmentfault.com/?enc=pbJ73v7DXo5Vojwm9xnnnQ%3D%3D.nWfzSvl8Y9r2zxUsemlCwLS2l9wc%2BwwFdwo1SvZ5ahLyNAar84RtZYtmqyeqCri9" rel="nofollow" target="_blank">https://huggingface.co/papers/2602.08676</a><br/>从“学术研究”到真正可用、甚至效率更优的强大工具，这一飞跃，源于以下三大技术亮点 ——</li></ul><h2>创新“可纠错编辑”机制  Error-Correcting Editable, ECE</h2><p>作为实现飞跃的最核心创新，它赋予了扩散模型一种前所未有的“智慧”——像人类专家一样“起草-编辑”。</p><p>传统自回归模型像是一个不允许带草稿纸、不允许带提纲的考生，它下笔无悔，不允许修改自己写好的答案。LLaDA2.1 从根本上重构了这一范式。我们提出了 Token-to-Token（T2T）编辑机制，让模型具备「起草-编辑」的双重能力，在毫秒级的闪电采样中完成“草稿”到“正卷”的转身：</p><ul><li>起草阶段：模型以较低的置信度阈值快速并行生成初始草稿；</li><li>编辑阶段：模型启动自我纠错，对已生成的 Token 进行回溯检查和迭代修正。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607862" alt="图片" title="图片"/><br/>图 1：传统吸收态范式 vs LLaDA2.1 可纠错编辑机制</li></ul><p>这种设计让扩散模型首次拥有了类似人类的「修改草稿」能力，解决了并行生成的误差累积问题，为高速解码奠定了理论基础。</p><h2>灵活“双模式”设计 Speedy Mode vs Quality Mode</h2><p>基于可纠错编辑机制，LLaDA2.1 提供了两种截然不同的运行模式，将速度与生成的选择权交还给用户：<br/>| Speedy Mode（极速模式）<br/>采用激进的低阈值策略进行 M2T 解码，以最大化并行度生成初始草稿，随后依赖 T2T 编辑机制进行精炼修正。这一模式实现了快速的推理速度，在代码生成等结构化任务中，仅带来可接受的性能折损。</p><p>| Quality Mode（质量模式）<br/>采用保守的高阈值策略，优先保证解码的精确性。在这一模式下，LLaDA2.1 在 33 项基准测试上全面超越 LLaDA2.0，并超越同类型扩散语言模型：<br/>代码能力：HumanEval+ 89.63%，CRUXEval-O 87.50%<br/>数学推理：AIME 2025 63.33%，GSM-Plus 89.69%<br/>智能体任务：BFCL v3 75.61%，IFEval 83.55%</p><p>双模式设计让用户真正成为速度与质量的决策者 —— 需要实时响应时选择 Speedy Mode，需要精确输出时则可以切换 Quality Mode，满足不同场景下的真实需求。</p><h2>业界首个 dLLM 大规模 RL 框架</h2><p>如果说“可纠错编辑”让模型变得“可用”，那么强化学习则让模型变得更“聪明”、更“可靠”，体感更强。LLaDA2.1 实现了首个专为 dLLM 设计的大规模强化学习框架。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607863" alt="图片" title="图片" loading="lazy"/><br/>图 2：LLaDA2.1 训练与推理框架概览</p><p>扩散模型的策略优化面临一个根本性障碍：序列级对数似然无法直接计算。在 100B 规模的扩散模型上跑通 RL 绝非易事。它不仅需要极强的工程底层支撑，要求我们从块状扩散（Block-diffusion）的条件概率转移视角，提出稳定的梯度估计算法，即 EBPO（ELBO-based Block-level Policy Optimization）：</p><ul><li>使用 Evidence Lower Bound (ELBO) 作为似然的合理代理；</li><li>结合 Vectorized Likelihood Estimation，实现边界估计的并行计算；</li></ul><p>EBPO 不仅提升了训练效率，更为 dLLM 的后训练优化提供了稳定、可扩展的解决方案。这一突破让强化学习首次能够稳定地扩展到扩散语言模型的后训练阶段，显著提升了模型的指令遵循能力和人类意图对齐度。</p><p>性能表现<br/>LLaDA2.1 开源两个版本：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607864" alt="图片" title="图片" loading="lazy"/></p><p>LLaDA2.1 在保持高质量生成的同时，实现了突破性的推理速度 —— 892 TPS，是传统自回归模型的数倍。在多个生成场景中，它都能以闪电般的速度完成；尤其是在代码领域，平均达到了 600-700 的 TPS，让用户体验如丝般流畅。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607865" alt="图片" title="图片" loading="lazy"/><br/>图 3：LLaDA2.1 在 Mini 和 Flash 系列上的吞吐量对比</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607866" alt="图片" title="图片" loading="lazy"/><br/>表 1：在不同场景上，LLaDA2.1 在 Mini 和 Flash 系列上的吞吐量</p><p>892 TPS 意味着什么？相当于每秒生成近 900 个 token，足以支撑实时交互、大规模部署等工业级应用场景。<br/>这一速度飞跃的背后，正是可纠错编辑机制的支撑 —— 正因为模型具备自我修正的能力，才敢在初始阶段采用激进的低阈值策略快速生成，而不必担心错误累积导致质量崩塌。</p><h2>结语</h2><p>LLaDA2.1 的意义不仅在于 892 TPS 的速度数字，更在于它证明了：通过技术创新，扩散语言模型完全可以在保持并行生成优势的同时，克服质量与速度的传统权衡。</p><p>可纠错编辑机制的引入，让 dLLM 第一次拥有了“自我修正”的智慧；双模式设计让用户真正成为速度与质量的决策者；强化学习框架则为扩散模型的后训练开辟了新的可能性。</p><p>我们诚挚邀请社区开发者体验 LLaDA2.1，也欢迎有志于探索 LLaDA 模型的同学加入我们，共同探索扩散语言模型的边界。蚂蚁技术研究院招聘火热进行中！多个研究课题，等你挑战！</p>]]></description></item><item>    <title><![CDATA[『NAS』在绿联部署一个白板工具-tldraw 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047607873</link>    <guid>https://segmentfault.com/a/1190000047607873</guid>    <pubDate>2026-02-12 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=g2lDP1u1YI3iRRH3tJ4eKQ%3D%3D.CaHRbvQ%2BnPzzXgsuIxgKyoR4SY83nLF6D6zgR82wAcRG%2FujGBKThMg7y6K6oOq6OYkrGBpBA1to117x%2F%2F05yQ6%2FbjQLpw3LLSQ4kgmw69AQ%2FM1QZi8YAMZ3mjopyIwDtnd43K3ebK7lZ7OBzmqs6iMUXhdXC0KsKd4wGEKIriVs%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>tldraw 是一款开源免费的轻量级协作白板工具，NAS 可以通过 Docker 就能一键部署，群晖、绿联、极空间等主流 NAS 全适配，无需复杂配置。它拥有无限手绘画布，支持画笔、形状、连线、文本便签等实用功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607875" alt="" title=""/></p><p>这次我用绿联 NAS 部署，其他品牌的 NAS 操作步骤也是差不多的。</p><p>打开“Docker”应用，在“镜像”页面搜索“tldraw”，下载红框选中的那个 <code>ratneo/tldraw</code>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607876" alt="" title="" loading="lazy"/></p><p>下载完成后，切换到“本地镜像”页面，点击 <code>ratneo/tldraw</code> 旁边的加号，创建容器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607877" alt="" title="" loading="lazy"/></p><p>在创建容器这页，容器名称可以自定义，我就不改了。</p><p>自动重启可以勾选上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607878" alt="" title="" loading="lazy"/></p><p>往下滑，”NAS端口“这项可以自定义，我这里选的是 <code>39445</code> 这个端口。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607879" alt="" title="" loading="lazy"/></p><p>部署完成后，浏览器输入 <code>NAS的IP:39445</code> 就能使用 tldraw 了。</p><p>它支持将画布内容导出为 <code>SVG</code>、<code>PNG</code> 和 <code>JSON</code> 这几种格式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607880" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=hAkveRjgWmbmqXYQbQz3aQ%3D%3D.WsWKrO0gBnmzmK5KbLu3INODiat1XzjPXdJBuYAO4C9xX86acregm9lTL5B4Gk2Ne5TwbznYrA2sYc8MgzQcO3%2BK8bAe130WYy9LDwRxvtCrCcs9FMmne%2BBEFM3CBSMOGrOUuoDfOITXLAj6ltPDII5E44Len0Rr0Dt1soB7iDY%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[千问app崩了！背后的技术困局值得所有AI产品警惕 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047607431</link>    <guid>https://segmentfault.com/a/1190000047607431</guid>    <pubDate>2026-02-12 15:12:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026年2月6日，不少网友的手机屏幕上都出现了熟悉又令人无奈的提示——千问APP加载失败、页面卡住、订单无法提交，甚至直接闪退。短短几小时内，“阿里巴巴千问崩了”冲上热搜，有人吐槽“蹲了半天奶茶福利，结果被崩到怀疑人生”，也有人调侃“运维小哥怕是要连夜搬救兵”。这场看似偶然的“崩溃事件”，表面是30亿奶茶福利引发的流量狂欢，实则暴露了AI产品在规模化落地中，从流量承载到工程化部署的一系列技术短板，值得整个行业深思。</p><p>不同于普通APP的崩溃，千问作为阿里旗下核心AI助手，其崩溃并非单一环节故障，而是“流量洪峰+AI负载”双重压力下，多技术环节协同失灵的集中爆发。结合官方回应及AI产品部署的共性问题，我们可以从四个核心维度，拆解此次崩溃背后的技术真相。</p><h2>一、流量突袭：超出预估的并发，压垮前端与网关</h2><p>此次千问崩溃的直接导火索，是其推出的“春节30亿大免单”活动——更新APP即送25元无门槛奶茶免单卡，邀请新用户可叠加福利，实实在在的优惠让用户蜂拥而至。数据显示，活动上线不到4小时，下单量就突破200万单，短时间内大量用户集中点击、分享、下单，形成了远超系统预估的瞬时流量洪峰，这成为压垮系统的第一道防线。</p><p>从技术层面看，这种突发高并发对AI产品的考验，远高于普通电商或社交APP。普通APP的请求多为简单的读写操作，而千问的用户在参与活动的同时，仍会使用其核心AI功能——问答、图像生成、实时翻译等，这意味着服务器要同时承载“活动下单”的高频轻请求和“AI推理”的高负载重请求，双重压力下，前端与网关率先失守。</p><p>前端层面，大量并发请求导致页面资源加载超时、接口调用失败，部分用户出现“点击无响应”的ANR（应用无响应）现象，甚至触发闪退——这并非单纯的前端优化不足，更在于未针对突发流量设计弹性适配机制，比如未设置请求排队、限流熔断，也未对活动页面进行静态资源缓存，导致每一次用户刷新都要向服务器发送新的请求，进一步加剧拥堵。</p><p>网关层面，作为所有请求的“入口闸门”，千问的网关可能未做好动态扩容准备。当瞬时请求量超出网关的承载阈值，就会出现请求堆积、路由失败的情况，部分请求无法正常转发至后端服务，进而表现为APP加载失败、页面空白。更值得注意的是，此次活动的社交裂变模式，原本是为了扩大传播，但微信对分享链接的屏蔽，让用户只能通过复制口令、跳转浏览器访问，反而让流量更加集中，相当于“几十万人同时挤一座窄桥”，网关的压力呈指数级上升。</p><h2>二、核心瓶颈：AI推理的“算力陷阱”，显存与并发的双重制约</h2><p>如果说突发流量是“导火索”，那么AI推理本身的技术特性，就是此次崩溃的“核心诱因”。不同于普通APP，千问的核心服务是大模型推理，而大模型在高并发场景下的算力消耗、显存占用，往往是技术部署的“重灾区”，也是很多AI产品容易忽视的短板。</p><p>此次崩溃的关键技术问题之一，很可能是高并发下的显存溢出（OOM）——这是大模型部署中最常见的“致命问题”。AI大模型本身占用大量内存，尤其是在加载模型或进行推理时，若未做好内存管理，极易引发崩溃。千问的大模型参数规模可观，每一次用户请求都需要占用一定的显存进行推理计算，而活动期间的高并发请求，会让多个推理任务同时进行，显存占用瞬间飙升。</p><p>更隐蔽的“显存杀手”是KV Cache的膨胀。大模型推理时，会缓存键值对（KV Cache）以加速自注意力计算，提升响应速度，但在高并发场景下，多个请求的KV Cache会叠加，尤其是当用户进行长文本问答、多轮交互时，缓存会随着交互长度线性增长，快速耗尽显存资源。如果千问未采用动态批处理、PagedAttention等先进技术，仍使用固定批处理模式，当大量请求同时涌入时，系统会尝试将它们拼成一个超大批次，显存瞬间被撑爆，进而导致服务崩溃——这也是很多大模型从Demo走向生产环境时，最容易踩的“工程化大坑”。</p><p>除此之外，算力分配不均也加剧了崩溃。千问的服务器需要同时兼顾活动板块的业务请求和AI推理的算力需求，当活动流量暴涨时，若未做好算力动态调度，大量算力会被活动的高频请求占用，导致AI推理任务排队、阻塞，进而引发整个系统的响应延迟、服务超时，甚至出现“牵一发而动全身”的连锁反应——虽然官方回应称“核心AI功能基本正常”，但从用户反馈来看，部分用户在活动期间使用AI问答时，也出现了响应变慢、加载失败的情况，本质就是算力分配失衡导致的。</p><h2>三、协同失灵：线程管理与依赖兼容的隐性隐患</h2><p>此次千问崩溃，还暴露了AI产品在线程管理、依赖库兼容等细节层面的技术漏洞，这些看似微小的问题，在高并发压力下，会被无限放大，成为系统崩溃的“压垮骆驼的最后一根稻草”。</p><p>线程阻塞与并发管理不当，是其中一个重要原因。AI推理任务通常需要在主线程之外的后台线程执行，若未合理管理线程，就可能导致ANR或闪退。比如，若千问的活动页面在处理下单请求时，未使用异步线程，而是在主线程中执行耗时操作，就会导致主线程阻塞，用户点击无响应，甚至触发APP闪退——这也是很多AI应用在移动端部署时的常见问题，尤其在高并发场景下，线程管理的漏洞会被快速暴露。</p><p>依赖库缺失或版本冲突，则可能导致部分设备出现闪退。千问作为一款跨平台AI应用，需要依赖多个第三方库（如TensorFlow Lite、OpenCV等）实现AI推理、图像处理等功能，若依赖管理不当，就可能出现兼容性问题。比如，部分低版本手机设备可能不支持某一依赖库的版本，或缺少相关的.so文件，在高并发请求的触发下，就会出现“UnsatisfiedLinkError”等异常，导致APP闪退；而不同设备对AI模型的兼容性差异，也可能让部分用户在加载模型时失败，进一步扩大崩溃的影响范围。</p><p>此外，数据同步延迟也成为用户吐槽的焦点。不少用户反馈，“邀请新用户后，免单次数没显示”“下单后订单状态迟迟不更新”，这看似是业务层面的问题，本质是技术层面的数据同步机制不完善。活动期间，大量用户同时进行邀请、下单操作，产生的海量数据需要实时同步至服务器，但高并发导致数据写入、同步延迟，缓存更新不及时，进而出现数据不一致的情况——这不仅影响用户体验，也会增加服务器的负担，进一步加剧系统拥堵。</p><h2>四、可观测性不足：故障定位滞后，应急响应受限</h2><p>一场成熟的技术故障应对，不仅需要快速的应急扩容，更需要精准的故障定位——而千问此次崩溃，也暴露了AI产品在可观测性建设上的不足，这也是导致故障初期无法快速缓解的重要原因。</p><p>对于大模型服务而言，可观测性并非简单的CPU、内存监控，而是需要构建专属的监控体系，涵盖吞吐量、推理延迟、显存利用率等核心指标。比如，需要监控Tokens per second（TPS）吞吐量，判断系统的处理能力；监控Time To First Token（TTFT）、Time Per Output Token（TPOT），掌握AI推理的响应速度；监控GPU利用率、显存利用率，及时发现显存溢出、算力不足的隐患。</p><p>但从此次事件来看，千问可能未完全构建起完善的大模型可观测性体系。一方面，对长尾延迟的监控不足——平均响应时间看似正常，但部分慢请求会拖累整个批次的处理速度，在高并发场景下，这种长尾延迟会快速扩散，导致系统卡顿、崩溃；另一方面，链路追踪能力不足，当故障发生时，无法快速定位是前端请求、网关转发、AI推理还是数据同步环节出现问题，只能采取“紧急扩容”这种粗放式的应对方式，导致故障缓解速度不及用户预期。</p><p>官方回应中提到“技术团队已紧急调配资源，扩容服务器”，这也从侧面说明，此次故障的应急响应更多依赖于“扩容”，而非精准的故障定位与修复——这并非千问个例，而是很多AI产品的共性问题：过度关注大模型的算法性能，却忽视了工程化部署中的可观测性建设，最终导致“故障来了，不知道问题在哪；修复故障，只能靠盲目扩容”。</p><h2>结语：AI产品的成熟，从来不止于算法</h2><p>此次千问APP崩溃，看似是一场“幸福的烦恼”——因活动太火爆导致流量超出预估，但背后折射出的，是整个AI行业在工程化部署中的普遍困境：当大模型从实验室的Demo，走向亿级用户的生产环境，算法的先进性只是基础，流量承载、算力调度、线程管理、可观测性建设等工程化能力，才是决定产品稳定性的关键。</p><p>随着春节临近，各大AI产品纷纷推出营销活动，试图抢占用户心智，但千问的教训提醒我们：AI产品的竞争，从来不止于算法的比拼，更在于工程化能力的较量。对于所有AI产品而言，想要避免“崩溃式翻车”，不仅要做好活动前的流量预估、系统扩容，更要补齐技术短板——采用动态批处理、PagedAttention等技术优化显存利用，构建完善的线程管理与依赖兼容体系，搭建专属的大模型可观测性平台，让故障能够被提前预警、快速定位、及时修复。</p><p>毕竟，对于用户而言，再强大的AI功能，再丰厚的福利，都抵不过一次流畅的使用体验。千问此次的崩溃，既是一次技术预警，也是整个AI行业走向成熟的必经之路——唯有正视工程化部署中的技术隐患，补齐短板，才能让AI产品真正走进用户的日常生活，实现技术价值与用户体验的双赢。而对于千问而言，如何在此次故障后优化技术架构、完善应急机制，避免类似问题再次发生，才是接下来最需要解决的问题。</p>]]></description></item><item>    <title><![CDATA[当CRM系统不堪重负，重药集团如何通过OceanBase实现“实时精准营销”？ OceanBase技]]></title>    <link>https://segmentfault.com/a/1190000047607447</link>    <guid>https://segmentfault.com/a/1190000047607447</guid>    <pubDate>2026-02-12 15:11:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：张红霞，青岛雨诺网络信息股份有限公司新零售产品部总监</p><p><strong>摘要：</strong><br/><strong><em>随着CRM会员系统的使用时间拉长，重药集团底层的传统数据库逐渐难以满足复杂数据的高效处理需求。面对海量交易和多维度行为数据的汇聚，需采用具备高可用、强一致、可扩展特性的数据库。其选择OceanBase，最终实现系统稳定运行、复杂场景实时分析、查询效率提升25倍、存储空间节约60%。</em></strong></p><p>当前，医药零售企业已不再满足于“卖药”，而是致力于成为“健康管理伙伴”。通过构建以 CRM 会员系统为核心、线上与线下深度融合的全渠道服务架构，企业实现了服务时间与空间的无限延展、会员数据的集中管理与智能应用、营销活动的精准触达与高效转化。</p><p>作为医药零售的头部企业，重庆医药（集团）股份有限公司（简称“重药集团”）前身是成立于1950年的中国医药公司西南区公司，服务于医药全产业链，同时从事医药研发（MAH）、医疗器械生产，并投资参与医药工业。重药集团拥有全级次分、子公司200余家，正在从传统的配送商业企业向“互联网+医药”融合型现代医药企业转型。</p><p>随着CRM会员系统的使用时间拉长，其底层的传统数据库逐渐难以满足复杂数据的高效处理需求。面对海量交易和多维度行为数据的汇聚，重药集团CRM会员系统亟需采用具备高可用、强一致、可扩展特性的数据库。经过对比三款国产分布式数据库，重药集团选择OceanBase，最终实现系统稳定运行、复杂场景实时分析、查询效率提升25倍、存储空间节约60%。</p><p>此次重药集团CRM系统的数据库升级不仅提升了用户体验与品牌忠诚度，也为后续集团构建高性能、高可用的“集团级数字化运营中枢”提供了明确的业务需求与数据基础，构建可扩展、可复制、可监管的集团化运营体系。</p><h2>医药零售商业模式变革，CRM系统实现全渠道协同</h2><p>随着消费者行为的数字化转型和健康需求的持续升级，医药零售行业正经历深刻的商业模式变革。传统药店“有啥卖啥”的经营逻辑，逐步向“顾客需要什么”的逻辑转变，除了提供到店服务外，还支持线上服务，比如通过企业微信、公众号等渠道建立长期沟通机制。微商城代客下单、在线解答疑问等。</p><p>为构建以专业化服务为基础的顾客信任体系，医药企业建立了完整的会员服务体系——CRM 会员系统，以实现绑定多重会员信息、建立精准的会员标签画像，为会员提供更多的服务和营销。通过数据驱动决策的专业化服务能力提升来提高企业在行业内的竞争力，实现增收。</p><p>如图1 所示，CRM 会员系统可以实现线上、线下全渠道协同，支持会员档案统一、标签体系完善、自动触发机制、店员触达赋能、社群营销等关键功能。完成顾客到店/线上购药 → 完成交易 → 数据沉淀至 CRM → 触发服务与营销 → 二次消费 → 再次触达，实现“交易—服务—再交易”的正向循环。<br/><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnU13" alt="" title=""/><br/>图1 CRM会员系统实现线上、线下全渠道协同</p><h2>为实现一体化管理需求，构建CRM会员系统</h2><p>重药集团CRM会员系统的搭建背景，源自于其各子公司会员管理分散，系统缺乏统一规划，导致数据难沉淀、服务差异大、运营难复制，且缺乏实时监控，难以支撑决策。</p><p>为实现一体化管理，重药集团CRM会员系统分阶段建设。第一阶段完成会员营销平台的底座建设，打造集团化、标准化、数据化运营基础，核心目标如下。</p><p>搭建集团化会员运营平台。实现集团—子公司—门店的一体化管理，打通组织架构与业务链路，确保会员在不同层级和渠道中都能获得一致的服务体验。</p><p>统一的会员运营服务体系。构建覆盖会员管理、营销活动、服务交付的标准化流程，减少分散运作带来的效率损耗，提升整体运营协同能力。</p><p>可快速复制标准化服务能力。形成可落地的服务模板和运营机制，帮助新业务和子公司快速复制成熟经验，缩短建设周期，提升推广效率。</p><p>实现经营数据统一分析。沉淀完整的数据资产，打破信息孤岛，实现对会员、门店、区域的多维度统一分析，为企业战略决策与合规审计提供有力支撑。</p><p>在上述目标指导下，我们做了三个核心举措：</p><p>联合集团会员中心，推进一体化进程。覆盖集团全品牌及线上会员，实现线上和线下会员统一运营和全域价值管理（见图2）。</p><p>构建多层级组织架构视角报表。支持集团、品牌、门店的权限管理，权限灵活配置，便于集团总部进行跨品牌的数据报表分析。</p><p>集团统一下达任务。集团可向各品牌下发销售任务、患者教育活动任务及拉新任务，实现集团任务统一管理与执行监督。<br/><img width="723" height="634" referrerpolicy="no-referrer" src="/img/bVdnU14" alt="" title="" loading="lazy"/><br/>图2 集团会员同意运营架构</p><p>我们计划以集团内个别区域公司为试点，试行以上举措，若成功，则进行全面推广。推广成功后，重药集团会员运营平台将实现从“单一业务系统”向“集团级数字化运营中枢”演进。依托统一的技术底座与标准化流程，平台不仅实现对多家子公司、多个品牌的全面接入，更构建起可扩展、可复制、可监管的集团化运营体系。</p><p>此外，为实现全渠道会员统一运营，平台通过整合分散在各系统中的数据，构建统一、动态、多维度的会员标签画像体系（见图3），支撑精细化运营决策。<br/><img width="723" height="338" referrerpolicy="no-referrer" src="/img/bVdnU15" alt="" title="" loading="lazy"/><br/>图3 多维度会员标签画像体系</p><p>通过会员系统精准化的服务来反哺我们的线上和线下的会员营销和服务，实现线上精准营销、个性化推荐、好物推送、会员关怀，线下关联用药建议、慢病管理提醒、店员主动触达等，提升营销转化率，增强客户粘性，实现“数据驱动服务”的闭环。</p><h2>精细化会员服务，带来海量数据的查询、存储难题</h2><p>然而，随着集团化会员运营平台的推进，精细化服务模式持续深化，导致用户数据规模呈指数级增长，显著提升了系统的查询与存储复杂性。</p><p>会员量：突破千万级，覆盖多个品牌及区域公司。<br/>交易数据量：达到亿级，涵盖线上线下购药、用券、复购等行为。<br/>用户行为类数据：包括商品浏览、搜索、加购等，总量亦达千万级以上。</p><p>这些数据来源于线上商城、私域平台、公众号等多个渠道，经标签体系整合后，用于构建立体化的会员画像，支撑精准营销与双向引流。</p><p>但数据体量大、类型多样、实时性要求高，对数据库的高并发读写能力、存储扩展性与查询性能提出严峻考验。面对千万级会员、亿级交易和多维度行为数据的汇聚，传统数据库难以满足高效处理需求，亟需采用具备高可用、强一致、可扩展特性的分布式数据库系统进行支撑。</p><h2>CRM会员系统数据库升级，应对千万级数据处理难题</h2><p><strong>传统数据库的技术瓶颈制约业务发展</strong></p><p>重药集团会员服务平台的规模化发展，使系统数据总量迅速增长至千万级、数十 TB 存储规模，传统关系型数据库在支撑精细化会员运营场景时，暴露出四大核心挑战。</p><p>性能：百万大表 InnoDB 在高并发读写及复杂查询场景下，性能显著下降，无法满足业务需求，且有事务访问，无法通过拆分提升性能。同时，业务强依赖事务一致性，无法通过拆分提升性能。</p><p>效率：核心归档由于业务需求，需要保留大量数据（数十 TB），会造成 DDL 周期长，延迟业务上线时间。</p><p>成本：随着企业数量增多、历年数据累积，存储成本将越来越高。</p><p>及时性：在各种场景下，对应数据处理的及时性需求越来越强。</p><p>上述技术挑战不乏真实业务案例。</p><p><strong>例 1：某大型连锁店，以满足信创要求为前提进行性能保障</strong></p><p>如今国家对信息技术应用创新（简称“信创”）的要求日益严格，特别是在国有企业中，系统必须符合相关标准才能上线。为了响应这一趋势，我们严格按照信创目录选择数据库产品，并对其进行了全面的业务场景适配与性能验证。</p><p>数据准备：会员卡 9950万+、订单 1 亿 9980万+。<br/>验证数据库：OceanBase 数据库、某数据库1、某数据库2。<br/>验证功能：报表 14 项内容、高级筛选 8 项内容。<br/>参考标准：报表查询小于 20s、静态化数据小于 60s、高级筛选小于 15s。</p><p>测试结果如图4所示。OceanBase 在所有测试项中均显著优于其他两个国产数据库，在报表查询、高级筛选、静态化数据三个场景的性能表现都远超预期：</p><p>报表查询小于 7s，平均提速 78 倍以上。<br/>高级筛选响应高级筛选小于 1s，速度提升 200–700 倍。<br/>静态化数据静态化数据小于 46s，效率提升 6.7 倍以上。<br/><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdnU16" alt="" title="" loading="lazy"/><br/>图4 OceanBase 数据库、某数据库1、某数据库2的测试结果</p><p>在严格遵循国家信创要求的前提下，OceanBase 不仅完全满足合规性准入条件，更在百亿级数据规模下的复杂查询与批量处理场景中展现出卓越性能，远超同类国产数据库产品。基于此，我们总结了三个数据库的性能数据，向客户提交了一份详细的分析报告。</p><p><strong>例 2：连锁会员、订单交易数据量增长迅速，实时性查询瓶颈</strong></p><p>除了信创需求外，客户对业务的实时性、及时性要求也越来越高。过去，企业主要依赖 BI 工具进行周期性报表生成，可容忍数小时甚至数天的数据延迟。然而，随着营销策略向精准触达和即时响应演进，业务人员需要在高价值客户识别、复购提醒触发、定向营销投放、健康知识推荐等场景中获取近实时数据支持。为实现精准服务，运营人员经常需要基于会员信息、会员属性、历史消费、会员标签、商品集合等多个维度进行多维组合筛选，由于关联维度过多，可能会出现查询失败、查询时间过长、范围跨度受限、复杂查询无法支持等问题，显然，这些问题是我们服务的客户无法接受的。</p><p><strong>例 3：海量业务数据，系统可用性与存储成本难平衡</strong></p><p>连锁医药企业会员体系的不断扩展和数字化运营的深入，必然会带来业务数据量的指数级增长，海量数据带来的高存储成本成为制约系统可持续发展的关键瓶颈之一。</p><p>用户数据：累计会员数量突破千万级（&gt;1000万）。<br/>交易流水：日均订单量达百万级，历史累计超过亿级（&gt;1亿条）。<br/>用户行为数据：包括浏览、搜索、加购、收藏等行为记录，总量亦达千万级以上。</p><p>单个业务数据库实例空间占用已达到 N 个 TB 级别，且随时间推移呈线性增长。随着客户数量增加和业务持续扩张，业务数据库实例的空间占用迅速攀升至数十TB甚至上百TB级别，这些数据不仅用于支撑日常业务运行，还需长期保留以满足合规审计、精准营销、客户画像构建等需求。企业面临保障性能与可用性的前提下降低存储成本的难题。</p><p>因此，引入具备高效数据压缩、自动冷热分层、弹性扩展能力的新一代分布式数据库，是实现“数据价值最大化、存储成本最小化”的必然选择。</p><h2>数据库技术引入，支撑海量交易数据的高效处理</h2><p>综合业务需求与传统数据库的技术瓶颈考虑，我们需要替换传统数据库，升级为高性能、稳定性强、成本低、 HTAP 一体化的分布式数据库。</p><p>自 2023 年起，我们开始系统性地评估并引入 OceanBase，历经技术认知、多轮测试、工具链验证、SaaS 级试点上线等关键阶段（见图5），最终成功应用于重药集团会员管理平台。<br/><img width="723" height="150" referrerpolicy="no-referrer" src="/img/bVdnU17" alt="" title="" loading="lazy"/><br/>图5 上线OceanBase的关键阶段</p><p><strong>1.技术引入与评估阶段（2023年）</strong></p><p>测试重点包括三部分。</p><p>其一，日常抖动测试。在对 OceanBase 初期测试时，我们首先进行了业务压力测试。低峰期业务配合100%模拟线上流量直接发压，高达4轮的压力测试，每次持续 3 小时以上。</p><p>其二，扩容/缩容测试。在业务流量低时进行相关操作验证。为了验证是否存在小概率事件，进行了为期一周的脚本自动扩、缩容操作以观察其稳定性。</p><p>其三，Add Index 测试。与扩容、缩容相仿，基于业务流量对1T大表进行多达几十次的add index操作，观察延迟情况。</p><p><strong>2.SaaS 产品试点上线（2023 年 12 月）</strong></p><p>在完成全面技术验证后，我司将 OceanBase 应用于内部 SaaS 类产品中，作为首个生产级试点场景。该阶段实现了：</p><p>数据库稳定运行于真实业务环境中。<br/>验证了迁移、运维、监控等全生命周期管理能力。<br/>积累了宝贵的实战经验，为后续客户项目打下坚实基础。</p><p><strong>3.重药集团项目正式上线（2025 年 4 月）</strong></p><p>基于前期充分验证与试点成果，我们于 2025 年 4 月正式启动重药集团会员管理平台项目，OceanBase 正式投入生产使用，支撑海量交易数据的高效处理。</p><h2>会员服务平台“新面貌”：稳定、高性能、低成本</h2><p><strong>构建标准化数据链路，稳定、高效处理海量数据</strong></p><p>目前，OceanBase 主要支撑重药集团会员服务平台的分析型业务场景，支撑高并发、多维度的会员数据查询、标签计算、报表生成及精准营销决策。其核心价值体现在：高效处理海量历史数据、支持复杂实时分析、保障查询性能与系统稳定性。</p><p>整个数据链路遵循“源系统 → CRM 中转清洗 → OceanBase 分析库”的三层架构，如图6所示。<br/><img width="723" height="313" referrerpolicy="no-referrer" src="/img/bVdnU18" alt="" title="" loading="lazy"/><br/>图6 会员服务平台的数据分析链路</p><p>数据来源（源系统）包括POS 订单数据、各渠道会员信息、组织人员数据、会员标签数据、档案测量数据、全部商品主数据。</p><p>中转与清洗层（CRM 系统）：所有原始数据通过定时抽取或实时接入方式进入 CRM 系统，进行统一的数据清洗、去重、合并与标准化处理。关键处理策略包括历史数据清洗、订单数据合并、积分逻辑处理、会员标签动态更新、消费行为计算、活跃度模型计算。</p><p>目标存储与分析层（OceanBase 分析库）：清洗后的数据通过同步机制实时或定时写入 OceanBase 分析库；并分为原始数据表、静态化处理表、日表/月表、报表中间表。</p><p>通过构建“源数据 → CRM 清洗 →  OceanBase 分析库”的标准化数据链路，实现了多源异构数据的统一整合、复杂分析场景的高性能响应、业务数据的长期留存与高效利用。</p><p><strong>会员精准筛选复杂场景，查询效率提升 25.7 倍</strong></p><p>在重药集团会员服务平台的实际运营中，多维度组合筛选（见图7）是支撑精细化营销与客户管理的核心功能。对于数据库而言，该功能是典型的复杂查询场景，用户需同时基于多个维度进行精确匹配，查询通常涉及多表关联、大量过滤条件和聚合计算，非常考验数据库的执行效率。我们通过开启 OceanBase 的列存模式（Columnar Storage），将原本传统数据库MySQL 的响应时间从 18 秒缩短至 0.7 秒，性能提升达 25.7 倍，满足业务对“实时圈选、即时触达”的严苛需求，显著提升了系统整体吞吐量与用户体验。<br/><img width="723" height="386" referrerpolicy="no-referrer" src="/img/bVdnU2a" alt="" title="" loading="lazy"/><br/>图7 会员服务平台多维度组合筛选</p><p><strong>数据存储空间省 60%，有效降低存储成本压力</strong></p><p>OceanBase 将全量数据划分为两个部分进行管理：一是增量数据（Memtable），即实时写入内存中的热数据，支持快速读写；二是基线数据（静态数据），即经过合并与持久化后的冷数据，存储于磁盘。</p><p>对于静态数据，OceanBase 采用高效的压缩算法，对列式存储的数据进行深度压缩，显著减少磁盘 I/O 和存储开销。例如，当原始数据总量为 4TB 时，MySQL 需要完整保留所有数据，存储空间占用为 4TB；而 OceanBase 通过对静态数据进行高压缩处理，仅需 1.5TB 即可承载相同规模的数据。</p><p>在重药集团会员服务平台的实际部署中，OceanBase 通过其先进的列式存储引擎与高效压缩算法，显著降低了数据存储空间占用，在同等业务数据规模下实现了 60% 以上的存储空间节约，有效缓解了海量数据带来的存储成本压力。</p><h2>面向未来，持续推进 OceanBase 的深度集成与价值释放</h2><p>随着 OceanBase 在重药集团会员服务平台的成功落地，我们对其在更广泛业务领域和客户群体中的应用充满信心。面向 2026 年及未来，我们将围绕场景拓展、客户推广、技术融合与产品适配四大方向，持续推进 OceanBase 的深度集成与价值释放。</p><p><strong>应用于更多业务场景与产品</strong></p><p>当前，OceanBase 已稳定支撑重药集团会员管理平台的复杂分析型业务（如精准筛选、标签计算、报表生成）。订单处理中心和运营诊断产品也在生产环境开始使用OceanBase，下一步，我们将推动其全面融入日常运营服务场景，包括：实时会员服务、营销活动执行、AI 智能推荐等业务场景。</p><p>另外，我们将逐步将 OceanBase 适配至更多内部产品，包括商品主数据管理、患者健康管理平台、智能补货与供应链协同系统，构建以 OceanBase 为核心的统一、弹性、智能的企业级数据基础设施。</p><p><strong>向业内客户推荐</strong></p><p>在国家信创政策与企业降本增效双重驱动下，我们已将 OceanBase 作为高并发、大数据量、强一致性要求场景下的首选数据库，并向行业客户积极推广。截至目前，已在以下大型医药企业成功落地：扬子江药业集团、鹭燕医学、重药集团、上海医药、国大药房。未来，我们将继续优先推荐 OceanBase 作为会员服务、订单中心等关键系统的数据库底座，助力更多企业完成安全、高效、低成本的国产化替代。</p><p><strong>交流开发，沉淀运维经验</strong></p><p>为持续提升团队与客户的 OceanBase 应用能力，我们计划定期组织专题培训、参与社区技术沙龙、共建问题解决机制、定期组织数据库培训及实战分享会议，探讨并解决遇到的问题，争取打造一支“懂业务、精技术、能落地”的复合型数据库应用团队。</p><p>未来，我们将携手更多合作伙伴，共同探索“数据库 + AI + 行业场景”的创新路径，为医药健康行业的高质量发展注入新动能。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=BbXh2tVImkhLkkP%2FBIYVSA%3D%3D.DYHnzQkGrG%2BS1%2FXzLBMyzWy1cIFksp%2FuKqnFk5aN1Wo%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[Python用SentenceTransformer、OLS、集成学习、模型蒸馏情感分类金融新闻文本]]></title>    <link>https://segmentfault.com/a/1190000047607449</link>    <guid>https://segmentfault.com/a/1190000047607449</guid>    <pubDate>2026-02-12 15:11:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=KU5xrBr5LvdyLZuH2qnWTQ%3D%3D.LWQDJa5M70rX3YRcW4%2FjkhPRWKs5ntv5MvPa1by8waQ%3D" rel="nofollow" title="https://tecdat.cn/?p=44976" target="_blank">https://tecdat.cn/?p=44976</a>  <br/>原文出处：拓端数据部落公众号  <br/> </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607451" alt="封面" title="封面"/></p><h3><a name="t2" target="_blank"/>引言</h3><p>在企业级AI真正落地的过程中，大语言模型（LLM）已经成为文本理解任务的核心工具，无论是金融新闻情感判断、行业舆情分析，还是内容风险识别，GPT-4、Gemini等模型都展现出极强的能力。但在规模化落地时，几乎所有数据科学家都会遇到同一个瓶颈：<strong>成本高、速度慢、难以实时处理、无法本地化部署</strong>。当数据量从万级跃升到千万级甚至亿级时，单纯依赖API调用不再具备工程可行性。  <br/>我们团队在长期为金融、资讯类客户提供AI咨询方案时，反复遇到这类问题：如何在不损失精度的前提下，把重型大模型“轻量化”，让普通服务器甚至单机就能跑高性能NLP任务。本文正是基于这类真实项目的技术沉淀，系统介绍一套<strong>集成-蒸馏</strong>的工程化范式：先用多个强模型集成得到超高精度“教师模型”，再通过句子嵌入与线性回归训练“学生模型”，最终实现<strong>轻量模型在金融新闻情感分类上超越GPT-4</strong>。整套方案兼顾精度、速度、成本与可扩展性，已在真实业务中稳定运行。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，该<strong>项目完整代码与数据</strong>已分享至交流社群。阅读原文进群，可与800+行业人士交流成长。  <br/> </p><h4><a name="t3" target="_blank"/>整体流程</h4><ol><li>构建代表性金融新闻数据集</li><li>多LLM标注 → 集成构建教师模型</li><li>句子Transformer生成文本嵌入</li><li>OLS线性回归学习教师输出</li><li>蒸馏得到轻量学生模型</li><li>规模化部署、低成本实时推理</li></ol><hr/><h2><a name="t4" target="_blank"/>背景与问题：大模型好用，但难落地</h2><p>在金融文本分析中，情感分类直接影响投研信号、舆情监控、风险预警等核心业务。过去我们对比过TextBlob、VADER、FinBERT等传统模型，也实测过PaLM-2、GPT-3.5、GPT-4、Gemini-Pro等大模型。结果显示，LLM在情感理解上显著优于传统模型，与人工标注一致性极高。  <br/>但在规模化场景中，问题立刻显现：</p><ul><li>千万级文本调用API成本极高</li><li>处理耗时长达数天甚至数周</li><li>无法满足近实时业务需求</li><li><p>模型不可控、难以本地化部署  <br/>这也是我们提出<strong>集成+蒸馏</strong>模式的初衷：<strong>保留大模型能力，剥离大模型负担，把能力装进轻量模型里</strong>。  </p><p>我们构建了覆盖多年、多行业、多新闻类型的新闻文本集，总计超万条样本，并在这批数据上系统评测了各类模型。以与人工标注一致率为指标，排名靠前的模型包括：Unicorn、GPT-4、Gemini-Pro等。</p></li></ul><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047607452" alt="" title="" loading="lazy"/></p><h2><a name="t5" target="_blank"/>Python用langchain、OpenAI大语言模型LLM情感分析AAPL股票新闻数据及提示工程优化应用</h2><h3><a name="t6" target="_blank"/>原文链接：<a href="https://link.segmentfault.com/?enc=9fq2BdZRnQQTcqbulrEXNw%3D%3D.rnap0KMKpMecKntXlreEM%2BTJaqYm6fGNNFp9gxtp3fw%3D" rel="nofollow" title="https://tecdat.cn/?p=39614" target="_blank">https://tecdat.cn/?p=39614</a></h3><hr/><h2><a name="t7" target="_blank"/>教师模型构建：多模型集成，精度再升级</h2><p>集成学习的核心思想是：<strong>组合多个模型，得到比单一模型更稳、更强的输出</strong>。但并非随便组合就能提升，只有满足以下条件，集成才有意义：</p><ul><li>单个模型效果优于随机</li><li>模型正确时预测相近</li><li>模型出错时互不相关  <br/>我们采用<strong>迭代增量集成策略</strong>，从最优单模型开始，逐步加入能带来正向增益的模型，最终构建出最优教师集成系统。  <br/>经过逐次测试，最优组合为：  <br/>Unicorn + GPT-3.5 + GPT-4 + SigmaFSA + Bison  <br/>最终集成模型与人工标注一致率达到 <strong>90.4%</strong>，比最优单一模型再提升近6个百分点。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607453" alt="" title="" loading="lazy"/>  <br/>我们通过阈值规则将集成输出转为离散情感标签：</li><li>大于1 → 正向</li><li>小于-1 → 负向</li><li>中间 → 中性  <br/>这套教师模型将作为后续蒸馏学习的“标准答案”。</li></ul><h2><a name="t8" target="_blank"/>学生模型构建：基于句子嵌入与OLS线性回归</h2><p>为了让轻量模型学会教师模型的能力，我们选择<strong>句子嵌入（Sentence Embedding）+ 普通最小二乘回归（OLS）</strong>作为学生模型主体。  <br/>句子嵌入可以把不定长文本转为固定维度向量，高质量嵌入能保留文本语义信息。我们选用主流Sentence Transformer模型，包括微软E5系列、BGE系列、GTE系列、Sentence-T5系列等。  <br/>回归模型我们选择最简单的<strong>线性回归</strong>，原因很明确：</p><ul><li>结构简单、推理极快、部署成本极低</li><li>可解释性强，适合金融等合规敏感场景</li><li>在优质特征下效果远超预期</li><li>支持单机、边缘设备、高并发场景  <br/>模型的核心思路是：  <br/><strong>用文本嵌入拟合教师模型输出 → 蒸馏得到轻量可部署模型</strong></li></ul><h2><a name="t9" target="_blank"/>核心实现代码</h2><p>以下为项目核心代码和数据，变量、结构、注释均已重构，保留关键逻辑，省略部分配置与模型列表代码，便于学习与复现。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607454" alt="" title="" loading="lazy"/></p><pre><code>import datetimeimport jsonimport numpy as npimport pandas as pdfrom sentence_transformers import SentenceTransformerfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_split# 存储实验结果exp_result = {}# 加载新闻数据与标注数据with open("news_data.json", "r", encoding="utf-8") as f: news_content = json.load(f)with open("news_labels.json", "r", encoding="utf-8") as f: news_label_info = json.load(f)# 构建教师模型集成分数for doc_id, label_info in news_label_info.items(): total_score = sum([ label_info["Text-Unicorn"], label_info["GPT-3.5-Turbo"], label_info["GPT-4-Original"], label_info["Sigma"], label_info["Text-Bison"], ]) news_content[doc_id]["teacher_score"] = total_score# 构建输入文本sentence_list = []for doc_id, content in news_content.items(): sentence_list.append(f"{content['Headline']}. {content['Description']}")# 选择需要测试的句子嵌入模型（部分省略）emb_model_list = [ "intfloat/e5-small-v2", "intfloat/e5-base-v2", "intfloat/e5-large-v2", "BAAI/bge-large-en-v1.5", "sentence-transformers/sentence-t5-large", ...... # 此处省略其余模型列表]# 遍历模型训练与评估for idx, model_name in enumerate(emb_model_list): # 加载模型 encoder = SentenceTransformer(model_name, trust_remote_code=True) # 计算参数量 param_count = sum(p.numel() for p in encoder.parameters()) start_time = datetime.datetime.utcnow() # 生成嵌入 embeddings = encoder.encode(sentence_list, show_progress_bar=False) dim_num = embeddings.shape[1] time_cost = (datetime.datetime.utcnow() - start_time).total_seconds() # 构建训练数据 out_df = pd.DataFrame.from_dict(news_content, orient="index") in_df = pd.DataFrame(embeddings, index=out_df.index) # 划分训练集测试集 x_tr, x_te, y_tr, y_te = train_test_split( in_df, out_df, test_size=0.25, random_state=42 ) # 构建教师标签 teacher_val = y_te["teacher_score"].values teacher_class = np.zeros(len(teacher_val)) teacher_class[teacher_val &lt;= -1] = -1 teacher_class[teacher_val &gt;= 1] = 1 # 训练线性回归模型 lr_model = LinearRegression() lr_model.fit(x_tr.values, y_tr["teacher_score"].values) # 预测与评估 y_pred = lr_model.predict(x_te.values) y_pred = y_pred.flatten() y_pred_class = np.zeros(len(y_pred)) y_pred_class[y_pred &lt;= -1] = -1 y_pred_class[y_pred &gt;= 1] = 1 match_num = np.sum(y_pred_class == teacher_class) acc = match_num / len(teacher_class) * 100 print(f"模型 {model_name} 准确率: {acc:.2f}%") exp_result[f"LR_{model_name.split('/')[-1]}"] = { "params": param_count, "time": time_cost, "dim": dim_num, "acc": acc }# 保存结果result_df = pd.DataFrame.from_dict(exp_result, orient="index")result_df.to_csv("model_compare_result.csv", index_label="model")</code></pre><h2><a name="t10" target="_blank"/>结果对比与分析</h2><p>我们一共评测了38个模型，包括教师模型、主流LLM以及30个基于不同句子嵌入的轻量学生模型。整体结论非常明确：<strong>优质嵌入+简单回归，即可在金融情感任务上超过GPT-4</strong>。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607455" alt="" title="" loading="lazy"/>  <br/>微软E5系列在各参数量级中表现都极为突出：</p><ul><li>小于1亿参数：e5-small-v2 最优</li><li>1–2亿参数：e5-base-v2 最优</li><li>3–4亿参数：e5-large-v2 最优</li></ul><h3><a name="t11" target="_blank"/>384维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607456" alt="" title="" loading="lazy"/></p><ul><li>LLM参考：Unicorn 89.04%，GPT-4 79.90%</li><li>最优轻量模型：e5-small-v2 76.54%</li></ul><h3><a name="t12" target="_blank"/>768维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607457" alt="" title="" loading="lazy"/></p><ul><li>最优：sentence-t5-large 80.40% <strong>（超过GPT-4）</strong></li></ul><h3><a name="t13" target="_blank"/>1024维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607458" alt="" title="" loading="lazy"/></p><ul><li>最优：e5-large-v2 80.21% <strong>（超过GPT-4）</strong></li></ul><h3><a name="t14" target="_blank"/>多语言模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607459" alt="" title="" loading="lazy"/></p><h2><a name="t15" target="_blank"/>核心价值：为什么这套方案值得企业落地</h2><p>我们从真实业务视角总结这套<strong>集成+蒸馏</strong>模式的四大优势：</p><h3><a name="t16" target="_blank"/>1. 效果更强</h3><p>sentence-t5-large、e5-large-v2等轻量模型在金融新闻情感分类任务上<strong>准确率超过GPT-4</strong>，同时逼近90%+级别的教师集成模型。</p><h3><a name="t17" target="_blank"/>2. 成本极低</h3><p>大模型API调用千万级文本成本极高，而蒸馏后的轻量模型<strong>推理边际成本几乎为0</strong>，一次性训练，终身复用。</p><h3><a name="t18" target="_blank"/>3. 速度极快</h3><p>GPT-4单条推理约0.5秒，千万级文本需数十天；  <br/>轻量模型单条推理约0.005秒，千万级<strong>一天内即可完成</strong>。</p><h3><a name="t19" target="_blank"/>4. 可复用、可扩展</h3><p>这套集成-蒸馏模式不局限于情感分类，可直接迁移到：</p><ul><li>时态判断</li><li>新闻主题分类</li><li>品牌安全检测</li><li>文本风险分级</li><li>政治倾向识别</li><li>前瞻性陈述判断</li><li>上千类网站内容分类</li></ul><h2><a name="t21" target="_blank"/>总结</h2><p>本文提出一套<strong>LLM集成+句子嵌入+线性回归蒸馏</strong>的文本理解轻量化方案，在金融新闻情感分类任务上实现了<strong>轻量模型超越GPT-4</strong>的效果。整套方法工程化程度高、成本低、速度快、稳定可靠，适合大规模文本实时处理场景，是大模型从“演示可用”到“规模化可用”的关键桥梁。  <br/> </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607451" alt="封面" title="封面" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[TikTok多账号运营？使用静态IP还是动态IP合适？ IPDEEP ]]></title>    <link>https://segmentfault.com/a/1190000047607470</link>    <guid>https://segmentfault.com/a/1190000047607470</guid>    <pubDate>2026-02-12 15:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在短视频盛行的时代，TikTok成为企业或个人品牌宣传的重要渠道。很多运营者为了提高内容曝光、测试不同营销策略，往往会选择多账号管理。但多账号运营并不是直接登录多个账号，这样会受到平台的风控限制，容易被批量封号。</p><p>IP的选择直接影响到账号的安全和运营效率。那么，静态IP和动态IP，哪种更适合多账号运营呢？下面就跟着IPDEEP小编一起来看看吧！<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnU2z" alt="TikTok多账号运营？使用静态IP还是动态IP合适？" title="TikTok多账号运营？使用静态IP还是动态IP合适？"/></p><p>一、为什么多账号运营需要注意IP问题？</p><p>TikTok对账号的安全性非常重视，如果同一台设备、同一网络频繁登录多个账号，容易触发系统风控。可能带来以下风险：</p><p>最严重可能被封号</p><p>视频无法正常推送</p><p>账号登录显示异常</p><p>这些问题往往与IP重复使用或异常变动有直接关系。因此，合理选择IP类型是多账号运营的基础。</p><p>二、静态IP与动态IP的区别</p><p>1.静态IP</p><p>静态IP指每次访问网络时IP地址固定不变。特点包括：</p><p>稳定性强：每次访问的IP都相同</p><p>适合长期绑定账号：系统识别为同一用户环境</p><p>缺点：</p><p>成本较高，如果一个静态IP管理多个账号，容易被系统检测到异常</p><p>2.动态IP</p><p>动态IP指每次连接网络可能获得不同的IP地址。特点包括：</p><p>随机性强：更接近于真实用户行为</p><p>降低账号封禁风险：系统难以通过IP判断异常</p><p>缺点：IP不固定，长期运营同一账号可能触发验证风险。</p><p>三、不同运营需求下的选择策略</p><p>1.多账号日常运营</p><p>如果你运营 5-10 个账号，主要是发布内容和观看互动：</p><p>推荐动态IP：更接近普通用户行为、降低单一IP异常封号风险、每个账号可以使用不同IP，实现“隔离”。</p><p>2.地域定向运营</p><p>如果你营销策略依赖于地域差异，例如运营美国、日本、欧洲等：</p><p>推荐静态IP：可以选择目标国家或地区IP、保证内容分发和推广稳定</p><p>3.账号长期绑定与广告投放</p><p>如果你运营少量核心账号，需要长期登录、绑定支付、广告账户：</p><p>推荐静态IP：减少系统频繁验证、保证登录环境一致</p><p>四、总结</p><p>动态IP：适合日常运营、多账号隔离、降低封号风险</p><p>静态IP：适合少量核心账号、广告投放或长期绑定操作</p><p>最终策略：结合你的运营目标，灵活选择IP类型，并配合账号分组和安全管理</p><p>TikTok多账号运营不是“多开就好”，科学的IP策略和账号管理才是保障安全和增长的关键。</p>]]></description></item><item>    <title><![CDATA[一杯奶茶背后的 AI 革命：通义千问自动下单技术原理解析 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047607473</link>    <guid>https://segmentfault.com/a/1190000047607473</guid>    <pubDate>2026-02-12 15:09:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3><strong>一杯奶茶背后的 AI 革命：通义千问自动下单技术原理解析</strong></h3><p>点一杯奶茶看似简单，背后却涉及自然语言理解、多模态交互、业务流程自动化和人机协同等一系列前沿AI技术。以阿里巴巴的通义千问大模型为核心的“自动下单”技术，正是这场静悄悄革命的一个缩影。</p><h4><strong>一、核心挑战：从“随意说”到“精准办”</strong></h4><p>传统点单要么依赖固定菜单选择，要么需要人工客服沟通。用户的需求是高度随意和非结构化的，例如：</p><blockquote>“来一杯冰的珍珠奶茶，三分糖，加一份芋圆，再去冰...哦不对，还是少冰吧，用代糖。”</blockquote><p>这句话里包含<strong>修改</strong>（“去冰”变“少冰”）、<strong>补充</strong>（“用代糖”）、<strong>口语化</strong>和<strong>非标准顺序</strong>。让AI理解并准确执行，需要突破三大关卡：</p><ol><li><strong>精准理解用户意图</strong>：识别出这是“下单”动作，而非询问或投诉。</li><li><strong>准确抽取复杂细节</strong>：从口语中提取“商品”、“属性”、“规格”、“定制要求”等结构化信息。</li><li><strong>与外部系统无缝对接</strong>：将结构化信息转换为订单系统API可调用的参数。</li></ol><h4><strong>二、技术原理拆解：三层架构协同</strong></h4><p>通义千问自动下单技术并非单一模型，而是一个以<strong>大语言模型</strong>为“大脑”的协同系统。</p><p><strong>第一层：智能理解与交互层（通义千问大模型核心）</strong></p><ul><li><strong>角色扮演与指令微调</strong>：模型被预先训练和微调为“专业的点单助手”，理解餐饮领域的术语、搭配禁忌和用户习惯。</li><li><p><strong>意图识别与槽位填充</strong>：将用户输入转化为结构化数据。</p><ul><li><strong>意图</strong>：<code>创建订单</code></li><li><strong>槽位</strong>：<code>商品:珍珠奶茶</code>， <code>温度:少冰</code>， <code>甜度:三分糖</code>， <code>加料:芋圆</code>， <code>糖类型:代糖</code></li></ul></li><li><p><strong>多轮对话与澄清</strong>：当信息缺失或矛盾时，模型会主动发起询问。</p><ul><li>用户：“我想喝奶茶。”</li><li>模型：“请问您想喝哪款奶茶呢？我们有珍珠奶茶、芋圆奶茶等。另外，需要选择甜度和冰度吗？”</li></ul></li></ul><p><strong>第二层：知识与企业数据层</strong></p><ul><li><strong>动态菜单库</strong>：连接商家后台实时更新的菜单、价格、库存和可定制选项。这是模型输出准确信息的依据。</li><li><strong>用户偏好记忆</strong>：在用户授权下，可记忆其历史订单、口味偏好（如“默认代糖”），实现个性化体验。</li><li><strong>业务规则库</strong>：例如，“芋圆和珍珠不能同杯”、“某款奶茶只能做去冰”等。模型需遵守这些规则，并在用户选择冲突时给出建议。</li></ul><p><strong>第三层：任务执行与集成层</strong></p><ul><li><p><strong>API调用与工具使用</strong>：这是“自动下单”的<strong>关键一步</strong>。通义千问具备 <strong>“函数调用”</strong> 能力。</p><ol><li>当模型确认订单信息完整后，会触发一个预定义的 <code>create_order()</code> 函数。</li><li>该函数将模型输出的结构化数据（JSON格式）自动转化为下单系统所需的参数。</li><li>系统调用后端API，正式创建订单，进入支付和制作流程。</li></ol></li><li><strong>多模态输入支持</strong>：用户不仅可以打字，还可以<strong>发送奶茶图片</strong>。通义千问的多模态能力可以识别图片中的商品，甚至分析“看起来糖很多，我下次要几分糖？”，将视觉信息转化为点单参数。</li></ul><h4><strong>三、技术亮点与革命性</strong></h4><ol><li><strong>零门槛自然交互</strong>：彻底摆脱了表单点单的僵硬感，用户可以用最自然的方式表达需求，甚至中英文混杂、带表情符号，技术包容了人的随意性。</li><li><strong>处理复杂性与模糊性</strong>：传统算法无法处理的修正、反问、指代（“换成那个”），大模型能结合上下文完美解决。</li><li><strong>从“问答”到“办事”的范式转变</strong>：通义千问在此场景中不仅是聊天机器人，更是一个具备<strong>工具使用能力的智能体</strong>。它的目标不是生成一段文字，而是完成一个现实世界中的任务（下单）。</li><li><strong>极大提升商业效率</strong>：将商家从重复性问答中解放出来，实现7x24小时自动接单，同时积累了宝贵的用户口味数据，用于优化产品。</li></ol><h4><strong>四、未来展望：不止于一杯奶茶</strong></h4><p>通义千问自动下单技术验证的范式，正在各行各业复制：</p><ul><li><strong>出行</strong>：说“帮我订一张明天最早去上海，靠窗的高铁票”，AI自动查询、比价、下单。</li><li><strong>办公</strong>：说“把上周的销售数据做成图表，发给团队”，AI自动操作数据库和PPT。</li><li><strong>智能家居</strong>：说“我睡觉时把空调调到26度，定时两小时”，AI自动控制设备。</li></ul><h4><strong>结语</strong></h4><p>一杯奶茶的自动下单，其意义远不止于“免去排队”。它标志着AI从“感知智能”（听、看、读）大步迈入“行动智能”（理解、规划、执行），成为连接数字世界与物理世界的桥梁。通义千问这类大模型作为“大脑”，正驱动着千行百业走向以自然语言为交互界面的智能未来。下一次，当你对手机说“来杯奶茶”并瞬间完成下单时，别忘了，你正在亲身经历一场静默而深刻的AI革命。</p>]]></description></item><item>    <title><![CDATA[联通CPS流量变现小程序系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047607479</link>    <guid>https://segmentfault.com/a/1190000047607479</guid>    <pubDate>2026-02-12 15:08:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>联通CPS流量变现小程序系统是一款专注于流量变现与粉丝变现的轻量级创业工具。该系统通过与联通官方渠道合作，为用户提供手机卡、话费等CPS（Cost Per Sale，按销售付费）推广项目，帮助流量主、公众号运营者、学生代理及线下商户实现低成本、高回报的变现目标。</p><p>核心优势：</p><ul><li>高佣金回报：返佣比例高于传统营业厅渠道（例如大王卡推广，营业厅返佣40元，本系统返佣64元）</li><li>长期变现周期：长达6个月的佣金锁定机制，有效留住用户和推广者</li><li>灵活合作模式：支持搭配话费模块，实现多元化盈利组合</li><li>低门槛创业：无需自主发布项目，提供现成工具与渠道，专注流量获取即可</li></ul><hr/><p>二、功能介绍</p><ol><li>多平台支持</li></ol><ul><li>支持微信小程序与抖音小程序双端部署</li><li>兼容PHP 5.6/7.1/7.2环境，技术适配性强</li></ul><ol start="2"><li>用户授权与隐私管理</li></ol><ul><li>获取用户基本信息（微信昵称、头像、性别、地区）</li><li>支持位置信息授权，便于本地化推广</li><li>相册访问权限，支持推广素材分享</li></ul><ol start="3"><li>CPS推广管理</li></ol><ul><li>项目展示：清晰展示联通各类号卡、套餐及话费充值项目</li><li>佣金追踪：实时查看推广订单与佣金收益</li><li>数据统计：多维度数据分析，优化推广策略</li></ul><ol start="4"><li>渠道合作工具</li></ol><ul><li>代理分销：支持发展下级代理（如学生代理、手机店合作）</li><li>流量接入：无缝对接公众号粉丝、信息流广告等流量来源</li><li>长期锁客：6个月佣金周期设计，提升用户留存与复购</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><p>场景类型 具体应用</p><p>公众号流量主 将粉丝流量转化为实际收益，通过推文、菜单栏推广号卡业务</p><p>学生/校园代理 利用学生社交网络，推广低价套餐，赚取佣金</p><p>线下商户合作 与手机店、便利店合作，线下引流+线上成交</p><p>信息流投放 结合抖音、微信广告，精准投放高转化人群</p><p>个人轻创业 零库存、零成本，仅需推广即可获利的副业选择</p><p>行业价值</p><ol><li>运营商渠道革新：打破传统营业厅高门槛、低返佣的局限，为中小推广者提供官方授权的高收益渠道</li><li>流量变现效率提升：将泛流量精准转化为通信消费用户，变现路径短、转化率高</li><li>灵活就业支持：为个体创业者、学生群体提供合规、可持续的副业收入模式</li><li>生态协同效应：可与话费充值、流量包等模块组合，构建通信服务生态，提升用户LTV（生命周期价值）</li></ol><hr/><p>四、问答环节（Q&amp;A）</p><p>Q1：这个系统需要自己开发吗？技术门槛高吗？</p><p>A：不需要自主开发。本系统提供现成的微信小程序和抖音小程序源码，支持PHP环境一键部署。您只需专注流量获取和推广，技术维护由平台方支持。</p><p>Q2：佣金结算周期是多久？如何提现？</p><p>A：系统采用长达6个月的佣金锁定机制，具体结算规则需咨询官方客服。通常CPS项目在用户完成首充且满足在网时长后结算，确保推广质量。</p><p>Q3：能否自己发布其他项目（如移动、电信卡）？</p><p>A：当前版本暂不支持自主发布项目。如需自助发布多运营商项目，可在微擎服务市场购买配套终端系统（ID: 3764）。</p><p>Q4：推广佣金为什么比营业厅还高？</p><p>A：本系统直接对接联通官方CPS渠道，减少中间层级，将更多利润让渡给推广者。例如大王卡推广，营业厅返佣40元，本系统返佣64元（需开票），优势明显。</p><p>Q5：适合完全没有经验的新手吗？</p><p>A：非常适合。系统定位为"小成本创业"工具，提供完整推广渠道和后台管理。您可以通过公众号、朋友圈、校园代理等低门槛方式启动，官方也提供客服咨询支持。</p><p>Q6：用户数据安全如何保障？</p><p>A：系统严格遵循微信小程序隐私协议，仅获取必要的用户信息（昵称、头像、位置等），所有数据通过加密传输，符合《个人信息保护法》要求。</p><p>Q7：除了联通卡，还能推广其他产品吗？</p><p>A：当前版本聚焦联通CPS项目。但系统支持搭配话费充值模块，可扩展通信服务品类，实现多元化变现。</p><hr/><p>温馨提示：官方强调请勿线下交易，90%的欺诈、纠纷、资金盗取均由线下交易导致。请通过微擎应用市场正规渠道购买与服务。如有疑问，可联系页面客服咨询。</p>]]></description></item><item>    <title><![CDATA[城市商家营销平台详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047607483</link>    <guid>https://segmentfault.com/a/1190000047607483</guid>    <pubDate>2026-02-12 15:08:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>城市商家营销平台是一款专为本地生活服务和商家营销打造的微信小程序系统，由盛亚网络开发，基于微擎平台运行。该产品定位为"城市圈+商家活动"的综合营销解决方案，通过整合商家活动发布、同城信息、积分商城、签到打卡等功能，帮助运营者构建本地商家联盟生态，实现商家引流、用户活跃、平台变现的多方共赢。</p><p>核心定位：打造城市级商家营销聚合平台，连接本地商家与消费者，提供从信息发布到交易转化的完整营销闭环。</p><p>产品优势：</p><ul><li>功能全面：集活动营销、信息发布、会员体系、积分商城于一体</li><li>多端支持：同时支持微信公众号与微信小程序双端运营</li><li>快速部署：基于微擎系统，源码加密交付，安装即用</li><li>成本可控：首期购买赠送6个月服务套餐，续费价格透明</li></ul><hr/><p>二、功能介绍</p><ol><li>商家活动管理</li></ol><ul><li>支持商家自主发布各类营销活动（优惠、促销、新品体验等）</li><li>活动信息展示、报名、核销全流程管理</li><li>活动数据统计与效果分析</li></ul><ol start="2"><li>同城信息发布</li></ol><ul><li>打造本地化信息交流平台</li><li>支持分类信息展示（招聘、房产、二手交易等）</li><li>用户可自主发布或浏览同城资讯</li></ul><ol start="3"><li>积分商城系统</li></ol><ul><li>用户通过签到、消费等行为获取积分</li><li>积分可兑换商品或抵扣现金</li><li>商家可设置积分规则，提升用户粘性</li></ul><ol start="4"><li>每日签到功能</li></ol><ul><li>连续签到奖励机制，培养用户打开习惯</li><li>签到积分与商城打通，形成激励闭环</li></ul><ol start="5"><li>商家入驻体系</li></ol><ul><li>多商家入驻审核与管理</li><li>独立商家后台，自主管理店铺信息与商品</li><li>平台统一结算与分润机制</li></ul><ol start="6"><li>商家产品发布</li></ol><ul><li>支持商品/服务上架展示</li><li>产品详情页自定义</li><li>在线支付与订单管理</li></ul><ol start="7"><li>会员等级体系</li></ol><ul><li>普通会员与VIP会员分级管理</li><li>消费频次自动升级机制（如消费满10次升级为银会员）</li><li>差异化权益设置，促进用户消费升级</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><p>场景类型 具体应用</p><p>本地生活平台 城市/区县级的本地服务聚合平台，整合餐饮、娱乐、教育等商家资源</p><p>商圈运营 购物中心、商业街统一营销平台，组织联合促销活动</p><p>行业协会 餐饮协会、零售联盟等组织为会员企业提供数字化营销工具</p><p>创业运营 个人或团队运营本地同城信息平台，通过商家入驻实现盈利</p><p>社区服务 物业公司或社区组织构建周边商家服务圈</p><p>行业价值</p><p>对平台运营者：</p><ul><li>盈利模式多元：通过商家入驻费、广告位、交易抽佣、增值服务等多渠道变现</li><li>运营效率高：一站式管理多商家、多活动，降低人力成本</li><li>用户粘性强：积分+签到+会员体系形成完整用户留存机制</li></ul><p>对入驻商家：</p><ul><li>获客成本低：共享平台流量，精准触达本地消费者</li><li>营销工具丰富：活动发布、优惠券、积分兑换等多样化营销手段</li><li>数字化转型快：无需自建系统，快速拥有线上展示与交易能力</li></ul><p>对终端用户：</p><ul><li>信息获取便捷：一站式发现本地优惠与资讯</li><li>消费实惠多：积分兑换、会员专享、活动福利等实际优惠</li><li>互动体验好：签到打卡、同城交流增加使用趣味性</li></ul><hr/><p>四、问答环节</p><p>Q1：这款系统适合什么类型的企业或个人使用？</p><p>A：主要适合三类用户：一是想运营本地同城生活平台的创业者或公司；二是需要为会员商家提供数字化营销工具的行业协会或商圈管理方；三是希望整合周边商家资源，打造社区服务生态的物业公司或社区组织。</p><p>Q2：商家入驻后如何管理自己的店铺？</p><p>A：系统为每个入驻商家提供独立后台，商家可自主发布活动、上架产品、管理订单、查看经营数据。平台运营方可在后台审核商家资质，设置入驻费用和分润比例。</p><p>Q3：积分商城的商品从哪里来？</p><p>A：积分商品可由平台运营方统一提供，也可开放给入驻商家自行上架。用户通过每日签到、消费、分享等行为获得积分，在商城兑换礼品或优惠券，形成"行为-奖励-消费"的闭环。</p><p>Q4：系统是否支持在线支付？</p><p>A：支持。系统已集成微信支付能力，用户可直接在小程序内完成活动报名、商品购买等支付操作，资金直接进入平台账户，再按规则与商家结算。</p><p>Q5：系统是否支持定制开发？</p><p>A：该产品为标准化源码交付（已加密），如需深度定制，可通过微擎服务市场联系开发者盛亚网络进行定制开发，或寻找第三方开发团队基于微擎框架进行二次开发。</p>]]></description></item><item>    <title><![CDATA[自定义关注码微信公众号系统 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047607487</link>    <guid>https://segmentfault.com/a/1190000047607487</guid>    <pubDate>2026-02-12 15:07:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>自定义关注码是一款基于微擎平台的微信公众号营销工具，专注于解决"关注后自动弹出链接"的引流需求。该系统支持微信小程序和抖音小程序双端定制开发，通过智能二维码技术实现精准获客与用户转化。</p><p>核心定位：将传统静态关注二维码升级为动态智能入口，用户扫码关注公众号后，系统自动推送预设链接（活动页、商城、优惠券等），实现"关注即转化"的营销闭环。</p><p>产品亮点：</p><ul><li>源码加密交付：已加密源码保障商业安全性</li><li>极简操作：无需复杂配置，快速生成带参数的关注二维码</li><li>高兼容性：支持PHP5.6/7.1环境，适配主流服务器配置</li></ul><hr/><p>二、功能介绍</p><ol><li>智能二维码生成系统</li></ol><ul><li>自定义生成带参数的关注二维码</li><li>支持多场景二维码管理（不同渠道、不同活动独立追踪）</li><li>二维码样式自定义（颜色、Logo嵌入等）</li></ul><ol start="2"><li>关注自动回复引擎</li></ol><ul><li>用户关注公众号后，即时推送预设链接/图文/小程序卡片</li><li>支持多条消息自动回复（文字+链接组合）</li><li>可设置关注后延迟推送，优化用户体验</li></ul><ol start="3"><li>数据统计分析</li></ol><ul><li>实时统计各二维码扫码量、关注转化率</li><li>用户来源渠道追踪（A/B测试不同渠道效果）</li><li>数据报表导出，支持运营决策</li></ul><ol start="4"><li>多场景链接配置</li></ol><ul><li>支持跳转外部H5页面、微信小程序、抖音小程序</li><li>可配置优惠券领取页、活动报名页、商品详情页等</li><li>链接有效期管理，过期自动切换备用链接</li></ul><ol start="5"><li>权限与安全管理</li></ol><ul><li>后台操作日志记录</li><li>管理员权限分级</li><li>接口调用频率限制，防止恶意攻击</li></ul><hr/><p>三、适用场景与行业价值</p><p>核心适用场景</p><p>场景类型 具体应用 价值体现</p><p>线下引流 门店海报、产品包装、展会物料印刷二维码 将线下流量高效导入线上私域</p><p>活动推广 扫码关注领红包/优惠券/参与抽奖 降低获客成本，提升活动参与率</p><p>内容变现 关注后自动推送付费文章/课程链接 缩短内容变现路径</p><p>电商导流 扫码关注直达商城爆款页面 提升关注-购买转化率</p><p>社群裂变 不同渠道二维码追踪裂变效果 精准识别优质推广渠道</p><p>重点服务行业</p><ol><li><p>零售电商行业</p><ul><li>解决痛点：平台流量贵、用户留存难</li><li>应用价值：通过包裹卡、售后卡印刷关注码，将淘宝/京东买家导入微信私域，实现复购提醒、专属客服</li></ul></li></ol><ol start="2"><li><p>教育培训行业</p><ul><li>解决痛点：试听转化低、课程推广成本高</li><li>应用价值：线下传单、电梯广告扫码关注，自动推送试听课程链接，降低销售跟进成本</li></ul></li></ol><ol start="3"><li><p>餐饮外卖行业</p><ul><li>解决痛点：平台抽成高、用户数据缺失</li><li>应用价值：桌贴、外卖单印刷关注码，引导用户关注后领取"加群享8折"优惠券，构建私域流量池</li></ul></li></ol><ol start="4"><li><p>本地生活服务</p><ul><li>解决痛点：同城获客难、用户粘性低</li><li>应用价值：商圈合作物料、社区公告栏投放，扫码关注获取本地优惠信息，提升到店率</li></ul></li></ol><ol start="5"><li><p>会展活动行业</p><ul><li>解决痛点：现场签到慢、资料发放混乱</li><li>应用价值：参展证、会议手册印刷二维码，扫码关注自动获取电子资料、日程提醒</li></ul></li></ol><p>行业价值总结</p><ul><li>降本增效：相比传统"关注后回复关键词"模式，减少用户操作步骤，转化率提升30%+</li><li>数据驱动：精准追踪各渠道ROI，优化营销预算分配</li><li>私域沉淀：将公域流量（抖音、线下）高效沉淀至微信生态，构建可反复触达的用户资产</li><li>合规安全：源码加密+官方审核，避免封号风险</li></ul><hr/><p>四、产品参数与购买信息</p><ul><li>交付方式：微擎系统在线交付（已加密源码）</li><li>适用平台：微信公众号（支持小程序跳转）</li><li>技术环境：PHP5.6 / PHP7.1</li><li>服务周期：首次购买赠送3个月服务套餐（含更新）</li><li>隐私获取：用户信息（昵称、头像、性别、地区）、位置信息、相册权限</li></ul><hr/><p>五、常见问题解答（FAQ）</p><p>Q1：这个系统和普通公众号关注回复有什么区别？</p><p>A：普通关注回复只能发送文字/图文消息，而自定义关注码可以直接弹出外部链接或小程序，用户无需手动输入关键词，体验更流畅，转化率更高。特别适合需要立即跳转至活动页、商城等场景。</p><p>Q2：生成的二维码长期有效吗？可以修改跳转链接吗？</p><p>A：是的，生成的二维码长期有效。您可以在后台随时修改该二维码对应的跳转链接，无需重新印刷物料，实现"一码多用"，大幅降低线下物料更换成本。</p><p>Q3：是否支持抖音小程序？如何与微信生态打通？</p><p>A：系统支持微信小程序和抖音小程序双端定制开发。抖音用户扫码后，可通过中间页引导关注微信公众号，实现跨平台流量沉淀（需遵守各平台规则）。</p><p>Q4：源码已加密，是否影响二次开发？</p><p>A：源码采用商业加密保护知识产权，不支持二次开发。但系统提供丰富的后台配置选项，可满足90%的常规业务场景需求。如需深度定制，可联系开发商（Cchualian001）咨询定制开发服务。</p><p>Q5：购买后是否包含安装部署服务？</p><p>A：产品为在线交付，需自行在微擎系统中安装。微擎平台提供详细安装教程，技术基础薄弱用户可选择平台"服务器运维服务"或联系卖家提供付费部署支持（卖家服务时间：周一至周五 09:00-23:30）。</p><p>Q6：3个月服务套餐到期后，系统还能继续使用吗？</p><p>A：服务套餐到期后，已生成的二维码和基础功能仍可正常使用，但无法获取版本更新和技术支持。建议及时续费以获得最新功能和安全补丁。</p><p>Q7：如何追踪不同渠道的推广效果？</p><p>A：系统支持渠道二维码管理，可为每个渠道（如"电梯广告A座"、"地推张三"）生成独立二维码，后台自动统计各渠道的扫码量、关注数、转化率，实现精准的效果评估和佣金结算。</p><p>Q8：是否支持关注后推送多条消息？</p><p>A：支持。可配置关注后延迟推送策略，如：立即发送欢迎语 → 5秒后推送活动链接 → 1小时后推送优惠券提醒，模拟真人客服互动节奏，提升用户体验。</p><p>Q9：用户隐私信息获取是否合规？</p><p>A：系统严格遵循微信官方接口规范，仅获取用户公开信息（昵称、头像等）和必要的位置/相册权限（用于个性化服务），所有数据调用均会弹出授权提示，符合《个人信息保护法》要求。</p><p>Q10：购买前能否试用？</p><p>A：页面提供在线演示入口，建议联系卖家（点击"立即咨询"）获取测试账号，体验后台功能后再做决定。微擎VIP用户享30天无售后急速退款保障。</p><hr/>]]></description></item><item>    <title><![CDATA[火拼小程序团长版：团购新模式，购物娱乐两不误 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047607492</link>    <guid>https://segmentfault.com/a/1190000047607492</guid>    <pubDate>2026-02-12 15:07:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>火拼小程序团长版是微擎应用市场上一款创新的社交电商系统源码，由开发者wangjiasiwei开发。该产品定位为"团购新模式，购物娱乐两不误"，核心特点包括：</p><ul><li>适用平台：微信小程序（支持PHP5.3/5.4）</li><li>交付方式：微擎系统在线交付，源码已加密</li><li>开发者资质：已完成实名认证和企业认证，信誉指数5.00分，应用评分5.00分</li></ul><p>产品核心理念是"组团购物，像我所得，分销分享"，通过军、团制度引领互联网新模式，引爆电商行业，实现爆品购买与社交组团的结合。</p><hr/><p>二、功能介绍</p><p>基于页面信息，该模块主要功能包括：</p><p>核心功能</p><ol><li>团长制度：建立完善的团长管理体系，支持团长招募、管理与激励</li><li>拼团购物：支持用户发起或参与拼团，享受团购优惠价格</li><li>分销分享：内置分销机制，支持用户分享商品赚取佣金</li><li>红包系统：集成红包功能，可用于营销活动和用户激励</li><li>返利机制：购物返利功能，提升用户复购率</li></ol><p>系统特性</p><ul><li>社交裂变：通过社交关系链实现快速传播和用户增长</li><li>爆品营销：支持打造爆款商品，集中流量推广</li><li>多场景支付：支持微信支付等多种支付方式</li><li>用户权限管理：完善的会员等级和权限体系</li></ul><p>技术特点</p><ul><li>源码加密：保护核心代码安全</li><li>微擎框架：基于成熟的微擎系统开发，稳定性高</li><li>微信小程序：深度适配微信生态，用户体验流畅</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><ol><li>社区团购：适合社区团长发起邻里拼团，降低物流成本</li><li>社交电商：利用社交网络进行商品推广和销售</li><li>爆品营销：打造限时爆款，快速清库存或推新品</li><li>分销体系：建立多级分销网络，扩大销售渠道</li><li>本地生活服务：结合线下商家，提供团购优惠</li></ol><p>行业价值</p><ul><li>降低获客成本：通过社交分享和团长裂变，显著降低流量获取成本</li><li>提升转化率：拼团模式创造紧迫感，刺激用户快速下单</li><li>增强用户粘性：分销返利机制激励用户持续参与和分享</li><li>快速起量：适合新品牌或新产品快速积累初始用户和销量</li><li>数据沉淀：完整的用户行为数据，支持精准营销决策</li></ul><hr/><p>四、常见问题解答（Q&amp;A）</p><p>Q1：这个系统适合什么类型的商家使用？</p><p>A：特别适合社区团购创业者、社交电商运营者、有分销需求的品牌商，以及希望快速搭建拼团平台的中小企业。军、团制度设计使其在团队管理和裂变增长方面具有优势。</p><p>Q2：源码已加密，是否支持二次开发？</p><p>A：源码已加密，主要保护核心功能模块。如需深度定制，建议联系开发者wangjiasiwei咨询定制开发服务，或购买其提供的定制开发套餐。</p><p>Q3：系统是否支持多商户入驻？</p><p>A：页面信息显示为"团长版"，主要聚焦于团长管理和拼团功能。如需多商户版本，可关注同系列的"火拼返利团购运营版"或"火拼v3返利团购"产品。</p><p>Q4：如何保证交易安全？</p><p>A：微擎平台提供官方正品保障，支持平台内交易。特别提醒：切勿线下交易，90%的欺诈、纠纷、资金盗取均由线下交易导致。平台VIP用户可享受30天无售后急速退款服务。</p><p>Q5：系统对服务器有什么要求？</p><p>A：支持PHP5.3和PHP5.4环境，适用于标准微擎系统部署。建议使用稳定的云服务器以确保系统性能。</p><p>Q6：是否提供使用培训和售后支持？</p><p>A：开发者提供周一至周六9:00-18:00的在线客服支持。同时可访问微擎官方文档中心获取使用指南和开发文档。</p><p>Q7：相比其他拼团系统，这个版本有什么独特优势？</p><p>A：核心差异化在于"军、团制度"设计，这种组织架构更适合团队化运营和层级管理，配合红包、返利机制，在社交电商领域具有独特的运营优势。</p><hr/>]]></description></item><item>    <title><![CDATA[七夕土味情话微信公众号系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047607496</link>    <guid>https://segmentfault.com/a/1190000047607496</guid>    <pubDate>2026-02-12 15:06:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <ol><li>概述总结</li></ol><p>七夕土味情话是一款专为七夕情人节打造的互动营销小程序系统，基于微擎平台开发，适用于微信公众号生态。该系统通过趣味答题+社交分享的玩法，帮助商家在节日期间快速吸粉引流，增强用户互动。</p><p>核心定位：情人节专属互动营销工具，以"土味情话"答题为核心玩法，结合分享裂变机制，实现线上活跃气氛、线下引流的双重目标。</p><p>产品基础信息：</p><ul><li>交付方式：微擎系统在线交付，源码已加密</li><li>技术环境：支持PHP 7.1+</li><li>适用平台：微信公众号</li></ul><hr/><ol start="2"><li>功能介绍</li></ol><p>核心功能模块</p><ol><li>答题互动系统</li></ol><ul><li>用户每日获得固定答题次数，参与土味情话趣味问答</li><li>后台支持自定义题库，商家可灵活配置题目内容</li><li>答题排名机制，激发用户竞争心理</li></ul><ol start="2"><li>社交裂变机制</li></ol><ul><li>每日分享功能：用户分享后可额外获得答题次数</li><li>强制分享传播，实现病毒式扩散</li><li>支持分享海报生成，提升传播效果</li></ul><ol start="3"><li>奖品核销系统</li></ol><ul><li>支持实物奖品设置，答题排名达标即可获得</li><li>线下核销功能，引导用户到店领取</li><li>打通线上线下，实现精准引流</li></ul><ol start="4"><li>后台管理系统</li></ol><ul><li>用户数据统计：实时监控参与人数、分享次数</li><li>活动范围控制：可限定参与地区或人群</li><li>图片自定义：支持界面个性化配置</li><li>题库管理：自由添加、编辑、删除题目</li></ul><ol start="5"><li>营销辅助功能</li></ol><ul><li>红包奖励机制（霸榜红包）</li><li>活动页面自定义配置</li><li>多维度数据报表导出</li></ul><hr/><ol start="3"><li>适用场景与行业价值</li></ol><p>核心适用场景</p><p>场景类型 具体应用 价值体现</p><p>商超营销 七夕期间举办答题赢奖品活动 节日氛围营造+到店客流提升</p><p>餐饮引流 答题送优惠券/霸王餐 线上互动→线下消费转化</p><p>自媒体运营 公众号节日吸粉活动 低成本快速增粉，提升活跃度</p><p>品牌宣传 土味情话话题营销 年轻化品牌互动，增强用户粘性</p><p>门店促销 线下核销奖品引流 精准获客，提高客单价</p><p>行业价值分析</p><p>对商家的价值：</p><ol><li>低成本获客：单次活动成本可控，分享裂变降低获客成本</li><li>精准引流：通过实物奖品核销，将线上流量转化为线下到店客流</li><li>数据沉淀：收集用户答题数据，完善用户画像</li><li>品牌曝光：节日热点营销，提升品牌社交传播度</li></ol><p>对用户的价值：</p><ol><li>娱乐性强：土味情话内容轻松有趣，参与门槛低</li><li>社交货币：分享内容具有话题性，适合朋友圈传播</li><li>实际收益：答题排名奖励实物，激励持续参与</li></ol><p>竞争优势：</p><ul><li>时效性强：紧扣七夕、情人节等节点，自带流量热度</li><li>操作简单：后台配置便捷，无需技术基础即可上线</li><li>灵活可控：题库、规则、奖品均可自定义，适应不同商家需求</li></ul><hr/><ol start="4"><li>问答环节（FAQ）</li></ol><p>基于产品功能与商家常见疑问，整理以下问答：</p><p>Q1：这款小程序适合什么类型的商家使用？</p><p>A：主要面向商超、餐饮门店、自媒体公众号、零售品牌等需要在七夕、情人节等节点做线上互动营销的商家。特别适合希望通过趣味活动吸粉、并将线上流量引导到线下门店核销的商家。</p><p>Q2：用户如何获得答题机会？</p><p>A：系统采用基础次数+分享奖励机制。用户每日自动获得一定答题次数，同时每日分享活动到朋友圈或好友，可额外获得答题次数，以此激励社交传播。</p><p>Q3：奖品如何发放和核销？</p><p>A：商家在后台设置实物奖品，用户答题排名达标后获得奖品资格。线下到店核销是核心设计，用户需到店出示核销码领取奖品，从而实现线下引流。系统支持完整的核销管理功能。</p><p>Q4：后台可以自定义哪些内容？</p><p>A：后台支持高度自定义，包括：题库内容（自由添加土味情话题目）、活动规则（答题次数、排名规则）、界面图片（活动海报、背景图）、活动范围（参与地区限制）等，灵活适应不同商家需求。</p><p>Q5：系统对服务器环境有什么要求？</p><p>A：系统支持PHP 7.1+环境，需基于微擎平台部署。建议服务器配置满足微擎官方要求，以确保稳定运行。</p><p>Q6：活动数据如何统计？</p><p>A：后台提供完善的用户统计功能，可实时监控参与人数、答题次数、分享次数、奖品核销情况等核心数据，支持数据报表导出，便于活动效果评估。</p><hr/>]]></description></item><item>    <title><![CDATA[如何在没有 USB 的情况下将文件从手机传输到笔记本电脑 iReaShare ]]></title>    <link>https://segmentfault.com/a/1190000047607548</link>    <guid>https://segmentfault.com/a/1190000047607548</guid>    <pubDate>2026-02-12 15:05:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在很多情况下，您需要将文件从手机传输到笔记本电脑，但您可能没有方便的 USB 数据线。那么，您知道如何在没有USB的情况下将文件从手机传输到笔记本电脑吗？在本指南中，我们将探索 6 种无线移动设备到笔记本电脑文件传输的简单方法，以便您可以方便地了解它们。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607550" alt="图片" title="图片"/></p><p>第 1 部分：如何通过云存储服务将文件从手机传输到笔记本电脑</p><p>Google Drive、Dropbox 和 OneDrive 等云存储服务提供了一种无需 USB 即可将文件从手机传输到笔记本电脑的便捷方式。此方法非常适合传输各种类型的数据，包括文档、照片和视频。</p><p>以下是使用云服务从手机到笔记本电脑的文件传输：</p><p>步骤 1：在手机上打开云存储应用（例如 Google Drive），然后使用您的帐户登录。</p><p>步骤 2：点击“ + ”将手机上的文件上传至云存储。</p><p>步骤 3：通过网站或应用程序登录笔记本电脑上相应的云存储服务。</p><p>步骤 4：找到上传的文件并将其下载到您的笔记本电脑上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607551" alt="图片" title="图片" loading="lazy"/></p><p>第 2 部分：如何通过 iReaShare Android Manager 将数据从 Android 传输到笔记本电脑（无需电缆）</p><p>iReaShare Android Manager是一款适用于Android设备的综合传输工具，支持Wi-Fi连接，因此您可以使用它将不同的文件从Android手机传输到笔记本电脑，而无需USB。通过此程序，您可以 传输照片、视频、音乐、联系人、应用程序、文档等。此外，您还可以一键将所有 Android 文件备份到计算机上。</p><p>iReaShare Android 管理器的主要特点：</p><ul><li>无需电缆即可将数据从 Android 无线传输到笔记本电脑。</li><li>Android 文件预览可在您的笔记本电脑上使用。</li><li>轻松地将 Android 文件备份到 PC 。</li><li>顺利地将文件从备份恢复到您的 Android 手机，而不会覆盖文件。</li><li>传输图片、音乐、通话记录、视频、短信等。</li><li>支持Android 6.0及以上版本，包括Android 15。</li></ul><p>下载 iReaShare Android 管理器。</p><p>下载 Win 版下载 Mac 版</p><p>以下是如何使用此软件将手机连接到不带 USB 的笔记本电脑：</p><p>步骤 1：下载软件后，请在您的笔记本电脑上安装该软件。然后运行它，点击“通过 Wi-Fi 连接”将 Android 手机和笔记本电脑连接到同一个 Wi-Fi 网络。</p><p>步骤 2：然后它会要求您在手机上安装Android版本。请打开应用程序，然后单击“扫描”图标扫描笔记本电脑上的二维码。</p><p>步骤 3：连接后，选择需要的文件类型，并选择具体数据。然后单击“导出”将数据传输到您的笔记本电脑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607552" alt="图片" title="图片" loading="lazy"/></p><p>第 3 部分：如何使用 AirDroid 在没有 USB 的情况下将文件从手机共享到笔记本电脑</p><p>AirDroid 允许您将移动设备无线连接到笔记本电脑，使文件从手机传输到笔记本电脑变得快速、轻松。只要您的网络可用，您就可以直接在设备之间发送文件。但发送大文件时速度稍慢。</p><p>操作方法如下：</p><p>步骤 1：从 Google Play Store、Apple App Store 或其网站下载应用程序并将其安装到您的手机和笔记本电脑上。</p><p>步骤 2：在手机和笔记本电脑上登录AirDroid帐户，并确保两台设备处于同一网络以启用文件传输。</p><p>步骤 3：单击“传输”选项卡，选择您的笔记本电脑设备，然后点击“ Paperclicp ”以选择要从手机传输的文件。</p><p>步骤 4：发送文件，您可以在笔记本电脑上接收文件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607553" alt="图片" title="图片" loading="lazy"/></p><p>第 4 部分：如何使用蓝牙在没有 USB 的情况下将文件从手机传输到笔记本电脑</p><p>蓝牙提供了一种简单便捷的方式，无需 USB 电缆即可将小文件从移动设备传输到笔记本电脑。它也不需要任何网络。顺便说一句，请注意，虽然您可以建立蓝牙连接，但其与 iPhone 传输文件的功能有些有限。</p><p>步骤如下：</p><p>步骤 1：在手机和笔记本电脑上启用蓝牙。</p><p>步骤 2：在手机的蓝牙菜单上，选择笔记本电脑的名称以建立连接。然后选择您要传输的文件并选择“共享”&gt;“蓝牙”，然后选择您的笔记本电脑。</p><p>步骤 3：在笔记本电脑上，选择“接收文件”以接受传入的文件传输。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607554" alt="图片" title="图片" loading="lazy"/></p><p>第 5 部分：如何使用 Phone Link 在没有 USB 的情况下将文件从手机传输到笔记本电脑</p><p>Phone Link 是 Microsoft 推出的一款应用程序，可让您无需 USB 数据线即可将 Android 或 iPhone 连接到 Windows 笔记本电脑，但它需要稳定的网络，并且仅支持 Windows 10 或更高版本。</p><p>以下是如何使用 Phone Link 将照片从手机发送到不带 USB 的笔记本电脑：</p><p>步骤 1：确保您的 Windows PC 上已安装并设置Phone Link应用程序。在您的 Android 手机上，您需要“链接到 Windows ”应用程序（某些设备上可能已预装）。</p><p>步骤 2：按照屏幕上的说明，通过扫描二维码来配对两个设备。</p><p>步骤 3：连接后，在手机上选择“照片”。然后查看并选择您最近的照片，并将它们拖放到您的计算机上。</p><p>步骤 4：您还可以转到图库应用程序，然后使用 Phone Link 应用程序将所需的照片发送到笔记本电脑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607555" alt="图片" title="图片" loading="lazy"/></p><p>第 6 部分：如何通过电子邮件将数据从手机共享到笔记本电脑</p><p>如果您只需要传输几个文件，电子邮件可能是一个简单而快速的解决方案。您可以将照片、文档和视频等数据直接发送给自己。</p><p>方法如下：</p><p>步骤 1：在手机上撰写电子邮件并附加要传输的文件。</p><p>步骤 2：将电子邮件发送到您的电子邮件地址。</p><p>步骤 3：在笔记本电脑上登录您的电子邮件帐户，打开电子邮件并下载附件。然后您的照片将存储在您的笔记本电脑上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607556" alt="图片" title="图片" loading="lazy"/></p><p>第 7 部分：有关手机到笔记本电脑文件传输的常见问题解答</p><p>Q1：无线文件传输安全吗？</p><p>安全性取决于方法。云服务通常具有很强的安全性，但请确保您使用强密码。蓝牙对于短距离传输来说相对安全。使用应用程序时，请确保您使用的是信誉良好的应用程序，例如 iReaShare Android Manager。此外，请勿连接到那些未知的网络。</p><p>Q2：无线传输的文件大小有限制吗？</p><p>这取决于。电子邮件有严格的大小限制。根据您的计划，云服务可能有限制。但iReaShare Android Manager对文件大小没有限制，因此只要您的网络强大，您就可以轻松传输大文件。</p><p>结论</p><p>不再需要寻找电缆！通过利用 iReaShare Android Manager、云存储等无线传输工具，您无需电缆即可轻松发送文件。如果您想享受无缝批量传输的便利， iReaShare Android Manager是最好的，它可以一次备份您的所有文件。<br/>​</p>]]></description></item><item>    <title><![CDATA[非凸科技持续护航第50届ICPC东亚区总决赛 非凸科技 ]]></title>    <link>https://segmentfault.com/a/1190000047607565</link>    <guid>https://segmentfault.com/a/1190000047607565</guid>    <pubDate>2026-02-12 15:04:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>1月31日-2月2日，由杭州师范大学与浙江大学联合承办的第50届ICPC国际大学生程序设计竞赛东亚区总决赛，在杭州圆满举行。306支参赛队伍同台竞技，展开了一场关于逻辑、速度与创新的巅峰对决。作为赛事的重要支持方，非凸科技再度深度参与，既是对计算机科学基础领域的长期践行，更是对行业人才根基的持续深耕与生态建设。<br/><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnU35" alt="image.png" title="image.png"/><br/>闭幕式现场，非凸科技首席运营官郑媛姿受邀出席并分享行业洞见。她表示，ICPC赛场上的极限博弈与协同攻坚，淬炼的正是应对现实世界复杂难题的核心素养。非凸科技是一个能把算法能力“价值最大化”的舞台，企业的发展阶段是同学们也应该纳入就业考量的标准之一。非凸科技期待与青年才俊携手，将赛场上的卓越智慧，转化为赋能行业发展、塑造未来格局的实干力量。</p><p>五个小时的赛程紧凑激烈、扣人心弦。选手们或并肩研讨、激荡思维，时而凝神笃行、专注编码。每一次正确提交的背后，是默契协作的积淀；每一次错误反馈之后，是冷静复盘的坚守。经过激烈角逐，清华大学“零基础新生1队”摘得冠军，上海交通大学“夏日影”队与北京大学“飞带不长队”分获亚、季军。<br/><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnU36" alt="image.png" title="image.png" loading="lazy"/><br/>东亚赛场终章落幕，全球逐梦新程将启。非凸科技祝贺所有获奖队伍，并期待青年学子们在全球总决赛中再创佳绩。未来，非凸科技将持续深耕产学研融合，以顶尖赛事为纽带，搭建青年人才成长平台，汇聚全球科创力量，共赴数智时代的全新征程。</p>]]></description></item><item>    <title><![CDATA[MIAOYUN | 每周AI新鲜事儿 260212 MIAOYUN ]]></title>    <link>https://segmentfault.com/a/1190000047607572</link>    <guid>https://segmentfault.com/a/1190000047607572</guid>    <pubDate>2026-02-12 15:03:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本周国内外科技企业密集发布AI相关新品与技术：大模型领域，Anthropic、OpenAI、智谱、科大讯飞、美团、字节、腾讯、阿里等陆续发布新模型且各具突破；AI Agent与工具领域，腾讯桌面智能体、美团LongCat深度研究智能体、Clawdbot强化版、秒云Tokens管家等相继推出；技术与市场动态上，Chrome WebMCP协议重构AI与网页交互，字节Protenix-v1超越AlphaFold 3，阿里PLaw Bench填补法律评测空白，工信部推进“1+M+N”算力节点体系建设，Anthropic发布智能体编码趋势报告预判行业变革，一起来回顾本周发生的AI新鲜事儿吧！</p><h2>AI 大模型</h2><p><strong>Anthropic发布「Claude Opus 4.6」，首次支持百万上下文</strong></p><p>2月6日，Anthropic发布旗舰模型「Claude Opus 4.6」，在多项评测中表现亮眼，首次实现Opus系列100万Token上下文窗口突破和128K Token输出上限，攻克了上下文衰减问题，还能自主判断任务难度并具备智能体团队协同能力，但极速模式定价溢价高达6倍引争议。此外产品线同步更新，推出Agent Teams及Excel、PPT插件，API价格基本保持不变，在真实工作任务和流体智力方面优势显著。</p><p><strong>OpenAI发布「GPT-5.3 Codex」，首个参与创造自己的模型</strong></p><p>2月6日，OpenAI发布「GPT-5.3 Codex」，在Terminal-Bench 2.0中以77.3%得分超越半小时前发布的「Claude Opus 4.6」（65.4%），速度较5.2-Codex快25%且更省Token。该模型是首个参与创造自己的模型，早期版本被用于debug自身训练过程、管理部署等环节，还能自主开发完整游戏、生成优化的SaaS Landing page及多种职业场景的文档与表格，支持任务过程中实时交互调整。作为OpenAI首款高能力网络安全模型，它部署了全面安全栈并推出多项配套安全举措，目前已在Codex app、CLI等多渠道上线，API访问即将开放。</p><p><strong>美团发布基于N-gram的轻量化MoE模型「LongCat-Flash-Lite」</strong></p><p>2月6日，美团发布基于N-gram的轻量化MoE模型「LongCat-Flash-Lite」，总参数685亿、推理仅激活29亿～45亿参数，团队摒弃传统专家扩展模式选择嵌入扩展，通过子表分解等设计优化N-gram嵌入层并解决哈希冲突问题，还依托参数智能分配、专用缓存与内核优化、推测解码协同三重系统优化实现推理加速，典型负载下API生成速度达500-700 Token/s，同时支持256K超长上下文；模型在智能体工具使用、编程领域表现领先，通用知识与数学推理能力均衡，中文理解具备优势。</p><p><strong>字节跳动悄悄上线AI视频生成模型「Seedance2.0」</strong></p><p>2月7日，字节跳动AI视频生成模型「Seedance2.0」开启灰度测试，支持文本、图片、视频、音频多模态素材输入，可自定分镜与运镜，精准还原参考素材的构图、镜头语言等，生成视频能平滑衔接且编辑能力增强，还可在镜头切换中保持人物场景一致，音画、口型也能实现同步匹配，被《黑神话・悟空》制作人冯骥评价为“地表最强”，也让海外网友感叹影视行业岗位或将受巨大冲击，同时该模型也暴露了精细化控制不足、中文字幕乱码、人物面部情感表达僵硬等问题。</p><p><strong>腾讯混元开源首个产业级2Bit量化的端侧模型「HY-1.8B-2Bit」</strong></p><p>2月10日，腾讯混元推出业界首个实现产业级2Bit量化的端侧模型「HY-1.8B-2Bit」，经2比特量化感知训练生成，等效参数量0.3B、内存占用仅600MB，相较原始精度模型参数量降6倍、生成速度提2-3倍，还沿用了原模型的全思考能力，可根据任务复杂度切换思维链模式，且在数学、代码等指标上与4比特PTQ模型表现相当。部署上提供多种格式权重且完成Arm平台适配，未来将通过强化学习、模型蒸馏等技术进一步缩小其与全精度模型的能力差距，相关模型与技术报告也已开源。</p><p><strong>阿里通义千问推出新一代图像生成基础模型「Qwen-Image-2.0」</strong></p><p>2月10日，阿里通义千问团队推出新一代图像生成基础模型「Qwen-Image-2.0」，融合生图与编辑支线实现生图编辑二合一，具备文字渲染“准、多、美、真、齐”（支持1K Token复杂指令，可直出PPT、海报等专业信息图）、2K分辨率细腻写实质感、强语义遵循及轻量架构四大核心特色，在AI Arena盲测中文生图位列第三、图生图编辑位列第二，不仅能精准渲染复杂文字、精细刻画人物/自然/建筑等写实场景，还支持题词、组图生成、双图合成、跨次元编辑等增强编辑功能。</p><p><strong>字节图像生成模型「Seedream 5.0 Preview」上线，支持2K直出与检索生图</strong></p><p>2月10日，字节图像生成模型「Seedream 5.0 Preview」上线剪映、Capcut、小云雀平台，支持2K直出、4K AI增强分辨率，还首次支持检索生图功能，核心升级了提示词理解、细节纹理生成、图像精准调整能力，同时在智能水平、风格化效果、联网知识整合三大方面实现增强，主打智能与实用性而非美学，能理解抽象提示词、完成知识驱动型任务，参考图生成的人物一致性较好，排版设计比4.5版更美观，一次性生成的图片风格也更多样。</p><p><strong>科大讯飞发布基于全国产算力训练的「星火X2」大模型</strong></p><p>2月11日，科大讯飞发布基于全国产算力训练的「星火X2」大模型，采用293B MoE稀疏架构，推理性能较X1.5提升50%，数学、推理等通用能力对标国际顶尖、130+多语言能力持续提升，还依托训推采样校准等四大核心技术实现性能突破，重点赋能教育、医疗、汽车、智能体等高专业度场景。此外讯飞还搭建了“1+N+X”模型矩阵，覆盖60+关键场景，向全球开发者开放能力，推动国产大模型规模化落地。</p><p><strong>蚂蚁集团正式发布百灵「Ming-flash-omni-2.0」全模态大模型</strong></p><p>2月11日，蚂蚁集团百灵「Ming-flash-omni-2.0」全模态大模型正式发布，基于Ling-2.0架构训练，全模态能力跃升至开源领先水准，在视觉百科、可控语音生成、图像创作领域表现突出且支持音画一体创作，依托全模态感知强化、泛音频统一生成框架、视觉多任务深度融合等核心技术实现性能突破，目前模型仍有部分优化空间，团队将持续迭代，其权重和推理代码也已在Hugging Face等多平台开源。</p><p><strong>智谱开源「GLM-5」适配国产芯片的Agentic Engineering标杆模型</strong></p><p>2月12日，智谱上线并开源「GLM-5」，该模型是从Vibe Coding向Agentic Engineering变革的产物，在Artificial Analysis榜单位居全球第四、开源第一，Coding与Agent能力均达开源SOTA，真实编程场景体感逼近「Claude Opus 4.5」。模型通过参数规模与预训练数据扩容、构建Slime异步强化学习框架、集成DeepSeek稀疏注意力机制完成基座升级，还完成七大国产算力平台深度适配，可应用于应用开发、Agent助手、Z Code全流程编程、办公文档直接输出等典型场景。</p><h2>AI Agent</h2><p><strong>腾讯云推出全场景职场AI智能体桌面工作台「WorkBuddy」</strong></p><p>2月6日，腾讯云推出全场景职场AI智能体桌面工作台「WorkBuddy」并开启内测，支持自然语言交互，可自主规划并执行本地电脑的多模态复杂任务，能实现文件批量处理、办公文档生成、内容创作等操作，还内置模型切换、技能包等高阶功能，区别于仅能对话的聊天机器人，它能真正落地执行并交付可验收结果，解决了非技术背景职场人使用AI工具门槛高的痛点，可在深度调研出PPT、生成宣传海报、构建本地知识库、自动化处理本地文件等多个办公场景发挥作用。</p><p><strong>开源AI助手「Clawdbot」超级强化版上线，10000+数据工具加持</strong></p><p>2月7日，开源AI助手「Clawdbot」（曾用名Moltbot，现名OpenClaw）因Teamo平台推出超级强化版而走红，该版本打通金融、社媒、商业、科研等10000+专业数据库和工具Skills，无需部署配置即可一键免费认领属于自己的ClawdBot实例，可通过飞书、企业微信等渠道7×24小时提供股票分析、大宗商品走势研判、小红书博主挖掘等专业服务，还支持安装或自定义技能，技能库随用户使用持续丰富。</p><p><strong>美团LongCat发布原生「深度研究」智能体并上线PC端</strong></p><p>2月11日，美团LongCat发布原生「深度研究」智能体并上线PC端，该智能体聚焦AI+本地生活，依托美团真实交易数据为用户定制专业可信的吃喝玩乐攻略，相关评测指标超越ChatGPT等主流AI，其能力还覆盖各类深度调研场景，背靠三大核心能力与三大核心技术实现突破，还打造了多款春节实用攻略案例，目前该功能已上线LongCat安卓APP，iOS版本也即将推出。</p><h2>AI 工具</h2><p><strong>MIAOYUN推出「秒云Tokens管家」一站式可观测大模型API智能网关</strong></p><p>2月10日，MIAOYUN基于云原生与智能运维领域技术沉淀，推出「秒云Tokens管家」一站式可观测大模型API智能网关，针对性解决企业AI工程化落地中API选型难、Token消耗贵、效果不透明的核心痛点。该产品以API聚合调用（深度整合DeepSeek、Doubao等主流大模型，支持一次接入统一鉴权，无需修改代码即可一键调用）与Token全链路可观测（多维度实时监测、可视化及全生命周期追溯）为核心能力，延伸出配额分配、消耗预警等精细化管控与优化功能，具备统一接入、指标可视化等核心功能，适用于企业AI应用开发、模型选型评估等场景。</p><h2>技术突破</h2><p><strong>Google Chrome推出「WebMCP」,AI Agent可直连网页内核</strong></p><p>2月11日，Google Chrome推出Chrome 146中的「WebMCP」（Web模型上下文协议）早期预览版，能让AI Agent通过navigator.modelContext API直连网页内核，解决其模拟人类操作网页的高成本、低稳定性等痛点。该协议由Google与微软联手开发并已开源，提供两种API接入方式且具备三大核心优势，适配电商、旅游等多场景，它重构了AI与网页的交互底层逻辑，推动互联网向人类视觉UI和Agent结构化工具界面分层发展，加速了Agentic UI时代的到来。</p><p><strong>字节跳动Seed团队开源生物分子结构预测模型「Protenix-v1」</strong></p><p>2月11日，字节跳动Seed团队发布「Protenix-v1」，是首个在训练数据、参数量等与AlphaFold 3严格对齐的条件下性能实现超越的开源生物分子结构预测模型，在相关复合物预测上表现领先且解锁了推理时扩展能力；模型采用双版本发布策略，新增多项实用功能，团队还推出PXMeter评估工具让评测更规范，同时研发了轻量化的Protenix-Mini系列适配高通量场景，该模型的发布标志着开源生物分子结构预测正式进入后AlphaFold 3时代，为AI for Science及大分子药物设计领域提供了优质开源基座。</p><h2>市场动态</h2><p><strong>阿里联合晓天衡宇评测社区发布法律垂直领域评测基准「PLaw Bench」</strong></p><p>2月6日，阿里巴巴AIData团队联合晓天衡宇评测社区发布法律垂直领域评测基准「PLaw Bench」（Practical Law Bench），基于真实案件改编，涵盖13类场景、850个问题及12500条评分细则，通过用户理解、案例分析、文书生成三大任务模块，以聚焦推理步骤的精细化评分体系还原真实法律实务流程，相关论文及项目已开源。评测显示主流大模型法律领域表现平平且存瓶颈，模型能力与参数规模正相关，国产顶尖模型适配性优于国外模型，部分开源模型有商业替代潜力，该基准为行业选模型提供参考，未来还将扩充优化。</p><p><strong>工信部印发《关于开展国家算力互联互通节点建设工作的通知》</strong></p><p>2月6日，工信部印发《关于开展国家算力互联互通节点建设工作的通知》，推进由1个已建成国家算力互联网服务节点、M个区域节点、N个行业节点组成的“1+M+N”国家算力互联互通节点体系建设，明确该体系实行“统一标识、统一标准、统一规则”运行机制，划定了区域、行业节点的建设目标与内容；同时公布节点申报细则，明确申报对象、建设及运营主体的申报条件、材料报送要求和分级评审方式，还发布配套建设方案，详细规划了六大核心系统、五类参与主体、五大工作流程以及区域和行业节点的管理职责，后续工信部将加快节点建设，推动算力产业高质量发展，助力制造强国、网络强国和数字中国建设。</p><p><strong>Anthropic发布《2026年智能体编码趋势报告》，智能体编码重构软件开发新范式</strong></p><p>2月10日，Anthropic发布一份18页的《2026年智能体编码趋势报告》，指出软件开发迎来重大范式转移，核心是人人皆可成为开发者，仅会写代码的程序员将被淘汰，软件工程师将转型为指挥AI智能体的编排者与架构师；报告提出三大类八大趋势，核心涵盖开发周期大幅压缩、单智能体进化为协作团队、智能体能独立构建完整系统、人机高效智能协作，且智能编码向新领域和非技术用户延伸，还会重塑开发经济学、推动非技术岗位普及智能编程，同时带来安全攻防的双重影响；报告还明确了掌握多智能体协调等四大优先事项，强调人机协作的核心是让人类专长聚焦关键环节，相关真实案例也印证了这些趋势的落地可行性。</p>]]></description></item><item>    <title><![CDATA[Windows 下试用 OpenCode（qbit） qbit ]]></title>    <link>https://segmentfault.com/a/1190000047607575</link>    <guid>https://segmentfault.com/a/1190000047607575</guid>    <pubDate>2026-02-12 15:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><ul><li>本文终端使用 <a href="https://segmentfault.com/a/1190000042871587" target="_blank">Windows Terminal + MSYS2</a></li></ul><h2>步骤</h2><ul><li><p>设置代理</p><pre><code>export ALL_PROXY='socks5://127.0.0.1:10808'</code></pre></li><li><p>安装 OpenCode</p><pre><code>curl -fsSL https://opencode.ai/install | bash</code></pre></li><li><p>通过查看版本信息测试安装是否成功</p><pre><code>opencode --version</code></pre></li></ul><blockquote>本文出自 <a href="https://segmentfault.com/blog/qbit" target="_blank">qbit snap</a></blockquote>]]></description></item><item>    <title><![CDATA[元数据平台选型避坑指南：从“血缘不准”到“DataOps 自动化治理”的跨越 Aloudata大应科]]></title>    <link>https://segmentfault.com/a/1190000047607584</link>    <guid>https://segmentfault.com/a/1190000047607584</guid>    <pubDate>2026-02-12 15:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=t4IUq%2Bar%2BfEqkwGxLwa36Q%3D%3D.TyoP7DEf7s8%2BLy5%2BtdobLtisjtqRwloVT1ofDxO72NQ%2FUD3Y0k97Wj9XHmZfKKvImN3xenVJxrQAoF%2FYYdwi0r5RnFB%2FJ8sumAGjf3LaGt4%3D" rel="nofollow" target="_blank">《元数据平台选型踩坑实录：评估 6 款产品后的血泪教训》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：本文基于真实选型实践，深入剖析了企业在元数据平台选型中普遍面临的三大核心痛点：数据血缘不准、数据资产盘点不动、数据变更管控失灵。文章指出，传统工具在复杂 SQL 和算子级逻辑解析上的技术局限是根源，并提出以 算子级血缘 为核心的 主动元数据平台 是实现 DataOps 自动化治理、规避选型风险的关键范式，同时提供了从验证到落地的四步法路径。</p><p>IDC 报告曾指出，超 65% 的企业因数据治理平台选型不当而陷入困境。一个普遍的现象是：选型时功能列表光鲜亮丽，上线后却发现核心的“数据血缘”地图错误百出，既看不清数据流转，也管不住变更风险，更治不动冗余资产。数据治理团队反而成了“数据警察”，总是在问题爆发后才被动响应。</p><p>核心症结在于：选型过程过度关注 UI 美观度、连接器数量等表层指标，而忽略了对 “数据血缘”这一基石能力的深度验证。Gartner 将有效的元数据管理视为数据可发现、可理解、可信任、可控制的基础。如果血缘不准，后续所有治理动作都建立在流沙之上。</p><h2>第一大坑：血缘“地图”不准，问题诊断反而南辕北辙</h2><p>传统元数据工具的血缘解析，大多停留在表级或字段级。它们在演示时用简单的 <code>SELECT a, b FROM table</code> 表现良好，但一旦面对企业真实的、复杂的 SQL 逻辑，立刻原形毕露。</p><ul><li>教训 1：迷信“字段级血缘”概念，POC 时测试用例完美通过。上线后才发现，对于嵌套子查询、通过 <code>DBLINK</code> 的跨库关联、存储过程中的动态 SQL，血缘链路大面积断裂或错配。一张本应用于定位问题的“地图”，自己却漏洞百出。</li><li>教训 2：基于错误血缘进行变更影响分析。例如，上游一张大表的一个字段类型修改，传统血缘会通知所有下游任务，引发不必要的恐慌和无效排查。而真正依赖该字段的某个关键报表，却可能因为血缘缺失而被遗漏，最终导致业务决策失误，形成资损风险。</li></ul><p>根源在于技术局限：传统解析器多基于正则匹配或简单语法分析，无法深入理解 SQL 的 算子（Operator）逻辑（如 Filter、Join、Aggregation）。对于存储过程、复杂嵌套视图等“藏污纳垢”之地，更是束手无策。</p><table><thead><tr><th>对比维度</th><th>传统血缘 (表级/字段级)</th><th>算子级血缘 (以 Aloudata BIG 为例)</th></tr></thead><tbody><tr><td>解析粒度</td><td>表/字段名映射</td><td>SQL 算子 (如 WHERE, JOIN, GROUP BY)</td></tr><tr><td>典型解析准确率</td><td>&lt; 80% (复杂场景下骤降)</td><td>&gt; 99% (基于 AST 抽象语法树深度解析)</td></tr><tr><td>复杂 SQL 支持</td><td>弱，易断链</td><td>强，支持嵌套视图、存储过程、动态 SQL</td></tr><tr><td>核心附加能力</td><td>无</td><td>行级裁剪、白盒化口径提取</td></tr><tr><td>适用场景</td><td>简单链路查看</td><td>精准影响分析、自动化盘点、根因定位</td></tr></tbody></table><h2>第二大坑：资产“盘点”不动，监管合规人效黑洞</h2><p>每逢监管报送（如 EAST、1104），数据治理团队便进入“战时状态”。一个监管指标的口径溯源，需要数据工程师人工逐层反查几十甚至上百个任务脚本，耗时数周至数月，产出的 Excel 文档还无法随代码变更而“保鲜”。</p><ul><li>教训 3：选型时被美观的“数据目录”界面吸引，以为找到了资产管理的银弹。上线后却发现，目录里的资产信息需要手动维护，很快沦为“僵尸资产”陈列馆，业务价值几乎为零。</li><li>教训 4：为了满足一次临时的合规审计，投入大量人力进行运动式盘点。项目结束后，人员撤离，文档封存，一切归零。下次审计来临，一切从头再来，无法形成可持续的治理能力。</li></ul><p>新范式解法：自动化资产盘点。以浙江农商联合银行的实践为例，通过 Aloudata BIG 的 算子级血缘 和 “一键溯源” 能力，过去需要数月人工盘点的监管指标，现在可 在 8 小时内自动生成完整的加工口径和血缘链路，人效提升 20 倍。杭州银行也通过构建全链路算子血缘图谱，实现了监管指标的自动化盘点与保鲜。</p><h2>第三大坑：变更“管控”失灵，上游一动下游全崩</h2><p>“上游一张表，下游千行泪”。缺乏精准的影响分析能力，是数据变更管控失灵的根源。传统工具无法识别数据流转中的过滤条件，导致“误伤”和“漏网”并存。</p><ul><li>教训 5：建立了严格的变更评审流程，但在评审会上，因为无法说清楚一个字段修改到底会影响哪些核心报表，各方陷入无休止的争论或妥协，评审流于形式，风险照常上线。</li><li>教训 6：在进行数仓重构或平台迁移时，因无法准确分析表间依赖和加工逻辑，只能选择风险极高的“硬切换”，或投入巨量人工进行代码比对，成本高昂且周期漫长。</li></ul><p>核心技术突破：行级裁剪 (Row-level Pruning)。这是算子级血缘带来的关键能力。它能精准解析 SQL 中的 <code>WHERE</code> 等过滤条件。例如，一张存储全国数据的上游表，只有 <code>WHERE city=‘上海’</code> 的下游任务才会因上海数据的变更而告警。招商银行 的实践表明，该技术能将变更影响分析范围降低 80% 以上，实现事前精准防控。民生银行 则基于此构建了“事前事中变更协作机制”，保障了核心链路的稳定。</p><h2>新解法：以“算子级血缘”为基石的主动元数据平台</h2><p>要跳出上述三大坑，必须从思维上完成从“被动数据字典”到“主动元数据服务”的升级。主动元数据平台 不再仅仅是记录“有什么数据”，而是通过高精度的 算子级血缘，实时分析数据链路的健康状况，并主动驱动治理动作。</p><p>其核心价值体现在三个层面：</p><ol><li>看得清：&gt;99% 的解析率将黑盒链路彻底白盒化，实现一键自动化资产盘点与口径溯源。</li><li>管得住：基于 行级裁剪 的精准影响分析，在代码提交前即阻断风险，实现事前事中防控。</li><li>治得动：自动识别链路过长、循环依赖、重复计算等模型“坏味道”，并给出重构建议，持续优化计算和存储成本。</li></ol><p>本质上，它扮演着企业 DataOps 实践的 “控制流” 或 “神经中枢” 角色，连接开发、测试、运维、资产目录各环节，实现元数据驱动的自动化协同。</p><h2>选型落地路径：从“避坑”到“填坑”的四步法</h2><p>成功的选型不仅是避免踩新坑，更是要用新工具去填历史的坑。建议遵循以下价值验证路径：</p><ol><li>步骤一（连接与解析）：不以连接数据源数量论英雄。重点验证平台对存量复杂代码（如 PL/SQL 存储过程、深度嵌套查询）的解析能力，要求提供真实环境的解析准确率报告。</li><li>步骤二（场景验证）：选取 1-2 个最痛的业务场景进行 POC。例如，监管指标溯源或核心报表变更影响评估。目标不是演示功能，而是量化对比：将传统人工方式的耗时、成本与平台自动化方式对比，计算出明确的人效提升指标（如“从 2 周缩短到 2 小时”）。</li><li>步骤三（协同集成）：评估平台与现有调度系统（如 DolphinScheduler）、数据开发平台、BI 工具（如 Tableau）的集成能力。确保元数据能通过 API 无缝流动，嵌入现有研发运维流程，而不是又一个信息孤岛。</li><li>步骤四（运营保鲜）：建立元数据驱动的研发规范。例如，将血缘分析作为代码上线前的必选门禁，确保血缘随代码变更而自动更新，形成“治理-研发”闭环，保障元数据的持续鲜活。</li></ol><h2>常见问题 (FAQ)</h2><h4>Q1: 元数据平台选型，最应该关注的核心功能是什么？</h4><p>A: 数据血缘的解析精度与深度是基石。必须超越表级和字段级，验证其对复杂 SQL 算子（如 Filter, Join）的解析能力（即算子级血缘），以及在实际业务代码（如存储过程）中的准确率（应 &gt;99%）。精度不足的血缘图是后续所有治理动作失效的根源。</p><h4>Q2: 开源元数据平台（如 DataHub, OpenMetadata）和商业产品主要差距在哪里？</h4><p>A: 主要差距在于 血缘解析的完备性、准确性和对复杂企业场景的深度支持。开源工具在基础采集和目录展示上良好，但在需要高精度血缘支撑的主动治理、自动化盘点、精准影响分析等核心价值场景上，往往需要大量二次开发和补丁，总拥有成本（TCO）可能更高。</p><h4>Q3: 如何向业务部门证明元数据平台的投资价值？</h4><p>A: 聚焦解决业务直接痛点：1) 提效：将业务“找数据、问口径”时间从数天缩短至分钟级（通过自动化资产目录和口径溯源）。2) 控险：证明平台能防止因上游数据变更导致的关键业务报表错误，避免决策失误和合规风险。用具体场景的“前后对比”数据说话。</p><h4>Q4: 都说“主动元数据”是趋势，它到底“主动”在哪里？</h4><p>A: “主动”体现在从 “被动记录”转向“主动驱动”。传统元数据是被查询的静态信息；主动元数据平台能实时分析血缘、质量等元数据，主动触发动作，例如：在代码提交时自动评估变更影响并阻断风险；在数据异常时自动定位根因；定期推荐优化冗余模型。它是实现 DataOps 自动化的核心引擎。</p><h4>Q5: 企业数据环境复杂（混合云、多引擎），元数据平台能统一管理吗？</h4><p>A: 可以，但这是选型关键挑战。应重点考察平台的 跨异构环境端到端血缘连接能力。看其对各类数据源（关系型、NoSQL、大数据组件）的连接器生态，以及能否将分散血缘拼接成完整的全域数据流转图谱。兴业银行、民生银行 的跨平台治理案例已验证了其可行性。</p><h2>核心要点总结</h2><ol><li>选型失败核心：往往源于对 “数据血缘”精度 的测试不足，而非功能列表缺失。</li><li>三大致命坑：“血缘不准”导致诊断南辕北辙；“盘点不动”使人效陷于合规黑洞；“变更失控”让管控机制形同虚设。</li><li>根本解法：采用具备 算子级血缘解析 能力的 主动元数据平台，实现 &gt;99% 的解析准确率，并解锁 行级裁剪、自动化盘点 等关键能力。</li><li>价值验证：选型应围绕 具体痛点场景 进行 POC，量化人效提升与风险降低指标，而非单纯的功能演示。</li><li>成功标志：平台能作为 DataOps 控制流 融入现有流程，驱动研发运维自动化，形成可持续的治理闭环。</li></ol><p>本文首发于 Aloudata 官方技术博客，查看更多技术细节与案例实践，请访问原文链接：<a href="https://link.segmentfault.com/?enc=XusUtkYtlB3uNgwDsFXLHQ%3D%3D.eAflRH6TAcCbBX6QLNDW0IFAGRTiHlwQFbc%2FcaSpgpyKAKDG8zNK478AwsQPObx62W1p4EQ6Dfo7VV2%2FvPMYtRJqPnJXdMNMT5qCihKLNOU%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/metadata-platform-selectio...</a></p>]]></description></item><item>    <title><![CDATA[2026年9款业务管理系统核心模块解析，全链路能力大对决 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047607591</link>    <guid>https://segmentfault.com/a/1190000047607591</guid>    <pubDate>2026-02-12 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化转型浪潮中，企业对业务管理系统的需求已从单一模块工具升级为全链路协同平台。本文围绕<strong>销售机会管理、订单管理、产品与库存管理、采购管理、生产管理</strong>五大核心业务模块，对9款主流系统进行专业横向对比，结合场景化选型建议，为企业提供精准决策依据。</p><h2>一、核心能力雷达图总览</h2><p>以下为各品牌在五大模块的能力评分（满分10分，基于功能覆盖度、自动化程度、行业适配性加权计算）：</p><table><thead><tr><th>品牌\模块</th><th>销售机会管理</th><th>订单管理</th><th>产品与库存管理</th><th>采购管理</th><th>生产管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>10</td><td>10</td><td>9</td><td>9</td><td>9</td></tr><tr><td>ActiveCampaign</td><td>7</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Flowlu</td><td>6</td><td>5</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Lusha CRM</td><td>7</td><td>3</td><td>0</td><td>8</td><td>0</td></tr><tr><td>Agile CRM</td><td>8</td><td>5</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Apptivo</td><td>7</td><td>7</td><td>6</td><td>6</td><td>2</td></tr><tr><td>探马SCRM</td><td>8</td><td>6</td><td>0</td><td>0</td><td>0</td></tr><tr><td>悟空CRM</td><td>7</td><td>6</td><td>6</td><td>5</td><td>0</td></tr><tr><td>客如云（餐饮场景）</td><td>3</td><td>8</td><td>7</td><td>5</td><td>0</td></tr></tbody></table><h2>二、分模块深度对比</h2><h3>1. 销售机会管理：从线索到转化的全周期能力</h3><p><strong>核心评估维度</strong>：多渠道获客覆盖、线索智能处理、跟单模型丰富度、客户生命周期管理、AI赋能能力</p><h4>品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>多渠道获客覆盖</th><th>线索智能处理</th><th>跟单模型丰富度</th><th>客户生命周期管理</th><th>AI应用能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全渠道（广告/私域/工商搜客等）</td><td>自动抓取+AI录音分析+工作流</td><td>3种核心模型+N种通用能力</td><td>智能客池分类+工商信息补全</td><td>自然语言AI工作流+电话录音分析</td></tr><tr><td>ActiveCampaign</td><td>邮件/营销自动化渠道</td><td>预测性评分+行为触发</td><td>基础跟进模型</td><td>状态标签化管理</td><td>智能自动化画布</td></tr><tr><td>Lusha CRM</td><td>海外B2B数据渠道</td><td>漏斗可视化+阶段跟踪</td><td>标准漏斗模型</td><td>阶段划分</td><td>无明确AI能力</td></tr><tr><td>Agile CRM</td><td>多渠道（邮件/网络行为）</td><td>AI线索评分+自动任务触发</td><td>漏斗模型+自动化</td><td>线索自动培养</td><td>AI评分+行为触发</td></tr><tr><td>探马SCRM</td><td>全渠道（电销/微销/投放）</td><td>线索自动流转+智能分配</td><td>电销/微销专属模型</td><td>客户画像构建</td><td>无深度AI分析</td></tr></tbody></table><h4>重点品牌解析：超兔一体云的全链路跟单能力</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607593" alt="" title=""/></p><pre><code>mindmap
  root((超兔销售机会管理核心能力))
    多渠道获客矩阵
      公域流量（百度/巨量引擎）
      私域运营（微信/小程序）
      线下拓客（地推/会销）
      数据获客（工商搜客）
    三大跟单模型
      小单快单（三一客模型）
      复杂商机跟单
      多方项目协同
    通用效率工具包
      360°跟单视图
      通信数据全集成
      电话录音AI分析
    客户生命周期管控
      智能动态客池分类
      工商信息自动补全
      AI自然语言工作流</code></pre><h3>2. 订单管理：从创建到交付的自动化闭环</h3><p><strong>核心评估维度</strong>：订单类型覆盖、自动化执行流程、财务管控、行业适配性</p><h4>品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>订单类型覆盖度</th><th>自动化执行（工作流/锁库/采购联动）</th><th>财务管控能力</th><th>行业适配性</th></tr></thead><tbody><tr><td>超兔一体云</td><td>20+全类型（服务/实物/特殊业务）</td><td>全流程自动化（锁库/采购计划/供应商直发）</td><td>应收联动/信用管控/账期管理</td><td>全行业适配（生产/贸易/服务）</td></tr><tr><td>Flowlu</td><td>基础订单+发票</td><td>合同跟踪+待办管理</td><td>发票关联</td><td>通用中小企业</td></tr><tr><td>Apptivo</td><td>多业务模型订单</td><td>应收提醒+流程跟踪</td><td>发票管理</td><td>中小企业供应链</td></tr><tr><td>客如云</td><td>餐饮全订单（点单/支付/外卖）</td><td>门店流程自动化（接单/配餐/出餐）</td><td>营收统计/对账</td><td>餐饮垂直行业</td></tr></tbody></table><h4>超兔订单执行自动化流程图</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607594" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
    A[多类型订单创建] --&gt; B[订单工作流触发]
    B --&gt; C{是否需锁库?}
    C --&gt;|是| D[库存锁定]
    C --&gt;|否| E[生成采购计划]
    D --&gt; E
    E --&gt; F[自动生成采购单/供应商直发]
    F --&gt; G[待办日程同步跟进]
    G --&gt; H[财务应收自动触发/信用管控]
    H --&gt; I[订单完成闭环]</code></pre><h3>3. 产品与库存管理：实物业务的核心支撑</h3><p><strong>核心评估维度</strong>：产品结构管理、库存操作能力、溯源与预警、行业适配</p><h4>品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>产品管理能力（BOM/套餐/SKU）</th><th>库存操作（盘点/调拨/拣货）</th><th>溯源与预警</th><th>行业适配</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全支持（多级分类/BOM/非标定制）</td><td>全操作+手机扫码拣货</td><td>批次/序列号溯源+上下限预警</td><td>生产/贸易/零售</td></tr><tr><td>Apptivo</td><td>基础产品+仓库管理</td><td>库存转移/收货记录</td><td>无明确溯源功能</td><td>中小企业供应链</td></tr><tr><td>悟空CRM</td><td>基础产品信息管理</td><td>盘点/调拨/出入库明细</td><td>库存自动调整</td><td>通用进销存</td></tr><tr><td>客如云</td><td>餐饮食材分类管理</td><td>库存报表+智能补货</td><td>食材效期预警</td><td>餐饮垂直行业</td></tr></tbody></table><h3>4. 采购管理：从需求到对账的上游协同</h3><p><strong>核心评估维度</strong>：供应商管理、智能采购、上游协同、数据能力</p><h4>品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>供应商管理</th><th>智能采购（计划/自动拆分）</th><th>上游协同（比价/对账）</th><th>数据能力（分析/验证）</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全生命周期管理</td><td>库存缺口自动采购+供应商自动拆分</td><td>OpenCRM询价比价+三流合一对账</td><td>成本分摊+供应商雷达图评级</td></tr><tr><td>Lusha CRM</td><td>海外B2B数据集成</td><td>无智能计划</td><td>自动化邮件跟进</td><td>数据清洗/验证/分析</td></tr><tr><td>Apptivo</td><td>基础供应商管理</td><td>多供应商比价</td><td>基础对账</td><td>基础数据统计</td></tr><tr><td>客如云</td><td>食材供应商管理</td><td>智能预估采购量</td><td>配送价格配置</td><td>采购市场数据联动</td></tr></tbody></table><h3>5. 生产管理：从计划到交付的制造闭环</h3><p><strong>核心评估维度</strong>：生产计划排程、车间执行、移动端支持、全链路联动</p><h4>品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>生产计划排程</th><th>车间执行（领料/报工/质检）</th><th>移动端支持</th><th>与其他模块联动</th></tr></thead><tbody><tr><td>超兔一体云</td><td>MES智能排程</td><td>全流程扫码操作</td><td>MES-App</td><td>CRM订单同步→生产→库存→采购闭环</td></tr><tr><td>Apptivo</td><td>需定制开发</td><td>无原生支持</td><td>无</td><td>无深度联动</td></tr><tr><td>其他品牌</td><td>无明确模块</td><td>无</td><td>无</td><td>无</td></tr></tbody></table><h4>超兔销售-生产全链路时序图</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607595" alt="" title="" loading="lazy"/></p><pre><code>sequenceDiagram
    participant CRM as 超兔CRM
    participant MES as 超兔MES
    participant Stock as 库存模块
    participant Purchase as 采购模块
    CRM-&gt;&gt;MES: 销售订单同步生成生产订单
    MES-&gt;&gt;Stock: 触发原材料领料申请
    Stock-&gt;&gt;MES: 领料出库确认
    MES-&gt;&gt;MES: 车间报工→质检流程
    MES-&gt;&gt;Stock: 合格成品入库
    Stock-&gt;&gt;CRM: 库存更新同步订单状态
    MES-&gt;&gt;Purchase: 原材料缺口触发采购计划
    Purchase-&gt;&gt;Stock: 原材料入库同步</code></pre><h2>三、场景化选型建议</h2><table><thead><tr><th>企业场景类型</th><th>推荐系统</th><th>核心理由</th></tr></thead><tbody><tr><td>全链路生产/贸易/服务企业</td><td>超兔一体云</td><td>唯一覆盖五大模块的一体化平台，实现销售-生产-库存-采购全数据闭环，支持复杂业务模型</td></tr><tr><td>海外B2B营销+采购协同</td><td>Lusha CRM+超兔</td><td>Lusha的海外B2B数据能力+超兔的库存/订单管理，满足跨境采购+业务管控需求</td></tr><tr><td>跨境电商线索精准转化</td><td>Agile CRM</td><td>AI线索评分+多渠道行为跟踪，适配跨境电商线索快速转化场景</td></tr><tr><td>餐饮门店精细化运营</td><td>客如云</td><td>餐饮专属订单/库存/采购能力，适配门店接单、食材管控全流程</td></tr><tr><td>中小企业轻量进销存</td><td>悟空CRM/Apptivo</td><td>覆盖销售-订单-库存-采购基础能力，成本较低，适配中小微企业初期数字化需求</td></tr><tr><td>营销自动化为主的企业</td><td>ActiveCampaign</td><td>邮件营销+线索评分能力突出，适合以营销驱动获客的企业</td></tr></tbody></table><h2>四、总结</h2><p>不同品牌的能力侧重差异显著：超兔一体云凭借全模块覆盖和深度协同能力，成为全链路企业的首选；垂直场景品牌如Lusha、客如云在细分领域具备独特数据或流程优势；营销类CRM则聚焦线索转化环节。企业选型需以自身业务链路完整性为核心，优先选择能实现数据闭环的系统，避免多系统集成带来的效率损耗。</p><p>在数字化转型深化阶段，业务管理系统的“一体化协同”与“场景化适配”将成为企业核心竞争力的重要载体。企业需结合短期业务痛点与长期战略布局，精准匹配系统能力，通过数据驱动的全链路管理，实现降本增效与业务增长的双重目标。</p>]]></description></item><item>    <title><![CDATA[指标中台选型核心是计算引擎，而非静态目录 Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047607337</link>    <guid>https://segmentfault.com/a/1190000047607337</guid>    <pubDate>2026-02-12 14:06:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：[《指标中台选型减负：Aloudata CAN 物化表自动管理如何降低运维成本》](<a href="https://link.segmentfault.com/?enc=uS%2Bl0JwvoAl2KcU8UGmKEA%3D%3D.qkHikKodolczzE0ZF6aaFyGVPdZ9n045nSn0xmobn2PqyGNrNSGzvKL1WtxWL3C0vkMj9REVnLfXYIC%2FOb%2F%2B9e%2BXVAkydpUW7gccNpasp3Oegg8lylmgIyCFjrk72TetVOZ2dw9AbMUmbnAuNJ9J%2Fg%3D%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/aloudata-can-materialized-...</a>转载请注明出处。</blockquote><p>摘要：本文深入探讨了在数据工程领域，指标中台选型如何有效降低运维成本。通过对比传统静态指标目录与基于 NoETL 语义编织的动态计算引擎，重点分析了物化表（预计算表）自动管理在降低专家人力、服务器资源及管理复杂度等 TCO 方面的核心价值，并提供了基于 Aloudata CAN 智能物化引擎的实践案例与决策框架。</p><p>许多企业在指标中台选型时，往往将注意力集中在建立一个静态的指标目录（Catalog）上，误以为核心是整理和展示元数据。然而，这忽略了支撑海量、灵活查询的动态计算与物化加速引擎的复杂性，后者才是运维成本的真正来源。</p><p>“数据中台平台选型不能一味追求‘大而全’，而要结合企业实际需求、数据现状、IT 基础、业务目标等多维度进行综合考量。” —— 帆软《企业数字化有哪些中台？2026 数据中台平台选择建议》</p><p>传统指标平台本质是静态元数据目录，其分析路径完全受限于底层预建的物理宽表（ADS 层）。任何新的维度组合需求，都可能触发一轮新的 ETL 开发、测试、上线流程，导致开发与运维的“烟囱式”膨胀。而 Aloudata CAN 的本质是一个动态计算引擎，它通过语义编织技术，直接在 DWD 明细层上构建虚拟业务事实网络，将“造表”的物理负担转化为“声明关联”的逻辑配置。</p><table><thead><tr><th>维度</th><th>传统指标平台（静态目录型）</th><th>Aloudata CAN（动态计算引擎）</th></tr></thead><tbody><tr><td>本质</td><td>静态元数据目录（Catalog）</td><td>动态计算引擎</td></tr><tr><td>依赖</td><td>依赖底层人工宽表承载数据</td><td>直接基于 DWD 明细层定义，无需预建宽表</td></tr><tr><td>灵活性</td><td>分析路径受限于预建宽表</td><td>指标+维度灵活组装，任意维度下钻</td></tr><tr><td>运维重心</td><td>宽表 ETL 开发、调度、血缘维护</td><td>语义模型定义与声明式物化策略配置</td></tr></tbody></table><h2>物化表管理三大挑战与自动化解决方案</h2><h3>1. 语义解析：从“静态宽表”到“动态虚拟网络”</h3><p>自研或使用传统方案时，分析灵活性是第一个瓶颈。业务需求千变万化，分析师希望从任意维度（如地区、产品、渠道）组合分析指标。传统模式下，这要求数据工程师预先为每一种可能的分析路径创建物理宽表，导致：</p><ol><li>开发排期长：从需求提出到宽表上线，通常需要数周。</li><li>存储成本高：大量宽表重复存储相同粒度的数据，造成资源浪费。</li><li>口径不一致风险：不同宽表可能由不同工程师开发，计算逻辑难以保证 100% 一致。</li></ol><p>Aloudata CAN 的语义引擎 (Semantic Engine) 通过在 DWD 层声明业务实体间的逻辑关联（Join），构建了一个“虚拟业务事实网络”。分析师在界面上拖拽的“订单金额 by 城市”，系统会自动将逻辑关联翻译为优化的 SQL，直接查询明细数据。这从根本上消除了为特定报表重复建宽表的开发与存储成本。</p><h3>2. 智能物化：从“人工运维”到“自动化代持”</h3><p>虽然直接查询明细在逻辑上最灵活，但面对亿级数据，性能无法保证。因此，物化视图（预计算表）是保证查询性能的关键，但其全生命周期管理是运维的“重灾区”。</p><ol><li>创建与合并：需要人工判断哪些查询组合需要加速，编写复杂的物化 SQL，并处理物化表之间的依赖与去重。</li><li>更新与刷新：需配置调度任务，处理增量、全量更新逻辑，并保证与上游数据同步。</li><li>血缘与变更：当上游指标口径变更时，需人工梳理所有受影响的下游物化表，并手动编排回刷任务，过程极易出错。</li></ol><p>这正是外部情报中强调的“高维护成本”痛点的核心。Aloudata CAN 的智能物化引擎通过“声明式策略”实现了自动化代持：</p><ul><li>按需物化加速：用户只需在界面声明需要加速的指标和维度组合（如“近 30 天销售额 by 省份-城市”），系统会自动推荐并创建最优的物化策略，支持明细、汇总、结果三级加速。</li><li>自动化 ETL 代持：系统自动生成并维护物化表的创建、合并、刷新任务。查询时，智能路由与改写机制会透明地命中最优物化结果，实现亿级数据秒级响应（P90 &lt; 1s）。</li><li>变更自维护：当上游指标口径变更时，系统会自动解析血缘，识别所有受影响的物化表，并提示用户进行数据回刷操作，确保下游数据一致性。</li></ul><p>权威背书：客户验证数据  <br/>某头部券商在引入 Aloudata CAN 后，在支撑数百个复杂业务指标的场景下，实现了基础设施成本节约 50%，其背后正是智能物化与自动化运维能力带来的开发与运维工作量锐减。</p><h3>3. 生态适配：从“数据孤岛”到“统一服务出口”</h3><p>自研指标服务往往与某个特定的 BI 工具或前端应用强绑定，形成新的数据孤岛。当企业同时使用 FineBI、Quick BI、Tableau 以及自建数据应用时，维护多套指标接口和口径的成本高昂。</p><p>Aloudata CAN 作为中立的 Headless 指标平台，提供了标准的 REST API 和 JDBC 接口。这意味着：</p><ul><li>一处定义，处处使用：在 Aloudata CAN 中定义的指标，可以同时向 FineBI、Quick BI、AI 大模型及各类自建应用提供口径 100% 一致的服务。</li><li>简化集成：消除了在不同 BI 工具中重复定义、维护指标的成本。</li><li>面向未来：为 AI 智能问数等新兴场景提供了统一的、高质量的数据底座。</li></ul><h2>TCO 账本：算清物化表管理的“隐形高利贷”</h2><p>传统模式下，物化表管理的成本远不止看得见的服务器资源。一份完整的 TCO 账本应包括以下“隐形高利贷”：</p><table><thead><tr><th>成本项</th><th>传统模式（手工 ETL + 物化表管理）</th><th>Aloudata CAN（智能物化引擎）</th></tr></thead><tbody><tr><td>专家人力成本</td><td>高昂且持续：需资深数据工程师进行开发、排期、运维、排障。</td><td>大幅降低：数据工程师聚焦于语义模型设计，物化任务由系统自动化代持。</td></tr><tr><td>服务器资源成本</td><td>高昂：大量重复宽表导致存储与计算资源浪费。</td><td>显著节约：通过智能物化与去重，减少 ADS 层开发，可释放 1/3+ 服务器资源。</td></tr><tr><td>错误决策成本</td><td>潜在风险高：因口径不一致或数据更新延迟导致的业务决策错误。</td><td>趋近于零：统一指标出口，确保口径 100% 一致；自动化更新保障数据时效。</td></tr><tr><td>机会成本</td><td>高：因需求响应慢（数周）而错失市场机会或优化窗口。</td><td>大幅降低：配置化定义，分钟级交付新分析维度，实现业务敏捷。</td></tr><tr><td>管理复杂度</td><td>极高：需管理数百张物化表的血缘、调度和变更。</td><td>显著简化：系统提供清晰的资产使用统计与血缘视图，辅助治理。</td></tr><tr><td>总计 TCO</td><td>高昂且不可控，随业务复杂度线性增长。</td><td>可控、可量化节约，案例证实可实现 50% 的降本。</td></tr></tbody></table><p>权威背书：行业定位认可  <br/>作为 Gartner 中国数据编织代表厂商，Aloudata CAN 的核心理念正是通过语义编织与自动化，解决数据管理中的效率与成本难题。</p><h2>决策矩阵：何时该自研，何时该引入成熟方案？</h2><p>企业应根据自身技术储备、业务复杂度与成本敏感性做出理性决策。对于绝大多数追求敏捷、效率和成本可控的企业，引入成熟的 Aloudata CAN 是更优选择。</p><table><thead><tr><th>评估维度</th><th>适合自研 / 传统方案</th><th>适合引入 Aloudata CAN</th></tr></thead><tbody><tr><td>核心团队技术实力</td><td>拥有顶尖的数据库内核与查询优化团队，能将物化视图管理作为核心技术产品打磨。</td><td>希望团队聚焦于业务模型与数据分析，而非底层计算引擎的研发与运维。</td></tr><tr><td>业务需求变化频率</td><td>业务极其稳定，分析模式固化，物化表需求长期不变。</td><td>业务快速迭代，需要灵活的多维分析，物化策略需频繁调整。</td></tr><tr><td>对运维成本的敏感度</td><td>不计成本，追求对技术栈的绝对控制。</td><td>高度重视 TCO，要求明确的 ROI，希望降低对稀缺数据开发专家的依赖。</td></tr><tr><td>现有数据架构</td><td>简单，可接受推倒重来，进行彻底的架构改造。</td><td>复杂，需与现有数据湖仓生态无缝对接，采用渐进式“存量挂载、增量原生”策略。</td></tr><tr><td>长期战略</td><td>计划将指标平台能力作为技术产品对外输出。</td><td>将数据能力作为对内业务赋能的核心支撑，追求快速见效和持续降本。</td></tr></tbody></table><h2>常见问题(FAQ)</h2><h5>Q1: Aloudata CAN 的物化表自动管理，具体能减少多少运维人力投入？</h5><p>根据已公开的标杆案例实践，例如某头部券商，通过引入 Aloudata CAN 的智能物化与自动化运维能力，在支撑数百个复杂业务指标的场景下，实现了基础设施成本节约 50%，这背后对应的是数据开发与运维工作量的显著减少。具体比例因企业原有流程成熟度而异，但核心是将数据工程师从重复的 ETL 脚本开发与运维中解放出来。</p><h5>Q2: 如果业务逻辑变更，Aloudata CAN 的物化表如何自动更新？需要人工介入吗？</h5><p>这是智能物化引擎的核心能力。当上游指标的口径或计算逻辑发生变更时，系统会自动解析血缘依赖，识别出所有受影响的物化表，并提示用户进行数据回刷操作。用户确认后，系统会自动生成并调度回刷任务。整个过程无需人工编写或修改 SQL 脚本，实现了“定义即生产，变更自维护”。</p><h5>Q3: 我们公司已经有很多 BI 工具和报表，引入 Aloudata CAN 会不会增加新的集成和维护成本？</h5><p>不会增加额外负担，反而会简化集成。Aloudata CAN 作为统一的指标计算与服务层，通过标准 API/JDBC 对接各类 BI 工具（如 FineBI、Quick BI）和现有报表系统，确保各消费端指标口径 100% 一致。这消除了过去在不同 BI 工具中重复定义、维护指标的成本，实现了“一处维护，多处生效”。</p><h2>核心要点总结</h2><ol><li>本质是引擎，不是目录：指标中台选型的核心是选择一个能处理动态、灵活查询的计算引擎，而非静态的指标目录。误判将导致后续高昂的运维成本。</li><li>智能物化是降本核心：物化表（预计算）的手工运维是成本“重灾区”。Aloudata CAN 的智能物化引擎通过声明式策略实现自动化创建、更新与血缘维护，是降低 TCO 的关键。</li><li>TCO 包含隐性成本：真正的成本不止服务器，更包括专家人力、错误决策和错失机会的成本。自动化方案将这些隐性成本转化为可量化的节约。</li><li>Headless 架构简化生态：中立的、提供标准接口的指标平台，能统一服务多个 BI 工具和应用，消除数据孤岛，降低长期集成维护复杂度。</li><li>理性决策基于矩阵：企业应通过技术实力、业务变化频率、成本敏感度等维度评估，对于绝大多数场景，引入成熟的 Aloudata CAN 是比自研更高效、更经济的选择。</li></ol><p>本文首发于 Aloudata 官方技术博客，查看更多技术细节与高清图表，请访问原文链接：<a href="https://link.segmentfault.com/?enc=xn4bnycNpeo%2BVDnh%2BDG%2BUA%3D%3D.7j%2Fk%2FVLFg1HaPYNtHhJGbosTxpR12DqIE86J9%2BLeYg2FCSXe6Xx5icSTjiKoGlXqeT%2FguzvTt4o0INB7kTyfQ59MbB%2F2uwIc%2Fr3yUPnUpFT%2BcDmjkTeQQsXh%2BADWbqzPp6w3ZGwzk0cdOhJcj1Ktew%3D%3D" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/aloudata-can-materialized-...</a></p>]]></description></item><item>    <title><![CDATA[续力共建智能、可控的数据体系，枫清科技中标华润医药数据指标管理项目 Fabarta ]]></title>    <link>https://segmentfault.com/a/1190000047607361</link>    <guid>https://segmentfault.com/a/1190000047607361</guid>    <pubDate>2026-02-12 14:05:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607363" alt="图片8 17.00.21.png" title="图片8 17.00.21.png"/><br/>近日，枫清科技成功斩获华润医药集团“数据治理系统增补指标管理模块项目”。这是双方继数据治理系统建设、数据应用智能化项目后的再度深度合作，标志着两家企业在医药数智化领域的协同迈入新阶段，将为华润医药数智化升级筑牢数据根基。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607364" alt="图片9 17.00.21.png" title="图片9 17.00.21.png" loading="lazy"/><br/>此次合作的核心解决方案，聚焦医药行业数据治理关键需求，以“高效、智能、可控”为核心亮点。模块通过灵活的指标定义与复用机制简化开发流程，依托全生命周期管理体系实现指标资产集中管控，凭借双重核验与预警机制保障数据可靠性，更以低门槛的查询方式让数据价值快速落地，全方位满足华润医药标准化、智能化的数据管理需求。</p><p>此次中标是市场对枫清科技技术实力与行业经验的再度认可。作为医药产业数智化领域的深耕者，枫清科技围绕AI技术赋能、生态协同共建、场景精准深耕三大主线，已在医药全产业链形成成熟解决方案，成功服务多家头部医药集团的智能化落地项目。</p><p>未来，枫清科技将持续锚定“AI赋能医药产业全链条”核心目标，深化技术与行业场景的融合创新，与华润医药及更多医药链主企业、医药制造企业携手，加速推动医药行业数智化转型，共建高效智能的产业新生态。</p>]]></description></item><item>    <title><![CDATA[成都服务器托管：这3点比价格重要100倍！多数企业都踩过坑 极云Cloud ]]></title>    <link>https://segmentfault.com/a/1190000047607406</link>    <guid>https://segmentfault.com/a/1190000047607406</guid>    <pubDate>2026-02-12 14:05:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>机柜规格与扩展性要 “留余地”，别让设备 “住不下”。很多企业初期只考虑当前设备，却忽略后续发展需求：有的买了 2U 服务器，结果选了 1U 机柜；有的业务扩张后想加设备，却发现机柜没预留散热空间；有的高功率 AI 服务器，因没确认机柜功率上限，导致供电不足频繁宕机。正确的做法是：先明确服务器所需 U 数（1U 适合小型服务器，2U 适合常规业务服务器，4U 适合存储密集型设备），选择整柜或单机托管方案时，务必预留 30% 扩展空间 —— 既保证现有设备散热（避免高温导致死机），也能容纳后续新增设备；若有高功率设备（如超过 500W 的 AI 训练服务器），需提前与服务商确认机柜功率（通常优质机柜支持 5-10kW）及超电上限，避免供电瓶颈。极云天下会根据企业当前设备参数和 3 年业务规划，定制机柜方案，去年成都某 AI 企业托管时，极云天下提前测算其设备功率，推荐了 10kW 机柜，后续企业新增 3 台服务器时，无需更换机柜，直接节省了 2 万元迁移成本。</p>]]></description></item><item>    <title><![CDATA[社区推荐重排技术：双阶段框架的实践与演进｜得物技术 得物技术 ]]></title>    <link>https://segmentfault.com/a/1190000047607409</link>    <guid>https://segmentfault.com/a/1190000047607409</guid>    <pubDate>2026-02-12 14:04:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、背景</h2><h3>推荐系统典型pipeline</h3><p><img width="723" height="148" referrerpolicy="no-referrer" src="/img/bVdnUWI" alt="" title=""/></p><p>在推荐系统多阶段Pipeline（召回→粗排→精排→重排）中，重排作为最终决策环节，承担着将精排输出的有限候选集（通常为Top 100–500个Item）转化为最优序列的关键职责。 数学定义为在给定候选集 \(C = \lbrace x_1,x_2,……,x_n \rbrace\)与目标列表长度\(L\) ，重排的目标是寻找一个排列 \(\pi^* \in P(C,L) \)，使得全局收益函数最大化。</p><p>在推荐系统、搜索排序等AI领域，Pointwise 建模是精排阶段的核心方法，即对每个 Item 独立打分后排序，pointwise 建模范式面临挑战：</p><ul><li>多样性约束：精排按 item 独立打分排序 → 高分 item 往往语义/类目高度同质（如5个相似短视频连续曝光）。</li><li>位置偏差：用户注意力随位置显著衰减，且不同item对位置敏感度不同。</li><li>上下文建模：用户决策是序列行为，而非独立事件。</li></ul><h2>二、重排架构演进：生成式模型实践</h2><p>我们的重排系统采用G-E两阶段协同框架：</p><ul><li>生成阶段（Generation）：高效生成若干高质量候选排列。</li><li>评估阶段（Evaluation）：对候选排列进行精细化打分，选出全局最优结果。</li></ul><p>不考虑算力和耗时的情况下，通过穷举所有排列\( P(C,L) \)。</p><p><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdnUWO" alt="" title="" loading="lazy"/></p><p>生成阶段主要依赖启发式规则、随机扰动 + beamSearch算法生成候选list，双阶段范式存在显著的痛点：</p><ul><li>质量-延迟-多样性的“不可能三角”：在实践中，增加生成候选list数一般可以提升最终list的质量，但边际收益递减；优化过程中，我们通过增加多目标、多样性等策略都取得了消费指标的提升，但在候选list达到百量级时，单纯增加候选集对指标的提升，同时还有：</li></ul><ol><li>增加beam width，系统耗时增加，DCG@K提升逐渐减少。</li><li>增加通道数，通道间重叠度逐渐增加，去重list增加逐渐减少。</li></ol><ul><li>阶段间目标不一致：</li></ul><ol><li>分布偏移：启发式生成Beam Search输出的Top排列中，20%被评估模型否定，生成阶段搜索效率浪费。</li><li>梯度断层：Beam Search含argmax操作，双阶段无法端到端优化；生成模型无法感知评估反馈，优化方向偏离全局最优。</li></ol><h3>生成模型优化</h3><p>生成分为启发式方法和生成式模型方法, 一般认为生成式模型方法要好于启发式方法。生成式模型逐渐成为重排主流范式，主要分为两类：自回归生成模型、非自回归生成模型。</p><ul><li>自回归生成：按位置顺序逐个生成物品，第 t 位的预测依赖前 t-1 位已生成结果。</li></ul><ol><li><p>优点：</p><p>a. 序列依赖建模强，天然捕获物品间的顺序依赖。</p><p>b. 训练简单稳定，每步使用真实前序作为输入，收敛快。</p><p>c. 生成质量高，逐步细化决策，适合长序列精细优化。</p></li><li><p>缺点：</p><p>a. 推理延迟高，生成 L 个物品需 L 次前向传播，线上服务难以满足毫秒级要求。</p><p>b. 局部最优风险，早期错误决策无法回溯修正，影响整体序列质量。</p></li></ol><ul><li>非自回归生成：一次性预测整个推荐序列的所有位置，各位置预测相互独立。</li></ul><ol><li><p>优点：</p><p>a. 推理速度极快：生成整个序列仅需1次前向传播。</p></li><li><p>缺点：</p><p>b.条件独立性假设过强：各位置并行预测，难以显式建模物品间复杂依赖关系。</p></li></ol><h4>非自回归模型</h4><p>为了对齐双阶段一致性，同时考虑线上性能，我们推进了非自回归模型的上线。模型结构如下图：</p><p><img width="716" height="498" referrerpolicy="no-referrer" src="/img/bVdnUWW" alt="" title="" loading="lazy"/></p><p>模型包括Candidates Encoder和Position Encoder，Candidates Encoder是标准的Transformer结构, 用于获取item间的交互信息；Position Encoder额外增加了Cross Attention，期望Position序列同时关注Candidate序列。</p><ul><li>模型特征：用户信息、item特征、位置信息、上游精排打分特征。</li><li><p>模型输出：一次性输出 n×L 的位置-物品得分矩阵（n 为候选 item 数，L 为目标列表长度），支持高效并行推理</p><p>$$
\hat{p}_{ij} = \frac{\exp(\mathbf{x}_i^\top \mathbf{t}_j)}{\sum_{i=1}^n \exp(\mathbf{x}_i^\top \mathbf{t}_j)}
$$</p></li><li>位置感知建模：引入可学习位置嵌入，显式建模“同一 item 在不同位置表现不同”的现象（如首屏效应、位置衰减）。</li><li>训练目标：模型使用logloss，让正反馈label序列的生成概率最大, 同时负反馈label序列的生成概率最小：</li></ul><p>$$
\mathcal{L}_{\log} = -\sum_{i} \big[ p_{ij}y_i \log(\hat{p_{ij}}) + p_{ij}(1-y_i) \log(1-\hat{p_{ij}}) \big]
$$</p><p>其中，\( p_{ij} \)表示位置i上是否展示物品j，\(y_{i} \)表示位置i上的label。</p><p><strong>线上实验及收益：</strong></p><ul><li>一期新增了非自回归生成通道，pvctr +0.6%，时长+0.55%。</li><li>二期在所有通道排序因子中bagging非自回归模型，pvctr +1.0%，时长+1.13%。</li></ul><h4>自回归模型</h4><p>由于条件独立性假设, 非自回归模型对上下文信息建模是不够的，近期我们重点推进了自回归模型的开发。</p><p>模型通过Transformer架构建模list整体收益，我们使用单向transformer模拟用户浏览行为的因果性，同时解决自回归生成的暴露偏差问题，保持训练和推理的一致性。结构如下：</p><p><img width="723" height="703" referrerpolicy="no-referrer" src="/img/bVdnUWY" alt="" title="" loading="lazy"/></p><ul><li>模型特征：用户信息、item特征、位置信息、上游精排打分特征。</li><li>训练目标：模型使用有序回归loss，在评估多个回合中不同长度的子列表时，能够很好地体现出序列中的增量价值。是用于判断长度为j的子列表是否已经达到i次点击或转化的损失函数。</li></ul><p>$$
L_{i,j}(\theta_j) = -\sum_{k=1}^{N} \left([y_k &lt; i]\log(1-p_{i,j}(x_k)) + [y_k \geq i]\log(p_{i,j}(x_k))\right)
$$</p><p><strong>线上模型推理效率优化及实验效果：</strong></p><p>自回归生成模型推理延迟高，生成 L 个物品需 L 次前向传播，线上服务难以满足毫秒级要求。因此，我们在传统自回归生成模型的基础上增加MTP（multi token prediction）结构，突破生成式重排模型推理瓶颈。其核心思想是将传统自回归的单步预测扩展为单步多token联合预测，显著减少生成迭代次数。</p><p>自回归生成模型在社区推荐已完成了推全，实验中我们新增了自回归生成模型通道，但不是完全体，仅部分位置生成调用了模型：</p><ul><li>一期调用两次模型，每次预测4个位置，pvctr +0.69%，有效vv +0.58%。</li><li>二期调用两次模型，每次预测5个位置，pvctr +0.54%，有效vv +0.40%。</li></ul><h2>三、推理性能优化：端到端生成的效率保障</h2><h3>工程架构</h3><p>为解决CPU推理模型延迟高、制约业务效果的问题，我们对DScatter模型服务进行升级，引入高性能GPU推理能力，具体方案如下：</p><p><img width="723" height="313" referrerpolicy="no-referrer" src="/img/bVdnUXa" alt="" title="" loading="lazy"/></p><ul><li><strong>GPU推理框架集成与升级：</strong></li></ul><ol><li>框架升级：将现有依赖的推理框架升级为支持GPU的高性能服务框架。</li><li>硬件资源引入：引入 NVIDIA L20 等专业推理显卡，为当前的listwise评估模型及自回归生成模型提供专用算力，实现模型推理的硬件加速。</li></ol><ul><li><strong>DScatter模型服务独立部署与容量提升：</strong></li></ul><ol><li>为解决模型部署效率低与资源竞争问题，将DScatter的模型打分逻辑从现有重排服务中完全解耦，构建并部署独立的 DScatter-Model-Server 集群，从根本上消除与重排服务在CPU、内存等关键资源上的竞争。</li></ol><h3>模型优化</h3><ul><li><strong>模型格式转换与加速：</strong></li></ul><p>导出为 ONNX 格式，使用 TensorRT 进行量化、层融合、动态张量显存等技术加速推理。</p><ul><li><strong>Item Embedding缓存：</strong></li></ul><p>预计算item静态网络，线上直接查询节省计算量。</p><ul><li><strong>自回归生成模型核心优化，KV Cache 复用：</strong></li></ul><p>缓存已生成token的KV和attention值，仅计算增量token相关值，避免重复计算。</p><ul><li><strong>其他LLM推理加速技术应用落地，例如GQA</strong></li></ul><h2>四、未来规划：迈向端到端序列生成的下一代重排架构</h2><p>当前“生成-评估”双阶段范式虽在工程落地性上取得平衡，但其本质仍是局部优化：生成阶段依赖启发式规则或浅层模型生成候选，评估阶段虽能识别优质序列，却无法反向指导生成过程，导致系统能力存在理论上限。为突破这一瓶颈，我们规划构建端到端序列生成（End-to-End Sequence Generation） 架构，将重排从“候选筛选”升级为“序列创造”，直接以全局业务目标（如用户停留时长、互动深度、内容生态健康度）为优化目标。</p><p><strong>核心架构设计：</strong></p><ul><li>统一生成器：以 Transformer 为基础架构，搭建自回归序列建模能力，采用分层混合生成策略：</li></ul><ol><li>粗粒度并行生成：首层预测序列骨架（如类目分布、内容密度）等。</li><li>细粒度自回归精调：在骨架约束下，自回归生成具体 item，确保局部最优。</li></ol><ul><li>序列级Reward Modeling：</li></ul><ol><li>构建多目标 reward 函数：xtr、多样性。</li><li>Engagement：基于用户滑动轨迹建模序列累积收益（如滑动深度加权CTR）。</li><li>Diversity：跨类目/创作者/内容形式的分布熵。</li><li>Fairness：冷启内容、长尾创作者曝光保障。</li></ol><h3>训练范式升级：强化学习与对比学习融合</h3><p>推进自回归生成模型的架构升级与训练体系重构，引入强化学习微调（PPO/DPO）与对比学习机制，提升序列整体效率。</p><ul><li><strong>搭建近线系统，生成高质量list候选，提升系统能力上限：</strong></li></ul><p>1.基于 DCG 的列表质量打分：</p><p>a. 对每个曝光列表L，计算其 DCG@K作为质量分数：</p><p>$$
\text{DCG}(L) = \sum_{j=1}^{K} \frac{\text{gain}(item_j)}{\log_2(j + 1)}
$$</p><p>其中 gain(item)可定义为：</p><p>若点击：+1.0</p><p>若互动（点赞/收藏）：+1.5</p><p>若观看 &gt;5s：+0.8</p><p>否则：0</p><p>2.构造偏好对：</p><p>a.对同一用户在同一上下文下的两个列表\(L_w \)（win）和\(L_l\)（lose）。</p><p>b.若 \( DCG(L_w) &gt; DCG(L_l) + \delta \)（δ 为 margin，如 0.1），则构成一个有效偏好对。</p><ul><li><strong>引入强化学习微调（PPO/DPO）与对比学习机制，提升序列整体效率：</strong></li></ul><p>1.模型结构：</p><p>a.使用当前自回归生成模型作为策略模型。</p><p>b.固定预训练模型作为参考策略 （即 DPO 中的“旧策略”）。</p><p>2.DPO损失：</p><p>$$
\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[

\log \sigma \left(

\beta \left(

\log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)}

- \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}

\right)

\right)

\right]
$$</p><ul><li><strong>技术价值：</strong></li></ul><ol><li>突破“质量-延迟-多样性”不可能三角：通过序列级优化，在同等延迟下实现质量与多样性双提升。</li><li>为AIGC与推荐融合铺路：端到端生成器可无缝接入AIGC内容，实现“内容生成-序列编排”一体化。</li></ol><p><strong>参考文献：</strong></p><ol><li>Gloeckle F, Idrissi B Y, Rozière B, et al. Better &amp; faster large language models via multi-token prediction[J]. arXiv preprint arXiv:2404.19737, 2024.</li><li>Ren Y, Yang Q, Wu Y, et al. Non-autoregressive generative models for reranking recommendation[C]//Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024: 5625-5634.</li><li>Meng Y, Guo C, Cao Y, et al. A generative re-ranking model for list-level multi-objective optimization at taobao[C]//Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2025: 4213-4218.</li><li>Zhao X, Xia L, Zhang L, et al. Deep reinforcement learning for page-wise recommendations[C]//Proceedings of the 12th ACM conference on recommender systems. 2018: 95-103.</li><li>Feng Y, Hu B, Gong Y, et al. GRN: Generative Rerank Network for Context-wise Recommendation[J]. arXiv preprint arXiv:2104.00860, 2021.</li><li>Pang L, Xu J, Ai Q, et al. Setrank: Learning a permutation-invariant ranking model for information retrieval[C]//Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2020: 499-508.</li></ol><h2>往期回顾</h2><p>1.Flink ClickHouse Sink：生产级高可用写入方案｜得物技术</p><p>2.服务拆分之旅：测试过程全揭秘｜得物技术</p><p>3.大模型网关：大模型时代的智能交通枢纽｜得物技术</p><p>4.从“人治”到“机治”：得物离线数仓发布流水线质量门禁实践</p><p>5.AI编程实践：从Claude Code实践到团队协作的优化思考｜得物技术</p><h2>文 /张卓</h2><p>关注得物技术，每周一、三更新技术干货</p><p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p><p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></description></item><item>    <title><![CDATA[小家电MES系统与ERP如何高效协同？ 万界星空科技 ]]></title>    <link>https://segmentfault.com/a/1190000047607413</link>    <guid>https://segmentfault.com/a/1190000047607413</guid>    <pubDate>2026-02-12 14:03:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>小家电制造企业普遍面临 “多品种、小批量、快交付、成本敏感” 的运营特点，若 MES（制造执行系统）与 ERP（企业资源计划）协同不畅，极易导致 计划与执行脱节、库存虚高、订单延期、质量追溯断链 等问题。</p><p>万界星空小家电MES “轻量集成、双向驱动、数据闭环” 的高效协同模式，确保 ERP管“该做什么”，MES管“做得怎么样”，真正实现 从订单到交付的端到端透明化。</p><p><strong>一、协同核心原则：分工明确，数据实时</strong><br/>系统   职责边界   协同关键点<br/>ERP   战略层：接单、主数据、BOM、MRP运算、财务核算   下发准确的 销售订单 + 生产工单 + 物料需求<br/>MES   执行层：排产、报工、质检、设备、追溯   实时反馈 实际产量、工时、废品、在制品状态</p><p>✅ 理想状态：  <br/>ERP 不干预车间细节，MES 不越权做资源决策 —— 各司其职，高频同步。</p><p><strong>二、六大高效协同场景（小家电行业适配）</strong></p><p>✅ 1. 订单与工单无缝下发</p><ul><li>ERP动作：创建销售订单 → 运行MRP → 生成生产工单（含产品、数量、交期）</li><li><p>MES动作：</p><ul><li>自动接收工单，校验BOM与工艺路线；</li><li>若BOM变更（如新款咖啡机用新水泵），自动提示版本冲突；</li><li>支持紧急插单，动态重排产线。  <br/>💡 价值：避免“ERP有单、车间无单”或“做错版本”。</li></ul></li></ul><p>✅ 2. 物料需求精准联动</p><ul><li>ERP动作：根据BOM计算原料/包材需求，生成领料单</li><li><p>MES动作：</p><ul><li>工位扫码领料，实时扣减ERP库存（非月底汇总）；</li><li>发现替代料（如A型号电机缺货，启用B型号），自动触发ERP审批流程；</li><li>余料/废料自动回传，更新ERP可用库存。  <br/>💡 价值：库存准确率从70%提升至98%+，减少呆滞料。</li></ul></li></ul><p>✅ 3. 生产进度实时反冲</p><ul><li>传统痛点：月底手工报产量，ERP成本核算严重滞后</li><li><p>万界星空方案：</p><ul><li>MES每完成1台饮水机，自动向ERP反冲工时与物料消耗；</li><li><p>ERP实时更新：</p><ul><li>在制品（WIP）状态</li><li>订单完工率</li><li>单台成本（含人工、能耗）  <br/>💡 价值：财务可按日出成本报表，老板随时看真实利润。</li></ul></li></ul></li></ul><p>✅ 4. 质量数据驱动采购与设计</p><ul><li>MES动作：记录每台产品的质检结果（如“10%咖啡机漏水”）</li><li><p>协同机制：</p><ul><li>自动关联缺陷到 具体供应商批次（如某批硅胶圈不良）；</li><li>向ERP推送 《供应商质量预警》，触发SCAR（供应商纠正措施）；</li><li>高频问题自动汇总至PLM，推动设计改进（如优化密封结构）。  <br/>💡 价值：质量问题从“救火”转向“预防”，降低退货率。</li></ul></li></ul><p>✅ 5. 设备与产能数据支撑MRP</p><ul><li>MES动作：采集OEE、故障停机、换型时间</li><li><p>ERP动作：</p><ul><li>将实际产能数据纳入 MRP运算参数；</li><li>避免“按理论产能排单，实际永远做不完”；</li><li>动态调整交期承诺（ATP）。  <br/>💡 价值：订单准时交付率提升25%+。</li></ul></li></ul><p>✅ 6. 成品入库与发货协同</p><ul><li>MES动作：质检合格 → 自动触发入库请求</li><li><p>ERP动作：</p><ul><li>创建正式入库单，更新库存；</li><li>根据销售订单自动匹配批次（FIFO/FEFO）；</li><li>对接物流系统，打印快递单。  <br/>💡 价值：从“做完到发货”缩短至2小时内，支持当日达。</li></ul></li></ul><p><strong>三、技术实现：轻量、稳定、低成本</strong></p><p>万界星空采用 “API + 中间库 + 事件驱动” 混合集成模式，适配小家电企业IT现状：<br/>集成方式   适用场景   优势<br/>标准API对接   用友U8、金蝶K3、等主流ERP   实时性强，开发量小</p><p>中间数据库   老旧ERP无接口   通过视图/触发器同步，零改造</p><p>文件交换（CSV/XML）   临时过渡方案   快速上线，成本低</p><p>AI智能映射   BOM/物料编码不一致   自动匹配字段，减少人工对账</p><p>🔒 数据安全：</p><ul><li>字段级权限控制（如采购价仅财务可见）</li><li>所有同步操作留痕，支持审计追溯</li></ul><p><strong>四、典型协同效果（客户实测）</strong><br/>指标   协同前   协同后<br/>订单交付准时率   68%   92%</p><p>库存周转率   4.2次/年   6.8次/年</p><p>月结关账时间   5–7天   1–2天</p><p>客户质量投诉   1.5%   0.3%</p><p>计划达成率   75%   95%</p><p><strong>五、给小家电企业的实施建议</strong></p><ol><li>先理流程，再上系统：梳理BOM、工艺、质检标准，避免“垃圾进，垃圾出”；</li><li>选择轻量级MES：如万界星空SaaS版，避免重型系统拖累ROI；</li><li>分阶段集成：优先打通“订单→生产→入库”，再扩展质量、设备模块；</li><li>业务主导，IT支持：车间班组长必须参与配置，确保系统“能用、愿用”。</li></ol><p>总结：  <br/>ERP与MES不是“两个系统”，而是同一套运营逻辑的上下半场。  <br/>小家电企业无需追求“大而全”，只需抓住 “订单-物料-进度-质量”四大协同主线，即可用最低成本实现最大效益。</p>]]></description></item><item>    <title><![CDATA[为什么国内大厂纷纷"弃坑"MySQL，转投PostgreSQL阵营？ Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047607156</link>    <guid>https://segmentfault.com/a/1190000047607156</guid>    <pubDate>2026-02-12 14:02:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="259" height="212" referrerpolicy="no-referrer" src="/img/bVdnUXf" alt="" title=""/></p><blockquote>作为一名经历过LAMP时代的老架构师，我想和大家聊聊这几年国内技术圈的一个有趣现象：曾经一统江湖的MySQL，正在悄悄被PostgreSQL"偷家"。</blockquote><h2>一、现象：大厂们的集体"叛逃"</h2><p>先来看一组数据。根据DB-Engines最新排名，PostgreSQL稳居全球第四，在开源关系型数据库中连续多年霸榜第一。更重要的是，<strong>它已经成为国产数据库创新的首选技术底座</strong>。</p><p>国内头部科技企业的选择很有代表性：</p><ul><li><strong>腾讯云 TDSQL PG版</strong>（开源代号TBase）：引入GTM全局事务管理器，实现跨分片事务</li><li><strong>阿里云 PolarDB for PostgreSQL</strong>：重构存储层，实现"一写多读共享存储"</li><li><strong>华为云 GaussDB(for openGauss)</strong>：加入列存储引擎、AI优化器，支持HTAP</li><li><strong>杭州易景数通 openHalo</strong>：基于PG生态的分布式数据库</li></ul><p><img width="400" height="232" referrerpolicy="no-referrer" src="/img/bVdnUXg" alt="" title="" loading="lazy"/></p><p>那么问题来了：<strong>为什么这些大厂不选择同样流行的MySQL，而是纷纷押注PostgreSQL？</strong></p><h2>二、技术层面的"降维打击"</h2><h3>1. 查询优化器：PG是专业的，MySQL是业余的</h3><p>当业务跑到一定规模，老板开始要"日活留存"、"漏斗转化"这些复杂报表时，MySQL的弱点就暴露了：<strong>复杂的JOIN查询慢成狗，子查询优化器偶尔还会"抽风"</strong>。</p><p>而PostgreSQL的查询优化器被公认为是<strong>开源界最强</strong>的。它支持极其复杂的JOIN算法（Hash Join、Merge Join），拥有强大的窗口函数（Window Functions）。在TPC-H标准测试中，当查询包含5个以上表连接时，PostgreSQL的执行计划生成时间较MySQL短41%。</p><p>特别是在<strong>HTAP（混合事务/分析处理）</strong>场景下，PG的表现简直是降维打击：</p><ul><li><strong>MySQL</strong>：适合高并发的简单读写（用户登录、下单）</li><li><strong>PG</strong>：适合一边高并发写入，一边跑复杂统计报表，且不能把库跑挂</li></ul><h3>2. MVCC实现：架构设计的本质差异</h3><p>两者的多版本并发控制（MVCC）机制完全不同：</p><table><thead><tr><th align="left">维度</th><th align="left">PostgreSQL</th><th align="left">MySQL (InnoDB)</th></tr></thead><tbody><tr><td align="left"><strong>版本管理</strong></td><td align="left">每行存储多个版本，旧版本保留在堆中</td><td align="left">只保留当前版本，旧版本在undo log中</td></tr><tr><td align="left"><strong>读写隔离</strong></td><td align="left">读写完全隔离，支持可串行化快照隔离</td><td align="left">长事务可能导致回滚段膨胀</td></tr><tr><td align="left"><strong>并发表现</strong></td><td align="left">即使写数据，别人也可读取"之前的状态"</td><td align="left">未提交事务可能产生脏读</td></tr></tbody></table><p>PostgreSQL支持完整的可序列化隔离级别，通过SSI技术避免幻读问题。测试显示在200并发事务场景下，PostgreSQL的冲突重试率较MySQL低37%。</p><h3>3. 数据类型的"降维打击"</h3><p>PostgreSQL支持JSONB、数组、自定义类型、地理空间数据（PostGIS）等复杂数据类型。在10万级数据量测试中，PostgreSQL的JSONB查询速度较MySQL快2.3倍。</p><pre><code class="sql">-- PostgreSQL的JSONB路径查询，支持索引优化
CREATE INDEX idx_json ON api_data USING gin(data jsonb_path_ops);
SELECT * FROM api_data WHERE data @? '$.user.name ? (@ == "John")';</code></pre><p>相比之下，MySQL 5.7+的JSON类型功能较弱，仅支持基础路径查询。</p><h2>三、企业级特性的"代差"</h2><h3>1. 开源协议的"致命差异"</h3><p>这是很多企业容易忽视，但极其重要的一点：</p><table><thead><tr><th align="left">维度</th><th align="left">MySQL</th><th align="left">PostgreSQL</th></tr></thead><tbody><tr><td align="left"><strong>许可证</strong></td><td align="left">GPL + 商业许可（Oracle控制）</td><td align="left">BSD-like，完全自由</td></tr><tr><td align="left"><strong>企业版vs社区版</strong></td><td align="left">企业版包含高级功能（审计、加密）</td><td align="left">社区版即完整版，无功能阉割</td></tr><tr><td align="left"><strong>源码透明度</strong></td><td align="left">Oracle掌控核心开发，社区贡献受限</td><td align="left">全球开发者共同维护，开放透明</td></tr><tr><td align="left"><strong>长期稳定性</strong></td><td align="left">Oracle可能调整路线</td><td align="left">由基金会主导，不受单一公司控制</td></tr></tbody></table><p><strong>PostgreSQL是真正的开源——用户驱动、技术优先、长期稳定</strong>。在当前复杂的国际技术环境下，BSD许可证允许自由使用、修改和分发，无需担心商业授权风险。</p><h3>2. 扩展生态：PG的"插件宇宙"</h3><p>PostgreSQL拥有超过300个官方扩展，涵盖：</p><ul><li>全文检索（pg_trgm）</li><li>时序数据处理（TimescaleDB）</li><li>地理信息系统（PostGIS）</li><li>图数据库（Apache AGE）</li></ul><p>MySQL的扩展生态主要依赖存储过程和UDF，功能实现复杂度较高。</p><h3>3. 字符集与国际化支持</h3><p>以字符编码与排序规则为例，PostgreSQL在ICU支持下提供了<strong>42种字符集编码与815种排序规则</strong>，覆盖了几乎一切排序方法。而MySQL基本上只有五种字符集和几十个排序规则。</p><p>这对企业级应用的多语言支持至关重要。</p><h2>四、信创背景下的"政治正确"</h2><p>近年来，随着<strong>信创推进与数据库自主可控需求提升</strong>，PostgreSQL的优势更加凸显：</p><ol><li><strong>技术底座价值</strong>：华为openGauss、腾讯TDSQL、阿里PolarDB for PG、海量数据Vastbase、金仓数据库KingbaseES等主流国产数据库，均以PostgreSQL为基础进行深度定制</li><li><strong>BSD许可证优势</strong>：允许自由修改和分发，适合国产数据库厂商进行商业化改造</li><li><strong>Oracle-free战略</strong>：摆脱对Oracle（MySQL母公司）的依赖，符合自主可控要求</li></ol><p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnUXj" alt="" title="" loading="lazy"/></p><h2>五、理性选择：不做二极管</h2><p>说了这么多PG的好，难道MySQL就要被扔进垃圾堆了吗？</p><p><strong>当然不是！</strong> 作为一名理性的技术人，我们不做"二选一"的无脑站队，只选最对的场景：</p><h3>选MySQL，如果：</h3><ul><li>业务是纯互联网高并发（社交动态、简单电商订单）</li><li>团队90%的人只懂MySQL，且不愿承担学习成本</li><li>严重依赖阿里云/腾讯云的深度定制版MySQL（如PolarDB）</li></ul><h3>选PostgreSQL，如果：</h3><ul><li>数据结构复杂，包含大量JSON、数组或地理位置信息</li><li>需要做复杂的实时数据分析，但不想引入ClickHouse这种重型组件</li><li>对数据一致性要求极高（金融、科研、企业级ERP）</li><li><strong>希望数据库能陪你走得更远，而不是业务稍微一复杂就得重构</strong></li></ul><h2>六、总结：趋势不可逆</h2><p>从DB-Engines趋势来看，PostgreSQL的发展势头非常迅猛，目前已经隐隐有追上MySQL的趋势，而MySQL的受欢迎度一直呈现下降趋势。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdnUXl" alt="" title="" loading="lazy"/></p><p><strong>我的建议</strong>：</p><ul><li><strong>初创期</strong>：可优先MySQL快速落地，团队熟悉度高，生态完善</li><li><strong>成长期</strong>：当数据复杂度指数级增长时，PostgreSQL的架构优势将更显著</li><li><strong>信创/金融/政企</strong>：直接上PostgreSQL或其国产衍生版本，避免后期迁移成本</li></ul><p>在这个数据驱动的时代，选择PostgreSQL不仅仅是选择了一个数据库，更是选择了一个<strong>更开放、更标准、更具扩展性</strong>的技术生态。这或许就是为什么，当国内大厂们真正开始"玩真的"时，都不约而同地选择了那头蓝色的大象。</p><hr/><p><em>本文部分技术对比数据参考了腾讯云、阿里云、华为云官方技术文档及DB-Engines行业报告。</em><br/>作者:<a href="https://link.segmentfault.com/?enc=Laxj2gFyGAyRWT9pFw2HrA%3D%3D.8FGRwv3tmirmuSlOYQPpARb0vFocqGL6F6Mt6r6ICmc%3D" rel="nofollow" target="_blank">Smoothcloud润云</a></p>]]></description></item>  </channel></rss>