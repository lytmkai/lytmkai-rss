<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[Dify+DeepSeek实战教程：从零]]></title>    <link>https://segmentfault.com/a/1190000047438102</link>    <guid>https://segmentfault.com/a/1190000047438102</guid>    <pubDate>2025-11-29 16:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在AI技术席卷全球的2025年，企业开发者正面临一个关键命题：如何用最低成本实现AI能力的高效落地？本文将以Dify（开源AI应用开发平台）与DeepSeek-R1（国产高性能大模型）为核心工具，通过知识库构建、智能客服、代码生成、合同审核四大实战场景，手把手教你搭建企业级AI应用流水线。</p><p>一、技术选型：为什么选择Dify+DeepSeek？</p><ol><li>黄金组合的核心优势<br/>Dify：开源低代码平台，支持可视化编排工作流、管理多模型、集成知识库，覆盖AI应用全生命周期。<br/>DeepSeek-R1：国产开源大模型，支持128K上下文窗口，在数学推理、代码生成、长文本理解等场景表现卓越，训练成本仅为行业标杆的1/30。<br/>协同效应：Dify的拖拽式工作流引擎与DeepSeek的强推理能力结合，可快速实现复杂业务逻辑的AI化。</li><li>典型应用场景<br/>知识管理：构建企业私有知识库，实现文档智能检索与问答。<br/>智能客服：自动处理用户咨询，支持多轮对话与上下文理解。<br/>代码生成：根据自然语言需求生成可运行的代码片段。<br/>合同审核：自动识别合同风险条款，生成审计报告。</li></ol><p>二、环境部署：30分钟搭建开发基础</p><ol><li>硬件与软件要求<br/>硬件：NVIDIA RTX 3090+/A100显卡，64GB+内存，1TB+存储。<br/>软件：Docker 20.10+、Python 3.10+、CUDA 12.0。</li><li><p>部署步骤<br/>步骤1：安装Dify<br/>bash</p><h2>克隆Dify仓库</h2><p>git clone <a href="https://link.segmentfault.com/?enc=qRFoU9IBeBYNUn1q9BElbw%3D%3D.EnDYJxM5Q8nljkDAskuENUgrr%2BlXAi7nZly4B2UF2ZgQYOK3ZJqk8kb7H0Lvmwi2" rel="nofollow" target="_blank">https://github.com/langgenius/dify.git</a><br/>cd dify/docker</p></li></ol><h2>复制环境配置文件</h2><p>cp .env.example .env</p><h2>启动Docker容器（V2版本）</h2><p>docker compose up -d<br/>访问 <a href="https://link.segmentfault.com/?enc=yYcw5oHXpe6qrhrSP1Ud%2FA%3D%3D.%2BdNBi5lQ5iD%2FlRFuhFkEPDndTCMM9a8QoJWvx8CnUts%3D" rel="nofollow" target="_blank">http://localhost:8000</a>，完成注册后登录控制台。</p><p>步骤2：部署DeepSeek-R1<br/>通过Ollama本地化部署（避免依赖云端API）：</p><p>bash</p><h2>安装Ollama（Linux示例）</h2><p>curl -fsSL <a href="https://link.segmentfault.com/?enc=rTYEBMWEZajJjnbc212dcA%3D%3D.1Z1ZEtGKX1RjKbCLBVnMK9ZwLC1Q0eeuDTiCaKK1fcM%3D" rel="nofollow" target="_blank">https://ollama.ai/install.sh</a> | bash</p><h2>下载DeepSeek-R1模型（7B基础版）</h2><p>ollama run deepseek-r1:7b</p><p>步骤3：Dify集成DeepSeek<br/>在Dify控制台进入「模型管理」，添加自定义LLM API。<br/>填写模型地址（如 <a href="https://link.segmentfault.com/?enc=uqW8BRJq4jNBtD9VxmngIg%3D%3D.ZGrLfAEz5FMo%2FoOaOpAOvlHzUJYUjG0ct6pDoZbu%2BzfP666TOgsgJrQKf%2BA4u%2BUV" rel="nofollow" target="_blank">http://host.docker.internal:11434/api/generate</a>）与请求体模板：<br/>json<br/>{<br/>  "model": "deepseek-r1:7b-chat",<br/>  "prompt": "{{sys.prompt}}",<br/>  "temperature": 0.6,<br/>  "max_tokens": 2048<br/>}<br/>点击「测试连接」，输入技术文本（如“解释Transformer架构的注意力机制”），验证模型输出。</p><p>三、实战案例：四大场景深度解析<br/>案例1：企业知识库构建<br/>目标：将产品手册、培训文档等结构化/非结构化数据转化为可检索的AI知识库。<br/>步骤1：上传文档<br/>在Dify控制台创建知识库，选择「导入本地文件」，上传PDF/DOCX/TXT等格式文档。<br/>配置分段策略：<br/>分段模式：父子分段（父区块保留段落，子区块按句子拆分）。<br/>相似度阈值：0.7（提高检索精度）。<br/>Embedding模型：选择 bge-m3（支持中英文混合向量化）。<br/>步骤2：配置检索逻辑<br/>在应用上下文中添加知识库，设置提示词：<br/>请根据以下文档内容回答用户问题，若信息不足可回复“未知”：<br/>{{#context#}}<br/>启用Rerank模型（如 BAAI/rerank-base），对检索结果二次排序。<br/>效果验证<br/>测试问题：“如何重置产品A的管理员密码？”<br/>预期输出：直接返回文档中相关步骤，而非整篇手册。</p><p>案例2：智能客服机器人<br/>目标：自动处理用户咨询，支持多轮对话与工单流转。<br/>步骤1：设计工作流<br/>创建新应用，选择「Agent类型」。<br/>添加以下节点：<br/>LLM节点：调用DeepSeek-R1，设置提示词：<br/>你是一名客服助手，请用友好语气回答用户关于{{product}}的问题：<br/>{{user_input}}<br/>Webhook节点：将用户咨询记录同步至CRM系统。<br/>条件判断节点：若问题未解决，触发工单创建流程。</p><p>步骤2：测试与优化<br/>测试用例：用户询问“订单发货延迟怎么办？”<br/>优化方向：<br/>增加意图识别模块，自动分类问题类型。<br/>对高频问题配置快捷回复模板。</p><p>案例3：代码自动生成<br/>目标：根据自然语言需求生成可运行的代码片段。<br/>步骤1：配置工作流<br/>创建新应用，选择「工作流类型」。<br/>添加以下节点：<br/>输入节点：接收用户需求（如“生成一个Python爬虫，抓取知乎热榜”）。<br/>LLM节点：调用DeepSeek-Coder（代码生成专用模型），设置提示词：<br/>生成一个{{language}}脚本，要求：</p><ol><li>功能：{{requirement}}</li><li>包含异常处理</li><li>输出格式：{{output_format}}<br/>代码检查节点：通过SonarQube扫描代码质量。</li></ol><p>步骤2：生成结果示例<br/>输入：“用Java实现一个快速排序算法。”<br/>输出：<br/>java<br/>public class QuickSort {</p><pre><code>public static void quickSort(int[] arr, int low, int high) {
    if (low &lt; high) {
        int pi = partition(arr, low, high);
        quickSort(arr, low, pi - 1);
        quickSort(arr, pi + 1, high);
    }
}
// 其余代码...</code></pre><p>}</p><p>案例4：合同风险审核<br/>目标：自动识别合同中的风险条款，生成审计报告。<br/>步骤1：数据预处理<br/>通过RPA机器人从ERP系统同步合同文档。<br/>使用OCR模型（如PaddleOCR）提取扫描件文本。</p><p>步骤2：构建审核工作流<br/>LLM节点：调用DeepSeek-Fin（金融领域微调版），设置提示词：<br/>分析以下合同条款，识别风险点并分类（如“付款延迟”“违约责任模糊”）：<br/>{{contract_text}}<br/>数据分析节点：将风险条款与历史数据对比，计算风险评分。<br/>报告生成节点：输出包含风险热力图的三维审计报告。<br/>效果数据<br/>某银行应用案例：<br/>审核耗时从4小时/份缩短至20分钟。<br/>风险发现率从71%提升至94%。</p><p>四、性能优化：从基准到极致</p><ol><li>响应延迟优化<br/>优化策略    原始延迟    优化后延迟    成本变化<br/>模型量化（FP16→INT8）    2.3s    0.9s    -30%<br/>请求批处理（batch_size=32）    2.3s    0.6s    +15%<br/>Dify缓存机制    2.3s    0.4s    -40%</li><li>并发能力扩展<br/>MCP自动扩缩容：根据负载动态调整GPU实例数量（如从1个T4扩展至5个）。<br/>K8s水平扩缩容：应对突发流量（如促销活动期间）。</li></ol><p>五、企业级落地指南</p><ol><li>安全加固方案<br/>传输加密：HTTPS+JWT令牌验证。<br/>审计日志：记录所有API调用与模型决策路径。<br/>权限控制：RBAC模型分级授权（如仅审计员可访问合同审核模块）。</li><li>灾备与扩展<br/>跨区域部署：在AWS北京区与宁夏区同时部署服务，通过DNS负载均衡实现故障转移。<br/>多模型AB测试：同时运行DeepSeek-R1与Claude 3.5，根据业务场景动态切换。</li></ol><p>六、未来展望：AI应用的新范式<br/>随着多模态大模型与具身智能技术的发展，Dify+DeepSeek的组合将延伸至物理世界交互领域：<br/>智能工厂：通过Dify编排工作流，统一调度RPA机器人、AGV物流车与质检机械臂。<br/>智慧医疗：结合医学影像大模型，实现“感知-决策-执行”一体化的手术辅助系统。<br/>立即行动：访问 Dify GitHub仓库 获取完整代码，或通过 DeepSeek官方文档 下载模型。技术变革的浪潮已至，这一次，别再做旁观者！</p>]]></description></item><item>    <title><![CDATA[嵌软与RTOS精要 星星上的柳树 ]]></title>    <link>https://segmentfault.com/a/1190000047438108</link>    <guid>https://segmentfault.com/a/1190000047438108</guid>    <pubDate>2025-11-29 16:01:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>嵌入式软件与实时操作系统（RTOS）是现代IC系统不可或缺的核心部分。无论是驱动层、API设计，还是实时调度，它们都直接影响IC的性能、可靠性和使用体验。如果你渴望系统提升技术深度，不妨在 EDA Academy 探索丰富的网课资源，一起驱动你的专业成长。</p><p>1、嵌入式软件开发：模块+仿真高效并行<br/><img width="723" height="388" referrerpolicy="no-referrer" src="/img/bVdncYT" alt="" title=""/><br/>嵌入式软件是硬件运行的“大脑”，从简单控制到复杂处理，都离不开它。<br/>模块化编程：将系统拆分成可复用模块，便于维护与扩展。<br/>仿真与原型验证：借助 FPGA 等平台进行软件原型开发，既能模拟真实情境，又能快速调试和迭代。某工程团队通过 FPGA 原型验证嵌入式软件，提效 30%、提升可靠性。<br/>学习提示：免费教学资源指出，嵌入式代码需兼顾硬件理解、资源管理与实时响应能力，对应工具与设计策略非常关键。</p><p>2、驱动与 API：软硬桥梁，性能关键点<br/>驱动和 API 是软件与硬件交互的核心纽带。<br/>HAL（硬件抽象层）：通过统一接口屏蔽底层差异，让驱动更加通用可靠。<br/>API 设计规范：文档完善、易用安全、错误处理健全的 API 可降低调试成本、提升系统稳定性。<br/>案例实战：某定制IC 项目成功通过 HAL 设计与优质 API 架构，驱动性能提升 25%，调试效率显著提高。<br/>驱动开发技巧：如使用中断或 DMA 替代轮询，可显著提升效率和能效。</p><p>3、RTOS：让任务更“准时”<br/>RTOS 是管理多任务与资源调度的利器，适用于汽车、工业控制等实时性要求高的领域。<br/>调度与优先级：FreeRTOS、VxWorks 等系统采用精细的任务调度机制，确保关键任务得以及时执行。<br/>实时调试工具：可追踪任务执行、定位性能瓶颈，提升系统调试效率。<br/>真实案例：某汽车电子公司采用 VxWorks RTOS，以高级调度 + 实时调试确保传感器处理与控制信号精准同步，显著提升系统响应与安全性能。</p><p>RTOS 典范：FreeRTOS 开源、轻量，支持多种嵌入式平台；TI-RTOS 提供从内核、驱动到底层服务的完整生态；RT-Thread 在国内也有活跃开源社区。</p><p>4、系统整合：驱动模块、API、RTOS，有机协同<br/><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdncYV" alt="" title="" loading="lazy"/><br/>在 IC 系统中，嵌入式软件、驱动层与 RTOS 需要无缝协作：<br/>模块化软件简化迭代；<br/>HAL 和 API 提供统一接口；<br/>RTOS 管理任务调度、实时性和调试；<br/>FPGA 原型加速开发测试；<br/>这种整合方式带来高效开发流程和高性能系统输出。</p><p>5、EDA Academy：你的技术成长与职业平台<br/>最新全面课程：涵盖嵌入式软件开发、驱动/API设计、RTOS 实战等全方位内容。<br/>支持双向发展：你可注册为学员精进技能，也可注册为导师，分享经验、深化影响。<br/>免费订阅更新：仅凭邮箱即可免费订阅 newsletter，获取前沿课程与技术洞察。<br/>销售联盟机会：推荐课程给他人即可获得 20%–50% 的佣金，实现学习与收益同步增长。</p><p>嵌入式软件与 RTOS 是打造IC系统实用性与可靠性的核心能力。通过模块化开发、接口抽象、实时调度与仿真验证，工程师能够构建更高效、更可控的系统。若你期待深入这一技术领域，并收获职业成长与收入机会，欢迎访问 EDA Academy（www.eda-academy.com），开启你的专业精攻之旅。<br/><img width="723" height="1098" referrerpolicy="no-referrer" src="/img/bVdm7s4" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[51CTO-宽哥【云原生开发】Go和Gi]]></title>    <link>https://segmentfault.com/a/1190000047438112</link>    <guid>https://segmentfault.com/a/1190000047438112</guid>    <pubDate>2025-11-29 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在云原生技术浪潮席卷全球的当下，企业级应用开发对开发者的技术栈提出了更高要求。👇🏻ke🍊：xingkeit点top/9707/Go语言凭借其卓越的并发性能与轻量化特性，结合Gin框架的高效路由机制，已成为构建微服务架构的核心工具链。51CTO平台推出的《Go+Gin零基础到云原生脚手架实战》课程，通过系统化的知识体系与实战案例，帮助开发者规避常见陷阱，快速掌握云原生开发的核心能力。</p><p>一、云原生开发的技术选型逻辑</p><ol><li>Go语言：云原生的天然适配者<br/>Go语言诞生于谷歌实验室，其设计初衷便是解决高并发场景下的性能瓶颈。通过goroutine（轻量级线程）与channel（通信管道）的组合，开发者可轻松实现数十万级并发连接，这一特性使其成为Kubernetes、Docker等云原生基础设施的底层开发语言。课程中深入解析了Go的并发模型优势：相比传统线程，goroutine的内存占用仅为KB级别，且通过调度器自动平衡多核CPU负载，避免了线程切换的开销。</li><li>Gin框架：高性能Web开发的加速器<br/>作为Go生态中最受欢迎的Web框架，Gin以“零内存分配”的路由算法实现毫秒级响应。课程通过对比测试数据展示其性能优势：在处理10万级QPS（每秒查询率）时，Gin的内存占用比传统框架降低60%，响应延迟稳定在2ms以内。其核心设计亮点包括：</li></ol><p>分层路由机制：基于基数树（Radix Tree）的路由匹配，支持动态参数与通配符路由。<br/>中间件生态：内置JWT认证、限流熔断、日志追踪等20+开箱即用的中间件。<br/>开发效率提升：通过ShouldBindJSON等函数实现请求参数的自动校验与绑定，减少样板代码。<br/>二、云原生脚手架的核心设计原则</p><ol><li>分层架构：解耦与可维护性的基石<br/>课程提出的“六边形架构”将项目划分为六层：</li></ol><p>cmd层：存放程序入口文件，每个子目录对应独立可执行程序（如Web服务、定时任务）。<br/>internal层：私有应用代码，包含应用初始化、配置管理、领域模型等模块。<br/>pkg层：公共可复用代码，如工具函数、第三方库封装。<br/>infra层：基础设施实现，涵盖数据库连接、缓存管理、消息队列等。<br/>transport层：协议适配层，支持HTTP、gRPC等多协议接入。<br/>docs层：项目文档，包括API规范、设计文档与部署指南。<br/>这种分层设计使代码职责清晰，例如某电商项目通过将订单服务与支付服务拆分至不同internal子模块，实现独立开发与部署，团队并行开发效率提升40%。</p><ol start="2"><li>配置管理：多环境隔离与动态更新<br/>课程强调配置管理的“三要素”：</li></ol><p>环境隔离：通过dev.yaml、prod.yaml等文件区分开发、测试、生产环境配置。<br/>敏感信息保护：使用Vault或Kubernetes Secrets管理数据库密码、API密钥等敏感数据。<br/>动态加载：结合viper库实现配置文件的热更新，无需重启服务即可应用变更。<br/>以某金融项目为例，通过动态配置管理，其风控规则的更新周期从4小时缩短至秒级，显著提升业务响应速度。</p><p>三、实战案例：从零构建企业级脚手架</p><ol><li>用户认证与安全防护<br/>课程通过JWT（JSON Web Token）实现无状态认证，结合以下机制提升安全性：</li></ol><p>Token生命周期管理：设置15分钟短期访问令牌与24小时刷新令牌，平衡安全性与用户体验。<br/>CSRF防护：在Gin中间件中校验请求头中的X-CSRF-Token，防止跨站请求伪造攻击。<br/>速率限制：使用github.com/ulule/limiter库实现IP级别的请求限流，抵御DDoS攻击。<br/>某社交平台应用上述方案后，恶意刷量行为减少90%，认证接口的响应时间稳定在3ms以内。</p><ol start="2"><li>日志与监控体系<br/>课程构建的日志系统包含三大模块：</li></ol><p>结构化日志：使用zap库输出JSON格式日志，包含请求ID、用户ID、耗时等关键字段。<br/>日志聚合：通过Filebeat将日志传输至ELK（Elasticsearch+Logstash+Kibana）栈，实现全文检索与可视化分析。<br/>链路追踪：集成OpenTelemetry，在Gin中间件中注入TraceID，关联微服务间的调用链路。<br/>某物流项目通过该体系，将系统故障定位时间从2小时缩短至5分钟，运维效率提升80%。</p><ol start="3"><li>自动化部署与弹性伸缩<br/>课程提供的CI/CD流水线包含以下环节：</li></ol><p>代码提交触发：GitHub Actions监听代码仓库变更，自动运行单元测试与代码扫描。<br/>镜像构建：使用多阶段Dockerfile生成最小化镜像（&lt;15MB），减少安全漏洞与资源消耗。<br/>Kubernetes部署：通过Helm Chart管理Pod副本数、资源配额与健康检查配置。<br/>弹性伸缩：基于Prometheus监控的CPU使用率，自动调整Pod数量应对流量高峰。<br/>某在线教育平台应用该流水线后，部署频率从每周1次提升至每日多次，系统可用性达到99.95%。</p><p>四、课程价值：从技能到职业的全面升级</p><ol><li>技术栈覆盖全周期<br/>课程采用“基础+进阶+实战”的三阶段设计：</li></ol><p>基础阶段：Go语言核心特性（并发编程、错误处理、标准库）、Gin框架基础（路由、中间件、参数绑定）。<br/>进阶阶段：云原生技术栈（Docker、Kubernetes、Service Mesh）、微服务架构设计（服务发现、熔断降级、API网关）。<br/>实战阶段：企业级脚手架开发、电商平台案例拆解、故障演练与性能调优。</p><ol start="2"><li>职业竞争力提升<br/>据学员反馈，完成课程后：</li></ol><p>薪资涨幅：平均跳槽薪资提升35%，部分学员进入阿里P7、腾讯T9等高级职级。<br/>项目经验：输出可复用的脚手架代码与架构文档，直接应用于企业招聘中的“项目经验”环节。<br/>内推资源：课程与500+企业建立合作，优秀学员可获得独家内推机会。<br/>五、结语：云原生时代的开发者进化路径<br/>在云原生技术日益成熟的2025年，开发者需具备“全栈化、智能化、工程化”的综合能力。51CTO宽哥的Go+Gin课程通过“理论+实践+工具链”的完整闭环，不仅教授技术原理，更注重培养开发者在真实场景中解决问题的能力。无论是零基础转行者，还是希望突破职业瓶颈的资深工程师，这门课程都将成为你迈向云原生架构师的关键跳板。</p>]]></description></item><item>    <title><![CDATA[ITSS体系建设的组织策略：从项目导向到]]></title>    <link>https://segmentfault.com/a/1190000047438056</link>    <guid>https://segmentfault.com/a/1190000047438056</guid>    <pubDate>2025-11-29 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>根据工信部历年信息服务业发展报告的数据，中国超过80%的企业在启动ITSS体系建设时，仍以“项目制”方式推进。这意味着他们把ITSS视为一个阶段性任务，而非组织能力建设的长期工程。结果很明显——项目结束那天，体系也随之停止。流程文件更新断档、岗位职责回归旧习、指标考核形同虚设。三个月后，所有成果几乎回到原点。<br/> 而另一类企业，在同样起点下，却能在两三年内形成稳定的流程管理体系，实现运维绩效的持续提升。两种结果之间的分水岭，正是从项目导向到体系化治理的转变。</p><p><img width="455" height="298" referrerpolicy="no-referrer" src="/img/bVdncX3" alt="" title=""/></p><hr/><p><strong>一、对比：项目导向思维的“快”与“短”</strong><br/>项目导向的思维模式，本质上是追求“可交付、可验收、可关闭”。这没错，但一旦用于ITSS体系建设，问题就出现了。<br/> 项目制往往关注“阶段目标”而非“持续运行”。例如：</p><ul><li>只在立项期分配资源，后续缺乏运维支撑；</li><li>只注重评审指标，忽略实际落地；</li><li>项目团队解散后，标准无人维护。<br/>在GB/T 28827.2《信息技术服务 管理体系要求》中，明确提出“体系运行应与组织业务持续运行相结合”。也就是说，标准要求的是长期的制度化运行机制，而非临时性项目成果。<br/> 项目导向像“短跑冲刺”，体系化治理则更像“长跑训练”——前者看速度，后者看耐力。</li></ul><hr/><p><strong>二、体系化治理：以标准为“骨架”，以文化为“血液”</strong><br/>体系化治理并不是要让流程僵化，而是让管理有持续的生命力。<br/> 在我辅导过的一家通信企业中，他们最初同样陷入“项目思维”误区。直到经历一次应急事件，他们才真正理解体系的重要性。<br/>某年国庆期间，客户核心机房突发存储故障。虽然他们的ITSS项目刚刚通过了二级评估，但事件响应过程却混乱不堪：</p><ul><li>没有统一的应急指挥通道；</li><li>各部门自行决策，沟通断层；</li><li>工单记录混乱，事后难以追溯。<br/>事后复盘发现，他们的体系文件完备，但没有转化为真实的运行机制。<br/> 这次事件让高层意识到：体系不是“通过评估”，而是“支撑行动”。他们随后重组了IT治理委员会，设立持续改进岗位，并将流程执行与绩效挂钩。半年后，平均故障响应时间下降了47%。<br/>正如ITSS标准所强调的，治理体系的价值在于“机制化”与“内生性”。它让每一次事件都能推动体系的成长，而非成为孤立事故。</li></ul><hr/><p><strong>三、从“项目”到“体系”的四个关键转向</strong><br/>如果说项目导向关注“做完”，体系化治理则关注“做久”。在实践中，这种转变至少包含四个关键维度：</p><ol><li>从短期目标到长期治理目标<br/> 体系化建设要以组织战略为牵引，而不是评审节点。目标应体现在年度绩效、客户满意度与运维成熟度的持续提升上。</li><li>从任务分工到角色职责体系<br/> 项目化通常只分配“谁负责哪块任务”；体系化治理要定义“角色+责任+权限”的组合。<br/> 例如，服务管理负责人、流程管理人、持续改进专员等，都需要在组织架构中有明确定位。</li><li>从模板执行到流程闭环<br/> 项目往往停留在“交付模板”层面，而体系建设要确保“流程有输入、有输出、有改进”。<br/> 这意味着流程不仅要“存在”，还要“运转”。</li><li>从外部验收到内部驱动<br/> ITSS评估只是一个阶段性镜像。真正的成熟，是企业能在没有外部顾问的情况下，自主更新和优化体系。<br/>艾拓先锋组织ITSS服务项目经理培训，大家可以来课堂上跟我就这个问题深入探讨。学员常问：“体系建设到底该谁牵头？”<br/> 我的回答是——要有一个‘业务理解+标准意识’兼具的中台部门，既懂运维又懂管理。因为体系化的关键，不是文件，而是认同。</li></ol><hr/><p><strong>四、案例对照：失败的项目化 vs 成功的体系化</strong><br/>我印象最深的案例，是两家规模相当的银行。<br/> 第一家在三个月内完成ITSS三级评估，但项目结束后没人维护配置库，导致半年后系统数据完全失真。<br/> 第二家银行则建立了“IT服务管理办公室（ITSMO）”，每季度组织改进会议，持续优化指标。三年后，他们未再额外投入咨询费用，但成熟度从二级跃升至三级。<br/> 对比之下你会发现：体系化的成本比项目化更低，收益却更稳。<br/>体系化治理让标准成为一种组织语言：</p><ul><li>让每次流程优化都能积累知识资产；</li><li>让每个指标改进都能量化绩效；</li><li>让每个项目结束都能反哺体系成长。</li></ul><hr/><p><strong>五、制度化支撑：让标准“长在组织里”</strong><br/>体系化建设的核心，是制度化。它既不是“文档管理”，也不是“官僚化”，而是通过制度让改进有连续性。<br/> 标准里提出的<strong>“持续改进机制”</strong>，其实可以分解为三步：</p><ol><li>建立度量体系（如流程效率、服务满意度等）；</li><li>设立改进议题（由管理委员会定期评估）；</li><li>将结果纳入绩效与激励机制。<br/>这种机制一旦成型，就能让ITSS从“外部认证”转变为“内部驱动力”。<br/> 这也是我认为最值得中大型企业重视的转折点：体系化治理不靠项目经理的努力，而靠制度的自我强化。</li></ol><hr/><p><strong>六、风险警示：别让ITSS变成“文件体系”</strong><br/>在全国ITSS评估案例中，约有30%的企业在通过认证一年后，体系形同虚设。<br/> 原因很简单——他们把ITSS当作一次性任务，而非治理机制。<br/> 项目化思维的风险在于：</p><ul><li>体系停留在文件层；</li><li>改进止步于评估期；</li><li>组织缺乏内生更新机制。<br/>当市场环境变化、团队成员流动时，原有体系无法自我修复，标准最终“空心化”。<br/> 因此，真正的风险不是没通过评估，而是体系无法持续运行。<br/>体系化治理的关键，在于“标准持续更新、责任持续传承、文化持续强化”。<br/> 只有这样，ITSS才能成为企业的“免疫系统”，而非“临时项目”。</li></ul>]]></description></item><item>    <title><![CDATA[海洋生物识别系统【最新版】Python+]]></title>    <link>https://segmentfault.com/a/1190000047437932</link>    <guid>https://segmentfault.com/a/1190000047437932</guid>    <pubDate>2025-11-29 14:01:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、介绍</h2><p>海洋生物系统，本项目基于深度学习技术，构建了一个集海洋生物识别、数据可视化和智能问答于一体的Web应用系统。系统采用TensorFlow框架搭建卷积神经网络模型，通过对22种常见海洋生物（包括海豚、鲸鱼、鲨鱼、珊瑚、海星等）数据集进行多轮迭代训练，最终获得高精度识别模型，并开发了功能完善的Web操作平台。</p><p><strong>前端</strong>: Vue3、Element Plus</p><p><strong>后端</strong>：Django</p><p><strong>算法</strong>：TensorFlow、卷积神经网络算法</p><p><strong>具体功能</strong>：</p><ol><li>系统分为管理员和用户两个角色，登录后根据角色显示其可访问的页面模块。</li><li>登录系统后可发布、查看、编辑文章，创建文章功能中集成了markdown编辑器，可对文章进行编辑。</li><li>在图像识别功能中，用户上传图片后，点击识别，可输出其识别结果和置信度</li><li>基于Echart以柱状图形式输出所有种类对应的置信度分布图。</li><li>在智能问答功能模块中：用户输入问题，后台通过对接Deepseek接口实现智能问答功能。</li><li>管理员可在用户管理模块中，对用户账户进行管理和编辑。</li></ol><p><strong>选题背景与意义</strong>：<br/>随着海洋资源的开发与保护日益受到重视，对海洋生物进行快速、准确的识别与分析成为科研、教育及生态保护领域的重要需求。然而，传统的海洋生物识别方法依赖专家经验，效率较低，且公众参与度有限。近年来，深度学习技术在图像识别领域取得了显著进展，为海洋生物的自动化识别与知识普及提供了新的可能。</p><p>本项目基于这一背景，利用TensorFlow框架构建卷积神经网络模型，针对22种常见海洋生物（如海豚、鲸鱼、珊瑚等）进行训练，实现了高精度的生物识别。同时，系统结合Vue3与Django开发了功能完善的Web平台，集成图像识别、数据可视化及智能问答等功能，为用户提供便捷的海洋生物认知与交互体验。通过这一系统，旨在推动海洋生物知识的普及，提升海洋保护的科技参与度。</p><h2>二、系统效果图片展示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437934" alt="图片" title="图片"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437935" alt="图片" title="图片" loading="lazy"/></p><h2>三、演示视频 and 完整代码 and 安装</h2><p>地址：<a href="https://link.segmentfault.com/?enc=%2FqOSPMjhLWK%2B%2FJgMo4Fi1w%3D%3D.jil7J6RGGPOMByhIYF7KEPPKTU5Tlh%2Ftvzm9DJ%2BTU1M%3D" rel="nofollow" target="_blank">https://ziwupy.cn/p/ZtMoo4</a></p><h2>四、卷积神经网络算法介绍</h2><p>卷积神经网络是一种专门用于处理网格状数据（如图像）的深度学习架构。其核心思想是通过<strong>卷积核</strong>在输入数据上进行滑动窗口式的“卷积”操作，以自动提取从低级到高级的层次化特征。</p><p>CNN主要由以下关键层构成：</p><ol><li><strong>卷积层</strong>：是CNN的核心。它使用多个卷积核在输入图像上滑动，通过计算局部区域的点积来生成特征图。这一过程能够有效捕捉图像中的局部特征，如边缘、角点等，并具有权值共享的特点，大大减少了模型参数。</li><li><strong>池化层</strong>：通常跟在卷积层之后，用于对特征图进行下采样。最常用的是<strong>最大池化</strong>，它取局部区域的最大值。池化层可以在保持主要特征的同时，减小数据尺寸，降低计算复杂度，并增强模型的平移不变性。</li><li><strong>全连接层</strong>：在网络的末端，将经过多次卷积和池化后提取到的展开特征进行综合，并映射到最终的输出类别上，完成分类任务。</li></ol><p>通过堆叠这些层，CNN能够逐步从原始像素中提取出越来越复杂的特征，最终实现高精度的图像识别。</p><p>以下是一个使用TensorFlow和Keras API构建一个简单的CNN模型，用于手写数字识别（MNIST数据集）的示例。</p><pre><code class="python">import tensorflow as tf
from tensorflow.keras import datasets, layers, models

# 1. 加载并预处理数据
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
# 将图像数据重塑为 (28, 28, 1) 并归一化
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

# 2. 构建CNN模型
model = models.Sequential([
    # 第一卷积块
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    # 第二卷积块
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # 将特征图展平以输入全连接层
    layers.Flatten(),
    # 全连接层
    layers.Dense(64, activation='relu'),
    # 输出层，10个神经元对应10个类别（数字0-9）
    layers.Dense(10, activation='softmax')
])

# 3. 编译模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 4. 训练模型
model.fit(train_images, train_labels, epochs=5, 
          validation_data=(test_images, test_labels))

# 5. 评估模型
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'\n测试准确率：{test_acc}')</code></pre><p>上述代码完整展示了使用CNN进行图像分类的流程。首先，我们加载MNIST数据集并将其处理成适合CNN输入的格式。接着，我们构建了一个顺序模型，其中包含两个卷积-池化层组合，用于特征提取；之后通过Flatten层将多维特征图展平，并连接两个全连接层进行分类。模型使用Adam优化器和稀疏分类交叉熵损失函数进行编译。最后，我们在训练集上训练5个轮次，并在测试集上评估最终性能，通常能达到很高的准确率。此代码是理解CNN应用的基础模板，本项目的海洋生物识别系统即是在此基础上，使用更复杂的网络结构和海洋生物数据集进行训练与优化的。</p>]]></description></item><item>    <title><![CDATA[【宠物识别系统】Python+Tenso]]></title>    <link>https://segmentfault.com/a/1190000047437970</link>    <guid>https://segmentfault.com/a/1190000047437970</guid>    <pubDate>2025-11-29 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、介绍</h2><p>宠物识别系统，本系统基于TensorFlow框架，采用卷积神经网络（CNN）算法，构建了一个能够识别37种常见宠物品种的智能识别系统。所使用的数据集涵盖了多个猫犬品种，例如阿比西尼亚猫、布偶猫、柴犬、哈士奇等。经过多轮迭代训练，最终得到了识别准确率较高的预测模型，并部署于Web端实现可视化交互。</p><p><strong>前端</strong>: Vue3、Element Plus</p><p><strong>后端</strong>：Django</p><p><strong>算法</strong>：TensorFlow、卷积神经网络算法</p><p><strong>具体功能</strong>：</p><ol><li>系统分为管理员和用户两个角色，登录后根据角色显示其可访问的页面模块。</li><li>登录系统后可发布、查看、编辑文章，创建文章功能中集成了markdown编辑器，可对文章进行编辑。</li><li>在图像识别功能中，用户上传图片后，点击识别，可输出其识别结果和置信度</li><li>基于Echart以柱状图形式输出所有种类对应的置信度分布图。</li><li>在智能问答功能模块中：用户输入问题，后台通过对接Deepseek接口实现智能问答功能。</li><li>管理员可在用户管理模块中，对用户账户进行管理和编辑。</li></ol><p><strong>选题背景与意义</strong>：<br/>随着社会经济发展与家庭结构变化，宠物在人们生活中的角色日益重要，宠物相关科技应用需求持续增长。然而，普通饲养者往往难以准确辨别宠物品种，导致在饲养、医疗及交流过程中存在信息障碍。尽管人工智能技术已在图像识别领域取得显著进展，但现有应用多集中于人脸或通用物体识别，专门针对多品种宠物、兼具实用性与交互性的识别系统仍较为缺乏。</p><p>基于此背景，本课题旨在开发一个基于TensorFlow与卷积神经网络（CNN）的智能宠物识别系统。该系统能够对包括阿比西尼亚猫、布偶猫、柴犬、哈士奇等在内的37种常见猫犬品种进行高效识别，并结合Vue3与Django框架构建完整的Web应用平台。系统不仅提供高准确率的图像识别功能，还集成文章管理、数据可视化及智能问答等多种服务，从而为宠物爱好者搭建一个集知识交流与智能识别于一体的实用工具，有效填补了细分领域的技术应用空白。</p><h2>二、系统效果图片展示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437972" alt="图片" title="图片"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437973" alt="图片" title="图片" loading="lazy"/></p><h2>三、演示视频 and 完整代码 and 安装</h2><p>地址：<a href="https://link.segmentfault.com/?enc=Rh%2FLW49quDuf3gP1PKSeLA%3D%3D.uM0p1ZWqfq7XbWu1pmoqhZpojFJlGofAAaGzJMqCyFQ%3D" rel="nofollow" target="_blank">https://ziwupy.cn/p/nTpUXb</a></p><h2>四、卷积神经网络算法介绍</h2><p>卷积神经网络是一种专为处理网格状数据（如图像）而设计的深度学习模型。其核心思想是通过<strong>卷积层</strong>自动提取图像中的空间层次特征。</p><p>一个典型的CNN包含以下关键层：</p><ul><li><strong>卷积层</strong>：使用多个可学习的滤波器（卷积核）在输入图像上滑动，通过计算局部区域的点积来提取特征（如边缘、角点、纹理）。这个过程保留了像素间的空间关系。</li><li><strong>池化层</strong>：紧随卷积层之后，用于降低特征图的维度，减少计算量并增强模型的平移不变性。最大池化是最常用的方式。</li><li><strong>全连接层</strong>：在网络的末端，将经过多次卷积和池化后提取到的高级特征图展平，进行综合判断，最终输出分类概率。</li></ul><p>通过堆叠这些层，CNN能够从低级特征（边缘）逐步组合形成高级特征（器官、物体），从而实现高效的图像识别。</p><p>以下是一个使用TensorFlow和Keras API构建一个简单的CNN模型，用于处理MNIST手写数字识别的示例。</p><pre><code class="python">import tensorflow as tf
from tensorflow.keras import datasets, layers, models

# 1. 加载并预处理数据
(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()
# 将图像重塑为 (28, 28, 1) 并归一化
train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255
test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255

# 2. 构建CNN模型
model = models.Sequential([
    # 第一个卷积层，使用32个3x3的滤波器，激活函数为ReLU
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    # 最大池化层，窗口大小为2x2
    layers.MaxPooling2D((2, 2)),
    # 第二个卷积层，使用64个3x3的滤波器
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # 将多维特征图展平为一维向量，以便输入全连接层
    layers.Flatten(),
    # 全连接层，128个神经元
    layers.Dense(128, activation='relu'),
    # 输出层，10个神经元对应10个数字类别，使用softmax激活函数输出概率分布
    layers.Dense(10, activation='softmax')
])

# 3. 编译模型
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 4. 训练模型
model.fit(train_images, train_labels, epochs=5, 
          validation_data=(test_images, test_labels))

# 5. 评估模型
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'\nTest accuracy: {test_acc}')</code></pre><p>上述代码演示了构建CNN的经典流程：数据准备、模型搭建、编译、训练与评估。对于宠物识别系统，核心原理与此相同，但更为复杂。：</p><ol><li><strong>准备更庞大的数据集</strong>：包含37个宠物品种的标注图像。</li><li><strong>使用更深的网络</strong>：可以迁移学习预训练模型（如ResNet, MobileNet），在其基础上进行微调，以适应宠物分类任务。</li><li><strong>调整输入尺寸</strong>：将图像调整为更适合现代网络的尺寸（如224x224）。</li><li><strong>部署模型</strong>：如所述，使用Django作为后端，将训练好的模型加载到服务中，处理用户上传的图片并返回识别结果和置信度。</li></ol>]]></description></item><item>    <title><![CDATA[openFuyao信息直升机 | 第5期]]></title>    <link>https://segmentfault.com/a/1190000047437831</link>    <guid>https://segmentfault.com/a/1190000047437831</guid>    <pubDate>2025-11-29 13:02:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>业务痛点：</h2><p>AI推理是AI领域中将大模型转化为应用效果与商业价值的核心技术，但在实际生产部署中仍然面临着多样化算力场景下的效率低与可部署性成本高，高并发、长上下文LLM推理场景中的性能和资源利用率瓶颈。</p><h2>根因分析：</h2><ul><li>用户体验与资源效率瓶颈：当前长上下文LLM推理的首Token延时普遍在数百毫秒至秒级，且长上下文场景下KV缓存显存占用呈线性增长，严重制约Agent的响应效率与部署规模；传统静态批处理无法适配动态负载，导致短请求被长请求阻塞（对头阻塞问题）。</li><li>企业生产级场景挑战：企业生产场景，普遍存在高推理成本、多样化算力（GPU算力 + 国产化算力）利用率低、SLA难以保障、生产级规模部署管理复杂等问题。</li><li>云原生AI全栈挑战：现有云原生调度（如Kubernetes）缺乏LLM感知能力，无法优化KV缓存生命周期、动态批处理等场景。</li></ul><h2>高性能AI推理服务化框架方案</h2><p>openFuyao通过“聚焦智能动态路由 + xPyD计算动态资源管理调度 + 分布式KVCache/KVCache优化 + 端到端易用性 + 推理场景可观测体系”高性能、可扩展子系统的构建，致力于系统性突破当前LLM推理的瓶颈，同时面向超节点场景进一步加速，支持灵衢、CXL、NVLink等高速总线：</p><p><img width="723" height="728" referrerpolicy="no-referrer" src="/img/bVdncUa" alt="" title=""/></p><ul><li>首Token延时（TTFT）降低：智能路由与缓存命中策略优化、近实时集群节点负载感知。</li><li>推理吞吐提升：弹性xPyD分离架构升级、高性能弹性配比。</li><li>N/S、E/W全局显存瓶颈突破：多级KVCache、集群KVCache池化；结合高性能传输协议和去中心化高性能硬件，进一步降低KVCache传输延迟。</li><li>资源利用率提升：通过动态资源调度配比和异构算力池化进一步提升资源利用率。</li></ul><p><img width="723" height="475" referrerpolicy="no-referrer" src="/img/bVdncUh" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[AI重塑招聘生态：从效率革命到职能升级 ]]></title>    <link>https://segmentfault.com/a/1190000047437836</link>    <guid>https://segmentfault.com/a/1190000047437836</guid>    <pubDate>2025-11-29 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI重塑招聘生态：从效率革命到职能升级<br/>当 HR 还在忙着追候选人时，AI 已经把招聘周期缩短了 40%。<br/>人工智能技术正在逐步改变招聘行业的运作方式。有数据显示，通过微表情分析与语义推理等技术手段，招聘周期可以实现40%的缩短；某大型银行引入AI系统后，面试到场率提升至90.7%；辉瑞制药在研发人才识别中采用AI技能图谱，创新周期相应缩短22%。<br/>这些变化表明，AI不仅释放了人力资源从业者从事重复性工作的时间，更推动了HR职能的转型。据统计，近半数企业已完成人力资源流程的智能化升级，HR逐渐从流程管理角色转向组织战略伙伴，更多参与到组织健康诊断、差异化薪酬设计及数据驱动决策中。例如，ING银行通过引入AI辅助，决策效率提升了50%。</p><p>一、AI面试评分：科学支持招聘决策<br/>传统招聘中，“凭感觉”选人的情况时有发生。当前AI面试系统通过效标效度与重测信度双重验证，并结合大规模人机对比实验，实现了与资深面试官判断的高度一致性。最新版本的系统进一步巩固了其在AI面试智能体领域的技术水平，评分结果可直接用于招聘决策。<br/>二、全流程精准化设计<br/>AI面试系统在多个环节实现精准优化：<br/>•一问多能：单题可评估多项胜任力，提升筛选效率50%以上<br/>•智能追问：根据回答内容实时生成追问，避免关键能力遗漏<br/>•简历深度解析：自动识别亮点与疑点，辅助防伪与人才发现<br/>•全维度测评：覆盖通用能力与专业技能，支持自动出题<br/>该系统已不仅作为面试工具使用，而是能够在初筛及部分技术复试中发挥实质作用。<br/>三、优化候选人面试体验<br/>传统AI面试常被诟病交互生硬，影响雇主形象。当前系统通过以下设计改善体验：<br/>•情绪识别与引导，缓解候选人紧张情绪<br/>•无断点交互，无需操作“开始/结束”，模拟真实对话<br/>•视觉呈现更为自然，提升沉浸感<br/>•支持多轮问答，及时回应岗位与公司相关问题<br/>在人才竞争日益激烈的背景下，面试过程已成为雇主品牌展示的重要环节。<br/>四、全流程自动化的人才寻访<br/>除AI面试外，自动化人才寻访系统可实现从识别、沟通到系统录入的全流程智能化：<br/>•快速初始化，即启即用<br/>•自动筛选与动态沟通，模拟人类交流方式<br/>•全覆盖应答，主动获取所需信息<br/>•数据自动同步至招聘系统<br/>此类系统大幅提升了人才寻访环节的整体效率。<br/>五、技术应用的验证路径<br/>目前，部分系统开放了较大规模的免费使用权限，用户可在实际招聘场景中验证其效果，涵盖面试、测评、筛选、寻访及数据同步等多个环节。这为招聘团队提供了低风险的体验途径。<br/>已有包括多家大型企业及高校在内的机构使用了相关系统，反映出这类AI招聘工具在实际应用中的认可度。<br/>当前，AI招聘技术的发展重点集中于精准选人与体验提升，最新版本系统在这两方面均展现了较高水平。</p>]]></description></item><item>    <title><![CDATA[【URP】Unity[内置Shader]]]></title>    <link>https://segmentfault.com/a/1190000047437734</link>    <guid>https://segmentfault.com/a/1190000047437734</guid>    <pubDate>2025-11-29 12:02:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=aaLejJ5ph02zfYWetdJ%2F6A%3D%3D.S%2FWx2Ri8vwnfr3VGhcoipNJ3b2yam%2B%2BzmUHR01w1Hhr6ubALPPE8hEruDonhCber7NvISX%2B%2FP%2FKBAohFeXxeWNvVWj0qBdFIT7oVxK75xmG3LOJmUJE9EphsFo1YHF5P71CRN8mJ7ARWux1h0Kfj00TzE9VqZO0999XCRPj4yonX91LMqjTDaGUfj4moq31ajSGRmi0XGIf2zZxc6s7Vt69EGrx7iwLwpVhsCzay7EQ%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong></blockquote><h2><strong>SimpleLit Shader的作用与原理</strong></h2><p><a href="https://link.segmentfault.com/?enc=7YGvrz27NhM1DUS1jBcEPw%3D%3D.lN2ktnr15ef%2FF4ntukhefMAI1GFvAa9N2pKTdKL0mogYGIjJrsOpVReUGwRorI1urF2gzxx5otdO1%2FGLPAXge8s5jTLWbuuNs2r33AZTvaa2zEflHaKNmUJfOYWoey3kDGxUER9L3SDDKB4OlTs31g%3D%3D" rel="nofollow" target="_blank">SimpleLit</a> Shader是Unity通用渲染管线(URP)中的一种轻量级着色器，主要用于低端设备或需要高效渲染的场景。它采用简化的Blinn-Phong光照模型，不计算物理正确性和能量守恒，从而实现了比标准Lit Shader更快的渲染速度。</p><h2><strong>核心原理</strong></h2><ul><li>‌<strong>简化光照模型</strong>‌：使用Blinn-Phong模型而非PBR，省略了复杂的物理计算</li><li>‌<strong>模块化结构</strong>‌：分为Surface Options(控制渲染方式)、Surface Inputs(描述表面特性)和Advanced Options(底层渲染设置)三部分</li><li>‌<strong>前向渲染路径</strong>‌：通过"UniversalForward" Pass实现，支持主光源阴影和附加光源计算</li><li>‌<strong>变体控制</strong>‌：通过Shader Feature和Multi Compile指令管理不同功能组合，减少不必要的变体</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437736" alt="image.png" title="image.png"/></p><h2><strong>发展历史</strong></h2><p>SimpleLit Shader随着URP的发展经历了多个版本迭代：</p><ul><li>早期版本(2019-2020)：作为URP首批内置Shader之一，提供基础光照功能</li><li>URP 7.x时期(2021)：优化了变体管理，增加了GPU实例化支持</li><li>URP 12.1.1(2022)：完善了SubShader错误处理，默认返回紫色洋葱效果</li><li>当前版本(2024-2025)：支持DOTS Instancing，增强了与Shader Graph的兼容性</li></ul><h2><strong>具体使用方法</strong></h2><h3><strong>基础应用示例</strong></h3><ul><li>创建材质：在Project窗口右键 &gt; Create &gt; Material</li><li>选择Shader：在材质Inspector中，选择"Universal Render Pipeline &gt; Simple Lit"</li><li><p>配置属性：</p><ul><li>Base Map：设置基础颜色纹理</li><li>Base Color：调整整体色调</li><li>Smoothness：控制表面光滑度</li><li>Emission：添加自发光效果</li></ul></li></ul><h3><strong>代码示例</strong></h3><pre><code class="csharp">csharp
// 通过代码设置SimpleLit材质属性
Material simpleLitMat = new Material(Shader.Find("Universal Render Pipeline/Simple Lit"));
simpleLitMat.SetTexture("_BaseMap", Resources.Load&lt;Texture&gt;("BaseTexture"));
simpleLitMat.SetColor("_BaseColor", Color.blue);
simpleLitMat.SetFloat("_Smoothness", 0.75f);</code></pre><h2><strong>Shader Graph中的应用</strong></h2><h3><strong>创建SimpleLit风格Shader</strong></h3><ul><li>创建新Graph：右键 &gt; Create &gt; Shader &gt; Universal Render Pipeline &gt; Blank Shader Graph</li><li><p>设置Graph设置：</p><ul><li>Material设置为"Simple Lit"</li><li>Surface设置为"Opaque"或"Transparent"</li></ul></li><li><p>构建节点网络：</p><ul><li>使用"Sample Texture 2D"节点获取基础颜色</li><li>通过"Normal From Texture"节点处理法线贴图</li><li>连接"Master Stack"的对应输入端口</li></ul></li></ul><h3><strong>示例Graph功能</strong></h3><ul><li><p>‌<strong>基础颜色混合</strong>‌：</p><ul><li>混合纹理采样和颜色参数</li><li>添加细节遮罩控制混合强度</li></ul></li><li><p>‌<strong>动态高光控制</strong>‌：</p><ul><li>使用时间节点驱动高光强度变化</li><li>通过顶点位置影响高光范围</li></ul></li><li><p>‌<strong>边缘发光效果</strong>‌：</p><ul><li>计算视角与法线夹角</li><li>使用Fresnel节点创建边缘光</li></ul></li></ul><h3><strong>保存与应用</strong></h3><ul><li>保存Graph后生成.shadergraph文件</li><li>创建材质并选择生成的Shader</li><li>通过材质参数面板调整公开属性</li></ul><h2><strong>Shader simple lit 对比 Lit</strong></h2><p>SimpleLit Shader与Lit Shader的性能差异主要体现在光照模型的计算复杂度上。SimpleLit采用简化的Blinn-Phong光照模型，而Lit基于物理渲染（PBR），两者的性能差距和适用场景如下：</p><h3><strong>性能对比</strong></h3><ul><li><p>‌<strong>渲染效率</strong>‌</p><p>SimpleLit通过忽略物理正确性和能量守恒计算，相比Lit Shader可提升约30%-50%的渲染性能，尤其在低端移动设备上表现更显著。Lit Shader因需计算微表面模型（GGX+Smith）和复杂的光照交互，对GPU负担较大。</p></li><li><p>‌<strong>变体复杂度</strong>‌</p><p>Lit Shader支持更多材质属性（如金属度、粗糙度），导致Shader变体数量远高于SimpleLit，增加了内存和编译开销。SimpleLit的变体更少，适合需要快速迭代或大量实例化的场景。</p></li></ul><h3><strong>材质实现能力</strong></h3><ul><li><p>‌<strong>SimpleLit适用材质</strong>‌</p><ul><li>卡通风格表面（非PBR）</li><li>低多边形（Low Poly）美术风格</li><li>需要快速渲染的静态物体或背景元素</li><li>通过调整<code>_SpecColor</code>和<code>_Glossiness</code>实现简单高光效果，但不支持动态环境光遮蔽（AO）或复杂反射。</li></ul></li><li><p>‌<strong>Lit适用材质</strong>‌</p><ul><li>金属/非金属PBR材质（如真实感金属、石材）</li><li>需要动态光照和阴影的高质量场景</li><li>支持屏幕空间全局光照（SSGI）等高级特性。</li></ul></li></ul><h3><strong>选择建议</strong></h3><ul><li>‌<strong>性能优先</strong>‌：选择SimpleLit，尤其针对移动端或低端设备。</li><li>‌<strong>效果优先</strong>‌：需真实物理交互的场景（如角色、动态物体）使用Lit。</li></ul><p>两者均支持URP的GPU实例化和SRP Batcher优化，但SimpleLit在批量渲染时优势更明显.</p><h2>卡通风格 非PBR 材质实现</h2><h3><strong>核心实现原理</strong></h3><ul><li>‌<strong>基础光照模型</strong>‌：采用Simple Lit的Lambert漫反射+Blinn-Phong高光组合，通过阈值化处理实现色阶分离</li><li>‌<strong>边缘光增强</strong>‌：利用菲涅尔效应节点(Fresnel Effect Node)生成轮廓光，通过Power参数控制边缘宽度</li><li>‌<strong>色块分割</strong>‌：使用Step或SmoothStep函数对光照结果进行离散化处理，形成卡通风格的色块过渡</li></ul><h3><strong>关键参数说明</strong></h3><ul><li>‌_RampThreshold：控制阴影与亮部的分界阈值，值越小阴影区域越大</li><li>‌_RampSmooth：色块边缘的平滑过渡范围，设为0时产生硬边缘</li><li>‌_RimPower：调整边缘光衰减速度，值越大轮廓线越细</li><li><p>ToonSimpleLit.shader</p><pre><code class="c">Shader "Universal Render Pipeline/Simple Toon"
{
    Properties
    {
        _BaseColor("Base Color", Color) = (1,1,1,1)
        _BaseMap("Base Map", 2D) = "white" {}
        _ShadowColor("Shadow Color", Color) = (0.4,0.4,0.4,1)
        _RampThreshold("Ramp Threshold", Range(0,1)) = 0.5
        _RampSmooth("Ramp Smooth", Range(0.01,0.1)) = 0.01
        _SpecColor("Specular Color", Color) = (0.9,0.9,0.9,1)
        _SpecThreshold("Spec Threshold", Range(0,1)) = 0.5
        _SpecSmooth("Spec Smooth", Range(0,0.1)) = 0.02
        _RimPower("Rim Power", Range(0,10)) = 5
        _RimColor("Rim Color", Color) = (0.8,0.8,0.8,1)
    }

    SubShader
    {
        Tags { "RenderType"="Opaque" "RenderPipeline"="UniversalPipeline" }

        HLSLINCLUDE
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl"
        #include "Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl"

        TEXTURE2D(_BaseMap);
        SAMPLER(sampler_BaseMap);

        CBUFFER_START(UnityPerMaterial)
        float4 _BaseColor;
        float4 _BaseMap_ST;
        float4 _ShadowColor;
        float _RampThreshold;
        float _RampSmooth;
        float4 _SpecColor;
        float _SpecThreshold;
        float _SpecSmooth;
        float _RimPower;
        float4 _RimColor;
        CBUFFER_END

        struct Attributes
        {
            float4 positionOS : POSITION;
            float3 normalOS : NORMAL;
            float2 uv : TEXCOORD0;
        };

        struct Varyings
        {
            float4 positionCS : SV_POSITION;
            float2 uv : TEXCOORD0;
            float3 normalWS : TEXCOORD1;
            float3 viewDirWS : TEXCOORD2;
            float3 positionWS : TEXCOORD3;
        };

        Varyings ToonPassVertex(Attributes input)
        {
            Varyings output;
            VertexPositionInputs vertexInput = GetVertexPositionInputs(input.positionOS.xyz);
            output.positionCS = vertexInput.positionCS;
            output.uv = TRANSFORM_TEX(input.uv, _BaseMap);

            VertexNormalInputs normalInput = GetVertexNormalInputs(input.normalOS);
            output.normalWS = normalInput.normalWS;
            output.viewDirWS = GetWorldSpaceViewDir(vertexInput.positionWS);
            output.positionWS = vertexInput.positionWS;
            return output;
        }

        half4 ToonPassFragment(Varyings input) : SV_Target
        {
            // 基础纹理采样
            half4 baseMap = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.uv);
            half3 baseColor = baseMap.rgb * _BaseColor.rgb;

            // 光照计算
            Light mainLight = GetMainLight();
            float3 lightDir = normalize(mainLight.direction);
            float3 normalWS = normalize(input.normalWS);
            float NdotL = dot(normalWS, lightDir);

            // 漫反射色阶化
            float diffuse = smoothstep(_RampThreshold - _RampSmooth, 
                                      _RampThreshold + _RampSmooth, 
                                      NdotL * 0.5 + 0.5);
            float3 diffuseColor = lerp(_ShadowColor.rgb, 1, diffuse) * baseColor;

            // 高光计算
            float3 viewDir = normalize(input.viewDirWS);
            float3 halfDir = normalize(lightDir + viewDir);
            float specular = pow(max(0, dot(normalWS, halfDir)), 32);
            specular = smoothstep(_SpecThreshold - _SpecSmooth,
                                 _SpecThreshold + _SpecSmooth,
                                 specular);
            float3 specularColor = specular * _SpecColor.rgb * mainLight.color;

            // 边缘光
            float rim = 1 - max(0, dot(viewDir, normalWS));
            rim = pow(rim, _RimPower);
            float3 rimColor = rim * _RimColor.rgb;

            // 最终合成
            float3 finalColor = diffuseColor * mainLight.color + specularColor + rimColor;
            return half4(finalColor, baseMap.a);
        }
        ENDHLSL

        Pass
        {
            Name "ToonPass"
            Tags { "LightMode"="UniversalForward" }

            HLSLPROGRAM
            #pragma vertex ToonPassVertex
            #pragma fragment ToonPassFragment
            ENDHLSL
        }
    }
}</code></pre></li></ul><h3><strong>扩展优化建议</strong></h3><ul><li>‌<strong>添加色阶贴图</strong>‌：使用1D纹理控制光照过渡曲线，实现更复杂的卡通色阶效果</li><li>‌<strong>描边效果</strong>‌：通过背面挤出法或后处理实现轮廓描边（需额外Pass）</li><li>‌<strong>材质变体</strong>‌：通过Shader变体支持不同风格切换（如赛璐璐/水彩风格）</li></ul><p>该Shader保留了URP轻量级特性，通过HLSL重写光照模型实现非PBR卡通效果，可直接在URP项目中使用。如需更复杂的艺术控制，可参考原神风格的贴图通道分配方案</p><h2><strong>性能优化建议</strong></h2><ul><li>‌<strong>变体控制</strong>‌：禁用不必要的Shader变体(如_SPECGLOSSMAP)减少内存占用</li><li>‌<strong>GPU Instancing</strong>‌：对相同材质的对象启用实例化减少Draw Call</li><li>‌<strong>纹理压缩</strong>‌：使用适当的压缩格式减少显存占用</li><li>‌<strong>LOD组合</strong>‌：与复杂Shader配合使用，根据距离切换SimpleLit</li></ul><p>SimpleLit Shader通过简化光照计算在保持基本视觉效果的同时显著提升渲染效率，特别适合移动平台和低端设备，是URP管线中平衡性能与效果的重要工具</p><hr/><blockquote><a href="https://link.segmentfault.com/?enc=J3mhePpq19OXgSxHAlThMg%3D%3D.Xx89dOF0SqCwTSFgjQx4%2FNSPJUQJUjPaZqE%2BkAziVuwX4U4PyYGCn45CStiQa52I4I13avJyStjEAy2oz7XTpIZ7towkma7jhTdeKFYu4Vt6IwyX4g1CkAWyMfwEn6%2ByeVvFeJprHPLioVuc%2BQTuNFw0%2B3IBCooejtq%2FuUj2SRzraffBfcCGWuDRhYDqjjw57Te7806wVTumGTlQeBGn%2BN3nM7kbG9OneTaJXqCnYJA%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[【赵渝强老师】阿里云大数据集成开发平台D]]></title>    <link>https://segmentfault.com/a/1190000047437807</link>    <guid>https://segmentfault.com/a/1190000047437807</guid>    <pubDate>2025-11-29 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>DataWorks是阿里云重要的PaaS（Platform as a Service）平台产品，为用户提供数据集成、数据开发、数据地图、数据质量和数据服务等全方位的产品服务，一站式开发管理的界面，帮助企业专注于数据价值的挖掘和探索。</p><p>DataWorks支持多种计算和存储引擎服务，包括离线计算MaxCompute、开源大数据引擎E-MapReduce、基于Flink的实时计算、机器学习PAI、图计算服务Graph Compute和交互式分析服务等，并且支持用户自定义接入计算和存储服务。DataWorks可以为用户提供全链路智能大数据及AI开发和治理服务。</p><p>用户可以使用DataWorks，对数据进行传输、转换和集成等操作，从不同的数据存储引入数据，并进行转化和开发，最后将处理好的数据同步至其它数据系统。</p><p>视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV1X5UvB6Epq/?aid=115616291888415&amp;cid=34285094320" target="_blank">https://www.bilibili.com/video/BV1X5UvB6Epq/?aid=115616291888...</a></p><p>DataWorks提供以下九个核心功能模块。</p><h2>一、 数据集成</h2><p>DataWorks的数据集成功能模块是稳定高效、弹性伸缩的数据同步平台，致力于提供复杂网络环境下、丰富的异构数据源之间高速稳定的数据移动及同步能力。DataWorks数据集成支持离线同步、实时同步，以及离线和实时一体化的全增量同步。其中：</p><ul><li>离线同步场景下，支持设置离线同步任务的调度周期。</li><li>支持数据库、数仓、NoSQL数据库、文件存储、消息队列等近50多种不同异构数据源之间的数据同步。</li><li>支持在各类复杂网络环境下，连通数据源的网络解决方案，在各种网络环境下均可使用DataWorks数据集成实现网络连通。</li><li>支持安全控制与运维监控，保障数据同步的安全、可控。</li></ul><p>下图展示了DataWorks的数据集成页面。<br/><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdncTO" alt="image.png" title="image.png"/></p><h2>二、 数据加工</h2><p>DataWorks的数据开发是数据加工的开发平台，运维中心是数据加工的管理平台。基于这两个功能模块，用户可以在DataWorks上规范、高效地构建和运维数据开发工作流。<br/>DataWorks的数据开发平台可以使用工具DataStudio进行支持。DataStudio的数据开发工具提供的功能如下：</p><ul><li>DataStudio支持MaxCompute、EMR、CDH、Hologres、AnalyticDB、Clickhouse等多种计算引擎，支持在统一的平台上进行各类引擎任务的开发、测试、发布和运维等操作。</li><li>DataStudio支持智能编辑器、可视化依赖编排，调度能力经过阿里集团内调度任务、复杂业务依赖的反复验证。</li><li>DataStudio提供隔离的开发和生产环境，结合版本管理、代码评审、冒烟测试、发布管控、操作审计等配套功能，帮助企业规范地完成数据开发。</li></ul><p>下图展示了DataStudio数据开发工具的主页面。<br/><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdncTP" alt="image.png" title="image.png" loading="lazy"/></p><p>DataWorks运维中心支持数据时效性保障、任务诊断、影响分析、自动运维、移动运维等功能。下图展示了DataWorks运维中心的主页面。<br/><img width="723" height="399" referrerpolicy="no-referrer" src="/img/bVdncTQ" alt="image.png" title="image.png" loading="lazy"/></p><h2>三、 数据建模</h2><p>数据建模是阿里云DataWorks自主研发的智能数据建模产品，沉淀了阿里巴巴十多年来数仓建模方法论的最佳实践，包含数仓规划、数据标准、维度建模及数据指标四大模块，帮助企业在搭建数据中台、数据集市建设过程中提升建模及逆向建模的能力，并通过数据建模快速构建企业数据资产。<br/>DataWorks数据建模可助力企业构建自身建模能力，挖掘企业的数据资产价值。它支持以下的场景：</p><ul><li><strong><em>海量数据的标准化管理</em></strong>：企业业务越庞大数据结构就越复杂，企业数据量会随着企业业务的快速发展而迅速增长，如何结构化有序地管理和存储数据是每个企业都将面临的一个挑战。</li><li><strong><em>业务数据互联互通，打破信息壁垒</em></strong>：公司内部各业务、各部门之间数据独立自主形成了数据孤岛，导致决策层无法清晰、快速地了解公司各类数据情况。如何打破部门或业务领域之间的信息孤岛是企业数据管理的一大难题。</li><li><strong><em>数据标准整合，统一灵活对接</em></strong>：同一数据不同描述，企业数据管理难、内容重复、结果不准确。如何制定统一的数据标准又不打破原有的系统架构，实现灵活对接上下游业务，是标准化管理的核心重点之一。</li><li><strong><em>数据价值最大化，企业利润最大化</em></strong>：在最大程度上用好企业各类数据，使企业数据价值最大化，为企业提供更高效的数据服务。</li></ul><p>下图展示了DataWorks数据建模的主页面。<br/><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdncTS" alt="image.png" title="image.png" loading="lazy"/></p><h2>四、 数据分析</h2><p>数据分析基于“人人都是数据分析师”的产品目标，旨在为更多非专业数据开发人员，如数据分析、产品、运营等工作人员提供更加简洁高效的取数、用数工具，提升大家日常取数分析效率。数据分析支持基于个人视角的数据上传、公共数据集、表搜索与收藏、在线SQL取数、SQL文件共享、SQL查询结果下载及用电子表格进行大屏幕数据查看等产品功能。</p><p>下图展示了DataWorks数据分析的主页面。<br/><img width="723" height="472" referrerpolicy="no-referrer" src="/img/bVdncTT" alt="image.png" title="image.png" loading="lazy"/></p><h2>五、 数据质量</h2><p>DataWorks的全流程数据质量监控功能为用户提供多种预设表级别、字段级别和自定义的监控模板。数据质量可以帮助用户第一时间感知到源端数据的变更与ETL（Extract Transformation Load）中产生的脏数据，自动拦截问题任务，有效阻断脏数据向下游蔓延。<br/>数据质量以数据集（DataSet）为监控对象，支持监控MaxCompute数据表和DataHub实时数据流。当离线MaxCompute数据发生变化时，数据质量会对数据进行校验，并阻塞生产链路，以避免问题数据污染扩散。同时，数据质量提供历史校验结果的管理，以便用户对数据质量进行分析和定级。</p><p>下图展示了DataWorks数据质量管理中的任务查询页面。<br/><img width="723" height="322" referrerpolicy="no-referrer" src="/img/bVdncTU" alt="image.png" title="image.png" loading="lazy"/></p><h2>六、 数据地图</h2><p>DataWorks的数据地图功能可以实现对数据的统一管理和血缘的跟踪。数据地图以数据搜索为基础，提供表使用说明、数据类目、数据血缘、字段血缘等工具，帮助数据表的使用者和拥有者更好地管理数据、协作开发。下图展示DataWorks的数据地图。<br/><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdncTW" alt="image.png" title="image.png" loading="lazy"/></p><h2>七、 数据服务</h2><p>DataWorks的数据服务功能模块是灵活轻量、安全稳定的数据API构建平台，旨在为企业提供全面的数据共享能力，帮助用户从发布审批、授权管控、调用计量、资源隔离等方面实现数据价值输出及共享开放。下图展示DataWorks的数据服务。<br/><img width="723" height="275" referrerpolicy="no-referrer" src="/img/bVdncT1" alt="image.png" title="image.png" loading="lazy"/></p><h2>八、 数据迁移</h2><p>DataWorks的数据迁移通过使用迁移助手支持将开源调度引擎的作业迁移至DataWorks，支持作业跨云、跨Region、跨账号迁移，实现DataWorks作业快速克隆部署，同时DataWorks团队联合大数据专家服务团队，上线迁云服务，帮助用户快速实现数据与任务的上云。下图展示DataWorks的数据迁移助手。<br/><img width="723" height="289" referrerpolicy="no-referrer" src="/img/bVdncT2" alt="image.png" title="image.png" loading="lazy"/></p><h2>九、 开放平台</h2><p>DataWorks开放平台是DataWorks对外提供数据和能力的开放通道。DataWorks开放平台提供开放API（OpenAPI）、开放事件（OpenEvent）、扩展程序（Extensions）的能力，可以帮助用户快速实现各类应用系统对接DataWorks、方便快捷的进行数据流程管控、数据治理和运维，及时响应应用系统对接DataWorks的业务状态变化。下图展示DataWorks的开放平台。<br/><img width="723" height="445" referrerpolicy="no-referrer" src="/img/bVdncT3" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Nessus 10.11 Auto In]]></title>    <link>https://segmentfault.com/a/1190000047437708</link>    <guid>https://segmentfault.com/a/1190000047437708</guid>    <pubDate>2025-11-29 11:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Nessus Professional 10.11 Auto Installer for Ubuntu 24.04 - Nessus 自动化安装程序</p><p>发布 Nessus 试用版自动化安装程序，支持 macOS Tahoe、RHEL 10、Ubuntu 24.04 和 Windows</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=YKoGhVOnUPopZKKtiz3RmQ%3D%3D.uKg3%2Bt9wE6s5wDURj4D04vzjb9sxhP49qF6La8ql7f0c4Q8QuGG4AjWgEDV%2FOWrddPgmOmSJWyHs502YTtCoEw%3D%3D" rel="nofollow" target="_blank">https://sysin.org/blog/nessus-auto-install-for-ubuntu/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=isIbKZGlCLHXPIXOuVa6lQ%3D%3D.4xYyykncfyqrYyRbALG%2FHq5gdykNjRyqxRhgPCWDmu4%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><h2>Nessus 简介</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436343" alt="Nessus" title="Nessus"/></p><p>Nessus Vulnerability Scanner</p><p><strong>漏洞评估领域的全球黄金标准</strong>，<strong>针对现代攻击面量身打造</strong>。</p><p>利用业界最受信赖的漏洞评估解决方案来评估现代攻击面。扩展到传统的 IT 资产之外 – 保护云基础设施和获取对与互联网相连的攻击面的可见性。</p><p><strong># 1 准确度</strong>：</p><p>Nessus 达到了 6 西格玛准确度，实现了业内最低的误报率</p><p>*每 100 万次扫描中仅有 0.32 次误报</p><p><strong># 1 覆盖面</strong>：</p><p>Nessus 拥有业内首屈一指的漏洞覆盖面深度和广度</p><p><strong># 1 采用率</strong>：</p><p>Nessus 深受数万家企业的信赖，全球下载次数达到 200 万次</p><p><strong># 1 口碑信誉</strong>：</p><p>口说无凭，无需赘言。为何全球安全专业人士对 Nessus 的信赖让您眼见为实</p><p><strong>Nessus 在漏洞评估领域一路领先</strong>！</p><p>从创立伊始，我们就与各类网络安全相关行业紧密协作。我们根据业界的反馈持续优化  Nessus，将其打造成市场中最准确全面的漏洞评估解决方案。20 年以来，我们不忘初心，始终专注于业界协作与产品创新  (sysin)，建立起最准确全面的漏洞数据库，让您的企业不会因忽视重要漏洞而暴露于风险之中。</p><p>今天，Nessus 深受全球数万家企业的信赖，是全球部署最为广泛的安全技术之一，而且是漏洞评估行业的黄金标准。</p><p><strong>94K+</strong> 个 CVE</p><p><strong>226,000+</strong> 款插件</p><p><strong>100+</strong> 款新插件，每周定期发布</p><p>Tenable 的零日研究对新漏洞和紧急漏洞提供全天候更新，因此您将始终具有全面的态势感知。</p><h2>自动化安装 Nessus 试用版</h2><p>Nessus 官方提供了试用版，可供个人学习和研究使用，主要功能没有限制，只是没有任何支持服务。网上文章冠以和谐版称之，是不存在的。个人学习和测试效果好，也是将来的潜在客户，Nessus 官方本来就支持试用的。</p><p>本程序只是将公开的方法简化一次性执行，降低试用门槛或者节约使用者的宝贵时间。运行效果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047333548" alt="nessus-10-auto-installer" title="nessus-10-auto-installer" loading="lazy"/></p><h2>系统要求</h2><p>要求 Ubuntu x86_64 16.04 及更新版本，建议 24.04 或者 22.04（程序有支持 aarch64 的能力，但未发布）。</p><ul><li><a href="https://link.segmentfault.com/?enc=1d8TjoDGQdxk7ZRz%2FXo7Bw%3D%3D.NpPWkHzQhII6oqz9zfumGJH3PTzvtzNC%2Bivl7o3HOZ75UHb%2FSilsu0LkkXNq7%2BCI" rel="nofollow" target="_blank">Ubuntu 24.04 x86_64</a>（<a href="https://link.segmentfault.com/?enc=OTFAysRcWuzefpTKKcJ3ng%3D%3D.AF2%2FoNRRwpt1MW5VMlnz3nk1Viv0sUDlBDkFETSP15J2Vr%2By5CKDTNHm%2FFER%2F7R1" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=qtFa3dJOYfSUjhoaLegIJQ%3D%3D.ctzZDyxkkFXWBnNXFV%2FE6rOO0PguEj4iuzsTXbc5ANtXh0CzoFH9tUsGsw6zEOwt" rel="nofollow" target="_blank">Ubuntu 22.04 x86_64</a>（<a href="https://link.segmentfault.com/?enc=uKQlodLZI3mPk0kXG8uKAg%3D%3D.bl%2FEcwB1v5Ny9ddSJzhyBMLkkTFXJPewL2HBTuzPNBlrGTFAJDmJjpt6XmQbTYRn" rel="nofollow" target="_blank">OVF</a>）</li><li><a href="https://link.segmentfault.com/?enc=EzL5TW%2Fz%2FcEeU6RgGdcEyA%3D%3D.yJ0jY795gXfJMv0Kz1exilrsolTwXJY%2BnylKiQxDh4ufeu1jYSCtqYA2KUeLFR02" rel="nofollow" target="_blank">Ubuntu 20.04 x86_64</a>（<a href="https://link.segmentfault.com/?enc=nESxu0r%2FVwx3SPPDJ0hpQw%3D%3D.%2FWmN7SGBedr94N7H7bId5rTg1aNs%2BtcAiB8%2FwEoo7gd%2BBlvemaFW1e7zjJPpSpJi" rel="nofollow" target="_blank">OVF</a>）</li></ul><p>建议运行在虚拟机环境中，<strong>推荐使用本站原创虚拟机模板 OVF</strong>，简单，精准，高效。</p><h2>下载地址</h2><p><strong>Nessus 10.11 Auto Installer</strong> for Ubuntu 22.04/24.04</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=T7MWeTIvP9EqvzTYKprjDg%3D%3D.uUEVz%2FtrdeXdXtBMcnSeUWEeoXMJvP6xxXL8xvo8VIxR8tehcofXm8u%2FRX1E7W8h6eBGfCHB6YzBQGk0wBx5dA%3D%3D" rel="nofollow" target="_blank">https://sysin.org/blog/nessus-auto-install-for-ubuntu/</a></li></ul><hr/><p>发布 Nessus 试用版自动化安装程序，支持 macOS Tahoe、RHEL 10、Ubuntu 24.04 和 Windows</p><ul><li><a href="https://link.segmentfault.com/?enc=YzFhyqQnCBsp%2BxwhyndJVA%3D%3D.vRbfREEfVQhSTw3zoHBzzCdZPFp%2BPTTh6V74LEVoaHzpGdZbhnfwFqOVECXBpKcKNR2ziJflGkiFdzMeMoJFsg%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for macOS Tahoe - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=WKvEFLEBpCLClUCKcUsCyQ%3D%3D.4yqvdH4wdvzgAbTT4OKhyKZyuMrcyhUEJs4PFwdlJxc8a2yjcFOj%2F5zSQ7TUOWU%2BVAEct8nMyEr7VJIPePpZhA%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for RHEL 10, AlmaLinux 10, Rocky Linux 10 - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=TjoYlaXoTN%2B8VhCzzWpS4g%3D%3D.yV4gxr6uPXg%2F16XXMNbNLe36ZNsQZ0vcaG%2Bb0Gm5AaF7Js82IZ2WN%2BUvFP2vZFmAkFfzIOjqbfuPF2PEfLK1Yw%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for Ubuntu 24.04 - Nessus 自动化安装程序</a></li><li><a href="https://link.segmentfault.com/?enc=T80vIq3dIaq4PxVEanf7gg%3D%3D.BObBAtpExbeT%2FUyL3LuIDtR10%2BTnKJo%2FI%2FCGZ7RREc6hovISU4o7wcxJtYA0POIf7X4rolYor5gRUoqYfsjG2w%3D%3D" rel="nofollow" target="_blank">Nessus Professional 10.11 Auto Installer for Windows - Nessus 自动化安装程序</a></li></ul><p>Nessus 10 系列版本下载</p><ul><li><a href="https://link.segmentfault.com/?enc=pAaAg%2B1b14oZLBKFsSUCWg%3D%3D.OJmNFOTWwqlY9B3HMqG2mQ%2BdsxJjvEAVQV0pu63f53P4y5ei%2Br3bW1tJc9joeqnW" rel="nofollow" target="_blank">Tenable Nessus 10.11 (macOS, Linux, Windows) - 漏洞评估解决方案</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=XIxFeW8vxKjQSLvtxENWvA%3D%3D.yN6%2F4BpSCaHDU4QpXn2IWQc8Qoy4%2FSh5izKm9Euqavo%3D" rel="nofollow" target="_blank">HTTP 协议与安全</a></p>]]></description></item><item>    <title><![CDATA[MCP 爬虫为什么离不开B2Proxy？]]></title>    <link>https://segmentfault.com/a/1190000047437725</link>    <guid>https://segmentfault.com/a/1190000047437725</guid>    <pubDate>2025-11-29 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>对于任何爬虫系统来说，IP 是最基本的身份标记。当一个进程持续从同一出口访问网站，或当大量并发流量集中在同一个 IP 时，系统几乎瞬间就能判断出异常行为。尤其是国际电商网站、社媒平台、内容平台和广告系统，它们的风控模型会基于 IP 地域、ISP 分类、IP 信誉度、频率波动等因素进行判定。<br/>这意味着 MCP 虽然能启动大量进程，但若缺乏合适的代理 IP，就相当于让几十个“工人”共用一扇门进出。速度再快，也很容易被“门卫”拦下。<br/>高质量代理 IP 尤其是住宅 IP，正是解决这个瓶颈的关键。住宅 IP 来自真实家庭宽带，其网络特征天然与普通用户一致，即便高频访问，也不容易被识别为爬虫。对于 MCP 来说，代理 IP 不仅提供更多“出口”，更提供更自然、更可信的网络身份，让每个进程看起来像来自完全不同的用户。</p><h2>代理 IP 如何提升 MCP 的采集效率？</h2><p>最明显的提升来自成功率。当 MCP 在高并发运行时，每个请求都可能触发网站的访问限制。代理 IP 会在出口层面分散压力，将流量分散到不同地区、不同网络类型的 IP 上，使目标网站难以做出统一的封锁。<br/>此外，住宅代理的真实性会让风控系统降低敏感度，从而显著减少验证码、限流、403 访问限制等情况，降低人工干预频率，让采集流程更加自动化。<br/>地理位置也是另一个重要因素。MCP 常常需要同步采集多个国家的数据，例如价格差异、广告投放内容、国际搜索结果等。代理 IP 提供的地理切换能力，使得爬虫可以像“本地用户”一样访问目标站点，为数据模型带来更真实、更本地化的样本。<br/>在某些需要登录的任务中，代理 IP 更能提供独立的账号环境，避免多个进程使用同一出口导致账号关联。这对于跨境电商、社交媒体、广告后台而言尤为关键。</p><h2>为什么许多 MCP 团队更倾向使用 <a href="https://link.segmentfault.com/?enc=ZPq6y9gFprHXgtn6hAwLaQ%3D%3D.nk%2Fj5mIsX19If3tQaTIFg1ee0zQ7b3rSbasHrSUdMJgtJRGQkR%2BOiP6Q1B5AARGj" rel="nofollow" target="_blank">B2Proxy</a>？</h2><p>随着数据采集需求的增长，市场上的代理服务很多，但质量却参差不齐。对于 MCP 这类高并发、高稳定性的系统来说，代理必须具备足够的规模、稳定性和真实可信度。<br/>B2Proxy 的优势正体现在这里。其全球超过8000万条真实住宅 IP，覆盖广、分布自然，适合需要多地域采集的任务。同时，它支持 HTTP、SOCKS5 多协议，可适配各种 MCP 架构、Scrapy、Playwright、Puppeteer、Selenium 等主流框架。<br/>更重要的是，<a href="https://link.segmentfault.com/?enc=96S7yYQF5k57R9OPq9%2BxFw%3D%3D.TQAq8YNsKxpSqpde609B9XSu8LvkgPzTCQ9VJvFUL%2FKW%2BMjA7c6H6MaT7mj9cmHp" rel="nofollow" target="_blank">B2Proxy</a> 的响应速度和节点纯净度在爬虫场景中表现稳定，极少触发风控限制，也不容易产生异常跳转。对于需要长时间稳定运行的采集系统来说，这是持续降低成本、提升成功率的关键能力。</p><h2>结语</h2><p>MCP 爬虫让数据采集的上限不断提升，但代理 IP 才是决定其稳定性与可扩展性的基础设施。两者之间不是可选关系，而是互相依存的关键环节。MCP 提供速度，代理 IP 提供可信度；MCP 扩展规模，代理 IP 扩展身份；MCP 构建体系，代理 IP 构建安全。<br/>在数据竞争越来越激烈的时代，谁能构建更稳定、更高效、更可信的采集系统，谁就能掌握更多商业主动权。而高质量代理，正是这场竞争中不可或缺的核心能力之一。</p>]]></description></item><item>    <title><![CDATA[JK2连接器使用教程：jakarta-t]]></title>    <link>https://segmentfault.com/a/1190000047437676</link>    <guid>https://segmentfault.com/a/1190000047437676</guid>    <pubDate>2025-11-29 10:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><code>jakarta-tomcat-connectors-jk2-src-current.zip</code> 包含的是 Apache 和 Tomcat 之间通信的老式连接器 JK2 的源代码。通过编译和配置，可以让 Apache HTTP Server 把请求转发给后端的 Tomcat 处理</p><ol><li><strong>先解压</strong>  <br/><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=Q%2FNDsPMu2xTIGsV7e2Mv4g%3D%3D.EgHInlQaiTrpMYmQwNtlXeNcQ9EnYsTgQF70FdTRya94LrWTIZTagCndbY21YJ0u" rel="nofollow" title="https://pan.quark.cn/s/95945d4f20cb" target="_blank">https://pan.quark.cn/s/95945d4f20cb</a> ，把你下载的 <code>jakarta-tomcat-connectors-jk2-src-current.zip</code> 解压到一个目录里，比如叫 <code>jk2-src</code>。</li><li><p><strong>装编译工具</strong>  <br/>因为这是源码包，得自己编译。你得有：</p><ul><li>Java SDK（JDK）</li><li>Apache 的开发头文件（比如 <code>httpd-devel</code> 包，Linux 下用 yum 或 apt 装）</li><li>C 编译器（比如 gcc）</li><li>Ant（因为这项目是用 Ant 构建的）</li></ul></li><li><strong>进 native 目录</strong>  <br/>解压后，进 <code>jk2-src/jk/native2/</code> 这个目录，里面是 C 代码，要编译成 Apache 模块。</li><li><p><strong>配置和编译</strong>  <br/>一般运行类似这样的命令（具体看里面的 README）：</p><pre><code>1./configure --with-apxs=/usr/sbin/apxs --with-java-home=/path/to/jdk
2make</code></pre></li></ol><pre><code>成功的话会生成 `mod_jk2.so` 文件。
</code></pre><ol><li><strong>把模块放进 Apache</strong>  <br/>把生成的 <code>mod_jk2.so</code> 复制到 Apache 的 modules 目录下，比如 <code>/etc/httpd/modules/</code>。</li><li><p><strong>配 Apache</strong>  <br/>在 Apache 的配置文件（比如 <code>httpd.conf</code>）里加一行：</p><pre><code>1LoadModule jk2_module modules/mod_jk2.so</code></pre></li></ol><pre><code>然后再写个 `workers2.properties` 文件（通常放 Apache 配置目录下），告诉它怎么连 Tomcat。
</code></pre><ol><li><strong>配 Tomcat</strong>  <br/>Tomcat 那边不用大改，但要确保 AJP 连接器开着（默认端口 8009），在 <code>server.xml</code> 里找 <code>&lt;Connector protocol="AJP/1.3" ... /&gt;</code>，取消注释就行。</li><li><strong>启动测试</strong>  <br/>先启 Tomcat，再启 Apache，然后访问 Apache 的地址，看能不能把请求转给 Tomcat。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[申威架构SW64 KY10系统安装tom]]></title>    <link>https://segmentfault.com/a/1190000047437680</link>    <guid>https://segmentfault.com/a/1190000047437680</guid>    <pubDate>2025-11-29 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​申威是咱们国家自己搞的一种CPU架构，和常见的x86、ARM不一样，所以装软件的时候得用专门给它编译好的版本，这个RPM包就是给KY10系统（也就是基于申威的系统）准备的Tomcat 9，版本号是9.0.10。Tomcat呢，就是一个跑Java Web应用的服务器，很多Java网站或者后台服务都靠它来跑。</p><ol><li><p><strong>先看看有没有装Java环境</strong>​</p><p>Tomcat跑起来得有JDK，你先在终端里敲 <code>java -version</code>看看有没有输出。如果提示找不到命令，就得先装个合适版本的JDK（注意要和申威架构匹配）。</p></li><li><p><strong>把RPM包弄到机器上</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=uJjIwSJ7HXfkDAqlHGFXfg%3D%3D.Tnv27vgJqHr5oEhZI8eeW49WdqoEu8AJDo1mwCoKq4nBvdP7FdWGYUiEL7r%2BbC0p" rel="nofollow" title="https://pan.quark.cn/s/f988d1ce2b96" target="_blank">https://pan.quark.cn/s/f988d1ce2b96</a> ，可以用U盘拷、用scp传，或者本地下载好，放到某个目录，比如 <code>/tmp</code>。</p></li><li><p><strong>用rpm命令安装</strong>​</p><p>打开终端，切换到放RPM包的目录，然后执行：</p><pre><code>sudo rpm -ivh tomcat-9.0.10-25.ky10.sw_64.rpm</code></pre></li></ol><pre><code>这里 `-i`是安装，`-v`显示过程，`-h`显示进度条。如果系统提示缺依赖，就先把缺的东西装上再试。
</code></pre><ol><li><p><strong>确认装好了没</strong>​</p><p>装完后可以看一眼安装的文件，一般在 <code>/usr/local/tomcat</code>（具体看你这个包的定义），也可以用：</p><pre><code>rpm -ql tomcat</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p><p>看看都装到哪儿了。</p></li><li><p><strong>启动Tomcat试试</strong>​</p><p>找到它的启动脚本，比如 <code>bin/startup.sh</code>，在终端里运行：</p><pre><code>cd /usr/local/tomcat/bin
./startup.sh</code></pre></li></ol><pre><code>看到类似 “Tomcat started” 的提示就说明起来了。
</code></pre><ol><li><p><strong>访问测试</strong>​</p><p>默认端口一般是8080，浏览器里输入 <code>http://服务器IP:8080</code>，能看到Tomcat欢迎页就OK了。</p></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[一.机器学习基础与特征工程 leafgo]]></title>    <link>https://segmentfault.com/a/1190000047437521</link>    <guid>https://segmentfault.com/a/1190000047437521</guid>    <pubDate>2025-11-29 01:02:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1.机器学习基础</h2><h3>1.1 数学基础</h3><p>需要的数学知识：<br/>高等数学、线性代数、概率与统计。</p><p>当然一开始不用深入进去，可以在学习过程中逐步积累。</p><h3>1.2 编程语言</h3><p>人工智能领域很火的领域的自然是Python，门槛也低，可以作为机器学习入门的首选语言。<br/>有精力的话，再学习C/C++，多一门语言傍身不是坏事。</p><h3>1.3 机器学习的开发流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000039750688" alt="在这里插入图片描述" title="在这里插入图片描述"/><br/>1）从公司原有的数据库或者爬虫等途径获得原始数据<br/>我们所拿到的数据，大致分为两种类型：离散型数据和连续型数据。<br/>2）对原始数据进行数据处理，python中可以使用pandas库。<br/>pandas 库 参考文档 <a href="https://link.segmentfault.com/?enc=1mpHwSM2T%2B7pYieZDiHaYA%3D%3D.WPlvuZDVYQiid1pS7cCvHIisYeLEioH%2BB4nfmYUt7AnCEXSgz3Je9kKC4zub1caR" rel="nofollow" target="_blank">https://pandas.pydata.org/pandas-docs/stable/</a><br/>3）特征处理：非常重要的一步，直接影响模型的预测结果<br/>4）然后选择合适的算法，构建模式<br/>5）对模型进行评估，是否符合预期，如何不理想，需要重新选择算法，或者对数据重做特征处理<br/>6）评估OK后，模型直接应用</p><h2>2.scikit-learn库</h2><blockquote>scikit-learn 是基于 Python 语言的机器学习工具:<br/>简单高效的数据挖掘和数据分析工具<br/>可供大家在各种环境中重复使用<br/>建立在 NumPy ，SciPy 和 matplotlib 上<br/>开源，可商业使用 - BSD许可证</blockquote><p>通过这个库来逐步揭开机器学习的面纱.</p><h3>2.1 特征工程</h3><h4>2.1.1 特征抽取</h4><p>scikit-learn库提供了特征抽取的API：sklearn.feature_extraction</p><ul><li>字典特征抽取 sklearn.feature_extraction.DictVectorizer</li></ul><pre><code class="python">from sklearn.feature_extraction import DictVectorizer

def dict_ex():
    """
    字典数据抽取
    :return:
    """
    list_demo = [{'city': '美国', 'new_add': 24500},
                 {'city': '俄罗斯', 'new_add': 4000},
                 {'city': '英国', 'new_add': 5600}]

    dict_ins = DictVectorizer(sparse=False) 

    data = dict_ins.fit_transform(list_demo)

    print(dict_ins.get_feature_names()) 
    print(data.astype('int')) 

if __name__ == '__main__':
    dict_ex()</code></pre><p>结果</p><pre><code class="bash">['city=俄罗斯', 'city=美国', 'city=英国', 'new_add']
[[    0     1     0 24500]
 [    1     0     0  4000]
 [    0     0     1  5600]]</code></pre><ul><li>文本特征抽取 sklearn.feature_extraction.text.CountVectorizer<br/>限制英文，可以使用分词工具</li></ul><pre><code class="python">from sklearn.feature_extraction.text import CountVectorizer

def text_ex():
    """
    文本特征抽取
    :return:
    """
    text_demo = ["""
        Beautiful is better than ugly.
        Explicit is better than implicit.
        Simple is better than complex.
        Complex is better than complicated.
        Flat is better than nested.
        Sparse is better than dense.
        Readability counts.
        Special cases aren't special enough to break the rules.
        Although practicality beats purity.
        Errors should never pass silently.
        Unless explicitly silenced.
        In the face of ambiguity, refuse the temptation to guess.
        There should be one-- and preferably only one --obvious way to do it.
        Although that way may not be obvious at first unless you're Dutch.
        Now is better than never.
        Although never is often better than *right* now.
        If the implementation is hard to explain, it's a bad idea.
        If the implementation is easy to explain, it may be a good idea.
        Namespaces are one honking great idea -- let's do more of those!
    """]

    cv = CountVectorizer()
    data = cv.fit_transform(text_demo)
    print(cv.get_feature_names())
    print(data) # 默认为sparse格式
    print(data.toarray()) # 转化为数组
  
if __name__ == '__main__':
    text_ex()  </code></pre><p>结果</p><pre><code class="bash">['although', 'ambiguity', 'and', 'are', 'aren', 'at', 'bad', 'be', 'beats', 'beautiful', 'better', 'break', 'cases', 'complex', 'complicated', 'counts', 'dense', 'do', 'dutch', 'easy', 'enough', 'errors', 'explain', 'explicit', 'explicitly', 'face', 'first', 'flat', 'good', 'great', 'guess', 'hard', 'honking', 'idea', 'if', 'implementation', 'implicit', 'in', 'is', 'it', 'let', 'may', 'more', 'namespaces', 'nested', 'never', 'not', 'now', 'obvious', 'of', 'often', 'one', 'only', 'pass', 'practicality', 'preferably', 'purity', 're', 'readability', 'refuse', 'right', 'rules', 'should', 'silenced', 'silently', 'simple', 'sparse', 'special', 'temptation', 'than', 'that', 'the', 'there', 'those', 'to', 'ugly', 'unless', 'way', 'you']
  (0, 9)    1
  (0, 38)    10
  (0, 10)    8
  :    :
  (0, 40)    1
  (0, 42)    1
  (0, 73)    1
[[ 3  1  1  1  1  1  1  3  1  1  8  1  1  2  1  1  1  2  1  1  1  1  2  1
   1  1  1  1  1  1  1  1  1  3  2  2  1  1 10  3  1  2  1  1  1  3  1  2
   2  2  1  3  1  1  1  1  1  1  1  1  1  1  2  1  1  1  1  2  1  8  1  5
   1  1  5  1  2  2  1]]
</code></pre><p>汉字的特征提取示例</p><pre><code class="python">from sklearn.feature_extraction.text import CountVectorizer
import jieba

def zh_exc():
    """
    中文特征值
    :return:
    """
    text_demo1 = """
    　我与父亲不相见已二年余了，我最不能忘记的是他的背影。那年冬天，祖母死了，父亲的差使也交卸了，
    正是祸不单行的日子，我从北京到徐州，打算跟着父亲奔丧回家。到徐州见着父亲，看见满院狼藉的东西，
    又想起祖母，不禁簌簌地流下眼泪。父亲说，“事已如此，不必难过，好在天无绝人之路！
    """
    text_demo2 = """
    沿着荷塘，是一条曲折的小煤屑路。这是一条幽僻的路；白天也少人走，夜晚更加寂寞。荷塘四面，长着许多树，
    蓊蓊郁郁的。路的一旁，是些杨柳，和一些不知道名字的树。没有月光的晚上，这路上阴森森的，有些怕人。
    今晚却很好，虽然月光也还是淡淡的。
    """

    c1 = jieba.cut(text_demo1)
    c2 = jieba.cut(text_demo2)

    # 转换成字符串
    text1 = ' '.join(list(c1))
    text2 = ' '.join(list(c2))

    cv = CountVectorizer()
    data = cv.fit_transform([text1, text2])
    print(cv.get_feature_names())
    print(data.toarray())
 
 if __name__ == '__main__':
    zh_exc()</code></pre><p>结果</p><pre><code class="bash">['一些', '一旁', '一条', '不必', '不禁', '不能', '东西', '事已如此', '二年', '交卸', '今晚', '冬天', '北京', '名字', '四面', '回家', '夜晚', '天无绝人之路', '奔丧', '寂寞', '少人', '差使', '幽僻', '徐州', '忘记', '怕人', '想起', '打算', '日子', '晚上', '曲折', '更加', '月光', '有些', '杨柳', '正是', '没有', '沿着', '流下', '淡淡的', '满院', '煤屑', '父亲', '狼藉', '白天', '相见', '看见', '眼泪', '知道', '祖母', '祸不单行', '簌簌', '背影', '荷塘', '蓊蓊郁郁', '虽然', '许多', '跟着', '路上', '还是', '这是', '那年', '长着', '阴森森', '难过']
[[0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 0 2 1 0 1 1 1 0 0 0 0 0 0 1
  0 0 1 0 1 0 5 1 0 1 1 1 0 2 1 1 1 0 0 0 0 1 0 0 0 1 0 0 1]
 [1 1 2 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 2 1 1 0
  1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 2 1 1 1 0 1 1 1 0 1 1 0]]</code></pre><ul><li>TF-IDF 核心内容就是评估一字词的重要程度<br/>对应的API  sklearn.feature_extraction.text.TfidfVectorizer</li></ul><pre><code class="python">from sklearn.feature_extraction.text import TfidfVectorizer
import jieba


def tfidf_ext():
    text_demo1 = """
        　我与父亲不相见已二年余了，我最不能忘记的是他的背影。那年冬天，祖母死了，父亲的差使也交卸了，
        正是祸不单行的日子，我从北京到徐州，打算跟着父亲奔丧回家。到徐州见着父亲，看见满院狼藉的东西，
        又想起祖母，不禁簌簌地流下眼泪。父亲说，“事已如此，不必难过，好在天无绝人之路！
        """
    text_demo2 = """
        沿着荷塘，是一条曲折的小煤屑路。这是一条幽僻的路；白天也少人走，夜晚更加寂寞。荷塘四面，长着许多树，
        蓊蓊郁郁的。路的一旁，是些杨柳，和一些不知道名字的树。没有月光的晚上，这路上阴森森的，有些怕人。
        今晚却很好，虽然月光也还是淡淡的。
        """

    c1 = jieba.cut(text_demo1)
    c2 = jieba.cut(text_demo2)

    # 转换成字符串
    text1 = ' '.join(list(c1))
    text2 = ' '.join(list(c2))

    tf = TfidfVectorizer()
    data = tf.fit_transform([text1, text2])
    print(tf.get_feature_names())
    print(data.toarray())
   
 if __name__ == '__main__':
    tfidf_ext()</code></pre><p>结果</p><pre><code class="bash">['一些', '一旁', '一条', '不必', '不禁', '不能', '东西', '事已如此', '二年', '交卸', '今晚', '冬天', '北京', '名字', '四面', '回家', '夜晚', '天无绝人之路', '奔丧', '寂寞', '少人', '差使', '幽僻', '徐州', '忘记', '怕人', '想起', '打算', '日子', '晚上', '曲折', '更加', '月光', '有些', '杨柳', '正是', '没有', '沿着', '流下', '淡淡的', '满院', '煤屑', '父亲', '狼藉', '白天', '相见', '看见', '眼泪', '知道', '祖母', '祸不单行', '簌簌', '背影', '荷塘', '蓊蓊郁郁', '虽然', '许多', '跟着', '路上', '还是', '这是', '那年', '长着', '阴森森', '难过']
[[0.         0.         0.         0.12598816 0.12598816 0.12598816
  0.12598816 0.12598816 0.12598816 0.12598816 0.         0.12598816
  0.12598816 0.         0.         0.12598816 0.         0.12598816
  0.12598816 0.         0.         0.12598816 0.         0.25197632
  0.12598816 0.         0.12598816 0.12598816 0.12598816 0.
  0.         0.         0.         0.         0.         0.12598816
  0.         0.         0.12598816 0.         0.12598816 0.
  0.62994079 0.12598816 0.         0.12598816 0.12598816 0.12598816
  0.         0.25197632 0.12598816 0.12598816 0.12598816 0.
  0.         0.         0.         0.12598816 0.         0.
  0.         0.12598816 0.         0.         0.12598816]
 [0.15617376 0.15617376 0.31234752 0.         0.         0.
  0.         0.         0.         0.         0.15617376 0.
  0.         0.15617376 0.15617376 0.         0.15617376 0.
  0.         0.15617376 0.15617376 0.         0.15617376 0.
  0.         0.15617376 0.         0.         0.         0.15617376
  0.15617376 0.15617376 0.31234752 0.15617376 0.15617376 0.
  0.15617376 0.15617376 0.         0.15617376 0.         0.15617376
  0.         0.         0.15617376 0.         0.         0.
  0.15617376 0.         0.         0.         0.         0.31234752
  0.15617376 0.15617376 0.15617376 0.         0.15617376 0.15617376
  0.15617376 0.         0.15617376 0.15617376 0.        ]]</code></pre><h4>2.1.2 特征处理</h4><p>什么是特征处理？就是通过特定的数学方法将数据转换成我们算法要求的数据。</p><ul><li>归一化<br/>min-max 归一化的公式为：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750714" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>mean 归一化的公式为：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750715" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>​​其中 mean(x)、min(x) 和 max(x) 分别是样本数据的平均值、最小值和最大值。<br/>再求X``</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000039750713" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>其中mx，mi分别为指定区间[mi, mx]，一般mx为1,mi为0。</p><p>归一化的目的：通过对原始数据进行变换把数据映射到指定区间之间，一般是[0,1]，那就是省去求X``, 这步可以省略。</p><p>sklearn库提供了归一化API：sklearn.preprocessing.MinMaxScaler<br/>代码示例：</p><pre><code class="python">from sklearn.preprocessing import MinMaxScaler

def mm_ex():
    """
    归一化示例
    :return:
    """
    test_dict = [[99,1,18,1002],
                [88,4,18,1400],
                [89, 4,25,1201],
                [97,2,19,2800]]
    mm = MinMaxScaler()
    data = mm.fit_transform(test_dict)
    print(data)
    
if __name__ == '__main__':
    mm_ex()</code></pre><p>结果</p><pre><code class="bash">[[1.         0.         0.         0.        ]
 [0.         1.         0.         0.22135706]
 [0.09090909 1.         1.         0.11067853]
 [0.81818182 0.33333333 0.14285714 1.        ]]
</code></pre><p>归一化的结果因最大值和最小值而变化，所以容易特异点数据影响，如</p><pre><code>    test_dict = [[99,1,18,200000],
                [88,4,18,1400],
                [89, 4,25,1201],
                [97,2,19,2800]]</code></pre><p>200000这个数值就对结果有很大影响。</p><ul><li>标准化<br/>公式<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750712" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>u为平均值，σ为标准差</li></ul><p>sklearn提供了标准化API:  scikit-learn.preprocessing.StandardScaler<br/>代码示例</p><pre><code class="python">from sklearn.preprocessing import StandardScaler

def standard_ex():
    """
    标准化API示例
    :return:
    """
    test_dict = [[99, 1, 18, 1002],
                 [88, 4, 18, 1400],
                 [89, 4, 25, 1201],
                 [97, 2, 19, 2800]]
    
    ss = StandardScaler()
    data = ss.fit_transform(test_dict)
    print(data)


if __name__ == '__main__':
    standard_ex()</code></pre><p>结果</p><pre><code class="bash">[[ 1.1941005  -1.34715063 -0.68599434 -0.84743801]
 [-1.09026568  0.96225045 -0.68599434 -0.28413057]
 [-0.88259602  0.96225045  1.71498585 -0.56578429]
 [ 0.7787612  -0.57735027 -0.34299717  1.69735287]]</code></pre>]]></description></item><item>    <title><![CDATA[二.机器学习算法篇-线性回归(1) le]]></title>    <link>https://segmentfault.com/a/1190000047437527</link>    <guid>https://segmentfault.com/a/1190000047437527</guid>    <pubDate>2025-11-29 01:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1.线性回归算法思想</h2><p>机器学习算法可以分为有监督学习和无监督学习。</p><p>什么是有监督学习算法？<br/>用已知某种或某些特性的样本作为训练集,以建立一个数学模型,再用已建立的模型来预测未知样本,此种方法被称为有监督学习,是最常用的一种机器学习方法。是从标签化训练数据集中推断出模型的机器学习任务。</p><p>回归算法是有监督学习算法的一种，从机器学习的角度来讲,回归算法用于构建一个算法模型，这个模型是属性(X)与标签(Y)之间的映射关系。</p><p>线性回归通过一个或者多个自变量与因变量之间之间进行建模的回归分析。<br/>它的特点为一个或多个称为回归系数的模型参数的线性组合。</p><h2>2.线性回归示例</h2><table><thead><tr><th>房屋面积(m^2)</th><th>房租(元)</th></tr></thead><tbody><tr><td>10</td><td>800</td></tr><tr><td>15 5</td><td>1200</td></tr><tr><td>20 2</td><td>1600</td></tr><tr><td>35.0</td><td>2500</td></tr><tr><td>48 3</td><td>3300</td></tr><tr><td>58.9</td><td>3800</td></tr><tr><td>65.2</td><td>4500</td></tr></tbody></table><p>将上面的数据看做一行样本，我们可以得到如下关系</p><pre><code>     x(房屋面积)       y(房租)
0   10                          800
1   15.5                        1200
...
5  65.2                        4500</code></pre><p>如下图<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750786" alt="在这里插入图片描述" title="在这里插入图片描述"/>根据上面这些数据，我们预测房屋面积80平米的房租会是多少呢？<br/>首先我们要找到这样房屋面积与价格的映射关系， y=ka+b， 如下图<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750788" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>然后通过y=f(x)映射关系，预测房租价格。</p><p>这是一个特征值的，那么如果是两个特征值呢？我们找的就是一个平面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000039750787" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>扩展到更多个特征值，我们要找的映射关系就是<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750782" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>​x就是特征值，<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750781" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>我们可以看做θ。*  x。， x。为1，然后j得到这个式子<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750784" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>用向量来表示上面这个式子<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750785" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>最终我们得到<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750780" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>3.误差</h2><p>我们得到这个模型，但显然预测值与真实值存在误差，用 ε来表示误差。<br/>对于每个样本则有<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750779" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>由概率论的中心极限定理，可知误差ε是独立并且具有相同的分布,并且服从均值为0方差为σ²的高斯分布。</p><p>所以<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750783" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>将(1)式带入(2)式<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750789" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>之后用到了似然函数：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750790" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>为了便于求解，取对数<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750792" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750791" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750793" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>当<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750796" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>最小时, 也就是为0时， logL(θ)值最大，这个式子也就是我们的损失函数。<br/>进一步变换：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750798" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>然后求它的偏导<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750795" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>令偏导为0，最终可以求得<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750797" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>这个就是最小二乘法，也是线性回归损失函数的求解方法之一。</p><p>对于上面的房屋面积与房租关系样本的代码示例</p><pre><code class="python">import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LinearRegression as lr


# 房屋面积数据
x_list = [10, 15.5, 20.2, 35.0, 48.3, 58.9, 65.2]
# 对应的房租数据
y_list = [800, 1200, 1600, 2500, 3300, 3800, 4500]

x = np.array(x_list).reshape(-1,1)
y = np.array(y_list).reshape(-1,1)

model = lr()
model.fit(x, y)

y_plot = model.predict(x)

print(model.coef_)

plt.figure(figsize=(5,5),dpi=80, facecolor='w')

plt.scatter(x, y, color='red', linewidths=2,)
plt.plot(x, y_plot, color='blue',)

x_tick = list(range(5, 70, 5))

plt.grid(alpha=0.4)

plt.xticks(x_tick)

plt.show()
</code></pre><p>结果</p><pre><code class="bash">[[63.66780288]]</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000039750794" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.波士顿房价预测示例</h3><p>从sklearn.datasets中获取相关数据集，使用标准线性回归，建立房价预测模型，并绘制房价预测值和真实房价的散点、折线图。<br/>代码示例</p><pre><code class="python"># coding:utf-8
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression as lr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from matplotlib import pyplot as plt
from matplotlib import font_manager

font = font_manager.FontProperties(fname="/usr/share/fonts/wps-office/msyhbd.ttf")

def my_predic_fun():
    """
    使用线性回归预测波士顿预测房价
    :return:
    """
    lb = load_boston()

    x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=0.2)

    x_std = StandardScaler()
    y_std = StandardScaler()

    x_train = x_std.fit_transform(x_train)
    x_test = x_std.transform(x_test)
    y_train = y_std.fit_transform(y_train.reshape(-1,1))
    y_test = y_std.transform(y_test.reshape(-1,1))


    model = lr()
    model.fit(x_train, y_train)

    y_predict = y_std.inverse_transform(model.predict(x_test))
    return y_predict, y_std.inverse_transform(y_test)


def draw_fun(y_predict, y_test):
    """
    绘制房价预测与真实值的散点和折线图
    :param y_predict:
    :param y_test:
    :return:
    """
    x = range(1,len(y_predict)+1)
    plt.figure(figsize=(20, 8), dpi=80)
    plt.scatter(x, y_test, label="真实值",color='blue')
    plt.scatter(x, y_predict,label='预测值', color='red')
    plt.plot(x,y_test)
    plt.plot(x,y_predict)

    x_tick = list(x)
    y_tick = list(range(0,60,5))

    plt.legend(prop=font, loc='best')
    plt.xticks(list(x), x_tick)
    plt.yticks(y_tick)
    plt.grid(alpha=0.4)
    plt.show()


if __name__ == '__main__':
    y_predict, y_test = my_predic_fun()
    draw_fun(y_predict, y_test)</code></pre><p>结果<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000039750799" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>参考：<a href="https://link.segmentfault.com/?enc=tiRaafA78p7C9k45Xw%2FG%2Bw%3D%3D.D69NLjyqm7wbEfCCSMoxq4C3Sv1zxsAtl2np0HExNdtjBM2ouom3Kg11zsmBqHH5tcUfWZQUT5p27WOnLzR9WA%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/guoyunfei20/article/details/78053999</a></p>]]></description></item><item>    <title><![CDATA[打造个人知识大脑：访答知识库深度体验 文]]></title>    <link>https://segmentfault.com/a/1190000047437428</link>    <guid>https://segmentfault.com/a/1190000047437428</guid>    <pubDate>2025-11-29 00:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>打造个人知识大脑：访答知识库深度体验</h2><h3>为什么需要私有知识库？</h3><p>在信息爆炸的时代，我们每天接触大量碎片化信息。这些散落在各处的知识就像未经整理的书籍，急需一个专属空间进行归档。私有知识库的出现，正是为了解决这一痛点——它让你成为自己知识宇宙的主宰。</p><h3>访答知识库：你的第二大脑</h3><p>在众多知识库工具中，知识库以其独特的本地化设计脱颖而出。它将复杂的知识管理变得像整理书房一样直观：创建笔记、建立关联、快速检索，整个过程行云流水。</p><h3>三大核心优势解析</h3><p><strong>安全私密</strong>：所有数据存储在本地，无需担心隐私泄露  <br/><strong>高效管理</strong>：智能标签和跨文档链接，让知识形成有机网络  <br/><strong>随时访问</strong>：无需网络连接，你的知识库永远在线</p><p>把访答知识库想象成你的外接大脑，它不会遗忘，随时待命。当灵感来临或需要查询时，这个忠实的伙伴总能给出精准回应。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncNV" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[因此当商业模 苦闷的键盘 ]]></title>    <link>https://segmentfault.com/a/1190000047437456</link>    <guid>https://segmentfault.com/a/1190000047437456</guid>    <pubDate>2025-11-29 00:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从生成式 AI 的惊艳亮相引起全球科技巨头军备竞赛般的投入开始，整个 AI 行业仿佛被注入了无限的想象力。似乎在宣告着即将出现一个生产力即将被彻底解放、商业模式即将被完全颠覆的光明未来。<br/>微软、谷歌、亚马逊等云巨头纷纷将资本支出的绝大部分押注于 AI 基础设施建设，而无数逐利而来的 AI 初创公司，更是如雨后春笋般涌现试图分一杯羹，全球 AI 领域的投资额也达到了史无前例的高度。<br/>然而，正如任何过热的淘金热最终都会迎来冷静期当技术以超乎预期的速度普及时，潜在的负面效应也以同样的速度被放大，正在悄然侵蚀着行业参与者。<br/>从 " 可选项 " 到 " 必选项 " 的巨额支出<br/>根据奇安信集团对外发布《2024 人工智能安全报告》来看，在 2023 年基于 AI 的深度伪造欺诈便已暴增了 3000%，基于 AI 的钓鱼邮件也增长了 1000%；而内容生成环节更是实现规模化生产。<br/>基于 Stable Diffusion 和 GPT-4 的定制模型，可每小时生成 2000 条伪原创研报、800 段逼真视频。暗网平台 "DarkGPT" 更是提供包月服务，1 万美元即可获得每日 5000 条金融虚假内容的产能。<br/>而且 "AI 滥用 " 的后遗症并不仅仅在社会新闻版块，可以说它已经穿透了科技公司的防火墙直接作用于其财务报表。而金融行业正是这场风暴的中心，当 AI 以假乱真的能力被精准地应用于金融诈骗时，其破坏力可以说是指数级的增长。<br/>据行业估算，2024 年由深度伪造技术引发的各类欺诈造成的全球经济损失已高达 120 亿美元。尤其在监管相对滞后、交易更为匿名的加密货币领域，AI 滥用更是如鱼得水。根据相关的报告也显示 2024 年仅 AI 深度伪造技术全年造成的损失便高达 46 亿美元。<br/>随着 AI 滥用事件的频发，过去模糊的 " 伦理指南 " 正在迅速转变为具有强制约束力的法律条文，而且这种转变直接导致了企业合规成本的急剧攀升。<br/>而且一旦出现违规需要付出的代价更是惨痛的，罚款最高可达全球年营业额的数个百分点或数千万欧元，而且合规也不再是法务部门的单一工作，而是渗透到研发、产品、市场的每一个环节。<br/>这些 " 反噬 " 也并非凭空产生，在 AI 商业化过程中对速度和规模的追求，长期以来压倒了对安全和伦理的考量所以形成了这种 " 原罪 "。因此未来合规成本的升高是不可避免的，而欧盟的《AI 法案》可以说是这一趋势的先行者。<br/>该法案于 2024 年 8 月 1 日正式生效并分阶段实施，着重对高风险的 AI 系统施加了严格的合规要求。而且这不仅仅是一项区域性法规，更可能产生 " 布鲁塞尔效应 " 从而影响全球的 AI 监管格局。<br/>监管的落地也将会直接转化为企业的合规成本。据公开信息推算，仅欧盟 AI 法案便可能导致欧洲企业的 AI 采纳成本增加约 310 亿欧元，并使 AI 投资减少近 20%。而美国联邦贸易委员会也已对 OpenAI 展开调查，谷歌等公司也不得不调整其营销话术，避免被处以巨额罚款。<br/>可以说 " 监管的铁幕 " 正在迫使整个行业从过去 " 快速行动，打破陈规 " 的互联网思维转向一种更为审慎、合规驱动的开发模式。可以说这种转变无疑会减缓创新速度并增加运营成本，对于那些资源有限的中小企业和初创公司构成尤为严峻的挑战。<br/>对 " 信任 " 的侵蚀或许是 AI 滥用最难修复的一种<br/>这源于在激烈的竞争压力下，企业急于抢占市场将产品快速推向用户，所以将风险控制和安全测试置于次要位置。但是这种 " 快速行动并打破规则 " 的心态在 AI 时代尤为危险，因为 AI 技术的潜在破坏力远超以往的软件应用。<br/>并且市场对于 AI 技术的可靠性极度敏感，甚至一次小小的失误都可能引发巨大的信任危机和财务损失。谷歌的 Bard 模型之前便在一次演示中出现事实性错误，竟然导致其母公司 Alphabet 的股价在单日内暴跌 7%，市值蒸发超过 1000 亿美元。<br/>并且随着 AI 投资的巨额支出持续攀升，投资者开始担忧其回报前景，这种悲观情绪导致 Meta、Microsoft、Alphabet 和 Nvidia 等 AI 领域的领军企业股价普遍承压下跌，市场也开始讨论 "AI 泡沫 " 的风险，并开始质疑哪些不计成本的 " 军备竞赛 " 式投资。<br/>更何况大量公司缺乏对 AI 伦理的明确责任归属，高管层面也并未对其有所调整。所以 AI 系统的决策过程像一个 " 黑箱 "，在责任主体模糊的情况下滥用和误用的风险便难以控制。企业内部也未建立有效的问责机制。<br/>但是更深层次的原因在于当前主流生成式 AI 商业模式本身所内含的风险。这些模型依赖于海量数据的投喂，其训练过程难以完全避免偏见和有害信息的吸收。而其强大的生成能力却为恶意利用提供了温床。<br/>因此当商业模式的核心是追求更强大的模型、更广泛的应用时，如果缺乏与之匹配的强大 " 安全刹车 " 系统，滥用就成了可预见的副产品。这种商业逻辑与伦理要求之间的结构性失衡才是导致 " 反噬 " 的根本内因。<br/>所以当企业享受了技术红利带来的增长，如今便也不得不为其模式所伴生的风险 " 买单 "。哪怕科技公司以 " 让世界更美好 " 的叙事推广 AI，公众在实际体验中，也会频繁受到隐私泄露、算法偏见、就业替代、虚假信息等负面影响。<br/>这种落差也导致了广泛的 "AI 焦虑 " 和不信任感。公众普遍认为现有的监管法规不足以应对 AI 带来的社会风险期望政府采取更加果断的行动。这种强大的民意压力也是推动监管机构加速行动的根本动力。<br/>面对公众的呼声和潜在的社会风险，监管机构的介入是必然的。但由于技术发展的速度远超立法速度监管往往表现出一定的滞后性，欧盟 AI 法案便被部分人士认为可能增加企业负担、抑制创新。<br/>全球主要经济体在 AI 领域的竞争，也使得监管变得更加复杂。各国都希望在鼓励创新和防范风险之间找到平衡点但这种平衡点的位置各不相同，因此形成了复杂的国际监管格局给跨国企业的合规带来了巨大挑战。<br/>而且这种外部滥用对整个 AI 行业的声誉造成了 " 连坐 " 效应。即使一家公司本身恪守伦理，也无法完全独善其身，因为公众对 AI 的信任是整体性的。恶意滥用行为如同向池塘中投下的毒药，在污染了整个水域后迫使所有 " 池中生物 " 共同承担后果。<br/>这场危机成为 AI 自我革新的契机<br/>这场 " 反噬 " 带来的阵痛，是 AI 产业从野蛮生长走向规范发展的必经阶段。它正在淘汰那些只想赚快钱、缺乏责任感的 " 玩家 "，筛选出真正有实力、有远见的长期主义者。从长远来看，这也是为 AI 产业的健康、可持续发展所必须付出的代价。<br/>其中最大的机遇在于将 " 信任 " 从一种道德呼吁，转变为一种可量化、可变现的商业资产和竞争壁垒。数据显示近 85% 的客户也更愿意与重视 AI 伦理实践的公司合作，而那些优先考虑伦理和透明度的公司收入增长也更快。<br/>可以说在 AI 产品同质化日益严重的未来，谁能赢得用户的信任谁就能赢得市场。" 负责任的 AI" 将不再仅仅是公关部门的口号，而是必须贯穿于产品设计、开发、部署全流程的核心战略。<br/>谷歌和微软等公司已经开始调整其策略，谷歌选择利用 AI 技术提升广告安全审核的效率，打击欺诈内容；微软则发布了负责任 AI 透明度报告，并推出了 AzureAIContentSafety 等服务，帮助客户构建更安全的 AI 应用。这些举措既是应对风险的防御，也是在构建新的竞争优势。<br/>正是 " 反噬 " 催生了全新的 " 安全即服务 " 市场。随着 AI 滥用风险的加剧企业对 AI 安全审计、风险评估、内容过滤、合规咨询等服务的需求将急剧增长。这为专门从事 AI 安全和伦理治理的科技公司、咨询机构创造了巨大的市场空间。<br/>而科技weibo.com/ttarticle/p/show?id=2309405238081853980981<br/>weibo.com/ttarticle/p/show?id=2309405238082340519957<br/>weibo.com/ttarticle/p/show?id=2309405238082680259138<br/>weibo.com/ttarticle/p/show?id=2309405238083024454029<br/>weibo.com/ttarticle/p/show?id=2309405238083368124710<br/>weibo.com/ttarticle/p/show?id=2309405238087684325623<br/>weibo.com/ttarticle/p/show?id=2309405238088032452744<br/>weibo.com/ttarticle/p/show?id=2309405238089986736563</p><p>巨头自身也可以将其内部成熟的安全工具和能力平台化、服务化，开拓新的收入来源。例如，谷歌和微软在内容审核、风险识别方面的技术积累，完全可以转化为对外输出的商业服务。<br/>虽然监管的收紧虽然带来了成本，但也为行业设定了 " 准入标准 "，能够率先满足高标准合规要求的企业将获得更强的市场公信力和竞争优势，从而在未来的市场整合中占据有利地位。这实际上是一种由监管驱动的市场出清和格局优化。<br/>从滥用事件的激增，到资本市场的审慎，再到全球监管的收紧，这股 " 反噬 " 之力正在重塑 AI 产业的发展轨迹。它迫使整个行业从过去对技术力量的无限崇拜，转向对技术责任和社会价值的深刻反思。<br/>麦肯锡预测，到 2030 年 AI 将为全球经济创造 13 万亿美元价值。但价值分配取决于我们如何驾驭这头猛兽。未来的竞争，将不仅仅是模型参数和算力大小的竞争，更是治理能力、责任担当和用户信任的竞争。</p>]]></description></item><item>    <title><![CDATA[中断不控，延迟难稳：低延迟系统的终极痛点]]></title>    <link>https://segmentfault.com/a/1190000047437462</link>    <guid>https://segmentfault.com/a/1190000047437462</guid>    <pubDate>2025-11-29 00:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、引言</h2><p>在金融交易、工业控制、高性能网络等要求微秒级时延的系统中，“中断”常常是影响尾延迟（Tail Latency）的关键因素之一。<br/>在普通系统里，中断带来的几十微秒抖动并不算问题，但在低延迟系统中，即使是 几微秒的不可控波动，都可能导致性能抖动、订单错失。<br/>本文将从原理、影响、排查等维度，系统性介绍中断对低延迟系统的影响。</p><h2>二、低延迟系统的特点</h2><p>低延迟系统的目标是在极短时间内完成处理，并保持响应时间高度稳定。这类系统通常有以下共性，使得它们对中断格外敏感：</p><h3>1. 强调确定性</h3><p>低延迟系统不仅追求“快”，更追求“稳定”。它希望每一次响应时间都尽量一致，不出现无规律的波动。而中断的触发往往具有不确定性，会破坏这种稳定性。</p><h3>2. 核心资源为业务专用</h3><p>为了减少干扰，低延迟系统通常会预留部分 CPU 只运行核心业务逻辑。   <br/>任何外来的系统任务、中断或调度行为进入这些核心，都会破坏业务线程的专注度，引发延迟尖刺。</p><h3>3. 对尾延迟特别敏感</h3><p>很多系统关注平均延迟，但低延迟系统还关注最差情况（例如 99.9%、99.99% 延迟）。     <br/>因此哪怕偶尔一次中断引发几十微秒的延迟，也会被放大成系统性能问题。</p><h3>4. 对干扰高度敏感</h3><p>低延迟系统通常关闭节能、规避抢占、避免内核后台任务等。     <br/>在这种极限优化状态下，即使是非常小的系统活动，例如一次定时器中断，也可能打断业务流程，从而产生可观的抖动。</p><h2>三、中断如何影响低延迟系统？</h2><p>中断在低延迟系统中的影响主要体现在三个层面：</p><h3>1. 抢占执行、打断业务线程</h3><p>中断会中止正在运行的业务代码，引起：</p><ul><li>CPU L1/L2 缓存污染</li><li>pipeline flush</li><li>TLB miss 增加</li><li>业务线程指令乱序恢复</li></ul><p>对于微秒级任务，这种打断是“毁灭性”的。</p><h3>2. 导致不可预测的时延尖刺</h3><p>中断不连续、不规则，例如：</p><ul><li>某个 IRQ 在负载高时突然 flood</li><li>某个 housekeeping 定时器每几秒被触发</li><li>某个软中断在 backlog 到阈值后被 forced 执行</li></ul><p>这些都会导致业务端出现 几微秒到几十微秒的波动。</p><h3>3. 软中断（SoftIRQ）更隐蔽</h3><p>很多系统管理员只关注“硬中断”，但真正杀伤力高的是：</p><ul><li>NET_RX</li><li>NET_TX</li><li>TIMER</li><li>TASKLET</li><li>RCU</li></ul><p>它们会在本 CPU 上被强制执行，且执行时长不可控，极容易破坏延迟稳定性。</p><h2>四、如何排查中断对延迟的影响？</h2><p>在低延迟系统中，中断是否落到了业务核心、是否存在异常增长，是判断延迟抖动来源的关键依据。   <br/>传统方式通常是直接 cat /proc/interrupts，但它不具备实时性，也难以观察变化趋势。   <br/>因此，使用自研的 irq_watch.py 脚本，实现实时监控中断变化，帮助快速定位问题。</p><h3>1. irq_watch.py 的功能概述</h3><p>该脚本的核心作用是：</p><ul><li>实时监控指定 IRQ 或 CPU 的中断增长情况</li></ul><p>脚本每秒读取 /proc/interrupts，计算变化量，只输出“发生变化的项”。</p><ul><li>支持 CPU 或 IRQ 白名单筛选<br/>避免全量输出带来的噪声。</li><li>支持稀疏 CPU（如 CPU0/7/8/9/12）<br/>可自动解析实际在线 CPU，而不依赖连续编号。</li><li>支持参数格式：1,5-7,10-12<br/>满足生产环境中常见的范围表达方式。</li><li>附带中断描述信息<br/>如网卡 MSI-X、IPI、timer 等，便于快速识别来源</li><li><p>兼容 Python 2 / Python 3<br/>适用于老旧发行版（如 RHEL7）。</p><h3>2. 参数说明</h3></li></ul><p>脚本提供三个最常用参数：</p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td><code>-irq</code></td><td>指定监控的 IRQ 号（支持范围表达式）</td></tr><tr><td><code>-cpu</code></td><td>指定监控的 CPU 核（支持范围表达式）</td></tr><tr><td><code>-i</code></td><td>刷新间隔，默认 1 秒</td></tr></tbody></table><h3>3. 示例</h3><p>只监控 CPU0 与 CPU1 中断情况</p><pre><code class="bash">./irq_watch.py -cpu 0,1</code></pre><p>监控特定中断号（如 36、37、40–45）</p><pre><code class="bash">./irq_watch.py -irq 36,37,40-45</code></pre><p>监控多 CPU + 多 IRQ（组合过滤）</p><pre><code class="bash">./irq_watch.py -cpu 0,7-9 -irq 33-50</code></pre><h3>4. 输出示例解析</h3><p>以下是监控cpu0和cpu1上每秒中断的情况：</p><pre><code class="bash">[root@instance-bguv65e0 ~]# ./irq_watch.py -cpu 0,1
2025-11-26 19:25:44
IRQ    37 CPU   0: +        10 (total  120514471)      PCI-MSIX-0000:02:00.0 1-edge virtio1-req.0
IRQ    38 CPU   1: +         5 (total  117021031)      PCI-MSIX-0000:02:00.0 2-edge virtio1-req.1
IRQ    45 CPU   0: +        13 (total   49011897)      PCI-MSIX-0000:01:00.0 1-edge virtio0-input.0
IRQ    46 CPU   0: +         5 (total   29506292)      PCI-MSIX-0000:01:00.0 2-edge virtio0-output.0
IRQ    47 CPU   1: +        13 (total   51737238)      PCI-MSIX-0000:01:00.0 3-edge virtio0-input.1
IRQ    48 CPU   1: +        21 (total   56302135)      PCI-MSIX-0000:01:00.0 4-edge virtio0-output.1
IRQ   LOC CPU   0: +       232 (total   41007210)      Local timer interrupts
IRQ   LOC CPU   1: +       186 (total  540671379)      Local timer interrupts
IRQ   CAL CPU   0: +         3 (total   26439851)      Function call interrupts
2025-11-26 19:25:45
IRQ    37 CPU   0: +         1 (total  120514472)      PCI-MSIX-0000:02:00.0 1-edge virtio1-req.0
IRQ    38 CPU   1: +         4 (total  117021035)      PCI-MSIX-0000:02:00.0 2-edge virtio1-req.1
IRQ    45 CPU   0: +         4 (total   49011901)      PCI-MSIX-0000:01:00.0 1-edge virtio0-input.0
IRQ    46 CPU   0: +         9 (total   29506301)      PCI-MSIX-0000:01:00.0 2-edge virtio0-output.0
IRQ    47 CPU   1: +         6 (total   51737244)      PCI-MSIX-0000:01:00.0 3-edge virtio0-input.1
IRQ    48 CPU   1: +         7 (total   56302142)      PCI-MSIX-0000:01:00.0 4-edge virtio0-output.1
IRQ   LOC CPU   0: +       234 (total   41007444)      Local timer interrupts
IRQ   LOC CPU   1: +       166 (total  540671545)      Local timer interrupts
IRQ   RES CPU   0: +         1 (total   16877997)      Rescheduling interrupts
IRQ   CAL CPU   0: +         3 (total   26439854)      Function call interrupts
2025-11-26 19:25:46
IRQ    45 CPU   0: +         4 (total   49011905)      PCI-MSIX-0000:01:00.0 1-edge virtio0-input.0
IRQ    46 CPU   0: +         8 (total   29506309)      PCI-MSIX-0000:01:00.0 2-edge virtio0-output.0
IRQ    47 CPU   1: +         5 (total   51737249)      PCI-MSIX-0000:01:00.0 3-edge virtio0-input.1
IRQ    48 CPU   1: +         4 (total   56302146)      PCI-MSIX-0000:01:00.0 4-edge virtio0-output.1
IRQ   LOC CPU   0: +       200 (total   41007644)      Local timer interrupts
IRQ   LOC CPU   1: +       204 (total  540671749)      Local timer interrupts
IRQ   RES CPU   0: +         7 (total   16878004)      Rescheduling interrupts
IRQ   RES CPU   1: +         2 (total   20872214)      Rescheduling interrupts
IRQ   CAL CPU   0: +         1 (total   26439855)      Function call interrupts</code></pre><p>解释如下：<br/>IRQ：中断号（或 LOC/CAL 等 IPI 名字）<br/>CPU：发生增量的 CPU 核编号<br/>+N：本次 interval 内增加多少次<br/>total：累计总次数<br/>描述：/proc/interrupts 最后一列，中断来源（网卡/磁盘/IPI/timer）</p><p>监控的中断数量没有变化会输出 (no change)：</p><pre><code class="bash">[root@instance-bguv65e0 ~]# ./irq_watch.py -irq 222
2025-11-26 19:44:35
no change
2025-11-26 19:44:36
no change
2025-11-26 19:44:37
no change</code></pre><p>这样可大幅降低无意义噪声，便于观察异常增长。</p><h2>五、典型排查场景</h2><h3>场景 1：业务核心是否被“脏”中断污染？</h3><p>在延迟敏感的系统中，关键的业务线程往往会被绑定到固定的核心上，比如业务核心被绑定在 6,7,8,9号核心，则可以用以下指令来监测：</p><pre><code class="bash">./irq_watch.py -cpu 6-9</code></pre><h3>场景 2：排查稀疏 CPU 布局下的中断落点</h3><p>在禁用了部分核的系统中（如 CPU0/7/8/9/12 在线）：</p><pre><code class="bash">./irq_watch.py -cpu 7-12</code></pre><p>脚本可自动识别真实在线 CPU，使排查更准确。</p><h3>场景 3：观察周期性延迟尖刺是否由某 IRQ 触发</h3><p>例如怀疑某个 timer 中断：</p><pre><code class="bash">./irq_watch.py -irq LOC</code></pre><p>或怀疑某特点中断</p><pre><code class="bash">./irq_watch.py -irq 64-100</code></pre><p>若被监控的中断数量周期性增长，则可以确定延迟尖刺原因。</p><h2>六、irq_watch.py 的优势总结</h2><table><thead><tr><th>能力</th><th>描述</th></tr></thead><tbody><tr><td>实时检测</td><td>定位瞬时 spike 不再困难</td></tr><tr><td>增量输出</td><td>不输出无变化项，噪声极低</td></tr><tr><td>CPU/IRQ 双过滤</td><td>灵活监控业务相关部分</td></tr><tr><td>稀疏 CPU 兼容</td><td>适配任意在线 CPU 布局</td></tr><tr><td>自动提取中断描述</td><td>快速识别来源</td></tr><tr><td>资源占用低</td><td>占用的硬件资源几乎可以忽略</td></tr></tbody></table><h2>七、如何获取？</h2><p>📌 获取方式如下：<br/>1️⃣ 关注我的微信公众号：Hankin-Liu的技术研究室<br/>2️⃣ 发送关键词：irq watch<br/>3️⃣ 获取脚本<br/>4️⃣ 发送关键词：”性能调优群“，加入技术交流群，可免费解答和交流性能调优相关技术问题。</p><h2>八、总结</h2><p>低延迟系统需要在高度可控、可预测的环境中运行，而中断是影响系统确定性的关键因素之一。理解中断在系统中的角色、其带来的不确定性来源，以及如何持续监测其行为，是打造稳定低延迟系统的基础能力。<br/>通过构建完善的监控机制、结合业务特性制定相应策略，并持续验证系统在实际负载下的表现，可以更好地确保低延迟系统在各种运行状态下保持一致的性能表现。中断管理不是一次性的工作，而是伴随系统运行全周期的持续任务。</p><p><strong>📬 欢迎关注VX公众号“Hankin-Liu的技术研究室”，持续分享信创、软件性能测试、调优、编程技巧、软件调试技巧相关内容，输出有价值、有沉淀的技术干货。</strong></p>]]></description></item><item>    <title><![CDATA[转发之外 - AI 网关内容安全实践 s]]></title>    <link>https://segmentfault.com/a/1190000047437386</link>    <guid>https://segmentfault.com/a/1190000047437386</guid>    <pubDate>2025-11-28 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>（本文只讨论文本生成过程中的内容安全，不涉及多模态）</p><p>将 AI 的输入输出接到某个内容安全的过滤系统，几乎是每个 AI 网关必备的功能。为了合规，一方面，上下文中的个人信息需要脱敏；另一方面，某些不合时宜的言论需要净化。市面上内容安全的过滤系统功能大体上都差不多：接收一段信息，返回处理结果（是否过滤、触犯了哪些规则、需要替换的文本等等）。事实上 AI 网关可以专门搞一个面向内容安全的子系统，放到代理路径上来。不同的内容安全厂商就是这个子系统的一个 provider，只是对接的格式和配置不同，基本的输入输出都是可以复用的。</p><h2>输入</h2><p>各家 LLM provider 的输入都是 JSON 格式，所以输入阶段一般都是解析 JSON，取出各家 LLM provider 特定的输入字段。</p><p>在讨论之前，先允许我重温下 chat 接口的结构。一个 chat 请求长这样：</p><pre><code>system prompt # 可选，系统内置的前提条件
---
user prompt # 通常是用户的输入
---
response to user prompt
---
user prompt
...</code></pre><p>有些 AI 网关产品默认只检查最新的一段（在某些产品里，甚至是<a href="https://link.segmentfault.com/?enc=cixsOQdxPCq3wQCIvOcDSg%3D%3D.kY7Ou7MYYnWIj19Y7fDFPkRfkyekKvAMnfW7qWmIsVRNdK43Opeq6geszvr%2FAvCHO%2FStRtGyCwd2hp%2Fo1xusu%2BdeFwSmFNeSUfaRBJj13wJJGeM%2BRVGHXamH407JMdSNJD6k0Bdx%2B5tN8xpWwy1nxVJsV8gR%2FuGYESX27G5JFbs%3D" rel="nofollow" target="_blank">唯一支持的行为</a>）。这其实是不够安全的。</p><p>对于不受信任的客户端（比如运行在用户电脑上的软件），因为整个会话历史都是攻击者提供的，他们可以篡改之前的内容。同样的道理也适用于只检查 user prompt 或检查 system prompt 之外的内容。</p><p>如果调用都是来自可信的客户端，比如请求是由一个后端程序发起的，它能保证用户的输入每次都附加到最后面，那么只看最新的内容是安全的吗？可惜并不是。大模型推理时，可不会只看 user prompt 或最新的一段输入，而是会看整个上下文。内容安全过滤时只看最新内容，视野就太窄了，无法理解上下文。举个例子：</p><p>假设内容安全过滤规则是不允许讨论某地区的政治。</p><pre><code>&gt; 某个地区，你懂的

&lt; 因相关法律法规，内容不予显示

&gt; 介绍下前面提到的地区的政治</code></pre><p>当内容安全系统只能看到最新内容时，它是没办法知道“前面提到的地区”是哪个地区，在收到最后一条时没办法拦截请求。当然开发者可以通过从用户输入历史中移除被拦截的内容，来保证安全性。如果你的功能需要依赖安全过滤，了解这种过滤的能力边界是重要的。</p><p>国内许多内容安全厂商（如腾讯云、阿里云等）在计费时是按条数算，而不是像国外那样按字符数算。这倒催生出一个特殊的省钱窍门 - 把每次内容检测调用变成一个任务，在一个时间窗口里合并相邻的任务，攒够接近 10000 个字符后才发送。当然注意要给个超时时间，避免任务一直等待在队列当中。同时解析响应时需要做分拆，还原出原始的内容检测调用。虽然有点复杂，但理论上用的好可以省下大部分的开销。</p><h2>输出</h2><p>大模型文本生成有两种形式：流式和非流式。假设一个业务采用了流式响应，要接入内容安全过滤系统。如果我们把它变成非流式（全部内容接收完才去请求内容安全过滤系统），可能会影响业务。比如原本用户可以看到内容一点点地生成出来，即使全部生成需要等上几分钟，也不会感到不耐烦。然而如果变成非流式之后，用户不得不等待几分钟才能看到结果，他们有可能会切换到竞品上。那么为什么不将流式的响应直接接入到内容安全系统呢？因为不安全的内容有可能恰好在流式的两个 chunk 之间被生成出来。</p><p>有没有折中的方案呢？有的 - 通过引入延迟缓冲区（buffering）。在开源之夏期间，学生杨伟诺在本人的指导下实现了一个<a href="https://link.segmentfault.com/?enc=mYwgFcAjufUYI5nfak5mmQ%3D%3D.nVVG2OlFdnWks%2BGThW6M0Vzvq2RKV8D7LRa6g64SJIMgwMSO2%2FnQnoRaSfpicCgD" rel="nofollow" target="_blank">基于缓冲区的内容安全过滤系统</a>。其核心思路是：在流式响应的过程中，维护一个缓冲区，存储最近生成的内容。当缓冲区达到一定大小或者超时或者请求结束的时候，调用内容安全过滤系统进行检查。<strong>如果没有发现不安全内容，则将缓冲区中除了尾部若干字符之外的内容发送给用户。尾部若干字符保留在缓冲区中，以防止不安全内容跨越 chunk 边界的情况发生，他们会在下一次检查时一并处理</strong>。这样可以在保证内容安全的同时，尽量减少对用户体验的影响。它背后的思想是：不安全的内容往往是局部的，不会像阅读欧亨利的小说那样等到结尾才迎来反转。所以只要保留一部分最近生成的内容进行检查，就能有效地捕捉到不安全内容。具体如下：</p><ol><li>收到 chunk 1: xxx...xbad c</li><li>发起安全内容检查 <code>xxx...xbad c</code>，通过</li><li>发送 chunk 1: xxx... 给用户，保留尾部内容 "xbad c" 在缓冲区</li><li>收到 chunk 2: ontent...yyy</li><li>将缓冲区内容和 chunk 2 拼接，变成 "xbad content...yyy"</li><li>发起安全内容检查 <code>xbad content...yyy</code>，发现 "bad content" 不安全</li></ol><p>这里的关键在于保留一个合适大小的缓冲区，既能捕捉到跨 chunk 边界的不安全内容，又不会让用户等待太久。通过调整缓冲区大小和检查频率，可以在内容安全和用户体验之间找到一个平衡点。</p><p>即使不考虑对业务的影响，使用缓冲区依然有其必要。因为内容安全系统一次请求中能处理的最大字符数是有限的，如果流式响应的内容过多，直接发送给内容安全系统可能会超出其处理能力。通过缓冲区，可以将流式响应拆分成多个小块，逐个发送给内容安全系统进行检查，避免超出其处理能力。</p><h2>结语</h2><p>在面向生成式文本的内容安全设计中，看似简简单单的“转发内容到过滤器”，其实里面也有不少的门道。</p><p>输入侧：不要只检查最新一段输入 —— 完整上下文才是模型决策的依据；对于不受信任的客户端要格外谨慎；面对按条计费的服务，可以通过合并请求节省成本但要注意超时与还原逻辑。</p><p>输出侧：引入缓冲区并保留尾部若干字符进行分段检测，是兼顾用户体验与安全性的务实方案；同时需要调优缓冲区大小、检查频率与超时策略。</p>]]></description></item><item>    <title><![CDATA[GraphRAG进阶：基于Neo4j与L]]></title>    <link>https://segmentfault.com/a/1190000047437335</link>    <guid>https://segmentfault.com/a/1190000047437335</guid>    <pubDate>2025-11-28 22:01:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>微软的GraphRAG算得上是最早一批成熟的GraphRAG系统，它把索引阶段（抽取实体、关系、构建层级社区并生成摘要）和查询阶段的高级能力整合到了一起。这套方案的优势在于，可以借助预先计算好的实体、关系、社区摘要来回答那些宏观的、主题性的问题，这恰恰是传统RAG系统基于文档检索难以做到的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437337" alt="" title=""/></p><p>本文的重点是DRIFT搜索：Dynamic Reasoning and Inference with Flexible Traversal，翻译过来就是"动态推理与灵活遍历"。这是一种相对较新的检索策略，兼具全局搜索和局部搜索的特点。</p><p>DRIFT的工作流程是这样的：先通过向量搜索建立一个宽泛的查询起点，再利用群信息把原始问题拆解成更细粒度的后续查询。然后动态地在知识图谱上游走，抓取实体、关系等局部细节。这种设计在计算效率和答案质量之间找到了一个不错的平衡点。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437338" alt="" title="" loading="lazy"/><br/>上图为使用 LlamaIndex 工作流和 Neo4j 实现的 DRIFT 搜索，核心流程分一下几步：</p><p>首先是<strong>HyDE生成</strong>，基于一份样例社区报告构造假设性答案，用来改善查询的向量表示。</p><p>接着<strong>社区搜索</strong>登场，通过向量相似度找出最相关的社区报告，给查询提供宏观上下文。系统会分析这些结果，输出一个初步的中间答案，同时生成一批后续查询用于深挖。</p><p>这些后续查询会在<strong>局部搜索阶段</strong>并行执行，从知识图谱里捞出文本块、实体、关系、以及更多社区报告。这个过程可以迭代多轮，每轮都可能产生新的后续查询。</p><p>最后是<strong>答案生成</strong>，把过程中积累的所有中间答案汇总起来，融合社区级别的宏观洞察和局部细节，生成最终响应。整体思路就是先铺开、再聚焦，层层递进。</p><p>本文用的是《爱丽丝梦游仙境》，刘易斯·卡罗尔的经典作品，这部小说角色众多、场景丰富、事件环环相扣，拿来演示GraphRAG的能力再合适不过。</p><h2>数据导入</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437339" alt="" title="" loading="lazy"/><br/>整个pipeline遵循标准的GraphRAG流程，分三个阶段：</p><pre><code> class MSGraphRAGIngestion(Workflow):  
    @step  
    async def entity_extraction(self, ev: StartEvent) -&gt; EntitySummarization:  
        chunks = splitter.split_text(ev.text)  
        await ms_graph.extract_nodes_and_rels(chunks, ev.allowed_entities)  
        return EntitySummarization()  

    @step  
    async def entity_summarization(  
        self, ev: EntitySummarization  
    ) -&gt; CommunitySummarization:  
        await ms_graph.summarize_nodes_and_rels()  
        return CommunitySummarization()  

    @step  
    async def community_summarization(  
        self, ev: CommunitySummarization  
    ) -&gt; CommunityEmbeddings:  
        await ms_graph.summarize_communities()  
         return CommunityEmbeddings()</code></pre><p>先从文本块里抽取实体和关系，再给节点和关系生成摘要，最后构建层级社区并生成社区摘要。</p><p>摘要做完之后，要给社区和实体都生成向量嵌入，这样才能支持相似性检索。社区嵌入的代码长这样：</p><pre><code> @step  
    async def community_embeddings(self, ev: CommunityEmbeddings) -&gt; EntityEmbeddings:  
        # Fetch all communities from the graph database  
        communities = ms_graph.query(  
            """  
    MATCH (c:__Community__)  
    WHERE c.summary IS NOT NULL AND c.rating &gt; $min_community_rating  
    RETURN coalesce(c.title, "") + " " + c.summary AS community_description, c.id AS community_id  
    """,  
            params={"min_community_rating": MIN_COMMUNITY_RATING},  
        )  
        if communities:  
            # Generate vector embeddings from community descriptions  
            response = await client.embeddings.create(  
                input=[c["community_description"] for c in communities],  
                model=TEXT_EMBEDDING_MODEL,  
            )  
            # Store embeddings in the graph and create vector index  
            embeds = [  
                {  
                    "community_id": community["community_id"],  
                    "embedding": embedding.embedding,  
                }  
                for community, embedding in zip(communities, response.data)  
            ]  
            ms_graph.query(  
                """UNWIND $data as row  
            MATCH (c:__Community__ {id: row.community_id})  
            CALL db.create.setNodeVectorProperty(c, 'embedding', row.embedding)""",  
                params={"data": embeds},  
            )  
            ms_graph.query(  
                "CREATE VECTOR INDEX community IF NOT EXISTS FOR (c:__Community__) ON c.embedding"  
            )  
         return EntityEmbeddings()</code></pre><p>实体嵌入同理，这样DRIFT搜索需要的向量索引就都建好了。</p><h2>DRIFT搜索</h2><p>DRIFT的检索思路其实很符合简单：先看大图，再挖细节。它不会一上来就在文档或实体层面做精确匹配，而是先去查群的摘要，因为这些摘要是对知识图谱主要主题的高层次概括。</p><p>拿到相关的高层信息后，DRIFT会智能地派生出后续查询，去精确检索特定实体、关系、源文档。这种两阶段的做法其实很像人类查资料的习惯：先大致了解情况再针对性地追问细节。既有全局搜索的覆盖面，又有局部搜索的精准度，而且不用把所有社区报告或文档都过一遍，计算开销控制得不错。</p><p>下面拆解一下各个阶段的实现。</p><h3>群搜索</h3><p>DRIFT用了HyDE技术来提升向量检索的准确率。不是直接拿用户query做embedding，而是先让模型生成一个假设性的答案，再用这个答案去做相似性搜索。道理很简单：假设答案在语义上跟真实的摘要更接近。</p><pre><code> @step  
async def hyde_generation(self, ev: StartEvent) -&gt; CommunitySearch:  
    # Fetch a random community report to use as a template for HyDE generation  
    random_community_report = driver.execute_query(  
        """  
    MATCH (c:__Community__)  
    WHERE c.summary IS NOT NULL  
    RETURN coalesce(c.title, "") + " " + c.summary AS community_description""",  
        result_transformer_=lambda r: r.data(),  
    )  
    # Generate a hypothetical answer to improve query representation  
    hyde = HYDE_PROMPT.format(  
        query=ev.query, template=random_community_report[0]["community_description"]  
    )  
    hyde_response = await client.responses.create(  
        model="gpt-5-mini",  
        input=[{"role": "user", "content": hyde}],  
        reasoning={"effort": "low"},  
    )  
     return CommunitySearch(query=ev.query, hyde_query=hyde_response.output_text)</code></pre><p>拿到HyDE query之后，做embedding，然后通过向量相似度捞出top 5的报告。接着让LLM基于这些报告生成一个初步答案，同时识别出需要深挖的后续查询。将初步答案存起来然后进行后续查询全部并行分发到局部搜索阶段。</p><pre><code>   
@step  
async def community_search(self, ctx: Context, ev: CommunitySearch) -&gt; LocalSearch:  
    # Create embedding from the HyDE-enhanced query  
    embedding_response = await client.embeddings.create(  
        input=ev.hyde_query, model=TEXT_EMBEDDING_MODEL  
    )  
    embedding = embedding_response.data[0].embedding  
      
    # Find top 5 most relevant community reports via vector similarity  
    community_reports = driver.execute_query(  
        """  
    CALL db.index.vector.queryNodes('community', 5, $embedding) YIELD node, score  
    RETURN 'community-' + node.id AS source_id, node.summary AS community_summary  
    """,  
        result_transformer_=lambda r: r.data(),  
        embedding=embedding,  
    )  
      
    # Generate initial answer and identify what additional info is needed  
    initial_prompt = DRIFT_PRIMER_PROMPT.format(  
        query=ev.query, community_reports=community_reports  
    )  
    initial_response = await client.responses.create(  
        model="gpt-5-mini",  
        input=[{"role": "user", "content": initial_prompt}],  
        reasoning={"effort": "low"},  
    )  
    response_json = json_repair.loads(initial_response.output_text)  
    print(f"Initial intermediate response: {response_json['intermediate_answer']}")  
      
    # Store the initial answer and prepare for parallel local searches  
    async with ctx.store.edit_state() as ctx_state:  
        ctx_state["intermediate_answers"] = [  
            {  
                "intermediate_answer": response_json["intermediate_answer"],  
                "score": response_json["score"],  
            }  
        ]  
        ctx_state["local_search_num"] = len(response_json["follow_up_queries"])  
      
    # Dispatch follow-up queries to run in parallel  
    for local_query in response_json["follow_up_queries"]:  
        ctx.send_event(LocalSearch(query=ev.query, local_query=local_query))  
     return None</code></pre><p>这就是DRIFT的核心思路，先用HyDE增强的社区搜索铺开，再用后续查询往下钻。</p><h3>局部搜索</h3><p>局部搜索阶段把后续查询并行跑起来，深入到具体细节。每个查询通过实体向量检索拿到目标上下文，生成中间答案，可能还会产出更多后续查询。</p><pre><code> @step(num_workers=5)  
async def local_search(self, ev: LocalSearch) -&gt; LocalSearchResults:  
    print(f"Running local query: {ev.local_query}")  
      
    # Create embedding for the local query  
    response = await client.embeddings.create(  
        input=ev.local_query, model=TEXT_EMBEDDING_MODEL  
    )  
    embedding = response.data[0].embedding  
      
    # Retrieve relevant entities and gather their associated context:  
    # - Text chunks where entities are mentioned  
    # - Community reports the entities belong to  
    # - Relationships between the retrieved entities  
    # - Entity descriptions  
    local_reports = driver.execute_query(  
        """  
CALL db.index.vector.queryNodes('entity', 5, $embedding) YIELD node, score  
WITH collect(node) AS nodes  
WITH  
collect {  
  UNWIND nodes as n  
  MATCH (n)&lt;-[:MENTIONS]-&gt;(c:__Chunk__)  
  WITH c, count(distinct n) as freq  
  RETURN {chunkText: c.text, source_id: 'chunk-' + c.id}  
  ORDER BY freq DESC  
  LIMIT 3  
} AS text_mapping,  
collect {  
  UNWIND nodes as n  
  MATCH (n)-[:IN_COMMUNITY*]-&gt;(c:__Community__)  
  WHERE c.summary IS NOT NULL  
  WITH c, c.rating as rank  
  RETURN {summary: c.summary, source_id: 'community-' + c.id}  
  ORDER BY rank DESC  
  LIMIT 3  
} AS report_mapping,  
collect {  
  UNWIND nodes as n  
  MATCH (n)-[r:SUMMARIZED_RELATIONSHIP]-(m)  
  WHERE m IN nodes  
  RETURN {descriptionText: r.summary, source_id: 'relationship-' + n.name + '-' + m.name}  
LIMIT 3  
} as insideRels,  
collect {  
  UNWIND nodes as n  
  RETURN {descriptionText: n.summary, source_id: 'node-' + n.name}  
} as entities  
RETURN {Chunks: text_mapping, Reports: report_mapping,  
   Relationships: insideRels,  
   Entities: entities} AS output  
""",  
        result_transformer_=lambda r: r.data(),  
        embedding=embedding,  
    )  
      
    # Generate answer based on the retrieved context  
    local_prompt = DRIFT_LOCAL_SYSTEM_PROMPT.format(  
        response_type=DEFAULT_RESPONSE_TYPE,  
        context_data=local_reports,  
        global_query=ev.query,  
    )  
    local_response = await client.responses.create(  
        model="gpt-5-mini",  
        input=[{"role": "user", "content": local_prompt}],  
        reasoning={"effort": "low"},  
    )  
    response_json = json_repair.loads(local_response.output_text)  
      
    # Limit follow-up queries to prevent exponential growth  
    response_json["follow_up_queries"] = response_json["follow_up_queries"][:LOCAL_TOP_K]  
      
     return LocalSearchResults(results=response_json, query=ev.query)</code></pre><p>下一步负责编排迭代深化的过程。用</p><pre><code>collect_events</code></pre><p>等所有并行搜索跑完，然后判断要不要继续往下挖。如果当前深度还没到上限（这里设的max depth=2），就把所有结果里的后续查询提取出来，存好中间答案分发下一轮并行搜索。</p><pre><code> @step  
async def local_search_results(  
    self, ctx: Context, ev: LocalSearchResults  
) -&gt; LocalSearch | FinalAnswer:  
    local_search_num = await ctx.store.get("local_search_num")  
      
    # Wait for all parallel searches to complete  
    results = ctx.collect_events(ev, [LocalSearchResults] * local_search_num)  
    if results is None:  
        return None  
          
    intermediate_results = [  
        {  
            "intermediate_answer": event.results["response"],  
            "score": event.results["score"],  
        }  
        for event in results  
    ]  
    current_depth = await ctx.store.get("local_search_depth", default=1)  
    query = [ev.query for ev in results][0]  

    # Continue drilling down if we haven't reached max depth  
    if current_depth &lt; MAX_LOCAL_SEARCH_DEPTH:  
        await ctx.store.set("local_search_depth", current_depth + 1)  
        follow_up_queries = [  
            query  
            for event in results  
            for query in event.results["follow_up_queries"]  
        ]  
          
        # Store intermediate answers and dispatch next round of searches  
        async with ctx.store.edit_state() as ctx_state:  
            ctx_state["intermediate_answers"].extend(intermediate_results)  
            ctx_state["local_search_num"] = len(follow_up_queries)  

        for local_query in follow_up_queries:  
            ctx.send_event(LocalSearch(query=query, local_query=local_query))  
        return None  
    else:  
         return FinalAnswer(query=query)</code></pre><p>这样就形成了一个迭代细化的循环，每一层都在前一层的基础上继续深挖。达到最大深度后，触发最终答案生成。</p><h3>最终答案</h3><p>最后一步把整个DRIFT搜索过程中积攒的所有中间答案汇总成一个完整的响应：这里包括社区搜索的初步答案，以及局部搜索各轮迭代产出的答案。</p><pre><code> @step  
async def final_answer_generation(self, ctx: Context, ev: FinalAnswer) -&gt; StopEvent:  
    # Retrieve all intermediate answers collected throughout the search process  
    intermediate_answers = await ctx.store.get("intermediate_answers")  
      
    # Synthesize all findings into a comprehensive final response  
    answer_prompt = DRIFT_REDUCE_PROMPT.format(  
        response_type=DEFAULT_RESPONSE_TYPE,  
        context_data=intermediate_answers,  
        global_query=ev.query,  
    )  
    answer_response = await client.responses.create(  
        model="gpt-5-mini",  
        input=[  
            {"role": "developer", "content": answer_prompt},  
            {"role": "user", "content": ev.query},  
        ],  
        reasoning={"effort": "low"},  
    )  

     return StopEvent(result=answer_response.output_text)</code></pre><h2>总结</h2><p>DRIFT搜索提供了一个挺有意思的思路，在全局搜索的广度和局部搜索的精度之间找到了平衡。从社区级上下文切入，通过迭代的后续查询逐层下探，既避免了遍历所有社区报告的计算负担，又保证了覆盖面。</p><p>这里还有改进空间，比如目前的实现对所有中间答案一视同仁，如果能根据置信度分数做个筛选，最终答案的质量应该会更好，噪声也能降下来。后续查询也可以先按相关性或信息增益排个序，优先追踪最有价值的线索。</p><p>另一个值得尝试的方向是加一个查询精炼步骤，用LLM分析所有生成的后续查询，把相似的归并起来避免重复搜索，过滤掉那些大概率没什么收获的查询。这样能大幅减少局部搜索的次数，同时不影响答案质量。</p><p>完整代码</p><p><a href="https://link.segmentfault.com/?enc=Zpc6fkApv%2BgjsT55iHH3JQ%3D%3D.ao%2BwJ3wRsZZXf1Hy41BdlBGnYEa%2F7L%2BOc6LdaNR5P31iPYoGbRseRzFNG2gbK%2BHrll5zXDNllSHitZNQGcbPLg%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/5eaca452dcc7422d8c2308586e7cfe56</a></p><p>有兴趣的可以自己跑跑看，或者在这个基础上做些改进。</p><p>作者：Tomaz Bratanic</p>]]></description></item><item>    <title><![CDATA[# 适合小企业使用的KVM虚拟机管理工具]]></title>    <link>https://segmentfault.com/a/1190000047437261</link>    <guid>https://segmentfault.com/a/1190000047437261</guid>    <pubDate>2025-11-28 21:05:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>背景：</h2><p>我们日常在管理KVM虚拟机主要使用的功能是批量克隆、修改、删除、启动、关闭KVM虚拟机，其中克隆、修改是最为重要的功能，因为其他功能我们用<code>virt-manager</code>也可以方便的操作，比如启动、关闭。虽然<code>virt-manager</code>也可以实现克隆、修改之类的操作，但是你若有一大批需要创建的虚拟机，那么它将是痛苦的，而<code>zzxia-kvm-manage</code>项目就是为了解决这个问题，你只需要通过编辑一个markdown表格文件就可以实现批量虚拟机的克隆和IP地址修改之类的预设操作，在此过程中无需人工干预，省心省力。项目地址：<strong><a href="https://link.segmentfault.com/?enc=BIaIFVTWaoqF57Xws1O8UQ%3D%3D.HxmbAFN3QUM3blI4dpOSi4z6JveQjpN5SB8CeKr5kIU6iQjILbKpLbLBTLtz3vY6" rel="nofollow" target="_blank">https://gitee.com/zhf_sy/zzxia-kvm-manage</a></strong></p><p>[toc]</p><h2>1 介绍</h2><p>批量克隆、修改、删除、启动、自动启动、关闭KVM虚拟机。适合小企业使用。</p><h3>1.1 功能：</h3><ol><li>克隆虚拟机：通过编辑my_vm.list文件定义虚拟机信息，然后运行vm-clone.sh，选择克隆模板，然后按照my_vm.list清单克隆出想要的虚拟机</li><li>修改虚拟机信息：【主机名、IP、IP子网掩码、网关、域名、DNS】，一般主要配合vm-clone.sh使用，也可以单独使用</li><li>批量启动指定虚拟机；批量启动清单中的虚拟机；批量启动选择的虚拟机</li><li>批量设置自动启动指定虚拟机；批量设置自动启动清单中的虚拟机；批量设置自动启动选择的虚拟机</li><li>批量关闭指定虚拟机；批量关闭清单中的虚拟机；批量关闭选择的虚拟机</li><li>批量删除指定虚拟机；批量删除清单中的虚拟机；批量删除选择的虚拟机</li></ol><h3>1.2 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk；<br/>&lt;img src="https://gitee.com/zhf_sy/pic-bed/raw/master/dao.png" alt="打赏" style="zoom:40%;" /&gt;</li></ol><h2>2 软件架构</h2><p>Linux shell</p><h2>3 安装教程</h2><p>克隆到KVM服务器上即可</p><h2>4 使用说明</h2><p>请使用-h|--help参数运行sh脚本即可看到使用帮助<br/>除了kvm，你还需要安装guestfs，在centos7上运行<code>yum install -y  libguestfs-tools</code></p><h3>4.1 环境变量文件<code>kvm.env</code></h3><p>基于<code>kvm.env.sample</code>创建环境变量文件<code>kvm.env</code>，根据你的环境修改相关环境变量，这个非常重要，否则你可能运行出错</p><pre><code class="bash">$ cat kvm.env.sample
#!/bin/bash

# 静默方式
export QUIET='no'     #--- yes|no

# KVM环境参数
export KVM_XML_PATH='/etc/libvirt/qemu'                 #-- KVM虚拟XML配置文件路径（CENTOS下XML的默认路径，如果是UBUNTU，请修改）

# 模板虚拟机参数
/dev/sda1】
export VM_NIC_CONF_FILE='/etc/sysconfig/network-scripts/ifcfg-eth0'   #-- 模板虚拟机CentOS系统内的网卡配置文件

# 新虚拟机默认参数，特殊值可以在【my_vm.list】中指定
export VM_DEFAULT_DNS='192.168.11.3,192.168.11.4'      #-- 默认DNS，最多两个DNS服务器，中间用【,】分隔，不要有空格
export VM_DEFAULT_DOMAIN='zjlh.lan'                    #-- 默认域名
export VM_DEFAULT_DISK_IMG_PATH='/var/lib/libvirt/images'   #-- 虚拟机磁盘文件默认路径</code></pre><h3>4.2 虚拟机列表文件<code>my_vm.list</code></h3><p>基于<code>my_vm.list.sample</code>创建虚拟机列表文件<code>my_vm.list</code>（默认，文件名可以是其他名称），根据自己的需要定制虚拟机信息，以逗号分隔，用#注释掉不需要的行：</p><pre><code class="text">$ cat  my_vm.list.sample
### 虚拟机克隆清单
###.
###   2【名称：NAME】= [自定义]
###     既是虚拟机名称，也是虚拟机主机名
###..
###   3【CPU：CPU】= [自定义数量]
###..
###   4【内存：MEM】= [自定义数量]
###     单位是GB
###..
###   5【网卡：NIC】= [自定义]
###.    KVM网卡名称
###..
###   6【IP地址：IP】= [自定义]
###.    IP地址
###..
###   7【IP掩码：IP_MASK】= [自定义]
###.    IP地址掩码，例如：24、16、8、12
###..
###   8【IP网关：IP_GATEWAY】= [自定义]
###.    IP网关
###..
###   9【DNS：DNS】= &lt; 自定义1 &lt;,自定义2&gt; &gt;
###.    可以定义0~2个，例如：1.1.1.1, 2.2.2.2，或者8.8.8.8
###.
###  10【域名：DOMAIN】= &lt;自定义&gt;
###.    虚拟机的域名
###.
###  11【磁盘IMG路径：IMG_PATH】= &lt;自定义&gt;
###.    虚拟机KVM磁盘文件存放路径，例如：/disk2/images
###.
###  12【备注：NOTE】= [ 自定义 ]
###     说明信息
###.
###
###     暂时不需要的行用'#'注释掉
###
###
#| NAME                   | CPU  | MEM  | NIC  | IP             | IP_MASK | IP_GATEWAY   | DNS                        | DOMAIN   | IMG_PATH                    | NOTE               |
#| **名称**               | CPU  | 内存 | 网卡 | **IP地址**     | IP掩码  | **IP网关**   | **DNS**                    | **域名** | **磁盘IMG路径**             | **备注**           |
#| ---------------------- | ---- | ---- | ---- | -------------- | ------- | ------------ | -------------------------- | -------- | --------------------------- |                    |
| v-192-168-11-190-deploy | 1    | 2    | br1  | 192.168.11.190 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-191-mast   | 4    | 8    | br1  | 192.168.11.191 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-192-node   | 4    | 8    | br1  | 192.168.11.192 | 24      | 192.168.11.1 | 8.8.8.8                    | zj.lan   | /var/lib/libvirt/images22   |                    |
| v-192-168-11-193-node   | 4    | 8    | br1  | 192.168.11.193 | 24      | 192.168.11.1 | 1.1.1.1, 2.2.2.2           | hb.lan   |                             |                    |
| v-192-168-11-194-etcd   | 2    | 4    | br1  | 192.168.11.194 | 24      | 192.168.11.1 |                            |          |                             |                    |
#| v-192-168-11-195-etcd   | 2    | 4    | br1  | 192.168.11.195 | 24      | 192.168.11.1 |                            |          |                             |                    |
#| v-192-168-11-196-etcd   | 2    | 4    | br1  | 192.168.11.196 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-197-repo   | 2    | 4    | br1  | 192.168.11.197 | 24      | 192.168.11.1 |                            |          | /disk2/images               |                    |</code></pre><h3>4.3 克隆</h3><p><strong>克隆前的建议：</strong></p><ul><li>建议先制作好一个较为完美的模板虚拟机，然后在克隆时选择使用他</li><li>查看KVM环境变量文件<code>kvm.env</code>，看是否与你的实际情况相同，否则请修改它</li></ul><pre><code class="bash">$ ./vm-clone.sh --help

    用途：KVM上虚拟机克隆，并修改相关信息（主机名、IP、IP子网掩码、网关、域名、DNS）
    依赖：
        ./vm-img-modify.sh
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-clone.sh  [-h|--help]
        ./vm-clone.sh  &lt;-f|--file {清单文件}&gt;  &lt; -q|--quiet  [-t|--template {虚拟机模板}] &gt;
        ./vm-clone.sh  &lt;-f|--file {清单文件}&gt;  &lt;-t|--template {虚拟机模板}&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -f|--file      虚拟机清单文件，默认为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -q|--quiet     静默方式
        -t|--templat   指定虚拟机模板
    示例:
        #
        ./vm-clone.sh  -h
        # 一般
        ./vm-clone.sh                       #--- 默认虚拟机清单文件【./my_vm.list】，非静默方式，手动选择模板
        ./vm-clone.sh  -t v-centos-1        #--- 默认虚拟机清单文件【./my_vm.list】，非静默方式，基于模板【v-centos-1】创建
        # 指定vm清单文件
        ./vm-clone.sh  -f xxx.list                      #--- 使用虚拟机清单文件【xxx.list】，非静默方式，手动选择模
        ./vm-clone.sh  -f xxx.list  -t v-centos-1       #--- 使用虚拟机清单文件【xxx.list】，非静默方式，基于模板【v-centos-1】创建
        # 静默方式
        ./vm-clone.sh  -q  -t v-centos-1                #--- 默认虚拟机清单文件【./my_vm.list】，静默方式，基于模板【v-centos-1】创建
        ./vm-clone.sh  -q  -t v-centos-1  -f xxx.list   #--- 使用虚拟机清单文件【xxx.list】，静默方式，基于模板【v-centos-1】创建</code></pre><h3>4.4 修改vm信息</h3><pre><code class="bash">$ ./vm-img-modify.sh 

    用途：修改KVM虚拟机主机名及网卡信息（主机名、IP、IP子网掩码、网关、域名、DNS）
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-img-modify.sh  [-h|--help]
        ./vm-img-modify.sh  &lt;-q|--quiet&gt;  [ {VM_NAME}  {NEW_IP}  {NEW_IP_MASK}  {NEW_GATEWAY} ]  &lt;{NEW_DOMAIN}&gt;  &lt;{NEW_DNS1}&lt;,{NEW_DNS2}&gt;&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -q|--quiet     静默方式
    示例:
        #
        ./vm-img-modify.sh  -h        #--- 帮助
        # 一般
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3,192.168.11.4
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1
        # 静默方式
        ./vm-img-modify.sh  -q  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3,192.168.11.4</code></pre><h3>4.5 启动（或自动启动）虚拟机</h3><pre><code class="bash">$ ./vm-start.sh -h

    用途：启动虚拟机；设置虚拟机自动启动
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-start.sh  [-h|--help]
        ./vm-start.sh  [-l|--list]
        ./vm-start.sh  [ &lt;-s|--start&gt;  &lt;-a|--autostart&gt; ]  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -s|--start     启动虚拟机
        -a|--autostart 开启自动启动虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
    示例:
        #
        ./vm-start.sh  -h                   #--- 帮助
        ./vm-start.sh  -l                   #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-start.sh  -s                   #--- 启动默认虚拟机清单文件【./my_vm.list】中的虚拟机
        ./vm-start.sh  -s  -a               #--- 启动默认虚拟机清单文件【./my_vm.list】中的虚拟机，并设置为自动启动
        ./vm-start.sh  -a                   #--- 自动启动默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-start.sh  -s  -f xxx.list      #--- 启动虚拟机清单文件【xxx.list】中的虚拟机
        ./vm-start.sh  -a  -f xxx.list      #--- 自动启动虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-start.sh  -s  -S               #--- 启动我选择的虚拟机
        ./vm-start.sh  -a  -S               #--- 自动启动我选择的虚拟机
        # 指定虚拟机
        ./vm-start.sh  -s  -A  vm1 vm2      #--- 启动虚拟机【vm1、vm2】
        ./vm-start.sh  -a  -A  vm1 vm2      #--- 自动启动虚拟机【vm1、vm2】</code></pre><h3>4.6 关闭虚拟机</h3><pre><code class="bash">$ ./vm-shutdown.sh -h

    用途：shutdown虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-shutdown.sh  [-h|--help]
        ./vm-shutdown.sh  [-l|--list]
        ./vm-shutdown.sh  &lt;-q|--quiet&gt;  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
        -q|--quiet     静默方式
    示例:
        #
        ./vm-shutdown.sh  -h               #--- 帮助
        ./vm-shutdown.sh  -l               #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-shutdown.sh                   #--- shutdown默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-shutdown.sh  -f xxx.list      #--- shutdown虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-shutdown.sh  -S               #--- shutdown我选择的虚拟机
        # 指定虚拟机
        ./vm-shutdown.sh  -A  vm1 vm2      #--- shutdown虚拟机【vm1、vm2】
        # 静默方式
        ./vm-shutdown.sh  -q               #--- shutdown默认虚拟机清单文件【./my_vm.list】中的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -f xxx.list  #--- shutdown虚拟机清单文件【xxx.list】中的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -S           #--- shutdown我选择的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -A  vm1 vm2  #--- shutdown虚拟机【vm1、vm2】，用静默方式</code></pre><h3>4.7 删除虚拟机</h3><pre><code class="bash">$ ./vm-rm.sh -h

    用途：删除虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-rm.sh  [-h|--help]
        ./vm-rm.sh  [-l|--list]
        ./vm-rm.sh  &lt;-q|--quiet&gt;  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
        -q|--quiet     静默方式
    示例:
        #
        ./vm-rm.sh  -h               #--- 帮助
        ./vm-rm.sh  -l               #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-rm.sh                   #--- 删除默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-rm.sh  -f xxx.list      #--- 删除虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-rm.sh  -S               #--- 删除我选择的虚拟机
        # 指定虚拟机
        ./vm-rm.sh  -A  vm1 vm2      #--- 删除虚拟机【vm1、vm2】
        # 静默方式
        ./vm-rm.sh  -q               #--- 删除默认虚拟机清单文件【./my_vm.list】中的虚拟机，用静默方式
        ./vm-rm.sh  -q  -f xxx.list  #--- 删除虚拟机清单文件【xxx.list】中的虚拟机，用静默方式
        ./vm-rm.sh  -q  -S           #--- 删除我选择的虚拟机，用静默方式
        ./vm-rm.sh  -q  -A  vm1 vm2  #--- 删除虚拟机【vm1、vm2】，用静默方式</code></pre><h3>4.8 列出已有虚拟机</h3><pre><code class="bash">$ ./vm-list.sh -h

    用途：列出KVM上的虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-list.sh  &lt;-h|--help&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
    示例:
        #
        ./vm-list.sh  -h                   #--- 帮助
        ./vm-list.sh                       #--- 列出KVM上的虚拟机</code></pre><h3>4.8 简单管理虚拟机命令</h3><p>看名字就知道他的用途了</p><pre><code class="bash">./easy-save-all-online-vm-list-to-file.sh
./easy-save-all-vm-list-to-file.sh
./easy-start-spec-vm-list.sh
./easy-shutdown-all-online-vm.sh
./easy-shutdown-spec-vm-list.sh
./easy-force-shutdown-spec-vm-list.sh</code></pre><h2>5 最后</h2><p>好用</p>]]></description></item><item>    <title><![CDATA[以 StarRocks 4.0 为核，引]]></title>    <link>https://segmentfault.com/a/1190000047437269</link>    <guid>https://segmentfault.com/a/1190000047437269</guid>    <pubDate>2025-11-28 21:04:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着人工智能与大数据技术的深度融合，数据分析平台正面临一场深刻的变革。传统的 T+1 批处理模式、孤立的分析系统以及仅面向内部用户的服务模式，已无法满足当今业务对实时性、灵活性和智能化的高度需求。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLg" alt="640.jpg" title="640.jpg"/></p><p>新时代的分析场景，正从高层战略延伸至一线运营，从批处理转向批流一体，服务对象也从内部员工扩展至外部客户乃至 AI Agent。在这一背景下，一个能够支撑极速、实时、统一分析的数据底座，已成为企业在 AI 时代保持竞争力的关键。</p><h4>StarRocks 进化：实时与统一</h4><p>技术演进的根本目标在于解决客户的业务问题。StarRocks 的发展路径正是这一理念的体现，从解决 BI 查询慢，到实现业务实时洞察，再到构建统一的湖仓分析能力。今天，<strong>StarRocks 4.0 聚焦AI实时湖仓”这一场景</strong>，成为了一个为 AI Agent 应用场景做好充分准备的数据引擎。</p><h4>StarRocks 4.0 核心能力：为 AI 湖仓注入强劲动力</h4><p>StarRocks 4.0 带来了多项重大改进，旨在全面提升分析性能和用户体验：</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLj" alt="640 (1).jpg" title="640 (1).jpg" loading="lazy"/></p><ul><li><strong>极致性能，加速洞察</strong>：通过深度的算子优化，StarRocks 将 Iceberg 数据湖上的分析性能提升了 60%，JSON 数据处理速度提升 3-15 倍。这意味着企业的业务团队可以更快地从海量数据中获得洞察，抢占市场先机。</li><li><strong>把握当下，实时加速</strong>：通过优化数据导入机制，<strong>对云存储的 API 调用降低了 70%-90%</strong>，实现了数据的秒级可见与分析。无论是实时风控、动态定价还是即时营销，企业的决策都可以基于最新的业务动态。</li><li><strong>极致统一，简化架构</strong>：StarRocks 提供原生的湖仓一体能力，支持对 Iceberg、Hudi、Paimon 等主流数据湖格式进行高性能读写，并以统一的 Catalog 管理权限，帮助企业告别繁杂的 ETL 和多套组件并存现状，构建一个开放、统一、高效的 One Data 体系。同时，通过全面向量化、CBO、现代化物化视图等上百项优化，为全场景分析提供性能保障。</li></ul><h4>落地实践：StarRocks 在多元场景中的应用</h4><p>StarRocks 已在金融、零售、制造等行业的头部企业中得到广泛应用，并取得了显著成效。</p><p><strong>金融：赋能大型城商行，实现架构现代化</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLk" alt="640 (5).jpg" title="640 (5).jpg" loading="lazy"/></p><p>该行原有的经典Lambda架构组件繁多、维护成本高昂，多套 OLAP 引擎并存导致数据孤岛问题严重。通过引入 StarRocks 作为统一分析引擎，替代了 Impala、HBase、Kylin 等多套系统。利用 StarRocks 直接分析内表与 Hive/Iceberg 外表，并借助物化视图等特性进行全场景加速。数据架构与 Pipeline 极大简化，运维成本显著降低，报表、驾驶舱、探查分析等所有应用均实现了极速响应。</p><p><strong>即时零售：支撑淘宝闪购，赢得即时零售战役</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLl" alt="640 (2).jpg" title="640 (2).jpg" loading="lazy"/></p><p>在竞争激烈的即时零售业务中，对实时决策、高并发查询和复杂多维分析的性能要求极高。淘宝闪购采用“Paimon + StarRocks”的实时湖仓架构，利用 StarRocks 对外表、分层物化视图的透明加速能力，实现了多维分析、UV 统计等复杂场景的极速响应，<strong>分析性能提升 10 倍、存储成本降低 60%</strong>，<strong>并稳定支撑每日超 17 万次的高并发查询。</strong></p><p><strong>电商：助力电商平台，迈向智能 BI 新阶段</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLm" alt="640 (3).jpg" title="640 (3).jpg" loading="lazy"/></p><p>该电商平台希望从“敏捷 BI”向“智能 BI”（ChatBI）演进，但面临技术栈复杂、多模态数据检索困难等问题。项目最终选用 StarRocks 作为统一数据底座。它不仅承载指标查询，同时凭借其内置的文本与向量检索能力，统一替代了原有的 ES 和 Milvus，简化了平台技术栈。该智能 BI 平台上线一年来，月活用户超百人，月均问答超 3000 次，<strong>准确率高达 90% 以上</strong>，有效降低数据分析门槛。</p><h4>未来：迈向 AI 增强的智能数据分析</h4><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLn" alt="640 (4).jpg" title="640 (4).jpg" loading="lazy"/></p><p>作为 StarRocks 开源项目在中国的主要贡献者和推广者，镜舟科技深知企业级客户对稳定性、安全性及专业服务的更高要求。通过企业版产品镜舟数据库，在 StarRocks 内核能力的基础上，增加了安全管控、灾备、可视化运维等企业级功能，致力于帮助更多企业构建卓越的数据分析系统，共同拥抱数据驱动的智能未来。</p><p>展望未来，镜舟科技将持续推动 StarRocks 在 AI 增强领域的创新，提供智能建模、AI 增强分析闭环等能力，让数据分析变得更加智能、自动化。</p><p>镜舟科技致力于成为企业在 AI 时代最值得信赖的数据技术伙伴，与客户携手共筑开放、统一、高性能的 AI 实时湖仓，共同迈向数据驱动的智能未来。</p>]]></description></item><item>    <title><![CDATA[Spec Kit 踩坑实录：为什么我严格]]></title>    <link>https://segmentfault.com/a/1190000047437278</link>    <guid>https://segmentfault.com/a/1190000047437278</guid>    <pubDate>2025-11-28 21:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>引言：完美的流程，崩塌的结果</h2><p>最近我在使用 Spec Kit 做需求开发。这套工具宣称通过标准化的 7 个步骤——<strong>Specify（定义）、Clarify（澄清）、Plan（计划）、Tasks（任务）、Analyze（分析）、Checklist（检查）、Implement（实现）</strong>——来生成高质量代码。</p><p>我的初衷是美好的：输入需求，喝杯咖啡，输出代码。</p><p>但现实给了我一记响亮的耳光。当我满怀期待地跑到第 7 步 <code>Implement</code> 时，生成的代码完全不可用：数据库方言错了、重复造了已有的轮子、业务逻辑不仅没对齐甚至还跑偏了。</p><p>这让我意识到一个残酷的真相：<strong>在 AI 辅助编程中，如果缺乏严格的每一步把控，AI 就会陷入“幻觉”，原本的“自动驾驶”会变成“事故现场”。</strong></p><p>今天这篇博客，就是一次血泪复盘。我总结了 Spec Kit 流程中的“避坑指南”，希望大家能少走弯路。</p><hr/><h2>核心复盘：蝴蝶效应与“全自动”陷阱</h2><p>我在这次实践中最大的教训是：<strong>不要相信 AI 的默认假设。</strong></p><p>Spec Kit 的流程是一个链式反应。第 1 步的 <code>Specify</code> 如果有 1% 的偏差，到了第 3 步 <code>Plan</code> 就会变成技术选型的错误，到了第 7 步 <code>Implement</code> 就会变成几百行完全跑不通的垃圾代码。这就是 AI 编程的<strong>蝴蝶效应</strong>。</p><p>以下是具体的踩坑现场与应对策略：</p><h3>1. Specify &amp; Clarify（定义与澄清）：别只顾着答题</h3><p><strong>❌ 踩坑现场：</strong><br/>在第 2 步 <code>Clarify</code> 时，AI 会像面试官一样问我不清楚的地方。我当时只顾着回答它的问题，却忽略了检查它基于第一步生成的 <code>specify.md</code>。<br/><strong>结果：</strong> AI 在文档里悄悄“脑补”了一些我没提到、但它认为合理的逻辑，或者定义了错误的数据结构。</p><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>回溯检查</strong>：不要只看 Question，一定要回头精读 <code>specify.md</code> 的 Summary 和 Description。</li><li><strong>不仅仅是澄清</strong>：除了回答 AI 的问题，还要主动思考：“你生成的文档是否包含了我没说、但我默认你该知道的约束？”如果不包含，立刻补充。</li></ul><h3>2. Plan（技术方案）：张冠李戴的重灾区</h3><p><strong>❌ 踩坑现场：</strong><br/>这是最容易翻车的地方。AI 往往不知道你项目的具体技术栈细节。</p><ul><li><strong>案例 A</strong>：我的项目明明是 <strong>PostgreSQL</strong>，AI 生成的 Plan 里却打算用 <strong>MySQL</strong> 的方言写 SQL，甚至引入 MySQL 的驱动依赖。</li><li><strong>案例 B</strong>：项目里规定用 MyBatis-Plus，AI 却在 Plan 里规划了一套 JPA 的实体类。</li></ul><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>上下文注入</strong>：在这一步，必须强制检查技术栈。</li><li><strong>明确否决</strong>：如果 Plan 里出现了错误的技术选型（例如用错了数据库、ORM 框架），必须立刻打回重做，绝不能想着“生成代码后再改”。<strong>Plan 错了，后面的代码不仅是错的，还是不可挽回的错。</strong></li></ul><h3>3. Tasks（任务拆解）：拒绝重复造轮子</h3><p><strong>❌ 踩坑现场：</strong><br/>AI 作为一个“外来户”，它不知道你项目里有什么。</p><ul><li><strong>现象</strong>：AI 生成的 Tasks 里，赫然列着“配置数据库连接池”、“编写 Redis 工具类”、“搭建日志切面”。</li><li><strong>后果</strong>：这些基础设施（Infrastructure）在现有代码里早就有了！如果照着执行，你的项目里就会出现两个 RedisUtil，两套鉴权逻辑，导致代码极其臃肿甚至冲突。</li></ul><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>做减法</strong>：毫不留情地删除所有“基建类”任务。</li><li><strong>强制复用</strong>：在 Task 描述里明确标注：“使用现有的 <code>com.example.common.RedisUtil</code>，不要新建”。</li></ul><h3>4. Checklist（检查清单）：最后的防线</h3><p><strong>❌ 踩坑现场：</strong><br/>经历了前面几步的折腾，人容易产生疲劳感。到了第 6 步 <code>Checklist</code>，我当时的心态是：“差不多得了，快生成代码吧。” 于是看都没看细则就点了通过。<br/><strong>结果：</strong> AI 生成代码时彻底放飞自我，刚才在 Plan 里没拦住的错误，在这里全部变成了具体的 Bug。</p><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>逐条审计</strong>：这步必须一个一个检查，不要放过任何疑点。</li><li><strong>脑内预演</strong>：看到 Checklist 时，脑子里要预演一遍生成的代码大概长什么样。如果这一步没把控住，AI 就会偏离你的初衷。<strong>只有这一步对了，Implement 才会是你想要的代码。</strong></li></ul><hr/><h2>方法论总结：从“指挥官”转变为“审计员”</h2><p>通过这次踩坑，我总结了一套使用 Spec Kit（或其他 AI 编程 Agent）的标准作业程序（SOP）：</p><h3>1. 角色转换</h3><p>不要把自己当成只会下命令的<strong>指挥官（Commander）</strong>，要把自己当成极其严格的<strong>代码审计员（Auditor）</strong>。AI 是实习生，你是 Tech Lead。</p><h3>2. 前置约束（Pre-Prompt）</h3><p>不要等到 AI 犯错再改。在开始流程前，最好贴一段<strong>“项目上下文声明”</strong>：</p><blockquote><p><strong>Project Context:</strong></p><ul><li><strong>DB</strong>: PostgreSQL (Not MySQL)</li><li><strong>ORM</strong>: MyBatis-Plus</li><li><strong>Infra</strong>: 禁止创建新的 DB/Redis 配置，必须复用 <code>src/common</code> 下的代码。</li></ul></blockquote><h3>3. 零容忍原则</h3><p><strong>每一步都需要严格把控，一个问题都不要放过。</strong><br/>如果在 <code>Plan</code> 阶段发现一个小偏差，不要幻想 AI 在 <code>Implement</code> 阶段会自动修正。相反，这个偏差会被放大 10 倍。</p><h3>4. 持续迭代你的 <code>prompt.md</code> 资产</h3><p>这是最高阶的玩法。<strong>AI 犯过的错，不要让它犯第二次。</strong><br/>Spec Kit 的核心配置通常在于 <code>prompt.md</code> 文件。每当你发现 AI 在某个环节踩坑（比如总是忘记加 <code>@Builder</code> 注解，或者总是想自己写 Util 类），<strong>请立刻将这条规则补充进你的 <code>prompt.md</code> 中。</strong><br/>随着你不断地“喂养”和修缮这个文件，它会变成一份<strong>“不懂你的人绝对用不好的”</strong>核心技术资产。你的 AI Agent 会越来越懂你的代码风格，效率也会呈现指数级上升。</p><h2>结语</h2><p>AI 编程工具确实能提升效率，但它目前还做不到“完全托管”。<strong>它能帮我们省去写样板代码的手速，但无法替代我们对技术方案的判断力。</strong></p><p>如果你也想用 Spec Kit 做出想要的代码，请记住：<strong>时刻保持警惕，把每一步的输出都当成必须要 Code Review 的代码来看待，并不断打磨你的 Prompt 资产。</strong></p><p>只有这样，AI 才能成为你的神队友，而不是那个坑你的实习生。</p><p>本文由<a href="https://link.segmentfault.com/?enc=U6vhj4QhA%2B9qLy%2FdsdM8hg%3D%3D.IJiUxVZjhZfoCAtkWoS9TBMhUmUOMh0gp8VoNWv%2Fpkc%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[# 好用且功能强大的自建CA证书服务器工]]></title>    <link>https://segmentfault.com/a/1190000047437288</link>    <guid>https://segmentfault.com/a/1190000047437288</guid>    <pubDate>2025-11-28 21:03:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>中文名：猪猪侠之CA服务器</p><p><strong>项目地址：<a href="https://link.segmentfault.com/?enc=gAvD%2BqsKB4DZb2Dq0RaGjQ%3D%3D.%2F5qvJRKYf3HSIbqnCxqCE5SWa1%2F4%2B2PEsh0XW%2FJgW3ctE9yGuY%2FAvdTPDGIRhpAkcHnxkdUZqHEQqfJKqzDV%2Fg%3D%3D" rel="nofollow" target="_blank">https://gitee.com/zhf_sy/zzxia-openssl-ca-server</a></strong></p><p>这基于openssl的CA证书服务器。你可以用它搭建自己的专属CA服务器，以方便为用户生成私钥、证书请求、颁发证书、吊销证书、证书续期、证书吊销列表等。它可以生成多种类型的证书，包括且不限于web服务器、代码、计算机、客户端、信任列表、时间戳、IPSec、Email、智能卡登陆及其他OID证书。只需简单在配置文件中指定即可，证书完全兼容与Windows、Linux、Android、iOS等PC及手机系统（自签名不兼容）。项目是产品化的，不用修改代码就可以管理CA服务器整个生命周期，计划未来增加web操作页面，实现用户从网页端申请、下载、续期等证书操作，以及证书吊销列表的分发。</p><p>[toc]</p><h2>1 介绍</h2><h3>1.1 背景</h3><p>由于现在https的盛行，我们经常需要在内网服务器、手机、PC上使用证书（内网域名没法使用免费的Letsencrypt证书），但多数时候大家只会生成自签名证书，不会以CA的方式颁发证书，更不会让用户安装CA证书，造成用户在使用过程中总是提示不安全，浪费时间且体验非常糟糕，再者，颁发证书的相关信息从来不保存，不具延续性，不是正经人的做法，哈哈哈哈哈哈哈哈哈！</p><p>另外：OpenSSL证书相关知识还是有点复杂的（虽然一般用的很简单），特别是一些概念，很多人搞不清用途与区别，所以生成较为复杂的证书就会走一些弯路（有别于简单的自签名证书），希望这个可以帮到你。也可以帮到我自己，免得要用的时候又得折腾，因为长时间不用，容易遗忘，算是知识的固化吧。</p><h3>1.2 功能</h3><ol><li>初始化CA服务器</li><li>一步生成CA服务器私钥及证书</li><li>一步生成用户私钥及证书</li><li>分开步骤，分别为用户生成【私钥、证书请求、证书】</li><li>为第三方证书请求颁发证书</li><li>为证书续期</li><li>吊销证书</li><li>生成CA证书吊销列表</li></ol><h3>1.3 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk； <img referrerpolicy="no-referrer" src="/img/remote/1460000047436584" alt="打赏" title="打赏"/></li></ol><h2>2 软件架构</h2><p>Linux shell</p><h3>2.1 设计理念</h3><ul><li>用Openssl搭建CA服务器</li><li>信任环境：在CA服务器上为用户生成私钥与证书</li><li>非信任环境：用户自己生成私钥与证书请求，将证书请求给到CA服务器，CA服务器根据用户提供的证书请求文件为用户生成证书（私钥一般是需要保密的，把自己假想成了NB的公共证书颁发者了）</li></ul><h3>2.2 目录结构</h3><blockquote>初始化后的目录结构：</blockquote><pre><code class="bash">$ tree
.
├── 0-init_ca.sh
├── 1-generate_CA_key_and_crt.sh
├── blog-自建CA及证书颁发-old.md
├── crlnumber
├── function.sh
├── index.txt
├── key_usage.md
├── LICENSE
├── m-1-generate_user_key.sh
├── m-2-generate_user_csr.sh
├── m-3-generate_user_crt.sh
├── m-3in1-generate_user_key-csr-crt.sh
├── m-x-revoke_user_crt.sh
├── m-x-renew_user_crt.sh
├── m-x-generate_CA_crl.sh
├── my_conf
│   ├── env.sh--CA.sample
│   ├── env.sh--model
│   └── env.sh--test.lan
├── README.md
└── serial

1 directory, 17 files</code></pre><h2>3 安装教程</h2><p>克隆到服务器上即可。 需要安装Linux 软件包<code>expect</code>。 在ubuntu上测试通过，理论上只要是基于Linux内核都行</p><h2>4 使用说明</h2><p>所有脚本都提供了<code>$0 -h|--help</code>参数，查看帮助即可。</p><h3>4.1 搭建CA</h3><ol><li>运行<code>./0-init_ca.sh -y</code>进行初始化</li><li>基于<code>./my_conf/env.sh--CA.sample</code>创建<code>./my_conf/env.sh--CA</code>CA的环境变量文件</li><li>运行<code>1-generate_CA_key_and_crt.sh -y</code>以生成CA服务器私钥与自签名证书</li></ol><blockquote>以上根据自己的信息填写即可</blockquote><h3>4.2 日常使用（为用户生成私钥、证书请求、证书）</h3><blockquote>运行脚本前，请先查看帮助，帮助中有相关脚本的依赖文件、参数说明及示例！ 多数脚本须依赖基于<code>./my_conf/env.sh--model</code>创建的<code>./my_conf/env.sh--证书相关名称</code>环境变量文件，仓库中提供了一个示例（test.lan）<code>./my_conf/env.sh--test.lan</code>供参考。</blockquote><h4>4.2.1 一步为用户生成私钥、证书请求、证书</h4><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;4(证书相关名称.key)
1--&gt;2(env.sh--证书相关名称)
2--&gt;3(openssl.cnf--证书相关名称)
3--&gt;5(证书相关名称.csr)
4--&gt;5
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-3in1-generate_user_key-csr-crt.sh -h

    用途：用于生成用户秘钥与证书
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建
    注意：
    用法:
        ./m-3in1-generate_user_key-csr-crt.sh  [-h|--help]
        ./m-3in1-generate_user_key-csr-crt.sh  [-n|--name {证书名称}]  &lt;-p|--privatekey-bits {私钥长度}&gt;  &lt;-c|--cert-bits {证书长度}&gt;  &lt;-d|--days {证书有效天数}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -p|--privatekey-bits  私钥长度，默认2048
        -c|--cert-bits 证书长度，默认2048
        -d|--days      证书有效期，默认365天
        -q|--quiet     静默方式运行
    示例:
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com
        #
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -d 730
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -p 4096
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -p 4096  -c 2048  -d 730
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -q</code></pre><h4>4.2.2 分步骤为用户生成私钥、证书请求、证书</h4><ol><li>生成私钥：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
1(证书相关名称)
1--&gt;4(证书相关名称.key)</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-1-generate_user_key.sh -h

    用途：用于生成用户秘钥
    依赖：
        ./function.sh
    注意：
    用法:
        ./m-1-generate_user_key.sh  [-h|--help]
        ./m-1-generate_user_key.sh  [-n|--name {证书相关名称}]  &lt;-p|--privatekey-bits {私钥长度}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -p|--privatekey-bits  私钥长度，默认2048
        -q|--quiet     静默方式运行
    示例:
        ./m-1-generate_user_key.sh  -n test.com
        ./m-1-generate_user_key.sh  -p 4096  -n test.com
        ./m-1-generate_user_key.sh  -q  -n test.com</code></pre><ol><li>生成证书请求：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
1(证书相关名称)
1--&gt;2(env.sh--证书相关名称)
2--&gt;3(openssl.cnf--证书相关名称)
3--&gt;5(证书相关名称.csr)
1--&gt;4(证书相关名称.key)
4--&gt;5</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-2-generate_user_csr.sh -h

    用途：用于生成用户证书请求
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建
    注意：
    用法:
        ./m-2-generate_user_csr.sh  [-h|--help]
        ./m-2-generate_user_csr.sh  [-n|--name {证书相关名称}]  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -q|--quiet     静默方式运行
    示例:
        ./m-2-generate_user_csr.sh  -n test.com
        ./m-2-generate_user_csr.sh  -n test.com -q</code></pre><ol><li>颁发证书（证书第一次颁发、证书续期重新颁发）：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;3(openssl.cnf--证书相关名称)
1--&gt;5(证书相关名称.csr)
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><p>或者：</p><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;3(openssl.cnf--证书相关名称)
5(来自外部.csr)--&gt;3
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-3-generate_user_crt.sh -h

    用途：用于颁发或更新用户证书
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建，当使用外部证书请求文件时，无须此配置文件
    注意：
    用法:
        ./m-3-generate_user_crt.sh  [-h|--help]
        ./m-3-generate_user_crt.sh  [-n|--name {证书相关名称}]  &lt;-c|--cert-bits {证书长度}&gt;  &lt;-d|--days {证书有效天数}&gt;  &lt;-f|--csr-file {证书请求文件}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -f|--csr-file  指定外部用户证书请求文件。一般只有在用户使用其他工具生成证书请求时使用此项
        -c|--cert-bits 证书长度，默认2048
        -d|--days      证书有效期，默认365天
        -q|--quiet     静默方式运行
    示例:
        ./m-3-generate_user_crt.sh  -n test.com
        #
        ./m-3-generate_user_crt.sh  -c 4096  -n test.com
        ./m-3-generate_user_crt.sh  -d 730   -n test.com
        ./m-3-generate_user_crt.sh  -c 4096  -d 730  -n test.com
        # 第三方证书请求
        ./m-3-generate_user_crt.sh  -f /path/to/xxx.csr  -n xxxxx
        ./m-3-generate_user_crt.sh  -c 4096  -d 730  -f /path/to/xxx.csr  -n xxxxx</code></pre><blockquote>如果曾经颁发的证书过期了，只需再次运行<code>m-3-generate_user_crt.sh</code>就可以了，为了便于用户理解，增加了个软连接名称<code>m-x-renew_user_crt.sh</code>。</blockquote><h3>4.3 其他使用</h3><h4>4.3.1 更新（renew）用户证书</h4><p>等同【4.2.2 - 3】为用户生成证书，请参考</p><h4>4.3.2 吊销（revoke）用户证书</h4><pre><code class="bash">./m-x-revoke_user_crt.sh</code></pre><h4>4.3.3 吊销（revoke）用户证书</h4><pre><code class="bash">./m-x-generate_CA_crl.sh</code></pre><h2>5 最后</h2><p>Enjoy！</p>]]></description></item><item>    <title><![CDATA[K-Mind 行业数智大脑：破解企业 A]]></title>    <link>https://segmentfault.com/a/1190000047437305</link>    <guid>https://segmentfault.com/a/1190000047437305</guid>    <pubDate>2025-11-28 21:02:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>大模型时代的机遇与困境：高成本与低落地率的矛盾</strong></h2><p>在人工智能技术飞速发展的当下，大模型已成为推动产业变革的核心力量，但其高昂的研发成本与落地挑战却让众多企业望而却步。除了资金投入，模型训练还面临数据质量要求高、模态结构复杂、算力消耗大、耗时久等多重难题，这些都成为企业入局大模型的 "高门槛"。</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdncLS" alt="" title=""/></p><p>2016-2023 年部分人工智能模型训练成本估算（图片来源于网络）</p><p>成本压力之下，企业 AI 工程化落地率同样不容乐观。麦肯锡调研数据显示，仅有 1% 的受访企业认为他们的 AI 投资已经达到成熟阶段。深入分析可见，企业在大模型落地过程中面临三重核心挑战：技术层面，模型微调需海量高质量数据，部署时集成兼容难度大，GPU 等异构资源调度效率低；应用层面，通用大模型 "幻觉" 控制难、行业知识薄弱，且评估体系复杂、安全要求高；业务层面，运维可靠性与稳定性难以保障，维护成本居高不下，难以匹配企业个性化业务需求。</p><h2><strong>破局之道：从通用大模型到行业专属模型的转型</strong></h2><p>面对企业 AI 落地困境，浪潮开务提出核心解决方案------推动大模型从 "通用" 向 "行业专属" 演进。通过对比可见，通用大模型虽覆盖范围广，但存在资金与研发投入大、行业适配性弱的劣势；行业垂域大模型依托专用行业引擎，能以更低成本实现行业适配；而企业专属大模型则可贴合企业自身特色，借助客户私有数据实现更敏捷、更快速的落地。</p><p><img width="723" height="355" referrerpolicy="no-referrer" src="/img/bVdncLT" alt="" title="" loading="lazy"/></p><p>通用大模型与行业垂域大模型、企业专属大模型优劣对比</p><p>这一转型的关键在于构建 "小算力 + 海量领域高质量数据" 的模式。浪潮KaiwuDB 以 KaiwuDB 3.0 为数据底座，结合多模态数据治理与合规能力，将分散的设备数据、业务数据、外部接口数据转化为 "AI-ready Data"，为模型训练与微调提供高质量数据源。同时，通过强化模型运营能力、优化资源调度策略、完善监控维护体系，不仅降低了整个模型系统的成本，更确保模型在动态环境中始终保持高效稳定，解决了企业 "如何获取管理训练数据"、"如何应对访问量波动"、"如何快速部署推理服务" 等核心疑问。</p><h2><strong>K-Mind：物联网行业大脑的全链路赋能</strong></h2><h4>K-Mind</h4><p>浪潮开务物联网行业数智大脑(K-Mind)是由时序、语言、视觉、图学习、科学计算、决策优化等六大核心基础模型协同工作的物联网行业大模型有机体。K-Mind 可为上层应用提供"感知-认知-决策-优化"的全链路 AI 能力,最终助力客户在物联网场景,如能源、水利、矿山等关键领域快速、低成本地实现AI工程化落地，并为业务的重构与智能化提供高效赋能。</p><p><img width="723" height="380" referrerpolicy="no-referrer" src="/img/bVdncLU" alt="" title="" loading="lazy"/></p><p>K-Mind 行业数智大脑架构图</p><p>浪潮KaiwuDB 依托 KaiwuDB 与浪潮开务物联网行业数智大脑 K-Mind，以 "数据洞见未来" 为核心，为企业 AI 工程化落地提供了全链路解决方案，助力能源、水利、矿山等关键领域迈入全域智能新时代。</p><p>以油气行业为例，K-Mind 油气行业大脑可接入石油实时产量数据、设备数据、水电气数据、人员数据等多源数据，经过数据清洗、聚合、实时计算等处理环节，结合油田生产经验数据与行业知识，实现产油能效评测、生产异常告警、产量预测等核心应用。</p><p><img width="723" height="281" referrerpolicy="no-referrer" src="/img/bVdncLV" alt="" title="" loading="lazy"/><br/>油田 K-Mind 架构图</p><p>从技术架构来看，K-Mind 采用分层设计，底层依托 KaiwuDB 分布式多模数据库与数据湖仓，实现多模态数据的高效存储与计算；中间层通过多模态数据治理与合规、语义层构建，将原始数据转化为 AI-ready 数据，为模型训练提供高质量数据支撑；上层则基于行业知识库、行业算法和模型库，构建行业智能体，通过 API/SDK 为应用层提供服务。</p><p><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdncLW" alt="" title="" loading="lazy"/></p><p>以 K-Mind 为核心的基建平台链路图</p><p>在能源行业，K-Mind 能源业务大脑能够支撑虚拟电厂（VPP）的运行，通过电价、负荷、出力预测，辅助交易决策，实现调度优化与故障诊断。</p><p><img width="723" height="303" referrerpolicy="no-referrer" src="/img/bVdncLX" alt="" title="" loading="lazy"/><br/>能源行业链路示意图</p><h2><strong>快、省、准、深------迈向全域智能的 "高速路"</strong></h2><p>K-Mind 物联网行业大脑之所以能够高效赋能企业 AI 工程化落地，源于其四大核心优势。其一，深厚的行业积累，浪潮在制造、交通、能源等物联网领域沉淀了丰富经验，能精准匹配行业需求；其二，完善的交付服务体系，从数据治理到模型部署，提供全流程支持；其三，强大的生态构建能力，内部可提供多元化产品满足不同用户需求，外部通过 API/SDK 实现灵活集成；其四，严格的安全保障，确保企业数据与业务运营的合规性与安全性。</p><p>K-Mind 正以 "<strong>快、省、准、深</strong>" 为核心，构建企业 AI 工程化落地的 "高速路"------"快" 即模型训练快、业务落地快，大幅缩短数智化转型周期；"省" 即省资源、省人力，通过高效资源调度与自动化运维降低成本；"准" 即行业适配准、经验沉淀准、模型预测准、业务结果准，贴合企业实际需求；"深" 即纵向深耕电力、石油、天然气、水务、矿山、冶金等领域，横向拓展辅助交易决策、故障诊断、智能运维等场景，实现全产业、全链路的智能升级。</p><p>目前，浪潮开务物联网行业数智大脑 K-Mind 已正式发布，企业可通过<strong>扫描下方二维码填写问卷</strong>获取并体验最新技术内容，抢先体验数智化转型的核心动力。未来，浪潮KaiwuDB 将持续以数据为基、以 AI 为翼，与企业携手共赴物联网全域智能的未来，让数据的价值在更多场景中绽放，推动产业高质量发展。</p><p><img width="280" height="280" referrerpolicy="no-referrer" src="/img/bVdncLY" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[《iOS相机/定位的精准适配指南》 程序]]></title>    <link>https://segmentfault.com/a/1190000047437324</link>    <guid>https://segmentfault.com/a/1190000047437324</guid>    <pubDate>2025-11-28 21:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数开发者在推进功能开发时，往往将重心放在镜头捕捉的流畅度、定位数据的实时性等显性指标上，却忽视了权限声明作为系统与应用达成交互共识的底层逻辑—它绝非简单的文本补充，而是应用融入iOS生态的前置承诺，是用户与应用建立信任关系的初始触点。曾在优化一款场景化服务类功能时，投入大量精力打磨定位与影像的协同体验，确保在不同光线、地形条件下的数据稳定性，却在实际验证过程中发现，部分场景下功能虽能启动却无法获取核心数据，既无系统提示，也无异常反馈，这种看似“功能正常却无效”的现象，起初让人陷入困惑，反复排查后才意识到，问题的根源在于权限声明未能精准传递功能的核心意图，导致系统在后台校验时未能建立有效的信任链路。这种经历让我深刻体会到，权限声明的配置绝非技术流程中的次要环节，而是对iOS权限机制设计哲学的深度理解，它要求开发者跳出功能实现的单一维度，站在系统安全架构、用户隐私诉求、生态交互规则的多重角度，重新审视每一处配置的深层意义。</p><p>权限声明的本质，是应用向系统与用户传递行为意图的语义载体，而相机与定位权限的特殊性，在于其直接关联设备硬件的调用权限与用户的核心隐私边界。iOS系统对这类权限的管控逻辑，早已超越了“允许”或“拒绝”的二元判断，而是构建了一套基于场景感知、意图验证、信任累积的动态评估体系。Info.plist中的声明文本，每一个表述都承载着系统对应用行为的预判依据，也影响着用户对权限使用合理性的判断标准，它需要在功能需求与用户认知之间找到精准的平衡点，既不能过于抽象导致系统无法识别场景，也不能过于繁琐引发用户的授权抵触。例如，同样是相机权限的声明，用于证件扫描功能时，需要突出“信息采集的准确性与安全性”，让用户明确权限使用的核心目的是为了快速提取有效信息；用于风景拍摄功能时，则应强调“场景记录的完整性与个性化”，契合用户对生活内容留存的需求；而用于AR互动功能时，需聚焦“虚实融合的沉浸式体验”，让用户理解权限调用对功能实现的必要性。这种细微的语义差异，不仅决定了系统在权限校验时的判定结果，更影响着用户在授权弹窗出现时的心理决策—当声明文本能够精准呼应用户的使用预期，用户的授权意愿会显著提升，反之则可能引发抵触情绪，甚至直接影响对应用的整体信任度。在长期的实践中发现，权限声明的语义精准度，往往与功能的实际使用效果形成隐性关联，那些能够清晰、真诚传递行为意图的声明，不仅能减少系统的隐性拦截，更能让用户在使用过程中感受到被尊重，从而建立起对应用的长期信任。</p><p>深入探究iOS的权限机制设计，会发现Info.plist中的声明配置，实则是系统权限管控链路的起点，它与应用的功能模块设计、用户交互流程、隐私保护策略形成了环环相扣的生态闭环。系统在处理相机或定位权限请求时，并非仅简单校验声明是否存在，而是会结合声明文本的语义指向、功能调用的时机与场景、用户的历史授权行为、应用的整体口碑等多维度信息，进行综合信任评估。这种评估机制的底层逻辑，是iOS对用户隐私保护的极致践行，也是对应用开发规范的刚性约束，它要求应用的每一次权限调用都具备合理的场景支撑与明确的意图说明。在实践中曾遇到过这样的情况：一款应用的相机功能在测试环境中运行稳定，所有权限声明均已按常规配置，但在用户反馈中却出现部分设备无法正常调用的问题，且这类问题集中在特定iOS版本与机型的组合中。经过多轮排查与测试，最终发现问题的根源在于声明文本中存在模糊表述，导致系统在特定版本的权限评估逻辑中，无法将声明意图与实际功能场景精准匹配，从而触发了隐性的权限拦截。这一经历让我深刻认识到，权限声明的配置并非一劳永逸的静态操作，而是需要随着系统版本的迭代、功能场景的拓展、用户需求的变化进行动态优化。开发者需要持续关注iOS系统的更新日志，深入理解每一次权限机制调整的底层逻辑，同时结合应用的实际使用数据，分析用户授权行为的变化趋势，不断优化声明文本的表述方式，确保其始终与系统的评估逻辑同频，与用户的认知预期同步。</p><p>权限声明的优化过程，本质上是开发者对功能场景与用户需求的深度解构与重构，它要求开发者跳出技术实现的思维定式，站在用户的视角审视每一处表述的合理性与真诚度。在进行相机权限声明的优化时，不能简单套用“需要访问相机”这类泛化表述，而应深入挖掘功能的核心价值与用户的真实使用场景—用于文档扫描功能时，需明确“为快速提取文档信息，提升办公效率”；用于美食拍摄功能时，可强调“为记录食材细节，生成个性化烹饪指南”；用于社交分享功能时，则应突出“为捕捉生活瞬间，实现好友间的情感传递”。每一种场景对应的声明文本，都应具备独特的语义指向，让用户在看到授权弹窗的瞬间，就能清晰理解权限使用的必要性与价值所在。定位权限的声明优化更是如此，不同的功能场景对定位精度的要求不同，声明文本也应随之调整：用于本地生活服务推荐时，需说明“为匹配周边优质资源，提供精准的生活建议”；用于运动轨迹记录时，应强调“为完整呈现运动数据，助力科学健身规划”；用于旅行导航功能时，则需明确“为实时规划最优路线，提升出行的便捷性”。这种基于场景的精准声明，不仅能有效降低用户的授权顾虑，提升授权转化率，更能让系统在权限管控过程中，准确把握应用的行为边界，从而减少不必要的拦截与限制。同时，声明文本的表述风格也应贴近用户的日常语言习惯，避免使用过于专业的技术术语，以真诚、简洁的方式传递核心信息，让用户感受到开发者对其隐私的重视与对使用体验的用心。</p><p>在权限声明的实践过程中，系统版本迭代带来的隐性变化是开发者必须关注的核心变量，这种变化往往体现在权限评估逻辑的细微调整上，需要通过持续的测试与总结，捕捉其中的规律与趋势。不同iOS版本对权限声明的语义解析能力、场景匹配精度、用户交互反馈都存在差异，某些在旧版本中能够正常使用的声明文本，在新版本中可能会因为语义模糊、意图不明确而被系统判定为不合理，从而影响功能的正常使用。因此，在完成权限声明配置后，开发者不能仅在单一版本中进行测试，而应覆盖多个主流版本，甚至包括测试版与预览版，通过模拟不同用户的授权行为、功能调用场景、设备使用环境，全面验证声明文本的兼容性与有效性。同时，还应建立完善的用户反馈收集机制，通过应用内反馈渠道、社区讨论、数据分析等多种方式，挖掘与权限声明相关的潜在问题—比如某些用户频繁拒绝授权，可能是因为声明文本未能清晰传递功能价值；某些功能在特定机型上出现调用异常，可能是因为声明文本与该机型的系统权限逻辑存在冲突；某些用户授权后仍无法正常使用功能，可能是因为声明文本与实际功能场景存在偏差。针对这些问题，需要进行针对性的优化调整，不断迭代声明文本的表述方式，确保其始终符合系统要求与用户预期。此外，还可以借鉴行业内的优秀实践，分析同类应用在权限声明上的表述方式与优化路径，但并非简单模仿，而是结合自身应用的功能特性与用户群体，形成具有独特性与适配性的声明方案，让权限声明成为应用差异化竞争的隐性优势。</p><p>权限声明的深层价值，早已超越了单纯的技术配置要求，成为应用生态适配、用户信任构建、品牌口碑沉淀的核心环节。在iOS生态日益强调隐私保护与用户体验的当下，权限声明不再是可有可无的辅助配置，而是应用能否获得系统认可、用户青睐的重要前提。一个精准、清晰、真诚的权限声明，能够让应用在系统权限管控的框架内，最大限度地发挥功能价值，同时也能让用户在使用过程中，感受到开发者对隐私保护的重视与对用户体验的尊重。这种尊重与重视，最终会转化为用户对应用的信任与依赖，成为应用核心竞争力的重要组成部分。在长期的开发实践中，我逐渐意识到，权限声明的优化过程，也是开发者技术认知不断深化、思维模式不断升级的过程，它要求我们跳出代码与功能的表层，站在生态、用户、系统的多重角度，审视每一个开发细节，培养一种全面、系统、动态的思考方式。这种思考方式不仅能解决权限声明相关的具体问题，更能迁移到应用开发的其他环节，帮助我们更好地应对复杂的技术挑战，构建出更符合生态规则、更贴近用户需求、更具核心竞争力的应用产品。</p>]]></description></item><item>    <title><![CDATA[《音频格式优化的底层逻辑：场景拆解与解码]]></title>    <link>https://segmentfault.com/a/1190000047437327</link>    <guid>https://segmentfault.com/a/1190000047437327</guid>    <pubDate>2025-11-28 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数开发者聚焦于播放控制逻辑、音效算法迭代等显性模块，却忽略了格式未优化引发的隐性体验损耗—它并非仅体现为文件体积冗余，而是在设备差异、网络波动、使用场景切换中触发的感知断层。曾在一次多场景音频适配测试中观察到，同一段音频在旗舰机型上呈现出清晰通透的听感，在中端设备上却伴随若隐若现的底噪与播放迟滞，而在弱网环境下加载进度条的缓慢蠕动更让体验断崖式下跌。起初将问题归咎于设备硬件规格或网络带宽限制，经过多轮交叉测试、排除硬件性能瓶颈与网络环境干扰后，才发现核心症结在于音频格式未进行全链路适配优化，编码方案与设备解码逻辑、网络传输特性、场景使用需求形成了隐性错配。这种经历让我深刻体悟到，音频格式优化绝非简单的格式转换或参数调整，而是对编码标准特性、设备硬件能力、用户使用场景三者协同关系的深度解构与重构，它要求开发者跳出“能播放即合格”的表层认知，站在生态兼容与感知体验的双重维度，重新定义优化的核心价值与实践路径。</p><p>音频格式未优化的本质，是编码逻辑与使用场景、设备能力的三重适配失衡，这种失衡往往被“正常播放”的表象所掩盖，通过细微却关键的体验差异影响用户感知。不同音频格式的编码架构、压缩算法、资源占用特性存在显著差异：部分格式以极致压缩比为核心优势，却需消耗更多设备算力进行解码，在性能有限的设备上易引发卡顿；部分格式追求无损音质呈现，却导致文件体积激增，在弱网环境下加载耗时过长，甚至触发加载失败；部分格式兼容性覆盖广泛，却在特定硬件解码芯片上出现适配损耗，导致音质畸变或播放中断。例如，面向专业音乐制作的无损格式，若直接应用于移动端即时语音通讯场景，会因解码延迟累积引发语音同步偏差，破坏实时交互体验；而针对短视频流媒体设计的高压缩格式，用于本地高清音频播放时，会出现高频细节丢失、声场变窄等音质短板，难以满足用户对听感品质的需求。在长期的实践观察中发现，格式未优化带来的问题具有极强的场景依赖性与设备关联性，它不会在单一测试环境中集中暴露，却在用户真实使用的复杂场景中频繁爆发—比如户外移动场景下的弱网加载超时、后台持续播放时的电量快速消耗、多音频连续切换时的瞬时静音、老旧设备上的播放卡顿与音画不同步。这些问题的核心症结，在于开发者未能将音频格式的编码逻辑与使用场景的核心需求、设备的硬件解码能力进行精准匹配，导致格式成为制约体验升维的隐性瓶颈，即便其他功能模块打磨得再完善，也难以实现整体体验的闭环。</p><p>深入探究音频格式的底层逻辑会发现，其优化的核心在于对“编码效率、解码兼容、场景需求”三者动态平衡的精准把握。每一种音频格式的诞生都承载着特定的设计初衷：早期格式多为适配存储设备的容量限制，以高压缩比为核心目标；专业领域的格式聚焦音质无损呈现，牺牲部分存储与解码效率；移动生态的格式则侧重低功耗、快解码特性，适配移动端设备的硬件资源与使用场景。这意味着，音频格式优化并非寻找某一种“万能最优格式”，而是根据具体场景的核心诉求，选择最适配的编码方案与参数组合。在实践中曾遇到这样的典型案例：一款音频类应用上线后，收到大量老旧机型用户反馈播放时音画不同步，经过深度排查发现，应用采用的音频格式解码复杂度较高，而老旧机型的硬件解码芯片算力有限，无法及时处理编码数据，导致解码延迟持续累积，最终引发音画偏差。通过深入研究该格式的编码架构，调整关键压缩参数，在保证可感知音质不受影响的前提下，降低解码时的算力消耗，同时优化音频数据的封装方式，使其更适配老旧设备的解码逻辑，最终成功解决了这一问题。这一经历让我深刻认识到，音频格式优化需要开发者对不同编码标准的底层逻辑有深入理解—比如有损编码的心理声学模型差异、无损编码的熵编码算法特点、自适应编码的动态码率调整机制，只有精准掌握这些核心信息，才能在复杂的场景与设备差异中做出最优选择，实现编码效率、解码兼容与场景需求的动态平衡。</p><p>音频格式优化的实践路径，始于对使用场景的深度拆解，再落地为编码方案的精准选型与参数的精细化调校。场景拆解的核心在于明确三个关键维度：使用环境（网络状态、噪声环境）、设备类型（硬件性能、解码芯片、系统版本）、核心需求（音质优先、流畅优先、低功耗优先、存储友好）。例如，面向离线本地播放的高清音频场景，核心需求是音质还原与存储平衡，可优先选择无损格式或高码率有损格式，在保留音频细节的同时，通过合理的压缩算法控制文件体积；面向移动端在线播放的场景，核心需求是弱网适配与流畅播放，需侧重选择高压缩比、低解码损耗的格式，确保在网络带宽有限的情况下快速加载，同时降低解码时的设备资源占用；面向即时通讯的语音场景，核心需求是低延迟与实时交互，需优先选择低复杂度、快解码的格式，避免解码延迟影响语音同步，同时控制文件体积以提升传输效率；面向户外运动场景的音频应用，核心需求是低功耗与抗噪声干扰，需选择解码功耗低、中高频表现力强的格式，兼顾续航与听感清晰度。在编码方案选型之后，参数调校是实现体验升维的关键环节：调整压缩比时，需通过大量听感测试找到“音质损耗阈值”与“文件体积优化”的平衡点，既不能为追求压缩率而牺牲关键音质细节，也不能因过度追求音质而导致文件体积失控；调整采样率与位深时，需结合设备的音频输出能力与用户的实际听感需求，过高的采样率若超出设备播放极限与人类听觉范围，只会造成资源浪费；调整码率分配策略时，需根据音频内容的特性动态分配码率，对人声、乐器等关键信息分配更高码率，对背景噪声等非关键信息适当降低码率，实现资源的高效利用。在实践过程中，往往需要构建多维度的测试矩阵，在不同价位的设备、不同网络带宽、不同噪声环境下进行反复测试，对比音质表现、加载速度、功耗消耗等关键指标，最终形成适配目标场景的最优参数方案。</p><p>优化过程中，设备兼容性与生态适配是容易被忽视却至关重要的环节，不同设备的硬件解码芯片、系统音频框架、驱动程序对音频格式的支持程度存在显著差异，这种差异直接影响格式优化的实际效果。部分在主流旗舰机型上表现优异的格式，在小众机型或老旧设备上可能出现解码失败、音质畸变、播放卡顿等问题；而某些新推出的高效编码格式，可能因系统版本不支持或硬件解码芯片未适配，无法发挥其优势，甚至出现兼容性故障。因此，在进行音频格式优化时，不能仅针对单一设备或系统版本进行测试，而应构建全面的兼容性测试矩阵，覆盖高中低端不同价位的机型、主流与小众品牌设备、新旧不同系统版本，确保优化方案在各类设备上都能稳定运行。同时，还需深入了解不同平台的音频生态规则与调度机制：例如，部分系统对特定音频格式的解码资源调度优先级更高，部分平台对音频文件的封装格式有特殊要求，部分生态对后台播放时的音频格式功耗控制有明确标准。这些生态规则直接影响音频格式的实际表现，若忽视这些细节，即便编码方案再优化，也可能出现体验问题。在实践中曾遇到这样的案例：一款应用采用了某新型高效音频格式，在多数安卓设备上表现良好，但在部分品牌的设备上出现后台播放时卡顿的问题，排查后发现，该品牌系统对该格式的后台解码资源调度优先级较低，当设备资源紧张时，音频解码会被抢占资源，导致播放卡顿。通过调整音频文件的封装方式，适配该系统的资源调度逻辑，同时优化解码初始化流程，最终解决了这一兼容性问题。这一经历让我深刻认识到，音频格式优化不仅是技术层面的编码调整，更是对整个音频生态规则、设备硬件特性的深度适配，只有实现技术与生态的协同，才能确保优化效果的落地。</p><p>音频格式优化的深层价值，早已超越了单一功能的体验提升，成为应用核心竞争力的隐性组成部分。在用户对多媒体体验要求日益严苛的当下，流畅的播放体验、清晰的音质表现、高效的资源占用、持久的续航能力，这些看似基础的需求，恰恰是区分优秀应用与普通应用的关键维度。一款能够根据场景智能适配音频格式的应用，不仅能在不同环境下保持稳定一致的体验表现，更能让用户感受到开发者对细节的极致追求与对用户需求的深度洞察，这种感知会转化为用户对应用的信任与依赖，成为应用长期发展的核心资产。在长期的开发实践中，我逐渐意识到，音频格式优化的过程，也是开发者技术认知不断深化、思维模式不断升级的过程—它要求我们跳出技术本身的局限，站在用户体验、设备特性、生态规则的多重角度进行系统性思考，培养“场景化思维”与“全链路思维”。这种思维模式的提升，不仅能解决音频格式优化的具体问题，更能迁移到多媒体开发的其他环节，比如视频格式适配、音视频同步优化、多媒体资源加载策略等，帮助我们更好地应对复杂的技术挑战。而对于开发者而言，这种对隐性细节的深耕、对底层逻辑的探究、对实践经验的沉淀，正是建立个人技术品牌、吸引同行关注的核心竞争力。</p>]]></description></item><item>    <title><![CDATA[采集华为云 CCI 日志到观测云最佳实践]]></title>    <link>https://segmentfault.com/a/1190000047437128</link>    <guid>https://segmentfault.com/a/1190000047437128</guid>    <pubDate>2025-11-28 20:03:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、背景与挑战</h2><p>华为云 CCE 提供了云原生日志采集插件，采集了包含 CCE 集群以及弹性到 CCI 的实例的容器内日志，但对观测云来讲，观测云可以基于 DataKit Operator 以及提供一个 DataKit 的 DaemonSet 部署来实现 CCE 各节点的容器内的日志文件采集，但针对于对于 CCI 的这种 serverless 的容器内日志采集，观测云采集思路包含：</p><ul><li>通过观测云的 logforward 的 sidecar 部署来实现日志转发给观测云，这种方式消耗大量的资源，并且要对原有的 CCE 的 Deployment 进行改造注入。</li><li>使用 lambda 函数将 LTS 采集的 OBS 的日志上报到观测云，因 CCE 的同一 Deployment 弹性到 CCI，这种方式基于 OBS 区分不出哪些是 CCI 的日志，哪些是 CCE 的日志。</li><li>华为云 CCE 云原生日志采集插件中包含了 Otel Collector 组件，通过改造 Otel Collector 的 exporter 配置实现 CCI 日志的导出，这种方式减少了日志接入的成本，避免了资源额外消耗的成本，即本篇重点阐述的最佳实践。</li></ul><h2>二、前置条件</h2><ul><li>DataKit：观测云的采集组件，负责 CCE 日志采集与接收 Otel Collector 的 CCI 日志收集导出。</li><li>观测云：统一日志检索、查询分析、仪表盘展示、智能告警等。</li><li>云原生日志采集插件：负责 CCE 日志和 CCI 日志的采集，插件版本要求 1.5.1 版本以上，插件说明如下。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437130" alt="图片" title="图片"/></p><ul><li>业务场景环境：华为 CCE 调度到 CCI 场景。</li></ul><h2>三、采集流程</h2><p>华为云 CCE 集群容器内日志通过观测云标准方案 <a href="https://link.segmentfault.com/?enc=TE2LDdmNblvnqoC38ed9KQ%3D%3D.C6oVuzAYGVO0uFJ3WGZw86dVL%2FQYTvzVSyBnTqjC3QEbWom%2FOjR4tFL5IHqXDhc5WQjakIZ63oG530AKk5QF%2Bw%3D%3D" rel="nofollow" target="_blank">DataKit Operator</a> 的方式采集，而弹性到 CCI 的日志通过云原生插件采集 Otel Collector 并导出到观测云 DataKit 服务，最终展示在观测云控制台，如下流程图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437131" alt="图片" title="图片" loading="lazy"/></p><h2>四、配置步骤</h2><h3>步骤 1：CCE 集群弹性到 CCI Demo 搭建</h3><ul><li>请自行创建 CCE 集群，并创建应用，测试可强制调度到 CCI，如下图：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437132" alt="图片" title="图片" loading="lazy"/></p><p>sp-demo2.yaml</p><pre><code>kind: Deployment

apiVersion: apps/v1

metadata:

  name: sp-demo2

  namespace: default

  uid: 403dd3e0-8591-44d8-bd7f-0c8585acb26d

  resourceVersion: '295573'

  generation: 1

  creationTimestamp: '2025-09-12T12:15:48Z'

  labels:

    appgroup: ''

    version: v1

    virtual-kubelet.io/burst-to-cci: enforce

  annotations:

    deployment.kubernetes.io/revision: '1'

    description: ''

    kubectl.kubernetes.io/last-applied-configuration: &gt;

      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"5","description":"","workload.cce.io/swr-version":"[{\"version\":\"Private

      Edition\"}]"},"labels":{"appgroup":"","version":"v1","virtual-kubelet.io/burst-to-cci":"enforce"},"name":"sp-demo2","namespace":"default"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"sp-demo2","version":"v1"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"app":"sp-demo2","version":"v1"}},"spec":{"containers":[{"env":[{"name":"PAAS_APP_NAME","value":"sp-demo2"},{"name":"PAAS_NAMESPACE","value":"default"},{"name":"PAAS_PROJECT_ID","value":"bacc65fb662f435dab3acda49acae0c9"}],"image":"swr.cn-north-4.myhuaweicloud.com/liurui_bj/springboot-server:openj8","imagePullPolicy":"IfNotPresent","name":"container-1","resources":{"limits":{"cpu":"250m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"512Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","imagePullSecrets":[{"name":"default-secret"}],"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}]}}}}

    workload.cce.io/swr-version: '[{"version":"Private Edition"}]'

  managedFields:

    - manager: kubectl-client-side-apply

      operation: Update

      apiVersion: apps/v1

      time: '2025-09-12T12:15:48Z'

      fieldsType: FieldsV1

      fieldsV1:

        f:metadata:

          f:annotations:

            .: {}

            f:description: {}

            f:kubectl.kubernetes.io/last-applied-configuration: {}

            f:workload.cce.io/swr-version: {}

          f:labels:

            .: {}

            f:appgroup: {}

            f:version: {}

            f:virtual-kubelet.io/burst-to-cci: {}

        f:spec:

          f:progressDeadlineSeconds: {}

          f:replicas: {}

          f:revisionHistoryLimit: {}

          f:selector: {}

          f:strategy:

            f:rollingUpdate:

              .: {}

              f:maxSurge: {}

              f:maxUnavailable: {}

            f:type: {}

          f:template:

            f:metadata:

              f:labels:

                .: {}

                f:app: {}

                f:version: {}

            f:spec:

              f:containers:

                k:{"name":"container-1"}:

                  .: {}

                  f:env:

                    .: {}

                    k:{"name":"PAAS_APP_NAME"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                    k:{"name":"PAAS_NAMESPACE"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                    k:{"name":"PAAS_PROJECT_ID"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                  f:image: {}

                  f:imagePullPolicy: {}

                  f:name: {}

                  f:resources:

                    .: {}

                    f:limits:

                      .: {}

                      f:cpu: {}

                      f:memory: {}

                    f:requests:

                      .: {}

                      f:cpu: {}

                      f:memory: {}

                  f:terminationMessagePath: {}

                  f:terminationMessagePolicy: {}

              f:dnsPolicy: {}

              f:imagePullSecrets:

                .: {}

                k:{"name":"default-secret"}: {}

              f:restartPolicy: {}

              f:schedulerName: {}

              f:securityContext: {}

              f:terminationGracePeriodSeconds: {}

              f:tolerations: {}

    - manager: kube-controller-manager

      operation: Update

      apiVersion: apps/v1

      time: '2025-09-12T12:16:19Z'

      fieldsType: FieldsV1

      fieldsV1:

        f:metadata:

          f:annotations:

            f:deployment.kubernetes.io/revision: {}

        f:status:

          f:availableReplicas: {}

          f:conditions:

            .: {}

            k:{"type":"Available"}:

              .: {}

              f:lastTransitionTime: {}

              f:lastUpdateTime: {}

              f:message: {}

              f:reason: {}

              f:status: {}

              f:type: {}

            k:{"type":"Progressing"}:

              .: {}

              f:lastTransitionTime: {}

              f:lastUpdateTime: {}

              f:message: {}

              f:reason: {}

              f:status: {}

              f:type: {}

          f:observedGeneration: {}

          f:readyReplicas: {}

          f:replicas: {}

          f:updatedReplicas: {}

      subresource: status

spec:

  replicas: 1

  selector:

    matchLabels:

      app: sp-demo2

      version: v1

  template:

    metadata:

      creationTimestamp: null

      labels:

        app: sp-demo2

        version: v1

    spec:

      containers:

        - name: container-1

          image: swr.cn-north-4.myhuaweicloud.com/liurui_bj/springboot-server:openj8

          env:

            - name: PAAS_APP_NAME

              value: sp-demo2

            - name: PAAS_NAMESPACE

              value: default

            - name: PAAS_PROJECT_ID

              value: bacc65fb662f435dab3acda49acae0c9

          resources:

            limits:

              cpu: 250m

              memory: 512Mi

            requests:

              cpu: 250m

              memory: 512Mi

          terminationMessagePath: /dev/termination-log

          terminationMessagePolicy: File

          imagePullPolicy: IfNotPresent

      restartPolicy: Always

      terminationGracePeriodSeconds: 30

      dnsPolicy: ClusterFirst

      securityContext: {}

      imagePullSecrets:

        - name: default-secret

      schedulerName: default-scheduler

      tolerations:

        - key: node.kubernetes.io/not-ready

          operator: Exists

          effect: NoExecute

          tolerationSeconds: 300

        - key: node.kubernetes.io/unreachable

          operator: Exists

          effect: NoExecute

          tolerationSeconds: 300

  strategy:

    type: RollingUpdate

    rollingUpdate:

      maxUnavailable: 25%

      maxSurge: 25%

  revisionHistoryLimit: 10

  progressDeadlineSeconds: 600

status:

  observedGeneration: 1

  replicas: 1

  updatedReplicas: 1

  readyReplicas: 1

  availableReplicas: 1

  conditions:

    - type: Available

      status: 'True'

      lastUpdateTime: '2025-09-12T12:16:19Z'

      lastTransitionTime: '2025-09-12T12:16:19Z'

      reason: MinimumReplicasAvailable

      message: Deployment has minimum availability.

    - type: Progressing

      status: 'True'

      lastUpdateTime: '2025-09-12T12:16:19Z'

      lastTransitionTime: '2025-09-12T12:15:48Z'

      reason: NewReplicaSetAvailable

      message: ReplicaSet "sp-demo2-7d9cd96c44" has successfully progressed.</code></pre><ul><li>查看 CCI 节点运行的 pod ：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437133" alt="图片" title="图片" loading="lazy"/></p><ul><li>本次要采集的 CCI 容器内日志为 server.log，目录如下：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437134" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 2：在 CCE 安装云原生日志采集插件</h3><ul><li>在 CCE 插件中心安装云原生日志采集插件，实例规格自定义配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437135" alt="图片" title="图片" loading="lazy"/></p><ul><li>在日志中心创建 CCI 日志采集策略</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437136" alt="图片" title="图片" loading="lazy"/></p><ul><li>华为云 LTS 日志采集展示</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437137" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 3：在 CCE 集群部署 DataKit</h3><ul><li>通过 kubectl apply -f datakit.yaml 命令实现在华为云 CCE 的的一个 Daemonset 部署，采集器要开启 opentelemetry 采集器，并通过亲和性设置不让 DataKit 调度到虚拟节点</li></ul><p>datakit.yaml</p><pre><code>kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: datakit
  namespace: datakit
  uid: 122c1472-03cd-4ec6-a684-0384e40b011c
  resourceVersion: '5351437'
  generation: 2
  creationTimestamp: '2025-09-16T10:45:45Z'
  labels:
    app: daemonset-datakit
  annotations:
    deprecated.daemonset.template.generation: '2'
    kubectl.kubernetes.io/last-applied-configuration: &gt;
      {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"daemonset-datakit"},"name":"datakit","namespace":"datakit"},"spec":{"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"daemonset-datakit"}},"template":{"metadata":{"labels":{"app":"daemonset-datakit"}},"spec":{"containers":[{"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"ENV_K8S_NODE_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"ENV_K8S_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"ENV_DATAWAY","value":"https://openway.guance.com?token=tkn_3a0052c9f6d3498c8ce9ca0988fd9c82"},{"name":"ENV_CLUSTER_NAME_K8S","value":"cce"},{"name":"ENV_GLOBAL_HOST_TAGS","value":"host=__datakit_hostname,host_ip=__datakit_ip"},{"name":"ENV_GLOBAL_ELECTION_TAGS","value":""},{"name":"ENV_DEFAULT_ENABLED_INPUTS","value":"statsd,dk,cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container,kubernetesprometheus,logfwdserver,opentelemetry"},{"name":"ENV_ENABLE_ELECTION","value":"enable"},{"name":"ENV_INPUT_CONTAINER_ENABLE_POD_METRIC","value":"true"},{"name":"ENV_HTTP_LISTEN","value":"0.0.0.0:9529"},{"name":"ENV_INPUT_OTEL_GRPC","value":"{\"addr\":
      \"0.0.0.0:4317\"}"},{"name":"HOST_PROC","value":"/rootfs/proc"},{"name":"HOST_SYS","value":"/rootfs/sys"},{"name":"HOST_ETC","value":"/rootfs/etc"},{"name":"HOST_VAR","value":"/rootfs/var"},{"name":"HOST_RUN","value":"/rootfs/run"},{"name":"HOST_DEV","value":"/rootfs/dev"},{"name":"HOST_ROOT","value":"/rootfs"}],"image":"swr.cn-north-4.myhuaweicloud.com/liurui_bj/datakit:1.79.0","imagePullPolicy":"IfNotPresent","name":"datakit","ports":[{"containerPort":9529,"hostPort":9529,"name":"http-port","protocol":"TCP"},{"containerPort":8125,"hostPort":8125,"name":"statsd-port","protocol":"UDP"},{"containerPort":4317,"hostPort":4317,"name":"otel-grpc-port","protocol":"TCP"},{"containerPort":9533,"hostPort":9533,"name":"logfwd-port","protocol":"TCP"}],"resources":{"limits":{"cpu":"500m","memory":"1Gi"},"requests":{"cpu":"200m","memory":"128Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/usr/local/datakit/cache","name":"cache","readOnly":false},{"mountPath":"/rootfs","mountPropagation":"HostToContainer","name":"rootfs"},{"mountPath":"/var/run","mountPropagation":"HostToContainer","name":"run"},{"mountPath":"/sys/kernel/debug","name":"debugfs"},{"mountPath":"/var/lib/containerd/container_logs","name":"container-logs"},{"mountPath":"/usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf","name":"datakit-conf","subPath":"kubelet.conf"}],"workingDir":"/usr/local/datakit"}],"dnsPolicy":"ClusterFirstWithHostNet","hostIPC":true,"hostNetwork":true,"hostPID":true,"restartPolicy":"Always","serviceAccount":"datakit","serviceAccountName":"datakit","tolerations":[{"operator":"Exists"}],"volumes":[{"configMap":{"name":"datakit-conf"},"name":"datakit-conf"},{"hostPath":{"path":"/"},"name":"rootfs"},{"hostPath":{"path":"/var/run"},"name":"run"},{"hostPath":{"path":"/sys/kernel/debug"},"name":"debugfs"},{"hostPath":{"path":"/root/datakit_cache"},"name":"cache"},{"hostPath":{"path":"/var/lib/containerd/container_logs"},"name":"container-logs"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
  managedFields:
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-16T10:45:45Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:deprecated.daemonset.template.generation: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:labels:
            .: {}
            f:app: {}
        f:spec:
          f:revisionHistoryLimit: {}
          f:selector: {}
          f:template:
            f:metadata:
              f:labels:
                .: {}
                f:app: {}
            f:spec:
              f:containers:
                k:{"name":"datakit"}:
                  .: {}
                  f:env:
                    .: {}
                    k:{"name":"ENV_CLUSTER_NAME_K8S"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_DATAWAY"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_DEFAULT_ENABLED_INPUTS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_ENABLE_ELECTION"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_GLOBAL_ELECTION_TAGS"}:
                      .: {}
                      f:name: {}
                    k:{"name":"ENV_GLOBAL_HOST_TAGS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_HTTP_LISTEN"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_INPUT_CONTAINER_ENABLE_POD_METRIC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_INPUT_OTEL_GRPC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_K8S_NODE_IP"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"ENV_K8S_NODE_NAME"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"HOST_DEV"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_ETC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_PROC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_ROOT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_RUN"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_SYS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_VAR"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"POD_NAME"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:name: {}
                  f:ports:
                    .: {}
                    k:{"containerPort":4317,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":8125,"protocol":"UDP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":9529,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":9533,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:securityContext:
                    .: {}
                    f:privileged: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/rootfs"}:
                      .: {}
                      f:mountPath: {}
                      f:mountPropagation: {}
                      f:name: {}
                    k:{"mountPath":"/sys/kernel/debug"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/usr/local/datakit/cache"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:subPath: {}
                    k:{"mountPath":"/var/lib/containerd/container_logs"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/var/run"}:
                      .: {}
                      f:mountPath: {}
                      f:mountPropagation: {}
                      f:name: {}
                  f:workingDir: {}
              f:dnsPolicy: {}
              f:hostIPC: {}
              f:hostNetwork: {}
              f:hostPID: {}
              f:restartPolicy: {}
              f:schedulerName: {}
              f:securityContext: {}
              f:serviceAccount: {}
              f:serviceAccountName: {}
              f:terminationGracePeriodSeconds: {}
              f:tolerations: {}
              f:volumes:
                .: {}
                k:{"name":"cache"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"container-logs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"datakit-conf"}:
                  .: {}
                  f:configMap:
                    .: {}
                    f:defaultMode: {}
                    f:name: {}
                  f:name: {}
                k:{"name":"debugfs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"rootfs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"run"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
          f:updateStrategy:
            f:rollingUpdate:
              .: {}
              f:maxSurge: {}
              f:maxUnavailable: {}
            f:type: {}
    - manager: cfe-apiserver
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-19T06:28:11Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          f:template:
            f:spec:
              f:affinity:
                .: {}
                f:nodeAffinity:
                  .: {}
                  f:requiredDuringSchedulingIgnoredDuringExecution: {}
    - manager: kube-controller-manager
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-19T06:28:19Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:currentNumberScheduled: {}
          f:desiredNumberScheduled: {}
          f:numberAvailable: {}
          f:numberMisscheduled: {}
          f:numberReady: {}
          f:observedGeneration: {}
          f:updatedNumberScheduled: {}
      subresource: status
spec:
  selector:
    matchLabels:
      app: daemonset-datakit
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: daemonset-datakit
    spec:
      volumes:
        - name: datakit-conf
          configMap:
            name: datakit-conf
            defaultMode: 420
        - name: rootfs
          hostPath:
            path: /
            type: ''
        - name: run
          hostPath:
            path: /var/run
            type: ''
        - name: debugfs
          hostPath:
            path: /sys/kernel/debug
            type: ''
        - name: cache
          hostPath:
            path: /root/datakit_cache
            type: ''
        - name: container-logs
          hostPath:
            path: /var/lib/containerd/container_logs
            type: ''
      containers:
        - name: datakit
          image: swr.cn-north-4.myhuaweicloud.com/liurui_bj/datakit:1.79.0
          workingDir: /usr/local/datakit
          ports:
            - name: http-port
              hostPort: 9529
              containerPort: 9529
              protocol: TCP
            - name: statsd-port
              hostPort: 8125
              containerPort: 8125
              protocol: UDP
            - name: otel-grpc-port
              hostPort: 4317
              containerPort: 4317
              protocol: TCP
            - name: logfwd-port
              hostPort: 9533
              containerPort: 9533
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: ENV_K8S_NODE_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
            - name: ENV_K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: ENV_DATAWAY
              value: https://openway.guance.com?token=tkn_3a0052c9f6d3498c8ce9ca0988fd9c82
            - name: ENV_CLUSTER_NAME_K8S
              value: cce
            - name: ENV_GLOBAL_HOST_TAGS
              value: host=__datakit_hostname,host_ip=__datakit_ip
            - name: ENV_GLOBAL_ELECTION_TAGS
            - name: ENV_DEFAULT_ENABLED_INPUTS
              value: statsd,dk,cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container,kubernetesprometheus,logfwdserver,opentelemetry
            - name: ENV_ENABLE_ELECTION
              value: enable
            - name: ENV_INPUT_CONTAINER_ENABLE_POD_METRIC
              value: 'true'
            - name: ENV_HTTP_LISTEN
              value: 0.0.0.0:9529
            - name: ENV_INPUT_OTEL_GRPC
              value: '{"addr": "0.0.0.0:4317"}'
            - name: HOST_PROC
              value: /rootfs/proc
            - name: HOST_SYS
              value: /rootfs/sys
            - name: HOST_ETC
              value: /rootfs/etc
            - name: HOST_VAR
              value: /rootfs/var
            - name: HOST_RUN
              value: /rootfs/run
            - name: HOST_DEV
              value: /rootfs/dev
            - name: HOST_ROOT
              value: /rootfs
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 128Mi
          volumeMounts:
            - name: cache
              mountPath: /usr/local/datakit/cache
            - name: rootfs
              mountPath: /rootfs
              mountPropagation: HostToContainer
            - name: run
              mountPath: /var/run
              mountPropagation: HostToContainer
            - name: debugfs
              mountPath: /sys/kernel/debug
            - name: container-logs
              mountPath: /var/lib/containerd/container_logs
            - name: datakit-conf
              mountPath: /usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf
              subPath: kubelet.conf
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: datakit
      serviceAccount: datakit
      hostNetwork: true
      hostPID: true
      hostIPC: true
      securityContext: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: bursting.cci.io/node-type
                    operator: NotIn
                    values:
                      - virtual-kubelet
      schedulerName: default-scheduler
      tolerations:
        - operator: Exists
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  revisionHistoryLimit: 10
status:
  currentNumberScheduled: 2
  numberMisscheduled: 0
  desiredNumberScheduled: 2
  numberReady: 2
  observedGeneration: 2
  updatedNumberScheduled: 2
  numberAvailable: 2</code></pre><ul><li>进入 datakit 容器，并执行 datakit monitor 查看 opentelemetry 采集器是否开启</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437138" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 4：重写 Otel Collector 的采集配置</h3><p>log-agent-otel-collector.yaml</p><pre><code>kind: Deployment
apiVersion: apps/v1
metadata:
  name: log-agent-otel-collector
  namespace: monitoring
  uid: c055d466-4287-4860-9ff7-d28cc036ae89
  resourceVersion: '7557223'
  generation: 3
  creationTimestamp: '2025-09-22T07:28:09Z'
  labels:
    app: log-agent-otel-collector
    app.kubernetes.io/managed-by: Helm
    release: cceaddon-log-agent
  annotations:
    deployment.kubernetes.io/revision: '3'
    kubectl.kubernetes.io/last-applied-configuration: &gt;
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"3","meta.helm.sh/release-name":"cceaddon-log-agent","meta.helm.sh/release-namespace":"monitoring"},"creationTimestamp":"2025-09-20T19:02:18Z","generation":3,"labels":{"app":"log-agent-otel-collector","app.kubernetes.io/managed-by":"Helm","release":"cceaddon-log-agent"},"name":"log-agent-otel-collector","namespace":"monitoring","resourceVersion":"7514159","uid":"180806a1-7260-4139-989c-73945d7b1a4c"},"spec":{"minReadySeconds":5,"progressDeadlineSeconds":120,"replicas":2,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"log-agent-otel-collector"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8888","prometheus.io/scheme":"http","prometheus.io/scrape":"true","redeploy-timestamp":"1758396245987","scheduler.alpha.kubernetes.io/tolerations":"[{\"key\":
      \"taint.alpha.kubernetes.io/nodedown\",\"value\": \"\",\"effect\": \"NoExecute\",\"operator\":
      \"Exists\"}]"},"creationTimestamp":null,"labels":{"app":"log-agent-otel-collector","release":"cceaddon-log-agent"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["log-agent-otel-collector"]}]},"topologyKey":"topology.kubernetes.io/zone"},"weight":100}]}},"containers":[{"args":["--config=/var/paas/ot-collector/ot-collector-service.yaml"],"command":["/var/paas/otel-collector/otelcol"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.podIP"}}},{"name":"Region","value":"cn-north-4"},{"name":"ProjectID","value":"9e92837f567145009ad4d230c4ac2c01"},{"name":"ClusterID","value":"74e8b92f-8f80-11f0-afe1-0255ac10026c"},{"name":"ClusterName","value":"cce-cci"},{"name":"WATCH_SECRET","value":"true"},{"name":"INSECURE_SKIP_VERIFY","value":"true"},{"name":"SCENE","value":"HWS"},{"name":"AKSK_SECRET_NAME","value":"paas.elb"},{"name":"WATCH_CLUSTER_CONFIG","value":"true"},{"name":"AOM_ENDPOINT","value":"https://aom.cn-north-4.myhuaweicloud.com"},{"name":"LTS_ACCESS_ENDPOINT","value":"https://lts-access.cn-north-4.myhuaweicloud.com:8102"},{"name":"CRYPTO_ENABLE","value":"true"},{"name":"PAAS_CRYPTO_PATH","value":"/etc/cipher"}],"image":"swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"exec":{"command":["/bin/bash","-c","exit
      0"]},"failureThreshold":3,"initialDelaySeconds":20,"periodSeconds":20,"successThreshold":1,"timeoutSeconds":10},"name":"otel-collector","ports":[{"containerPort":8006,"protocol":"TCP"},{"containerPort":4317,"protocol":"TCP"},{"containerPort":8888,"name":"metric-port","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"2Gi"},"requests":{"cpu":"200m","memory":"1Gi"}},"securityContext":{"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsGroup":10000,"runAsUser":10000},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/paas/otel-collector/conf","name":"otel-collector-config-vol","readOnly":true},{"mountPath":"/var/paas/ot-collector/ot-collector-service.yaml","name":"ot-collector-service","readOnly":true,"subPath":"ot-collector-service.yaml"},{"mountPath":"/var/paas/sys/log","name":"logpath"},{"mountPath":"/etc/cipher/root.key","name":"rootkey","readOnly":true},{"mountPath":"/etc/cipher/common_shared.key","name":"commonsharedkey","readOnly":true},{"mountPath":"/var/paas/cert","name":"cert","readOnly":true}]}],"dnsConfig":{"options":[{"name":"ndots","value":"3"}]},"dnsPolicy":"ClusterFirst","initContainers":[{"command":["/bin/sh","-c","mkdir
      -p /var/paas/sys/log/otel \u0026\u0026 chmod 750 /var/paas/sys/log/otel \u0026\u0026 chown -R 10000:10000
      /var/paas/sys/log/otel"],"image":"swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4","imagePullPolicy":"IfNotPresent","name":"init","resources":{"limits":{"cpu":"200m","memory":"200Mi"},"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/paas/sys/log","name":"logpath"}]}],"priorityClassName":"system-cluster-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{"fsGroup":10000},"serviceAccount":"log-agent-serviceaccount","serviceAccountName":"log-agent-serviceaccount","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":30},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":30},{"key":"role","operator":"Exists"},{"effect":"NoSchedule","key":"distribution.io/category","operator":"Equal","value":"IES"}],"volumes":[{"name":"otel-collector-config-vol","secret":{"defaultMode":384,"secretName":"log-agent-otel-collector-config"}},{"configMap":{"defaultMode":420,"items":[{"key":"ot-collector-service.yaml","path":"ot-collector-service.yaml"}],"name":"ot-collector-service"},"name":"ot-collector-service"},{"name":"cert","secret":{"defaultMode":416,"items":[{"key":"caCert","path":"caCert"},{"key":"serverCert","path":"serverCert"},{"key":"serverKey","path":"serverKey"}],"secretName":"log-agent-cert-secret"}},{"hostPath":{"path":"/var/paas/sys/log","type":""},"name":"logpath"},{"hostPath":{"path":"/var/paas/srv/kubernetes/root.key","type":""},"name":"rootkey"},{"hostPath":{"path":"/var/paas/srv/kubernetes/common_shared.key","type":""},"name":"commonsharedkey"}]}}},"status":{"conditions":[{"lastTransitionTime":"2025-09-20T19:02:18Z","lastUpdateTime":"2025-09-20T19:37:31Z","message":"ReplicaSet
      \"log-agent-otel-collector-8fbf8c694\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-09-22T06:15:38Z","lastUpdateTime":"2025-09-22T06:15:38Z","message":"Deployment does not have minimum availability.","reason":"MinimumReplicasUnavailable","status":"False","type":"Available"}],"observedGeneration":3,"replicas":2,"unavailableReplicas":2,"updatedReplicas":2}}
    meta.helm.sh/release-name: cceaddon-log-agent
    meta.helm.sh/release-namespace: monitoring
  managedFields:
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:28:09Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
            f:meta.helm.sh/release-name: {}
            f:meta.helm.sh/release-namespace: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/managed-by: {}
            f:release: {}
        f:spec:
          f:minReadySeconds: {}
          f:progressDeadlineSeconds: {}
          f:replicas: {}
          f:revisionHistoryLimit: {}
          f:selector: {}
          f:strategy:
            f:rollingUpdate:
              .: {}
              f:maxSurge: {}
              f:maxUnavailable: {}
            f:type: {}
          f:template:
            f:metadata:
              f:annotations:
                .: {}
                f:prometheus.io/path: {}
                f:prometheus.io/port: {}
                f:prometheus.io/scheme: {}
                f:prometheus.io/scrape: {}
                f:scheduler.alpha.kubernetes.io/tolerations: {}
              f:labels:
                .: {}
                f:app: {}
                f:release: {}
            f:spec:
              f:affinity:
                .: {}
                f:podAntiAffinity:
                  .: {}
                  f:preferredDuringSchedulingIgnoredDuringExecution: {}
              f:containers:
                k:{"name":"otel-collector"}:
                  .: {}
                  f:args: {}
                  f:command: {}
                  f:env:
                    .: {}
                    k:{"name":"AKSK_SECRET_NAME"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"AOM_ENDPOINT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"CRYPTO_ENABLE"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ClusterID"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ClusterName"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"INSECURE_SKIP_VERIFY"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"LTS_ACCESS_ENDPOINT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"PAAS_CRYPTO_PATH"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"POD_IP"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"ProjectID"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"Region"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"SCENE"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"WATCH_CLUSTER_CONFIG"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"WATCH_SECRET"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:livenessProbe:
                    .: {}
                    f:exec:
                      .: {}
                      f:command: {}
                    f:failureThreshold: {}
                    f:initialDelaySeconds: {}
                    f:periodSeconds: {}
                    f:successThreshold: {}
                    f:timeoutSeconds: {}
                  f:name: {}
                  f:ports:
                    .: {}
                    k:{"containerPort":4317,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:protocol: {}
                    k:{"containerPort":8006,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:protocol: {}
                    k:{"containerPort":8888,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:name: {}
                      f:protocol: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:securityContext:
                    .: {}
                    f:allowPrivilegeEscalation: {}
                    f:readOnlyRootFilesystem: {}
                    f:runAsGroup: {}
                    f:runAsUser: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/etc/cipher/common_shared.key"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/etc/cipher/root.key"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/cert"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/ot-collector/ot-collector-service.yaml"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                      f:subPath: {}
                    k:{"mountPath":"/var/paas/otel-collector/conf"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/sys/log"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
              f:dnsConfig:
                .: {}
                f:options: {}
              f:dnsPolicy: {}
              f:initContainers:
                .: {}
                k:{"name":"init"}:
                  .: {}
                  f:command: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:name: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/var/paas/sys/log"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
              f:priorityClassName: {}
              f:restartPolicy: {}
              f:schedulerName: {}
              f:securityContext:
                .: {}
                f:fsGroup: {}
              f:serviceAccount: {}
              f:serviceAccountName: {}
              f:terminationGracePeriodSeconds: {}
              f:tolerations: {}
              f:volumes:
                .: {}
                k:{"name":"cert"}:
                  .: {}
                  f:name: {}
                  f:secret:
                    .: {}
                    f:defaultMode: {}
                    f:items: {}
                    f:secretName: {}
                k:{"name":"commonsharedkey"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"logpath"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"ot-collector-service"}:
                  .: {}
                  f:configMap:
                    .: {}
                    f:defaultMode: {}
                    f:items: {}
                    f:name: {}
                  f:name: {}
                k:{"name":"otel-collector-config-vol"}:
                  .: {}
                  f:name: {}
                  f:secret:
                    .: {}
                    f:defaultMode: {}
                    f:secretName: {}
                k:{"name":"rootkey"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
    - manager: cfe-apiserver
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:40:23Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          f:template:
            f:metadata:
              f:annotations:
                f:redeploy-timestamp: {}
    - manager: kube-controller-manager
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:40:31Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:deployment.kubernetes.io/revision: {}
        f:status:
          f:availableReplicas: {}
          f:conditions:
            .: {}
            k:{"type":"Available"}:
              .: {}
              f:lastTransitionTime: {}
              f:lastUpdateTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Progressing"}:
              .: {}
              f:lastTransitionTime: {}
              f:lastUpdateTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:observedGeneration: {}
          f:readyReplicas: {}
          f:replicas: {}
          f:updatedReplicas: {}
      subresource: status
spec:
  replicas: 2
  selector:
    matchLabels:
      app: log-agent-otel-collector
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: log-agent-otel-collector
        release: cceaddon-log-agent
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '8888'
        prometheus.io/scheme: http
        prometheus.io/scrape: 'true'
        redeploy-timestamp: '1758526823089'
        scheduler.alpha.kubernetes.io/tolerations: '[{"key": "taint.alpha.kubernetes.io/nodedown","value": "","effect": "NoExecute","operator": "Exists"}]'
    spec:
      volumes:
        - name: otel-collector-config-vol
          secret:
            secretName: log-agent-otel-collector-config
            defaultMode: 384
        - name: ot-collector-service
          configMap:
            name: ot-collector-service
            items:
              - key: ot-collector-service.yaml
                path: ot-collector-service.yaml
            defaultMode: 420
        - name: cert
          secret:
            secretName: log-agent-cert-secret
            items:
              - key: caCert
                path: caCert
              - key: serverCert
                path: serverCert
              - key: serverKey
                path: serverKey
            defaultMode: 416
        - name: logpath
          hostPath:
            path: /var/paas/sys/log
            type: ''
        - name: rootkey
          hostPath:
            path: /var/paas/srv/kubernetes/root.key
            type: ''
        - name: commonsharedkey
          hostPath:
            path: /var/paas/srv/kubernetes/common_shared.key
            type: ''
      initContainers:
        - name: init
          image: swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4
          command:
            - /bin/sh
            - '-c'
            - mkdir -p /var/paas/sys/log/otel &amp;&amp; chmod 750 /var/paas/sys/log/otel &amp;&amp; chown -R 10000:10000 /var/paas/sys/log/otel
          resources:
            limits:
              cpu: 200m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - name: logpath
              mountPath: /var/paas/sys/log
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      containers:
        - name: otel-collector
          image: swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4
          command:
            - /var/paas/otel-collector/otelcol
          args:
            - '--config=/var/paas/ot-collector/ot-collector-service.yaml'
          ports:
            - containerPort: 8006
              protocol: TCP
            - containerPort: 4317
              protocol: TCP
            - name: metric-port
              containerPort: 8888
              protocol: TCP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: Region
              value: cn-north-4
            - name: ProjectID
              value: 9e92837f567145009ad4d230c4ac2c01
            - name: ClusterID
              value: 74e8b92f-8f80-11f0-afe1-0255ac10026c
            - name: ClusterName
              value: cce-cci
            - name: WATCH_SECRET
              value: 'true'
            - name: INSECURE_SKIP_VERIFY
              value: 'true'
            - name: SCENE
              value: HWS
            - name: AKSK_SECRET_NAME
              value: paas.elb
            - name: WATCH_CLUSTER_CONFIG
              value: 'true'
            - name: AOM_ENDPOINT
              value: https://aom.cn-north-4.myhuaweicloud.com
            - name: LTS_ACCESS_ENDPOINT
              value: https://lts-access.cn-north-4.myhuaweicloud.com:8102
            - name: CRYPTO_ENABLE
              value: 'true'
            - name: PAAS_CRYPTO_PATH
              value: /etc/cipher
          resources:
            limits:
              cpu: '1'
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          volumeMounts:
            - name: otel-collector-config-vol
              readOnly: true
              mountPath: /var/paas/otel-collector/conf
            - name: ot-collector-service
              readOnly: true
              mountPath: /var/paas/ot-collector/ot-collector-service.yaml
              subPath: ot-collector-service.yaml
            - name: logpath
              mountPath: /var/paas/sys/log
            - name: rootkey
              readOnly: true
              mountPath: /etc/cipher/root.key
            - name: commonsharedkey
              readOnly: true
              mountPath: /etc/cipher/common_shared.key
            - name: cert
              readOnly: true
              mountPath: /var/paas/cert
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - '-c'
                - exit 0
            initialDelaySeconds: 20
            timeoutSeconds: 10
            periodSeconds: 20
            successThreshold: 1
            failureThreshold: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 10000
            runAsGroup: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      serviceAccountName: log-agent-serviceaccount
      serviceAccount: log-agent-serviceaccount
      securityContext:
        fsGroup: 10000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - log-agent-otel-collector
                topologyKey: topology.kubernetes.io/zone
      schedulerName: default-scheduler
      tolerations:
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 30
        - key: node.kubernetes.io/unreachable
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 30
        - key: role
          operator: Exists
        - key: distribution.io/category
          operator: Equal
          value: IES
          effect: NoSchedule
      priorityClassName: system-cluster-critical
      dnsConfig:
        options:
          - name: ndots
            value: '3'
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 5
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 120
status:
  observedGeneration: 3
  replicas: 2
  updatedReplicas: 2
  readyReplicas: 2
  availableReplicas: 2
  conditions:
    - type: Available
      status: 'True'
      lastUpdateTime: '2025-09-22T07:28:16Z'
      lastTransitionTime: '2025-09-22T07:28:16Z'
      reason: MinimumReplicasAvailable
      message: Deployment has minimum availability.
    - type: Progressing
      status: 'True'
      lastUpdateTime: '2025-09-22T07:40:31Z'
      lastTransitionTime: '2025-09-22T07:28:09Z'
      reason: NewReplicaSetAvailable
      message: ReplicaSet "log-agent-otel-collector-5cfd6f4c7c" has successfully progressed.</code></pre><ul><li>为避免配置覆盖以及确保配置生效，指定 Otel Collector 启动加载生效的配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437139" alt="图片" title="图片" loading="lazy"/></p><ul><li>Otel Collector 挂载新的配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437140" alt="图片" title="图片" loading="lazy"/></p><ul><li>关闭健康检查</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437141" alt="图片" title="图片" loading="lazy"/></p><ul><li>若要实现 LTS 和观测云的数据双写，挂载的配置如下：</li></ul><pre><code>exporters:
  aom/default-event-aom:
    endpoint: https://aom.cn-north-4.myhuaweicloud.com
    events:
    - name: DeleteNodeWithNoServer
      name_cn: 废弃节点清理
  ...
  lts/default-stdout:
    compress_type: gzip
    endpoint: https://lts-access.cn-north-4.myhuaweicloud.com:8102
    log_type: log
    lts_group_id: d6b393b8-484f-4835-ba9f-xxxxx
    lts_stream_id: 8e02106f-8aeb-4da5-a5e1-xxxxx
  otlphttp:
    endpoint: http://datakit-service.datakit:9529/otel
    tls:
      insecure: true          
processors:
  batch/default-event:
    send_batch_max_size: 1000
    send_batch_size: 500
    timeout: 1000000000
  ...
  filter/cci-log:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: cci-log
  filter/datakit:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: datakit
service:
  pipelines:
    logs/cci-log:
      exporters:
      - lts/cci-log
      - otlphttp
  ...</code></pre><ul><li>挂载的配置若是只写到观测云，配置如下：</li></ul><pre><code>exporters:
  otlphttp:
    endpoint: http://datakit-service.datakit:9529/otel
    tls:
      insecure: true
processors:
  batch/logs:
    send_batch_max_size: 2000
    send_batch_size: 2000
  filter/cci-log:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: cci-log
receivers:
  fluentforward:
    endpoint: ${POD_IP}:8006
    tls:
      cert_file: /var/paas/cert/serverCert
      client_ca_file: /var/paas/cert/caCert
      key_file: /var/paas/cert/serverKey
  k8s_events: {}
service:
  pipelines:
    logs/cci-log:
      exporters:
      - otlphttp
      processors:
      - filter/cci-log
      - batch/logs
      receivers:
      - fluentforward
  telemetry:
    logs: {}
    metrics:
      address: ${POD_IP}:8888
      level: basic</code></pre><h3>步骤 5：容器 demo 发起请求，产生日志</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437142" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 6：在观测云验证日志接入</h3><ul><li>登录观测云控制台 → 日志查看器 ，可以看到相关日志已经被采集到了观测云。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437143" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[AI重塑招聘决策：从“经验判断”到“数据]]></title>    <link>https://segmentfault.com/a/1190000047437213</link>    <guid>https://segmentfault.com/a/1190000047437213</guid>    <pubDate>2025-11-28 20:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI重塑招聘决策：从“经验判断”到“数据支撑”的变革<br/>AI破解招聘隐性损耗：重构面试与寻访的核心逻辑<br/>传统招聘中，企业常将招聘困境归咎于“简历不足”“人才难寻”，但真正消耗HR与业务部门精力的，是面试判断不准、沟通流程低效、候选人体验不佳三大隐性问题。这些问题虽不直接爆发，却在持续侵蚀企业的用人成本、品牌口碑与人才转化率。在AI技术深度渗透人力资源领域的当下，企业需要的已非单纯执行工具操作的AI，而是具备专业判断力、能支撑招聘决策的智能系统。<br/>这类AI招聘系统的核心价值，在于彻底解决传统招聘“面试不准”的痛点，推动招聘决策从“凭感觉”走向“靠数据”，为企业构建更高效、精准的人才选拔体系。</p><p>从“参考辅助”到“决策核心”：AI重构招聘评估标准<br/>对企业而言，AI招聘系统的核心价值判断标准，在于其评分结果是否精准、能否直接支撑招聘决策。当前领先的AI面试系统，已在评估精准度上实现突破性进展，完全能够满足企业的决策需求。<br/>其可靠性源于多维度的科学验证：评分结果可与人工面试官进行一对一“背靠背”对比校准，同时通过了效标效度与重测稳定信度两大心理学指标检验，确保评分的客观性与稳定性。这意味着，AI的评估结果足够可靠，可直接作为招聘决策依据，帮助企业摆脱“凭经验选人”的困境。经过持续迭代，当前顶尖的AI面试系统，已将判断能力提升至国际领先水准。<br/>全环节技术渗透：AI解决招聘“三难”问题<br/>传统AI面试常陷入“只会提问、不会判断”的误区，而领先的AI面试系统将“精准度”落实到面试的每一个细节环节，实现评估质量与效率的双重提升：<br/>•一问多能提升效率：单道题目可同步评估多项胜任力，实现初筛与技术复试的无缝衔接，让招聘评估效率提升50%以上，减少流程冗余。<br/>•自由追问捕捉关键：能够根据候选人的回答即时生成针对性追问，避免遗漏核心信息，打破模板化面试的局限，实现“有思考的互动评估”。<br/>•简历挖掘防范风险：自动识别简历中的模糊信息与潜在疑点，生成递进式提问，既能有效防范信息造假，又能避免HR因主观疏忽错失优质候选人；同时覆盖通用能力与专业能力考察，既能评估沟通、协作等通用素质，也能针对编程、算法、工程、财务等专业领域精准出题，大幅减轻HR与专业面试官的工作负担。<br/>从“机械交互”到“品牌增值”：AI重塑候选人体验<br/>过去，AI面试常因交互冷漠、流程呆板引发候选人投诉，影响雇主品牌形象。而新一代AI面试系统通过拟人化设计，实现了候选人体验的全面升级，让面试成为企业展示品牌价值的窗口：<br/>•情绪感知式沟通：系统能实时捕捉候选人的语速变化、情绪波动，甚至从回答中解读潜台词，用引导式语言帮助候选人缓解紧张，充分展现真实能力，而非机械地“完成问答流程”。<br/>•无干扰流畅体验：无需候选人手动操作“开始答题”“结束录制”，系统可自动识别回答状态，无缝衔接下一问题，交互节奏如同与真人HR面对面交流，消除流程中断带来的不适感。<br/>•沉浸式视觉交互：语音与口型实现高精度同步，彻底告别传统AI“纸片人”般的疏离感，让候选人在更自然的场景中完成面试。<br/>•双向信息互通：候选人可随时提出关于岗位职责、薪酬福利、职业发展路径等疑问，AI能即时给出准确解答，帮助候选人全面了解企业，提升其入职意愿。在优质人才稀缺的当下，良好的面试体验已成为企业吸引人才的重要竞争力。<br/>全流程自动化：AI重构人才寻访链路<br/>AI招聘的能力已突破单一面试环节，延伸至人才寻访全流程，形成一套可独立运行的自动化招聘体系，彻底改变传统人才寻访模式：<br/>•快速启动无延迟：仅需30-60秒完成参数设置，系统即可自动启动服务，无需专人值守，大幅降低人力成本。<br/>•智能筛选精准匹配：根据企业预设的学历、工作经验、技能要求等条件，自动从简历库中识别符合要求的候选人，排除无效信息干扰。<br/>•拟人化动态沟通：模拟人类交流语气发起对话，通过问答式互动进一步了解候选人意向与能力，若发现不匹配则即时终止沟通，避免资源浪费。<br/>•全量消息无遗漏：自动遍历所有未读消息，针对不同候选人的疑问给出个性化回复，确保不错失任何潜在人才。<br/>•数据自动流转：当候选人信息不全时，系统会主动以自然语言索取简历；确认候选人符合条件后，自动下载简历并同步至企业ATS系统，生成完整档案，实现“筛选-沟通-归档”全链路自动化。</p>]]></description></item><item>    <title><![CDATA[# Wireguard服务器管理工具之 ]]></title>    <link>https://segmentfault.com/a/1190000047437247</link>    <guid>https://segmentfault.com/a/1190000047437247</guid>    <pubDate>2025-11-28 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>你知道的，Wireguard服务器是没有图形界面管理工具的，默认只能在命令行编辑配置文件进行添加删除用户，更没有其他高级功能了，这非常不方便，然后就自己开发了这个工具，已经用了好多年了，现在推荐给大家：<code>zzxia-wireguard-manage</code>，项目地址是：<code>https://gitee.com/zhf_sy/zzxia-wg-manage</code>。</p><p>[toc]</p><h2>1 介绍</h2><p>wireguard 服务器管理工具。提供服务器配置、重启、警报、报表等功能；提供用户的列出、添加、删除、导出配置、二维码分享等功能</p><h3>1.1 功能：</h3><ol><li>配置服务器</li><li>列出账号</li><li>添加删除账号</li><li>导出用户配置文本，也可以二维码分享</li><li>重启</li><li>显示用户在线状态</li><li>用户登录发送钉钉消息</li><li>用户离线发送钉钉消息</li><li>每天生成用户报告</li><li>总报告</li></ol><h3>1.2 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk；<br/>&lt;img src="http://pic-bed.zzxia.vip/pic1/20210429155627295.jpg" alt="打赏" style="zoom:50%;" /&gt;</li></ol><h2>2 软件架构</h2><p>Linux shell</p><h2>3 安装教程</h2><ul><li>克隆下来即可</li><li>wireguard服务器的安装方法请参考官网</li></ul><h2>4 使用说明</h2><p>请使用-h|--help参数运行sh脚本即可看到使用帮助</p><h3>4.1 创建修改环境变量文件</h3><p>基于<code>env.sh.sample</code>创建环境变量文件<code>env.sh</code>，并根据自己的环境修改它：</p><pre><code class="bash">$ cat env.sh 
#!/bin/bash

## ----- 一般不需要修改 -----
# server env
SERVER_CONF_FILE_PATH="/etc/wireguard"             #--- wireguard服务器配置文件路径
WG_IF='wg0'                                        #--- wireguard服务器网卡
IP_PREFIX='172.30.0'                               #--- wireguard服务器网络地址前3节
IP_NETMASK='24'                                    #--- wireguard服务器IP掩码
# run
SERVER_CONF_FILE="${SERVER_CONF_FILE_PATH}/${WG_IF}.conf"
SERVER_PRIVATE_KEY="${SERVER_CONF_FILE_PATH}/private.key"
TODAY_WG_USER_LATEST_LOGIN_FILE="/tmp/wg-user-first-login-today.txt"

## ----- 一般需要修改 -----
# 钉钉
export DINGDING_API_URL_FOR_LOGIN="https://oapi.dingtalk.com/robot/send?access_token=填上你的token在这里"      #-- 用来发送钉钉消息
# server env
SERVER_CONNECT_INFO='服务器IP或域名:端口如51820'            #--- wireguard服务器用以接受用户连接的IP或域名及端口，用来生成用户的wg配置文件
# user env
USER_DNSs='192.168.11.3,192.168.11.4'                       #--- 用户的DNS，用来设置用户的DNS
USER_ALOWED_IPs="${IP_PREFIX}.0/${IP_NETMASK},0.0.0.0/0"    #--- 用户端走Wireguard链路的网络地址范围（用来设置用户端路由）</code></pre><h3>4.2 服务器设置</h3><p>运行<code>0-init-setup.sh</code>用于第一次配置服务器：</p><pre><code class="bash"># ./0-init-setup.sh</code></pre><h3>4.3 服务器管理</h3><pre><code class="bash"># ./wg-manage.sh -h

    用途：用于wireguard的用户管理
    依赖：./env.sh
          qrencode
    注意：
    用法：
        ./wg-manage.sh  [-h|--help]
        ./wg-manage.sh  [-l|--list]
        ./wg-manage.sh  [-a|--add {用户名}]  &lt;{IP第4段}&gt;
        ./wg-manage.sh  [-r|--rm|-o|--output-config  {用户名}]
        ./wg-manage.sh  [-R|--reload]
        ./wg-manage.sh  [-s|--status]
    参数说明：
        \$0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出现有用户
        -a|--add       添加用户
        -r|--rm        删除用户
        -o|--output-config 输出用户配置文件
        -R|--reload    重启服务器
        -s|--status    服务器状态
    示例:
        #
        ./wg-manage.sh  -l              #--- 列出用户清单
        #
        ./wg-manage.sh  -a 猪猪侠 11    #--- 添加用户【猪猪侠】，IP地址尾号为【11】
        ./wg-manage.sh  -a 猪猪侠       #--- 添加用户【猪猪侠】，IP地址尾号自动分配
        #
        ./wg-manage.sh  -r 猪猪侠       #--- 删除用户【猪猪侠】
        #
        ./wg-manage.sh  -o 猪猪侠       #--- 输出用户【猪猪侠】的配置文件
        #
        ./wg-manage.sh  -R              #--- 重启服务器
        #
        ./wg-manage.sh  -s              #--- 查看服务器状态</code></pre><h3>4.3 用户登录警报级用户登录报告（可选）</h3><p>添加计划任务：</p><pre><code class="bash"># ./1-add-crontab.sh</code></pre><p>查看报告：</p><pre><code class="bash"># cat ./report/wg-report.list
+------------+----------+---------+---------+----------+-------------+----------------+
|日期        |用户名    |总流浪MiB|IN流量MiB|OUT流量MiB|IP           |远程IP          |
+------------+----------+---------+---------+----------+-------------+----------------+
|2021-07-19  |HB清      |.3       |.1       |.2        |172.30.5.23  |113.111.63.250  |
|2021-07-19  |HB颖      |28.9     |3.4      |25.5      |172.30.5.31  |113.111.63.250  |
|2021-07-20  |HB宇      |45.3     |8.3      |37.0      |172.30.5.22  |113.111.63.250  |
|2021-07-20  |HB华      |708.9    |101.6    |607.3     |172.30.5.23  |113.111.63.250  |
+------------+----------+---------+---------+----------+-------------+----------------+</code></pre><p>警报需要钉钉API，方法是建立钉钉群，然后添加一个钉钉机器人，然后把得到的api url写入到<code>env.sh</code>即可。</p><p>测试警报：</p><pre><code class="bash"># ./wg-login-alert-cron.sh</code></pre><h2>5 总结</h2><p>真的好好用！</p>]]></description></item><item>    <title><![CDATA[朝阳永续基于阿里云 Milvus 构建金]]></title>    <link>https://segmentfault.com/a/1190000047436995</link>    <guid>https://segmentfault.com/a/1190000047436995</guid>    <pubDate>2025-11-28 19:08:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、客户简介</h2><p>朝阳永续是先进的金融数据与智能服务提供商，致力于为基金管理公司、证券研究机构及专业投资者提供高质量、精准和全面的数据分析与决策支持工具。依托多年深耕金融行业的数据积累与投研经验，朝阳永续推出其核心产品——AI 小二，一款融合大模型技术的 AI 金融投研智能体。</p><p>AI 小二基于生成式 AI 能力，结合阿里云向量检索服务 Milvus 版（简称阿里云 Milvus），打造了集“智能问答、极速研究、深度分析、主题推荐、报表生成”于一体的智能化投研平台，全面赋能投资研究流程，显著提升用户的研究效率与决策质量。<br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdncwC" alt="image.png" title="image.png"/></p><h2>二、业务背景与诉求</h2><p>在金融投研领域，信息的准确性、权威性和时效性至关重要。AI 小二作为面向专业用户的严谨型 AI 应用，要求所有输出内容必须具备可追溯的信源依据，并确保引用观点来自权威、合规的数据来源。</p><p>然而，在面对日益增长的多源异构金融数据时，传统技术架构面临四大核心挑战：<br/>1. 数据规模庞大且复杂度高<br/>朝阳永续需处理包括上市公司公告、财报、卖方研究报告、新闻资讯、基金数据在内的海量非结构化与半结构化数据，日均新增数据量持续攀升。传统的关键词检索方式难以理解语义关联，无法满足用户对精准匹配的需求。</p><p>2. 检索精度与召回率难以兼顾<br/>用户常以自然语言提出复杂查询需求，例如：“某公司在过去四个财报季中管理层表述的变化趋势”。此类问题涉及跨文档、跨时间维度的语义理解，传统系统因缺乏上下文感知能力，导致漏检率高、误判频发，严重影响研究结果的可靠性。</p><p>3. 系统性能与稳定性压力巨大<br/>金融市场具有极强的实时性要求，尤其在交易时段，任何延迟都可能影响投资决策。原有自建检索系统在数据量达到亿级后，出现查询延迟波动、节点负载不均等问题，部分复杂查询响应时间超过数分钟，已无法满足高频、低延迟的业务需求。</p><p>4. 运维成本高昂，资源管理复杂<br/>随着集群规模扩大，运维团队需投入大量人力进行监控、调优、故障恢复和版本升级。这不仅增加了运营负担，也严重挤占了核心技术研发的时间与资源。</p><h2>三、阿⾥云的解决⽅案</h2><p>为突破上述瓶颈，朝阳永续携手阿里云，引入 阿里云向量检索服务 Milvus 版，构建高性能、高可用的金融级语义检索底座，支撑 AI 小二实现从“数据到洞察”的高效转化。<br/><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdncwD" alt="image.png" title="image.png" loading="lazy"/><br/>系统采用分层架构：<br/>底层：存储汇聚上市公司公告、研报、财报等原始文本数据；<br/>中间层：通过经由 PDF 解析、 Embedding 等处理环节形成向量化数据，并存入阿里云 Milvus；<br/>上层：利用阿里云 Milvus 强大的语义检索与条件过滤能力，实现多模态召回；<br/>接入层：通过标准化接口为 AI 小二提供低延迟、高并发的检索服务。</p><p>整个系统实现了从数据摄入、向量化、存储到检索的全链路自动化，为金融级 AI 应用提供了坚实的技术支撑。</p><h2>四、典型应用场景与效果验证</h2><p><strong>案例一：追踪上市公司“互动易”表述变化（上市公司互动易数据）</strong><br/>上市公司在历史多时期内的互动易表述，往往隐含管理层对未来业绩、行业趋势的前瞻性判断。这类非结构化文本难以通过关键词检索准确提取。</p><p>借助阿里云 Milvus 的高精度语义匹配能力，AI 小二能够快速定位并召回特定时间段内相关问答内容，帮助研究员高效捕捉企业战略动向与情绪变化，显著提升信息挖掘效率。</p><p><strong>案例二：分析卖方分析师观点演变（卖方研报数据）</strong><br/>朝阳永续累计收录超300万份经合规授权的卖方研究报告，每日新增数千篇。投资者常需分析某分析师对某一公司的长期观点演变。</p><p>阿里云 Milvus 支持大规模向量实时写入与高效检索，在财报披露高峰期仍能稳定响应复杂查询，确保用户及时获取最新市场研判，强化投研时效性。<br/><img width="723" height="208" referrerpolicy="no-referrer" src="/img/bVdncwE" alt="image.png" title="image.png" loading="lazy"/><br/><strong>案例三：解读财报中的管理层表述（上市公司财报数据）</strong><br/>财报信息是众多专业投资者关注的重要一手信息，其中，管理层在财报中的经营表述可能蕴含重要的投资信息。AI 小二通过将百万级财报文本向量化，并基于阿里云 Milvus 实现精准语义召回，可快速提取与用户问题相关的段落，再由大模型进行情感分析与趋势提炼，大幅提升专业投资者财报研究效率。</p><h2>五、为什么选择阿⾥云 Milvus</h2><p>在向量数据库选型过程中，朝阳永续曾长期使用开源 PostgreSQL 向量扩展方案，但在实际应用中暴露出性能瓶颈与运维难题。最终全面迁移至 阿里云 Milvus 向量检索服务，主要基于以下三大关键考量：<br/><strong>1. 性能飞跃：查询速度提升十倍以上</strong><br/>经验证：开源 PG 库平均响应速度600ms，阿里云Milvus平均响应速度50ms。已有的上市公司公告、基金公司公告、卖方研究报告，专利及法律文档等，Embedding完成后，向量的数量级已达到十亿量级。开源 PG 库在结合标量条件筛选，并开启向量检索和全文检索的混合检索模式下，平均响应时间已冲到600ms，极端情况长达数分钟。切换至阿里云 Milvus 后，同样规模的数据，类似的检索方式，平均响应速度提升至50ms，提速十倍以上，充分满足金融业务对实时性的严苛要求。</p><p><strong>2. 运维大幅降低，工作量下降80%</strong><br/>开源 PG 方案遵循一切自己动手的原则，大量监控框架均需要搭配其他开源项目进行部署，需要花费运维人员大量的时间调研，且不完全符合运维需要，须定制整合，成本高，运维难度大。</p><p>阿里云 Milvus 具备完善的监控能力，提供包括 CPU 和内存使用率在内的逾百项监控指标，并支持自定义报警规则，可灵活适应多样化的业务需求。这一全方位的监控体系有效帮助朝阳用户技术实现对集群运行状态的精准感知与及时响应。同时，阿里云 Milvus 还支持灵活的资源调整机制，可根据业务负载变化，平滑实现资源的扩容或缩容，保障服务持续稳定运行。</p><p><strong>3. 生产级稳定性：历经百次高峰冲击，零故障运行</strong><br/>在开盘、重大政策发布等流量高峰期间，原有 PG 集群频繁出现CPU打满、服务卡顿甚至宕机现象。</p><p>切换至阿里云 Milvus 后，面对这些投研及投顾领域的热点事件及开盘、收盘时间点的高频应用时间。阿里云 Milvus 历经半年百余次瞬时访问峰值考验，始终保持 0 故障、0 中断，展现出卓越的高可用性与抗压能力，真正达到金融级标准。</p><h2>六、总结</h2><p>通过采用 阿里云向量检索服务 Milvus 版，朝阳永续成功构建了高性能、高可靠的金融语义检索引擎，有效解决了海量非结构化数据下的检索效率、精度与稳定性难题，为“AI 小二”提供了强大的底层支撑，显著提升了智能投研服务的用户体验与商业价值。</p><p>未来，朝阳永续期待与阿里云在大模型、向量检索、Agent 记忆系统等领域深化合作，共同探索其在智能风控、资产配置、合规审查等更多金融场景的创新应用，携手推动中国金融行业的数字化转型与智能化升级。</p>]]></description></item><item>    <title><![CDATA[在 Gemini CLI 中使用 Gem]]></title>    <link>https://segmentfault.com/a/1190000047437007</link>    <guid>https://segmentfault.com/a/1190000047437007</guid>    <pubDate>2025-11-28 19:07:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>现在可以在 Gemini CLI 中用 Gemini 3 Pro 啦。</p><p>这下子终端不仅仅是一个输入指令的窗口，而是变成了一个具备执行力的开发环境，并且还能通过 Agentic Coding（代理编码）处理复杂的工程任务，并通过调用外部工具优化工作流。</p><p>目前 Google AI Ultra 订阅用户或持有付费 Gemini API Key 的用户可以使用，而其他用户可以加入<a href="https://link.segmentfault.com/?enc=%2B8yY1SZFW14zCDDkfMwxzw%3D%3D.I0hAbAaNcPG2IyDgcuvU6HQvS4KjwqyQ97XgPEKc0TRp8%2BCFaN27mgwcOkYR5L%2Bi" rel="nofollow" target="_blank">加入候补名单</a>，等官方开放权限。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncHi" alt="image.png" title="image.png"/></p><h3>如何在 Gemini CLI 使用 Gemini 3 Pro</h3><p>首先要部署好<a href="https://link.segmentfault.com/?enc=VhxgUMSeQ4i6Cgj91USbaQ%3D%3D.kpEtmOXNFhl1iBv%2FeJFfEAGeEcmE37Dc5Jv25aOGF4XfFNez8gY09sJy5zBpWbaD" rel="nofollow" target="_blank">Node.js 20或以上的环境</a>，如果不知道怎么安装。可以使用<a href="https://link.segmentfault.com/?enc=Dwy8QbOE5BdcMOmYNDVp7g%3D%3D.RnbXqf4extGSY%2Bq9VobrmfS9Om3Cn73wfr8L9FKelgM%3D" rel="nofollow" target="_blank">ServBay</a>，一键安装。</p><p>ServBay能够支持不同Node.js版本同时运行，并且能一键切换它们。只需要安装好ServBay后，在左边菜单的「软件包」中找到Node.js，点击安装即可。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdncHj" alt="image.png" title="image.png" loading="lazy"/></p><p>然后输入命令安装好Gemini。</p><pre><code>npm install -g @google/gemini-cli</code></pre><h4><strong>升级</strong> <strong>CLI</strong> <strong>版本</strong></h4><ol><li>所有这些工作准备好之后，输入下面的命令升级。</li></ol><pre><code>npm install -g @google/gemini-cli@latest</code></pre><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHl" alt="image.png" title="image.png" loading="lazy"/></p><ol start="2"><li><strong>开启预览功能</strong></li></ol><p>安装完成后，在终端运行 <code>/settings</code>，将 <code>Preview features</code> 设置为 <code>true</code>。此时，Gemini CLI 将默认使用 Gemini 3 Pro 模型。</p><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHm" alt="image.png" title="image.png" loading="lazy"/></p><p>以下是 4个具体场景，展示如何利用 Gemini 3 Pro 加速开发。</p><hr/><h3>利用 Agentic Coding 在终端直接构建应用</h3><p>Gemini 3 Pro 擅长整合文本、代码和视觉信息，并能遵循极其复杂的指令，一句话就能直接从生成一个可运行的项目骨架。</p><p><strong>实操案例：构建</strong> <strong>高保真</strong> <strong>3D 仿真场景</strong></p><p>传统的 3D 开发需要配置图形库、本地服务器和大量样板代码。现在，可以将创意简报和技术规格合并为一个 Prompt，让 CLI 直接生成结果。</p><p>比如输入以下指令，要求它生成一个金门大桥的 3D 模拟：</p><pre><code>Objective: Build a visually stunning, photorealistic 3D Voxel simulation of the Golden Gate Bridge using Three.js, prioritizing quality and complex visuals (no simple blocks), atmospheric depth and 60FPS performance.

Visuals  &amp;  Atmosphere:
Lighting: Slider (0-24h) controlling sun position, light intensity, sky color, and fog color.
Fog: Volumetric-style fog using sprite particles that drift and bob. Slider 0-100. 0 = True Zero (Crystal Clear). 100 = Dense but realistic (not whiteout).
Water: Custom GLSL shader with waves, specular reflections, and manual distance-based fog blending (exp2) for seamless horizon integration.
Post-Processing: ACESFilmic Tone Mapping and UnrealBloom (optimized for glowing lights at night).

Scene Details:
Bridge: Art Deco towers with concrete piers (anchored to seabed), main span catenary cables, and suspenders.
Terrain: Low-poly Marin Headlands and SF Peninsula.
Skyline: Procedural city blocks on the SF side.
Traffic: Up to 400 cars using InstancedMesh, positioned accurately on top of the deck (ensure vertical alignment prevents clipping into the concrete). Each car features emissive headlights (white) and taillights (red).
Ships: Procedural cargo ships with hull, containers, and functional navigation lights (Port/Starboard/Mast/Cabin) moving along the water.
Nature: Animated flocking birds.
Night Mode: At night, activate city lights, car headlights, ship navigation lights, tower beacons, street lights.

Tech  &amp;  Controls:
Core: Must output only single HTML file golden_gate_bridge.html to be run in a blank Chrome tab. Import Three.js/Addons via CDN map.
Libs: three (Core library) via CDN (ES Modules); three/examples/jsm/... modules via Import Map.
Build: No build step (Vite/Webpack). Pure HTML/JS.
UI: Visually appealing sliders for Time (0-24h), Fog Density (0-100%), Traffic Density (0-100%), and Camera Zoom.
Optimization: InstancedMesh for all repetitive elements (cars, lights, birds).</code></pre><p>Gemini 3 Pro 会理解对光照、GLSL 着色器和性能优化的具体要求，直接生成一个独立的 HTML 文件。</p><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHn" alt="image.png" title="image.png" loading="lazy"/></p><h3>多模态开发：从草图到代码</h3><p>如果想做一个视觉创意，Gemini 3 Pro 的多模态能力可以快速实现 UI 原型。只需将草图拖入终端，配合文字描述，它就能识别布局并生成代码。</p><p><strong>实操案例：还原</strong> <strong>赛博朋克</strong> <strong>风格</strong> <strong>UI</strong></p><p>假设正在设计一个网络安全监控工具，需要独特的视觉风格。将线框图（<code>@wireframe.png</code>）拖入终端，并输入以下指令：</p><pre><code>Build a UI prototype for "CyberSentinel," a real-time network security monitor. The visual style should be gritty Cyberpunk: neon green and hot pink grid lines against a deep void background. Instead of typical charts, visualize data streams as cascading "digital rain" or glitch-art pillars. When hovering over a data node, a holographic, semi-transparent info card should pop up with glitch effects, styled using Tailwind CSS. I have a rough wireframe here to guide the layout: @wireframe.png.</code></pre><p>模型不仅会还原线框图的结构，还会根据开发者对 "Cyberpunk"、"Digital Rain" 和 "Glitch effects" 的描述，编写对应的 CSS 动画和布局逻辑。</p><h3>逆向工程：自动生成项目文档</h3><p>Gemini 3 Pro 能够深入理解代码逻辑，而不仅仅是语法。这使得它非常适合为老旧项目或复杂的开源代码库补充文档。</p><p><strong>实操案例：为无文档代码生成说明书</strong></p><p>如果接手一个没有文档的项目时，可以让 Gemini 分析代码并生成结构化文档：</p><pre><code>This is an undocumented application. Please read through the entire codebase to understand the logic first, then generate user documentation for me. The documentation should include: user interactions (command-line options, authentication, etc.), explanations of core concepts (such as MCP), an architectural overview, and how to contribute to the open-source project. Please ensure the format is clear and easy to read; do not just provide a simple HTML page.</code></pre><h3>跨服务联动：排查云端故障</h3><p>Gemini 3 Pro 支持工具调用（Tool Use），可以根据你的指令制定多步骤计划，联动日志监控、安全扫描和代码库来解决复杂问题。</p><p><strong>实操案例：排查 Cloud Run 服务延迟</strong></p><p>当遇到线上性能问题时，它能化身为一个 SRE 助手：</p><pre><code>Users are reporting that the 'Save Changes' button is slow to respond. Please investigate the status of the 'tech-stack' service.</code></pre><p>它会自动连接 Cloud Run 查看指标，调用 Snyk 等工具扫描潜在问题，并结合代码变更记录，最终给出根因分析甚至修复补丁。</p><h3>总结</h3><p>这些案例只是冰山一角。Gemini 3 Pro 的真正价值在于它能适应开发具体场景，无论是优化一行命令，还是构建一个完整的功能模块。</p><p>感兴趣可以升级体验。</p>]]></description></item><item>    <title><![CDATA[活动推荐丨「实时互动 × 对话式 AI」]]></title>    <link>https://segmentfault.com/a/1190000047437028</link>    <guid>https://segmentfault.com/a/1190000047437028</guid>    <pubDate>2025-11-28 19:06:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437030" alt="" title=""/></p><p>声网博客www.shengwang.cn/blog/ 正式启航啦！这是一个聚焦实时互动（RTE） × 对话式 AI（Conversational AI） 的内容空间。</p><p>我们关注技术背后的Why，探讨应用场景的What if，分享实战经验的How better。在声网博客，我们希望呈现：</p><ul><li>技术解析：从原理到落地的深度剖析。</li><li>应用创新：实时互动与对话式AI 如何重塑场景。</li><li>实战经验：工程师世界里的技术挑战与解法。</li><li>趋势洞察：对未来技术范式的思考。</li></ul><p>博客刚刚开张，内容区还像一个刚初始化、等待被写入的模块组，所以诚邀你成为第一批内容共建者。</p><p>为此，我们全新推出「时空切片」特刊，用以捕捉和记录大家在“实时互动 X 对话式 AI ” 世界里的思想火花、实战经验……并将它们沉淀成一帧帧可被分享的时空标注。</p><h2>活动主题</h2><p><strong>为了让大家更轻松地加入，特刊第一期定了个能快速上手的征文主题：</strong></p><p><strong>《“实时互动 X 对话式AI”工作簿》</strong></p><p>我们想听听你在“RTE × 对话式 AI” 这件事上，看到了什么、做过什么、想过什么。</p><p>无论是技术拆解、实践经验、场景思考，还是灵光一现的洞察，<strong>只要与“实时互动、对话式AI”相关</strong>，欢迎你的“内容投喂”。</p><h2>活动奖励</h2><p>文章一旦入选，就有机会被数万技术er刷到。至于能不能“爆”，就让阅读量这个指标来跑分吧。</p><ul><li><p>阳光奖</p><p>作品入选，即赠 定制 3C 认证充电宝</p></li><li><p>内容战力榜（按单篇阅读量从高到低排序）</p><p>No.1  AirPods</p><p>No.2-3 各 500 元京东卡</p><p>No.4-6 各 200 元京东卡</p></li><li><p>好文助力出圈</p><p>连更 ≥3 篇，为你开设个人专栏</p><p>优秀作品将在小红书、CSDN 、InfoQ 等平台原文署名发布</p></li></ul><h2>活动评选与投稿方式</h2><h3>评选方式</h3><ul><li>初审关卡：博客内容小组会对提交作品进行初审，入选后将发布在博客征文活动页。</li><li>内容战力榜：所有入选作品会按阅读量排名，阅读量=内容战力值（同一作者若多篇冲榜，只取阅读量最高的那篇计名）。</li><li>光荣榜公布：获奖结果将统一公示在博客活动页，人人都能查。</li></ul><h3>活动时间</h3><ul><li><strong>2025 年 11 月 17 日 - 2025 年 12 月 31 日</strong></li></ul><h3>征文要求</h3><ul><li>字数：1000字以上，Word 或 Markdown 都行。</li><li>内容：必须原创，未在其他平台发布，可加图表、代码等。</li><li>主题范围：实时互动、对话式AI、应用场景、实战经验……总之跟“RTE × 对话式AI”沾边的都欢迎！</li></ul><h3>提交方式</h3><ul><li>邮件格式：[声网博客]+ 文章标题 + 作者姓名，记得附上作品哟！</li><li>发送至邮箱：<a href="mailto:blog@shengwang.cn" target="_blank">blog@shengwang.cn</a></li></ul><h2>Q&amp;A</h2><p>1）奖品什么时候发？</p><ul><li>作品入选后，小编会邮件联系你确认信息；奖品将在征文活动结束后两周内发出。</li></ul><p>2）投稿遇到问题？</p><ul><li>对征文活动有任何疑问，统统可以邮件联系我们blog\@shengwang.cn，备注[博客征文]，你的疑问，我们都会认真回复。</li></ul><p>逛博客请查看</p><p><a href="https://link.segmentfault.com/?enc=YZyGYmsRy%2BT3nATjBpTl8g%3D%3D.QWDFFSw40VmLjRZLiDienGnt%2FZnHlMpO8UWsmGWsoaE%3D" rel="nofollow" target="_blank">https://www.shengwang.cn/blog/</a> </p><p>活动详情查看</p><p><a href="https://link.segmentfault.com/?enc=17QIs7rYERUbMZnti3xaQg%3D%3D.hBLknFtKT7uR51tYbIccfmaD3p549WFLnocd8eq8Wn9yKSy6ZOgBd0liS%2FZm7GcLBKV7hTP9HSAzPNuRWG86oQ%3D%3D" rel="nofollow" target="_blank">https://www.shengwang.cn/blog/blogdetail/RTExConvoAI-campaign/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437031" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437032" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=y%2B3BtIckZ8n8BYNHP1QcNA%3D%3D.WtDJwsVXDplwQmB7P9VHjRCSs5cyTSYUY%2Ft%2BQQ4KIFs%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437033" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[用 Python 将 `.py` 文件转]]></title>    <link>https://segmentfault.com/a/1190000047437040</link>    <guid>https://segmentfault.com/a/1190000047437040</guid>    <pubDate>2025-11-28 19:05:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在开发工作中，我们经常需要分享或归档 Python 脚本文件。<code>.py</code> 文件虽然在编辑器中可读性强，但直接发送给他人时，缺乏格式统一、排版美观，且打印体验不佳。尤其是在技术文档、培训资料或项目报告中，将代码转换为 PDF 不仅便于阅读，也能保证跨平台展示效果一致。</p><p>Python 生态中有多种方式可以实现代码转 PDF，而 <strong><a href="https://link.segmentfault.com/?enc=AwWL7ubnvKEutlLm4qv1sw%3D%3D.OsQRc7w6vrqrq0J9tpbK%2Fi2fydSOSqvYynfU8ImnLJRln4saTIki1l1gpVuugx9E68u8f2VcMonOJegRsYp%2Fhg%3D%3D" rel="nofollow" target="_blank">Free Spire.Doc for Python</a></strong> 提供了稳定、易用的方案。本文将介绍两种常见的实现方法：<strong>普通文本方式</strong>和<strong>带语法高亮的 HTML 插入方式</strong>，并详细分析两者的差异与使用场景。</p><hr/><h2>1. 使用 Spire.Doc 将 Python 代码按行写入 PDF</h2><p>最简单的方法是将 <code>.py</code> 文件逐行读取，并将每一行以固定字体插入到 PDF 中。这种方式适合不需要语法高亮，只希望保留原始代码排版的场景。</p><pre><code class="python">from spire.doc import Document, FileFormat, BreakType, Color, LineSpacingRule, LineNumberingRestartMode

# 读取 Python 文件
with open("Python.py", "r", encoding="utf-8") as f:
    python_code = f.read()

# 创建文档对象
doc = Document()
section = doc.AddSection()
paragraph = section.AddParagraph()

# 逐行添加代码
for line_number, line in enumerate(python_code.split("\n")):
    tr = paragraph.AppendText(line)
    tr.CharacterFormat.FontName = "Courier New"  # 设置等宽字体
    tr.CharacterFormat.FontSize = 10.5
    if line_number &lt; len(python_code.split("\n")) - 1:
        paragraph.AppendBreak(BreakType.LineBreak)

# 可选格式设置
paragraph.Format.BackColor = Color.get_WhiteSmoke()  # 背景色
paragraph.Format.LineSpacingRule = LineSpacingRule.Multiple
paragraph.Format.LineSpacing = 14.0

# 行号设置
section.PageSetup.LineNumberingStartValue = 1
section.PageSetup.LineNumberingStep = 1
section.PageSetup.LineNumberingRestartMode = LineNumberingRestartMode.RestartPage
section.PageSetup.LineNumberingDistanceFromText = 12.0

# 保存为 PDF
doc.SaveToFile("output/Python-PDF.pdf", FileFormat.PDF)</code></pre><p>转换结果：</p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdncHF" alt="转换py文件为PDF" title="转换py文件为PDF"/></p><p><strong>说明与优化点</strong>：</p><ul><li><strong>字体选择</strong>：使用等宽字体（如 Courier New）保证代码对齐整齐。</li><li><strong>行间距</strong>：设置多倍行距可提高可读性。</li><li><strong>背景色与行号</strong>：轻微灰色背景搭配行号，更适合打印或阅读。</li></ul><p>这种方式的优势在于实现简单，代码结构完全保留，兼容性高，但缺点是无法提供语法高亮效果，对于较长或复杂代码可读性稍差。</p><hr/><h2>2. 使用 Pygments 生成带语法高亮的 PDF</h2><p>如果希望 PDF 中的代码带颜色区分关键字、注释、字符串等，可以先使用 <strong>Pygments</strong> 将 Python 代码转换为 HTML，再通过 Spire.Doc 将 HTML 插入 PDF。</p><pre><code class="python">from spire.doc import Document, FileFormat
from pygments import highlight
from pygments.lexers import PythonLexer
from pygments.formatters import HtmlFormatter

def py_to_inline_html(py_file_path):
    with open(py_file_path, "r", encoding="utf-8") as f:
        code = f.read()
    # 生成行内 HTML，带行号
    formatter = HtmlFormatter(noclasses=True, linenostart=1, linenos='inline')
    return highlight(code, PythonLexer(), formatter)

html_result = py_to_inline_html("Python.py")

doc = Document()
section = doc.AddSection()
paragraph = section.AddParagraph()
paragraph.AppendHTML(html_result)

# 保存带高亮的 PDF
doc.SaveToFile("output/Python-PDF-Highlighted.pdf", FileFormat.PDF)</code></pre><p>转换结果：</p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdncHH" alt="转换py文件为PDF" title="转换py文件为PDF" loading="lazy"/></p><p><strong>关键说明</strong>：</p><ul><li><code>HtmlFormatter(noclasses=True, linenos='inline')</code>：生成内联样式 HTML，并带行号。</li><li><code>AppendHTML</code> 方法可以直接将 HTML 内容插入到 PDF，保留语法高亮效果。</li><li>使用这种方法生成的 PDF 更美观，适合演示文档、教程或培训资料。</li></ul><hr/><h2>3. 两种方法的对比与使用建议</h2><table><thead><tr><th>特性</th><th>按行插入文本</th><th>HTML 语法高亮插入</th></tr></thead><tbody><tr><td>复杂度</td><td>简单</td><td>中等，需要 Pygments</td></tr><tr><td>可读性</td><td>一般</td><td>高，关键字、注释颜色区分明显</td></tr><tr><td>打印效果</td><td>普通</td><td>良好，但颜色需打印机支持</td></tr><tr><td>适用场景</td><td>快速生成、代码归档</td><td>教学文档、演示、报告</td></tr></tbody></table><p>总结来看，如果对语法高亮要求不高，按行插入文本即可；如果希望 PDF 更美观、可读性高，HTML 高亮方式更合适。</p><hr/><h2>4. 扩展说明</h2><ol><li><strong>合并重复操作</strong>：在按行插入的方式中，背景色、行距、字体等可封装为函数，避免重复设置，提高代码复用性。</li><li><strong>代码排版与打印</strong>：PDF 是固定排版格式的文档，将 Python 代码导出后可确保不同环境中显示一致，避免字体或缩进混乱。</li><li><strong>批量处理</strong>：可将以上方法封装为函数，循环处理多个 <code>.py</code> 文件，实现批量生成 PDF，适合团队协作或项目文档归档。</li></ol><hr/><h2>总结</h2><p>本文介绍了两种将 Python 脚本转换为 PDF 的方法：一种是按行插入文本，另一种是通过 HTML 生成语法高亮效果。前者简单高效，适合快速归档；后者美观专业，适合文档和演示场景。通过掌握 <strong>Spire.Doc</strong> 的 <code>AppendText</code> 和 <code>AppendHTML</code> 方法，以及 Pygments 的 HTML 转换能力，可以轻松生成结构清晰、可读性强的 Python PDF 文档。</p><p>无论是个人笔记整理、项目文档归档，还是教学演示，这两种方法都能满足不同需求，提高代码分享和管理的效率，同时保留排版美观性。</p>]]></description></item><item>    <title><![CDATA[共筑AI时代开源OS新生态，龙蜥社区走进]]></title>    <link>https://segmentfault.com/a/1190000047437046</link>    <guid>https://segmentfault.com/a/1190000047437046</guid>    <pubDate>2025-11-28 19:05:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>「龙蜥社区“走进系列”MeetUp」是由龙蜥社区与生态合作伙伴联合主办的系列月度活动，每期走进一家企业，聚焦龙蜥社区和合作伙伴的技术、产品和创新动态，展示硬核技术，共建繁荣生态。</p><p>龙蜥社区“走进系列”MeetUp 第 19 期将携手 Arm，聚焦“共筑 AI 时代开源 OS 新生态”主题，举办一场深度技术研讨会。随着大语言模型（LLM）与生成式 AI 吹响人工智能时代的新号角，本期 MeetUp 将围绕龙蜥操作系统与 Arm Neoverse 平台，全面呈现 AI 进化的全链路实践。内容涵盖硬件 IP、服务器平台、Java 虚拟机、模型推理框架与引擎、优化分析工具，并特别分享在智能驾驶场景中的系统优化与部署经验。诚邀您一同探索 AI 时代下开源操作系统新生态的开放演进之路。</p><p>时间：12 月 11 日 13:00-17:10</p><p>地点：上海漕河泾万丽酒店2楼</p><p>报名链接：<a href="https://link.segmentfault.com/?enc=oS41hP5k0%2FmcFnFH3%2BUTcg%3D%3D.dxZDpMtG%2F75w%2B%2BtypZdXKgRb3RDHTTBPgsVAulMxnuAHRCkpJNK4ueLcRca27bbC" rel="nofollow" target="_blank">https://openanolis.mikecrm.com/3sbZgtt</a></p><p>更多详细议程见下方海报：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437048" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[直播预告：LLM for AIOPS，是]]></title>    <link>https://segmentfault.com/a/1190000047437051</link>    <guid>https://segmentfault.com/a/1190000047437051</guid>    <pubDate>2025-11-28 19:04:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在 AI 与本土化双重浪潮之下，服务器操作系统正迎来历史性变革。由龙蜥社区理事长单位阿里云联合 InfoQ 打造的直播 IP 栏目《AI 进化论：智算时代操作系统的破局之路》，以云、AI、安全等技术与服务器操作系统如何融合演进为主线，聚焦服务器操作系统在智算时代的进化之路，特邀学术权威、行业专家、客户代表围绕原生智能、原生安全、软硬协同等热点议题展开深度对话。截至目前，已直播五期，线上观看人次达 30 万+。</p><p>当大模型浪潮席卷运维领域，LLM Agent 被寄予厚望——它究竟是解决复杂运维难题的“银弹”，还是被过度炒作的“泡沫”？《AI 进化论：智算时代操作系统的破局之路》系列直播第六期将于 12 月 8 日 14:00 开始，特别邀请到云杉网络总裁向阳，阿里云智能集团运维总监、龙蜥社区运维联盟主席冯富秋，InfoQ 极客传媒总经理、总编辑王一鹏三位嘉宾，围绕 “LLM for AIOPS，是泡沫还是银弹？” 这一主题展开深度探讨，聚焦 LLM Agent 在 AIOPS 中的真实价值与落地挑战，深入剖析其在故障诊断、根因分析、自动化响应等场景中的典型应用与常见误区；探讨如何通过提示工程、工具调用与反馈机制有效抑制幻觉，释放大模型在运维领域的潜力。</p><p>更多直播亮点，可点击下方海报了解，欢迎大家打开微信，扫描二维码预约直播：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437053" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[干货推荐：Java内存排查太难？这个「内]]></title>    <link>https://segmentfault.com/a/1190000047437059</link>    <guid>https://segmentfault.com/a/1190000047437059</guid>    <pubDate>2025-11-28 19:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>背景</h3><p>随着汽车行业加速智能化转型，从传统线下 IDC 集群向云端迁移并进行容器化改造，经常会遇到关于 Pod 内存异常、Pod发生 OOMKilled 的问题， 这些问题主要的矛盾点在于：<br/>1、Pod（容器）内存占用比 JVM 内存监控（堆内和堆外内存）占用大很多。<br/>2、总是有一部分消失的内存无法找出具体是哪部分占用。<br/>3、同一业务同一 JDK 版本，切换 OS 或容器化改造之后，才出现了 1、2 现象。</p><p>虽然 Java 工具千千万，但是选用什么工具排查起这类 Java 内存问题也是一个头疼的问题；甚至有时候翻遍了工具百宝箱，最后还是没有排查出问题的根因。经历过这些问题的洗礼之后，我们也从中总结了一些排查思路，并沉淀成一个阿里云操作系统控制台的 Java 内存诊断功能，帮助用户结合应用和操作系统的角度，快速揪出 Java 应用内存占用的元凶。</p><h3>消失的 Java 内存</h3><p>Java 内存组成<br/>为了找出消失的内存，我们首先要了解 Java 进程的主要内存组成以及现有工具和监控主要覆盖的部分；如下图所示可分为：</p><p>JVM 内存</p><ul><li>堆内存：可通过 -Xms/-Xmx 参数控制，内存大小可通过 memorymxbean 等获取。</li><li>堆外内存：包括元空间、压缩类空间、代码缓冲区、直接缓冲、线程栈等内存组成；它们分别可以通过-XX:MaxMetaspaceSize（元空间）、-XX:CompressedClassSpaceSize （压缩类空间）、 -XX:ReservedCodeCacheSize（ 代码缓冲区）、-XX:MaxDirectMemorySize (直接缓冲)、-Xss（线程栈）参数限制。</li></ul><p>非 JVM 内存</p><ul><li>JNI本地内存：即通过本地方法接口调用 C、C++ 代码（原生库），并在这部分代码中调用 C 库（malloc）或系统调用（brk、mmap）直接分配的内存。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437061" alt="图片" title="图片"/></p><h3>常见 Java 内存黑洞</h3><p>JNI 内存</p><p>通过对 Java 内存组成的了解，我们其实已经可以揭开第一个容易造成内存黑洞的隐藏 Boss-JNI 内存，因为这部分内存暂时没有工具可以获取其占用大小。</p><p>通常来说，编写相关业务代码的同学会认为代码中没有使用本地方法直接调用 C 库，所以不会存在这些问题，但是代码中引用的各种包却有可能会使用到 JNI 内存，比如说经典的使用 ZLIB 压缩库不当导致的 JNI 泄漏问题[2]。</p><p>LIBC 内存<br/>熟悉 Java 的同学都知道，JVM 是由 C++ 编写的，JVM 调用 malloc、free 申请/释放内存的过程中其实还要经过一个二道贩子 libc 库；以 gibc 中默认的内存分配器 ptmalloc 为例：</p><p>chunk 是 glibc 堆内存分配的最小单位，表示一段连续的内存区域。ptmalloc 会为每一个线程维护一个 Arena，每一个 Arena 中会有一个大 chunk（Top chunk）和 chunk 的缓存集合（bins）；当应用通过 malloc、free 申请/释放内存时、ptmalloc 会优先从 bins 中拿取和释放 chunk，如果没有符合要求的 bins，再从 top chunk 里面获取、再没有就通过 brk、mmap 系统调用从 OS 获取。</p><p>从上面的流程，我们可以发现，libc 作为二道贩子，很有可能多申请、扣留一些内存，从而导致 JVM 内存和进程实际内存的差异；我们也总结一下 libc 常见的一些问题：</p><ul><li>多线程 64M Arena 内存占用，libc 会为每个线程创建一个 64M 大小的 Arena，默认配置下在线程数量较多时会导致一定的内存浪费 [3]。</li><li>Top chunk 由于内存空洞导致无法及时释放回 OS [4]。</li><li>bins 缓存，JVM 释放的内存被缓存在 bins 中，导致内存差异 [4]。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437062" alt="图片" title="图片" loading="lazy"/></p><p>透明大页<br/>到了 OS 层，Linux 中的透明大页（Transparent Huge Page）机制也是造成 JVM 内存和实际内存差异的一大元凶。简单来说，THP 机制就是 OS 会将 4kb 页变成 2M 的大页，从而减少 TLB miss 和缺页中断，提升应用性能，但是也带来了一些内存浪费。如应用申请了一段 2M 的虚拟内存，但实际只用了里面的 4kb，但是由于 THP 机制，OS 已经分配了一个 2M 的页了[5]。</p><h3>通过阿里云操作系统控制台揪出Java内存占用元凶</h3><p>操作系统控制台是阿里云推出的一站式运维管理平台，充分结合了阿里在百万服务器运维领域的丰富经验。集成了监控、系统诊断、持续追踪、AI 可观测、集群健康度和 OS Copilot 等核心功能，专门应对云端高负载、网络延迟抖动、内存泄漏、内存溢出(OOM)、宕机、I/O 流量分析及性能抖动等各种复杂问题。</p><p>下面将以汽车行业客户在从线下 idc 集群迁移至云上 ACK 集群时遇到的由于 JNI 内存泄漏导致 Pod 频繁 OOM 为例，介绍如何通过操作系统控制台的内存全景分析功能[5]来一步步找出 Java 内存占用的元凶。</p><p>背景：</p><p>客户云上多个集群多个服务中的一些 Java 业务 pod 在没有任何服务异常、请求异常、流量异常的迹象下会偶发 OOM，且从 jvm 监控上内存也没有很大的波动（客户设置 5G limit，正常水位在 3G 左右）。</p><p>排查过程：</p><ul><li>尝试在内存高水位时对 Pod 发起内存全景分析，我们可以了解到当 Pod 中容器内存使用已经接近 limit，从诊断结论和容器内存占用分析中，我们可以看到容器内存主要是由于 Java 进程内存占用导致。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437063" alt="图片" title="图片" loading="lazy"/></p><p>对 Java 进程发起内存分析，查看诊断报告。报告展示了 Java 进程所在 Pod 和容器的 rss 和 WorkingSet（工作集）内存信息、进程 Pid、JVM 内存使用量（即 JVM 视角的内存使用量）、Java 进程内存使用量（进程实际占用内存），进程匿名用量以及进程文件内存用量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437064" alt="图片" title="图片" loading="lazy"/></p><p>通过诊断结论和 Java 内存占用饼图我们可以发现，进程实际内存占用比 JVM 监控显示的内存占用大 570M，全都由 JNI 内存所贡献。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437065" alt="图片" title="图片" loading="lazy"/></p><p>开启 JNI（Java Native Interface）内存分配 profiling，报告列出当前 Java 进程 JNI 内存分配调用火焰图，火焰图中为所有分配 JNI 内存的调用路径。（说明：由于是采样采集，火焰图中的内存大小不代表实际分配大小）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437066" alt="图片" title="图片" loading="lazy"/></p><ul><li>从内存分配火焰图中，我们可以看到主要的内存申请为 C2 compiler 正在进行代码 JIT 预热；</li><li>但是由于诊断的过程中没有发现 pod 有内存突增；所以我们进一步借助可以常态化运行的 Java CPU 热点追踪功能[7]尝试抓取内存升高时的进程热点，并通过热点对比[8]尝试对内存正常时的热点进行对比。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437067" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437068" alt="图片" title="图片" loading="lazy"/></p><ul><li>通过热点栈和热点分析对比，发现内存突增时间点的 CPU 栈也是 c2 compiler 的JIT 栈，且 c2 compiler 热点前有部分业务流量突增，且业务代码使用了大量反射操作（反射操作会导致 c2 compiler 进行新的预热）。</li></ul><p>排查结论：</p><p>C2 compiler JIT 过程申请 JNI 内存，且由于 glibc 内存空洞等原因导致申请内存放大且延时释放。</p><p>缓解方法：</p><p>1.调整 C2 compiler 参数，让其编译策略更保守，可以尝试调整相关参数，观察内存消耗变化。</p><p>2.调整 glibc 环境变量 MALLOC_TRIM_THRESHOLD_，让 glibc 及时将内存释放回操作系统。</p><p>联系我们 </p><p>您在使用操作系统控制台的过程中，有任何疑问和建议，可以搜索群号：94405014449 加入钉钉群反馈，欢迎大家扫码加入交流。</p><p>相关链接：</p><p>【1】阿里云操作系统控制台PC端链接：<a href="https://link.segmentfault.com/?enc=8Tg0g%2FnICAG4WHVYp5BLfw%3D%3D.Xw%2Bcb2tEEhHHKsYvrQIqqC%2B6Iic9eCeFN3x%2BYihqdd5gwHZmoj3XbI6041cJN8HA" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p>【2】java.util.zip内存泄露：<a href="https://link.segmentfault.com/?enc=1hTGkkUR8LNQ6UCng1igww%3D%3D.MiPVJ8IvJeVh7%2FlcaP06w4JPdY0x68DU4gSnnRj3tomzc9fW%2FwVNSDlnOX3Ldt9j" rel="nofollow" target="_blank">https://bugs.openjdk.org/browse/JDK-8257032</a></p><p>【3】glibc 64M arena内存浪费：<a href="https://link.segmentfault.com/?enc=8oJ5IiTuLetkUKRg7qjZYA%3D%3D.lbW5m%2BwSbTOS5cqqdoZz%2B%2FhCGgrxEEIQZHtysOtyZRJGGXnmYHUn6f4rP2UrG0gL" rel="nofollow" target="_blank">https://bugs.openjdk.org/browse/JDK-8193521</a></p><p>【4】glibc top chunk/fast bin内存不回收：<a href="https://link.segmentfault.com/?enc=1YvEEop8mZ1DfwmPjt7Gow%3D%3D.x1c2f9XrwRNFMI1RupovTjEjV2a%2FxKhoUJ0MHiU6ROcSSdZIciptSN%2FxiWr3RvJTolLF7rc1FxvVYTuNZkrJOFs55kFcCt775iXf7H4Ya9MiBEIfeWKkXVJBoxShDWIK" rel="nofollow" target="_blank">https://wenfh2020.com/2021/04/08/glibc-memory-leak/#332-fast-...</a></p><p>【5】go应用由于thp导致内存膨胀：<a href="https://link.segmentfault.com/?enc=m4Rc6t4yochtDoHMRqnR%2FA%3D%3D.BqTMjdFs4krcTZ8gHy2SqxqmuCNrZrMGlRxGrrBdaguBwvLlkGl7DAJ1hdXpJtB8" rel="nofollow" target="_blank">https://github.com/golang/go/issues/64332</a></p><p>【6】操作系统控制台内存全景分析：<a href="https://link.segmentfault.com/?enc=xSd6ffYjgwyHUw3sdKb4UQ%3D%3D.BeG7NA3ihFzjUToD2ylQW6IgAgT1cG7B2mKQaJIbFXyTFJ52sSaXac5b1rUMWhK%2FYCT8YX0%2FpMuJjaJLCKnPR99Fzl651SL46sjq%2FjF4sIoy0JDvOvHh64yhuq8Ud7KMzl1qzQc4P6I7oOdRW%2FmMtKLwAo2Ds%2BV3EQl9NuQZSjE%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/memory-panorama-...</a></p><p>【7】操作系统控制台热点追踪：<a href="https://link.segmentfault.com/?enc=RQaJ3yClBTPPahDcmRYp5Q%3D%3D.YlxKfuI8751UCeBWW1yryzhY%2BKXk%2FomTKb31Asz0wUq3DUWI9MeyaC5DlpBjNROgKT4nESetcPN9zdDUzUSjvGROCopvktWdK2Wr65iD5bYvq5%2F%2BXNwxhIWICBIgjShbtYhOGY%2FDVVJgYYCGoE1BzscCliTJMIPGg8UOzDihhahanc3U%2BuvMRU30q3EOcBmLy7q0mFud9be%2FILv23vav4PCNOn03EThAv8X4Dukl8zaOYLdYUODHnWeAhWicgxdJ" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/process-hotspot-...</a></p><p>【8】操作系统控制台热点对比分析：<a href="https://link.segmentfault.com/?enc=jVo9LBH%2B7SGFyzCxUrCrxQ%3D%3D.kHe5dKr7BEl49I67Kn8FTGQBwXV5FeOcXHKmQ%2Fv58JvQRC1ofe7CxeIE1%2ByFZeVPjkR%2BrWCOPKJXP%2FFnXwWcIgkSL%2F4MD5VNmtvt6FUkVg3Bnv8iI8RT1Xc%2BqE0oXPYVBdeLtIF8acRs136vz7%2FgT0LUbRpxQ1aA6%2FsbH%2FhpBVcuyH1Z9EPzZNsxhsSLEkEERxAy4zt35Ndi0lDp2Mgam%2FnNFPn%2Ffp8ITViCQqzXNlW1CD%2F0y8n%2FkxlI7N2tx%2BZT" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/hot-spot-compara...</a></p>]]></description></item><item>    <title><![CDATA[告别「耗时无果」：迭代构建AI知识库，帮]]></title>    <link>https://segmentfault.com/a/1190000047437078</link>    <guid>https://segmentfault.com/a/1190000047437078</guid>    <pubDate>2025-11-28 19:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437080" alt="图片" title="图片"/><br/>作者：王传阳<br/>枫清科技（Fabarta）技术合伙人</p><h3>现状与挑战</h3><p>企业在构建AI应用时，通常会由业务部门负责构建相关的业务知识库。业务部门在构建AI知识库时，普遍面临两大核心挑战：其一，对AI知识库与传统知识库的本质差异认知不足，缺乏适配AI语义理解的知识梳理方法，导致知识应用准确率难以达标；其二，存在 “一劳永逸” 的认知误区，倾向于耗时数月构建覆盖全场景的 “大而全” 知识库，既造成资源浪费，又因需求变化导致知识失效，无法快速响应业务需求。</p><h3>企业AI应用的知识库构建核心理念 - 迭代</h3><p><strong>一、迭代理念的核心内涵</strong></p><p>AI 应用语境下的迭代，是指打破 “一次性构建” 的传统思维，通过场景拆分 - 小库验证 - 逐步扩展的路径，实现知识库的动态优化。具体而言，先聚焦单一细分场景，构建最小化可用知识库与配套测试集，在验证准确率达标后，再逐步扩展场景边界、补充知识内容、升级测试标准，最终形成覆盖完整需求的知识体系。这种模式下，场景迭代与知识库迭代深度绑定，知识库的每一次优化都直接服务于应用效果提升，而应用反馈又反向驱动知识完善。</p><p><strong>二、迭代的起点：场景与知识的精准拆分</strong></p><p>迭代的关键起点是“化整为零”：针对计划构建的完整应用，按业务流程、用户需求或知识领域完成场景拆分，同步将全域知识拆解为相互独立的知识模块，选取其中一个高频、低复杂度的场景作为切入点，形成“小应用 + 小知识库” 的初始组合。同时，需围绕该小应用的核心功能，构建包含高频问题、边缘案例、歧义场景的效果测试集，明确准确率衡量标准。</p><p><strong>三、知识库迭代的实施路径</strong></p><p><strong>1.首轮迭代：最小知识库的打磨</strong><br/>业务部门围绕初始场景，收集整理相关知识文档（政策文件、操作手册、FAQ 等）并上传至平台。这一阶段的核心是 “精准适配”：通过多轮优化提升测试集准确率，具体包括：</p><ul><li>内容完善：补充缺失的关键信息，删除冗余无效内容；</li><li>结构优化：按“核心概念 - 操作流程 - 常见问题” 分层组织文档，优化目录层级；</li><li>格式适配：统一文档格式（优先 word/pdf 等可编辑格式），处理图片、表格等多模态内容；</li><li>实践积累：记录每轮优化的有效方法，形成专属最佳实践库。</li></ul><p><strong>2.迭代扩展：从单一场景到全域覆盖</strong><br/>当首轮迭代的准确率达标后，小应用即可上线产生业务价值。随后启动扩展迭代：</p><ul><li>功能扩展：新增关联场景功能（如从“信用卡申请查询” 扩展至 “信用卡还款咨询”）；</li><li>知识扩容：纳入新增场景的相关知识，必要时联动更多业务部门；</li><li>实践复用：将首轮积累的最佳实践直接应用于新场景，降低跨部门协作成本；</li><li>测试升级：扩展测试集覆盖新场景的业务逻辑，保持准确率标准一致性。</li></ul><p><strong>3.全域收敛：形成完整知识体系</strong><br/>经过多轮场景扩展与知识补充，逐步整合所有细分模块，通过统一的知识管理规范（如术语统一、结构对齐）实现全域知识的协同，最终完成完整应用的构建。</p><p><strong>四、迭代式构建的核心优势</strong></p><p>1.降低准入门槛：无需业务部门一次性掌握所有AI知识梳理技巧，在小场景实践中快速积累经验；<br/>2.快速产生价值：小应用可在短期内上线，避免“长期投入无产出” 的困境；<br/>3.提升构建效率：最佳实践的复用，减少跨部门协作中的重复试错，据行业案例验证，可降低 30% 以上的知识整理成本；<br/>4.保障效果稳定：每轮迭代均以测试集准确率为核心目标，避免全域上线后出现大规模准确率问题；<br/>5.适配需求变化：迭代过程中可灵活调整知识内容，应对业务政策、用户需求的动态变化。</p><p><strong>五、金融行业迭代构建案例分享</strong></p><p>某金融企业需构建覆盖“信贷、理财、支付、保险、财富管理”5 大领域的智能问答应用，涉及 5 个业务部门，迭代路径如下：</p><p>1.首轮迭代：聚焦“个人信贷” 单一领域选取高频场景“个人住房贷款申请咨询” 作为起点，由信贷部门独立负责知识库构建：</p><ul><li>知识范围：房贷申请条件、所需材料、审批流程、利率计算 4 类内容；</li><li>最佳实践积累：<br/>格式规范：优先上传可编辑 PDF/word 文档，避免扫描件（OCR 识别误差率降低 40%）；<br/>多模态处理：贷款流程图前后需添加文字说明（如“下图为房贷审批全流程，其中面签环节需携带以下材料：……”）；<br/>业务描述：利率政策需明确“计息周期 + 适用人群 + 调整规则”（如 “首套房年利率 3.6%（按年计息），适用于无逾期记录的刚需购房者，每年 1 月 1 日调整”）；<br/>分段优化：操作流程类文档按 1024 字符分段，核心概念类按 512 字符分段，重叠字符数设置为 100；<br/>提示词配置：添加“严格依据文档内容回答，无相关信息时回复‘暂无对应政策说明’” 的系统指令。</li></ul><p>经过 3 轮优化，测试集准确率从 65% 提升至 92%，“房贷申请咨询” 子应用上线。</p><p>2.二次迭代：扩展至 3 个业务领域新增“理财”“支付” 两大领域，由理财部、支付部加入协作：</p><ul><li>复用实践：直接套用信贷部门积累的格式规范、分段规则、多模态处理方法，理财部快速完成基金产品说明书的结构化处理，支付部顺利整理跨行转账流程文档，未出现“图片无法识别”“分段混乱” 等初期问题；</li><li>协同优化：统一三大领域的术语规范（如“‘年化收益率’统一表述为‘七日年化收益率’”），补充跨领域关联知识（如 “理财赎回资金的支付到账时效”）；</li><li>价值延续：原“房贷咨询” 子应用持续稳定运行，新增的 “理财产品查询”“转账问题咨询” 子应用经 2 轮迭代后准确率达标上线。</li></ul><p>3.三轮迭代：完成 5 大领域全覆盖纳入“保险”“财富管理” 领域，5 个业务部门协同推进：</p><ul><li>实践升级：基于前两轮经验，制定《金融知识 AI 适配统一规范》，明确保险条款需按 “保障范围 - 免责条款 - 理赔流程” 分层，财富管理方案需标注 “风险等级 + 适配人群”；</li><li>知识融合：构建跨领域知识关联（如“保险理赔资金的理财建议”“信贷客户的财富管理方案推荐”）；</li></ul><h3>总结</h3><p>回顾企业业务部门构建 AI 知识库的初始困境：对 “AI 与传统知识库差异” 的认知盲区，让知识梳理陷入 “无方法、低准确率” 僵局；“一步到位建大库” 的误区，导致资源空耗与需求错配。而迭代式构建理念恰恰提供破局路径 —— 通过 “小场景起步 + 实践积累”，业务部门无需初期掌握复杂 AI 技巧，在打磨小知识库中摸清语义理解规律，破解准确率难题；通过 “分阶段扩展 + 快速上线”，避免全场景一次性构建浪费，让小应用快速产生价值，灵活应对需求变化。</p><p>枫清科技企业知识中台产品的功能体系，与迭代式构建的全周期精准适配，为业务部门提供全流程支撑：</p><ul><li>在场景与知识拆分阶段，针对“化整为零” 需求，通过无代码智能体应用工具快速创建小应用，搭配标签管理按业务域 / 场景划分知识模块，同步用问答集管理构建测试集。</li><li>在首轮知识库打磨阶段，围绕多模态适配与准确率验证，提供文档管理（支持 30 + 格式解析，含图文增强处理）、高级切片设置（自定义字符大小 / 重叠度）与在线调试功能。</li><li>在迭代扩展阶段，针对跨部门协作与新场景接入，通过组织管理划分协作单元、知识库关联功能快速对接新领域知识，搭配权限标签实现“理财部仅见基金知识” 等安全隔离需求。</li><li>在全域收敛阶段，依托统一语义层整合多模态知识、词表管理统一跨域术语，结合知识运营监控全域效果。</li></ul><p>枫清科技深度依托多行业客户交付经验持续打磨知识中台产品，通过客户实践反哺产品优化，优化后产品再赋能更多客户，实现产品与客户的共同成长。</p>]]></description></item><item>    <title><![CDATA[全球首个语音 AI 广告平台问世；Sam]]></title>    <link>https://segmentfault.com/a/1190000047437082</link>    <guid>https://segmentfault.com/a/1190000047437082</guid>    <pubDate>2025-11-28 19:02:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437084" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@鲍勃 和 Gemini（尽量不生产 AI Slop）</em></p><h2>01 有话题的技术</h2><p><strong>1、Vision Agents + Gemini + Ultralytics YOLO 构建 AI 语音瑜伽教练</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437085" alt="" title="" loading="lazy"/></p><p><strong>Vision Agents</strong> 发布了一个教程，将 LLM、实时视频分析和语音转语音 API 相结合，创建一个名为「AI 瑜伽教练」的交互式 Python 应用。该教练可以通过用户的摄像头分析瑜伽姿势，并提供实时的语音指导和反馈，旨在革新居家和健身房的锻炼体验。</p><ul><li><strong>实时姿势分析与反馈：</strong> 利用 Ultralytics YOLO 模型实时检测用户姿势，并通过 Gemini LLM 进行分析，提供即时语音反馈以纠正错误和改进动作。</li><li><strong>全栈语音交互：</strong> 集成 Gemini Live API，实现低延迟的语音转语音交互，用户可以通过语音与 AI 教练进行自然对话。</li><li><strong>易于集成的框架：</strong> Vision Agents 提供了一个开源框架，简化了语音和视频 AI 应用的开发，支持多种第三方 AI 模型和服务集成。</li><li><strong>多场景应用潜力：</strong> 该模式不仅限于瑜伽教练，还可以扩展到体育指导、物理治疗、无人机监控等需要实时视频分析和语音交互的领域。</li><li><strong>快速原型开发：</strong> 通过预置的插件和简单的 Python 配置，开发者可以快速构建和部署 AI 语音助手。</li></ul><p>教程：</p><p><a href="https://link.segmentfault.com/?enc=4XZjYJfUITPj%2Fm1GDCKDHg%3D%3D.XlxX8OSqqXukjVK6scFH1LAz44lmqqq2WgcsGn7Hux7Lp70XpmELwPkg6h2fXLhfpvlDR3UsuAhvUm3vt%2FgrSg%3D%3D" rel="nofollow" target="_blank">https://getstream.io/blog/ai-voice-yoga-instructor/</a></p><p>(@Vision Agents Blog)</p><h6><strong>2、巨人与清华、西工大发布 「视频 X 音乐」 多模态生成新进展</strong></h6><p>多模态生成技术在图像、视频、语音等方向的快速突破，使 「视频 × 音乐」 的多模态生成变成新的研究热点。然而在真实业务场景中，仍然存在诸多未被充分解决的技术空白，例如：</p><ul><li>在音乐驱动的视频生成中，仍缺乏对长时序一致性、音画节奏对齐与镜头运动的系统建模；</li><li>歌声转换（SVC）方面，在大量真实歌曲输入下仍面临音色稳定性不足、和声干扰导致破音等业界难题；</li><li>歌声合成（SVS）场景，缺乏能够在零样本条件下稳健适配不同歌词长度与旋律结构的模型。</li></ul><p>在此背景下，巨人网络 AI Lab 继 2024 年发布 YingGame 有声游戏生成模型之后，继续在多模态领域发力，本次联合清华大学与西北工业大学推出三项研究成果：<strong>YingVideo-MV、YingMusic-SVC 与 YingMusic-Singer</strong>，分别面向音乐驱动的视频生成、歌声转换与歌声合成任务，完善了真实业务场景中多项关键能力链路，为 「视频 × 音乐」 的多模态生成方向带来了系统性的技术进展。</p><p>详细介绍：</p><p><a href="https://link.segmentfault.com/?enc=j9MEx9WwgtjDwNInMKptYA%3D%3D.yje4F7uwKu3D%2BI2ngt8V3Ts1EWzuEz6axqhWQ%2FVDYLGlWDqZp5C1tFpo5XzUyqBXKOJNUrkTA%2BNWZJnIHmYZTA%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/r8de9g9tGFbgk466i8-2Gg</a></p><p>（@巨人网络 AI Lab）</p><p><strong>3、All Voice AI 联手 Factory Berlin 推出全球首个语音 AI 广告平台，将 AI 电话变为收入渠道</strong></p><p>U.S. 公司「All Voice AI」与欧洲创投机构「Factory Berlin」合作，推出了全球首个能在实时 AI 语音通话中嵌入广告的平台。这项技术旨在将传统的客户支持电话转变为可直接变现的收入渠道，通过在对话中适时推送相关优惠，为品牌开辟了全新的互动广告模式。</p><ul><li>实时对话内广告：该平台能在用户与 AI 进行语音通话时，根据上下文实时插入相关的促销或优惠信息。所有广告均为用户选择性加入 （opt-in），旨在确保良好的用户体验。</li><li>将成本中心转为收入中心：传统上，语音通话多为客户支持等成本部门。该平台通过广告变现，帮助企业将这一渠道转变为新的收入来源，目前已支持 57 种语言。</li><li>高接受度与市场潜力：「All Voice AI」的调查显示，高达 97% 的受访者表示，如果广告内容与他们的通话目的相关，他们愿意在电话中接收此类优惠信息，这预示着巨大的市场潜力。</li><li>技术生态与专利保护：该平台技术与「OpenAI」、「Twilio」和「AWS」等巨头合作。其核心创新已申请专利保护，并提供技术授权给商业合作伙伴。</li></ul><p>(@PR Newswire)</p><hr/><h2>02 有亮点的产品</h2><p><strong>1、Sam Altman 和 Jony Ive 透露合作硬件：「如湖畔山间小屋般平静」</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437086" alt="" title="" loading="lazy"/></p><p>品玩 11 月 27 日讯，据 TechCrunch 报道，OpenAI CEO Sam Altman 与苹果前首席设计师 Jony Ive 近日在旧金山 Emerson Collective 活动上透露，双方合作的 AI 硬件设备已进入原型阶段，预计两年内面世。</p><p>该设备被描述为「无屏幕、口袋大小」，强调极致简约与宁静体验。Altman 称其愿景是打造一款如「湖畔山间小屋般平静」的产品，能长期理解用户情境、主动过滤干扰，并赢得用户信任。Ive 表示，理想设计应「看似天真简单」，却内含高度智能，让人无负担地自然使用。</p><p>目前 OpenAI 尚未公布具体技术细节。</p><p>（@品玩）</p><p><strong>2、阿里发布 AI 眼镜夸克 S1，双目 AR 光波导+AI 拍摄</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437087" alt="" title="" loading="lazy"/></p><p>继小米、百度之后，国内第三家科技互联网巨头发布 AI 眼镜。</p><p>11 月 27 日，阿里夸克在北京举办「先见之明」新品发布会，正式发布了「夸克 AI 眼镜 S1」。作为阿里 AI 战略中的关键落子，夸克 S1 在技术路径的选择上并未采用保守的 ODM 通用方案，而是展现出了极强的「创新欲」：搭载「千问」对话助手、采用双芯片架构、双目 AR 光波导与 AI 拍摄结合的高集成度方案。</p><p>这种从底层技术逻辑出发的产品定义，让夸克 AI 眼镜 S1 与此前小米、百度等厂商推出的以「拍摄」为主的 AI 眼镜形成了显著差异。它不局限于单一的影像捕捉，而是基于「近眼显示」能力，将阿里庞大的服务生态通过 AI 多模态形式延展至眼镜端。更为难得的是，在堆叠如此复杂硬件的同时，整机重量依然被控制在了 51g（含 0 度近视镜片）。</p><p>(@VR陀螺)</p><p><strong>3、Gloo 收购 XRI Global，开发全球数千种语言 AI 模型</strong></p><p>Gloo 公司，一家技术平台，近日宣布收购 AI 公司 XRI Global。此次战略性举动旨在将其平台 Gloo AI 和 Gloo360 嵌入 XRI Global 先进的多语言和语音 AI 能力，从而大幅拓展 Gloo 的市场潜力，并赋能全球数千种语言的用户。</p><ul><li>战略性技术整合： Gloo 收购 XRI Global，旨在整合其在多语言和语音 AI 领域的尖端技术，特别是针对「低资源语言」的创新。</li><li>弥补 AI 语言鸿沟： XRI Global 专注于开发能够覆盖全球数千种语言（包括缺乏训练数据或书写系统的语言）的 AI 模型，旨在解决目前全球约 6,800 种语言未被现代 AI 创新覆盖的问题。</li><li>独特方法论与成果： XRI Global 拥有一套经过研究验证的方法论，已在过去 18 个月内为 30 多种语言构建了 AI 模型。其团队由来自 Meta 和 Google 的机器学习、计算语言学博士组成。</li></ul><p>(@Gloo)</p><p><strong>4、迪士尼下场做机器人，把《冰雪奇缘》雪宝（Olaf）从电影里硬生生地「拽」了出来！</strong></p><p>（@香港迪士尼乐园度假区、@机器人前瞻）</p><h2>03 有态度的观点</h2><p><strong>1、「AI slop」（AI 劣质内容）当选澳洲 2025 年度词汇</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437088" alt="" title="" loading="lazy"/></p><p>据澳洲九号台新闻 11 月 25 日报道，《Macquarie Dictionary》宣布「AI slop」成为 2025 年度词汇，用以描述由生成式人工智能大量生产的、缺乏意义且充满错误的低质量内容。</p><p>该词语的入选，反映了公众对人工智能技术滥用现象的关注日益增强。</p><p>每年，《Macquarie Dictionary》都会组织特别委员会评选年度词汇。今年的评审成员包括词典编辑团队、广播主持人兼作家 David Astle，以及语言研究专家 Tiger Webb。</p><p>委员会指出，2025 年「我们已理解『slop』的含义——即无意义、无用途的 AI 产物」，并进一步提出：「那些摄取并传播这类内容的人，是否也将被称作『AI sloppers』？」</p><p>除「AI slop」外，今年的荣誉提名还包括「clanker」，用于贬义地称呼取代人类完成任务的 AI 机器人，以及「medical misogyny」，意指医疗和知识体系中，特别是在女性生殖健康领域存在的性别偏见。</p><p>这些新词汇反映出科技发展、社会结构与公共意识在过去一年中的显著变化。</p><p>（@澳洲九号台）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437089" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437090" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=MVb5pC5GCzFkmvZ1lSwS9A%3D%3D.pluk4ChLbsVZWWJ3NLAG2Pa4VIO06Czlja1RzL7%2FISc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437091" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[重磅揭晓！「2025龙蜥社区年度优秀贡献]]></title>    <link>https://segmentfault.com/a/1190000047437117</link>    <guid>https://segmentfault.com/a/1190000047437117</guid>    <pubDate>2025-11-28 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437119" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[promise 的三个方法 freema]]></title>    <link>https://segmentfault.com/a/1190000047436814</link>    <guid>https://segmentfault.com/a/1190000047436814</guid>    <pubDate>2025-11-28 18:13:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>promise.race\promise.all\promise.allSettled <br/>需要所有操作都成功才能继续时，用 Promise.all。<br/>只关心最快出结果的那个操作（比如设置超时），用 Promise.race。<br/>需要知道每个操作的最终状态（无论成功失败），比如批量处理任务后生成报告，用 Promise.allSettled。</p><pre><code>let params1 = {
        dataType: 19, //高频
        deviceId: this.current.vin,
      };
      let params2 = {
        dataType: 20, //低频
        deviceId: this.current.vin,
      };
      let params3 = {
        dataType: 120, //周期数据
        deviceId: this.current.vin,
      };
      let params4 = {
        dataType: 122, //事件数据
        deviceId: this.current.vin,
      };
      let p1 = this.findAllUploadTradReq(params1, "highFreData");
      let p2 = this.findAllUploadTradReq(params2, "lowFreData");
      let p3 = this.findAllUploadTradReq(params3, "cycleData");
      let p4 = this.findAllUploadTradReq(params4, "eventData");
      Promise.all([p1, p2, p3, p4])
        .then((res) =&gt; {
          let [highFreData, lowFreData, cycleData, eventData] = res;
          // console.log(highFreData,lowFreData,cycleData,eventData);
          if (highFreData || lowFreData || cycleData || eventData) {
            this.tableList[0].lowFreData = "已上传"; // 燃油车检测成功
            if (
              this.current.bleVersion == "2.0" &amp;&amp;
              this.current.bleThreeValueStatus === "0"
            ) {
              this.checkingText = "蓝牙检测中...";
              // this.disconnect();
              this.connect();
              return;
            }
            this.unqualified(2);
            return;
          } else {
            this.tableList[0].highFreData = "未上传";
            this.tableList[0].lowFreData = "未上传";
            this.tableList[0].cycleData = "未上传";
            this.tableList[0].eventData = "未上传";
            let temp = JSON.parse(JSON.stringify(this.tableList[0]));
            this.$set(this.tableList, 0, temp);
            this.unqualified(1);
            this.prolineErrMsg = "燃油车数据检测未通过！";
          }
        })
        .catch((err) =&gt; {
          this.unqualified(1);
          console.log(err);
        });
    },</code></pre>]]></description></item><item>    <title><![CDATA[MES 系统到底管什么？11大核心模块、]]></title>    <link>https://segmentfault.com/a/1190000047436817</link>    <guid>https://segmentfault.com/a/1190000047436817</guid>    <pubDate>2025-11-28 18:12:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>很多朋友都是从《什么是MES，MES系统主要包括哪些功能？》和《为什么MES难以标准化？》这几篇内容关注的我。那么今天我就再来发挥余热，和大家好好聊聊本人对于MES的全新观点。希望能帮助大家更透彻的理解MES。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436819" alt="image.png" title="image.png"/></p><p>在全民工业智能化时代，AI人工智能的兴起，让每个公司都热衷于用不同的方式推进机器换人与智能生产的落地。而在这一进程中，制造执行系统（MES）起到很大的作用。</p><p>谈及MES必须先谈生产（即“生产体系模型”，如下图所示），生产体系模型设计到人、财、物、信息等资源，产、供、销等环节，以及供应商、客户与合作伙伴等多个维度。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436820" alt="image.png" title="image.png" loading="lazy"/></p><p>其中，生产管理是通过对生产系统的战略计划、组织、指挥、实施、协调、控制等活动，实现系统的物质变换、产品生产、价值提升的过程。企业要善于运用“计划—执行—检查—处理”（P-D-C-A）的循环机制，持续推进生产运营战略的设计、实施、控制与优化，从而准确分析问题、追溯根源、制定对策并实现生产过程的持续改善。</p><p>作为企业价值链中的主要环节，生产管理一直是构成企业核心竞争力的关键内容。而MES作为企业资源计划（ERP）系统与底层控制系统之间的桥梁，整合了各类工厂管理系统，是提升企业制造能力与生产管理水平的重要工具。其功能覆盖生产计划与调度、物料平衡与物流管理、库存控制、工艺技术管理、过程监控、质量管理、健康安全环境（HSE）、设备管理、能源管理、成本控制以及绩效管理等多个方面。</p><h2>一、什么是MES？为何要实施MES？</h2><p>核心定义：</p><p>MES（Manufacturing Execution System）即制造企业生产过程执行系统，是一套面向制造企业车间执行层的生产信息化管理系统。MES可以为企业提供包括制造数据管理、计划排产管理、生产调度管理、库存管理、质量管理、人力资源管理、工作中心/设备管理、工具工装管理、采购管理、成本管理、项目看板管理、生产过程控制、底层数据集成分析、上层数据集成分解等管理模块，为企业打造一个扎实、可靠、全面、可行的制造协同管理平台。</p><p>历史争论：</p><p>由于MES一词相对其他信息系统在中国流行较晚。很多企业决策者开始常提到：为何实施MES？外国也有同论：Why MES？因为许多规划是先实施ERP，后实施MES。因此产生了为ERP实施MES的结论。其实不尽然。</p><p>核心价值：</p><p>MES系统是在过程控制层面以上，ERP系统之下，起到承上启下作用的重要企业信息管理系统。MES的同步数据采集技术应用于企业内部物流的全线追溯、制造工程配置、生产及品质过程控制，它能提升制造环节的透明度，填补了生产现场到计划系统间的“信息鸿沟”，为计划系统的再调整提供可以信赖的决策依据。MES提供了自演化能力的建模特性，采用螺旋式渐进的敏捷实施方法论，主动匹配并优化客户的业务过程，能显著降低系统部署风险，最大限度提升企业执行力，助其实现核心价值，并快速取得成功。</p><p>客观分析：</p><p>1、企业生存环境的客观需要</p><p>因为工厂级、车间级管理面临着新的挑战：需要上下游车间的高效沟通，信息的及时性、准确性，随时面临的计划变动，越来越小的任务单元，越来越高的质量追溯要求，管理人员成本的不断升高等。</p><p>2、企业管理的内在规律</p><p>MES从时间空间角度以及业务链条看，是企业管理中的不可或缺的领域和环节。</p><p>3、是信息技术发展的必然结果</p><p>信息技术在企业中的应用，首先是解决生产过程控制的问题，而且发展迅速。接着就是提升经营管理水平和手段，产生了以ERP为代表的一大批软件。</p><p>但是，在实施ERP的过程中，人们发现：ERP的规模大、周期长，导致ERP项目有46%逾期完成；支出多、投入大，导致41%超过预算；多种原因致使49%没有达到预期的社会效益、经济效益和目标。其中一个重要因素是与生产现场的连接与集成被忽略了，而生产现场的数据，即完美的生产信息是ERP的基础，是集成的关键。</p><p>由于ERP层和DCS层的工作是分别进行的，因此产生了两个问题：</p><ul><li>一是横向系统之间的信息孤岛（Island of Information）；<br/>二是ERP和DCS两层之间形成缺损环或断缺链接（Missing Ring or Link）。</li></ul><p>这也是催生MES的重要原因。</p><p>因此，国际上公认的信息化总体架构是原则上将框架划分为3层：PCS层、MES层、ERP层。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436821" alt="image.png" title="image.png" loading="lazy"/></p><p>PCS层为底层，以硬设备为主，主要面向操作工人，实现生产过程操作运转自动化，减少操作工人编制；</p><p>MES层为中间层，承上启下，以生产运行管理软件为主，主要面向生产管理人员，实现生产管理信息化，以及管理组织的扁平化和紧密化；</p><p>ERP层为最高层，以经营管理软件为主，主要面向经营管理和决策人员，实现经营决策管理信息化以及管理组织的扁平化和集约化。</p><p>近几年从最上层分离出决策层，演变成4层结构。突出实时制造性能监控、实时运营智能等管理理念。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436822" alt="image.png" title="image.png" loading="lazy"/></p><h2>二、MES与ERP的区别</h2><p>MES和ERP各有作用，但在制造企业运营中，两个系统又互有补足，可以互相整合。二者的主要区别有：</p><p>1、管理目标不同</p><p>ERP的重点在于销售、订单、供应链、财务，也就是从业财一体化的角度出发，来对企业的资源进行计划，相关的模块也是以人员、业务、财务为核心展开，最终的管理数据是集中到经营报表上。</p><p>MES的重点在于制造，也就是以产品质量、准时交货、设备利用、流程控制等作为管理的目标。因为不同的企业管理重点不同，在选择信息系统的组成时，重点也不同。集团公司、商业企业、物流企业等更着重于ERP管理，而制造企业更需要MES管理。</p><p>2、管理范围不同</p><p>ERP的管理范围较大，涉及采购、财务、销售、订单管理、发运管理、成品仓储计划控制等计划层面，主要对企业资源进行有效共享与利用，使企业资源在购、存、产、销、人、财、物等各个方面能够得到合理地配置与利用，但是不够详细具体。</p><p>MES管理比ERP细致，主要涉及车间的工单派发、制程防错、产品谱系、SPC质量分析、设备数据分析、制程追溯等执行层面，能更细致到每个制造工序，对每个工序进行任务的下达、执行的控制和数据采集、现场调度。</p><p>3、管理功能不同</p><p>除了财务管理、人力资源管理、客户关系管理等功能，ERP在制造管理方面的功能主要是编制生产计划，收集生产数据。</p><p>而MES除了细化生产计划和收集生产数据外，还有批次级的生产控制和调度的管理功能，例如：批次级的工艺流程变更，对制造设备、人员和物料的验证控制，批次分拆、合并，批次的生产订单变更等现场调度功能。</p><p>4、实现方式不同</p><p>ERP主要采用填写表单和表单抛转的方式实现管理，现场收到的制造任务是通过表单传达，现场制造数据也是通过填写表单完成收集。</p><p>MES是采用事件的方式实现管理，监测生产订单的变化和现场的制造情况，通过MES内置的WIP（在制品）引擎立刻触发相关事件，要求相关人员或设备采取相应的行动。因此，MES可以减少数据的输入工作，通过信息系统实现工作现场的指令下达和数据收集，减少差错，提高及时性。</p><p>5、时间周期不同</p><p>正是因为MES采取了WIP（在制品）引擎来驱动管理，能够做到现场的“实时管理”：上级生产计划和生产调度能立刻反映在制造现场的作业界面，现场的生产数据和异常情况也能实时反映在管理岗位的监督界面，在企业上下层之间提供一个双向的生产信息流，使得及时调度成为可能。</p><p>ERP的表单方式则不可避免会存在录入的周期，管理的数据以周、天为时间周期，无法对现场执行进行实时的有效管控，即所谓的在制造过程中存在“信息黑洞”，这个“信息黑洞”对制造过程的管理和控制的影响愈发不利。</p><p>6、技术要求不同</p><p>ERP主要处理计划数据，数据量小，不需要和底层硬件交互，易于采用集中的方式管理，在实施时，计划的流程相对固定。</p><p>MES的数据粒度小，数据量大，和工厂的工艺、车间管理流程、自动化程度密切相关，不同企业实施时差异很大，需要不断适应车间管理模式的变革，此外，MES系统直接记录生产的过程数据，在系统的可靠性和稳定性方面比ERP要求更高。</p><h2>三、MES的核心功能</h2><p>MES主要管理4种资源，包括生产活动中的人力资源（Personnel Resources）、生产设备（Equipment）、物料和能源（Material and Energy）以及工艺过程链（Process Segments）；在企业经营计划层面与生产过程控制层面之间，实现生产能力信息的交换、产品信息的交换、生产调度信息的交换、生产绩效信息的交换（4P交换功能）。</p><p>AMR组织定义的MES有11个功能：</p><p>（1）生产资源分配与监控；</p><p>（2）作业计划和排产；</p><p>（3）工艺规格标准管理；</p><p>（4）数据采集（装置在线连接采集实时数据和各种参数信息，控制系统接口，生成生产数据记录、质量数据、绩效信息、台账累计）；</p><p>（5）作业员工管理；</p><p>（6）产品质量管理；</p><p>（7）过程管理（过程控制、APC、基于模型的分析与模拟、与外部解析系统接口）；</p><p>（8）设备维护；</p><p>（9）绩效分析；</p><p>（10）生产单元调度；</p><p>（11）产品跟踪。</p><p>AMR组织则又把按着11个功能实现的整体解决方案称为MESⅡ（Manufacturing Execution Solution）。其中生产资源计划、排产与调度是主线。如图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436823" alt="image.png" title="image.png" loading="lazy"/></p><p>但随着近些年的发展与不断演练，业界针对大多数制造企业的实际需求，又重新梳理出一个全新的MES功能，具体如：</p><p>1、生产流程监控模块</p><p>解决问题：漏工序、生产进度难以管控、生产瓶颈问题难以发现。</p><p>通过MES系统对产品的加工流程进行实时监控，可杜绝漏工序问题，并可实时了解生产进度，发现生产瓶颈问题，保证产品质量，确保如期交货。</p><p>2、人员绩效管理模块</p><p>解决问题：人员有效工作时间、人员每日产量、每小时产能、产出良率等</p><p>MES系统人员绩效管理模块较为典型的应用为人员上岗离岗（非上班下班）刷卡记时，统计人员有效工作时间。人员在各工位通过登录系统并记录产出数量（可依据实际情况选择人工录入或自动获取），并由系统自动统计出每日总产量、产出良率等。从而实现为绩效考核和生产计划制定提供数据基础的目的。</p><p>3、智能排产模块</p><p>解决问题：资源约束，均衡生产，快速反应。优化排产，降低库存，减少成本。</p><p>智能排产模块于ERP与MES的应用基础，使得计划与生产一线信息得以实时反馈，从而解决了传统排产软件人工干预比较多的问题，使计划更科学，更合理，更准确，更方便。对接受的订单进行交期承诺，有限产能约束优化生产计划，物料计划及机组作业安排，并将结果下达到MES。同时MES又将生产线发生的实时状态反馈给计划，进行动态调整。</p><p>4、SPC统计过程控制模块</p><p>解决问题：品质预警，过程控制，品质分析。</p><p>SPC应用在现代制造中越来越广泛，随着生产力的进一步发展，大规模生产的形成，如何控制大批量产品质量成为一个突出问题，单纯依靠事后检验的质量控制方法已不能适应当时经济发展的要求，必须改进质量管理方式。而SPC统计过程控制软件则是其中的一个核心工具。SPC管理模块将SPC应用引擎内置于MES系统中，对质量管理提供一个质的飞越。</p><p>5、看板报表管理模块</p><p>解决问题：实时掌控生产及物流信息、作业指导书集中管理实时更新、增强展示效果提升企业形象等。</p><p>电子看板是用于显示当前产线生产进度、良品率等信息，以直观的形式表现在员工面前，便于员工能及时调整或改善生产环境，利于生产水平的提高；一般用TV或LED屏来作显示设备，也可利用PC来打开系统中的电子看板。</p><p>MES系统看板报表管理模块较为典型的应用包括产线看板、仓库备料看板、会客厅展示看板、工位看板及各类型的统计分板报表。其中工位看板支持DOC、XLS、PDF等多种格式文档，服务器对文档进行集中管理和设置，可展示指定文档的某一页，也可以多页轮播等。</p><p>6、智能仓储模块</p><p>解决问题：物料先进先出，收发物料效率低下，新员工收发物料特别容易出错。</p><p>作为一套完整MES系统的起点，物料收发工作所起的作用十分关键，MES系统可以在来料入库阶段通过对接点料机、打印机、扫描枪或PDA等外设，实现快速收料入库和打印条码标签。同时，系统支持与智能货架进行对接，实现快速的存放和寻找物料，提高收发物料的效率和准确度，大幅降低出错风险。</p><p>7、条码管理模块</p><p>解决问题：条码重复、条码使用混乱、标签格式多、打印机品牌型号多等。</p><p>通过在系统中统一生成和领用标签，使标签得到清晰有序的管理，从而杜绝因条码问题导致的返工。MES系统具有强大而灵活的条码规则设置功能，用户可以通过JavaScript自定义条码规则。标签的格式亦可通过所见即所得的方式自定义，支持一维码和二维码。</p><p>8、上料防错与追溯模块</p><p>解决问题：上料错误，物料追溯困难、用错PCB、用错锡膏、用错钢网等。</p><p>上料防错：通过在系统中导入站位表，上料时扫描料盘条码与站位表进行对比实现防错目的。</p><p>物料追溯：通过投板扫描及炉后扫描、分板扫描，实现追溯到产品使用了哪些物料、物料用在了哪些产品上。</p><p>9、销售管理模块</p><p>解决问题：准确快速发货，售后问题，异常处理，如何计划，如何改正。</p><p>直接按订单分拣货物，打印发货地址及配送，防止发货出错。</p><p>10、品质/质量管理模块</p><p>解决问题：如何控制（QC）预防出错，异常处理。如何计划，如何改正。</p><p>MES质量管理模块，实现了制造业务和质量管控过程的自然融合，确保了质量活动与制造过程的完美交互，制造过程中所有静态和动态的数据在系统中，随着制造业务的开展，自然而然的从各个环节被自动采集，形成庞大的制造数据集合，为质量活动的设计、执行、评价和改进提供了丰富的数据基础；系统中的质量管控、质量分析等模块对自动采集得到的海量数据进行筛选、分析与反馈控制，形成数字化为特征的企业车间质量管理体系，能够有效提高质量管理活动的执行效率，并使制造过程的质量反应能力和质量控制能力得到极大的提高。</p><p>11、设备管理、设备通讯模块</p><p>解决问题：设备的使用防错，数据收集，状态监控，保养维护、维修、报废等</p><p>设备管理模块较为典型的应用包括印刷机、SPI、贴片机、AOI、回流炉、烧录机、测试测量治具等设备的定期保养维护，稼动率分析，使用防错，数据采集，使用次数统计，部分工具的定期校验（内校或外校）等等。</p><h2>四、实施MES的关键</h2><p>1、对企业制造执行能力进行分析</p><p>MES是制造执行系统，为生产制造管理服务，旨在提高生产制造执行能力和水平。因此，实施MES前，首要问题是对现有的制造执行能力进行评估。从制造战略、制造质量、供应链协调、信息收集、绩效管理与改进、制造与IT基础设施等角度把企业的制造执行能力成熟程度划分为劣、可、良、优、未来追求目标共5个等级。使用该成熟度模型，根据企业目前状况和环境对企业的要求，对企业进行评价，找出差距，定出目标。</p><p>2、确定好功能模块</p><p>MES系统中最重要的模块——生产运行管理模块的核心，即生产计划、统计与调度。生产运行的基础是设备及其运行，因此设备管理模块和数据采集和存储（实时数据库），也应是MES的基本模块。一般还有产品质量管理模块，成本管理模块，物料平衡与仓储管理模块等。目前，随着管理的集约化和精细化，又有新的需求。产生了新的信息技术和软件，增加了MES的功能。</p><p>3、做好集成</p><p>随着信息技术的进步，企业管理的空间、时间范围在扩大，同时管理的细度或粒度又在缩小。从宏观和微观上都要求可视化和实时化，这就需要集成技术。一是MES各模块的集成，二是MES和PCS层面、ERP层面的集成，三是企业内部与企业外部的集成。目前流行的理念和技术是实时绩效管理、制造绩效服务，如图所示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436824" alt="image.png" title="image.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436825" alt="image.png" title="image.png" loading="lazy"/></p><h2>五、最后的个人观点总结</h2><p>1、当下的信息化，普遍设计为有机相连的3或4层架构。</p><p>2、ERP与MES的界限是模糊的，有些功能有重叠。所以，其模块边界的划分是有学问的。ERP不能代替MES，对流程行业尤其是这样。</p><p>3、以DCS为重点的控制系统，以ERP为代表的经营管理系统，以MES为核心的生产运行指挥调度系统，是企业信息化的3大领域，其中MES的效率和效益最具潜力。</p><p>4、MES重点关注“人财物”的“物”，“产供销”的“产”，以及生产运行的“安、稳、长、满、优”。</p><p>5、实施MES可先从基础的、基本的模块做起，再实施扩充的、增强的、高级的模块。</p><p>6、没有一家的产品能包打天下。MES模块应优中选优，再通过第三方集成平台软件进行综合集成是上策。</p><p>7、除了选取MES应用模块、软件外，选取集成商或主承包商提供整体解决方案和集成最关键的。</p><p>最后建议，企业在引入信息化系统初期，切记要合理有效地运用好工具，这样一来不仅可以让公司业务高效地运行，还能最大程度保证团队目标的达成。同时还能大幅缩短系统开发和部署的时间成本。特别是有特定需求功能需要定制化的企业，可以采用我们公司自研的企业级低代码平台：织信。</p><p>织信平台基于数据模型优先的设计理念，提供大量标准化的组件，内置AI助手、组件设计器、自动化（图形化编程）、脚本、工作流引擎（BPMN2.0）、自定义API、表单设计器、权限、仪表盘等功能，能帮助企业构建高度复杂核心的数字化系统。如ERP、MES、CRM、PLM、SCM、WMS、项目管理、流程管理等多个应用场景，全面助力企业落地国产化/信息化/数字化转型战略目标。</p><p>不管说得再天花乱坠，都不能代替产品本身，好产品，值得大家切身体验。</p>]]></description></item><item>    <title><![CDATA[MatrixOne Intelligen]]></title>    <link>https://segmentfault.com/a/1190000047436839</link>    <guid>https://segmentfault.com/a/1190000047436839</guid>    <pubDate>2025-11-28 18:11:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>MatrixOne Intelligence 4.0 全新升级：让数据智能触手可及！</h2><h3>MatrixOne Intelligence 介绍</h3><p>MatrixOne Intelligence 是一套面向多模态数据的 AI 数据智能平台，旨在帮助企业应对数据碎片化、多模态数据整合复杂、GenAI 应用落地困难等挑战。通过数据接入、智能解析、数据工作流、超融合的湖仓底座，MatrixOne Intelligence 为企业提供了一站式的端到端平台，将企业内部的自有数据变成可以服务于 GenAI 落地应用的 AI-Ready 数据。该平台基于创新的云原生架构和存算分离设计，支持结构化和非结构化数据的统一管理和高效处理，具备高度灵活的部署能力，可适配公有云、私有云及本地数据中心的多种环境。</p><p>MatrixOne Intelligence 致力于帮助企业充分挖掘和释放私域数据的潜能，让企业私域数据在 AI 时代得到充分利用，成为其独特竞争力的关键来源。</p><h3>核心升级亮点，全面助力企业数字化转型</h3><h4>一、工作流智能化升级——解放双手，自动完成复杂任务</h4><p>全新引入的Agent模式，让系统能根据用户意图和数据特征自动选择最优执行路径。无需再手动设计复杂的逻辑流程，平台自动感知业务需求，智能做出决策并执行，打造具备"感知 + 决策 + 行动"能力的智能任务，显著降低操作复杂度和时间成本。</p><p><img width="723" height="525" referrerpolicy="no-referrer" src="/img/bVdncD6" alt="" title=""/></p><h4>二、丰富工作流模板，极速搭建自动化流程</h4><p>针对不同业务场景，新增多款开箱即用的工作流模板，覆盖文本解析、数据提取、内容生成等典型应用。用户无需从零开始配置，即可快速构建自动化流程。</p><p>典型应用示例：</p><ul><li><strong>多模态文档RAG数据准备</strong>：支持图文混合内容的数据生成，助力多模态问答系统。</li><li><strong>人才简历信息提取</strong>：自动抓取简历中的核心字段，实现简历数据结构化管理，助力HR高效筛选。</li><li><strong>法律知识数据生成</strong>：从海量法律文本中提取高质量问答对，助力法律大模型的微调与优化。</li></ul><p><img width="723" height="447" referrerpolicy="no-referrer" src="/img/bVdncEn" alt="" title="" loading="lazy"/></p><h4>三、多模态信息提取更强大</h4><p>全新升级的信息提取节点，内置多款针对财务报告、发票、简历等场景的智能模板。用户无需手动定义复杂schema，只需一键即可获得精准结构化结果。结合最新多模态模型技术，实现图文混合内容的深度理解与抽取，极大提升提取准确率和效率。</p><p><img width="723" height="833" referrerpolicy="no-referrer" src="/img/bVdncEo" alt="" title="" loading="lazy"/></p><h4>四、数据中心结构再进化</h4><p>为更好地管理多模态数据，MOI 推出了三级数据中心结构：目录 → 库 → 卷。这一设计支持更灵活的数据隔离与权限管理，满足不同生命周期和业务场景下的多层级治理需求。</p><ul><li><strong>目录</strong>：数据治理最高层，支持生命周期划分（如生产、开发、敏感数据等）；</li><li><strong>库</strong>：组织结构化与非结构化数据，灵活按业务或阶段分类；</li><li><strong>卷</strong>：非结构化文件的逻辑容器，提供高效存储与访问。</li></ul><p><img width="723" height="204" referrerpolicy="no-referrer" src="/img/bVdncEp" alt="" title="" loading="lazy"/></p><h4>五、复杂文档解析更精准，文件处理更灵活</h4><p>新增支持复杂表格结构的智能识别与解析，丰富数据维度和深度。同时，文件处理粒度更加细化，新增一次处理模式下可按单个文件精细选择与操作，极大提升处理的灵活性和控制力。无论是批量处理还是针对特定文件的精准操作，MatrixOne Intelligence都能满足您的多样化需求，帮助您轻松驾驭复杂多样的数据内容。</p><h4>六、数据集成能力再提升</h4><p>新增支持将处理结果直接导出至 MatrixOne、标准 S3、阿里云 OSS 等平台，帮助企业在多种系统间实现更高效的数据流转与管理。</p><p><img width="723" height="340" referrerpolicy="no-referrer" src="/img/bVdncEq" alt="" title="" loading="lazy"/></p><h4>七、HDFS 连接器支持 Kerberos 认证</h4><p>全新 HDFS 连接器已支持企业级 Kerberos 安全认证，在保障数据安全与合规的同时，兼容简单认证模式，让切换更灵活。</p><p><img width="723" height="781" referrerpolicy="no-referrer" src="/img/bVdncEr" alt="" title="" loading="lazy"/></p><h4>八、用户体验全面焕新</h4><p>为提升用户体验，平台做了如下优化：</p><ul><li><strong>账户体系升级</strong>：支持手机或邮箱注册登录，实现官网与 MOI 账号统一使用，一次登录即可访问多个平台。</li><li><strong>产品工作区分离</strong>：MatrixOne 与 GenAI 工作区独立，产品边界清晰，同时支持无缝切换。</li><li><strong>快速上手模块</strong>：新增快速开始功能，将核心流程和功能整合，帮助用户快速熟悉产品。</li><li><strong>流程辅助优化</strong>：在数据处理及工作流中增加操作辅助与帮助提示，使使用流程更顺畅、直观。</li></ul><p><img width="723" height="424" referrerpolicy="no-referrer" src="/img/bVdncEs" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[告别“数据孤岛”，基金公司如何构建秒级响]]></title>    <link>https://segmentfault.com/a/1190000047436847</link>    <guid>https://segmentfault.com/a/1190000047436847</guid>    <pubDate>2025-11-28 18:11:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在资产管理规模不断攀升与金融科技深度渗透的双重驱动下，基金行业正面临前所未有的数据变革。当客户数量突破亿级、日交易量迈向千万级，数据不再仅仅是后台的记录，而是驱动营销获客、投资决策与风险控制的核心资产。</p><p>然而，面对爆发式增长的数据体量与日益复杂的异构数据源，传统的数仓架构正逐渐显露疲态。如何打破数据孤岛，实现海量数据的极速分析与统一管理，成为基金公司数字化转型的关键命题。</p><p>本文将结合某头部基金公司的实战案例，深入解析基金行业的数据痛点，并分享镜舟科技基于 StarRocks 的现代化数据分析解决方案。</p><h3><strong>一、行业现状：数据架构面临代际挑战</strong></h3><p>当前，基金行业的数据建设正面临从报表时代向智能决策时代的跨越，这一过程中主要面临三大挑战：</p><p><strong>数据体量的爆发：</strong> 随着互联网渠道的爆发，个人投资者数量激增。叠加高频交易、定投扣款、以及埋点日志数据，核心业务表的数据量级已从千万级跃升至百亿级。</p><p><strong>时效性要求严苛：</strong> 无论是基金经理的盘中风控，还是营销部门的盘后复盘，业务方对数据的容忍度已从T+1缩短至T+0甚至秒级。</p><p><strong>异构数据源的融合：</strong> 估值系统、TA系统（登记过户）、CRM系统往往采用不同的数据库技术，数据分散在各类孤岛中，跨源分析极其困难。</p><h3><strong>二、行业痛点：被慢与乱拖累的业务</strong></h3><p><strong>当传统架构跑不动海量数据</strong></p><p>在与多家行业客户的交流中，我们发现，为了解决上述问题，IT 部门往往被迫采用加法，导致架构日益臃肿。</p><p><strong>痛点一：多维分析跑不动，决策滞后</strong></p><p>基金公司的核心数据往往达到数亿甚至数百亿行。在 Hadoop 或传统 MPP 数据库上，涉及多表关联的复杂查询响应时间通常在分钟级。</p><p>例如，业务分析师想要进行“某特定客群在不同市场行情下的资产留存分析”，由于查询涉及海量历史流水与客户标签的关联，系统响应往往超过 5-10 分钟，甚至直接超时失败。</p><p><strong>痛点二：组件繁杂，运维高昂</strong></p><p>为了满足不同场景，企业往往被迫引入多套组件：用 <strong>NoSQL 数据库</strong> 承接高并发点查，用 <strong>Cube 预计算引擎</strong> 加速固定报表，用 <strong>传统 MPP</strong> 处理批量计算。</p><p>这种拼盘式架构运维成本极高，还导致了数据搬运过程中的一致性问题。数据开发工程师不得不花费大量时间在 ETL 链路的维护上，而非业务价值的创造。</p><p><strong>痛点三：灵活性差，响应迟钝</strong></p><p>业务需求瞬息万变，而基于预计算的技术方案灵活性极差。一旦市场部想要增加一个新的分析维度（例如新增“渠道偏好”标签），IT 部门需要重新定义模型并回刷历史数据，开发周期以周为单位，完全跟不上市场热点的变化节奏。</p><h3><strong>三、解决方案：化繁为简，构建极速统一数据湖仓</strong></h3><p>针对基金行业数据量大、关联复杂、时效要求高的特点，镜舟科技提出了极速统一的湖仓分析方案。该方案以 StarRocks 为核心，旨在通过一套引擎解决 90% 以上的分析需求。</p><p><strong>1. 极速查询引擎：秒级响应复杂分析</strong></p><p>利用 StarRocks 独有的全面向量化引擎和 CBO 优化器，在无需宽表打平的情况下，即可实现星型/雪花模型的多表关联极速查询。无论是数千亿行的持仓分析，还是复杂的营销圈人，均可实现秒级响应。</p><p><strong>2. 联邦查询：打破数据孤岛</strong></p><p>无需进行繁重的数据迁移，StarRocks 可通过 External Catalog 直接挂载 Oracle、SQL Server、Hive、MySQL 等外部数据源。业务人员可以通过一个统一的 SQL 入口，查询全域数据。既保护了原有资产，又实现了数据的逻辑统一。</p><p><strong>3. 实时与离线融合：简化数据链路</strong></p><p>支持高并发的实时写入与更新，同时具备强大的离线批处理能力。替代原有多组件的复杂组合，一套系统同时满足实时大屏、即席查询（Ad-hoc）和固定报表需求，大幅降低 TCO。</p><h3><strong>四、最佳实践：某头部基金公司的架构重构</strong></h3><p><strong>【客户背景】</strong></p><p>该客户为创新型基金公司，业务发展迅猛。其数据应用场景涵盖了投研、营销、财务、估值四大核心条线，对数据的准确性与实时性有着极高要求。</p><p><strong>【面临挑战】</strong></p><p>客户原有架构高度依赖某传统老牌数据库。随着业务积累：</p><ul><li><strong>性能瓶颈：</strong> 信用评级系统的查询耗时超过 5 分钟，严重影响风控效率。</li><li><strong>时效滞后：</strong> 核心报表跑批需耗时近 6 小时，大客户定制报表排期严重积压。</li><li><strong>稳定性差：</strong> 在绩效系统计算“累计收益率”等指标时，Java 程序经常因内存消耗过大而崩溃。</li></ul><p><strong>【实施方案】</strong></p><p>客户引入 StarRocks 作为全公司统一的 OLAP 分析引擎，重构了数据流转链路：</p><ul><li><strong>数据同步层：</strong> 业务系统（TP库）数据实时同步至从库。</li><li><p><strong>数据接入层：</strong></p><ul><li><strong>实时链路： </strong>通过 CDC 技术捕获变更数据发送至 Kafka，利用 StarRocks 的 Routine Load 实现秒级写入。</li><li><strong>离线链路：</strong> 利用 DataX 将批量历史数据定期同步至 StarRocks。</li></ul></li><li><strong>数据应用层： </strong>基于 StarRocks 构建 ADS、DWS 和 DWD，统一对外提供查询服务。</li></ul><p><strong>【应用效果：从6小时跑批到5分钟就绪】</strong></p><p><strong>1. 核心查询性能提升 10 倍+</strong></p><p>新架构上线后，关键业务系统的响应速度有了极大提升：</p><p>信评系统重，复杂关联查询从原有的 5 分钟缩短至 秒级，风控人员可实时获取最新评级信息，并且解决了估值线在大跨度时间范围内查询数据的性能卡顿问题。</p><p><strong>2. 报表时效性从小时级至分钟级</strong></p><p>通过 MPP 架构的极速计算能力，替代了原有的慢速跑批，报表延迟骤降，业务报表的整体数据准备时间从 6 小时 压缩至 5 分钟以内，真正实现了T+0的动态经营分析。</p><p><strong>3. 架构统一，成本可控</strong></p><p><strong>技术栈收敛：</strong>一套 StarRocks 引擎同时满足了实时大屏、自助报表、固定跑批等多种需求，避免了引入 Elasticsearch 或 HBase 等额外组件的复杂性。</p><p><strong>计算下推：</strong>将复杂的净值计算逻辑从 Java 层下沉至数据库层，不仅提升了稳定性，更大幅降低了应用服务器的硬件资源消耗。</p><h3><strong>结语：让数据跑在业务前面</strong></h3><p>该基金公司的实践证明，在数据体量庞大、业务逻辑复杂的金融场景下，StarRocks 不仅能解决金融机构查询慢和成本高的问题，更能释放数据的实时价值，赋能投研与营销的每一次决策。</p><p>对于基金行业而言，数字化转型不仅是技术的更迭，更是业务模式的重塑。镜舟科技致力于通过极速统一的数据分析底座，帮助金融机构卸下历史包袱，让数据从成本中心转变为价值中心，在瞬息万变的资本市场中抢占先机。</p>]]></description></item><item>    <title><![CDATA[能级跃迁！数字孪生从可视化迈向智能决策 ]]></title>    <link>https://segmentfault.com/a/1190000047436852</link>    <guid>https://segmentfault.com/a/1190000047436852</guid>    <pubDate>2025-11-28 18:10:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年第三季度，浙江省数字孪生水利平台在防汛防台中发挥关键作用，通过精准推演洪峰演进轨迹，提前72小时预测淹没范围，指导人员转移，使应急响应效率提升50%以上。这一成功实践，体现了数字孪生技术从 “精准映射”到“智能干预” 的能级跃迁。</p><p>数字孪生技术已从简单的三维可视化和状态监测，演进为具备预测预警和自主决策能力的智能系统。其能级跃迁的核心在于突破了静态映射的局限，实现了感知、分析、决策、控制的闭环。<br/><img width="723" height="279" referrerpolicy="no-referrer" src="/img/bVdncEx" alt="" title=""/></p><h2>一、精准映射：从物理实体到数字空间的毫秒级同步</h2><p>精准映射是数字孪生的基础，其目标是在数字空间中构建一个与物理实体高度一致且实时同步的虚拟模型。这不仅包括几何形状的还原，更包含物理属性、行为规则乃至环境交互的全面复现。</p><p>技术实现核心在于空—天—地—水—工一体化感知网络的构建。通过部署数以万计的传感器（如浙江水利系统覆盖数万个点位），实时采集物理实体的状态数据（如水位、温度、振动等）。数据通过时间敏感网络（TSN） 等技术实现毫秒级低延时传输和同步。</p><p>关键的一步是高保真建模与渲染。采用如NVIDIA Omniverse等引擎，进行复杂物理场（如流体动力学、结构力学）的仿真，使得虚拟模型能够精准反映物理实体的动态行为，将虚实空间的位置误差控制在毫米级。<br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdncEw" alt="" title="" loading="lazy"/></p><h2>二、模拟推演：从状态监测到未来预测的跨越</h2><p>在精准映射的基础上，数字孪生能级跃迁的第二步是模拟推演，即利用虚拟模型对物理实体的未来状态进行预测。这依赖于水利、机械、电气等专业模型与数据分析模型的深度融合。</p><p>例如，在水利领域，系统通过求解圣维南方程组等水动力学模型来推演洪峰演进：<br/> ∂Q/∂t + ∂(Q²/A)/∂x + gA(∂h/∂x) + gAS_f = 0<br/> （其中Q为流量，A为过水面积，h为水位，S_f为摩擦坡度）。在智能制造领域，则可能采用深度强化学习等AI算法。系统通过分析海量历史运行数据与实时数据，构建预测模型，其目标函数可表述为最大化长期奖励：V^π(s) = E[∑γ^t R(s_t,a_t)]。这使得系统能够提前预测设备故障或模拟不同生产策略的效果。</p><h2>三、智能干预：从虚拟仿真到实体执行的闭环</h2><p>数字孪生技术的最高能级体现在智能干预，即将虚拟空间中仿真优化后得出的最佳决策，反向作用于物理实体，形成闭环控制。</p><p>决策优化是智能干预的前提。系统通常需要在多重约束下（如成本、能耗、安全边界），寻找最优解。其数学模型可简化为一个多目标优化问题：min[f₁(x), f₂(x), …, f_m(x)]^T。通过蒙特卡洛模拟、粒子群优化等算法，在数字孪生体中评估成千上万种可能方案，并选出最优策略。<br/><img width="594" height="441" referrerpolicy="no-referrer" src="/img/bVdncEz" alt="" title="" loading="lazy"/></p><p>最终，优化的决策指令通过数字线程精准下发至物理世界的执行机构（如调节阀门、启停设备、改变机器人运动轨迹）。这一过程强调指令的精准性与执行的实时性。例如，在智慧水网中，系统可自动生成引水方案并执行调度，有效缓解旱情。</p><p>凡拓数创的实践为我们提供了观察这一跃迁的窗口。通过数字孪生致力于实现从物理实体到虚拟空间的高精度映射与实时交互，在甘泉堡智慧园区中数字孪生技术为精准映射和模拟推演提供了支持。此外，与时俱进布局具身智能，旨在进一步强化数字孪生系统的智能干预能力，通过仿真训练与推演，优化智能体的决策执行水平。<br/><img width="723" height="205" referrerpolicy="no-referrer" src="/img/bVdncED" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[“疫苗临床试验”思维导图创作实践解析 图]]></title>    <link>https://segmentfault.com/a/1190000047436863</link>    <guid>https://segmentfault.com/a/1190000047436863</guid>    <pubDate>2025-11-28 18:09:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="272" referrerpolicy="no-referrer" src="/img/bVdncEO" alt="" title=""/><br/>                “疫苗临床试验”思维导图</p><p><a href="https://link.segmentfault.com/?enc=J3Y%2FpemdjAbQXaOye9QUCg%3D%3D.EZV12otxoRHdv2VuAq78nYjHKL74%2Ba943%2B9ZimO9o97vWvoptZQT1R9jCKpugEaOhk6QRD8Ehl%2FCux9zKeq9aA2BktXYTgz27ohbBXvyc78%3D" rel="nofollow" target="_blank">“疫苗临床试验”思维导图模板获取链接</a></p><h2>一、核心主题确定</h2><p>以“疫苗临床试验”为核心主题，鉴于其涉及多维度科学管理流程，需系统梳理关键环节。以临床试验的逻辑链为主线，整合变量控制、分组策略、数据采集及质量管控，构建闭环知识框架，确保全面覆盖试验管理的各个方面。</p><h2>二、导图结构设计</h2><h3>主分支划分</h3><ul><li>依据试验执行阶段，将导图拆解为四大核心模块，即变量体系、分组逻辑、数据采集、质量管理，确保导图结构能够完整覆盖试验全流程。</li></ul><h3>层级逻辑</h3><ul><li><strong>变量体系</strong>：首先区分自变量与因变量，进而细化设计要素，如疫苗类型、剂量梯度等，随后补充干扰因素的控制措施，包括基线平衡、环境监测等，形成层次分明的变量控制体系。</li><li><strong>分组逻辑</strong>：从对照组类型（如安慰剂对照、阳性对照、交叉对照）出发，逐步深入到随机化方法的选择，层层递进，体现科学严谨的分组原则。</li><li><strong>数据采集与质量管理</strong>：按照时间节点（如筛查期、免疫期）与检测维度（实验室检测、临床评估）进行交叉设计，同时设置独立的质量管理模块，监控数据准确性与伦理合规性。</li></ul><h2>三、导图样式设计</h2><ul><li><strong>布局架构</strong>：采用<strong>鱼骨图</strong>布局的基础结构，串联起各个核心模块，以此增强导图的叙事流动性和视觉吸引力。</li><li><strong>颜色搭配</strong>：运用图形天下思维导图提供的<strong>17套主题配色</strong>，对各分支进行区分，如绿色代表变量体系、红色代表分组逻辑等，以提升信息检索的效率和导图的整体美观度。</li><li><strong>层次结构</strong>：针对导图中信息量较大的分支，如“干扰因素控制”下的基线平衡、治疗限制等子项，可使用<strong>分支折叠</strong>功能，将详细信息隐藏于主分支之下，避免单节点信息过载，提升导图的整体平衡性和可读性。</li></ul><p><a href="https://link.segmentfault.com/?enc=tupL6Rx0kwnVOz6Pd7eqnQ%3D%3D.xA%2BbhXC3q81cP1IGgTfuns4EeJenKfpPB9OKsMoX560QFny1TN%2FoooT9%2BA%2FGqJBBT9gXNf0H0%2Fyu%2BX7mAx7uVUKWRXndCopDSlZRnMt7GWmKptgccIBwVynFAEfbHQbV" rel="nofollow" target="_blank">“疫苗临床试验”思维导图模板在线免费体验链接</a></p><h2>四、导图工具与流程</h2><ul><li><strong>工具选择</strong>：使用图形天下思维导图（Amind）软件进行创作，该软件支持复杂分支的拖拽调整与样式的统一管理，极大提升了创作效率。特别是其提供的<strong>12种结构化布局图形</strong>和<strong>37种计算逻辑组合</strong>，能够满足多样化设计需求。</li><li><p><strong>创作流程</strong></p><ul><li><strong>大纲整理</strong>：提炼疫苗试验的核心要素，归类至预设模块。</li><li><strong>结构设计</strong>：利用<strong>鱼骨图</strong>布局，搭建主分支框架，填充子节点内容，确保逻辑连贯。同时应用<strong>分支折叠</strong>功能优化导图的可读性。</li><li><strong>样式调整</strong>：使用<strong>样式设置</strong>对导图进行整体优化，调整字体、颜色、布局等，提升导图的美观度和可读性。</li><li><strong>校对验证</strong>：检查术语准确性（如“区组随机化”定义），确保与临床试验标准一致。</li></ul></li></ul><p><a href="https://link.segmentfault.com/?enc=bsUWkRg2Wl1kISIyNEScgA%3D%3D.AH0aYh0pBQX8uiaegEvxgfJcX6AjBfxx%2F8JRGfSCYPvhXYE%2Bg%2FZN4SHPwDvHv2tOI5p8LwCRObXm%2FhLJ5msK3w%3D%3D" rel="nofollow" target="_blank">图形天下思维导图（Amind）软件免费下载链接</a></p><h2>五、总结</h2><p>本次疫苗临床试验规划思维导图的创作，充分利用了图形天下思维导图（Amind）软件的<strong>鱼骨图</strong>布局和<strong>分支折叠</strong>等功能，该导图不仅覆盖了试验的全流程，还通过视觉优化和层级管理，提升了信息的可读性和检索效率，为临床试验的科学管理提供了有力支持。</p><p>访问图形天下思维导图（Amind）<strong>模板库</strong>与<strong>教程资源</strong>，获取更多免费导图素材与实操指南，激发你的无限创意。</p><ul><li><a href="https://link.segmentfault.com/?enc=en6XtYmlIhGxHN%2FzLKKRog%3D%3D.YIN01KqRLLJ5HGWvjvcMx2M%2FJqjbX5yx3OXt5SY0B4pyxHHWP57lXHQTuLpUik%2FOcpZsEHTb3w2kQWuHCiK8Tw%3D%3D" rel="nofollow" target="_blank">Amind思维导图模板库</a></li><li><a href="https://link.segmentfault.com/?enc=1xejkqO0ki6tda%2BA6SVeaA%3D%3D.FVXlweZrvgQahmGe3yWTRdzITfoPXUt1LFQa5mqc%2BwQjlvzFhDvVG6iCX2UubUmmmKqUZQNKpiPXmp3LSnVwnA%3D%3D" rel="nofollow" target="_blank">思维导图使用教程资源</a></li></ul>]]></description></item><item>    <title><![CDATA[开放原子大会上最意外的一幕：时序数据库 ]]></title>    <link>https://segmentfault.com/a/1190000047436880</link>    <guid>https://segmentfault.com/a/1190000047436880</guid>    <pubDate>2025-11-28 18:08:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当 AI 与开源成为产业级趋势时，我们不得不思考几个现实问题：真正能把开源工程体系打磨扎实的团队有多少？又有多少项目能在技术浪潮之外，持续保持高质量迭代与社区活跃？</p><p>上周举行的 <strong>2025 开放原子开发者大会</strong>，给出了一个有代表性的答案。大会主题定为「一切为了开发者——AI 共智，开源共享」，更关注项目实践、工程落地和开发者生态，而不仅仅停留在概念层面的讨论。而今年的 <a href="https://link.segmentfault.com/?enc=aWAEGiM1f%2F6RmKIFqSR69A%3D%3D.I7yOUg4J28fPC8zxwwkFtVXYePeLLvKxwEfmO%2Fh2inK815gg%2FxGNLf9KDihw2wjD86pmB%2FR18ROD70ijsL9vM59RqfOSrlRAuqJ0awnnK14NYdc7RKnjO3ttRYgg8Lg%2F" rel="nofollow" target="_blank">TDengine</a>，在这个舞台上出现了两次：一次在主论坛，一次在颁奖仪式。</p><h2>“一切用代码说话”：TDengine 的工程方法论</h2><p>在主论坛演讲中，<a href="https://link.segmentfault.com/?enc=NnPsWZ2DMiLHF%2F7Da1xXuQ%3D%3D.vmFNqFjquo%2FVV9zHM1GsZl%2BSZDi7%2FSFQOUG1afTQ%2FFpV4O94ryoaoPLDfSWmywgg7zUvPAz7QI4M25JMgxdPfDhfEjWPi0Gux9IvSyUIPhi%2FzIrMVc6It%2Bjr40sEvqak" rel="nofollow" target="_blank">涛思数据</a>（TDengine）创始人 &amp; CEO 陶建辉带来了《一切用代码说话》的主题分享——一个来自一线研发团队的实践复盘。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436882" alt="" title=""/></p><p>他在演讲中提到一个颇具冲击力的事实：<strong>2020 年以前，TDengine 的研发效能是如今的至少 5 倍。</strong></p><p>这一对比背后，是他长期在中美研发团队观察到的问题：新人环境上手慢、规范分散、测试覆盖不可追踪、测试资源长期紧张……这些情况并非某个团队的特例，而是大量软件团队的共性挑战。TDengine 给出的解决思路很直接——<strong>流程、规范、测试、配置，全都代码化。</strong></p><p>代码化带来的结果同样直接：工程规范变得可审查、可复现、可协作，测试场景可追踪，研发节奏不再被资源瓶颈阻断。更重要的是，随着团队规模扩大，这套体系避免了“人走知识散”的反复重建，让工程质量可以持续累积。在《研发天天加班，但总是忙不过来，为什么这样，有解吗？》这篇文章中，陶建辉也更系统地阐述了这一点，强调只有把所有关键环节代码化，研发效率才能真正提升。</p><p>在一个以开源为底的项目里，工程效率不是锦上添花，而是保证社区持续活跃、保证版本稳定交付的基础设施。陶建辉的分享，本质是在回答一个问题：<strong>中国开源项目如何构建长期生命力？</strong></p><h2>两位 TDengine 开发者入选“开源之星”：属于工程师的现场答卷</h2><p>如果说主论坛让大家看到了 <a href="https://link.segmentfault.com/?enc=Fg1jReQpPiwu2JysoNh01w%3D%3D.z%2B1ZLkJpckWn5jnZHnkJtgbHBuTP51tibZ2Blv9lgfLR%2F6ytp5E9EG2jFZ83dQVx8SmBK9lj22j1bo1Kkwtw%2BFHKKcffcms3fj2XWZNvD3iM5Pxi16sizBNFA2BPfUhq" rel="nofollow" target="_blank">TDengine</a> 创始人对于研发效率与工程实践的思考，那么在“开源创新力量致谢仪式”上，TDengine 的另一面被更多人看见了。</p><p>在今年覆盖操作系统、数据库、人工智能三大领域的评选中，共有 47 位开发者获得表彰，TDengine 的两位研发同事——<strong>谭雪峰、邝金清</strong>——同时入选开源先锋项目开发者及“开源之星”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436883" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436884" alt="" title="" loading="lazy"/></p><p>这两位同事长期投入在核心代码、质量改进、社区维护等关键工作中。对于开源项目来说，这些投入往往不在台前，却直接影响着项目的稳定性与演进速度。正是这些日常而持续的工程贡献，构成了一个开源项目能够长期发展的基础。</p><p>从主论坛到颁奖现场，今年 TDengine 在大会上的两次亮相，展示的是同一个事实：<strong>开源项目真正的价值，始终由实打实的工程投入和开发者自身的贡献决定。</strong></p><h2>工程为先，生态自来</h2><p>今年开放原子大会也设置了多个主题论坛和开源市集，展示从基础软件到 AI 工具链的多种开源能力。整体呈现出一个趋势：产业不再单纯追求“项目数量”，而是开始关注“项目质量”。</p><p>对于 TDengine 来说，这意味着两件事：</p><ul><li>在工程体系中继续把研发、测试、规范统一到可复现的框架里；</li><li>在社区层面，让更多参与者愿意基于工程质量继续投入贡献。</li></ul><p>开源项目的竞争不在发布会，而在代码库；不在宣传稿，而在工程体系，最终能把生态撑起来的，始终是过硬的工程能力。</p>]]></description></item><item>    <title><![CDATA[FlowyAIPC 发布全新 4.0：开]]></title>    <link>https://segmentfault.com/a/1190000047436904</link>    <guid>https://segmentfault.com/a/1190000047436904</guid>    <pubDate>2025-11-28 18:07:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>【2025年11月28日】 —— 端侧 AI 生产力工具 <strong>FlowyAIPC</strong>正式发布全新 <strong>4.0</strong> 版本。本次更新围绕 <strong>“主动生产力”“端侧智能”“本地化大模型加速”</strong> 三个方向进行了深度演进，进一步推动 AI 从工具型能力向真正的智能操作系统层能力迈进。</p><p><img width="723" height="733" referrerpolicy="no-referrer" src="/img/bVdncuo" alt="" title=""/></p><p>FlowyAIPC 4.0 聚焦于让一台普通电脑成为可主动协助用户处理任务的<strong>个人 AIPC（AI Personal Computer）</strong> ，支持本地大模型推理、桌面级智能交互、会议全流程自动化、个人资料深度理解等功能，且完全可离线运行，适用于企业、开发者和个人用户等多场景。</p><h3>深度融合 WML，NPU 本地推理速度显著提升</h3><p>在 4.0 版本中，FlowyAIPC 与 Windows 全新的 <strong>Windows Machine Learning（WML）</strong>  机制深度融合。本地模型将直接运行在设备 NPU 上，相比 CPU/GPU 方案，拥有更低功耗、更快响应以及更流畅的多任务处理能力。</p><p>这意味着用户在处理文本生成、代码补全、总结归纳、文件搜索等任务时，几乎无需等待，端侧 AI 真正实现“随叫随到”。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncuv" alt="" title="" loading="lazy"/></p><h3>OCR：智能截图，打破单一截图的局限</h3><p>FlowyAIPC 4.0 将截图能力升级为 <strong>智能 OCR 流程</strong>：可通过AI鼠标的左侧前进键、键盘快捷键（Alt+B）或使用悬浮球截图后，系统可自动完成 <strong>文字提取 → 实时翻译 → AI 问答 → 一键存入笔记</strong> 的闭环操作。用户无需切换工具，就能把屏幕信息立刻转化为可检索、可编辑的内容。</p><p>场景示例：网页、PPT、图片、视频字幕的快速提取与翻译；截取资料后马上“问问 Flowy”获取上下文解读；一键存入个人知识库，方便后续检索与复用。<br/><img width="723" height="526" referrerpolicy="no-referrer" src="/img/bVdncuw" alt="" title="" loading="lazy"/></p><h3>全新的会议智能助手：一键从录音到会议纪要</h3><p>FlowyAIPC 4.0 将会议流程拆解成可自动执行的步骤：</p><ul><li>一键录音、实时转字幕</li><li>实时翻译多语种内容</li><li>自动捕捉关键议题与行动项</li><li>会议结束后自动生成正式<strong>会议纪要文档</strong></li></ul><p>整个流程全自动执行，让每个人都能拥有一个随身的“智能会议秘书”。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdm4dz" alt="" title="" loading="lazy"/></p><p><img width="723" height="402" referrerpolicy="no-referrer" src="/img/bVdm4dR" alt="" title="" loading="lazy"/></p><h3>桌面浮球：升级为主动提醒与工作中心</h3><p>FlowyAIPC 4.0 引入更先进的“主动关怀提醒系统”，可在本地分析用户资料、任务节点、文档内容及日程信息，实现：</p><ul><li>天气变化提醒</li><li>待办事项提醒</li><li>重要事项的跟踪与复盘</li><li>根据使用习惯提出下一步工作建议（如寻找资料、起草文稿、整理内容）</li></ul><p>所有计算均在本地完成，可离线使用，兼顾隐私与效率。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdncuz" alt="" title="" loading="lazy"/></p><h3>Agent Store：智能体商店，人人都能创建专属智能体</h3><p>FlowyAIPC 4.0 引入 Agent Store（智能体商店），内置丰富多样的智能体模版（例如：周报生成、文本续写、合同审核等）。用户可直接选用模型与模版，并通过极简提示词配置其行为。无需编程，即可快速搭建、调整专属智能体，实现高度定制化的桌面自动化与工作流优化。<br/><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdncuB" alt="" title="" loading="lazy"/></p><h3>本地个人知识库：让 AI 更懂你、更有记忆</h3><p>FlowyAIPC 4.0 支持上传本地笔记、会议记录、工作资料，结合端侧模型形成用户专属的“个人模型记忆”。相比云端大模型，它对本地文件更敏锐，也能持续记住用户偏好和工作方式。</p><p><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdncuC" alt="" title="" loading="lazy"/></p><h3>受邀亮相 Intel 英特尔技术创新与产业生态大会</h3><p>FlowyAIPC 受邀参加由 Intel 主办的<strong>英特尔技术创新与产业生态大会（Intel Connection）</strong> ，并将在大会期间面向行业伙伴展示全新的 <strong>FlowyAIPC 4.0</strong> 版本。</p><p>此次亮相将重点展示 FlowyAIPC 在 Windows AI PC 生态下的本地化智能体验，以及其在 NPU 加速、本地推理、端侧知识库等方面的能力，展现面向未来的个人智能环境。</p><p><img width="723" height="676" referrerpolicy="no-referrer" src="/img/bVdncuD" alt="" title="" loading="lazy"/></p><h3>面向未来：让 AI 成为长期陪伴的桌面伙伴</h3><p><strong>FlowyAIPC 4.0</strong> 的目标不是让 AI 只是“回答问题”，而是让它真正融入个人电脑的日常使用场景，成为长期陪伴、深度理解用户的桌面伙伴。</p><p>FlowyAIPC 团队表示，未来版本将继续聚焦端侧智能、跨设备协同、知识库隐私能力等方向，推动 <strong>AIPC</strong> 成为面向个人的核心生产力工具。</p><p><strong>立即访问官网，体验FlowyAIPC 4.0：</strong><br/>👉 <a href="https://link.segmentfault.com/?enc=kO2d4DddO%2B7wmdUtbRemTQ%3D%3D.qzp3XACBbDWtO5NR4HMvzUKY2%2BmrPXHWqI2jyu5dws0%3D" rel="nofollow" target="_blank">www.flowyaipc.cn</a></p>]]></description></item>  </channel></rss>