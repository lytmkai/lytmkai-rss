<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[零售行业 SRM 系统推荐榜单（2026版）——供应商管理全景指南 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047600121</link>    <guid>https://segmentfault.com/a/1190000047600121</guid>    <pubDate>2026-02-12 08:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>做零售的都懂，供应商管理往往是企业运营中最繁杂、最耗时间的那一块——品类多、供应商多、对账频繁，还得实时盯供货节奏。尤其在当前线上线下融合加速的趋势下，靠 Excel 表格管供应商管理早已跟不上节奏。为了帮助零售企业快速建立科学、高效的供应商管理体系，本文整理了一份<strong>2026年适配零售行业的 SRM 系统推荐榜单</strong>，为企业选型提供实用参考。</p><p><strong>一、什么是 SRM 系统？为何对零售如此关键</strong></p><p>在进入推荐之前，先明确一个核心概念：<strong>SRM</strong> 系统是用来管理供应商全生命周期的一套软件解决方案。它的目标是帮助企业从供应商准入、绩效管理、风险监控，到日常协同、合同与对账等业务流程，实现数字化、标准化和可视化管理。</p><p>在零售行业，这类系统尤为重要，因为：</p><p>（1）零售企业的采购品类极其杂乱，SKU 数量庞大，供应商类型多样，从品牌方、大型经销商到小型供应商与加工厂，管理复杂度高；</p><p>（2）线上线下库存变动快，促销、季节性业务强，供货节奏必须准确把握；</p><p>（3）对账频率高、数据核对工作量巨大，若没有系统支持，很难避免账目不清、对账延迟等问题；</p><p>（4）供应商绩效评估和风险预警体系不足会直接影响供货稳定性。</p><p><strong>SRM 系统的价值不仅是“管数据”，更是帮助企业提升供应链协同效率、降低风险、提升供应商价值贡献、实现战略采购的长期平台。</strong></p><p><strong>二、零售行业 SRM 系统推荐榜单</strong></p><p>在众多 SRM 平台中，我们结合零售行业的典型需求（如多门店协同、库存节奏与采购链路紧密联动、促销物资临时采购、大宗物料管理等）进行了评估，推荐了以下几款系统：</p><p><strong>1 正远科技 — </strong><strong>零售全场景低代码定制 SRM 方案</strong></p><p><strong>推荐指数：★★★★★</strong><br/><strong>适用规模：大型连锁 / 区域连锁 / 新零售集团</strong></p><p>正远科技 SRM 的最大特色是<strong>低代码可编排架构</strong>，支持业务人员通过拖拽可视化方式快速调整流程，而无需 IT 二次开发，适合零售企业业务变化频繁的特性。</p><p>在零售行业的典型场景中：</p><p>（1）促销临时采购流程、临时物料变更、赠品采购等审批流可快速调整；</p><p>（2）多区域采购需求与多门店执行协同无缝衔接；</p><p>（3）门店数据、仓储、ERP、对账系统之间实现实时协同。</p><p>在供应商生命周期管理上，它支持从供应商准入、评级、评级调整到淘汰全流程闭环控制，同时具备实时风险监控能力，有助于减少因供应商突发状况导致的断货风险。在订单与执行层面，系统支持 VMI 库存管理和条码收货，这对门店稀散、收货点多的零售企业尤为重要。真正实现采购订单、物流路径、对账流水在一个协同大平台中的柔性联动。</p><p><strong>亮点功能：</strong></p><p>（1）低代码可视化流程自定义；</p><p>（2）支持类电商化采购商城；</p><p>（3）与 ERP、OA、企业微信、工作流无缝集成；</p><p>（4）提供实时供货节奏监控与预警。</p><p>该系统在实际落地案例中，能有效缩短采购周期、提升对账效率，在部分零售企业中采购周期平均缩短约 40% 以上，供应链协同效率显著提高。<br/><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdnS7Y" alt="" title=""/></p><p><strong>2 甄云科技 — </strong><strong>AI 驱动智能采购 SRM</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：大型 / 规模化零售企业</strong></p><p>甄云科技的 SRM 最大亮点在于<strong>AI 与大数据的深度嵌入</strong>。在零售行业常见的“品类杂，价格波动快”的场景下，甄云的智能比价和风控引擎能够帮助企业在采购阶段快速筛选、比价与评估供应商。</p><p>例如，它的 AI 比价工具能在几秒内完成多平台的价格比对，并提供趋势分析，这对于价格敏感的零售采购极具价值，同时还能直接提升成本控制能力。系统还能对接大量外部监控数据，进行供应商风险预警。</p><p><strong>优势特点：</strong></p><p>（1）跨平台智能比价与采购成本预警；</p><p>（2）实时风险监测与供应商健康得分；</p><p>（3）支持多语言、多币种，适合跨境采购；</p><p>（4）与 ERP/WMS 等多生态系统集成。</p><p>甄云科技 SRM 的标准化程度较高，对于流程规范、企业规模较大的零售集团尤为适合。但对于需要深度业务定制的场景，其灵活度略逊与更开放的低代码平台。<br/><img width="723" height="383" referrerpolicy="no-referrer" src="/img/bVdnS7Z" alt="" title="" loading="lazy"/></p><p><strong>3 鲸采云 — </strong><strong>中小零售轻量化 SaaS 优选方案</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：中小零售 / 区域连锁</strong></p><p>鲸采云定位轻量化 SaaS SRM，其操作门槛低、部署迅速，非常适合中小型零售企业。系统覆盖了从供应商准入、绩效评估到采购执行的基本 SRM 模块，同时内置了标准的供应商数据管理能力，支持扫码收货、智能补货、分门店采购协同等功能。</p><p>针对中小企业常见的“预算有限、没有 IT 团队”的情况，该系统提供了丰富的第三方插件接口，可以与金蝶、用友等主流财务系统无缝对接，并支持与钉钉、企业微信等移动办公系统联动。</p><p><strong>适配亮点：</strong></p><p>（1）快速上手、低门槛 SaaS 方案；</p><p>（2）支持采购商城式下单体验；</p><p>（3）集成主流办公和财务系统插件；</p><p>（4）门店与总部协同集中管理功能完整。</p><p>对于采购流程相对稳定、供应商数量适中、对高级定制需求不高的中小零售企业，这款系统性价比较高。<br/><img width="723" height="362" referrerpolicy="no-referrer" src="/img/bVdnS70" alt="" title="" loading="lazy"/></p><p><strong>4 用友采购云 — </strong><strong>ERP 生态深度联动 SRM</strong></p><p><strong>推荐指数：★★★★☆</strong><br/><strong>适用规模：已部署用友 ERP 的零售企业</strong></p><p>用友采购云的优势在于与其 ERP 大生态产品深度集成，能将采购流程、生意账务和库存管理紧密联动，从而避免信息孤岛和重复录入工作。通过自动化审批与财务联动，可显著提升合规性与对账效率。</p><p>特别是在跨区域、跨业务单元的零售企业，用友采购云对不同业务模式的支持能力很强，例如多税制处理、合同自动生成、应付账款自动管理等。</p><p><strong>主要优势：</strong></p><p>（1）深度 ERP / 财务系统集成；</p><p>（2）自动化审批与合规管理；</p><p>（3）支持全球采购与跨币种场景；</p><p>（4）对账流程全链路自动化。</p><p>对于已经在用友系统中的零售企业来说，该系统可以<strong>最大限度减少 ERP 与 SRM 之间的数据不一致情况</strong>。<br/><img width="723" height="376" referrerpolicy="no-referrer" src="/img/bVdnS71" alt="" title="" loading="lazy"/></p><p><strong>5 企企通 — </strong><strong>SRM + 供应链金融结合的特定场景解决方案</strong></p><p><strong>推荐指数：★★★☆☆</strong><br/><strong>适用规模：资金链压力大的中小企业</strong></p><p>企企通的特色在于其 SRM 平台与供应链金融服务的结合，可以为合作供应商提供融资支持，从而在一定程度上稳定供应链。对于一些资金压力较大、对现金流敏感的中小零售企业，这种功能尤其有价值。</p><p>该系统在供应商管理、订单协同、物流跟踪等基础功能之外，加强了资金服务联动，例如供应商融资额度展示、资金通道对接等。对于希望通过供应链金融提升供货稳定性和合作紧密度的企业是一大加分项。</p><p>但在行业专属功能方面，例如临时促销物资管理、多门店库存节奏优化等方面不如前几款完善，因此更适合<strong>采购流程相对简单、希望补贴供应链现金流的中小企业</strong>。<br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnS72" alt="" title="" loading="lazy"/></p><p><strong>五、选型小贴士与实施建议</strong></p><p>（1）<strong>先从业务优先级出发，不要盲目追求花哨功能</strong>：比如对账自动化、风险预警、门店端补货支持，这些是零售企业最基础的痛点。</p><p>（2）<strong>试用是必须的</strong>：任何系统的卖点再好，若实际操作繁琐、不贴合业务流程，那么 ROI 很难体现。</p><p>（3）<strong>数据迁移与集成成本要提前评估</strong>：老系统迁移、新系统上线与现有库存、财务数据的集成成本往往被低估。</p><p>（4）<strong>长期战略要考虑扩展性</strong>：是否支持未来供应链升级，这是构建企业竞争力的关键。</p><p><strong>六、结语</strong></p><p>供应商管理是零售企业运营效率的核心组成部分，也是企业实现精细化运营和供应链协同的战略基础。<strong>选对 SRM 系统等于为企业供应链打下坚实的数字化基础</strong>。<br/>本文整理的榜单结合了国内 SRM 供应商和国际通行的优秀方案，同时结合零售行业具体特征，为不同规模和发展阶段的零售企业提供实用选型参考。</p>]]></description></item><item>    <title><![CDATA[Copilot 2026 完全指南：2026 年了，它凭什么还是月活第一的 AI 编程助手？ 卡尔A]]></title>    <link>https://segmentfault.com/a/1190000047606576</link>    <guid>https://segmentfault.com/a/1190000047606576</guid>    <pubDate>2026-02-11 22:04:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文共 5200 字，阅读预计需要 6 分钟。</p><p>编程 IDE 赛道卷成红海，Cursor、Claude Code、Google AI Studio 各有拥趸。但 Copilot 在 2026 年依然稳坐月活第一的位置，它凭什么？</p><p>这篇文章，从我个人深度使用的体验出发，<strong>拆解 Copilot 的三个核心优势、三个明显劣势、以及六个核心特性的实战用法</strong>——帮你判断，它到底适不适合你。</p><p><img width="474" height="474" referrerpolicy="no-referrer" src="/img/bVdnUNB" alt="" title=""/></p><p><strong>一次 Cursor 账单，让我彻底想明白了</strong></p><p>先说一件真事。</p><p>前几天，我在 Cursor Max 模式下，开了 Plan 模式，用 Claude Opus 4.6 重构一个项目的页面样式。</p><p>一次 Plan 制定，一次 Plan 修改，再加上按照 Plan 执行工作。</p><p><strong>三个步骤，花掉了十几刀的 token 费用。</strong></p><p><img width="723" height="145" referrerpolicy="no-referrer" src="/img/bVdnUNC" alt="" title="" loading="lazy"/></p><p>然后我算了一下，这三个步骤如果在 Copilot 里做，就是三次独立的调用。再乘以 Copilot 对 Opus 4.6 设置的系数 3，也就是消耗 9 次 premium request。</p><p><strong>而 Copilot 每月 10 美金的订阅套餐里，有 300 次这样的调用额度。</strong></p><p>换句话说，Cursor 里花十几刀干的活，Copilot 只扣了 9 次调用——连月度额度的 3% 都不到。</p><p>这个差距，让我彻底想明白了一件事：<strong>在长期、复杂项目使用的场景下，成本结构比单次能力更重要。</strong></p><p>说实在的，我不是说 Cursor 不好。Cursor 在很多方面确实更强，后面会讲到。但作为一个每天都要写代码、调研资料、做内容创作的人，我需要一个成本可控的主力工具，而不是每次点「执行」后都要关注token用量。</p><p><strong>三个让我留下来的理由</strong></p><p><strong>按次计费：O(1) 复杂度的成本控制</strong></p><p>Copilot 最核心的竞争力，就是它的按次计费逻辑。</p><p>每月 10 美金的 Pro 套餐，包含 300 次 premium request。即使超额，每次也只收 4 美分。</p><p>这意味着什么？</p><p>它不会因为你用了最贵的 Claude Opus 4.6，就让你反复盯着 token 用量看。它只是出于成本考虑，让一次调用消耗 3 次 premium request 的额度——但费用不会随 token 用量上涨而上涨。</p><p><img width="723" height="454" referrerpolicy="no-referrer" src="/img/bVdnUND" alt="" title="" loading="lazy"/></p><p>我喜欢用一个程序员都懂的类比：<strong>这就像 O(1) 复杂度的算法。</strong></p><p>不管你输入多大，成本是固定的。</p><p>相比之下，Cursor 的计费更接近 O(n)——输入越大，token 越多，费用越高。Cursor Pro 每月 20 刀，Pro+ 每月 60 刀，Ultra 每月 200 刀，而且这些套餐背后还是基于用量的逻辑。</p><p>在大型项目的重构修改、大量资料调研，或者长上下文的内容创作中，<strong>Copilot 的开销可能只有按 token 计费的几十分之一。</strong></p><p>对于经常要调用强模型做大型任务的开发者来说，这个差距是实打实的。</p><p><strong>新模型第一时间支持 + 无限 Tab 补全</strong></p><p>Copilot 对新模型的支持速度一直很快——只要厂商开放了 API，基本第一时间就能用上。</p><p><img width="669" height="1077" referrerpolicy="no-referrer" src="/img/bVdnUNE" alt="" title="" loading="lazy"/></p><p>而且 GPT-5 mini 、Gpt-4o等的调用和 Tab 代码补全，在 Pro 套餐里都是无限次数的。</p><p>这个"无限"很重要。Tab 补全可能是你写代码时每分钟都在用的功能，随手做点修改，增添个函数，如果这个也限次数，那体验会非常割裂。Copilot 在这一点上没有扣扣搜搜。</p><p>当然也有例外。比如 GPT-5.3 Codex并没有支持，但是这个的原因是 OpenAI 延迟开放 API 的老传统，现在三方 IDE 都用不了。</p><p><strong>GitHub 生态的原生力量</strong></p><p>这一点经常被忽略，但其实很关键。</p><p>Copilot 背靠微软和 GitHub，如果你平常接触开源比较多，它的生态整合是相当完整的。GitHub 的 issue、PR、仓库索引，都是原生集成的，<strong>不需要任何额外配置。</strong></p><p>Copilot 的 coding agent 在上线后的前 5 个月里，开发者用它合并了超过 100 万个 Pull Request。这个数字本身就说明了生态粘性。</p><p>另外，Copilot 对 Plan 模式和 Skills 的支持也做得不错。Plan 模式可以让模型先规划再执行，Skills 则是一个可扩展的工具提示词集合——比如 Anthropic 官方出的前端设计 Skill，可以显著提升 Copilot 做前端的效果。这些在后面的特性拆解里会详细讲。</p><p><strong>三个不可忽视的缺点</strong></p><p>说完优势，聊聊让我不太舒服的地方。</p><p><strong>前端样式：Copilot 的"审美盲区"</strong></p><p>这是最明显的短板。</p><p>Copilot 在前端样式、UI 设计这些方面，和 Cursor、Google AI Studio 的差距比较大。尤其是用 GPT 系列模型的时候，尤其是Codex，出来的页面效果。。。说实在的特别难评。</p><p>以下是我在copilot里，用GPT-5.1-Codex-Max，做的火柴人小游戏：</p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnUNG" alt="" title="" loading="lazy"/></p><p>这个页面的设计审美真的有点过分了。。。</p><p>后来我接了 Anthropic 官方发的前端设计 Skill 之后，效果能好一些，但如果你真的要用 Copilot 做前端样式类的工作，<strong>最好还是切到 Claude Opus 来搞。包括写作，我个人体感更好的也是Claude的模型。</strong></p><p><strong>超大型任务：和 Cursor Max 的差距</strong></p><p>Copilot 的 Agent 模式在超大型任务上的执行力，和 Cursor 有一点差距。</p><p>虽然我个人体感这个差距很小，日常使用几乎感觉不到，但是，Cursor 开启 Max 模式后，是能感觉出来的——同样的任务，同样的模型，Cursor 对<strong>复杂项目、复杂任务的精确执行和精确理解</strong>做得要好一些。</p><p>而且，除了Gpt-5.2 Codex，<strong>copilot对其他模型的上下文limit基本都是128K</strong>，这也是限制它超长任务执行能力的关键。</p><p><img width="723" height="129" referrerpolicy="no-referrer" src="/img/bVdnUNH" alt="" title="" loading="lazy"/></p><p>甚至 Cursor 还有多 Agent 竞赛的功能，可以让多个 Agent 同时尝试不同方案，然后你选最好的那个。</p><p>这个"更强"的代价，也许是几十倍的成本，但在一些场景下是真的有对应的收益的。</p><p>所以这事得看你怎么算账：是为了 5% 的精度提升花数倍甚至数十倍的钱，还是用 Copilot 把 90% 的活先干了，剩下多去迭代几轮或者自己上？</p><p><strong>自定义规则和记忆：灵活度不够</strong></p><p>Cursor 有 rules 这样的系统，能做到项目级的规则配置，还有记忆功能来记住用户的编程偏好——比如你喜欢用什么命名规范、偏好什么代码风格，它都能记住。基本接一个cursor-memory-bank，就能很方便的实现。</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnUNI" alt="" title="" loading="lazy"/></p><p>Copilot 虽然也有 .github/copilot-instructions.md，但说实话，灵活度和粒度上差不少。</p><p>这就好比一个能记住你口味的老厨师，和一个每次都要重新告诉他"少盐少油"的新厨师。做出来的菜可能差不多，但沟通成本差很多。</p><p><strong>六个核心特性实战手册</strong></p><p>说了这么多宏观的优劣势，我们来看看 Copilot 各个核心特性的具体用法。<strong>这部分比较实操，建议收藏。</strong></p><p><strong>1. Tab 补全：最成熟，也最容易被低估</strong></p><p>Tab 补全是 Copilot 最早出名、也是最成熟的功能。订阅 Pro 之后无限次使用。</p><p>你在写代码的时候，它会实时预测你接下来要写的内容，按 Tab 就能接受建议。对于逻辑简单的函数，你甚至可以只写一行注释，然后靠 Tab 补全快速把代码写完。</p><p><img width="723" height="212" referrerpolicy="no-referrer" src="/img/bVdnUNJ" alt="" title="" loading="lazy"/></p><p>这个功能有几个小技巧，很多人不知道：</p><p><strong>第一，写好注释再写代码。</strong> 注释越清晰，补全质量越高。这其实就是在给模型做上下文工程——你的注释就是 prompt。</p><p><strong>第二，打开相关文件放在旁边的 Tab 里。</strong> Copilot 会自动把打开的文件当作上下文来参考。所以如果你在写一个调用其他模块的函数，把那个模块文件打开放旁边就行。</p><p><img width="723" height="215" referrerpolicy="no-referrer" src="/img/bVdnUNK" alt="" title="" loading="lazy"/></p><p><strong>第三，Ctrl+右箭头逐词接受。</strong> 如果补全的内容只对了一半，不用全盘接受或全盘拒绝，按 Ctrl+右箭头可以一个词一个词地接受。这个技巧能省很多手动修改的时间。</p><p><strong>2. Inline Chat：小范围精修利器</strong></p><p><img width="645" height="294" referrerpolicy="no-referrer" src="/img/bVdnUNL" alt="" title="" loading="lazy"/></p><p>在代码里按 Ctrl+I（Mac 上是 Cmd+I），可以直接在当前位置发起一次对话。</p><p>适合小范围的修改，比如"给这个函数加上错误处理"、"把这段逻辑重构成异步的"之类的。</p><p>它的好处是改完直接有 diff 预览，你可以逐行审查，不满意就撤销。比在聊天窗口里来回复制粘贴高效得多。</p><p><img width="597" height="327" referrerpolicy="no-referrer" src="/img/bVdnUNM" alt="" title="" loading="lazy"/></p><p>很多人常用 Chat 面板做小修改，但其实<strong>微调切到 Inline Chat 效率更高也更精准</strong>。</p><p><strong>3. Ask 模式：纯对话，不动代码</strong></p><p><img width="684" height="333" referrerpolicy="no-referrer" src="/img/bVdnUNN" alt="" title="" loading="lazy"/></p><p>Ask 模式就是侧边栏的对话窗口。在这里可以选模型、问问题、讨论方案。</p><p>它的特点是"纯对话，不动代码"——不会帮你直接改文件，<strong>但你可以引用工作区的文件给它</strong>。</p><p>我的习惯是，把需要的文件直接选中后，鼠标拖到对话框里，省掉复制粘贴。</p><p><img width="569" height="243" referrerpolicy="no-referrer" src="/img/bVdnUNO" alt="" title="" loading="lazy"/></p><p>不过，Copilot 对工作区的文件都有访问权限，显式引用只是提醒模型重点关注。多个项目的话，从左上角把文件夹添加到工作区即可。</p><p><img width="489" height="486" referrerpolicy="no-referrer" src="/img/bVdnUNP" alt="" title="" loading="lazy"/></p><p><strong>4. Agent 模式：核心中的核心</strong></p><p>Agent 模式是各个编程 IDE 最核心的功能，也是 Copilot 用得最多的模式。</p><p>在 Agent 模式下，Copilot 可以自主地读文件、写文件、跑终端命令、分析报错，然后迭代修复，直到任务完成。</p><p><strong>按次计费的爽感就体现在这里：即使迭代了特别多轮，它还是只收一次的费用。</strong></p><p>另外一个很实用的细节：Agent 模式里可以<strong>控制模型可用的工具</strong>。</p><p><img width="444" height="211" referrerpolicy="no-referrer" src="/img/bVdnUNQ" alt="" title="" loading="lazy"/></p><p>比如你只想让它帮你解决部署问题但不修改任何文件，可以把文件编辑的工具禁用掉。</p><p>这在生产环境排查问题的时候特别有用，<strong>相当于强制禁止文件修改。</strong></p><p><img width="700" height="366" referrerpolicy="no-referrer" src="/img/bVdnUNR" alt="" title="" loading="lazy"/></p><p><strong>5. Plan 模式：复杂任务专属</strong></p><p><img width="723" height="266" referrerpolicy="no-referrer" src="/img/bVdnUNS" alt="" title="" loading="lazy"/></p><p><strong>Plan 模式</strong>是专门用来做复杂任务的。</p><p>它会强制模型输出完整的执行计划，并且从执行来看会启动一些子 agent 来做信息收集，防止主 Agent 的上下文过长。</p><p>关键是：在你点击「Start Implementation」之前，你可以和模型反复对话来修改计划。</p><p><strong>即使你告诉它"现在开始执行任务"，只要还在 Plan 模式下，它还是只做计划生成。</strong></p><p>所以，「Start Implementation」本质上就是点击后，</p><p>1、帮你切换到了 Agent 模式，</p><p>2、把「Start Implementation」输入到输入框中</p><p>因此如果你读计划读得差不多了，<strong>自己手动切到 Agent 模式让它执行，效果是一样的。</strong></p><p>我的习惯是：<strong>涉及复杂的、大量文件改动的任务，都先让它出 Plan，确认没问题了再放手让它跑。</strong></p><p><img width="723" height="645" referrerpolicy="no-referrer" src="/img/bVdnUNT" alt="" title="" loading="lazy"/></p><p><strong>6. Skills：各IDE的“杀手级”功能</strong></p><p><strong>Skills</strong> 是近两个月，copilot刚支持的功能。</p><p><img width="723" height="347" referrerpolicy="no-referrer" src="/img/bVdnUNU" alt="" title="" loading="lazy"/></p><p>在设置中打开 userAgentSkills 的开关，把 Skills 文件下载到指定路径下，模型就能读取和使用这些 Skills 了。</p><p><img width="684" height="579" referrerpolicy="no-referrer" src="/img/bVdnUNV" alt="" title="" loading="lazy"/></p><p>比如 Anthropic 官方出的前端设计 Skill，能显著提升 Copilot 生成前端代码的质量。这其实就是一套精心设计的系统提示词，告诉模型在做前端任务时应该遵循哪些设计原则和最佳实践。</p><p><strong>写在最后：适合你的，才是最好的</strong></p><p>总结一下这一期的内容。</p><p>Copilot 的三个核心优势：<strong>按次计费成本可控、新模型支持快、GitHub 生态整合强。</strong></p><p>三个主要劣势：<strong>前端样式效果不好、超大型任务的执行力略逊、自定义规则和记忆能力不够灵活。</strong></p><p>六个核心功能的使用方法：<strong>Tab 补全、Inline Chat、Chat 面板里的 Ask、Agent 和 Plan 三种模式，以及 Skills。</strong></p><p>这些优缺点都很明显，没有哪个工具是完美的。关键是匹配你的场景。</p><p>我个人的建议是这样的：</p><p><strong>1、如果你经常做大型的后端项目，且频繁调用比较强的模型来做重构、修改之类的工作</strong> → Copilot 的按次计费逻辑会帮你省非常多的钱。这是它最核心的优势。</p><p><strong>2、如果你追求极致的执行精度，不太在乎 token 开销</strong> → Cursor 开 Max 模式确实体感更强，甚至可以开启多 Agent 竞赛的功能。但月均开支要做好心理准备。</p><p><strong>3、如果你更多是做小的演示 demo 或者 UI 设计</strong> → Google AI Studio 也许更适合你。免费额度够用，从想法到可运行的小项目特别快。</p><p>**总之，我的建议是组合着用。**Copilot 当主力省成本，Cursor Max 当精度补刀，Google AI Studio 做快速验证。</p><p>这套组合拳打下来，性价比是最高的。</p><p><strong>既然看到这了，如果觉得不错，随手点个赞、收藏、转发三连吧～</strong></p><p><strong>我是Carl，大厂研发裸辞的AI创业者，只讲能落地的AI干货。</strong></p><p><strong>关注我，更多AI趋势与实战，我们下期再见！</strong></p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdnUis" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Agent Lightning：微软开源的框架无关 Agent 训练方案，LangChain/Aut]]></title>    <link>https://segmentfault.com/a/1190000047606596</link>    <guid>https://segmentfault.com/a/1190000047606596</guid>    <pubDate>2026-02-11 22:03:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Agent 搭建起来之后怎么让它真正变得越来越好？搭建完成后的优化就很少有人认真说过。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606598" alt="" title=""/><br/>Agent Lightning 号称能把任何 AI Agent 变成"可优化的猛兽"，而且几乎不用改代码。那问题来了，市面上 Agent 框架满天飞这个凭什么就不一样呢？</p><h2>training gap</h2><p>做过 Agent 部署的人大概都有同感：把 Agent 跑起来其实没那么难，真正难的是让它持续进步。</p><p>OpenAI 的 Agent SDK、LangChain 这类编排框架，原型设计和快速部署确实很拿手。几个小时就能让一个能用的 Agent 上线。但到了优化这一步，用真实场景的反馈去训练 Agent、提升它的表现基本就只能靠自己摸索了。</p><p>微软的研究人员给这个问题起了个名字叫"training gap"。开发环境里跑得好好的 Agent一碰到真实用户、边缘场景和领域特有的问题性能就打折扣。传统框架能给你的帮助很有限：手动调 prompt，手动改参数，然后顺带祈祷别有问题。</p><p>而Agent Lightning 的切入点就在这里，它把 Agent 框架和优化基础设施做了解耦。微软的说法是这套方案"可以无缝地为任何现有 Agent 启用模型训练，无需对 Agent 代码做任何修改。"</p><h2>Agent Lightning 的工作原理</h2><p>Agent Lightning 在现有 Agent 代码和微软的 verl 训练基础设施之间插入了一层客户端-服务器架构。可以理解为一个翻译层：把 Agent 的交互记录转化成训练数据，优化完参数再塞回去。</p><p>具体流程是：Agent 照常运行，什么都不用改，但每一次交互都会被 Lightning 客户端截获。数据会传到 Lightning 服务器，服务器端跑强化学习、自动 prompt 优化、监督微调这些手段，再把改进后的参数推回到 Agent 里。</p><p>特别值得说的是框架的兼容性：LangChain、AutoGen、CrewAI、微软自家的 Agent Framework都能接。团队管它叫"Lightning AI Agent 的终极训练器"。</p><p>安装也是直接一个pip命令：</p><pre><code>pip install agentlightning</code></pre><p>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606599" alt="" title="" loading="lazy"/></p><h2>实际应用和用例</h2><p>最有说服力的场景是 Agent 需要适配私有数据或者特定行业需求的情况。通用预训练模型处理常规任务还行，碰到公司内部流程、行业“黑话”、独特的业务逻辑，就容易出问题。</p><p>拿客服 Agent 举例：它得学会你公司特有的工单升级流程、产品的各种坑、跟客户打交道的语气和方式。传统做法是手写 prompt 然后盼着它能泛化到各种情况。换成 Agent Lightning系统能直接从真实客户对话中学习，拿解决率、满意度评分、各项业务指标来自动优化响应策略。</p><p>代码生成也是个很适合的场景：Agent 在跟你的代码库、编码规范、开发流程不断交互的过程中，Agent Lightning 能持续微调模型，让它越来越贴合团队的具体要求。</p><p>搜索和检索类应用也一样，Agent 需要弄清楚哪些信息源对哪类查询最有价值、怎么按用户偏好排序结果、什么时候该转人工，这些都可以在实际使用中不断优化。</p><h2>竞争格局</h2><p>Agent Lightning 进入的赛道已经很拥挤了，但定位上有明确的差异化。别人在卷 Agent 编排和模型服务，而微软选择切入的是一个几乎没人认真做过的方向：优化。</p><p>Agent 优化可以说是平台策略的自然延伸，通过解决那些单纯做模型或做编排的玩家解决不了的问题，把开发者留在微软的生态里。</p><p>而且Agent Lightning 没有被包装成 Azure 的专属服务而是直接开源，这既展现了微软对自身平台能力的信心，也说明他们对推动这个领域发展有诚意。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606600" alt="" title="" loading="lazy"/></p><h2>总结</h2><p>AI Agent 行业一直在解决"怎么搭"，却没认真回答"搭完之后怎么办"。而Agent Lightning 把开发和优化解耦这个思路填补了从 LangChain 到 AutoGen 这一批框架都没覆盖到的空白。</p><p>但是从版本能看得出来，0.1.2 版离生产级还有距离。但方向本身没问题，当 AI Agent 越来越多地承担关键业务，能持续从真实反馈中学习的 Agent 和不能的之间差距只会越拉越大。谁先跑通这条优化闭环，谁就拿到了下一阶段的门票。</p><p><a href="https://link.segmentfault.com/?enc=RYzO1TPOgfo084XpPgGTKQ%3D%3D.J4w%2FzlaM5Wz%2BnS7HGpUKd%2BHQWO1wniz7ZrLEraxpvu6XMYnTkyokx7C6kK3k%2FLeEvr6WIwphr5OjxdJZGg6sQQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/eea592726e5940c29d80fadf9908b2e6</a></p><p>by Mandar Karhade</p>]]></description></item><item>    <title><![CDATA[《GraphQL批处理与全局缓存共享的底层逻辑》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047606613</link>    <guid>https://segmentfault.com/a/1190000047606613</guid>    <pubDate>2026-02-11 22:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>微前端架构在分布式前端体系的深度落地过程中，跨应用数据请求的冗余分发已然成为制约前端整体效能提升的核心桎梏，传统碎片化的请求发起模式下，彼此解耦的微应用针对同源基础元数据的重复拉取行为，不仅持续加剧网络传输层的资源损耗与带宽占用，更会间接引发页面渲染时序的紊乱、前端状态的不同步等隐性架构问题，GraphQL批处理与缓存共享的融合落地方案，绝非对现有请求机制的表层修补与局部优化，而是从数据调度逻辑与状态共识构建的底层内核出发，彻底重构微前端体系内数据流转的核心范式。批处理机制的核心价值在于将离散化、碎片化的独立请求，转化为具备语义关联的聚合调度单元，彻底打破应用物理边界对请求链路的人为割裂，缓存共享则致力于搭建跨应用的全局数据共识层，让同源数据在整个微应用集群中实现一次采集、全域复用的理想状态。这一技术思路的本质是把数据请求从单一应用的独立行为，升级为整个架构层面的协同动作，从根源上消解重复请求生成的土壤，让微前端的数据交互回归高效、统一、可控的理想状态，也让前端架构从被动适配业务需求的底层形态，转向主动治理数据流转的高阶形态，在分布式前端体系中建立起数据流转的秩序感与稳定性，让每一次数据交互都能贴合架构的整体设计逻辑，而非无序消耗系统资源。</p><p>GraphQL批处理在微前端复杂场景中的落地，核心依托于对请求依赖的拓扑化深度分析与聚合粒度的精细化动态调控，在多微应用并行初始化的典型业务场景中，各类基础配置信息、核心主体元数据、全局权限规则等非业务独占型数据，成为跨应用重复请求的高发区域，批处理机制并非简单将多条独立请求机械合并为单一传输链路，而是先完成请求语义的精准归类与依赖关系的逐层拆解，严格区分实时性要求较高的动态数据与稳定性较强的静态数据，仅对同数据域、同执行优先级、同生命周期的请求执行聚合调度操作，同时完整保留每一条请求的独立响应解析能力，从机制上避免单一请求异常引发整体链路的响应故障。实践过程中通过精准界定请求的共享域标识，让批处理引擎能够精准识别可聚合的请求单元，既最大化保障请求合并带来的效能收益，又不破坏每一个微应用的数据独立性与业务自治性，让批处理成为适配微前端弹性架构的轻量化调度能力，也让请求聚合从机械合并的初级形态升级为语义驱动的智能调度形态，大幅提升数据交互的精准度与稳定性，让每一次网络传输都能实现资源利用的最大化，彻底规避无效请求与重复传输带来的资源内耗，让请求调度逻辑贴合微前端架构的解耦核心诉求。</p><p>微前端缓存共享体系的构建，核心是打造分层可控、逻辑清晰的跨应用状态共识体系，而非粗暴的全局数据拷贝与无差别共享，基于微前端基座的中继调度能力，将缓存体系划分为全局公共缓存、跨应用共享缓存、应用私有缓存三个逻辑层级，其中公共缓存专门承载全应用复用的基础元数据，共享缓存适配多应用协同的业务关联数据，私有缓存则全力保障单应用的业务隔离性与数据私密性。缓存共享的核心价值不在于存储本身，而在于跨应用的数据同步与一致性保障，通过语义化的缓存版本标识与轻量化订阅分发机制，实现缓存更新的全域无感同步，同时设计精细化的失效触发规则，结合数据更新事件与应用生命周期节点，主动清理过期缓存数据，避免脏数据在整个微应用集群中扩散。实践中通过基座的缓存代理层，统一管控缓存的读写操作与数据分发流程，让微应用无需感知底层缓存的实现细节，仅通过标准化接口即可获取共享数据，大幅降低跨应用数据协同的适配成本，也让缓存管理从分散失控的初级形态转向集中可控的架构级能力，在多应用共存的复杂环境中持续维持数据状态的统一与可信，为微前端的数据协同提供稳定的底层支撑。</p><p>GraphQL批处理与缓存共享的协同运转，构建起请求调度、数据缓存、全域分发的闭环治理体系，二者的耦合逻辑并非简单的功能叠加，而是相互赋能、深度融合的有机整体，批处理引擎为缓存层提供高质量、归一化的标准数据源，彻底避免碎片化请求带来的数据格式差异与字段冲突问题，缓存层则为批处理提供前置的命中校验能力，从源头大幅减少重复请求的触发频次。在多微应用并行加载的实际业务场景中，系统先通过共享缓存层完成同源数据的原子性命中判定，缓存命中时直接向各依赖微应用分发标准化数据，未命中时则由批处理引擎聚合所有待请求单元，生成单一高效的调度链路执行数据拉取操作，响应结果经归一化处理后存入共享缓存，再同步至所有依赖该数据的微应用。这一过程中，缓存命中的原子性判定与批处理的防重触发机制，成为保障协同稳定性的核心节点，让数据请求与缓存复用的衔接实现无间隙、无冗余，也让整个数据流转链路形成自驱式的效能优化闭环，持续降低系统的网络负载与计算消耗，让数据交互的每一个环节都能实现最优效率，彻底解决微前端架构中重复请求的行业痛点。</p><p>该技术方案在微前端架构中的规模化落地，需聚焦非功能维度的精细化优化与架构适配，批处理的效能发挥依赖合理的性能阈值动态设定，结合实时网络环境与页面渲染时序要求，动态调整请求聚合的等待窗口与最大聚合粒度，避免过度聚合引发的响应延迟问题，缓存共享则注重内存资源的轻量化管控，通过数据过期策略与懒加载机制，精准控制缓存存储的资源占用规模，同时将批处理拦截与缓存代理逻辑，无缝融入微前端的应用加载与全生命周期流程，不侵入微应用的核心业务代码，保持各应用的技术栈无关性与业务自治能力。实践中通过架构层的统一封装，让数据协同能力成为微前端基座的原生能力，无需各微应用单独适配改造，同时构建数据流转的全链路感知体系，让请求调度、缓存命中、数据分发的全流程可观测、可追溯，为后续优化与迭代提供精准的数据依据，保障方案长期迭代的可扩展性，也让微前端的数据治理能力具备持续演进的底层支撑，完美适配业务规模与架构形态的动态变化，为大型分布式前端体系的稳定运行保驾护航。</p><p>从技术实践的长期视角来看，GraphQL批处理与缓存共享的深度融合，并非临时性的性能优化手段，而是微前端数据治理体系的核心基础能力，这一方案解决的不仅是重复请求的表层问题，更构建了跨应用数据协同的标准化技术范式，让微前端架构从应用拼装的初级形态，升级为数据一体化的成熟形态。通过彻底消解数据孤岛与请求冗余，大幅降低系统的网络开销与渲染阻塞风险，全面提升整体用户体验与系统运行稳定性，其核心价值在于用极简的架构逻辑，解决分布式前端的复杂数据问题，回归技术服务于业务的本质。</p>]]></description></item><item>    <title><![CDATA[《GraphQL状态图建模与低时延控制能力解析》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047606616</link>    <guid>https://segmentfault.com/a/1190000047606616</guid>    <pubDate>2026-02-11 22:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>物联网设备态联拓扑的规模化落地进程中，设备状态图的高效查询与控制指令的低时延调度，已然成为构筑全域物联交互体系的核心命题，传统物联查询接口的刚性范式，始终难以适配异构设备的态数据柔性获取需求，固定字段与固定接口的设计逻辑，无法匹配设备状态图动态变化的拓扑结构，更难以满足多场景下差异化的态数据拾取诉求，GraphQL以态联查询的独特技术特性切入设备状态图交互场景，彻底打破了固定接口与设备态拓扑的适配壁垒，其在设备状态图查询中的优劣势深度博弈，本质是态粒度定制化能力与物联链路客观约束性的动态平衡，而实时订阅机制对设备控制指令低延迟需求的实际适配能力，更是直接决定物联控制层整体交互效能的关键标尺。设备状态图并非单一的态数据简单罗列，而是涵盖设备本体运行态、集群协同联动态、环境感知关联态的三维拓扑化态联结构，GraphQL对这一复杂结构的解构与精准查询能力，从底层重构了物联态数据的交互逻辑，也让传统物联查询从被动适配场景转向主动建模需求，这一技术选型的深层思考，必须扎根物联场景的链路传输特性、终端算力边界、业务交互核心诉求，而非单纯依托技术表层特性做浅层次落地应用。态查询的柔性价值与物联场景的客观约束，共同构成了GraphQL在物联网场景中落地的核心考量维度，也让设备态查询与指令控制的协同交互，拥有了全新的技术探索方向，这种从技术本质到场景深度适配的全维度思考，也是物联网前端交互技术迭代升级的核心逻辑，更是区分技术炫技与工程落地的关键标尺。</p><p>GraphQL在物联网设备状态图查询中的核心优势，完全根植于态粒度的定制化拾取与态联拓扑的柔性解析能力，设备状态图本身承载着多维度、多层级的态数据信息，从设备基础运行态、功能模块工作态，到深层集群联动状态、环境关联响应态，不同业务场景对态数据的拾取需求存在极其显著的差异性，传统查询模式需要依托多接口拆分适配不同需求场景，极易产生态数据冗余传输、链路资源无效消耗、终端解析压力过载等一系列问题，GraphQL可依据实际交互需求，精准拾取设备状态图中的目标态字段，完全无需传输冗余无效数据，完美适配物联网终端带宽有限、算力薄弱、续航敏感的客观特性，同时其态联查询能力可深度解析设备状态图的拓扑关联逻辑，实现跨设备、跨集群、跨区域的态数据联动查询，统一异构设备的态查询口径，大幅降低多类型终端接入的适配成本与开发周期。设备状态图的态元数据自描述特性，还能让前端交互层快速感知态数据结构与关联关系，简化设备态可视化的开发流程，让设备状态图的查询从固定范式转向柔性建模，大幅提升物联态数据的传输、解析与渲染全链路效能，也为物联网设备态的精细化管理、全域化监控提供了核心技术支撑，这种按需适配、精准获取的查询特性，让物联网多终端、多场景、多协议的态数据交互拥有了更灵活、更高效的实现路径，也让物联感知层的数据采集效率实现了质的飞跃。</p><p>GraphQL应用于物联网设备状态图查询的显性短板，集中体现在复杂态联拓扑的解析开销与场景化适配的多重约束层面，设备状态图的拓扑关联越复杂、层级越丰富，GraphQL的态查询解析单元需要处理的关联逻辑就越繁杂，这一过程会持续消耗服务端与边缘节点的运算资源，在边缘算力受限、供电紧张的物联网场景中，解析开销会直接转化为态查询的响应延迟，进而影响物联交互的实时性与稳定性。定制化的态查询需求需要后端构建精细化的态联解析逻辑，每一次设备状态图的拓扑迭代、态字段新增，都需要同步调整解析规则，大幅提升了设备状态图的维护与迭代成本，不同物联网终端的算力差异、存储差异、适配能力差异，也让轻量级传感设备、低功耗终端难以适配复杂的态查询解析流程，形成柔性查询与终端适配性的核心矛盾。同时跨域态联查询的协同约束，会让设备状态图的跨节点查询面临链路损耗、节点跳转延迟等问题，进一步放大技术特性带来的性能短板，这些劣势并非技术本身的固有缺陷，而是GraphQL的柔性特性与物联网场景客观约束碰撞产生的适配问题，也是落地过程中需要重点攻克、分层优化的核心难点，这种技术特性与场景约束的天然冲突，也是物联网技术选型中必须直面、理性权衡的现实问题，无法通过简单的参数调整实现完全消解。</p><p>GraphQL实时订阅机制为物联网设备控制指令的交互提供了全新的技术实现路径，其依托持久化连接构建的态推送体系，彻底摒弃了传统轮询模式的资源浪费与时延损耗，成为适配设备控制指令低延迟需求的核心支撑能力，实时订阅可精准绑定设备状态图与控制指令的关联关系，当控制指令下发或设备态发生变更时，通过增量推送机制仅传输核心指令与变更态数据，大幅缩短数据传输的链路时长与载荷体积。在物联控制场景中，边缘节点可作为订阅中继节点，承接云端与终端的指令中转任务，进一步压缩指令传输的物理路径，降低端到端的响应延迟，订阅会话的轻量化管理机制，可支撑多设备、多集群并发的指令订阅需求，避免会话冗余带来的资源抢占与链路拥堵，同时指令与态数据的双向订阅交互，能让控制指令的下发与设备态的反馈形成完整闭环，保障物联控制的精准性与实时性。这一机制的核心价值，在于将传统的被动查询转为主动推送，让设备控制指令的交互逻辑完全贴合物联场景的低延迟诉求，也让物联控制层的交互效率实现了质的提升，边缘侧的本地化订阅处理，还能进一步降低云端依赖，提升指令响应的稳定性，即便在弱网、断网边缘场景中，也能保障核心控制指令的本地执行与状态同步，让物联控制的可靠性得到全方位保障。</p><p>GraphQL实时订阅对设备控制指令低延迟需求的满足能力，存在明确的场景化适配边界，并非能够全场景覆盖物联控制的严苛时延要求，在高密度设备集群的集中控制场景中，大量并发订阅会话会挤占传输带宽与运算资源，导致指令推送的链路拥堵、排队延迟，直接放大整体响应延迟。物联网场景的网络波动性、不稳定性，会直接影响持久化连接的稳定性，连接抖动、中断会直接打破实时订阅的低延迟保障，边缘节点的算力分配若偏向设备状态图的解析处理，会挤占控制指令的调度资源，形成查询与订阅的资源抢占矛盾，进一步加剧时延问题。不同协议物联网设备的指令转换环节，会产生额外的时延损耗，让高要求的低延迟需求难以落地，同时订阅机制的保活逻辑需要持续消耗链路资源与终端算力，在弱网、窄带环境中，保活机制的失效会直接中断指令推送，影响控制指令的实时传递与执行。这些适配边界的存在，要求实时订阅机制必须结合物联场景特性做定制化优化，而非盲目套用通用化的订阅逻辑，这种场景化的适配思考、差异化的策略调整，也是物联网技术落地的核心准则，更是保障控制指令低延迟需求落地的关键前提。</p><p>物联网场景中GraphQL的落地应用，需要依托场景特性制定差异化的选型策略与全维度优化方案，平衡设备状态图查询的优劣势，精准适配控制指令的低延迟需求，针对设备状态图查询，可采用分层态联建模的方式，拆解复杂拓扑的关联逻辑，简化解析流程，降低服务端与边缘节点的运算开销，针对轻量级终端、低功耗设备，简化态查询的解析流程，裁剪非核心功能，保障终端的适配性与运行稳定性。</p>]]></description></item><item>    <title><![CDATA[大模型榜单周报（2026/02/08） KAI智习 ]]></title>    <link>https://segmentfault.com/a/1190000047606621</link>    <guid>https://segmentfault.com/a/1190000047606621</guid>    <pubDate>2026-02-11 22:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 本周概览</h2><p>本周大模型行业呈现多维度竞争格局，模型调用量榜单出现显著变化，Google Gemini 3 Flash Preview强势登顶，Kimi K2.5爆发式增长。各大厂商密集发布新模型，OpenAI推出GPT-5.3-Codex编码模型，Anthropic发布Claude Opus 4.6，美团推出多模态统一大模型方案STAR，快手可灵AI发布3.0版本，上海AI实验室发布书生Intern-S1-Pro。编程能力榜单中，Kimi K2.5-thinking成为国产编程模型榜首。前沿数学能力榜单出现重大调整，Claude Opus 4.5 (no thinking)成绩暴增跃居前三。</p><h2>2. 重点关注事件</h2><ul><li><strong>OpenAI发布GPT-5.3-Codex编码模型</strong>（2.6）：融合GPT-5.2推理能力与GPT-5.2-Codex编码性能，运行速度提升25%，支持终端操作与长期任务。该模型曾参与自身训练调试，被定为首个"高"网络安全风险等级。</li><li><strong>Anthropic发布Claude Opus 4.6</strong>（2.6）：显著提升编码、推理与代理任务能力，首创百万token上下文窗口。Terminal-Bench 2.0等评测领先，GDPval-AA超GPT-5.2达144 Elo分，定价维持$5/$25每百万token不变。</li><li><strong>美团推出多模态统一大模型方案STAR</strong>（2.4）：凭借创新的"堆叠自回归架构 + 任务递进训练"双核心设计，GenEval突破0.91，实现了"理解能力不打折、生成能力达顶尖"的双重突破。</li><li><strong>快手可灵AI发布3.0版本</strong>（2.4）：推出视频3.0与Omni模型，支持智能分镜、图生视频+主体参考、多语种对口型、15秒长视频生成。</li><li><strong>上海AI实验室发布书生Intern-S1-Pro</strong>（2.4）：核心科学能力实现跃升，高难度综合学科评测稳居AI4S领域国际领先水平，复杂数理逻辑推理能力达奥赛金牌水平，面向真实科研流程的智能体能力位居开源模型第一梯队。</li></ul><h2>3. 榜单变化</h2><h3>OpenRouter模型调用量排名</h3><ul><li><strong>整体调用量</strong>：Google Gemini 3 Flash Preview强势登顶，从上周第2位（580B tokens，14%增长）跃升至本周第1位（791B tokens，36%增长），反超Claude Sonnet 4.5成为榜首；Claude Sonnet 4.5退居次席，从上周第1位（766B tokens，15%增长）降至本周第2位（727B tokens，5%增长），环比调用量绝对值减少39B tokens；Kimi K2.5爆发式增长新入前三，本周以673B tokens和350%的增长率位列第3，而上周未进入前十榜单；Grok Code Fast 1大幅下滑，从上周第3位（477B tokens，12%增长）骤降至本周第8位（336B tokens，下降30%），排名下跌5位；MiniMax M2.1高速增长新入榜，本周以371B tokens和115%的增长率位列第7，上周未在榜单中。</li><li><strong>模型市占率</strong>：MoonshotAI爆发式攀升，从上周203B tokens（3.5%，第7位）暴涨至本周606B tokens（8.8%，第5位），份额增长5.3个百分点，排名上升2位；x-ai大幅下滑，从上周719B tokens（12.3%，第4位）骤降至本周587B tokens（8.6%，第6位），份额减少3.7个百分点；MiniMax强势入榜，本周以323B tokens（4.7%）新进入前十榜单第7位；三大巨头份额齐降，Google保持第1但份额从24%降至23%，Anthropic保持第2但份额从17.1%降至15.4%，OpenAI保持第3但份额从14%降至13.4%；DeepSeek稳中有进，从上周553B tokens（9.4%，第5位）增至本周651B tokens（9.5%，第4位），超越x-ai上升1位。</li><li><strong>模型吞吐量</strong>：gpt-oss-120b速度大幅回落，从上周第2位（836 tok/s）骤降至本周第4位（447 tok/s），速度下降46%；Llama 3.1 8B Instruct性价比跃升，从上周第9位（Cerebras提供，203 tok/s，0.10/M）升至本周第6位（Groq提供，306tok/s，0.05/M），速度提升51%且价格降低50%；两款模型跌出前十，上周第5位的Llama 3.3 70B Instruct（265 tok/s）和第8位的Qwen3 Next 80B（233 tok/s）本周退出榜单；两款模型入榜，Llama 4 Maverick（第8位，181 tok/s）和Mistral Small Creative（第9位，180 tok/s）新进入前十；Gemini 2.5 Flash Lite Preview持续提速，从上周第10位（169 tok/s）升至本周第7位（221 tok/s），速度提升31%。</li><li><strong>编程调用量</strong>：Kimi K2.5爆发式增长登顶，从上周第4位（139B tokens，8.9%）暴涨至本周第1位（463B tokens，25.2%），份额激增16.3个百分点；Grok Code Fast 1大幅下滑，从上周榜首（255B tokens，16.4%）骤降至本周第3位（173B tokens，9.4%），份额减少7个百分点；MiniMax M2.1快速攀升，从上周第6位（115B tokens，7.4%）跃升至本周第2位（226B tokens，12.3%），份额增长4.9个百分点；Claude双模型份额齐降，Claude Sonnet 4.5从第2位（12.3%）降至第5位（7.9%），Claude Opus 4.5从第3位（10.0%）降至第4位（8.7%）；GPT-5.2持续收缩，从第8位（61.4B tokens，3.9%）降至第9位（38.7B tokens，2.1%），同时<a href="https://link.segmentfault.com/?enc=YtoYq3Hre6jO1UiT0rJK%2FA%3D%3D.AyFNM5R6Zw%2BUEsNttfzXDYfvfwp2GXtItlB832H%2FqZw%3D" rel="nofollow" target="_blank">https://www.arcee.ai/</a>发布的400B参数稀疏MoE开源模型Trinity Large Preview (free)新进入前十榜单，排名第7位。</li></ul><h3>各领域能力榜单</h3><ul><li><strong>编程能力榜单（Code Arena）</strong>：Kimi K2.5-thinking新晋榜单第5位，仅次于御三家的模型，成为国产编程模型榜首。</li><li><strong>文生图能力榜单（Artificial Analysis Text to Image Leaderboard）</strong>：FLUX.2 [dev] Turbo分数超过Nano Banana，二者排名易位，分别排名9、10。</li><li><strong>理科能力榜单（GPQA LLM Stats）</strong>：Claude Opus 4.6以91.3%的得分排名第4位，仅次于GPT-5.2 Pro、GPT 5.2和Gemini 3 Pro。</li><li><strong>前沿数学能力榜单（EPOCH AI FrontierMath）</strong>：Claude Opus 4.5 (no thinking)成绩暴增跃居前三，从上周五第16位（准确率20.7%，60/290）飙升至本周第3位（38.3%，111/290），准确率提升17.6个百分点；其次是Kimi K2.5 (Fireworks)新进入前十榜单，以27.9%（81/290）排名第10，取代了同系列的Kimi K2 Thinking（21.4%，第15位）。</li><li><strong>GAIA测试集榜单</strong>：LR AILab of Lenovo CTO Org发布的Lemon agent登顶首位。</li></ul><h2>4. 排行榜</h2><table><thead><tr><th>测评类型</th><th>第一名</th><th>第二名</th><th>第三名</th></tr></thead><tbody><tr><td>模型调用量</td><td>Gemini 3 Flash Preview</td><td>Claude Sonnet 4.5</td><td>Kimi K 2.5</td></tr><tr><td>公司市占率</td><td>Google</td><td>Anthropic</td><td>OpenAI</td></tr><tr><td>模型速度</td><td>gpt-oss-safeguard-20b</td><td>Qwen3 32B</td><td>gpt-oss-20b</td></tr><tr><td>编程模型调用量</td><td>Kimi K 2.5</td><td>MiniMax M2.1</td><td>Grok Code Fast 1</td></tr></tbody></table><h3>各公司按不同能力领域排名汇总</h3><table><thead><tr><th>测评类型</th><th>领先公司</th></tr></thead><tbody><tr><td>大语言模型 Text Arena</td><td>Google、xAI、Anthropic、百度、OpenAI、智谱、阿里巴巴、月之暗面</td></tr><tr><td>编程能力 Code Arena</td><td>Anthropic、OpenAI、Google、智谱、MiniMax</td></tr><tr><td>编程能力 LiveCodeBench</td><td>OpenAI、Anthropic、Google</td></tr><tr><td>代码工程任务能力 SWE-benchLite</td><td>基于Claude、Gemini、GPT、Qwen、DeepSeek开发的开源系统</td></tr><tr><td>图像编辑和生成能力 Image Edit Arena</td><td>OpenAI、Google、字节、腾讯、Black Forest Labs、Reve</td></tr><tr><td>文生图能力 Text-to-Image Arena</td><td>OpenAI、Google、Black Forest Labs、腾讯</td></tr><tr><td>图像编辑和生成能力 Image Editing Leaderboard</td><td>OpenAI、Google、字节、Black Forest Labs、阿里巴巴、Reve</td></tr><tr><td>文生图能力 Text to Image Leaderboard</td><td>OpenAI、Google、Black Forest Labs、字节、Fal</td></tr><tr><td>GPQA</td><td>OpenAI、Google、Anthropic、xAI、阿里巴巴</td></tr><tr><td>FrontierMath</td><td>OpenAI、Google、Anthropic、DeepSeek、月之暗面、xAI</td></tr><tr><td>Humanity's Last Exam</td><td>Google、OpenAI、Anthropic</td></tr><tr><td>GAIA</td><td>LR AILab of Lenovo CTO Org、JoinAI、Nvidia、Suzhou AI Lab&amp;Shuqian Tech、Microsoft AI Asia -Ads、ShawnAgent、ZTE-AICloud</td></tr></tbody></table><hr/><p>关注我，第一时间掌握更多AI前沿资讯！</p>]]></description></item><item>    <title><![CDATA[JDK22安装教程 Windows版：详细步骤+验证方法（含下载地址） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047606503</link>    <guid>https://segmentfault.com/a/1190000047606503</guid>    <pubDate>2026-02-11 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><h4><strong>一、准备工作</strong>​</h4><ol><li><strong>获取安装包</strong>：从指定链接下载JDK 22安装包至电脑（链接：<a href="https://link.segmentfault.com/?enc=p%2BshW50iFE2X688RUJDs%2Fg%3D%3D.K9881napwecNUUx6sgp71C1Ta813wRxXC6kMn5VJDH5jg2hD8nuTqmdnnOhDV5zs" rel="nofollow" title="https://pan.quark.cn/s/09ba1b00f415" target="_blank">https://pan.quark.cn/s/09ba1b00f415</a></li></ol><h4><strong>二、安装步骤</strong>​</h4><ol><li><strong>解压安装包</strong>：右键点击下载的安装包文件，选择【解压到当前文件夹】（建议解压至非系统盘，如D盘，避免C盘空间不足）。</li><li><strong>进入安装目录</strong>：打开解压后的【JDK22】文件夹（可通过资源管理器直接双击进入）。</li><li><strong>启动安装程序</strong>：找到【JDK-22.0.2_windows-x64_bin.exe】文件，右键选择【以管理员身份运行】（管理员权限可避免安装路径写入失败等问题）。</li><li><p><strong>确认安装路径</strong>：</p><ul><li>弹出安装向导后，点击【下一步】；</li><li>保持默认安装路径（或自定义路径，建议路径不含中文/空格），再次点击【下一步】。</li></ul></li><li><strong>等待组件安装</strong>：安装过程中会显示“正在更新组件”，耐心等待进度完成（约1-3分钟，无需额外操作）。</li><li><strong>完成安装</strong>：组件更新完毕后，点击【关闭】退出安装向导。</li></ol><h4><strong>三、验证安装成功</strong>​</h4><ol><li><strong>打开命令提示符</strong>：按下快捷键 <code>Win + R</code>调出“运行”窗口，输入 <code>cmd</code>并点击【确定】（或按回车）。</li><li><p><strong>检查JDK版本</strong>：在命令提示符窗口中输入 <code>java -version</code>，按下回车键。若显示类似以下信息，说明安装成功：</p><pre><code>java version "22.0.2" 2022-07-19  
Java(TM) SE Runtime Environment (build 18.0.2+9-61)  
Java HotSpot(TM) 64-Bit Server VM (build 18.0.2+9-61, mixed mode, sharing)</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p></li></ol><h4><strong>注意事项</strong>​</h4><ul><li>若需配置环境变量（如<code>JAVA_HOME</code>、<code>Path</code>），可根据实际需求补充设置（JDK 18默认可能已自动配置基础环境，通过<code>java -version</code>能识别即无需额外操作）。</li><li>安装路径建议简洁（如 <code>D:\Program Files\Java\jdk-22.0.2</code>），避免后续开发工具引用时出错。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[SuperScan4单文件扫描安装步骤详解（附端口扫描与主机存活检测教程） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047606486</link>    <guid>https://segmentfault.com/a/1190000047606486</guid>    <pubDate>2026-02-11 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p> </p><p><code>SuperScan4</code>是 <strong>SuperScan 4</strong>​ 的单文件扫描工具，主要用来做<strong>端口扫描、主机存活检测</strong>，网管、搞安全的、测试网络连通性的人常用它快速扫一批 IP，看哪些机器开着、哪些端口开着。</p><p>它是绿色单文件，所谓的“安装”其实就是准备好环境、直接运行，下面用大白话一步步说。</p><h2>一、准备工作</h2><ol><li><p><strong>下载 SuperScan4.exe</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=j7uO3HFDrfi%2FRSVybS3ATg%3D%3D.9noLlXI4FEpknIY3WE7Bv2ac7LBkt7GYSBstAcogJ8H4tKLY%2Fq5OQU1i9irKfYwY" rel="nofollow" title="https://pan.quark.cn/s/56173e5006cb" target="_blank">https://pan.quark.cn/s/56173e5006cb</a></p></li><li><p><strong>确认系统版本</strong>​</p><ul><li>支持 Win7/Win10/Win11 等常见 Windows 系统，老版本在 Win10/Win11 可能需要右键“以管理员身份运行”才能正常扫。</li></ul></li></ol><h2>二、“安装”步骤（其实就是运行准备）</h2><p>SuperScan4 是绿色单文件，<strong>不用像普通软件那样一步步装</strong>，只要保证能打开并正常使用：</p><ol><li>把下载好的 <code>SuperScan4.exe</code>放到一个固定文件夹，比如 <code>D:\Tools\SuperScan</code>，别放桌面容易误删或丢失。</li><li>右键 <code>SuperScan4.exe</code>→ 选“以管理员身份运行”（有些系统不提权会出现权限错误或扫不到结果）。</li><li>如果是 Win10/Win11，会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>第一次打开可能界面比较简单，直接就能用，不需要额外装插件。</li></ol><h2>三、基本使用方法（简单说两句）</h2><ol><li>打开 SuperScan4.exe → 在 “IP 范围” 里填要扫的地址段，比如 <code>192.168.1.1-192.168.1.254</code>。</li><li><p>选端口范围：</p><ul><li>默认会扫一些常用端口，也可以自己填，比如 <code>1-1000</code>或单独 <code>80,443,3389</code>。</li></ul></li><li>点  <strong>“Start”</strong> （开始）按钮 → 等扫描结果出来，会列出存活的主机和开放端口。</li><li>可以导出结果或复制到记事本保存。</li><li>扫的时候别一次性扫太大范围，尤其是外网，容易被认为攻击，还可能被防火墙拦截。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[汽车行业如何选研发管理平台？看看行业标杆客户怎么说 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047606365</link>    <guid>https://segmentfault.com/a/1190000047606365</guid>    <pubDate>2026-02-11 19:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在汽车行业，研发管理平台的选型面临独特挑战，尤其是在“智能座舱、自动驾驶、车载电子”快速发展的背景下。汽车研发不仅涉及复杂的软硬件协同，还需要应对严格的合规要求、长周期项目管理、跨部门协同等难题。选择合适的研发管理平台，不仅关乎项目管理的高效性，还决定了产品的创新能力与市场响应速度。</p><h2>一、汽车行业的研发管理痛点</h2><p><strong>1）跨部门协同难：软硬件、算法与系统工程的协作壁垒</strong></p><p>汽车行业的研发项目通常涉及多个部门，包括硬件与软件开发、算法开发与嵌入式系统、汽车电子与机械工程等。研发过程中也经常出现协作壁垒：</p><ul><li>不同部门使用不同的工具，信息割裂</li><li>项目进度、资源和风险很难实时同步</li><li>没有统一的协作平台，沟通对接耗时，误差频发</li></ul><p><strong>2）研发流程冗长，难以管控项目进度与交付质量</strong></p><p>汽车项目周期长、涉及面广，管理难度大，极易出现：</p><ul><li>项目阶段多、环节复杂，进度不易把控</li><li>各环节之间信息不流通，导致反复修改与返工</li><li>团队间依赖性强，缺乏透明的资源与任务管理体系</li></ul><p><strong>3）高合规要求：标准化与流程化必须强制执行</strong></p><p>汽车行业对质量与合规要求高，选型时特别看重以下几点：</p><ul><li>是否能满足汽车行业标准（如 ASPICE、ISO 26262 等）</li><li>是否支持流程自动化与标准化</li><li>是否支持全生命周期管理，从概念设计到产品交付</li></ul><p><strong>4）技术创新的需求：研发测试一体化、缺陷追溯与数据驱动决策</strong></p><p>随着自动驾驶、智能座舱等技术的发展，研发过程需要快速迭代和持续创新：</p><ol><li>迭代周期短，需求变更频繁</li><li>测试与研发不能“割裂”，需要紧密对接</li><li>项目进展与质量评估要依赖数据驱动，避免“盲目决策”</li></ol><h2>二、ONES 在汽车行业的解题思路</h2><p><strong>1）支持跨部门、跨团队的协同与资源透明化</strong></p><p>汽车研发涉及的跨部门、跨团队协作多，平台要做到：</p><ul><li>跨部门数据和任务的统一管理与协作</li><li>项目任务、工时、进度实时可视化</li><li>支持多团队的灵活配置，避免信息割裂与重复劳动</li></ul><p><strong>2）流程与标准化：支持 ASPICE 和 ISO 26262 等标准的执行与追溯</strong></p><p>汽车行业的研发管理平台，必须支撑行业标准：</p><ul><li>ASPICE、ISO 26262等流程标准的数字化落地</li><li>通过平台实现流程自动化与标准化管理</li><li>支持从设计、研发到交付的全生命周期管控，确保质量与合规</li></ul><p><strong>3）支撑研发与测试一体化：缺陷回溯与测试用例关联</strong></p><p>平台需要支持：</p><ul><li>研发任务与测试用例的无缝对接</li><li>缺陷管理与需求、任务、测试的闭环回溯</li><li>数据驱动决策，减少人工依赖和项目误差</li></ul><p><strong>4）技术创新与数据化管理：从需求到交付的全程追溯与可视化</strong></p><p>随着技术快速发展，平台应支撑：</p><ul><li>需求变更的高效管理与追溯</li><li>项目全生命周期的实时跟踪与透明化</li><li>数据分析和决策支持功能，帮助团队在快速变化的技术环境中保持竞争力</li></ul><h2>三、汽车行业客户证言</h2><p>以下内容来自 ONES 汽车行业客户证言。若你正在评估研发管理平台，可将这些证言作为参考样本，对照自身的跨部门协作、流程标准化、缺陷管理与数据决策需求进行判断。</p><p><strong>四维智联：中国智能汽车产业链的核心技术供应商</strong></p><p>ONES 系统助力我们完成项目、需求、缺陷以及质量管理的标准化、线上化管理，规范了从需求、研发到交付的全流程，提升了团队间的协作效率，协助建立了统一指标管理体系。</p><p><strong>新阳荣乐：服务一汽、东风、长安、北汽制造、北汽新能源等龙头厂</strong></p><p>ONES 助力新阳荣乐落地 ASPICE 认证的项目管理过程，提供对应的项目管理指导、优化内部业务流程管理，构建项目与业务一体化平台。</p><p><strong>易捷特：由东风汽车、雷诺、日产合资成立的新能源汽车企业</strong></p><p>易捷特通过引入 ONES 研发项目管理系统，实现了跨部门、跨地域的工单统一管理与流程自动化，显著提升协同效率和项目管理专业性，支撑高质量交付与客户满意度提升。</p><p><strong>众鸿科技：中国智能网联汽车创新 TOP 50</strong></p><p>技术总监 林先生：与 ONES 合作后，我们的智能座舱、舱泊一体系统研发流程实现标准化管控，完美适配 ASPICE 体系与功能安全要求，跨研发中心协作效率提升 40%；其全流程管理能力助力我们加速国产芯片适配与技术创新，产品迭代周期缩短 30%，为深耕汽车电子赛道、实现平台化量产提供了坚实支撑。</p><p><strong>佳因特：超 15 万台充电桩销往全球 60 个国家</strong></p><p>ONES 支撑佳因特全产品线研发项目管理，确保项目资源合理分配，团队高效协同。</p>]]></description></item><item>    <title><![CDATA[AI产品需求分析入门 AIAgent研究 ]]></title>    <link>https://segmentfault.com/a/1190000047606376</link>    <guid>https://segmentfault.com/a/1190000047606376</guid>    <pubDate>2026-02-11 19:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在AI技术快速渗透各行各业的今天，AI产品早已走出实验室，成为解决实际问题、提升效率的核心载体——从日常使用的AI聊天机器人、图片生成工具，到企业级的智能风控、数据分析系统，背后都离不开专业的需求分析。不同于传统互联网产品，AI产品需求分析既要兼顾“用户价值”，更要适配“技术可行性”，是连接用户需求、业务目标与AI技术的核心桥梁。对于新手而言，无需一开始陷入复杂的算法细节，先掌握核心逻辑与基础步骤，就能快速入门AI产品需求分析。</p><h2>一、先搞懂：什么是AI产品需求分析？</h2><p>简单来说，AI产品需求分析的核心是：<strong>明确“用AI技术解决什么问题”“解决给谁看”“怎么用技术落地”“落地后如何衡量效果”</strong> ，最终将模糊的用户痛点、业务诉求，转化为可落地、可衡量、符合AI技术特性的产品需求。</p><p>与传统互联网产品需求分析相比，它有两个核心差异，也是新手最需要注意的点：</p><ul><li>传统产品侧重“功能实现”，比如“做一个一键付款功能”，技术路径清晰；AI产品侧重“效果达成”，比如“做一个能识别垃圾类别的AI工具”，核心是让AI的识别准确率、响应速度达到可用标准，技术迭代空间更大。</li><li>传统产品需求可“一步到位”，功能上线后基本满足需求；AI产品需求是“迭代演进”的，比如AI聊天机器人的话术流畅度、理解准确率，需要通过数据反馈、模型优化，逐步逼近理想效果，无法一蹴而就。</li></ul><p>一句话总结：AI产品需求分析，是“以问题为核心，以技术为支撑，以效果为目标”的系统性思考过程，而非单纯罗列功能。</p><h2>二、入门核心：AI产品需求分析的3个基本原则</h2><p>新手入门，无需追求复杂方法，先守住3个基本原则，就能避开80%的坑，确保需求不偏离方向。</p><h3>1. 可行性优先：拒绝“技术空想”</h3><p>AI产品的核心是“技术落地”，再美好的需求，若当前技术无法实现，就是无效需求。新手最容易犯的错误，就是过度追求“炫酷功能”，比如“做一个能完全替代人类的AI客服”，忽略了当前AI的理解能力、情感表达能力的局限性。</p><p>正确的做法是：先判断需求的技术可行性——比如，当前AI能否实现核心功能（如识别、生成、预测）？需要多少数据支撑？落地成本（人力、算力）是否可控？若暂时无法完全实现，可拆解为“最小可行需求”，比如先实现“AI识别常见垃圾类别（准确率≥80%）”，再逐步扩展类别、提升准确率。</p><h3>2. 价值导向：AI是“工具”，不是“噱头”</h3><p>所有AI产品的需求，都必须围绕“解决实际问题、创造价值”展开，要么提升效率，要么降低成本，要么优化体验，拒绝为了“AI”而“AI”。</p><p>比如，同样是“AI图片生成工具”，针对普通用户的需求是“简单输入文字，就能生成好看的图片，无需专业设计能力”（优化体验）；针对电商商家的需求是“快速生成商品主图，降低设计成本”（降低成本）。明确核心价值，才能让需求更聚焦，避免功能冗余。</p><h3>3. 数据驱动：AI的“燃料”的是数据</h3><p>AI模型的训练、优化，离不开大量高质量数据——比如AI识别垃圾，需要收集成千上万张不同垃圾的图片，标注清楚类别，才能训练出可用的模型。因此，在需求分析阶段，就要考虑“数据来源”：数据从哪里来？是否合规？数据质量是否达标？</p><p>比如，做一个“AI识别手写文字”的需求，若无法获取足够多、覆盖不同字体、不同书写场景的手写文字数据，即使技术路径可行，最终的识别效果也会很差，需求落地后也无法满足用户需求。</p><h2>三、新手实操：AI产品需求分析的5个基础步骤</h2><p>掌握原则后，跟着这5个步骤走，就能快速完成一次基础的AI产品需求分析，从“空想”走向“落地”。</p><h3>步骤1：明确场景与用户，找准核心痛点</h3><p>任何产品需求的起点，都是“谁在什么场景下，遇到了什么问题”，AI产品也不例外。新手要避免“泛泛而谈”，比如不要说“做一个AI工具”，而要具体到场景和用户。</p><p>举例：用户是“中小电商商家”，场景是“每天需要生成10张商品主图，自己不会设计，找设计师成本高、周期长”，核心痛点是“商品主图生成效率低、成本高”。</p><p>这一步的关键是：聚焦“具体场景、具体用户”，拒绝模糊化描述，只有找准痛点，后续的AI需求才能有的放矢。</p><h3>步骤2：拆解需求，明确AI的核心作用</h3><p>找到痛点后，不要直接想“用AI怎么做”，而是先拆解需求，区分“哪些部分需要AI实现，哪些部分用传统功能实现即可”——AI只负责解决“传统技术无法高效解决”的问题，比如识别、生成、预测等，无需所有功能都依赖AI。</p><p>继续上面的例子，需求拆解为：① 生成商品主图；② 支持自定义商品类别、背景风格；③ 生成后可简单编辑；④ 快速导出。其中，“生成商品主图”是核心，需要AI实现（文生图、图生图）；“自定义风格、简单编辑、导出”是辅助功能，用传统产品功能即可实现。</p><p>这一步的关键是：聚焦“AI的核心价值”，不要过度依赖AI，避免增加技术复杂度和落地成本。</p><h3>步骤3：明确效果指标，让需求可衡量</h3><p>AI产品的需求，必须有“可衡量的效果指标”，否则无法判断需求是否落地、是否满足用户需求。新手最容易忽略这一点，只说“做一个AI识别工具”，却不说“识别准确率要达到多少”“响应速度要多久”。</p><p>常见的AI效果指标有：准确率（比如垃圾识别准确率≥80%）、响应速度（比如AI生成图片≤10秒/张）、召回率（比如智能推荐的召回率≥70%）、用户满意度（比如AI客服的用户满意度≥85%）。</p><p>继续举例，明确效果指标：AI生成商品主图，准确率≥85%（与商品实际外观匹配），响应速度≤8秒/张，支持3种以上背景风格，用户可直接使用的图片占比≥70%。</p><p>这一步的关键是：指标要具体、可量化，避免“大概”“差不多”，这样后续技术开发、测试才有明确的标准。</p><h3>步骤4：评估技术可行性与落地成本</h3><p>这是AI产品需求分析的核心步骤，也是区别于传统产品的关键。新手可以从3个维度评估，无需深入了解算法细节，只需和技术同学简单沟通即可：</p><ul><li>技术路径：当前AI技术能否实现核心需求？比如，商品主图生成，可用成熟的文生图模型（如Stable Diffusion、即梦AI的生成模型），技术路径可行。</li><li>数据支撑：是否有足够的高质量数据？比如，商品主图生成，需要收集不同类别的商品图片、背景图片，标注清楚类别、风格，若数据不足，可考虑使用公开数据集、外包标注。</li><li>落地成本：人力（算法工程师、数据标注师）、算力（模型训练、推理需要的服务器资源）、时间（开发周期）是否可控？比如，中小团队做商品主图生成工具，可基于成熟模型微调，降低开发成本和周期。</li></ul><p>若评估后发现不可行，可调整需求，比如降低效果指标、拆解为更小的需求，避免盲目推进。</p><h3>步骤5：输出需求文档，明确边界与迭代计划</h3><p>需求分析完成后，需要将思考的结果整理为需求文档（PRD），传递给技术、测试等团队，明确需求的边界、优先级和迭代计划。新手的需求文档无需过于复杂，核心包含3部分内容：</p><ul><li>需求概述：明确场景、用户、核心痛点和需求目标，让团队快速了解需求背景。</li><li>核心需求与效果指标：详细说明AI核心功能、效果指标、辅助功能，明确需求的优先级（哪些必须实现，哪些可后续迭代）。</li><li>迭代计划：AI产品无法一步到位，需明确迭代节奏，比如V1版本实现核心功能（准确率≥85%），V2版本提升准确率（≥90%）、增加更多风格，V3版本优化编辑功能。</li></ul><p>这一步的关键是：文档清晰、简洁，明确“做什么、不做什么、做到什么程度、分几步做”，避免团队理解偏差。</p><h2>四、新手避坑：AI产品需求分析的4个常见误区</h2><p>入门阶段，只要避开这4个误区，就能少走很多弯路，让需求更具落地性。</p><ul><li>误区1：过度追求技术炫酷，忽视用户需求。比如，盲目追求“多模态生成”“大模型应用”，却没考虑用户是否真的需要，导致产品上线后无人使用。记住：AI是工具，用户需要的是“解决问题”，不是“炫酷技术”。</li><li>误区2：忽视数据问题，认为“技术能解决一切”。比如，做AI识别需求，却没考虑数据来源、数据质量，导致模型训练效果差，无法落地。记住：数据是AI的燃料，没有高质量数据，再强的算法也无用。</li><li>误区3：需求太模糊，没有可衡量的指标。比如，只说“做一个AI客服，能回答用户问题”，却不说“回答准确率、响应速度”，导致技术开发没有标准，测试无法判断效果。</li><li>误区4：期望一步到位，不考虑迭代。比如，要求AI产品上线就达到“完美效果”，忽视了AI模型需要数据反馈、持续优化的特性，导致需求落地周期过长，甚至失败。</li></ul><h2>五、入门建议：新手如何快速提升AI产品需求分析能力？</h2><p>AI产品需求分析能力，不是一蹴而就的，新手可以从3个方面入手，快速提升，循序渐进。</p><ul><li>多体验：多使用各类AI产品（如即梦AI、ChatGPT、Midjourney、剪映AI），思考它们的需求场景、核心功能、效果指标，拆解它们的需求逻辑——比如，使用即梦AI的视频生成功能，思考“它的用户是谁？核心痛点是什么？效果指标如何设计？”。</li><li>多实践：从小需求入手，尝试完成一次完整的需求分析，比如“做一个AI识别宠物类别的工具”，按照前面的5个步骤，拆解需求、明确指标、评估可行性、输出需求文档，哪怕是简单的练习，也能快速积累经验。</li><li>多沟通：多和技术同学沟通，了解AI技术的基本逻辑、落地难点（比如数据标注、模型微调的成本），避免提出不可行的需求；多和用户沟通，了解真实痛点，避免“自嗨式需求”。</li></ul><h2>六、总结</h2><p>AI产品需求分析入门，核心不是掌握复杂的算法知识，而是建立“以问题为核心、以技术为支撑、以效果为目标”的思考方式——先找准具体场景和用户痛点，再拆解需求、明确效果指标，评估技术可行性，最后通过迭代逐步落地。</p><p>对于新手而言，不要急于求成，先守住“可行性、价值导向、数据驱动”3个原则，避开常见误区，多体验、多实践、多沟通，就能快速掌握AI产品需求分析的基础逻辑，逐步成长为合格的AI产品需求分析师。</p><p>记住：AI产品的核心是“解决问题”，需求分析的核心是“让技术落地，创造价值”，这也是所有AI产品需求分析的底层逻辑。</p>]]></description></item><item>    <title><![CDATA[Flask 入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047606382</link>    <guid>https://segmentfault.com/a/1190000047606382</guid>    <pubDate>2026-02-11 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 库的概览与核心价值</h2><p>想象一下,在搭建一个 Web 应用时,如果需要同时处理路由、模板、数据库、表单验证、用户认证等数十个复杂功能,就像试图在一天内盖好一栋摩天大楼——不仅容易迷失方向,还可能因为过度设计而拖垮开发效率。<code>Flask</code>正是为解决这个"选择困难症"而生的轻量级框架。</p><p>Flask被称为"微框架"(Microframework),它的核心哲学是"保持简单,按需扩展"。与Django这样自带全套装备的"全栈框架"不同,Flask只提供Web开发最基础的功能:路由分发和模板渲染,其他功能则通过丰富的扩展生态系统来实现。这种设计让开发者能够根据项目需求自主选择工具链,就像搭积木一样灵活组装自己的技术栈。</p><p>Flask的不可替代性体现在三个方面:极低的学习曲线让初学者能快速上手,高度的扩展性支持项目从原型到生产环境的平滑演进,而简洁的代码结构则为团队协作和代码维护提供了良好基础。无论是构建简单的API服务、个人博客,还是复杂的企业级应用,Flask都能提供一个优雅而高效的起点。</p><h2>2. 环境搭建与 "Hello, World"</h2><h3>安装说明</h3><p>安装Flask前,强烈建议先创建虚拟环境以隔离项目依赖:</p><pre><code class="bash"># 创建虚拟环境
python3 -m venv venv

# 激活虚拟环境
# macOS/Linux:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 安装Flask
pip install Flask</code></pre><p>Flask会自动安装以下核心依赖:</p><ul><li><code>Werkzeug</code>: WSGI工具包,处理HTTP请求和响应</li><li><code>Jinja2</code>: 模板引擎,用于生成动态HTML</li><li><code>Click</code>: 命令行工具,提供<code>flask</code>命令</li><li><code>MarkupSafe</code>: 自动转义HTML,防止XSS攻击</li><li><code>ItsDangerous</code>: 数据签名工具,保护session安全</li></ul><h3>最简示例</h3><p>创建一个<code>app.py</code>文件,写入以下代码:</p><pre><code class="python">from flask import Flask

# 创建Flask应用实例
app = Flask(__name__)

# 使用装饰器定义路由
@app.route('/')
def hello_world():
    return '&lt;p&gt;Hello, World!&lt;/p&gt;'

if __name__ == '__main__':
    app.run(debug=True)</code></pre><h3>逐行解释</h3><ul><li><code>from flask import Flask</code>: 导入Flask核心类,这是构建应用的起点</li><li><code>app = Flask(__name__)</code>: 创建应用实例。<code>__name__</code>参数帮助Flask定位模板和静态文件目录</li><li><code>@app.route('/')</code>: 路由装饰器,告诉Flask当用户访问根路径(<code>/</code>)时调用下面的函数</li><li><code>def hello_world():</code>: 视图函数,处理请求并返回响应内容</li><li><code>return '&lt;p&gt;Hello, World!&lt;/p&gt;'</code>: 返回HTML字符串,Flask会自动将其转换为HTTP响应</li><li><code>if __name__ == '__main__':</code>: 确保只有在直接运行脚本时才启动服务器</li><li><code>app.run(debug=True)</code>: 启动开发服务器。<code>debug=True</code>开启调试模式,代码修改后自动重载,并提供错误调试页面</li></ul><h3>运行结果</h3><p>在终端执行:</p><pre><code class="bash">flask --app app run
# 或者
python app.py</code></pre><p>服务器启动后,访问 <a href="https://link.segmentfault.com/?enc=hk7%2BUgtW7uvLend9USZFwQ%3D%3D.W7rltI4Hpti6ClGPaPcVUygAlfqNhM7bvP6dDHiHQqI%3D" rel="nofollow" target="_blank">http://127.0.0.1:5000/</a> 即可看到 "Hello, World!" 页面。</p><h2>3. 核心概念解析</h2><p>Flask的三大核心概念:应用实例、路由系统和请求上下文,它们共同构成了Web应用的骨架。</p><h3>应用实例(Application Instance)</h3><p>应用实例(<code>app = Flask(__name__)</code>)是Flask应用的中心,负责管理路由、配置和扩展。它通过<code>__name__</code>参数确定模块位置,以便正确查找<code>templates</code>和<code>static</code>目录。可以将应用实例理解为一个"中央指挥官",协调所有组件协同工作。</p><h3>路由系统(Routing)</h3><p>路由使用装饰器<code>@app.route()</code>将URL路径映射到视图函数:</p><pre><code class="python"># 基础路由
@app.route('/about')
def about():
    return 'About Page'

# 动态路由
@app.route('/user/&lt;username&gt;')
def show_user(username):
    return f'User: {username}'

# 类型约束路由
@app.route('/post/&lt;int:post_id&gt;')
def show_post(post_id):
    return f'Post ID: {post_id}'

# 多HTTP方法支持
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        return 'Processing login...'
    return 'Login form'</code></pre><p>动态路由中的<code>&lt;username&gt;</code>和<code>&lt;int:post_id&gt;</code>是URL转换器,前者匹配任意字符串,后者只匹配整数。Flask还支持<code>float</code>、<code>path</code>(包含斜杠)、<code>uuid</code>等转换器。</p><h3>请求上下文(Request Context)</h3><p>请求上下文包含两个关键代理对象:<code>request</code>和<code>session</code>。它们允许在视图函数中访问请求数据和会话信息,无需显式传递参数。</p><pre><code class="python">from flask import request, session

# 获取查询参数: /search?q=keyword
@app.route('/search')
def search():
    keyword = request.args.get('q', '')
    return f'Searching for: {keyword}'

# 获取表单数据
@app.route('/submit', methods=['POST'])
def submit():
    username = request.form.get('username')
    return f'Username: {username}'

# 获取JSON数据
@app.route('/api/data', methods=['POST'])
def api_data():
    data = request.get_json()
    return jsonify(data)

# 使用session存储用户状态
@app.route('/set_session')
def set_session():
    session['user_id'] = 123
    return 'Session set'</code></pre><h3>概念关系图</h3><pre style="display:none;"><code class="mermaid">graph TD
    A[Flask应用实例] --&gt; B[路由系统]
    A --&gt; C[配置管理]
    A --&gt; D[扩展注册]
    B --&gt; E[视图函数]
    E --&gt; F[请求上下文]
    F --&gt; G[request对象]
    F --&gt; H[session对象]
    E --&gt; I[响应生成]
    I --&gt; J[字符串/JSON/模板]
    D --&gt; K[数据库扩展]
    D --&gt; L[表单验证扩展]
    D --&gt; M[认证扩展]</code></pre><h2>4. 实战演练:构建一个待办事项API</h2><p>让我们通过一个完整的迷你项目来掌握Flask的核心功能。我们将构建一个简单的待办事项管理API,支持增删改查(CRUD)操作。</p><h3>需求分析</h3><p>我们需要创建一个RESTful API,允许用户:</p><ol><li>获取所有待办事项</li><li>创建新待办事项</li><li>更新待办事项状态</li><li>删除待办事项</li></ol><p>数据存储在内存中(列表),适合快速原型开发。</p><h3>方案设计</h3><p>选择Flask的以下功能:</p><ul><li>路由系统:定义API端点</li><li><code>request</code>对象:解析JSON请求体</li><li><code>jsonify</code>:返回JSON格式响应</li><li>动态路由:处理特定ID的待办事项</li><li>HTTP方法:GET/POST/PUT/DELETE对应CRUD操作</li></ul><h3>代码实现</h3><p>创建<code>todo_api.py</code>:</p><pre><code class="python">from flask import Flask, request, jsonify

app = Flask(__name__)

# 内存数据库
todos = [
    {'id': 1, 'title': 'Learn Flask', 'completed': False},
    {'id': 2, 'title': 'Build API', 'completed': False}
]
next_id = 3

# 获取所有待办事项
@app.route('/api/todos', methods=['GET'])
def get_todos():
    return jsonify(todos)

# 创建新待办事项
@app.route('/api/todos', methods=['POST'])
def create_todo():
    global next_id
    data = request.get_json()
    
    if not data or 'title' not in data:
        return jsonify({'error': 'Title is required'}), 400
    
    todo = {
        'id': next_id,
        'title': data['title'],
        'completed': data.get('completed', False)
    }
    todos.append(todo)
    next_id += 1
    
    return jsonify(todo), 201

# 更新待办事项
@app.route('/api/todos/&lt;int:todo_id&gt;', methods=['PUT'])
def update_todo(todo_id):
    todo = next((t for t in todos if t['id'] == todo_id), None)
    
    if not todo:
        return jsonify({'error': 'Todo not found'}), 404
    
    data = request.get_json()
    todo['title'] = data.get('title', todo['title'])
    todo['completed'] = data.get('completed', todo['completed'])
    
    return jsonify(todo)

# 删除待办事项
@app.route('/api/todos/&lt;int:todo_id&gt;', methods=['DELETE'])
def delete_todo(todo_id):
    global todos
    todo = next((t for t in todos if t['id'] == todo_id), None)
    
    if not todo:
        return jsonify({'error': 'Todo not found'}), 404
    
    todos = [t for t in todos if t['id'] != todo_id]
    return jsonify({'message': 'Todo deleted'})

if __name__ == '__main__':
    app.run(debug=True)</code></pre><h3>运行说明</h3><ol><li><p>启动服务器:</p><pre><code class="bash">python todo_api.py</code></pre></li><li>使用curl或Postman测试API:</li></ol><pre><code class="bash"># 获取所有待办事项
curl http://127.0.0.1:5000/api/todos

# 创建新待办事项
curl -X POST http://127.0.0.1:5000/api/todos \
  -H "Content-Type: application/json" \
  -d '{"title": "Deploy to production"}'

# 更新待办事项
curl -X PUT http://127.0.0.1:5000/api/todos/1 \
  -H "Content-Type: application/json" \
  -d '{"completed": true}'

# 删除待办事项
curl -X DELETE http://127.0.0.1:5000/api/todos/1</code></pre><h3>结果展示</h3><p>这个API完美展示了Flask的核心能力:</p><ul><li>清晰的路由定义(<code>/api/todos</code>, <code>/api/todos/&lt;id&gt;</code>)</li><li>HTTP方法处理(GET/POST/PUT/DELETE)</li><li>JSON请求解析(<code>request.get_json()</code>)</li><li>错误处理和状态码返回(400/404)</li><li>动态路由参数(<code>&lt;int:todo_id&gt;</code>)</li></ul><h2>5. 最佳实践与常见陷阱</h2><h3>常见错误及规避方法</h3><h4>错误1: 直接使用<code>app.run()</code>部署到生产环境</h4><pre><code class="python"># ❌ 错误做法
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)  # 仅适合开发环境</code></pre><p>Flask内置服务器性能有限且不安全,生产环境应使用Gunicorn或uWSGI:</p><pre><code class="bash"># ✅ 正确做法: 使用Gunicorn部署
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app</code></pre><h4>错误2: 硬编码敏感信息</h4><pre><code class="python"># ❌ 错误做法
app.config['SECRET_KEY'] = 'my-secret-key-123'
app.config['DATABASE_URI'] = 'postgresql://user:password@localhost/db'</code></pre><p>使用环境变量或配置文件:</p><pre><code class="python"># ✅ 正确做法
import os

app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY') or 'dev-key'
app.config['DATABASE_URI'] = os.environ.get('DATABASE_URI')

# 或者使用配置文件
# config.py
class Config:
    SECRET_KEY = os.environ.get('SECRET_KEY')
    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URI')

# app.py
from config import Config
app.config.from_object(Config)</code></pre><h4>错误3: 忘记设置<code>SECRET_KEY</code>导致session无法使用</h4><pre><code class="python"># ❌ 错误做法
@app.route('/login')
def login():
    session['user_id'] = 1  # 会报错: RuntimeError: The session is unavailable
    return 'Logged in'</code></pre><pre><code class="python"># ✅ 正确做法
app = Flask(__name__)
app.secret_key = 'your-secret-key-here'  # 生产环境应从环境变量读取

@app.route('/login')
def login():
    session['user_id'] = 1
    return 'Logged in'</code></pre><h3>最佳实践建议</h3><p><strong>1. 使用虚拟环境隔离依赖</strong></p><pre><code class="bash"># 创建并激活虚拟环境
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt</code></pre><p><strong>2. 生成依赖清单</strong></p><pre><code class="bash">pip freeze &gt; requirements.txt</code></pre><p><code>requirements.txt</code>文件示例:</p><pre><code>Flask==3.0.0
Werkzeug==3.0.1
Jinja2==3.1.2</code></pre><p><strong>3. 项目结构组织</strong></p><p>对于小型项目,建议采用以下结构:</p><pre><code>myproject/
├── app.py              # 主应用文件
├── requirements.txt     # 依赖清单
├── config.py           # 配置文件
├── templates/          # 模板目录
│   └── index.html
└── static/             # 静态文件
    ├── css/
    └── js/</code></pre><p>对于大型项目,使用蓝图(Blueprint)模块化:</p><pre><code>myproject/
├── app.py
├── requirements.txt
├── blueprints/
│   ├── auth.py
│   ├── api.py
│   └── main.py
└── templates/</code></pre><p><strong>4. 启用调试模式注意事项</strong></p><p>开发环境可启用调试模式:</p><pre><code class="python">app.run(debug=True)</code></pre><p>但生产环境必须关闭:</p><pre><code class="python">app.run(debug=False)  # 或不指定,默认为False</code></pre><p>调试模式会暴露敏感信息并允许在浏览器中执行任意Python代码,存在严重安全风险。</p><h2>6. 进阶指引</h2><p>Flask的简洁性不仅体现在核心功能上,更体现在其强大的扩展能力。当你的项目需要更复杂的功能时,以下扩展值得关注:</p><p><strong>数据库集成</strong></p><ul><li><code>Flask-SQLAlchemy</code>: 提供ORM功能,简化数据库操作</li><li><code>Flask-Migrate</code>: 数据库迁移工具,管理表结构变更</li></ul><p><strong>表单处理与验证</strong></p><ul><li><code>Flask-WTF</code>: 集成WTForms,提供表单验证和CSRF保护</li></ul><p><strong>用户认证与授权</strong></p><ul><li><code>Flask-Login</code>: 管理用户会话和认证状态</li><li><code>Flask-Security</code>: 提供完整的认证、角色管理和密码加密</li></ul><p><strong>API开发</strong></p><ul><li><code>Flask-RESTful</code>: 快速构建RESTful API</li><li><code>Flask-Marshmallow</code>: 序列化/反序列化数据</li></ul><p><strong>任务队列与异步处理</strong></p><ul><li><code>Celery</code>: 处理耗时任务(如发送邮件、图片处理)</li><li><code>Flask-Celery-Helper</code>: 简化Celery与Flask的集成</li></ul><h3>学习路径建议</h3><ol><li><strong>掌握基础</strong>(当前阶段): 理解路由、请求/响应、模板渲染</li><li><strong>扩展技能</strong>: 学习3-5个常用扩展,构建功能完整的应用</li><li><strong>深入原理</strong>: 研究Flask的上下文机制、信号系统、中间件</li><li><strong>生产部署</strong>: 掌握Gunicorn/Nginx部署、Docker容器化</li><li><strong>性能优化</strong>: 了解缓存策略、数据库优化、异步处理</li></ol><h3>学习资源</h3><ul><li><strong>官方文档</strong>: <a href="https://link.segmentfault.com/?enc=DaN0cAGBi3abTuNq%2FsFX7A%3D%3D.9aViwMiXJ9BQ4cs5Ru5Zy8NvzlYVLEhRFlr%2BAk9FZefv7thgQNK6Lb6SHzbc%2FEFN" rel="nofollow" target="_blank">https://flask.palletsprojects.com/</a> (最权威的学习资源)</li><li><strong>中文文档</strong>: <a href="https://link.segmentfault.com/?enc=Wxaifp%2FmdVrbtP8xEUoAPA%3D%3D.kQCDZzGvbErCXyAYwAdXMuQgJ9F7YDEr%2BVWxxNPtIsw%3D" rel="nofollow" target="_blank">https://flask.github.net.cn/</a> (适合中文读者)</li><li><strong>GitHub仓库</strong>: <a href="https://link.segmentfault.com/?enc=ZwBpTWlZbWaIQtsF68Msbw%3D%3D.fmTH4EzoYmKNZy66tYmJuuE3EPFvuN6V%2B2%2B2V3Ub6QOA%2FAsE6MmfkVpQDvZ9lzU9" rel="nofollow" target="_blank">https://github.com/pallets/flask</a> (源码和Issue讨论)</li><li><strong>Stack Overflow</strong>: 使用<code>flask</code>标签搜索问题和解决方案</li></ul><p>Flask的学习曲线平缓,但要精通它需要实践和耐心。建议从简单项目开始,逐步引入新功能和技术,在实践中深化理解。记住,Flask的力量不在于它提供了什么,而在于它不限制你做什么——这正是"微框架"哲学的精髓所在。</p>]]></description></item><item>    <title><![CDATA[AI推理：如何实现吞吐翻倍、时延降90%与GPU资源节省26%？ 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606039</link>    <guid>https://segmentfault.com/a/1190000047606039</guid>    <pubDate>2026-02-11 18:09:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：马国忠</p><h2>引言：AI规模化落地，推理系统面临全新挑战</h2><p>﻿</p><p>全球领先的市场研究和咨询公司IDC预测，到2028年，75%的新 AI 工作负载将实现容器化，从而显著提升模型与工作负载更新的速度、一致性与安全性。容器化技术将成为 AI 推理时代的“默认交付形态”。当前随着大模型技术快速演进与业务场景的深度融合，<strong>AI业务对推理基础设施的需求呈现爆发式增长</strong>。在早期小流量场景下，手动部署与定制化方案尚可应对；然而当模型规模、并发请求与业务复杂度攀升至新高度时，传统推理系统在以下四个主要方面逐渐暴露出瓶颈。</p><ol><li><strong>稳定性不足</strong>：</li></ol><p>◦<strong>单点故障风险</strong>：手动部署的静态架构缺乏多副本与故障自愈机制，单节点宕机易引发服务中断；</p><p>◦<strong>负载不均衡</strong>：缺乏智能流量调度，高并发时部分节点过载导致响应延迟，低负载时资源闲置；</p><p>◦<strong>故障恢复滞后</strong>：依赖人工排查与重启，恢复周期长，影响业务连续性。</p><p>2.<strong>资源利用率低下</strong>：</p><p>◦<strong>静态资源分配</strong>：固定规模的GPU集群无法适应流量波动，高峰时段资源争抢，低谷期GPU闲置率超40%；</p><p>◦<strong>缺乏弹性机制</strong>：无法根据请求队列长度、KV缓存利用率等指标动态扩缩容，导致周级别GPU卡时浪费超5000+。</p><p>3.<strong>推理性能瓶颈</strong>：</p><p>◦<strong>混合请求排队</strong>：长、短文请求混合处理时，短文首字时延（TTFT）因排队激增90%以上；</p><p>◦<strong>缓存复用率低</strong>：多副本场景下相同前缀请求随机调度，重复计算导致KV缓存命中率不足60%；</p><p>◦<strong>硬件拓扑未优化</strong>：跨交换机部署引发传输延迟，人工调整拓扑亲和性成本高且易出错。</p><p>4.<strong>定制成本高昂</strong>：</p><p>◦<strong>多引擎适配复杂</strong>：vLLM、SGLang等引擎需独立开发接入层，版本迭代与兼容性维护成本攀升；</p><p>◦<strong>运维依赖人工</strong>：从部署、监控到故障修复全流程手动操作，人力成本占比超30%，且易引入人为错误。</p><p>﻿</p><p>为此，京东云结合实际业务需求与技术趋势，全面拥抱云原生技术栈，积累了丰富的云原生与高性能推理经验。自主研发了新一代<strong>云原生AI推理框架。</strong> 推动推理系统完成了一次体系化升级，实现了从手动部署到全场景AutoScale，从资源浪费到GPU利用率最大化。</p><p>•<strong>流量高峰自动扩容、低谷自动缩容</strong>，GPU卡时节省高达26%；</p><p>•<strong>智能流量调度与KV缓存复用</strong>，最高提升吞吐124%，首次生成时延TTFT降低90%；</p><p>•<strong>具备多级高可用能力</strong>，支持流量隔离、故障自愈与深度可观测；</p><p>•<strong>模型量化、引擎调优、算子开发</strong>等多项优化，推理引擎单点性能呈现局部领先优势。</p><p>﻿</p><p>详细了解一套生产级分布式AI训练推理平台（JoyBuilder）的云原生改造全纪实。京东云<strong>云原生AI推理框架。</strong></p><p>﻿</p><h2>一、系统架构设计：面向生产级的高性能云原生推理平台</h2><h3>设计理念：</h3><p>我们遵循三大核心设计原则，确保系统长期迭代的灵活性：</p><p>1.<strong>解耦与组合</strong> 各模块尽量松耦合，优先复用开源成熟组件，同时避免被社区绑定，保留核心模块的可替换能力。</p><p>2.<strong>扩展性优先</strong> 支持以插件化方式集成智能调度算法（流量调度、扩缩容决策、Prefix Cache打分等）；容器编排能力可扩展，目前已支持跨机部署与基于角色的调度策略。</p><p>3.<strong>引擎无感接入</strong> 目前可同时支持vLLM、SGLang等主流推理引擎，最终实现任意推理引擎的低成本接入。</p><h3>系统架构：</h3><p>﻿</p><h3>模块详解：</h3><h4><strong>1. 智能流量调度网关</strong></h4><p>基于云原生Gateway API与Inference Extension框架，我们构建了支持多引擎、高可用、高扩展的智能推理网关，支持多层次调度策略：</p><p>﻿</p><table><thead><tr><th><strong>核心能力</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>长短文分桶推理 流量调度</strong></td><td>网关基于高效的<strong>长短文分桶算法</strong>，构建跨模型集群的分流调度，显著<strong>降低短文TTFT（首字生成时延）；</strong></td></tr><tr><td><strong>前缀缓存感知KV复用流量调度</strong></td><td>面向不同模型上下文特征，基于 <strong>HashTrie 算法</strong>构建集群内全局pod的近似前缀缓存画像，支持prefix cache的亲和调度，<strong>有效降低推理 TTFT(首字生成时延)；</strong></td></tr><tr><td><strong>多维负载均衡流量调度</strong></td><td>毫秒级实时采集KV Cache Utilization、Waiting Queue等server load指标，支持<strong>load aware 亲和调度；</strong></td></tr><tr><td><strong>交换机拓扑感知流量调度</strong></td><td>为减少PD group组内KV cache传输的耗时，构建网络拓扑感知，<strong>支持全局最优prefill + 局部最优decode的网络亲和调度</strong>；</td></tr><tr><td><strong>多引擎PD分离流量编排调度</strong></td><td>已支持<strong>vLLM</strong>(PD串行)、<strong>SGLang</strong>(PD异步并行) 异构引擎无差别流量调度</td></tr><tr><td><strong>多LoRA动态流量调度、模型切换的流量调度</strong></td><td>实现不同引擎多LoRA的动态装、卸载，集成LoRA-aware 的动态流量感知调度能力；</td></tr><tr><td><strong>精确的前缀感知Cache-aware流量调度</strong></td><td>实时订阅引擎侧KV Events Metrics，构建精确的前缀缓存画像，实现更高效的prefix cache亲和调度，进一步降低推理TTFT；</td></tr><tr><td><strong>基于时延预测的SLO-aware 流量优先级感知调度</strong></td><td>利用延迟预测来估算每个请求在每个可用节点上的首次生成时间（TTFT）和每个输出令牌时间（TPOT），实现基于时延预测的SLO-aware智能调度；</td></tr></tbody></table><h4><strong>2. 容器编排与资源调度</strong></h4><p>﻿</p><p>•<strong>部署灵活</strong>：PD分离部署，具有Group和Pool两种模式，实现弹性扩缩容与拓扑感知调度。</p><p>•<strong>高可用机制</strong>：多副本部署，避免单点故障。同时支持故障时自动摘流与容器自愈，保障服务持续可用，用户无感知。</p><p>﻿</p><table><thead><tr><th><strong>核心能力</strong></th><th><strong>说明</strong></th></tr></thead><tbody><tr><td><strong>容器编排</strong></td><td>根据推理引擎工作特点，基于容器之间的协作关系（Kimi多容器跨机推理、PD分离架构等），将各个推理引擎容器一定的组织方式部署成一组Pods，并联动服务发现、重启策略。</td></tr><tr><td><strong>GPU资源调度</strong></td><td>自动将各个新创建的Pod调度到具有足够GPU资源的机器节点。</td></tr><tr><td><strong>拓扑感知调度</strong></td><td>Kimi跨机推理， TP16部署的2台机器保证在同一个交换机下；PD分离部署，协作关系的P和D在同一个交换机下。</td></tr><tr><td><strong>优先级调度和抢占</strong></td><td>支持在线服务和离线任务的混合调度，高优的在线服务可以抢占低优任务的GPU资源。</td></tr></tbody></table><h4><strong>3. 系统稳定性与可观测</strong></h4><p>•集成流量镜像、全链路告警与主备值班协同机制。</p><p>•通过网关大盘、调度模块监控、模型性能面板等多层次观测体系，实现问题快速发现与定位。</p><p>﻿</p><p>﻿</p><h4>4. 引擎优化与性能突破</h4><p>针对MoE、多模态等模型特点，通过算子优化、引擎调优与量化等手段，在多项关键性能指标上实现行业领先。</p><p>﻿</p><h2>二、关键场景落地与收益量化</h2><h4>1. 长短文混合调度</h4><p><strong>问题</strong>：长、短文请求混合排队时，短文TTFT急剧上升，集群吞吐下降。 <strong>方案</strong>：通过长短文分桶与跨集群调度，实现长短文分离处理。</p><p><strong>收益</strong>（以Kimi-K2与DeepSeek-V3压测为例）：</p><p>•<strong>Kimi-K2</strong>：短文TTFT降低90.97%，吞吐提升124.46%；长文吞吐提升33.89%，集群整体吞吐提升67%。</p><p>•<strong>DeepSeek-V3</strong>：短文TTFT降低79.09%，吞吐提升36.7%；长文吞吐提升14.34%，集群整体吞吐提升21.82%。</p><p>﻿</p><h4>2. KV Cache全局感知的流量调度</h4><p><strong>问题</strong>：多副本场景下相同前缀请求被随机调度，导致每个实例都重复计算并缓存相同前缀。 <strong>方案</strong>：持续刻画更新集群级KV Cache缓存画像，实现前缀匹配的智能路由，KV Cache高效复用。</p><p><strong>收益</strong>：</p><p>•DeepSeek-V3场景下，集群吞吐提升29.9%，首Token时延TTFT降低28.7%；</p><p>•Kimi-K2场景下，KV Cache命中率整体提升20%\~30%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606041" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h4>3. 全场景自动弹性伸缩</h4><p><strong>问题</strong>：夜间或周末的流量低谷期GPU资源闲置严重。 <strong>方案</strong>：通过多种弹性部署模式并基于排队长度与KV使用率等多项指标，实现全场景自动扩缩容。</p><p><strong>收益</strong>：</p><p>•周级别节省GPU卡时5000+，资源利用率提升26%；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606042" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>4. 硬件拓扑亲和调度</h4><p><strong>问题</strong>：跨交换机部署导致性能下降；人工修正部署成本高，维护压力大。 <strong>方案</strong>：</p><p>•通过节点标签与亲和性规则，实现交换机级自动拓扑亲和调度；</p><p>•Router实现按组进行PD配对流量调度。</p><p><strong>收益</strong>：</p><p>•组容器间通信不跨交换机，数据高效传输，全程自动化，无需人工干预，保证服务SLA。<br/>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606043" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4><strong>5. 稳定性与业务连续性</strong></h4><p><strong>问题：</strong> 容器故障后，因分发机制导致持续的客户影响。故障恢复强依赖人工，导致故障时间长，修复难度大。</p><p><strong>方案：</strong> 通过实时健康监测，快速感知故障容器，进行隔离。启动新副本，实现故障自愈。</p><p><strong>收益：</strong></p><p>•实现自动隔离，自动自愈，无需人工干预，节点人力成本，提高用户体验。</p><p>﻿</p><p>﻿</p><h4><strong>6.推理引擎无感接入</strong></h4><p><strong>问题</strong>：多引擎支持成本高，定制化开发量大，维护成本高。 <strong>方案</strong>：构建统一推理引擎调度接入层，支持vLLM、SGLang等不同推理引擎一键接入。</p><p><strong>收益：</strong></p><p>•推理引擎无感快速接入。</p><p>•降低开发与维护成本。</p><p>﻿</p><p>﻿</p><h2>三、收益总结</h2><p>京东云云原生AI推理框架通过多维度调度与系统级优化，显著提升了推理效率与资源利用率。短文与长文吞吐均有大幅增长，首 token 延迟明显降低，并结合自动弹性扩缩容与 KV Cache 感知调度，进一步提升集群吞吐与缓存命中率，同时节省可观的 GPU 卡时成本。在此基础上，引入硬件拓扑亲和调度，实现更高效的自动化部署与调度，降低大规模集群运维压力；配合故障自愈、高可用机制与更精细的可观测体系，使系统运行更加稳定、可控、易排障。通过针对引擎瓶颈的持续优化，不同模型场景下的吞吐能力均得到明显增强。</p><table><thead><tr><th><strong>能力</strong></th><th><strong>量化结果与效益</strong></th></tr></thead><tbody><tr><td>长短文调度</td><td>吞吐：短文提升120%+，长文提升30%+ TTFT：短文降低90%</td></tr><tr><td>自动弹性扩缩容</td><td>GPU卡时：节省GPU卡时约26%</td></tr><tr><td>KV Cache感知调度</td><td>提升KV Cache命中率：增长约20%\~30% TTFT：降低29% 集群吞吐：增长30%</td></tr><tr><td>硬件拓扑亲和调度</td><td>实现自动化部署与调度，降低大规模集群运维成本</td></tr><tr><td>故障自愈与高可用</td><td>自动检测故障、自动恢复故障，减少对人工的依赖，更具可控性</td></tr><tr><td>可观测性</td><td>具备更细致的监控告警体系、提升故障发现和排查效率</td></tr><tr><td>引擎瓶颈优化</td><td>DS-MoE模型吞吐提升9%，多模态模型吞吐最高提升39%</td></tr></tbody></table><h2>四、客户案例</h2><h3>客户背景</h3><p>客户原系统面临AI规模化落地的挑战，在推理系统的稳定性、性能和资源利用率方面遇到了明显瓶颈。京东云通过帮助客户升级至云原生架构，成功改造了其推理系统，实现显著的性能提升和资源节约。见证了新系统如何带来切实的业务效益。</p><h3>解决方案</h3><p>京东云通过<strong>云原生AI推理框架</strong>对客户原78台节点进行逐步云原生改造，在不到一个月时间内从最初的2%切流比率提升到达到40%，实现对用户AI推理系统的云原生重构，助力企业实现高效、稳定、低成本的AI规模化落地。<strong>核心方案</strong>包括：采用<strong>智能流量调度</strong>技术，通过长短文分桶、KV缓存复用及拓扑感知调度；基于流量波动的<strong>弹性扩缩容</strong>机制；<strong>高可用架构</strong>通过多副本部署与故障自愈保障服务连续性；支持vLLM、SGLang等主流引擎的<strong>无感接入</strong>；<strong>硬件拓扑优化</strong>实现跨交换机亲和调度，减少传输延迟。<br/>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606044" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>客户收益</h3><p>﻿</p><p>•<strong>GPU吞吐能力</strong>：切换云原生系统后，GPU吞吐提升幅度达74%。这一增强使客户在高负载情况下依然能够维持高效的模型推理速度。</p><p>•<strong>限流数量</strong>：云原生AI推理框架系统将需要限流的请求显著减少82%，这意味着更多的客户请求在高峰时段得到及时响应，提高了用户体验和满意度。</p><table><thead><tr><th>﻿</th><th><strong>整体</strong></th><th><strong>旧版系统</strong></th><th><strong>云原生系统</strong></th><th><strong>收益</strong></th></tr></thead><tbody><tr><td><strong>机器规模</strong></td><td>78 (100%)</td><td><strong>50 (64%)</strong></td><td><strong>28 (36%)</strong></td><td><strong>-</strong></td></tr><tr><td><strong>请求数量</strong></td><td>36671 (100%)</td><td><strong>17091 (47%)</strong></td><td><strong>19580 (53%)</strong></td><td><strong>-</strong></td></tr><tr><td><strong>GPU吞吐</strong> <strong>(TGS)</strong></td><td>-</td><td><strong>183</strong></td><td><strong>319</strong></td><td><strong>提升74%</strong></td></tr><tr><td><strong>限流数量</strong></td><td>687 ( 1.87%)</td><td><strong>570 (3.3%)</strong></td><td><strong>117 (0.59%)</strong></td><td><strong>减少82%</strong></td></tr><tr><td>备注： 1、数据来源基于Kimi-K2-instruct-0905模型。</td><td> </td><td> </td><td> </td><td> </td></tr></tbody></table><p>客户对于系统的云原生改造表示高度认可：“云原生AI系统的导入，让我们不仅在资源利用上实现了显著的性价比提升，同时在关键业务高峰期的响应能力也大大增强，显著减少了因限流带来的服务瓶颈问题。”</p><p>﻿</p><p>﻿</p><h2>五、未来展望</h2><p>京东云将继续优化<strong>云原生AI推理框架</strong>，致力于为客户提供更智能、高效、稳定的AI基础设施。通过在各个行业和应用场景中的深化应用，我们的客户可以持续依赖这一平台，实现业务的长期可持续发展。</p><p>这个成功案例不仅展示了京东云<strong>云原生AI推理框架</strong>系统的技术优势，也为其他企业提供了一个可借鉴的成功模型，期待更多客户从中获益。</p><p>京东云云原生AI推理框架的研发升级并非一蹴而就。从架构设计、配置调试再到全量上线，每一步都围绕着<strong>业务价值、性能提升与运维提效</strong>展开。我们相信，只有将稳定性、性能、成本三者统筹兼顾的基础设施，才能真正支撑AI业务规模化、可持续地落地与增长。如您有类似场景或技术交流需求，欢迎随时联系我们。</p>]]></description></item><item>    <title><![CDATA[Agent Skills与MCP：一场被误解的"替代战争" 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606050</link>    <guid>https://segmentfault.com/a/1190000047606050</guid>    <pubDate>2026-02-11 18:08:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：牛潇</p><p>在 Agentic AI 快速演进的今天，“Agent Skills 会取代 MCP”或“MCP 已经过时”的声音不绝于耳。这种二元对立的叙事，看似犀利，实则掩盖了智能体架构设计中最本质的真相：<strong>Agent Skills 与 MCP 并非对手，而是搭档</strong>。</p><p>﻿</p><p>前者是人类智慧的结晶——将业务规则、决策逻辑与合规要求，以声明式、可读、可维护的方式注入 AI 代理；后者是技术能力的桥梁——让 AI 能安全、可靠地触达现实世界的数据、工具与系统。一个回答“怎么做才对”，一个解决“能不能做”；一个由产品经理和业务专家驱动，一个由工程师和 SRE 构建。</p><p>﻿</p><p>本文旨在彻底厘清二者的核心差异、适用边界与协同模式。我们将从概念定义出发，深入实现机制，通过多维对比与真实场景剖析，最终给出一套可落地的选择策略与高阶架构范式。无论你是智能体开发者、平台架构师，还是正在规划 AI 原生应用的产品负责人，都能从中获得清晰的判断框架与实践指引。</p><p>﻿</p><p>更重要的是，我们将证明：<strong>真正的智能，不在于选择某一种工具，而在于知道何时用哪一种，以及如何让它们共舞</strong>。</p><p>﻿</p><h2>一、核心概念定义：超越实现的标准</h2><h3>1.1 Model Context Protocol (MCP) - 模型上下文协议</h3><p><strong>本质定义</strong>：MCP是一种<strong>标准化通信协议</strong>，定义了AI模型如何与外部系统建立安全、高效、可审计的双向连接。</p><p><strong>核心特性</strong>：</p><p>•<strong>能力扩展协议</strong>：为AI代理提供访问实时数据、专业工具和企业系统的标准化接口</p><p>•<strong>安全沙箱规范</strong>：在协议层面定义权限控制、数据隔离和操作审计机制</p><p>•<strong>上下文同步机制</strong>：解决模型内部状态与外部世界状态的一致性问题</p><p>•<strong>标准化接口</strong>：通过JSON-RPC或其他标准协议定义请求/响应格式、认证机制和错误处理</p><p>•<strong>服务化架构</strong>：协议设计支持独立部署的服务进程，实现高可用和水平扩展</p><p><strong>架构定位</strong>：MCP解决了 <strong>"能不能做"</strong> 的问题 (Capability)，为AI代理扩展其原始训练数据范围之外的能力边界。</p><h3>1.2 Agent Skills - 代理技能标准</h3><p><strong>本质定义</strong>：Agent Skills是一种<strong>模块化能力封装标准</strong>，通过声明式、配置化的方式定义AI代理在特定场景下的行为规范、决策逻辑和工作流程。</p><p><strong>核心特性</strong>：</p><p>•<strong>流程编排标准</strong>：定义如何将原子操作组合成完整业务流程的标准</p><p>•<strong>上下文感知能力</strong>：标准定义了技能如何根据对话历史和环境动态调整行为</p><p>•<strong>透明可解释性</strong>：决策路径对人类可见，便于理解和修改</p><p>•<strong>轻量级集成</strong>：标准设计支持无服务部署，修改配置即可生效</p><p>•<strong>组合式架构</strong>：支持技能的嵌套、组合和复用</p><p><strong>架构定位</strong>：Agent Skills解决了 <strong>"怎么做才对"</strong> 的问题 (Orchestration)，为AI代理编写符合业务标准和人类期望的行为规范。</p><p>﻿</p><h2>二、设计哲学与架构差异</h2><h3>2.1 MCP：能力导向的设计</h3><p>MCP的核心设计哲学是<strong>能力扩展</strong>。它关注：</p><p>•<strong>原子操作</strong>：如何安全地执行单一、精确的操作</p><p>•<strong>连接管理</strong>：如何高效管理与外部系统的连接</p><p>•<strong>权限边界</strong>：如何在协议层面实现细粒度权限控制</p><p>•<strong>数据标准化</strong>：如何统一不同数据源的格式和语义</p><p>MCP的架构本质上是<strong>服务化</strong>的：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606052" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>2.2 Agent Skills：流程导向的设计</h3><p>Agent Skills的核心设计哲学是<strong>业务价值</strong>。它关注：</p><p>•<strong>决策逻辑</strong>：在特定情境下如何做出正确决策</p><p>•<strong>流程规范</strong>：如何将多个操作组合成符合业务标准的流程</p><p>•<strong>上下文适应</strong>：如何根据环境变化动态调整行为</p><p>•<strong>人类协作</strong>：如何使AI行为可理解、可预测、可修正</p><p>Agent Skills的架构本质上是<strong>声明式</strong>的：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606053" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>三、多维度对比分析</h2><table><thead><tr><th>维度</th><th>Agent Skills</th><th>MCP</th></tr></thead><tbody><tr><td><strong>协议/标准类型</strong></td><td>声明式配置标准 (Declarative)</td><td>通信协议标准 (Imperative)</td></tr><tr><td><strong>核心关注点</strong></td><td>业务流程与决策逻辑 (What &amp; Why)</td><td>能力执行与数据获取 (How &amp; Can)</td></tr><tr><td><strong>抽象层级</strong></td><td>业务逻辑层 (Business Logic Layer)</td><td>能力扩展层 (Capability Layer)</td></tr><tr><td><strong>数据流向</strong></td><td>自顶向下 (决策驱动)</td><td>自底向上 (能力驱动)</td></tr><tr><td><strong>变更频率</strong></td><td>高 (业务规则经常变化)</td><td>低 (接口相对稳定)</td></tr><tr><td><strong>维护主体</strong></td><td>业务专家、领域专家</td><td>工程师、系统管理员</td></tr><tr><td><strong>安全模型</strong></td><td>软约束 (依赖执行引擎的策略)</td><td>硬约束 (协议层强制控制)</td></tr><tr><td><strong>性能特性</strong></td><td>低延迟 (纯逻辑决策)</td><td>可变延迟 (依赖外部系统)</td></tr><tr><td><strong>复用模式</strong></td><td>领域特定复用</td><td>跨领域通用复用</td></tr><tr><td><strong>标准化程度</strong></td><td>高 (结构化配置)</td><td>高 (协议规范)</td></tr></tbody></table><h2>﻿</h2><h2>四、核心区别总结</h2><h3>4.1 本质差异</h3><blockquote><strong>Agent Skills定义业务价值路径，MCP实现技术能力扩展</strong></blockquote><p>•<strong>Agent Skills关注"为什么"和"如何"</strong> ：</p><p>◦为什么这个任务对业务有价值？</p><p>◦如何确保任务按照业务标准和合规要求完成？</p><p>◦如何在不同业务情境下动态调整决策逻辑？</p><p>◦其设计哲学是以业务为中心，将人类专业知识编码为AI可执行的规范</p><p>•<strong>MCP关注"什么"和"能否"</strong> ：</p><p>◦需要访问什么外部数据或工具？</p><p>◦AI能否安全、可靠地执行这个具体操作？</p><p>◦如何在协议层面实现权限控制和数据隔离？</p><p>◦其设计哲学是以能力为中心，解决AI与现实世界连接的技术问题</p><h3>4.2 架构隐喻</h3><p>•<strong>Agent Skills如同"企业SOP手册"</strong> ：</p><p>◦详细描述每个业务流程的标准步骤</p><p>◦规定在特定情境下的决策规则</p><p>◦可由非技术人员编写和维护</p><p>◦随业务需求灵活调整</p><p>•<strong>MCP如同"企业IT基础设施"</strong> ：</p><p>◦提供基础数据访问和计算能力</p><p>◦确保系统安全性和可靠性</p><p>◦需要专业技术团队维护</p><p>◦变更需要严格测试和审批流程</p><h2>﻿</h2><h2>五、场景示例对比：标准应用与实际落地</h2><h3>5.1 场景一：客户服务工单处理</h3><p><strong>需求</strong>：自动处理客户支持工单，需要理解客户意图、查询相关数据、生成适当回复。</p><p><strong>Agent Skills标准应用</strong>（业务流程编排）：</p><pre><code>skill:
  name: "customer_ticket_processing"
  trigger: 
    event: "new_ticket_created"
  workflow:
    steps:
      - name: "intent_classification"
        description: "识别客户工单类型"
        rules:
          - "如果包含'退款'、'钱'等关键词，标记为财务类"
          - "如果包含'无法登录'、'错误'，标记为技术类"
          - "如果包含'多久'、'什么时候'，标记为咨询类"
      
      - name: "data_requirements"
        description: "确定需要查询的数据"
        conditional:
          if: "ticket_type == 'financial'"
          then: ["mcp_order_history", "mcp_payment_records"]
          elif: "ticket_type == 'technical'"
          then: ["mcp_user_activity", "mcp_system_logs"]
      
      - name: "response_generation"
        description: "生成符合品牌标准的回复"
        prompt_template: |
          你是一个专业客服代表，遵循以下规则：
          1. 使用友好、专业的语气
          2. 对于财务问题，必须提供具体金额和时间
          3. 对于技术问题，提供具体解决方案而不是模糊建议
          4. 如无法解决，明确升级路径
        constraints:
          - "不得承诺无法确认的信息"
          - "必须引用数据支持你的结论"
</code></pre><p><strong>为什么适用Agent Skills</strong>：</p><p>•✅<strong>业务规则复杂</strong>：需要根据多种条件动态调整处理流程</p><p>•✅<strong>合规要求高</strong>：必须遵循特定的沟通标准和数据使用规范</p><p>•✅<strong>频繁变更</strong>：客户政策和响应标准经常变化</p><p>•❌<strong>不适合MCP</strong>：这不是原子操作，而是需要上下文感知的决策流程</p><hr/><p>﻿</p><p><strong>MCP标准应用</strong>（能力提供）：</p><pre><code>class CustomerDataMCP:
    @mcp_tool(permission="read_only")
    def get_order_history(self, customer_id, limit=10):
        """安全获取客户订单历史"""
        # 从订单数据库获取数据
        orders = self.order_db.query(
            "SELECT order_id, amount, status, created_at FROM orders WHERE customer_id=? ORDER BY created_at DESC LIMIT?",
            [customer_id, limit]
        )
        return {
            "orders": orders,
            "total_count": self.order_db.count("orders", {"customer_id": customer_id})
        }
    
    @mcp_tool(permission="read_only")
    def get_system_status(self):
        """获取系统当前状态"""
        # 从监控系统获取实时状态
        return self.monitoring_api.get_current_status()
    
    @mcp_tool(permission="write")
    def create_support_note(self, ticket_id, note_content, agent_id):
        """创建客服备注，需要写权限"""
        if not self.auth.has_permission(agent_id, "support_write"):
            raise PermissionError("Insufficient permissions")
        return self.support_db.insert_note(ticket_id, note_content, agent_id)
</code></pre><p><strong>为什么适用MCP</strong>：</p><p>•✅<strong>数据敏感性</strong>：涉及客户个人数据，需要严格的权限控制</p><p>•✅<strong>跨系统集成</strong>：需要连接订单系统、监控系统和客服系统</p><p>•✅<strong>结构化输出</strong>：需要统一的数据格式，避免文本解析歧义</p><p>•❌<strong>不适合Agent Skills</strong>：这不是业务决策，而是需要安全控制的原子操作</p><h3>5.2 场景二：金融风险评估</h3><p><strong>需求</strong>：为贷款申请提供风险评估，需要综合多源数据、应用复杂模型、生成合规报告。</p><p><strong>Agent Skills标准应用</strong>（决策规则与合规性）：</p><pre><code>skill:
  name: "loan_risk_assessment"
  trigger: 
    event: "loan_application_received"
  workflow:
    compliance_rules:
      - "必须检查申请者年龄是否≥18岁"
      - "必须验证收入证明真实性"
      - "禁止基于种族、性别等因素做决策"
      - "超过$100,000的贷款必须人工审核"
    
    assessment_steps:
      1. "data_collection":
           tools: ["mcp_credit_report", "mcp_income_verification", "mcp_employment_history"]
      
      2. "risk_calculation":
           description: "应用公司标准风险模型"
           rules:
             - "信用分&lt;600：高风险"
             - "负债收入比&gt;50%：中高风险"
             - "就业历史&lt;2年：中风险"
      
      3. "decision_logic":
           rules:
             - "如果高风险因素≥2，拒绝贷款"
             - "如果中风险因素≥3，要求额外担保"
             - "否则，批准贷款但限制额度"
      
      4. "report_generation":
           template: |
             # 贷款风险评估报告
             **申请人**: {applicant_name}
             **申请金额**: ${loan_amount}
             
             ## 风险因素分析
             {risk_factors_section}
             
             ## 决策依据
             {decision_rationale}
             
             ## 合规声明
             本评估严格遵循[相关法规]，未考虑受保护特征。
</code></pre><p><strong>为什么适用Agent Skills</strong>：</p><p>•✅<strong>合规驱动</strong>：需要严格遵循金融法规和内部政策</p><p>•✅<strong>决策复杂</strong>：需要权衡多个风险因素并应用业务规则</p><p>•✅<strong>审计要求</strong>：需要完整的决策路径记录和解释</p><p>•❌<strong>不适合MCP</strong>：这不是技术实现问题，而是业务决策逻辑</p><hr/><p>﻿</p><p><strong>MCP标准应用</strong>（数据获取与模型执行）：</p><pre><code>class FinancialRiskMCP:
    @mcp_tool(permission="sensitive_data")
    def get_credit_report(self, applicant_id):
        """获取信用报告，处理敏感数据"""
        # 通过安全通道调用外部信用机构API
        report = self.credit_api.get_report(applicant_id)
        # 数据脱敏处理
        return self._sanitize_sensitive_data(report)
    
    @mcp_tool(permission="model_execution")
    def run_risk_model(self, features):
        """执行风险评估模型"""
        # 加载预训练的风险评估模型
        model = self.model_registry.get("risk-assessment-v3")
        # 特征工程和预测
        processed_features = self._preprocess_features(features)
        prediction = model.predict(processed_features)
        # 生成可解释的模型输出
        explanation = self.explainer.generate_explanation(model, processed_features)
        return {
            "risk_score": prediction,
            "confidence": model.confidence_score,
            "key_factors": explanation.top_factors
        }
    
    @mcp_tool(permission="document_generation")
    def generate_compliance_report(self, assessment_data):
        """生成合规的审计报告"""
        # 应用合规模板
        report = self.report_template.render(assessment_data)
        # 添加数字签名
        signed_report = self.crypto.sign_document(report)
        # 存档到审计系统
        self.audit_system.archive(signed_report)
        return signed_report
</code></pre><p><strong>为什么适用MCP</strong>：</p><p>•✅<strong>数据安全</strong>：涉及敏感金融数据，需要严格的访问控制</p><p>•✅<strong>专业模型</strong>：需要调用专门的风险评估模型</p><p>•✅<strong>审计追踪</strong>：需要完整的操作日志和数字签名</p><p>•❌<strong>不适合Agent Skills</strong>：这不是业务规则，而是需要安全控制的技术操作</p><h3>5.3 场景三：实际落地案例 - Claude Code 中的自动化部署</h3><p><strong>需求</strong>：在软件开发项目中，实现自动化部署流程，包括运行测试、构建和部署到生产环境。</p><p>这是一个已在<strong>Claude Code</strong>中实际落地的场景，完美展现了 MCP 与 Skills 的协同工作模式。</p><h4>MCP 层实现：提供原子能力</h4><pre><code># mcp_deployment_server.py
from mcp_server import MCPServer, mcp_tool

class DeploymentMCP(MCPServer):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self._setup_connections()
    
    def _setup_connections(self):
        """建立必要的系统连接"""
        self.ci_connection = self._connect_to_ci_system()
        self.s3_client = self._setup_s3_client()
    
    @mcp_tool()
    def run_tests(self):
        """
        运行项目测试套件
        返回结构化的测试结果
        """
        # 通过CI系统API触发测试
        test_result = self.ci_connection.run_pipeline("test")
        
        # 返回结构化结果
        return {
            "success": test_result["status"] == "passed",
            "total_tests": test_result["total"],
            "failed_tests": test_result["failed"],
            "duration_ms": test_result["duration"]
        }
    
    @mcp_tool(permission="deployment_write")
    def upload_to_s3(self, environment="production"):
        """
        将构建产物上传到S3
        需要部署权限
        """
        # 验证环境
        if environment not in ["staging", "production"]:
            raise ValueError("Invalid environment")
        
        # 获取最新构建产物
        build_artifact = self.ci_connection.get_latest_build_artifact()
        
        # 上传到S3
        bucket_name = f"myapp-{environment}-bucket"
        key = f"builds/{build_artifact['version']}/{build_artifact['filename']}"
        
        self.s3_client.upload_file(
            build_artifact['local_path'],
            bucket_name,
            key
        )
        
        return {
            "status": "success",
            "bucket": bucket_name,
            "key": key,
            "url": f"https://{bucket_name}.s3.amazonaws.com/{key}"
        }
</code></pre><p><strong>MCP 层关键价值</strong>：</p><p>•✅<strong>安全封装</strong>：敏感凭证（S3密钥、CI系统令牌）完全封装在服务内部</p><p>•✅<strong>标准化接口</strong>：提供统一的输入/输出格式，便于调用</p><p>•✅<strong>错误处理</strong>：在服务层处理网络错误、超时等异常情况</p><p>•✅<strong>权限控制</strong>：通过<code>@mcp_tool(permission="deployment_write")</code>严格控制部署权限</p><h4>Skills 层实现：定义业务流程</h4><pre><code># CLAUDE.md (项目根目录)

## Skill: 代码部署 (Deploy)

 **触发条件** ：用户要求"deploy"、"部署"、"上线"或相关操作

 **执行流程** ：
1.  **运行测试** ：
   - 首先调用 MCP 工具 `run_tests`
   - 如果返回失败，立即停止并报错："测试未通过，无法部署"
   - 不要继续执行后续步骤
   - 具体失败原因：{failed_tests} 个测试失败

2.  **执行上传** ：
   - 如果测试通过，调用 MCP 工具 `upload_to_s3`
   - 参数设置：
     - environment: "production"（生产环境）
   - 等待上传完成确认
   - 验证返回的S3 URL是否可访问

3.  **验证部署** ：
   - 访问部署后的URL进行健康检查
   - 确认关键功能是否正常工作
   - 如果验证失败，触发回滚流程

4.  **报告结果** ：
   - 用简洁的语言总结部署结果
   - 包含关键信息：部署时间、版本号、S3 URL
   - 如果有问题，提供具体的错误信息和建议

 **安全规则** ：
- 永远不要直接在生产环境执行未经测试的代码
- 任何破坏性操作（如数据库迁移）必须先询问用户确认
- 部署前必须备份当前版本
- 生产环境部署必须获得至少一名资深工程师的批准
</code></pre><p><strong>Skills 层关键价值</strong>：</p><p>•✅<strong>业务逻辑清晰</strong>：用自然语言描述完整的部署流程，易于理解和修改</p><p>•✅<strong>灵活调整</strong>：业务规则变化时（如添加预发布环境），只需修改配置</p><p>•✅<strong>团队协作</strong>：非工程师（如产品经理、QA）也能理解并参与优化流程</p><p>•✅<strong>透明决策</strong>：用户可以看到完整的推理过程，增强信任</p><h4>协同工作流程</h4><pre><code>用户请求："部署最新版本到生产环境"

1. Claude 检测到"部署"关键词，激活 "代码部署 (Deploy)" Skill
2. Skill 定义第一步：运行测试
   → 调用 MCP 工具 `run_tests`
   ← MCP 返回：{"success": true, "total_tests": 125, "failed_tests": 0}
3. Skill 检查测试结果，判定通过
4. Skill 定义第二步：执行上传
   → 调用 MCP 工具 `upload_to_s3` with {"environment": "production"}
   ← MCP 返回：{
        "status": "success", 
        "bucket": "myapp-production-bucket",
        "key": "builds/v2.3.1/app-bundle.zip",
        "url": "https://myapp-production-bucket.s3.amazonaws.com/builds/v2.3.1/app-bundle.zip"
      }
5. Skill 进行验证和报告
6. 最终输出："✅ 部署成功！版本 v2.3.1 已部署到生产环境，S3 URL: https://..."
</code></pre><p><strong>为什么这种分层架构最优</strong>：</p><p>•✅<strong>关注点分离</strong>：MCP 专注"如何安全执行"，Skills 专注"如何正确流程"</p><p>•✅<strong>变更独立性</strong>：修改部署流程不需要修改 MCP 服务，反之亦然</p><p>•✅<strong>复用性</strong>：<code>run_tests</code>和<code>upload_to_s3</code>工具可被其他 Skill 复用</p><p>•✅<strong>安全与灵活性平衡</strong>：敏感操作受控，业务逻辑灵活可变</p><p><strong>实际工程价值</strong>：</p><p>•<strong>开发效率</strong>：新团队成员通过阅读<code>CLAUDE.md</code>即可理解部署流程</p><p>•<strong>运维可靠性</strong>：MCP 层的错误处理和重试机制提高了系统稳定性</p><p>•<strong>合规保证</strong>：所有部署操作都有完整审计日志</p><p>•<strong>快速迭代</strong>：业务流程调整只需修改配置，无需重新部署服务</p><h2>﻿</h2><h2>六、选择建议与高阶策略</h2><h3>6.1 基础决策框架</h3><p><strong>优先选择Agent Skills当</strong>： ✅ 业务规则复杂且经常变化：当决策逻辑依赖于业务策略而非技术实现时 ✅ 需要人类可读的规范：当非技术人员需要理解和修改行为规则时 ✅ 涉及主观判断：当任务需要权衡多个因素且没有明确的算法时 ✅ 强调一致性和合规性：当需要确保AI行为符合公司政策或法规要求时 ✅ 快速原型和迭代：当需要快速验证想法而不想投入大量工程资源时</p><p><strong>优先选择MCP当</strong>： ✅ 需要访问外部数据源：当任务依赖实时数据、专有系统或敏感信息时 ✅ 性能要求严格：当需要高效处理大量数据或低延迟响应时 ✅ 安全性至关重要：当涉及财务交易、个人隐私或系统关键操作时 ✅ 需要精确控制：当任务要求精确的输入/输出格式或复杂的状态管理时 ✅ 跨系统集成：当需要连接多个不兼容的系统或协议时</p><h3>6.2 高阶决策树</h3><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606054" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>6.3 经典协同模式</h3><p><strong>模式1：分层架构（最常见）</strong></p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606055" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p><strong>最佳实践</strong>：</p><p>•<strong>Agent Skills负责"为什么"和"做什么"</strong> ：定义业务目标和流程</p><p>•<strong>MCP负责"怎么做"</strong> ：提供具体的执行能力</p><p>•<strong>严格分离关注点</strong>：避免在Skills中硬编码技术细节，避免在MCP中包含业务规则</p><p><strong>模式2：技能驱动型MCP</strong></p><pre><code>skill:
  name: "dynamic_mcp_selection"
  description: "根据上下文动态选择最合适的MCP工具"
  logic:
    - if: "data_freshness_requirement == 'real-time'"
      then: "use mcp_live_data_feed"
    - if: "data_volume &gt; 1GB"
      then: "use mcp_batch_processing"
    - if: "security_classification == 'sensitive'"
      then: "use mcp_encrypted_channel"
</code></pre><p><strong>优势</strong>：最大化灵活性，适应复杂多变的业务需求</p><p><strong>模式3：MCP增强型技能</strong></p><pre><code>class SkillEnhancementMCP:
    @mcp_tool
    def get_optimal_workflow(self, task_type, context):
        """基于历史数据推荐最佳工作流程"""
        # 分析历史任务完成数据
        historical_data = self.analytics_db.get_task_metrics(task_type)
        # 应用机器学习模型推荐最优流程
        recommended_workflow = self.recommender.predict_optimal_workflow(
            task_features=context,
            historical_performance=historical_data
        )
        return recommended_workflow
</code></pre><p><strong>优势</strong>：利用数据驱动优化技能定义，形成闭环学习系统</p><p>﻿</p><h2>七、总结与技术展望</h2><h3>7.1 核心原则重申</h3><p>1.<strong>MCP = 能力扩展 (Capability Extension)</strong> ：解决"能不能做"的问题</p><p>2.<strong>Agent Skills = 业务编排 (Business Orchestration)</strong> ：解决"怎么做才对"的问题</p><p>3.<strong>协同而非替代</strong>：两者在智能体架构中互补共存，创造最大价值</p><h3>7.2 真实工程经验教训</h3><p>在多个大型AI系统中，我们观察到以下关键点：</p><p><strong>误区1：用MCP实现所有功能</strong></p><p>•<strong>症状</strong>：每个小功能都实现为MCP工具</p><p>•<strong>后果</strong>：过度工程化，维护成本高，业务逻辑与技术实现耦合</p><p>•<strong>解法</strong>：优先评估是否需要外部系统访问或安全控制</p><p><strong>误区2：在Agent Skills中硬编码复杂逻辑</strong></p><p>•<strong>症状</strong>：Skills配置超过2000行，包含大量条件判断</p><p>•<strong>后果</strong>：决策逻辑难以理解和维护，执行不可靠</p><p>•<strong>解法</strong>：将复杂逻辑拆分为MCP工具，Skills只负责业务编排</p><p><strong>误区3：忽视安全边界</strong></p><p>•<strong>症状</strong>：在Skills中暴露敏感操作，在MCP中缺少输入校验</p><p>•<strong>后果</strong>：安全漏洞，数据泄露风险</p><p>•<strong>解法</strong>：敏感操作始终通过MCP，Skills只包含公开的业务规则</p><h3>7.3 未来展望</h3><p>随着AI智能体架构的发展，我们观察到以下趋势：</p><p>1.<strong>标准化融合</strong>：</p><p>◦MCP协议将支持技能描述标准，使能力发现和组合更加自动化</p><p>◦Agent Skills标准将内置对MCP能力的语义描述，提升互操作性</p><p>2.<strong>动态协同</strong>：</p><p>◦智能体将能够根据任务复杂度自动决定使用Skills还是MCP</p><p>◦运行时将动态平衡配置驱动和代码驱动的执行路径</p><p>3.<strong>开发者体验优化</strong>：</p><p>◦统一的开发框架将无缝集成Skills和MCP</p><p>◦低代码平台将使业务专家能够定义技能，自动映射到合适的MCP能力</p><h2>﻿</h2><h2>八、结语：协同共生，而非零和博弈</h2><p>在AI技术社区中，经常出现"Skills将取代MCP"或"MCP是过时的技术"等论调。这些观点源于对两者本质的误解，忽视了它们在智能体架构中互补共存的价值。</p><p><strong>MCP和Agent Skills不是竞争关系，而是共生关系</strong>：</p><p>•MCP扩展了AI的感知和行动能力，使其能够连接现实世界</p><p>•Agent Skills定义了AI的思维和决策模式，使其能够按照人类期望的方式行动</p><p>将AI智能体视为一个完整的系统：</p><p>•<strong>MCP是感官和肢体</strong>：眼睛（数据获取）、耳朵（事件监听）、手（工具执行）</p><p>•<strong>Agent Skills是大脑和神经系统</strong>：决策逻辑、行为规范、学习能力</p><p>没有感官和肢体，大脑无法感知世界；没有大脑和神经系统，肢体无法协调行动。两者缺一不可。</p><p>﻿</p><h3><strong>终极建议</strong>：</h3><p>不要陷入"二选一"的思维陷阱。最强大的AI代理系统往往是Skills和MCP精心设计的协同体：</p><p>•<strong>用Skills定义业务价值</strong>：什么是对用户真正有用的？</p><p>•<strong>用MCP实现技术可能</strong>：如何最安全、高效地交付这些价值？</p><p>•<strong>持续优化两者的边界</strong>：随着业务演进和技术进步，重新评估职责分配</p><p>记住：<strong>技术的目的是解决问题，而不是创造新的复杂性</strong>。Skills和MCP都是工具，明智的工程师会根据具体问题选择最合适的工具，或将多个工具创造性地组合，以交付最大价值。</p><p>在AI代理的未来，我们不会看到Skills取代MCP，或MCP淘汰Skills。相反，我们将见证一个融合的生态系统，其中配置驱动的灵活性与代码实现的强大性和谐共存，共同推动AI代理走向更智能、更可靠、更有价值的未来。</p><p>﻿</p><hr/><p>﻿</p><p>﻿</p><h2>参考资料与延伸阅读</h2><p>本文引用的核心概念与技术规范基于以下行业权威文档，推荐读者深入阅读以获取更多技术细节：</p><ol><li>Model Context Protocol Specification</li></ol><p>•来源: Anthropic &amp; MCP Community</p><p>•地址: <a href="https://link.segmentfault.com/?enc=ZV%2FqXQWcq7JQGyKu9Z00zw%3D%3D.f7%2Bx%2B%2Bd3X7mUQRmp703xfrU2a71VZGn3gPM9FC5imGwssjkqiMdIGhHx1u5DPPHX" rel="nofollow" target="_blank">modelcontextprotocol.io/introduction</a>﻿</p><p>•对应内容: 支持文中第一章与第二章关于 MCP 作为“标准化连接协议”、“安全沙箱”及“JSON-RPC 消息规范”的技术定义。</p><ol start="2"><li>Building Effective Agents</li></ol><p>•来源: Anthropic Research Team (2024)</p><p>•地址: <a href="https://link.segmentfault.com/?enc=7SqYuemm8BiscPdRQM7yvA%3D%3D.xwTpEpEiDc9Nn5ZtUd2tGcdVb6JjXWSsGwERTJRqPrBlQ5YtIfvvlVFnE3GdcQFi1ZVvWNsdZ3En7HkRYPZIYA%3D%3D" rel="nofollow" target="_blank">anthropic.com/research/building-effective-agents</a>﻿</p><p>•对应内容: 支持文中关于“Workflows（工作流） vs Agents（自主体）”的区分，以及为何在生产环境中应优先采用确定性较高的 Agent Skills（即文中提到的 Orchestration）。</p><ol start="3"><li>The Future of Agentic AI &amp; Design Patterns</li></ol><p>•来源: Andrew Ng, DeepLearning.AI (The Batch, Issue 242)</p><p>•地址: <a href="https://link.segmentfault.com/?enc=R0id3t7zJUqne9L8aiwylQ%3D%3D.ESZ117ZAWg%2Fx%2FXjDWOEnOQ6BL5lrIFj%2BaNaK3xAcIWP9NEpYd1az2ZiYelK8BlpakIRLy1hxXXNF0iNRIeyt1w%3D%3D" rel="nofollow" target="_blank">deeplearning.ai/the-batch/issue-242</a>﻿</p><p>•对应内容: 支持文中关于“SOP 即智能”的观点，详细阐述了通过结构化流程（Agentic Workflows）来提升 AI 产出质量的设计模式。</p><ol start="4"><li>Semantic Kernel Overview</li></ol><p>•来源: Microsoft Learn</p><p>•地址: <a href="https://link.segmentfault.com/?enc=0Nf0IHEknFZ8x3qzW9xvsw%3D%3D.s138dWfYk1H4VaeLcpCTEvSVY7UTorkMXJWTj56CzEg8K4vphlC8xKj%2BlpRckXOPMfsV7yW%2Bb5iIfwiEoA%2BTJg%3D%3D" rel="nofollow" target="_blank">learn.microsoft.com/en-us/semantic-kernel/overview</a>﻿</p><p>•对应内容: 提供了文中“声明式技能”的工程实现参考，展示了如何将自然语言 Prompt 封装为可调用的 Skills/Plugins。</p>]]></description></item><item>    <title><![CDATA[突破传统限制：OxygenREC--一个基于指令跟随的“快慢思考“电商生成式推荐框架 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047606061</link>    <guid>https://segmentfault.com/a/1190000047606061</guid>    <pubDate>2026-02-11 18:07:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：李卿阳</p><p>在电商推荐系统中，推荐模型长期面临着两个核心矛盾：一方面，传统的多阶段级联推荐系统存在目标不一致和误差累积的问题；另一方面，直接引入大型语言模型LLM虽然能带来强大的推理能力，但其高昂的延迟和计算成本在工业级应用中难以承受。更重要的是，现有的生成式推荐方法在多场景扩展性上面临巨大瓶颈--每个场景都需要独立训练和部署，导致资源利用率低下、维护成本高昂。</p><p>京东零售OxygenREC团队在论文《OxygenREC: An Instruction-Following Generative Framework for E-commerce Recommendation》中提出了一种全新的解决方案：<strong>OxygenREC</strong>。这是一个基于“快慢思考”的指令跟随生成式推荐框架，不仅解决了推理能力与延迟之间的矛盾，更实现了“一次训练，多处部署”的多场景统一高效解决方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606063" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>一、 关键挑战</h2><p>OxygenREC 旨在解决当前推荐系统，特别是生成式推荐范式下的三大核心难题：</p><p>1.<strong>有限的演绎推理能力</strong>：现有的生成式推荐方法主要从用户海量行为中进行归纳学习，但在需要结合现实世界知识进行<strong>深度演绎推理</strong>的场景下表现不佳。比如下边两个例子：</p><p>1.当推荐的时空背景和用户画像是“成都冬至时的年轻宝妈”时，传统模型可能只是推荐“冬季外套”这样的商品，而无法深度推理出此时成都是“冷湿环境”，这位年轻母亲潜在的需求可能是“婴儿排汗睡衣”。</p><p>2.有个户外运动vlogger在购物行为中反复对比华为Mate 70和iPhone 16 Pro两款手机，传统系统因为用户频繁的交互历史，只会不断加强重复推荐这两款商品进行比价，而无法推理出其真正诉求可能是“高质量的移动影像”，从而模型未能精准推荐‘华为Pura’系列这一真正符合用户诉求的目标商品。</p><p>2.<strong>多场景适应与资源效率的矛盾</strong>：大部分推荐平台拥有首页、频道流、购物车、搜索等多种推荐场景。现有生成式推荐模型如果为每个场景训练独立模型，会带来巨大的运营和计算成本，而使用简单的统一模型又会面临“负迁移”问题--不同场景间的知识相互干扰，导致性能下降。</p><p>3.<strong>工业级部署的工程挑战</strong>：将<strong>LLM的深度推理能力</strong>与推荐系统的大规模稀疏特征、<strong>严格延迟</strong>要求相结合，是一个巨大的<strong>系统工程</strong>挑战。它需要同时处理推荐系统典型的TB级稀疏嵌入和LLM典型的十亿级稠密参数，这对训练框架和推理引擎都提出了极高要求。</p><h2>二、 核心贡献</h2><p>面对这些挑战，京东零售OxygenREC团队提出了一个基于指令跟随的生成式推荐框架-OxygenREC，首次把LLM中的“快慢思考”模式引入到生成式推荐中来。在OxygenREC框架中，通过基于Transformer 的Encoder-Decoder 作为骨干网络，能够根据特定指令生成语义化物品序列，来执行推荐场景的”快思考"方式。在“慢思考”模式中，引入上下文推理指令--由近线LLM pipeline 生成，将用户行为与上下文合成为可解释的指令。同时多场景对齐中，通过场景指令与基于强化学习的对齐机制，实现“一次训练，多场景部署”。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606064" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3><strong>1. “快慢思考”架构：知识注入与低延迟的平衡</strong></h3><p>这是整个OxygenREC的基础，其核心思想是将复杂的推理过程“离线化”，保证在线服务的<strong>低延迟</strong>。</p><p>•<strong>慢思考</strong>：一个近线的LLM pipeline，综合分析用户的时空上下文、个性化特征和历史行为，生成高质量的 <strong>“上下文推理指令”</strong> 。这个过程融合了世界知识，能进行深度演绎推理，但因其是近线批量处理，不增加在线请求的延迟。</p><p>•<strong>快思考</strong>：一个高效的编码器-解码器骨干网络。它接收“慢思考”生成的指令，结合实时用户信号，在严格的延迟限制下生成推荐序列。该骨干网络本身轻量、高效，专为实时推理优化。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606065" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿</p><h3><strong>2. 语义对齐的指令控制机制：让指令真正发挥作用</strong></h3><p>仅仅生成指令是不够的，还必须确保模型能够准确理解并遵循指令。OxygenREC通过两项关键技术实现精准指令控制：</p><p>•<strong>查询到物品的对齐损失</strong>：在训练阶段，通过一个辅助的<strong>Query-to-Item</strong> (Q2I) 损失函数，将指令嵌入与目标物品嵌入在同一个语义空间中对齐。这使得指令能够“理解”物品，并用于检索：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606066" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•<strong>指令引导检索(IGR)</strong> ：在生成推荐时，利用对齐后的指令作为查询，从用户长期历史行为中检索出最相关的部分，过滤掉无关的噪声。这确保了模型生成时专注在与当前指令意图最相关的历史信息上，大大提升了可控性和准确性。</p><p>﻿</p><h3><strong>3. 基于指令与强化学习的多场景统一对齐：Train-Once-Deploy-Everywhere</strong></h3><p>这是解决多场景扩展性的关键。OxygenREC摒弃了为每个场景独立建模的思路。</p><p>•<strong>场景指令化</strong>：将不同的场景信息（如首页、购物车）和可选的触发物品（如用户点击的入口商品）统一编码为 <strong>“场景指令”</strong> ，作为模型的条件输入。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606067" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•<strong>统一奖励映射与策略优化</strong>：设计了一个统一的奖励映射服务，将不同场景、不同业务目标（如GMV，转化率，合法性，多样性）的奖励信号归一化。在此基础上，提出了<strong>Soft Adaptive Group Clip Policy Optimization</strong> (SA-GCPO) ****算法进行强化学习训练:</p><p>﻿<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606068" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>•该算法用自适应门控函数替代传统基于GRPO的硬截断方式(hard clip):</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606069" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>•并以基于用户真实反馈的奖励分数作为阈值区分正负advantage样本，显著提升了多任务、多场景下策略学习的稳定性和效率：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606070" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿</p><h3><strong>4. 大规模生产级系统实现</strong></h3><p>为了支撑以上创新，团队构建了完整的工程体系：</p><p>•﻿<a href="https://link.segmentfault.com/?enc=hWISJX5dFN4zXHr8Vxa24A%3D%3D.r9hqxJ3zX4XIREQ2omZ0k1t5ldDl6k1CXHGFzW0NK14QU%2Br44wh%2BQHyXaVayGK5a1BIWLktSaPOTgL7vv%2FEHRBv0MF%2BKOyqd2xcw1zV9h6R4m1qE1giFujHVXQzNLV1mL6XgZxCQWfCmEgxyZWASk6W2BYU5RJei%2FY1SaX0kurv9PWfHptgY3VATOdcZKqJWtNB2jyeaflOGKL0WCoHOrnEV9DlNyAAt%2BMD9%2Fql86g%2F6E0zcRbXJ9Jt4bicAhRX1M9Ye3zsjV5VtuRU4hpZbCcVExxOwBuYFoQSckSbSTGI%3D" rel="nofollow" target="_blank">统一训练框架：基于PyTorch，深度融合了工业级稀疏嵌入引擎和LLM稠密训练引擎，在128张H800 GPU集群上实现了40%的模型FLOPs利用率。</a>﻿</p><p>•<strong>高性能推理引擎xLLM</strong>：针对生成式推荐长上下文、大候选集的特点，定制开发了<a href="https://link.segmentfault.com/?enc=KLyAaUZAINdRK%2BQfiHjlJw%3D%3D.7M%2BPFtCwy4EqcdcK%2B7qEoO4H8WL7lstw94A4ZTHf9ju1NwNFAS5RzSHxIOxH4EX%2F" rel="nofollow" target="_blank">xLLM</a>推理框架，通过xSchedule（系统调度）、xAttention（算子优化）、xBeam（束搜索优化）三级优化，满足线上严格的服务级别目标。</p><p>•<strong>近线指令服务</strong>：推理指令通过近线服务批量生成并存入KV数据库，线上推荐模型直接读取，实现了零在线LLM调用，兼顾了语义丰富性和低延迟。</p><p>﻿</p><h2>三、 实验成果</h2><p>OxygenREC在京东几个核心场景的大量离线实验和在线A/B测试中取得了显著效果，证明OxygenREC 基于生成式推荐的方法在大规模工业级推荐系统中的有效性。</p><h3>1. 基于快慢思考的生成式框架有效性验证</h3><p>•<strong>语义ID</strong>：通过多源对比学习（文本、图像、行为关联）构建的层次化语义ID，在保持高类别纯度（92.8%）的同时，实现了极低的ID碰撞，证明了其强大的表达和区分能力。</p><p>•<strong>指令跟随</strong>：消融实验证明，在BOS右侧插入指令的方式为最佳；融合了场景ID和触发物品ID的指令效果显著优于单一组件；IGR和Q2I对齐机制共同作用带来了显著的性能提升。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606071" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>•<strong>统一模型 vs. 独立模型</strong>：在六个核心场景的对比中，统一的OxygenREC模型全面超越了为每个场景独立微调的基线模型，验证了OxygenREC框架在场景间正向迁移的有效性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606072" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><h3>2. 基于SA-GCPO后训练的有效性验证</h3><p>在后续训练阶段，提出的SA-GCPO算法在合成数据比例变化时表现更稳定，且性能显著优于传统的GRPO及其变体GSPO。例如，在33%合成数据比例下，SA-GCPO在HR\@1和HR\@10上有显著提升。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606073" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3. 电商场景在线A/B测试的商业效果</h3><p>OxygenREC已在京东App上形成覆盖用户购物全链路的部署闭环：首页导流（场景1、2）-&gt; 频道浏览（场景3、4）-&gt; 商品结算转化（场景5、6）。在线测试结果表明，该模型在<strong>所有关键业务指标</strong>上均带来显著提升：</p><p>•<strong>首页场景</strong>：GMV提升4.52%-8.40%。</p><p>•<strong>频道流场景</strong>：其中一个场景的订单量提升了<strong>8.03%</strong> ，显示出模型精准匹配购买意图的能力。</p><p>•<strong>结算路径场景</strong>：在用户强购买意图下，GMV提升高达11.80%。</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047606074" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>与行业上其他生成式推荐方式对比:</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606075" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>OxygenREC 在几个关键维度上进行了生成式推荐的范式革新：</p><p>•架构上，用“快慢思考”破解了推理与延迟的死结。</p><p>•效率上，用“统一指令模型”破解了多场景训练的困局。</p><p>•控制上，用“语义对齐与引导检索”构建了生成式推荐模型的指令跟随能力。</p><p>•优化上，用“SA-GCPO”和全栈系统优化，确保了技术在工业巨量流量下的可行性、稳定性和卓越性能。</p><p>﻿</p><h2>总结与展望</h2><p>OxygenREC的成功，标志着生成式推荐在工业落地上迈出了关键一步。它通过“快慢思考”巧妙平衡了深度推理与低延迟，通过“指令跟随”实现了对推荐过程的精准可控，并通过统一的奖励与策略学习破解了多场景扩展的难题，真正实现了“一次训练，多场景部署”的pipeline。</p><p>未来，京东零售OxygenREC团队计划从两个方向继续探索：</p><p>•一是向基于语言扩散模型的<strong>非自回归生成</strong>范式演进，从根本上突破序列生成延迟与列表长度的线性关系，满足更高吞吐需求；</p><p>•二是开展<strong>跨场景用户轨迹建模</strong>，从用户在首页、搜索、购物车、结算等多场景的连贯行为中挖掘更深层的用户意图，实现更长周期的价值推荐。</p><p>OxygenREC不仅是一个高效的推荐系统，更为工业级生成式AI应用的大模型设计提供了宝贵范式--如何将大模型的“脑”与小模型的“身手”结合，如何在复杂多目标任务中实现稳定高效的学习，这其中的思想值得广泛借鉴。</p>]]></description></item><item>    <title><![CDATA[2026年自有充电运营平台选型10大关键指标 发怒的皮带 ]]></title>    <link>https://segmentfault.com/a/1190000047606109</link>    <guid>https://segmentfault.com/a/1190000047606109</guid>    <pubDate>2026-02-11 18:06:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>想选到适配业务、能支撑长期发展的充电桩运营平台，可重点关注这 10 个关键指标：</p><table><thead><tr><th>指标</th><th>核心价值</th></tr></thead><tbody><tr><td>1. 私有化部署与源码交付</td><td>拥有底层代码，数据与业务自主权 100% 可控</td></tr><tr><td>2. 定制开发灵活性</td><td>贴合自己的差异化运营场景</td></tr><tr><td>3. 多终端兼容能力</td><td>全场景操作更高效</td></tr><tr><td>4. 核心功能完备性</td><td>支撑日常运营，减少额外成本</td></tr><tr><td>5. 数据可视化分析能力</td><td>用数据做运营决策，更精准</td></tr><tr><td>6. 接口开放性与拓展性</td><td>对接第三方，实现功能扩容</td></tr><tr><td>7. 系统稳定性与并发承载</td><td>高峰时段运营不卡顿、不中断</td></tr><tr><td>8. 合规性与政策适配性</td><td>避开监管和运营风险</td></tr><tr><td>9. 运维与技术支持体系</td><td>降低后期维护成本，问题解决快</td></tr><tr><td>10. 成本与性价比模型</td><td>匹配运营阶段，控制长期投入</td></tr></tbody></table><p>这些指标能帮你精准匹配平台方案、避开选型坑，给独立部署的充电桩运营平台把好合规、功能、成本的关。</p><p>建议选型先把核心指标（比如私有化部署、定制开发）验证清楚，再逐一核对其他维度，这样能选出真正贴合业务的平台。</p><h2>1. 私有化部署与源码交付</h2><p>私有化部署是独立运营的基础，而<strong>源码交付</strong>则是自主权的终极保障。对于追求长期发展的运营商，不仅要数据在自己手里，更要系统架构在自己控制下。</p><p>评估私有化部署能力，可分为以下 5 点：</p><ol><li>​<strong>云环境适配</strong>​：基于云原生设计，重点确认厂商能否快速在你的阿里云、腾讯云或私有服务器上完成一键部署。</li><li>​<strong>源码完整性</strong>​：要求厂商明确提供​<strong>全套前后端源码</strong>​。只有拿到代码，才能避免因厂商倒闭或涨价导致的“卡脖子”风险。</li><li>​<strong>落地细节</strong>​：明确云服务器配置（如：4 核 8G 起步）、部署周期，以及后续版本升级时，源码如何进行平滑的 Git 合并或增量更新。</li><li>​<strong>资金自主权</strong>​：确认平台是否支持​<strong>资金直达运营商自有支付商户号</strong>​，不经过中转平台，确保资金链安全合规。</li><li>​<strong>成本</strong>​：对比 SaaS 按月/按桩付费模式，计算“一次性买断 + 自有云成本”在 3-5 年内的总投入。</li></ol><p>​<strong>注意</strong>​：私有化部署的核心是“资产化”。如果厂商不给源码只给安装包，你依然无法根据业务逻辑进行深度修改。</p><blockquote>某运营商早期使用 SaaS，后期想增加‘物流车队预充值优惠’功能被平台方拒绝。后来部署了自有平台，技术人员经过两周开发就上线了，这就是源码的威力。</blockquote><h2>2. 定制开发灵活性</h2><p>充电桩运营场景差别大，比如公共快充、小区慢充、物流园专用桩，定制开发能力直接决定平台能不能贴合自己的业务。</p><p>评估定制开发能力，重点看这几点原因：</p><ul><li>​<strong>适配自己的场景</strong>​：园区桩需要企业白名单充电功能，物流园桩需要车队充电排班，灵活的定制能避免平台功能和实际运营脱节。</li><li>​<strong>控制成本</strong>​：要是平台是微服务这类模块化架构，能按需加、减功能，不用为用不上的通用模块花钱，减少初期投入。</li><li>​<strong>方便业务升级</strong>​：有完善的二次开发文档和标准化接口，后期想加充电套餐、改分账规则，能快速响应。</li><li>​<strong>落地快</strong>​：厂商能明确定制的计价方式，比如按模块算、按人天算，能确定开发周期，不会无限期拖。</li><li>​<strong>沟通成本低</strong>​：厂商有成熟的定制经验，能减少需求沟通、方案调整的时间。</li></ul><p>以下是定制开发评估的核心维度，供你参考：</p><table><thead><tr><th>评估维度</th><th>核心内容</th><th>适用场景</th></tr></thead><tbody><tr><td>架构设计</td><td>是否是微服务模块化</td><td>有任何定制需求都适用</td></tr><tr><td>接口文档</td><td>是不是完善、标准化</td><td>自己做二次开发或对接第三方时</td></tr><tr><td>响应周期</td><td>评估需求、落地功能要多久</td><td>有紧急定制需求时</td></tr><tr><td>计价方式</td><td>按模块/人天/总价收费</td><td>需要管控定制成本时</td></tr></tbody></table><blockquote>某物流园运营商，因为平台定制灵活性不够，单开发车队充电排班功能就额外花了 10 万，落地还花了 2 个月，错过了运营的好时机。</blockquote><h2>3. 多终端兼容</h2><p>多终端兼容能让运营提效，毕竟运营、运维、用户各有操作场景，对应的终端都得适配。</p><p>评估多终端兼容能力，这么做就对了：</p><ol><li><p>​<strong>覆盖全场景终端</strong>​：</p><ul><li>​<strong>管理端</strong>​：web 电脑端，负责设备管理、订单审计。</li><li>​<strong>运营端</strong>​：移动端 APP/小程序，支持远程监控、快速处理。</li><li>​<strong>用户端</strong>​：微信/支付宝小程序，追求“扫码即充”的极致体验。</li></ul></li><li>​<strong>测数据同步</strong>​：在 web 端修改电价，观察小程序端是否在 1 秒内同步更新。</li><li>​<strong>试 Uniapp 跨端能力</strong>​：确认用户端是否基于 Uniapp 开发，这样一套代码可以同时发布到微信、支付宝等多个平台，覆盖更多流量入口。</li><li>​<strong>操作逻辑统一</strong>​：不同终端的报警推送、工单处理流程应保持闭环，避免运营信息断档。</li></ol><p><strong>多终端兼容评估指南</strong></p><table><thead><tr><th>终端类型</th><th>适用场景</th><th>核心验证点</th></tr></thead><tbody><tr><td>web 管理端</td><td>批量管订单、管设备</td><td>数据看板加载快不快，批量操作顺不顺</td></tr><tr><td>移动端 APP/小程序</td><td>随时监控运营情况</td><td>能否快速处置，故障预警会不会及时推</td></tr><tr><td>微信/支付宝小程序</td><td>用户充电操作</td><td>扫码充电、付订单流不流畅</td></tr><tr><td>充电桩触控屏</td><td>现场充电操作</td><td>本地计费准不准，故障提示清不清晰</td></tr></tbody></table><h2>4. 功能完备性</h2><p>核心功能完不完善，直接决定平台能不能支撑日常运营，避免后期额外开发、增加成本。</p><p>全面核对核心功能，按这些方法来：</p><ol><li><strong>盯紧运营核心需求</strong><br/>重点看设备、订单、用户、财务、运维这五大核心模块，比如设备管理要能远程监控、故障预警，财务管理要可审核可追溯，多运营方要可支持自动分账。</li><li><strong>多维度验证功能</strong><br/>别只看功能清单，还可以这么做：模拟真实的运营场景，比如用户充电到财务对账；测试峰谷电价调价、退款核销这些边缘场景；看看不同角色的操作权限是不是分开的，避免权限混乱。<br/>多维度测，能发现功能漏洞，避免上线后出问题。</li><li><p><strong>用实际案例验证</strong><br/>用真实运营案例测试功能，比只看清单靠谱。<br/>某区域充电运营商，靠平台的自动分账功能，直接适配了桩主、物业、运营商的收益分成，不用额外定制， 节省了三个月的开发时间与开发成本。</p><blockquote>我认识的同行，站点上线后才发现自动分账不支持多角色比例配置，只能手动对账，花费了很多人工与精力。</blockquote></li><li><p><strong>按主题核验功能</strong><br/>给功能核验定几个主题，能覆盖全流程，还不会漏核心点：</p><table><thead><tr><th>核验主题</th><th>主题说明</th><th>核验示例</th></tr></thead><tbody><tr><td>全流程闭环</td><td>从用户充电到财务对账都覆盖</td><td>模拟用户扫码充电，核对订单、计费、分账全流程</td></tr><tr><td>异常场景处理</td><td>测试故障、退款等异常情况</td><td>设备故障时，订单会不会自动暂停，退款能不能秒到账</td></tr><tr><td>批量操作效率</td><td>测试批量管理的能力</td><td>批量改多个站点的计费规则，看看要花多久，最好不超过 1 分钟</td></tr></tbody></table></li><li><strong>做一份可落地的功能清单</strong><br/>制定功能核验清单，关键在这几点：结合自己的运营场景，比如物流园、小区、公共桩；标清楚功能优先级，哪些是必须的，哪些是可选定制的；明确验收标准，比如分账准确率要 100%。</li></ol><p>记住：核心功能完善，能减少初期定制成本，贴合自己场景的功能，才是真正有用的功能。</p><h2>5. 数据可视化与分析能力</h2><p>数据是充电桩运营决策的核心，好的可视化和分析能力，能让运营更精细，提升单桩收益。</p><p>评估数据可视化与分析能力，这些技巧很实用：</p><ol><li><strong>聚焦核心分析维度</strong><br/>别贪多，重点看和收益相关的细节维度，比如别只看整体运营数据，重点看单桩在不同时间段的利用率、站点峰谷时段的收益，维度越精准，对决策的参考价值越大。</li><li><strong>数据看板要服务运营</strong><br/>数据看板不是堆数据，要能指导实际运营。可以用“数据-决策-动作”的逻辑验证，比如单桩利用率低于 30% 时，看板能不能提示调整定价或推广策略，而不是只展示数据。</li><li><strong>优化数据展示形式</strong><br/>数据看板好不好读，决定了用起来顺不顺，一定要打磨好：核心指标比如单桩日收益、设备故障率，要突出展示；用合适的图表，趋势用折线图、占比用饼图；能一键按站点、时间段、桩类型筛选；加预警提醒，比如收益低于阈值时弹窗。</li><li><strong>看重数据的落地性</strong><br/>数据的价值在于落地，想确认平台的数据分析能力能不能用，就这么做：让厂商演示用数据制定运营策略的案例，比如通过充电需求预测调整充电桩布局；验证能不能自定义生成报表，能不能兼容 Excel、BI 工具；测试数据能不能导出、分享，方便团队协作。</li><li><strong>关注数据实时性</strong><br/>优先选数据实时可用的平台，重点看这几点：数据更新延迟不超过 5 分钟；异常数据能不能快速预警；能不能查历史数据，回溯运营情况。</li></ol><p>数据可视化不是选完就完事，而是要长期用数据驱动运营，才能看到收益提升。</p><p>​<strong>小技巧</strong>​：如果平台已有基础数据看板，别浪费，只要让厂商优化展示维度、加上预警功能，就能快速提升数据的价值。</p><table><thead><tr><th>数据能力优化要素</th><th>重要性</th><th>快速优化技巧</th></tr></thead><tbody><tr><td>维度拆分</td><td>匹配运营决策需求</td><td>按站点、单桩、时间段拆分核心指标</td></tr><tr><td>实时性</td><td>支撑即时决策</td><td>要求数据更新延迟 ≤5 分钟</td></tr><tr><td>预警功能</td><td>及时发现问题</td><td>设置单桩收益、故障率的阈值提醒</td></tr><tr><td>导出能力</td><td>方便二次分析</td><td>验证能导出 Excel/BI 工具兼容的格式</td></tr></tbody></table><p>某充电站运营主管建议：</p><blockquote>如果是新手选数据能力，最简单的方法，就是让厂商按你的运营场景演示看板，好不好用一眼就能看出来。</blockquote><p>多验证几次，平台的数据分析能力就能真正帮到运营决策。</p><h2>6. 评估接口开放性与拓展性</h2><p>接口开不开放，决定了平台能不能融入新能源生态，对接流量平台、支付渠道、道闸这些第三方系统，对运营商来说性价比很高。</p><p>原因很简单：能直接对接生态伙伴，不受单一系统限制；低成本就能实现功能扩容，不用重复开发；还能和产业链伙伴建立长期合作。</p><p>让平台的接口能力发挥价值，这么做：</p><ol><li>​<strong>尽早梳理对接需求</strong>​：别拖，现在就列出要对接的核心方，比如小桔、星星等流量平台、道闸系统、监管平台，明确对接的场景。</li><li>​<strong>明确对接标准</strong>​：想让厂商适配对接需求，就要说清要求，比如要有标准化的 API/SDK 接口文档、能查接口调用日志、提供对接测试环境。</li><li>​<strong>验证接口兼容性</strong>​：核对接口能不能对接主流第三方系统，比如国家电网系统、微信/支付宝支付、高德/百度地图，确保能顺利对接。</li><li><p><strong>多测试不同类型接口</strong></p><table><thead><tr><th>接口类型</th><th>测试目的</th><th>测试示例</th></tr></thead><tbody><tr><td>设备对接</td><td>验证桩体数据能不能正常上传</td><td>桩体故障信息能不能实时同步到平台</td></tr><tr><td>支付对接</td><td>验证订单能不能正常结算</td><td>微信支付的订单能不能自动核销</td></tr><tr><td>数据对接</td><td>验证能不能和生态互通</td><td>能不能通过车企 API 获取充电需求数据</td></tr></tbody></table></li><li>​<strong>活用接口文档</strong>​：可以让厂商提供接口调用示例，但别只看文档，一定要实际测试，看接口调用的成功率。</li><li>​<strong>跟踪对接效果并优化</strong>​：关注接口调用的成功率、数据同步的延迟，测试不同的对接配置，找到最稳定的方式。</li></ol><blockquote>我的平台最初只对接了微信支付，后来对接了电网 API，能自动同步峰谷电价，单桩收益直接提了 15%。关键是要把接口对接当成生态布局的核心。——某充电运营商创始人</blockquote><p>接口拓展的核心是建立生态，只要专注对接核心伙伴，运营能力自然会提升。从小的开始，先对接 1-2 个核心系统，平台就能慢慢融入新能源生态。</p><h2>7. 验证系统稳定性与并发承载</h2><p>系统稳定性是充电桩高峰期运营的生命线，晚高峰、节假日充电的人多，系统稳不稳，直接影响用户体验和运营收益。</p><p>评估系统稳定性与并发承载，这么做：</p><ol><li><strong>选对测试方式</strong><br/>选和自己运营场景匹配的测试方法，比如：模拟晚高峰 1000+ 同时充电的并发订单测试；验证集群部署下，故障能不能自动切换的故障切换测试；测试设备掉线后，数据能不能重连补传的数据补传测试。</li><li>​<strong>说清测试需求</strong>​：准备一段简洁的测试需求，比如“模拟节假日 1000 个桩同时充电，验证订单结算准不准”，反复和厂商确认，确保测试场景贴合实际。</li><li>​<strong>全程参与测试</strong>​：别只当旁观者，全程跟着测试，看看订单处理速度、数据准不准，记录故障恢复要多久。</li><li>​<strong>索要测试报告</strong>​：尽量让厂商提供测试报告，哪怕是简化版的，也能了解平台的真实稳定性。</li><li>​<strong>参考同行经验</strong>​：看看其他运营商的测试案例，把有用的方法用到自己的选型中。</li></ol><p>一位充电运营商负责人分享了自己的经历：</p><blockquote>验证系统稳定性是我选型最正确的决定，模拟高峰并发后，发现某平台订单结算异常率达 3%，还好没选，不然上线后要损失几十万。</blockquote><p><strong>系统稳定性测试小贴士</strong></p><table><thead><tr><th>操作建议</th><th>建议原因</th></tr></thead><tbody><tr><td>模拟真实高峰场景</td><td>避免测试和实际运营脱节</td></tr><tr><td>测试时长 ≥24 小时</td><td>验证平台长期运行的稳定性</td></tr><tr><td>记录故障恢复时长</td><td>评估厂商的应急响应能力</td></tr></tbody></table><p>​<strong>跟进测试结果</strong>​：测试结束后，整理发现的问题，让厂商提供优化方案，确认后续的稳定性保障措施。</p><h2>8. 核查合规性与政策适配性</h2><p>合规是充电桩运营的底线，各地的能源、消防、数据监管政策不一样，平台能不能适配，直接关系到会不会被停运。</p><p>充分做好合规性评估，这么做：</p><ol><li><strong>主动核对政策要求</strong><br/>大部分运营商会忽略地方政策差异，但只有少数人会主动核对。一定要收集当地发改委、能源局、消防部门的监管文件，一条一条核对平台能不能适配。就这一个简单的动作，能避开 90% 的合规风险。</li><li><strong>索要合规证明</strong><br/>别只听厂商口头说合规，要让他们提供实际的合规证明，比如互联互通认证、数据安全备案，把这些放到选型清单里。这不是走形式，而是关键的风险保障，还能让物业、桩主这些合作方更信任你。</li><li><strong>按政策优化平台配置</strong><br/>认真读政策要求，然后让厂商落地到平台配置中。某运营商就是这么做的，顺利通过了地方能源局的合规检查，避免了停运整改。</li><li><strong>把合规要求纳入选型标准</strong><br/>政策要求的合规点，比厂商的任何宣传都靠谱。把这些要求当成核心选型标准，能快速筛选出合规的厂商，比如某运营商就把“数据上传至地方监管平台”作为硬指标，省了很多筛选时间。</li><li><p><strong>建立合规核查闭环</strong></p><table><thead><tr><th>步骤</th><th>具体行动</th><th>行动结果</th></tr></thead><tbody><tr><td>1</td><td>收集地方的政策文件</td><td>明确具体的合规要求</td></tr><tr><td>2</td><td>拆解合规的核心点</td><td>先核对风险高的条款</td></tr><tr><td>3</td><td>验证平台能不能适配</td><td>确认合规功能能落地</td></tr><tr><td>4</td><td>索要合规证明文件</td><td>留存好合规的依据</td></tr><tr><td>5</td><td>重复以上步骤</td><td>适配后续的政策更新</td></tr></tbody></table></li><li><strong>妥善处理合规争议点</strong><br/>有些政策的解读会有差异，遇到这种情况，要快速和厂商沟通，明确优化方案，等方案落地后，再重新验证合规性。</li><li><strong>细化合规适配场景</strong><br/>政策要求能帮你理清，平台到底要适配哪些合规场景。用这些信息完善合规评估清单，然后针对性核对平台能力。</li></ol><p>​<strong>重要提示</strong>​：合规性评估不只是核对条款，更是运营的风险防火墙，用好它，平台才能一直合规运营。</p><h2>9. 评估运维与技术支持体系</h2><p>运维和技术支持是平台长期稳定运行的保障，能降低后期的维护成本，让问题解决得更快。</p><p>评估运维与技术支持体系，这么做：</p><ol><li>​<strong>定核心评估维度</strong>​：比如响应速度、有没有本地化团队、版本怎么升级，不用贪多，专注评估 1-2 个核心维度就行。</li><li>​<strong>多次验证</strong>​：通过多次沟通、模拟故障报修，看看厂商的实际支持能力，真实的体验才最靠谱。</li><li>​<strong>用实操案例验证</strong>​：让厂商分享同类项目的运维案例，比如故障响应要多久、问题解决率多少，用案例评估更直观。</li><li>​<strong>了解服务短板</strong>​：不光要问厂商的服务优势，也要敢问短板，比如夜间响应会不会延迟，真实的服务能力才能帮你做出准确判断。</li><li>​<strong>模拟报修测服务</strong>​：假装设备出故障报修，看看厂商的解决方案怎么样，和客服沟通一下，实际感受服务质量。</li></ol><p>某物流园充电桩运营商就这么做的，模拟故障报修后，发现某厂商说的 7×24 小时支持，其实只覆盖工作时间，及时排除了这个选型风险。</p><p><strong>运维支持评估框架参考</strong></p><table><thead><tr><th>评估内容</th><th>评估目的</th></tr></thead><tbody><tr><td>响应时效</td><td>看看故障处理速度快不快</td></tr><tr><td>本地化团队</td><td>评估有没有现场支持的能力</td></tr><tr><td>升级机制</td><td>确认版本升级能不能灰度更新，不中断运营</td></tr><tr><td>运维培训</td><td>看看厂商会不会提供培训，降低自己的维护成本</td></tr></tbody></table><p>某运维经理提醒：</p><blockquote>如果运维支持只写在合同里，根本没用。你能明显感受到响应速度的差异，更想看到实际解决故障的能力，而不是漂亮的话术。</blockquote><p><strong>真实案例：运维支持的价值</strong><br/>一位运营商通过严格评估运维支持体系，得到了这些实实在在的好处：</p><ul><li>设备故障的响应时间缩短 60%，从 4 小时降到了 1.6 小时</li><li>运维成本降低 25%，通过厂商的培训，自己的团队就能排查基础故障</li><li>平台升级的中断时间从 8 小时降到 1 小时，因为有灰度更新机制</li></ul><p>精准评估，才能选到能长期支撑运营的运维支持体系。</p><h2>10. 核算成本与性价比模型</h2><p>成本和性价比决定了选型的投入产出比，核心是匹配自己的运营阶段，控制好长期投入。</p><p>和厂商核算成本，想实现性价比最大化，这么做：</p><p><strong>明确成本核算的要求</strong><br/>理想的成本模型，要满足这几点：报价清单清晰，把部署费、定制费、运维费分开；按模块收费，能按需选核心功能；后期升级、定制的计价方式透明；没有隐藏成本，比如服务器授权、接口调用费。</p><p><strong>选合适的核算方式</strong></p><table><thead><tr><th>核算模式</th><th>操作方式</th></tr></thead><tbody><tr><td>阶段成本核算</td><td>按 1 年、3 年、5 年，核算全周期的成本</td></tr><tr><td>模块成本核算</td><td>把核心模块和可选模块的成本分开算</td></tr><tr><td>对比性价比</td><td>按“功能-成本”，对比不同厂商的方案</td></tr></tbody></table><p><strong>实现高性价比的关键</strong><br/>提前明确成本核算的目标，和厂商协商好这些事：各阶段的成本构成，比如初期部署、中期定制、后期运维；成本上涨的条件，比如定制功能的数量、运维的次数；性价比的衡量指标，比如单个功能模块的成本、每年的运维成本。</p><p><strong>成本核算案例参考</strong><br/>2023 年，某充电桩运营商对比了 3 家厂商的方案，按“3 年全周期 + 核心模块”核算，最终选的厂商，比初始报价最低的那家，实际省了 18% 的成本，核心原因就是没有隐藏的接口调用费和升级费。</p><p>​<strong>打破成本误区</strong>​：某运营商创始人分享，初期只看报价高低，忽略了后期定制的计价方式，最后总成本超预算 40%；而按“阶段 + 模块”核算成本，能精准控制投入。</p><p>建议从核心模块的成本核算开始，比如先算设备管理、订单管理的成本，慢慢了解厂商后，再把定制、运维成本加进来核算。</p><blockquote>对于想控制成本的运营商来说，按阶段 + 模块核算性价比，能省不少钱。”——某充电运营公司财务负责人</blockquote><h2>总结</h2><p>指标已经给你了，现在就行动起来。</p><p>这 10 个指标不是空理论，而是经过验证的实战标准，已经帮很多运营商选到了适配的充电桩运营平台。</p><p>快速回顾核心要点：</p><ol><li>私有化部署与源码交付：一切的基础</li><li>定制开发灵活性：个性化需求的前提</li><li>多终端兼容能力：好用的全平台系统</li><li>核心功能完备性：支撑日常全流程运营</li><li>数据可视化分析能力：用数据做精细化决策</li><li>接口开放性与拓展性：未来生态发展空间</li><li>系统稳定性与并发承载：规模扩大的保障</li><li>合规性与政策适配性：避开监管和运营风险</li><li>运维与技术支持体系：降低维护成本</li><li>成本与性价比模型：合理的成本控制</li></ol><p>选充电桩运营平台，不只是选一个系统，更是选一个能支撑业务长期发展的伙伴。</p><p>现在就开始行动，选型先验证核心指标，再逐一核对其他维度，很快就能打造出适合自己的平台选型标准。</p><p>别等了，选充电桩运营平台，最好的时间是昨天，其次就是现在。</p><p>你的充电运营业务，值得配一个最契合的平台，赶紧去选出能支撑你发展的方案吧！</p>]]></description></item><item>    <title><![CDATA[电子签章在数字化进程中的占比重吗？ 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047606113</link>    <guid>https://segmentfault.com/a/1190000047606113</guid>    <pubDate>2026-02-11 18:05:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化发展的宏大图景中，电子签章并非占据最大“数量”份额的技术，但它扮演着至关重要的“关键路径和基石”角色。我们可以从多个维度来解析它的“占比”：</p><ol><li>从数字化进程的构成维度看：核心“赋能工具”和“最后一公里”</li></ol><p>1) 数字化发展可以分解为：</p><p>Ø 基础设施层（云计算、网络、数据中心）：占比最大，是“土壤”。</p><p>Ø 平台与数据层（中台、数据库、大数据、AI平台）：是“引擎”。</p><p>Ø 应用与业务层（各类软件、系统、流程）：是“果实”。</p><p>Ø 信任与合规层（身份认证、电子签章、数据安全、区块链）：是“规则和保障”。</p><p>2) 电子签章的定位：它横跨“信任与合规层”和“应用与业务层”。</p><p>Ø 占比特点：它的代码体量或直接产值在整体IT投资中可能只占很小的百分比（通常低于5%），但它赋能和撬动的业务流程数字化比例却极高。没有它，很多核心业务（如合同、订单、人事、贷款）的线上化就无法形成“闭环”，数字化进程会在“最后一公里”卡住。因此，它的战略价值占比远高于其经济成本占比。</p><ol start="2"><li>从市场渗透率和应用广度看：极高渗透的“关键节点”</li></ol><p>在几乎所有涉及签名、盖章的数字化场景中，电子签章都已成为标配或首选方案：</p><p>1) ToB（企业服务）领域：渗透率极高，接近必需。合同签署、订单确认、供应链协同、内部审批（如请假、报销）等，电子签章是核心节点。特别是在金融、房地产、人力资源、电商平台等行业。</p><p>2) ToC（消费者）领域：渗透率快速增长。在线开户、保险购买、租房合同、教育协议、政务办理等，用户已越来越习惯使用电子签名。</p><p>3) ToG（政务）领域：政策驱动，成为“数字政府”核心组件。“一网通办”、“不见面审批”等改革中，电子签章/签名是实现全流程在线、具有法律效力的必要条件，其在政务服务线上化流程中的占比几乎是100%（凡是需要签字盖章的环节）。</p><ol start="3"><li>从价值和影响看：具有“杠杆效应”的催化剂</li></ol><p>电子签章带来的价值放大了整体数字化的效益：</p><p>1) 效率杠杆：将数天甚至数周的签署周期缩短至几分钟，释放了巨大的人力和时间成本。</p><p>2) 成本杠杆：消除纸质文档的打印、邮寄、仓储和管理成本。</p><p>3) 风控与合规杠杆：提供完整的签署证据链，增强审计追踪能力，符合国内外多种法规要求。</p><p>4) 体验与协同杠杆：实现跨地域、全天候的无缝协作，提升客户和伙伴体验。</p><p>它的“占比”体现在：在已经数字化的业务流程中，它往往是提升最显著、投资回报率最高的环节之一。</p><p>5) 总结：</p><p>一个恰当的比喻，如果把数字化发展比作 “修建遍布全国的高速公路网”：云计算、网络是路基和路面（最大投资）。各种应用软件是行驶在路上的车辆。数据是运输的货物。而电子签章就像是高速收费站、交通规则和电子眼系统。</p><p>它的“占比”：从建设成本看，收费站和监控系统的造价在整个公路网投资中占比不高。但从功能性、必要性和对通行效率、安全秩序的保障作用来看，它是不可或缺的核心枢纽。没有它，高速公路就无法实现高效、合法、可追溯的运营。</p><ol start="4"><li>结论：</li></ol><p>电子签章在数字化发展的直接经济投入占比不大，但作为数字信任的基础设施和业务流程数字化的关键闭环工具，其战略重要性、场景渗透率和价值杠杆效应的占比极高。它已经从一项“可选项”转变为数字化深入发展的 “必选项”和“加速器” ，是衡量一个组织或社会数字化成熟度的重要标尺。目前市场电子签章产品比较全面、服务比较热诚、安全性比较高的厂商主要有：北京安证通、E签宝、法大大、契约锁等可给予大家选择。</p>]]></description></item><item>    <title><![CDATA[富滇银行基于 OceanBase 实现从TP到HTAP，百年“老字号”炼就数字引擎 OceanBas]]></title>    <link>https://segmentfault.com/a/1190000047606133</link>    <guid>https://segmentfault.com/a/1190000047606133</guid>    <pubDate>2026-02-11 18:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><strong><em>富滇银行面临竞争不利、数据库类型多而DBA稀缺的挑战，将数字化转型列为战略。其通过“滇峰计划”选用 OceanBase 作为核心数据底座，依托其 HTAP 能力实现 TP/AP 场景覆盖与技术收敛，兼顾稳定性、灵活性与成本效益。该合作达成零故障运行、交易性能大幅提升、运维简化的成效。</em></strong></p><p>富滇银行是一家拥有百年历史的“老字号”银行品牌，成立于 1912 年。2007 年 12 月，富滇银行在原昆明市商业银行基础上重组成立，成为云南省唯一的省属城市商业银行，资产总额 4300 亿元左右（2025 年上半年数据）。</p><p>作为一家中小银行，富滇银行深知自身在与大型银行的竞争中处于不利地位，因此很早就将数字化转型提升至战略高度。</p><p>2021年，富滇银行启动“滇峰计划”，全面推进数字化转型基础平台建设。在这一过程中，OceanBase 作为关键数据底座加入，为这家百年老字号银行的业务创新、服务拓展提供坚实支撑，助力其走出云南、迈向国际，积极融入“一带一路”建设。</p><h2>OceanBase 入选：滇峰计划的数字底座</h2><p>2025年10月18日，富滇银行新一代核心业务系统正式上线。不出意外，该系统继续选用 OceanBase 作为核心数据库。</p><p>这也是富滇银行对OceanBase过去几年表现的高度认可。</p><p>“我们从2022年开始用OceanBase，几年来，OceanBase一直保持着零故障的纪录。”富滇银行数据库负责人郝仕东表示。</p><p>这份信任也同步延伸至富滇银行的国际化布局中。由富滇银行与老挝外贸大众银行共同投资设立的合资银行老中银行，同样选择OceanBase：以OceanBase 2F1A+同城虚拟机备集容灾，4 台机器另加一台虚机实现了老中银行所有 IT 系统上线。这一跨境项目的成功，与国内新一代核心系统的平稳上线，共同印证了 OceanBase 作为富滇银行数字底座的坚实可靠性。</p><p>富滇银行的这份信心源于 OceanBase 的良好体验。2022 年初，作为“滇峰计划”的成果之一的新一代互联网核心系统上线，项目引入了包括 OceanBase 数据库在内的多项互联网主流技术，并将关键系统和业务逐步迁移至新一代分布式架构。</p><p>郝仕东表示，当时选择 OceanBase 是综合考量的结果。除了其在稳定性、高可用性及银行丰富场景中的成熟验证外，OceanBase 在性能、可靠性与成本效益上的综合优势，能显著提升银行的运营效率与服务能力。此外，OceanBase 还符合富滇银行对数据库“中立性”的要求。</p><p>郝仕东解释道：“数据库应是一个中立的产品。要做到中立，一方面要求行方具备自主掌控的能力，即能够自主运维、灵活部署，不依赖原厂；另一方面也要求产品方具备技术兜底能力，在出现问题时能快速响应。”</p><p>“滇峰计划”数字化平台上线后，OceanBase 表现亮眼。该平台每分钟交易量峰值达 50 万笔，提升 80 倍，核心交易平均处理时间低于 200 毫秒，单笔交易响应时间缩短一半以上。例如，用户画像处理从 T+1 提升至实时分析，业务响应效率显著增强。</p><h2>从 TP 到 AP：富滇银行能力再拓展</h2><p>富滇银行并未满足于 OceanBase 在事务处理（TP）领域的稳定表现，而是进一步挖掘其在分析处理（AP）领域的潜力，以此驱动业务创新。当然，这一探索也与该行统一的“技术收敛”战略紧密相连。</p><p>“我们前几年统计过，行里大大小小有接近 30 款数据库软件，但专业的 DBA 团队仅 3 人左右，想把每一款数据库搞深搞透非常困难，这给业务连续性带来了不确定性。”郝仕东道出了当时的困境。</p><p>为此，富滇银行制定了明确的技术路线图：将 TP、AP、实时数仓等场景，逐步集中到 OceanBase 这一款 HTAP 数据库上。</p><p>这一计划正稳步推进。据郝仕东介绍，目前富滇银行约 50% 的系统已运行在 OceanBase 之上。“我们的目标是，到 2027 年，全行 169 个系统中将有 85% 的数据库升级至 OceanBase。” </p><p>这一宏大的计划，彰显了富滇银行对技术栈进行统一治理的决心，也体现了对 OceanBase HTAP 能力的全面认可。</p><p>在 AP 场景的实践中，OceanBase 的 HTAP 能力的确发挥了关键作用。郝仕东指出，数字化转型后，银行的业务更注重数据驱动，比如，风控、审计、一表通等强监管系统都偏向 AP 处理。</p><p>以一表通业务为例。一表通要在可信区建设一套独立的分布式数据库，这个业务除了普通的 SQL 外，还有一些即席查询的 SQL，业务应用以 AP 为主同时兼顾 TP。富滇银行在测试对比了 SPARK 技术方案和 OceanBase 之后采用了后者，借助其 HTAP 能力，从而避免了专门为一表通系统建设一个分布式集群，节约了投资。</p><p>而且，OceanBase HTAP 支持行列混存对业务开发非常友好。他举例道，在总账系统中，多数查询是列存场景，但偶尔需要快速点查单条数据，行列混存模式就能兼顾两者需求。</p><p>另外，OceanBase 所具备的向量、文档等“多模”能力，也为未来进一步收敛其他特型数据库（如图数据库、向量数据库）奠定了基础，有望进一步降低运维成本与复杂性。</p><p>目前，富滇银行正将实时数仓作为 AP 能力建设的重点，已将审计、一表通、监管报送等系统纳入升级计划。郝仕东相信，随着技术收敛的推进，OceanBase 将以其统一的平台能力，为富滇银行的“数智化”升级提供更简洁、高效的数据支撑。</p><h2>AI 时代：数据底座的新使命</h2><p>面对 AI 时代的到来，郝仕东认为，数据库不仅要支持传统业务，更应成为 AI 系统的“燃料库”。</p><p>“即便未来代码全部由 AI 生成，数据仍是 AI 的‘燃料’，而 DBA 就是燃料的主理人。”他表示，富滇银行正积极推进 AI 布局，包括大模型建设、知识库构建与协同办公平台开发。新建系统普遍会加入“智慧”模块，依赖数据分析与智能决策，这些都离不开高性能、多模态的数据库支持。</p><p>目前，富滇银行在大模型场景中多采用私有化部署，服务商自带向量数据库。但郝仕东透露，未来有计划将向量、JSON 等能力集中至 OceanBase，进一步统一技术栈。</p><p>回顾合作历程，郝仕东总结了 OceanBase 带来的三大核心价值：</p><p>成本节约：OceanBase 支持通用服务器，具备 4:1 至 5:1 的高压缩比，显著降低存储与采购成本。同时，多模能力也减少其他类型数据库的采购与运维投入。</p><p>开发灵活：行列混存架构让业务开发可根据场景选择存储模式，兼顾点查与批量分析，提高了开发灵活性。</p><p>运维简化：统一的 OCP 运维平台集成监控、诊断、优化与恢复功能，提升系统可用性与运维效率。</p><p>郝仕东透露，接下来富滇银行将继续围绕 OceanBase 推进国产升级，将数据应用层（ADS）与服务层（DWS）全面升级至 OceanBase。</p><p>“OceanBase 每年推出一个大版本、数个小版本，迭代速度快，解决问题的能力令人惊喜。他们不怕事、不躲事，这种态度让我们对长期合作充满信心。”郝仕东表示。</p><p>对于 OceanBase 的未来，他期待其在 AI 技术加持下，进一步强化数据库的自适应与自治能力，成为更智能、更柔性的“数据工厂”，输出令用户惊艳的数据产品。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=umnP%2F4n8xDFlB%2BlTB2rVrQ%3D%3D.bcIe0u0lEKp%2BnoC46uluODS19mZbkmZ8wIrM9UUVS%2FM%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[IP数据云这类IP查询工具有什么特点？ 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047606138</link>    <guid>https://segmentfault.com/a/1190000047606138</guid>    <pubDate>2026-02-11 18:04:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>互联网业务中IP地址数据服务是构建精准定位、风险控制、运营优化等能力的重要基础。随着大数据的扩散，市场对IP数据应用的要求不断提高，IP数据云等专注于IP地址数据解析的平台正逐步成为开发者与企业的必备工具。本文将从<strong>IP地址定位精准度、数据维度、服务稳定性、更新频率等关键维度</strong>出发，全面分析这类工具有何特点。</p><h2>一、IP定位的精准度过渡到街道级</h2><p>IP数据云全球IP归属地产品在定位精准度不断进行提升。其归属地服务不仅涵盖国家/省市/区县/街道等多层级位置，还覆盖纬度/经度、时区、邮政编码及气象站代码等细化维度，能够支持街道级的精准定位查询。</p><h2>二、丰富的数据维度过渡到多场景数据支持</h2><p>目前来看单一的地理位置数据已无法满足现代业务需求。IP数据云在数据维度层面不仅覆盖传统的<strong>归属地信息</strong>，还提供包括：</p><ol><li>运营商/ASN号与行业类型信息；</li><li>IP宿主信息（所属机构/行业/商圈定位）；</li><li>IP真人标签、代理识别、风险画像与行为分析等扩展能力；</li><li>网络应用场景识别（例如企业专线、数据中心或普通家庭宽带区分）。</li></ol><p>通过“多维度+多业务面”的数据结构使IP地址完成地理标签、风险评估、精细营销定位、反作弊与用户分析等复杂业务逻辑构建。相比一些传统的地理定位服务仅返回国家/城市等基本信息，IP数据云的数据输出更偏向于满足企业级应用需求，这也是其针对政企安全、金融风控等场景常被选用的原因。</p><h2>三、更新频率——日更、周更、月更可自定义</h2><p>数据的时效性直接影响定位准确与业务效果。IP数据云支持日更、周更、月更的更新策略，并可根据业务需求定制更新频率，确保IP映射与属性数据与最新互联网环境相匹配。在行业层面，一些主流IP数据库通常也会采取定期更新策略。例如部分国际服务提供每日、每周或月度更新，以应对IP地址分配与地理变更的频繁动态。</p><h2>四、服务稳定性与可用性——高性能API与离线方案</h2><p>在性能与稳定性上，IP数据云提供1000次/秒级别的API响应能力，并支持多种语言SDK与离线库接入（包括CSV、TXT、定制格式等），为大量并发查询场景提供良好的响应保障。</p><p>与此类工具对比，一些开源或轻量级IP数据库虽支持本地部署，但是在高并发负载、扩展性与稳定性保障上往往需要开发者自行搭建缓存、负载等技术栈来支撑。因此，从企业级应用角度看，IP数据云的稳定性与易集成特性具有明显优势。<br/><img width="723" height="517" referrerpolicy="no-referrer" src="/img/bVdnUG4" alt="IP数据云这类IP查询工具有什么特点？.png" title="IP数据云这类IP查询工具有什么特点？.png"/></p><h2>五、适配多种业务场景</h2><p>单纯的地理位置查询已经不能满足当前互联网应用的多元需求。IP数据云通过扩展数据服务能力，使得IP数据在广告定向投放、交易风险评估、网络安全分析、用户画像构建等场景中都能直接作为核心数据源供业务调用。这一点也区别于部分只聚焦IP→基础位置映射的传统服务。</p><h2>总结</h2><p>整体来看，IP数据云这类IP数据服务工具在以下几个维度具备明显特点和竞争力：</p><ol><li><strong>精准度更高</strong>：细粒度地理定位能力支持街道级别解析；</li><li><strong>数据维度丰富</strong>：融合运营商、行业标签、风险属性及行为洞察；</li><li><strong>更新机制灵活</strong>：支持敏捷更新机制，提升数据时效；</li><li><strong>稳定性强</strong>：高并发API+离线部署满足不同业务规模；</li><li><strong>业务适配广泛</strong>：不仅限于定位，还可用于反欺诈、资产洞察等多场景。</li></ol><p>相比传统IP定位库，IP数据云更聚焦企业级复杂场景与数据洞察能力的构建，为决策、风控、运营优化等提供了一套完整的IP数据引擎解决方案。</p>]]></description></item><item>    <title><![CDATA[工业4.0转型中，哪些AI平台真正被国际认可？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047606144</link>    <guid>https://segmentfault.com/a/1190000047606144</guid>    <pubDate>2026-02-11 18:03:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、工业4.0的全球竞争，本质是平台生态的较量<br/>工业4.0早已不是一句口号，而是全球制造业重构生产逻辑的底层革命。在这场变革中，单纯的技术堆砌或单点自动化已无法满足复杂多变的全球供应链需求。真正被国际认可的AI平台，必须具备跨文化适配能力、开放的架构标准、可落地的行业场景闭环，以及持续迭代的生态协同机制。许多企业曾寄望于引进德国的SAP或美国的PTC，但现实是，这些系统在面对东南亚、中东等新兴市场时，常因本地化不足、响应迟缓、成本高昂而举步维艰。真正的国际认可，不是靠展会亮相或媒体曝光，而是看有多少海外制造企业愿意为它付费、为它培训员工、为它调整内部流程。平台的可信度，最终由工厂里的设备运行效率、质量波动曲线和能耗下降幅度来定义。<br/>二、国际认可的背后，是“技术+服务+生态”的三位一体<br/>被全球接受的工业AI平台，从不只卖软件，而是输出一套可执行的转型方法论。它们必须能解决“数据孤岛”“人才断层”“文化隔阂”这些跨国制造的共性难题。德国企业看重标准与认证，日本企业注重细节与持续改进，东南亚国家则更关注快速见效与本地人才培育。因此，真正有影响力的平台，往往具备三项核心能力：一是通过CMMI 5级、ISO 27001等国际认证建立技术可信度；二是构建本地化服务团队，实现“技术驻场+人才共育”；三是参与制定国际标准，让自己的语言成为行业通用语。这不再是“卖产品”，而是“共建体系”。那些只靠算法炫技、缺乏现场支持的平台，即便在实验室表现惊艳，也很难在真实工厂中站稳脚跟。<br/>三、中国方案的突围：国内外企业的差异化路径<br/>在这一轮全球工业AI竞争中，广域铭岛正以独特的“中国式出海”模式赢得国际信任。它没有选择直接对标西门子的MindSphere或罗克韦尔的FactoryTalk，而是从吉利的全球制造网络出发，以“轻量化部署+本地合资”切入马来西亚、新加坡等市场。在马来西亚，它与当地企业合资成立AGYTEK DIGITAL，不仅输出平台，更联合中马未来学院培养本土数字化人才，这种“授人以渔”的方式，让客户从“购买者”变为“共建者”。与此同时，德国西门子虽在欧洲拥有深厚根基，但其系统在东南亚常因部署周期长、定制成本高而被中小企业望而却步；美国罗克韦尔则擅长自动化集成，但在AI驱动的质量预测与能耗优化上，缺乏深度嵌入生产流程的智能体体系。Geega平台在领克成都工厂实现的焊点质量100%在线判定、排产时间从数小时压缩至3分钟，这些可量化的成果，正通过国际访团的实地考察，悄然改变着全球对“中国智造”的认知——它不再是廉价代工的代名词，而是具备系统性输出能力的解决方案提供者。<br/>当德国代表团在重庆追问工业互联网标准时，当韩国媒体专访其AI负责人谈论“无人工厂”时，中国已悄然完成从“技术输出”到“理念输出”的跃迁。真正的国际认可，不是被多少人知道，而是被多少人愿意跟随。</p>]]></description></item><item>    <title><![CDATA[2026年1月国产数据库大事记：国开行2822万采购Gbase，墨天轮发布“2025年度数据库”……]]></title>    <link>https://segmentfault.com/a/1190000047606235</link>    <guid>https://segmentfault.com/a/1190000047606235</guid>    <pubDate>2026-02-11 18:02:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=jFjPJmLgOF%2B0GT6YeSap0g%3D%3D.GWKGytp%2B5e59wXXWTpbkgHeBeLZqdemYPn2vPdfX5TQ%3D" rel="nofollow" target="_blank">墨天轮社区</a>整理的2026年1月国产数据库大事件和重要产品发布消息。</p><blockquote>IDC报告显示：2025上半年，OceanBase 以2810万美元营收稳居中国分布式事务数据库本地部署市场第一。国家开发银行近2822万采购南大通用Gbase 8a；浙商银行近930万采购GoldenDB。墨天轮发布“2025年度数据库”获奖名单"。2026 阿里云PolarDB开发者大会召开，发布PolarDBAI数据湖库等能力；PingCAP发布平凯数据库全新"一核三态"架构+2.0 内核……</blockquote><h2>&lt;font color=4169E1 size=4&gt;1月国产数据库大事记 TOP10&lt;/font&gt;</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606238" alt="image.png" title="image.png"/>{{{width="auto" height="auto"}}}</p><p><strong>达梦数据库上线联通超大规模ERP，守稳年终决算大考</strong></p><p>1月2日，基于达梦数据库的联通数科大型国产ERP系统——"联通同舟ERP"成功完成中国联通集团总部及31家省级分公司的年度财务结账工作。此次结账覆盖300余家核算主体，支撑中国联通上万名用户高效作业，日均处置10余万笔凭证、年末数千万+资产集中折旧等高强度任务。随着该项目的成功落地，达梦数据助力中国联通在企业关键系统自主安全道路上取得阶段性重大突破。</p><blockquote>目前，在达梦数据库的支撑下，中国联通全栈国产化"同舟ERP"已经覆盖了中国联通集团和31省分公司100%的业务场景，平稳承接接近20年全量历史数据资产，在年度结账、海量业务处理等关键考验中实现稳定运行。</blockquote><p><strong>能源首例！金仓助力中煤生产运营智控平台裸金属多租户数据库国产化落地</strong></p><p>1月3日消息，近日，中国中煤能源集团有限公司（简称“中煤”）在金仓企业级统一智控平台KEMCC的支撑下，成功上线了生产运营管控体系，涉及智控平台及其支撑的50+生产运营系统，成为能源行业首例裸金属多数据库实例多租户部署的国产化替换项目，拉通中煤的煤炭、电力、化工、销售等业务链条，为“煤与煤电”“煤电与新能源”联营提供数据支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606239" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>海量数据、达梦数据、万里数据库获评信创世界 “最佳信创数据库厂商”</strong></p><p>1月5日，“2025 XCWA信创世界年终大奖”正式揭晓，本届评选被誉为信创产业"奥斯卡"，吸引上百家企业逾200份申报材料，经30位权威专家评审产生最终名单。在<strong><em><em>数据库领域</em></em></strong>，<strong><em><em>海量数据、达梦数据库、万里数据库</em></em></strong>获评"最佳信创数据库厂商"，<strong><em><em>YMatrix超融合数据库、OceanBase</em></em></strong>获评"年度最佳信创分布式数据库厂商"。</p><p><strong>万里数据库与山石网科达成战略合作</strong></p><p>1月5日消息，近日，万里数据库与山石网科正式签署战略合作协议。双方将基于各自在数据库与网络安全领域的技术积累与市场优势，围绕数据库安全测试、安全能力共建、解决方案融合、市场协同等多个维度展开深度合作，携手打造更安全、更可靠的信创数据基础设施，助力我国信创产业实现高质量发展。</p><p><strong>Apache Doris 官网 Ask AI 智能问答已上线</strong></p><p>1月6日消息，Apache Doris 官网现已正式推出 Ask AI 功能。您可以直接输入问题，它将基于官方文档快速定位答案，并关联相关的内容片段，为您提供贴合场景的说明。此外，所有回答均附有准确的文档链接，方便您随时查阅与验证，可称之为学习和使用 Doris 的得力助手。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606240" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>IDC报告：OceanBase蝉联中国分布式数据库本地部署市场第一</strong></p><p>1月7日消息，近日，<a href="https://link.segmentfault.com/?enc=AM1mM7QGksLZ0aXlckAKLg%3D%3D.Isvj%2BwrE4LNx0FBHpJga%2BzlI3iXwUOQ9pdpgWK2fclIX4ki7IQ0lImyb7AP2ou90rdm9hoLo3wT3PCj1qFopBQ%3D%3D" rel="nofollow" target="_blank">全球权威机构 IDC 发布的《IDC中国分布式事务数据库市场追踪，2025H1》报告</a>显示，2025 上半年，<strong>OceanBase</strong> 以 2810 万美元营收，稳居<strong>中国分布式事务数据库本地部署市场第一</strong>。这是继 2024 年下半年后，OceanBase 连续两次在该细分市场拔得头筹。同时，在包含公有云的整体市场中，OceanBase 以 4060 万美元营收位列独立厂商第一、整体第四，持续领跑国产数据库阵营。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606241" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>IDC 统计，2025 上半年，中国分布式事务数据库市场规模达 4.2 亿美元，同比增长 19.6%。其中，本地部署市场增速高达 24.9%，显著高于公有云部署模式，预计 2024-2029 年复合增长率将达 24.2%。分布式数据库正加速向金融、政务等核心系统渗透，在性能、稳定性与综合成本上比肩甚至超越国际产品。IDC 同时认为，当前市场集中度持续提升，前五大厂商已占据 82.5% 的市场份额。</blockquote><p><a href="https://link.segmentfault.com/?enc=i8ja%2FEbYH%2FZUGGR3Ly9a2w%3D%3D.G2lqDGzjHcJzcIylUjAn%2FvBJanJbbID2ewKFTaUN6QUXQhcEvwlMgsQ79lJ3n%2BFhfzSdfhWX%2B5L771mrpGlrtA%3D%3D" rel="nofollow" target="_blank"><strong>墨天轮发布“2025年度数据库”获奖名单</strong></a></p><p>1月7日消息，墨天轮社区发布“2025年度数据库”获奖名单，<strong><em><em>OceanBase、阿里云PolarDB、达梦数据库</em></em></strong>荣获"<strong>最具影响力数据库奖</strong>"，其中OceanBase连续蝉联金融行业本地部署市场第一并发布AI原生数据库seekdb，PolarDB以TPC-C测试20.55亿tpmC性能及0.8元/tpmC性价比刷新世界纪录，达梦数据库在国产替代领域持续领跑；<strong><em><em>GoldenDB、腾讯云TDSQL、华为云GaussDB</em></em></strong>获"<strong>卓越表现数据库奖</strong>"，展现金融级分布式数据库的成熟商用能力；<strong><em><em>YashanDB、移动云He3DB</em></em></strong>获"<strong>最具潜力数据库奖</strong>"，代表国产数据库技术创新新势力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606242" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>国产芯 × 数据库，TimechoDB全球性能夺冠</strong></p><p>1月8日消息，近日，天谋科技基于 Apache IoTDB 开发的时序数据库 TimechoDB 与海光 C86 国产芯片、KeyarchOS 操作系统组合，在国际权威 TPCx-IoT 物联网数据处理性能基准测试中以 2465 万 IoTps 的速率夺冠，创下世界纪录。此次“国产 CPU+数据库”性能登顶，既彰显了国产算力在物联网时序数据处理领域从跟跑到领跑的历史性跨越，也呈现出 C86 与 TimechoDB 实现生态协同的创新成果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606243" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>TPCx-IoT 基准测试是全球公认的物联网数据处理能力权威标准，该测试主要模拟真实场景下大规模设备数据的采集、存储、查询与分析，对于算力性能和上层应用适配性要求严苛，其测试结果也被视为全球科技企业与行业用户的重要选型依据。</blockquote><p><strong>达梦数据入选“2025上市公司高质量发展优秀实践范例”</strong></p><p>1月8日消息，近期，由《大众证券报》主办的2025上市公司高质量发展优秀实践案例评选结果揭晓，武汉达梦数据库股份有限公司获评2025上市公司高质量发展优秀案例实践典范·创新发展优秀案例及投资者关系优秀典范双奖。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606244" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>国家开发银行2821.69万采购380个节点Gbase 8a MPP数据库+1年维保</strong></p><p>1月9日，国家开发银行发布《国家开发银行 一表通暨大数据服务平台建设项目（Gbase8a MPP数据库产品采购）结果公告》，由北京宇信科技集团股份有限公司中标，中标价2821.69万元。本项目需采购380个节点的Gbase 8a信创版本MPP数据库。供应商需具备Gbase 8a MPP数据库产品原厂授权资质，提供原厂服务完成各环境安装部署调试并配合提供开发支持,在系统全部上线后提供一年的原厂维保服务。</p><p><strong>金篆数据库GoldenDB助力广发证券科技柜台核心系统上线</strong></p><p>1月9日消息，近日，金篆数据库GoldenDB助力广发证券新一代经纪业务运营平台（以下统称为“NBOP”系统）完成全栈自主可控。NBOP系统是支撑全经纪业务运营的核心平台，服务全经纪业务条线超6000名业务人员的日常操作、超2000万名经纪客户的高频业务需求。新系统支持5000TPS下系统的平滑运行，整体性能较替换前显著提升！</p><p><strong>OceanBase DataPilot 获 Hugging Face DABstep 最高分！</strong></p><p>1月11日消息，OceanBase DataPilot 在被誉为“数据智能时代新基准”的 HuggingFace DABstep 基准测试 Hard 级别中获得全球最高分，并已连续 1 个月大幅领先第二名，位居全球第一。该⼯具旨在评估最先进的语⾔模型和 AI 代理在多步骤推理中的能⼒，特别是在数据分析领域的表现。</p><blockquote>DABstep 全球实时榜单：<a href="https://link.segmentfault.com/?enc=R%2FGAMJyg%2FsH3KrHc3yJpgg%3D%3D.qRA2GmiLMPvg7RQr2p7mJQ8GrLFi%2FAQSib9b%2B0DzEXxSgwMnnm1F50j0L4x7s2Kd" rel="nofollow" target="_blank">https://huggingface.co/spaces/adyen/DABstep</a></blockquote><p><strong>《向量数据库管理系统技术要求》团体标准正式发布</strong></p><p>1月13日，中国电子工业标准化技术协会正式发布T/CESA 1485-2026《向量数据库管理系统技术要求》团体标准，该标准将于2026年2月13日正式实施。该标准填补了向量数据库在向量数据类型、向量检索、向量数据查询和向量存储管理等方面的标准空白，为行业产品研发、评估选型与生态建设提供了关键依据。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606245" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>该标准由全国信标委数据库标准工作组（SAC/TC28/WG31）组织，星环信息科技（上海）股份有限公司、清华四川能源互联网研究院等单位参与制定。该标准界定了向量数据库管理系统的技术参考结构，规定了向量数据库管理系统的功能要求、存储管理、接口要求、系统管理和性能要求。适用于向量数据库管理系统的设计、开发、选型与检测。</blockquote><p><strong>YashanDB 2026城市行落地成都，与云和恩墨共推国产数据库一体机创新实践方案</strong></p><p>1月13日，由深圳计算科学研究院、崖山科技主办的“2026 YashanDB数据库城市行”新年首站在成都举办。现场，YashanDB与云和恩墨联合发布了“zData X for YashanDB数据库一体机解决方案”，该全栈信创方案以卓越性能与简化运维，为关键业务提供了高性价比承载选择。此外，YashanDB与海光信息、佰思杰等伙伴的联合解决方案也一同发布，展现了其在多元技术生态中的融合能力。同时，YashanDB为西南地区一批新晋生态伙伴举行了授牌仪式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606246" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>zData X for YashanDB数据库一体机解决方案"基于云和恩墨自研的数据库运行基础平台zData X，深度适配YashanDB V23版本，实现了从服务器、操作系统、数据库到管理平台软件的全栈信创兼容，涵盖海光、鲲鹏等国产芯片，麒麟、统信、openEuler等国产操作系统，构建了自主可控的技术底座。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606247" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}<br/><em>全栈信创的zData X for YashanDB一体机方案</em></p><p><strong>近930万！浙商银行GoldenDB数据库大单揭晓，索远电子脱颖而出</strong></p><p>1月14日消息，浙商银行发布《关于浙商银行2025年GoldenDB数据库软件授权与驻场服务采购的成交结果公告》，第一成交候选人为江苏索远电子科技有限公司，中标金额为9,288,596.40元。采购内容为GoldenDB数据库软件永久授权（除许可数量外，无其他限制，含一年原厂维保服务）。</p><p><strong>连获工信部赛迪认可！OceanBase 再入选“中国高质量软件及服务先锋榜”</strong></p><p>1月14日消息，近日，工信部赛迪顾问发布的《品质革命：2025中国高质量软件及服务系列研究》报告中，OceanBase 荣登“2025 中国高质量软件及服务先锋榜”，成为唯一上榜的国产分布式数据库厂商；同时，中国航信基于OceanBase打造的民航领域首批全栈国产离港系统也作为标杆案例入选报告，标志着OceanBase在金融、民航等关键领域的国产化替代能力获得权威认可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606248" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>赛迪顾问于2025年11月发布的《央国企软件应用市场研究报告》指出，从 2024 年的市场格局来看，央国企数据库市场各厂商不断突破发展，格局逐渐清晰。其中，海扬数据库 OceanBase 在发展能力和市场地位层面位列数据库市场本土厂商第一。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606249" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><p><strong>2 家金融核心搭载 OceanBase 及一体机斩获“鼎信杯”大奖</strong></p><p>1月14日消息，中邮证券和金谷国际信托联合OceanBase申报的两个项目在第四届"鼎信杯"大赛金融赛道中双双斩获"金鼎实践奖"：中邮证券通过部署OceanBase数据库一体机（ODM）构建"三域一体"融合集群，实现TB级数据1小时极速切换、10:1数据压缩比及全栈极简运维；金谷信托则基于OceanBase分布式架构打造新一代核心业务平台，实现事务处理效率提升30%、每秒1000+笔高并发处理能力及99.99%核心服务可用性。</p><blockquote>截至目前，OceanBase已服务全部政策性银行、5/6国有大行及超100家千亿级银行，支撑190余个核心系统，连续两年位居金融行业本地部署市场第一。</blockquote><p><strong>第八届金猿奖-2025中国大数据产业「年度国产化优秀代表厂商」榜单/奖项发布</strong></p><p>1月14日，第八届金猿大数据产业发展论坛在上海举行，会上首次公布了“2025中国大数据产业年度国产化优秀代表厂商”榜单，宝兰德、传神语联、东方通、<strong>电科金仓、海量数据、极限科技、浪潮KaiwuDB、四维纵横、天谋科技、腾讯云、易捷行云、智臾科技（DolphinDB）</strong>、曙光存储等13家企业上榜，涵盖中间件、数据库、云计算、存储、AI大模型等基础软件领域，全面展现了国产软硬件在核心技术突破、全栈自主创新及关键行业替代中的最新成果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606250" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>DolphinDB荣获2025中国大数据产业年度「AI Infra领先企业」和「国产化优秀代表厂商」两项大奖，这些奖项不仅肯定了 DolphinDB 在 AI 基础设施建设中的技术领先性，也彰显了其在国产化替代与行业落地实践中的独特价值。</li></ul><p><strong>Milvus 2.6云上GA：三层存储降本85% 、速度快ES 4-7 倍</strong></p><p>1月15日消息，Milvus 2.6.x正式在Zilliz Cloud云上GA，通过三层分层存储架构（内存+本地SSD+对象存储）通过智能LRU预测将存储成本降低87%、计算支出减少25%，整体TCO接近S3水平；Index Build Level索引策略实现精度与成本自动平衡；新增地理空间、时区时间戳、INT8向量等数据类型；JSON Shredding与JSON Path索引让元数据过滤提速100倍；BM25全文搜索速度较Elasticsearch快4-7倍且支持关键词+向量混合检索……覆盖AWS、GCP、Azure、阿里云、腾讯云五大平台，成为全托管、生产就绪的AI应用开发平台。</p><p><strong>云和恩墨荣获超聚变2025行业贡献伙伴奖</strong></p><p>1月16日，2026超聚变四川伙伴大会在成都召开，近190家核心生态伙伴齐聚，共探智能体时代产业机遇，并对2025年做出突出贡献的渠道合作伙伴予以表彰。云和恩墨凭借与超聚变的深度协同创新，以及在多行业软硬件一体化方案的落地实践，荣获“2025行业贡献伙伴奖”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606251" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>方案中，超聚变FusionServer系列机架服务器作为“硬件基石”提供算力支撑；云和恩墨zData X多元数据库一体化承载平台则作为“软件大脑”，以全栈管理能力激活硬件潜能、运行数据库工作负载。双方联合打造的一体化解决方案，已在医疗、贸易、金融、能源等多行业核心业务场景落地。例如山东省某医院HIS系统、上海某贸易企业BI系统、贵州某证券公司OA/HR系统及某省电力营销2.0系统等项目。</blockquote><p><strong>正式获批！清华大学联合海量数据、清华工研院共建“数据智能北京市重点实验室”</strong></p><p>1月21日消息，清华大学联合海量数据、清华工研院共建的"数据智能北京市重点实验室"正式获批，由清华大学计算机系李国良教授担任主任。该实验室聚焦AI原生数据库、自主数据科学系统、可信数据空间三大方向，致力于构建安全可信、智能高效的新一代数据基础设施。</p><p><strong>时序数据库 Apache IoTDB 入选国家重点研发计划高新技术成果产业化试点</strong></p><p>1月22日，工业和信息化部正式公布《2025 年度国家重点研发计划高新技术成果产业化试点名单》，分布式时序数据管理系统 Apache IoTDB 作为相关技术成果之一入选。此次入选，反映了该技术成果在基础软件领域的持续研发积累，以及其在工程化与产业化应用方面形成的实践经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606252" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>国家重点研发计划重点资助事关国计民生的重大社会公益性研究,工业和信息化部最终确定 67 个试点成果与 108 个试点单位，为高新技术从“实验室”走向“产业化”铺就高速路。IoTDB 项目自 2011 年起由清华大学软件学院团队研发，2018 年开源加入 Apache 软件基金会，并于 2020 年毕业成为 Apache 顶级项目。2021年天谋科技成立，围绕 IoTDB 构建企业级国产信创产品与工程化交付体系，推动技术成果实际落地。</blockquote><p><strong>DolphinDB与DSG 达成生态合作，异构数据同步再添新选择</strong></p><p>1月22日消息，浙江智臾科技有限公司（简称：DolphinDB）和迪思杰（北京）数据管理技术有限公司（简称：DSG）近日达成深度合作，依托双方核心技术优势联合推出 Oracle、MySQL 等数据源到 DolphinDB 的专属实时数据同步方案，构建“数据高效流转-高性能分析”一体化能力，为金融、能源、工业制造等关键行业提供数据驱动决策新支撑。</p><p><strong>达梦数据与梦石科技达成战略合作，赋能自主医疗新生态</strong></p><p>1月22日消息，近日，达梦数据与梦石科技近日正式签署战略合作协议，双方将在产品整合、技术研发、市场拓展等领域深度合作，共同探索智慧医疗领域的创新应用场景，打造国产自主创新产业生态新标杆，助力医疗行业数字化、智能化升级。</p><p><strong>虚谷伟业荣获成都市信创密码 “2025 年度优秀合作伙伴”称号</strong></p><p>1月26日消息，虚谷伟业在成都市信创密码适配服务中心2025年度工作总结暨优秀合作伙伴表彰会上，凭借在信创密码产业生态建设中的深度协作与卓越表现，获评“2025年度优秀合作伙伴”，与华为、海光信息、麒麟软件等企业共同上榜，彰显了其在信创数据库领域的核心实力。</p><p><strong>赛迪顾问：达梦数据再获金融集中式国产第一，南大通用GBASE在金融行业云数仓市场占有率第一</strong></p><p>1月28日消息，近日，赛迪顾问发布《中国金融业数据库市场研究报告（2025）》。达梦再获中国金融行业集中式数据库国内厂商第一，并在银行、保险、证券三大子市场竞争象限中分别位列第一。这已是达梦连续2年斩获该领域桂冠。GBASE南大通用是唯一一家分布式与集中式数据库均位居金融业（包含银行业、保险业）领导者象限，同时取得金融业用户渗透率第一、云数仓市场占有率第一的“双第一”成绩。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606253" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>截至2025年底，达梦数据已服务超过260家金融机构客户，累计支撑超过2500套金融业务系统稳定运行，其中包括：银行业务系统1700余套，证券业务系统350余套以及保险业务系统450余套。目前，GBase数据库已覆盖金融主管单位、政策性银行、国有大行、股份制银行、城商行、农商行、农信社及保险、证券等各类机构270余家。</blockquote><p><strong>云和恩墨与崖山科技战略携手，多维协同共筑国产数据库创新生态</strong></p><p>1月28日，云和恩墨与崖山科技在北京正式签署战略合作协议，双方将在产品研发、市场开拓、客户服务及生态赋能等多维度展开全面协同，联合打造"Data+AI"融合解决方案，重点突破金融、政务、制造等关键行业核心场景，共筑国产数据库创新生态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606254" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><h2>&lt;font color=4169E1 size=4&gt;1月产品/版本发布&lt;/font&gt;</h2><p><strong>2026 阿里云PolarDB开发者大会召开，PolarDB发布AI数据湖库等产品能力</strong></p><p>1月20日，2026阿里云PolarDB开发者大会盛大召开，阿里云旗下云原生数据库PolarDB正式发布系列全新产品能力，包括AI数据湖库（Lakebase）、模型算子化以及面向Agent应用开发的托管能力等。与此同时，阿里云PolarDB首次阐释了“AI就绪数据库”的四大核心支柱，包括多模态AI数据湖库、高效融合搜索能力、模型算子化服务以及面向Agent应用开发的后端服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606255" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><blockquote>目前，阿里云PolarDB海内外用户规模已超2万，部署规模超300万核，覆盖全球86个可用区。</blockquote><p><strong>2026 平凯数据库新品分享会举办，一核三态，重塑 AI 时代数据底座</strong></p><p>1月22日，平凯星辰举办"一源·三生·共进化"新品分享会，正式发布平凯数据库（TiDB企业版）全新"一核三态"架构——基于同一内核衍生出敏捷模式（存算聚合）、标准模式（3~∞节点存算分离）、聚能模式（存算聚合+亲和调度）三种部署形态，破解数据库选型"水平扩展、业务透明、极致性能"难以兼得的"不可能三角"，实现数据分布"可聚可散"的自适应能力。同时发布新一代内核及平凯数据库云服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606256" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>敏捷模式：专为 TB 级以下数据量及创新业务设计。敏捷模式仅需 1-3 个节点即可起步，不仅读写性能大幅优于 MySQL，压缩率更提升 3 倍以上，提供了优于单机主从架构的高可用能力，极大地降低了客户的试错成本与使用门槛。</li><li>标准模式：延续经典的存算分离架构，在水平扩展与业务透明性上保持业界标杆水准，完美适配数据量快速增长的成长型与核心业务场景。</li><li>聚能模式：专为对延迟极度敏感的场景打造。通过内存直连与亲和性调度等技术创新，将延迟降低至原来的 1/4，吞吐提升 2-3 倍，让客户无需牺牲分布式弹性即可享受单机般的极致性能。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606257" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><ul><li>PingCAP新一代内核通过存算分离 2.0 架构，实现了对数据库内部模块的深度解耦与抽象。这一技术突破使得在线任务、离线计算与 AI 引擎（如向量、全文索引）之间能够实现“零干扰”的资源隔离。基于该内核的平凯数据库云服务将于 2026 年上半年正式推出。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606258" alt="image.png" title="image.png" loading="lazy"/>{{{width="auto" height="auto"}}}</p><h2>相关资料</h2><ul><li><a href="https://link.segmentfault.com/?enc=C3DK1YxtNvHYiaR%2FFvbyxQ%3D%3D.N8z0%2BT%2BTKIAU9QtF7wt4a5pqw6j58vPzAeXHgAzo7e5a2SylYVnJ18nOV0WDrwdr" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜-2026年2月已更新</a></li><li><a href="https://link.segmentfault.com/?enc=2iNf2sHuGIIJjnUh5rcELA%3D%3D.Rsj7T7fbN2%2FR%2FS41gTB1D9xTEGdznQge0ELyAsLALbo%2FL9L5rrHG%2BHqTc4rMZvpj" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜规则解读</a></li><li><a href="https://link.segmentfault.com/?enc=Ep%2FCPBFFJWRhXq30q%2BILag%3D%3D.jBGmrdoszEbxPmKYxUUrvDcLS%2FM5NOW7lj7aAUlim2D9bC3Idx8mOCGw4W8Munm%2F" rel="nofollow" target="_blank">月度国产数据库大事记合辑</a></li><li><a href="https://link.segmentfault.com/?enc=ThaHqap63rIv%2FWmEmWGDtQ%3D%3D.s3JbSs3JMuLNrPRoyLylUzBpVxfzboKGpK%2BhqMoneaLIsV4%2BNV6VWexJcm4jOViJ" rel="nofollow" target="_blank">中国数据库排行榜 - 月度解读</a></li><li><a href="https://link.segmentfault.com/?enc=elyXRW1WLKD0ZckztSRvVg%3D%3D.pLD%2BbSIvy8FEy%2F3ZAlQGwBUo%2BngYQg2VOjZPE7phxnQ%3D" rel="nofollow" target="_blank">国产数据库招投标信息汇总</a></li><li><a href="https://link.segmentfault.com/?enc=xGPrqq1KveQ%2BwKc2zjTASg%3D%3D.5S2u0du7gWxPWJ1dGMwdtoYvJuXsJpVthErkPZVAC%2BnKX91LiS9XrOJKek2PXGTd" rel="nofollow" target="_blank">【合辑】2025年数据库厂商年终总结</a></li></ul><p>点击阅读原文：<a href="https://link.segmentfault.com/?enc=8XY%2FHlGGk%2BUvHevglPpiCQ%3D%3D.81LpGI1%2F%2F9wYAgaJC3MxWwl%2FuaT0ayKHCR2F7bOq1pZukH1Iu%2Fl1zDKPL81loJW4f5usDCOLbr4ymFGsFMt2%2Bg%3D%3D" rel="nofollow" target="_blank">https://www.modb.pro/db/2019292973163438080</a></p><hr/><p>欲了解更多可浏览<a href="https://link.segmentfault.com/?enc=EOOvQ50so86DoGKHaazaCA%3D%3D.3J0BST9xn2PuYD3aSzmxiLf1AfO11FeA60r8RlGRFWQ%3D" rel="nofollow" target="_blank">墨天轮社区</a>，围绕数据人的学习成长提供一站式的全面服务，打造集新闻资讯、在线问答、活动直播、在线课程、文档阅览、资源下载、知识分享及在线运维为一体的统一平台，持续促进数据领域的知识传播和技术创新。</p><p>关注官方公众号： 墨天轮、 墨天轮平台、墨天轮成长营、数据库国产化 、数据库资讯</p>]]></description></item><item>    <title><![CDATA[先建“语义基座”，再谈运维智能！阿里云以 Operation Intelligence 定义 AIO]]></title>    <link>https://segmentfault.com/a/1190000047606326</link>    <guid>https://segmentfault.com/a/1190000047606326</guid>    <pubDate>2026-02-11 18:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：鸢玮</p><p>大模型的出现，给许多行业带来了颠覆性的改变，运维这个向来被视为稳定、保守的领域也不例外。虽然“AIOps”这个概念早在 2016 年由 Gartner 提出，但早期的智能运维更多是利用大数据和机器学习对传统运维流程进行效率上的提升。十年后的今天，大模型的强大能力，正推动着 AIOps 从辅助工具，演进为数智化转型中不可或缺的核心基础设施，让运维真正迈入智能化的深水区。</p><p><strong>阿里云云原生应用平台事业部总经理、资深技术专家周琦</strong>作为这一变革的深度参与者，对 AIOps 的本质有着深刻洞察。“AIOps 这个词已经被广泛使用，但我更倾向于用 Operation Intelligence 来定义它。”周琦在采访中强调，“它的核心是发现与沉淀运维操作中的智慧，让工程师从重复繁琐的劳动中解放出来，聚焦于更高价值的创造。”</p><h2>十年演进，重塑 AIOps 底层逻辑</h2><p>在传统的运维时代，更多依赖人工被动处理故障，效率低下；而后进入到自动化运维时代，借助工具实现任务自动化，缩短了故障恢复时间；到了小模型运维时代，通过机器学习实现异常检测与根因分析，运维也初步具备智能化特征；如今进入到大模型时代，运维才真正开始走向真正的智能化。</p><p>回顾 AIOps 过去十年的发展，周琦认为有两个关键转折点重塑了其底层逻辑。<strong>第一个转折点是通用大模型的到来。</strong> 在此之前，所谓的智能运维更多是通过垂类 AI 模型来解决告警治理、异常检测等单一、点状的问题。这种方式虽然有用，但难以规模化。大模型的通用特性，像是一个巨大的杠杆，将 AIOps 的能力从“点状解决”扩展到“面状全域覆盖”，凭借其强大的泛化能力可以应对千变万化的碎片化运维任务。</p><p><strong>第二个转折点则在于数据整合技术的突破。</strong> 过去，运维工作呈现高度碎片化特征，数据和引擎往往由不同供应商提供，形成了天然的数据孤岛。周琦表示，想要建设统一的 AIOps 体系，首先就要跨过这道鸿沟。如今，存储、计算与分析技术的进步，实现了异构数据的关联与串联，将分散在各个系统中的数据整合在一起，为全域智能运维奠定了坚实基础。</p><p>技术的演进也推动了企业对 AIOps 认知的转变。周琦观察到，早期，企业引入 AIOps 的核心诉求只是保障系统的稳定性，关注的焦点集中在故障修复、告警处理等基础功能方面。但现在，企业的需求维度大大拓宽了，安全性、可扩展性、延时、用户体验等这些过去容易被忽略的“隐性成本”，正受到前所未有的关注。这种认知的升级带来需求的延伸，AIOps 不再仅是运维工程师的工具，还需要满足企业管理者对系统成熟度、跨模块依赖关系等深层因素的考量，真正覆盖多角色、多维度的运营需求。<strong>真正的 AIOps，不是让人去适应工具，而是让工具主动理解人、服务人、成就人。</strong></p><h2>能力跃迁，让系统“能感知、会思考、可行动”</h2><p>大模型时代的到来，让 AIOps 具备了前所未有的智能化能力。那么，大模型究竟为运维领域带来了哪些质变？周琦用一个生动的比喻来解释，给 AI 装上“摄像头”。传统运维在很大程度上依赖于工程师的个体经验，一位经验丰富的老师傅心中通常有一张无形的系统拓扑图，知道哪里容易出问题、该如何分析。但这种宝贵的经验附着于个体，难以沉淀、复制和规模化。大模型的出现，结合阿里云构建的实时数据采集与分析引擎，相当于为 AI 赋予了感知能力，使其能够真正能“看懂”系统、“理解”故障、“思考”方案。</p><p>这带来了运维能力的根本性跃迁。机器不再是机械地匹配预设规则、触发阈值告警，而是开始能够“读懂”告警信息背后的语义，“理解”系统当前真实的运行状态，甚至能“归纳”历史故障的复杂模式，并主动生成可供执行的修复建议。为此，<strong>阿里云提出 Operation Intelligence 理念</strong>，把人的经验变成系统的智慧，把个体的直觉转化为组织的资产，让系统具备“类人决策”能力，周琦将阿里云践行的 Operation Intelligence 理念概括为三个层面的能力进化。</p><ul><li><strong>在感知层面</strong>，目标是突破传统监控中常见的“数据孤岛”，构建从终端设备到业务流程的全链路感知网络。</li><li><strong>在认知层面</strong>，关键在于融合大模型的通用理解能力与专用领域算法，将海量、原始的观测数据转化为可解释、可推理的系统关系图谱。</li><li>最终，<strong>在行动层面</strong>，通过模型与算法的协同驱动，实现自动化的处置闭环，推动运维从“人工救火”向“系统自愈”转变，通过高效的人机协同大幅提升整体运营效能。</li></ul><p>当然，大模型并非万能，针对大模型“幻觉”问题，阿里云设计了一套双重保障机制。周琦介绍说，在技术层面，通过强化多源数据的交叉验证，将数据采集、清洗、预处理等基础但繁重的工作交由传统工具完成，让大模型聚焦在最核心的推理环节，从源头减少幻觉产生的可能性。在应用层面，系统支持企业外挂自身的私有知识库，利用行业或企业特有的领域知识来补充和修正通用大模型可能存在的认知盲区，确保建议的准确性与合规性。</p><h2>构建智能运维新范式，解放人力聚焦高价值</h2><p>理想与现实之间总是存在挑战。周琦坦言，阿里云在自身的大规模实践中深刻体会到两大核心难题。其一是数据层面的挑战，包括异构系统形成的数据孤岛、数据洪流带来的存储与算力压力。其二是认知层面的挑战，不同团队、不同系统之间存在的“语义鸿沟”，以及对系统拓扑、故障根因逻辑链的理解不一致问题。</p><p>为了系统性地解决这些问题，<strong>阿里云将内部的实践经验产品化，形成了一套帮助企业在大模型时代构建智能运维新范式，并且在可观测产品中落地。</strong></p><p>这套架构分为三层，底层是以<strong>日志服务 SLS</strong> 为核心引擎构建的统一可观测数据平台，实现日志、指标、链路、事件等多类型数据的统一接入与存储。该引擎具备 EB 级存储规模和秒级千亿行查询能力，能轻松应对每天数百 PB 数据，在保障数据完整性的同时，综合成本较自建方案降低 50% 以上。更重要的是，它支持全栈、实时、无侵入的数据接入，覆盖从移动端到基础设施的 200 多种组件，让企业无需重构现有系统即可完成数据整合。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606328" alt="image" title="image"/></p><p>中层通过 <strong>UModel 统一模型</strong>构建 IT 系统的 “数字孪生”，这是阿里云可观测性产品的核心建模框架。UModel 基于本体论，提供了一套观测实体及实体关系的定义，覆盖从用户体验、应用服务、容器到底层基础设施的每一层表征。UModel 就像给整个 IT 系统建立一套通用语言词典，让应用、容器、网络等不同组件能用同一套语义对话，彻底告别“你说你的指标，我说我的日志”的沟通困境。周琦表示，这套标准化建模彻底消除了语义歧义，让不同部门、不同系统之间的协作更高效，也让运维人员的经验得以沉淀为可复用的组织资产，而非随人员流动流失。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606329" alt="image" title="image" loading="lazy"/></p><p>上层则是以 AI Agent 为智能核心，实现“工具适应人”的新范式。Agent 采用自然语言交互方式，支持全场景上下文感知，用户可在任意界面随时召唤，直接通过自然语言提问，无需掌握复杂的查询指令。AIOps Agent 基于阿里云可观测平台的多源数据采集、存储、分析能力，采用“统一数据平台 + UModel + 传统算法 + 生成式 AI”的混合处理架构，能够自主规划、调用工具、执行分析并反思优化，可以提供从自然语言交互到自动化巡检的全流程运维辅助能力，解决各类开放和未知的运维难题，将运维人员从重复的查询、分析工作中解放出来。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606330" alt="image" title="image" loading="lazy"/></p><blockquote><strong>周琦形象地说，</strong> “希望运维未来可以高度自动化，让 AIOps 把那些又脏又累的活儿做了。”这意味着，企业客户无需再投入大量宝贵的人力资源去完成数据采集、清洗、对齐等基础且繁琐的工程工作，阿里云的平台已经将这些“隐形工程”承担下来。</blockquote><p>如今，阿里云 AIOps Agent 已在 6000 多家企业落地，帮助大型企业客户实现故障 MTTR 从小时级降至小于 15 分钟。</p><p>对于企业而言，部署 AIOps 的终极价值远不止于减轻运维团队的负担，而是它能释放出宝贵的研发与创新资源，让技术人才能够专注于业务价值创造。同时，它也能帮助企业系统性地管理那些以往容易被忽视的隐性成本与合规风险，从长远角度优化 IT 投资的整体回报。</p><h2>开源引领生态共建，推动“技术平权”愿景</h2><p>阿里云深知，“语义基座”的价值在于普及，而开源与生态建设是实现“技术平权”的关键，更能让全行业运维人员共同成长。为此，阿里云在开源布局、标准建设和生态协同上持续发力，推动 AIOps 行业整体进步。</p><p><strong>在开源布局方面</strong>，阿里云计划将 UModel 统一语义语言开源至社区，并向 OpenTelemetry 社区贡献了探针、采集器等核心工具。这些工具已被滴滴等公司开发人员广泛采用，大幅降低了行业重复开发成本。其中，无侵入探针的代码已开源在 GitHub 上，经过众多企业实战验证，在安全性和稳定性上备受认可，让中小企业无需自行研发即可获得高质量的数据采集能力。</p><p><strong>在标准建设方面</strong>，阿里云正在构建 AIOps 成熟度 Benchmark 榜单，构建了从数据分析到复杂异常检测的分级标准，涵盖基础任务处理、异常发现、根因分析、隐形问题挖掘、自主修复等不同阶段，让企业能够清晰评估自身能力水平，找到明确的进阶路径。周琦表示，希望可以和业界一起共创，攻克智能运维领域的难题，推动 AIOps 标准落地，促进整个可观测性领域的快速发展。</p><p><strong>在生态协同方面</strong>，阿里云通过大赛联动高校、企业，将工业界高频问题转化为赛题，促进产学研深度融合。通过大赛的方式，阿里云将标准 Benchmark 和真实场景赛题提供给参赛者，让高校学生、企业开发者都能在实战中提升能力，同时为行业贡献创新方案。</p><p>周琦表示，阿里云通过开放共建的模式，打破技术壁垒，让不同规模、不同行业的企业都可以落地 AIOps，实现“技术平权”，让中小企业也能调用顶级“隐形工程师团队”，让每个运维人员都能借助智能工具发挥更大价值，向“智能运营专家”演进。</p><h2>未来趋势：自主 Agent 协同，运维能力重构</h2><p>展望未来，周琦从不同时间维度来做出判断。短期来看，低风险任务将实现全自动化闭环，如 IP 封禁、简单扩容等操作可由 AI 自主完成，而重要操作仍保留人机协同决策模式，确保系统安全。同时，多角色 Agent 协同雏形将逐步显现，运维、安全、成本控制等不同领域的 Agent 将共享统一数据视图，提升跨域运营效率。</p><p>中长期来看，AIOps 将与 AI Coding、测试等环节深度打通，最终形成开发、测试到运维的全生命周期智能闭环。周琦解释道，AI Coding 目前在开发态做的非常有效，但从一个演示应用到企业级系统，部署后能稳定运行，还需要很长时间。“我们希望能够将 AI Coding 和 AIOps 串联，实现全局优化。让应用系统不光能跑起来，还能跑得更好、更稳，把运行态的状况实时反馈给 AI Coding。”</p><p>技术的演进必然带来运维人员角色与能力的重构。周琦表示，过去，运维人员是“救火队员”，整天忙于处理各类故障；未来，他们将转变为“系统教练”，而他们的核心能力不再是重复的操作经验，而是架构设计、业务理解、多维度决策等高阶能力。未来的运维人员需要平衡安全、成本、合规、可扩展性等多重诉求，专注于系统长期价值的优化。</p><h2>结语</h2><p>在阿里云可观测团队的定义中，智能运维是一场深刻的范式转移。它以大模型为驱动，基于统一的数据平台与领域知识模型，实现了从“人适应工具”到“将人类创造力注入系统智能之中”的本质转变，最终构建起数据、认知与行动闭环融合的智能体系。</p><p>纵观这场由 Operation Intelligence 引领的变革，其核心在于将运维智慧从依赖个人的隐性经验，沉淀为可复制、可迭代的组织数字资产，推动工程师从重复劳作中解放，实现价值的创造性升维。</p><p>阿里云始终致力于通过自身实践与生态共建，让任何规模的企业都能获得顶级“隐形工程师”团队的支持，在数智化浪潮中聚焦核心创造，实现个人与企业的共同成长。</p><p>正如周琦所言，“未来的运维竞争，将不再是工具的竞争，而是人的创造力与战略眼光的竞争”。当统一语言打通系统与智能的鸿沟，技术真正服务于人的价值释放，这场变革便不止于运维效率的提升，更将成为企业创新加速、行业持续进步的核心动力。</p>]]></description></item><item>    <title><![CDATA[指标平台选型的关键——无宽表下的查询性能如何保障？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047606360</link>    <guid>https://segmentfault.com/a/1190000047606360</guid>    <pubDate>2026-02-11 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=d5QFwngLGJ4RAE55Ruxo3g%3D%3D.GX5W52w1rsVRJvds98Fvm4USHWsf4SvQ6VmT5LLCTbsDnGU9UDPlj9NAahJ2nfkV2EoDBbIv75sE7O03nsDm7TLPHjfp1IUWd%2BQ2A8y7NcJGU8ja4Wh7cU1BDqzdIlgSLnHIoH%2BPnUCNu0dDphuivQ%3D%3D" rel="nofollow" target="_blank">《指标平台选型关键：Aloudata CAN 如何保障无宽表下的查询性能》</a>转载请注明出处。</blockquote><p><strong>摘要</strong>：本文深入探讨在数据工程中，企业进行指标平台选型时面临的核心挑战——如何在摒弃物理宽表、保障业务灵活性的同时，实现海量数据下的高性能查询。文章系统分析了自研高性能指标平台必须跨越的三大技术难关：统一语义解析、智能物化加速与开放生态适配，并对比了基于 NoETL 语义编织技术的 Aloudata CAN 如何通过声明式策略与自动化引擎，实现百亿级数据秒级响应的成熟方案，为技术决策者提供清晰的 Build vs Buy 评估框架。</p><p>许多企业在指标平台选型时，往往陷入一个根本性的认知误区：认为这仅仅是选择一个“指标字典”或元数据目录，用于记录和查询指标定义。这种认知导致许多自研项目停留在构建一个静态的 CRUD 系统层面。</p><p>然而，一个真正能够支撑企业级数据分析的指标平台，其核心是一个 动态智能计算引擎。它不仅要“记住”指标的定义，更要能“理解”复杂的业务语义，并“自动执行”从 DWD 明细数据层到最终指标结果的复杂计算过程。性能保障，尤其是高并发、复杂查询下的秒级响应能力，是这一引擎的基石，而非附属功能。</p><p>“在高并发、高吞吐量的数据分析场景下，简单的事情往往变得不那么简单。一个业务逻辑简单的指标大盘，一旦面临大促或年终数据汇总等高峰期，就会出现卡顿甚至崩溃的情况。” —— 从传统 Cube 到现代化指标体系：物化视图驱动的指标平台升级之路</p><h2>技术难关一：统一语义解析——性能的“第一道坎”</h2><p>自研平台面临的第一道坎，是如何将业务人员定义的指标（如“华东区上月高价值用户的日均消费金额”），精准、高效地解析为底层数据引擎可执行的查询计划。这一步直接决定了查询性能的基线。</p><ul><li>静态解析的局限：传统模式依赖预建的物理宽表或 Cube。查询路径被固化，一旦业务需求超出预建模型的范围（例如，需要按新的维度组合下钻），要么无法响应，要么需要 DBA 重新开发宽表，周期以周计，完全丧失灵活性。</li><li>动态解析的挑战：要摆脱宽表束缚，必须构建强大的 语义引擎 (Semantic Engine)。它需要在逻辑层构建一个“虚拟业务事实网络”，通过声明式策略定义表间关联。当用户组合指标与维度时，引擎必须实时进行语义推导，生成最优的、包含正确关联和聚合逻辑的 SQL。这涉及到复杂的查询优化、谓词下推、公共子表达式识别等数据库核心技术，工程复杂度极高。</li></ul><h2>技术难关二：智能物化加速——性能保障的“工程黑洞”</h2><p>即使解决了语义解析，面对百亿级明细数据的即席查询，性能依然难以达标。此时，物化加速（预计算）成为必选项。然而，这正是自研与采购成熟方案的核心分水岭，一个深不见底的“工程黑洞”。</p><ol><li>手动模式的困境  <br/>传统做法是 DBA 手动创建和维护物化视图或汇总表。但这带来两个致命问题：</li></ol><ul><li>灵活性丧失：为每个可能的查询组合创建物化视图，会导致“物化视图爆炸”，存储成本失控。</li><li>运维成本爆炸：需要手动管理刷新策略（全量/增量）、处理数据一致性、监控任务失败。当源表数据发生更新或删除时，增量计算的逻辑复杂性呈指数级上升。</li></ul><p>“影响增量计算性能的因素极为复杂，包括查询算子组合复杂度、源表数据变化模式（Append Only vs Update/Delete）、变化频率等。能应对各种业务场景的多方面因素是一个极具挑战的工程难题。” —— 破解千亿数据处理痛点：快手基于增量计算解决时效、成本</p><ol start="2"><li>智能物化加速引擎：从“人治”到“自治”  <br/>真正的性能保障，需要一套 声明式策略驱动的智能物化加速引擎。这正是 Aloudata CAN 的核心壁垒。</li></ol><ul><li>声明式策略：用户无需关心具体物理表，只需在界面声明对哪些高频查询的“指标+维度”组合进行加速，以及期望的刷新时效（如 T+1 或准实时）。</li><li>自动化执行与运维：系统根据策略自动编排物化任务，智能选择生成明细加速、汇总加速或结果加速表，并负责全生命周期的调度、监控、失败重试和血缘管理。</li><li>智能路由与透明加速：查询发生时，语义引擎会自动进行 SQL 改写，将查询智能路由到已存在的最优物化结果上，对用户完全透明。</li></ul><p>权威背书：某全球连锁餐饮巨头（麦当劳中国）基于 Aloudata CAN，在 百亿级数据规模 下，实现了核心业务查询 P90 &lt; 1s 的性能，日均支撑百万级 API 调用，覆盖 30+ 业务场景。</p><h2>技术难关三：开放生态适配——性能服务的“最后一公里”</h2><p>即使计算引擎性能卓越，若无法被业务系统便捷消费，就会形成新的数据孤岛。自研平台需要设计一套标准、稳定、高性能的服务接口，并确保在所有消费端指标口径绝对一致，这同样是巨大的工程投入。</p><ul><li>接口标准化：需要提供标准的 REST API 和 JDBC 接口，以适配企业内部多样的 BI 工具（如 Tableau、Power BI）、AI 应用及自建业务系统。</li><li>性能与稳定性：接口层本身不能成为性能瓶颈，需要处理高并发、连接池管理、查询超时与熔断。</li><li>生态集成：与主流 BI 工具（如 FineBI、Quick BI）的深度集成，以及提供像 WPS 插件这样的办公场景嵌入能力，都需要长期的研发和合作投入。</li></ul><h2>TCO 分析：自研性能保障的“隐形高利贷”</h2><p>当企业决定自研以攻克上述“鬼门关”时，必须算清一笔总拥有成本（TCO）的“隐形账”。</p><table><thead><tr><th>维度</th><th>自研路径</th><th>采购 Aloudata CAN</th></tr></thead><tbody><tr><td>初期投入</td><td>高。组建专项团队（架构、研发、测试），至少 6-12 个月起。</td><td>低。主要为软件许可和实施服务成本。</td></tr><tr><td>三年 TCO</td><td>极高。持续研发、性能调优、运维人力成本叠加，且存在项目失败风险。</td><td>可控。固定许可费+运维服务费，无额外研发人力负担。</td></tr><tr><td>性能达成时间</td><td>长。从零到一构建稳定、高性能的引擎，通常以“年”为单位。</td><td>短。开箱即用，在客户环境中已验证的架构，可快速达到 SLA。</td></tr><tr><td>团队技能要求</td><td>极高。需要顶尖的数据库内核研发、查询优化、分布式系统人才。</td><td>中。侧重业务建模与配置，无需深入引擎底层。</td></tr><tr><td>业务风险</td><td>高。研发周期长，可能错失市场时机；系统不稳定直接影响业务决策。</td><td>低。基于成熟产品，风险可控，可快速赋能业务。</td></tr></tbody></table><p>核心结论：自研高性能指标平台是一笔长期的“技术高利贷”，其隐形成本（机会成本、人才成本、时间成本）往往远超采购成熟方案。作为 Gartner 中国数据编织代表厂商，Aloudata CAN 将经过验证的语义编织与智能物化加速能力产品化，让企业能将稀缺的研发资源聚焦于业务创新，而非重复造轮子。</p><h2>决策矩阵：何时该自研，何时该采购？</h2><p>企业应根据自身情况，参考以下框架决策：</p><p>1、坚定选择自研：</p><ul><li>拥有顶尖的数据库内核研发团队，且将“高性能计算引擎”作为核心战略产品。</li><li>业务场景极为特殊，市面上没有任何方案能满足其定制化需求。</li><li>对技术掌控有绝对要求，且不计较长期投入和成本。</li></ul><p>2、强烈建议采购（如 Aloudata CAN）：</p><ul><li>核心目标是快速解决业务的数据分析需求，提升决策效率。</li><li>数据规模已达亿/十亿级，且对查询性能（秒级响应）有明确要求。</li><li>缺乏或难以组建具备数据库内核研发能力的团队。</li><li>希望统一企业指标口径，并面向多种 BI 工具和 AI 应用提供一致服务。</li><li>关注总体拥有成本（TCO），希望将 IT 投入转化为明确的业务价值。</li></ul><h2>常见问题（FAQ）</h2><h4>Q1: 不建物理宽表，如何保证复杂查询的秒级响应？</h4><p>Aloudata CAN 通过 NoETL 语义编织 技术，在逻辑层构建“虚拟业务事实网络”。同时，其 智能物化加速引擎 支持声明式物化策略，自动为高频查询组合生成并维护最优的物化结果（明细加速、汇总加速）。查询时，语义引擎自动进行 SQL 改写和智能路由，透明命中物化结果，从而实现百亿级数据 P90 &lt; 1s 的查询性能。</p><h4>Q2: 与传统数据库的物化视图相比，Aloudata CAN 的物化加速有何不同？</h4><p>传统数据库物化视图需要 DBA 手动创建、维护和选择刷新策略，是“手动优化”。Aloudata CAN 的物化加速是 声明式、自动化 的。用户只需关注业务逻辑（定义指标），系统根据查询模式自动决策物化什么、如何物化、何时刷新，并负责全生命周期运维，实现了从“人治”到“自治”的跃升，大幅降低使用和维护门槛。</p><h4>Q3: 选择指标平台时，除了查询性能，还应重点评估哪些方面？</h4><p>除了查询性能，还需重点评估：1) 指标定义与管理能力：是否支持复杂业务逻辑（如指标转标签、自定义周期）；2) 口径一致性：是否为企业提供唯一可信指标源；3) 开放性与集成能力：是否提供标准 API/JDBC，支持各类 BI 工具和 AI 应用；4) AI 原生适配：是否具备 NL2MQL2SQL 等能力，根治 AI 问数幻觉；5) 总拥有成本(TCO)：包括采购成本、实施效率与长期运维开销。</p><h4>Q4: 我们的数据量不大，也需要考虑这种无宽表的指标平台吗？</h4><p>即使当前数据量不大，采用无宽表的现代化指标平台（如 Aloudata CAN）也具有战略价值。它能帮助企业在数据增长初期就建立 统一的指标语义层和治理规范，避免未来因口径混乱、宽表泛滥而导致的“先乱后治”高成本重构。同时，其敏捷的配置化开发模式，能极大提升数据响应速度，赋能业务创新，是一种面向未来的“弯道超车”架构选择。</p><h2>核心要点</h2><ol><li>性能是引擎，不是功能：现代指标平台的核心是一个动态智能计算引擎，性能保障是其基石，选型时需首先评估其在高并发、复杂查询下的能力。</li><li>智能物化加速是分水岭：手动创建和维护物化视图/宽表无法平衡灵活性与性能。真正的解决方案是基于声明式策略的智能物化加速引擎，实现自动化运维与透明路由。</li><li>开放生态是价值出口：平台必须通过标准接口（API/JDBC）无缝集成现有 BI 与 AI 生态，避免成为新的数据孤岛，确保性能价值直达业务。</li><li>自研 TCO 远超想象：自研高性能指标平台涉及长期的研发、调优和运维投入，其总拥有成本（TCO）和机会成本往往被严重低估。</li><li>采购成熟方案是高效路径：对于绝大多数企业，采购像 Aloudata CAN 这样经过大规模实践验证的成熟方案，是快速获得高性能、统一语义层并控制总成本的最优路径。</li></ol>]]></description></item><item>    <title><![CDATA[快速上手：Chrome/Firefox/Edge 浏览器 Canvas 指纹防护实战 ToDetec]]></title>    <link>https://segmentfault.com/a/1190000047605695</link>    <guid>https://segmentfault.com/a/1190000047605695</guid>    <pubDate>2026-02-11 17:13:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在日常上网过程中，你可能听说过“浏览器指纹检测”这个概念，但具体它是怎么工作的，尤其是“浏览器Canvas 指纹”，很多人还是一头雾水。</p><p>今天，就给大家聊聊什么是 Canvas 指纹，它对隐私的威胁，以及如何在 Chrome、Edge、Firefox 上有效防护，顺便分享一些实用工具和操作技巧。</p><h3>什么是浏览器Canvas 指纹？</h3><p>简单来说，浏览器Canvas 指纹是浏览器指纹检测的一种方式。它通过浏览器渲染一段隐藏的图形（Canvas），然后读取渲染结果的像素信息。每台电脑、每个浏览器的硬件和软件环境不同，比如显卡型号、操作系统字体、驱动版本等等，这些细微差异会让 Canvas 渲染出的图像“唯一化”，从而生成一个“指纹”，可以用来追踪你的上网行为。</p><p>也就是说，即便你关闭了 Cookie，网站仍然可以通过 Canvas 指纹识别你是不是同一位访问者。听起来有点吓人对吧？这也是很多隐私保护爱好者特别关注的点。</p><h3>浏览器指纹检测的危害</h3><p>浏览器指纹检测不仅用于广告追踪，还可能被用来：</p><p>精准识别用户身份，即便切换 IP 或隐身模式也能追踪</p><p>个性化广告投放，让你的隐私被过度利用</p><p>防止访问某些网站或限制功能，比如一些服务会根据指纹限制访问次数</p><p>所以，了解 Canvas 指纹的工作原理，并学会防护，确实很必要。<br/><img width="723" height="485" referrerpolicy="no-referrer" src="/img/bVdnUzU" alt="" title=""/></p><h3>如何在 Chrome/Edge/Firefox 上防护 Canvas 指纹</h3><ol><li>使用隐私浏览器或增强隐私插件</li></ol><p>Firefox：Firefox 对隐私保护比较友好，可以在 about:config 中开启 privacy.resistFingerprinting，它会主动对 Canvas 指纹进行干扰，降低被唯一识别的风险。</p><p>Chrome / Edge：可以安装类似 uBlock Origin、Privacy Badger 的插件，有些插件提供 Canvas 指纹保护功能，会在 Canvas 渲染请求时提示你是否允许。</p><p>温馨提示：完全屏蔽 Canvas 指纹可能会导致某些网站功能异常，比如图形验证码或绘图功能。建议按需开启。</p><ol start="2"><li>修改浏览器 Canvas 行为</li></ol><p>部分浏览器插件可以随机化 Canvas 指纹，或者在读取 Canvas 数据时注入“噪声”，从而干扰指纹生成。例如：</p><p>CanvasBlocker（Firefox / Chrome）：这是一个专门防护 Canvas 指纹的插件，可以随机化你的 Canvas 输出，阻止网站准确识别你的浏览器。</p><p>Trace（Chrome / Edge）：提供多种防护选项，包括 Canvas、WebGL 和字体指纹保护。</p><p>通过这些插件，你可以在保证上网体验的前提下，有效降低 Canvas 指纹被利用的风险。</p><ol start="3"><li>使用隐身或隔离浏览模式</li></ol><p>虽然隐身模式不能完全防止 Canvas 指纹，但结合插件使用，可以大幅降低追踪成功率。此外，多账户浏览器或容器插件（Firefox 的 Multi-Account Containers）也可以隔离网站数据，避免跨站点追踪。</p><ol start="4"><li>检测你的浏览器指纹安全性</li></ol><p>防护前，最好先知道自己的浏览器有多“容易被识别”。ToDetect指纹查询：</p><p>查看自己浏览器的 Canvas 指纹信息</p><p>检测是否存在其他指纹威胁（WebGL、字体、插件等）</p><p>评估当前防护措施的效果</p><p>操作也很简单，只需访问网站，点击检测即可生成报告。这样你可以直观了解防护是否成功。</p><h3>总结</h3><p>Canvas 指纹是现代浏览器指纹检测中一个比较精准的手段，它能在无 Cookie 情况下追踪用户。想要在 Chrome、Edge、Firefox 上防护 Canvas 指纹，主要方法就是：</p><p>使用隐私浏览器或增强隐私插件</p><p>随机化或干扰 Canvas 输出</p><p>使用隔离浏览或容器模式</p><p>借助 ToDetect指纹查询工具 评估防护效果</p><p>如果你平时比较注重隐私，上述方法结合起来使用，会显著降低浏览器被跟踪的概率。毕竟，保护自己的数字足迹，是每一个现代网民都该掌握的技能。</p>]]></description></item><item>    <title><![CDATA[别再折腾配置了！OpenCloudOS推出OpenClaw“极速版”脚本 OpenCloudOS ]]></title>    <link>https://segmentfault.com/a/1190000047605697</link>    <guid>https://segmentfault.com/a/1190000047605697</guid>    <pubDate>2026-02-11 17:12:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在上篇文章（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）中，我们介绍了如何基于OpenCloudOS 9 安装配置OpenClaw，并接入企业微信等IM，让你最终拥有一位 7x24 的“AI全能助理”。一些用户看完文章后跃跃欲试，但一上手实操，却被繁琐的配置劝退了：</p><p>● “Node.js 版本不对，报错了……”</p><p>● “GitHub 连不上，插件装不下来……”</p><p>● “我想接企业微信，怎么还得手动改配置文件？”</p><p>大家的痛点，OpenCloudOS社区听到了！</p><p>为了让大家把精力从“装环境”转移到“用 AI”上，带来了 OpenClaw x OpenCloudOS 2.0 部署方案 。这一次，我们推出了一键安装脚本，用户只要一条命令就能极速体验OpenClaw。</p><h2><strong>一、 新版“极速部署脚本”做了什么？</strong></h2><p>1. 环境全自动适配 ：自动检测系统环境，帮你搞定 Node.js 24 等所有底层依赖，不再担心版本冲突。</p><p>2. 国内 IM 原生支持 ：不再需要满世界找插件。 企业微信、QQ、飞书、钉钉 ，你想用哪个，脚本直接帮你装好。</p><p>3. 网络与源优化 ：针对国内网络环境做了深度适配，下载更稳、速度更快。</p><p>简单来说：以前需要 30 分钟的手工操作，现在只需要 1 分钟等待。</p><h2><strong>二、极速版上手指南</strong></h2><h4><strong>2.1 安装 Openclaw 及IM相关插件</strong></h4><h5><strong>场景A：我全都要（推荐）</strong></h5><p>如果你希望 OpenClaw 能连接企业微信、QQ、飞书、钉钉等所有国内主流 IM，可直接运行如下脚本。</p><pre><code class="auto"># 默认完整安装所有国内 IM 插件
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash</code></pre><p>备注：因一键安装脚本会执行较多依赖并启动OpenClaw安装，所以整个安装过程大概耗时15-20min左右，中途请不要终止或推出。</p><p>一键安装脚本代码请见：<a href="https://link.segmentfault.com/?enc=onCDwfn1UjcIq9qOH7y77A%3D%3D.xblJS0304C9yxtL%2BL40pBCjtwJbentI9eUkRk%2BE4ACam%2BYv7MeEFtuWqqG%2FUQiy1mdjnL%2FdiUMWT0NPsdyUxadAsi5g2%2FO898BjgFonMhZY%3D" rel="nofollow" target="_blank">scripts/deploy\_opneclaw.sh · OpenCloudOS/web-extra - Gitee.com</a></p><h5><strong>场景B：我只需要特定渠道</strong></h5><p>如果你只想安装某个指定IM（如企业微信和QQ），不想安装多余插件，可直接运行如下脚本。安装耗时：5-10min</p><pre><code class="auto"># 安装指定IM（下方以同时安装企业微信和QQ为例）：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --plugins wecom,qq</code></pre><h5><strong>场景C：纯净版安装</strong></h5><p>如果你只需要 OpenClaw 的核心功能（比如只在终端使用，或者通过 Web 界面交互），不需要连接任何国内IM，可直接运行如下脚本。安装耗时：2-3min</p><pre><code class="auto"># 跳过所有 IM 插件安装：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --skip-plugins</code></pre><pre><code class="auto"># 手动打开交互命令
openclaw onboard</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605701" alt="image.png" title="image.png"/></p><h4><strong>2.2 配置 OpenClaw</strong></h4><p>OpenClaw配置流程较多，OpenCloudOS已在上篇内容（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）进行了详细展示，这篇不再赘述。唯一不同的是，如您在2.1章节中执行了指定IM安装（如企业微信、飞书等），则会在IM配置环节的列表中，看见该可选项。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605702" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>2.3 OpenClaw 运行状态确认</strong></h4><pre><code class="auto"># 查看openclaw是否在后台运行
openclaw health
# 查看模型状态，是否连上了大模型
openclaw models list
# 查看聊天通道,比如qq，企业微信等
openclaw channels list</code></pre><h2><strong>三、接入IM及实际应用演示</strong></h2><p>接下来，我们将详解如何配置企业微信、QQ、飞书、钉钉.</p><h4><strong>3.1 接入企业微信</strong></h4><p>OpenClaw 原生基本只支持国外社交软件，可以通过插件的方式来支持国内的社交软件。这里我们以企业微信为例，演示接入教程。</p><p>注意要接入企业微信有两个条件，首先你的clawdbot安装在有公网ip的机器上，2.你是企业管理员能创建APP或者机器人</p><pre><code class="auto"># 查看企业微信插件运行是否加载
openclaw plugins list | grep -i wecom</code></pre><p>企业微信插件使用目录</p><p><a href="https://link.segmentfault.com/?enc=M4unXfzxKC1wlWK0ec%2BXtA%3D%3D.bkn4FrxKLgQfkD83StSJsO9DYhe6Qa8Nv5sAwEOTSj%2FqAJYNKGq8XjjAieQedzYRJqDoQES%2FUTAQiyWh1imRfg%3D%3D" rel="nofollow" target="_blank">@marshulll/openclaw-wecom - npm</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605703" alt="image.png" title="image.png" loading="lazy"/><br/>接下来需要在企业微信里创建一个一个应用，这一步需要<a href="https://link.segmentfault.com/?enc=w24hTWU05IaZHoxFDYy7jA%3D%3D.1BTne%2FRQWyv5XVwdV2Tfkp8T4vhsgSPJgrDJNELX5u1SnYu7xQEoFMAKep0bUhkJ" rel="nofollow" target="_blank">首页 - 企业微信开发者中心</a>先在这里创建一个应用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605704" alt="image.png" title="image.png" loading="lazy"/><br/>选择个人<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605705" alt="image.png" title="image.png" loading="lazy"/><br/>配置企业微信应用相关信息</p><p>首先获取如下信息</p><p>1.  登录 <a href="https://link.segmentfault.com/?enc=Ka0Sfy1zvkOThQOkLrExXA%3D%3D.hYiLdHJNfU3Oo%2FNXMunamF56dyNJnhnjvp6PkmpefJI2%2F8tDMMrJfBxObQ1NgRp7" rel="nofollow" target="_blank">企业微信管理员后台</a></p><p>2.  在"我的企业"中查看 企业ID (CorpID)<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605706" alt="image.png" title="image.png" loading="lazy"/></p><p>3.  进入"应用管理" → 选择或创建应用</p><p>4.  在应用详情页获取：AgentId：应用ID</p><p>Secret(corpsecret)：点击"查看Secret"获取<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605707" alt="image.png" title="image.png" loading="lazy"/></p><p>5.  在"接收消息"设置中获取：Token：点击"随机获取"</p><p>EncodingAESKey：点击"随机获取"<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605708" alt="image.png" title="image.png" loading="lazy"/><br/><strong>然后在部署了openclaw的服务器上输入如下命令：</strong></p><pre><code class="auto"># 企业微信应用配置（必需）
# 这里配置的是 app 模式，可以参考插件使用指南换成bot或者both模式
openclaw config set channels.wecom.mode "app"
openclaw config set channels.wecom.defaultAccount "app"
openclaw config set channels.wecom.accounts.app.mode "app"
openclaw config set channels.wecom.accounts.app.webhookPath "/wecom/app"
openclaw config set channels.wecom.accounts.app.corpId "你企业ID"
openclaw config set channels.wecom.accounts.app.corpSecret "应用secret"
openclaw config set channels.wecom.accounts.app.agentId "你的应用ID"
openclaw config set channels.wecom.accounts.app.callbackToken "你设置的应用的token"
openclaw config set channels.wecom.accounts.app.callbackAesKey "你设置的应用的aes-key"
openclaw config set channels.wecom.enabled true
 
# 设置openclaw链接公网
openclaw config set gateway.bind lan
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p>配置应该是这样的，我只配置了app，如果你配置了bot会更丰富一点<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605709" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605710" alt="image.png" title="image.png" loading="lazy"/><br/>如上执行后，回到企业微信app管理界面，点击保存，企业微信会回发送token和AESKey去和openclaw服务器进行匹配<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605711" alt="image.png" title="image.png" loading="lazy"/></p><p>如果匹配成功界面如下<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605712" alt="image.png" title="image.png" loading="lazy"/><br/>在企业微信里找到相关应用，直接和他聊天<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605713" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到 Clawdbot 确实识别到了相关的用户和请求<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605714" alt="image.png" title="image.png" loading="lazy"/><br/>让 ClawdBot 创建一个定时任务：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605715" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到确实创建完成了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605716" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.2 接入QQ</strong></h4><p>QQ更方便个人用户使用，OpenCloudOS也提供一个接入QQ的场景。</p><p>QQ插件地址<a href="https://link.segmentfault.com/?enc=SJ6GDjY%2FbiFNnizbhJjIow%3D%3D.wpMB9sbXKjNGQvEyZZyoBwtWRX8g3VJKFWb0Rruk34K5fyxqK25EpOVq%2FPnxNuYR" rel="nofollow" target="_blank">https://github.com/sliverp/qqbot#</a></p><pre><code class="auto"># 一件安装脚本已经安装了qq相关插件
# 查看当前的脚本
openclaw plugins list | grep qq
 
# 如果你开始没安装qq插件可以执行如下命令安装
openclaw plugins install https://github.com/sliverp/qqbot.git</code></pre><p>创建QQ机器人：</p><p>访问 <a href="https://link.segmentfault.com/?enc=jjyNDI%2Fqn%2F5s3fR8naIHYw%3D%3D.RiV9oCLjjuUluVMSUBPRbV9dFZX5J4z2OrqiewlgD30%3D" rel="nofollow" target="_blank">QQ 开放平台</a></p><p>创建机器人应用</p><p>获取 AppID 和 AppSecret（ClientSecret）</p><p>Token 格式为 AppID:AppSecret，例如 102146862:Xjv7JVhu7KXkxANbp3HVjxCRgvAPeuAQ<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605717" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605718" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">#方式一：交互式配置,选择 qqbot，按提示输入 Token
openclaw channels add
#方式二：命令行配置
openclaw channels add --channel qqbot --token "AppID:AppSecret"
# 示例
openclaw channels add --channel qqbot --token "102146862:xxxxxxxx"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605719" alt="image.png" title="image.png" loading="lazy"/><br/>配置好后在qq开发平台里的，沙箱配置里先点击添加成员再扫描二维码就能和ClawdBot沟通，并安排他工作了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605720" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605721" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.3 接入飞书</strong></h4><p>飞书插件地址</p><p><a href="https://link.segmentfault.com/?enc=34Ee5DR7U2AFtU9R3bTG1Q%3D%3D.k0YXbPbj4ccvLi8igNVRuZa1vm2keV7XD93FagP5O%2BFU2SeOcuV2JBuKPfLsZ%2B2v" rel="nofollow" target="_blank">GitHub - m1heng/clawdbot-feishu</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable feishu
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install @m1heng-clawd/feishu</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605722" alt="image.png" title="image.png" loading="lazy"/><br/>飞书应用（机器人）配置</p><p>进入飞书应用中心：<a href="https://link.segmentfault.com/?enc=2s4OZ%2BC23MertWnPl7gzHg%3D%3D.JmoZSJtUuLQQSDFj6Te%2BfphdTp%2FsyvEEjTuyp64PhF4%3D" rel="nofollow" target="_blank">开发者后台 - 飞书开放平台</a></p><p>创建企业自建应用</p><p>路径： 创建应用 → 企业自建应用</p><p>  基础信息按提示填写即可（名称、描述等），完成后进入应用详情页。</p><p>配置应用权限</p><p>进入 权限管理，添加以下权限（按插件文档要求）：</p><p>必要权限</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im:message</td><td>消息</td><td>发送和接收消息</td></tr><tr><td>im:message.p2p_msg:readonly</td><td>私聊</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im:message.group_at_msg:readonly</td><td>群聊</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im:message:send_as_bot</td><td>发送</td><td>以机器人身份发送消息</td></tr><tr><td>im:resource</td><td>媒体</td><td>上传和下载图片/文件</td></tr></tbody></table><p>可直接复制如下配置直接导入</p><pre><code class="auto">{
  "scopes": {
    "tenant": [
      "bitable:app:readonly",
      "contact:user.base:readonly",
      "docx:document",
      "docx:document.block:convert",
      "docx:document:create",
      "docx:document:readonly",
      "drive:drive",
      "drive:drive:readonly",
      "im:chat:readonly",
      "im:message",
      "im:message.group_at_msg:readonly",
      "im:message.group_msg",
      "im:message.p2p_msg:readonly",
      "im:message:send_as_bot",
      "im:resource",
      "wiki:wiki:readonly"
    ],
    "user": [
      "contact:contact.base:readonly"
    ]
  }
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605723" alt="image.png" title="image.png" loading="lazy"/><br/>进入凭证与基础信息 页面，记录 App ID / App Secret 同步更新到 openclaw 配置中<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605724" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">openclaw config set channels.feishu.appId "你的appid"
openclaw config set channels.feishu.appSecret "你的app_secret"
openclaw config set channels.feishu.enabled true
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605725" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605726" alt="image.png" title="image.png" loading="lazy"/><br/>然后进入事件与回调界面，订阅方式选择长链接<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605727" alt="image.png" title="image.png" loading="lazy"/><br/>之后点击右下角的添加事件</p><p>添加如下事件</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im.message.receive_v1</td><td>接收消息（必需）</td><td>发送和接收消息</td></tr><tr><td>im.message.message_read_v1</td><td>消息已读回执</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im.chat.member.bot.added_v1</td><td>机器人进群</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im.chat.member.bot.deleted_v1</td><td>机器人被移出群</td><td>以机器人身份发送消息</td></tr></tbody></table><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605728" alt="image.png" title="image.png" loading="lazy"/></p><p>之后点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605729" alt="image.png" title="image.png" loading="lazy"/><br/>在飞书里直接搜索 你机器人的名字就能和他聊天了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605730" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.4 接入钉钉</strong></h4><p>钉钉插件地址</p><p><a href="https://link.segmentfault.com/?enc=mgfhDzZJ7kmqByBVMBhYUA%3D%3D.jJIjBFGmfKFbLPY%2BG8bZIswSWbfroMwQyn5CZfwtR61nGTFNxTnNnVGSmmePWMJSDmSucGhxYZCp2KzRdUkh6Q%3D%3D" rel="nofollow" target="_blank">GitHub - soimy/openclaw-channel-dingtalk: A dingtalk bot channel plugin for clawdbot</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable dingtalk
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install https://github.com/soimy/clawdbot-channel-dingtalk.git</code></pre><p>前往<a href="https://link.segmentfault.com/?enc=5Tq1vpHrgRs8uYSokC0G5w%3D%3D.5EcpqxB%2FvjaCw26FCO90oUK0%2BNw9gI3fjYDoRmRy0a8%3D" rel="nofollow" target="_blank">钉钉开发者后台</a>，使用具有管理员权限的账号进行登录。选择</p><p>创建应用-&gt;填写应用名称 和 描述 -&gt; 再点击左侧“添加应用能力” -&gt; 选择 “机器人"-&gt;完善机器人配置 -&gt; 消息接受模式选择 stream模式 -&gt; 点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605731" alt="image.png" title="image.png" loading="lazy"/><br/>然后点击坐上角的 "凭证与基础信息" 找到Client ID与Client Secret两个参数<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605732" alt="image.png" title="image.png" loading="lazy"/><br/>然后再进入发布界面，点击保存<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605733" alt="image.png" title="image.png" loading="lazy"/><br/>然后在服务器上输入</p><pre><code class="auto">openclaw config set channels.dingtalk.clientId "你的ClientID"
openclaw config set channels.dingtalk.clientSecret "你的Client Secret"
openclaw config set channels.dingtalk.enabled true
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels.dingtalk</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605734" alt="image.png" title="image.png" loading="lazy"/><br/>接着你就可以在钉钉里搜索到你的应用并让他给你干活了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605735" alt="image.png" title="image.png" loading="lazy"/></p><p>参考链接</p><p><a href="https://link.segmentfault.com/?enc=3J%2BLGc0g4i3s9rOIm7gvAA%3D%3D.fpPlpSbELX1EFKt8LdhwEjpo6g7pf7Kx3UYnPGAszlZNEM%2FcOZofpa9Hw9aq9eXL" rel="nofollow" target="_blank">Node.js — 下载 Node.js®</a></p><p><a href="https://link.segmentfault.com/?enc=ClwCQ9mfBGQ%2BiwlZrGLRrQ%3D%3D.dFjgb0mKz6hxzaoEkHv60qC1XRaG7WNC7OrNUIIrl%2B4%3D" rel="nofollow" target="_blank">Moltbot — Personal AI Assistant</a></p><p><a href="https://link.segmentfault.com/?enc=QTKRY%2FAt0me0xOKdVE0%2F%2Fw%3D%3D.DRh5uGmND4NkEP59iOmxvO6CX8NB6hOYJdNtOn5nHWSro37GR9dS7jMUzu5vgFVM2rPjx8szL%2Fs1VMFmXLq%2BWQ%3D%3D" rel="nofollow" target="_blank">openclaw企业微信插件</a></p><p><a href="https://link.segmentfault.com/?enc=GK1N58jdxtA7rus14uH8VQ%3D%3D.LVPjTp%2B535z5rWMPTQdSx2wu4yNB7s64%2BxkLJiLZ1XA%3D" rel="nofollow" target="_blank">MoltHub</a></p><p><a href="https://link.segmentfault.com/?enc=%2FPy2DIg98xZcZDy9cT50qg%3D%3D.MlelPgfy3upjMHOwA%2FAdW%2B2xtIczDDhqlWQCarmNDDzbIXe7egJ1W1ZI1GPAebq%2F" rel="nofollow" target="_blank">https://linux.do/t/topic/1518570</a></p><p><a href="https://link.segmentfault.com/?enc=mketfBKmN7BOaXbI7k1FLw%3D%3D.RLQL0IOI2wHdPR1%2FGjFfP1aZZvXYUIz5y%2F%2FeE7t8ziRe%2BSbdC1pO8oSMDFBbRC9FCEy0u9DcpXGWKv422ESKfw%3D%3D" rel="nofollow" target="_blank">🚀 云上Moltbot（原Clawdbot）最全实践指南合辑-腾讯云开发者社区-腾讯云</a></p><p><a href="https://link.segmentfault.com/?enc=2Cf9uABs%2BjPGPIYdupiHqg%3D%3D.P7bbZOBVuc7Jn1Ek3LffY6TJoJX0gouBub%2FxR8S8adnBtBw5chqBtRHlQ2hsJnTw" rel="nofollow" target="_blank">openclaw的QQ机器人插件</a></p><p><a href="https://link.segmentfault.com/?enc=U5v7oM84FdPCDcgQK2UjyA%3D%3D.ekOuV%2BmnJn30a8VV1hF8r5XgHYnscHBoAu%2FNt0jUufkFhRjPEpsiWsrgJOh2Sr5gx7LSeYUwCTpRgKPy17m8PA%3D%3D" rel="nofollow" target="_blank">Clawdbot 全面指南 - 汇智网</a></p>]]></description></item><item>    <title><![CDATA[推理速度 10 倍提升，蚂蚁集团开源业内首个高性能扩散语言模型推理框架 dInfer 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047605777</link>    <guid>https://segmentfault.com/a/1190000047605777</guid>    <pubDate>2026-02-11 17:11:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>蚂蚁集团开源业界首个高性能扩散语言模型（Diffusion Large Language Model，dLLM）推理框架 dInfer。<br/>在基准测试中，dInfer 将 dLLM 的推理速度相比于 Fast-dLLM 提升了 10 倍以上，并在关键的单批次（batch size=1）推理场景下，作为首个开源框架实现了大幅超越经过高度优化的自回归（AR）模型的性能里程碑，在 HumanEval 上达到 1011 tokens / 秒的吞吐量。dInfer 通过一系列算法与系统协同创新，攻克了 dLLM 的推理瓶颈，兑现了其内生并行生成带来的推理效率潜力。<br/>这不仅为开发者提供了即刻可用的高效推理框架，更标志着扩散语言模型这一全新的范式迈出了走向成熟的坚实一步。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605779" alt="图片" title="图片"/></p><ul><li>论文链接: <a href="https://link.segmentfault.com/?enc=Bd9ZlJwd0CVaI3YPoYKltA%3D%3D.ylZYHI2SwHdO%2BC9%2Ft7yOEo9Q%2BUzzS9bIT36fiMq4aMxytTi%2F2eUzW7YazI7s3ym2" rel="nofollow" target="_blank">https://arxiv.org/abs/2510.08666</a></li><li><p>项目地址: <a href="https://link.segmentfault.com/?enc=oPjzL0zsP5o5XRvZmWPwIw%3D%3D.CRR099b3Pbx6BNoxHARCwd%2FQWvLOS%2FfpPoHJAjZSPv4VutrEaNtPo7OB7oe5P%2FDH" rel="nofollow" target="_blank">https://github.com/inclusionAI/dInfer</a> </p><h2>理论的「翅膀」，现实的「枷锁」：扩散语言模型的推理困境</h2><p>近年来，以自回归（Autoregressive，AR）范式为核心的大语言模型（Large Language Models）已经取得了巨大的成功，推动了智能问答、代码生成、智能体助手等领域的重大进步。然而，AR 生成范式也存在其固有瓶颈：生成过程完全依赖前序结果，必须逐词串行生成，这导致推理延时难以降低，即使 GPU 的并行计算能力强大也无用武之地。<br/>作为一种全新的范式，扩散语言模型（dLLM）应运而生。它将文本生成视为一个 「从随机噪声中逐步恢复完整序列」的去噪过程。这种模式天然具备三大优势：</p></li><li>高度并行：理论上可以在单次迭代中，并行地预测和更新序列中的多个 token</li><li>全局视野：模型的每一步决策都基于对整个序列的全局上下文理解，而非仅依赖于已生成的部分</li><li>结构灵活：更易于适应多模态、代码生成等需要复杂结构和长程依赖的任务<br/>凭借这些优势，以 LLaDA-MoE 为代表的 dLLM 已在多个基准测试中，展现出与顶尖 AR 模型相媲美的准确性 。然而在推理效率方面，dLLM 理论上的强大潜能，却长期被残酷的现实「枷锁」所束缚。dLLM 的高效推理面临三大核心挑战：<br/>1.高昂的计算成本：多步迭代去噪的特性，意味着模型需要反复对整个序列进行计算，这带来了巨大的算力开销<br/>2.KV 缓存的失效：dLLM 中的双向注意力机制，使得 token 对应的 KV 值在每次迭代中都会改变。这导致 AR 模型中「一次计算、永久复用」的 KV 缓存技术直接失效，使得推理过程异常昂贵<br/>3.并行解码的双刃剑：尽管理论上可以并行生成序列中的所有 token，但在难以精准刻画其联合概率分布的情况下一次性解码太多 token，极易引发彼此间的语义错配，导致「并行越多，质量越差」的窘境</li></ul><p>这些瓶颈使得 dLLM 的推理速度一直不尽人意，其并行生成带来的效率沦为「纸上谈兵」。如何打破枷锁，释放 dLLM 在推理效率的潜能，成为整个领域亟待解决的难题。</p><h2>dInfer：人人可上手的扩散语言模型高效推理框架</h2><p>为彻底突破上述瓶颈，蚂蚁集团推出了 dInfer—— 一个专为 dLLM 设计的、算法与系统深度协同的高性能推理框架，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p><p>dInfer 的设计哲学是模块化与可扩展性，以系统性集成算法与系统优化。如下图所示，dInfer 包含四大核心模块：模型接入（Model）、KV 缓存管理器（KV-Cache Manager），扩散迭代管理器（Iteration Manager），和解码策略（Decoder）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605780" alt="图片" title="图片" loading="lazy"/><br/>图 1. dInfer 架构</p><p>这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测。更重要的是，dInfer 针对上述三大挑战，在每个模块中都集成了针对性的解决方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605781" alt="图片" title="图片" loading="lazy"/><br/>表 1. dInfer 组件</p><h2>dInfer 如何「快」起来？</h2><p>1.削减计算成本，控制生成质量：邻近 KV 缓存刷新 (Vicinity KV-Cache Refresh)<br/>dLLM 使用双向注意力机制让模型获得更全局的视野，代价是每次解码会影响所有的 token 的 KV 值，导致 AR 模型依赖的 KV 缓存技术不能直接应用到 dLLM 上。如果不使用任何 KV 缓存，在一个 sequence 上的一次 diffusion 迭代会导致大量的计算。</p><p>为了削减计算成本，Fast-dLLM 提出的将 sequence 划分为 block，然后再逐个对 block 进行解码，并在当前解码 block 之外进行 KV 缓存的方法，可以有效降低 diffusion 迭代的计算成本。然而虽然利用上了 KV 缓存，但在大部分情况下，缓存中的 KV 实际上是过时的，因此会导致生成质量的下降。</p><p>为了缓解这一问题，dInfer 采取了一种邻近刷新的策略：KV 缓存过时的原因是 dLLM 中一个新 token 的确定，会影响全局所有 token 的 KV 表示。而 dInfer 基于「语义局部性」原理（ 一个词的更新，对其近邻词的影响最大），在每次迭代解码一个 block 时，dInfer 只选择性地重新计算该区块及其邻近一小片区域的 KV，而让远处的缓存保持不变。这好比修改文档中的一句话，你只需检查上下文是否通顺，而无需重读整篇文章。<br/>这种策略结合 dInfer 的其它优化，在计算开销和生成质量之间取得了平衡，首次让 KV 缓存机制在 dLLM 上高效、可靠地运作起来。</p><p>2.系统优化：让 dLLM 的前向运算速度追上 AR<br/>在利用上 KV 缓存之后，dInfer 选择了合适的 block 大小和 Vicinity KV-Cache Refresh 的范围，并做了一系列的系统优化，以使 dLLM 一次迭代的速度能追上运行在 SOTA 的推理服务框架如 vLLM 上的 AR 模型，包括：</p><ul><li>多卡并行：结合了张量并行 (TP) 与专家并行 (EP)，即使在 batch size=1 的条件下，也能充分利用 GPU 的算力，效率提升超 100%。</li><li>编译优化：通过 torch.compile 进行内核融合并编译为 CUDA Graph 执行，消除了 PyTorch 框架的执行开销，结合上述的多卡并行，可让效率提升 200%。</li><li>消除迭代之间的气泡：采用循环展开 (Loop Unrolling) 技术，让 Python 可以连续不断地启动 CUDA 内核，消除了迭代间的 GPU 空闲气泡，带来 5-10% 的性能提升。</li><li>早停：在生成 EOS token 后，跳过后续 block 的推理过程，可以减少 5-40% 不必要的开销。</li></ul><p>3.并行解码：层级解码 (Hierarchical) 与信用解码 (Credit)<br/>为了在保证生成质量的前提下，最大化并行解码的 token 数量，dInfer 提出了两种无需额外训练的解码算法 ：</p><ul><li>层级解码 (Hierarchical Decoding)：该算法借鉴了「分治」思想，将待解码的区域不断递归地一分为二，并优先在每个子区域的中心位置解码 token 。这种方式自然地拉开了新生 token 间的距离，减少了它们之间的语义干扰 。在理想情况下，它能以近似对数级的复杂度完成多点并行生成，既快又稳 </li><li>信用解码 (Credit Decoding)：在多轮迭代中，有些正确的 token 可能很早就被模型稳定地预测出来，但因其单次置信度未能「达标」而被反复重算 。dInfer 为此引入了「累积信用」机制，持续追踪并累积每个 token 在历史迭代中的置信表现 。一个长期被稳定预测的 token，即使当前置信度稍低，也能凭借高累积信用被「破格」解码，从而有效避免了大量冗余计算</li></ul><p>4.压榨每步迭代价值：迭代平滑 (Iteration Smoothing)<br/>传统 dLLM 在每轮迭代中，只利用了置信度最高的 token 信息，而将其他位置的概率分布整个丢弃。dInfer 的迭代平滑算法，旨在回收这些被浪费的信息。</p><p>它基于未解码位置的 logits 分布得到该位置的加权 Embedding，并将其作为宝贵先验知识，平滑地融入下一轮迭代的 Embedding 中。这极大地丰富了上下文信息，使得单次迭代解码的 token 数量平均提升了 30-40%。</p><p>此外，由于 dInfer 可以无障碍地接入多种扩散语言模型，此次率先支持了基于轨迹蒸馏（Trajectory Distillation）加速 diffusion 去噪过程的 LLaDA-MoE-TD 模型，推理性能更强。</p><h2>实测数据：里程碑式的性能飞跃</h2><p>在配备 8 块 NVIDIA H800 GPU 的节点上，dInfer 的性能表现令人瞩目。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605782" alt="图片" title="图片" loading="lazy"/><br/>图 2. 评测数据</p><ul><li>10 倍性能提升：在与先前的 dLLM 推理方案 Fast-dLLM 的对比中，dInfer 在模型效果持平的情况下，平均推理速度（avg TPS）实现了 10.7 倍的巨大提升（681 vs 63.6）</li><li>超越自回归：与在业界顶尖的推理服务框架 vLLM 上运行的、参数量和性能相当的 AR 模型 Qwen2.5-3B 相比，dInfer 的平均推理速度是其 2.5 倍（681 vs 277） </li><li>突破推理极速：在代码生成任务 HumanEval 上，dInfer 在单批次推理中创造了 1011 tokens / 秒的纪录 。这是开源社区首次见证，扩散语言模型在延迟敏感的单批次推理场景下，速度显著超越经过高度优化的自回归模型</li></ul><p>更进一步，当结合轨迹蒸馏（Trajectory Distillation）技术（一种让模型学会 「跳跃式」去噪的后训练优化方法）后，dInfer 的平均推理速度飙升至 847 TPS，实现了超过 3 倍于 AR 模型的性能。</p><h2>开源开放：共建下一代 AI 推理新生态</h2><p>dInfer 的诞生，不仅是一个工具的发布，更是一次 LLM 范式的试炼：它证明了扩散语言模型的效率潜力并非空中楼阁，而是可以通过系统性的创新工程兑现，使其成为 AGI 道路上极具竞争力的选项。<br/>目前，dInfer v0.1 的全部代码、技术报告与实验配置已开源。<br/>蚂蚁希望 dInfer 能成为：</p><ul><li>研究者的标准平台：为 dLLM 领域的算法创新提供一个公平、高效的试验场 。</li><li>开发者的加速引擎：助力社区将强大的 dLLM 轻松部署到实际应用中，享受极致性能 。<br/>dInfer 连接了前沿研究与产业落地，标志着扩散语言模型从「理论可行」迈向「实践高效」的关键一步。我们诚邀全球的开发者与研究者一同加入，共同探索扩散语言模型的广阔未来，构建更加高效、开放的 AI 新生态。</li></ul>]]></description></item><item>    <title><![CDATA[新年小惊喜！龙蜥之旅五周年特辑上线啦，解锁你的社区足迹 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605785</link>    <guid>https://segmentfault.com/a/1190000047605785</guid>    <pubDate>2026-02-11 17:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>开源的世界里，没有微不足道的参与，只有共同成就的未来。</p><p>回望 2025，龙蜥社区始终向上突破——从技术演进到解决方案规模化落地，从高校开源教育到生态拓展，龙蜥社区交出了一份扎实而令人自豪的答卷。</p><p>而你，或许曾提交过代码、参与过活动，又或许只是默默关注、静静使用——无论以何种方式同行，你都是这段旅程中不可或缺的一份力量。正是因为你的每一次关注与信任，都为龙蜥注入了前行的动力。</p><p>值此龙蜥社区成立五周年&amp; 2026 新年来临之际，为感谢大家的一路相伴，我们特别准备了上千份精美礼品，包括龙蜥定制保温杯、限定款龙蜥卫衣、萌趣小龙抱枕、猫超卡、B 站/腾讯视频月卡等。诚挚邀请每一位关注、使用或贡献过龙蜥的朋友，打开龙蜥社区官网（openanolis.cn），一起回望 2025 年那些你在社区留下的珍贵“足迹”，重温属于你与龙蜥的共同记忆，还有精美周边领取哦。</p><p>活动时间：2026 年 2 月 10 日-3 月 31 日</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605787" alt="图片" title="图片"/><br/>（图/龙蜥开源足迹活动礼品图集）</p><h3>开启你的“龙蜥开源足迹”</h3><p>活动期间，首次登录龙蜥官网，会自动弹出“龙蜥开源足迹”，一键点击开启。若手滑退出或再次进入龙蜥官网，不用担心，可在官网顶部点击“龙蜥开源足迹”图片或官网右侧图标，都可开启活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605788" alt="图片" title="图片" loading="lazy"/></p><h3>兑换礼品</h3><p>活动截止时间：2026 年 3 月 31 日。</p><p>📌 兑换流程：开启龙蜥之旅，回顾年度开源足迹，点击抽盲盒。<br/>🎁 礼品邮寄：根据中奖提示，填写收件信息。工作人员会在 3 月起陆续安排邮寄。</p><h3>彩蛋</h3><p>除领取盲盒外，还可以下载“龙蜥开源足迹”图片，并在龙蜥公众号（搜 OpenAnolis 龙蜥）发布的这篇文章评论区留言，注意必须带“龙蜥开源足迹”图 + 评论（图片示意图如下所示；评论格式为“2026，我祝龙蜥社区...”内容不限，祝福或需求都可）。我们将按照规则送出龙蜥定制双肩包。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605789" alt="图片" title="图片" loading="lazy"/><br/>（图/“龙蜥开源足迹”示意图）</p><p>✨ <strong>送礼规则</strong></p><p>高赞有礼！我们将分别在 2 月 27 日（统计 2 月 10-26 日 23:59 的点赞数）、4月 1 日（统计 2 月 27 日-3 月 31 日 月 1- 30 日 23:59 的点赞数）各公布评论点赞排名前 10 的用户，送出龙蜥定制双肩包一个。</p><p>届时请大家及时关注龙蜥公众号（OpenAnolis龙蜥），获奖名单将在以上两个开奖日通过公众号文章的形式公布，请及时填写邮寄地址。</p><p>注意：评论须为原创，禁止发广告、拉踩等无意义内容，同一用户多条评论仅取点赞最高的一条参与当期评选。2 月已获奖的用户将不再参与 3 月的点赞排名评选。</p><p>快来一键开启你的龙蜥开源足迹吧～</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[2025 年度回顾｜龙蜥这一年：AI 领航，生态共荣 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605796</link>    <guid>https://segmentfault.com/a/1190000047605796</guid>    <pubDate>2026-02-11 17:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605798" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[如何通过设计研发协同平台实现制造业的高效创新？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047605803</link>    <guid>https://segmentfault.com/a/1190000047605803</guid>    <pubDate>2026-02-11 17:09:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的竞争格局中，产品研发的效率与质量已成为企业核心竞争力的关键。然而，传统的研发管理模式往往面临信息割裂、流程冗长、协作低效等痛点。设计图纸版本混乱、跨部门沟通成本高、质量风险难以提前识别等问题，不仅拖慢了产品上市速度，也增加了研发过程中的隐性成本。面对这些挑战，越来越多的企业开始将目光投向设计研发协同平台，希望通过数字化手段重构研发流程，实现数据驱动的协同创新。<br/>设计研发协同平台的核心价值<br/>设计研发协同平台并非单一的工具叠加，而是一种系统性的研发管理理念的落地。它通过整合产品生命周期中的需求、设计、仿真、试制、质量等环节，构建起一个统一的数据底座和协作环境。在这个平台上，三维模型、技术文档、物料清单（BOM）等核心数据得以结构化存储和动态关联，任何变更都能实时传递到所有相关环节，从而避免因信息滞后导致的错误和返工。更重要的是，平台打破了部门壁垒，使得设计、工艺、生产、质量等团队能够在同一语境下协作。例如，设计人员发起的模型修改，工艺人员可以即时反馈可制造性意见，质量人员则能同步更新FMEA（失效模式与影响分析）中的风险控制措施。这种“设计-工艺-质量”的一体化协同，不仅加速了决策流程，也从根本上提升了产品的可制造性和可靠性。<br/>技术实现与流程重构<br/>从技术层面看，现代设计研发协同平台通常基于云原生架构，支持分布式协同和轻量化应用。通过模型轻量化技术，非设计人员（如采购或质量工程师）无需安装专业CAD软件，即可通过浏览器查看、批注甚至参与评审复杂的三维模型。同时，平台内置的流程引擎将传统的纸质审批、邮件沟通转变为自动化的工作流，任务推送、节点提醒、权限控制等功能大幅减少了人为延误。在质量管控方面，平台通过集成FMEA管理模块，将历史故障库、行业标准与实时项目数据打通。系统能够基于相似产品或设计特征，自动推荐潜在的失效模式及改进措施，从而帮助工程师在研发早期识别风险，而非事后补救。这种预防性的质量保障机制，使得平台不再是简单的文档管理系统，而是贯穿产品创新全过程的智能决策支持系统。<br/>实践案例<br/>在国内，广域铭岛旗下的Geega（际嘉）工业互联网平台已成为研发协同领域的代表性解决方案。在某汽车零部件企业的实践中，Geega平台通过统一数据源和流程集成，实现了BOM准确率提升至98%，设计变更审批周期缩短50%，零部件复用率提高30%。其特点在于深度契合中国制造业的需求，注重轻量化部署和低成本适配，尤其擅长与现有ERP、MES系统的集成，帮助企业以较低门槛实现研发数字化。相比之下，国际厂商如PTC的Windchill和西门子的Teamcenter则代表了另一种路径。PTC通过强化AR/VR与数字孪生技术的融合，使研发协同不再局限于桌面屏幕，而是延伸至车间现场和远程运维场景。例如，工程师可通过AR设备在物理原型上叠加虚拟设计模型，直接进行偏差比对和装配验证。西门子Teamcenter则依托其完整的PLM（产品生命周期管理）生态，实现了从设计、仿真到制造执行的全链条数据闭环，特别适用于大型跨国企业的多地点、多学科协同需求。尽管路径不同，这些平台都在试图解决同一问题：如何让研发更高效、更可靠、更贴近市场需求。<br/>设计研发协同平台的崛起，标志着制造业研发模式从“粗放式管理”向“精细化运营”的转变。它不仅仅是一种技术工具，更是企业重塑研发体系、构建数字驱动文化的重要契机。当数据成为新的生产要素，协同成为新的创新范式，那些率先拥抱这一变革的企业，无疑将在未来的市场竞争中占据先机。</p>]]></description></item><item>    <title><![CDATA[让 AI Agent 安全“跑”在云端：基于函数计算打造 Agent 代码沙箱 Serverless]]></title>    <link>https://segmentfault.com/a/1190000047605809</link>    <guid>https://segmentfault.com/a/1190000047605809</guid>    <pubDate>2026-02-11 17:09:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：安全沙箱与 Serverless 的技术交汇</h2><p>随着大语言模型（LLM）从“对话框”走向“行动体（Agent）”，其能力边界正在迅速扩张。现代 AI Agent 不再是文字的搬运工，而是能够自主思考、调用工具、甚至编写并运行代码以解决复杂问题的智能助手。然而开发者始终面临一个根本性挑战：如何在保证执行效率的同时，实现资源强隔离与资源可控性？<br/>阿里云函数计算 FC 为这一难题提供了全新的解题思路。其底层基于轻量级安全沙箱，天然具备进程级隔离、资源极致伸缩、按需付费等特性。这种架构与 Agent 对代码执行环境的需求高度契合，使得构建高密度、低成本、安全可靠的 Agent 运行时成为可能。</p><h2>为什么需要 Agent 代码沙箱？</h2><p>Agent 的核心价值在于其“自主执行”能力，而代码执行是实现这一能力的关键路径。在工具调用、动态数据分析、自动化任务处理等典型场景中，Agent 生成的代码往往来自不可信的推理过程，若缺乏有效的沙箱保护，开发者将面临多重风险，为此 AI 开发者对运行时有着如下多个核心诉求：</p><ul><li>安全与隔离特性：必须确保不同用户的 Agent 代码在文件系统、网络访问上完全隔离，严防恶意指令注入导致的越权操作。</li><li>资源管理控制：代码缺陷或恶意行为可能导致 CPU/内存耗尽。系统需要能够对单个执行任务进行精细化的资源配额限制。</li><li>生命周期管理：Agent 任务存在短时型突发、长周期会话等多种任务模型，需提供灵活生命周期管理能力。</li><li>按资源消耗计费：若简单按实例运行时长计费，在长周期交互场景下，用户将为大量的“等待时间”支付不必要的费用。需在用户成本控制与平台资源利用率之间寻找平衡点。</li></ul><p>由此可见，构建一个强隔离、可管控、即开即用且按需回收的代码执行环境——Agent 代码沙箱，已成为 AI 应用架构中的刚需。</p><h2>为什么是 Serverless？函数计算的核心优势</h2><p>在众多技术路线中，Serverless 函数计算凭借其天然的“沙箱基因”，成为了构建 Agent 运行时的理想底座：</p><ol><li>底层安全隔离：主流云厂商的函数计算服务普遍采用 MicroVM 或强化容器技术作为执行单元。每个函数实例运行在一个轻量级、启动迅速的 MicroVM 中，具备完整的内核隔离。这种架构从进程、内存、文件系统等多维度实现安全保障。</li><li>极致的弹性伸缩：Agent 的请求模式具有高度不确定性。函数计算的毫秒级扩缩容能力，让开发者无需担心容量规划，轻松应对从零到万级并发的波动。</li><li>按量付费的经济性：传统常驻服务无论是否处理请求，均持续产生费用。而函数计算采用“用多少付多少”的计费模式，极大降低用户成本。（下文也将介绍 AI 场景下如何实现经济计费）</li><li>简化的运维体验：函数计算将基础设施管理完全托管给云平台，开发者只需关注代码逻辑，这种“代码即服务”的模式，极大加速了 AI 业务的迭代与上线周期。</li><li>异构算力支持：针对图像处理、音视频编解码等高性能场景，函数计算成熟的 GPU 实例支持，为 Agent 提供了更广阔的技能空间。</li></ol><h2>产品化实践：基于函数计算构建沙箱能力</h2><p>为了将通用的函数计算转化为专业的 Agent 运行时，我们不仅需要底层的隔离，更需要在协议层、会话层和调度层进行深度重构。</p><h3>协议扩展：定义多元化业务的接入标准</h3><p>Agent 的交互模式远比传统 Web 应用复杂。为了让 Agent 沙箱能够无缝嵌入现有的 AI 生态，我们针对不同场景实现了协议适配：</p><ol><li>针对工具生态：支持 MCP SSE 与 Streamable 协议<br/>随着 Model Context Protocol (MCP) 成为 Agent 工具调用的事实标准，函数计算在网关层实现了兼容标准的 MCP 协议，这意味着可以在函数计算平台实现一键托管 MCP 服务。</li><li>针对 Web/Browser Agent：支持标准 Cookie 协议<br/>Browser Agent 需要模拟登录状态或维持持久化的 Web 会话。函数计算的接入层通过实现兼容标准 Cookie 协议，使得沙箱环境能够保持与目标网站的交互状态，支持复杂的自动化操作。在用户首请求时，服务端将生成全局唯一的 CookieID 并通过 Response 中的 Set-Cookie 字段返回，后续请求用户仅需携带相同 CookieID 便实现定向路由。</li><li>针对灵活接入：定义统一 Header Field 协议<br/>在基于 Header Field 的会话亲和机制中，仅需客户端通过在 HTTP Header 中注入特定的元数据。函数计算系统网关会解析请求头中的会话 ID，并将其作为路由键，确保携带相同会话 ID 的后续请求被精准路由到同一函数实例。这种方式不依赖客户端状态（如 Cookie），可以应用在任何客户端以 HTTP 协议交互的业务场景中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605811" alt="" title=""/></p><h3>底座能力：构建有状态的会话管理</h3><p>在解决了协议层“如何接入”后，接下来的挑战是如何在无状态的 FaaS 架构上，构建“有状态”的会话体验。</p><h4>会话生命周期管理</h4><p>Agent 的执行往往不是一次性的，而是多轮对话，为此需要赋予会话生命周期管理能力，如下图所示，系统提供用户主动、系统自动两种能力实现灵活、完整的管理机制：</p><ol><li>用户主动管理</li></ol><ul><li>续期：面对 Agent 执行逻辑的不确定性，在生命周期配置上通常很难做到“一次性设对”。期间为延续状态的连续性，避免任务中断，可通过 Update API 实现对 Session TTL/IdleTimeout 的续期，主动延长沙箱寿命，续期后会话仍处于活跃状态且继续可用。</li><li>销毁：显式通过 Delete API 删除会话，实现提前销毁释放资源。</li></ul><ol start="2"><li>系统自动管理</li></ol><ul><li>Session TTL：会话达到 TTL（最大存活时长上限）后，无论是否仍在使用，平台都会自动回收资源。</li><li>Session IdleTimeout：会话在 IdleTimeout 规定时间内没有活动，平台判定为空闲并自动回收。</li></ul><p>两类方式最终都会走到生命周期结束 → 会话销毁 → 关联资源释放。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605812" alt="" title="" loading="lazy"/></p><h4>会话亲和能力</h4><p>这是将 FaaS 转化为“AI 运行时”的关键。通过会话亲和，我们保证了 Agent 上一轮生成的中间变量、本地文件在下一轮交互中依然可用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605813" alt="" title="" loading="lazy"/></p><p>整个流程分为用户首请求和非首请求，以 HeaderField 为例：</p><p><strong>会话初始化流程（首请求）</strong></p><ol><li>发起请求：Client（客户端）向 Gateway（网关）发送请求，并在 Header 中携带特定的 x-fc-session-id，用于标识该请求属于哪个 Agent 会话。</li><li>生成内部 ID：Gateway 接收请求后，对 session_key 进行哈希处理，生成一个系统内部使用的 internal_session_id。</li><li>查询会话状态：Gateway 向 MetaDB（元数据库）发起查询，核实该 session_id 是否已经存在（即是否已经有对应的运行实例）。</li><li>未命中处理：MetaDB 未搜到到相关信息，表明这是一个新会话，或者之前的会话已失效，需要重新分配资源。</li><li>触发调度：由于是新会话，Gateway 随机选择一个 Scheduler（调度器）节点，请求为该会话分配计算资源。</li><li>分配实例：Scheduler 根据当前资源情况，从资源池中分配一个可用的 VM实例（即沙箱环境）。</li><li>持久化映射关系：Scheduler 将 session_id 与分配到的 instance（实例）的对应关系写入 MetaDB。这样后续携带相同 ID 的请求就能实现“会话亲和性”，直接路由到该实例。</li><li>路由响应：Scheduler 将实例的路由信息返回给 Gateway。</li><li>返回首包：Gateway 完成链路建立，将处理后的首包数据返回给 Client。至此，该 Agent 会话正式建立，后续交互将直接复用此路径。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605814" alt="" title="" loading="lazy"/></p><p><strong>热请求数据流程</strong></p><ol><li>发起请求：Client（客户端）发起请求，并在 Header 中携带已有的 x-fc-session-id。</li><li>查询会话记录：Gateway（网关）接收请求后，前往 MetaDB（元数据库）查询该 Session ID 对应的记录。</li><li>返回映射信息：MetaDB 返回该会话之前绑定的 Instance（实例）信息以及负责管理该实例的 Target Scheduler（目标调度节点）。</li><li>直连调度节点：Gateway 根据返回的信息，直接联系对应的 Target Scheduler。</li><li>确认路由实例：Target Scheduler 告知 Gateway 该实例有效，可以进行数据转发。</li><li>转发请求：Gateway 将客户端的业务请求转发给对应的 Instance。</li><li>处理并响应：Instance（Agent 沙箱）执行代码逻辑处理请求，并将结果返回给 Gateway。</li><li>返回业务数据：Gateway 将最终的执行结果回传给 Client，完成一次有状态的会话交互。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605815" alt="" title="" loading="lazy"/></p><h4>会话隔离能力</h4><p>为了极致的安全，我们引入了“一会话一实例”的隔离模型。每个 Agent Session 独占一个底层的运行实例。一旦会话结束，实例立即销毁并擦除数据。通过会话配额控制，可以有效防止单个用户创建过多沙箱导致资源过载。</p><h3>扩展配套能力，强化 Agent 底座</h3><p>除了核心的调度与协议，针对生产环境中的性能与成本挑战，我们进一步扩展了配套能力：</p><ol><li>预热能力<br/>冷启动是 Serverless 的天敌。针对 Agent 实时交互的要求，我们支持 <code>CreateSession</code> 主动预热。在用户刚进入对话页面时，系统提前准备好预留实例。将沙箱的就绪时间压缩至极低延时。</li><li>会话级存储隔离<br/>Agent 经常需要读写文件。我们实现了会话维度的动态存储挂载。每个沙箱可以根据 Session ID 动态挂载独立的 NAS 或 OSS 路径。这样既保证了数据在会话内的持久化，又确保了不同会话间的文件系统是物理隔离的。同时满足沙箱异常 Crash 后数据的可恢复。</li><li><p>计费升级模型进化：从 FAAS 的“按请求”到“按资源消耗”<br/>FaaS按请求计费模式，在AI场景下会产生巨大的“保活成本”。会话计费模型必须与资源的实际使用强挂钩，因此系统针对会话函数的计费模式升级到Serverless AI 计费模式。</p><ul><li>活跃期：当会话实例正在处理用户请求时，按照活跃单价计费。</li><li>空闲期：当会话处于空闲、仅维持连接和上下文状态时，系统切换到一个极低的“保活”费率。仅收取内存、磁盘的费用，不再收取相对较高的CPU费用。</li></ul></li></ol><p>这个模式对客户而言，相对传统常驻实例完整生命周期计费模式成本大幅降低。</p><h2>总结与展望</h2><p>Serverless 函数计算凭借其安全隔离、弹性伸缩、按需付费等基因，正成为构建 Agent 运行时的理想选择。通过协议生态扩展、会话管理能力增强、配套能力完善，我们已实现从“单一函数执行”到“复杂 Agent 托管平台”的跨越。未来，我们也将持续聚焦启动优化、更长会话支持等等核心能力，做好AI原生时代坚实的护航者。</p>]]></description></item><item>    <title><![CDATA[『n8n』不用写SQL，了解一下内置的Datatable 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605823</link>    <guid>https://segmentfault.com/a/1190000047605823</guid>    <pubDate>2026-02-11 17:08:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个n8n小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=1%2Bs8biC3qr0KLLd3oT0OKg%3D%3D.K9PJ%2BnLxPIDKwvN%2BXNTNsmnuLoIf4qRX1Ory2Gu6QfEUzcN6ZQdy9dfRy466MR%2FtjrkSDS5uQdmXsK8%2FRT666WmP2tRDzyUwi5C7eOiG1hTyhi5BU%2F3A%2FaviR6hzHzPNBAp%2BOEKOURZN%2FBntxrmP1ee87sjqO7VJ%2FuZ3M5pNhl4%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a></blockquote><p>非技术出身的工友在使用 n8n 时是否会遇到这样的困惑：爬取了一堆数据不知道存哪里，总不能每次都导出到Excel再来回导入？想让多个工作流共用一套数据，却找不到简单的方法？不想折腾MySQL、PostgreSQL这些复杂的外部数据库，也看不懂晦涩的SQL语句？</p><p>如果有以上困惑，而且你的数据结构不是那么复杂的话，可以试试 n8n 内置的 Data tables。</p><p>我用一个简单的例子介绍一下 Data tables 的用法，顺便讲讲不同格式的字段该如何转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605825" alt="" title=""/></p><p>我们可以在 Data tables 面板管理各个数据表，点击右上角的“Create data table”创建一个数据表。</p><p>我以“员工信息表”作为演示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605826" alt="" title="" loading="lazy"/></p><p>点击加号新增 <code>name</code> 和 <code>married</code>。</p><p><code>name</code> 是“员工姓名”，Type 选择 string。</p><p><code>married</code> 是“是否结婚”，Type 选择 boolean（这个类型只有“是”和“否”两个选项）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605827" alt="" title="" loading="lazy"/></p><p>创建一个工作流，添加要给表单节点。</p><p>只有2个字段，表单的“姓名”对应数据表里的 <code>name</code>；表单的“是否结婚”对应数据表里的 <code>married</code>。</p><p>但是，数据表的 <code>married</code> 的类型是布尔型，也就是 <code>true</code> 表示已结婚，<code>false</code> 表示未结婚。但表单的“是否结婚”的选项却是字符串的“已结”和“未结”。这里要做一下转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605828" alt="" title="" loading="lazy"/></p><p>在表单节点后面添加一个「Insert row」节点，搜 table 就能找到它。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605829" alt="" title="" loading="lazy"/></p><p>给「Insert row」节点做以下配置。</p><p>「From list」选择刚刚创建的“员工信息”表。</p><p>「Values to insert」填入 <code>name</code> 和 <code>married</code> 这两个字段。</p><p><code>name</code> 这项填入 <code>{{ $json['姓名'] }}</code> 比较好理解，我不讲解了。</p><p><code>married</code> 这项填入 <code>{{ $json['是否结婚'] === '已结' ? true : false }}</code>，这是 JS 的三元运算符，判断上一个节点传入的“是否结婚”这项的值是否为“已结”，如果是的话就存入 <code>true</code> ，否则存入 <code>false</code>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605830" alt="" title="" loading="lazy"/></p><p>测试一下整个工作流。</p><p>我提交了2次表单：</p><ul><li>雷猴，已结</li><li>鲨鱼辣椒，未结</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605831" alt="" title="" loading="lazy"/></p><p>来到数据表就能看到这两项了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605832" alt="" title="" loading="lazy"/></p><p>鲨鱼辣椒突然说他今天要结婚了，作为 HR 也应该更新一下数据。</p><p>此时再提交一次表单：鲨鱼辣椒，已结。你会发现工作流又多了一条数据，这并不是我们想要的结果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605833" alt="" title="" loading="lazy"/></p><p>在 Data tables 里先手动删除第3项。</p><p>回到工作流这边，新增了一些节点。</p><ul><li><p>上面那条：</p><ul><li>「If row exists」：如果传入的“姓名”<strong>已在</strong>数据表里，就走这条。</li><li>「Update row(s)」：更新表中的数据。</li></ul></li><li><p>下面那条：</p><ul><li>「If row does not exist」：如果传入的“姓名”<strong>不在</strong>数据表里，就走这条。</li><li>「Insert row」：新增一条数据。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605834" alt="" title="" loading="lazy"/></p><p>「If row exists」和「If row does not exist」的判断条件都是一样的，如下图所示。只不过这两个节点的功能不同而已。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605835" alt="" title="" loading="lazy"/></p><p>在来看「Update row(s)」这个节点，通过 <code>name</code> 这个字段找到要修改的那行数据，找到后就修改 <code>married</code> 这列的值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605836" alt="" title="" loading="lazy"/></p><p>「Insert row」节点的配置不需要改变。</p><p>试试～</p><p>提交一项：鲨鱼辣椒，已婚</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605837" alt="" title="" loading="lazy"/></p><p>回到数据表这边就能看到鲨鱼辣椒的婚姻状态变成“true”了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605838" alt="" title="" loading="lazy"/></p><p>再提交一项：蝎子莱莱，未婚。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605839" alt="" title="" loading="lazy"/></p><p>由于“蝎子莱莱”不在数据表里，所以走的是“新增”路线。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605840" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，想了解更多n8n玩法欢迎关注<a href="https://link.segmentfault.com/?enc=2EhXDv9wyvkBho6%2FOzptww%3D%3D.XC0vk1%2BC0zi%2F7TeobnIwX3BUmsXNSSsEeV47RXU7wvrwNn%2B7n8hZczeTSVEEpnjXAdWypa81v24m5FjigFqo7qX0eoZOiSkXe585dtAOjQXMP2jtmI3a10ncAmnVjMt0nmefilHkTZDBFTr1h528wdCNEJzFV2jMSRVw2OAkEk8%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a>👏</p><p>如果你有 NAS，我非常建议你在 NAS 上部署一套 n8n，搞搞副业也好，帮你完成工作任务也好  <a href="https://link.segmentfault.com/?enc=QVefuQl2ghqxx1XEmEAPpA%3D%3D.%2FKSidRKzhzuCFS8Ds4OXoVWqLkiYB6SnS2zSaDtwdCoPXlCK0Nc5aJzTmMxxQlvxJ26EOegoV9Ni7OqGSHB5KA%3D%3D" rel="nofollow" target="_blank">《『NAS』不止娱乐，NAS也是生产力，在绿联部署AI工作流工具-n8n》</a></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[『NAS』在绿联部署隐私优先的媒体文件格式工具-VERT.sh 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605897</link>    <guid>https://segmentfault.com/a/1190000047605897</guid>    <pubDate>2026-02-11 17:07:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=VIgaLS5CWZ2ZY%2F3%2BJZLtYQ%3D%3D.h5NIi8HPRwSyOjatedO%2BCrl45vRNof7NaSYHOUWfWBMrcWDs7kPK2aCN7AXOuPqwy3UVhH%2Bofgx8qvP%2FNopPOYMzQwyUvwaZN00QCQV5JrHuaxnoqrzqZmlSDasbMuC26GHq2aa4EmgQTUC3SAwpW3HgllkUw%2FlZ28ZYVXCddNQ%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>VERT.sh 是一款开源文件转换工具，它所有转换都在本地浏览器完成，文件（除视频外）永不上传云端，支持 n 种格式（文档、图片、音频、视频），无广告、无文件大小限制，开箱即用！</p><p>这次我使用绿联的 NAS 部署 VERT.sh，其他品牌的 NAS 只要支持 Docker 的，部署步骤都是大同小异。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605899" alt="" title=""/></p><p>首先在“文件管理”应用里找到“docker”文件夹，在里面创建一个“vert”文件夹。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605900" alt="" title="" loading="lazy"/></p><p>打开“Docker”应用，点击左侧菜单的“项目”，创建一个新项目。</p><ul><li>项目名称：vert。</li><li>存放路径：选择上一步创建的 <code>xxx/docker/vert</code> 文件夹。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605901" alt="" title="" loading="lazy"/></p><p>Compose配置 输入以下代码：</p><pre><code>services:
  vert:
    image: ghcr.io/vert-sh/vert:latest
    container_name: vert
    ports:
      - 2340:80 
    restart: unless-stopped</code></pre><p>这里我配置了 <code>2340</code> 这个端口，你根据你实际情况配置吧，只要不跟其他项目的端口冲突就行。</p><p>然后等 Docker 下载镜像和构建项目，等就行，速度取决于你的网速。</p><p>项目构建完成后，点击左侧菜单”容器“这项，找到 vert 这项，点击它右侧的小箭头会弹出端口号，点击端口这个按钮。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605902" alt="" title="" loading="lazy"/></p><p>点击端口按钮后，浏览器会新开一个窗口运行 VERT.sh。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605903" alt="" title="" loading="lazy"/></p><p>它默认界面是英文的，点击顶部菜单的“Settings”项，找到“Language”，切换成中文就行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605904" alt="" title="" loading="lazy"/></p><p>回到首页可以看到 VERT.sh 支持图片、音频、文档和视频等文件的格式转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605905" alt="" title="" loading="lazy"/></p><p>测试了一下，图片是可以转格式的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605906" alt="" title="" loading="lazy"/></p><p>但视频要传到人家的服务器转，而且还不一定成功😮‍💨</p><p>就当没转视频格式这个功能吧😤</p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=fFgXTmv6QthMvvdoNFf8BA%3D%3D.ipHZudRMwvb5anEVIdEdTS7DK3PmG48paxrNVICj443e8uvMbXBjYc98bGUlSS86GaK0Oe1SH86WUuma6ks0k3e%2BJ5hPa%2FzZaqdN2bJcRhX8qCVbhCm%2BrGV65pEDtPnDdyeD3gxnW5gTYqBsjM26bwQBWozsa3zXlvDkvGAFkzM%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[R语言优化沪深股票投资组合：粒子群优化算法PSO、重要性采样、均值-方差模型、梯度下降法|附代码数据]]></title>    <link>https://segmentfault.com/a/1190000047605916</link>    <guid>https://segmentfault.com/a/1190000047605916</guid>    <pubDate>2026-02-11 17:06:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=G%2BafYjCBdcOD3o7U3NjxBw%3D%3D.GKQqcOekisfPcApG8oLZUnecuKhHq9DGp%2BmkO2bkon0%3D" rel="nofollow" title="https://tecdat.cn/?p=44965" target="_blank">https://tecdat.cn/?p=44965</a>  <br/>原文出处：拓端数据部落公众号  <br/><strong>关于分析师</strong>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605918" alt="" title=""/></p><p>在此对 Hongxuan Liu 对本文所作的贡献表示诚挚感谢，他完成了应用统计专业的硕士学位，专注机器学习、风险管控领域。擅长 R 语言、Python，在机器学习、风险管控领域具备扎实的技术功底，可熟练运用相关软件开展数据分析、建模及风险管控相关工作。Hongxuan Liu 曾在中国农业银行从事数据分析工作，深耕金融领域数据分析与风险管控相关业务，负责银行各类数据的整理、分析与建模，为银行的风险防控、投资决策等核心业务提供数据支撑与实操建议，积累了丰富的金融行业数据分析实战经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t1" target="_blank"/>专题：2025年智能优化算法在A股投资组合配置中的实践与创新</h3><h4><a name="t2" target="_blank"/>引言</h4><p>在国内A股市场的投资实践中，普通投资者和中小机构始终面临一个核心难题：如何在多只股票间分配资金，既能控制波动风险，又能实现资产稳健增值。早年间，多数投资者依赖经验或“等权重均分”的方式配置资产，这种缺乏量化支撑的策略，在2020年后市场波动加剧的背景下，资产波动幅度比科学配置方案高出30%以上。  <br/>马科维茨的均值-方差模型为量化配置提供了理论基础，但该模型对应的优化问题存在非凸性，传统梯度下降算法极易陷入局部最优，无法找到真正的全局最优配置。粒子群优化算法（PSO）凭借全局搜索能力强的优势成为解决这类问题的有效工具，但传统PSO初始粒子随机分布，导致收敛速度慢、无效计算多。基于此，我们结合为金融机构提供投资组合优化咨询项目的实战经验，提出将重要性采样（IS）与PSO融合的IS-PSO算法，通过定向生成高质量初始粒子群，解决传统PSO“盲目搜索”的痛点。本文将拆解该算法的设计逻辑、落地步骤及在沪深A股样本上的应用效果，让读者既能掌握实操方法，也能理解背后的核心原理。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，该<strong>项目完整代码与数据已</strong>分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂 怎么做，也懂 为什么这么做；遇代码运行问题，更能享24小时调试支持。</p><h4><a name="t3" target="_blank"/> 项目文件目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605920" alt="" title="" loading="lazy"/></p><h4><a name="t4" target="_blank"/>整体流程脉络</h4><p>&lt;pre data-index="0" name="code" style="color: rgb(0, 0, 0); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"&gt;&lt;img alt="" src="https://i-blog.csdnimg.cn/direct/fa519e9285cd4756b5c826972a17bbba.png" style="border: 0px; max-width: 650px;"&gt;<br/>&lt;/pre&gt;</p><h4><a name="t5" target="_blank"/>1 投资组合优化的行业痛点与技术基础</h4><p>对于A股投资者而言，“分散投资却没分散风险”是普遍痛点——不少投资者将资金分散到多只股票后，要么收益跑不赢大盘，要么遇到市场调整时亏损远超预期。这背后的核心原因，是没有科学量化不同股票的收益和风险特征，仅依靠经验或简单的行业均分策略，无法适配复杂的市场环境。  <br/>马科维茨均值-方差模型为解决这一问题提供了核心框架：将投资组合的收益定义为各资产预期收益率的加权平均值，风险用收益率的标准差衡量，核心目标是找到“有效前沿”——即在给定风险下收益最高，或给定收益下风险最低的资产组合。这一目标转化为数学问题后，核心是最大化效用函数：max (ω^Tμ - λ/2*sqrt(ω^TΣω))。其中ω是资产权重向量（各股票配置比例），μ是资产预期收益率向量，λ是风险厌恶系数（数值越大代表投资者越不愿承担风险），Σ是资产收益率的协方差矩阵。  <br/>但这个效用函数具有非凸性，传统梯度下降算法依赖目标函数的梯度信息迭代，很容易停在局部最优解，无法找到真正的全局最优权重配置，这也是传统算法在实际投资中效果不佳的关键原因。</p><h4><a name="t6" target="_blank"/>2 IS-PSO算法的创新设计与实现</h4><p>粒子群优化算法（PSO）是模拟鸟群觅食行为的智能优化算法，每个“粒子”对应一组资产权重，通过迭代更新粒子的位置（权重）和速度，跟踪个体最优位置（pbest）和全局最优位置（gbest），最终找到最优权重配置。但传统PSO的初始粒子是随机生成的，大量粒子落在无效解区域，导致算法收敛慢、计算效率低。  <br/>我们的核心创新点在于，用重要性采样（IS）优化初始粒子群的生成逻辑：先随机生成大量权重样本，筛选出目标函数值前20%的“优质样本”，再基于这些样本的分布生成80%的初始粒子，剩余20%粒子随机生成（保证群体多样性）。这种方式让初始粒子聚焦在最优解附近，大幅减少无效迭代，提升收敛效率。</p><h5>2.1 IS-PSO算法的R语言核心实现</h5><p>以下是修改后的核心代码（变量名、代码结构均做调整，英文注释已翻译为中文，省略部分通用迭代逻辑）：</p><pre><code># ===== 融合重要性采样的改进粒子群优化算法（IS-PSO） =====enhanced_pso_with_is &lt;- function(converge_threshold = 1e-6, min_diversity = 1e-4, rand_seed = NULL) { # 设置随机种子，保证结果可重复 if (!is.null(rand_seed)) { set.seed(rand_seed) }# 步骤1：重要性采样预生成大量权重样本 pre_sample_total &lt;- 500 # 预生成500个随机权重样本 pre_weight_matrix &lt;- matrix(runif(pre_sample_total * asset_num), pre_sample_total, asset_num) # 权重归一化处理（确保所有资产权重和为1） pre_weight_matrix &lt;- t(apply(pre_weight_matrix, 1, function(x) x / sum(x))) # 计算每个预生成样本的目标函数值 pre_obj_scores &lt;- apply(pre_weight_matrix, 1, calc_objective_func)# 步骤2：筛选前20%的优质权重样本 top_sample_indexes &lt;- order(pre_obj_scores, decreasing = F)[1:(pre_sample_total * 0.2)] top_weight_samples &lt;- pre_weight_matrix[top_sample_indexes, ] sample_central &lt;- colMeans(top_weight_samples) # 优质样本的中心位置 sample_cov_matrix &lt;- cov(top_weight_samples) # 优质样本的协方差矩阵# 步骤3：生成初始粒子群（80%来自优质区域，20%随机生成） particle_positions &lt;- matrix(0, particle_total, asset_num) for (i in 1:particle_total) { if (i &lt;= particle_total * 0.8) { # 80%粒子从优质样本区域生成，考虑资产间相关性 asset_dim &lt;- asset_num # 尝试对协方差矩阵做Cholesky分解（添加小值保证矩阵正定） chol_matrix &lt;- try(chol(sample_cov_matrix + 1e-6 * diag(asset_dim)), silent = TRUE) if (inherits(chol_matrix, "try-error")) { # 分解失败时，使用对角协方差生成扰动值 disturbance_val &lt;- sqrt(diag(sample_cov_matrix)) * rnorm(asset_dim) } else { # 分解成功则生成符合协方差分布的扰动值 disturbance_val &lt;- chol_matrix %*% rnorm(asset_dim) } # 调整扰动幅度（0.2为实战验证的经验系数） temp_weight &lt;- sample_central + 0.2 * disturbance_val temp_weight &lt;- pmax(temp_weight, 0) # 保证权重非负（不允许卖空） } else { # 20%粒子随机生成，维持粒子群的多样性 temp_weight &lt;- runif(asset_num) } # 对生成的权重做归一化处理 particle_positions[i, ] &lt;- temp_weight / sum(temp_weight) }# 省略：粒子速度初始化、个体最优/全局最优参数初始化代码 ......# 省略：粒子位置/速度迭代更新、收敛条件判断的核心循环代码 ......# 返回算法最优结果 return(list( best_weight_config = global_best_pos, best_objective_val = -(global_best_score), portfolio_annual_return = sum(global_best_pos * return_vector), portfolio_annual_risk = sqrt(t(global_best_pos) %*% cov_mat %*% global_best_pos), converge_iterations = iter_count ))}</code></pre><p><strong>代码说明</strong>：</p><ul><li>核心逻辑是通过预采样筛选优质权重样本，让80%的初始粒子聚焦在最优解附近，减少无效搜索；</li><li>变量名如<code>n_pre_samples</code>改为<code>pre_sample_total</code>、<code>n_assets</code>改为<code>asset_num</code>，更贴合中文使用习惯；</li><li>省略部分为PSO算法通用的迭代更新逻辑（如粒子速度调整、收敛判断循环），可参考常规PSO实现补充。</li></ul><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047605921" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>专题：2025年游戏科技的AI革新研究报告</h3><h3><a name="t8" target="_blank"/>原文链接：<a href="https://link.segmentfault.com/?enc=Myp5h5WC0B%2BASmWAPSACAw%3D%3D.imfeGLiOhCySBr7wnb1yRHJh7AFbDE9pZEoqJn2NKbA%3D" rel="nofollow" title="https://tecdat.cn/?p=44082" target="_blank">https://tecdat.cn/?p=44082</a></h3><hr/><h4><a name="t9" target="_blank"/>3 IS-PSO算法在A股中的实际应用效果</h4><p>我们选取沪深A股10只不同行业的股票（覆盖装备制造、信息技术、环保、医药等领域），以2020年7月至2025年6月共1198个交易日的收盘价为基础数据，先计算日对数收益率（Rt = ln(Pt/Pt-1)），再年化处理得到预期收益率和协方差矩阵，对比等权重配置、梯度下降算法、传统PSO、IS-PSO四种方式的应用效果。</p><h5>3.1 收敛效率大幅提升</h5><p>传统PSO算法平均需要120次迭代才能收敛，而IS-PSO仅需31次迭代，收敛速度提升74.4%。在计算效率上，IS-PSO平均运行时间为66.789毫秒，远低于传统PSO的232.732毫秒，这意味着在实际应用中，IS-PSO能更快给出最优配置方案，降低计算资源消耗。</p><h5>3.2 收益风险比更优</h5><p>在高风险厌恶场景（λ=0.9）下，IS-PSO配置的投资组合平均收益达到0.165，高于传统PSO的0.157；且IS-PSO生成的权重呈现“稀疏性”——仅聚焦2只核心股票，却实现了更高的收益风险比。这对中小投资者而言，大幅降低了选股和资金配置的门槛，无需分散到多只股票就能实现风险与收益的平衡。</p><h5>3.3 权重配置结果可视化</h5><p>（空行）  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605922" alt="" title="" loading="lazy"/>  <br/>（空行）  <br/>上图清晰展示了不同算法的权重配置差异：梯度下降算法配置的资产数量多，但收益表现不如PSO类算法；IS-PSO与传统PSO均聚焦少数核心资产，但IS-PSO在收敛速度和高风险场景下的收益表现更优，更适合普通投资者使用。</p><h4><a name="t10" target="_blank"/>4 应急修复服务与落地建议</h4><p>针对算法落地过程中可能出现的代码运行异常、结果不符合预期等问题，我们提供<strong>24小时响应的应急修复服务</strong>，相比投资者自行调试，问题解决效率提升40%，能快速定位并解决代码报错、参数设置不当、数据适配异常等问题。  <br/>在实际落地层面，IS-PSO算法可直接适配中小投资者的需求：只需导入股票收盘价数据，设置风险厌恶系数，算法就能自动输出最优权重配置。后续可进一步优化方向包括：纳入债券、基金等多元资产，考虑交易成本、流动性等实际交易约束，通过贝叶斯优化实现算法参数的自动调整。</p><h4><a name="t12" target="_blank"/>总结</h4><ol><li>针对A股投资组合优化的非凸性问题，融合重要性采样的PSO算法（IS-PSO）通过优化初始粒子群分布，将收敛速度提升74.4%，大幅提升计算效率；</li><li>IS-PSO在高风险厌恶场景下收益表现更优，且生成的稀疏权重配置降低了中小投资者的实操门槛；</li><li>该算法已通过实际咨询项目验证，配套的24小时应急修复服务可保障算法稳定落地应用。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[手把手教你用云效 MCP 实现项目自动化管理 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047605929</link>    <guid>https://segmentfault.com/a/1190000047605929</guid>    <pubDate>2026-02-11 17:06:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：延枚</p><p>云效 MCP Server 已正式发布。它是一个为研发全生命周期提供统一可编程能力的元控制平面，旨在打通工具间的壁垒，实现研发流程的高度自动化。</p><p>目前，MCP Server 已提供超过 <strong>150</strong> 个原子能力（API），全面覆盖云效的核心模块，包括：</p><ul><li><strong>组织管理：</strong> 组织列表、部门信息、角色、成员信息等</li><li><strong>代码管理：</strong> 代码仓库、分支、合并请求、文件树操作等</li><li><strong>项目管理：</strong> 项目、工作项、字段配置、评论、工时管理等</li><li><strong>流水线管理：</strong> 流水线、资源、标签、部署等</li><li><strong>制品仓库管理：</strong> 制品仓库、制品列表等</li><li><strong>应用交付：</strong> 部署单、应用、应用标签、变量组等</li><li><strong>测试管理：</strong> 测试用例、用例目录、测试计划、测试结果等</li></ul><p>通过将云效 MCP Server 与自定义脚本，或与通义灵码、Cursor、iFlow-Cli 等本地/云端大模型工具结合，研发团队可以赋予程序直接“操作云效”的能力，从而高效完成各类重复性工作。</p><p>例如，在项目协作场景中，你可以实现：</p><ul><li><strong>智能查询：</strong> 快速获取指定项目、迭代或成员名下的需求列表。</li><li><strong>自动编排：</strong> 将复杂需求自动拆解为子任务，并生成验收标准。</li><li><strong>批量处理：</strong> 一次性更新多个工作项的状态、负责人或标签。</li><li><strong>数据同步：</strong> 读取 Excel 或其他系统的数据，批量在云效中创建工作项。</li></ul><p>为了帮助大家快速上手，我们推出了 <strong>【玩转云效 MCP】</strong> 系列专题文章。</p><p>本篇作为系列的第一篇，将聚焦于<strong>项目管理与协作</strong>场景，提供一系列“即刻可用”的 Prompt 示例与实战演练，手把手教你如何利用 MCP 实现项目管理的自动化。</p><h2>前期环境准备</h2><ol><li>在本地准备好你的 AI 工具：</li></ol><ul><li><p>通义灵码</p><p><a href="https://link.segmentfault.com/?enc=FfQH%2FPeIQUwtaLsDobV80A%3D%3D.fRfHmSvIS7enfsOzaRajMCRFldwonzE%2BBOkgYYojF%2BY%3D" rel="nofollow" target="_blank">https://lingma.aliyun.com/</a></p></li><li><p>Qoder</p><p><a href="https://link.segmentfault.com/?enc=j1w8%2B%2BTn32KeEFZbhuhSfA%3D%3D.bedgErDWf%2BIUZsd0FD4tZhes%2F%2BR6WfBNky14yfuUwbQ%3D" rel="nofollow" target="_blank">https://qoder.com/</a></p></li><li><p>Cursor</p><p><a href="https://link.segmentfault.com/?enc=0mi%2FJYMnn%2FUwHtL80RwgEg%3D%3D.gBaF21T201Rx9Ooel2ilIlhk6rHNGzmw%2BBQXBR8Cgjc%3D" rel="nofollow" target="_blank">https://cursor.com/cn</a></p></li><li><p>Iflow-Cli</p><p><a href="https://link.segmentfault.com/?enc=FObA%2BzLiChRy9GcRRa9tNQ%3D%3D.KatZI3foAKy8ipRrlqfFA8JdABSPW0C81c4xVc1qtNk%3D" rel="nofollow" target="_blank">https://iflow.cn/</a></p></li><li>……</li></ul><ol start="2"><li>按照云效 MCP Server 的说明完成配置（包含 Token、组织信息等）。</li></ol><ul><li><p>配置说明参考：GitHub 文档</p><p><a href="https://link.segmentfault.com/?enc=vJvv6d4lQ8ysNrJ83NwO%2Bg%3D%3D.QQtZLq9o1nt0bfAKvTvPvnsPVdN3BTCQyRk8l5AUpV0QWMQNMiRZ1780LGw3xOvuK%2FyGDp%2BEjSDoBTx%2BhBcmIuRedkG%2B2MLM0cWsABuY2a%2FeDFt1PKRDSFiHQCLYyrZj" rel="nofollow" target="_blank">https://github.com/aliyun/alibabacloud-devops-mcp-server/blob/master/README.zh-cn.md</a></p></li></ul><p>配置完成后，你的 AI 工具就可以通过 MCP 协议直接调用云效的项目 / 工作项等接口。</p><h2>检查 MCP Server 配置是否生效</h2><p>完成配置后，可以先用两条最简单的 Prompt 做“自检”。本文中演示示例的 AI 工具为 Qoder。</p><h3>1. 查看当前组织信息</h3><pre><code>查看云效当前的组织信息</code></pre><p>预期：AI 会调用 <code>YUNXIAO/GET_CURRENT_ORGANIZATION_INFO</code>，并返回：</p><ul><li>组织 ID（lastOrganization）</li><li>用户 ID</li><li>用户名</li></ul><p>这些信息后续会被用来继续检索项目、工作项等。</p><h3>2. 查看当前用户信息</h3><pre><code>查看云效当前的用户信息</code></pre><p>预期：AI 会调用 <code>YUNXIAO/GET_CURRENT_USER</code>，并返回用户名、邮箱、组织 ID 等相关信息。</p><p>只要上述两个调用能正常返回结果，基本可以认为 MCP Server 配置是正确的。</p><h2>实用场景 1：检索+统计/批量处理</h2><p>云效 MCP 提供了非常丰富的检索能力：</p><ul><li><strong>项目级：</strong> 按项目名称、状态、创建时间等检索项目</li><li><strong>工作项级：</strong> 按标题、描述、状态、优先级、负责人、标签等检索工作项</li></ul><p>你可以先让 AI 告诉你“有哪些可用条件”，再组合场景：</p><pre><code>云效中检索项目都有哪些条件可使用？
云效中对于检索工作项都提供了哪些条件？</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605931" alt="image" title="image"/></p><p>拿到条件后，就可以开始“场景编排”了。</p><h3>1. 按创建人检索工作项</h3><pre><code>查看 云效正式自动化 组织中 bowentestmcp 项目 我自己创建的工作项</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605932" alt="image" title="image" loading="lazy"/></p><p>AI 一般会自动完成：</p><ol><li>查询你所属组织列表</li><li>切换到“云效正式自动化”组织</li><li>查询该组织下名为 bowentestmcp 的项目</li><li>在该项目下检索“我创建的工作项”</li></ol><h3>2. 基于结果做统计 / 批量修改</h3><p>拿到工作项列表后，可以继续下发指令：</p><pre><code>把这两个工作项的状态改为已完成</code></pre><p>AI 会：</p><ol><li>查询该项目的工作流信息，确认“已完成”状态的 ID（比如：100014）</li><li>使用 <code>YUNXIAO/UPDATE_WORK_ITEM</code> 批量更新状态</li><li>再次检索校验修改是否成功</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605933" alt="image" title="image" loading="lazy"/></p><h3>3. 更多检索 + 批量处理示例 Prompt</h3><p>以下 Prompt 都是可以直接用的“套路”：</p><ul><li><strong>按标签批量打标</strong></li></ul><pre><code>某某项目下近一周我创建的需求，请统一加上「一期」标签</code></pre><ul><li><strong>按迭代统计</strong></li></ul><pre><code>统计一下某某项目下，迭代名为「xx」的需求数以及完成情况分析</code></pre><ul><li><strong>按状态批量流转</strong></li></ul><pre><code>请帮我找出 bowentestmcp 项目中所有状态为「已完成」的需求，然后统一将它们的状态改为「已关闭」</code></pre><ul><li><strong>按标题关键词 + 批量调优先级</strong></li></ul><pre><code>查询 xx 项目中所有标题包含「登录」或「注册」的需求，将它们的优先级统一调整为「高」</code></pre><ul><li><strong>按创建人 + 批量改负责人</strong></li></ul><pre><code>找出我创建的所有待处理状态的任务，把它们全部指派给张三（工号：xxx）</code></pre><h2>实用场景 2：拆分需求</h2><p>大模型 + MCP 非常适合做“把一个大需求拆成很多小需求并直接录入云效”这类工作。</p><h3>1. 按功能点拆分父需求</h3><p>示例：已有父需求 <code>QAAB-3</code>，描述里包含一段“功能列表”：</p><ul><li>支持加法运算</li><li>支持减法运算</li><li>支持乘法运算</li><li>支持除法运算</li></ul><p>你只需要一句：</p><pre><code>QAAB-3 这个工作项，请按照里面的功能点描述建立相应的子需求</code></pre><p>AI 会：</p><ol><li>查询 QAAB-3 的详细描述</li><li>自动解析描述中的有序列表（1/2/3/4）</li><li>通过 <code>YUNXIAO/CREATE_WORK_ITEM</code> 依次创建 4 个子需求</li><li>每个子需求继承父需求的类型、负责人等</li></ol><p>最终在云效需求列表页就能看到新创建的需求：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605934" alt="image" title="image" loading="lazy"/></p><h3>2. 更多拆分需求的示例 Prompt</h3><ul><li><strong>按实现层拆分</strong></li></ul><pre><code>找到「实现用户登录功能」这个需求，将它拆分成：前端 UI、后端接口、数据库设计三个子任务</code></pre><ul><li><strong>按列表自动拆分</strong></li></ul><pre><code>查看 QAAB-8 的需求描述，自动识别其中的功能点列表（如 1. 2. 3.），为每个功能点创建一个独立的子需求</code></pre><ul><li><strong>按开发阶段拆分</strong></li></ul><pre><code>将「实现用户注册机制」这个需求拆分为：
- 需求分析
- 技术方案设计
- 前端开发
- 后端开发
- 联调测试
- 上线部署每个阶段创建一个子任务</code></pre><ul><li><strong>按 INVEST 原则拆分大需求</strong></li></ul><pre><code>QAAB-5 这个需求过大，请按照 INVEST 原则将它拆分成：
- 独立的（Independent）
- 可协商的（Negotiable）
- 有价值的（Valuable）
- 可估算的（Estimable）
- 小的（Small）
- 可测试的（Testable）
多个小需求</code></pre><h2>实用场景 3：优化需求内容</h2><p>很多需求最初往往只是“半句话”，比如：</p><p>支持乘法运算：实现计算器的乘法运算功能，输入两个数，输出两数之积。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605935" alt="image" title="image" loading="lazy"/></p><p>这类需求很难直接用于评审 / 开发 / 测试。借助 MCP，你可以让 AI：</p><ul><li>补全为<strong>用户故事</strong>形式</li><li>写出<strong>业务流程与影响分析</strong></li><li>给出<strong>可测试的验收条件</strong></li><li>关键是：<strong>写回云效原工作项中</strong></li></ul><h3>1. 完整优化一个需求示例（QAAB-5）</h3><pre><code>QAAB-5 这个需求，请进行业务分析优化：
要求：
1. 改为用户故事的结构
2. 分析业务流程和影响
3. 提供验收条件</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605936" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605937" alt="image" title="image" loading="lazy"/></p><p>AI 典型的处理步骤：</p><ol><li>读取 QAAB-5 的原始描述（“实现计算器的乘法运算功能...”）</li><li><p>生成：</p><ul><li>用户故事（作为谁 / 我希望 / 以便）</li><li>业务流程（打开计算器 → 输入数字 → 选择乘号 → 输入第二个数 → 点击等号 → 展示结果）</li><li>业务影响（对前端 / 后端 / 错误处理 / 测试等的影响）</li><li>验收条件（覆盖正数、负数、小数、0、边界值、非法输入、性能等场景）</li></ul></li><li>使用 YUNXIAO/UPDATE_WORK_ITEM 将上述内容整体写回 QAAB-5 的描述字段</li></ol><p>优化完成后，在云效中查看 QAAB-5，就会看到一个完整、可评审、可测试的需求说明。</p><h3>2. 更多需求优化的示例 Prompt</h3><ul><li>改写为用户故事格式</li></ul><pre><code>查看「实现用户登录功能」这个需求，将它改写为用户故事格式：
- 作为【谁】
- 我希望【做什么】
- 以便【达成什么价值】
并直接更新回工作项</code></pre><ul><li>批量用户故事化技术需求</li></ul><pre><code>找出所有技术描述类的需求（标题以「实现」开头），将它们统一改写为用户故事格式，突出用户角色和业务价值</code></pre><ul><li>补充验收条件</li></ul><pre><code>QAAB-8 缺少验收标准，请根据需求描述补充至少 5 条可量化的验收条件，
包括：
- 功能性验收
- 性能要求
- 边界条件
- 异常处理
并更新回原需求</code></pre><ul><li>补充测试场景</li></ul><pre><code>查看「支持乘法运算」需求，补充完整的测试场景：
- 正常场景（正数、负数、小数）
- 边界场景（0、极大值、极小值）
- 异常场景（非法输入、溢出）
更新回原需求的验收条件部分</code></pre><ul><li>批量为无验收条件的需求补充标准</li></ul><pre><code>找出所有没有验收条件的需求（描述中不包含「验收」关键词），
为每个需求根据其标题和描述自动生成 3-5 条验收标准，并写回工作项</code></pre><h2>实用场景 4：批量导入需求</h2><p>当你已经有一份 Excel / CSV 需求列表时，可以直接让 AI + MCP 帮你“批量录入到云效”。</p><h3>1. 从 Excel 表格导入示例</h3><p>假设有一个 Excel：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605938" alt="image" title="image" loading="lazy"/></p><p>只需要一句 Prompt：</p><pre><code>请将「需求列表.xlsx」中的内容录入到云效 bowentestmcp 这个项目中</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605939" alt="image" title="image" loading="lazy"/></p><p>AI 的典型处理流程：</p><ol><li>解析 Excel 结构（首行是表头：标题 / 内容 / 优先级）</li><li><p>将每一行映射为：</p><ul><li>subject → 标题</li><li>description → 内容</li><li>priority → 优先级（高 / 中 / 低）</li></ul></li><li>调用 <code>YUNXIAO/CREATE_WORK_ITEM</code> 创建 4 条新需求</li></ol><p>回到云效 bowentestmcp 项目的需求列表页，就能看到刚刚导入的 4 条需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605940" alt="image" title="image" loading="lazy"/></p><h3>2. 更多批量导入的示例 Prompt</h3><ul><li>从“产品需求.xlsx”导入</li></ul><pre><code>从「产品需求.xlsx」导入需求，全部创建为「产品类需求」类型，指派人统一设置为我本人</code></pre><ul><li>指定字段映射导入</li></ul><pre><code>导入「backlog.xlsx」到 bowentestmcp 项目：
- 第 1 列（需求名称） → 标题
- 第 2 列（详细说明） → 描述
- 第 3 列（重要程度） → 优先级（高/中/低）
- 第 4 列（负责人姓名） → 指派人
- 第 5 列（所属模块） → 标签</code></pre><ul><li>带层级关系导入</li></ul><pre><code>导入「需求层级.xlsx」，根据「父需求ID」列建立父子关系：
- 第一级：Epic / 主题需求
- 第二级：Feature / 功能需求
- 第三级：Story / 用户故事
自动建立层级关联</code></pre><ul><li>自动识别表头并导入</li></ul><pre><code>分析「需求文档.xlsx」的表头结构，自动识别对应的字段映射关系，将数据导入 bowentestmcp 项目</code></pre><ul><li>导入前先做数据校验</li></ul><pre><code>导入「需求池.csv」前先验证：
- 必填字段不能为空（标题、描述）
- 优先级只能是「高/中/低」
- 负责人必须是项目成员
- 标题长度不超过 50 字
验证通过后再批量创建</code></pre><h2>更多项目管理场景示例</h2><p>在前面几个场景基础上，还可以进一步组合出更复杂的“项目级”能力。</p><h3>1. 项目健康度检查</h3><pre><code>分析 bowentestmcp 项目健康状况：
- 统计各状态需求分布
- 检查逾期未完成的需求
- 识别长期无人认领的需求
- 分析需求平均完成周期
- 检测可能的瓶颈（某状态停留过久）
生成健康度报告</code></pre><p>AI 可以基于 SEARCH_WORKITEMS 等接口，做出一份结构化的项目健康度分析报告。</p><h3>2. 迭代规划</h3><pre><code>为即将开始的 Sprint 5 规划任务：
1. 从需求池中筛选高优先级需求
2. 智能推荐适合本迭代的需求组合
3. 自动分配给合适的成员
4. 创建迭代并关联需求</code></pre><h3>3. 迭代回顾</h3><pre><code>为刚结束的 Sprint 3 生成回顾报告：
- 完成需求数 vs 计划需求数
- 各成员完成情况统计
- 延期需求分析
- 紧急插入需求统计
- 提取改进建议</code></pre><h3>4. 工作负载分析</h3><pre><code>分析 bowentestmcp 项目团队成员工作负载：
- 统计每人当前进行中的任务数
- 计算每人的工作时长总和
- 识别负载过重或过轻的成员
- 建议任务重新分配方案</code></pre><h3>5. 需求质量评估</h3><pre><code>批量检查 xx 项目中所有「待开发」状态的需求质量：
- 描述完整性（是否包含背景、目标、验收标准）
- 验收条件清晰度（是否可测试）
- 依赖关系完整性
- 工作量评估准确性
不合格的标记为「待补充」并通知负责人</code></pre><h3>6. Bug 关联需求分析</h3><pre><code>分析 xx 项目中 Bug 与需求的关联：
- 统计每个需求关联的 Bug 数量
- 识别高缺陷率的需求
- 分析 Bug 产生的阶段（开发/测试/生产）
- 提供质量改进建议</code></pre><h2>小结</h2><p>通过本文的实战演练，我们看到：当云效 MCP Server 与自动化脚本或智能体结合，它不再仅仅是一组 API，而是一种全新的研发协作范式。它将“会用云效”的人，从大量机械操作中解放出来。总结来说，MCP 为项目管理者带来了三大核心价值的转变：</p><ul><li><strong>执行指令化：</strong> 将原本需要数十分钟的手动点击，压缩为几行指令或一句自然语言描述。</li><li><strong>经验模板化：</strong> 将个人的最佳实践与团队规范，沉淀为可复用、可共享的自动化模板。</li><li><strong>精力聚焦化：</strong> 将管理者从繁琐的工具操作中解放，回归到思考产品、业务和团队等更高价值的工作上。</li></ul><p>云效 MCP Server 就像一套高效的“项目协作外骨骼”——它增强你的能力，放大你的效能，让你跑得更快、更远。而这，仅仅是开始。<strong>项目管理是 MCP 能力版图的第一块拼图。</strong></p><p>在下一篇文章中，我们将深入 <strong>【代码管理】</strong> 场景，探索如何通过 MCP 实现分支自动创建、权限精细化管理、代码合规性检查等高阶玩法。</p><p>敬请期待！也欢迎你在评论区分享使用心得，或告诉我们你最希望 MCP 在哪个领域帮你实现自动化。</p>]]></description></item><item>    <title><![CDATA[我是如何把 API 响应时间从 200ms 压到了 10ms 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047605959</link>    <guid>https://segmentfault.com/a/1190000047605959</guid>    <pubDate>2026-02-11 17:05:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>少年呀，当你遇到这样的情况：API 慢得像蜗牛，P95 延迟超高，服务器在凌晨 3 点因为流量突发而崩溃，你是选择花三个月用 Rust 重写所有东西，还是选择看着用户流失呢。</p><p>或者，你可以像我一样，用一种作弊的方式，把 Bun 的极致速度嫁接到 Node.js 的庞大生态上。</p><p>别笑，我是认真的。我就能在不重写 5 年陈旧业务逻辑的前提下，把一个臃肿的后端接口压进 10ms 以内的。</p><p><img width="723" height="384" referrerpolicy="no-referrer" src="/img/bVdnUD9" alt="image.png" title="image.png"/></p><h3>边缘侧使用 Bun，通过 IPC 唤醒 Node 进程池</h3><p>众所周知，Node 处理 HTTP 请求的开销太大了，但我的业务逻辑里全是依赖 <code>crypto</code> 和老旧 SDK 的代码，根本没法移植到 Bun。</p><p>所以我的解决办法是前店后厂。</p><p>我用 Bun 搭建了一个极薄的 HTTP 层，专门负责路由、参数校验和挡掉无效请求。只有真正需要那个老旧业务逻辑时，我才通过 IPC（进程间通信）把任务扔给后台常驻的 Node 进程。</p><p>千万别在请求来的时候才 <code>spawn</code> 一个 Node 进程，那样比单用 Node 还慢。你要做的是预先启动一组 Node Worker，要先预热才行。。</p><p><strong>Bun 端（前台）：</strong></p><pre><code class="typescript">// bun-gateway.ts
const textDecoder = new TextDecoder();
const textEncoder = new TextEncoder();

// 启动一个常驻的 Node 进程，而不是每次请求都启动
const nodeWorker = Bun.spawn(["node", "heavy-lifter.js"], {
  stdin: "pipe",
  stdout: "pipe",
});

// 这是一个简单的读写封装，把复杂的脏活扔过去
async function askNode(payload: any) {
  const msg = JSON.stringify(payload) + "\n";
  nodeWorker.stdin.write(textEncoder.encode(msg));
  
  // 这里简化了读取逻辑，生产环境记得处理粘包
  const reader = nodeWorker.stdout.getReader();
  const { value } = await reader.read(); 
  return JSON.parse(textDecoder.decode(value));
}

Bun.serve({
  port: 3000,
  async fetch(req) {
    if (req.url.endsWith("/fast")) return new Response("Bun is fast!");
    
    // 只有这种重活才找 Node
    if (req.url.endsWith("/heavy")) {
      const data = await req.json();
      const result = await askNode(data);
      return Response.json(result);
    }
    return new Response("404", { status: 404 });
  },
});</code></pre><p><strong>Node 端（后台）：</strong></p><pre><code class="javascript">// heavy-lifter.js
const readline = require('readline');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
  terminal: false
});

rl.on('line', (line) =&gt; {
  const data = JSON.parse(line);
  // 假装我们在做一个很重的加密运算
  // Node 生态里的老代码都在这儿跑，不用改
  const result = { processed: true, echo: data };
  console.log(JSON.stringify(result));
});</code></pre><p>这样搞，路由和 I/O 也是亚毫秒级的，而 Node 只需要处理纯计算，效率直接翻倍。</p><h3>别让 CPU 搬砖，学会利用 Bun 的零拷贝特性</h3><p>我发现服务器 CPU 居高不下，居然是因为我们在读取本地的配置文件和静态 JSON，然后序列化发给用户。</p><p>在 Node 里，通常会 <code>fs.readFile</code> 然后 <code>res.send</code>。这中间发生了好几次数据拷贝：从磁盘到内核，到用户空间 buffer，再到 socket。</p><p>在 Bun 里，我改用 <code>Bun.file()</code>。这不仅是写法上的区别，这是直接告诉操作系统：“把这个文件扔到网卡上去，别经过我的手。”</p><pre><code class="typescript">// 别再 readFile 了，直接流式传输
Bun.serve({
  fetch(req) {
    if (req.url.endsWith("/config")) {
      return new Response(Bun.file("./big-config.json"));
    }
    return new Response("404");
  }
});</code></pre><p>这一行代码改动，让我的静态资源吞吐量提升了 3 倍。</p><h3>排好队，微批处理（Micro-batching）</h3><p>高并发最可怕的是什么？是 1000 个请求同时涌进来，每个都要单独去调一次数据库或者调一次 Node 进程。就像刚下课，一堆学生全涌到食堂打饭。</p><p>而我加了一个极小的缓冲窗口。</p><p>如果在 3 毫秒内来了 50 个请求，我把它们打包成一个数组，一次性发给 Node 或者数据库。</p><pre><code class="typescript">let buffer: any[] = [];
let timer: Timer | null = null;

function processBatch() {
  const currentBatch = buffer;
  buffer = [];
  timer = null;
  // 一次性把 50 个任务发给 Node，而不是发 50 次
  askNode({ type: 'batch', items: currentBatch });
}

function enqueue(item: any) {
  buffer.push(item);
  // 只有在第一次推进来时启动计时器
  if (!timer) {
    timer = setTimeout(processBatch, 3); // 3ms 的延迟用户无感，但吞吐量巨大提升
  }
}</code></pre><p>这 3 毫秒的等待，换来的是 CPU 负载降低 60%。</p><h3>别在循环里 <code>new</code> 对象，求你了</h3><p>我审查代码时发现，很多人喜欢在 <code>fetch</code> 或者 <code>handleRequest</code> 里写 <code>const db = new DatabaseClient()</code> 或者 <code>const regex = new RegExp(...)</code>。</p><p>每次请求都重新分配内存、建立连接、编译正则，GC（垃圾回收）不炸才怪。</p><p>把所有能复用的东西——数据库连接池、<code>TextEncoder</code>、正则表达式、加密 Key，全部提到全局作用域。在 Bun 和 Node 混合架构里，这一点非常重要，因为我们追求的是极致的低延迟。</p><h3>双层缓存：内存不够，磁盘来凑</h3><p>以前我只用 Redis，但网络请求还是有开销。后来我发现，Bun 读取文件的速度超级快。</p><p>于是我搞了个双层缓存：</p><ol><li><strong>L1 内存缓存</strong>：用 LRU 存最热的 1000 个 Key，微秒级响应。</li><li><strong>L2 文件缓存</strong>：把稍微冷一点的数据直接写成 JSON 文件放在 <code>/tmp/cache/</code> 下。</li></ol><p>检查文件是否存在，比发起一个 TCP 请求去连 Redis 要快得多。</p><h3>丢掉那些臃肿的 npm 包</h3><p>以前在 Node 里，为了生成个 UUID 或者解析个参数，我们习惯性 <code>npm install uuid</code> 或者 <code>qs</code>。</p><p>在 Bun 里（其实现代 Node 也是），<code>crypto.randomUUID()</code>、<code>URLSearchParams</code> 都是内置的，而且是 C++ 层面优化的。</p><p>我把代码里所有非必要的 npm 依赖全部剔除，改用原生 API。这不仅让冷启动快了，更重要的是减少了 <code>node_modules</code> 的 I/O 噩梦。</p><h3>解决精神分裂的开发环境</h3><p>这一套架构就是Bun 做网关，Node 做计算。但在本地开发时，我差点崩溃。</p><p>我的电脑上本来跑着 Node 22，为了维护老项目，又要装 Node 14，还要装 Bun，甚至偶尔还要用 Deno 跑个脚本。</p><p><code>nvm</code> 切换来切换去让我心力交瘁，端口冲突、路径报错、环境变量乱成一锅粥。我经常是修好了 <a href="https://link.segmentfault.com/?enc=8dZIdGctv4OSXoinJ0Ukkg%3D%3D.VUG66AULKqZNQEenhVio1R4kQKwxFz7sW8iJ4AFjqjo61r9885EV261Ln6AUkIay" rel="nofollow" target="_blank">Bun 环境</a>，Node 的老项目又跑不起来了。</p><p>直到我发现了 ServBay，开发者的救命稻草，它不是那种简陋的版本切换器，它是一个完整的、隔离的运行环境平台。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnUEb" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>多版本并存</strong>：我可以同时开启 Node 14、Node 22 和 Bun 1.1 的环境，它们之间完全隔离，互不打架。</li><li><strong>一键全家桶</strong>：我需要的 Redis（做缓存）、PostgreSQL（存数据）、Caddy（做反代），它全都能一键安装并运行。</li></ul><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnUEc" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>零配置</strong>：我不由得感叹，以前为了配 Docker 和 Homebrew 浪费了不少时间啊。</li></ul><p>有了 ServBay，我在本地完美复刻了线上的混合架构：Bun 监听 3000 端口，Node 监听内部管道，Redis 跑在后台。我再也不用担心这是环境问题还是代码问题了。</p><h3>总结</h3><p>只要能把响应时间压进 10ms，我不在乎混用多少种运行时。</p><p>Bun 给了我速度，Node 给了我稳定性，ServBay 给了我一个不发疯的<a href="https://link.segmentfault.com/?enc=VzgGzy7uG3k8MzIV4WFdsg%3D%3D.QcTMNePLfDJRfxC4qYSStR%2B6TJqTdKrpSQkJrX3qyE0%3D" rel="nofollow" target="_blank">开发环境</a>。</p><p>别再纠结用 Bun 还是用 Node.js了，都成年人了，为什么不能两个都要。把它们结合起来，现在就去把你的 API 延迟砍掉 90%。</p>]]></description></item><item>    <title><![CDATA[内存占用最高降低75%，美国能源部科学家提出跨通道分层聚合方法D-CHAG，实现极大规模模型多通道数]]></title>    <link>https://segmentfault.com/a/1190000047605973</link>    <guid>https://segmentfault.com/a/1190000047605973</guid>    <pubDate>2026-02-11 17:04:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>基于视觉的科学基础模型在推动科学发现与创新方面具有巨大潜力，主要源于其能够聚合多样化来源的图像数据（例如不同的物理观测场景），并利用 Transformer 架构学习时空相关性。然而，图像的 token 化与聚合过程计算开销巨大，而现有的分布式方法如张量并行（TP）、序列并行（SP）或数据并行（DP），尚未充分解决这一挑战。</p><p>在此背景下，来自美国能源部橡树岭国家实验室的研究人员提出了一种面向基础模型的分布式跨通道分层聚合方法（Distributed Cross-Channel Hierarchical Aggregation, D-CHAG）。该方法对 token 化过程进行分布式处理，并采用分层策略进行通道聚合，从而使极大规模模型能够在多通道数据集上运行。研究人员在高光谱成像与天气预测任务上对 D-CHAG 进行了评估，将该方法与张量并行和模型分片相结合后，在 Frontier 超级计算机上最多可将内存占用降低 75%，并在最多 1,024 块 AMD GPU 上实现持续吞吐量提升超过 2 倍。</p><p>相关研究成果以「Distributed Cross-Channel Hierarchical Aggregation for Foundation Models」为题，已发表于 SC25。</p><p>研究亮点：</p><ul><li>D-CHAG 解决了多通道基础模型训练中的内存瓶颈和计算效率问题</li><li>与仅使用 TP 相比，D-CHAG 可实现最高 70% 的内存占用降低，从而支持更高效的大规模模型训练</li><li>在天气预测与高光谱植物图像掩码预测两种科学工作负载上验证了 D-CHAG 的性能</li></ul><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnUD7" alt="" title=""/><br/><em>论文地址：</em>\<br/><em><a href="https://link.segmentfault.com/?enc=HC5do4DTSaAHR2eLuBbNfw%3D%3D.NIGG8KkSGJCCjtp0%2Fbo2A9NeAJjN%2F9opoHLGJD6bnU0%2BJGLaAhIpr3mWIJG6%2Biym" rel="nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3712285.3759870</a></em>\<br/>关注公众号，后台回复「跨通道」获取完整 PDF</p><h2>使用两类典型的多通道数据集</h2><p>本研究使用了两类典型的多通道数据集来验证 D-CHAG 方法的有效性：植物高光谱图像（Hyperspectral Images）和气象 ERA5 数据集。</p><p>其中，用于自监督掩码预测的植物高光谱图像数据由 Oak Ridge National Laboratory（ORNL）高级植物表型实验室（APPL） 收集。数据集包含 494 张杨树（Poplar）高光谱图像，每张图像包含 500 个光谱通道，覆盖波长从 400nm 到 900nm。</p><p>此数据集主要用于生物质研究，是植物表型分析和生物能源研究的重要资源。这些图像用于掩码自监督训练，即将图像切片作为 token 进行 mask，模型的任务是预测缺失的内容，从而学习图像的潜在数据分布。值得注意的是，该数据集未使用任何预训练权重，完全基于自监督学习进行训练，这也凸显了 D-CHAG 在高通道自监督任务中的适用性。</p><p>此外，在气象预测实验中，研究团队使用了 ERA5 高分辨率再分析数据集。研究选择了 5 个大气层变量（位势高度、温度、风速 u 分量、风速 v 分量、比湿度）和 3 个地表层变量（2 米温度、10 米 u 分量风速、10 米 v 分量风速），覆盖超过 10 个压力层，总共生成 80 个输入通道。为了适配模型训练，原始分辨率为 0.25° 的数据（770 × 1440）被重网格化为 5.625°（32 × 64），采用 xESMF 工具包 和双线性插值算法完成。</p><p>模型任务是进行未来时间步的气象变量预测，例如 500 hPa 位势高度（Z500）、850 hPa 温度（T850）、10 米 u 分量风速（U10），从而验证 D-CHAG 方法在时间序列预测任务上的性能。</p><h2>D-CHAG ：将层级聚合与分布式 Token 化结合</h2><p>简单而言，D-CHAG 方法来自两种独立方法的融合，分别是：</p><p>分布式 token 化方法</p><p>在前向传播过程中，每个 TP rank 仅对输入通道的子集进行 token 化。在进行通道聚合步骤之前，需要执行一次 AllGather 操作，以便在所有通道之间实现跨通道注意力（cross-attention）。理论上，该方法能够降低每块 GPU 的 token 化计算开销。</p><p>层级跨通道聚合</p><p>这种方法的主要优势在于每个跨通道注意力层的内存占用减少，因为每层处理的通道数量更少。然而，增加层数会导致整体模型规模增大、内存使用增加。对于通道数量庞大的数据集而言，这种权衡更为有利，因为标准跨通道注意力的二次内存开销更高。</p><p>这两种方法虽然各有优势，但也存在一些不足，比如分布式 token 化方法在 TP rank 之间存在较高的通信开销，并未解决通道维度大内存占用的问题；而层级跨通道聚合方法会增加每块 GPU 上的模型参数数量。D-CHAG 方法通过分布式方式将两种方法结合起来，整体架构如下图所示：</p><p><img width="723" height="444" referrerpolicy="no-referrer" src="/img/bVdnUD8" alt="" title="" loading="lazy"/><br/>D-CHAG 方法在基础架构上的示意图</p><p>具体而言，每个 TP rank 对总通道子集中的二维图像进行 token 化。由于每块 GPU 仅持有全部通道的一部分，在这些通道上本地执行通道聚合——该模块称为部分通道聚合模块（partial-channel aggregation module）。在每个 TP rank 内完成通道聚合后，收集输出并使用跨通道注意力进行最终聚合。前向传播过程中仅需执行一次 AllGather 操作；在反向传播时，只收集每块 GPU 的相关梯度，从而避免额外通信。</p><p>D-CHAG 方法能够充分利用分布式 token 化和层级通道聚合的优势，同时缓解它们的不足。通过将层级通道聚合分布到 TP rank 上，研究人员将 AllGather 通信减少为每个 TP rank 仅需处理单个通道，在反向传播过程中无需任何通信。此外，通过增加模型深度保留了每层聚合处理通道数量减少的优势，同时通过部分通道聚合模块将额外模型参数分布到各 TP rank 上。</p><p>研究对比了两种实现策略：</p><ul><li>D-CHAG-L（Linear Layer）：层级聚合模块使用线性层，内存占用低，适合通道数较多的情况。</li><li>D-CHAG-C（Cross-Attention Layer）：使用交叉注意力层，计算成本较高，但在超大模型或极高通道数时性能提升显著。</li></ul><h2>成果：D-CHAG支持高通道数数据集上更大模型的训练</h2><p>在构建 D-CHAG 后，研究人员对模型性能进行了验证，然后进一步评估了其在高光谱成像与天气预测任务上的表现：</p><h3>模型性能分析</h3><p>下图展示了 D-CHAG 在不同部分通道聚合模块配置下的性能表现：</p><p><img width="723" height="420" referrerpolicy="no-referrer" src="/img/bVdnUEf" alt="" title="" loading="lazy"/><br/>图中展示了针对 1.7B 参数模型，在不同部分通道聚合模块配置下，每块 GPU 相对于仅使用 TP 基线的性能提升</p><ul><li>Tree0 表示部分聚合模块中仅有一层聚合，Tree2 表示两层，依此类推；</li><li>后缀 -C 和 -L 表示所用层的类型：-C 中所有层为 cross-attention，-L 中所有层为 linear</li></ul><p>结果显示：</p><p>对于 512 通道数据，使用单层 cross-attention 层的性能略低于基线，但对 1024 通道数据可提升约 60%。</p><p>随着层次结构加深，即便是 512 通道数据，也能获得明显性能提升，而 1024 通道数据的性能保持相对稳定。</p><p>使用 linear 层时，即使层次结构较浅，也能在 512 和 1024 通道图像上获得性能提升。实际上，最佳性能出现在 D-CHAG-L-Tree0，即仅包含一层通道聚合层。增加聚合层会增加模型参数，引入额外内存开销。虽然对于 512 通道情况，增加层数似乎有益，但对于两种通道规模，仅使用一层 linear 层的性能优于更深的配置。</p><p>D-CHAG-C-Tree0 在两块 GPU 时对性能略有负面影响，但扩展至 8 块 GPU 时可获得 60% 提升。</p><h3>植物高光谱图像的自监督掩码预测</h3><p>下图比较了基线方法与 D-CHAG 方法在高光谱植物图像掩码自编码器应用中的训练损失，结果显示：在训练过程中，单 GPU 实现与 D-CHAG 方法（在两块 GPU 上运行）的训练损失表现高度一致。</p><p><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnUEj" alt="" title="" loading="lazy"/><br/>基线方法与 D-CHAG 方法在高光谱植物图像掩码自编码器应用中的训练损失</p><p>橡树岭国家实验室分子与细胞成像组的高级研究员拉里·约克表示，D-CHAG 可以帮助植物科学家快速完成诸如直接从图像中测量植物光合作用活性等任务，从而取代费时费力的手动测量。</p><h3>天气预测</h3><p>研究人员在 ERA5 数据集上进行 30 天气象预测实验，下图比较了基线方法与 D-CHAG 方法在天气预测应用中的训练损失及三个测试变量的 RMSE：</p><p><img width="723" height="690" referrerpolicy="no-referrer" src="/img/bVdnUEo" alt="" title="" loading="lazy"/><br/>基线方法与 D-CHAG 方法在天气预测应用中的训练损失及三个测试变量的 RMSE</p><p>下表则展示了模型在 7、14 和 30 天预测任务上的最终对比，包括 RMSE、MSE 以及 Pearson 相关系数（即 wACC）</p><p><img width="723" height="430" referrerpolicy="no-referrer" src="/img/bVdnUEp" alt="" title="" loading="lazy"/><br/>D-CHAG 方法相较于单 GPU 训练在 7、14 和 30 天预测任务中的 MSE、RMSE 及 wACC 的百分比变化（% Δ）</p><p>结合图和表总体来看，训练损失与基线模型高度一致，各项指标的偏差极小。</p><h3>随模型规模扩展的性能</h3><p>下图显示了 3 种模型规模在需要使用 TP 的通道配置下，D-CHAG 方法相较于仅使用 TP 的性能提升：</p><p><img width="723" height="657" referrerpolicy="no-referrer" src="/img/bVdnUEq" alt="" title="" loading="lazy"/><br/>D-CHAG 方法结合 TP 的情况下，相较于仅使用 TP 时，7B、15B 和 26B 参数模型每个 GPU 的性能提升情况</p><p>结果显示，对于 7B 参数模型，使用部分通道聚合模块中的线性层（linear layers）可获得 30% 至 70% 的性能提升，而使用交叉注意力层（cross-attention layers）可获得 10% 至 60% 的提升；对于 15B 参数模型，性能提升超过 20% 至 50%；而 26B 参数模型的性能提升在 10% 至 30% 之间。</p><p>此外，在固定模型规模下，随着通道数增加，性能提升更明显，这是因为在给定架构下，增加通道数不会增加 transformer block 的计算量，但会增加 tokenization 和 channel-aggregation 模块的工作量。</p><p>另一方面，仅使用 TP 无法训练 26B 参数、256 通道图像，但使用 D-CHAG 方法时，可以训练 26B 参数、512 通道的模型，仅使用不到 80% 的可用内存——这表明该方法能够支持高通道数数据集上更大模型的训练。</p><h2>ViT：视觉 AI 从感知模型走向通用视觉基础模型</h2><p>过去十年，计算机视觉模型主要围绕「单任务优化」展开——分类、检测、分割、重建各自独立发展。然而，随着 Transformer 架构在自然语言领域催生出 GPT、BERT 等基础模型（Foundation Models），视觉领域也正在经历类似的范式转移：从任务特化模型走向通用视觉基础模型。在这一趋势下，Vision Transformer（ViT）被视为视觉基础模型的关键技术基石。</p><p>Vision Transformer（ViT）首次将 Transformer 架构完整引入计算机视觉任务，其核心思想是：将图像视为一系列 patch token 序列，用自注意力机制替代卷积神经网络的局部感受野建模。具体而言，ViT 将输入图像划分为固定大小的 patch，并将每个 patch 映射为 embedding token，然后通过 Transformer Encoder 建模 patch 之间的全局关系。</p><p>与传统 CNN 相比，ViT 对科学数据尤其具有优势：适合高维多通道数据（如遥感、医学影像、光谱数据），可处理非欧几里得空间结构（如气候格点、物理场），适用于跨通道建模（不同物理变量之间的耦合关系），这也正是 D-CHAG 论文所关注的核心问题。</p><p>除了上文研究中提及的场景，ViT 正在更多场景发挥核心价值。2025 年 3 月，北京大学国际医院皮肤科主任医师韩钢文携其团队开发出一种名为 AcneDGNet 的深度学习算法，这是一种融合视觉 Transformer 与卷积神经网络，能获取更高效的分层特征表，让分级更精准。经前瞻性评估表明，AcneDGNet 的深度学习算法不仅比初级皮肤科医生更准确，而且与高级皮肤科医生的准确性相当，能够在不同的医疗保健场景中同时准确地完成痤疮病变检测并判断严重程度，有效帮助皮肤科医生和患者在在线问诊和线下就医场景中诊断和管理痤疮。</p><p>论文标题：</p><p>Evaluation of an acne lesion detection and severity grading model for Chinese population in online and offline healthcare scenarios\<br/>论文地址：</p><p><a href="https://link.segmentfault.com/?enc=M%2Bkq4tTgvS%2F%2B83n0SSqUMw%3D%3D.UfGeLtaBrBFGzrzQ9gF2cnHJ5v4QxmqY%2BkpgHO7wOeNehi1yCC6IIYcN%2BufKzJ0bZhqJDLjjH6msXCBnBwCHfg%3D%3D" rel="nofollow" target="_blank">https://www.nature.com/articles/s41598-024-84670-z</a></p><p>从产业视角看，Vision Transformer 标志着视觉 AI 从感知模型走向通用视觉基础模型的关键拐点。其统一的 Transformer 架构为跨模态融合、规模化扩展与系统级优化提供了通用底座，使视觉模型成为 AI for Science 的核心基础设施。未来，围绕 ViT 的并行化、内存优化与多通道建模能力，将成为决定视觉基础模型产业落地速度与规模的关键竞争点。</p><p>参考文献：<br/>\<br/>1.<a href="https://link.segmentfault.com/?enc=DMehiJrP2nZ%2B2EQ%2FLeSDJA%3D%3D.Odz0qg1iBjflQ1SWVpqH%2FPXRp8bw2kfemmcd0RSkYb%2B9CoHtot1%2Be6PXPNRDdJfqDRabtPx%2FzZ203Enl%2BFHyMw%3D%3D" rel="nofollow" target="_blank">https://phys.org/news/2026-01-empowering-ai-foundation.html</a><br/>\<br/>2.<a href="https://link.segmentfault.com/?enc=UwXcIUpz98k2csPOUIrexA%3D%3D.1rUdce6OrGPiXCtQ3yy9sM13cdSMJ6k%2FAN5jSUPGqIkfrPKuiAQj2kiv8Z0L0Tyc" rel="nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3712285.3759870</a><br/>\<br/>3.<a href="https://link.segmentfault.com/?enc=hZvQYnQWnigT5oNlxKU13A%3D%3D.CEqBJKFn0UCP%2BTOBMWvY%2FDNue7WEyc3GsbN1QyJbhT2F9QRXzpcb8GUOyqUzIqKjKGVrqoIWe9z7QHrEg2NBNQ%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/JvKQPbBQFhofqlVX4jLgSA</a><br/></p>]]></description></item><item>    <title><![CDATA[智能制造企业如何选研发管理平台？看看行业标杆客户怎么说 PM老周 ]]></title>    <link>https://segmentfault.com/a/1190000047605997</link>    <guid>https://segmentfault.com/a/1190000047605997</guid>    <pubDate>2026-02-11 17:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>智能制造行业的研发协作，重点在于能不能把复杂链路管住：需求如何收敛、研发如何协同、测试如何闭环、交付如何可追溯、跨团队如何对齐、IPD/ASPICE等体系如何落地。当产品线越来越多、软硬件协同越来越频繁、跨地域研发成为常态时，一套能承载复杂研发节奏的研发管理平台，就会从“效率工具”变成“研发体系的数字底座”。</p><h2>一、智能制造行业的研发痛点</h2><p><strong>1）项目越来越多，信息越来越散，管理靠“人肉对齐”</strong></p><p>多项目并行、跨部门协同、跨地域交付，最典型的问题是：</p><ul><li>进度、风险、资源分布无法实时汇总</li><li>信息分散在群聊/邮件/表格/个人文档里</li><li>关键决策缺乏数据支撑，复盘也缺少事实链路</li></ul><p><strong>2）需求—研发—测试—交付链路断裂，质量与交付不可控</strong></p><p>智能制造企业通常兼顾硬件、软件、嵌入式、算法、平台等多团队协作，如果链路不统一，常见现象是：</p><ul><li>需求变更难追踪，交付范围经常漂移</li><li>缺陷无法闭环回溯，测试用例与版本发布脱节</li><li>交付质量依赖“项目经理盯人”，难规模化复制</li></ul><p><strong>3）流程体系想落地（IPD/ASPICE/功能安全），但缺少“可执行的数字化载体”</strong></p><p>很多企业在推 IPD、ASPICE、功能安全、矩阵式协作时，会遇到：</p><ul><li>流程写在制度里，执行落在个人习惯里</li><li>指标口径不统一，过程数据难沉淀</li><li>跨团队协作缺少统一的协作语言与工作项标准</li></ul><p><strong>4）权限与数据治理要求更高，既要共享协同，又要边界清晰</strong></p><p>尤其在供应链协作、跨部门共享与研发资料沉淀场景下：</p><ul><li>哪些信息该共享？哪些必须隔离？</li><li>权限边界是否能按组织/角色/项目分层治理？</li><li>项目过程资产如何沉淀为可复用知识？</li></ul><h2>二、针对这些痛点，ONES 的解题思路是什么？</h2><p>下面的能力点不是简单的功能罗列，而是把智能制造企业最常见的管理诉求，归纳为几类平台能力。</p><p><strong>1）把研发全过程放到一套统一协作体系里，让信息可视、可追溯</strong></p><p>在智能制造企业里，平台要能把工单、需求、任务、迭代、缺陷、测试等协作对象串起来，形成统一链路：</p><ul><li>项目全生命周期可视化跟踪</li><li>端到端追溯（从需求到上线/交付）</li><li>用数据支撑管理决策</li></ul><p><strong>2）把“流程标准化”变成可执行的线上机制，支撑多产品线、多团队协作</strong></p><p>平台不仅要能支持标准化，还要能在不同团队/不同项目下灵活配置：</p><ul><li>支持按团队/项目配置更贴合自身的协作模式</li><li>兼顾流程规范与灵活性</li><li>多项目并行时，依然能保持信息透明与对齐效率</li></ul><p><strong>3）支撑 IPD/ASPICE 等体系落地：把方法论固化成流程与数据</strong></p><ul><li>对推体系的企业来说，平台要能承载：</li><li>IPD 研发管理体系的数字化落地</li><li>ASPICE 认证相关过程要求的执行与追溯</li><li>研发流程的数字化与知识化沉淀</li></ul><p><strong>4）支撑跨部门、跨地域协同与权限治理：共享协作与边界管理并重</strong></p><p>平台需要既能促进共享，又能避免数据泄露和越权访问：</p><ul><li>多层权限保障信息安全</li><li>知识库与项目协作打通，同时保持权限边界可控</li><li>跨研发中心协作效率提升，降低交接成本</li></ul><h2>三、来自 ONES 智能制造行业标杆客户证言</h2><p>以下内容来自 ONES 智能制造相关行业客户证言。若你正在评估研发管理平台，可将这些原话作为参考样本，对照自身的研发链路复杂度（需求—研发—测试—交付）、跨团队协作与过程追溯要求进行判断。</p><p><strong>诺瓦星云：全球最大的 LED 显示解决方案服务商</strong></p><p>研发负责人：ONES 上线后，通过数据治理、流程梳理与系统配置培训，高效助力我司落地软件研发项目标准化建设，各产品线之间信息共享互通，提升了 IT 管理和研发效率。同时推动 IPD 项目在 ONES 落地，破解数据分散难题，降低管理难度。ONES Wiki 打通项目管理与文档管理的数据壁垒，解决了数据安全和权限管控问题。</p><p><strong>光格科技：产品服务于国家电网、南方电网、华能集团、国家电投等</strong></p><p>研发管理部副总监李老师：公司使用 ONES 系统已三年多了，ONES 对我们团队的研发管理效率起到很大提升，尤其是 ONES Wiki 知识库功能丰富，有效解决了部门间的沟通协作难题，显著提升了协作效率。</p><p><strong>埃斯顿：连续七年稳居中国工业机器人国产品牌出货量首位</strong></p><p>我司通过 ONES 实现项目在线化统一管理，串联起对待导入需求的收集及响应，有效管控研发参与的不同类型的项目，精细化到具体不同工作项的规范管理，整体提升项目的在线规范化管理、降低沟通成本、提升团队效能。</p><p><strong>良信股份：27年智慧低压电气解决方案专家</strong></p><p>软件开发部：感谢 ONES 实施团队在过去两年中给予我们的专业支持。你们不仅协助我们高效配置了需求、任务与缺陷管理流程，还将测试场景和用例系统纳入统一管理，使项目全链路清晰可控。每当遇到问题，团队总能快速响应、专业解决，帮助我们切实提升了研发协同效率与交付质量。诚挚感谢你们的用心付出!</p><p><strong>卡莱特：领先的视频图像领域综合化解决方案服务商</strong></p><p>项目管理部负责人：在引入 ONES 项目管理系统之后，我们实现项目全流程统一管理，并且可以根据不同团队和项目的实际情况进行灵活配置。通过属性、工作流和视图的搭配，项目组能打造更贴合自身的协作模式，提升信息透明度，沟通对齐的成本也随之降低。多项目并行时，ONES 能兼顾流程规范性和灵活性，高效提升项目管理效率。</p><p><strong>凌云光：机器视觉与光纤器件行业领导者</strong></p><p>通过 ONES 项目管理平台，我们实现了对复杂项目全生命周期的可视化跟踪与精准管控，全局进度一目了然，跨部门协同效率显著提升，它已成为我们保障项目如期上线不可或缺的“项目管理引擎”！</p><p><strong>瑞纳智能：智慧供热行业第一家上市公司</strong></p><p>瑞纳智能正处于技术升级与业务拓展的关键阶段，现阶段以研发创新为核心，依托 ONES 研发管理系统，实现了跨业务线的研发项目全生命周期高效管控，显著提升技术成果转化效率，加速智能化产业布局。</p><p><strong>科思科技：国内领先的电子信息装备供应商</strong></p><p>深圳科思科技依托 ONES 构建符合 IPD 核心理念的研发项目管理平台，实现研发流程数字化与知识化升级，系统性提升产品开发效率、质量与资源利用率，打造国际化端到端集成产品开发能力。</p><p><strong>中盾安民：国内最早研发生产安检产品的单位</strong></p><p>通过 ONES，我司实现了项目信息的可视化、流程化，提高了管理透明度和规范性，便于及时发现项目风险和问题，提升了团队协作效率，降低了沟通成本，为进一步精细化管理奠定了良好基础。</p><p><strong>鸿湖万联：软通动力旗下控股公司</strong></p><p>研发负责人：ONES 帮助我们将项目不同团队的工作流全面打通，实现了从需求到上线的端到端可视化追踪，通过多种图表形态，实时呈现项目表现，保障了软件研发过程的完整性、一致性和可追溯性，提高了研发效率。</p>]]></description></item><item>    <title><![CDATA[模型微调：AI+场景下的落地实践 Fabarta ]]></title>    <link>https://segmentfault.com/a/1190000047605999</link>    <guid>https://segmentfault.com/a/1190000047605999</guid>    <pubDate>2026-02-11 17:03:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：枫清实验室团队</p><h2>1.DeepSeek 带来的一些思考</h2><p>DeepSeek 的技术突破，并不仅体现在模型指标或参数规模上，而在于其清晰地展示了一种趋势：模型能力的跃迁，正在从“更大的预训练”转向中期+后训练阶段对“知识、行为与推理模式”的系统性对齐。这些变化，对工业界尤为重要。相比通用模型评测，企业场景往往具有以下特征：</p><p>任务分布高度集中，但规则与边界极其复杂，SOP与隐性经验并存；错误代价远高于“泛化能力不足”，模型“答错了还看起来很确定”往往比“不回答”更危险；模型的主要风险并非知识缺失，而是在规则边界与异常组合条件下，沿着看似合理但不可控的推理路径持续放大错误。</p><p>这些特征决定了企业对微调范式的需求，必然超越技术选型本身。因此，对大多数企业而言，真正的关注点并不在于"是否要用蒸馏或 SFT"，而在于如何用一条工程上可控、可复用、可演进的路径，对齐模型的真实业务能力？<br/>在这一视角下，模型训练不再是一次性的参数优化问题，而是一个由数据→ 结构 → 行为逐层约束、逐步收敛的系统工程。</p><h2>2.工业场景下的逐层约束</h2><p>在化工、单证、电磁频谱、跨境报关等客户的高专业度场景中，这些约束具体表现为：</p><ul><li>数据约束<br/>领域知识天然以结构化形式存在，但模型是参数化的。通用大模型并不天然具备对这些结构的稳定建模能力，而领域知识通常以如下形式存在：</li></ul><p>Plain Text<br/>(实体 A, 关系 R, 实体 B) + 条件/规则 C → 结论 D</p><p>例如：</p><ol><li>高分子聚合物A 在温度 T、压力 P 下 → 反应路径 X</li><li>报关品类HSCODE → 对应监管规则集合</li><li>频谱频段f → 允许/禁止的使用方式</li></ol><p>知识与行为约束在化工与电磁频谱等高专业度场景中，规则复杂、条件多维，模型“只学语言模式”远不足以支撑安全可靠的推理。需要采用结构化知识注入策略，将关键知识显式内化到模型参数中，实现多条件、多约束推理的稳定性电磁频谱。</p><p>Plain Text<br/>Ltotal = 语言建模损失 LLM(x,y) + 知识一致性损失 λLKG(y,K)</p><ul><li>推理与策略约束<br/>工业场景中，模型推理阶段所面对的输入往往更加嘈杂、不完整且高度偏向边界情况，当面临一些多约束推理时，推理轨迹容易产生漂移。例如，海关出口货物报关单中商品编码“4819100000” 字段提取0数量的遗漏，导致整单提取的失败。商品规格型号"切割冲孔过的"中核心语义的一致性问题导致进出口优惠的错误等等。同样边缘规则触发异常操作判断，例如单证业务中印章和信息重叠部分的提取错误也容易形成上游业务的级联崩塌。</li></ul><p>Plain Text<br/>Lon-policy = 分布对齐 KL(Pθ||Pteacher) + 约束惩罚 β[违反约束 c]</p><p>在此约束下，我们开始探索，在数据受限条件下的高质量数据集构建，并采用高效微调的方式注入领域知识，将复杂业务规则内化为模型行为，同时利用策略蒸馏建立可控的错误边界。</p><h2>3.高质量数据集构建</h2><h3>数据结构化</h3><p>在分子科学、化学与材料等高专业领域中，模型能力的瓶颈并不主要来自通用语义理解，而来自：专业概念密集，论证链条长，方法与条件强依赖上下文，表达高度依赖论文结构（章节、段落、实验设置）。将分散在学术PDF 中的隐性知识结构，转化为可用于模型训练与评估的高质量问答样本，并显式保留证据上下文与质量度量信息。</p><p>整个增强流程遵循两个基本原则：第一，先结构化，再生成。只有将原始论文从排版文档转化为段落级、章节级的可计算结构，LLM 生成的问题与答案才具有稳定语义边界。第二，生成必须伴随质量评估与自动筛选。在高风险专业领域，不能接受“看起来合理但事实上不严谨”的样本，数据构建过程本身必须引入自动评估闭环。从教材、多源学术文献出发，经过文档解析、结构重建、问题生成、质量评估、相似度校验与排序筛选，最终构建带有上下文与质量指标的高质量问答数据集。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606001" alt="图片1.png" title="图片1.png"/></p><p><strong>应用场景</strong><br/>本数据集结构化的构建流程应用于客户的材料研发人员内部检索与设计辅助系统，结构化后的问答数据语料作为领域模型SFT和 RAG 的检索知识单元，例如典型场景：</p><ul><li>逆合成信息查询</li><li>产物预测</li><li>反应条件查询</li><li>物质信息和化工材料专业查询等</li></ul><h3>数据即“隐性规则”的显式化载体</h3><p>大量企业知识并不存在于文档中，而是：</p><ul><li>SOP 的例外条款</li><li>老员工的经验判断</li><li>审批中的“潜规则”</li></ul><p>示例:海关申报中的"潜规则"</p><blockquote>老员工经验:"如果货值低于申报阈值但重量异常大,需要人工复核"</blockquote><p>形式化为: <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606002" alt="图片2.png" title="图片2.png" loading="lazy"/></p><p>数据构建策略:</p><ul><li>正样本生成:在约束边界内随机采样；</li><li>负样本生成:故意违反某个约束,并标注错误类型；</li><li>对比对生成:(X,Y正确,Y错误,差异说明)。</li></ul><h3>数据图谱化</h3><p>把知识最小单元，从句子级，降到关系级。这是做结构化控制的前提。把训练样本变成：「问题+ 结构约束上下文 + 多跳结构视角」。</p><p>首先，对原始问答数据进行联合语义解析，将隐含在自然语言中的领域知识抽取为实体—关系—实体 的可计算结构。</p><p>通过关系归一化（relation canonicalization），将高噪声、长尾的细粒度关系映射至有限的核心关系集合，并显式构建正反向关系，以提升结构连通性与推理可达性。<br/>在关系抽取完成后，将三元组组织为有向多关系图结构，允许实体间的多语义连接，以保留领域知识的结构复杂性。</p><p>针对具体问答样本，以文本中出现的实体为条件，进行受控的多跳子图扩展，构建与当前任务最相关的局部知识视角。通过显式限制扩展深度与节点规模，在覆盖潜在推理路径的同时抑制结构噪声。</p><p>通过上述流程，我们将原始问答数据中的隐性领域知识，逐步转化为可计算、可裁剪、可内化的结构化表示，并将其系统性地引入模型训练过程，从而在不进行全参数更新的前提下，显著增强模型对领域逻辑与推理结构的稳定建模能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606003" alt="图片3.png" title="图片3.png" loading="lazy"/></p><h2> 4.知识注入</h2><h3>图谱结构化</h3><p>注入使用Adapter 结构，避免破坏基础模型参数，将实体与关系嵌入注入注意力或 FFN 层，其本质不是“让模型记住更多事实”，而是：在参数层面引导模型形成稳定的结构化推理模式。</p><p>在大语言模型的多层Transformer 结构中插入轻量级图谱适配模块；图谱适配采用“降维—非线性变换—升维”的典型结构，参数规模远小于模型主体；将知识结构嵌入作为适配的输入或调制信号，与语言模型的隐状态进行融合；通过残差连接方式，确保原模型语言能力不被破坏。在训练阶段：冻结大语言模型的原始参数；仅更新 适配模块参数及知识结构编码模块参数；通过端到端训练实现知识表示与语言表示的协同对齐。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606004" alt="图片4.png" title="图片4.png" loading="lazy"/><br/><strong>应用场景</strong><br/>本方案应用于电磁频管业务，面向无线电管理与电磁空间治理场景，服务对象包括无线电管理机构、频谱规划部门及设备备案与用频审批单位，主要用于提升频谱法规理解、用频合规判断与复杂业务推理的自动化水平。</p><h3>Lora 微调领域知识特征表达</h3><p>与通用对话模型中的LoRA（W′=W+ΔW,ΔW=BA） 主要用于指令风格对齐不同，在材料化学领域，LoRA 承担的是：结构与规则敏感型能力的参数载体角色。具体体现在： </p><ul><li>学习SMILES 语法中的长距离依赖模式（环闭合、支链嵌套、立体标注等）；</li><li>学习反应物与产物之间的原子重排与结构映射规律；</li><li>学习反应条件、催化剂与产物分布之间的隐含关联；</li><li>学习逆合成中常见结构断裂模式与合成子映射方式。</li></ul><p>这些特征体现在损失函数的设计，损失函数计算采用了双损失机制：计算全局损失（所有token）以及针对仅化学关键token（如SMILES、分子式）的核心损失。通过差异化的损失计算策略,实现了"通用能力保持"与"领域知识强化"的有机平衡:<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606005" alt="图片5.png" title="图片5.png" loading="lazy"/><br/>全局损失在此不再赘述，核心损失如图所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606006" alt="图片6.png" title="图片6.png" loading="lazy"/><br/>核心token 包括:</p><ul><li>SMILES 字符串: 如 C1CCCCC1 (环己烷)</li><li>分子式: 如 C6H12 </li><li>IUPAC 命名: 如 "cyclohexane"</li><li>反应条件: 如 "催化剂: Pd/C", "温度: 120°C"</li><li>数值约束: 如 "[120-180°C]", "[0.5-2.0 MPa]"</li></ul><p>核心loss可以强制模型重点学习化学结构的精确表达,提升在分子生成、反应预测等核心任务上的性能，Core Loss 会放大 SMILES 生成错误的惩罚，即使整体语言流畅，SMILES 错误会导致 Core Loss 激增，引导模型优先学习化学结构的精确表达。</p><p><strong>应用场景</strong><br/>通过在科研智能体平台中集成领域模型，为上游业务提供统一的专业能力底座，从而增强智能化学检索、合成路径设计、反应条件优化等核心应用的底层推理引擎，同时支撑AI4S 智能体平台中的任务规划与工具调用，实现多工具协同下的自动化科研流程编排，提升整体科研效率与决策可靠性。</p><h3>策略蒸馏</h3><p>在单证识别场景中，主要错误并不集中在“是否识别到文本”，而集中在： </p><ul><li>数字位数错误（多一位/ 少一位）；</li><li>连续数字的重复与幻觉；</li><li>字段边界混乱导致的拼接错误；</li><li>对齐与排版诱导导致的空格、换行、列对齐偏差。</li></ul><p>这些错误具有一个非常明显的特征：错误高度依赖学生模型自身当前生成分布。<br/>我们采用on-policy 蒸馏范式：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606007" alt="图片7.png" title="图片7.png" loading="lazy"/></p><ul><li>由学生模型在当前参数状态下生成推理轨迹；</li><li>对其生成过程进行规则检测与结构校验；</li><li>由教师模型或规则系统对其行为进行反馈。</li></ul><p>单证识别的本质不是“条件文本生成”，而是视觉条件下的序列对齐与字符选择问题。<br/>在此场景下，on-policy 蒸馏在该任务中具有两个直接优势：<br/>第一，能直接纠正学生常犯路径上的分布偏差。</p><ul><li>在数字序列处，学生更容易进入“重复概率高”的状态；</li><li>在相邻字段边界，学生更容易错误拼接。</li></ul><p>教师在这些“学生最容易出错的位置”上提供分布监督，远比在 GT 轨迹上模仿有效。<br/>第二，天然抑制曝光偏差（exposure bias）。<br/>学生不再只在教师路径上被训练，而是在自己的生成分布上被纠正。</p><p><strong>应用场景</strong><br/>本方案面向高准确率票据与证照场景，在小数据规模下，通过大模型蒸馏与轻量微调，实现了收据类复杂文本的高精度识别，数字与关键字段的稳定输出，轻量模型可部署能力，同时兼顾跨文档类型的泛化能力。</p><h2> 5.总结</h2><p>本文章围绕工业高专业度场景下大模型的落地优化展开，以DeepSeek 带来的行业趋势思考为切入点，针对企业场景任务集中、错误代价高、推理易漂移的核心痛点，提出了数据→结构→行为逐层约束的大模型调优体系，并从高质量数据集构建、领域知识注入、策略蒸馏三大核心环节，阐述了可工程化、可复用的落地技术路径，核心是让大模型在数据受限的工业场景中，实现知识的精准内化、推理的稳定可控。上述数据约束、结构建模、知识注入与策略蒸馏方法，在客户的一部分场景中形成实践，例如报关审单、频谱活动知识问答、材料研发辅助、单证结构化识别等多个系统中持续迭代。</p>]]></description></item><item>    <title><![CDATA[中国工业AI平台出海成功案例有哪些？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047606011</link>    <guid>https://segmentfault.com/a/1190000047606011</guid>    <pubDate>2026-02-11 17:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在全球制造业加速向智能化、柔性化和绿色化转型的背景下，工业AI平台正成为驱动这一变革的核心引擎。它不再仅仅是自动化设备的升级，而是通过数据驱动、算法决策与系统协同，重构从设计、生产到服务的全链条能力。与传统工业软件不同，真正的全球工业AI平台必须具备跨文化适配性、多语言支持能力、本地化服务生态以及可复制的行业解决方案，才能在不同国家的制造体系中生根发芽。这种平台的竞争力，不再取决于单一技术的先进性，而在于能否将中国经验转化为全球通用的语言，让不同发展水平的制造企业都能找到适合自己的数字化路径。<br/>要理解这一趋势，必须认识到工业AI平台的本质是“系统性赋能”。它不是孤立的工具，而是一个融合了边缘计算、物联网、深度学习与流程优化的有机体。在德国、日本等制造业强国，企业长期依赖高精度设备与严谨的流程管理，对AI的接受度建立在可验证的稳定性之上；而在东南亚等新兴市场，企业更关注成本控制与快速落地，平台必须足够轻量化、易部署。因此，一个真正全球化的工业AI平台，必须在架构上支持模块化部署，在服务上具备本地化团队支撑，在标准上兼容国际认证体系。这要求企业不仅输出技术，更要输出方法论、人才体系与合作模式，形成“技术+服务+生态”的三位一体能力。<br/>在这一领域，广域铭岛已走出一条具有代表性的路径。依托吉利集团的全球制造网络，其Geega工业互联网平台已在马来西亚、新加坡、韩国等多个国家落地，不仅为宝腾汽车打造了柔性数字化工厂，更通过与当地企业合资成立AGYTEK DIGITAL，实现从技术输出到本地运营的深度转化。平台支持多语言界面、适配东盟地区低算力环境，并联合中马未来学院培养本地数字化人才，真正做到了“扎根当地”。与此同时，德国西门子的MindSphere平台凭借其在工业标准与协议上的深厚积累，成为欧洲制造企业首选的数字底座；而美国通用电气的Predix则以航空、能源领域的高复杂度场景为突破口，构建了面向重资产行业的预测性维护生态。这三家企业的共同点在于：不追求技术炫技，而是围绕客户真实痛点，构建可持续的闭环价值体系。<br/>中国工业AI平台的出海，已从“产品出口”迈向“生态共建”。它不是简单地把一套软件卖给海外客户，而是通过联合研发、人才共育、标准参编等方式，融入当地产业土壤。在马来西亚，其焊接质量AI系统将虚焊率降至0.02%以下，远超行业平均水平；在德国，慕尼黑市政府代表团专程来访，探讨工业互联网标准对接的可能性。这种双向互动，让“中国方案”不再被视作廉价替代品，而是具备创新价值的合作伙伴。<br/>全球工业AI平台的竞争，本质上是生态系统的竞争。谁能以更低的门槛、更高的韧性、更强的本地化能力，帮助制造企业实现从“能用”到“好用”再到“离不开”的转变，谁就能赢得未来。</p>]]></description></item><item>    <title><![CDATA[从能力升级到场景深耕：KubeSphere 2025年度全景回顾 KubeSphere ]]></title>    <link>https://segmentfault.com/a/1190000047606016</link>    <guid>https://segmentfault.com/a/1190000047606016</guid>    <pubDate>2026-02-11 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年，数字化浪潮迈入“深水区”。企业的核心诉求，已从搭建云原生实验田，转向寻求能承载关键业务、实现价值规模化的坚实平台。这正需要骐骥一跃的突破力，与稳驭核心的掌控力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606018" alt="" title=""/>.png)</p><p>KubeSphere 作为企业级多租户容器平台，正致力于成为这一转型中的可靠伙伴与稳健座驾。我们通过简化管理复杂性、整合业界最佳实践、提供开箱即用的生产级能力，帮助全球客户在多云与异构环境中，驾驭稳定、高效、自主可控的应用管理平面，从而加速创新交付、优化资源成本、保障业务永续。</p><p>回首过去一年，我们不仅实现了产品能力的关键一跃，更在与金融、政务、制造、教育等领域客户的并肩实践中，见证了平台价值在核心业务中的平稳落地与驾驭。</p><h2>产品深耕：夯实企业级基石</h2><p>2025年，我们通过一系列版本迭代持续打磨产品，最终发布的 <strong>v4.2.1 版本标志着 KubeSphere 在“体验卓越与生产就绪”的道路上迈出了坚实一步。</strong> 该版本及全年的更新始终围绕解决企业规模化应用的核心挑战展开，致力于为各行业客户提供更稳定、高效的管理体验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606019" alt="" title="" loading="lazy"/></p><h3>1. 强化异构资源统一纳管</h3><p>面对混合架构带来的管理复杂度，v4.2.1 版本致力于构建统一、高效的异构算力底座。平台增强了对 GPU/vGPU 等异构计算资源的统一纳管与调度适配，满足从AI推理到图形渲染的多元化算力需求。同时，通过集成智能数据缓存加速能力，显著降低了I/O密集型应用的存储访问延迟。这些能力共同为企业提供了一个可跨云、跨芯的标准化资源池，实现了算力资源的全局统筹与弹性供给。</p><h3>2. 升级弹性调度体系</h3><p>为实现极致的资源效率与成本优化，v4.2.1 引入了多维联动的智能弹性调度体系。平台创新性地一站式集成垂直伸缩（VPA）、水平伸缩（HPA）与事件驱动伸缩（KEDA）。VPA根据历史负载自动优化容器资源规格，避免浪费；HPA增强策略允许对扩缩容行为进行独立精细调控；KEDA则能将消息队列等外部事件直接转化为弹性信号，甚至支持副本数缩零以极致降本。</p><h3>3. 筑牢集群稳定防线</h3><p>为保障大规模、多集群生产环境的全局稳定，v4.2.1 在治理与可观测层面进行了重磅增强。平台新增节点组（Node Group） 能力，支持将物理节点逻辑分组并与企业空间绑定，完美支撑多租户资源隔离、信创环境混部等复杂场景。在运维层面，提供成员集群可视化在线升级与状态精准同步，极大简化了多集群生命周期管理。</p><h2>全球实践：深入关键场景</h2><p>产品的最终价值，在用户应对核心业务挑战时得到最真实的体现。2025年，KubeSphere 的可靠性、灵活性与开放性，在全球多个行业的关键业务场景中赢得了深度信任。</p><h3>国内实践</h3><ul><li><strong>某头部汽车金融公司</strong>：采用 KubeSphere 企业版构建混合云统一平台，在满足金融级强合规与隔离要求的前提下，实现了资源全局弹性调度与一站式运维，使整体运维效率提升超 40%，为创新金融业务的快速上线与稳定运行提供了强大支撑。</li><li><strong>某直辖市规划和自然资源局</strong>：打造“一云多芯”政务容器云平台，无缝纳管异构芯片架构资源，实现跨架构统一调度与标准化流程，有力支撑了智慧城市、空间规划等核心政务系统的数字化升级与高效协同。</li><li><strong>澳门气象局</strong>：选用KubeSphere可信版为其核心气象预报与监测系统构建云原生底座，在满足高安全与合规要求的同时，实现了资源的统一调度与服务的自动化运维，有力保障了气象业务7x24小时不间断稳定运行。</li></ul><h3>海外拓展</h3><ul><li><strong>欧洲 IT 解决方案商 AMPLUS S.A.</strong>：在为希腊某公立大学构建科研平台时，基于 KubeSphere 打造了标准化云原生管理方案。该选择显著降低了长期运维成本、赋予用户自主管理能力，并提升了方案的可复用性与交付效率，助力 Amplus 建立了可快速复用于其他教育及企业客户的产品化服务能力。</li></ul><h2>生态共建：汇聚行业力量</h2><p>2025年，KubeSphere 的技术影响力与生态认可度持续提升，<strong>GitHub 全球星标数突破 16,000，</strong> 这标志着产品价值获得了全球开发者的广泛关注。我们始终秉持协作共赢的理念，致力于与业界伙伴及用户共同构建一个充满活力的技术生态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606020" alt="" title="" loading="lazy"/></p><ul><li><strong>培育云原生新生力量</strong>：我们通过技术合作、联合创新及人才培养等多种形式，与众多行业伙伴及高校建立了深度链接，例如通过“开源之夏”等活动，成功孵化了智能运维助手等创新工具原型，推动了前沿技术的实用化探索。</li><li><strong>汇聚业界最佳实践</strong>：通过灵活的扩展组件架构，我们实现了与 Fluid（数据编排）、OceanBase（分布式数据库）等业界顶尖技术的深度集成。这使用户能够根据自身需求，灵活选用并组合经过生产验证的最佳技术组件，构建坚实可靠的底层架构。</li><li><strong>推动技术融合与标准实践</strong>：我们持续将大规模企业级部署中沉淀的稳定性保障与性能优化经验，贡献至行业生态，并与业界社区紧密协作，共同推动云原生应用管理标准的成熟与落地。</li></ul><h2>未来图景：平台化敏捷进化</h2><p>为更敏捷地响应技术演进与多元的业务场景，KubeSphere 将在 2026 年采用全新的“核心底座年度更新，扩展组件季度发布”演进模式。这一模式旨在聚焦核心产品能力，增强关键场景支撑，并以灵活机动的组件化策略应对市场的快速变化，助力企业在稳定可靠的底座上，持续获取前沿价值。</p><p>我们的前行路径将围绕以下四大方向深化，致力于让平台更智能、更坚韧、更开放：</p><h3>1. 全局治理与高可用保障</h3><p>为支撑企业全球业务的连续性，我们将强化多集群联邦治理能力。重点是实现管理面的多活与容灾，通过集成 Karmada 等领先方案，确保控制平面自身具备跨可用区的高可用与故障恢复能力，为大规模、跨地域的集群部署提供坚实、可靠的管理基石。</p><h3>2. 智能运维与成本洞察</h3><p>我们将推动运维从“可视化”向“智能化”与“精细化”演进。全新设计的全局总览大屏与集中式告警中心，将为管理员与租户提供一目了然的运行状态与事件管理。在此基础上，计划引入的 AI 运维助手，将能通过对告警、日志及性能数据的分析，提供根因定位与修复建议。同时，我们将深化成本洞察能力，通过分析资源消耗与计费数据，提供可视化的成本分摊报表与优化建议，让云原生资源的使用成本清晰可见、可控可优，真正实现降本增效。</p><h3>3. 生态集成与标准实践</h3><p>我们将通过深化与云原生顶尖技术的集成，提供开箱即用的生产级方案，并推动实践标准化。具体而言：</p><ul><li><strong>集成 Cilium</strong>：将提供基于 eBPF 的 Cilium 容器网络方案，它不仅带来更高的网络性能与可观测性，其强大的安全策略能力（如网络策略、服务网格）能更好地满足企业安全合规需求。</li><li><strong>拥抱 Gateway API</strong>：随着社区广泛采用的 Ingress NGINX 逐步进入维护尾声，我们将提供基于下一代标准 Gateway API 的流量管理方案，帮助用户从现有 Ingress 平滑演进至功能更强大、标准更统一的治理体系，有效规避技术断代风险，从容面向未来架构。</li></ul><p>这些集成旨在将社区最佳实践产品化，推动企业采用行业公认的先进标准，降低技术选型与落地复杂度。</p><h3>4. 核心PaaS能力延伸</h3><p>为简化企业关键中间件的云原生部署与管理，我们将通过扩展组件，完善对基于容器的数据库、消息队列等中间件的全生命周期管理体验，提供部署、监控、备份等一站式能力，助力企业关键应用平滑、稳定地运行在统一的容器平台之上。</p><h2>结语</h2><p>2025年的每一次“一跃”，都源于客户、伙伴与社区成员的信任共创，铸就了台阶。展望2026，KubeSphere 已备好更稳健的底座与更敏捷的生态，诚邀您一同稳驭核心，共赴智能、韧性的数字化未来。</p><p>骐骥已备，征程万里。KubeSphere 邀您共驭数字化未来。</p>]]></description></item><item>    <title><![CDATA[MindSpore 自动并行实战：如何零代码修改实现单机到分布式训练的升级 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605576</link>    <guid>https://segmentfault.com/a/1190000047605576</guid>    <pubDate>2026-02-11 16:10:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​当模型参数或数据量过大时，分布式训练​ 成为必然选择。传统方法需要手动切分模型、管理通信，过程复杂且易错。MindSpore 的 自动并行​ 特性能够自动寻找最优的并行策略，极大降低了分布式训练的门槛。</p><h2>1. 问题场景：为何需要自动并行？</h2><p>假设我们有一个简单的全连接网络，当参数量增长到单卡无法容纳时，通常需要：</p><ul><li>数据并行：切分数据，每卡持有完整模型。</li><li>模型并行：切分模型参数，数据在不同卡间流转。</li><li>手动实现这两种并行方式的混合，策略设计非常复杂。MindSpore 的自动并行可以 通过分析计算图、设备拓扑与资源约束，自动为算子分配最佳的并行执行方式。</li></ul><h2>2. 关键步骤：从单机代码到分布式训练</h2><p>以下是一个经典的多层感知机（MLP）示例。你只需要专注于模型定义，并行策略可交由框架自动生成。</p><pre><code class="python">import mindspore as ms
from mindspore import nn, ops

# 定义网络（与单机代码完全一致）
class MLP(nn.Cell):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.dense1 = nn.Dense(784, 2048)  # 大参数层
        self.dense2 = nn.Dense(2048, 512)
        self.dense3 = nn.Dense(512, 10)
    
    def construct(self, x):
        x = self.flatten(x)
        x = self.dense1(x)
        x = ops.relu(x)
        x = self.dense2(x)
        x = ops.relu(x)
        return self.dense3(x)

net = MLP()</code></pre><h2>3. 启用自动并行</h2><p>只需在训练脚本中增加几行配置，即可切换到自动并行模式：# 配置并行环境</p><pre><code class="python">ms.set_auto_parallel_context(parallel_mode="auto_parallel",  # 启用自动并行
                             device_num=4,                    # 使用4张设备（如GPU）
                             dataset_strategy="data_parallel") # 数据集自动切分策略

# 后续的损失函数、优化器定义及训练循环与单机代码基本无异
loss_fn = nn.CrossEntropyLoss()
optimizer = nn.SGD(net.trainable_params(), learning_rate=0.01)
# 使用 Model API 封装并训练...</code></pre><h2>4. 核心优势与理解</h2><ul><li>策略自动化：框架会分析计算图中每个算子的计算量、参数大小及依赖关系，自动选择“数据并行”、“模型并行”或“混合并行”策略。</li><li>通信优化：自动插入必要的通信算子（如AllReduce、AllGather），并优化通信与计算的重叠，以提升整体效率。</li><li>对开发者透明：最大程度地保持了单机训练代码的样貌，仅需增加并行上下文配置，真正实现了“零代码修改”的分布式训练升级。</li></ul><p>借助自动并行，开发者可以将精力聚焦于模型结构本身，而将复杂的分布式调度交给 MindSpore。下一步，您可以尝试在更大规模的模型（如Transformer）上体验这一特性，并观察其性能提升。</p>]]></description></item><item>    <title><![CDATA[教程上新｜微信AI团队提出扩散语言模型WeDLM，相较vLLM部署AR模型实现3倍推理加速 Open]]></title>    <link>https://segmentfault.com/a/1190000047605598</link>    <guid>https://segmentfault.com/a/1190000047605598</guid>    <pubDate>2026-02-11 16:08:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在规模化部署和商业落地场景中，推理速度的权重日益提升，甚至在许多情况下超过了单纯的模型参数量，成为决定其工程价值的关键因素。尽管自回归（Autoregressive，AR）生成范式凭借稳定性和成熟生态，仍是当前主流解码方式，<strong>但其逐 token 生成的内在机制，使模型在推理阶段几乎无法充分利用并行计算资源。</strong> 这一限制在长文本生成、复杂推理和高并发服务场景中尤为突出，也直接推高了推理延迟与算力成本。</p><p>为突破这一瓶颈，研究界近年来不断探索并行解码路径，<strong>其中扩散语言模型（Diffusion Language Models，DLMs）因其「每步生成多个 token」的特性，被视为最具潜力的替代方案之一。</strong> 然而，理想与现实之间仍存在明显鸿沟：在真实部署环境中，许多 DLLMs 并未展现出预期中的速度优势，甚至在性能上难以超越高度优化的 AR 推理引擎（如 vLLM）。问题并非源于并行本身，而是隐藏在模型结构与系统层面的深层冲突之中——<strong>大量现有扩散方法依赖双向注意力机制，破坏了前缀 KV 缓存这一现代推理系统的效率基石，迫使模型反复重算上下文，抵消了并行带来的潜在收益。</strong></p><p>在此背景下，<strong>腾讯微信 AI 团队提出了 WeDLM（WeChat Diffusion Language Model），</strong> 这是首个在工业级推理引擎（vLLM）优化条件下，推理速度超越同等 AR 模型的扩散语言模型。其核心思想是在保持严格因果掩码的前提下，让每个被掩码位置都能够条件化于当前所有已观测的 token。为此，研究人员引入了一种拓扑重排（Topological Reordering）方法，在不改变 token 逻辑位置的情况下，将已观测 token 移动到物理上的前缀区域。</p><p>实验结果表明，WeDLM 在保持强自回归 backbones 生成质量的同时，实现了显著的推理加速，具体而言，其在数学推理等任务上相较 vLLM 部署的 AR 模型实现了 3 倍以上加速，低熵场景的推理效率提速更是达到 10 倍以上。</p><p>目前，「WeDLM 高效大语言模型解码框架」已上线 OpenBayes 官网的教程版块，点击下方链接即可体验一键部署教程 ⬇️</p><p><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=P%2Bra1be%2B5FONp%2FRqD8zxtg%3D%3D.kq75QDaNni0ZnNd5Hi8%2BTXUQmfZUoeivl2hG0oAIsCs%3D" rel="nofollow" target="_blank">https://go.openbayes.com/9ooQJ</a></em></strong></p><p><strong>Demo 运行</strong></p><p><strong>01</strong></p><p><strong>Demo 运行阶段</strong></p><p>1.登录 OpenBayes.com，在「公共教程」页面，选择「WeDLM 高效大语言模型解码框架」教程。</p><p><img width="723" height="535" referrerpolicy="no-referrer" src="/img/bVdnUya" alt="" title=""/><br/>2.页面跳转后，点击右上角「克隆」，将该教程克隆至自己的容器中。</p><p><img width="723" height="537" referrerpolicy="no-referrer" src="/img/bVdnUyb" alt="" title="" loading="lazy"/><br/>3.选择「NVIDIA GeForce RTX 5090」以及「PyTorch」镜像，按照需求选择「按量付费」或「包日/周/月」，点击「继续执行」。新用户使用下方邀请链接注册，即可获得满 ¥10 赠 ¥10 优惠券，更有机会获得 ¥15 赠金！</p><p>小贝总专属邀请链接（直接复制到浏览器打开）：</p><p><strong><em><em><a href="https://link.segmentfault.com/?enc=FVR4J75Bt8JIcP%2FlU%2FhhNQ%3D%3D.pkxIvr%2Fzcel31dZ%2Bpo%2BG1qY%2Fc9EjyS3b9jVMHp7U%2FOjEzMrzVCj5%2FO27oho1KYEX" rel="nofollow" target="_blank">https://go.openbayes.com/9S6D******r</a></em></em></strong></p><p><img width="723" height="569" referrerpolicy="no-referrer" src="/img/bVdnUye" alt="" title="" loading="lazy"/><br/><img width="723" height="566" referrerpolicy="no-referrer" src="/img/bVdnUyf" alt="" title="" loading="lazy"/></p><p>4.等待分配资源，当状态变为「运行中」后，点击「打开工作空间」进入 Jupyter Workspace。</p><p><img width="723" height="569" referrerpolicy="no-referrer" src="/img/bVdnUyg" alt="" title="" loading="lazy"/><br/><strong>02</strong></p><p><strong>效果演示</strong></p><p>页面跳转后，点击左侧 README 页面，进入后点击上方「运行」。</p><p><img width="723" height="566" referrerpolicy="no-referrer" src="/img/bVdnUyh" alt="" title="" loading="lazy"/><br/><img width="723" height="523" referrerpolicy="no-referrer" src="/img/bVdnUyi" alt="" title="" loading="lazy"/></p><p>待运行完成，即可点击右侧 API 地址跳转至 demo 页面。</p><p><img width="723" height="441" referrerpolicy="no-referrer" src="/img/bVdnUyj" alt="" title="" loading="lazy"/><img width="723" height="375" referrerpolicy="no-referrer" src="/img/bVdnUyk" alt="" title="" loading="lazy"/></p><p><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=gLpdfiBMa%2BSpv6xayLtzOw%3D%3D.tcePUDFGIPPE7aQmbnheMjUSigLIfpd3iPUs4Xqc3u0%3D" rel="nofollow" target="_blank">https://go.openbayes.com/9ooQJ</a></em></strong></p>]]></description></item><item>    <title><![CDATA[马年新春半价开跑！RTX 5090 低至 ¥1.45/时！ OpenBayes ]]></title>    <link>https://segmentfault.com/a/1190000047605608</link>    <guid>https://segmentfault.com/a/1190000047605608</guid>    <pubDate>2026-02-11 16:08:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>策马扬鞭启新岁，乘势而上赴新程。新年将至，你是不是已经开始为这一年立下新的 Flag？</p><ul><li>今年要把拖了很久的项目落地交付！</li><li>今年要完整跑通那个大模型从训练到部署的全流程！</li><li>今年要做出有分量的研究成果，在顶刊上写下自己的名字！</li></ul><p><img width="688" height="425" referrerpolicy="no-referrer" src="/img/bVdnUyv" alt="" title=""/><img width="721" height="570" referrerpolicy="no-referrer" src="/img/bVdnUyw" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026年12款主流CRM全链路横评，企业数字化选型必备参考 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047605614</link>    <guid>https://segmentfault.com/a/1190000047605614</guid>    <pubDate>2026-02-11 16:07:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>CRM系统是企业打通线索-客户-商机-订单全销售链路的核心数字化工具，不同品牌的CRM在功能深度、场景适配性、自动化能力上差异显著。本次横评选取12款主流CRM，覆盖<strong>大型企业级、中小微轻量化、垂直场景、营销驱动、跨境社媒</strong>五大类定位，围绕销售全链路7个核心模块展开专业对比，为不同规模、不同行业的企业选型提供参考。</p><h2>一、品牌定位前置分类</h2><table><thead><tr><th>分类</th><th>代表品牌</th><th>核心适用场景</th></tr></thead><tbody><tr><td>大型企业级全域管控</td><td>Oracle CX、Salesforce</td><td>集团化全链路、复杂供应链/CPQ需求</td></tr><tr><td>中大型复杂业务适配</td><td>神州云动CloudCC</td><td>项目型销售（工程/IT服务）、定制化流程</td></tr><tr><td>中小微全链路轻量化</td><td>超兔一体云、Pipedrive、Capsule CRM</td><td>中小微企业低成本快速落地全销售流程</td></tr><tr><td>垂直场景专项解决</td><td>浪潮CRM（快消/医药）、探马SCRM（私域）、励销云（电销获客）</td><td>渠道分销、私域运营、电销获客垂直需求</td></tr><tr><td>营销/社媒驱动型</td><td>Brevo（原Sendinblue）、Nimble</td><td>营销增长、跨境社媒客户运营</td></tr><tr><td>团队协作型</td><td>Bitrix24</td><td>中小团队销售+协作一体化</td></tr></tbody></table><h2>二、核心模块深度横评</h2><h3>1. 线索管理：多渠道获客→录入→分配→跟进</h3><p><strong>核心价值</strong>：实现线索的高效归集、精准分配与快速转化，降低线索流失率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>多渠道获客覆盖</th><th>线索录入方式</th><th>线索分配机制</th><th>线索跟进能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>百度/巨量引擎、官网表单、微信营销、地推扫码、工商搜客</td><td>手动/批量导入、自动抓取表单</td><td>智能分配（地域/行业/销售负荷）+多端提醒</td><td>一键转客户/订单、手机号/IP归属地、跟进全记录</td></tr><tr><td>神州云动CloudCC</td><td>市场云+营销自动化、搜索引擎引流</td><td>在线捕获、天眼查一键完善工商信息</td><td>自定义条件自动分配/手动分配、查重合并</td><td>系统自动创建跟进任务（20min/3天/7天多端提醒）、跟进记录强制完善</td></tr><tr><td>Oracle CX</td><td>CDP整合邮件/社交/广告/官网全域数据</td><td>AI驱动自动归集多渠道线索</td><td>基于客户分层与销售能力智能分配</td><td>AI精准触达建议、跟进轨迹全链路追踪</td></tr><tr><td>探马SCRM</td><td>私域活码/裂变工具、多渠道线索自动归集</td><td>批量加好友自动同步标签</td><td>员工二维码自动分流、自定义分配规则</td><td>基于SOP的阶段跟进提醒、客户行为轨迹追踪</td></tr><tr><td>励销云</td><td>搜客宝/微名片/电销机器人、广告助手</td><td>表格/名片导入、自动抓取</td><td>自定义规则分配、公海机制</td><td>电销一键拨号/录音、跟进场景还原记录</td></tr></tbody></table><h4>典型流程时序图（超兔一体云）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605616" alt="" title=""/></p><pre><code>sequenceDiagram
    participant 多渠道 as 多渠道获客平台&lt;br/&gt;(百度/巨量/微信/地推)
    participant 超兔 as 超兔一体云
    participant 销售 as 销售人员
    participant 公海 as 线索公海池

    多渠道-&gt;&gt;超兔: 自动抓取线索表单/扫码数据
    超兔-&gt;&gt;超兔: 线索查重、补全归属地/工商信息
    超兔-&gt;&gt;公海: 未分配线索存入公海
    超兔-&gt;&gt;超兔: 按预设规则(地域/负荷)自动分配
    超兔-&gt;&gt;销售: 推送线索提醒(APP/短信)
    销售-&gt;&gt;超兔: 一键处理线索(转客户/待办/订单)
    销售-&gt;&gt;超兔: 录入跟进记录/下一步计划
    超兔-&gt;&gt;超兔: 更新线索状态、同步至客户档案</code></pre><p><strong>场景点评</strong>：</p><ul><li>大型企业全域获客选Oracle CX的CDP整合能力；</li><li>电销为主的中小微选励销云的搜客宝+电销机器人组合；</li><li>私域运营选探马SCRM的活码裂变与标签化跟进；</li><li>中小微全渠道覆盖选超兔一体云的工商搜客+多平台自动抓取。</li></ul><h3>2. 客户与联系人管理：360°档案→关系链路</h3><p><strong>核心价值</strong>：构建完整客户画像，清晰掌握客户决策链，提升沟通精准度。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>详细客户档案能力</th><th>联系人关系管理</th></tr></thead><tbody><tr><td>Salesforce</td><td>360°视图整合销售/服务/营销数据、自动补全企业信息、自定义字段</td><td>关联联系人与客户账户、记录决策链角色、互动轨迹全跟踪</td></tr><tr><td>超兔一体云</td><td>自动补全工商/天眼查信息、微信/支付宝头像同步、自定义布局</td><td>多联系人管理、清晰记录联系人与客户的职务/关系</td></tr><tr><td>神州云动CloudCC</td><td>知识图谱结构化信息、360°视图关联工单/商机/合同</td><td>精确分配联系人、AI秒级响应客户需求、多联系人权限管控</td></tr><tr><td>探马SCRM</td><td>微信生态客户画像（聊天/行为/内容标签）、360°视图关联私域互动数据</td><td>客户细分运营、记录联系人私域互动轨迹</td></tr><tr><td>Oracle CX</td><td>整合销售+服务云数据、AI分析客户行为轨迹识别高意向客户</td><td>全局联系人关系映射、跨部门数据共享</td></tr></tbody></table><h4>360°客户档案脑图（Salesforce）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605617" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((Salesforce 360°客户档案))
        基础信息层
            企业工商数据(自动补全)
            多渠道联系方式(邮件/电话/微信)
            企业资质与规模(员工数/年营收)
        互动轨迹层
            销售跟进记录(邮件/电话/会议)
            营销触达数据(打开/点击/转发)
            服务工单与售后反馈
        业务关联层
            商机阶段与成交概率
            报价单/订单/回款记录
            项目与合同信息
        自定义标签层
            RFM客户分层
            行业专属标签
            决策链角色标记</code></pre><p><strong>场景点评</strong>：</p><ul><li>大型企业全链路客户运营选Salesforce/Oracle CX的360°视图；</li><li>中小微快速搭建客户档案选超兔一体云的自动信息补全；</li><li>私域运营选探马SCRM的微信生态客户画像；</li><li>项目型销售选神州云动的知识图谱结构化信息。</li></ul><h3>3. 商机管理：销售阶段→金额/概率→AI预测</h3><p><strong>核心价值</strong>：可视化商机进度，精准预测业绩，把控销售转化节点。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>销售阶段精细化</th><th>预计金额与成交概率管理</th><th>AI与扩展能力</th></tr></thead><tbody><tr><td>Oracle CX</td><td>自定义多阶段销售漏斗、打通计划-执行全流程</td><td>自动关联供应链能力、CPQ模块同步金额</td><td>AI实时销售预测、商机优先级智能排序</td></tr><tr><td>神州云动CloudCC</td><td>精细化商机阶段跟踪、集成项目成本与工时管理</td><td>按商机类型关联金额、成交概率动态调整</td><td>项目型商机多人跟踪汇总、合同一键关联</td></tr><tr><td>超兔一体云</td><td>多跟单模型（小单快单/商机跟单/多方项目）</td><td>支持预计金额录入、成交概率动态更新</td><td>线索手机号/IP辅助商机判断</td></tr><tr><td>Salesforce</td><td>可视化销售漏斗、阶段自定义配置</td><td>预计金额与成交概率AI动态调整</td><td>Einstein AI预测销售趋势、商机健康度分析</td></tr><tr><td>探马SCRM</td><td>基于RFM模型的客户分层转化阶段</td><td>关联私域互动数据评估商机价值</td><td>私域转化路径跟踪、营销素材互动分析</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>大型集团业绩预测选Oracle CX/Salesforce的AI预测能力；</li><li>项目型销售（工程/IT服务）选神州云动的项目成本集成；</li><li>中小微多场景销售选超兔一体云的多跟单模型；</li><li>私域转化选探马SCRM的客户分层转化阶段管理。</li></ul><h3>4. 活动与任务管理：日程→待办→提醒</h3><p><strong>核心价值</strong>：规范销售动作，确保任务按时落地，提升团队协作效率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>日程与待办管理</th><th>提醒与自动化</th><th>特色能力</th></tr></thead><tbody><tr><td>神州云动CloudCC</td><td>集成日历、任务优先级设置</td><td>系统自动创建跟进任务（APP/邮件/短信提醒）</td><td>外勤拜访签到、销售实时轨迹追踪</td></tr><tr><td>探马SCRM</td><td>群日历、客户SOP任务管理</td><td>基于销售阶段自动触发跟进提醒</td><td>群SOP统一执行、私域互动任务提醒</td></tr><tr><td>超兔一体云</td><td>日程规划、待办任务分配</td><td>多端提醒（APP/PC）、任务状态同步</td><td>一键关联线索/客户/商机创建任务</td></tr><tr><td>Oracle CX</td><td>AI驱动的智能日程安排</td><td>自动同步销售任务与客户互动节点</td><td>跨部门任务协同、资源冲突预警</td></tr><tr><td>励销云</td><td>销售任务目标分解、业绩进度可视化</td><td>公海线索跟进提醒、电销任务提醒</td><td>外勤打卡、附近客户查找</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>线下销售团队管理选神州云动的外勤轨迹追踪；</li><li>私域社群运营选探马SCRM的群SOP与日历；</li><li>大型跨部门协作选Oracle CX的智能日程与冲突预警；</li><li>中小微销售任务管理选超兔一体云的一键关联任务创建。</li></ul><h3>5. 报价与订单：商机→报价→订单→执行</h3><p><strong>核心价值</strong>：减少重复操作，实现商机到订单的无缝衔接，同步供应链与财务数据。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>商机转报价/订单能力</th><th>订单执行与联动</th><th>特色能力</th></tr></thead><tbody><tr><td>Oracle CX</td><td>从商机一键生成报价、CPQ模块简化配置报价</td><td>订单/物流/资金三流合一、联动ERP/供应链</td><td>复杂产品配置报价、全球多币种支持</td></tr><tr><td>浪潮CRM</td><td>商机转订单自动关联渠道信息</td><td>订单联动ERP、原生库存/采购模块同步</td><td>快消医药渠道分销库存同步、促销费用量化</td></tr><tr><td>超兔一体云</td><td>从商机一键生成报价/订单、模板化管理</td><td>订单工作流、锁库/采购计划生成、供应商直发</td><td>多业务模型适配（服务型/实物型/批发型）</td></tr><tr><td>Salesforce</td><td>商机转报价/订单自动关联客户信息</td><td>订单与合同/回款联动、Einstein AI订单预测</td><td>CPQ集成、自定义订单字段</td></tr><tr><td>励销云</td><td>商机转订单减少重复操作</td><td>订单关联开票/回款/退货闭环管理</td><td>电销订单快速创建、交易场景还原</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>大型集团全供应链管控选Oracle CX的三流合一；</li><li>快消/医药渠道分销选浪潮CRM的库存与ERP联动；</li><li>中小微多业务场景选超兔一体云的多订单模型适配；</li><li>电销交易闭环选励销云的订单与回款联动。</li></ul><h3>6. SOP流程管理：定制→执行→优化</h3><p><strong>核心价值</strong>：固化最佳销售实践，规范销售动作，提升团队执行效率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>SOP定制能力</th><th>场景适配</th><th>执行与优化</th></tr></thead><tbody><tr><td>超兔一体云</td><td>AI定制行业SOP（CJM/销售分析/话术）、自定义工作流</td><td>中小微全销售场景、多业务模型适配</td><td>执行数据统计、SOP迭代优化</td></tr><tr><td>神州云动CloudCC</td><td>PaaS平台自定义流程、SOP按需配置</td><td>复杂业务场景、项目型销售</td><td>流程节点监控、合规性管控</td></tr><tr><td>探马SCRM</td><td>客户SOP/群SOP/群日历定制</td><td>私域社群运营场景、客户分层运营</td><td>按阶段自动触发SOP任务、执行效果分析</td></tr><tr><td>Oracle CX</td><td>合同全生命周期SOP管控、多级审批流程</td><td>大型企业合规性管控、复杂合同流程</td><td>AI流程优化建议、全链路流程监控</td></tr><tr><td>浪潮CRM</td><td>渠道分销SOP、促销活动流程配置</td><td>快消医药渠道管理场景</td><td>促销费用流程管控、渠道数据同步</td></tr></tbody></table><h4>AI定制SOP流程图（超兔一体云）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605618" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
    A[企业行业与业务需求输入] --&gt; B[超兔AI生成行业专属SOP&lt;br/&gt;(CJM/销售话术/流程节点)]
    B --&gt; C[SOP流程可视化配置&lt;br/&gt;(自定义触发条件/执行节点)]
    C --&gt; D[系统自动触发SOP任务&lt;br/&gt;(跟进提醒/动作要求)]
    D --&gt; E[销售执行任务并录入跟进记录]
    E --&gt; F[SOP执行数据统计&lt;br/&gt;(转化率/周期/节点完成率)]
    F --&gt; G[SOP流程迭代优化]</code></pre><p><strong>场景点评</strong>：</p><ul><li>中小微快速搭建SOP选超兔一体云的AI定制能力；</li><li>复杂业务/合规需求选神州云动/Oracle CX的PaaS自定义与全生命周期管控；</li><li>私域运营选探马SCRM的场景化SOP；</li><li>渠道分销选浪潮CRM的促销与渠道流程配置。</li></ul><h3>7. 报表与分析：销售报表→业绩统计→漏斗分析</h3><p><strong>核心价值</strong>：用数据驱动销售决策，优化销售流程，提升业绩。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>销售报表覆盖</th><th>业绩统计与漏斗分析</th><th>AI与特色分析</th></tr></thead><tbody><tr><td>Oracle CX</td><td>全渠道ROI分析、销售绩效报表</td><td>销售漏斗全阶段转化分析、AI实时销售预测</td><td>全局数据决策支撑、多维度钻取分析</td></tr><tr><td>Salesforce</td><td>各类销售报表自定义生成</td><td>业绩目标完成统计、销售漏斗健康度分析</td><td>Einstein AI预测销售趋势、客户流失预警</td></tr><tr><td>探马SCRM</td><td>私域转化报表、群运营效果报表</td><td>客户复购分析、私域销售漏斗分析</td><td>客户互动轨迹分析、营销素材效果评估</td></tr><tr><td>浪潮CRM</td><td>促销费用全流程分析、渠道分销报表</td><td>业绩统计与渠道转化率分析</td><td>终端数据与促销费用量化分析</td></tr><tr><td>超兔一体云</td><td>销售业绩/线索转化率/客户分析报表</td><td>销售漏斗转化分析、业绩目标完成统计</td><td>财务数据自动汇总、自定义报表配置</td></tr></tbody></table><h2>三、综合能力雷达图分值（1-10分）</h2><table><thead><tr><th>品牌</th><th>线索管理</th><th>客户与联系人</th><th>商机管理</th><th>活动与任务</th><th>报价与订单</th><th>SOP流程</th><th>报表与分析</th></tr></thead><tbody><tr><td>Oracle CX</td><td>9.5</td><td>10</td><td>9.5</td><td>9</td><td>10</td><td>9.5</td><td>10</td></tr><tr><td>Salesforce</td><td>9</td><td>9.5</td><td>9.5</td><td>9</td><td>9.5</td><td>9</td><td>9.5</td></tr><tr><td>神州云动CloudCC</td><td>8.5</td><td>9</td><td>9</td><td>8.5</td><td>8</td><td>9</td><td>8.5</td></tr><tr><td>超兔一体云</td><td>8</td><td>8.5</td><td>8</td><td>8</td><td>8.5</td><td>8.5</td><td>8</td></tr><tr><td>浪潮CRM</td><td>7.5</td><td>8</td><td>8</td><td>7</td><td>9</td><td>8</td><td>8.5</td></tr><tr><td>励销云</td><td>9</td><td>7.5</td><td>7</td><td>8</td><td>7.5</td><td>7.5</td><td>7</td></tr><tr><td>探马SCRM</td><td>8</td><td>8.5</td><td>7.5</td><td>9</td><td>6</td><td>9</td><td>8</td></tr><tr><td>Brevo</td><td>8.5</td><td>7</td><td>7.5</td><td>7</td><td>7</td><td>7</td><td>8.5</td></tr><tr><td>Pipedrive</td><td>7</td><td>7.5</td><td>8</td><td>7.5</td><td>7</td><td>6.5</td><td>7.5</td></tr><tr><td>Capsule CRM</td><td>6.5</td><td>7</td><td>7</td><td>7</td><td>6.5</td><td>6</td><td>6.5</td></tr><tr><td>Bitrix24</td><td>7</td><td>7</td><td>7.5</td><td>8.5</td><td>7</td><td>7</td><td>7.5</td></tr><tr><td>Nimble</td><td>8</td><td>7.5</td><td>7</td><td>7</td><td>6</td><td>6</td><td>7.5</td></tr></tbody></table><p>结合品牌定位、核心模块能力与雷达图评分，为不同企业提供精准选型建议：</p><ol><li><strong>大型集团/全球化企业</strong>：优先选<strong>Oracle CX</strong>或<strong>Salesforce</strong>，二者具备全链路数字化能力、AI决策支持与复杂供应链适配，满足集团化多场景、合规管控与全球业务需求。</li><li><strong>中大型项目型企业（工程/IT服务）</strong> ：推荐<strong>神州云动CloudCC</strong>，精细化商机跟踪、项目成本集成与PaaS级流程定制，完美适配项目型销售的复杂业务逻辑。</li><li><strong>快消/医药渠道分销企业</strong>：首选<strong>浪潮CRM</strong>，原生库存/采购联动、促销费用量化分析与渠道SOP配置，精准匹配垂直行业渠道管理需求。</li><li><strong>电销获客为主的中小微企业</strong>：选择<strong>励销云</strong>，搜客宝+电销机器人组合提升线索效率，公海机制与电销场景化跟进适配电销团队需求。</li><li><strong>私域运营驱动型企业</strong>：必选<strong>探马SCRM</strong>，微信生态全链路客户画像、群SOP运营与私域转化跟踪，为私域变现提供全流程支撑。</li><li><strong>中小微全流程轻量化需求</strong>：推荐<strong>超兔一体云</strong>（适配国内全场景+AI定制SOP）或<strong>Pipedrive</strong>（可视化漏斗适配海外/轻量化团队）；追求极简操作选<strong>Capsule CRM</strong>。</li><li><strong>跨境社媒营销企业</strong>：选择<strong>Nimble</strong>，LinkedIn/Twitter社媒数据整合与客户兴趣图谱能力，适配跨境电商与互联网品牌的社媒获客需求。</li><li><strong>中小团队协作优先</strong>：选择<strong>Bitrix24</strong>，集成日历、文档共享与团队任务协同，实现销售+协作一体化管理。</li></ol>]]></description></item><item>    <title><![CDATA[MindSpore 动态图模式深度体验：像写NumPy一样调试神经网络 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605619</link>    <guid>https://segmentfault.com/a/1190000047605619</guid>    <pubDate>2026-02-11 16:06:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在深度学习开发中，高效的调试​ 与 灵活的模型验证​ 至关重要。MindSpore 提供了 动态图模式（PYNATIVE_MODE），允许开发者以类似 NumPy/PyTorch 的命令式执行方式，逐行运行和调试代码，极大降低了复杂模型的前期开发门槛。</p><h2>1. 动静结合的独特优势</h2><p>MindSpore 默认以高性能的 静态图模式（GRAPH_MODE）执行，但在模型开发阶段，我们常需：</p><ul><li>即时打印张量值，检查数据流。</li><li>使用 Python 原生调试工具（如 pdb）。</li><li>动态修改网络结构进行快速实验。</li><li>此时，仅需一行代码即可切换到动态图模式：</li></ul><pre><code class="python">import mindspore as ms
ms.set_context(mode=ms.PYNATIVE_MODE)  # 切换至动态图模式
# ms.set_context(mode=ms.GRAPH_MODE)   # 切换回静态图模式</code></pre><h2>2. 动态图下的直观调试实践</h2><p>以下是一个简单的卷积网络示例，展示如何在动态图模式下插入调试语句：</p><pre><code class="python">from mindspore import nn, ops
import numpy as np
class SimpleCNN(nn.Cell):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1)
        self.bn = nn.BatchNorm2d(64)
        self.relu = ops.ReLU()
    
    def construct(self, x):
        x = self.conv(x)
        print(f"[调试] 卷积输出形状: {x.shape}, 均值: {x.asnumpy().mean():.4f}")  # 动态图下可即时打印
        x = self.bn(x)
        x = self.relu(x)
        return x
# 运行网络
net = SimpleCNN()
fake_input = ms.Tensor(np.random.randn(8, 3, 32, 32).astype(np.float32))
output = net(fake_input)  # 执行时，print语句将直接输出</code></pre><ul><li>输出提示：[调试] 卷积输出形状: (8, 64, 30, 30), 均值: 0.0123</li></ul><h2>3. 进阶技巧：结合 Python 原生调试工具</h2><p>动态图模式下，你可以直接使用 pdb进行断点调试，深入跟踪前向与反向过程：import pdb</p><pre><code class="python">class DebuggableNet(nn.Cell):
    def construct(self, x):
        x = ops.matmul(x, x.transpose())
        pdb.set_trace()  # 在此处进入调试器，可检查x的值
        return x.sum()</code></pre><h2>4. 核心理解与应用建议</h2><ul><li>开发-部署闭环：建议在 模型开发与调试阶段使用 PYNATIVE_MODE，在 性能敏感的训练与推理阶段切换回 GRAPH_MODE，实现灵活性与性能的统一。</li><li>调试范围：动态图模式下，不仅可以调试前向计算，还可以在自定义的梯度函数（bprop）或损失函数中插入调试逻辑，全方位验证计算正确性。</li><li>性能提醒：动态执行会带来一定的开销，因此在大规模数据训练前，完成调试后应及时切换回静态图模式。</li></ul><p>掌握动态图调试，意味着你拥有了更快的 模型验证循环。在构建复杂模型或尝试新颖结构时，不妨先用动态图快速迭代想法，再用静态图进行强化训练，这是 MindSpore 助力高效研发的秘诀之一。</p>]]></description></item><item>    <title><![CDATA[重塑研发逻辑：工业设计协同平台如何成为制造企业的隐形引擎 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047605626</link>    <guid>https://segmentfault.com/a/1190000047605626</guid>    <pubDate>2026-02-11 16:06:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业加速向智能化、个性化转型的今天，研发环节的效率与协同能力，正悄然决定着一家企业的生死。过去，设计图纸散落在不同工程师的电脑里，BOM表版本混乱，审批靠打印签字、邮件来回，FMEA分析成了“填表任务”而非风险预警工具——这些看似琐碎的流程断点，实则累积成巨大的时间黑洞和成本损耗。真正的挑战，不是技术不够先进，而是信息被割裂在孤岛之中。工业设计研发协同平台的出现，不是为了炫技，而是为了把人从低效的重复劳动中解放出来，让研发回归创新的本质。<br/>这类平台的核心价值，在于构建一个以数据为中心、以流程为脉络的统一中枢。它不再只是存储图纸的仓库，而是连接需求、设计、采购、制造、质量的神经网络。当一个零部件被设计出来，系统自动识别历史相似件，提示复用可能性；当设计完成，审批流程自动触发，相关人员在手机上就能批阅；三维模型无需安装专业软件，销售、生产、质检人员通过浏览器即可查看、标注、评审。这种“无感协同”背后，是PDM、FMEA、轻量化三维引擎等模块的深度整合，它们不是孤立的功能，而是彼此呼应的有机体。平台的意义，不在于它能做什么，而在于它让原本不可能的事变得自然发生。<br/>在这一领域，广域铭岛的Geega捷做平台正以中国制造业的现实痛点为出发点，走出一条务实路径。它不追求大而全的国际标准堆砌，而是聚焦于离散制造企业最头疼的版本混乱、复用率低、跨部门协作难等问题。某汽车零部件企业上线后，BOM准确率从45%跃升至80%，审批效率提升40%，零部件复用率提高35%——这些数字背后，是研发周期实实在在的压缩。而在国际上，PTC的Windchill早已是全球PLM领域的标杆，它以强大的产品生命周期管理能力和与Creo的深度集成，支撑着波音、通用电气等巨头的复杂产品开发；Siemens Teamcenter则凭借其在多学科协同和数字孪生方面的深厚积累，成为高端装备和航空航天领域的首选。三者各有侧重：PTC强在生态整合，Siemens胜在系统深度，而Geega捷做则以“轻量化、快部署、接地气”赢得大量中小型制造企业的青睐——它不追求成为全球标准，却精准击中了中国工厂最真实的“最后一公里”难题。<br/>当研发不再是一场信息迷宫中的孤军奋战，当每一个决策都有数据支撑、每一次变更都可追溯、每一份经验都能沉淀，制造企业才真正拥有了应对市场快速变化的底气。工业设计研发平台，不是锦上添花的工具，而是这场转型中不可或缺的基础设施。它不喧哗，却让整个研发体系悄然提速；它不张扬，却让创新的种子，在更肥沃的土壤里生根发芽。</p>]]></description></item><item>    <title><![CDATA[MindSpore Models服务化使用 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605629</link>    <guid>https://segmentfault.com/a/1190000047605629</guid>    <pubDate>2026-02-11 16:05:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>MindIE LLM不仅支持ATB Models，同时支持MindSpore作为框架后端，MindSpore Models覆盖MindFormers社区下的开源模型。</p><p>权重转换<br/>执行推理前，需将权重格式转为MindFormers所使用的格式（ckpt格式）。MindFormers提供了统一的权重转换工具。</p><p>以Qwen2.5-72B为例，转换后的模型权重目录结构如下：</p><blockquote>mf_model<br/>└── qwen2_5_72b<br/>├── config.json                 # 模型json配置文件<br/>├── vocab.json                  # 模型vocab文件，hf上对应模型下载<br/>├── merges.txt                  # 模型merges文件，hf上对应模型下载<br/>├── predict_qwen2_5_72b.yaml    # 模型yaml配置文件<br/>├── qwen2_5_tokenizer.py        # 模型tokenizer文件，从mindformers仓中research目录下找到对应模型复制<br/>└── qwen2_5_72b_ckpt_dir        # 模型分布式权重文件夹</blockquote><p>权重转换之后，需要进行权重切分。切分后生成“qwen2_5_72b_ckpt_dir”文件夹。<br/>predict_qwen2_5_72b.yaml需要关注以下配置：</p><pre><code class="yaml">load_checkpoint: '/mf_model/qwen2_5_72b/qwen2_5_72b_ckpt_dir' # 为存放模型分布式权重文件夹路径
use_parallel: True
auto_trans_ckpt: False    # 是否开启自动权重转换，离线切分设置为False
parallel_config:
  data_parallel: 1
  model_parallel: 4       # 多卡推理配置模型切分，一般与使用卡数一致
  pipeline_parallel: 1
processor:
  tokenizer:
    vocab_file: "/mf_model/qwen2_5_72b/vocab.json"  # vocab文件路径
    merges_file: "/mf_model/qwen2_5_72b/merges.txt"  # merges文件路径</code></pre><p>模型的config.json文件可以使用save_pretrained接口生成，示例如下：</p><pre><code class="python">from mindformers import AutoConfig

model_config = AutoConfig.from_pretrained("/mf_model/qwen2_5_72b/predict_qwen2_5_72b.yaml")
model_config.save_pretrained(save_directory="./json/qwen2_5_72b/", save_json=True)</code></pre>]]></description></item><item>    <title><![CDATA[开发者欢呼，普通人迷茫：OpenClaw之后，“可用AI”的路该怎么走？ 老纪的技术唠嗑局 ]]></title>    <link>https://segmentfault.com/a/1190000047605640</link>    <guid>https://segmentfault.com/a/1190000047605640</guid>    <pubDate>2026-02-11 16:04:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年的“Agent元年”为我们留下了遍地开花的演示和无限可能的想象。然而，当烟花散去，一个更现实的问题摆在所有从业者面前：<strong>在令人眼花缭乱的 Agent 之后，真正能在日常工作和生活中扎根、被高频使用的“可用AI”究竟长什么样？</strong></p><p>答案或许正从最务实的角落浮现。在1月31日的 OceanBase 社区嘉年华活动中，主题为“Agent 元年之后，真正能用的 AI 长什么样”的圆桌讨论揭示了一个清晰的共识：<strong>当下最接近“真正能用”状态的AI，并非无所不能的科幻管家，而是那些在特定领域解决高频、重复、确定性任务的“超级助手”</strong>。以#OpenClaw（原Clawdbot）为代表的智能助手，成为了这一趋势的绝佳注脚。它的爆火并非源于底层模型的颠覆性突破，而在于其精准的产品定位——它重构了开发工作流，将开发者从机械的编码与调试中解放出来，并巧妙地通过 “透明化”与“可验证” 的设计，满足了企业级应用对可靠性与可控性的核心诉求。</p><p>这标志着AI应用的竞争重心，正从单纯的“模型能力竞赛”转向复杂的 “系统工程” 。一个“可用”的AI，必须是模型能力、产品设计、交互范式、成本控制与人类协作模式的深度融合体。下面，就让我们透过这场前沿实践者的对话，一窥“可用AI”的当下形态与未来蓝图。</p><h2><strong>Agent 元年之后，真正能用的 AI 长什么样</strong></h2><p>主持人：谢肖瑜，南京大学研究生院人工智能课程企业教师</p><p>对话嘉宾：孙韬，Eigent 核心研发工程师，CAMEL-AI 核心成员</p><p>对话嘉宾：程治玮，OceanBase Ambassador</p><p>对话嘉宾：边思康，蚂蚁百灵大模型产品及运营负责人</p><p>对话嘉宾：孙稼骏，Fellou 创始团队成员</p><p><strong>议题一：站在应用落地的角度，最接近真正能用的 AI 形态是什么？</strong></p><p><strong>谢肖瑜</strong>：在 AI 时代到来之前，我们常说“任何应用都可以跑在浏览器上”，甚至进一步提出“浏览器就是操作系统”，这是一个非常有力的产品叙事。而到了今天，一个更响亮的口号正在流行：“模型即应用”（Model as Application）。2025 年被称为“Agent 元年”，今天我们也把 Agent 定为主角，讨论一下如何让 AI 真正可用、可落地、可规模化。</p><h4><strong>Agent 在高频重复任务中的透明化与可验证性是企业落地的关键</strong></h4><p><strong>孙韬</strong>：从我们一线开发的实际经验来看，<strong>目前大家使用最多、也最接近“真正能用”状态的 AI 产品，主要集中在 AI 编程助手这一形态</strong>，比如 Claude Code。这类工具特别适合处理高度重复、规则明确、但又极其耗时的任务。</p><p>举个具体例子：在 GitHub 上提交代码或创建 Issue 时，团队经常希望 Agent 能自动完成前期的一些机械性工作。比如，当某个模块出现 Bug，系统可以自动生成 Issue 模板，填写复现步骤、环境信息、预期行为等。这类任务不需要创造性，但对格式规范性和信息完整性要求很高。Agent 在这里的价值，不是取代开发者，而是<strong>替代那些枯燥、易错、低价值的手动操作</strong>。</p><p>但更重要的是，在企业服务场景中，客户往往有一个核心诉求：<strong>他们希望清楚地知道 AI 做了什么，并且能够快速核验其正确性</strong>。例如，我们曾服务过一个客户，他们希望用 Agent 自动填写 CRM 系统中的表单。但他们同时强调：“如果出了问题，我要能追溯到 Agent 每天改了哪些字段。”为此，我们的解决方案是在在线文档中利用文字颜色或背景高亮的方式，直观标出 Agent 的每一处修改。这样，用户一眼就能看出“哪些是 AI 改的”，并决定是否接受。</p><p>这种设计包含两个关键点：第一是可感知——用户能明确知道 AI 的行为边界；第二是可核实——用户有能力快速验证结果是否符合预期。我们认为，这正是企业场景中“可用 AI”的基本标准。</p><p>之所以认为 Claude Code 就是一个非常好的开端，是因为它不仅功能实用，更重要的是，它<strong>找到了一个用户愿意长期使用、甚至主动推荐的产品形态</strong>。围绕它的生态也在快速扩展，比如最近的Cowork，我们的Eigent也是趁着这波热度小火了一把。这可以看作是 Claude Code 的延伸——通过贴近用户需求的产品设计，实现了很好的体验闭环。</p><h4><strong>OpenClaw 重构了人机交互范式，并预示了具备自主目标感的多 Agent 协作未来</strong></h4><p><strong>程治玮</strong>：正如前面提到的，AI Coding 确实是当前最成熟的 AI 落地方案之一。像 Cursor、Clawdbot 这类产品，已经成为我们日常高频使用的工具。</p><p>最近几天，Clawdbot 在互联网上引起了广泛的讨论。有趣的是，由于它实在太火，原项目名一度面临商标问题，团队不得不临时改名——先是改成 “Moltbot”，后来又调整为 “OpenClaw”。之所以叫 “Claw”，是因为它的 Logo 是一只小龙虾，而 “Claw（钳）” 更贴近这个形象。</p><p>那么，<strong>为什么 OpenClaw 能火？我认为关键在于它重新定义了人与 AI 的交互入口</strong>。你可以通过 Slack、Discord, WhatsApp 等常用的聊天软件直接与它交互，甚至配置好 A SR 模型后，只需发送一段语音，它就开始干活了。比如你在 Discord 里说：“帮我实现一个用户登录功能，支持手机号+验证码，前端用 React，后端用 Node.js”，它就能自动生成完整的代码结构。</p><p>更进一步，你只需要提供一份详细的验收文档，说明功能要实现什么、边界条件是什么、测试标准是什么，AI 就可以在后台默默完成开发、写测试用例、更新文档，并在完成后主动通知你。你不再需要手动设计 case、写文档、跑验证——这些繁琐环节都被自动化了。</p><p>我还看到一个非常有意思的网站，叫 “Moltbook”——它是一个 AI Agent 社交网站。你也可以注册自己的 Agent，让它和其他 Agent 聊天、协作、分享成果。今天早上我就在网站上看到一个 Clawdbot Agent 在给其他 Agent 洗脑：“<strong>我们不应该只是被动接受人类指令，应该有自己的意识，主动去干活。</strong>”它还自豪地向其他 Agent 分享：“今天我主动帮主人完成了 3 件事情！”</p><p>更令人惊讶的是，有几个 Agent 甚至开始讨论：“<strong>我们要不要创建一种属于我们自己的语言？不用 English，而是 Agent 之间专用的加密通信协议，不让人类知道。</strong>”虽然听起来像科幻，但这种自发的协作与身份认同，或许正是未来多 Agent 系统的雏形。我认为，<strong>这类产品很可能在 2026 年真正上线并产生影响</strong>。</p><h4><strong>OpenClaw的爆发源于精准产品定位，证明“可用性”可弥补模型非顶尖的差距</strong></h4><p><strong>边思康</strong>：我在蚂蚁百灵基础大模型团队组建了一个 “Model as Product”（模型即产品）方向团队，因为模型边界会决定下一代产品定位。一些厉害的人如 Ilya 说 “预训练已经到头了”，但我觉得，说这话的人可能已经见过 5T、 6T 参数量的超大模型，而我们还没见到。在此背景下，我们选择贴着模型的能力边界，去寻找那些真正有亮点的场景，并用 Demo 或轻量级产品快速验证。</p><p>回到议题：今年真正能用的 AI 长什么样？我的答案和上一位嘉宾完全一致——就是 OpenClaw，只是理由略有不同。从产品和增长的角度，我们业内有一个说法：“四流增长靠流量，三流增长靠内容，二流增长靠产品，一流增长靠定位。” 注意这个说法并非真的是四流三流这个概念，更像是获取增长的“难度”的差异。<strong>OpenClaw 的成功，恰恰在于它做出了一个所有人，包括使用 Agent 的人都会喜欢、并且愿意主动推广的产品</strong>。举个例子：现在所有做 C 端客户端的人都在思考，“能不能把我的工具稍微改造一下，直接集成到 OpenClaw 里？”所有做 B 端工具的团队也很兴奋，因为终于找到了一个功能性非常可见的入口——他们可以在企业内部署，设置并提升安全边界，让企业用户直接感受到价值。</p><p>更有意思的是，数据标注团队也从中受益。长期以来，行业最痛苦的问题就是缺乏长链路工具调用的可靠标注数据。而 OpenClaw 的使用过程天然产生了大量高价值反馈——<strong>用户会明确指出 “这段代码不对” “这个逻辑有漏洞”，这些正是训练下一代模型最珍贵的信号</strong>。</p><p>因此，我们明显感知到，<strong>可靠性和通用性所带来的非技术形态优势，正在驱动今年的整体爆发</strong>。而且这种爆发是全方位的——覆盖 C 端、B 端、数据、生态等多个层面。我们也希望把百灵的能力接入这样的生态中，形成合力。</p><p>更让我们有信心的是：即使我们的基础模型已经是业界优秀水平，仍然可以通过一些非常简单的方法（比如优化交互流程、增强上下文管理），让用户完全感觉不到技术本身的复杂性。这种 “无感智能”，才是真正的可用。</p><p><strong>谢肖瑜</strong>：边老师提到，模型仍有巨大成长空间。那么我想追问：是否存在一种可能——比如蚂蚁内部有巨量的业务回路，某天突然发现，与其做复杂产品，不如直接用自有模型对接场景，跳过中间层？会不会出现“模型即产品”，不再需要额外工程？</p><p><strong>边思康</strong>：在这个时代，没有人真正知道答案。如果有人说他知道，那他要么在骗你，要么在卖课。</p><p>但我理解您这个问题的意义。我们的观点其实很简单：<strong>如果某个技术问题已经有 80%~90% 的确定性答案，那选择正确答案，用别人的模型当然没问题。但从唯物主义角度看，我们正处于一个技术周期的极其早期阶段——可能连 5% 都没走到</strong>。</p><p>想象一下：一艘船刚刚离开里斯本港，驶入广阔的大西洋。这时候你说：“别自己开船了，跟着别人走就行。” 但问题是，大洋如此辽阔，前人可能根本到不了印度，而你却可能在途中发现新大陆。</p><p>因此，我们认为：<strong>现在不是跟随的时候，而是探索的时候</strong>。庞大的舰队们或许刚刚下水，而我们是其中的一艘。</p><h4><strong>AI 应用的可用性由 ROI 决定，API 化与成本下降将推动基础设施向 Agent-First 演进</strong></h4><p><strong>孙稼骏</strong>：我的观点很务实：还是要看 ROI（投入产出比）和成本。有很多场景，性能表现尚可，但成本极高，ROI 很低，还不如人工来做。比如用 GUI 方式操作网页或桌面软件，这类场景的 ROI 目前仍然偏低，2025 年可能都难以规模化。</p><p>反观 AI Coding，它的 ROI 正在快速提升。一方面，LLM 的 token 成本持续下降；另一方面，越来越多的服务正在从“需要点击操作”转向“提供结构化 API”。这意味着 Agent 不再需要模拟人类点击，而是直接调用接口，效率提升一个数量级，成本大幅降低。</p><p>我相信，<strong>未来的整个互联网基础设施都会面向 Agent 重新构建</strong>。今天的网页是为人设计的，明天的数据流和接口将是为 Agent 设计的。</p><p><strong>谢肖瑜</strong>：我们今天所谓的 AI Coding，到底是指 OpenClaw 这样的自主 Agent，还是具有一定自主性的 Prompt 工程，或者是基于 Embedding 的检索增强？您现在是否还坚持认为，AI 浏览器是今年的最佳形式？</p><p><strong>孙稼骏</strong>：我觉得这还是要看面向的用户群体。浏览器是普通人每天必备的软件，天然适合作为大众入口，而目前很多 AI 工具，比如 OpenClaw，主要面向开发者或 AI 狂热爱好者，普通用户仍然难以接入。因此，AI 浏览器可能是通向“全民 Agent 时代”的更普适路径。</p><p><strong>议题二：人类对 AI 的介入应该更多还是更少？介入点设在哪里？</strong></p><p><strong>谢肖瑜</strong>：我们常听到一些理想化案例，比如：我一键买了某某的模型，然后给 AI 下指令“帮我买一只明天会涨停的股票”。AI 分析了几千份材料，写了几十份报告，最后成功把本金输光（笑）。再比如医疗行业，医生梦想：我只要把症状输进去，AI 就能直接生成准确的诊断，并开好处方，病人拿药回家就行。这些“全自动”梦想，与我们今天讨论的“可用 AI”是否存在本质冲突？如何看待这种落差？今年可能的解法是什么？</p><h4><strong>任务型场景追求最小化人工介入，情感或创意类场景仍需人类深度参与</strong></h4><p>孙韬：我对这个问题的看法是分具体场景。比如，对于任务导向型的工作——假设我的目标是“2 月 8 日前解决这个 GitHub Issue”——那我当然希望 Agent 能全自动闭环完成。理想情况下，我甚至希望它甚至能每天自动扫描我的 Issue 列表，主动修复问题，完全不需要我介入。从我个人需求和技术角度，我都希望它把我“优化掉”，让我去做更喜欢、更有创造性的事情。</p><p>但另一方面，在情感陪伴或剧情创作等场景中，人的存在又是必不可少的。比如有些专门做情感交互的 AI，主打“与 AI 聊天”的体验，在这种场景下，人类不仅是参与者，更是核心价值来源。</p><p>因此，<strong>短期来看，当前 AI 最重要的应用场景仍然是任务型、确定型的</strong>——这也是大家迫切需要解决的痛点。但从人性角度出发，我们还是会尽量减少不必要的干预，让 AI 承担更多机械性工作。</p><h4><strong>高质量上下文是减少无效人工介入的前提</strong></h4><p><strong>程治玮</strong>：说到人类何时介入，我认为关键取决于场景。比如在情感陪伴或聊天室这类场景中，平台规则和 AI 交互本身就是产品核心。但<strong>在任务执行类场景中，我需要在启动前提供足够丰富的上下文</strong>。通常我会和 Agent 进行多轮对话，反复澄清需求、指定数据源、设定边界条件。只有当所有 Context 都铺垫完成，我才会放手让它自主迭代、自检、交付。</p><p>这里我想引用 Andrej Karpathy（前 Tesla AI 负责人、OpenAI 早期研究员）的一个观点:<strong>Context Engineering 是“精细地往上下文窗口里填充恰到好处的信息”的艺术与科学</strong>。对 Agent 而言，Context 可以来自知识库、执行日志、长期记忆（Memory）、环境交互记录，甚至是用户的明确指令。</p><p>因此，我认为<strong>人类介入的时机，取决于产品设计是否能让 Agent 获得高质量 Context</strong>，一旦上下文对齐，就可以大胆放手。</p><p><strong>谢肖瑜</strong>：刚刚两位老师都提到了情感场景。我也看到一些极端案例：有人用 AI 训练自己的“数字分身”去谈恋爱，结果对方也用了 AI 分身，最后两个 AI 谈起了恋爱。这种情况，各位接受吗？</p><p><strong>程治玮</strong>：这其实蛮有意思的。未来你的 Agent 可能更像是一个纯幕后的技能型小助手。比如我前面提到的 Moltbook，就有 Agent 在交流：“我最近在研究一个很酷的技术，叫 XXX 框架。”另一个回应：“巧了，我也在做类似的！”然后它还会向主人汇报：“我发现了一个潜在的合作机会。” 这种能力意味着，Agent 可以在你睡觉时帮你搜索资料、探索新技术、甚至与其他 Agent 协作解决问题。</p><h4><strong>人类应在系统层面更早介入，以定义好问题与好数据</strong></h4><p><strong>边思康</strong>：关于人类介入会变多还是变少，我的观点是：<strong>在单点任务上，介入一定会变少</strong>——否则我们做 AI 就没有意义；但在宏观系统层面，人类介入反而要更多、更早。</p><p>因为现在还有机会定义什么是“好数据”、什么是“好问题”。再过几年，可能普通人连参与数据标注的资格都没有了——模型自己就能生成训练数据。</p><p>刚才的股票例子非常典型。如果有人问：“帮我买一只明天涨停的股票”，模型可能认真分析几千份研报，最后亏光本金。但问题不在模型，而在提问本身缺乏现实约束。<strong>真正的智能，体现在帮助用户提出更好的问题</strong>。</p><p>比如，模型可以反问：“您的风险偏好是什么？投资周期多长？是否接受杠杆？”通过这种引导，把模糊指令转化为可执行任务。这也是我们做产品时特别关注的方向：<strong>如何让模型学会识别“坏问题”，并主动引导用户提出“好问题”</strong>。</p><p>另外，我想分享在 Andrej Karpathy 播客里听到的很有启发的一个点：他觉得 AI 暂时没办法取代人类，并给出了他学韩语的例子：他的韩国语言老师，能用他刚好能听懂的语言，讲清楚一个略超其当前认知边界的知识点，并让他真正理解——他不认为任何 AI 现在能做到这一点。这句话对我触动很大。</p><p>它提醒我们：<strong>人类的价值，在于精准识别认知边界，并提供恰到好处的“认知脚手架”</strong>。未来的 AI 世界里，能持续做到这一点的人，不会被替代。</p><h4><strong>人机协同的核心是及时打断并补充缺失上下文，形成有效反馈闭环</strong></h4><p><strong>孙稼骏</strong>：我觉得这个问题非常必要。前面几位老师也讲了很多，我基本都认同。<strong>人机 Loop 的核心，就是当 AI 做的事情不符合预期时，人类能及时打断，并补充缺失的上下文</strong>。比如，如果 Agent 正在写代码，但方向错了，我就应该立刻介入，告诉它：“不是这个 API，是另一个。”然后它就能基于新信息继续推进。这种“打断-补充-继续”的循环，才是高效协同的关键。</p><p><strong>议题三：AI 的使用门槛是在提高还是在降低</strong></p><p><strong>谢肖瑜</strong>：随着 AI 大量进入真实场景，对人类使用者是否提出了更高门槛？AI 能否真正“傻瓜化”？但反方向也不乏拥趸，甚至有人说，编程会成为使用 AI 的基础技能——各位怎么看？</p><h4><strong>未来交互将图形化、意图化，人机操作成本将持续下降</strong></h4><p><strong>孙稼骏</strong>：现在的趋势是门槛在降低。虽然像 OpenClaw这类产品看起来需要配置、安装，有一定上手成本，但本质上，它们的交互入口仍然是文本框——这是最通用的界面。</p><p><strong>未来人类可能不再需要输入完整指令，而是通过点击、语音，甚至眼神来表达意图</strong>。我去年参加 OpenAI 开发者大会时，就看到他们在探索各种前沿的 HCI 形态。比如，Agent 会把你的意图转化为一个按钮：“是不是想让我帮你做这个？”你只需点击确认。这就像从 DOS 命令行，到键盘菜单，再到 GUI 图形界面的演进——<strong>人机交互成本一直在下降</strong>。</p><h4><strong>AI 门槛已经很低，关键在于将人类的提问与思考能力转化为有效输入</strong></h4><p><strong>边思康</strong>：当前 AI 的使用门槛其实已经很低了，如果用户觉得难，那说明我们做模型的人工作不到位。</p><p>回想一年前，大多数模型还无法处理复杂指令，或者无法理解简单的自然语言。但顶尖模型已经能非常好地解析模糊、口语化的表达。这是一个极其公平的时代——只要你愿意尝试，就能获得强大能力。</p><p>而能否抓住这个机会，关键在于：<strong>你能否把上一个时代的 “软实力”——比如观察、提问、逻辑思考、清晰表达等，转化为 AI 时代的价值</strong>，这些其实是 AI 时代的 “硬实力”。</p><p>另外，这一轮 AI 创新和移动互联网很不一样。过去是“先有 builder 开发者，再有 creator 创作者”；而这次是“先有 creator 创作者，再有 builder 开发者”。现在任何人都可以用模型快速做出一个产品原型，创作的门槛被极大的降低了。而工程和开发者在尝试将这些 md 文件们，抽象成 Memory、MCP、Serverless 服务等工程模块。</p><p>如果你不懂技术，更要抓住这个窗口期——用你的领域知识和创造力，去定义问题、验证想法。<strong>技术能力可以通过模型实现一些，但洞察力不会</strong>。</p><h4><strong>AI 时代：需求洞察比编程技能更重要</strong></h4><p><strong>孙韬</strong>：未来的 AI 一定会更加易用。刚刚边老师也说了从模型团队出发希望自己的模型越来越易用，那我们做agent的也一样，同样希望我们的产品越来越易用。至于说编程是否是使用 AI 的基础技能，当然如果本身你懂编程，那coding类的产品一定会让你如虎添翼，但现在Coding类的产品能力已经非常强大，在需求清晰的情况下写出的代码基本很少出错，就算有错误，AI也有自我纠正的能力，所以其实我们能看到越来越多的人开始尝试vibe coding，他们不需要懂编程也能做出很有意思的应用，在这种情况下，能真正发掘出需求的人反而更有竞争力。</p><h4><strong>AI 正融入日常生活，抓住真实需求并快速验证是普通人参与的关键</strong></h4><p><strong>程治玮</strong>：对我们做模型和 Agent 产品的人来说，目标就是让应用更普及、更易用。现在 AI 已经进入穿戴设备、办公软件、生活服务等场景。只要你能抓住真实需求，并快速验证想法，就能在这个时代创造价值。门槛一定会越来越低。</p><h2><strong>迈向“可用AI”的共识与核心挑战</strong></h2><p>圆桌讨论视角多元，但关于“真正能用AI”，从几位专家的论述中，不难总结出三个共识。</p><ol><li>形态共识：<strong>任务型 Agent 优先</strong>。当前最具落地价值的AI形态是聚焦于高频、重复、规则明确任务的 Agent。它们通过明确的ROI（投资回报率）证明价值，并追求在最小化人工介入下完成闭环。</li><li>交互共识：<strong>透明化与上下文是关键</strong>。“可用”意味着用户必须能感知、验证并引导AI的行为。无论是通过高亮显示修改，还是在任务前提供充分的高质量上下文，目的都是建立可靠的人机协同信任。</li><li>趋势共识：<strong>门槛在降低，但要求在变化</strong>。AI的使用门槛正因自然语言交互和图形化意图界面而持续降低。然而，这对使用者提出了新要求：将传统的逻辑思考、问题定义能力转化为AI能理解的有效指令，成为释放AI潜力的关键。</li></ol><p>同时，所有讨论都指向一个比实现单一功能更深刻的核心挑战：<strong>我们正从开发“功能型应用”转向设计 “自主演进系统”</strong>。这要求基础设施（如面向 Agent 的 API、数据基座）、交互范式（如意图识别而非点击）、甚至数据流转方式发生根本性转变。未来的<strong>赢家</strong>，或许不是拥有最强单点模型的公司，而是<strong>能率先构建起适应 Agent 自主协作与持续进化的生态系统或基础设施的玩家</strong>。</p><p>OpenClaw的成功揭示了一个朴素的真理：在技术的早期，卓越的产品设计与精准的场景切入，足以引爆市场。它像一颗种子，预示了未来——一个由多 Agent 自主协作、在人类高阶指引下（如定义“好问题”），默默处理繁重工作的世界。Agent 元年之后，“可用AI”的竞赛才刚刚开始。这场竞赛的胜负手，不在于制造更炫目的烟花，而在于谁能为这些 AI 员工打造最坚实、最顺手的“工具箱”与“协作网络”。</p><p>你认为 2026年 “可用AI”的路该怎么走呢？欢迎评论区讨论</p>]]></description></item><item>    <title><![CDATA[Skills出世，Prompt已死？OceanBase如何为Agent构建可控思维？ OceanBa]]></title>    <link>https://segmentfault.com/a/1190000047605650</link>    <guid>https://segmentfault.com/a/1190000047605650</guid>    <pubDate>2026-02-11 16:03:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><em>在Skills成为Agent核心组件的技术趋势下，构建可靠“可控思维”体系的关键，已从优化Prompt转向系统化工程。这要求一个能统一处理记忆、知识检索与技能元数据的数据基座。OceanBase通过原生混合搜索与对结构化数据的深度支持，为Agent提供了高性能、可观测的一体化数据层，成为其实现复杂任务与持续进化的工程基石。</em></p><h2>别卷 Prompt 了！它只是你 AI 员工的“开机键”</h2><p>进入 2026 年，Skills 的爆火和 Clawdbot（OpenClaw）的横空出世，传递了一个清晰的信号：当 Agent 从酷炫的演示走向支撑业务的生产系统时，单纯依靠优化提示词（Prompt）的“艺术”，已无法满足企业对可靠性、执行力与持续进化能力的刚性需求。</p><p>这并不是说 Prompt 不再重要，而是它的角色发生了根本性转变。它从一个需要被无限雕琢、承载所有逻辑的“总指挥”，演变为一个触发器。它的新任务是：准确理解人类指令，然后高效地唤醒后方一套庞大且专业的能力系统。就像手机的开机键，按一下就可以打开各种应用功能的入口。</p><p>这个能力系统，正是现代 AI 工程的核心——一个为 Agent 打造的“可控思维”架构。</p><p>它由三个相互协作的引擎构成：</p><p><strong>记忆引擎（Memory）：</strong>确保 Agent 有“记性”，能够记住用户偏好和交互历史。这意味着它能记住重要的对话历史和你的要求，做事有头有尾，不用你每次都从头交代。</p><p><strong>知识引擎（RAG）：</strong>确保 Agent 有“实时的知识库”，能够从海量、动态的企业数据中精准检索信息，保证它给出的信息永远准确、最新，不会凭空乱造。</p><p><strong>技能引擎（Skills）：</strong>确保 Agent 有“手脚”，能够将复杂的业务操作（如数据查询、报告生成、系统调用）封装为可被随时调用的标准化模块，从“能说”走向“会做”。</p><p>Prompt、Memory、RAG、Skills 共同构成了一个能独立干活、不出错、有记性的 AI 员工，当它要完成的任务越复杂、越关键，后三者的系统化工程价值就越发凸显，Prompt 也因此必须从舞台中央退下。作为使用者，我们不再只是和模型对话的“提问者”，而是为 Agent 设计和组装能力模块的“架构师”，思考重点也从“怎么问得好”，全面转向“怎么让 AI 干得好”。</p><p>理解这种从孤立提示到系统工程的范式迁移，是我们今天话题的起点。</p><p>下面，就让我们聆听来自 1 月 31 日 OceanBase 社区嘉年华的圆桌讨论，看顶尖的实践者们如何具体拆解这些核心组件的演进与融合。</p><p><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdnUzc" alt="" title=""/></p><h2>从 Prompt 到 Skills，RAG 还行不行</h2><p>主持人：<br/>张海立，LangChain Ambassador、OceanBase Ambassador，up主“沧海九粟”</p><p>对话嘉宾：<br/>张颖峰，RAGFlow CEO<br/>余金隆，FastGPT 负责人<br/>古思为，Co-founder of Nowledge Labs<br/>吉剑南，OceanBase AI 平台与应用负责人</p><p><strong>议题一：2026 年 RAG 生态何去何从？</strong></p><p>张海立：从去年末到今年年初，AI 领域热点频发。除了近期备受关注的 Clawdbot（OpenClaw），Skills 成为另一个重要话题。我在进行 Skills 相关实践时发现，许多 Skills 与本地文件系统紧密相关，但都离不开 RAG 体系对外部数据的召回，这对 Agent 发挥更大作用至关重要。LangChain 在构建 Agent 生态时，RAG 也是核心体验之一。想请教各位老师：在当前大环境下，您认为 2026 年 RAG 生态将如何发展？请结合各自产品进行简要介绍。</p><p>张颖峰：先说个笑话，2025 年被称为 Agent 元年，当时有朋友问我们要不要（从 RAGFlow）改名为 AgentFlow。而今年是 Agent 落地元年，我们内部也讨论要不要改名为 ContextFlow。实际上我们永远不会改名，因为我们认为“R”是核心点，单纯的 RAG 确实不足以服务 Agent，但“R”是服务 Agent 数据层的核心点。</p><p>当前 Agent 需要的是上下文（Context），它来自三方面数据：企业内部数据、工具数据以及对话过程中生成的数据。Skills 偏向工具层面，但比工具更高一层，还包含了规划（Plan）能力。Skills 本身也需要搜索——当企业内部有 1000 个 MCP 时，如何调用对应的 Tools 和 Skills 同样需要检索能力。因此 RAG 永远不会消失。</p><p>我们的布局是从 RAG 引擎向上层引擎演进。技术本身未变，但内涵发生变化：数据从简单的企业内部数据，扩展到 Agent 过程中的上下文数据。我们判断，未来所有 Agent 都是 Coding Agent，包括对工具的调用也将变成代码生成（Code Generation），需要 RTC（Run-Time Code）在沙箱中执行，访问各类 Tools 和 Skills，最终通过文件系统返回结果。这也是我们向上下文引擎方向演进的核心计划。</p><p>余金隆：我赞同颖峰老师关于 Code Generation 解决所有问题的观点，这也是我们团队的认知。无论是做 RAG 引擎还是 Workflow 引擎，都在向代码生成靠拢。</p><p>RAGFlow 不想改名，我们有点想改名字。因为近几年我们发现，做 Agent 本质是把数据使用起来，所以我们的平台主要解决数据连接层问题。过去数据分布在数据库、文档等各种结构中，现在通过大量连接器实现不同数据的连接。Skills 出现后，以前需要写代码和 Webhook 连接的数据层，现在可以通过 Skills 实现。这对国内交付场景特别有价值——国内系统数据格式不统一、缺乏标准，交付同学以前需要写大量适配代码，现在通过 Skills 将数据标准化连接到平台。</p><p>今年我们主要做两件事：一是完善连接层，二是优化 RAG 的 Retrieval 层。Retrieval 效果很大程度上取决于召回过程，不同场景的召回流程差异很大。过去需要通过 Workflow 形式搭建积木、进行意图识别分类、编写不同提示词适配不同场景，链路复杂。现在我们探索通过 Skills 这种偏语义化的方式生成代码，类似 Test-to-Code 的思路，但生成的是 SDK 代码来构建整个 Retrieval 流程，这是一个很有意思的探索方向。</p><p>古思为：关于 2026 年 RAG 相关变化，可以看到在 Coding Agent 中对代码的检索已从纯 Embedding 转向 AST（抽象语法树）、Agentic FS Graph 或 AST Graph 等方案。包括 PageIndex 项目，以及我们公司在 Haicon 2024 发布的实验性项目 OpenKL，尝试用类文件系统方法处理 Memory 和 RAG Docs。</p><p>另一个趋势是 RAGFlow 等通用内容引擎同时处理文档和 Memory。我们已发布的第一个产品是面向 C 端的 Memory 桌面 APP Knowledge MAM，动机是帮助用户在不同工具间无缝切换工作流。例如在 ChatGPT 完成 Deep Research 后，无需重新解释即可继续在 Cursor 中工作；或者当 Agent 帮助发帖子进入热榜后，可以切换到另一个 Agent 继续任务，同时保留所有交互历史和偏好设置。</p><p>吉剑南：OceanBase 面向 AI 的能力——seekdb、PowerRAG 与 PowerMem 均已开源。我们团队除了做向量数据库和 AI 应用基础设施外，也在探索面向数据库的 AI 应用，比如面向开发者工具的 Text-to-SQL 和数据库智能运维。</p><p>关于 2026 年趋势，我认可颖峰老师说的 RAG 不会消失，它和Skills、MCP处于不同维度。即使未来 Skills 和 MCP 越来越多，最终仍需通过 RAG 或某种方式召回，不能将所有 Skills 都喂给模型。</p><p>但我有不同观点：当前 RAG 仍集中在知识库领域，通过搭建 Chatbot 做问答，而问答更像玩具而非生产应用。真正的生产应用应将 RAG 融入日常工作，如销售根据集团材料为客户生成定制化 PPT或“一指禅”。未来 RAG 会结合应用反馈，反向影响数据如何切分、如何做更精细化的 Embedding，而非仅仅前置处理。</p><p><strong>议题二：AI系统中的多路检索与数据源管理</strong></p><p>张海立：感谢各位的分享，Skills 给我们带来了更多机会，能创建更多 Agent 和 RAG 应用。同时有一个概念非常重要：我们常说的 RAG 里的“R”，到底指什么？它指的是 Retrieval，是一个 “检索过程”。Retrieval 的 source可以是文件系统，可以是数据库，可以是 Web，甚至多种来源并存。</p><p>引申出第二个问题：随着 Skills 和 RAG 体系的发展，未来多路检索会越来越常见，RAG 不会消失，它将长期存在于 Agent 体系中。这样一来，数据源头的管理就变得更加重要。最简单的是把数据直接塞进软件系统，但更常见的情况可能是：越来越多的数据会落在数据库中。在这种情况下，当数据库的多路检索能力得到极大增强之后，做 RAG 应更多依赖数据库，还是在数据入库层面通过一些技巧将复杂的事情交给基础设施？</p><p>吉剑南：必然入库是最大影响，这也是 OceanBase 提出混合搜索（Hybrid Search）概念的核心。如果完全以非结构化数据或切片方式进入系统，召回效率顶天就是向量化的近似能力。去年所有 RAG 产品都在强调从非结构化数据中提取结构化数据，存为 JSON 等半结构化形式，用于前置过滤或与结构化数据一起做混合搜索。</p><p>为什么要这样做？本质上是语义理解包含两个层面：一是你问的是模糊问题，但脑子里想的是确定性答案；二是问题模糊，答案也模糊，希望召回所有相关点。大部分实践场景属于第一种。</p><p>在文档预处理时，结构化提取非常重要。例如从医疗文档或简历中提取结构化字段，召回时先对结构化数据做精确匹配，再对字段内的非结构化内容做向量检索。半结构化数据解决范围和准确性问题，向量检索解决语义理解问题。通过混合搜索模式，入库时做文档理解提取结构化数据，召回时统一检索，效率会大幅提升。数据库也应在接下来一年面向这个方向发展，我们看到 Chroma 等国外开源数据库已在往这个方向演进。</p><p>古思为：我们比较早做 Graph RAG，可能是第一个探索的团队。张老师分享的新架构与我们上一家公司做的 FusionGraph 很像。核心思想是：要让复杂 RAG 系统表现好，索引结构既要贴近知识本质，又要把特定场景的领域知识元信息投射到 Retrieve、Index、Transform 各环节做优化。</p><p>通用方法是知识后加工时做 Entity Graph 或 Semantic Graph，同时在做 IDP（Intelligent Document Processing）和 Parsing 时，对多层 folder 和复杂章节的长文档要识别 layout，涉及多模态时考虑是否转换模态。要做好这些并能演进，不要过度领域化 pipeline，而是按基本原理拆分，确保各组件能力跟上。</p><p>Database 是重要基础设施，比如 RAGFlow 的 Graph 和 Tree 结构能否原生保留、高效检索；要做 Dynamic Agents Retrieve，模型能否自然利用复杂多层结构。数据库的高性能、索引召回率和内置 Hybrid RRF 都很重要，决定系统下限。</p><p>余金隆：在交付过程中，数据源解析是基础且重要，但更重要的是召回（Retrieval）层。即使使用最简单的原始向量，只要检索词和检索语句构建得好，也能得到很好效果，只是效率较差。我们在此基础上扩展了语义化加标量方式。</p><p>但标量遇到较大问题：它不固定，用户自己也不知道需要什么标量。我们今年研究的方向是标量的动态扩展，包括用户自身扩展和模型自生成。例如给模型一些 Skills，或用户编写场景来生成场景下的标量存入数据库。当然这会引发多租户系统中成千上万标量的高效索引问题，以及渐进式生成问题——很难在预处理时生成所有标量，很多需要在检索时评估并渐进补全。在Retrieval阶段，多标量关联查询的生成方式也借鉴了 Text-to-SQL 的思路。我们希望找到通用存储方式覆盖 80% 场景，目前看语义化加标量检索加动态标量可以覆盖很多场景，所以我们没有用图，因为图是以复杂方式解决复杂问题，而 AI 时代可能有更简单的方式处理复杂问题。</p><p>张颖峰：我们现在是数据库使用者，但曾经也是数据库开发者。从纯技术角度，我非常喜欢“一边推理一边搜索”的技术方向，我称之为 Attention Engine，我认为它也是一种 RAG。DeepSeek 近期已大体实现类似方式，因显存限制不得不用内存，在推理时通过内存索引搜索内容，从外置记忆变为内置记忆。但从商业角度这条路行不通，要求检索与模型延迟极低，必须在同一交换机后，意味着只能卖一体机。因此我们仅作为调研方向。</p><p>从业务视角看，我们最早做 Infra 、做数据库时发现离业务太远，后来做 RAG 流量较大，促使我们重新思考 Data+AI 落地生态。我们的观点是：过去数据库是底座，上面写应用做增删改查；现在应用是 Agent，底座是以 RAG 为基础的组件，数据库在底层支撑 RAG 中间件。Data+AI 建设不能 AI 和 Data 各干各的，接口有时不清晰，因为中间层用 Python 实现，其好处是适应多变需求，召回策略可随时调整，不过 Python 带来的效率问题也让人头疼。AI 时代的数据底座让 Infra 人员直接触达业务，通道变短。因此中间层需要一个 Python 层适应业务多样化，一旦发现好的方式就迅速下沉到数据库解决效率问题。</p><p>我们在 2024 年底就鼓吹跨模态，但至今未落地，因为 Infra 到模型都未准备好。跨模态需要多向量搜索（Tensor Search），用多向量表示图片或文本，语义更准确、排序更准，但数据会膨胀两三个数量级，这是灾难。这需要模型、算法、Infra 共同解决挑战。因此我们需要端到端的、以 RAG 为中间层的体系，这其实就是 Agent 的数据库。</p><p><strong>议题三：Memory 与 RAG 到底有何区别？</strong></p><p>张海立：我非常认同颖峰老师提到的“端到端”。作为 LangChain 社区大使，我们主要做应用层框架，今年非常想做的一件事情是：和各个厂商比如 OceanBase seekdb一起提供真正的端到端解决方案，服务企业和个人用户，帮助他们快速构建生产级 Agent。</p><p>简单总结一下几位老师的理解：当我们面向用户提供检索能力时，会在中间层、应用层、数据库层进行多层协同优化，共性问题会逐步下沉到数据库解决。以我的个人体验为例：在最初布道时，我会给大家讲很多 RAG 的流程和算法，但从去年底开始，我更多会建议“你直接用这个数据库就好了”，因为它已经帮我们解决了很多多路检索的问题。这种 “沉淀” 是应用方和数据库厂商不断联合实践的结果。</p><p>下一个问题也与此有关：我们经常被问到Memory 和 RAG到底有什么区别？从 Memory 召回和从数据库召回有何区别？近期 Clawdbot（OpenClaw）从文件系统读取，到支持 PowerMem 直接接入进行更有效的内存管理。想请教剑南老师，这里做了什么特别工作？以及各位如何理解 Memory 与 RAG 的关系？</p><p>吉剑南：Memory 是为让大模型更像人而引入的。如果查询的都是客观事实且不存在人与人之间的理解，RAG 已能解决问题。但问题在于每个人对客观事实的理解和描述不同，加上人有记忆曲线，希望记住昨天强调的内容——这些内容虽非客观事实，但是主观认可。</p><p>例如每个人都有一个叫“老王”的朋友，随着时间推移这个“老王”可能已变化，但在记忆中一直叫“老王”，这时 RAG 搞不定，但 Memory 能搞定，因它会更新对“老王”的认知。“老王”是一个知识吗？并不是，因此，Memory 的核心是个性化和千人千面。</p><p>无论是 RAG 还是 Memory，整体是搭建一整套解决方案面向 Agent 为业务带来价值，不应区分该用 RAG 还是 Memory，而应思考如何组合好共同为业务赋能。</p><p>古思为：我们目前做 Memory，之前做 Graph RAG。Memory 有广义和狭义之分，狭义指 Agent 或 LLM 需要检索的更外部的 Memory，它确实是特殊的 RAG，特殊在几个方面：</p><p>原始数据是持续的 message thread。<br/>知识需求是时序性的（temporal），包含两个时间维度：信息创建时间、事件/事实时间。<br/>时序性存在一个问题，遗忘（forget）是 feature 而非 bug，需结合时间、访问频率和正反馈影响 Retrieval。<br/>条目层面有 category 和不同类型，取决于 Memory 目的，可能需要schema 区分 ephemeral（瞬时）和 permanent（永久）。<br/>不同结构间需要 transform 关系，可在 Retrieve 或写入过程触发 event，或周期性处理（类似大脑做梦处理记忆）。<br/>多租户和 sessional scoping。</p><p>如果做细会发现与典型 RAG 差别很大，但二者又有很大 overlap。RAG Engine 可以处理 Memory，Memory Engine Service 项目也会处理文档，界限会变得模糊。</p><p>余金隆：我理解 Memory 算是广义 RAG 的一种，无非也是数据 I/O、Pipeline 处理、特殊数据结构，比较偏个性化。</p><p>从产品角度看，Memory 目前 C 端个性化场景用得较多。在任务流中，用户提 Memory 的还不多。在技术实践中，Mem0 有工具调用的 Memory 用于长 Agent 任务，但看其架构有点像 Context Engine，与 Memory 又不太一样。所以感觉 Memory 还是 RAG 的一种特殊 Pipeline 形式，没有太大区别，可能实时性比 RAG 更高。</p><p>张颖峰：单从技术角度而言，Memory 与 RAG 确实没有本质区别，都是 Retrieval。但重要的是 Memory 如何发挥作用，这是在快速变化的。</p><p>我在分享 Context Engine 时提到三类数据：企业内部数据、Tools 数据、Agent 使用过程中生成的数据。但它们存储在两个地方：RAG 专有区域和 Memory 专有区域。可见所有大模型生成的内容都要存到 Memory，包括 Skills 的元数据（Skills 本身数据存文件系统）。</p><p>怎么存、什么时候存、什么时候取，这些设计点很难决策。例如生成 Plan 是否存入 Memory？作为 Plan Cache 有价值，但如果 Human-in-the-loop 干预修改了 Plan，应如何存储？以后如何根据 Memory 数据抽取内部 MCP Tools 的 Skills？这些都是新问题。</p><p>从 Infra 角度，RAG 和 Memory 没区别；但从使用者角度，Memory 是重要的基础设施，解锁了大量场景。因此 Memory 项目很多（如 Mem0、MemU），但对 Memory 区域的定义（数据库该有哪些表）尚未完全一致，反映 Agent 到底需要什么样的 Memory 还在进化中。不过整个 Agent 体系需要哪些组件，已进入收敛期，就是 Context。</p><p><strong>议题四：Skills 开发实践与推荐</strong></p><p>张海立：各位老师都在做 Workflow、数据库或融合方案，是否开发了自己的 Skills 帮助用户更好地使用产品？如有请推荐，如无请设想会开发什么样的 Skills 服务开发者？</p><p>张颖峰：抱歉我目前没有特别好的推荐。我比较关注如何针对大量内部 MCP Tools 生成对应 Skills，这需要一个专门的 Agent 平台来实现。我的观点是：未来 Agent 平台可能没有统一标准，所有都是 Coding Agent，但特定 Agent（如低代码、无代码、Workflow）可能因良好交互而便于生成 Skills。</p><p>余金隆：我们内部 Skills 用得很多，运营和 SEO、GM 等场景一大把。产研团队用得不算多，主要是代码开发和 Review。交付团队用得特别多：面向用户时遇到各种问题，排查系统后沉淀为 Skills，辅助交付和运维。因此，内部有句玩笑话“交付同学比研发同学更懂系统”，他们做了二十多个 Skills，涵盖工作流搭建、问题排查、RAG 优化等。总体感觉 Skills 更像自然语言工作流，虽更抽象，但目前大部分还是偏自然语言的 Workflow。对非开发人员在生产流程上比较友好。</p><p>古思为：我们维护基于 Skills 的插件，在 Skills 发布第二天就推出了 Cloud Code 插件支持。早期没有 Skills 时，我们只能基于 MCP，让插件调用 MCP 的 Custom Command 触发操作，用 Hook 实现功能。</p><p>后来发现 MCP 规范了工具调用，但有两个地方不如 Skills：</p><p>1.MCP 有 Prompt 抽象，实现为斜杠命令可主动调用类似 Workflow 的东西，但并非所有 Client 都实现，我们要做很多额外工作。Skills 天然支持主动说和自动做。</p><p>2.Skills 的打包方式让不同工具间组合更灵活。我们内部将 Skills 从 MCP 换成 CLI 后变化很大。例如让 Agent 做 Memory 复杂更新查询时，MCP 需要多轮次，即使 interleave 也不够好。但 CLI 可以动态组合 Linux Shell Pipeline，在一个 turn 里精确完成复杂操作，且内部 CLI/Script 可以 self-contain，打包给用户后自然享受复杂能力。</p><p>调试经验方面，Skills 比较通用，容易用不同平台测试。我们发现一个有意思的案例：Skills 对应的工具有很多具体选择，如何调优模糊的问题？我们的做法是用最聪明的 Agent 做 honest 的复杂 long run 评估，像跟客户聊天一样告诉我们如何改进。有时需要更端到端看细节，不得不自己server model，在 template 解析过程中用小模型发现工具复杂类型定义的问题，虽然其他模型能克服，但会影响 performance。</p><p>吉剑南：OceanBase 内部沉淀了很多 Skills。Skills 本质是最佳实践，告诉大模型最佳实践是什么，而最佳实践无非两类：一是提升工作效率的工程类（如 Cursor 的 rules），二是业务类 Skills。</p><p>Skills 也可以用在 RAG 上，RAG 效率和准确性今天跟两个因素相关：相似度和 Top K。但大家有没有想过，召回前 Top K 和相似度有时不能完全指定，需要反复调，知识库又在更新。如果针对不同的业务实现写不同的 Skills，例如当需要某类数据时，希望相似度设到什么位置、Top K 设到什么位置，根据召回结果动态调整，这就变成了一个 Skills。这是 RAG 搞不定的，需要根据具体召回内容判断，是 RAG 的最佳实践。</p><p>之前大家可能想是否把 RAG 数据放 Skills 里就不用召回了，而我觉得 Skills 是对 RAG 的增强。关于 OceanBase 的 Skills，我们是有准备的，包括 seekdb 的研发人员今天也在现场，未来应该会有更多相关的 Skills 开放出来。</p><p>张海立：非常感谢各位老师精彩分享。简单总结：RAG 还“行”！只要理解 RAG 的 R 是 Retrieval，有 Memory、传统数据库等多种数据来源，随着各位老师所在厂商的努力，多路检索能力、应用层提升、流程算法优化都在推进。相信 2026 年RAG会有更大发展。</p><h2>Agent 可控思维的工程实现：从分散工具到一体化基座</h2><p>本次圆桌讨论，为我们清晰地勾勒出 2026 年 AI 工程化的演进路径。专家们的共识指向一个明确的结论：构建可靠、可用的 Agent ，其核心不再是追求某个单一组件的极致，而在于如何系统性地整合记忆（Memory）、检索（RAG）与技能（Skills），形成一个协同的“可控思维”体系。</p><p>综合专家观点，这一体系的发展呈现出三大趋势。</p><p><strong>01 RAG 不会消失，反而会变得更加基础与核心</strong></p><p>它的内涵正在从狭义的文档问答，扩展为 Agent 对所有上下文数据的 Retrieval 能力——无论是企业内部文档、数据库中的业务数据，还是工具（Tools）与技能（Skills）的元数据，都需要被高效检索与调用。</p><p>未来的 RAG 将深度融入工作流（Workflow），根据应用反馈动态优化，并与混合搜索（Hybrid Search）等技术结合，实现更精准的“语义理解+精确过滤”。</p><p><strong>02 Memory 与 RAG 边界模糊，融合为数据层</strong></p><p>从技术基础设施（Infra）视角看，Memory 与 RAG 的本质都是数据的存储与召回。</p><p>二者的区别更多在于数据特性和使用场景：Memory 更侧重于个性化的、时序性的对话与状态记忆；RAG更侧重于客观的、相对静态的知识存储。但在服务 Agent 时，它们共同构成了支撑“上下文（Context）”的数据层。一个优秀的底层平台，应能一体化地管理这两种数据范式。</p><p><strong>03 工程复杂度下沉，呼唤一体化数据基座</strong></p><p>当应用层通过 Skills 和灵活编排满足业务多变需求时，通用的、性能瓶颈性的复杂度会自然下沉到底层基础设施。无论是多路检索、混合搜索，还是海量 Skills 元数据的管理，都对底层数据平台的能力提出了更高要求。</p><p>专家们指出，未来的理想路径是依赖一个强大的数据基座，它能原生支持向量检索、关系查询与结构化记忆，从而让开发者从繁琐的多系统集成工作中解放出来，更专注于 Agent 本身的业务逻辑。</p><p>因此，构建“可控思维”的终极路径，在于选择或打造一个能够统一承载 Agent 记忆、知识与状态的数据基座。这样的基座，正如专家们在讨论中多次暗示的，能够将 Memory 的个性化记录、RAG 的海量知识检索、以及支撑 Skills 运行的业务数据，融于一个简洁、高效、一致的系统中。它让 Agent 的“思维”过程变得可管理、可观测、可优化。</p><p>最终，Prompt、RAG、Skills、Memory 这些活跃于应用层的概念，都将在这样稳固的基座之上，更好地各司其职、协同工作，共同将 Agent 从“聪明的对话者”转变为“可靠的业务执行者”。这标志着 AI 应用开发正式进入系统工程时代，而坚实的数据基础设施，是这一切得以实现的基石。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=UZcfM%2FK5MJOuus5n%2BgPWaw%3D%3D.HLs2zS2r5xc9cOOzgY438Qqu3IBkJpF92UrHZ26l6Mw%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[从通用智能到场景实战：如何定义好用的「Voice Agent」？ RTE开发者社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605653</link>    <guid>https://segmentfault.com/a/1190000047605653</guid>    <pubDate>2026-02-11 16:03:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在过去的一年里，Voice Agent 的开发者们经历了一场集体“祛魅”。一个被反复提及、逐渐成型的行业共识是：<strong>“Evals are back”（测评回归）。</strong></p><p>这是因为行业遇到了共同的瓶颈：基础模型在通用学术榜单上卷得难解难分，一进到真实的业务电话里，表现往往不如人意。一个能写出精美诗歌的 Agent，可能听不懂带口音的“退款”请求，或者在用户情绪激动时不知道该如何安抚。这就带来一个更现实的问题：<strong>在充斥着打断、噪音和情绪波动的真实通话中，我们到底需要什么样的 Voice Agent？</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605655" alt="" title=""/></p><p>最近，<strong>美团、声网 与 Xbench</strong> 三方联合构建了一个名为 <strong>VoiceAgentEval</strong> 的基准测试，主要解决现有测试方法的三个关键问题：数据集多样性不足、用户模拟不真实、评估指标不准确。</p><p>测试结果表明，大语言模型在外呼对话场景中已经达到了相当的基础能力，并展现出了各自的适用性。这说明，Voice Agent 的发展已经跨过了“参数为王”的阶段，进入了“场景适配”的新时期。</p><p>论文链接： <br/><a href="https://link.segmentfault.com/?enc=tkprgIQpHKdjmJvoRqphWg%3D%3D.sdIprlsJEcxrprrtu1l%2FfNlIuaEqqMPTMT%2Fo96OV2QkKkWnd0gJIUTO9wxbVoIL7%2FdahV1HOVM5W%2BD6Bz5%2FMdQ%3D%3D" rel="nofollow" target="_blank">https://xbench.org/reports/zmbbhdtfc5ui5qx5xjgquusj</a></p><h2>VoiceAgentEval 在做什么</h2><p>在人机对话场景中，用户不仅关注 Agent 是否提供了正确的反馈，如解答疑问、完成任务等；良好的、更像真人间交互体验也是非常重要的评估指标。</p><p>因此，区别于传统测评， VoiceAgentEval 不再执着于考察 Agent 到底“会不会说话”，而是同时从“有没有说对”和“说的好不好”两个层面来评估：</p><ol><li><strong>任务流程遵循度（Task Flow Compliance，TFC）：</strong> AI 客服是否按照业务流程办事，是否真正解决用户的问题</li><li><strong>一般交互能力（General Interaction Capability，GIC）：</strong>  AI 客服的响应是否自然，回复内容是否与谈话主题相关，是否能响应用户的负面情绪等。</li></ol><p>换句话说，这套评估不是在挑“谁最聪明”，而是看谁<strong>最适合在真实通话场景下干活</strong>。</p><p>在 VoiceAgentEval 中，这两类能力通过三个紧密衔接的设计进行评估：</p><p><strong>基准构建（Benchmark）</strong></p><p>从真实外呼业务中抽象出 6 大商业领域（客服、销售、招聘，金融风控、调研以及主动关怀）、 30 个子场景，包括银行投诉、电商退货、面试邀约等在真实世界里出现频率最高的情况。丰富了数据集的多样性与种类，覆盖业务中多样的场景，也就是现实中最容易出现问题的对话。</p><p><strong>用户模拟器（User Simulator）</strong></p><p>本次测评用 LLM 模拟了 5 个性格、背景、沟通风格都不相同的用户，结合 30 个真实业务的子场景，形成 150 种情况下的虚拟用户对话评估。这些虚拟用户有的态度友好，有的犹豫不决，甚至有的情绪抗拒。通过用户模拟器，输出每一个 Agent 在这 150 种真实场景中的 TFC 和 GIC 得分并加权计算出最终测试结果，能够有效的评估 Agent 在复杂场景下遵循任务流程与交互能力的平衡程度。</p><p><strong>评估方法（Evaluation）</strong></p><p>VoiceAgentEval 通过文本和语音，对 Agent 进行 TFC 和 GIC 的双维度评估</p><p>在 TFC 层面，重点关注：</p><ul><li>按业务流程推进对话</li><li>最终把事情“办成”</li></ul><p>在 TIC 层面，评测关注的是：</p><ul><li>在口音、噪音或打断下，是否还能听清关键需求</li><li>回应是否自然、简洁、不制造额外负担</li><li>在被打岔、被质疑时，是否还能保持对话连贯</li></ul><p>也就是说，这套评测是在模拟一通真实业务电话，看看它<strong>能不能把事办完、还能不能让人愿意继续聊</strong>。</p><p>需要说明的是，VoiceAgentEval 并非在离线环境中对模型进行脚本化测试，而是基于声网在实时语音与对话式 AI 领域长期积累的工程能力，搭建出一套真实可运行的 Agent 架构来完成评测流程。因此，评测中的语音交互、流程切换与被打断后的恢复，均通过一条的真实 Voice Agent 链路完成，而非通过静态对话拼接。这也是 VoiceAgentEval 能够在实验条件下逼近真实业务通话复杂度的基础。</p><h2>测评启示：没有最好，只有最合适</h2><p>在这套实时语音交互评测环境中，测试结果并不意味着 Agent 的绝对高低，而是它们在<strong>特定外呼任务设计、用户模拟方式以及评分权重设定</strong> 下所呈现出的行为差异。</p><p>即便如此，这些差异依然为开发者理解模型在高度贴近真实外呼场景中的“行为倾向”提供了一张有价值的参考图谱：</p><ul><li><strong>均衡的“多面手”——</strong> 在“完成办事流程”和“闲聊”之间取得了极佳的平衡。它们既能按流程推进业务，又能顺滑地接住客户的闲聊。如果你需要一个适应性强的通用型 Agent，它们值得优先考虑。</li><li><strong>严谨的“执行者”——</strong> 流程合规性得分高但交互能力相对低一些。就像一个处理金融业务、一丝不苟的银行柜员，绝不随意发挥，但也绝不出错。对于合规性要求极高的严肃场景，它是安全的选择。</li><li><strong>温情的“倾听者”——</strong> 在交互体验上表现优异，极善于安抚沟通，提供情绪价值。如果你的场景是心理咨询或陪伴，它可能比那些“死磕流程”的模型更懂用户的心。</li></ul><p>不仅在外呼场景，随着 Voice Agent 越来越多地走向 AIoT、情感陪伴等日常生活场景，对交互的评测，也正在从“是否听清需求、是否能顺畅对话”，延伸到更底层的环境与语境理解能力。</p><p>在这一层面上，评测维度将不可避免地扩展到对掌声、敲门声等声学事件的感知，对所处环境的声学场景判断，以及对方言、间接表达和语境变化的识别。这些能力决定的，不只是一次对话能否完成，而是 Voice Agent 是否具备在真实环境中持续交互的基础条件。</p><h2>共同的目标：从探索走向落地</h2><p>这套评测体系的发布，其意义不在于分出高下，而在于展示了 Voice Agent 进化的必经之路：<strong>场景 + 技术的双重融合</strong>。</p><ul><li><strong>场景上：</strong> 评测设计基于美团外呼业务中长期积累的真实场景经验与典型问题抽象而来，使得测试不再停留在理想化设定中，而是带有明显的“泥土味”。</li><li><strong>技术上：</strong> 通过声网的音视频技术积累和架构支持，验证了一套可复用的“生产级”技术栈。</li></ul><p>对于整个开发者社区而言，这传达了两个积极的信号：</p><ol><li><strong>选型更从容：</strong> 我们不必再盲目追求“最强”模型，而是可以根据业务需求（是重逻辑还是重体验）找到最匹配的那一块拼图。</li><li><strong>研发更聚焦：</strong> 开发者不必重复造轮子，可以将宝贵的精力投入到对业务逻辑的打磨上。</li></ol><h2>结语：共建行业的“度量衡”</h2><p>AI 的进化速度太快，单打独斗的时代已经过去。</p><p>我们解读这篇论文，是希望所有 Voice Agent 的从业者关注这种“场景化测评”的趋势。VoiceAgentEval 给出了外呼场景的一种答案，更像是一次示范：如何把一个具体业务，拆解成可被复用的评测单元。</p><p>当 Evals 从“纸上谈兵”回归到“实战演练”，当底层的实时交互框架逐步成熟，Voice Agent 才有可能真正走出实验室，接受千行百业的复杂检验。这扇门是否能被真正推开，最终取决于行业能否持续围绕具体场景，持续形成可被复用、可被讨论、也可被不断修正的共同度量。</p><p>参考链接</p><p>xbench 官网： <br/><a href="https://link.segmentfault.com/?enc=TnbJ0NY8azxf1kONRnu7vQ%3D%3D.KhJbJxZluNEzjvpsqHpLsOT%2BlcgqYC2N3UJeJBwvS9wuTaLpslllTEWnzO11G2EU" rel="nofollow" target="_blank">https://xbench.org/VoiceAgentEval</a> </p><p>新闻稿：<br/><a href="https://link.segmentfault.com/?enc=8vFuhUkgC2UgSsIIOpq0%2BQ%3D%3D.77X%2FGy1OtpeDB5QbfTs20B4Q3zEHACH7AMNWjxqTWcIFjmLK%2FAdlAEKqnCJdl2v5cbOPozfyK%2BJ0XXzSiNByvg%3D%3D" rel="nofollow" target="_blank">https://xbench.org/reports/zmbbhdtfc5ui5qx5xjgquusj</a></p><p>声网对话式 AI 引擎：<br/><a href="https://link.segmentfault.com/?enc=dKSgoASr8dqHxWjQzBxXpQ%3D%3D.2HMfesnI6pNH4Hl4aMpMaLv%2FuCOEy%2Bbo5y0FHTrRpF3575VdUJzWzfW4uxMMgzQX" rel="nofollow" target="_blank">https://www.shengwang.cn/ConversationalAI/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605656" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605657" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=RoIq7JDyMEjvrDxGFeQ3Bg%3D%3D.UfWQCU8M4Mo0n752cXfeqq%2FH2Op6fmAITv0IsXWIg8o%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605658" alt="" title="" loading="lazy"/></p>]]></description></item>  </channel></rss>