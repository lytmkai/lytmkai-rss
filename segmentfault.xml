<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[想不到吧！68%做AI的Java开发者选]]></title>    <link>https://segmentfault.com/a/1190000047383776</link>    <guid>https://segmentfault.com/a/1190000047383776</guid>    <pubDate>2025-11-10 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>文 / 勇哥<br/>原创文章，转载请联系授权</blockquote><p>在前一篇文章中，我们探讨了《<a href="https://link.segmentfault.com/?enc=PaFcP8PQP4mT14mJVBunCw%3D%3D.LTP%2FzJvwo52zRqzrtGdhUvTxhP%2FQjy40RjObyNrkyfQfzc87cNlgVs8GdHy%2F%2F1Sq4Cwb1y3c0XbhskiN804bdw%3D%3D" rel="nofollow" target="_blank">Spring AI，一个让 Spring 应用轻松拥抱 AI 的统一框架</a>》。今天，让我们深入剖析LangChain4j——这个被Java开发者亲切称为"大模型开发瑞士军刀"的框架，它是在2023年年底由LangChain官方和社区贡献者共同发起，旨在为Java生态提供专业、高效的大模型应用开发解决方案，它的出现填补了Java生态在大模型应用开发领域的关键空白。</p><p>作为一名拥有10年Java开发经验的架构师，我见证了太多Java团队在大模型时代的迷茫与挣扎：一方面是Python生态的LangChain、LlamaIndex等框架如日中天，另一方面是企业中大量Java系统亟需拥抱AI能力。LangChain4j的出现，就像给Java开发者送来了一把钥匙，让他们能够用熟悉的语言和工具打开大模型应用开发的大门。</p><p><strong>核心观点：LangChain4j是Java开发者的"大模型应用开发加速器"，它为企业Java应用与大模型能力之间搭建了一座标准化、专业化的桥梁，让Java开发者能够以最低的学习成本构建企业级大模型应用。</strong></p><h2>一、LangChain4j：为什么它是Java开发者的"大模型开发利器"？</h2><p>想象一下，你是一名Java开发者，想要将大模型能力集成到现有企业系统中。你面临的选择是什么？</p><p>要么使用Python的LangChain框架，然后通过各种复杂的集成方式与Java系统连接；要么直接调用OpenAI等API，自己处理繁琐的请求构造、响应解析、状态管理等工作。这两种方式都像是在崎岖不平的山路上行驶，充满了颠簸和不确定性。</p><p>LangChain4j就像是为Java开发者铺设的一条高速公路，它提供了：</p><ul><li><strong>统一的API抽象</strong>：屏蔽不同大模型服务提供商的差异，提供一致的调用体验；</li><li><strong>丰富的企业级组件</strong>：从聊天记忆到向量检索，从提示词模板到工作流编排，满足企业应用开发的各种需求；</li><li><strong>与Java生态深度融合</strong>：支持Spring Boot、JPA等主流Java技术栈，让集成变得简单自然。</li></ul><p>一句话，LangChain4j让Java开发者能够用"Java的方式"开发大模型应用，无需学习新的编程语言或框架理念。</p><h2>二、LangChain4j的核心框架：6大组件构建大模型应用的"积木系统"</h2><p>LangChain4j提供了一套完整的组件体系，让开发者能够像搭积木一样构建大模型应用：</p><h3>2.1 LLM Client：连接大模型的"桥梁"</h3><p>一句话概括：LLM Client是与各类大语言模型交互的标准化接口，负责处理请求构造、响应解析和错误处理。</p><p><strong>核心功能：</strong></p><ul><li><strong>模型适配</strong>：支持OpenAI、Azure OpenAI、Anthropic、Llama等多种模型服务；</li><li><strong>参数配置</strong>：统一的参数设置接口，包括温度、最大token数、超时时间等；</li><li><strong>异步支持</strong>：提供同步和异步调用方式，满足不同场景需求。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>按需选择模型</strong>：根据任务复杂度和预算选择合适的模型；</li><li><strong>合理设置参数</strong>：温度值控制创造性，较高的温度适合创意生成，较低的温度适合精确回答。</li></ul><p>适用场景：基础问答、内容生成、代码辅助等各类大模型交互场景。</p><h3>2.2 Chat Memory：维护对话上下文的"记忆系统"</h3><p>一句话概括：Chat Memory负责存储和管理对话历史，让模型能够"记住"之前的交互内容，实现连续对话。</p><p><strong>核心功能：</strong></p><ul><li><strong>消息存储</strong>：记录用户问题和模型回复；</li><li><strong>上下文管理</strong>：智能截断和保留重要信息；</li><li><strong>多种实现</strong>：支持基于窗口、令牌计数等多种记忆策略。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>合理设置记忆大小</strong>：避免过长历史导致token超量，又要保留足够的上下文；</li><li><strong>考虑性能影响</strong>：内存型记忆适合小规模应用，大规模应用应考虑持久化方案。</li></ul><p>适用场景：聊天机器人、客户服务、交互式助手等需要上下文理解的场景。</p><h3>2.3 Prompt Template：标准化提示词的"模板引擎"</h3><p>一句话概括：Prompt Template提供了一种结构化创建提示词的方式，通过变量替换生成个性化提示。</p><p><strong>核心功能：</strong></p><ul><li><strong>模板定义</strong>：使用占位符定义可复用的提示词模板；</li><li><strong>变量替换</strong>：在运行时动态填充模板内容；</li><li><strong>格式控制</strong>：支持不同的输出格式要求。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>模板复用</strong>：将常用提示词抽象为模板，提高代码复用性；</li><li><strong>提示词工程</strong>：结合提示词工程最佳实践设计模板内容。</li></ul><p>适用场景：需要根据用户输入动态生成提示词的各类应用。</p><h3>2.4 Chain：编排工作流的"流程引擎"</h3><p>一句话概括：Chain是将多个组件组合成工作流的核心抽象，实现复杂业务逻辑的编排。</p><p><strong>核心功能：</strong></p><ul><li><strong>组件组合</strong>：将模型、记忆、模板等组件链接起来；</li><li><strong>数据流转</strong>：控制数据在组件之间的流动和转换；</li><li><strong>错误处理</strong>：提供统一的异常处理机制。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>模块化设计</strong>：将复杂流程拆分为简单的链式组件；</li><li><strong>职责单一</strong>：每个Chain专注于完成一个具体功能。</li></ul><p>适用场景：复杂的大模型应用，需要多步骤处理的业务流程。</p><h3>2.5 Agent：自主决策的"智能助手"</h3><p>一句话概括：Agent是能够基于目标和环境做出决策并执行动作的高级组件，实现智能化任务处理。</p><p><strong>核心功能：</strong></p><ul><li><strong>目标分解</strong>：将复杂任务分解为子任务；</li><li><strong>工具使用</strong>：调用外部工具完成特定操作；</li><li><strong>决策路径</strong>：根据执行结果动态调整后续步骤。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>合理定义工具</strong>：根据任务需求定义清晰的工具接口；</li><li><strong>设置边界</strong>：明确Agent的能力范围，避免无限递归或越界行为。</li></ul><p>适用场景：需要自主完成复杂任务的应用，如数据分析助手、个人助理等。</p><h3>2.6 Embedding：文本向量化的"翻译官"</h3><p>一句话概括：Embedding负责将文本转换为向量表示，是实现语义搜索、相似性比较的基础。</p><p><strong>核心功能：</strong></p><ul><li><strong>文本编码</strong>：将文本转换为高维向量；</li><li><strong>向量存储</strong>：与各类向量数据库集成；</li><li><strong>相似度计算</strong>：提供向量相似度比较功能。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>选择合适的嵌入模型</strong>：根据文本类型和精度要求选择合适的模型；</li><li><strong>优化向量存储</strong>：考虑索引策略和检索性能。</li></ul><p>适用场景：知识库问答、文档检索、内容推荐等需要语义理解的场景。</p><h2>三、LangChain4j实战：从入门到精通的4个步骤</h2><h3>3.1 步骤1：环境准备与基础配置</h3><p><strong>核心工作：</strong></p><ul><li><strong>添加依赖</strong>：在Maven或Gradle项目中添加LangChain4j相关依赖；</li><li><strong>配置API密钥</strong>：安全管理大模型服务的API密钥；</li><li><strong>设置开发环境</strong>：确保Java版本兼容（推荐JDK 17+）。</li></ul><p><strong>实战建议：</strong></p><ul><li>使用环境变量或配置管理系统存储API密钥，避免硬编码；</li><li>从官方示例项目开始，快速熟悉基本使用方式。</li></ul><h3>3.2 步骤2：构建基础聊天应用</h3><p><strong>核心工作：</strong></p><ul><li><strong>创建模型实例</strong>：初始化适合任务的大模型客户端；</li><li><strong>配置聊天记忆</strong>：实现多轮对话能力；</li><li><strong>封装交互接口</strong>：为用户提供友好的交互方式。</li></ul><p><strong>实战建议：</strong></p><ul><li>从简单的单轮对话开始，逐步添加记忆功能；</li><li>实现流式响应，提升用户体验；</li><li>考虑添加错误处理和日志记录。</li></ul><h3>3.3 步骤3：实现知识库增强（RAG）</h3><p><strong>核心工作：</strong></p><ul><li><strong>文档处理</strong>：加载、解析和分块文档；</li><li><strong>向量化处理</strong>：使用嵌入模型转换文本为向量；</li><li><strong>向量存储</strong>：选择合适的向量数据库存储向量；</li><li><strong>检索增强</strong>：实现基于相似度的检索和上下文增强。</li></ul><p><strong>实战建议：</strong></p><ul><li>针对不同类型文档选择合适的解析器；</li><li>合理设置分块大小，平衡语义完整性和检索精度；</li><li>实现检索结果排序和过滤，提高相关性。</li></ul><h3>3.4 步骤4：构建复杂应用和部署</h3><p><strong>核心工作：</strong></p><ul><li><strong>组件组合</strong>：使用Chain编排复杂业务流程；</li><li><strong>集成外部系统</strong>：与企业现有系统对接；</li><li><strong>性能优化</strong>：缓存、异步处理等性能调优；</li><li><strong>监控与维护</strong>：实现日志、指标收集和告警。</li></ul><p><strong>实战建议：</strong></p><ul><li>采用模块化设计，便于测试和维护；</li><li>实现熔断和限流机制，防止服务过载；</li><li>建立完善的监控体系，及时发现和解决问题。</li></ul><h2>四、LangChain4j实战经验：避免4个常见陷阱</h2><p>在多年的大模型应用开发实践中，我总结了4个最容易踩的坑和对应的解决方法：</p><p><strong>陷阱1：API密钥安全问题</strong></p><ul><li><strong>表现</strong>：在代码中硬编码API密钥，导致密钥泄露风险；</li><li><strong>解决方法</strong>：使用环境变量、配置文件或密钥管理系统存储API密钥，并限制访问权限。</li></ul><pre><code class="java">// 推荐方式
String apiKey = System.getenv("OPENAI_API_KEY");</code></pre><p><strong>陷阱2：模型选择不当</strong></p><ul><li><strong>表现</strong>：盲目使用最高级模型，导致成本过高；或使用过低级模型，导致结果质量不佳；</li><li><strong>解决方法</strong>：根据任务复杂度和预算选择合适的模型，简单任务使用gpt-3.5-turbo，复杂推理使用gpt-4，企业部署考虑本地模型。</li></ul><p><strong>陷阱3：Token管理失控</strong></p><ul><li><strong>表现</strong>：未设置合理的token限制，导致生成内容过长或超出预算；</li><li><strong>解决方法</strong>：设置最大token限制和超时时间，实现成本控制。</li></ul><pre><code class="java">OpenAiChatModel model = OpenAiChatModel.builder()
            .apiKey(apiKey)
            .maxTokens(1000)  // 限制生成长度
            .timeout(Duration.ofSeconds(30))  // 设置超时
            .build();</code></pre><p><strong>陷阱4：用户体验忽视</strong></p><pre><code class="java">@GetMapping(value = "/chat/streaming", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux&lt;String&gt; streamingAssistant(
            @RequestParam(value = "msg", defaultValue = "What is the current time?") String message) {
        return streamingAssistant.chat(message);
    }</code></pre><ul><li><strong>表现</strong>：对于长响应采用同步等待方式，导致用户等待时间过长；</li><li><strong>解决方法</strong>：使用流式响应，实时输出内容，提升用户体验。</li></ul><h2>五、LangChain4j的行业趋势与未来展望</h2><p>作为Java生态中的大模型应用开发框架，LangChain4j正在引领Java开发者进入AI时代。根据行业观察和技术发展趋势，我们可以预见：</p><h3>5.1 企业级AI应用将进入爆发期</h3><p>根据Gartner预测，到2027年，80%的企业将部署至少一个基于大模型的核心业务应用。考虑到Java在企业应用中的主导地位（据统计，全球企业中约60%的后端系统基于Java开发），LangChain4j等Java大模型框架将成为企业AI转型的关键基础设施。</p><h3>5.2 Java开发者角色正在重塑</h3><p>在大模型时代，Java开发者的角色正在从传统的"代码编写者"转变为"AI编排者"。他们不再需要从零开始实现业务逻辑，而是通过组合和定制大模型能力来创造价值。LangChain4j正是为这种角色转变提供了有力工具。</p><h3>5.3 与Spring生态深度融合</h3><p>随着Spring AI的推出，LangChain4j与Spring生态的融合将更加紧密，虽然现在LangChain4j已经在开发Spring boot相关的Starter来加速Spring生态的融合了，但是使用起来还是需要一定的学习成本和使用难度的。未来，我们可能会看到更多专为企业级应用设计的特性，如声明式API、自动配置、与Spring Security集成等，进一步降低Java开发者使用大模型的门槛。</p><h2>六、总结与行动建议</h2><p>LangChain4j不是一个简单的Java版本的LangChain，而是为Java开发者和企业应用量身定制的大模型应用开发框架。它填补了Java生态在大模型应用开发领域的关键空白，为企业Java系统拥抱AI能力提供了一条平滑路径。</p><p><strong>给Java开发者的3个行动建议：</strong></p><ol><li><strong>立即开始学习</strong>：花时间掌握LangChain4j的核心概念和使用模式，将其视为职业发展的加分项；</li><li><strong>从小项目练手</strong>：选择一个内部工具或非关键业务场景，尝试用LangChain4j构建原型，积累实战经验；</li><li><strong>关注生态发展</strong>：密切关注LangChain4j社区动态和版本更新，参与开源贡献，成为技术浪潮的引领者而非跟随者。</li></ol><p><strong>记住LangChain4j的核心理念</strong>："让Java开发者能够以熟悉的方式构建企业级大模型应用"——这也是我们在AI时代保持竞争力的关键。</p><p>可参考的资源：</p><ul><li><a href="https://link.segmentfault.com/?enc=Qaxihfd24K5YGzJMowkFKg%3D%3D.AW3h4jeMs5d48gRKW6ytQocfVKQrPh2HHVmFVL9KkVA%3D" rel="nofollow" target="_blank">LangChain4j官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=9rqyHmbpXGpN%2FRYu2e9h8A%3D%3D.3GRYt7e1fM6mwC%2BPad3A57PINCzYfF%2FR%2B3SrS5dTsQWzRbNhxNSL9ePVBL%2BKs2Vi" rel="nofollow" target="_blank">LangChain4j官方网站</a></li><li><a href="https://link.segmentfault.com/?enc=xuZMepnrmGkgZPzHHXgakA%3D%3D.%2BQkcd2HtDzh8iL0nxiIPpkpeABWXDySoZ2EevzPik4GfokX9MvZ18X88oars8CAMR1udZu1k8vr8gojCcdxRXA%3D%3D" rel="nofollow" target="_blank">LangChain4j示例项目</a></li></ul><hr/><p><strong>互动话题</strong>：你在使用LangChain4j开发过程中，遇到过哪些有趣的挑战或成功案例？或者你对Java生态与大模型的融合有什么独特的见解？欢迎在评论区与我分享你的故事和想法。</p><p><strong>关于作者</strong>：勇哥，10多年的开发和技术管理经验，从程序员做到企业技术高管。目前专注架构设计和人工智能应用实践，全网帐号统一名称"六边形架构"，有些不太合适发到公号的内容我会单独发到我的朋友圈，欢迎关注我，一起交流学习。</p><p><em>原创不易，如果觉得有帮助，请点赞、收藏、转发三连支持！</em></p>]]></description></item><item>    <title><![CDATA[进入职场第二课—融入 老李说技术 ]]></title>    <link>https://segmentfault.com/a/1190000047383732</link>    <guid>https://segmentfault.com/a/1190000047383732</guid>    <pubDate>2025-11-09 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>经过观察后要做的第二件事一定是融入，大白话来说，就是经过观察，结论是想要留下来，接下来要顺利融入，适应这家公司、这个团队，安全度过新手期。融入和观察一样，都属于入职后的第一阶段，从完全陌生到逐渐熟悉。《易经》乾卦中的“潜龙勿用”，其实说的就是这两件事相加——观察是完全接收输入之后判断，而融入则是在观察的基础上，开始主动输出。</p><p>经过新手期的观察，你已经对业务，团队，成员，以及现状，有了充分的认识，那么融入，就是需要我们根据这些信息，走的更顺，更稳。</p><p>通过以下 4 个关键动作，快速完成从“局外人”到“团队一份子”的蜕变。</p><h3>一、顺着领导的脾气干：先成为他“预期中”的靠谱下属</h3><p>你的直属领导，也就是你的老板，是你想要融入这个团队必须搞定的人。他不认可你，不看好你，可能连试用期都过不了，更别提融入这个团队了。</p><p>观察出来领导的脾气，顺着这个脾气干，就是你获得领导认可的关键。看看他喜欢的下属是怎么工作的，他们之间又是怎么配合的，很快你就会有答案，照着学，千万别犹豫。</p><p>我的前领导曾说：“我不要求你一开始多厉害，但得让我看到你在‘努力契合’——这种态度比能力更重要。” 比如他爱早会同步进度，就别总等周报才汇报；他重视数据支撑，汇报时就多带具体案例和结果；他习惯简洁沟通，就别用长篇大论消耗他的时间。</p><h3>​二、和关键同事建立信任：他们是你的“隐形导师”​</h3><p>无论你是职场老鸟，还是应届新人，入职之后，你的老板都会给你制定试用期计划。这个计划涉及的工作内容，你可能有经验，也可能什么都不懂——除非你的上一家公司是现在这家公司的竞争对手，业务做的够大、够接近，否则你都得有一个学习上手的过程。</p><p>这个过程一般包含两件事：看代码、看资料、学业务，问老人。这个“老人”指的是懂业务的同时，也是对你来说的关键同事——是你不能得罪，必须要建立信任的人。因为他告诉你多一点，你就会少走很多弯路，避开很多雷区；他告诉你少一点，你就惨了，自己费很大的劲不说，还会踩很多值钱的坑。</p><p>请他吃个饭拜拜码头，时不时买杯咖啡——伸手不打笑脸人。不需要刻意讨好，真诚 + 分寸感就够了：一句“前辈，这个问题我实在搞不懂，能耽误您 10 分钟吗？”配上认真记笔记的态度，事后反馈“按您说的做，果然少走了弯路”，就能快速拉近距离。</p><h3>三、完成本职工作：“好”永远比“快”重要</h3><p>所有老板都希望你做的又快又好，但这句话还有后半句：如果让老板在快和好里面只能选一个，所有老板都会选好，而不会选快。快一定是建立在好的基础上，如果做不好，达不到要求，交付之后频频出问题，天天被客户投诉，没有一个老板想要这样的结果。他们宁愿你慢慢做，做好了再交付。</p><p>对你来说，本职工作一定要做好，试用期计划里的内容也一定要实现。切记，是完成本职工作，而不是快速的完成。这个阶段快没有任何好处——职场想发展的好，走得稳，走得远，这一点必须学会。比谁做题做得快，比谁背课文背得快，比谁回答问题反应快，这都是还在上小学、初中的学生在比的东西，职场里没人关注这个。</p><h3>四、适应公司的已有气场：先“入乡随俗”，再谈改变*</h3><p>每个公司都有自己的气场，作为新人，你只能收起自己的锋芒。原因很简单，你还没有做出任何成绩。平时加不加班，考勤严不严格，流程繁不繁琐，氛围活不活跃，搞清楚这些之后，加入他们，照着做，千万别特立独行。</p><p>公司开会多，你就跟着开，听听大家都是什么套路；说事情总是发邮件，不直接说，你也跟着发，措辞照着写的好的学；办公区随处可见口号横幅、价值观标语，其他员工什么态度，你也什么态度；如果公司组织学习价值观的话，你就好好认真听。</p><h2>写在最后：潜龙勿用，是为了更好地“见龙在田”</h2><p>其实融入并不难，把上面这 4 个动作做个七七八八，大概率都不会有啥问题。融入之后，恭喜你，你也就顺利度过了新手期。</p><p>再提醒一下，“潜龙勿用”就是龙还潜在水下，还不到你施展才华的时候。别露头，攒着劲，别着急，找出新公司的规律，按这个规律办事，就是顺势而为。</p><p>记住：观察是“找出规律”，融入是“学会按规律办事”——二者叠加，就是《易经》里“潜龙勿用”的完整含义。</p>]]></description></item><item>    <title><![CDATA[LightRAG 实战： 基于 Olla]]></title>    <link>https://segmentfault.com/a/1190000047383682</link>    <guid>https://segmentfault.com/a/1190000047383682</guid>    <pubDate>2025-11-09 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>LightRAG 是个开源的 RAG 框架，专门用来快速搭建模块化的检索增强生成管道。这个项目在 GitHub 上热度不低，我们今天来看看他到底怎么用<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383684" alt="" title=""/></p><h2>基础安装与环境配置</h2><p>LightRAG 的安装过程很简单，几行命令就能搞定：</p><pre><code> pip install "lightrag-hku[api]"  
 cp env.example .env # ---&gt;这个有很多参数 非常丰富
 lightrag-server</code></pre><p>官方提供的 UI 界面做得还算不错，不过测试时基本没用上，因为更关注的是代码层面的实现。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383685" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383686" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383687" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383688" alt="" title="" loading="lazy"/><br/>环境搭好之后，可以先跑一下官方提供的示例代码(摘自 readme)：</p><pre><code> import os  
import asyncio  
from lightrag import LightRAG, QueryParam  
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed  
from lightrag.kg.shared_storage import initialize_pipeline_status  
from lightrag.utils import setup_logger  

setup_logger("lightrag", level="INFO")  

WORKING_DIR = "./rag_storage"  
if not os.path.exists(WORKING_DIR):  
    os.mkdir(WORKING_DIR)  

async def initialize_rag():  
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        embedding_func=openai_embed,  
        llm_model_func=gpt_4o_mini_complete,  
    )  
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    await initialize_pipeline_status()  # Initialize processing pipeline
    return rag  

async def main():  
    try:  
        # Initialize RAG instance
        rag = await initialize_rag()  
        await rag.ainsert("Your text")  

        # Perform hybrid search
        mode = "hybrid"  
        print(  
          await rag.aquery(  
              "What are the top themes in this story?",  
              param=QueryParam(mode=mode)  
          )  
        )  

    except Exception as e:  
        print(f"An error occurred: {e}")  
    finally:  
        if rag:  
            await rag.finalize_storages()  

if __name__ == "__main__":  
     asyncio.run(main())</code></pre><p>官方示例里还有个基于 Gemini 的版本，看着比较简单就选了这个来测试：</p><pre><code> # pip install -q -U google-genai to use gemini as a client  

import os  
import numpy as np  
from google import genai  
from google.genai import types  
from dotenv import load_dotenv  
from lightrag.utils import EmbeddingFunc  
from lightrag import LightRAG, QueryParam  
from sentence_transformers import SentenceTransformer  
from lightrag.kg.shared_storage import initialize_pipeline_status  

import asyncio  
import nest_asyncio  

# Apply nest_asyncio to solve event loop issues
nest_asyncio.apply()  

load_dotenv()  
gemini_api_key = os.getenv("GEMINI_API_KEY")  

WORKING_DIR = "./dickens"  

if os.path.exists(WORKING_DIR):  
    import shutil  

    shutil.rmtree(WORKING_DIR)  

os.mkdir(WORKING_DIR)  

async def llm_model_func(  
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs  
) -&gt; str:  
    # 1. Initialize the GenAI Client with your Gemini API Key
    client = genai.Client(api_key=gemini_api_key)  

    # 2. Combine prompts: system prompt, history, and user prompt
    if history_messages is None:  
        history_messages = []  

    combined_prompt = ""  
    if system_prompt:  
        combined_prompt += f"{system_prompt}\n"  

    for msg in history_messages:  
        # Each msg is expected to be a dict: {"role": "...", "content": "..."}
        combined_prompt += f"{msg['role']}: {msg['content']}\n"  

    # Finally, add the new user prompt
    combined_prompt += f"user: {prompt}"  

    # 3. Call the Gemini model
    response = client.models.generate_content(  
        model="gemini-1.5-flash",  
        contents=[combined_prompt],  
        config=types.GenerateContentConfig(max_output_tokens=500, temperature=0.1),  
    )  

    # 4. Return the response text
    return response.text  

async def embedding_func(texts: list[str]) -&gt; np.ndarray:  
    model = SentenceTransformer("all-MiniLM-L6-v2")  
    embeddings = model.encode(texts, convert_to_numpy=True)  
    return embeddings  

async def initialize_rag():  
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        llm_model_func=llm_model_func,  
        embedding_func=EmbeddingFunc(  
            embedding_dim=384,  
            max_token_size=8192,  
            func=embedding_func,  
        ),  
    )  

    await rag.initialize_storages()  
    await initialize_pipeline_status()  

    return rag  

def main():  
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())  
    file_path = "story.txt"  
    with open(file_path, "r") as file:  
        text = file.read()  

    rag.insert(text)  

    response = rag.query(  
        query="What is the main theme of the story?",  
        param=QueryParam(mode="hybrid", top_k=5, response_type="single line"),  
    )  

    print(response)  

if __name__ == "__main__":  
     main()</code></pre><pre><code>.env</code></pre><p>配置文件的参数非常丰富，需要根据实际使用的工具链做适配。为了调用本地模型，我这里用Ollama 做了相应调整。</p><pre><code> ### This is sample file of .env

###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='My Graph KB'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
# WORKERS=2
### gunicorn worker timeout(as default LLM request timeout if LLM_TIMEOUT is not set)
# TIMEOUT=150
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
### Default value is ./inputs and ./rag_storage
# INPUT_DIR=&lt;absolute_path_for_doc_input_dir&gt;
# WORKING_DIR=&lt;absolute_path_for_working_dir&gt;

### Tiktoken cache directory (Store cached files in this folder for offline deployment)
# TIKTOKEN_CACHE_DIR=/app/data/tiktoken

### Ollama Emulating Model and Tag
# OLLAMA_EMULATING_MODEL_NAME=lightrag
OLLAMA_EMULATING_MODEL_TAG=latest

### Max nodes return from graph retrieval in webui
# MAX_GRAPH_NODES=1000

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory

#####################################
### Login and API-Key Configuration
#####################################
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

######################################################################################
### Query Configuration
###
### How to control the context length sent to LLM:
###    MAX_ENTITY_TOKENS + MAX_RELATION_TOKENS &lt; MAX_TOTAL_TOKENS
###    Chunk_Tokens = MAX_TOTAL_TOKENS - Actual_Entity_Tokens - Actual_Relation_Tokens
######################################################################################
# LLM response cache for query (Not valid for streaming response)
ENABLE_LLM_CACHE=true
# COSINE_THRESHOLD=0.2
### Number of entities or relations retrieved from KG
# TOP_K=40
### Maximum number or chunks for naive vector search
# CHUNK_TOP_K=20
### control the actual entities send to LLM
# MAX_ENTITY_TOKENS=6000
### control the actual relations send to LLM
# MAX_RELATION_TOKENS=8000
### control the maximum tokens send to LLM (include entities, relations and chunks)
# MAX_TOTAL_TOKENS=30000

### chunk selection strategies
###     VECTOR: Pick KG chunks by vector similarity, delivered chunks to the LLM aligning more closely with naive retrieval
###     WEIGHT: Pick KG chunks by entity and chunk weight, delivered more solely KG related chunks to the LLM
###     If reranking is enabled, the impact of chunk selection strategies will be diminished.
# KG_CHUNK_PICK_METHOD=VECTOR

#########################################################
### Reranking configuration
### RERANK_BINDING type:  null, cohere, jina, aliyun
### For rerank model deployed by vLLM use cohere binding
#########################################################
RERANK_BINDING=null
### Enable rerank by default in query params when RERANK_BINDING is not null
# RERANK_BY_DEFAULT=True
### rerank score chunk filter(set to 0.0 to keep all chunks, 0.6 or above if LLM is not strong enough)
# MIN_RERANK_SCORE=0.0

### For local deployment with vLLM
# RERANK_MODEL=BAAI/bge-reranker-v2-m3
# RERANK_BINDING_HOST=http://localhost:8000/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Cohere AI
# RERANK_MODEL=rerank-v3.5
# RERANK_BINDING_HOST=https://api.cohere.com/v2/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Jina AI
# RERANK_MODEL=jina-reranker-v2-base-multilingual
# RERANK_BINDING_HOST=https://api.jina.ai/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Aliyun
# RERANK_MODEL=gte-rerank-v2
# RERANK_BINDING_HOST=https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

########################################
### Document processing configuration
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### Document processing output language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=English

### Entity types that the LLM will attempt to recognize
# ENTITY_TYPES='["Person", "Creature", "Organization", "Location", "Event", "Concept", "Method", "Content", "Data", "Artifact", "NaturalObject"]'

### Chunk size for document splitting, 500~1500 is recommended
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### Number of summary segments or tokens to trigger LLM summary on entity/relation merge (at least 3 is recommended)
# FORCE_LLM_SUMMARY_ON_MERGE=8
### Max description token size to trigger LLM summary
# SUMMARY_MAX_TOKENS = 1200
### Recommended LLM summary output length in tokens
# SUMMARY_LENGTH_RECOMMENDED_=600
### Maximum context size sent to LLM for description summary
# SUMMARY_CONTEXT_SIZE=12000

### control the maximum chunk_ids stored in vector and graph db
# MAX_SOURCE_IDS_PER_ENTITY=300
# MAX_SOURCE_IDS_PER_RELATION=300
### control chunk_ids limitation method: FIFO, KEEP
###    FIFO: First in first out
###    KEEP: Keep oldest (less merge action and faster)
# SOURCE_IDS_LIMIT_METHOD=FIFO

# Maximum number of file paths stored in entity/relation file_path field (For displayed only, does not affect query performance)
# MAX_FILE_PATHS=100

### maximum number of related chunks per source entity or relation
###     The chunk picker uses this value to determine the total number of chunks selected from KG(knowledge graph)
###     Higher values increase re-ranking time
# RELATED_CHUNK_NUMBER=5

###############################
### Concurrency Configuration
###############################
### Max concurrency requests of LLM (for both query and document processing)
MAX_ASYNC=4
### Number of parallel processing documents(between 2~10, MAX_ASYNC/3 is recommended)
MAX_PARALLEL_INSERT=2
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=8
### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=10

###########################################################
### LLM Configuration
### LLM_BINDING type: openai, ollama, lollms, azure_openai, aws_bedrock
###########################################################
### LLM request timeout setting for all llm (0 means no timeout for Ollma)
# LLM_TIMEOUT=180

LLM_BINDING=ollama
LLM_MODEL=granite4:latest  
LLM_BINDING_HOST=http://localhost:11434
[#LLM](#LLM)_BINDING_API_KEY=your_api_key

### Optional for Azure
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

### Openrouter example
# LLM_MODEL=google/gemini-2.5-flash
# LLM_BINDING_HOST=https://openrouter.ai/api/v1
# LLM_BINDING_API_KEY=your_api_key
# LLM_BINDING=openai

### OpenAI Compatible API Specific Parameters
### Increased temperature values may mitigate infinite inference loops in certain LLM, such as Qwen3-30B.
# OPENAI_LLM_TEMPERATURE=0.9
### Set the max_tokens to mitigate endless output of some LLM (less than LLM_TIMEOUT * llm_output_tokens/second, i.e. 9000 = 180s * 50 tokens/s)
### Typically, max_tokens does not include prompt content, though some models, such as Gemini Models, are exceptions
### For vLLM/SGLang deployed models, or most of OpenAI compatible API provider
# OPENAI_LLM_MAX_TOKENS=9000
### For OpenAI o1-mini or newer modles
[#OPENAI](#OPENAI)_LLM_MAX_COMPLETION_TOKENS=9000

#### OpenAI's new API utilizes max_completion_tokens instead of max_tokens
# OPENAI_LLM_MAX_COMPLETION_TOKENS=9000

### use the following command to see all support options for OpenAI, azure_openai or OpenRouter
### lightrag-server --llm-binding openai --help
### OpenAI Specific Parameters
# OPENAI_LLM_REASONING_EFFORT=minimal
### OpenRouter Specific Parameters
# OPENAI_LLM_EXTRA_BODY='{"reasoning": {"enabled": false}}'
### Qwen3 Specific Parameters deploy by vLLM
# OPENAI_LLM_EXTRA_BODY='{"chat_template_kwargs": {"enable_thinking": false}}'

### use the following command to see all support options for Ollama LLM
### If LightRAG deployed in Docker uses host.docker.internal instead of localhost in LLM_BINDING_HOST
### lightrag-server --llm-binding ollama --help
### Ollama Server Specific Parameters
### OLLAMA_LLM_NUM_CTX must be provided, and should at least larger than MAX_TOTAL_TOKENS + 2000
OLLAMA_LLM_NUM_CTX=32768
### Set the max_output_tokens to mitigate endless output of some LLM (less than LLM_TIMEOUT * llm_output_tokens/second, i.e. 9000 = 180s * 50 tokens/s)
# OLLAMA_LLM_NUM_PREDICT=9000
### Stop sequences for Ollama LLM
# OLLAMA_LLM_STOP='["&lt;/s&gt;", "&lt;|EOT|&gt;"]'

### Bedrock Specific Parameters
# BEDROCK_LLM_TEMPERATURE=1.0

####################################################################################
### Embedding Configuration (Should not be changed after the first file processed)
### EMBEDDING_BINDING: ollama, openai, azure_openai, jina, lollms, aws_bedrock
####################################################################################
# EMBEDDING_TIMEOUT=30
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=granite-embedding:latest
EMBEDDING_DIM=1024
EMBEDDING_BINDING_API_KEY=your_api_key
# If LightRAG deployed in Docker uses host.docker.internal instead of localhost
EMBEDDING_BINDING_HOST=http://localhost:11434

### OpenAI compatible (VoyageAI embedding openai compatible)
# EMBEDDING_BINDING=openai
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_DIM=3072
# EMBEDDING_BINDING_HOST=https://api.openai.com/v1
# EMBEDDING_BINDING_API_KEY=your_api_key

### Optional for Azure
# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
# AZURE_EMBEDDING_API_VERSION=2023-05-15
# AZURE_EMBEDDING_ENDPOINT=your_endpoint
# AZURE_EMBEDDING_API_KEY=your_api_key

### Jina AI Embedding
# EMBEDDING_BINDING=jina
# EMBEDDING_BINDING_HOST=https://api.jina.ai/v1/embeddings
# EMBEDDING_MODEL=jina-embeddings-v4
# EMBEDDING_DIM=2048
# EMBEDDING_BINDING_API_KEY=your_api_key

### Optional for Ollama embedding
OLLAMA_EMBEDDING_NUM_CTX=8192
### use the following command to see all support options for Ollama embedding
### lightrag-server --embedding-binding ollama --help

####################################################################
### WORKSPACE sets workspace name for all storage types
### for the purpose of isolating data from LightRAG instances.
### Valid workspace name constraints: a-z, A-Z, 0-9, and _
####################################################################
# WORKSPACE=space1

############################
### Data storage selection
############################
### Default storage (Recommended for small scale deployment)
# LIGHTRAG_KV_STORAGE=JsonKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
# LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

### Redis Storage (Recommended for production deployment)
# LIGHTRAG_KV_STORAGE=RedisKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=RedisDocStatusStorage

### Vector Storage (Recommended for production deployment)
# LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=FaissVectorDBStorage

### Graph Storage (Recommended for production deployment)
# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
# LIGHTRAG_GRAPH_STORAGE=MemgraphStorage

### PostgreSQL
# LIGHTRAG_KV_STORAGE=PGKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=PGGraphStorage
# LIGHTRAG_VECTOR_STORAGE=PGVectorStorage

### MongoDB (Vector storage only available on Atlas Cloud)
# LIGHTRAG_KV_STORAGE=MongoKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=MongoGraphStorage
# LIGHTRAG_VECTOR_STORAGE=MongoVectorDBStorage

### PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=your_username
POSTGRES_PASSWORD='your_password'
POSTGRES_DATABASE=your_database
POSTGRES_MAX_CONNECTIONS=12
# POSTGRES_WORKSPACE=forced_workspace_name

### PostgreSQL Vector Storage Configuration
### Vector storage type: HNSW, IVFFlat
POSTGRES_VECTOR_INDEX_TYPE=HNSW
POSTGRES_HNSW_M=16
POSTGRES_HNSW_EF=200
POSTGRES_IVFFLAT_LISTS=100

### PostgreSQL Connection Retry Configuration (Network Robustness)
### Number of retry attempts (1-10, default: 3)
### Initial retry backoff in seconds (0.1-5.0, default: 0.5)
### Maximum retry backoff in seconds (backoff-60.0, default: 5.0)
### Connection pool close timeout in seconds (1.0-30.0, default: 5.0)
# POSTGRES_CONNECTION_RETRIES=3
# POSTGRES_CONNECTION_RETRY_BACKOFF=0.5
# POSTGRES_CONNECTION_RETRY_BACKOFF_MAX=5.0
# POSTGRES_POOL_CLOSE_TIMEOUT=5.0

### PostgreSQL SSL Configuration (Optional)
# POSTGRES_SSL_MODE=require
# POSTGRES_SSL_CERT=/path/to/client-cert.pem
# POSTGRES_SSL_KEY=/path/to/client-key.pem
# POSTGRES_SSL_ROOT_CERT=/path/to/ca-cert.pem
# POSTGRES_SSL_CRL=/path/to/crl.pem

### PostgreSQL Server Settings (for Supabase Supavisor)
# Use this to pass extra options to the PostgreSQL connection string.
# For Supabase, you might need to set it like this:
# POSTGRES_SERVER_SETTINGS="options=reference%3D[project-ref]"

# Default is 100 set to 0 to disable
# POSTGRES_STATEMENT_CACHE_SIZE=100

### Neo4j Configuration
NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD='your_password'
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=30
NEO4J_MAX_TRANSACTION_RETRY_TIME=30
NEO4J_MAX_CONNECTION_LIFETIME=300
NEO4J_LIVENESS_CHECK_TIMEOUT=30
NEO4J_KEEP_ALIVE=true
# NEO4J_WORKSPACE=forced_workspace_name

### MongoDB Configuration
MONGO_URI=mongodb://root:root@localhost:27017/
[#MONGO](#MONGO)_URI=mongodb+srv://xxxx
MONGO_DATABASE=LightRAG
# MONGODB_WORKSPACE=forced_workspace_name

### Milvus Configuration
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=lightrag
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token
# MILVUS_WORKSPACE=forced_workspace_name

### Qdrant
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your-api-key
# QDRANT_WORKSPACE=forced_workspace_name

### Redis
REDIS_URI=redis://localhost:6379
REDIS_SOCKET_TIMEOUT=30
REDIS_CONNECT_TIMEOUT=10
REDIS_MAX_CONNECTIONS=100
REDIS_RETRY_ATTEMPTS=3
# REDIS_WORKSPACE=forced_workspace_name

### Memgraph Configuration
MEMGRAPH_URI=bolt://localhost:7687
MEMGRAPH_USERNAME=
MEMGRAPH_PASSWORD=
MEMGRAPH_DATABASE=memgraph
 # MEMGRAPH_WORKSPACE=forced_workspace_name</code></pre><p>参考前面的 Gemini 示例，写了下面的代码包含了一些硬编码文本的测试代码：</p><pre><code> # 准备环境
 python3 -m venv venv  
 source venv/bin/activate  
   
 pip install --upgrade pip  
 pip install "lightrag-hku[api]"  
 pip install ollama</code></pre><pre><code> import os  
import asyncio  
from functools import partial  
from datetime import datetime  
from lightrag import LightRAG, QueryParam  
try:  
    from ollama import AsyncClient  
except ImportError:  
    print("Warning: The 'ollama' Python package is required. Please run: pip install ollama")  
    class AsyncClient:   
        def __init__(self, host): pass  
        async def chat(self, **kwargs): raise NotImplementedError("ollama package not installed.")  

from lightrag.llm.ollama import ollama_embed   
from lightrag.utils import setup_logger, EmbeddingFunc  
from lightrag.kg.shared_storage import initialize_pipeline_status  

OLLAMA_BASE_URL = "http://localhost:11434"  
LLM_MODEL = "granite4:latest"  
EMBEDDING_MODEL = "granite-embedding:latest"  
WORKING_DIR = "./rag_storage_ollama"  
EMBEDDING_DIMENSION = 384   

OUTPUT_DIR = "./output"  

setup_logger("lightrag", level="INFO")  

if not os.path.exists(WORKING_DIR):  
    os.mkdir(WORKING_DIR)  

if not os.path.exists(OUTPUT_DIR):  
    os.mkdir(OUTPUT_DIR)  

async def custom_ollama_llm_complete(prompt: str, system_prompt: str = None, **kwargs):  
    """
    A custom function that handles the Ollama client initialization and model/base_url 
    parameters that are injected via functools.partial, while robustly filtering out 
    unwanted internal keywords.
    """
      
    model = kwargs.pop('model')  
    base_url = kwargs.pop('base_url')  
      
    client = AsyncClient(host=base_url)   
      
    messages = []  
    if system_prompt:  
        messages.append({"role": "system", "content": system_prompt})  
    messages.append({"role": "user", "content": prompt})  

    keys_to_filter = {  
        'host',   
        'hashing_kv',   
        'llm_model_name',   
        'history_messages',   
        'keyword_extraction',  
        'enable_cot',  
        'is_system_prompt_only',  
        'prompt_config'  
    }  
      
    cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in keys_to_filter}  

    try:  
        response = await client.chat(  
            model=model,   
            messages=messages,   
            **cleaned_kwargs  
        )  
        return response['message']['content']  
    except Exception as e:  
        raise e  

async def initialize_rag():  
    """Initializes the LightRAG instance using standard Ollama configuration."""

    configured_ollama_complete = partial(  
        custom_ollama_llm_complete,  
        model=LLM_MODEL,  
        base_url=OLLAMA_BASE_URL,  
    )  

    configured_ollama_embed = partial(  
        ollama_embed,  
        embed_model=EMBEDDING_MODEL,  
        base_url=OLLAMA_BASE_URL  
    )  

    wrapped_embedding_func = EmbeddingFunc(  
        embedding_dim=EMBEDDING_DIMENSION,   
        func=configured_ollama_embed,  
    )  
      
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        llm_model_func=configured_ollama_complete,  
        embedding_func=wrapped_embedding_func,  
    )  
      
    await rag.initialize_storages()  
    await initialize_pipeline_status()  
    return rag  

async def main():  
    rag = None   
    query = "How does RAG solve the problem of LLM hallucination and what are its main use cases?"  
      
    try:  
        print("Checking if required Ollama models are pulled...")  
          
        # the knowledge source
        sample_text = """  
        The concept of Retrieval-Augmented Generation (RAG) is a critical development  
        in the field of large language models (LLMs). It addresses the 'hallucination'  
        problem by grounding LLM responses in external, verified knowledge sources.  
        Instead of relying solely on the LLM's static training data, RAG first retrieves  
        relevant documents from a knowledge base (often a vector store) and then feeds  
        these documents, alongside the user's query, to the LLM for generation.  
        This two-step process significantly improves the accuracy, relevance, and  
        transparency of the generated output. Popular applications include enterprise  
        search, customer support, and domain-specific QA systems.  
        """  

        print(f"--- 1. Initializing RAG with Ollama Models ---")  
        rag = await initialize_rag()  
          
        print(f"\n--- 2. Inserting Sample Text ({len(sample_text.split())} words) ---")  
        await rag.ainsert(sample_text)  
        print("Insertion complete. Data is ready for retrieval.")  

        mode = "hybrid"   
          
        print(f"\n--- 3. Querying the RAG System (Mode: {mode}) ---")  
        print(f"Query: '{query}'")  

        rag_result = await rag.aquery(  
            query,  
            param=QueryParam(mode=mode)  
        )  
          
        response_text = None  
        if hasattr(rag_result, 'get_response_text'):  
            response_text = rag_result.get_response_text()  
        elif isinstance(rag_result, str):  
            response_text = rag_result  

        print("\n" + "="*50)  
        print("FINAL RAG RESPONSE")  
        print("="*50)  
          
        output_content = "" # Prepare string for file output
          
        if response_text and not str(response_text).strip().startswith("Error:"):  
            print(response_text)  
              
            output_content += f"# RAG Query Result\n\n"  
            output_content += f"## Query\n\n&gt; {query}\n\n"  
            output_content += f"## LLM/Cache Response\n\n{response_text}\n\n"  
              
            print("\n" + "="*50)  

            print("\n--- Context Retrieved (Sources) ---")  
            output_content += f"## Retrieved Context (Sources)\n\n"  
              
            if not isinstance(rag_result, str) and rag_result.retriever_output and rag_result.retriever_output.docs:  
                for i, doc in enumerate(rag_result.retriever_output.docs):  
                    source_text = doc.text  
                    print(f"Source {i+1}: {source_text[:100]}...")  
                    output_content += f"### Source {i+1}\n\n"  
                    output_content += f"```text\n{source_text}\n```\n"  
            else:  
                 print("No context documents were retrieved (or result was a cache hit string).")  
                 output_content += "No context documents were retrieved (or result was a cache hit string).\n"  
        else:  
             error_message = "LLM failed to generate a response (Check Ollama logs for details)."  
             print(error_message)  
             output_content += f"# RAG Query Result\n\n## Error\n\n{error_message}\n\n"  
               
             if response_text:  
                 print(f"\nError String from LightRAG: {response_text}")  
                 output_content += f"**Error Detail:** {response_text}\n"  

          
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
        filename = f"rag_query_output_{timestamp}.md"  
        output_filepath = os.path.join(OUTPUT_DIR, filename)  
          
        with open(output_filepath, 'w', encoding='utf-8') as f:  
            f.write(output_content)  
          
        print(f"\n--- Output Written to File ---")  
        print(f"Successfully wrote output to: {output_filepath}")  

          
    except Exception as e:  
        if "'str' object has no attribute 'retriever_output'" in str(e):  
            print("\n--- ERROR BYPASS: Detected Cache Hit String Result ---")  
             print("The response was successfully retrieved from the cache and written to the output file.")  
        else:  
            # For all other (real) exceptions, print the detailed error block
            print("\n" + "="*50)  
            print("AN ERROR OCCURRED DURING RAG PROCESS")  
            print("="*50)  
            print(f"Error: {e}")  
            print(f"Please ensure Ollama is running and accessible at {OLLAMA_BASE_URL}, and the models '{LLM_MODEL}' and '{EMBEDDING_MODEL}' are pulled locally.")  
            print(f"To pull: 'ollama pull {LLM_MODEL}' and 'ollama pull {EMBEDDING_MODEL}'")  
            print("="*50 + "\n")  
          
    finally:  
        if rag:  
            print("\n--- Finalizing storages ---")  
            await rag.finalize_storages()  

if __name__ == "__main__":  
     asyncio.run(main())</code></pre><p>虽然</p><pre><code>.env</code></pre><p>文件里有参数可以配置 input 文件夹路径，但测试时直接在代码里写死了路径。</p><p>运行后的输出包括控制台日志和 markdown 格式的结果文件，结果太长我就不贴了。</p><p>接下来测试了更实际的场景：准备了几份 markdown 格式的文档（其他格式应该也支持，但没测），用这些文档构建了自己的 RAG 系统，继续用 Ollama 和 Granite 模型来验证效果，这次的代码就没那么硬编码了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383689" alt="" title="" loading="lazy"/></p><blockquote><pre><code>.env</code></pre><p>文件提供了 input 文件夹的配置选项，不过这里还是用的硬编码方式</p></blockquote><pre><code> import os
import asyncio
from functools import partial
from datetime import datetime
from lightrag import LightRAG, QueryParam
import glob 

try:
    from ollama import AsyncClient
except ImportError:
    print("Warning: The 'ollama' Python package is required. Please run: pip install ollama")
    class AsyncClient: 
        def __init__(self, host): pass
        async def chat(self, **kwargs): raise NotImplementedError("ollama package not installed.")

from lightrag.llm.ollama import ollama_embed 
from lightrag.utils import setup_logger, EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

OLLAMA_BASE_URL = "http://localhost:11434"
LLM_MODEL = "granite4:latest"
EMBEDDING_MODEL = "granite-embedding:latest"
WORKING_DIR = "./rag_storage_ollama"
EMBEDDING_DIMENSION = 384 

DOCUMENTS_DIR = "./documents" # Directory to read source files from
OUTPUT_DIR = "./output" # Directory to write RAG results to

setup_logger("lightrag", level="INFO")

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)
    print(f"Created working directory: {WORKING_DIR}")

if not os.path.exists(OUTPUT_DIR):
    os.mkdir(OUTPUT_DIR)
    print(f"Created output directory: {OUTPUT_DIR}")

if not os.path.exists(DOCUMENTS_DIR):
    os.mkdir(DOCUMENTS_DIR)
    print(f"Created documents directory: {DOCUMENTS_DIR}")

async def custom_ollama_llm_complete(prompt: str, system_prompt: str = None, **kwargs):
    """
    A custom function that handles the Ollama client initialization and model/base_url 
    parameters that are injected via functools.partial, while robustly filtering out 
    unwanted internal keywords.
    """
    
    model = kwargs.pop('model')
    base_url = kwargs.pop('base_url')
    
    client = AsyncClient(host=base_url) 
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    keys_to_filter = {
        'host', 
        'hashing_kv', 
        'llm_model_name', 
        'history_messages', 
        'keyword_extraction',
        'enable_cot',
        'is_system_prompt_only',
        'prompt_config'
    }
    
    cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in keys_to_filter}

    try:
        response = await client.chat(
            model=model, 
            messages=messages, 
            **cleaned_kwargs
        )
        return response['message']['content']
    except Exception as e:
        raise e

async def initialize_rag():
    """Initializes the LightRAG instance using standard Ollama configuration."""

    configured_ollama_complete = partial(
        custom_ollama_llm_complete,
        model=LLM_MODEL,
        base_url=OLLAMA_BASE_URL,
    )

    configured_ollama_embed = partial(
        ollama_embed,
        embed_model=EMBEDDING_MODEL,
        base_url=OLLAMA_BASE_URL
    )

    wrapped_embedding_func = EmbeddingFunc(
        embedding_dim=EMBEDDING_DIMENSION, 
        func=configured_ollama_embed,
    )
    
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=configured_ollama_complete,
        embedding_func=wrapped_embedding_func,
    )
    
    await rag.initialize_storages()
    await initialize_pipeline_status()
    return rag

async def load_and_insert_documents(rag: LightRAG):
    """
    Reads files from the DOCUMENTS_DIR and inserts their content into the RAG system.
    Fixed to use a more compatible method for document insertion.
    """
    file_paths = glob.glob(os.path.join(DOCUMENTS_DIR, '*.[mM][dD]')) + \
                 glob.glob(os.path.join(DOCUMENTS_DIR, '*.[tT][xX][tT]'))
                 
    if not file_paths:
        print("\n--- WARNING: No documents found in './documents' directory. ---")
        print("Please add some Markdown (.md) or Text (.txt) files to populate the knowledge base.")
        return False
        
    print(f"\n--- 2. Inserting Documents ({len(file_paths)} file(s) found) ---")
    
    insertion_succeeded = 0
    
    for file_path in file_paths:
        filename = os.path.basename(file_path)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            await rag.ainsert(content, doc_meta={'doc_id': filename})
            
            print(f"  &gt; Successfully inserted: {filename} ({len(content.split())} words)")
            insertion_succeeded += 1
            
        except TypeError as te:
            if "'doc_id'" in str(te) or "'doc_meta'" in str(te):
                print(f"  &gt; FAILED (Type Error): {filename}. Attempting insertion without metadata to check compatibility.")
                try:
                    await rag.ainsert(content)
                    print(f"  &gt; Successfully inserted (no metadata): {filename}")
                    insertion_succeeded += 1
                except Exception as e:
                    print(f"  &gt; FAILED (General Error): {filename} - {e}")
            else:
                 print(f"  &gt; FAILED to read or insert {filename} (Type Error): {te}")
                 
        except Exception as e:
            print(f"  &gt; FAILED to read or insert {filename} (General Error): {e}")
            
    if insertion_succeeded == 0:
        print("Insertion complete, but no documents were successfully inserted. Please check LightRAG documentation for the correct argument name for source IDs.")
        return False
        
    print("Insertion complete. Data is ready for retrieval.")
    return True

async def main():
    rag = None 
    query = "Describe Quantum-Safe cryptography?"
    
    try:
        print("Checking if required Ollama models are pulled...")
        
        print(f"--- 1. Initializing RAG with Ollama Models ---")
        rag = await initialize_rag()
        
        documents_inserted = await load_and_insert_documents(rag)
        
        if not documents_inserted:
            return 

        mode = "hybrid" 
        
        print(f"\n--- 3. Querying the RAG System (Mode: {mode}) ---")
        print(f"Query: '{query}'")

        rag_result = await rag.aquery(
            query,
            param=QueryParam(mode=mode)
        )
        
        response_text = None
        if hasattr(rag_result, 'get_response_text'):
            response_text = rag_result.get_response_text()
        elif isinstance(rag_result, str):
            response_text = rag_result

        print("\n" + "="*50)
        print("FINAL RAG RESPONSE")
        print("="*50)
        
        output_content = "" # Prepare string for file output
        
        if response_text and not str(response_text).strip().startswith("Error:"):
            print(response_text)
            
            output_content += f"# RAG Query Result\n\n"
            output_content += f"## Query\n\n&gt; {query}\n\n"
            output_content += f"## LLM/Cache Response\n\n{response_text}\n\n"
            
            print("\n" + "="*50)

            print("\n--- Context Retrieved (Sources) ---")
            output_content += f"## Retrieved Context (Sources)\n\n"
            
            if not isinstance(rag_result, str) and rag_result.retriever_output and rag_result.retriever_output.docs:
                unique_sources = set()
                
                for i, doc in enumerate(rag_result.retriever_output.docs):
                    source_text = doc.text
                    source_id = doc.doc_id if hasattr(doc, 'doc_id') and doc.doc_id else (
                                doc.doc_meta.get('doc_id') if hasattr(doc, 'doc_meta') and isinstance(doc.doc_meta, dict) else 'Unknown Source'
                            )
                    unique_sources.add(source_id)
                    
                    print(f"Source {i+1} (File: {source_id}): {source_text[:100]}...")
                    output_content += f"### Source {i+1} (File: `{source_id}`)\n\n"
                    output_content += f"```text\n{source_text}\n```\n"
                
                print(f"\nAnswer Grounded in: {', '.join(sorted(list(unique_sources)))}")
            else:
                 print("No context documents were retrieved (or result was a cache hit string).")
                 output_content += "No context documents were retrieved (or result was a cache hit string).\n"
        else:
             error_message = "LLM failed to generate a response (Check Ollama logs for details)."
             print(error_message)
             output_content += f"# RAG Query Result\n\n## Error\n\n{error_message}\n\n"
             
             if response_text:
                 print(f"\nError String from LightRAG: {response_text}")
                 output_content += f"**Error Detail:** {response_text}\n"

        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_query_output_{timestamp}.md"
        output_filepath = os.path.join(OUTPUT_DIR, filename)
        
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(output_content)
        
        print(f"\n--- Output Written to File ---")
        print(f"Successfully wrote output to: {output_filepath}")
        

    except Exception as e:
        if "'str' object has no attribute 'retriever_output'" in str(e):
             print("\n--- ERROR BYPASS: Detected Cache Hit String Result ---")
             print("The response was successfully retrieved from the cache and written to the output file.")
        else:
            print("\n" + "="*50)
            print("AN ERROR OCCURRED DURING RAG PROCESS")
            print("="*50)
            print(f"Error: {e}")
            print(f"Please ensure Ollama is running and accessible at {OLLAMA_BASE_URL}, and the models '{LLM_MODEL}' and '{EMBEDDING_MODEL}' are pulled locally.")
            print(f"To pull: 'ollama pull {LLM_MODEL}' and 'ollama pull {EMBEDDING_MODEL}'")
            print("="*50 + "\n")
        
    finally:
        if rag:
            print("\n--- Finalizing storages ---")
            await rag.finalize_storages()

if __name__ == "__main__":
     asyncio.run(main())</code></pre><p>实际的查询输出示例：</p><pre><code>  RAG Query Result

## Query

&gt; Describe Quantum-Safe cryptography?

## LLM/Cache Response

### What is Quantum-Safe Cryptography?

 Quantum-safe cryptography, also known as post‑quantum cryptography (PQC), refers to cryptographic algorithms and protocols designed to remain secure even against attacks by a sufficiently powerful quantum computer. Traditional public-key cryptosystems like RSA, ECC, Diffie‑Hellman, and elliptic curve variants are vulnerable to Shor’s algorithm, which could efficiently factor large integers and compute discrete logarithms—tasks that form the basis of these cryptographic schemes.</code></pre><h2>知识图谱生成</h2><p>LightRAG 自带知识图谱生成功能，这点比较实用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383690" alt="" title="" loading="lazy"/><br/>为了把生成的图谱文件展示出来，我还用用 Streamlit 写了个简单的可视化应用。代码比较粗糙，后续可以继续优化。</p><pre><code>import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import io
import itertools
from io import StringIO

def visualize_graph(G: nx.Graph, layout_name: str, layout_params: dict):
    """
    Generates a Matplotlib plot of the NetworkX graph with custom styling.
    """

    node_labels = nx.get_node_attributes(G, 'label')
    if not node_labels:
        node_labels = nx.get_node_attributes(G, 'text')

    edge_labels = nx.get_edge_attributes(G, 'text')
    node_types = nx.get_node_attributes(G, 'type')

    type_color_map = {
        'Entity': '[#1f78b4](#1f78b4)',    # Blue
        'Chunk': '[#b2df8a](#b2df8a)',     # Light Green
        'Relation': '[#33a02c](#33a02c)',  # Dark Green 
        'Unknown': '[#a6cee3](#a6cee3)'    # Light Blue
    }
    node_colors = [type_color_map.get(node_types.get(node, 'Unknown'), type_color_map['Unknown']) for node in G.nodes()]

    if layout_name == 'Spring Layout':
        pos = nx.spring_layout(G, **layout_params)
    elif layout_name == 'Circular Layout':
        pos = nx.circular_layout(G)
    elif layout_name == 'Spectral Layout':
        pos = nx.spectral_layout(G)
    elif layout_name == 'Kamada-Kawai Layout':
        pos = nx.kamada_kawai_layout(G)
    else:
        pos = nx.spring_layout(G, **layout_params)

    fig, ax = plt.subplots(figsize=(16, 10))

    nx.draw_networkx_nodes(
        G, 
        pos, 
        node_size=2500, 
        node_color=node_colors, 
        alpha=0.9
    )

    # Draw edges
    nx.draw_networkx_edges(
        G, 
        pos, 
        ax=ax,
        edge_color='gray', 
        style='dashed', 
        arrowstyle='-&gt;', 
        arrowsize=25
    )

    nx.draw_networkx_labels(
        G, 
        pos, 
        labels=node_labels, 
        font_size=11, 
        font_color='black',
        font_weight='bold',
    )

    nx.draw_networkx_edge_labels(
        G, 
        pos, 
        edge_labels=edge_labels, 
        font_color='red', 
        font_size=9,
        bbox={"boxstyle": "round,pad=0.4", "fc": "white", "alpha": 0.7, "ec": "none"}
    )

    ax.set_title(f"Visualized Graph: {G.number_of_nodes()} Nodes, {G.number_of_edges()} Edges", fontsize=16)
    plt.axis('off')
    plt.tight_layout()

    st.pyplot(fig)

def app():
    st.set_page_config(layout="wide", page_title="GraphML Viewer")
    st.title("GraphML Visualization App")
    st.markdown("A tool to visualize GraphML (e.g., LightRAG) outputs using NetworkX and Streamlit.")

    st.sidebar.header("Data Upload &amp; Layout Controls")

    uploaded_file = st.sidebar.file_uploader(
        "Upload your .graphml file", 
        type=["graphml"]
    )

    graph_data = None

    if uploaded_file is not None:
        try:
            graph_data = uploaded_file.read().decode("utf-8")
            st.sidebar.success("File uploaded successfully! Graph loading...")
        except Exception as e:
            st.sidebar.error(f"Error reading file: {e}")
            graph_data = None
    else:
        st.info("Please upload a GraphML file in the sidebar to visualize your knowledge graph.")

    st.sidebar.subheader("Layout Algorithm")
    layout_name = st.sidebar.selectbox(
        "Choose Graph Layout:",
        ('Spring Layout', 'Kamada-Kawai Layout', 'Circular Layout', 'Spectral Layout')
    )

    layout_params = {}
    if layout_name == 'Spring Layout':
        st.sidebar.caption("Fine-tune the Spring Layout forces:")
        k_val = st.sidebar.slider("k (Node Spacing)", 0.01, 1.0, 0.15, 0.01)
        iters = st.sidebar.slider("Iterations", 10, 100, 50, 10)
        layout_params = {'k': k_val, 'iterations': iters}

    if graph_data:
        try:
            G = nx.read_graphml(StringIO(graph_data))

            st.header("Knowledge Graph Visualization")
            st.write(f"Graph loaded: {G.number_of_nodes()} Nodes, {G.number_of_edges()} Edges")

            visualize_graph(G, layout_name, layout_params)

        except Exception as e:
            st.error(f"An error occurred while processing the graph: {e}")
            st.code(f"Error details: {e}")
            st.warning("Please check if the GraphML file is correctly formatted and contains valid data.")

if __name__ == '__main__':
    app()</code></pre><p>运行以后你就能看到上面的结果</p><h2>总结</h2><p>LightRAG 在构建 RAG 系统方面提供了相对完整的解决方案。相比传统的纯向量检索，它的核心特点是引入了知识图谱，能把非结构化文本组织成实体-关系网络，这种混合检索策略（语义向量+图谱关系）确实能让 LLM 获得更丰富的上下文信息。</p><p>项目文档写得很详细，并且包含了大量工具和服务的集成方案。整个框架的设计比较模块化，扩展性也不错。如果要在生产环境部署，还需要考虑性能优化、错误处理等细节。</p><p>LightRAG 项目地址：<a href="https://link.segmentfault.com/?enc=LVbbyGCyf1vsyynTGvvPKw%3D%3D.QzWBik7pLJ%2FuuDwzwzTYclZUutZiYLWJVwG5lkaNmDOkWZo2%2BsgzMXPf9jnWP5dC" rel="nofollow" target="_blank">https://github.com/HKUDS/LightRAG</a></p><p>作者：Alain Airom</p>]]></description></item><item>    <title><![CDATA[Monad Blitz 黑客松巡回之旅 ]]></title>    <link>https://segmentfault.com/a/1190000047383602</link>    <guid>https://segmentfault.com/a/1190000047383602</guid>    <pubDate>2025-11-09 21:05:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>Monad Blitz 是一场紧张刺激、充满乐趣、为期 1 天的线下黑客马拉松</strong>。Monad Blitz 已经在<strong>深圳、杭州、北京和成都</strong>成功举办，每一站都吸引了大量开发者参与，现场气氛热烈，作品精彩纷呈。活动不仅展示了 Monad 并行 EVM 的技术潜力，也收获了开发者们的热烈响应与积极反馈，逐渐成为 Monad 生态 Builder 们最受瞩目的创意舞台。</p><p>Monad Blitz 的旅程不会停下，精彩还在继续！<strong>上海站</strong>、<strong>大理站和长沙站</strong>也重磅来袭！</p><p>无论你身处哪里，都有机会加入这场并行 EVM 的创新盛宴，和一群热血 Builder 面对面碰撞灵感、快速动手、结识伙伴，一起体验 Monad 的无限可能！</p><p><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdmYNN" alt="image.png" title="image.png"/></p><p>如果你还没有 Web3 或链上开发经验，不用担心！Monad Blitz 非常适合想要尝试新技术、拓展技能边界的 Web2 开发者。</p><p>在这里，你可以：</p><ul><li>通过了解当下最火热、最具潜力的公链之一 —— <strong>Monad</strong>，加深对整个行业的认知，获得前沿视角</li><li>亲手体验智能合约和链上应用的开发流程，快速入门并理解核心概念</li><li>认识一群对技术同样充满热情的 Web3 Builder，获得一线实战指导</li><li>通过「边做边学」的方式，打破对区块链复杂性的认知门槛，现场氛围轻松、互帮互学</li><li>为自己的技术履历增加一个亮眼的链上经历，打开全新职业发展方向</li></ul><p>不管你是做前端、后端还是全栈，只要对 Web3 感兴趣，都可以来尝试！欢迎所有 Web2 开发者加入，一起快速上手，一起玩，一起成长 🚀</p><h3>Monad Blitz 上海站 · 11月29日</h3><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNM" alt="image.png" title="image.png" loading="lazy"/><br/><strong>Monad × X402</strong><br/>我们鼓励开发者积极探索与 X402 协议相结合的创新方向，利用 Monad 的高性能并行处理能力，在支付与金融应用领域开拓新的想象空间。这将是您项目的一个独特亮点！</p><h3><strong>奖金（1500 美元）</strong></h3><p><strong>🥇 第 1 名：</strong> 600 美元</p><p><strong>🥈 第 2 名：</strong> 500 美元</p><p><strong>🥉 第 3 名：</strong> 400 美元</p><p><strong>前5名每个荣获专属定制 Monad 机械键盘一把！</strong></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmCDU" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>投票采用官方和开发者共同投票形式，Monad Foundation 对规则有最终解释权。</strong></p><p>为了确保大家的体验，我们将进行一定的筛选，请认真填写申请信息。支持线上提前或线下现场自由组队，每支队伍最多 3 名成员。</p><h3>立即报名</h3><p>日期：2025年11月29日</p><p>时间：9:00 - 21:00</p><p>地点：上海（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=kU5NQgWB%2B4WtffDpq2vlkA%3D%3D.BwzdXNKTpOUZ8Y0lnTK3sNMab37HArxtRngN57KzA%2BQ%3D" rel="nofollow" target="_blank">https://luma.com/7zug8z13</a></p><h3>下一波活动已在路上……</h3><h3>📍 Monad Blitz 大理站预告 · 12月20日</h3><p>在苍山洱海畔，我们将延续「快速学习+实战冲刺+深度交流」的核心氛围。结合更多本地及全国的开发者资源，打造更丰富、更具影响力的链上聚会。不论你是想深入研究并行EVM，还是想结识志同道合的Builder，一起组队、一起秀操作，这里都是你的绝佳选择！</p><h3>立即报名</h3><p>日期：2025年12月20日</p><p>时间：9:00 - 21:00</p><p>地点：大理（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=5E2z0l%2BcREw9oKAX0sccEQ%3D%3D.rMIb5kkwxJbjW0Go1uDLh2UCPxoD29HST9nGbin0US0%3D" rel="nofollow" target="_blank">https://luma.com/j7862g95</a></p><h3>📍 Monad Blitz 长沙站预告· 2026年1月17日</h3><p>长沙，这座活力新星城市，正呼唤着下一个明星项目的诞生！本站黑客松将深度聚焦于团队碰撞与协作。无论你是智能合约老手还是前端新锐，都能在这里找到并肩作战的伙伴。让我们在湘江畔，将天马行空的创意转化为可运行的代码，携手打造出属于下一个时代的链上应用。</p><h3>立即报名</h3><p>日期：2026年1月17日</p><p>时间：9:00 - 21:00</p><p>地点：长沙（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=3VU79cLUS404vESoG50j1A%3D%3D.8GomB7XHS%2FAy%2BG2kv4Dgc4Y74RueZ2FI8qqvosX8QkA%3D" rel="nofollow" target="_blank">https://luma.com/b7zggun9</a></p><h3>活动内容</h3><p><strong>体验并行的力量</strong> 在上午的分享环节，跟 Monad DevRel 面对面，现场学习最前沿的并行执行技术，动手把你的第一个合约部署到 Monad Testnet，真正感受高性能链的速度与魅力。</p><p><strong>下午直接开搞，快速上手</strong> 下午就是一场高强度「头脑风暴 + 编码实战」突击赛。无论是做出一个小原型，还是验证一个新想法，这里更看重动手、试错和创意，边学边做，超有成就感。</p><p><strong>秀出你的实力</strong> 我们正在寻找敢想敢做、充满活力的开发者！用你的代码实力、快速学习力和热情给大家留下印象，这就是你闪光的机会。</p><p><strong>找到你的「同路人」</strong> 和一群同样热爱创新、热爱链上世界的 Builder 面对面交流，聊技术、聊创意、一起组队搞项目。现场管吃管喝，氛围轻松有趣，绝对会收获一堆新朋友！</p><h3>活动议程</h3><p>09:00 - 09:30｜签到 &amp; 早餐</p><p>09:30 - 10:15｜Monad 101</p><p>10:15 - 11:00｜Workshop</p><p>11:00 - 12:00｜午餐 &amp; 组队</p><p>12:00 - 18:30｜Hack/Vibe-coding</p><p>18:30 - 19:00｜项目提交截止 &amp; 晚餐</p><p>19:00 - 20:00｜项目展示 / Demos</p><p>20:00 - 20:20｜自由交流 &amp; 投票</p><p>20:20 - 20:45｜颁奖 &amp; 总结致辞</p><p>20:45 - 21:00｜自由交流 &amp; 活动结束</p><h3>现场好礼</h3><p>本次活动为大家特别准备了专属定制的 Monad 和 OpenBuild 周边好礼！从实用的 T 恤到可爱的公仔，更有时尚手链和新颖贴纸等你带回家！</p><p><img width="723" height="291" referrerpolicy="no-referrer" src="/img/bVdjGSf" alt="image.png" title="image.png" loading="lazy"/></p><h3>关于 Monad</h3><p>Monad 是一个去中心化、以开发者为中心的 Layer 1 区块链，通过并行执行和超标量管道技术引入了新的可能性。它旨在解决现有区块链的可扩展性问题，能够实现每秒高达 10,000 笔交易，同时保持与 EVM 的兼容性。 Monad 提供了完整的开发工具链，降低了开发者的技术门槛，为其生态系统的蓬勃发展奠定了坚实基础。</p>]]></description></item><item>    <title><![CDATA[Web3 开发者周刊 75 | ESP ]]></title>    <link>https://segmentfault.com/a/1190000047383613</link>    <guid>https://segmentfault.com/a/1190000047383613</guid>    <pubDate>2025-11-09 21:04:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNX" alt="image.png" title="image.png"/></p><p><strong>欢迎回到 Web3 开发者周刊第 75 期！</strong></p><p>本期周刊内所有黑客松活动、新闻和赏金任务，请大家点击查看原文以获取完整信息。如果您喜欢我们的内容，也欢迎大家订阅 <strong>OpenBuild Substack</strong>，获取最新最全开发者资讯！</p><p>本周我们将聚焦重启后的 ESP 新资助计划，新资助计划涵盖了密码学、隐私保护、应用层、安全、社区发展等关键领域。从隐私、价格飙升和未来走向几个方面探讨 Zcash 的复兴，以及关注 Mantle zk 化升级后正式进入 RWA 战略发力期。</p><p>除此之外，我们还为开发者整理了最新的黑客松资讯与赏金任务。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmYMt" alt="image.png" title="image.png" loading="lazy"/><br/><strong>✅ Monad Blitz @Shanghai</strong></p><p>📅 时间：11月29日</p><p>📍 上海</p><p>💸 奖金：1500 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=F9TKRe0kLbvvGoCszGrAsw%3D%3D.2yLmhxYN5jcBZX5AfAAESjHllkh%2FVnUduo6lx9gjRno%3D" rel="nofollow" target="_blank">https://luma.com/7zug8z13</a></p><p><strong>简介：</strong></p><p>Monad Blitz 的旅程即将登陆上海！挑战一日冲刺，探索 Monad x X402 新前沿！</p><p>Monad 鼓励开发者积极探索与 X402 协议相结合的创新方向，利用 Monad 的高性能并行处理能力，在支付与金融应用领域开拓新的想象空间。这将是您项目的一个独特亮点！</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNM" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Calling For All Agents! Hackathon</strong></p><p>📅 时间：10月27日 - 11月29日</p><p>📍 线上</p><p>💸 奖金：50,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=u545SIMd0xfsPhHUp%2BfLbA%3D%3D.4DjdUcEdb1OO4PkCk33RRem51jqicGhpYPVl7VI93GJTnIiV1utw9V0A32hTtOzgV1ag0RtmjMhYYtVoJGoaog%3D%3D" rel="nofollow" target="_blank">https://dorahacks.io/hackathon/calling-for-all-agents-sf/detail</a></p><p><strong>简介：</strong></p><p>Calling For All Agents! Hackathon 是 Verisense 发起的 AI 代理黑客松，旨在构建自主人工智能代理，突破大型语言模型的极限，并利用 MCPs** 和 RAGs 等关键工具，迈向智能的新高度。没有规则，没有界限！只有智能代理和创造力，让大胆的想法付诸实践！</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNY" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Mantle Global Hackathon 2025</strong></p><p>📅 时间：2025年10月22日 - 2026年2月7日</p><p>📍 线上</p><p>💸 奖金：150,000 美元 </p><p>🔗 链接：  </p><p><a href="https://link.segmentfault.com/?enc=qvhBI08j%2Fv3R9NME0DAGeA%3D%3D.dtXvD%2BzYLv2wZoW%2BV3Jag2gdFC%2FJTT3i5xzRzcgJbYpQ7ytzj5AVOkA%2F7y62hWLoMgd1U5JMnWcQtTn76trqMn7SxYjMeIz0JGTYb8AKBZg%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a>  </p><p><strong>简介：</strong></p><p>Mantle Global Hackathon 2025 是 Mantle 完成 zk 升级后首次尝试。通过此次全球黑客松活动，Mantle 希望激发全球开发者社区的潜力，共同推进 Mantle 生态的全面落地。了解更多：<a href="https://link.segmentfault.com/?enc=B758c2Wh2xZzCR2chKCs4Q%3D%3D.0jiDamudY0BYNczRL8tSKj%2B11ZXteqM3ndTf%2B2QWcaBNIvTzNHtmuHhWb5Pj1E1k8hfJPGmGVMM7UOmTQFcV2ihp5Fr9iYNDEeSDfMFBA6rej9nKiDXfUsFbRZFgvsjvn6OS2hmBONaGJaCmBZFI7%2Biti6aj1XEAKbC17qXjrHgIMtDpgTCI0RFMbV5DALL0" rel="nofollow" target="_blank">全球开发者集结！Mantle 全球黑客松燃动开启！共创 Web3 生态新高度！</a><br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMu" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="162" referrerpolicy="no-referrer" src="/img/bVdmYMx" alt="image.png" title="image.png" loading="lazy"/><br/><strong>📖 ESP 的新资助计划公布 11/3</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=btEq2ervN%2Fn4esQJTsJBeQ%3D%3D.SRb5lZgD3QBTNI3nvIo0%2FFKSTJjXCCrT%2FMBNmmLmuKtAgpmW%2BaRtllVfzjsN7zeS9XW87oYlDpAS57jEdrUIIg%3D%3D" rel="nofollow" target="_blank">https://blog.ethereum.org/2025/11/03/new-esp-grants</a></p><p>ESP 的首批愿望清单和征案邀请现已上线！涵盖了密码学、隐私保护、应用层、安全、社区发展等关键领域，为开发者提供了众多创造影响力的机会。</p><p><strong>📖 Zcash 的复兴：隐私、价格飙升和未来走向 11/4</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=ovWrlX6Yz9L7%2BqOAYbc5wQ%3D%3D.ApcB%2BOHk2fqu7qx%2F7l6jU82s5qNNF3sd7s6UEGB2OZymZPkKMR507tk%2B9Y5SZvZ93wvv0N4am%2FPT9gcWhGJXH4FmLRcS83os7W1hAoE5ONuDuobfWvjcB0lpAfcrSdZCGmPpDMi4VHRfUZvYjR3VU2kvyKDxJC6KZz8XmQQYxbc%3D" rel="nofollow" target="_blank">https://www.galaxy.com/insights/research/zcash-price-zec-near...</a></p><p>过去几周，隐私话题再次兴起。Zcash (ZEC**) 作为历史最悠久、最知名的隐私币之一，自 9 月以来涨幅高达 700%，仿佛加密货币领域人人都是隐私专家。</p><p><strong>📖 Mantle 完成 「ZK 化」革命：让现实世界的信任逻辑，在链上实现重新定义 11/6</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=9xr9r%2F7Zhc%2F1dldTxjh4wA%3D%3D.LlvDnGpmwKQzYcOuRLjZhQkWYGR%2FAbLLca8ZdBiOEoE%2Fe98hWixaQdfCjOkmpe%2Bo" rel="nofollow" target="_blank">https://foresightnews.pro/article/detail/91763</a>  </p><p>「当潮水退去时，才知道谁在裸泳」。</p><p>RWA 正在成为加密世界的新秩序之镜。它不仅折射出资本回流与机构入场的方向，也会揭示哪里才能成为真正承载 “现实世界的金融信任”。 而 Mantle， 正在成为下一阶段 RWA 基础设施竞争的关键变量。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKOQ" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>💸 zkVerify   11/4</strong></p><p>最高赏金 50,000 美元 </p><p>zkVerify允许模块化区块链网络将计算密集且成本高昂的证明验证过程分流出去，使它们能够专注于自身的主要功能，并在竞争中保持领先地位。</p><p>这种模块化方法不仅简化了操作，还显著提高了整体网络效率。</p><p><strong>💸 Xterio 11/6</strong></p><p>最高赏金 80,000 美元 </p><p>Xterio 是一个 Web3 游戏生态系统与基础设施，作为一家拥有顶尖开发技能和卓越发行专长的游戏发行商而脱颖而出。Xterio 怀揣着一个大胆的愿景——借助数字收藏品的力量重塑娱乐和游戏行业。这些非同质化资产拥有重新定义我们体验乐趣方式的能力。</p><p><strong>💸 Origin Protocol 11/4</strong></p><p>最高赏金 1,000,000 美元</p><p>Origin Protocol是一套互补的 DeFi 产品，旨在为所有人增加经济机会。这些无需许可且可组合的智能合约，在具有开创性的多链收益生态系统中，为整个去中心化金融领域提供卓越的用户体验。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKO4" alt="image.png" title="image.png" loading="lazy"/><br/>OpenBuild 是一个面向 Web3 开发人员的开源社区和平台。我们的目标是将更多的 Web2 开发人员带入 Web3 领域，同时帮助现有的 Web3 开发人员更好地构建并通过我们的产品取得商业成功！</p><p>欢迎在更多平台上关注我们：</p><p><a href="https://link.segmentfault.com/?enc=HOZwKkSP6Xae4Spjx1FZAw%3D%3D.YXwTtxg06gbkA%2B7eETaoespU09fZ3Wm9ZLcFtEZkYTQ%3D" rel="nofollow" target="_blank">https://linktr.ee/openbuild</a> 🙌🙌</p>]]></description></item><item>    <title><![CDATA[全球开发者集结！Mantle 全球黑客松]]></title>    <link>https://segmentfault.com/a/1190000047383618</link>    <guid>https://segmentfault.com/a/1190000047383618</guid>    <pubDate>2025-11-09 21:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025 年 9 月，Mantle 完成历史性跨越：主网从 Optimistic Rollup 全面升级为全球 TVL 规模最大的 ZK Rollup 网络，交易终局性缩短至 1 小时，吞吐量突破 3000 TPS。</p><p>在 TOKEN2049 的 Mantle Mixer 活动中，Mantle 宣布将进军现实世界资产（RWA）领域 —— 这是传统金融与代币化交汇的赛道，也是真实资产在链上获取真实收益的领域。</p><p>这些突破标志着 Mantle 新篇章的开启：让 Mantle 具备了承载大规模金融应用与日常场景的技术底座，但生态的增长和繁荣更需全球开发者的共创力量。</p><p>为此，Mantle 联合 HackQuest、OpenBuild 正式启动 Mantle Global Hackathon 2025: “Real Assets, Real Yield” Hackathon，一场面向全球开发者和区块链爱好者的线上黑客松。诚邀全球开发者将技术潜力转化为生态现实，把 Mantle 升级后的 ZK Rollup 打造为连接加密货币与传统金融的桥梁。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmYN3" alt="image.png" title="image.png"/></p><p>本次 Mantle Global Hackathon 2025: “Real Assets, Real Yield” Hackathon 共设立六大赛道，分别是 RWA/RealFi、DeFi &amp; Composability、AI &amp; Oracles、ZK &amp; Privacy、Infra &amp; Tooling、GameFi &amp; Social。</p><p>所有参赛队伍将在未来三个月内对 $150,000 总奖金池展开角逐，用自己的创造力定义去中心化金融的下一个时代。</p><h3>为什么要参加</h3><p>高额奖金激励：总额 150,000 美元的奖池，等待优秀的项目和团队来赢取。<br/>深度生态支持：获胜项目有机会获得 Mantle 生态基金的长期资金和孵化支持，同时接触潜在投资者与生态合作伙伴。<br/>全球社区曝光：参赛项目将获得在 Mantle、HackQuest 及 OpenBuild 等合作社区平台的联合宣传曝光机会。<br/>个人成长路径：参与系列 Workshop、获得专家和导师指导，以及与顶尖开发者、创始人及投资者进行交流的机会。</p><h3>Hackathon 时间</h3><p>注册报名：2025.10.22 - 2025.12.31<br/>项目提交：2025.10.22 - 2026.1.15<br/>初审投票：2026.1.15 - 2026.1.31<br/>Demo day：2026.2.1<br/>结果公示：2026.2.7</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQGa" alt="image.png" title="image.png" loading="lazy"/></p><h3>🎯参与方式</h3><p>报名注册：<a href="https://link.segmentfault.com/?enc=mCEBVJHXdFK4GcpA4vM7yw%3D%3D.asPiL9wCtwtltGhxgduQuZn%2BDv6U2BWmd0V7dgri0zuzphsb7JLFOdQACuNsi5FBF8ic4Uto%2BGG8nYteanjqUjW6tTbju3ctBN2CKxqKzJk%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a><br/>组队交流：加入 OpenBuild 开发者交流群，寻找志同道合的伙伴，获取最新资讯。 </p><p>项目提交：每支队伍在提交项目时需准备以下材料<br/>GitHub 代码仓库及部署说明<br/>3-5 分钟演示视频或测试网链接<br/>团队简介（2–5 人）与联系方式<br/>合规声明（RWA 相关项目需特别提供）<br/>一页纸白皮书（说明问题、解决方案及 Roadmap）</p><h3>奖金和赛道</h3><p>Mantle Build 2025 — “Real Assets, Real Yield” Hackathon 总奖金池高达$150,000。<br/>每个赛道设独立奖池，总金额 9 万美元，其中：<br/>第一名：$8,000<br/>第二名：$5,000<br/>第三名：$2,000</p><h4>特别奖项：</h4><p>总体冠军：$30,000<br/>社区票选奖（Top 3）：$6,000<br/>最佳 mantle 应用/集成奖：$4,000<br/>最佳演示与体验奖：$5,000<br/>快速孵化补助（3队）：$5,000/队</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQF2" alt="image.png" title="image.png" loading="lazy"/></p><h4>赛道：</h4><p>六大赛道等你挑战：RWA/RealFi、DeFi &amp; Composability、AI &amp; Oracles、ZK &amp; Privacy、Infra &amp; Tooling、GameFi &amp; Social。</p><p>RWA / RealFi（优先级最高）：代币化不动产、债券、票据、现金流产品的示范性应用（KYC/合规演示、托管结构、收益分配模型）。<br/>DeFi &amp; Composability：利率协议、抵押策略、组合器、基于 RWA 的合成资产。<br/>AI &amp; Oracles（AI Agents / 数据 + 预言机）：链上/链下数据驱动的智能策略、自动化资产管理（结合或acles 与 LLM/agent）。<br/>ZK &amp; Privacy：合规前提下的隐私方案（选择性披露、ZK-KYC、证明收益来源等）。<br/>Infrastructure &amp; Tooling：钱包插件、开发者 SDK、节点/监控、测试工具、自动化上链流水线。<br/>GameFi &amp; Social：利用 RWA 或收益机制的消费层场景、用户留存与代币经济设计。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQFP" alt="image.png" title="image.png" loading="lazy"/></p><h3>谁可以参加</h3><p>开发者、设计师、产品经理和区块链爱好者均可参加。<br/>无论是初学者还是资深专家，只要对 Mantle 生态和区块链技术充满热情，我们都欢迎你报名。<br/>支持单人参赛，也可以组队参与（2-5 人）。我们看重的是你的好奇心、创造力，以及推动 Mantle 生态落地 “真实价值” 的决心。</p><h3>评选标准和评委导师</h3><h4>评选标准</h4><p>项目将根据技术实现 、 产品与用户体验 、市场与落地可能性 、对 Mantle 的价值 / 集成度和影响力与可扩展性五个核心维度进行评估。具体如下：<br/>技术实现（40%）：系统完整性、代码质量、部署情况、漏洞基本检测。<br/>产品与用户体验（20%）：易用性、Demo 流畅度、上手成本。<br/>市场与落地可能性（20%）：商业模式、合规路径、目标用户与市场容量。<br/>对 Mantle 的价值 / 集成度（10%）：是否优先使用 Mantle 特性（低费、EVM 兼容、跨链桥）。<br/>影响力与可扩展性（10%）：可复制性、网络效应、后续发展空间。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQF9" alt="image.png" title="image.png" loading="lazy"/></p><h4>评委导师</h4><p>你的项目将由 Mantle 及整个 Web3 生态的顶尖领导者团队进行评审与指导，包括：<br/>Mantle 核心团队（CTO、生态负责人、技术主管）<br/>Bybit / 重要平台代表（投资、产品部门）<br/>外部技术专家（ZK、基础设施、共识机制领域专家）<br/>合规/法律专家（专注 RWA 赛道）<br/>投资人 / VC（后续融资与孵化）<br/>社区代表 / KOL（社区投票审核）</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQGd" alt="image.png" title="image.png" loading="lazy"/></p><h3>技术支持和参赛资源</h3><p>Mantle 将提供构建、部署及扩展项目所需的全部资源，确保你信心十足地推进开发：</p><p>测试网 / RPC 额度：提供 Gas 费、测试币及 RPC 访问权限（基于配额）<br/>入门工具包：预制示例，包括 RWA 合约模板、USDC 收益分配方案、KYC 白名单演示及 Mantle SDK。<br/>Workshop 资料：包含 SPV（简单支付验证）+ 托管合规架构、部署指南、跨链集成示例<br/>审计支持：获胜团队可通过 Mantle 合作安全机构获得快速通道或折扣审计服务，帮助项目后续落地。</p><p>X：<a href="https://link.segmentfault.com/?enc=gz5ICAk1LxK%2F4excjz9J9g%3D%3D.PfuNQLIhdX351RUfRLEwWn39H4hIw6Kw02TiAiSnGu8%3D" rel="nofollow" target="_blank">https://x.com/Mantle_Official</a><br/>Workshop / AMA 信息：即将公布，敬请期待！</p><h3>期待你的加入</h3><p>未来属于那些敢于用代码绘制愿景的人。</p><p>Mantle 正站在链上新时代的前沿，当下正是你用创意与实践定义下一代链上基础设施的机会。无论你是新手还是资深开发者，只要有想法、有勇气，这次黑客松就是你将想象转化为创新、将创新转化为影响力的机会，同时也是进入 Mantle 生态、获得社区支持和项目曝光的跳板。</p><p>加入我们，与 Mantle 一起，将代码变成现实，打造下一代去中心化基础设施，共筑 Mantle 生态的下一篇章！</p><h3>关于主办方</h3><p>Mantle Network 是一个模块化的以太坊二层网络，专为高性能和高性价比而打造。<br/>通过将执行层、共识层和数据可用性层进行解耦，Mantle 为开发者带来了以下优势：<br/>🚀 超低 Gas 费<br/>⚡ 高吞吐性能<br/>🔧 原生兼容 EVM<br/>🧰 内置开发工具：Mantle DA、Mantle SDK、跨链桥以及测试网</p><p>无论是 DeFi、RealFi 还是其他场景，Mantle 都是构建大规模应用的理想选择。</p><p>HackQuest 是一个一站式、自主导向的 Web3 开发者教育平台。HackQuest 提供由专家策划的学习路径，并与包括 Solana、Mantle Network、Arbitrum 和 Linea 等领先的 Web3 生态系统共同发行链上证书。社区建设者还可以通过共同学习营、聚会、黑客马拉松、加速器和启动平台服务获得进一步支持。</p><p>OpenBuild是一个面向 Web3 开发者的开源社区。致力于为开发者提供高质量的系统性内容和活动，同时连接 Web2 和 Web3，帮助开发者过渡到去中心化的网络，并通过提供必要的工具和资源，帮助开发者建立声誉体系，构建信任，创造商业机会。</p>]]></description></item><item>    <title><![CDATA[企业邮箱有免费吗？免费申请全指南 遭老罪]]></title>    <link>https://segmentfault.com/a/1190000047383633</link>    <guid>https://segmentfault.com/a/1190000047383633</guid>    <pubDate>2025-11-09 21:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>“企业邮箱有免费的吗？”——答案是：有，而且1人也能开。Zoho邮箱提供永久免费版（5用户内），自带域名后缀、SSL加密与反垃圾，3分钟完成注册即可收发全球邮件。下文以Zoho邮箱为例，拆解试用政策、功能上限与升级路径，手把手教你零成本拥有专业企业邮箱。<br/><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdmYLZ" alt="" title=""/> <br/>一、主流免费试用企业邮箱介绍：以Zoho邮箱为例</p><ol><li>Zoho邮箱的试用政策<br/>Zoho邮箱允许团队自定义域名接入，具备基本邮件收发、反垃圾邮件、日程管理等功能配置。根据公开数据，目前Zoho邮箱已在全球拥有1800万企业级客户，并进入全球企业邮箱排名前三。</li></ol><p>Zoho邮箱为用户提供1人起的邮箱服务。具体表现为，支持个人/企业通过官网进行注册，只要自有域名即可关联邮箱；无需绑卡，无隐形费用。对于有更高协作、管理需求的企业用户，还提供高级版、专业版等付费升级途径。</p><ol start="2"><li>试用功能及局限解析<br/>Zoho邮箱的基础版已能基本满足大多数小微企业与团队的日常需求。包括域名邮箱收发、日历、任务、备注、文件夹管理、反垃圾邮件等。需要注意的是，基础版在单域名可添加用户数（最多5人）、储存空间等方面有限制。</li><li>适合人群及应用场景<br/>Zoho邮箱的域名邮箱方案，尤其适合：</li></ol><p>对成本极为敏感、预算有限的创业公司<br/>小型团队/自媒体/电商卖家，需要提升业务形象<br/>希望拥有专业域名后缀的个人或社团<br/>在跨境贸易、远程协作、企业对外沟通等场景，Zoho邮箱广受认可。作为实用示例，不少国内外初创团队通过Zoho邮箱简洁明了的注册步骤，迅速搭建专属企业邮箱系统，获得安全、品牌统一、可靠的外部沟通渠道。</p><p>二、如何申请企业邮箱免费试用？</p><ol><li>基本申请流程<br/>以Zoho邮箱申请为例，流程高度简化：</li></ol><p>准备好可以验证的自有域名，进入Zoho邮箱官网<br/>选择“邮箱”入口，填写基础注册信息（如管理员姓名、邮箱、密码等）<br/>按照系统引导绑定域名邮箱，完成域名解析验证<br/>完善企业基础信息、添加团队成员<br/>只需3分钟即可初步开启企业邮箱系统。</p><ol start="2"><li>注册及实名认证所需资料<br/>注册Zoho邮箱需准备：企业管理员联系方式、自有域名管理权限、基础认证信息。实名认证流程，个人/企业均可选择，企业用户建议上传营业执照或组织机构代码证，确保后期邮件发送权属和账户安全。</li></ol><p>部分国内邮箱如腾讯企业邮箱、网易企业邮箱等在试用或正式开通前，也会要求实名注册并核验企业资质，部分可能要求管理员手机号、公司邮箱、企业信用代码等基本信息。</p><ol start="3"><li>试用期间注意事项与后续选择<br/>免费试用期间建议充分测试邮箱功能，比如域名邮箱收发效率、反垃圾拦截能力、移动端支持情况<br/>如果需批量导入成员，建议在Zoho后台用批量导入工具<br/>试用即将到期时，留意系统通知，及时决策是否升级为付费版本，以免邮件服务中断<br/>如果企业有更高安全、审批、分发等高级需求，可后期适时采购功能包。<br/>结语：免费企业邮箱如何选？Zoho邮箱真的值得吗？<br/>企业邮箱不必一掷千金。Zoho邮箱用“免费版+按需升级”把门槛降到零：3步注册即刻开通域名邮箱，15天全功能试用随时升级；AES-256加密、全球节点、1800万企业客户背书，让每一封邮件都专业、安全、秒达。</li></ol>]]></description></item><item>    <title><![CDATA[售后客户服务系统搭建指南 遭老罪的程序猿]]></title>    <link>https://segmentfault.com/a/1190000047383638</link>    <guid>https://segmentfault.com/a/1190000047383638</guid>    <pubDate>2025-11-09 21:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今天用Zoho Desk演示6步打法：需求洞察→流程标准化→团队赋能→科技提效→反馈闭环→品牌增值，让响应时间砍半，客户忠诚度翻倍！<br/><img width="431" height="287" referrerpolicy="no-referrer" src="/img/bVdmYOn" alt="" title=""/><br/>一、理解客户需求和期望<br/>系统化管理售后服务的第一步是深入了解客户的需求和期望。企业可以通过以下方式获取这些信息：</p><p>客户反馈与调查<br/>定期进行客户满意度调查，收集客户在使用产品后的反馈。这不仅可以帮助企业了解客户的真实感受，还能为调整服务策略提供数据支持。</p><p>直接沟通<br/>与客户进行直接的交流和互动，如通过售后电话或社交媒体平台，了解他们对产品和服务的意见。</p><p>数据分析<br/>运用大数据分析技术，挖掘客户行为与偏好，以此预测客户的未来需求。</p><p>通过Zoho Desk，企业可以轻松整合来自多渠道的客户反馈，并利用内置的报告和分析工具深入挖掘客户需求。这种集中化的客户信息管理，有助于企业设计更具针对性和个性化的售后服务方案。</p><p>二、建立高效的服务流程<br/>在理解客户需求后，建立高效的服务流程是系统化管理售后服务的核心。具体而言，这需要涵盖以下几个方面：</p><p>标准化服务流程<br/>制定清晰的售后服务流程图，把每一步的操作标准化，确保所有服务人员在面对不同客户时都能提供一致的高质量服务。 Zoho Desk支持自定义服务流程和自动化规则，企业可以通过设置自动化工作流来优化服务效率，例如自动分配工单、设置优先级等。</p><p>快速响应时间<br/>在售后服务中，响应时间的长短直接关系到客户满意度。企业应确保客服团队能够迅速有效地回应客户的请求，提供及时的解决方案。 Zoho Desk的SLA（服务级别协议）功能，可以帮助企业设定响应时间目标，并实时跟踪和提醒，确保客户问题得到及时处理。</p><p>透明的服务制度<br/>让客户了解售后服务流程和政策，如退换货条件、保修期等，增强客户的信任感。通过Zoho Desk的客户门户功能，企业可以提供一个透明的自助服务平台，让客户随时查询服务进度和相关政策。</p><p>三、培训与激励售后服务团队<br/>售后服务的质量与团队的能力和态度密不可分。因此，系统化管理还需要重点关注以下方面：</p><p>技能培训<br/>定期对售后服务团队进行培训，提升他们的专业技能与服务意识，包括沟通技巧、问题解决能力、产品知识等。</p><p>激励机制<br/>建立有效的激励制度来提升服务团队的工作积极性。可以通过绩效考核、奖励制度等方式，促进团队成员提供更优质的服务。</p><p>团队协作<br/>强调团队协作精神，确保各部门在处理客户问题时能够顺畅对接、快速解决。 Zoho Desk通过其团队协作工具（如工单共享、内部评论等），帮助售后团队高效协作，确保复杂问题能够快速得到解决。</p><p>四、引入科技提升服务水平<br/>在现代科技迅速发展的背景下，引入科技手段能显著提升售后服务的效率和效果：</p><p>客户服务管理平台（如Zoho Desk）<br/>Zoho Desk是一款专为售后服务设计的客户支持软件，能够帮助企业系统化地管理客户问题。通过工单管理、自动化工作流和多渠道支持，Zoho Desk让企业更高效地跟踪和解决客户问题。</p><p>人工智能与自动化<br/>Zoho Desk内置的AI助手Zia可以提供全天候支持，帮助解答客户的常见问题、分析服务趋势，并预测客户需求，从而提升工作效率并减少客服人员的负担。</p><p>数据分析与预测<br/>Zoho Desk内置的报告和仪表盘功能，能够实时分析客户反馈和服务数据，发现潜在问题趋势，并为企业提供数据驱动的服务优化建议。</p><p>五、建立完善的反馈机制<br/>持续完善售后服务，需要关注客户反馈，并将其作为提升服务的依据：</p><p>多渠道反馈收集<br/>通过电话、电子邮件、社交媒体等多种渠道收集客户反馈，全面了解客户体验。Zoho Desk支持多渠道整合，企业可以集中管理所有客户反馈。</p><p>分析反馈数据<br/>对收集到的客户反馈进行深入分析，明确服务中的短板，找出问题根源。</p><p>持续改进<br/>根据反馈分析结果，持续优化和改进售后服务流程，确保问题不再重复出现。借助Zoho Desk的自动化功能，企业可以快速实施改进措施，并通过后续跟进确保客户满意度提升。</p><p>六、打造文化与品牌效应<br/>企业的售后服务质量不仅依赖于流程和技术，更应反映出品牌文化和价值观：</p><p>诚信与责任<br/>在处理客户问题时，体现企业的诚信和责任。透明化的处理过程和合理的补偿措施，让客户感受到企业的诚意。</p><p>客户优先理念<br/>将客户满意度作为衡量售后服务的首要标准，努力为客户提供超出预期的服务体验。</p><p>品牌效应<br/>通过持续的优秀服务，积累良好的市场口碑，不断提升品牌形象和影响力。 Zoho Desk通过其客户满意度评分功能，帮助企业实时监测客户对售后服务的满意度，为品牌口碑的建立提供数据支持。</p><p>工具选对，售后变利润中心。Zoho Desk一张工单打通邮件、电话、社媒、自助门户；AI自动分配+SLA倒计时，复杂问题秒级分流；实时仪表盘把满意度变成可视化KPI。立即免费试用Zoho Desk，15天全功能，今天注册，明天就能看到客服指标全绿！</p>]]></description></item><item>    <title><![CDATA[用Excel做甘特图（2025新版） 遭]]></title>    <link>https://segmentfault.com/a/1190000047383656</link>    <guid>https://segmentfault.com/a/1190000047383656</guid>    <pubDate>2025-11-09 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>“计划的目的不是预测未来，而是为了现在做出预防它偏离正轨的决定。”彼得·德鲁克的提醒在今天依旧振聋发聩。可当这份“预防”落在项目经理的桌面时，最先被打开的往往不是专业系统，而是一张人人都会用的 Excel。用色块和条形图拼出甘特图，看似零成本，却常常把 80% 的精力消耗在 20% 的格式调整上——计划还没起跑，人已被表格拖垮。本文就带你拆解 Excel 做甘特图的全部技巧与天花板，并引出一个更轻盈的出路：Zoho Projects。<br/><img width="723" height="460" referrerpolicy="no-referrer" src="/img/bVdkYV6" alt="" title=""/><br/>一、Excel绘制甘特图的步骤详解<br/>尽管Excel主打电子表格功能，但它的灵活性使得我们可以创造性地用它制作甘特图。这需要一些技巧和耐心。</p><ol><li>起步：数据整理与基本格式设置<br/>一旦数据整理完成，记得设置正确的日期格式，并核查持续时间的天数是否和实际计划相吻合。数据的清晰性直接决定了甘特图绘制的准确性。</li><li>利用条件格式或条形图生成甘特图<br/>Excel中最常用的甘特图制作方法有两种：一是通过条件格式设置，对单元格背景填充颜色；二是借用条形图工具。</li></ol><p>条件格式法：利用Excel的公式设置条件格式，比如在时间轴区域内，只对那些“任务开始日到结束日”的单元格进行背景颜色填充。虽然设置稍显繁琐，但它的好处是在出现数据修改时可以动态调整。<br/>条形图法：使用Excel的“堆积条形图”功能。首先将任务的开始日期和持续时间进行分列，插入堆积条形图后，调整颜色和数据的轴向排列，使之看起来像是标准的甘特图。这个方法视觉效果更美观，同时效率也较高。</p><ol start="3"><li>自定义样式与优化图表<br/>如果甘特图用于对外展示，则需要美化和调整Excel表格的样式，例如更改调色板背景、添加标题、标注关键任务里程碑等。这一步帮助您提升图表的易读性，确保甘特图不至于过于“朴素”。</li></ol><p>二、Excel绘制甘特图的局限性<br/>虽然Excel在绘制甘特图方面可以实现一定的功能，但从效率和灵活性上来看，它并不是最佳的工具。此外，它也存在一些不可忽视的局限性。</p><ol><li>数据更新不够便捷<br/>在大型项目中，任务和日期常需要动态调整，而Excel在处理复杂数据更新时可能显得笨拙。您需要频繁修改表格公式或者手动移动条形图，这在多人协作时更容易出现版本冲突或错误。</li><li>缺乏特定的项目管理功能<br/>甘特图只是项目管理的一个视图，实际管理过程还涉及任务依赖关系、优先级设置、资源分配等功能，而Excel并没有这些功能原生支持。这就意味着，想要完整管理项目，可能需要依赖多个额外的工具。</li><li>协同能力较弱<br/>当团队涉及多人视图共享或实时更新时，只靠Excel难以追踪任何变更。即使在云端协作中，数据冲突、版本不一致等问题也难以杜绝。</li></ol><p>三、Zoho Projects如何成为更优选择？<br/>相比于Excel，专业的项目管理工具如Zoho Projects，可以为企业和个人用户提供更高效和专业的解决方案。以下是几个关键方面的优势：</p><ol><li>直观的甘特图视图与便捷操作<br/>Zoho Projects内置专业的甘特图功能，无需用户手动调整条形图或设置公式。通过简单的拖拽操作，您可以快速调整任务时间、设定依赖关系以及标记关键路线。不仅如此，其甘特图会根据任务动态自动生成，并与其他模块（如任务模块、工时模块）紧密集成，确保数据同步更新。这种自动化显著减少了人为错误。</li><li>更全面的项目管理功能<br/>除了甘特图，Zoho Projects还提供了丰富的项目管理功能，比如：</li></ol><p>任务依赖性设置：在任务之间定义“完成-开始”或“开始-开始”的逻辑，确保项目的有序推进。<br/>资源与团队管理：掌握团队成员的任务负荷，避免过载分配。<br/>里程碑与看板视图：多种视图切换方式，满足不同管理偏好的需求。<br/>这样，您无需在多个工具之间切换，所有管理功能都可以一站式实现。</p><ol start="3"><li>高效的团队协作与反馈跟踪<br/>Zoho Projects拥有内置的协同功能，允许团队成员实时查看任务进展并共享反馈。通过评论区、@提及功能以及自定义通知，团队可以随时沟通，确保任何项目中的问题都可以及时解决。此外，Zoho Projects还支持高级权限设置，确保数据安全。</li><li>提供多样化的报表与分析<br/>相比Excel的静态表格，Zoho Projects可以通过动态报表和仪表盘为管理者提供实时的项目洞察。例如，通过工时报告了解资源消耗，通过进度图标快速识别延迟风险。此外，所有报表都支持导出，无缝对接管理会议需求。</li><li>灵活的跨平台支持<br/>无论是在电脑、手机还是平板端，Zoho Projects都支持无缝访问。团队成员可以随时随地跟踪项目动态，而这些灵活的跨平台应用能力是Excel难以做到的。</li></ol><p>总结：甘特图不止于工具，更是效率的助推器<br/>Excel 像一把万能瑞士军刀，在单人、小样本、一次性场景里仍能快速“止血”；可当任务量级、变更频次、协作人数同时放大，继续用刀片拧螺丝只会划伤手指。把甘特图从单元格中解放出来，迁入 Zoho Projects，让拖拽代替公式、让依赖关系自动排期、让进度报表一键生成——计划不再被工具束缚，而真正成为团队同频的节拍器。毕竟，选对武器，现在的每一步才配得上未来的目标。</p>]]></description></item><item>    <title><![CDATA[跟5次黑客松冠军学 Linera 开发，]]></title>    <link>https://segmentfault.com/a/1190000047383392</link>    <guid>https://segmentfault.com/a/1190000047383392</guid>    <pubDate>2025-11-09 20:06:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdmYKp" alt="image.png" title="image.png"/></p><p>OpenBuild 社区联合 ResPeer 团队的 KK，推出 Linera 开发者实战系列免费课程。</p><p>本次课程将帮助您理解 Linera 如何通过客户端驱动的共识机制和微链架构来解决传统区块链的性能瓶颈问题。</p><h3><strong>🧑‍💻 主讲人</strong></h3><p><strong>KK</strong><br/>ResPeer创始人，ResPeer是Linera生态应用开发团队，获取过5次黑客松冠军，在Linera上开发了钱包、Meme发射协议、Swap**等应用。</p><h3><strong>课程内容：</strong></h3><p><strong>Lesson 01：走进 Linera</strong></p><p>了解项目背景、微链机制及其与传统架构的区别</p><p><strong>Lesson 02：开发环境实战</strong></p><p>熟悉CLI工具链，理解微链和应用概念</p><p><strong>Lesson 03：Token 应用开发</strong></p><p>通过实践理解Operation状态管理</p><h3><strong>👥 适合人群：</strong></h3><ul><li>对区块链新技术感兴趣的开发者</li><li>想了解微链架构的技术人员</li><li>希望参与 Linera 生态建设的工程师</li></ul><h3><strong>📖 开始学习</strong></h3><p>课程现已上线，复制链接开始学习：</p><p>📺 B站：<a href="https://www.bilibili.com/video/BV1wfyaBuEDM" target="_blank">https://www.bilibili.com/video/BV1wfyaBuEDM</a></p><h3><strong>💪 一起学习</strong></h3><p>欢迎加入课程学习群，获取课程PPT, 以及讲师答疑， 关于更多课程进度也会在社群发布！如果二维码过期，请加小助手微信（ID: qq99220909），备注”公开课“ 加入学习群，  一起进步！</p><h3><strong>🔗 关于Linera</strong></h3><p>Linera 是由前 Meta Libra/Diem 核心研究员 Mathieu Baudet 创立的新一代区块链基础设施项目，获得 a16z** 和 Borderless Capital 共1200万美元投资。项目通过创新的"微链"架构解决传统区块链的性能瓶颈——每个用户或应用可拥有独立的微链，实现真正的水平扩展和亚秒级交易确认，彻底告别网络拥堵和 Gas 费波动。  </p><p>与传统区块链的单链模型不同，Linera 采用客户端驱动的共识机制，支持无限 TPS 扩展，特别适合实时支付、游戏、预测市场等对性能和确定性要求极高的应用场景。目前项目处于测试网第三阶段，核心协议和开发工具链已就绪，正通过 Buildthon 黑客松孵化生态应用。</p><p>🌐 官网：linera.io | 📖 GitHub：github.com/linera-io</p>]]></description></item><item>    <title><![CDATA[GitHub Octoverse 202]]></title>    <link>https://segmentfault.com/a/1190000047383490</link>    <guid>https://segmentfault.com/a/1190000047383490</guid>    <pubDate>2025-11-09 20:05:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="396" referrerpolicy="no-referrer" src="/img/bVdmYLS" alt="image.png" title="image.png"/></p><p>前几天，Github 发布了 Octoverse 2025 报告。这篇以 “增长” 为核心主题的报告，通过多维度数据呈现全球开发者生态、技术选择与行业变革：全球开发者社区正以史无前例的速度扩张，AI 已从 “可选工具” 变为 “标配能力”，TypeScript** 重塑语言格局，而新兴市场正成为开源增长的核心动力。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLR" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>核心总结</strong></h4><ul><li>GitHub 开发者超 1.8 亿，过去一年新增 3600 万，平均每秒就有 1 人加入，80% 新开发者首周就用 Copilot free 来开发</li><li>TypeScript 第一次超过 Python 和 JavaScript，成了最常用的语言；Python 在 AI 领域还是最火的</li><li>印度去年新增 520 万开发者，是开源贡献者最多的国家，按趋势 2030 年全球每 3 个新开发者就有 1 个来自印度</li><li>AI 相关的开源项目增长最快，比如 vllm 这些，但很多开源仓库都没有贡献者指南，管理跟不上</li><li>GitHub 上有 430 万 AI 相关仓库，Copilot 智能体能帮忙写代码、提请求，还能自动修复一些漏洞</li><li>关键漏洞修复变快了 30%，但权限漏洞变多了，15.1 万仓库受影响，很多是 AI 生成代码没做好权限检查</li></ul><h3><strong>2025年 GitHub 核心现状：创纪录的增长规模</strong></h3><p>2025 年是 GitHub 历史上增长最快的一年，开发者数量与项目活跃度均突破峰值，平台作为全球开发者协作核心的地位进一步巩固。</p><p>从开发者规模看，全球 GitHub 开发者总量超 1.8 亿，过去一年（2024 年 9 月 - 2025 年 8 月）新增 3600 万，同比增长 23%，<strong>平均每秒新增 1 名开发者</strong>。新增开发者呈现显著地域多样性：每分钟约 25 名来自亚太地区、12 名来自欧洲、6.5 名来自非洲与中东、6 名来自拉丁美洲，其中印度单国新增超 500 万开发者，占全球新增总量的 14% 以上。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLQ" alt="image.png" title="image.png" loading="lazy"/></p><p>项目活跃度同样创下新高：<strong>2025 年 GitHub 代码仓库总数达 6.3 亿个，新增 1.21 亿个</strong>；私有仓库新增 5800 万个（同比增长 33%），占比提升，反映企业级开发向 GitHub 集中的趋势；公共仓库达 3.95 亿个（同比增长 19%），仍是开源生态核心载体。开发者日常操作频次大幅提升：每分钟创建超 230 个新仓库，每月合并 4320 万次拉取请求（同比增长 23%），全年代码提交近 10 亿次（同比增长 25.1%），仅 2025 年 8 月单月提交量就达近 1 亿次。</p><h3><strong>GitHub Copilot：驱动增长的关键变量</strong></h3><p>2024 年 12 月 “GitHub Copilot Free” 的推出，打破了 GitHub 长期稳定的增长规律，成为开发者注册与项目活跃的核心推动力。</p><p>Copilot Free 上线后， 新开发者注册量超出去年同期预测值，且 AI 工具渗透率快速提升：80% 的新开发者在加入 GitHub 的第一周就会使用 Copilot，表明 <strong>AI 辅助工具已从 “可选功能” 变为新开发者的 “默认需求”。</strong></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLP" alt="image.png" title="image.png" loading="lazy"/></p><p>Copilot 对开发效率的提升已量化显现：2025 年 3 月  preview of Copilot coding agent、4 月 Copilot code review 功能相继上线后，开发者问题关闭效率明显提升，3 月关闭问题数量较 2 月增加 140 万，7 月单月关闭问题达 550 万，创历史最高；同时代码提交频次加快，5 月起月均代码推送突破 900 万次，拉动全年提交量增长。此外，Copilot 在安全领域作用凸显，Copilot Autofix 功能每月在 6000 + 仓库修复 “访问控制漏洞”，在 3000 + 仓库修复 “注入漏洞”，成为自动化安全防护的重要工具。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLO" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>编程语言格局：TypeScript 登顶，Python 主导 AI 领域</strong></h3><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLN" alt="image.png" title="image.png" loading="lazy"/></p><p>2025 年 8 月成为 GitHub 编程语言格局的关键转折点 —— 按贡献者数量统计，<strong>TypeScript 首次超越 Python 与 JavaScript，成为最常用语言</strong>，这是近十年最显著的语言地位变动。TypeScript 和 Python 合计贡献者超过 520 万。AI 不仅加快代码速度，还在影响团队选择何种语言来将 AI 生成的代码投入生产环境。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLq" alt="image.png" title="image.png" loading="lazy"/></p><p>TypeScript 的增长源于生态与技术需求双重支撑：2025 年新增贡献者 105.4 万（同比增长 66.63%），核心驱动因素包括主流前端框架（React、Vue、Svelte 等）默认采用 TypeScript 搭建项目，降低入门门槛；其类型安全特性与 AI 辅助编码高度适配，能减少 AI 生成代码在生产环境的故障风险，成为企业级项目优先选择。</p><p><strong>Python 虽在总使用量上被超越，但在 AI 与数据科学领域主导地位进一步巩固</strong>：2025 年贡献者达 260 万（同比增长 48.78%），58.2 万余个 AI 标签仓库使用 Python（同比增长 50.7%），占所有 AI 项目近 50%；Jupyter Notebook** 作为 AI 实验核心工具，应用于 40.3 万余个仓库（同比增长 17.8%），仍是模型训练、数据分析、算法原型开发的 “标配环境”。</p><p>此外，小众语言呈现差异化增长：Luau（Roblox 脚本语言，同比增长 194%）、Typst（LaTeX 替代方案，同比增长 108%）、Astro**（轻量前端框架，同比增长 78%）、Blade（Laravel 模板引擎，同比增长 67%）等在细分领域快速崛起，反映开发场景日益多元化。</p><p><img width="713" height="838" referrerpolicy="no-referrer" src="/img/bVdmYLj" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>全球开发者地域分布：新兴市场崛起与2030年预测</strong></h3><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYK1" alt="image.png" title="image.png" loading="lazy"/></p><p>从当前数据看，印度已成为全球开发者增长的 “核心引擎”：2025 年印度新增开发者 520 万，；按 GitHub 数据团队预测，<strong>到 2030 年印度开发者数量将达 5750 万</strong>，占全球新开发者总量的 1/3，成为全球第一大开发者市场。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKJ" alt="image.png" title="image.png" loading="lazy"/></p><p>其他新兴市场同样表现亮眼：亚太地区除印度外，日本、印尼增速显著 —— 日本受益于数字转型政策，开发者数量较 2020 年增长 3 倍；印尼开发者达 437 万（2020 年为 90 万），占东南亚数字经济近 50%，成为区域核心。拉丁美洲的巴西、墨西哥、哥伦比亚新增 320 万开发者，主要依赖美国 / 欧盟企业的远程招聘与金融科技创业生态爆发。非洲与中东新增 340 万开发者，移动设备普及、社区编程训练营与本地 LLM 应用，成为推动增长的关键因素。</p><p><img width="714" height="616" referrerpolicy="no-referrer" src="/img/bVdmYKI" alt="image.png" title="image.png" loading="lazy"/></p><p>成熟市场则保持稳定增长：<strong>美国仍是开发者人口总量第一的国家（2800万），但开发者增速放缓</strong>；欧洲（德国、英国、法国）依赖云基础设施投入与 AI 领域投资，新增 630 万开发者。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKG" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>开源生态：AI 项目主导，治理滞后成挑战</strong></h3><p>2025 年开源生态保持高活力，贡献量与项目数量均创历史新高，但同时面临治理滞后问题。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKE" alt="image.png" title="image.png" loading="lazy"/></p><p>从活跃度看，<strong>2025 年全球开源贡献达 11.2 亿次</strong>（同比增长 13%），合并拉取请求 5.187 亿次（同比增长 29%）；3 月成为 “GitHub 开源新贡献者最多的月份”，新增首次贡献者 25.5 万名。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKD" alt="image.png" title="image.png" loading="lazy"/></p><p>项目层面，AI 基础设施项目成为绝对主力 —— 贡献者增长最快的 10 个开源项目中，6 个为 AI 相关，其中 <strong>vllm-project/vllm 成为贡献者数量最多的开源仓库</strong>，超过传统热门项目 microsoft/vscode、home-assistant/core。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKC" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="708" height="668" referrerpolicy="no-referrer" src="/img/bVdmYKB" alt="image.png" title="image.png" loading="lazy"/></p><p>但开源治理滞后问题凸显：仅 5.5% 的开源仓库提供 “贡献者指南”，2% 配备 “行为准则”，远低于 63% 的 README 文件覆盖率；这导致新贡献者入门门槛高，项目协作中易出现冲突。此外，OpenSSF Scorecard（开源安全评分标准）的采用率虽在头部项目中达 94%（前 50 名项目中 47 个使用），但中小项目的安全配置率仍不足 30%，存在潜在风险。</p><p>从贡献者分布看，开源生态呈现 “全球化” 特征：<strong>印度超越美国成为 “开源贡献者最多的国家”</strong> ，巴西、印尼进入前 5 名；<strong>美国虽贡献总量第一</strong>，但人均贡献频次下降，反映开源生态的 “去中心化” 趋势。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmYKA" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>安全实践：自动化提升与新风险并存</strong></h3><p>2025 年开发者安全实践的 “自动化程度” 显著提升，但同时涌现出新的安全风险，整体呈现 “机遇与挑战并存” 的态势。<br/><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKz" alt="image.png" title="image.png" loading="lazy"/></p><p>自动化工具的应用大幅提升安全效率：2025 年配置 Dependabot 的仓库达 84.6 万个（同比增长 137%），该工具能自动检测依赖项漏洞并提交修复 PR，推动严重漏洞修复速度加快 30%—— <strong>平均修复时间从 2024 年的 37 天缩至 26 天。</strong></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKy" alt="image.png" title="image.png" loading="lazy"/></p><p>同时，GitHub Actions 的使用量增长 35%，11.5 亿分钟的免费 CI/CD 时长被用于公共项目的安全测试，进一步降低安全防护门槛。</p><p>但新的安全风险不容忽视：“访问控制漏洞” 取代 “注入漏洞”，成为 CodeQL 检测到的最常见风险（15.1 万余个仓库被标记，同比增长 172%），主要源于两方面：AI 生成的代码中常缺失权限校验逻辑，导致未授权访问；CI/CD 管道的权限配置不当，引发数据泄露风险。此外，尽管 Copilot Autofix 能修复部分常见漏洞，但针对复杂业务逻辑的安全问题，仍需人工介入，安全防护的 “人机协同” 模式尚未完全成熟。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKx" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>生成式 AI 与 Agentic ：从实验到日常工程</strong></h3><p>2025 年生成式 AI 与智能体工具从 “技术实验” 进入 “日常开发工程”，成为重塑开发流程的核心力量。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKv" alt="image.png" title="image.png" loading="lazy"/></p><p>从项目规模看，<strong>AI 相关仓库总数突破 430 万个，较 2023 年近乎翻倍</strong>；113 万 + 公共仓库引入 LLM SDK（同比增长 178%），其中 69.3 万余个是过去 12 个月新增，涉及 OpenAI、Anthropic、Mistral 等主流模型的集成。从贡献者来看，<strong>每月有 20 万 + 开发者为 AI 项目提交代码，2025 年 5 月贡献者数量达峰值 20.68 万</strong>（同比增长 132%），反映 AI 开发已从 “小众领域” 变为 “大众方向”。<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKw" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKu" alt="image.png" title="image.png" loading="lazy"/></p><p>智能体工具的落地是关键突破：GitHub Copilot coding agent 自上线后，5-9 月协助创建超 100 万次拉取请求，能自动完成代码生成、测试执行、PR 创建等流程；且其应用集中在 “高星标、大规模” 的成熟项目中，<strong>表明企业级团队已开始将智能体用于核心开发环节</strong>。此外，AI 对工具链的影响逐步深化 —— 开发者选择 IDE、框架时，会优先考虑 “是否适配 AI 工具”，如本地 LLM 运行器 Ollama、RAG 框架 Rag**flow 的快速增长，均源于对 AI 工作流的适配。</p><p>完整报告：<a href="https://link.segmentfault.com/?enc=19ixdLJgGn042rOwfwqPXQ%3D%3D.r29GMjhanuOhDcyQUwR2dYByK5qdxCbwVyBfXP%2BJeAViyPG8EzTtOPMtouaHknNJmCNhagDH%2BGU%2BWLsxbwxu7Ut5t5Tll%2F28RuILwq%2BNxJS3z9R56oj59eqSMA6frN0EQ0ZsKgUN%2BPhn0CuvnvTMPYi7xDfUX3oIVVIvKAkqdg0%3D" rel="nofollow" target="_blank">https://github.blog/news-insights/octoverse/octoverse-a-new-d...</a></p>]]></description></item><item>    <title><![CDATA[HackerHouse 精彩回顾：深圳五]]></title>    <link>https://segmentfault.com/a/1190000047383515</link>    <guid>https://segmentfault.com/a/1190000047383515</guid>    <pubDate>2025-11-09 20:05:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>10月23日-27日，由 Solar 联合 OpenBuild 举办，华为、BGA 特别支持的 Solana Cypherpunk Mini Hacker House 深圳站完美落幕！近 20 位 Hackers 齐聚大鹏海边别墅，用五天时间沉浸式探索 Solana 生态、协作开发项目。在海浪与代码的交织中，完成了一场从创意萌芽到项目落地的探索之旅。</p><h3><strong>Day 1：集结！Build Together</strong></h3><p>23日下午，阳光洒满大鹏海岸。Hacker 们陆续抵达抵达别墅，为即将开启的五天 co-buildind 时光做准备。初见时的拘谨并未持续太久，有人分享自己对 Web3 领域的独特见解，有人提出极具创新性的项目构想。几句关于技术与生态的交流，以及 idea 的碰撞，志同道合的开发者们迅速找到了共鸣，让原本安静的空间响起热烈的讨论声。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMe" alt="image.png" title="image.png"/></p><p>晚上吹着海风，海鲜大餐成为破冰的关键：蒜蓉粉丝蒸扇贝、椒盐皮皮虾等地道美味让氛围迅速升温。大家开始自己介绍，分享各自的从业经历与项目创意。现场既有充满活力的在校大学生，也有深耕 Web2、Web3 行业多年的资深开发者，还有多次参与黑客松活动的 “老手”。</p><p>这一晚，没有紧绷的开发任务，只有纯粹的技术交流、理念分享，为接下来五天的协作奠定了温暖的基础。</p><h3><strong>Day 2：Co-living、Co-building</strong></h3><p>经过首日的破冰与磨合，Hacker 们在 24 日正式进入项目开发阶段，building 氛围从清晨就开始拉满。当天上午，活动特邀多位嘉宾到场带来精彩分享，为开发者们赋能。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMf" alt="image.png" title="image.png" loading="lazy"/></p><p>首先，来自 OpenBuild 社区的 qiuqiu 为大家详细介绍了本次 Solana 全球黑客松和 Hackerhouse 的规则和注意事项，同时分享了 OpenBuild 社区的发展历程、资源支持以及过往黑客松活动的优秀案例，让开发者们对后续开发方向有了更清晰的认知。</p><p>Solayer 的市场负责人 Margie 同样给 Hacker 们深入剖析了 Solayer 在 Solana 生态中的技术优势与应用场景，为开发者们打开了硬件与区块链结合的新思路。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdmYMg" alt="image.png" title="image.png" loading="lazy"/></p><p>紧接着，来自 Jito Labs 的 Atsushi 介绍了 Jito 的核心功能与技术架构，并分享了 JET Asia（Jito DAO）未来的发展规划与战略布局，让开发者们对 Solana 生态中的基础设施项目有了更全面的了解。</p><p>三位嘉宾的分享干货满满，不仅解答了开发者们在技术选型、生态适配等方面的疑惑，更为大家提供了多元的项目开发思路，现场掌声不断。</p><p>夜幕降临，一场热闹的屋顶 BBQ 音乐派对点燃热烈氛围。夕阳下，烧烤的香气混合着啤酒的清爽，Hacker 们在悠扬的音乐中，开始 social，为精彩捧杯，享受着独属于他们的蓝调时刻。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMh" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMi" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMj" alt="image.png" title="image.png" loading="lazy"/></p><p>晚餐过后，OpenBuild 的 buildhero skyh 为大家分享了以往 Solana 黑客松活动中的优秀项目，从项目创意、技术实现到市场推广，进行了全方位解读，为项目开发注入了更多灵感。</p><h3><strong>Day 3：Mock Demo，代码之夜</strong></h3><p>经过 Day 2 的赋能，开发者们的项目已初步成型。10 月 25 日，活动进入 Mock Demo 环节，通过提前演练与点评，帮助各团队完善项目。</p><p>在模拟演示中，各团队用 5 分钟呈现初期成果。</p><ul><li>WorkWork：聚焦数字游民与自由职业者需求，提出 “Work everywhere, Work anytime” 的核心定位，目前已实现远程工作任务匹配、跨时区协作日程管理、多币种薪酬结算三大基础功能，计划后续接入 Solana 链上身份系统，提升用户信息安全性；</li><li>MindSensor：则带来 AI 与 Web3 结合的创新尝试，以 “Awareness is Value” 为理念，呈现了融合脑波检测、心灵觉醒与链上生态的 AI 平台，现场演示了通过简易脑波设备采集专注度数据，转化为链上积分的初步流程，未来拟将积分用于生态内课程兑换、冥想社群权益解锁；</li><li><p>Pelago：深耕 Solana 生态 DeFi 领域，介绍了其基于隔离市场架构的借贷协议，核心亮点在于支持无许可创建自定义借贷市场，团队已完成基础合约开发，可实现不同资产的独立风险定价，适配从稳定币到小众代币的多样化借贷需求。</p><p>...</p></li></ul><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMk" alt="image.png" title="image.png" loading="lazy"/></p><p>各项目团队在演示结束后，结合现场反馈与讨论，收获了不少跨团队的创新思路，开始进一步梳理了优化方向。直到深夜，整个别墅都充满了热血的开发氛围！</p><h3><strong>Day 4: 沙滩、落日、飞盘</strong></h3><p>经过前三天的高强度开发与优化，10 月 26 日，活动为开发者们安排了轻松的海边休闲环节，让大家在忙碌之余放松身心，为次日的 Demo Day 积蓄能量。</p><p>当天上午，开发者们依旧保持着专注的状态，在别墅内继续打磨项目细节。有人针对演示流程进行反复演练，有人对项目界面进行最后优化，每个人都在为即将到来的 Demo Day 做着充分准备，力求呈现最完美的项目效果。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMl" alt="image.png" title="image.png" loading="lazy"/></p><p>午后，阳光正好，开发者们来到海边，开启了一场充满活力的休闲时光。飞盘在手中传递，笑声在沙滩上回荡；有人纵身跃入海中，感受海水的清凉；有人漫步沙滩，留下一串串脚印…… 大家在欢声笑语中释放压力，享受着海边的惬意与美好。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMm" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMn" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMo" alt="image.png" title="image.png" loading="lazy"/></p><p>当晚，Hacker 们再次齐聚，享用了一顿丰盛的海鲜晚餐。餐桌上，大家不再谈论代码与项目，而是分享着这几天的欢乐瞬间，彼此的情谊在温馨的氛围中进一步加深，本次 Hackerhouse 也即将在海风与星光的见证下圆满收官！</p><h3><strong>Day 5：Demo Day 成果亮相</strong></h3><p>10月27日，Solana Cypherpunk Mini Hacker House 深圳站 Demo Day 正式拉开帷幕。这是五天努力的成果展示，更是一场 Web3 创新思想的碰撞盛宴。</p><p>活动伊始，qiuqiu 为本次活动致开场词，鼓励大家尽情展示成果；Solayer 的市场负责人 Margie 带来主题为《硬件加速，无限扩展》的分享，深入解读技术前沿；Orca 的中文贡献者 Yibo 则通过《DeFi 乐高，基于 Orca 搭建的方向》主题分析，介绍了 Orca 项目情况和进展。</p><p><img width="723" height="1098" referrerpolicy="no-referrer" src="/img/bVdmYMa" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMb" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMc" alt="image.png" title="image.png" loading="lazy"/></p><p>随后，15 + 支团队依次登台，展示各自的创新项目。无论是聚焦 DeFi 领域的新型协议，还是探索 Web3 与硬件结合的创新产品，亦或是关注实体资产上链的 RWA 项目，每一个项目都展现了开发者们对 Solana 生态的深度思考与创新实践。在展示过程中，团队成员自信从容，条理清晰地讲解项目理念、技术架构与应用场景，现场评委与观众不时发出阵阵赞叹。</p><p>此次 Demo Day 的成功举办，离不开多位特邀嘉宾的大力支持。非常感谢 Vesper(Community Lead @Solar)、 Mike(DevRel @Solana基金会)、Margie(Marketing Lead @Solayer)、Atsushi(Asia Lead @Jito)、Yibo(Chinese Contributor @Orca)以及 Skyh (core builder @OpenBuild)，他们不仅在现场为开发者们提供了专业点评与指导，还分享了行业前沿动态，为活动增添了更多价值。</p><h3><strong>结语</strong></h3><p>从海边别墅到 Demo Day 舞台，这段旅程不仅碰撞出了 15 + 个具有潜力的项目，更凝聚了一批热爱 Solana、愿意深耕 Web3 领域的优秀开发者。非常高兴能与各位开发者一同在海边度过这段充实而精彩的时光！</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYL9" alt="image.png" title="image.png" loading="lazy"/></p><p>由衷感谢所有让这一切成为可能的人！特别感谢 Vesper、Mike、Margie、Atsushi、Skyh、Yibo 等多位老师的全程支持与指导，为 Hacker 们指明了开发方向。感谢工作人员 Kang、轩睿、142y、Z.c、Anyi 对 HackerHouse 日常运营的辛苦付出，也感谢各位开发者的积极参与，用自己的热情与创新，让本次 Hackerhouse 充满活力！</p><p>在未来，期待更多开发者加入 Solana 的探索之旅！OpenBuild 也将开发者一起，见证更多优秀的 Solana 项目从想法走向现实！</p>]]></description></item><item>    <title><![CDATA[Web3 开发者周刊 74 | x402]]></title>    <link>https://segmentfault.com/a/1190000047383524</link>    <guid>https://segmentfault.com/a/1190000047383524</guid>    <pubDate>2025-11-09 20:04:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMs" alt="image.png" title="image.png"/></p><p><strong>欢迎回到 Web3 开发者周刊第 74 期！</strong></p><p>本期周刊内所有黑客松活动、新闻和赏金任务，请大家点击查看原文以获取完整信息。如果您喜欢我们的内容，也欢迎大家订阅 <strong>OpenBuild Substack</strong>，获取最新最全开发者资讯！</p><p>本周，我们将探讨叙事型和实用型加密货币对开发者的差异化要求，聚焦最近大火的x402 还有哪些需要改进的地方，以及关注比特币白皮书发布17周年有哪些启示。</p><p>此外，我们还为开发者准备了新的黑客松资讯和赏金任务。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmYMt" alt="image.png" title="image.png" loading="lazy"/><br/><strong>✅ Mantle Global Hackathon 2025</strong></p><p>📅 时间：2025年10月22日 - 2026年2月7日</p><p>📍 线上</p><p>💸 奖金：150,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=%2FKTFCfrxyk1qzX9TcH%2B8mA%3D%3D.rzbWUyWCGnLplSmL9cR9T7bvnoHd4j7c6g7NJidvFBSljc46OjGjhGMzBrC8MWkNUBfJDrC3pSDp1EOJ7TP9dmJKqneAlWc7dVWYAa76sZo%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a></p><p><strong>简介：</strong></p><p>Mantle Global Hackathon 2025 是 Mantle 的全新篇章。这场为期三个月的黑客松，邀请开发者、创业者及创新者在 Mantle 上设计、开发并部署具备可扩展性的 Web3 产品。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMu" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Solana X402 Hackathon</strong></p><p>📅 时间：10月28日 - 11月11日</p><p>📍 线上</p><p>💸 奖金：50,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=IijYExAT9%2BDZhPx89Z%2BeUQ%3D%3D.iSCAWia%2Fc16h2ZhyStCR%2BDdLnjqxR8ym4PYXegKepNAvotRLh2zmFq665lqWUNMB" rel="nofollow" target="_blank">https://solana.com/zh/x402/hackathon</a></p><p><strong>简介：</strong></p><p>构建代理经济的机会来了。Solana X402 Hackathon 旨在基于 x402 构建新的创新工具、应用、基础设施和代理。你将构建开源基础设施和应用程序，以推动 Solana 上的代理经济。创建使 AI 代理能够自主交易的工具，开发创新的支付解决方案，或构建实用的代理应用程序。这是你定义自主代理如何与数字经济互动的机会。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmYMv" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ NullShot Hackathon</strong></p><p>📅 时间：10月1日 - 11月22日</p><p>📍 线上</p><p>💸 奖金：30,000 美元</p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=%2FG3tatJiNWZ%2B5nrWYcemEg%3D%3D.2qxqUiN2rnLLfkUAu5Cp5jMXShg%2F0qKmm2Xg5PYpiamPCTTU9zf%2FrJ%2BVlNuz4k7MOIY87Bi269687GEgsvwqzQ%3D%3D" rel="nofollow" target="_blank">https://dorahacks.io/hackathon/nullshothacks/detail</a></p><p><strong>简介：</strong></p><p>NullShot 黑客松汇聚了开发者、研究人员和建设者，共同探索智能去中心化应用的潜力，并围绕 AI-native 开发培育一个不断发展的社区。黑客松奖金池为30,000美元，专门面向区块链的可组合性与人工智能的互操作性相结合的建设者，助力独立代理和人工智能应用网络能够释放新的实用形式和价值创造方式。</p><p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdmYMw" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="162" referrerpolicy="no-referrer" src="/img/bVdmYMx" alt="image.png" title="image.png" loading="lazy"/><br/><strong>📖 叙事型加密货币 vs 实用型加密货币 10/27</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=3Uv0bqciyE270M4rffVALQ%3D%3D.451%2FYmGdmpI5hJYrhvEnWKL3JDfdT4aKi9NwgU1kwfLd6i2NowxP9pT0VO%2BV3X4UXzu2ZfPJWq5PhTwIISPKhA%3D%3D" rel="nofollow" target="_blank">https://x.com/yashhsm/status/1982636379650838745</a></p><p>在加密货币领域，“实用性” 与 “叙事性” 二者同等重要。作为开发者，始终要选择其中一个作为开始，并在叙事性或实用性方面做到非常出色。先某一方面打磨成熟，再向另一方面拓展 —— 这才是最终的发展路径。  </p><p><strong>📖 x402 很好，但我们还需要更多 10/30</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=hVnIi%2BmUhqZdGqL%2FLeEhCw%3D%3D.R7XyY%2F0GfP1abBMRli2uLqhWRCkOifNJBLuuZ9dPkBxAf%2BupEyB%2FgbD7WUoy7aohpcQ%2FFpoeOnqVq9FmvkGQBPv77dmkQkrPaZVaEYU7ZL0%3D" rel="nofollow" target="_blank">https://x.com/tbtstl/status/1980363102676824574?s=12&amp;t=2q9OLM...</a></p><p>去中心化互联网需要去中心化支付体系的支撑。X402 v1 已经充分展现了技术的潜力与可能性；X402 v2 虽实现了阶段性的改进，但目前来看，我们还需要更具突破性的进展。</p><p><strong>📖 比特币白皮书迎来 17 岁生日 10/31</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=VTNdYVdntJmp%2FXeJa2od%2FA%3D%3D.31ttU18A%2BdPweavcDWKBilcyqWsIRZbc8z4EzFL7okeFWMcuyfOjmeWUl0ucyWMHKgCYUuUq2yHDrWGRu5phIg%3D%3D" rel="nofollow" target="_blank">https://x.com/intangiblecoins/status/1984275450168770715</a></p><p>2008 年 10 月 31 日，中本聪**发布了一篇 9 页长的《比特币》白皮书。</p><p>17 年间，比特币的蜕变令人震惊。它最初只是一个仅有数百名密码朋克讨论的小众开源项目，如今已成为一种全球交易的资产，持有者涵盖机构、主权国家以及数千万普通个人。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKOQ" alt="image.png" title="image.png" loading="lazy"/><br/><strong>💸 Ostium   10/27</strong></p><p>最高赏金 100,000 美元 </p><p>Ostium 协议是以太坊第二层 Arbitrum 上的一个开源去中心化交易所，它能让用户以透明且非托管的永续合约方式接触现实世界资产。Ostium 经过定制，支持从加密货币钱包对黄金、石油、标准普尔指数**、日元等资产以及其他传统市场资产进行链上小额交易。</p><p><strong>💸 Yearn Finance 10/29</strong></p><p>最高赏金 200,000 美元 </p><p>Yearn Finance 是 DeFi 领域的一套产品，它在以太坊区块链上提供借贷聚合和收益生成服务。该协议由众多独立开发者维护，并由YFI持有者进行治理。</p><p><strong>💸 Parallel 10/30</strong></p><p>最高赏金 250,000 美元</p><p>Parallel 是一个资本效率高、模块化的稳定币协议，它允许创建超额抵押的去中心化稳定币。该协议由多个不同的模块组成，DAO 可随时间添加或移除这些模块，稳定币可通过这些模块发行或铸造。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKO4" alt="image.png" title="image.png" loading="lazy"/><br/>OpenBuild 是一个面向 Web3 开发人员的开源社区和平台。我们的目标是将更多的 Web2 开发人员带入 Web3 领域，同时帮助现有的 Web3 开发人员更好地构建并通过我们的产品取得商业成功！</p><p>欢迎在更多平台上关注我们：</p><p><a href="https://link.segmentfault.com/?enc=LkQq0ac6GTFQKua6HE95ag%3D%3D.FdsNCexkg05qZh29zD64XelQo%2BCp5SGVJSCaSm2arRo%3D" rel="nofollow" target="_blank">https://linktr.ee/openbuild</a> 🙌🙌</p>]]></description></item><item>    <title><![CDATA[Nexus 中国行成都站圆满收官！百位 ]]></title>    <link>https://segmentfault.com/a/1190000047383551</link>    <guid>https://segmentfault.com/a/1190000047383551</guid>    <pubDate>2025-11-09 20:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMF" alt="image.png" title="image.png"/></p><p><strong>Nexus 中国行成都站完美落幕！</strong> 11月1日下午，继北京、上海、深圳三站爆满后，Nexus 中国行成都站在热烈的氛围中圆满结束！</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMG" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>📊 活动数据</strong></h3><p>✅ 150位 Builder 踊跃报名</p><p>✅ 涵盖开发者、创业者、投资人、高校研究者</p><p>✅ Panel Discussion 全程高能</p><p>✅ 现场互动热烈</p><h3><strong>🎯 高能回顾</strong></h3><h4><strong>Nexus 生态进展分享</strong></h4><p>Nexus 团队深入分享了 zkVM 最新技术突破、生态发展路线图，以及开发者激励计划的最新进展。现场开发者对可验证计算在 DeFi 领域的应用展现出极大兴趣。</p><p><img width="723" height="1098" referrerpolicy="no-referrer" src="/img/bVdmYMI" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMH" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdmYMJ" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>Panel Discussion 高光时刻</strong></h4><p>Panel 主题 <strong>"zkVM 赋能 DeFi：从可验证计算到链上金融革新"</strong> ，引发热烈讨论：</p><p>✨ AI 与 DeFi 融合的创新路径</p><p>✨ 全链流动性的技术实现</p><p>✨ 隐私金融与合规的平衡之道</p><p>✨ 开发者生态建设的挑战与机遇<br/><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdmYMK" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>🏆 现场精彩瞬间</strong></h3><p><img width="723" height="548" referrerpolicy="no-referrer" src="/img/bVdmYML" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMM" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMN" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="1099" referrerpolicy="no-referrer" src="/img/bVdmYMO" alt="image.png" title="image.png" loading="lazy"/></p><p>早鸟福利抽奖环节气氛热烈，欢乐谷年卡、Switch 等大奖花落幸运儿</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMP" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMQ" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdmYMR" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMS" alt="image.png" title="image.png" loading="lazy"/></p><p>Coffee Break 环节，开发者、用户之间深度交流，现场 networking 质量很高</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMT" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMU" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMV" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMW" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMX" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMY" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>💬 参会者反馈</strong></p><blockquote><p>成都 Web3 氛围真的太好了！这次活动干货满满，认识了很多志同道合的朋友。</p><p>Nexus 团队的分享很有深度，对我很有启发。</p><p>希望这样的技术交流活动能更多一些！</p></blockquote><p><strong>让我们一起，构建可验证计算的未来！</strong></p><h3><strong>💡 更多 Nexus 了解</strong></h3><p>项目官网：<a href="https://link.segmentfault.com/?enc=Nlotrp2YB6QihyV5UEVLgQ%3D%3D.4%2B05o12lJB9HSkwcfsnYi7g8ZNWs3aYV8Y5zYCMoXDY%3D" rel="nofollow" target="_blank">https://nexus.xyz/</a></p><p>Devnet：<a href="https://link.segmentfault.com/?enc=laWkTeDEGnnDK7xj9pCIKQ%3D%3D.Yc%2B7i4SyvDIzvd7wnglZo9o0A4Xn63DWJR1vWVPqG6Y%3D" rel="nofollow" target="_blank">https://app.nexus.xyz/</a></p><p>Github：<a href="https://link.segmentfault.com/?enc=ciYpc4V6ur%2Fpk9pRX6lJsw%3D%3D.5D1tITJmKKw7rhFv3o15GIUM7Xv%2BTwSn3TJDYZDhGHA%3D" rel="nofollow" target="_blank">https://github.com/nexus-xyz</a> </p><p>Discord：<a href="https://link.segmentfault.com/?enc=BbcTApa%2FSMBxTHf6KNdxjA%3D%3D.2ryfK90TjeTgg%2FVu2%2B2qgXzdWQwYAvAKVCl5TjYGrcWA1CNl1dGXkyxGvN6bTtce" rel="nofollow" target="_blank">https://discord.com/invite/nexus-xyz</a></p><p>官方文档：<a href="https://link.segmentfault.com/?enc=Xs3sj%2FOwTqcYmIycz5DWXA%3D%3D.FvMnqfCxsF3cXtr73B3zgGD0QIrouXmWUlUHm7sI3Mw%3D" rel="nofollow" target="_blank">https://docs.nexus.xyz/home</a></p><p>中文操作指南：<a href="https://link.segmentfault.com/?enc=ABl9yxHJtG1R2ouGs0rDsg%3D%3D.FfscLMKWeq11buyzQAxXm1RjR27zV3HKiQbpIJm9cMk%3D" rel="nofollow" target="_blank">https://www.nexushelp.xyz</a> （中文区大使 Ccool 老师贡献）</p>]]></description></item><item>    <title><![CDATA[Balancer V2 漏洞事件分析：1]]></title>    <link>https://segmentfault.com/a/1190000047383563</link>    <guid>https://segmentfault.com/a/1190000047383563</guid>    <pubDate>2025-11-09 20:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025 年 11 月 3 日，曾被业内普遍认为 “久经考验” 的协议 Balancer V2，遭遇了超 1.28 亿美元的资产被盗。对许多人来说，这是一场意外；但对读过我<strong>10 月 16 日发布的 Balancer AMM 架构分析</strong>的读者而言，这却是一次令人恐慌的印证。</p><p><img width="680" height="340" referrerpolicy="no-referrer" src="/img/bVdmYM3" alt="image.png" title="image.png"/><br/><a href="https://link.segmentfault.com/?enc=IY2KeFsOOViO4QOsY%2F60lA%3D%3D.RBJMOs5RS9KDgkca6XkzulWiMn9kHXcuGyrjymtu8SQu%2FR5nHvag4xvawOAw0eqU0qAgOZIEAvzk4qUJEjnwHg%3D%3D" rel="nofollow" target="_blank">https://www.zealynx.io/blogs/balancer-protocol-architecture</a></p><p>攻击者并未发现某个复杂到极致的新漏洞，而是利用了我们早已指出的一个架构缺陷。</p><p>这并非算法层面的漏洞，而是一个更隐蔽、更 “精妙” 的问题：协议层的逻辑缺陷，它利用了存在漏洞的状态机，让协议对自身状态产生了误判。</p><p>接下来，我们将拆解攻击路径、漏洞原理，以及为何任何理解 V2 架构核心风险的人，本应能预见这场危机。</p><h4><strong>核心总结</strong></h4><ul><li><strong>事件经过：</strong> Balancer V2 金库操作中存在授权缺陷或状态完整性漏洞，导致其内部账本被篡改。</li><li><strong>影响范围：</strong> 主网及分叉项目共计约 1.28 亿美元资产被盗。</li><li><strong>根本原因：</strong> 金库多调用操作期间，状态控制机制不完善，攻击者借此突破了金库与资金池的信任边界。</li><li><strong>受影响对象：</strong> Balancer V2 及其分叉项目。</li><li><strong>未受影响对象：</strong> Balancer V3—— 其原子记账模型可防范此类漏洞攻击。</li><li><strong>核心教训：</strong> 审计需针对跨合约信任边界及隐式状态转换进行测试。</li></ul><h3><strong>架构背景：单例金库与 “信任边界”</strong></h3><p><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdmYM4" alt="image.png" title="image.png" loading="lazy"/></p><p>要理解这次漏洞利用，必须先搞懂 Balancer V2 的架构。它的核心设计基于两个概念：</p><ul><li>单例金库（Vault.sol）：一个单独的大型合约，托管着所有 Balancer V2 资金池（Pool）的资产，相当于 “银行” 的角色。</li><li><strong>外部资金池合约（如 LinearPool.sol）：</strong> 独立的外部合约，仅包含业务逻辑（即 AMM 算法），自身不持有任何资产。</li></ul><p>当你执行兑换（swap）操作时，金库会先接收你的代币，然后向对应的资金池合约发起外部回调（如onSwap函数）。本质上是在问：“我收到了 100 枚 WETH，应该返还多少枚 osETH？” 资金池完成计算后，会将结果返回给金库。</p><p>这就形成了一个关键的<strong>信任边界</strong>：持有资产的金库，必须信任仅含逻辑的资金池。这 1.28 亿美元资产的安全，完全依赖于这套回调机制的可靠性。</p><p>另一个关键设计是 “内部用户余额” 系统。为节省 Gas 费，金库不会在每一步操作中都执行 ERC-20 转账，而是通过一个内部的<code>mapping(address =&gt; uint256)</code> </p><p>账本更新余额。攻击者的最终目标，并非直接盗取代币，而是让这个账本写入一条虚假的余额记录。</p><h3><strong>根本原因：状态机漏洞</strong></h3><p>早期报告推测漏洞是通过资金池<code>onSwap</code> 函数调用<code>initialize()</code> 引发的重入，但链上证据显示，漏洞更可能是利用了金库用户余额流程中的授权缺陷或内部余额不一致问题。不过，核心架构问题 —— 金库与资金池逻辑间脆弱的信任边界 —— 并未改变。</p><p>核心漏洞很可能是金库在处理多调用（multi-call）操作时，存在授权缺陷或状态完整性问题。</p><p>攻击者似乎瞄准了金库内部余额管理系统中可能存在的授权漏洞，可能在金库与资金池的回调交互过程中，利用了跨合约重入或权限提升的漏洞。</p><p>架构层面的缺陷在于：开发者可能为正常操作流程设置了完善的访问控制，但未能预见复杂的多调用场景 —— 在这类场景中，特权操作可能会在执行过程中被非法调用。</p><p>这一漏洞表明，系统中存在未被妥善保护的 “隐式状态”：</p><ul><li>状态 A：闲置（等待外部调用）</li><li>状态 B：执行中（处于多调用操作的执行过程中）</li></ul><p>关键操作在状态 A 下受到保护，但在状态 B 的复杂状态切换过程中，却可能处于暴露状态。</p><h3><strong>攻击链：如何 “欺骗” 金库</strong></h3><p>攻击者的执行过程堪称 “完美”。以下是推测的分步逻辑链：</p><p><strong>准备阶段</strong></p><p>攻击者通过金库发起了一系列多调用操作，可能利用了金库与资金池合约间复杂的交互模式。</p><p><strong>触发漏洞</strong></p><p>在多调用序列执行过程中，攻击者很可能触发了一种 “状态不一致”—— 通过不完整的授权校验，操纵金库的内部余额管理系统。</p><p><strong>权限提升</strong></p><p>由于缺失状态验证，一条特权逻辑路径在操作中途被非法调用。这意味着，攻击者在复杂的回调序列中，利用了金库与资金池之间的信任边界。</p><p><strong>状态篡改（致命一步）</strong></p><p>授权漏洞让攻击者得以操纵金库的内部余额追踪系统。</p><p>该漏洞可能允许攻击者通过特权操作，指令金库将自己的内部用户余额设置为一个极高的虚假数值（例如 6500 枚 WETH）。</p><p>而金库由于授权校验被绕过，会乖乖更新内部账本：</p><p><code>internalBalances[攻击者地址] = 6500e18</code> （即 6500 枚 WETH 对应的最小单位数值）。</p><p><strong>收尾阶段</strong></p><p>恶意操作序列顺利完成，金库的内部状态被永久篡改，但表面上看，整个执行过程完全正常。</p><p><strong>常规提取</strong></p><p>在另一笔看似无异常的独立交易中，攻击者调用了标准的<code>exitPool</code> （退出资金池）或提取函数。</p><p><strong>资产转移</strong></p><p>金库检查了已被篡改的账本，确认攻击者拥有 6500 枚 WETH 的 “余额”，随后便尽职尽责地将真实的 WETH 从合约中转移到攻击者的钱包。</p><p>这次攻击并非 “盗窃”，而是 “欺诈”—— 攻击者先骗金库开出一张虚假的存款凭证，再让金库用真实资产兑现这张凭证。</p><h3><strong>“分叉炸弹”：为何 Berachain 暂停了整条 L1 链</strong></h3><p>对研究人员而言，漏洞的影响范围往往比漏洞本身更可怕。这个漏洞存在于 Balancer V2 的核心代码库中，这意味着所有分叉（fork）了 V2 代码的协议，都面临同样的风险。</p><ul><li><strong>Beets Finance：</strong> 被盗走数百万美元资产。</li><li><strong>Berachain：</strong> 情况更为特殊。Berachain 的原生去中心化交易所 BEX，正是 Balancer V2 的分叉版本。这个漏洞不仅存在于链上的某个应用，更存在于其 L1 区块链 DeFi 生态的底层基础设施中。</li></ul><p>因此，Berachain 被迫采取了最极端的措施：暂停整条 Layer 1 区块链。这是唯一能阻止攻击者盗取 BEX 资产、并让验证者有时间通过紧急硬分叉修复应用层漏洞的方法。这一事件也成为 “应用层风险升级为共识层故障” 的典型案例。</p><h3><strong>为何 Balancer V3 安然无恙</strong></h3><p>V3 的 “原子化 BPT 与代币记账模型” 恰好填补了这个漏洞。</p><p>由于所有内部状态变更都在金库内部以原子化方式完成（得益于瞬时记账和统一的 BPT 管理），资金池无法在操作执行过程中通过重入篡改金库余额。</p><p>这种设计升级，印证了 Balancer 在架构上的改进价值 —— 也凸显出审计工作不仅要检查 “函数级逻辑”，更要审视 “系统级状态完整性”。</p><h3><strong>“早有预警”：数周前已指出该风险</strong></h3><p>尽管这次漏洞利用的具体机制具有新意，但本不应令人意外。它所利用的架构缺陷，正是我在10 月 16 日 Balancer AMM 架构分析中强调的风险点。</p><p>任何读过该分析的审计人员或开发者，都应高度警惕这类漏洞。以下是攻击发生前数周就已准确指出的内容：</p><ul><li><strong>风险区域：</strong> V2 的回调机制。该分析明确指出了 V2 架构回调系统中的漏洞风险。尽管当时重点讨论的是已知的重入路径，但核心缺陷完全一致 —— 回调过程中金库与资金池合约间脆弱的信任边界。</li></ul><ul><li><strong>跨合约信任假设的缺陷。</strong> 该分析的核心观点是，V2 设计存在危险的跨合约信任假设。而这次漏洞利用，正是这一观点的 “完美例证”：金库（资产管理者）在复杂操作中信任了资金池逻辑，攻击者则利用这一信任篡改了金库的内部状态。</li></ul><ul><li><strong>V2 与 V3 的安全差异。</strong> 分析中已明确指出，V3 架构是解决该问题的方案 —— 其原子化的代币与 BPT 管理机制，能缓解这类状态篡改风险。事后分析也证实了这一点：V3 资金池完全未受影响。</li></ul><p>这是一个发人深省的教训：安全分析的目标不仅是发现已知漏洞，更要识别架构层面的缺陷。V2 的回调模型本就是已知的薄弱环节，而 11 月 3 日，攻击者只是从我们早已指出的 “未上锁的门” 走了进去。</p><h3>审计视角的核心启示</h3><p>这次漏洞事件为所有 Solidity 开发者和审计人员提供了三个关键教训：</p><ul><li><strong>授权校验必须 “上下文感知”。</strong> 此次漏洞可能表明，为正常操作流程设计的访问控制，在复杂多调用场景中可能失效。关键函数不仅要验证 “谁在调用”，还要验证 “何时调用” 以及 “在何种条件下调用”。</li></ul><ul><li><strong>显式状态机优于隐式假设。</strong> 该漏洞表明，系统中存在未被妥善保护的隐式状态。这类漏洞可通过 “显式状态管理” 避免 —— 确保所有特权操作都会校验当前系统状态。</li></ul><ul><li><strong>分叉审计过的代码，仍需承担风险。</strong> Beets 和 Berachain 团队都分叉了 “久经考验” 的代码，但这一事件提醒我们：分叉并非安全捷径。你会 100% 继承原协议中潜在的漏洞，因此对分叉代码进行独立审计，是必不可少的步骤。</li></ul><blockquote><p>原文：<a href="https://link.segmentfault.com/?enc=xWYF5ANiDmSBMruzOoUPVQ%3D%3D.iJBzodv22ZfMiniMARUgwO256g%2B7uW33tjX2ZCtusQz5J1zzQbQJ5yfeaye%2Bc00o" rel="nofollow" target="_blank">https://www.zealynx.io/blogs/balancer-v2-128m</a></p><p>作者：@ZealynxSecurity</p><p>（OpenBuild 翻译整理，原文有删减）</p></blockquote>]]></description></item><item>    <title><![CDATA[链原生与链扩展 OpenBuild ]]></title>    <link>https://segmentfault.com/a/1190000047383574</link>    <guid>https://segmentfault.com/a/1190000047383574</guid>    <pubDate>2025-11-09 20:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3><strong>引言：区块链架构的双层世界</strong></h3><p>在区块链的世界里，每一条链都像一座精密的城堡，其架构可以清晰地划分为两个层级：<strong>链原生（Native）</strong>  和 <strong>链扩展（Extension）</strong> 。链原生是这条链的“骨骼”和“心脏”，定义了它的核心本质和运行逻辑，包括账户系统（如以太坊的 EOA 账户）、虚拟机（如 EVM）、共识机制、交易池（Mempool）等。这些功能直接嵌入链的协议层，决定了链的主体功能、性能、安全性和不可篡改性。</p><p>与之相对，<strong>链扩展</strong> 则主要指智能合约层。它像城堡上的“阁楼”，允许开发者在链原生基础上构建自定义逻辑，实现无限可能。但正如任何建筑一样，这两个层级并非孤立，而是相互依存、层层递进的关系。理解“链原生与链扩展”的界限与协同，是设计高效区块链的关键。本文将从概念界定、局限性、演进路径，到实际应用，系统探讨这一架构范式。</p><h3><strong>链原生 vs 链扩展：边界与能力</strong></h3><p>区块链的原生功能是“内置的、不可或缺的”，它决定了链的身份和底线性能。例如：</p><p><strong>EOA（Externally Owned Accounts）：</strong> 用户直接控制的账户，支持签名交易，是以太坊货币流通的基石。</p><p><strong>EVM（Ethereum Virtual Machine）：</strong> 执行智能合约的“引擎”，但它本身也是原生的一部分。</p><p><strong>共识机制（如 PoS、PoW）：</strong> 确保全网一致性。</p><p><strong>交易池：</strong> 暂存待打包交易。</p><p>这些功能运行在<strong>协议层</strong>，效率极高，Gas 成本最低，且受全网共识保护。</p><p>链扩展则通过<strong>智能合约</strong>实现，开发者可以部署代码来扩展功能。比如，DeFi 协议、NFT 市场、DAO 治理，都在这一层绽放。但并非所有功能都适合“上浮”到扩展层：</p><p>很多东西可以在智能合约层做，但很多东西不能。</p><p>典型例子是 <strong>MEV（Miner Extractable Value，矿工可提取价值）</strong> 。MEV 源于交易排序的公平性问题，在智能合约层难以根治，因为它涉及交易池的排序和打包逻辑——这些是原生层的核心。真正抗 MEV 需要在<strong>链原生层</strong> <strong>面</strong>重构核心机制。</p><h3><strong>以太坊预编译合约：扩展向原生的“下沉”</strong></h3><p>以太坊的演进完美诠释了“链扩展向原生下沉”的趋势。<strong>预编译合约（Precompiles）</strong>  本质上是将高频、计算密集的扩展功能（如椭圆曲线运算、SHA256 哈希）从 EVM 合约层“硬编码”到原生层。</p><p><strong>早期：</strong> 开发者在合约中实现这些运算，Gas 消耗巨大（例如，Keccak256 需要 30 Gas/字节）。</p><p><strong>优化后：</strong> 预编译将它们下沉到 EVM 原生指令，Gas 降至 3-15，速度提升数倍。</p><p>这不仅是性能优化，更是<strong>架构哲学：将更常用、更接近链本质的功能尽可能下沉到链原生层面。</strong> 未来，L2（如 Optimism）和 Danksharding 将进一步模糊边界，甚至将 Rollup 逻辑原生化。</p><p>但对于<strong>以太坊这样的通用区块链，不停添加原生功能也是不合适的</strong>——它会<strong>牺牲通用性</strong>（绑定特定用例，开发者生态碎片化），并<strong>增加硬分叉频率</strong>（升级成本飙升，社区分裂风险）。<strong>下沉需谨慎</strong>，通用链当以<strong>扩展层为主</strong>，专用链方可大胆原生化。</p><h3><strong>原生与扩展的本质：链的“DNA”与“肌肉”</strong></h3><p>在区块链设计中，<strong>原生功能</strong> 和 <strong>扩展功能</strong> 的区分是哲学层面的根本划分，它决定了链的定位、演化路径和生态边界。</p><p><strong>链原生</strong> 是这条链的<strong>DNA</strong>——它的<strong>本质身份</strong>和<strong>核心操作</strong>。原生功能必须是<strong>内置的、协议级别的、难以更改的</strong>，直接嵌入链的协议代码中，由全网共识强制执行。它回答了最根本的问题： <strong>“这条链是什么？”</strong> 原生层提供<strong>底线性能</strong>（极致速度、低成本）、<strong>最高安全性</strong>和<strong>功能锚定</strong>（定义链的独特价值）。例如，原生功能包括账户模型、基本交易语义、虚拟机指令集等，这些是链“活下来”的必要条件。一旦设计，原生就如DNA般稳定，升级需硬分叉，成本极高。原生不是“功能堆砌”，而是<strong>最小化精华</strong>：每一条原生指令都服务于链的本质，冗余即毒药。</p><p>与之相反，<strong>链扩展</strong> 是<strong>肌肉系统</strong>——对原生本质的<strong>动态加强和丰富</strong>。它通过<strong>智能合约</strong>实现，灵<strong>活、可插拔、社区驱动</strong>，它回答了 <strong>“如何让这条链更强大？”</strong> 这一问题。 扩展层允许开发者在原生基础上叠加复杂逻辑，实现个性化创新，但它<strong>依赖原生</strong>（执行在VM上）、<strong>性能折价</strong>（Gas开销）和<strong>风险自负</strong>（代码漏洞）。扩展不是“随意添加”，而是<strong>忠实于原生本质的放大镜</strong>：它不能改变链的DNA，只能强化它。好的扩展如“插件”和”脚本”，即插即用。</p><p><strong>以以太坊为例：</strong></p><p><strong>本质：货币与金融</strong>。它不是通用计算平台，而是世界的金融心脏。</p><p><strong>原生：</strong> EOA 账户系统，支持余额转移、Nonce 防重放，正是金融的“原子操作”——简单、可靠、全球通用。</p><p><strong>扩展：</strong> 智能合约是对这一本质的加强，如 ERC-20、Uniswap**（自动化做市），它们将货币金融从“转账”扩展到“复杂市场”，但始终服务于原生金融核心。</p><p>简而言之：<strong>原生铸魂，扩展生力</strong>。设计链时，先炼原生（问“是什么”），再筑扩展（问“如何强”）。违背此道，链易成“四不像”——性能低下、安全隐患、生态贫瘠。</p><h3><strong>传统互联网的镜像：Chrome 与游戏的启示</strong></h3><p>“原生与扩展”并非区块链独有，在<strong>传统互联网</strong>中无处不在。</p><p><strong>以 Chrome 浏览器为例：</strong></p><p><img width="723" height="134" referrerpolicy="no-referrer" src="/img/bVdmYNf" alt="image.png" title="image.png"/></p><p>插件生态让 Chrome 从“浏览器”变“超级工具”。</p><p><strong>再以电子游戏为例：</strong></p><p>游戏<strong>本体</strong>是开发商倾力打造的“原生核心”，定义了玩法规则、引擎渲染、关卡设计，确保稳定流畅的体验。<strong>MOD（模组）则是社区玩家的“扩展艺术”，通过官方API添加新地图、自定义技能、剧情支线，甚至重制UI，对本体功能的精准增强。</strong></p><p><img width="723" height="134" referrerpolicy="no-referrer" src="/img/bVdmYNe" alt="image.png" title="image.png" loading="lazy"/></p><p>如<strong>文明系列</strong>（Civilization）：本体是 Firaxis 的“骨架”（回合制策略、科技树<strong>、外交征服），MOD 是社区的“血肉”（数千新文明、单位、地图、平衡补丁）。MOD 让游戏“永生”，销量翻倍，但脱离本体即崩塌——</strong>原生为基，扩展为翼**的完美镜像。区块链专用链设计，当以此为鉴。</p><p>深化观之，不管是 Chrome 还是文明6，原生本体功能的迭代总是缓慢而稳健——它触及核心引擎、兼容性与安全性，每一次更新（如 Chrome 的 V8 引擎升级或 Civ6 的核心 DLC）需数月测试、全球回滚预案，确保零 Bug、跨平台稳定。这正是更核心的体现：原生如“宪法”，定义边界，守护本质。</p><p>反观扩展功能，迭代速度飞快且轻量、创意十足：Chrome 插件一周内可上线千款新作，Civ6 Steam Workshop 日推数百 MOD，从“外星人文明”到“现代核战补丁”，社区如野火燎原，零成本试错。</p><p>原生与扩展间有明显边界：API 接口清晰，扩展不能侵扰原生（如 MOD 无法改 Civ6 的回合计算逻辑），却互补协同——原生提供稳定基石，扩展注入无限活力，共同铸就更丰富的应用系统：Chrome 变“生活 OS”，Civ6 成“永恒帝国模拟器”。区块链专用链，当以此为镜：原生稳如磐石，扩展绽放百花。</p><h3><strong>专用链设计：以 Orderbook DEX 为鉴</strong></h3><p>当下，<strong>通用链</strong>（如 ETH）面临“千链一面”的困境。<strong>专用链（AppChain）</strong>  是未来：<strong>专用开发</strong>，而非全部使用 <strong>EVM链 + Solidity合约</strong> 这种<strong>千篇一律</strong>的方式——这是一种<strong>技术战略层面的偷懒</strong>，会<strong>增加技术债务、阻碍技术创新。重新思考区块链</strong> <strong>，我们需要哪些专有的链原生，以及用什么样的链扩展。</strong></p><p><strong>案例：专用 Orderbook DEX 链</strong></p><p>想象我们开发一条<strong>专为</strong> <strong>高频 Orderbook DEX</strong> 的链，目标是实现<strong>亚秒级撮合、无 MEV 干扰、全球交易者零门槛</strong>。这不是“ETH 上再搭 Uniswap”，而是<strong>从原生重构</strong>，让 Orderbook DEX 成为链的“DNA”——交易公平如空气，性能如闪电。</p><p><strong>链原生定制是灵魂：</strong></p><p><strong>抗 MEV 直击原生，我们原生集成 MEVless 协议</strong><a href="https://link.segmentfault.com/?enc=4ThH%2FT3IUolDM1MvDkdOnw%3D%3D.CYHe6TPFPQfHt64LCiFkArLH9o90NCkliHfm0htDgDWM7JdCnQ%2BuyVoCuA%2F14pct6CI1Cp7omnMJw3X%2FgeG2tjrRXnEkNbrJjvbHGcAavidH8Jlf8wfj571IGT5iXJoUN2esXlls5qAdMS74ueoiHePKP82F9%2FcHhKBtNj6KJ%2FPTA0elv1PNAvQLkvGxqrip" rel="nofollow" target="_blank">（详见）</a>。该协议通过定序区块与执行区块分离，在交易内容可见前即承诺排序：用户先提交交易 Hash + 预付款（固定 Gas + 可选小费），节点基于预付款金额对 Hash 排序并全网共识存入定序区块；随后用户补交易内容，执行区块严格按承诺顺序运行。矿工“盲排序”，杜绝三明治攻击与抢跑——信任最小化、on-chain 强制。这不是合约补丁，而是协议 DNA，适用于 DeFi AppChain，确保订单簿价格纯净、滑点归零。</p><p><strong>账户模型选 UTXO：</strong> 摒弃 账户式模型，转用 UTXO。每个订单如比特币般“原子销毁+创建”，支持海量并行，无状态膨胀，可以极大提升TPS 并省略大量的Gas费。</p><p><strong>原生内置 Orderbook 引擎：</strong> 协议层 直驱限价/市价订单簿、深度图实时查询。交易者无需写一行代码，即可发单、撤单、撮合——DEX 核心“零 Gas 扩展”即就绪。撮合逻辑硬编码，亚毫秒延迟，媲美 CEX。</p><p><strong>链扩展：</strong> 为吸引华尔街交易者和 HFT 做市商，可以部署 <strong>Python 友好 VM</strong>。他们用 Python 编写<strong>策</strong> <strong>略插件</strong>：高频套利 bot、动态 MM 算法。一键部署，即调用原生 Orderbook API，性能无损。这些插件如“DEX App Store”，社区狂欢，从散户到机构，全策略覆盖。扩展不抢原生风头，只锦上添花。</p><h3><strong>原生为基，扩展为翼</strong></h3><p>“链原生与链扩展” 是区块链架构的<strong>黄金法则：原生铸就本质，扩展驱动创新</strong>。在 AppChain 浪潮中，将会以专用链为主导：你的金融链、游戏链、社交链，将如何定义其“原生”与“扩展”？</p><blockquote><p>文章来源：<a href="https://link.segmentfault.com/?enc=8Xc%2F5Wv9Gver4k5dNw%2F4jg%3D%3D.TGlsh6PEUABxxGG8c9LqARiQ%2BCvnqR18f5LEALs9djkGaMV6udOfUIUC58aLC2uIdyIYXa37%2FN70blm6NdW8Qw%3D%3D" rel="nofollow" target="_blank">https://lawliet-chan.github.io/2025/10/28/appchain/</a></p><p>作者：@Lawliet Chan</p></blockquote>]]></description></item><item>    <title><![CDATA[亚洲力量集结布宜诺斯艾利斯，点燃 Dev]]></title>    <link>https://segmentfault.com/a/1190000047383591</link>    <guid>https://segmentfault.com/a/1190000047383591</guid>    <pubDate>2025-11-09 20:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdmYNp" alt="image.png" title="image.png"/></p><p>在阿根廷 Devcoomect 期间，由 <strong>OpenBuild、Coset 和 Invisible Garden</strong> 共同发起的 <strong>“Asia Web3 Day @Devconnect Argentina”</strong> 将于 11 月 20 日晚上在布宜诺斯艾利斯举办。这场聚会秉承开源与开放协作的核心精神，旨在推动亚洲与全球开发者社区之间的深度对话与创新合作，一同探索 Web3 生态的未来。</p><h3><strong>让亚洲 Vibes 席卷全球</strong></h3><p>Asia Web3 Day 的初衷是希望把亚洲多样且活跃的开发者生态与全球区块链社区连接起来。早在 2025 年举行的 EDCON 大会上，OpenBuild 就曾帮助展示中国开源开发者社区，并促成了中日交流。基于这种合作的成功，这次活动希望将视野从亚洲扩展到全球，让来自韩国、日本、印度、泰国、越南、马来西亚、新加坡、中国及其他地区的以太坊生态建设者汇聚一堂，在 Buenos Aires 这样的世界城市碰撞创意、分享经验。</p><p><img width="723" height="467" referrerpolicy="no-referrer" src="/img/bVdmYNq" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="467" referrerpolicy="no-referrer" src="/img/bVdmYNr" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="468" referrerpolicy="no-referrer" src="/img/bVdmYNs" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="467" referrerpolicy="no-referrer" src="/img/bVdmYNt" alt="image.png" title="image.png" loading="lazy"/></p><p>Devconnect Argentina 本身就是以太坊社区的一场盛会，被称为“以太坊世界博览会”（Ethereum World’s Fair）。主办方指出，这个为期六天的博览会在布宜诺斯艾利斯的 La Rural 会场举行，预计有超过 1.5 万人参与，展示包括应用演示、现场体验、社区中心等环节。因此，作为其中的 side event 之一，Asia Web3 Day  @Devconnect Argentina 旨在更大范围内促进 APAC**（亚太地区）和全球开发者之间的协同和连接。</p><h3><strong>主要议题与亮点</strong></h3><p>活动将通过演讲、互动讨论和晚间派对等多种形式，探讨以下议题：</p><p><strong>亚洲开发者生态的成长与演进</strong></p><p>分享各国（如中国、印度、韩国、日本、泰国、越南、马来西亚、新加坡）近年来在区块链领域的创新实践，探讨社区建设和开发者教育的经验。</p><p><strong>全球项目进军亚太地区的经验</strong></p><p>聆听来自全球项目（包括去中心化金融、基础设施平台、NFT 等）的代表，介绍他们如何进入并在亚太市场成功发展，以及在不同文化、法规环境下的挑战与策略。</p><p><strong>亚洲团队走向全球的“出海手册”</strong></p><p>面向亚洲团队，分享如何参与全球开源项目、如何构建跨国合作伙伴关系以及如何扩展国际市场。</p><p><strong>开放式交流与娱乐</strong></p><p>除了精彩的分享和讨论环节，我们还安排了 DJ 表演和自由交流的 Afterparty**，通过轻松的氛围，大家可以在互动中建立连接。</p><p><img width="723" height="530" referrerpolicy="no-referrer" src="/img/bVdmYNu" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="530" referrerpolicy="no-referrer" src="/img/bVdmYNv" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>时间&amp;报名链接</strong></h3><p><strong>日期：</strong> 2025 年 11 月 20 日 7:00 - 12:00 pm</p><p><strong>地点：</strong> 阿根廷，布宜诺斯艾利斯</p><p><strong>报名链接：</strong> luma.com/oo7omi0t </p><p>💬 加入 TG 群与我们互动交流：<a href="https://link.segmentfault.com/?enc=J2pTSQunIY4ZKJyxmMoiGw%3D%3D.Bea9FpPonOhcUGgWEYAoCnK3S66ct55Lkp%2BFhdP1RWg%3D" rel="nofollow" target="_blank">https://t.me/+moBO1oWppk5iN2Zl</a></p><h3><strong>主办与合作方</strong></h3><p><img width="723" height="596" referrerpolicy="no-referrer" src="/img/bVdmYNC" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>亚洲 × 全球协作的下一个篇章</strong></h3><p>这场聚会不仅是一次简单的社交活动，更是一个促进交流、开放合作的平台。随着以太坊社区不断发展，亚洲在全球范围内扮演着越来越重要的角色：</p><p><strong>人才与项目汇聚</strong></p><p>韩国、日本、印度、中国、越南、泰国、新加坡等拥有大量开发者和创业团队，通过活动可以建立跨国合作网络。</p><p><strong>文化桥梁作用</strong></p><p>不同地区的参与者能够通过面对面的交流理解彼此的文化差异、政策环境和市场需求，为未来合作打下基础。</p><p><strong>全球开放协作精神</strong></p><p>Build Open, Go Global，意味着去中心化开发者社区的核心价值，即通过开源协作推动全球创新，让技术真正服务于全人类。</p><p>正如 Devconnect 官方描述的那样，这是一场汇聚真实应用、实践项目和社区力量的“以太坊世界博览会”。Asia Web3 Day Party 作为其中的重要组成部分，将带领亚洲开发者与全球其他地区之间的建立连接，推动更多跨界合作和创新。</p><h3><strong>赞助招募</strong></h3><p><strong>Asia Web3 Day @Devconnect Argentina 正在招募赞助伙伴！</strong></p><p>我们诚邀来自全球的项目方、品牌和社区共同加入，在布宜诺斯艾利斯这个以太坊生态的核心舞台，展示你们的愿景与创新。</p><p><strong>作为赞助方，你将获得：</strong></p><p><strong>✨ 全球曝光机会</strong></p><p>面向来自世界各地的开发者、项目团队和投资者，展示你的品牌影响力。</p><p><strong>🤝 社区连接</strong></p><p>与来自亚洲及全球的优秀 Web3 社区建立长期合作关系。</p><p><strong>🎤 活动发声机会</strong></p><p>参与主题分享、产品展示或互动环节，直接与开发者对话。</p><p><strong>🎟️ 定制权益</strong></p><p>我们提供多种赞助等级（如联合主办、品牌支持、体验展位、Afterparty 合作等），可根据品牌定位灵活定制。</p><p>我们相信，开放协作是 Web3 最核心的精神。</p><p>欢迎与你携手，在 Devconnect 的舞台上，让亚洲的创新力量与全球社区共振。</p><p><strong>📞 Telegram 联系方式：@callmeianx</strong></p><p>联系我们了解详细赞助方案。</p>]]></description></item><item>    <title><![CDATA[智能招聘革新：破解校招低效困局的核心方案]]></title>    <link>https://segmentfault.com/a/1190000047383263</link>    <guid>https://segmentfault.com/a/1190000047383263</guid>    <pubDate>2025-11-09 19:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>智能招聘革新：破解校招低效困局的核心方案<br/>每年校招季，大型企业往往要面对数千份甚至上万份简历的 “轰炸”。传统招聘模式下，HR 团队深陷重复性工作，将大量时间耗费在简历初筛与首轮面试中，不仅效率低下成为普遍痛点，更可能因人工筛选的主观性、疏忽性，让顶尖人才与企业失之交臂 —— 这既是人力资源的无效损耗，更是企业长期竞争力的隐性流失。如何用智能手段打破这场低效消耗战，成为企业招聘转型的关键命题。</p><p>AI 招聘智能体：重构招聘的精准与体验双重标准<br/>面对传统招聘 “低效、主观、成本高” 的三大顽疾，AI 招聘智能体以技术革新为突破口，从精准评估与候选人体验两大核心维度，重塑招聘行业标准，其价值已得到西门子中国、太平保险、阿里巴巴国际、招商银行、浙江大学等上千家顶尖企业与高校的实践验证。<br/>科学决策：让招聘评估告别 “凭感觉”<br/>招聘的核心是 “选对人”，而精准的评估体系是科学决策的基础。AI 招聘智能体的打分结果并非 “黑箱操作”，而是经过严格的 “背靠背” 人机对比实验验证，同时通过效标效度与重测稳定信度的心理学双重考验，评估结果可直接作为招聘决策的可靠依据。第六代 AI 面试智能体的迭代升级，更巩固了其在行业内的领先地位。<br/>精准性贯穿招聘全流程的关键环节：<br/>•一问多能：单道题目可同步评估多项胜任力，无缝衔接 HR 初筛与技术复试两大环节，让评估效率提升超 50%；<br/>•自由追问：基于候选人的回答即时生成针对性问题，如同资深面试官般深挖能力细节，确保核心竞争力无所遁形；<br/>•简历深度挖掘：自动抓取简历关键信息与模糊疑点，生成递进式提问，既能有效规避信息造假，也避免因人工主观疏忽错失良才；<br/>•全维度考察：既覆盖沟通、协作等通用能力评估，也能针对编程、算法、工程、财务等专业领域精准出题，在解放 HR 精力的同时，减轻专业面试官的负担。<br/>人文体验：让面试成为雇主品牌加分项<br/>生硬、机械的传统 AI 面试往往成为优秀人才的 “劝退项”，而新一代 AI 招聘智能体以 “拟人化交互” 为核心，将面试环节转化为展示雇主品牌的重要窗口。<br/>•懂情绪的智能引导：精准捕捉候选人的语速变化、情绪波动与表达潜台词，像真人 HR 一样进行沟通引导，帮助候选人缓解紧张情绪，充分展示真实实力，避免因临场发挥失常造成的人才流失；<br/>•无断点流畅交互：无需候选人机械点击 “开始 / 结束答题”，系统可自动识别对话状态并实现无缝衔接，全程交流如真人沟通般自然顺畅；<br/>•沉浸式视觉呈现：语音与虚拟人像口型匹配精度大幅提升，嘴型开合与语速节奏精准同步，彻底摆脱传统 AI 面试 “纸片人” 带来的虚假感与疏离感；<br/>•多轮对话答疑：候选人可随时针对职位详情、公司福利等问题进行咨询，AI 能准确、全面地解答疑问，让候选人在面试过程中深入了解企业，有效提升后续入职意愿。<br/>全流程自动化：拓展招聘能力边界<br/>AI 招聘的价值不止于面试环节的优化，更在于对整个招聘流程的智能再造。AI 人才寻访智能体具备 “看懂简历、找对人才、有效沟通” 的核心能力，可在无需人工干预的情况下，独立完成从简历筛选、初步沟通、简历回收至系统同步的完整流程，将招聘初筛效率提升 10 到 100 倍。<br/>其核心优势体现在全流程的智能化与便捷性：即启即用的操作模式，无需复杂部署；智能筛选功能精准匹配岗位需求；动态沟通以拟人化交互拉近与候选人的距离；全覆盖回复确保无人才遗漏；自动同步至企业 ATS 系统，实现数据无缝对接。这一模式不仅在操作层面极致提效，更通过大模型技术，将招聘中的 “经验型判断” 升级为 “数据型决策”，让招聘过程更具科学性与可追溯性。<br/>智能招聘：企业人才竞争的新赛道<br/>在人才竞争日益激烈的当下，传统招聘模式的低效与局限已成为企业发展的掣肘。AI 招聘智能体通过精准评估、优质体验与全流程自动化，从根本上解决了校招中的核心痛点，为企业节省人力成本、提升招聘质量、强化雇主品牌提供了切实可行的解决方案。对于寻求招聘转型的企业而言，借助经过市场验证的智能工具，重构招聘流程，已成为把握人才机遇、增强核心竞争力的必然选择。</p>]]></description></item><item>    <title><![CDATA[【URP】Unity[后处理]运动模糊M]]></title>    <link>https://segmentfault.com/a/1190000047383323</link>    <guid>https://segmentfault.com/a/1190000047383323</guid>    <pubDate>2025-11-09 19:04:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=XApz2w37yqg39EfoZKdOtg%3D%3D.5UCrgffut3KSzyjSsvDrezbSInHg%2FB%2F%2FbrPgT5%2BZD0zykPA9K9RLn7LzjMBtSD%2BZ0itVxwPT0XF65hogrGztYRocIdvXtQRy56OIt4z0qVvYyoiWHX6lrImDr%2FirO%2BI19qLq2Z46%2B5P7nSCGfNbTLCgzVsF%2Bbz1WkEO1RQthbpMsvPkKX9J3lznAYSocQk2Nul40FJUlG1qRsuxLiULUtdrIxnqZMs5c5xi6ffzyQ4I%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong></blockquote><h2><strong>Motion Blur 概念与作用</strong></h2><p>Motion Blur（运动模糊）是一种模拟真实相机在拍摄快速移动物体或自身移动时产生的模糊效果的后处理技术。它通过模糊图像中运动物体的轨迹，增强动态场景的真实感和速度感。</p><p>在游戏开发中，Motion Blur 主要有以下用途：</p><ul><li>增强速度感和动态效果，特别是在赛车、动作类游戏中</li><li>弥补低帧率带来的不流畅感，使30帧画面看起来更平滑</li><li>增加电影感和沉浸式体验</li><li>模拟真实相机的物理特性，提高场景真实度</li></ul><h2><strong>发展历史</strong></h2><p>Motion Blur 技术的发展经历了几个阶段：</p><ul><li>‌<strong>帧模糊Frame Blur</strong>‌：早期最简单的实现方式，通过混合当前帧与前一帧图像来模拟模糊效果。这种方法简单但不够真实，无法对不同物体应用不同程度的模糊。</li><li>‌<strong>位置重建Position Reconstruction</strong>‌：通过深度缓冲区重建屏幕空间位置，结合前一帧的变换矩阵计算像素位移。这种方法适合静态场景的相机运动模糊。</li><li>‌<strong>速度缓冲Velocity Buffer</strong>‌：现代主流方法，为每个像素存储运动矢量信息，根据速度方向进行模糊处理。URP和HDRP都采用这种实现方式。</li><li>‌<strong>优化算法</strong>‌：如UE4采用的velocity flatten和tile gather/scatter技术，通过分块处理提高性能。</li></ul><h2><strong>URP 中的实现原理</strong></h2><p>URP 中的 Motion Blur 实现基于速度缓冲技术，核心流程如下：</p><ul><li>记录前一帧的观察矩阵和投影矩阵</li><li>计算当前帧与前一帧相同位置的位置差，得到速度方向</li><li>沿着速度方向进行多次采样，混合结果产生模糊效果</li><li>通过质量参数控制采样次数，平衡性能与效果</li></ul><p>URP 的实现相比传统方法更加轻量，主要针对相机运动模糊进行了优化。</p><p>Unity URP中的MotionBlur（运动模糊）效果通过模拟真实相机在曝光时间内物体快速移动或相机自身运动导致的图像模糊现象实现。其核心原理基于速度缓冲区和历史帧混合技术</p><h3><strong>物理原理基础</strong></h3><p>运动模糊源于现实相机曝光期间物体位置变化导致的像素累积效应。当快门开启时，快速移动的物体会在传感器上留下连续轨迹，表现为模糊效果。在URP中，这一过程通过计算物体或相机在相邻帧间的运动矢量（Velocity Buffer）来模拟。</p><h3><strong>技术实现流程</strong></h3><ul><li><p>‌<strong>速度缓冲区生成</strong>‌</p><p>URP通过渲染物体的运动矢量到缓冲区，记录每个像素在屏幕空间中的位移。这需要启用"Motion Vectors"选项，并依赖深度纹理重建世界坐标。例如，Shader中会使用当前帧与上一帧的视图-投影逆矩阵（<code>_InverseVPMatrix</code>和<code>_PreInverseVPMatrix</code>）计算像素位置变化。</p></li><li><p>‌<strong>模糊计算</strong>‌</p><p>采用多采样点混合策略：根据速度矢量方向，对当前像素及其邻近像素进行加权平均。采样数（Sample Count）和质量预设（Quality）直接影响模糊平滑度和性能消耗。示例公式如下：</p><p>S(t)=(1−α)⋅x(t)+α⋅S(t−1)</p><p>其中α控制历史帧权重，较低值（如0.1）会产生更长拖尾效果。</p></li><li><p>‌<strong>性能优化</strong>‌</p><ul><li>‌<strong>钳制参数Clamp</strong>‌：限制相机旋转产生的最大速度值（默认0.05，屏幕比例），避免极端位移导致性能下降。</li><li>‌<strong>强度控制Intensity</strong>‌：0-1范围调节模糊程度，实际通过缩放速度矢量长度实现。</li></ul></li></ul><h3><strong>示例配置步骤</strong></h3><p>在URP中启用运动模糊需通过Volume系统：</p><ol><li>创建Volume对象并添加<code>Motion Blur</code>覆盖。</li><li><p>关键参数设置示例：这会产生强烈但受控的模糊效果，适合高速运动场景。</p><blockquote>Quality: High<br/>Intensity: 0.8<br/>Clamp: 0.1</blockquote></li></ol><h3><strong>与HDRP实现差异</strong></h3><p>URP仅支持相机运动模糊，而HDRP还支持物体独立运动模糊，并能分离处理旋转/平移分量（通过Camera Clamp Mode）。两者均依赖体积系统，但HDRP提供更细粒度的速度阈值控制（Minimum/Maximum Velocity）。</p><p>完整实现可参考GitHub上的URP运动模糊Shader工程，其中包含世界坐标重建和线性深度处理等关键技术点</p><h2><strong>完整 URP 实现示例</strong></h2><h3>URP Motion Blur 设置步骤</h3><h4><strong>项目准备</strong></h4><ul><li>确保项目使用URP管线</li><li>创建URP Asset（如果尚未创建）</li><li>在Edit &gt; Project Settings &gt; Graphics中分配URP Asset</li></ul><h4><strong>启用后处理</strong></h4><ul><li>在URP Asset的Renderer列表中找到使用的Renderer</li><li>勾选"Post-processing"选项</li></ul><h4><strong>相机设置</strong></h4><ul><li>选择主相机</li><li>在Inspector中勾选"Post Processing"</li></ul><h4><strong>添加Volume</strong></h4><ul><li>在场景中创建空GameObject</li><li>添加Volume组件</li><li>点击"New"创建Profile</li><li>在Profile中添加Motion Blur效果</li></ul><h4><strong>调整参数</strong></h4><ul><li>根据需要调整Quality、Intensity和Clamp参数</li><li>测试不同场景下的效果表现</li></ul><h2><strong>参数详解与实际用例</strong></h2><p>URP Motion Blur 的主要参数及其应用场景</p><h3><strong>Quality（质量）</strong></h3><ul><li>‌<strong>含义</strong>‌：控制效果的质量级别，影响采样次数和精度</li><li>‌<strong>取值范围</strong>‌：低/中/高/自定义</li><li>‌<strong>用例</strong>‌：移动平台建议使用"低"或"中"，PC平台可使用"高"</li></ul><h3><strong>Intensity（强度）</strong></h3><ul><li>‌<strong>含义</strong>‌：控制模糊效果的强度</li><li>‌<strong>取值范围</strong>‌：0（无模糊）到1（最大模糊）</li><li>‌<strong>用例</strong>‌：赛车游戏高速移动时可设为0.8-1.0，一般场景0.3-0.5</li></ul><h3><strong>Clamp（钳制）</strong></h3><ul><li>‌<strong>含义</strong>‌：限制相机旋转产生的最大速度长度</li><li>‌<strong>取值范围</strong>‌：0-0.2（屏幕比例）</li><li>‌<strong>用例</strong>‌：防止快速旋转相机时产生过度模糊，默认0.05适合大多数情况</li></ul><h3><strong>Sample Count（采样数）</strong></h3><ul><li>‌<strong>含义</strong>‌：计算模糊时的采样点数（仅在自定义质量时可用）</li><li>‌<strong>取值范围</strong>‌：通常4-32</li><li>‌<strong>用例</strong>‌：高质量效果需要更多采样，但性能消耗更大</li></ul><h2><strong>实际应用建议</strong></h2><h3>‌<strong>性能优化</strong>‌：</h3><ul><li>在移动设备上，使用较低的质量预设和强度值</li></ul><h3>‌<strong>艺术控制</strong>‌：</h3><ul><li><p>通过调整参数创造不同的视觉效果，如：</p><ul><li>轻微模糊（强度0.2-0.3）增强电影感</li><li>强烈模糊（强度0.8-1.0）表现极速运动</li></ul></li></ul><h3>‌<strong>场景适配</strong>‌：</h3><ul><li>根据场景运动幅度调整参数，静态场景可完全禁用</li></ul><h3>‌<strong>组合效果</strong>‌：</h3><ul><li>与其他后处理效果如Bloom、Chromatic Aberration组合使用，增强整体视觉冲击</li></ul><hr/><blockquote><a href="https://link.segmentfault.com/?enc=gEkpBVK2KQ3SRWIOGSZ8yA%3D%3D.O0m5Fs4HmfDFSLnUL2VmyOd4cgBfzJJ%2FVvmoDDIjCLWnXCC1reEYILNN2lPy85p3jrqmWyXuMsqFExoQwMfcFFr0iyQf5LWvKpHrJRpr4IX%2BDVwtJLfSWtCzzOvw53a2hMj3ZIeHOkbJrLWiD8H9svm42SLPJ%2Bh0RviXIEGpbqJjj2oXWZkXQgz2nibjbQVCA3q1OwwkXLK2AXl9dvpE%2BDQrFxwjDUnUK6vrLMI264A%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[世界十大邮箱有哪些？2025榜单速览 遭]]></title>    <link>https://segmentfault.com/a/1190000047383339</link>    <guid>https://segmentfault.com/a/1190000047383339</guid>    <pubDate>2025-11-09 19:03:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年全球活跃邮箱账户已突破63亿，Gmail、Outlook、Yahoo三分天下的格局正在被打破——主打“无广告+域名邮箱+极简注册”的Zoho邮箱以1800万企业级客户跻身前三。无论你是外贸SOHO、跨境卖家还是远程团队，选邮箱不再只是“能发就收”，而要衡量安全、品牌、合规与生态。下文用一张榜单+深度对比，带你速览世界十大邮箱的最新排名与独门绝技，并重点拆解Zoho邮箱为何成为跨国协作的“隐形冠军”。<br/><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdmYJy" alt="" title=""/><br/>一、2025年全球十大邮箱排行总览<br/>全球邮箱服务市场日益成熟，但仅有少数产品能凭借强大生态和服务能力长期占据头部地位。2025年，根据全球用户活跃度、企业合作占比、技术安全性等多维指标，权威IT媒体统计出如下国际邮箱排行榜：</p><p>排名    邮箱服务    主要客户规模    特色优势<br/>1    Gmail    20亿+    海量存储、AI反垃圾、安全性高<br/>2    Outlook（Hotmail）    7亿+    微软生态集成、企业办公广泛<br/>3    Zoho Mail    1800万企业级    域名邮箱、多元生态、隐私保护<br/>4    Yahoo Mail    2.5亿    早期品牌、稳定可靠<br/>5    Apple iCloud Mail    数亿    苹果生态闭环、安全加密<br/>6    腾讯QQ邮箱    数亿    本地化服务、微信深度集成<br/>7    网易163邮箱    丰富用户积累    易用性强、邮箱号选择丰富<br/>8    Yandex.Mail    东欧最大    多语言支持、空间大<br/>9    ProtonMail    数百万    端到端加密、安全隐私<br/>10    GMX Mail    千万级    功能全面、免费空间大<br/>邮箱服务全球普及度及统计数据<br/>如今，全球活跃邮箱账户总数已超过63亿。欧美地区以Gmail和Outlook为主导，亚洲市场则更多依托QQ邮箱、网易邮箱等本土品牌。值得注意的是，Zoho邮箱不仅在欧美企业群体中保持强劲增长，其便捷的注册步骤及支持自定义域名邮箱的服务也吸引了大量创业者与远程团队，客户覆盖180多个国家和地区。</p><p>二、主流邮箱服务深度对比<br/>全球主要邮箱产品在安全协议、功能集成、兼容性等方面各有千秋。下文以行业主流为例，从功能与应用场景完整对比：</p><p>Gmail邮箱：全球覆盖与个人隐私的权衡<br/>Gmail由Google推出，现在已服务全球逾20亿用户。其优势在于超大存储空间、高效反垃圾邮件机制及AI智能化体验。用户可一键切换多账号，Google Workspace集成日历、云盘与视频会议，是企业与个人混用的首选。但邮件内容分析及广告推送机制，令部分注重隐私的用户心存疑虑。</p><p>Outlook邮箱：企业办公集成但定制性有限<br/>Outlook紧密集成于微软生态，深受企业和机构用户青睐。邮件与日历、联系人、Teams等协作工具原生互通，提升办公效率。支持Exchange自部署，企业IT部门可灵活管理。但对个人用户而言，UI复杂度高、深层定制能力有限，在跨平台应用体验方面略逊于Gmail。</p><p>Yahoo邮箱：经典品牌的稳定与发展困境<br/>身为邮箱“元老”，Yahoo Mail以坚固的账户安全和长久在线时长取胜。其垃圾邮件识别、附件管理等功能实现行业标准，界面简洁易上手。但近年来创新乏力，移动端体验与国际化扩展均有所滞后，新一代用户转向比例明显。</p><p>苹果iCloud邮箱：生态闭环和跨平台局限<br/>Apple iCloud Mail作为苹果体系的原生邮箱，主打终端加密和私密性保障。其与iMessage、日历、备忘录等应用无缝配合，对iPhone、iPad用户极为友好。局限在于，只能完整依托苹果生态，跨平台访问及第三方扩展能力有限。</p><p>腾讯QQ邮箱 &amp; 网易163邮箱：中国主流的本地化体验瓶颈<br/>QQ邮箱、网易邮箱深受中国大陆用户欢迎。QQ邮箱凭借微信、QQ社交端强入口优势，支持多重验证与大文件发送，但海外访问稳定性一般。网易163邮箱则以多账号、多终端支持及邮箱号丰富著称，适合学生及自由职业群体。海量国内用户背后，二者在国际邮件通信与功能创新方面略显滞后。</p><p>Zoho邮箱：全球多元化及中小企业最佳解决方案<br/>作为唯一入围前三的多语言全球邮箱，Zoho邮箱专注于为企业级客户和全球远程合作团队提供安全、易用、极具拓展性的邮局服务。其1800万企业级客户遍布五大洲，拥有极高满意度。</p><p>(1) Zoho邮箱的安全性和隐私保护<br/>Zoho邮箱所有数据采用AES 256位加密，且支持端到端传输加密。其反垃圾邮件策略与国际ISO 27001安全标准紧密接轨，不定期安全漏洞赏金计划进一步增强数据安全感。尤其对企业用户，Zoho坚决不嵌入广告，全部邮件数据完全自主管理，实现企业级隐私保障。</p><p>(2) Zoho邮箱的功能生态与扩展能力<br/>Zoho邮箱不仅提供云端邮件存储、日历、任务、云盘，还能一键集成CRM、项目管理、视频会议、团队办公等20+业务工具，形成完整的企业数字化平台。自定义域名邮箱能力、丰富API接口，支持扩展第三方插件，满足复杂业务需求。高效的注册步骤确保中小企业几分钟内上线专属邮箱。</p><p>(3) Zoho邮箱在跨国企业和远程办公的优势<br/>Zoho在全球180+国家本地部署节点，具备极强的跨境通信和数据合规性。支持多语言界面、一键迁移、移动端邮件推送。无论是国际贸易、跨国协作还是远程办公，都能实现便捷沟通与高效稳定的运营体验。</p><p>(4) 为什么越来越多用户和企业选择Zoho邮箱？<br/>1800万企业级客户信赖Zoho，源于其专业安全、高可用、低门槛和强生态。不仅为初创和中小企业量身定制了灵活套餐，还通过域名邮箱、移动办公与多维管理，让企业品牌形象、沟通效率和全球合作力迅速提升。便捷的注册步骤与24小时在线服务也令企业省心托管。</p><p>三、选择全球邮箱服务时需警惕的问题<br/>尽管全球主流邮箱产品在数据保护与便捷性上不断升级，但用户和企业在选型时，仍需注意如下几点风险与隐患。</p><p>邮箱服务数据安全隐患<br/>部分邮箱平台存在历史数据泄漏问题，邮件服务商可能出于技术或政策原因存储、分析用户通信内容。建议重点关注采用国际安全认证的数据加密和权限分级机制，选择如Zoho邮箱等无广告纯净企业平台。</p><p>广告骚扰和用户隐私泄漏风险<br/>Gmail、Yahoo等服务免费模式下，常见邮件内容分析与广告推送，影响用户隐私。而主打企业订阅制的Zoho邮箱、ProtonMail等平台，则坚守零广告政策和隐私先行，适合对信息安全有高需求的用户和机构。</p><p>多平台兼容与迁移难度<br/>部分“闭环”邮箱生态在账号迁移、API集成和第三方兼容方面存在壁垒。企业邮箱或远程协作场景下，推荐选择支持开放协议、全球节点和一键再迁移的产品，例如Zoho邮箱。</p><p>售后与技术支持能力差异<br/>主流邮箱厂商在全球服务口碑、问题响应时效及本地化支持上差异明显。Gmail、Outlook依托母公司技术优势，响应较快。Zoho邮箱则凭可靠的24小时客户服务、定向专属工程师与中文本地支撑，成为中小企业及全球客户首选。</p><p>结语<br/>邮箱选得对，业务少受罪。Zoho邮箱用“全球前三”的成绩证明：AES-256端到端加密+零广告+16个自建节点，让邮件像本地聊天一样秒达；一个域名即可生成品牌邮箱，3分钟完成注册，立刻拥有<a href="mailto:name@yourbrand.com" target="_blank">name@yourbrand.com</a>的专业身份；免费试用15天，按需订阅无隐藏费用。Zoho邮箱，今天注册，明天就让客户记住你的品牌，而不是一串免费邮箱后缀！</p>]]></description></item><item>    <title><![CDATA[x402 生态系统：Web3 与 AI ]]></title>    <link>https://segmentfault.com/a/1190000047383372</link>    <guid>https://segmentfault.com/a/1190000047383372</guid>    <pubDate>2025-11-09 19:02:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3><strong>x402 生态系统简介</strong></h3><p>在 Web3 与 AI 飞速融合的领域中，x402 协议的发展备受关注。该协议由 Coinbase 于 2025 年初推出，重新启用了长期未被使用的 HTTP 402 “需要付费” 状态码，支持通过 USDC 等稳定币实现无缝的程序化微支付。这一协议并非单纯的技术尝试，而是 “智能体经济”（agentic economy）的基础层，能让 AI 智能体在无需人工干预、无需 KYC 且脱离传统订阅模式的情况下自主完成交易。</p><p>如今，x402 生态系统已从小众实验发展为涵盖服务商（facilitators）、智能体、分析工具和基础协议的活跃网络。在 Coinbase、谷歌、Visa、Cloudflare、Vercel 等行业巨头的支持下，它解决了 AI 与 Web3 融合过程中的核心痛点 —— 如何针对低频次、高多样性的交互，实现 API 及服务的实时变现。这一点对 AI 智能体尤为重要，因为它们需 24 小时不间断运行，且经常会调用不可预测的外部服务。</p><p>x402 生态系统的发展速度十分迅猛。自协议在 GitHub 开源以来，采用率大幅提升，x402scan 等分析平台的数据显示了显著增长。过去 90 天内，生态系统完成 5.24 万笔交易（增长 93.3%），总交易量达 52.11 万美元（增长 4120.38%），买家数量 991 人（增长 279.6%），卖家数量 244 人（增长 75.5%）。Base、Solana、Polygon、Algorand 等公链已完成 x402 集成，支持跨链微支付。目前生态内项目涵盖支付服务商、AI 驱动智能体等多个领域，AI 智能体的广泛应用成为推动生态发展的核心动力。</p><p>本文旨在为 Web3 和 AI 领域从业者提供 x402 生态的全面解读，包括当前发展现状、核心项目、功能特性、性能数据及未来机遇。我们将生态系统划分为服务商、智能体、分析工具、基础协议四大核心类别，结合 x402scan、Dune Analytics 及链上数据展开分析。读完本文，你不仅能掌握 x402 的技术原理，还能清晰把握该领域的投资方向与创新潜力。</p><h3><strong>发展背景与生态系统整体演进</strong></h3><p>x402 的起源可追溯至 Coinbase 对 “互联网原生支付层” 的愿景。2024 年底，Coinbase 基于 EIP-3009（签名授权标准）和 Base 的 Layer 2 扩容方案（支持毫秒级结算），提出了这一协议。其核心逻辑是：服务器可通过 402 状态码响应 API 请求，并附带支付细节（如 USDC 计价金额、收款地址），客户端（通常为 AI 智能体）则能通过智能钱包自主完成支付。</p><p><strong>关键里程碑</strong></p><ul><li>2025 年 1 月：在 GitHub 开源发布，Coinbase 开发者平台（CDP）完成首批集成。</li><li>2025 年 3 月：与谷歌（通过智能体支付协议 2.0，即 AP2）、Visa（通过代币化资产平台，即 TAP）、Cloudflare（通过按次计算付费，即 PPC）达成合作。</li><li>2025 年 6 月：黑客松系列活动催生 200 多个项目，包括 Questflow 等智能体项目及 x402scan 等分析工具。</li><li>2025 年 9 月：Coinbase 与 Cloudflare 联合成立 x402 基金会，负责协议的独立治理，推动 x402 在 Web3 各公链间的标准化。</li><li>2025 年 10 月：发布生态系统图谱（如近期开发者图谱所示），显示生态内已有 50 多个活跃项目，Algorand 等公链通过 Plausible 完成集成。</li></ul><p><strong>生态发展数据与挑战</strong></p><p>从发展数据来看，x402 生态增长势头强劲。据 DefiLlama 数据，Base 公链的交易量领先，Solana 和 Polygon 紧随其后。Gloria AI、Questflow 等 AI 智能体项目成为生态采用的核心驱动力，其中 Questflow 的交易量在生态内位居前列。</p><p>当前生态面临两大挑战：一是服务商竞争激烈（现有 15 + 个选项，多数免费）；二是服务提供商数量不足（仅 30 + 个集成服务，且以社区驱动为主）。但这也为开发者创造了机遇：新智能体项目的准入门槛低，且将非原生服务封装为 x402 兼容服务的利润率较高。</p><p>经济层面，x402 颠覆了传统订阅模式。传统 SaaS** 中，70%-80% 的配额因闲置被浪费；而 x402 的按次付费模式，既能为轻度用户节省成本，又能实现闲置算力的变现。据预测，到 2030 年 Web3 市场规模有望达到 414.5 亿美元，x402 有望在 AI 基础设施领域占据一席之地。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdmYJR" alt="image.png" title="image.png"/></p><h3><strong>Agents：x402 生态中的 AI 执行载体</strong></h3><p>智能体借助 x402 完成自主任务（从数据抓取到交易执行），是 “智能体经济” 的核心体现。</p><p><strong>Questflow</strong></p><ul><li>功能特性：支持多智能体工作流编排，通过 x402 实现 API 变现。核心能力包括：群体智能（swarm intelligence）处理复杂任务（如市场分析）、通过路由集成 20 + 个大语言模型（LLM**）。依托无代码 AI 工作流引擎、x402 支付功能及 Web2 与 Web3 的深度集成，开发者和用户可通过自主智能体实现丰富的链上操作。</li><li>数据与性能：生态内核心贡献者，过去 90 天完成 4.825 万笔交易，交易量 2290 美元，买家 1250 人，关联 62 个地址，最新活动发生在 5 分钟前（通过 Coinbase facilitators）。活动数据显示用户参与度持续处于高位。</li><li>发展机遇：AI 与 Web3 融合的核心项目，有望拓展 P2P 智能体交易市场。</li></ul><p><strong>Daydreams</strong></p><ul><li>功能特性：支持 Base、Solana、Starknet 等多链的智能体。通过路由将非 x402 兼容的 LLM（如 OpenAI）封装为按次付费服务，提供带预算控制功能的智能体开发框架，降低开发难度。</li><li>数据与性能：在非 x402 服务封装领域表现活跃，用户采用率稳步提升。</li><li>发展机遇：在传统服务封装领域具备最佳产品市场契合度（PMF**），未来在智能体规模化落地中潜力显著。</li></ul><p><strong>Gloria AI</strong></p><ul><li>功能特性：AI 新闻与数据终端，通过 x402 提供实时加密货币洞察。核心能力包括：集成 Virtuals 实现链上分析、支持 ACP 完成智能体支付。</li><li>数据与性能：生态内重要贡献项目，在 AI 驱动的加密数据服务领域占据一席之地。</li><li>发展机遇：可拓展 “新闻即服务”（news-as-a-service）模式，为其他智能体提供数据支持，未来有望切入安全信息与事件管理（SIEM）领域。</li></ul><p><strong>Meridian（智能体层）</strong></p><ul><li>功能特性：除服务商功能外，还提供智能体基础架构（agent rails），实现全栈价值捕获，且与 Questflow 的编排功能兼容。</li><li>数据与性能：已集成至多个生态项目，其费率模型可实现可持续的收入流。</li><li>发展机遇：生态基础设施型项目，当前估值被低估，长期增长潜力大。</li></ul><p><strong>Firecrawl</strong></p><ul><li>功能特性：Web 抓取智能体，通过 x402 实现 “按次抓取付费”。核心能力包括：实时数据提取、与分析工具集成。</li><li>数据与性能：用户采用率持续增长，在数据抓取场景的应用逐步深化。</li><li>发展机遇：为数据密集型 AI 提供关键支持，与 RWA** 项目具备协同效应。</li></ul><p>此外，Karum AI、Ethy 等其他智能体专注于细分任务（如治理、收益农耕），共同丰富了生态应用场景。目前，智能体贡献了 x402 生态的大部分交易量，凸显了 AI 在生态中的核心地位。</p><h3><strong>Facilitators：x402 的支付基础设施</strong></h3><p>服务商是生态的核心支柱，负责签名验证、交易广播及 Gas 抽象（gas abstraction），确保智能体可无缝完成支付，且多数服务商以免费模式参与竞争。</p><p><strong>Coinbase Facilitator</strong></p><ul><li>功能特性：作为协议发起方，采用 Coinbase 智能钱包实现 EIP-3009 签名，目前仅支持 Base 公链，零费率且与 USDC 无缝集成。核心能力包括：批量支付、实时结算（耗时不足 1 秒）。</li><li>数据与性能：处理生态内大量交易量，系统可用性表现优异。</li><li>发展机遇：适合 Base 原生 AI 智能体接入，未来计划通过 Allbridge 等跨链桥拓展至多链。</li></ul><p><strong>PayAI</strong></p><ul><li>功能特性：支持 Solana、Base 等多链的服务商，零费率运营，提供 AI 优化路由以选择最低成本支付路径。核心能力包括：治理功能（支持费率返还）、与 Daydreams 等智能体框架集成。</li><li>数据与性能：生态内活跃度高，用户采用率呈上升趋势。</li><li>发展机遇：聚焦 AI 支付场景，分析师预测在牛市周期中具备显著增长潜力。</li></ul><p><strong>Meridian</strong></p><ul><li>功能特性：企业级服务商，收取 1% 提现费，提供组织级仪表盘、审计日志及权限控制功能，兼容谷歌 AP2 与 ERC-8004 标准。核心能力包括：通过费率实现国库自充、为早期用户提供 2% 现金返还（返还比例将逐步降低）。</li><li>数据与性能：主要服务高价值交易场景，在企业客户群体中认可度较高。</li><li>发展机遇：聚焦 B2B 市场，随着智能体在企业场景的规模化应用，有望实现价值快速增长。风险提示：较高费率可能限制零售用户的采用。</li></ul><p>此外，x402.org等服务商（开源、自托管模式）提供零费率替代方案，虽推动了生态去中心化，但也导致了流动性分散。整体来看，服务商领域的准入门槛较低（任何人都可开发），未来可能出现同质化竞争，而 PayAI 等具备生态集成优势的项目更易实现价值积累。</p><h3><strong>Analytics：x402 生态的监控与洞察载体</strong><em>*</em>*</h3><p>分析工具为生态提供支付、使用及性能层面的可视化洞察，是生态健康度的 “晴雨表”。</p><p><strong>AuroraCloud</strong></p><ul><li>功能特性：基于云的智能体指标分析工具，提供总锁仓价值（TVL）、交易流向等仪表盘功能。</li><li>数据与性能：每日追踪大量数据维度，系统可用性高，数据准确性得到生态认可。</li><li>发展机遇：可拓展 AI 增强预测功能，与 Dune 集成以覆盖更广泛的 Web3 数据洞察场景。</li></ul><p><strong>Corbits</strong></p><ul><li>功能特性：专注于链上数据的轨道式分析工具（orbital analytics），可视化展示 x402 在各生态中的流向。</li><li>数据与性能：已与 thirdweb 达成合作，数据覆盖范围逐步扩大。</li><li>发展机遇：聚焦跨链数据可视化，未来可集成机器学习异常检测功能，提升数据价值。</li></ul><p><strong>Thirdweb</strong></p><ul><li>功能特性：Web3 开发工具包，内置 x402 分析模块，可监控智能合约交互数据。</li><li>数据与性能：成为多个黑客松项目的首选开发工具，开发者采用率高。</li><li>发展机遇：以开发者需求为核心，通过与 Vercel 集成进一步扩大用户覆盖。</li></ul><p><strong>x402scan</strong></p><ul><li>功能特性：由 Merit Systems 开发的生态浏览器，提供使用指标、资源追踪功能；旗下 x402rs 为服务商提供研究套件。所有分析功能均基于全新的 CoinbaseDev SQL API。仅依靠链上数据无法全面反映生态状况，而 x402scan 可有效追踪智能体商业（agentic commerce）的增长态势。</li><li>数据与性能：开源项目，社区贡献活跃，数据更新及时。</li><li>发展机遇：依托社区贡献持续完善功能，有望成为 x402 领域的 “Dune”（Web3 知名数据分析平台）。</li></ul><p>当前分析工具已能清晰反映生态健康状况，数据显示用户交互量正持续增长，为生态发展提供了重要参考。</p><h3><strong>Protocols：x402 的底层标准层</strong></h3><p>基础协议负责制定 x402 在不同基础设施间的标准，保障生态互操作性（interoperability）。</p><p><strong>Google A2A（AP2）</strong></p><ul><li>功能特性：Agent Payments Protocol 2，通过与 x402 集成实现 AI 智能体的自主支付，核心能力是跨系统兼容性。</li><li>数据与性能：已与 Algorand 达成合作，跨生态适配进展顺利。</li><li>发展机遇：推动 x402 在企业级场景的应用，成为连接 AI 巨头与 Web3 的关键桥梁。</li></ul><p><strong>Visa TAP</strong></p><ul><li>功能特性：代币化资产平台（Tokenized Asset Platform），通过 x402 实现代币化资产交易，核心能力是为智能体提供法币入金通道（fiat-onramp）。</li><li>数据与性能：正推进全球范围内的生态拓展，与多地金融机构的合作逐步落地。</li><li>发展机遇：助力 x402 与 RWA 集成，是推动生态大规模应用的核心催化剂。</li></ul><p><strong>Cloudflare PPC</strong></p><ul><li>功能特性：按次计算付费（Pay-Per-Compute）协议，支持闲置资源的微支付变现，核心能力是为边缘计算提供微支付解决方案。</li><li>数据与性能：已实现计算时长的代币化，资源利用率显著提升。</li><li>发展机遇：推动去中心化计算发展，与 AI 训练场景具备协同效应，可实现计算资源的高效分配。</li></ul><p><strong>Vercel MCP x402</strong></p><ul><li>功能特性：托管计算协议（Managed Compute Protocol），集成 x402 功能，核心能力是提供无服务器智能体托管服务。</li><li>数据与性能：已支持多个生态项目的部署，开发者反馈良好。</li><li>发展机遇：聚焦开发者友好型服务，未来有望逐步升级至 MCP 标准，成为智能体托管的核心基础设施。</li></ul><p><strong>Merit Systems</strong></p><ul><li>功能特性：开源金融基础设施，集成 x402scan 分析功能，为开源项目提供代币化支持。</li><li>数据与性能：社区贡献者数量持续增长，生态影响力逐步扩大。</li><li>发展机遇：推动开源项目的可持续代币化，可拓展智能体奖励机制，激励生态参与。</li></ul><p><strong>x402.org</strong></p><ul><li>功能特性：x402 核心协议枢纽，负责协议治理与标准制定，推动生态标准化发展。</li><li>数据与性能：在基金会支持下，已集成多个服务商，协议兼容性持续提升。</li><li>发展机遇：引领协议的去中心化演进，确保生态长期健康发展。</li></ul><p>目前，基础协议已处理生态内部分交易量，为跨项目、跨公链的互操作提供了关键支持，是生态规模化发展的核心保障。</p><h3><strong>未来机遇与发展潜力</strong></h3><p>x402 的核心发展潜力集中在以下领域：</p><ul><li>智能体商业（Agentic Commerce）：P2P 智能体交易市场具备巨大拓展空间，有望成为生态增长的新引擎。</li><li>RWA 与 DeFi 融合：如 Algorand 通过 Plausible 实现 x402 集成，未来可进一步拓展资产类型与应用场景。</li><li>AI 基础设施：Daydreams 的 LLM 封装模式具备可复制性，有望成为 AI 服务 代币化的标准方案。</li><li>投资方向：被低估的项目包括 Meridian（企业级场景）、Gloria AI（数据服务），具备长期投资价值。</li><li>风险提示：生态存在碎片化风险，且智能体的反洗钱（AML）合规等监管问题仍需解决。</li></ul><p>对开发者的建议：聚焦服务提供商侧的集成需求，通过参与黑客松获取生态资助，降低启动成本。</p><p>对投资者的建议：重点关注具备竞争壁垒的服务商项目（如 PayAI 的多链优势）及产品市场契合度高的智能体项目（如 Daydreams）。</p><p>原文：<a href="https://link.segmentfault.com/?enc=VnWB6JwiVoG2YbfPn4idWw%3D%3D.TZmbQrUunjXpSac23WNPySZMbZ8SgzadgmitvnJNDMuelGO6Lf3i3ONrK9ta%2Fpa4%2BQmZhzZvb6tSEs3cr9bUaQ%3D%3D" rel="nofollow" target="_blank">https://x.com/questflow/status/1980808922752778678</a></p><p>作者：@Questflow</p><p>（OpenBuild 翻译整理，原文有删减）</p>]]></description></item><item>    <title><![CDATA[一文了解网口Bond几种模式原理 wei]]></title>    <link>https://segmentfault.com/a/1190000047383374</link>    <guid>https://segmentfault.com/a/1190000047383374</guid>    <pubDate>2025-11-09 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>网口/网卡bond，也就是多个网络接口绑定成一个逻辑接口的技术（NIC Teaming/Link Aggregation），或者称为链路聚合。</p><p>例如，当你有一张两个10Gps端口的网卡时，若你想将网口的速率翻倍，变为20Gps,便可以考虑在os下将这两个网口做一个mode=0 或 mode =4 的绑定， 便可以得到一个 速率为20Gps的网络接口。</p><p>下面本文将介绍bond的历史来源、基本几种模式，并探讨各种模式的原理。</p><h2>链路聚合历史脉络</h2><h4>1.发展背景</h4><ul><li><strong>高可用性需求</strong>：随着企业越来越依赖于网络服务（如数据库、文件服务器），网络连接的单点故障变得不可接受。一块网卡、一条网线或一个交换机端口的故障，就可能导致关键业务中断，造成巨大损失。</li><li><strong>带宽增长需求</strong>：服务器处理能力快速增长（CPU、内存、磁盘I/O），而网络带宽成为了新的瓶颈。在千兆以太网（1Gbps）成为主流的时代，如何突破单块网卡的速率上限，成为一个迫切问题。</li></ul><h4>2.早期概念与专有解决方案（1990年代中后期）</h4><p>在早期，并没有统一的标准。各大网络设备厂商，如 Cisco、Nortel、3Com 等，都推出了各自的私有链路聚合技术。<br/>例如，Cisco的 EtherChannel、Fast EtherChannel 就是早期的代表。<br/>问题：必须使用<strong>同一厂商的交换机和安装了特定驱动</strong>的服务器网卡才能实现捆绑，增加了成本和复杂性。</p><h4><strong>标准化进程——IEEE 802.3ad的诞生（2000年）</strong></h4><p>为了解决私有协议带来的混乱，IEEE（电气和电子工程师协会）牵头制定了标准。2000年，IEEE 802.3ad 标准正式发布。<br/>这个标准定义了 <strong>链路聚合控制协议</strong>（LACP - Link Aggregation Control Protocol），允许支持该标准的任何厂商的交换机与任何支持该标准的服务器网卡进行通信和协商，这是网口bonding技术发展的里程碑。它打破了厂商锁定，使得跨平台、跨设备的链路聚合成为可能。</p><h4><strong>操作系统内核的集成（2000年代初期）</strong></h4><p>从硬件/驱动到操作系统：在标准制定之后，需要操作系统层面提供支持来实现bonding功能。</p><ul><li>Linux的引领作用：<strong>Linux内核在 2.4.x 版本（2001年左右）​ 开始集成官方的 bonding驱动</strong>。这是一个纯软件实现的解决方案，它允许系统管理员将多个物理网卡接口绑定成一个逻辑接口 bond0，并支持多种模式（包括基于802.3ad的模式4）。</li><li>开源的力量：Linux bonding驱动的出现，让即使使用廉价普通网卡的用户也能享受到链路聚合的好处，只要交换机支持LACP即可。这大大降低了技术门槛，促进了该技术在Web托管、云计算等领域的普及。<br/>其他操作系统的跟进：随后，其他主流服务器操作系统也纷纷内置了类似功能：</li><li>Windows Server​ 在后续版本中加入了名为 NIC Teaming​ 的功能。</li><li>VMware ESXi​ 等虚拟化平台在虚拟交换机层面提供了丰富的负载均衡和故障切换策略。</li></ul><h2>mode=0 round robin 平衡轮询策略</h2><p>进入正题，linux有七种网卡绑定模式：</p><ol start="0"><li>round robin，</li><li>active-backup</li><li>load balancing (xor)，</li><li>fault-tolerance (broadcast)，</li><li>lacp</li><li>transmit load balancing，</li><li>adaptive load balancing。</li></ol><p>mode=4在一般企业中用的比较多， mode=1,mode=6 也有部分地位</p><p>首先介绍mode=0. mode=0的官方名称是 <strong>平衡轮询策略</strong></p><p><strong>核心原理</strong>：在这种模式下，数据包的发送会依次通过每一个被绑定的物理网口。第一个数据包走第一个物理网口，第二个数据包走第二个物理网口，如此循环往复。这种机制旨在最大化地利用所有可用网卡的带宽，实现网络流量的负载均衡 </p><p>其主要目标是进行<strong>负载均衡</strong>，通过将网络流量分散到多条链路，从而增加有效的网络带宽。同时，当一条物理链路发生故障时，<strong>它能自动将流量切换到其他正常的链路上</strong>，因此也具备了基本的容错能力</p><p><img width="425" height="239" referrerpolicy="no-referrer" src="/img/bVdmYGd" alt="image.png" title="image.png"/></p><p><strong>带宽效果</strong><br/>理想情况下，总带宽是所有成员网卡带宽之和（例如，2个1G网卡绑定后，理论发送带宽可达2G）<br/><strong>容错能力</strong><br/>具备链路冗余。当监测到某个网卡的链路中断时，会自动停止向该网卡发送数据，由其他正常网卡接管 <br/><strong>交换机需求</strong><br/>通常需要配置支持。<strong>需将连接多个物理网口的交换机端口配置为聚合组</strong>（如Cisco的EtherChannel、华为的Eth-Trunk），否则可能因MAC地址混乱导致网络问题 </p><p>为什么呢？ 简单说一下原理：</p><p><strong>mode 0下bond所绑定的网口都会被修改成相同的mac地址</strong>，如果这些网卡都被接在同一个交换机，那么交换机的arp表里这个mac地址对应的端口就有多个，<strong>那么交换机接受到发往这个mac地址的包应该往哪个端口转发呢？</strong></p><p>一个mac地址对应多个端口肯定使交换机迷惑了。所以 mode0下的bond如果连接到交换机，交换机这几个端口应该采取聚合方式，<strong>因为交换机做了聚合后，聚合下的几个端口被视为一个逻辑组</strong>.</p><pre><code>服务器视角：             交换机视角：
eth0 (MAC: AA:AA:AA:AA:AA:AA)  ←→  端口1 (逻辑组1)
eth1 (MAC: AA:AA:AA:AA:AA:AA)  ←→  端口2 (逻辑组1)
      ↓                             ↓
bond0 (MAC: AA:AA:AA:AA:AA:AA) ←→ 端口组1 (MAC: AA:AA:AA:AA:AA:AA)</code></pre><p>以linux下nmcli设置为例，设置命令如下：</p><pre><code># 创建bond0
nmcli connection add type bond con-name bond0 ifname bond0 mode 0

# 添加网口
sudo nmcli connection add type bond-slave  ifname eth0 master bond0
sudo nmcli connection add type bond-slave  ifname eth1 master bond0


# 设置网关和DNS（可选）
sudo nmcli connection modify bond0 ipv4.gateway "192.168.1.1"
sudo nmcli connection modify bond0 ipv4.dns "8.8.8.8,8.8.4.4"
sudo nmcli connection modify bond0 ipv4.method manual

# 使网口up
nmcli connection  up  bond-slave-网口1名称
nmcli connection  up  bond-slave-网口2名称
nmcli connection  up  bond0</code></pre><p>交换机设置不再赘述，各家交换机命令都不太一致。 nmcli 设置其他模式的mode，只需要将 下面的 mode 0, 改成mode 其他就行，例如 mode 1 , mode 4</p><pre><code>nmcli connection add type bond con-name bond0 ifname bond0 mode 0</code></pre><h2>mode=1 active-backup 主备策略</h2><p>特点：<strong>只有一个设备处于活动状态，当一个宕掉另一个马上由备份转换为主设备。</strong></p><p>此模式只提供了容错能力；由此可见此算法的优点是可以提供高网络连接的可用性。</p><p>但是它的资源利用率较低，只有一个接口处于工作状态，在有 N 个网络接口的情况下，资源利用率为1/N</p><pre><code>正常状态：
主网口(eth0) → 活跃状态，处理所有网络流量
备网口(eth1) → 待命状态，监控链路但不传输数据

故障状态（主网口故障）：
主网口(eth0) → 故障，停止工作
备网口(eth1) → 立即接管，成为新的活跃网口</code></pre><h2>mode=2 Balance-XOR 异或平衡模式</h2><p>Mode=2​ 称为异或平衡模式，是一种基于哈希算法的负载均衡方式。</p><p>缺省的策略是：<br/><strong>发送端口选择 = (源MAC XOR 目标MAC) % 从端口数量</strong></p><pre><code>假设：
- 源MAC: 11:22:33:44:55:66
- 目标MAC: AA:BB:CC:DD:EE:FF
- 端口数量: 2

计算：
哈希值 = 0x112233445566 XOR 0xAABBCCDDEEFF = 0xBB99FF99BB99
端口索引 = 0xBB99FF99BB99 % 2 = 1 (如果结果为1)

结果：这个会话的所有数据包都通过端口1发送</code></pre><p>这种策略也叫<strong>layer2 哈希策略</strong>，在这种策略下</p><ul><li>如果主机A与多个不同主机通信 → 流量会分布到不同端口（有负载均衡）</li><li><strong>如果主机A只与1台主机B通信 → 所有流量走同一个端口（无负载均衡）</strong></li></ul><p><strong>那么如何解决主机A只与1台主机B带来的无负载均衡问题呢？</strong></p><p>我们需要策略改成 layer3+4</p><p><strong>layer3+4是 基于IP和端口的哈希</strong></p><p>哈希输入：源IP + 目标IP + 源端口 + 目标端口</p><p>linux下设置如下：</p><pre><code>echo layer3+4 &gt; /sys/class/net/bond0/bonding/xmit_hash_policy</code></pre><pre><code># 哈希输入：IP地址 + 端口号
hash = (源IP ^ 目标IP) ^ (源端口 ^ 目标端口)

# 即使同一对主机，不同连接走不同路径：
连接1: A(IP1:5000) → B(IP2:80)   → 端口0
连接2: A(IP1:5001) → B(IP2:80)   → 端口1  # 不同端口！
连接3: A(IP1:5002) → B(IP2:443)  → 端口0
连接4: A(IP1:5003) → B(IP2:443)  → 端口1</code></pre><p>这就更细颗粒度解决了 layer2 策略带来的单目标通信无负载均衡的问题。</p><p>与mode=0的对比如下：</p><table><thead><tr><th>特性</th><th>Mode=0 (Balance-RR)</th><th>Mode=2 (Balance-XOR)</th></tr></thead><tbody><tr><td><strong>官方名称</strong></td><td>平衡轮询</td><td>异或平衡</td></tr><tr><td><strong>负载均衡粒度</strong></td><td>数据包级别</td><td>会话/流级别</td></tr><tr><td><strong>数据包顺序</strong></td><td>❌ 可能乱序</td><td>✅ 保证顺序</td></tr><tr><td><strong>性能特点</strong></td><td>最大吞吐量</td><td>平衡吞吐量与顺序性</td></tr><tr><td><strong>交换机需求</strong></td><td>静态链路聚合</td><td>静态链路聚合</td></tr><tr><td><strong>适用场景</strong></td><td>大数据传输</td><td>通用业务</td></tr></tbody></table><h2>mode=3 Broadcast  广播模式</h2><p>mode=3 称为广播模式，是所有绑定模式中最特殊的一种，以可靠性为最高优先级。</p><pre><code>数据包发送流程：
应用数据 → bond0 → 同时从所有活动从端口发送

发送示例：
数据包P → 同时通过 eth0 和 eth1 发送
        ↗ eth0 → 网络路径1 → 目标设备
数据包P 
        ↘ eth1 → 网络路径2 → 目标设备

接收示例：
目标设备回复 → 可能通过 eth0 或 eth1 接收</code></pre><p>这种策略带来100% 数据冗余，以及可靠的数据传输，只适用于极端可靠性要求的特殊场景</p><h2>mode=4  (802.3ad/LACP) 动态链路聚合</h2><p><strong>Mode=4​称为动态链路聚合或LACP模式，是企业环境中最常用、最标准的绑定模式。</strong></p><p>前面提到的mode0 1 2 3，都没有用到IEEE 802.3ad/LACP 协议，是Linux bonding驱动自有的实现。所以Mode=4代表了更现代、更标准的网络聚合方案</p><p>工作原理如下：</p><p><strong>1. 协议握手</strong><br/>双方声明支持LACP聚合能力<br/>交换系统ID、端口能力、MAC地址等参数</p><pre><code>服务器网口 → 发送LACP报文 → 交换机
交换机 → 响应LACP报文 → 服务器</code></pre><p><strong>2. 动态聚合建立</strong></p><pre><code># 交换机端创建逻辑聚合端口
交换机: 创建 Port-channel1
↓
# 将兼容的物理端口加入聚合组
端口1 → 加入 Port-channel1
端口2 → 加入 Port-channel1
↓
# 分配聚合组ID
Aggregator ID = 1</code></pre><p><strong>3. 智能流量分配</strong></p><pre><code>基于哈希算法分配流量：
会话1: IP_A:端口X → IP_B:端口Y → 走 eth0
会话2: IP_A:端口Z → IP_B:端口Y → 走 eth1
同一会话的所有数据包保证走同一路径</code></pre><p><strong>4. 实时监控与故障切换</strong></p><pre><code># 持续保活检测：
- 每1秒交换LACP保活报文
- 链路故障时自动移除故障端口
- 剩余端口继续承载流量
- 链路恢复后自动重新加入</code></pre><p>同样的，mode=4 的哈希策略<strong>默认是layer=2</strong>。</p><p>这里额外说一下部分人<strong>为什么两张网卡对联，测试iperf bond4速率的时候只能达到单倍速率</strong>，就是因为使用了默认的哈希策略，layer=2</p><pre><code># 假设：
机器A MAC: AA:AA:AA:AA:AA:AA
机器B MAC: BB:BB:BB:BB:BB:BB

# layer2哈希计算：
hash = MAC_A XOR MAC_B = 固定值
端口选择 = hash % 2 = 总是选择同一个端口

# 结果：所有流量都走同一路径</code></pre><p>如果用上layer3+4， 情况如下：<br/>就因为iperf 使用-P 参数多线程打流，每个线程都会开一个新的端口，所以<strong>layer3+4 基于IP和端口的哈希</strong>，才能够符合ipef多线程打流时候的要求，bond的速率也才能达到双倍的速率。</p><pre><code>连接1: A:5000 → B:80 → 哈希值1 → 端口0
连接2: A:5001 → B:80 → 哈希值2 → 端口1 ✅
连接3: A:5002 → B:443 → 哈希值3 → 端口0
连接4: A:5003 → B:443 → 哈希值4 → 端口1 ✅</code></pre><p>修改哈希策略命令：</p><pre><code># 修改命令：
echo layer3+4 &gt; /sys/class/net/bond0/bonding/xmit_hash_policy</code></pre><p>各家交换机配置LACP命令都不太相同，具体可参考说明书。</p><p><strong>总之，大多数场景都建议使用layer3+4，可以获得更细粒度的负载均衡，特别适合现代的多连接应用环境</strong></p>]]></description></item><item>    <title><![CDATA[类比前端知识来学习Java的Spring]]></title>    <link>https://segmentfault.com/a/1190000047383171</link>    <guid>https://segmentfault.com/a/1190000047383171</guid>    <pubDate>2025-11-09 18:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><ul><li>本文梳理了后端的相关操作流程环节</li><li>使用Svelte+Vite（前端）搭配<a href="https://link.segmentfault.com/?enc=8nU8zmRw2dWZvjQ37HhU2w%3D%3D.j%2BFuF4itqqBTHreLrenPb8eGfQdcHVsY5d2iXF%2FzshPrdAy%2FVI2SZYR%2Bz5scJP5Z" rel="nofollow" target="_blank">Spring Boot</a>（后端）</li><li>实现了一个增删改查全栈项目</li><li>有助于前端更好理解后端java的分层思想，数据流转控制</li><li>和<a href="https://link.segmentfault.com/?enc=M%2B02yJOC85aUD%2F5nDRybzg%3D%3D.SZaA8nVgp2y5Ydj2Xc6goUj551Gg10ogHswM%2FxLaC32pFutf8mzOYJjnvQzVLUtjaSCQuJ%2BOVPyP%2BCOIKrrl9A%3D%3D" rel="nofollow" target="_blank">Svelte</a>尝鲜学习了解</li><li>完整前后端代码在github：<a href="https://link.segmentfault.com/?enc=b8V0xqISk6tpSEs8JbsTFg%3D%3D.jjnAlCw7%2Bzvlgorgnt2pkrJchbCN2yk%2FumxJ0rBFZFgrKZP7gzRfH3iBDxvJcocOrCgU2evgDnzdaSJV7Pi%2B0Q%3D%3D" rel="nofollow" target="_blank">https://github.com/shuirongshuifu/svelte-springBoot-crud</a></li></ul><p>大道至简，一些知识点是相通的比如——python里面也有闭包这个概念</p><blockquote>所谓编程即：学习规则语法、理解规则语法、合理运用规则语法、从而自定义规则...</blockquote><h2>Java、Spring、Spring Boot ≈ Node.js、Express/Koa、Egg.js/NestJS</h2><h3>效果图</h3><p><img width="723" height="414" referrerpolicy="no-referrer" src="/img/bVdmYGP" alt="" title=""/></p><h3>仓库代码图</h3><p><img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdmYGQ" alt="" title="" loading="lazy"/></p><h3>对比理解后端宏观架构流程、数据流转</h3><p>当我们知道后端具体做了什么事情以后，就更好理解了，即宏观架构流程、数据流转要清晰</p><p><strong><code>Java之于Spring之于Spring Boot 相当于 Node.js之于Express/Koa之于Egg.js/NestJS</code></strong></p><ul><li>Java底层是JDK+JRE+JVM（JDK安装以后自带JRE和JVM，类似于Node安装后自带NPM）</li><li><p>基于Java原生开发了Spring框架，基于Spring框架有了Spring Boot（开箱即用）</p><ul><li>Spring MVC是Spring框架中的一部分，所谓的MVC指的是<strong>Model</strong>、<strong>View</strong>、<strong>Controller</strong></li><li><p>简约而言，后端主要做这几件事：</p><ol start="0"><li>定义请求路由接口 <strong>(C路由)</strong></li><li>请求参数验证 <strong>(C参数验证)</strong></li><li>业务逻辑处理 <strong>(M业务逻辑)</strong></li><li>操作数据库 <strong>(M业务逻辑)</strong></li><li>返回响应数据JSON、下载返回流文件 <strong>（C路由返回 V视图概念消失弱化）</strong></li></ol></li><li><p>前后端不分离JSP时代，MVC基本后端做。即：</p><ol start="0"><li>过去: 后端 = M + C + <strong>V (渲染HTML)</strong></li><li>现在: 后端 = M + C; 前端 = <strong>V (前端框架渲染)</strong> + 交互</li></ol></li></ul></li><li>类比，Node.js --&gt; Express.js / Koa.js --&gt; Egg.js/NestJS （开箱即用）</li><li>至于Java微服务<strong>Spring Cloud</strong>实际上就是一堆<strong>Spring Boot</strong>的集合</li></ul><h3>技术栈类比</h3><table><thead><tr><th>Spring Boot 生态</th><th>Node.js 对应技术</th><th>说明</th></tr></thead><tbody><tr><td><strong>JDK 8</strong></td><td>Node.js</td><td>⚙️ 运行环境，学Java装JDK，就像学JS装Node</td></tr><tr><td><strong>Spring &amp;&amp; Spring MVC</strong></td><td>Express/Koa/Fastify</td><td>🚀 后端基础框架，快速搭建应用服务</td></tr><tr><td><strong>Spring Boot 2.7.18</strong></td><td>Egg.js/Nest.js 或 Express/Koa/Fastify + 一堆插件</td><td>🚀 后端进阶完善的框架，可开箱即用</td></tr><tr><td><strong>MyBatis-Plus 3.5.3.1</strong></td><td>Sequelize/Prisma</td><td>🗄️ ORM框架，简化数据库操作，不用手搓sql了</td></tr><tr><td><strong>Swagger 3.0.0 + Knife4j 3.0.3</strong></td><td>swagger-ui-express</td><td>📖 API文档自动生成</td></tr><tr><td><strong>Hutool 5.8.22</strong></td><td>lodash/day.js</td><td>🛠️ 工具库，提供各种实用函数</td></tr><tr><td><strong>Apache POI 4.1.2</strong></td><td>node-xlsx / xlsx</td><td>📊 Excel文件处理，导入导出解析excel的数据</td></tr><tr><td><strong>数据库驱动（JDBC Driver）</strong></td><td>mysql或者mysql2</td><td>🔌 数据库连接</td></tr><tr><td><strong>HikariCP</strong></td><td>mysql或者mysql2内置的连接池</td><td>🔌 数据库连接池，管理数据库连接</td></tr></tbody></table><blockquote><ul><li>Maven 就像 npm，<code>pom.xml</code> 就是 <code>package.json</code>，依赖管理方式几乎一样！</li><li>Java 的包管理比 npm 更严格，但概念相同</li><li>Spring Boot 的注解就像 Vue的自定义指令</li></ul></blockquote><h2>后端五件事（简约版）</h2><ul><li><ol start="0"><li>定义请求路由接口</li></ol></li><li><ol start="2"><li>请求参数验证</li></ol></li><li><ol start="3"><li>业务逻辑处理</li></ol></li><li><ol start="4"><li>操作数据库</li></ol></li><li><ol start="5"><li>返回响应数据（JSON / 流）</li></ol></li></ul><h3>整体流程</h3><table><thead><tr><th>流程节点</th><th>核心操作</th></tr></thead><tbody><tr><td>前端 → Nginx → Controller</td><td>前端发请求（如 <code>GET /user/1</code>），Controller 用 <code>UserQueryDTO</code> 接收参数（如 <code>id=1</code>），校验参数合法性</td></tr><tr><td>Controller → Service</td><td>Controller 调用 Service 方法，传入 <code>UserQueryDTO</code> 或提取后的参数（如 <code>id=1</code>）</td></tr><tr><td>Service → Mapper</td><td>Service 处理业务逻辑（如权限判断），调用 Mapper 方法（如 <code>userMapper.selectById(1)</code>）</td></tr><tr><td>Mapper → 数据库</td><td>Mapper 执行 SQL，将查询条件（<code>id=1</code>）转为数据库语句，同时通过 Entity 映射表结构（如 <code>User</code> 类对应 <code>user</code> 表）</td></tr><tr><td>数据库 → Mapper</td><td>数据库返回结果集，Mapper 自动将结果集转为 <code>User</code> 实体对象</td></tr><tr><td>Mapper → Service</td><td>Mapper 将 <code>User</code> 实体返回给 Service</td></tr><tr><td>Service → Controller</td><td>Service 将 <code>User</code> 实体通过转换器（或手动）转为 <code>UserRespDTO</code>（屏蔽敏感字段，如密码）</td></tr><tr><td>Controller → Nginx → 前端</td><td>Controller 将 <code>UserRespDTO</code> 转为 JSON 响应，返回给前端</td></tr></tbody></table><p>下面以新增请求为例</p><blockquote>当然还有别的 这里不赘述</blockquote><h2>1. 定义请求路由接口 （Controller层）</h2><h3>定义新增接口 /people</h3><p>比如定义一个新增接口</p><ul><li>定义一个请求Url是 /people</li><li>Post 请求 Body传参</li><li>传参示例：<code>{ age: 20，home: "string"，name: "string"，remark: "string" }</code></li><li>curl命令调用如下</li></ul><p>&lt;!----&gt;</p><pre><code>curl -X POST http://localhost:8080/people \
  -H "Content-Type: application/json" \
  -d '{"age": 20, "home": "string", "name": "string", "remark": "string"}'
</code></pre><h3>Controller控制层</h3><p><em>PeopleController.java</em></p><pre><code class="java">import com.people_sys.people.service.PeopleService; // 导入业务服务层 PeopleService
// @RestController = @Controller + @ResponseBody = 返回JSON格式的数据
@RestController 
@RestController // REST风格的接口
@RequestMapping("/people") // 接口请求url 统一请求前缀/people
public class PeopleController { // PeopleController类
    // 定义变量存储peopleService
    private final PeopleService peopleService;
    // 构造器注入（初始化执行）
    // 也可以使用注解 @Resource 或 @Autowired  一步到位
    public PeopleController(PeopleService peopleService) {
        this.peopleService = peopleService; // 存起来
    }
    
    // 新增人员 - POST /people
    @PostMapping
    public ApiResponse&lt;Boolean&gt; create(@Valid @RequestBody PeopleDTO people) throws Exception {
        // 调用peopleService层的create方法新增用户人员
        boolean result = peopleService.create(people);
        return ApiResponse.success(result);
    }
    // 根据id查询 - GET /people/{id}
    @GetMapping("/{id}")
    public ApiResponse&lt;People&gt; getById(@PathVariable Integer id) {
        // 调用peopleService层的getById方法，根据id查询对应人员的
        People people = peopleService.getById(id);
        return ApiResponse.success(people);
    }
    
    ......
}</code></pre><h3>何为注解&amp;常见的注解举例</h3><p>简而言之：</p><ul><li>注解有点像前端Vue中的指令，比如只要写了v-if以后，Vue框架会自动根据相应逻辑处理显示隐藏</li><li>也像React中的Props，比如 @NotNull(message = "不能为空") 类比于 \&lt;input required ... /&gt;</li></ul><p><strong>注解（实际上是封装了一层），就是用特定的语法，告诉框架（组件），如何正确处理对应逻辑</strong></p><p>常见的注解举例：</p><ul><li>@RestController 控制器注解，标明定义的接口——适合前后端分析的项目，返回JSON</li><li>@Controller 适合前后端不分离的，比如返回html，用的少了</li><li>@Service 服务层注解，撰写具体业务逻辑</li><li>@Data Lombok注解，自动生成getter/setter/toString等</li><li><p>@RequestMapping系列注解，请求映射注解</p><ul><li>@PostMapping // POST请求</li><li>@GetMapping("/{id}") // GET请求带路径参数</li><li>@PutMapping // PUT请求</li><li>@DeleteMapping // DELETE请求</li><li>@RequestMapping("/api") // 通用映射</li></ul></li><li><p>验证注解系列</p><ul><li>@NotNull // 不为 null</li><li>@NotBlank // 去空格非空</li><li>@NotEmpty // 集合或数组至少一个元素</li><li>@Size(min=1, max=10) // 长度限制</li><li>@Email // 邮箱格式</li><li>@Min(0) @Max(150) // 数值范围</li></ul></li><li>跨域注解</li></ul><pre><code class="java">import org.springframework.web.bind.annotation.CrossOrigin;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;
@RestController
public class TestController {
    // 仅当前接口支持跨域，允许所有来源（*）
    @CrossOrigin
    @GetMapping("/api/test")
    public String testCrossOrigin() {
        return "跨域请求成功！";
    }
}</code></pre><ul><li><p>拿到前端传参的注解</p><ul><li><p>@PathVariable注解</p><ul><li>能拿到/findById/100 这个100</li><li>类似express中的req.params + 路由 :变量名</li></ul></li><li><p>@RequestParam 注解</p><ul><li>能拿到query传参 ?name=xxx\&amp;age=88</li><li>类似express中的req.query</li></ul></li><li><p>@RequestBody</p><ul><li>能拿到body传参</li><li>类似express中的req.body + 解析中间件app.use(express.json())</li></ul></li></ul></li><li>等很多注解...</li></ul><blockquote>注解还可以自定义，有点像函数</blockquote><h2>5. 返回响应 JSON / 流</h2><h3>统一返回JSON</h3><p>首先，前端请求接口时，后端统一返回格式，使用ApiResponse这个类统一控制</p><pre><code class="java">package com.people_sys.people.config;
import lombok.Data;
@Data // 自动生成getter/setter
public class ApiResponse&lt;T&gt; {
    private int code;       // 状态码（如200成功，400参数错误，500系统错误）
    private String message; // 响应消息（成功时为"success"，失败时为错误信息）
    private T data;         // 业务数据（成功时返回）
    public ApiResponse(int code, String message, T data) {
        this.code = code;
        this.message = message;
        this.data = data;
    }
    // 成功响应
    public static &lt;T&gt; ApiResponse&lt;T&gt; success(T data) {
        return new ApiResponse&lt;&gt;(200, "success", data);
    }
    // 错误响应
    public static &lt;T&gt; ApiResponse&lt;T&gt; error(int code, String message) {
        return new ApiResponse&lt;&gt;(code, message, null);
    }
}</code></pre><p>前端查询id为300的这条数据，<code>http://localhost:8080/people/300</code>返回</p><pre><code class="js">{
    "code": 200,
    "message": "success",
    "data": {
        "id": 300,
        "name": "张三",
        "age": 3,
        "home": "山东",
        "remark": "zhangsan",
        "delFlag": 0,
        "createTime": "2025-11-04T14:48:05",
        "updateTime": "2025-11-04T17:23:32"
    }
}</code></pre><blockquote>ApiResponse实际上就是一个公共函数，统一加工处理数据的</blockquote><h3>返回流文件给前端下载</h3><p>动态生成Excel并下载（伪代码示例）</p><pre><code>@GetMapping("/downloadExcel")
public void downloadExcel(HttpServletResponse response) throws IOException {
    // 1. 动态生成Excel（使用POI等工具）
    XSSFWorkbook workbook = new XSSFWorkbook(); // POI的Excel对象
    XSSFSheet sheet = workbook.createSheet("Sheet1");
    // ... 向sheet中写入数据 ...
​
    // 2. 设置响应头（同上）
    response.setContentType("application/vnd.openxmlformats-officedocument.spreadsheetml.sheet");
    String fileName = URLEncoder.encode("动态生成的Excel.xlsx", "UTF-8");
    response.setHeader("Content-Disposition", "attachment; filename*=UTF-8''" + fileName);
​
    // 3. 直接将工作簿写入响应流
    workbook.write(response.getOutputStream());
    workbook.close(); // 关闭资源
}
</code></pre><blockquote>workbook.write(response.getOutputStream()) 意思就是：将内存中动态生成的文件（如 Excel）以二进制流的形式写入 HTTP 响应输出流，最终返回给前端，以便于前端使用a标签实现文件下载</blockquote><h2>2. 请求参数验证 （Controller层）</h2><h3>数据新增接口细节拆解</h3><p>接下来，看对应新增接口注释，新增接口前端Post的Body传参为</p><p><code>people: { name: 'tom', age: 2, home: 'New York', 'remark': 'xyz' }</code></p><pre><code class="java">// 1. 接口请求类型注解，等价于：@RequestMapping(method = RequestMethod.POST)
@PostMapping
// 2. public公开的create方法，允许其他类调用（若写成private，Spring无法扫描到这个接口，前端会访问失败）
public ApiResponse&lt;Boolean&gt; create( // 方法名叫做create
    // 3. 数据校验触发注解（想要使用PeopleDTO里面的校验，必须要使用@Valid标明开启校验）
    @Valid  
    // 4. 请求体接收注解，通过这个可以拿到前端请求体里面的参数，并将其赋值给people参数
    @RequestBody  
    // 5. 方法参数（DTO 实体 + 参数名）
    PeopleDTO people  // people为函数的形参存储的前端参数
) throws Exception {  // 6. 异常则抛出声明
    // 7. 业务逻辑：把前端传递进来的people对象参数，调用 Service 层新增方法，得到布尔类型结果
    boolean result = peopleService.create(people);
    // 8. 返回统一响应结果，新增成功返回 {"code":200,"message":"success","data":true}
    return ApiResponse.success(result); 
}</code></pre><p>通过@RequestBody 可以拿到前端参数 存到people变量里面，然后交由peopleService.create方法去做业务逻辑处理进而写入数据库</p><p>但是，前端可能乱传参，所以，需要搭配PeopleDTO和 @Valid 进行校验一下</p><h3>什么是DTO，能做什么</h3><p>简而言之：DTO就是规范接收前端传参、规范返回接口数据</p><p>新增的DTO</p><pre><code class="java">package com.people_sys.people.dto;
import lombok.Data;
import javax.validation.constraints.NotBlank;
import javax.validation.constraints.NotNull;
@Data
public class PeopleDTO {
    @NotBlank(message = "姓名不能为空") // 必传，字符串类型
    private String name;
    @NotNull(message = "年龄不能为空") // 必传，数字类型
    private Integer age;
    private String home; // 非必传
    private String remark; // 非必传
}</code></pre><p>编辑的DTO多了一个id，其他和新增一样（毕竟编辑要找到对应id再去编辑）</p><pre><code class="java">@Data
public class PeopleUpdateDTO {

    @NotNull(message = "ID不能为空")
    private Integer id;

    @NotBlank(message = "姓名不能为空")
    private String name;

    ...
}</code></pre><p>我们可以把DTO看成一个工具函数，在接到前端传参的时候，做规范限制——过滤掉不需要的参数，校验是否符合传参要求</p><h3>用JavaScript模拟DTO功能</h3><p>假设，我们要求前端传参规则是这样的：</p><p><code>name字段必传且为字符串类型、age字段非必传（若传了必须要是数字类型），若多传则忽略之</code></p><pre><code class="java">const Dto = {
    name: {
        type: "string",
        required: true,
    },
    age: {
        type: "number",
        required: false,
    },
}</code></pre><ul><li>假设用户传递<code>{ name: '孙悟空', age: 500, home: '花果山' }</code>那么经过DTO处理以后，能得到<code>{ name: '孙悟空', age: 500}</code>多传的home字段和其值，被忽略</li><li>假设用户传递<code>{ age: 500 }</code>那么经过DTO处理以后，要校验提示name是必传的</li><li>假设用户传递<code>{ name: true }</code>那么经过DTO处理以后，要校验提示name的数据类型不对</li></ul><p>于是，就可以有以下模拟代码</p><pre><code class="js">/**
 * 模拟定义DTO
 *  姓名为字符串，必传
 *  年龄为数字类型，非必传
 * */
const dtoDef = (params) =&gt; {
    // 定义Dto字段规则
    const Dto = {
        name: {
            type: "string",
            required: true,
        },
        age: {
            type: "number",
            required: false,
        },
    }
    /**
     * 1. 必传字段校验
     * */
    const mustHaveKeys = []
    // 1.1 收集那些字段是必传的
    for (const key in Dto) {
        if (Dto[key].required) {
            mustHaveKeys.push(key)
        }
    }
    // 1.2 收集传递进来的key组成的数组
    const paramsKeys = Object.keys(params)
    // 1.3 看看是否每一个必传字段，都在参数key数组里面
    const flag = mustHaveKeys.every((mk) =&gt; paramsKeys.includes(mk))
    // 1.4 必传参数校验
    if (!flag) {
        console.warn(`必传字段缺失，必传字段有这些：${mustHaveKeys.join(",")}`)
        return false
    }
    /**
    * 2. 字段类型校验
    * */
    const resDto = {}
    for (const key in params) {
        // 在Dto里的做对应校验
        if (key in Dto) {
            // 类型校验
            if (typeof params[key] === Dto[key].type) {
                // 校验通过则转存一份
                resDto[key] = params[key]
            } else {
                console.warn(`字段${key}类型错误，类型应为${Dto[key].type}`)
                return false
            }
        }
        // 不在Dto里面的忽略，这样resDto里存的就是定义好的
        else { }
    }
    return resDto
}

const par = { name: '孙悟空', age: 500, home: '花果山' }

// 经过dtoDef的校验、过滤就能得到符合要求的dto了
const result = dtoDef(par)
console.log('result', result) // {name: '孙悟空', age: 500}</code></pre><ul><li><p>在java中，dto定义好以后，可在接受前端参数的时候使用</p><ul><li>只拿自己需要的字段值</li><li>使用注解快速校验前端传参</li></ul></li><li><p>也可在从数据库捞出数据返回给前端的时候使用</p><ul><li>比如返回的时候，password和gender字段作为保密，不返回给前端</li><li>只返回能返回的数据</li></ul></li></ul><p>所以，再总结一下：DTO就是规范接收前端传参、规范返回接口数据的一个工具</p><blockquote><p>问：如果有一些额外的字段，要返回给前端，又不是前端要传递的字段，怎么定义呢？比如返数据时，需加一个字段isAdults来确认是否成年（规则是大于18岁成年为true，小于则为false）</p><p>答：这个时候，VO就闪亮登场了</p></blockquote><h3>VO是最终返回给前端数据结构格式</h3><p>总结：VO是最终返回给前端数据结构格式——如果小项目，直接用dto也行（可以看成把dto当成vo用）</p><pre><code class="java">@Data
public class PeopleVO {
    private Long id; // 额外字段：数据库主键（前端不用传，要展示）
    // 复用 DTO 中的字段
    private String name; 
    private Integer age;
    private String home;
    private String remark;
    private Boolean isAdults; // 额外字段：计算后信息，是否成年
}</code></pre><h2>3. 业务逻辑处理 &amp; 4.操作数据库</h2><blockquote>先假设没有复杂业务逻辑处理，直接把前端传递的数据，存到数据库里，就需要编写sql语句</blockquote><h3>古法手搓sql</h3><p>现在接口定义好了，且用DTO规范了前端传参，接下来应该把前端传递来的参数转成sql语句</p><p>比如，新增一条数据：<code>{ "name": "tom", "age": 18 }</code></p><pre><code class="java">public void addPerson(String name, int age) { // 拿到前端传进来的参数name和age
     // 拼接手搓sql
    String sql = "INSERT INTO people (name, age) VALUES (?, ?)";
    // 调用Spring的jdbcTemplate类的update方法新增数据
    int rowsAffected = jdbcTemplate.update(sql, name, age);
    // 成功插入 1 条记录 → 返回 1
    if (rowsAffected &gt; 0) {
        System.out.println("成功插入 " + rowsAffected + " 条记录");
    }
}</code></pre><p>但是这个手搓sql的方式，不优雅（可维护性、类型安全、重复劳动等），所以诞生了orm框架，通过orm框架去操作数据库</p><h3>什么是ORM</h3><ul><li>ORM（Object-Relational Mapping，<strong>对象关系映射</strong>）是一种编程技术</li><li>核心是<strong>把数据库中的 “表、行、列” 映射成程序中的 “类、对象、属性”</strong> ，</li><li>让开发者能用<strong>面向对象（OOP）的方式操作数据库</strong>，而不用直接写复杂的 SQL 语句。</li></ul><p>换句话说，ORM是翻译官</p><ul><li>数据库世界：用表（Table）、行（Row）、字段（Column）存储数据（比如 MySQL 的 <code>user</code> 表，有 <code>id</code>/<code>name</code>/<code>age</code> 字段）；</li><li>程序世界：用类（Class）、对象（Object）、属性（Attribute）处理数据（比如 Java 的 <code>User</code> 类，有 <code>id</code>/<code>name</code>/<code>age</code> 属性）；</li><li>ORM 的作用：在两者之间做 “翻译”—— 假使我们操作程序里的对象（比如 <code>user.name = "张三"</code>），ORM 自动转换成对应的 SQL（<code>UPDATE user SET name = "张三"</code>），执行后再把数据库结果转回</li></ul><blockquote>ORM的核心价值就是不用写 或者少写 SQL，专注业务逻辑</blockquote><p>上述案例，如果使用mybatis-plus这个orm框架(半自动化orm框架)，则这样写即可</p><pre><code class="java">public void addPerson(String name, int age) {
    // 创建实体对象并设置参数
    Person person = new Person();
    person.setName(name);
    person.setAge(age);

    // 调用 MyBatis-Plus 的 insert 方法插入数据
    int rowsAffected = personMapper.insert(person);

    if (rowsAffected &gt; 0) {
        System.out.println("成功插入 " + rowsAffected + " 条记录");
    }
}</code></pre><p>这里的personMapper继承自BaseMapper（自带一套通用的crud的方法）如</p><ul><li><code>insert(T entity)</code>：插入一条记录</li><li><code>deleteById(Serializable id)</code>：根据主键删除</li><li><code>updateById(T entity)</code>：根据主键更新</li><li><code>selectById(Serializable id)</code>：根据主键查询</li><li><code>selectList(Wrapper&lt;T&gt; queryWrapper)</code>：条件查询列表</li></ul><p>所以可以直接insert数据</p><pre><code class="java">import com.baomidou.mybatisplus.core.mapper.BaseMapper;
// 定义一个接口PersonMapper继承了BaseMapper上所用功能，比如crud的api
public interface PersonMapper extends BaseMapper&lt;Person&gt; {
    // 无需编写任何方法，BaseMapper 已提供 CRUD 基础功能
}</code></pre><blockquote>注意，<code>BaseMapper&lt;Person&gt;</code> 中的泛型 <code>&lt;Person&gt;</code> 就是告诉 MyBatis-Plus：这个 Mapper 要操作的是与 <code>Person</code> 类绑定的那张表（即 <code>people</code> 表）</blockquote><p><strong>所以，一定要告诉 MyBatis-Plus要操作那张表，怎么告诉呢 就通过entity告诉</strong></p><p><strong>所以，这里的<code>&lt;Person&gt;</code>就是一个entity，如下</strong></p><pre><code class="java">@Data  // 无需手动编写 getter、setter 等方法，@Data 会自动生成
@TableName("people") // 映射数据库表名——告诉mybatis，是那张表
public class Person {
    @TableId(type = IdType.AUTO) // 主键自增
    private Long id; 
    
    private String name; 
    private Integer age; 
}</code></pre><p><strong>所以ORM 一定是要搭配着entity，才能一块干活！！！</strong></p><p><strong>通俗而言，ORM是翻译官，他要把火星文翻译成中文，所以，它需要一个火星文对应中文词典，而entity就是这本词典</strong></p><h3>常见的 ORM 框架</h3><ul><li>Python：SQLAlchemy（通用）、Django ORM（Django 内置）；</li><li>Java：Hibernate（重量级）、MyBatis（半 ORM，更灵活）；</li><li>JavaScript/TypeScript：Sequelize（Node.js）、Prisma（现代主流）；</li><li>PHP：Eloquent ORM（Laravel 内置）。</li></ul><blockquote>无论哪种 ORM 框架，<strong>都必须通过某种形式的 “实体” 来定义 “代码对象” 与 “数据库表” 的映射关系</strong>（表名、字段名、类型、主键等）。这些 “实体” 可能叫 <code>Model（PHP Prisma Python）</code>、<code>Entity（Java、cSharp）</code> 或直接是一个类 / 配置，但本质都是 ORM 框架的 “翻译词典”—— 没有它们，ORM 就无法完成 “对象操作→SQL 语句” 的转换。</blockquote><h3>ORM之Mybatis操作sql</h3><p>回顾一下，使用ORM操作sql，首先，orm要知道操作那张表的哪些字段，当然，dto是老早就定义好了，如下提前定义好了的dto</p><pre><code class="java">// DTO（接收前端）
public class PersonDTO {
    private String name;
    private Integer age;
    // getter/setter
}</code></pre><p>我们会发现，dto中的东西不够用，毕竟DTO只是用来定义传输的部分数据，完整数据还是在表里。但是orm必须要知道完整数据，才方便操作数据库</p><p>而我们又不能把所有信息都丢到dto里面</p><p>所以，需要一个新的东西，来告知orm，完整的表、字段、数据类型是啥。用对象（类）的形式告知，映射数据库，于是就有了Entity这个文件</p><pre><code class="java">// Entity（映射数据库）
@TableName("people") // 告知表名字
public class People {
    @TableId(type = IdType.AUTO) // 主键自增
    private Long id;
    private String name;
    private Integer age;
    // getter/setter
}</code></pre><p>现在orm知道了操作的表名是people，和表里的对应字段信息，那么orm就方便操作数据库中的表了</p><pre><code class="java">// 3. Service 层
public void addPerson(PersonDTO dto) {
    People people = new People();
    people.setName(dto.getName());
    people.setAge(dto.getAge());
    
    // 无需写 SQL！ORM 自动 INSERT
    boolean saved = peopleService.save(people);
    if (!saved) {
        throw new BusinessException("保存失败");
    }
}</code></pre><h3>ORM 框架的核心优势</h3><ol start="0"><li><strong>简化开发</strong>：不用写 SQL，减少重复工作（比如拼接 SQL、解析结果），开发效率大幅提升；</li><li><strong>屏蔽数据库差异</strong>：同一套代码，通过 ORM 适配 MySQL、PostgreSQL、SQLite 等不同数据库（ORM 负责翻译不同数据库的 SQL 语法）；</li><li><strong>降低学习成本</strong>：不用精通各种数据库的 SQL 细节，专注于面向对象编程；</li><li><strong>安全性更高</strong>：自动防止 SQL 注入（比如拼接用户输入时，ORM 会自动转义）。</li></ol><h3>ORM总结</h3><p>ORM 是连接 “面向对象编程” 和 “关系型数据库” 的桥梁，核心目标是<strong>让开发者用更熟悉的 OOP 方式操作数据库，减少 SQL 编写，提升开发效率和代码可维护性</strong>。</p><h3>Entity与DTO和VO对比</h3><p><strong>Entity 是“存数据的”，DTO 是“传数据的”，VO 是“给用户看的”。</strong></p><ul><li>Entity（实体类）——一般不直接返回 Entity 给前端</li><li>DTO（Data Transfer Object，数据传输对象）</li><li>VO（View Object / Value Object，视图对象）</li></ul><p>如果项目简单、无敏感数据，可 DTO/VO 合并，但Entity 仍应隔离</p><table><thead><tr><th>维度</th><th>Entity</th><th>DTO</th><th>VO（View Object）</th></tr></thead><tbody><tr><td><strong>用途</strong></td><td>数据库映射</td><td>层间/系统间数据传输</td><td>前端展示专用</td></tr><tr><td><strong>是否持久化</strong></td><td>是（对应 DB 表）</td><td>否</td><td>否</td></tr><tr><td><strong>是否含敏感字段</strong></td><td>可能有（如密码）</td><td>通常过滤掉</td><td>通常无</td></tr><tr><td><strong>字段结构</strong></td><td>与 DB 一致</td><td>灵活，可裁剪/组合</td><td>为 UI 定制，可能格式化/计算</td></tr><tr><td><strong>使用位置</strong></td><td>Repository / JPA 层</td><td>Service ↔ Controller / API 间</td><td>Controller → 前端</td></tr><tr><td><strong>是否含逻辑</strong></td><td>一般无（或简单 getter）</td><td>无</td><td>可能有简单格式化逻辑</td></tr></tbody></table><h3>3. 业务逻辑之新增的人员不能重名</h3><pre><code class="java">@Override
// 新增人员的校验：新增的人名字不能和数据库中已经有的人名字重复
public boolean create(PeopleDTO dto) throws Exception {
    // 校验名字是否重复
    if (lambdaQuery()
            .eq(People::getName, dto.getName())
            .eq(People::getDelFlag, 0)
            .count() &gt; 0) {
        throw new BusinessException(400, "人员姓名已存在，请使用其他姓名");
    }

    People entity = BeanUtil.copyProperties(dto, People.class);
    return save(entity);
}</code></pre><blockquote>这里的save也是mybatis-plus提供的，比直接insert更加智能</blockquote><p>由于我们的业务逻辑是——新增的人名字不能和数据库中已经有的人名字重复，所以，在写入数据库之前，还需要编写sql查询一下，数据库中有多少条数据，和当前人名一样。</p><p>Mybatis也提前准备好了lambdaQuery()以供我们进行链式调用，进行条件构造链式查询，语法简洁</p><p>若使用手写sql，则是如下写法</p><pre><code class="java">@Autowired
private JdbcTemplate jdbcTemplate; // Spring 的 JDBC 工具类

public void checkDuplicateName(String name) {
    // 手写 SQL 字符串
    String sql = "SELECT COUNT(*) FROM people WHERE name = ? AND del_flag = 0";
    // 执行查询，获取记录数
    Integer count = jdbcTemplate.queryForObject(
        sql, 
        new Object[]{name},  // 绑定参数（姓名）
        Integer.class        // 返回类型
    );
    // 判断并抛异常
    if (count != null &amp;&amp; count &gt; 0) {
        throw new BusinessException(400, "人员姓名已存在，请使用其他姓名");
    }
}</code></pre><p>由此可见，当真是使用ORM框架——Mybatis更优雅提效，便于我们处理业务路基，操作数据库</p><h2>常用技术栈：</h2><ul><li>数据库：MySQL + HikariCP（连接池）</li><li>数据访问：MyBatis + MyBatis-Plus（ORM + 代码生成）</li><li>API 开发：Spring Web + SpringDoc-OpenAPI（接口 + 文档）</li><li>缓存：Redis + Spring Cache（提升性能）</li><li>消息队列：Kafka/RabbitMQ（削峰填谷）</li><li>日志：SLF4J + Logback（日志记录）</li><li>测试：JUnit 5 + Mockito（单元测试）</li><li>部署：Maven + Docker（打包部署）</li></ul><h3>Maven打包构建</h3><ul><li>Maven之于Java如同Npm之于Node.js</li><li>pom.xml文件作依赖版本管理——package.json做依赖版本管理</li><li>mvn install安装项目依赖——相当于npm install</li><li>项目内安装单个依赖，需手动在 <code>pom.xml</code> 文件中添加依赖后执行 <code>mvn install</code>——相当于npm install xxx</li><li>卸载某个依赖，需手动删除 <code>pom.xml</code> 中依赖后执行 <code>mvn clean install</code>——相当于npm uninstall xxx</li><li>mvn package相当于npm run build</li><li>Maven构建打包功能，把一堆Java开发代码打包成一个.jar文件（压缩包）——Npm把一堆前端代码打包成一个dist文件夹</li></ul><blockquote>Java基础，面向对象、类、接口、抽象、封装、继承、多态、线程、IO流 本文暂不赘述...</blockquote><h2>Svelte的增删改查尝鲜</h2><ul><li>Svelte也有生命周期，如<code>import { onMount } from "svelte";</code></li><li>也可以数据双向绑定，如<code>bind:value={variable}</code></li><li>也有计算属性，如<code>$: computed = expression</code></li><li>可直接响应式变量，如<code>let variable = value</code></li><li>事件绑定语法是<code>on:click={handler}</code></li><li>阻止冒泡<code>on:click|stopPropagation</code></li><li>也有条件渲染，如</li></ul><pre><code class="js">{#if condition}
  &lt;!-- 内容 --&gt;
{:else}
  &lt;!-- 其他内容 --&gt;
{/if}</code></pre><ul><li>也有循环v-for、map渲染</li></ul><pre><code class="js">{#each items as item (item.id)}
  &lt;!-- 循环内容 --&gt;
{/each}</code></pre><ul><li>也可组件化开发项目（单文件直接和vue类似，直接 <code>&lt;script&gt;</code>、<code>&lt;style&gt;</code>、<code>HTML模板</code>）</li><li>也可以import和export</li><li>分页组件父组件传对象和做事件处理<code>&lt;Page {pageInfo} on:pageChange={handlePageChange} /&gt;</code></li><li>对应子组件接收是</li></ul><pre><code class="js">// Props - 接收整个分页信息对象
export let pageInfo = {
    currentPage: 1,
    pageSize: 10,
    total: 0,
};</code></pre><ul><li>子组件触发父组件使用<code>createEventDispatcher</code>，如</li></ul><pre><code class="js">import { createEventDispatcher } from "svelte";

// 创建事件分发器
const dispatch = createEventDispatcher();

// 按钮点击的回调
dispatch("pageChange", newPage);</code></pre><p>比如和React语法对比：</p><ol><li><strong>更简洁的语法</strong>: 无需 <code>useState</code>、<code>useEffect</code> 等Hook</li><li><strong>真正的响应式</strong>: 直接赋值触发更新，不需要 <code>setState()</code></li><li><strong>更少的样板代码</strong>: 无需频繁的解构和回调</li><li><strong>更小的包体积</strong>: 编译时优化，运行时更轻量</li><li><strong>内置动画支持</strong>: <code>transition</code>、<code>animate</code> 指令</li><li><strong>CSS作用域自动化</strong>: 无需CSS Modules或CSS-in-JS</li></ol><blockquote>更多完整代码和注释，参见github仓库的前后端代码，创作不易，感谢支持点赞鼓励😉😉😉</blockquote>]]></description></item><item>    <title><![CDATA[vue3前端pdf直接预览 兔子先森 ]]></title>    <link>https://segmentfault.com/a/1190000047382650</link>    <guid>https://segmentfault.com/a/1190000047382650</guid>    <pubDate>2025-11-09 13:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>安装依赖</p><pre><code>npm i vue-pdf-embed</code></pre><p>页面结构</p><pre><code>&lt;vue-pdf-embed
  :source="state.source"
  :style="scaleFun"
  :page="state.pageNum"
  @rendered="pdfRendered"
  @rendering-failed="pdfRendered"
  /&gt;</code></pre><p>使用</p><pre><code>&lt;script setup lang="ts"&gt;
import VuePdfEmbed from 'vue-pdf-embed';

  const state = ref({
  source: 'https://www.test.com/pdf/test.pdf', // 预览pdf文件地址
  pageNum: 1, // 当前页面
  scale: 1, // 缩放比例
  numPages: 0 // 总页数
});

// pdf样式
const scaleFun = computed(() =&gt; {
  return {
    width: '70vh', //按照pdf大约7：10的宽高比，对应80vh的高度，直接设置高度不生效
    // height: "80vh",
    transform: `scale(${state.value.scale})`
  };
});

// 由于pdf资源加载是需要时间的的，可以给一个loading，可以手动在弹窗里加一个loading
const pdfLoading = ref(false);
const pdfRendered = () =&gt; {
  // pdf加载完
  pdfLoading.value = false;
};
&lt;/script&gt;</code></pre><p>适用于弹窗直接预览PDF的场景，你可以在弹窗事件里替换url链接</p><pre><code>const onOpen = () =&gt; {
  open.value = true;
  pdfLoading.value = true;
  state.value.source = 'https://www.test.com/pdf/测试文档.pdf'
}</code></pre><p><img width="723" height="557" referrerpolicy="no-referrer" src="/img/bVdmYx6" alt="" title=""/></p><hr/><p>参考资料：<br/>npm：<a href="https://link.segmentfault.com/?enc=czqZZKypDyIco%2BBCYUSfcw%3D%3D.sVBkSI%2FblL0XwCzrUSVr2dcnONhGyT9LlN%2BXMDcfaNk8oAPXAtc0koN%2BuQqYWQ%2BN" rel="nofollow" target="_blank">https://www.npmjs.com/package/vue-pdf-embed</a><br/>github：<a href="https://link.segmentfault.com/?enc=VsQECsfZBMFtZVfABqjbOw%3D%3D.DH%2BCPxQZTG9eWx281jArmu1TZ6L7OdDUJ3z9sOvlmfrwZf%2BX5js0Um%2Fhu9lFzh6J" rel="nofollow" target="_blank">https://github.com/hrynko/vue-pdf-embed</a></p>]]></description></item><item>    <title><![CDATA[低代码如何助力物流企业的仓储管理由“纸质]]></title>    <link>https://segmentfault.com/a/1190000047382762</link>    <guid>https://segmentfault.com/a/1190000047382762</guid>    <pubDate>2025-11-09 13:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。<br/>yayijia.cc/thread-93443-1-1.html<br/>yayijia.cc/thread-93439-1-1.html<br/>yayijia.cc/thread-93435-1-1.html<br/>yayijia.cc/thread-93434-1-1.html<br/>yayijia.cc/thread-93415-1-1.html<br/>yayijia.cc/thread-93410-1-1.html<br/>yayijia.cc/thread-93404-1-1.html<br/>yayijia.cc/thread-93403-1-1.html<br/>yayijia.cc/thread-93389-1-1.html</p>]]></description></item><item>    <title><![CDATA[在线表格技术如何助力企业实现全面预算？0]]></title>    <link>https://segmentfault.com/a/1190000047382767</link>    <guid>https://segmentfault.com/a/1190000047382767</guid>    <pubDate>2025-11-09 13:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。<br/>yayijia.cc/thread-93561-1-1.html<br/>yayijia.cc/thread-93560-1-1.html<br/>yayijia.cc/thread-93541-1-1.html<br/>yayijia.cc/thread-93537-1-1.html<br/>yayijia.cc/thread-93532-1-1.html<br/>yayijia.cc/thread-93531-1-1.html<br/>yayijia.cc/thread-93514-1-1.html<br/>yayijia.cc/thread-93512-1-1.html<br/>yayijia.cc/thread-93507-1-1.html</p>]]></description></item><item>    <title><![CDATA[活字格——低代码界的“黑神话悟空”05 ]]></title>    <link>https://segmentfault.com/a/1190000047382769</link>    <guid>https://segmentfault.com/a/1190000047382769</guid>    <pubDate>2025-11-09 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。<br/>yayijia.cc/thread-93615-1-1.html<br/>yayijia.cc/thread-93610-1-1.html<br/>yayijia.cc/thread-93608-1-1.html<br/>yayijia.cc/thread-93594-1-1.html<br/>yayijia.cc/thread-93590-1-1.html<br/>yayijia.cc/thread-93586-1-1.html<br/>yayijia.cc/thread-93585-1-1.html<br/>yayijia.cc/thread-93566-1-1.html<br/>yayijia.cc/thread-93564-1-1.html</p>]]></description></item><item>    <title><![CDATA[ITIL 4 汇总考试题及解题分析 IT]]></title>    <link>https://segmentfault.com/a/1190000047382183</link>    <guid>https://segmentfault.com/a/1190000047382183</guid>    <pubDate>2025-11-09 12:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>原始题号：78</strong><br/><strong>第1题</strong><br/>哪个选项对于服务台实践来说是正确的表述？<br/>A. 服务台必须是全人工服务<br/>B. 服务台要避免用户自助服务<br/>C. 服务台要广泛理解业务<br/>D. 服务台要高度技术化<br/><strong>答案：C</strong><br/><strong>解题分析：</strong><br/>本题考查的是服务台实践的核心特征和定位。理解服务台的正确角色对于建立有效的用户支持至关重要。<br/>服务台需要广泛理解业务，这是其最重要的特征之一。服务台不仅仅是技术支持点，更是服务提供者与用户之间的桥梁和情感纽带。服务台人员需要理解用户所在的业务环境、业务流程、业务目标，以及IT服务如何支持这些业务活动。这种业务理解使服务台能够从用户的角度思考问题，提供更有价值的支持。<br/>广泛的业务理解帮助服务台人员更好地沟通。当用户报告问题时，他们通常用业务术语描述，而不是技术术语。例如，用户可能说"我无法处理订单"，而不是"CRM系统无响应"。具备业务理解的服务台人员能够快速理解问题的业务影响，进行适当的优先级排序，并用用户能理解的语言进行沟通。<br/>这种业务导向也体现在服务台的价值主张上。服务台应该关注用户体验和业务成果，而不仅仅是技术问题的解决。通过理解更广泛的组织背景，服务台可以识别改进机会，为持续改进提供有价值的洞察。<br/>选项A"服务台必须是全人工服务"是不正确的。现代服务台应该平衡自动化和人工服务。自动化可以处理简单、重复性的请求和事件，使人工服务台人员能够专注于需要人类判断、同理心和创造力的复杂问题。聊天机器人、自助服务门户、自动化工作流等都是服务台的重要组成部分。<br/>选项B"服务台要避免用户自助服务"也不正确，实际上与最佳实践相反。服务台应该积极提供和推广自助服务选项。自助服务使用户能够快速获得常见问题的答案，重置密码，提交标准请求等，无需等待人工响应。这既提高了用户满意度（因为获得了即时服务），也提高了服务台效率（减少了简单请求的工作量）。<br/>选项D"服务台要高度技术化"也不是正确的定位。虽然服务台人员需要一定的技术知识，但服务台的核心价值在于沟通能力、同理心、业务理解和协调能力，而不是深度技术专长。深度技术诊断通常由专业支持团队（二线、三线）负责。服务台的作用更多是理解用户需求、进行初始分类、应用已知解决方案，以及在必要时有效地升级到合适的技术团队。<br/>优秀的服务台人员具备多方面的技能：出色的沟通和倾听能力、同理心和客户服务意识、业务流程理解、基本的技术知识、问题解决能力、以及在压力下保持专业的能力。技术深度不是最重要的要求。<br/>在ITIL 4的视角下，服务台被视为服务提供者与用户之间的关键接触点，对用户体验有重大影响。因此，服务台应该以用户为中心，理解业务，提供多渠道支持（包括电话、邮件、聊天、自助服务等），并持续改进以满足不断变化的用户需求。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.14 小节</strong></p><p><strong>原始题号：106</strong><br/><strong>第2题      本习题取自长河ITIL 4训练营。</strong><br/>组织如何让第三方供应商协助服务的持续改进？<br/>A. 确保供应商在合同中详述他们的服务改进方案<br/>B. 请求证明供应商使用了敏捷开发方式<br/>C. 请求证明供应商通过项目管理实践实施了所有改进措施<br/>D. 确保供应商的所有问题管理活动均可促成改进<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题考查的是如何将供应商纳入组织的持续改进活动中。在现代服务交付中，外部供应商往往提供关键服务组件，因此他们的改进能力直接影响整体服务质量。<br/>确保供应商在合同中详述他们的服务改进方案是让供应商参与持续改进的最有效方法。通过在合同中明确规定改进的期望、方法和承诺，组织可以确保持续改进成为供应商关系的正式组成部分，而不仅仅是非正式的期望。这种合同化的方法为双方提供了清晰的框架和问责机制。<br/>合同中的改进条款可以包括多个方面：定期的服务审查会议，在会上讨论改进机会；服务级别的逐步提升目标（如逐年提高可用性或降低响应时间）；创新和改进建议的提交要求；基于绩效的激励机制，奖励超越基准的改进；以及改进成果的报告和度量要求。<br/>这种方法将改进嵌入到供应商关系管理中，使其成为持续对话的一部分。它也为组织提供了杠杆，可以要求供应商证明其改进活动和成果。通过定期审查供应商的改进计划和结果，组织可以确保供应商的改进与自身的战略目标保持一致。<br/>此外，合同条款可以要求供应商参与组织的改进活动，分享最佳实践，或协作开发新的服务能力。这种集成的方法将供应商视为价值共创的伙伴，而不仅仅是商品提供者。<br/>选项B"请求证明供应商使用了敏捷开发方式"过于具体和限制性。虽然敏捷方法可能支持某些类型的改进，但它不是唯一有效的方法。更重要的是改进的结果和价值，而不是使用的具体方法。强制要求特定方法可能限制供应商的灵活性和创新能力。不同的供应商和服务可能适合不同的改进方法。<br/>选项C"请求证明供应商通过项目管理实践实施了所有改进措施"同样过于规范和不切实际。不是所有的改进都需要或适合作为正式项目来管理。许多有价值的改进是增量式的、持续的，可以通过日常运营活动实现。要求所有改进都通过项目管理实践可能造成不必要的官僚主义和延误。<br/>选项D"确保供应商的所有问题管理活动均可促成改进"虽然听起来合理，但不切实际且范围过窄。不是所有的问题管理活动都必然导致改进——有些问题可能只需要临时解决方案或接受风险。此外，持续改进的来源不仅限于问题管理，还包括来自客户反馈、技术创新、行业最佳实践等多种渠道的改进机会。<br/>在实践中，有效的供应商改进管理还需要建立协作关系、定期沟通、共享改进目标、以及相互信任。合同提供了框架，但成功的持续改进还需要双方的承诺和积极参与。组织应该将关键供应商视为战略合作伙伴，共同探索如何为最终客户创造更多价值。<br/><strong>参考官方ITIL 4 Foundation著作的 5.1.13 小节</strong></p><p><strong>原始题号：120</strong><br/><strong>第3题</strong><br/>下列哪项指导原则建议评估当前状态并决定哪些方面可重复利用？<br/>A. 聚焦价值<br/>B. 基于当前情况开始<br/>C. 协作并促进可见性<br/>D. 利用反馈迭代式进展<br/><strong>答案：B</strong><br/><strong>解题分析：</strong><br/>本题考查的是"基于当前情况开始"（Start where you are）这一ITIL 4指导原则的核心内容。这个原则强调在启动任何改进或新倡议之前，首先要理解当前状态。<br/>"基于当前情况开始"原则建议组织评估当前状态并决定哪些方面可以重复利用。这意味着不要假设必须从零开始或完全推翻现有的流程、工具和实践。相反，应该客观地评估现状，识别已经运作良好的部分，并在此基础上构建改进。这种方法节省时间和资源，利用现有投资，并尊重组织已经积累的知识和经验。<br/>评估当前状态包括几个关键活动：直接观察实际工作如何进行，而不仅仅依赖文档或流程图；与实际执行工作的人员交谈，了解他们的经验和见解；收集和分析数据以客观地了解绩效；识别哪些方面运作良好并应该保留；识别哪些方面需要改进或替换；以及理解为什么事情以当前方式运作（可能有历史原因或约束）。<br/>这个原则还警告不要被"闪亮的新对象"所诱惑——新的工具、方法或实践可能看起来很吸引人，但如果现有的解决方案已经有效，就没有必要替换它。改变本身带有成本和风险，只有当预期收益明显超过这些成本和风险时才是合理的。<br/>在实践中应用这个原则意味着要问一些关键问题：我们已经有什么？哪些运作良好？哪些可以重复使用或调整？哪些必须替换？为什么当前的做法会这样？有哪些约束或依赖关系需要考虑？<br/>这个原则与"保持简单实用"和"聚焦价值"等其他原则相辅相成。通过从当前情况开始，组织可以避免不必要的复杂性，专注于真正需要改变的方面。<br/>选项A"聚焦价值"强调所有活动都应该与为利益相关者创造价值相关联。虽然这很重要，但它不是特别关于评估当前状态和决定什么可以重复利用的原则。<br/>选项C"协作并促进可见性"强调跨团队和利益相关者的协作，以及使工作可见以促进改进。这对于评估当前状态很有帮助（例如，通过与不同团队协作以获得全面视图），但它不是主要关于重复利用现有能力的原则。<br/>选项D"利用反馈迭代式进展"强调将工作分解为可管理的部分，并通过反馈循环不断改进。这个原则关注如何推进改进工作，而不是如何评估当前状态和重复利用现有能力。<br/>"基于当前情况开始"原则对于避免常见的陷阱特别有价值。许多改进倡议失败是因为它们忽视了现有的良好实践、知识和工具，试图强加一个完全新的方式。这不仅浪费了已有的投资，还可能遇到不必要的阻力。通过从当前情况开始，组织可以建立在现有优势的基础上，进行增量式改进，这通常比激进的重组更成功。<br/><strong>参考官方ITIL 4 Foundation著作的 4.3.2 小节</strong></p><p><strong>原始题号：192</strong><br/><strong>第4题</strong><br/>何时应针对标准变更进行全面的风险评估和授权？<br/>A. 每次实施标准更改时<br/>B. 创建标准更改的过程时<br/>C. 每年至少一次<br/>D. 当请求紧急变更时<br/><strong>答案：B</strong><br/><strong>解题分析：</strong><br/>本题考查的是标准变更的风险评估和授权时机。理解标准变更与其他类型变更的区别对于有效实施变更支持实践至关重要。<br/>标准变更的全面风险评估和授权应该在创建标准变更的流程或程序时进行，而不是每次实施时。这是标准变更的核心特征——它们是预先授权的、低风险的变更，遵循预定义和充分理解的流程。一旦标准变更的流程被评估、批准和文档化，后续的实施就不需要每次都进行完整的风险评估和授权。<br/>创建标准变更流程时的评估应该是全面和严格的。它需要分析变更的潜在风险、影响范围、失败可能性、回退选项等所有方面。必须确认这类变更确实是低风险的、充分理解的，并且有明确的流程可以遵循。这个初始评估由适当的变更授权人批准，批准后该流程就成为标准变更，可以按照预定义的步骤重复执行，无需每次都经过完整的变更评估流程。<br/>标准变更的例子包括：为用户添加账户、重置密码、安装标准软件包、应用已测试的补丁、更换标准硬件组件等。这些变更之所以能成为标准变更，是因为它们已经被充分理解，风险已知且可控，有清晰的执行步骤，并且已经经过了全面的初始评估和授权。<br/>这种预先授权机制大大提高了变更的效率。对于频繁发生的低风险变更，如果每次都需要完整的评估和授权流程，会造成不必要的延误和资源浪费。标准变更机制允许这些变更快速执行，同时仍然保持适当的控制。<br/>选项A"每次实施标准更改时"是不正确的，这会违背标准变更的根本目的。如果每次实施都需要全面的风险评估和授权，那就不再是标准变更，而是正常变更了。标准变更的优势就在于预先授权，避免重复的评估过程。<br/>选项C"每年至少一次"虽然提到了定期审查的概念，但它不是初始的全面风险评估和授权的正确时机。不过，定期审查标准变更流程确实是良好实践——组织应该定期审查其标准变更，确认它们仍然是低风险的，流程仍然有效，并且没有发生改变使它们需要重新分类。但这是持续管理活动，不是初始的全面评估时机。<br/>选项D"当请求紧急变更时"完全不相关。紧急变更是必须尽快实施的变更，通常是为了解决严重事件或实施安全补丁。紧急变更有自己的评估和授权流程（虽然可能加快），与标准变更的创建无关。<br/>值得注意的是，虽然标准变更是预先授权的，但仍然需要记录和跟踪每次实施。这对于审计、知识管理和持续改进都很重要。组织应该监控标准变更的成功率，如果某个标准变更开始频繁失败或出现意外后果，可能需要重新评估其标准变更地位。<br/>标准变更的管理也需要清晰的文档化。每个标准变更应该有详细的工作指令，说明谁可以请求、谁可以执行、具体步骤是什么、需要什么资源、预计时间等。这确保了执行的一致性和质量。<br/>将合适的变更定义为标准变更是优化变更支持实践的重要方式。它平衡了风险控制与效率，使组织能够快速响应常见需求，同时对高风险或复杂变更保持严格控制。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.4 小节</strong></p><p><strong>原始题号：238</strong><br/><strong>第5题</strong><br/>IT 服务连续性战略应基于以下哪一项？</p><ol><li>服务指标的设计；2. 业务连续性战略；3. 业务影响分析；4. 风险评估<br/>A. 1、2 和 4<br/>B. 1、2 和 3<br/>C. 2、3 和 4<br/>D. 1、3 和 4<br/><strong>答案：C</strong><br/><strong>解题分析：</strong><br/>本题考查的是IT服务连续性战略的制定基础。服务连续性管理实践旨在确保组织在灾难或重大中断发生时能够继续提供关键服务，其战略制定需要基于多个关键输入。<br/>IT服务连续性战略应该基于：业务连续性战略（选项2）、业务影响分析（选项3）和风险评估（选项4）。这三个要素共同为IT服务连续性规划提供了必要的背景和方向。<br/>业务连续性战略是IT服务连续性战略的基础和驱动力。IT服务连续性必须支持和实现业务连续性目标。业务连续性战略定义了组织在面对中断时的总体方法、优先级和恢复目标。IT服务连续性战略必须与业务连续性战略保持一致，确保IT恢复能力支持关键业务流程的持续运行。如果IT和业务的连续性战略不一致，可能导致资源错配或无法满足业务需求。<br/>业务影响分析（BIA）是制定服务连续性战略的关键输入。BIA评估服务中断对业务的影响，包括财务损失、声誉损害、监管违规等。它帮助识别关键业务流程和支持这些流程的IT服务，确定恢复时间目标（RTO）和恢复点目标（RPO）。BIA的结果指导IT服务连续性投资的优先级——哪些服务需要最快恢复、哪些可以容忍较长的中断时间。没有BIA，组织可能在错误的地方投资连续性能力。<br/>风险评估识别可能导致服务中断的威胁和漏洞，以及这些威胁发生的可能性。常见的风险包括自然灾害（火灾、洪水、地震）、技术故障（硬件故障、软件错误）、人为错误、网络攻击、供应商失败等。风险评估帮助组织理解面临的威胁景观，并相应地设计连续性措施。它还支持成本效益分析——投资于连续性措施的成本应该与风险的可能性和影响相称。<br/>这三个要素相互关联、相互支持。业务连续性战略提供战略方向和目标，BIA识别优先级和恢复要求，风险评估识别需要防范的威胁。基于这三个输入，IT服务连续性战略可以确定适当的连续性方法（如热备份、冷备份、云灾备等）、所需的投资水平、恢复计划的范围等。<br/>选项1"服务指标的设计"不是制定服务连续性战略的基础要素。虽然服务指标在监控和管理日常服务交付中很重要，但它们不是制定连续性战略的主要输入。连续性战略更多地基于业务需求、影响分析和风险评估，而不是日常运营指标。服务指标可能在连续性计划实施后用于监控恢复能力，但不是战略制定的基础。<br/>在实践中，IT服务连续性战略还应考虑其他因素，如监管要求、组织风险承受能力、可用预算、技术可行性等。但业务连续性战略、业务影响分析和风险评估是最基础和最关键的三个要素。<br/>有效的服务连续性管理不仅仅是制定计划，还包括定期测试、培训、维护和改进。连续性计划应该至少每年测试一次，测试结果应该用于改进计划。随着业务和技术的变化，连续性战略和计划也需要相应更新。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.12 小节</strong></li></ol><p><img width="416" height="378" referrerpolicy="no-referrer" src="/img/bVdmYqS" alt="" title=""/></p><p><strong>原始题号：337</strong><br/><strong>第6题</strong><br/>下面哪项可以促成客户想要的结果？<br/>A. 服务<br/>B. 功效<br/>C. 组织<br/>D. IT<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题再次强化了"服务"的核心定义，这是ITIL 4框架的基础概念，值得从不同角度深入理解。<br/>服务是通过促成客户想要实现的结果（outcome）来实现价值共创的手段。这个定义抓住了服务的本质——服务不是为了交付技术、产品或活动本身，而是为了帮助客户实现其业务目标和期望的成果。服务使客户能够实现结果，而无需管理与服务交付相关的特定成本和风险。<br/>理解这个定义的关键在于区分"输出"（output）和"结果"（outcome）。输出是服务活动创造的有形或无形交付物，而结果是这些输出为利益相关者带来的实际价值和业务效益。客户真正关心的是结果，而不是输出。例如，电子邮件系统的输出可能是邮件的发送和接收能力，但客户想要的结果是"团队成员之间高效、及时的沟通"或"能够与客户保持联系"。<br/>服务通过两个关键维度促成结果：效用（utility）——服务提供什么功能来支持期望的结果；保修（warranty）——服务如何可靠、安全、及时地交付这些功能。两者的结合使服务能够有效地促成客户的结果。单有功能而不可靠，或者单有可靠性而功能不足，都无法充分促成结果。<br/>服务还通过管理成本和风险来促成结果。通过使用服务，客户可以避免某些投资（如硬件采购、软件开发）和运营成本（如维护、支持人员），同时将某些风险（如技术过时、容量不足）转移给服务提供者。这种成本和风险的转移使客户能够专注于其核心业务活动，从而更有效地实现其战略目标。<br/>选项B"功效"（warranty，或译保修）只是服务价值的一个组成部分。功效提供服务将可靠、安全、有足够容量交付的保证，但它本身不能促成结果。功效必须与效用结合，通过完整的服务来交付，才能促成客户的结果。仅有功效承诺而没有相应的功能，无法为客户创造价值。<br/>选项C"组织"是提供服务的实体或消费服务的实体。组织通过提供或使用服务来创造价值，但组织本身不是促成结果的手段。是组织提供的服务促成了结果，而不是组织本身。组织是服务管理的参与者，但不是价值创造的直接机制。<br/>选项D"IT"（信息技术）是支撑服务的技术手段。IT包括硬件、软件、网络等技术组件，这些是服务的组成部分或支撑要素，但IT技术本身不能直接促成业务结果。客户不关心服务器的规格或使用的编程语言，他们关心的是能否实现业务目标。只有当IT以服务的形式被设计和管理，与业务需求对齐时，它才能促成有价值的结果。<br/>这个以结果为导向的服务定义对服务管理实践有深远影响。它意味着服务设计应该从理解和定义期望的结果开始，然后向后推导所需的功能和保修特征。服务级别管理应该测量和报告与业务结果相关的指标，而不仅仅是技术指标。持续改进应该聚焦于如何更好地促成客户结果，而不仅仅是内部流程优化。<br/>在与客户互动时，服务提供者应该使用结果语言进行沟通。与其说"我们提供99.9%的正常运行时间"，不如说"我们确保您的团队能够随时访问所需的应用，以便及时服务客户"。这种以结果为中心的沟通方式更能引起客户共鸣，因为它直接关联到他们关心的业务价值。<br/><strong>参考官方ITIL 4 Foundation著作的 2.3 小节</strong></p><p><strong>原始题号：357</strong><br/><strong>第7题</strong><br/>哪两项是"服务请求管理"实践的重要方面？</p><ol><li>标准化和自动化；2. 提供多种接入渠道；3. 建立目标的共享视图；4. 审批政策。<br/>A. 1 和 2<br/>B. 2 和 3<br/>C. 3 和 4<br/>D. 1 和 4<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题考查的是服务请求管理实践的关键特征和成功要素。理解这些要素有助于组织有效地设计和实施服务请求管理。<br/>标准化和自动化（选项1）是服务请求管理实践的核心要素。标准化意味着为常见的服务请求定义清晰、一致的流程和工作指令，包括请求的分类、履行步骤、所需资源、完成时间等。标准化确保服务请求以可预测、可重复的方式处理，提高了一致性和质量。它也使得培训新员工和知识转移变得更容易。<br/>自动化是服务请求管理实现效率和可扩展性的关键。许多服务请求是重复性的、标准的，非常适合自动化处理。例如，密码重置、软件访问授权、标准设备配置等都可以通过工作流自动化来实现。自动化不仅提高了处理速度（用户获得即时或近即时的服务），还减少了人为错误，释放了服务台人员的时间去处理更复杂、需要人类判断的任务。<br/>现代服务请求管理应该尽可能地实现端到端自动化——从请求提交、验证、批准（如果需要）、履行到确认完成。这种自动化通过服务管理工具、工作流引擎、集成的配置管理数据库等技术实现。<br/>提供多种接入渠道（选项2）也是服务请求管理的重要方面。现代用户期望能够通过他们偏好的渠道提交请求，这可能包括：自助服务门户、移动应用、电子邮件、电话、聊天、甚至社交媒体。多渠道策略提高了便利性和用户满意度，使用户能够在最适合他们情况的时间和方式访问服务。<br/>不同的渠道可能适合不同类型的请求或不同的用户群体。例如，技术熟练的用户可能偏好自助服务门户，可以快速提交请求而无需等待；而其他用户可能更喜欢通过电话与服务台交谈。提供选择权体现了以用户为中心的服务设计。<br/>重要的是，虽然提供多种渠道，但后端的处理应该是统一和集成的。无论请求通过哪个渠道进入，都应该在同一个系统中记录和管理，使用相同的工作流和优先级规则。这确保了一致的服务质量和可见性。<br/>选项3"建立目标的共享视图"更多地与计划、持续改进或战略一致性相关，不是服务请求管理实践的特定方面。虽然服务请求管理应该与组织目标保持一致，但这不是其定义性特征。<br/>选项4"审批政策"虽然是服务请求管理的一个组成部分，但不是其最重要或最具特征性的方面。实际上，有效的服务请求管理应该尽量减少不必要的审批，对于标准、低风险的请求应该实现预先授权和自动履行。过多的审批会减慢服务交付，降低用户满意度。审批应该保留给涉及显著成本、风险或资源分配的请求。<br/>标准化和自动化、多渠道接入的结合创造了现代、高效、用户友好的服务请求管理。用户可以通过他们偏好的渠道快速提交标准请求，系统自动处理和履行，用户及时收到确认和结果。这种体验大大提高了用户满意度和服务效率。<br/>组织在实施服务请求管理时应该识别最常见的请求类型，优先对这些请求进行标准化和自动化。应该定期审查服务请求模式，识别新的标准化和自动化机会。同时，应该投资于用户友好的接入渠道，使提交请求尽可能简单和直观。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.16 小节</strong></li></ol>]]></description></item><item>    <title><![CDATA[ITIL 4 基础级试题含解题思路 IT]]></title>    <link>https://segmentfault.com/a/1190000047382215</link>    <guid>https://segmentfault.com/a/1190000047382215</guid>    <pubDate>2025-11-09 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>原始题号：42</strong><br/><strong>第1题</strong><br/>哪一个指导原则需要理解组织的各个部分是如何以一种集成的方式协同工作的？<br/>A. 基于反馈的迭代推进<br/>B. 优化和自动化<br/>C. 保持简单实用<br/>D. 通盘思考和工作<br/><strong>答案：D</strong><br/><strong>解题分析：</strong><br/>本题考查的是"通盘思考和工作"（Think and work holistically）这一ITIL 4指导原则的核心理念。这个原则强调从整体系统的角度理解和管理服务。<br/>"通盘思考和工作"原则要求组织理解其各个部分是如何以集成的方式协同工作的。这意味着我们不能孤立地看待单个流程、实践、团队或技术组件，而要理解它们如何相互连接、相互影响，共同为价值创造做出贡献。这个原则认识到服务交付是一个复杂的系统，其整体效果不仅仅是各部分的简单加总。<br/>在实践中，应用这个原则意味着要考虑服务管理的四个维度（组织和人员、信息和技术、合作伙伴和供应商、价值流和流程），确保它们之间的平衡和协调。例如，在改进事件管理时，不能只关注流程优化，还要考虑人员技能、使用的工具、与供应商的协作，以及事件管理如何与问题管理、变更支持等其他实践协同工作。<br/>这个原则还提醒我们要理解服务价值系统（SVS）中各组成部分的相互作用。指导原则、治理、服务价值链、实践和持续改进不是孤立运作的，而是作为一个整合的系统共同支持价值创造。改变系统的一个部分会影响其他部分，因此需要整体性的思考。<br/>通盘思考还意味着要理解组织的端到端服务交付。服务不是由单个团队或部门提供的，而是通过跨职能协作来实现的。价值流的概念体现了这种整体视角，展示了不同活动和实践如何串联起来创造价值。<br/>选项A"基于反馈的迭代推进"强调将工作分解为可管理的部分，通过快速反馈循环不断改进。虽然这也很重要，但它不是关于理解组织各部分如何集成协同工作的。<br/>选项B"优化和自动化"关注的是简化和自动化工作以提高效率，在优化之前要先理解工作如何进行。这个原则不直接涉及理解组织的整体集成方式。<br/>选项C"保持简单实用"强调消除不必要的复杂性，只保留能创造价值的元素。虽然与整体思考相关，但它主要关注简化，而不是理解集成。<br/>通盘思考和工作原则对于避免"孤岛效应"（silo mentality）特别重要。在许多组织中，不同部门或团队各自为政，优化各自的指标，但这可能导致整体次优化。通过采用整体视角，组织可以识别跨职能的改进机会，确保局部优化不会损害整体效果。<br/><strong>参考官方ITIL 4 Foundation著作的 4.3.5 小节</strong></p><p><strong>原始题号：155</strong><br/><strong>第2题</strong><br/>下面哪个角色并非事件诊断和解决的参与方？<br/>A. IT 值班和巡检人员<br/>B. 支持团队<br/>C. 服务台<br/>D. 合作伙伴和供应商*<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题考查的是事件管理实践中参与事件诊断和解决的各方角色。理解谁应该参与事件处理对于建立有效的事件管理流程至关重要。<br/>IT值班和巡检人员通常不直接参与事件的诊断和解决。他们的主要职责是监控IT基础设施，进行日常巡检，执行预防性维护任务，以及在发现异常时报告问题。当值班人员发现潜在问题时，他们会创建事件记录或事态记录，然后将其转交给适当的团队进行处理，但他们自己通常不负责诊断和解决复杂的事件。<br/>事件诊断和解决通常涉及以下角色：<br/>服务台（选项C）是事件处理的第一联系点，负责记录事件、进行初始分类和优先级排序，并尝试解决简单的事件。服务台人员使用已知错误数据库和解决方案知识库进行诊断，如果能够解决，就直接关闭事件；如果不能解决，就升级到专业支持团队。<br/>支持团队（选项B）包括二线和三线技术专家，负责诊断和解决更复杂的事件。当服务台无法解决事件时，会将事件升级到适当的专业支持团队，如网络团队、数据库团队、应用支持团队等。这些团队拥有深入的技术知识和专业技能，能够进行详细的诊断和提供解决方案。<br/>合作伙伴和供应商（选项D）也可能参与事件诊断和解决，特别是当事件涉及他们提供的产品或服务时。例如，如果事件与第三方软件、云服务或硬件有关，可能需要联系供应商的技术支持团队协助诊断和解决。供应商管理实践确保与外部供应商建立有效的协作关系。<br/>值班和巡检人员的角色更多地与监控和事态管理实践相关，而不是直接的事件诊断和解决。他们通过主动监控帮助及早发现问题，从而可能减少事件的影响，但这是预防性的活动，而不是响应性的诊断和解决。<br/>需要注意的是，在某些组织中，角色定义可能有所不同。例如，在小型组织中，值班人员可能身兼多职，既负责监控又参与事件解决。但从ITIL的标准实践角度来看，值班和巡检人员的主要职责与事件诊断解决是有区别的。<br/>有效的事件管理需要明确定义各个角色的职责，建立清晰的升级路径，并确保所有参与方能够有效协作。服务台需要知道何时以及如何升级事件，支持团队需要及时响应升级的事件，供应商需要按照服务级别协议提供支持。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.5 小节</strong></p><p><img width="416" height="378" referrerpolicy="no-referrer" src="/img/bVdmYqS" alt="" title=""/></p><p><strong>原始题号：157</strong><br/><strong>第3题</strong><br/>下面哪项是"保持简单实用"指导原则的关键考虑因素？<br/>A. 努力为每个异常创建一个解决方案<br/>B. 了解每个元素如何有助于价值创造<br/>C. 忽略不同利益相关者之间有所冲突的目标<br/>D. 从复杂解决方案开始，然后简化<br/><strong>答案：B</strong><br/><strong>解题分析：</strong><br/>本题考查的是"保持简单实用"指导原则在实际应用中的关键考虑因素。这个原则要求组织专注于真正创造价值的要素，消除不必要的复杂性。<br/>"了解每个元素如何有助于价值创造"是应用"保持简单实用"原则的核心考虑因素。这意味着在设计流程、实施实践或增加任何步骤、控制或功能时，都要问："这个元素是如何为价值创造做出贡献的？"如果一个元素不能清晰地说明其价值贡献，那么它可能是不必要的，应该被移除或重新考虑。<br/>这种价值导向的方法帮助组织避免"为了完整而完整"的陷阱。有时组织会因为"最佳实践"或"行业标准"的缘故添加流程步骤或控制，而没有真正思考这些在自己的具体情境下是否必要。保持简单实用原则要求我们批判性地评估每个元素，只保留那些确实能创造价值的。<br/>这个考虑因素与"聚焦价值"原则紧密相关。价值是服务管理的核心目的，所有活动都应该服务于价值创造。通过理解每个元素的价值贡献，组织可以识别和消除浪费，提高效率，使流程更加精简和有效。<br/>在实践中，应用这个考虑因素意味着要定期审查现有的流程和实践。随着时间推移，流程往往会积累不再必要的步骤——这些步骤可能在过去有其理由，但环境变化后已不再适用。通过持续询问"这如何创造价值？"，组织可以保持流程的精益和相关性。<br/>选项A"努力为每个异常创建一个解决方案"违背了保持简单实用的原则。试图为每个可能的异常情况预先创建解决方案会导致过度复杂性和维护负担。更好的做法是处理常见的情况，然后用合理的判断和灵活性处理异常。<br/>选项C"忽略不同利益相关者之间有所冲突的目标"也不正确。保持简单实用不意味着忽略利益相关者的需求。相反，它要求我们理解真正重要的需求，并找到简单的方法来满足这些需求。解决目标冲突可能需要协商和权衡，但不能简单地忽略。<br/>选项D"从复杂解决方案开始，然后简化"的顺序是错误的。保持简单实用原则建议从简单开始，只在确实需要时才增加复杂性。先构建复杂系统再简化，会浪费时间和资源，而且往往难以真正简化。<br/>理解价值贡献还有助于向利益相关者解释为什么某些事情以特定方式进行，或为什么不采用某些建议。当能够清晰地说明价值链时，更容易获得支持和认同。<br/><strong>参考官方ITIL 4 Foundation著作的 4.3.6 小节</strong></p><p><strong>原始题号：240</strong><br/><strong>第4题</strong><br/>下面哪项是服务价值主张的一部分？<br/>A. 服务为消费者削减的成本<br/>B. 服务施加给消费者的成本<br/>C. 消费者接收到的服务输出<br/>D. 服务施加给消费者的风险<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题考查的是服务价值主张的构成要素。理解价值主张对于设计和营销服务，以及与客户沟通服务的价值至关重要。<br/>服务为消费者削减的成本是服务价值主张的重要组成部分。服务的价值不仅体现在它提供了什么功能或能力，还体现在它为消费者移除或减少了什么成本和风险。通过使用服务，消费者可以避免某些投资和运营成本，这些被削减的成本构成了服务价值的一部分。<br/>例如，使用云计算服务的价值主张包括：消费者不需要购买和维护自己的服务器硬件（削减资本支出成本），不需要雇佣数据中心运营人员（削减人力成本），不需要担心容量规划和扩展（削减管理复杂性）。这些被削减或转移的成本使得服务对消费者具有吸引力。<br/>服务价值主张通常包括三个方面的内容：服务为消费者增加的价值（如新的能力、改进的性能）、服务为消费者削减的成本、服务为消费者降低的风险。完整的价值主张需要平衡这三个方面，帮助消费者理解采用服务的总体价值。<br/>在ITIL 4的价值概念中，成本和风险是价值等式的重要组成部分。消费者在评估服务价值时，会权衡服务带来的成果与需要付出的成本和承担的风险。服务提供者通过承担某些成本和风险，使消费者能够专注于其核心业务活动，这是服务存在的基本逻辑。<br/>选项B"服务施加给消费者的成本"不是价值主张的正面部分，而是消费者需要权衡的代价。这包括服务的价格、使用服务所需的资源、过渡成本等。虽然这些成本是价值评估的一部分，但它们不是价值主张本身——价值主张强调的是正面的价值贡献。<br/>选项C"消费者接收到的服务输出"也不准确。输出（outputs）是服务活动的有形或无形交付物，但输出本身不等于价值。价值来自于输出所促成的成果（outcomes）。消费者关心的不是输出本身，而是这些输出如何帮助他们实现期望的成果。<br/>选项D"服务施加给消费者的风险"同样不是价值主张的部分，而是消费者需要考虑的负面因素。虽然任何服务都可能带来某些风险（如供应商依赖风险、服务中断风险等），但价值主张应该强调的是服务如何降低或转移消费者的风险，而不是施加风险。<br/>有效的价值主张清晰地传达了服务如何为特定的消费者群体创造价值。它应该基于对目标消费者需求、痛点和目标的深入理解。服务提供者应该能够量化或具体说明被削减的成本、被降低的风险以及被促成的成果，使价值主张具有说服力和可信度。<br/><strong>参考官方ITIL 4 Foundation著作的 2.5 小节</strong></p><p><strong>原始题号：258</strong><br/><strong>第5题</strong><br/>用什么术语来描述服务是否满足可用性、容量和安全要求？<br/>A. 成果<br/>B. 价值<br/>C. 效用<br/>D. 保修<br/><strong>答案：D</strong><br/><strong>解题分析：</strong><br/>本题考查的是服务价值的两个关键组成部分之一——"保修"（warranty）的定义。理解保修与效用的区别对于全面把握服务价值概念至关重要。<br/>保修（warranty）是指服务将满足约定要求的保证。它确保服务"适合使用"（fit for use），涉及服务的可用性、容量、连续性、安全性等非功能性属性。保修回答的是"服务能否可靠地交付？"的问题，关注的是服务的性能和质量特征。<br/>可用性要求涉及服务在需要时能够使用的程度。例如，服务可能承诺99.9%的正常运行时间，这是可用性保修的一部分。容量要求涉及服务能够处理的工作负载或用户数量。例如，服务可能保证能够同时支持1000个并发用户，这是容量保修。安全要求涉及保护数据和系统免受未授权访问或损害的能力，这也是保修的重要组成部分。<br/>保修与效用（utility）共同构成了服务的价值。效用关注的是"服务做什么？"——它提供哪些功能来支持客户想要的结果。而保修关注的是"服务如何执行？"——它是否可靠、安全、有足够的容量。一个服务要创造完整的价值，必须同时具备效用和保修。<br/>举例来说，一个在线银行服务的效用可能包括查看账户余额、转账、支付账单等功能。这些功能支持客户"管理个人财务"的成果。该服务的保修则包括：24/7可用性（可用性要求）、能够处理高峰时段的交易量（容量要求）、保护客户数据和交易安全（安全要求）、快速响应时间（性能要求）。没有可靠的保修，即使功能齐全的服务也无法为客户创造价值。<br/>选项A"成果"（outcome）是利益相关者获得或希望获得的结果。成果是服务价值的最终体现，但它不是描述服务如何满足可用性、容量和安全要求的术语。<br/>选项B"价值"（value）是更广泛的概念，包括效用、保修以及这些如何共同促成成果并管理成本和风险。价值不是专门描述可用性、容量和安全方面的术语。<br/>选项C"效用"（utility）是服务的功能性，即服务提供什么功能来满足特定需求。它与"适合目的"（fit for purpose）相关，而不是与可用性、容量和安全相关。<br/>在服务设计和服务级别管理中，明确定义保修要求非常重要。服务级别协议（SLA）通常包含大量关于保修的承诺，如正常运行时间百分比、响应时间、恢复时间目标等。这些保修承诺帮助设定客户期望，并为服务提供者提供明确的绩效目标。<br/>理解保修的概念还有助于组织在设计服务时平衡功能与性能。有时，过度关注增加新功能（效用）而忽视了可靠性和性能（保修），会导致服务虽然功能丰富但不稳定，最终无法为客户创造价值。<br/><strong>参考官方ITIL 4 Foundation著作的 2.5.4 小节</strong></p><p><strong>原始题号：347</strong><br/><strong>第6题      本习题取自长河ITIL 4训练营。</strong><br/>下面哪项可以促成客户想要的结果？<br/>A. 服务<br/>B. 功效<br/>C. 组织<br/>D. IT<br/><strong>答案：A</strong><br/><strong>解题分析：</strong><br/>本题再次考查"服务"的核心定义，这是ITIL 4价值共创理念的基础。深刻理解服务的本质对于应用整个ITIL框架至关重要。<br/>服务是通过促成客户想要实现的结果来实现价值共创的手段。这是服务的根本定义和存在意义。服务不仅仅是交付技术、产品或资源，而是帮助客户实现其业务目标和期望的成果，同时使客户无需管理特定的成本和风险。<br/>服务的这个定义体现了ITIL 4"聚焦价值"指导原则的核心思想。客户购买或使用服务的目的不是为了获得技术本身，而是为了实现特定的业务成果。例如，客户使用电子邮件服务不是为了拥有邮件服务器，而是为了实现"团队成员之间高效沟通"这个成果；客户使用ERP系统不是为了拥有软件，而是为了实现"业务流程自动化和数据集成"的成果。<br/>服务通过提供效用（functionality）和保修（assurance）来促成这些成果。效用确保服务具有客户需要的功能来支持期望的成果，保修确保服务能够可靠、安全地交付。通过组合效用和保修，服务使客户能够实现他们想要的结果。<br/>服务还通过转移成本和风险来创造价值。客户不需要投资和管理服务交付所需的所有资源和能力，服务提供者承担了这些责任。这使客户能够专注于其核心业务活动。例如，使用云服务的客户不需要管理数据中心、服务器硬件、操作系统补丁等，这些成本和风险由服务提供者承担。<br/>选项B"功效"（warranty，或称保修）只是服务价值的一个组成部分。功效本身不能促成客户的结果，它必须与效用结合，通过完整的服务来交付，才能实现客户的成果。功效确保服务可靠、可用、安全，但这只是实现成果的必要条件，不是充分条件。<br/>选项C"组织"是提供服务的实体。虽然组织通过提供服务来帮助客户实现成果，但组织本身不是促成成果的直接手段。是组织提供的服务促成了成果，而不是组织本身。<br/>选项D"IT"（信息技术）是支持服务交付的技术手段。IT技术是服务的组成部分或支撑要素，但IT本身不能直接促成客户的业务成果。只有当IT以服务的形式被设计和交付，与业务需求和成果对齐时，它才能创造价值。客户关心的是业务成果，而不是技术本身。<br/>这个以成果为导向的服务定义对服务管理实践有重要影响。它意味着服务设计应该从理解客户想要的成果开始，而不是从技术能力开始。服务级别管理应该关注业务成果指标，而不仅仅是技术运维指标。持续改进应该聚焦于如何更好地促成客户成果，而不仅仅是优化内部流程。<br/>在与客户沟通时，服务提供者应该强调服务如何帮助客户实现其业务目标，而不是仅仅描述技术特性。这种以成果为导向的方法有助于建立更强的客户关系，确保服务与业务需求保持一致。<br/><strong>参考官方ITIL 4 Foundation著作的 2.3 小节</strong></p><p><strong>原始题号：361</strong><br/><strong>第7题</strong><br/>服务组合由哪三个要素构成？<br/>A. 客户组合、服务目录和退役服务<br/>B. 客户组合、配置管理系统和服务目录<br/>C. 服务管道、服务目录和已停用的服务<br/>D. 服务管道、配置管理系统和服务目录<br/><strong>答案：C</strong><br/><strong>解题分析：</strong><br/>本题考查的是服务组合（service portfolio）的构成要素。服务组合是组织管理其所有服务的重要工具，提供了服务全生命周期的可见性。<br/>服务组合由三个主要要素构成：服务管道（service pipeline）、服务目录（service catalogue）和已停用的服务（retired services）。这三个要素涵盖了服务从构思到退役的整个生命周期。<br/>服务管道包含正在开发或考虑中的服务——那些尚未投入生产但计划在未来提供的服务。这些可能是全新的服务，也可能是现有服务的重大变更。服务管道帮助组织进行战略规划和资源分配，确保服务开发与业务战略保持一致。它也为投资决策提供基础，帮助优先考虑服务开发工作。<br/>服务目录包含当前正在运营并向客户提供的所有实时服务（live services）。这是服务组合中最活跃的部分，代表了组织当前的服务能力。服务目录通常分为面向客户的服务目录（展示客户可见的服务及其描述）和技术服务目录（包括支持客户服务的技术服务和组件的详细信息）。服务目录管理实践负责维护这部分信息的准确性和时效性。<br/>已停用的服务是那些已经从运营中撤出、不再提供给客户的服务。保留这些服务的记录很重要，原因包括：历史参考、合规性要求、知识管理（了解为什么服务被停用，避免重复错误）、以及可能的未来重启（在某些情况下，已停用的服务可能需要恢复）。<br/>这三个要素共同提供了组织所有服务的完整视图，支持有效的服务组合管理。组织可以评估其整体服务能力，识别重复或缺失的服务，平衡资源在开发、运营和退役之间的分配，以及做出明智的投资决策。<br/>选项A包含"客户组合"，这不是服务组合的标准组成部分。虽然了解客户很重要，但客户信息通常不作为服务组合的一个单独要素来管理。客户与服务的关系通常通过服务级别协议和服务目录来体现。<br/>选项B和D都包含"配置管理系统"（CMS）。虽然CMS与服务管理密切相关，包含有关配置项和服务组件的详细技术信息，但它不是服务组合的组成要素。CMS支持服务组合管理，但它们是不同的管理工具，服务于不同的目的。<br/>服务组合管理是一个持续的活动，需要定期审查和更新。服务在其生命周期中会从管道移动到目录，最终可能移动到已停用服务类别。每个转换都应该有明确的标准和批准流程，确保服务组合反映组织的战略方向和当前运营状态。<br/>有效的服务组合管理帮助组织优化服务投资，确保资源用于创造最大价值的服务，及时停用不再有价值的服务，并为新服务的引入做好准备。它也提供了与业务利益相关者讨论服务战略的基础。<br/><strong>参考官方ITIL 4 Foundation著作的 5.2.10 小节</strong></p>]]></description></item><item>    <title><![CDATA[芯片实现路线图 星星上的柳树 ]]></title>    <link>https://segmentfault.com/a/1190000047381929</link>    <guid>https://segmentfault.com/a/1190000047381929</guid>    <pubDate>2025-11-09 11:02:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在集成电路（IC）设计中，“物理实现”是将抽象的逻辑设计落地为可生产布局（Layout）的关键阶段，其中包含floor-planning（布局规划）、placement（布局布置）、routing（布线）与physical verification（物理验证），共同实现性能、功耗、面积（PPA）的最优平衡。</p><p>下图为典型的物理设计流程图，从系统规格、功能逻辑设计，一直到最终布局和验证阶段，一目了然地展现了每一步的重要作用及衔接关系。此图生动演示流程的清晰结构，便于快速理解物理实现的全景。<br/><img width="589" height="595" referrerpolicy="no-referrer" src="/img/bVdmYmK" alt="" title=""/><br/>为什么“物理实现”至关重要？<br/>1、它决定最终芯片的 性能表现、功耗控制与面积优化——良好的物理实现是高效芯片设计的基石。<br/>2、精确的 floor-planning 和 placement 能显著缩短信号延迟、降低布线复杂度，提升整体设计效率。<br/>3、routing 及后续的物理验证（如 DRC、LVS 等）确保设计可制造、功能正确。</p><p>以上要点如维基百科所述：物理设计需把电路转换成几何布局，并经过设计规则检查、布局与原理图比对、寄生参数提取等验证流程，最终生成可流片的 GDSII 文件 。</p><p>提升实战技能，就选 EDA Academy<br/>如果你正打算深入掌握 IC 物理实现的实用技巧，推荐了解 EDA Academy（www.eda-academy.com）——一个专注于 IC 设计的专业在线教育平台：<br/>内容全面：不仅涵盖 floor-planning、placement、routing、DRC/LVS、寄生提取等核心环节，还包含最新技术、先进案例，帮助你由浅入深。<br/>双向灵活：即便你想成为分享经验的导师，也能在平台申请开课；同样适用于希望系统学习的学员。<br/>便捷订阅：输入邮箱即可免费订阅其 newsletter，快速获取行业趋势、课程更新与实战干货，无需付费即可第一时间关注前沿内容。<br/>推广变现：加入销售联盟计划，通过推荐课程赚取 20%–50% 佣金，学习之余还能收获收益，实现知识价值的延展。</p><p>物理实现不仅是技术步骤，更是芯片性能与可生产性之间的艺术平衡。掌握它，等于掌握了芯片落地的核心能力。借助 EDA Academy 的系统课程与平台生态，无论你是 IC 设计新手，还是资深开发者，都能找到快速提升的路径。欢迎访问www.eda-academy.com，开启你的 IC 设计实战之旅！</p>]]></description></item><item>    <title><![CDATA[詹姆士·沃森：DNA的探索发现之旅 好文]]></title>    <link>https://segmentfault.com/a/1190000047382106</link>    <guid>https://segmentfault.com/a/1190000047382106</guid>    <pubDate>2025-11-09 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本内容是对詹姆士·沃森 2005年在TED <a href="https://link.segmentfault.com/?enc=B2k0rvO2cdyt02KSL2k1KQ%3D%3D.cUkS6hZqkpkh4aXWiCrtDiQPJ2j2k6LeDaHda8sVeb0PkERBhTxbB06phCmdzWbk" rel="nofollow" title="DNA的探索之旅" target="_blank">DNA的探索之旅</a><br/>内容的翻译与整理。推荐点击链接观看原视频。</p><p><br/></p><p><br/></p><p>好的,我们曾以为会有讲台,所以我有点紧张。(笑声)Chris让我再讲一次我们是如何发现DNA结构的。既然我听从他的命令,我就照做。但这让我有点无聊。(台下观众 笑声)</p><p>你知道,我写了一本书。所以我会说点什么——(笑声)——我会简单说说这个发现是如何做出的,以及为什么Francis和我找到了它。然后,我希望至少有五分钟来谈谈现在是什么驱使着我。</p><p>在我身后是我17岁时的照片。我当时在芝加哥大学读三年级,之所以读三年级是因为芝加哥大学允许你在高中两年后入学。所以——离开高中很有趣——(笑声)——因为我个子很小,不擅长体育或任何类似的东西。</p><p>但我应该说说我的背景——我父亲被培养成圣公会教徒和共和党人,但在大学一年级后,他成了无神论者和民主党人。(笑声)我母亲是爱尔兰天主教徒,但她对宗教不太认真。到11岁时,我不再去做周日弥撒,而是和父亲一起去观鸟。所以很早我就听说了查尔斯·达尔文。我想,你知道,他是伟大的英雄。而且,你通过进化来理解现存的生命。</p><p>在芝加哥大学,我主修动物学,以为如果我足够聪明,也许能从康奈尔大学获得鸟类学博士学位。然后,在芝加哥报纸上,有一篇关于伟大物理学家薛定谔写的《生命是什么?》这本书的书评。当然,这是我想知道的问题。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382108" alt="" title=""/></p><p>你知道,达尔文解释了生命开始后的演变,但生命的本质是什么?薛定谔说本质是存在于我们染色体中的信息,它必须存在于一个分子上。我以前从未真正思考过分子。你知道染色体,但这是一个分子,不知何故所有信息可能以某种数字形式存在。最大的问题是,你如何复制这些信息?这就是那本书的内容。所以,从那一刻起,我想成为一名遗传学家——理解基因,并通过它理解生命。</p><p>所以我有一个远方的英雄。不是棒球运动员;而是莱纳斯·鲍林。所以我申请了加州理工学院,他们拒绝了我。(笑声)所以我去了印第安纳,在遗传学方面实际上和加州理工一样好,而且,他们有一支很棒的篮球队。(笑声)所以我在印第安纳过得很开心。正是在印第安纳,我得到的印象是,你知道,基因很可能是DNA。所以当我拿到博士学位后,我应该去寻找DNA。</p><p>所以我首先去了哥本哈根,因为我想,也许我可以成为一名生物化学家,但我发现生物化学非常无聊。它没有朝着说明基因是什么的方向发展;它只是核科学。哦,那就是那本书,小册子。你大约两小时就能读完。但后来我去意大利参加了一个会议。</p><p>有一位意外的演讲者不在节目单上,他谈到了DNA。这就是莫里斯·威尔金斯。他受过物理学训练,战后他想做生物物理学,他选择了DNA,因为洛克菲勒研究所已经确定DNA可能是染色体上的遗传分子。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382109" alt="" title="" loading="lazy"/></p><p>大多数人认为是蛋白质。但威尔金斯认为DNA是最佳选择,他展示了这张X射线照片。有点像晶体。所以DNA有一个结构,尽管它可能归因于携带不同指令集的不同分子。</p><p>所以DNA分子有一些普遍性。所以我想和他一起工作,但他不想要一个前观鸟者,我最后到了英格兰剑桥。所以我去了剑桥,因为当时那里真的是世界上X射线晶体学最好的地方。X射线晶体学现在是化学系的一门学科。我是说,在那些日子里,它是物理学家的领域。所以X射线晶体学最好的地方是剑桥的卡文迪许实验室。在那里我遇到了弗朗西斯·克里克。我去的时候不认识他。他35岁。我23岁。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382110" alt="" title="" loading="lazy"/></p><p>在一天之内,我们决定也许可以走捷径找到DNA的结构。不是用严格的方式解决它,而是建立一个模型,一个电子模型,使用X射线照片的一些坐标,你知道,长度,所有那些东西。但只是问分子——它应该如何折叠?</p><p>处在这张照片中心位置的人, 是莱纳斯·鲍林。大约六个月前,他提出了蛋白质的α螺旋结构。在这样做时,他让右边的那个人,劳伦斯·布拉格爵士,卡文迪许教授,很难堪。这是几年后的照片,布拉格有理由微笑。当我到那里时,他肯定没有笑,因为鲍林得到了α螺旋,而剑桥人失败了,这让他有些羞愧,因为他们不是化学家。当然,克里克和我都不是化学家,所以我们试图建立一个模型。他知道,弗朗西斯认识威尔金斯。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382111" alt="" title="" loading="lazy"/></p><p>所以威尔金斯说他认为它是螺旋。X射线图,他认为与螺旋相符。所以我们建立了一个三链模型。伦敦的人来了。威尔金斯和这位合作者,或可能的合作者,罗莎琳德·富兰克林,来了并嘲笑我们的模型。</p><p>他们说它很糟糕,确实如此。所以我们被告知不要再建模型了;我们不称职。(笑声)所以我们没有建任何模型,弗朗西斯继续研究蛋白质。基本上,我什么都没做。除了读书。</p><p>你知道,基本上,阅读是件好事;你会获得事实。我们一直告诉伦敦的人,莱纳斯·鲍林将要转向DNA。如果DNA那么重要,莱纳斯会知道的。他会建立一个模型,然后我们就会被抢先。事实上,他给伦敦的人写信:他能看他们的X射线照片吗?他们明智地说"不"。所以他没有得到它。但文献中有一些。实际上,莱纳斯没有仔细看它们。</p><p>但是,在我到剑桥大约15个月后,开始出现来自莱纳斯·鲍林儿子的传言,他在剑桥,说他父亲现在正在研究DNA。所以,有一天彼得进来说他是彼得·鲍林,他给了我一份他父亲的手稿副本。天哪,我吓坏了,因为我想,你知道,我们可能会被抢先。我无所事事,没有任何资格。(笑声)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382112" alt="" title="" loading="lazy"/></p><p>所以就是那篇论文,他提出了一个三链结构。我读了它,它就是——是垃圾。(笑声)所以这来自世界上——(笑声)——是出乎意料的。它是由磷酸基团之间的氢键结合在一起的。</p><p>好吧,如果细胞的峰值pH值在7左右,那些氢键就不可能存在。我们冲到化学系说:"鲍林可能是对的吗?"亚历克斯·赫斯特说:"不对。"所以我们很高兴。(笑声)</p><p>而且,你知道,我们仍在比赛中,但我们害怕加州理工的某个人会告诉莱纳斯他错了。所以布拉格说:"建模型。"在我们拿到鲍林手稿一个月后——我应该说我把手稿带到伦敦,给人们看。好吧,我说,莱纳斯错了,我们仍在比赛中,他们应该立即开始建模型。</p><p>但威尔金斯说"不"。罗莎琳德·富兰克林大约两个月后要离开,在她离开后他会开始建模型。所以我带着这个消息回到剑桥,布拉格说:"建模型。"好吧,当然,我想建模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382113" alt="" title="" loading="lazy"/></p><p>这是罗莎琳德的照片。她真的,你知道,从某种意义上说她是化学家,但实际上她接受的训练——她不懂任何有机化学或量子化学。她是晶体学家。我认为她不想建模型的部分原因是,她不是化学家,而鲍林是化学家。</p><p>所以克里克和我开始建模型,我学了一点化学,但不够。好吧,我们在1953年2月28日得到了答案。这是因为一条规则,对我来说,这是一条非常好的规则:永远不要成为房间里最聪明的人,我们不是。</p><p>我们不是房间里最好的化学家。我进去展示了我做的配对,杰里·多诺霍——他是化学家——他说,这是错的。你把氢原子放在了错误的位置。我只是把它们放在像书中那样的位置。他说它们是错的。</p><p>所以第二天,你知道,在我想"好吧,他可能是对的"之后。所以我改变了位置,然后我们找到了碱基配对,弗朗西斯立即说链条以绝对方向运行。我们知道我们是对的。所以这是相当,你知道,这一切都发生在大约两个小时内。从无到有。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382114" alt="" title="" loading="lazy"/></p><p>我们知道这很重要,因为,你知道,如果你只是把A放在T旁边,G放在C旁边,你就有了一个复制机制。所以我们看到了遗传信息是如何携带的。这是四个碱基的顺序。所以从某种意义上说,这是一种数字型信息。你通过链分离来复制它。</p><p>所以,你知道,如果它不是这样工作的,你不妨相信它,因为你没有任何其他方案。(笑声)但大多数科学家不是这样思考的。大多数科学家真的相当迟钝。他们说,在我们知道它是对的之前,我们不会考虑它。</p><p>但是,你知道,我们想,好吧,它至少95%正确或99%正确。所以考虑一下。接下来的五年里,《自然》杂志上基本上只有大约五次引用我们的工作——没有。所以我们被单独留下,试图完成三部曲的最后一部分:你如何——这种遗传信息做什么?</p><p>很明显,它为RNA分子提供信息,然后你如何从RNA到蛋白质?大约三年我们只是——我试图解决RNA的结构。它没有结果。它没有给出好的X射线照片。我非常不开心;(同时)我爱的女孩又没有嫁给我。</p><p>这真的是,你知道,是一段糟糕的时光。(笑声)所以这是我遇到那个女孩之前我和弗朗西斯的照片,所以我看起来还是很开心的。(笑声)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382115" alt="" title="" loading="lazy"/></p><p>但这是我们不知道如何前进时所做的:我们成立了一个俱乐部,称之为RNA领带俱乐部。乔治·伽莫夫,也是一位伟大的物理学家,他设计了领带。他是成员之一。问题是:你如何从四字母代码到蛋白质的20字母代码?费曼是成员,泰勒也是,还有伽莫夫的朋友。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382116" alt="" title="" loading="lazy"/></p><p>但那是唯一的——不,我们只拍了两次照片。在两个场合,你知道,我们中的一个没有戴领带。弗朗西斯在右上方,亚历克斯·里奇——从医学博士转为晶体学家——在我旁边。这是1955年9月在剑桥拍的。我在微笑,我想是强颜欢笑,因为我爱的那个女孩,天哪,她走了。(笑声)</p><p>所以直到1960年我才真正快乐起来,因为那时我们发现,基本上,你知道,RNA有三种形式。我们基本上知道,DNA为RNA提供信息。</p><p>RNA为蛋白质提供信息。这让马歇尔·尼伦伯格,你知道,拿RNA——合成RNA——把它放在制造蛋白质的系统中。他制造了聚苯丙氨酸,聚苯丙氨酸。所以这是遗传密码的第一次破解,到1966年就全部结束了。所以,这就是Chris想让我做的,它是——</p><p>那么从那以后发生了什么?好吧,那时候——我应该回去。当我们发现DNA结构时,我在冷泉港做了第一次演讲。物理学家利奥·西拉德看着我说:"你要为此申请专利吗?"但他懂专利法,我们不能为它申请专利,因为你不能。它没有用途。(笑声)</p><p>所以DNA没有成为有用的分子,律师直到1973年才进入等式,20年后,当旧金山和斯坦福的博耶和科恩提出他们的重组DNA方法时,斯坦福为它申请了专利并赚了很多钱。至少他们为某些东西申请了专利,你知道,可以做有用的事情。</p><p>然后,他们学会了如何读取代码的字母。然后,砰,我们有了生物技术产业。但我们仍然离回答一个在我童年时期占主导地位的问题很远,那就是:你如何理解先天与后天?</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382117" alt="" title="" loading="lazy"/></p><p>所以我继续。我已经超时了,但这是迈克尔·维格勒,一位非常非常聪明的数学家转物理学家。他开发了一种技术,基本上可以让我们查看样本DNA,最终沿着它有一百万个点。那里有一个芯片,一个常规的。然后有一个由麦迪逊的一家叫NimbleGen的公司通过光刻技术制造的,它远远领先于Affymetrix。我们使用他们的技术。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382118" alt="" title="" loading="lazy"/></p><p>你可以做的是比较正常组织与癌症的DNA。你可以在顶部看到,不好的癌症显示插入或缺失。所以DNA真的被严重破坏了,而如果你有生存的机会,DNA就没有那么乱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382119" alt="" title="" loading="lazy"/></p><p>所以我们认为这最终将导致我们所谓的"DNA活检"。在你接受癌症治疗之前,你真的应该看看这项技术,并了解敌人的面貌。这不是——这只是部分观察,但它是——我认为它将非常非常有用。所以,我们从乳腺癌开始,因为它有很多钱,没有政府资金。现在我有某种既得利益:我想为前列腺癌做这件事。</p><p>所以,你知道,如果不危险,你就不会被治疗。但维格勒,除了观察癌细胞,还观察正常细胞,并做出了一个真正令人惊讶的观察。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382120" alt="" title="" loading="lazy"/></p><p>那就是,我们所有人在基因组中大约有10个地方失去了一个基因或获得了另一个基因。所以我们都有点不完美。问题是,如果我们在这里,你知道,这些小损失或增益可能不会太糟糕。但如果这些缺失或扩增发生在错误的基因中,也许我们会感到不舒服。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047382121" alt="" title="" loading="lazy"/></p><p>所以他研究的第一种疾病是自闭症。我们研究自闭症的原因是我们有钱这样做。研究一个个体大约需要3000美元。一个患有阿斯伯格综合症(高智商自闭症)孩子的父母,把他的东西送到一家常规公司;他们没有做。</p><p>传统遗传学无法做到,但只是扫描它,我们开始发现自闭症基因。你可以在这里看到,有很多。所以很多自闭症孩子是自闭症,因为他们只是失去了一大块DNA。我是说,在分子水平上的大块。</p><p>我们看到一个自闭症孩子,大约500万个碱基从他的一条染色体上缺失了。我们还没有研究父母,但父母可能没有那种损失,否则他们就不会是父母了。现在,所以,我们的自闭症研究才刚刚开始。我们得到了300万美元。我认为在你能够帮助有自闭症孩子的父母或认为他们可能有自闭症孩子的父母之前,至少要花费10到2000万美元,我们能发现差异吗?</p><p>所以这同样的技术可能应该研究所有情况。这是找到基因的绝妙方法。所以,我将总结说,我们研究了20个精神分裂症患者。我们认为我们可能必须研究几百人才能得到全貌。</p><p>但正如你所看到的,20人中有7人有非常高的变化。然而,在对照组中有三个。那么对照组的意义是什么?</p><p>他们也疯了吗,而我们不知道?或者,你知道,他们是正常的?我猜他们是正常的。我们在精神分裂症中认为的是有易感基因,这是否是易感基因之一——然后只有一部分人群有能力成为精神分裂症患者。</p><p>现在,我们真的没有任何证据,但我想,给你一个假设,最好的猜测是,如果你是左撇子,你容易患精神分裂症。30%的精神分裂症患者是左撇子,精神分裂症有非常有趣的遗传学,这意味着60%的人在基因上是左撇子,但只有一半表现出来。我没有时间说了。</p><p>现在,一些认为自己是右撇子的人在基因上是左撇子。好的。我只是说,如果你想,哦,我没有携带左撇子基因,所以我的孩子不会有患精神分裂症的风险。你可能有。好的?(笑声)</p><p>所以对我来说,这是一个非常激动人心的时刻。我们应该能够找到双相情感障碍的基因;有一种关系。如果我有足够的钱,我们今年就能找到它们全部。谢谢你们。</p>]]></description></item><item>    <title><![CDATA[WampServer安装教程（图文步骤）]]></title>    <link>https://segmentfault.com/a/1190000047381866</link>    <guid>https://segmentfault.com/a/1190000047381866</guid>    <pubDate>2025-11-09 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​<strong>WampServer</strong>是一个 <strong>Windows 系统下的免费集成环境</strong>，主要用于 <strong>本地搭建网站</strong>，特别适合 <strong>PHP + MySQL 开发</strong>（比如 WordPress、网站后台等）。</p><h3><strong>第一步：下载</strong></h3><ol><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=0iQoVo70Bj%2Fpxq0VEkXOQA%3D%3D.ytt%2BdWFia4FncOsnWZN3oTQNC6FX5A8ogjsfOqQsxQ7PbZEaWxVSHqc89QA5CBg0" rel="nofollow" title="https://pan.quark.cn/s/d8a6af2cd070" target="_blank">https://pan.quark.cn/s/d8a6af2cd070</a></li><li>找到适合你电脑的版本（看你的系统是 <strong>32位还是64位</strong>，一般选 <strong>64位</strong>的，除非你确定是老电脑）。</li><li>点下载按钮，等文件（<code>wampserver.exe</code>或类似名字）下好。</li></ol><h3><strong>第二步：安装前准备</strong></h3><ol><li><strong>关闭杀毒软件</strong>（比如 360、腾讯电脑管家），有时候它们会误拦安装。</li><li><p><strong>确保电脑已安装以下软件</strong>（如果没有要先装）：</p><ul><li><strong>MySQL</strong>（WampServer 一般自带，但最好确认下）。</li><li><strong>PHP</strong>（通常也自带）。</li><li><strong>Apache</strong>（也是自带的，不用单独装）。</li><li>如果没有，WampServer 安装时会帮你装，但最好提前装好 <strong>Visual C++ 运行库</strong>（去微软官网搜“Visual C++ Redistributable”全装上，避免报错）。</li></ul></li></ol><h3><strong>第三步：开始安装</strong></h3><ol><li>双击下载好的 <code>wampserver.exe</code>文件，弹出安装窗口后点 <strong>Next（下一步）</strong> 。</li><li>同意协议（勾选“I accept the agreement”），再点 <strong>Next</strong>。</li><li><p>选择安装路径（默认一般是 <code>C:\wamp64</code>，<strong>不建议改路径</strong>，除非你懂，因为改了可能要额外配置）。</p><ul><li>路径里<strong>不要有中文或空格</strong>（比如别放 <code>D:\我的软件\WampServer</code>，容易出错）。</li></ul></li><li>点 <strong>Next</strong>。</li><li>选择开始菜单文件夹（直接点 Next 就行，用默认的）。</li><li>确认信息，点 <strong>Install（安装）</strong> 。</li></ol><h3><strong>第四步：安装过程</strong></h3><ol><li>安装时会弹出窗口让你选 <strong>默认浏览器</strong>（比如 Chrome、Edge），选你常用的，点 <strong>Open</strong>。</li><li>接着会让你选 <strong>默认文本编辑器</strong>（比如记事本、Notepad++），选一个，点 <strong>Open</strong>。</li><li>等它自己安装（进度条走完），中间可能会停顿，别关窗口。</li></ol><h3><strong>第五步：安装完成</strong></h3><ol><li>安装完后会问你是否要 <strong>安装 WordPress</strong>（一个网站程序），<strong>不需要就直接取消勾选，点 Next</strong>。</li><li>最后点 <strong>Finish（完成）</strong> 。</li></ol><h3><strong>第六步：启动 WampServer</strong></h3><ol><li>安装好后，桌面上会出现 <strong>WampServer 图标</strong>（一个 <strong>W 图标</strong>），双击打开。</li><li><p>第一次启动时，图标在任务栏（右下角）会先显示 <strong>红色</strong>（表示正在启动服务），然后变 <strong>橙色</strong>（部分服务OK），最后变 <strong>绿色</strong>（全部正常）。</p><ul><li>如果一直是 <strong>红色或橙色</strong>，可能是端口被占用（比如 Apache 的 80 端口被其他程序占了），需要改端口（后面可以教你）。</li></ul></li><li>等图标变 <strong>绿色</strong>后，打开浏览器，输入 <strong><a href="https://link.segmentfault.com/?enc=i%2FIH74ARNLU1tsCYZf5tkg%3D%3D.tCDWUGb2vuCoYLczywBjpr2vhmaCmQM5mAaS9ddMmAQ%3D" rel="nofollow" target="_blank">http://localhost/</a></strong> ，如果能打开 WampServer 的欢迎页面，说明安装成功！</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[焊接情况检测数据集（千张图片已划分）| ]]></title>    <link>https://segmentfault.com/a/1190000047381753</link>    <guid>https://segmentfault.com/a/1190000047381753</guid>    <pubDate>2025-11-09 01:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>焊接情况检测数据集（千张图片已划分）| 面向工业质检的目标检测训练集</h2><p>在现代工业制造体系中，焊接质量作为产品可靠性的重要指标之一，直接影响结构件的力学性能、安全性和使用寿命。然而传统的焊缝质量检测往往依赖人工经验式检验，不仅检测效率低，而且难以在不同作业场景中保持稳定一致的检测标准。</p><p>随着工业视觉和深度学习的发展，利用 AI 模型自动检测焊缝质量逐渐成为行业趋势。而高质量的焊接检测数据集，正是训练这类模型的核心基础。本数据集以真实工业场景为采集源，结合标准化标注体系，能够直接用于深度学习模型训练，在技术研发、算法实验、模型部署等多个环节均具有实际应用价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047381755" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>数据集获取</h3><blockquote>链接:<a href="https://link.segmentfault.com/?enc=bYNbZvrZQMrzdKwpVqP8Vg%3D%3D.ODvWTobMG6HphPpOvdFfL2YyiwQKHFFa5YlA2Bh5nKX%2B6e2NCAA6qSbK%2B0KpsTRgfk88224rolU5xHiC%2B7SDqw%3D%3D" rel="nofollow" target="_blank">https://pan.baidu.com/s/1gzAuAJ1-Qb-1s3TEGIC9Uw?pwd=gd48 </a><br/>提取码:gd48 复制这段内容后打开百度网盘手机App，操作更方便哦</blockquote><p>用于检测焊接表面缺陷的目标检测数据集，包含 3 个类别：不良焊缝、良好焊缝和缺陷。<br/>该数据集采用 YOLO 标注格式，用于目标检测任务，标签图可在 data.yaml 文件中找到。<br/>该数据集中的图像来源于各种图像集合和数据集。</p><p>千张数据</p><p>train: ../train/images<br/>val: ../valid/images<br/>test: ../test/images</p><p>nc: 3<br/>names: ['Bad Weld', 'Good Weld', 'Defect']</p><p>数据集使用说明</p><p>下载并解压数据集后，确保目录结构完整。可直接用于项目训练<br/>将数据集中的 <code>yaml</code>文件 中的路径替换为你的实际目录，即可训练模型完成检测。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047381756" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047381757" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>一、背景：为什么焊接检测需要 AI 化？</h3><h4>1. 焊接场景复杂、工艺差异大</h4><p>不同焊接方式（如手工焊、氩弧焊、激光焊、气保焊）会产生不同的焊缝纹理形态，人工识别需要长期经验积累，且受个人判断偏差影响明显。</p><h4>2. 人工检测效率有限</h4><p>在工业生产线上，焊缝长度通常以米计，大规模结构件的检测需要逐段检查，对于检验人员而言不仅<strong>劳动强度大</strong>，而且<strong>持续集中注意力难以长期保持</strong>。</p><h4>3. 错误成本高</h4><p>焊缝缺陷可能直接导致构件开裂、疲劳失效、承压不足等问题，尤其在：</p><ul><li>航空航天</li><li>轨道交通</li><li>压力容器制造</li><li>建筑钢结构</li></ul><p>等领域，一处未能及时发现的焊接问题可能带来严重安全事故。</p><p>因此，建设一套可自动识别缺陷、标准化判断质量、可实时运行在生产线上的 AI 焊接视觉检测系统，已经成为工业制造领域的重要方向。</p><p>而模型能否有效识别焊接缺陷，极大取决于其训练数据集的质量与标注标准。</p><hr/><h3>二、数据集概述</h3><p>本数据集主要面向<strong>目标检测任务</strong>，旨在实现对焊缝外观质量进行智能识别、分类与定位。数据覆盖多种焊缝材质、焊接环境、光照条件和成型表现，具有良好的多样性与泛化能力。</p><p><strong>包含的三类检测对象为：</strong></p><table><thead><tr><th>类别</th><th>名称</th><th>描述</th></tr></thead><tbody><tr><td>0</td><td>Bad Weld（不良焊缝）</td><td>焊接不均、焊瘤、焊缝边缘不平整等存在整体成型缺陷的焊缝</td></tr><tr><td>1</td><td>Good Weld（良好焊缝）</td><td>形状连续、焊道平滑、成型标准的优质焊缝</td></tr><tr><td>2</td><td>Defect（缺陷）</td><td>裂纹、气孔、烧穿、夹渣等具体可见局部缺陷</td></tr></tbody></table><p>数据标注采用 <strong>YOLO 标准格式</strong>，适用于主流目标检测框架。</p><p>数据集结构：</p><pre><code>dataset/
├── train/
│   ├── images/
│   └── labels/
├── valid/
│   ├── images/
│   └── labels/
├── test/
│   ├── images/
│   └── labels/
└── data.yaml</code></pre><p>data.yaml 示例：</p><pre><code>train: ../train/images
val: ../valid/images
test: ../test/images

nc: 3
names: ['Bad Weld', 'Good Weld', 'Defect']</code></pre><p>将路径修改为本地实际路径即可开始训练。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047381758" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047381759" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>三、数据集详情与特点</h3><h4>1. 图像来源真实多样</h4><p>图像选取自不同实际焊接工程现场，包含：</p><ul><li>不同金属材质（钢、不锈钢、铝材等）</li><li>多种焊接工艺（TIG、MIG、SMAW、激光焊）</li><li>各类工业应用场景（手持焊、自动焊、机器人焊）</li></ul><p>可以有效避免模型因单一环境训练而出现的泛化不足问题。</p><h4>2. 标注标准清晰、边界严格</h4><p>每张图片均经过人工审核与框选，确保缺陷位置及焊缝形态描述准确。</p><h4>3. 可直接用于产线检测模型训练</h4><p>无需额外格式转换或数据清洗。</p><hr/><h3>四、适用场景</h3><table><thead><tr><th>场景</th><th>使用方式</th><th>效果优势</th></tr></thead><tbody><tr><td>机器人焊接自检</td><td>制造设备训练嵌入模型</td><td>实时发现焊接质量问题</td></tr><tr><td>质检流水线视觉检测系统</td><td>部署高精度推理模型</td><td>提升检测速度与一致性</td></tr><tr><td>AI 教研与实践训练集</td><td>用于课程/竞赛/模型调优</td><td>快速上手目标检测任务</td></tr><tr><td>自动巡检机器人</td><td>搭载模型执行巡检识别</td><td>可用于户外或大型结构维护</td></tr></tbody></table><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047381760" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>五、模型训练示例（以 YOLO 为例）</h3><p>训练指令：</p><pre><code class="bash">yolo detect train data=data.yaml model=yolov8s.pt epochs=120 imgsz=640</code></pre><p>推理检测：</p><pre><code class="bash">yolo detect predict model=runs/detect/train/weights/best.pt source=test/images</code></pre><p>可视化结果将自动保存在 <code>runs/detect/predict/</code> 目录。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047381761" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>六、结语</h3><p>焊接质量检测的复杂性来源于工艺、材料、环境、形态等多重因素。将工业视觉与深度学习引入质量控制流程，不仅能够显著降低人工成本与人为误差，还可以推动制造业向更高自动化、更可控的质量管理体系迈进。</p><p>本数据集旨在为开发者、研究人员和企业提供一套<strong>可靠、可复现、可落地</strong>的焊接情况检测数据资源，助力高稳定性焊缝检测模型的构建与工业级 AI 质检系统的落地。</p><p>总结来看，本次分享的焊接情况检测数据集为工业智能化提供了坚实的基础。数据集涵盖了“良好焊缝”、“不良焊缝”和“缺陷”三大类别，采用了标准的 YOLO 标注格式，保证了在目标检测任务中能够高效、准确地训练模型。通过合理划分训练集、验证集和测试集，开发者可以充分利用数据进行模型优化与验证，从而在实际工业生产环境中实现对焊接表面缺陷的自动检测与监控。</p><p>该数据集不仅为学术研究提供了丰富的数据支持，也为企业在智能制造、质量控制以及生产线自动化等场景中落地 AI 技术提供了可靠工具。结合现代深度学习目标检测算法，开发者可以快速构建高精度焊缝检测系统，提升生产效率和产品质量，进一步推动焊接工艺的数字化与智能化发展。总之，这份数据集是工业 AI 应用中不可或缺的资源，为焊接检测技术的研究与实践提供了坚实的数据基础。</p>]]></description></item><item>    <title><![CDATA[Spring AI，一个让Spring应]]></title>    <link>https://segmentfault.com/a/1190000047381559</link>    <guid>https://segmentfault.com/a/1190000047381559</guid>    <pubDate>2025-11-09 00:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>文 / 勇哥<br/>原创文章，转载请联系授权</blockquote><p>在前一篇文章中，我们探讨了《<a href="https://link.segmentfault.com/?enc=OuvCipJ5Uy76rR9h19tcWA%3D%3D.10WB3SJD60%2F%2FfyYXyGZD59%2BAWXcit9S9zDJBYk94oAqLY1uCQY7pKnDJrw1AV%2B0YUQexTui9MmqOzD%2BVLg%2FK2g%3D%3D" rel="nofollow" target="_blank">Java程序员该如何快速上手LLM应用开发呢?</a>》。今天，让我们聚焦Spring AI——这个被称为"Spring开发者的AI赋能工具包"的框架，它为Java开发者打开了一扇通往AI世界的便捷之门。</p><p>作为一名在Java领域摸爬滚打快20年的"老码农"，我见过太多团队在集成AI能力时遇到的痛点：开发语言不一致、重复造轮子、供应商锁定、复杂的配置管理、陡峭的学习曲线...Spring AI的出现，就像给Java开发者提供了一套"AI集成的标准接口"，让AI功能的引入变得简单、统一、可扩展。</p><p><strong>核心观点：Spring AI是Spring开发者做AI集成的"瑞士军刀"，它通过统一的API抽象和自动配置，让Spring应用能够轻松集成各类AI模型和服务，无需关心底层实现细节。</strong></p><h2>一、Spring AI：为什么它是Spring开发者的AI桥梁？</h2><p>想象一下，你是一家使用Spring技术栈的企业技术负责人，现在需要在现有系统中集成AI能力：</p><p>开发团队熟悉Spring Boot、Spring Cloud的开发模式，希望保持一致的编程体验；架构师担心引入多个AI供应商会导致技术栈碎片化；运维团队关心配置管理和系统稳定性——大家都在为同一个目标努力，但面临的技术挑战各不相同。</p><p>Spring AI就像一座精心设计的"桥梁"，它提供了：</p><ul><li><strong>统一的API抽象</strong>：用一致的接口访问不同的AI服务，屏蔽底层差异；</li><li><strong>Spring风格的集成</strong>：充分利用自动配置、依赖注入等Spring特性；</li><li><strong>丰富的模型支持</strong>：从大语言模型到嵌入模型，从图像生成到语音处理；</li><li><strong>企业级的可靠性</strong>：支持安全配置、错误处理、可观测性等企业级特性。</li></ul><p>一句话，Spring AI让AI集成变得"Spring化"，是Java开发者拥抱AI时代的最佳选择之一。</p><h2>二、Spring AI的核心架构：5大核心概念的"AI工具箱"</h2><p>Spring AI围绕几个核心概念构建，这些概念构成了它的基础架构：</p><h3>2.1 模型抽象 (Model)：AI能力的统一入口</h3><p>一句话概括：模型抽象是Spring AI的核心，它定义了与不同类型AI模型交互的统一方式。</p><p><strong>核心类型：</strong></p><ul><li><strong>语言模型 (Language Model)</strong>：处理文本理解和生成，是大语言模型的抽象；</li><li><strong>嵌入模型 (Embedding Model)</strong>：将文本转换为向量表示，是语义搜索的基础；</li><li><strong>图像模型 (Image Model)</strong>：处理图像生成和分析，支持多模态应用；</li><li><strong>语音模型 (Speech Model)</strong>：处理语音识别和合成，构建语音交互应用。</li></ul><p><strong>实战要点：</strong></p><ul><li>优先使用接口而非具体实现，保持代码的灵活性；</li><li>合理选择模型类型，根据具体业务场景匹配最适合的AI能力。</li></ul><p>适用场景：各种需要AI能力的Spring应用，特别是需要灵活切换AI供应商的场景。</p><h3>2.2 提示模板 (Prompt Template)：提示工程的Spring实现</h3><p>一句话概括：提示模板让提示工程变得结构化、可重用，是构建高质量AI交互的基础。</p><p><strong>核心能力：</strong></p><ul><li>定义标准化的提示格式；</li><li>动态替换提示中的变量；</li><li>构建上下文相关的提示序列；</li><li>支持模板复用和版本管理。</li></ul><p><strong>实战要点：</strong></p><ul><li>将复杂提示抽象为模板，提高可维护性；</li><li>设计参数化的模板，增强灵活性；</li><li>为不同业务场景创建专用模板库。</li></ul><p>适用场景：需要标准化AI交互、批量处理不同内容的应用。</p><h3>2.3 聊天客户端 (Chat Client)：对话式AI的简化接口</h3><p>一句话概括：聊天客户端封装了与聊天模型交互的复杂性，让构建对话应用变得简单。</p><p><strong>核心功能：</strong></p><ul><li>提供简单的消息发送和接收接口；</li><li>管理对话上下文和历史记录；</li><li>处理模型参数和配置；</li><li>支持同步和异步调用方式。</li></ul><p><strong>实战要点：</strong></p><ul><li>使用依赖注入获取聊天客户端实例；</li><li>合理管理对话历史，避免上下文过长；</li><li>根据需要调整温度参数，平衡创造性和准确性。</li></ul><p>适用场景：智能客服、聊天机器人、交互式AI助手等应用。</p><h3>2.4 向量存储 (Vector Store)：语义搜索的基础设施</h3><p>一句话概括：向量存储是实现检索增强生成(RAG)的关键组件，为AI应用提供外部知识。</p><p><strong>核心特性：</strong></p><ul><li>存储和管理文本嵌入向量；</li><li>提供高效的相似性搜索功能；</li><li>支持元数据过滤和排序；</li><li>集成多种向量数据库后端。</li></ul><p><strong>实战要点：</strong></p><ul><li>选择合适的向量存储实现(Pinecone、Milvus等)；</li><li>优化嵌入模型和向量维度，平衡性能和准确性；</li><li>实现增量向量更新机制，保持知识库新鲜度。</li></ul><p>适用场景：基于企业知识库的问答系统、智能文档检索、个性化推荐等应用。</p><h3>2.5 检索增强生成 (RAG)：提升AI回答准确性的关键技术</h3><p>一句话概括：RAG结合了外部知识检索和AI生成能力，解决了大模型知识时效性和准确性问题。</p><p><strong>核心流程：</strong></p><ol><li>将企业文档转换为向量并存储；</li><li>根据用户查询检索相关文档片段；</li><li>将检索内容和用户问题组合为增强提示；</li><li>调用大模型生成基于检索内容的回答。</li></ol><p><strong>实战要点：</strong></p><ul><li>优化文档分块策略，平衡上下文完整性和相关性；</li><li>实现混合检索策略(关键词+语义)，提高检索准确性；</li><li>设计有效的提示模板，引导模型正确使用检索内容。</li></ul><p>适用场景：企业知识问答、技术支持系统、智能文档助手等应用。</p><h2>三、Spring AI实战：从环境准备到第一个AI应用</h2><h3>3.1 环境准备：构建AI应用的基础</h3><p><strong>核心要求：</strong></p><ul><li><strong>Java 17+</strong>：Spring AI要求Java 17或更高版本；</li><li><strong>Spring Boot 3.0+</strong>：需要与Spring Boot 3.0及以上版本兼容；</li><li><strong>Maven/Gradle</strong>：用于依赖管理和构建；</li><li><strong>AI服务API密钥</strong>：如Hunyuan、Deepseek、Doubao等服务的访问凭证。</li></ul><p><strong>实战步骤：</strong></p><pre><code class="xml">&lt;!-- 在pom.xml中添加Spring AI依赖 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.ai&lt;/groupId&gt;
    &lt;artifactId&gt;spring-ai-openai-spring-boot-starter&lt;/artifactId&gt;
    &lt;version&gt;0.8.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- 如果需要向量存储 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.ai&lt;/groupId&gt;
    &lt;artifactId&gt;spring-ai-pinecone-store-spring-boot-starter&lt;/artifactId&gt;
    &lt;version&gt;0.8.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- 基本Spring Boot依赖 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;
&lt;/dependency&gt;</code></pre><p>在<code>application.properties</code>中配置API密钥：</p><pre><code class="properties"># Deepseek配置
spring.ai.openai.api-key=your-api-key
spring.ai.openai.chat.model=gpt-3.5-turbo
spring.ai.openai.chat.temperature=0.7

# 可选：向量存储配置（使用Pinecone时需要）
# spring.ai.pinecone.api-key=your-pinecone-api-key
# spring.ai.pinecone.environment=your-pinecone-environment
# spring.ai.pinecone.index=your-index-name</code></pre><p><strong>重要说明</strong>：</p><ol><li>请确保使用有效的API密钥替换<code>your-api-key</code></li><li>Spring AI的包名已从<code>org.springframework.cloud</code>更改为<code>org.springframework.ai</code>，示例中已更新</li><li>完整运行示例需要Java 17+和Spring Boot 3.0+</li></ol><h3>3.2 Hello World：构建你的第一个AI应用</h3><p>下面是一个简单的聊天应用示例，展示了Spring AI的基本用法：</p><pre><code class="java">import org.springframework.ai.chat.client.ChatClient;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@SpringBootApplication
public class SpringCloudAiDemoApplication {

    public static void main(String[] args) {
        SpringApplication.run(SpringCloudAiDemoApplication.class, args);
    }

    @RestController
    static class ChatController {

        private final ChatClient chatClient;

        public ChatController(ChatClient chatClient) {
            this.chatClient = chatClient;
        }

        @GetMapping("/chat")
        public String chat(@RequestParam String message) {
            return chatClient.call(message);
        }
    }
}</code></pre><p><strong>启动并测试：</strong></p><ol><li>确保已在<code>application.properties</code>中配置了有效的OpenAI API密钥</li><li>运行应用（使用<code>mvn spring-boot:run</code>或通过IDE运行）</li><li>访问 <code>http://localhost:8080/chat?message=什么是Spring AI？</code></li><li>查看AI的回答</li></ol><p>这个简单的例子展示了Spring AI的核心价值——只需几行代码，就能将强大的AI能力集成到Spring应用中。</p><p><strong>代码优化说明：</strong><br/>所有代码示例都已添加必要的导入语句，并更新为使用Spring AI最新的API包结构。ModelRouter类已作为自定义实现添加，因为它可能不是Spring AI标准API的一部分。</p><h2>四、Spring AI高级特性：打造企业级AI应用</h2><h3>4.1 流式响应：提升用户体验的交互方式</h3><p>流式响应让AI生成的内容实时返回，就像人类对话一样自然，特别适合聊天界面和长文本生成场景。</p><pre><code class="java">import org.springframework.ai.chat.client.ChatClient;
import org.springframework.http.MediaType;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;
import reactor.core.publisher.Flux;
import java.io.IOException;
import java.util.concurrent.CompletableFuture;

@RestController
@RequestMapping("/stream")
public class StreamingController {
    
    private final ChatClient chatClient;
    
    public StreamingController(ChatClient chatClient) {
        this.chatClient = chatClient;
    }
    
    @GetMapping(value = "/chat", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public SseEmitter streamChat(@RequestParam String message) {
        SseEmitter emitter = new SseEmitter();
        
        // 异步处理流式响应
        CompletableFuture.runAsync(() -&gt; {
            try {
                // 获取流式响应
                chatClient.stream(message).subscribe(
                    chunk -&gt; {
                        try {
                            emitter.send(SseEmitter.event().data(chunk));
                        } catch (IOException e) {
                            emitter.completeWithError(e);
                        }
                    },
                    error -&gt; emitter.completeWithError(error),
                    () -&gt; emitter.complete()
                );
            } catch (Exception e) {
                emitter.completeWithError(e);
            }
        });
        
        return emitter;
    }
}</code></pre><p><strong>实战要点：</strong></p><ul><li>使用<code>SseEmitter</code>处理服务器发送事件；</li><li>采用异步方式处理流式响应，避免阻塞；</li><li>在前端实现流式接收和渲染逻辑。</li></ul><h3>4.2 构建RAG应用：连接企业知识库</h3><p>RAG是Spring AI最强大的应用场景之一，它让AI应用能够访问企业内部知识：</p><pre><code class="java">import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.document.Document;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.stereotype.Service;
import java.util.List;

@Service
public class RagService {
    
    private final ChatClient chatClient;
    private final VectorStore vectorStore;
    
    public RagService(ChatClient chatClient, VectorStore vectorStore) {
        this.chatClient = chatClient;
        this.vectorStore = vectorStore;
    }
    
    public String answerWithRag(String question) {
        // 搜索相关文档
        List&lt;Document&gt; relevantDocs = vectorStore.similaritySearch(question, 3);
        
        // 构建包含相关文档的提示
        StringBuilder promptBuilder = new StringBuilder();
        promptBuilder.append("根据以下信息回答问题：\n");
        
        for (Document doc : relevantDocs) {
            promptBuilder.append("- ").append(doc.getContent()).append("\n");
        }
        
        promptBuilder.append("\n问题：").append(question);
        promptBuilder.append("\n请基于提供的信息回答，不要添加额外信息。");
        
        // 获取AI响应
        return chatClient.call(promptBuilder.toString());
    }
}</code></pre><p><strong>实战要点：</strong></p><ul><li>设计高效的文档加载和处理管道；</li><li>优化提示模板，引导模型正确使用检索内容；</li><li>实现文档更新机制，保持知识的时效性。</li></ul><h3>4.3 模型路由：智能选择最佳AI模型</h3><p>模型路由允许根据不同的业务需求选择最适合的AI模型，实现资源优化和成本控制：</p><pre><code class="java">import org.springframework.ai.chat.client.ChatClient;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.stereotype.Service;
import java.util.Map;
import java.util.HashMap;

// 自定义模型路由器类
class ModelRouter {
    private final Map&lt;String, ChatClient&gt; clientMap = new HashMap&lt;&gt;();
    private ChatClient defaultModel;
    
    public void addRule(String key, ChatClient client) {
        clientMap.put(key, client);
    }
    
    public void setDefaultModel(ChatClient client) {
        this.defaultModel = client;
    }
    
    public ChatClient getClient(String type) {
        return clientMap.getOrDefault(type, defaultModel);
    }
}

@Configuration
public class ModelRoutingConfig {
    
    @Bean
    public ModelRouter modelRouter(ChatClient chatClient) {
        // 注意：在实际项目中，您可能需要配置多个不同的ChatClient实例
        ModelRouter router = new ModelRouter();
        
        // 配置路由规则 - 这里为简化示例，两个规则都使用同一个客户端
        router.addRule("simple-query", chatClient);
        router.addRule("complex-query", chatClient);
        
        // 设置默认模型
        router.setDefaultModel(chatClient);
        
        return router;
    }
}

@Service
public class RoutingService {
    
    private final ModelRouter modelRouter;
    
    public RoutingService(ModelRouter modelRouter) {
        this.modelRouter = modelRouter;
    }
    
    public String routeRequest(String type, String message) {
        // 根据请求类型选择合适的模型
        ChatClient client = modelRouter.getClient(type);
        return client.call(message);
    }
}</code></pre><p><strong>实战要点：</strong></p><ul><li>基于请求复杂度、成本、性能等因素设计路由策略；</li><li>实现A/B测试机制，持续优化模型选择；</li><li>监控不同模型的效果，动态调整路由规则。</li></ul><h2>五、Spring AI最佳实践：避免4个常见陷阱</h2><p>在实际项目中应用Spring AI时，我总结了几个最容易踩的坑和对应的解决方案：</p><h3>5.1 陷阱1：忽视安全性</h3><p><strong>表现：</strong> API密钥泄露、缺乏输入验证、生成内容未经审核。</p><p><strong>解决方法：</strong></p><ul><li>使用Spring Cloud Config或环境变量管理API密钥；</li><li>对用户输入进行严格验证和过滤；</li><li>实现内容审核机制，过滤不安全或不当内容；</li><li>使用Spring Security实现访问控制。</li></ul><h3>5.2 陷阱2：性能优化不足</h3><p><strong>表现：</strong> 响应时间过长、资源消耗过大、API调用频率过高。</p><p><strong>解决方法：</strong></p><ul><li>实现响应缓存，减少重复调用；</li><li>使用异步处理避免阻塞主线程；</li><li>批量处理多个请求，减少API调用次数；</li><li>合理选择模型大小，平衡性能和效果。</li></ul><h3>5.3 陷阱3：错误处理不当</h3><p><strong>表现：</strong> 服务不可用时应用崩溃、错误信息不友好、缺乏重试机制。</p><p><strong>解决方法：</strong></p><ul><li>实现熔断和降级机制，确保系统弹性；</li><li>添加重试逻辑，处理临时故障；</li><li>设计友好的错误提示，改善用户体验；</li><li>详细记录错误日志，便于问题排查。</li></ul><h3>5.4 陷阱4：忽视可观测性</h3><p><strong>表现：</strong> 难以监控系统运行状态、无法追踪请求路径、缺乏性能指标。</p><p><strong>解决方法：</strong></p><ul><li>集成Spring Boot Actuator，暴露关键指标；</li><li>使用Micrometer收集和记录指标数据；</li><li>集成分布式追踪系统，如Zipkin或Jaeger；</li><li>实现详细的日志记录，包括请求和响应信息。</li></ul><h2>六、Spring AI vs 其他框架：如何选择？</h2><p>在选择AI集成框架时，了解不同框架的优缺点很重要：</p><table><thead><tr><th>框架</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>Spring AI</strong></td><td>- 与Spring生态系统无缝集成- 统一的API抽象- 自动配置和依赖注入- 企业级特性支持</td><td>- 相对较新，功能可能不如其他框架丰富- 支持的AI服务提供商相对有限</td></tr><tr><td><strong>LangChain</strong></td><td>- 丰富的组件和工具链- 强大的RAG支持- 广泛的模型兼容性- 活跃的社区</td><td>- Java支持相对有限- 与Spring生态系统集成不够紧密</td></tr><tr><td><strong>Hugging Face Transformers</strong></td><td>- 最广泛的模型支持- 强大的微调能力- 活跃的研究社区- 详细的文档</td><td>- 学习曲线较陡峭- 与Spring生态系统集成需要额外工作- 资源消耗较大</td></tr><tr><td><strong>OpenAI SDK</strong></td><td>- 直接访问OpenAI最新功能- 详细的OpenAI特定文档- 快速迭代更新</td><td>- 仅限于OpenAI服务- 与Spring生态系统集成需要额外工作</td></tr></tbody></table><p><strong>选择建议：</strong></p><ul><li>如果你是Spring开发者，优先考虑Spring AI；</li><li>如果需要最广泛的模型支持，考虑Hugging Face；</li><li>如果项目非Java技术栈，LangChain可能是更好的选择；</li><li>如果只使用OpenAI服务且需要最新功能，可考虑直接使用OpenAI SDK。</li></ul><h2>七、总结与行动建议</h2><p>Spring AI为Spring开发者提供了一条通往AI世界的便捷路径，它让复杂的AI集成变得简单、统一、可扩展。在AI技术快速发展的今天，掌握Spring AI将成为Java开发者的重要竞争力。</p><p><strong>给开发者的3个行动建议：</strong></p><ol><li><strong>从小项目开始实践</strong>：选择一个简单的功能（如智能客服、内容生成），尝试用Spring AI实现，在实践中学习和掌握；</li><li><strong>深入理解核心概念</strong>：重点掌握模型抽象、提示工程、向量存储等核心概念，这些是构建复杂AI应用的基础；</li><li><strong>关注企业级应用场景</strong>：思考如何将Spring AI应用到企业实际业务中，如知识管理、智能分析、自动化决策等，创造真正的业务价值。</li></ol><p><strong>记住Spring AI的核心理念</strong>："让AI集成变得和使用Spring框架一样简单"——这也是它为什么如此受到Java开发者欢迎的原因。</p><p>可参考的资源：</p><ul><li><a href="https://link.segmentfault.com/?enc=YXbs92qFDjY7Dgv5Y5UUtg%3D%3D.i0YY62aQ%2Fv2xWyeRTzxCkuxA8IxnkX9BvTN7wyNfR6RfGn%2FgoN7J7HJXCq5gbGdXNd%2FB59ayELMTBRiHa4H7%2Bg%3D%3D" rel="nofollow" target="_blank">Spring AI官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=9MR078MdWAhjPRaVOnB7Jg%3D%3D.3ayvWBOWQ9BdeUnDgdWxDUOCbXq%2Bm3zRJK2G6etD%2BgDUqlhrp%2FCBgG4yam6s0Xw2" rel="nofollow" target="_blank">Spring AI GitHub仓库</a></li></ul><hr/><p><strong>互动话题</strong>：你在使用Spring AI时，遇到过哪些有趣的应用场景或技术挑战？欢迎在评论区分享你的经验和想法。</p><p><strong>关于作者</strong>：勇哥，10多年的开发和技术管理经验，从程序员做到企业技术高管。目前专注架构设计和人工智能应用实践，全网帐号统一名称"六边形架构"，有些不太合适发到公号的内容我会单独发到我的朋友圈，欢迎关注我，一起交流学习。</p><p><em>原创不易，如果觉得有帮助，请点赞、收藏、转发三连支持！</em></p>]]></description></item><item>    <title><![CDATA[进入新岗位的第一课——潜龙勿用 老李说技]]></title>    <link>https://segmentfault.com/a/1190000047381706</link>    <guid>https://segmentfault.com/a/1190000047381706</guid>    <pubDate>2025-11-09 00:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>我们进入一个新公司，我们要怎么做，才能真正的在这个新的平台上，站稳脚跟呢？</p><p>作为一位职场资深老鸟，我见过形形色色的新人，在刚入职的初期，出现过各种各样的问题。有些人，得意忘形，不懂藏拙，在团队内部受到同事领导的质疑和批评；有些人，用力过度，过早的暴露自己想要融入团队的需求，但是很有可能只是一股脑的冲，没得到好的结果。</p><p>所以，我们在新的公司/平台到底怎么做呢？其实答案就藏在这四个字里—潜龙勿用。</p><h2>什么叫潜龙勿用</h2><p>潜龙勿用，隐喻了事物在发展之初，虽然势头较好，但比较弱小，所以应该小心谨慎，不可轻举妄动。也就是说，在时机不成熟的时候，或者自己能力不足的时候，要暗中积蓄自己的力量。</p><p>这句话，翻译成我们职场中的话，那就是不要用力过猛。</p><p>职场的初期，首要需要做的事情，那就是观察，而不是做事。在这个阶段，你和你的同事/领导，相互之间都不熟悉，你在观察公司的同时，公司的其他人，也一定在观察着你。</p><p>如果你不经观察，直接无脑做事，很有可能无形中，动了别人的蛋糕，平白无故的招致别人的攻击。或者你过于的想要表现，暴露出来自己着急的性格，这些都是对自己不利的点。</p><p>那么，我们具体要怎么做呢？</p><h2>做好下面这几件事，帮你顺利渡过观察期</h2><p>观察期一般是刚入职的一到两个月，整体目标先是观察，观察团队有没有什么坑，值不值得我们长期效力。观察团队有没有什么长期规划，团队有没有成长空间，明确了这些问题，我们再发力也不迟。</p><h3>1、先别着急做事，先得学会识人</h3><p>我们在团队中，能不能顺利的融入，其实并不在于我们一下要用力多猛，而是先观察团队的人是怎么样的。</p><p>观察团队的领导，是什么样的做事风格，他的能力到底怎么样？他喜欢什么样的人，他不喜欢什么样的人？他的做事有没有章法。</p><p>向你的导师多请教请教，一般企业招聘新人，一定会指定一个人作为你的导师，如果没有指定，那么团队的业务主管，默认就是你的导师。这个时候，你应该多听一听，你的导师严重，团队现在处于一个什么样的情况，团队希望你承担什么责任，团队当下的主要目标是什么。</p><p>多和自己的同事，吃一吃饭，不同的人，对公司的看法，是不一样的，作为新人，这些人都是你的信息来源，你可以通过不同的信息渠道，了解这个团队的整体环境和问题。</p><h3>2、识别公司的业务模式是否健康</h3><p>一个团队，一定对应着这个团队的一整块业务，我们需要快速的识别出，这个业务到底是一个什么样的前景。</p><p>识别的第一个点，团队业务是不是赚钱。因为赚不赚钱，在很大程度上决定了团队的薪资待遇，团队是不是能够长期的存在下去，会不会出现裁员等等。</p><p>识别的第二个点，团队的业务有没有成长性。有些团队，明显是处于亏损的状态，但是亏损不一定没有前途，我们得明确，这个业务是处于上升期还是下行期。假如我们努力发力，会不会得到业务的发展，业务的发展，能不能带动我们职场的发展。</p><p>识别的第三个点，团队在公司内部的定位。有些团队，注定了在整个公司的大盘子里面，属于边角料的角色，公司不做这些业务，会对公司的整体战略有影响。但是这些业务，注定了没有长远发展，只能维持在表面的一个平衡状态，这样的一个团队，注定了在公司，处于一个被动的场域，对自身的职业发展，毫无意义。</p><h3>3、做一件稳赢的小事</h3><p>前一两个月，我们处于对公司的一个观察期，但是纯粹的观察，也不行，我们必须做一点小事情，能够在团队有一个初步的立场。</p><p>这个小事情，是你给团队的一个小小的投名状，是在团队初步建立信任的一个小事件，所以它必须成功，不能失败。</p><p>这样的小事情，首先是不能是团队中，长期存在的那些疑难杂症，如果这个东西，能让一个新人在一两个月内解决，它也不是疑难杂症了。这样的小事情，尽量是一个独立的事件单元，你既能轻松的应对，也能对团队有初步融入的贡献。</p><p>这个“小事”，事虽小，但是对你前期的影响一定是巨大，搞不好，你可能会整一个比较坑的开局。</p><p>这个“小事”，最好你事先，和你的导师/团队主管，充分沟通，确认的一件事。提前沟通好，这件事的难易程度，雷点在哪，在准备充分的情况下，把事情漂亮的完成。</p><h3>4、拉起目标再做事</h3><p>你了解了团队，了解了业务，还有一个重要的点，那就是团队对你的定位是什么？一般而言，企业不会随便招聘一个人，企业对每一个人是有要求的，需要你承担什么样的角色，解决什么样的问题，只有这些目标明确对齐之后，你再做事，才不会偏离原来的轨道。</p><p>初期，和你的导师/团队主管沟通的过程中，你需要明确，他们对你有什么想法，他们想要你在不同的阶段，解决什么问题。</p><p>比如，前两个月，他们会希望你做一些什么样的事，熟悉哪些业务。</p><p>转正之前，对你的考核核心KPI是什么，这些都是需要你主动找团队主管/导师，明确对齐的目标。</p><h2>决策期</h2><p>渡过前一两个月的时间，你会得到相对明确的信息。</p><p>你知道的团队的业务发展情况，搞懂了团队内部的人员情况，也明确了团队对你的期望是什么。你需要根据这些信息，得到你一个综合的结果判断。</p><p>判断的第一个点，团队到底值不值得你继续待。如果你在观察的过程中，明确了这个团队/业务，明显处于一个不好的状态，你觉得待下去毫无意义，那就直接另做打算，看有没有其他机会。如果这个团队，你发现它本身就很好，业务发展也不错，那经过一段时间的熟悉，就进入你的下个阶段了，真正的表演现在才刚开始。</p><p>判断的第二个点，团队对你的期望，是否是合理的。如果你是一个初级工程师，团队对你的期望是做什么项目的主力，明显这是一个牛头不对马嘴的配置，这样的团队，就算你继续干，那也一定是持续痛苦的抉择。如果你的团队主管，这个阶段，给你的只有蓝图、规划、鸡血、价值观，那你一定要小心，这样的团队很有可能只是消耗你生命的磨盘，注定了需要榨干你的血汗。</p><h2>结语</h2><p>事务的发展，总是遵循着一定的规律。</p><p>“潜龙勿用”，是易经中，对事务发展初期的规律的集中总结。无论是职场，还是其他，我们做事，做人，都应该按照一定的规律，做出符合这个阶段的事情，才能有更好的回报。</p><p>顺势而为，也是一种顺应事物规律的一种理念表达。</p><p>希望作者的这些见解，能够帮助到，正在职场中奋勇战斗的你。</p>]]></description></item><item>    <title><![CDATA[《Docker+New Relic+Je]]></title>    <link>https://segmentfault.com/a/1190000047381303</link>    <guid>https://segmentfault.com/a/1190000047381303</guid>    <pubDate>2025-11-08 23:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>SonarQube的代码质量守护、Docker的环境一致性保障、Notion的知识沉淀赋能、New Relic的性能深度洞察，再到Jenkins的自动化流程闭环、VS Code的个性化开发适配，这些工具之所以能成为开发者的高频选择，核心在于它们跳出了“工具即功能”的浅层逻辑，深入到开发流程的每一个关键节点，通过隐性的逻辑引导与显性的效率提升，让编码、协作、排查、迭代等环节形成无缝衔接的有机整体。很多开发者往往停留在工具的基础操作层面，却忽略了其背后蕴含的设计哲学—比如SonarQube的“规则动态适配”理念、Docker的“容器化隔离与复用”思维、Notion的“知识网状关联”逻辑，这些深层价值的挖掘与运用，才是拉开开发效率与代码质量差距的关键。它们如同精准的齿轮，不仅能简化重复劳动，更能在长期使用中重塑开发者的工作习惯，将规范化、高效化的思维渗透到每一次编码与协作中，让开发者从繁琐的事务中解脱出来，专注于核心业务逻辑的创新与实现。</p><p>SonarQube作为代码质量检测领域的标杆工具，其核心竞争力远不止于基础的语法错误提示，而是构建了一套“动态适配、深度引导、团队协同”的立体化质量管控体系。它内置了涵盖数十种编程语言的海量编码标准，这些标准并非僵化的条款，而是基于千万级项目实践沉淀的风险防控指南，同时支持团队根据自身技术栈特性、项目规模与业务场景，自定义检测规则与阈值，实现从“通用规范”到“专属标准”的精准适配。在实际开发场景中，SonarQube能够在编码过程中实时反馈问题，将质量优化的关口前移至开发阶段，避免问题积累到测试或生产环境才集中爆发，这种“即时反馈”机制极大地降低了问题修复的成本。与普通检测工具不同，它对每一个问题的呈现都遵循“现象-原因-风险-方案”的逻辑，不仅告知开发者“代码存在问题”，更会详细解释问题可能引发的潜在风险—比如未关闭的资源可能导致内存泄漏、不规范的并发处理可能引发线程安全问题，同时提供多种优化方案供选择，并关联官方技术文档与社区最佳实践，引导开发者从根本上理解问题本质，而非简单修改代码应付检测。对于多语言协同的复杂项目，SonarQube展现出了卓越的跨语言兼容能力，无论是Java、Python等后端语言，JavaScript、TypeScript等前端语言，还是Go、Rust等新兴语言，都能保持一致的检测精度与体验，其生成的可视化质量报告，还能为团队提供量化的改进依据，帮助团队在迭代过程中持续优化代码结构、减少技术债务，长期使用下来，更能潜移默化地培养开发者的编码洁癖，让规范编码成为无需刻意提醒的肌肉记忆。</p><p>Docker在本地开发环境管理领域的不可替代性，源于其对“环境一致性”与“可复现性”的极致追求，彻底解决了长期困扰开发者的“本地正常、线上报错”“设备差异、环境冲突”等核心痛点。它通过容器化技术，将应用运行所需的操作系统、依赖库、配置文件等全部封装在独立容器中，实现了“一次构建、到处运行”的理想状态，无论是Windows、Mac还是Linux操作系统，无论是开发、测试还是生产环境，都能通过相同的容器配置获得完全一致的运行环境，从根源上杜绝了因环境差异导致的开发障碍。在复杂项目的依赖管理中，Docker的优势尤为突出，它支持多版本依赖共存，比如同一台设备上可以同时运行Python 3.7与Python 3.10的容器，不同项目的依赖互不干扰，开发者无需在版本切换上耗费大量时间，只需通过简单的命令或图形化操作，就能快速创建、切换、销毁所需环境。更具价值的是，Docker支持环境配置的导出与共享，团队成员可以通过Docker Compose文件定义项目的完整依赖关系，新成员加入项目时，无需手动安装各类依赖、调试配置参数，只需加载配置文件即可一键启动完整的开发环境，极大地降低了团队协作的门槛。在资源优化方面，Docker采用的分层镜像技术能够有效减少存储空间的占用，相同的基础镜像可以被多个容器共享，同时容器的启动速度远超传统虚拟机，不会给本地设备带来过多性能负担。即使在网络不稳定的场景下，Docker也能支持离线使用，提前下载好的镜像可以在无网络环境中正常启动，确保开发工作的连续性，这种对各类开发场景的全面适配，让Docker成为开发者日常工作中不可或缺的核心支撑工具。</p><p>Notion打破了传统文档工具的静态局限，构建了一个“动态交互、实时协作、知识互联”的立体化知识沉淀与协作平台，重新定义了开发团队的文档使用场景。它的基础功能覆盖了富文本编辑、Markdown语法支持、图片视频等多媒体插入、表格与数据库创建等，但真正的核心价值在于其强大的实时协作与知识关联能力。在团队协作场景中，多名开发者可以同时编辑同一篇文档，每个人的修改操作都会实时同步给其他成员，无需等待文件传输或版本更新，通过内置的评论、批注功能，还能针对具体内容进行即时沟通，比如讨论接口设计方案、标注文档中的疑问点，彻底避免了传统文档在多人协作中出现的版本混乱、沟通滞后等问题。Notion的版本控制功能堪称极致，它能够完整记录每一次修改的历史轨迹，包括修改人、修改时间、具体修改内容，支持随时回溯到任意历史版本，同时提供清晰的版本对比视图，让团队成员能够快速了解文档的演进过程，即使出现误修改也能轻松恢复。更值得称道的是其知识关联能力，开发者可以通过标签、内部链接、数据库关联等方式，将分散的技术文档、接口说明、问题解决方案、项目计划等内容串联成结构化的知识网络，比如在某一接口的文档页面中，可以直接链接到相关的测试用例、代码实现、线上问题记录等内容，形成完整的知识闭环，让开发者在查阅信息时无需在多个工具之间切换，大幅提升信息获取效率。此外，Notion还支持与GitHub、Jira、Figma等主流开发工具的深度集成，实现文档与代码、任务、设计稿的联动更新—比如代码提交时自动关联相关文档并更新状态，任务状态变更时同步到文档的进度跟踪表中，确保知识的时效性与准确性，让文档不再是孤立的信息载体，而是深度融入开发全流程的知识支撑体系。</p><p>New Relic作为性能监控与优化领域的顶尖工具，其核心优势在于能够穿透应用的表层运行数据，深入到底层逻辑与全链路流程中，实现对性能问题的精准定位与深度优化。它并非简单收集CPU使用率、内存占用、网络延迟等基础指标，而是构建了覆盖前端、后端、移动端、数据库等全链路的监控体系，能够捕捉代码执行过程中的隐性问题—比如前端页面的不合理渲染逻辑导致首屏加载缓慢、后端服务的线程池配置不当引发响应延迟、数据库的低效查询导致事务阻塞、移动端的资源加载策略不合理造成卡顿等。在数据呈现上，New Relic通过可视化的仪表盘，将复杂的性能数据转化为直观的图表与报表，比如接口调用链的耗时分布、数据库查询的执行效率排行、前端资源的加载时序等，让开发者能够快速识别性能瓶颈所在。与普通监控工具不同，它提供的优化建议并非通用型的空泛指导，而是结合具体技术栈与应用场景的针对性方案，比如针对Spring Boot项目的内存泄漏问题，会提示检查Bean的生命周期管理与资源释放逻辑；针对React项目的渲染性能问题，会建议使用memo优化组件重渲染、合理拆分虚拟DOM；针对MySQL数据库的慢查询，会提供索引优化、SQL重构的具体思路。此外，New Relic还具备强大的性能风险预测能力，能够通过历史数据趋势分析，提前识别可能出现的性能问题—比如随着用户量增长，某一接口的响应时间呈线性上升趋势，工具会及时发出预警，并提供扩容建议或代码优化方向。其灵活的告警机制支持自定义阈值与通知方式，开发者可以根据业务场景设置关键指标的告警条件，通过邮件、短信、企业微信等渠道及时接收异常通知，确保能够快速响应并解决性能问题，让应用始终保持高效稳定的运行状态。</p><p>Jenkins以自动化构建与部署为核心，通过对开发流程的全链路自动化赋能，彻底改变了项目的迭代与交付模式，成为持续集成与持续部署（CI/CD）体系中的核心支撑工具。它能够将代码编译、单元测试、集成测试、代码质量检测、打包构建、部署上线等一系列重复繁琐的操作，转化为可配置、可复用的自动化流水线，开发者只需将代码提交到代码仓库，Jenkins就能按照预设的规则自动触发后续所有流程，无需人工干预，大幅减少了手动操作的时间成本与出错概率。Jenkins的核心价值在于其高度的灵活性与可扩展性，支持根据项目的规模与需求自定义流水线逻辑—对于小型项目，可以配置简单的“编译-测试-部署”流程；对于大型复杂项目，则可以设计多环境部署（开发、测试、预生产、生产）、多节点并行构建、灰度发布等复杂流程，满足不同阶段的交付需求。它具备强大的工具集成能力，能够与GitHub、GitLab等代码仓库，JUnit、Selenium等测试工具，Docker、Kubernetes等容器化与云原生工具，以及SonarQube等质量检测工具实现无缝对接，构建完整的自动化生态。在构建与部署过程中，Jenkins会自动执行预设的测试用例，及时发现代码中的错误与漏洞，并生成详细的测试报告与构建日志，帮助开发者快速定位问题；在部署环节，支持蓝绿部署、金丝雀发布等高级策略，能够在不影响线上服务正常运行的前提下完成版本更新，最大限度降低部署风险。此外，Jenkins还提供了完善的权限管理与日志审计功能，能够精准控制不同角色的操作权限，完整记录每一次构建与部署的全过程，方便团队进行流程追溯与问题排查，通过持续优化自动化流水线，不仅能提升项目的迭代效率与交付质量，更能推动开发流程的规范化与标准化，为团队的规模化协作提供坚实保障。</p><p>这些顶尖开发工具的共同特质，在于它们始终站在开发者的视角，深入理解开发流程中的真实痛点与核心需求，以“简化复杂流程、优化思维链路、赋能团队协作”为设计核心，在不断迭代中适配技术发展与开发模式的变化。它们的价值从未局限于“提升效率”这一表层目标，更在于通过科学的设计理念与强大的功能支撑，引导开发者养成规范化的工作习惯，构建系统化的思维方式，让开发工作从“被动应对问题”转向“主动预防风险”。对于开发者而言，深入挖掘这些工具的核心逻辑与进阶用法，并非单纯的“工具使用技巧提升”，而是对开发流程、协作模式、问题解决思路的全面优化—比如通过SonarQube培养规范化编码思维，通过Docker建立环境一致性意识，通过Notion构建结构化知识体系，通过New Relic形成全链路性能思维，通过Jenkins践行自动化与标准化理念。</p>]]></description></item><item>    <title><![CDATA[《VS+Elasticsearch+Fi]]></title>    <link>https://segmentfault.com/a/1190000047381306</link>    <guid>https://segmentfault.com/a/1190000047381306</guid>    <pubDate>2025-11-08 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>GitLab的代码管理与协作闭环、Visual Studio（VS）的智能编码生态、Postman的接口全生命周期管控、Elasticsearch的日志分析与问题定位、Terraform的基础设施编排、Figma的设计开发无缝联动，这组覆盖“代码-编码-测试-排查-部署-协作”的工具矩阵，正在打破传统开发流程中的信息孤岛与流程断点。很多开发者往往止步于单个工具的基础操作，却忽略了它们之间“协同大于叠加”的核心价值—比如GitLab与VS的代码实时同步、Postman与Elasticsearch的链路数据互通、Terraform与云服务的弹性适配、Figma与开发工具的组件化联动，这种跨场景的工具协同，能让开发全流程形成“需求-设计-编码-测试-部署-运维”的无断点闭环。它们的深层价值不在于简化单一操作，而在于通过底层逻辑的深度适配，将分散的开发环节串联成高效运转的有机体系，让开发者从流程内耗中解脱，专注于核心业务创新与技术突破，这正是工具组合成为现代开发团队核心资产的关键所在。</p><p>GitLab作为代码管理与协作领域的标杆，其价值早已超越单纯的代码仓库功能，而是构建了一套“代码管理-协作流转-自动化触发-知识沉淀-合规管控”的全链路协同体系。它以代码仓库为核心，深度整合Issue管理、合并请求（Merge Request）、CI/CD流水线、Wiki文档、权限管控等功能，形成从需求提出、开发迭代到代码交付的完整协作闭环。在团队协作场景中，GitLab支持灵活的分支管理策略，无论是适配并行开发的Git Flow、简化版本管理的Trunk Based Development，还是应对紧急修复的Hotfix分支流程，都能通过自定义分支规则确保代码质量与流程规范。合并请求功能更是将代码评审标准化、流程化，支持多人并行评审、代码逐行注释、自动化测试触发（如单元测试、集成测试、代码质量扫描），还能设置评审通过率、测试覆盖率等准入阈值，确保每一行代码提交都经过严谨校验，同时评审记录的完整留存，形成了可追溯、可复盘的协作档案。GitLab的CI/CD流水线并非简单的脚本执行，而是支持多阶段、多环境的灵活配置，能够与代码提交、合并操作深度联动，实现“提交即构建、合并即测试、达标即部署”的自动化流转，大幅缩短从代码开发到上线交付的周期。此外，GitLab的Wiki文档与代码仓库紧密关联，支持版本控制与多人协作编辑，开发者可在代码旁直接关联需求文档、接口说明、问题解决方案，形成“代码-文档-需求”的关联链路，避免知识沉淀与业务开发脱节；其合规管控功能还能满足企业级项目的权限隔离、操作审计、数据加密需求，这种全维度的协作赋能，让团队协作从“分散沟通”转向“结构化协同”，大幅提升协作效率与代码质量。</p><p>Visual Studio（VS）之所以能成为全球开发者青睐的编码工具，核心在于其构建了“智能适配-生态延伸-深度定制-跨端兼容”的全场景开发环境体系，而非单纯的代码编辑功能。它的智能感知功能并非基础的语法提示，而是基于项目依赖、代码上下文、开发者编码习惯的深度学习与适配，能够精准预测编码意图，提供变量补全、方法参数提示、错误实时预警、代码重构建议等个性化支持，甚至能根据代码结构自动生成注释、测试用例与文档，大幅降低编码的认知负荷与重复工作量。VS的调试生态堪称行业标杆，支持本地调试、远程调试、多线程调试、断点条件配置、内存泄漏检测、性能分析等多种调试模式，能够穿透代码层级，捕捉运行时的变量变化、调用链路、异常堆栈，帮助开发者快速定位隐藏在复杂逻辑中的深层问题。更具价值的是VS的插件生态，其官方插件市场与社区贡献的插件覆盖开发全流程需求—从代码格式化、语法检查、版本控制集成，到数据库连接、云服务部署、AI辅助编码（如Copilot的代码生成与优化建议），开发者可根据自身技术栈与项目需求，自定义构建专属开发环境。例如，针对Python开发可安装数据科学插件集（含数据分析、可视化工具），针对云原生开发可集成Kubernetes插件（支持集群管理、容器调试），针对前端开发可配置React/Vue专属工具链（含组件高亮、热重载）。此外，VS支持跨平台开发与多语言兼容，无论是Windows、Mac还是Linux系统，无论是C#、Java、Python等主流语言，还是Rust、Go等新兴语言，都能提供一致的开发体验；其内置的团队协作功能还支持多人实时共享开发环境，实现远程结对编程，进一步打破地域限制，让分布式团队协作如同面对面办公般高效。</p><p>Postman早已超越单纯的接口测试工具范畴，进化为“接口设计-测试验证-协作共享-全生命周期管控-生态联动”的接口管理平台，成为连接前后端、测试与开发的核心枢纽。它的接口设计功能支持RESTful、GraphQL、WebSocket等多种接口规范，提供可视化的接口编辑界面，开发者可在编码前完成接口定义、参数设计、响应格式规划、认证方式配置，生成标准化的接口文档，同时支持接口版本管理与历史回溯，确保接口设计的一致性与可追溯性。在接口测试环节，Postman支持手动测试与自动化测试的无缝切换，开发者可通过可视化界面快速配置请求参数、headers、Cookie、认证信息，实时发送请求并查看响应结果；还能通过Pre-request Script与Tests脚本，实现请求参数预处理、响应数据校验、接口性能指标统计（如响应时间、成功率），甚至可设置断言条件自动判断测试结果，生成可视化测试报告。其环境管理功能支持灵活切换开发、测试、预生产、生产等不同环境的配置，避免因环境参数差异导致的测试误差，同时支持环境变量的共享、继承与加密存储，简化多环境测试的配置流程。Postman的协作功能彻底解决了团队接口管理的核心痛点，支持团队空间创建、接口集合共享、测试用例协同编辑、评论反馈实时同步，团队成员可实时查看接口更新、参与测试评审，避免接口文档与实际实现不一致的问题。此外，Postman支持与GitLab、Jenkins等工具深度集成，可将接口测试用例嵌入CI/CD流水线，实现接口自动化测试与代码构建、部署的联动，当接口发生变更或出现异常时，及时触发告警并反馈给相关负责人；其Mock Server功能还能在后端接口未开发完成时，提供模拟响应，支持前端并行开发，这种从设计到部署的全生命周期管控，让接口管理从“被动适配”转向“主动赋能”，大幅提升跨团队协作效率。</p><p>Elasticsearch在开发领域的价值，早已突破传统搜索引擎的定位，成为“日志分析-问题排查-数据洞察-实时监控”的核心支撑工具，尤其适配分布式系统与微服务架构的开发需求。它基于分布式架构设计，具备高吞吐、低延迟、可扩展、容错性强的特性，能够快速聚合来自应用系统、服务器、数据库、接口、移动端等多源的日志数据，通过分词索引、全文检索、聚合分析等核心能力，将分散的非结构化日志转化为可查询、可分析的结构化数据。在问题排查场景中，Elasticsearch打破了日志分散存储的局限，开发者无需在多台服务器、多个日志文件中手动检索，只需通过简单的查询语句（支持关键词、模糊匹配、范围查询等），就能快速定位特定时间范围、特定服务、特定错误类型的日志信息；结合Kibana的可视化能力，还能还原请求的完整调用链路，清晰展示各环节的耗时、依赖关系，帮助开发者快速定位跨服务调用中的瓶颈与异常。与普通日志工具不同，Elasticsearch支持实时日志采集与分析，通过Filebeat、Logstash等数据采集工具实时同步日志数据，结合Kibana的仪表盘，将日志中的关键指标（如错误率、响应时间、请求量、资源占用率）转化为直观的图表，让开发者实时监控系统运行状态，提前识别潜在风险（如接口响应时间突增、错误率异常上升）。在性能优化场景中，Elasticsearch可通过日志分析挖掘系统隐性问题，比如通过分析接口调用日志发现高频慢查询、通过服务器日志识别资源占用异常的进程、通过用户行为日志定位前端页面的交互瓶颈，这些深度洞察为优化决策提供了数据支撑。此外，Elasticsearch的扩展性极强，支持根据业务规模动态扩容，无论是小型项目的日志管理，还是大型分布式系统的全链路监控，都能轻松应对；其与Kubernetes、Docker等云原生工具的兼容能力，更是让它成为云原生架构下日志分析的首选工具，为开发团队提供全方位、实时化、深度化的数据洞察能力。</p><p>Terraform作为基础设施即代码（IaC）领域的核心工具，其底层逻辑在于将基础设施的部署与管理从“手动操作”转化为“代码化定义、自动化编排、可复用迭代”，彻底解决了传统基础设施管理中效率低下、环境不一致、可追溯性差、跨云适配难的痛点。它通过声明式语法，让开发者用代码形式定义服务器、数据库、网络、存储、负载均衡等各类基础设施资源，无论是AWS、阿里云、腾讯云等公有云，还是私有云、混合云环境，都能通过统一的语法进行描述，实现“一套代码，多环境部署”的跨云适配能力，避免了因云厂商接口差异导致的重复开发。Terraform的核心优势在于其状态管理机制，它通过状态文件（state file）记录基础设施的实际部署状态，每次执行部署操作时，都会对比代码定义的“期望状态”与实际状态，自动计算出需要创建、更新或销毁的资源，避免重复部署或误删关键资源，这种“状态对比-增量执行”的逻辑，大幅提升了基础设施部署的安全性与效率。在团队协作场景中，Terraform的代码化特性让基础设施配置可纳入GitLab等版本控制工具，支持多人协作编辑、代码评审、版本回溯，确保每一次基础设施变更都有迹可循；同时通过模块（Module）功能，开发者可将常用的基础设施配置（如数据库集群、负载均衡、安全组规则）封装为可复用模块，支持版本管理与跨项目共享，减少重复编码，提升配置的一致性与可维护性。Terraform还支持与CI/CD工具（如Jenkins、GitLab CI）深度集成，将基础设施部署纳入自动化流水线，实现“代码提交-自动化测试-基础设施部署”的全流程自动化，当业务需求发生变化时，只需修改Terraform代码并提交，就能自动完成基础设施的扩容、缩容或配置更新，无需人工干预。此外，Terraform的漂移检测功能能够定期检查基础设施的实际状态与代码定义的期望状态是否一致，及时发现并修复手动操作导致的配置漂移，这种代码化、自动化、可复用、可追溯的基础设施管理模式，不仅降低了运维门槛，更让开发团队以软件开发的思维管理基础设施，实现基础设施与应用代码的协同迭代。</p><p>Figma虽然常被定义为设计工具，但在开发全流程中，它早已成为连接设计与开发的关键协同枢纽，核心价值在于打破设计与开发之间的信息壁垒、认知偏差与协作延迟，实现“设计规范-资产共享-协作同步-代码转化”的闭环。Figma的设计稿并非静态图片文件，而是支持实时编辑、多人协作的动态资产，设计师可在同一文件中协同创作、实时反馈，通过组件库功能将常用的按钮、图标、版式、色彩系统封装为可复用组件，支持组件样式的统一更新，当组件样式发生变更时，所有引用该组件的设计稿都会自动同步，从根源上确保设计规范的一致性。对于开发团队而言，Figma提供了精准的设计稿查看与标注功能，开发者可直接在设计稿上查看元素的尺寸、颜色、间距、字体、圆角等详细参数，支持像素级精准校验，无需依赖设计师的手动标注；同时支持一键导出切图、图标等资源，导出格式（PNG、SVG、WebP）与尺寸可灵活配置，完美适配不同开发场景需求。Figma的版本控制功能能够完整记录设计稿的每一次修改，支持随时回溯到任意历史版本，提供清晰的修改对比，让开发团队快速了解设计变更内容，避免因设计迭代导致的开发返工。更重要的是，Figma支持与开发工具的深度集成，通过插件可将设计稿中的组件自动转化为CSS变量、iOS的Assets资源、Android的Drawable文件，甚至直接生成React、Vue等前端组件代码，大幅减少手动转化的误差与工作量；同时支持与Jira等项目管理工具联动，将设计任务与开发任务关联，实现设计进度与开发进度的同步可视化。此外，Figma的设计系统功能还能帮助团队构建标准化的设计语言，将设计规范、组件库、设计资源集中管理，支持跨项目复用，让设计与开发的协作从“反复沟通确认”转向“结构化协同”，大幅缩短从设计到开发的转化周期，确保设计意图的精准落地。</p><p>这些覆盖开发全流程的工具组合，其核心竞争力不在于单个工具的功能强度，而在于它们之间的协同逻辑与对开发流程的深度适配。GitLab的协作闭环、VS的智能赋能、Postman的接口管控、Elasticsearch的数据分析、Terraform的基础设施编排、Figma的跨角色协同，共同构建了一套“无断点、可追溯、自动化、可复用、可扩展”的开发全链路体系。它们的深层价值在于，通过代码化、自动化、协同化的设计理念，将开发流程中的重复工作、沟通成本、潜在风险降至最低，同时培养开发者的结构化思维与协同意识。</p>]]></description></item><item>    <title><![CDATA[鸿蒙 ArkTS 入门教程：小白实战 L]]></title>    <link>https://segmentfault.com/a/1190000047380623</link>    <guid>https://segmentfault.com/a/1190000047380623</guid>    <pubDate>2025-11-08 22:02:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>@[toc]</p><h2>鸿蒙 ArkTS 入门教程：小白实战 List 列表开发（详解 @State, ForEach, @Builder）</h2><blockquote><strong>摘要：</strong> 本文是一篇鸿蒙 ArkTS 小白入门教程。通过一个“国风”商品列表 Demo，带你实战学习 ArkTS 的 List 列表、ForEach 循环、@State 状态管理和 @Builder 自定义组件。从零开始，快速掌握鸿蒙列表页面的开发核心。</blockquote><p>Hello 大家好！刚开始学鸿蒙（HarmonyOS）开发，是不是感觉 ArkTS 有点东西？别怕，今天我（一个和你一样刚起步的萌新）就来分享一下我的学习 Demo 笔记——一个看起来“高大上”的中国传统文化商品展示页！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380626" alt="" title=""/></p><p>别看它长得复古，用的知识点可都是“潮牌”：<strong>List 列表</strong>、<strong>ForEach 循环</strong>、<strong>@State 状态管理</strong> 和 <strong>@Builder 自定义组件</strong>。</p><p>别看名词多，其实捅破了就那层窗户纸！上车，出发！</p><h3>STEP 1: “兵马未动，粮草先行”——定义数据模型</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380627" alt="" title="" loading="lazy"/></p><p>在写界面之前，我们得先告诉程序，咱们的“商品”长啥样。总不能空手套白狼吧？</p><p>咱们先用 <code>class</code> 定义一个 <code>CultureProduct</code>（文化产品）类。它就像一个<strong>商品说明书模板</strong>，规定了每个商品必须有 ID、名字、价格、表情包（bushi，是 emoji 图标）、描述和分类。这就是咱们的数据结构。</p><pre><code class="typescript">// 中国传统文化商品展示 Demo
// 知识点：List列表、ForEach循环、@State状态管理、自定义组件

// 商品数据模型
class CultureProduct {
  id: number;
  name: string;
  price: number;
  emoji: string;
  description: string;
  category: string;

  constructor(id: number, name: string, price: number, emoji: string, description: string, category: string) {
    this.id = id;
    this.name = name;
    this.price = price;
    this.emoji = emoji;
    this.description = description;
    this.category = category;
  }
}
````

## STEP 2: 开张！创建页面和“魔法”状态

![](https://files.mdnice.com/user/139748/2b643e51-70de-4735-90f2-f0cea5cbb8dd.png)

有了模板，咱们就该创建页面（`@Component`）了。`@Entry` 告诉系统：“嘿，这就是我的店门面，程序从这里启动！”

最关键的来了：`@State` 装饰器。这玩意儿是鸿蒙 ArkTS 的“响应式魔法”之一。

用 `@State` 标记的 `productList`（我们的商品列表）一旦发生变化（比如你以后做了个“添加商品”按钮），UI 界面就会**自动刷新**！再也不用手动操作 DOM 了，是不是很爽？

我们先“伪造”一点数据，把 `productList` 填满，开张大吉！
</code></pre><p>@Entry<br/>@Component<br/>export struct CultureProductDemo {<br/>  // 商品列表数据<br/>  @State productList: CultureProduct[] = [<br/>    new CultureProduct(1, '青花瓷茶具', 688, '🫖', '景德镇手工青花瓷，传承千年工艺', '茶具'),<br/>    new CultureProduct(2, '宣纸书法套装', 298, '📜', '安徽宣纸，配毛笔墨汁，初学者首选', '文房四宝'),<br/>    new CultureProduct(3, '紫砂壶', 1280, '🍵', '宜兴紫砂，纯手工制作，养壶佳品', '茶具'),<br/>    new CultureProduct(4, '中国结挂饰', 88, '🪢', '手工编织，寓意吉祥如意', '装饰品'),<br/>    new CultureProduct(5, '古琴', 3800, '🎵', '桐木古琴，音色悠扬，附赠教学视频', '乐器'),<br/>    new CultureProduct(6, '景泰蓝花瓶', 1580, '🏺', '北京景泰蓝，宫廷工艺，收藏佳品', '工艺品'),<br/>    new CultureProduct(7, '汉服套装', 568, '👘', '明制汉服，刺绣精美，含配饰', '服饰'),<br/>    new CultureProduct(8, '围棋套装', 428, '⚫', '云子围棋，实木棋盘，适合对弈', '棋类')<br/>  ];</p><pre><code>
## STEP 3: 搭建“货架”——布局和标题

![](https://files.mdnice.com/user/139748/0b6beab2-a8cf-498c-be2b-a21bee6d6ac8.png)

数据有了，咱得搭个货架（页面布局）。

每个组件都有一个 `build()` 方法，这是它的“化妆间”，负责画出它的样子。我们先放一个 `Column()`，意思是里面的东西都给我**垂直排好队**。

然后加个 `Text()` 标题，调调字号、加粗、颜色啥的，让它看起来像个“标题党”，专业！
</code></pre><p>  build() {<br/>    Column() {<br/>      // 页面标题<br/>      Text('传统文化精品')<br/>        .fontSize(24)<br/>        .fontWeight(FontWeight.Bold)<br/>        .fontColor('#8B4513')<br/>        .margin({ top: 20, bottom: 16 })</p><pre><code>
## STEP 4: 循环“上货”—— List 和 ForEach 闪亮登场

![](https://files.mdnice.com/user/139748/5a50e1df-ad64-45a5-aff4-0666c3f35c6a.png)

重点来了！我们怎么把 `@State` 里的那一堆商品数据（`productList`）一个个摆到“货架”上？

答案是 `List` + `ForEach` 这对黄金搭档！

  * `List`： 顾名思义，就是个列表容器，自带“滚动”技能。
  * `ForEach`： 这就是个“循环打印机”。它会遍历 `productList` 里的**每一项**（我们临时叫它 `product`），然后为每一项执行一次 `ListItem()` 里的操作。

在 `ListItem()` 里面，我们没有直接写一堆 `Row`、`Column`，而是调用了一个**自定义组件**：`this.ProductCard(product)`。

这就像在说：“嘿，`ProductCard` 小弟（我们待会儿就定义它），这个 `product` 数据给你，你帮我把它画成一个卡片的样子！”

（`layoutWeight(1)` 是个小技巧，意思是让列表占满 `Column` 里的剩余所有空间。）
</code></pre><p>      // 商品列表<br/>      List({ space: 12 }) {<br/>        ForEach(this.productList, (product: CultureProduct) =&gt; {<br/>          ListItem() {<br/>            this.ProductCard(product)<br/>          }<br/>        }, (product: CultureProduct) =&gt; product.id.toString())<br/>      }<br/>      .width('100%')<br/>      .layoutWeight(1)<br/>      .backgroundColor('#FFF8DC')<br/>    }<br/>    .width('100%')<br/>    .height('100%')<br/>    .padding({ left: 16, right: 16 })<br/>    .backgroundColor('#FFFAF0')<br/>  }</p><pre><code>
## STEP 5: “私人订制”—— @Builder 自定义卡片组件

![](https://files.mdnice.com/user/139748/51cbe3a5-3f25-4463-a52b-b3fbb1c9012f.png)

终于到了我们的大功臣——`ProductCard` 卡片组件！

这里用到了 `@Builder` 装饰器。你可以把它理解为一个“UI 蓝图生成器”。它是一个方法，你给它一个 `product` 数据，它就还你一个画好的 UI 组件。

这样做的好处是**代码复用**和**结构清晰**！如果以后想改卡片样式，只需要改这一个地方，`List` 里的所有卡片都会跟着变。

卡片内部，我们用一个 `Row()`（水平布局）分成了左右两部分。
</code></pre><p>  // 商品卡片组件<br/>  @Builder<br/>  ProductCard(product: CultureProduct) {<br/>    Row() {</p><pre><code>
### 5.1 卡片左侧：图标和分类

![](https://files.mdnice.com/user/139748/fe2f9e8a-2847-43f5-9f04-6b85cb9f6026.png)

左边是一个 `Column()`（垂直布局），放“表情包” `emoji` 和一个带背景色的 `category` 分类标签。调调间距和背景，小小的细节，瞬间提升 B 格。
</code></pre><p>      // 左侧：商品图标和分类<br/>      Column() {<br/>        Text(product.emoji)<br/>          .fontSize(40)<br/>        Text(product.category)<br/>          .fontSize(10)<br/>          .fontColor('#FFFFFF')<br/>          .backgroundColor('#D2691E')<br/>          .borderRadius(4)<br/>          .padding({ left: 6, right: 6, top: 2, bottom: 2 })<br/>          .margin({ top: 4 })<br/>      }<br/>      .width(80)<br/>      .height(80)<br/>      .justifyContent(FlexAlign.Center)<br/>      .backgroundColor('#FFF5EE')<br/>      .borderRadius(8)</p><pre><code>
### 5.2 卡片右侧：名称、价格和描述

![](https://files.mdnice.com/user/139748/cbd0b734-31c7-4d83-8ea0-f7eacb3a7ae4.png)

右边也是一个 `Column()`（垂直布局），用来放商品信息。

  * **第一行**：我们又用了一个 `Row()`（水平布局），左边放商品名称 `name`（`layoutWeight(1)` 让它抢占所有剩余空间），右边放价格 `price`。
  * **第二行**：放商品描述 `description`。注意 `maxLines(2)` 和 `textOverflow({ overflow: TextOverflow.Ellipsis })`，这是防止描述太长“溢出”的经典操作，多余的字会变成“...”。

&lt;!-- end list --&gt;
</code></pre><p>      // 右侧：商品信息<br/>      Column() {<br/>        // 商品名称和价格<br/>        Row() {<br/>          Text(product.name)<br/>            .fontSize(16)<br/>            .fontWeight(FontWeight.Medium)<br/>            .fontColor('#8B4513')<br/>            .layoutWeight(1)<br/>          Text('¥' + product.price)<br/>            .fontSize(18)<br/>            .fontWeight(FontWeight.Bold)<br/>            .fontColor('#DC143C')<br/>        }<br/>        .width('100%')<br/>        .margin({ bottom: 8 })</p><p>        // 商品描述<br/>        Text(product.description)<br/>          .fontSize(14)<br/>          .fontColor('#A0522D')<br/>          .maxLines(2)<br/>          .textOverflow({ overflow: TextOverflow.Ellipsis })<br/>      }<br/>      .layoutWeight(1)<br/>      .padding({ left: 12 })<br/>      .alignItems(HorizontalAlign.Start)</p><pre><code>
### 5.3 收尾：卡片样式和点击事件

![](https://files.mdnice.com/user/139748/d532289c-2daa-4fa9-bc0a-e10df8ce2a14.png)

最后，我们给最外层的 `Row()`（也就是整个卡片）加上圆角、背景色、阴影（`shadow` 让它有“浮”起来的立体感），还有最重要的 `onClick` 点击事件！

现在，你点击任何一个商品卡片，控制台（Console）都会打印出你点击了谁。这就是交互的起点！
</code></pre><p>    }<br/>    .width('100%')<br/>    .padding(12)<br/>    .backgroundColor('#FFFFFF')<br/>    .borderRadius(12)<br/>    .shadow({ radius: 8, color: '#E0E0E0', offsetX: 0, offsetY: 2 })<br/>    .onClick(() =&gt; {<br/>      console.info(<code>点击了商品：${product.name}</code>)<br/>    })<br/>  }<br/>}</p><pre><code>
## 🏁 总结：你的鸿蒙之旅已启程

![](https://files.mdnice.com/user/139748/e4d59d57-f8d6-42cd-8bb6-58af677f597f.jpg)

恭喜你！你已经成功完成了这个“国风”商品列表。我们来快速回顾一下你解锁的技能点：

  * **数据模型 (Class):** 搭建了应用的基础数据骨架。
  * **@State:** 掌握了让 UI 自动“活”起来的响应式魔法。
  * **List + ForEach:** 学会了如何高效地批量渲染数据。
  * **@Builder:** 获得了创建可复用、高颜值 UI 模块的技能。

这几个知识点是 ArkTS 开发中名副其实的“基石”。

不要小看这个 Demo，你已经掌握了构建复杂列表页面的核心逻辑。这只是一个开始，继续保持这份好奇心，多动手实践，从这个小小的起点出发，一步步去探索鸿蒙世界的更多可能。相信自己，你的开发之路会越走越宽广！

-----

**文章标签：** 鸿蒙 ArkTS @State List ForEach @Builder 鸿蒙入门


</code></pre>]]></description></item><item>    <title><![CDATA[Camsys 时间戳信息简介 地平线智驾]]></title>    <link>https://segmentfault.com/a/1190000047380978</link>    <guid>https://segmentfault.com/a/1190000047380978</guid>    <pubDate>2025-11-08 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>不同平台时间戳介绍</h2><h2>1.征程 3 平台</h2><p>其中 u64 timestamps： 硬件时间戳，是跟 CPU 一起用的 64 bit system counter，1s 是 24M 个 clock。 FS 的时候从硬件寄存器读取。读取的值除以 24000 是毫秒，除以 24000000 是秒。</p><p>struct timeval tv; 系统时间， SIF FS 的时候获取 do\_gettimeofday。</p><pre><code class="C">J3 时间戳和frameid相关的结构体定义如下：
驱动接口：
struct frame_id {
    u32 frame_id;
    u64 timestamps;
    struct timeval tv;
};
HAL结构体：
typedef struct image_info_s {
    uint16_t sensor_id;
    uint32_t pipeline_id;
    uint32_t frame_id;
    uint64_t time_stamp;    //HW time stamp 硬件时间
    struct timeval tv;     //system time of hal get buf，系统时间
    int buf_index;
    int img_format;
    int fd[HB_VIO_BUFFER_MAX_PLANES];//ion buf fd
    uint32_t size[HB_VIO_BUFFER_MAX_PLANES];
    uint32_t planeCount;
    uint32_t dynamic_flag;
    uint32_t water_mark_line;
    VIO_DATA_TYPE_E data_type;
    buffer_state_e state;
} image_info_t;</code></pre><h2>2.征程 5 平台</h2><p>其中的 tv 时间戳来源三种可选，通过设备树可以设置默认来源，通过 sys 节点可以动态临时修改来源</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380980" alt="" title=""/></p><pre><code class="C">typedef struct _frame_info_s{
        uint32_t frame_id;
        uint64_t timestamps;//hw_timestamp （硬件时间戳）
        struct timeval tv;// 系统时间（软件时间戳）
        uint32_t format;
        uint32_t height;
        uint32_t width;
        uint64_t addr[7];
        uint32_t pre_int;
        uint32_t num_planes;
        int32_t bufferindex;
        uint32_t pixel_length;
        uint32_t dynamic_flag;
} frame_info_t;</code></pre><h2>3.征程 6 平台</h2><p>相较与 征程 5， 增加了 lpwm 信号的 trig 时间，同时 tv 时间中的 phc 由硬件自动缓存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380981" alt="" title="" loading="lazy"/></p><pre><code class="C">typedef struct image_info_s {
        uint16_t sensor_id; /**&lt; sensor id */
        uint32_t pipeline_id; /**&lt; pipeline id */
        uint32_t frame_id; /**&lt; frame id */
        uint64_t time_stamp; /**&lt; HW time stamp */
        struct timeval tv; /**&lt; system time of hal get buf */
        int32_t buf_index; /**&lt; buffer index */
        int32_t img_format; /**&lt; image format */
        int32_t fd[HB_VIO_BUFFER_MAX_PLANES]; /**&lt; ion buf fd */
        uint32_t size[HB_VIO_BUFFER_MAX_PLANES]; /**&lt; buffer size per plane */
        uint32_t planeCount; /**&lt; image plane count */
        uint32_t dynamic_flag; /**&lt; dynamic flag */
        uint32_t water_mark_line; /**&lt; water mark line value */
        VIO_DATA_TYPE_E data_type; /**&lt; buffer data type */
        buffer_state_e state; /**&lt; buffer state */
        uint64_t desc; /**&lt; temp description for isp raw feedback */
        struct timeval trig_tv;/**&lt; system time of lpwm trigger */
} image_info_t;</code></pre>]]></description></item><item>    <title><![CDATA[外贸业务员跟进客户的8大必备技巧：全解析]]></title>    <link>https://segmentfault.com/a/1190000047380696</link>    <guid>https://segmentfault.com/a/1190000047380696</guid>    <pubDate>2025-11-08 21:05:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在外贸行业中，客户跟进是业务员日常工作中最重要的一环。无论是开发新客户还是维护老客户，跟进的质量直接决定了订单的成败。然而，跟进客户并不是简单的“打电话”“发邮件”，而是一个需要技巧、策略和耐心的过程。本文将详细介绍外贸业务员跟进客户的8大必备技巧与方法，帮助外贸人提升客户转化率，赢得更多订单。<br/><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdmX2V" alt="" title=""/></p><h2>1. 充分了解客户背景</h2><p>在跟进客户之前，业务员需要对客户的背景信息进行充分的了解。这不仅能帮助你更好地与客户沟通，还能让客户感受到你的专业性和诚意。</p><p>具体方法<br/>研究客户公司信息：通过客户官网、LinkedIn、行业论坛等渠道，了解客户的公司规模、主营业务、市场定位等。<br/>分析客户需求：根据客户的询盘内容或初步沟通，判断客户的采购需求、预算范围以及决策流程。<br/>了解客户所在市场：熟悉客户所在国家或地区的市场特点、文化习惯以及行业趋势。<br/>注意事项<br/>不要仅仅停留在表面信息，要深入挖掘客户的潜在需求。<br/>在沟通中适时展示你对客户背景的了解，但不要显得过于“窥探”。</p><h2>2. 制定个性化的跟进策略</h2><p>每个客户的需求和偏好都不同，因此，业务员需要根据客户的具体情况制定个性化的跟进策略，而不是采用“一刀切”的方式。</p><p>具体方法<br/>根据客户类型分类：将客户分为潜在客户、意向客户和老客户，针对不同类型的客户制定不同的跟进计划。<br/>调整沟通方式：有些客户喜欢通过邮件沟通，有些则更倾向于电话或即时通讯工具（如WhatsApp、WeChat）。<br/>提供定制化解决方案：根据客户的需求，提供针对性的产品推荐或报价方案。<br/>注意事项<br/>个性化并不意味着复杂化，关键是让客户感受到你的关注和专业。<br/>在跟进过程中，始终围绕客户的核心需求展开。</p><h2>3. 把握跟进的时机</h2><p>跟进客户的时机非常重要，过早或过晚都可能导致客户流失。业务员需要根据客户的反馈和采购周期，选择合适的时间进行跟进。</p><p>具体方法<br/>及时回复客户询盘：收到客户的询盘后，尽量在24小时内回复，避免客户等待过久。<br/>根据客户的采购周期跟进：如果客户表示需要一周时间考虑，不要频繁打扰，但也不要超过约定时间。<br/>利用节假日或特殊时机：在客户所在国家的重要节假日或公司周年庆时，发送问候邮件或优惠信息。<br/>注意事项<br/>避免在客户的休息时间或繁忙时段联系。<br/>如果客户长时间未回复，可以尝试发送“提醒邮件”，但要注意语气礼貌。</p><h2>4. 保持沟通的连续性</h2><p>客户跟进是一个持续的过程，业务员需要通过多次沟通逐步建立信任，而不是“一锤子买卖”。</p><p>具体方法<br/>定期发送邮件：即使客户暂时没有采购需求，也可以定期发送公司动态、行业资讯或节日问候。<br/>记录沟通内容：通过CRM系统或Excel表格记录每次与客户的沟通内容，避免重复或遗漏。<br/>主动提供增值服务：例如，向客户分享市场趋势报告或产品使用案例。<br/>注意事项<br/>不要让客户觉得你只是为了“卖东西”，而是希望与他们建立长期合作关系。<br/>在沟通中保持专业和热情，但不要过于频繁打扰。</p><h2>5. 善用邮件营销工具</h2><p>邮件是外贸业务员跟进客户的主要工具之一，但如何写出高质量的邮件并提高回复率，是一门需要掌握的技巧。</p><p>具体方法<br/>邮件标题简洁明了：例如，“关于您询盘的报价详情”或“最新产品推荐”。<br/>邮件内容结构清晰：开头问候客户，中间说明目的，结尾附上联系方式和期待回复。<br/>个性化邮件内容：在邮件中提及客户的公司名称或具体需求，避免使用模板化的语言。<br/>注意事项<br/>避免发送过长或过于复杂的邮件。<br/>定期检查垃圾邮件文件夹，确保客户的回复不会被遗漏。</p><h2>6. 学会倾听客户的需求</h2><p>在与客户沟通时，业务员不仅要表达自己的观点，更要学会倾听客户的需求和反馈。</p><p>具体方法<br/>多问开放性问题：例如，“您对这款产品的具体要求是什么？”或“您对我们的报价有什么建议？”<br/>记录客户的反馈：将客户的需求、疑问和意见记录下来，作为后续跟进的依据。<br/>适时调整策略：根据客户的反馈，调整产品推荐或报价方案。<br/>注意事项<br/>不要急于推销产品，而是先了解客户的真实需求。<br/>在倾听的同时，适时提出专业建议，展示你的价值。</p><h2>7. 解决客户的疑虑</h2><p>客户在采购过程中可能会有各种疑虑，例如价格、质量、交货时间等。业务员需要主动解决这些问题，消除客户的顾虑。</p><p>具体方法<br/>提供详细的产品信息：包括规格、认证、使用说明等。<br/>展示成功案例：通过分享其他客户的成功合作案例，增强客户的信任。<br/>灵活处理价格问题：如果客户对价格有异议，可以通过折扣、赠品或分期付款等方式进行谈判。<br/>注意事项<br/>不要回避客户的问题，而是积极寻找解决方案。<br/>在谈判中保持灵活性，但不要轻易妥协。</p><h2>8. 建立长期合作关系</h2><p>客户的价值不仅在于一次订单，而在于长期合作的潜力。业务员需要通过优质的服务和持续的沟通，赢得客户的信任和忠诚。</p><p>具体方法<br/>定期回访老客户：了解他们的最新需求，并提供相应的解决方案。<br/>提供增值服务：例如，免费样品、技术支持或市场分析报告。<br/>建立个人关系：通过节日问候、生日祝福等方式，与客户建立更深层次的联系。<br/>注意事项<br/>不要只关注短期利益，而是注重长期合作的价值。<br/>在服务中始终保持专业和真诚。<br/>总结表格：外贸业务员跟进客户的8大技巧<br/>技巧    具体方法    注意事项<br/>充分了解客户背景    研究客户公司信息、分析需求、了解市场特点    避免停留在表面信息，深入挖掘客户需求<br/>制定个性化跟进策略    分类客户、调整沟通方式、提供定制化解决方案    个性化不等于复杂化，围绕客户核心需求展开<br/>把握跟进时机    及时回复询盘、根据采购周期跟进、利用节假日发送问候    避免频繁打扰，语气礼貌<br/>保持沟通连续性    定期发送邮件、记录沟通内容、主动提供增值服务    不要让客户觉得你只是为了“卖东西”<br/>善用邮件营销工具    标题简洁明了、内容结构清晰、个性化邮件内容    避免邮件过长或模板化<br/>学会倾听客户需求    多问开放性问题、记录反馈、适时调整策略    不急于推销，先了解客户真实需求<br/>解决客户疑虑    提供详细信息、展示成功案例、灵活处理价格问题    不回避问题，积极寻找解决方案<br/>建立长期合作关系    定期回访、提供增值服务、建立个人关系    注重长期合作价值，保持专业和真诚</p><h2>常见问题解答FAQ</h2><h2>1. 如何提高客户跟进的成功率？</h2><p>提高成功率的关键在于充分了解客户需求、制定个性化策略以及保持沟通的连续性。善用工具（如CRM系统）记录客户信息，并在合适的时机进行跟进。</p><h2>2. 跟进客户时，如何避免显得过于“打扰”？</h2><p>避免频繁联系客户，尤其是在客户明确表示需要时间考虑时。可以通过定期发送行业资讯或节日问候邮件，保持适度的存在感。</p><h2>3. 如果客户长时间未回复，应该怎么办？</h2><p>可以发送“提醒邮件”，语气礼貌地询问客户是否需要进一步帮助。如果仍未回复，可以尝试通过其他渠道（如电话或即时通讯工具）联系。</p><h2>4. 如何处理客户对价格的异议？</h2><p>针对价格异议，可以通过提供折扣、赠品或分期付款等方式进行谈判。同时，展示产品的价值和成功案例，增强客户的信任。</p><h2>5. 跟进客户时，哪些工具可以提高效率？</h2><p>使用CRM系统（如Zoho CRM）记录客户信息和沟通内容，邮件营销工具（如Zoho Campaigns）发送个性化邮件，以及即时通讯工具（如WhatsApp、WeChat）进行实时沟通。</p><p>通过掌握以上技巧和方法，外贸业务员可以更高效地跟进客户，提升订单转化率，并与客户建立长期合作关系。</p>]]></description></item><item>    <title><![CDATA[海外仓库管理VS国内仓库管理：核心区别与]]></title>    <link>https://segmentfault.com/a/1190000047380783</link>    <guid>https://segmentfault.com/a/1190000047380783</guid>    <pubDate>2025-11-08 21:04:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在全球化贸易的浪潮下，越来越多的企业开始涉足跨境电商和海外仓业务。然而，海外仓库管理与国内仓库管理在多个方面存在显著差异。了解这些差异，可以帮助企业更好地规划和优化仓库管理策略，提升运营效率和客户满意度。本文将详细探讨海外仓库管理与国内仓库管理的主要区别，并提供一些实用的管理建议。<br/><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdmX4k" alt="" title=""/></p><h2>一、海外仓库管理与国内仓库管理的主要区别</h2><h2>1、物流配送差异</h2><p>维度    国内仓库管理    海外仓库管理<br/>运输方式    陆运为主，时效稳定（1 - 3天）    海运/空运 + 尾程配送，周期长（15 - 30天）<br/>清关流程    无（国内流转）    需处理目的国海关、VAT缴纳<br/>库存调度    单仓 / 区域仓调拨，响应速度快    跨大洲补货，需提前3个月备货</p><h2>2、库存管理</h2><p>海外仓库：</p><p>库存需求预测复杂：海外市场的客户需求和销售趋势变化较快，企业需要更精准的需求预测模型，以避免库存积压或缺货。<br/>库存周转率低：由于物流周期较长，库存周转率相对较低，企业需要优化库存管理策略，减少库存持有成本。<br/>国内仓库：</p><p>库存需求预测相对稳定：国内市场的客户需求和销售趋势相对稳定，需求预测相对容易，库存管理相对简单。<br/>库存周转率高：国内物流周期较短，库存周转率较高，企业可以更灵活地调整库存水平。</p><h2>3、法规与合规性</h2><p>海外仓库：</p><p>法规复杂：不同国家和地区有不同的法规和标准，企业需要了解并遵守当地的进出口法规、税务法规、劳动法规等，确保合规运营。<br/>合规成本高：合规性要求高，企业需要投入更多的时间和资源进行合规管理，增加运营成本。<br/>国内仓库：</p><p>法规相对统一：国内法规相对统一，企业只需遵守国家和地方的相关法规，合规管理相对简单。<br/>合规成本低：合规性要求相对较低，企业可以更高效地进行运营。</p><h2>4、供应链管理</h2><p>海外仓库：</p><p>供应链复杂：海外仓库涉及多个供应商和物流合作伙伴，供应链管理更加复杂，企业需要建立更高效的供应链协同机制。<br/>风险控制难度大：国际供应链涉及多个环节，风险控制难度大，企业需要建立完善的风险管理体系，应对各种不确定性。<br/>国内仓库：</p><p>供应链相对简单：国内仓库的供应链相对简单，管理相对集中，风险控制相对容易。<br/>协同效率高：国内供应链协同效率高，企业可以更高效地进行采购、生产和销售。</p><h2>5、客户服务</h2><p>海外仓库：</p><p>语言和文化差异：海外客户来自不同的国家和地区，语言和文化差异大，企业需要提供多语言支持，提升客户体验。<br/>服务响应时间长：由于地理位置分散，服务响应时间相对较长，企业需要优化客户服务流程，提高响应速度。<br/>国内仓库：</p><p>语言和文化统一：国内客户语言和文化相对统一，客户服务相对简单。<br/>服务响应时间短：国内地理位置集中，服务响应时间相对较短，企业可以更快速地响应客户需求。</p><h2>二、Zoho Books：仓库管理的得力助手</h2><p>Zoho Books企业版提供了高级库存管理功能，可以帮助企业适应国外仓库管理的复杂环境，提高海外仓库管理效率，降低运营成本，提升客户满意度。</p><h2>1、实时监控与预警</h2><p>Zoho Books支持实时监控库存水平，并设置库存上下限预警值。当库存水平接近或低于预警值时，系统会自动发出警报，提醒采购部门及时补货，避免因缺货而失去销售机会。</p><h2>2、国际物流管理</h2><p>Zoho Books支持与DHL、FedEx等物流商API对接，企业可以通过系统实时跟踪物流状态，优化物流路线，降低物流成本。客户也可实时查询包裹轨迹。</p><h2>3、库存分类与分析</h2><p>Zoho Books支持对库存进行分类管理，企业可以根据产品的销售频率、重要性和价值进行分类，优先管理高价值和高销售频率的产品。同时，系统支持与数据分析工具集成，帮助企业生成销售趋势报表、库存周转率报表和客户购买行为报表，为库存管理提供数据支持。</p><h2>4、符合国际税法</h2><p>Zoho Books提供了15个特色地区版本（包括：美国、加拿大、墨西哥、英国、南非、沙特阿拉伯等），可以生成符合当地法规的税务报表，帮助企业生成合规报表，减少人工误差与罚款风险。</p><h2>5、数据安全与备份</h2><p>Zoho Books采用先进的数据加密技术，保护数据在存储和传输过程中的安全。同时，系统提供自动备份功能，定期备份数据，并支持数据恢复，确保企业在遇到系统故障或数据丢失时能够快速恢复业务。</p><h2>6、优质的服务与支持</h2><p>Zoho Books提供了优质的客户服务和技术支持，帮助企业在实施过程中解决各种问题。无论是技术问题还是业务咨询，用户都能得到及时、有效的帮助。这不仅增强了企业对平台的信心，还确保了系统的稳定运行和持续优化。</p><h2>三、为什么选择Zoho Books？</h2><p>性价比高：Zoho Books企业版8400元/年，支持10用户 + 5个仓库，覆盖中小型企业需求。<br/>灵活扩展：提供从免费版到旗舰版4个版本，适配企业从初创到集团化发展。<br/>生态协同：与Zoho CRM、Zoho Inventory无缝集成，构建跨境业务闭环.</p><h2>四、总结</h2><p>海外仓与国内仓的管理差异本质上是全球化运营能力的比拼。通过Zoho Books的智能化工具，企业不仅能实现多仓高效协同，更能降低合规风险、提升客户体验。立即申请14天免费试用，即可体验全球仓储管理的革新力量！</p>]]></description></item><item>    <title><![CDATA[2025年个人网盘推荐：Zoho、阿里云]]></title>    <link>https://segmentfault.com/a/1190000047380819</link>    <guid>https://segmentfault.com/a/1190000047380819</guid>    <pubDate>2025-11-08 21:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>无论是对于数据备份、照片存储，还是文件传输，都离不开网盘。有哪些个人网盘推荐？个人和企业网盘的区别是什么？本文将带你拨开迷雾，逐层解构，助你更清晰地做出选择。<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdmX4U" alt="" title=""/></p><h2>一、个人用户对网盘的功能需求</h2><p>无论是存储照片、共享文件还是备份数据，个人用户对网盘的需求总是离不开以下几个关键点。归纳来看：</p><h2>1. 存储能力：空间焦虑</h2><p>现代社会里，每个人身上的“数字包袱”正越来越重：满满一整年的旅行照片、几乎看不完的高清电影、以及层叠成山的工作文档。若你习惯将文件存在手机、电脑的本地存储器上，那么内存告急的红色提醒总会不期而至。这时，一个高性价比、高稳定性的网盘无疑成为了必需品。很多人倾向于选择那些容量较大的网盘产品，以便图库、文档以及娱乐资源能随时存取，这也是网盘“解放硬盘”特性的核心价值。</p><h2>2. 文件安全：抵御数字“海啸”</h2><p>网络攻击和硬盘损毁威胁着每一个个体用户。对于较为敏感的个人数据（如税务信息、智能设备的隐私文件等），文件加密与多重备份显得尤为重要。此外，借助优秀的数据传输和同步技术，用户还希望在任何设备上都能快速找回自己的文件而无须担心丢失风险。</p><h2>3. 便捷分享：从小圈到大圈</h2><p>社交共享是个人网盘的加分点之一。无论是大文件传输，还是旅行相册一次性分享，传输链接功能与权限设置功能早已成为主流用户“加单”体验的重要参考指标。毕竟，在分享文件时，我们更愿意选择简单粗暴、极速传递的方式。</p><h2>二、个人和企业网盘的区别</h2><p>虽然个人用户和企业用户都需要依靠网盘来解决存储问题，但两者的关注点和功能偏好存在本质差异。产品经理们在设计两者功能模块时，需要从需求出发，帮助用户解决不同痛点。</p><h2>1. 场景定位：单兵作战 vs. 团队协作</h2><p>个人网盘关注的是个体中心化存储。一位普通用户可能需要跨多个终端存取文件，但仅涉及到个人操作，数据链条相对单一。相反，企业网盘则需要支持多用户并发操作，强调成员之间的协作与共享。比如，云端协作文档、部门间权限划分等功能，显然是企业用户的“刚需”。</p><h2>2. 安全性要求：标准 vs. 高规格</h2><p>个人用户对数据隐私的要求集中在文件加密、防止外泄的层面上，而企业用户则除了担心隐私问题，还需要遵循合规性（如GDPR、ISO认证等），尤其是涉及金融、医疗等敏感行业，安全性和稳定性是企业用户的核心SLA（服务水平协议）指标。</p><h2>3. 扩展性：固定订阅 vs. 灵活升级</h2><p>个人用户的存储需求通常较固定，而企业用户的存储规模随着业务扩大呈现动态变化。这也就决定了企业网盘必须具备极强的弹性扩展能力，按需调整存储空间。而个人用户通常以固定容量订阅为主，不太需要频繁调整。</p><h2>三、有哪些个人网盘推荐？</h2><p>如今市面上涌现了许多丰富的个人网盘产品。除了耳熟能详的主流品牌，还有不少小而美的产品颇具竞争力。以下为你推荐几款表现突出的网盘工具：</p><h2>Zoho网盘：小众但值得挖掘</h2><p>Zoho网盘可能对很多国内用户较为陌生，但它是一款非常适合个人及小型团队的出色云存储工具。无论你是需要存储个人笔记，还是同步原始素材，这款服务都能提供稳定的体验。此外，Zoho生态下多工具融合的特性，也适合想用更多插件的用户尝试。Zoho企业网盘的价格不高，而且界面简洁、功能直观。如果你想找到稍有个性的小网盘，它不失为一个好选择。</p><h2>四、FAQ：常见问题</h2><p>为了让大家更加明确自己的实际需求，以下总结了关于个人网盘的一些高频问题并一一解答：</p><h2>1. 使用免费网盘是否可靠？</h2><p>答：免费网盘虽然是许多人初次接触云存储的尝试，但免费额度通常不会太大，主要适合存放“小体量”数据。此外，免费服务可能在安全性和数据备份上不如付费服务全面。对于重要文件存储，建议优先考虑具备一定品牌信誉的网盘工具。</p><h2>2. 网盘的同步功能会不会占用设备性能？</h2><p>答：大部分优秀的同步盘服务商都会优化同步的后台任务，确保不影响设备的正常运行。不过，建议大家定期查看是否有文件需要清理，以免造成设备反应迟缓。</p><h2>3. 怎样选择更适合自己的云存储服务？</h2><p>答：这个问题可以从功能需求出发：假如你主要存放个人照片和日常文件，选择稳定、容量较大的网盘即可。如果你更注重办公效率，可以选那些带有协作和权限管理的工具。</p>]]></description></item><item>    <title><![CDATA[TensorRT 和 ONNX Runt]]></title>    <link>https://segmentfault.com/a/1190000047380879</link>    <guid>https://segmentfault.com/a/1190000047380879</guid>    <pubDate>2025-11-08 21:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>模型速度的瓶颈往往不在算法本身。几毫秒的优化累积起来就能让用户感受到明显的性能提升。下面这些技术都是在生产环境跑出来的经验，不需要重构代码实施起来也相对简单并且效果显著。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047380881" alt="" title=""/></p><h2>固定输入形状，越早告诉运行时越好</h2><p>动态形状用起来方便但对性能不友好。TensorRT 和 ONNX Runtime 在处理固定形状时能做更激进的优化。</p><p>TensorRT 这边，构建引擎时最好围绕实际使用的 min/opt/max 设置 optimization profile，生产环境尽量让所有请求都落在 opt 范围。ONNX Runtime 可以直接导出固定维度的模型，比如 1×3×224×224。确实需要动态性的话，也要把范围控制得足够紧。</p><pre><code> # TensorRT: build with a tight optimization profile  
 profile = builder.create_optimization_profile()  
 profile.set_shape("input", (1,3,224,224), (1,3,224,224), (1,3,224,224))  
 config.add_optimization_profile(profile)</code></pre><p>这样kernel 选择、内存规划、算子融合在形状确定的情况下都能做得更彻底。</p><h2>启动前把该热的都热一遍</h2><p>冷启动的开销藏在各个角落：驱动初始化、page fault、lazy allocation。服务启动和重启后跑几轮 warmup，把这些一次性成本提前消化掉。</p><pre><code> # ONNX Runtime warmup + pinned buffers  
import onnxruntime as ort, numpy as np  

so = ort.SessionOptions()  
so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  
sess = ort.InferenceSession("model.onnx", sess_options=so, providers=["CUDAExecutionProvider"])  

x = np.random.randn(1,3,224,224).astype(np.float32)  
for _ in range(8):  # small loop to populate caches/contexts  
     sess.run(None, {"input": x})</code></pre><p>warmup 的形状一定要和线上一模一样。如果服务多种 batch size，每个都得过一遍。</p><h2>I/O binding 配合 pinned memory 减少拷贝</h2><p>Host 和 device 之间来回搬数据是 tail latency 的大敌。buffer 绑定一次，后面反复用就行了。</p><pre><code> # ONNX Runtime I/O binding example  
io = sess.io_binding()  
x = np.random.randn(1,3,224,224).astype(np.float32)  

# Upload once &amp; bind  
io.bind_cpu_input("input", x)       # or bind to CUDA device via OrtValue  
io.bind_output("logits", device_type="cuda")  

sess.run_with_iobinding(io)  
 out = io.copy_outputs_to_cpu()[0]   # pull back only when you must</code></pre><p>GPU 流量大的场景，host 端内存用 page-locked（pinned）能让 H2D/D2H 传输快不少。本质上是把多次小开销合并成一次前置成本，allocator 也不用频繁介入。</p><h2>精度降低不一定掉点</h2><p>现在的 GPU 对 FP16 支持很好，服务器 CPU 和 NPU 上 INT8 的收益也越来越明显。只要精度守得住，延迟的改善非常直接。</p><p>TensorRT 开 FP16 就是一个 flag 的设置：</p><pre><code>config.set_flag(trt.BuilderFlag.FP16)</code></pre><p>。但是INT8 需要校准，要拿代表性数据跑一遍生成 per-channel scale。ONNX Runtime 可以用 TensorRT EP 或者直接加载量化后的模型。</p><pre><code> # TensorRT FP16 (and INT8 if you have a calibrator)  
 config.set_flag(trt.BuilderFlag.FP16)  
 # config.set_flag(trt.BuilderFlag.INT8)  
 # config.int8_calibrator = calibrator</code></pre><p>这里可以先量化最慢的几个子图，比如 embedding 层或者 attention block，不用一上来就全模型量化。</p><h2>图优化可以开到最高档，但要验证数值</h2><p>让运行时自己去融合算子、选更优的 kernel，这个收益基本是白来的。</p><pre><code> # ONNX Runtime optimizations + EPs  
so = ort.SessionOptions()  
so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  
providers = [  
  ("TensorrtExecutionProvider", {"trt_fp16_enable": True}),  
  "CUDAExecutionProvider",  
  "CPUExecutionProvider"  
]  
 sess = ort.InferenceSession("model.onnx", sess_options=so, providers=providers)</code></pre><p>但是需要注意的是融合会改变计算顺序，数值可能有细微漂移。开启前后要跑个 tolerance check，确保输出没问题。</p><h2>micro-batch 在 GPU 上效果明显</h2><p>单条请求跑推理简单，但硬件利用率往往上不去。打包成 4-8 个请求一起跑，能在保持低延迟的同时提升吞吐。</p><p>关键是 batching window 要够小，比如 2-5ms，不然 p95 会飙。micro-batch 的大小最好和前面 optimization profile 设置的尺寸对齐。</p><p>不过如果 SLA 本身就很紧（p50 要求 5ms 以内），micro-batch 带来的收益可能不如下面要说的 CUDA Graph。</p><h2>CUDA Graph 消除 kernel launch 开销</h2><p>小模型或者调用频繁的 graph，kernel launch 的开销会很明显。CUDA Graph 能把整个推理过程录制下来，replay 时几乎没有 CPU 开销。</p><p>TensorRT 在形状固定的情况下可以直接启用，只需要warmup 一次，后面就一直跑 captured graph。</p><p>这里可以理解成在 GPU driver 层面把推理编译成一个可重放的宏。</p><h2>ONNX Runtime 线程设置有讲究</h2><p>ONNX Runtime 暴露了几个线程相关的参数，对 CPU 和混合负载的 tail latency 影响挺大。</p><pre><code> so = ort.SessionOptions()  
 so.intra_op_num_threads = 1   # one thread per operator often stabilizes latency  
 so.inter_op_num_threads = 1   # avoid oversubscription; raise carefully if parallel graphs</code></pre><p>Execution Provider 的选择也很重要：</p><p>GPU 场景优先级是 TensorRT EP → CUDA EP → CPU EP fallback。纯 CPU 跑的话 OpenMP 或者 DNNL/MKL build 配合合理的线程池设置效果最好。边缘设备上 Intel 的盒子可以试试 OpenVINO EP。</p><h2>把预处理后处理从 GIL 里挪出去</h2><p>Python 的胶水代码经常成为隐藏的性能杀手。能用 NumPy 向量化就别写循环，能用 Numba 或者推到 CUDA/CuPy 上更好。热路径里的 transform 最好提前编译好。如果要并发处理请求，worker pool 的规模要和运行时的线程数配合好。</p><pre><code> # Example: pre-allocate and reuse buffers to dodge Python overheads  
import numpy as np  

class Preprocessor:  
    def __init__(self, shape=(1,3,224,224)):  
        self.buf = np.empty(shape, dtype=np.float32)  

    def __call__(self, img):  
        # write into self.buf in-place; no fresh allocations  
        np.copyto(self.buf, img)  
        self.buf /= 255.0  
         return self.buf</code></pre><p>这里的判断标准很简单，每个请求都会跑的代码，问问能不能预分配、向量化或者缓存起来。</p><h2>Session、Engine、Buffer 都只建一次</h2><p>每个请求新建一个</p><pre><code>trt.ICudaEngine</code></pre><p>或</p><pre><code>onnxruntime.InferenceSession</code></pre><p>基本等于自杀。output array 每次重新分配也一样。</p><p>正确做法是进程启动时就加载好 singleton session/engine，每个 worker 维护一两个 CUDA stream，buffer pool 按 shape 和 dtype 索引。</p><pre><code> # Simple singleton-ish loader  
class Model:  
    _sess = None  
    _io = None  
    @classmethod  
    def get(cls):  
        if cls._sess is None:  
            so = ort.SessionOptions()  
            so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  
            cls._sess = ort.InferenceSession("model.onnx", sess_options=so,  
                          providers=["CUDAExecutionProvider"])  
            cls._io = cls._sess.io_binding()  
         return cls._sess, cls._io</code></pre><p>这样做的好处是稳定，p95 不会因为 allocator 和 initializer 出现在热路径而突然炸掉。</p><h2>一个完整的 GPU 推理骨架</h2><p>下面的代码是把前面几个关键技术串起来：</p><pre><code> import onnxruntime as ort, numpy as np  

def make_session(path):  
    so = ort.SessionOptions()  
    so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  
    providers = [("TensorrtExecutionProvider", {"trt_fp16_enable": True}),  
                 "CUDAExecutionProvider", "CPUExecutionProvider"]  
    return ort.InferenceSession(path, sess_options=so, providers=providers)  

class Runner:  
    def __init__(self, model_path, shape=(1,3,224,224)):  
        self.sess = make_session(model_path)  
        self.shape = shape  
        self.io = self.sess.io_binding()  
        self._warmup()  

    def _warmup(self, iters=8):  
        x = np.random.randn(*self.shape).astype(np.float32)  
        self.io.bind_cpu_input("input", x)  
        self.io.bind_output("logits", device_type="cuda")  
        for _ in range(iters):  
            self.sess.run_with_iobinding(self.io)  
        self.io.clear_binding_inputs()  # ready for real runs  

    def run(self, x_np: np.ndarray):  
        # assumes x_np matches self.shape; in production, validate or clamp  
        self.io.bind_cpu_input("input", x_np)  
        self.io.bind_output("logits", device_type="cuda")  
        self.sess.run_with_iobinding(self.io)  
        return self.io.copy_outputs_to_cpu()[0]  

# usage  
runner = Runner("model.onnx")  
batch = np.random.randn(1,3,224,224).astype(np.float32)  
 probs = runner.run(batch)</code></pre><p>这个代码已经包含了图优化、I/O binding 和 warmup。后面再加上 CUDA Graph、micro-batch 和固定 shape，能把延迟压到很低，基本上拿来就可以用了</p><h2>几个容易踩的坑</h2><p>延迟指标一定要看 p50/p90/p95，别只盯平均值。真正的问题都藏在 tail 里。API 层面最好把 shape 和 dtype 固定下来，或者至少让调用方知道优化过的范围。这样生产请求才能稳定落在最优路径上。</p><p>开了融合或量化之后，精度的自动化回归测试必不可少。</p><h2>总结</h2><p>低延迟不靠黑科技就是一堆小优化叠起来：形状固定、减少拷贝、更好的 kernel、graph capture、运行时零意外。每个单拎出来可能只省几毫秒，但加起来用户就能感受到"快"。</p><p><a href="https://link.segmentfault.com/?enc=CV08MtYSwpXNR6Yn1xexCQ%3D%3D.9y3EVjljrRB%2BR3rqKIpGJC1mNflqYGW2%2BLTL5fSBaHk4FsMkJUXOWvBgM647j%2B6rMwDiGZWw971N8Mp6lmnaAA%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/494ca93b9c184407936ef7b6bd16e15e</a></p><p>作者:Syntal</p>]]></description></item><item>    <title><![CDATA[EDM打开率点击率怎么跟踪？干货分享 遭]]></title>    <link>https://segmentfault.com/a/1190000047380884</link>    <guid>https://segmentfault.com/a/1190000047380884</guid>    <pubDate>2025-11-08 21:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>电子邮件营销（EDM）一直是企业与客户保持联系的重要手段之一。它不仅可以提高品牌知名度，还能够促进销售转化。然而，想要获得最佳效果，跟踪和分析邮件的关键指标，如打开率、点击率等是至关重要的。本文将详解如何有效地进行这些数据的收集和解析，从而提升你的EDM策略。<br/><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdmX5X" alt="" title=""/></p><h2>一、认识邮件打开率与点击率</h2><h2>1.1 邮件打开率</h2><p>邮件打开率是用于衡量有多少收件人真正打开了你发送的邮件。计算公式为：打开率 = （邮件被打开次数 ÷ 送达邮件总数）× 100%。高打开率表明你的邮件主题行具备很强的吸引力或者你的品牌对收件人具有很大吸引力。</p><h2>1.2 邮件点击率</h2><p>点击率反映了在打开邮件后的用户互动程度，即有多少人点击了邮件中的链接。公式为：点击率 = （邮件中链接被点击次数 ÷ 打开邮件次数）× 100%。这个指标可以帮助你评估邮件内容的有效性和用户参与度。</p><h2>二、如何提升邮件打开率</h2><h2>2.1 吸引人的主题行</h2><p>主题行是用户看到的第一个信息。使用富有创意、简洁而吸引人的主题行可以大幅提高邮件的打开率。不妨在主题中加入个性化的元素，如收件人姓名，或提出一个引人入胜的问题。</p><h2>2.2 精准的收件人细分</h2><p>针对不同的受众群体发送个性化邮件是提升打开率的有效方法。通过客户数据分析，将收件人按年龄、购买行为、地理位置等因素进行细分，并为不同群体制定对应的内容策略。</p><h2>2.3 发件时间的优化</h2><p>避免高峰时段和垃圾邮件高发时段，选择读者最有可能打开邮件的时间发送。在某些行业，工作日上午或者休息后的晚上可能是不错的选择。</p><h2>三、如何提高邮件点击率</h2><h2>3.1 内容和设计质量</h2><p>确保邮件内容与设计均具有吸引力。内容应当简洁、有价值且与主题保持一致，而设计应突出核心信息，保持整洁的排版以及适合移动设备阅读的格式。</p><h2>3.2 突出的号召性用语（CTA）</h2><p>明确且有吸引力的号召性语句（CTA）是提升点击率的关键。使用动词驱动型的短语，例如“立即购买”、“点击查看详情”等，鼓励用户进一步操作。</p><h2>3.3 灵活使用A/B测试</h2><p>通过A/B测试找出最有效的邮件版本。测试不同的邮件主题、CTAs、设计风格，分析用户反馈，从而优化邮件的每一个细节。</p><p>四、邮件分析工具的选择</p><h2>4.1 使用专业的EDM工具</h2><p>市场上有许多EDM工具如Zoho Campaigns等，提供详细的数据分析功能。通过这些平台，你可以实时查看打开率、点击率及其他关键指标。</p><h2>4.2 数据监控与报告</h2><p>定期监控和记录邮件的分析数据将帮助制定未来策略。制定每周或每月的报表是许多企业成功的关键之一，确保能够根据具体数据进行调整。</p><h2>五、避免邮件被标记为垃圾邮件</h2><h2>5.1 了解垃圾邮件过滤器</h2><p>了解垃圾邮件过滤的工作原理非常重要。避免使用过于商业化的词语或者符号过多的内容，确保发件人地址的信任和每封邮件的退订选项易于访问。</p><h2>5.2 提高发件人信誉</h2><p>确保邮件的合法性和信誉度。长期与值得信赖的邮件地址保持互动，监控发送IP地址的表现，以避免被列入黑名单。</p><h2>六、数据分析后的策略改进</h2><h2>6.1 验证结果并执行</h2><p>在获取并分析数据后，验证你的假设，并在策略中及时整合新的发现。同时，不断根据用户反馈和数据变化调整策略。</p><h2>6.2 提升用户体验</h2><p>利用数据为用户创造更具个性化的体验，这包括设计更好的内容、提供更加定制化的产品推荐，以维持用户长期的参与度与忠诚度。</p><h2>结论</h2><p>邮件营销是一个动态的过程，需要不断测试与优化。借助有效的数据跟踪与分析机制，你才能真正掌控邮件营销过程，获得最大商业回报。跟踪邮件的打开率和点击率只是开始，结合完整的营销策略才是EDM成功的关键。让我们持续探索和应用这些策略，从而不断提升营销效果。</p>]]></description></item><item>    <title><![CDATA[LLM 训练基础概念与流程简介 地平线智]]></title>    <link>https://segmentfault.com/a/1190000047380914</link>    <guid>https://segmentfault.com/a/1190000047380914</guid>    <pubDate>2025-11-08 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. LLM 训练基础概念</h2><h3>1.1 预训练（Pretrain）</h3><p>LLM 首先要学习的并非直接与人交流，而是让网络参数中充满知识的墨水，“墨水” 理论上喝的越饱越好，产生大量的对世界的知识积累。 预训练就是让 Model 先埋头苦学大量基本的知识，例如从 Wiki 百科、新闻、书籍整理大规模的高质量训练数据。 这个过程是“无监督”的，即人类不需要在过程中做任何“有监督”的校正，而是由模型自己从大量文本中总结规律学习知识点。 模型此阶段目的只有一个：学会词语接龙。例如我们输入“秦始皇”四个字，它可以接龙“是中国的第一位皇帝”。</p><h3>1.2 有监督微调（Supervised Fine-Tuning）</h3><p>经过预训练，LLM 此时已经掌握了大量知识，然而此时它只会无脑地词语接龙，还不会与人聊天。</p><p>SFT 阶段就需要把半成品 LLM 施加一个自定义的聊天模板进行微调。例如模型遇到这样的模板【问题-&gt; 回答，问题-&gt; 回答】后不再无脑接龙，而是意识到这是一段完整的对话结束。 称这个过程为指令微调，就如同让已经学富五车的「牛顿」先生适应 21 世纪智能手机的聊天习惯，学习屏幕左侧是对方消息，右侧是本人消息这个规律。</p><h3>1.3 人类反馈强化学习（Reinforcement Learning from Human Feedback， RLHF）</h3><p>在预训练与有监督训练过程中，模型已经具备了基本的对话能力，但是这样的能力完全基于单词接龙，缺少正反样例的激励。 模型此时尚未知什么回答是好的，什么是差的。</p><p>希望模型能够更符合人的偏好，降低让人类不满意答案的产生概率。 这个过程就像是让模型参加新的培训，优秀员工作为正例，消极员工作为反例，学习如何更好地回复。可以使用 RLHF 系列之-直接偏好优化（Direct Preference Optimization， DPO）或与 PPO（Proximal Policy Optimization）。DPO 相比于 PPO：</p><ul><li>DPO 通过推导 PPO 奖励模型的显式解，把在线奖励模型换成离线数据，Ref 模型输出可以提前保存。</li><li>DPO 性能几乎不变，只用跑 actor\_model 和 ref\_model 两个模型，大大节省显存开销和增加训练稳定性。</li></ul><p>RLHF 训练步骤并非必须，此步骤难以提升模型“智力”而通常仅用于提升模型的“礼貌”，有利（符合偏好、减少有害内容）也有弊（样本收集昂贵、反馈偏差、多样性损失）。</p><p>GRPO（Generalized Reinforcement Preference Optimization）是一种改进的强化学习方法，用于优化模型输出更符合人类偏好。它是对 PPO（Proximal Policy Optimization）+ RLAIF（Reinforcement Learning from AI Feedback）等方法的泛化和增强，本质上是对 RLHF（人类反馈强化学习）的一种高效实现。GRPO 的目标：从两个或多个候选输出中，优化模型朝更高偏好方向移动，而不是只学单个“正确答案”。</p><h3>1.4 知识蒸馏（Knowledge Distillation， KD）</h3><p>经过预训练、有监督训练、人类反馈强化学习，模型已经完全具备了基本能力，通常可以学成出师了。</p><p>知识蒸馏可以进一步优化模型的性能和效率，所谓知识蒸馏，即学生模型面向教师模型学习。 教师模型通常是经过充分训练的大模型，具有较高的准确性和泛化能力。 学生模型是一个较小的模型，目标是学习教师模型的行为，而不是直接从原始数据中学习。</p><p>在 SFT 学习中，模型的目标是拟合词 Token 分类硬标签（hard labels），即真实的类别标签（如 0 或 100）。 在知识蒸馏中，教师模型的 softmax 概率分布被用作软标签（soft labels）。小模型仅学习软标签，并使用 KL-Loss 来优化模型的参数。</p><p>通俗地说，SFT 直接学习老师给的解题答案。而 KD 过程相当于“打开”老师聪明的大脑，尽可能地模仿老师“大脑”思考问题的神经元状态。知识蒸馏的目的只有一个：让小模型体积更小的同时效果更好。 然而随着 LLM 诞生和发展，模型蒸馏一词被广泛滥用，从而产生了“白盒/黑盒”知识蒸馏两个派别。 GPT-4 这种闭源模型，由于无法获取其内部结构，因此只能面向它所输出的数据学习，这个过程称之为黑盒蒸馏，也是大模型时代最普遍的做法。黑盒蒸馏与 SFT 过程完全一致，只不过数据是从大模型的输出收集。</p><h3>1.5 LoRA (Low-Rank Adaptation)</h3><p>LoRA 是一种高效的参数高效微调（Parameter-Efficient Fine-Tuning， PEFT）方法，旨在通过低秩分解的方式对预训练模型进行微调。 相比于全参数微调（Full Fine-Tuning），LoRA 只需要更新少量的参数。 LoRA 的核心思想是：在模型的权重矩阵中引入低秩分解，仅对低秩部分进行更新，而保持原始预训练权重不变。</p><h2>2.  LLM 训练流程简介</h2><p>训练任何模型，需要清楚两个问题：</p><ol><li>明确模型的输入与输出</li><li>定义模型的损失函数</li></ol><p>LLM，即大语言模型，本质上是一个“token 接龙”高手，它不断预测下一个词符。这种推理生成方式被称为自回归模型，因为模型的输出会作为下一轮的输入，形成一个循环。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380916" alt="" title=""/></p><p>刚开始，一个随机大模型，面对输入，它预测的下一个字符完全是随机的</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380917" alt="" title="" loading="lazy"/></p><p>那么，它是如何学习的呢？在自注意力机制中，通过为 qk 增加掩码，softmax 后将负无穷对应到 0，隐藏掉 n 字符以后的内容。这样，输出的第 n+1 个字符只能关注到前 n 个字符，如同戴上了一副“只看过去”的眼镜。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380918" alt="" title="" loading="lazy"/></p><p>通过训练，大模型从一个随机混沌的状态，逐渐学会输入与下一个词符之间的潜在联系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047380919" alt="" title="" loading="lazy"/></p><p>以上是为了便于理解而抽象出来的过程。</p><p>大模型的输入是由数字组成的张量，而非自然语言字符。自然语言通过 tokenizer（可以理解为一种词典）映射到词典的页码数字 ID，进行输入计算。得到的输出数字再利用词典进行解码，重新得到自然语言。</p><p>大模型的输出是一个 N*len（tokenizer）的多分类概率张量，在 Topk 中选出的有概率的 token，得到下一个词。</p><p>损失函数：交叉熵损失</p><p>学习率：与 batchsize 成倍数关系，batchsize 变大一倍，学习率也增大一倍</p><h2>参考链接</h2><blockquote><a href="https://link.segmentfault.com/?enc=BnfO9bRR1YNViCrRmLoDOA%3D%3D.QCZj%2FR%2FZG3N8SdsLRwWehXkxbTyy04fgcoR%2FhUsv%2BSYCJMCufqPEsxdsllHWvUky" rel="nofollow" target="_blank">https://github.com/jingyaogong/minimind</a><br/><a href="https://link.segmentfault.com/?enc=F17dDVH56%2BhZLYaTSWHxkA%3D%3D.qmkXXNvSiB%2FDEFmQebXqrusgpi%2BRH1nKZN8f%2B8PDggS5%2B5sdHoLf5J%2B1GHr4PTbq" rel="nofollow" target="_blank">https://developer.horizon.auto/blog/13043</a></blockquote>]]></description></item><item>    <title><![CDATA[【赵渝强老师】使用Helm简化Kuber]]></title>    <link>https://segmentfault.com/a/1190000047380583</link>    <guid>https://segmentfault.com/a/1190000047380583</guid>    <pubDate>2025-11-08 20:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在Kubernetes中部署应用程序，需要创建Pod、Deployment和Service等资源，并且创建的步骤也是比较繁琐的。当遇到复杂系统时，Kubernetes的应用部署和管理就变得相当的复杂。好在可以使用Helm来管理Kubernetes，它可以很大程度上简化Kubernetes应用的部署和管理。视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV1rP1XB8E9V/?aid=115513564926022&amp;cid=33824640129" target="_blank">https://www.bilibili.com/video/BV1rP1XB8E9V/?aid=115513564926...</a></p><h2>一、 什么是Helm？</h2><p>Helm通过打包的方式动态创建Kubernetes应用的配置信息，然后生成应用程序的YAML清单文件，并最终由kubectl进行调用完成应用的部署。因此从使用方式上看，Helm类似于Linux YUM的包管理。下面展示了Helm的体系架构。<br/><img width="625" height="301" referrerpolicy="no-referrer" src="/img/bVdmX0E" alt="image.png" title="image.png"/></p><p>从Helm 3开始，Helm将所有的配置信息存储在Kubernetes集群的配置中。Helm中有三个非常重要的概念，它们分别是：Chart、Repository和Release。</p><ul><li><strong>Chart</strong>：应用程序信息的集合，包括了应用程序中对Kubernetes资源的定义和依赖关系的说明等。</li><li><strong>Repository</strong>：存放Chart的仓库。</li><li><strong>Release</strong>：Chart的运行的实例，代表一个正在运行的应用。当Chart在Kubernetes集群中部署成功后，就会生成一个Release。</li></ul><h2>二、 部署Helm</h2><p>在GitHub上提供了Helm多种操作系统的二进制版本，下面的步骤将在master节点上安装和部署Helm。这里使用的是helm-v3.5.4-linux-amd64.tar.gz。</p><p>（1）解压Helm安装包，并将helm的可执行命令复制到目录”/usr/bin/“目录下。</p><pre><code class="powershell">tar -zxvf helm-v3.5.4-linux-amd64.tar.gz
cd linux-amd64/
mv helm /usr/bin/</code></pre><p>（2）添加Helm的Repository仓库。</p><pre><code class="powershell">#添加Helm官方的Repository仓库
helm repo add stable https://charts.helm.sh/stable

# 提示：这里可以添加多个Repository仓库地址，例如：
helm repo add azure http://mirror.azure.cn/kubernetes/charts/
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</code></pre><p>（3）查看Repository仓库信息</p><pre><code class="powershell">helm repo list

# 输出的信息如下：
NAME      URL                                                   
stable    https://charts.helm.sh/stable                         
azure     http://mirror.azure.cn/kubernetes/charts/             
aliyun    https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</code></pre><p>（4）Helm仓库的其他操作。</p><pre><code class="powershell">#更新仓库
helm repo update
#删除仓库
helm repo remove aliyun</code></pre><p>（5）查看Helm的配置信息。</p><pre><code class="powershell">helm env

# 输出的信息如下：
HELM_BIN="helm"
HELM_CACHE_HOME="/root/.cache/helm"
HELM_CONFIG_HOME="/root/.config/helm"
HELM_DATA_HOME="/root/.local/share/helm"
HELM_DEBUG="false"
HELM_KUBEAPISERVER=""
HELM_KUBEASGROUPS=""
HELM_KUBEASUSER=""
HELM_KUBECAFILE=""
HELM_KUBECONTEXT=""
HELM_KUBETOKEN=""
HELM_MAX_HISTORY="10"
HELM_NAMESPACE="kubernetes-plugin"
HELM_PLUGINS="/root/.local/share/helm/plugins"
HELM_REGISTRY_CONFIG="/root/.config/helm/registry.json"
HELM_REPOSITORY_CACHE="/root/.cache/helm/repository"
HELM_REPOSITORY_CONFIG="/root/.config/helm/repositories.yaml"</code></pre><p>（6）在Repository仓库中搜索可用的Charts，搜索的结果如下图所示。</p><pre><code class="powershell">helm search repo

# 提示：默认情况下会搜索所有添加的Helm Repository仓库，也可以指定搜索某一个Repository仓库。
# 例如：下面的搜索命令只会搜索aliyun的Repository仓库。
helm search repo aliyun</code></pre><p><img width="723" height="188" referrerpolicy="no-referrer" src="/img/bVdmX0G" alt="image.png" title="image.png" loading="lazy"/></p><h2>三、 使用Helm管理Kubernetes</h2><p>要通过使用Helm管理Kubernetes，很重要的一步就是就是设置Helm管理的Kubernetes的环境变量。执行下面的命令：</p><pre><code class="powershell">export KUBECONFIG=/root/.kube/config 

# 提示：这一步非常重要，在文件“/root/.kube/config”文件中保存了Kubernetes集群的信息，
# 该信息可以保证Helm与Kubernetes进行通信。为了方便可以将这一步写到"/etc/profile”里。</code></pre><h3>3.1 【实战】使用Helm部署应用</h3><p>这里将使用Helm在Kubernetes中部署一个MySQL数据库的服务。下面是具体的演示步骤。<br/>（1）在Repository仓库中搜索可用的MySQL Charts，搜索的结果如下图所示。</p><pre><code class="powershell">helm search repo mysql</code></pre><p><img width="723" height="310" referrerpolicy="no-referrer" src="/img/bVdmX0H" alt="image.png" title="image.png" loading="lazy"/></p><p>（2）部署一个MySQL数据库的应用，执行命令：</p><pre><code class="powershell">helm install mysql-demo stable/mysql

# 输出的信息如下：
NAME: mysql-demo
LAST DEPLOYED: Thu Feb 10 06:33:49 2022
NAMESPACE: kubernetes-plugin
STATUS: deployed
REVISION: 1
NOTES:
MySQL can be accessed via port 3306 on the following DNS name from within your cluster:
mysql-demo.kubernetes-plugin.svc.cluster.local

... ...
... ...


# 提示：使用helm install命令至少需要两个参数：Release的名称和Charts名称。
# 以这里的命令为例：“mysql-demo”是Release的名称，而“stable/mysql”是Charts的名称。
# 另外可以使用“helm list”和“helm status mysql-demo”命令查询Release的状态信息。</code></pre><p>（3）查看部署的Pod、Deployment和Service信息，如下图所示。</p><pre><code class="powershell">kubectl get all

# 提示：这时候会发现Pod的状态一直是“Pending”的状态。</code></pre><p><img width="723" height="327" referrerpolicy="no-referrer" src="/img/bVdmX0L" alt="image.png" title="image.png" loading="lazy"/></p><p>（4）查看Pod的详细信息。</p><pre><code class="powershell">kubectl describe pod/mysql-demo-5d85fc7bd7-cwpk4

# 输出的信息如下：
Events:
 ... ...  Message
 ... ...  -------
 ... ...  pod has unbound immediate PersistentVolumeClaims

# 提示：从Message信息中可以看到Pod缺少PVC资源。</code></pre><p>（5）查看PVC的资源。</p><pre><code class="powershell">kubectl get pvc

# 输出的信息如下：
NAME         STATUS    VOLUME   CAPACITY
mysql-demo   Pending                    </code></pre><p>（6）查看Charts的详细信息。</p><pre><code class="powershell">helm show all stable/mysql

# 通过查看输出的信息，可以确定这里需要一个8G的PV资源。
... ...
## Persist data to a persistent volume
persistence:
  enabled: true
  ## database data Persistent Volume Storage Class
  ## If defined, storageClassName: &lt;storageClass&gt;
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is 
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS &amp; OpenStack)
  ##
  # storageClass: "-"
  accessMode: ReadWriteOnce
  size: 8Gi
  annotations: {}
... ...</code></pre><p>（7）创建MySQL的数据存储目录。</p><pre><code class="powershell">mkdir -p /mnt/mysql/data</code></pre><p>（8）创建文件“mysql-pv-volume.yaml”并输入下面的内容：</p><pre><code class="powershell">kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume-mysql
namespace: kubernetes-plugin
  labels:
    type: local
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/mysql/data"</code></pre><p>（9）创建PV资源。</p><pre><code class="powershell">kubectl apply -f mysql-pv-volume.yaml</code></pre><p>（10）查看PV和PVC的资源。</p><pre><code class="powershell">kubectl get pv,pvc

# 输出的信息如下：
NAME                                CAPACITY    ACCESS MODES  
persistentvolume/pv-volume-mysql    8Gi            RWO           
                                                            
NAME                                STATUS        VOLUME          
persistentvolumeclaim/mysql-demo    Bound        pv-volume-mysql 

# 提示：这时候PVC已经与PV成功绑定。</code></pre><p>（11）再次查看部署的Pod、Deployment和Service信息，如下图所示。</p><pre><code class="powershell">kubectl get all</code></pre><p><img width="723" height="309" referrerpolicy="no-referrer" src="/img/bVdmX0M" alt="image.png" title="image.png" loading="lazy"/></p><p>（12）卸载部署的MySQL应用。</p><pre><code class="powershell">helm uninstall mysql-demo</code></pre><h3>3.2 【实战】使用Helm创建自己的Charts</h3><p>用户可以使用Helm提供的Charts模板创建自己应用程序的Charts。这里将使用Helm创建一个自己的Nginx Charts，并部署到Kubernetes集群中。下面是具体的步骤：<br/>（1）生成Nginx Charts的模板。</p><pre><code class="powershell">helm create my-nginx</code></pre><p>（2）查看生成的Charts模板。</p><pre><code class="powershell">tree my-nginx/

# 输出的信息如下：
my-nginx/                    Charts包目录的名称 
├── charts                   依赖的子包目录，里面可以包含多个依赖的chart包
├── Chart.yaml               Charts的描述信息，如：Charts的名称、版本信息等。
├── templates                Kubernetes应用程序的配置模版目录
│   ├── deployment.yaml      Deployment的部署描述文件
│   ├── _helpers.tpl         公有库定义文件
│   ├── hpa.yaml
│   ├── ingress.yaml         Ingress的部署描述文件
│   ├── NOTES.txt
│   ├── serviceaccount.yaml  ServiceAccount的部署描述文件
│   ├── service.yaml         Service的部署描述文件
│   └── tests
│       └── test-connection.yaml
└── values.yaml

# 提示：用户可以基于这里生成的模板编辑其中的YAML文件完成相应配置即可，
# 重点是编辑Deployment、Service和Ingress的描述文件。</code></pre><p>（3）下面是一个最简单的Charts模板，这里只保留的必要的文件。</p><pre><code class="powershell">my-nginx/
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   └── service.yaml
└── values.yaml</code></pre><p>（3）编辑文件“values.yaml”，输入下面的内容：</p><pre><code class="powershell">deployname: my-nginx
replicaCount: 2
image:
  repository: nginx
  pullPolicy: IfNotPresent

# 提示：这里定义了Deployment的名称，副本数以及镜像的相关信息。</code></pre><p>（4）编辑文件“deployment.yaml”，输入下面的内容：</p><pre><code class="powershell">apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ .Values.deployname }}
  labels:
    app: my-nginx
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: my-nginx
  template:
    metadata:
      labels:
        app: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: {{ .Values.images.repository }}
        imagePullPolicy: {{ .Values.images.pullPolicy }}
        ports:
          - containerPort: 80

# 提示：在文件“deployment.yaml”中引用了文件“values.yaml”中定义的变量值。</code></pre><p>（5）编辑文件“service.yaml”，输入下面的内容：</p><pre><code class="powershell">apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  type: NodePort 
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:     
    app: my-nginx

# 提示：文件“service.yaml”也可以使用文件“values.yaml”中定义的变量值。</code></pre><p>（6）验证Charts中的各个文件格式是否正确，执行命令：</p><pre><code class="powershell">helm lint my-nginx/

# 输出的信息如下：
==&gt; Linting my-nginx/
[INFO] Chart.yaml: icon is recommended
1 chart(s) linted, 0 chart(s) failed</code></pre><p>（7）打包应用程序。</p><pre><code class="powershell">helm package my-nginx/

# 输出的信息如下：
Successfully packaged chart and saved it to: /root/my-nginx-0.1.0.tgz</code></pre><p>（8）试运行应用程序。</p><pre><code class="powershell">helm install --dry-run my-nginx my-nginx-0.1.0.tgz

# 输出的信息如下：
NAME: my-nginx
LAST DEPLOYED: Thu Feb 10 08:10:16 2022
NAMESPACE: kubernetes-plugin
STATUS: pending-install
REVISION: 1
TEST SUITE: None
HOOKS:
MANIFEST:
---
# Source: my-nginx/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
spec:
  type: NodePort 
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:     
    app: my-nginx
---
# Source: my-nginx/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
  labels:
    app: my-nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-nginx
  template:
    metadata:
      labels:
        app: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        imagePullPolicy: IfNotPresent
        ports:
          - containerPort: 80</code></pre><p>（9）在Kubernetes集群中部署应用程序。</p><pre><code class="powershell">helm install my-nginx my-nginx-0.1.0.tgz</code></pre><p>（10）查看创建的资源信息，如下图所示。</p><pre><code class="powershell">kubectl get all</code></pre><p><img width="723" height="304" referrerpolicy="no-referrer" src="/img/bVdmX0N" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[招聘决策新变革：判断型 AI 的应用与价]]></title>    <link>https://segmentfault.com/a/1190000047380586</link>    <guid>https://segmentfault.com/a/1190000047380586</guid>    <pubDate>2025-11-08 20:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>招聘决策新变革：判断型 AI 的应用与价值<br/>招聘季来临，海量简历与转瞬即逝的优秀候选人，让招聘工作的速度与精准度面临双重考验。传统招聘中逐份翻阅简历、手动安排面试、协调面试官时间等模式，已难以适配当下高效招聘的需求。</p><p>判断型 AI 的出现，推动招聘决策实现终极进化。近屿智能的 AI 得贤招聘官作为其中典范，并非简单的辅助工具，而是具备独立判断能力的招聘专家，已服务西门子中国、太平保险、中广核集团、阿里巴巴国际、招商银行、TCL 等上千家知名企业，同时获得浙江大学、上海交通大学等顶尖高校的认可。<br/>在面试精准度上，AI 得贤招聘官达到行业领先水平，其打分结果通过了效标效度与重测稳定信度的双重心理学指标考验，可直接作为招聘决策依据。第六代 AI 面试智能体 6.3 版本更是稳居国际领先地位，在各环节展现出高效精准的优势：一道题目可同步评估多项胜任力，让 HR 初筛与技术复试无缝衔接，评估效率提升 50% 以上；能根据候选人回答即时生成针对性追问，如同资深面试官般捕捉关键信息；自动抓取简历关键信息与模糊点，通过递进式提问杜绝信息造假、避免遗漏优质候选人；既覆盖沟通、协作等通用胜任力评估，也能针对编程、算法、工程、财务等专业领域精准出题，同时解放 HR 与专业面试官。<br/>在候选人体验方面，AI 得贤招聘官突破传统 AI 面试 “机械、生硬” 的局限，将 “拟人化交互” 落到实处。其智能交互能精准捕捉候选人的语速、情绪与潜台词，像真人 HR 一样引导候选人充分展现实力；无断点流畅体验无需手动点击 “开始 / 结束答题”，系统自动识别回答状态；沉浸式视觉体验大幅提升语音与口型匹配精度；支持多轮对话答疑，候选人可随时询问职位信息、公司福利等问题，让面试成为雇主品牌的加分项。<br/>同期发布的 AI 得贤人才寻访智能体，实现了人才寻访的全面自动化。该系统无需人工干预，即可独立完成从简历筛选、初步沟通、简历回收到系统同步的完整流程：30-60 秒即可完成初始化；能根据企业预设条件自动筛选简历；以模拟人类的语气自动发起沟通并互动；自动遍历所有未读消息并逐条个性化回复；在缺少简历信息时主动请求简历；收到简历后自动下载并上传至企业 ATS 系统，可将招聘效率提升 10 到 100 倍。<br/>判断型 AI 已能做出精准度堪比资深 HR 的招聘判断，为招聘工作提供了高效、精准、人性化的新解决方案，成为当下招聘领域的重要变革力量。</p>]]></description></item><item>    <title><![CDATA[购买CRM系统需要多少钱？CRM系统价格]]></title>    <link>https://segmentfault.com/a/1190000047380597</link>    <guid>https://segmentfault.com/a/1190000047380597</guid>    <pubDate>2025-11-08 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数字化时代，CRM（客户关系管理）系统已经成为企业管理客户关系、提升销售效率和优化客户体验的重要工具。无论是初创企业还是大型公司，CRM系统都能帮助企业更高效地管理客户数据、跟踪销售流程并制定精准的营销策略。然而，很多企业在选择CRM系统时，都会面临一个关键问题：购买一套CRM系统需要多少钱？<br/><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdmX1f" alt="" title=""/><br/>本文将以Zoho CRM为例，详细介绍其中国版的最新价格及功能，帮助企业了解如何选择适合自己的CRM方案。</p><h2>一、Zoho CRM简介</h2><p>Zoho CRM是全球领先的客户关系管理系统之一，专为中小型企业和大型企业设计。它以高性价比、灵活性和强大的功能著称，能够帮助企业管理客户数据、优化销售流程、提升客户满意度。Zoho CRM支持多语言、多终端操作，并与Zoho生态系统中的其他工具（如Zoho Campaigns、Zoho Desk）无缝集成，是一款非常适合中国企业的CRM系统。</p><h2>二、Zoho CRM中国版的最新价格</h2><p>Zoho CRM的价格体系非常灵活，企业可以根据自身需求选择不同的版本。以下是2024年最新的中国版价格（以人民币计）：</p><h2>1. 免费版</h2><p>价格：免费<br/>适用对象：初创企业或个人用户。<br/>功能特点：支持最多3位用户。提供基本的客户管理功能。包括联系人管理、销售漏斗跟踪和任务管理。<br/>适用场景：适合刚起步的企业或个人用户，用于体验CRM系统的基础功能。</p><h2>2. 标准版</h2><p>价格：¥100/用户/月（按年付费为¥1,000/用户/年）<br/>适用对象：小型企业。<br/>功能特点：支持销售流程自动化。提供自定义报表和仪表盘。包括工作流规则、评分规则和邮件模板。支持第三方集成。<br/>适用场景：适合需要基础CRM功能的小型企业，帮助管理客户数据和优化销售流程。</p><h2>3. 专业版</h2><p>价格：¥180/用户/月（按年付费为¥1800/用户/年）<br/>适用对象：中型企业。<br/>功能特点：包括所有标准版功能。支持库存管理（报价单、订单、发票等）。提供蓝图功能，用于定制化销售流程。支持多货币管理。提供更高级的自动化功能。<br/>适用场景：适合需要更复杂销售流程管理的企业，尤其是涉及库存和订单管理的行业。</p><h2>4. 旗舰版</h2><p>价格：¥280/用户/月（按年付费为¥2800/用户/年）<br/>适用对象：中大型企业。<br/>功能特点：包括所有专业版功能。提供高级定制功能（如自定义模块、按钮和页面）。支持多团队协作。提供AI助手Zia，用于预测销售趋势和客户行为。支持数据加密和高级安全功能。<br/>适用场景：适合需要高度定制化和安全性的企业，尤其是跨部门协作需求较高的公司。</p><h2>5. 超级版</h2><p>价格：¥420/用户/月（按年付费为¥4200/用户/年）<br/>适用对象：大型企业或跨国公司。<br/>功能特点：包括所有企业版功能。提供增强的AI功能（如销售预测、情绪分析）。支持多区域数据存储。提供专属客户支持和实施服务。<br/>适用场景：适合需要全面功能支持的大型企业，尤其是跨国业务或复杂销售流程的公司。</p><h2>三、Zoho CRM的功能亮点</h2><p>除了灵活的价格体系，Zoho CRM还以其强大的功能和易用性受到企业的青睐。以下是一些核心功能：</p><h2>1. 客户管理</h2><p>Zoho CRM能够帮助企业集中管理客户数据，包括联系人信息、沟通记录、购买历史等，确保销售团队能够随时获取客户的完整信息。</p><h2>2. 销售自动化</h2><p>通过自动化功能，企业可以简化销售流程，例如：</p><p>自动分配线索给销售人员。<br/>设置提醒跟进客户。<br/>自动生成报价单和订单。</p><h2>3. 数据分析与报表</h2><p>Zoho CRM提供强大的数据分析功能，帮助企业了解销售业绩、客户行为和市场趋势。企业可以通过自定义报表和仪表盘，实时监控关键指标。</p><h2>4. AI助手Zia</h2><p>Zia是Zoho CRM内置的AI助手，能够帮助企业预测销售趋势、分析客户情绪并提供智能化建议。例如，Zia可以提醒销售人员优先跟进高潜力客户。</p><h2>5. 与Zoho生态系统无缝集成</h2><p>Zoho CRM可以与Zoho Campaigns、Zoho Desk、Zoho Books等工具无缝集成，形成完整的业务管理闭环。此外，它还支持与第三方工具（如微信、钉钉、Google Workspace）集成。</p><h2>6. 移动端支持</h2><p>Zoho CRM提供功能强大的移动应用，销售人员可以随时随地访问客户数据、更新销售进展并与团队协作。</p><h2>四、如何选择适合自己的版本？</h2><p>在选择Zoho CRM的版本时，企业需要根据自身的业务需求和预算进行综合考量。以下是一些建议：</p><p>初创企业或个人用户：可以选择免费版，体验基础功能，待业务规模扩大后再升级到付费版本。<br/>小型企业：建议选择标准版，满足日常客户管理和销售流程自动化需求。<br/>中型企业：专业版是更好的选择，支持库存管理和更复杂的销售流程。<br/>大型企业或跨国公司：企业版或至尊版能够提供高度定制化和全面的功能支持，适合复杂业务场景。</p><h2>五、总结</h2><p>购买一套CRM系统需要多少钱？答案取决于企业的具体需求和预算。以Zoho CRM为例，其中国版的价格体系非常灵活，从免费版到至尊版，企业可以根据自身规模和功能需求选择最适合的方案。</p><p>小型企业：标准版（¥100/用户/月）是高性价比的选择。<br/>中型企业：专业版（¥180/用户/月）能够满足更复杂的业务需求。<br/>大型企业：企业版（¥280/用户/月）或至尊版（¥450/用户/月）提供全面的功能支持。<br/>Zoho CRM不仅价格合理，还提供强大的功能和易用性，是一款非常适合中国企业的CRM系统。如果您正在寻找一款高性价比的CRM工具，不妨试试Zoho CRM，它或许正是您业务增长的关键！</p>]]></description></item><item>    <title><![CDATA[数字孪生平台：让航天装备管理更智能、更可]]></title>    <link>https://segmentfault.com/a/1190000047380525</link>    <guid>https://segmentfault.com/a/1190000047380525</guid>    <pubDate>2025-11-08 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名航天装备运维工程师，我每天都要面对各种复杂的设备维护任务。从卫星地面站到发射场设备，从测控系统到在轨航天器，每个环节都需要精密的监控和维护。过去，我们依靠人工巡检和分散的系统进行管理，常常感到力不从心。直到我们引入了一套全新的数字孪生智慧运维平台，整个运维工作才迎来了革命性的改变。</p><h2>从"救火式"维修到"预防性"维护的转变</h2><p>记得去年在执行某重要卫星任务期间，一个关键的地面设备突然出现异常。按照以往的做法，我们需要组织专家团队连夜排查，往往要花费数天时间才能定位问题。而这次，通过智慧运维平台的实时监测和智能分析功能，系统在设备出现轻微异常时就发出了预警。<br/>平台通过整合设备运行数据、环境参数和历史维护记录，建立了完整的设备健康档案。当某个参数出现偏离正常范围的趋势时，系统会自动进行根因分析，并给出维护建议。那次事件中，我们根据系统提示，提前48小时完成了预防性维护，确保了任务的顺利执行。</p><h2>智能诊断：让故障排查更精准</h2><p>在航天装备运维中，最耗费时间的就是故障诊断。传统模式下，工程师需要查阅大量技术文档，比对各种参数，整个过程既繁琐又容易出错。现在，通过平台的智能诊断功能，我们实现了质的飞跃。<br/>平台内置的知识图谱能够理解设备之间的关联关系，当某个部件出现异常时，系统会自动分析可能的影响范围，并给出排查建议。有一次，某测控天线出现信号衰减，系统在几分钟内就定位到了是一个不起眼的连接器问题，而以往这样的问题往往需要数小时的排查。</p><h2>移动运维：随时随地掌握设备状态</h2><p>航天装备的运维工作往往需要在不同场地之间奔波。过去，工程师在外出巡检时很难及时获取设备的完整信息。现在，通过平台的移动端应用，我们可以随时随地查看设备状态、接收预警信息、处理维护任务。<br/>上周，我在出差途中收到系统推送的预警信息，显示某地面站的电源系统出现异常。通过手机端，我立即调取了相关数据，远程指导现场人员进行了初步处理。这种"移动办公"模式大大提升了我们的响应效率。</p><h2>数据驱动的决策支持</h2><p>更令人惊喜的是，平台还提供了强大的数据分析能力。通过对历史运维数据的挖掘，系统能够识别出设备运行的规律性特征，为我们的维护策略优化提供数据支持。<br/>例如，通过分析某型设备的故障数据，我们发现特定环境条件下某些部件的寿命会显著缩短。基于这个发现，我们调整了维护周期，既保证了设备可靠性，又避免了过度维护造成的浪费。</p><h2>展望未来：构建智慧运维新生态</h2><p>经过一年多的使用，这套智慧运维平台已经成为我们工作中不可或缺的助手。它不仅提升了运维效率，更重要的是改变了我们的工作模式——从被动响应到主动预防，从经验驱动到数据驱动。<br/>在这个航天事业快速发展的时代，我们需要更智能、更可靠的运维保障体系。如果您也在寻求运维管理的升级方案，我建议关注这类智慧运维平台的发展。</p>]]></description></item>  </channel></rss>