<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[AI巨头连夜亮剑，普通人如何抓住这波技术红利？ 曾经爱过的烤面包 ]]></title>    <link>https://segmentfault.com/a/1190000047459564</link>    <guid>https://segmentfault.com/a/1190000047459564</guid>    <pubDate>2025-12-08 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>阿里和华为同日放出技术大招，当行业格局被重新定义，掌握前沿技术不再只是工程师的特权。</p><p>阿里Ovis团队12月3日发布了文本渲染图像生成模型Ovis-Image，专门为高质量文本渲染优化，同时保持低计算成本。这一模型基于Ovis-U1构建，通过增加MMDiT参数和优化结构设计，采用以文本为核心的训练流程，结合大规模预训练与精心设计的后训练优化。</p><p>模型整体由三大核心组件精密咬合而成：作为大脑的Ovis 2.5多模态大模型负责构思；作为手的多模态扩散Transformer负责执行；来自FLUX.1-schnell的变分自编码器则负责视觉信息的压缩与解压，确保视觉特征的稳定性。</p><p>01 <br/>技术突破</p><p>在同一天，华为发布了 openPangu-R-7B-Diffusion，这一模型基于openPangu-Embedded-7B进行少量数据续训练，成功将扩散语言模型的上下文长度扩展至32K。</p><p>它在注意力机制上创新性地融合了自回归的前文因果注意力掩码，从架构层面解决了适配难题。训练策略上延续了BlockDiffusion的思路，但进行了关键优化，拼接带掩码的Block与无掩码的Context，展现出更强的适应性和效率。</p><p>阿里和华为在同一天发布多模态大模型重要进展，标志着AI技术竞赛进入新阶段。高质量文本渲染与长上下文处理能力的突破，正在重塑内容创作、设计、教育等多个行业的边界。</p><p>当技术门槛不断降低，应用场景却呈指数级增长，一个明显的趋势是：掌握这些技术不再局限于研究实验室里的少数专家。</p><p>02 <br/>变革</p><p>模型技术的进步正在产生连锁反应。Ovis-Image的低计算成本特性意味着中小企业和个人开发者也能使用高质量的文本渲染图像生成技术。</p><p>而华为的32K上下文长度突破，则为处理长篇文档、复杂对话和连续创作任务提供了可能。这两项进展共同指向一个方向：多模态AI正从炫技阶段走向实用化、普及化阶段。</p><p>行业变革的节奏超出了大多数人的预期。那些原本需要专业设计师数小时完成的工作，现在可能只需要几句文字描述；复杂的文档分析与生成任务，也能通过长上下文模型高效完成。</p><p>变革的核心逻辑在于，技术突破降低了专业门槛，但提高了应用广度。这意味着非技术背景的人士也有机会借助这些工具创造价值，前提是他们理解这些技术能做什么、不能做什么，以及如何将其融入工作流程。</p><p>03 <br/>技能</p><p>技术快速迭代的背景下，传统技能框架正在失效。过去，掌握单一技能可能足够应对职业挑战；现在，理解技术边界、能够跨领域整合的能力变得尤为重要。</p><p>市场对既懂技术原理又懂应用场景的人才需求急剧增加。企业需要的不再是纯粹的技术专家，而是能够将AI能力转化为实际解决方案的“桥梁型”人才。</p><p>AI技术普及带来了新的职业机会，但也对现有职业构成挑战。内容创作者需要学习如何与文本生成模型协作，设计师需要掌握图像生成工具的新特性。</p><p>产品经理则需要理解多模态技术的可能性与局限性，以设计出真正符合用户需求的产品。这些变化要求从业者保持持续学习的状态，不断更新自己的技能树。</p><p>04 <br/>学习</p><p>面对技术浪潮，系统化学习成为应对不确定性的最佳策略。专业课程的价值不仅在于传授知识，更在于提供经过验证的学习路径和实践机会。</p><p>随着阿里华为等技术巨头持续推进AI边界，行业对掌握多模态大模型应用能力的人才需求将持续增长。那些能够将最新技术转化为实际应用的专业人士，将在这个技术驱动的时代中获得独特优势。</p><p>系统化学习和实战训练为普通人提供了掌握前沿技术的可行路径。当技术门槛降低，理解并应用这些技术的能力将成为新的职业分水岭。行业变革的浪潮中，持续学习是抓住机会的最佳策略。</p><p>选择合适的学习路径，培养跨领域整合能力，普通人也能在这场技术革命中找到自己的位置。</p>]]></description></item><item>    <title><![CDATA[PyTorch推理扩展实战：用Ray Data轻松实现多机多卡并行 本文系转载，阅读原文
https]]></title>    <link>https://segmentfault.com/a/1190000047459515</link>    <guid>https://segmentfault.com/a/1190000047459515</guid>    <pubDate>2025-12-08 22:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>单机 PyTorch 模型跑推理没什么问题，但数据量一旦上到万级、百万级，瓶颈就暴露出来了：内存不够、GPU 利用率低、I/O 拖后腿，更别说还要考虑容错和多机扩展。</p><p>传统做法是自己写多线程 DataLoader、管理批次队列、手动调度 GPU 资源，这哥工程量可不小，调试起来也麻烦。Ray Data 提供了一个更轻量的方案：在几乎不改动原有 PyTorch 代码的前提下，把单机推理扩展成分布式 pipeline。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459517" alt="" title=""/></p><h2>原始的 PyTorch 代码</h2><p>典型的推理场景：模型加载、预处理、批量预测，一套下来大概长这样：</p><pre><code> import torch  
import torchvision  
from PIL import Image  
from typing import List

class TorchPredictor:  
    def __init__(self, model: torchvision.models, weights: torchvision.models):  
        self.weights = weights  
        self.model = model(weights=weights)  
        self.model.eval()  
        self.transform = weights.transforms()  
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  
        self.model.to(self.device)  
    def predict_batch(self, batch: List[Image.Image]) -&gt; torch.Tensor:  
        with torch.inference_mode():  
            batch = torch.stack([  
                self.transform(img.convert("RGB")) for img in batch  
            ]).to(self.device)  
            logits = self.model(batch)  
            probs = torch.nn.functional.softmax(logits, dim=1)  
             return probs</code></pre><p>处理几张图片完全没问题：</p><pre><code> predictor = TorchPredictor(  
    torchvision.models.resnet152,   
    torchvision.models.ResNet152_Weights.DEFAULT  
)

images = [  
    Image.open('/content/corn.png').convert("RGB"),  
    Image.open('/content/corn.png').convert("RGB")  
]  
 predictions = predictor.predict_batch(images)</code></pre><h2>大数据量</h2><p>图片数量从几张变成几万张、几百万张，情况完全不一样了。</p><p>内存撑不住，不可能把所有图一股脑塞进去；GPU 利用率上不去，多卡场景下吞吐量优化是个棘手的问题；万一跑到一半挂了怎么办？分布式部署能不能用上集群资源？还有个容易被忽视的点：数据加载的 I/O 往往才是真正的瓶颈。</p><p>自己从头写一套健壮的 pipeline 处理这些问题，少说得折腾好几天。</p><h2>Ray Data 的思路</h2><p>Ray Data 是个分布式数据处理框架，跟 PyTorch 配合得很好。关键是改造成本极低，原有代码基本不用大动。</p><p><strong>第一步：改造 Predictor 类</strong></p><p>把</p><pre><code>predict_batch</code></pre><p>方法换成</p><pre><code>__call__</code></pre><p>，输入从 PIL Image 列表改成包含 numpy 数组的字典：</p><pre><code> import numpy as np  
from typing import Dict

class TorchPredictor:  
    def __init__(self, model: torchvision.models, weights: torchvision.models):  
        self.weights = weights  
        self.model = model(weights=weights)  
        self.model.eval()  
        self.transform = weights.transforms()  
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  
        self.model.to(self.device)  
    def __call__(self, batch: Dict[str, np.ndarray]):  
        """Ray Data passes a dict batch with numpy arrays."""  
        # Convert numpy arrays back to PIL Images  
        images = [Image.fromarray(img_array) for img_array in batch["image"]]  
        with torch.inference_mode():  
            tensor_batch = torch.stack([  
                self.transform(img.convert("RGB")) for img in images  
            ]).to(self.device)  
            logits = self.model(tensor_batch)  
            probs = torch.nn.functional.softmax(logits, dim=1)  
              
            # Get top prediction  
            top_probs, top_indices = torch.max(probs, dim=1)  
        return {  
            "predicted_class_idx": top_indices.cpu().numpy(),  
            "confidence": top_probs.cpu().numpy()  
         }</code></pre><p>改动点说明：</p><pre><code>__call__</code></pre><p>替代</p><pre><code>predict_batch</code></pre><p>；输入类型从</p><pre><code>List[Image.Image]</code></pre><p>变成</p><pre><code>Dict[str, np.ndarray]</code></pre><p>；方法内部把 numpy 数组转回 PIL Image；输出改成 dict 格式；结果要搬回 CPU（数据在进程间的移动由 Ray 负责）。</p><p>还有个细节要注意，Ray Data 用 numpy 数组而非 PIL Image，因为 numpy 数组跨进程序列化效率更高。</p><p><strong>第二步：构建 Ray Dataset</strong></p><p>根据场景选择合适的创建方式，小数据集直接从内存构建：</p><pre><code> import ray  
import numpy as np  

ray.init()  

# Convert PIL Images to numpy arrays  
images = [  
    Image.open("/path/to/image1.png").convert("RGB"),  
    Image.open("/path/to/image2.png").convert("RGB")  
]  

# Create Ray Dataset from numpy arrays  
 ds = ray.data.from_items([{"image": np.array(img)} for img in images])</code></pre><p>中等规模数据集推荐从文件路径延迟加载：</p><pre><code> # Create dataset from paths  
image_paths = ["/path/to/img1.png", "/path/to/img2.png"]  
ds_paths = ray.data.from_items([{"path": path} for path in image_paths])  

# Load images lazily  
def load_image(batch):  
    images = [np.array(Image.open(path).convert("RGB")) for path in batch["path"]]  
    return {"image": images}  

 ds = ds_paths.map_batches(load_image, batch_size=10)</code></pre><p>生产环境首选</p><pre><code>read_images()</code></pre><p>，Ray 全权接管：</p><pre><code> # Most efficient - Ray handles everything  
 ds = ray.data.read_images("/path/to/image/directory/")  
 # or with specific files  
 ds = ray.data.read_images(["/path/img1.png", "/path/img2.png"])</code></pre><p><strong>第三步：跑分布式推理</strong></p><p>核心代码如下：</p><pre><code> weights = torchvision.models.ResNet152_Weights.DEFAULT  

# Distributed batch inference  
results_ds = ds.map_batches(  
    TorchPredictor,  
    fn_constructor_args=(torchvision.models.resnet152, weights),  
    batch_size=32,  
    num_gpus=1,  
    compute=ray.data.ActorPoolStrategy(size=4)  # 4 parallel actors  
)  
# Collect results  
results = results_ds.take_all()  
# Process results  
for result in results:  
    class_idx = result['predicted_class_idx']  
    confidence = result['confidence']  
     print(f"Predicted: {weights.meta['categories'][class_idx]} ({confidence:.2%})")</code></pre><p>搞定了。新版 Ray 里</p><pre><code>concurrency</code></pre><p>参数已经废弃，要换成</p><pre><code>compute=ActorPoolStrategy(size=N)</code></pre><p>这种写法。</p><p>改动总结：</p><p>自动分批，Ray 自己决定最优 batch size；</p><p>分布式执行，多 worker 并行跑；</p><p>GPU 调度，自动把卡分配给 worker；</p><p>流式处理，数据在 pipeline 里流动，不用一次性全加载进内存；</p><p>容错机制，worker 挂了会自动重试。</p><h2>生产环境</h2><p>RAY还可以直接读云存储的数据，S3、GCS、Azure Blob 都支持：</p><pre><code> # Read directly from S3, GCS, or Azure Blob  
ds = ray.data.read_images("s3://my-bucket/images/")  

results = ds.map_batches(  
    predictor,  
    batch_size=64,  
    num_gpus=1,  
    concurrency=8  # 8 parallel GPU workers  
 )</code></pre><p>多节点集群也可以用同一套代码，10 台机器还是 100 台机器，根本不用改：</p><pre><code># Connect to your Ray cluster  
ray.init("ray://my-cluster-head:10001")  

# Same code as before  
ds = ray.data.read_images("s3://my-bucket/million-images/")  
results = ds.map_batches(predictor, batch_size=64, num_gpus=1)</code></pre><h2>进阶用法</h2><p>每个 batch 都重新加载模型太浪费了，用 ActorPoolStrategy 让模型实例常驻内存：</p><pre><code>from ray.data import ActorPoolStrategy  

results = ds.map_batches(  
    TorchPredictor,  
    fn_constructor_args=(torchvision.models.resnet152, weights),  
    batch_size=32,  
    num_gpus=1,  
    compute=ActorPoolStrategy(size=4)  # Keep 4 actors alive  
)</code></pre><p>这样吞吐量提升很明显。</p><p>CPU、GPU 资源可以细调</p><pre><code>results = ds.map_batches(  
    TorchPredictor,  
    fn_constructor_args=(torchvision.models.resnet152, weights),  
    batch_size=32,  
    num_gpus=1,  # 1 GPU per actor  
    num_cpus=4,  # 4 CPUs per GPU worker  
    compute=ActorPoolStrategy(size=8)  
)</code></pre><p>推理完直接写到云存储：</p><pre><code>results.write_parquet("s3://my-bucket/predictions/")</code></pre><h2>几个容易踩的坑</h2><p>Ray Data 没法直接序列化 PIL Image 对象，得先转成 numpy 数组：</p><pre><code># ❌ This will fail  
ds = ray.data.from_items([{"image": pil_image}])  

# ✅ This works  
ds = ray.data.from_items([{"image": np.array(pil_image)}])  

# ✅ Or use read_images() (best)  
ds = ray.data.read_images("/path/to/images/")</code></pre><p>Ray 2.51 之后</p><pre><code>concurrency</code></pre><p>不能用了：</p><pre><code># ❌ Deprecated  
ds.map_batches(predictor, concurrency=4)  

# ✅ New way  
ds.map_batches(predictor, compute=ActorPoolStrategy(size=4))</code></pre><p>batch size 太大容易 OOM，保守起见可以从小的开始试：</p><pre><code># Monitor GPU memory and adjust batch_size accordingly  
results = ds.map_batches(  
    predictor,  
    batch_size=16,  # Start conservative  
    num_gpus=1  
)</code></pre><h2>实践建议</h2><p>batch size 可以从小往大试，观察 GPU 显存占用：</p><pre><code># Too small: underutilized GPU  
batch_size=4  

# Too large: OOM errors  
batch_size=256  

# Just right: depends on your model and GPU  
# For ResNet152 on a single GPU, 32-64 works well  
batch_size=32</code></pre><p>ActorPoolStrategy 处理 20 张图大概要 9.7 秒，而原生 PyTorch 跑 2 张图几乎瞬间完成。所以图片量少的时候 Ray Data 的启动开销反而不划算，所以这个方案是几百上千张图的场景才能体现优势。</p><p>Ray 自带 dashboard，默认在 8265 端口：</p><pre><code># Check Ray dashboard at http://localhost:8265  
ray.init(dashboard_host="0.0.0.0")</code></pre><p>代码中可以包一层 try-except 防止单个样本出错拖垮整个任务：</p><pre><code>def safe_predictor(batch: dict):  
    try:  
        return predictor(batch)  
    except Exception as e:  
        return {"error": str(e), "probs": None}</code></pre><p>跑之前加个计时，可以进行性能 profiling：</p><pre><code>import time  

start = time.time()  
results = ds.map_batches(predictor, batch_size=32)  
results.take_all()  
print(f"Processed in {time.time() - start:.2f} seconds")</code></pre><h2>总结</h2><p>适合的场景：数据集太大内存放不下；需要多卡或多机并行；长时间任务需要容错；不想自己写分布式代码。</p><p>不太必要的场景：图片量在百张以内；数据集轻松塞进内存；只有一张卡而且短期内不打算扩展。</p><p>Ray Data 的好处在于迁移成本低。PyTorch 代码改动很小，换个方法签名、把数据包成 Ray Dataset，就能换来从单卡到多机的无痛扩展、自动 batching 和并行优化、内置容错、云存储无缝对接等功能。</p><p>如果你下次写多线程 data loader 或者手动管理 GPU pool 之前，可以先考虑一下这哥方法，把分布式系统的脏活累活交给 Ray，精力留给构建模型本身。</p><p><a href="https://link.segmentfault.com/?enc=6tu94Y5qIzEgz0bmdfip9A%3D%3D.ElinHbZyxfnmA0S2bINuFVYCzHXam4TMb9oA9MRhApxK67fZtC8Q7mI3BJAoanaCcTcy%2BQbtaqFdsK5BkPjNiA%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/6320b9b6e1a14e0ba4c3384c83d06986</a></p><p>作者：Moutasem Akkad</p>]]></description></item><item>    <title><![CDATA[《Nginx在嵌入式场景的高效配置与运维逻辑》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047459530</link>    <guid>https://segmentfault.com/a/1190000047459530</guid>    <pubDate>2025-12-08 22:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>轻量型Nginx的核心魅力，正在于打破“功能全量加载”的固有思维，通过对核心功能的聚焦与非必要模块的精准剥离，在有限的资源边界内实现高效稳定的服务输出。这种轻量并非妥协式的功能删减，而是基于场景需求的理性取舍，它要求部署者既要深刻理解Nginx的底层架构，清楚每个模块的功能定位与资源消耗，又要精准把握业务的核心诉求，明确哪些功能是不可或缺的，哪些是可替代或暂时不需要的。在长期的技术实践中，这种轻量部署思路不仅解决了边缘场景的实际痛点，技术工具的本质—工具的价值不在于功能的堆砌，而在于与场景的高度适配，这也是轻量架构在边缘计算、嵌入式系统等领域愈发普及的核心原因，更是技术人在复杂环境中寻求高效解决方案的必然选择。</p><p>轻量型Nginx的环境搭建，核心逻辑是“按需构建、资源适配”，这一过程最能体现技术人的取舍智慧与底层认知。不同于传统部署中“一键安装全量依赖”的便捷操作，轻量部署需要从源头把控每一个资源占用环节。首先要明确服务的核心定位，是仅提供静态资源分发，还是需要简单的请求转发、访问控制，不同的定位直接决定了依赖模块的选择。例如，若仅用于嵌入式设备的本地静态配置文件分发，便无需加载SSL、反向代理等模块，仅保留最基础的HTTP核心模块即可，这样能最大程度减少内存占用。在操作系统选择上，轻量部署更倾向于采用Alpine Linux、BusyBox这类精简版系统，它们剔除了普通Linux发行版中大量不必要的预装组件与服务，自身占用资源极低，能为Nginx预留更多运行空间。在依赖安装环节，需要逐一甄别每个依赖包的作用，比如编译依赖中的gcc、make等工具，在编译安装完成后便失去了存在的意义，可及时通过系统自带的包管理工具清理，进一步压缩环境体积；而pcre、zlib等核心依赖，也需选择轻量版本或仅编译必要功能，避免因全量安装带来的资源浪费。这种搭建方式看似繁琐，实则是对资源利用效率的极致追求，每一步操作都围绕“最小资源占用、最大功能输出”的目标，在实践中不断调试、优化，最终形成一套适配资源受限场景的高效部署流程。</p><p>配置环节是轻量型Nginx发挥效能的关键，其核心思路是“功能聚焦、性能适配”，拒绝无意义的配置项堆砌。轻量配置的本质是让每一项设置都有明确的场景指向，每一个参数都能对应具体的性能优化目标，避免因盲目照搬通用配置导致的资源浪费。在实践中，首先要基于业务场景确定配置重心：如果是静态资源服务，配置重点应放在资源缓存策略与传输效率上，比如根据资源类型设置差异化的缓存时长，静态图片、CSS等可设置较长缓存周期，而动态生成的静态文件则缩短缓存时间，同时优化传输模式，启用压缩功能减少数据传输量，降低带宽占用；如果是简单的请求转发场景，则需聚焦于连接管理与响应速度，合理设置连接超时时间，避免无效连接长时间占用资源，同时根据CPU核心数调整工作进程数，让每个进程都能充分利用硬件资源，避免进程过多导致的调度开销。此外，轻量配置还需充分考虑运行环境的资源上限，比如根据可用内存大小设置并发连接数，若内存仅有512MB，便不宜将并发连接数设置过高，否则会导致内存溢出；根据CPU性能调整请求处理模型，在单核CPU环境下采用单进程多线程模型，在多核环境下则可采用多进程模型，实现资源与性能的最佳平衡。这种配置思路要求部署者不仅要熟悉Nginx的配置选项，更要具备对系统资源的敏感度与把控力，在实践中通过反复测试、调试，找到最适合当前场景的配置方案，让轻量Nginx在有限资源下发挥出最优性能。</p><p>轻量型Nginx的动态适配能力，是其在复杂边缘场景中保持竞争力的核心优势，这种适配并非依赖复杂的插件或工具，而是源于对Nginx核心机制的灵活运用与对场景变化的快速响应。在实际部署中，业务需求往往会随时间推移发生变化，比如最初仅需提供静态资源服务，后续可能需要新增简单的API转发功能；或者原本低流量的服务，因业务推广出现阶段性流量峰值。此时，轻量环境的配置调整需要遵循“最小改动、精准扩容”的原则，在保留原有核心配置的基础上，按需添加必要模块与设置，避免因全面重构导致的资源浪费与稳定性风险。例如，当需要新增API转发功能时，无需重新编译安装Nginx，可通过加载轻量的反向代理模块，仅配置必要的转发规则与健康检查参数，即可实现功能扩展，同时避免加载其他无关模块；当流量出现阶段性增长时，可通过调整工作进程数、连接池大小等参数，在不增加额外硬件资源的前提下提升处理能力，若流量峰值持续时间较短，还可设置临时配置文件，峰值过后自动恢复原配置，避免资源长期占用。在实践中，我曾遇到过边缘网关因业务扩展需要新增访问控制功能的场景，最初考虑加载复杂的权限管理模块，但测试后发现会增加近30%的内存占用，后来通过利用Nginx核心配置中的基础规则，结合IP白名单与简单的请求头校验，同样实现了精准的访问控制，且资源占用几乎无明显增加。这种动态适配的思路，核心是“以最小的资源代价满足变化的需求”，它要求部署者深刻理解Nginx的配置逻辑与模块特性，能够快速定位功能扩展的核心关键点，在实践中不断积累调整经验，形成一套灵活高效的适配方法论。</p><p>长期运维中的“轻量坚守”，是保障Nginx环境持续高效运行的关键，这种坚守并非墨守成规，而是在日常维护中始终保持对资源占用与功能冗余的警惕。轻量环境的运维核心是“持续优化、动态清理”，因为即使初始配置再精简，随着业务迭代与环境变化，也可能出现冗余配置、无效模块占用资源的情况。在日常运维中，我会定期对Nginx环境进行“资源体检”，重点关注内存占用、CPU使用率、连接数等核心指标，通过系统自带的监控工具（文字描述功能，无代码）观察资源变化趋势，若发现内存占用持续上升，会逐一排查是否存在未清理的临时配置、冗余模块或无效连接。例如，曾在一次运维中发现，某边缘设备的Nginx内存占用在一周内增长了20%，排查后发现是之前测试时添加的日志模块未及时禁用，该模块会实时记录详细日志，导致内存持续累积，禁用后内存占用迅速恢复正常。日志管理也是轻量运维的重点，默认的日志配置会记录大量冗余信息，不仅占用存储空间，还会增加IO开销，因此我会根据实际需求设置日志级别，仅保留错误日志与核心访问日志，同时配置日志轮转策略，定期压缩归档旧日志，避免日志文件过大占用资源。此外，对于不再使用的模块，我会及时通过编译工具卸载，避免其在后台占用系统资源，同时定期更新Nginx版本，但仅选择轻量版更新，避免新版本中新增的冗余功能增加资源负担。这种运维思路，将“精简高效”的理念贯穿于环境生命周期的每一个环节，通过持续的监控、清理与优化，让轻量Nginx始终保持最佳运行状态，这也是从长期实践中总结出的运维智慧。</p><p>轻量型Nginx的部署与配置，本质上是一场对技术本质的回归，它剥离了冗余的功能外壳与复杂的配置套路，让工具回归到“解决核心问题”的原始定位。在这个过程中，我所积累的不仅是具体的操作方法，更是一种“精准适配”的技术思维—无论是环境搭建时的依赖取舍，还是配置优化中的参数调整，亦或是运维过程中的资源管控，核心都是围绕“场景需求”与“资源上限”进行动态平衡。这种思维不仅适用于Nginx的轻量部署，更可以迁移到其他技术工具的使用中，比如在边缘场景部署数据库时，同样可以采用“核心功能保留、冗余模块剥离”的思路，选择轻量型数据库版本；在开发嵌入式应用时，遵循“最小资源占用”的原则设计架构。技术的发展往往是从“复杂”到“简单”的循环，当我们习惯了各种功能强大的工具与框架后，反而容易陷入“功能依赖”的误区，而轻量部署的实践让我明白，真正高效的技术方案，往往是最贴合场景的方案，它不需要华丽的功能堆砌，只需要精准解决核心问题。</p>]]></description></item><item>    <title><![CDATA[《TXT与专用HSTS记录的浏览器安全通信轻量配置指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047459534</link>    <guid>https://segmentfault.com/a/1190000047459534</guid>    <pubDate>2025-12-08 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在网络安全架构向轻量化、多场景适配演进的实践中，我们面对的并非复杂的企业级服务集群，而是静态博客、边缘计算节点、无服务器架构应用等资源受限或权限受限的场景，传统的服务器响应头配置方式往往受限于环境约束—比如静态站点无法自定义后端响应头，多域名管理场景下逐一配置服务器过于繁琐，边缘节点缺乏复杂配置的硬件支撑。而通过TXT记录或专用HSTS记录告知浏览器的方式，恰恰以“轻量无侵入”的特性，打破了这些场景限制，它无需深度改造服务架构，无需占用过多系统资源，仅通过简洁的记录配置，就能实现对浏览器访问行为的精准安全引导。这种配置思路的核心价值，在于将安全策略从服务器环境中剥离，转化为可跨场景复用的“信任凭证”，无论是静态资源分发、临时站点部署，还是多域名统一安全管理，都能快速落地。在长期的安全实践中，这种轻量配置方式不仅解决了多类场景的安全痛点，更让我深刻意识到，安全配置的本质是“信任的高效传递”—浏览器与服务器的安全通信，无需依赖复杂的技术堆砌，只需通过标准化的记录约定，就能建立起可靠的信任关系，这也是轻量安全架构在当下多场景部署需求中愈发重要的核心原因。</p><p>要真正掌握这种配置方式，必须先穿透技术表象，理解HSTS的底层信任机制，它的核心并非简单的HTTP强制跳转，而是通过浏览器的本地缓存形成“安全访问记忆”，从根源上阻断非加密访问的可能。当浏览器首次获取到HSTS记录后，会将对应的域名标记为“强制HTTPS访问”对象，并在本地缓存该策略一段时间，在此期间，所有针对该域名的HTTP请求都会被浏览器自动拦截并转换为HTTPS请求，无需等待服务器响应，既提升了访问安全性，又减少了跳转带来的性能损耗。而TXT记录与专用HSTS记录的核心差异，在于信任凭证的存储与传递载体：专用HSTS记录依赖服务器的HTTP响应头，当浏览器发起首次HTTP请求时，服务器通过响应头将安全策略传递给浏览器，适用于具备后端配置权限的场景，生效速度快且兼容性覆盖主流浏览器；TXT记录则将安全策略存储于域名解析系统中，浏览器在解析域名时会同步获取该记录，无需依赖后端服务的响应，这种特性使其成为静态站点、无服务器架构、嵌入式设备等无法自定义响应头场景的最优解。在实践中，这种差异直接决定了配置方案的选择逻辑—有后端配置权限时，优先选择专用HSTS记录以保障兼容性；无配置权限或场景受限，TXT记录则成为安全加固的关键路径，这种“因场景制宜”的选择思维，正是安全技术实践中最核心的底层逻辑。</p><p>通过TXT记录配置HSTS的实践过程，需围绕“规范定义、精准配置、验证闭环”三个核心环节层层推进，每个环节都需兼顾行业标准与场景适配性，避免因细节疏漏导致配置失效。首先是记录内容的规范定义，虽然不能涉及代码，但需明确核心要素的逻辑：记录类型需选择域名解析系统支持的专用安全类型，确保浏览器能够识别；内容需包含四项关键策略—HTTPS强制生效的有效期、是否将子域名纳入策略范围、是否允许浏览器预加载该策略、是否禁用HTTP降级访问，这些要素的设置直接影响安全效果与业务可用性。例如，有效期的设置需要平衡安全性与灵活性，过长可能导致配置错误后难以快速修正，过短则会频繁触发策略重新验证，建议根据业务稳定性调整，静态站点可设置较长有效期，频繁迭代的站点则适当缩短；子域名策略需根据实际需求选择，若多子域名统一管理，可开启子域名包含功能，若仅需保护主域名，则关闭该选项。其次是解析配置环节，需登录域名管理平台，找到DNS解析模块，新增一条TXT记录，准确填入定义好的策略内容，同时注意记录的“主机记录”字段设置，确保覆盖目标域名及所需保护的子域名范围，配置完成后需等待DNS解析全球同步，不同服务商的同步周期从几分钟到几小时不等，期间需避免频繁修改配置。最后是验证闭环环节，解析生效后，可通过两种方式确认配置效果：一是直接访问目标域名的HTTP地址，观察浏览器是否自动跳转至HTTPS，且地址栏显示安全锁标识；二是使用行业认可的在线检测工具，输入域名后查看HSTS策略的识别状态，确认策略中的各项参数是否被正确解析。在实践中，验证环节往往需要多次调试，比如排查记录内容是否存在格式偏差、解析是否完全同步、浏览器缓存是否影响首次验证结果等，这些细节的把控直接决定了配置的成功率，也是技术实践中积累经验的关键过程。</p><p>专用HSTS记录的配置逻辑，更侧重于“后端响应头的精准管控”，适用于具备服务器配置权限的场景，其核心优势在于生效即时性与浏览器兼容性，是企业级服务、动态站点等场景的首选安全方案。与TXT记录不同，专用HSTS记录无需依赖DNS解析，而是通过服务器在处理HTTP请求时，主动在响应头中携带安全策略信息，浏览器首次接收后便缓存该策略，后续访问直接生效。配置的核心环节在于服务器响应头的自定义设置，需根据服务器类型调整配置思路：静态服务器可通过修改配置文件，全局启用HSTS响应头；应用服务器可在应用代码中统一配置响应头，或通过中间件实现策略分发。无论哪种方式，都需确保响应头的名称与内容格式符合行业标准，策略参数与TXT记录保持一致，包括有效期、子域名包含、预加载权限等关键信息。在实践中，配置时需注意“灰度过渡”原则，避免直接启用严格策略导致业务异常：首次配置可设置较短的有效期（如几小时），同时关闭预加载功能，测试主流浏览器的兼容性与业务访问稳定性，确认无异常后，再逐步延长有效期并开启预加载；若业务存在特殊需求，需临时允许HTTP访问，可通过缩短有效期快速调整策略，待需求结束后恢复严格配置。此外，专用HSTS记录支持将域名提交至浏览器厂商维护的HSTS预加载列表，提交通过后，浏览器在首次访问前就已内置该域名的安全策略，无需等待首次HTTP请求，进一步提升安全防护的即时性，尤其适用于用户基数大、安全需求高的场景。</p><p>无论是TXT记录还是专用HSTS记录，配置后的持续优化与动态监控，都是保障安全策略长期有效、适配业务变化的关键，核心在于建立“策略迭代-效果监控-问题修复”的闭环机制。安全策略并非一成不变，需根据业务发展与安全需求动态调整：当域名新增子域名时，需及时更新HSTS记录，将新子域名纳入策略范围，避免出现安全防护盲区；当业务架构调整，如从动态站点转为静态站点，需同步切换配置方案，从专用HSTS记录改为TXT记录；当安全漏洞出现时，可通过缩短有效期快速更新策略，关闭存在风险的配置项。监控环节需聚焦两个核心维度：一是浏览器兼容性监控，定期测试主流浏览器及不同版本的访问情况，确认策略在各类环境中都能正常生效，避免因浏览器版本差异导致策略失效；二是策略执行效果监控，通过分析服务器访问日志，统计HTTP请求的转换率，判断是否存在未被拦截的HTTP请求，同时关注是否有因策略配置导致的访问异常，如HTTPS证书失效时，策略会导致用户无法访问，需及时预警并处理。在实践中，可结合安全监控工具，定期扫描域名的HSTS配置状态，自动检测策略参数是否合规、是否存在配置漏洞，同时建立配置变更记录台账，每次调整后及时记录原因与效果，便于后续追溯与优化。这种“动态优化+持续监控”的思路，体现了安全防护的“主动防御”理念，只有让策略始终适配业务与安全的变化，才能实现长期稳定的安全保障。</p><p>通过TXT记录或专用HSTS记录告知浏览器的配置方式，本质上是轻量安全架构的典型实践，它剥离了传统安全配置的复杂流程与资源消耗，以“最小化干预”实现“最大化安全”，完美适配了当下多场景、轻量化的部署需求。在这个过程中，积累的不仅是具体的操作方法，更是一种“场景化安全”的技术思维—安全配置不应是标准化的模板套用，而应是基于场景特性的精准适配，不同的业务架构、不同的权限边界、不同的用户群体，都需要匹配对应的安全方案。这种思维不仅适用于HSTS配置，更可以迁移到其他安全技术的实践中，比如静态站点的跨域安全配置、边缘节点的访问控制策略等，核心都是“以最小成本实现核心安全需求”。</p>]]></description></item><item>    <title><![CDATA[Pixelmator Pro for Mac v3.5.4.dmg 安装方法｜简单易懂 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047459382</link>    <guid>https://segmentfault.com/a/1190000047459382</guid>    <pubDate>2025-12-08 21:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p>Pixelmator Pro 是 Mac 上一款很受欢迎的图片编辑软件，界面清爽、操作简单，功能却挺全，像修图、调色、抠图、加特效都能搞定。它支持图层、蒙版、矢量图形这些专业玩法，但对新手也很友好，不用学太多复杂操作就能出效果</p><ol><li>先下好安装包</li></ol><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=05q%2FtQohWeq99RjWDSxWKA%3D%3D.wC0xUb3ZYcKXW6RkoQVcRoAu%2Fxdlgkds81eb01oDDqqU8%2BKEpOFVrt43rbLi8v%2Fl" rel="nofollow" title="https://pan.quark.cn/s/537dce9946a1" target="_blank">https://pan.quark.cn/s/537dce9946a1</a>，把 <code>Pixelmator Pro for Mac v3.5.4.dmg</code>下载到电脑里，记住放哪了，别等会儿找不着（一般默认在“下载”文件夹）。</p><h3>2. 打开 DMG 文件</h3><p>找到刚下载的 <code>.dmg</code>文件，双击它！这时候会弹出一个新窗口，里面能看到 Pixelmator Pro 的图标和一个箭头（或者叫“应用程序”文件夹的快捷方式）。</p><h3>3. 拖图标到“应用程序”</h3><p>重点来了：直接按住 Pixelmator Pro 的图标，往右边那个“应用程序”文件夹的快捷方式上拖——拖过去松开鼠标，等它自己复制完（进度条跑完就OK）。</p><h3>4. 等复制完，关掉窗口</h3><p>复制好了之后，把刚才弹出的 DMG 窗口关掉就行，不用留着。</p><h3>5. 打开软件试试</h3><p>现在去“启动台”（屏幕底部火箭图标）找 Pixelmator Pro，点一下打开。第一次开可能会提示“是否信任”，选“打开”就行（Mac 有时候对新软件会多问一句，正常操作）。</p><p>​</p>]]></description></item><item>    <title><![CDATA[腾讯新闻APP的消息推送Push架构技术重构实践 JackJiang ]]></title>    <link>https://segmentfault.com/a/1190000047459419</link>    <guid>https://segmentfault.com/a/1190000047459419</guid>    <pubDate>2025-12-08 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文由腾讯技术团队颜勇分享，原题“腾讯新闻PUSH架构升级之路”，有修订和重新排版。</p><h2>1、引言</h2><p>68 万行代码精简到8.6 万；Golang 重写大部分 C++模块；解决过度微服务化问题…… 这是新闻 PUSH 架构团队取得的技术收益。PUSH 是腾讯新闻精品资讯的重要分发途径，也是新闻 App 重要的促活手段。作为 PUSH 架构团队，我们一方面在积极支持好新闻护盘，同时也在对 PUSH 架构进行不断的升级与进化，以持续提升 PUSH 系统的稳定性与质量、研发效率，同时持续减少运营成本。<br/>本文主要分享的是腾讯技术团队近年来对腾讯新闻消息推送PUSH系统做的架构优化和技术实践。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459421" alt="图片" title="图片"/></p><h2>2、Push平台介绍</h2><p>2.1 概述PUSH 是腾讯新闻内容重要的分发渠道，新闻 PUSH 平台承担着将新闻资讯触达到新闻用户、满足用户及时获取精品资讯的需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459422" alt="图片" title="图片" loading="lazy"/></p><p>总体上，新闻 PUSH 链路分为下面两部分。</p><p>2.2 PUSH触发</p><p>按触发方式的不同，新闻 PUSH 分为三类：<br/>1）人工 PUSH：运营在 push cms 系统指定要发送的文章、要触达的人群包，人工触发push发送；这类 PUSH 目前主要用于推送热点事件/热点资讯等；<br/>2）自动化 PUSH：周期性地给用户计算他可能感兴趣的内容，这类推送由后台自动触发；<br/>3）功能性 PUSH：由业务系统触发，主要是为了实现一些业务功能通知，比如评论通知、关注通知等。</p><p>2.3 PUSH下发</p><p>对于所有 PUSH 触发 的PUSH 进行调度（包括避让、打散和频控等）和触达（通过自有通道或厂商通道推送给用户）。新闻业务对新闻 PUSH 平台最重要的要求是：</p><p>1）要保证精品咨讯触达的及时性：新闻 PUSH 最重要的是要体现“新”，因为腾讯新闻用户有及时获取热点/突发资讯的诉求，用户经常有这样的体感，有热点突发事件时，所有 App 都会尝试第一时间向用户发起推送，用户大概率会点击收到的第一个推送。在了解了相关热点事件后，对于后续其它 App 的推送，对用户而言就没信息量了，大概率会被忽略，甚至可能会被用户视为一种打扰，影响用户体验。从我们实验数据来看，当P USH 下发延迟降低 50%，PUSH 点击量会提升 10%。所以新闻 PUSH 一直以来的目标是：热点资讯需要第一时间触达给用户，要做到“全网首推”。</p><p>2）要保证推送的用户体验和较好的拉起效率：PUSH 是新闻重要的促活手段，需要有较好的促活效率，这要求保证用户较好的推送体验，因为用户如果感觉推送体验不好，用脚投票，把 App 的 PUSH 系统开关给关了，这对 PUSH 而言就基本上就永远丧失了给这个用户推送的机会了。</p><p>这就要求要尽量保证在合适的时间点给推送用户感兴趣的内容，推送要有合理的频次，相邻 PUSH 之间要有合理的时间间隔，推送内容要做合适的打散。</p><p>其实这两个要求其实在一定层面上是有冲突的：<br/>a.如果要保证推送的及时性，就要求尽量减少计算，拿到消息消息后无脑推到消息通道，这个肯定最快；<br/>b.如果要保证良好的推送用户体验，就需要做很多的判断、考量和计算，这些考虑越多就需要做更多的计算和 io 操作，会影响推送的及时性；最近几年，业务成本的考虑也是 PUSH 关注的重点，需要削减使用的机器和资源，就要求用更少的机器如何发得更快更好。<br/>总结而言，之前新闻 PUSH 业务的突出问题主要有两个方面，请继续往下阅读。</p><h2>3、Push平台问题1：推送速度慢</h2><p>我们团队从 2022 年年中开始接手新闻 PUSH 平台。交接工作刚启动，就遇到了一次 S 级热点事件——一个国际级突发新闻。那天晚上，全网用户都在密切关注它的最新进展。这个事件有两个特点：热度极高、且并非完全突发——早在一个月前就已经有明确预告，因此运营部门提前布置了应急预案。我们刚接手系统时，对整个下发链路还不够熟悉，只能凭直觉扩容机器，希望能抗住峰值。结果现实很快给了我们一记当头棒喝。当晚，很多内部同事都装着多个新闻 App，一眼能看到谁家的推送更快。那晚我们的延迟问题非常明显，甚至有用户在热点过去一个多小时后才收到通知。事后有专门的评测团队做了分析，指出“PUSH 下发耗时过长，高活用户 P90 均值达 20 分钟”，报告还发到了高层群里——对我们来说，那无疑是一次刻骨铭心的教训。</p><h2>4、Push平台问题2：开发效率和问题排查效率低</h2><p>之前 PUSH 链路特别长，新闻 PUSH 内部有 30+ 个模块，同时还依赖其它两个跨业务团队。经常一个需求开发要改多个模块，要团队几个人一起开发，约定交互协议，开发后再联调测试，在多个模块起联合实验；然后还得给中台提需求，然后匹配中台的排期后，才能完成需求上线；这一系列操作就拉长了 push 需求的leadtime。线上有 case 时，问题排查也需要串联多个模块，关联多个模块数据，甚至需要跨部门拉上其它这边来一起来排查，排查效率非常低。push case 非常多，比如用户为什么收到了/没收到某条 push 之类的典型 case，之前需要关联链路20来个模块的日志，还要联合中台一起排查，每次 case 排查时间都在天级；之前在case 排查上，每天都耗费我们大量的人力。既要持续提升 PUSH 触达的及时性、又要持续提升推送的用户体验和拉活效率，还要持续降低运营成本，客观而言，在技术上是一个较大的挑战。本文主要详述，我们如何通过技术架构升级来支撑这个既要&amp;又要&amp;还要的目标。</p><h2>5、老Push架构的问题梳理</h2><p>5.1 模块链路过长，内耗过多</p><p>一条快速PUSH，从推送内容过审后，到最终发出去，最长要经过18个模块，另外还需要经过中台多个模块。一条待推送的数据最多要经历 17 次内部 rpc 转发，多个模块之间腾挪流转，各种网络 rpc，各种内耗，肯定发得慢。一个最典型的例子：原架构有个模块叫scheduler，它主要负责决定一个push该不该发，直观上感觉它里面应该囊括了各种过滤策略，但是原架构做成了多个微服务。scheduler 模块里本身有一些过滤逻辑，另外有一个叫做 filter 的模块，专门负责品牌、开关等硬规则过滤；另外有一个叫做 policy 的模块，专门负责配额等软规则过滤；所有过滤规则都通过后，进入一个叫做 channer 模块，就决定下这次推送走哪个通道；然后又走到一个叫 worker 的模块里，而它只做对接下游中台的协议适配。总体上看，原链路就是过度微服务化了：1）模块多会导致数据流转的低效，模块间网络 rpc 会浪费处理耗时；2）其次会影响迭代效率，模块数不是越多越好，因为经常一个需求需要改多个模块，做多次上线；3）同时模块过多也对联调&amp;测试效率，影响线上 case 排查效率。这就违反了“模块内高内聚，模块间低耦合”的架构设计原则，进而会影响业务迭代效率。</p><p>5.2 依赖服务有瓶颈</p><p>上文提到的 S 级热点事件时，我们将下发服务机器扩了一倍，但是下发速度并没有提升，说明瓶颈不在下发服务本身下，而是在依赖服务上；通过链路debug，我们定位到了链路瓶颈：号码包拉取。在发送人工 push，运营会指定受众人群包（几百万到几亿不等），这时候需要分页拉取该号码包数据进行处理。之前老架构使用了底层平台的人群包服务，新闻所有 push 人群包都上传到了该人群包服务，当发送指定人群包，需要请求平台侧接口分页拉取人群包数据，当时因为平台侧人群包功能实现比较复杂，能支持一些比较高级的能力，因此这个分页接口耗时比较长。但其实我们只用到了最简单的数据分页的功能，完全可以采用更简单的实现方案，以减少接口耗时。</p><p>5.3 链路稳定性不好</p><p>5.3.1）容错能力差：之前链路基本无容错能力，发生了过一次因上游未按约定协议跟我们请求交互，导致我们服务挂了半天，是一次典型的 P0 级事故。</p><p>5.3.2）缺少节点自动故障转移：scheduler 负责 push 调度，原架构为了提升处理效率，scheduler 里做了本地缓存；为了避免缓存失效，起了一个服务 dispatch 消费触发侧生产的待推送的消息，然后按照用户设备号一致性哈希来 sharding，通过 rpc 请求对应的 scheduler，scheduler接受到请求后，塞入到它本地的内存队列里，如果队列满了就直接丢弃。它原来存在有这些问题：dispatch无脑往下游转发，sharding规则非常僵硬，一个用户的push一定要打到某个节点，未做故障转移；当某节点异常满载时，dispatch还是会往这个节点打，导致丢消息或者是 push发送得慢。而且当节点满载时，有限的cpu还需要耗费在rpc解包、无法插入内存队列而丢弃之类的无用消耗上。</p><p>5.4 链路处理无优先级区分</p><p>运营人工发的 PUSH 和自动化 PUSH 都使用同一个下发链路，热点突发事件资讯多由运营人工发送，而自动化 PUSH 多发一些用户可能感兴趣的内容，其实它对于推送速度并没那么敏感；当有人工推送的热点突发内容时，自动化 PUSH 会和它一起争抢有限的链路资源。另外，在链路总吞吐量一定的情况下，其实处理顺序可以调整，让链路资源有限保证人工推送的热点突发内容的发送；</p><p>5.5 技术栈不统一</p><p>之前 push 下发链路有 C++/Go 两种技术栈，技术栈不统一不利于代码复用，影响需求迭代效率。push下发链路本质上是一个高 io 型的流程，其实可以完全可以统一到 Golang 技术栈。</p><p>5.6 链路测试效率低</p><p>push 链路业务逻辑比较多，在日常密集业务需求迭代中，新功能我们可以在线上通过构造对应的功能 case 来进行冒烟测试，但是比较难评估是否影响了线上已有的业务逻辑。之前缺乏有效的回归测试手段，由于担心影响线上业务指标，为了验证是否影响线上已有业务逻辑，我们大的修改都会开比较长的小流量实验验证，比如我们在做调度架构升级时，开了一个近两个月的小流量实验，测试效率比较低也会导致需求迭代效率比较低。</p><h2>6、新Push架构优化1：消息通道自建</h2><p>之前新闻 PUSH 依赖于平台侧的消息通道，业务侧主要负责 PUSH 调度，即业务侧决定触发和过滤，平台侧负责 PUSH 触达给用户终端。由于 PUSH 是新闻增长护盘的重点方向，有较频繁的业务迭代，对底层消息通道我们有较多的业务需求，在业务迭代过程中我们发现平台侧需求 leadtime 比较长，无法满足业务侧迭代效率的要求；在经平台侧这边商量且同意后，我们完成新闻push消息通道的自研，直接对接厂商推送并搭建了长链接通道，实现了 push 全链路在业务侧的全闭环。我们在自建 push 消息通道时，对原来的架构做了重写：1）精简链路，模块整合，减少系统复杂度：去掉我们不关心的无用功能，将原链路15个模块，代码 68 万行整合为了6个模块，代码共8.6万行；通过代码精简能减少系统复杂度，有助于提升业务迭代效率；同时能避免模块之间的rpc通信开销，提升链路处理效率。2）客户端/服务端交互接口整合，提升数据通信成功率：以前 PUSH 注册依赖于注册&amp;绑定&amp;上报三个接口请求，任何一次请求出错，push 注册就会失败；我们在新流程里将注册&amp;绑定&amp;上报需要的所有数据，都一起传给新接口，由服务端在一个接口里实现注册、绑定和上报；将注册成功率从90%提升到了99.9%。3）与新闻技术技术架构保持统一：将原架构发现/rpc技术栈的基础组件升级为腾讯新闻自用的基础组件，尽量使用我们熟练使用的技术栈，以提升业务开发&amp;运维效率。4）优化了原来链路一些不合理的地方：对原来链路的限流机制、通道选择策略做了优化，增加了必要的功能，比如小流量实验环境的支持。</p><h2>7、新Push架构优化2：统一技术栈</h2><p>之前 push 链路有 C++/Golang 两种技术栈，除了 push 推荐服务外， 其它 C++链路模块全部使用 Golang 模块进行了重写，以提升业务迭代效率和链路稳定性。</p><h2>8、新Push架构优化3：链路整合升级，提升效率</h2><p>一个架构如果如果过度微服务化了，会带来各种问题：1）模块间耦合严重，影响研发效率：本来是一个模块应该完成的工作，硬拆成了2个模块，有改动需要都需要改两个模块，需要模块间联调测试，影响需求迭代效率。2）架构效率低：拆成微服务后，函数本地调用变成了RPC网络调用，需要增加大量的拆包、解包的操作，资源白白浪费在这些无用的内耗上了。对于频繁迭代的地方，单独抽成单独的微服务是有助于提升迭代效率的；但是我们review历史push需求，都比较分散，没有集中到一个特定的地方，我们按照“一个需求尽量只用改一个模块”的原则，对原来的push链路的所有模块进行了整合升级。具体的升级内容是：a. 触发侧合并为了1个模块：将原来触发侧的5个模块合并为1个模块；b. 调度侧合并为了1个模块：将原来调度侧的5个模块合并为了1个模块；c. 将消息通道侧模块做了整合：如上所述，我们将push消息通道原来15个模块合并为了5个。经过链路整合后：以前一个 PUSH 消息最多要经过 18 个模块，17次内部链路rpc转发；升级后，只用经过 3 个模块，只用经过 2 次 rpc 转发；这样就显著提升了链路效率；而且模块减少后，业务需要迭代无需开发多个模块，避免模块之间联调和测试，提升了业务迭代效率；同时，线上 case 排查时，无需做多模块的日志 join，提升了 case 排查效率。</p><h2>9、新Push架构优化4：自建号码包服务，提升号码包获取速度</h2><p>如上文所述：之前号码包的拉取慢是系统的主要瓶颈所在，而在我们这个场景比较简单，因此我们考虑自建号码包服务，针对于我们自己的需求来定制开发，以提升服务性能。我们的需求只有一个，就是对离线包进行分页，并提供服务接口返回指定页的数据。1）画像中台圈选兴趣包，并按页切成若干个小文件，每个兴趣包一个文件夹，并上传到cos，兴趣包里带着数据版本号；2）构建包管理服务，提供获取指定兴趣包指定页数据的能力；包管理服务定期从cos上check是否有更新的数据（比较本地数据版本和cos最新的数据版本），如果有，则拉取最新的数据更新本地数据；当接收到拉取指定包指定页数据的请求后，则定位到对应文件夹读取对应页文件数据并返回；3）集群有个数据一致性哨兵，定期检查集群节点的数据版本，当发现集群数据版本不一致时，给集群所有节点发信号，强制让每个节点同步cos上的最新数据，让集群所有节点数据跟最新数据保持一致。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459423" alt="图片" title="图片" loading="lazy"/></p><h2>10、新Push架构优化5：在线过滤改成离线预处理，避免在线处理耗时</h2><p>运营在发PUSH时会选择受众人群包，同时会指定系统、品牌等筛选项，之前的处理流程是先把人群包一股脑发到链路里，然后在下发链路里根据用户画像数据，对数据进行实时过滤。在线过滤增加了链路下发的耗时。其实系统&amp;品牌过滤完全可以前置到离线侧，我们将号码包按品牌和系统维度进行了拆分，比如“社会”包按 android/ios、huawei/oppo/vivo/honor/xiaomi，拆成了13个包，当运营选择指定的筛选项时，直接拉取对应的号码包，这样就避免了在线过滤的耗时，减少了下发的延时。</p><h2>11、新Push架构优化6：将单IO操作自动聚合成批量操作</h2><p>push下发链路有大量io操作，比如获取用户维度的多路数据（比如用户系统、品牌、下发&amp;曝光&amp;点击历史等），获取文章维度的多路数据（文章正排数据等）。链路其实主要耗时还是在io部分，如果能提升io吞吐量，就能提升PUSH链路的吞吐量，减少下发延时；io操作批处理肯定能提升吞吐量。但是在具体业务流程中，不同push类型、不用品牌用户，处理逻辑会有不同，因为每个push的处理流程可能都不一样，无法直接批处理。所以之前调度主链路流程是从队列里按单个消费进行处理的。为了提升链路吞吐量，我们对每一类io操作做了一个类，对外暴露一个单个io请求接口，外部调用该接口后，将请求压入一个异步队列，同时开始等待结果的返回；这样该类io请求都会在该异步队列里进行了汇聚。下层会开若干个处理协程，批量从异步队列消费出若干请求任务，拼成批量的io请求，然后拿到批量io结果，按序向上层返回io结果；这样对上层而言，看到的还是单个的同步io接口，上层业务逻辑开发流程无需做改造，底层其实已经自动做了io的批量聚合，显著提升了链路吞吐量。</p><h2>12、新Push架构优化7：优先推送热点突发内容，优先保证高价值用户及时性体验</h2><p>在链路吞吐量一定的情况下，一个推送任务小到几百万，大到一两亿的发送量，都需要处理时间。这时候先处理比后处理的时延要少。</p><p>其实可以考虑对链路发送进行调度：<br/>1）链路优先保障热点突发PUSH的发送，我们建立了任务优先级队列，当有热点突发PUSH在发送时，其它PUSH延迟发送；<br/>2）同一个PUSH任务，对用户推送顺序也做了排序：活跃度高、历史push点击率高、预估商业化价值高、对push时延敏感的用户优先发送。通过优先级调度，最大程度保障了热点突发内容和高价值用户的推送及时性的体感。</p><h2>13、新Push架构优化8：增加自动故障恢复能力</h2><p>为了提升链路吞吐量，调度节点进程通过 LRU cache 缓存了大量数据，所以在推送消息处理的 sharding 方式上采用了按设备号一致性哈希。很多时候某个节点异常时，会出现慢而不死的情况：处理能力陡降，但是节点存活正常。北极星未能把它摘掉，相当一部分设备会打到该节点，即使该节点已经满载了，之前架构为了避免缓存失效而导致处理耗时增加，还是会一致性哈希将流量打往该节点，导致这部分用户处理耗时异常增加，甚至发送失败。新架构对于推送任务sharding做了优化：在一致性哈希的基础上，每个节点计算出4个固定的backup；当某节点的失败率或处理耗时超过一定阈值时，将该节点的流量均匀低分给他的backup。通过这种方式就支持单节点异常时的自动故障恢复。</p><h2>14、新Push架构优化9：构建push链路自动化测试能力</h2><p>构建了接口自动化回归测试流程：1）case覆盖push链路的核心逻辑；2）合并master时自动触发回归测试流程的执行。构建了自动化diff测试流程：diff流程大体思路都类似，通过录制线上流量的真实请求和返回结果，在测试环境进行回放，观察同一请求下，返回结果是否会有差别；如果无差别，说明测试环境跟线上一样，上线不会引起线上数据异常；如果有差别，就需要分析这些差别是否是符合预期的。diff测试基本能回归到线上所有业务逻辑分支，能弥补回归测试覆盖度有限的问题。主要挑战：push依赖的数据变化比较快，导致在同一时间，同一请求的返回结果会不同；比如push为了避免重复下发同一篇文章，会依赖于下发历史数据，线上录制了刚下发的某篇文章，在测试环境去回放肯定就不能下发了，因为线上刚把这篇文章写入到下发历史里，导致回放请求时返回结果是不能下发了，这样自然就产生了diff。解决方案：在流量录制时，除了录制请求之外，同时录制各个依赖数据，在回放时，依赖数据以依赖数据为准，通过这种方案就避免了依赖数据易变而引入diff的问题。</p><h2>15、架构升级后的系统表现</h2><p>1）push运营成本显著降低：通过持续的 push 架构优化，新闻 push 总运营成本下降70%；2）PUSH链路性能（吞吐量）显著提升：通过持续的 push 架构优化，显著提升了 push 链路的性能，push推送量（出口）峰值吞吐量提升了3.5倍；3）热点突发（全国/快速）PUSH全链路耗时下降明显：a. 热点突发（全国/快速）PUSH内部链路耗时P90下降了90%；b. 内部链路耗时指的是从push审核通过到推送给厂商的时间，即我们内部链路总的耗时时长；c. 热点突发（全国/快速）PUSH全链路耗时（包括内部链路耗时和厂商链路耗时）下降了90%d. 全链路耗时指的是从push审核通过到用户收到PUSH时间，即包括内部链路和厂商链路总的耗时时长.我们完成一些架构升级后，还是评测团队对了评测，腾讯新闻的PUSH已经领先于竞品1～4分钟了。4）提升了PUSH点击效果：push推送速度提升后，push点击数据也能看到明显受益，热点突发PUSH点击pv提升了10%，push大盘点击UV也能看到显著的正向收益；线上收不到PUSH的用户客诉也减少到25年H1 0 例，提升了用户产品体验。5）稳定性良好：push链路主要重构完成后，PUSH链路稳定性&amp;质量明显提升，2025.02以后 0 故障。</p><h2>16、参考资料</h2><p>[1] 极光推送系统大规模高并发架构的技术实践分享<br/>[2] 魅族2500万长连接的实时消息推送架构的技术实践分享<br/>[3] 专访魅族架构师：海量长连接的实时消息推送系统的心得体会<br/>[4] 一个基于长连接的安全可扩展的订阅/推送服务实现思路<br/>[5] 实践分享：如何构建一套高可用的移动端消息推送系统？<br/>[6] Go语言构建千万级在线的高并发消息推送系统实践(来自360公司)<br/>[7] 腾讯信鸽技术分享：百亿级实时消息推送的实战经验<br/>[8] 百万在线的美拍直播弹幕系统的实时推送技术实践之路<br/>[9] 京东京麦商家开放平台的消息推送架构演进之路<br/>[10] 技术干货：从零开始，教你设计一个百万级的消息推送系统<br/>[11] 长连接网关技术专题(四)：爱奇艺WebSocket实时推送网关技术实践<br/>[12] 喜马拉雅亿级用户量的离线消息推送系统架构设计实践<br/>[13] 直播系统聊天技术(四)：百度直播的海量用户实时消息系统架构演进实践<br/>[14] 消息推送技术干货：美团实时消息推送服务的技术演进之路<br/>[15] 揭秘vivo百亿级厂商消息推送平台的高可用技术实践<br/>[16] 得物从零构建亿级消息推送系统的送达稳定性监控体系技术实践<br/>[17] B站千万级长连接实时消息系统的架构设计与实践<br/>[18] 转转千万级用户量消息推送系统的架构演进之路<br/>[19] 企业级实时消息推送系统的架构设计，一文即懂！</p><p>即时通讯技术学习：</p><ul><li>移动端IM开发入门文章：《新手入门一篇就够：从零开发移动端IM》</li><li>开源IM框架源码：<a href="https://link.segmentfault.com/?enc=nSoY4erMvS%2FLirrDJKmvoQ%3D%3D.RgJWh1UTnDHCxgVsArJzyPGaZu7shQ1hoYR6eFFVx%2F0dQBozE6ODiH75fZyvJUCa" rel="nofollow" target="_blank">https://github.com/JackJiang2011/MobileIMSDK</a>（备用地址点此）<br/>（本文已同步发布于：<a href="https://link.segmentfault.com/?enc=xpDsbUclHyZOt7Uyt%2Fv1VA%3D%3D.5yhkXMsMVwhJsWE3J%2F%2BsTfSybYiaN6%2B27gT722pYkZYGBCGRb2%2Bwqxr9uxMAZ3Gj" rel="nofollow" target="_blank">http://www.52im.net/thread-4883-1-1.html</a>）</li></ul>]]></description></item><item>    <title><![CDATA[一文了解 openFuyao“低底噪容器底座” openFuyao ]]></title>    <link>https://segmentfault.com/a/1190000047459299</link>    <guid>https://segmentfault.com/a/1190000047459299</guid>    <pubDate>2025-12-08 20:04:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>业务痛点</h2><p>在资源受限的运行环境（如单节点、嵌入式）中，Kubernetes（以下简称“K8s”）自身的资源占用与调度瓶颈，制约了业务 Pod 的可靠拉起与预期规模的实现。</p><h2>根因分析</h2><ul><li>K8s 生态集成多种功能，组件较多，架构略重，自身运行需要系统资源较多。</li><li>K8s 组件独立进程运行，APIServer 与 etcd 需通过网络协议通信，流量较高场景中组件间交互成为瓶颈。</li><li>containerd 创建 Pod 同时产生 shim 进程，随 Pod 数量线性占用内存资源，阻碍高密部署场景。</li></ul><h2>低底噪容器底座方案</h2><p>openFuyao 采用从编排系统、容器运行时、操作系统多层次入手，消减容器环境底噪和提升容器环境性能，打造业界首个单节点部署 1000+Pod 容器环境。<br/><img width="723" height="470" referrerpolicy="no-referrer" src="/img/bVdniuG" alt="" title=""/><br/><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdniuH" alt="" title="" loading="lazy"/></p><h3>Kubernetes 子系统：</h3><ul><li>将 K8s 及其周边组件整合为单进程，并将 APIServer 和 etcd 的网络通信优化为进程内内存交互，从而显著降低系统底噪，提升容器编排性能。100Pod 场景可降低 500+MB 内存。</li><li>使用文件探测等低成本方式代替传统消息交互方式，降低高密场景下探针消息对 CPU 和网络的影响。</li><li>最小化 K8s 基础功能，裁剪内存占用较多且不使用的特性（如 OpenAPI v3 ）。</li></ul><h3>运行时子系统：</h3><ul><li>消减 shim 进程，支持 containerd 通过 shimless 方式运行，降低底噪，使单节点可部署 1000+Pod，领先业界 4~10 倍。该典型场景可降低 20GB 内存占用。</li><li>通过启用 cgroup v2，为容器提供了更精细、高效的资源管理能力，使得高密部署时容器管理性能不下降。</li></ul>]]></description></item><item>    <title><![CDATA[React项目里，Record<string, any>和{ [key: string]: any ]]></title>    <link>https://segmentfault.com/a/1190000047459303</link>    <guid>https://segmentfault.com/a/1190000047459303</guid>    <pubDate>2025-12-08 20:03:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在 React 项目中，<code>Record&lt;string, any&gt;</code> 和 <code>{ [key: string]: any }</code> <strong>在功能层面几乎等价</strong>（都表示「键为字符串、值为任意类型的对象」），但在<strong>类型语义、语法灵活性、TS 内置特性</strong>上存在关键区别，以下是详细拆解（结合 React 实战场景说明）：</p><h3>一、核心结论先明确</h3><table><thead><tr><th>维度</th><th><code>Record&lt;string, any&gt;</code></th><th><code>{ [key: string]: any }</code></th></tr></thead><tbody><tr><td>核心功能</td><td>表示「键类型固定、值类型固定」的对象</td><td>表示「索引签名为字符串、值任意」的对象</td></tr><tr><td>语义侧重</td><td>强调「键值对映射关系」（内置工具类型）</td><td>强调「自定义索引规则」（基础语法）</td></tr><tr><td>语法灵活性</td><td>仅支持「单一键类型 + 单一值类型」</td><td>可扩展（混合固定属性 + 索引签名）</td></tr><tr><td>React 常用场景</td><td>状态/Props 快速声明、对象映射</td><td>自定义组件 Props（混合固定/动态属性）</td></tr><tr><td>类型推导</td><td>无额外扩展能力</td><td>可结合接口/类型别名扩展</td></tr></tbody></table><h3>二、具体区别与 React 实战示例</h3><h4>1. 语义与设计初衷</h4><ul><li><code>Record&lt;K, V&gt;</code> 是 TypeScript <strong>内置工具类型</strong>，设计初衷是「明确表示一个<strong>键类型为 K、值类型为 V 的键值对映射对象</strong>」，语义更聚焦“映射”；</li><li><code>{ [key: string]: V }</code> 是 TypeScript <strong>基础索引签名语法</strong>，设计初衷是「定义对象的索引规则」，语义更聚焦“对象的索引方式”。</li></ul><p>在 React 中，比如声明一个“动态配置对象”：</p><pre><code class="tsx">// Record：语义更清晰（“字符串键 → 任意值”的映射）
const formConfig: Record&lt;string, any&gt; = {
  username: { label: '用户名', required: true },
  password: { label: '密码', type: 'password' },
};

// 索引签名：语义偏“对象的索引规则”，功能等价
const formConfig: { [key: string]: any } = {
  username: { label: '用户名', required: true },
  password: { label: '密码', type: 'password' },
};</code></pre><h4>2. 语法灵活性（React 中最关键的区别）</h4><p><code>{ [key: string]: any }</code> 支持<strong>混合「固定属性 + 动态索引」</strong>，而 <code>Record&lt;string, any&gt;</code> 只能表示「纯动态键值对」—— 这在 React 组件 Props 定义中尤为常用：</p><pre><code class="tsx">// ✅ 合法：索引签名 + 固定属性（React Props 常用）
interface InputProps {
  // 固定属性
  defaultValue: string;
  onChange: (value: string) =&gt; void;
  // 动态属性（兼容其他未显式声明的 props）
  [key: string]: any;
}

// ❌ 非法：Record 无法混合固定属性
interface InputProps extends Record&lt;string, any&gt; {
  defaultValue: string; // 语法上允许，但语义矛盾（Record 是纯动态映射）
  onChange: (value: string) =&gt; void;
}</code></pre><p>比如 React 中封装通用组件时，常需要「固定核心 Props + 兼容任意扩展属性」，此时只能用索引签名，而 Record 做不到这种混合：</p><pre><code class="tsx">// 正确：用索引签名封装通用按钮 Props
interface ButtonProps {
  type: 'primary' | 'default';
  size: 'small' | 'large';
  [key: string]: any; // 兼容 className、style 等原生属性
}

const Button = (props: ButtonProps) =&gt; {
  const { type, size, ...rest } = props;
  return &lt;button className={`btn-${type}-${size}`} {...rest} /&gt;;
};

// 错误：Record 无法区分“固定属性”和“动态属性”
type ButtonProps = Record&lt;string, any&gt;; // 丢失 type/size 的类型校验</code></pre><h4>3. 类型参数扩展（非 React 专属，但影响写法）</h4><p><code>Record&lt;K, V&gt;</code> 的 <code>K</code> 支持<strong>联合类型</strong>（比如 <code>string | number</code>），而索引签名的 <code>key</code> 只能是 <code>string</code>/<code>number</code>/<code>symbol</code> 单一类型（但实际中 <code>number</code> 键会被转为 <code>string</code>，效果等价）：</p><pre><code class="ts">// ✅ Record 支持联合键类型
type MixedKeyObj = Record&lt;string | number, any&gt;;
const obj: MixedKeyObj = {
  name: '张三',
  123: '数字键', // 合法
};

// ✅ 索引签名也支持 number，但实际键会转字符串
type MixedKeyObj2 = { [key: number]: any };
const obj2: MixedKeyObj2 = {
  123: '数字键', // 合法（键实际是 "123"）
  // 'name': '张三' // ❌ 索引签名是 number，不允许字符串键
};</code></pre><h4>4. 代码简洁性</h4><ul><li>当只需声明「字符串键 + 任意值」时，<code>Record&lt;string, any&gt;</code> 比 <code>{ [key: string]: any }</code> 更简洁；</li><li><p>当需要自定义值类型（比如 <code>string | number</code>），两者简洁度相当：</p><pre><code class="ts">// 等价写法
type StrNumObj1 = Record&lt;string, string | number&gt;;
type StrNumObj2 = { [key: string]: string | number };</code></pre></li></ul><h3>三、React 项目中的选择建议</h3><table><thead><tr><th>场景</th><th>推荐写法</th><th>原因</th></tr></thead><tbody><tr><td>临时声明纯动态对象（如 state、临时变量）</td><td><code>Record&lt;string, any&gt;</code></td><td>语义清晰、代码更短</td></tr><tr><td>组件 Props（混合固定属性 + 动态扩展）</td><td><code>{ [key: string]: any }</code></td><td>支持固定属性 + 索引签名，适配 React 原生属性（如 className）</td></tr><tr><td>明确“键值映射”语义（如配置对象、字典）</td><td><code>Record&lt;string, T&gt;</code></td><td>语义更贴合“映射”场景，可读性更高</td></tr><tr><td>需扩展/复用类型（如接口继承）</td><td><code>{ [key: string]: T }</code></td><td>可与接口/类型别名无缝混合，灵活性更高</td></tr></tbody></table><h3>四、避坑提醒（React 中常见误区）</h3><ol><li>不要滥用 <code>any</code>：无论是 <code>Record&lt;string, any&gt;</code> 还是 <code>{ [key: string]: any }</code>，<code>any</code> 会丢失 TypeScript 类型校验，React 中建议尽量指定具体值类型（比如 <code>Record&lt;string, FormItemConfig&gt;</code>）；</li><li><p>函数组件 Props 扩展：如果想兼容 React 原生 HTML 属性，推荐用 <code>React.HTMLAttributes&lt;HTMLElement&gt;</code> 而非纯索引签名，比如：</p><pre><code class="tsx">interface CustomInputProps extends React.HTMLAttributes&lt;HTMLInputElement&gt; {
  value: string;
  onChange: (value: string) =&gt; void;
}</code></pre></li></ol><h3>最终总结</h3><p>在 React 项目中，<code>Record&lt;string, any&gt;</code> 和 <code>{ [key: string]: any }</code> <strong>功能上等价</strong>（都表示字符串键的任意对象），核心差异在：</p><ul><li><code>Record</code> 更简洁、语义聚焦“映射”，适合纯动态对象；</li><li>索引签名更灵活，支持混合固定属性，适合组件 Props 等场景。</li></ul><p>日常开发中可根据“是否需要混合固定属性”选择，无需过度纠结，重点是避免滥用 <code>any</code>，尽量指定具体类型。</p>]]></description></item><item>    <title><![CDATA[VibeCoding 翻新个人站 (Nextjs+Django) alpha94511 ]]></title>    <link>https://segmentfault.com/a/1190000047459306</link>    <guid>https://segmentfault.com/a/1190000047459306</guid>    <pubDate>2025-12-08 20:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近从个人站引来了德国客户，成功开出海外服务第一单✌️，也让我意识到该把自己的古早个人站做个大升级。折腾了几个小时终于把上线，欢迎大家+友链  <a href="https://link.segmentfault.com/?enc=lvvgY6Yh0fQSZuUVs%2FjF3g%3D%3D.8O4I7zjMRGsl1u9iYpI8%2BRsL3G3lp%2FQbqmEiIxoZwOg%3D" rel="nofollow" target="_blank">https://www.hephaestus.fr/</a></p><p>前端 Nextjs部署在 Vercel上<br/>后端 Django+S3+PostgreSQL 部署在了 DigitalOcean上，资源使用了CDN加速</p><p>后续工作主要是SEO优化，持续引流</p>]]></description></item><item>    <title><![CDATA[漏算的 Token：AI 网关限额机制的攻防博弈 spacewander ]]></title>    <link>https://segmentfault.com/a/1190000047459312</link>    <guid>https://segmentfault.com/a/1190000047459312</guid>    <pubDate>2025-12-08 20:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>攻</h2><p>AI 网关通常有这样的功能：基于 token 消耗量来做限额操作。有些地方叫做 ai-rate-limiting，有些地方叫做 ai-quota。无论名字为何，原理同出一辙，都是基于推理请求结束时返回的 token usage 信息。</p><p>那么绕过限制的方式就显而易见了，只需找找有没有能让网关看不到推理请求结束时的 usage 信息即可。有些时候，用户在无意中就能绕过这些限制。比如在 OpenAI chat 接口里，默认 streaming 的时候就不会返回 usage，除非用户请求时指定了 include_usage：<a href="https://link.segmentfault.com/?enc=G5trmV9jTtPAgCMyhA2EuQ%3D%3D.ZroCVxC7NoOhrMJXG%2FRhRrjq07ZiUzdvDfXpMQqxB%2BqrxZ11FbltNSctbJJMsrwelwB1GUlSJ%2B%2B1huib5lJ00QcQFyObPhcnfGOnd2nwOPoTTBeHpUMiuNtmmNv%2BDG0kn98A89KqtHlgeSe5m5KTwA%3D%3D" rel="nofollow" target="_blank">https://platform.openai.com/docs/api-reference/chat/create#ch...</a>。</p><p>假设模型供应商总是会提供 token usage，抑或网关在处理用户请求时做了点 hack 额外加上 include_usage，保证了 token usage 总是在推理请求结束时存在，那该怎么办了？方法还是有的，让推理请求提前中断即可。我们可以插入一段 prompt，指定在返回结果结束时输出一个 stop word，然后再执行一个耗时的任务。当客户端收到这个 stop word 后，就可以安心地把连接中断掉。只要请求是提前中断的，网关就不会继续保持和上游的请求，自然收不到上游最后发过来的 usage 了。当然有些配置项可以修改这种行为，比如 Nginx 的 proxy_ignore_client_abort。但如果这么做的话，万一是正常的客户端想要提前终止推理，结果因为网关还是继续和上游通信而导致被多算钱就麻烦了。这种小伎俩可以骗过中间件，不过推理引擎侧还是能知道 prefill 时收到多少 input token，decode 时发出了多少 output token。所以最后给到来的账单还是正常的。</p><h2>守</h2><p>上述各种攻击手段，本质上揭示了当前 AI 网关在流式传输场景下的架构痛点：计费的异步性。在传统的 Request-Response 模型中，网关可以轻松拦截并统计流量；但在 LLM 的流式交互中，Token 的消耗是一个随着时间推移动态累加的过程，而精准的 token usage 报告往往滞后于请求的结束。只要网关依赖于“事后”的上报数据，客户端就有机会利用断连等手段制造“计费黑洞”。</p><p>那么有什么可靠的方式，能够不依赖推理请求中的 token usage 信息，自己在通信过程中算出实际的 token 用量？</p><p>最简单粗暴的方法，就是将字节数乘上一个 magic number 系数，作为找不到 token usage 时的 fallback。如果能在准确性上睁一只眼闭一只眼，这倒是开销最小的方案。</p><p>官方的做法，是调用模型提供商自己的 count token 接口。对于开源的推理引擎像是 vllm 或 TensorRT-LLM，也有对应的 tokenize 接口。只是要让网关在每次请求时额外发起多次 HTTP 调用，代价有点高，尤其在流式处理响应的时候。</p><p>一些编码库提供了本地 tokenize 的能力，如：</p><ul><li>huggingface/tokenizers</li><li>openai/tiktoken 和它的 Go 移植：pkoukk/tiktoken-go</li></ul><p>但是这些 tokenizer 在工作时需要知道模型的 tokenizer 配置，而模型提供商大概率不会公布这些数据。不过市面上也有这些私有模型的开源版本，比如 Gemma 之于 Gemini。不知道这些开源版本的 tokenizer 配置和私有的差别有多少，基于它们的 tokenizer 配置和官方的 count token 接口返回结果是否近似。</p><p>如果是自己部署的模型，那么理论上有了 tokenizer 配置就能自己本地做 tokenize，无需依赖一个远程的 tokenizer 服务。</p><p>假设 token usage 不是由本地提供，而是依赖远程的返回结果，出于谨慎起见，最好在基于 token 限额的同时加上基于请求数（或字节数，有的话更好）的限额，这样一旦远端无法返回 token usage，不至于出现完全不设防的情况。</p>]]></description></item><item>    <title><![CDATA[未来应用生态变革趋势探讨 小虫_top ]]></title>    <link>https://segmentfault.com/a/1190000047459352</link>    <guid>https://segmentfault.com/a/1190000047459352</guid>    <pubDate>2025-12-08 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>豆包手机助手遭到各方质疑，围绕微信、银行App等与人工智能的对抗成为热议焦点。但我们眼光放长远一些，或许能看到一个更本质的趋势：<strong>未来的应用生态，很可能会从“一个个孤立的产品”逐渐转向“一组组开放的接口”</strong>。</p><p>甚至连App这个概念本身都会慢慢淡化，成为数字发展史上的一个阶段性产物。越来越多的后端服务，应该在经过安全评审之后直接对外开放。面向终端用户的产品形态，也会像今天的阿里云、百度智能云那样——除了依赖固定的界面，也可以通过API的形式提供数据挖掘、图像识别、AI能力等核心功能。<br/><strong>用户无需被既定操作流程束缚</strong>，只需遵循接口规范，就能自主调用、组合这些服务，实现真正个性化的需求。这正是“智能化”走向深水区的体现：<strong>技术不再仅仅是给人用的工具，更成为可被自由调用的“数字积木”</strong>。</p><p>所有人都能感受到近两年大模型的迅猛发展，技术演进的速度远超我们的想象。与其争论“AI是否会取代程序员”，不如看清一个宏观事实：开发一款产品的门槛正在快速降低，周期不断缩短，智能程度持续提高。这意味着，<strong>未来的产品竞争维度必将发生改变</strong>。</p><p>一款优秀的产品，除了需要“直观、简洁、易懂”的人性化交互界面，也必然要提供“开放、安全、高效”的“机性化”可调用接口，这不再是一种选择，而会逐渐成为标配。</p><p>当然，<strong>任何变革都无法一蹴而就</strong>。如果现强行推动这样一场“接口化革命”，对许多依赖现有商业模式的公司来说，无疑会造成巨大冲击，比如现有的广告营收体系，或将面临重构。<strong>但方向已经清晰，新的竞赛其实早已悄然开始</strong>。</p><p>如果今天的独角兽们仍固守封闭的生态思维，执着于现有旧叙事，那么很可能，它们就会成为下一个诺基亚、下一个柯达——<strong>不是败给技术，而是输给了趋势</strong>。</p><p>时代从不停留，而唯一能确定的是：<strong>开放、连接与智能融合，正在重新定义我们与数字世界交互的方式</strong>。</p>]]></description></item><item>    <title><![CDATA[在 Pycharm 中 debug Scrapy 项目 codists ]]></title>    <link>https://segmentfault.com/a/1190000047459002</link>    <guid>https://segmentfault.com/a/1190000047459002</guid>    <pubDate>2025-12-08 19:05:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>缘起</h2><p>为什么写这篇文章呢？因为自己想在 Scrapy 项目里 debug, 看看 Response 有哪些属性。但是 Scrapy 的官方文档的 debug 说明只有 VSCode 的，没有 Pycharm 的(详见：<a href="https://link.segmentfault.com/?enc=eBtdfMM8hwzj6h%2BGwrf%2BTw%3D%3D.6sNNIP1co39SaiiTqJF%2B0dt6M5woeG%2FyofyFN%2BAAV3FO26y2ZcBPfMbV62lH%2Fb0OtuaTPAyLtLxUyNzQTQg7LQ%3D%3D" rel="nofollow" target="_blank">https://docs.scrapy.org/en/latest/topics/debug.html</a>)：</p><pre><code>{
    "version": "0.1.0",
    "configurations": [
        {
            "name": "Python: Launch Scrapy Spider",
            "type": "python",
            "request": "launch",
            "module": "scrapy",
            "args": [
                "runspider",
                "${file}"
            ],
            "console": "integratedTerminal"
        }
    ]
}</code></pre><p>当然，如果熟悉 VSCode 的人看到这个配置就明白其实执行方式是：python -m scrapy runspider xxx_spider.py (注：这里的 xxx_spider.py 指 spider 文件，如官方文档里面的 quotes_spider.py)。如果这个人同时还熟悉 Pycharm, 那么他就知道在 Pycharm 里面配置进行 debug：</p><p><img width="723" height="396" referrerpolicy="no-referrer" src="/img/bVdnipG" alt="" title=""/></p><p>很遗憾，我不是这样的人，所以就有了这篇文章。</p><h2>说明</h2><p>时间：2025/12/06</p><p>Pycharm 版本：2025.2.4</p><p>Python 版本：3.12.0</p><p>Scrapy 版本：2.13.4</p><p>Windows 版本：Win 11</p><h2>main.py</h2><p>在与 scrapy.cfg 文件同层级的目录中新建一个名为 main.py 的文件，用于 debug。示例：</p><pre><code># main.py
from scrapy.cmdline import execute


if __name__ == '__main__':
    print(1)
    print(2)
    execute(['scrapy', 'crawl', 'manning'])</code></pre><p>项目结构：</p><p><img width="723" height="234" referrerpolicy="no-referrer" src="/img/bVdnipK" alt="" title="" loading="lazy"/></p><h2>TypeError: 'Task' object is not callable</h2><p>当 Debug'main'时， 出现错误：</p><pre><code>2025-12-06 10:51:15 [asyncio] ERROR: Exception in callback &lt;Task pending name='Task-1' coro=&lt;ExecutionEngine.open_spider() running at D:\Projects\PythonProjects\python-talk\backend\venv\Lib\site-packages\scrapy\core\engine.py:430&gt; cb=[Deferred.fromFuture.&lt;locals&gt;.adapt() at D:\Projects\PythonProjects\python-talk\backend\venv\Lib\site-packages\twisted\internet\defer.py:1255]&gt;()
handle: &lt;Handle &lt;Task pending name='Task-1' coro=&lt;ExecutionEngine.open_spider() running at D:\Projects\PythonProjects\python-talk\backend\venv\Lib\site-packages\scrapy\core\engine.py:430&gt; cb=[Deferred.fromFuture.&lt;locals&gt;.adapt() at D:\Projects\PythonProjects\python-talk\backend\venv\Lib\site-packages\twisted\internet\defer.py:1255]&gt;()&gt;
Traceback (most recent call last):
  File "D:\Apps\Python3.12\Lib\asyncio\events.py", line 84, in _run
    self._context.run(self._callback, *self._args)
TypeError: 'Task' object is not callable</code></pre><p><img width="723" height="145" referrerpolicy="no-referrer" src="/img/bVdnipO" alt="" title="" loading="lazy"/><br/>之所以产生这个问题，不是代码的问题，是 Pycharm debuger 的问题，我还没梳理完，故暂不展开，只讲怎么解决。</p><h2>Debug 方式</h2><h3>方法 1：TWISTED_REACTOR</h3><ol><li>Settings &gt; Python &gt; Debugger，取消 Gevent compitable 的勾选。<br/><img width="723" height="187" referrerpolicy="no-referrer" src="/img/bVdnipP" alt="" title="" loading="lazy"/></li></ol><p>2.在项目的 settings.py 文件里设置 TWISTED_REACTOR = 'twisted.internet.selectreactor.SelectReactor'</p><p><img width="723" height="273" referrerpolicy="no-referrer" src="/img/bVdnipQ" alt="" title="" loading="lazy"/></p><h3>方法 2：python.debug.asyncio.repl</h3><p>1.Settings &gt; Python &gt; Debugger，取消 Gevent compitable 的勾选(这步和方法 1 是一样的)。<br/><img width="723" height="187" referrerpolicy="no-referrer" src="/img/bVdnipP" alt="" title="" loading="lazy"/></p><p>2.双击 Shift 键打开搜索窗口。</p><p>双击 Shift 的意思是“search everywhere，详见 <a href="https://link.segmentfault.com/?enc=q8IEv8FcpgZfiKocEztE2A%3D%3D.D2DsrKtXTM8%2F1uRCX%2FdWDe5DqJvefCJ%2FERPvaKQLD3UiBcFiqFtgagIDwaoMxhUduGkjxsEkyl7hdEZQYc4tNHxRu8sj9mg5osMDHSkuPQo%3D" rel="nofollow" target="_blank">https://www.jetbrains.com/help/pycharm/searching-everywhere.html</a>”。</p><p><img width="723" height="197" referrerpolicy="no-referrer" src="/img/bVdnipR" alt="" title="" loading="lazy"/><br/>3.点击 ALL 选项，输入 registry，最后点击 Regisry 选项。</p><p><img width="723" height="319" referrerpolicy="no-referrer" src="/img/bVdnipS" alt="" title="" loading="lazy"/></p><p>4.找到 python.debug.asyncio.repl，取消勾选 Value 列的方框。 <br/><img width="723" height="431" referrerpolicy="no-referrer" src="/img/bVdnipT" alt="" title="" loading="lazy"/></p><h2>验证</h2><p><img width="723" height="422" referrerpolicy="no-referrer" src="/img/bVdnipU" alt="" title="" loading="lazy"/></p><p>如上图所示，设置后可以 debug。</p><h2>参考资料</h2><p>1.Scrapy 文档, Debugging Spiders: <a href="https://link.segmentfault.com/?enc=iEQOrkid%2B1eoNdACgabyAw%3D%3D.cSxZkF5J7Xp%2BVIiWuz7gf220%2FSq394SZog3jJnu8k4yKwOx7%2BRps5wyiDzj4kO0WhXHqsDkE4e5mtNFg%2FpZ2WA%3D%3D" rel="nofollow" target="_blank">https://docs.scrapy.org/en/latest/topics/debug.html</a></p><p>2.Pycharm 文档，Search for a target by name：<a href="https://link.segmentfault.com/?enc=YyI1Rjbq92AkKo4jaYwFKw%3D%3D.mI97mAjvRoznZLOriWa399OPHYdbtnmZPpZv67xlZ7A5zSo5CranddGipgrYIJfri2CuhXSP2FANyHcFTgzhMOnJWYDJDCPwJxzZbltrnWA%3D" rel="nofollow" target="_blank">https://www.jetbrains.com/help/pycharm/searching-everywhere.html</a><br/><img width="723" height="263" referrerpolicy="no-referrer" src="/img/bVdfTXK" alt="" title="" loading="lazy"/><br/>欢迎搜索及关注：编程人(a_codists)，如有问题请留言。</p>]]></description></item><item>    <title><![CDATA[警惕“上下文污染”：为什么建议你频繁重置 AI 对话？ 飞奔的毛巾 ]]></title>    <link>https://segmentfault.com/a/1190000047459123</link>    <guid>https://segmentfault.com/a/1190000047459123</guid>    <pubDate>2025-12-08 19:05:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在使用 LLM 时，我们常遇到“风格漂移”和“逻辑幻觉”。 比如你让 AI 扮演 Python 专家“只写代码不解释”，但因为你中间追问了一次“为什么”，它在后续的回答里就开始喋喋不休地解释。</p><p>这是因为大语言模型的注意力是有限的。 当异质性内容（不同类型的话题）在历史记录中堆积，初始指令的权重就会被不可避免地削弱。</p><p>解决办法：</p><ol><li>一事一议 绝不混用窗口。写代码的窗口别用来写诗，翻译的窗口别用来做数学题。</li><li>物理隔离 任务一旦结束，或者话题一旦转换，立刻关闭当前对话。</li><li>学会“手动垃圾回收” 当你发现 AI 开始不听话，试图通过打字去纠正它（比如“请回到刚才的设定”）通常效果很差，因为这增加更多的噪音。 最高效的方法是：直接开新窗口，重新输入提示词。</li></ol><p>让每一个对话窗口都只为一个明确的目标服务。你会发现，那个“听话、聪明、精准”的AI，又回来了。</p>]]></description></item><item>    <title><![CDATA[SQL Server到Oracle：不同事务机制下的数据一致性挑战 RestCloud ]]></title>    <link>https://segmentfault.com/a/1190000047459136</link>    <guid>https://segmentfault.com/a/1190000047459136</guid>    <pubDate>2025-12-08 19:04:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今企业数据架构日益复杂的背景下，跨数据库平台的数据同步已成为许多组织的常态化需求。当数据需要从SQL Server迁移至Oracle时，我们不仅面临语法差异的挑战，更需深入理解两大数据库在事务处理机制上的本质区别。本文将深入探讨在异构数据库同步过程中，通过使用ETLCLoud的离线数据集成及实时数据集成功能，确保数据在跨平台传输时的一致性与完整性，为构建可靠的数据流通体系提供实践指导。</p><h3>一、创建数据源连接</h3><p>在平台首页左侧模块菜单栏找到数据源管理模块，下拉选择数据源列表选项。</p><p>右侧面板点击新建数据源按钮创建一个新的数据源连接。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459139" alt="图片 1" title="图片 1"/></p><p>根据自己的数据库类型选择，这里要连接SqlServer。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459140" alt="图片 2" title="图片 2" loading="lazy"/></p><p>根据面板信息填写相关信息，影响能否连接的主要配置有账号、密码、数据库IP端口，注意不能有空格。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459141" alt="图片 1" title="图片 1" loading="lazy"/></p><p>配置完信息后点击保存并测试连接按钮，上方弹出测试成功证明数据库连通。如果连接失败可以到监控中心查看控制台日志。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459142" alt="图片 2" title="图片 2" loading="lazy"/></p><p>再创建一个目标端Oralce的数据源。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459143" alt="图片 1" title="图片 1" loading="lazy"/></p><h3>二、创建离线同步流程</h3><p>在左侧离线数据集成模块找到流程管理，点击新建流程创建一个新的流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459144" alt="图片 1" title="图片 1" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459145" alt="图片 2" title="图片 2" loading="lazy"/></p><p>点击流程设计进入流程设计页面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459146" alt="图片 3" title="图片 3" loading="lazy"/></p><p>从左侧组件栏拖取组件到右侧画布，并用路由线从开始连接到最后。</p><p>这里使用一个库表输入组件从SqlServer表拉取数据，用库表输出组件将数据推送到目标表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459147" alt="图片 4" title="图片 4" loading="lazy"/></p><p>库表输入配置：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459148" alt="图片 5" title="图片 5" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459149" alt="图片 6" title="图片 6" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459150" alt="图片 7" title="图片 7" loading="lazy"/></p><p>库表输出组件配置：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459151" alt="图片 8" title="图片 8" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459152" alt="图片 9" title="图片 9" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459153" alt="图片 10" title="图片 10" loading="lazy"/></p><p>配置完流程，点击运行按钮运行数据同步任务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459154" alt="图片 11" title="图片 11" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459155" alt="图片 12" title="图片 12" loading="lazy"/></p><p>等待流程运行，流程运行结束即完成同步任务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459156" alt="图片 13" title="图片 13" loading="lazy"/></p><p>检查目标表数据</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459157" alt="图片 14" title="图片 14" loading="lazy"/></p><h3>三、实时数据同步</h3><p>离线同步数据后，后续源表如果有增量数据（数据增删改）想要同步到目标表，ETLCloud可以通过采集数据库日志的方式去读取表的增量数据，这样就不必每次同步都读取整张表造成资源的浪费，并且实时数据集成能让源表目标表达到毫秒级的数据一致。</p><p>但是实时数据集成需要对数据库做一下配置，因为主要是采集数据库归档日志，每种数据库开启CDC的步骤不一样，可以到官网帮助文档查看开启方法。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459158" alt="图片 15" title="图片 15" loading="lazy"/></p><p>开启数据库的CDC后，来到实时数据集成模块创建数据库监听器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459159" alt="图片 16" title="图片 16" loading="lazy"/></p><p>这里源表和目标表表机构一致就采用直接传到到目标的同步方式，如果需要对增量数据做特殊处理可以使用传输到ETL的方式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459160" alt="图片 17" title="图片 17" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459161" alt="图片 18" title="图片 18" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459162" alt="图片 19" title="图片 19" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459163" alt="图片 20" title="图片 20" loading="lazy"/></p><p>配置好监听器后点击增量启动监听器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459164" alt="图片 21" title="图片 21" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459165" alt="图片 22" title="图片 22" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459166" alt="图片 23" title="图片 23" loading="lazy"/></p><p>对源表进行数据更改</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459167" alt="图片 24" title="图片 24" loading="lazy"/></p><p>数据库监听器捕获到了源表的变更数据，并且直接将源端的增删改都同步到目标表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459168" alt="图片 25" title="图片 25" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459169" alt="图片 26" title="图片 26" loading="lazy"/></p><p>检查目标表数据与源表一致。</p><h3>四、最后</h3><p>通过从SQL Server到Oracle的完整同步实践，我们看到在异构数据库环境中维护数据一致性需要系统性的解决方案。无论是离线全量同步还是实时增量同步，关键在于深入理解不同数据库的事务特性，并选择与之匹配的同步策略。ETLCloud通过CDC机制实现了近乎实时的数据同步，有效解决了异构环境下的数据一致性问题。随着企业数据生态的不断发展，掌握跨数据库平台的同步技术将成为数据工程师的核心能力，为构建更加弹性、可靠的数据架构奠定坚实基础。</p>]]></description></item><item>    <title><![CDATA[AI 正在“杀死”敏捷开发？它反而让我们重新读懂敏捷的真谛 悲伤的斑马 ]]></title>    <link>https://segmentfault.com/a/1190000047459203</link>    <guid>https://segmentfault.com/a/1190000047459203</guid>    <pubDate>2025-12-08 19:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近在技术论坛刷到个热门话题：“AI 时代，敏捷开发是不是要凉了？”有人贴出代码生成工具的截图，配文“现在AI 10分钟能写完的代码，还要什么迭代开发？”；也有人悲观预言：“敏捷的核心是‘人’，AI来了，人的价值被稀释了。”</p><p>作为从业8年的产品经理，我亲历过从瀑布模型到敏捷转型的阵痛，也玩过ChatGPT写需求文档、GitHub Copilot生成代码。但越用越觉得：AI不是在替代敏捷，而是在用最直接的方式，把敏捷开发中那些“形式化”的泡沫戳破，逼我们回归最本真的敏捷。</p><p>一、被误解的敏捷：我们早就偏离了初心<br/>先问个问题：你所在的团队，真的在做“敏捷”吗？</p><p>我见过太多“伪敏捷”现场：</p><p>每天站会变成“汇报表演”，15分钟扯皮1小时；<br/>用户故事拆得比分子还细，但没人关心真实需求；<br/>迭代评审会成了“背锅大会”，开发吐槽产品改需求，产品吐槽测试漏bug；<br/>最讽刺的是，有些团队连“敏捷教练”都配齐了，但交付的产品依然离用户十万八千里。<br/>敏捷开发的本质是什么？《敏捷宣言》的四大价值观早就写明白了：<br/>个体与互动 &gt; 流程与工具<br/>可工作的软件 &gt; 全面的文档<br/>客户合作 &gt; 合同谈判<br/>响应变化 &gt; 遵循计划</p><p>但现实中，我们往往把敏捷做成了“流程崇拜”——用Jira看板划分任务状态，用燃尽图证明“我们在敏捷”，用固定两周的迭代周期掩盖对需求的逃避。当敏捷变成一套标准化的SOP，它就已经死了。</p><p>二、AI 来了，先“杀死”的是伪敏捷<br/>现在AI登场了，它最先冲击的，恰恰是这些“形式化敏捷”的痛点。</p><ol><li>代码生成工具：打破“为迭代而迭代”的怪圈<br/>以前我们拆用户故事，总爱把一个功能切成“前端页面”“接口开发”“联调测试”三期，美其名曰“小步快跑”。但AI可以直接生成完整可运行的代码模块，甚至自动补全测试用例。这时候再强行拆解迭代，反而成了效率拖累——敏捷的“快速交付”不是目的，快速验证价值才是。</li><li>需求分析工具：倒逼我们直面真实用户<br/>用AI做用户调研是什么体验？输入“25-30岁一线城市女性，健身爱好者，想通过APP记录饮食”，它能瞬间生成10条用户故事，甚至模拟出使用场景对话。但这些“完美需求”背后，藏着更残酷的真相：如果AI都能替代我们理解用户，那产品经理的核心价值是什么？<br/>答案是：比AI更懂“人”。敏捷强调“客户合作”，但很多团队把“客户”简化成了产品经理自己。AI的出现，逼我们走出办公室，去和真实用户聊天——因为只有人的洞察，才能让需求从“正确”变成“惊艳”。</li><li>自动化测试：让“响应变化”不再昂贵<br/>传统敏捷中，测试是瓶颈：改一行代码可能触发连锁反应，回归测试要花半天。但AI驱动的自动化测试能实时监控代码变更，自动生成测试报告。这意味着什么？我们可以更勇敢地调整需求了——因为试错成本被AI拉低了，敏捷的“响应变化”才能真正落地。</li></ol><p>三、AI 时代，我们需要怎样的敏捷？<br/>说到底，AI不是敏捷的敌人，而是“敏捷升级”的催化剂。它让我们看清：敏捷的核心从来不是“快”，而是“灵活”——灵活地理解需求、灵活地调整方向、灵活地创造价值。</p><p>未来真正稀缺的敏捷团队，会具备这三种能力：</p><ol><li>人类独有的“价值判断力”<br/>AI能生成代码，但判断“这个功能该不该做”“用户会不会买单”的，只能是人。敏捷中的“用户故事”，未来会从“作为XX，我需要XX”变成“作为XX，我愿意为XX付费”——因为AI让试错成本降低，我们可以更聚焦商业价值。</li><li>跨领域的“系统思维”<br/>当AI接管了代码、测试、甚至部分设计工作，团队成员需要跳出单一角色，理解整个产品链路。比如产品经理要懂技术架构，开发要懂用户心理——因为敏捷的“个体与互动”，在AI时代会升级为“多学科碰撞”。</li><li>持续学习的“反脆弱”心态<br/>AI在进化，敏捷团队也必须进化。那些抱着“我懂敏捷流程”吃老本的人，终将被淘汰；但那些把AI当工具、不断拓展能力边界的人，会成为新时代的“敏捷超级个体”。</li></ol><p>最后：敏捷从未过时，过时的是我们对敏捷的想象<br/>20年前，敏捷宣言是对“重型流程”的反叛；20年后，AI是对“形式化敏捷”的反叛。变化的从来不是敏捷本身，而是我们理解敏捷的方式。</p><p>所以下次再有人问你“AI来了，敏捷还重要吗？”，你可以这样回答：<br/>“AI不是在替代敏捷，而是在帮我们撕掉敏捷的‘标签’，回到那个最本质的问题：我们究竟在为什么而敏捷？”</p>]]></description></item><item>    <title><![CDATA[2025年团队知识库与知识管理工具选型指南：评估维度与思维框架 许国栋 ]]></title>    <link>https://segmentfault.com/a/1190000047459231</link>    <guid>https://segmentfault.com/a/1190000047459231</guid>    <pubDate>2025-12-08 19:02:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>在企业数据驱动转型的过程中，仅靠项目管理、CI/CD、代码仓库工具，往往难以形成系统化的“组织知识资产”。团队知识库成为连接“人—项目—知识—复用”的关键桥梁。本文聚焦主流团队知识库工具，从战略与执行双层视角分析其适用性、优势与局限，并提出“工具之外”的思维框架，帮助中高层研发负责人、PMO、效能管理专家在选型时作出理性决策。</blockquote><h2>为什么知识库建设对现代研发组织至关重要</h2><p>在大型、复杂的 B2B 研发组织中，技术规范、架构设计、需求文档、测试方案、运维流程、项目复盘、新人 onboarding、跨团队协作……这些知识与经验，往往分散在代码仓库、即时通讯、文件共享、项目管理系统、邮件、甚至 “某个老员工脑袋里”。</p><ul><li>这样的分布方式，会导致知识难以检索、沉淀和复用。每当类似问题重复出现，团队无时借鉴，容易“重新造轮子”；</li><li>新员工 onboarding、跨团队合作、知识传承成本高，效率低下；</li><li>当关键人员离职、业务扩展、合规审计、交接与培训出现时，知识流失与风险暴露更为严重。</li></ul><p>研究表明，将组织隐性知识转为显性知识，是企业知识管理的核心任务。</p><p>一个好的知识库，远不应只是“文档存储”的集合——它应该承担组织的“记忆”和“学习”功能。通过结构化、分类、权限、版本控制、搜索、标签／元数据管理、关联项目与任务、与工具链集成、审计与治理机制，一个知识库能真正成为企业的长期知识资产。</p><p>企业实践也表明，系统化知识管理可以显著提升决策效率、减少重复劳动、加速协作、缩短新人成熟周期，并为创新、合规与风险管理提供基础。</p><p>因此，知识库建设，是组织从“项目驱动型”向“能力／资产驱动型”跃迁的重要一步。</p><h2>主流团队知识库工具测评（2025 年终总结）</h2><p>以下是几款当前国内外广泛使用、适合不同发展阶段和组织规模的知识库工具，包括 ONES Wiki、Confluence、GitBook、Tettra、Notion、Nuclino。它们各有定位，没有“万能最优”，关键在于与你组织的阶段、规模、治理水平、战略规划匹配。</p><h4>ONES Wiki——一体化的文档协同和知识库管理工具</h4><p>核心功能：成熟的企业级知识管理平台，能把“文档／知识／经验／流程”组织起来，支持多人协同编辑、版本控制、权限管理、模板机制、与项目/任务管理系统集成、文档关联项目/任务、支持多种内容嵌入（思维导图、代码片段、流程图等）以及内容结构化。支持组织细粒度权限、安全、审计等。</p><p>适用场景：中大型企业、复杂项目 / 多团队协作、有合规／审计／安全要求、需要知识与任务／项目全流程关联、追求长期知识资产积累与治理的组织；尤其适合技术、产品、运维、管理等多角色协作与知识共享。</p><p>优势亮点：<br/>一体化：将知识库与项目任务管理、DevOps／交付流程关联，减少信息孤岛。<br/>权限与治理：支持分类、读写权限、版本控制、模板机制、结构化管理，便于制度化管理与审计。<br/>灵活性与扩展性：支持多种内容类型，可嵌入代码、流程图、表格等，适合复杂业务与混合团队需求。</p><p>【ONES 官网：<a href="https://link.segmentfault.com/?enc=7nrAkJTBvthL7GNMlCAurw%3D%3D.PxOLi5TykxP9YDLaDu%2Fm7gxPs7PVXuQGuMP09wUole8%3D" rel="nofollow" target="_blank">https://ones.cn/</a> 】</p><p><img width="723" height="436" referrerpolicy="no-referrer" src="/img/bVdnirQ" alt="ONES Wiki 文档协同和知识库管理工具" title="ONES Wiki 文档协同和知识库管理工具"/></p><h4>Confluence——稳定的企业级 Wiki</h4><p>核心功能：包括空间（Space）和页面划分、多级页面结构、版本控制、权限管理、模板与蓝本、历史版本、全文搜索、富文本／表格／宏／流程图嵌入等。与项目管理／需求管理工具（如 Jira）在生态中常有集成。</p><p>适用场景：中到大型组织、已有 Atlassian 生态基础、对文档规范、流程文档、制度文档、架构设计、长期技术／管理文档管理有需求；适合文档规范化、流程制度化、需要稳定可靠文档平台的组织。</p><p>优势亮点：<br/>成熟、稳定、功能全面；适合建立系统化文档体系、规范反馈机制、文档审批、审计与版本控制；<br/>与项目管理工具集成，有助于将文档、任务、需求、缺陷等信息统一管理，实现 traceability；<br/>对于技术 / 管理 /制度文档、规范、安全政策文件等，需要严谨格式、统一管理的内容特别适合。</p><p>局限与挑战：<br/>灵活性、现代体验、结构化／数据库式内容支持较弱；不太适合“结构化条目 + 元数据 + 枚举 + 数据 + 文档混合”的复杂知识形式；<br/>对非文档型、快速变化型、需要轻量、快速响应的团队而言，上手和维护成本较高；<br/>如果仅作为“文档仓库”，与项目／交付流程及工具链分离，知识与执行脱节，也降低沉淀和复用价值。<br/>【官网：<a href="https://link.segmentfault.com/?enc=ceKceiEVDqPrL7c7NSWC5A%3D%3D.KNMSujJ39j7dNE650V5Z%2F9PLr5DMk6SjOzqag5Vuj%2Fd%2Br47QHRQiEtBShEBlwawxrzMlJCcOe3%2BVb36g3qxfPQ%3D%3D" rel="nofollow" target="_blank">https://www.atlassian.com/zh/software/confluence</a> 】<br/><img width="723" height="428" referrerpolicy="no-referrer" src="/img/bVdnirR" alt="" title="" loading="lazy"/></p><h4>GitBook—— 开发文档与技术知识库专家</h4><p>核心功能：以 Markdown 为基础的在线文档与知识库平台，支持文档编辑、版本控制、多用户协作、目录／导航结构、全文搜索、导出、历史版本、评论／审核等。适用于技术文档、API 手册、操作手册、对内／对外文档库等维护。</p><p>适用场景：技术团队、产品团队、需要维护 API 文档、技术规范、用户手册、内部／对外技术文档、轻量／中量级文档库的组织。也适合快速搭建文档库、对文档结构有一定规范要求，但对流程／项目管理要求不高的情况。</p><p>优势亮点：<br/>对开发者友好（Markdown + 版本管理 + 与 Git 思维兼容）；<br/>前端简洁、专注文档本身，适合轻量、中量级文档管理；<br/>适合技术文档/规范/说明书等对格式、结构、可读性有要求的内容；易于对外分享。</p><p>局限与挑战：<br/>不具备复杂权限管理、内容治理、版本审批、任务／项目／交付／流程关联、结构化数据管理等能力；<br/>不适合将知识库作为“公司级知识资产管理 + 知识治理 + 持续维护 + 流程闭环”的平台；<br/>【官网：<a href="https://link.segmentfault.com/?enc=5T44I8q5KHLmGX3uIz9TRQ%3D%3D.pkxplMHOQZQnG7EsrPzvrdG4Rnyj5LVUe2CMjviQS7U%3D" rel="nofollow" target="_blank">https://www.gitbook.com/</a> 】<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdnirV" alt="" title="" loading="lazy"/></p><h4>Tettra——轻量团队内部知识共享平台</h4><p>核心功能：轻量级团队 Wiki / 知识库平台，强调易用性、快速部署、与协作／沟通工具（例如 Slack）集成、知识 Q&amp;A / FAQ /流程说明、标签／分类、全文搜索、共享与协作。适合快速建立团队内部知识共享机制。</p><p>适用场景：小型／中型团队、初创公司、远程／分布式团队、跨职能协作频繁、需要轻量共享内部经验、流程说明、FAQ、SOP 的场景。适合希望快速搭建知识库并降低维护成本的组织。</p><p>优势亮点：<br/>上手门槛低，部署速度快，适合敏捷、灵活、小规模团队；<br/>与协作 / 沟通工具集成，降低使用门槛，提高知识访问频率；<br/>适合动态知识、经验总结、流程说明、FAQ 等轻量／非结构化内容管理。</p><p>局限与挑战：<br/>权限控制、版本管理、文档生命周期管理、审计、内容结构化、分类／标签治理等能力较弱；<br/>随着团队规模扩大、内容体量增长，容易出现混乱、重复、冗余、难以维护的问题；<br/>难以支撑复杂项目、多团队、多角色、长期知识资产化、治理与合规需求。<br/>【官网：<a href="https://link.segmentfault.com/?enc=9ipp5yXjQEd4FRHOBfJhmQ%3D%3D.rSJKCd4lGSquLt66EoliZOScQc8m8t34nNOcCS8LNYA%3D" rel="nofollow" target="_blank">https://tettra.com/</a> 】<br/><img width="723" height="377" referrerpolicy="no-referrer" src="/img/bVdnirW" alt="" title="" loading="lazy"/></p><h4>Notion——灵活的混合内容与协作空间</h4><p>核心功能：模块化工作空间，包括文档、页面、数据库/表格/看板、页面嵌套、模板、数据库视图、任务管理与内容混合管理。适合文档、数据、任务、协作混合管理。</p><p>适用场景：小型／中型团队、跨职能团队、对灵活性、快速响应、混合内容管理（例如文档 + 数据表 + 流程 +任务）的需求较高的组织；适合研发、产品、设计、运营混合团队；适合快速搭建、迭代、试错。</p><p>优势亮点：<br/>灵活、模块化、高度自定义，能够适应快速变化、需求不确定的业务环境；<br/>支持混合内容（文档 + 数据 +任务 +看板 +流程）；适合多角色、多职能协作团队；<br/>用户友好，界面现代，适合非技术背景的团队成员。</p><p>局限与挑战：<br/>权限治理、结构化治理、审计与合规能力弱，不适合对文档安全性、审批流程、长期维护有严格要求的组织；<br/>随着内容与团队规模扩大，容易出现分类混乱、权限混乱、内容重复与冗余、缺乏结构化治理。<br/>【官网：<a href="https://link.segmentfault.com/?enc=2HpJ2BE0LPa%2Fyr05rScJjg%3D%3D.Vw9w7EdZ9OgLKvhBSljT2or6kvjbCrvsYTaP5%2BgnEE0%3D" rel="nofollow" target="_blank">https://www.notion.com/</a> 】<br/><img width="723" height="474" referrerpolicy="no-referrer" src="/img/bVdnirY" alt="" title="" loading="lazy"/></p><h4>Nuclino——简洁轻量的团队知识库</h4><p>核心功能：轻量团队协作／知识库工具，支持实时协作、多用户编辑、标签／分类、知识图谱／知识关系地图、全文搜索、版本历史、简单结构化与导航。适合构建内部知识库、团队 Wiki、经验共享库。</p><p>适用场景：初创／中小型团队、分布式团队、跨职能协作、希望快速建立知识共享和协作机制、内容体量适中、结构不复杂的组织。</p><p>优势亮点：<br/>界面简洁、上手成本低；适合快速启动知识管理；<br/>支持标签、分类、知识关系图谱／地图，便于知识结构化和关联；<br/>实时协作、多人编辑、快速编辑 / 更新，适合动态、频繁变化的知识内容。</p><p>局限与挑战：<br/>权限控制、内容治理、版本审批、审计、安全合规、长期维护机制缺乏；<br/>对复杂组织结构、多团队、多角色、合规审计、多项目交付的组织支持不足；<br/>【官网：<a href="https://link.segmentfault.com/?enc=D4nS%2BtauHWaaQZ8eW3IRnQ%3D%3D.jfSb63qmV%2FEyqfUZOmyeoltbtVn6V3axvwkEGeGvHz4%3D" rel="nofollow" target="_blank">https://www.nuclino.com/</a> 】<br/><img width="723" height="438" referrerpolicy="no-referrer" src="/img/bVdnir1" alt="" title="" loading="lazy"/></p><h2>从战略视角看：关键维度对比与决策要素</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459233" alt="图片" title="图片" loading="lazy"/></p><h2>工具之外：构建组织知识管理能力的战略框架</h2><p>作为管理者，我建议将知识库建设作为组织战略能力建设的一部分，而不仅仅是工具部署。以下是必须同步建设的能力与机制。</p><h4>知识文化 &amp; 贡献机制 —— 知识不是“写一次就完事”</h4><p>责任与所有权：明确谁负责文档撰写、谁负责审核、谁负责更新／归档。知识不是某个人的副产品，而是组织的资产。</p><p>激励与制度化：通过绩效、考核、奖励机制，鼓励团队贡献与维护文档；将文档／知识产出／更新纳入项目交付／迭代流程 — 即“项目完成 + 文档归档”成为标准步骤。</p><p>标准、模板与规范体系：制定统一文档模板、分类／标签体系、版本管理规则、内容生命周期定义、审查与归档流程。保证文档风格一致、可管理、可检索、易维护。</p><h4>与项目／DevOps／工具链深度融合</h4><p>构建知识 ↔ 流程 ↔ 执行 ↔ 复盘闭环。</p><p>将知识库与代码仓库、CI/CD、测试／发布／运维工具、项目管理系统集成，使经验／方案／决策／复盘／文档与交付流程关联 — 打通“需求 → 设计 → 实施 → 复盘 → 文档／知识沉淀 → 下一轮复用”的闭环。</p><p>确保每个项目、每次交付、每次复盘都有文档与知识沉淀产出，形成可持续的知识积累机制，使知识库真正成为组织基础设施的一部分。</p><h4>数据驱动与知识资产分析 —— 可衡量、可优化</h4><p>对知识库的使用情况（访问频率、文档活跃度、内容覆盖率、过期文档、贡献者分布、更新频率等）进行监控与统计。</p><p>将这些数据与业务 / 项目绩效（交付周期、缺陷率、新人上手速度、重复问题数量等）关联分析，以量化知识库对效率、质量、协作、风险降低等方面的贡献 — 即衡量知识库 ROI。</p><p>基于分析结果，识别知识薄弱领域、制定补充／优化策略、调整文档结构与内容、优化流程与治理机制，推动持续改进。</p><h4>可扩展性 / 未来适应性 —— 为组织长期发展留足弹性</h4><p>随着组织规模增长、团队分布广、业务复杂度提升、合规需求上升、国际化发展，需要支持灵活扩展、模块化治理、多租户／多团队管理、多语言、多地域协作、权限分级、审计合规、知识图谱、多内容类型、移动／云端访问。</p><p>同时，应规划适应未来趋势 — 支持 AI / NLP / 知识图谱 / 智能搜索 / 智能推荐 / 自动分类 / 内容质量检查 / 跨系统同步等能力，使知识库持续进化为“智能知识资产管理平台”。</p><h2>知识库建设，是组织能力建设，而不仅仅是工具部署</h2><p>当下，知识库工具众多，从轻量、灵活、快速部署，到企业级、功能全面、治理规范。关键不在于“哪个工具最流行”，而在于是否与组织的阶段、规模、治理需求、未来规划契合。</p><p>真正能够带来组织效能提升与竞争力增强的，不是某一个好用的工具，而是：</p><ol><li>一个 制度化 + 文化化 的知识贡献与管理机制；</li><li>一个 与项目 / DevOps / 工具链深度融合 的知识闭环体系；</li><li>一个 将知识视为组织资产 的理念体系，具备 数据驱动、可衡量、可治理、可复用、可持续 的知识管理能力；</li><li>一个 具备扩展性与未来适应性 的平台化／架构化布局，为组织长期发展留足空间。</li></ol><p>因此，对于中大型、B2B、业务线复杂、团队多元、强调数据驱动与协同效率的研发组织，应把知识库建设视为战略基础设施 — 把“知识库”当作“知识资产管理 + 组织能力建设”的长期项目来规划。</p><p>与其纠结“今天选择哪个工具”，不如先问自己三个问题：</p><ul><li>我们希望未来的研发组织是什么样子？</li><li>我们希望知识库在未来承载怎样的能力与价值？</li><li>我们是否准备好为知识管理设立制度、流程、责任与文化？</li></ul><p>当你对这些问题有明确答案时，再去选工具、建机制、落实推进 — 将比仅仅选一个“好工具”更具战略价值。</p>]]></description></item><item>    <title><![CDATA[『京墨文库』鸿蒙版上线！ hefengbao ]]></title>    <link>https://segmentfault.com/a/1190000047459251</link>    <guid>https://segmentfault.com/a/1190000047459251</guid>    <pubDate>2025-12-08 19:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>用了二十多天，边学习边做项目，使用官方提供的 ArkTS、ArkUI、ArkData、ArkWeb、NetworkKit 等技术栈开发的原生 APP，完成了第一个版本的基础功能，相对于 Android 版而言，功能有些单薄，以后一点一点迭代添加了。先发布一个版本，看看使用情况。</p><p>之前使用 Android Jetpack Compose 、 Room 、Datastore(Preference) 等技术实现了 Android 版，对照之下对于 ArkUI、ArkData 等技术理解起来也轻松一点，虽然有些磕磕绊绊，但还是完成了基础的功能。</p><p>申请上架 3 次被驳回，第 4 次终于成功上架华为应用市场，如果使用华为系手机和纯血鸿蒙系统（HarmonyOS  NEXT），感兴趣的话可以下载试一下。</p><p><img width="723" height="222" referrerpolicy="no-referrer" src="/img/bVdnitV" alt="" title=""/></p><p>最低支持的 HarmonyOS 版本选择了 5.1.1(19)，研究了版本变迁列表，这是 HarmonyOS 5 最新的版本，既然都用了 HarmonyOS 5 系统，那升级到最新版也是不错的选择吧😄。</p><p><img width="723" height="414" referrerpolicy="no-referrer" src="/img/bVdnitW" alt="" title="" loading="lazy"/></p><p>项目仍然是开源的：</p><p><a href="https://link.segmentfault.com/?enc=iA0TeZISNnqvM9ri5nuYWA%3D%3D.eaMu4oBFvsYmKeIdUJ4UTelxcHR8jmwDn8%2FcvqsDqolAiJULeX14m70cZWQorIi2zbL%2BN3PWVNxsosjSzi2YAg%3D%3D" rel="nofollow" target="_blank">https://github.com/hefengbao/jingmo-for-HarmonyOS</a></p><p><a href="https://link.segmentfault.com/?enc=qv0ToI6AS54EDKB8hHzlRQ%3D%3D.kFbn3UI8n3DxgluWv87nAmbqUF1yylxvVqwCaKVS96pWGV9AGuc4tY6FZj%2FVMfptBtUnfnpdkl9GO8j1lAUOGA%3D%3D" rel="nofollow" target="_blank">https://gitee.com/hefengbao/jingmo-for-HarmonyOS</a></p><p>另外也附上 Android 版的仓库：</p><p><a href="https://link.segmentfault.com/?enc=57W%2F93L9X0Hj%2Fry7adcUpw%3D%3D.nk1ao%2FQ14kLFyRnqb6J8BSZlsjRbfOL9tAqqxpr7DMgZZoz14m8NLnJcrBeHC3m5" rel="nofollow" target="_blank">https://gihub.com/hefengbao/jingmo</a></p><p><a href="https://link.segmentfault.com/?enc=mGd%2BE%2BB8vI3loAhusZeZeQ%3D%3D.a%2FhBHBqUompydzC%2B8A2TDqx2XzICU4XwpoHo3nYcp%2FBxSPw2TJcNYmR9tziYIGDl" rel="nofollow" target="_blank">https://gitee.com/hefengbao/jingmo</a></p><p>IDE 使用官方提供的 DevEco Studio，模拟器用起来都挺方便。但也遇到过一些问题：</p><p><strong>经检测发现，您的应用使用了HarmonyOS beta版本的API。</strong></p><p>修改建议：为提升消费者使用体验，请使用HarmonyOS release版本的API开发应用，申请上架。请参考版本说明集成release版本API：<a href="https://link.segmentfault.com/?enc=lEty9bw6VUjcTkwAu0LNDw%3D%3D.2D2w1SiZBDOl309DcU%2Fm7EJdS1dqGgkubdvLVCMcwFg4uA17vYNWyxjrBzcOMPmdJ5FHvQkuEEB3n4yjB%2F1Jnun4nzldwpSr3Jpg%2F7%2BFfyGWFrLMiJj8zAZo8d7t0DD%2F" rel="nofollow" target="_blank">https://developer.huawei.com/consumer/cn/doc/harmonyos-releas...</a></p><p>解决：下载使用 release 版本的 IDE</p><p><strong>Navigation 添加了路由后不生效</strong></p><p>点击 “构建” - “清理构建” 后，重新运行项目。</p>]]></description></item><item>    <title><![CDATA[AI 招聘智能体核心功能清单 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047459285</link>    <guid>https://segmentfault.com/a/1190000047459285</guid>    <pubDate>2025-12-08 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI 招聘智能体核心功能清单<br/>AI招聘智能体：重塑招聘决策的智能化变革<br/>传统招聘正逐渐成为企业发展的“隐性天花板”，人才短缺、筛选耗时、复试标准不一、候选人体验不佳等问题频发。与此同时，AI智能体在人力资源管理领域的应用加速渗透，为企业带来降本增效、业务协同、提升员工体验、数据化决策四大核心战略价值，推动招聘行业迎来根本性变革。</p><p>招聘智能化的核心：精准与体验双突破<br/>AI招聘智能体的核心竞争力集中在“精准”与“体验”两大维度，针对性解决传统招聘低效、主观、成本高的核心痛点，成为企业决胜招聘的关键支撑。<br/>精准评估：告别“凭感觉”的科学决策<br/>AI招聘智能体的打分体系经过多重实证验证，不仅能与企业面试官进行一对一“背靠背”比对，还通过效标效度与重测信度双重验证，评估结果可直接作为招聘决策依据，而非单纯的辅助参考。<br/>这种精准性贯穿招聘全环节：<br/>•一问多能，一道题目即可同步评估多项能力，无需HR初筛与技术复试反复衔接，评估效率提升50%以上。<br/>•具备自由追问能力，根据候选人回答生成针对性问题，如同资深面试官般精准捕捉核心能力点。<br/>•自动深度挖掘简历信息，抓取关键亮点与模糊疑点，通过递进式提问杜绝造假行为，同时避免遗漏优质候选人。<br/>•覆盖通用能力与专业领域考察，从沟通协作到编程算法、工程财务等专业技能，均可精准出题评估，大幅减轻HR与专业面试官的工作负担。<br/>体验升级：让面试成为雇主品牌加分项<br/>传统AI面试因机械、生硬的交互模式备受诟病，新一代AI招聘智能体通过“拟人化交互”彻底优化候选人体验：<br/>•能识别候选人的语速、情绪及暗含信息，通过有效引导帮助候选人稳定发挥，展现真实能力。<br/>•自动识别答题状态，无需手动点击操作，全程无打断，营造自然流畅的交流氛围。<br/>•实现语音与口型精准同步，打造沉浸式视觉体验，摆脱“纸片人AI”的刻板印象。<br/>•支持多轮对话答疑，实时回应候选人关于岗位职责、福利待遇、招聘流程等疑问，有效提升入职意愿。<br/>优质的面试体验已不再是附加项，而是企业展示雇主品牌、吸引优质人才的重要竞争力。<br/>从自动化到“自动识人”：招聘全流程智能闭环<br/>AI人才寻访智能体的出现，将招聘流程从单纯的自动化推向“自动识人”的新阶段，构建起完整的智能招聘流水线：<br/>•配置便捷，30-60秒即可启动使用，开启后无需人工值守。<br/>•按年龄、学历、薪资等预设条件自动筛选简历，精准识别符合要求的候选人。<br/>•模拟人类语气与候选人动态沟通，具备提问、交流、筛选淘汰的完整交互能力。<br/>•全覆盖处理未读消息，逐条进行个性化回复，确保沟通无遗漏。<br/>•拟人化交互细节拉满，缺简历时主动请求投递，模仿真实打字节奏交流，增强沟通自然感。<br/>•自动下载简历并上传至企业ATS系统，生成完整候选人档案，同时保障数据流转安全。<br/>这一闭环体系将招聘中的“经验型判断”彻底升级为“数据型决策”，推动招聘流程更科学、更高效。<br/>拥抱AI招聘：把握行业变革机遇<br/>AI技术正在重构招聘行业的底层逻辑，传统依赖人工的招聘模式已难以适应企业快速发展的需求。AI招聘智能体通过精准评估提升招聘质量，通过流程优化降低时间与人力成本，通过体验升级强化雇主品牌，全方位破解传统招聘痛点。<br/>对于企业而言，拥抱招聘智能化已不是可选项，而是在人才竞争中占据优势的必然选择，更是顺应行业发展趋势、突破增长瓶颈的关键举措。</p>]]></description></item><item>    <title><![CDATA[【赵渝强老师】TiDB的备份恢复策略 赵渝强老师 ]]></title>    <link>https://segmentfault.com/a/1190000047458791</link>    <guid>https://segmentfault.com/a/1190000047458791</guid>    <pubDate>2025-12-08 18:12:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>数据库在运行过程中会出现各种故障，因此对数据库进行必要的备份是非常重要的。有了数据库的备份就可以在数据库出现错误时保证数据的安全。因此TiDB数据库提供了强大的数据库备份与恢复机制。</p><p>基于Raft协议和合理的部署拓扑规划，TiDB实现了集群的高可用，当集群中少数节点挂掉时，集群依然能对外提供服务。在此基础上，为了更进一步保证用户数据的安全，TiDB还提供了集群的备份与恢复(Backup &amp; Restore,BR)功能，作为数据安全的最后一道防线，使得集群能够免于严重的自然灾害，提供业务误操作“复原”的能力。</p><p>TiDB备份恢复功能可以用于满足以下业务的需求：</p><ul><li>备份集群数据到灾备系统，并保证Recovery Point Objective(RPO)低至5分钟，减少灾难场景下数据的丢失。</li><li>处理业务数据写错的案例，提供业务操作的“复原”能力。</li><li>审计业务的历史数据，满足司法审查的需求。</li><li>复制(Clone)生产环境，方便问题诊断、性能调优验证、仿真测试等。</li></ul><p>TiDB支持四种备份恢复策略，分别是：全量（快照）备份与恢复、日志备份与恢复、数据的逻辑导出和导入和闪回。下面分别进行介绍。</p><p>视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV1ph2rBKEJG/?aid=115682142328793&amp;cid=34585247791" target="_blank">https://www.bilibili.com/video/BV1ph2rBKEJG/?aid=115682142328...</a></p><h2>一、 全量（快照）备份与恢复</h2><p>全量备份是对集群某个时间点的全量数据进行备份，TiDB的全量备份也可以叫做快照备份。因为TiDB集群快照数据包含某个物理时间点上集群满足事务一致性的所有数据。</p><p>全量备份一般会占用较大的存储空间，且只包含某个时间点的集群数据。执行tiup br backup full命令，可以备份TiDB最新的或者指定时间点的快照数据。执行tiup br backup full --help可获取该命令的使用帮助。下面的步骤将对数据库集群进行全量备份。</p><p>（1）创建一个目录用于保存集群快照备份产生的文件</p><pre><code class="powershell">mkdir -p /backup/snapshot/full
chown -R tidb:tidb /backup/snapshot/full</code></pre><p>（2）执行备份集群快照</p><pre><code class="powershell">tiup br backup full \
    --pd "192.168.79.10:2379" \
    --storage "local:///backup/snapshot/full" \
    --log-file /backup/snapshot/full/backupfull.log
    
# 输出的信息如下：
Starting component br: 
         /root/.tiup/components/br/v8.5.1/br backup full \
         --pd 192.168.79.10:2379 \
         --storage local:///backup/snapshot/full \
         --log-file /backup/snapshot/full/backupfull.log
Detail BR log in /backup/snapshot/full/backupfull.log 
Full Backup &lt;-------------------------------&gt; 100.00%
Checksum &lt;----------------------------------&gt; 100.00%</code></pre><p>（3）查看产生的快照备份文件。</p><pre><code class="powershell">tree /backup/snapshot/full/

# 输出的信息如下：
/backup/snapshot/full/
├── 5
│   ├── ......
│   ├── 32_235_e4f50bb7685_1739865447022_write.sst
│   ├── 32_235_e5f8771bee7_1739865446968_write.sst
│   ├── 32_235_ea38515343d_1739865446917_write.sst
│   ├── 32_235_ed85b58a2c9_1739865447042_default.sst
│   ├── 32_235_ed85b58a2c9_1739865447042_write.sst
│   ├── 32_235_f0f9b1a4aa3_1739865446926_write.sst
│   ├── 32_235_f4dd8b9e556_1739865446922_write.sst
│   ├── 32_235_f70c98a950f_1739865446864_write.sst
│   ├── 32_235_fd54db249c0_1739865446984_write.sst
│   ├── ......
│   └── 32_235_fe4f4fe208b_1739865446841_write.sst
├── backupfull.log
├── backup.lock
├── backupmeta
├── backupmeta.datafile.000000001
├── backupmeta.json
├── backupmeta.schema.000000002
└── checkpoints
    └── backup</code></pre><p>快照备份会产生如下类型文件：</p><ul><li><strong>SST文件</strong>：存储TiKV备份下来的数据信息。单个SST文件大小等于TiKV Region的大小。SST是Static Sorted Table的缩写。</li><li><strong>Backup meta文件</strong>：存储本次备份的元信息，包括备份文件数、备份文件的Key区间、备份文件大小和备份文件Hash(sha256)值。</li><li><strong>backup.lock文件</strong>：用于防止多次备份到同一目录。</li></ul><p>当备份数据到本地磁盘上时，SST文件以下面的格式命名。其中</p><ul><li><strong>regionID</strong>：Region编号</li><li><strong>regionEpoch</strong>：Region版本号</li><li><strong>keyHash</strong>：Range startKey的Hash(sha256)值，以确保唯一性</li><li><strong>timestamp</strong>：TiKV节点生成SST文件名时刻的Unix时间戳</li><li><strong>cf</strong>：RocksDB的列族信息，取值：default或者write</li></ul><p>完整的SST文件名格式如下：</p><pre><code class="powershell">&lt;regionID&gt;_&lt;regionEpoch&gt;_&lt;keyHash&gt;_&lt;timestamp&gt;_&lt;cf&gt;.sst</code></pre><h2>二、 日志备份与恢复</h2><p>全量备份一般会占用较大的存储空间，且只包含某个时间点的集群数据。如果需要灵活地选择恢复的时间点（即：实现PITR，Point in Time Recovery），可以使用日志备份和日志恢复。有了日志备份后，通过tiup br restore point功能，可以指定要恢复的时间点。BR会自动判断和读取恢复需要的数据，然后将这些数据依次恢复到指定的集群。执行tiup br log命令来开启和管理日志备份任务，下面展示了该命令的帮助信息：</p><pre><code class="powershell"># tiup br log --help

Usage:
  br log [command]

Available Commands:
  metadata    查询备份存储中备份数据的元信息
  pause       暂停日志备份任务
  resume      重启暂停的备份任务
  start       启动一个日志备份任务
  status      查询日志备份任务状态
  stop        停止备份任务
  truncate    从备份存储中清理日志备份数据</code></pre><p>执行tiup br log start命令可以在备份集群启动一个日志备份任务。该任务在TiDB集群持续地运行，及时地将KV变更日志保存到备份存储中。执行tiup br log start --help命令可获取该子命令使用介绍：</p><pre><code class="powershell"># tiup br log start --help

start a log backup task

Usage:
  br log start [flags]

Flags:
  -h, --help               展示帮助信息
      --start-ts string   指定开始备份日志的起始时间点。
如果未指定，则选取当前时间作为start-ts
      --task-name string  指定日志备份任务名。
该名称也用于查询备份状态、暂停、重启
和恢复备份任务等操作

Global Flags:
  -u, --pd strings        指定备份集群的PD访问地址。
  -s, --storage string   指定备份存储地址。
  ......</code></pre><p>下面的步骤将启动一个日志备份任务。<br/>（1）创建一个目录用于保存日志备份产生的文件</p><pre><code class="powershell">mkdir -p /backup/log
chown -R tidb:tidb /backup/log</code></pre><p>（2）启动日志备份任务</p><pre><code class="powershell">tiup br log start \
  --task-name=pitr \
  --pd="192.168.79.10:2379" \
  --storage='local:///backup/log'

# 输出的信息如下：
Starting component br: 
  br log start --task-name=pitr \
               --pd=192.168.79.10:2379 \
               --storage=local:///backup/log
Detail BR log in /tmp/br.log.2025-02-18T18.04.50+0800 
[2025/02/18 18:04:50.647 +08:00] [INFO] ["log start success summary"] </code></pre><p>（3）查看目录/backup/log</p><pre><code class="powershell">tree /backup/log

# 输出的信息如下：
/backup/log
├── backup.lock
└── backupmeta</code></pre><p>（4）查看日志备份任务的状态信息。</p><pre><code class="powershell">tiup br log status --task-name=pitr --pd="192.168.79.10:2379"

# 输出的信息如下：
● Total 1 Tasks.
&gt; #1 &lt;
              name: pitr
            status: ● NORMAL
             start: 2025-02-18 18:04:50.561 +0800
               end: 2090-11-18 22:07:45.624 +0800
           storage: local:///backup/log
       speed(est.): 0.00 ops/s
checkpoint[global]: 2025-02-18 18:07:18.411 +0800; gap=47s

命令输出中的字段含义如下：
● status：任务状态，包括NORMAL（正常）、ERROR（异常）和PAUSE（暂停）三种状态。
● start：日志备份任务开始的时间，该值为备份任务启动时候指定的start-ts。
● storage：备份存储。
● speed：日志备份任务的总QPS（每秒备份的日志个数）。
● checkpoint[global]：集群中早于该checkpoint的数据都已经保存到备份存储，它也是备份数据可恢复的最近时间点。</code></pre><p>（5）再次查看目录/backup/log</p><pre><code class="powershell">tree /backup/log

# 输出的信息如下：
/backup/log
├── backup.lock
├── backupmeta
└── v1
    ├── 20250218
    │   └── 10
    │       └── 1
    │           ├── 456097302173712388-2dd0ae7b-e8c7-4656-a497-e0e02d0e4fe4.log
    │           └── 456097333619195905-7bd1f194-ba97-4824-94e9-45f223382d82.log
    ├── backupmeta
    │   ├── 456097317141872643-0af28b08-6cc4-4123-af8a-f8798e967378.meta
    │   └── 456097332870512641-ce854c81-95ab-444e-91fd-e9e9fb8d2e58.meta
    └── global_checkpoint
        ├── 1.ts
        ├── 4.ts
        └── 5.ts

6 directories, 9 files</code></pre><p>日志备份会产生如下类型文件：</p><ul><li><strong>{min_ts}-{uuid}.log文件</strong>：存储备份下来的kv数据变更记录。其中{min_ts}是该文件中所有kv数据变更记录数对应的最小ts；{uuid}是生成该文件的时候随机生成的。</li><li><strong>{checkpoint_ts}-{uuid}.meta文件</strong>：每个TiKV节点每次上传日志备份数据时会生成一个该文件，保存本次上传的所有日志备份数据文件。其中{checkpoint_ts}是本节点的日志备份的checkpoint，所有TiKV节点的最小的checkpoint就是日志备份任务最新的checkpoint；{uuid}是生成该文件的时候随机生成的。</li><li><strong>{store_id}.ts文件</strong>：每个TiKV节点每次上传日志备份数据时会使用global checkpoint ts更新该文件。其中{store_id}是TiKV的storeID。</li><li><strong>v1_stream_truncate_safepoint.txt文件</strong>：保存最近一次通过br log truncate删除日志备份数据后，存储中最早的日志备份数据对应的ts。</li></ul><h2>三、 数据的逻辑导出和导入</h2><p>在备份与恢复场景中，如果需要全量备份少量数据且不要求备份速度，还可以使用Dumpling从TiDB数据库导出数据进行备份，再使用TiDB Lightning将数据导入至TiDB数据库实现恢复。</p><blockquote>Dumpling和TiDB Lightning属于逻辑备份与逻辑恢复，因此适用于数据量较小的情况，例如小于50G的数据。</blockquote><p>数据导出工具Dumpling可以把存储在TiDB或MySQL中的数据导出为SQL或CSV格式，用于逻辑全量备份。Dumpling也支持将数据导出到Amazon S3中。</p><p>TiDB Lightning是用于从静态文件导入TB级数据到TiDB集群的工具，常用于TiDB集群的初始化数据导入。TiDB Lightning 支持的文件类型有：Dumpling生成的文件、CSV文件和Parquet文件</p><h2>四、 TiDB的闪回</h2><p>TiDB修改数据时并不会将旧版本数据之间删除，而是在新旧数据上打上不同的版本号，从而实现了MVCC基准。当旧版本数据满足了GC垃圾回收的触发条件时，TiDB才会将旧版本数据彻底删除。换句话说，在GC垃圾回收旧版本数据之前，任然可以读取旧版本数据从而达到恢复的目的。这就是闪回的核心机制，它是一种轻量级的恢复技术，不需要备份即可完成。通过查询系统变量tidb_gc_life_time可以获取旧版本数据保留的时间，默认10分钟。</p><pre><code class="sql">tidb&gt; select @@tidb_gc_life_time ;
+---------------------+
| @@tidb_gc_life_time |
+---------------------+
| 10m0s               |
+---------------------+</code></pre><p>下面的查询语句将查询当前的安全点（safePoint），即GC已经清理到的时间点。换句话说，该时间点以后的数据都可以恢复。</p><pre><code class="sql">tidb&gt; select * from mysql.tidb where variable_name = 'tikv_gc_safe_point' \G;
*************************** 1. row ***************************
 VARIABLE_NAME: tikv_gc_safe_point
VARIABLE_VALUE: 20250307-21:50:44.781 +0800
       COMMENT: All versions after safe point can be accessed. (DO NOT EDIT)
1 row in set (0.01 sec)</code></pre><p>TiDB中支持闪回集群、闪回数据库和闪回表三种不同的闪回。</p>]]></description></item><item>    <title><![CDATA[云原生网关 Higress 与服务注册 Nacos 的创新结合：打造零代码扩展 AI 工具的能力 阿]]></title>    <link>https://segmentfault.com/a/1190000047458821</link>    <guid>https://segmentfault.com/a/1190000047458821</guid>    <pubDate>2025-12-08 18:11:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：陆胤任</p><h2>背景</h2><p>在 AI 大爆发的时代，已经有非常多的 AI 助手，结合 RAG 通过智能问答帮助用户解答问题。单纯地依靠智能问答帮助客户自助解答是远远不够的，我们需要让 AI 助手能够直接调用已有的丰富接口，朝着更强大的智能体演进。我们选用当下最为火热，且已逐步成为标准的 MCP 作为模型和接口之间通信的传输协议。关于 MCP，已有非常多的介绍文章，本文不再赘述。</p><p>在企业对外服务的场景下，MCP Server 需要解决以下几个问题：</p><ol><li>在服务的多实例高可用场景下，使用 SSE 通信方式如何维护 session；</li><li>如何做到动态更新 MCP 工具 Prompt，做到快速更新&amp;调试&amp;验证；</li><li>租户隔离的云服务场景下如何对用户的工具调用进行鉴权。</li></ol><p>Higress 可以很好地解决上面的问题 1，同时还有完善的运维监控体系，可视化易操作的控制台界面。为了解决问题 2，我们引入了 Nacos 负责注册后端服务以及管理维护 MCP 工具的元数据等信息。在整个 MCP 服务中，Higress 担任 MCP Proxy 的角色，Nacos 担任 MCP Registry 的角色。对于问题 3 租户隔离问题，会在下面鉴权章节中进行详细说明。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458823" alt="image" title="image"/></p><p>Higress 和 Nacos 都是云原生的应用，在部署方面，自然选择使用 K8s 集群进行云原生部署。同时很多企业有自己的专属生产网络环境，一般和外网不通，因此本文会围绕如何利用社区版本的 Higress 和 Nacos（Apache-2.0 开源协议）进行私有化部署。因为内部环境的限制，我们没有办法直接通过 Helm 操作 K8s 集群进行部署，因此本文会围绕如何基于 Higress 和 Nacos 的 docker 镜像在 K8s 集群上进行分角色部署。</p><p>通过这套自建的网关服务，使用配置即可实现零代码扩展 Tool，新应用的注册、应用下面工具的扩展、工具 prompt 更新验证都能通过服务集成的可视化控制台，更新发布配置快速完成，<strong>接入方式极其简单！更新验证极其快速！</strong> 同时利用 Nacos 的命名空间能力可以做到服务和工具集的隔离，给不同的用户提供不同的 MCP 工具集。</p><h2>私有化部署</h2><h3>Higress</h3><p>Higress 支持三种部署方式：Helm、docker compose 和基于 all-in-one 的 docker 镜像进行部署。Higress 官方推荐使用 Helm 的方式进行生产环境的部署，将依赖的模块部署在不同的 pod 上。而因上述环境原因，这里选择使用第三种基于 all-in-one 的 docker 镜像 Dockerfile <strong>[</strong> <strong>1]</strong> 进行部署，将 Higress 依赖的组件以进程的方式部署在同一 pod 上面，通过多副本的方式实现服务高可用，也实现了对 K8s 集群 Ingress 的无侵入式部署。</p><p>我们先尝试直接引用 docker 镜像进行部署时，会报 WASM 的插件错误，查看报错信息是通过 oci 地址去下载 WASM 插件的时候出现了问题。同时 Higress 实现 MCP 功能也依赖了 WASM 插件，这是一个绕不开的问题。﻿</p><pre><code>FROM higress-registry.cn-hangzhou.cr.aliyuncs.com/higress/all-in-one:latest</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458824" alt="image" title="image" loading="lazy"/></p><h4>WASM 插件独立部署</h4><p>Higress 的 plugin-server <strong>[</strong> <strong>2]</strong> 项目就是为了“解决私有化部署 Higress 网关时拉取插件的痛点，优化了插件的下载与管理效率”，使 Higress 通过 http 的方式去下载独立部署的插件库，而不是通过 oci 去访问外部公开仓库，避免因网络问题导致插件拉取不下来。解决过程主要分为以下三个步骤：</p><p><strong>1）私有化部署 plugin-server</strong></p><pre><code>FROM higress-registry.cn-hangzhou.cr.aliyuncs.com/higress/plugin-server:1.0.0</code></pre><p><strong>2）为 plugin-server 集群申请 K8s Service（Cluster IP）</strong></p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: higress-plugin-server
  namespace: higress-system
  labels:
    app: higress-plugin-server
    higress: higress-plugin-server
spec:
  type: ClusterIP
  ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
  selector:
    app: higress-plugin-server
    higress: higress-plugin-server</code></pre><p>K8s 集群内置的 DNS 为此创建的域名解析记录的格式为 <code>&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code>。</p><p>在没有 K8s 的场景下也可以为 plugin-server 集群申请内网 VIP 或者 SLB 做好服务发现和负载均衡。</p><p><strong>3）修改 Higress 内置插件下载地址</strong></p><p>依照 github 中的示例，在基于 Higress 镜像的项目 Dockerfile 中声明插件的下载地址。这里有个地方需要注意下，readme 中给出的示例是环境变量的格式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458825" alt="image" title="image" loading="lazy"/></p><p>在 Dockerfile 中声明需要转义一下，&amp;dollar;{name}/&amp;dollar;{version} 的形式才可以被正确解析。</p><pre><code>...
# 模版
ENV HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN=http://[申请的k8s service地址]/plugins/\${name}/\${version}/plugin.wasm
# mcp wasm 插件下载地址
ENV MCP_SERVER_WASM_IMAGE_URL=http://[申请的k8s service地址]/plugins/mcp-server/1.0.0/plugin.wasm
...</code></pre><p>配置完独立的插件 HTTP 下载地址后重新部署，在服务器上可以看到 8080 端口以及 8443 端口可以被正常监听，说明 Higress 具备代理和网关功能的核心数据面组件已经可以正常服务了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458826" alt="image" title="image" loading="lazy"/></p><p>解决完 WASM 插件下载问题，基于 docker 镜像的 Higress 服务就可以被成功拉起并运行了。只不过基于这种模式部署的<strong>每个 pod 都是独立、对等、包含全部组件、功能完整的 Higress 服务，需要通过多副本的方式实现高可用。</strong></p><p>这种部署模式下，通过 Higress 自身集成的控制台去运维服务&amp;更改配置是不现实的，只能操作一台实例的配置变更，无法让实例间进行配置同步。因此在这种模式下的缺点是，只能通过在项目代码中维护配置文件，需要更改时走发布流程，将配置发布到每台实例上面。不过在我们这个场景下，需要变更配置的情况不多。</p><h4>粘性会话</h4><p>在 MCP SSE 通信方式下，天然需要解决粘性会话的问题，Higress 基于 Redis 帮我们解决了这个问题。提前部署好 Redis 实例之后，打开 Higress 的 MCP 功能，并将 Redis 配置更新进去，重新部署一下就可以使用 MCP 的功能了。</p><pre><code>...
data:
  higress: |-
    mcpServer:
      enable: true
      sse_path_suffix: /sse
      redis:
        address: xxx.redis.zhangbei.rds.aliyuncs.com:6379
        username: ""
        password: "xxx"
        db: 0
...</code></pre><p>这份配置文件可以维护在自己的基于 Higress 镜像的项目中，在部署的时候将配置文件 COPY 到指定目录（这种部署模式下，所有的配置文件都应该这么做）。</p><pre><code>...
# custom config
COPY config/configmaps/higress-config.yaml /data/configmaps/higress-config.yaml
COPY config/mcpbridges/default.yaml /data/mcpbridges/default.yaml
COPY config/secrets/higress-console.yaml /data/secrets/higress-console.yaml
RUN chmod +x /data/configmaps/higress-config.yaml &amp;&amp; \
    chmod +x /data/secrets/higress-console.yaml &amp;&amp; \
    chmod +x /data/mcpbridges/*
...</code></pre><p>当整个 MCP 网关搭建完并使用的时候，在 redis 上通过 PSUBSCRIBE mcp-server-sse:* 命令可以看到如下的调用信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458827" alt="image" title="image" loading="lazy"/></p><h4>自定义构建镜像</h4><p>官方构建出来的镜像一般会要求体积小，满足最小运行要求，所以很多功能其实并不集成在 Higress 的镜像中。如果你的企业有自己约定的通用镜像，或者是想在原本的基础上集成一些新的功能，如使用阿里云的 SLS、云监控等功能，就需要根据 all-in-one 镜像的 Dockerfile 内容进行自定义构建。这里有个注意的点是，Higress 中的 envoy 模块要求的 glibc 是 2.18 及以上版本。</p><p>其实只需要将 Higress 的 Dockerfile 文件内容移植过来就行，然后<strong>再声明下独立部署的 WASM 插件下载地址</strong>，就能实现基于指定镜像进行 Higress 自定义构建打包部署了。</p><p>Higress 服务搭建好后，就可以走对外公网访问的流程了：（1）一个是绑定 8001 端口，通过 Higress 控制台进行查看相关配置的域名，限制为只允许内网访问。注：这种模式下无法通过控制台直接去更改配置；（2）另一个是绑定 8080 端口，对外提供 MCP 网关服务的域名。</p><p>完整的 Dockerfile 如下：</p><pre><code>FROM [企业内部基础镜像]
# 下面为 Higress all-in-one dockerfile中的内容
ARG HUB=higress-registry.cn-hangzhou.cr.aliyuncs.com/higress
...
# 模版
ENV HIGRESS_ADMIN_WASM_PLUGIN_CUSTOM_IMAGE_URL_PATTERN=http://[申请的k8s service地址]/plugins/\${name}/\${version}/plugin.wasm
# mcp wasm 插件下载地址
ENV MCP_SERVER_WASM_IMAGE_URL=http://[申请的k8s service地址]/plugins/mcp-server/1.0.0/plugin.wasm
...
# 注意 dockerfile 中会去 github 下载对应处理器架构下的 yq 模块，企业内网环境下可以提前下载下来
COPY ./yq_linux_[arch] /usr/local/bin/yq
...
# custom config
COPY config/configmaps/higress-config.yaml /data/configmaps/higress-config.yaml
COPY config/mcpbridges/default.yaml /data/mcpbridges/default.yaml
COPY config/secrets/higress-console.yaml /data/secrets/higress-console.yaml
RUN chmod +x /data/configmaps/higress-config.yaml &amp;&amp; \
    chmod +x /data/secrets/higress-console.yaml &amp;&amp; \
    chmod +x /data/mcpbridges/*
...</code></pre><h3>Nacos</h3><p>Nacos 的部署相对简单，除了通过 kubectl 或者 nacos-operator 工具直接操作 K8s 集群部署外，还可以直接基于 nacos-server 的镜像进行部署 Dockerfile <strong>[</strong> <strong>3]</strong> 。因上文提到的内部环境问题，我们这里选择基于 nacos-server 的镜像，将服务部署于 K8s 集群上面。</p><pre><code>FROM nacos-registry.cn-hangzhou.cr.aliyuncs.com/nacos/nacos-server:latest</code></pre><h4>集群模式部署</h4><p>Nacos 集群模式下使用的一致性协议是基于 Raft 实现的，因此最小需要部署 3 台实例。</p><p>在引用 nacos-server 镜像的 dockerfile 中，声明 cluster 的部署模式。我们查看 nacos 的启动脚本，发现在 peer-finder（插件）目录不存在的情况下，如果定义了 $NACOS_SERVERS 变量，会将 $NACOS_SERVERS 变量中的值写入 $CLUSTER_CONF 文件中，$CLUSTER_CONF 文件的默认路径是 /home/nacos/conf/cluster.conf，其中定义的就是 Nacos 集群的静态成员地址列表，它在集群首次启动时会被读取，用于告知每个节点“邻居”在哪，从而让它们能够互相发现、建立连接，并初始化 Raft 一致性协议。</p><pre><code>...
PLUGINS_DIR="/home/nacos/plugins/peer-finder"
function print_servers() {
   if [[ ! -d "${PLUGINS_DIR}" ]]; then
    echo "" &gt;"$CLUSTER_CONF"
    for server in ${NACOS_SERVERS}; do
      echo "$server" &gt;&gt;"$CLUSTER_CONF"
    done
  else
    bash $PLUGINS_DIR/plugin.sh
    sleep 30
  fi
}
...</code></pre><p>因此我们可以在 Dockerfile 中维护当前集群下的 [实例 IP:端口] 列表，供 Nacos 集群启动时读取并初始化。</p><pre><code>...
ENV MODE=cluster
ENV NACOS_AUTH_TOKEN=xxx
ENV NACOS_AUTH_IDENTITY_KEY=xxx
ENV NACOS_AUTH_IDENTITY_VALUE=xxx
ENV NACOS_SERVERS="10.0.0.1:8848 10.0.0.2:8848 10.0.0.3:8848"
# nacos 用户名密码
ENV NACOS_USERNAME=xxx
ENV NACOS_PASSWORD=xxx
...</code></pre><h4>实例间动态发现</h4><p>上面这种固定 IP 列表的方式<strong>缺点是显而易见的</strong>。它是一个静态的配置，当出现集群的扩缩容时，实例是没有办法自动去更新成员 IP 列表的，需要手动修改并发布，整个过程非常繁琐，严重情况下可能会影响线上服务的稳定性；且在云原生容器化背景下，IP 并不是固定的，随时有可能会因为故障迁移而改变 IP，维护静态 IP 列表与云原生的理念背道而驰。线上生产是完全不推荐这种方式的。</p><p>再回到上面 docker-startup.sh 脚本，可以通过 peer-finder 插件来实现集群间实例的发现，取代手动维护 cluster.conf 文件。peer-finder 插件运行依赖于 K8s 集群 Headless Service 域名，会去执行类似于 nslookup 命令查找 Service 下面的所有健康 Pod 的 IP 列表，类比于服务发现的能力 <strong>[</strong> <strong>4]</strong> ，这样就不用再手动去维护实例 IP 列表。</p><p>但是 peer-finder 的运行依赖于 StatefulSet 的实例部署模式，需要每个实例有固定的实例名。因为我们内部环境的限制，我们现在部署的都是无状态的实例，所以没有办法通过 peer-finder 来做这个事情。但是我们可以参照 peer-finder 脚本的实现思路，来自己写一个启动脚本。</p><p><strong>1）首先为 Nacos 集群申请 Headless 的 Service。</strong></p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: nacos-headless
  namespace: mcp-nacos
  labels:
    app: mcp-nacos
    nacos: mcp-nacos
spec:
  clusterIP: None
  ports:
  - name: peer-finder-port
    port: 8848
    protocol: TCP
    targetPort: 8848
  selector:
    app: mcp-nacos
  sessionAffinity: None
  type: ClusterIP</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458828" alt="image" title="image" loading="lazy"/></p><p><strong>2）这里修改下 nacos-docker 的启动脚本，提供一个简单的实现。（仅供参考）</strong></p><pre><code>...
原docker-startup.sh内容
...
# 新增内容
# 注释掉 JAVA启动命令
# exec $JAVA ${JAVA_OPT}
export JAVA_OPT # export JAVA 启动参数，方面下面读取
HEADLESS_SERVICE_FQDN="xxx.svc.cluster.local"
CLUSTER_CONF_FILE="/home/nacos/conf/cluster.conf"
UPDATE_SCRIPT="/home/nacos/bin/update-cluster.sh" # 原子更新脚本
NACOS_START_CMD="$JAVA $JAVA_OPT"
# 1. 动态创建 update-cluster.sh 脚本
cat &gt; ${UPDATE_SCRIPT} &lt;&lt; 'EOF'
#!/bin/bash
set -e
NACOS_PORT=${NACOS_APPLICATION_PORT:-8848}
CLUSTER_CONF_FILE="/home/nacos/conf/cluster.conf"
TMP_CONF_FILE="/home/nacos/conf/cluster.conf.tmp"
&gt; "${TMP_CONF_FILE}"
# 从标准输入读取 nslookup 的原始输出
awk '
/^Name:/ { flag=1; next }
flag &amp;&amp; /^Address:/ { print $2; flag=0 }
' | while IFS= read -r ip; do
    if [ -n "$ip" ]; then
      echo "${ip}:${NACOS_PORT}" &gt;&gt; "${TMP_CONF_FILE}"
    fi
done
# 排序以确保文件内容的一致性，避免不必要的更新
sort -o "${TMP_CONF_FILE}" "${TMP_CONF_FILE}"
# 只有在新旧配置不同时才执行更新
# 检查旧文件是否存在
if [ ! -f "${CLUSTER_CONF_FILE}" ] || ! cmp -s "${TMP_CONF_FILE}" "${CLUSTER_CONF_FILE}"; then
    echo "[$(date)][update-script] Peer list changed. Updating config."
    mv "${TMP_CONF_FILE}" "${CLUSTER_CONF_FILE}"
    echo "[$(date)][update-script] cluster.conf updated:"
    cat "${CLUSTER_CONF_FILE}"
else
    rm "${TMP_CONF_FILE}"
fi
EOF
chmod +x ${UPDATE_SCRIPT}
# 2. 启动前的初始化循环
MAX_INIT_RETRIES=30
RETRY_COUNT=0
MIN_PEERS=3 # 期望的集群最小副本数量
echo "[INFO] Initializing cluster config. Waiting for at least ${MIN_PEERS} peers to be available..."
while true; do
  # 直接将 nslookup 的输出通过管道传给更新脚本
  nslookup "${HEADLESS_SERVICE_FQDN}" | ${UPDATE_SCRIPT}
  # 检查生成的配置文件行数
  LINE_COUNT=$(wc -l &lt; "${CLUSTER_CONF_FILE}")
  if [ "${LINE_COUNT}" -ge "${MIN_PEERS}" ]; then
    echo "[INFO] Initial cluster.conf is ready with ${LINE_COUNT} peers."
    break
  fi
  RETRY_COUNT=$((RETRY_COUNT+1))
  if [ "${RETRY_COUNT}" -gt "${MAX_INIT_RETRIES}" ]; then
    echo "[WARN] Could not find ${MIN_PEERS} peers after ${MAX_INIT_RETRIES} retries. Starting with ${LINE_COUNT} peers found."
    break
  fi
  echo "[INFO] Found ${LINE_COUNT} peers. Waiting for more... Retrying in 5 seconds."
  sleep 5
done
# 3. 在后台启动我们自己的监控循环
(
  while true; do
    sleep 15 # 每 15 秒检查一次
    echo "[$(date)][monitor] Checking for peer updates..."
    nslookup "${HEADLESS_SERVICE_FQDN}" | ${UPDATE_SCRIPT}
  done
) &amp;
# 4. 启动 Nacos 主进程
echo "[INFO] Starting Nacos server..."
exec sh -c "${NACOS_START_CMD}"</code></pre><p>这样我们 cluster.conf 文件中的成员 IP 列表就实现了自动更新。</p><p>线上生产环境还是推荐使用有状态 StatefulSet 的部署模式，并结合 peer-finder 的能力实现实例间的互相发现。而不是用无状态的实例，自己去写脚本实现。后续我们也会升级到 StatefulSet 的模式进行部署。</p><h4>配置外置 Mysql</h4><p>在集群部署模式下，就无法使用 Nacos 内置的不支持数据共享的 Derby 数据库，需要配置外置的 Mysql 数据库。提前部署好 Mysql 实例之后，按照 Nacos 中的 mysql-schema.sql <strong>[</strong> <strong>5]</strong> 数据库配置文件将表初始化，再将 mysql 配置信息写入 Dockerfile 中即可。</p><pre><code>...
# mysql config
ENV SPRING_DATASOURCE_PLATFORM=mysql
ENV MYSQL_DATABASE_NUM=1
ENV MYSQL_SERVICE_HOST=xxx.mysql.zhangbei.rds.aliyuncs.com
ENV MYSQL_SERVICE_PORT=3306
ENV MYSQL_SERVICE_DB_NAME=nacos
ENV MYSQL_SERVICE_USER=xxx
ENV MYSQL_SERVICE_PASSWORD=xxx
...</code></pre><p>在为 Nacos 做服务暴露的时候，只需要暴露 Nacos 控制台的 8080 端口，且限制为只允许内网访问即可。因为 Nacos 只是内部作为维护管理 MCP 工具元数据信息的 MCP Registry 使用，对用户侧不感知；且 Higress 和 Nacos 都部署在内网的 K8s 集群上面，内部通信通过 K8s 的 Service 即可，无需将 Nacos 的 8848 端口暴露给公网。</p><h4>申请 K8s Service 供 Higress 使用</h4><p>注意 Higress 拉取/订阅 Nacos 中的配置会通过 gRPC 的方式调用，这里的 Service 需要<strong>暴露 8848 和 9848 两个端口</strong>给 Higress 使用。</p><pre><code>apiVersion: v1
kind: Service
metadata:
  name: pre-oss-mcp-nacos-endpoint
  namespace: aso-oss-mcp-nacos
  labels:
    app: mcp-nacos
    nacos: mcp-nacos
spec:
  type: ClusterIP
  ports:
  - name: subscribe-port
    port: 8848
    protocol: TCP
    targetPort: 8848
  - name: grpc-port
    port: 9848
    protocol: TCP
    targetPort: 9848
  selector:
    app: nacos</code></pre><p>同理，如果想使用企业内部的镜像，或者是想在原本的基础上即成一些新的功能，如使用阿里云的 SLS、云监控等功能，也可根据 Nacos 的 Dockerfile 进行自定义构建部署。</p><h2>鉴权</h2><p>Higress 自身提供了丰富的鉴权 <strong>[6</strong> <strong>]</strong> 能力，如果你的企业本身就基于 Higress 搭建了自己的网关并使用了 Higress 提供的鉴权能力，这种场景下直接复用原来的方案即可。</p><p>另一种场景下，企业中会有多个服务 Provider，每个 Provider 有不同的鉴权方式。如下图所示，某个服务提供者会通过拦截器对请求中携带的用户 Cookie 进行 RAM 鉴权；另一个服务提供者会通过 tengine lua 脚本对请求进行自定义鉴权；以及后续注册的服务可能有其他的鉴权方式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458829" alt="image" title="image" loading="lazy"/></p><p>一方面，我们并不希望使用 Higress 的鉴权能力去覆盖全部的鉴权场景，开发维护成本过高，我们优先考虑直接复用服务提供者已有的鉴权能力；另一方面，如果通过网关层鉴权需要将 AK 或者认证信息存放在 Higress 服务上，在安全层面也不是一个合适的做法。</p><p>这里推荐的做法是直接在 MCP 工具调用的时候，将鉴权信息透传给服务提供者，让服务提供者完成鉴权。</p><h2>MCP 验证</h2><p>根据文档 <strong>[</strong> <strong>7]</strong> 中的操作示例，我们可以简单做个全链路测试验证。主要分为以下三步：</p><p><strong>1）在 Nacos 中注册服务，并配置 MCP 工具的元数据信息：</strong></p><p>在 public 命名空间下，创建服务信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458830" alt="image" title="image" loading="lazy"/></p><p>在机器上将自己的服务作为永久实例注册进去。（这里为了快速验证黑屏登陆机器操作，线上生产环境还是须要白屏操作）</p><pre><code>curl -X POST 'http://127.0.0.1:8848/nacos/v1/ns/instance?namespaceId=[namespace]&amp;serviceName=[service_name]&amp;groupName=[group_name]&amp;ip=[服务域名]&amp;port=[服务端口]&amp;ephemeral=false'</code></pre><p>注册完之后，就能在 Nacos 控制台上看到注册的服务配置以及健康状态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458831" alt="image" title="image" loading="lazy"/></p><p>接着在 Nacos 控制台上配置 MCP 工具，添加一个简单工具，可以选择一个无参数 GET 接口，并发布。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458832" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458833" alt="image" title="image" loading="lazy"/></p><pre><code>{
  "requestTemplate": {
    "url": "/xxx/list.json",
    "method": "GET",
    "headers": [],
    "argsToUrlParam": true
  },
  "responseTemplate": {
    "body": "{{.}}"
  }
}</code></pre><p><strong>2）在 Higress 中配置 MCP Nacos 的服务来源：</strong></p><p>这里为了快速测试关闭了 Nacos 的认证，线上环境建议开启 Nacos 的认证。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458834" alt="image" title="image" loading="lazy"/></p><p><strong>3）在 Cursor/Cherry Studio 中配置对外暴露的 Higress 服务地址和 uri，即可使用 MCP 工具：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458835" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458836" alt="image" title="image" loading="lazy"/></p><h2>设计图</h2><h3>容灾架构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458837" alt="image" title="image" loading="lazy"/></p><p>进入浏览器查看原图：<a href="https://link.segmentfault.com/?enc=QYUCpxZ7e5iJtijYt%2FENJA%3D%3D.34IySCM%2FcpXT5nN94p%2BUoVQH8a%2FiOAd9DjOkGdxPn59ChC%2FVbIX888AkrMQ2ABs2PSqLFOKgxxKSfXRGzMuwQA%3D%3D" rel="nofollow" target="_blank">https://img.alicdn.com/imgextra/i2/O1CN0138v82b1L7vNY3RQdo_</a>!!6000000001253-2-tps-6507-5451.png</p><p>在整个 MCP 网关中，通过 uri 来路由不同的 MCP 工具，实现工具的隔离。</p><h3>逻辑模块图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458838" alt="image" title="image" loading="lazy"/></p><h3>时序图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458839" alt="image" title="image" loading="lazy"/></p><p><strong>附录：</strong></p><p>[1] 基于 all-in-one 的 docker 镜像</p><p><a href="https://link.segmentfault.com/?enc=sQEArVEaBrB3FQ9NDNfsrQ%3D%3D.aOvNrKVs%2FMFp5Oz7ISx6ttTUbgu25Zf7%2Fsu3hvPNNoRKwzehWUEcXm68ONfLS6JVavJuSlRZmuo2iChfEjb80VlDK3Ii2XIuWngFzNjyL3BO9CZsnZMUqWh4vCtY%2Frsx" rel="nofollow" target="_blank">https://github.com/higress-group/higress-standalone/blob/main/all-in-one/Dockerfile</a></p><p>[2] higress-plugin-server</p><p><a href="https://link.segmentfault.com/?enc=mCfrMpmgSWUgScjyfoNkgQ%3D%3D.Fn6B6ffA9QQxtDpvidSAKJ8YIhgCiUJSPayz%2BifELFImcOrhsVSXGyA2rLGqati9" rel="nofollow" target="_blank">https://www.cnkirito.moe/higress-plugin-server/</a></p><p>[3] 基于 nacos-server 的镜像进行部署</p><p><a href="https://link.segmentfault.com/?enc=vWkVUK6F6lROoNLSJZC%2Fug%3D%3D.zwPxGvtJv2lG8hBbjOQkfdWoretRREQTGobJyS6%2FD0jKOPEl1KxYj9MDkcRVPtGXJfPFHhNvx8vUQEjfWDEVn%2F80oHE%2BR%2FY8LUPdkqEq4Lg%3D" rel="nofollow" target="_blank">https://github.com/nacos-group/nacos-docker/blob/master/build/Dockerfile</a></p><p>[4] 脚本源码</p><p><a href="https://link.segmentfault.com/?enc=XRSdxaLI7nEi6HF0yrQTYA%3D%3D.u9SEDlwD4LujnoA%2FSDoLjkqq8bd6dc1YyUr8%2FVsKhd8jDNMAlyY6hUfTrUjR3AIrAiVoEN93wzGoXQ%2BXnvvCAuQNF%2BjY5E51APj0QKSJW9c%3D" rel="nofollow" target="_blank">https://github.com/kmodules/peer-finder/blob/master/peer-finder.go</a></p><p>[5] mysql-schema.sql</p><p><a href="https://link.segmentfault.com/?enc=doqbpbZz6TQ3W5HxWr%2FOKQ%3D%3D.g%2F6l28Kde2tq5mmijpH4NT5U2Kx1PRE%2BWKaqUafp%2FWdu1hhWybhrbPDNKuKjkFp10S2R7XcrlkVj8vqDuUS0OVPN4S9dCUp302bGop%2B4ROSyKsylDlWeFfcT9Q%2Bsiugn" rel="nofollow" target="_blank">https://github.com/alibaba/nacos/blob/develop/distribution/conf/mysql-schema.sql</a></p><p>[6] Higress 提供丰富鉴权能力</p><p><a href="https://link.segmentfault.com/?enc=9yFhiOV2U%2Bn7LKEpkhIB5g%3D%3D.Nj2UN7LCCTDunRwj4C5KilzKw3UzxnLkTEQBgsgdeSKxzmmDf3VIjublvxEbHGco5SxHk2b95rxH13F2A26IYHE7DNu7r3vLNJldMVSdsWA%3D" rel="nofollow" target="_blank">https://higress.cn/docs/latest/plugins/authentication/basic-a...</a></p><p>[7] <a href="https://link.segmentfault.com/?enc=Ki3ht48aN4Aygr%2Fk%2Fr5UYA%3D%3D.m0aeDx9ly4D4RlNMJeIAdax0T240q%2F987Bc%2F80wB61njAx2It0NHvid0tDzQ86zx0C%2BEI0jYy3KT16vtAHCU4vaL6784RnLDZ2k3BonS%2FwbAWYz4FeKuwSwx5najiRCMZC32rlNsK39F62suxFBVi96ramPd43ZoHBF%2BDbJfHajm5kPc95tjWNtP5BqRcMPU" rel="nofollow" target="_blank">基于 Nacos + Higress  的 MCP 开发新范式，手把手教程来了！</a></p><p>[8] <a href="https://link.segmentfault.com/?enc=vJPkjZPODxywqGeTIW7cTQ%3D%3D.gINgQuTxaE%2FuiCLvH2fCq0bl6R3HkeK9PRkvirVyArzfHaJY1CJp8kMUonXtfwIGU071aMoyrjxP5RNfp0x%2BTnTBdcHJjpfJDEHxwzKH17hDmFQ%2FANtXpKBKHeEl5Z4aBpeqjvufs1qNCfkS3FiUrS7OWibhYBXD7gYxkFvzMYI2exiaHkIYq4DsrPZPPeSb" rel="nofollow" target="_blank">Nacos 3.0 正式发布：MCP Registry、安全零信任、链接更多生态</a></p><p>[9] 修改内置插件的镜像地址</p><p><a href="https://link.segmentfault.com/?enc=AnbffnZyTXMn86tFYinS%2BA%3D%3D.EZX%2BkBpSfuHRIi2pbCKH7X%2F7cwndK9jfCbB9cJR%2FcKrvwZECA%2BQn5TC1yySei3E7pWITmabyEwLAHkQlz09slw%3D%3D" rel="nofollow" target="_blank">https://higress.cn/docs/latest/ops/how-tos/builtin-plugin-url/</a></p><p>[10] Nacos 集群模式</p><p><a href="https://link.segmentfault.com/?enc=BqqmezPiLioYJAs7HANN8A%3D%3D.PTrzpAdsWO5f7MhrCPx6w7NCTwrmBuLlqT2nd0aBuYbb%2FkXY%2FTCaqNlmbwo7J3aon%2FA%2BaiQct5pDQdINzawuKdtB5CyiXOaWTULYyGhCyKQ%3D" rel="nofollow" target="_blank">https://nacos.io/docs/latest/manual/admin/deployment/deployme...</a></p>]]></description></item><item>    <title><![CDATA[线下活动速递丨AI 原生应用开源开发者沙龙·杭州站 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047458895</link>    <guid>https://segmentfault.com/a/1190000047458895</guid>    <pubDate>2025-12-08 18:11:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <ul><li><strong>时间：12 月 12 日（周五）13：30</strong></li><li><strong>地点：杭州阿里巴巴云谷园区 2 号楼访客中心 225 景逸书院</strong></li></ul><p>了解 AI 原生应用开发的前沿趋势和核心产品技术，全面 get 典型应用场景及硬核实战经验。</p><p>现场完成实操，颁发专属证书！</p><p>免费报名链接：<a href="https://link.segmentfault.com/?enc=YyM%2FlL%2FZjMYLw1U3xqV%2B5Q%3D%3D.DoaJrgMlPfBq%2Fj94d1NEabFYzmkeIvkBi3wbGleej8sAAMiMBBE6INFBT42iywRYwItXMEwbWlcQmBF1wufZ4w%3D%3D" rel="nofollow" target="_blank">https://www.aliyun.com/activity/middleware/2025-ai-hangzhou</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458897" alt="image" title="image"/></p>]]></description></item><item>    <title><![CDATA[“答开发者问”之HarmonyOS技术问题解析 第18期 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047458900</link>    <guid>https://segmentfault.com/a/1190000047458900</guid>    <pubDate>2025-12-08 18:10:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>本文原创发布在<a href="https://link.segmentfault.com/?enc=e3gIT8O4tLZ99%2B8d1DIgXA%3D%3D.ImjpEQClcTmc6b%2Fa%2BMkIZRhLi%2BkbWq3tFfPwbZr34N74ljgFwUuPNg8M5QEf5%2BssHWArzRj7X3lLF%2BecpJ%2B7p622aPnBWpEdUFqhGafxOr8Eed1atXOuNAHwEDGZ9x12" rel="nofollow" target="_blank">华为开发者联盟社区</a>，欢迎前往与更多开发者进行互动。<br/>更多相关问题可点击原帖进行交流：<a href="https://link.segmentfault.com/?enc=XCJvLIBu7asmaM%2F%2BNMRPmw%3D%3D.F8lv46PidgKxNl2MWqXkSfe3hdJLpnHDaMuDTVsnasJlMOxNBF%2FE6UzVVECixUXMEWBz4DOISlDAkhpxMTb61eQRgFVsA0AIGn45BFwCX6sfuHd3YOWEcm0cqJ304wgQO29qB1ihWBoMxSTs2Yu1W%2FRJBVYtpv4NTkJygy1IwrKaWL1Cj2vc8nJFASzKPhwn" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第18期</a> 。</blockquote><p><strong>本期问题如下：</strong></p><p>1、<a href="https://link.segmentfault.com/?enc=j1Z1UNZzsri1ngP%2FYNZazw%3D%3D.lD%2FraKgxtCrDdLc%2ByVdUxFZr81DksGARGU8OSlPV2g3%2Bz1ykpFPABLEn6948qdIg73bh8vz9sqosHH4oEtDOPP1tw6CKmVPZI%2FDGS06NVecRKfX5TCNJZCpq%2BLHzz16%2BB9IcHugkrcgh4wEAyRONlFbiLtAt6XViMrOTfTUF%2F6mNeNwyzr624VxKx0nVw1Mi" rel="nofollow" target="_blank">如何比较方便的获取设备的UDID？</a></p><p>2、<a href="https://link.segmentfault.com/?enc=h17muGiEwuw1FrtUWoyEwg%3D%3D.HG9JYeQqt7UHXNrE7p8%2FHy6t9x4iY4mPrCTGmpovQKHkaQUt3NtCf9j%2FtUrLobCR3ka%2Bd8T0FGYhVfiSat2MOHWDsKtUqQ05Viv%2F%2FFQV0CKQp5xXAh4Dvjh2HHt3Kaq1XHhyRe%2BwbxMlbd12rD6%2BD0vaIOSf33mN5UJHhFBW57oxLa1jRVkb6NNt%2BSVBGCWL" rel="nofollow" target="_blank">无内购无广告且不联网的游戏上架时是否需要版号？</a></p><p>3、<a href="https://link.segmentfault.com/?enc=a7ljcmcPN%2F8ipIS6klBgoA%3D%3D.UGOM1xbIQevF1EpeGPhI%2B7L5H3G0BAxCZbzF%2BYP2Z65dU9etRh9R4pNA8X58nsouRfZLGAo13gKLJV%2BbobAQbgwhYy6196oaYfnsRcLLIpuqNAyJWyQr6SZf1A1IGb696yT1WmxnWvE0aBg9FcnwR8y4SKS%2FWIkdSe5eNiCN8feZdmuL1nIevX3%2FiFeF%2FUUx" rel="nofollow" target="_blank">怎么实现类似于练字的功能？</a></p><p>4、<a href="https://link.segmentfault.com/?enc=VqfqwUptm3wynqy8YdQB%2Fg%3D%3D.U7Jygs4Wn9jDmXUSc%2FdqBTP1T1Rj0qdLNC9HEH%2Bj3UBuhbnfOR%2F8azZGOYRN3tW%2BsoAhOCRwvdg9ZSAs8UxeVodsYM5SeEE2a8Tg5h6%2F1js3TgEfwgzQIks3tNEtjLkJuzHmWVBIKS36LzFZoHbMkgRPyYfU4JMeEj4dsrpRPHAMdS9jI8O7B%2FVOry2XZSm4" rel="nofollow" target="_blank">Arkweb如何正确加载web的当前title？</a></p><p>5、<a href="https://link.segmentfault.com/?enc=4L6J%2Bkk2ge5wpDOv1g815g%3D%3D.uU%2B8533DXOigjcVcs3Iya%2FxcxRT7Ag%2FwfPdAzS7%2BelZFVkbba36X2LXw0om8vwDpy0KPdpaJRAhEAAfR3H%2FkbwhoDpeAsaVhnzU5BJh58uSRqsgKY22HjgsNY7m4%2FN4g85othyelm69iL%2BaCi3zJGw%3D%3D" rel="nofollow" target="_blank">HarmonyOS NEXT应用测试都有哪些工具可以使用，它们的使用场景有哪些区别？</a></p><p><strong>问题一：如何比较方便的获取设备的UDID？</strong></p><p>使用命令符时出现：'hdc' 不是内部或外部命令，也不是可运行的程序或批处理文件。请问如何通过hdc命令获取设备的UDID？</p><p><strong>解决方案：</strong></p><p>可以使用<code>hdc shell bm get --udid</code>获取设备UDID。</p><p>关于<code>'hdc' 不是内部或外部命令，也不是可运行的程序或批处理文件</code>这个问题，需要将DevEcoStudio的安装目录DevEcoStudio\sdk\default\openharmony\toolchains配置到系统环境变量path里，详情可参考<a href="https://link.segmentfault.com/?enc=jLSXsjjgFu2%2BM2hd9Pi1Uw%3D%3D.Ny8W7Ot2po94PbTHP57nI0R5p%2FwZ4yfEvpc7l2x1B%2BJjGXJQNDJmeoPSnEHMXI2mHhy5vg6d2CIJGezt4zX9ucuinKdJty4DMrtUiRMTCCVzSBtdDPIlPo3gsWZ%2FE%2BSn9MixjOMG0vSnGluIg7qYq6%2B%2FRnZOxKM%2BuZC610spE4xaCAMYbtt49udwWfubnfDN" rel="nofollow" target="_blank">HDC配置</a>。</p><p><strong>原链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=m3w6PMdjSc5zwAC9PjoeLA%3D%3D.rjB3ytt9vt3Gk4nKwbAzJB5vb4WsQ0C7VioxWllqey1QlaDBHLAI2myApiwPbQCBVHUVwec5hC%2Fhd%2FEG%2FAHLOJsiis2skefyfTZQs35YwTbTfTaXqLhxi3mSipwoGmhngnaVbz88PAT18YO7NZtGIyuXfax3%2Fb4De3QTu4o77GkO0gtRjohEOKtL%2FAoz6QJb" rel="nofollow" target="_blank">如何比较方便的获取设备的UDID啊？-华为开发者问答 | 华为开发者联盟 (huawei.com)</a></p><p><strong>问题二：无内购无广告且不联网的游戏上架时是否需要版号？</strong></p><p>我是个人开发者，写了个小游戏，无内购，无广告，也不联网，上架时是否需要版号？</p><p><strong>解决方案：</strong><br/>根据华为应用市场的审核要求，单机游戏需要版号。</p><p>以下是具体说明：<a href="https://link.segmentfault.com/?enc=qY8CD6wHAspQHoj9lx79Vg%3D%3D.hm5BLPwgVzDQTfyl1xoMzpo6pLrVww0cYYIUVd7dqjzaFBRdPKAeLrGJeJcANmxB5u6Mlhicd9qcYelTKanqpjfyXrZ5%2FGwuI0aMJWbEApR%2FbJpm7EBIBlcfXdduSbxKrlb9p1VES9tY9kwBmxLTUQ%3D%3D" rel="nofollow" target="_blank">游戏版权与版号规定</a>：</p><ul><li>华为应用市场明确要求，无论单机还是网络游戏，均需提供 《网络游戏出版物号（ISBN）》或《版号批文》 等合法资质文件。该规定适用于所有在中国大陆地区发布的游戏应用。</li><li>资质审核流程：<br/>游戏上架前必须通过 资质审核，且版号是核心审核项之一。若未提交有效版号，应用将无法通过审核。</li><li><p>常见误区澄清：</p><ol><li>单机游戏是否例外？<br/>否。华为应用市场未对单机游戏豁免版号要求，所有游戏类应用均需遵守国家新闻出版署的版号管理规定。</li><li>未调用联网功能是否影响？<br/>不影响。即使游戏为纯单机模式，仍需提供版号。</li></ol></li></ul><p><strong>原链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=5z8GTqP5VDa%2FmmH3si1y9w%3D%3D.AWfrFxtfaqBZrkBMNY22QNVaBjsUM2REHLB2cnZMLNqXhSt0%2BBVqwogVlojyk%2BEilZyOjsVGYinpjG6aiv4eziobnz717LtjpMoI4t1VaPLGHSPWyY5FIFCXEEZ%2FcLJtaGsXnTKz1hgZikROm0vS6uUWfCF3AI3sT%2FEMuN%2F%2BXXqgbZAIUYa9t1ulOM11vwYh" rel="nofollow" target="_blank">无内购，没有接入广告，不联网的小游戏上架时是否需要版号？-华为开发者问答 | 华为开发者联盟 (huawei.com)</a></p><p><strong>问题三：怎么实现类似于练字的功能？</strong></p><p>想要实现类似于练字的功能，有没有什么好的方法推荐？</p><p><strong>解决方案：</strong><br/>可参考<a href="https://link.segmentfault.com/?enc=%2FA%2F0hI5sZMkt2hofp2d3Aw%3D%3D.xwL1VFcTYtkDbtXz8mpPK85EdTjRbK9540urE%2FOfLOutACHMZKvcdK9Mx%2Bb3qngYSYJbw1QxnGvrDk%2B1zdBmVnd7n5WJypou%2Bv0kehSaaj%2BhVKPfFILDrYCUpyIHB5f%2BVnPQhoWGv1BiH7%2FuLjpA%2F2jVM2UF4pRJN8FADpRH2MDykkUZjqLabYYZuPzRAu1d" rel="nofollow" target="_blank">儿童练字板</a>示例，通过<a href="https://link.segmentfault.com/?enc=UxSboxHo9HIsq58LNoToBg%3D%3D.pmNZpkNGIiLh98N%2BVtsIAP70nD7eFB4nWaVJPF0j3ywCLGPK0J%2BlIw9tkv6wbipe8Dg%2Bxv2fFlsF3NGVrIW03YHyv6AFC97h7wBuar2ZmT9LTucqujrqOiT3iWieMIEmgFiWpo%2BoZczkzuMEA9jcCgZIm4UICiPQILXHLB2Wc5%2BC1RqIlGdbSAISBVO%2BKcLe" rel="nofollow" target="_blank">Canvas</a>展示了儿童练字板场景，为儿童提供了在移动设备上练习书法的机会。</p><ol><li>通过ontouch事件，监听用户手指按下、滑动、抬起，获取触点坐标。</li><li>利用<a href="https://link.segmentfault.com/?enc=VtpNDsDxQLzBlfzcpUSaCw%3D%3D.CwjeCn8ZjKr7DFlxzQRrM%2FJbUV50VFwO1D%2BAwIEn%2BT0PtjHswC8oomqFSc91xYNCeBvLbqLSJh5QLhHkTpBJxxOBzcWXlzhe2cp520AFN6T8y91SBJtuu%2Bz0lhKetPVitOXRo6zRWdoaJhjWiktTEO4wcjOgkYFhioNZxxjIkMM87jJ061sRjgTfGyGgMuwT" rel="nofollow" target="_blank">CanvasRenderingContext2D</a>进行绘制。</li><li>利用clearRect方法删除画布指定区域的内容。</li></ol><pre><code class="TypeScript">// 构造练字板的米字格
drawLine(ctx: CanvasRenderingContext2D, r: number);
// 手绘板的获取
Canvas(this.context){}
.ontouch();
// 删除画布指定区域的内容
context.clearRect(0, 0, this.canvasWidth, this.canvasHeight);</code></pre><p><strong>原链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=fjq5%2FG3dujHCBEW3dwlOXQ%3D%3D.DlQZLClrEBzufDVkGnNKoUclNf5Q3jr3gYUU6A3DVD3eojVXpNA5nhn414%2FVV%2BtinVXvM8Vm47JdWo5Ae1MGYZD8XEUlpSUy05Gb%2BSJdJJunuWbaOp7a%2F6qykzVVDYBg9Cp6Y6fzNaP5XODnG9cPd%2FY7k3I8WoWnoiI00Bm44k5koCDopCzrobRxiWXqfeMI" rel="nofollow" target="_blank">怎么实现类似于练字的功能？-华为开发者问答 | 华为开发者联盟 (huawei.com)</a></p><p><strong>问题四：Arkweb如何正确加载web的当前title？</strong></p><p>使用arkweb的onTitleReceive获取web的title有时候并不是和document.title是一致的，而且onTitleReceive经常会返回url字符串，请问这种问题应该如何应对？</p><p><strong>解决方案：</strong></p><ul><li>方案一：在<a href="https://link.segmentfault.com/?enc=opNboOVhAiDuv1eJCP3JhQ%3D%3D.vW%2FtStY04aIfJDMra3FFqIMzpGRwnYcO5gOaxSHksDDR9GSvDZ2SeB2ckOsl%2F4JVL1%2FADHFMUYRtgvu%2FPEjlMt7pWve1bqKb6HyBN%2F8kWFUMEa9%2BbAfplyirIayG7dGVPZ3fVMq94Q2OYkmVqIT3qkJrofnBJ5gKCBfRSYSk2K%2BukTxrLB3Tz0NFvkQYthPvDR0SulKp1ttR39IEtf%2Fwqw%3D%3D" rel="nofollow" target="_blank">onTitleReceive</a>中通过webController.getTitle()获取网页的标题。</li><li><p>方案二：通过<a href="https://link.segmentfault.com/?enc=1kub7htypTHCzEj98UUBVw%3D%3D.KvOCuR56W10%2FR6%2BGZI6nalZBu%2F34vJvO88zzCmM%2Fzclm0bWpiwevUIc4tYS1%2FHIGs%2BzhsHFRHTjo12a%2F1xXTZ53UFSbr%2FuSb%2F9Nb7PFbZoQq2zNsK8e8W3xwD1qWihBRT%2BTqiHIRlg1oSVZVJYEV0JHpBb%2B%2FF70rxAeCauyIPWrKw5RZGUUdUCImMrbTnOohc%2FVVrwUwQN6JLX2vQnrkGg%3D%3D" rel="nofollow" target="_blank">runJavaScript</a>执行JavaScript代码来获取文档的标题。</p><ul><li>如果getTitle返回的是网页url，那是因为当前网页未设置title。正常来说通过webController.getTitle()获取到的网页标题和document.title是一致，如果遇到不一致的情况，可以自由选择方式一或者二。</li><li>具体参考如下demo:</li></ul></li></ul><pre><code>  import { webview } from '@kit.ArkWeb';
   import { BusinessError } from '@kit.BasicServicesKit';

   @Entry
   @Component
   struct Question2 {
     context: Context = this.getUIContext()?.getHostContext() as Context;
     webviewController: webview.WebviewController = new webview.WebviewController();
     @State title: string = '';

     build() {
       Column() {
         Text("title:" + this.title)
         Web({ src: $rawfile('question/question4.html'), controller: this.webviewController })
           .fileAccess(true)
           .domStorageAccess(true)
           .onTitleReceive((event) =&gt; {
             if (event) {
               // 方式一：在onTitleReceive回调中使用getTitle获取标题
               this.title = this.webviewController.getTitle();

               // 方式二：在onTitleReceive通过runJavaScript执行JavaScript脚本获取标题,和方式一二选一
               this.webviewController.runJavaScript('getTitle()', (error, result) =&gt; {
                 if (error) {
                   console.error(`run JavaScript error, ErrorCode: ${(error as BusinessError).code},  Message: ${(error as BusinessError).message}`);
                   return;
                 }
                 if (result) {
                   this.title = JSON.parse(result);
                 }
               })
             }
           })
       }
       .height('100%')
       .width('100%')
     }
   }</code></pre><pre><code>  &lt;!-- index.html --&gt;
   &lt;!DOCTYPE html&gt;
   &lt;html&gt;
   &lt;title&gt;测试title&lt;/title&gt;
   &lt;head&gt;
       &lt;style&gt;
           #demo {
               font-size: 24px;
               font-weight: 700;
           }
       &lt;/style&gt;
   &lt;/head&gt;
   &lt;body&gt;
   &lt;p id="demo"&gt;&lt;/p&gt;
   &lt;script&gt;
       function getTitle() {
               return document.title;
          }
   &lt;/script&gt;
   &lt;/body&gt;
   &lt;/html&gt;</code></pre><p><strong>原链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=hi3uwlUrTaczdLQLv253wA%3D%3D.xdb2qxXOWhXI0pLvfjW09%2BcBm6JoNwXiLnYScnUnLUqQM2jtGd2%2FqgW6Y%2FcBPI1DkDJWz07zVK3O5CZGX7LijyezLbj7TauGCxd24YAh%2FxuRKUKmBx%2FmTIYy1z%2BFCBrYLN9WjJCp6f0SDPRRH9ZcrL%2Fw9%2BrW5%2Fm7N54SP8I7jbgyeC%2B9WZ2IJua6hwoQx70Y" rel="nofollow" target="_blank">Arkweb如何正确加载web的当前title？-华为开发者问答 | 华为开发者联盟 (huawei.com)</a></p><p><strong>问题五：HarmonyOS NEXT应用测试都有哪些工具可以使用，它们的使用场景有哪些区别？</strong></p><p>目前HarmonyOS NEXT应用测试都有哪些工具，这些工具的使用场景是什么呢？</p><p><strong>解决方案：</strong><br/><strong>【问题现象】</strong><br/>目前HarmonyOS NEXT应用测试都有哪些工具，这些工具的使用场景是什么呢？</p><p><strong>【背景知识】</strong><br/><a href="https://link.segmentfault.com/?enc=FTMqToS4e3OrZpZXlenXMA%3D%3D.i8Y4f6FoAPFcwTKxlCNStTiS5GT%2B4d5g3DvojR3EnC5pTCchaGFc3boT9puuHSytk78335Pll4yDss4UYfnQxX%2BFmGgK8izT%2B8mWzdzCngjtO7GzGNQCNGDDPIDpfTZVtIXB1Imdxv9Ttg4ug1qk7nMtR0tYzvHm2dTNEYXwYIw%3D" rel="nofollow" target="_blank">应用测试概述</a>主要介绍HarmonyOS NEXT应用的单元测试、UI测试和专项测试。</p><p>AppGallery Connect<a href="https://link.segmentfault.com/?enc=eKGwzNkdwAqj2dIRQISThQ%3D%3D.ngzjXxKAIK2w%2Bp7PCMXaG%2B1BrLwTgUTqBWycOjkGpfILb15O5S6eUdtbsHbOCrBkEwcWXtQy18pXzOAFuEe4Glq8TxCekowI3xdyM1CEw6y%2BR4zd2%2FNTmmKeEtF57JRqC9C%2Bsk8OMKcuCvlfTtRV9JvgBMHLwr6ngzyawzu7cnRB5k8uvSZtuP9tAH0C3cn0" rel="nofollow" target="_blank">云测试</a>致力于提供便捷的一站式应用测试服务，解决应用开发、测试过程中面临的成本、技术和效率问题。</p><p><a href="https://link.segmentfault.com/?enc=EkTc3WKOQm1Azqxxs%2FGUGA%3D%3D.788QjzpIgnQb6LHPWJ4gjy1EJVlSaFUtzWbQsiFEjoGQEws0NrlHh6BbH3Xy5opHBeflBiv4bvvXY2D4mOhhAdQm6sB49XDAS9Xyq48BKyw%2B03qX3owKMsD94BYadyBszIoPjW2gt3VXESyV9soV4zMltxCR%2FaVz88H%2Frrzq4f1lYSxpinviTODPZicqI0EO" rel="nofollow" target="_blank">应用体验建议</a>主要介绍基础功能和兼容性、稳定性、功耗、性能、安全和UX这6大核心质量维度在开发阶段和测试阶段需要关注的体验建议。</p><p><strong>【解决方案】</strong><br/>如下图所示，这是应用在开发过程中典型的测试活动模型和测试活动质量目标，一般分为单元测试，集成测试、UI测试、体验测试和用户测试。</p><p><img width="723" height="295" referrerpolicy="no-referrer" src="/img/bVdnim0" alt="image.png" title="image.png"/></p><ul><li><p>单元测试：通过自动化测试保障代码、函数逻辑实现正确，异常处理充分。</p><ul><li>测试工具：开发者可基于DevEco Studio提供的单元测试框架<a href="https://link.segmentfault.com/?enc=iZkWaz6EEh%2BAkBSfdDmG9w%3D%3D.%2FWp8GurUnZTAgGhds3QRD%2B8FYP8ghqLshc20LTujVajwJFXQaD2erqSoRoCfVyR%2BSzeWEUVf3bEdCYdZjfqDgnoEOO9sNfH0vP9mCYu8zzdy5ACtqY5n1Qp6CMc%2B90iXu9JIHr%2BDdE5vrRhp%2FqLVlz0kTiwMqYIbuoj7F5UgPOU%3D" rel="nofollow" target="_blank">JsUnit</a>、UI测试框架<a href="https://link.segmentfault.com/?enc=BIfD3mTPnQshQiF3f831uA%3D%3D.mO9QlNg6cIyQtMl3G34YrcvtwnnYh0K%2BuuICvTYEGAC2rXPMwQ3kf%2FrHdFjZZ%2FWtLmk0uPKOunLsVlpj0QxRteMSnICDmQZpr04dszqBp9SBdygLpJNHxbKvSCCOrDVwIAHkZIx2vcwsVIL8k%2FWx7jG9VYf628%2FoXNVXtgl3kvmAmas3B%2FyKfrrjSVVoODZ6" rel="nofollow" target="_blank">UITest</a>和白盒性能测试框架<a href="https://link.segmentfault.com/?enc=1qvjgz6YHqyO64h5FqEQ7A%3D%3D.VHarOdQKDXIs2Q0mouES%2BUEaT4rFmlbGznU%2BZBVHH47Ed%2Bnz2rYap%2F4QtcVUk7RkSDDFqoxAG4SfISFeUFJJHfAmtWt6WmrsoiMHHB9fZGwrgMbB3ytuxPnxqhn79Hk%2BIlcCa24eBodAAc%2BAO6dfO%2B2TEpdGZwb586L0vis4N4Y%3D" rel="nofollow" target="_blank">PerfTest</a>进行用例编写和自测试，支持<a href="https://link.segmentfault.com/?enc=wF5VZhrjSwfjRZx1O4waFw%3D%3D.DC5FJoujPDc0yGWFju3MiCPY92ix9LP%2BzQlOdle2gMx81n7DD1pHzHp3RTK05A3qklAu0EMQOP6BZs0wueV04aKmTiWUOZhtSsU7ebWzvw4e8xFcmihUzCmS4PeOIDTbOpOVdk21hkZ0i4WuOQL4GNaOMiEXNCmKgO%2Bw7OxQ8LCXsITNhQ9woRzpz6cV6nkK" rel="nofollow" target="_blank">黑盒覆盖率统计</a>和<a href="https://link.segmentfault.com/?enc=rBU4B1NHw58u1r2UCfxcSA%3D%3D.RhZkgXkQ1DoPg%2BCKxG5bxfRO4FXl7zRUOvKvLpUB9pSJKYvBrAQLxBVm4eFbTQ9kckXXidyu856EW4ZrJOZiG%2FHQ6hTtV0sJhDJd%2Bz%2B9namJwfcPxqxEab1POrsObvbFcWoxN0iDVGJVLHWjEo4lTT5uP6BuGw4i5MfLM1RPfGaFQdNdEMOumpSKsgfmZh64" rel="nofollow" target="_blank">Mock能力</a>。</li></ul></li><li><p>集成测试：组件实现符合设计，接口正确和组件完整。</p><ul><li>测试工具：同单元测试，集成测试检查更大子系统的行为，或者多个类和函数的组合。</li></ul></li><li><p>UI测试：应用功能正确实现，用户场景目标可达成。</p><ul><li>测试工具：使用基于Python语言的<a href="https://link.segmentfault.com/?enc=247L%2BVUsnCn4sJD0jz5HQw%3D%3D.YIQJPRfFpDwUQPkmySEAOwC7OO8GOk5DvLkGXdZDMXPJjU7%2Be6HozyQLRNNC4QJhaNMPvzU0O14ajIyXy12O%2BfQWpVYWJ95tkkollTMGJ3ZLz46q5ecIXW5JQ2IPXj56vWLaDq07sTVE3FMOBFrSSsbL0vLEdXAyCnUs0xvEe5U%3D" rel="nofollow" target="_blank">DevEco Testing Hypium</a>进行UI自动化测试，提升测试效率。</li></ul></li><li><p>体验测试：主要包括兼容性、稳定性、安全、性能、功耗、UX等，开发者可通过专项测试工具来保证应用基础体验良好，流畅、精致、安全等。同时开发者在应用上架前可以提前进行上架预检测试，提前发现问题，提高上架审核通过率。</p><ul><li>测试工具：</li><li>如果您本地有HarmonyOS真机设备，可使用<a href="https://link.segmentfault.com/?enc=Odac1Qbt%2FIcm5u%2FJs7SXqA%3D%3D.L6gSuXFsMGs6djUTLSpWQbDkcj5UvodMOo8A9mMpoontOO7eJR3GT3KUTT7lEX%2FEuCnXV6A%2BN1uM64wMte3Mwsb2%2B%2F3tJLyMq10WQbtgUxifpcW6HveQ0pfFATu7%2BrjHDCHln9bnOIkv%2B53KJo241RQ1TOu9FsRYSZShb1QhY5fWfblq2u8VZd%2B4jHalGJ8u" rel="nofollow" target="_blank">DevEco Testing</a>进行专项测试服务。优点：以服务卡片的形式呈现，安装工具后，即插即用，一键执行测试任务。</li><li>如果您本地无HarmonyOS真机设备，可使用<a href="https://link.segmentfault.com/?enc=PFtx%2BacmCxky8Kd28CkO%2BA%3D%3D.pzU78PAFv7pYnPne%2BTVDWQO9zJFgr8DfQpY0FiTugSq0P%2BF3nfJ4lifuv3IDfiNHgloEuDqmzccH%2F%2F%2Bex%2B6TxnFdb6jm0LjTFDLEb%2BcUwqdzjRWecX0BqmrXyQQMK9tajpm%2FRMbOWEIbesSX3qlClT85un%2Fil6k24lSBiD54cN4%3D" rel="nofollow" target="_blank">云测试</a>进行专项测试服务。优点：提供海量远程真机，无需开发者自备真机，可申请多台设备并行测试，解决应用开发、测试过程中面临的成本、技术和效率问题。</li></ul></li><li><p>用户测试：用户感知卓越、好用、爱用。</p><ul><li>测试工具：</li><li>开发团队内进行<a href="https://link.segmentfault.com/?enc=2DbbvdHo%2Fgagk9fxOe0cnw%3D%3D.HqoaUZBiSc8dzynfqiie2YHZds5UNxrUhhNAp%2FujK3ZpacTxEPVxydVk0a0Muf4nDpRdWR7k4%2Bwp7sYIfhTvK36TnhZN08JqC3vq%2F4CdqX%2BmVCjtgg7RtzrAqN74vCmSgAys5l0o8Vi5Egp%2B82O2YOdRUfMuxbodbW1GbTLq3rJ3oD18y6S8o2SWZ7Smi6A%2B" rel="nofollow" target="_blank">内部测试</a>。</li><li>选择特定用户群组进行<a href="https://link.segmentfault.com/?enc=ujQ%2BPdI%2FdAL9s0W6d%2F4%2Fkg%3D%3D.99Itaf0jxuopxyxB5oLLqbRXHnrLhtJ%2FaGRcg9zAP%2F%2BOPGsqwL7e5K1WIA7g%2FQT2quJx1ZPYiXhY3VbvDBU5Fkj8tpCWZMh7OgYRjOsCvhfgv51HPaF6LP9rnMAm4UcLG6Vv0gFJ51GQLd5rAcWqt37m368we5HwIhOjrs1yZ2Y%3D" rel="nofollow" target="_blank">邀请测试</a>。</li><li>面向全网公开招募用户进行<a href="https://link.segmentfault.com/?enc=e89q2vV0uoFMUDhTW3c%2FVw%3D%3D.NYgwP%2BPw5fammaV3LzUaBh2baATfyMu99xE4KBbwZhxi%2FUMqHiHG3KwqtnoQxgchpi9xuxs0k5iKY9Byyd62qNgZBwpzDzOCEj5Aax58XY%2F2XgA61bh%2BhSrrwhRJ6%2FPVLkJFV6j0wzWfaP5WuGDbmImfkD6RJYJYD4O%2FPwSV6Nw%3D" rel="nofollow" target="_blank">公开测试</a>。</li></ul></li></ul><p><strong>原链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=bTbJQxwYlAr6x48SsUSaxw%3D%3D.aOw6cKzm2Pooz67IXMPDUQWOpHnMt3%2BwVltYgaVxjF1Oc%2FcZou7GB6baWHDWscwRL%2Bz8oqpLRnogzh1pLzToJxLn21wJdzH6ye0J0lPHNHyn6EjbEPJrPKLMYx0pV51hEdcCV2wGQHhIgqeqVYJ4rg%3D%3D" rel="nofollow" target="_blank">HarmonyOS NEXT应用测试都有哪些工具可以使用，它们的使用场景有哪些区别？-华为开发者问答 | 华为开发者联盟 (huawei.com)</a></p><p><strong>答开发者问系列汇总：</strong></p><p><a href="https://link.segmentfault.com/?enc=sbHlVXUCk%2B7DbyQc9OSGAg%3D%3D.Sk5XVYTURBgm2xaDUkpz%2FlO1CyfutpW%2FdNmtTeT7m1Qg7vPDXw6Zaj7wY7DmcNj4vHAI%2BFQ2uDSseoP5%2FojzE5kMh7Jgb742cpz0OmaEOYh6HcsxleWl4uai9I%2FuiIFYb6NFT3CGlnrInr9igOXhgOiMbXKZwubFy6fgPKyoPfsIkvPrmK2e5%2F565jv9bIm1" rel="nofollow" target="_blank">“答开发者问”系列汇总（持续更新中...）</a></p><p><strong>往期问题回顾：</strong></p><p><a href="https://link.segmentfault.com/?enc=nzYa8Rb4XO%2B6Eo%2B9qOJGdA%3D%3D.FDH9iap3phL8xkkH2Wll0vGzLlJXEghAKR4Gi3brV8G3bZkXqEq4cKRdqn7EJ7Ogj7rS%2FEmbUVGqRsFCMMsr%2BR%2FZEb8gzABDge32dkhYCE6MsWOS1ec3KRn0s%2B2yZ0SFYx6nYfNofc%2BAPbYO7HZwwb6kDElWMFreNJ88xasUwRjNEdvotmB3M9QLt7hVFbZE" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第1期</a></p><p><a href="https://link.segmentfault.com/?enc=SFwQSBz4HcY0VeHC%2Fc7Bzg%3D%3D.jj7th8IS0Lb8nqUFDws7dzUPvHEM03srN%2BkyvAuZrv0%2FppHjVg0E9739G743t9SRD3Gori3U66mFh24KxspuOWSZHhokxMO2S5IvzILo1PRvZrHtnZU8%2BWbPhq8r%2Bbv9X8KI9HH5PFZXZWDFPXMirKPtc9DDbozV8re%2FSRC1uNMlxQNoZ292tJuS0cQB9LGf" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第2期</a></p><p><a href="https://link.segmentfault.com/?enc=aKWwlPSo5RCJ69lcGNC9sw%3D%3D.0P6qB%2Fc%2Fpof34%2F86nTQ0tymvu6nXcm9FaVui8v19R2z9e0G%2Fux8uPBRVNyh6i6ag3zA3lz6%2Fr4KtfQ1BCBAaNgPuqHzXo1S%2BqrGAQgqkDsr7veAumd2tsWKGvrgYfJK3hv%2FWLjwFrY89zdWhXUoNrp78X3wTETZK19%2FmnAilJga3sXWxJfTxHqjCEN8DfnQt" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第3期</a></p><p><a href="https://link.segmentfault.com/?enc=e43i5mr9CAVh2JaUr2b%2BxA%3D%3D.t7kirSof0P99Yrh1cu%2Fn47f%2BEYSwYmptGRvHoPR8tdTRQ4%2BFQ25j5vt2I0NFl6npgpUl7jZwOfp40gNcMyZ65X5oCFRpyTCa3ys%2BloAgPR7IkbTJIvacQ7njwxfTl%2FGtdeUGJa%2F%2FudDzmZqcof9o924UCdGxMbS6Qub%2B28tpBOrxWWSTyfG7HGTeViKP6Dd8" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第4期</a></p><p><a href="https://link.segmentfault.com/?enc=J7pd%2BJEFoJsOyQ25%2F5PX5w%3D%3D.5NNdkF5A%2FEGrOV6CudA%2FqobjG1L4l1VrPD5vV82ZtrU1KgDobfJgGZtuDwYzl2xuVXBKfs5ZjxD1oI3Sos%2B%2BVNP4rvYsArqxMfstu404pQ2UwdrGheCZb13j8b23r1ywpYrOIDGQth00ex0hY26zbsamIZ0g1TTFnudQSL%2FeNNF%2B6U%2F4rv9YaXRlOlidKWPY" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第5期</a></p><p><a href="https://link.segmentfault.com/?enc=Mhjt%2FStrnIo7dbJZJTidhw%3D%3D.9NOHWkjG5pW2JeD2cbBmMI%2Fh6zgvxUmUhZJsjVLOdQ4IxrBv89WfHM2IxG%2FsVxAOxEDlJ1W%2FxGsKX8wYu287dH5OpW0mzwOoogUNYlkwjR95wAFTvzOYKww8VIy2q%2BCh1NrrmcFpQB%2BUtGNsIEftolVhCOTgPOx56DPGNpIS3BR7jQTSHZkawfbyrHlAtFt%2B" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第6期</a></p><p><a href="https://link.segmentfault.com/?enc=G7i9SAYR5HmHkU7CDW7bbQ%3D%3D.Su6Tht9R1%2BBgpRiXLVbxcW0LVj1Jz3V7Hl0qZ7SZGYzmuaEnAjjbB5mlt0KUrACGv81YPsVunSpNceK4BUBQyuwfhPkiG3RLX1UKexdLuT8wgY7A6gViIEvOnlIfQVrwGLp1ZneHEmit3v6Kacuj3rOo9JVmoFxcYWdaHXJcZ%2F2XfKfiV%2FBqLXsx2TVx%2B9MU" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第7期</a></p><p><a href="https://link.segmentfault.com/?enc=pLe2Fjc0zrRwrz5lW9B3eA%3D%3D.gLQEVp3XFwD%2B2JG6P66qp%2BtctHbvVK%2BjzkAKh1HU7n8AjakKvRyCQUB2Q4FEb8Kekbt0fUFNCsmuUth9sbLEUPls7hsRbIMHJlMqrmzBCLKnlpc%2FvfNyrcURCUjInokqv5CmTQj4SEzY80E1gFwacFAU%2FSC53PgP258ZVG7sYhx%2BOP%2BhtwLEx6VgeIU%2BCqBr" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第8期</a></p><p><a href="https://link.segmentfault.com/?enc=6OrgSCqG4eC7XD0bXH3xYQ%3D%3D.mAdmZ0vILEOTr5ZSyq9sBbXiXwzOio3nYSgDYA43S491Kj5%2BvSsyrXPYPQy1XJY9eDCOEw2oFGRoIHV1FqW4x9x2xfQzqVMh2SWGo0aGjO5pQDaRZVcIehsBnM3RSU7J%2B%2FY7Gp2xW9uJf7bmauq5xEoF3WxFpr3b5C4QpXvOdMUsc9r4kgWAPwB77wCrliRp" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第9期</a></p><p><a href="https://link.segmentfault.com/?enc=QzrDab%2Foa7oUfNyqLxGHig%3D%3D.BSpJrZPdRqSEFDYsHYQNHdljQF4HjJtDOfP8TiT3amMnRn1J3zQSyqtXXnwQMMkwVABynLdlCQ7oW%2Fl2eJs2pVOV4r9NKjHA9qmGhkoB2wnGYKBQEIIBOTQd2LLGVSZ9qzs%2BbUpDLwwTJJtipCqqFdcdkK%2FIKVyZAfHNawTi%2BYfMB3KAs8fMLMhZ130jzjr9" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第10期</a></p><p><a href="https://link.segmentfault.com/?enc=ekzfxpU5ogvC%2F%2BiOhyomLw%3D%3D.gOOobZXOdsjQh3KR%2FByrwda%2Fsk7lw4czM0ARhJn86woC84fLkfAt2LHlG1rUWmwN0sEk7576cAIh%2BQIm706ce%2FqEkrM5pO9GqdwNIo6SIzpZ1DzgVg0aVatIhEaDfT4vzBW82duUCnNKd0BCvC7FC7J%2F4wJMOHZCoIaA9J88EYc5tBr%2BlAqRFmeu5L4dAZA6" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第11期</a></p><p><a href="https://link.segmentfault.com/?enc=P%2F2U3RQkuPt2Pit%2B%2Ful%2FzQ%3D%3D.ngKBYh%2Fjz5FFIpvzqUE3cASp4N%2F57urBO8QriVhdEfXgrsCbosq%2FZG5nCxvUvX1CpFvlcaqoGjNUZyfhs4j%2BCxT%2FW8F4eb5Y1vQ5P8HMirNDr27PahFfA7%2FieoREe9b8%2Fyxi6YE4T9md91R0RrFjgwAea7VujmhjHG2IvHiHGpdo8SDmNlbC633F7eRL9QzT" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第12期</a></p><p><a href="https://link.segmentfault.com/?enc=6lbRs8rYIqyCwo8M9WU8iQ%3D%3D.NVeZgDxG2%2FMvpOkalUMhEFKtP5S9xPrvrAcSjVlVEQ7qaGBlTIIwNQvimEChDqS9e11zVv6MDDL5Xdf6dSyZu7IVizGEXzsfjDLqMW3cSiF5qzRDlLIV2%2FbMUqcZmKjC5VUYRlZ57n5wKCilzxH7hgcQwp30VHbAzDZSdWEGmMVn%2B6g2Cr1FzqAtGm0UHSt4" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第13期</a></p><p><a href="https://link.segmentfault.com/?enc=hRWfagFW1NhixN15mjA3gw%3D%3D.VtYPMcKDkNkmdPrPv3ehPE5lsNWoPIIe7UtFJv048yzXDrukWraovFmKoCo4sITFbdC%2BSzv1hvR1MI3r3xmkEndEIpODEBmvRVUfK9YTZU%2BM2%2BD1gBHvNoZK9QJ%2FAl2G45ySb1mDsaUAL0gGTHuPkbUAu0HRSwMmiVDy9tBSN%2B90vJC6huelC57QF0MhIXaC" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第14期</a></p><p><a href="https://link.segmentfault.com/?enc=y%2FhZrIg9LGjBD18Xd41GAA%3D%3D.g67vVYpJLRhzeQDjng5tmNGaKrXftMrN%2FE0NAhO3ladGfIkZBBxxLST3QzvGUtsbf2ubuI5g64nWe%2BfnDT7xsOUIxTzuocr1ES5NYalndqNUEcux0JdK%2BOMS%2B55TD4jQSnQehLgk6Vu%2ByC8%2BIiI2KnELWM1O1bit09aHOu6Y1cQWZoD1O%2BMqbVmPPFhXv%2B%2Fp" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第15期</a></p><p><a href="https://link.segmentfault.com/?enc=PAFPQ7H874UB%2Fh%2B50F2v2A%3D%3D.OIlbG9i03%2BiQU%2FgYOyfzJXGKyYS1tErfUKRRut3kl6k0z6Np%2Fb2lxr7xsaPrn75VXNu0lNcJBeukMxRD8dleWUt9otyMe4O%2FMGg0EyNGYxPU2ZttAMXxKFGq%2BJ0%2FWx%2B4qfHrmNyLmHBHZBggqfH35uKg1hoJApwGJK6uQyFNRpfx37o78V4n2a1hXOwofmQh" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第16期</a></p><p><a href="https://link.segmentfault.com/?enc=h%2BJJlV97%2BtMzhJ4BZHQvmw%3D%3D.YHmBBTJgswTC7kRc7NqdLd0bqoqYEwBKpW3A%2Fvoo2TLda6oyBa1cS2mAfc7DjQzPl1YJG4addQECBlTiuQ1VtOR1p46JkuTRmomjQyTFz%2F0PpIfeImgffLawq3t6jVjCaTLUR9U5ZoeHVx1Pfk3eI%2BEPDRsfRnbt4y52OsTrsVQO6ks2BhRTfevduHEIRhn%2B" rel="nofollow" target="_blank">“答开发者问”之HarmonyOS技术问题解析 第17期</a></p><p><strong>注意：</strong></p><p><a href="https://link.segmentfault.com/?enc=TmrE5PxXXuoe%2FLOAtsZ6fg%3D%3D.hE4oAnJSr7JdCyMdpISJDtxqz9PvkEqbQ%2B2bAuotpuzjgzUNdj80lsug%2FCc3JhGGWwzCeLtbc0UkQiJZ84%2Bw%2B4EPQMjYtBFrhUHG8epjV4HSZLpo1avRcEk%2Bzz%2BhNDDy7opWe3eqKOkw1gtusirVJpITQvF9PZcr544%2F20d8pfz%2FtRaaLhJKZH3Lm9%2BVVZyt" rel="nofollow" target="_blank">开发者小伙伴们，规范提问，高效沟通！更快得到问题答案的秘诀来啦，点击链接直达</a></p>]]></description></item><item>    <title><![CDATA[AIGC项目中的【模板进程】方案的设计实践 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047458904</link>    <guid>https://segmentfault.com/a/1190000047458904</guid>    <pubDate>2025-12-08 18:09:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1 项目介绍</h2><h3>1.1 项目背景</h3><p>简单一句话：模板进程是流程的子流程；往往用于比较复杂的aigc项目流程中。</p><p>由于一个模板有多个流程，一个运营人员可以操作多个流程，也可创建多个流程。在模板推荐时，就会导致不知道是哪次流程。</p><h3>1.2 项目目标</h3><p>为了区分模板中流程，就需要增加进程的概念（子流程），为了方便运营理解，此处也叫模板进程。</p><h2>2 需求分析</h2><h3>2.1 底层逻辑</h3><p>1、场景模板、指令触发模板均支持实例，模板数据支持根据实例进行隔离（原来启航项目创建多个SC，每次都需要澄清，现在根据进程隔离，当一个进程中存在多个SC时，才需要澄清），公共信息存储需要新增实例查询等能力</p><p>2、进程不会结束，支持移除（逻辑删除，不真实删除），仅进程创建人可删除自己创建的进程，项目管理员可删除所有进场，无权限不显示删除按钮（需要增加埋点，记录操作人及时间）</p><p>3、模板卡片的步骤流程状态，根据进程独立显示。</p><h3>2.2 触发方式</h3><p>1、【自动显示】每次进入项目详情页，若全部进程中存在进程，自动显示此卡片，无进程不显示。</p><p>2、【指令触发】输入：进程/场景进程/模板进程</p><p>3、无进程，用户触发任意步骤，均创建一个新的进程</p><p>4、用户可根据需求选择【新建进程】</p><h3>2.3 进程分类</h3><p>1、区分：全部进程、我的进程</p><p>2、每次触发卡片。默认打开【全部进程】</p><p>3、卡片引导文案，如下</p><p>全部进程：以下当前项目下正在进行中的所有进程，请选择。</p><p>我的进程：以下是您在当前项目下正在进行中的所有进程，请选择。</p><p>4、全部进程显示逻辑：显示当前项目的所有进程，按照创建时间倒序显示</p><h3>2.4 进程详情</h3><p>1、显示字段</p><p>进程名称：默认显示模板名称，支持编辑</p><p>创建时间：进程创建时间，年月日 时分秒</p><p>创建人：显示创建人头像、中文名，点击支持快速唤起京ME进行对话</p><p>模板进度：显示当前模板进程实例中步骤完成情况</p><p>当前步骤信息：显示当前板进程实例中最新的正在操作/代操作的步骤</p><p>当前步骤操作人：若当前步骤有操作人，显示当前操作人信息，像是规则同创建人，若当前步骤操作人不显示该字段信息</p><h3>2.5 进程名称修改</h3><p>1、点击编辑按钮，进程名称可编辑（保留原名称），最多支持1-20汉字长度，支持特殊字符。</p><p>2、删除空内容时，显示提示内容：支持1-20汉字</p><p>3、点击其他区域直接保存内容（若保存时，名称无内容，直接填充原始内容-模板名称）</p><h3>2.6 删除进程</h3><p>1、仅进程创建人可删除自己创建的进程，项目管理员可删除所有进场，无权限不显示删除按钮</p><p>2、点击删除按钮，显示弹窗，二次确认</p><p>弹窗内容：是否确认删除此进程，进程删除后对应产生的数据建无法修改以及编辑，请慎重操作！</p><h3>2.7 新建进程</h3><p>点击新建进程，后自动唤起场景模板引导卡片，新卡片无进程，用户点击任意步骤后，创建新进程实例</p><p>﻿</p><h2>3 概要设计</h2><h3>3.1 系统流程图</h3><p>﻿<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047458906" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><p>﻿</p><h3>3.2 进程设计逻辑</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458907" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><h3>3.3 进程卡片逻辑</h3><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458908" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>4 详细设计</h2><h3>4.1 各模块实现方案</h3><p>1、<strong>自动卡片展示</strong>：每当用户访问项目详情页面时，系统将自动检测当前是否有任何进程正在运行。若存在进程，则立即显示相应的卡片信息；若当前无进程进行，则卡片不会显示，以保持界面的整洁性。</p><p>2、<strong>指令式激活</strong>：用户可通过输入特定的指令来触发相关功能，这些指令包括“进程”、“场景进程”或“模板进程”。输入任一指令后，系统将根据指令内容执行相应的操作或展示相关信息。</p><p>3、<strong>新建进程机制</strong>：若当前系统检测到没有正在进行的进程，并且用户尝试通过任何方式（如点击按钮、输入指令等）触发与进程相关的操作，系统将自动为用户创建一个全新的进程实例，以满足用户的操作需求。</p><p>4、<strong>用户自定义新建</strong>：此外，为了提供更高的灵活性和便捷性，用户还可以根据自己的具体需求，主动选择【新建进程】的选项来手动创建一个新的进程。这一功能允许用户随时根据自己的工作计划或项目需求，快速启动新的任务或项目进程。</p><p>5、<strong>进程的增删改查</strong>：添加、修改名字、搜索等逻辑。</p><h3>4.2 实现方案详细设计</h3><p>以下为详细设计方案</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458909" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>4.3 模版进程卡片设计</h3><p>卡片样式配置规则</p><p>subType: "subType"</p><p>cardStyle: "subType\_card\_style" （控制样式专用）</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458910" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>卡片数据结构</p><pre><code>"cardInfo": {
    "title": "", // 卡片名称
    "subType": "full_work_card", // 卡片标识
    "workItem":{
        "allItem":"全部进程",
        "userItem": "仅我创建",
        "myTurnItem": "轮到我的",
    }
    "newItem":"新建进程",
}
// 返回给后端结构
{
    "ext":{
        "skillCall": {
            "domainCode": "",
            "commandCode": "",
            "workId": ""
        }
    }
}
</code></pre><h2>5 实际效果</h2><p>点击项目详情，聊天助手打开进程卡片：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458911" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>点击 “测2” 进程，进入如下页面：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458912" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿﻿</p><p>**</p>]]></description></item><item>    <title><![CDATA[移动端设备上稀奇古怪的前端问题收集（一） 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047458914</link>    <guid>https://segmentfault.com/a/1190000047458914</guid>    <pubDate>2025-12-08 18:08:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名开发者，bug 往往是我们最怕遇见的东西；而比遇到 bug 更可怕的事情，是定位不到 bug。作为一名前端开发者，与业务逻辑相关的 bug 还相对好定位、好解决一些；而一些与语法特性、平台与设备差异相关的 bug 则更令人头疼一些。这里记录下我在工作中遇到过的稀奇古怪的前端问题，作为给自己的记录和提醒。</p><h2><strong>用 vh 定义全屏显示的问题</strong></h2><p>很多页面因为设计效果的需要，要求正好铺满一整个显示界面、也不允许上下滑动。做类似的需求时，往往直觉会使用这样的代码解决问题：</p><pre><code>{
 height: 100vh;
}</code></pre><p>这样的代码看似很优雅，但是往往会有兼容性问题——不同浏览器定义的视口高度的定义不一致，导致 <code>100vh</code> 并不能真正覆盖全视口高度；还有不少浏览器视口高度数值不变但实际视口大小可变，比如移动端 Chrome 浏览器的导航栏时不时隐藏但网页获取的视口高度不变，这都会导致最终显示效果不符合预期。</p><p>如果要实现全屏幕覆盖不可滑动，更为稳妥和保险的方法是使用绝对定位：</p><pre><code>{
 position: fixed;
 top: 0;
 bottom: 0;
 left: 0;
 right: 0;
}</code></pre><h2><strong>带 alpha 通道的 hex 颜色值失效的问题</strong></h2><p>在较新的 web 标准中，hex 格式的颜色代码也可以表示透明度了，只需要在常见的六位 hex 颜色代码后加两位表示透明度的 hex 值，例如 <code>#66ccff</code> 表示一种蓝色，而 <code>#66ccff80</code> 表示透明度 50% 的这种蓝色（80 是 16 进制的 128，是 256 的一半，即 50% 透明度）。虽然直接这样写代码的行为在前端开发中不普遍，但是设计师交付的视觉稿给出的参考值有不少是这种格式。如果直接把这样的颜色代码用于生产中，可能会出现以下两种问题：</p><p>◦如果你编写的项目引入了 less 或者 sass，在进行打包构建的操作时，部分预处理器无法正确识别带 alpha 通道的 hex 颜色值，因此这部分代码无法被正确转译，最终构建出的生产环境代码中这部分颜色可能丢失。</p><p>◦部分移动端浏览器并未适配带 alpha 通道的 hex 颜色值，因此即使是使用原生 css 完成的代码，也有可能出现在部分手机或部分浏览器颜色不正常的问题。</p><h2><strong>生命周期函数不执行的问题</strong></h2><p>在页面刚打开或准备关闭时，我们往往需要进行一些诸如数据初始化、登入登出、数据上报等行为，而这些往往是借助 Vue 或 React 的生命周期函数完成的。不过，生命周期函数不执行也是常被忽略的 bug，详细来说，又可以分为两类原因——</p><h3><strong>组件被 keep alive 导致未被卸载或重新加载</strong></h3><p>如果是 Vue 中使用 <code>keep-alive</code> 包裹的组件，或在 React 中使用类似的第三方库 keep alive 的组件，只会在第一次加载时执行生命周期初始化函数，且不会执行生命周期卸载函数。这导致的不符合预期的行为很好解决，只需要使用 <code>onActivated</code> 代替 <code>onMounted</code> ，用 <code>onDeactivated</code> 代替 <code>onUnmounted</code> 即可。</p><h3><strong>页面被直接关闭导致框架生命周期函数无法执行</strong></h3><p>不管是 Vue 还是 React，生命周期函数的正确执行都依赖于 Vue 或 React 实例的存在。而当用户直接关闭浏览器页面的时候，Vue 或 React 实例已经被销毁了，生命周期卸载函数当然就无法执行了。处理这种情况也并不麻烦，只需要在生命周期初始化函数中添加对 window 卸载事件的监听，然后把想要进行的操作放到 window 卸载事件函数里就好了。</p><pre><code>onMonted(() =&gt; {  
  window.addEventListener('beforeunload', () =&gt; {    
    // 需要执行的代码 
  });
});</code></pre><h2><strong>文本中的 emoji 上下被裁剪</strong></h2><p>UGC 内容中经常出现文本和 emoji 混排的场景，而有时可能遇到 emoji 上下边缘被裁剪的问题。这往往是由于开发页面时为了限定文本高度和间距或其他排版方面的要求，将 line-height 和 font-size 设置为同样的值，且 overflow 属性被设置为 hidden 。如果出现类似情况，建议去除 line-height 的限制，而通过 margin 等方式控制行距，从而避免 emoji 被裁减。</p><h2><strong>输入框被弹起的软键盘覆盖的问题</strong></h2><p>如果移动端页面中有输入框，那么很可能面临输入框被弹起的软键盘覆盖的问题。一般来讲，对于需要弹起软键盘的场景，较新的浏览器或者移动端 app 的 webview 会自动聚焦到输入框中并滚动到相应位置，来保证输入框的正常显示；但是，对于如下两种情况，弹起的软键盘会将输入框覆盖，影响用户输入。</p><h3><strong>浏览器未能主动聚焦到输入框</strong></h3><p>软键盘弹起时，一般会从底部将页面顶起、压缩视口；视口高度变低了，原先处于显示区域的输入框可能就被挤到输入框外了。如果用户使用的浏览器版本较早或 app 内置 webview 较为特殊，有可能在软键盘弹出后浏览器未能主动聚焦到输入框上。这时，开发者必须主动聚焦到输入框并使输入框滚动到视口内。</p><pre><code>const inputEle = document.querySelector('#target-input');inputEle.focus();inputEle.scrollIntoView();</code></pre><h3><strong>软键盘采用覆盖在视口上层而非压缩视口的方式弹出</strong></h3><p>如果浏览器或 webview 版本较为特殊，且输入框处于页面靠下的位置或者针对视口绝对定位于底部，那么可能会面临更加复杂的情况。刚才已经提到，正常情况下，软键盘弹起的标准做法是从底部将页面顶起、压缩视口高度；但是某些情况下，软键盘并不改变视口尺寸，而是直接盖在视口上方。这就导致页面逻辑上是展示完整的、输入框也正常显示在视口中；但软键盘遮挡了半个页面，也就真正意义上“覆盖”在输入框上。目前主流移动端浏览器较新的版本都不会出现这个问题，但是部分 app 内置 webview 会设置为“软键盘覆盖在 webview 上方”；因此要解决这个问题，必须由客户端更改 webview 的软键盘设置。如果是很旧的浏览器版本或者无法推动客户端开发解决问题，那就只能放弃治疗了。</p>]]></description></item><item>    <title><![CDATA[MQ消息乱序问题解析与实战解决方案 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047458917</link>    <guid>https://segmentfault.com/a/1190000047458917</guid>    <pubDate>2025-12-08 18:07:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. 背景</h2><p>在分布式系统中，消息队列（MQ）是实现系统解耦、异步通信的重要工具。然而，MQ消费时出现的消息乱序问题，经常会对业务逻辑的正确执行和系统稳定性产生不良影响。本文将详细探讨MQ消息乱序问题的根源，并提供一系列在实际应用中可行的解决方案。</p><h2>2. MQ消息乱序问题分析</h2><p>常见的MQ消息乱序问题的根源主要可以归结为以下几点：</p><h3>2.1 相同topic内的消息乱序</h3><h4>1). 并发消费：</h4><p>在分布式系统中，为了提高消息处理的吞吐量，通常会配置多个消费者实例来并发消费同一个队列中的消息。然而，由于消费者实例的机器性能、网络延迟以及处理速度的差异，可能导致消息的消费顺序与发送顺序不一致。</p><h4>2). 消息分区：</h4><p>为了支持更高效的消息存储和消费，MQ系统通常会采用分区化的设计。然而，当同一业务逻辑的多条消息被分发到不同的分区时，消费者在消费这些消息时就可能出现乱序现象。</p><h4>3). 网络延迟与抖动：</h4><p>消息在传输过程中可能会受到网络延迟和抖动的影响，导致消息到达消费者端的时间顺序与发送顺序不一致。</p><h4>4). 消息重试与故障恢复：</h4><p>当消费者处理消息失败或出现故障时，MQ系统通常会进行消息重试或故障恢复操作。如果重试机制或故障恢复策略设计不当，也可能导致消息乱序。</p><h3>2.2 不同topic的消息乱序</h3><p>从相对时间的视角来审视，消息被消费的顺序并不等同于其被发送的顺序。例如，系统A在12:00时向TopicA发送了消息msgA-12:00，而紧接着系统B在12:01时向TopicB发送了消息msgB-12:01。当系统C同时订阅并消费这两个Topic时，它无法预设msgA-12:00会必然先于msgB-12:01被接收。这是由于消息系统在处理过程中，受到诸如消息分区策略、各个Consumer的处理能力以及其诸如网络、堆积、重试等他综合因素的影响，导致无法确保消息遵循严格的先进先出原则。</p><h2>3. 案例分析</h2><h3>3.1 数据迁移过程中的mq消费乱序场景</h3><p>在数据迁移或同步过程中，尤其是双写场景（即数据既写入旧系统，又通过MQ发送到新系统进行异步处理），MQ乱序可能导致严重的数据不一致问题。</p><p>﻿</p><p>!<a href="" target="_blank"/></p><p>﻿﻿</p><p>具体来说，当数据写入时发送INSERT MQ，数据更新时发送UPDATE MQ，如果UPDATE MQ先于INSERT MQ到达目标系统，目标系统可能会基于一个不存在的数据记录进行更新操作。这会导致以下几种情况：</p><p><strong>数据丢失</strong>：如果目标系统没有处理UPDATE MQ中提到的数据记录（因为该记录尚未通过INSERT MQ创建），则更新操作会失败，可能导致数据变更丢失或遗漏。</p><p><strong>数据覆盖</strong>：在高频修改的情况下，频繁更新可能会面临旧数据覆盖新数据的风险，比如UPDATE MQ携带的是旧数据且先于新数据的UPDATE MQ到达。</p><h3>3.2 业务风险分析</h3><p>MQ乱序对数据迁移和同步过程的影响是深远的：</p><p><strong>数据一致性受损</strong>：最直接的影响是数据一致性受损。目标系统中的数据可能与源系统不一致，导致业务决策基于错误的数据。</p><p><strong>用户体验下降</strong>：数据不一致可能导致用户看到错误的信息或遇到功能故障，从而降低用户体验。</p><p><strong>业务中断</strong>：在严重的情况下，数据不一致可能导致业务中断或系统故障，影响企业的运营和声誉。</p><h2>4. 解决方案</h2><p>为了解决这个问题，可以采取以下措施：</p><h3>4.1 顺序消息</h3><p>消息顺序性保证：虽然Kafka不保证全局消息顺序，但可以通过合理的分区策略和消息键来确保同一账单的消息被发送到同一个分区，从而在一定程度上保证消息的顺序性。</p><p>比如RocketMQ支持顺序消息。但是需要注意这是局部有序，非全局后续。具体实现过程：</p><p>1.发送mq消息时，通过selector将同一个业务主键的消息，发送到同一队列中</p><p>2.消费方使用MessageListenerOrderly消费局部有序的消息</p><p>该方案需要发送方和消费方同步改造。</p><p>生产侧：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458919" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><p>消费侧：</p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047458920" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>﻿</p><h3>4.2 前置检测</h3><p>•在消费者处理消息之前，进行前置条件检查。例如，可以查询一个消息辅助表，确保上一个消息已经被成功消费或存入死信队列中。这种检查可以确保消息按照正确的顺序被处理。</p><p>•另一种方法是，在消息中添加序列号或时间戳，并在消费者端进行验证。如果当前消息的序列号或时间戳不符合预期顺序，则暂停处理并等待正确的消息到达。</p><h3>4.3 状态机</h3><p>在消息处理系统中，状态机可以用来定义和处理消息的顺序。每个状态代表系统当前所处的特定条件或阶段，而状态之间的转换则是由接收到的消息触发的。当系统接收到一个消息时，它会检查当前的状态和消息类型，然后决定是否要转移到另一个状态并执行相应的动作。</p><p>对于消息乱序问题，状态机可以通过以下方式解决：</p><p>1.<strong>定义状态转换规则</strong>：首先，需要定义一套明确的状态转换规则。这些规则应该基于业务逻辑来确定，以确保消息按照正确的顺序被处理。例如，如果系统要求先处理事件A再处理事件B，那么状态机就应该在接收到事件A后转移到能够处理事件B的状态。</p><p>2.<strong>状态检查与消息缓存</strong>：当系统接收到一个消息时，它会检查当前的状态是否允许处理该消息。如果当前状态不允许处理该消息（即消息的顺序不正确），则可以将该消息缓存起来，等待状态机转移到正确的状态后再进行处理。</p><p>3.<strong>状态转移与消息处理</strong>：一旦状态机转移到正确的状态，它就可以处理缓存中的消息。这可以确保消息按照正确的顺序被处理，即使它们最初是以乱序到达的。</p><h3>4.4 监控与报警</h3><p>建立系统的监控和报警机制，及时发现并处理消息错乱等异常情况。</p><p>通过采取以上措施，可以大大降低账单还款系统中消息错乱导致的问题，提高系统的稳定性和用户体验。</p><h2>5. 小结</h2><p>MQ消息乱序是分布式系统的常见难题，影响系统稳定性和业务一致性。本文深入解析问题根源，探讨了顺序消息、前置检查、状态机等实战解决方案，为实际开发中的问题解决提供有力参考。</p><p><em>文章中难免会有不足之处，希望读者能给予宝贵的意见和建议。谢谢！</em></p>]]></description></item><item>    <title><![CDATA[DORA 2025：AI 能力模型与软件研发效能成熟度路线图 PM老周 ]]></title>    <link>https://segmentfault.com/a/1190000047458921</link>    <guid>https://segmentfault.com/a/1190000047458921</guid>    <pubDate>2025-12-08 18:07:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>在前一篇文章中，我们通过分析 DORA 2025 报告的七类团队画像，帮助企业识别不同团队在 AI 研发效能提升中的位置。本文将深入探讨 DORA 2025 提出的 AI 能力模型，并结合企业的实际情况，构建一个 软件研发效能成熟度路线图，为中高层管理者和 PMO 提供一套分阶段实施的可行方案，确保 AI 投资能够稳定、持续地提升研发效能。</blockquote><h2>从“能力模型”到“成熟度路线图”：理解 AI 研发效能的系统性</h2><p>DORA 2025 报告强调，AI 是放大器，而非万能钥匙。这意味着 AI 不会自动修复组织中的问题，而只是放大已有的优势或短板。这一观点对于很多企业来说，尤其是中国本土企业，具有特别的现实意义。在我与众多企业合作的过程中，我发现很多公司过于依赖工具的引入，而忽视了自身能力基础的建设，导致 AI 在实践中的效果远低于预期。</p><p>DORA 2025 提出的 AI 能力模型 直接回应了这一挑战。它帮助团队从技术基础、流程治理、数据管理等多维度进行自我评估，确保 AI 的引入能够获得实实在在的效益。</p><p><strong>本节小结：</strong>如果你希望通过 AI 获得长期、稳定、可持续的研发提升，就必须先评估自身是否具备“承载 AI 的能力基础”。AI 能力模型，正是量化这个基础的标准。</p><h2>DORA 2025：AI 能力模型的七项关键能力</h2><h4>1. AI 能力模型的框架</h4><p>DORA 2025 提出的七项关键能力涵盖了 AI 成功实施的各个维度，从技术能力到流程管理，再到团队文化和组织结构。这些能力是实现 AI 研发效能的基础，缺一不可。</p><ol><li>明确且已共识的 AI 立场：团队和组织对 AI 的使用政策、目标、权限和控制有清晰的共识。只有当组织全员理解并支持 AI 立场时，才能有效避免冲突和内耗。</li><li>健康的数据生态系统：数据是 AI 的基础，数据治理的规范化、数据质量的提升至关重要。拥有干净、结构化、规范化的数据系统，是确保 AI 提高研发效能的前提。</li><li>AI 可访问的内部数据：AI 工具应能安全访问内部数据系统，包括代码库、文档、知识库等，才能在实际工作中产生真正的效能提升。</li><li>稳健的版本控制与变更管理实践：AI 带来的变更往往更加频繁和大规模，因此在引入 AI 后，确保版本控制和变更管理的稳定性至关重要。</li><li>小批量 / 小颗粒度工作模式：AI 有助于减少传统开发中的大规模变更，通过小步快跑、频繁提交、快速反馈等方式，降低交付不稳定性。</li><li>以用户/价值为中心的优先级与决策机制：团队要始终以用户和产品的实际价值为导向，优先处理最能为用户创造价值的工作。</li><li>高质量内部平台与基础设施：包括 CI/CD 流水线、自动化测试、合规性检查、监控等基础设施，这些系统必须支持快速部署、回滚以及 AI 工具的无缝集成。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458923" alt="图片" title="图片"/></p><p>我曾多次遇到这样的情况：企业投入了大量的资金购买 AI 工具，并在各个团队中进行推广，然而效果却远不如预期。核心原因在于：工具本身并不决定研发效能，反而是组织的整体能力决定了工具能否发挥真正的价值。</p><p>AI 能力模型的七项能力，正是帮助团队和组织诊断并逐步完善这一能力基础。通过逐步构建这些能力，组织可以确保在 AI 的辅助下，团队效能与研发效能能够持续提升。</p><h2>AI 研发效能成熟度模型：分阶段实施的可行路径</h2><h4>1. AI 研发效能的成熟度分阶段</h4><p>DORA 2025 提出了四个阶段的 AI 研发效能成熟度模型，帮助企业通过阶段性实施，逐步提升 AI 能力和研发效能。每个阶段都有明确的目标与关键行动，确保企业能够稳步推进 AI 的应用，并在实践中积累经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458924" alt="图片" title="图片" loading="lazy"/></p><h4>2. 为什么需要分阶段实施？</h4><p>企业在引入 AI 的过程中，往往会急于追求技术突破和快速见效。然而，技术本身并不会自动解决组织中的复杂问题。分阶段实施成熟度模型可以帮助企业避免盲目加速，同时确保在每个阶段有充分的准备和基础支撑，避免技术落地后的风险。</p><h4>3. 管理层注意事项：</h4><p>从基础开始：从 Level 0 到 Level 1，团队首先需要搭建起稳定的研发基础设施，解决流程瓶颈。</p><p>逐步引入 AI 工具：在 Level 1 和 Level 2 阶段，逐步引入 AI 工具，并将其与已有的开发流程深度融合，保证稳定性。</p><p>强调协同与文化建设：到达 Level 3 阶段时，企业的核心是推动组织文化的变革，确保 AI 工具和团队协作能够无缝结合，实现系统化的研发效能提升。</p><h4>4. 如何落地实施？</h4><p>Level 0 → Level 1：打好基础：重点建设团队基础设施（版本控制、自动化测试、CI/CD 流水线），并为 AI 引入打好基础数据管理和安全权限架构。</p><p>Level 1 → Level 2：工具引入与集成：根据团队画像分析，选择合适的 AI 工具，逐步引入 AI 助手（如代码生成、测试工具、需求分析等），提升研发和交付质量。</p><p>Level 2 → Level 3：全面优化与智能化：整合 AI 进产品设计、需求分析和决策过程中，借助 AI 推动更智能化的产品优化和创新。</p><h2>如何进行组织和团队的 AI 能力评估？</h2><p>为了评估团队的 AI 能力，可以从以下几个维度进行自我诊断：</p><ul><li>AI 立场：团队是否已经达成对 AI 使用的统一认识，是否有明确的使用政策和审批机制？</li><li>数据治理与访问：数据是否结构化，能否方便地接入 AI 工具进行分析？</li><li>平台与基础设施：团队是否具备支持 AI 工具顺利运行的平台和基础设施？</li><li>协作与文化：团队的文化是否支持 AI 的顺利引入，是否具备自我学习和持续优化的能力？</li></ul><p>评估结果将帮助管理者确定当前阶段所在，并制定符合团队实际情况的实施路径。通过分阶段实施，管理者能够清晰地定义每个阶段的目标与行动步骤，确保 AI 工具的引入能够与组织的成熟度相匹配。</p><p>在 DORA 2025 的框架下，我们可以看到 AI 研发效能的提升是一个复杂而渐进的过程。通过明确的 AI 能力模型 和 分阶段的成熟度路线图，团队能够有效地避免盲目跟风，确保 AI 投资能在团队的具体需求下发挥最大价值。</p><p>对于管理者而言，AI 研发效能不仅仅是工具问题，更是组织能力建设和文化变革的系统工程。在 AI 技术日新月异的今天，只有坚持从能力提升和流程优化入手，才能确保 AI 对研发效能的持续增值。</p><p>在下一篇文章中，我们将进一步探讨 AI 驱动的价值流管理与端到端研发效能提升实践，并展示如何将 AI 与价值流管理（VSM）结合，打造具有可持续竞争力的研发体系。</p><p>敬请期待：《DORA 2025：AI 驱动的价值流管理与端到端研发效能提升实践》</p>]]></description></item><item>    <title><![CDATA[【有搜必应】HarmonyOS TOP5热搜技术问题解析第四期 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047458951</link>    <guid>https://segmentfault.com/a/1190000047458951</guid>    <pubDate>2025-12-08 18:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>本文原创发布在<a href="https://link.segmentfault.com/?enc=JM6Ro0BiQDxpjAA9gKUHCw%3D%3D.UEPP7TitWsxosLK%2FM5Z8pxnq1cR88llAYs5Yr%2BRJLlMx%2FtTHlwepHbl7eXqEhTG%2BwdzqKprlZN44PtNTq%2F36uVw%2BWtTrqs4T4CTk0Giz0bmNLAQYd5GLMBF%2BCHVqcWix" rel="nofollow" target="_blank">华为开发者联盟社区</a>，欢迎前往与更多开发者进行互动。<br/>更多相关问题可点击原帖进行交流：<a href="https://link.segmentfault.com/?enc=jWXlBhrLzwKuUb0V2EX0Fw%3D%3D.Uu0L%2BjaPOiJaP8eTToCi1H%2F3TY6HeKSbgQtlj8RNhogaNtMyqBeo5hXEBmGYx6QSwSYc2hZaG9ar%2F9NfFdO%2F%2BULJuzmzpqR4EE%2BMdShYjQtlqjMXwpNWqNZP5eaoFb1Hg2H83PvXEiKkJdOT1LA3%2BPhDFHZH3O10dFShh41PEWYV7jOJs%2FBr6cTDigvNIh1Q" rel="nofollow" target="_blank">【有搜必应】HarmonyOS TOP5热搜技术问题解析第四期</a> 。</blockquote><h4>本期热搜揭秘：</h4><p><a href="https://link.segmentfault.com/?enc=tR2C3YLKtDG17mnQf60PMw%3D%3D.dEFge1O6mt9oLeesT2bADJ4vAnNLCIS80YNIYQn%2FOpMFUwjpIQOd0VsjYG%2B5kruHB0TsCAnHYc0Ag7aNL3qx6%2FlVD1e3lv%2B7zOnmLgdsqDxuiMYamV8whYnCDrjipV0X372S%2BURj%2B1L0zgU1M1kPBw%3D%3D" rel="nofollow" target="_blank">【编译工具】打开工程结构为空，且编译报错hvigor ERROR: 00308002 Operation Error</a></p><p><a href="https://link.segmentfault.com/?enc=DI40fmLmJNAZtMcF3xpF6g%3D%3D.BNhn%2F%2FUOJAYr9whyox%2Fe0hONOfS7fhbesJvVl050MkWwNUd%2B%2Bd07feytDaKxmg1y6thssX971Dp0c9DaHYduSR8NIDfkLWxIrZU8J%2BDf5IYKvpRBgXYDq3t4jHIqWDNWohig1oTXEeEb%2BhsjxvHJZQ%3D%3D" rel="nofollow" target="_blank">【编译工具】DevEco Studio 中使用 sys.media 图标报红问题解析</a></p><p><a href="https://link.segmentfault.com/?enc=N12RYIzNVLJ%2FcZZ8NBb7XA%3D%3D.RIN5jwBWhZmasgXOs3vDOvQCcdEBJuEeAjUi9CbPtZ2Y1lNZxxCwFGT14Ww%2F2oqnFd7ZIMTgJR8WFDs8ECft4Mlop6z7PuqUaRLH9BqJGvooAF98eoIc2cSe27HxUY0dsuZ2RogN%2BzN94LvvotIxRb4kD64FHlMtsefEFb7BXwbnvWJt3i56BYbyd%2FmJEoRq" rel="nofollow" target="_blank">【ArkUI】如何在其他组件或者模块中得到windowStage并使用getMainWindow()方法</a></p><p><a href="https://link.segmentfault.com/?enc=F5Pswk8Rve%2BkYkrr0%2Bkr5w%3D%3D.OtTGJM0ji8bnlLDfjsKbcE%2Fz5%2FZXQjoKcYgQQe%2FlFmpEr7wx6vzUxey0ONZm5ZBJoSIk7LSTFK8rFI5at8ENCUHPb7yrSdOe3NmIw%2BMAFY4JaKfC9ya7xKwVPMtHVY1AlYMt0xj%2FCInbeuY6Mkn%2Bb5utOzMQLpdCRYw8MXpSbCPMAHOzBy9%2FfZz2bUQso5EV" rel="nofollow" target="_blank">【ArkUI】V1装饰器如何迁移至V2</a></p><p><a href="https://link.segmentfault.com/?enc=T9%2B1oSsq7RGGt%2FiJHaIZ3Q%3D%3D.1aPNb3qsjdw%2BLAdTbEqkOsHJ2tVLFBlycjeqa5E%2B5Id2lZONXirngD8kpnMAHb6ZycHd7UjffDrtYRHiV2%2FhPNH03HAB%2BjcuextNb7uwoUJNOXGHmwVOlyg37vmyT%2FPErkCY%2F%2BVJUwPStVNuDPj7YJKFzlqrOhQj4qjaup2CToMnVjgLjgBHjJCfWFgNAhpW" rel="nofollow" target="_blank">【媒体&amp;图形】如何将图片保存到本地相册</a></p><p>期待您在论坛中继续发声：无论是提出新的疑惑、发表见解、或分享实战经验，都会为鸿蒙社区注入前行的力量，也是让我们做得更好的动力！若您存在疑惑，可使用社区-问答-"我要提问题"进行提问。<a href="https://link.segmentfault.com/?enc=qnQBQ4IQnQvzlt%2F0vyK%2F4w%3D%3D.%2Fh%2BMOdjvKK8IOgmICGrxoc%2BY6CHJdgfT8N%2Fc797V3xSXh1uqB0qh5nGtUva7GulWHJwF%2BNcl7vOIpkQ3KfdcI4QEYSIScylY4j52i8hPgmvCNguTVXD6g%2FEeS9SR98DL" rel="nofollow" target="_blank">问答专区-华为/鸿蒙开发者论坛</a></p><h4>往期问题回顾：</h4><p><a href="https://link.segmentfault.com/?enc=38FSC30g1u7wXBkhd2h47Q%3D%3D.oje%2BOOXSofmR48zXPzJ5hKv31N%2FjW3gwFYO3fKUXhLqP2LtIIYh%2FjAFWgE0i2q8WpPtk3E5T%2Fqbz4lYtzZzV4dIG1gJCwprjmU10%2BsclczbZnGbI1DIL9bFc3rf4KR%2FKoO7m3gcaY3fJkvALqDXKblZSycaAIvxyCbi%2BeLQsVhSzxdC4jup1VyUO33DHTNds" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第一期</a></p><p><a href="https://link.segmentfault.com/?enc=CIMrvETCM6JXRIYqkp74fg%3D%3D.DLn5y5jqGaAiQPmOVxSXKYUhOZ%2B%2FTMY9ecmAHdJao5SmedRxNZXCX4LZ%2B66gIxRT0ibl%2FVMURfCmxVcRg7Cr3bJa82g4ebeTF71oHBaIiOKe1J7YQo4NT86EBDnDiA6CZkF5sgCMqSLpbrMM7NdRo951kwEVuvmaRmREpZlrzxKJTBWEnpbZZCQodVu9Tf%2Ba" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第二期</a></p><p><a href="https://link.segmentfault.com/?enc=fvBHgfHwwaEookVPBY%2BpUg%3D%3D.5A7RzqtTbPlx6bzHxbuEvBxK9k0NlXHzdaKO3THVgbF56sVkYmtk%2FgvZfkjoQEp3Pv%2BDlR2%2BcD983n3bm%2FMlZFTLWY9mw6PFaQmQANk71Rx8EtLTpslKSc7im6cBGvZlC8pUjxwsURWR097ii9Jgpd%2Fb%2BpuyIlDrVeGCr4bM574kmVBfUAL23n8SlC3lolsd" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第三期</a></p>]]></description></item><item>    <title><![CDATA[【有搜必应】之HarmonyOS热搜技术问题解析 系列汇总（持续更新中...） 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047458981</link>    <guid>https://segmentfault.com/a/1190000047458981</guid>    <pubDate>2025-12-08 18:05:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>本文原创发布在<a href="https://link.segmentfault.com/?enc=gnUXn%2FdszOFkWpoeF89Nzg%3D%3D.Vj36AgbqAy6kL9RWFnTtzy%2BUUcxXZtpS%2F7AUQyM2MoNI30VAAxd5Pjs9rtx7cPhd58XkoC9B6AaueOVtEP4sOJ6KuIyvKWiCS%2FHfl2IDt3mh%2BFL32V%2Fh73EEsxVcSxlI" rel="nofollow" target="_blank">华为开发者联盟社区</a>，欢迎前往与更多开发者进行互动。<br/>更多相关问题可点击原帖进行交流：<a href="https://link.segmentfault.com/?enc=2SEct2sF%2FljoiPUEvH7m4Q%3D%3D.Cy56ahsuiyfZGDNlFbCq8SsbMqNVsJg6V9QF0sQ6968y7nXGV4JjL0%2BQPzdn3LoMNGOhv6cxdBSyDVdH7nZi66S0YOHZgzcnXdpTS%2FhlbxnHpBMZlsFuPpFA1GtBfuIaK5oBTIVEtnvRsrn25QJ5p2u2%2BxByoz72A1gnUpcWSVi13xAUzbG4QUR0vtoD8XZP" rel="nofollow" target="_blank">【有搜必应】之HarmonyOS热搜技术问题解析 系列汇总（持续更新中...）</a> 。</blockquote><p>HarmonyOS开发者小伙伴们，每一个热索词的背后，都是您最迫切的技术问题诉求与最真实的痛点；每一个热搜词的背后，更代表了众多开发者遇到的共性难题。为助力大家扫清Top开发障碍，我们选取了社区高频的热搜问题，进行深入剖析，推出《有搜必应》专栏，旨在集中解决共性问题，为大家勾勒一份鸿蒙开发的“热点地图”。在精准定位问题的基础上，我们将提供一份经过验证的解决方案与最佳实践，化热搜问题为能力提升的阶梯，让每一次技术探索事半功倍，助力大家在鸿蒙开发之路上行得更稳、更远。</p><p>在此，我们由衷地感谢每一位热心参与、乐于分享的开发者，是你们的热情与智慧，让这个社区充满了生机与活力，每一次的解答都是对技术探索精神的最好诠释。同时，我们也诚挚邀请更多的开发者加入到这场智慧碰撞的盛宴中来。无论是抛出难题寻求解答，还是慷慨解囊分享经验，您的每一份参与都将为鸿蒙开发者社区注入新的活力，推动我们共同前行，在技术的海洋中扬帆远航。</p><p>请持续关注我们的《有搜必应》系列帖，我们会定期更新内容，助开发者一臂之力。让我们携手共进，共创鸿蒙开发的辉煌未来！</p><h4>链接直达问题详情及解析：</h4><p><a href="https://link.segmentfault.com/?enc=NMe%2B0tCSVrQe9eYkhv1Piw%3D%3D.YzwKdxMUH0tTnyxSipR1jLRo7emNigWWlqdXiB9NZZW9DF330QICOXuQZUcSUzhPIZv%2BumbV3anguc1%2B3aaLl%2FxzWelQB5j%2B7sLQ28aUzAhpZfV0POemKbplc9WYxy0gYSDEMVvslP7Mmt8dn6GbTUXwv0mmgp2Ab5ryvrjhoes6lA2pmdtM4hTlmo0RxVV3" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第一期</a></p><p><a href="https://link.segmentfault.com/?enc=ISNRjdKkJFiyq%2FbjnRcNVQ%3D%3D.ULfPFs3Mp5q2tVs714%2BjqCTt8YaDled4h3pbUbzcDI3EVVnhK4k7r9NuegpSBO7tMNmQ1ICcw4UGeTEQyiLdGr5dFIuZaDVwAZBRKwuSpGSkOHkhDzOsVSASzioa9YwUCyR5UkIJqZ8pHs1PiLRanBfk0hK8CSr0N1maDn9hrZDuld8qeofF2sv3z5qDWvZV" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第二期</a></p><p><a href="https://link.segmentfault.com/?enc=CxwGLGkfdcULWh8UUOxrgA%3D%3D.%2FdttsTyY1oTKVIQMBu%2FPfdL2kInjBm3w%2F9zLjJ4k%2B63cs98pekxtQt0PRpTpKyiIh3GDvHpS2C4qOoH0vkx6BN6QG82UlDyUujdYjGVyBcKReQl%2FFpnkhngDvfcqYT1BHcKQhqAX1R3wxWDRjdMFxAN50XtNtFrnSzr9r0JN9e6JDtqQC0bbOLGOn6XOqEiS" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第三期</a></p><p><a href="https://link.segmentfault.com/?enc=Uc3cH7EJkO0qKMoW4x8hnw%3D%3D.Xp5JMiXI4fV4aXfTU0a5m6msYhsiPxhNJat4qjLUToSYjL87q1YjseFy%2BNrZ74iwwlvoFXMRekezUNrYxxeNm2aGyuMBts5UFHCIAt2SY8emZ1KG3R7PfXZuOqmKZmutJ%2BOKzEwT%2Fhs2eN%2B5Cd6tL4emi2Nw8Hp6P8bSOJzPUrV0ki9iS%2FZ5SFrZK81z5956" rel="nofollow" target="_blank">【有搜必应】HarmonyOS 热搜技术问题解析第四期</a></p>]]></description></item><item>    <title><![CDATA[从“是什么”到“为什么”：Aloudata Agent 智能归因的底层逻辑与配置指南 Aloudat]]></title>    <link>https://segmentfault.com/a/1190000047458988</link>    <guid>https://segmentfault.com/a/1190000047458988</guid>    <pubDate>2025-12-08 18:05:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当数据看板上销售额那条红色曲线突然掉头向下时，业务主管的第一反应不再是手忙脚乱地召集数据分析师会议，而是转向电脑屏幕，平静地输入一个最直接的问题：“为什么？”</p><p>面对海量指标波动、业务异常或营销效果变化，分析师往往只能回答“发生了什么”（What），却难以深入解释“为什么会这样”（Why）。这种从“What”到“Why”的鸿沟，正是 Aloudata Agent 智能归因功能试图解决的核心问题。</p><p>Aloudata Agent 是 Aloudata 推出的一套分析决策智能体，将 NoETL 明细语义层作为数据底座，以指标为中心进行语义一致的对话式数据分析。通过自然语言即刻获取数据结果，支持智能数据结果解读，以及智能多维归因和因子归因分析，让企业深层次洞察异常数据波动原因。</p><p>本文将深入剖析 Aloudata Agent 智能归因的底层逻辑，并提供一套实用的配置指南，帮助用户真正实现从“是什么”到“为什么”的跃迁。</p><p>01 智能归因：从数据报表到决策引擎<br/>传统 BI 工具擅长展示数据的当前状态和历史趋势，但当业务人员看到指标异常时，仍需依赖经验猜测，或向数据团队提出新的分析需求，这个过程缓慢且低效。</p><p>Aloudata Agent 的智能归因功能，让每一次数据波动分析都具备可组合、可追溯、可解释、可复用的业务价值，真正赋能企业在复杂数据环境中做出敏捷、精准、可执行的决策。</p><p>现代企业面临的数据环境日益复杂，指标间的关联性不断增强。单个业务指标的波动往往由多个维度、多个因子共同作用导致。智能归因系统能够穿透数据表象，在多维业务空间中精准定位问题根源，将数据从静态报表转化为动态决策引擎。</p><p>02 技术基石：NoETL 指标语义层如何支撑可信分析<br/>Aloudata Agent 智能归因功能的核心支撑是其独创的 NoETL 指标语义层。这一技术架构解决了企业数据智能分析中长期存在的“数据幻觉”、口径不一致和灵活性不足等痛点。</p><p>与传统数据分析架构不同，NoETL 指标语义层在物理数据层和应用层之间构建了一个逻辑语义层，系统化管理指标、维度、业务计算逻辑及指标间的血缘关系。</p><p>这张“业务地图”为智能归因提供了统一的语义理解基础，确保不同用户对同一业务概念的理解完全一致。</p><p>当用户进行归因分析时，大模型首先借助语义层理解用户意图，将其转换为包含指标、维度、过滤和时间查询等规范的标准查询请求（MQL），再转化为 100% 准确的、可执行的 SQL 语句。</p><p>这种“NL2MQL2SQL”的技术路径与传统的“NL2SQL”或“NL2DSL2SQL”相比，从根本上保障了分析的一致性与准确性。</p><p>指标语义层在企业数据分析中扮演三大关键角色：一是消除“大宽表依赖”，支持灵活的维度归因下钻；二是沉淀计算逻辑，赋能大模型识别因子关系；三是依据指标类型，智能匹配贡献度算法。</p><p>对于“销售额=客单价×客户数”这样的复合指标，语义层明确定义了计算逻辑，使系统能自动识别指标间的计算关系，并将变化归因于相应因子。</p><p>03 双路径归因：维度拆解与因子追溯的精准诊断<br/>Aloudata Agent 的智能归因功能通过双路径归因框架实现多维度、多层次的根因洞察。这一框架包括维度归因和因子归因两条互补路径，分别从不同角度揭示数据波动的本质。</p><p>维度归因专注于识别影响目标指标变化的关键业务维度，如渠道、区域、品类、门店等。系统通过维度下钻与贡献度计算，量化各维度对整体变化或差异的贡献权重，帮助用户锁定问题焦点。例如当某电商企业发现“ 618 销售额下降”时，Aloudata Agent 通过维度归因识别出两大主因：直播渠道转化率下降 15%、客单价减少 8%。</p><p>因子归因则聚焦驱动目标指标变动的关联因子指标，通过指标间的计算逻辑与影响路径，识别哪些前置因子的变化是导致最终结果差异的根本动因。对于复合指标（如销售额=客流量×转化率×客单价），因子归因能追溯其构成要素的变化，提供更具操作性的改进方向。</p><p>为了全面覆盖业务分析场景，Aloudata Agent 将归因分析需求归纳为四象限场景矩阵，包括“维度归因x时间波动”、“因子归因x时间波动”、“维度归因x同类对比”和“因子归因x同类对比”。</p><p>这种设计确保企业无论面对时间序列波动还是实体间差异，均能快速定位根因。</p><p>04 场景实战：从数据异常到业务决策的闭环分析<br/>以连锁餐饮品牌 A/B 门店业绩差距分析为例，当用户提出“A 门店销售额比 B 门店高 20%，原因是什么？”时，Aloudata Agent 首先进行维度归因，自动拆解至客群结构、促销策略、店员配置等维度，发现 A 门店外卖订单占比高 23%、B 门店高峰时段等位时长多 12 分钟。</p><p>接着进行因子归因，进一步分析构成因子，识别出 A 门店的“外卖客单价”比 B 门店高 15 元、“高峰时段翻台率”低 0.3 次/小时。</p><p>基于这些分析，最终生成策略建议：B 门店优化外卖菜单设计提升客单价，A 门店增加高峰时段人力提升翻台率。整个过程无需数据工程师预处理数据，业务人员通过自然语言交互即可完成分析。</p><p>另一个典型场景是汽车企业分析“毛利率下降”。Aloudata Agent 通过因子归因计算出：原材料成本上涨贡献 60% 影响、生产效率降低贡献 30% 影响。</p><p>进一步拆解发现，原材料成本上涨源于钢材价格波动，而生产效率降低则与生产线故障率上升直接相关。这种层层下钻的分析方法，使企业能够精准定位问题根源，而非停留在表面现象。</p><p>05 配置与实践：构建企业专属的智能归因体系<br/>要充分发挥 Aloudata Agent 智能归因的价值，企业需要系统性地进行配置与落地。这一过程可以分为数据准备、语义构建、场景适配和知识沉淀四个关键阶段。</p><p>首先，企业需要将数仓中的 DWD 层数据接入 NoETL 明细级语义层，标准化定义基础指标和维度。这一步确保数据源的完整性与准确性，为后续分析奠定基础。例如，仅需定义“销售额”这一基础指标，系统便能支持用户围绕时间趋势、渠道分布、品牌表现等多种维度进行灵活查询和分析。</p><p>其次，企业应基于业务逻辑构建指标间的计算关系和因子树。对于 GMV 这样的复合指标，需要在语义层明确定义其计算表达式（如“GMV=客单价×客户数”），使系统能够自动识别和利用这些关系进行因子归因。</p><p>同时，针对比率型指标（如折扣率、利润率），需要配置相应的贡献度算法，以准确计算各维度对变化的具体贡献。</p><p>在场景适配方面，Aloudata Agent 支持创建场景化智能分析助手，如财务分析助手、人资数据助手、区域经营数据助手等。</p><p>每个助手可配置独立的资源管理，确保信息隔离，避免跨业务领域的数据干扰。这种设计让不同业务角色能够更直接地获取所需数据结果和分析报告。</p><p>最后，知识沉淀是确保智能归因持续优化的关键。Aloudata Agent 支持用户维护个人术语知识和分析思路，并将打磨好的报告保存为模板，将个人分析框架转化为团队可复用的数字资产。</p><p>06 核心优势：智能归因如何重塑企业决策逻辑<br/>与传统的归因分析方法相比，Aloudata Agent 的智能归因展现出多维度优势，这些优势共同重塑着企业的数据决策逻辑。</p><p>它解决了传统方法中常见的“指标口径不一致”问题。基于统一的指标语义层，无论谁提问、如何提问，指标的计算口径始终保持一致。这种一致性对于跨部门协作和长期趋势分析至关重要，避免了因口径差异导致的决策偏差。</p><p>智能归因提供了传统方法难以实现的分析灵活性。用户可自由选择分析维度，系统自动检索指标与维度，生成对应的归因查询，无需依赖预先生成的大宽表。这种灵活性使业务人员能够根据实际需求动态组合维度，快速定位影响指标变化的关键因素。</p><p>在查询性能方面，智能物化加速和查询路由改写技术保障了海量数据查询的秒级响应，即使面对百亿级数据，也能稳定产出分析结果。这种性能优势使实时决策成为可能，大幅缩短了从数据异常到行动干预的时间窗口。</p><p>安全可控是智能归因的另一重要优势。基于指标权限管控和行列级数据权限配置，系统能够保障数据查询的安全可控。在归因分析过程中，系统会动态验证用户是否具备访问相关指标及行级数据的权限，确保数据安全合规。</p><p>07 未来演进：智能归因在企业智能化转型中的角色<br/>面向未来，智能归因将与更多的 AI 能力融合，形成更强大的分析决策智能体。Aloudata Agent 已在这方面进行了有益探索，通过“智能融合报告”功能，将归因分析结果自动整合到结构化报告中，生成包含趋势图表、归因结论、文本解读和策略建议的可执行洞察。</p><p>更关键的是，Aloudata Agent 智能融合报告”功能允许分析师自定义报告结构与章节逻辑，将个人分析方法论沉淀为团队可复用的数字资产。这种知识沉淀机制使企业的分析能力不再依赖个人经验，而是转化为可持续迭代的组织能力。</p><p>随着技术发展，智能归因有望实现更高级的预测性分析。基于历史归因数据和业务知识，系统不仅能解释已发生的波动，还能预测潜在风险，提前预警并给出预防建议，真正实现从“事后归因”到“事前预防”的转变。</p>]]></description></item><item>    <title><![CDATA[人才盘点分析解决方案：助力企业精准识才，实现人岗高效匹配 容智信息 ]]></title>    <link>https://segmentfault.com/a/1190000047459005</link>    <guid>https://segmentfault.com/a/1190000047459005</guid>    <pubDate>2025-12-08 18:04:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459007" alt="图片" title="图片"/><br/>在人才竞争日趋激烈的市场环境下，企业的人力资源管理正面临前所未有的挑战。某高速成长的科技企业人力资源负责人对此感触颇深：“我们每天需要处理数百份来自不同渠道的简历，但招聘效率却不尽如人意。更关键的是，即便人才入职后，我们也缺乏系统化的方法来评估其真实潜力，导致内部晋升和转岗决策常常依赖管理者的主观印象。”这家公司的困境并非个例。在传统人力资源管理模式中，简历筛选耗时耗力、人才评估标准不一、内部人才透明度不足、人岗匹配度难以量化等问题，已成为制约许多企业组织效能提升的普遍难题。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459008" alt="图片" title="图片" loading="lazy"/><br/>为系统化解决这些痛点，该公司开始寻求人力资源管理的数字化转型路径。经过多方评估，他们引入了一套智能化人才盘点分析解决方案，旨在通过技术手段提升人才管理的精准性与科学性。该解决方案并非简单地替代人力资源专业人员，而是通过结构化数据处理与智能分析能力，为其提供更全面、更客观的决策支持，将人力资源团队从繁琐的事务性工作中解放出来，专注于更具战略价值的人才规划与发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459009" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459010" alt="图片" title="图片" loading="lazy"/><br/>过去，该公司的招聘团队需要手动处理大量不同格式的简历文件，平均每个岗位的初步筛选需耗费1-2个工作日。引入新系统后，情况发生了显著变化。具体应用：系统通过自然语言处理和文档解析技术，能够自动识别并提取简历中的关键信息，包括教育背景、工作经历、专业技能、项目经验等，并将其结构化存入统一的人才数据库。量化成效：原来需要3天完成的300份简历初步筛选工作，现在可缩短至2小时内完成。系统还能根据预设的岗位要求，自动生成包含匹配度评分的候选人短名单，使招聘专员能够快速聚焦于最合适的潜在人选，将核心岗位的平均招聘周期缩短了约40%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459011" alt="图片" title="图片" loading="lazy"/><br/>“我们之前对内部人才的了解往往是片面的、零散的，”该公司人力资源负责人坦言，“不同部门对同一位员工的能力评价可能截然不同。”具体应用：该系统整合了员工的绩效数据、项目经历、技能认证、培训记录等多维度信息，构建出统一、全面的人才数字档案。这些档案不仅包含“硬技能”标签，还通过分析项目角色与贡献，识别出员工的协作能力、问题解决风格等“软性特质”。实际价值：在最近一次内部竞聘中，HR部门利用该系统为三位候选人分别生成了详细的能力雷达图与发展建议报告。这份客观的数据支撑，使晋升委员会的讨论更加聚焦、决策过程更加透明，最终入选者也因此获得了更高的团队认可度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459012" alt="图片" title="图片" loading="lazy"/><br/>该公司曾面临一个典型困境：某个关键岗位空缺时，是优先考虑外部招聘，还是在内部寻找有潜力的人选进行培养？决策常常在两难中徘徊。具体应用：系统提供两种核心匹配模式。一是“岗位-人才”匹配：当出现岗位空缺时，可基于该岗位的能力模型，从内外部人才库中寻找匹配度最高的候选人。二是“人才-岗位”匹配：针对特定员工，分析其能力特质与组织内其他岗位的适配度，为内部调岗或职业发展提供参考。典型实例：公司希望为新兴的数字营销业务组建团队。通过系统的“相似人才寻找”功能，以现有优秀数字营销专家为标杆，从内部其他部门发现了两位具备相关潜质的员工，经评估后成功转岗。这一方面快速填补了人才缺口，另一方面也提升了员工满意度，实现了双赢。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459013" alt="图片" title="图片" loading="lazy"/><br/>实施该解决方案六个月后，该公司在人力资源管理的关键指标上取得了显著改善：招聘效率：核心岗位平均招聘周期缩短35%，简历初筛耗时减少80%；人才匹配度：新入职员工半年内绩效达标率提升22%；内部流动性：内部转岗/晋升比例从15%提升至28%，岗位适应期平均缩短30%；管理决策支持：人力资源数据分析报告产出时间从数天缩短至实时可获取。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047459014" alt="图片" title="图片" loading="lazy"/><br/>“这套系统的价值远不止于效率提升，”该公司人力资源负责人在回顾转型历程时总结道，“它正在改变我们人力资源部门与业务部门的对话方式。我们现在能够基于数据，与业务领导者深入讨论人才结构优化、关键岗位继任计划等战略议题，真正从支持部门转型为战略伙伴。”当前，人力资源管理的数字化转型已进入深水区。领先的企业正从简单的事务自动化，迈向基于数据分析的人才战略规划。这种转变的核心，在于将人力资源管理的重心从“流程与事务”转向“人与价值”，通过精准识才、科学用人，最终构建起持续的组织竞争力。<br/>通过智能化工具赋能，人力资源专业人员得以更专注于理解业务需求、设计发展体系、营造组织文化——这些才是人才管理工作中真正创造差异化价值的部分。在这一进程中，技术始终是手段而非目的，其最终价值体现在帮助组织更好地认识、发展和保留其最宝贵的资产：人才。</p>]]></description></item><item>    <title><![CDATA[「实操看我的」征文：聚焦数据库性能优化，分享你的实战方案 墨天轮 ]]></title>    <link>https://segmentfault.com/a/1190000047459035</link>    <guid>https://segmentfault.com/a/1190000047459035</guid>    <pubDate>2025-12-08 18:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>9月墨天轮社区举办的<a href="https://link.segmentfault.com/?enc=2dfbv0DMtHZM8pTD6wMrgw%3D%3D.VDY7jSd%2FhDoQBi59YBVnLfv%2BjQieXxNZejRDaFF0L83Vtniriyvv6nm%2BLY3wK6wRzOMjYtcegUxS71cSj%2FTmyg%3D%3D" rel="nofollow" target="_blank">「实操看我的」数据库征文活动</a>，收到了很多DBA分享的故障处理、性能优化、安装部署等数据库实操干货，文章也得到了很多读者朋友的收藏。为了让创作者的干货获得更聚焦的认可，我们决定升级栏目形式，举办「实操看我的」多期不同主题的系列征文活动，每期将聚焦一个DBA高频刚需的技术实操方向，集中征集该主题的实战方案、避坑技巧。当然，您的投稿文章亦可同步参与社区常规月度征文活动<a href="https://link.segmentfault.com/?enc=U1q1Fzt4tHKB9x40L0QyZw%3D%3D.jiQy1vEJweUvSncx2C%2BBjqLxwtzgvesF5WSFmzF%2F9HYUO8TRcV82Xl1KdlYU%2BG6O" rel="nofollow" target="_blank">“墨力原创作者计划”</a>。</p><p>首期主题为——<strong>“数据库性能优化”</strong>，不论是慢查询优化、索引设计、参数调优，还是架构层面的性能突破，只要你有真实场景、完整步骤、可复现的调优经验，都欢迎你来分享！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459037" alt="" title=""/></p><h2>活动时间</h2><p>11月26日-2026年1月25日 （活动期间每人投稿篇数不限）</p><h2>投稿主题</h2><p>需聚焦您在<strong>数据库日常运维实操中遇到的性能优化场景</strong>，可以是真实生产环境的中的案例复盘、也可以是对某个优化语句的实验验证，总之需要时真实可落地的数据库运维实操。</p><p>以下列举了部分可投稿主题，包含但不限于：</p><ul><li><strong>慢查询优化</strong>：SQL 改写技巧、执行计划分析、索引设计、统计信息维护等</li><li><strong>资源参数调优</strong>：内存、CPU、IO、连接数等核心参数调优等</li><li><strong>架构层面优化</strong>：读写分离、分库分表、缓存穿透 / 击穿解决方案、数据库分片策略等</li><li><strong>特殊场景调优</strong>：高并发秒杀场景、大数据量查询优化、OLAP/OLTP 混合场景调优、国产化系统（麒麟 / 统信）适配调优等</li><li><strong>调优工具实战</strong>：AWR/ASH/Performance Schema/Explain Plan/Percona Toolkit 等工具的使用案例</li></ul><p>数据库类型不限，Oracle、MySQL、PG及国产数据库等均可。</p><h2>参与规则</h2><p>原创文章首发于墨天轮，并带上 “数据库实操” “性能优化” “墨力计划” 三个标签，即算成功参与活动。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459038" alt="" title="" loading="lazy"/></p><p>如您不放心，亦可将投稿文章标题及链接评论于本文评论区、或私信发至墨天轮小助手-小墨（VX：modb666）。</p><blockquote>当您加上“墨力计划”标签则默认同步参与“墨力原创作者计划”，可参与当月墨力计划评奖，点击查看<a href="https://link.segmentfault.com/?enc=i%2FIctdSDX8bYGyG%2F7I5cGg%3D%3D.8HA3JmEI3NE4f2R5tuN76c2p5zeij%2F1A3ZXqsJzXXiNQ71nZNANhLwBWd%2FJ20h5f" rel="nofollow" target="_blank">墨力计划奖励情况</a>。</blockquote><h2>合格及评优规则</h2><ul><li><h3>合格要求</h3></li></ul><ol><li><strong>需包含关键技术要素</strong>：活动侧重实操，需结合真实运维场景，包含调优完整步骤，如“问题现象→原因分析→调优方案→实施步骤→效果验证”；  <br/>（ps：如为运维理念讨论、技术原理分析等非实操类主题无法参与本次特别活动，而属墨力计划常规投稿）</li><li><strong>基础要求</strong>：文章需原创、首发，文章字数不少于 500 字（其中代码占比不可超80%）、阅读量需达100；</li></ol><blockquote>不可为搬运文、流水账、翻译文、广告文或AI代写、刷阅读量，其他要求均同墨力计划，点击查看<a href="https://link.segmentfault.com/?enc=AVB5RDvyD0uhrMHuMkuInw%3D%3D.5EoaNG%2FF0dW3E6YQqp4r3pel5szWAqWdw9u1UPr9nd30jdCJPkC14EQvnmFlJGvh" rel="nofollow" target="_blank">墨力计划参与规则及合格要求</a></blockquote><ol start="3"><li><strong>其他</strong>：建议搭配关键截图（执行计划、监控图表等）、核心代码、性能对比数据等，提升内容可信度与可读性。</li></ol><ul><li><h3>评优规则</h3></li></ul><p>1、<strong>调优干货奖</strong></p><p>将根据问题复杂性、步骤完整度、实操主题借鉴意义等质量维度，以及文章受欢迎维度进行对所有合格文章综合评优，评选出若干篇<strong>调优干货奖</strong>。</p><p>ps：人可投稿多篇，最多可重复获得2篇调优干货奖。</p><p>2、<strong>调优先锋奖</strong></p><p>将对投稿作者合格文章进行评优，综合文章质量、受欢迎程度以及发文数量、优质内容占比等维度评选出 3 名<strong>调优先锋奖</strong>。</p><p>ps：该奖项获得者不可重复获得最佳实操奖。</p><h2>奖项设置</h2><table><thead><tr><th>奖项</th><th>奖项数量</th><th>奖品名称</th></tr></thead><tbody><tr><td><strong>合格奖</strong></td><td>若干篇</td><td>在墨力计划合格奖基础上，可额外获得50墨值+墨天轮优化限定勋章（虚拟）</td></tr><tr><td><strong>调优先锋奖</strong></td><td>3名</td><td>调优先锋限定勋章（虚拟）+实物奖品（依次获得以下单项奖品）：第1名-罗技MK540无线键鼠套装；第2名-倍思10合一拓展坞4K60Hz ；第3名-小米充电宝10000mAh（3C认证）</td></tr><tr><td><strong>调优干货奖</strong></td><td>若干篇</td><td>调优干货限定勋章（虚拟）+实物奖品（以下奖品可二选一）：1、墨天轮定制法兰绒毛毯； 2、墨天轮定制墨天轮logo抱枕</td></tr></tbody></table><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459039" alt="" title="" loading="lazy"/></p><p>此外所有带有“墨力计划”标签的合格文章均可参与月度墨力计划的奖项评选，主要包括优秀文章奖、墨力之星、新人奖、勤更奖等，奖励包含现金、实物奖品等。</p><ul><li>点击查看<a href="https://link.segmentfault.com/?enc=G4ELcGnACA9gBNVjkwudcw%3D%3D.O9h3rI%2FmDQdHaFpX7iYRrUh%2BOKUb4TNyThiDrG9%2BPlHypnGU7mhDhDC8NVtqeTpjM9jIBa2Ns43rp8VJfYkcUQ%3D%3D" rel="nofollow" target="_blank">本次征文活动原帖</a></li><li>点击查看<a href="https://link.segmentfault.com/?enc=vOnK91ggHk7XkiTzNNifnA%3D%3D.YvJLfmI2EcTOXnZX3yDtwuqB9HArxm8z9bptkHgqHdFEOQZ5bsNvnl3Jx7D%2Fiy96" rel="nofollow" target="_blank">墨力计划奖励情况</a>。</li></ul><hr/><p>欲了解更多可浏览<a href="https://link.segmentfault.com/?enc=SY7XfaU79nA7y%2FP%2FZCIteg%3D%3D.EOokmR3Pzn6pJtIFf%2F7X0keqQ3nv9gBDImpXLAuYrrA%3D" rel="nofollow" target="_blank">墨天轮社区</a>，围绕数据人的学习成长提供一站式的全面服务，打造集新闻资讯、在线问答、活动直播、在线课程、文档阅览、资源下载、知识分享及在线运维为一体的统一平台，持续促进数据领域的知识传播和技术创新。</p><p>关注官方公众号： 墨天轮、 墨天轮平台、墨天轮成长营、数据库国产化 、数据库资讯</p>]]></description></item><item>    <title><![CDATA[JuiceFS + MinIO：Ariste AI 量化投资高性能存储实践 JuiceFS ]]></title>    <link>https://segmentfault.com/a/1190000047459047</link>    <guid>https://segmentfault.com/a/1190000047459047</guid>    <pubDate>2025-12-08 18:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Ariste AI 是一家专注于 AI 驱动交易的公司，业务涵盖自营交易、资产管理、高频做市等多个领域。在量化交易研究中，数据的读取速度和存储效率，往往直接决定了研究迭代的速度。</p><p>Ariste AI 团队在构建量化研究基础设施的过程中，面对总规模超过 500TB，行情与因子数据，经历了从本地盘到最终选择在 MinIO 对象存储之上叠加 JuiceFS 文件系统的四个阶段。通过缓存机制与分层架构，团队实现了高频数据的快速访问与集中管理。<strong>这一实践验证了“缓存加速、弹性对象存储与 POSIX 兼容”三位一体方案在量化场景下的可行性</strong>，希望这一经验能为同行提供一些参考。</p><h2>01 量化投资存储挑战：规模、速度与协作的平衡</h2><p>量化投资流程依次包括数据层、因子与信号层、策略与仓位层及执行与交易层，构成从数据获取到交易执行的完整闭环。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459049" alt="" title=""/></p><p>在整个过程中，存储系统面临多重挑战，主要体现在以下几个方面：</p><ul><li><strong>数据规模与增速</strong>：量化研究所需处理的数据总量较大，涵盖历史行情数据、新闻数据以及自行计算的因子数据等。目前，这些数据的总量已接近 500T。并且，企业每日新增的行情数据也达数百 GB。若采用传统磁盘进行存储，显然无法满足如此巨大的数据存储需求。</li><li><strong>高频访问与低延迟要求</strong>：高频的数据访问依赖于低延迟的数据读取。数据读取的速率直接决定了研究效率的高低。若数据读取速度较快，研究进程便能迅速推进；反之，则会导致研究效率低下。</li><li><strong>多团队并行与数据治理</strong>：在量化研究过程中，通常会有多个团队同时开展不同的实验。为确保各团队研究工作的独立性与数据安全性，需要进行安全的隔离，以避免数据混淆与泄露。</li></ul><p>为应对上述量化全流程对数据存储的需求，打造面向未来的存储系统，<strong>我们的目标是实现：高性能、易扩展与可治理，三者有机统一</strong>：</p><ul><li>高性能：单节点读写带宽突破 500MB/s，访问延迟低于本地磁盘感知阈值；</li><li>易扩展：支持存储与计算资源按需水平扩容，业务无需改造即可实现平滑弹性伸缩；</li><li>可治理：提供细粒度权限控制、操作审计与数据生命周期策略的一站式管理能力。</li></ul><h2>02 存储架构的演进</h2><h3>阶段一：本地盘极速起步</h3><p>在项目初期，我们采用了 Quantrabyte 研究框架，该框架内置了 ETF 模块，可直接将数据存储在本地磁盘上，数据读取速度较快。研究员可根据自身需求，直接运行所需数据，迭代过程较为迅速。然而，这一阶段也存在一些问题：</p><ul><li>重复下载造成资源浪费：多个研究员若使用相同数据，会进行多次下载。</li><li>存储容量不足：研究服务器的存储容量有限，仅约 15T，难以满足日益增长的数据存储需求。</li><li>协作困难：当需要复用他人的研究结果时，操作过程不够便捷。</li></ul><h3>阶段二：MinIO 集中管理的双刃剑</h3><p>为解决第一阶段存在的问题，我们引入了 MinIO 进行集中管理。将所有存储数据集中在 MinIO 上，通过拆分出的模块将数据全部存入。同时，将具体因子数据也存入 MinIO，实现公共数据的统一下载。并通过权限隔离，实现多团队数据共享，提升存储空间利用率。</p><p>然而，这一阶段也出现了新的瓶颈：</p><ul><li>高频随机读延迟大：在进行高频数据 I/O 操作时延迟较大，影响数据读取速度。</li><li>无缓存导致读写慢：由于 MinIO 社区版无缓存功能，读写高频公共数据时速度较慢。</li></ul><h3>阶段三：JuiceFS 引入缓存加速</h3><p>为解决上述瓶颈，经充分调研，我们最终引入 JuiceFS 的缓存加速方案。该方案通过客户端本地 RAID5 存储进行挂载，借助高效的缓存机制，<strong>成功将读写性能提升约三倍，显著改善了高频共享数据的访问体验</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459050" alt="" title="" loading="lazy"/></p><p>随着业务数据量突破 300TB，本地存储的扩容瓶颈逐渐显现。由于数据存储在本地，扩容需重新配置存储设备，而 RAID5 架构下扩容速度缓慢且风险较高，难以满足业务持续增长的需求。</p><h3>阶段四：JuiceFS + MinIO 集群终局架构</h3><p>为解决扩容难题，我们最终采用了JuiceFS+MinIO 集群架构。该方案具备以下优势：</p><ul><li>持续高性能：JuiceFS 提供充足的缓存能力，充分满足高频数据访问场景的性能需求；</li><li>便捷集群扩展：基于集群化方案，可快速实现横向扩容，仅需添加同类型磁盘即可灵活提升存储容量，大幅增强系统扩展性。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459051" alt="" title="" loading="lazy"/></p><p><strong>通过四阶段演进，我们验证了缓存加速、弹性对象存储与 POSIX 兼容三位一体方案在量化场景的可行性</strong>。此方案可为同行业提供可复制、可落地的最佳实践范本，在性能、成本与治理之间取得了卓越平衡。</p><h2>03 性能与成本收益</h2><p>通过采用 JuiceFS 与 MinIO相 结合的存储架构，系统带宽与资源利用效率得到质的飞跃，目前已完全满足研究业务对存储性能的需求。引入 JuiceFS 缓存层后，<strong>回测任务执行效率大幅提高，1 亿条 Tick 数据回测耗时由之前的数小时降至数十分钟</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459052" alt="" title="" loading="lazy"/></p><p>同时，基于我们完整的数据生命周期分层存储体系策略，实现存储单价由高到低的平滑过渡，整体存储成本下降40% 以上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459053" alt="" title="" loading="lazy"/></p><h2>04 运维实践与展望</h2><h3>多租户治理</h3><p>在数据隔离与权限管理方面，我们建立了完善的管理体系：</p><p>通过命名空间实现逻辑隔离，采用类似 <code>/factor/A</code>、<code>/factor/B</code> 的路径规划，确保各业务数据边界清晰。在权限控制层面，支持用户、团队、项目三个维度的精细化管理，并与 POSIX ACL 权限体系无缝对接。同时建立完整的审计日志系统，实现访问行为的实时追踪与变更历史回溯，全面满足合规性要求。</p><h3>可观测性与自动化运维</h3><p><strong>我们围绕四大核心指标构建了完整的监控体系：缓存命中率、I/O 吞吐量、I/O 延迟与写入重试率，系统在指标异常时可自动触发告警</strong>。</p><p>基于 Grafana 实现了运维闭环管理，持续监控节点健康状态与存储容量。在每次扩容前，会通过模拟压测验证系统承载能力，确保业务无感知。整体运维体系实现了自动化、可预测、可回滚的高标准运维目标。</p><h3>回测系统中的数据更新设计</h3><p>我们在回测系统设计中采用基于 DAG（Directed Acyclic Graph，有向无环图）的架构，以提升系统的计算效率与可维护性。<strong>该框架以计算节点和依赖关系为核心，将数据处理、特征计算、信号生成等环节抽象为节点，并通过依赖图统一管理</strong>。系统内置版本控制机制，当数据版本更新时，可依托依赖图自动识别受影响的节点，精确定位需重算部分，从而实现高效的增量更新与结果追溯。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459054" alt="" title="" loading="lazy"/></p><h2>未来展望</h2><p>在未来规划中，我们将从以下三个方向持续优化存储架构：</p><ol><li>元数据高可用升级：计划将元数据存储从 Redis 迁移至 TiKV 或 PostgreSQL，以构建跨机房高可用架构，显著提升系统容灾与快速恢复能力。</li><li>混合云分层存储：通过对接公有云 S3 与 Glacier 存储服务，构建智能冷热分层体系，在实现存储容量无限弹性的同时，达成成本最优化目标。</li><li>研究数据湖统一治理：计划构建统一的研究数据湖平台，集成 Schema 注册、自动数据清洗与统一目录治理等核心服务，全面提升数据资产的发现与管理效率。</li></ol><p>我们希望本文中的一些实践经验，能为正在面临类似问题的开发者提供参考，如果有其他疑问欢迎加入 <a href="https://link.segmentfault.com/?enc=V1THZ8Q7DSqVMU3MmGCnVQ%3D%3D.r2GOT6plWniCBhArsFZnmueBEmQ1Z6qwiijxUVp9%2F%2B4%3D" rel="nofollow" target="_blank">JuiceFS 社区</a>与大家共同交流。</p>]]></description></item><item>    <title><![CDATA[观测云告警对接华为 WeLink 最佳实践 观测云 ]]></title>    <link>https://segmentfault.com/a/1190000047459079</link>    <guid>https://segmentfault.com/a/1190000047459079</guid>    <pubDate>2025-12-08 18:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>华为云 WeLink 是华为推出的全场景数字化协同办公平台，旨在帮助企业实现高效、安全的在线协作。观测云则是专为 IT 工程师打造的全链路可观测性平台，具备完善的异常监测体系，能够对基础设施、应用程序及日志等各类数据进行实时监控，并在发现异常时自动触发告警。本文主要介绍如何将观测云的告警信息推送至 WeLink 的实现方式。</p><h2>前置条件</h2><ul><li><a href="https://link.segmentfault.com/?enc=Pf41EsCMS5tQesH3Y9p9sQ%3D%3D.88J52THuzSzxU%2FtW6jqP3DuLXnwPJ9H9s1j5%2BC85DMP2R5MjNOlM2VRWAZ4DipKL" rel="nofollow" target="_blank">观测云 SaaS</a></li><li><a href="https://link.segmentfault.com/?enc=tZPsuwD%2BCPVvMn4yjOkFoQ%3D%3D.HoAOo6Uk9AFa0hnZYUkxkqo%2FK5JQ78gUbr741ryfA9BwOCMsLm4ORUC5I7mThB2%2FBqf%2BbhSqlfdhb%2FZ290Ko6Q%3D%3D" rel="nofollow" target="_blank">安装 Func</a></li></ul><h2>配置 Webhook 机器人</h2><p>点击群右上角的齿轮图表，弹出讨论组，在“讨论组管理”下面点击“群助手”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459081" alt="图片" title="图片"/></p><p>点击“添加群助手”，再点击“创建”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459082" alt="图片" title="图片" loading="lazy"/></p><p>点击“去创建”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459083" alt="图片" title="图片" loading="lazy"/></p><p>输入名称后，点击“添加”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459084" alt="图片" title="图片" loading="lazy"/></p><p>保存 Webhook 后，点击“保存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459085" alt="图片" title="图片" loading="lazy"/></p><h2>配置 Func</h2><h3>编写脚本</h3><p>登录 Func，点击“开发”-&gt;“脚本库”-&gt;“新建脚本集”，输入 “webhook”，鼠标移到 webhook 上，点击“新建脚本”，输入“welink_prod”，编辑脚本，输入如下内容，最后点击“发布”。</p><pre><code>import requests
import json
import time
import uuid

@DFF.API('自定义发送 weLink')
def send_welink(**kwargs):
    token = _DFF_HTTP_REQUEST.get('query').get('token')
    title = kwargs.get('df_title','事件告警')
    messageJson = [kwargs.get('df_message','告警')]
    status = kwargs.get('df_status','info')
    msgStr = messageJson[0]
    url = kwargs.get('df_event_link')
    jumpHtml = url
    webhook_url = "https://open.welink.huaweicloud.com/api/werobot/v1/webhook/send"
    params = {
        "token": token,
        "channel": "standard"
    }
    # 请求头（Headers）
    headers = {
        "Content-Type": "application/json",  # 必须指定 JSON 格式
        "Accept": "application/json"
    }

    # 请求体（Body），发送的消息内容
    payload = {
        "messageType": "text",
        "content": {
            "text": msgStr + "\n\n" + jumpHtml  # 消息内容
        },
        "timeStamp": int(time.time() * 1000),   # 毫秒时间戳
        "uuid": uuid.uuid4().hex                # 32 位十六进制字符串
    }    

    # 发送 POST 请求
    response = requests.post(
        webhook_url,
        params=params,  # URL 参数（token 和 channel）
        headers=headers,
        data=json.dumps(payload)  # 将字典转为 JSON 字符串
    )
    # 打印响应
    print("Status Code:", response.status_code)
    print("Response:", response.json())</code></pre><p>配置截图</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459086" alt="图片" title="图片" loading="lazy"/></p><h3>配置函数</h3><p>依次进入 Func 界面的“管理”-&gt;“函数 API”，点击“新建”，运行函数选择上步编写的脚本，点击“保存”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459087" alt="图片" title="图片" loading="lazy"/></p><p>在函数 API 界面，刚创建的函数行后面有个“示例”，点击后，复制“POST简化形式(JSON)”里面的 url。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459088" alt="图片" title="图片" loading="lazy"/></p><h2>创建监控器</h2><h3>新建通知对象</h3><p>登录观测云，进入“监控”-&gt;“通知对象管理”-&gt;“新建通知对象”，选择“webhook”，Webhook地址中粘贴上步复制的 url，最后点击“确认”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459089" alt="图片" title="图片" loading="lazy"/></p><h3>新建告警策略管理</h3><p>进入“监控”-&gt;“告警策略管理”-&gt;“新建告警策略”，通知配置中按下图配置，最后点击“保存”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459090" alt="图片" title="图片" loading="lazy"/></p><p>进入“监控”-&gt;“监控器”-&gt;“从模版新建”，选择一个监控器，在告警配置中选择上步创建的告警策略。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459091" alt="图片" title="图片" loading="lazy"/></p><h2>效果展示</h2><p>当监控器达到触发条件后，WeLink 收到告警。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459092" alt="图片" title="图片" loading="lazy"/></p><h2>总结</h2><p>观测云借助 Func 功能，能够将告警信息实时推送至 WeLink，帮助用户第一时间掌握系统运行状态。该功能有效避免了因系统故障未能及时发现而可能引发的更大损失，进一步提升了运维效率与系统可靠性。</p>]]></description></item><item>    <title><![CDATA[2025CRM厂商全流程数字化能力对比 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047459108</link>    <guid>https://segmentfault.com/a/1190000047459108</guid>    <pubDate>2025-12-08 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在企业数字化转型中，<strong>潜客精准营销、销售订单智能拆分、生产排程优化、库存调拨管理、应收应付对账</strong>是贯穿“获客-转化-生产-交付-回款”全链路的核心场景。不同品牌的解决方案因定位（中小/大型、制造/商贸）、技术侧重（AI/流程/集成）差异显著。本文基于9大CRM品牌官方功能与实际场景，从<strong>痛点解决能力、核心功能差异、适用场景</strong>三个维度展开深度对比。</p><h2>一、整体能力框架对比（雷达图分值）</h2><p>先通过雷达图直观呈现各品牌在五大场景的综合能力（<strong>10分为满分</strong>，分值基于功能覆盖度、场景匹配度、自动化水平）：</p><table><thead><tr><th>品牌</th><th>潜客精准营销</th><th>销售订单智能拆分</th><th>生产排程优化</th><th>库存调拨管理</th><th>应收应付对账</th><th>核心定位</th></tr></thead><tbody><tr><td>超兔一体云</td><td>8</td><td>9</td><td>9</td><td>8</td><td>9</td><td>中小工贸企业全流程闭环</td></tr><tr><td>销售易</td><td>9</td><td>8</td><td>8</td><td>7</td><td>8</td><td>营销驱动的产销协同</td></tr><tr><td>SAP</td><td>5</td><td>7</td><td>8</td><td>8</td><td>9</td><td>大型制造合规化管理</td></tr><tr><td>管家婆</td><td>7</td><td>6</td><td>7</td><td>7</td><td>8</td><td>中小商贸业财一体化</td></tr><tr><td>金蝶</td><td>6</td><td>5</td><td>7</td><td>7</td><td>8</td><td>轻量化业财协同</td></tr><tr><td>Zoho</td><td>7</td><td>2</td><td>3</td><td>6</td><td>7</td><td>AI驱动的跨境/中小场景</td></tr><tr><td>Microsoft CRM</td><td>6</td><td>2</td><td>2</td><td>2</td><td>2</td><td>Office生态的客户管理</td></tr><tr><td>Oracle CX</td><td>2</td><td>2</td><td>2</td><td>2</td><td>7</td><td>销售与财务数据验证</td></tr><tr><td>Pipedrive</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>简单CRM，非全流程覆盖</td></tr></tbody></table><h2>二、五大核心场景深度对比</h2><h3><strong>场景1：潜客精准营销——解决“获客难、线索杂、转化低”</strong></h3><p><strong>企业痛点</strong>：获客渠道分散、线索质量参差不齐、营销投入 ROI 难衡量。 <strong>核心评估维度</strong>：获客覆盖、线索处理效率、营销自动化、智能推荐能力。</p><h4>各品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>获客渠道覆盖</th><th>线索处理能力</th><th>营销自动化/智能推荐</th><th>优势场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>线上（百度/抖音/官网/微信/小程序）+线下（地推/工商搜客）</td><td>一键转客户/待办/订单、归属地识别、自动提醒</td><td>营销物料库（话术/文件）、竞品管理</td><td>全渠道获客的中小制造企业</td></tr><tr><td>销售易</td><td>智能名片/企微活码/微信客服+工商数据</td><td>智能线索打分（互动行为）、批量转化</td><td>营销自动化、智能客户推荐（种子归因）</td><td>高价值线索挖掘的营销型企业</td></tr><tr><td>管家婆</td><td>CRM客户数据库整合</td><td>客户指标分析（活跃度/销售表现）</td><td>生日/偏好精准触达（短信/优惠券）</td><td>客户数据集中的商贸企业</td></tr><tr><td>Zoho</td><td>网站（SalesIQ）+邮件/通话</td><td>Zia AI分析（跟进时机）、访客追踪</td><td>智能机器人互动</td><td>注重AI跟进的跨境企业</td></tr><tr><td>SAP</td><td>流程合规型获客</td><td>预测分析/智能报价</td><td>无突出自动化</td><td>大型制造的合规性营销</td></tr></tbody></table><h4>超兔潜客精准营销流程图（Mermaid）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459110" alt="" title=""/></p><pre><code>flowchart LR
    A[集客渠道] --&gt;|线上：百度/抖音/官网/微信/小程序| B[线索获取]
    A --&gt;|线下：地推（专属二维码）/工商搜客| B
    B --&gt; C[线索处理]
    C --&gt;|一键加客户/老客户待办/订单| D[客户库]
    C --&gt;|手机号/IP归属地| E[线索分配]
    E --&gt; F[自动消息提醒销售]
    C --&gt; G[市场活动成本均摊]
    G --&gt; H[营销效果评估]
    D --&gt; I[营销物料库（话术/文件）]
    D --&gt; J[竞品管理]
    I &amp; J --&gt; K[潜客培育]
    K --&gt; L[潜客转化]</code></pre><h3><strong>场景2：销售订单智能拆分——解决“订单复杂、拆分低效、供应链脱节”</strong></h3><p><strong>企业痛点</strong>：多业态订单（B2B/B2C/O2O）处理难、拆分规则不灵活、采购/生产/仓储联动慢。 <strong>核心评估维度</strong>：订单模型覆盖、拆分规则智能性、供应链协同能力。</p><h4>各品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>订单模型覆盖</th><th>拆分规则</th><th>供应链协同</th><th>优势场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>6大类30种（装配/租赁/定制等）</td><td>库存/供应商/产能智能匹配</td><td>拆分后同步采购/生产/仓储</td><td>多业态的中小制造企业</td></tr><tr><td>销售易</td><td>直销/分销/私域</td><td>产销协同（订单→生产→交付）</td><td>合同全生命周期管控</td><td>注重风险的产销型企业</td></tr><tr><td>管家婆</td><td>通用订单</td><td>后台配置（物流/库存规则）</td><td>批量拆单/合并</td><td>商贸企业的批量处理</td></tr><tr><td>SAP</td><td>按订单生产（MTO）</td><td>自定义审批/定价策略</td><td>SD模块联动生产/采购</td><td>大型定制化制造企业</td></tr></tbody></table><h4>超兔订单智能拆分流程图（Mermaid）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459111" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
    A[销售订单输入] --&gt; B{匹配订单模型}
    B --&gt;|装配/租赁/定制等| C[智能拆分规则引擎]
    C --&gt;|库存/供应商/产能| D[拆分子订单]
    D --&gt; E[同步采购部门]
    D --&gt; F[同步生产部门]
    D --&gt; G[同步仓储部门]
    E --&gt; H[采购执行]
    F --&gt; I[生产排程]
    G --&gt; J[库存准备]
    H &amp; I &amp; J --&gt; K[供应链协同履约]</code></pre><h3><strong>场景3：生产排程优化——解决“排程不合理、进度失控、产销脱节”</strong></h3><p><strong>企业痛点</strong>：排程方式单一、工序/班组分配乱、销售订单与生产不同步。 <strong>核心评估维度</strong>：排程方式灵活性、进度管控、CRM/ERP联动能力。</p><h4>各品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>排程方式</th><th>进度管控</th><th>数据联动</th><th>优势场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>正排（从首道工序推进）+倒排（从末道反向）</td><td>甘特视图+车间大屏实时监控</td><td>CRM订单→生产BOM→库存回传</td><td>中小制造的灵活排程</td></tr><tr><td>销售易</td><td>数据驱动排产（订单需求）</td><td>动态调整（插单/库存变化）</td><td>MES+ERP同步物料/财务</td><td>注重应变的产销企业</td></tr><tr><td>管家婆</td><td>产能平衡排程</td><td>全流程覆盖（计划→物料→派工）</td><td>无突出联动</td><td>商贸企业的简单生产排程</td></tr><tr><td>SAP</td><td>SD模块联动排程</td><td>生产排程优化系统（整体最优）</td><td>ERP深度联动（销售→生产→采购）</td><td>大型制造的全局排程</td></tr></tbody></table><h4>超兔生产排程优化脑图（Mermaid）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047459112" alt="" title="" loading="lazy"/></p><pre><code>mindmap
  生产排程优化
    排程方式
      正排：按交付时间从早到晚，首道工序推进
      倒排：按交付时间从晚到早，末道工序推导
    排程策略
      最快时间：紧急订单，优先保障交付
      最小班组：产能闲置，控制人力成本
    任务管理
      自动生成生产任务表（工序数量/计划时间/负责班组）
      甘特视图+车间大屏：实时监控进度
    数据联动
      CRM销售订单→生产BOM/订单
      MES数据→CRM（领料/退料/报工/质检）</code></pre><h3><strong>场景4：库存调拨管理——解决“库存积压/短缺、调拨低效、追溯难”</strong></h3><p><strong>企业痛点</strong>：多仓库库存不透明、调拨流程繁琐、库存变化无法追溯。 <strong>核心评估维度</strong>：多仓库支持、调拨流程自动化、数据追溯能力。</p><h4>各品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>多仓库支持</th><th>调拨流程</th><th>数据追溯</th><th>优势场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>最多500个仓库+权限控制</td><td>自动化流程（出入库/盘点/调拨）</td><td>流水/批次/序列号溯源</td><td>多仓库的中小制造企业</td></tr><tr><td>管家婆</td><td>多仓库实时监控</td><td>一键调拨</td><td>无突出溯源</td><td>商贸企业的跨仓库调拨</td></tr><tr><td>销售易</td><td>全链路协同（CRM→供应链）</td><td>库存数据同步</td><td>多维度分析（库存分布/周转）</td><td>产销协同的库存优化</td></tr><tr><td>SAP</td><td>全球供应链可视化</td><td>调拨预警</td><td>无突出溯源</td><td>跨国企业的库存管控</td></tr></tbody></table><h3><strong>场景5：应收应付对账——解决“对账慢、数据错、风险大”</strong></h3><p><strong>企业痛点</strong>：应收/应付数据与业务脱节、对账依赖手动、坏账风险高。 <strong>核心评估维度</strong>：业财联动、自动化对账、风险控制。</p><h4>各品牌能力对比表</h4><table><thead><tr><th>品牌</th><th>业财联动</th><th>自动化对账</th><th>风险控制</th><th>优势场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>订单→应收/应付自动同步</td><td>三角联动（应收→开票→回款）</td><td>账期/信用度管理、超发预警</td><td>中小制造的全流程对账</td></tr><tr><td>管家婆</td><td>业务→财务自动同步</td><td>自动化对账+电商平台对接</td><td>无突出风险控制</td><td>商贸企业的简单对账</td></tr><tr><td>销售易</td><td>CRM→财务→合同联动</td><td>返利返点自动计算、在线对账</td><td>回款进度跟踪</td><td>营销型企业的财务闭环</td></tr><tr><td>SAP</td><td>订单→应收→财务闭环</td><td>多币种合规、审计支持</td><td>无突出风险控制</td><td>大型制造的合规对账</td></tr><tr><td>Oracle CX</td><td>销售→应收数据验证</td><td>Address Alignment智能体</td><td>数据准确性保障</td><td>注重数据验证的企业</td></tr></tbody></table><h2>三、综合结论与选型建议</h2><ol><li><strong>中小制造企业</strong>：优先选<strong>超兔一体云</strong>（覆盖全流程，从潜客到财务闭环，性价比高）。</li><li><strong>营销型企业</strong>：选<strong>销售易</strong>（智能线索挖掘+产销协同，适合高价值客户转化）。</li><li><strong>大型制造企业</strong>：选<strong>SAP</strong>（全局排程+合规对账，适合复杂生产场景）。</li><li><strong>中小商贸企业</strong>：选<strong>管家婆</strong>（客户数据集中+简单对账，适合商品流通）。</li><li><strong>跨境企业</strong>：选<strong>Zoho</strong>（AI跟进+多币种对账，适合海外业务）。</li></ol><p>通过以上对比可见，企业需结合<strong>规模、行业、核心痛点</strong>选择解决方案——没有“最好”的品牌，只有“最匹配”的能力。</p><p>在当今竞争激烈且复杂多变的商业环境中，企业数字化转型已成为提升竞争力和实现可持续发展的必由之路。选择适合自身的数字化解决方案，就如同为企业配备了精准的导航系统，能够助力企业在“获客 - 转化 - 生产 - 交付 - 回款”的全链路中稳健前行。希望各企业能够依据上述对比分析和选型建议，审慎考量，做出最契合自身发展需求的决策，从而开启高效、智能、创新的数字化运营新篇章。</p>]]></description></item><item>    <title><![CDATA[LazyLLM教程 | 第18讲：高阶RAG：Agentic RAG 商汤万象开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047458159</link>    <guid>https://segmentfault.com/a/1190000047458159</guid>    <pubDate>2025-12-08 17:10:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458162" alt="" title=""/></p><blockquote><p>前面教程中，我们学习到了如何构建 RAG 系统，以及对 RAG 系统进行效果提升、速度优化、功能扩展等等方面。</p><p>本教程我们将在此基础上进一步介绍最近很火的<strong>Agentic RAG</strong>，它是RAG的变种，但更加智能，让我们开始吧！</p></blockquote><p>如果把 <strong>RAG </strong>比作带着<strong>书本</strong>去考试的考生，那么<strong>Agentic RAG</strong>就是同时带着<strong>老师和书</strong>一起去考试的考生！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458163" alt="" title="" loading="lazy"/></p><p>Agentic RAG 就是<strong>整合了 AI Agent 的 RAG</strong>。本文将先从 RAG、AI Agent 等概念为基础引出 Agentic RAG；然后详细介绍 Agentic RAG 的基本原理和组成；紧接着介绍为什么要用 Agentic RAG，并与传统的 RAG 进行对比；最后介绍如何搭建一个 Agentic RAG。</p><hr/><h2><strong>一、基本概念</strong></h2><p>什么是 Agentic RAG ? 让我们将这个复杂的概念先拆解为 <strong>RAG</strong> 和 <strong>AI Agent</strong>（Agentic 就是引入了 AI Agent）来逐个进行介绍。</p><h3><strong>（一）回顾 RAG 系统</strong></h3><p>首先让我们先回顾一下 RAG 的基本概念。</p><h4><strong>1. 基本概念</strong></h4><p><strong>检索增强生成（Retrieval-Augmented Generation，简称RAG）</strong>技术是一种利用<strong>外挂知识源</strong>为大语言模型补充上下文来强化输入从而提高大语言模型生成内容质量，并减少幻觉（hallucinations，幻觉即 LLM 自信地编造信息随意发挥生成的不真实的内容）的技术。</p><p>打个比方来说，RAG就是一个带着书本去考试的考生。考题就是输入，书本就是外挂的知识库，考生就是大模型，考生作答的内容就是大模型生成的内容。一般来说如果一门闭卷考能够带着教科书去考试，那答卷的分数都会很高，这也正是RAG能提高大模型生成内容质量的一个形象解释。</p><h4><strong>2. 基本组件</strong></h4><p>RAG 主要包括了两个组件：</p><ul><li><strong>检索组件（Retrieval Component）</strong>：检索组件用于根据输入去匹配知识库中的信息，打个比方就是带着考题去教科书中搜索答案。</li><li><strong>生成组件（Generative Component）</strong>：生成组件用于把输入和检索到的信息送给大模型来生成高质量的回复，打个比方就是：考生结合题目和从教科书中找到的内容来回答试题。</li></ul><h4><strong>3. 工作流程</strong></h4><p>RAG （Retrieval-Augmented Generation）这个名字已经将这个技术的工作流程给揭示了出来，让我们结合图示并将名字进行拆解来看：</p><p>（1）首先我们输入一个 query：</p><ul><li><strong>Retrieval：检索</strong>，query 首先被用于在一个知识库中进行检索（这里简化了 RAG 中embedding、向量化等细节，详细可见往期教程 [第2讲：10分钟上手一个最小可用RAG系统]，知识库中的文档以及 query 都会被向量化以便进行相似度计算，下文图中 Vector Search 对应的就是对知识库的搜索）；</li><li><strong>Augmented：增强</strong>，将检索到的内容（context）与我们输入的 query 进行拼接，以达到增强 query 的效果；</li><li><strong>Generation：生成</strong>，将上一步增强后的 query 送入到 LLM 大模型来生成回复的内容。</li></ul><p>（2）将生成的内容返回。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458164" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>（二）AI Agent 简介</strong></h3><h4><strong>1. 基本概念</strong></h4><p>首先<strong>Agent</strong>是什么？中文中常见翻译为：<strong>代理人</strong>。</p><p>我们要做一件事，一般有两种方式：一是自己一步一步来达成；另外就是找个人，这个人就叫做代理人，我们全权授权给代理人而不用关心他怎么做，只管他能帮我们达到目的。前者我们需要操心每个细节，而后者我们可以坐享其成。</p><p>所以 Agent 的一个特点就是：不需要我们去关心达成某个任务的细节，而只需要放心把任务交给他，让他去帮我们达成。</p><p>回到<strong>AI 智能体（AI Agent）</strong>，AI Agent 一般被认为是一个具有特定角色和任务的 LLM，它可以访问记忆和外部工具。</p><p>但我觉得 AI 智能体更像是一个人，我们请来的代理人。我更愿意把它比作一个有着高度专业能力的人——专家。LLM 是其大脑，借助他聪明的大脑，他可以自动规划步骤，结合反馈反复采取行动（比如调用工具）来解决手头的任务，整个过程不需要我们操心，我们只需要放权让他去做就好！</p><p>想象你是一位国王，当你想扩张领土的时候，你并不需要自己亲历亲为，你只需要找代理人——你的大将（即：带兵作战的专家），放权让大将去做，他自己会规划作战计划（规划）、调兵遣将（调用工具）、冲锋陷阵（采取行动）。你只需要等待他凯旋的好消息。这个大将就像是我们的 AI 智能体。</p><h4><strong>2. 基本组件</strong></h4><p>一个 AI Agent 主要由下面组件构成：</p><ul><li><strong>LLM</strong>：这个是智能体的大脑，对应大将军的大脑；</li><li><strong>记忆（Memory）</strong>：智能体的记忆，对应了大将军对某个领土扩张任务从开始到结束的所有记忆，甚至是之前的战斗记忆；</li><li><strong>规划（Planning）</strong>：智能体可以进行反思、自我批评、自动路由（采取行动）等，对应了国王放权给大将军，让他能按照自己的想法去达成任务；</li><li><strong>工具（Tools）</strong>：是智能体可以调用的工具，对应大将军可以调用的兵力，可以使用的武器等等；</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458165" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3. 工作流程</strong></h4><p>AI Agent 有很多类型的工作流程，这里介绍几种常见的工作流程：Function Call Agent、ReAct、PlanAndSolve 以及 ReWOO。AI 智能体的工作流程主要就是其行为模式，就像是一个人做事的行为习惯：</p><ul><li><strong>Function Call Agent</strong>：在该智能体接到任务后，它会不断尝试以各种参数调用工具和观察输出，直到解决问题或达到最大重复次数。</li><li><strong>ReAct</strong>：该智能体接到任务后，它会先思考，然后再尝试调用工具和观察输出，不断重复这个过程直到解决问题或达到最大重复次数。</li><li><strong>PlanAndSolve</strong>：该智能体接到任务后，会先计划把任务分解，然后尝试解决当前步骤任务，根据当前步骤的结果来继续执行任务或者重新计划后面的任务，直到任务被解决或达到最大重复次数。</li><li><strong>ReWOO</strong>：该智能体接到任务后，也会先计划把任务分解，然后将所有步骤全部执行完毕，综合所有步骤的结果来进行反馈。</li></ul><h5><strong>（1）Function Call Agent</strong></h5><p>Function Call Agent 主要包括以下的流程：</p><ol><li><strong>行动（Action）</strong>：Agent 收到一个 query 后，它会直接行动，比如去调用某个工具；</li><li><strong>观察（Observation）</strong>: Agent 观察到行动的反馈，比如工具的输出。</li></ol><p>上面过程会不断循环往复，如果观察到行动的反馈没问题，满足了 query 的要求，或者达到了最大的迭代次数，那么 Agent 会退出并返回结果 response。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458166" alt="image.png" title="image.png" loading="lazy"/></p><p>我们可以在LazyLLM中使用AI Agent，首先定义工具，然后把定义好的工具注册进 LazyLLM 中，之后就可以定义模型，并使用 FunctionCall Agent：</p><pre><code>from typing import Literal
import json
import lazyllm
from lazyllm.tools import fc_register, FunctionCall, FunctionCallAgent
@fc_register("tool")
def get_current_weather(location: str, unit: Literal["fahrenheit", "celsius"] = "fahrenheit"):
    ...
@fc_register("tool")
def get_n_day_weather_forecast(location: str, num_days: int, unit: Literal["celsius", "fahrenheit"] = 'fahrenheit'):
    ...
llm = lazyllm.TrainableModule("internlm2-chat-20b").start()  # or llm = lazyllm.OnlineChatModule()
tools = ["get_current_weather", "get_n_day_weather_forecast"]
fc = FunctionCall(llm, tools)
query = "What's the weather like today in celsius in Tokyo and Paris."
ret = fc(query)
print(f"ret: {ret}")
agent = FunctionCallAgent(llm, tools)
ret = agent(query)
print(f"ret: {ret}")
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458167" alt="image.png" title="image.png" loading="lazy"/></p><h5><strong>（2）React</strong></h5><p>React 主要包括以下的流程：</p><ol><li><strong>思考（Thought）</strong>: Agent 在收到 query 后，它会先给出下一步要采取的行动；</li><li><strong>行动（Action）</strong>: Agent 会采取并执行一个行动，比如使用工具（或者继续思考）；</li><li><strong>观察（Observation）</strong>: Agent 观察行动的反馈，比如工具的输出；</li></ol><p>上面过程也是会不断循环往复，直到满足 query 的请求，或者达到了最大的迭代次数。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458168" alt="image.png" title="image.png" loading="lazy"/></p><p>ReactAgent 执行流程和 FunctionCallAgent 的执行流程一样，唯一区别是<strong>prompt 不同</strong>，并且 ReactAgent 每一步都要有 <strong>Thought 输出</strong>，而普通 FunctionCallAgent 可能只有工具调用的信息输出，没有 content 内容。示例如下：</p><pre><code>import lazyllm
from lazyllm.tools import fc_register, ReactAgent
@fc_register("tool")
def multiply_tool(a: int, b: int) -&gt; int:
    return a * b
@fc_register("tool")
def add_tool(a: int, b: int):
    return a + b
tools = ["multiply_tool", "add_tool"]
llm = lazyllm.OnlineChatModule(source="sensenova", model="DeepSeek-V3")
agent = ReactAgent(llm, tools)
query = "What is 20+(2*4)? Calculate step by step."
res = agent(query)
print(res)
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458169" alt="image.png" title="image.png" loading="lazy"/></p><h5><strong>（3）PlanAndSolve</strong></h5><p>PlanAndSolve 主要包括以下的流程：</p><ol><li><strong>计划（Plan）</strong>：Agent 在收到 query 后，它会将这个任务分解为更小的子任务；</li><li><strong>行动（Action）</strong>: Agent 对当前的子任务进行执行；</li><li><strong>观察（Observation）</strong>: Agent 观察当前行动的结果，如果解决问题就返回，如果仅解决当前子任务就继续执行计划，如果没解决当前子任务就重新计划后续步骤；</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458170" alt="image.png" title="image.png" loading="lazy"/></p><p>* 注意： 上图中 ② Action x 1 表示每次行动只执行一个子任务（不会全部将子任务执行完，区别 ReWOO的对应流程中的 ② Action x N）。</p><p>PlanAndSolveAgent由两个组件组成：首先，将整个任务分解为更小的子任务，其次，根据计划执行这些子任务。最后结果作为答案进行输出。</p><pre><code>import lazyllm
from lazyllm.tools import fc_register, PlanAndSolveAgent
@fc_register("tool")
def multiply(a: int, b: int) -&gt; int:
    return a * b
@fc_register("tool")
def add(a: int, b: int):
    return a + b
llm = lazyllm.OnlineChatModule(source="sensenova", model="DeepSeek-V3")
tools = ["multiply", "add"]
agent = PlanAndSolveAgent(llm, tools=tools)
query = "What is 20+(2*4)? Calculate step by step."
ret = agent(query)
print(ret)
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458171" alt="image.png" title="image.png" loading="lazy"/></p><h5><strong>（4）ReWOO</strong></h5><p>ReWOO (Reasoning WithOut Observation) 主要包括以下流程：</p><ol><li><strong>计划（Plan）</strong>：Agent 在收到 query 后，它会生成一个计划表，计划表中包含了这个任务分解的更小子任务，子任务间的执行结果用占位符表示；</li><li><strong>行动（Action）</strong>: Agent 对每个子任务依次进行执行（调用工具），将结果都填入计划表的占位符中；</li><li><strong>解决（Solve）</strong>: Agent 观察所有行动的反馈，将结果response返回给用户；</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458172" alt="image.png" title="image.png" loading="lazy"/></p><p>ReWOOAgent 包含三个部分：Planner 、 Worker 和 Solver。</p><p>其中， <strong>Planner</strong> 使用可预见推理能力为复杂任务创建解决方案蓝图； <strong>Worker</strong> 通过工具调用来与环境交互，并将实际证据或观察结果填充到指令中； <strong>Solver</strong> 处理所有计划和证据以制定原始任务或问题的解决方案。</p><pre><code>import lazyllm
from lazyllm import fc_register, ReWOOAgent, deploy
import wikipedia
@fc_register("tool")
def WikipediaWorker(input: str):
    try:
        evidence = wikipedia.page(input).content
        evidence = evidence.split("\n\n")[0]
    except wikipedia.PageError:
        evidence = f"Could not find [{input}]. Similar: {wikipedia.search(input)}"
    except wikipedia.DisambiguationError:
        evidence = f"Could not find [{input}]. Similar: {wikipedia.search(input)}"
    return evidence
@fc_register("tool")
def LLMWorker(input: str):
    llm = lazyllm.OnlineChatModule(stream=False)
    query = f"Respond in short directly with no extra words.\n\n{input}"
    response = llm(query, llm_chat_history=[])
    return response
tools = ["WikipediaWorker", "LLMWorker"]
llm = lazyllm.TrainableModule("Qwen2-72B-Instruct-AWQ").deploy_method(deploy.vllm).start()
agent = ReWOOAgent(llm, tools=tools)
query = "What is the name of the cognac house that makes the main ingredient in The Hennchata?"
ret = agent(query)
print(ret)
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458173" alt="image.png" title="image.png" loading="lazy"/></p><p>让我们简单总结如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458174" alt="" title="" loading="lazy"/></p><h4><strong>4. 简化Agent工作流程</strong></h4><p>在Agent开发中，重复造轮子、工具接口不统一、上下文管理复杂等问题让开发流程冗长且低效。</p><p>为了解决这些难点，我们可以通过“<strong>MCP协议+LazyLLM</strong>”的框架，提升开发效率、降低门槛，让开发者能专注于核心业务和创新设计，从而推动大模型应用更快落地。</p><h5><strong>（1）MCP协议的基本概念</strong></h5><p><strong>MCP（Model Context Protocol，模型上下文协议）</strong>是由Anthropic公司于2024年11月推出的一种<strong>开放标准协议</strong>，旨在让大语言模型能够“无缝连接”外部工具和数据源。</p><p>简单来说，MCP就是为了解决开头那些痛点而生的“标准化利器”。一个更形象的比喻是：<strong>MCP 相当于 AI 应用的USB-C接口</strong>。</p><p>正如USB-C统一了不同品牌电子设备的充电和数据接口一样，MCP则标准化了<strong>AI与外部世界交互的方式</strong>，使得模型能够以<strong>标准化</strong>的形式<strong>高效调用</strong>数据库、工具和网络搜索等多种资源，从而实现<strong>模型与外部系统的高效联动</strong>。</p><p>换句话说，过去每接入一个新工具就头大的“接口不统一”问题，有了MCP后就像使用统一接口的外设一样，<strong>插上就能用</strong>。这样一来，<strong>无需二次开发</strong>，多种数据库、Web API、文件系统、GitHub…海量而强大的功能统统都可以通过这一个协议<strong>轻松接入</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458175" alt="image.png" title="image.png" loading="lazy"/></p><p>以前，想让AI Agent查天气、读PDF、执行Python代码，可能需要针对每个功能写一堆集成代码，其中包含工具的描述、入参等等，并封装成“工具（Tool）”给到模型。</p><p>而有了MCP，只需要把符合需求的MCP服务器接上，模型就会自动知道有什么工具可用、该如何调用，并且输入输出格式也是统一好的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458176" alt="image.png" title="image.png" loading="lazy"/></p><p>整个过程就像给笔记本电脑插上<strong>扩展坞</strong>的瞬间，额外冒出HDMI、SD卡、网线等接口等<strong>繁琐的对接细节</strong>由协议帮你搞定，从此开发者无需关心那些转换过程。</p><p>因此，MCP的出现<strong>大幅提升了AI Agent应用开发的效率</strong>。</p><h5><strong>（2）MCP的技术架构</strong></h5><p>从技术架构上看，MCP遵循的是典型的<strong>客户端-服务器模型</strong>，它把AI应用的<strong>内部逻辑和外部扩展功能解耦</strong>为三个核心模块：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458177" alt="image.png" title="image.png" loading="lazy"/></p><p>1️⃣<strong>Host（主机）</strong></p><p>指运行AI应用（类似支持AI对话的IDE插件如Cursor、桌面应用如Claude Desktop以及我们所创建的智能体应用）本身的<strong>宿主环境</strong>。Host负责<strong>提供AI交互环境</strong>，并在内部<strong>启动MCP Client</strong>。</p><p>2️⃣<strong>Client（客户端）</strong></p><p>运行在Host内部的客户端，它与MCP Server建立连接，充当AI应用和外部世界沟通的<strong>桥梁</strong>。MCP客户端维持与服务器的 1:1 连接，当AI模型需要调用工具或获取数据时，都是由Client按照协议与Server通信来完成。</p><p>3️⃣<strong>Server（服务器）</strong></p><p>MCP服务器提供具体的功能和数据，相当于AI大脑可以远程调用的<strong>外设</strong>。一个服务器上通常会暴露几类内容供AI使用：</p><ul><li><strong>Tools（工具）</strong>：允许大模型调用的功能函数。例如代码执行、网页浏览、发送邮件等，这些能力都可以作为可调用的工具由Server打包并提供给AI。</li><li><strong>Resources（资源）</strong>：给大模型提供的数据或内容。例如数据库记录、文件内容、浏览网页截图等，Server可以将这些外部数据通过协议发送给AI应用，以充当LLM的上下文。</li><li><strong>Prompts（提示模板）</strong>：预设的可复用提示词模板或交互工作流。Server可以储存一些常用提示词，按需提供给AI，避免每次都从零编写复杂提示。</li></ul><p>更多MCP技术架构的细节可查阅：<a href="https://link.segmentfault.com/?enc=TBM3dq6yEZ7aYWnAs5fN6g%3D%3D.z8Pilk7wBzXjzRLB5h1IP3Nt%2F2SwhmaSOyvbqOC%2F8kuEIVFjRgb2ezYE5xcnY38ik5NF5Z3W9pf3Hk7rhSPsTw%3D%3D" rel="nofollow" target="_blank">https://modelcontextprotocol.io/docs/concepts/architecture</a></p><hr/><p>通过上述架构，过去东拼西凑解决的难题，现在有了明确的协议规范可循，那么，MCP、Agent、LLM、Tool Call...这些名词之间到底有什么关系？</p><ul><li><strong>LLM</strong>是Agent的“<strong>大脑</strong>”，能够根据输入信息（如系统提示词、用户指令、历史对话信息、可用工具集信息等），输出对应的文字内容，其中可能是阶段性的工具调用信息，也有可能是任务完成后的最终输出内容。</li><li><strong>Tool Call</strong>是LLM经过大量训练后具备的一种<strong>工具调用能力</strong>，这种能力允许LLM能够综合历史信息和可用工具信息，动态决策并输出格式化的工具调用指令（决定使用哪个工具、工具调用时具体传入什么参数），通过这种指令指导Agent正确的完成工具调用，从而实现特定动作（如操作文件、执行代码）、获取必要信息（如返回网页爬虫结果）。</li><li><strong>MCP Server</strong>则是遵循MCP协议的<strong>工具供应商</strong>，其提供给Agent强大的工具集，以供LLM辨识并执行Tool Call，同时接收Agent给到的Tool Call指令，安全的与外部资源进行交互，以实现特定动作或返回特定信息。</li><li><strong>Agent</strong>作为智能体应用与用户交互的<strong>唯一入口</strong>，在接收到任务指令后，会有序地调用LLM、各种工具，以完成任务。</li></ul><h5><strong>（3）实践：在LazyLLM中使用MCP</strong></h5><p>针对MCP，LazyLLM提供了两种接入方式：直接接入和<strong>部署并远程接入</strong>。</p><ul><li><strong>直接接入</strong>：将指定MCP Server的启动配置直接给到lazyllm.tools.MCPClient，以Stdio模式启动Server，并获取Agent可调用的工具集。</li><li><strong>部署并远程接入</strong>：针对一些资源占用高，或者期望启动的MCP Server可复用的场景，LazyLLM支持MCP Server的一键部署，只需一行命令，便可以将MCP Server单独启动，随后便可以SSE模式远程接入MCP Server。</li></ul><p>具体来说，步骤如下：</p><p><strong>1️⃣配置LazyLLM所需要的所有依赖</strong></p><p>首先参考 <a href="https://link.segmentfault.com/?enc=%2FOKpL%2Fb7jQ0nf7qBUcbwiA%3D%3D.od6NzhP4DaizGjkMyQQp1WVClppawgurCrLM1fXkb3lrzLxHQjUWw%2F4oUDq0nfNZ" rel="nofollow" target="_blank">https://docs.lazyllm.ai/zh-cn/latest/</a> 的Getting started部分，安装LazyLLM并完成环境配置。</p><p>同时，由于MCP Server的使用依赖Node.js和npm，可参考<a href="https://link.segmentfault.com/?enc=6Oo21BPmdZ8lRmJQ2vhobQ%3D%3D.1KfNyTte9iqMWvvIu3rj00Jgh5G6GifyDhz5iGTH5HI%3D" rel="nofollow" target="_blank">https://nodejs.org/en/download</a> 完成最新版本的安装和配置。</p><p><strong>2️⃣利用已有的MCP服务</strong></p><p>若需接入已有的 MCP 服务（如高德地图的地理位置服务），可通过 LazyLLM 的 MCPClient 工具直接连接，无需自行部署 Server。</p><p><strong>SSE URL 接入（以高德 MCP 为例）：</strong></p><p>无需启动本地 Server，直接通过服务提供商提供的 SSE 长连接 URL 配置 Client。需将”xxx”替换为自己的key。</p><p>（创建key：<a href="https://link.segmentfault.com/?enc=ZikqZbmLYvy%2BAwcNPwK1Yg%3D%3D.4xCRO8tznFI3J8mlCkX3u1b0BjbXgyGWW0zOYVIdrBGU%2FQkJjW53ypKeI8eQFf%2FFZDcAiyKIGpX%2Ba%2BcizB5dIMYs5E0tQ9RutYzu5ppkMIY%3D" rel="nofollow" target="_blank">https://lbs.amap.com/api/mcp-server/create-project-and-key）</a></p><pre><code>import lazyllm
from lazyllm.tools.agent import ReactAgent
from lazyllm.tools import MCPClient
mcp_configs = {
    "amap_mcp": {
        "url": "http://mcp.amap.com/sse?key=xxx"
    }
}
client = MCPClient(command_or_url=mcp_configs["amap_mcp"]["url"])
llm = lazyllm.OnlineChatModule(source='qwen', model='qwen-max-latest', stream=False)
agent = ReactAgent(llm=llm.share(), tools=client.get_tools(), max_retries=15)
print(agent("查询北京的天气"))
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458178" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>3️⃣使用直接接入的方式调用MCP</strong></p><ul><li><strong>配置获取</strong></li></ul><p>我们选择一个文件管理 MCP Server 并获取启动配置：</p><pre><code>{  
    "mcpServers": {    
        "filesystem": {     
            "command": "npx",      
            "args": [        
                "-y",        
                "@modelcontextprotocol/server-filesystem",        
                "/Users/username/Desktop"      
            ]    
        }  
    }
}
</code></pre><p>注意，如果你是Windows系统，command需要使用"cmd"，同时启动参数开头需要加上"/c"。启动配置会有些变化：</p><pre><code>{  
    "mcpServers": {    
        "filesystem": {     
            "command": "cmd",      
            "args": [
                "/c", 
                "npx",         
                "-y",        
                "@modelcontextprotocol/server-filesystem",        
                "/Users/username/Desktop"      
            ]    
        }  
    }
}
</code></pre><ul><li><strong>MCP接入</strong></li></ul><p>随后便可使用LazyLLM的MCPClient工具实现MCP Server的接入（这里的路径示例/xxx/xxx/xxx）。</p><pre><code>import lazyllm
from lazyllm.tools import MCPClient
config = {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-filesystem", "/xxx/xxx/xxx"]}
client = MCPClient(command_or_url=config["command"], args=config["args"], env=config.get("env"))
</code></pre><ul><li><strong>工具集获取</strong></li></ul><pre><code>&gt;&gt;&gt; tools = client.get_tools()
Secure MCP Filesystem Server running on stdio
Allowed directories: [ '/Users/username/Desktop' ]
&gt;&gt;&gt; tools
[&lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269cad11c0&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91e520&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91d800&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91d8a0&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91e5c0&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91e0c0&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91d940&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91e480&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91db20&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91da80&gt;, &lt;function generate_lazyllm_tool.&lt;locals&gt;.dynamic_lazyllm_func at 0x7f269c91dda0&gt;]
</code></pre><p><strong>代码讲解</strong>：</p><p>调用client.get\_tools()可以获取当前连接的MCP Server中所有的工具（在异步环境中，以下代码改为tools = await client.aget\_tools()即可）。</p><p>同时，LazyLLM支持开发者通过传入工具名称列表至方法的方式获取特定的工具集，例如client.get\_tools(["tool\_name1", "tool_name2"])。</p><ul><li><strong>工具调用</strong></li></ul><p><strong>代码讲解</strong>：</p><p>遍历从MCP Server获取的tools，其中每个成员都是一个函数。每个功能函数都有函数名（<strong>name</strong>）、函数描述（<strong>doc</strong>，包含了功能描与参数描述）以及入参声明（<strong>annotations</strong>），调用对应函数时，只需要传入正确的参数即可。</p><p>下面给出两个函数调用的例子：</p><ul><li>调用文件读取工具read_file，传入所需入参path，即可获取读取文件后的返回信息；</li><li>调用获取有权限路径工具list\_allowed\_directories，该工具无需任何入参，传入空即可获得工具返回。</li></ul><pre><code>&gt;&gt;&gt; for t in tools:
...     print(f"\nTool name:\n{t.__name__}\nTool desc:\n{t.__doc__}\nTool params:\n{t.__annotations__}\n")
... 
Tool name:
read_file
Tool desc:
Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Only works within allowed directories.
Args:    
    path (str): type: string.
Tool params:
{'path': &lt;class 'str'&gt;}
Tool name:
write_file
Tool desc:
Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.
Args:    
    path (str): type: string.    
    content (str): type: string.
Tool params:
{'path': &lt;class 'str'&gt;, 'content': &lt;class 'str'&gt;}
......
Tool name:
list_allowed_directories
Tool desc:
Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.
Args:    
    No parameters.
Tool params:
{}
</code></pre><pre><code>&gt;&gt;&gt; t1 = tools[0]
&gt;&gt;&gt; t1.__name__
'read_file'
&gt;&gt;&gt; t1(path="xxx/xxx/xxx/test.md")
Secure MCP Filesystem Server running on stdio
Allowed directories: [ 'xxx/xxx/xxx' ]
'Tool call result:\nReceived text message:\nThis is a test file for LazyLLM and MCP.\n\nEnd\n'
&gt;&gt;&gt; t2 = tools[-1]
&gt;&gt;&gt; t2.__name__
'list_allowed_directories'
&gt;&gt;&gt; t2()
Secure MCP Filesystem Server running on stdio
Allowed directories: [ 'xxx/xxx/xxx' ]
'Tool call result:\nReceived text message:\nAllowed directories:\n/xxx/xxx/xxx'
</code></pre><p><strong>4️⃣使用LazyLLM部署MCP Server并接入</strong></p><p>LazyLLM支持MCP Server的一键部署，只需一行命令，便可以将MCP Server单独启动，主程序可使用SSE模式接入MCP Server。</p><ul><li><strong>一键部署MCP Server</strong></li></ul><p>选择浏览器工具 playwright（<a href="https://link.segmentfault.com/?enc=UHMqfWxxI7NLKzc1uli3Lg%3D%3D.ufStJI%2FTgiEfa35qFOg2OlQ5Lx1Ujm76knWXhpq7f4RjPdTFg0EGtV5Hv4D%2F6ef0" rel="nofollow" target="_blank">https://github.com/microsoft/playwright-mcp</a> ），获取配置信息：</p><pre><code>{  
    "mcpServers": {    
        "playwright": {      
            "command": "npx",      
            "args": [        
                "@playwright/mcp@latest"     
            ]    
        }  
    }
}
</code></pre><p>在命令行中只需要使用“lazyllm deploy mcp_server xxxxxx”命令，并配置host、port，即可完成MCP Server的部署。由于linux环境没有GUI，这里演示Windows环境下的启动命令：</p><pre><code>lazyllm deploy mcp_server --sse-port 11238 cmd -- /c npx @playwright/mcp@latest
</code></pre><p>启动后如下所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458179" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>接入部署完成的MCP Server</strong></li></ul><p>我们可以在其他程序中传入url，以SSE的方式接入MCP Server，注意，这里的url需要加上'/sse'，否则无法正常运行：</p><pre><code>&gt;&gt;&gt; config = {"url": "http://127.0.0.1:11238/sse"}
&gt;&gt;&gt; client = MCPClient(command_or_url=config["url"])
</code></pre><p>用以上方式接入MCP Server后，具体的工具获取、工具调用方式与直接接入保持一致。</p><p><strong>5️⃣LazyLLM调用MCP工具</strong></p><p>步骤 1：获取工具列表</p><pre><code>tools = client.get_tools()  # 同步获取
# 或 tools = await client.aget_tools()  # 异步环境
</code></pre><p>步骤 2：查看工具详情</p><pre><code>for t in tools:
    print(f"Tool name: {t.__name__}")
    print(f"Tool desc: {t.__doc__}")
    print(f"Tool params: {t.__annotations__}\n")
</code></pre><p>步骤 3：调用MCP工具</p><p>以读取文件工具为例，假设 tools[0] 为 read_file。</p><pre><code>t1 = tools[0]
result = t1(path="xxx/xxx/xxx/test.md")
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458180" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>6️⃣LazyLLM+MCP智能体Demo</strong></p><p>接下来我们使用filesystem+playwright，结合LazyLLM的Agent模块，创建一个智能体：</p><pre><code>import lazyllm
import lazyllm.tools.agent 
from lazyllm.tools import ReactAgent
import MCPClient
if __name__ == "__main__":    
    mcp_configs = {        
        "file_system": {            
            "command": "cmd",            
            "args": [                
                "/c",                
                "npx",                
                "-y",                
                "@modelcontextprotocol/server-filesystem",                
                "./"            
            ]        
        },        
        "play_wright": {            
            "url": "http://127.0.0.1:11244/sse"        
        }    
    }    
client1 = MCPClient(command_or_url=mcp_configs["file_system"]["command"], args=mcp_configs["file_system"]["args"])    
client2 = MCPClient(command_or_url=mcp_configs["play_wright"]["url"])    
llm = lazyllm.OnlineChatModule(source="deepseek")    
agent = ReactAgent(llm=llm.share(), tools=client1.get_tools()+client2.get_tools(), max_retries=15)    
print(agent("浏览谷歌新闻，并写一个今日新闻简报，以markdown格式保存至本地。"))
</code></pre><p>通过本次实践，我们可以了解到，MCP Server的出现直接省去了Agent开发环节中工具研发和调试的成本，<strong>大大提升了研发效率</strong>。LazyLLM对于MCP提供了灵活的接入方式，让开发者使用MCP的<strong>成本大大降低</strong>。</p><p><strong>总结</strong>：</p><p>在大模型时代，<strong>开发效率就是核心竞争力</strong>。从头造轮子或许可以练手，但在真正落地AI应用的过程中，我们更应该把宝贵的时间和脑力，留给真正创造价值的部分——如业务逻辑设计、用户体验优化、创新交互方式等，而不是重复造工具、上下文拼接等基础组件。</p><p><strong>MCP</strong> 提供了一套高效、统一的<strong>标准协议</strong>；<strong>LazyLLM</strong> 则提供了一套<strong>灵活的MCP接入方案</strong>，让每一个开发者都能轻松上手，快速构建属于自己的智能Agent应用，从而站在<strong>社区和开源生态</strong>的“肩膀”上看得更远、做得更多。</p><h5><strong>（4）理性看待 MCP</strong></h5><p>尽管MCP简化了开发流程，但需注意其局限性：</p><ul><li><strong>依赖性风险</strong>：过度依赖第三方MCP服务可能导致业务受制于外部稳定性与政策变化。</li><li><strong>工具选择</strong>：MCP没有解决当前Agent的一个困境：当工具比较多的时候，如何快速而准确地选到最合适的工具。</li></ul><p>开发者应根据实际需求权衡选择，优先在轻量级场景中尝试MCP，逐步验证其适用性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458181" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>（三）Agentic RAG 简介</strong></h3><h4><strong>1. 基本概念</strong></h4><p>Agentic RAG 是 RAG 的一种扩展，它通过引入 AI智能体 来增强 RAG 的功能，使得系统能够执行更复杂的任务。</p><p>举个例子，如果说 RAG 是带着<strong>书本</strong>去考试的考生，AI Agent 是<strong>专家</strong>，那么 Agentic RAG 就是带着<strong>专家</strong>去考试的考生！</p><p>简单来看，下图中，单个 LLM 就好比一个去参加闭卷考的学生；我们给这个学生带本书，那么就可以获得一个RAG；如果我们把书替换为专家，那我们就获得了一个 Agentic RAG。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458182" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>2. 基本架构</strong></h4><p>Agentic RAG 就是引入了 AI智能体的 RAG。</p><p>前面的示例中，我们将 RAG 的搜索组件（Retrieval Component）给替换为了<strong>单AI智能体</strong>。</p><p>除此之外我们还可以将<strong>搜索组件</strong>给替换为<strong>多AI智能体</strong>，甚至也可以把<strong>生成组件</strong>（Generative Component）给替换为<strong>AI 智能体</strong>。</p><h5><strong>（1）单 Agent RAG</strong></h5><p>下面是一个常见的 Agentic RAG，其中的 AI Agent 模块提供了两个外挂知识库、一个网络搜索工具、一个计算器和一个数据库，这样智能体可以根据上下文的需求，决定从哪里来<strong>检索信息</strong>。并且如果在一轮检索中不能获得满意的信息，智能体还可以<strong>再次重新检索</strong>（它可以自动更换检索的关键词，选取不同的工具等等）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458183" alt="image.png" title="image.png" loading="lazy"/></p><p>在 Agentic RAG 中，可以将 AI 智能体融入<strong>检索组件</strong>，形成 Retrieval Agent。检索过程变得智能，智能体能根据 query 循环检索，动态优化结果。同时，智能体可接入<strong>网络、数据库等多种工具</strong>，突破单一知识库限制，获取更丰富、准确的上下文信息。</p><p>单Agent RAG的工作流程可以拆解为：</p><ul><li>用户输入Query → Agent动态规划检索策略</li><li>多次检索（更换关键词/工具）→ 多源数据融合</li><li>结果增强 → LLM生成回复</li></ul><p>引入智能体后，查询过程实现自动化与智能化，系统可自主多轮检索，无需人工干预即可提升信息匹配效果。</p><h5><strong>（2）多 Agent RAG</strong></h5><p>我们还可以引进专家组！是的就是<strong>多Agent智能体</strong>，下图中 Retrieval Agent A 专家负责两个知识库的检索，Retrieval Agent B 专家负责网络搜索，Retrieval Agent C 专家负责两个数据库的搜索，他们都是各个数据源的搜索专家，最后有一个Retrieval Agent 专家作为总指挥，他擅长搜索任务的分配。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458184" alt="image.png" title="image.png" loading="lazy"/></p><p>如果你想，我们当然也可以把生成模块给替换为一个 AI 智能体，如下图所示。这样我们就拥有了两个专家，一个专家负责检索，另外一个专家负责生成内容。</p><p>如下图所示，<strong>检索专家</strong>拥有很多途径来自主决策检索信息，<strong>生成专家</strong>也可以边搜索边生成内容，如果它觉得生成的内容不满意，还会自动重新生成！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458185" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3. 工作流程</strong></h4><p>在 Agentic RAG 中，可以将 AI 智能体融入到不同的组件中，一般常见的是将检索组件替换为 <strong>AI 智能体</strong>（变成：Retrieval Agent）。这也就意味着检索组件将变得智能，可以根据 query 不断地去检索来获取更加丰富和准确的上下文。</p><p>同时由于 AI 智能体可以接入很多工具，这极大增强了检索的能力，甚至如果在知识库中无法检索到合适的内容，AI 智能体也能从网络、数据库或者其他一切可访问的工具中来获得更多的内容。</p><p>让我们以<strong>单 Agent RAG</strong> 为例，如下图所示，来看一下在<strong>不同的智能体工作流</strong>下是如何完成检索的。</p><p>1️⃣首先，一条 query 被传给了智能体①</p><ul><li>如果智能体是 <strong>Function Call Agent</strong>，那么它会根据 query 来不断调用工具，并观察查询到的信息，以此不断循环②直到查询到令它满意的信息，或者达到最大循环次数；</li><li>如果智能体是 <strong>React</strong>，那么它先根据 query 来做个思考，然后开始调用工具，并观察查询到的信息，以此不断循环②，也是直到查询到令它满意的信息，或达到最大循环次数；</li><li>如果智能体是 <strong>PlanAndSolve</strong>，那么它会先根据 query 来做个计划将查询任务进行分解为子任务，然后它开始执行子任务，比如调用查询知识库的工具，在知识库返回信息后，它会观察结果，如果结果不满意它会重新修改计划，如果结果还行它会继续沿着计划执行下一个子任务，以此不断循环②，直到最后完成它自己制定的所有任务而获得查询的信息；</li><li>如果智能体是 <strong>ReWOO</strong>，那么它也会根据 query 来做个计划，将查询任务分解为子任务，然后它会依次将子任务全部执行完毕②，最后将综合所有的执行结果来给出它查询的结果。</li></ul><p>2️⃣在智能体查询到信息后，就回到了经典的 RAG 工作流：查询到的信息（已经是被智能体将 query 融合增强后的结果）③会被送给 LLM 来完成内容生成任务④。</p><p>至此，一个单 Agent RAG 的工作流程就完成了。</p><p>从中我们可以看出，在查询阶段，由于我们引入了智能体，查询变得更加智能，智能体会自己不断去查询，我们不用操心查询的过程，以及担心只查一次找不到匹配的信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458186" alt="image.png" title="image.png" loading="lazy"/></p><hr/><h2><strong>二、引入动机</strong></h2><p>Agentic RAG 仅是在原有 RAG 的工作流中将其组件替换为了智能体。为什么要这样？为什么要搞出 Agentic RAG? 或者说为什么要给 RAG 中引入智能体？</p><p>一个很简单原因就是<strong>为了让它更强大！更加智能化。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458187" alt="image.png" title="image.png" loading="lazy"/></p><ol><li><p>经典的 RAG 仅进行单次查询，如果单次无法召回到合适的文档信息，那么后续的生成过程的效果是无法保障的。</p><p>但是 Agentic RAG 可以进行<strong>多次查询</strong>(multiple query)，如果此次召回效果不好，智能体会自动更换表示方式或更换工具进行检索；</p></li><li><p>经典的 RAG 的数据来源很单一，往往只有一个知识库。</p><p>但是 Agentic RAG 可以接入大量的知识库，而且不止于此，它还可以<strong>接入数据库</strong>，甚至是<strong>联网搜索</strong>，这意味着 Agentic RAG 的<strong>数据来源是多样的</strong>(multi source)；</p></li></ol><p>多样的数据源，不仅可以补充单数据源的信息不足，拥有更多的信息；</p><p>多样的数据源，也可以对查询到的信息进行相互佐证，保障查询结果的准确性；</p><ol start="3"><li>Agentic RAG 额外还有<strong>多工具调用</strong>的能力，这充满了无限的功能(multi-function)，它可以对信息进行处理和加工；</li><li>Agentic RAG 更重要的是它可以<strong>智能决策</strong>(smart decision-making)！它可以自动制定计划来实现复杂的查询过程。整个过程都不需要我们操心。</li></ol><p>可以想象这就是带着一本教科书和带着专家去考试的区别！</p><hr/><h2><strong>三、搭建实现</strong></h2><p>让我们从一个基础的 RAG 开始，然后示例在 LazyLLM 中如何注册工具并使用 React AI 智能体，最后将两者结合实现一个简单的 Agentic RAG。</p><h3><strong>（一）搭建基础 RAG</strong></h3><p>在之前教程的基础上，我们可以使用 LazyLLM 来快速搭建一个 RAG 应用。该应用的逻辑如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458188" alt="image.png" title="image.png" loading="lazy"/></p><p>具体代码如下所示，在这个RAG中，我们设置了个检索器 Retriever 和 Reranker 用于检索知识库。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=QKnQOduM1wHY4hpp0pBunQ%3D%3D.4HhcUWl3V4W4HYDfS2zeMUztm2zTQEreLQhw2K84o2RStDJA7i8HbCugNEryPy3RP1INA%2FSkxhBfn0ItVpUKDTb6oMxrclxXIEorSe5oCHfaDRcLUYCTRQqbMyvDUG1sUEaZjFTIsQO4EN7hcSYbTFaBQnLvTH19r9x3XK1tRKc%3D" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter18/basic_rag.py）</a></p><pre><code>import lazyllm
from lazyllm import pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Document, Retriever, Reranker

prompt = 'You will play the role of an AI Q&amp;A assistant and complete a dialogue task. In this task, you need to provide your answer based on the given context and question.'

documents = Document(dataset_path="rag_master", embed=OnlineEmbeddingModule(), manager=False)
documents.create_node_group(name="sentences", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)

with pipeline() as ppl:
    ppl.retriever = Retriever(documents, group_name="sentences", similarity="cosine", topk=1)
    ppl.reranker = Reranker("ModuleReranker", model=OnlineEmbeddingModule(type="rerank"), topk=1, output_format='content', join=True) | bind(query=ppl.input)
    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)
    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(prompt, extro_keys=["context_str"]))

if __name__ == "__main__":
    lazyllm.WebModule(ppl, port=range(23467, 24000)).start().wait()
</code></pre><p>让我们运行一下看看结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458189" alt="image.png" title="image.png" loading="lazy"/></p><h3>（二）<strong>AI智能体 React</strong></h3><p>Agentic RAG 就是引入了 AI 智能体的 RAG，这里让我们用 LazyLLM 来注册一个假的知识库搜索工具，实现一个 React:</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=egq5GTdwpKuArff8hBHbAA%3D%3D.VdCeA4MxC3ufFuh7CinFUYi1Urgd%2BAHxIthlqTgW3HsuVbjp0pMJ0gkmhEYxditcghn1nwNzlAmdw6F0YTssHBxKQKqIi%2B7fpVkDlbH60v8QHOhHGmrz2GNP5jb8M3tQ" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/react.py）</a></p><pre><code>import json
import lazyllm
from lazyllm import fc_register, ReactAgent

@fc_register("tool")
def search_knowledge_base(query: str):
    '''
    Get info from knowledge base in a given query.

    Args:
        query (str): The query for search knowledge base.
    '''
    return "无形"

llm = lazyllm.OnlineChatModule(stream=False)

tools = ["search_knowledge_base"]
agent = ReactAgent(llm, tools)

if __name__ == "__main__":
    res = agent("何为天道？")
    print("Result: \n", res)
</code></pre><p>让我们尝试来运行一下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458190" alt="image.png" title="image.png" loading="lazy"/></p><p>有了 React，我们就可以将它的工具替换为 RAG 中的 Retriever 和 Reranker 来作为一个真实的知识库。让它可以调用检索器：</p><pre><code>import lazyllm
from lazyllm import (pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Reranker,
                     Document, Retriever, fc_register, ReactAgent)

documents = Document(dataset_path="rag_master", embed=OnlineEmbeddingModule(), manager=False)
documents.create_node_group(name="sentences", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)
with pipeline() as ppl_rag:
    ppl_rag.retriever = Retriever(documents, group_name="sentences", similarity="cosine", topk=3)
    ppl_rag.reranker = Reranker("ModuleReranker", model=OnlineEmbeddingModule(type="rerank"), topk=1, output_format='content', join=True) | bind(query=ppl_rag.input)

@fc_register("tool")
def search_knowledge_base(query: str):
    '''
    Get info from knowledge base in a given query.

    Args:
        query (str): The query for search knowledge base.
    '''
    return ppl_rag(query)

tools = ["search_knowledge_base"]
llm = lazyllm.OnlineChatModule(stream=False)

agent = ReactAgent(llm, tools)

if __name__ == "__main__":
    res = agent("何为天道？")
    print("Result: \n", res)
</code></pre><p>运行结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458191" alt="image-2.png" title="image-2.png" loading="lazy"/></p><h3><strong>（三）实现 Agentic RAG</strong></h3><p>让我们将 RAG 的检索组件替换为带单个知识库的React，实现下面逻辑（这里简单起见，只用了一个知识库作为工具）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458192" alt="image.png" title="image.png" loading="lazy"/></p><p>对应代码如下：</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=3VV2aVGV13jJo3HZGJOmrg%3D%3D.7P6y2Xpm8h2K6ByfjcYOiF0ga7nLUi0p1NceLga%2BBblgZQZ%2FZhr%2FAj6TacoPAMDQWJcIJNJI1r%2BdQn76dpwCdH1xE3pGu3eVAZzfO4bGxF0BRfd1zW5b2ZR%2FzEu%2BNU%2Bs" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_react.py）</a></p><pre><code>import lazyllm
from lazyllm import (pipeline, bind, OnlineEmbeddingModule, SentenceSplitter, Reranker,
                     Document, Retriever, fc_register, ReactAgent)

prompt = 'You will play the role of an AI Q&amp;A assistant and complete a dialogue task. In this task, you need to provide your answer based on the given context and question.'

documents = Document(dataset_path="rag_master", embed=OnlineEmbeddingModule(), manager=False)
documents.create_node_group(name="sentences", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)
with pipeline() as ppl_rag:
    ppl_rag.retriever = Retriever(documents, group_name="sentences", similarity="cosine", topk=3)
    ppl_rag.reranker = Reranker("ModuleReranker", model=OnlineEmbeddingModule(type="rerank"), topk=1, output_format='content', join=True) | bind(query=ppl_rag.input)

@fc_register("tool")
def search_knowledge_base(query: str):
    '''
    Get info from knowledge base in a given query.

    Args:
        query (str): The query for search knowledge base.
    '''
    return ppl_rag(query)

tools = ["search_knowledge_base"]

with pipeline() as ppl:
    ppl.retriever = ReactAgent(lazyllm.OnlineChatModule(stream=False), tools)
    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)
    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(prompt, extro_keys=["context_str"]))

if __name__ == "__main__":
    lazyllm.WebModule(ppl, port=range(23467, 24000)).start().wait()
</code></pre><p>效果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458193" alt="image.png" title="image.png" loading="lazy"/></p><p>至此一个简单的 Agentic RAG 我们就实现了。</p><h3><strong>（四）更多的尝试</strong></h3><p>你可以尝试使用不同的 AI 智能体工作流来替换上面的 React：</p><ul><li><p>FunctionCallAgent代码GitHub链接</p><p>（<a href="https://link.segmentfault.com/?enc=vrpceYv6XzTY1deWW4LRDQ%3D%3D.AIwvfG23QC%2F7qrK2Do%2F7QTVmfnqhDvsv1vtfKyAXoVNRNVtT4Cl8n0QSd57aU1PR3Syw%2FPZCCliRKqkjeCnKmphB5crngbBdIjE6S25z0ia8NRtwqalyectP4Bdn3P3J" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_functioncall.py）</a></p></li><li><p>PlanAndSolveAgent代码GitHub链接</p><p>（<a href="https://link.segmentfault.com/?enc=fmDja3DY06he%2BrjWMNTu5Q%3D%3D.1YOSo8J6mHM4jMcCJWC1tMEdbw%2FAWybV7Lcbi7gVe7kGGDybJoP2I1M7joa7et4OQWAlcO6%2FP%2FkBbpRgRfw6DOzjjrmb34FQqs%2BN1f%2FMLc6Tc3sRej15v5b3%2FBhjCzOZ" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_planandsolve.py）</a></p></li><li><p>ReWOOAgent代码GitHub链接</p><p>（<a href="https://link.segmentfault.com/?enc=ojEAEMT6NSEc%2BE1b7YIuMw%3D%3D.dv6Ay0EIH5hWfi8X1zzep8uyT8JDW09qxT5G3aF2%2BQ0F1T8UZ641tuTERGsrsPmtCOSMXQb9jrEYTbZzizWK52g2uEslB1BTaXSWZXYSQ3fdc7sravJa4zXjkbiEuo9j" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter18/rag_rewoo.py）</a></p></li></ul><pre><code>from lazyllm import FunctionCallAgent, PlanAndSolveAgent, ReWOOAgent

# Use FunctionCallAgent:
ppl.retriever = FunctionCallAgent(lazyllm.OnlineChatModule(), tools)
# Use PlanAndSolveAgent:
ppl.retriever = PlanAndSolveAgent(lazyllm.OnlineChatModule(), tools)
# Use ReWOOAgent:
ppl.retriever = ReWOOAgent(lazyllm.OnlineChatModule(), tools)
</code></pre><p>这里我们尝试将ReactAgent 分别替换为FunctionCallAgent, PlanAndSolveAgent, ReWOOAgent来查看效果：</p><h4><strong>1. FunctionCallAgent</strong></h4><p>FunctionCallAgent的效果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458194" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>2. PlanAndSolveAgent</strong></h4><p>PlanAndSolveAgent的效果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458195" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3. ReWOOAgent</strong></h4><p>ReWOOAgent的效果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458196" alt="image.png" title="image.png" loading="lazy"/></p><p>甚至你也可以引入多AI智能体，以及更多的RAG组件！快试试看吧。</p><hr/><h2><strong>四、扩展案例：多Agent RAG</strong></h2><p>为提升复杂问题的覆盖率与响应质量，还可以引入多Agent RAG的架构设计：</p><p><strong>🔧 Agent 分工</strong></p><ul><li><strong>检索Agent</strong>：根据查询内容确定检索工具（本地知识库/网络搜索）。</li><li><strong>Agent A（知识库专家）</strong>：负责本地知识库的高效检索，优先处理结构化、稳定信息。</li><li><strong>Agent B（网络搜索专家）</strong>：执行网页搜索、数据内容提取，并写入本地。</li></ul><p>检索完成后，所有结果统一送入LLM生成响应，保证语言质量与上下文一致性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458197" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>MCP网络搜索工具定义与注册</strong></p><pre><code># MCP-Search Web and Save Local
mcp_client1 = lazyllm.tools.MCPClient(command_or_url="python", args=["-m", "mcp_server_fetch"],)
search_agent = CustomReactAgent(llm=lazyllm.OnlineChatModule(source="sensenova", stream=False),
    stream=False, custom_prompt=search_prompt, tools=mcp_client1.get_tools())

@fc_register("tool")
def search_web(query: str):
    '''
    Perform targeted web content retrieval using a combination of search terms and URL.
    This tool processes both natural language requests and specific webpage addresses 
    to locate relevant online information.
    Args:
        query (str): Combined input containing search keywords and/or target URL 
                   (e.g., "AI news from https://example.com/tech-updates")
    '''
    query += search_prompt
    res = search_agent(query)
    return res
</code></pre><p><strong>RAG工具定义与注册+应用编排</strong></p><pre><code># RAG-Retriever
documents = Document(dataset_path='path/to/kb', manager=False)
documents.add_reader('*.json', process_json)
with pipeline() as ppl_rag:
    ppl_rag.retriever = Retriever(documents, Document.CoarseChunk,
        similarity="bm25", topk=1, output_format='content', join='='*20)
@fc_register("tool")
def search_knowledge_base(query: str):
    '''
    Get info from knowledge base in a given query.
    Args:
        query (str): The query for search knowledge base.
    '''
    res = ppl_rag(query)
    return res

# Agentic-RAG:
tools = ['search_knowledge_base', 'search_web']
with pipeline() as ppl:
    ppl.retriever = CustomReactAgent(lazyllm.OnlineChatModule(stream=False), tools, agent_prompt, stream=False)
    ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=ppl.input)
    ppl.llm = lazyllm.OnlineChatModule(stream=False).prompt(lazyllm.ChatPrompter(gen_prompt, extra_keys=["context_str"]))
# Launch: Web-UI
lazyllm.WebModule(ppl, port=range(23467, 24000), stream=True).start().wait()
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458198" alt="image.png" title="image.png" loading="lazy"/></p><hr/><h2><strong>五、多模态Agentic RAG论文系统</strong></h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458199" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>（一）配置两个MCP工具及Agent</strong></h3><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=SNk8H%2B%2BEASX%2FTxSHPVJWjQ%3D%3D.7HiAjb7Iqd6%2BSyORPPvTo4ohkJD8aDjgScryM3ttMDpnanqqiXKL6Mfbrtu%2FKiKEr5%2BmjsbLUEpLyWGuUPZcCjpB46nRBRMkjEQSXBNa8kwN1IhWaAeirDHipcoiCC7qbc%2FwgU8n2YeEpfnmbqV4VA%3D%3D" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/courseware\_codes/chapter18/mcp\_agent.py）</a></p><pre><code>import json
import lazyllm
from lazyllm import ReactAgent
mcp_client1 = lazyllm.tools.MCPClient(
    command_or_url="python",
    args=["-m", "mcp_simple_arxiv"],
)
mcp_client2 = lazyllm.tools.MCPClient(
    command_or_url="python",
    args=["-m", "mcp_server_calculator"],
)
llm = lazyllm.OnlineChatModule(stream=False)
paper_agent = ReactAgent(llm, mcp_client1.get_tools(), return_trace=True)
calculator_agent = ReactAgent(llm, mcp_client2.get_tools(), return_trace=True)
</code></pre><p>环境中需提前安装好两个工具：</p><pre><code>pip install mcp-simple-arxiv
pip install mcp-server-calculator
</code></pre><h3><strong>（二）应用编排</strong></h3><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=u5xx373bAdQR7RJD2xSOgQ%3D%3D.7YeDD03CSy3bkvUw9789UW%2Fc0c7LMj5bz0Mh4FNDG0%2FAe3u785Xlb8pNfkT3dQXdtpBOTMnZuUPX3ND7Eg8J8qGSCteQPr0Z3jIiy5qs3%2BigigEaH87BaKOjKPp75EWyBBZ%2BG1%2BcRBEV7W5aSkqbJeXulbVJd%2FQn0F0%2Bzkimmuz%2BbpXjNeTYNvEvvjR05t2P1BzpjRFnB5N985vpOKPNvQ%3D%3D" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/courseware\_codes/chapter18/paper\_assistant_multimodal.py#L25）</a></p><pre><code># 构建 rag 工作流和统计分析工作流
rag_ppl = build_paper_rag()
sql_ppl = build_statistical_agent()
# 搭建具备知识问答和统计问答能力的主工作流
def build_paper_assistant():
    llm = OnlineChatModule(source='qwen', stream=False)
    vqa = lazyllm.OnlineChatModule(source="sensenova",\
        model="SenseNova-V6-Turbo").prompt(lazyllm.ChatPrompter(gen_prompt))
    with pipeline() as ppl:
        ppl.ifvqa = lazyllm.ifs(
            lambda x: x.startswith('&lt;lazyllm-query&gt;'),
            lambda x: vqa(x), lambda x:x)
        with IntentClassifier(llm) as ppl.ic:
            ppl.ic.case["论文问答", rag_ppl]
            ppl.ic.case["统计问答", sql_ppl]
            ppl.ic.case["计算器", calculator_agent]
            ppl.ic.case["网页最新论文搜索", paper_agent]
    return ppl
if __name__ == "__main__":
    main_ppl = build_paper_assistant()
    lazyllm.WebModule(main_ppl, port=23459, static_paths="./images", encode_files=True).start().wait()
</code></pre><hr/><p>更多技术内容，欢迎移步 "LazyLLM" 讨论！</p>]]></description></item><item>    <title><![CDATA[怎么选择一套真正落地的工业解决方案？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047458566</link>    <guid>https://segmentfault.com/a/1190000047458566</guid>    <pubDate>2025-12-08 17:10:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在工业4.0的风暴中心，一场静默而深刻的革命正重塑制造业的骨骼与神经——这便是“工业解决方案”所承载的使命：它不是零散技术的堆砌，不是自动化设备的简单叠加，而是一场以数据为血脉、以智能为大脑、以场景为肌理的系统性重构。当传统制造还在依赖经验与纸质工单挣扎时，真正的工业解决方案，早已在无声中接管了决策权，将模糊的工艺、割裂的流程、沉睡的数据，熔铸成一个能感知、会思考、可进化的有机体。<br/>广域铭岛，正是这场重构的先锋。它不满足于做一名设备供应商，而是以Geega工业互联网平台为中枢，构建起一个贯通“感知—决策—执行—进化”的智能生态。在冲压车间，模具的每一次冲压不再是孤立的机械动作，而是被GQCM智能管理APP实时记录、分析、预警，维修工单如神经信号般自动触发，排产系统随之轻盈调整——这不再是“管理”，而是“预判”。在焊接线上，3000多个焊点的数据流如星河倾泻，数字孪生技术在虚拟空间中复刻每一处微小的形变，AI智能体在20分钟内锁定异常根源，而过去，这需要两名工程师耗时两小时在噪音与油污中盲寻。这不是效率的提升，是时间的压缩，是经验的数字化涅槃。<br/>工业解决方案的真正力量，在于它消弭了孤岛。仓储不再是冷冰冰的货架堆叠，而是与生产计划、供应链波动、甚至市场预测深度耦合的动态生命体。广域铭岛在吉利工厂打造的智能立体仓库，物料自动预约、智能分拣、精准配送，形成闭环，供应链响应速度飙升50%。当缺料警报响起，12类智能体在五分钟内协同生成应急方案——采购、排产、物流如交响乐团般精准合奏，而传统模式下，这可能是一场持续数日的混乱争吵。库存周转周期缩短一半，流动资金释放上亿，这不是财务报表上的数字，是企业呼吸的节奏被重新校准。<br/>更令人震撼的是，它让“不可言传”的技艺得以传承。那些老师傅眼中“手感”、“火候”、“听声辨位”的绝技，被拆解、标准化、封装为可复用的“智能体配方”。当新车型上线，“工艺大师Agent”在十五分钟内生成标准作业程序，人力成本骤降四成——这不是替代，是升华。知识，终于挣脱了人脑的局限，成为可迭代、可共享、可进化的公共资产。在电池涂布工艺中，这种封装使能量密度提升5%，年创效益过亿；在视觉质检中，人工录入被数千倍效率的AI取代，错误率归零。<br/>而这一切，正从“优化”走向“原生”。广域铭岛提出的“AI原生工厂”，不是给工厂装上AI，而是让工厂从诞生之初就由AI驱动。感知型智能体如神经末梢捕捉每一丝温度波动，决策型智能体如大脑权衡能耗、效率与质量的三角博弈，执行型智能体如四肢精准执行指令——三者在统一知识图谱下形成闭环，自主进化。仓储系统不再被动响应，而是预测需求、预判瓶颈；AGV不再按固定路径搬运，而是动态规划最优轨迹，空驶率骤降40%；碳排放，也不再是成本项，而是被算法主动优化的指标，仓储环节能耗降低15%，绿色，成为智能的副产品。<br/>工业解决方案的终极形态，是让机器学会“理解”——理解工艺的逻辑、理解人的意图、理解市场的脉动。它不再只是“更快、更准、更省”，而是让整个制造系统具备了生命般的韧性与智慧。广域铭岛的实践昭示：当数据成为语言，算法成为思维，平台成为土壤，工业便从冰冷的流水线，蜕变为一个能自我修复、自我优化、自我进化的智能生命体。这，才是智能制造的真正答案——不是技术的胜利，而是知识的重生。</p>]]></description></item><item>    <title><![CDATA[怎么实现模具管理的智能化转型？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047458570</link>    <guid>https://segmentfault.com/a/1190000047458570</guid>    <pubDate>2025-12-08 17:09:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在现代制造业的精密肌理中，模具——这沉默而坚韧的“工业之母”，早已超越了工具的范畴，蜕变为承载效率、质量与成本命运的智能资产。然而，曾几何时，它被囚禁于纸本台账的尘埃里，被淹没在经验主义的迷雾中：保养靠记忆，维修凭直觉，状态如盲盒，故障如天灾。直到数据的潮水冲破了这道陈旧的堤坝，模具管理才真正从一场被动的救火，升华为一场主动的精密交响。<br/>这场变革的引擎，正由广域铭岛等先锋企业驱动。他们不再满足于简单的台账登记或流程固化，而是以GQCM模具智能管理APP为利刃，剖开了传统模式的层层桎梏。系统不再只是记录“何时保养”，而是追问“为何需要保养”——它通过多源数据融合引擎，将压机PLC的冲次脉冲、ANDON系统的停机警报、材料硬度的微观反馈、乃至历史维修知识图谱中的故障模式，编织成一张动态的神经网络。每一副模具，从此拥有了自己的“健康指数”（EHI），一个由算法持续计算、不断演化的生命体征。当某副用于生产黑色高光件的注塑模具因表面易划伤，其EHI值悄然攀升，系统便自动将保养周期从30天压缩至15天；当高强度钢模具因回弹应力累积，系统便提前预警导柱磨损风险，推送“更换+优化润滑”的复合方案——这不是机械的提醒，而是基于深度学习的预判，是数据在无声中为生产决策注入的智慧。<br/>广域铭岛的实践，早已超越了单点突破。在领克汽车成都工厂，故障响应时间从令人窒息的两小时，骤降至15分钟；润滑剂消耗下降18%，备件库存周转率飙升40%——这些数字背后，是系统对模具全生命周期的精准掌控：从设计图纸的电子归档，到每一次使用与维修的DNA级记录；从RFID标签赋予的实时定位，到与MES、ERP系统无缝贯通的全局协同。模具不再是孤立的零件，而成为生产网络中的智能节点，其状态、位置、寿命、历史，皆在数字孪生的镜像中清晰映射。当某批次产品出现尺寸偏差，追溯不再是大海捞针，而是点击几下，便能还原三天前某次保养中导柱润滑不足的微小裂痕——知识，从此不再随老技师的退休而流失，而是沉淀为可复用、可迭代的企业核心资产。<br/>这不仅是技术的胜利，更是工业文明范式的跃迁。传统模具管理依赖的是个体经验的碎片，而智能模具管理构建的，是一套自我进化、自我优化的智能体矩阵。它让保养从“周期性仪式”变为“健康性干预”，让维修从“成本黑洞”转为“价值投资”，让库存从“资金压舱石”化为“精准弹药库”。在家电、工程机械等多元场景中，这套系统正以惊人的适应力复制成功：大型覆盖件模具寿命从8万次跃升至12万次，非计划停机率断崖式下跌，产品质量稳定性如磐石般稳固。<br/>未来，这条路径将更深地融入5G的脉动、边缘计算的神经末梢与AI预测性维护的幽深蓝海。嵌入式传感器将实时监测温度、压力与振动，云端AI将模拟模具在极端工况下的应力演化，数字孪生模型将成为决策者的虚拟沙盘。广域铭岛所代表的，不是某个软件的更新，而是一种全新的工业哲学——模具，不应是被消耗的消耗品，而应是被理解、被预测、被珍视的智能资产。当每一套模具都拥有自己的数字灵魂，当每一次保养都源于数据的低语而非人力的猜测，制造业的效率边界，便被重新定义。这，正是智能模具管理的终极使命：让沉默的钢铁，学会说话；让混沌的生产，重获秩序；让工业的未来，由数据与智能共同书写。</p>]]></description></item><item>    <title><![CDATA[工期滞后、协同低效？追踪复杂工程的痛点破解方案来了 Zoey的笔记本 ]]></title>    <link>https://segmentfault.com/a/1190000047458575</link>    <guid>https://segmentfault.com/a/1190000047458575</guid>    <pubDate>2025-12-08 17:08:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、复杂工程追踪的核心痛点：为什么常规工具根本不够用？</h2><p>超大型基建、EPC 总承包、地铁管廊、商业综合体等复杂工程，往往面临 “工期紧、接口多、资源散、变更频” 的困境：多级计划脱节导致进度滞后，BIM 模型与现场施工脱节引发返工，资源调度失衡造成窝工或短缺，成本与进度不同步导致超支风险，跨团队协同低效引发信息壁垒…… 这些痛点绝非简单的表格或基础项目工具能解决，必须依赖 “计划 - 协同 - 可视化 - 数据” 深度融合的专业工具体系。</p><h2>二、按场景精准选型：不同复杂工程的 “工具最优解”</h2><p>复杂工程的核心需求差异显著，盲目选型只会适得其反。以下结合项目类型、规模与核心诉求，整理出高适配性工具组合，覆盖从超大型基建到中小项目的全场景：</p><p>对于轻量化协同项目（如咨询、设计、中小施工队），若追求快速上线、团队规模小且无需复杂配置，板栗看板/ 蓝燕云 更合适，其表格化搭建逻辑、移动端填报功能及低代码自动化特性，能以最低成本实现高效协同。</p><p>中大型通用工程（如政府基建、国企项目）适配通用场景，团队多熟悉 Office 操作且需快速输出报表，Microsoft Project + Power BI 组合性价比更高，前者支持甘特图 / CPM 关键路径管理，后者可搭建自定义仪表盘，搭配 Office 生态兼容优势，能快速落地项目追踪需求。</p><p>针对超大型基建 / EPC 项目（如高铁、核电、跨海大桥），作业数通常超万、涉及跨地域多标段、资源投入超千人且接口复杂度极高，首选Oracle Primavera P6/Cloud，其支持十万级作业承载、多级计划嵌套、资源负荷优化、挣值管理（EVM）及多项目组合管控，能轻松应对大规模项目的复杂调度需求。</p><p>企业级重资产项目（如大型集团、工厂化建造）若有强财务管控需求、需对接集团 ERP 系统且涉及重资产投入，SAP S/4HANA Cloud 是核心选择，它能实现项目财务与 ERP 深度集成，构建预算 - 支付闭环，确保全流程可溯源，满足集团化管控要求。</p><p>对于建筑 / BIM 施工项目（如房建、商业综合体、装配式建筑），若以 BIM 为核心、需可视化推演施工过程且设计与施工衔接紧密，Autodesk Construction Cloud（Build） 是最优选择，它具备 BIM+4D 进度模拟、冲突检测、设计 - 施工一体化及现场协同能力，可有效减少设计与施工脱节引发的返工。</p><p>而市政 / 地铁 / BIM5D 项目（如地铁、综合管廊、智慧工地），因地下工程多、现场数据密集且需动态管控质量与安全，广联达数字项目平台 / BIM5D 更适配，其能实现 BIM 与进度、成本、物资的全链路拉通，结合数字孪生与 IoT 实时监控，让现场管理更精准高效。</p><h2>三、复杂工程追踪的 “核心能力标配”：少了这些等于白选</h2><p>无论选择哪种工具，必须覆盖以下核心能力，才能真正解决复杂工程的管理痛点：</p><ol><li>多级计划联动：支持 “企业 - 项目 - 标段 - 作业” 四层嵌套，通过前锋线实时追踪进度偏差，提前预警关键路径风险，避免 “上层计划好看、下层执行混乱”。</li><li>4D BIM 进度模拟：将三维 BIM 模型与进度计划绑定，可视化推演施工过程，提前检测时空冲突（如管线碰撞、场地占用矛盾），减少返工成本。</li><li>资源负荷优化：实现千人级人力曲线、设备 / 材料动态调度，精准匹配资源需求与供给，避免 “窝工浪费” 或 “资源短缺拖工期”。</li><li>挣值管理（EVM）：通过 BCWP（已完工作预算费用）、BCWS（计划工作预算费用）、ACWP（已完工作实际费用）三值对比，量化进度与成本绩效，及时纠偏超支风险。</li><li>合同与变更闭环：覆盖 “变更单 - 签证 - 索赔 - 支付” 全流程，每一步可追溯、可关联，防止变更失控导致的成本超支与纠纷。</li><li>现场协同与 IoT 集成：支持移动端打卡、拍照、数据填报，对接 IoT 设备（如塔吊监控、扬尘传感器）实时上传现场数据，形成 “现场 - 后台” 无缝闭环，避免信息滞后。</li></ol><h2>四、落地不踩坑：复杂工程工具实施的四步走策略</h2><p>选对工具只是第一步，科学的实施路径才能让工具发挥实际价值，建议按以下步骤推进：</p><ol><li>明确范围与指标：先界定 WBS 工作分解结构层级、关键路径、里程碑节点、资源约束、成本基准及变更审批流程，避免 “工具功能用不全、核心需求没覆盖”。</li><li>工具组合搭配：超大型工程建议 “Primavera P6/Cloud（计划 + 资源）+ Build / 广联达（BIM + 现场）” 组合；中大型工程用 “Project+Power BI” 快速落地；中小项目直接用轻量化工具启动，避免过度配置。</li><li>数据打通是关键：统一项目编码规范，对接 BIM 模型、IoT 设备、合同系统与财务系统，实现 “数出一源、实时共享”，杜绝 “数据孤岛” 导致的决策失误。</li><li>培训与迭代优化：重点培训计划编制、EVM 分析、4D 模拟与移动端操作，按周 / 月复盘工具使用效果，优化流程与配置，让工具持续适配项目推进需求。</li></ol><h2>五、快速选型口诀：30 秒锁定适合你的工具</h2><p>· 轻量化、快上线 → 板栗看板 / 蓝燕云；</p><p>· 通用场景、Office 友好 → Microsoft Project + Power BI；</p><p>· 超大型、多专业、接口杂 → Primavera P6/Cloud + BIM 工具；</p><p>· BIM 为核心、要可视化 → Autodesk Construction Cloud / 广联达 BIM5D；</p><p>· 需 ERP 集成、强财务管控 → SAP S/4HANA Cloud；</p><h2>结语：复杂工程追踪，工具是手段，闭环是核心</h2><p>追踪复杂工程的本质，是实现 “计划 - 进度 - 资源 - 成本 - 现场” 的全链路数字化闭环。选对工具能让管理效率翻倍，但更重要的是结合项目实际场景，打通数据、优化流程、落地执行。建议先选择一个标段试点，跑通多级计划、4D 模拟与 EVM 闭环，再逐步推广至全项目，避免 “一步到位” 的实施风险。</p><p>如果你的项目有明确的类型（如地铁 / 房建 / EPC）、规模（作业数 / 人数）、是否依赖 BIM 及预算限制，可进一步定制 “一页式选型与实施清单”，让工具落地更精准、更高效。</p>]]></description></item><item>    <title><![CDATA[UModel 查询：驯服“可观测性混乱”，阿里云的图模型建模利器！ 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047458586</link>    <guid>https://segmentfault.com/a/1190000047458586</guid>    <pubDate>2025-12-08 17:07:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><a href="https://www.bilibili.com/video/BV1AWSxBqEtQ/" target="_blank">点击此处，立即查看视频课程！</a></p><h2>背景</h2><p>想象一下，你站在一个巨大的图书馆里，这里有成千上万本书，但每本书的目录都散落在不同的房间里，而且每间房间的索引方式都不一样。当你想要找一本关于“服务调用”的书时，你需要在 APM 房间、K8s 房间、云资源房间之间来回奔波，还要记住每个房间不同的查找规则...</p><p>这就是很多企业在可观测性领域面临的真实困境。而 UModel 就像是为这个混乱的图书馆建立了一套统一的“智能管理系统”，让你能够轻松探索和理解整个知识图谱的结构。</p><h3>1.1 UModel 是什么</h3><p>UModel 是一种基于图模型的可观测数据建模方法，旨在解决企业级环境中可观测数据采集、组织和利用的核心挑战。UModel 采用 Node（节点）和 Link（边）组成的图结构来描述 IT 世界，通过标准化的数据建模方式，实现可观测数据的统一表示、存储解耦和智能分析。</p><p>作为阿里云可观测体系的数据建模基础，UModel 为企业提供了一套通用的可观测“交互语言”，让人、程序和 AI 都能够理解和分析可观测数据，从而构建真正的全栈可观测能力。</p><h4>核心概念</h4><p>UModel 采用图论的基本概念，使用 Node（节点）和 Link（边）组成有向图来描述 IT 系统：</p><ul><li>Node（节点）：核心部分为 Set（数据集），表示同类型实体或数据的集合，如 EntitySet（实体集）、MetricSet（指标集）、LogSet（日志集）等；此外还包含数据集的存储类型（Storage），如 SLS、Prometheus、MySQL 等</li><li>Link（关联）：表示 Node 之间的关系，如 EntitySetLink（实体关联）、DataLink（数据关联）、StorageLink（存储关联）等</li><li>Field（字段）：用于约束和描述 Set 和 Link 的属性，包含名称、类型、约束规则、分析特性等 20 多种配置项</li></ul><h3>1.2 UModel 查询是什么</h3><p>UModel 查询是 EntityStore 中用于查询知识图谱元数据的专用查询接口，通过 <code>.umodel </code>查询语法，可以探索 EntitySet 定义、EntitySetLink 关系以及完整的知识图谱结构，为数据建模分析和 Schema 管理提供强大支持。</p><h4>查询目标区分</h4><p>UModel 查询与其他查询类型的区别：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458588" alt="image" title="image"/></p><p>UModel 查询专注于元数据层面的探索，帮助用户理解数据模型的结构和定义，而非具体的运行时数据。</p><h2>UModel 查询</h2><h3>2.1 数据模型</h3><h4>数据结构</h4><p>UModel 查询返回的数据具有固定的五字段结构：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458589" alt="image" title="image" loading="lazy"/></p><p>注意：<code>metadata、schema</code>、<code>spec</code> 是 JSON 格式的 string，需要使用 <code>json_extract_scalar</code> 函数进行提取。</p><h4>数据示例</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458590" alt="image" title="image" loading="lazy"/></p><h3>2.2 查询语法</h3><h4>基础查询语法</h4><pre><code>-- 基础查询格式
.umodel | [SPL操作...]
-- 带限制条件的查询
.umodel | where &lt;condition&gt; | limit &lt;count&gt;</code></pre><h4>核心查询模式</h4><h5>1. List 场景 - 列表查询</h5><p>查询所有 UModel 数据：</p><pre><code>-- 列出所有umodel数据（不建议使用）
.umodel
-- 带分页的查询
.umodel | limit 0, 10</code></pre><p>按类型过滤：</p><pre><code>-- 查询所有EntitySet定义
.umodel | where kind = 'entity_set' | limit 0, 10
-- 查询所有EntitySetLink定义
.umodel | where kind = 'entity_set_link' | limit 0, 10
-- 查询所有边类型（关系定义）
.umodel | where __type__ = 'link' | limit 0, 10
-- 查询所有节点类型（实体定义）
.umodel | where __type__ = 'node' | limit 0, 10</code></pre><p>按属性过滤：</p><pre><code>-- 查询特定名称的实体定义
.umodel | where json_extract_scalar(metadata, '$.name') = 'acs.ecs.instance' | limit 0, 10
-- 查询特定域的所有定义
.umodel | where json_extract_scalar(metadata, '$.domain') = 'apm' | limit 0, 10
-- 查询多个域的定义
.umodel | where json_extract_scalar(metadata, '$.domain') in ('acs', 'apm', 'k8s') | limit 0, 10</code></pre><h5>2. 图计算场景 - 关系分析</h5><p>UModel 支持基于元数据的图计算，用于分析 EntitySet 之间的关系：</p><p>基础图查询语法：</p><pre><code>.umodel | graph-match &lt;path&gt; project &lt;output&gt;</code></pre><p>基础概念：</p><p>在图查询中，有两个关键性的图概念：</p><p>节点类型，即 label 信息，在 UModel 的元数据图查询中，为 <code>&lt;domain&gt;@&lt;kind&gt;</code>，例如 <code>apm@entity_set</code><br/>节点 ID，即 <code>__entity_id__</code> 信息，在 UModel 的元数据图查询中，为 <code>kind::domain::name</code>，例如 <code>entity_set::apm::apm.service</code></p><p>图查询路径（PATH）使用 ASCII 字符描述关系方向：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458591" alt="image" title="image" loading="lazy"/></p><p>查询 EntitySet 的邻居关系：</p><pre><code>-- 查询特定EntitySet的所有关联关系
.umodel 
| graph-match (s:"acs@entity_set" {__entity_id__: 'entity_set::acs::acs.ecs.instance'})
              -[e]-(d) 
  project s, e, d | limit 0, 10</code></pre><p>方向性关系查询：</p><pre><code>-- 查询指向某个EntitySet的关系
.umodel 
| graph-match (s:"acs@entity_set" {__entity_id__: 'entity_set::acs::acs.ecs.instance'})
              &lt;--(d) 
  project s, d | limit 0, 10
-- 查询从某个EntitySet出发的关系  
.umodel 
| graph-match (s:"acs@entity_set" {__entity_id__: 'entity_set::acs::acs.ack.cluster'})
              --&gt;(d) 
  project s, d | limit 0, 10</code></pre><h3>2.3 高级查询功能</h3><h4>JSON 路径提取</h4><p>由于 UModel 数据采用 JSON 结构存储，需要使用 JSON 函数进行字段提取：</p><pre><code>-- 提取基础信息
.umodel 
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    entity_domain = json_extract_scalar(metadata, '$.domain'),
    entity_description = json_extract_scalar(metadata, '$.description.zh_cn')
| project entity_name, entity_domain, entity_description | limit 0, 100</code></pre><h4>复杂条件筛选</h4><pre><code>-- 多条件组合查询
.umodel 
| where kind = 'entity_set'
  and json_extract_scalar(metadata, '$.domain') in ('apm', 'k8s')
  and json_array_length(json_extract(spec, '$.fields')) &gt; 5
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    field_count = json_array_length(json_extract(spec, '$.fields'))
| sort field_count desc
| limit 20</code></pre><h4>聚合分析</h4><pre><code>-- 按域统计EntitySet数量
.umodel 
| where kind = 'entity_set'
| extend domain = json_extract_scalar(metadata, '$.domain')
| stats entity_count = count() by domain
| sort entity_count desc</code></pre><h3>2.4 性能优化建议</h3><h4>使用精确过滤</h4><pre><code>-- 优化前：范围过大
.umodel | where json_extract_scalar(metadata, '$.name') like '%service%'
-- 优化后：精确匹配
.umodel | where kind = 'entity_set' 
  and json_extract_scalar(metadata, '$.domain') = 'apm'
  and json_extract_scalar(metadata, '$.name') = 'apm.service'</code></pre><h4>过滤前置</h4><pre><code>-- 优化前：后期过滤
.umodel 
| extend name = json_extract_scalar(metadata, '$.name')
| where name = 'apm.service'
-- 优化后：过滤前置
.umodel 
| where json_extract_scalar(metadata, '$.name') = 'apm.service'
| extend name = json_extract_scalar(metadata, '$.name')</code></pre><h4>图查询优化</h4><pre><code>-- 优化前：全图搜索
.umodel | graph-match (s)-[e]-(d) project s, e, d
-- 优化后：指定起始点
.umodel 
| graph-match (s:"apm@entity_set" {__entity_id__: 'entity_set::apm::apm.service'})
              -[e]-(d) 
  project s, e, d</code></pre><h2>UModel 查询具体应用场景</h2><p>UModel 查询在实际应用中能够解决多种场景下的问题，为数据建模、Schema 管理和知识图谱分析提供强大支持。</p><h3>3.1 Schema 探索与发现</h3><h4>场景描述</h4><p>在大型可观测性系统中，可能存在数百个 EntitySet 定义，分布在不同的域（domain）中。用户需要快速了解系统中定义了哪些实体类型，以及它们的基本信息。</p><h4>应用示例</h4><p>探索所有实体类型：</p><pre><code>-- 列出所有EntitySet及其基本信息
.umodel 
| where kind = 'entity_set'
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    entity_domain = json_extract_scalar(metadata, '$.domain'),
    description = json_extract_scalar(metadata, '$.description.zh_cn')
| project entity_name, entity_domain, description
| sort entity_domain, entity_name
| limit 0, 100</code></pre><p>按域分类查看：</p><pre><code>-- 查看特定域（如APM）下的所有实体定义
.umodel 
| where kind = 'entity_set' 
  and json_extract_scalar(metadata, '$.domain') = 'apm'
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    description = json_extract_scalar(metadata, '$.short_description.zh_cn')
| project entity_name, description
| limit 0, 50</code></pre><h3>3.2 数据建模分析</h3><h4>场景描述</h4><p>在进行数据建模优化时，需要分析现有 EntitySet 的字段复杂度、主键设计、索引配置等信息，以便识别需要优化的模型。</p><h4>应用示例</h4><p>分析字段复杂度：</p><pre><code>-- 分析各域下EntitySet的字段数量分布
.umodel 
| where kind = 'entity_set'
| extend 
    domain = json_extract_scalar(metadata, '$.domain'),
    entity_name = json_extract_scalar(metadata, '$.name'),
    field_count = json_array_length(json_extract(spec, '$.fields'))
| stats 
    avg_fields = avg(field_count),
    max_fields = max(field_count),
    min_fields = min(field_count),
    entity_count = count()
  by domain
| sort entity_count desc</code></pre><p>查找复杂实体：</p><pre><code>-- 找出字段数量最多的EntitySet（可能需要优化）
.umodel 
| where kind = 'entity_set'
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    domain = json_extract_scalar(metadata, '$.domain'),
    field_count = json_array_length(json_extract(spec, '$.fields'))
| sort field_count desc
| limit 20</code></pre><h3>3.3 关系图谱分析</h3><h4>场景描述</h4><p>理解 EntitySet 之间的关系对于构建完整的知识图谱至关重要。通过图查询可以分析实体间的关联关系，发现数据模型中的依赖和连接。</p><h4>应用示例</h4><p>查询实体的所有关联关系：</p><pre><code>-- 查询某个EntitySet（如apm.service）的所有关联关系
.umodel 
| graph-match (s:"apm@entity_set" {__entity_id__: 'entity_set::apm::apm.service'})
              -[e]-(d) 
  project s, e, d
| limit 0, 50</code></pre><p>分析关系类型分布：</p><pre><code>-- 统计不同关系类型的数量
.umodel 
| where kind = 'entity_set_link'
| extend 
    link_name = json_extract_scalar(metadata, '$.name'),
    link_type = json_extract_scalar(metadata, '$.link_type')
| stats limk_count = count() by link_type
| sort limk_count desc</code></pre><p>查找特定关系：</p><pre><code>-- 查找所有"runs_on"类型的关系定义
.umodel 
| where kind = 'entity_set_link'
  and json_extract_scalar(metadata, '$.link_type') = 'runs_on'
| extend 
    link_name = json_extract_scalar(metadata, '$.name'),
    source = json_extract_scalar(metadata, '$.source'),
    target = json_extract_scalar(metadata, '$.target')
| project link_name, source, target</code></pre><h3>3.4 元数据质量检查</h3><h4>场景描述</h4><p>确保 UModel 元数据的完整性和一致性，检查缺失的描述、未定义的字段等问题。</p><h4>应用示例</h4><p>检查缺失描述的 EntitySet：</p><pre><code>-- 找出没有中文描述的EntitySet
.umodel 
| where kind = 'entity_set'
  and (json_extract_scalar(metadata, '$.description.zh_cn') = '' 
       or json_extract_scalar(metadata, '$.description.zh_cn') is null)
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    domain = json_extract_scalar(metadata, '$.domain')
| project entity_name, domain</code></pre><p>验证字段定义完整性：</p><pre><code>-- 检查没有定义字段的EntitySet
.umodel 
| where kind = 'entity_set'
  and (json_extract(spec, '$.fields') is null 
       or json_array_length(json_extract(spec, '$.fields')) = 0)
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    domain = json_extract_scalar(metadata, '$.domain')
| project entity_name, domain</code></pre><h3>3.5 跨域关联分析</h3><h4>场景描述</h4><p>在复杂的可观测性系统中，不同域（如 APM、K8s、云资源）的实体可能存在关联关系。通过 UModel 查询可以分析这些跨域的关联模式。</p><h4>应用示例</h4><p>查找跨域关系：</p><pre><code>-- 查找连接不同域的EntitySetLink
.umodel 
| where kind = 'entity_set_link'
| extend 
    link_name = json_extract_scalar(metadata, '$.name'),
    source_domain = json_extract_scalar(spec, '$.src.domain'),
    target_domain = json_extract_scalar(spec, '$.dest.domain')
| where source_domain != target_domain
| project link_name, source_domain, target_domain
| limit 0, 50</code></pre><p>分析域间连接度：</p><pre><code>-- 统计各域之间的连接关系数量
.umodel 
| where kind = 'entity_set_link'
| extend 
    source_domain = json_extract_scalar(spec, '$.src.domain'),
    target_domain = json_extract_scalar(spec, '$.dest.domain')
| stats count = count() by source_domain, target_domain
| sort count desc</code></pre><h3>3.6 版本与演进分析</h3><h4>场景描述</h4><p>UModel Schema 会随着业务发展而演进，需要跟踪 Schema 的版本变化和演进历史。</p><h4>应用示例</h4><p>查看 Schema 版本信息：</p><pre><code>-- 查看所有EntitySet的Schema版本
.umodel 
| where kind = 'entity_set'
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    schema_version = json_extract_scalar(schema, '$.version'),
    schema_url = json_extract_scalar(schema, '$.url')
| project entity_name, schema_version, schema_url
| limit 0, 100</code></pre><h3>3.7 快速定位与检索</h3><h4>场景描述</h4><p>在大量元数据中快速找到特定的 EntitySet 或关系定义，支持模糊匹配和精确查询。</p><h4>应用示例</h4><p>按名称模糊搜索：</p><pre><code>-- 搜索包含"service"的EntitySet
.umodel 
| where kind = 'entity_set'
  and json_extract_scalar(metadata, '$.name') like '%service%'
| extend 
    entity_name = json_extract_scalar(metadata, '$.name'),
    domain = json_extract_scalar(metadata, '$.domain')
| project entity_name, domain
| limit 0, 20</code></pre><p>精确查找特定实体：</p><pre><code>-- 精确查找特定EntitySet的完整定义
.umodel 
| where json_extract_scalar(metadata, '$.name') = 'apm.service'
| limit 1</code></pre><h2>总结</h2><p>UModel 查询作为 EntityStore 中专门用于查询知识图谱元数据的接口，为可观测性数据建模提供了强大的支持能力。通过 UModel 查询可以：</p><ol><li>探索 Schema 结构：快速了解系统中定义的所有实体类型和关系类型</li><li>分析数据模型：深入分析 EntitySet 的字段设计、主键配置、复杂度等</li><li>构建关系图谱：通过图查询分析实体间的关联关系，理解知识图谱的拓扑结构</li><li>质量检查：验证元数据的完整性和一致性</li><li>跨域分析：分析不同域之间的关联模式</li><li>快速检索：在大量元数据中快速定位目标定义</li></ol><p>这些能力使得 UModel 查询成为数据建模分析、Schema 管理和知识图谱探索的不可或缺的工具，为构建和维护高质量的可观测性数据模型提供了坚实的基础。</p><p>点击<a href="https://www.bilibili.com/video/BV1AWSxBqEtQ/" target="_blank">此处</a>查看视频演示。</p>]]></description></item><item>    <title><![CDATA[分享一下最近的面试题 王中阳讲编程 ]]></title>    <link>https://segmentfault.com/a/1190000047458592</link>    <guid>https://segmentfault.com/a/1190000047458592</guid>    <pubDate>2025-12-08 17:06:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>分享一下训练营内部学员最近的面经，希望对大家有帮助。</p><h2>1 供应链跨境电商二面 外包</h2><ol><li>自我介绍</li><li>询问 一般来说 会从哪些方面去code review</li><li>空结构体用过吗？什么作用？为什么会有这个作用?</li><li>询问 你怎么去设计一个10万QPS的系统。（redis单飞是什么）</li><li>多大的服务器 或者说怎么配置一个服务器 能撑起10W的QPS</li><li>Mysql 覆盖索引、联合索引的概念</li><li>唯一索引和二级索引（非唯一索引）在插入读写效率上有什么区别吗？</li><li>一个能如期交付且客户满意的项目，你认为应该有哪些要素？</li><li>你认为你过去的项目中 你遇到的最大难题是什么？</li><li>一个项目中 一个功能模块 或者说整体的架构设计 该怎么做 有了解过吗？</li><li>在你之前用过的这么多的框架中，各自工程框架的优缺点都有哪些？</li></ol><h2>2 回想科技（剧本杀 潮玩） 千岛APP 业务组 正岗</h2><ol><li>自我介绍</li><li>询问项目内容，比如AI项目 相关的架构是怎么选择的？</li><li>milvus的索引了解吗？怎么选择的？</li><li>RAG与大模型之间的一个业务交互逻辑</li><li>如果让你去优化这个AI项目的话 接下来你会从哪些角度去优化AI的调用？</li><li>电商系统中，如果用户出现退货操作，你这个业务逻辑会去怎么处理？</li><li>你在订单与支付的交互中，是怎么去确保避免重复消费的，以及确保数据一致性？</li><li>你的那个接口优化能展开说说吗？</li><li>你用的事务是第三方框架给的事务？还是mysql本身的事务？</li><li>如果在高并发场景下，保证数据的一致性</li><li>msyql中 行锁 、gap lock、next lock的区别？</li><li>联合索引的使用注意事项</li><li>什么情况下会出现索引下推？</li><li>mysql undolog redolog的区别？</li><li>golang  GC的大概过程</li><li>context包 在业务中会怎么用？一般场景是怎么？</li><li>K8S或者网关的一些研究 了解过吗？</li><li>算法考核</li><li>业务反问</li></ol><h2>3 gate.io web3 区块链</h2><ol><li>自我介绍</li><li>简单介绍AI项目助手的一个业务链路过程</li><li>简单讲讲，你为了增加订单服务的承载能力，做了哪些操作？</li><li>redis缓存存热点商品，详细讲下具体的设计</li><li>rabbitMQ的 失败重试处理</li><li>消息队列的防丢失、与重复处理</li><li>context超时取消 怎么做比较合适？</li><li>context怎么去避免泄漏 或者优雅取消？（捕获panic错误）</li><li>golang MAP的底层实现 包括扩容</li><li>有线程安全的Map吗？如果你来操作 你如何让一个map变线程安全？</li><li>channel的实现，以及关闭channel时出现的问题</li><li>GMP机制介绍</li><li>waitgroup用过吗？他的使用要注意什么？</li><li>golang GC的机制</li><li>什么场景下 Golang的GC压力更大</li><li>遇上GC的话 GMP的调度机制 会如何处理？</li><li>一条SQL语句的执行过程</li><li>索引的一个查询流程</li><li>什么情况下 索引不需要回表？</li><li>讲一讲Mysql有哪些锁的种类。介绍其中几种（介绍的乐观 悲观 意向）</li><li>可重复读级别会有什么问题，怎么解决？</li><li>你作为一个团队新人，会如何上手业务</li><li>业务反问</li></ol><h2>4 百度千帆 外包</h2><ol><li>介绍项目经历（把之前的大部分项目从背景、架构、技术选型 、负责模块 、业务逻辑 都讲了一遍）</li><li>你项目中遇到的最大困难是什么？怎么解决？</li><li>mysql常见的引擎有哪些？区别是什么?</li><li>MYSQL出现慢查询怎么排查？怎么优化？</li><li>哪些场景导致索引失效？</li><li>分布式锁有哪几种实现方式？区别又有哪些？</li><li>简单问一下项目的部署上线流程？</li><li>云服务的管理、K8S有了解吗？</li><li>docker的常用命令？</li><li>channel有缓没缓区别?</li><li>select如何监听多个channel？多个channel都传值的情况下 怎么处理?</li><li>context的作用和场景是什么？</li><li>有无了解过golang 最新版本的一些特性？</li><li>golang怎么做性能分析？</li><li>算法题，启动100个协程 要全部执行完，但是同时只有10个能执行</li></ol><h2>5 gateio 二面 web3 正岗</h2><ol><li>自我介绍</li><li>项目介绍</li><li>扩展提问 你的项目从需求到落地的一个过程怎么说？</li><li>高并发场景下，设计一个简单的计数器，你怎么去控制并发相关的问题？</li><li>限流器相关设计过吗？有哪些设计思路？</li><li>一个服务如果内存突然升高，怎么排查？</li><li>内存泄漏一般会和哪些场景有关？</li><li>一个新服务怎么实现优雅关闭？优雅关闭的作用是什么？</li><li>假如一个服务需要更新，但是线上更新后 发现新配置没生效，你会怎么排查？</li><li>mysql的binlog redolog undolog什么作用？</li><li>一个订单表很大，千万级别，要加一个新的字段 允许null、none ,那么对这个表的读写性能有什么影响？</li><li>假如一个表的字段 一个是var 一个是varchar 两者区别？</li><li>redis事务怎么实现的？支持回滚吗？</li><li>redis key的长度限制</li><li>一个超大redis KEY的风险 怎么解决？</li><li>rabbitMQ的 exchange有哪些类型？</li><li>rabbitMQ的确认机制与原理</li><li>rabbitMQ的镜像队列</li><li>https比http更安全，为什么黑客更容易攻击https</li><li>业务反问</li></ol><h2>6 极豆车联网 智能座舱 外包</h2><ol><li>自我介绍</li><li>channel 主要用在哪些场景上？</li><li>goframe框架与Gozero框架你觉得他们的区别在哪？</li><li>对消息队列的理解或者说它的作用 讲一讲？</li><li>RAG的流程，文档清洗的一些细节？</li><li>goroutine泄露的场景与避免</li><li>超时业务怎么处理？</li><li>介绍一下你的电商项目的整体框架？</li><li>ES主要在你们项目中担当什么作用？</li><li>谈一谈你的接口优化？</li><li>你们的超卖遇到过吗？具体怎么处理的？</li><li>redis的热点商品，你们是怎么去做缓存和更新的？</li><li>定时任务你们一般怎么实现？golang原生还是第三方的库？</li><li>你们的日志追踪怎么做？</li></ol><h2>7 百度千帆 TOB 外包 二面</h2><ol><li>自我介绍</li><li>为了支持高并发 你们做了哪些设计？</li><li>为什么选择ETCD做分布式锁？</li><li>讲讲你们的rag实现？</li><li>搜索 生成有了 那增强你们考虑过没？</li><li>你们的项目为什么没选择gin 选择了gf 和gozero？</li><li>业务中发现panic 我们怎么去定位？</li><li>写代码的时候 应该从哪些地方 避免出现Panic?</li><li>make和new区别</li><li>设计模型了解过哪些？（单例和工厂）</li><li>k8s常用命令 了解吗？</li><li>项目初期的索引你们会怎么做？</li><li>联合索引什么情况下 有效 什么情况下无效？</li><li>简单算法思路：两个二叉树，判断公共节点？</li><li>简单写个冒泡排序</li><li>(百度最近加班急眼了,正编加班，但是有外包不配合 到点就走开始旁敲侧击面试人了) 你怎么看外包？（我条件在这了 我有自知之明 我肯定说点好话）</li><li>（不演了）毕竟你是外包 签的另外一个公司主题，和项目的核心人员还是有区别的，请问你到时候怎么去确保节奏、态度、时长和正式百度员工对齐（开始表演了 真话不全说 不说假话 正反我都提）</li><li>反问 各路大厂的大模型 是不是目前都到瓶颈阶段了，都开始配合云平台落地toB业务定制了？</li></ol><blockquote>如果对你有帮助，麻烦点个小小的爱心和关注，后续会持续更新优质内容。</blockquote>]]></description></item><item>    <title><![CDATA[拒绝同质化！从“源码”到“原创”，构建有竞争力的代练平台？ 伊伊DK ]]></title>    <link>https://segmentfault.com/a/1190000047458611</link>    <guid>https://segmentfault.com/a/1190000047458611</guid>    <pubDate>2025-12-08 17:06:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在代练行业看似红海的今天，真正成功的平台屈指可数。市场上充斥着大量基于同质化源码搭建的“僵尸平台”——功能雷同、体验相似、毫无特色。如果你决心不只是“又做一个代练平台”，而是要打造一个有真正竞争力的原创产品。<br/><img width="723" height="495" referrerpolicy="no-referrer" src="/img/bVdmvG0" alt="" title=""/><img width="401" height="883" referrerpolicy="no-referrer" src="/img/bVdnijr" alt="" title="" loading="lazy"/><br/><strong>功能创新：在源码基础上做 “独家增量”</strong><br/>源码提供的是 “基础功能骨架”，原创则是在骨架上添 “独家血肉”—— 不用推翻源码，而是围绕核心痛点做 “微创新”，让功能成为你的差异化标签：</p><ol><li>针对代练员：打造 “留人型” 原创机制<br/>代练员是平台的核心资产，通用源码的 “抽佣固定、派单随机” 模式留不住优质代练。可基于 PHP 后台的模块化架构，新增原创规则：<br/>分级签约体系：把代练员分为 “普通、金牌、王牌” 三级，金牌代练抽佣降低 5%-8%，王牌代练可自主定价，且优先匹配高客单价订单（源码默认 “按单派单”，改为 “按等级 + 口碑派单”）；<br/>代练员成长体系：源码只有 “接单量统计”，新增 “代练员教学分成”（金牌代练可发布代练教程，学员购买后平台与代练员分成）、“师徒体系”（新代练员绑定老代练，接单佣金分润给师傅），让代练员不仅能接单，还能赚 “睡后收入”；</li><li>针对用户：设计 “锁单型” 原创功能<br/>用户的核心诉求是 “安全、省心、有性价比”，可基于 UniApp 前端，新增原创体验：<br/>智能定价系统：源码默认 “固定价格表”，改为 “动态定价”—— 根据游戏时段（深夜加价 10%）、段位（高分段阶梯加价）、紧急程度（极速单加价 20%）自动计算价格，用户可直观看到 “价格构成”，比同行的 “一口价” 更透明；<br/>订单可视化管理：新增 “代练进度条”（比如 “王者星耀上王者” 拆分为 “打满星 - 晋级赛 - 上王者” 三个阶段，每个阶段完成自动点亮）、“代练员实时定位”（仅授权后可见，避免代练员虚报上线时间）；<img width="723" height="1234" referrerpolicy="no-referrer" src="/img/bVdm5Lz" alt="" title="" loading="lazy"/><img width="723" height="556" referrerpolicy="no-referrer" src="/img/bVdmx76" alt="" title="" loading="lazy"/><img width="723" height="247" referrerpolicy="no-referrer" src="/img/bVdmcMZ" alt="" title="" loading="lazy"/></li></ol>]]></description></item><item>    <title><![CDATA[裁员为什么先裁技术人员？网友一针见血！ 悲伤的煎鸡蛋_cQXuXF ]]></title>    <link>https://segmentfault.com/a/1190000047458618</link>    <guid>https://segmentfault.com/a/1190000047458618</guid>    <pubDate>2025-12-08 17:05:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近逛职场社区的时候，刷到一个职场话题，老生常谈了，但是每次参与讨论的同学都好多。</p><p>这个问题问得比较扎心：</p><p>“为什么有些企业的裁员首先从技术人员开始？”<br/><img width="723" height="322" referrerpolicy="no-referrer" src="/img/bVdnihC" alt="" title=""/></p><p>关于这个问题，网上有一个被讨论很多的比喻：</p><p>“房子都盖起来了，还需要工人么？”</p><p>有一说一，这个比喻虽然刺耳，但却非常形象地揭示了某些企业的用人逻辑，尤其在某些非技术驱动型的公司里。</p><p>在某些非技术驱动的公司（比如传统企业转型、或者业务模式成型的公司），其实技术部门很多时候是会被视为「成本中心」，而非「利润中心」的，我相信在这类企业待过的技术同学肯定是深有体会。</p><p>就像盖大楼一样，公司需要做一个 App，或者搞一个系统，于是高薪招来一帮程序员“垒代码”。</p><p>当这个产品上线，业务跑通了，进入了平稳运营期，公司某些大聪明老板总会觉得“房子”已经盖好了。</p><p>这时候，一些开发人员在老板眼里就变成了“冗余”的成本。</p><p>大家知道，销售部门、业务部门能直接带来现金流，市场部能带来用户，而技术部门的代码是最看不见摸不着的。</p><p>一旦没有新的大项目启动，老板会觉得技术人员坐在那里就是在“烧钱”。</p><p>那抛开这个“盖楼”的比喻，在这种非技术驱动的公司里，从纯粹的财务角度来看，裁技术岗往往是因为“性价比”太低。</p><p>所以这里我们不得不面对的一个现实是：技术人员通常是公司里薪资最高的一群人。</p><p>高薪是一把双刃剑呐。</p><p>一个初级程序员的月薪可能抵得上两个行政，一个资深架构师的年薪可能抵得上一个小团队的运营费用。当公司面临现金流危机，需要快速削减成本时，裁掉一个高级技术人员省下来的钱，相当于裁掉好几个非技术岗位人员。</p><p>除此之外还有一个比较尴尬的事情那就是，在技术团队中，往往存在着一种“金字塔”结构。</p><p>随着工龄增长，薪资涨幅很快，但产出效率（在老板眼里）未必能线性增长。</p><p>脑补一下这个场景就知道了：</p><pre><code>一个 35 岁的高级工程师，月薪 4 万，可能要养家糊口，精力不如 20 多岁的小年轻，加班意愿低。
一个 23 岁的小年轻，月薪 1 万 5，充满激情，能扛能造。

</code></pre><p>这时候某些大聪明老板的算盘就又打起来了：</p><p>裁掉一个 4 万的老员工，招两个 1 万 5 的小年轻，代码量翻倍，团队氛围更活跃，成本还降了，这种“优化”在管理层眼里，简直是“降本增效”的典范。</p><p>所以综合上面这种种情形分析，这时候，文章开头的那个问题往往也就会逐渐形成了。</p><p>所以事就是这么个事，说再多也没用。</p><p>既然环境不能左右，那作为个体，我们又该如何自处呢？</p><p>这里我不想灌鸡汤，只想务实地聊一聊我所理解的一些对策，希望能对大家有所启发。</p><p>同时这也是我给很多后台私信我类似问题小伙伴们的一些共同建议。</p><p><strong>坑位</strong></p><p>技术大厂，前端-后端-测试，新一线和一二线城市等地均有<a href="https://link.segmentfault.com/?enc=jRTdKvJjm9FRrvBnK3Y11Q%3D%3D.tIgU8Q9BHHlX%2B7hxEiRi27ukxwwyT6sYhAiQz%2FDfbmA%3D" rel="nofollow" target="_blank">坑位</a>，感兴趣可以试试。待遇和稳定性都不错~</p><h3>1、跳出技术思维，建立业务思维</h3><p>千万不要只盯着你的 IDE 和那一亩三分地代码，抽空多了解了解业务和流程吧，比如：</p><pre><code>项目是靠什么赚钱的？
你的代码在哪个环节为公司省钱或挣钱？
如果你是老板，你会怎么优化现在的系统？

</code></pre><p>当你能用技术手段去解决业务痛点（比如提升转化率、降低服务器成本）时，你就不再是成本，而是资产。</p><h3>2、别温水煮青蛙，要保持技能更新</h3><p>这一点之前咱们这里多次提及，在技术行业，吃“老本”是最危险的。</p><p>当今的技术世界变化太快，而作为程序员的我们则恰好处于这一洪流之中，这既是挑战，也是机会。</p><p>还是那句话，一定要定期评估一下自己的市场价值：如果明天就离开现在的公司，你的技能和经验是否足以让你在市场上获得同等或更好的位置？</p><p>无论在公司工作多久，都要不断更新自己的技能和知识，确保自己始终具有市场竞争力。</p><h3>3、别让自己的工作经验烂掉，有意识地积累职业资产</h3><p>这一点我们之前其实也聊过。</p><p>除了特定的技术、代码、框架可以作为自己可积累的能力资产之外，其实程序员的职业生涯里也是可以有很多可固化和可积累的有形资产的。</p><p>比如你的技术经历、思维、经验、感悟是不是可以写成技术博客文字？你写的代码、工具、框架是不是可以形成开源项目？你的工作笔记和踩坑记录是不是可以整理成技术手册？</p><p>千万不要让自己的工作经验烂掉，而是要有意识地将自己的技术资产化，将自己的过往经验、知识、能力转化成在行业里有影响力的硬通货。</p><h3>4、尽早构建 Plan B，提升抗风险能力</h3><p>当然这一点虽然说的简单，其实对人的要求是比较高的。前面几点做好了，这一点有时候往往就会水到渠成。</p><p>我觉得总体的方向应该是：尽量利用你的技术特长来构建一个可持续的 Plan B。</p><p>比方说：开发一个小工具、写写技术专栏、或者运营一个 GitHub 项目、在技术博客或社区中建立个人品牌...等等，这些不仅仅能增加收入，往往还能拓展你的人脉圈。</p><p>其实很多程序员在年龄大了之后越来越焦虑的一个重要原因就是因为生存技能太过单一了，所以千万不要给自己设限，埋头赶路的同时也不要忘记时常抬头看看周围的环境和机会。</p><p>好了，今天就先聊这么多吧，希望能对大家有所启发，我们下篇见。</p><p>——转载自：CodeSheep</p>]]></description></item><item>    <title><![CDATA[卓普云亮相曼谷Traffic Connect，与50+企业共话全球增长 DigitalOcean ]]></title>    <link>https://segmentfault.com/a/1190000047458620</link>    <guid>https://segmentfault.com/a/1190000047458620</guid>    <pubDate>2025-12-08 17:04:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​<strong>12 月 2 日，扬帆出海携手 PhotonPay、卓普云 AI Droplet 在泰国曼谷联合举办了一场《Bangkok Traffic Connect-全球互联网企业营销交流晚宴》</strong>​，晚宴中，汇聚了​<strong>50+ 全球 ADX、网盟企业高管以及曼谷 AWA 参展企业高层</strong>​，在 2 小时中实现面对面的紧密交流，共探出海合作机遇。</p><p>本场晚宴上，<strong>扬帆出海 创始人&amp;CEO 刘武华、PhotonPay Sales ​VP</strong>​​<strong>​ Joey Xu、卓普云 AI Droplet GM 杨刚依次进行了开场致辞演讲</strong>​，勾勒出互联网营销合作新图景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458622" alt="" title=""/></p><p>图：卓普云 AI Droplet GM 杨刚</p><p>在行业晚宴上，卓普云科技 AI Droplet GM 杨刚发表致辞，向来自 ADX 与效果广告行业的嘉宾介绍了 DigitalOcean 在全球云基础设施与 AI 领域的最新发展，并强调中国企业在全球市场扩张中可从中获得的切实收益。</p><p>杨刚首先介绍，卓普云科技由 DigitalOcean 控股股东 Access Technology Ventures 在中国设立，是 DigitalOcean 中国区独家战略合作伙伴，旨在帮助中国企业以更简单、透明、高性价比的方式使用 DigitalOcean 的全球云资源，并协助其更好服务出海客户。作为一家 2012 年成立、2021 年登陆纽交所的年轻云厂商，DigitalOcean 以轻量化、专注与高效率著称，凭借极具竞争力的成本结构获得全球数百万开发者与数十万企业用户的认可。数据显示，从其他云平台迁移到 DigitalOcean 的客户，平均可节省超过 35% 的整体成本，在部分 AI 场景中节省幅度最高可达 75%。</p><p>随着全球 AI 的快速崛起，DigitalOcean 的 AI/ML 业务在 2024 年实现超过 100% 的收入增长，并通过持续的产品迭代逐步形成企业级能力，包括弹性扩容、托管 Kubernetes、高性能 GPU 集群等。今年 DigitalOcean 发布的 Gradient™ AI 平台进一步补齐端到端 AI 工作流程，覆盖训练、微调到推理的全链路，并支持 NVIDIA 与 AMD 双路线 GPU。值得一提的是，DigitalOcean 还是 AMD GPU Developer Cloud 的官方托管平台。</p><p>DigitalOcean 在全球市场持续吸引大型企业客户，包括 Bright Data、Fal、Nobid 等。在月消费较高的企业用户群体中，其收入保持 35% 以上的年增长率，显示其在支撑大规模业务方面的能力不断增强。以海外广告技术公司 Nobid 为例，其在 DigitalOcean 上的多区域集群每秒处理逾 30 万请求，每月数据量达 1.3 PB。从 AWS 迁移至 DigitalOcean 后，其整体成本已下降 16%，预计通过持续优化可降至 30%。在中国市场，Webeye 等出海企业也选择将国际业务迁移至 DigitalOcean，以获得更稳健的成本表现。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458623" alt="" title="" loading="lazy"/></p><p>杨刚表示，这些案例证明 DigitalOcean 的技术架构、高性价比与性能特征，与 ADX 与效果广告行业的需求高度契合。作为 DigitalOcean 在中国的关键合作伙伴，卓普云科技希望不仅提供本地化服务与架构支持，也希望成为中国出海企业的长期战略伙伴，帮助企业连接全球产品与工程团队，在国际竞争中获得更具确定性的基础设施优势。</p><p>他最后表示，期待与行业伙伴共同打造更健康、高效、可持续的全球基础设施生态，为广告技术行业带来更明确的成本优势与竞争力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047458624" alt="" title="" loading="lazy"/></p><p>图：卓普云团队与 DigitalOcean 高级解决方案架构师</p><p>在本次活动中，<strong>Vlion、Tec-Do2.0、GEONODE、RollerAds、Novabeyond、Mejoy、Yeahmobi、Viking Media、Bidnex、Nasimobi、MobUpps、Touchpoint、AdMergeX、Blitzads</strong> 等 50+ 全球互联网营销企业共聚一堂，共同交流了全球合作的机遇与无限可能。与此同时，DigitalOcean 高级解决方案架构师 Sri Charan Madhavapeddi 也出席了本次活动，并与现场多位嘉宾一同分享交流了 DigitalOcean 在全球，特别是亚太地区的行业落地经验。</p><p>在未来，卓普云 AI Droplet 不仅会帮助更多中国企业利用 DigitalOcean 这张“云船票”扬帆出海，还会联结更多亚太的企业共同探索全球市场。</p>]]></description></item><item>    <title><![CDATA[如何在 Kuscia 上运行 SCQL 联合分析任务 隐语SecretFlow ]]></title>    <link>https://segmentfault.com/a/1190000047458632</link>    <guid>https://segmentfault.com/a/1190000047458632</guid>    <pubDate>2025-12-08 17:04:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>打开链接即可点亮社区Star，照亮技术的前进之路。</p><p>Github 地址：<em><a href="https://link.segmentfault.com/?enc=AvIwuAfN6OtFASIMVmIjKw%3D%3D.e9z%2BFxOItolRXL9Bu0Zl%2BunSYvgZOkJcENJ%2F9oI3HnG37taQnbg7jU%2BC5aJVPMMo" rel="nofollow" target="_blank">https://github.com/secretflow/kuscia</a></em></p><p>本教程将以 <a href="../reference/apis/summary_cn.md" target="_blank">KusciaAPI</a> 创建本地数据源作为示例，介绍如何在 Kuscia 上运行 SCQL 联合分析任务。</p><h3>准备节点</h3><ul><li>体验部署请选择<a href="../getting_started/quickstart_cn.md" target="_blank">快速入门</a>。</li><li>生产部署请选择<a href="../deployment/Docker_deployment_kuscia/index.rst" target="_blank">多机部署</a>。</li></ul><p>本示例在<strong>点对点组网模式</strong>下完成。在中心化组网模式下，证书的配置会有所不同。</p><p>{#cert-and-token}</p><h3>获取 KusciaAPI 证书和 Token</h3><p>在下面<a href="./run_scql_on_kuscia_cn.md#alice-准备测试数据" target="_blank">准备数据</a>步骤中需要使用到 KusciaAPI，如果 KusciaAPI 启用了 MTLS 协议，则需要提前准备好 MTLS 证书和 Token。协议参考<a href="../troubleshoot/concept/protocol_describe.md" target="_blank">这里</a>。</p><h4>点对点组网模式</h4><p>证书的配置参考<a href="../deployment/Docker_deployment_kuscia/deploy_p2p_cn.md#配置授权" target="_blank">配置授权</a></p><p>这里以 Alice 节点为例，接口需要的证书文件在 ${USER}-kuscia-autonomy-alice 节点的 <code>/home/kuscia/var/certs/</code> 目录下：</p><table><thead><tr><th>文件名</th><th>文件功能</th></tr></thead><tbody><tr><td>kusciaapi-server.key</td><td>服务端私钥文件</td></tr><tr><td>kusciaapi-server.crt</td><td>服务端证书文件</td></tr><tr><td>ca.crt</td><td>CA 证书文件</td></tr><tr><td>token</td><td>认证 Token ，在 headers 中添加 Token: { token 文件内容}</td></tr></tbody></table><h4>中心化组网模式</h4><p>证书文件在 ${USER}-kuscia-master 节点的 <code>/home/kuscia/var/certs/</code> 目录下：</p><table><thead><tr><th>文件名</th><th>文件功能</th></tr></thead><tbody><tr><td>kusciaapi-server.key</td><td>服务端私钥文件</td></tr><tr><td>kusciaapi-server.crt</td><td>服务端证书文件</td></tr><tr><td>ca.crt</td><td>CA 证书文件</td></tr><tr><td>token</td><td>认证 Token ，在 headers 中添加 Token: { token 文件内容}</td></tr></tbody></table><h3>准备数据</h3><p>您可以使用本文示例的测试数据文件，或者使用您自己的数据文件。</p><p>在 Kuscia 中，在节点容器的 <code>/home/kuscia/var/storage</code> 目录存放内置测试数据文件，下面 Alice 和 Bob 节点分别使用的是 scql-alice.csv 和 scql-bob.csv，您可以在容器中查看这两个数据文件。</p><h4>准备测试数据</h4><h5>Alice 准备测试数据</h5><ol><li><p>这里以 Docker 部署模式为例，登录到 alice 节点中</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice bash</code></pre></li><li><p>创建 DomainDataSource</p><p>下面 datasource_id 名称以 scql-demo-local-datasource 为例：</p><pre><code class="bash">export CTR_CERTS_ROOT=/home/kuscia/var/certs
curl -k -X POST 'https://localhost:8082/api/v1/domaindatasource/create' \
 --header "Token: $(cat ${CTR_CERTS_ROOT}/token)" \
 --header 'Content-Type: application/json' \
 --cert ${CTR_CERTS_ROOT}/kusciaapi-server.crt \
 --key ${CTR_CERTS_ROOT}/kusciaapi-server.key \
 --cacert ${CTR_CERTS_ROOT}/ca.crt \
 -d '{
  "domain_id": "alice",
  "datasource_id":"scql-demo-local-datasource",
  "type":"localfs",
  "name": "DemoDataSource",
  "info": {
      "localfs": {
          "path": "/home/kuscia/var/storage/data"
      }
  },
  "access_directly": true
}'</code></pre><p>:::{tip}<br/>K8S RunK 模式部署 Kuscia 时，此处需要使用 <a href="../reference/apis/domaindatasource_cn.md#id5" target="_blank">OSS 数据源</a>，并将 /home/kuscia/var/storage/data/scql-alice.csv 示例数据放入 OSS 中。<br/>:::</p></li><li><p>创建 DomainData</p><p>下面 domaindata_id 名称以 scql-alice-table 为例：</p><pre><code class="bash">export CTR_CERTS_ROOT=/home/kuscia/var/certs
curl -k -X POST 'https://localhost:8082/api/v1/domaindata/create' \
 --header "Token: $(cat ${CTR_CERTS_ROOT}/token)" \
 --header 'Content-Type: application/json' \
 --cert ${CTR_CERTS_ROOT}/kusciaapi-server.crt \
 --key ${CTR_CERTS_ROOT}/kusciaapi-server.key \
 --cacert ${CTR_CERTS_ROOT}/ca.crt \
 -d '{
  "domain_id": "alice",
  "domaindata_id": "scql-alice-table",
  "datasource_id": "scql-demo-local-datasource",
  "name": "alice001",
  "type": "table",
  "relative_uri": "scql-alice.csv",
  "columns": [
    {
      "name": "ID",
      "type": "str"
    },
    {
      "name": "credit_rank",
      "type": "int"
    },
    {
      "name": "income",
      "type": "int"
    },
    {
      "name": "age",
      "type": "int"
    }
  ]
}'</code></pre></li></ol><h5>Bob 准备测试数据</h5><ol><li><p>这里以 Docker 部署模式为例，登录到 Bob 节点中</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob bash</code></pre></li><li><p>创建 DomainDataSource</p><p>下面 datasource_id 名称以 scql-demo-local-datasource 为例：</p><pre><code class="bash">export CTR_CERTS_ROOT=/home/kuscia/var/certs
curl -k -X POST 'https://localhost:8082/api/v1/domaindatasource/create' \
 --header "Token: $(cat ${CTR_CERTS_ROOT}/token)" \
 --header 'Content-Type: application/json' \
 --cert ${CTR_CERTS_ROOT}/kusciaapi-server.crt \
 --key ${CTR_CERTS_ROOT}/kusciaapi-server.key \
 --cacert ${CTR_CERTS_ROOT}/ca.crt \
 -d '{
  "domain_id": "bob",
  "datasource_id":"scql-demo-local-datasource",
  "type":"localfs",
  "name": "DemoDataSource",
  "info": {
      "localfs": {
          "path": "/home/kuscia/var/storage/data"
      }
  },
  "access_directly": true
}'</code></pre><p>:::{tip}<br/>K8S RunK 模式部署 Kuscia 时，此处需要使用 <a href="../reference/apis/domaindatasource_cn.md#id5" target="_blank">OSS 数据源</a>，并将 /home/kuscia/var/storage/data/scql-bob.csv 示例数据放入 OSS 中。<br/>:::</p></li><li><p>创建 DomainData</p><p>下面 domaindata_id 名称以 scql-bob-table 为例：</p><pre><code class="bash">export CTR_CERTS_ROOT=/home/kuscia/var/certs
curl -k -X POST 'https://localhost:8082/api/v1/domaindata/create' \
 --header "Token: $(cat ${CTR_CERTS_ROOT}/token)" \
 --header 'Content-Type: application/json' \
 --cert ${CTR_CERTS_ROOT}/kusciaapi-server.crt \
 --key ${CTR_CERTS_ROOT}/kusciaapi-server.key \
 --cacert ${CTR_CERTS_ROOT}/ca.crt \
 -d '{
  "domain_id": "bob",
  "domaindata_id": "scql-bob-table",
  "datasource_id": "scql-demo-local-datasource",
  "name": "bob001",
  "type": "table",
  "relative_uri": "scql-bob.csv",
  "columns": [
    {
      "name": "ID",
      "type": "str"
    },
    {
      "name": "order_amount",
      "type": "int"
    },
    {
      "name": "is_active",
      "type": "int"
    }
  ]
}'</code></pre></li></ol><h3>部署 SCQL</h3><h4>Alice 部署 SCQL</h4><ol><li><p>登陆到 alice 节点容器中</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice bash</code></pre><p>如果是中心化组网模式，则需要登录到 master 节点容器中。</p><pre><code class="bash">docker exec -it ${USER}-kuscia-master bash</code></pre></li><li><p>获取 SCQL 应用的镜像模版 AppImage</p><p>从 SCQL 官方文档中，获取 AppImage 具体内容，并将其内容保存到 scql-image.yaml 文件中。 具体模版内容，可参考 <a href="https://link.segmentfault.com/?enc=MAkSl0MLfomuWmIMlObVPQ%3D%3D.9ObL0eXfEGV7z%2FyOLEKZRxhdjHkcRcnUMGNdiAtqfTZln7rF%2BpNuncMuD5ldUcXVjcyqhfstxqfNuIoRs8nN8%2FRzdyNrJ9ZBiLden83DSdjOohiryaqhPm2OAQSA0ZJg" rel="nofollow" target="_blank">SCQL AppImage</a>。</p><blockquote><p>注意：</p><ol><li>如果 <code>secretflow/scql</code> 仓库访问网速较慢，可以替换为 <code>secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/scql</code>。</li><li>请删除 <code>#--datasource_router=kusciadatamesh</code> 代码行前面的 # 符号，以启用 Datamesh 本地数据源配置。</li><li>在 <code>engineConf</code> 字段加上 <code>--enable_restricted_read_path=false</code> 限制 csv 文件的读取路径。</li><li>K8S RunK 模式部署 Kuscia 时，需要使用 MySQL 存储 Broker 元数据。修改 <code>storage</code> 字段的 <code>type</code> 为 MySQL 和 <code>conn_str</code> 对应的数据库连接字符串。</li><li>如果 AppImage 配置有改动可以重启 Kuscia 或重新创建 Broker 使配置生效。示例命令：<code>kubectl delete KusciaDeployment scql -n cross-domain</code> <code>kubectl apply -f broker-deploy.yaml</code> 。</li></ol></blockquote></li><li>创建 SCQL 应用的镜像模版 AppImage</li></ol><pre><code class="bash">kubectl apply -f scql-image.yaml</code></pre><ol start="4"><li>部署 Broker</li></ol><pre><code class="bash">kubectl apply -f /home/kuscia/scripts/templates/scql/broker_alice.yaml</code></pre><h4>Bob 部署 SCQL</h4><ol><li><p>登陆到 Bob 节点容器中</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob bash</code></pre><p>如果是中心化组网模式，则需要登录到 master 节点容器中。</p></li><li><pre><code class="bash">docker exec -it ${USER}-kuscia-master bash</code></pre></li><li><p>获取 SCQL 应用的镜像模版 AppImage</p><p>从 SCQL 官方文档中，获取 AppImage 具体内容，并将其内容保存到 scql-image.yaml 文件中。 具体模版内容，可参考 <a href="https://link.segmentfault.com/?enc=OkaS9V1IieghCE%2FhWyCXbA%3D%3D.HGeOHmJNE5WMaYkNm1YR779Iq9q2dAd5GNTKXGAoAx2Sb3hkNp2VewF0iU7qtbHYawWHdMATW1UtZ5bNc4ncOwcgwgTvnqJw9CjA%2ByuGrHada3tPGy4O2nCPUPwIMR%2B8" rel="nofollow" target="_blank">SCQL AppImage</a>。</p><blockquote><p>注意：</p><ol><li>如果 <code>secretflow/scql</code> 仓库访问网速较慢，可以替换为 <code>secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/scql</code>。</li><li>请删除 <code>#--datasource_router=kusciadatamesh</code> 代码行前面的 # 符号，以启用 Datamesh 本地数据源配置。</li><li>在 <code>engineConf</code> 字段加上 <code>--enable_restricted_read_path=false</code> 限制 csv 文件的读取路径。</li><li>K8S RunK 模式部署 Kuscia 时，需要使用 MySQL 存储 Broker 元数据。修改 <code>storage</code> 字段的 <code>type</code> 为 MySQL 和 <code>conn_str</code> 对应的数据库连接字符串。</li><li>如果 AppImage 配置有改动可以重启 Kuscia 或重新创建 Broker 使配置生效。示例命令：<code>kubectl delete KusciaDeployment scql -n cross-domain</code> <code>kubectl apply -f broker-deploy.yaml</code> 。</li></ol></blockquote></li><li><p>创建 SCQL 应用的镜像模版 AppImage</p><pre><code class="bash">kubectl apply -f appimage.yaml</code></pre></li><li><p>部署 Broker</p><pre><code class="bash">kubectl apply -f /home/kuscia/scripts/templates/scql/broker_bob.yaml</code></pre><h4>查看 broker 是否部署成功</h4><p>下面以 Alice 节点为例，Bob 节点类似</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get po -A</code></pre></li></ol><h2>When the Pod status is Running, it indicates that the deployment was successful:</h2><p>NAMESPACE   NAME                           READY   STATUS    RESTARTS   AGE<br/>alice       scql-broker-6f4f85b64f-fsgq8   1/1     Running   0          2m42s</p><pre><code>
## 使用 SCQL 进行联合分析

下面仅以流程步骤作为示例展示，更多接口参数请参考 [SCQL API](https://www.secretflow.org.cn/zh-CN/docs/scql/main/reference/broker-api)。

### 创建项目并邀请参与方加入

#### Alice 创建项目，并邀请 Bob 加入

1. 登录到 Alice 节点容器中
   </code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice bash</p><pre><code>2. 创建项目

下面项目名称以 "demo" 为例：
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=UNu5yfzcsyvjRVba9qxa4Q%3D%3D.yTZ1apWEubxuffGL7eu%2BC%2BvjI7sy79swpdv9hbkMuIOAdWTy55QKa5nWS9HPs7Ad" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/project/create</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -d '{</p><pre><code>   "project_id":"demo",
   "name":"demo",
   "conf":{
       "spu_runtime_cfg":{
       "protocol":"SEMI2K",
       "field":"FM64"
       }
   },
  "description":"this is a project"</code></pre><p>}'</p><pre><code>3. 查看项目
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=WiZXO3O4LbHakmtCsba49g%3D%3D.M2QU9u%2FAaKIIY3MxDzH8HtiI7TbLqaIc%2FJFNysB6oeCRYjZxbQni0ZoNrZZ0Z0p4" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/project/list</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice"</p><pre><code>4. 邀请 Bob 加入到 "demo" 项目中
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=nE6dp8dakE%2F2U4s7z78x2g%3D%3D.bNcVx%2BZ%2FC5ujNHdmqaaAQg%2BFBeaOPUUrnbChx4WsBYDU5scwp%2B5ZnoeOvRBppadj" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/member/invite</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -d '{</p><pre><code>   "invitee": "bob",
   "project_id": "demo"</code></pre><p>}'</p><pre><code>5. 查看邀请状态
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=hXocVKcBgHIahUJXwhwALg%3D%3D.L6r0FHkatJ9o%2Fx4Jl3OcB8EkcYt7gWz%2FBTAw9FwUtS3uS28X1Z0VOvz0d1zPNwvC" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/invitation/list</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice"</p><pre><code>
#### Bob 接受邀请

1. 登录到 Bob 节点容器中
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-bob bash</p><pre><code>2. Bob 接受 Alice 的入项邀请
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=Tfz%2FTh86mwS4elZCEJqhiw%3D%3D.obiceyBTTS0UnIxyObWCXX7RKC31GwOINl45zA98WL5UtoELB4SwrMbsa7G9N5Dy" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/invitation/process</a> \<br/>   --header "host: scql-broker-intra.bob.svc" \<br/>   --header "kuscia-source: bob" \<br/>   -d '{</p><pre><code>   "invitation_id":1,
   "respond":0</code></pre><p>}'</p><pre><code>
### 创建数据表

#### Alice 创建数据表

1. 登录到 Alice 节点容器中
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice bash</p><pre><code>2. 创建数据表

&gt; 下面 table_name 以 ta 为例，ref_table 参数的值为[创建 DomainData](./run_scql_on_kuscia_cn.md#alice-准备测试数据)时的 `domaindata_id`
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=gnR%2FFuvVhs%2FvDJwJ1uCdSQ%3D%3D.LtM9wAYCGxk4mk%2BLFzSpnczucQgTIJQRlAE13DHXO9oqch9ypYaTKj9vh86tG1rZ" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/table/create</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"table_name": "ta",
"ref_table": "scql-alice-table",
"db_type": "csvdb",
"columns": [
    {"name":"ID","dtype":"string"},
    {"name":"credit_rank","dtype":"int"},
    {"name":"income","dtype":"int"},
    {"name":"age","dtype":"int"}
]</code></pre><p>}'</p><pre><code>
#### Bob 创建数据表

1. 登录到 Bob 节点容器中
   </code></pre><p>docker exec -it ${USER}-kuscia-autonomy-bob bash</p><pre><code>2. 创建数据表

&gt; 下面 table_name 以 ta 为例，ref_table 参数的值为[创建 DomainData](./run_scql_on_kuscia_cn.md#bob-准备测试数据)时的 `domaindata_id`
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=WhilE%2BvC62Ns6QcwIIXYow%3D%3D.pxQRgl5wG7K5%2BK46iOK2Uoh%2FcxNFYCYtKZxSVgRP67gIlH1gZd%2FjSW1gT8Zb%2B4QT" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/table/create</a> \<br/>--header "host: scql-broker-intra.bob.svc" \<br/>--header "kuscia-source: bob" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"table_name": "tb",
"ref_table": "scql-bob-table",
"db_type": "csvdb",
"columns": [
    {"name":"ID","dtype":"string"},
    {"name":"order_amount","dtype":"double"},
    {"name":"is_active","dtype":"int"}
]</code></pre><p>}'</p><pre><code>
### 查看数据表

下面以 Alice 为例，Bob 节点类似
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=59L63xoM3hbvQKAVAKXZdQ%3D%3D.gmqYOySAjgC20C%2FBJqkoPdDMbZlerhG0wFhZmii7o9zhxrCl6Ss%2BRDXNT60E3K3p" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/table/list</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo"</code></pre><p>}'</p><pre><code>
### 删除数据表

若想删除创建的数据表时，可以参考下面命令。以 Alice 节点为例，Bob 节点类似。
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=YV2RyIyEkpJndSdUpRBlqw%3D%3D.V0CMYRzp%2BbTVmnoNs4CMzEaiMFoIJ%2FntrV2ZHJHuMV4RgOvth%2FgepMGyO2bjaDmE" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/table/drop</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"table_name":"ta"</code></pre><p>}'</p><pre><code>
### 数据表授权

#### Alice 的数据表授权

1. 将 ta 数据表授权给 Alice
   </code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=4e%2BI9bxno4ktVLtMaPPpBA%3D%3D.HgmSflW7uhD0QZee64RNwXUR%2FjVPORs8P544fYuuiHufAIcNyBX8k4Wr52OuRp%2FB" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/grant</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>   "project_id": "demo",
   "column_control_list":[
   {"col":{"column_name":"ID","table_name":"ta"},"party_code":"alice","constraint":1},
   {"col":{"column_name":"age","table_name":"ta"},"party_code":"alice","constraint":1},
   {"col":{"column_name":"income","table_name":"ta"},"party_code":"alice","constraint":1},
   {"col":{"column_name":"credit_rank","table_name":"ta"},"party_code":"alice","constraint":1}
   ]</code></pre><p>}'</p><pre><code>2. 将 ta 表授权给 Bob 节点
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=8OtgttJ%2Bn%2BmBZkys%2Bwn4xg%3D%3D.3AL1gtQn4RbR0DS%2Fo7cC2kjMt1DZxjZDwXL6huEd3niTec%2FrlK5SOXYvJ4jky9ZU" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/grant</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>   "project_id": "demo",
   "column_control_list":[
   {"col":{"column_name":"ID","table_name":"ta"},"party_code":"bob","constraint":1},
   {"col":{"column_name":"age","table_name":"ta"},"party_code":"bob","constraint":1},
   {"col":{"column_name":"income","table_name":"ta"},"party_code":"bob","constraint":1},
   {"col":{"column_name":"credit_rank","table_name":"ta"},"party_code":"bob","constraint":1}
   ]</code></pre><p>}'</p><pre><code>
#### Bob 的数据表授权

1. 将 tb 表授权给 Alice 节点
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=8yaAzFLoDL6yzmLPbLH89w%3D%3D.rGVkFkuCnJUY8ylnojz8OufPnCtA0LlnJXUZgNcJMd2c3iHOi9i8TgFHJGnzdKQv" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/grant</a> \<br/>   --header "host: scql-broker-intra.bob.svc" \<br/>   --header "kuscia-source: bob" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>     "project_id": "demo",
     "column_control_list":[
     {"col":{"column_name":"ID","table_name":"tb"},"party_code":"alice","constraint":1},
     {"col":{"column_name":"is_active","table_name":"tb"},"party_code":"alice","constraint":1},
     {"col":{"column_name":"order_amount","table_name":"tb"},"party_code":"alice","constraint":1}
     ]</code></pre><p>}'</p><pre><code>2. 将 tb 表授权给 Bob 节点
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=K7IWmM8XcLqn6qdoWonoiA%3D%3D.RlOmlfdqDjILwuV7iYd%2BJq2JhTmhsyQrn1KRiW9ELcW0DLoCUcEdLbzXEPGrzY6c" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/grant</a> \<br/>   --header "host: scql-broker-intra.bob.svc" \<br/>   --header "kuscia-source: bob" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>   "project_id": "demo",
   "column_control_list":[
   {"col":{"column_name":"ID","table_name":"tb"},"party_code":"bob","constraint":1},
   {"col":{"column_name":"is_active","table_name":"tb"},"party_code":"bob","constraint":1},
   {"col":{"column_name":"order_amount","table_name":"tb"},"party_code":"bob","constraint":1}
   ]</code></pre><p>}'</p><pre><code>
### 查看数据表授权

下面以 Alice 为例，Bob 节点类似
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=6%2FK6GOnbd5cRNGBmFUloyw%3D%3D.IK1YnXiLEFpgS9X0eua9QMNu0HPMjLWhSeBoEECyuC8qncsToDvLTPO2jxmD2S%2BJ" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/show</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"tables":["ta"],
"dest_parties":["alice"]</code></pre><p>}'</p><pre><code>
### 撤销数据表授权

若想撤销数据表授权，那么可以参考下面命令。以 Alice 节点为例，Bob 节点类似。
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=h7dXnnCAzk35G2edASmBTw%3D%3D.VKMPlyCtlHH1azvWgrsZw0X%2FnqBBsmvy8Gj5QYS1eOroGgCDYHJXE20UuAYe9eXM" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/ccl/revoke</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"column_control_list":[
{"col":{"column_name":"ID","table_name":"ta"},"party_code":"alice","constraint":1},
{"col":{"column_name":"age","table_name":"ta"},"party_code":"alice","constraint":1},
{"col":{"column_name":"income","table_name":"ta"},"party_code":"alice","constraint":1},
{"col":{"column_name":"credit_rank","table_name":"ta"},"party_code":"alice","constraint":1}
]</code></pre><p>}'</p><pre><code>
### 进行联合分析

#### 同步查询

下面以 Alice 节点查询为例 Bob 节点类似。
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=Dom69RLA4%2BD%2FQt0Glrf6FQ%3D%3D.RKS3QczIl5nTZu%2FcMjDV56Aa2aekTJcqrjq1Di6Eo0Y%3D" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/query</a> \<br/>--header "host: scql-broker-intra.alice.svc" \<br/>--header "kuscia-source: alice" \<br/>-H "Content-Type: application/json" \<br/>-d '{</p><pre><code>"project_id": "demo",
"query":"SELECT ta.credit_rank, COUNT(*) as cnt, AVG(ta.income) as avg_income, AVG(tb.order_amount) as avg_amount FROM ta INNER JOIN tb ON ta.ID = tb.ID WHERE ta.age &gt;= 20 AND ta.age &lt;= 30 AND tb.is_active=1 GROUP BY ta.credit_rank;"</code></pre><p>}'</p><pre><code>
返回的成功结果如下:
</code></pre><p>{</p><pre><code>"status": {
    "code": 0,
    "message": "",
    "details": []
},
"affected_rows": "0",
"warnings": [],
"cost_time_s": 7.171298774,
"out_columns": [{
    "name": "credit_rank",
    "shape": {
        "dim": [{
            "dim_value": "2"
        }, {
            "dim_value": "1"
        }]
    },
    "elem_type": "INT64",
    "option": "VALUE",
    "annotation": {
        "status": "TENSORSTATUS_UNKNOWN"
    },
    "int32_data": [],
    "int64_data": ["6", "5"],
    "float_data": [],
    "double_data": [],
    "bool_data": [],
    "string_data": [],
    "ref_num": 0
}, {
    "name": "cnt",
    "shape": {
        "dim": [{
            "dim_value": "2"
        }, {
            "dim_value": "1"
        }]
    },
    "elem_type": "INT64",
    "option": "VALUE",
    "annotation": {
        "status": "TENSORSTATUS_UNKNOWN"
    },
    "int32_data": [],
    "int64_data": ["3", "1"],
    "float_data": [],
    "double_data": [],
    "bool_data": [],
    "string_data": [],
    "ref_num": 0
}, {
    "name": "avg_income",
    "shape": {
        "dim": [{
            "dim_value": "2"
        }, {
            "dim_value": "1"
        }]
    },
    "elem_type": "FLOAT64",
    "option": "VALUE",
    "annotation": {
        "status": "TENSORSTATUS_UNKNOWN"
    },
    "int32_data": [],
    "int64_data": [],
    "float_data": [],
    "double_data": [438000, 30070],
    "bool_data": [],
    "string_data": [],
    "ref_num": 0
}, {
    "name": "avg_amount",
    "shape": {
        "dim": [{
            "dim_value": "2"
        }, {
            "dim_value": "1"
        }]
    },
    "elem_type": "FLOAT64",
    "option": "VALUE",
    "annotation": {
        "status": "TENSORSTATUS_UNKNOWN"
    },
    "int32_data": [],
    "int64_data": [],
    "float_data": [],
    "double_data": [4060.6666666666665, 3598],
    "bool_data": [],
    "string_data": [],
    "ref_num": 0
}]</code></pre><p>}</p><pre><code>
#### 异步查询

下面以 Alice 节点为例，Bob 节点类似。

1. 提交 query
   </code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=gdzp%2FueCdZgeo9XAbcmXcw%3D%3D.o1kTSIFH2uey6jjZCl5NYxCQtPm8TQayAK6LT%2Bvn1Ux%2Fdpb5abjlXtuIqjcCUdBr" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/query/submit</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>   "project_id": "demo",
   "query":"SELECT ta.credit_rank, COUNT(*) as cnt, AVG(ta.income) as avg_income, AVG(tb.order_amount) as avg_amount FROM ta INNER JOIN tb ON ta.ID = tb.ID WHERE ta.age &gt;= 20 AND ta.age &lt;= 30 AND tb.is_active=1 GROUP BY ta.credit_rank;"</code></pre><p>}'</p><pre><code>2. 获取结果
</code></pre><p>curl -X POST <a href="https://link.segmentfault.com/?enc=lXCDX013qlltliyAy6KBsQ%3D%3D.2sYLDOcg%2Fu9IV9jyX3ckt0OE7cB%2Fjm9QuvKao6iUBhBXOPvAV4YKm8bFOiT%2FwINN" rel="nofollow" target="_blank">http://127.0.0.1:80/intra/query/fetch</a> \<br/>   --header "host: scql-broker-intra.alice.svc" \<br/>   --header "kuscia-source: alice" \<br/>   -H "Content-Type: application/json" \<br/>   -d '{</p><pre><code>     "job_id":"3c4723fb-9afa-11ee-8934-0242ac12000"</code></pre><p>}'</p><pre><code>
## 参考

### 常用命令

查看 broker kd 状态：
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl get kd -n cross-domain</p><pre><code>
查看 broker deployment 状态
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl get deployment -A</p><pre><code>
查看 broker 应用状态
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl get po -A</p><pre><code>
查看 broker configmap
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl get cm scql-broker-configtemplate -n alice -oyaml</p><pre><code>
查看 appImage
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl get appimage</p><pre><code>
删除 broker
</code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice kubectl delete kd scql -n cross-domain</p><pre><code>
### 如何查看 SCQL 应用容器日志

在 Kuscia 中，可以登陆到节点容器内查看 SCQL 应用容器的日志。具体方法如下。

1. 登陆到节点容器中
   
   下面以 Alice 节点为例：
   </code></pre><p>docker exec -it ${USER}-kuscia-autonomy-alice bash</p><pre><code>2. 查看日志

在目录 `/home/kuscia/var/stdout/pods` 下可以看到对应 SCQL Broker 和 Engine 应用容器的目录。后续进入到相应目录下，即可查看应用的日志。
</code></pre><p># View the current application container's directory<br/>   ls /home/kuscia/var/stdout/pods</p><p># View the application container's logs, example as follows:<br/>   cat /home/kuscia/var/stdout/pods/alice_xxxx_engine_xxxx/secretflow/0.log<br/>   cat /home/kuscia/var/stdout/pods/alice_xxxx_broker_xxxx/secretflow/0.log</p>]]></description></item><item>    <title><![CDATA[慧云自助收银系统：赋能零售场景的智慧收银解决方案 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047458672</link>    <guid>https://segmentfault.com/a/1190000047458672</guid>    <pubDate>2025-12-08 17:03:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、概述总结</strong><br/>慧云自助收银系统是一款面向零售行业的物联网应用，涵盖独立 APP、微信小程序及抖音小程序等多形态产品，以微擎系统交付为核心，提供从软件部署到硬件适配的一站式收银解决方案。首次购买赠送 1 年免费更新服务，支持安卓设备运行及 PHP5.6 环境，源码未加密且保障官方正品，可满足多商户、多设备、跨硬件的收银需求，凭借刷脸支付、扫码结算等核心功能，助力商家实现高效运营。</p><p><strong>二、功能介绍</strong><br/>（一）核心收银功能<br/>支持多渠道支付，涵盖微信支付分、微信收付通、支付宝代扣、支付宝直付通等，同时兼容刷脸支付（微信、支付宝、银联）与手机二维码支付。</p><p>可通过 PC 端、手机端、小程序灵活设置收银参数，调用云端商品库快速完成结算，支持票据自动打印。</p><p>（二）设备适配与拓展<br/>硬件兼容安卓 RK3128 及以上主板（推荐 RK3288 及以上），支持 10-32 寸不同分辨率屏幕，涵盖自助收银机、普通收银机等设备类型。</p><p>支持组装设备与整机设备适配，可搭配扫码仪器、小票打印机、条码打印机等硬件，部分设备支持壁挂、支架立式、桌面式等多安装方式。</p><p>（三）后台管理能力<br/>提供设备管理功能，可实现设备添加、搜索、通电 / 断电、编辑、删除等操作，支持输入设备编号精准查询。</p><p>支持商户绑定、打印机绑定、设备坐标设置等配置，兼容多商户管理模式，满足跨场景设备统筹需求。</p><p><strong>三、适用场景与行业价值</strong><br/>（一）适用场景<br/>广泛应用于超市、书店等自助结算场景，奶茶店、蛋糕店等零售店铺的自助收银需求，餐厅自助点餐场景，以及酒店、公司的访客登记场景。</p><p>（二）行业价值<br/>对商家而言，无需过多人工干预即可完成收银流程，大幅节省人力成本，提升结算效率，减少顾客排队等待时间。</p><p>支持多设备、跨硬件协同，适配不同规模商户的运营需求，从小型店铺到无人超市均可灵活部署。</p><p>丰富的支付接口与硬件适配能力，降低商家设备升级与支付渠道拓展成本，助力商家快速接入智慧零售生态。</p><p><strong>四、问答环节</strong><br/>问：慧云自助收银系统的交付方式是什么？</p><p>答：采用在线交付与微擎系统交付结合的方式，购买后可获得官方 APP 样版，自主运行需进行重新封装。</p><p>问：组装设备是否支持刷脸支付功能？</p><p>答：组装设备暂不支持刷脸支付，若需使用微信刷脸支付，需将组装设备报送微信支付官方进行检测及认证。</p><p>问：系统支持哪些硬件设备的适配？</p><p>答：支持收银机、扫码仪器、小票打印机、条码打印机等硬件，主板需为安卓 RK3128 及以上（推荐 RK3288 及以上），屏幕尺寸涵盖 10-32 寸。</p><p>问：后台系统可实现哪些管理操作？</p><p>答：可进行设备添加、搜索、编辑、通电 / 断电、删除等操作，支持商户绑定、打印机绑定、设备坐标设置，以及多商户、多设备的统一管理。</p>]]></description></item><item>    <title><![CDATA[蓝图如何自动您的任务管理？ 英勇无比的羽毛球 ]]></title>    <link>https://segmentfault.com/a/1190000047458709</link>    <guid>https://segmentfault.com/a/1190000047458709</guid>    <pubDate>2025-12-08 17:03:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>我们经常会遇到状态相同的任务工作流程。重复的任务流程处理起来很繁琐，对吧？因此，我们推出了任务自动化蓝图。蓝图是一款用于组织任务并根据您设计的工作流程和定义的条件自动执行任务的工具。</p><p><strong>如何将任务与蓝图关联？</strong><br/>您可以在创建蓝图时设置条件，这样，当您发布蓝图时，满足这些条件的任务就会与该蓝图关联。例如，如果您使用项目名称设置蓝图条件，那么在该项目下创建的所有新任务都将与该蓝图关联。</p><p><strong>蓝图有哪些功能？</strong><br/>在蓝图编辑器中，您可以可视化任务流程，并使用状态和转换自定义工作流程。状态是指任务布局中可用的自定义状态。您还可以在蓝图中创建新状态，这些状态仅在发布蓝图时才会保存。但是，您可以将蓝图保存为草稿，直到您完全完成工作流程的设计和配置。</p><p>转换是指连接任意两个状态的链接，当执行转换时，任务会从一个状态推进到另一个状态。</p><p>您可以通过选择可以查看转换的用户或用户角色来配置“转换前”设置。这些用户可以在任务详情页面中看到以按钮形式显示的转换。他们可以点击转换按钮并执行相应的操作。</p><p>蓝图中的转换可以配置为在每次状态更新时更新某些字段。这些字段在“转换期间”设置中进行配置。这样，您可以确保每次任务状态更新都会执行一些操作。您还可以在“转换期间”中配置消息，这些消息可以是信息或说明，将在用户执行转换时显示。</p><p>自动化是蓝图的关键。您可以在配置“转换后”设置时自动执行一些例行操作。当任务状态更新时，用户将收到更新通知。但是，有时您可能不想向所有用户发送相同的任务更新通知电子邮件，对吗？在这种情况下，您可以创建多个包含自定义内容的电子邮件提醒，并分别发送给不同的用户角色或项目/客户用户。</p><p>您还可以更新相关字段。例如，如果您想更新任务完成百分比，可以设置完成百分比值；如果您想在特定任务状态更新时将任务重新分配给某人，可以设置任务负责人。完成转换后，这些字段将自动更新。</p><p><strong>如何执行蓝图？</strong></p><p>发布蓝图后创建且符合蓝图条件的任务将进入蓝图流程。要预览蓝图，请在任务详情页面点击“蓝图预览”按钮。您可以点击转换按钮并使用值更新字段。完成后，任务状态将根据蓝图进行更新。</p>]]></description></item><item>    <title><![CDATA[常见触发器类型解析 星星上的柳树 ]]></title>    <link>https://segmentfault.com/a/1190000047458714</link>    <guid>https://segmentfault.com/a/1190000047458714</guid>    <pubDate>2025-12-08 17:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>“触发器决定了数字电路的“节奏与记忆”。”<br/>在数字系统中，触发器（Flip-Flop）是构建时序逻辑电路的核心元件。它能够存储一个二进制状态，并在时钟或控制信号的作用下改变输出。不同类型的触发器在功能和用途上略有差异：有的仅在特定时钟沿触发状态变化，有的支持置位、复位或翻转操作。理解各种触发器的特性，是掌握寄存器设计、计数器实现以及有限状态机建模的基础。</p><p>1、触发器的基本概念触发器是一种双稳态电路，即电路具有两个稳定输出状态，通常用 Q 表示当前状态，用 Q’ 表示其反相。时序逻辑的关键特性在于：输出不仅取决于当前输入，还取决于历史状态。<br/>在实际设计中，触发器用于存储单个位数据，并根据时钟信号控制其更新。它们是寄存器、计数器、状态机等复杂逻辑的核心组成部分。</p><p>2、D触发器（D Flip-Flop）D触发器是最常见的一种类型，名称中的“D”代表“Data”。它在时钟上升沿时将输入D的值锁存为输出Q的下一状态（Q+）。在时钟信号未触发时，输出保持不变。<br/>逻辑关系：Q⁺ = D<br/>这意味着在时钟沿到来时，输出等于当时输入的值。D触发器非常适合在同步系统中用作数据寄存器，因为它能确保数据只在时钟信号变化的瞬间更新，从而避免毛刺与竞争风险。<br/>D触发器的设计简单、行为稳定，是大多数同步逻辑系统的首选基础单元。</p><p>3、J-K触发器（J-K Flip-Flop）J-K触发器功能更灵活，可实现置位（Set）、复位（Reset）与翻转（Toggle）操作。它的输入端为J和K，输出为Q。其状态转移规则如下：<br/><img width="723" height="249" referrerpolicy="no-referrer" src="/img/bVdnikZ" alt="" title=""/><br/>这种设计解决了早期S-R触发器存在的“无效状态”问题，并且在时钟触发下能灵活切换状态。由于其具备多种功能，J-K触发器常用于计数器和状态机中。<br/>其核心优势在于：只需通过不同的输入组合，即可实现存储、清零与状态反转，大幅提高逻辑利用率。</p><p>4、T触发器（T Flip-Flop）T触发器可看作J-K触发器的简化形式，当J与K输入相同且命名为T时，即形成T型触发器。其工作规则极为简洁：当T = 1时，输出Q翻转（0变1，1变0）；当T = 0时，输出保持不变。<br/>逻辑关系：Q⁺ = T ⊕ Q<br/>这种触发器特别适合用于计数器设计，例如二进制递增计数器。多个T触发器级联，可以实现二进制序列的自动递增。由于结构简单、响应明确，T触发器是实现时钟分频、脉冲计数等应用的关键元件。</p><p>5、S-R触发器（S-R Flip-Flop / Latch）S-R触发器是最早的触发器形式，由两个NOR门交叉连接构成。S表示“Set”（置位），R表示“Reset”（复位）。<br/>其工作规则如下：<br/><img width="723" height="249" referrerpolicy="no-referrer" src="/img/bVdnilf" alt="" title="" loading="lazy"/><br/>当S和R同时为1时，两个输出端都为0，这种状态是不允许的，因此该组合在实际设计中应避免。S-R触发器不依赖时钟信号，是一种电平敏感锁存器（Latch），适合实现简单的控制逻辑或暂存功能。</p><p>6、D锁存器（D Latch）D锁存器是基于S-R锁存器的改进版本，也称为透明锁存器（Transparent D Latch）。它的输入端为D，控制端为G（或称为Enable）。当G=1时，输出Q紧跟输入D变化；当G=0时，Q保持上一次的值。<br/>逻辑关系：当G=1时，Q = D；当G=0时，Q保持不变。<br/>D锁存器常用于需要在某段时间内保持输入值的电路中，例如暂存寄存器。由于它是电平敏感的，不具备严格的时钟同步特性，因此常与D触发器配合使用以构建安全的时序逻辑。</p><p>7、不同触发器的比较<br/><img width="723" height="262" referrerpolicy="no-referrer" src="/img/bVdnilh" alt="" title="" loading="lazy"/><br/>通过合理选择触发器类型，设计者可以在不同场景下平衡逻辑复杂度、功耗和速度。例如，D触发器适合同步寄存；T触发器适用于低功耗计数；而J-K触发器适合多模式控制电路。<br/>触发器是数字系统记忆的核心。从最早的S-R结构到现代同步D触发器，它们共同构建了逻辑电路的“时间维度”。理解各种触发器的差异与用途，能帮助设计者更精准地控制数据流与时序，实现更高效、更可靠的硬件设计。<br/>《EDA网院》出品 · 与全球工程师一起探索芯片的世界</p>]]></description></item><item>    <title><![CDATA[对接印度股票市场数据 (India api) 实时k线图表 CryptoRzz ]]></title>    <link>https://segmentfault.com/a/1190000047458768</link>    <guid>https://segmentfault.com/a/1190000047458768</guid>    <pubDate>2025-12-08 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. 基础参数配置</h2><ul><li><strong>接口域名</strong>: <code>https://api.stocktv.top</code></li><li><strong>印度 Country ID</strong>: <strong>14</strong></li><li><strong>主要交易所</strong>: NSE (National Stock Exchange), BSE (Bombay Stock Exchange)</li><li><strong>认证方式</strong>: URL 参数 <code>key=您的API密钥</code></li></ul><hr/><h2>2. 核心接口流程</h2><p>对接逻辑：先通过 <strong>列表接口</strong> 查询印度股票的 PID（系统ID），再使用 PID 获取 <strong>K线</strong> 或 <strong>实时行情</strong>。</p><h3>第一步：获取印度股票列表</h3><p>查询印度市场的股票代码、名称及 PID。</p><ul><li><strong>接口</strong>: <code>/stock/stocks</code></li><li><strong>方法</strong>: <code>GET</code></li><li><p><strong>参数</strong>:</p><ul><li><code>countryId</code>: <strong>14</strong> (必填)</li><li><code>pageSize</code>: <code>10</code></li><li><code>key</code>: <code>您的Key</code></li></ul></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/stocks?countryId=14&amp;pageSize=10&amp;page=1&amp;key=YOUR_KEY</code></pre></li><li><p><strong>预期数据</strong>:</p><ul><li><code>id</code>: <strong>PID</strong> (后续接口使用)</li><li><code>symbol</code>: 股票代码 (如 "RELIANCE", "TCS", "INFY")</li><li><code>name</code>: 公司名称</li><li><code>exchangeId</code>: 交易所ID (46=NSE, 74=BSE)</li></ul></li></ul><h3>第二步：获取印度指数 (Nifty 50 / Sensex)</h3><p>获取印度主要的 <strong>Nifty 50</strong> 和 <strong>BSE Sensex</strong> 指数行情。</p><ul><li><strong>接口</strong>: <code>/stock/indices</code></li><li><strong>方法</strong>: <code>GET</code></li><li><strong>参数</strong>: <code>countryId=14</code></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/indices?countryId=14&amp;key=YOUR_KEY</code></pre></li></ul><h3>第三步：获取 K 线数据</h3><p>使用第一步获取的 <code>id</code> (PID) 查询历史数据。</p><ul><li><strong>接口</strong>: <code>/stock/kline</code></li><li><strong>方法</strong>: <code>GET</code></li><li><p><strong>参数</strong>:</p><ul><li><code>pid</code>: <strong>股票ID</strong></li><li><code>interval</code>: <strong>周期</strong> (<code>P1D</code>=日线, <code>PT1H</code>=1小时, <code>PT15M</code>=15分钟)</li></ul></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/kline?pid=12345&amp;interval=P1D&amp;key=YOUR_KEY</code></pre></li></ul><h3>第四步：涨跌排行榜 (可选)</h3><p>获取印度市场的涨幅榜或跌幅榜。</p><ul><li><strong>接口</strong>: <code>/stock/updownList</code></li><li><strong>方法</strong>: <code>GET</code></li><li><p><strong>参数</strong>:</p><ul><li><code>countryId</code>: <strong>14</strong></li><li><code>type</code>: <code>1</code> (涨幅榜) 或 <code>2</code> (跌幅榜)</li></ul></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/updownList?countryId=14&amp;type=1&amp;key=YOUR_KEY</code></pre></li></ul><hr/><h2>3. 完整代码示例 (HTML + KlineCharts)</h2><p>这是一个可以直接运行的 HTML 文件示例。它会自动请求印度股票列表，打印到控制台，并允许您输入 PID 来渲染 K 线图。</p><pre><code class="html">&lt;!DOCTYPE html&gt;
&lt;html lang="zh-CN"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;印度股票 K线演示 (CountryID=14)&lt;/title&gt;
    &lt;script src="https://cdn.jsdelivr.net/npm/klinecharts/dist/klinecharts.min.js"&gt;&lt;/script&gt;
    &lt;style&gt;
        body { font-family: sans-serif; padding: 20px; }
        .control-panel { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 8px; }
        .log-panel { background: #333; color: #0f0; padding: 10px; height: 100px; overflow-y: scroll; font-family: monospace; margin-bottom: 10px; }
        #chart { width: 100%; height: 500px; border: 1px solid #ccc; }
        button { padding: 8px 15px; cursor: pointer; background: #007bff; color: white; border: none; border-radius: 4px; }
        input { padding: 8px; width: 200px; }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;

    &lt;h2&gt;StockTV 印度市场对接 (ID: 14)&lt;/h2&gt;

    &lt;div class="control-panel"&gt;
        &lt;p&gt;1. &lt;strong&gt;获取列表&lt;/strong&gt;：点击按钮获取印度股票列表，查看控制台或下方日志获取 PID。&lt;/p&gt;
        &lt;button onclick="fetchIndiaList()"&gt;获取印度股票列表&lt;/button&gt;
        &lt;hr&gt;
        &lt;p&gt;2. &lt;strong&gt;渲染K线&lt;/strong&gt;：输入 PID 查看图表。&lt;/p&gt;
        &lt;input type="text" id="pidInput" placeholder="请输入股票 PID (例如: 12345)"&gt;
        &lt;button onclick="renderChart()"&gt;生成 K 线图&lt;/button&gt;
    &lt;/div&gt;

    &lt;div class="log-panel" id="logPanel"&gt;等待操作...&lt;/div&gt;
    &lt;div id="chart"&gt;&lt;/div&gt;

    &lt;script&gt;
        // === 配置区域 ===
        const API_KEY = 'YOUR_API_KEY'; // 请在此填入您的 Key
        const COUNTRY_ID = 14;          // 印度 Country ID
        const BASE_URL = 'https://api.stocktv.top';

        // 初始化图表
        const chart = klinecharts.init('chart');

        // 日志辅助函数
        function log(msg) {
            const panel = document.getElementById('logPanel');
            panel.innerHTML += `&lt;div&gt;&gt; ${msg}&lt;/div&gt;`;
            panel.scrollTop = panel.scrollHeight;
            console.log(msg);
        }

        // 1. 获取股票列表
        async function fetchIndiaList() {
            const url = `${BASE_URL}/stock/stocks?countryId=${COUNTRY_ID}&amp;pageSize=10&amp;page=1&amp;key=${API_KEY}`;
            log(`正在请求列表: ${url}`);
            
            try {
                const res = await fetch(url);
                const json = await res.json();
                
                if (json.code === 200 &amp;&amp; json.data.records) {
                    log(`获取成功! 共有 ${json.data.total} 条数据。`);
                    log("--- 前3条示例 ---");
                    json.data.records.slice(0, 3).forEach(stock =&gt; {
                        log(`名称: ${stock.name} | 代码: ${stock.symbol} | PID: ${stock.id}`);
                    });
                    log("------------------");
                    
                    // 自动填充第一个PID方便测试
                    if(json.data.records.length &gt; 0) {
                        document.getElementById('pidInput').value = json.data.records[0].id;
                        log(`已自动填充示例 PID: ${json.data.records[0].id}`);
                    }
                } else {
                    log("错误: " + json.message);
                }
            } catch (err) {
                log("网络请求失败");
                console.error(err);
            }
        }

        // 2. 渲染 K 线
        async function renderChart() {
            const pid = document.getElementById('pidInput').value;
            if(!pid) return alert('请输入 PID');

            // 请求日线数据 P1D
            const url = `${BASE_URL}/stock/kline?pid=${pid}&amp;interval=P1D&amp;key=${API_KEY}`;
            log(`请求 K 线: PID=${pid}`);

            try {
                const res = await fetch(url);
                const json = await res.json();

                if (json.code === 200 &amp;&amp; json.data) {
                    // 数据格式转换 StockTV -&gt; KlineCharts
                    const dataList = json.data.map(item =&gt; ({
                        timestamp: item.time,
                        open: Number(item.open),
                        high: Number(item.high),
                        low: Number(item.low),
                        close: Number(item.close),
                        volume: Number(item.volume)
                    }));
                    
                    // 排序
                    dataList.sort((a, b) =&gt; a.timestamp - b.timestamp);
                    
                    chart.applyNewData(dataList);
                    log(`图表已更新，加载数据 ${dataList.length} 条`);
                } else {
                    log("无 K 线数据或 API 报错");
                }
            } catch (err) {
                log("请求 K 线失败");
                console.error(err);
            }
        }
    &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre><h2>4. 常见印度蓝筹股 (供参考)</h2><p>如果在测试时需要验证数据，可以在列表中留意以下代码：</p><ul><li><strong>RELIANCE</strong>: Reliance Industries</li><li><strong>TCS</strong>: Tata Consultancy Services</li><li><strong>HDFCBANK</strong>: HDFC Bank</li><li><strong>INFY</strong>: Infosys</li><li><strong>ICICIBANK</strong>: ICICI Bank</li></ul>]]></description></item><item>    <title><![CDATA[做外贸如何合法使用国外软件？有哪些解决方案？ 明点跨境OSDWAN ]]></title>    <link>https://segmentfault.com/a/1190000047458315</link>    <guid>https://segmentfault.com/a/1190000047458315</guid>    <pubDate>2025-12-08 16:04:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在外贸行业，很多工作都依赖海外平台和工具，比如 Google、Meta Ads、WhatsApp、LinkedIn、Tik Tok等。然而不少外贸人发现：这些平台在国内无法直接访问、加载缓慢、后台卡顿，甚至会频繁掉线。</p><p>想要高效工作，首先就得解决“合法、稳定访问国外软件”的问题。</p><p>下面为大家讲清楚：外贸人如何合规、稳定地访问海外工具，并给出最适合企业和团队使用的解决方案。</p><p>一、关于做外贸如何使用合法的网络</p><p>外贸业务本质是面向海外市场，因此访问国外网站、社交平台、广告平台是“刚需”。</p><p>但很多人为了图方便，会选择一些免费的、不合规的工具，结果不仅不稳定，还可能让账号风险增加、广告投放失败、甚至严重影响企业业务。</p><p>合法使用国外软件的核心原则只有两点：</p><ul><li>必须使用正规渠道提供的跨境网络服务</li><li>必须保证访问线路、安全、来源合法<br/>只有这样，才能保证海外平台正常加载，同时不会给企业带来风险。</li></ul><p><img width="723" height="230" referrerpolicy="no-referrer" src="/img/bVdnieO" alt="image.png" title="image.png"/></p><p>⚠ 注意：避免使用免费网络或不合规工具</p><p>特别提醒新手：</p><p>小火箭、R2Vay等免费或不合规的网络工具，不仅不稳定，而且风险极高。</p><p>容易被平台识别为异常IP</p><p>广告后台、WhatsApp、TikTok 账号容易被限制</p><p>可能泄露企业隐私数据</p><p>可能对公司带来不可控的违规风险</p><p>外贸行业本来就需要长期品牌建设，千万不要因为贪便宜而“得不偿失”。</p><p>二、合法的外贸网络专线有哪些?</p><p>目前合法、稳定、适合外贸企业使用的跨境网络，主要有两种：</p><p>1、传统国际网络专线（MPLS/IEPL）</p><p>特点：</p><p>由运营商提供<br/>稳定性强、质量高<br/>带宽大、安全性强<br/>专线点对点接入<br/>适用场景：</p><p>适合大型外贸企业、集团公司、跨国分支机构，以及需要大量数据传输的团队。</p><p>缺点：</p><p>开通流程复杂、价格高、灵活性不够，所以大部分企业不会选择。</p><p>2、SD-WAN国际网络专线（更适合外贸团队）</p><p>这是目前外贸行业使用率最高、性价比最好的一种方式。</p><p>它的优势主要体现在：</p><p>成本比传统专线低<br/>开通快、灵活性高<br/>可以自由切换多国节点，用于 Facebook、Google、WhatsApp、TikTok 等不同业务<br/>网络稳定，不会像“普通网络”一样丢包、延迟、卡顿<br/>合规部署、安全可靠<br/>对跨境电商、外贸独立站、海外营销团队来说，SD-WAN 已经是主流选择。</p><p>三、做外贸如何合法使用国外软件？以OSDWAN为例</p><p>为了兼顾合法性、稳定性与成本，现在外贸企业最常用、最省心的方式就是：使用合规的 SD-WAN 国际网络专线，例如 OSDWAN。</p><p>为什么推荐 OSDWAN？</p><p>合规 SD-WAN 网络，安全可靠，通过工信部备案的拥有合法资质的，走的是和电信一样的线路。</p><p>支持多国家节点：美国、欧洲、东南亚、中东等<br/>稳定访问各类海外平台：Google、WhatsApp、Facebook、TikTok、Shopify、独立站后台等<br/>线路低延迟不卡顿，不会掉线<br/>提供企业级管理后台，多账号分配更方便<br/>支持团队共用、办公、广告投放、运营等全场景<br/>无论你是外贸公司、工厂、跨境电商团队、独立站团队，都可以放心使用。<br/>开通流程一般是：</p><p>联系顾问 → 说明用途（ChatGPT等AI访问 / 外贸办公 / 社媒运营等）<br/>选择线路节点（美国、新加坡、日本等）<br/>开通账号，下载软件并登录OSDWAN</p><p><img width="723" height="474" referrerpolicy="no-referrer" src="/img/bVdm3df" alt="image.png" title="image.png" loading="lazy"/></p><p>四、SD-WAN 国际网络专线哪家好?</p><p>判断一个跨境网络服务商是否靠谱，可以参考以下几点：</p><p>是否合规、安全可查<br/>线路是否稳定（丢包率、延迟）<br/>是否有多个国家节点，满足不同业务需求<br/>是否有企业后台、权限管理功能<br/>售后是否及时（外贸行业经常需要跨时区工作）<br/>从外贸客户的使用反馈来看，OSDWAN 属于目前行业中专业性较强的服务商，稳定性和售后支持都比较到位，适合正在选择网络方案的团队。</p><p>五、常见问答(FAQ)</p><ol><li>做外贸必须使用专线吗？</li></ol><p>如果你只偶尔打开海外网站，可以不用专线;</p><p>但如果你做推广、广告投放、社媒运营、客户沟通，就强烈建议使用。</p><ol start="2"><li>使用不合规的工具会有什么影响？</li></ol><p>账号不稳定、登录异常、广告被限制、邮箱延迟、数据泄露风险，对外贸账号来说，这些都是不可逆损失。</p><ol start="3"><li>SD-WAN 和 VPN 有什么区别？</li></ol><p>SD-WAN 属于企业级网络专线，合规、稳定、可控，V*N 属于非法工具，不适合外贸业务场景</p><ol start="4"><li>一个公司能否多台设备同时使用？</li></ol><p>可以，OSDWAN 支持多设备、多用户接入。</p><ol start="5"><li>支持哪些国家的网络？</li></ol><p>英国、美国、日本、韩国、新加坡、中东、东南亚、欧洲等主流外贸国家都可以覆盖。</p><p>OSDWAN兼具合规合法、稳定安全、简单易用、高性价比等优势，支持一键访问全球互联网。是企业办公、网络营销、跨境直播的不二之选。</p>]]></description></item>  </channel></rss>