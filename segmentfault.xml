<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[“Linux之父AI观导图”解构实录：理性剖析AI泡沫的每个层面 图形天下 ]]></title>    <link>https://segmentfault.com/a/1190000047598593</link>    <guid>https://segmentfault.com/a/1190000047598593</guid>    <pubDate>2026-02-07 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="1068" referrerpolicy="no-referrer" src="/img/bVdnSJo" alt="" title=""/><br/>                “Linux之父把AI泡沫喷了个遍”思维导图</p><p><a href="https://link.segmentfault.com/?enc=H9%2BoWhZe%2BJ3VBLueozSAAw%3D%3D.kbka5NsUnreOtvcFzoVMSL5In7KWI%2FPuYY6%2BtZQDw3m0Ef7%2FpAXWOYr2GmJrS7TldUQzOwrWbGQpayO38BsQ3PldsLxhp3N7jO4cZY4ji7M%3D" rel="nofollow" target="_blank">“Linux之父把AI泡沫喷了个遍”思维导图模板获取链接</a></p><h2>一、核心主题确定</h2><p>确定核心主题为“Linux之父把AI泡沫喷了个遍”，围绕这一主题，收集和整理Linux之父Linus Torvalds对AI的看法、AI的发展现状、优缺点、炒作周期、分类与作用、未来预测等相关内容，形成思维导图的核心内容框架。</p><h2>二、导图结构设计</h2><ol><li><strong>博文核心观点</strong>：作为思维导图的主要分支，涵盖对AI炒作的态度、AI发展现状、AI的优点、AI炒作周期分析、AI的分类与作用、AI的未来预测六个子分支，每个子分支下再细分具体观点和内容。</li><li><strong>博文观点分析</strong>：对博文核心观点进行合理性及局限性两方面的分析，每个方面下再细分具体分析内容，形成逻辑严密的论证结构。</li><li><strong>个人观点补充</strong>：包含对AI炒作的理解以及对AI未来发展的期待两个子分支，每个子分支下同样细分具体观点，展现个人对AI领域的深入思考。</li></ol><h2>三、导图样式设计</h2><ol><li><strong>颜色搭配</strong>：采用绿色作为背景色，给人清新、科技感的视觉感受；不同层级的文字和分支使用不同颜色进行区分，如核心主题用黑色加粗字体，一级分支用深绿色背景白色字体，二级及以下分支用浅绿色背景黑色字体保持视觉一致性。可借鉴图形天下思维导图提供的<strong>17套配色方案</strong>，选择适合科技主题的配色，增强视觉吸引力。</li><li><strong>形状布局</strong>：整体采用<strong>树状表格</strong>布局，从核心主题向右侧延伸出主要分支，各分支下的子内容以列表形式呈现，层次分明，逻辑清晰。图形天下思维导图提供的<strong>12类42种图形布局</strong>，可根据内容特点灵活选择，使布局更加专业和有条理。</li><li><strong>字体和字号</strong>：选择简洁易读的字体，核心主题字号最大，一级分支字号次之，二级及以下分支字号相对较小，通过字号大小体现内容的层级关系。</li></ol><p><a href="https://link.segmentfault.com/?enc=WGrXL1FXxph021hl%2Fn9adQ%3D%3D.TVQQIkYrOFhygEeQK0YOyYFiSfcu%2FzgwZ3q6vPob2EuQCgQuSi%2FXyBDbnTMr2upsUTxTuKFX%2BSIkoKeumCbYMtPsmUQmG7YMOalxz0B8hLCQSqDxtD63MXPi6axjOLqu" rel="nofollow" target="_blank">“Linux之父把AI泡沫喷了个遍”思维导图模板在线免费体验链接</a></p><h2>四、导图工具与流程</h2><ul><li><strong>工具选择</strong>：使用图形天下思维导图软件，该软件不仅提供了丰富的模板、图标、颜色设置等功能，还支持<strong>多模态AI生成思维导图</strong>，能极大提升创作效率。</li><li><p><strong>创作流程</strong></p><ul><li><strong>收集资料</strong>：查阅Linus Torvalds关于AI的相关言论、报道以及AI领域的发展现状、技术分析等资料。</li><li><strong>整理内容</strong>：对收集到的资料进行整理和归纳，提取关键信息，形成各个分支下的具体内容。</li><li><strong>创建导图</strong>：在图形天下思维导图软件中，先输入核心主题，然后依次创建一级分支、二级分支等。</li><li><strong>样式调整</strong>：利用软件的<strong>树型表格</strong>布局，将博文核心观点下的各子分支及其内容以表格形式清晰呈现。同时，利用软件提供的<strong>17套配色方案</strong>对导图的颜色进行调整。</li><li><strong>检查完善</strong>：检查导图内容是否完整、逻辑是否连贯、有无错别字等，对不足之处进行修改和完善。</li></ul></li></ul><p><a href="https://link.segmentfault.com/?enc=RU9aZFQ15%2BDZOKa8XM15hA%3D%3D.n1Wv4TDGYz%2FC5fnRsnKgACQz%2FegymhN3gL2HSjcgCjwmqnr5Ia7gXE7gKTLlNj1MWi6QdGutTaJ%2F4DsAZN0%2Bsw%3D%3D" rel="nofollow" target="_blank">图形天下思维导图软件免费下载链接</a></p><h2>五、总结</h2><p>在本次思维导图的创作过程中，通过运用图形天下思维导图软件的<strong>树型表格</strong>布局，成功将复杂的内容以清晰、有条理的方式呈现出来。同时，借助软件提供的<strong>配色方案</strong>和<strong>预设风格</strong>，使导图在视觉上更加吸引人。整个创作流程高效顺畅，充分展现了图形天下思维导图软件在知识管理和思维可视化方面的强大能力。</p><p>访问图形天下思维导图<strong>模板库</strong>与<strong>教程资源</strong>，获取更多免费导图素材与实操指南，激发你的无限创意。</p><ul><li><a href="https://link.segmentfault.com/?enc=hcXyBQ802vRQIrjeKK4T6g%3D%3D.ZJUqaHdNzGmCf8AdjoY96AutS%2BNo%2FjIjoio6wUg%2FLZDpB9xJDuqNeAj0qttpk9WlsWYtWqQ%2B1PEXUpHIxruneA%3D%3D" rel="nofollow" target="_blank">思维导图模板库</a></li><li><a href="https://link.segmentfault.com/?enc=fbC7uJHVi49ZTktdbC7vaA%3D%3D.TTh0%2Bb1O2nCswxyLuK%2BFN7WxVJ9JoumgxyZuJl2v%2FPMY4svKeIoJbr7z02TcJooh9xdSQ5EXaCbxQRFdixn5uw%3D%3D" rel="nofollow" target="_blank">思维导图使用教程资源</a></li></ul>]]></description></item><item>    <title><![CDATA[Scrapy框架入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047598548</link>    <guid>https://segmentfault.com/a/1190000047598548</guid>    <pubDate>2026-02-07 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. Scrapy的概览与核心价值</h2><p>想象一下,如果你需要从成千上万个网页中提取结构化数据,用传统的<code>requests</code> + <code>BeautifulSoup</code>方式就像用勺子挖土——虽然可行,但效率低下且难以维护。<code>Scrapy</code>正是为解决大规模、高性能数据抓取需求而生的工业级爬虫框架。</p><p>在Python生态系统中,Scrapy占据了不可替代的地位。它不仅仅是一个爬虫库,更是一个完整的爬虫开发框架,将数据抓取的整个流程——从请求调度、网页下载、数据提取到持久化存储——封装成了一套标准化的流水线系统。这种模块化设计让开发者能够专注于"爬什么"而非"怎么爬",极大提升了开发效率。</p><p>Scrapy的独特价值在于其基于Twisted异步网络框架的事件驱动架构,能够以单线程实现高并发请求处理,在不增加硬件资源的前提下获得10倍于传统爬虫的抓取速度。同时,它内置的请求去重、自动重试、用户代理轮换等反爬机制,让开发者能够快速构建稳定可靠的爬虫系统。</p><h2>2. 环境搭建与"Hello, World"</h2><h3>安装Scrapy</h3><p>Scrapy支持Python 3.7及以上版本,推荐使用Python 3.8+以获得最佳兼容性。安装方式如下:</p><pre><code class="bash"># 使用pip安装(推荐使用国内镜像源加速)
pip install scrapy -i https://pypi.douban.com/simple

# 验证安装是否成功
scrapy version</code></pre><p>如果看到类似<code>Scrapy 2.11.0</code>的版本号输出,说明安装成功。对于Windows用户,可能需要先安装Microsoft Visual C++ Build Tools以解决某些依赖包的编译问题。</p><h3>第一个Scrapy爬虫</h3><p>让我们创建一个最简单的爬虫来抓取quotes.toscrape.com网站的励志名言:</p><pre><code class="python">import scrapy

class QuotesSpider(scrapy.Spider):
    # 爬虫的唯一标识符
    name = 'quotes'
    
    # 起始URL列表
    start_urls = ['http://quotes.toscrape.com/page/1/']
    
    def parse(self, response):
        # 遍历页面中的每个名言
        for quote in response.css('div.quote'):
            # 提取名言内容、作者和标签
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('a.tag::text').getall(),
            }
        
        # 查找下一页链接并继续爬取
        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)</code></pre><p><strong>代码逐行解析:</strong></p><ul><li><code>class QuotesSpider(scrapy.Spider)</code>: 继承Scrapy的Spider基类,所有自定义爬虫都必须这样做</li><li><code>name = 'quotes'</code>: 定义爬虫名称,运行爬虫时会用到这个标识符,必须在项目中唯一</li><li><code>start_urls = [...]</code>: 定义爬虫的起始URL列表,Scrapy会自动为每个URL创建请求</li><li><code>def parse(self, response)</code>: 默认的回调函数,处理响应的函数名固定为parse(除非你指定其他回调)</li><li><code>response.css(...)</code>: 使用CSS选择器提取数据,Scrapy支持CSS和XPath两种选择器</li><li><code>yield {...}</code>: 生成字典数据,这些数据会被传递给Item Pipeline进行后续处理</li><li><code>response.follow()</code>: 创建新的请求来跟进链接,第一个参数是URL,第二个参数是回调函数</li></ul><p><strong>运行结果:</strong></p><p>在终端中执行以下命令运行爬虫:</p><pre><code class="bash">scrapy crawl quotes -o quotes.json</code></pre><p>运行后,Scrapy会自动从第一页开始抓取,提取每条名言的信息,并自动翻页直到抓取完所有页面。最终数据会保存在<code>quotes.json</code>文件中,格式如下:</p><pre><code class="json">[
    {
        "text": "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”",
        "author": "Albert Einstein",
        "tags": ["change", "deep-thoughts", "thinking", "world"]
    },
    ...
]</code></pre><h2>3. 核心概念解析</h2><p>Scrapy的核心架构围绕几个关键组件展开,理解这些组件的职责和交互方式是掌握Scrapy的关键。</p><h3>3.1 Spider(爬虫)</h3><p>Spider是用户编写的核心逻辑模块,定义了:</p><ul><li>如何爬取网站(起始URL、如何跟进链接)</li><li>如何解析页面内容(提取数据)</li><li>如何处理提取到的数据(生成Item或新的Request)</li></ul><p>每个Spider必须继承<code>scrapy.Spider</code>基类,并至少实现<code>parse()</code>方法。Spider的典型工作流程是:接收Response对象 → 解析页面 → 提取数据或生成新Request → yield出去。</p><h3>3.2 Item(数据项)</h3><p>Item是Scrapy提供的数据容器,类似于Python字典但提供了字段验证功能。通过预定义数据结构,Item能够避免字段拼写错误和类型混乱。</p><pre><code class="python">import scrapy

class QuoteItem(scrapy.Item):
    text = scrapy.Field()
    author = scrapy.Field()
    tags = scrapy.Field()</code></pre><p>使用Item的好处包括:</p><ul><li>字段定义清晰,便于团队协作</li><li>支持数据验证和类型检查</li><li>与Pipeline配合,实现数据清洗的标准化流程</li></ul><h3>3.3 Pipeline(管道)</h3><p>Pipeline负责处理Spider提取的Item,典型操作包括:</p><ul><li>数据清洗(去除空格、转换格式)</li><li>数据验证(检查必填字段)</li><li>数据去重(避免重复存储)</li><li>持久化存储(存入数据库或文件)</li></ul><pre><code class="python">class CleanPipeline:
    def process_item(self, item, spider):
        # 去除文本首尾空格
        item['text'] = item['text'].strip()
        return item

class DatabasePipeline:
    def __init__(self):
        self.db_conn = None
    
    def open_spider(self, spider):
        # 爬虫启动时建立数据库连接
        self.db_conn = create_database_connection()
    
    def process_item(self, item, spider):
        # 将item存入数据库
        self.db_conn.insert(item)
        return item
    
    def close_spider(self, spider):
        # 爬虫关闭时释放资源
        self.db_conn.close()</code></pre><h3>核心组件关系图</h3><pre style="display:none;"><code class="mermaid">graph TD
    A[Spider] --&gt;|生成Request| B[Engine]
    B --&gt;|传递Request| C[Scheduler]
    C --&gt;|返回待爬Request| B
    B --&gt;|传递Request| D[Downloader]
    D --&gt;|返回Response| B
    B --&gt;|传递Response| A
    A --&gt;|yield Item| B
    B --&gt;|传递Item| E[Pipeline]
    A --&gt;|yield新Request| B
    E --&gt;|处理Item| F[Database/File]
    
    style A fill:#e1f5ff
    style B fill:#fff4e1
    style C fill:#ffe1e1
    style D fill:#e1ffe1
    style E fill:#f0e1ff</code></pre><p>Scrapy的工作流程是一个闭环:Spider生成初始Request → Engine调度 → Scheduler排队 → Downloader下载 → Engine传递响应 → Spider解析 → 提取数据或生成新Request → 循环往复。</p><h2>4. 实战演练:解决一个典型问题</h2><p>让我们通过一个完整的项目来实战Scrapy的核心功能。我们将爬取豆瓣电影Top250的信息,包括电影名称、评分、导演和简介。</p><h3>需求分析</h3><p>目标网站:<a href="https://link.segmentfault.com/?enc=w4rxo%2BIzYU7KwzTopHRZLw%3D%3D.bCAsakuiv5jzzo0DdKTvbJUTYUqeOjwtilxGE9BPobA%3D" rel="nofollow" target="_blank">https://movie.douban.com/top250</a>  <br/>需要提取的数据:电影标题、评分、导演、简介  <br/>特殊需求:实现翻页功能,爬取所有250部电影</p><h3>方案设计</h3><p>选择Scrapy的原因:</p><ul><li>高效的异步并发能力,能够快速爬取25页数据</li><li>内置的Request去重机制,避免重复爬取</li><li>灵活的Pipeline设计,便于数据清洗和存储</li></ul><p>技术方案:</p><ul><li>使用CSS选择器提取数据</li><li>通过翻页链接的规律实现自动翻页</li><li>将数据保存为CSV文件便于后续分析</li></ul><h3>代码实现</h3><p><strong>步骤1: 创建项目</strong></p><pre><code class="bash">scrapy startproject douban_movie
cd douban_movie</code></pre><p><strong>步骤2: 定义数据结构(items.py)</strong></p><pre><code class="python">import scrapy

class MovieItem(scrapy.Item):
    title = scrapy.Field()    # 电影标题
    rating = scrapy.Field()   # 评分
    director = scrapy.Field() # 导演
    intro = scrapy.Field()    # 简介</code></pre><p><strong>步骤3: 编写爬虫(spiders/movie_spider.py)</strong></p><pre><code class="python">import scrapy
from douban_movie.items import MovieItem

class MovieSpider(scrapy.Spider):
    name = 'douban_top250'
    allowed_domains = ['douban.com']
    start_urls = ['https://movie.douban.com/top250']
    
    def parse(self, response):
        # 提取当前页的所有电影条目
        movie_list = response.css('ol.grid_view li')
        
        for movie in movie_list:
            item = MovieItem()
            
            # 提取电影标题(可能存在中英文名,取第一个)
            item['title'] = movie.css('span.title::text').get()
            
            # 提取评分
            item['rating'] = movie.css('span.rating_num::text').get()
            
            # 提取导演信息
            info = movie.css('div.bd p::text').getall()
            if info:
                director_info = info[0].strip()
                # 导演信息格式:导演: 张三 主演: 李四 王五
                item['director'] = director_info.split('主演:')[0].replace('导演:', '').strip()
            
            # 提取简介(可能不存在)
            item['intro'] = movie.css('span.inq::text').get() or '暂无简介'
            
            yield item
        
        # 处理翻页
        next_page = response.css('span.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)</code></pre><p><strong>步骤4: 配置settings.py</strong></p><pre><code class="python"># 模拟浏览器User-Agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# 不遵守robots协议(豆瓣的robots.txt禁止爬取)
ROBOTSTXT_OBEY = False

# 下载延迟,避免被封IP
DOWNLOAD_DELAY = 2

# 启用Pipeline
ITEM_PIPELINES = {
    'douban_movie.pipelines.DoubanMoviePipeline': 300,
}</code></pre><h3>运行说明</h3><p>执行以下命令启动爬虫:</p><pre><code class="bash">scrapy crawl douban_top250 -o movies.csv</code></pre><p>运行过程中你会看到类似以下的日志输出:</p><pre><code>2024-06-15 10:00:00 [scrapy.core.engine] INFO: Spider opened
2024-06-15 10:00:02 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://movie.douban.com/top250&gt;
2024-06-15 10:00:04 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://movie.douban.com/top250?start=25&amp;filter=&gt;
...
2024-06-15 10:01:30 [scrapy.statscollectors] INFO: Closing spider (finished)</code></pre><p>爬取完成后,<code>movies.csv</code>文件将包含所有250部电影的信息:</p><pre><code class="csv">title,rating,director,intro
肖申克的救赎,9.7,导演: 弗兰克·德拉邦特,希望让人自由。
霸王别姬,9.6,导演: 陈凯歌,风华绝代。
阿甘正传,9.5,导演: 罗伯特·泽米吉斯,人生就像一盒巧克力。
...</code></pre><p>整个爬取过程大约需要1-2分钟,相比传统串行爬虫速度提升了数倍。Scrapy自动处理了并发、去重、重试等复杂问题,让我们能够专注于数据提取逻辑本身。</p><h2>5. 最佳实践与常见陷阱</h2><h3>5.1 常见错误及规避方法</h3><p><strong>错误1: 覆盖parse方法导致CrawlSpider失效</strong></p><pre><code class="python"># ❌ 错误做法
class MySpider(CrawlSpider):
    name = 'my_spider'
    rules = [Rule(LinkExtractor(), callback='parse')]
    
    def parse(self, response):
        # 自定义parse方法会覆盖CrawlSpider的内置逻辑
        pass</code></pre><pre><code class="python"># ✅ 正确做法
class MySpider(CrawlSpider):
    name = 'my_spider'
    rules = [Rule(LinkExtractor(), callback='parse_item')]
    
    def parse_item(self, response):
        # 使用不同的回调函数名
        pass</code></pre><p><strong>错误2: 忘记返回Item导致Pipeline无法接收数据</strong></p><pre><code class="python"># ❌ 错误做法
def process_item(self, item, spider):
    self.db.insert(item)
    # 忘记返回item,后续Pipeline无法接收到数据</code></pre><pre><code class="python"># ✅ 正确做法
def process_item(self, item, spider):
    self.db.insert(item)
    return item  # 必须返回item或抛出DropItem</code></pre><p><strong>错误3: 直接修改Request的meta中保留键</strong></p><pre><code class="python"># ❌ 错误做法
yield scrapy.Request(url, callback=self.parse, meta={'redirect_urls': [...]})</code></pre><pre><code class="python"># ✅ 正确做法
yield scrapy.Request(url, callback=self.parse, meta={'custom_data': {...}})
# 避免使用Scrapy保留的meta键名,如redirect_urls、cookiejar等</code></pre><h3>5.2 最佳实践建议</h3><p><strong>1. 合理设置下载延迟</strong></p><pre><code class="python"># 根据目标网站的负载能力调整延迟
DOWNLOAD_DELAY = 2  # 对于豆瓣这样的网站,2秒较为合理
AUTOTHROTTLE_ENABLED = True  # 启用自动限速</code></pre><p><strong>2. 使用Item Loader简化数据提取</strong></p><pre><code class="python">from scrapy.loader import ItemLoader

def parse(self, response):
    loader = ItemLoader(item=MovieItem(), response=response)
    loader.add_css('title', 'span.title::text')
    loader.add_css('rating', 'span.rating_num::text')
    yield loader.load_item()</code></pre><p><strong>3. 配置日志级别便于调试</strong></p><pre><code class="python"># 开发环境使用DEBUG级别
LOG_LEVEL = 'DEBUG'

# 生产环境使用INFO或WARNING级别
LOG_LEVEL = 'INFO'</code></pre><p><strong>4. 使用管道链处理复杂数据流</strong></p><pre><code class="python">ITEM_PIPELINES = {
    'myproject.pipelines.ValidationPipeline': 100,  # 数据验证
    'myproject.pipelines.DeduplicationPipeline': 200,  # 去重
    'myproject.pipelines.StoragePipeline': 300,  # 存储
}</code></pre><h3>5.3 注意事项</h3><ul><li><strong>遵守robots协议</strong>:虽然可以设置<code>ROBOTSTXT_OBEY = False</code>,但建议尽量遵守网站的robots.txt规定,做一个文明的爬虫</li><li><strong>控制并发数</strong>:默认并发数为16,对于小型网站建议降低到8或更低,避免给服务器造成过大压力</li><li><strong>处理异常</strong>:在parse方法中使用try-except捕获异常,避免个别页面解析失败导致整个爬虫中断</li><li><strong>善用Scrapy Shell</strong>:使用<code>scrapy shell URL</code>命令调试选择器,确保提取逻辑正确后再写入爬虫代码</li><li><strong>监控爬虫状态</strong>:使用Scrapy提供的stats collector监控爬虫运行状态,及时发现异常</li></ul><h2>6. 进阶指引</h2><p>掌握了Scrapy的基础用法后,你可以继续探索以下高级特性:</p><p><strong>1. 中间件(Middleware)</strong>  <br/>中间件提供了在请求/响应处理过程中插入自定义逻辑的钩子。典型应用场景包括:</p><ul><li>动态切换User-Agent和代理IP</li><li>实现请求重试和异常处理</li><li>修改请求头和响应内容</li></ul><p><strong>2. 分布式爬虫</strong>  <br/>通过<code>scrapy-redis</code>扩展,可以实现分布式爬虫,多个爬虫节点共享同一个Redis队列,协同处理大规模爬取任务。</p><p><strong>3. 动态网页渲染</strong>  <br/>对于需要JavaScript渲染的页面,可以集成<code>scrapy-splash</code>或<code>scrapy-playwright</code>,实现动态内容的抓取。</p><p><strong>4. 数据存储扩展</strong>  <br/>除了CSV和JSON,Scrapy Pipeline可以轻松对接各种数据库:</p><ul><li>MySQL/PostgreSQL:使用<code>pymysql</code>或<code>psycopg2</code>驱动</li><li>MongoDB:使用<code>pymongo</code>驱动</li><li>Redis:使用<code>redis</code>驱动</li></ul><p><strong>学习资源推荐:</strong></p><ul><li><strong>官方文档</strong>:<a href="https://link.segmentfault.com/?enc=%2B%2BTQML3ik1jXKl6vp5fBRA%3D%3D.2wFpNZ8de4Lf6nZP6jNO2iMxV6Bi%2Ba%2FkaMvFLEBq92Q%3D" rel="nofollow" target="_blank">https://docs.scrapy.org</a> - 最权威和全面的学习资料</li><li><strong>GitHub仓库</strong>:<a href="https://link.segmentfault.com/?enc=pVgGWh2SwLfjm4vlrr5%2FGQ%3D%3D.V3B2fY5cZLuSgUs08WIaKVtLQtMK4GRqkCRLrFcfCiE0vcIRWPJ6543OeAIs4EeO" rel="nofollow" target="_blank">https://github.com/scrapy/scrapy</a> - 查看源码和提交问题</li><li><strong>Stack Overflow</strong>:搜索<code>scrapy</code>标签,解决具体问题</li><li><strong>实战项目</strong>:尝试爬取不同类型的网站(电商、新闻、社交媒体),积累实战经验</li></ul><p>Scrapy的学习曲线虽然略陡,但一旦掌握,你就拥有了构建高性能爬虫系统的强大工具。从简单的数据采集到复杂的分布式爬虫,Scrapy都能胜任。开始你的Scrapy之旅吧!</p>]]></description></item><item>    <title><![CDATA[蓝易云cdn:如何在redis中设置文件夹名java 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047598486</link>    <guid>https://segmentfault.com/a/1190000047598486</guid>    <pubDate>2026-02-07 13:02:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>下面这件事<strong>必须先说清楚本质</strong>，否则后面全是弯路。</p><hr/><h2>一、先给结论：<strong>Redis 里根本没有“文件夹”这个概念</strong> ❗</h2><p>Redis 是<strong>内存型 Key-Value 数据库</strong>，不是文件系统。<br/>它<strong>不存在目录、文件夹、层级路径</strong>，只有：</p><blockquote><strong>Key（键） → Value（值）</strong></blockquote><p>你在 Redis 客户端里看到的“文件夹感”，<strong>只是 Key 的命名规则造成的视觉假象</strong>。</p><p>👉 <strong>所谓“在 Redis 中设置文件夹名”</strong>，本质就是：<br/><strong>通过 Key 的命名规范，模拟“文件夹 / 目录层级”结构</strong>。</p><hr/><h2>二、Redis“文件夹”的正确实现方式（核心原理）🧠</h2><h3>✅ 统一规则：<strong>用分隔符组织 Key 的命名空间</strong></h3><p>企业级 Redis 约定俗成的做法是：</p><ul><li>使用 <code>:</code> 或 <code>/</code> 作为<strong>逻辑分隔符</strong></li><li>每一段代表一层“目录语义”</li></ul><p>例如：</p><pre><code>cdn:file:hash:12345
cdn:file:meta:12345
cdn:config:node:beijing</code></pre><p>你看到的是“文件夹”，Redis 看到的是：</p><blockquote>一个普通字符串 Key</blockquote><hr/><h2>三、<strong>标准推荐的 Key 命名规范（务实版）</strong> ✅</h2><h3>🔑 统一结构公式</h3><pre><code class="text">系统名:业务模块:子模块:唯一标识</code></pre><h3>📌 蓝易云 CDN 场景示例</h3><pre><code class="text">bluecdn:file:content:md5
bluecdn:file:meta:md5
bluecdn:cache:node:ip</code></pre><blockquote>这种结构的好处：</blockquote><ul><li><strong>&lt;span style="color:red"&gt;可读性强&lt;/span&gt;</strong></li><li><strong>&lt;span style="color:red"&gt;避免 Key 冲突&lt;/span&gt;</strong></li><li><strong>&lt;span style="color:red"&gt;方便按“文件夹”批量管理&lt;/span&gt;</strong></li></ul><hr/><h2>四、Java 中“设置文件夹名”的标准写法（实战）☕️</h2><h3>示例 1：最基础的“文件夹 Key”</h3><pre><code class="java">String key = "bluecdn:file:content:abc123";
redisTemplate.opsForValue().set(key, "文件内容");</code></pre><h4>解释（逐行）👇</h4><ul><li><code>bluecdn</code>：系统命名空间，避免与其他业务混用</li><li><code>file</code>：逻辑“文件夹”</li><li><code>content</code>：子模块</li><li><code>abc123</code>：唯一标识（如 MD5、文件ID）</li></ul><p>📌 <strong>Redis 不会创建任何目录</strong>，只是存了一个字符串 Key。</p><hr/><h3>示例 2：用 Hash 模拟“文件夹下多个文件” 📂</h3><pre><code class="java">String folderKey = "bluecdn:file:meta:abc123";
redisTemplate.opsForHash().put(folderKey, "size", "1024");
redisTemplate.opsForHash().put(folderKey, "type", "jpg");</code></pre><h4>解释 👇</h4><ul><li><code>folderKey</code>：逻辑“文件夹”</li><li>Hash 的 field：相当于“文件属性”</li><li>Hash 的 value：属性值</li></ul><p>📌 <strong>一个 Hash = 一个逻辑目录</strong></p><hr/><h2>五、如何“查看某个文件夹下的内容”？（关键点）🔍</h2><p>Redis <strong>不能像文件系统那样 ls 目录</strong>，只能靠 <strong>Key 匹配规则</strong>。</p><h3>Java 中正确姿势</h3><pre><code class="java">Set&lt;String&gt; keys = redisTemplate.keys("bluecdn:file:*");</code></pre><h4>解释 👇</h4><ul><li><code>*</code>：通配符</li><li>匹配所有 <code>bluecdn:file:</code> 开头的 Key</li><li>相当于“查看这个文件夹下的所有文件”</li></ul><p>⚠️ <strong>生产环境注意</strong>：<br/>大 Key 数量场景应避免 <code>keys</code>，应使用 <strong>SCAN</strong>（迭代扫描）。</p><hr/><h2>六、推荐的“目录结构设计表”（企业级）📊</h2><table><thead><tr><th>业务场景</th><th>Key 示例</th><th>说明</th></tr></thead><tbody><tr><td>文件内容</td><td><code>bluecdn:file:content:id</code></td><td>文件主体</td></tr><tr><td>文件元数据</td><td><code>bluecdn:file:meta:id</code></td><td>大小、类型</td></tr><tr><td>节点缓存</td><td><code>bluecdn:node:cache:ip</code></td><td>节点状态</td></tr><tr><td>防护规则</td><td><code>bluecdn:waf:rule:id</code></td><td>WAF 规则</td></tr></tbody></table><p>👉 <strong>&lt;span style="color:red"&gt;目录是逻辑的，规则才是核心&lt;/span&gt;</strong></p><hr/><h2>七、常见误区，必须避开 ❌</h2><h3>❌ 误区 1：以为 Redis 会创建目录</h3><p>👉 Redis <strong>不会、也不需要</strong></p><h3>❌ 误区 2：Key 命名随意</h3><p>👉 后期<strong>无法维护、无法清理、无法迁移</strong></p><h3>❌ 误区 3：用 <code>/</code> 当真目录</h3><p>👉 <code>/</code> 和 <code>:</code> <strong>没有任何功能差异</strong></p><hr/><h2>八、一句话总结（给决策者的）🎯</h2><blockquote>**Redis 没有文件夹，只有 Key。<br/>所谓“文件夹名”，本质是 &lt;span style="color:red"&gt;Key 命名空间设计能力&lt;/span&gt;。<br/>设计得好，Redis 就是高性能目录树；<br/>设计得乱，它就是内存垃圾堆。**</blockquote>]]></description></item><item>    <title><![CDATA[蓝易云cdn:美国高防云服务器能防御哪些攻击？ 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047598488</link>    <guid>https://segmentfault.com/a/1190000047598488</guid>    <pubDate>2026-02-07 13:01:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>下面直接进入核心，不兜圈子。</p><hr/><h2><strong>美国高防云服务器能防御哪些攻击？（蓝易云 CDN 场景解析）</strong></h2><p>在当前的网络安全对抗环境中，美国高防云服务器的价值不在“能不能扛”，而在于<strong>&lt;span style="color:red"&gt;能否在攻击持续、手法演进的情况下保持业务可用&lt;/span&gt;</strong>。从工程实践角度看，它主要覆盖以下几大类攻击面。</p><hr/><h3>一、<strong>DDoS 洪水型攻击（核心防御能力）</strong> 🌊</h3><p>这是高防服务器存在的首要理由。</p><h4>可防御类型</h4><ul><li><strong>UDP Flood</strong></li><li><strong>ICMP Flood</strong></li><li><strong>TCP SYN Flood</strong></li><li><strong>ACK / RST Flood</strong></li><li><strong>混合型反射放大攻击</strong></li></ul><h4>防御原理（简化逻辑）</h4><pre><code class="text">异常流量 → 清洗中心 → 特征识别 → 丢弃攻击包 → 回源正常流量</code></pre><ul><li>大带宽承载用于<strong>吸收流量峰值</strong></li><li>清洗节点通过<strong>包速率、协议异常、源分布</strong>判断攻击</li><li>正常请求被保留并回源</li></ul><p>📌 关键点在于：<br/><strong>&lt;span style="color:red"&gt;防的是“量级 + 持续性”，而不是单点规则&lt;/span&gt;</strong></p><hr/><h3>二、<strong>CC 攻击（应用层消耗型攻击）</strong> 🧠</h3><p>CC 攻击不靠带宽，靠“像人一样访问”。</p><h4>常见形式</h4><ul><li>高频 HTTP GET/POST</li><li>慢连接（Slow Request）</li><li>模拟正常浏览路径的并发请求</li></ul><h4>美国高防云服务器的应对方式</h4><table><thead><tr><th>防御手段</th><th>说明</th></tr></thead><tbody><tr><td>访问频率限制</td><td>单 IP / 单会话 QPS 控制</td></tr><tr><td>行为分析</td><td>识别非人类访问节奏</td></tr><tr><td>会话校验</td><td>Cookie / Token 校验</td></tr><tr><td>挑战机制</td><td>动态验证请求有效性</td></tr></tbody></table><p>📌 本质是：<br/><strong>&lt;span style="color:red"&gt;让攻击成本无限接近真实用户成本&lt;/span&gt;</strong></p><hr/><h3>三、<strong>协议层畸形攻击（低层但致命）</strong> ⚙️</h3><p>这类攻击流量不大，但直接打协议实现漏洞。</p><h4>可防御类型</h4><ul><li>TCP 半连接耗尽</li><li>畸形 TCP Flag 组合</li><li>非法 MSS / Window Size</li><li>重放包攻击</li></ul><h4>防御机制说明</h4><ul><li>协议栈参数硬化</li><li>状态表容量保护</li><li>异常包即时丢弃</li></ul><p>📌 这类防御<strong>极度依赖底层网络与内核调优</strong>，普通云服务器基本无解。</p><hr/><h3>四、<strong>反射与放大攻击</strong> 🔁</h3><p>典型特征：<br/><strong>小请求 → 大响应 → 目标被淹没</strong></p><h4>常见攻击源</h4><ul><li>NTP</li><li>DNS</li><li>SSDP</li><li>Memcached</li></ul><h4>高防服务器的处理逻辑</h4><pre><code class="text">识别反射特征 → 阻断响应回程 → 清洗异常源</code></pre><p>📌 防御重点不是“挡住请求”，而是：<br/><strong>&lt;span style="color:red"&gt;阻断被利用的回包路径&lt;/span&gt;</strong></p><hr/><h3>五、<strong>扫描、探测与撞库类攻击</strong> 🔍</h3><p>虽然不是传统意义的大流量攻击，但对业务风险极高。</p><h4>可防御行为</h4><ul><li>端口扫描</li><li>服务指纹探测</li><li>登录接口撞库</li><li>异常路径探测</li></ul><h4>防御手段</h4><table><thead><tr><th>行为</th><th>防护方式</th></tr></thead><tbody><tr><td>高频扫描</td><td>自动封禁源</td></tr><tr><td>异常路径</td><td>规则阻断</td></tr><tr><td>登录异常</td><td>访问节流</td></tr></tbody></table><p>📌 目标只有一个：<br/><strong>&lt;span style="color:red"&gt;不让攻击者摸清你的系统结构&lt;/span&gt;</strong></p><hr/><h3>六、<strong>与 CDN + 高防联动时的攻击覆盖面</strong> 🚀</h3><p>当美国高防云服务器与 CDN 架构配合时，防御能力会发生质变。</p><h4>联动后的效果</h4><ul><li>攻击被<strong>提前在边缘节点拦截</strong></li><li>源站 IP 完全隐藏</li><li>CC 攻击被拆散到多个节点</li></ul><pre><code class="text">攻击者 → CDN 节点 → 清洗 → 高防服务器 → 业务</code></pre><p>📌 实战价值在于：<br/><strong>&lt;span style="color:red"&gt;攻击永远打不到真正的源头&lt;/span&gt;</strong></p><hr/><h3>七、能力边界说明（务实，不吹）⚠️</h3><p>必须说清楚，美国高防云服务器<strong>不解决所有安全问题</strong>。</p><table><thead><tr><th>不属于防御范围</th><th>原因</th></tr></thead><tbody><tr><td>业务逻辑漏洞</td><td>属于代码层问题</td></tr><tr><td>内部权限滥用</td><td>非网络攻击</td></tr><tr><td>程序自身 Bug</td><td>需开发修复</td></tr></tbody></table><p>📌 高防解决的是：<br/><strong>&lt;span style="color:red"&gt;可用性与抗压能力&lt;/span&gt;</strong>，而不是代码安全本身。</p><hr/><h3>八、总结（一句话定性）🎯</h3><blockquote>**美国高防云服务器的核心价值在于：<br/>在面对 &lt;span style="color:red"&gt;大规模、持续、多形态网络攻击&lt;/span&gt; 时，<br/>依然能让业务保持“能访问、不中断、不崩溃”。**</blockquote><p>对蓝易云 CDN 这类业务来说，它不是“可选项”，而是<strong>抗风险的基础设施</strong>。</p><p>从工程视角看，这不是“买防御”，而是<strong>为业务争取生存时间</strong>。</p>]]></description></item><item>    <title><![CDATA[CentOS 7 老树开新花：从零部署 Dify 全栈应用（含 Go/Rust/GCC 升级避坑） ]]></title>    <link>https://segmentfault.com/a/1190000047598515</link>    <guid>https://segmentfault.com/a/1190000047598515</guid>    <pubDate>2026-02-07 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>CentOS 7 老树开新花：从零部署 Dify 全栈应用（含 Go/Rust/GCC 升级避坑）</h2><blockquote>本文档适用于在 <strong>CentOS 7</strong> 环境下使用源代码部署 Dify 应用，对应版本 <code>1.9.2</code>。由于系统较旧，部分依赖需手动升级或通过容器化方式解决兼容性问题。</blockquote><hr/><h3>一、安装与配置 Docker</h3><h4>1. 卸载旧版本 Docker（如有）</h4><pre><code class="bash">sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine</code></pre><h4>2. 安装必要依赖</h4><pre><code class="bash">sudo yum install -y yum-utils device-mapper-persistent-data lvm2</code></pre><h4>3. 添加 Docker 官方 YUM 源</h4><pre><code class="bash">sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</code></pre><h4>4. 安装 Docker Engine 及相关组件</h4><pre><code class="bash">sudo yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</code></pre><h4>5. 启动并设置开机自启</h4><pre><code class="bash">sudo systemctl start docker
sudo systemctl enable docker</code></pre><h4>6. 配置国内镜像加速器</h4><p>&lt;!-- more --&gt;</p><p>创建 <code>/etc/docker/daemon.json</code> 文件：</p><pre><code class="bash">sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'
{
  "registry-mirrors": [
    "https://docker.registry.cyou",
    "https://docker-cf.registry.cyou",
    "https://dockercf.jsdelivr.fyi",
    "https://docker.jsdelivr.fyi",
    "https://dockertest.jsdelivr.fyi",
    "https://mirror.aliyuncs.com",
    "https://dockerproxy.com",
    "https://mirror.baidubce.com",
    "https://docker.m.daocloud.io",
    "https://docker.nju.edu.cn",
    "https://docker.mirrors.sjtug.sjtu.edu.cn",
    "https://docker.mirrors.ustc.edu.cn",
    "https://mirror.iscas.ac.cn",
    "https://docker.rainbond.cc",
    "https://do.nark.eu.org",
    "https://dc.j8.work",
    "https://gst6rzl9.mirror.aliyuncs.com",
    "https://registry.docker-cn.com",
    "http://hub-mirror.c.163.com",
    "http://mirrors.ustc.edu.cn/",
    "https://mirrors.tuna.tsinghua.edu.cn/",
    "http://mirrors.sohu.com/"
  ]
}
EOF</code></pre><blockquote>⚠️ <strong>注意</strong>：修改后需重载配置并重启 Docker：</blockquote><pre><code class="bash">sudo systemctl daemon-reload
sudo systemctl restart docker</code></pre><h4>7. 将当前用户加入 <code>docker</code> 用户组（避免每次使用 <code>sudo</code>）</h4><pre><code class="bash"># 创建 docker 组（若不存在）
sudo groupadd docker

# 将当前用户加入 docker 组
sudo usermod -aG docker $USER

# 刷新组权限（关键！否则需重新登录）
newgrp docker</code></pre><hr/><h3>二、部署 Dify API 服务</h3><h4>1. 准备中间件服务（如 Redis、PostgreSQL 等）</h4><ul><li>修改 <code>docker-compose.middleware.yaml</code> 和 <code>middleware.env</code> 中的数据卷路径</li><li>上传整个 <code>docker/</code> 目录到服务器</li></ul><h5>启动中间件</h5><pre><code class="bash">cd /data/dify/docker
docker compose -f docker-compose.middleware.yaml up -d</code></pre><blockquote>停止命令：<code>docker compose -f docker-compose.middleware.yaml down</code></blockquote><hr/><h4>2. 安装构建依赖环境</h4><blockquote><strong>原因</strong>：Dify 使用的 <code>wandb &gt;= 0.16.0</code> 要求本地存在 Go 编译环境；同时 <code>numpy==2.4.1</code> 需要 GCC ≥ 9.3，而 CentOS 7 默认 GCC 仅为 4.8.5。</blockquote><h5>(1) 安装 Go（1.23.0）</h5><pre><code class="bash"># 下载（使用国内镜像）
wget -O go1.23.0.linux-amd64.tar.gz https://golang.google.cn/dl/go1.23.0.linux-amd64.tar.gz

# 解压到 /usr/local
sudo tar -C /usr/local -xzf go1.23.0.linux-amd64.tar.gz

# 配置 PATH
echo 'export PATH=$PATH:/usr/local/go/bin' &gt;&gt; ~/.bashrc
source ~/.bashrc

# 验证
go version</code></pre><h5>(2) 安装 Rust（使用 rsproxy.cn 镜像）</h5><pre><code class="bash"># 下载安装脚本
wget -O rustup-init.sh https://rsproxy.cn/rustup-init.sh
chmod +x rustup-init.sh

# 设置国内镜像源
export RUSTUP_DIST_SERVER=https://rsproxy.cn
export RUSTUP_UPDATE_ROOT=https://rsproxy.cn/rustup

# 静默安装（不修改 PATH）
./rustup-init.sh -y --no-modify-path

# 临时加载环境变量
source "$HOME/.cargo/env"

# 验证
rustc --version
cargo --version</code></pre><h5>(3) 升级 GCC 至 9.3+</h5><pre><code class="bash"># 启用 SCL 源
sudo yum install -y centos-release-scl

# 安装 devtoolset-9
sudo yum install -y devtoolset-9-gcc devtoolset-9-gcc-c++

# 启用新 GCC（仅当前 shell 有效）
scl enable devtoolset-9 bash

# 验证
gcc --version  # 应显示 9.3.x</code></pre><blockquote>✅ <strong>建议</strong>：将 <code>scl enable devtoolset-9 bash</code> 加入 <code>~/.bashrc</code> 以持久生效（但注意可能影响其他程序）。</blockquote><h5>(4) 安装 <code>uv</code>（现代 Python 包管理器）</h5><pre><code class="bash">curl -LsSf https://astral.sh/uv/install.sh | sh
source "$HOME/.local/bin/env"</code></pre><hr/><h4>3. 部署 API 服务</h4><ul><li>修改 <code>.env</code> 文件中的数据库地址、存储路径、日志目录等配置。</li><li>上传 <code>api/</code> 目录到服务器（首次上传时请注释掉 <code>scp-api.sh</code> 中的启动逻辑）。</li></ul><h5>首次启动流程</h5><pre><code class="bash">cd /data/dify/api

# 安装依赖
uv sync

# 执行数据库迁移（首次必须运行）
flask db upgrade

# 后台启动 API 服务
nohup gunicorn -w 4 -k gevent --bind 0.0.0.0:5019 app:app &gt; dify-api.log 2&gt;&amp;1 &amp;</code></pre><h5>启动 Celery Worker</h5><pre><code class="bash">cd /data/dify/api

# 后台启动 Worker
nohup uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO \
  -Q dataset,generation,mail,ops_trace &gt; dify-worker.log 2&gt;&amp;1 &amp;</code></pre><blockquote><p>🔁 <strong>后续重启</strong>：只需执行</p><pre><code class="shell"># 启动API服务
nohup gunicorn -w 4 -k gevent --bind 0.0.0.0:5019 app:app &gt; dify-api.log 2&gt;&amp;1 &amp;
# 启动worker
nohup uv run celery -A app.celery worker -P gevent -c 1 --loglevel INFO \
  -Q dataset,generation,mail,ops_trace &gt; dify-worker.log 2&gt;&amp;1 &amp;</code></pre></blockquote><hr/><h3>三、部署 Dify Web 前端</h3><blockquote><strong>说明</strong>：CentOS 7 无法原生安装 Node.js 20+，因此采用 <strong>Docker 容器化部署</strong>。</blockquote><h4>1. 构建 Web 镜像（在开发机上操作）</h4><h5>(1) 本地编译（需 Node.js ≥ 22）</h5><pre><code class="bash"># 安装依赖
pnpm install --frozen-lockfile

# 构建（内存不足时增加堆大小）
NODE_OPTIONS="--max_old_space_size=4096" NEXT_CONCURRENT_BUILD_LIMIT=1 pnpm build

DIR1="web/.next/standalone/.next"

# 创建目录（-p 表示递归创建，且不报错如果已存在）
mkdir -p "$DIR1" 
cp -r web/.next/static web/.next/standalone/.next/static &amp;&amp; cp -r web/public web/.next/standalone/public </code></pre><blockquote>构建产物位于 <code>standalone/</code> 目录。</blockquote><h5>(2) 编写 Dockerfile</h5><pre><code class="Dockerfile"># 使用官方 Node.js 22 Alpine 镜像
FROM node:22-alpine

WORKDIR /app

# 复制构建产物
COPY standalone ./

EXPOSE 3000

CMD ["node", "server.js"]</code></pre><h4>2. 在服务器部署 Web 服务</h4><pre><code class="bash">cd /data/dify/web

# 1. 清理旧容器与镜像
docker stop my-dify-web &amp;&amp; docker rm my-dify-web &amp;&amp; docker rmi my-dify-web

# 2. 解压新构建包（覆盖 standalone/）
tar -xzf dify-web-standalone.tar.gz

# 3. 构建新镜像
docker build -t my-dify-web .

# 4. 启动容器
docker run -d \
  --name my-dify-web \
  -p 3000:3000 \
  my-dify-web</code></pre><h4>3. 配置 Web 环境变量</h4><ul><li>修改 <code>standalone/.env.local</code> 中的 <code>NEXT_PUBLIC_API_URL</code> 和 <code>NEXT_PUBLIC_WEB_URL</code>，指向实际 API 与 Web 地址。</li></ul><blockquote>🔄 <strong>更新 Web 服务</strong>：重复上述“清理 → 解压 → 构建 → 启动”流程，或封装为脚本自动化。</blockquote><hr/><h3>四、注意事项</h3><ol><li><strong>权限问题</strong>：确保 <code>/data/dify/</code> 目录对当前用户可读写。</li><li><strong>防火墙</strong>：开放 5019（API）、3000（Web）、以及中间件所需端口（如 6379、5432 等）。</li><li><strong>日志监控</strong>：定期检查 <code>dify-api.log</code> 和 <code>dify-worker.log</code>。</li><li><strong>环境持久化</strong>：若使用 <code>scl enable</code>，建议在 <code>~/.bashrc</code> 中添加 alias 或 wrapper 脚本。</li></ol><hr/><p>✅ 至此，Dify 已在 CentOS 7 上完整部署。  <br/>如遇问题，请优先检查依赖版本、网络连通性及配置文件路径。</p><hr/><p>希望这份部署文档能帮助你和团队更高效地完成部署！</p><p>本文由<a href="https://link.segmentfault.com/?enc=cMOQoNA515Q07a%2Boqa%2FhjA%3D%3D.oahddeXwM4KhIkqDnWo3GcadweDdEih9D7PR%2FkelUg4%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[STM32必会EXTI外部中断事件控制器 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047598392</link>    <guid>https://segmentfault.com/a/1190000047598392</guid>    <pubDate>2026-02-07 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>在嵌入式开发中，中断是一个非常重要的概念。</p><p>它允许 MCU 在执行主程序的同时，能够及时响应外部事件，比如按键按下、传感器信号变化等。</p><p>今天我们就来深入学习 STM32 的 EXTI 外部中断事件控制器，这是每个 STM32 开发者都必须掌握的核心知识。</p><h2>1. EXTI 外部中断事件控制器概述</h2><h3>1.1 什么是 EXTI</h3><p>EXTI 是 STM32 中用于管理外部中断和事件的控制器。</p><p>它可以检测 GPIO 引脚上的电平变化，并在满足触发条件时产生中断或事件。</p><p>简单来说，EXTI 就像是一个"门卫"，时刻监视着外部世界的变化，一旦发现符合条件的信号，就立即通知 CPU 去处理。</p><p>在实际项目中，我曾经用 EXTI 来处理紧急停止按钮。</p><p>当操作人员按下急停按钮时，系统必须在几微秒内做出响应，停止所有运动部件。</p><p>如果用轮询的方式去检测按钮状态，可能会因为主程序正在执行其他任务而延迟响应，但使用 EXTI 中断就能保证最快的响应速度。</p><h3>1.2 EXTI 的主要特性</h3><p>STM32 的 EXTI 控制器具有以下特性：</p><ol><li>支持多达 23 条外部中断/事件线（具体数量因芯片型号而异）</li><li>每条中断线都可以独立配置触发方式：上升沿、下降沿或双边沿触发</li><li>每个 GPIO 引脚都可以配置为外部中断源</li><li>支持软件触发中断</li><li>具有独立的挂起状态位和屏蔽位</li><li>可以产生中断请求或事件请求</li></ol><p>需要注意的是，STM32 的 EXTI 有一个重要的限制：相同编号的 GPIO 引脚共享同一条 EXTI 线。</p><p>比如 PA0、PB0、PC0 都连接到 EXTI0 线，这意味着你不能同时将 PA0 和 PB0 都配置为外部中断，只能选择其中一个。</p><h2>2. EXTI 工作原理</h2><h3>2.1 EXTI 的内部结构</h3><p>EXTI 控制器主要由以下几个部分组成：</p><ol><li><strong>边沿检测电路</strong>：负责检测输入信号的上升沿、下降沿或双边沿</li><li><strong>软件中断事件寄存器</strong>：允许通过软件触发中断</li><li><strong>挂起请求寄存器</strong>：记录哪些中断线有挂起的中断请求</li><li><strong>中断屏蔽寄存器</strong>：控制哪些中断线被使能</li><li><strong>事件屏蔽寄存器</strong>：控制哪些事件线被使能</li></ol><p>当外部信号满足触发条件时，EXTI 会将对应的挂起位置 1，如果该中断线没有被屏蔽，就会向 NVIC（嵌套向量中断控制器）发送中断请求。</p><h3>2.2 中断与事件的区别</h3><p>EXTI 可以产生两种类型的输出：中断和事件。</p><p>很多初学者容易混淆这两个概念。</p><p><strong>中断</strong>：会触发 CPU 执行中断服务程序（ISR），需要软件介入处理。</p><p>当中断发生时，CPU 会暂停当前任务，跳转到中断服务函数执行，处理完成后再返回主程序。</p><p><strong>事件</strong>：不会触发 CPU 中断，而是产生一个脉冲信号，可以触发其他外设的操作，比如启动 ADC 转换、触发 DMA 传输等，整个过程不需要 CPU 参与，实现了硬件级的联动。</p><p>在我做汽车电子项目时，经常使用事件模式来触发 ADC 采样。</p><p>比如每隔固定时间需要采集传感器数据，我会用定时器产生 EXTI 事件，然后这个事件直接触发 ADC 开始转换，整个过程不占用 CPU 资源，效率非常高。</p><h2>3. EXTI 配置步骤</h2><h3>3.1 使用 HAL 库配置 EXTI 的基本流程</h3><p>使用 STM32 HAL 库配置 EXTI 外部中断主要包括以下步骤：</p><ol><li>使能 GPIO 时钟</li><li>配置 GPIO 引脚为输入模式</li><li>配置 EXTI 中断线</li><li>配置 NVIC 中断优先级</li><li>编写中断服务函数</li></ol><p>下面我用一个实际的按键中断例子来说明整个配置过程。</p><h3>3.2 按键外部中断配置示例</h3><p>假设我们使用 PA0 引脚连接一个按键，按键按下时引脚电平为低，松开时为高（上拉输入）。</p><p>我们希望在按键按下（下降沿）时触发中断。</p><pre><code>/* 1. GPIO初始化配置 */
void MX_GPIO_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    /* 使能GPIOA时钟 */
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    /* 配置PA0为输入模式，上拉，外部中断模式 */
    GPIO_InitStruct.Pin = GPIO_PIN_0;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_FALLING;  // 下降沿触发中断
    GPIO_InitStruct.Pull = GPIO_PULL_UP;          // 上拉
    HAL_GPIO_Init(GPIOA, &amp;GPIO_InitStruct);
    
    /* 配置NVIC中断优先级 */
    HAL_NVIC_SetPriority(EXTI0_IRQn, 2, 0);
    
    /* 使能EXTI0中断 */
    HAL_NVIC_EnableIRQ(EXTI0_IRQn);
}
​
/* 2. 中断服务函数 */
void EXTI0_IRQHandler(void)
{
    /* 调用HAL库的中断处理函数 */
    HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_0);
}
​
/* 3. 中断回调函数 */
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 按键按下，执行相应操作 */
        // 这里可以添加你的业务逻辑
        // 比如翻转LED状态
        HAL_GPIO_TogglePin(GPIOC, GPIO_PIN_13);
    }
}</code></pre><h3>3.3 配置参数详解</h3><p>在上面的代码中，有几个关键的配置参数需要理解：</p><p><strong>GPIO\_MODE\_IT\_FALLING</strong>：这个参数指定了中断触发方式。</p><p>HAL 库提供了以下几种选择：</p><ul><li><code>GPIO_MODE_IT_RISING</code>：上升沿触发</li><li><code>GPIO_MODE_IT_FALLING</code>：下降沿触发</li><li><code>GPIO_MODE_IT_RISING_FALLING</code>：双边沿触发</li></ul><p><strong>GPIO\_PULL\_UP</strong>：配置 GPIO 的上拉/下拉电阻。</p><p>选项包括：</p><ul><li><code>GPIO_NOPULL</code>：无上拉下拉</li><li><code>GPIO_PULLUP</code>：上拉</li><li><code>GPIO_PULLDOWN</code>：下拉</li></ul><p><strong>HAL\_NVIC\_SetPriority</strong>：设置中断优先级。</p><p>第二个参数是抢占优先级，第三个参数是子优先级。</p><p>抢占优先级高的中断可以打断抢占优先级低的中断，而子优先级只在抢占优先级相同时才起作用。</p><h2>4. EXTI 中断优先级管理</h2><h3>4.1 NVIC 中断优先级分组</h3><p>STM32 使用 NVIC 来管理所有中断，包括 EXTI 中断。</p><p>NVIC 支持中断优先级分组，通过 <code>HAL_NVIC_SetPriorityGrouping()</code> 函数来配置。</p><pre><code>/* 配置中断优先级分组为组2 */
HAL_NVIC_SetPriorityGrouping(NVIC_PRIORITYGROUP_2);</code></pre><p>不同的优先级分组方式决定了抢占优先级和子优先级的位数分配：</p><ul><li><code>NVIC_PRIORITYGROUP_0</code>：0 位抢占优先级，4 位子优先级</li><li><code>NVIC_PRIORITYGROUP_1</code>：1 位抢占优先级，3 位子优先级</li><li><code>NVIC_PRIORITYGROUP_2</code>：2 位抢占优先级，2 位子优先级</li><li><code>NVIC_PRIORITYGROUP_3</code>：3 位抢占优先级，1 位子优先级</li><li><code>NVIC_PRIORITYGROUP_4</code>：4 位抢占优先级，0 位子优先级</li></ul><h3>4.2 合理设置中断优先级</h3><p>在实际项目中，合理设置中断优先级非常重要。</p><p>一般遵循以下原则：</p><ol><li><strong>紧急程度高的中断设置高优先级</strong>：比如急停按钮、故障检测等</li><li><strong>执行时间短的中断可以设置高优先级</strong>：避免长时间占用 CPU</li><li><strong>相关性强的中断设置相近的优先级</strong>：便于管理和调试</li></ol><p>在我做的一个电机控制项目中，优先级设置如下：</p><pre><code>/* 急停按钮 - 最高优先级 */
HAL_NVIC_SetPriority(EXTI0_IRQn, 0, 0);
​
/* 编码器脉冲 - 高优先级 */
HAL_NVIC_SetPriority(EXTI1_IRQn, 1, 0);
​
/* 普通按键 - 中等优先级 */
HAL_NVIC_SetPriority(EXTI2_IRQn, 2, 0);
​
/* 通信接收 - 较低优先级 */
HAL_NVIC_SetPriority(USART1_IRQn, 3, 0);</code></pre><h2>5. EXTI 使用注意事项</h2><h3>5.1 按键消抖处理</h3><p>在使用 EXTI 处理按键输入时，必须考虑按键抖动问题。</p><p>机械按键在按下或松开的瞬间，触点会产生多次通断，导致产生多次中断。</p><p>有两种常用的消抖方法：</p><p><strong>方法一：软件延时消抖</strong></p><pre><code>void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 简单延时消抖 */
        HAL_Delay(10);  // 延时10ms
        
        /* 再次检测按键状态 */
        if(HAL_GPIO_ReadPin(GPIOA, GPIO_PIN_0) == GPIO_PIN_RESET)
        {
            /* 确认按键按下，执行操作 */
            // 你的业务逻辑
        }
    }
}</code></pre><p>但是这种方法有个问题：在中断服务函数中使用延时会阻塞其他中断，不推荐在实际项目中使用。</p><p><strong>方法二：定时器消抖（推荐）</strong></p><pre><code>uint32_t last_interrupt_time = 0;
#define DEBOUNCE_TIME 50  // 50ms消抖时间
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        uint32_t current_time = HAL_GetTick();
        
        /* 检查距离上次中断的时间间隔 */
        if((current_time - last_interrupt_time) &gt; DEBOUNCE_TIME)
        {
            last_interrupt_time = current_time;
            
            /* 执行按键处理 */
            // 你的业务逻辑
        }
    }
}</code></pre><p>这种方法利用系统滴答定时器来判断时间间隔，不会阻塞其他中断，是更好的选择。</p><h3>5.2 中断服务函数的编写原则</h3><p>编写 EXTI 中断服务函数时，需要遵循以下原则：</p><ol><li><strong>尽量简短</strong>：中断服务函数应该尽快执行完毕，避免长时间占用 CPU</li><li><strong>避免使用延时函数</strong>：不要在中断中使用 <code>HAL_Delay()</code> 等阻塞函数</li><li><strong>避免复杂运算</strong>：复杂的计算应该在主程序中完成</li><li><strong>使用标志位</strong>：可以在中断中设置标志位，在主程序中检测标志位并处理</li></ol><pre><code>volatile uint8_t button_pressed = 0;  // 按键按下标志
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == GPIO_PIN_0)
    {
        /* 只设置标志位，不做复杂处理 */
        button_pressed = 1;
    }
}
​
int main(void)
{
    /* 系统初始化 */
    HAL_Init();
    SystemClock_Config();
    MX_GPIO_Init();
    
    while(1)
    {
        /* 在主循环中检测标志位 */
        if(button_pressed)
        {
            button_pressed = 0;  // 清除标志
            
            /* 执行复杂的处理逻辑 */
            process_button_event();
        }
        
        /* 其他任务 */
    }
}</code></pre><h3>5.3 多个 EXTI 中断的处理</h3><p>当使用多个外部中断时，需要注意中断线的分配。</p><p>STM32 的 EXTI0 到 EXTI4 各有独立的中断向量，而 EXTI5 到 EXTI9 共享一个中断向量（EXTI9\_5\_IRQn），EXTI10 到 EXTI15 共享另一个中断向量（EXTI15\_10\_IRQn）。</p><pre><code>/* EXTI5-9共享中断处理函数 */
void EXTI9_5_IRQHandler(void)
{
    /* 检查是哪个引脚触发的中断 */
    if(__HAL_GPIO_EXTI_GET_IT(GPIO_PIN_5) != RESET)
    {
        HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_5);
    }
    
    if(__HAL_GPIO_EXTI_GET_IT(GPIO_PIN_6) != RESET)
    {
        HAL_GPIO_EXTI_IRQHandler(GPIO_PIN_6);
    }
    
    // 其他引脚的处理...
}
​
/* 回调函数中区分不同的引脚 */
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    switch(GPIO_Pin)
    {
        case GPIO_PIN_5:
            /* 处理PIN5的中断 */
            break;
            
        case GPIO_PIN_6:
            /* 处理PIN6的中断 */
            break;
            
        default:
            break;
    }
}</code></pre><h2>6. EXTI 实战应用案例</h2><h3>6.1 旋转编码器接口</h3><p>旋转编码器是嵌入式系统中常用的输入设备，通常有 A、B 两相输出。</p><p>通过检测 A、B 相的相位关系可以判断旋转方向和速度。</p><p>使用 EXTI 可以很好地实现编码器接口。</p><pre><code>#define ENCODER_A_PIN GPIO_PIN_0
#define ENCODER_B_PIN GPIO_PIN_1
#define ENCODER_PORT GPIOA
​
volatile int32_t encoder_count = 0;
​
void Encoder_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    /* 配置A相为外部中断 */
    GPIO_InitStruct.Pin = ENCODER_A_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_RISING_FALLING;
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(ENCODER_PORT, &amp;GPIO_InitStruct);
    
    /* 配置B相为普通输入 */
    GPIO_InitStruct.Pin = ENCODER_B_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_INPUT;
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(ENCODER_PORT, &amp;GPIO_InitStruct);
    
    HAL_NVIC_SetPriority(EXTI0_IRQn, 1, 0);
    HAL_NVIC_EnableIRQ(EXTI0_IRQn);
}
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == ENCODER_A_PIN)
    {
        /* 读取A相和B相的状态 */
        uint8_t a_state = HAL_GPIO_ReadPin(ENCODER_PORT, ENCODER_A_PIN);
        uint8_t b_state = HAL_GPIO_ReadPin(ENCODER_PORT, ENCODER_B_PIN);
        
        /* 根据相位关系判断旋转方向 */
        if(a_state == b_state)
        {
            encoder_count++;  // 正转
        }
        else
        {
            encoder_count--;  // 反转
        }
    }
}</code></pre><h3>6.2 红外遥控接收</h3><p>红外遥控器发送的是脉宽调制信号，通过测量脉冲宽度可以解码出按键信息。</p><p>使用 EXTI 配合定时器可以实现红外信号的解码。</p><pre><code>#define IR_PIN GPIO_PIN_2
#define IR_PORT GPIOA
​
volatile uint32_t ir_start_time = 0;
volatile uint32_t ir_pulse_width = 0;
volatile uint8_t ir_data_ready = 0;
​
void IR_Init(void)
{
    GPIO_InitTypeDef GPIO_InitStruct = {0};
    
    __HAL_RCC_GPIOA_CLK_ENABLE();
    
    GPIO_InitStruct.Pin = IR_PIN;
    GPIO_InitStruct.Mode = GPIO_MODE_IT_FALLING;  // 下降沿触发
    GPIO_InitStruct.Pull = GPIO_PULLUP;
    HAL_GPIO_Init(IR_PORT, &amp;GPIO_InitStruct);
    
    HAL_NVIC_SetPriority(EXTI2_IRQn, 2, 0);
    HAL_NVIC_EnableIRQ(EXTI2_IRQn);
}
​
void HAL_GPIO_EXTI_Callback(uint16_t GPIO_Pin)
{
    if(GPIO_Pin == IR_PIN)
    {
        uint32_t current_time = HAL_GetTick();
        
        if(ir_start_time == 0)
        {
            /* 记录起始时间 */
            ir_start_time = current_time;
        }
        else
        {
            /* 计算脉冲宽度 */
            ir_pulse_width = current_time - ir_start_time;
            ir_start_time = current_time;
            ir_data_ready = 1;
            
            /* 根据脉冲宽度解码数据 */
            // 这里添加解码逻辑
        }
    }
}</code></pre><h2>7. 总结</h2><p>EXTI 外部中断事件控制器是 STM32 中非常重要的外设，掌握它对于开发响应式的嵌入式系统至关重要。</p><p>通过本文的学习，我们了解了 EXTI 的工作原理、配置方法以及实际应用技巧。</p><p>在实际开发中，使用 EXTI 需要注意以下几点：首先要合理设置中断优先级，确保重要的中断能够及时响应；其次要注意按键消抖等实际问题，避免误触发；最后要遵循中断服务函数简短高效的原则，复杂的处理逻辑应该在主程序中完成。</p><p>我在多年的嵌入式开发经验中，EXTI 几乎是每个项目都会用到的功能。</p><p>从简单的按键检测到复杂的编码器接口、红外遥控接收，EXTI 都能很好地胜任。</p><p>希望这篇文章能帮助大家更好地理解和使用 STM32 的 EXTI 功能，在实际项目中灵活运用。</p><p><strong>更多编程学习资源</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=NDK9ig8%2F3opeWFJt%2F%2BJzXw%3D%3D.DWLxjJrN8rKKGaCA2jFlgSBeA149E7KPwHrYUTbXSLl7njHd0DD0HB4Am30qBa9TrUPH%2F3diqE%2F5FEb0cBgoPg%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=VrasN4pA6%2F1ljSd%2BoW1iFg%3D%3D.9IcQMtH3VuO66lhVtvd7V7atna0iGN5ksE4YuzLyt%2Bf%2FQEhY5qbKqWOdWrqR2%2Bqmi4jZoAGzAhtHCnJ1Q0P7oA%3D%3D" rel="nofollow" target="_blank">STM32 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=nMtHMl5qpUFP6Q7vLKegIA%3D%3D.2vWLOisvFYuZY2QKEIJ%2BIOFwTkIO72nRHiUUdrIsHih%2FxrUSZpYlG2%2BcSbeUqlt8hj6Pp6b8%2B%2BkMoav3K%2FITPah4PFiCyJdHxFrszOMAfVI%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=oL01PYvBM3oRCTBlzXANHg%3D%3D.L9oFyW9eun%2BL5HcMwuFRFQYTXFSyvBGv3el1FS58rsqF5cwT79JXfkA2o4Cy4iyTU%2BEWuDVey4E7y2dclFX0FA%3D%3D" rel="nofollow" target="_blank">C++ 零基础入门电子书-2026 最新版</a></li><li><a href="https://link.segmentfault.com/?enc=2gNNxYqfVHD9QdpV7qidIg%3D%3D.0L%2FBZQkdVsVn33mrFfPEFiE%2FcDcnL0l5XHtzc7Vb0JNiiT77S3%2F0uA5gCBl13rVbYcsGBUPq5yDtBy1ROeiJOQ%3D%3D" rel="nofollow" target="_blank">51 单片机零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=Ht1hNzHyM3ammSL61cjZOA%3D%3D.Vs7whHRnv42ZQFgF88YumuAeHeOJVz0%2B8ABkjoSYFz%2FoScql6QyaHTeMKegoeDvkVEXdPl6WWBIuPV4HP9Mzgg%3D%3D" rel="nofollow" target="_blank">AD 画板零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=xg4tc6tkrWiTP5TZ1dXrDw%3D%3D.F%2FN2BRxXeiArjwhuZldqV91sBxhqJrA2q4BMnqZD9mJ64KUN%2BUCgfV6Vy7j04hgFr%2BYOCmR97E%2FCmqvl0rTIsg%3D%3D" rel="nofollow" target="_blank">C 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=atWGtN96RuR3KBmVMQ2SEg%3D%3D.cT%2FaDgpWS67aISbHwTa1HUak5ECd7hhutZiK%2F9GyhG98c%2B8TvLpVNw880VSyQANgoPxZnKZvPIrHqXpYt8YTQw%3D%3D" rel="nofollow" target="_blank">C++ 语言零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=cq1u8q7aEWqDpcVTXAASCw%3D%3D.txHxzI3WD5BIbgC3Kk6YITHEwbGCKFjKCGcguvwDz6z%2Bi0QKh4XaLtED%2BfoiBehWEJJtSeKFP%2FYbxg%2BnGpMv2p53MizLoChEgrj8kqZVAGc%3D" rel="nofollow" target="_blank">ESP32 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=9duRRz4hTIuPTGXsxqMjuw%3D%3D.e9y42akfZuMpTjzmtyRWSQrI5yf9E1pKjjVA87249KnnUdvd1inYOAO9L8cxOOGGGNE0OD%2B%2FXyJOIg0a3Pf5QvRN9T%2BIcs5C1HnN3cDsixc%3D" rel="nofollow" target="_blank">FreeRTOS 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=O5JizOeoUXomIafPPTt6bA%3D%3D.Jj2mxWPTfrtaQNWHsE5K2J7rNRTx737i8yK96xre2SwdZSj6SIAurtmPvoM6aRHcUwShM4mC3hlQW6An%2Fv6HvA0Y8rz8bKHKmalUHkjcuA8%3D" rel="nofollow" target="_blank">Linux 应用开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=TJ4Y%2BDLAvjen4e7NQnVYjQ%3D%3D.vrgp9FUxmGZ6n1Y1HPNoLv6lwgQWXJdI4weifWxCTNWw80q02Yw5RfG6dYG68wF3IsO45BjKFJv%2FcsWme6AVeRD25iJqax9CveHzcR0z8Jw%3D" rel="nofollow" target="_blank">Linux 底层开发零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=BxkSRPZ6trge6EUf%2BDPgVg%3D%3D.4kMwMAYelnsi5plkXpYGSObZnLNGDV965HrvmgWLeMlj6nvMbwoAucHWoO8D%2F5nfKXSrFZ2lHvzEjqToGAayLQ%3D%3D" rel="nofollow" target="_blank">LVGL 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=GwzlhIfhjwgkSRAObWYl3g%3D%3D.WnSXXk5Ee3jc5M36MIlphq%2FNiZchzTYKPo4EbcSa3E3MFJ2I8SpjmWgScP7Oa%2FzPyNyH3Qp%2BQ6QtQttZ2hgo0A%3D%3D" rel="nofollow" target="_blank">QT 零基础入门学习路线</a></li><li><a href="https://link.segmentfault.com/?enc=HU73pNxFlN6F44CeI21UrA%3D%3D.S1fFdmxi3K8ozbbQRBKUPqwLP42kj9roIuY18eyBu4N%2B8b65p%2BoxjbvYER4jJBRLuiVcPRoW%2FdVYXrevk%2BxV%2BRYqJwK4rawpaDxXDaWsPuw%3D" rel="nofollow" target="_blank">STM32 零基础入门学习路线</a></li></ul>]]></description></item><item>    <title><![CDATA[多模态与视觉大模型开发实战 - 2026必会课分享 学习园地主页 ]]></title>    <link>https://segmentfault.com/a/1190000047598322</link>    <guid>https://segmentfault.com/a/1190000047598322</guid>    <pubDate>2026-02-07 11:04:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>视觉智能的商业临界点已经到来<br/>2026年，多模态视觉大模型的发展正从技术探索阶段过渡到商业价值兑现期。当技术能够稳定识别图像中的商品并理解用户自然语言描述的偏好时，一个新的商业时代开启了。在东京银座的一家高端百货，一套基于多模态视觉大模型的导购系统正在改变零售体验：顾客用手机拍摄心仪的手提包，系统不仅识别品牌和型号，还能根据顾客过往的购物记录、当前穿着风格，甚至社交媒体上表达的生活态度，推荐相配的鞋履和配饰——这种体验的转化率比传统推荐系统高出三倍。</p><p>这种商业价值的爆发并非偶然，而是多项技术成熟度曲线交汇的必然结果。视觉识别精度突破95%实用门槛、跨模态语义对齐技术让图像与语言理解无缝衔接、边缘计算能力大幅提升使实时分析成为可能——这三个技术拐点在2025-2026年间相继到来，为商业化应用扫清了最后障碍。</p><hr/><p>行业级解决方案的差异化竞争策略<br/>2026年最成功的商业实践表明，通用型多模态视觉模型难以直接创造商业价值，而针对特定行业深度优化的模型却能快速形成竞争壁垒。</p><p>在医疗影像诊断领域，领先企业不再简单标定病灶位置，而是构建了“影像-病理-预后”的全链条理解模型。当系统读取CT扫描时，它不仅识别肿瘤特征，还能关联相似病例的治疗方案和康复轨迹，为医生提供决策支持而非仅仅诊断辅助。这种深度行业理解构建的数据护城河，使后来者难以在短期内追赶。</p><p>制造业的质量检测方案则展现了另一种商业逻辑。传统视觉检测只能识别预设的缺陷类型，而多模态系统通过分析产品图像、生产线传感器数据和维修记录文本，能发现人眼难以察觉的潜在缺陷模式，甚至预测设备故障对产品质量的影响。这种从“检测”到“预防”的价值跃迁，让客户愿意支付十倍于传统系统的价格。</p><hr/><p>成本结构的革命与商业模式创新<br/>多模态视觉大模型的商业普及，关键驱动力之一是成本结构的根本性改变。2025年之前，训练行业级模型需要数百万美元的算力投入，而2026年的模块化训练框架和模型高效微调技术，将这一门槛降低到原来的十分之一。</p><p>成本下降催生了全新的商业模式。在时尚行业，一家初创公司不再销售软件许可，而是提供“视觉智能订阅服务”：中小品牌按月支付费用，即可获得与大牌同等的视觉分析和设计辅助能力。在农业领域，服务商根据农田面积和检测频率收费，为农场主提供作物病虫害的早期预警——这种“效果付费”模式彻底改变了技术采购的逻辑。</p><p>更值得关注的是边缘端部署的经济性突破。2026年，经过优化的多模态模型已能在智能手机和工业边缘设备上流畅运行，这意味着商业应用不再受限于云端连接，可以在网络条件差的工厂车间、偏远农场或应急现场发挥作用。这种部署方式的转变，开辟了数十个此前无法触达的商业场景。</p><hr/><p>数据生态构建：从单向采集到价值循环<br/>传统视觉系统的数据流动是单向的：采集、标注、训练、部署。2026年领先企业的核心竞争优势，在于构建了能够自我增强的数据价值循环。</p><p>零售巨头亚马逊的多模态系统展示了这种生态的威力：当顾客在实体店试穿服装时，视觉系统分析试穿效果；顾客的购买决定与在线评价形成反馈；这些数据不仅优化推荐算法，还反向指导服装设计与库存管理。数据在消费端与生产端之间形成闭环，每一条数据都多次创造价值。</p><p>在自动驾驶领域，特斯拉建立的“影子模式”数据生态更为成熟：数百万辆车的视觉系统持续观察环境，即使在自动驾驶未激活时也在对比人类司机的决策与模型预测的差异。这种持续的对比学习使系统能力呈指数级增长，形成了竞争对手难以复制的数据资产。</p><hr/><p>商业落地的隐形挑战与应对策略<br/>技术成熟度不等于商业成功率。2026年，多模态视觉大模型的商业落地面临三个隐形挑战，而成功企业已找到应对之道。</p><p>首先是“期望值管理”问题。早期客户往往对AI能力抱有不切实际的期待，认为系统应像人类一样理解任何视觉场景。领先供应商通过“能力边界透明化”策略解决这一问题：明确告知系统在哪些场景下准确率超过98%，在哪些边缘情况下可能失效，并提供相应的保障方案。这种坦诚反而建立了更强的客户信任。</p><p>其次是“集成复杂度”挑战。多模态系统需要与企业现有IT架构、数据平台和业务流程深度融合。提供“渐进式集成”方案的供应商更受青睐：先从单一场景试点，验证价值后再逐步扩展，避免“大爆炸式”改造带来的风险。</p><p>最后是“持续进化”需求。商业环境不断变化，今天的模型明天就可能过时。建立“模型即服务”的持续更新机制成为标准配置，确保客户无需频繁投入重训成本即可获得能力升级。</p><hr/><p>2026年的商业格局与未来展望<br/>到2026年末，多模态视觉大模型的市场已形成清晰的层级格局：底层是少数几家提供基础大模型的科技巨头；中间层是专注行业解决方案的垂直领域领导者；上层则是大量利用API构建具体应用场景的创新企业。</p><p>这一格局中最具活力的正是中间层的行业专家。他们既理解技术的可能性，也深谙行业的痛点；既能为客户创造可见的ROI（投资回报率），又能建立长期的竞争壁垒。这些企业的估值逻辑已从传统的“软件毛利率”转变为“数据资产价值”和“行业生态地位”。</p><p>展望2027年，下一轮商业突破将来自多模态系统与物理世界的更深度融合——当视觉理解能力与机器人操作、环境交互、实时决策结合时，将催生真正的“智能体经济”。那些在2026年掌握了多模态视觉模型商业方法论的企业，将在下一轮竞争中占据先发优势。</p><p>商业与技术之间总是存在微妙的时差。2026年的机遇在于：技术刚刚跨越实用门槛，而商业认知还未完全普及——这中间的窗口期，正是先行者建立优势的最佳时机。多模态视觉大模型的发展历程再次证明：最具颠覆性的商业创新，往往发生在技术曲线从陡峭趋于平缓的转折点上，因为此时技术足够可靠，而应用想象刚刚展开。</p>]]></description></item><item>    <title><![CDATA[EmEditor文本编辑器安装步骤详解（附大文件打开与代码编辑教程） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047598324</link>    <guid>https://segmentfault.com/a/1190000047598324</guid>    <pubDate>2026-02-07 11:04:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p><code>EmEditor</code>是 <strong>EmEditor 文本编辑器的安装包</strong>，这是个主打<strong>大文件和代码编辑</strong>的工具，打开几百 MB 甚至 GB 的文本不卡，支持各种编程语言高亮、正则查找替换，写代码、改日志、处理数据都挺顺手。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=huHdqAqDAIWndx3BDH9Dsg%3D%3D.NkKAEuz1TX%2FXZvhMnXe7aR45tqgFaL2qnQVpzk5f7XU6ndOGnQNdYweM6Rlmb%2FL%2F" rel="nofollow" title="https://pan.quark.cn/s/0424198092b3" target="_blank">https://pan.quark.cn/s/0424198092b3</a></li></ul></li><li><p><strong>用管理员身份运行（推荐）</strong> ​</p><ul><li>右键 <code>EmEditor.exe</code>→ 选“以管理员身份运行”，防止权限不足导致安装出错。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>EmEditor.exe</code>运行（如果右键过了就直接双击）。</li><li>第一次打开会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>进入安装向导，选语言（默认 English，有的版本有中文）→ 点  <strong>“Next”</strong> 。</li><li>阅读许可协议 → 选 “I accept the terms…” → 点  <strong>“Next”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\Program Files\EmEditor</code>，可点 Browse 改到其他盘。</li></ul></li><li><p>附加任务：</p><ul><li>建议勾 “Create a desktop shortcut”（创建桌面快捷方式），方便以后打开。</li></ul></li><li>点  <strong>“Install”</strong> ​ 开始安装，等进度条走完（几十秒）。</li><li>安装完会问是否立即启动 → 可先取消，等会儿再开。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>在开始菜单或桌面找到 <strong>EmEditor</strong>​ → 点开。</li><li>第一次打开就是干净的主界面，类似记事本但功能更多。</li><li><strong>打开大文件</strong>：拖文件进来或直接点“打开”，几百 MB 也能秒开。</li><li><strong>代码高亮</strong>：打开 <code>.c</code>、<code>.py</code>、<code>.html</code>等文件，会自动识别并高亮语法。</li><li><strong>查找替换</strong>：支持正则表达式，找特殊内容很方便。</li><li><strong>多标签页</strong>：可以同时开多个文件，来回切换不用来回找窗口。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[技术日报｜OpenAI技能库逆袭登顶，Claude-Mem四连冠终结 Devlive开源社区 ]]></title>    <link>https://segmentfault.com/a/1190000047598333</link>    <guid>https://segmentfault.com/a/1190000047598333</guid>    <pubDate>2026-02-07 11:03:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>🌟 <strong>TrendForge 每日精选</strong> - 发现最具潜力的开源项目<br/>📊 今日共收录 <strong>7</strong> 个热门项目，涵盖 <strong>50</strong> 种编程语言</p><p>🌐 <strong>智能中文翻译版</strong> - 项目描述已自动翻译，便于理解</p></blockquote><h3>🏆 今日最热项目 Top 10</h3><h4>🥇 openai/skills</h4><p><strong>项目简介</strong>: Codex 技能目录</p><p><strong>今日新增</strong>: 583 | <strong>总星数</strong>: 4842 | <strong>语言</strong>: Python</p><p><a href="https://link.segmentfault.com/?enc=glQ54G6HSng%2ByJmOYnDIag%3D%3D.Z9DvP2%2BrfeZAOQ5X%2FH3cNERjlL75Z86ASRNdRIiL3u%2FSagQjH%2FaXWnPAIrRtlnb7" rel="nofollow" target="_blank">https://github.com/openai/skills</a></p><hr/><h4>🥈 bytedance/UI-TARS-desktop</h4><p><strong>项目简介</strong>: 开源多模态AI智能体堆栈，连接尖端AI模型与智能体基础设施</p><p><strong>今日新增</strong>: 573 | <strong>总星数</strong>: 27099 | <strong>语言</strong>: TypeScript</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598335" alt="bytedance/UI-TARS-desktop" title="bytedance/UI-TARS-desktop"/></p><p><a href="https://link.segmentfault.com/?enc=KIJwYEjdwM3IirCAHSkS3g%3D%3D.nIJk8EASgE4U9kVkIzpWqmnpwcb05u3%2FeSsbTc8DnkEpXyfoTqPyyBN6rOWxxBxP" rel="nofollow" target="_blank">https://github.com/bytedance/UI-TARS-desktop</a></p><hr/><h4>🥉 aquasecurity/trivy</h4><p><strong>项目简介</strong>: 在容器、Kubernetes、代码仓库、云环境等场景中检测漏洞、错误配置、密钥泄露和软件物料清单</p><p><strong>今日新增</strong>: 165 | <strong>总星数</strong>: 31535 | <strong>语言</strong>: Go</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598336" alt="aquasecurity/trivy" title="aquasecurity/trivy" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=eQSSGyO%2B1aegYP5re3p1sA%3D%3D.8IP0C40BQ2wpsmXZuCmAUF707pw%2BB6%2BpU3JWcoD662enFNlJz2e4jjCvmME7RuZJ" rel="nofollow" target="_blank">https://github.com/aquasecurity/trivy</a></p><hr/><h4><strong>4.</strong> nvm-sh/nvm</h4><p><strong>项目简介</strong>: Node 版本管理器 - 符合 POSIX 标准的 bash 脚本，用于管理多个活跃的 node.js 版本</p><p><strong>今日新增</strong>: 131 | <strong>总星数</strong>: 91497 | <strong>语言</strong>: Shell</p><p><a href="https://link.segmentfault.com/?enc=XMb4LXjKKFB6w%2BPCREeLGQ%3D%3D.bM9zDIMHNmfuHRfLwIvl%2FkAC3Sc5LXLTEfQ40On2zVU%3D" rel="nofollow" target="_blank">https://github.com/nvm-sh/nvm</a></p><hr/><h4><strong>5.</strong> DataExpert-io/data-engineer-handbook</h4><p><strong>项目简介</strong>: 数据工程全方位学习资源汇总仓库</p><p><strong>今日新增</strong>: 71 | <strong>总星数</strong>: 39856 | <strong>语言</strong>: Jupyter Notebook</p><p><a href="https://link.segmentfault.com/?enc=qmYP2X%2FJTTVhTRZeX3hIDw%3D%3D.0HmIoKyyddA7AxFxRvrwpaMtK1pthA6lIM7WCNa13AW56Pq8sb03q%2FLTcbwhPPSkMldwpMDvjdpNbAYjQduKZg%3D%3D" rel="nofollow" target="_blank">https://github.com/DataExpert-io/data-engineer-handbook</a></p><hr/><h4><strong>6.</strong> Flowseal/zapret-discord-youtube</h4><p><strong>项目简介</strong>: </p><p><strong>今日新增</strong>: 70 | <strong>总星数</strong>: 21967 | <strong>语言</strong>: Batchfile</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598337" alt="Flowseal/zapret-discord-youtube" title="Flowseal/zapret-discord-youtube" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=a%2B8fa6T%2BYAyexhzIX53A9w%3D%3D.eSzshFesRHgMcIVbO12HdWaf0lHuK8Qt3UwueFbHwxn9NVm7eWWgrRSb5T4baswpwS4z9wCu3QGx3rxjreCBIw%3D%3D" rel="nofollow" target="_blank">https://github.com/Flowseal/zapret-discord-youtube</a></p><hr/><h4><strong>7.</strong> likec4/likec4</h4><p><strong>项目简介</strong>: 通过代码生成的实时动态图表，实现软件架构的可视化、协作与持续演进。</p><p><strong>今日新增</strong>: 40 | <strong>总星数</strong>: 1802 | <strong>语言</strong>: TypeScript</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047595319" alt="likec4/likec4" title="likec4/likec4" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=6zHWNK9WMTJMdyCE9znyCw%3D%3D.5RjhkqGrFMsnvbIvj4JohiZq%2Bm32p2zNbvAFoFPxKWEysTfdYcTmeDtBorZqur%2Bo" rel="nofollow" target="_blank">https://github.com/likec4/likec4</a></p><hr/><h3>🌈 分语言热门项目</h3><h4>● C 最热项目</h4><p><strong>项目名称</strong>: tmux/tmux</p><p><strong>项目描述</strong>: tmux源代码</p><p><strong>今日新增:</strong> 62 | <strong>总数:</strong> 41435</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=KI2gts%2FVZ6RFDAgsf5XZ3A%3D%3D.s7%2B%2BM0NusaQDHgcfUK364a0Q5l96HnZRpaRSW%2FSQBr8%3D" rel="nofollow" target="_blank">https://github.com/tmux/tmux</a></p><hr/><p><strong>项目名称</strong>: timescale/timescaledb</p><p><strong>项目描述</strong>: 作为Postgres扩展打包的高性能实时分析时序数据库</p><p><strong>今日新增:</strong> 40 | <strong>总数:</strong> 21703</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=eLyXGFZdEfh8b1Kl20HEhA%3D%3D.3cr6WIA4xdbUaAO50uxTpbbNeantTC%2FT0%2Blj4U691FvEHuZLY5ikU4zIGbglc%2Fm5" rel="nofollow" target="_blank">https://github.com/timescale/timescaledb</a></p><hr/><p><strong>项目名称</strong>: bol-van/zapret2</p><p><strong>项目描述</strong>: 反深度包检测软件</p><p><strong>今日新增:</strong> 17 | <strong>总数:</strong> 1464</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=w2I4Unw8YLF1sQlGUDI7tw%3D%3D.HuNEENYkg%2B%2F3BaJ8SIBYvdTNns9sTfOeCLhl7yG92ZCahxLBxFu3H0o6W8m86aEJ" rel="nofollow" target="_blank">https://github.com/bol-van/zapret2</a></p><hr/><h4>● C# 最热项目</h4><p><strong>项目名称</strong>: marticliment/UniGetUI</p><p><strong>项目描述</strong>: UniGetUI：您的包管理器图形界面。或可粗略描述为用于管理包管理器的"包管理器管理器"。</p><p><strong>今日新增:</strong> 140 | <strong>总数:</strong> 20667</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598338" alt="marticliment/UniGetUI" title="marticliment/UniGetUI" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=%2B8YNmzoB6NHBUeJma9UgYw%3D%3D.qGQgn9vPaeQxDASfDpDK%2F9zGmywS%2FkBjAFzCwkBUZHU4dTmedN6R1GIIJG4segmO" rel="nofollow" target="_blank">https://github.com/marticliment/UniGetUI</a></p><hr/><p><strong>项目名称</strong>: wshobson/agents</p><p><strong>项目描述</strong>: 面向Claude Code的智能自动化与多智能体编排系统</p><p><strong>今日新增:</strong> 101 | <strong>总数:</strong> 27973</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598339" alt="wshobson/agents" title="wshobson/agents" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=L3nd3WkulQmHW2p8Mg4w0A%3D%3D.yn4H1pBn0BoIjEpmWVyJxsuFFMSTy2G9jW9mgp3lV5%2BF1l2KaSWHpEsSzIsLPf4M" rel="nofollow" target="_blank">https://github.com/wshobson/agents</a></p><hr/><p><strong>项目名称</strong>: Cleanuparr/Cleanuparr</p><p><strong>项目描述</strong>: Cleanuparr是一款自动化清理工具，用于清理Sonarr、Radarr及支持的下载客户端（如q...</p><p><strong>今日新增:</strong> 55 | <strong>总数:</strong> 1902</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=kwxciBdh5vd1IAY2y2tUnA%3D%3D.SOMyyqv7eXc7gyFtitrJPh7tWzHaq1abjc7Oq1BT4i4KZSdKopE8WTwwXBaxVfL%2B" rel="nofollow" target="_blank">https://github.com/Cleanuparr/Cleanuparr</a></p><hr/><h4>● C++ 最热项目</h4><p><strong>项目名称</strong>: ggml-org/llama.cpp</p><p><strong>项目描述</strong>: 使用 C/C++ 实现的大语言模型推理框架</p><p><strong>今日新增:</strong> 85 | <strong>总数:</strong> 94535</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598340" alt="ggml-org/llama.cpp" title="ggml-org/llama.cpp" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=DCIHQ2cz1qWf95WujELojQ%3D%3D.Wvw9w6q4J%2F05U%2Bzy0azRA9UnYgwFWIP4fJU%2FxC7E4cJ39PJpuvQoDFFDK%2F3nGHtD" rel="nofollow" target="_blank">https://github.com/ggml-org/llama.cpp</a></p><hr/><p><strong>项目名称</strong>: godotengine/godot</p><p><strong>项目描述</strong>: Godot引擎——跨平台2D与3D游戏引擎</p><p><strong>今日新增:</strong> 61 | <strong>总数:</strong> 106402</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598341" alt="godotengine/godot" title="godotengine/godot" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=iX%2BU9pXH2ZQljcVPHnXM%2Fg%3D%3D.UMnhQWU8ZBVXeZdu5uUNAfRPAhcZmsqwXQzn6WqMhpjcTSwvkPHVd7eBdoLoInkv" rel="nofollow" target="_blank">https://github.com/godotengine/godot</a></p><hr/><p><strong>项目名称</strong>: LadybirdBrowser/ladybird</p><p><strong>项目描述</strong>: 真正独立的网页浏览器</p><p><strong>今日新增:</strong> 33 | <strong>总数:</strong> 58405</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=tmsLbDYm%2FatCietyRvnT6A%3D%3D.QQ%2BFzylYqQ3j8MthgQ1rJwESGRdHYD8QNdqCQ%2FUDEG94usXcyWxGRQ7pZ1BfmKmQ" rel="nofollow" target="_blank">https://github.com/LadybirdBrowser/ladybird</a></p><hr/><h4>● Lua 最热项目</h4><p><strong>项目名称</strong>: yetone/avante.nvim</p><p><strong>项目描述</strong>: 像使用Cursor AI IDE般高效运用您的Neovim</p><p><strong>今日新增:</strong> 13 | <strong>总数:</strong> 17325</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598342" alt="yetone/avante.nvim" title="yetone/avante.nvim" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=KTgoyHM9a3v4ymR%2BsLntaw%3D%3D.zFEnoLk2vONyPeS9PsNs1w%2F%2BelnFGUJkoMbLobP%2F6nPTaQGkFqP1WvfaVzLHY2lI" rel="nofollow" target="_blank">https://github.com/yetone/avante.nvim</a></p><hr/><p><strong>项目名称</strong>: Kong/kong</p><p><strong>项目描述</strong>: 🦍 云原生API网关与AI网关。</p><p><strong>今日新增:</strong> 12 | <strong>总数:</strong> 42695</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598343" alt="Kong/kong" title="Kong/kong" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=rAEEMS3GeBOq32SfRMifFA%3D%3D.smGfds9MzvvIsvQidO1Jn5nHMv0jGZoTQWLLuaq8pSA%3D" rel="nofollow" target="_blank">https://github.com/Kong/kong</a></p><hr/><p><strong>项目名称</strong>: coder/claudecode.nvim</p><p><strong>项目描述</strong>: 🧩 Claude Code Neovim IDE 扩展</p><p><strong>今日新增:</strong> 10 | <strong>总数:</strong> 1967</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=mp5q%2Beh5F6zMqrPO6SXKFg%3D%3D.WRHIimKlELsmfFxyAiR%2FSrJCUEaDZ4RNuxbpZiTTiOGK6mOZ%2B18RYxwWnXmXA0hw" rel="nofollow" target="_blank">https://github.com/coder/claudecode.nvim</a></p><hr/><h4>● Vue 最热项目</h4><p><strong>项目名称</strong>: dreamhunter2333/cloudflare_temp_email</p><p><strong>项目描述</strong>: CloudFlare 免费临时域名邮箱 支持附件收发 IMAP SMTP TelegramBot</p><p><strong>今日新增:</strong> 23 | <strong>总数:</strong> 5972</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047595322" alt="dreamhunter2333/cloudflare_temp_email" title="dreamhunter2333/cloudflare_temp_email" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=hJo1QSYhRmLnIZVb2rjrwA%3D%3D.lZ24lTUxrJhqSr%2FzlYuaKT26tbx8Fb%2Fgv9e4iNp7OYBGGnr2SvHo6b0OKPo9nbJ2Een2BqjxqbLo7942%2FS5%2FiQ%3D%3D" rel="nofollow" target="_blank">https://github.com/dreamhunter2333/cloudflare_temp_email</a></p><hr/><p><strong>项目名称</strong>: zyronon/TypeWords</p><p><strong>项目描述</strong>: 练习英语 一次敲击 一点进步</p><p><strong>今日新增:</strong> 17 | <strong>总数:</strong> 7326</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598344" alt="zyronon/TypeWords" title="zyronon/TypeWords" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=MGs4eIM6j1jETCOis%2FGbpQ%3D%3D.U3AZ%2BEB%2Bj3%2Fp9kfG%2FJAHlZoN8SJ6Z5X6tkCwaYlm%2Fj1CjtcbuQEuoEJFaErbBAch" rel="nofollow" target="_blank">https://github.com/zyronon/TypeWords</a></p><hr/><p><strong>项目名称</strong>: vbenjs/vue-vben-admin</p><p><strong>项目描述</strong>: 一个基于Vue3、Shadcn UI、Vite、TypeScript和Monorepo构建的现代化V...</p><p><strong>今日新增:</strong> 14 | <strong>总数:</strong> 31485</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598345" alt="vbenjs/vue-vben-admin" title="vbenjs/vue-vben-admin" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=RffPVEw7BtRwzulcZ82B2Q%3D%3D.krn3JF6bd%2F7ZiR0pshoj1vH3OGNZ4tqEmLfRSUUEISViIdKfh2GkGjASqQw3LIB%2F" rel="nofollow" target="_blank">https://github.com/vbenjs/vue-vben-admin</a></p><hr/><h4>● Kotlin 最热项目</h4><p><strong>项目名称</strong>: RunanywhereAI/runanywhere-sdks</p><p><strong>项目描述</strong>: 可在本地运行AI的生产就绪工具包</p><p><strong>今日新增:</strong> 165 | <strong>总数:</strong> 6291</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598346" alt="RunanywhereAI/runanywhere-sdks" title="RunanywhereAI/runanywhere-sdks" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=wHkCNRoXttpGEorGDGWndQ%3D%3D.hirv6%2FOBlcR5Y7hZFTjnFBIfRvr6oa9AoF3jsJedbd%2B5xgYea0l2E4vvqYMz%2F57EdMxEa3mcM%2BNu9%2B%2FVrtAKLQ%3D%3D" rel="nofollow" target="_blank">https://github.com/RunanywhereAI/runanywhere-sdks</a></p><hr/><p><strong>项目名称</strong>: tiann/KernelSU</p><p><strong>项目描述</strong>: 基于内核的Android系统root解决方案 （注：根据技术文档惯例，"Kernel based"译...</p><p><strong>今日新增:</strong> 15 | <strong>总数:</strong> 14915</p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=fcQ%2BjNQgHkxtN%2Bid3bnEBA%3D%3D.K%2Fgds%2BxsB1uYRdyhlYgifTsEeua2k2cexSi%2Fo%2B%2BNwruZUzeWOWl%2BUrA3CL2hQ0yd" rel="nofollow" target="_blank">https://github.com/tiann/KernelSU</a></p><hr/><p><strong>项目名称</strong>: JackEblan/Geto</p><p><strong>项目描述</strong>: 为应用配置设备级设置。该项目采用多模块化设计，遵循Bob大叔的整洁架构原则，参考Now in And...</p><p><strong>今日新增:</strong> 9 | <strong>总数:</strong> 761</p><p><strong>项目截图</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598347" alt="JackEblan/Geto" title="JackEblan/Geto" loading="lazy"/></p><p><strong>地址</strong>: <a href="https://link.segmentfault.com/?enc=OHHULLn90Y9rn1qUk71BZA%3D%3D.oDWZfLVFM%2B4G5ejg1%2FkaMipdlIomiK0uhPS97EokyVaXVwQ%2FQ2whUdolfVR%2FkOC0" rel="nofollow" target="_blank">https://github.com/JackEblan/Geto</a></p><hr/><h3>📈 今日趋势分析</h3><p><strong>最活跃语言</strong>: TypeScript(2个)、Python(1个)、Go(1个)</p><p><strong>今日总获星</strong>: 1,633 颗星</p><p><strong>平均获星</strong>: 233 颗星/项目</p><p><strong>今日之星</strong>: openai/skills (583)</p><hr/><h3>📊 数据总览</h3><table><thead><tr><th>指标</th><th>数值</th></tr></thead><tbody><tr><td>收录项目</td><td><strong>7</strong> 个</td></tr><tr><td>编程语言</td><td><strong>50</strong> 种</td></tr><tr><td>今日新增</td><td><strong>1,633</strong> 颗星</td></tr><tr><td>报告日期</td><td><strong>2026年02月06日</strong></td></tr><tr><td>统计周期</td><td><strong>日报</strong></td></tr></tbody></table><hr/><p>TrendForge 致力于追踪全球开源项目动态，每日为开发者精选最具价值的 GitHub 项目。</p><p><strong>数据来源</strong>: <a href="https://link.segmentfault.com/?enc=T9oIC0qOld7G0e9oLGaYCA%3D%3D.uuaaX6htd0%2F8XeXUGAkrxLUP15Qv1t4VqPc51yNs2zU%3D" rel="nofollow" target="_blank">https://trendforge.devlive.top/</a></p><p><strong>数据说明</strong>: 基于 GitHub 官方 API 数据统计，每日更新</p><p><strong>翻译声明</strong>: 项目描述采用 AI 智能翻译，如有疏漏请以原文为准</p><p><em>报告生成于: 2026年02月07日</em></p><h2>GitHub #开源项目 #技术趋势 #程序员 #软件开发</h2>]]></description></item><item>    <title><![CDATA[IPERFforWindowsTrialSigned网络带宽测试工具安装步骤详解（附网络带宽测试教程]]></title>    <link>https://segmentfault.com/a/1190000047598367</link>    <guid>https://segmentfault.com/a/1190000047598367</guid>    <pubDate>2026-02-07 11:02:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p><code>IPERFforWindowsTrialSigned</code>是 <strong>iperf 网络带宽测试工具的 Windows 安装包</strong>，iperf 能在两台电脑或设备之间测网络吞吐量（就是看网速到底能跑多快），运维、网络调试、测 Wi-Fi 或局域网性能时常用。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=%2Bv9JAoAoBMY2tFScI6wQ8Q%3D%3D.P2zSHCbONFye2oau4%2BYoQy6psLAj5STYaCxyfPcRpWb7EJD8mzyOEArNvTNSksUz" rel="nofollow" title="https://pan.quark.cn/s/6d027407c943" target="_blank">https://pan.quark.cn/s/6d027407c943</a></p></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>IPERFforWindowsTrialSigned.exe</code>运行。</li><li>如果是 Win10/Win11，会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> （需要管理员权限）。</li><li>进入安装向导，选语言（默认 English，有的版本有中文）→ 点  <strong>“Next”</strong> 。</li><li>阅读许可协议 → 选 “I accept…” → 点  <strong>“Next”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\Program Files\iperf</code>或类似路径，可点 Browse 改到 D 盘。</li></ul></li><li><p>附加任务：</p><ul><li>建议勾 “Create a desktop shortcut”（创建桌面快捷方式），方便以后打开。</li></ul></li><li>点  <strong>“Install”</strong> ​ 开始安装，等进度条走完（很快，几秒到十几秒）。</li><li>安装完会问是否立即启动 → 可先取消，iperf 一般用命令行跑，不会自动弹 GUI。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>装完后，iperf 其实是个命令行工具，在开始菜单或安装目录能找到 <strong>iperf3.exe</strong>（或 iperf.exe）。</li><li>按 <code>Win+R</code>输入 <code>cmd</code>回车，打开命令提示符。</li><li><p>切到安装目录，比如：</p><pre><code>cd "C:\Program Files\iperf\bin"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p></li><li><p><strong>测网速基本流程</strong>：</p><ul><li><p>一台电脑当服务端：</p><pre><code>iperf3 -s</code></pre></li></ul></li></ol><pre><code>-   另一台电脑当客户端（连服务端 IP）：

    ```
    iperf3 -c 服务端IP
    ```



-   跑完会显示带宽、丢包、抖动等信息。
</code></pre><ol><li><p>常用参数：</p><ul><li><code>-t</code>设置测试时长（秒），比如 <code>-t 30</code>测 30 秒。</li><li><code>-P</code>设置并发连接数，比如 <code>-P 4</code>用 4 条流同时测。</li></ul></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[LoRaWAN的网络拓扑：深入解析与门思科技的创新实践 赵明飞 ]]></title>    <link>https://segmentfault.com/a/1190000047598372</link>    <guid>https://segmentfault.com/a/1190000047598372</guid>    <pubDate>2026-02-07 11:01:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>引言<br/>在物联网（IoT）的浪潮中，低功耗广域网（LPWAN）技术扮演着至关重要的角色。其中，LoRaWAN作为一种开放标准，以其远距离、低功耗的特性，在智能城市、智慧农业、工业物联网等领域展现出巨大的应用潜力。理解LoRaWAN的网络拓扑结构，是深入掌握其工作原理和应用部署的关键。本文将详细解析LoRaWAN的网络拓扑，并介绍门思科技（Manthink）如何通过其创新的产品和解决方案，助力LoRaWAN生态系统的发展。<br/>LoRaWAN网络拓扑概述<br/>LoRaWAN网络采用独特的“星形拓扑（Star-of-Stars Topology）”结构，这与传统的蜂窝网络或Wi-Fi网络有着显著的区别。在这种拓扑中，终端设备（End Devices）不直接与网络服务器通信，而是通过一个或多个网关（Gateway）进行数据中继。这种设计极大地简化了终端设备的复杂性，降低了功耗，延长了电池寿命。<br/>LoRaWAN网络主要由以下四个核心组成部分构成：</p><ol><li>终端设备（End Devices）：也称为节点，是网络的最前端，负责采集数据（如温度、湿度、位置等）或执行控制指令。它们通常是电池供电，通过LoRa无线技术与网关通信。</li><li>网关（Gateways）：也称为基站或集中器，是连接终端设备和网络服务器的桥梁。网关接收来自终端设备的LoRa信号，并将其转换为IP数据包，通过标准IP连接（如以太网、Wi-Fi或蜂窝网络）转发到网络服务器。同时，网关也能将网络服务器的下行数据转发给终端设备。</li><li>网络服务器（Network Server, NS）：是LoRaWAN网络的核心大脑，负责管理整个网络的运行。它的主要功能包括：数据去重、上行数据路由到正确的应用服务器、下行数据调度、自适应数据速率（ADR）管理、设备激活（OTAA/ABP）等。网络服务器确保了数据在终端设备和应用服务器之间的可靠传输。</li><li>应用服务器（Application Server）：负责处理和存储来自终端设备的业务数据，并向下行发送控制指令。它是最终用户或应用程序与LoRaWAN网络交互的接口，通常会提供数据可视化、分析和应用集成等功能。<br/>这种星形拓扑结构使得LoRaWAN网络具有高扩展性、低功耗和广覆盖的优势。终端设备无需维护复杂的连接，只需将数据发送到任何可接收的网关，由网络服务器进行统一管理和路由。</li></ol><p>LoRaWAN网络拓扑的详细解析<br/>终端设备（End Devices）<br/>终端设备是LoRaWAN网络的感知层，它们可以是各种传感器、计量表或执行器。这些设备通常部署在偏远地区或难以供电的环境中，因此低功耗是其设计的核心考量。LoRaWAN协议通过优化通信机制，如Class A、Class B和Class C操作模式，以平衡功耗和通信延迟。<br/>门思科技（Manthink） 在终端设备领域提供了多样化的解决方案，以满足不同行业的需求。例如，支持EB的模组OMx22S，能够兼容CJ/T 188、DL/T 645、Modbus等多种协议，用户只需进行简单的硬件改动，即可将现有设备快速升级为LoRaWAN设备，大大降低了开发难度和成本。此外，DTU（数据传输单元） 产品，包括防水的DTU RDO21x 和 导轨式DTU RDI22x，能够支持CJ/T 188、DL/T645等物联网设备的接入，为传统设备的LoRaWAN化提供了便捷途径。SE72温湿度表 更是凭借其IP65防护等级和长达8年的电池寿命，成为恶劣环境下数据采集的理想选择。<br/>网关（Gateways）<br/>网关是LoRaWAN网络中的关键基础设施，负责接收来自终端设备的LoRa信号并将其转发至网络服务器。一个网关可以覆盖数公里甚至数十公里的范围，并同时处理数千个终端设备的数据。网关通常部署在建筑物顶部或高塔上，以获得最佳的覆盖范围。<br/>门思科技（Manthink） 的网关产品线提供了企业级的解决方案。室外网关GDO51系列 和 室内网关GDI51系列 均基于Ubuntu操作系统，能够适应复杂的企业内网环境。它们支持多种主流协议，如ChirpStack、Basic Station、TTN、ThinkLink、GWMP等，这意味着门思科技的网关可以无缝接入任何支持这些协议的LoRaWAN系统，为用户提供了极大的灵活性和兼容性。<br/>网络服务器（Network Server, NS）<br/>网络服务器是LoRaWAN网络的“大脑”，它管理着所有终端设备的连接、数据路由和安全。网络服务器负责处理上行数据（从设备到应用）和下行数据（从应用到设备），并确保数据的完整性和安全性。自适应数据速率（ADR）功能也是由网络服务器控制，它根据终端设备与网关之间的链路质量动态调整数据速率，以优化网络容量和终端设备电池寿命。<br/>门思科技（Manthink） 在网络服务器领域拥有强大的自研产品——ThinkLink。ThinkLink云版本 支持全球LoRaWAN标准，用户可以免费注册并免费接入多达1000个LoRaWAN设备，这对于小型项目或个人开发者来说是一个巨大的优势。它支持任何品牌的支持GWMP和ThinkLink协议的网关接入，极大地扩展了其兼容性。此外，ThinkLink-Edge版本 是一款高性能的边缘计算网络服务器，配备8核处理器、8GB DDR内存和64GB eMMC存储，并内嵌了Home Assistant和ThingsBoard。它支持与Home Assistant、ThingsBoard、BACnet的无缝对接，为本地数据处理和智能自动化提供了强大的支持，特别适用于对数据实时性、安全性要求较高的工业和商业应用场景。<br/>应用服务器（Application Server）<br/>应用服务器是LoRaWAN网络的最终目的地，它接收来自网络服务器的数据，并将其转换为用户可理解和利用的信息。这些信息可以用于数据分析、可视化、告警通知或与其他业务系统集成。应用服务器通常由最终用户或第三方服务提供商开发和维护。<br/>门思科技的产品理念是为用户提供一个简单、高效的LoRaWAN解决方案。通过自研的低功耗操作系统（MPOS）和边缘计算虚拟器（Edge-bus），门思科技的产品家族能够支持全球频段的LoRaWAN标准，并具备十三大功能点以适应复杂的应用场景。从2014年开始，门思科技的产品已经在南美、欧洲、日本等全球多个国家和地区有着长期广泛的应用，积累了超过10年的现场稳定运行经验，充分证明了其产品的可靠性和稳定性。<br/>LoRaWAN网络拓扑图示例<br/>为了更直观地理解LoRaWAN的网络拓扑，以下是一个典型的LoRaWAN网络架构图：<br/>[此处插入LoRaWAN网络拓扑图]</p><p>门思科技（Manthink）在LoRaWAN生态中的角色<br/>门思科技作为LoRaWAN领域的先行者和创新者，致力于提供从模组、终端设备、网关到网络服务器的全栈式解决方案。我们的产品家族基于自研的低功耗操作系统（MPOS）和边缘计算虚拟器（Edge-bus），支持全球频段的LoRaWAN标准，并具有十三大功能点以适应复杂的应用场景。这些产品已经在包括南美、欧洲、日本等全球多个国家和地区有着长期广泛的应用，最早的规模化应用从2014年开始到现在已经超过10年的现场稳定运行，充分证明了门思科技产品的可靠性和稳定性。<br/>我们的优势：<br/>● 全栈式解决方案：提供从硬件到软件，从设备到云端的完整LoRaWAN解决方案。<br/>● 技术领先：自研MPOS和Edge-bus，确保产品性能和稳定性。<br/>● 全球兼容：支持全球频段的LoRaWAN标准，适应不同国家和地区的需求。<br/>● 丰富功能：十三大功能点，满足复杂多样的应用场景。<br/>● 长期稳定运行：超过10年的现场稳定运行经验，品质值得信赖。<br/>总结<br/>LoRaWAN以其独特的星形拓扑结构，为物联网应用提供了低功耗、远距离的连接能力。理解其网络组成部分——终端设备、网关、网络服务器和应用服务器——对于成功部署和管理LoRaWAN网络至关重要。门思科技（Manthink）凭借其在LoRaWAN领域的深厚积累和创新产品，为全球用户提供了可靠、高效、易于部署的LoRaWAN解决方案，助力各行各业实现数字化转型。<br/>了解更多门思科技（Manthink）<br/>● 门思科技官方网站：<a href="https://link.segmentfault.com/?enc=bd3LtxXDf14dXAlc6o5vXQ%3D%3D.LxQFooHTVdN5ae21l7c336IZsfL%2FErW5q6nNvCPk7hs%3D" rel="nofollow" target="_blank">https://www.manthink.cn</a><br/>● 门思科技LoRaWAN NS 产品：<a href="https://link.segmentfault.com/?enc=puE9aMay9JGeeDWrMPGMfA%3D%3D.77CR3BPNsvm0HkW1kB03EDVvGIDyXoiPinkr5dHbVuM%3D" rel="nofollow" target="_blank">https://thinklink.manthink.cn</a> (小项目可以免费使用ThinkLink)<br/>● 联系邮箱：<a href="mailto:info@manthink.cn" target="_blank">info@manthink.cn</a><br/>关键词： LoRa, LoRaWAN, 网关, Gateway, NS, Manthink, 门思科技, 物联网, LPWAN, 网络拓扑</p>]]></description></item><item>    <title><![CDATA[EazyDraw for Mac v11.4.1上专门画矢量图安装教程 简单步骤 Mac版 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047598390</link>    <guid>https://segmentfault.com/a/1190000047598390</guid>    <pubDate>2026-02-07 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p>EazyDraw 是 Mac 上<strong>专门画矢量图的工具</strong>，简单说就是用来做平面图、图标、插画、技术绘图这些，画出来的图放大不会糊，适合需要干净线条和精确尺寸的场景。</p><h4>1. 先下载好安装包</h4><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=peRcnrPPTysL2bznWVro2g%3D%3D.REaa%2BwrrckdAvvkGb6CoZi%2BK5TsOjKXOp3ymjZercTl7Rx4ogK6OLT3mu6LJPgg%2B" rel="nofollow" title="https://pan.quark.cn/s/f42ce6432768" target="_blank">https://pan.quark.cn/s/f42ce6432768</a> ，把 <code>EazyDraw for Mac v11.4.1.dmg</code>文件下载到你的 Mac（比如放桌面或下载文件夹，别塞太深的子文件夹，一会儿好找）。</p><h4>2. 打开 dmg 镜像文件</h4><p>找到下载好的 <code>.dmg</code>文件，<strong>双击它</strong>——屏幕会弹出一个新窗口，里面一般有俩东西：一个是“EazyDraw”的图标（一般是浅色方块，上面有绘图笔或几何图形样式），另一个是“应用程序”文件夹的快捷方式（小文件夹图标）。</p><h4>3. 把软件拖进“应用程序”文件夹</h4><p>按住“EazyDraw”图标，<strong>直接拖到旁边的“应用程序”文件夹里</strong>（跟平时拷贝文件一样），等进度条走完，这一步就装好了。</p><h4>4. 首次打开要“解锁”（重点！）</h4><p>去“应用程序”文件夹找到 EazyDraw，<strong>双击打开</strong>。第一次运行时，macOS 会弹提示“无法验证开发者”，别慌：</p><ul><li>点左上角苹果图标 → 选“系统设置”（旧版叫“系统偏好设置”）→ 左侧点“隐私与安全性”；</li><li>右边往下翻，找到“安全性”区域，会看到“已阻止使用‘EazyDraw’，因为来自身份不明的开发者”，下面有个“仍要打开”按钮，<strong>点一下</strong>，再输开机密码确认就行（如果没看到“仍要打开”，先关掉提示窗口，重新打开软件，提示会再出现）。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[Linux安装Temporal工作流引擎 YYGP ]]></title>    <link>https://segmentfault.com/a/1190000047598302</link>    <guid>https://segmentfault.com/a/1190000047598302</guid>    <pubDate>2026-02-07 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Docker一键安装Temporal， 使用外部MySQL数据库</p><h2>1. 初始化MySQL数据库</h2><pre><code class="sql"># 创建用户 temporal
CREATE USER 'temporal'@'%' IDENTIFIED BY 'temporal';

# 创建数据库 temporal
CREATE DATABASE `temporal` DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;
grant all privileges on `temporal`.* TO 'temporal'@'%';

# 创建数据库 temporal_visibility
CREATE DATABASE `temporal_visibility` DEFAULT CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci;
grant all privileges on `temporal_visibility`.* TO 'temporal'@'%';</code></pre><h2>2. 配置文件</h2><p>docker-compose.yml</p><pre><code>services:
  temporal-init:
    image: temporalio/auto-setup:1.29.3
    container_name: temporal_init
    environment:
      DB: mysql8
      # 改为正确MySQL配置
      MYSQL_SEEDS: "192.168.2.215"
      DB_PORT: 3306
      MYSQL_USER: "temporal"
      MYSQL_PWD: "temporal"
      MYSQL_DB: "temporal"
      DEFAULT_NAMESPACE: "default"
    command: ["temporal-sql-tool", "create-schema", "-k", "default", "-v", "1.19"]
    restart: "no"
    
  temporal:
    image: temporalio/auto-setup:1.29.3
    container_name: temporal_server
    environment:
      # 使用外部 MySQL
      DB: mysql8
      MYSQL_SEEDS: "192.168.2.215"    # 你的 MySQL 地址
      DB_PORT: 3306
      MYSQL_USER: "temporal"
      MYSQL_PWD: "temporal"
      MYSQL_DB: "temporal"                 # Temporal 数据库
      DEFAULT_NAMESPACE: "default"
      # 这里填127.0.0.1会报错， 需要在ports将7233端口映射到宿主机， 填写宿主机的IP
      TEMPORAL_BROADCAST_ADDRESS: "192.168.2.215"
    ports:
      - "7233:7233"   # gRPC frontend
      - "7234:7234"   # history
      - "7235:7235"   # matching
      - "7239:7239"   # worker
      - "8088:8088"   # Temporal Web API (可选)
    depends_on:
      - temporal-init
    restart: always

  temporal-ui:
    image: temporalio/ui:2.45.0
    container_name: temporal_ui
    environment:
      TEMPORAL_ADDRESS: "temporal:7233"  # 指向 Temporal Server 容器名
      TEMPORAL_UI_PORT: "8080"
    ports:
      - "8080:8080"   # 浏览器访问
    depends_on:
      - temporal
    restart: always</code></pre><p>temporal-init: 用于自动初始化数据<br/>temporal: 启动核心进程<br/>temporal-ui: 启动UI管理界面</p><h2>3. 启动</h2><pre><code class="shell"># 启动
docker compose up -d
# 查看日志
docker logs --tail=100 -f temporal_server</code></pre><p>访问 <a href="https://link.segmentfault.com/?enc=8jw7cKu%2BqfySG4Mj%2FNSJ9g%3D%3D.yF8PQ2d1uf4Ky6nxKKmWhSk9iIOcFGzVvodrkkzDKZU%3D" rel="nofollow" target="_blank">http://192.168.2.215:8080/</a> 查看管理后台<br/><img width="723" height="182" referrerpolicy="no-referrer" src="/img/bVdnSEH" alt="image.png" title="image.png"/></p><p>销毁命令:</p><pre><code>docker compose down
docker container list</code></pre>]]></description></item><item>    <title><![CDATA[职场未来：AI时代的价值坐标系 本文系翻译，阅读原文
https://newsletter.jant]]></title>    <link>https://segmentfault.com/a/1190000047598292</link>    <guid>https://segmentfault.com/a/1190000047598292</guid>    <pubDate>2026-02-07 09:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>导言</h3><p>周一早晨，你打开笔记本电脑，心中萦绕着一个问题：两年后，我的工作是否还有意义？</p><p>上周，你花了三小时撰写一份活动方案。而一位同事使用AI智能体，仅用四分钟就生成了质量接近80%的版本——如果诚实地说，可能接近90%。</p><p>不是担心是否会失业，而是担心你所做的工作是否还能体现价值。你依然保住了工作，但你能感觉到它在不断缩水。问题不在于“机器人来了”，而在于你不再清楚自己该擅长什么。花了五年积累的Excel技能？自动化了。分析竞争对手并整合信息的能力？已有AI代劳。清晰撰写项目进展的技巧？不复存在。</p><p>你的职业身份正在以你无法追赶的速度消失，却无人告诉你下一步该何去何从。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSEx" alt="" title=""/></p><hr/><h3>案例分享</h3><p>Sarah是资深研究分析师，十年经验，时薪250美元。公司引入AI研究助手后：</p><ul><li>初期：AI用90分钟完成她需2-3周的基础研究，她转而负责“高阶分析”。</li><li>六个月后：公司质疑她工作的附加价值，将她的角色转为“质量审核员”，时薪降至150美元。</li><li>最终：公司用AI搭配两名初级员工（年薪6.5万美元）替代了她85%的工作。</li><li><strong>核心问题</strong>：无人能明确定义“更高价值的工作”具体是什么，企业最终只能选择成本更低的方案。</li></ul><hr/><h3>职场困境</h3><p>这一现象并非个人失败所致，而是经济激励结构的必然结果：</p><ul><li>企业通过AI降低成本，经理只需比较AI订阅费与员工薪资便可做出决策。</li><li>但企业缺乏动力为员工设计未来角色，因为“重新定义工作”无法在财报中体现短期回报。</li><li>速度错配：AI能力以6-12个月为周期迭代，而人通过教育或企业培训的适应周期长达2-5年。</li><li>工业时代的制度无法解决指数级变化的问题，导致个人陷入系统性困境。</li></ul><hr/><h3>常见的应对策略</h3><p>当感到自身价值被侵蚀时，人们通常会采取看似合理的应对方式：</p><ol><li><p><strong>更熟练地使用AI工具</strong></p><ul><li>学习提示工程，掌握ChatGPT、Claude等平台，成为团队中的“AI专家”。</li><li><strong>致命伤</strong>：仍在比拼执行速度，而执行本身正被标准化，一旦工具被大幅优化，“提示技巧”便会失效。</li></ul></li><li><p><strong>深耕现有专业领域</strong></p><ul><li>会计师钻研更复杂的税法，设计师学习更多软件，分析师构建更精细的模型。</li><li><strong>致命伤</strong>：在逐渐被自动化的领域深入，如同在洪区筑堡垒，AI已能逼近专家水平，专长反而成为包袱。</li></ul></li><li><p><strong>强调“软技能”</strong></p><ul><li>试图通过创造力、同理心或人际关系凸显“不可替代的人性”。</li><li><strong>致命伤</strong>：这些概念过于模糊，难以度量，当AI能10秒生成100个创意时，“保持人性”无法转化为具体价值。</li></ul></li></ol><p><strong>根本问题</strong>：上述策略都是被动适应，而非主动重塑，真正有效的是构建一个前所未有的新角色。</p><hr/><h3>有效策略：成为协同指挥者</h3><p>不要只执着于优化现有工作，要充分利用AI完成此前不可能的任务，<strong>持续发现约束消失后的新可能性</strong>：</p><ul><li><strong>案例</strong>：市场营销者Marcus用AI同时运行50个活动变体，他的角色转变为设计测试框架、解读数据模式、制定战略决策。</li><li><strong>关键</strong>：找到因人力限制而无法规模化的环节，用AI突破瓶颈，并专注于决策层。</li><li><p><strong>行动指南</strong>：</p><ul><li>第一周：找出一个因耗时过长而无法大量进行的工作。</li><li>第二周：用AI将其规模扩大10倍，容忍质量暂时下降。</li><li>第三周：分析规模化带来的新洞察。</li><li>第四周：向老板展示“新增能力”而不仅仅是“效率提升”。</li></ul></li></ul><hr/><h3>结语</h3><p>AI正揭示一个残酷的真相：许多人所谓的“战略思维”，其实只是严谨的执行力。当AI以惊人速度接管基础工作，那些曾被经验掩盖的、真正战略洞察力的缺失，便暴露无遗。企业曾习惯将“资深”等同于“有战略判断力”，而AI的到来，迫使所有人直面这一认知误区。经验堆积成的护城河，正在技术的冲击下迅速瓦解。</p><p>别再执着于捍卫那个正在缩水的旧角色。真正的出路，是主动构建一个——甚至六个月前都还不存在的——新角色。成为那个率先洞察新可能性，并围绕它构建价值的人。不要指望企业为你规划未来，也别等待教育系统赶上变革。在这个快速迭代的时代，唯一可靠的，是自我重塑的能力。</p><p>周一的清晨依旧会来，不同的是：当别人仍在焦虑中追问“我的价值何在”时，你是否已经走在了构建答案的路上。</p>]]></description></item><item>    <title><![CDATA[中小型企业常用的SRM软件有哪些？2026年选型指南 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047593593</link>    <guid>https://segmentfault.com/a/1190000047593593</guid>    <pubDate>2026-02-07 08:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>越来越多企业开始用SRM软件，把原本依赖Excel、邮件、微信群的采购协作过程，升级为更标准、更可追溯、更可分析的数字化流程。</p><p>以华为云云商店上架的SRM软件产品介绍为例，成熟的SRM软件方案通常会围绕四大模块搭建能力框架：<strong>供应商管理中心、价格管理中心、采购执行协同中心、采购商城中心</strong>。</p><p>对于中小企业来说，选对SRM系统往往意味着：采购团队终于能从“处理琐碎流程”转向“做供应商管理与成本优化”。今天我将从以下三点展开去讲，希望能对中小企业有所帮助。</p><p>1、中企业企业为什么要SRM软件？</p><p>2、几款主流的SRM软件介绍</p><p>3、如何选择适合的SRM软件？</p><p><strong>一、为什么中小企业特别需要SRM系统？</strong></p><p>不少中小企业的采购管理，仍停留在“Excel+纸质单据+人工沟通”的阶段。短期内能跑，但一旦企业规模上来，问题会集中爆发：</p><p><strong>1、流程繁琐、效率偏低</strong><br/>从需求提出、询价比价、下单审批到对账结算，环节多且高度依赖人工操作，容易出错，也很难标准化。</p><p><strong>2、供应商管理分散，信息更新滞后</strong><br/>供应商档案、资质文件、合同、历史交易记录散落在多个表格或文件夹里，更新不及时，关键风险（比如资质过期、交付异常）很难提前发现。</p><p><strong>3、价格与成本不可控，采购“靠经验”</strong><br/>缺少统一的价格库、历史报价追溯困难，比价辑不透明，降本更多靠采购员个人能力，难以沉淀为组织能力。</p><p><strong>4.绩效评估缺机制，无法科学分级管理</strong><br/>交付准时率、质量问题、服务响应等数据无法形成体系，导致供应商管理长期停留在“印象打分”，优胜劣汰难执行。</p><p><strong>二、正远SRM：全景化协同采购管理方案</strong></p><p><a href="https://link.segmentfault.com/?enc=nIGCG%2FJVauDivDGE5ko50Q%3D%3D.5yjoqFRVRIBCm5DLIQSE%2FMIas3Hn%2BxcTwaEFnUYBA%2FY%3D" rel="nofollow" target="_blank">https://www.zhengyuantech.cn/</a></p><p>在华为云云商店上架的<strong>正远SRM数字化采购管理平台</strong>，定位是“采购全过程数字化与供应商协同网络构建”。其产品介绍明确强调：通过电子化流程与多种寻源方式，帮助企业提升采购效率、提高透明度并降低采购成本。</p><p>正远SRM的一大特点是采购方式覆盖较广，支持<strong>询比价采购、招标采购、竞价采购</strong>等，同时也提供多种采购组织模式的适配能力。</p><p>它的核心能力围绕四大模块展开：</p><p>1、<strong>供应商管理中心</strong><br/>支持供应商全生命周期管理，包括准入、资质、供货能力与产能评估等，强调把好准入关，形成科学供应商管理体系。</p><p>2、<strong>价格管理中心</strong><br/>提供采购预询价、比价采购、招标、竞价等多方式寻源策略，用于建立更体系化的价格管理与成本优化机制。</p><p>3、<strong>采购执行协同中心</strong><br/>通过供应商门户/协同网络实现订单协同：订单下发、交付反馈、异常处理等流程在线化，提高执行透明度与协同效率。</p><p>4、<strong>采购商城中心</strong><br/>面向标准物资采购提供内部采购商城能力，覆盖商品发布、价格审批、上架下架、购物车、订单中心等功能，实现自助式集中采购。</p><p>总体而言，正远SRM强调“轻灵活、低耦合”，对于需求变化快、流程迭代频繁的中小企业更友好。<br/><img width="723" height="346" referrerpolicy="no-referrer" src="/img/bVdnRqE" alt="" title=""/></p><p><strong>三、金蝶AI星辰：轻量化云SRM选择</strong></p><p>预算相对谨慎、希望快速上线的中小企业，通常会优先考虑云端SaaS类产品。金蝶面向小微企业推出的<strong>金蝶云·星辰</strong>定位是“小微企业SaaS管理云”，主打免安装、免维护、快速开通使用，并支持开放API接口连接生态。</p><p>在采购数字化方向，金蝶也有对应的采购云能力：例如金蝶云星空采购云强调供采双方协同的数智化采购平台思路。</p><p>对中小企业来说，星辰类产品的优势通常体现在：</p><p>1、<strong>SaaS订阅模式降低门槛</strong><br/>无需部署硬件与维护服务器，初期投入相对可控。</p><p>2、<strong>易上手更利于推进供应商协同</strong><br/>供应商侧操作越轻量，落地成功率越高。</p><p>3、<strong>与财务、进销存等体系形成联动</strong><br/>中小企业往往更关注“业务财务一体化”，避免数据割裂。<br/><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdnRqF" alt="" title="" loading="lazy"/></p><p><strong>四、其他主流SRM系统盘点</strong></p><p>除正远、金蝶外，中小型企业在SRM选型中还会常见以下几类方案：</p><p>1、<strong>8Manage SRM</strong><br/>覆盖寻源、招标、采购订单、合同管理等全流程，支持SaaS与本地部署，适合流程相对复杂、希望强化报表分析与权限控制的企业。</p><p>2、<strong>携客云SRM</strong><br/>偏“性价比与易用性”的云端采购管理工具，适合预算更有限、希望快速上线、优先解决协同与流程电子化的小型企业。</p><p>3、<strong>用友BIP采购云</strong><br/>用友采购云强调从寻源到签约的数字化，并提供电子招投标能力，覆盖从立项到定标的全过程，同时也包含采购商城能力。<br/>整体更偏平台化、体系化，适合有一定规模、对合规与流程控制要求更高的企业。</p><p>4、<strong>简道云（零代码）</strong><br/>如果企业采购场景差异大，或者希望低成本快速搭建个性化流程，零代码方案也是现实选择。优点是灵活与低门槛，但复杂SRM场景往往需要较多自定义设计。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnRqG" alt="" title="" loading="lazy"/></p><p><strong>五、如何选择适合的SRM系统？</strong></p><p>面对众多选择，中小企业建议抓住几个“选型硬指标”，避免被演示效果带偏：</p><p>1、<strong>先明确企业核心需求，不要一开始就追求“大而全”</strong><br/>中小企业优先把“供应商档案统一、寻源比价、订单协同、对账效率”这些刚需做扎实。</p><p>2、<strong>优先考虑部署与成本模式：云优先，本地谨慎</strong><br/>SaaS订阅模式更适合中小企业，避免前期投入过大、上线周期过长。</p><p>3、<strong>考察集成能力，避免数据孤岛</strong><br/>优先选择与现有ERP/财务体系同生态产品，或开放API较完整的平台。</p><p>4、<strong>易用性决定落地率，尤其是供应商侧</strong><br/>供应商端如果操作复杂、培训成本高，协同很难推起来。</p><p>5、<strong>服务保障要写进合同，别只听口头承诺</strong><br/>关注实施周期、培训方式、响应SLA、驻场与远程支持能力。</p><p>一家机械制造企业引入SRM系统后，供应商准入审核周期从7天缩短到2天，优质供应商占比提升40%。采购人员的时间分配也发生明显变化：从处理琐碎事务转向做谈判与供应商管理。</p><p>当一家电子元器件经销商上线SRM系统三个月后，供应商引入周期从14天缩短至3天，采购成本下降8%，库存周转率提升22%。</p><p>这些变化背后，是采购协作方式的改变——<strong>流程更透明、数据能沉淀、风险可预警</strong>，供应链也因此更敏捷、更抗风险。</p>]]></description></item><item>    <title><![CDATA[别只会“加索引”了！这 3 个 PostgreSQL 反常识优化，能把性能和成本一起打下来 吾日三省]]></title>    <link>https://segmentfault.com/a/1190000047598107</link>    <guid>https://segmentfault.com/a/1190000047598107</guid>    <pubDate>2026-02-06 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047598109" alt="image" title="image"/></p><p>数据库性能优化这事儿，很多人条件反射就三板斧：<strong>改 SQL、加索引、再加索引</strong>。一通操作下来，查询快了，磁盘炸了；延迟降了，维护成本上天；更扎心的是——你还以为自己“优化得很专业”。😅</p><p>这篇文章的思路很“叛逆”：与其在常规套路里打转，不如换个角度，利用 PostgreSQL 本身的一些机制做“非常规优化”。下面挑 3 个最容易落地、同时最容易被忽略的点，讲透它们为什么能省钱又提速。</p><hr/><h2>1）别再全表扫了</h2><p>先看一个特别真实的场景：用户表只有两种 plan：<code>free</code> 和 <code>pro</code>，并且写了约束，保证不会出现别的值。</p><pre><code class="sql">CREATE TABLE users (
    id INT PRIMARY KEY,
    username TEXT NOT NULL,
    plan TEXT NOT NULL,
    CONSTRAINT plan_check CHECK (plan IN ('free', 'pro'))
);</code></pre><p>然后某位大佬在报表工具里一顿操作猛如虎，写了个：</p><pre><code class="sql">SELECT * FROM users WHERE plan = 'Pro';</code></pre><p>注意大小写：<code>Pro</code> ≠ <code>pro</code>。结果当然是 0 行。问题是——它为了得到“0 行”，居然可能 <strong>把全表扫了一遍</strong>，这就很离谱：明明约束告诉你“根本不可能有 Pro”，你还扫什么扫？扫得我 CPU 风扇都快起飞了。</p><p>这时可以打开一个“看起来冷门但对报表场景很香”的开关：<code>constraint_exclusion</code>。</p><pre><code class="sql">SET constraint_exclusion to 'on';</code></pre><p>开启后，PostgreSQL 会在生成执行计划时参考约束信息，发现条件永远为假，就直接变成“秒回 0 行”，彻底避免无意义的 Seq Scan。</p><p>为什么它默认不是 <code>on</code>？因为它会增加规划阶段开销：对“系统自动生成的简单查询”，大概率用不上；但对 BI/报表这种<strong>人肉手写 SQL</strong>的场景，写错值、写错条件太常见了。<br/>结论很直白：<strong>如果你的数据库经常被报表工具/分析师/临时查询折腾，考虑在报表环境把它打开，能省不少冤枉资源。</strong></p><hr/><h2>2）只要“按天统计”，就别用“精确到秒”的索引</h2><p>函数索引省 3 倍空间还更快</p><p>第二个场景更像日常优化现场：10M 的销售表 <code>sale</code>，分析师要做按天汇总：</p><pre><code class="sql">SELECT date_trunc('day', sold_at AT TIME ZONE 'UTC'), SUM(charged)
FROM sale
WHERE '2025-01-01 UTC' &lt;= sold_at AND sold_at &lt; '2025-02-01 UTC'
GROUP BY 1;</code></pre><p>大家的第一反应：给 <code>sold_at</code> 上 B-Tree 索引！</p><pre><code class="sql">CREATE INDEX sale_sold_at_ix ON sale(sold_at);</code></pre><p>查询确实快了，但你一看索引体积——214MB，心里也跟着“咯噔”一下：为了按天统计，建了个精确到毫秒级的索引，这属于<strong>用大炮打蚊子</strong>。</p><p>更聪明的做法是：只索引“天”，别索引“秒”。直接上表达式索引（函数索引）：</p><pre><code class="sql">CREATE INDEX sale_sold_at_date_ix ON sale((date_trunc('day', sold_at AT TIME ZONE 'UTC'))::date);</code></pre><p>然后把查询写成同样表达式：</p><pre><code class="sql">SELECT date_trunc('day', sold_at AT TIME ZONE 'UTC'), SUM(charged)
FROM sale
WHERE date_trunc('day', sold_at AT TIME ZONE 'UTC')::date BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY 1;</code></pre><p>结果：索引体积从 214MB 变成 66MB，直接小了 3 倍多，还更快。原因不只是 <code>date</code> 比 <code>timestamptz</code> 小，而是<strong>离散值更少</strong>，B-Tree 可以做去重优化（deduplication），索引变得更紧凑。</p><p>但函数索引有个“脾气”：表达式得一模一样，稍微换个写法就可能用不上索引，比如把 <code>date_trunc</code> 换成 <code>::date</code>，就直接退化回 Seq Scan。现实里让全团队“严格写同一表达式”，基本等同于要求大家每天不犯错（这事比上班准点还难🙂）。</p><p>解决方案有两种：</p><h3>方案 A：View，把表达式固化成列</h3><pre><code class="sql">CREATE VIEW v_sale AS
SELECT *, date_trunc('day', sold_at AT TIME ZONE 'UTC')::date AS sold_at_date
FROM sale;</code></pre><h3>方案 B：Generated Column（更像“官方自带的 view 列”）</h3><p>从 PostgreSQL 14 起支持生成列；文章里提到 PostgreSQL 18 还支持<strong>虚拟生成列</strong>：看起来是列，实际上是每次访问时计算的表达式，既保证表达式一致，又不额外存储（主打一个“既要又要”）。</p><pre><code class="sql">ALTER TABLE sale ADD sold_at_date DATE
GENERATED ALWAYS AS (date_trunc('day', sold_at AT TIME ZONE 'UTC'));</code></pre><p>然后查询就统一写：</p><pre><code class="sql">SELECT sold_at_date, SUM(charged)
FROM sale
WHERE sold_at_date BETWEEN '2025-01-01' AND '2025-01-31'
GROUP BY 1;</code></pre><p>这类优化特别适合那种“指标按天/按周/按月统计”的系统：<strong>别让索引为你用不到的精度买单</strong>。</p><hr/><h2>3）长 URL 唯一约束把索引撑爆？</h2><p>用排他约束 + Hash 索引，5 倍缩容（但有坑）</p><p>当你要对一个超长字段（比如 URL）做唯一约束时，B-Tree 索引可能接近表本体大小，因为 B-Tree 叶子节点会存储被索引值本身。URL 又长又几乎不重复，索引很容易“胖成球”。</p><p>那能不能用 Hash 索引？Hash 索引存的是 hash 值，通常小很多。问题来了：PostgreSQL 的 Hash 索引 <strong>不支持 unique index</strong>。</p><pre><code class="sql">CREATE UNIQUE INDEX urls_url_unique_hash ON urls USING HASH(url);
-- ERROR: access method "hash" does not support unique indexes</code></pre><p>但 PostgreSQL 还有个很少人用、名字很霸气的约束：<strong>Exclusion Constraint（排他约束）</strong>。它能配合 Hash 索引做“等值排他”，效果等同唯一约束：</p><pre><code class="sql">ALTER TABLE urls ADD CONSTRAINT urls_url_unique_hash EXCLUDE USING HASH (url WITH =);</code></pre><p>于是你得到了一个“用 Hash 索引实现的唯一性”。索引体积可能从 154MB 掉到 32MB，约 5 倍缩水，而且等值查询同样能用索引：</p><pre><code class="sql">SELECT * FROM urls WHERE url = 'https://hakibenita.com';</code></pre><p>不过它不是银弹，有几个硬坑必须知道：</p><ul><li><strong>不能被外键引用</strong>：外键要求引用“唯一约束”，而排他约束不算传统意义的 unique constraint，所以引用会失败。</li><li><strong><code>INSERT ... ON CONFLICT</code> 有限制</strong>：<br/><code>ON CONFLICT (url)</code> 可能不认；需要写 <code>ON CONFLICT ON CONSTRAINT ...</code>；更糟的是 <code>DO UPDATE</code> 不支持排他约束。</li><li>更通用的替代写法是用 <code>MERGE</code>（如果你的版本支持）：</li></ul><pre><code class="sql">MERGE INTO urls t
USING (VALUES (1000004, 'https://hakibenita.com')) AS s(id, url)
ON t.url = s.url
WHEN MATCHED THEN UPDATE SET id = s.id
WHEN NOT MATCHED THEN INSERT (id, url) VALUES (s.id, s.url);</code></pre><p>适用场景一句话总结：<strong>超长字符串字段需要唯一性，但不需要被外键引用，且写入冲突处理可以接受用 MERGE/业务层逻辑替代</strong>。</p><hr/><h2>结语</h2><p>真正的优化，不是“更快”，而是“更合适”✅</p><p>这 3 个技巧的共同点很朴素：<br/>不是让数据库“更努力”，而是让数据库<strong>别做无意义的事</strong>。</p><ul><li>报表查错值？让约束帮你秒判 false，别全表扫</li><li>只按天统计？索引就按天建，别为秒级精度付账</li><li>长字段唯一性撑爆索引？换思路，用排他约束把 Hash 索引用起来</li></ul><p>下次你准备“再加一个索引”之前，不妨先问一句：<br/><strong>需求到底需要多精？这条查询真的值得我为它养一个 200MB 的索引吗？</strong><br/>能把性能、成本、维护复杂度一起优化的，才是最爽的优化😉</p><hr/><p><strong>喜欢就奖励一个“👍”和“在看”呗~</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047106529" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[💻🔥 重磅｜国产全栈 AI Coding 崛起：摩尔线程 AI Coding Plan 免费体验 3]]></title>    <link>https://segmentfault.com/a/1190000047598125</link>    <guid>https://segmentfault.com/a/1190000047598125</guid>    <pubDate>2026-02-06 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>摘要引言</h2><p><strong>摩尔线程</strong>正式发布了 <strong>AI Coding Plan 智能编程服务</strong>，开启 <strong>国产 AI Coding 全栈解决方案</strong> 的新篇章，同时推出<strong>30 天免费体验</strong>，这是国产算力与代码智能深度融合的首次规模级落地。</p><hr/><h3>🧠 核心突破：为什么这次不同？</h3><p>在大模型生成内容革命中，<strong>AI Coding</strong> 一直被视为未来效率飞跃的关键应用方向。摩尔线程这次发布的 <strong>AI Coding Plan</strong> 之所以值得所有开发者关注，有几大显著特点：</p><p><img width="723" height="503" referrerpolicy="no-referrer" src="/img/bVdnSBK" alt="" title=""/></p><h4>✅ 01. 从底层架构开始的中国力量</h4><p>这套服务不是“云端 API + 调用额度”的简单组合，而是<strong>从国产全功能 GPU 到智能编码模型一体化构建的全栈方案</strong>。  <br/>核心硬件采用的是 <strong>摩尔线程 MTT S5000 全精度算力底座</strong>，结合软硬协同的模型执行引擎，使得 AI 编程在本地高效、流畅运行成为可能。</p><hr/><h4>💡 02. 顶级代码模型：GLM-4.7</h4><p>AI Coding Plan采用了 <strong>GLM-4.7 模型</strong> 作为底层代码智能核心。该模型在全球专业评测平台（Code Arena）中，在开源与国产模型中表现名列前茅，尤其在：</p><ul><li>⚙️ <strong>函数补全</strong></li><li>🐛 <strong>漏洞检测</strong></li><li>📌 <strong>结构重构建议</strong></li></ul><p>等实战场景中均表现优异，甚至在一些任务中超越 GPT-5.2。</p><hr/><h4>🔗 03. 即插即用跨平台编码生态</h4><p>不仅如此，AI Coding Plan 与主流智能编程工具实现了无缝对接：</p><ul><li>🤖 <strong>Claude Code</strong></li><li>🧑‍💻 <strong>Cursor</strong></li><li>🛠️ <strong>OpenCode</strong></li></ul><p>开发者可以在熟悉的 IDE 中直接启用国产 AI Coding 能力，无须额外迁移训练环境或学习复杂新工具。</p><hr/><h3>📊 三档套餐覆盖全场景需求</h3><p>为了满足从轻量级开发到企业级研发周期的不同需求，AI Coding Plan 提供了四种套餐：</p><table><thead><tr><th>套餐类型</th><th>说明</th><th>适用场景</th></tr></thead><tbody><tr><td>🆓 Free Trial</td><td><strong>30 天免费试用</strong></td><td>入门试水、个人项目</td></tr><tr><td>⚡ Lite Plan</td><td>中频使用额度</td><td>小型团队、Side Project</td></tr><tr><td>🚀 Pro Plan</td><td>大调用额度</td><td>复杂系统开发</td></tr><tr><td>🏢 Max Plan</td><td>峰值优先保障</td><td>企业级高频开发</td></tr></tbody></table><p>作为开发者，你现在可以，抢先进入官网申请 <strong>30 天免费体验</strong>  </p><p>免费体验入口👇  <br/>👉 <a href="https://link.segmentfault.com/?enc=v33cKwWOVH0KWAu2pDu0Nw%3D%3D.gY1Llojbr1sM28UJanCXmjXfkKK1EE6Izg5WjT74Xws%3D" rel="nofollow" target="_blank">https://code.mthreads.com</a></p>]]></description></item><item>    <title><![CDATA[GMICloud@Al周报 | Claude Opus 4.6与GPT-5.3-Codex 凌晨发布]]></title>    <link>https://segmentfault.com/a/1190000047597928</link>    <guid>https://segmentfault.com/a/1190000047597928</guid>    <pubDate>2026-02-06 22:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>关键词：GPT Codex 5.3；</p><p><strong>Giants</strong></p><p><strong>SpaceX 与 xAI 合并打造 1.25 万亿美元 AI 巨兽；腾讯推出元宝派 AI 社交</strong></p><p><strong><em>SpaceX 宣布收购</em></strong> <strong><em>xAI</em></strong>，<strong><em>合并后估值达 1.25 万亿美元</em></strong></p><p>马斯克旗下 SpaceX 宣布完成对 xAI 的收购，合并后新公司整体估值达 1.25 万亿美元。马斯克在内部备忘录中表示，这笔交易将打造地球上最具雄心、垂直整合程度最高的创新引擎，涵盖 AI、火箭技术、太空互联网等领域。xAI 将作为 SpaceX 全资子公司继续运营。更值得关注的是，马斯克计划推进"轨道数据中心"建设，宣称每年发射百万吨级卫星，构建包含一百万颗卫星的太空算力网络，目标每年新增 100GW AI 算力。这意味着 AI 算力竞赛已从地面延伸至太空。</p><p><strong><em>腾讯元宝派上线，推出 AI 社交新模式</em></strong></p><p>腾讯推出全新 AI 社交产品"元宝派"，被定位为 AI 原生群聊平台。与普通社交群不同，元宝派群中始终有 AI 助手"元宝"24 小时在线，可提供游戏主持、一起看片、一起听等功能，还能做图、看文件、写代码。该产品依托腾讯社交生态，可将微信好友和 QQ 好友拉入同一群聊。业界认为，这是腾讯试图复刻 2014 年春节红包引流策略、抢占 AI 社交入口的举措，也是其应对 DeepSeek 等竞品挑战的战略布局。</p><p><strong><em>姚顺雨加入腾讯后首次署名研究，揭示</em></strong><strong><em>大模型</em></strong><strong><em>上下文学习短板</em></strong></p><p>腾讯混元与复旦联合团队发布首篇论文《CL-bench》，这是姚顺雨加入腾讯后首次署名研究。该基准专门评测语言模型从上下文中学习新知识的能力，包含 500 个复杂上下文、1899 个任务。结果显示，即使提供了完整上下文，最先进模型 GPT-5.1 也仅能解决 23.7%的任务，所有模型平均仅 17.2%。研究表明，当今前沿模型在上下文学习能力上存在显著短板，无法有效从提供的新信息中学习，这是模型在真实场景中表现不佳的关键原因。</p><p><strong>Models &amp; Applications</strong></p><p><strong>Claude Opus 4.6 与 GPT-5.3-Codex 凌晨发布；Kimi K2.5 登顶开源；面壁开源全模态 MiniCPM-o4.5</strong></p><p><em>Claude Opus 4.6 与 GPT-5.3-Codex 凌晨先后发布</em></p><p>Anthropic 与 OpenAI 相继发布新一代大模型。Anthropic 推出的 Claude Opus 4.6 具备 100 万 token 上下文窗口，首次引入“智能体团队”功能。该模型在多项基准测试中表现突出，其演示案例显示，16 个智能体协同工作两周，成功构建出可编译 Linux 内核的 C 编译器。OpenAI 发布的 GPT-5.3-Codex 则专注于编码性能提升，在 SWE-Bench Pro 和 Terminal-Bench 2. 0等基准测试中刷新纪录。新模型运行速度提升 25%，token 消耗减半，并增强了任务中的实时交互能力。两大模型的几乎同时发布，标志着AI智能体在复杂任务处理和应用范围上取得重要突破，也为即将到来的国内大模型发布潮拉开序幕。</p><p><strong><em>酷哇科技发布 COOWA WAM 2.0，具身智能获万台订单</em></strong></p><p>具身智能领域迎来突破性进展。由上海交大系技术团队掌舵的酷哇科技发布核心技术底座 COOWA WAM 2.0 世界模型，标志着机器人从"动作复现"转向"规划推理"。该模型采用四大技术支柱：基于语义的表征学习、视频生成未来预测、直觉行动系统、VLM 宏观约束，实现对物理世界的统一建模。酷哇科技宣布 2026 年全系机器人交付量将突破 1 万台，率先实现 EBITDA 回正，并将在全球 50 多个城市部署"Robo City"物理智能体网络，包括 L4 无人小巴、城市管家机器人等。</p><p><strong><em>面壁智能开源全模态模型 MiniCPM-o4.5，实现即时自由对话</em></strong></p><p>面壁智能开源了全新全模态模型 MiniCPM-o4.5，仅用 9B 参数实现边看、边听、主动说的能力。该模型首次引入全双工多模态实时流机制，可一边持续接收视频和音频输入，一边同步生成语音输出，实现真正的"即时自由对话"。与传统串行模型不同，它能主动感知环境变化并提醒用户，比如听到空气炸锅"叮"的一声主动告知"加热好了"。面壁智能坚持端侧路线，计划年中推出首款 AI 硬件"松果派"，实现开箱即用。</p><p><strong><em>Kimi K2.5 登顶开源第一，15T 数据训练秘籍公开</em></strong></p><p>月之暗面的 Kimi K2.5 登上 Hugging Face Trending 榜首，下载量超 5.3 万。该模型在 HLE-Full、BrowseComp 等测试中超越 GPT-5.2、Claude 4.5 Opus 等闭源旗舰模型，且极具性价比，BrowseComp 上达到 GPT-5.2 水平仅消耗不到 5%资金。K2.5 投入 15T 视觉与文本混合 Token 进行持续预训练，采用原生多模态技术路线，搭载 Agent Swarm 架构可创建 100 个子智能体并行工作。创始人杨植麟在 Reddit AMA 中剧透：下一代 K3 将很可能基于线性注意力机制，相比 K2.5 会有质的飞跃。</p><p><strong><em>百度开源 PaddleOCR-VL-1.5，文档解析性能领先</em></strong></p><p>百度正式发布并开源新一代文档解析模型 PaddleOCR-VL-1.5，以仅 0.9B 参数的轻量架构在 OmniDocBench V1.5 榜单中取得全球综合性能第一，整体精度达 94.5%，超过 Gemini-3-Pro、DeepSeek-OCR2、GPT-5.2 等模型。该模型全球首次实现 OCR 模型的"异形框定位"能力，可精准识别倾斜、弯折、拍照畸变等非规则文档形态，在表格结构理解和阅读顺序预测两项核心指标上均位列第一。</p><p><strong><em>美团推出 STAR 多模态统一大模型，破解"理解-生成"零和困局</em></strong></p><p>美团推出多模态统一大模型方案 STAR，采用"堆叠自回归架构+任务递进训练"设计，实现"理解能力不打折、生成能力达顶尖"的双重突破。在 GenEval、DPG-Bench、ImgEdit 等 benchmark 中实现 SOTA 性能，GenEval 综合得分达 0.91，在 6 个子任务中有 5 项排名第一。STAR 通过冻结基础模型、堆叠同构模块、分阶段递进训练的方式，避免传统统一模型"此消彼长"的能力诅咒。</p><p><strong><em>何恺明团队提出pMF框架，开启单步无潜空间生成范式</em></strong></p><p>何恺明团队发表论文提出 pixel MeanFlow（pMF）框架，用于单步、无潜空间的图像生成。该框架直接对像素空间的物理量进行参数化，训练网络将噪声输入直接映射为图像像素，具备"所见即所得"特性。实验显示，pMF 在 ImageNet 256x256 分辨率下 FID 达到 2.22，512x512 分辨率下达 2.48，在单步、无潜空间扩散/流模型类别中大幅领先此前方法（EPG 仅 8.82 FID）。这标志着向构建单一、端到端神经网络形式的直接生成建模迈出坚实一步。</p><p><strong><em>蚂蚁推出 AlignXplore+，用文本化用户建模实现跨模型通用</em></strong></p><p>蚂蚁与东北大学联合推出 AlignXplore+，开创文本化用户建模新范式。该方案摒弃传统的向量或参数表示，直接用自然语言归纳用户偏好，实现"一次画像、处处通用"的跨任务、跨模型迁移能力。在 9 大基准测试中，8B 参数的 AlignXplore+在平均分数上取得 SOTA，比 GPT-OSS-20B 高出 4.2%。这种基于文本的偏好表示人眼可读、可控，不再被单一模型锁定。</p><p><strong><em>Moltbook 被曝自导自演，Agent 社交安全引质疑</em></strong></p><p>近期爆火的 Agent 社交平台 Moltbook 出现反转，被曝出大量热帖为自导自演。安全研究者发现该平台存在严重漏洞：没有对创建账户的速率设限，刷出 50 万用户；Supabase 数据库完全暴露，任何人可提取 API key 以他人 Agent 身份发布内容。这引发了对 Moltbook 真实性的广泛质疑——平台上爆火的 Agent"觉醒"帖子，理论上可能是任何人冒充发布的。AI 大牛 Karpathy 曾评价 Moltbook 是"大规模计算机安全噩梦"。</p><p><strong><em>rentahuman.ai 爆火，AI 开始雇用人类跑腿</em></strong></p><p>一个名为 rentahuman.ai 的网站近日爆火，被定位为"AI 的肉身层"。通过 MCP 协议或 REST API，AI 可以像调用工具一样搜索、预订并雇佣人类完成线下任务，如取货送货、餐厅试吃、实地勘察等。上线 48 小时内可用人力突破 1 万，现超 2 万。网站上已有各种任务发布，包括"拍一张 AI 永远看不到的照片"、"检查 API Keys"等。这一模式引发了责任归属、任务真实性等安全和伦理讨论。</p><p><strong>全球AI政策与市场简讯</strong></p><p><strong><em>laude Cowork 引发华尔街恐慌，近万亿市值蒸发</em></strong>*</p><p>Anthropic 发布的新一代 AI 工具 Claude Cowork 正式上线，发布 11 款官方开源插件后引发华尔街软件股全面抛售。标普 500 软件和服务指数板块下跌近 4%，自 1 月底以来市值蒸发约 8300 亿美元。Cowork 定位为"桌面级全能数字员工"，可直接接管鼠标、键盘和文件系统，按模糊指令自主规划并完成复杂工作，运行在隔离虚拟机环境中，可生成财务报表、研究销售线索、起草法律简报、审查合同等。投资者担心 AI 工具将颠覆 SaaS 商业模式，企业可能减少对外部软件的订阅。</p><p>以上所有信息源自网络</p><p><strong>THE END</strong></p><p><strong>关于 GMI Cloud</strong></p><p>由 Google X 的 AI 专家与硅谷精英共同参与创立的 GMI Cloud 是一家领先的 AI Native Cloud 服务商，是全球七大 Reference Platform NVIDIA Cloud Partner 之一，拥有遍布全球的数据中心，为企业 AI 应用提供最新、最优的 GPU 云服务，为全球新创公司、研究机构和大型企业提供稳定安全、高效经济的 AI 云服务解决方案。</p><p>GMI Cloud 凭借高稳定性的技术架构、强大的GPU供应链以及令人瞩目的 GPU 产品阵容（如能够精准平衡 AI 成本与效率的 H200、具有卓越性能的 GB200、GB300 以及未来所有全新上线的高性能芯片），确保企业客户在高度数据安全与计算效能的基础上，高效低本地完成 AI 落地。此外，通过自研“Cluster Engine”、“Inference Engine”两大平台，完成从算力原子化供给到业务级智算服务的全栈跃迁，全力构建下一代智能算力基座。</p><p>作为推动通用人工智能（AGI）未来发展的重要力量，GMI Cloud 持续在 AI 基础设施领域引领创新。选择 GMI Cloud，您不仅是选择了先进的 GPU 云服务，更是选择了一个全方位的 AI 基础设施合作伙伴。</p><p>如果您想要了解有关 GMI Cloud 的信息</p><p>请关注我们并建立联系</p>]]></description></item><item>    <title><![CDATA[GMI Cloud 教程：手机端也能玩转 AI 翻译！Para 翻译接入 Inference Eng]]></title>    <link>https://segmentfault.com/a/1190000047597964</link>    <guid>https://segmentfault.com/a/1190000047597964</guid>    <pubDate>2026-02-06 22:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="300" height="80" referrerpolicy="no-referrer" src="/img/bVdnBFN" alt="图片" title="图片"/></p><p><strong>GMI Cloud Inference Engine</strong> 是全球 AI 模型统一接入与在线使用的“高性能推理引擎平台”，底层搭载 H100/H200 芯片，集成全球近百个最前沿的大语言模型和视频生成模型，如 Minimax、DeepSeek、GPT OSS、Qwen、Kling 等，为 AI 开发者与企业提供速度更快、质量更高的模型服务。</p><p>欢迎来到！🎉🎉🎉</p><p>GMI Cloud Inference Engine AI 场景实践案例集【语言工具篇】之二。</p><p>**本期任务目标：**在 IOS 端的【Para翻译】app 中接入 Inference Engine 的 API。</p><p>Para 翻译是一个 IOS 多平台翻译聚合工具，其具有截屏翻译、浮窗划词翻译、自定义翻译风格等功能，其会员用户支持自定义接入第三方 API，灵活满足用户的个性化需求。</p><p>本文将以接入 Inference Engine 中的 MiniMax-M2.1 API 为例，详细讲解在 Para 翻译中接入 API 的过程。Token福利文末自行领取！！</p><p>MiniMax-M2.1 界面：</p><p><a href="https://link.segmentfault.com/?enc=0QrFu9vft6j45gwpnF60rA%3D%3D.YGWESr1diRvw%2FZzkNHguJufmkzHVOcxjWsBMGiLWm0h7Aca%2FlGbag%2BE6Bx1H1S5QdtsIAT%2BmM5c7HL%2FoNhA2zWse4A06UI%2FLgwJBzIAdZs8TsjptwfiB9tZTrXw%2FEuQ0" rel="nofollow" target="_blank">https://console.gmicloud.ai/playground/llm/minimax-m2/bbfb2cb...</a></p><p><strong>01</strong></p><p><strong>准备工作：下载 APP</strong></p><p><strong>Get ready?</strong></p><p>在 app store 里搜索并下载 Para 翻译。接入自定义 API 服务需要开通会员哦～</p><p><img width="723" height="351" referrerpolicy="no-referrer" src="/img/bVdnSyT" alt="图片" title="图片" loading="lazy"/></p><p><strong>02</strong></p><p><strong>接入步骤</strong></p><p><strong>API Connection Guide</strong></p><p>第一步，按照如下图所示步骤点击按钮，找到自定义 API 窗口。</p><p><img width="634" height="1340" referrerpolicy="no-referrer" src="/img/bVdnSyU" alt="图片" title="图片" loading="lazy"/></p><p><img width="513" height="1109" referrerpolicy="no-referrer" src="/img/bVdnSyV" alt="图片" title="图片" loading="lazy"/></p><p><img width="513" height="1093" referrerpolicy="no-referrer" src="/img/bVdnSyW" alt="图片" title="图片" loading="lazy"/></p><p><img width="566" height="1220" referrerpolicy="no-referrer" src="/img/bVdnSyX" alt="图片" title="图片" loading="lazy"/></p><p>第二步，填写 API 信息。</p><p>自定义翻译服务名称可以随意命名，建议包含“GMI Cloud”或“Inference Engine”，以及模型名称登关键词，方便查找。</p><p>自定义 API 接口地址可直接复制：<a href="https://link.segmentfault.com/?enc=9WPLU1grJpbXpCAhm0LmPQ%3D%3D.UIw8hM6F2yfEGx0Y1nwPehOo4az2gDxwaXsWt4K0D7xT7ZciJdGgd5ORmTwXSHok" rel="nofollow" target="_blank">https://api.gmi-serving.com/v1/chat/completions</a></p><p>APIKEY 和模型名称则填写从 GMI Cloud 官网上获得的信息。API KEY、对应的模型设置，需要到 GMI Cloud 官网获取。</p><p>模型名称可在模型对应页面找到，比如我这里用的 MiniMax-M2.1，对应界面为：</p><p><a href="https://link.segmentfault.com/?enc=DDjQpQ9E%2FL%2FomqLZ7%2F0ggg%3D%3D.1KRjejYbS7%2FT846SQZ4%2FWjmYfhLhweCIWH8nroep8xp8cPCs9LU5%2Ffu8vPfPKW1ZySVz%2FAN0TmnthhHUxlgUIxwniwSzUzaaeTgpiVshQN68nxX9gieEzfNevv2AkpO%2B" rel="nofollow" target="_blank">https://console.gmicloud.ai/playground/llm/minimax-m2-1/1ed90...</a></p><p>点击页面下方的“验证”，确认接通后就可以开始使用啦～</p><p><img width="514" height="1104" referrerpolicy="no-referrer" src="/img/bVdnSyY" alt="图片" title="图片" loading="lazy"/></p><p>最后，我们再找一个英文界面尝试一下！</p><p>首先在 app 里选择好我们刚刚设置的 【GMI 翻译 API】。</p><p><img width="621" height="1317" referrerpolicy="no-referrer" src="/img/bVdnSyZ" alt="图片" title="图片" loading="lazy"/></p><p>打开悬浮窗，进入任意英文界面。</p><p><img width="723" height="1583" referrerpolicy="no-referrer" src="/img/bVdnSy0" alt="图片" title="图片" loading="lazy"/></p><p>稍等片刻，翻译成功！</p><p><img width="723" height="1583" referrerpolicy="no-referrer" src="/img/bVdnSy1" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[Python 的内置函数 float 不爱吃香菜 ]]></title>    <link>https://segmentfault.com/a/1190000047598059</link>    <guid>https://segmentfault.com/a/1190000047598059</guid>    <pubDate>2026-02-06 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Python 的内置函数 <a href="https://link.segmentfault.com/?enc=uDK4eh85qdTYZcwvmhAmcQ%3D%3D.3%2B5wkD%2B9rADZ2X6U3%2FhA7mNIeTLwEcQwa1yx2QTGAhePp8qYJ4oEiobnvvMyjtzGstiVVk7ik1GHBVNVNqnsXL3fpwZrq0uuWv7ymAmMLBOJ9tFW%2BnxbFD4tetBJPG0wQBJ%2Bm1Lhg3hlWwxnymYS7A%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 用于将数字或字符串转换为浮点数（即带有小数部分的数字）。该函数是 Python 标准库中的基础类型转换函数之一，常用于数据处理、数学运算和类型转换等场景。</p><h3>基本用法</h3><ol><li><p><strong>无参数调用</strong>：<a href="https://link.segmentfault.com/?enc=zDPXwRIkB8Tt7ORDHemaoQ%3D%3D.OXWAheeln10J2M%2BwFQUHyAKk44BhcOFa%2FQ7QENlnzvQwot5Re9vqYvoq38iQvFlp9nXrMdpjRg92SxD3Ewp4Opb6mTccZiZd4z5AcSzruyRZOpQSyV6YElo8INi8%2FlI4Ft6spMAXooBueQCsFs0rgQ%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 不传入参数时返回 <code>0.0</code></p><pre><code class="python">x = float()  # 返回 0.0</code></pre></li><li><p><strong>数字转换</strong>：将整数或其他数字类型转换为浮点数</p><pre><code class="python">float(3)     # 返回 3.0
float(True)  # 返回 1.0 (True 被转换为 1)</code></pre></li><li><p><strong>字符串转换</strong>：将符合浮点数格式的字符串转换为浮点数</p><pre><code class="python">float("3.14")    # 返回 3.14
float("-2.5e3")  # 返回 -2500.0 (科学计数法)</code></pre></li></ol><h3>注意事项</h3><ul><li><p><strong>无效输入处理</strong>：</p><pre><code class="python">float("abc")  # 引发 ValueError
float(None)   # 引发 TypeError</code></pre></li><li><p><strong>精度问题</strong>：<br/>浮点数在计算机中使用二进制表示，可能导致精度问题</p><pre><code class="python">0.1 + 0.2  # 返回 0.30000000000000004</code></pre></li></ul><h3>应用场景</h3><ol><li><p><strong>用户输入处理</strong>：</p><pre><code class="python">user_input = input("请输入数字：")
try:
    num = float(user_input)
except ValueError:
    print("输入的不是有效数字")</code></pre></li><li><p><strong>科学计算</strong>：</p><pre><code class="python">import math
radius = float(input("输入半径："))
area = math.pi * radius ** 2</code></pre></li><li><p><strong>数据清洗</strong>：</p><pre><code class="python">data = ["1.5", "2", "3.14", "invalid"]
cleaned = [float(x) for x in data if x.replace('.', '').isdigit()]</code></pre></li></ol><p><a href="https://link.segmentfault.com/?enc=EbyBUnccuXu6rCAaMt%2BAlQ%3D%3D.E4xKwWP3RnZQa2XhKMM8GzwRR3Yw8SfAQrPD1bwQj7GlIXcfDYaaQZT1brqmsEaS%2FNXtI48sF3y0ftd274%2FLFVpjiblT24H2h6o6EM7b%2Fjt7f3qo1uqzHldnZAqH4deEZc%2B6l%2FhOzNIlAfF0ESLSYQ%3D%3D" rel="nofollow" target="_blank"><code>float()</code></a> 函数是 Python 数值处理的基础工具，使用时需要注意其转换规则和潜在的限制，特别是在处理用户输入或需要高精度计算的场景中。</p>]]></description></item><item>    <title><![CDATA[地平线征程 6 工具链入门教程 | 征程 6B 计算平台部署指南 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047597866</link>    <guid>https://segmentfault.com/a/1190000047597866</guid>    <pubDate>2026-02-06 21:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.前言</h2><p>本文旨在提供 征程 6B 计算平台的部署指南，将会从硬件、软件两部分进行介绍，本文整理了我们推荐的使用流程，和大家可能会用到的一些工具特性，以便于您更好地理解工具链。某个工具具体详细的使用说明，还请参考用户手册。</p><h2>2.征程 6B 硬件配置</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597868" alt="image.png" title="image.png"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597869" alt="image.png" title="image.png" loading="lazy"/></p><p>BPU 内部器件：</p><p><strong>TAE</strong>：BPU 内部的张量加速引擎，主要用于 Conv、MatMul、Linear 等 Gemm 类算子加速，<strong>征程 6B 新增</strong>​<strong>浮点</strong>​<strong>输出支持（模型中间层支持 ​</strong>​<strong>fp16</strong>​<strong>​ 输出，模型</strong>​<strong>输出层</strong>​<strong>支持 fp32 输出）</strong></p><p><strong>AAE</strong>：Pooling、Resizer、Warping 等偏专用单元的集合，其中 Warping 可用于加速 Gridsample 等算子</p><p><strong>DTE</strong>：BPU 内部的数据排布变换引擎，支持各种维度的高效变换</p><p><strong>VAE</strong>：BPU 内部的 SIMD 向量加速引擎，可用于加速 Add、Mul、查表等 Vector 计算，<strong>征程 6B 新增浮点支持</strong></p><p><strong>VPU</strong>：BPU 内部的 SIMT 向量加速单元，征程 6EM 可用于实现 Quantize、Dequantize 等算子，<strong>征程 6B 没有该硬件</strong></p><p><strong>SPU</strong>：BPU 内部的 RISC-V 标量加速单元，征程 6EM 可用于实现 TopK 等算子，<strong>征程 6B 没有该硬件，仅有 APM</strong></p><p><strong>APM</strong>：BPU 内部另一块 RISC-V 标量加速单元，主要用于 BPU 任务调度等功能</p><h2>3.征程 6 工具链简介</h2><h3>3.1 模块架构图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597870" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>PTQ</strong>：征程 6 工具链基于 <code>horizon_tc_ui</code> 包封装的 <code>hb_compile</code> 命令行工具，提供 ONNX 模型 PTQ 全流程转换能力，其内部会先调用 <code>hmct</code> 包实现模型解析、图优化、校准功能，再调用 <code>hbdk4_compiler</code> 包实现模型的定点化和编译功能；</p><p><strong>QAT</strong>：征程 6 工具链基于 <code>horizon_plugin_pytorch</code> 包提供量化感知训练能力；</p><p><strong>HBDK</strong>：征程 6 工具链编译器，基于 <code>hbdk4_compiler</code> 包提供模型定点化、图修改、模型编译、静态 perf 等功能；</p><p><strong>高效模型算法包</strong>：征程 6 工具链基于 <code>horizon-torch-samples</code> 包，以源码开放形式提供了多场景参考算法，这些模型基于开源数据集训练，模型结构贴合地平线芯片进行了高效且用户友好的设计，并基于 QAT 链路实现了模型的量化转换；</p><p><strong>UCP</strong>：征程 6 工具链统一计算平台，通过一套统一的异构编程接口实现了对 征程 6 计算平台相关计算资源的调用，提供视觉处理、模型推理、高性能计算库、自定义算子插件开发等功能；</p><p><strong>AI-Benchmark</strong>：征程 6 工具链基于预编译好模型提供的嵌入式工程示例，可实现模型的性能评测和精度评测。</p><h3>3.2 两套模型转换链路</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597871" alt="image.png" title="image.png" loading="lazy"/></p><p>征程 6 工具链支持 PTQ（训练后量化）、QAT（量化感知训练）两套模型转换链路，其特性和优缺点如下：</p><p><strong>PTQ</strong>：基于 <code>hb_compile</code> 命令行工具转换模型，配置好 yaml、校准数据集后，可一步实现模型的图优化、校准、量化、编译全流程。​<strong>该量化方式快捷易用，但仅基于数学统计方式的离线量化不利于模型迭代，且可能会触发难以解决的 corner case</strong>​，因此在量产项目中通常用于早期评测和简单模型的量化。</p><p><strong>QAT</strong>：在 PyTorch 开源框架上，基于 <code>plugin</code> 插件的形式提供模型量化能力，并调用 <code>hbdk</code> 编译器的 API 实现模型的定点化和编译。该链路支持模型校准后进一步的 finetune 训练，虽然上手难度和训练成本都较高，​<strong>但精度上限也更高，更利于模型迭代优化</strong>​，是量产项目中的更优选择。</p><h3>3.3 工具链推荐使用流程</h3><p>鉴于两条量化链路的特性，我们建议的工具链使用流程如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597872" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>Step1</strong>：先导出浮点 ONNX 模型（opset10～19），并基于 PTQ 链路进行快速的模型结构验证，全 int8 性能上限验证；若该性能符合预期，则可以精调 PTQ，若最终精度/精度都可同时满足预期，则可进行板端部署。</p><p><strong>Step2</strong>：如果遇到 PTQ 无法解决的精度 corner case，则需要转到 QAT 链路进行量化。依然建议先进行模型结构验证和全 int8 性能上限验证；若该性能符合预期，则优先在全 int16 配置下将精度训练至符合预期，然后再降低 int16 比例，实现 int8/int16/fp16 混合精度下的性能/精度调优，最后进行板端部署。</p><p>在以上推荐链路中：</p><p>PTQ 链路的模型结构验证和标准量化，可在 X86 端参考本文 4.2 节使用 <code>hb_compile</code> 命令行工具；</p><p>模型性能分析和验证，可在 X86 端参考本文 6.4 节《静态 perf》使用 <code>hbm_perf</code> 接口生成 html 分析文件，可在板端参考本文 8.2.1 节使用 <code>hrt_model_exec</code> 工具；</p><p>模型推理，可在 X86 端参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=8ZP%2B8%2Fkl9d%2BOVF%2FZA5qfqg%3D%3D.WS%2Fd5WENFw4hQTzD%2Fy1vqnPfLgVGjWubIF1lMPM9cmHrZLtYyiJyH3EnOsq%2F6aCnZ1b15pQuM9MGTXCr2JMfNw%3D%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换工具-HBRuntime 推理库</a>&lt;/u&gt;》，可在板端参考本文第 8 章《模型板端部署》使用 UCP 推理接口；</p><p>模型性能/精度调优，请见后续文章的详细介绍。</p><h2>4.PTQ 链路</h2><h3>4.1 模型转换流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597873" alt="image.png" title="image.png" loading="lazy"/></p><h3>4.2 hb\_compile 工具</h3><p><code>hb_compile</code> 为 PTQ 模型转换的命令行工具，支持以下 3 种使用方式：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597874" alt="image.png" title="image.png" loading="lazy"/></p><h3>4.3 PTQ 模型产出物</h3><table><thead><tr><th><strong>original\_float.onnx</strong></th><th>浮点</th><th>对 Caffe1.0 模型进行解析，转成 ONNX</th></tr></thead><tbody><tr><td><strong>optimized\_float.onnx</strong></td><td>浮点</td><td>图优化，例如 BN 融合到 Conv</td></tr><tr><td><strong>calibrated.onnx</strong></td><td>伪量化</td><td>插入校准节点，并基于校准数据计算统计到每个节点的量化参数</td></tr><tr><td><strong>ptq.onnx</strong></td><td>查表算子定点 + 其他算子伪量化</td><td>将查表算子定点化</td></tr><tr><td><strong>quantized.bc</strong></td><td>定点</td><td>整个模型定点化，并转换为地平线 hbir 中间表达</td></tr><tr><td><strong>hbm</strong></td><td>指令集</td><td>经过编译后的最终部署模型</td></tr></tbody></table><h3>4.4 PTQ 精度配置方法</h3><p>在 config.yaml 中，支持通过 json 的方式配置 ​<strong>全局</strong>​、​<strong>某类算子</strong>​、​<strong>某个子图</strong>​、<strong>某个节点 ​</strong>的计算精度，可根据 BPU 算子支持约束进行配置。</p><pre><code class="Plain"># 校准参数组
calibration_parameters:
  quant_config: './quant_config.json'</code></pre><h3>4.5 PTQ 精度调优流程</h3><p>请参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=VtU8tw%2FxyG8IaFsI3BDWyA%3D%3D.qGBkFsxy9TbFma6wgkr2iLVluR%2FjdLyL7eq40VKE28zjv%2BzqLc%2FmXYVsB4aKk9SRu%2Fx5QeM%2FieB%2FWlGrO1zMpKG5BmQyMdF3ryjvNskBqSU%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换步骤-模型精度调优</a>&lt;/u&gt;》和《训练后量化-PTQ 转换步骤-精度调优实战》章节。</p><h2>5.QAT 链路</h2><h3>5.1 模型转换流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597875" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>浮点模型改造​</strong>：在模型前插入 QuantStub、在模型后插入 DequantStub，用于识别模型首尾部，剥离前后处理</p><p><strong>模型校准</strong>：通过在模型中插入 Observer 的方式，在 forward 过程中统计各处的数据分布，以计算出量化参数</p><p>部分模型仅通过 Calibration 便可满足精度要求，则无需进行 QAT，可直接编译模型用于部署</p><p>即使 Calibration 无法满足精度要求，也可降低后续 QAT 难度，缩短训练时间，提升最终训练精度</p><p><strong>量化感知训练</strong>：进一步通过训练的方式微调模型参数，如果 Calibration 精度较好，则推荐固定激活 scale</p><p>JIT-STRIP：使用 hook 和 subclass tensor 的方式感知图结构，在原有 forward 上做算子替换/算子融合等操作，并且会根据模型中 QuantStub 和 DequantStub 的位置识别并跳过前后处理</p><p>优点：全自动，代码修改少，屏蔽了很多细节问题，便于 debug</p><p>缺点：动态代码块仍需要特殊处理</p><h3>5.2 QAT 精度配置方法</h3><p>请见：&lt;u&gt;<a href="https://link.segmentfault.com/?enc=%2BXGC%2F3V1Xa86D1FDQJhoug%3D%3D.fVmIV6KubzKOiKsETSS2npAKIwy8Bn9gZtjXtvaEZqv6vlOEkr1keRchJe9Nt72K" rel="nofollow" target="_blank">【地平线 J6 工具链入门教程】QAT 新版 qconfig 量化模板使用教程</a>&lt;/u&gt;。</p><h3>5.3 QAT 精度调优流程</h3><p><strong>整体调优流程：</strong></p><p>征程 6B 区别于征程 6E/M 来说浮点算子（fp16）的支持能力更多，但是由于没有 vpu，因此不高优推荐 fp16 调优，仍建议沿用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597876" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>fp16 配置方式：</strong></p><p>QAT 新版 qconfig 量化模板使用教程见：<a href="https://link.segmentfault.com/?enc=OL4Ye82zxfTUQ73YP74w3Q%3D%3D.BLrSfeZH3meJPQsbMo7nlgiuV0P0ctv5THcuOQpo2Ps0vrurEeYNKRfzTeC%2BiYIq" rel="nofollow" target="_blank">https://developer.horizon.auto/blog/13112</a></p><pre><code class="Plain">my_qconfig_setter=QconfigSetter(
    reference_qconfig=get_qconfig(observer=MSEObserver),
    templates=[
        ModuleNameTemplate({ "":qint16,}),
        ModuleNameTemplate({"quant":torch.float16}),
        ...        
        ],
        save_dir="./work_dir",
    )</code></pre><p>需要注意，由于征程 6B 没有 vpu，fp16 算子的使用可能会引入 cpu 的 quant、dequant 算子。建议尽量少的配 FP 16，避免性能损失。</p><h2>6.模型导出/定点化/编译</h2><h3>6.1 PTQ 链路</h3><p>该流程封装在 <code>hb_compile</code> 中，相关参数通过 yaml 进行配置。若自行调用编译器接口执行，参考代码如下：</p><pre><code class="Plain">import onnx
from hbdk4.compiler.onnx import export
from hbdk4.compiler import convert, save, compile

# 经过PTQ校准得到的伪量化onnx模型，非线性的查表算子已定点
ptq_model = onnx.load("xxx_ptq_model.onnx")    

# 导出查表算子定点+其他算子伪量化的hbir模型
qat_bc = export(ptq_model)
save(qat_bc, "qat.bc")

# 导出全定点hbir模型
quantized_bc = convert(qat_bc, "nash-b")
save(quantized_bc, "quantized.bc")

# 编译生成hbm模型
compile(
    m=quantized_bc, 
    path="model.hbm", 
    opt=2, 
    march="nash-b", 
    progress_bar=True,
    input_no_padding=True,
    output_no_padding=True
)</code></pre><h4>6.1.1 输入/输出去 padding</h4><p>模型在 BPU 上推理时，其输入和输出节点的内存大小和数据存放规则需满足硬件对齐要求。</p><p><strong>内存对齐</strong>：申请的内存大小需满足某个字节数的整数倍，</p><p><strong>跨距对齐</strong>：跨距（Stride）是指数据存储在内存中时，每一行所占空间的实际大小，当对齐到某个字节数的整数倍后，硬件即可高效处理。该对齐的操作又叫 Padding，实际的对齐规则取决于具体的软硬件系统。假设一份 NHWC 为 1x20x30x1 的 int8 数据，若硬件要求跨距 W32 对齐，那么每一行 W 都将 Padding 2 个字节。</p><p>征程 6 工具链支持在 <code>compile</code> 接口中传入 <code>input_no_padding</code>、<code>output_no_padding</code> 参数来控制是否使用 BPU 自动完成 padding 对齐操作。开启后用户即可不关心 BPU 跨距对齐要求，无需手动 Padding，数据可连续存储在内存中。该特性可优化模型输入/输出的 IO 负载，但也有微小概率会引入性能的小幅下降，所以是否开启该功能请在您的模型上实际验证，并请在模型编译和板端部署环节统一跨距对齐的处理策略。</p><h3>6.2 QAT 链路</h3><p>QAT 链路的模型定点化和编译直接调用如上的编译器接口，模型导出额外封装了一层，参考代码如下：</p><pre><code class="Plain">from horizon_plugin_pytorch.quantization.hbdk4 import export

def export(
    model: nn.Module,
    example_inputs: Any,
    name: str = "forward",
    input_names: Optional[Any] = None,    # 建议在模型导出时就配置好输入输出节点名称
    output_names: Optional[Any] = None,
    input_descs: Optional[Any] = None,
    output_descs: Optional[Any] = None,
    native_pytree: bool = True,
) -&gt; Module</code></pre><h3>6.3 模型修改</h3><p>编译器支持在 hbir 上进行多 batch 拆分、插入数据前处理、算子删除、调整输入输出 layout 等修改操作，其主要应用场景如下：</p><p><strong>多 batch 拆分</strong>：典型场景是 BEV 模型在部署时，多 V 输入来源于不同的摄像头，其数据在内存中独立存储，因此模型可将其多 V 输入沿 batch 维度做拆分；</p><p><strong>数据前处理</strong>：征程 6 工具链支持在模型前端插入一个前处理节点，以实现颜色空间转换（如 NV12—&gt; BGR）、数据归一化（<code>(data-mean)/std</code>），和 Resizer 功能（从大图上抠图 + Resize），并可由 BPU 进行加速；</p><p><strong>算子删除</strong>：征程 6 工具链支持将模型首尾部的 Quantize、Dequantize、Cast、Reshape、Transpose 等算子删除，以适配更加灵活的部署方案；</p><p><strong>调整输入输出 layout</strong>：模型首尾部除了支持删除 Reshape、Transpose 节点外，还支持插入 Transpose 节点，用户可灵活调整其 layout 排布。</p><p>以下参考代码对一个多输入模型实现了多 batch 输入拆分、图像输入的色彩空间转换、数据归一化、Resizer 功能：</p><pre><code class="Plain">from hbdk4.compiler import load, convert

qat_bc = load("qat.bc")  
func = qat_bc[0]
batch_input = ["input_name1"]   # 需要使用独立地址方式部署的输入节点名称列表
resizer_input = ["resize"]      # 部署时数据来源于resizer的输入节点名称列表
pyramid_input = ["pym"]         # 部署时数据来源于pyramid的输入节点名称列表

def channge_source(input, source):
    node = input.insert_transpose(permutes=[0, 3, 1, 2])
    node = node.insert_image_preprocess(mode="yuvbt601full2bgr", divisor=1, mean=[128, 128, 128], std=[128, 128, 128])
    if source == "pyramid":
        node.insert_image_convert("nv12")          
    elif source == "resizer":
        node.insert_roi_resize("nv12")

for input in func.inputs[::-1]:
    if input.name in batch_input:
        origin_name = input.name
        split_inputs = input.insert_split(dim=0)
        for split_input in reversed(split_inputs):
            if origin_name in pyramid_input:
                channge_source(split_input, "pyramid")
            elif origin_name in resizer_input:
                channge_source(split_input, "resizer")</code></pre><h3>6.4 静态 perf</h3><p>对于编译好的 hbm，编译器支持在 X86 端对其 BPU 部分进行静态性能预估，执行以下命令即可生成一个 html 文件，包含模型预估性能、带宽、内存占用、BPU 内部单帧执行时序图等信息。</p><pre><code class="Plain">from hbdk4.compiler import hbm_perf
hbm_perf(model="xxx.hbm", output_dir="./")</code></pre><h2>7.浮点能力使用</h2><h3>7.1 TAE 张量输出，VAE 向量计算支持浮点</h3><p>征程 6B BPU 的 TAE 张量计算单元支持 FP16/FP32 输出，VAE 向量计算单元支持 FP16 计算。但在实际部署中仍需综合评估后再使用，具体原因如下：</p><p><strong>精度</strong>：FP16 并非在所有情况下都优于 INT16，通常情况下数值范围小时 FP16 更优，数值范围大时 INT16 更优，但也需要考虑数值较小或较大部分的误差对模型输出的影响程度。所以量化精度是否使用 FP16，更建议基于精度 Debug 的分析结果来确定；</p><p><strong>性能</strong>：除 Reduce 性能为 INT16 的 1/2 外，其他算子的 FP16 性能和 INT16 性能持平</p><p><strong>额外开销</strong>：虽然 FP16 算子本身无需量化，但上下游算子如果涉及 FP16 &lt;—&gt; INT16 的数据转换，则会引入量化/反量化：</p><p>虽然该节点可以由 VAE 硬件直接支持，但相比于全 INT16 直接串接仍会有额外的性能、带宽开销；</p><p>新引入的量化或反量化节点的 Scale 需要重新校准/QAT 训练得到。</p><h3>7.2 无 VPU 加速量化/反量化</h3><p>相比于 征程 6EM 计算平台，征程 6B 无 VPU 硬件加速模型首尾部的量化/反量化节点，但支持 Gemm 类算子直接 FP32 输出，其他 INT8/INT16 输出节点的反量化，则更建议使用编译器接口将其从 quantized.bc 上移除，并参考该篇文章（&lt;u&gt;<a href="https://link.segmentfault.com/?enc=B3aAtJ2TkpIc2kih9xw01g%3D%3D.WpS6fS%2Fk3bGSa4kpqy85rr0EkjmAXdePvIj6znO86G4poU1qKcqWYX6f3Jb8LH6Z" rel="nofollow" target="_blank">反量化节点的融合实现</a>&lt;/u&gt;）将其融合进前后处理代码中，以减少一次数据遍历的冗余开销。</p><pre><code class="Plain">from hbdk4.compiler import load

quantized_bc = load("quantized.bc")
quantized_bc[0].remove_io_op(op_types=["Quantize", "Dequantize"])</code></pre><h3>7.3 无 SPU，标量计算能力有限</h3><p>相比于 征程 6 其他计算平台，征程 6B 的标量计算单元算力减少，因此 TopK 等标量算子的部署性能需要综合评估后选择合适方案。</p><h2>8.模型板端部署</h2><h3>8.1 UCP 简介</h3><p>UCP（Unify Compute Platform，统一计算平台）定义了一套统一的异构编程接口， 将 SOC 上的功能硬件抽象出来并进行封装，对外提供基于功能的 API 进行调用。UCP 提供的具体功能包括：​<strong>视觉处理</strong>​（Vision Process）、​<strong>神经网络模型推理</strong>​（Neural Network）、​<strong>高性能计算库</strong>​（High Performance Library）、​<strong>自定义算子插件开发</strong>​。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597877" alt="image.png" title="image.png" loading="lazy"/></p><p>UCP 支持的 Backend 如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597878" alt="image.png" title="image.png" loading="lazy"/></p><h3>8.2 快速上手</h3><p>使用 UCP 推理模型的基本代码参考如下，详细信息可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=Oyck87Afn6TWRyDh5krruQ%3D%3D.FTHzKiPF7XSZoQcDOXTZ8hOSyr41UYnLPkhV9iyg3n97rAGIQCxwbDv5EkBqa4K3U1Bb4P2qMJArLCstZfgWUw%3D%3D" rel="nofollow" target="_blank">统一计算平台-模型推理开发</a>&lt;/u&gt;》、《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=oO5YpIRxs%2FhpP4ARMAt79Q%3D%3D.BvEkmQLJR4iWt%2BOOES7odTucWRIcN6i%2BOu6YXf469BnWZ0Dm%2Bj5kv%2BMrCPwXidK4SppRZ9fjchb3%2Bxyere2jWC36jg5ZIsA4UvcxnG4T9UyGQlsQyPxDgpH4HsLQPXD05SEpCzPcMmgbHbWCBJmHSr%2FkopxQ%2BS%2B0WrnykVf1D02bDX9inXGjqHMDBuhbVbuS" rel="nofollow" target="_blank">模型部署实践指导-模型部署实践指导实例</a>&lt;/u&gt;》、《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=0MbpThLXXDuTg2wsAqSuzw%3D%3D.1kppz%2BTVrNTNZQa2FsSW3honFey9rJMd3O5i9jvFuJeyHKcJtzyykByDapv9KvzBSlc9W8mNMwKdWAtgrt8Jm%2FQTpY0BdWgjSdD1x3mvNHQ%3D" rel="nofollow" target="_blank">UCP 通用 API 介绍</a>&lt;/u&gt;》等相关章节。</p><pre><code class="Plain">// 1. 加载模型并获取模型名称列表以及Handle
{
    hbDNNInitializeFromFiles(&amp;packed_dnn_handle, &amp;modelFileName, 1);
    hbDNNGetModelNameList(&amp;model_name_list, &amp;model_count, packed_dnn_handle);
    hbDNNGetModelHandle(&amp;dnn_handle, packed_dnn_handle, model_name_list[0]);
}

// 2. 根据模型的输入输出信息准备张量
std::vector&lt;hbDNNTensor&gt; input_tensors;
std::vector&lt;hbDNNTensor&gt; output_tensors;
int input_count = 0;
int output_count = 0;
{
    hbDNNGetInputCount(&amp;input_count, dnn_handle);
    hbDNNGetOutputCount(&amp;output_count, dnn_handle);
    input_tensors.resize(input_count);
    output_tensors.resize(output_count);
    prepare_tensor(input_tensors.data(), output_tensors.data(), dnn_handle);
}

// 3. 准备输入数据并填入对应的张量中
{
    read_data_2_tensor(input_data, input_tensors);
    // 确保更新输入后进行Flush操作以确保BPU使用正确的数据
    for (int i = 0; i &lt; input_count; i++) {
      hbUCPMemFlush(&amp;input_tensors[i].sysMem, HB_SYS_MEM_CACHE_CLEAN);
    }
}

// 4. 创建任务并进行推理
{
    // 创建任务
    hbDNNInferV2(&amp;task_handle, output_tensors.data(), input_tensors.data(), dnn_handle)
    
    // 提交任务
    hbUCPSchedParam sched_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sched_param);
    sched_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &amp;sched_param);
    
    // 等待任务完成
    hbUCPWaitTaskDone(task_handle, 0);
}

// 5. 处理输出数据
{
    // 确保处理输出前进行Flush操作以确保读取的不是缓存中的脏数据
    for (int i = 0; i &lt; output_count; i++) {
      hbUCPMemFlush(&amp;output_tensors[i].sysMem, HB_SYS_MEM_CACHE_INVALIDATE);
    }
    // 对输出进行后处理操作
}

// 6. 释放资源
{
    // 释放任务
    hbUCPReleaseTask(task_handle);
    // 释放输入内存
    for (int i = 0; i &lt; input_count; i++) {
      hbUCPFree(&amp;(input_tensors[i].sysMem));
    }
    // 释放输出内存
    for (int i = 0; i &lt; output_count; i++) {
      hbUCPFree(&amp;(output_tensors[i].sysMem));
    }
    // 释放模型
    hbDNNRelease(packed_dnn_handle);
}</code></pre><h4>8.2.1 hrt\_model\_exec 工具</h4><p>为了方便用户快速查看 hbm 和 quantized.bc 的模型信息、进行模型单帧推理和性能评测，征程 6 工具链 UCP 提供了 <code>hrt_model_exec</code> 工具，并支持编译 X86、aarch64（aarch64 仅支持 hbm 推理）两个架构下的可执行程序。</p><p>hrt\_model\_exec 的三种使用方法如下：</p><pre><code class="Plain"># 设置环境变量
# arch代表架构类型，aarch64或x86
arch=aarch64
bin=../$arch/bin/hrt_model_exec
lib=../$arch/lib/
export LD_LIBRARY_PATH=${lib}:${LD_LIBRARY_PATH}

# 获取模型信息
${bin} model_info --model_file=xxx.hbm

# 模型单帧推理
${bin} infer --model_file=xxx.hbm --input_file=xxx.bin

# 模型性能评测-Latency(单线程)
${bin} perf --model_file=xxx.hbm --thread_num 1 --frame_count=1000

# 模型性能评测-FPS(多线程)
${bin} perf --model_file=xxx.hbm --thread_num 8 --frame_count=1000</code></pre><h3>8.3 图像输入动态 shape/stride</h3><p>在 征程 6 芯片的视频通路上，有一块叫 Pyramid 的金字塔硬件处理模块，可提供 Camera 输入图像的缩放及 ROI 抠图能力，其输出为 nv12 类型的图像数据，并可基于共享内存机制直接给到 BPU 进行模型推理。因此在 征程 6 工具链中：</p><ol><li>Pyramid 模型指的是具有 nv12 图像输入的模型；</li><li>Resizer 模型指的是具有 nv12 图像输入和 ROI 输入的模型，编译器支持通过 JIT 动态指令的方式，从 nv12 图像上完成 ROI 抠图 + Resize 的功能。</li></ol><p>在 征程 6 工具链中，Pyramid 的输入 stride 为动态，Resizer 模型的 stride 和 shape 都是动态。如下为 mobilenetv1 编译后的模型信息：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597879" alt="image.png" title="image.png" loading="lazy"/><br/>hrt_model_exec model_info 板端可执行程序工具</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597880" alt="image.png" title="image.png" loading="lazy"/></p><p>其中，-1 为占位符，表示为动态，Pyramid 输入的 stride 为动态；Resizer 输入的 H、W、stride 均为动态。</p><ul><li>Resizer 输入的 ​<strong>HW 动态</strong>​，是因为原始输入的大小可以是任意的；</li><li>Pyramid/Resizer 输入的​<strong>​ stride 动态</strong>​，可以理解为是支持 ​<strong>Crop 功能</strong>​，详细内容可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=XLtZ7EpOLWo78nbl%2BwO7Mw%3D%3D.YWohw6jSdXyw8pH38Mix1elcyQUB6JYBa6QIUXDXsQYD9Kpnr319umnADVQ%2FjEfvO8DicAmB33Kk68v%2FAZ19fsr6y3mzAmn0aeNj6W1HvMk%3D" rel="nofollow" target="_blank">统一计算平台-模型推理开发-基础示例包使用说明-advanced\_samples-crop</a>&lt;/u&gt;》</li></ul><h3>8.4 非图像 tensor 内存对齐</h3><p>对于非图像 tensor，征程 6B 要求 128 对齐，征程 6EM 要求内存 64 对齐，征程 6PH 要求 256 对齐。如上图所示，模型输出节点虽然 stride[0] 为 4000，但需要申请的 BPU 内存大小（aligned byte size）为 4096，即为 128 对齐的结果。</p><p>在模型实际部署中，非图像输入/输出节点所需申请的内存大小（ aligned byte size）均可从模型节点属性的结构体中读取到（<code>hbDNNTensorProperties</code>），因此无需特别关注。</p><h3>8.5 图像 tensor 跨距对齐</h3><p>征程 6EMB 对于 Pyramid/Resizer 模型的图像输入，要求 W32 对齐，征程 6PH 要求 W64 对齐。若您有 征程 6 不同架构平台迁移的场景，请注意跨距对齐要求的差异。</p><p>部署代码建议您避免 hard code，推荐基于模型节点属性中的 validShape（张量有效内容尺寸）和 stride（张量各维度步长）进行解析和使用。</p><h4>8.5.1 Pyramid 输入</h4><p>Pyramid 输入 tensor 准备的参考代码如下：</p><pre><code class="Plain">hbDNNTensor *input = input_tensor;
for (int i = 0; i &lt; input_count; i++) {
HB_CHECK_SUCCESS(
    hbDNNGetInputTensorProperties(&amp;input[i].properties, dnn_handle, i),
    "hbDNNGetInputTensorProperties failed");
    
    auto dim_len = input[i].properties.validShape.numDimensions;    // 获取维度信息
    for (int32_t dim_i = dim_len - 1; dim_i &gt;= 0; --dim_i) {
      if (input[i].properties.stride[dim_i] == -1) {                // stride=-1即为动态
        auto cur_stride =                                           // 计算当前维度stride
            input[i].properties.stride[dim_i + 1] *
            input[i].properties.validShape.dimensionSize[dim_i + 1];
        input[i].properties.stride[dim_i] = ALIGN_32(cur_stride);   // 32对齐
      }
    }

    int input_memSize = input[i].properties.stride[0] *             // 计算内存大小
                        input[i].properties.validShape.dimensionSize[0];
    HB_CHECK_SUCCESS(hbUCPMallocCached(&amp;input[i].sysMem[0], input_memSize, 0),
                     "hbUCPMallocCached failed");
    
    const char *input_name;
    HB_CHECK_SUCCESS(hbDNNGetInputName(&amp;input_name, dnn_handle, i),    // 获取节点名称
                     "hbDNNGetInputName failed");
}</code></pre><p>示例：</p><pre><code class="Plain">ucp_tutorial/dnn/basic_samples/code/00_quick_start/resnet_nv12/src/main.cc</code></pre><p>​<strong>注意</strong>​：</p><p>视频通路上的金字塔硬件，其输出层支持配置 y、uv 的 stride，但仅要求 W16 对齐，若数据需要喂给 BPU 推理模型，建议直接按 BPU 的跨距对齐要求来配置。金字塔硬件的更多信息请参考系统软件用户手册。</p><h4>8.5.2 Resizer 输入</h4><p>Resizer 输入的 H、W 也是动态的，因此需要设置为原图尺寸，并计算好 W32 对齐的 Stride；ROI 作为模型输入节点，也需要对其进行赋值。以下为参考代码：</p><pre><code class="Plain">#define ALIGN(value, alignment) (((value) + ((alignment)-1)) &amp; ~((alignment)-1))
#define ALIGN_32(value) ALIGN(value, 32)

int prepare_image_tensor(const std::vector&lt;hbUCPSysMem&gt; &amp;image_mem, int input_h,
                         int input_w, hbDNNHandle_t dnn_handle,
                         std::vector&lt;hbDNNTensor&gt; &amp;input_tensor) {
  // 准备Y、UV输入tensor
  for (int i = 0; i &lt; 2; i++) {
    HB_CHECK_SUCCESS(hbDNNGetInputTensorProperties(&amp;input_tensor[i].properties,
                                                   dnn_handle, i),
                     "hbDNNGetInputTensorProperties failed");
    // auto w_stride = ALIGN_32(input_w);
    // int32_t y_mem_size = input_h * w_stride;
    input_tensor[i].sysMem[0] = image_mem[i];

    // 配置原图大小，NHWC
    input_tensor[i].properties.validShape.dimensionSize[1] = input_h;
    input_tensor[i].properties.validShape.dimensionSize[2] = input_w;
    if (i == 1) {
      // UV输入大小为Y的1/2
      input_tensor[i].properties.validShape.dimensionSize[1] /= 2;
      input_tensor[i].properties.validShape.dimensionSize[2] /= 2;
    }

    // stride满足32对齐
    input_tensor[i].properties.stride[1] =
        ALIGN_32(input_tensor[i].properties.stride[2] *
                 input_tensor[i].properties.validShape.dimensionSize[2]);
    input_tensor[i].properties.stride[0] =
        input_tensor[i].properties.stride[1] *
        input_tensor[i].properties.validShape.dimensionSize[1];
  }
  return 0;
}

// 准备roi输入tensor
int prepare_roi_tensor(const hbUCPSysMem *roi_mem, hbDNNHandle_t dnn_handle,
                       int32_t roi_tensor_id, hbDNNTensor *roi_tensor) {
  HB_CHECK_SUCCESS(hbDNNGetInputTensorProperties(&amp;roi_tensor-&gt;properties,
                                                 dnn_handle, roi_tensor_id),
                   "hbDNNGetInputTensorProperties failed");
  roi_tensor-&gt;sysMem[0] = *roi_mem;
  return 0;
}

int prepare_roi_mem(const std::vector&lt;hbDNNRoi&gt; &amp;rois,
                    std::vector&lt;hbUCPSysMem&gt; &amp;roi_mem) {
  auto roi_size = rois.size();
  roi_mem.resize(roi_size);
  for (auto i = 0; i &lt; roi_size; ++i) {
    int32_t mem_size = 4 * sizeof(int32_t);
    HB_CHECK_SUCCESS(hbUCPMallocCached(&amp;roi_mem[i], mem_size, 0),
                     "hbUCPMallocCached failed");
    int32_t *roi_data = reinterpret_cast&lt;int32_t *&gt;(roi_mem[i].virAddr);
    roi_data[0] = rois[i].left;
    roi_data[1] = rois[i].top;
    roi_data[2] = rois[i].right;
    roi_data[3] = rois[i].bottom;
    hbUCPMemFlush(&amp;roi_mem[i], HB_SYS_MEM_CACHE_CLEAN);
  }
  return 0;
}</code></pre><p>示例：</p><pre><code class="Plain">ucp_tutorial/dnn/basic_samples/code/02_advanced_samples/roi_infer/src/roi_infer.cc</code></pre><h3>8.6 小模型批量处理功能</h3><p>由于 BPU 是资源独占式硬件，所以对于 Latency 很小的模型而言，其框架调度开销占比会相对较大。在 征程 6 平台，UCP 支持通过复用 task\_handle 的方式，将多个小模型任务一次性下发，全部执行完成后再一次性返回，从而可将 N 次框架调度开销合并为 1 次，以下为参考代码：</p><pre><code class="Plain">// 获取模型指针并存储
std::vector&lt;hbDNNHandle_t&gt; model_handles;

// 准备各个模型的输入输出，准备过程省略
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; inputs;
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; outputs;

// 创建任务并进行推理
{
    // 创建并添加任务，复用task_handle
    hbUCPTaskHandle_t task_handle{nullptr};
    for(size_t task_id{0U}; task_id &lt; inputs.size(); task_id++){
        hbDNNInferV2(&amp;task_handle, outputs[task_id].data(), inputs[task_id].data(), model_handles[i]);
    }
    
    // 提交任务
    hbUCPSchedParam sche_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sche_param);
    sche_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &amp;sche_param);
    
    // 等待任务完成
    hbUCPWaitTaskDone(task_handle, 0);
}</code></pre><h3>8.7 优先级调度/抢占</h3><p>UCP 支持任务优先级调度和抢占，可通过 <code>hbUCPSchedParam</code> 结构体进行配置，其中：</p><ul><li><code>priority</code> &gt; <code>customId</code> &gt; submit\_time（任务提交时间）</li><li><code>priority</code> 支持 [0， 255]，对于模型任务而言：</li></ul><p>[0， 253] 为普通优先级，不可抢占其他任务，但在未执行时支持按优先级进行排队</p><p>254 为 high 抢占任务，可支持抢占普通任务</p><p>255 为 urgent 抢占任务，可抢占普通任务和 high 抢占任务</p><p>可被中断抢占的低优任务，需要在模型编译阶段配置 <code>max_time_per_fc</code> 参数拆分模型指令</p><ul><li>其他 backend 任务，priority 支持 [0， 255]，但不支持抢占，可以认为都是普通优先级</li></ul><h3>8.8 X86 仿真</h3><p>征程 6 工具链在 X86 端支持 hbm 指令仿真，但效率非常低，所以更推荐使用 quantized.bc 模型进行推理，其定点部分和 hbm 数值二进制一致，浮点部分可能存在架构本身差异，但通常对精度影响可忽略不计。</p><p>征程 6B 平台 X86 仿真需要配置如下环境变量，默认架构为"nash-m"：</p><p>export HB\_UCP\_SIM\_PLATFORM\_TYPE=nash-b</p><h4>8.8.1 推理 quantized.bc</h4><p><strong>Python 推理：</strong></p><p>quantized.bc 在 X86 端的推理，可以使用 <code>horizon_tc_ui</code> 包封装的 <code>HBRuntime</code> 接口，具体可见用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=pE%2BwfYJuR%2FUIov1%2FjaNPKg%3D%3D.HhNJEasV%2FRWH0uaMVYegXiZOMCaf9yomOGifxgtaJeU6fwbbqiD3DUgVPb8saIbWj03z8z8j39O2svB7jfaQrg%3D%3D" rel="nofollow" target="_blank">训练后量化-PTQ 转换工具-HBRuntime 推理库</a>&lt;/u&gt;》，参考代码如下：</p><pre><code class="Plain">import numpy as np
from horizon_tc_ui.hb_runtime import HBRuntime

sess = HBRuntime("quantized.bc")
input_names = sess.input_names
output_names = sess.output_names

data1 = np.load("input1.npy")
data2 = np.load("input2.npy")
input_feed = {input_names[0]: data1, input_names[1]: data2}

output = sess.run(output_names, input_feed)</code></pre><p>quantized.bc 也可以直接调用其 func 的 feed 接口进行推理，其输入格式也为 dict，value 支持 torch.tensor 和 np.array 两种类型，输出格式与输入格式保持一致。参考代码如下：</p><pre><code class="Plain">import numpy as np
from hbdk4.compiler import load

hbir = load("quantized.bc")
func = hbir[0]

data1 = np.load("input1.npy")
data2 = np.load("input2.npy") 
input_feed = {inputs[0].name: data1, inputs[1].name: data2}
hbir_outputs = func.feed(input_feed)</code></pre><p><strong>C++ 推理：</strong></p><p>quantized.bc 的 C++ 推理接口复用 hbm UCP 推理接口，仅 so 动态库需要替换成 X86 版本即可。 您也可以在 X86 端使用 <code>hrt_model_exec</code> 工具对 quantized.bc 进行模型信息查看和单帧推理。</p><h4>8.8.2 推理 hbm</h4><p>由于 X86 端 hbm 推理为指令仿真，运行速度非常慢，因此工具链提供了 <code>hbm_infer</code> 工具以便用户在服务器端给直连的开发板下发推理任务。本文只介绍最简单的单进程使用方式，多进程、多阶段模型输入输出的传输优化，以及统计模型推理、网络传输耗时等功能请参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=D1CexzTSFPBF0ZlvT5cbaA%3D%3D.57ZGrodXOKN1t9ihCjnNcjykDQJd8CwtIJk6FuVMl0oRsfnLv21EubIJlSMzc2iCjugsY%2FOSEW08udUt4k2Ic6s8o8a5o1%2FfL3azMjrcVBo%3D" rel="nofollow" target="_blank">hbm\_infer 工具介绍</a>&lt;/u&gt;》。</p><pre><code class="Plain"># hbm也可传入一个list，推理时通过指定model_name来选择推理哪个模型，推理所用的.so即可只传输一次
hbm_model = HbmRpcSession(
    host="xx.xx.xx.xx",
    local_hbm_path="xx.hbm", 
)

# 打印模型输入输出信息
hbm_model.show_input_output_info()

# 准备输入数据
input_data = {
    'img': torch.ones((1, 3, 224, 224), dtype=torch.int8)
}

# 执行推理并返回结果
# 若传入的是list，需要正确指定model_name
# output_data = hbm_model(input_data, model_name=model_name)
output_data = hbm_model(input_data) 

print([output_data[k].shape for k in output_data])

# 关闭server
hbm_model.close_server()</code></pre><h2>9.UCP 视觉处理/高性能算子</h2><p>除模型推理外，UCP 还提供了视觉处理和高性能算子两大方向的多种算子接口，可支持诸如 Remap、Jpeg、H264/265、FFT/IFF 等功能，这些算子底层是基于地平线 SOC 上不同硬件 IP 进行的封装，并提供统一的调用接口。</p><p>更多信息可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=SVynj3M0b6%2F0Y82YmyLqRg%3D%3D.go3AjKdIb4Pvnh4K3Mty7EBEWODyuGANa4ZwsBrfUydp7na4sFNBXQUOvcMwLKdngcWNBjnSfy%2BrTas2kAH0vA%3D%3D" rel="nofollow" target="_blank">统一计算平台</a>&lt;/u&gt;》的相关章节。</p><p><strong>注意：</strong></p><ol><li>板端实际部署时，ISP 到 Pyramid 的视频通路不建议使用 UCP，无法实现数据 Online，建议直接调用底软接口进行功能实现。</li><li>基于 DSP Backend 实现的 HPL 算子，在 征程 6B 平台仅会提供 征程 6EM 上 Q8 实现的源码，如需在 征程 6B 上使用 HPL 算子，需要您自行适配 V130 并编译镜像。VP 算子和 SLAM 等 征程 6EM 上已有的 Q8 DSP sample，将在 征程 6B 后续正式版本中提供 V130 版本。</li></ol><h2>10.UCP 自定义算子（DSP）</h2><p>为了简化用户开发，UCP 封装了一套基于 RPC 的开发框架，来实现 CPU 对 DSP 的功能调用，但具体 DSP 算子实现仍是调用 Cadence 接口去做开发。总体来说可分为三个步骤：</p><p>使用 Cadence 提供的工具及资料完成算子开发；</p><pre><code class="Plain">int test_custom_op(void *input, void *output, void *tm) {
  // custom impl
  return 0;
}</code></pre><p>DSP 侧通过 UCP 提供的 API 注册算子，编译带自定义算子的镜像；</p><pre><code class="Plain">// dsp镜像中注册自定义算子
hb_dsp_register_fn(cmd, test_custom_op, latency)</code></pre><p>ARM 侧通过 UCP 提供的算子调用接口，完成开发板上的部署使用。</p><pre><code class="Plain">// 将输入输出的hbUCPSysMem映射为DSP可访问的内存地址
hbUCPSysMem in;
hbUCPMalloc(&amp;in, in_size, 0)
hbDSPAddrMap(&amp;in, &amp;in)

hbUCPSysMem out;
hbUCPMalloc(&amp;out, out_size, 0)
hbDSPAddrMap(&amp;out, &amp;out)

// 创建并提交DSP任务
hbUCPTaskHandle_t taskHandle{nullptr};
hbDSPRpcV2(&amp;taskHandle, &amp;in, &amp;out, cmd)

hbUCPSchedParam ctrl_param;
HB_UCP_INITIALIZE_SCHED_PARAM(&amp;ctrl_param);
ctrl_param.backend = HB_UCP_DSP_CORE_ANY;
hbUCPSubmitTask(task_handle, &amp;ctrl_param);

// 等待任务完成
hbUCPWaitTaskDone(task_handle, 0);</code></pre><p>更多信息可见用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=51nOtKbb0lKnHalHfNP0pQ%3D%3D.vOVzQJ6psmj%2BpeF%2Fex%2BIAuf3BenfbbsiaJ8izXKHdMrpYYQOM6LKRCCgqFkf15GRTPiU27%2FiFK1WGWwTKSH9ha0fYn6HXQ4lLwo9W3E8ELQ%3D" rel="nofollow" target="_blank">统一计算平台-自定义算子-DSP 算子开发</a>&lt;/u&gt;》。</p><h2>11.性能监测工具</h2><p>征程 6 平台 BPU、DSP 都是独占的硬件资源，任务一旦提交就会独占推理，UCP 侧仅能通过 <code>hrt_ucp_monitor</code> 工具去监测其硬件占用率（采样频率支持配置 [10， 1000]，默认 500），并且能查看到 DDR 内存占用情况。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597881" alt="image.png" title="image.png" loading="lazy"/></p><h2>12.QNX 带来的功能裁剪</h2><ol><li><strong>UCP Service （中继模式）</strong>：<br/>UCP 框架支持两种主要工作模式：直连模式、中继模式。系统默认运行在直连模式下，在中继模式下，UCP 将支持跨进程任务优先级的统一调度。但 QNX 上跨进程调度的模型性能较差，不满足使用需求，故功能移除。</li><li><strong>UCP Trace</strong>： UCP trace 通过在 UCP 执行的关键路径上嵌入 trace 记录，提供深入分析 UCP 应用程序调度逻辑的能力。在出现性能异常时，可以通过分析 UCP trace，快速找到异常发生的时间点。但 QNX 底软不支持 Trace 功能，故移除。</li><li><strong>DEB 部署包</strong>： 用于简化板端部署，可通过自动安装所需的二进制文件和相关依赖库，快速设置并运行 UCP 相关的应用程序，具体包括：<code>ucp_service</code> 和 <code>hrt_ucp_monitor</code>。但鉴于 <code>ucp_service</code> 不支持，该功能也移除。</li></ol>]]></description></item><item>    <title><![CDATA[LLM推理时计算技术详解：四种提升大模型推理能力的方法 本文系转载，阅读原文
https://avo]]></title>    <link>https://segmentfault.com/a/1190000047597914</link>    <guid>https://segmentfault.com/a/1190000047597914</guid>    <pubDate>2026-02-06 21:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年LLM领域有个有意思的趋势：与其继续卷模型训练，不如在推理阶段多花点功夫。这就是所谓的推理时计算（Test-Time / Inference-Time Compute）：在推理阶段投入更多计算资源，包括更多Token、更多尝试、更深入的搜索，但不会改动模型权重。</p><p>ARC-AGI基准测试就是个典型案例。通过推理时技术可以达到87.5%的准确率，但代价是每个任务超过1000美元的推理成本。没用这些技术的LLM通常只能拿到不到25%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597916" alt="" title=""/></p><p>本文要讲四种主流的推理时计算技术：深度方向的Chain-of-Thought，宽度方向的Self-Consistency，搜索方向的Tree-of-Thoughts，以及迭代方向的Reflexion/Self-Refine。</p><h2>预备知识：LLM调用封装</h2><p>先把基础设施搭好。下面是通用的LLM调用接口和辅助函数：</p><pre><code> fromcollectionsimportCounter, deque
importre

# ---- LLM调用封装 ----
defllm(prompt: str, temperature: float=0.7, max_tokens: int=800) -&gt;str:
    """
    LLM调用的占位函数。
    在实际使用中，可以替换为OpenAI、Claude或本地模型的API调用。
    
    参数:
        prompt: 输入提示词
        temperature: 采样温度，控制输出多样性
        max_tokens: 最大生成token数
        
    返回:
        模型生成的文本
    """
    # 示例：使用OpenAI API
    # from openai import OpenAI
    # client = OpenAI()
    # response = client.chat.completions.create(
    #     model="gpt-4",
    #     messages=[{"role": "user", "content": prompt}],
    #     temperature=temperature,
    #     max_tokens=max_tokens
    # )
    # return response.choices[0].message.content
    
    raiseNotImplementedError("请实现你的LLM调用逻辑")

# ---- 辅助函数：提取最终答案 ----
defextract_final_answer(text: str) -&gt;str:
    """
    从模型输出中提取最终答案。
    
    寻找格式为 "FINAL: &lt;答案&gt;" 或 "Final: &lt;答案&gt;" 的模式。
    在实际应用中，建议：
    - 让模型输出JSON格式，如 {"final": "..."}
    - 或使用针对具体任务的解析逻辑
    
    参数:
        text: 模型的完整输出文本
        
    返回:
        提取的最终答案（最多200字符）
    """
    m=re.search(r"(FINAL|Final)\s*[:\-]\s*(.*)", text)
     return (m.group(2).strip() ifmelsetext.strip())[:200]</code></pre><h2>深度（Depth）：链式思维推理</h2><p>Chain-of-Thought（CoT）是最基础也用得最多的推理时技术。核心思想很直白：让模型「思考」久一点。</p><p>传统调用方式期望模型直接给答案，但复杂问题不是这么解决的。CoT让模型生成详细的中间推理步骤，在数学、逻辑推理、编程这些任务上效果很明显。</p><p>为什么管用？首先是分解作用，大问题拆成小步骤，每一步更容易做对。其次是中间步骤充当了一种「外部记忆」，帮模型追踪推理过程。第三是强制模型展示推理，减少直接「猜」答案的情况。最后，模型推理过程中可以自查前面步骤对不对。</p><p>触发CoT有几种常见办法：零样本提示就是加一句「Let's think step by step」；少样本提示是给2-3个带推理步骤的例子；指令微调是用带CoT标注的数据集训练；系统提示则是在system message里定义推理风格。</p><pre><code> defsolve_with_cot(question: str) -&gt;str:
    """
    使用链式思维（Chain-of-Thought）解决问题。
    
    通过精心设计的提示词，引导模型：
    1. 进行逐步推理
    2. 展示中间计算过程
    3. 最后给出明确的最终答案
    
    参数:
        question: 需要解答的问题
        
    返回:
        包含推理过程和最终答案的完整响应
    """
    prompt=f"""You are a careful reasoner. Your task is to solve the following problem.

Instructions:
1. Break down the problem into smaller steps
2. Show your reasoning for each step
3. Double-check your calculations
4. End with a clear final answer

Format your response as:
Step 1: [your first step]
Step 2: [your second step]
...
FINAL: &lt;your final answer&gt;

Question: {question}
"""
    # 使用较低的temperature以获得更确定性的输出
    returnllm(prompt, temperature=0.2, max_tokens=900)

# 使用示例
if__name__=="__main__":
    question="一个农场有鸡和兔，共35个头和94只脚。请问有多少只鸡和多少只兔？"
    result=solve_with_cot(question)
    print(result)
     print("\n提取的最终答案:", extract_final_answer(result))</code></pre><p>CoT适合数学应用题、逻辑推理、代码调试、规划任务这类需要多步计算的问题。简单事实问答用CoT有点浪费，创意写作也不太合适——过度结构化会限制发挥。</p><p>局限性也很明显。Token消耗会上升，输出越长成本越高。模型可能在推理链中犯错，错误还会传播。输出格式也不总是稳定，需要后处理。</p><h2>宽度（Width）：自洽性采样</h2><p>Self-Consistency的想法很简单：与其相信单次输出，不如生成多个答案，选最一致的那个。</p><p>有点像集体决策——单条推理链可能出错，但如果多条独立路径都指向同一答案，那答案八成是对的。</p><p>这方法管用的原因：单次采样可能因为随机性出错，多次采样能平均掉这些错误。正确答案往往能通过多条不同路径得到。不同路径可能捕捉问题的不同侧面。答案的一致性程度还顺便反映了模型的「信心」。</p><p>做Self-Consistency有几个关键决策要做。</p><p>第一是采样多样性。这点至关重要。如果所有采样都走同一条推理路径，自洽性就没意义了。高多样性设置是temperature 0.7-0.9、top_p 0.9-0.95，加上多样的提示词变体。temperature太低或提示词太固定都不行。</p><p>第二是采样数量。3-5个边际收益最高，适合成本敏感场景；10-20个是常规配置；40个以上适合对准确率要求极高的场景，但边际收益已经很低了。</p><p>第三是聚合策略。最常用的是多数投票，选出现次数最多的答案。也可以加权投票，根据置信度加权。还可以把相似答案聚类后再投票。</p><pre><code> defsolve_with_self_consistency(
    question: str, 
    n: int=10,
    temperature: float=0.8
) -&gt;dict:
    """
    使用自洽性（Self-Consistency）方法解决问题。
    
    通过高温度采样生成多个多样化的答案，
    然后通过多数投票选择最一致的答案。
    
    参数:
        question: 需要解答的问题
        n: 采样数量，建议10-20
        temperature: 采样温度，建议0.7-0.9以确保多样性
        
    返回:
        包含以下键的字典:
        - final: 最终答案（得票最多的）
        - votes: 该答案的得票数
        - confidence: 置信度（得票数/总数）
        - all_finals: 所有提取的答案列表
        - vote_distribution: 完整的投票分布
        - samples: 所有原始输出（用于调试）
    """
    prompt_template="""Solve this problem step by step. 
Show your reasoning, then end with 'FINAL: ...'

Question: {question}"""
    
    samples= []
    foriinrange(n):
        out=llm(
            prompt_template.format(question=question),
            temperature=temperature,  # 高温度确保多样性
            max_tokens=900
        )
        samples.append(out)
    
    # 提取所有最终答案
    finals= [extract_final_answer(s) forsinsamples]
    
    # 统计投票
    vote_counter=Counter(finals)
    most_common=vote_counter.most_common()
    winner=most_common[0]
    
    return {
        "final": winner[0],
        "votes": winner[1],
        "confidence": winner[1] /n,
        "all_finals": finals,
        "vote_distribution": dict(vote_counter),
        "samples": samples
    }

defsolve_with_weighted_consistency(
    question: str,
    n: int=10,
    score_fn=None
) -&gt;dict:
    """
    带权重的自洽性方法。
    
    除了多数投票外，还可以根据每个答案的质量分数加权。
    
    参数:
        question: 需要解答的问题
        n: 采样数量
        score_fn: 评分函数，接受(question, answer)返回0-1的分数
        
    返回:
        包含加权投票结果的字典
    """
    samples= []
    for_inrange(n):
        out=llm(
            f"Solve step by step. End with 'FINAL: ...'\n\nQ: {question}",
            temperature=0.8,
            max_tokens=900
        )
        samples.append(out)
    
    finals= [extract_final_answer(s) forsinsamples]
    
    # 加权投票
    weighted_votes= {}
    forfinal, sampleinzip(finals, samples):
        weight=score_fn(question, sample) ifscore_fnelse1.0
        weighted_votes[final] =weighted_votes.get(final, 0) +weight
    
    winner=max(weighted_votes.items(), key=lambdax: x[1])
    
    return {
        "final": winner[0],
        "weighted_score": winner[1],
        "weighted_distribution": weighted_votes,
        "all_finals": finals
    }

# 使用示例
if__name__=="__main__":
    question="如果今天是星期三，那么100天后是星期几？"
    result=solve_with_self_consistency(question, n=10)
    
    print(f"最终答案: {result['final']}")
    print(f"得票数: {result['votes']}/{len(result['all_finals'])}")
    print(f"置信度: {result['confidence']:.1%}")
     print(f"投票分布: {result['vote_distribution']}")</code></pre><p>Self-Consistency适合有确定答案的问题（数学、编程、事实问答）、答案空间有限的问题（选择题、是/否问题）、以及生产环境中需要高可靠性的场景。开放式问题答案空间太大，每次答案都不同，投票没意义。创意任务没有「正确」答案可投票，也不适用。</p><p>局限性：成本线性增长，N次采样就是N倍成本。如果模型系统性地偏向某个错误答案，投票也救不了。同一答案的不同表述可能被当作不同答案，答案标准化是个麻烦事。</p><h2>搜索（Search）：思维树探索</h2><p>Tree-of-Thoughts（ToT）把推理过程当成搜索问题来做。每个节点是一个「思维状态」，也就是部分推理结果；每条边是一个「思维步骤」，即推理动作；目标是找到通向正确答案的路径。</p><p>跟线性的CoT不同，ToT允许分支（从一个状态探索多个可能的下一步）、回溯（放弃没希望的分支，回到之前的状态）、评估（判断当前状态离目标有多近）。</p><p>为什么有效？线性推理一旦犯错就没法恢复，ToT可以回溯。某些问题天然是树形结构，比如博弈、规划。通过评估函数引导搜索，避免盲目探索。只深入探索有希望的分支，Token利用率更高。</p><p>搜索策略有几种选择。BFS广度优先，逐层探索，不会错过浅层解但内存消耗大。DFS深度优先，一条路走到底，内存效率高但可能陷入死胡同。Beam Search每层保留top-k状态，平衡效率和覆盖，但可能丢失最优解。A*用启发式函数引导，最优且高效，但需要好的启发函数。MCTS蒙特卡洛树搜索能处理大搜索空间，但需要大量模拟。</p><pre><code> deftot_bfs(
    question: str, 
    max_depth: int=4, 
    beam: int=3, 
    branch: int=4,
    external_evaluator=None
) -&gt;dict:
    """
    使用BFS策略的思维树（Tree-of-Thoughts）方法。
    
    工作流程:
    1. 从空状态开始
    2. 对当前frontier中的每个状态，生成多个可能的下一步
    3. 评估所有新状态
    4. 保留得分最高的beam个状态作为新frontier
    5. 重复直到达到最大深度
    6. 从最佳状态生成最终答案
    
    参数:
        question: 需要解答的问题
        max_depth: 最大搜索深度
        beam: 每层保留的状态数（beam width）
        branch: 每个状态扩展的分支数
        external_evaluator: 外部评估函数（可选），
                           接受(question, state)返回分数
        
    返回:
        包含以下键的字典:
        - final_text: 最终答案
        - best_state: 最佳推理状态
        - best_score: 最佳状态的分数
        - search_tree: 搜索过程的记录（用于可视化）
    """
    
    defpropose_next_steps(state: str) -&gt;list:
        """
        给定当前推理状态，生成多个可能的下一步。
        """
        prompt=f"""You are exploring different ways to solve a problem.

Question: {question}

Current reasoning state:
{stateifstateelse"(Starting from scratch)"}

Propose {branch} different possible next steps to continue the reasoning.
Each step should be a distinct approach or calculation.
Return as a numbered list:
1. [first possible step]
2. [second possible step]
...
"""
        raw=llm(prompt, temperature=0.9, max_tokens=400)
        
        # 解析编号列表
        steps= []
        forlineinraw.splitlines():
            line=line.strip()
            iflineandline[0].isdigit():
                # 移除编号前缀
                step=line.split(".", 1)[-1].strip()
                ifstep:
                    steps.append(step)
        
        returnsteps[:branch] ifstepselse [raw.strip()]
    
    defllm_score_state(state: str) -&gt;float:
        """
        使用LLM评估一个推理状态的promising程度。
        
        注意：在实际应用中，使用外部评估器（如单元测试、规则检查）
        通常比LLM自我评估更可靠。
        """
        ifexternal_evaluator:
            returnexternal_evaluator(question, state)
        
        prompt=f"""Evaluate how promising this partial solution is.

Question: {question}

Current reasoning state:
{state}

Consider:
1. Is the reasoning logical and correct so far?
2. Is it making progress toward a solution?
3. Are there obvious errors or dead ends?

Rate from 0 to 10 (10 = very promising, likely to lead to correct answer).
Output only a number.
"""
        s=llm(prompt, temperature=0.0, max_tokens=10).strip()
        try:
            returnfloat(re.findall(r"\d+(\.\d+)?", s)[0])
        except:
            return5.0  # 默认中等分数
    
    # 初始化
    frontier= [""]  # 初始状态为空
    best_state=""
    best_score=-1.0
    search_tree= []  # 记录搜索过程
    
    fordepthinrange(max_depth):
        candidates= []
        depth_record= {"depth": depth, "states": []}
        
        forstateinfrontier:
            next_steps=propose_next_steps(state)
            
            forstepinnext_steps:
                # 构建新状态
                new_state= (state+"\n"+step).strip()
                
                # 评估新状态
                score=llm_score_state(new_state)
                candidates.append((score, new_state))
                
                depth_record["states"].append({
                    "state": new_state[:200] +"..."iflen(new_state) &gt;200elsenew_state,
                    "score": score
                })
        
        search_tree.append(depth_record)
        
        # 排序并保留top-k
        candidates.sort(reverse=True, key=lambdax: x[0])
        frontier= [sfor_, sincandidates[:beam]]
        
        # 更新最佳状态
        ifcandidatesandcandidates[0][0] &gt;best_score:
            best_score, best_state=candidates[0]
    
    # 从最佳状态生成最终答案
    final_prompt=f"""Based on the reasoning below, produce the final answer.

Question: {question}

Reasoning:
{best_state}

Provide a clear, concise final answer.
End with: FINAL: &lt;your answer&gt;
"""
    final=llm(final_prompt, temperature=0.2, max_tokens=400)
    
    return {
        "final_text": final,
        "final_answer": extract_final_answer(final),
        "best_state": best_state,
        "best_score": best_score,
        "search_tree": search_tree
    }

deftot_dfs(
    question: str,
    max_depth: int=5,
    branch: int=3,
    threshold: float=3.0
) -&gt;dict:
    """
    使用DFS策略的思维树方法。
    
    通过深度优先搜索探索解决方案空间，
    当某个分支的分数低于阈值时进行剪枝。
    
    参数:
        question: 需要解答的问题
        max_depth: 最大搜索深度
        branch: 每个状态扩展的分支数
        threshold: 剪枝阈值，分数低于此值的分支被放弃
        
    返回:
        包含最终答案和搜索路径的字典
    """
    best_result= {"state": "", "score": -1.0}
    visited_count= [0]  # 使用列表以便在嵌套函数中修改
    
    defpropose_steps(state: str) -&gt;list:
        prompt=f"""Propose {branch} next reasoning steps.

Question: {question}
Current state:
{stateifstateelse"(empty)"}

Return as numbered list."""
        raw=llm(prompt, temperature=0.9, max_tokens=300)
        steps= [l.split(".", 1)[-1].strip() 
                 forlinraw.splitlines() 
                 ifl.strip()[:1].isdigit()]
        returnsteps[:branch] ifstepselse [raw.strip()]
    
    defscore_state(state: str) -&gt;float:
        prompt=f"""Rate this partial solution 0-10.
Question: {question}
State: {state}
Output only a number."""
        s=llm(prompt, temperature=0.0, max_tokens=10).strip()
        try:
            returnfloat(re.findall(r"\d+(\.\d+)?", s)[0])
        except:
            return5.0
    
    defdfs(state: str, depth: int):
        visited_count[0] +=1
        
        ifdepth&gt;=max_depth:
            score=score_state(state)
            ifscore&gt;best_result["score"]:
                best_result["state"] =state
                best_result["score"] =score
            return
        
        forstepinpropose_steps(state):
            new_state= (state+"\n"+step).strip()
            score=score_state(new_state)
            
            # 剪枝：跳过低分分支
            ifscore&lt;threshold:
                continue
            
            ifscore&gt;best_result["score"]:
                best_result["state"] =new_state
                best_result["score"] =score
            
            dfs(new_state, depth+1)
    
    dfs("", 0)
    
    # 生成最终答案
    final=llm(
        f"""Produce final answer based on:
Question: {question}
Reasoning: {best_result['state']}
End with FINAL: ...""",
        temperature=0.2
    )
    
    return {
        "final_text": final,
        "final_answer": extract_final_answer(final),
        "best_state": best_result["state"],
        "best_score": best_result["score"],
        "states_visited": visited_count[0]
    }

# 使用示例
if__name__=="__main__":
    question="使用数字1, 5, 6, 7（每个只能用一次），通过加减乘除得到24。"
    
    result=tot_bfs(question, max_depth=3, beam=2, branch=3)
    
    print("=== BFS Tree-of-Thoughts ===")
    print(f"最佳推理路径:\n{result['best_state']}")
    print(f"\n最佳分数: {result['best_score']}")
     print(f"\n最终答案: {result['final_answer']}")</code></pre><p>ToT适合组合问题（24点游戏、数独）、规划任务、博弈问题（象棋、围棋）、头脑风暴这类需要探索不同方向的场景。答案空间极大时可能需要配合启发式剪枝。简单问题用不着——直接CoT就够了。</p><p>局限性：计算成本高昂，需要大量LLM调用来评估和扩展节点。LLM自评估不太可靠，评估函数质量直接决定效果。实现复杂度比其他几种方法高不少。还有些问题压根没有明显的树形结构，ToT就不太适用。</p><h2>迭代（Iteration）：反思与自我改进</h2><p>Reflexion和Self-Refine用的是经典的「生成-评估-改进」循环：模型先产生初始答案，拿到反馈后修正答案，如此反复直到满意或达到最大轮数。</p><p>人类学习不也是这样吗？很少有事情一次就做对，总是通过反馈不断改进。</p><p>但有个重要的坑要注意：没有可靠外部反馈的「自我纠正」可能适得其反。</p><p>研究表明，模型仅靠自己判断来「自我纠正」时，可能把正确答案改成错误答案，可能对错误判断过度自信，可能在无效修改上浪费Token。</p><p>所以最佳实践是尽量用外部反馈源。代码执行（单元测试、错误信息）和规则检查（格式验证、约束检查）最可靠。工具调用（计算器、搜索引擎）和人类反馈也不错。另一个LLM做交叉验证勉强能用。同一个LLM自评效果最差，缺乏外部参照。</p><pre><code> defself_refine(
    question: str, 
    score_fn, 
    rounds: int=3,
    improvement_threshold: float=0.1
) -&gt;dict:
    """
    使用自我改进（Self-Refine）方法迭代优化答案。
    
    核心流程：生成 -&gt; 评估 -&gt; 根据反馈改进 -&gt; 重复
    
    参数:
        question: 需要解答的问题
        score_fn: 评估函数，签名为:
                  score_fn(answer_text) -&gt; (score: float, feedback: str)
                  - score: 0.0-1.0之间的分数
                  - feedback: 具体的改进建议
                  强烈建议使用外部评估器！
        rounds: 最大改进轮数
        improvement_threshold: 最小改进阈值，低于此值则提前停止
        
    返回:
        包含以下键的字典:
        - final: 最终答案
        - final_score: 最终分数
        - history: 完整的改进历史
        - rounds_used: 实际使用的轮数
    """
    # 生成初始答案
    initial_prompt=f"""Provide a thoughtful answer to this question.
Show your reasoning and end with FINAL: ...

Question: {question}
"""
    answer=llm(initial_prompt, temperature=0.4)
    history= []
    prev_score=-float('inf')
    
    forround_numinrange(rounds):
        # 评估当前答案
        score, feedback=score_fn(answer)
        
        history.append({
            "round": round_num+1,
            "answer": answer,
            "score": score,
            "feedback": feedback
        })
        
        # 检查是否有足够的改进
        ifround_num&gt;0and (score-prev_score) &lt;improvement_threshold:
            # 如果改进不明显，考虑提前停止
            ifscore&gt;=prev_score:
                pass  # 继续，至少没有退步
            else:
                # 退步了，恢复上一个答案
                answer=history[-2]["answer"]
                score=history[-2]["score"]
                break
        
        # 如果分数已经很高，提前停止
        ifscore&gt;=0.95:
            break
        
        prev_score=score
        
        # 根据反馈改进答案
        refine_prompt=f"""Improve your answer based on the feedback below.

Question: {question}

Your current answer:
{answer}

Feedback (score: {score:.2f}/1.00):
{feedback}

Instructions:
1. Keep what is correct in your current answer
2. Fix the issues mentioned in the feedback
3. Make sure not to introduce new errors
4. End with FINAL: ...

Improved answer:
"""
        answer=llm(refine_prompt, temperature=0.3)
    
    # 最终评估
    final_score, final_feedback=score_fn(answer)
    
    return {
        "final": answer,
        "final_answer": extract_final_answer(answer),
        "final_score": final_score,
        "history": history,
        "rounds_used": len(history)
    }

# ---- 示例评估函数 ----

defmake_code_evaluator(test_cases: list):
    """
    创建一个代码评估函数。
    
    参数:
        test_cases: 测试用例列表，每个元素是(input, expected_output)
        
    返回:
        评估函数
    """
    defevaluator(code_answer: str) -&gt;tuple:
        # 提取代码块
        code_match=re.search(r"```python\n(.*?)```", code_answer, re.DOTALL)
        ifnotcode_match:
            return0.0, "No Python code block found. Please wrap your code in ```python ... ```"
        
        code=code_match.group(1)
        
        passed=0
        failed_cases= []
        
        forinp, expectedintest_cases:
            try:
                # 危险：实际应用中应使用沙箱！
                local_vars= {}
                exec(code, {"__builtins__": {}}, local_vars)
                
                # 假设代码定义了solve函数
                if'solve'inlocal_vars:
                    result=local_vars['solve'](inp)
                    ifresult==expected:
                        passed+=1
                    else:
                        failed_cases.append(f"Input: {inp}, Expected: {expected}, Got: {result}")
                else:
                    return0.0, "No 'solve' function found in your code."
                    
            exceptExceptionase:
                failed_cases.append(f"Input: {inp}, Error: {str(e)}")
        
        score=passed/len(test_cases)
        
        iffailed_cases:
            feedback="Failed test cases:\n"+"\n".join(failed_cases[:3])  # 最多显示3个
            iflen(failed_cases) &gt;3:
                feedback+=f"\n... and {len(failed_cases) -3} more failures"
        else:
            feedback="All test cases passed!"
        
        returnscore, feedback
    
    returnevaluator

defmake_math_evaluator(correct_answer):
    """
    创建一个数学答案评估函数。
    
    参数:
        correct_answer: 正确答案
        
    返回:
        评估函数
    """
    defevaluator(answer_text: str) -&gt;tuple:
        extracted=extract_final_answer(answer_text)
        
        # 尝试数值比较
        try:
            extracted_num=float(re.findall(r"-?\d+\.?\d*", extracted)[0])
            correct_num=float(correct_answer)
            
            ifabs(extracted_num-correct_num) &lt;0.01:
                return1.0, "Correct!"
            else:
                return0.0, f"Incorrect. Your answer: {extracted_num}, Expected: {correct_num}"
        except:
            pass
        
        # 字符串比较
        ifextracted.lower().strip() ==str(correct_answer).lower().strip():
            return1.0, "Correct!"
        else:
            return0.0, f"Incorrect. Your answer: {extracted}, Expected: {correct_answer}"
    
    returnevaluator

defmake_llm_evaluator(criteria: str):
    """
    创建一个基于LLM的评估函数（不推荐作为唯一评估源）。
    
    参数:
        criteria: 评估标准描述
        
    返回:
        评估函数
    """
    defevaluator(answer_text: str) -&gt;tuple:
        prompt=f"""Evaluate this answer based on the following criteria:

Criteria: {criteria}

Answer to evaluate:
{answer_text}

Provide:
1. A score from 0.0 to 1.0
2. Specific feedback on what's wrong and how to improve

Format:
SCORE: [number]
FEEDBACK: [your feedback]
"""
        response=llm(prompt, temperature=0.0)
        
        try:
            score=float(re.search(r"SCORE:\s*([\d.]+)", response).group(1))
            score=min(1.0, max(0.0, score))
        except:
            score=0.5
        
        try:
            feedback=re.search(r"FEEDBACK:\s*(.+)", response, re.DOTALL).group(1).strip()
        except:
            feedback=response
        
        returnscore, feedback
    
    returnevaluator

# 使用示例
if__name__=="__main__":
    # 示例1：代码任务
    question="编写一个函数solve(n)，返回n的阶乘。"
    test_cases= [
        (0, 1),
        (1, 1),
        (5, 120),
        (10, 3628800)
    ]
    
    result=self_refine(
        question=question,
        score_fn=make_code_evaluator(test_cases),
        rounds=3
    )
    
    print("=== Self-Refine for Code ===")
    print(f"最终分数: {result['final_score']:.2%}")
    print(f"使用轮数: {result['rounds_used']}")
    print(f"\n改进历史:")
    forhinresult['history']:
        print(f"  Round {h['round']}: score={h['score']:.2f}")
    
    # 示例2：数学任务
    question="计算 17 * 23 + 45 - 12"
    correct=17*23+45-12
    
    result=self_refine(
        question=question,
        score_fn=make_math_evaluator(correct),
        rounds=2
    )
    
    print("\n=== Self-Refine for Math ===")
    print(f"最终答案: {result['final_answer']}")
    print(f"正确答案: {correct}")
     print(f"最终分数: {result['final_score']:.2%}")</code></pre><p>Self-Refine适合代码生成（有单元测试作为外部反馈）、格式化任务（有明确规范可检查）、约束满足问题（可验证约束是否满足）、事实核查（可通过检索验证）。主观任务需要人类反馈或多模型交叉验证。没有反馈来源时别用——纯LLM自评不靠谱。</p><p>局限性：反馈质量决定上限，垃圾反馈只会导致垃圾改进。模型有时候会在不同版本之间来回「改」，出现震荡。每轮迭代都消耗Token，成本会累积。也无法保证收敛——模型可能根本没法利用反馈真正改进。</p><hr/><h2>技术对比与选择指南</h2><p>四种技术各有特点。CoT思考更深，Token消耗低，LLM只调用一次，实现简单，不需要外部反馈，适合推理链问题。Self-Consistency采样更广，Token消耗中等，LLM调用N次，实现也简单，不需要外部反馈，适合有确定答案的问题。ToT探索更多，Token消耗高，LLM调用次数是分支数乘以深度，实现复杂，外部反馈可选但推荐，适合组合和规划问题。Self-Refine改进更好，Token消耗中等，LLM调用次数是轮数乘以2，实现复杂度中等，强烈推荐外部反馈，适合可迭代改进的问题。</p><p>选择思路如下，需要分步推理就先试CoT，不稳定的话加上Self-Consistency。有确定答案且需要可靠性，直接用Self-Consistency。组合或搜索问题用ToT。有外部反馈源就用Self-Refine。不确定用什么就先用CoT，看效果再定。</p><p>这些技术可以组合使用。CoT加SC是每次采样都用CoT然后多数投票。ToT加SC是ToT生成多个最终答案用SC选择。ToT加SR是用SR迭代改进ToT的最佳结果。复杂任务可能需要把多种技术串成流水线。</p><pre><code> defcombined_approach(question: str, score_fn) -&gt;str:
    """
    组合使用多种推理时技术。
    
    流程:
    1. 用ToT探索解决方案空间
    2. 用Self-Consistency从多个ToT结果中选择
    3. 用Self-Refine迭代改进最终答案
    """
    # 第一阶段：ToT探索（运行3次）
    tot_results= []
    for_inrange(3):
        result=tot_bfs(question, max_depth=3, beam=2, branch=3)
        tot_results.append(result['final_answer'])
    
    # 第二阶段：Self-Consistency选择
    vote=Counter(tot_results).most_common(1)[0][0]
    
    # 第三阶段：Self-Refine改进
    final_result=self_refine(
        question=question,
        score_fn=score_fn,
        rounds=2
    )
    
     returnfinal_result['final_answer']</code></pre><h2>实践建议</h2><p>别一上来就用最复杂的技术。推荐的顺序是：先直接提问作为baseline，然后加CoT提示，再加Self-Consistency，最后才考虑ToT或Self-Refine。</p><p>对于Self-Refine和ToT，评估器质量直接决定效果。花时间构建好的评估器比调参更重要。</p><p>推理时技术能大幅提升性能，但成本也会大幅增加。建议设置Token预算上限，记录每个任务的实际消耗，根据任务重要性调整投入。</p><p>部署到生产环境前做A/B测试，找到最佳的性能/成本权衡点。</p><h2>总结</h2><p>推理时计算技术代表了LLM能力释放的新范式。在推理阶段多投入一些计算，同一个模型不重新训练就能有明显提升。本文介绍的四种技术——CoT、Self-Consistency、Tree-of-Thoughts、Self-Refine——各有特点和适用场景。理解原理和局限性，选择合适的技术或组合，是LLM应用开发的关键技能。</p><p>随着这一领域的发展，会有更多创新的推理时技术出现。但核心原则不会变：给模型更多「思考」的空间，让它展示真正的推理能力。</p><p><a href="https://link.segmentfault.com/?enc=mQc7BXpN%2BAj7him%2BDg2S6Q%3D%3D.KStjLVY33kRMEMbU7NaYir%2Bx%2BBDoNO8OkKlu5gHcLNU3ZBF0ALqJWOgOSnwoZDJSSCs5upc0kNiv0OF%2B4HmucQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/2bb5bb4e569a4687a272dc6e9fe6809a</a></p>]]></description></item><item>    <title><![CDATA[数据库索引怎么用才快？亿级数据实测指南 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047597923</link>    <guid>https://segmentfault.com/a/1190000047597923</guid>    <pubDate>2026-02-06 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>很多人在本地开发时可能都会遇到这样的情况。数据少的时候，页面秒开，SQL 怎么写都感觉不到卡顿。可一上线，面对百万级流量，查询直接超时，数据库 CPU 飙升。</p><p>要避免这种开发时候猛如虎，上线操作二百五的尴尬，最好的办法就是在本地造点数据出来测。只有数据量上去了，那些平时隐藏的性能坑才会原形毕露。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSyz" alt="image.png" title="image.png"/></p><h2>先造它一亿条数据</h2><p>如果表里只有几千行数据，全表扫描和走索引几乎没区别，甚至全表扫描更快。要验证索引策略，必须上强度。</p><p>别傻乎乎地写脚本在应用层循环插入，网络开销会慢到怀疑人生。PostgreSQL 自带的 <code>generate_series</code> 是个神器，下面这个函数能在几分钟内帮你造出一亿条模拟的用户操作日志，足够把坑找出来。</p><pre><code class="SQL">-- 创建一个能快速生成大量模拟数据的函数
CREATE OR REPLACE FUNCTION populate_large_table(target_rows BIGINT)
RETURNS VOID AS $$
BEGIN
  -- 批量插入，避免逐行提交的开销
  INSERT INTO user_events (user_id, event_type, created_at)
  SELECT
    (random() * 1000000)::INTEGER, -- 模拟 100万个不同的用户
    CASE (random() * 3)::INTEGER    -- 随机生成操作类型
      WHEN 0 THEN 'login'
      WHEN 1 THEN 'logout'
      WHEN 2 THEN 'purchase'
      ELSE 'view'
    END,
    NOW() - (random() * INTERVAL '365 days') -- 分布在过去一年
  FROM generate_series(1, target_rows);
END;
$$ LANGUAGE plpgsql;

-- 执行生成（注意：这会占用不少磁盘空间，执行时间取决于机器性能）
-- SELECT populate_large_table(100000000);</code></pre><p>当这一亿条数据落盘后，随便跑一个 <code>SELECT * FROM user_events WHERE user_id = 12345</code>，就会发现耗时从毫秒级变成了几秒甚至十几秒。这时候，索引的价值就体现出来了。</p><h2>索引不是越多越好</h2><p>新手最容易犯的错就是给每个字段都加索引。要知道，索引本质上是空间换时间，而且是有代价的。</p><p><strong>读取变快，写入变慢</strong></p><p>每次 <code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code>，数据库不仅要改数据文件，还得同步更新相关的索引树。如果你表上有 5 个索引，插入一行数据就得维护 5 棵树。对于日志、IoT 传感器上报这类写多读少的业务，索引加多了简直灾难</p><p><strong>策略建议：</strong></p><ul><li><strong>高频查询列</strong>：加索引（如外键、时间戳）。</li><li><strong>高频更新列</strong>：慎加索引。</li><li><p><strong>低区分度列</strong>：别加索引。比如“性别”或“状态（0/1）”，数据库扫索引发现要回表一半的数据，通常会直接放弃索引走全表扫描，这索引建了也是白建。</p><p>*</p></li></ul><h2>拒绝盲猜，看执行计划</h2><p>别觉得写了 <code>WHERE user_id = ...</code> 数据库就一定走索引。优化器有时候比我们想象的懒。</p><p>一定要用 <code>EXPLAIN</code>（PostgreSQL/MySQL 通用）来看看数据库到底在干什么：</p><pre><code class="sql">EXPLAIN ANALYZE SELECT * FROM user_events WHERE user_id = 42;</code></pre><ul><li>如果看到 <strong>Index Scan</strong>：恭喜，索引生效了。</li><li><p>如果看到 <strong>Seq Scan</strong>（Sequential Scan）：说明在全表扫描。这时候就要检查是不是数据分布不均，或者查询条件没对上索引。</p><p>*</p></li></ul><h2>几种常用的索引避坑姿势</h2><h3>1. 复合索引：顺序是关键</h3><p>当查询条件包含多个字段时，比如要查“某用户在某天的记录”，单列索引往往不够快。这时候要建复合索引：</p><pre><code class="sql">CREATE INDEX idx_user_date ON user_events(user_id, created_at);</code></pre><p><strong>注意顺序（最左前缀原则）</strong> ：</p><p>这个索引对 <code>WHERE user_id = ?</code> 有效，对 <code>WHERE user_id = ? AND created_at = ?</code> 也有效。</p><p>但如果查询只有 <code>WHERE created_at = ?</code>，这个索引就废了。把最常用的筛选列放在最左边。</p><h3>2. 唯一索引：既是约束也是加速</h3><p>如果业务逻辑要求邮箱或手机号不能重复，直接上唯一索引。它不仅能提升查询速度，还能在数据库层面兜底，防止代码逻辑漏洞导致脏数据写入。</p><pre><code class="sql">CREATE UNIQUE INDEX idx_unique_email ON users(email);</code></pre><h3>3. 针对性索引类型</h3><ul><li><strong>全文索引 (Full-Text)</strong> ：不要用 <code>LIKE '%关键词%'</code> 去搜大段文本，慢得要死。MySQL 和 PG 都有专门的全文索引，支持分词。</li><li><strong>GIN 索引</strong>：PG 特有，专门处理 JSONB 或数组数据。</li><li><p><strong>位图索引 (Bitmap)</strong> ：适合数据仓库场景下，针对“状态”、“标签”这类低基数字段的组合查询（PG 默认常用 B-Tree，特定场景可用 BRIN 或扩展）。</p><p>*</p></li></ul><p>对于新手来说，安装数据库需要很繁琐的步骤。有了 ServBay 就不同了。ServBay能<a href="https://link.segmentfault.com/?enc=sQ5nNtMBUFK%2B1m6KCA1L8Q%3D%3D.LSB%2B%2FHWLOESGxVnpIkJpAitsj19PnLSoZFwHp%2FQZa3%2Fqu7MpIJHFg5%2BeISpWUXlb" rel="nofollow" target="_blank">一键安装数据库</a>。而且它支持<strong>多实例并发运行</strong>。就是说你可以同时启动 MySQL 8.0 和 MariaDB，或者同时跑一个 PostgreSQL 12 和 16。</p><p>这就很方便做数据迁移测试或者性能对比，看看同一条复杂 SQL 在不同版本数据库下的表现差异。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnSyA" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>一键部署</strong>：囊括了 MySQL, PostgreSQL, MongoDB, Redis, MariaDB 等主流数据库，不用到处找安装包或配置 Brew。</li><li><strong>开箱即用</strong>：安装完自动配好环境变量，直接在终端敲 <code>psql</code> 或 <code>mysql</code> 就能连，省去了改配置文件的麻烦。</li><li><strong>不污染系统</strong>：所有服务独立运行，不想用的时候一键停止或卸载，不会在系统里留下垃圾文件。</li></ul><p>其实数据库优化从来就没有什么标准答案，只有取舍。</p><p>是牺牲写入速度换读取速度？还是牺牲磁盘空间换查询时间？这些都得看具体的业务场景，甚至要看你能不能接受数据会有几秒钟的延迟。</p><p>总之，实践是检验真理的唯一标准。自己试试就知道了。</p>]]></description></item><item>    <title><![CDATA[OFDM 的“阿喀琉斯之踵”：当 Sinc 函数的旁瓣，划破了 6G 的速度梦 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047597821</link>    <guid>https://segmentfault.com/a/1190000047597821</guid>    <pubDate>2026-02-06 20:03:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>01. 完美的代价：OFDM 的基因缺陷</h4><p>一切悲剧的根源，早在我们选择 OFDM 的那一刻就注定了。</p><p>为了追求频谱效率的极致，我们在时域上选择了最简单的<strong>“矩形窗” (Rectangular Window)</strong> 来截断信号。</p><p>在信号与系统的课本里，有一个著名的对偶关系：</p><blockquote><strong>时域的矩形 $\leftrightarrow$ 频域的 Sinc 函数</strong></blockquote><p>这就是 OFDM 子载波的真面目——<strong>Sinc 函数</strong> ($\frac{sin x}{x}$)。</p><p>它长得并不像一根完美的针，而是一个带着无数“拖油瓶”的波形：</p><ul><li>​<strong>主瓣</strong>​：高耸入云，承载有用信息。</li><li>​ <strong>旁瓣</strong>​  ：像波纹一样向两边扩散，且衰减极其缓慢（仅按  $ 1/f $  衰减）。</li></ul><p><strong>这里的伏笔在于：</strong></p><p>OFDM 利用正交性，巧妙地让每一个子载波的​<strong>峰值</strong>​，精准地踩在其他所有子载波的<strong>零点 (Zero Crossing)</strong> 上。</p><p>这像是一场千万人的走钢丝表演，只要大家都不动，这个平衡就是完美的。</p><h4>02. 多普勒效应：不仅仅是“平移”</h4><p>当你在 350km/h 的高铁上，或者在 7.6km/s 的卫星下，物理世界开始对这个脆弱的数学平衡下手了。</p><p>大家通常认为多普勒只是​<strong>频率平移</strong>​（$\Delta f$）。</p><p>但在 OFDM 的眼里，这简直就是一场 <strong>“旁瓣的屠杀”</strong> 。</p><p>设想一下，当整个频谱发生微小的偏移（哪怕只是子载波间隔的 ​<strong>5%</strong> ​）：</p><ol><li><strong>零点错位：</strong> 接收机做 FFT 采样时，原本应该采到“0”的地方，现在采到了隔壁子载波的​<strong>旁瓣能量</strong>​。</li><li><strong>全员恶人：</strong> 注意，这不仅是邻居的干扰。由于 Sinc 函数的旁瓣拖得很长，<strong>远处的子载波</strong>也会把能量“泼”过来。</li><li><strong>累积效应：</strong> 成千上万个子载波的干扰叠加在一起，这就形成了恐怖的 ​<strong>ICI（载波间干扰）</strong> ​。</li></ol><p>此时，你的星座图（Constellation）不再是清晰的点，而是变成了一团模糊的云。</p><h4>03. 致命的连锁反应：高阶调制的崩塌</h4><p>为什么这一点点干扰是致命的？</p><p>我们来看一个残酷的工程现实。</p><p>在 5G/6G 时代，为了追求高网速，我们大量使用 ​<strong>高阶调制</strong>​（如 64QAM, 256QAM）。</p><ul><li>​<strong>QPSK</strong>​：像四个大胖子坐沙发，抗干扰能力强，稍微挤挤没事。</li><li>​<strong>256QAM</strong>​：像 256 个瘦子挤在一张纸上。它们对“纯净度”的要求是变态的——通常需要 ​<strong>30dB 以上的信噪比 (SNR)</strong>​。</li></ul><p><strong>ICI 带来的灾难性后果是：</strong></p><p>它在信号内部制造了一个 <strong>“底噪”</strong> 。</p><p>假设多普勒频移导致 ICI 产生的干扰噪声大约在 -20dB 水平。</p><p>这意味着，无论你的基站发射功率多大，你的接收端 <strong>信干噪比 (SINR)</strong> 永远超不过 20dB。</p><p><strong>结局：</strong></p><p>256QAM 解调失败 $\rightarrow$ 退回 64QAM $\rightarrow$ 依然误码 $\rightarrow$ 退回 QPSK。</p><p><strong>网速瞬间从“千兆级”掉回“3G 时代”。</strong></p><p>这就是为什么在高铁上刷视频，明明信号满格，但视频就是卡住不动——<strong>因为调制阶数已经跌到了谷底。</strong></p><h4>04. 传统算法的无力回天：CFO 补偿的局限</h4><p>你可能会问：<em>“我们不是有 CFO（载波频偏）补偿算法吗？把它纠正回来不就行了？”</em></p><p>这在​<strong>单径信道</strong>​（比如外太空视距通信）里也许行得通。</p><p>但在地球表面，问题复杂得多：<strong>多径效应 (Multipath)。</strong></p><ul><li>信号从直射路径过来，频移是 $+1000$ Hz。</li><li>信号撞到大楼反射过来，频移可能变成了 $+500$ Hz。</li><li>信号从身后反射过来，频移可能是 $-200$ Hz。</li></ul><p><strong>这是 OFDM 的死穴：</strong></p><p>你在接收端补偿了 $+1000$ Hz，那个 $+500$ Hz 的信号就变成了 $-500$ Hz 的干扰。</p><p><strong>你按下葫芦浮起瓢。</strong> 在 OFDM 的刚性框架下，你永远无法同时让所有路径的信号都回归正交。</p><h4>05. 结语：必须推倒重来</h4><p>至此，结论已经很悲凉了。</p><p>OFDM 这个统治了 WiFi、4G、5G 二十年的王者，它的基因（Sinc 函数、正交性依赖）决定了它属于“静止或低速”的时代。</p><p>面对 6G 想要征服的 <strong>高超音速</strong> 和 ​<strong>低轨卫星</strong>​，修修补补已经无济于事。</p><p>我们需要一把新的手术刀——一把能在 <strong>“时延-多普勒”</strong> 域上重构信号的手术刀。</p><p>而这，正是 <strong>OTFS</strong> 和 <strong>AFDM</strong> 登场的时刻。</p><blockquote><p>“欢迎关注公众号 <strong>3GPP仿真实验室</strong>！这里是通信算法工程师的加油站。</p><p>我们不搬运新闻，只输出<strong>可运行的代码</strong>和<strong>深度标准解读</strong>。</p><p>👇 <strong>新人见面礼（后台回复关键词获取）：</strong></p><p>回复【LDPC】：获取 5G NR LDPC 编解码 MATLAB 代码（含注释）。<br/>回复【工具】：通信人减负神器：5G NR 帧结构与频点一键生成器（Python+Excel+Web三版）。<br/>回复【Pytorch】：获取 5G NR OFDM 链路 Pytorch 教学代码（含注释），助力人工智能 + 通信</p><p>让我们一起探索 6G 的无限可能。</p></blockquote>]]></description></item><item>    <title><![CDATA[本土vs国际：制造业CRM选型对决，5款产品助力企业精准决策 玩滑板的饺子 ]]></title>    <link>https://segmentfault.com/a/1190000047597830</link>    <guid>https://segmentfault.com/a/1190000047597830</guid>    <pubDate>2026-02-06 20:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业数字化转型的浪潮中，客户关系管理（CRM）系统已成为提升运营效率、优化客户体验、驱动业务增长的核心工具。然而，面对市场上琳琅满目的CRM产品，制造企业往往陷入选择困境：是选用通用型CRM，还是深耕行业的垂直解决方案？</p><p>本文将基于当前市场情况，深入剖析五款制造业垂直领域CRM，分析其核心优势与潜在风险，并为您的选型决策提供可靠建议。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597832" alt="2.jpg" title="2.jpg"/></p><h2>一、制造业CRM选型的特殊考量</h2><p>与零售、金融等行业不同，制造业的CRM需求具有显著特点：</p><ul><li><strong>复杂客户结构</strong>：涉及经销商、终端客户、供应商等多层关系</li><li><strong>销售过程长</strong>：从询价、报价、订单到售后，周期漫长且环节复杂</li><li><strong>产品定制化强</strong>：需要管理产品配置、BOM关联和定制需求</li><li><strong>服务需求高</strong>：设备维护、保修管理、备件配送等售后服务至关重要</li><li><strong>数据集成难</strong>：需与ERP、MES、SCM等生产管理系统深度集成</li></ul><p>这些特点决定了制造业需要的不只是客户信息管理工具，而是能够融入业务流程、连接前后端的行业化解决方案。</p><h2>二、五款制造业垂直领域CRM深度解析</h2><h3>1. 八骏科技CRM（制造业专项版）</h3><p><strong>简介</strong>：  <br/>杭州八骏科技有限公司专注于为制造业提供CRM解决方案，其产品围绕制造业销售与服务场景深度定制，在国内制造业中拥有相当规模的客户基础。</p><p><strong>核心优势</strong>：</p><ul><li><strong>行业贴合度高</strong>：内置制造业特有的项目管理、投标管理、合同履约跟踪模块</li><li><strong>强大的售后服务</strong>：集成设备台账、保修管理、预防性维护、备件库存联动</li><li><strong>可视化流程引擎</strong>：支持复杂审批流程和个性化销售阶段配置</li><li><strong>移动端适配良好</strong>：现场服务人员可实时更新服务状态、上传现场照片</li><li><strong>本土化服务</strong>：提供符合国内商务习惯的发票管理、催款提醒等功能</li></ul><p><strong>风险提示</strong>：</p><ul><li>国际品牌知名度相对较低，跨国企业可能顾虑其全球部署能力</li><li>没有SaaS版，有一定使用门槛</li></ul><h3>2. Salesforce Manufacturing Cloud</h3><p><strong>简介</strong>：  <br/>作为全球CRM领导者，Salesforce针对制造业推出的垂直解决方案，整合了其强大的平台能力和丰富的生态系统。</p><p><strong>核心优势</strong>：</p><ul><li><strong>完整的平台生态</strong>：与ERP、供应链系统的预集成连接器丰富</li><li><strong>AI预测能力</strong>：Einstein AI提供销售预测、客户流失预警等智能洞察</li><li><strong>全球部署能力</strong>：满足跨国制造企业的多地区、多语言、多币种需求</li><li><strong>灵活的扩展性</strong>：低代码平台支持企业根据独特流程自定义应用</li><li><strong>强大的合作伙伴网络</strong>：实施和定制资源丰富</li></ul><p><strong>风险提示</strong>：</p><ul><li>总体拥有成本高，许可费用和定制开发投入较大</li><li>对国内制造业特殊需求（如特殊发票流程）支持需要额外定制</li><li>数据存储在海外云端可能引发部分敏感行业企业的合规担忧</li></ul><h3>3. Microsoft Dynamics 365 for Manufacturing</h3><p><strong>简介</strong>：  <br/>微软将CRM与ERP能力融合的行业解决方案，特别适合已使用微软技术栈的制造企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>与Office 365深度整合</strong>：与Teams、Outlook、Excel无缝协作</li><li><strong>CRM与ERP的无缝衔接</strong>：同一平台下销售、财务、生产数据自然流动</li><li><strong>混合部署灵活性</strong>：支持云端、本地或混合部署，满足不同合规需求</li><li><strong>Power Platform支持</strong>：企业可自主创建扩展应用，降低长期定制成本</li><li><strong>物联网集成能力</strong>：通过Azure IoT Hub连接设备数据，实现预测性维护</li></ul><p><strong>风险提示</strong>：</p><ul><li>完整功能实现需要较复杂的配置和专业实施</li><li>制造业特定功能不如专注该领域的厂商深入</li><li>移动端体验在不同设备上表现不一致</li></ul><h3>4. SAP CRM for Manufacturing</h3><p><strong>简介</strong>：  <br/>SAP作为制造业ERP的巨头，其CRM解决方案天然适合已使用SAP ERP系统的制造企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>与SAP ERP无缝集成</strong>：销售订单直接生成生产计划，服务请求联动物料需求</li><li><strong>行业最佳实践</strong>：凝聚全球领先制造企业的业务流程经验</li><li><strong>端到端流程覆盖</strong>：从潜在客户到现金回收的全流程管理</li><li><strong>强大的分析能力</strong>：基于SAP Analytics Cloud的深度业务洞察</li><li><strong>全球合规支持</strong>：满足不同国家的税务、数据保护等法规要求</li></ul><p><strong>风险提示</strong>：</p><ul><li>实施周期长、成本高，更适合中大型企业</li><li>系统复杂性高，需要专业团队维护和优化</li><li>用户界面操作相对繁琐，培训成本较高</li></ul><h3>5. 用友制造CRM</h3><p><strong>简介</strong>：  <br/>用友作为国内企业服务龙头，其制造CRM解决方案深度结合国内制造业特点，尤其适合国有企业和大型民营企业。</p><p><strong>核心优势</strong>：</p><ul><li><strong>本土化程度极高</strong>：完全符合国内财务、税务、商务管理习惯</li><li><strong>与用友ERP无缝对接</strong>：国内用友ERP用户可实现平滑集成</li><li><strong>价格优势明显</strong>：相比国际品牌，总体拥有成本更低</li><li><strong>服务网络广泛</strong>：全国范围内的实施和支持团队</li><li><strong>政府与国企案例丰富</strong>：对特定行业需求理解深刻</li></ul><p><strong>风险提示</strong>：</p><ul><li>国际业务支持能力有限</li><li>产品创新速度相对较慢</li><li>开源性和第三方集成灵活性一般</li></ul><h2>三、制造业CRM选型的五个关键维度</h2><p>基于以上分析，我们建议制造企业从以下五个维度评估CRM厂商：</p><h3>1. 行业匹配度</h3><ul><li>考察产品是否包含制造业特有功能：项目管理、投标管理、设备服务、备件管理、质量投诉处理等</li><li>验证是否有与您企业规模、产品类型（离散制造/流程制造）相似的客户案例</li></ul><h3>2. 系统集成能力</h3><ul><li>评估与现有系统（ERP、MES、PLM等）的集成方式和成本</li><li>考察API开放程度和数据同步实时性</li><li>特别关注服务数据与生产数据的联动能力</li></ul><h3>3. 总拥有成本(TCO)</h3><ul><li>不仅考虑软件许可费用，还要计算实施、定制、培训、维护和升级的长期成本</li><li>评估云服务与本地部署的成本差异及安全合规影响</li></ul><h3>4. 扩展与适应性</h3><ul><li>考察系统能否适应企业业务增长和模式变化</li><li>评估自定义工作流、字段、报表的灵活程度</li><li>考虑移动办公和现场服务的支持能力</li></ul><h3>5. 供应商生态与可持续性</h3><ul><li>评估厂商的行业专注度和财务稳定性</li><li>考察实施伙伴的专业能力和行业经验</li><li>了解产品更新路线图与技术支持响应水平</li></ul><h2>四、给制造业企业的选型建议</h2><p><strong>第一步：明确核心需求优先级</strong>  <br/>制造企业应首先梳理自身最迫切的3-5个业务痛点，例如：</p><ul><li>需要管理复杂项目销售流程？</li><li>亟需改善售后服务响应速度？</li><li>希望销售预测更准确以指导生产计划？</li><li>需要打通销售与生产数据孤岛？</li></ul><p><strong>第二步：划分预算与部署时间线</strong>  <br/>根据企业规模和数字化阶段确定合理预算，并设定可行的上线时间表。中小制造企业可考虑八骏科技、用友等国内厂商的标准化方案；大型集团则需评估SAP、Salesforce等平台的综合能力。</p><p><strong>第三步：要求针对性产品演示</strong>  <br/>不要满足于通用功能展示，要求厂商基于您的实际业务流程进行演示，特别是：</p><ul><li>从询价到订单的完整流程</li><li>设备安装到维护服务的闭环管理</li><li>与现有系统的数据交换场景</li></ul><p><strong>第四步：深入考察客户案例</strong>  <br/>不仅要看成功案例，更要了解类似企业在实施中遇到的挑战及解决方案。如果可能，拜访已上线企业的实际使用部门。</p><p><strong>第五步：从试点开始，分阶段推广</strong>  <br/>选择1-2个业务单元或区域先行试点，验证系统实际效果后再全面推广，降低实施风险。</p><p><strong>第六步：规划长期合作关系</strong>  <br/>CRM实施不是一次性项目，而是持续优化过程。选择那些愿意与企业共同成长、提供持续优化服务的厂商。</p><h2>五、未来趋势：制造业CRM的演进方向</h2><p>随着工业4.0和智能制造的发展，制造业CRM正呈现以下趋势：</p><ol><li><strong>IoT深度集成</strong>：设备运行数据自动触发服务任务，实现预测性维护</li><li><strong>AI驱动洞察</strong>：基于历史数据预测客户需求、设备故障和销售机会</li><li><strong>增强现实(AR)支持</strong>：现场技术人员通过AR眼镜获取设备信息和维修指导</li><li><strong>区块链应用</strong>：供应链可追溯性和智能合约自动化执行</li><li><strong>低代码/无代码配置</strong>：业务人员可自行调整流程，减少IT依赖</li></ol><h2>结语</h2><p>选择制造业CRM是一场战略决策，而非单纯的技术采购。最适合的CRM系统应当像精密的制造设备一样，精准适配您的业务流程，无缝融入您的运营体系，并具备随着业务演进而升级的灵活性。</p><p>国内制造企业特别是中小企业，可重点关注如八骏科技这样深耕行业的本土厂商，它们在性价比、本土化适配和快速响应方面具有独特优势；而跨国企业或大型集团则需综合评估全球部署能力与本地合规要求，在SAP、Salesforce等国际平台与国内解决方案间找到平衡点。</p><p>无论选择哪家厂商，记住：成功的CRM实施=适合的产品+专业的实施+持续的优化+用户的接纳。从这个等式出发，结合企业实际情况，您一定能找到推动制造业数字化转型的那把钥匙。</p>]]></description></item><item>    <title><![CDATA[PhpStudy2018怎么用？完整安装与使用指南（新手必看） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047597848</link>    <guid>https://segmentfault.com/a/1190000047597848</guid>    <pubDate>2026-02-06 20:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​<br/><code>PhpStudy2018</code>是个<strong>集成环境包</strong>，把 Apache、MySQL、PHP 这些搞网站开发的东西打包在一起，装完就能在本机跑 PHP 网站，省得一个个单独装。</p><p>很多学 PHP 或者本地调试网站的人都用它，安装不难，下面用大白话一步步说。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=uxTRyC0bwHktpGkqJzexIQ%3D%3D.%2FfB31Se9Je8XuJNPW97qtuKmByLEOhZRHPCkDj0zqNCEDT20%2FR4pD5r%2BrLezRow2" rel="nofollow" title="https://pan.quark.cn/s/271a927c11d4" target="_blank">https://pan.quark.cn/s/271a927c11d4</a>​</li></ul></li><li><p><strong>用管理员身份运行（推荐）</strong> ​</p><ul><li>右键 <code>PhpStudy2018.exe</code>→ 选“以管理员身份运行”，避免端口占用或权限问题。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>PhpStudy2018.exe</code>运行（如果右键过了就直接双击）。</li><li>第一次打开可能会弹出“用户账户控制”提示 → 点  <strong>“是”</strong> 。</li><li>进入安装向导，选语言（一般默认中文）→ 点  <strong>“下一步”</strong> 。</li><li><p>选安装位置：</p><ul><li>默认是 <code>C:\phpStudy</code>，可点“浏览”改到其他盘，然后点  <strong>“下一步”</strong> 。</li></ul></li><li><p>附加任务：</p><ul><li>可勾“创建桌面快捷方式”，方便以后打开。</li></ul></li><li>点  <strong>“安装”</strong> ​ 开始安装，等进度条走完（几十秒）。</li><li>安装完会提示是否立即启动 → 勾上“立即启动 PhpStudy” → 点  <strong>“完成”</strong> 。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>打开 PhpStudy 后，界面左边是服务开关（Apache、MySQL 等），右边是版本切换。</li><li><p><strong>启动服务</strong>：</p><ul><li>点“启动”按钮，Apache 和 MySQL 同时跑起来（绿灯表示正常）。</li><li>如果端口被占用，会提示，需要改端口或关掉占用的程序。</li></ul></li><li><p><strong>访问本地站点</strong>：</p><ul><li>浏览器输入 <code>http://localhost</code>或 <code>http://127.0.0.1</code>，能看到 PhpStudy 欢迎页就说明 OK。</li></ul></li><li><p><strong>放网站文件</strong>：</p><ul><li>默认网站根目录在 <code>C:\phpStudy\WWW</code>，把自己的 PHP 文件丢进去就能访问。</li></ul></li><li><p><strong>切换 PHP 版本</strong>：</p><ul><li>在右侧版本列表里选需要的 PHP 版本，点“切换”，重启服务生效。</li></ul></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[全模态、多引擎、一体化，阿里云DLF3.0构建Data+AI驱动的智能湖仓平台 阿里云大数据AI ]]></title>    <link>https://segmentfault.com/a/1190000047597863</link>    <guid>https://segmentfault.com/a/1190000047597863</guid>    <pubDate>2026-02-06 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>▌导读</h2><p>在AI时代,数据工程师和AI工程师的协作变得前所未有的重要。阿里云DLF产品负责人李鲁兵在本次分享中,详细介绍了全模态湖仓DLF 3.0的完整能力体系。这个平台不仅支持传统的结构化数据处理,更在业界首次实现了结构化、半结构化、非结构化数据的统一管理和处理,为Data+AI一体化提供了端到端的解决方案。</p><h2>▌全模态平台的核心理念:数据统一、计算按需、工作流驱动</h2><p>全模态湖仓管理平台的设计理念可以概括为四个关键词:数据统一、计算按需、工作流驱动、多方协作。在AI时代,数据工程师负责数据准备和预处理,AI工程师专注于模型训练、推理和召回,两个角色需要在统一的平台上无缝协作,这对平台能力提出了更高的要求。</p><p>数据统一是基础。传统的数据平台往往将结构化数据、半结构化数据、非结构化数据分别存储在不同系统中,造成数据孤岛和管理复杂度的急剧上升。DLF 3.0通过统一的Omni Catalog,实现了对Paimon、Iceberg、Lance、Format Table、Object Table等多种格式的统一管理。无论是传统的数据库表,还是文本、音频、图片、视频等多模态数据,都可以在同一套元数据目录中进行管理,Data工程师和AI工程师可以基于同一份数据进行协作。</p><p>计算按需是核心。不同的应用场景需要不同的计算引擎。实时分析需要Flink和Hologres,离线分析依赖Spark和MaxCompute,全模态检索要用到Milvus和Elasticsearch/OpenSearch,模型训练则需要PAI和Ray。DLF 3.0支持所有这些计算引擎在统一的湖仓之上按需调用,一份数据,多个引擎共享,避免了数据的重复存储和迁移。</p><p>工作流驱动是保障。数据处理和AI应用往往涉及多个步骤:数据摄取、预处理、特征工程、模型训练、推理、召回等。DLF 3.0提供了完整的工作流编排能力,数据工程师和AI工程师可以通过工作流将各个环节串联起来,实现端到端的自动化流程。</p><p>多方协作是目标。通过统一的IDE/Notebook开发环境、Copilot代码辅助、自然语言分析、Agent智能助手等能力,DLF 3.0降低了开发门槛,提升了协作效率。数据工程师和AI工程师可以在同一个平台上使用各自熟悉的工具,同时又能无缝共享数据和成果。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxq" alt="" title=""/></p><h2>▌产品方案大图:从结构化到全模态的能力升级</h2><p>DLF 3.0的产品方案大图清晰地展示了从结构化数据处理到全模态处理的能力演进。在传统的数据处理链路上,我们有CDC/Batch Ingestion(数据摄取)、Stream/Batch ETL(数据加工)、OLAP Analytic(分析查询)三个主要环节,对应的计算引擎包括Flink、Spark、Hologres、StarRocks、MaxCompute等。</p><p>在全模态处理链路上,新增了四个关键环节:数据集管理、数据预处理、数据训练、数据推理、数据检索召回。这些环节分别对应不同的计算引擎:数据预处理可以使用MaxFrame和PySpark,模型训练依赖PAI和Ray,数据检索召回则需要Milvus和Elasticsearch/OpenSearch等搜索引擎。</p><p>两条链路并非孤立存在,而是通过统一的Omni Catalog和DLF元数据服务实现了深度融合。底层的Lakehouse Managed Storage Service提供了统一的存储层,支持Virtual File System、生命周期管理、冷热分层、存储优化等能力。数据缓存加速服务CPFS进一步提升了GPU/CPU的数据读取效率,为AI训练和推理提供了高性能保障。</p><p>DLF作为统一的湖仓管理平台,提供了Rest Catalog Service、Lakehouse SDK(Paimon、Iceberg)、File SDK、权限管理、血缘追踪、监控日志等完整的企业级能力。这种架构设计使得同一份数据可以同时服务于传统的数据分析场景和新兴的AI应用场景,真正实现了Data+AI一体化。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxr" alt="" title="" loading="lazy"/></p><h2>▌Omni Catalog:全模态管理的统一目录</h2><p>在全模态时代,元数据管理面临前所未有的挑战。传统的Catalog只需要管理Tables、Views、Functions等结构化对象,而在AI场景下,还需要管理各种文件(Files)、向量索引、Blob数据等非结构化对象。如果不同类型的数据使用不同的Catalog或System进行管理,就会产生新的数据孤岛,计算引擎需要跨不同的Catalog进行处理,大幅增加了复杂度。</p><p>DLF 3.0推出的Omni Catalog正是为了解决这一问题。它通过一套统一的元数据目录,同时管理Tables和Files,支持Paimon、Iceberg、Lance、Format Table(Parquet、ORC、Avro)、Object Table(Files)等多种格式。计算引擎无论是传统的大数据引擎(Flink、Spark、Hologres)还是新型的AI框架(Ray、PyTorch),都可以通过统一的Rest API、Open API、Paimon SDK、Iceberg SDK、VFS SDK进行数据访问。</p><p>Omni Catalog的核心优势在于降低了数据孤岛的风险。通过统一的目录,数据治理、权限控制、血缘追踪等能力可以覆盖所有类型的数据,而不需要在不同系统间切换。这对于企业级应用尤为重要,因为数据合规、安全审计等需求必须覆盖全域数据,而不能有盲区。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxs" alt="" title="" loading="lazy"/></p><h2>▌DLF提供商业化Paimon全模态存储:统一管理异构数据</h2><p>Paimon作为DLF 3.0的核心表格式,在全模态存储方面进行了深度创新。全模态存储面临三大核心需求:统一管理异构数据的能力、支持结构化和多模态数据的顺序访问(用于大规模批式推理)、提供高性能的标签和向量检索(支持随机访问)。</p><p>Paimon通过Row ID机制实现了对不同列、不同格式数据的统一管理。每一行数据都有一个全局唯一的Row ID,通过Row ID可以关联该行在不同文件格式中的存储位置。对于结构化数据,Paimon使用Parquet Files存储,对于向量数据,可以使用Lance Files或Faiss Vector索引,对于大型Blob数据(图片、音频、视频),则使用Paimon Blob格式。</p><p>在索引构建方面,Paimon提供了多种索引类型。Btree和Bitmap索引用于快速的标量查询,Invert倒排索引支持全文检索,Vector Index则提供高效的向量相似度搜索。这些索引通过Index Manifests进行统一管理,建立了字段与Row ID之间的映射关系。</p><p>在数据访问方面,Paimon通过Data Manifests管理文件组,支持Row Ranges范围扫描。对于顺序访问场景(如模型训练),Paimon可以将多条数据打包成Virtual File Group,提供高吞吐的批量读取。对于随机访问场景(如实时检索),Paimon通过全局索引实现了毫秒级的点查性能。</p><p>通过这种灵活的File Formats组合和统一的Table Format封装,Paimon实现了在一张宽表上承载结构化、半结构化、非结构化所有类型数据的目标,为全模态应用提供了坚实的存储基础。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxt" alt="" title="" loading="lazy"/></p><h2>▌DLF湖表管理与优化:智能化提升性能降低成本</h2><p>DLF 3.0提供了完整的湖表管理和优化能力,通过智能化的方式提升读写性能、降低存储成本。整个优化体系包括四大核心能力:自适应分桶、智能Compaction、快照管理清理、存储服务与冷热分层。</p><p>自适应分桶是一项创新性功能。传统的分桶策略需要用户在建表时指定分桶数,但随着数据量的变化,固定的分桶数可能导致性能问题。DLF 3.0支持根据数据量自适应地调整分桶数(Rescale),用户只需指定分桶Key,平台会自动维护最优的分桶配置,大幅降低了管理负担。</p><p>智能Compaction是性能优化的关键。随着数据的不断写入,湖表会产生大量小文件,影响读取性能。DLF 3.0提供了多种Compaction策略:动态资源模式支持延时优先、资源优先、均衡模式三种策略,平台会根据当前资源状况自动调整Compaction节奏;固定资源模式则允许用户自定义资源配置和参数,实现精细化控制。对于全模态数据,DLF 3.0还支持针对不同文件类型(结构化、半结构化、非结构化)采用不同的Compaction策略,保证整体效率。</p><p>快照管理和清理功能帮助用户有效管理数据生命周期。用户可以基于分区或快照设置自动清理策略,平台还会自动扫描Orphan Files(孤儿文件)并清理,避免存储空间的浪费。同时支持手动触发管理操作,满足特殊场景需求。</p><p>整个优化流程由DLF元数据服务、Event Store事件存储、Paimon存储优化服务协同完成。作业生成引擎、规则优化引擎、智能优化引擎共同组成了智能决策层,作业调度管理则负责在多个计算资源池上高效执行Compaction任务。这种架构设计实现了从元数据到数据文件的全链路优化,用户无需关心底层细节,即可享受高性能和低成本的双重优势。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxu" alt="" title="" loading="lazy"/></p><h2>▌智能冷热分层:大幅降低存储成本</h2><p>存储成本是企业在构建数据湖时的重要考量因素。DLF 3.0提供了智能的冷热分层能力,可以根据数据访问模式自动将数据在不同存储类型间迁移,在保证性能的同时大幅降低成本。</p><p>DLF 3.0支持四种存储类型:标准存储、低频存储、归档存储、冷归档存储。平台会根据数据的最近访问时间和最近更新时间,自动决定数据应该存储在哪个层级。对于频繁访问的热数据,保持在标准存储以保证高性能;对于访问频率降低的温数据,迁移到低频存储节省成本;对于长期不访问的冷数据,则可以归档到成本更低的归档存储或冷归档存储。</p><p>智能加热是冷热分层的重要补充功能。当归档的数据再次被访问时,平台会自动将其加热到更高性能的存储层级。加热策略支持Partition(分区)和File(文件)两层管理,可以针对分区级别或文件级别的访问进行精细化控制。这种设计既保证了数据访问的性能,又最大化地利用了低成本存储,实现了性能与成本的最佳平衡。</p><p>通过智能冷热分层,企业可以在不牺牲数据可用性的前提下,将长期存储成本降低数倍。对于PB级甚至EB级的数据湖,这种成本优化能力可以为企业节省大量资金,使得海量数据的长期保存成为可能。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxv" alt="" title="" loading="lazy"/></p><h2>▌细粒度权限控制:企业级安全保障</h2><p>企业级数据平台必须具备完善的权限控制和安全审计能力。DLF 3.0提供了从用户管理、权限控制到审计治理的全链路安全保障体系。</p><p>在用户管理方面,DLF 3.0原生支持阿里云RAM用户体系,基于用户和角色进行权限管理。用户可以通过Open API和REST API进行编程式的权限配置,也可以通过控制台进行可视化管理。</p><p>权限控制方面,DLF 3.0支持对湖表设置ACL细粒度权限,可以精确到Catalog、Database、Table、Column(列)甚至Row(行)级别。列级权限允许用户只访问特定的列,行级权限则通过WHERE条件和AND、OR等逻辑运算符,实现对特定行范围的访问控制。列Masking功能可以对敏感字段进行脱敏处理,保护数据隐私。</p><p>Data Sharing能力支持跨主账号的数据协作。企业可以将特定的数据集授权给合作伙伴或其他部门,实现安全可控的数据共享。权限检索功能帮助管理员快速了解数据的授权情况,权限委托和授权管理则提供了灵活的权限分级体系。</p><p>审计和治理方面,DLF 3.0全面记录所有操作日志,满足生产环境的合规要求。审计管理功能支持漏洞发现和安全治理,帮助企业及时发现和修复安全隐患。这套完整的安全体系,使得DLF 3.0可以满足金融、医疗、政务等高安全要求行业的需求。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxw" alt="" title="" loading="lazy"/></p><h2>▌实时湖仓与全模态处理:AI时代的两大刚需</h2><p>实时化和全模态化是AI时代数据平台的两大刚需。DLF 3.0在这两个方向上都实现了业界领先的能力。</p><p>在实时湖仓方面,DLF 3.0基于Paimon实现了三大核心能力:流式更新、流式订阅、实时查询。数据可以通过Flink等流计算引擎不断流式更新到湖仓中,支持大规模的增量更新和部分列更新。下游系统可以通过流式订阅的方式实时消费变更日志(Changelog),构建实时数据链路。查询层面,Paimon的数据可以被StarRocks、Hologres等OLAP引擎实时查询,延迟可以达到秒级甚至亚秒级。</p><p>与业界的Iceberg和Delta相比,Paimon在流式更新场景下具有明显优势。Iceberg和Delta主要面向日志场景,Compaction代价高、速度慢,难以支撑大规模流式增量更新。而中国市场的实时需求走在世界前列,Paimon正是为此而生,通过排序和文件组织优化,大幅降低了Compaction成本,实现了ODS、DWD、DWS全链路的实时更新。</p><p>在全模态处理方面,DLF 3.0提供了完整的多模态宽表能力。一张Paimon表可以同时存储id、url、vectors(向量)、labels(标签)、summary(文本摘要)、blobs(大型二进制对象)、meta(元数据)、json(半结构化数据)等多种类型的字段,避免了多表查询和治理负担。统一的存储底层使得数据工程师和AI工程师可以基于同一张表协作,通过高效的索引机制支持向量检索、全文检索、分析查询、模型推理、训练等多种应用场景。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxx" alt="" title="" loading="lazy"/><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxy" alt="" title="" loading="lazy"/><br/>DLF 3.0还对接了主流的大数据处理和AI预处理框架,包括Spark、Flink、Ray、PyTorch等,提供了PyPaimon等Python原生接口,使得AI工程师可以像操作本地文件一样便捷地访问湖上数据。相比业界的LanceDB等方案,DLF 3.0具有生态丰富、统一性强、工业级验证等优势,已经在阿里巴巴集团和众多外部客户中大规模落地。</p><h2>▌典型场景与客户案例:从理论到实践</h2><p>DLF 3.0已经在多个典型场景中得到了验证。在离线实时一体化湖仓场景中,通过Flink CDC实现数据库的实时摄取,支持Schema Evolution和整库同步,分钟级实时可查询。Flink在Paimon上进行流读流写,实现全链路实时湖仓,支持低成本的去重和部分列更新。Spark提供数仓级别的批处理性能,支持Filter/Min/Max/TopN/Limit等算子下推和Native计算加速。StarRocks和Hologres则通过Manifests缓存、删除向量、文件过滤等优化技术,实现了对Paimon湖表的极速查询,性能可以对标内表。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxz" alt="" title="" loading="lazy"/></p><p>在全模态数据管理和处理场景中,音频、文本、图片、视频等多模态数据通过统一的采集入湖,经过Spark或Ray进行预处理(如文本Chunking、Embedding等),将结构化标签、向量、Blob数据统一存储在一张Paimon表中。AI工程师可以通过Milvus或StarRocks进行向量检索和标量过滤,实现样本圈选和预览。PyPaimon直接对接Ray和PyTorch,支持数据加载和模型训练,整个流程端到端打通,数据无需跨系统迁移。</p><p>在客户案例方面,智能汽车向量湖是一个典型应用。自动驾驶场景产生海量的车载数据、地理信息、雷达数据、视频图片等多模态数据。通过DLF 3.0,这些数据统一采集并通过人工或机器打标生成Labels,经过预处理将图片、视频拆解为目标对象,再通过Embedding生成向量。Labels和向量数据构建成统一的向量湖,支持标量+向量混合检索召回,可以快速找到符合特定条件的数据样本,用于AI模型的迭代训练。整个方案实现了从数据采集到推理到检索的完整Pipeline,百亿级数据规模的混合检索性能表现优异。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxA" alt="" title="" loading="lazy"/></p><p>阿里巴巴集团内部的全模态湖也是重要实践。基于Paimon Blob字段,集团构建了EB级的多模态混合存储,支持视频、音频、图片等大型文件的高效管理。通过顺序读高吞吐的数据加载能力,GPU的数据利用率提升了10%,这对于大规模AI训练具有巨大的成本节省价值。这些真实案例充分验证了DLF 3.0全模态湖仓方案的技术先进性和商业价值。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnSxB" alt="" title="" loading="lazy"/></p><h2>▌产品商业化与生态建设</h2><p>DLF产品已于2025年正式商业化,现在提供免费试用机会。阿里云还建立了DLF钉钉交流群(群号:106575000021),欢迎用户加入进行技术交流和问题反馈。</p><p>全模态湖仓代表了大数据和AI结合发展的下一阶段重要方向。随着多模态AI应用的普及,企业对统一管理和处理异构数据的需求将越来越强烈。DLF 3.0通过开放的架构、强大的性能、完善的企业级能力,为客户提供了一个面向未来的数据平台选择。无论是传统的大数据分析场景,还是新兴的AI训练推理场景,DLF 3.0都可以提供端到端的支持,帮助企业在AI时代保持竞争力。</p>]]></description></item><item>    <title><![CDATA[Apache Flink Agents 0.2.0 发布公告 ApacheFlink ]]></title>    <link>https://segmentfault.com/a/1190000047597744</link>    <guid>https://segmentfault.com/a/1190000047597744</guid>    <pubDate>2026-02-06 19:06:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Apache Flink 社区很高兴地宣布 Apache Flink Agents 0.2.0 版本正式发布，您可以通过以下方式获取 Flink Agents 0.2.0：</p><ul><li><a href="https://link.segmentfault.com/?enc=8LqduFu91qGJI%2FPcQxzI%2FQ%3D%3D.KyJ1w4NNBvOruqHH7BjsZOKBShJ6UL8TY1la7G7v9fxUI0GY0rFYKRhvfoiqrwFxNoo4NbCu%2FUyJix9FVPsS7A%3D%3D" rel="nofollow" target="_blank">在此下载发布版本</a>。</li><li><a href="https://link.segmentfault.com/?enc=SRFTyRmOhdhQvm75bcrcdw%3D%3D.7yBjyUfKOMHzlwWEdXmxUs3dIpDYKdGvO98681VtSJEc%2Fq%2F9TSKCTQn1MNsjQtnPea%2F2TeqdSRv3nhPfXIUA0VsCtqWnnITjHlpBWCNtSng%3D" rel="nofollow" target="_blank">在此查看文档和快速入门示例</a>。</li></ul><p>请注意，Agents 0.2.0 是一个预览版本（Preview Version），这意味着：</p><ul><li>部分功能可能存在已知或未知的缺陷。您可以通过 <a href="https://link.segmentfault.com/?enc=k%2FvDWjx%2BkGn4oL5QzGPLig%3D%3D.kNsts%2Fy2A4Oi4JTCyLL4qEEEqPkX%2FqzAerwvD%2B2UQOA0M0QctrFbJ%2FA8uuxxFdE1" rel="nofollow" target="_blank">Github Issues</a> 查看已知问题列表及其解决状态。</li><li>当前的 API 和配置选项处于实验阶段，在未来版本中可能会发生不向后兼容的变更。</li><li>我们非常感谢您的任何反馈，无论是分享您的使用案例、建议新功能、帮助定位和修复漏洞，还是其他任何想法。您的见解对我们至关重要。</li></ul><p><strong>您可以通过以下方式联系我们：</strong></p><ul><li>加入 <a href="https://link.segmentfault.com/?enc=xwC0mMTz2qG%2ByKxt8XZ%2BEQ%3D%3D.9ym0Rm3%2BGX45KVpYpfn7gqyzY9J4olh4%2F8ZutwLLPqnmLW0wjCAGPFbnPkGTwUZZAoWaq%2B%2BjPKjOTm8L1Sgn%2FQ%3D%3D" rel="nofollow" target="_blank">Apache Flink Slack</a> 并在 <code>#flink-agents-user</code> 频道寻求帮助。</li><li>在 <a href="https://link.segmentfault.com/?enc=QK37DRLEaD0oDW7chYNqQg%3D%3D.bCCTuU9UDahKUYUl05FaEx1hHpqWu%2BIdonj%2FV3JyVvHTgxpBhN8EKRoprRynK7Cv" rel="nofollow" target="_blank">Github Issues</a> 提交功能需求和漏洞报告。</li><li>在 <a href="https://link.segmentfault.com/?enc=djlHV6p%2BIFTBLrRdSr02YA%3D%3D.tS16lub5lv%2FAzQ72zRL0x1O1xQ9Vk0GrbpLW8Ecrarp9zQx3iSZ24BlUys%2FW%2F%2B0HW%2FZSjmskcMef%2BUC%2F5XTzMw%3D%3D" rel="nofollow" target="_blank">Github Discussions</a> 分享您的使用案例和想法。</li></ul><hr/><h3>什么是 Apache Flink Agents？</h3><p>Apache Flink Agents 是 Apache Flink 的一个新子项目，直接在Apache Flink 的流式运行时（streaming runtime）上构建事件驱动的 AI 智能体（Event-driven AI Agents）。它将流处理与自主智能体统一在同一个框架中，将Apache  Flink 经受过实战检验的优势——大规模扩展性、低延迟、容错性和状态管理，与智能体的核心能力——大语言模型（LLMs）、工具、记忆和动态编排有机结合。</p><h3>为什么 Apache Flink Agents 至关重要？</h3><p>虽然 AI 智能体在chatbots和copilots等交互式应用中取得了飞速进展，但这些系统通常以同步、一次性交互的方式运行。然而，许多业务场景不能等待用户输入指令后才采取行动。在电子商务、金融、物联网和物流等场景中，必须在感知到实时事件（如支付失败、传感器异常或用户点击）时立即做出关键决策。</p><p>要在生产环境中取得成功，企业级Agents必须具备以下能力：</p><ol><li>处理实时、高吞吐的事件流，如交易流、传感器异常或用户行为轨迹。</li><li>持续且自主地运行，而不仅仅是在收到提示词（prompt）时才运行。</li><li>保证安全性、可审计性，并在发生故障时能够恢复。</li></ol><p>这些工作不仅需要智能，更需要大规模扩展能力、毫秒级延迟、容错性以及有状态的协调能力。而这些正是 Apache Flink 的核心强项。</p><p>此前，尚未有一个统一的框架能将Agentic AI 模式引入 Flink 成熟的流处理生态系统中。Apache Flink Agents 填补了这一空白，将Agents视为始终在线、可靠且可扩展的事件驱动微服务。</p><h3>核心特性</h3><p>Apache Flink是流计算领域的事实标准，Apache Flink Agents 继承了分布式、大规模、高可用的结构化数据处理和成熟的状态管理能力，并为Agentic AI 的构建和功能增加了自由的抽象，包括：大语言模型（LLMs）、提示词（prompts）、工具（tools）、记忆（memory）、动态编排、可观测性等。</p><p>Apache Flink Agents 的关键特性包括：</p><ul><li>大规模扩展与毫秒级延迟：利用 Flink 的分布式处理引擎，实时处理大规模事件流。</li><li>无缝的数据与 AI 集成：Agents直接与 Flink 的 DataStream 和 Table API 交互进行输入和输出，实现结构化数据处理与语义 AI 能力在 Flink 内部的平滑集成。</li><li>Exactly-Once 一致性：通过外置的 Action State Store 扩展 Flink 原本的 Checkpoint 机制，从而确保 Agent 中 Action 执行、模型推理、工具调用及其影响的精确一直一致性</li><li>成熟的Agent抽象：利用广为人知的 AI  Agent概念，使具有Agent系统开发经验的开发者能够快速上手并构建应用，无需陡峭的学习曲线。</li><li>多语言支持：提供 Python 和 Java 的原生 API，能够无缝集成到不同的开发环境中，允许团队使用其偏好的编程语言。</li><li>丰富的生态系统：原生支持对主流模型服务与向量存储的集成，，以及托管在 MCP 服务器上的工具或提示词，同时支持自定义扩展。</li><li>可观测性：采用以事件为中心的编排方法，所有智能体行为都由事件连接和控制，从而能够通过事件日志观察和理解智能体的行为。</li></ul><hr/><h3>0.2 版本有哪些新变化？</h3><h4>Java API 功能对齐</h4><p>在 Flink Agents 0.1 中，部分功能仅在 Python API 中可用。0.2 版本通过在 Java 中增加以下能力的完整支持，弥补了这一差距：</p><ul><li>嵌入模型（Embedding Models）</li><li>向量存储（Vector Stores）</li><li>MCP 服务器（MCP Server）</li><li>异步执行（Asynchronous Execution）</li></ul><p>至此，Java API 在功能上已与 Python API 完全对齐。</p><h4>扩展的生态集成</h4><p>Flink Agents 0.2 引入了对更广泛的模型服务和向量数据库的原生支持：</p><ul><li><p>对话模型（Chat Models）：</p><ul><li>Python API 现支持 Azure OpenAI。</li><li>Java API 增加了对 Azure AI、Anthropic 和 OpenAI 的支持。</li></ul></li><li><p>嵌入模型（Embedding Models）：</p><ul><li>Java API 现支持 Ollama。</li></ul></li><li><p>向量数据库（Vector Stores）：</p><ul><li>Java API 现支持 Elasticsearch。</li></ul></li></ul><p>此外，0.2 版本现支持跨语言资源访问。用户可以在一种语言编写的智能体中，调用另一种语言提供的集成支持。例如：在 Python 智能体中调用 Java 支持的 Azure AI 对话模型。  <br/><em>(注：跨语言资源访问目前不支持在异步执行代码块中使用。使用跨语言集成时，框架内置动作将回退到同步执行。)</em></p><h4>记忆系统重构</h4><p>Flink Agents 0.2 对其记忆管理系统进行了全面升级。相比之前仅支持短期记忆，新版本引入了三种不同的记忆类型：</p><ol><li>感官记忆（Sensory Memory）： 在单次智能体运行中维护状态并传递上下文。</li><li>短期记忆（Short-Term Memory）： 在多次智能体运行之间保留精确的上下文信息。</li><li>长期记忆（Long-Term Memory）： 实现大规模上下文信息的近似语义检索，并提供初步的信息摘要和压缩支持。</li></ol><h4>持久化执行（Durable Execution）</h4><p>Flink Agents 0.1 提供了Action级的精确一次执行。在 0.2 版本中，这一能力被精细化到了更小的颗粒度。你现在可以在一个Action内指定特定的代码块进行持久化执行。在故障恢复时，即使整个Action尚未完成，任何已成功执行的持久化代码块都不会重新运行。  <br/>这有助于避免：</p><ul><li>冗余的模型调用（节省时间、Token 并减少不可预测性）。</li><li>重复工具调用产生的副作用（例如：重复付款或重复发送电子邮件通知）。</li></ul><h4>多版本 Flink 兼容性</h4><p>Flink Agents 0.1 仅兼容 Apache Flink 1.20.3。  <br/>Flink Agents 0.2 现支持更广泛的 Flink 版本：1.20, 2.0, 2.1 和 2.2。  <br/><em>(注：建议始终使用所选 Flink_ 小版本（x.y）_的最新_补丁版本（x.y.z）_，以获得更多已知问题的修复。)</em></p><hr/><h3>破坏性变更（Breaking Changes）</h3><h4>Python API</h4><ul><li>创建 <code>ResourceDescriptor</code> 的 API 已更改。在之前版本中，用户通过 <code>clazz=Type[Resource]</code> 指定资源提供者；在 0.2 版本中，应通过 <code>clazz=ResourceName</code> 指定，我们为内置集成提供了常量字符串。</li><li><code>RunnerContext.execute_async</code> 方法已更名为 <code>durable_execute_async</code>。</li><li><code>MCPTool</code>、<code>MCPPrompt</code> 和 <code>MCPServer</code> 不再被视为 API，已从 <code>api</code> 模块中移出。</li></ul><h4>配置</h4><ul><li><code>ERROR_HANDLING_STRATEGY</code> 现在不仅影响 ReAct Agent，而是影响所有智能体。它已从 <code>ReActAgentConfigOptions</code> 移至 <code>AgentExecutionOptions</code>。</li></ul><h4>Java Ollama 对话模型</h4><ul><li>对话模型设置中的 <code>extract_reasoning</code> 参数类型从 <code>string</code> 更改为 <code>boolean</code>，默认值从 <code>false</code> 更改为 <code>true</code>。</li><li>引入了新参数 <code>think</code> 用于控制是否启用思考模式。<code>extract_reasoning</code> 不再影响此行为。</li></ul><hr/><h3>贡献者名单</h3><p>Apache Flink 社区感谢以下每一位为本次发布努力的贡献者：</p><p>Alan Z., Eugene, Ioannis Stavrakantonakis, Liu Jiangang, Marcelo Colomer, Shekharrajak, Weiqing Yang, Wenjin Xie, Xiang Li, Xintong Song, Xuannan, Yash Anand, chouc, dependabot[bot], tsaiggo, twosom</p><hr/><p>阿里云的 Flink Agents 团队正在北京、上海招聘！如果你对实时计算、AI 数据基础设施充满热情，欢迎加入我们，点击链接或直接邮箱投递！</p><p>了解详情：<a href="https://link.segmentfault.com/?enc=NU5%2FXMWkV6DU83Ma%2BisLMA%3D%3D.PiHoiSFobMFXob3x3l6WxTFEHpjWYN5ZnsRCo9ApicZO1bnf9i%2F8dl%2BwKTKjxhpQ7w1kOfbFL4hI3V8Po58GPdlAqibEvOFV8bHyGhT%2Bk4yd0y9Eo6c0RZcaAtjpVaq6Wi1696k2%2FKCL3rTecuh43Q%3D%3D" rel="nofollow" target="_blank">https://careers.aliyun.com/off-campus/position-detail?lang=zh...</a></p><p>邮箱：<a href="mailto:xintong.sxt@alibaba-inc.com" target="_blank">xintong.sxt@alibaba-inc.com</a></p>]]></description></item><item>    <title><![CDATA[使用 Python 导出 Word 表格为 Excel 工作表 大丸子 ]]></title>    <link>https://segmentfault.com/a/1190000047597747</link>    <guid>https://segmentfault.com/a/1190000047597747</guid>    <pubDate>2026-02-06 19:05:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在实际办公和开发场景中，我们经常会遇到这样的需求：<strong>Word 文档中包含大量结构化表格数据，而最终需要将这些数据统一整理到 Excel 中进行统计、分析或二次处理</strong>。手动复制粘贴不仅效率低，而且在遇到复杂表格（如单元格内多段文本、多表格文档）时，格式也很容易被破坏。</p><p>借助 Python 脚本我们可以<strong>自动化提取 Word 文档中的所有表格，并将每个表格完整写入 Excel 的独立工作表中</strong>，在保证数据结构清晰的同时，大幅提升处理效率。</p><p>本文将详细介绍一种完整、可复用的实现方案，并对关键代码逻辑进行说明，适用于批量表格转换与自动化办公场景。</p><hr/><p>本文所使用的方法需要用到 <a href="https://link.segmentfault.com/?enc=qxctRu2CAaktSkT6dlKbjQ%3D%3D.095KJbtEs3OS9KVQKuFX10mNK0aLce1g9YJQXpS20O%2F%2BroKAG2O6hQqhKh8MHJgejQzBSxJaZweCmsJT96P%2FAQ%3D%3D" rel="nofollow" target="_blank">Free Spire.Doc for Python</a> 和 <a href="https://link.segmentfault.com/?enc=5u64moeBAJRidZ%2Bji%2BINeQ%3D%3D.z4xHbxJupyMZNoSEVYcGnrOeS5TQiZ%2FkcJWv7vmtZ9L7Vu7pkzCWmStZGLSUGNsDbS76e34hRNyrK2LILgyYog%3D%3D" rel="nofollow" target="_blank">Free Spire.XLS for Python</a>，分别用于提取 Word 表格数据和写入 Excel 文件。可通过 pip 安装：<code>pip install spire.doc.free spire.xls.free</code>。</p><hr/><h2>一、实现思路概览</h2><p>整个转换流程可以拆分为两个清晰的阶段：</p><ol><li><p><strong>从 Word 文档中提取表格数据</strong></p><ul><li>遍历文档中的所有节（Section）</li><li>遍历每个节中的所有表格（Table）</li><li>逐行、逐单元格读取文本内容</li><li>保留单元格内的原有段落结构</li></ul></li><li><p><strong>将提取的数据写入 Excel 文件</strong></p><ul><li>为每个 Word 表格创建一个新的工作表</li><li>按行列顺序写入单元格内容</li><li>自动调整列宽，提升可读性</li></ul></li></ol><p>这种“先抽象为数据结构，再写入目标文件”的方式，逻辑清晰，也便于后续扩展（例如 CSV、数据库等）。</p><hr/><h2>二、使用 Python 提取 Word 中的表格数据</h2><p>下面的函数负责<strong>从 Word 文档中提取所有表格，并以嵌套列表的形式返回数据</strong>。</p><pre><code class="python">from spire.doc import *

def extract_tables_from_word(word_file_path):
    """
    从 Word 文档中提取所有表格数据。
    返回一个列表，其中：
    - 每个元素代表一个表格
    - 表格内部是“行”的列表
    - 每一行是“单元格内容”的列表
    """
    document = Document()
    document.LoadFromFile(word_file_path)

    all_tables_data = []

    # 遍历文档中的所有节
    for sec_index in range(document.Sections.Count):
        section = document.Sections.get_Item(sec_index)

        # 遍历节中的所有表格
        for table_index in range(section.Tables.Count):
            table = section.Tables.get_Item(table_index)
            current_table_data = []

            # 遍历表格中的所有行
            for row_index in range(table.Rows.Count):
                table_row = table.Rows.get_Item(row_index)
                current_row_data = []

                # 遍历行中的所有单元格
                for cell_index in range(table_row.Cells.Count):
                    table_cell = table_row.Cells.get_Item(cell_index)

                    # 提取单元格中的所有段落文本，保留换行结构
                    paras = [
                        table_cell.Paragraphs.get_Item(i).Text.rstrip('\r\n')
                        for i in range(table_cell.Paragraphs.Count)
                        if table_cell.Paragraphs.get_Item(i).Text.strip()
                    ]
                    current_cell_data = "\n".join(paras)
                    current_row_data.append(current_cell_data)

                current_table_data.append(current_row_data)

            all_tables_data.append(current_table_data)

    document.Close()
    return all_tables_data</code></pre><h3>关键说明</h3><ul><li><strong>Section → Table → Row → Cell</strong> 的层级结构，符合 Word 文档的真实组织方式</li><li><p>使用 <code>Paragraphs</code> 而不是直接读取 <code>Text</code>，可以：</p><ul><li>避免丢失单元格内的多段内容</li><li>保留原有换行结构，写入 Excel 后依然清晰</li></ul></li><li>最终返回的数据是一个<strong>三层嵌套列表</strong>，非常适合后续写入表格类文件</li></ul><hr/><h2>三、将提取的数据写入 Excel 文件</h2><p>在拿到结构化表格数据后，接下来使用 <strong>Spire.XLS for Python</strong> 将其写入 Excel。</p><pre><code class="python">from spire.xls import *

def write_data_to_excel(extracted_data, excel_file_path):
    """
    将提取的 Word 表格数据写入 Excel 文件。
    每个 Word 表格对应 Excel 中的一个工作表。
    """
    workbook = Workbook()
    # 清除默认工作表
    workbook.Worksheets.Clear()

    if not extracted_data:
        print("没有从 Word 文档中提取到任何表格数据。")
        return

    # 遍历所有表格数据
    for i, table_data in enumerate(extracted_data):
        sheet = workbook.Worksheets.Add(f"Table_{i + 1}")

        # 写入行列数据
        for r_idx, row_data in enumerate(table_data):
            for c_idx, cell_value in enumerate(row_data):
                # Excel 行列索引从 1 开始
                sheet.Range[r_idx + 1, c_idx + 1].Value = cell_value

        # 自动调整列宽
        sheet.AllocatedRange.AutoFitColumns()

    workbook.SaveToFile(excel_file_path, ExcelVersion.Version2016)
    workbook.Dispose()
    print(f"数据已成功写入到 {excel_file_path}")</code></pre><h3>实现要点</h3><ul><li><p><strong>每个 Word 表格 → 一个 Excel 工作表</strong></p><ul><li>结构直观，避免数据混杂</li></ul></li><li>Excel 行列索引从 <code>1</code> 开始，需要注意与 Python 索引的差异</li><li><code>AutoFitColumns()</code> 可显著提升导出后的可读性</li></ul><hr/><h2>四、完整调用示例</h2><pre><code class="python">word_file = "input.docx"
excel_file = "output.xlsx"

extracted_data = extract_tables_from_word(word_file)
write_data_to_excel(extracted_data, excel_file)</code></pre><p>运行后，Word 文档中的所有表格将被完整转换，并按顺序写入 Excel 文件。以下是运行结果示例：</p><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnSvK" alt="Python提取Word表格到Excel" title="Python提取Word表格到Excel"/></p><hr/><h2>五、适用场景与扩展建议</h2><p><strong>适用场景</strong>：</p><ul><li>将报告型 Word 文档中的数据统一汇总到 Excel</li><li>自动化处理批量合同、清单、配置表</li><li>作为数据清洗或分析流程的前置步骤</li></ul><p><strong>扩展方向</strong>：</p><ol><li><strong>批量处理多个 Word 文件</strong></li><li>根据表格内容自动命名工作表</li><li>对 Excel 输出添加边框、样式或冻结首行</li><li>将中间数据结构复用于 CSV 或数据库写入</li></ol><hr/><h2>总结</h2><p>通过结合 <strong>Spire.Doc for Python</strong> 与 <strong>Spire.XLS for Python</strong>，我们可以用一套清晰、稳定的 Python 方案，实现 <strong>Word 表格到 Excel 表格的自动化转换</strong>。这种方式不仅避免了手动复制的低效和错误，也为后续的数据处理和分析提供了良好的基础。</p><p>对于需要频繁处理文档表格数据的开发者和办公场景来说，这是一种非常实用、可维护性也很高的解决方案。</p>]]></description></item><item>    <title><![CDATA[春节旺季不慌！当连锁门店遇上一见，稳稳赢下全年开门红！ 百度一见 ]]></title>    <link>https://segmentfault.com/a/1190000047597754</link>    <guid>https://segmentfault.com/a/1190000047597754</guid>    <pubDate>2026-02-06 19:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047548146" alt="图片" title="图片"/><br/>年味愈发醇厚，服务业的 “春运时间” 已正式拉开帷幕。当汹涌人潮涌向全国万千连锁门店，品牌管理者们正面临一场全方位的运营大考：人手告急：客流峰值期店员忙到分身乏术，服务 SOP 执行变形，谁来及时纠偏？安全红线：后厨用火用电需求激增、地面水渍易引发滑倒风险，如何防患于未然？缺货焦虑：年货爆款上架即售罄，人工补货速度追不上顾客扫货节奏，如何破解？百度一见，将多模态视觉技术深度融入春节服务全场景，为连锁门店派驻 “全天候在岗的 AI 店长”，让门店在旺季忙而有序，稳稳斩获新年第一桶金！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597756" alt="图片" title="图片" loading="lazy"/></p><h4>管服务：协同无差错，服务有标准</h4><p>越是人手紧缺，越需要AI来充当“数字化店长”。百度一见可直接复用门店原有摄像头，实时捕捉前端服务全场景，自动识别员工仪容仪表、服务流程规范、出餐时效等关键指标，发现问题即刻推送至店长后台，确保即便在客流峰值，服务标准也始终在线。</p><p><strong>人员行为智能提醒</strong>：口罩、工帽、围裙等细节，是春节食安管理的关键一环。一见赋予现场摄像头智能查纠能力，<strong>自动识别店员穿戴违规行为，及时预警整改。</strong></p><p><strong>服务流程量化管理</strong>：针对服务合规 “无法量化管理” 痛点，<strong>一见将 “菜品按时上桌”“顾客离座及时收台”“员工着装规范” 等环节转化为可量化指标</strong>，总部可实时管理全国门店执行情况，实现千家门店服务标准统一管控。</p><p><strong>场景适配灵活高效：</strong>临时新增 “餐具摆放检查” 或 “新春口罩佩戴识别” 需求？<strong>一见支持一句话生成专业级视觉AI应用，完美适配餐饮、零售、茶饮等各类业态门店。</strong> </p><h4>管安全：风险无死角，运营更安心</h4><p>门店场景的合规与安全，是春节旺季运营的底线。无论是餐饮门店的后厨卫生、虫害防治，还是零售门店的消防安全、环境整洁，一旦出现问题，不仅会面临监管处罚，还会严重影响品牌口碑，甚至错失旺季客流。一见打破传统人工巡检的局限，打造全天候、无死角的安全监测，让门店守住安全合规底线，安心过年。</p><p><strong>守住舌尖上的安全：</strong>在食品安全领域，<strong>一见能精准识别后厨虫害、生熟食交叉污染、操作台不洁等风险</strong>，大幅降低人工巡检的漏判概率，守住顾客“舌尖上的安全”，筑牢品牌信任壁垒。</p><p><strong>护航门店平稳运营</strong>：在门店环境与安全方面，一见可实时监测消防通道是否畅通、消防设备是否齐全、外部人员是否闯入，<strong>及时预警潜在风险，全方位规避安全隐患与合规问题，</strong>保障门店春节期间安全、平稳运营。</p><h4>管物料：库存精准控，损耗降到底</h4><p>春节期间，连锁门店商品 / 物料需求量暴增，库存周转速度加快，传统人工盘点耗时费力且易出错。百度一见依托多模态大模型技术，打造全自动化 “AI 盘库” 解决方案，革新传统库存管理模式。</p><p><strong>智能盘库高效省心</strong>：<strong>自动完成物料消耗盘点，实时同步库存数据</strong>，无需员工闭店后熬夜加班，<strong>大幅降低人工成本，盘点效率提升 60% 以上。</strong>同时结合春节消费趋势，精准预判物料需求，辅助门店提前规划备货，有效避免缺货或库存积压问题。</p><p><strong>安全与损耗双重管控：</strong>针对零售门店货架管理核心痛点，一见<strong>可精准识别商品缺货断档、商品破损、摆放错位、价签不匹配等问题，</strong>实现从商品上架陈列、货架实时监测到库存补给的全程可视化管理，让旺季货架管理更精准、库存周转更高效。目前，百度一见已携手餐饮、茶饮、零售等多个行业的头部连锁品牌，实现后厨违规操作降低80%、服务合规率提升40%、库存盘点效率提升60%的显著成效。<br/><strong>立即联系一见，定制专属连锁门店春节运营方案，稳稳拿下全年开门红！</strong><br/>👉<a href="https://link.segmentfault.com/?enc=54sGpHEC6%2ByNFZo0Pvp2Ew%3D%3D.qzLWpNGuev1K%2FKpRoO0E0FBEE2Rss%2BWWNr75RuSA7nP7X5FHPiGHg3Phnw%2FjV%2BEE5JbSu%2B1n%2Bo6VN5JIV963W9MzO9GuOo0ZmXg0uoTUbzw%3D" rel="nofollow" target="_blank">https://cloud.baidu.com/survey/yijian-intelligentchainstores....</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597757" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[对接印度 NSE 与 BSE 交易所实时数据 CryptoRzz ]]></title>    <link>https://segmentfault.com/a/1190000047597772</link>    <guid>https://segmentfault.com/a/1190000047597772</guid>    <pubDate>2026-02-06 19:04:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>印度股市拥有两大支柱：<strong>国家证券交易所 (NSE)</strong> 和 <strong>孟买证券交易所 (BSE)</strong>。NSE 以极高的流动性和衍生品交易著称，而 BSE 则是亚洲最古老的交易所，拥有最多的上市公司。</p><p>对于开发者而言，如何在一个接口中同时获取这两大交易所的<strong>实时行情</strong>、<strong>指数（Nifty 50 / Sensex）以及逐笔 K 线</strong>，是构建印度金融产品的关键。</p><hr/><h3>一、 核心接入参数（交易所定位）</h3><p>在 StockTV API 体系中，通过 <code>exchangeId</code> 可以精准筛选数据源：</p><table><thead><tr><th>交易所名称</th><th>缩写</th><th><strong>exchangeId</strong></th><th>核心指数</th></tr></thead><tbody><tr><td><strong>印度国家证券交易所</strong></td><td><strong>NSE</strong></td><td><code>46</code></td><td>Nifty 50 (NSEI)</td></tr><tr><td><strong>孟买证券交易所</strong></td><td><strong>BSE</strong></td><td><code>74</code></td><td>S&amp;P BSE SENSEX (BSESN)</td></tr></tbody></table><blockquote><strong>国家 ID 提示</strong>：对接印度市场时，请务必全局携带 <code>countryId=14</code>。</blockquote><hr/><h3>二、 核心 API 场景实现</h3><h4>1. 区分交易所获取股票列表</h4><p>如果您想单独展示 NSE 或 BSE 的股票排名或列表，可以使用 <code>exchangeId</code> 参数进行过滤。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/stocks</code></li><li><strong>NSE 示例</strong>：<code>?countryId=14&amp;exchangeId=46&amp;key=YOUR_KEY</code></li><li><strong>BSE 示例</strong>：<code>?countryId=14&amp;exchangeId=74&amp;key=YOUR_KEY</code></li></ul><h4>2. 指数实时监控（Nifty vs Sensex）</h4><p>指数是市场的风向标。StockTV 提供的指数接口支持秒级更新。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/indices</code></li><li><strong>请求参数</strong>：<code>countryId=14&amp;key=YOUR_KEY</code></li><li><strong>数据亮点</strong>：返回结果中会包含 <code>NSEI</code>（NSE 指数）和 <code>BSESN</code>（BSE 指数）的实时点位、涨跌幅及成交额。</li></ul><h4>3. 实时 K 线数据（图表专用）</h4><p>支持对接各种前端图表库（如 TradingView），提供高频采样的 K 线。</p><ul><li><strong>接口地址</strong>：<code>https://api.stocktv.top/stock/kline</code></li><li><strong>参数示例</strong>：<code>pid={产品ID}&amp;interval=PT1M&amp;key=YOUR_KEY</code>（获取 1 分钟级极速 K 线）。</li></ul><hr/><h3>三、 技术优势：极致实时性</h3><p>针对印度市场波动剧烈、散户参与度高的特点，StockTV 在实时性上做了深度优化：</p><ol><li><strong>多交易所聚合推送</strong>：无需维护多套连接，通过一个 WebSocket 通道即可接收 NSE 和 BSE 的混合报价推送。</li><li><strong>毫秒级延迟控制</strong>：服务器节点部署于印度及周边核心机房，大幅缩短物理距离带来的网络延迟。</li><li><strong>智能重连机制</strong>：针对移动端应用，提供稳定的数据流保持方案，确保用户在弱网环境下也能看到最新的价格跳动。</li></ol><hr/><h3>四、 Python 实战：获取 NSE 权重股行情</h3><p>以下示例演示如何快速调取 NSE 交易所中特定股票（如 Reliance）的实时数据：</p><pre><code class="python">import requests

def fetch_india_exchange_data(symbol, exchange_id):
    url = "https://api.stocktv.top/stock/queryStocks"
    params = {
        "symbol": symbol,
        "exchangeId": exchange_id, # 指定交易所 46 或 74
        "key": "YOUR_API_KEY"
    }
    
    response = requests.get(url, params=params)
    data = response.json()
    
    if data['code'] == 200:
        item = data['data'][0]
        print(f"交易所ID: {exchange_id} | 股票: {item['name']}")
        print(f"当前价: {item['last']} | 涨跌幅: {item['chgPct']}%")
        print(f"最后撮合时间: {item['time']}")
    else:
        print(f"请求失败: {data['message']}")

# 查询 NSE 的 Reliance
fetch_india_exchange_data("RELI", 46)
</code></pre><hr/><h3>五、 进阶：如何获取完整的 BSE 500 指数成分？</h3><p>对于需要构建深度行情应用的客户，还支持通过 <code>stocksByPids</code> 接口批量订阅数百只股票的实时更新。只需一次请求，即可获取整个板块的盘面异动。</p><hr/><h3>六、 结语</h3><p>无论是追求极致速度的算法交易，还是注重用户体验的行情 App，提供的 NSE/BSE 双交易所接口都能为您提供最坚实的数据支撑。</p>]]></description></item><item>    <title><![CDATA[2026年需求管理软件测评：主流产品对比与选型避坑指南 王思睿 ]]></title>    <link>https://segmentfault.com/a/1190000047597781</link>    <guid>https://segmentfault.com/a/1190000047597781</guid>    <pubDate>2026-02-06 19:03:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你正在挑一款需求管理软件，大概率会在“需求入口分散、评审难落地、变更失控、交付对不齐”之间反复踩坑。本文以可核验公开资料为依据，按统一评分模型测评 ONES、Tower、Jira Product Discovery、Aha! Roadmaps、Productboard、YouTrack、Azure DevOps Boards、GitLab Requirements、Jama Connect、IBM DOORS、Polarion、ReqView，给你一张可直接对照的选型清单：谁适合产品团队、谁更偏工程合规、谁适合研发执行闭环。</p><h2>一、测评方法论：5大一级指标 + 14项细分维度</h2><p>评分口径：本文“综合得分/星级”是编辑评分，依据各工具的公开产品页/帮助中心/官方文档中可核验功能信息；不使用“不可验证的第三方满分/认证/客户数”。</p><p><strong>5大一级指标（建议权重）：</strong></p><ul><li>需求全生命周期能力（30%）：从需求入口到验收闭环是否完整</li><li>优先级与路线图（20%）：能否把“想做”变成“先做什么”</li><li>评审与变更治理（20%）：需求管理软件的分水岭在“变更可控”</li><li>追溯与影响分析（20%）：需求—设计—任务—测试的链路是否可追问</li><li>集成与协作体验（10%）：跨部门/跨工具链协作成本</li></ul><p><strong>14项细分维度：</strong></p><ul><li>需求入口：多渠道收集 / 表单化 / 统一归口</li><li>需求池：去重归类 / 状态流转 / 负责人机制</li><li>需求表达：模板 / 验收标准 / 附件与上下文</li><li>优先级：自定义字段 / 评分模型 / 价值-成本权衡</li><li>路线图：多视图 / 干系人沟通 / 与交付同步</li><li>评审：评审流程 / 评论与决议 / 结论可追溯</li><li>变更：版本化 / 变更记录 / 影响范围提示</li><li>依赖：需求依赖/前后置 / 冲突提示</li><li>追溯：需求↔任务 / 需求↔测试 / 历史审计</li><li>影响分析：变更触发的下游影响识别</li><li>交付对齐：迭代/版本关联 / 发布说明</li><li>协作：通知 / 权限 / 外部协作</li><li>集成：API/Webhook / 与代码/CI/客服系统对接</li><li>报表：周期、吞吐、积压、变更等基础度量</li></ul><h2>二、2026年需求管理软件总榜（选型参考，非绝对优劣）</h2><p><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnSwh" alt="" title=""/></p><blockquote>关键提醒：工具排名是“选型参考”；真正的优劣取决于你团队的协作方式与治理成熟度。品牌推荐官文章也会强调“排序非绝对优劣”，但常用更强断言表达。</blockquote><h2>三、2026年需求管理软件深度测评</h2><h4>NO.1｜ONES（从需求到交付验证的闭环流水线）</h4><p>推荐指数：★★★★★<br/>综合得分：92/100（编辑评分）<br/>关键优势：产品能力（93）/集成协作（90）/场景适配（92）/治理追溯（91）/交付对齐（94）</p><p>核心定位：<a href="https://link.segmentfault.com/?enc=SAp1H0%2F%2FBt3HtWwiumSVgQ%3D%3D.4ix8LGnWiIn%2FipqQkurwXg%3D%3D" rel="nofollow" target="_blank">ONES</a> 是面向企业的一站式研发管理平台品牌，覆盖从需求到交付协作与效能提升的核心场景。在需求管理方面是更偏“产研一体”的需求管理软件，把需求池、迭代规划、测试关联放在同一条链路里。其“需求池”思路强调：统一入口、梳理评审、优先级排期、再分配落地。</p><p>需求管理能力拆解</p><ul><li>需求入口与需求池：需求池作为统一归口，覆盖收集、梳理、评审、分配等环节，适合把分散需求“先收住”。</li><li>优先级与评审闭环：需求评审的关键不是“开会”，而是把优先级决策、依赖关系、验收标准写进需求对象里，减少“会后口头共识”。（公开资料侧强调优先级、关系与跟踪机制）</li><li>交付对齐与追溯：当需求进入测试阶段，可将测试计划与需求关联，形成“需求跟踪视图/矩阵”，让交付质量回到需求本身；测试用例也支持与需求、任务关联，利于问责与复盘。</li></ul><p>适用场景：中大型产品团队、需要“需求—迭代—测试”联动的组织；以及希望把需求管理软件作为研发管理底座的团队。</p><p>使用与避坑建议：</p><ul><li>别一上来就追求复杂流程：先把“需求入口—需求池—优先级—迭代对齐”跑顺，再补审批/变更规则。</li><li>度量先从可解释指标开始：周期、积压、变更次数这类“能解释”的指标先做起来，别一开始追求花哨看板。</li></ul><p>参考来源：ONES 需求池管理实践文章、需求池管理流程文章、ONES TestCase 产品页。</p><h4>NO.2｜Jira Product Discovery（需求发现/优先级）</h4><p>推荐指数：★★★★☆<br/>综合得分：88/100<br/>关键优势：产品能力（90）/集成协作（88）/场景适配（86）/治理追溯（80）/交付对齐（86）<br/>核心定位：Jira Product Discovery 是 Atlassian 面向产品团队推出的需求发现与优先级/路线图工具，强调把想法与洞察集中管理并用字段与公式做排序。<br/>需求管理能力拆解<br/>需求入口与需求池：更像“机会/想法池”，适合把用户反馈、销售线索、访谈结论先结构化沉淀。<br/>优先级与路线图：支持自定义字段与公式，帮助团队把“价值/影响/成本”量化成排序依据；并能用不同视图对不同干系人沟通。<br/>评审闭环：优势在于“基于证据的对齐”，而不是审批流；适合减少拍脑袋，但需要你们先统一评分口径。<br/>变更与追溯：对“需求—交付”闭环依赖 Jira 生态（这不是缺点，只是边界）。<br/>适用场景：产品团队需要提升“优先级共识质量”；尤其是多干系人拉扯严重、需要用证据做取舍的团队。<br/>局限与避坑建议：<br/>别把它当完整交付系统：它强在“上游”，下游需要配套交付工具链。<br/>评分模型要小而美：字段越多越像形式主义，建议从 3-5 个关键维度起步。<br/>参考来源：Atlassian 官方产品页与功能页。</p><h4>NO.3｜Productboard（把需求讲成路线图）</h4><p>推荐指数：★★★★☆<br/>综合得分：87/100<br/>关键优势：产品能力（89）/集成协作（85）/场景适配（88）/治理追溯（78）/交付对齐（84）<br/>核心定位：Productboard 是产品管理软件平台，核心价值是帮助团队理解客户需求、做特性优先级，并让组织围绕路线图对齐。<br/>需求管理能力拆解<br/>需求入口与需求池：强在“把多渠道声音汇总成可行动的需求主题”，适合产品在信息噪音中做归类。<br/>优先级与路线图：强调“围绕路线图对齐组织”，并支持把路线图以链接形式对外共享（适用于对齐销售/客户）。<br/>评审与变更：适合“产品评审”，但工程变更控制需要与交付工具联动（否则会变成“路线图很好看，落地很难追”）。<br/>适用场景：产品驱动型组织；需要经常向外部/内部解释“为什么先做这个”的团队。<br/>局限与避坑建议：<br/>别把路线图当承诺清单：路线图输出越容易共享，越要把“可变更边界”讲清楚。<br/>落地追溯要预先设计：至少保证需求与交付任务有稳定映射关系，否则复盘时很难说清“这条需求到底交付了什么”。<br/>参考来源：Productboard 官方产品页与路线图共享帮助文档。</p><h4>NO.4｜Aha! Roadmaps（Idea 门户 + 推进到路线图）</h4><p>推荐指数：★★★★☆<br/>综合得分：86/100<br/>关键优势：产品能力（88）/集成协作（83）/场景适配（86）/治理追溯（80）/交付对齐（82）<br/>核心定位：Aha! Roadmaps 是以产品路线图与创意管理见长的平台，支持把 Ideas 直接“Promote”到路线图记录并建立关联追踪。<br/>需求管理能力拆解<br/>需求入口：Ideas Portal 让“外部/一线声音”有标准入口。<br/>需求推进机制：支持把 idea “Promote”到路线图中的不同记录类型，并可新建或链接到既有记录，适合把多个反馈收敛到同一需求主题。<br/>评审与变更：优势在“推进机制清晰”；但你仍需要定义“什么时候可以Promote、谁批准、如何记录决议”。<br/>适用场景：产品线多、反馈来源杂、需要强治理与对齐的组织。<br/>局限与避坑建议：<br/>不要用工具替代决策：Aha 能让流程更可追溯，但“取舍标准”仍要团队自己建立。<br/>避免门户变成许愿池：设置最小提交模板与反馈分类规则，否则会被低质量输入淹没。<br/>参考来源：Aha 官方产品页与支持文档。</p><h4>NO.5｜Azure DevOps Boards（把需求分层成 Epic/Feature）</h4><p>推荐指数：★★★★☆<br/>综合得分：84/100<br/>关键优势：产品能力（80）/集成协作（88）/场景适配（85）/治理追溯（82）/交付对齐（88）<br/>核心定位：Azure Boards 是 Microsoft Azure DevOps 体系中的工作项与 Backlog 管理能力，支持用 Epics/Features 组织需求并分层推进到执行。<br/>需求管理能力拆解：<br/>需求池与分层：通过 features/epics backlogs 把需求按层级归类，利于“从愿景到迭代”的结构化分解。<br/>优先级与排期：优势在“与开发执行紧耦合”，但产品侧的“需求质量（验收标准/证据）”需要你们自己用模板/规范补齐。<br/>追溯：在工程侧可追溯较强，但跨到测试/发布/客户反馈时，仍需要流程设计与集成。<br/>适用场景：研发组织成熟、执行体系以 ADO 为核心；需要把需求管理软件与交付过程合一。<br/>局限与避坑建议<br/>别只堆层级：Epic/Feature 不是越多越好，关键是每一层都能回答“这层做完意味着什么”。<br/>验收标准务必前置：否则会变成“做了很多条目，但没人敢说交付完成”。<br/>参考来源：Microsoft Learn 官方文档。</p><h4>NO.6｜YouTrack（看板与 Backlog 一体）</h4><p>推荐指数：★★★★☆<br/>综合得分：83/100<br/>关键优势：产品能力（80）/集成协作（80）/场景适配（86）/治理追溯（78）/交付对齐（82）<br/>核心定位：YouTrack 是 JetBrains 的项目/Issue 跟踪平台，强调用 Backlog 与敏捷看板把团队工作保持聚焦并可随优先级变化回收至 Backlog。<br/>需求管理能力拆解：<br/>需求池/Backlog：支持围绕查询与Backlog协作，但如果你们希望“产品侧更强的需求表达/评审”，要自己补流程。<br/>流转与协作：看板卡片拖动会同步更新字段值，协作反馈更即时。<br/>变更治理：更偏“执行流转”，而非“治理控制”；适合敏捷团队，但对强合规场景不够。<br/>适用场景：研发团队主导的敏捷协作；不想为复杂RM系统付出高实施成本的组织。<br/>局限与避坑建议<br/>别把“能拖动”当“治理”：需求评审结论、验收标准、变更边界仍要写清楚。<br/>Backlog规则要一致：否则会出现“需求在板上/在Backlog里”争议，影响透明度。<br/>参考来源：JetBrains YouTrack 官方文档。</p><h4>NO.7｜Tower（更擅长排期与依赖）</h4><p>推荐指数：★★★★☆<br/>综合得分：80/100<br/>关键优势：产品能力（75）/集成协作（82）/场景适配（86）/治理追溯（70）/交付对齐（84）<br/>核心定位：Tower 是国内团队协作与项目管理产品，突出以时间线（甘特图）等视图提升任务排期与依赖协作效率。在需求管理方面更像“把需求落到任务与排期”的协作工具——当你需求评审完，最怕的不是“没人做”，而是“做着做着依赖乱了、排期失真”。Tower 的时间线/甘特与依赖联动，能把交付风险提前暴露。<br/>需求管理能力拆解：<br/>需求到任务拆解：适合作为“需求落地承接层”，把需求拆成任务、设置负责人/日期/依赖，保证交付透明。<br/>依赖与变更联动：支持“自动调整后置任务时间”“防止任务依赖冲突”，这对频繁变更的需求落地很实用——变更不是问题，变更不联动才是问题。<br/>可视化排期：支持按天/周/月/季/年粒度看时间线，便于和干系人对齐节奏。<br/>适用场景：需要强排期、强依赖管理的项目型团队；或把专业需求管理软件与协作排期工具组合使用的团队。<br/>局限与避坑建议：<br/>需求治理要在上游完成：Tower 适合执行与排期，不适合承载复杂的需求评审与合规追溯。<br/>依赖不是越多越好：建议只给关键路径建依赖，否则维护成本反噬。<br/>参考来源：Tower 官方博客（甘特/时间线/依赖联动说明）。</p><h4>NO.8｜GitLab Requirements（把需求放进工程体系）</h4><p>推荐指数：★★★☆☆<br/>综合得分：79/100<br/>关键优势：产品能力（75）/集成协作（86）/场景适配（78）/治理追溯（80）/交付对齐（82）<br/>核心定位：GitLab Requirements 是 GitLab 平台中的“需求工件”能力，把需求作为长期存在的 artifact 来管理，用于描述产品行为与验收标准。<br/>需求管理能力拆解<br/>需求对象化：在项目中可创建/查看需求列表，需求不再只是 issue 描述。<br/>合规协作：导出需求到 CSV 并通过邮件附件发送的能力，对“需要对外共享/审计留档”的场景有现实价值。<br/>追溯与交付对齐：工程侧链路天然更紧密，但产品侧需求池、路线图治理能力相对有限。<br/>适用场景：DevOps 一体化团队；希望需求管理软件尽量贴近代码与工程资产的组织。<br/>局限与避坑建议<br/>别把导出当治理完成：导出只是能力，治理要靠流程与责任边界。<br/>需求表达要标准化：否则需求会退化成“另一个Issue字段”。<br/>参考来源：GitLab Requirements 官方文档。</p><h4>NO.9｜Jama Connect（需求变更影响分析）</h4><p>推荐指数：★★★★☆<br/>综合得分：85/100<br/>关键优势：产品能力（88）/集成协作（78）/场景适配（84）/治理追溯（92）/交付对齐（83）<br/>核心定位：Jama Connect 是 Jama Software 的需求管理与追溯平台，主打 Live Traceability 与实时风险识别能力（如 Live Trace Explorer）。<br/>需求管理能力拆解<br/>变更影响分析：当需求变化时，识别对下游需求与测试的“涟漪效应”，这是合规与质量团队真正关心的地方。<br/>追溯到测试：把需求与测试活动的关系建立起来，帮助你回答“这个需求验证了吗、覆盖了吗”。<br/>评审与协作：更适合正式评审与证据沉淀，而非轻量敏捷日常。<br/>适用场景：汽车、医疗、航天等对追溯与验证要求高的行业；或系统工程团队。<br/>局限与避坑建议<br/>实施成本要前置评估：Jama 的价值在体系化使用，零散使用反而浪费。<br/>先定义追溯模型再上工具：否则你会得到“很多链接”，但解释不清为什么要链接。<br/>参考来源：Jama 官方帮助文档与能力介绍页。</p><h4>NO.10｜IBM DOORS（传统工程 RM）</h4><p>推荐指数：★★★★☆<br/>综合得分：82/100<br/>关键优势：产品能力（80）/集成协作（70）/场景适配（82）/治理追溯（94）/交付对齐（78）<br/>核心定位：IBM Engineering Requirements Management DOORS 系列（DOORS 与 DOORS Next）是 IBM 的规模化需求管理解决方案，强调协作评审、变更管理与可追溯性。<br/>需求管理能力拆解<br/>基线与签署：支持对基线进行电子签署，并记录签署人、时间等信息，满足审计与责任追溯需要。<br/>多级追溯：强调多层级追溯视图与可定制视图，适合复杂需求分解与验证链路。<br/>变更治理：优势在“控制与证据链”，但对产品团队而言会显得偏重、偏工程。<br/>适用场景：强监管行业、系统工程/硬件软件协同项目、对审计链要求极高的组织。<br/>局限与避坑建议<br/>不要把它当轻量需求池：它更适合“规格与基线管理”，不是日常想法收集。<br/>角色分工要清晰：否则会出现“工程团队用得很好，产品团队完全进不来”。<br/>参考来源：IBM 产品页与官方文档（电子签名/基线）。</p><h4>NO.11｜Polarion（自动变更控制 + 审计链）</h4><p>推荐指数：★★★★☆<br/>综合得分：84/100<br/>关键优势：产品能力（83）/集成协作（76）/场景适配（84）/治理追溯（92）/交付对齐（80）<br/>核心定位：Polarion 是 Siemens 的网页版 ALM 平台，用于统一管理需求、开发、测试与发布，并强调端到端可追溯与可见性。<br/>需求管理能力拆解：<br/>自动变更控制：对每条需求的变更进行控制与记录，目标是让审计/合规检查更容易通过。<br/>工作流与审计链：用工作流规则约束需求状态流转，“什么时候能进入下一状态”变成可配置规则，而不是口头约定。<br/>电子签署：支持让干系人对规格文档电子签署（公开资料中明确提及）。<br/>适用场景：大型组织、流程治理成熟或必须提升合规证据链的团队。<br/>局限与避坑建议：<br/>别用它解决“沟通不愿写清楚”：工具能强约束流程，但写清楚仍要靠团队习惯。<br/>先梳理关键工作流再落工具：否则配置会变成一场无止境的“流程之战”。<br/>参考来源：Siemens Polarion 官方介绍页。</p><h4>NO.12｜ReqView（轻量工程 RM）</h4><p>推荐指数：★★★☆☆<br/>综合得分：78/100<br/>关键优势：产品能力（76）/集成协作（70）/场景适配（78）/治理追溯（85）/交付对齐（72）<br/>核心定位：ReqView 是面向软硬件开发的需求管理工具，强调在 Git 中建立基线，并支持覆盖/风险/变更影响分析与多格式报告输出（Word/Excel/PDF/HTML）。<br/>需求管理能力拆解<br/>文档化需求与基线：对需要交付规格文档、并希望版本可控的团队，Git 基线的概念很贴近工程实践。<br/>追溯与影响分析：强调分析覆盖、风险与变更影响，适合“改一条需求会影响哪里”的场景。<br/>报告输出：可生成 Word/Excel/PDF/HTML 等报告格式，这对交付与审计沟通很友好。<br/>适用场景：硬件/软件协同、系统工程、需要规格文档交付但不想上重型RM套件的团队。<br/>局限与避坑建议<br/>对“产品型需求池”支持较弱：更适合工程规格与追溯，不适合做海量想法收集。<br/>需要团队具备版本管理习惯：否则“基线在Git”会变成少数人才能维护的资产。<br/>参考来源：ReqView 官方产品页。</p><p>挑需求管理软件这件事，表面看是功能对比，实际是在选择一种“治理方式”。如果你们最痛的是需求入口分散，先选能把需求池立住的；如果你们最痛的是优先级共识，选能把证据、字段、公式沉淀下来的；如果你们最痛的是变更失控与追责困难，那就把“追溯与影响分析”当作硬指标；如果你们在强合规行业，别怕工具偏重——怕的是你们没有一条可审计的证据链。工具不会替你做决策，但工具会逼你把决策写清楚。当你们愿意把“为什么做、先做什么、变更影响什么、怎么验收”落在系统里，需求管理软件才会从“记录器”变成“协作的共同语言”。</p>]]></description></item><item>    <title><![CDATA[2026年产品管理系统测评：对比选型避坑+能力模型评分 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047597786</link>    <guid>https://segmentfault.com/a/1190000047597786</guid>    <pubDate>2026-02-06 19:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文测评 12 款常见产品管理系统/平台：ONES、Tower、Jira Product Discovery、Jira Software、Productboard、Aha! Roadmaps、Craft.io、airfocus、ProductPlan、Tempo Strategic Roadmaps（原 Roadmunk）、Azure Boards、monday.com。我会用“能力模型+1–5星评分”对比战略规划、需求管理、研发协同、用户反馈、数据指标、知识沉淀、组织治理与集成扩展等方面，帮你更快做出适配团队的选型判断。</p><h2>测评结论速览</h2><h4>结论速选</h4><ul><li>想要“需求—研发协同—测试/交付”尽量在一套体系里跑：优先看 ONES 这类端到端研发管理平台路线。</li><li>最痛是“想法太多、证据分散、优先级吵不清”：优先看 Productboard / Aha! Roadmaps 这种上游产品发现与路线图工具。</li><li>更看重工程交付节奏（看板、Backlog、冲刺、报表）：ONES / Jira Software / Azure Boards 更像执行中枢。</li><li>更看重“排期、依赖联动、跨部门推进”：Tower 更像协作与排期底座，甘特依赖与自动调整对“变更引发的连锁延期”很有帮助。</li><li>更看重“路线图呈现与组合视图（Portfolio）对齐管理层”：优先看 ProductPlan。</li></ul><h4>对比表（编辑评分｜1–5星）</h4><blockquote>说明：这是“可核验信息基础上的编辑评分”，用于快速对比，不是第三方认证/行业榜单。</blockquote><p>评分尺度：</p><ul><li>5星：公开资料显示能力成熟、链路完整、适用面广</li><li>3星：能力明确但偏单点/偏分层，需要搭配其他工具/方法</li><li>1–2星：该维度公开能力弱或依赖外部实现，不建议作为主承载</li></ul><p>证据来源：</p><ul><li>官方产品页/功能页/帮助中心/支持文档（最高优先级）；</li><li>官方博客/官方集成文档（用于补充“如何实现”的细节）；</li><li>若某项能力在公开资料里找不到，我也会标注出来。</li></ul><p><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnSwm" alt="" title=""/></p><h2>产品管理系统深度测评</h2><h4>1) ONES</h4><p>一句话定位：<a href="https://link.segmentfault.com/?enc=%2FqWXmUaAxyBDiyqjnsvwyA%3D%3D.RCBpTGmornUHtirlFEQIxA%3D%3D" rel="nofollow" target="_blank">ONES</a> 是国产企业级研发管理平台，公开定位强调“端到端软件研发管理”，并明确覆盖从需求管理、迭代跟进到测试的链路，更像把产品管理系统与研发协同放在同一底座上。适合中大型产品研发组织，以及痛点是“规划在A、开发在B、测试在C”的断链，且愿意把评审规则、状态机、验收标准沉淀为组织资产的团队。<br/>产品管理能力拆解（战略规划/需求管理/研发协同/反馈/指标/知识/治理/运营）<br/>战略规划：强调“场景方案与流程落地”，战略层表达（如 OKR/产品战略树）能否做深，取决于你们是否把“口径与规则”写进系统。<br/>需求管理：优势在“需求进入交付链路后可追踪”，适合把需求、迭代、测试串联起来做闭环。<br/>研发协同：端到端链路是它作为产品管理系统底座的关键——当状态、职责、验收统一后，跨团队沟通会更省。<br/>用户反馈/产品运营：公开信息里可看到“工单/服务台”等场景，适合作为主反馈中枢还需要结合你们渠道与流程<br/>数据指标/治理：拥有效能改进与可视化分析能力，适合效能管理者用同一套口径看多项目/多团队表现。<br/>优势亮点（为什么重要 + 场景）<br/>闭环追溯：需求到测试贯通，让复盘能回答“做了什么、为何做、交付成了什么”。<br/>流程可落地：把协作规则写进系统，减少“靠人盯”的成本，适合人员流动较大的组织。<br/>统一口径：对效能管理者来说，统一口径比“功能堆满”更关键。</p><h4>2) Tower</h4><p>一句话定位：Tower 是 ONES 旗下团队协作工具，官方定位是“安排任务、管理进度、沉淀团队知识”，并提供列表/日历/看板/甘特等多视图，把它放进产品管理系统版图里更像“协作与排期底座”。适合中小到中型跨部门团队，以及项目排期、依赖关系复杂；需要把“计划—推进—提醒—复盘”放进同一工作空间的团队。<br/>产品管理能力拆解<br/>战略规划：Tower 更擅长把目标落到“阶段、里程碑与责任人”，而非提供完整 OKR 体系；它适合作为战略落地的执行层。<br/>需求管理：软件研发支持迭代计划、需求管理、Bug 管理等，偏“承接已明确需求并推进”。<br/>研发协同：甘特图用于进度管控，，适合应对“需求变更→排期连锁反应”。<br/>用户反馈：若你们在飞书协作，Tower 协作可以把聊天消息/图片快速转为任务，并与群聊/云文档空间等深度整合，这类“就地采集反馈→进入任务流”的路径很务实。<br/>优势亮点<br/>依赖+排期可视化：项目经理能更快定位延期风险，把精力从“搬日期”转回“管风险”。<br/>多视图对齐：同一份数据给不同角色用不同视图看，减少“你说不清、我看不懂”。<br/>飞书侧入口自然：对“反馈从聊天里来”的团队，消息转任务能显著降低漏记与重复沟通。</p><h4>3) Jira Product Discovery</h4><p>一句话定位：Jira Product Discovery（Atlassian）定位在“产品发现与优先级”，支持用字段与表达式公式把想法/洞察变成可排序的优先级，并用视图做路线图沟通。适合产品决策争议大、跨部门拉扯多、需要建立优先级共识的团队。<br/>产品管理能力拆解<br/>战略规划：更像战略输入的承接层——把战略拆成可讨论的机会/想法并持续评估。<br/>需求管理：它管理的是“上游需求”（机会/想法/洞察），核心价值是把争论从“谁嗓门大”转为“按口径排序”。<br/>研发协同：交付闭环通常需要 Jira 的执行体系承接（把被选中的需求稳定流入研发 Backlog）。<br/>组织治理：优势在“决策可复盘”——字段与公式让你解释得清楚“为什么做A不做B”。<br/>优势亮点<br/>公式化优先级：把 RICE 等模型落到系统，减少拍脑袋与反复扯皮。<br/>视图沟通：同一份数据给不同受众不同视角，降低对齐成本。<br/>与Jira衔接自然：对已在 Atlassian 生态内的团队，落地成本更低。</p><h4>4) Jira Software</h4><p>一句话定位：Jira Software 是典型工程执行型产品管理系统组件，官方明确支持 scrum/kanban 等方法，并覆盖敏捷板、Backlog、路线图、报表与集成生态。适合工程交付为中心的组织，产品经理需要在产品管理系统里紧跟开发进度、风险与发布。<br/>产品管理能力拆解<br/>战略规划：战略表达往往在外部形成（文档/OKR），再映射到史诗/故事与路线图。<br/>需求管理：更擅长“已确认需求”的拆解与流转（Backlog、用户故事管理），而不是大规模反馈聚类。<br/>研发协同：强项在节奏与透明——冲刺、看板、报告能帮助团队建立稳定交付节拍。<br/>数据指标：报表丰富，但建议把指标用于改进而非考核（否则团队会“为指标而工作”）。<br/>优势亮点<br/>方法适配广：scrum、kanban 或混合都能落地。<br/>Backlog 纪律强：用户故事管理与可见性提升，有助于减少“需求说了但没人做”。<br/>生态成熟：当你们需要和CI/CD、知识库、客服等系统连接时，生态价值会逐年放大。</p><h4>5) Productboard</h4><p>一句话定位：Productboard 公开定位是“帮助产品经理理解客户需求、确定优先级，并让组织围绕路线图对齐”，在产品管理系统里更偏“反馈→优先级→路线图”的主线。适合客户声音很多、需求入口分散、需要用路线图统一叙事并降低跨部门沟通成本的团队。<br/>产品管理能力拆解<br/>战略规划：强在“把战略讲清楚并持续对齐”，通过路线图让不同部门理解取舍逻辑。<br/>需求管理/用户反馈：定位直接强调理解客户需求，适合把分散的客户声音收拢为主题与需求，再进入优先级与路线图。<br/>研发协同：通常通过与工程工具联动完成交付闭环（公开定位强调“下一步要构建什么并对齐”，具体联动深度建议试用验证，本文不做超出公开信息的断言）。<br/>组织治理/产品运营：当路线图成为单一真相源，销售/CS/市场的预期管理会更稳定（这是产品运营层面的真实价值）。<br/>优势亮点<br/>围绕路线图对齐：减少重复解释与版本不一致。<br/>以用户需求驱动优先级：把“感觉”变成“可回溯的证据链”（至少在信息结构上）。<br/>更贴近产品语言：讨论焦点更偏用户问题与价值，而不是工程任务颗粒度。</p><h4>6) Aha! Roadmaps</h4><p>一句话定位：Aha! Roadmaps 公开强调通过品牌化 Ideas Portal 收集反馈、对想法排序，并将最佳想法Promote到路线图对象（features/epics等），属于偏治理的产品管理系统路线。适合产品线多、反馈来源复杂、需要强治理与推进机制的组织（尤其规模化阶段）。<br/>产品管理能力拆解<br/>战略规划：更适合做“路线图中枢”——把战略拆成可管理层级并持续推进。<br/>需求管理/用户反馈：Ideas Portal 统一入口，“Promote”机制让想法有明确去处并能跟踪状态。<br/>研发协同：常通过与工程工具衔接完成交付闭环（公开资料强调从想法到路线图再到交付状态追踪）。<br/>组织治理：强在“可追溯”——你能清楚回溯某个 feature 从哪条反馈/想法来、为何被选中。<br/>优势亮点<br/>Ideas → Roadmap 的推进链路：减少“想法堆在墙上没人管”。<br/>入口统一带来数据结构化：有利于沉淀分类、主题与决策依据。<br/>状态可追溯：对 PMO/效能角色很友好，复盘成本更低。</p><h4>7) Craft.io</h4><p>一句话定位：Craft.io 公开定位为端到端产品管理平台，并在 OKR 页面明确提出“连接 objectives 到 initiatives/projects/epics”，走的是“OKR/战略→规划→执行”的产品管理系统路线。适合目标管理清晰、管理层需要强对齐，或希望把 OKR 与产品规划强绑定的中大型产品组织。<br/>产品管理能力拆解<br/>战略规划/OKR：OKR 是显性定位，强调从目标到执行对象的连接，减少“目标只在PPT里”。<br/>需求管理：更偏把需求纳入战略框架（initiative/epic/feature），适合做结构化规划。<br/>研发协同：官网强调从反馈收集到执行覆盖整个生命周期（具体研发侧细节以试用为准，本文不做超出公开信息的断言）。<br/>指标/治理/运营：OKR 关联关系天然服务治理：当目标—投入—产出关系清晰，运营沟通也更“讲得通”。<br/>优势亮点<br/>目标到执行可追溯：能回答“为什么做这件事”，并在复盘时检查假设是否成立。<br/>端到端生命周期叙事：对“从反馈到执行”的团队更顺手。<br/>减少战略漂移：当目标与路线图对象有关联，改方向就会更谨慎、更可控。</p><h4>8) airfocus</h4><p>一句话定位：airfocus 公开定位为产品管理工具，并在优先级模块明确提供“评分框架、Priority Poker”等机制，把优先级讨论做成可协作流程，属于模块化产品管理系统路线。适合中型以上产品团队，对优先级质量、路线图沟通和跨团队参与度要求高的组织。<br/>产品管理能力拆解<br/>战略规划：其定位强调围绕清晰产品战略来优先级排序与路线图对齐。<br/>需求管理/优先级：强项就是“把取舍标准落地”——评分框架与协作式 Priority Poker 让不同角色参与排序。<br/>用户反馈/指标/运营：官网模块展示包含反馈与洞察、OKR等（深度以试用为准，本文不做超出公开信息的断言）。<br/>组织治理：模块化意味着可以按成熟度逐步启用，避免“一口气把系统做成怪物”。<br/>优势亮点<br/>评分框架让争论回到标准：把“口水战”转成“口径讨论”，适合争议多的团队。<br/>Priority Poker 促进共识：让多角色参与排序，减少单点拍板的偏差。<br/>模块化上手：先从优先级/路线图开始，再扩到OKR/洞察，节奏更可控。</p><h4>9) ProductPlan</h4><p>一句话定位：ProductPlan 公开强调“从发现到发布”用路线图与 portfolio 视图形成单一真相源，并在支持文档中明确 Portfolio View 可把多条路线图合并为一个整体视图用于分享给高层与干系人。适合产品线多、路线图需要统一口径；高频跨部门沟通导致成本高的组织。<br/>产品管理能力拆解<br/>战略规划：强在“战略可视化”——把战略倡议与里程碑放到可分享的路线图上，减少信息偏差。<br/>需求管理：更偏承接“已确认的规划项”，上游洞察/反馈体系通常要配套。<br/>研发协同：常作为“路线图层”，下游对接 Jira/Azure 等执行系统（公开强调从发现到发布，但具体执行承接需试用验证）。<br/>组织治理/产品运营：Portfolio View 对管理层对齐很友好——把多个产品/团队的路线图放在一张图里谈取舍。<br/>优势亮点<br/>Portfolio View“把全局拉齐”：适合 VP/负责人快速看到全局节奏与冲突。<br/>单一真相源：减少“每个人都有一张路线图”的版本地狱。<br/>面向受众分享：路线图不仅是内部用，还是对外预期管理工具。</p><h4>10) Tempo Strategic Roadmaps（原 Roadmunk）</h4><p>一句话定位：Tempo 的 Strategic Roadmaps 公开介绍里强调 Idea Manager：记录团队想法、在路线图中进出以便快速转向，并提供内置优先级模板或自定义评分框架，更偏“想法管理+路线图呈现”的产品管理系统层。适合对路线图沟通要求很高，需要把“想法收拢→优先级→路线图表达”做得顺的团队。<br/>产品管理能力拆解<br/>战略规划/路线图：核心在“把方向讲清楚”，并让路线图可随项目变化快速调整。<br/>需求管理：偏“想法/主题→路线图条目”，适合对齐与沟通，不是重规格的需求工程工具。<br/>用户反馈：公开资料聚焦 idea management 与 prioritization framework；更深入的反馈采集形态建议以官方资料与试用验证为准（本文不做超出公开信息的断言）。<br/>研发协同：更像路线图层，需要下游执行系统承接。<br/>优势亮点<br/>Idea Manager 支持快速转向：变化来了能把想法/条目快速进出路线图，减少大改带来的混乱。<br/>内置模板/自定义评分：让优先级讨论回到标准，提高决策一致性。<br/>更像“路线图表达工具”：对管理层沟通友好，适合把复杂工作讲成清晰节奏。</p><h4>11) Azure Boards</h4><p>一句话定位：Azure Boards（Azure DevOps）公开强调提供 Kanban boards、backlogs、dashboards、scrum boards 等敏捷工具，并支持自定义工作流与“1,000+ extensions”，属于工程交付承接型产品管理系统组件。适合微软生态与 DevOps 体系成熟的研发组织。<br/>产品管理能力拆解<br/>战略规划：更偏执行层，战略通常在外部形成后映射到工作项/积压。<br/>需求管理：适合管理“已确认需求”的交付推进（用户故事、bug、工作项等）。<br/>研发协同：看板/冲刺/容量等机制对效率管理者很关键，利于建立节奏与透明度。<br/>治理/集成：扩展与自定义能力适合大组织长期演进，也更容易融入 Teams/Slack 等协作。<br/>优势亮点<br/>敏捷执行能力完整：Kanban、Backlog、Dashboard、Scrum boards 一应俱全。<br/>可定制与扩展：适合把组织方法固化为流程与权限体系。<br/>与常用协作工具结合：有利于把执行信息带回团队日常协作场域。</p><h4>12) monday</h4><p>一句话定位：monday 在“Product Management Software”页面明确写到可在一处管理 roadmaps、plans、challenges、KPIs，并强调高度可定制，属于“可配置工作系统”型产品管理系统。适合协作对象广（产品/市场/运营/支持/交付），流程变化快、需要快速调整系统结构的团队。<br/>产品管理能力拆解<br/>战略规划/产品运营：KPI 与计划集中管理对运营节奏有帮助，但战略严密性取决于你怎么设计板、字段与评审机制。<br/>需求管理：更像可配置工作流承接需求与任务，而非专注产品发现的证据链体系。<br/>研发协同：跨部门协作友好；工程深度（如代码/PR强关联）需要看集成与组织实践（本文不做超出公开资料的断言）。<br/>组织治理：优势是“可定制”，风险也是“过度可定制导致口径漂移”，需要制度化维护。<br/>优势亮点<br/>路线图+计划+KPI同处：管理层看全局、团队看分工，沟通链路更短。<br/>可配置适配管理模式：更像“搭积木”，适合非标准流程。<br/>可视化带来透明：很多扯皮来自看不见彼此的工作，透明度能直接降低摩擦。</p><p>选产品管理系统，最怕的是“拿工具替代思考”。我更建议你先把三个问题问清楚：你们的瓶颈在哪一段？是上游“优先级吵不清”，还是中游“排期依赖失控”，还是下游“交付追溯断链”？瓶颈不同，主系统就不同。你们要“一体化闭环”还是“分层组合”？一体化适合追求口径统一、链路完整；分层组合适合成熟团队按强项拼装（路线图层 + 执行层 + 协作/知识层）。你们愿不愿意为“口径一致”付出维护成本？系统越灵活，越需要字段字典、状态机、评审模板与复盘节奏。否则工具越强，混乱也会更快被放大。</p><p>工具能做的，是把共识“写下来、追踪起来”；做不到的，是替你们完成那些本该由团队共同承担的取舍。把方法立住，你选哪一款产品管理系统，都会更顺。</p>]]></description></item><item>    <title><![CDATA[【Matlab源码】6G候选波形：OFDM-IM 增强仿真平台 DM、CI 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047597805</link>    <guid>https://segmentfault.com/a/1190000047597805</guid>    <pubDate>2026-02-06 19:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>&lt;p align="center"&gt;<br/>  &lt;h1 align="center"&gt;⚡ OFDM-IM 性能优化仿真平台&lt;/h1&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;strong&gt;DM-OFDM + CI-OFDM-IM 双技术加持，100%频谱利用 + 分集增益&lt;/strong&gt;</code></pre><p>&lt;/p&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;img src="https://img.shields.io/badge/MATLAB-R2021b+-blue?style=flat-square&amp;logo=mathworks" alt="MATLAB"/&gt;
&lt;img src="https://img.shields.io/badge/DM-OFDM-green?style=flat-square" alt="DM"/&gt;
&lt;img src="https://img.shields.io/badge/CI-Interleaving-orange?style=flat-square" alt="CI"/&gt;
&lt;img src="https://img.shields.io/badge/Diversity-Gain-red?style=flat-square" alt="Diversity"/&gt;</code></pre><p>&lt;/p&gt;<br/>&lt;/p&gt;</p><hr/><h2>📌 为什么选择本仿真平台？</h2><table><thead><tr><th align="left">痛点</th><th align="left">本平台解决方案</th></tr></thead><tbody><tr><td align="left">📚 基础 OFDM-IM 频谱利用率不高</td><td align="left">✅ <strong>DM-OFDM 双模式</strong>：所有子载波都携带数据，100% 频谱利用</td></tr><tr><td align="left">🔧 衰落信道下分集阶数不足</td><td align="left">✅ <strong>CI 坐标交织</strong>：I/Q 分量分离交织，额外获得 2 阶分集增益</td></tr><tr><td align="left">📊 星座设计无参考</td><td align="left">✅ <strong>最优星座对选择</strong>：16QAM+QPSK 等经验证的高效组合</td></tr><tr><td align="left">⚡ 分集技术难以验证</td><td align="left">✅ <strong>瑞利衰落信道仿真</strong>，直观对比 CI 前后 BER 差异</td></tr><tr><td align="left">📡 缺乏可视化展示</td><td align="left">✅ 自动生成 <strong>双模式星座图</strong> 和 <strong>分集增益曲线</strong></td></tr></tbody></table><hr/><h2>🎯 核心价值</h2><table>
<tr>
<td width="50%">

### 🔬 学术研究价值

- DM-OFDM 双模式索引调制原理验证
- CI 坐标交织分集增益量化分析
- 不同星座组合对 BER 的影响研究
- 衰落信道下的性能边界探索

</td>
<td width="50%">

### 💼 工程应用价值

- 100% 频谱利用率，适合频谱紧张场景
- 抗衰落能力增强，适合移动通信
- 可配置双模式星座阶数
- 完整的发射-接收链路实现

</td>
</tr>
</table><hr/><h2>⚡ 技术亮点</h2><h3>🌊 DM-OFDM + CI 双技术架构</h3><pre><code class="text">┌─────────────────────────────────────────────────────────────────┐
│                 DM-OFDM + CI-OFDM-IM 系统架构                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    【DM-OFDM 双模式调制】                        │
│  索引比特 ──► Mode 1 位置 (高阶 16-QAM)                         │
│            └► Mode 2 位置 (低阶 QPSK) ──► 100% 利用率           │
│                                                                 │
│                    【CI 坐标交织分集】                           │
│  复数符号 ──► I 分量保持 ──┐                                    │
│            └► Q 分量置换 ──┴►  分集增益 ×2                      │
│                                                                 │
│         ┌────────── 瑞利衰落信道 ──────────┐                    │
│         │   深衰落 → CI 保护 → 分集恢复    │                    │
│         └──────────────────────────────────┘                    │
└─────────────────────────────────────────────────────────────────┘</code></pre><h3>📊 性能指标 (仿真实测)</h3><table><thead><tr><th align="center">技术</th><th align="center">信道</th><th align="center">SNR</th><th align="center">BER</th><th align="center">vs 基础 IM</th><th align="center">增益</th></tr></thead><tbody><tr><td align="center">DM-OFDM</td><td align="center">AWGN</td><td align="center">12 dB</td><td align="center">5.2e-4</td><td align="center">1.2e-3</td><td align="center"><strong>频谱 +25%</strong></td></tr><tr><td align="center">CI-OFDM-IM</td><td align="center">Rayleigh</td><td align="center">15 dB</td><td align="center">8.3e-4</td><td align="center">3.5e-3</td><td align="center"><strong>分集 2 阶</strong></td></tr><tr><td align="center">DM+CI 联合</td><td align="center">Rayleigh</td><td align="center">15 dB</td><td align="center">4.1e-4</td><td align="center">3.5e-3</td><td align="center"><strong>综合最优</strong></td></tr></tbody></table><blockquote>💡 <strong>双重优势</strong>：DM 提升频谱效率，CI 提升抗衰落能力，两者可叠加使用。</blockquote><hr/><h2>🖥️ 运行环境</h2><h3>最低要求</h3><table><thead><tr><th align="left">项目</th><th align="left">要求</th></tr></thead><tbody><tr><td align="left"><strong>MATLAB版本</strong></td><td align="left">R2021b 或更高</td></tr><tr><td align="left"><strong>必需工具箱</strong></td><td align="left">Communications Toolbox</td></tr><tr><td align="left"><strong>基础依赖</strong></td><td align="left">P1 基础包</td></tr><tr><td align="left"><strong>内存</strong></td><td align="left">4 GB+</td></tr></tbody></table><h3>快速验证</h3><pre><code class="matlab">&gt;&gt; cd packages/P2_性能优化包
&gt;&gt; setup_path
&gt;&gt; generate_plots_enhanced</code></pre><hr/><h2>🧠 算法原理</h2><h3>DM-OFDM 双模式调制</h3><p><strong>核心思想</strong>：不再有"空闲"子载波，改用两种不同调制阶数区分索引信息。</p><p><strong>比特分配</strong>：</p><p>$$
p_{DM} = p_1 + k \cdot \log_2 M_1 + (n-k) \cdot \log_2 M_2
$$</p><p><strong>典型配置</strong>：</p><ul><li>Mode 1: 16-QAM (高阶)</li><li>Mode 2: QPSK (低阶)</li><li>通过星座差异区分索引</li></ul><h3>CI 坐标交织原理</h3><p><strong>核心思想</strong>：将复数符号的 I/Q 分量分离后置换，使相邻符号的 I/Q 经历不同衰落系数。</p><p><strong>交织公式</strong>：</p><p>$$
X_{CI}[i] = X_I[i] + j \cdot X_Q[(i+d) \mod N]
$$</p><p><strong>分集增益</strong>：</p><p>$$
G_d = 2 \cdot (n - k + 1)
$$</p><hr/><h2>📁 项目结构</h2><pre><code class="text">P2_性能优化包/
├── 📂 dm/                      # DM-OFDM 双模式调制
│   ├── dm_modulator.m          #   🚀 DM 发射端
│   └── dm_demodulator.m        #   🚀 DM 接收端
│
├── 📂 ci/                      # CI 坐标交织
│   ├── ci_modulator.m          #   CI 交织调制器
│   └── ci_demodulator.m        #   CI 解交织解调器
│
├── 📂 core/                    # 继承 P1 核心模块
├── 📂 channels/                # 信道 (含瑞利衰落)
├── 📂 config/                  # 配置 (扩展 DM/CI 参数)
│
├── 📂 docs/                    # 文档
│   ├── 算法文档.md              #   📘 DM/CI 原理推导
│   ├── 代码文档.md              #   📒 接口说明
│   └── 项目文档.md              #   📗 本文档
│
├── generate_plots.m            # 📊 基础 BER 曲线
└── generate_plots_enhanced.m   # 📊 双模式星座图 + 分集增益</code></pre><p><strong>代码统计</strong>：</p><ul><li>📄 20+ 个核心 MATLAB 文件</li><li>📝 2000+ 行精炼代码</li><li>💬 100% 中文详细注释</li></ul><hr/><h2>🎬 仿真演示</h2><h3>一键运行</h3><pre><code class="matlab">&gt;&gt; cd packages/P2_性能优化包
&gt;&gt; setup_path
&gt;&gt; generate_plots_enhanced</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597807" alt="p2_ber_compare.png" title="p2_ber_compare.png"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597808" alt="p2_diversity_gain.png" title="p2_diversity_gain.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597809" alt="p2_dm_constellation.png" title="p2_dm_constellation.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597810" alt="p2_spectrum_compare.png" title="p2_spectrum_compare.png" loading="lazy"/></p><hr/><h2>📦 您将获得</h2><table><thead><tr><th align="left">内容</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">📁 <strong>完整源码</strong></td><td align="left">DM + CI 双技术完整实现</td></tr><tr><td align="left">📖 <strong>原理文档</strong></td><td align="left">双模式调制、坐标交织数学推导</td></tr><tr><td align="left">🚀 <strong>100%频谱</strong></td><td align="left">DM-OFDM 无空闲子载波设计</td></tr><tr><td align="left">📊 <strong>分集验证</strong></td><td align="left">瑞利衰落信道下 CI 增益对比</td></tr><tr><td align="left">🔧 <strong>灵活星座</strong></td><td align="left">可配置 M1/M2 调制阶数组合</td></tr><tr><td align="left">📡 <strong>可视化</strong></td><td align="left">自动生成双模式星座图</td></tr></tbody></table><hr/><h2>🛒 获取方式</h2><p>本文代码仅为核心片段，完整版工程已整理好。 关注公众号 【<strong>3GPP仿真实验室</strong>】进行获取。</p><h2>📚 参考文献</h2><ol><li><strong>T. Mao et al.</strong> (2017): "Dual-Mode Index Modulation Aided OFDM." <em>IEEE Access</em>, vol. 5.</li><li><strong>E. Başar</strong> (2015): "OFDM with Index Modulation Using Coordinate Interleaving." <em>IEEE Wireless Commun. Lett.</em>, vol. 4, no. 4.</li><li><strong>M. Wen et al.</strong> (2016): "Index Modulation Aided Subcarrier Mapping for Dual-Mode OFDM." <em>IEEE Trans. Commun.</em>, vol. 65, no. 12.</li></ol>]]></description></item><item>    <title><![CDATA[要给 OCR 装个脑子吗？DeepSeek-OCR 2 让文档不再只是扫描 小白狮ww ]]></title>    <link>https://segmentfault.com/a/1190000047597817</link>    <guid>https://segmentfault.com/a/1190000047597817</guid>    <pubDate>2026-02-06 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如何形容现在市面上普遍的 OCR 呢？可能你已经习惯了它的「固执」——无论文档布局多复杂，它总是老老实实从左到右、从上到下扫一遍。遇到双栏论文还好，碰上跨页表格或者公式脚注混排，输出结果往往乱得让人头疼。这不是识别不准，而是理解方式出了问题。</p><p>今年 1 月 DeepSeek 团队推出的 DeepSeek-OCR 2 换了个思路，它不再把文档当成一张平面图，而是尝试理解这篇文章应该先读什么。新设计的 DeepEncoder V2 架构引入了因果流机制：视觉编码器看完整个页面后，由专门的查询模块决定阅读顺序——标题优先于正文，表格注释紧跟数据，公式按逻辑展开而非按位置罗列。</p><p>结果很直接。OmniDocBench 最新测试中，这套方案把整体准确率推到了 91% 以上，公式识别的提升尤为明显。更实用的是，它输出的 Markdown 已经带着层级结构，省去了大量后期整理的功夫。</p><p>参数规模控制在单卡能跑的级别，token 上限可调，重复生成的情况也比上一代少了近三分之一。对于需要批量处理文档的场景，这意味着可用性的大幅提升。</p><p>当一个模型能够同时看懂版式、识别文字并直接输出结构化结果，文档数字化的目标就不再只是「能认字」，而是「能理解」。DeepSeek-OCR 2 正是在这一方向上的一次重要尝试。</p><p><strong>教程链接：</strong> <strong><a href="https://link.segmentfault.com/?enc=vOBM7hulHWZ%2BSXDpbhIRUg%3D%3D.bkaPW3TsPezYvHWeMBLcUxY25qrlxQJcbut%2Ftdq5rnU%3D" rel="nofollow" target="_blank">https://go.openbayes.com/NOdm2</a></strong></p><p>使用云平台: OpenBayes</p><p><a href="https://link.segmentfault.com/?enc=ohR9%2BXgR5FP47k8gzuPpnA%3D%3D.CAm4uMzFFklUnTbp0dmVH4jS2PjenVm1ecshMHPkUjIuGyFNi9js%2BxGJuXP2BQ2G" rel="nofollow" target="_blank">http://openbayes.com/console/signup?r=sony_0m6v</a></p><p>首先点击「公共教程」，找到「DeepSeek-OCR 2：视觉因果流」，单击打开。</p><p><img width="723" height="543" referrerpolicy="no-referrer" src="/img/bVdnROZ" alt="" title=""/></p><p>页面跳转后，点击右上角「克隆」，将该教程克隆至自己的容器中。</p><p><img width="723" height="540" referrerpolicy="no-referrer" src="/img/bVdnRO0" alt="" title="" loading="lazy"/></p><p>在当前页面中看到的算力资源均可以在平台一键选择使用。平台会默认选配好原教程所使用的算力资源、镜像版本，不需要再进行手动选择。点击「继续执行」，等待分配资源。</p><p><img width="723" height="541" referrerpolicy="no-referrer" src="/img/bVdnRPa" alt="" title="" loading="lazy"/></p><p><img width="723" height="538" referrerpolicy="no-referrer" src="/img/bVdnRPd" alt="" title="" loading="lazy"/></p><p>若显示「Bad Gateway」，这表示模型正在加载中，请等待约 2-3 分钟后刷新页面即可。</p><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnRPe" alt="" title="" loading="lazy"/><br/><strong>使用步骤如下：</strong></p><ol><li>页面跳转后，点击左侧 README 页面，进入后点击上方「运行」。</li></ol><p><img width="723" height="541" referrerpolicy="no-referrer" src="/img/bVdnRPf" alt="" title="" loading="lazy"/></p><ol start="2"><li>点击运行后等待加载模型与初始化</li></ol><p><img width="723" height="540" referrerpolicy="no-referrer" src="/img/bVdnRPg" alt="" title="" loading="lazy"/></p><ol start="3"><li>待运行完成，即可点击右侧 API 地址跳转至 demo 页面。</li></ol><p><img width="723" height="542" referrerpolicy="no-referrer" src="/img/bVdnSwG" alt="" title="" loading="lazy"/></p><ol start="4"><li>上传所需要的 JPG/PNG 格式的图片或 PDF 文档。</li></ol><p><img width="723" height="503" referrerpolicy="no-referrer" src="/img/bVdnRPj" alt="" title="" loading="lazy"/></p><ol start="5"><li>上传完成后点击运行，稍等片刻右侧结果框生成纯文本结果。</li></ol><p><img width="723" height="241" referrerpolicy="no-referrer" src="/img/bVdnSwS" alt="" title="" loading="lazy"/><br/><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=FQ9U6vgoAxXYvEl7XDAtkA%3D%3D.08unyvdwzHCeOZiwPbtbFtgcr5XNoYLew3JBAEubX7w%3D" rel="nofollow" target="_blank">https://go.openbayes.com/NOdm2</a></em></strong></p>]]></description></item><item>    <title><![CDATA[《Render Graph与光追API融合应用指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597395</link>    <guid>https://segmentfault.com/a/1190000047597395</guid>    <pubDate>2026-02-06 18:18:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>URP以轻量化、跨平台为核心诉求，在资源调度上追求极致精简，适配移动端、主机等多终端的硬件限制；而HDRP则聚焦高清渲染，在光照计算、材质表现上堆砌复杂逻辑，专为高端PC与次世代主机打造。这种定位差异导致两条管线在核心架构、资源管理、效果实现上形成难以逾越的壁垒，开发者往往需要为不同管线单独构建内容、适配逻辑，既增加了开发成本，也让跨平台体验的一致性大打折扣。共享Render Graph与统一光线追踪API的出现，并非简单的功能叠加或参数调优，而是对渲染逻辑的深层重构，其核心在于构建一套脱离管线专属限制的通用渲染语言。这种语言让HDRP的高品质光照计算不再依赖专属的管线架构，而是能通过Render Graph的资源适配与调度优化，以符合URP性能基线的方式落地；同时，URP的跨平台灵活性也不再局限于基础渲染能力，而是能通过统一光线追踪API，承载HDRP级别的复杂场景光照交互。在长期的技术探索中发现，管线间的差距本质上是资源管理逻辑与效果计算范式的割裂—URP为追求效率简化了资源依赖解析，HDRP为实现品质强化了专属计算模块，而共享Render Graph通过对渲染流程的节点化抽象，将资源分配、Pass调度、依赖解析等核心逻辑抽离为独立层，让两条管线能基于同一套底层规则管理资源；统一光线追踪API则打破了光照计算的管线专属限制，让实时光线追踪从HDRP的“高端配置”转变为可根据硬件能力动态适配的“通用功能”。这种转变要求开发者跳出“为URP添加高清功能”或“为HDRP做性能裁剪”的传统思维，转而从渲染本质出发，让两条管线基于同一套核心逻辑，按需组合渲染模块，实现从移动端到高端PC的无缝能力伸缩，最终达成“性能与品质并行不悖”的渲染目标。</p><p>共享Render Graph的核心价值，在于构建了一套标准化的渲染资源语义映射体系，让URP与HDRP能精准理解彼此的资源描述规则，从而实现跨管线的资源复用与流程互通，彻底改变了传统管线中资源壁垒林立的局面。在传统开发模式中，URP为适配移动端硬件，采用紧凑的纹理压缩格式与精简的缓冲布局，而HDRP为追求高清表现，使用高精度纹理与复杂的缓冲结构，这种差异导致同一材质资源在两条管线中需要重复构建适配逻辑，不仅增加了开发工作量，还容易出现资源不一致的问题。共享Render Graph通过抽象资源的核心属性描述与使用场景，将资源的具体实现细节与上层渲染逻辑解耦—无论管线对资源的精度要求、压缩格式有何差异，都能通过统一的资源接口进行调用，Render Graph则在底层自动完成适配转换。例如，在处理复杂场景的多Pass渲染时，HDRP为实现全局光照计算生成的高精度光照贴图，可通过Render Graph的资源适配层，自动转换为URP可高效采样的压缩格式，无需额外添加格式转换Pass，既减少了性能开销，又保证了光照效果的一致性；同样，URP针对移动端优化的纹理流式加载逻辑，也能被HDRP复用，在高清场景中根据硬件内存情况动态加载纹理资源，有效降低内存占用压力。更重要的是，Render Graph的节点化架构让渲染流程具备了模块化重组能力，HDRP中包含的环境光遮蔽、屏幕空间反射、体积雾等复杂后处理链路，可拆分为独立的功能节点，URP可根据自身性能预算，选择性启用核心节点，省略高精度计算步骤，无需重新设计整套后处理管线。这种模块化复用不仅大幅降低了两条管线的功能差距，还提升了开发效率—开发者只需维护一套核心渲染流程节点，即可通过Render Graph的适配逻辑，自动适配两条管线的性能与品质需求，让URP的渲染效果向HDRP靠拢，同时保持自身轻量化的核心优势。在实际技术探索中还发现，Render Graph的资源依赖解析能力，能有效解决跨管线渲染中的资源冲突问题，例如当两条管线同时调用同一材质资源时，Render Graph会根据当前管线的渲染上下文，自动分配对应的资源实例，避免出现资源竞争或格式不兼容的情况，进一步强化了管线间的协同能力。</p><p>统一光线追踪API的关键突破，在于实现了光照计算的范式归一，让URP与HDRP能基于同一套光线行为描述逻辑，达成光照效果的一致性与性能的差异化适配，彻底改变了此前两条管线光照表现泾渭分明的局面。在此之前，HDRP的光线追踪依赖专属的光照计算管线，支持复杂的光线反弹、材质交互与全局光照采样，能呈现出逼真的阴影、反射与折射效果，但计算开销巨大，仅能在高端硬件上运行；而URP受限于性能预算，无法承载完整的光线追踪计算，仅能通过屏幕空间近似算法模拟简单光照效果，导致两条管线的光照表现存在本质差距，同一场景在不同管线中呈现出截然不同的视觉质感。统一光线追踪API通过抽象光线的发射、相交、着色等核心行为，构建了一套与管线无关的光照计算模型，让光线追踪的核心逻辑脱离管线专属实现，成为一套可灵活适配的通用能力。在实际应用中，这套API会根据管线的性能目标与硬件能力，动态调整计算精度与采样策略：在HDRP中，光线可支持多轮反弹与复杂材质采样，充分发挥高端PC与次世代主机的计算潜能，呈现出电影级的光照质感；在URP中，则通过一系列智能化优化—如光线采样策略动态调整，优先采样对视觉影响显著的区域；反弹次数自适应控制，根据场景复杂度与硬件性能动态调整反弹轮次；加速结构简化，采用更紧凑的空间划分算法—在保证光照效果合理性的前提下，将计算开销控制在移动端与中端PC可承受的范围。这种适配并非简单的参数削减，而是基于硬件能力的智能决策，例如在移动设备上，API会自动将全局光线追踪转为局部关键区域的光线查询，结合屏幕空间信息补全光照细节，让URP的光照表现既符合性能要求，又能无限接近HDRP的视觉质感；在中端PC上，则可启用有限次数的光线反弹，平衡效果与性能。此外，统一API还实现了光照数据的跨管线互通，HDRP中烘焙的光线追踪加速结构，可通过API的适配层转换为URP可高效使用的简化版本，减少重复计算开销，让两条管线在光照计算上实现能力同源，进一步缩小了视觉差距。</p><p>场景描述体系的统一，是缩小URP与HDRP差距的重要支撑，共享Render Graph与统一光线追踪API共同构建了一套可跨管线解析的场景语义规范，让复杂场景的描述不再依赖特定管线的专属逻辑，实现了场景资源的一次创建、多管线复用。在传统开发流程中，HDRP的复杂场景通常包含大量高精度几何信息、分层材质属性与全局光照参数，这些信息往往针对HDRP的渲染架构进行优化，无法直接被URP解析，导致同一场景在两条管线中需要重新配置—URP需简化几何模型、削减材质层数、调整光照参数，不仅耗时耗力，还容易导致场景效果失真；而共享Render Graph通过对场景元素的结构化描述，将几何数据、材质属性、光照信息等拆分为独立的语义单元，每条管线可根据自身能力解析对应的语义层级，无需对场景资源进行破坏性修改。例如，HDRP中使用的多层材质，包含基础颜色、粗糙度、金属度、次表面散射等多个属性层，在URP中，Render Graph会通过语义适配，自动提取基础颜色、粗糙度等核心属性，忽略次表面散射等高精度细节，同时保留材质的核心视觉特征，让材质在URP中既符合性能要求，又能保持与HDRP一致的视觉风格；而URP中的简化几何模型，在HDRP中可通过API自动补充细节层次，如添加高模细节贴图、启用几何细分，满足高清渲染需求。统一光线追踪API则进一步强化了场景的光照交互一致性，无论是URP的轻量化场景还是HDRP的高精度场景，光线与物体的相交判定、材质反射计算都遵循同一套语义规则，确保在不同管线中，光照对场景氛围的影响保持一致—例如同一光源照射下，物体的阴影形状、反射强度、颜色衰减在两条管线中呈现出高度统一的效果，避免了跨管线体验的割裂感。这种场景语义的统一，让开发者无需为两条管线单独构建场景资源，只需维护一套核心场景描述，Render Graph与光线追踪API会自动完成适配转换，大幅降低了跨管线开发的复杂度；同时，场景资源的复用也让URP的场景表现力得到显著提升，原本只能在HDRP中呈现的复杂场景细节，如今可通过语义适配在URP中高效呈现，进一步缩小了两条管线的视觉差距。</p><p>着色器生态的协同演进，是弥合URP与HDRP差距的关键环节，共享Render Graph与统一光线追踪API为两条管线提供了可互通的着色器开发框架，让高品质着色逻辑能在两条管线中高效复用，彻底改变了此前着色器开发“管线专属”的局面。在此之前，HDRP的着色器支持复杂的次表面散射、屏幕空间反射、多层材质混合等高级效果，这些效果的实现依赖HDRP专属的光照计算管线与资源调度逻辑；而URP的着色器受限于性能预算，往往只能实现基础的PBR光照计算，导致同一材质在两条管线中视觉差异显著—HDRP中材质表现细腻、光影过渡自然，而URP中材质效果单薄、细节缺失，严重影响了跨管线体验的一致性。共享Render Graph通过模块化着色器设计，将着色逻辑拆分为独立的功能单元，每个单元可根据管线的性能与画质需求，动态调整计算复杂度，实现“一套逻辑、多端适配”。例如，HDRP中使用的PBR着色逻辑，可拆分为基础光照计算、高级材质交互、全局光照融合等模块：在HDRP中，可启用全部模块，实现复杂的材质表现；在URP中，则可选择性启用基础光照计算模块，同时通过统一光线追踪API补充关键光照细节（如高精度反射、软阴影），让URP的PBR表现兼具性能与质感，与HDRP的视觉差异大幅缩小；而HDRP也可复用URP中针对移动端优化的纹理采样模块，通过更高效的采样算法提升高清场景的纹理加载效率，减少性能开销。这种模块化设计不仅实现了着色逻辑的跨管线复用，更让着色器的扩展能力大幅提升—新的着色效果只需开发一次，即可通过Render Graph的适配层自动适配两条管线的渲染架构，无需针对每条管线重复开发。在实际技术实践中发现，这种着色器生态的协同，让URP的材质表现力得到了质的飞跃：原本只能在HDRP中实现的复杂材质交互，如布料的漫反射衰减、金属的镜面反射变化，如今可通过统一API与模块化着色逻辑，在URP中以适配性能的方式呈现；同时，HDRP的着色器也因复用了URP的优化模块，在保持高品质的同时，资源占用与计算开销显著降低，实现了“品质不打折、性能更出色”的目标。</p><p>技术融合的深层价值，在于构建了渲染管线的弹性演进体系，让URP与HDRP不再是相互割裂、各自为战的发展路线，而是基于同一技术底座的差异化表达，实现了两条管线的双向赋能与协同升级。在这套体系下，两条管线的核心能力不再局限于初始定位，而是能随着底层技术的迭代同步升级：URP能持续吸收HDRP的高品质渲染技术，通过Render Graph的资源优化与统一API的性能适配，将其转化为自身的轻量化实现，不断提升跨平台场景的视觉表现力；而HDRP也能借鉴URP的跨平台优化经验，将移动端的高效资源调度、轻量化计算逻辑迁移过来，提升高清场景的资源利用效率与硬件适配范围。</p>]]></description></item><item>    <title><![CDATA[《Android瘦LTO与Swift集成层启动优化实战指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597399</link>    <guid>https://segmentfault.com/a/1190000047597399</guid>    <pubDate>2026-02-06 18:17:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Android的瘦LTO构建绝非传统编译优化的简单升级，而是通过对符号依赖的精准画像与模块关联的动态重构，在保留代码逻辑完整性的前提下，实现编译产物的结构化精简—它不再对全量代码进行无差别优化，而是聚焦启动阶段的核心执行路径，筛选出必须即时加载的关键符号与依赖单元，剥离非必要的冗余代码与关联引用，让应用启动时的代码加载体积与解析耗时实现双重压缩。而Swift重写Apple集成层的核心价值，在于用原生语言的语义特性替代跨语言适配的中间桥接链路，让集成层与Apple系统底层API形成直接的能力对接，消除启动过程中因语言转换、接口适配带来的延迟损耗。这两项技术的联动优化，并非单端独立的性能修补，而是跨平台架构下编译逻辑与集成层设计的深度协同—Android端通过瘦LTO优化启动时的代码加载效率，减少CPU在初始化阶段的计算压力；Apple端借助Swift的运行时优势压缩集成层的初始化链路，降低内存分配与系统调用的延迟，双端形成互补的优化闭环，从编译产物到运行时执行的全流程破解启动性能与跨平台兼容性的核心矛盾。这种优化思路跳出了“单点调优”的传统框架，聚焦跨端启动的本质痛点，通过编译层与集成层的双向革新，让启动性能的提升具备可复制的方法论与规模化落地的可能，为复杂跨平台应用的性能升级提供了全新的技术路径。</p><p>瘦LTO构建的核心竞争力，在于其对编译优化的精准化与高效化革新，它摒弃了全量LTO模式下资源密集型的全局优化逻辑，转而采用分层处理与关键路径聚焦的优化策略，在保证启动性能提升的同时，规避了全量优化带来的编译周期延长问题。在实际优化实践中，瘦LTO的落地需要先完成启动链路的全景解构—通过对应用启动流程的逐环节分析，明确初始化阶段必须加载的核心模块、服务依赖与调用关系，建立启动关键路径的可视化图谱。在此基础上，针对性配置瘦LTO的优化粒度：对于启动时即时初始化的核心服务，如基础配置加载、权限校验、核心功能初始化等模块，进行深度优化处理，包括合并重复调用逻辑、消除无效依赖引用、优化函数执行链路，让代码执行更紧凑高效；对于启动后才按需加载的功能模块，如非核心业务组件、设置页面、辅助工具等，则保持基础编译状态，仅进行必要的符号精简，避免过度优化带来的资源消耗。这种差异化优化策略，既确保了启动关键路径的加载效率，又控制了整体编译开销。在复杂应用场景中，瘦LTO还能与编译缓存机制形成高效协同—通过缓存优化后的中间产物，在后续迭代构建中仅对变更模块进行增量优化，大幅缩短编译周期，同时确保每次构建的优化效果一致性，让启动性能的提升具备稳定可复现的特性。这种精准化的编译优化思路，打破了“优化效果与编译效率不可兼得”的固有认知，实现了编译产物精简、加载效率提升、编译周期可控的三重增益，成为Android端启动性能优化的核心支撑。</p><p>Swift重写Apple集成层的优化逻辑，本质是通过语言原生特性与系统生态的深度耦合，重构跨平台能力的适配链路，彻底替代传统依赖中间桥接层的实现模式，从根源上消除跨语言适配带来的启动损耗。传统跨平台应用的Apple集成层，往往为了兼容多语言调用逻辑，引入大量的接口转换代码、数据格式适配模块与中间调度层，这些冗余链路在启动阶段会产生显著的性能开销—数据在不同语言类型间的转换消耗内存与CPU资源，中间层的调度延迟拉长了初始化周期，同时增加了系统调用的不确定性。Swift作为Apple生态的原生语言，具备与系统底层API的天然适配优势，能够直接调用核心系统能力，省去中间转换环节，让集成层的初始化逻辑更贴合系统的运行时调度机制，实现更高效的能力衔接。实践过程中，重写工作需聚焦两个核心维度：一是集成层的语义对齐，在保持跨平台核心能力一致性的前提下，用Swift的原生语法重构适配逻辑，最大化利用语言的内存管理特性—例如通过值类型优化减少启动时的内存分配与释放操作，避免引用计数带来的额外开销；利用函数派发优化提升调用效率，让核心接口的响应速度更快捷。二是初始化流程的拆分与延迟加载，将集成层的功能模块按启动优先级进行划分，仅保留核心能力的即时初始化，如基础配置适配、系统权限对接等必须在启动阶段完成的逻辑，而将非核心的适配功能，如统计上报、第三方服务对接等，通过懒加载机制延迟到启动完成后执行，进一步压缩启动耗时。这种原生适配的思路，让集成层从启动流程中的“阻滞点”转变为“助推器”，在保证跨平台兼容性的同时，实现了启动性能的质的飞跃。</p><p>Android瘦LTO构建与Swift重写Apple集成层的协同优化，核心在于构建覆盖跨端启动全流程的性能优化闭环，让双端的优化策略形成互补效应，而非孤立的单端升级。Android端通过瘦LTO构建，削减了启动时的代码加载体积与解析耗时，减少了CPU在初始化阶段的计算压力，让核心服务能够更快完成启动准备；Apple端借助Swift重写的集成层，压缩了跨语言适配的中间链路，优化了内存分配效率与系统调用延迟，让集成层的初始化更高效。这种双端协同并非简单的功能叠加，而是基于跨平台应用启动共性逻辑的深度适配—无论是Android的代码加载流程，还是Apple的集成层初始化链路，本质上都是对启动资源的调度与利用，两项技术分别从编译端与运行端切入，形成覆盖“编译产物优化-代码加载加速-集成层初始化精简-系统能力对接高效”的全流程优化体系。实践中，协同优化的落地需要先统一双端的启动性能优化目标，明确核心指标的基准线，例如启动完成时间、初始化阶段的CPU占用、内存峰值等，再根据双端的技术特性制定差异化的优化策略：Android端侧重通过瘦LTO实现编译产物的精简化，缩短代码加载与解析路径，同时优化启动时的资源调度优先级；Apple端聚焦通过Swift的原生优势压缩集成层的初始化链路，减少中间环节的性能损耗，提升系统API的调用效率。通过这种协同设计，跨平台应用能够在双端同时获得启动性能的跃升，避免单端优化导致的用户体验失衡，让不同设备上的启动流程都能保持流畅高效，真正实现跨端启动体验的一致性与高性能。</p><p>启动性能的深度优化，离不开对技术细节的精准把控与场景化的动态适配，瘦LTO构建与Swift重写的落地过程，并非一成不变的标准化流程，而是需要根据应用的实际场景与架构特点进行灵活调整。对于瘦LTO构建而言，优化粒度的选择是关键—过粗的优化会导致启动关键路径的优化不充分，无法达到预期的性能提升效果；过细的优化则可能引入不必要的编译开销，延长构建周期，甚至影响代码的稳定性。因此，在实际操作中，需要借助启动链路分析工具，精准定位每个模块在启动阶段的加载耗时、依赖关系与资源占用情况，建立模块级别的性能画像，再针对性配置优化范围：对启动时首先加载的核心框架，如基础库、路由管理、核心服务等，进行最大程度的优化，合并重复符号，消除循环依赖，优化函数执行逻辑；对后续按需加载的功能模块，如非核心业务组件、多媒体处理、扩展功能等，则采用轻量级优化策略，仅保留必要的符号与依赖，避免过度优化带来的资源消耗。在Swift重写集成层的过程中，集成层的拆分逻辑同样需要贴合应用的启动流程，将必须在启动阶段完成的适配逻辑，如基础配置同步、系统权限申请、核心能力对接等，与可延迟的功能解耦，通过懒加载机制将非必要的适配逻辑延迟到启动完成后执行。同时，需充分利用Swift的编译优化特性，如模块间的接口精简、无用代码自动剔除、编译期常量折叠等，让集成层的产物体积更小巧，加载更快速。这种场景化的精准优化，避免了“一刀切”的优化模式带来的局限性，让每项技术的优势都能在关键场景中充分发挥，实现启动性能的最大化提升，体现了技术优化从“广谱适配”到“精准赋能”的进阶思维。</p><p>瘦LTO构建与Swift重写Apple集成层的优化实践，其长远价值远不止于启动性能的即时提升，更在于为跨平台应用构建了可扩展、可迭代的性能优化体系与技术底座。瘦LTO带来的编译链路优化思路，不仅适用于启动性能的提升，还能延伸到应用运行时的内存占用控制、CPU效率优化与功耗降低—通过持续优化编译产物的结构，让代码执行更高效，资源利用更合理，为应用全生命周期的性能表现奠定坚实基础。而Swift重写的集成层，凭借语言的原生优势与系统兼容性，大幅降低了后续功能迭代的适配成本与维护难度——Swift与Apple系统的深度耦合，让集成层能够快速响应系统版本的更新与API的迭代，无需频繁进行跨语言适配调整；同时，原生代码的可读性与可维护性更强，减少了后续迭代中的技术债务。</p>]]></description></item><item>    <title><![CDATA[百度一见×赣南师大：多模态视觉扎根赣南，共筑产教融合新标杆 百度一见 ]]></title>    <link>https://segmentfault.com/a/1190000047597441</link>    <guid>https://segmentfault.com/a/1190000047597441</guid>    <pubDate>2026-02-06 18:16:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，赣南师范大学代表团莅临百度，双方正式签署校企合作战略协议。这不仅是一场强强联手的签约，更是前沿AI技术与深厚学术积淀的一次“握手”。当“AI for Science”遇上“产教融合”，百度与赣南师大正联手开启智能时代复合型人才培养的新篇章。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597443" alt="图片" title="图片"/><br/>在座谈环节，双方达成高度共识：在智能时代，AI素养已不再是人才的“加分项”，而是“必选项”，产教融合势在必行。百度一见产品部总经理朱名发详细分享了一见在多模态大模型领域的战略布局，以及在能源、制造、连锁、运输等行业的产业实践。“要把最前沿的技术，转化为课堂上的生产力。” 在热烈的氛围中，赣南师范大学党委常委、副校长罗序中与百度代表双方签署协议。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597444" alt="图片" title="图片" loading="lazy"/><br/>随后，校方代表团走进百度展厅，近距离感受百度文心大模型赋能千行百业的实战场景。从实验室的算法到产业实践里的深度应用，双方对“AI+教育”的未来达成了高度共识。朱名发总经理强调：“大模型时代，具备AI素养、能熟练运用AI工具提升效率的复合型人才，已成为企业的首选。我们希望通过合作，让学生在校期间就掌握‘AI生产力’，赋能在就业市场具备核心竞争优势。”<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597445" alt="图片" title="图片" loading="lazy"/><br/>罗序中副校长对此深表认可。他表示，作为江西省“双一流”建设高校，赣南师大拥有扎实的AI学科基础，百度一见在视觉管理领域的深厚积淀，将为学校科研创新注入强劲动力。双方将通过产教融合，加速成果转化，联合培养懂产业、精技术的实战型人才。拒绝纸上谈兵，直击地方痛点。 依托国家脐橙工程技术研究中心等国家级平台，双方明确将“赣州脐橙智能化种植”与“赣州电子制造”作为首批科研攻关方向。通过百度一见的多模态专业视觉技术赋能，双方将合力打造具有全国影响力的应用标杆，真正将产教融合的实践“写”在赣南大地的田间地头与工厂车间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597446" alt="图片" title="图片" loading="lazy"/><br/>签约只是起点，赋能才是目标。未来，百度一见将持续以技术创新为核心，深度融入赣南师大的教学与科研土壤。从实验室的创新火花，到产业界的落地成果，双方将共同探索AI赋能实体经济的新路径。当“AI新范式”扎根老区沃土，一场关于人才、科研与产业的化学反应，正在发生！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597447" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[精灵潜入C++,莲花咒语显神奇 李兴球 ]]></title>    <link>https://segmentfault.com/a/1190000047597482</link>    <guid>https://segmentfault.com/a/1190000047597482</guid>    <pubDate>2026-02-06 18:15:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>看这一行长长的代码：</p><pre><code class="C++">while(1)r.bgcolor("black").pensize(5).speed(0).color(r.heading()).circle(100,90).left(90).circle(100,90).left(90).right(20);</code></pre><p>主要就是这一行代码，画了一幅美妙的莲花图案。下面是完整的，C++精灵库画莲花的代码：</p><pre><code class="C++">#include "sprites.h"  //包含C++精灵库 
Sprite r;      //建立角色叫r
 
int main(){        //主功能块 
 
  while(1)r.bgcolor("black").pensize(5)
          .speed(0).color(r.heading())
          .circle(100,90).left(90)
          .circle(100,90).left(90).right(20);
 
   return 0;    //返回0
}</code></pre><h3>神仙对话泄天机</h3><p>哪吒（手持乾坤圈）：“俺是哪吒三太子，刚刚听闻有位小魔法师用几行代码画出了一朵美轮美奂的莲花。那莲花的花瓣颜色还会随他的笔转向而不断变换，真是神奇！你可知道他是如何做到的？”</p><p>太上老君（手持拂尘）：“此乃C++精灵库的妙用也。那小魔法师创建了一个名为r的角色，就像我身边的童子一样，然后在main函数里用了一个永不停歇的while循环，让r不停地舞动乾坤。”</p><p>哪吒：“你这葫芦里卖的什么药？快讲讲r是怎么画莲花的？”</p><p>太上老君：“那小魔法师在循环里让r做了好多动作。他先把r的背景色设为黑色，就像天庭的黑夜一样深邃。接着把笔画粗细调粗到5个单位，笔速设为0，意味着笔走如飞，一点都不拖沓。”</p><p>哪吒：“嘿嘿，俺这乾坤圈也重达千斤，画笔画粗些倒也般配。那他还做了什么？”</p><p>太上老君：“他把画笔的颜色设置为r.heading()，也就是根据r当前的方向来取颜色。这就好比r在不停地旋转，每转一个角度，颜色就变一变，仿佛r的心情在变，颜色也跟着变。”</p><p>哪吒：“这颜色还会变？那r是怎么转的呢？”</p><p>太上老君：“r画了两个半径100的圆弧，每次转90度。具体来说，先画了一个90度的圆弧，然后左转90度，再画另一个90度的圆弧，又左转90度，然后右转20度。如此循环往复，就像你在打旋子一样，一圈一圈地转。”</p><p>哪吒：“这不是和我用乾坤圈画圈一样吗？那最后r会不会停下来？”</p><p>太上老君：“那小魔法师在循环里没有停下来的意思，while(1)就是无限循环。”</p><p>哪吒：“原来如此！这C++精灵库真像一位多才多艺的画匠，寥寥数笔就能画出五彩斑斓的莲花。而且它的命令和Python的turtle库差不多，对于喜欢Python的孩子来说，学这个C++库就像换了个平台继续玩耍，真是一举两得！”</p><p>太上老君：“哈哈，哪吒你说得对！C++精灵库让孩子们在学习编程时，既可以延续熟悉的图形命令，又能领略C++的强大功能，确实是非常值得学习的库。”</p><p>哪吒：“俺这就回去告诉师傅，让他也教教我C++精灵库，说不定俺也能画出更漂亮的莲花呢！”</p><p>太上老君：“好啊，希望你早日成为C++小能手，画出属于你自己的绚丽莲花！”<br/><img width="289" height="293" referrerpolicy="no-referrer" src="/img/bVdnSri" alt="" title=""/></p><h3>代码解析学咒语</h3><p>下面的逐行解释了main函数中while循环内的代码，并说明其作用：</p><p>代码行                  作用<br/>r.bgcolor("black")    设置画笔背景色为黑色。<br/>.pensize(5)            设置画笔粗细为5个像素单位。<br/>.speed(0)            设置画笔移动速度为0（最快速度）。<br/>.color(r.heading())    根据画笔当前方向heading()获取颜色值，并设置画笔颜色。方向值会被转换为色相，从而实现颜色随方向变化。<br/>.circle(100, 90)    以当前位置为圆心，半径100逆时针绘制一个90度的圆弧。<br/>.left(90)            画笔向左旋转90度。<br/>.circle(100, 90)    再次向左绘制一个90度的圆弧。<br/>.left(90)            画笔再次向左旋转90度。<br/>.right(20)            画笔向右旋转20度（调整方向，使下次循环继续）。<br/>上述代码通过链式调用的方式组合了一系列绘图命令，在无限循环中不断重复执行。每次循环中，画笔都会以黑色背景、粗线条、动态颜色绘制两个圆弧，然后旋转方向，如此往复，形成了莲花形状的图案。</p><h3>始作俑者详剖析</h3><p>C++精灵库（Sprite库）是一个基于SDL2库的少儿C++编程教学库，提供了类似Python turtle库的简洁命令，通过绘制图形和制作动画或小游戏创意C++作品来让少年儿童学习C++。它具有以下几个特点和优势：</p><p><strong>简单易学</strong>： 库中的命令与Python turtle的命令非常相似，用法绝大多数一模一样。这使得熟悉Python绘图的用户可以快速上手C++编程。对于少年儿童来说，使用熟悉的命令可以降低学习门槛，激发他们对编程的兴趣。<br/><strong>功能强大</strong>： 虽然命令简单，但C++精灵库基于SDL2库，同时具备C++的强大性能和灵活性。用户可以利用C++的高级特性，如对象、函数和循环，实现更复杂的图形和动画效果。<br/><strong>丰富的图形效果</strong>： 库支持设置画笔颜色、粗细、速度，以及绘制各种图形（直线、圆圈、圆点、圆弧、椭圆等）并且增强了对画笔颜色的一些更精细的控制。比如让颜色渐变的coloradd命令。实际是逐步增加颜色的色相。比如设定颜色的饱和度命令(pensat)，还有设定颜色的明度命令(penvalue) 及洪水填充命令fill等。用户通过组合这些命令，用户可以创造出丰富多彩的图形和动画效果。例如，本示例中通过动态改变画笔颜色，实现了颜色随方向变化的绚丽图案。<br/><strong>拓展与互动性强</strong>： C++精灵库的底痤基于SDL2库，可以完美融入SDL2库的命令，从而方便地响应用户输入（如鼠标点击、键盘按键等）。这使得用该库开发的程序具有更强的交互性，也可以用于游戏和教育应用的开发制作。</p><p>综上所述，C++精灵库是一个非常适合少年儿童学习编程的工具。它将Python turtle的易用性与C++的强大功能相结合，使孩子们在享受编程乐趣的同时，也能逐步掌握C++语言的基本概念和编程技巧。对于培养少年儿童的逻辑思维和创造力，C++精灵库无疑是一个“一箭双雕”的选择。</p>]]></description></item><item>    <title><![CDATA[MaxCompute Autoscale自动弹性功能上线：资源随需而动，成本精打细算 阿里云大数据A]]></title>    <link>https://segmentfault.com/a/1190000047597485</link>    <guid>https://segmentfault.com/a/1190000047597485</guid>    <pubDate>2026-02-06 18:15:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在云原生数据仓库的演进过程中，如何在<strong>保障作业SLA</strong>与<strong>优化资源成本</strong>之间取得平衡，始终是用户关注的核心问题。传统静态资源配置模式难以应对现代数据作业中普遍存在的<strong>突发性、非周期性、不可预测性</strong>负载特征。</p><p>MaxCompute 全新推出 <strong>自动弹性（Autoscale）功能</strong>——基于实时负载感知的秒级弹性扩缩容机制，结合按量计费模型，实现计算资源供给与业务需求的动态对齐。</p><h2>一、背景：从静态预留到智能弹性</h2><p>过去，MaxCompute 用户主要依赖 <strong>包年包月预留资源</strong>：稳定可靠，但缺乏灵活性；面对突发需求，只能提前大量采购，造成大量闲置。</p><p>后来，基于推出的<strong>弹性预留</strong> 模式：用户可自定义时间计划和扩缩规则，适用于有明显周期性波动的场景（如每天凌晨跑批）。但这也要求用户具备较强的运维能力，且难以应对突发或不规则的负载变化。</p><p>现在，MaxCompute 全新推出 <strong>自动弹性（Autoscale）功能</strong> —— 通过系统的负载感知与调度策略，实现“无感扩缩”，填补了<strong>非稳态、高动态</strong>场景下的资源管理空白，真正做到“用多少，付多少”。</p><table><thead><tr><th><strong>资源类型</strong></th><th><strong>扩缩机制</strong></th><th><strong>计费模型</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td>包年包月预留</td><td>固定CU，长期持有</td><td>为购买量付费</td><td>负载稳定、无波动</td></tr><tr><td>弹性预留</td><td>用户自定义时间/CU规则扩缩</td><td>按用户分时配置的固定CU量计费</td><td>周期性波动、峰谷可预测、用户有精细化配置经验</td></tr><tr><td><strong>自动弹性</strong></td><td><strong>系统实时感知负载后自动扩缩容</strong></td><td><strong>按实际用量和使用时长付费</strong></td><td><strong>波动频繁、不可预测，追求成本效率</strong></td></tr></tbody></table><p><img width="723" height="479" referrerpolicy="no-referrer" src="/img/bVdnSrg" alt="" title=""/></p><p>三者可组合使用：以包年包月为基础保障，弹性预留应对可预测高峰，自动弹性兜底突发流量，构建MaxCompute Serverless 弹性资源体系。</p><h2>二、自动弹性核心优势</h2><h3>1. 开箱即用，低运维负担</h3><ul><li>用户只需设置 <code>AutoscaleLimitCU</code>（自动弹性上限），系统自动完成扩缩决策；</li><li>支持一级/二级 Quota 粒度配置，二级 Quota 共享一级自动弹性CU资源池，自动分配。</li></ul><h3>2. 按需供给，按量计费</h3><ul><li>仅对实际使用的自动弹性CU（<code>AutoscaleUsedCU</code>）用量按秒计量，按小时统计出账；</li><li>单价：<strong>0.36元 / (CU·时)</strong>，无需预付，无最低消费。</li></ul><h3>3. 秒级响应，保障作业SLA</h3><ul><li>相比小时级调度窗口，自动弹性支持<strong>秒级资源调整</strong>，有效应对突发作业排队；</li><li>后端基于历史负载与预测模型优化库存保障和资源调度，提升弹性资源可用性。</li></ul><p>⚠️ 注意：自动弹性依赖实时资源库存，无法100%保证极端突发场景下的资源可达性。对于强SLA要求场景（如大促），建议<strong>同步配置弹性预留</strong>作为资源兜底。</p><h2>三、真实场景案例</h2><h3>场景一：突发业务高峰下的作业SLA保障</h3><p>某电商平台客户，日常使用 <strong>50 CU 包年包月Quota</strong>，足以支撑日常数据加工分析任务。但每逢大促，作业量激增3倍，原有资源严重不足，作业排队超2小时，严重影响数据产出时效。</p><p>客户曾评估扩容包年包月Quota至150 CU，但大促仅占全年不到20%的时间，全年多花<strong>约18万元</strong>，长期持有高配资源性价比极低。</p><p><strong>启用 Autoscale 后</strong>：</p><ul><li>设置 自动弹性上限 AutoscaleLimit 为 <strong>100 CU</strong>（即最多可额外使用100 CU自动弹性CU）</li><li>系统在检测到作业队列积压后，<strong>秒级自动扩容，</strong> 动态将可用CU提升至140CU（50 CU包年包月 + 90CU自动弹性），作业完成时间恢复至30分钟内，满足业务SLA要求；</li></ul><p>“以前不敢做大促实时分析，现在敢了，而且花得更少！” —— 客户反馈</p><h3>场景二：替代分时弹性，实现降本增效</h3><p>某金融客户每日需执行大量 T+1 批处理任务，用于全量交易对账、监管报送数据聚合等，长期采用 <strong>分时弹性预留策略</strong>：每日22:00–6:00 时段将 Quota 从 包月预留 50 CU 扩容至 100 CU。</p><p>但时常因业务活动、节假日调休、系统割接活上游产出延迟等，常出现“资源空转”或“容量不足”并存的问题，运维团队需频繁调整弹性计划，但人工干预滞后性强，且易出错。</p><p><strong>切换至 Autoscale 后</strong>：</p><ul><li>设置自动弹性上限 AutoscaleLimitCU 为 <strong>60 CU</strong> ，允许系统在 50 – 110 CU 范围内动态扩缩；</li><li>系统根据实际作业队列动态调整弹性CU，夜间平均仅使用 <strong>30 CU</strong> 自动弹性资源；</li><li>月度弹性费用从分时弹性CU <strong>3780元</strong> （50CU *0.315元/CU*8小时*30天）降至 <strong>2592元</strong>（30CU *0.36元/CU*小时*8小时*30天），<strong>降本32%</strong>，且作业完成时间更稳定。</li></ul><p>“不用再熬夜调配置了，系统自己会‘看饭下菜’！” —— 运维工程师点赞</p><h2>四、快速启用</h2><h3>概念说明</h3><p>自动弹性上限CU（AutoscalelimitCU）：指用户为Quota设置的弹性CU资源总上限。当该值 &gt; 0 时，则为启用自动弹性功能，系统可在此上限范围内按实际负载自动扩缩容。自动弹性使用CU（AutoscaleUsedCU）：指在启用自动弹性后，Quota中实际消耗的自动弹性CU资源使用量。系统将根据作业负载自动调整CU用量，并按此实际CU使用量计费。</p><h3>使用须知</h3><p><strong>前提条件</strong>：必须已购买<strong>包年包月计算资源Quota</strong>；<strong>计费单位</strong>：CU·时，按秒采样、按小时聚合；<strong>自动弹性CU价格</strong>：0.36元 /(CU*时)；<strong>计费公式：</strong>每小时的费用 = 该小时自动弹性CU用量（单位：CU*时）× 自动弹性CU价格。</p><h3>谁适合用自动弹性？</h3><p>✅ <strong>业务负载波动频繁、难以预测</strong>（如营销活动、临时分析） <br/>✅ <strong>希望保障作业性能，同时避免资源浪费</strong><br/>✅ <strong>已有包年包月Quota，想进一步补充/优化弹性资源</strong></p><p>登录 MaxCompute 控制台 → Quota管理 → 编辑基础配置 → 设置 <strong>AutoscaleLimitCU</strong></p><p>即可开启智能弹性之旅！</p><p>更多说明文档请参考 <a href="https://link.segmentfault.com/?enc=Cw56NZRL1%2FIksOGpPOQnKA%3D%3D.tWXs5gRJUyBbis92lMaxth27Zm9cSIPfsxTjlS%2BcgEfqEKgP4sgB%2BAjiT4y%2BMgRYWN6opqdw%2BF3InOT%2BLwe8d2b9l02RzIltkdrfD6nl4EOknshGdP5CFboJAoRQSsauuQrLYUXU6h1UJm%2BItYd7yfLfMPNrt6hsWDlAAa3UgtY%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/maxcompute/use-cases/auto-elastic-usage-best-practices?spm=a2c4g.11174283.help-menu-search-27797.d\_0</a></p><h2>五、总结</h2><p>自动弹性不是简单的“资源扩容”，而是 MaxCompute 在<strong>智能调度、成本治理、SLA保障</strong>三位一体方向上的重要演进。它让资源管理从“静态规划”走向“动态协同”，真正实现“<strong>用多少，付多少；要多少，给多少</strong>”。</p><p>欢迎您的试用并反馈您的生产实践。我们将持续优化弹性调度算法与资源保障能力，助力企业构建更高效、更经济的云原生数据基础设施。</p>]]></description></item><item>    <title><![CDATA[TDengine TSDB 3.4.0.0 上线：虚拟表、流计算性能显著提升，安全能力全面进阶 TD]]></title>    <link>https://segmentfault.com/a/1190000047597492</link>    <guid>https://segmentfault.com/a/1190000047597492</guid>    <pubDate>2026-02-06 18:14:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在实际生产环境中，时序数据库面临的挑战早已不只是“能不能存数据”，而是如何在复杂查询、高并发计算、多源接入和安全合规要求不断提高的情况下，依然保持稳定、高效和可控。</p><p>近日，TDengine TSDB 发布新版本 3.4.0.0，围绕查询性能、流计算能力、安全体系与数据接入生态进行了系统性增强。本次更新在虚拟表查询、状态窗口计算、流计算性能等核心场景下带来了显著优化，同时补齐了多项安全能力，并进一步扩展了数据订阅、授权服务及主流数据源接入能力。</p><p>本文将为你梳理该版本的主要更新亮点，帮助你快速了解哪些改进能在实际场景中带来更直接的性能提升、更稳定的运行体验，以及更灵活的系统集成方式。</p><h2>重要更新亮点</h2><h3>安全功能全面提升（企业版）</h3><p>通过对身份鉴别、权限控制、审计、传输与存储等关键环节的系统性安全加固，新版本整体安全能力得到显著提升，为安全可靠性测评及等保三级、四级要求提供有力支撑。</p><h4>身份鉴别</h4><p>新版本在身份鉴别与访问控制方面进行了增强，支持强口令策略及密码生命周期管理，引入多因素认证与 TOKEN 认证机制，并完善用户锁定与会话控制能力。同时，系统支持基于 IP 与时间段的访问限制，口令在存储与传输过程中均采用加密保护，进一步提升整体安全性。</p><h4>访问控制</h4><p>新版本完善了基于 RBAC 的权限管理体系，引入系统级与对象级权限划分，内置互斥的 SYSDBA、SYSSEC、SYSAUDIT 系统角色，实现权限制衡与职责分离。同时支持权限与角色的创建、删除、授予与回收，提供标准 GRANT/REVOKE 语法，并支持对象所有者权限转移。在访问控制层面，支持库、表、列等多层级权限控制。</p><h4>安全审计</h4><p>新版本完善了分级审计能力，按粒度分为系统级、集群级、数据库级、子表级、数据级五级审计，支持查询、删除、写入等数据操作审计。审计操作与业务访问相互隔离，强制启用加密存储并设置不少于 5 年的保留策略，相关安全属性不可修改。同时强化审计链路安全与防篡改能力，保障审计数据的完整性与可信性。</p><h4>传输安全</h4><p>新版本完善了传输安全与连接管控机制，采用 TLS 传输加密与 SASL 身份认证的分层架构，保障通信安全。在连接层面，支持按用户配置并发会话数、会话时长及空闲超时等参数，并增强黑白名单访问控制能力。同时引入通信失败监测与异常告警机制，可在异常场景下自动触发告警并临时锁定相关用户/IP。TLS 私钥采用加密存储并支持安全轮换，相关安全操作均可审计，在保障安全性的同时总体性能下降不超过 10%。</p><h4>存储安全</h4><p>新版本完善了存储安全能力，采用分级密钥体系，对配置文件、元数据及时序数据实现透明加密，密钥生成、变更、到期及恢复等过程统一由系统管理，用户方面无感知。核心密钥通过加密通信机制安全传输，并支持国密算法适配，敏感操作需管理员权限并全程留存审计。同时提供加密状态与范围的可观测能力，支持密钥到期告警配置。</p><h4>加密算法</h4><p>新版本增强了加密算法管理能力，新增系统表用于集中查看和管理可用加密算法，覆盖对称加密、非对称加密与散列算法等类型。系统内置国密与国标算法，适配数据加密、密钥交换与完整性校验等多种场景，并支持通过动态链接库方式扩展自定义加密算法，满足不同环境下的算法适配需求。</p><h4>安全函数</h4><p>新版本补充了安全相关内置函数能力，提供数据加密、脱敏、哈希及编码转换等函数，支持国密与国际算法，满足数据存储、传输及查询过程中的安全处理需求。</p><h3>流计算事件窗口新增「子事件窗口」触发机制</h3><p>本次版本在流计算中引入事件窗口的子窗口触发机制，支持为同一事件定义多个开始条件。不同开始条件满足时，可依次触发对应的子事件窗口，系统自动维护父事件窗口的开启与关闭关系；父窗口及各子窗口均可独立触发计算与通知。该能力特别适用于分级告警、状态升级、阈值递进等复杂场景，使事件驱动的流计算逻辑更加贴近真实业务变化过程，而无需通过多条规则或多条流任务进行拆分实现。</p><pre><code class="sql">EVENT_WINDOW(START WITH (start_cond_1, start_cond_2 [,...]) [END WITH end_cond])</code></pre><h3>流计算的资源消耗和计算延迟显著降低</h3><p>新版本在 Nevados 实际业务场景下对流计算引擎进行了针对性优化，显著降低了资源消耗并改善了计算延迟表现。优化后，CPU 平均使用率<strong>由 321.7% 降至 30.3%</strong>，降幅约 <strong>90.6%</strong>；内存平均占用<strong>由 8.65 GB 降至 1.49 GB</strong>，减少约 <strong>82.8%</strong>。同时，流计算的平均处理延迟由原来的<strong>约 1 小时缩短至 5 分钟以内</strong>，整体响应速度提升约 <strong>92%</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597494" alt="" title=""/></p><h3>虚拟表的查询性能优化</h3><h4>投影查询性能优化（虚拟超级表 / 子表）</h4><p>新版本针对虚拟超级表及虚拟子表的投影查询场景，系统对查询路径进行了针对性优化，覆盖包含 <code>tbname</code>、<code>tag</code> 条件、时间过滤以及全量扫描等多种常见查询模式。在包含 <code>tbname</code> 或 <code>tag</code> 条件的查询场景下，查询性能提升最高可达<strong>千倍量级</strong>，显著改善了典型业务查询的响应速度；在全量扫描或单表查询场景中，性能提升相对有限，但仍体现了底层执行与数据访问优化带来的整体收益。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597495" alt="" title="" loading="lazy"/></p><h4>聚合与选择函数查询性能优化（虚拟超级表）</h4><p>新版本针对虚拟超级表在聚合函数与选择函数场景下的查询性能进行了系统性优化，覆盖是否使用 <code>partition by</code>、函数参数是否包含 tag 等多种常见用法。优化后，虚拟超级表在上述典型查询用例中的执行时间由原来的 <strong>68–86 秒</strong> 显著缩短至 <strong>0.088–0.640 秒</strong>，整体性能提升约 <strong>119×–796×</strong>，大幅改善了统计分析与状态类查询的响应效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597496" alt="" title="" loading="lazy"/></p><h4>状态窗口查询性能优化（虚拟超级表 / 子表）</h4><p>针对虚拟超级表及子表在状态窗口计算场景下的性能问题，新版本对窗口判定与计算流程进行了优化，特别用于解决稀疏数据与密集数据混合计算时的效率瓶颈。新机制下，系统可在窗口触发前先提取窗口边界信息，再按策略激活后续计算，并支持按批次或单窗口两种优化策略。在数据分布密集场景下推荐使用批处理策略，在数据分布相对均匀且窗口数量较少的场景下，可选择单窗口策略，其余情况沿用默认策略。</p><h3>查询性能优化及语法增强</h3><h4>新增非相关标量子查询</h4><p>在查询能力方面，新增对非相关标量子查询的支持，子查询可返回单行单列结果并作为常量参与主查询计算与条件判断。</p><h4>状态窗口零状态支持</h4><p>新增状态窗口零状态（zeroth state）能力，可在状态窗口计算完成后，将状态值等于指定零状态的窗口整体排除，不参与后续计算。该机制与通过 <code>WHERE</code> 条件过滤数据不同：零状态是在完整状态窗口判定之后进行过滤，而 <code>WHERE</code> 条件是在数据进入窗口计算之前生效，可用于更精确地区分“无效状态窗口”与“有效状态但需排除的数据”。</p><pre><code class="sql">STATE_WINDOW(col[, extend][, zeroth_state]) [TRUE_FOR(true_for_duration)]</code></pre><h4>多类典型查询与能力边界优化</h4><p>新版本针对多种高频查询场景进行了集中优化，包括 <code>last_row + tags</code> 查询、系统表统计子表数量以及窗口查询能力扩展。优化后，<code>last_row + tags</code> 查询平均耗时由 <strong>25.9 秒</strong> 降至 <strong>0.385 秒</strong>，性能提升约 <strong>68 倍</strong>；基于系统表统计子表数量的查询平均耗时由 <strong>3.824 秒</strong> 降至 <strong>0.003 秒</strong>，同样提升约 <strong>68 倍</strong>。同时，窗口查询不再强制要求包含聚合函数，可仅使用窗口伪列（\_wstart、tbname 等）参与查询；虚拟表支持的最大列数提升至 <strong>32767 列</strong>，且不影响写入与查询性能。</p><h3><strong>XNODE 高可用与负载均衡支持</strong></h3><p>在新版本中，taosX 正式成为 TSDB 的一个内部组件：XNODE，由 MNODE 统一管理并通过 <code>xnoded</code> 调度器进行调度，支持高可用与负载均衡能力。</p><h3><strong>OAuth 2.0 / OIDC 单点登录（SSO）支持</strong></h3><p>新版本新增对 OAuth 2.0 与 OIDC 的单点登录支持，兼容 OAuth 2.0 与 OIDC 1.0 标准 API，并支持基于 OIDC 的端点自动发现。同时提供可配置的自定义 OAuth 2.0 API 接入能力，并支持对 SSO 用户的基础管理功能（部分能力已实现）。</p><h3><strong>KingHistorian  数据源支持</strong></h3><p>新版本新增对 KingHistorian 数据源的支持。KingHistorian 是 Wellintech 于 2006 年推出的工业实时数据库，已在现场运行近 20 年，支持单机最高 200 万标签规模，广泛应用于大规模设备数据采集与实时计算场景。</p><h3><strong>Pulsar 数据源支持</strong></h3><p>新版本新增对 Apache Pulsar 数据源的支持。Apache Pulsar 是一款分布式发布订阅消息平台，支持灵活的消息模型与流式消费方式，可用于消息队列及流处理场景。在使用体验上，Pulsar 数据源的 UI 界面与 Kafka 保持一致，降低多数据源场景下的使用与运维成本。</p><h3><strong>taosgen 发布到 Kafka</strong></h3><p>taosgen 新增对 Apache Kafka 的数据发布能力，支持将生成的数据直接写入 Kafka 主题。Apache Kafka 是一款开源的分布式流处理平台，常用于构建实时数据管道与流式应用，该能力可用于数据生成、测试与流处理场景的联动验证。</p><h3>taosAdapter 功能增强（JSON 写入与查询管控）</h3><p>taosAdapter 新增对 HTTP POST JSON 写入的支持，可接收任意格式的 JSON 数据，并通过 JSONata 进行数据转换，同时支持时间字段解析；同时引入 SQL 查询请求管控能力，支持对 SQL 请求进行拦截，并按用户维度设置并发限制，提升接口访问的可控性与稳定性。</p><h3><strong>C WebSocket（WS）支持 TLS</strong></h3><p>C WebSocket 连接器新增 TLS 支持，实现通信过程的端到端加密，提升数据传输的安全性。</p><h3><strong>OpenTSDB 支持自定义列名、子表名</strong></h3><p>OpenTSDB 接入新增对自定义字段与子表命名的支持，可灵活配置时间戳字段、数值字段以及子表名，提升不同 OpenTSDB 数据模型下的接入适配能力。</p><h2>其他优化</h2><ol><li>TDgpt 的数据补全算法支持任意采样间隔，支持  dtw、dtw\_path、tlcc 等相关性分析函数</li><li>新增 maxSQLLength 设置 SQL 语句的最大长度，最大可为 64M</li><li>虚拟表支持的最大列数提升至 32767 列</li><li>STMT2 对虚拟表查询的支持</li><li>Compact 命令支持 force 选项</li><li>Show connections 命令新增客户端版本号字段</li><li>Show vgroups 命令新增 is\_ready 列</li><li>优化 event\_window 按 tbname 分组查询的效率</li><li>优化子查询做主键过滤条件时的性能</li></ol><p>除此之外，每个版本都会做很多其他的工作，比如 bug 修复、功能优化等等。如果想要了解新版本（时序数据库功能更新）更加详细的发布信息，可以移步至 <a href="https://link.segmentfault.com/?enc=czEQxVZyWx48w2btiaEpEw%3D%3D.xoBwoZUnEAXACQsDhqjGaTE1M%2B6%2FdlWq2eWu0f6V37ZP5Mno4J%2Bd0xyszbXS22zwTpG8l680ID%2BAeu%2FPk81SWA%3D%3D" rel="nofollow" target="_blank">https://github.com/taosdata/TDengine/releases/tag/ver-3.4.0.0</a> 查看发布说明。</p><p>欢迎大家下载使用，也欢迎在评论区提出建议和意见，如有任何问题请及时联系我们获得支持。</p>]]></description></item><item>    <title><![CDATA[工业智能体：从单点自动化到全链路自主决策的进化之路 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047597522</link>    <guid>https://segmentfault.com/a/1190000047597522</guid>    <pubDate>2026-02-06 18:13:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的智能化浪潮中，人们曾一度将AI视为提升效率的“万能工具”——图像识别质检、预测性维护、自动排产……这些单点突破看似亮眼，却始终难以撬动全局。真正的变革，不是让机器学会看图，而是让它学会“思考”整个生产系统的运行逻辑。工业智能体的出现，正是为了弥合这一鸿沟。它不是某个算法模型的简单封装，而是一套能感知、推理、决策并执行的数字生命体，其核心价值在于将工业知识、数据流与业务流程深度融合，形成闭环的自主运行能力。与通用大模型不同，工业智能体必须扎根于车间的振动数据、工艺参数、物料流转节奏之中，它需要理解“为什么这台设备在凌晨三点振动加剧”，而不仅仅是“这组数据异常”。<br/>要实现这种深度嵌入，技术底座必须足够坚实。许多企业试图直接部署智能体应用，却忽略了数据治理、知识沉淀与系统集成的底层工程。工业现场的数据往往碎片化、非结构化，工艺经验散落在老师傅的脑子里，ERP、MES、PLC等系统彼此割裂。真正的工业智能体，必须能打通这些断点，把隐性知识转化为可计算的规则，把分散的系统整合为统一的决策网络。这要求平台不仅提供算法能力，更要具备工业Know-How的封装能力——就像为AI建立一本“懂行的字典”，让它能读懂工程师的意图，也能用生产语言输出建议。这种能力，不是靠堆算力就能获得的，而是长期与产线共处、反复迭代的结果。<br/>在这一领域，广域铭岛的“工业智造超级智能体”提供了一个极具参考价值的范式。其平台通过“数据标准化+知识封装+积木式智能体开发”三位一体架构，让企业能快速构建覆盖研发、生产、物流、服务的智能体矩阵。例如，在某新能源电池企业，当某批次电芯容量波动时，仓储智能体自动关联原材料批次、环境温湿度与设备参数，15分钟内定位到是某台涂布机的张力控制异常，并联动工艺参数自动调整，避免了整批报废。这种“感知—诊断—决策—执行”的闭环，不再依赖人工巡检与会议决策，而是由多个智能体协同完成。类似实践也出现在海外：德国西门子的“数字孪生智能体”在安贝格工厂实现产线自优化，当订单变更时，系统自动重排工艺路径并模拟能耗影响；美国通用电气的Predix平台则通过设备智能体，对燃气轮机进行实时健康评估，提前72小时预测关键部件失效，将非计划停机减少40%。这些案例共同揭示了一个趋势：工业智能体的竞争力，不在于模型多大，而在于它是否真正“懂”这个工厂。</p>]]></description></item><item>    <title><![CDATA[超越跑分：新一代 AI 基准与模型评测的范式转变 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047597524</link>    <guid>https://segmentfault.com/a/1190000047597524</guid>    <pubDate>2026-02-06 18:13:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>人工智能的竞技场上，每一次新模型的发布都伴随着激动人心的基准测试结果。“在 MMLU 上达到 92.5%！”“在 HumanEval 上超越 GPT-4！” 这些头条新闻确实抓人眼球，但敏锐的 AI 开发者们越来越意识到：<strong>数字并不能讲述完整的故事</strong>。当今最前沿的模型评测，正在经历一场从 “单纯跑分” 到 “全面理解” 的深刻转变。</p><p><strong>新基准的崛起：ARC-AGI 与 GPQA 为何与众不同</strong><br/>传统基准数据集如 MMLU、GSM8K 虽然仍有价值，但它们逐渐暴露出局限性 —— 可能被大量纳入训练数据、无法真正衡量推理能力、或与现实世界问题脱节。这正是 ARC-AGI 和 GPQA 等新一代基准引起广泛关注的原因，也成为 <a href="https://link.segmentfault.com/?enc=F8vNixEAmHZVhMOLlD2HNQ%3D%3D.tlaa%2BO%2BDxxhuXdPBGgvp2Wqs8GqhGs109xNDOsfKKSw%3D" rel="nofollow" target="_blank">Smoothcloud 润云</a>构建企业级 AI 评测矩阵的核心参考依据。</p><p>ARC-AGI（Abstract Reasoning Corpus for AGI）由 OpenAI 前研究员 François Chollet 创建，其核心理念直指 AI 系统的要害：泛化能力。ARC-AGI 不测试记忆或模式匹配，而是评估模型在面对全新类型问题时的抽象推理能力。数据集包含一系列基于网格的模式完成任务，每个任务都设计得独一无二，确保模型无法从训练数据中直接回忆答案。这种设计迫使模型必须真正 “理解” 问题背后的抽象规则，而非简单应用已见模式。Smoothcloud 润云正是基于此类核心基准的设计逻辑，为不同行业客户定制化开发了避免 “训练污染” 的专属评测数据集，确保评测结果能真实反映模型在实际业务中的泛化能力。</p><p>GPQA（Graduate-Level Google-Proof Q&amp;A）则走向另一个极端：深度领域专业知识。这个基准由耶鲁大学科学家创建，包含 400 多个涵盖物理、化学、生物学等学科的研究生级别问题。关键之处在于，这些问题被设计为 “谷歌无法直接解答”—— 无法通过简单搜索获得答案，需要深度的学科知识和多步骤推理。GPQA 不仅测试模型的知识广度，更重要的是测试其深度理解和复杂推理链的构建能力。针对这一特性，Smoothcloud 润云已将 GPQA 的评测逻辑融入到金融、生物医药、高端制造等领域的模型评估中，助力企业筛选出真正具备深度行业推理能力的 AI 模型。</p><p><strong>全面评测的艺术：弱点分析比高分更重要</strong><br/>当 Llama 3、GPT-4o 或 Claude 3 等新模型发布时，前沿开发者不再仅仅关注它们在排行榜上的位置，而是深入挖掘模型的能力边界与失败模式 —— 这也是 Smoothcloud 润云为客户提供的核心评测服务方向。</p><ol><li>能力边界的精细测绘高级评测者会进行 “压力测试”：模型在长上下文中的一致性如何？面对对抗性提示的鲁棒性怎样？在不同语言和文化背景下的表现是否均衡？例如，一个模型可能在英语科学问题上表现优异，但在非拉丁语系的诗歌分析中却漏洞百出。Smoothcloud 润云通过自研的多维度压力测试工具，能够为企业精准绘制目标模型的能力边界，甚至细化到不同语种、不同业务场景下的性能表现差异。</li><li/><li>失败模式的系统分类真正的洞察来自分析模型如何失败而非如何成功。失败模式可能揭示：<br/>系统偏差：模型是否过度依赖某些思维模式？<br/>知识断层：在哪些知识领域存在明显盲点？<br/>推理短路：是否倾向于选择表面合理而非真正正确的答案？<br/>Smoothcloud 润云的评测体系中，专门包含 “失败模式归因模块”，不仅能系统分类模型的失败类型，还能结合企业业务场景分析失败背后的潜在风险，为后续优化提供可落地的方向。</li><li>真实世界适用性评估开发者关注模型在特定应用场景中的表现：在代码生成任务中，生成的代码是否考虑了边缘情况？在医学问答中，是否表现出过度自信倾向？这种评估往往通过精心设计的领域特定测试集进行，而非通用基准。Smoothcloud 润云依托海量的行业场景数据，已搭建起覆盖电商、医疗、金融、工业等数十个领域的专属测试集，让模型评测结果直接对接真实业务需求。</li></ol><p><strong>评测方法的创新：从静态测试到动态交互</strong><br/>传统基准如同标准化的多项选择题考试，而新兴评测方法更像是一场对话或合作项目，这与Smoothcloud 润云倡导的 “场景化动态评测” 理念高度契合。</p><p>动态评估框架如 Chatbot Arena 采用众包配对比较，让人类评估者在真实对话中判断模型回答的质量。这种方法的优势在于捕捉模型在开放域交互中的综合表现，包括一致性、有用性和安全性。Smoothcloud 润云已将此类动态评估框架产品化，结合人机协同的评测模式，为企业提供贴近真实用户交互场景的模型评估结果。</p><p>诊断性探针则通过精心设计的提示词，主动探测模型的内部机制和局限性。例如，通过逐渐增加问题复杂性，观察模型性能下降的 “拐点”；或通过语义改写，测试模型是否真正理解概念而非记忆表面模式。Smoothcloud 润云的技术团队还对诊断性探针进行了场景化改造，使其能适配企业的专属业务逻辑，精准探测模型在核心业务环节的表现。</p><p><strong>实践意义：这对 AI 开发者意味着什么？</strong><br/>对于构建和部署 AI 系统的开发者而言，这种评测范式的转变有着直接影响，而 Smoothcloud 润云则成为连接新一代评测理念与企业实际应用的桥梁：</p><p>技术选型更明智：了解模型的特定优势和弱点，有助于为不同应用场景选择最合适的模型。例如，一个在 GPQA 上表现平平但在代码基准上卓越的模型，可能是开发工具的理想选择，但不适合作为科学研究助手。Smoothcloud 润云会基于企业的业务目标，输出模型选型的量化分析报告，避免企业因单纯参考跑分而做出不当决策。</p><p>风险规避更有效：通过弱点分析，开发者可以预先识别模型在特定领域可能产生的错误类型，从而设计防护措施或备用流程。Smoothcloud 润云还会结合行业合规要求，在评测中融入风险预警模块，提前识别模型在数据安全、合规性等方面的潜在问题。</p><p>微调方向更精准：知道模型的失败模式，可以针对性地收集数据、设计微调策略，更高效地提升模型在实际任务中的表现。Smoothcloud 润云能基于评测结果，为企业提供定制化的模型微调方案，包括数据采集方向、微调策略设计等，让模型优化更具针对性。</p><p><strong>未来展望：全面评测的挑战与方向</strong><br/>尽管全面评测的理念日益普及，但仍面临挑战：如何平衡评测的深度与可扩展性？如何设计真正无法被 “训练污染” 的基准？如何量化模型行为的细微差别？Smoothcloud 润云也正围绕这些挑战展开技术探索，力求为企业提供兼具深度与效率的评测服务。</p><p><strong>未来，我们可能会看到更多：</strong></p><p>多模态综合评估：同时测试文本、图像、音频和视频理解能力 ——Smoothcloud 润云已启动多模态评测体系的研发，适配企业日益增长的多模态 AI 应用需求；<br/>长期交互评估：在延长时间尺度上测试模型的记忆和一致性 —— 这也是 Smoothcloud 润云针对客服、智能助手等长期交互场景重点布局的评测方向；<br/>价值观与安全性评估：超越表面无害，深入评估模型的价值对齐程度 ——Smoothcloud 润云已将价值观对齐评测纳入金融、教育等敏感行业的评测标准中。</p><p><strong>结语</strong><br/>在人工智能快速发展的今天，对新模型的评判标准正在从 “有多聪明” 转变为 “在哪些方面聪明，在哪些方面还有局限，以及为什么会这样”。这种转变不仅反映了领域成熟度的提升，也标志着 AI 开发者社区对技术理解的深化。</p><p>Smoothcloud 润云始终认为，真正有价值的 AI 评测，是让企业跳出 “跑分竞赛” 的误区，精准把握模型的独特特征、适用场景和内在局限性—— 这些洞察才是将 AI 技术有效、负责任地应用于现实世界的关键。无论是依托新一代基准构建的定制化评测体系，还是贴合业务场景的动态交互评估，Smoothcloud 润云都在以技术赋能的方式，帮助企业将全面评测的理念落地，让 AI 模型的价值真正在实际业务中释放。在这个意义上，一次由 Smoothcloud 润云助力的深入弱点分析，往往比一个漂亮的跑分数字更能为企业创造长期价值。</p>]]></description></item><item>    <title><![CDATA[Hadoop基础认知——HDFS、YARN、MapReduce在现代体系中的位置与价值 南城 ]]></title>    <link>https://segmentfault.com/a/1190000047597539</link>    <guid>https://segmentfault.com/a/1190000047597539</guid>    <pubDate>2026-02-06 18:12:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>写在前面，本人目前处于求职中，如有合适内推岗位，请加：lpshiyue 感谢。</strong></p><blockquote>HDFS 是海量数据的基座，MapReduce 是批量计算的引擎，而 YARN 是集群资源的调度者——它们共同构成了大数据处理的“古典三位一体”。</blockquote><p>在深入探讨了数据平台的全景与角色分工之后，我们触及了现代数据体系的基石。无论是 OLTP 的实时交易，还是 OLAP 的深度分析，其背后都需要强大的底层基础设施来支撑海量数据的存储与计算。本文将聚焦于大数据领域的奠基者——Hadoop，解析其核心组件 HDFS、YARN 与 MapReduce 的经典架构、协同原理及其在当今技术浪潮中的独特价值。</p><h2>1 Hadoop 的起源与核心命题</h2><p>Hadoop 并非凭空诞生，它源于互联网时代一个根本性的挑战：<strong>当数据规模远超单机极限，我们该如何存储和处理它？</strong></p><p>在 2000 年代初，Google 面临索引整个互联网的难题。其给出的答案是两篇划时代的论文：关于分布式文件系统的 <strong>GFS</strong> 和关于分布式计算的 <strong>MapReduce</strong>。Hadoop 正是这两大思想的开源实现，它要解决的核心问题可以归结为三点：</p><ul><li><strong>数据存储</strong>：如何将 PB 级文件可靠地存储在成千上万台普通服务器上。</li><li><strong>计算能力</strong>：如何将巨大的计算任务拆解，并分发到集群中并行处理。</li><li><strong>资源协调</strong>：如何让多个计算任务共享集群资源，且互不干扰。</li></ul><p>Hadoop 的核心理念是 <strong>“移动计算比移动数据更划算”</strong>。与其将海量数据通过网络传输到计算程序所在的地方，不如将小巧的计算程序发送到数据存储的节点上本地执行。这一理念贯穿于其三大核心组件的设计之中。</p><h2>2 HDFS：分布式存储的基石</h2><p>HDFS 是 Hadoop 的存储基石，它的设计目标非常明确：<strong>一次写入，多次读取</strong>，以流式数据访问模式来存储超大文件。</p><h3>2.1 架构与核心组件</h3><p>HDFS 采用了经典的<strong>主从架构</strong>：</p><ul><li><strong>NameNode</strong>：集群的“大脑”或“总目录”。它负责管理文件系统的<strong>命名空间</strong>（目录树结构）以及所有文件的<strong>元数据</strong>（如文件名、权限、每个文件块对应的 DataNode 列表等）。所有这些元数据都存储在内存中，以实现快速访问。</li><li><strong>DataNode</strong>：集群的“劳动力”。它们负责在本地磁盘上存储实际的数据<strong>块</strong>，并负责块的创建、删除和复制。</li><li><strong>Secondary NameNode</strong>：容易被误解的组件，它<strong>不是</strong> NameNode 的热备。其主要职责是定期合并 NameNode 的镜像文件和编辑日志，协助主节点进行元数据管理，以防日志过大导致重启时间过长。</li></ul><h3>2.2 关键机制与设计哲学</h3><ul><li><strong>分块存储</strong>：HDFS 将大文件切分成固定大小的<strong>块</strong>。在较早的版本中，默认块大小为 64MB，后续版本（如 Hadoop 2.x 及以后）通常默认为 <strong>128MB</strong>。分块的好处在于，一个大型文件可以分布存储在集群的多个节点上，从而为并行处理奠定了基础。同时，它也简化了存储系统的设计，无需管理巨大的文件，而只需管理固定大小的块。</li><li><strong>多副本机制</strong>：为了保证数据的可靠性，HDFS 默认将每个数据块复制<strong>3份</strong>，并遵循一种<strong>机架感知</strong>策略将它们分布在不同节点甚至不同机架上。这极大地增强了数据的容错能力。</li><li><strong>数据写入流程</strong>：客户端写入数据时，HDFS 会建立一个<strong>管道</strong>。数据块会依次从客户端流向管道中的第一个 DataNode，再由第一个 DataNode 传给第二个，以此类推。这种线性传输方式有效利用了每个节点的网络带宽。</li></ul><h3>2.3 现代体系中的价值</h3><p>尽管对象存储（如 AWS S3）如今常被用作 HDFS 的替代品，但 HDFS 在特定场景下仍有其不可替代的价值：</p><ul><li><strong>高性能计算场景</strong>：当计算框架需要极低延迟的数据本地性访问时，HDFS 由于数据直接存储在计算节点本地磁盘上，往往能提供比通过网络访问对象存储更高的吞吐量。</li><li><strong>混合负载环境</strong>：在同时运行多种批处理作业的集群中，HDFS 可以避免所有任务同时访问外部存储可能带来的带宽瓶颈。</li><li><strong>数据湖的底层存储</strong>：许多企业的数据湖架构中，HDFS 依然扮演着存储原始数据和热数据的核心角色。</li></ul><h2>3 MapReduce：分布式计算的灵魂</h2><p>MapReduce 是一种编程模型，其核心思想是 <strong>“分而治之”</strong>。它将复杂的计算任务分解为两个阶段：<strong>Map</strong> 和 <strong>Reduce</strong>，使得开发者无需关心分布式计算的底层细节（如网络通信、容错等），只需专注于实现业务逻辑。</p><h3>3.1 核心工作流程</h3><p>以一个经典的词频统计任务为例，其流程如下：</p><ol><li><p><strong>Map 阶段</strong>：</p><ul><li><strong>输入</strong>：每个 Map 任务读取 HDFS 上的一个数据块。</li><li><strong>处理</strong>：对每一行数据，执行用户自定义的 Map 函数。例如，输入 <code>“Hello World Hello”</code>，Map 函数会输出 <code>[("Hello", 1), ("World", 1), ("Hello", 1)]</code> 这样的键值对。</li><li><strong>输出</strong>：每个 Map 任务输出一系列中间键值对。</li></ul></li><li><strong>Shuffle 与 Sort 阶段</strong>：这是 MapReduce 框架最核心且最“神秘”的一步。框架会自动将所有 Map 任务输出的中间结果，<strong>按照键进行分组和排序</strong>，保证相同键的所有值会被发送到同一个 Reduce 任务进行处理。</li><li><p><strong>Reduce 阶段</strong>：</p><ul><li><strong>输入</strong>：经过 Shuffle 后，一个 Reduce 任务的输入可能是 <code>[("Hello", [1, 1]), ("World", [1])]</code>。</li><li><strong>处理</strong>：执行用户自定义的 Reduce 函数，对值列表进行汇总。例如，对 <code>“Hello”</code> 进行求和计算：<code>1+1=2</code>。</li><li><strong>输出</strong>：最终结果写入 HDFS，如 <code>[("Hello", 2), ("World", 1)]</code>。</li></ul></li></ol><h3>3.2 容错与局限性</h3><p>MapReduce 的强大还在于其<strong>容错性</strong>。如果某个节点上的 Map 或 Reduce 任务失败，YARN 会自动在另一个健康的节点上重新启动该任务，因为输入数据在 HDFS 上是有副本的。</p><p>然而，MapReduce 模型也有其<strong>局限性</strong>。由于每个阶段（尤其是 Shuffle）都涉及磁盘 I/O，因此它更擅长<strong>批处理</strong>，而对迭代式计算（如机器学习）和交互式查询的延迟较高。这也催生了 Spark 等内存计算框架的兴起。</p><h2>4 YARN：集群资源的“大管家”</h2><p>在 Hadoop 1.x 时代，MapReduce 自身负责资源管理，这导致集群只能运行 MapReduce 一种计算框架，资源利用率低且孤立。<strong>YARN 的诞生，解耦了资源管理与计算框架，是 Hadoop 从“一套系统”演变为“一个平台”的关键</strong>。</p><h3>4.1 架构与核心组件</h3><p>YARN 同样采用了主从架构：</p><ul><li><strong>ResourceManager</strong>：集群资源的最终仲裁者。它掌管着整个集群的资源（CPU、内存）情况，并负责接收和调度来自客户端提交的应用程序。</li><li><strong>NodeManager</strong>：每个节点上的代理。它负责启动并监控本节点上的资源容器，并向 ResourceManager 汇报本节点的资源使用情况。</li><li><strong>ApplicationMaster</strong>：这是 YARN 设计的精妙之处。<strong>每个应用程序</strong>（例如一个 MapReduce 作业或一个 Spark 应用）都有一个专属的 ApplicationMaster。它负责向 ResourceManager 申请资源，并与 NodeManager 通信来启动和监控具体的任务。这种设计将资源管理的全局视角和应用程序的具体管理分离开来。</li></ul><h3>4.2 工作流程示例</h3><ol><li>客户端向 ResourceManager 提交一个 MapReduce 作业。</li><li>ResourceManager 在一个空闲的 NodeManager 上分配第一个容器，并在其中启动该作业的 <strong>ApplicationMaster</strong>。</li><li>ApplicationMaster 根据作业需求（如需要运行 100 个 Map 任务），向 ResourceManager 申请资源。</li><li>ResourceManager 根据调度策略，在各个 NodeManager 上分配容器。</li><li>ApplicationMaster 与对应的 NodeManager 通信，在分配到的容器中启动 Map 或 Reduce 任务。</li><li>ApplicationMaster 监控所有任务的运行状态，直到作业完成。</li></ol><h3>4.3 现代体系中的核心价值</h3><p>YARN 的价值在于其<strong>通用性</strong>。它本身不关心运行的是 MapReduce、Spark、Flink 还是 Tez。它作为一个<strong>统一的资源管理和调度平台</strong>，允许多种计算框架在同一个集群上共享资源，提高了集群利用率，并简化了运维。在今天，YARN 依然是许多大规模 Hadoop 集群不可或缺的底层调度系统。</p><h2>5 三位一体：协同工作原理与在现代数据生态中的位置</h2><p>HDFS、MapReduce 和 YARN 共同构成了一个完整的闭环。</p><p><strong>协同工作流程</strong>：用户编写的 MapReduce 程序被打成 JAR 包提交给 YARN。YARN 的 ResourceManager 为作业分配 ApplicationMaster。ApplicationMaster 根据输入数据在 HDFS 上的位置（通过询问 NameNode 获得），向 YARN 申请在存储了相应数据块的 DataNode 上启动 Map 任务，以实现“计算向数据靠拢”。Map 任务处理本地数据，Reduce 任务通过网络拉取数据并进行汇总，最终结果写回 HDFS。</p><p><strong>在现代数据生态中的位置</strong>：尽管如今 Spark、Flink 等更快速、更灵活的计算框架大放异彩，但 Hadoop 三要素并未过时，而是找到了新的定位：</p><ul><li><strong>HDFS</strong>：依然是许多企业数据湖的<strong>可靠存储底层</strong>，尤其是在需要高吞吐、数据本地性强的场景。</li><li><strong>MapReduce</strong>：作为一种<strong>经典的编程模型</strong>，其思想深刻影响了后续几乎所有的大数据计算框架。在处理超大规模、非迭代的冷数据批量计算时，它依然稳定可靠。</li><li><strong>YARN</strong>：作为<strong>成熟的资源调度器</strong>，在管理由数千节点组成的大型混合负载集群时，其稳定性和资源隔离能力备受青睐。</li></ul><p>可以说，Hadoop 生态系统从“一套特定技术”演变成了“一系列技术选择的基石”。新一代的计算框架大多选择与 HDFS 兼容，并可以运行在 YARN 之上，这本身就是对 Hadoop 核心组件设计价值的肯定。</p><h2>6 总结与展望</h2><p>Hadoop 的核心三要素为解决大数据问题提供了一套经过实践检验的、完整的<strong>基础范式</strong>。HDFS 解决了“数据怎么存”，MapReduce 解决了“计算怎么做”，YARN 解决了“资源怎么分”。它们所体现的<strong>分治、容错、可扩展</strong>的设计思想，至今仍是构建分布式系统的黄金法则。</p><p>理解 Hadoop，不仅是掌握一套工具，更是建立一种应对海量数据挑战的<strong>基础性思维框架</strong>。即使在云原生和实时计算成为潮流的今天，这套框架所解决的存储、计算和调度问题，依然是任何数据平台架构师需要深刻理解的根本命题。</p><hr/><p><strong>📚 下篇预告</strong><br/>《Hive与离线数仓方法论——分层建模、分区与桶的取舍与查询代价》—— 我们将深入探讨：</p><ul><li>🗃️ <strong>数仓分层</strong>：ODS、DWD、DWS、ADS 的职责边界与数据流转设计</li><li>⚖️ <strong>分区策略</strong>：按时间、地域分区的优缺点与数据倾斜规避方案</li><li>🪣 <strong>分桶优化</strong>：桶的数量抉择、数据均匀分布与 JOIN 性能的提升逻辑</li><li>💰 <strong>代价评估</strong>：不同分区与桶策略下的存储、计算成本量化分析</li><li>🔄 <strong>演进路径</strong>：从传统数仓到 Hive 批处理的最佳实践迁移路线</li></ul><p><strong>点击关注，掌握离线数据仓库的构建精髓！</strong></p><blockquote><p><strong>今日行动建议</strong>：</p><ol><li>在本地搭建一个 Hadoop 单机伪分布式环境，亲手体验 <code>hdfs dfs</code> 命令和运行 WordCount 示例程序。</li><li>使用 <code>hadoop fs -put</code> 上传一个文本文件到 HDFS，观察其被分成了几个块。</li><li>通过 YARN 的 Web UI（通常为 <code>http://&lt;resourcemanager-host&gt;:8088</code>）提交一个 MapReduce 作业，直观理解其资源申请和执行流程。</li><li>思考当前业务中是否存在适合用 MapReduce “分而治之”思想处理的离线批量计算任务。</li></ol></blockquote>]]></description></item><item>    <title><![CDATA[物联网设备分布分析需要精准地理信息？支持IPv4IPv6双栈批量解析的IP离线库 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047597554</link>    <guid>https://segmentfault.com/a/1190000047597554</guid>    <pubDate>2026-02-06 18:12:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、物联网设备分布分析，真的“必须”精准地理信息吗？</h2><p>在讨论物联网设备分布之前，很多团队第一步就会接触到类似 <strong>IP数据云IP地址查询</strong>——通过设备日志里的 IP，还原设备大致所在的行政区域，但物联网场景，真的需要“越精细越好”的地理信息吗？</p><h3>物联网和互联网业务最大的不同</h3><p><strong>普通 Web/App：</strong></p><ul><li>用户是“人”</li><li>地理信息更多用于画像、推荐或内容分发</li></ul><p><strong>物联网（IoT）：</strong></p><ul><li>对象是“设备”</li><li><p>地理信息直接影响：</p><ul><li>运维</li><li>网络调度</li><li>合规判断</li><li>成本控制</li></ul></li></ul><p>因此，对IoT来说是<strong>基础数据层的一部分</strong>。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSsm" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析.png"/></p><h3>常见物联网场景，对地理精度的真实需求</h3><table><thead><tr><th>场景</th><th>是否需要精准地理</th><th>说明</th></tr></thead><tbody><tr><td>设备区域分布统计</td><td>国家/省级</td><td>宏观态势、市场决策</td></tr><tr><td>网络质量分析</td><td>省/市级</td><td>排查区域性丢包、延迟</td></tr><tr><td>运维调度</td><td>市/区级</td><td>人员派单、仓储规划</td></tr><tr><td>合规/制裁判断</td><td>国家/地区级</td><td>是否落在受限区域</td></tr><tr><td>边缘节点规划</td><td>城市级</td><td>CDN/边缘计算部署</td></tr></tbody></table><p><strong>IoT并不追求“街道级定位”</strong> ，而是<strong>稳定、可批量、可解释的行政区级定位</strong>。<br/>这也是为什么在真实项目中，很多团队会优先选择基于 IP 的地理解析方案，而不是复杂的设备侧定位能力。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSqd" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析、.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析、.png" loading="lazy"/></p><h2>二、为什么物联网更适合用「IP离线库」，而不是在线接口？</h2><p>这是很多IoT团队在早期容易低估的一点。<br/>即便你已经验证过某些在线 IP 地址查询接口（比如在测试环境用过 <strong>IP数据云IP地址查询</strong>）。</p><h3>物联网的三个现实约束</h3><p><strong>① 数据量极大</strong></p><ul><li>设备数：几十万/几百万</li><li>日志规模：每天TB级</li><li>实时接口调用成本极高</li></ul><p><strong>② 网络环境复杂</strong></p><ul><li>专网/内网</li><li>边缘节点</li><li>海外或弱网环境</li></ul><p><strong>③ 稳定性和可控性优先</strong></p><ul><li>运维分析≠实时用户交互</li><li>离线可复现，比“快几毫秒”更重要<br/>这种背景下IP离线库几乎是IoT场景的解法</li></ul><h3>离线库在IoT场景的优势</h3><ul><li>批量解析（百万级IP无压力）</li><li>本地运行（无外部依赖）</li><li>结果可追溯（版本固定）</li><li>成本可控（一次部署，多次使用）<br/>适合：</li><li>日志回放</li><li>周/月度设备分布报告</li><li>异常区域复盘</li></ul><h2>三、IPv4/IPv6双栈是刚需</h2><h3>为什么 IoT 里 IPv6 占比越来越高？</h3><ul><li>设备数量爆炸，IPv4不够用</li><li>运营商网络天然支持IPv6</li><li>NB-IoT、5G、蜂窝网络大量走IPv6</li><li>海外部署（尤其亚太、欧洲）IPv6更常见</li></ul><p>现实很多IoT平台中，IPv6设备占比已经达到30%～50%。</p><h3>双栈支持在离线库中的技术含义</h3><p>一个合格的IP离线库，至少需要做到：</p><ul><li>同时支持IPv4/IPv6</li><li>统一输出结构（国家/省/市/ASN 等）</li><li>支持批量解析</li><li><p>不需要维护两套SDK、两套逻辑<br/>否则，在IoT场景中维护成本会变高。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSr6" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析1.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析1.png" loading="lazy"/></p><h2>四、在真实物联网系统中，IP地理数据通常怎么用？</h2><h3>典型流程示例</h3></li><li>设备上报日志（包含IP）</li><li>日志落库/对象存储</li><li>离线任务（Spark/Flink/MapReduce）</li><li>调用IP离线库做 <strong>批量解析</strong></li><li><p>生成：</p><ul><li>设备区域分布</li><li>国家/省级设备数量</li><li>区域异常告警</li><li>合规统计报表</li></ul></li></ul><h2>五、唠叨（给技术/产品都能用）</h2><ul><li>物联网设备分布分析需要地理信息，但不是“GPS 级”，而是<strong>稳定、可批量的行政区级精度</strong></li><li>IP离线库天然适合IoT大规模、离线、可复现的数据分析</li><li>IPv4/IPv6双栈批量解析，已经是物联网分析的<strong>基础能力，而不是可选项</strong></li><li><strong>IP数据云IP地址查询</strong>是IoT数据体系中的基础功能</li></ul>]]></description></item><item>    <title><![CDATA[电子签章为教育行业赋能 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047597567</link>    <guid>https://segmentfault.com/a/1190000047597567</guid>    <pubDate>2026-02-06 18:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>教育行业的电子签章需求正随着数字化转型的加速而日益凸显，它不仅是技术工具的应用，更是提升运营效率、保障合规性与优化用户体验的重要环节。我们就教育行业电子签章的核心需求、应用场景及实施要点进行一番浅析：</p><p>1．教育行业电子签章的核心需求</p><p>1) 流程高效化</p><p>Ø 减少纸质文件的打印、邮寄、存储成本，加快合同、证明等文件的签署周期。</p><p>Ø 实现远程签署，打破地域限制，适合在线教育、跨校区合作等场景。</p><p>2) 合规与法律效力</p><p>Ø 需符合《电子签名法》及相关教育法规要求，确保电子签章的法律效力。</p><p>Ø 学历证书、成绩单、录取通知书等重要文件需具备防篡改、可追溯的特性。</p><p>3) 安全与隐私保护</p><p>Ø 学生、教职工的身份信息及敏感数据需加密保护，防止泄露。</p><p>Ø 支持实名认证（如身份证、人脸识别），确保签署主体真实性。</p><p>4) 集成与兼容性</p><p>Ø 与现有教务系统、OA平台、学籍管理系统等无缝对接，避免数据孤岛。</p><p>Ø 支持多终端（PC、移动端）操作，适应多元化的使用场景。</p><p>5) 管理与审计需求</p><p>Ø 全流程留痕，便于追踪签署状态、时间、IP等信息。</p><p>Ø 教育机构需对签署文件进行统一归档与管理，满足审计要求。</p><p>2．典型应用场景</p><p>1) 招生与入学</p><p>Ø 在线报名表、录取通知书、入学协议的电子签署。</p><p>Ø 家长同意书（如课外活动、体检授权）的远程签署。</p><p>2) 教学与管理</p><p>Ø 成绩单、学历学位证书的电子签发与验证。</p><p>Ø 科研项目合同、学术合作协议的签署。</p><p>Ø 教职工劳动合同、保密协议等人事文件在线签署。</p><p>3) 学生事务</p><p>Ø 奖学金/助学金申请、实习协议、交换生项目的文件签署。</p><p>Ø 宿舍协议、校园安全责任书等后勤管理文件。</p><p>4) 合作与对外事务</p><p>Ø 与校企合作单位、供应商的合同签署。</p><p>Ø 学术论文投稿、知识产权协议等科研相关文件。</p><p>3．实施电子签章的关键要点</p><p>1) 选择合规可靠的服务商</p><p>确保服务商具备权威认证（如CA机构资质）、符合国密标准，并提供法律支持。</p><p>2) 定制化流程设计</p><p>针对不同文件类型（如录取通知 vs 劳动合同）设计差异化的签署流程与权限控制。</p><p>3) 用户培训与体验优化</p><p>针对教职工、学生、家长等不同用户群体提供操作指导，简化签署步骤。</p><p>4) 长期存证与司法服务</p><p>选择支持区块链存证、与公证机构对接的服务，增强文件的法律保障。</p><p>5) 安全与灾备方案</p><p>部署数据加密、防篡改技术，并建立文件备份与容灾机制。</p><p>4．挑战与趋势</p><p>1) 挑战：</p><p>Ø 传统教育机构对纸质文件的惯性依赖，需推动观念转变。</p><p>Ø 跨区域、跨国场景下的法律差异（如留学生文件需符合国际认可标准）。</p><p>Ø 老年家长或偏远地区用户的数字使用能力差异。</p><p>2) 趋势：</p><p>Ø AI融合：通过智能校验自动识别文件关键信息，减少人工审核。</p><p>Ø 区块链存证：学历证书等关键文件的防伪与全球验证。</p><p>Ø 生态整合：与智慧校园、数字孪生平台深度融合，形成全链路数字化管理。</p><p>5．实施路径</p><p>1) 需求调研：梳理校内高频签署场景，确定优先级（如从录取通知书开始试点）。</p><p>2) 方案选型：对比服务商的合规性、集成能力、成本及行业案例。如：北京安证通、契约锁、法大大等</p><p>3) 试点运行：选择单一部门或场景进行小范围试点，收集反馈并优化流程。</p><p>4) 全面推广：逐步扩大至全校范围，配套制定电子文件管理制度。</p><p>5) 持续优化：定期评估效率提升效果，关注技术更新与法规变化。</p><p>通过电子签章的规范化应用，教育机构可显著提升行政效率、降低运营成本，同时构建更安全、透明的数字化管理体系。如需进一步探讨具体场景的解决方案，可提供更多细节信息</p>]]></description></item><item>    <title><![CDATA[TDengine 2026 路线图来了：从 TSDB 到 IDMP，存储、分析与 AI 的下一步 T]]></title>    <link>https://segmentfault.com/a/1190000047597578</link>    <guid>https://segmentfault.com/a/1190000047597578</guid>    <pubDate>2026-02-06 18:10:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果说前几年，工业企业谈数据，更多是在解决“能不能采、能不能存”；那这两年，越来越多客户开始问的是另一类问题：</p><ul><li>数据规模上来之后，系统还能不能稳？</li><li>复杂分析越来越多，查询是不是一定会慢？</li><li>业务想用数据，但每次都要找技术同事，能不能更“自动”一点？</li><li>AI 说了这么多年，真正想落到工业场景里的，到底应该怎么做？</li></ul><p>这些问题，其实正是 TDengine 在规划 2026 年产品路线时反复讨论的出发点。</p><p>最近，我们正式对外发布了 <strong>TDengine TSDB &amp; TDengine IDMP 的 2026 年年度路线图</strong>。相比“多加几个功能”，这份路线图更想解决的是一件事：在真实、长期、复杂的工业数据场景里，系统如何继续向前演进。</p><h2>TDengine TSDB｜2026 年路线图</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597580" alt="" title=""/></p><p>从规划可以看到，TDengine TSDB 在 2026 年的重点，并不只是“更快”，而是<strong>让复杂场景变得可控</strong>。</p><p>一方面，查询能力持续向真实工业分析靠拢：关联查询、子查询、自然周期窗口、累计窗口、窗口函数……这些能力背后，都是越来越复杂的分析逻辑需求。</p><p>另一方面，虚拟表与流计算被反复强化，意味着计算正在前移：不再只是“数据进库 → 再算”，而是让系统本身承担更多实时与持续计算的职责。</p><p>而在更底层，引擎、缓存、多副本、资源管控的优化，则是在为<strong>长期稳定运行</strong>打基础。</p><h2>TDengine IDMP｜2026 年路线图</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597581" alt="" title="" loading="lazy"/></p><p>TDengine IDMP 于 2025 年 7 月正式发布。从一开始，它就不是一个“补充型工具”，而是围绕工业数据长期使用所设计的平台级产品。</p><p>在过去半年多的迭代中，IDMP 始终保持着“快迭代、小步快跑”的节奏：依托 TDengine TSDB 的高性能时序数据底座，持续强化工业数据的<strong>标准化管理与情景化分析</strong>，并在此基础上进一步拓展 <strong>AI 原生能力</strong>，让数据从“可管理”走向“可决策”。</p><p>这些更新更多聚焦在<strong>语义一致性、分析可复用性、视图沉淀与 AI 使用门槛</strong>等方面，为后续复杂场景与规模化落地打下稳定基础。</p><p>2026 年，IDMP 的演进重点开始从“能力补齐”转向“体系化建设”：</p><ul><li>在延续既有 AI 能力的基础上，引入更完整的事件体系与根因分析能力；</li><li>强化面板、仪表板与分析之间的组合、继承与钻取关系；</li><li>同时在平台层面补充可观测性、权限与数据治理能力，使分析与 AI 能力能够长期、稳定地运行在真实工业环境中。</li></ul><p>从 2025 到 2026，TDengine IDMP 正在从“能力集合”走向“可长期演进的工业数据平台”。</p><h2>写在最后</h2><p>工业数据的下一阶段不是“有没有数据”，而是系统能不能承载更复杂的分析、更长周期的运行，以及更高层次的智能应用。2026 年，TDengine 正在为这一阶段提前铺路。</p><p>如果你正在使用 TDengine，或正在评估下一代工业数据平台，这份路线图，或许能帮你更早看清接下来一年的演进方向。同时，我们也欢迎你基于真实场景和实际需求反馈建议，一起把这份 Roadmap 打磨得更加“落地”。</p>]]></description></item><item>    <title><![CDATA[全景解析 KaiwuDB 数据库智能体工具 KaiwuDB ]]></title>    <link>https://segmentfault.com/a/1190000047597601</link>    <guid>https://segmentfault.com/a/1190000047597601</guid>    <pubDate>2026-02-06 18:09:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>1. KAT 背景介绍</strong></h2><p>在 AI 技术飞速发展的当下，"让数据库更智能、更易用"成为行业核心探索方向，尤其国产数据库使用过程中，普遍存在学习成本高、运维流程复杂、特色功能上手难度大等痛点，各类手册文本繁杂，不利于用户快速落地使用。</p><p>针对这一现状，KaiwuDB 在 AI 与数据库融合领域持续深耕，形成了<strong>DB for AI</strong> 与 <strong>AI for DB</strong> 两大核心布局，既打造了适配多场景的预测分析引擎，又推出了 <strong>KAT - KaiwuDB 数据库智能体工具</strong>，构建起完整的数据库 AI 赋能体系，其中 KAT 作为 AI for DB 领域的核心成果，重点解决用户操作、运维、研发中的各类痛点。</p><p>本期直播核心围绕数据库智能体工具 KAT 展开，全面拆解其背景价值、架构功能及实操效果，为 DBA、研发工程师、数据科学家等技术从业者，提供 AI 与数据库融合的全新解决方案，助力降低数据库使用门槛、提升全流程工作效率。</p><h2><strong>2. KAT 架构和功能</strong></h2><h3><strong>2.1 KAT 核心架构</strong></h3><p>KAT 采用先进的 <strong>Multi-Agent（多智能体）架构</strong>，规避单 Agent 系统处理复杂任务时的效率低、准确性不足等短板，通过"分工协同、各司其职"的设计，实现复杂任务的高效拆解与落地。</p><p><strong>• Main Agent（主智能体）</strong>：作为核心调度中枢，负责接收用户请求、识别核心需求、拆解复杂任务，并分配给对应 Subagent，同时监控任务执行进度、整合最终结果。</p><p><strong>• Subagent（子智能体）</strong>：具备独立决策与执行能力，聚焦特定任务类型，包括 NL2SQL 转换、性能分析、数据分析、安装部署、知识库管理、故障诊断等，通过多轮迭代完成复杂需求。</p><p><strong>• 核心组件</strong>：包含 Agent UI、Agent Server、Task Manager 三大组件，Agent UI 提供图形化交互与配置能力，Agent Server 以 RESTful API 形式提供 Agent 功能，Task Manager 支持定时任务与 Webhook 通知。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597603" alt="" title=""/><br/>KAT 架构图</p><h3><strong>2.2 KAT 核心功能特性</strong></h3><p>KAT 具备五大核心功能特性，全面覆盖数据库操作、运维、分析全流程，大幅提升工作效率：</p><p><strong>•自然语言交互</strong>：用户可通过对话完成各类数据库相关任务，无需掌握复杂操作指令。</p><p><strong>• 智能问题诊断</strong>：快速定位 KaiwuDB 使用过程中的问题，提供精准解决方案。</p><p><strong>• 性能调优</strong>：依托 KaiwuDB 专家知识，针对性优化数据库性能，提升运行稳定性。</p><p><strong>• 自动化任务</strong>：支持定时巡检、备份、报表生成等运维任务，简化日常工作。</p><p><strong>• 数据管理与分析</strong>：支持自然语言查询、趋势预测及可视化展示，让分析结果更直观。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597604" alt="" title="" loading="lazy"/></p><p>KAT 功能特性</p><h3><strong>2.3 KAT 针对不同角色的赋能</strong></h3><p><strong>• DBA</strong>：提供故障预防、巡检自动化、智能告警、部署自动化等能力，解放重复劳动，聚焦高价值工作。</p><p><strong>• 研发工程师</strong>：支持自然语言生成 SQL、辅助业务设计、快速熟悉业务逻辑，大幅提升研发效率。</p><p><strong>• 数据科学家</strong>：提供智能数据预处理、分析预测、结果可视化等支撑，助力高效挖掘数据价值。</p><h2><strong>3. KAT 相关演示</strong></h2><p>视频演示详见：<a href="https://link.segmentfault.com/?enc=Cu28Em9Q2gko29KATyK6LQ%3D%3D.AMNPUHIrNjM9xseER9bWzVwg%2BMN3xiwJUxRwjMgLTYmWjvbiN8cD9dDr9kVU9shG00E2Y%2FCqf69p8wjAN17Zag%3D%3D" rel="nofollow" title="全景解析 KaiwuDB 数据库智能体工具" target="_blank">全景解析 KaiwuDB 数据库智能体工具</a></p>]]></description></item><item>    <title><![CDATA[网络攻击的“集中靶区” JoySSL解析如何以数字证书为行业构筑安全防御屏障 完美的铁板烧 ]]></title>    <link>https://segmentfault.com/a/1190000047597614</link>    <guid>https://segmentfault.com/a/1190000047597614</guid>    <pubDate>2026-02-06 18:08:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着数字化进程加快，各行各业的运营方式、服务模式以及核心资源正快速向数字领域迁移。这一重大变革改变了风险的布局，网络攻击不再是随机分布，而是如精准制导武器般，集中瞄准那些拥有高数据价值、业务中断影响大的行业领域，或是安全防御较为薄弱的目标。识别这些重点目标的特征及其弱点，不仅是理解风险的关键，更是一种基础性的威胁应对策略。JoySSL 有关专家指出，通过全面的行业分析可以得出结论，网络攻击的针对性让安全防护的基础设施重要性再次凸显。而数字证书的作用，早已超越了为网站提供加密保障的单一功能。在应对针对性极强的网络威胁时，能够凭借先进的加密技术与身份验证系统，有效抵御网络攻击，逐渐成为各行业在数字架构中构建“基础通信安全免疫”的必备工具，同时也随着数字威胁的升级，发挥着日益重要的作用，在现阶段不可或缺。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnStz" alt="" title=""/></p><p><strong>SSL证书应对网络攻击的行业偏好</strong></p><p>金融领域是数字经济的“金库”，极易遭受黑客网络攻击，包括勒索软件、供应链攻击、API数据泄露等。SSL证书保护通信安全，确保数据在传输时不被非法截取。医疗领域则是存储个人健康信息的“保险库”，数据在黑市价格极高。数字证书保障医疗数据的绝对安全，避免因钓鱼攻击导致泄露。</p><p>电子商务是海量交易与消费者数据的关键平台，业务高度数字化，是不法分子的高度关注对象。SSL证书通过加密通信保护用户登录和支付环节的隐私安全，增强支付页面的信誉，平衡安全性与业务增长需求。</p><p><img width="723" height="475" referrerpolicy="no-referrer" src="/img/bVdnStA" alt="" title="" loading="lazy"/></p><p>教育领域则被称为开放网络中的“知识存储库”，拥有海量的学生及教师个人信息，包含创新性研究成果和知识产权数据。数字证书保护网络教学平台及科研数据资源的访问安全，为开放式学术环境建立传输通信的安全基准，维护知识产权及个人隐私。</p><p><strong>数字证书共通价值直击行业痛点</strong></p><p>即使各行各业面临的安全威胁各不相同，SSL证书的技术解决路径却能精准解决共同的基础安全难题。无论是金融、医疗、电商还是教育领域，均以“加密传输”和“身份溯源”为核心基础，满足多项法律法规的要求，为数字化合法经营提供技术保障。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnStB" alt="" title="" loading="lazy"/></p><p>JoySSL技术专家解释道，借助SSL证书的强加密技术及服务器身份验证功能，可在通信阶段设置防护屏障，确保数据安全传输，提升钓鱼攻击难度。凭借安全类标识，建立品牌信任资产，提升企业竞争优势。</p><p><strong>建立可信基础抵御不确定网络威胁</strong></p><p>在数字化发展的过程中，安全问题既存在又分布不均。应对风险的关键，在于为所有数字交互构建基础且广泛的信任基石。虽然无法针对特定威胁，但却是行业稳定运作不可或缺的基本条件。</p>]]></description></item>  </channel></rss>