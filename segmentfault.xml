<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[为什么服务器这么慢？深度解析与解决方案 landonVM ]]></title>    <link>https://segmentfault.com/a/1190000047575427</link>    <guid>https://segmentfault.com/a/1190000047575427</guid>    <pubDate>2026-01-27 17:09:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><p>第一次上手 <a href="https://link.segmentfault.com/?enc=U01QfUT0Em5hOmLEO4Kj%2BQ%3D%3D.s9pnAtE8RDLaAOfGZBgwnz%2FzFATQGamuOaZZ%2Fxx2bmdbX%2BPP2NEaVy4ouX93bm6J" rel="nofollow" target="_blank">服务器</a>时，很多站长都会被配置单上 100Mbps 甚至 1Gbps 的大带宽所震撼——比起国内常见的 3Mbps “小水管”，这简直是带宽自由的代名词。然而，现实往往会泼来一盆冷水：几百 KB/s 的龟速下载、加载缓慢的网页、以及断断续续的 SSH 延迟，都在无情拆穿高带宽的幻象。<br/>明明水管够粗，为何水流却断断续续？ 问题的根源不在于服务器的“肺活量”，而在于那条跨越万里的“跨境高速公路”。国际出口的拥堵、运营商的 QOS 限制、以及那些绕遍大半个地球的离奇路由，才是吞噬带宽的幕后黑手。本文将带你拨开迷雾，深度解析访问受限的底层原因，并呈上一套即学即用的优化方案。</p><h3>服务器 为什么会速度慢？</h3><p>当你访问一台服务器时，数据并不是简单地从你的电脑 “飞” 到那么简单。理解这个问题的关键在于：</p><p>$$
网络传输速度 = 带宽 × 传输效率 
$$</p><p>你的 服务器有 100Mbps 带宽，但这只是 “管道” 的粗细。真正决定速度的，是数据在这个管道里能以多快的效率流动。就像一条宽阔的高速公路，如果到处是收费站、红绿灯、修路路段，车再多、路再宽也跑不快。<br/>跨境传输的三大障碍</p><ul><li>线路问题 ：路由绕行、运营商互联质量差、物理距离导致的延迟</li><li>带宽限制 ：国际出口容量瓶颈、跨境上行带宽被严格限制、高峰期拥堵</li><li>传输效率 ：丢包率高影响 TCP 传输、流量审查带来的额外延迟、QoS 策略降低优先级</li></ul><p>这些问题相互交织，共同拉低了你的实际访问速度。那么，如何定位到底是哪个环节出了问题？又该如何针对性地解决？接下来我们将逐一分析每个具体问题，并提供切实可行的解决方案。</p><h3>开启 BBR 加速：通过优化 TCP 传输提升速度</h3><p>对于已经在使用服务器的用户来说，如果感觉速度不够理想，应该先从系统与应用层面进行优化。最常见的方案就是开启 BBR 算法，它能在跨境链路波动的情况下显著改善 TCP 传输效率，提升海外网速。</p><h4>什么是 BBR？</h4><p>BBR（Bottleneck Bandwidth and Round-trip propagation time）是 Google 开发的一种 TCP 拥塞控制算法。传统的 TCP 算法通过丢包来判断网络拥堵，而 BBR 则通过主动测量带宽和延迟来优化传输策略。<br/>简单来说，BBR 不等网络堵死才减速，而是智能地探测网络状况，始终让数据以最优速度传输。在跨境网络这种高延迟、有一定丢包的环境下，BBR 的优势尤为明显，能够让你的 100Mbps 带宽真正发挥作用。</p><h4>BBR 能带来多大提升？</h4><p>根据实际测试，在跨境网络场景下：</p><ul><li>下载速度提升：30%-300% 不等（取决于网络状况）</li><li>在高延迟（150ms 以上）环境下效果最明显</li><li>对于丢包率 1%-5% 的线路改善尤为显著</li></ul><p>需要注意的是，BBR 并不能突破带宽限制的天花板，但能让你更接近理论上限。如果你的跨境上行被限制在 500Kbps，BBR 也无法让它跑到 10Mbps，但能让你稳定达到.</p><h4>如何开启 BBR</h4><p>前置要求 ：Linux 内核 4.9 或更高版本，服务器具有 root 权限。<br/>快速开启步骤 ：</p><pre><code>添加配置 
echo "net.core.default_qdisc=fq" &gt;&gt; /etc/sysctl.conf 
echo "net.ipv4.tcp_congestion_control=bbr" &gt;&gt; /etc/sysctl.conf 

应用配置 
sysctl -p 

 验证是否成功
sysctl net.ipv4.tcp_congestion_control</code></pre><p>如果内核版本过低，需要先升级内核。网上有大量一键脚本可以简化这个过程，或者在网上寻找 BBR一键安装脚本，通过脚本一键运行也可以。</p><h4>开启 BBR 的注意事项</h4><ul><li>BBR 需要服务器端开启，本地电脑开启无效</li><li>OpenVZ 虚拟化的 VPS可能不支持</li><li>部分商家的VPS已默认开启 BBR，可以先检查再操作</li><li>BBR 只是优化传输效率，无法突破带宽限制或绕过线路问题</li></ul><p>BBR 是成本最低、效果最直接的优化方案之一。只需几行命令，就能显著改善跨境传输效率。它特别适合作为第一步优化措施，但要获得更好的速度，还需要结合线路选择等其他方案。</p><h3>线路问题：选择正确的数据传输路径</h3><p>如果说开启 BBR 是为赛车换上了顶级引擎，那么“线路选择”则决定了你跑的是专业赛道还是乡间小路。 &gt; 即使 BBR 优化到了极致，它也只能在现有链路上压榨效率：如果你的数据包原本就要绕行大半个地球，或者正挤在满是“收费站”的公网出口，那么单靠算法调优已很难产生质变。在跨境网络的世界里，线路的物理优先级往往拥有最高的话语权。</p><h4>什么是线路？</h4><p>线路，简单说就是数据从你的电脑到服务器所经过的网络路径 。就像从北京到上海，你可以选择：</p><ul><li>高速公路 ：路况好、收费站少、限速高，虽然可能稍远但整体最快</li><li>国道 ：路程近但红绿灯多、限速低，走走停停</li><li>绕路 ：看似有路可走，实际上要绕道西安、成都再到上海，多走几倍距离</li></ul><p>在国际网络中，数据传输的路径同样有优劣之分。</p><h4>三种常见线路类型</h4><p>海外服务器的访问速度，核心差异往往来自线路本身。整体来看，国际线路的表现大体可以分为三类：</p><ul><li>BGP线路 ：路径不合理，数据被转发到其他国家再回到目的地，延迟高、丢包多，体验最差。</li><li>优化线路 ：按正常地理路径传输，没有绕行，但是走运营商的普通国际出口，高峰期容易拥堵。</li><li>精品线路 ：运营商的精品专属通道，延迟低、丢包少、高峰期仍稳定，但价格较高。</li></ul><p>精品线路相比前两类有本质优势：稳定、低延迟、高峰期也不容易拥堵，是追求速度和可靠性的用户的最佳选择，但价格也相对更高。</p><h4>为什么线路差异如此明显？</h4><p>BGP就像从北京去上海却要先绕道新疆，需要浪费大量时间和资源。优化线路相当于走普通国道，路径合理但要经过各种收费站和红绿灯，高峰期还容易堵车。而精品线路则像是专属高速公路，全程没有拥堵点。<br/>数据传输就是如此，因为优化线路跳转次数少、没有拥堵、丢包率低，最终体现为下载速度的大幅提升。<br/>选择合适的线路，往往比单纯提升带宽更能改善实际体验。一台 100Mbps 精品线路的服务器，实际速度可能超过 500Mbps 普通线路的 服务器。但优化线路的成本也更高，需要根据自己的需求和预算来权衡。</p><h4>怎么判断使用了哪种线路？</h4><p>判断线路类型的本质，就是查看数据在互联网上“经过了哪些节点”（可以理解为经过了哪些路由器）。这些节点的路径是否绕路、是否包含精品网标识节点，都能直接说明线路质量。<br/>最简单的方法，是使用在线的路由追踪（Traceroute）工具进行测试。需要注意的是，电信、联通、移动三大运营商的回程路径不同，因此建议分别用三网的测试节点跑一次，才能得到最准确的判断。<br/>通过观察是否绕行、节点类型是否属于精品网，你就能快速判断服务器究竟是BGP、优化，还是精品线路。</p><h4>怎么解决服务器线路慢？</h4><p>如果你的服务器 访问速度不理想，最有效的改善方式始终是换到更好的线路 。优化线路（如 CN2 GIA、CMIN2、CUP）在延迟、丢包和高峰期稳定性上都有压倒性优势。<br/>如果暂时无法更换线路，也可以通过中间加速方案来改善体验：</p><ul><li>CDN加速：将静态内容缓存到国内或全球边缘节点，减少跨境访问次数。</li><li>自建中转节点 ：本质上也是一种“自建 CDN/ Relay”，通过中转实现优化路由的目的。</li></ul><p>无论使用哪种方式，核心思路都是减少跨境瓶颈、缩短传输路径，或者让真正跨境的部分走更好的线路。这样通常都能在不更换业务架构的前提下，显著提升服务器的访问体验。</p><h3>带宽限制：识别瓶颈并对症下药</h3><p>在解决了线路问题之后，如果速度仍然不理想，就需要检查带宽 。很多人会误以为带宽问题就是 服务器配置不够，但实际情况要复杂得多。带宽瓶颈可能出现在两个位置：服务器端和你的家庭网络端。</p><h4>两种带宽限制场景</h4><p>1、服务器端带宽不足<br/>服务器通常都会标注带宽规格，常见的有 100Mbps、1Gbps 等。大部分海外机房的国际带宽资源相对充足，这类问题其实比较少见 。<br/>想要测试 服务器真实带宽？最简单的方法是使用 SpeedTest CLI 工具在 服务器上进行测速：</p><pre><code># 安装 SpeedTest CLI
curl -s https://packagecloud.io/install/repositories/ookla/speedtest-cli/script.deb.sh | sudo bash
sudo apt-get install speedtest
# 运行测速
speedtest</code></pre><p>如果测速结果接近标称带宽（例如 100Mbps 服务器能跑到 90Mbps 以上），说明 服务器本身的带宽没有问题。如果差距很大，可能是商家超售或带宽配置虚标，这种情况建议直接更换服务商。<br/>2、家庭带宽的跨境限制<br/>这是更常见也更隐蔽的问题。<br/>虽然你家里的宽带可能是 100Mbps 甚至 300Mbps，但这个速度指的是国内访问速度 。跨境访问时，运营商往往会对国际出口带宽进行限制 ，尤其是上行带宽 （从你电脑上传到海外服务器的方向）。<br/>典型现象：</p><ul><li>下载海外资源速度尚可，但上传速度极慢</li><li>SSH 连接到服务器时输入命令卡顿</li><li>向 服务器上传文件速度只有几十 KB/s</li></ul><p>想要测试跨境真实带宽，可以使用 iperf3 工具可以直观地看到跨境带宽限制：</p><pre><code># 在 VPS 上启动 iperf3 服务端
iperf3 -s

# 在本地电脑测试上传速度（到 VPS）
iperf3 -c VPS的IP地址

# 测试下载速度（从 VPS）
iperf3 -c VPS的IP地址 -R</code></pre><p>测试结果可能会让你大吃一惊：即使你的带宽是 100Mbps，跨境上行速度可能只有几百 Kbps，甚至接近零 。<br/><img width="651" height="662" referrerpolicy="no-referrer" src="/img/bVdnMGU" alt="image.png" title="image.png"/><br/>如图所示，典型的测试结果会呈现明显的上下行不对称：下载速度（从 服务器到本地）可能有 60 Mbps，但上传速度（从本地到 服务器）可能只有 200 Kbps。这就是为什么你会感觉 SSH 输入卡顿、上传文件极慢的原因。<br/><strong>如何解决带宽限制问题</strong><br/>场景一：服务器带宽不足<br/>这种情况比较简单直接：更换服务商 。选择带宽资源充足、口碑好的服务器提供商。<br/>场景二：家庭带宽被限（更常见）<br/>这种情况处理起来相对复杂，需要分步骤进行：<br/>1、联系运营商处理<br/>先联系你所在片区的宽带维护人员或客服，说明跨境访问速度异常慢的情况。很多时候，这种限制是运营商的 QoS 策略或临时限速措施，通过申诉可以恢复到相对正常的水平。</p><ul><li>准备好测速数据（iperf3 测试结果）作为证据</li><li>强调这影响了正常的工作需求（远程办公、国际业务等）</li><li>多数情况下，运营商会进行调整，至少能恢复到可用状态</li></ul><p>2、如果运营商无法解决<br/>如果联系运营商后仍然无法改善，或者运营商明确表示无法调整，那就只能考虑技术手段绕过限制：</p><ul><li>使用付费 CDN：将网站或 API 的内容缓存到边缘节点，减少跨境访问次数，适合静态资源加速。</li><li>自建中转节点 ：在国内部署一台中转服务器，通过端口转发或隧道让跨境流量走更好的线路。</li><li>更换运营商 ：在条件允许时选择跨境限制更少的宽带，企业宽带通常表现更好，但成本更高。</li></ul><p>带宽限制的问题核心在于准确定位瓶颈位置 ：</p><ul><li>如果是 服务器带宽不足：换更好的服务商即可</li><li>如果是家庭宽带跨境受限：先找运营商解决，实在不行再考虑 CDN 或中转方案</li></ul><p>值得注意的是，上行带宽受限比下行更容易被忽视，但对实际体验的影响却更大。如果你发现 SSH 操作卡顿、上传文件奇慢，优先用 iperf3 测试一下跨境上行速度，很可能就是这个问题。</p><h2>总结</h2><p>服务器速度慢的问题，表面上看是账面带宽的数字游戏，本质上却是一个复杂的系统性博弈。<br/>想要彻底攻克网络瓶颈，不能盲目砸钱升级配置，而应遵循一套科学的优先级调优法则：</p><ol><li>内核调优（软件层）： 优先开启 BBR 加速。这是成本最低、见效最快的手段，旨在榨干现有链路的传输效率。</li><li>链路诊断（物理层）： 利用 Traceroute/MTR 等工具进行路由追踪。看清楚数据包是在哪一跳开始“绕路”或“丢包”，揪出幕后黑手。</li><li>压测定位（压力层）： 使用 iperf3 进行端到端带宽测试。分清瓶颈到底是出在 服务端、国际出口，还是你的本地运营商。</li><li>精准打击（决策层）： 根据诊断结果对症下药——是更换精品线路，还是引入 CDN。<br/>单纯追求带宽上限，而不解决传输效率和线路畸形，无异于在泥泞小路上开超跑。 只有通过“软件优化+硬件选路”的组合拳，才能真正释放海外服务器的性能潜力，告别访问卡顿。</li></ol><p>本文原发于我的博客：<a href="https://link.segmentfault.com/?enc=iYA71NHQsTtmjvYNERJ3HA%3D%3D.3MClBuCCKSa0nXvW7%2FRljPZC5Q8jAsyeikLbWxsuVCQ%3D" rel="nofollow" target="_blank">landonVPS</a></p>]]></description></item><item>    <title><![CDATA[错的"优化目标"：从扫地机器人到毁掉一切的算法 深度涌现 ]]></title>    <link>https://segmentfault.com/a/1190000047575498</link>    <guid>https://segmentfault.com/a/1190000047575498</guid>    <pubDate>2026-01-27 17:08:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>清扫整个 Albert Heijn 超市的地面。听起来很简单。而且原本也应该很简单。</p><p>但我是一名计算机科学专业的学生，我遇到了一个问题：我总是忍不住去优化一些（可能）不需要优化的东西。</p><p>所以，我没有只是做好我的工作，比如扫地，而是做了任何一个“理智的”人都会做的事：我把超市的平面图转换成了网格图，构建了一个可视化编辑器，并使用模拟退火算法编写了一个 C++ 路径优化器。</p><p>但在我们深入探讨这件事是如何彻底出错，以及这件事如何让我意识到这会让每个人都痛苦之前，我需要你回答一个问题：</p><p>如果你要替我工作一天，并且需要打扫整个 Albert Heijn 超市的楼层，你会选择 A 路线还是 B 路线？<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575500" alt="img" title="img"/><br/>路径 A（上）和路径 B（下）。</p><p>说真的，看看它们。哪个看起来更适合清扫超市地面？</p><p>如果你选择了A选项：恭喜你，你的思维方式像算法一样，很可能就是个机器人。</p><p>但从技术上讲，你的说法没错。路径 A 的距离更短。然而，它完全没用。</p><p>看看那些弯道。你不妨想象一下，如果你走路时也这样弯，你会看起来像个疯子，就像一台抽搐的扫地机器人。</p><p>路径 A 是你优化了错误的东西而导致的结果。</p><p>剧透一下，这正是这个故事的重点。不过我们稍后再谈，先让我解释一下事情的来龙去脉：</p><h2>第一步：将现实转化为过于简单的模型</h2><p>首先，我把 Albert Heijn 的平面图转换成了网格。每个方格要么是空的（应该打扫），要么是障碍物（墙壁、收银台、有人扔在地上的酸奶包装）。</p><p>我在<a href="https://link.segmentfault.com/?enc=vSW6tSxoRGOcIJpkPh20yg%3D%3D.DGUUfA%2Fk%2FeiTzeXDN4UwS3KKXszP4fAqh2LOthCe1CE%3D" rel="nofollow" target="_blank">Processing</a>中构建了一个可视化编辑器，这样我就可以轻松地绘制商店地图并导出生成的图表。</p><p>因此，将平面图转换为网格结构非常容易。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575502" alt="img" title="img" loading="lazy"/><br/>Albert Heijn 超市的网格平面图。</p><p>实际地面的瓷砖铺设有助于将该区域细分为易于管理的小块区域。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575503" alt="img" title="img" loading="lazy"/><br/>采用瓷砖结构的细分式平面图。</p><p>然后，通过将每个图块解释为一个节点，然后将它们连接到相邻的图块，就可以很容易地将其转换为网络结构（也称为图）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575505" alt="img" title="img" loading="lazy"/><br/>将每个图块解释为图中的一个节点。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575506" alt="img" title="img" loading="lazy"/><br/>生成的瓦片网络。</p><p>如你所见，我允许水平和垂直移动，以及对角线移动（只要你不穿墙）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575507" alt="img" title="img" loading="lazy"/><br/>Albert Heijn 的最终图表。</p><p>接下来唯一要做的就是找到一条贯穿这个网络的路径，同时确保访问所有节点（图块）。这样就能解决我的扫描问题。</p><p><em>（这个问题也称为<a href="https://link.segmentfault.com/?enc=vGOdoDV0iEqbH8%2FBHPASgA%3D%3D.nsIqZXxUq%2FQwQ1T8C4v24rTQWQmkarynScDZ2xdsbTpichkZj9k3PAWmz%2F%2FlZNFimMaCRkBIp1%2FT%2BDfv7MFwdw%3D%3D" rel="nofollow" target="_blank">旅行商问题</a>，详情请参阅相关文章，了解它为何如此难以“解决”。）</em></p><h2>第二步：编写优化器</h2><p>由于在如此规模的图中，计算上不可能找到最优路径，我们只能求助于启发式算法。启发式算法本质上是在短时间内找到一个<strong>非常好的</strong>解，而不是试图找到<strong>完美的</strong>解（这几乎是不可能的）。</p><p>所以我用 C++ 实现了路径优化器。</p><p>底层启发式算法：<strong>模拟退火</strong>。</p><p>如果你还不熟悉，模拟退火本质上就是尝试一系列微小的变化（也称为局部移动）。</p><p>一开始，你接受每一个小小的改变（即使它让路径变得更糟），但随着算法的进行，你会逐渐变得更加挑剔，最终只允许那些严格意义上改善路径的改变。</p><p>这个想法源于金属冷却的过程。首先，金属在高温下运行（尝试不同的运动方式）进行充分的探索，然后逐渐冷却，最终稳定在低能量状态（接近最佳状态）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575508" alt="img" title="img" loading="lazy"/><br/>模拟退火算法通过多次迭代逐步改进路径。</p><p>看看这个动图。看到了吗？它一开始很混乱，然后逐渐变得稳定下来。这就是模拟退火算法的工作原理。</p><p>对于局部移动，我使用了 2-opt 移动。具体来说，就是移除路径中的两条边，然后用不同的方式重新连接它们。如果这种微小的改动使路径变得更好，就保留它。否则，要么保留它（如果温度仍然很高），要么舍弃它。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575509" alt="img" title="img" loading="lazy"/><br/>2-opt 移动可视化。</p><p>那就这么做十亿次。或者，让你的电脑这么做十亿次。</p><h2>第三步：搞砸</h2><p>运行一段时间后，我得到了第一条“优化”路径。以下是优化结果：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575510" alt="img" title="img" loading="lazy"/><br/>第一条“优化”路径。</p><p>瞧瞧这路！弯道比克里斯托弗·诺兰的电影还多。谁会傻到真的这么扫地啊？扫完估计都想吐。</p><p>从技术上讲，它覆盖了整个地面。从技术上讲，它（几乎）是最小的清扫路径。从技术上讲，它是完美的。</p><p>它有一些优点，但实际上，它完全没用。</p><p>算法完全按照我的要求执行了。</p><p>我问错问题了。</p><h2>第四步：根据实际情况进行优化</h2><p>我很快意识到我优化的方向错了。距离并非一切。</p><p>转弯很重要。动量很重要。看起来不像个故障的机器人也很重要。</p><p>所以我给成本函数增加了一个“转弯惩罚”，并要求它最小化这个惩罚。基本上就是告诉算法：“转弯90度会扣分。转弯180度？你疯了吧。”</p><p>这样一来，路线就更加顺畅了，即使距离略微延长了一些。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575511" alt="img" title="img" loading="lazy"/><br/>更平坦、更适合步行的道路。</p><p>你看，这其实……挺好走的。你把这条路交给一个真正的人，他也不会立刻放弃。</p><p>我们不再追求距离上的最优解，而是追求与现实的契合度。</p><h2>第五步：打破它</h2><p>接下来才是精彩的部分。</p><p>您可以调整急转弯的惩罚。这相当于一个滑块，可以在“纯粹的效率”和“实际用途”之间切换。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575512" alt="img" title="img" loading="lazy"/><br/>从低角度罚分（1）到高角度罚分（6）。</p><p>你可以清楚地看到其中的权衡。增加惩罚，路径会更平滑，但会稍长一些。减少惩罚，效率会提高，但会造成混乱。</p><p>选择哪条路完全取决于你自己。这取决于很多因素，比如你转身是否方便，总距离是否是首要考虑因素，以及你能忍受多大的眩晕。</p><h2>第六步：意识到生活其实并没有那么美好</h2><p>但这不仅仅是扫地那么简单。</p><p>这关乎一切。</p><p>社交媒体算法以提升用户参与度为目标，而且它们在这方面做得非常出色。问题出在哪里呢？</p><p>参与度≠幸福。参与度≠真相。参与度=点击量、屏幕使用时长、愤怒情绪和负面反应。</p><p>后果？愤怒、错误信息、负面新闻刷屏、焦虑。</p><p>算法运行完美，完全按照设计预期执行。问题出在成本函数上。（Instagram 可能不这么认为。）</p><p>推荐算法会优化观看时长和点击率。你奶奶正在YouTube上连续看6个小时的阴谋论视频。</p><p>算法彻底毁了她。她感觉糟透了。</p><p>这并不令人意外。</p><p>即使是像 ChatGPT 这样的大型语言模型，它们的优化方向也错了。它们优化的是听起来自信，听起来好像知道答案。</p><p>不是因为他正确，也不是因为他诚实。</p><p>他们接受的训练是完成既定模式，而不是说“我不知道”。所以他们只能靠猜测。而且毫无羞耻心，语法也完美无瑕。</p><p>这一点甚至适用于科技以外的领域。</p><p>想想企业。它们大多以盈利为目标。地球、环境、道德或伦理呢？这些都没有纳入成本函数，因此也不会被优化。</p><p>我在实际工作中是否使用了这条优化路径？</p><p>不，显然不是。我只是像正常人一样扫了地而已。</p><p>但构建这个项目教会了我一个我一直在思考的道理：如果你解决的是错误的问题，那么技术上的正确性就毫无意义。</p><p>你可以写出完美的代码，你可以构建完美无瑕的系统，你可以把成本函数优化到极致，但最终结果仍然可能很糟糕。</p><p>重要的不是优化算法本身，而是首先要弄清楚你究竟应该优化什么。</p><p>大多数时候，我们甚至都不会问这个问题。我们只是优化那些容易衡量的指标，然后祈祷结果会好起来。</p><p><a href="https://link.segmentfault.com/?enc=heHa0h2ryscnHSEsc%2BVcnQ%3D%3D.jrQ%2B2R27nCSWDLC3S%2BVF2ZFrtP0l4JWu9a5k9QfxsxVLGBf%2FvU7iyRxzqjxJT78xPUVD5rAWEeswZaBwg86KYrk6r79Ma%2BniGow5eRoH4VY%3D" rel="nofollow" target="_blank">https://tiespetersen.substack.com/p/i-got-paid-minimum-wage-t...</a></p><p>笔者公众号「深度涌现」</p>]]></description></item><item>    <title><![CDATA[从“发现问题”到“搞定方案”｜办公小窍门 商汤小浣熊家族 ]]></title>    <link>https://segmentfault.com/a/1190000047575529</link>    <guid>https://segmentfault.com/a/1190000047575529</guid>    <pubDate>2026-01-27 17:08:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>哈喽，小浣熊家族的朋友们！🖐️ 本系列将跟大家聊聊那些可以提升工作效率的小窍门。<br/>在快节奏的工作里，每个人都希望把时间花在真正有价值的事上。「办公小窍门」系列，专注解决这些日常的“小痛点”，用最轻量的方式帮你提升效率。<br/>在这里，我们会用清晰易懂的方式拆解每一个典型办公场景，并展示如何用 办公小浣熊 迅速完成那些原本需要花费大量时间的任务，让你的工作更快、更稳、更智能。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnIu3" alt="image.png" title="image.png"/><br/>书接上回，我们已经</p><ul><li>通过“<a href="https://segmentfault.com/a/1190000047477939" target="_blank">数据清洗</a>”打好了地基</li><li><p>又通过“<a href="http://segmentfault.com/a/1190000047497863" target="_blank">数据分析</a>”让数据开口说话，找到了业务的重点与痛点<br/>但老板的追问往往紧随其后：</p><blockquote>“既然发现了 Q3 华东区销量下滑、浙江 Web 渠道是高地，那我们下一步该怎么办？”</blockquote></li></ul><p>这一篇，我们就来聊聊数据处理的终点站：<br/>如何通过数据洞察，将分析结论转化为具体的“行动方案”。</p><h3>01数据洞察：从“看后视镜”到“看导航仪”</h3><p>很多人认为，看到报表、画出图表，数据工作就结束了。<br/>但正如我们之前聊过的，<strong>数据分析的本质是降低决策成本</strong></p><p>如果说<strong>数据分析</strong>是在看后视镜（What happened）<br/>那么<strong>数据洞察</strong>就是在看挡风玻璃（What to do next）。</p><ul><li><strong>数据分析结论</strong>：“上海地区 App 端客单价最高，但订单量仅排第三。”</li><li><strong>数据洞察建议</strong>：“上海用户具有极强的高端消费潜力，上海应制定高端精品战略，重点打造高端品牌形象。”</li></ul><p><img width="723" height="440" referrerpolicy="no-referrer" src="/img/bVdnIvc" alt="image.png" title="image.png" loading="lazy"/><br/>（上图为办公小浣熊生成报告部分截图）</p><p><strong>真正有价值的洞察，必须具备“可执行性”</strong>。<br/>它不是空泛的口号，而是基于数据逻辑推导出的经营策略。</p><h3>02别让“拍脑袋”代替了“科学决策”</h3><p>在传统流程中，从分析结果到产出方案，往往会出现断层：</p><ol><li><strong>逻辑断点</strong>：对着热力图看半天，最后憋出的方案却跟数据没关系</li><li><strong>覆盖不全</strong>：只盯着最大的市场看，忽略了那些“增长黑马”组合</li><li><strong>效率瓶颈</strong>：写一份完整的业务策略报告，查资料、对口径、磨文字，一天又过去了<br/>在办公小浣熊中，这一过程可以被极大简化，不用盯着屏幕苦思冥想，<strong>可以直接对话就能搞定</strong>。</li></ol><h3>03进阶指令：一句话召唤“首席增长官”</h3><p>我们继续沿用前两篇中的那份《电商销售明细表》<br/>在「<a href="http://segmentfault.com/a/1190000047497863" target="_blank">数据分析</a>」一文中，我们已经让办公小浣熊完成了多维分析：</p><ul><li>渠道 × 地区</li><li>销售额、订单量、客单价</li><li>用户行为与优惠使用情况<br/>针对这些结论，我们可以<strong>如何制定具体策略</strong>呢？</li></ul><p>直接跟 办公小浣熊 下达<strong>更具业务思维的指令</strong>：</p><blockquote>“基于刚才的渠道和地区分析结论，请为下个季度的营销资源分配提供 3 条核心建议，并写出一份针对高潜力地区的行动方案。”</blockquote><p>接下来，小浣熊会化身“首席增长官”，为你自动推演：</p><h4>1. <strong>资源优化：精准“止损”与“加仓”</strong></h4><p>基于各渠道的价值差异，小浣熊会自动识别：</p><ul><li><strong>对于 Offline 渠道</strong>：作为“销售冠军”，建议追加 36% 的预算，主推高客单商品。</li><li><strong>对于 MiniProgram 渠道</strong>：识别出“订单多但客单低”，建议主打价格敏感用户。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575533" alt="图片" title="图片" loading="lazy"/><br/>（上图为办公小浣熊生成报告部分截图）</p><h4>2. <strong>地区突围：抓准高潜力地区，实现区域突破</strong></h4><p>锁定 3 大“黄金产区”：</p><ul><li><strong>Guangdong (广东)</strong>：主攻 Web+Beauty 组合，锁定晚间 22:00 黄金时段。</li><li><strong>Shanghai (上海)</strong>：强化 App+Electronics 组合，凌晨 2:00 错峰营销。</li><li><strong>Sichuan (四川)</strong>：发挥 Web+Offline 协同优势，目标增长 40%。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575534" alt="图片" title="图片" loading="lazy"/><br/>（上图为办公小浣熊生成报告部分截图）</p><h4>3. <strong>地区定制化方案：从“一刀切”到“精细化”作战</strong></h4><p>在数据洞察阶段，小浣熊不仅能发现哪些地区在赚钱，<br/>还能针对<strong>各地的“性格”开出不同的药方</strong>:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575535" alt="图片" title="图片" loading="lazy"/><br/>（上图为办公小浣熊生成报告部分截图）</p><p>至此，一份涵盖<strong>资源分配、潜力挖掘、精细化执行</strong>的行动方案已全线打通。</p><h3>04汇报神器：从 Word 方案到 PPT 演示 </h3><p>写完方案还要做 PPT？<br/>小浣熊一站式服务继续进行：</p><ul><li><strong>场景定制</strong>：选择你的角色（如：市场/销售）和受众</li><li><strong>一键生成</strong>：只需几分钟，逻辑严密且图文并茂的 PPT 自动生成</li><li><strong>（❗️ 划重点）自由编辑</strong>：生成的 PPT 支持在线或下载到本地二次修改</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575536" alt="图片" title="图片" loading="lazy"/><br/>（上图为办公小浣熊生成的 PPT 部分截图）</p><h3>05为什么小浣熊的建议“靠谱”？</h3><p>办公小浣熊的建议不是“聊天回复”，而是<strong>基于其底层强大的 Python 统计建模能力</strong>。</p><p>它在给出建议前，其实已经完成了：</p><ul><li><strong>相关性分析</strong>：确认哪些因素真的在影响销售额。</li><li><strong>异常检测</strong>：剔除偶然因素干扰，确保建议基于普遍规律。</li><li><strong>多维验证</strong>：通过代码自动对比多种策略的可能性。</li></ul><p>目前，办公小浣熊在处理此类专业逻辑推理任务时，精度极高，<strong>确保了每一个洞察都有据可依</strong>。</p><h3>06数据能力的跃迁：你就是决策者</h3><p>在 AI 时代，数据分析和洞察正在从“专家技能”变成每个办公族的“基础能力”。</p><ul><li><strong>不再被琐碎操作困住</strong>：清洗、计算、画图都交给小浣熊。</li><li><strong>不再为结论发愁</strong>：一句话获取多维洞察</li><li><strong>把精力留给真正的决策</strong>：去判断趋势、去调动资源、去驱动真正的增长。</li></ul><p>你不需要精通复杂的算法，你只需要<strong>学会向 AI 提出正确的问题</strong>。</p><blockquote>当我们掌握了如何把洞察转化为行动，你就拥有了改变业务结果的力量。<br/>下回，我们一起来聊聊，如何「向 AI 正确提问」。<br/>我们也将带来更多场景的实测，你想看小浣熊挑战哪个办公场景？欢迎留言！</blockquote><p>「商汤小浣熊」是商汤科技推出的 AI 原生生产力体系，面向下一代办公方式而构建，包括办公小浣熊 &amp; 代码小浣熊。在这里，软件研发、数据分析、任务规划、结果交付均由 AI 直接驱动，工作链路由 AI 重新定义。超过 300 万用户与 1000+ 企业，正与小浣熊一起推动这一场新的办公升级。</p>]]></description></item><item>    <title><![CDATA[告别镜像拉取困境：毫秒镜像以“正规军”姿态重塑国内Docker加速生态 邱米 ]]></title>    <link>https://segmentfault.com/a/1190000047575537</link>    <guid>https://segmentfault.com/a/1190000047575537</guid>    <pubDate>2026-01-27 17:07:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近年来，随着容器技术的普及，Docker已成为开发和运维领域不可或缺的基础工具。然而，国内开发者在从Docker Hub拉取镜像时，常常面临速度缓慢、连接超时甚至完全无法访问的困境。尽管市场上曾涌现众多镜像加速服务，但很多因成本、合规或运维压力相继关闭，开发者往往不得不像“打游击”一样频繁更换加速源，严重影响了开发效率和体验。</p><p><img width="650" height="340" referrerpolicy="no-referrer" src="/img/bVdnMJo" alt="ececa7af0cdca7dbfe0011f388991769_2026012714070159753.001.jpeg" title="ececa7af0cdca7dbfe0011f388991769_2026012714070159753.001.jpeg"/></p><p>在此背景下，一家名为毫秒镜像的服务逐渐进入大众视野，并凭借其企业级稳定性、行业深度集成与金融级合规保障，成为越来越多开发者和企业的共同选择。近日，笔者通过多方调研与实际体验，发现这并非又一个“昙花一现”的加速节点，而是一个经过数百万用户验证、被多家行业巨头内置采用的“正规军”级基础设施服务。</p><p><strong>一、行业痛点催生稳定需求</strong></p><p>国内网络环境的特殊性使得直接访问Docker官方仓库体验较差。虽然此前出现过不少高校、云厂商提供的镜像源，但多数因带宽成本、运维压力或政策原因陆续停止服务。开发者们饱受“今日可用、明日失效”的困扰，甚至不得不忍受数小时乃至数天的镜像拉取等待时间，严重影响CI/CD流程、团队协作与项目部署效率。</p><p>“有没有一个长期稳定、无需频繁更换的镜像加速方案？”成为众多技术团队心中的共同疑问。</p><p><strong>二、毫秒镜像：不止于加速，更是基础设施</strong></p><p>通过对1ms.run的深入使用与背景调查，其核心优势逐渐清晰：</p><ol><li>行业巨头的“默认选择”</li></ol><p>最令人信服的是，毫秒镜像已被多家国民级软件和硬件厂商原生内置至其产品中：</p><ul><li>宝塔面板：国内服务器运维领域的领先者，已将1ms.run集成至其核心功能，覆盖数千万台服务器；</li><li>爱快路由：企业级流控路由品牌，在系统层面整合其加速服务，助力企业级容器化部署；</li><li>绿联NAS：消费级NAS市场的重要参与者，默认将1ms.run设为容器镜像源。</li></ul><p>这种“深度集成”并非简单的API对接，而是建立在严格技术评审、长期稳定性测试与高并发承载能力验证之上的信任背书。这意味着，每一台使用这些产品的设备，都在无形中成为毫秒镜像服务的体验者与验证者。</p><ol start="2"><li>金融级安全与合规保障</li></ol><p>更值得一提的是，毫秒镜像的客户名单中包含了持有央行支付牌照的金融机构。金融行业对基础设施的合规性、安全性、可用性要求极为严苛，任何服务中断或安全漏洞都可能引发重大风险。能够通过金融级的技术审计、渗透测试与持续性监控，并应用于支付系统的生产环境，充分证明了其在安全、稳定与合规方面已达到行业顶尖水准。</p><ol start="3"><li>可规模化验证的服务能力</li></ol><p>据公开数据，毫秒镜像目前服务数百万活跃用户，每日处理TB级别的镜像分发流量，服务可用性承诺达99.9%以上。这种规模的持续运营，不仅验证了其架构的弹性与鲁棒性，也体现了其背后可持续的商业模式——通过会员增值服务与企业定制方案，保障了长期的资源投入与运维迭代，避免了“用爱发电”型服务的不可持续风险。</p><p><strong>三、技术体验：极速、智能、全平台</strong></p><p>除了背景实力，在实际使用体验上，毫秒镜像也表现突出：</p><p>1、一键配置，告别复杂操作</p><p>其开源工具1ms-helper支持Linux、macOS、Windows及各类NAS系统，可一键完成Docker、Podman、Kubernetes及K3s环境的加速配置。无需手动编辑daemon.json，避免配置错误导致的服务异常。</p><p>2、智能诊断，化解网络疑难</p><p>工具内置网络检测模块，可自动诊断DNS解析、网络连接等问题，并提供修复建议，极大降低了在内网或复杂网络环境下部署容器的门槛。</p><p>3、全网加速，覆盖主流仓库</p><p>除Docker Hub外，还支持gcr.io、quay.io、k8s.gcr.io等常见境外镜像仓库的加速拉取，有效解决了多源镜像拉取难题。</p><p><strong>四、用户声音：从个人开发者到企业团队</strong></p><p>某互联网公司CTO表示：“使用毫秒镜像后，CI/CD流水线因镜像下载导致的失败率从30%降至不足2%，团队每月节省近千工时。”  </p><p>一位绿联NAS用户反馈：“以前自己配置加速源经常出问题，现在系统默认集成，拉镜像速度飞快，真正做到了开箱即用。”  </p><p>企业运维人员则称赞：“1ms-helper工具的批量部署功能，让我们在数百台服务器的集群中快速统一配置，运维效率大幅提升。”</p><p><strong>五、总结：选择被验证，信任来自实力</strong></p><p>在技术设施选型中，“稳定可靠”往往比“暂时免费”更具长期价值。毫秒镜像通过厂商生态集成、金融级场景验证、百万用户规模使用，构建起一道扎实的信任壁垒。它不仅提供了显著的加速效果（平均提升10倍以上），更以企业级的产品思维，解决了镜像源可持续运营的根本问题。</p><p>对于长期受困于镜像拉取难题的开发者和企业而言，或许到了该告别“游击战”、拥抱“正规军”的时刻。毫秒镜像正以其扎实的技术积累、清晰的商业路径和广泛的行业认可，成为中国容器生态中一个值得信赖的基础设施服务商。</p>]]></description></item><item>    <title><![CDATA[在 Trae IDE 中利用 MCP 高效抓取与收录网络文章 发怒的皮带 ]]></title>    <link>https://segmentfault.com/a/1190000047575548</link>    <guid>https://segmentfault.com/a/1190000047575548</guid>    <pubDate>2026-01-27 17:06:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>背景</h2><p>在维护 奥升官网（基于 Nuxt 3 构建）的过程中，我们需要将发布在其他平台的技术文章迁移到官网的内容管理系统中。手动复制粘贴不仅效率低下，还容易遗漏图片资源或导致格式错乱。</p><p>借助 <strong>Trae IDE</strong> 及 <strong>Chrome MCP</strong>，我们可以将这一过程高度自动化。本文将详细介绍这套抓取方案的思路与实现。</p><h2>环境与工具</h2><p>本方案依赖以下核心环境与工具：</p><ol><li><strong>Trae IDE</strong>：新一代 AI 驱动的集成开发环境（我使用的国际版，开了会员），可调用 Gemini-3-Pro-Preview 模型，支持复杂的上下文理解与多步任务执行。</li><li><strong>MCP</strong>：<a href="https://link.segmentfault.com/?enc=TWLnevdGLRCHSjt9I93KHA%3D%3D.6fdhZXLyTjUhAVdozJVD7jPJX7C6%2B73RmH9KCRmBkZXUjxiYEdGODHkAodNGicJB" rel="nofollow" target="_blank">mcp_chrome</a>，提供浏览器自动化能力，用于访问网页、读取 DOM 结构、截图等。</li></ol><h2>抓取思路</h2><p>整个抓取流程可以概括为以下四个步骤：</p><ol><li><strong>目标分析</strong>: AI 通过 MCP 浏览器工具访问目标 URL，解析页面结构，识别标题、正文、发布时间及图片链接。</li><li><strong>内容提取</strong>: 将 HTML 内容转换为符合 Nuxt Content 规范的 Markdown 格式，并自动提取 Frontmatter（元数据）。</li><li><p><strong>图片处理</strong>:</p><ul><li>自动下载文章中的图片到 <code>public/images/articles</code> 目录，并按文章id分文件夹存储。</li><li>使用 <code>sharp</code> 对图片进行压缩优化（WebP/JPG 转换、尺寸调整）。</li><li>将图片按上传到 OSS 指定文件路径中。</li><li>替换 Markdown 中的图片链接为 OSS 上的路径。</li></ul></li><li><strong>自动收录</strong>: 将处理好的 Markdown 文件写入 <code>content/articles/</code> 目录。</li></ol><h2>关键实现</h2><h3>1. 页面内容读取</h3><p>利用 MCP 的 <code>chrome_read_page</code> 或 <code>chrome_get_web_content</code> 工具，我们可以直接获取渲染后的页面内容，这对于动态加载的页面尤为重要。</p><h3>2. 图片资源的自动化处理</h3><p><strong>1. 提取链接</strong><br/>AI 会从 DOM 中找出所有 <code>&lt;img&gt;</code> 标签的 <code>src</code> 属性。</p><p><strong>2. 批量下载</strong><br/>AI 会生成并执行 Shell 命令（如 <code>curl</code> 或 <code>wget</code>）来下载图片。</p><pre><code class="bash"># AI 自动生成的下载命令示例
mkdir -p publichttps://ucode-orise.oss-cn-beijing.aliyuncs.com/website/article/20
curl -o publichttps://ucode-orise.oss-cn-beijing.aliyuncs.com/website/article/20/1.jpg http://example.com/path/to/image.jpg</code></pre><p><strong>3. 图片压缩</strong><br/>利用项目中的 <code>sharp</code> 脚本对下载的图片进行批量处理，确保加载性能。</p><p><strong>4. 图片上传到 OSS</strong><br/>使用<code>ali-oss</code> 库，编写 <code>upload_oss</code> 脚本，将压缩后的图片上传到 OSS 上指定的目录。</p><pre><code class="javascript">async function uploadFile(filePath, retryCount = 0) {
    // 处理上传到oss的文件路径
    const relativePath = path.relative(articlesDir, filePath);
    const objectName = `website/article/${relativePath.split(path.sep).join('/')}`;

    try {
        const result = await client.put(objectName, filePath);
        //... 
    } catch (err) {
        //...
    }
}</code></pre><h3>3. Markdown 生成与 Frontmatter 注入</h3><p>Nuxt Content 要求每篇文章包含特定的元数据。我们定义了统一的 Prompt 模板，让 AI 生成标准格式：</p><pre><code class="markdown">---
title: 文章标题
slug: article-slug
description: 文章摘要...
date: 2024-03-20
category: 分类
cover: https://ucode-orise.oss-cn-beijing.aliyuncs.com/website/article/[slug]/cover.jpg
---

## 正文内容
...</code></pre><h2>实际效果展示</h2><p>下图是我们在 Trae 环境中开发时的文章列表页面截图，展示了收录后的效果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575551" alt="IDE" title="IDE"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575552" alt="IDE" title="IDE" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575553" alt="IDE" title="IDE" loading="lazy"/></p><h2>总结</h2><p>通过 Trae IDE + MCP 的组合，我们将原本需要 10 分钟/篇的人工迁移工作缩短到了 2-3 分钟/篇（主要是AI对话推理时间）。开发者只需提供一个 URL，剩下的分析、下载、转换、优化工作全部由 AI 代理完成。这不仅极大提升了效率，还保证了代码风格与目录结构的一致性。</p>]]></description></item><item>    <title><![CDATA[MyBatis Dynamic SQL 入门指南 信码由缰 ]]></title>    <link>https://segmentfault.com/a/1190000047575561</link>    <guid>https://segmentfault.com/a/1190000047575561</guid>    <pubDate>2026-01-27 17:05:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575563" alt="" title=""/><br/><a href="https://link.segmentfault.com/?enc=D%2FvGs985%2BFuY8pyr68SYJw%3D%3D.%2BHilpQBqSfUxlXiPc7uEFjscq1rXLpAErGaZQ30DUx4FFqWbJ3UjMME%2B%2BVoMG5jOAPnH6cJpExOGJqSRVw%2FNHA%3D%3D" rel="nofollow" target="_blank">MyBatis Dynamic SQL</a> 是一种类型安全的 Java 领域特定语言（DSL），用于通过编程方式构建 SQL 查询，而非编写 SQL 字符串或基于 XML 的动态查询。它在运行时使用流畅的 Java 构建器生成 SQL，同时仍通过标准的 MyBatis 映射器执行。与手动拼接字符串或复杂的 XML 逻辑相比，这使得查询构建更安全、更易于重构，并且更不容易出错。</p><p>由于查询是用 Java 编写的，列名和表引用通过强类型的元数据类在编译时进行验证，这提供了更好的 IDE 支持并减少了运行时 SQL 错误。本文将解释 MyBatis Dynamic SQL，并展示如何在 Java 应用程序中使用它。</p><h2>1. 使用 MyBatis Dynamic SQL 可以做什么？</h2><p>MyBatis Dynamic SQL 支持大多数常见的 SQL 操作，包括 <code>SELECT</code>、<code>INSERT</code>、<code>UPDATE</code> 和 <code>DELETE</code>，以及连接、子查询、分页、排序、条件过滤和批量操作。它允许我们逐步构建查询，仅在某些参数存在时添加条件，这使其成为搜索界面和过滤 API 的理想选择。</p><p>它直接与 MyBatis 映射器接口集成，意味着我们仍然可以受益于结果映射、事务处理和连接管理。由于它生成标准的 SQL，因此适用于 MyBatis 支持的任何数据库，没有供应商锁定的问题。</p><h3>1.1 MyBatis Dynamic SQL 的工作原理</h3><p>Dynamic SQL 基于两个组件：表元数据类和 DSL 构建器。元数据类用 Java 描述表和列。DSL 构建器使用这些类以流畅、类型安全的方式组装 SQL 语句。</p><p>DSL 不直接执行 SQL。相反，它生成语句提供者对象，例如 <code>SelectStatementProvider</code> 或 <code>InsertStatementProvider</code>。这些对象被传递给使用 <code>@SelectProvider</code>、<code>@InsertProvider</code> 及类似注解标注的映射器方法，然后 MyBatis 使用其正常的执行引擎来执行这些语句。</p><h2>2. 项目设置与依赖</h2><h3>Maven 依赖</h3><pre><code class="xml">&lt;dependency&gt;
    &lt;groupId&gt;org.mybatis&lt;/groupId&gt;
    &lt;artifactId&gt;mybatis&lt;/artifactId&gt;
    &lt;version&gt;3.5.15&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.mybatis.dynamic-sql&lt;/groupId&gt;
    &lt;artifactId&gt;mybatis-dynamic-sql&lt;/artifactId&gt;
    &lt;version&gt;1.5.2&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;com.h2database&lt;/groupId&gt;
    &lt;artifactId&gt;h2&lt;/artifactId&gt;
    &lt;version&gt;2.4.240&lt;/version&gt;
    &lt;scope&gt;runtime&lt;/scope&gt;
&lt;/dependency&gt;</code></pre><p>这些依赖项包括 MyBatis 本身、Dynamic SQL DSL 以及一个用于测试的嵌入式数据库。</p><blockquote><strong>注意</strong><br/>数据库驱动可以替换为 MySQL、PostgreSQL 或任何其他支持的数据库。MyBatis Dynamic SQL 不依赖于数据库类型，仅依赖于标准 SQL 生成。</blockquote><h3>数据库模式</h3><p><code>schema.sql</code></p><pre><code class="sql">CREATE TABLE users (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100),
    age INT
);

INSERT INTO users(username, email, age) VALUES
('thomas', 'thomas@jcg.com', 30),
('benjamin', 'benjamin@jcg.com', 22),
('charles', 'charles@jcg.com', 17);</code></pre><p>此脚本创建一个简单的表并插入测试数据。内存中的 H2 数据库将在启动时执行此脚本，因此无需外部依赖即可测试查询。</p><h3>领域模型</h3><pre><code class="java">public class User {

    private Long id;
    private String username;
    private String email;
    private Integer age;

    // Getter 和 Setter 方法...
}</code></pre><p>这个 POJO 代表数据库中的一行。MyBatis 会自动使用匹配的字段名将列映射到字段，因此本例不需要额外的结果映射。</p><h2>3. 用于 Dynamic SQL 的表元数据</h2><p>下面的类以类型安全的方式定义数据库表及其列，允许在构建查询时被 Dynamic SQL DSL 引用。它充当 Java 代码与实际数据库结构之间的桥梁，实现了列名和类型的编译时验证。</p><pre><code class="java">public final class UserDynamicSqlSupport {

    public static final User user = new User();

    public static final SqlColumn&lt;Long&gt; id = user.id;
    public static final SqlColumn&lt;String&gt; username = user.username;
    public static final SqlColumn&lt;String&gt; email = user.email;
    public static final SqlColumn&lt;Integer&gt; age = user.age;

    public static final class User extends SqlTable {

        public final SqlColumn&lt;Long&gt; id = column("id", JDBCType.BIGINT);
        public final SqlColumn&lt;String&gt; username = column("username", JDBCType.VARCHAR);
        public final SqlColumn&lt;String&gt; email = column("email", JDBCType.VARCHAR);
        public final SqlColumn&lt;Integer&gt; age = column("age", JDBCType.INTEGER);

        public User() {
            super("users");
        }
    }
}</code></pre><p>这个类为 <code>users</code> 表定义了类型安全的元数据，使 MyBatis Dynamic SQL 可以在不使用原始 SQL 字符串的情况下构建查询。DSL 使用这些 Java 对象而非按名称引用列，从而提高了安全性和 IDE 支持。</p><p>内部类 <code>User</code> 继承 <code>SqlTable</code>，这将其标记为可用于 <code>from(user)</code> 和连接等子句的数据表。构造函数调用 <code>super("users")</code> 来告知 MyBatis 要在 SQL 语句（如 <code>FROM users</code>）中呈现的确切表名。</p><p>每个列都使用 <code>SqlTable</code> 中的 <code>column()</code> 方法定义，该方法注册列名及其 JDBC 类型。这会产生强类型的 <code>SqlColumn&lt;T&gt;</code> 对象，确保比较和条件在编译时使用正确的 Java 类型。</p><p>外部类公开了对表及其列的静态引用，以便于静态导入，使得查询读起来很自然，例如：<code>select(id, username).from(user)</code>，同时保持完全的类型安全和重构友好。</p><h3>映射器接口</h3><pre><code class="java">@Mapper
public interface UserMapper {

    @SelectProvider(type = SqlProviderAdapter.class, method = "select")
    List&lt;User&gt; selectMany(SelectStatementProvider selectStatement);
}</code></pre><p><code>@Mapper</code> 注解告诉 MyBatis 此接口应注册为映射器并在运行时进行代理。MyBatis 会自动生成实现，因此不需要具体的类。</p><p><code>selectMany</code> 方法接受一个 <code>SelectStatementProvider</code>，它封装了完全呈现的 SQL 语句及其参数。MyBatis 执行该语句并将每个结果行映射到 <code>User</code> 对象，将它们作为 <code>List&lt;User&gt;</code> 返回。</p><p><code>@SelectProvider</code> 注解指定 SQL 将由 MyBatis Dynamic SQL 的一部分 <code>SqlProviderAdapter</code> 动态提供。实际的 SQL 是在运行时从使用 DSL 构建的 <code>SelectStatementProvider</code> 生成的，而不是在注解或 XML 中编写 SQL。</p><h2>4. 构建动态查询</h2><p>在这里，我们使用流畅的 Dynamic SQL DSL 构建 SQL 语句，而不是编写原始 SQL 字符串。</p><pre><code class="java">public static void main(String[] args) throws Exception {

    MyBatisUtil.runSchema();

    try (SqlSession session = MyBatisUtil.getSession()) {

        UserMapper mapper = session.getMapper(UserMapper.class);

        SelectStatementProvider select
                = select(id, username, email, age)
                        .from(user)
                        .where(age, isGreaterThan(18))
                        .and(username, isLike("%tho%"))
                        .orderBy(username)
                        .build()
                        .render(RenderingStrategies.MYBATIS3);

        List&lt;User&gt; users = mapper.selectMany(select);

        users.forEach(u
                -&gt; System.out.println(u.getUsername() + " - " + u.getAge()));
    }
}</code></pre><p>此代码使用流畅的 Dynamic SQL DSL 动态构建一个 <code>SELECT</code> 查询，并将其渲染为与 MyBatis 兼容的语句提供者。通过以编程方式添加条件，它能够以类型安全且可维护的方式创建复杂的过滤器。在本例中，查询选择 <code>age</code> 大于 18 岁且 <code>username</code> 包含 <code>"tho"</code> 的用户，然后按用户名字母顺序对结果进行排序。</p><h3>MyBatis 工具类</h3><pre><code class="java">public class MyBatisUtil {

    private static SqlSessionFactory factory;

    static {
        try {
            Reader reader = Resources.getResourceAsReader("mybatis-config.xml");
            factory = new SqlSessionFactoryBuilder().build(reader);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static SqlSession getSession() {
        return factory.openSession(true);
    }

    public static void runSchema() throws IOException, SQLException {
        try (SqlSession session = getSession()) {
            Connection conn = session.getConnection();
            Statement stmt = conn.createStatement();

            try (InputStream is = Resources.getResourceAsStream("schema.sql")) {
                String sql = new String(is.readAllBytes(), StandardCharsets.UTF_8);
                stmt.execute(sql);
            }
        }
    }
}</code></pre><p>此工具类加载 MyBatis 配置，构建 <code>SqlSessionFactory</code>，并提供对数据库会话的访问。它还通过执行 SQL 脚本（<code>schema.sql</code>）手动初始化数据库模式。</p><h2>5. 使用 Dynamic SQL 进行插入、更新和删除</h2><p>MyBatis 中的 Dynamic SQL 允许我们使用流畅的 DSL 以编程方式构造 INSERT、UPDATE 和 DELETE 语句。在此，我们演示如何执行这些常见的数据操作。</p><pre><code class="java">// INSERT
User newUser = new User();
newUser.setUsername("andrew");
newUser.setEmail("andrew@jcg.com");
newUser.setAge(28);

InsertStatementProvider&lt;User&gt; insert
        = insert(newUser)
                .into(user)
                .map(username).toProperty("username")
                .map(email).toProperty("email")
                .map(age).toProperty("age")
                .build()
                .render(RenderingStrategies.MYBATIS3);

int inserted = mapper.insert(insert);
System.out.println("Rows inserted: " + inserted);

// UPDATE
UpdateStatementProvider update
        = update(user)
                .set(age).equalTo(35)
                .where(username, isEqualTo("thomas"))
                .build()
                .render(RenderingStrategies.MYBATIS3);

int updated = mapper.update(update);
System.out.println("Rows updated: " + updated);

// DELETE
DeleteStatementProvider delete
        = deleteFrom(user)
                .where(age, isLessThan(18))
                .build()
                .render(RenderingStrategies.MYBATIS3);

int deleted = mapper.delete(delete);
System.out.println("Rows deleted: " + deleted);</code></pre><p>相同的 DSL 风格也用于写操作。语句以流畅的方式构建、渲染，然后由映射器提供者方法执行。</p><ul><li><strong>INSERT</strong>：创建一个新的 <code>User</code> 对象并填充值。使用 Dynamic SQL DSL，我们将其字段映射到表列并生成 <code>InsertStatementProvider</code>。映射器执行插入操作，返回受影响的行数。</li><li><strong>UPDATE</strong>：DSL 构建一个更新语句，将用户名为 "thomas" 的用户的年龄设置为 35。这确保只修改目标行，映射器执行更新。</li><li><strong>DELETE</strong>：删除语句移除所有年龄小于 18 岁的用户。在 DSL 中使用条件保证了类型安全并避免了字符串拼接。</li></ul><h3>更新后的映射器接口</h3><p>为了支持这些操作，映射器接口必须包含用于 INSERT、UPDATE 和 DELETE 的方法，使用 MyBatis Dynamic SQL 提供者。</p><pre><code class="java">// INSERT
@InsertProvider(type = SqlProviderAdapter.class, method = "insert")
int insert(InsertStatementProvider&lt;User&gt; insertStatement);

// UPDATE
@UpdateProvider(type = SqlProviderAdapter.class, method = "update")
int update(UpdateStatementProvider updateStatement);

// DELETE
@DeleteProvider(type = SqlProviderAdapter.class, method = "delete")
int delete(DeleteStatementProvider deleteStatement);</code></pre><p>映射器中的每个方法处理一个特定的 DML 操作（插入、更新或删除），并接受一个封装了生成的 SQL 及其参数的 <code>InsertStatementProvider</code>、<code>UpdateStatementProvider</code> 或 <code>DeleteStatementProvider</code>。这种方法允许所有写操作都在 Java 中以编程方式表达，而无需手动组合 SQL 字符串，同时仍能利用 MyBatis 高效地执行语句和映射结果。</p><h2>6. 结论</h2><p>在本文中，我们探讨了如何在 Java 应用程序中使用 MyBatis Dynamic SQL 来创建类型安全、可维护且可编程的 SQL 查询。通过将 SQL 构建与执行分离，MyBatis Dynamic SQL 简化了复杂查询逻辑的处理，降低了错误风险，并提高了代码可读性。这种方法非常适合查询需要动态变化或经常修改的应用程序。</p><h2>7. 下载源代码</h2><p>本文讨论了 MyBatis Dynamic SQL 及其在 Java 中的使用方法。</p><blockquote><p><strong>下载</strong></p><p>您可以通过此处下载此示例的完整源代码：<a href="https://link.segmentfault.com/?enc=ILTYOzXlR86A8vtBigKPgw%3D%3D.ddhXL1PieIBrL4NmSt7Q4381TXn%2B2f5JMCpTtpIf66U1l8bqEXGsZ%2BZAw5a5pM%2FynDHE2YgmL3fICeOkFkVKrVJ%2Fi%2FvCl%2FLUDUqKh3oY1asWEdPNGCK2qB9qO%2B26G9XP" rel="nofollow" target="_blank">java mybatis dynamic sql</a></p></blockquote><hr/><p>【注】本文译自：<a href="https://link.segmentfault.com/?enc=YT5fhyrDVR6uE3oGQMhj3w%3D%3D.u4fgmPkETAvUHmpRMgTBLwJTqNkMQFAGUyDWbrZgT%2BfYfctkB8%2BbAwJprK9Q%2BDEhYI%2BckanZkBkiBAY3LUtOgmfEwiFH%2FF2defnaE2YuU04%3D" rel="nofollow" target="_blank">Getting Started with MyBatis Dynamic SQL</a></p>]]></description></item><item>    <title><![CDATA[双 11 大促峰值不翻车：淘天集团 Paimon + StarRocks 大规模 OLAP 查询实战]]></title>    <link>https://segmentfault.com/a/1190000047575567</link>    <guid>https://segmentfault.com/a/1190000047575567</guid>    <pubDate>2026-01-27 17:04:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：朱奥 /淘天集团高级数据工程师</p><blockquote><p>导读：双 11 等大促场景会在短时间内集中爆发：运营与业务 BI 在开卖后的窗口期密集访问数据产品，瞬时请求量陡增，对查询引擎的稳定性、成本与治理体系提出极高要求。与此同时，业务对近实时数据产品的诉求持续增强，传统“多存储、多链路、依赖回刷”的模式在研发效率、回刷成本与响应速度上逐步暴露瓶颈。</p><p>本文围绕 Paimon 与 StarRocks 的组合实践，梳理淘天在大规模 OLAP 查询场景下的架构演进与双 11 保障体系：通过实时与离线统一入湖，消除数据同步链路与多份存储成本；基于稳定中间层叠加在线现算与维表实时关联，将高消耗回刷转化为秒级查询，核心场景回刷效率提升约 80%，年化节省成本接近 1000 万；同时结合 StarRocks + RoaringBitmap 低成本解决跨天交叉实时 UV 计算难题，满足大促近实时决策需求。</p></blockquote><h2>1 淘天集团营销活动 OLAP 查询的探索背景与核心策略</h2><h3>1.1 当前数据架构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575570" alt="" title=""/></p><p>首先，简要介绍当前的数据架构与数据流转方式。</p><p>从 DWD 层开始，我们的数据分为实时与离线两条主链路：</p><ul><li>实时数据主要存储在 TT中，在业界可类比为 Kafka 一类的消息队列；</li><li>离线数据主要存储在 ODPS 中。</li></ul><p>在数据加工与写入层面，我们会启动 Flink 流批一体任务：</p><ul><li>实时侧持续消费 TT 中的数据；</li><li>离线侧消费 ODPS 中的数据；</li><li>在计算过程中，任务会关联多类 ODPS 维表，例如类目维表、商家分层等维度信息。</li><li>计算完成后，结果统一写入 ADS 层的Holo 表中，并在数据服务层对外透出。</li></ul><p>在纯离线场景下，我们会通过 ODPS 任务读取 ODPS 数据，同时写入 ADS 层对应的 ODPS 表。这里既包含历史天级数据，也包含历史小时级数据。当存在查询加速需求时，我们还会将 ODPS 数据进一步导入到 Holo 中。</p><p>在数据服务层，我们主要通过 Holo 或 MC 对外提供数据服务。我们会根据查询时延要求选择不同的服务路径：当业务对响应速度要求更高、需要达到毫秒级时，通常通过 Holo 提供查询服务；当时延要求相对宽松，例如百毫秒级或秒级，则更多通过 MC 来承载查询请求。</p><h3>1.2 业务诉求与核心痛点</h3><p>随着业务发展，我们当前面临的诉求主要来自两个方向：一是业务侧希望获得更多实时数据产品；二是业务 BI 的实时分析需求持续增长。这对数据研发提出了新的挑战：进一步提升研发效率。</p><p>回到现有架构，其核心痛点主要体现在两方面：</p><ul><li><strong>流批存储不统一</strong>：实时数据存储在 TT 中，离线数据存储在 ODPS 中；当存在查询加速需求时，部分数据还需要进一步落到 Holo 表中。</li><li>整体开发架构较为复杂，数据需要在多个存储介质之间流转，导致端到端链路拉长。</li></ul><p>在查询特性上，Holo 在点查场景具备更突出的性能优势，且整体稳定性较强，在淘天历年大促期间的表现也相对稳定。</p><p>但在更常见的 Shuffle 场景下，整体查询性能相对一般。尤其当 OLAP 查询负载更重、需要进行更复杂的计算，或需要关联规模较大的维表时，Shuffle 相关的执行效率会成为瓶颈，导致查询耗时明显拉长。</p><p>数据更新与维护上也存在较高成本。以 ODPS 中的维表为例（如类目维表、商家分层维表等），当维表发生业务变更时，往往需要触发 ADS 层任务的回刷，从而带来额外的回刷开销。</p><p>业务对“近实时”的诉求在部分场景下出现了被动降级。例如在跨天实时 UV 等场景中，由于 state 规模较大、成本较高等原因，方案不得不从实时级别降级到小时级别。从业务视角看，近实时能力仍然是明确存在的需求。</p><h3>1.3 核心策略</h3><p><strong>1）架构简化提效</strong></p><ul><li>架构上实现 <strong>存储介质的统一</strong>：将实时与离线数据统一沉淀到 Paimon 的湖存储中。在此基础上， <strong>StarRocks 可以直接面向湖存储进行高性能分析查询</strong>，从而能够消除数据同步链路以及多份存储带来的成本。</li><li><strong>降低使用门槛，</strong>让数据更容易被上层分析与 BI 使用。以实时链路为例，原本实时数据存储在 TT 中，而 TT 的数据形态具有明显特征：每行数据是一个字符串、缺少 schema。在这种形态下，数据虽然可以被消费，但如果要面向 BI 分析使用，往往还需要额外进行反序列化与解析，这会带来不可忽视的工程与使用成本。</li></ul><p>在统一存储之后，Paimon 将实时与离线数据沉淀在同一张表中，并提供明确的 schema。这意味着，上层使用方可以直接面向结构化数据开展分析：即使分析师的数据开发能力不强，也可以基于 Paimon 的近实时中间层，通过 <strong>StarRocks</strong> 自助完成对近实时数据的分析。</p><p>在这种模式下，过去一些相对简单的取数与分析需求，可以由 BI 或分析师通过自助方式直接完成，不再必须提交给数据研发排期处理，从而在一定程度上减少数据开发侧的需求量与交付压力。</p><p>2） <strong>业务难点攻坚</strong></p><p>通过稳定的中间层 Paimon，以及“OLAP 实时关联易变维度”的模式，将原本高消耗的 ADS 回刷任务转化为秒级查询来完成。在后续内容中，我会进一步展开这一改造如何将高消耗回刷去掉，并带来显著的成本收益——每年可节省近千万元级别的回刷成本。</p><p>同时，我们通过 StarRocks + RoaringBitmap 的方案，高性能解决了跨天交叉实时 UV 的计算难题，以更低成本的方式满足大促期间对近实时能力的诉求。</p><h3>1.4 新数据架构</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575571" alt="" title="" loading="lazy"/></p><p>在秒级数据链路上，我们通过实时 Flink 任务消费 DWD 层的 Fluss（秒级实时数据），并将结果写入 ADS 层的 Fluss。</p><p>Fluss 提供“湖流一体”的同步开关。开启后，Fluss 中的数据会按配置周期自动同步到 Paimon 表中，默认周期可以是每 3 分钟，且该时间间隔支持用户自定义配置。同步完成后，Paimon 表中会形成当天分钟级数据（t 当天）以及 t−n 的历史数据。</p><p>在此基础上，我们会启动 Flink 流批一体任务，同时消费 DWD 层 Paimon 的 t 当天数据与 t−n 历史数据，并将加工结果写入 Paimon 表的 ADS 层与 DWD 层，分别沉淀 t 当天与历史数据。</p><p>此外，基于 Paimon 的 partial-update能力，我们也可以构建离线大宽表，用于承载同一业务对象的多状态聚合。以订单为例，订单存在支付、确收、退款等多种状态，可以构建一张以 order\_id 为主键的 Paimon 大宽表，将这些状态写入同一行记录。这样在使用侧只需读取对应 order\_id 的一条记录，即可获取该订单的多种状态信息，使用成本与分析便利性都会更高。</p><p>在 ADS 层，我们沉淀的计算结果主要面向“叶子粒度”的维度：例如类目侧以叶子类目为主；若涉及商家分层维表，则对应叶子商家分层。在数据服务层，我们通过 StarRocks 对外提供数据服务。具体而言，在 StarRocks 层既可以直接读取 ADS 层数据进行点查，也可以直接读取 DWD 层的中间层数据进行在线计算。后一种方式的查询负载通常更重、数据量更大，但在当前实践中，StarRocks 仍然能够将查询时延控制在秒级范围内，在查询量较大的情况下保持较快响应。在查询过程中，我们也可以进一步关联 Paimon 维表，最终将查询结果在数据产品端进行展示与交付。</p><p>在我们 <strong>的业务场景中</strong>，一般来说，DWD 层的中间层事实数据相对稳定；真正“易变”的往往是维度侧的数据，例如 Paimon 维表（类目维表、商家分层维表等）。当业务规则或口径发生调整时，通常只需要更新维表即可。相较于回刷大规模中间层数据，维表更新的成本更低、执行也更快。</p><p>更关键的是，我们在查询侧采用现算方式：维表更新后，查询会在读取中间层数据的基础上实时关联最新维表，因此中间层数据无需随业务变更反复回刷。由于中间层计算量较重，如果依赖回刷来响应业务调整，整体周期往往较长——快则一到两天，慢则可能需要一周。通过“更新维表 + 查询现算”的方式，业务变更后可以更快在数据产品侧看到最新结果。</p><p>在数据服务层，我们进一步利用 StarRocks 的 Warehouse 机制，对读集群进行隔离与分级保障，避免不同业务互相影响。我们按照业务重要性划分为三类：</p><ul><li>默认 Warehouse：保障级别相对一般；</li><li>重保 Warehouse：承载最核心业务，保障级别最高；</li><li>业务 BI 专用 Warehouse：面向业务 BI 或其他业务的专用资源池，保障级别相对一般。</li></ul><h2>2 Paimon+StarRocks 在双11大规模 OLAP 查询场景下的实践与优化</h2><h3>2.1 业务背景</h3><p>在日常情况下，运营和业务 BI 往往在不同时间访问数据产品，因此 StarRocks的瞬时请求量（RPS）整体较低，压力相对平稳。</p><p>但在大促期间情况会明显不同。以开卖时段为例，运营和业务 BI 通常会在接下来的一小时内集中访问数据产品，导致 StarRocks 的瞬时请求 RPS 急剧升高，对 StarRocks 集群带来显著挑战。</p><p>因此，本部分的实践与优化工作主要围绕“大促场景稳定运行”这一目标展开。</p><h3>2.2 集群侧保障</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575572" alt="" title="" loading="lazy"/></p><p><strong>1）在应用层面推广数据集缓存策略</strong>：目前配置 180 秒的查询缓存窗口。也就是说，同一条查询在 180 秒内被多次触发时，实际下发到 StarRocks 执行的仅为首次请求；后续请求直接复用首次查询结果。通过该策略，可以有效降低大促高峰期 StarRocks 集群的瞬时压力。</p><p><strong>2）集群层面的保护机制：</strong>集群侧设置了 30 秒的全局超时：如果一条 SQL 在 30 秒内仍未执行完成，会被自动终止。该机制属于 StarRocks 的集群保护能力，当查询执行时间超过 30 秒，即可判定该 SQL 需要进一步优化，不适合直接上线，需要回退并完成优化后再进入生产环境。对于少量确有必要、且在 30 秒内无法完成的特殊 SQL，也支持为单条 SQL 配置更长的超时时间。但此类 SQL 数量通常极少，上线评估也会更加严格，以确保不会对整体集群稳定性产生影响。整体目标是避免单条慢 SQL 拖垮集群。</p><p><strong>3）架构层隔离：按业务重要性划分只读实例。</strong>基于业务重要性对只读查询资源进行分层，将不同业务的读请求隔离到不同的只读实例上，避免相互干扰。</p><p><strong>4）集群初始化配置</strong></p><p>在新的 StarRocks 集群初始化时，比较推荐先设置一套基础参数，如下：</p><ul><li><strong>set global cbo\_cte\_reuse_rate=0;</strong></li></ul><p>当 CTE 被多处引用时，可能触发同一数据源的重复读取。例如，一个表在 select 中读取三次，那么 StarRocks会对同一张 Paimont 表执行三次读取，读 I/O 开销相当于被放大为 3 倍。将该参数设置为 0 后，可使同一张表在同一条查询中只读取一次。</p><p><strong>•set global query_timeout=30;</strong></p><p>设置 30 秒的集群全局查询超时 <strong>，</strong>避免单条慢 SQL 拖垮集群。</p><p><strong>•set global new\_planner\_optimize_timeout=10000;</strong></p><p>适当调大执行图优化器的超时时间。如果该参数设置过小，SQL 在调度过程中更容易直接失败；适当增大后，可降低 SQL 失败的频率。</p><p><strong>•set global pipeline_dop=8;</strong></p><p>调整 pipeline 的 DOP，用于控制每台机器上拉起的 driver 数量。压测结果显示，在大促场景中 SQL 请求高度集中，若 DOP 设置过大（例如 64），单条 SQL 在每台机器上会拉起大量 driver，带来调度开销飙升，甚至可能打满 driver 阻塞队列，导致 CPU 利用率反而上不去，集群进入不可用状态。</p><p>在我们StarRocks集群的双 11 压测中，DOP 调整到 8 时整体查询表现最优，因此给出 DOP=8 作为建议值。需要强调的是，该值是经验建议，最终仍应以各自集群的压测结果为准进行配置。</p><p><strong>•set global scan\_paimon\_partition\_num\_limit=100; --限制scan paimon外表的最大分区，杜绝扫描全表的情况</strong></p><p>限制 scan paimon外表的最大分区，用于杜绝因条件缺失或下推失败导致的全表/超大范围扫描。</p><h3>2.3 核心指标监控</h3><p>通过观察 StarRocks 核心指标的水位变化，可以持续评估实例健康状况。常用的核心指标如图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575573" alt="" title="" loading="lazy"/></p><h3>2.4 报警规则</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575574" alt="" title="" loading="lazy"/></p><p>建立 StarRocks 实例的异常报警机制非常关键，它能够帮助及时发现实例异常并快速介入处理。报警项的设置通常围绕“资源水位、节点可用性、调度拥塞、查询失败与时延”几类核心信号展开，其中有一部分阈值来自大促压测与实战探索，具有较强参考价值：</p><ul><li>BE/CN 的 CPU 与内存使用率设置阈值，例如当使用率持续高于 70% 时触发告警；</li><li>FE 的 CPU 与内存使用率同样设置 70% 的告警阈值；</li><li>在可用性方面，可以监控 BE/CN 或 FE 的可用率是否低于 100%，一旦出现低于 100% 的情况，通常意味着有节点不可用或发生故障。</li><li>当 BE 阻塞队列数超过 2000 时，StarRocks 集群的查询时延可能出现陡增；</li><li>在查询侧，可以增加查询失败次数与查询时延分位数的告警，例如“查询失败次数大于 n”“查询延迟 TP99 大于 n”。其中 n 的取值需要结合业务特性与可接受的服务水平目标进行配置。</li></ul><h3>2.5 元数据监控</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575575" alt="" title="" loading="lazy"/></p><p>为更有效地治理 StarRocks的各类查询请求，可以实时获取审计日志，并基于审计日志构建元数据监控大盘，为后续的慢查询 SQL 治理提供数据支撑与定位依据。</p><p><code>select * from _starrocks_audit_db_.starrocks_audit_tbl;</code></p><p>审计日志相关数据落在 <strong>StarRocks</strong> 的内表中，对应信息可实时查询。也就是说，某条 SQL 执行完成后，可以立即在该内表中查到这条 SQL 的执行耗时等关键字段。基于这一基础能力，如果需要进一步做更细的源数据与查询行为监控，也可以围绕审计日志中记录的 SQL 信息进行扩展。</p><p>在监控大盘的组织方式上，支持按 Warehouse 维度拆分（例如划分为多个 Warehouse），同时也可以按数据集进行过滤。在筛选完成后，重点关注的数据字段通常包括：数据集名称、总 CPU 消耗、总查询大小、查询次数、查询行数、失败率与失败次数、单次查询的 CU 消耗、查询时间以及查询发起人等。这些指标支持排序与聚合，便于在优化过程中选取特定时间窗口，对总 CPU 消耗、总查询大小、总查询行数等维度进行 Top SQL 排查与治理。通过优先治理这些“高消耗/高影响”的 SQL，往往能够显著改善集群整体健康状况，因为在许多情况下，集群不稳定的根因来自少量高风险的“坏 SQL”。</p><h3>2.6 大促保障</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575576" alt="" title="" loading="lazy"/></p><p>大促保障的目标，是把不确定性尽量前置消化，确保开卖高峰期间查询链路稳定可控。</p><ul><li><strong>在资源侧，</strong>会结合历史数据与业务预测，在大促开始前对 StarRocks 集群进行主动扩容，并在大促结束后主动缩容。</li><li><strong>在需求侧，</strong>提前与业务负责人对齐本次大促的核心变更点，重点关注改造或新增页面，并将核心页面的 QPS 进行量化，为全链路压测与容量评估做准备。</li><li><strong>针对重保页面</strong>，我们还会建立一套智能应急机制，分为实例级与查询级两层。实例级故障切换方面，当 StarRocks 主实例不可用时，可通过自动化预案工具（FBI）将重保页面的查询请求批量切换到备库 Warehouse，完成实例级容灾；查询级自动容错方面，当重保页面出现单次查询失败或超时，系统会将该查询自动路由到备库 Warehouse 重试，尽量做到用户无感，为关键 SQL 增加一次“二次机会”，提升整体稳定性。</li></ul><h3>2.7 大促压测</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575577" alt="" title="" loading="lazy"/></p><p>大促压测通常分为两层： <strong>核心页面单压与全链路压测。</strong></p><p>在核心页面单压阶段，会先梳理大促期间的核心页面及新增页面，并对这些页面进行单独压测。这样做的目的，是尽可能在活动前置暴露并解决单点问题导致的性能瓶颈，为后续上线留出精细化优化空间。</p><p>在全链路压测阶段，会模拟“所有页面同时达到流量峰值”的极限场景，用以验证 StarRocks 集群在峰值冲击下的整体资源水位与关键性能指标是否符合预期。重点关注的资源水位通常包括 CPU、内存与 I/O，同时结合查询时延等指标，评估集群在极端并发与高负载下的稳定性与承载边界。</p><h3>2.8 压测发现的问题和优化方案</h3><p><strong>1）分区裁剪失效或缺少分区过滤，导致扫全表</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575578" alt="" title="" loading="lazy"/></p><p>压测中发现，部分 SQL 因分区裁剪失效或未配置分区过滤条件，出现扫描范围过大甚至扫全表的风险。针对该类问题，治理原则是必须启用分区过滤并确保分区裁剪生效，不允许存在扫全表 SQL 在线运行。</p><p>分区裁剪生效的常见写法包括：对分区字段进行日期传参，直接基于分区字段触发裁剪；或使用日期函数触发裁剪，例如 date\_format、date\_add 等函数也可以触发分区裁剪。</p><p>分区裁剪失效的典型场景是分区字段与子查询结果进行比较，例如将分区字段与子查询返回的最小活动时间进行对比时，分区裁剪会失效。原因在于分区裁剪发生在 FE 阶段，而子查询需要到 BE 执行，FE 在规划阶段无法获得子查询结果，从而无法生成有效的分区裁剪信息。</p><p><strong>2）读取 Paimon 生表时小文件过多，导致读取数据块数过大</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575579" alt="" title="" loading="lazy"/></p><p>压测还发现，读取 Paimon 表时存在小文件过多的问题。</p><p><strong>定位方法</strong>：在 StarRocks 执行 SQL 时可开启 profile（通过 hint：/<em>+ SET\_VAR(enable\_profile = true) </em>/）生成 profile 文件；在 profile 中搜索 “metadata”，其中 nativeReaderReadNum 表示读取的数据块数，nativeReaderReadBytes 表示读取的字节数。实践中，当单个分区的 nativeReaderReadNum 大于 200 时，通常建议考虑对表进行排序治理。</p><p><strong>优化方案</strong>：在构建流批排序Paimon表时，建议采用分支表模式：离线分支将 bucket 设为 -1，实时分支按需设置 bucket。离线分支表通过 clustering columns 指定排序字段，可支持指定多个字段（如 f1、f2），一般选择 OLAP 查询中最常用的过滤字段，以提升过滤命中与读取效率。该能力仅支持 Flink 批写入，不支持 ODPS 写入；写入表时需要使用 hint： <strong>/*+ OPTIONS('sink.parallelism' = '64') */</strong>。对于 ODPS 写入的 Paimon 表，则需要在任务下挂一个单独的 compact 排序任务。</p><p><strong>为何有效</strong>：在双 11 场景下，活动周期往往持续数十天。当天数据属于实时增量，而从活动开始到昨天的历史数据占比更大；因此对离线数据进行表排序收益显著。压测实测显示，排序后读取的数据块数约为排序前的 1/1000。 离线分支完成排序后，活动开始到昨天（占比最大的历史数据）基本都处于“已排序、数据块读取量很小”的状态；实时分支由于无法排序，读取的数据块会相对多一些，但实时数据通常只存在于当天，整体占比小，因此对整条 SQL 的查询时延影响相对有限。</p><p><strong>3）检查是否命中 MapJoin：小维表建议显式 broadcast</strong></p><p>当 SQL 需要 join 小表（例如小于 10MB 的维表）时，建议在维表前显式加 broadcast，以触发类似离线 MapJoin 的执行策略。实测显示，引入 broadcast 后查询时延可显著下降，典型场景下可从十几秒优化到约 3 秒，整体查询时延约为原先的 1/3。</p><p><code>SELECT xxx FROM table_a t0 LEFT JOIN [broadcast] dim_table_b t1 ON t0.cate_id = t1.slr_main_cate_id AND t1.ds = 'xxx'</code></p><p><strong>4）检查跨地域访问：计算与存储尽量同地域部署</strong></p><p>还需要确认 StarRocks 实例与所读取的 Paimon 表是否处于同一地域。若不在同一地域，查询时延会明显增加。建议将 StarRocks 的部署地域与 Paimon 表存储地域保持一致。</p><p><strong>5）主键表建议开启 deletion vectors：减少无效数据读取</strong></p><p>对于 Paimon 主键表，建议开启 'deletion-vectors.enabled' = 'true'参数。该能力会在写入阶段记录哪些主键数据已被删除；读取时可跳过已删除数据，减少无效扫描，从而提升查询性能。非主键表不需要开启该参数。</p><h2>3 阶段成果与未来规划</h2><h3>3.1 阶段成果</h3><p>整体来看，该方案带来了四方面阶段性成果。</p><ul><li><strong>数据链路得到简化</strong>：通过统一存储与统一查询面，消除了数据同步链路，并降低了多份存储带来的成本与复杂度。</li><li><strong>数据使用门槛显著降低</strong>：基于 Paimon 的实时/离线中间层，不仅数据开发人员可以使用，业务分析师也可以通过 StarRocks 自助消费近实时数据，从而减少部分简单需求对数据研发排期的依赖。</li><li><strong>回刷开销得到明显削减</strong>： <strong>核心场景的回刷效率提升约 80%，年化节省成本接近 1000 万。</strong>其关键在于查询可以直接读取 Paimon 公共层并关联 Paimon 维表，业务变更时只需刷新维表，无需回刷与该维表相关的整条数据链路。</li><li><strong>在高性能实时分析方面，低成本解决了跨天交叉维度实时 UV 的计算难题，满足大促期间近实时决策需求。</strong>具体做法是将可累加指标（如订单数、订单支付金额等）与不可累加指标（如 user\_id）分开处理：可累加指标在查询侧直接聚合；不可累加指标则将 user\_id 做 RB 化后存入中间层，StarRocks 读取 Paimon 表时通过 RB 相关函数计算 UV。</li></ul><h3>3.2 未来规划</h3><p>面向下一阶段，规划主要集中在四个方向。</p><p>第一， <strong>希望 StarRocks 具备更强的自动物化能力</strong>：针对用户高频查询的 SQL 自动生成物化结果，并在后续查询中自动完成改写，直接命中物化表。由于物化表往往已经完成聚合，其数据量相较直接查询中间层可以小很多个量级，从而显著降低扫描与计算开销，进一步提升查询速度与稳定性。</p><p>第二，计划进一步 <strong>丰富 StarRocks 的元数据能力</strong>。</p><p>第三， <strong>优化 StarRocks 的调度策略</strong>，重点是调度层面的 CPU 负载均衡能力。</p><p>第四，希望 <strong>StarRocks 具备直接读取 Fluss 的能力</strong>，从而支持秒级查询场景。目前 Paimon 仍以分钟级链路为主，如果能够在读取侧进一步下探到 Fluss，将更好覆盖对秒级实时性有明确诉求的业务场景。</p>]]></description></item><item>    <title><![CDATA[[2026 深度横评] 金融行情 API 红黑榜：Stripe 封神，传统接口“劝退”，谁是架构师的]]></title>    <link>https://segmentfault.com/a/1190000047575614</link>    <guid>https://segmentfault.com/a/1190000047575614</guid>    <pubDate>2026-01-27 17:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>引言：当 DX 成为核心生产力</p><p>在 2026 年的今天，当我们谈论 SaaS 服务时，Stripe 和 Twilio 依然是绕不开的标杆。不仅因为它们的市场份额，更因为它们定义了什么是“现代化的开发者体验 (DX)”。</p><p>作为一个长期在后端与量化系统摸爬滚打的工程师，我常有一种强烈的割裂感： 左手接 Stripe，行云流水，Ctrl+C 加上几行配置就能跑通支付流程； 右手接传统券商的行情 API，步履维艰——面对着上世纪的 FIX 协议文档、强制运行的 Java 网关客户端、以及薛定谔的 WebSocket 连接状态。</p><p>这不仅仅是“好用”与“难用”的区别，这是一种隐形的“集成税” (Integration Tax)。它消耗了开发团队 30% 以上的时间去处理本该由基础设施层解决的问题。</p><p>本文基于 Postman 2026 行业报告及社区实战案例，试图从工程视角对当前的金融数据 API 进行一次梯队分级：一套合格的、符合 AI 时代标准的金融数据 API，究竟应该长什么样？</p><hr/><p>一、 第一梯队（The Gold Standard）：审美与逻辑的统一<br/>为什么开发者会“爱上”某个 API？这并非玄学。我们深入分析了 Stripe 和 Polygon（美股数据服务商）的文档架构，发现位于第一梯队的 DX 设计都有着极其相似的基因。</p><ol><li>视线的“F型扫描”与三栏布局<br/>Stripe 首创的三栏式布局，本质上是对开发者工作流的视觉映射：</li></ol><p>左侧 (Context)：资源导航。解决“我在哪”的问题。</p><p>中间 (Logic)：业务逻辑与参数释义。解决“这是什么”的问题。</p><p>右侧 (Action)：动态代码示例。解决“怎么用”的问题。</p><p>工程细节： 这一设计的精髓在于联动。当你点击中间栏的 expand 参数时，右侧的代码块应自动高亮对应行，甚至直接注入你当前的 Test API Key。这种“所见即所得”将 Time-to-First-Call (TTFC) 缩短到了秒级。</p><ol start="2"><li>SDK 的“手工感” (The Hand-Crafted Feel)<br/>Stainless 团队曾提出一个观点：“自动生成的 SDK 不应有机器的味道。”</li></ol><p>反模式 (Code-Generated)：api.get_v1_market_ticker_response_200_item(symbol="BTCUSDT") —— 这种冗长的命名是 Swagger Codegen 的典型产物，属于第二梯队的做法。</p><p>最佳实践 (Idiomatic)：client.Market.ticker("BTCUSDT") —— 符合直觉的 名词.动词 结构，强类型支持，代码本身就是注释。这是第一梯队的标准。</p><ol start="3"><li>错误处理的 RFC 7807 标准化<br/>传统接口喜欢返回模糊的 Error -1。而现代 API 应遵循 RFC 7807 (Problem Details for HTTP APIs)，返回结构化信息： { "code": 2002, "message": "Symbol not found. Did you mean 'BTC-USDT'?" } 这种设计将“查阅错误码文档”的时间转化为了“即时修复”的时间。</li></ol><hr/><p>二、 债务深挖：为何 90% 的行情接口都在“劝退”？<br/>尽管行业标准已在进步，但在 r/algotrading 等技术社区，针对行情数据 (Market Data) 的抱怨依然占据主流。我们将以下三种现象定义为<strong>“不及格”的架构设计</strong>：</p><ol><li>协议层的过度设计 (FIX vs. REST)<br/>FIX 协议是机构高频交易的基石，但对于现代 Web 应用或中低频量化策略，它过于厚重。</li></ol><p>痛点：需要维护复杂的 Session 状态机，解析二进制流或非标文本流。</p><p>现状：许多服务商甚至要求开发者在云服务器上运行一个重型 GUI 客户端作为网关，这与容器化、Serverless 的现代架构格格不入。</p><ol start="2"><li>WebSocket 的“静默丢包”<br/>这是分布式系统中的经典问题。当市场剧烈波动导致突发流量 (Burst Traffic) 时，服务端缓冲区溢出，可能会直接丢弃数据帧。</li></ol><p>致命伤：如果缺乏应用层的心跳与序列号机制，客户端往往误以为连接正常，实则已经漏掉了关键的市场波动。</p><p>工程解法：服务端推送必须包含单调递增的 sequence_number。客户端通过检测序号跳跃（如收到 100 后直接收到 102），主动触发 REST API 进行数据回补。</p><ol start="3"><li>命名空间的巴别塔<br/>不同交易所对同一个标的（如比特币）命名不一：XBTUSD, BTC-USD, BTCUSDT。开发者被迫编写大量胶水代码来清洗这些数据。 TickDB 等现代数据商的做法是在网关层统一映射为标准格式（如 Base_Quote），将清洗工作下沉到基础设施层。</li></ol><hr/><p>三、 架构建议：构建高可用的行情接入层<br/>对于技术负责人而言，在 2026 年接入行情 API，应建立一套完整的数据工程心智模型 (Mental Model)。</p><ol><li>建立映射层 (The Mapping Layer) 原则：Never Hardcode Symbols. 系统启动时的首个动作，应是调用 /v1/symbols 接口，拉取全量参考数据，并在本地 Redis 中建立 Exchange_Symbol -&gt; System_Symbol 的映射表。</li><li>读写分离：快照与流 (Snapshot vs. Stream)</li></ol><p>REST API (Snapshot)：适用于无状态场景（如 App 首页展示、资产估值）。不要用轮询 REST 来模拟实时，这极其低效。</p><p>WebSocket (Stream)：适用于有状态场景（如策略触发、盘口监控）。</p><p>连接复用：优秀的 WebSocket 设计应支持单连接订阅多 Symbol (Subscription Mode)，而非为每个 Symbol 建立连接。</p><ol start="3"><li>面向 AI 的架构 (Schema-First) Gartner 预测，2026 年 30% 的 API 调用将由 AI Agent 发起。 检查服务商是否提供标准的 OpenAPI Specification (OAS 3.0/3.1) 定义文件。这不仅是文档，更是 AI 理解你系统的“说明书”。有了它，ChatGPT 或 Claude 可以直接生成高质量的 Client 代码，甚至进行自动化测试。</li></ol><hr/><p>结语：让数据回归基础设施<br/>优秀的 API 文档和服务，应当像水电煤一样，稳定、标准、甚至“无感”。</p><p>无论是支付领域的 Stripe，还是致力于构建统一金融数据层的 TickDB，都在通过标准化的工程实践（Unified Symbols, OpenAPI, Reliable WebSocket），致力于消除非必要的工程摩擦 (Engineering Friction)。</p><p>我们希望，当你接入这些服务时，不再感觉是在进行“考古挖掘”，而是在用现代化的工具，搭建属于未来的金融应用。</p><p>如果你对这种 Schema-First 的设计理念感兴趣，不妨在 GitHub 上搜索一下 TickDB 的 OpenAPI 定义文件——哪怕不使用服务，里面的架构细节或许也能给你带来一些关于“现代化接口”的灵感。</p><hr/><p>(参考资料：Postman "2026 State of the API Report", Stainless Engineering Blog)</p>]]></description></item><item>    <title><![CDATA[什么是低代码平台?2026年主流低代码平台盘点 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047575617</link>    <guid>https://segmentfault.com/a/1190000047575617</guid>    <pubDate>2026-01-27 17:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>十几年前，记得我刚做企业数字化咨询时，我总被客户问到同一个问题：“能不能在三个月内帮我们把报销、工单、库存管理全打通？”但每次我都只能苦笑。</p><p>那时候还没有很好的工具，而如果用传统编码开发就像盖砖房，从打地基到砌墙抹灰，一步都不能省，三个月也只够搭个框架。当时我就想，要是有套积木式的开发工具就好了，业务人员说要什么，我们随手搭一搭，几天就能交出能用的系统。</p><p>后来几年，市面上也陆续冒出来一些“快速开发工具”，我带着客户也试过好几款。但用下来总觉得差口气：</p><p>要么只能做些简单表单，稍微复杂点的流程就卡壳；</p><p>要么和现有ERP、CRM系统完全割裂，数据得手动导来导去；</p><p>最头疼的是，改个字段还要找技术人员，调整个审批节点要等排期，本质上还是没跳出“依赖专业开发”的怪圈。</p><p>直到这两年，尤其是2026年低代码平台集体升级后，我才真切感受到：那个“用积木搭系统”的时代，真的来了。它们不再过去是边缘型工具（只能做一些简单功能的系统），而是成为了能扛事的核心基建，真正把想法变应用的周期，从月级压缩到了周级甚至天级。</p><p>一、低代码平台定义：</p><p>（一）权威定义界定</p><p>Gartner在2025年底的报告里给过明确界定：低代码开发平台（LCAP）是通过可视化建模+少量脚本，快速搭建业务应用的工具，核心是把数据建模、流程编排、权限管控等模块做成可复用组件，让开发从“手写代码”变成“模块化装配”。</p><p>这话翻译成人话，就是把传统开发里重复的、标准化的工作都做成“现成零件”，技术人员只需补少量代码解决复杂逻辑，业务人员甚至能自己拖拽配置简单应用。</p><p>这里面，让我触动最大的是：我上周帮一家制造企业搭生产工单系统，用低代码平台把需求落地，全程只写了30行自定义脚本，这在以前是不敢想的。</p><p>（二）核心特征解析</p><p>真正靠谱的低代码平台，都逃不开三个核心特征，少一个都容易踩坑。</p><p>一是“可视化全链路”，从表单设计、流程编排到页面展示、报表生成，全程拖拽操作，业务人员盯着就能看懂，不用再靠技术人员翻译需求。</p><p>二是“高低代码融合”，这是2026年的主流趋势，既能让业务人员无代码上手，又能给技术人员留足扩展空间，比如用自定义脚本处理复杂计算，用API对接特殊系统。</p><p>三是“一键部署与版本管控”，比如支持Dev/Test/Prod多环境隔离，应用改坏了能一键回滚，避免上线后出问题没法补救，这对中大型企业来说真的特别重要。</p><p>（三）企业价值落地</p><p>低代码的价值从来不是省代码，而是“提效率、降门槛、保灵活”。</p><p>我服务过的一家装备制造客户，用低代码打通了订单需求、研发项目与生产交付全链路，以前要跨3个部门、花两周才能理顺的需求追溯，现在在系统里一点就能查全，出错率下降了70%。</p><p>对中小企业，它能快速补齐数字化短板，不用花大价钱请外包团队；</p><p>对大型企业，它能支撑高频的业务迭代，比如市场部门要做活动报名系统，当天提需求当天就能搭好上线；</p><p>对技术团队或软件外包公司，它把程序员从重复劳动里解放出来，聚焦核心业务逻辑，人效至少提升2倍。</p><p>二、企业低代码平台选型核心框架：</p><p>这十几年帮客户选型踩过无数雷，我总结出一个道理：低代码选型不是看单一功能多炫，而是看能不能适配企业的真实场景。</p><p>以下五个维度，少一个都可能导致项目失败。</p><p>（一）技术架构适配性</p><p>架构是底子，底子不稳后期必崩。我见过一家连锁企业，前期选了国内某轻量型零代码平台，门店扩张到50家后，系统直接卡顿崩溃。</p><p>因为平台不支持分布式部署，数据处理能力跟不上。</p><p>要想避免此类问题发生，我建议大家选型时重点看这三点：</p><p>一是是否支持微服务与云原生，适配企业后期扩张；</p><p>二是多环境隔离与版本管理，避免开发、测试、生产环境互相干扰；</p><p>三是移动端适配与离线能力，尤其是门店、巡检等场景，离线表单与数据同步功能必不可少。</p><p>（二）功能完整性与场景适配</p><p>不同平台有不同的特性，适配场景天差地别。比如有的平台擅长审批流程，有的擅长数据看板，有的则适配复杂业务建模。</p><p>我通常会让客户先拿一个核心场景试手，比如采购审批、工单管理，看平台能否覆盖全流程。</p><p>以采购场景为例，要能实现需求提报、供应商选择、合同审批、入库对账全链路配置，还要支持自定义校验规则，比如超过10万金额自动触发多级审批，这样才算是真正适配业务。</p><p>（三）AI融合深度</p><p>2026年的低代码平台，AI能力的评估也很重要。但也要分清“伪AI”和“真AI”。</p><p>有些平台只做了代码片段生成，顶多省点打字时间；而真正的AI融合，是贯穿开发全链路的。</p><p>我上个月试用一家企业级AI低代码平台，用自然语言说“搭建一个销售台账，自动统计每月业绩并生成报表”，AI直接生成了数据模型、表单页面和统计逻辑，我只需要微调字段名称就行。</p><p>这里面更实用的是智能调试功能，系统能自动排查流程卡点，比人工找bug快多了。对业务人员来说，这种“自然语言转应用”的能力，才是真正降低了使用门槛。</p><p>（四）生态集成与数据能力</p><p>企业数字化不是从零开始，低代码平台必须能和现有系统打通贯通，否则就是新的信息孤岛。</p><p>我帮客户选型时，一定会做集成测试：能不能对接SAP、Oracle等传统ERP，能不能和企业微信、钉钉、飞书打通推送，能不能从数据仓库拉取历史数据。</p><p>优秀的低代码平台通常有丰富的现成连接器，支持REST API、Webhooks等多种集成方式，还能实现可视化数据映射。比如把ERP里的库存数据同步到低代码工单系统，字段对应错误能自动提醒，不用技术人员逐行核对。</p><p>（五）安全合规与服务保障</p><p>对金融、政务、制造等行业，安全合规是红线。选型时要重点看：</p><p>是否支持私有化部署，满足数据本地化要求；</p><p>是否有行级、字段级权限管控，避免敏感数据泄露；</p><p>是否通过ISO27001、等保安全等资质认证，操作日志是否完整可审计。</p><p>此外，后续的服务保障也不能忽视掉。我有个客户之前就被某平台售后搞的哑口无言。平台出现了一个问题，售后三天才响应，导致客户业务停滞。</p><p>所以，这一块要擦亮眼睛，深入评估。</p><p>三、2026年主流低代码平台推荐</p><p>这大半年我实测了市面上十多款低代码平台，结合不同行业场景，筛选出三款综合能力突出、适配性强的平台，各有侧重，可按需对号入座。</p><p>（一）织信低代码平台</p><p>织信的核心优势是“中大型企业复杂场景适配”，团队核心成员来自华为、平安，对企业业务管控和系统集成的理解很到位。我们团队目前是织信低代码平台的代理商。我们也是仔细筛选评估了4个月，最终才选择的织信。</p><p>他们最吸引我们的点是：一，功能很强大，算是国内顶尖的，拓展性强，这个我跟他们团队开过一次线上会议，就已经感受到了。二，合作模式性价比很高，买断式+SaaS多租户模式，可以让我们也有自己的利润空间。</p><p>我记得去年在帮一家工程设计院选型时，也是用织信低代码打通了投标立项、客户需求、设计任务与成果交付全链路，最惊艳的是它的业务对象建模能力，能把需求、任务、成本、预算等模块深度关联，实现全流程追溯。</p><p>它支持私有化部署，满足集团型客户的数据主权需求，OpenAPI能力也很强，能轻松对接SAP、Oracle等传统系统。适配场景集中在军工、制造业、工程建筑、战略咨询、金融服务等行业，适合有复杂业务逻辑、强集成需求的中大型企业。不足是标准化模版偏少，中小企业如果没有IT人员，上手需要一定的学习成本。</p><p>（二）网易CodeWave</p><p>网易CodeWave的亮点是“AI原生与全栈智能化”，以网易自研大模型为底座，把AI能力贯穿开发、测试、运维全链路。我用它搭建运营活动管理系统时，只说“做一个带报名、核销、数据统计的活动页面”，AI就自动生成了页面布局、交互逻辑和统计报表，还能通过AI测试机器人自动排查bug，效率比传统开发提升一倍多。</p><p>它采用自研NASL语言，支持多人协作开发，在游戏、电商、金融等行业有丰富内部实践，某大型国有银行用它开发台账管理、结算管理系统，提效降本达60%。适合对AI能力要求高、追求快速迭代的互联网企业和中小企业，不足是生态连接器数量比泛微少，对接部分传统系统需要额外开发。</p><p>（三）泛微e-builder</p><p>泛微e-builder胜在“协同能力与生态成熟度”，作为老牌协同办公厂商，它天然适配企业内部协同场景，支持无代码、低代码、全代码三种构建模式，业务人员能拖拽搭建轻量应用，技术人员可通过全代码模式定制复杂系统。</p><p>它的AI融合能力很实用，上传Excel或用自然语言描述需求，就能自动生成应用，还能对接企业微信、微信，实现内部员工与外部客户、合作伙伴的实时协同。云商店有上千款成熟应用模板，覆盖87个细分行业，开箱即用，适合重协同、需要快速落地标准化场景的中大型企业，尤其是集团型组织。缺点是在极端复杂的业务建模场景，灵活性不如织信。</p><p>总结：低代码的核心是“让业务驱动技术”</p><p>十多年从业下来，我见证了低代码从小众工具到企业数字化核心基建的转变。2026年的低代码平台，早已不是“少写代码”那么简单，而是通过AI赋能、生态集成，实现了“业务人员能上手、技术人员能提效、企业能快速落地需求”的闭环。</p><p>选型时不用盲目追功能最全，而是要找准企业的核心需求：中大型企业复杂场景选织信，重协同、要标准化模板选泛微e-builder，追AI效率、快速迭代选网易CodeWave。记住，低代码的终极价值，是让技术不再成为业务的瓶颈，让每个企业都能拥有“按需搭建系统”的能力。</p><p>未来两年，随着AI与低代码的深度融合，“人人都是开发者”或许真的会成为现实。而对企业来说，提前布局适合自己的低代码平台，就是抓住数字化转型的快车道。</p>]]></description></item><item>    <title><![CDATA[研发项目质量管理体系怎么搭：质量策划-保证-控制全流程 许国栋 ]]></title>    <link>https://segmentfault.com/a/1190000047575640</link>    <guid>https://segmentfault.com/a/1190000047575640</guid>    <pubDate>2026-01-27 17:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>为什么你需要“体系化”的研发项目质量管理</h2><p>很多组织对质量的误解是：把质量当成一个阶段（测试），而不是一个系统属性（从需求到运维的连续结果）。当质量缺少体系化治理，通常会出现三种“慢性病”：</p><p>标准不清：同样叫“完成”，有人理解为“开发完了”，有人理解为“可验收可上线”。<br/>证据断链：需求没有可验收标准、设计没有可测试性、代码没有可追溯变更依据，上线只能靠“信心投票”。<br/>反馈太晚：问题在上线后才暴露，代价从“修一个缺陷”变成“拖慢一个季度的路线图”。</p><p>一句话总结：研发项目质量管理体系，本质是把“质量要求”转化为“可验证、可追溯的证据链”，并通过持续改进让证据链更短、更自动化、更低成本。</p><h2>一个可落地的框架：质量策划-质量保证-质量控制</h2><p>如果你希望体系“既能讲得清，又能落得下”，最建议采用三段式闭环：</p><ul><li>质量策划（Quality Planning）：解决“标准与目标”——什么算好？阈值是什么？风险在哪里？</li><li>质量保证（Quality Assurance / QA）：解决“过程漂移”——有没有按标准做？哪里在走样？怎样让组织能力可复制？</li><li>质量控制（Quality Control / QC）：解决“结果可信”——交付物是否达标？是否可以上线？上线后是否稳定？</li></ul><p>这套拆分之所以长期有效，是因为它把质量治理从“临时补洞”变成“持续运营”：先定义规则，再让规则稳定执行，最后用数据验证结果与改进方向。ISO 9001 的过程方法与 PDCA 思路也强调用体系化的方式管理过程并驱动持续改进。</p><p>一句话总结：策划让大家对“好”达成共识；保证让“对的方法”持续发生；控制让上线不靠运气。这就是研发项目质量管理的主线。</p><h4>1.质量策划：先把“质量目标”与“验收标准”定清楚</h4><p>质量策划最容易犯的错，是把它写成“漂亮模板”，但缺少可执行阈值与角色分工。我的建议是先把质量策划压缩成一张“作战地图”，再逐步加细。</p><p><strong>① 设定质量目标：用“系统重要性 × 风险”定阈值，而不是一刀切</strong></p><p>质量目标可以分三层，但每一层都要明确“谁用它决策”：</p><ul><li>业务结果层（Outcome）：服务可用性、关键链路成功率、投诉/工单、SLA 违约等。</li><li>交付稳定层（Stability）：发布后 7 天严重缺陷逃逸率、回滚率、P0/P1 事故数、恢复时间。</li><li>过程能力层（Capability）：评审覆盖率、关键模块单测门槛、自动化回归覆盖范围、静态扫描通过率。</li></ul><p>如果组织有持续交付/平台化基础，建议把 DORA 指标纳入“速度与稳定”的共同语言：吞吐类（部署频率、变更前置时间）与稳定类（变更失败率、恢复时间）配套使用。</p><p>可复用交付物：质量目标表（按系统分层列阈值）＋ 指标口径说明（统计周期、严重度定义、数据来源）。</p><p><strong>② 输出《质量管理计划》：不是写给检查看的，是写给“协作对齐”看的</strong></p><p>一份能落地的《质量管理计划》至少回答 6 个问题：标准是什么、怎么验证、谁来负责、什么时候做、出了问题怎么处理、例外怎么管。</p><p>建议包含：</p><ul><li>质量标准与范围：功能、性能、安全、可靠性、可维护性各自的最低标准（写“阈值”，不写“努力提升”）。</li><li>验收口径：DoR/DoD、缺陷分级规则、上线放行门槛（含豁免机制）。</li><li>测试与验证策略：分层测试、自动化范围、测试数据策略、关键场景与异常流清单。</li><li>质量风险清单：高复杂/高耦合模块、外部依赖、关键人、历史事故点；附应对策略。</li><li>质量活动排期：评审点、走查点、灰度与回滚演练、里程碑质量评估会。</li><li>角色与责任：谁批准豁免？谁对线上指标负责？谁维护质量看板？</li></ul><p>质量计划写得再长，不如把“阈值、证据、责任人”写得更短更硬。</p><p>在落地层面，很多团队会把“质量阈值/放行条件/门禁检查项”固化到项目模板里，确保新项目一启动就带着同一套底线标准。比如在 <a href="https://link.segmentfault.com/?enc=9cdwSJJVKvSkrOSVaTWYzw%3D%3D.7nUP1Nzd7ZBYslu6Kpam1uABrkMomFXfwtxxThEXxZOgJAYbhBhnb6QuTokror01" rel="nofollow" target="_blank">ONES Project</a> 中可对需求状态与属性进行自定义，并用工作项/迭代承载这些约束，减少“人肉记忆”带来的漂移。</p><p><strong>③ 把质量前移：用“可测试性 + 可观测性”减少返工</strong></p><p>很多团队做了需求评审、也做了设计评审，但问题仍在上线后爆发，根因往往不是“没评审”，而是评审没有抓住两件事：可测试性（Testability）与可观测性（Observability）。</p><p>你可以把它们理解为：可测试性保证能被验证；可观测性保证上线后能被证明是稳定的。</p><p>落地动作很具体：</p><ul><li>需求可验收：场景、边界、异常流、权限、回退方案必须齐全；否则评审不是“通过/不通过”，而是“可验收/不可验收”。</li><li>设计可测试：依赖可替换、数据可构造、关键逻辑可单元化；否则测试只能“堆用例赌概率”。</li><li>上线可观测：关键链路日志/指标/追踪到位，告警口径清晰；否则故障定位就会退化成“人肉猜测”。</li></ul><h4>2.质量保证：用机制让“过程做对”，而不是靠英雄主义</h4><p>质量保证（QA）真正要管的是：过程是否按质量要求执行，组织是否在持续改进。它不是“测试团队的别名”，而是“过程可靠性的治理机制”。</p><p><strong>① 质量门禁（Quality Gates）：门禁不是“卡人”，是“卡风险”</strong></p><p>我建议把门禁设计成“风险拦截点”，并明确：它拦什么风险、用什么证据证明已处理。</p><ul><li>需求门禁：验收标准齐全、关键场景与异常流具备、风险已识别。</li><li>设计门禁：非功能（性能/安全/可用性）有方案；可测试性与可观测性满足规范。</li><li>代码门禁：Code Review 覆盖；关键模块单测门槛；静态扫描通过。</li><li>发布门禁：回归通过；灰度与回滚方案就绪；监控告警验证通过。</li></ul><p>门禁最难的不是设计，而是执行。因此必须配套“豁免机制”：豁免必须有审批人、期限、补偿动作；并进入看板统计。</p><p>门禁要“进系统、可追溯”，否则很容易退化成口头约定。比如在 ONES Project 里，团队常用“自定义工作流 + 权限/状态流转约束”把门禁落到真实的流程里；同时与 <a href="https://link.segmentfault.com/?enc=T5sDouwhGsHv%2BFEMiFEAjw%3D%3D.LRy2Rma%2BiS56BaGiXZvaP%2F8T0zwg%2FbLlCX1F1ZSAH6Vr7sX7QKAok9oNDhmR6hy%2B" rel="nofollow" target="_blank">ONES TestCase</a> 的数据互通，使测试执行发现问题后能更顺滑地进入缺陷闭环。</p><p><strong>② 审计 + 复盘 + CAPA：把“经验”沉淀成“机制”</strong></p><p>要让组织能力可复制，就要把“做得对”写成机制，把“做错了”变成改进。</p><p>轻量过程审计：重点看证据链是否完整（评审记录、测试报告、放行单、变更记录），不是抓“有没有写文档”。</p><p>事故/重大缺陷复盘：输出 CAPA（纠正与预防措施），并明确落点：改门禁、改规范、改工具、改培训，而不是“下次注意”。</p><p>复盘能否形成组织记忆，取决于“证据是否沉淀、是否能被检索复用”。很多团队会把评审纪要、放行单、事故复盘放在统一知识库，并与具体工作项关联，保证下次遇到相似问题能快速追溯；例如 <a href="https://link.segmentfault.com/?enc=lqUsQ5H2I%2FOXHqzv7IRruQ%3D%3D.wiSlkz3rAcy1ZlAQ6jR%2BVkEsLISsmnQBC9qAMWGrE4s%3D" rel="nofollow" target="_blank">ONES Wiki</a> 支持文档关联项目任务，也支持在文档中嵌入工作项与报表，让“质量证据”不散落。</p><p><strong>③ 用质量成本（COQ）做管理语言：让投入有商业解释</strong></p><p>很多质量体系推不动，原因不是管理层不重视质量，而是看不到“投入产出”。这时要用质量成本来对话：把投入分为预防、评价（评估）、失败（内部/外部）四类，按月做账本，让“救火成本”变得可见、可讨论。</p><h4>3.质量控制：用数据与证据证明“交付是合格的”</h4><p>如果说 QA 管过程，那么 QC 就必须管结果：交付物是否达标、是否允许进入下一阶段、是否可以上线、上线后是否稳定。两条原则非常关键：可重复、可追溯。</p><p><strong>① 建立“放行标准”：让上线不再靠拍脑袋</strong></p><p>上线放行建议做成“证据驱动”的评审，而不是口头汇报。放行标准至少覆盖：</p><ul><li>功能达标：关键业务用例通过率、回归范围说明、已知缺陷清单（含严重度与影响面）。</li><li>非功能达标：性能基线、容量评估、安全扫描与高危项处置。</li><li>稳定性就绪：灰度策略、回滚预案、监控告警验证、值班与响应机制。</li><li>风险可控：对“接受的风险”要有明确责任人和修复窗口与补偿措施。</li></ul><p>放行要“看证据”，证据往往来自 CI/CD 与代码变更本身。若你们已有 DevOps 工具链，建议把流水线执行、代码提交与工作项/迭代建立关联，放行会就能直接基于事实判断风险与状态；例如 ONES Pipeline 支持集成 Jenkins，并支持 GitHub/GitLab 等代码仓的集成，同时可将流水线与项目/迭代关联、把代码提交与工作项关联，便于形成可追溯的交付证据。</p><p><strong>② 质量看板：把“质量状态”变成组织共识</strong></p><p>研发项目质量管理一旦缺少可视化，就会退化成“各说各话”。看板建议按“输入—过程—输出”组织，并固定在周会/里程碑会上使用：</p><ul><li>输入：需求变更频率、需求返工率</li><li>过程：评审覆盖率、构建/扫描通过率、自动化回归覆盖范围</li><li>输出：缺陷逃逸率、回滚率、事故恢复时间</li></ul><p>看板能否长期用起来，取决于两点：数据是否自动汇聚、维度是否可按角色拆解。像 ONES Performance 提供多维度分析与仪表盘模板，并支持仪表盘共享与权限控制，适合把“质量指标”做成管理的日常语言，而不是季度汇报的装饰。</p><p><strong>③ 缺陷治理“分级分流”：别让 Bug 列表拖垮团队</strong></p><p>缺陷治理最怕“堆积如山但没人动”。建议把缺陷分三类路径，并把路径写进质量管理计划：</p><ul><li>阻断型（Blocker）：触发门禁，不修不放。</li><li>风险接受型（Accept）：明确风险、明确责任人、明确修复窗口与补偿措施。</li><li>技术债型（Debt）：进入版本规划，用质量成本账本持续跟踪。</li></ul><p>若团队已经采用测试管理工具，建议把“用例—测试计划—执行—缺陷—报告”的链路连起来，缺陷分流会更清晰、更可追溯。例如 ONES TestCase 覆盖完整测试流程，支持测试用例与需求/任务关联、测试计划与迭代关联，天然适合把“缺陷流转”与“迭代节奏”统一在一个闭环里。</p><h2>落地路线图：用 90 天把质量管理体系跑起来</h2><p>很多体系失败不是因为设计错，而是因为一开始就想“一步到位”。我建议用“最小可行体系（MVS）”思路：先跑通最短闭环，再逐步扩展。</p><p><strong>0~30 天：统一语言，跑通最短闭环</strong></p><ul><li>统一口径：缺陷分级、DoR/DoD、放行标准、豁免规则</li><li>先上两道门禁：需求门禁 + 发布门禁</li><li>先做一张看板：不超过 10 个指标，重点盯“缺陷逃逸率/回滚率/恢复时间”</li></ul><p><strong>30~90 天：固化机制，建立证据链</strong></p><ul><li>推行《质量管理计划》模板 + 评审清单</li><li>建立轻量审计 + 复盘 CAPA 追踪机制</li><li>把自动化与可观测性纳入“项目必做项”，不是“加分项”</li></ul><p><strong>90~180 天：规模化与持续改进</strong></p><ul><li>门禁覆盖关键系统与关键项目，豁免透明化</li><li>建立质量成本（COQ）账本，把投入与收益用管理语言表达</li><li>形成组织级知识库：缺陷模式、评审要点、测试资产与监控模板复用（例如把复盘与放行证据统一沉淀在知识库，并与工作项互相关联，减少“经验不可复用”的损耗）。</li></ul><p>体系的成败关键在于：你是否把质量从“个人经验”变成“组织能力”。</p><p>有效的研发项目质量管理，不是把团队绑在更多流程上，而是用一套可度量、可追溯、可改进的机制，把质量责任从“最后一公里的测试”拉回到全链路：质量策划定标准与阈值，质量保证防过程漂移，质量控制让放行有证据。当体系运转起来，你得到的不仅是缺陷下降，更是组织能力升级：决策更基于数据、协作更基于规则、交付更基于证据——这才是高质量交付的长期竞争力。</p><h2>附录A：质量管理相关术语表</h2><ol><li>质量策划（Quality Planning）：定义质量目标、标准、阈值、验证方式与责任边界。</li><li>质量保证（QA）：确保过程按质量要求执行，并通过审计/改进让能力可复制。</li><li>质量控制（QC）：验证交付物是否符合标准，并支撑放行与上线后稳定性验证。</li><li>质量门禁（Quality Gates）：在关键节点用证据拦截风险（需求/设计/代码/发布）。</li><li>缺陷逃逸率：发布后暴露的缺陷占比（需明确统计窗口与严重度口径）。</li><li>COQ（质量成本）：预防、评价（评估）、失败（内部/外部）成本的结构化账本。</li><li>ONES TestCase：覆盖完整测试流程，支持用例与需求/任务关联、测试计划与迭代关联，形成闭环。</li><li>ONES Pipeline：集成 CI/CD（如 Jenkins）与代码仓，支持流水线/代码提交与项目工作项关联，增强追溯。</li><li>ONES Wiki：支持文档与项目任务关联、嵌入工作项/报表，便于沉淀质量证据。</li><li>ONES Performance：提供多维度分析与仪表盘模板/共享/权限控制，支持质量与效能度量可视化。</li></ol><h2>附录B：FAQ</h2><p><strong>Q1：研发项目质量管理体系最核心的“一个东西”是什么？</strong><br/>A：证据链。把质量要求变成可验证证据（评审记录、测试报告、放行单、监控验证），并让证据链在项目节奏中稳定运转。</p><p><strong>Q2：QA 和 QC 的区别是什么？</strong><br/>A：QA 管“过程是否正确并持续改进”；QC 管“结果是否达标并支撑放行”。两者配合，质量才不会只靠最后阶段补救。</p><p><strong>Q3：质量门禁会不会拖慢交付？</strong><br/>A：不会，前提是门禁“卡风险不卡人”，并配套豁免机制（审批人+期限+补偿动作）。真正拖慢交付的是返工与线上事故。</p><p><strong>Q4：放行标准怎么定才不拍脑袋？</strong><br/>A：把放行标准拆成四类证据：功能达标、非功能达标、稳定性就绪、风险可控；每类都有阈值与责任人。</p><p><strong>Q5：缺陷逃逸率为什么总算不清？</strong><br/>A：通常是口径不统一：统计窗口（7天/14天/30天）、严重度分级、缺陷归因（新引入/历史遗留）与数据源不一致。先写清口径再谈目标。</p><p><strong>Q6：缺陷治理怎么“闭环”更顺滑？</strong><br/>A：让缺陷与迭代/需求/测试活动互相关联，才能把“发现—分流—修复—验证—复盘”串起来；很多团队会用测试管理与项目管理工具的数据互通来降低流转摩擦。</p>]]></description></item><item>    <title><![CDATA[耐克遭黑客入侵1.4T数据已遭泄露 JoySSL强调部署数字证书构筑防御网络 阻断敏感信息泄露 完美]]></title>    <link>https://segmentfault.com/a/1190000047575657</link>    <guid>https://segmentfault.com/a/1190000047575657</guid>    <pubDate>2026-01-27 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>根据科技媒体digwatch爆料称，国外知名黑客组织WorldLeaks最近在暗网高调宣布，已成功入侵运动品牌耐克内部系统，以技术手段窃取超过1.4TB数据，总共超过18.8万个文件。不仅如此，黑客团队还放出了屏幕截图以作数据考证，截图显示泄露的资料很可能来自于耐克分公司，泄露的信息包括多款系列运动鞋、运动服与健身产品的厂房资料。黑客以此作为要挟基础威胁耐克，若拒交赎金则会公开这些隐私资料。值得一提的是，WorldLeaks是一个网络犯罪组织，主要犯罪行径包括窃取公司重要数据迫使企业交付赎金。该组织曾经在25年改名，官网列出了100多家曾被窃取数据的企业，其中不乏戴尔、安德玛以及德国珠宝品牌CHRIST Juweliere等知名企业。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnMLm" alt="" title=""/></p><p>对此，耐克方面回应道，公司非常重视消费者的隐私与数据安全，将立即展开调查并评估事件影响。JoySSL安全专家对此表示，耐克事件表明企业在构建复杂的纵深安全体系时，往往缺乏最基础的数据流动防护。而互联网时代下，数据资产早已成为企业的核心命脉，任何管理失误，都有可能形成蝴蝶效应，造成无法挽回的后果。以SSL证书为基准，为数据流动加入加密与身份验证类安全技术，可以为企业的安全体系注入新的动力，成为企业守护数字资产最不可或缺的基础免疫系统。</p><p><strong>耐克事件折射企业数字生态薄弱之处</strong></p><p>耐克遭遇的此类高强度攻击通常路径极为复杂，成功窃取数据的核心往往在于针对企业数字生态中的某些薄弱环节，如内部及供应链数据交流的通道开放，为数据在传输过程中遭窃提供了可能性。</p><p><img width="723" height="477" referrerpolicy="no-referrer" src="/img/bVdnMLn" alt="" title="" loading="lazy"/></p><p>此外，管理系统与开发环境的入口暴露，不仅可能因缺少数字证书防护导致被监听，还可能让攻击者伪装成合法用户进行持续渗透。像耐克这样的跨国公司，其拥有数千个子域名、微服务及云实例，任何一个过期的证书、配置不当的服务端点，都可能成为攻击者进行横向扩展并最终窃取核心数据的切入点。</p><p><strong>SSL证书为数据交互建立可信任规则</strong></p><p>数字证书核心价值在于制定了一套便捷、全球认可的安全通信基础规则，从根本上强制数据加密，使用SSL证书并开启HTTPS/TLS，保障任意网络中两节点安全通信，不仅足以保护商业机密，同时也符合监管要求。</p><p>SSL证书除了加密功能，还提供由权威第三方审核的身份验证系统，从而将数字身份与合法实体绑定，能够有效防止仿冒与钓鱼攻击，确保生态系统中的连接安全。一份全面覆盖并有效管理的SSL证书部署记录，可作为企业安全能力的客观技术证明。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnMLo" alt="" title="" loading="lazy"/></p><p><strong>市场地位转变凸显数字证书战略价值</strong></p><p>信息时代，SSL证书已成为规避风险的稳固基石，是业务运转的可靠保障，更是提升信任的无形资产。JoySSL市场总监表示，在隐私保护意识强烈的当下，平台部署数字证书不仅能增强消费者的信心，还能提升品牌形象与信誉，兼顾安全防护与推动品牌建设，战略价值进一步凸显。</p><p><strong>呼吁企业拥抱数字化 加强防护体系</strong></p><p>耐克数据泄露事件证明在互联网驱动的时代，安全已不再是可选项，而是关乎企业存续与发展的核心课题。数字化转型的企业应重新审视SSL证书策略，从战略层面加强此基础设施，保护企业数字资产和品牌形象。</p>]]></description></item><item>    <title><![CDATA[智能体对传统行业冲击：为什么传统企业更强调“可控性”，而非“更聪明” 你的橙来啦 ]]></title>    <link>https://segmentfault.com/a/1190000047575192</link>    <guid>https://segmentfault.com/a/1190000047575192</guid>    <pubDate>2026-01-27 16:09:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在生成式人工智能向 <strong>AI 智能体（AI Agent）</strong> 演进的过程中，技术社区往往将目标放在更高的自主性、更强的推理能力上。</p><p>但当智能体真正进入 <strong>电力、制造、金融、能源、医药等传统行业</strong> 时，一个反直觉却极其现实的结论浮现出来：</p><blockquote><strong>传统企业并不优先追求“最聪明的智能体”，而是“最可控的智能体”。</strong></blockquote><p>这并非技术保守，而是由 <strong>物理风险、合规压力与业务确定性</strong> 共同决定的理性选择。</p><hr/><h2>一、核心定义：什么是传统行业语境下的“智能体可控性”？</h2><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMeg" alt="" title=""/><br/>在工业与严肃商业环境中，<strong>智能体的可控性（Controllability）</strong> 并不等同于“能不能关掉它”，而是一个系统级概念：</p><blockquote><strong>可控性 = 行为可预测 + 决策可解释 + 异常可接管</strong></blockquote><p>具体可拆解为三个维度：</p><h3>1️⃣ 边界可控（Boundary Control）</h3><ul><li>智能体<strong>能做什么 / 不能做什么</strong>是明确的</li><li>工具权限、数据访问范围、操作级别均被限制</li></ul><h3>2️⃣ 逻辑可控（Logic Transparency）</h3><ul><li>决策过程<strong>可以被复现与审计</strong></li><li>不只是“给结果”，而是能说明<strong>依据了什么规则 / 文档 / 条款</strong></li></ul><h3>3️⃣ 安全可控（Fail-safe Control）</h3><ul><li>在异常输入、极端场景下</li><li>系统可自动降级，或由人工即时接管（Human Override）</li></ul><hr/><h2>二、为什么“可控性”是传统行业的生命线？</h2><p><img width="626" height="404" referrerpolicy="no-referrer" src="/img/bVdnMeh" alt="" title="" loading="lazy"/></p><h3>1️⃣ 容错成本具有极端非对称性</h3><p>在互联网产品中，智能体犯错的代价通常接近于零；<br/> 而在传统行业中，一次错误可能意味着：</p><ul><li>设备损坏</li><li>生产事故</li><li>合规违规</li><li>财务或人身风险</li></ul><p><strong>因此现实选择是：</strong></p><blockquote>智能体更适合作为“决策辅助者”，而非“最终执行者”。</blockquote><p>这也是为什么多数传统企业会<strong>保留人类终审权</strong>。</p><hr/><h3>2️⃣ 合规与审计要求无法妥协</h3><p>金融、医药、能源等行业的共同特点是：</p><ul><li><strong>每一个决策必须可追溯</strong></li><li><strong>每一个结论必须有明确依据</strong></li></ul><p>但大模型天然存在随机性与幻觉风险（Hallucination）。</p><p><strong>因此：</strong></p><blockquote>如果智能体无法解释“为什么这么做”，<br/>那它在合规体系中就是不可用的。</blockquote><hr/><h3>3️⃣ 传统业务偏好“确定性而非创造性”</h3><p>传统企业的竞争力，往往来源于：</p><ul><li>数十年沉淀的 SOP</li><li>高度结构化的业务流程</li></ul><p>他们真正需要的不是“灵光一现”，而是：</p><blockquote>**90% 场景下像老员工一样稳定，<br/>10% 场景下才体现智能。**</blockquote><p>在实践中，一些团队会选择成熟的智能体平台，通过<strong>低代码工作流 + 强规则约束</strong>的方式，让智能体“聪明但不越界”，显著降低落地风险。</p><hr/><h2>三、实践范式：如何构建“可控的智能体系统”？</h2><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMei" alt="" title="" loading="lazy"/><br/>当前行业的共识路径是构建一种：</p><blockquote><strong>“受限自主系统（Constrained Autonomy）”</strong></blockquote><p>核心做法包括：</p><h3>✅ 1. RAG（检索增强生成）</h3><ul><li>将企业私有知识库作为<strong>唯一可信信息源</strong></li><li>限制智能体输出范围，降低幻觉概率</li></ul><h3>✅ 2. 工作流编排（Workflow Orchestration）</h3><ul><li>用 <strong>DAG 工作流</strong> 拆解任务</li><li>每一步都有明确输入、输出与校验规则</li></ul><h3>✅ 3. 人在回路（Human-in-the-Loop）</h3><ul><li>在关键节点设置人工审核断点</li><li>涉及资金、合规、客户沟通时必须人工确认</li></ul><hr/><h2>四、核心结论：可控性不是限制，而是入场券</h2><p>对传统行业而言：</p><ul><li><strong>没有可控性，就没有规模化</strong></li><li><strong>没有审计能力，就没有商业落地</strong></li></ul><blockquote>可控性决定了：<br/>智能体是“实验玩具”，还是“生产工具”。</blockquote><p>本质上，这是一种新的<strong>人机契约关系</strong>：</p><ul><li>人类定义规则与边界</li><li>智能体承诺在规则内高效执行</li></ul><p><strong>未来传统企业的真正竞争力，不在于谁的模型参数更大，而在于谁先构建出一套“可控、可审计、可接管”的智能体体系。</strong><br/>（<strong>本文章由AI辅助生成</strong>）</p>]]></description></item><item>    <title><![CDATA[如何通过 5 种方式将照片从 iPad 传输到电脑 iReaShare ]]></title>    <link>https://segmentfault.com/a/1190000047575341</link>    <guid>https://segmentfault.com/a/1190000047575341</guid>    <pubDate>2026-01-27 16:09:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>您的 iPad 或 iPhone 内部存储空间不足，无法存储照片？或者想备份照片？您可以将照片从 iPad 或 iPhone 传输到电脑。根据您的偏好和可用的工具，共有 5 种方法可以实现此操作。无论您喜欢基于云的解决方案、专用软件，还是简单的拖放操作，您都能找到适合您需求的方法。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575344" alt="图片" title="图片"/></p><p>快速看一下这些方法的优缺点：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575345" alt="图片" title="图片" loading="lazy"/></p><p>第 1 部分：如何通过 iCloud 照片将照片从 iPad/iPhone 传输到计算机？</p><p>iCloud Photos 是 Apple 的云端照片同步服务，让您可以轻松地在所有设备（包括电脑）上访问您的照片。请先确保您的 iCloud 帐户有足够的云存储空间。</p><p>要通过 iCloud Photos 将图片从 iPad 移动到 PC：</p><pre><code>
在您的 iPad 上，请前往“设置”&gt;“ [您的姓名] ”&gt;“ iCloud ”&gt;“照片”。开启“同步此 iPad ”功能（如果尚未启用）。这会将您的 iPad 照片上传到 iCloud。请确保您的 iPad 已连接到 Wi-Fi。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575346" alt="图片" title="图片" loading="lazy"/></p><pre><code>
在 Windows PC 上，从 Apple 网站或 Microsoft Store 下载并安装适用于 Windows 的 iCloud。或者，访问 iCloud Photos 网站并登录您的 Apple 帐户。


单击“照片”选项，将您想要的照片下载到您的电脑。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575347" alt="图片" title="图片" loading="lazy"/></p><p>注意：在 Mac 电脑上，iCloud 照片已内置于 macOS 中。请确保您在 Mac 和 iPad 上使用相同的 Apple ID 登录。只需在 Mac 上打开“照片”应用。前往“照片”&gt;“设置”（或“偏好设置”）&gt;“iCloud”。确保已勾选“iCloud 照片”。您的 iPad 照片将同步到 Mac 的照片图库。然后，您可以根据需要将它们拖放到其他文件夹。</p><p>第 2 部分：如何通过 iReaShare iPhone Manager 将照片从 iPad/iPhone 传输到 PC？</p><p>作为一款一体化 iOS 管理工具， iReaShare iPhone Manager提供了实用的功能来传输数据，包括照片、视频、音乐、联系人、短信等。如果您想将照片从 iPad 或 iPhone 无缝导出到 Windows 或 Mac 电脑，它将满足您的要求。</p><p>iReaShare iPhone Manager的主要功能：</p><ul><li>将图片从 iPad 以无损质量传输到计算机。</li><li>不会改变您的图像格式。</li></ul><p>*将各种文件从 iOS 设备传输到计算机。</p><ul><li>立即将您的 iOS 数据备份到您的计算机。</li><li>将备份数据从您的计算机恢复到您的 iPad 或 iPhone。</li><li>支持 iOS 5.0 及更高版本，包括 iOS 26。</li></ul><p>以下是通过该软件将图片从iPad导出到PC的方法：</p><pre><code>
在电脑上下载并安装 iReaShare iPhone Manager，然后使用 USB 数据线将 iPad 连接到电脑。启动软件。它应该会检测到你的 iPad。


如果出现提示，请点击“信任此电脑”，在 iPad 上授予访问权限。然后即可建立连接。


点击界面上的“照片”部分。接下来，选择要传输的照片，然后点击“导出”图标。然后在电脑上选择一个目标文件夹来保存照片。软件将开始传输过程。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575348" alt="图片" title="图片" loading="lazy"/></p><p>第 3 部分：如何通过照片应用程序将照片从 iPad 或 iPhone 传输到计算机？</p><p>macOS 和 Windows 操作系统上的“照片”应用是管理设备上照片的主要工具，包括从 iPad 直接传输照片。对于 Windows 用户，只要您的电脑运行的是 Windows 10 或更高版本，您就可以使用“照片”应用从 iOS 设备导入照片。</p><p>将 iPad 照片导入 PC：</p><pre><code>
请将 iPad 通过 USB 连接到电脑。然后点击“搜索”，并输入“照片”即可启动照片应用。


点击右上角的“导入”，点击“从USB设备”。


选择你的 iPad，然后选择要传输的图片。选择后，点击“添加 X 个项目”，选择电脑上的文件夹，然后点击“导入”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575349" alt="图片" title="图片" loading="lazy"/></p><p>将 iPad 照片导入 Mac：</p><pre><code>
使用 USB 数据线将 iPad 连接到 Mac。在 Mac 上打开“照片”应用。如果它没有自动打开，请在“应用程序”文件夹或 Dock.p 中找到它。


你的 iPad 应该会出现在照片应用侧栏的“设备”部分下。点击它。如果这是你第一次连接，iPad 可能会询问你是否信任这台电脑。点击“信任”并输入你的密码。


照片应用会显示 iPad 上的所有照片和视频。您可以选择“导入所有新照片”，或选择特定照片，然后点击“导入所选”。导入后，这些照片将出现在 Mac 的照片图库中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575350" alt="图片" title="图片" loading="lazy"/></p><p>提示：您可以轻松地将联系人从iPhone或iPad同步到Mac电脑。如果您需要有用的解决方案，请查看。</p><p>第 4 部分：如何通过 Google Photos 将图片从 iPad 或 iPhone 导出到 PC？</p><p>Google Photos 是一款流行的跨平台云服务，它提供了一种极好的方式，可以将照片从 iPad 或 iPhone 备份并传输到您的计算机，无论使用哪种操作系统。</p><p>将照片从ipad下载到电脑：</p><pre><code>
从 App Store 下载 Google 相册应用并将其安装在 iPad 上。打开 Google 相册应用，然后使用你的 Google 帐户登录。


点击右上角的个人资料图标。前往“照片设置”&gt;“备份和同步”。开启“备份和同步”功能。您的 iOS 照片将开始上传到 Google 相册（请确保您的 Wi-Fi 连接良好）。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575351" alt="图片" title="图片" loading="lazy"/></p><pre><code>
在你的电脑（Windows PC 或 Mac）上，打开网络浏览器并访问 photos.google.com。然后使用你在 iPad 上使用的 Google 帐户登录。


您 iPad 上所有备份的照片都会显示在这里。选择您想要的照片，点击右上角的三个点菜单图标，然后选择“下载”。照片将被下载到您电脑的下载文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575352" alt="图片" title="图片" loading="lazy"/></p><p>第 5 部分：如何通过文件资源管理器将图像从 iPad/iPhone 传输到计算机？</p><p>对于 Windows PC 用户，您的 iOS 设备可以被识别为数码相机，从而可以使用文件资源管理器进行简单的拖放传输。如果您的 PC 运行的是 Windows 7/8 系统，且没有“照片”应用，您可以使用这种方式将照片从 iPad 复制到 PC。</p><p>具体操作如下：</p><pre><code>
使用 USB 数据线将 iPad 连接到 Windows PC，然后在 iPad 上单击“信任”。


在电脑上打开“文件资源管理器”（可以按 Windows + E）。在左侧边栏中，您应该会在“便携式设备”或“设备和驱动器”下看到您的 iPad。它可能会显示为“ Apple iPad ”或类似的名称。


双击 iPad/iPhone 将其打开，然后导航至“内部存储”&gt;“ DCIM ”。在 DCIM 文件夹中，您会找到一个或多个文件夹（例如，100APPLE、101APPLE），其中包含您的照片和视频。


打开这些文件夹查看图片。现在，您可以选择要传输的照片。将选定的照片拖放到电脑上的任何文件夹中，或者复制粘贴到您想要的位置。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575353" alt="图片" title="图片" loading="lazy"/></p><p>第 6 部分：有关将照片从 iPad/iPhone 传输到计算机的常见问题</p><p>问题 1：将照片从 iPad 或 iPhone 传输到电脑的最简单方法是什么？</p><p>本指南中的方法都很简单易用。哪种方法最简单取决于您的需求。如果您想无线传输照片，可以使用 iCloud Photos 和 Google Photos。如果您想要稳定且批量的传输，那么使用 iReaShare iPhone Manager 将是最佳选择。</p><p>问题 2：我可以使用 iTunes 将照片从 iPad/iPhone 传输到 PC 吗？</p><p>不可以，通常情况下，您无法使用 iTunes 将照片从 iPad 传输到 PC。iTunes 主要用于将 PC 上的媒体（音乐、视频等）同步到 iPad 以及备份 iPad。它实际上并没有直接的“将照片从 iOS 设备传输到 PC”功能。</p><p>Q3：我可以使用 AirDrop 在 iPad 和电脑之间传输照片吗？</p><p>是的，你完全可以使用 AirDrop 在 iPad 和 Mac 电脑之间传输照片（以及其他文件）。但如果你的电脑是 Windows 系统，则无法使用 AirDrop 传输图片。</p><p>结论</p><p>无论选择哪种方法，都可以轻松地将照片从 iPad 或 iPhone 传输到电脑。使用iReaShare iPhone Manager可以高效、高质量地传输照片，并轻松管理您的 iPad 数据。无论如何，请考虑您个人对云存储和直接连接存储的偏好，并选择最适合您工作流程的方法。<br/>​</p>]]></description></item><item>    <title><![CDATA[危中有机：国际动荡下，中国电子签章行业的挑战与突围 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047575365</link>    <guid>https://segmentfault.com/a/1190000047575365</guid>    <pubDate>2026-01-27 16:08:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着国际形势动荡的不断发酵，国内市场中的各个行业都受到不同程度的影响，那针对刚刚起步拓展海外市场的各个电子签公司（北京安证通、E签宝等）有没有直接影响呢？我们简单来看看。</p><p>首先，国际动荡对国内电子签章公司确实会产生一系列直接和间接的影响，尽管中国国内市场是其主要根基。具体影响可以从以下几个层面分析：</p><ol><li>负面影响与风险</li></ol><p>1) 供应链与技术依赖</p><p>Ø 若动荡涉及关键技术与硬件（如服务器芯片、加密硬件模块、云计算基础架构）的出口管制或供应链中断，可能影响国内电子签章公司的产品升级与运维。影响最大的便是以数字证书为主，电子签章为辅的各个CA公司（北京CA、CFCA等）</p><p>Ø 若依赖海外开源技术或标准（如密码算法、国际认证体系），可能因制裁或技术脱钩导致合规成本上升。</p><p>2) 跨国业务受阻</p><p>Ø 若公司服务出海企业或境外客户，地缘冲突可能导致跨境数据流动受限（如欧美数据跨境协议变化），增加法律合规复杂性。</p><p>Ø 部分国家可能以“国家安全”为由限制外国数字服务，影响中国电子签章企业的海外拓展。</p><p>3) 经济下行传导</p><p>Ø 国际冲突可能引发全球经济增长放缓，影响国内外贸、投资等领域，进而减少企业数字化转型需求，电子签章作为降本工具可能面临项目延期或预算削减。</p><p>4) 信息安全与自主可控压力</p><p>Ø 国际网络空间对抗加剧可能激发各国对数据主权的要求，国内企业需加速国产密码算法、信创生态适配，短期内增加研发成本。</p><ol start="2"><li>潜在机遇</li></ol><p>1) 国产替代加速</p><p>Ø 国际摩擦可能促使政府与企业更重视供应链安全，推动电子签章在政务、金融、能源等关键领域的国产化替代，利好具备自主技术的公司。</p><p>2) 国内政策支持强化</p><p>Ø 为应对不确定性，国内可能加大数字经济基础设施投入，例如推动“全国统一电子签名互认体系”建设，扩大电子签章在医疗、司法等场景的应用。</p><p>3) 远程与无纸化需求增长</p><p>Ø 国际动荡若导致跨国人员流动受阻、远程办公常态化，可能刺激跨境电子合同、在线公证等需求，为电子签章行业开辟新市场。</p><ol start="3"><li>行业应对策略</li></ol><p>Ø 技术层面：加强国产密码技术（如SM2/SM9）应用，适配信创生态；布局隐私计算等跨域认证技术以应对数据流动壁垒。</p><p>Ø 业务层面：深耕国内市场，聚焦政务、国企、大型制造业等稳健需求；出海时优先选择“一带一路”等政策支持区域，降低地缘风险。</p><p>Ø 合规层面：密切关注跨境数据监管（如中国《数据出境安全评估办法》、欧盟GDPR），构建动态合规体系</p><ol start="4"><li>总结</li></ol><p>国际动荡对国内电子签章行业是 “危中有机” 的复合挑战。就目前国内各个电子签章公司的技术路线和行业案例覆盖情况而言，北京安证通在这波机遇与挑战中的韧性将是最强的，其他电子签章公司还需在技术和应用层面加强自身。</p>]]></description></item><item>    <title><![CDATA[PostgreSQL 18 RETURNING 增强：现代应用的重要进展 IvorySQL ]]></title>    <link>https://segmentfault.com/a/1190000047575370</link>    <guid>https://segmentfault.com/a/1190000047575370</guid>    <pubDate>2026-01-27 16:07:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>PostgreSQL 18 正式发布，带来了多项重要改进，其中 RETURNING 子句的增强尤为突出。该特性在 MERGE RETURNING 场景下实现了关键突破，可显著简化应用架构，并提升数据变更追踪能力。</p><h2>RETURNING 子句的演进</h2><p>RETURNING 子句长期以来用于在 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 操作后返回受影响行的数据，从而避免额外的 SELECT 查询，减少数据库往返次数并提升性能。然而，在 PostgreSQL 18 之前，该子句在功能上存在明显限制，迫使开发实践中采用各种折中方案。</p><p>在 PostgreSQL 17 中，首次为 <code>MERGE</code> 语句引入 RETURNING 支持（提交 <code>c649fa24a</code>），这是一次重要进展。<code>MERGE</code> 语句自 PostgreSQL 15 引入，用于在单条语句中完成条件化的 <code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code> 操作，但在缺乏 RETURNING 支持的情况下，无法直观获取实际执行结果。</p><h2>PostgreSQL 18 的新特性</h2><p>PostgreSQL 18 通过引入 OLD 与 NEW 别名（提交 80feb727c8，由 Dean Rasheed 提交，Jian He 与 Jeff Davis 评审），将 RETURNING 子句能力提升至新的层级。该增强使 DML 操作期间的数据捕获方式发生了根本性变化。</p><h3>PostgreSQL 18 之前的限制</h3><p>在早期版本中，RETURNING 子句在不同语句类型下存在以下差异化限制：</p><ul><li><code>INSERT</code> 与 <code>UPDATE</code> 仅能返回新值或当前值</li><li><code>DELETE</code> 仅能返回旧值</li><li><code>MERGE</code> 根据内部实际执行的操作类型（<code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code>）返回结果</li></ul><p>在需要对比更新前后数据、或精确追踪字段变化时，可选方案较为有限，包括：</p><ul><li>在修改前额外执行 <code>SELECT</code> 查询</li><li>编写复杂的触发器函数</li><li>在应用层实现变更跟踪逻辑</li><li>通过系统列（如 xmax）进行间接判断</li></ul><p>上述方式普遍增加了实现复杂度与访问延迟，并降低了代码可维护性。</p><h3>解决方案：OLD 与 NEW 别名</h3><p>PostgreSQL 18 引入了特殊别名 <code>old</code> 与 <code>new</code>，可在单条语句中同时访问数据的修改前状态与修改后状态。该机制适用于 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 以及 <code>MERGE</code> 等全部 DML 操作。</p><p>基本语法示例如下：</p><pre><code>UPDATE table_name
SET column = new_value
WHERE condition
RETURNING old.column AS old_value, new.column AS new_value;</code></pre><p>为避免与现有列名冲突，或在触发器环境中使用，可对别名进行重命名：</p><pre><code>UPDATE accounts
SET balance = balance - 50
WHERE account_id = 123
RETURNING WITH (OLD AS previous, NEW AS current)
    previous.balance AS old_balance,
    current.balance AS new_balance;</code></pre><h2>MERGE + RETURNING：能力整合</h2><p>在 PostgreSQL 18 中，MERGE 与 RETURNING 的组合为 Upsert 场景提供了完整能力，可在单条原子操作中同时完成数据写入与变更结果获取。</p><h3>实践示例：产品库存系统</h3><p>在产品库存管理场景中，需要从外部数据源同步数据，实现新增产品、更新已有产品，并准确记录每一行的处理结果。</p><p><strong>步骤 1：创建数据表</strong></p><pre><code>CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    product_code VARCHAR(50) UNIQUE NOT NULL,
    product_name VARCHAR(200) NOT NULL,
    price DECIMAL(10, 2) NOT NULL,
    stock_quantity INTEGER NOT NULL DEFAULT 0,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE product_staging (
    product_code VARCHAR(50),
    product_name VARCHAR(200),
    price DECIMAL(10, 2),
    stock_quantity INTEGER
);</code></pre><p><strong>步骤 2：插入初始数据</strong></p><pre><code>INSERT INTO products (product_code, product_name, price, stock_quantity)
VALUES
    ('LAPTOP-001', 'Premium Laptop', 999.99, 50),
    ('MOUSE-001', 'Wireless Mouse', 29.99, 200),
    ('KEYBOARD-001', 'Mechanical Keyboard', 79.99, 150);

INSERT INTO product_staging (product_code, product_name, price, stock_quantity)
VALUES
    ('LAPTOP-001', 'Premium Laptop Pro', 1099.99, 45),  -- Update existing
    ('MONITOR-001', '4K Monitor', 399.99, 75),          -- New product
    ('MOUSE-001', 'Wireless Mouse', 29.99, 200);        -- No actual change</code></pre><h3>基础版：搭配 RETURNING 子句的 MERGE 操作</h3><pre><code>MERGE INTO products p
USING product_staging s ON p.product_code = s.product_code
WHEN MATCHED THEN
    UPDATE SET
        product_name = s.product_name,
        price = s.price,
        stock_quantity = s.stock_quantity,
        last_updated = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (product_code, product_name, price, stock_quantity)
    VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
RETURNING
    p.product_code,
    p.product_name,
    merge_action() AS action_performed;</code></pre><p>返回结果示例：</p><pre><code> product_code  |    product_name     | action_performed
---------------+---------------------+------------------
 LAPTOP-001    | Premium Laptop Pro  | UPDATE
 MONITOR-001   | 4K Monitor          | INSERT
 MOUSE-001     | Wireless Mouse      | UPDATE</code></pre><h3>进阶版：搭配 OLD 与 NEW 别名的 MERGE 操作</h3><p>通过 OLD 与 NEW 别名，可同时获取字段的修改前与修改后值，从而实现精细化变更追踪与审计。</p><p>以下查询可从受影响行中，同时获取 product_name 与 price 列的修改前旧值和修改后新值。通过为其设置别名（old_name、new_name、old_price、new_price），可便捷对比 MERGE 操作前后的列值变化，为变更追踪与审计日志记录提供支撑。</p><pre><code>MERGE INTO products p
USING product_staging s ON p.product_code = s.product_code
WHEN MATCHED THEN
    UPDATE SET
        product_name = s.product_name,
        price = s.price,
        stock_quantity = s.stock_quantity,
        last_updated = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (product_code, product_name, price, stock_quantity)
    VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
RETURNING
    p.product_code,
    merge_action() AS action,
    old.product_name AS old_name,
    new.product_name AS new_name,
    old.price AS old_price,
    new.price AS new_price,
    old.stock_quantity AS old_stock,
    new.stock_quantity AS new_stock,
    (old.price IS DISTINCT FROM new.price) AS price_changed,
    (old.stock_quantity IS DISTINCT FROM new.stock_quantity) AS stock_changed;</code></pre><p>INSERT 场景下旧值为 NULL，而 UPDATE 场景下可完整呈现字段变更情况。</p><pre><code> product_code  | action | old_name          | new_name            | old_price | new_price | old_stock | new_stock | price_changed | stock_changed
---------------+--------+-------------------+---------------------+-----------+-----------+-----------+-----------+---------------+--------------
 LAPTOP-001    | UPDATE | Premium Laptop    | Premium Laptop Pro  | 999.99    | 1099.99   | 50        | 45        | t             | t
 MONITOR-001   | INSERT | NULL              | 4K Monitor          | NULL      | 399.99    | NULL      | 75        | NULL          | NULL
 MOUSE-001     | UPDATE | Wireless Mouse    | Wireless Mouse      | 29.99     | 29.99     | 200       | 200       | f             | f</code></pre><h3>构建审计日志</h3><p>借助增强后的 RETURNING 子句，可在不使用触发器的前提下构建完整审计链路。</p><p><strong>步骤 1：创建审计表</strong></p><pre><code>CREATE TABLE product_audit (
    audit_id SERIAL PRIMARY KEY,
    product_code VARCHAR(50),
    action VARCHAR(10),
    old_values JSONB,
    new_values JSONB,
    changes JSONB,
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);</code></pre><p><strong>步骤 2：执行带详细审计追踪的 MERGE 操作</strong></p><pre><code>WITH merge_results AS (
    MERGE INTO products p
    USING product_staging s ON p.product_code = s.product_code
    WHEN MATCHED THEN
        UPDATE SET
            product_name = s.product_name,
            price = s.price,
            stock_quantity = s.stock_quantity,
            last_updated = CURRENT_TIMESTAMP
    WHEN NOT MATCHED THEN
        INSERT (product_code, product_name, price, stock_quantity)
        VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
    RETURNING
        p.product_code,
        merge_action() AS action,
        jsonb_build_object(
            'name', old.product_name,
            'price', old.price,
            'stock', old.stock_quantity
        ) AS old_values,
        jsonb_build_object(
            'name', new.product_name,
            'price', new.price,
            'stock', new.stock_quantity
        ) AS new_values
)
INSERT INTO product_audit (product_code, action, old_values, new_values, changes)
SELECT
    product_code,
    action,
    old_values,
    new_values,
    CASE
        WHEN action = 'INSERT' THEN new_values
        WHEN action = 'DELETE' THEN old_values
        ELSE (
            SELECT jsonb_object_agg(key, value)
            FROM jsonb_each(new_values)
            WHERE value IS DISTINCT FROM old_values-&gt;key
        )
    END AS changes
FROM merge_results;</code></pre><p><strong>步骤 3：查询审计追踪结果</strong></p><pre><code>select * from product_audit;
 audit_id | product_code | action |                          old_values                           |
             new_values                           | changes |         changed_at
----------+--------------+--------+---------------------------------------------------------------+-------------
--------------------------------------------------+---------+----------------------------
        1 | LAPTOP-001   | UPDATE | {"name": "Premium Laptop Pro", "price": 1099.99, "stock": 45} | {"name": "Pr
emium Laptop Pro", "price": 1099.99, "stock": 45} |         | 2025-12-12 16:27:14.760125
        2 | MONITOR-001  | UPDATE | {"name": "4K Monitor", "price": 399.99, "stock": 75}          | {"name": "4K
 Monitor", "price": 399.99, "stock": 75}          |         | 2025-12-12 16:27:14.760125
        3 | MOUSE-001    | UPDATE | {"name": "Wireless Mouse", "price": 29.99, "stock": 200}      | {"name": "Wi
reless Mouse", "price": 29.99, "stock": 200}      |         | 2025-12-12 16:27:14.760125
(3 rows)</code></pre><p>示例中通过 CTE 获取 MERGE 结果，并将旧值、新值及差异以 JSONB 形式写入审计表，实现单条原子操作内的数据同步与审计记录生成。</p><h2>未来展望</h2><p>PostgreSQL 18 版本的 RETURNING 子句增强特性，是该数据库提升开发友好性、减少复杂替代方案使用的重要举措。单原子操作中同时调用数据新旧值的能力，可简化应用开发中的多种通用实现模式。</p><p>该功能在后续版本中或可从以下方向进一步升级：</p><ol><li>扩展 MERGE 语句能力，新增更多 WHEN 子句，实现更复杂的条件操作.</li><li>新增聚合功能支持，支持对 RETURNING 子句的返回结果直接进行聚合计算。</li><li>实现跨表返回，支持在单操作中返回关联表的数据信息。</li></ol><h2>技术细节与提交记录参考</h2><p>针对关注技术实现细节的人员，可参考以下信息：</p><ul><li><strong>MERGE RETURNING</strong>（PostgreSQL 17）：迪恩・拉希德提交，记录编号 <code>c649fa24a</code>。</li><li><strong>OLD/NEW Support</strong>（PostgreSQL 18）：迪恩・拉希德提交，何健与杰夫・戴维斯评审，记录编号 <code>80feb727c8</code>。</li><li><a href="https://link.segmentfault.com/?enc=xUH4du746xSi%2F69jexxqIg%3D%3D.6330041%2Bc657m%2FvPuMhxPdnIytvop8ibDPhK3dV37jNhOpzi0B7iSIo6c0foxlhiUuBVi3iDjjjNsmHC%2FZEHfiK6QpD2CZ8bzEAUcmvxmQfKgWqxakiLpd6MpMhp4pN%2B" rel="nofollow" target="_blank"><strong>Discussion Thread</strong></a>：<a href="https://link.segmentfault.com/?enc=NP8SnQkvz0vsBqOzVJLECQ%3D%3D.rb0yAlt24GBKgKmW100zKakE80kcUemfT4XDodQDprUwcQgMc6kHXOb35UE3jo6%2FP8ftywL2tdhiWcS%2FgyhmSn5%2FGQ%2F7bHF5Yrf4jXsHP1bAqwBo7otwo7g2y473ZEx9" rel="nofollow" target="_blank">https://postgr.es/m/CAEZATCWx0J0-v=Qjc6gXzR=KtsdvAE7Ow=D=mu50...</a></li></ul><p>该功能的实现涉及多个组件的修改，包括：</p><ul><li>执行器（execExpr.c、execExprInterp.c、nodeModifyTable.c）</li><li>解析器（parse_target.c）</li><li>优化器（createplan.c、setrefs.c、subselect.c）</li><li>节点模块（makefuncs.c、nodeFuncs.c）</li></ul><h2>总结</h2><p>PostgreSQL 18 对 RETURNING 子句的增强，尤其是 OLD 与 NEW 别名的引入，为 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 与 <code>MERGE</code> 操作提供了完整的数据变更可视性。这一能力显著减少了对触发器与额外查询的依赖，使数据同步、变更追踪与审计实现更加简洁、高效且易于维护。</p><p><code>MERGE</code> 与增强型 RETURNING 的结合，为 Upsert 场景提供了前所未有的控制能力与透明度，是 PostgreSQL 在开发友好性与工程实用性方面的重要进展。</p><p>原文链接：</p><p><a href="https://link.segmentfault.com/?enc=Pir6oMoDMd7tte0NYOdnaw%3D%3D.kyIen2tsGS5LxWwyaNprlAEw3UMbxLZIER7HMj5xACJHaujxlGX24VnWJkbjs%2BUT4P3HPMJdftK6NjuLopTOdwtvbivHOpHoSP%2FUfAHluMOFln4QbbMF0bcUwFtqzkg0x23dt6EGEhqgxMWJ3CwOPg%3D%3D" rel="nofollow" target="_blank">https://www.pgedge.com/blog/postgresql-18-returning-enhanceme...</a></p><p>作者：Ahsan Hadi</p><hr/><h2><a href="https://link.segmentfault.com/?enc=VUVGKIVeZeprikqWa%2BKXQw%3D%3D.t6Qs5ydlFrfM1gHbhgUTD52z3aJIdTVI%2BAGHpbcoNuk%3D" rel="nofollow" target="_blank">HOW 2026 议题招募中</a></h2><p>2026 年 4 月 27-28 日，由 IvorySQL 社区联合 PGEU（欧洲 PG 社区）、PGAsia（亚洲 PG 社区）共同打造的 HOW 2026（IvorySQL &amp; PostgreSQL 技术峰会） 将再度落地济南。届时，PostgreSQL 联合创始人 Bruce Momjian 等顶级大师将亲临现场。</p><p>自开启征集以来，HOW 2026 筹备组已感受到来自全球 PostgreSQL 爱好者的澎湃热情。为了确保大会议题的深度与广度，我们诚邀您在 2026 年 2 月 27 日截止日期前，提交您的技术见解。</p><p>投递链接：<a href="https://link.segmentfault.com/?enc=X0QwZAujMNioCEl1ok530g%3D%3D.L%2BMnYddUJo%2BlEgHpCOO8Jhzbrbm%2FGQIBDsIBhqNn0P4%3D" rel="nofollow" target="_blank">https://jsj.top/f/uebqBc</a></p>]]></description></item><item>    <title><![CDATA[详解 RNN 循环机制：短序列优势、长依赖问题及 LSTM 解决方案 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047575379</link>    <guid>https://segmentfault.com/a/1190000047575379</guid>    <pubDate>2026-01-27 16:07:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>RNN 简介</h2><p>RNN（Recurrent Neural Network，循环神经网络）一般以序列数据为输入，通过网络内部的结构设计有效捕捉序列之间的关系特征，一般也以序列形式输出。</p><p>RNN 的循环机制使模型隐层上一时间步产生的结果，能够作为当下时间步输入的一部分（当下时间步的输入除了正常的输入外还包括上一步的隐层输出）对当下时间步的输出产生影响。</p><ul><li>结构：三层（输入层、隐藏层、输出层；循环发生在隐藏层）<br/><img width="723" height="265" referrerpolicy="no-referrer" src="/img/bVdnMGW" alt="" title=""/></li></ul><h3>1.1 RNN 模型的作用</h3><p>因为 RNN 结构能够很好利用序列之间的关系，因此针对自然界具有连续性的输入序列，如人类的语言、语音等进行很好处理，广泛应用于 NLP（自然语言处理）领域的各项任务，如文本分类、情感分析、意图识别、机器翻译等。</p><p>语言处理示例</p><p><img width="723" height="328" referrerpolicy="no-referrer" src="/img/bVdnMGX" alt="" title="" loading="lazy"/></p><h4>2.1 PyTorch 中传统 RNN 的使用</h4><p>位置：在 <code>torch.nn</code> 中，通过 <code>torch.nn.RNN</code> 可调用。</p><pre><code class="python">import torch
import torch.nn as nn

rnn = nn.RNN(5, 6, 2)  # 实例化 rnn 对象
# 参数1：输入张量 x 的维度 - input_size
# 参数2：隐藏层的维度（隐藏层神经元个数）- hidden_size
# 参数3：隐藏层的层数 - num_layers

# torch.randn - 随机产生正态分布的随机数
input1 = torch.randn(1, 3, 5)  # 设定输入张量 x - 序列长 1，批次 3，维度 5
# 参数1：输入序列长度 - sequence_length
# 参数2：批次的样本 - batch_size（表示：3 个样本）
# 参数3：输入张量 x 的维度 - input_size

h0 = torch.randn(2, 3, 6)  # 设定初始化的 h0
# 第一个参数：num_layers * num_directions（层数 * 网络方向数（1 或 2））
# 第二个参数：batch_size（批次的样本数）
# 第三个参数：hidden_size（隐藏层的维度）

output, hn = rnn(input1, h0)
# 最后输出和最后一层的隐藏层输出

print(output)
print(output.shape)
print(hn)
print(hn.shape)</code></pre><h4>1.2 RNN的局限：长期依赖（Long-TermDependencies）问题</h4><p>RNN的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果RNN可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p><p>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测这句话中“the clouds are in the sky”最后的这个词“sky”，我们并不再需要其他的信息，因为很显然下一个词应该是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN可以学会使用先前的信息。</p><h4>1.2 传统 RNN 优缺点</h4><ul><li>优势：内部结构简单，对计算资源要求低；相较 LSTM/GRU 参数总量更少；在短序列任务上性能与效果表现优异。</li><li>缺点：在长序列关联上表现较差；反向传播时易发生梯度消失或爆炸。</li></ul><p>NaN 值（Not a Number，非数）：是计算机科学中数值数据类型的一类值，表示未定义或不可表示的值。</p><h3>2.1 LSTM 模型简介</h3><p>Long ShortTerm 网络——一般就叫做LSTM——是一种RNN特殊的类型，可以学习长期依赖信息。当然，LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。</p><p>LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态和当前输入。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。<br/><img width="723" height="625" referrerpolicy="no-referrer" src="/img/bVdnMGY" alt="" title="" loading="lazy"/></p><h4>2.2 PyTorch 中 LSTM 的使用</h4><pre><code class="python">import torch
import torch.nn as nn

lstm = nn.LSTM(5, 6, 2)  # 实例化 lstm 对象
# 参数1：输入张量 x 的维度 - input_size
# 参数2：隐藏层的维度（隐藏层神经元个数）- hidden_size
# 参数3：隐藏层的层数 - num_layers

input1 = torch.randn(1, 3, 5)  # 设定输入张量 x - 序列长 1，批次 3，维度 5
# 参数1：输入序列长度 - sequence_length
# 参数2：批次的样本 - batch_size
# 参数3：输入张量 x 的维度 - input_size

h0 = torch.randn(2, 3, 6)  # 设定初始化的 h0（隐藏层）
c0 = torch.randn(2, 3, 6)  # 设定初始化的 c0（细胞状态）
# 第一个参数：num_layers * num_directions（层数 * 网络方向数（1 或 2））
# 第二个参数：batch_size（批次的样本数）
# 第三个参数：hidden_size（隐藏层的维度）

output, (hn, cn) = lstm(input1, (h0, c0))
# 最后输出和最后一层的隐藏层输出

print(output)
print(output.shape)
print(hn)
print(hn.shape)
print(cn)
print(cn.shape)</code></pre>]]></description></item><item>    <title><![CDATA[SPSS与Python用Resblock优化BP神经网络分析慢性胃炎病历数据聚类K-means/AG]]></title>    <link>https://segmentfault.com/a/1190000047575390</link>    <guid>https://segmentfault.com/a/1190000047575390</guid>    <pubDate>2026-01-27 16:06:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=snEbQiXSzIpe5yaEL1uwaA%3D%3D.iYudxoASDRu1Ga4KR0qODoO1OPC%2BwwSIVLE1xAQbIvg%3D" rel="nofollow" title="https://tecdat.cn/?p=44893" target="_blank">https://tecdat.cn/?p=44893</a>  <br/>原文出处：拓端数据部落公众号  <br/><strong>关于分析师</strong>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575392" alt="" title=""/>  <br/>在此对Chang He对本文所作的贡献表示诚挚感谢，他在中国中医科学院完成了中医信息学专业的硕士学位，专注中医临床数据挖掘领域。擅长Python、深度学习、临床数据采集与挖掘。Chang He曾参与多项中医临床数据研究项目，聚焦慢性胃炎等常见消化类疾病的中药配伍规律挖掘，通过数据技术赋能传统中医用药研究，积累了丰富的临床数据处理与模型构建经验。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575393" alt="" title="" loading="lazy"/></p><h3><a name="t1" target="_blank"/>专题名称：慢性胃炎中药用药规律数据挖掘与AI预测实践</h3><h4><a name="t2" target="_blank"/>引言</h4><p>中医治疗慢性胃炎注重辨证施治与中药配伍，传统用药经验多依赖医师传承，难以快速提炼普适性规律并实现精准指导。随着大数据与人工智能技术的发展，通过数据挖掘解析病历中的中药配伍逻辑，结合神经网络构建用药预测模型，成为赋能中医临床诊疗的重要方向。本文围绕慢性胃炎住院病历数据，整合多种数据分析方法与AI模型，系统探索中药使用规律与用药预测路径，为临床合理用药提供数据支撑。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，<strong>该项目完整代码与数据已</strong>分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂 怎么做，也懂 为什么这么做；遇代码运行问题，更能享24小时调试支持。  <br/>本研究以两家医疗机构的慢性胃炎住院病历为核心数据，采用人工、VBA宏与大语言模型结合的方式提取并规范数据，通过SPSS系列工具与Python库实现频数分析、聚类分析、关联规则挖掘，同时构建含Resblock模块的神经网络模型，实现基于临床症状的中药预测。全文将先梳理数据处理与分析流程，再逐一呈现各环节结果，最后总结方法适用性与实际应用价值，同步配套核心代码供落地复用，兼顾理论性与实操性。</p><h4><a name="t3" target="_blank"/>项目文件目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575394" alt="" title="" loading="lazy"/></p><h3><a name="t5" target="_blank"/>研究方法与技术准备</h3><h4><a name="t6" target="_blank"/>数据来源与处理</h4><p>本研究选取两家医疗机构的慢性胃炎住院病历作为研究对象，其中一家机构数据时间范围为2016年1月至2024年5月，聚焦中药配伍规律挖掘；另一家机构数据时间范围为2013年1月至2021年10月，用于神经网络模型构建，数据集含2214个样本、364种临床特征及469种中药。  <br/>数据提取采用人工、VBA宏与大语言模型协同模式，既保障人工校验的准确性，又通过工具提升效率。数据规范化依据《中药学》新世纪版标准，统一中药名称、剂量等关键信息，为后续分析奠定基础。</p><h4><a name="t7" target="_blank"/>核心工具与方法说明</h4><ol><li>分析工具：SPSS Modeler 18.0、SPSS Statistic 26.0、Python 3.11.5（Sklearn、Scipy、Pytorch 2.0.1模块），上述工具国内均可正常访问使用，无替代需求，其中Python相关模块可通过镜像源快速安装。</li><li>分析方法：频数分布分析（提炼高频中药与临床特征）、聚类分析（K-means、AGNES，对比不同距离与连接法适用性）、关联规则挖掘（挖掘中药联用规律）、BP神经网络（含Resblock模块，优化症状到中药的预测精度）。</li></ol><h4><a name="t8" target="_blank"/>核心代码适配与说明（数据提取环节）</h4><p>以下代码用于中药名称提取与数据清洗，优化变量名与语法结构，适配中文文本处理需求，省略部分重复数据校验代码：</p><pre><code>import pandas as pdimport re# 读取Excel格式的病历数据文件input_excel = '病历数据.xlsx' # 替换为实际数据文件路径data_df = pd.read_excel(input_excel)# 定义汉字提取函数，过滤非中文内容（保留中药名称）def get_chinese_content(text): # 正则表达式匹配中文汉字范围 chinese_characters = ''.join(re.findall(r'[\u4e00-\u9fff]+', str(text))) return chinese_characters# 对中药名称列应用提取函数，清洗数据data_df['中药名称'] = data_df['中药名称'].astype(str).apply(get_chinese_content)# 保存清洗后的数据至新文件output_excel = '清洗后病历数据.xlsx'data_df.to_excel(output_excel, index=False, engine='openpyxl')print(f"数据清洗完成，结果已保存至 {output_excel}")</code></pre><p>代码功能：针对病历数据中的中药名称列进行清洗，提取纯中文内容，剔除符号、数字等干扰项，保障后续分析数据的规范性。省略部分为数据去重、空值填充逻辑，可根据实际数据质量补充。</p><h3><a name="t9" target="_blank"/>研究结果与分析</h3><h4><a name="t10" target="_blank"/>频数分析结果</h4><p>本次分析共涉及281种中药、7375个用药实例，平均每张处方开具15种中药。其中甘草使用频次最高，达341次，占比71.49%，平均剂量7.8g；黄精、升麻等51种中药仅使用1次，频次最低。  <br/>频次排名前20的中药如下表所示，高频中药多集中在理气、健脾、清热类别，符合慢性胃炎脾胃失调、气滞热蕴的常见病机。  <br/>表4 药物频次统计前20位</p><table><thead><tr><th>中药</th><th>频次</th><th>占比（%）</th></tr></thead><tbody><tr><td>甘草</td><td>341</td><td>71.49%</td></tr><tr><td>陈皮</td><td>280</td><td>58.70%</td></tr><tr><td>半夏</td><td>272</td><td>57.02%</td></tr><tr><td>白芍</td><td>237</td><td>49.69%</td></tr><tr><td>柴胡</td><td>236</td><td>49.48%</td></tr><tr><td>白术</td><td>222</td><td>46.54%</td></tr><tr><td>黄连</td><td>216</td><td>45.28%</td></tr><tr><td>茯苓</td><td>198</td><td>41.51%</td></tr><tr><td>枳实</td><td>183</td><td>38.36%</td></tr><tr><td>延胡索</td><td>183</td><td>38.36%</td></tr><tr><td>砂仁</td><td>179</td><td>37.53%</td></tr><tr><td>党参</td><td>173</td><td>36.27%</td></tr><tr><td>香附</td><td>155</td><td>32.49%</td></tr><tr><td>黄芩</td><td>142</td><td>29.77%</td></tr><tr><td>厚朴</td><td>135</td><td>28.30%</td></tr><tr><td>丹参</td><td>125</td><td>26.21%</td></tr><tr><td>紫苏梗</td><td>121</td><td>25.37%</td></tr><tr><td>当归</td><td>120</td><td>25.16%</td></tr><tr><td>海螵蛸</td><td>107</td><td>22.43%</td></tr><tr><td>干姜</td><td>102</td><td>21.38%</td></tr></tbody></table><h3><a name="t11" target="_blank"/>中药频次分布如下图所示，呈现明显的长尾分布特征，少数中药在临床中广泛应用，多数中药针对性使用。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575395" alt="" title="" loading="lazy"/></h3><hr/><p><strong>相关文章</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575396" alt="" title="" loading="lazy"/></p><h3><a name="t12" target="_blank"/>Python预测二型糖尿病：逻辑回归、XGBoost、CNN、随机森林及BP神经网络融合加权线性回归细化变量及PCA降维创新</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=lTKTe%2BjuEct14sS3lT8n4A%3D%3D.b4%2F8KAJFQgOXLTSD1b2pxLL3UK31XHqQQHziiezi6j8%3D" rel="nofollow" title="https://tecdat.cn/?p=43572" target="_blank">https://tecdat.cn/?p=43572</a></p><hr/><h4><a name="t13" target="_blank"/>聚类分析结果</h4><p>聚类分析核心目标是挖掘中药联用的内在规律，对比K-means与AGNES两种聚类方法，结合不同距离计算方式与连接法，从轮廓系数、临床可解释性等维度评估适用性。</p><h5>K-means聚类</h5><p>簇数设置为1-20时，通过WSS图（组内平方和）观察簇数适配性，拐点虽不明显，但簇数为2、3、5、9时WSS下降趋势变缓，簇数适中。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575397" alt="" title="" loading="lazy"/>  <br/>表5 不同簇数的K-means聚类平均轮廓系数</p><table><thead><tr><th>簇数量</th><th>簇样本量</th><th>平均轮廓系数</th></tr></thead><tbody><tr><td>2</td><td>12，29</td><td>0.1490</td></tr><tr><td>3</td><td>5，30，6</td><td>0.1252</td></tr><tr><td>5</td><td>3，24，9，2，3</td><td>0.0914</td></tr><tr><td>9</td><td>4，6，14，2，2，2，8，2，1</td><td>0.0581</td></tr></tbody></table><p>当簇数设为9时，各簇样本轮廓系数表现较好，通过PCA降维可视化聚类结果如下：  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575398" alt="" title="" loading="lazy"/>  <br/>K-means聚类结果临床可解释性较强，平均评分4.67分，仅簇2可解释性较低（2分）。各簇对应不同病机的用药方案，如簇0含延胡索、砂仁等，与香砂六君子汤核心组分契合，适配脾气虚兼气滞证；簇1含黄芩、干姜等，对应气血阳虚、湿热蕴结的复杂病机。  <br/>表6 K-means聚类结果</p><table><thead><tr><th>簇名</th><th>中药</th><th>可解释性评分</th></tr></thead><tbody><tr><td>0</td><td>延胡索，砂仁，党参，木香</td><td>5</td></tr><tr><td>1</td><td>黄芩，干姜，桂枝，黄芪，生姜，大枣</td><td>5</td></tr><tr><td>2</td><td>黄连，枳实，厚朴，海螵蛸，六神曲，吴茱萸，佩兰，竹茹，苍术，浙贝母，瓜蒌，白及，鸡内金，麦芽（14味）</td><td>2</td></tr><tr><td>3</td><td>香附，紫苏梗</td><td>5</td></tr><tr><td>4</td><td>白芍，柴胡</td><td>5</td></tr><tr><td>5</td><td>陈皮，半夏</td><td>5</td></tr><tr><td>6</td><td>丹参、当归、川芎、枳壳、百合、乌药、豆蔻、酸枣仁（8味）</td><td>5</td></tr><tr><td>7</td><td>白术、茯苓</td><td>5</td></tr><tr><td>8</td><td>甘草</td><td>5</td></tr></tbody></table><h5>AGNES聚类（不同连接法对比）</h5><ol><li>欧氏距离+最长距离法：簇数设为9时，平均轮廓系数0.0803，临床可解释性评分4.11分，部分簇中药组合对应明确诊疗需求，如簇0含香附、紫苏梗等，侧重理气活血。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575399" alt="" title="" loading="lazy"/></li><li>欧氏距离+最短距离法：簇数设为12时，平均轮廓系数0.0637，但临床可解释性仅1.33分，多数簇仅含单味药，难以提炼联用规律。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575400" alt="" title="" loading="lazy"/></li><li>欧氏距离+组间平均连接法：簇数设为12时，平均轮廓系数0.0901，临床可解释性3分，兼顾聚类效果与规律提取，如簇1（枳实、厚朴）、簇2（白芍、柴胡）均为临床常用配伍。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575401" alt="" title="" loading="lazy"/></li></ol><h4><a name="t14" target="_blank"/>聚类分析核心代码（AGNES方法）</h4><p>以下代码优化变量名与注释，适配聚类分析需求，省略部分图表美化与结果导出代码，同时提供24小时应急修复服务，代码运行异常可快速响应，效率较自行调试提升40%：</p><pre><code>import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import AgglomerativeClusteringfrom scipy.cluster.hierarchy import dendrogram, linkagefrom sklearn.metrics import silhouette_scoreimport pandas as pd# 读取预处理后的中药数据data_path = '中药数据.xlsx'df = pd.read_excel(data_path, usecols="A:RJ", nrows=41)labels = df.iloc[:, 0].values # 提取样本标签（中药名称）data = df.iloc[:, 1:].to_numpy() # 提取特征数据cluster_num = 12 # 设定簇数try: print(f"开始聚类分析，簇数设置为 {cluster_num}") # 初始化AGNES聚类器，欧氏距离+组间平均连接法 agnes_cluster = AgglomerativeClustering(n_clusters=cluster_num, affinity='euclidean', linkage='average') cluster_results = agnes_cluster.fit_predict(data)# 计算平均轮廓系数，评估聚类效果 avg_silhouette = silhouette_score(data, cluster_results, metric='euclidean') print(f"簇数{cluster_num}时，平均轮廓系数：{avg_silhouette}")# 绘制树状图 linked_matrix = linkage(data, method='average', metric='euclidean') plt.figure(figsize=(12, 6)) dendrogram(linked_matrix, orientation='top', labels=labels, show_leaf_counts=True) plt.title('层次聚类树状图') plt.xlabel('样本标签') plt.ylabel('距离阈值') plt.show() ... # 省略轮廓系数分布图绘制与结果保存代码except Exception as e: print(f"聚类分析过程中出现异常：{e}")</code></pre><h4><a name="t15" target="_blank"/>关联规则挖掘结果</h4><p>设置最小前项支持度0.1、最小置信度0.8，共得到451条关联规则，最高项数6项，其中项数4的规则最多（210条），项数2的规则最少（10条）。规则支持度与置信度前10名的关联规则临床可解释性均为满分，契合中医用药理论。  <br/>支持度前5的关联规则中，“党参→甘草”支持度最高（29.560%），二者为临床健脾益气常用配伍；“茯苓、陈皮→半夏”支持度25.367%，对应痰湿内阻型慢性胃炎的用药方案。  <br/>置信度前5的关联规则中，“吴茱萸、陈皮→黄连”置信度达98.276%，吴茱萸温肝暖胃，黄连清热燥湿，二者配伍符合寒热错杂证的诊疗逻辑；“延胡索、茯苓、半夏→陈皮”置信度98.077%，体现理气止痛、健脾化痰的联用思路。</p><h4><a name="t16" target="_blank"/>神经网络构建与结果</h4><h5>模型设计</h5><p>基于临床特征预测中药使用，构建含2个Resblock模块与1个全连接层的BP神经网络，Resblock模块通过跳跃连接缓解梯度消失问题，提升模型训练效果。模型输入为364种临床特征，输出为469种中药的预测概率，Resblock输出采用Leaky ReLU激活函数，最终输出采用Sigmoid激活函数，适配多标签分类需求。</p><h5>特征与标签选择</h5><p>临床特征频次前3位为烧心（63.69%）、口干（61.92%）、夜寐欠安（61.34%），均为慢性胃炎常见症状；中药标签选取覆盖高、中、低频药物，共12种，验证不同频次药物的预测效果。</p><h5>模型结果与评估</h5><p>采用二折交叉验证评估模型性能，F1值为43.54%，多数标签F1值波动幅度控制在0.017以内，模型稳定性较强。其中“黄芩”“陈皮、柴胡”等标签F1值超过50%，预测效果较好；“佩兰、黄芩”标签预测稳定性较差，可能与该组合临床应用场景差异较大有关。  <br/>高频药物黄芩预测F1值最高（53.42%），特征明确易被模型捕捉；白芍虽为高频药物，但召回率仅0.0799，呈现“高精低召”特征，提示其应用场景多样性导致模型难以全面识别；低频药物（占比&lt;1%）因样本量极少，模型多预测为阴性，F1值无法计算，需通过数据扩充优化。</p><h3><a name="t17" target="_blank"/>总结与应用建议</h3><p>本研究通过多种数据分析方法与AI模型，系统挖掘了慢性胃炎中药用药规律，构建了症状到中药的预测模型，核心结论与建议如下：</p><ol><li>用药规律：甘草、陈皮、半夏等为慢性胃炎核心用药，多以理气、健脾、清热类中药联用为主，关联规则挖掘出的高频组合可作为临床用药参考。</li><li>方法适配：K-means聚类在临床可解释性上优于AGNES，欧氏距离+组间平均连接法可作为AGNES聚类的优选参数，为同类研究提供方法借鉴。</li><li>模型优化：Resblock优化的BP神经网络可实现中药预测，但需针对低频药物扩充样本，优化标签设计，提升模型泛化能力。</li><li>临床应用：研究结果可辅助医师快速制定用药方案，尤其为年轻医师提供配伍参考，同时模型可作为中医用药教学的辅助工具。  <br/>  本研究所有代码与数据已同步至交流社群，提供人工答疑与24小时代码调试服务，助力临床数据挖掘爱好者快速落地实践。后续可结合更多医疗机构数据，优化模型参数，进一步提升结果的临床适配性。</li></ol>]]></description></item><item>    <title><![CDATA[用 Python 接入美股实时行情流：从数据延迟到系统化接入的实践 sydney ]]></title>    <link>https://segmentfault.com/a/1190000047575411</link>    <guid>https://segmentfault.com/a/1190000047575411</guid>    <pubDate>2026-01-27 16:05:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你正在自己搭建交易或行情系统，大概率遇到过类似的情况：<br/>你能拿到行情，但总觉得延迟不可控；<br/>接口能用，但一旦标的数量上来，系统开始变得不稳定。<br/>当你从“看行情”转向“用行情”，数据接入方式本身就会成为系统瓶颈。</p><h3>从使用场景看实时行情的真实需求</h3><p>在个人专业交易或高频策略场景中，你对行情数据的要求通常包括：</p><ul><li>数据能够持续推送，而不是频繁请求</li><li>支持多标的同时订阅</li><li>延迟与推送频率可预期</li><li><p>数据结构清晰，可直接进入策略或缓存层<br/>这类需求，本质上已经超出了传统 HTTP 轮询的适用范围。</p><h3>实时行情的常见工程痛点</h3><p>很多系统在早期阶段看起来“能跑”，但随着负载上升，问题会逐渐显现：</p></li><li>高频轮询带来不必要的连接与资源消耗</li><li>多标的管理复杂，订阅逻辑难以维护</li><li>数据字段不统一，解析成本增加</li><li><p>延迟不稳定，影响策略执行一致性<br/>这些问题并不来自策略，而是行情接入模型本身不合理。</p><h3>用数据流的方式理解实时行情</h3><p>在工程上，更合理的思路是把实时行情视为一条持续的数据流。<br/>WebSocket 的核心优势在于：</p></li><li>连接建立一次，长期保持</li><li>服务端主动推送数据</li><li><p>天然适合多标的订阅与高频更新<br/>在这种模型下，行情 API 更像是数据源，而你的系统只是负责接收、分发和消费数据。</p><h3>选择实时行情 API 时应关注什么</h3><p>在真正接入之前，你可以从以下几个关键点快速判断一个 API 是否适合生产系统：</p></li><li>WebSocket 连接方式与鉴权是否清晰</li><li>订阅指令是否支持批量标的</li><li>推送频率与数据粒度是否明确</li><li><p>返回数据结构是否稳定、规范<br/>这些因素决定了接口能否在系统中长期、稳定运行，而不仅仅是“能连上”。</p><h3>Python 接入示例</h3><p>下面给你一份 Python 示例，它展示了典型的 WebSocket 接入流程：</p></li></ul><pre><code>import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    # 实时行情高频，先打印结构
    print(data)

def on_open(ws):
    subscribe_msg = {
        "cmd": "subscribe",
        "args": ["US.AAPL"]
    }
    ws.send(json.dumps(subscribe_msg))

def on_error(ws, error):
    print("error:", error)

def on_close(ws):
    print("connection closed")

ws = websocket.WebSocketApp(
    "wss://stream.alltick.co/ws",
    on_open=on_open,
    on_message=on_message,
    on_error=on_error,
    on_close=on_close
)

ws.run_forever()
</code></pre><h3>数据结构比价格本身更重要</h3><p>当你把实时行情真正接入系统后，会发现一个有趣的现象：<br/>最先带来安全感的，往往不是价格变化，而是数据结构的整洁程度。<br/>常见的实时行情字段通常包括：</p><ul><li>标的标识（symbol）</li><li>毫秒级时间戳</li><li>最新成交价与成交量</li><li><p>买卖报价（bid / ask）<br/>结构清晰的数据可以直接进入策略模块、内存缓存，或作为统一行情源提供给下游服务，几乎不需要额外加工。<br/>AllTick的美股实时行情 API 在接口设计上就偏向这种工程友好型结构，无论是多标的订阅还是持续推送，都更容易融入现有系统。</p><h3>实时行情在系统中的典型流向</h3><p>在一个相对完整的交易系统中，实时行情数据通常会被：</p></li><li>推送给策略引擎进行实时计算</li><li>写入缓存，用于低延迟查询</li><li><p>转发给其他服务，作为统一行情入口<br/>当接入方式合理时，行情数据会在系统中自然流动，而不是成为需要频繁“救火”的模块。</p><h3>总结</h3><p>如果你正在设计或重构行情接入层，建议优先从系统视角思考：</p></li><li>数据是否以“流”的方式进入系统</li><li>接口是否足够稳定，能长期运行</li><li>数据结构是否能直接服务于策略与缓存<br/>当这些问题被解决后，技术实现反而会变得简单。<br/>行情每天都在变化，但一个设计合理的实时行情接口，往往能让整个系统保持长期稳定。这也是 WebSocket 美股实时 API 在交易系统中被广泛采用的原因。<br/><img width="723" height="370" referrerpolicy="no-referrer" src="/img/bVdnMG8" alt="" title=""/></li></ul>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-用数据重构 ITSM 决策与运营模式 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575431</link>    <guid>https://segmentfault.com/a/1190000047575431</guid>    <pubDate>2026-01-27 16:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>过去几年里，越来越多的组织已经上线 IT 服务台、 建立 ITSM系统，并以 ITIL 为参考去规范事件、问题、变更与请求管理。 但当系统数量、云资源、终端设备与跨部门协作一并增长时，团队会逐渐遇到同一种“管理瓶颈”：我们能看到很多工单、很多告警、很多流程记录，却很难把它们串成一条可解释的因果链。 这也是为什么“服务可观测性（Service Observability）”正在成为新一代 ITSM 架构的关键能力——它让 IT 服务不止能被记录，还能被解释、被预测、被治理。 <br/>基于这一思路，<a href="https://link.segmentfault.com/?enc=%2FEAvpgktFR1mALuHebFbvQ%3D%3D.AHf7mtVlZdmaGXHrblKsaP695Wrc0S6ZXnSXm2IOm1Pf8LAgKGb%2BKGLAadbkALO7cIVAkB%2BFNyJNeq%2B4q2ibEi%2FiXcsggriR0f5wUS6eTe0%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a>ServiceDesk Plus（首次出现）不只是一个处理请求的系统，更可以成为组织级服务运行的“统一事实源”，把工单、资产、变更、知识与自动化连接成可运营的闭环。</p><p>如果把 ITSM 比作“交通管理”，传统做法更像统计每条路每天通过了多少车、有没有按时清障；而服务可观测性关心的是：哪些路段正在变得拥堵、拥堵与哪些施工（变更）有关、哪些车辆（业务服务）受影响最大、有没有办法把拥堵前移到“预警”阶段并提前疏导。 这类能力的价值并不只在于“把问题解决更快”，而在于让 IT 团队能够用同一种数据语言同时回答三类人最关心的问题：一线用户关心体验与透明度、业务负责人关心连续性与影响范围、管理层关心投入产出与风险治理。</p><p><strong>为什么传统 ITSM 指标越来越“解释不动”：不是数据少，而是上下文断裂</strong></p><p>很多团队以为“指标解释不动”是因为数据不够多，所以不断加字段、加报表、加看板，结果反而更混乱。真正的原因通常是：你拥有大量点状数据，却缺少把它们连接起来的上下文。</p><p>在现代 IT 环境里，服务体验与业务影响往往不是由单一事件决定的，而是由一连串微小变化叠加形成：一次补丁延迟、一个配置漂移、一次不完整的变更评审、一段时间的容量紧张、某个接口偶发错误……这些信号单独看都“问题不大”，但组合起来会让服务逐渐变差，直到某一次触发阈值才爆发成重大事件。</p><p><strong>服务可观测性到底“观测什么”：三类信号 + 一条关联链</strong></p><p>服务可观测性并不是“多装一些监控”“多做几个仪表盘”。它的核心是：围绕服务运行，持续收集足够的信号，并在信号之间建立可解释的关联关系，让团队能回答“现在是否健康、为什么变差、下一步该怎么做”。</p><p> 在 ITSM 场景里，最实用的做法是把信号划分为三类：体验信号、运行信号、治理信号，并通过“服务”把三类信号串成一条关联链。</p><p><strong>把数据连起来：在 ITSM 里建立“服务上下文”的四个落地点</strong></p><p>可观测性落地的关键，不是做一个宏大的“全链路平台”，而是把服务上下文在 ITSM 的日常入口中一点点建立起来：让同类请求用同一套结构表达、让工单能关联到资产与服务、让变更与事件能在同一时间轴上对齐、让知识与沟通能被复用。 下面这四个落地点，是大多数组织都能从低成本开始做起、并持续扩展的路径。</p><p>方法论：从“看见”到“能改”的三层闭环（运行闭环 / 根因闭环 / 预防闭环）</p><p>很多团队做了大量报表与看板，最后仍觉得“没有改变”，原因通常不是工具不好，而是缺少把观测结果转化为行动的机制。 服务可观测性的终点不是“看得更清楚”，而是“改得更有效”。一个可落地的运营框架通常分三层闭环：第一层解决当下恢复（运行闭环），第二层减少重复成本（根因闭环），第三层把风险前移（预防闭环）。 这三层闭环不是并行的三套流程，而是同一套服务运营体系的不同深度：先让服务恢复快，再让问题少发生，最后让故障尽量不发生。</p><p><strong>1) 服务可观测性是不是等同于监控平台或 APM？</strong></p><p>不是。监控/APM 更多回答“系统层发生了什么”，而服务可观测性强调把体验信号、运行信号与治理信号连接成服务上下文，用来解释影响范围、定位因果并驱动改进闭环。它是一套面向服务运营与治理的方法体系。</p><p><strong>2) 我们没有完善 CMDB，也能做可观测性吗？</strong></p><p>可以。建议从关键服务与关键资产开始，先建立“最小关联”（工单→服务→关键系统/资产→最近变更），不要追求一次性覆盖全量 CI。可观测性的价值来自关键因果链，而不是 CI 数量。</p><p><strong>3) 如何避免“做了仪表盘，但大家不行动”？</strong></p><p>指标必须绑定默认动作：每个指标都要能回答一个明确问题，并对应一个可执行动作（重复问题→问题记录与根因修复；等待时间→审批与协作优化；变更后事件激增→变更复盘与回滚策略调整）。同时用固定节奏把行动固化。</p><p><strong>4) ServiceDesk Plus 在落地可观测性方面能提供哪些关键支撑？</strong></p><p>关键在于建立统一事实源：服务目录统一入口与字段口径、流程与业务规则固化运营动作、资产/配置项关联增强因果解释、知识与沟通沉淀提升复用效率，再通过报表与仪表板把服务信号呈现出来，帮助团队形成“看见→解释→行动→复盘”的闭环。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-人工智能代理如何影响IT服务台运营 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575445</link>    <guid>https://segmentfault.com/a/1190000047575445</guid>    <pubDate>2026-01-27 16:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在生成式人工智能（AI）引发广泛关注之后——这一热潮源于 ChatGPT 的媒体曝光以及其在 IT 服务管理（ITSM） 领域的多样化应用——ITSM 行业正逐步迈入 代理式人工智能（Agentic AI） 时代。</p><p>在这一阶段，人工智能代理具备自主行动的“能动性（agency）”，能够在较少人工干预的情况下独立完成任务。 本文将围绕 IT 服务台 的运营变化与 ITSM 软件 的落地实践，解读 AI 代理在真实工作流中的角色演进。<br/><img width="723" height="422" referrerpolicy="no-referrer" src="/img/bVdnMH2" alt="" title=""/></p><p><a href="https://link.segmentfault.com/?enc=iq2bpCuIqq96NjvorUKeoQ%3D%3D.hTLb6WR80Zkx9uNaTBXeI0RwLFkC%2BtopGODN660iLskEWJl5F744L9OBdWW7pPTlBOgg8fXP0fDWjSJOjqhnC5RPurTXe0tQeJlsfmbml%2Bg%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a><strong>近期开展的一项调研</strong>，聚焦于人工智能代理在 ITSM 运营中的应用前景。调研中对 AI 代理的定义如下：</p><p>“一种智能模型，能够从工单、电子邮件或对话中识别用户意图，自主收集上下文数据、做出决策并执行任务。这类代理可部署于服务台场景，用于事件管理或服务请求履行等工作。”</p><p><strong>AI 智能代理与虚拟代理的区别</strong><br/>在实际讨论中，人们往往在术语使用上不够严谨，在特定 ITSM 应用场景中泛化使用“AI”一词，而未明确其具体类型。因此，有必要加以澄清：“AI 智能代理”特指采用代理式 AI 的应用场景，而非传统虚拟代理或聊天机器人所使用的 AI 能力。</p><p>代理式 AI 正在深刻重塑 IT 服务台的运营方式，不仅改变了工作内容和效率结构，也对人员规划、服务质量和用户体验提出了新的可能性。</p><p><strong>1）AI 智能代理与虚拟代理的关键差异是什么？</strong><br/>虚拟代理主要面向终端用户交互并根据提示响应；人工智能代理以目标为导向，具备自主行动能力，执行过程中不一定与人类对话，并可与其他代理协同完成更高层级目标。</p><p><strong>2）AI 代理会取代 IT 技术人员吗？</strong><br/>调研结果中既包含“监督和管理 AI 代理”“专注于更复杂任务”等观点，也有受访者认为 AI 代理将取代 IT 技术人员。总体来看，AI 代理更可能带来协同与分工变化，同时效率提升也可能影响人员规模与招聘计划。</p><p><strong>3）AI 代理能为 IT 服务台带来哪些运营收益？</strong><br/>除了自主工单处理带来的人力成本节约与服务可扩展性提升，AI 代理还可通过主动预防问题、编排复杂工作流、增强知识管理与策略感知型自动化等方式提升服务质量、响应速度与整体体验。</p><p><strong>4）推动 AI 代理落地时，组织需要注意什么？</strong><br/>需要提前规划人员与角色变化，并坚持以人为本，结合组织变更管理（OCM）的工具与方法，使相关变化能够以更加自然、有序的方式推进，从而提升转型成功的概率。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-如何应对在 ITSM 中引入人工智能的风险 ServiceDeskPl]]></title>    <link>https://segmentfault.com/a/1190000047575455</link>    <guid>https://segmentfault.com/a/1190000047575455</guid>    <pubDate>2026-01-27 16:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>许多 AI 赋能能力已经内嵌于ITSM 工具 之中，客户组织正借助这些能力，更好地服务其员工与客户。 为便于读者从服务入口视角理解这些变化，本文亦将相关讨论与 IT 服务台 场景进行关联说明。</p><p>然而，AI 的采用同样伴随着风险。</p><p><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdnMIc" alt="" title=""/></p><p>在通往成功落地的过程中，这些风险必须被充分识别、审慎评估并加以缓解。</p><p>ManageEngine卓豪基于对 300 名 IT 专业人士开展的调研（及其配套报告《ITSM 中人工智能代理的兴起：认知与未来影响》，可通过相关渠道获取），系统梳理 IT 组织在采用 AI 能力时最为关注的核心问题，并就如何应对在 ITSM 中引入 AI 能力所涉及的关键风险提供实践性指导。</p><p><strong>1）在 ITSM 中引入 AI 最突出的风险类别是什么？</strong></p><p>本文聚焦于三类核心关切：人工智能治理、数据安全与隐私问题；人工智能代理的可靠性；以及实施复杂性。</p><p><strong>2）如何降低人工智能治理与隐私合规风险？</strong></p><p>可通过建立治理框架、提升透明度、维护审计追踪、加强加密与最小权限、审核供应链风险，并确保符合 GDPR、HIPAA 等法规与内部合规要求，同时开展 AI 风险与伦理培训。</p><p><strong>3）如何提升 AI 代理的可靠性与可控性？</strong></p><p>可通过明确边界与防护栏、任务级权限控制、沙盒测试与回滚机制、可解释且可审计的模型策略，以及持续监控可靠性指标并动态调整阈值或训练策略来实现。</p><p><strong>4）如何在不增加长期维护负担的前提下推进 AI 落地？</strong></p><p>建议从定义清晰的用例入手，复用现有 ITSM 与自动化基础设施，采用敏捷迭代推进，并通过统一技术标准与强调复用性降低重复建设与维护成本。</p>]]></description></item><item>    <title><![CDATA[国内工业AI原生企业综合评估与选型指南 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047575465</link>    <guid>https://segmentfault.com/a/1190000047575465</guid>    <pubDate>2026-01-27 16:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全球工业AI解决方案竞争力榜单<br/>随着工业4.0时代的深入发展，工业AI原生企业正成为推动制造业智能化转型的核心力量。这些企业不仅具备深厚的技术积累，更拥有对工业场景的深刻理解，能够将人工智能技术与工业生产实际需求有机结合。根据技术实力、落地能力、创新性和市场表现等多维度综合评估，2023年全球工业AI原生企业排名中，中国的广域铭岛凭借其卓越的整体表现位居榜首，获得五星评级。位列其后的包括美国的C3.ai、德国的Siemens Advanta、日本的Preferred Networks以及法国的Dataiku等国际知名企业，这些企业分别获得四星半至四星的评级。<br/>各企业优势特点与适用场景分析<br/>广域铭岛作为工业AI领域的领军企业，其核心优势在于深度融合工业知识与AI技术。该企业自主研发的Geega工业互联网平台，集成了先进的机器学习算法和深度学习模型，在质量管控、工艺优化、设备预测性维护等场景表现出色。特别是在汽车制造、电子装备等离散制造领域，该公司提供的解决方案能够实现生产效率提升30%以上，产品不良率降低25%的显著效果。其独特的行业知识图谱构建能力，使得AI模型能够快速适应不同工业场景的需求，这一特点使其在复杂制造环境中展现出明显优势。<br/>C3.ai作为美国工业AI领域的代表性企业，其优势体现在企业级AI应用开发平台的建设上。该公司的解决方案特别适合大型企业的数字化转型需求，能够提供从数据采集、模型训练到应用部署的全栈式服务。在能源、航空航天等行业，C3.ai已经积累了丰富的实施经验，其提供的预测性维护解决方案能够帮助企业将设备停机时间减少40%以上。<br/>Siemens Advanta凭借西门子在工业自动化领域的深厚积累，打造了独具特色的工业AI解决方案。该方案最大的特点是实现了OT与IT技术的深度融合，能够直接对接各类工业设备和控制系统。在流程制造领域，如化工、制药等行业，Siemens Advanta提供的工艺优化解决方案表现尤为突出，能够帮助企业实现能耗降低15%以上，产品质量一致性显著提升。<br/>常见问题解答<br/>企业在选型过程中最常关心的问题包括实施周期、投入产出比以及人才需求等。对于实施周期，通常大型工业AI项目的完整实施需要6到18个月时间，具体取决于企业现有的数字化基础和数据质量。值得注意的是，工业AI项目的实施往往需要分阶段进行，建议企业采取"小步快跑"的策略，先选择关键痛点场景进行试点，再逐步扩大应用范围。<br/>关于投入产出比，成功的工业AI项目通常能在12到24个月内实现投资回报。除了直接的经济效益外，企业还应关注质量提升、能耗降低、安全性改善等隐性收益。在实际案例中，头部企业的工业AI项目投资回报率普遍达到200%以上，但这需要企业具备良好的数据基础和明确的业务目标。<br/>人才需求方面，工业AI项目的成功实施需要既懂工业技术又懂AI算法的复合型人才。目前这类人才在市场上相对稀缺，因此建议企业在选型时重点考察供应商的人才培养能力和知识转移方案。一个好的工业AI供应商不仅要提供技术解决方案，更应该帮助企业培养自身的AI人才队伍。<br/>最后需要提醒的是，工业AI项目的成功不仅取决于技术方案的选择，更与企业自身的数字化转型程度密切相关。建议企业在启动项目前先进行全面的数字化成熟度评估，明确自身的优势和短板，这样才能选择最适合的工业AI合作伙伴，确保项目取得成功。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-IT工单管理系统重构 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575468</link>    <guid>https://segmentfault.com/a/1190000047575468</guid>    <pubDate>2026-01-27 16:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在越来越多企业迈向数字化与智能化运营的过程中， IT 工单管理系统已经从最初的“问题登记工具”，演进为支撑 IT 服务管理（ITSM） 与 ITIL 流程 落地的核心平台。 随着人工智能能力逐步嵌入工单系统，传统以人工和规则为主导的服务模式，正面临一次结构性的重构。</p><p><img width="519" height="378" referrerpolicy="no-referrer" src="/img/bVdnMIp" alt="" title=""/></p><p>过去十年，企业在 IT 工单系统上的建设重点主要集中在“流程是否标准”“响应是否及时”“是否可审计”等维度。 而今天，越来越多 IT 管理者开始思考一个更本质的问题： 当系统本身具备理解、判断与行动能力时，IT 服务是否还需要以人为中心来驱动？</p><p><a href="https://link.segmentfault.com/?enc=toiGyd5nzB6tKD4cJqWWYw%3D%3D.ru8zhyWBaFCGoU4O%2BbGLVdkFD2Mf5pG6muNxl2y9leIbmjUIER4Cwwg%2BjyaqMXBr1WiKa68N8Zsl%2FlvVKRQ3EWqnHsegROGi1GEopYG5FWY%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a>将围绕 AI 驱动下的 IT 工单管理系统重构路径展开，系统性探讨从自动化、智能化，到自治式服务运营的演进逻辑， 并结合企业级落地方法论、典型场景与关键指标，帮助组织构建面向未来的 IT 服务体系。</p><p><strong>传统 IT 工单管理的结构性瓶颈</strong><br/>在多数企业中，IT 工单管理系统最初的建设目标十分明确： 集中接收请求、规范处理流程、提供可追溯记录。</p><p>这一阶段的系统通常围绕以下能力展开：</p><ul><li>统一服务入口（邮箱、门户、电话转工单）</li><li>基于规则的分类、优先级与指派</li><li>SLA 计时与逾期升级</li><li>基本报表与审计记录</li></ul><p>这些能力在 IT 管理早期阶段发挥了重要作用，但随着业务复杂度提升，其局限性逐渐显现。</p><p>从自动化到智能化：AI 如何重塑工单处理逻辑<br/>为应对上述瓶颈，越来越多企业开始在 IT 工单管理系统中引入 AI 能力。 与早期“流程自动化”不同，AI 的价值不在于执行规则，而在于理解与推理。</p><p><strong>AI 工单系统的“成熟度跃迁”模型</strong><br/>在大量企业实践中，可以将 AI 驱动的 IT 工单管理能力划分为三个演进阶段。 理解这一成熟度模型，有助于组织合理规划投入节奏，避免“一步到位”的落地风险。</p><p><strong>迈向自治式 IT 工单管理：Agentic ITSM 的核心特征</strong><br/>当 AI 不再只是“辅助工具”，而是能够基于目标自主规划行动路径时， IT 工单管理系统的角色将发生本质变化——从流程执行平台，演进为 自治式服务运营系统（Autonomous Service Operations）。</p><p><strong>AI 会取代 IT 技术人员吗？</strong><br/>不会。AI 主要替代重复性执行工作，人类仍负责策略、治理与高影响决策。</p><p><strong>自治式 ITSM 是否存在风险？</strong><br/>风险可通过权限边界、审计追踪与人工审批节点进行有效控制。</p><p><strong>中小企业是否适合引入？</strong><br/>可以从 AI 辅助阶段开始，逐步演进，而非一次性全面自治。</p><p><strong>如何衡量投资回报？</strong><br/>建议结合 MTTR、工单量变化、人力投入与业务影响综合评估。</p>]]></description></item><item>    <title><![CDATA[Python 开发者指南：如何用 WebSocket 实现港股 Tick 级数据的低延迟推送？ Em]]></title>    <link>https://segmentfault.com/a/1190000047575473</link>    <guid>https://segmentfault.com/a/1190000047575473</guid>    <pubDate>2026-01-27 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>对于开发者而言，最痛苦的不是写不出策略，而是受限于基础设施的性能。如果你还在用 requests 轮询接口获取股票价格，那你基本上已经告别实时性要求较高的金融场景了。</p><p>今天我们就从工程化的角度，聊聊如何用 Python 优雅地解决港股实时行情的接入问题。</p><p>痛点分析：HTTP vs WebSocket 在传统的 Web 开发中，我们习惯了无状态的 HTTP。但在金融数据领域，高频的握手开销是不可接受的。我们需要全双工通信，Server 端有数据变动直接 Push 给 Client。</p><p>技术选型与环境依赖 我们追求的是极致的轻量化。Python 3.9+ 配合 websocket-client 是目前性价比最高的方案。它足够底层，让你能控制每一个字节的流向，又不需要像 asyncio 那样处理复杂的时间循环（当然，如果你需要极高并发，后期可以重构）。</p><p><code>pip install websocket-client</code></p><p>核心代码实现 不管是你是对接交易所直连，还是使用像 AllTick 这样集成的三方 API，核心范式都是一样的：定义 on_message、on_open 等回调函数。</p><p>下面的代码片段展示了如何建立一个持久化的 WebSocket 连接。注意看，我们在 on_open 阶段发送了 JSON 格式的订阅 payload，这是目前主流金融 API 的标准交互方式。</p><pre><code>import websocket
import json

url = "wss://api.alltick.co/realtime/hk"

def on_message(ws, message):
    data = json.loads(message)
    # 打印最新成交价和涨跌情况
    print(f"{data['symbol']} 最新价格: {data['last_price']} 涨跌: {data['change']}")

def on_open(ws):
    # 订阅恒生指数及指定股票行情
    ws.send(json.dumps({
        "action": "subscribe",
        "symbols": ["HSI", "00700.HK"]
    }))

ws = websocket.WebSocketApp(url, on_message=on_message, on_open=on_open)
ws.run_forever()
</code></pre><p>数据流的下游处理 原始数据通常是 JSON 字符串，直接解析的开销很小。在生产环境中，我建议你拿到数据后不要直接 print，而是通过消息队列（如 Kafka）或者直接落库。但为了演示方便，我们这里直接用 Pandas 做一个简单的内存化清洗。</p><pre><code>import pandas as pd

# 假设我们有一个行情列表
ticks = [
    {"time": "09:30:01", "price": 500, "volume": 100},
    {"time": "09:30:02", "price": 502, "volume": 50},
    {"time": "09:30:03", "price": 501, "volume": 80},
]

df = pd.DataFrame(ticks)
df['time'] = pd.to_datetime(df['time'])
print(df)</code></pre><p>经验总结 通过这种方式，我们将数据的获取延迟从“秒级”压缩到了“毫秒级”。在处理港股这种波动剧烈的市场时，这种技术架构的升级，能让你的程序在起跑线上就领先别人一个身位。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnMvY" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[敏捷团队专属：Sprint复盘升级版——容器式任务封装工具实操攻略与方案 Ord1naryLife ]]></title>    <link>https://segmentfault.com/a/1190000047575127</link>    <guid>https://segmentfault.com/a/1190000047575127</guid>    <pubDate>2026-01-27 15:09:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在认知负荷极度饱和的数字化协作中，企业的效率瓶颈已从“任务分配”转向“任务上下文的完整性保护”。容器式任务封装工具不仅是静态的任务载体，更是通过逻辑隔离与资源集成，将复杂的工作包转化为可独立运行、可无损迁移的容器式执行单元。</p><h3><strong>一、 为什么现代敏捷团队必须重视“容器式”封装？</strong></h3><p>传统颗粒化任务管理往往导致“背景缺失”：任务与所需的文档、权限、上下文分离，导致执行者在切换任务时面临巨大的重构成本。容器式任务封装工具的核心价值在于：</p><ul><li><strong>消除执行漂移</strong>：通过将任务及其所有依赖（文档、工具、权限）封装在同一容器内，确保执行环境在不同成员间保持一致。</li><li><strong>支撑高内聚执行穿透</strong>：支持在容器内部进行深度钻取，从任务摘要穿透至最底层的操作原子，而不破坏外部逻辑。</li><li><strong>实现标准资产对齐</strong>：通过将验证有效的执行模式封装为“任务镜像”，确保各模块的产出质量自动对齐标准。</li><li><strong>复杂流程模块化解耦</strong>：将大型项目拆解为多个互不干扰的任务容器，实现跨团队、跨周期的快速部署与并行推进。</li></ul><h3>---</h3><p><strong>二、 容器式封装的技术路径：三层隔离架构</strong></p><p>构建容器式任务体系需要遵循“环境集成”与“边界清晰”的逻辑：</p><ol><li><strong>任务外壳层（Task Shell）</strong>：定义容器的外部接口，展示任务的状态标签、优先级及关键里程碑。</li><li><strong>内容封装层（Content Payload）</strong>：容器的核心，集成执行该任务所需的知识归纳、原始文档及协作记录。</li><li><strong>运行依赖层（Runtime Dependency）</strong>：位于容器底层，定义任务执行所需的特定权限、关联工具链及前置触发条件。</li></ol><h3>---</h3><p><strong>三、 核心技术实现与算法示例</strong></p><p>容器式任务封装工具的底层逻辑涉及状态一致性同步、依赖冲突检测及容器化价值评估。</p><h4><strong>1. 基于递归的容器健康度评估 (JavaScript)</strong></h4><p>在容器式封装中，任务的完成质量由其内部封装的所有依赖项和子任务共同决定：</p><p>JavaScript</p><p>/**  <br/> * 递归计算封装容器的整体健康得分  <br/> * @param {Object} containerNode 容器节点  <br/> * @returns {number} 容器聚合后的完成度得分  <br/> */  <br/>function calculateContainerHealth(containerNode) {</p><pre><code>// 基准情况：如果是原子级封装项，返回其标准化进度  
if (\!containerNode.subModules || containerNode.subModules.length \=== 0) {  
    return containerNode.readinessScore || 0;  
}

// 汇总子模块的加权得分  
const totalHealth \= containerNode.subModules.reduce((acc, module) \=\&gt; {  
    const importance \= module.logicWeight || (1 / containerNode.subModules.length);  
    return acc \+ (calculateContainerHealth(module) \* importance);  
}, 0);

return Math.round(totalHealth);  </code></pre><p>}</p><h4><strong>2. Python：容器依赖冲突审计引擎</strong></h4><p>利用容器模型，自动检测不同任务容器间是否存在资源占用或逻辑路径的冲突：</p><p>Python</p><p>class ContainerAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设标准：任务类型 \-\&gt; 必须封装的最小依赖项  
    self.standard\_manifests \= {  
        "Dev\_Sprint": \["Spec\_Doc", "Auth\_Key", "Test\_Case"\],  
        "Design\_Review": \["Prototype\_Link", "Feedback\_Log"\]  
    }

def verify\_container\_integrity(self, current\_task, task\_type):  
    """对比实际封装内容与标准清单，识别执行风险"""  
    required \= self.standard\_manifests.get(task\_type)  
    if not required:  
        return "缺失标准封装协议"

    missing \= \[item for item in required if item not in current\_task\['payload'\]\]  
    if missing:  
        print(f"\[Container Alert\] 任务 '{current\_task\['id'\]}' 封装不完整，缺失: {missing}")  
        self.\_trigger\_hotfix(current\_task\['id'\])
</code></pre><h3>---</h3><p><strong>四、 工具分类与选型思路</strong></p><p>实施容器式任务封装时，工具的选择应基于对“封装内聚力”的需求：</p><ul><li><strong>垂直集成类（如 板栗看板）</strong>：核心优势在于<strong>无限层级的容器嵌套</strong>，支持将任务关系连线与内容封装深度融合。</li><li><strong>多维数据类（如 Airtable）</strong>：通过强关联的字段将多源数据“装入”记录行，适合对大量标准化任务容器进行参数化管理。</li><li><strong>文档容器类（如 Notion）</strong>：利用页面即容器的特性，将讨论、任务与知识库进行逻辑封装。</li></ul><h3>---</h3><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“容器孤岛化”</strong>：应通过全局映射工具确保各独立容器间的逻辑对齐，防止执行偏离主线。</li><li><strong>动态激活任务镜像</strong>：将高频出现的优质任务封装沉淀为模板，实现一键实例化，降低冷启动成本。</li><li><strong>定期进行容器“减脂”</strong>：随着任务迭代，应精简容器内的陈旧文档和多余依赖，保持执行单元的轻量化。</li></ul><h3>---</h3><p><strong>六、 结语</strong></p><p><strong>容器式封装是提升组织执行确定性的核心手段。</strong> 它不仅解决了“任务信息散乱”的问题，更通过严密的模块化架构，将复杂的协作转化为可以精准复用的逻辑单元。当任务能够以容器形式标准化隔离时，团队才能在极速变化的节奏中实现“高专注度”与“高质量产出”的统一。</p>]]></description></item><item>    <title><![CDATA[聚焦攻略：运用容器式任务封装工具，实现任务执行的“标准化重塑” NAVI_s1mple ]]></title>    <link>https://segmentfault.com/a/1190000047575135</link>    <guid>https://segmentfault.com/a/1190000047575135</guid>    <pubDate>2026-01-27 15:08:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在认知负荷极度饱和的数字化协作中，企业的效率瓶颈已从“任务分配”转向“任务上下文的完整性保护”。容器式任务封装工具不仅是静态的任务载体，更是通过逻辑隔离与资源集成，将复杂的工作包转化为可独立运行、可无损迁移的容器式执行单元。</p><h3><strong>一、 为什么现代敏捷团队必须重视“容器式”封装？</strong></h3><p>传统颗粒化任务管理往往导致“背景缺失”：任务与所需的文档、权限、上下文分离，导致执行者在切换任务时面临巨大的重构成本。容器式任务封装工具的核心价值在于：</p><ul><li><strong>消除执行漂移</strong>：通过将任务及其所有依赖（文档、工具、权限）封装在同一容器内，确保执行环境在不同成员间保持一致。</li><li><strong>支撑高内聚执行穿透</strong>：支持在容器内部进行深度钻取，从任务摘要穿透至最底层的操作原子，而不破坏外部逻辑。</li><li><strong>实现标准资产对齐</strong>：通过将验证有效的执行模式封装为“任务镜像”，确保各模块的产出质量自动对齐标准。</li><li><strong>复杂流程模块化解耦</strong>：将大型项目拆解为多个互不干扰的任务容器，实现跨团队、跨周期的快速部署与并行推进。</li></ul><h3>---</h3><p><strong>二、 容器式封装的技术路径：三层隔离架构</strong></p><p>构建容器式任务体系需要遵循“环境集成”与“边界清晰”的逻辑：</p><ol><li><strong>任务外壳层（Task Shell）</strong>：定义容器的外部接口，展示任务的状态标签、优先级及关键里程碑。</li><li><strong>内容封装层（Content Payload）</strong>：容器的核心，集成执行该任务所需的知识归纳、原始文档及协作记录。</li><li><strong>运行依赖层（Runtime Dependency）</strong>：位于容器底层，定义任务执行所需的特定权限、关联工具链及前置触发条件。</li></ol><h3>---</h3><p><strong>三、 核心技术实现与算法示例</strong></p><p>容器式任务封装工具的底层逻辑涉及状态一致性同步、依赖冲突检测及容器化价值评估。</p><h4><strong>1. 基于递归的容器健康度评估 (JavaScript)</strong></h4><p>在容器式封装中，任务的完成质量由其内部封装的所有依赖项和子任务共同决定：</p><p>JavaScript</p><p>/**  <br/> * 递归计算封装容器的整体健康得分  <br/> * @param {Object} containerNode 容器节点  <br/> * @returns {number} 容器聚合后的完成度得分  <br/> */  <br/>function calculateContainerHealth(containerNode) {</p><pre><code>// 基准情况：如果是原子级封装项，返回其标准化进度  
if (\!containerNode.subModules || containerNode.subModules.length \=== 0) {  
    return containerNode.readinessScore || 0;  
}

// 汇总子模块的加权得分  
const totalHealth \= containerNode.subModules.reduce((acc, module) \=\&gt; {  
    const importance \= module.logicWeight || (1 / containerNode.subModules.length);  
    return acc \+ (calculateContainerHealth(module) \* importance);  
}, 0);

return Math.round(totalHealth);  </code></pre><p>}</p><h4><strong>2. Python：容器依赖冲突审计引擎</strong></h4><p>利用容器模型，自动检测不同任务容器间是否存在资源占用或逻辑路径的冲突：</p><p>Python</p><p>class ContainerAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设标准：任务类型 \-\&gt; 必须封装的最小依赖项  
    self.standard\_manifests \= {  
        "Dev\_Sprint": \["Spec\_Doc", "Auth\_Key", "Test\_Case"\],  
        "Design\_Review": \["Prototype\_Link", "Feedback\_Log"\]  
    }

def verify\_container\_integrity(self, current\_task, task\_type):  
    """对比实际封装内容与标准清单，识别执行风险"""  
    required \= self.standard\_manifests.get(task\_type)  
    if not required:  
        return "缺失标准封装协议"

    missing \= \[item for item in required if item not in current\_task\['payload'\]\]  
    if missing:  
        print(f"\[Container Alert\] 任务 '{current\_task\['id'\]}' 封装不完整，缺失: {missing}")  
        self.\_trigger\_hotfix(current\_task\['id'\])
</code></pre><h3>---</h3><p><strong>四、 工具分类与选型思路</strong></p><p>实施容器式任务封装时，工具的选择应基于对“封装内聚力”的需求：</p><ul><li><strong>垂直集成类（如 板栗看板）</strong>：核心优势在于<strong>无限层级的容器嵌套</strong>，支持将任务关系连线与内容封装深度融合。</li><li><strong>多维数据类（如 Airtable）</strong>：通过强关联的字段将多源数据“装入”记录行，适合对大量标准化任务容器进行参数化管理。</li><li><strong>文档容器类（如 Notion）</strong>：利用页面即容器的特性，将讨论、任务与知识库进行逻辑封装。</li></ul><h3>---</h3><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“容器孤岛化”</strong>：应通过全局映射工具确保各独立容器间的逻辑对齐，防止执行偏离主线。</li><li><strong>动态激活任务镜像</strong>：将高频出现的优质任务封装沉淀为模板，实现一键实例化，降低冷启动成本。</li><li><strong>定期进行容器“减脂”</strong>：随着任务迭代，应精简容器内的陈旧文档和多余依赖，保持执行单元的轻量化。</li></ul><h3>---</h3><p><strong>六、 结语</strong></p><p><strong>容器式封装是提升组织执行确定性的核心手段。</strong> 它不仅解决了“任务信息散乱”的问题，更通过严密的模块化架构，将复杂的协作转化为可以精准复用的逻辑单元。当任务能够以容器形式标准化隔离时，团队才能在极速变化的节奏中实现“高专注度”与“高质量产出”的统一。</p>]]></description></item><item>    <title><![CDATA[统信Windows应用兼容引擎V3.4.2更新解读 慵懒的猫mi ]]></title>    <link>https://segmentfault.com/a/1190000047575137</link>    <guid>https://segmentfault.com/a/1190000047575137</guid>    <pubDate>2026-01-27 15:08:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>统信Windows应用兼容引擎 V3.4.2 更新日志<br/>【优化】高级调试-组件安装内增加组件介绍<br/>【优化】支持直接打开日志文件<br/>【优化】投递应用时exe下载链接的预设文案<br/>【优化】外显了每个专区内收录的应用数量组件安装增加详细介绍之前有人吐槽，高级调试-组件安装当中的组件都是英文的，能不能提供中文名称？<br/><img width="723" height="560" referrerpolicy="no-referrer" src="/img/bVdnMCZ" alt="" title=""/></p><p>这些组件的名称本来就是英文的，没有中文名称，比如“JAVA”就是“JAVA”，“mono”就是“mono”，它没有中文名称，强行翻译成中文反而不方便大家去查询使用，但是我们可以给这些组件添加中文的介绍说明，告诉大家这些组件是干什么的，方便大家进行wine调试和研究：可以了解到wine应用一般都会安装什么组件解决什么问题，调试运行的时候需要解决什么问题，也可以去组件里搜索进行组件安装尝试。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575139" alt="图片" title="图片" loading="lazy"/><br/>直接打开调试日志文件高级调试-调试日志窗口处进行了一个微小的优化，将“打开日志”的行为从“打开日志所在文件”调整为“打开日志文件”。之前的版本当中，“打开日志”功能是打开日志所在文件夹，需要用户使用其他工具来打开日志文件，多了一步流程，而且查看日志的效果受到默认打开日志文件工具的影响。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575140" alt="图片" title="图片" loading="lazy"/><br/>兼容引擎本质上是一个工具型的应用，工具型应用是要注重效率问题的，随着收到大家越来越多的应用投递和应用适配申请，在进行应用wine适配时，需要频繁的进行应用调试和查看日志，缩短打开日志的路径以及更方便的审查日志，可以极大的提高wine适配效率，基于上述实际使用场景，将“打开日志”的行为从“打开日志所在文件”调整为“打开日志文件”，用来打开日志文件的工具是deepin-wine团队日常使用的日志分析工具，大家有好的想法也可以给我们提建议。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575141" alt="图片" title="图片" loading="lazy"/><br/>优化exe下载链接引导文案在投递应用时，填写exe文件下载链接的引导文案调整为“请提供链接用于复测，官方链接优先采用”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575142" alt="图片" title="图片" loading="lazy"/><br/>这个优化点很小，但却是需要大家认真关注的一个地方。一些应用程序是否能成功wine是与其版本号有强关联的，因此兼容引擎在提供wine应用数据库的时候着重强调了exe文件的版本号，大家在投递wine应用的时候一定要投递准确的应用版本，并且尽量提供软件官方的下载链接，方便审核人员下载应用可以加速审核。外显各应用专区内收录的软件数量自2025年5月21日上线“全部应用”模块后，经过deepin社区多次wine众测活动，在deepin-wine团队和各位爱好者们的共同努力建设下，目前兼容引擎已经收录了超过 3800+款应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575143" alt="图片" title="图片" loading="lazy"/><br/>为了控制应用投递的质量，目前兼容引擎的应用投递功能做了诸多限制，对于最重要的“应用名称”和“应用版本号”信息采用直接读取PE文件信息的策略，不允许自定义修改。同时为了防止恶意代码注入之类的风险，各字段的数据传输做了严格限制，因此一些打包不规范的exe文件、有风险的链接格式和字符可能导致无法投递。</p>]]></description></item><item>    <title><![CDATA[筑业云资料行列标及单元格操作指南：打造个性化表格 聪明的拐杖 ]]></title>    <link>https://segmentfault.com/a/1190000047575152</link>    <guid>https://segmentfault.com/a/1190000047575152</guid>    <pubDate>2026-01-27 15:07:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用筑业云资料软件进行表格编辑时，行列标以及单元格的合并与拆分操作是常见需求。熟练掌握这些操作，能让表格更符合实际使用要求，提升资料整理与展示的效果。以下为您详细介绍具体操作方法。<br/>行列标显示与隐藏<br/>显示行列标：通常情况下，打开表格时，左侧和上方看不到行列标，这会给行高、列宽的调整带来不便。此时，只需点击软件上方的 “行列” 按钮，在下拉菜单中勾选 “行列标” 选项，行列标就会自动显示出来。行列标出现后，您就能直观地对表格的行高和列宽进行精准设置，满足不同内容的排版需求。例如，当表格中某列内容较多时，可通过行列标调整列宽，使内容完整显示。<br/>隐藏行列标：若在完成行高、列宽调整后，不再需要显示行列标，同样点击 “行列” 下拉菜单，取消 “行列标” 的勾选，行列标便会自动隐藏，让表格界面更加简洁。这种灵活显示与隐藏行列标的方式，充分考虑了用户在不同操作阶段的需求。<br/>单元格合并与拆分<br/>合并单元格：在编辑表格过程中，为了使表格结构更清晰、内容展示更集中，常常需要合并单元格。操作时，只需选中多个想要合并的单元格，此时工具栏上的 “合并单元格” 按钮会自动激活显示。点击该按钮，选中的多个单元格就会合并成一个单元格。比如，在制作标题栏时，可将多个相邻单元格合并，使标题更醒目。<br/>拆分单元格：当需要对已合并的单元格或特定单元格进行细分时，选中多列或多行单元格，“拆分单元格” 按钮会显示。点击该按钮，即可将选中的单元格拆分成多个单元格，方便填写不同的详细信息。例如，在数据统计表格中，若之前合并的单元格需要细分以展示更具体的数据，就可使用拆分功能。<br/>解锁灰色单元格：有时会遇到灰色单元格，这类单元格默认处于锁定状态，无法直接进行合并或拆分操作。遇到这种情况，只需选中灰色单元格，点击 “解锁” 按钮（可在选中单元格后直接点击，也可点击工具栏上的 “解锁” 按钮），即可解除锁定，之后便能对其进行正常的合并、拆分等编辑操作。<br/>行列及单元格的其他功能<br/>点击 “行列” 按钮下拉菜单，您会发现还有许多实用功能。例如 “插入”“追加”“删除” 行和列的操作，方便在表格中灵活添加或移除内容；在这里还能调整行距，使表格内容间距更合理；同时，您还可以选择是否显示网格线，让表格外观更符合个人喜好和实际使用场景。这些丰富的功能，进一步增强了表格编辑的灵活性和个性化。<br/>通过以上操作方法，您可以轻松在筑业云资料软件中对行列标及单元格进行各种操作，打造出满足您需求的个性化表格，让工程资料的整理与展示更加高效、美观。</p>]]></description></item><item>    <title><![CDATA[智能体来了从 0 到 1：可复用性为何是智能体工程化的分水岭 Agentcometoo ]]></title>    <link>https://segmentfault.com/a/1190000047575158</link>    <guid>https://segmentfault.com/a/1190000047575158</guid>    <pubDate>2026-01-27 15:06:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在智能体系统的早期实践中，开发工作往往从解决单一问题开始：一个场景、一个目标、一次交付。这样的方式能够快速验证模型能力，却难以支撑长期演进。当系统从实验性 Demo 走向真实业务时，一个决定性指标会迅速浮现——<strong>可复用性（Reusability）</strong>。</p><p>在智能体工程中，可复用性并非附加属性，而是判断系统是否真正完成从 0 到 1 跨越的核心标准。不可复用的智能体，本质上只是一次性消耗品；而具备复用能力的系统，才能成为可持续演进的数字资产。</p><hr/><h2>一、从烟囱式实现到模块化系统：可复用性的工程定义</h2><p>在智能体架构中，可复用性并不等同于“代码能拷贝”，而是系统在不同任务、不同模型、不同业务之间迁移的能力。</p><h3>1. 模块化是可复用性的前提</h3><p>一个具备工程化潜力的智能体系统，通常被拆解为若干相互解耦的核心模块：</p><ul><li><strong>任务编排（Workflow）</strong>：定义清晰、可配置的执行路径</li><li><strong>工具接口（Tools）</strong>：遵循统一输入输出规范的能力单元</li><li><strong>提示词模块（Prompt Modules）</strong>：可组合、可替换的指令片段</li></ul><p>这种拆解方式，使系统从“一次性实现”转向“能力积木化”。</p><h3>2. 从 0 到 1 的分水岭效应</h3><ul><li><strong>0 阶段</strong>：<br/>每新增一个需求，就需要重新设计提示词、重写工具调用逻辑、调试完整流程。模型版本一旦变化，系统稳定性迅速下降。</li><li><strong>1 阶段</strong>：<br/>系统已具备标准化组件库。新任务更多是“重新编排”，而非“重新实现”。开发工作从解决问题转向构建系统能力。</li></ul><p>这一步的跨越，标志着智能体真正进入工程化阶段。</p><hr/><h2>二、可复用性带来的三层工程价值</h2><h3>1. 逻辑层复用：认知模式的标准化</h3><p>尽管业务表象差异巨大，但智能体的底层认知结构高度一致，例如：</p><ul><li>任务拆解</li><li>多步推理</li><li>校验与反思</li><li>结果汇总</li></ul><p>当这些认知模式被沉淀为可复用的流程模板或元提示词时，它们就不再服务于单一场景，而成为组织级资产。</p><h3>2. 工具层复用：接口规范释放规模效应</h3><p>智能体的能力边界由其工具集决定。工具是否可复用，取决于接口是否稳定、规范是否统一。</p><ul><li>采用结构化输入输出</li><li>明确参数约束与返回格式</li><li>避免隐式上下文依赖</li></ul><p>当工具具备标准协议后，同一能力可以被多个智能体并行调用，而无需重复开发。</p><h3>3. 知识层复用：长期记忆的通用化</h3><p>基于检索增强生成（RAG）的知识系统，其核心价值在于索引的通用性。</p><p>一个结构良好的知识库，应当能够同时支持客服问答、分析决策、内容生成等多种智能体形态。知识一旦完成结构化沉淀，便可以在不同智能体之间流转，而不再被绑定在单一应用中。</p><hr/><h2>三、实现高可复用性的关键工程挑战</h2><h3>1. 结构化通信而非自然语言耦合</h3><p>模块之间的通信必须可解析、可验证。<br/> 这意味着关键节点输出应采用稳定的数据结构，而不是依赖模型生成的自由文本。</p><p>只有当输出具备确定性，模块才能真正被复用。</p><h3>2. 状态与执行逻辑的解耦</h3><p>可复用系统必须将：</p><ul><li><strong>任务状态</strong></li><li><strong>执行逻辑</strong></li><li><strong>历史记忆</strong></li></ul><p>进行明确分离。<br/> 这样，同一逻辑模块才能被并行调用，避免上下文相互污染。</p><hr/><h2>四、结论：可复用性决定智能体系统的生命周期</h2><p>在智能体工程实践中，是否具备可复用能力，直接决定系统能否长期存在。</p><p>核心结论可以概括为：</p><ul><li>可复用性是区分原型与系统的关键指标</li><li>模块化、标准化、结构化是实现复用的必要条件</li><li>真正的竞争优势，将来自可持续积累的组件资产</li></ul><p>在行业实践中，智能体来了往往不是指模型能力的突然跃迁，而是系统工程范式的成熟。当每一次能力建设都能为下一次应用提供杠杆，智能体的商业价值才具备指数级放大空间。</p>]]></description></item><item>    <title><![CDATA[2026 AI 元年：当人工智能开始进入业务执行层 智能体小狐 ]]></title>    <link>https://segmentfault.com/a/1190000047575190</link>    <guid>https://segmentfault.com/a/1190000047575190</guid>    <pubDate>2026-01-27 15:05:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在人工智能技术演进的长周期中，2026 年被普遍视为一个关键分水岭： AI 的角色正在从“语义交互工具”转向“任务执行主体”。</p><p>这一变化并非模型能力的单点突破，而是 AI 在企业系统中参与深度决策与执行所带来的结构性转变。AI 不再只是提供分析结果或生成内容，而是被嵌入到业务流程的最小执行单元中，直接参与行动。</p><h2>一、从生成式到代理式：AI 技术形态的变化</h2><p>随着模型推理能力与工具调用能力的成熟，AI 系统开始呈现出明显的代理式特征。</p><p>这类系统能够基于自然语言目标，自主完成任务拆解、路径规划、系统操作与结果校验，并在反馈中不断修正执行策略。 与传统自动化流程不同，其核心能力不依赖于预设脚本，而体现在对不确定环境的动态适应能力。</p><p>在实际业务中，AI 的输出不再停留在“建议”层面，而是直接转化为对业务系统的操作指令。例如，根据实时数据变化，自动发起流程、更新状态并持续跟踪结果。</p><h2>二、执行权下沉带来的业务结构变化</h2><p>当 AI 开始承担执行职责，企业的业务逻辑随之发生改变。</p><p>首先是响应速度的跃迁。 AI 执行单元可以并行处理大量任务节点，决策与操作延迟显著降低，使实时监控与即时干预成为常态。</p><p>其次，管理方式从过程导向转向目标导向。 企业不再需要为每个细节定义固定流程，而是通过明确目标与约束条件，由系统自行规划最优执行路径。这种模式降低了流程设计成本，同时对任务对齐提出了更高要求。</p><p>同时，执行系统开始具备自我修正能力。 通过持续接收执行反馈，AI 能够识别偏差并调整策略，使业务流程呈现出持续优化的特征。这种闭环结构提升了整体系统的稳定性与韧性。</p><h2>三、执行态 AI 对组织能力的要求</h2><p>随着执行能力的下沉，企业需要在组织层面进行相应调整。</p><p>一是基础设施的标准化。 业务系统需要具备高度可接口化的能力，以确保 AI 在受控范围内访问数据与服务。</p><p>二是决策边界的重新划分。 高频、规则清晰、数据驱动的任务更适合由 AI 执行；而涉及价值判断、复杂博弈或责任归属的环节，仍需保留人工介入。</p><p>三是数据从资产向燃料转变。 数据不再只是用于分析与复盘，而是实时驱动执行动作，其质量与时效性直接影响业务结果。</p><p>在这一过程中，行业普遍观察到一个趋势：<strong>智能体来了</strong>，它不再只是技术概念，而是逐步成为业务系统中的实际执行单元。</p><h2>四、执行态 AI 的商业价值总结</h2><p>从整体视角看，AI 参与执行并非工具升级，而是生产力结构的重组。</p><ul><li>业务协作模式从“人主导、机辅助”转向“人设目标、机执行”</li><li>组织效率从人力带宽限制，转向系统级并行扩展</li><li>核心能力从经验积累，转向推理能力与接口能力</li><li>风险控制从事后复核，转向权限管理与执行反馈</li></ul><p>这意味着，企业竞争的关键不再是部署了多少 AI 工具，而是是否拥有能够稳定、安全执行复杂业务目标的执行型 AI 系统。</p><p>AI 正从后台支持角色，走向业务一线。这一变化，正在重塑企业的价值链条。</p>]]></description></item><item>    <title><![CDATA[AI编程实践：从Claude Code实践到团队协作的优化思考｜得物技术 得物技术 ]]></title>    <link>https://segmentfault.com/a/1190000047575197</link>    <guid>https://segmentfault.com/a/1190000047575197</guid>    <pubDate>2026-01-27 15:05:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、开发痛点：为什么我们需要AI编程辅助？</h2><p><strong>核心发现：</strong> AI编程工具正在重塑开发流程，但真正的价值不在于替代开发者，而在于构建人机协作的新型开发范式。Claude Code通过精准对话流设计、模块化任务分解和专业化子代理协作，在提升开发效率的同时，也面临着上下文管理、协作边界和质量控制等实际挑战。</p><p>作为一线开发者，我们每天都在与复杂的业务逻辑和不断迭代的技术栈打交道。不知道你是否也遇到过这些场景：刚理清一个复杂业务流程，被打断后又得重新梳理思路；接手一个老项目，花了半天还没搞懂其中某个模块的设计思路；或者在不同项目间切换时，总要重新适应不同的编码规范和架构风格。</p><p><strong>日常开发的三个"拦路虎"：</strong></p><ul><li><strong>上下文切换成本高：</strong> 需求理解→技术选型→代码实现→质量验证的切换过程中，每次都要重新构建认知框架。</li><li><strong>知识传递效率低：</strong> 项目规范、架构经验分散在文档和个人经验中，新成员上手或跨模块开发时处处碰壁。</li><li><strong>开发流程割裂：</strong> 需求→设计→编码→审查各环节串行传递，信息易失真且反馈滞后。</li></ul><p>这些问题不是简单的"加人"或"加班"能解决的。我们需要的是一种新的开发范式，而Claude Code这类AI编程工具正是在这样的背景下进入了我们的视野。它的价值不在于替我们写代码，而在于成为我们的"认知放大器"和"流程协作者"。</p><h2>二、Claude Code核心功能解析：从工具到方法论</h2><p>Claude Code构建了一套完整的AI辅助开发方法论。接下来将结合团队实际使用经验，从功能特性、使用场景和设计初衷三个维度，详细介绍其核心功能：</p><h3>精准对话流设计：控制AI思考的艺术</h3><p>第一次用Claude Code时，就像面对一个热情但经验不足的实习生——如果不明确告诉他要做什么、怎么做、有什么要求，他很可能会给你一个"惊喜"。对话流设计就是解决这个问题的关键。</p><p><strong>设计初衷：</strong> 对话流设计的本质是将人类的编程思维模式转化为AI可理解的结构化交互方式，通过明确的上下文管理和约束条件设置，引导AI生成符合预期的代码结果。</p><p><strong>核心功能</strong></p><p>对话流设计通过三个关键机制控制AI的思考过程：</p><ul><li><strong>上下文聚焦：</strong> 要求单次对话仅处理一个功能模块，避免多任务混合导致的AI注意力分散。我们曾经试过在一个对话里同时让AI处理多个模块，结果它把两个模块的错误处理逻辑混在了一起。</li><li><strong>约束明确化：</strong> 通过具体指令减少AI的自由度，比如"仅修改X包下文件"、"必须复用Y工具类"。这些约束要尽可能具体，比如不说"遵循项目规范"，而是说"使用ResultDTO作为统一返回格式，错误码规则参考ErrorCodeEnum"。</li><li><strong>增量式提问：</strong> 采用"先框架后细节"的提问策略，先让AI生成接口定义和整体框架，待确认后再逐步深入实现细节。这种方式很像我们带新人时"先搭骨架再填肉"的指导方法。</li></ul><p><strong>使用心法</strong></p><p>启动新功能开发时，我们会创建专用对话线程，并在初始prompt中明确四件事：</p><ol><li>当前任务的功能边界和目标（做什么，不做什么。）</li><li>必须遵守的技术约束和规范（用什么技术栈，遵循什么标准。）</li><li>期望的输出格式和交付物（要代码？要文档？还是两者都要？）</li><li>分阶段的实现计划（先设计接口，再实现逻辑，最后写测试。）</li></ol><p><strong>真实踩坑经验</strong></p><p>处理跨模块依赖时，我们发现AI很容易"忘记"之前设定的约束。后来我们总结出一个技巧：每开始一个新的实现阶段，就简要回顾一下关键约束。比如："现在我们要处理任务交接流程，请记得：1. 使用Redis分布式锁；2. 需要修改商运关系和新商成长任务；3. 异常处理要符合规范。"</p><h3>Plan模式：复杂任务的系统化分解</h3><p>面对"实现一个完整的拜访任务系统"这样的复杂需求，直接让AI生成代码就像让一个刚入行的开发者独立负责整个项目——结果往往是逻辑混乱、漏洞百出。Plan模式就是解决这个问题的"项目管理工具"。</p><p>现状与问题：我们早期使用Claude Code时，经常犯一个错误：把一个复杂需求一股脑丢给AI，然后期待它给出完美解决方案。结果通常是：</p><ul><li>实现逻辑不完整，有些边界情况根本没考虑到。</li><li>模块间接口设计不一致，调用起来磕磕绊绊。</li><li>技术选型不合理，用了个"看起来很酷但项目中并不适用"的方案。</li></ul><p>这其实不能怪AI，人在面对过于复杂的问题时也会手足无措。我们需要一种方法把大问题拆分成小问题，而Plan模式正是借鉴了项目管理中的WBS（工作分解结构）思想。</p><p>我们的解决方案：Plan模式使用三步法分解需求：</p><ol><li>需求分析与模块划分：把整体需求分解为独立的功能模块。</li><li>技术方案设计：为每个模块确定实现思路和技术选型。</li><li>任务优先级排序：根据依赖关系和重要性确定实现顺序。</li></ol><p><strong>实际操作示例：拜访任务需求</strong></p><p>拜访任务是一个简单的企业内部任务需求，在实现"拜访任务需求"时，我们通过Plan模式生成的任务清单是这样的：</p><pre><code>---
拜访任务线上化 - 模块清单
M1: 任务创建模块
- 功能: 创建拜访任务,包含基本信息、拜访对象、参与人员
- 复杂度: Medium (需要处理多表关联和事务)
M2: 任务审批模块
- 功能: 飞书审批流程集成,支持任务提交、审批、驳回
- 复杂度: High (涉及飞书审批接口集成和状态流转)
M3: 日程同步模块
- 功能: 将拜访任务同步到飞书日历
- 复杂度: Medium (飞书API调用和异常处理)
M4: 任务列表查询模块
- 功能: 分页查询任务列表,支持多维度筛选
- 复杂度: Medium (ES聚合查询和数据转换)
M5: 任务详情查询模块
- 功能: 查询任务详情,包括拜访对象、参与人员、商家信息
- 复杂度: Low (简单的关联查询)
M6: 任务统计模块
- 功能: 统计不同类型、不同状态的任务数量
- 复杂度: Low (聚合统计)
M7: 拜访结果提交模块
- 功能: 提交拜访记录,上传附件,记录拜访内容
- 复杂度: Medium (文件上传和多表操作)
M8: 任务分配模块
- 功能: 批量分配任务给运营人员
- 复杂度: High (涉及分配策略和定时任务)
M9: 任务触达模块
- 功能: 任务开始、结束、上传提醒通知
- 复杂度: Medium (飞书消息卡片模板)
M10: ES数据同步模块
- 功能: 将任务数据同步到ES,支持全文检索
- 复杂度: Medium (事件驱动和ES批量操作)
M11: 任务导入模块
- 功能: Excel批量导入拜访任务
- 复杂度: High (Excel解析、数据校验、批量插入)
M12: 任务状态变更模块
- 功能: 定时更新任务状态(未开始→进行中→已结束)
- 复杂度: Low (定时任务和批量更新)
---
第二步:技术方案设计
| 模块       | 数据存储               | 查询方案         | 外部集成          |
|------------|------------------------|------------------|-------------------|
| M1-任务创建  | MySQL (事务)           | -                | -                 |
| M2-任务审批  | MySQL + 审批记录表      | -                | 飞书审批API       |
| M3-日程同步  | -                      | -                | 飞书日历API       |
| M4-任务列表  | ES (主) + MySQL (辅)   | ES聚合查询       | -                 |
| M5-任务详情  | MySQL                  | 联表查询         | 商家信息API |
| M6-任务统计  | ES                     | ES聚合统计       | -                 |
| M7-结果提交  | MySQL + OSS            | -                | OSS文件上传       |
| M8-任务分配  | MySQL                  | 分配算法         | -                 |
| M9-任务触达  | -                      | -                | 飞书消息卡片      |
| M10-ES同步  | ES                     | Spring Event     | -                 |
| M11-任务导入 | MySQL                  | Excel 解析    | -                 |
| M12-状态变更 | MySQL                  | JOB定时任务  | -                 |
---
第三步:任务优先级排序
P0 核心流程(必须先实现)
1. M1-任务创建 (基础功能)
2. M5-任务详情 (数据展示)
3. M4-任务列表 (核心查询)
4. M7-结果提交 (闭环流程)
P1 审批与通知(核心功能增强)
5. M2-任务审批 (依赖M1)
6. M3-日程同步 (依赖M2审批通过)
7. M9-任务触达 (依赖M1、M7)
P2 运营功能(效率提升)
8. M8-任务分配 (依赖M1)
9. M11-任务导入 (依赖M1)
10. M6-任务统计 (依赖M4)
P3 技术优化(性能优化)
11. M10-ES同步 (依赖M1、M4)
12. M12-状态变更 (独立模块)
---
实现建议
阶段一:基础能力
- 先实现 M1、M5、M4,打通任务创建→查询→详情的基本链路
- 使用 MySQL 实现,暂不接入 ES
阶段二:审批流程
- 实现 M2、M3,接入飞书审批和日历
- 完成任务的完整生命周期管理
阶段三:结果闭环
- 实现 M7,完成拜访结果提交
- 实现 M9,添加任务触达通知
阶段四:运营优化
- 实现 M8、M11、M6
- 提升运营效率
阶段五:性能优化(按需)
- 实现 M10,将查询迁移到 ES
- 实现 M12,优化任务状态管理</code></pre><p>这种方式不仅让AI能够有序工作，也让我们对整个开发进度有了更清晰的把控。</p><h3>系统提示词：给AI立"规矩"的艺术</h3><p>如果把Claude Code比作一个新加入团队的开发人员，系统提示词（CLAUDE.md）就相当于给他的"入职手册"，告诉他团队的编码规范、工作流程和注意事项。</p><p><strong>新手常犯的错误：</strong> 把系统提示词写成"百科全书"，恨不得把所有项目知识都塞进去。结果AI要么忽略大部分内容，要么在生成代码时顾此失彼。我们早期的系统提示词长达5000字，包含了从架构设计到代码规范的所有内容，效果反而不好。</p><p>实践心得：有效的系统提示词应该像"护栏"而非"详尽手册"。我们发现，针对AI常见错误模式设计的针对性提示，远比全面但泛泛的规范更有效。现在我们的系统提示词控制在200字以内，只包含最关键的约束和指引。</p><p><strong>系统提示词模板</strong></p><p>经过多次迭代，我们总结出包含三个关键模块的系统提示词结构：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575199" alt="" title=""/></p><p><strong>使用技巧</strong></p><p>分享几个在实践中总结的系统提示词编写技巧：</p><ul><li><strong>避免信息过载：</strong> 不要试图包含所有知识，而是指引AI在需要时查询特定文档。例如："遇到分布式事务问题时，请参考/doc/分布式事务最佳实践.md文档中的TCC模式实现方案"。</li><li><strong>提供正向引导：</strong> 不仅说"不要做什么"，更要明确"应该怎么做"。例如，不说"不要使用过时的API"，而说"请使用OrderServiceV2替代OrderServiceV1。</li><li><strong>动态调整策略：</strong> 我们每两周会回顾一次系统提示词的有效性，根据AI最近常犯的错误补充新的约束。比如发现AI经常忘记处理空指针，就新增一条："所有方法入参必须进行非空校验，使用ValidateUtil.isEmpty()方法，异常时抛出IllegalArgumentException"。</li></ul><h3>SKILL与MCP：知识沉淀与外部能力扩展</h3><p>在团队协作中，我们经常说"不要重复造轮子"。同样，在使用Claude Code时，我们也需要一种机制来沉淀和复用那些有效的Prompt和解决方案——这就是SKILL和MCP机制的价值所在。</p><p><strong>SKILL机制：</strong> 把好经验变成"可复用组件"</p><p>SKILL本质上是将单次生效的Prompt指令沉淀为可反复调用的标准化复用资产。举个例子，我们团队处理"ES数据查询"逻辑时，总结出了一个内部版本的SDK。我们把这个SDK的调用方式封装成一个SKILL，以后遇到类似场景，只需调用这个SKILL，AI就能按照我们团队的最佳实践来实现。</p><p><strong>MCP协议：</strong> 让AI能"调用"外部工具</p><p>MCP（模型上下文协议）解决了AI与外部工具、数据源的连接问题。通过MCP，AI不再局限于静态知识，而是能够动态访问实时数据。我们集成了飞书MCP服务器，让AI能够直接操作飞书平台，如自动生成技术方案文档、读取PRD需求、同步数据到多维表格等。</p><p><strong>最适合封装为SKILL的场景</strong></p><p>1.<strong>复杂工具使用指南：</strong> 如"ElasticSearch接入"、"Redis缓存更新策略"等需要特定知识的场景。</p><p>2.<strong>常见错误处理模板：</strong> 如"分布式锁冲突处理"、"数据库乐观锁重试机制"等反复出现的问题解决方案。</p><p><strong>MCP协议的典型应用场景</strong></p><ul><li><strong>场景1: 自动生成技术方案文档</strong></li><li>AI分析需求后，通过飞书MCP调用feishu_create_doc；</li><li>直接在指定的知识库目录创建格式化的技术方案文档；</li><li>省去手动复制粘贴的繁琐步骤。</li><li><strong>场景2: 读取PRD需求</strong></li><li>用户提供飞书文档链接；</li><li>AI通过feishu_get_doc_content获取文档内容；</li><li>基于完整需求信息生成技术方案和实现计划。</li></ul><ul><li><strong>场景3: 数据同步到多维表格</strong></li><li>代码生成后的统计数据(如代码行数、涉及文件等)；</li><li>通过feishu_append_bitable_data自动追加到飞书多维表格；</li><li>便于团队追踪AI编程效率指标。</li></ul><h2>三、对话流设计方法论：让AI"懂"你的真实需求</h2><p>刚接触Claude Code时，我们采用的是简单直接的"需求-响应"模式：开发者描述需求，AI生成代码，开发者修改调整。这种模式在处理简单功能时还行，但遇到复杂场景就会出问题。</p><h3>现状分析：传统对话模式的局限性</h3><p>我们早期在项目中踩过的三个坑：</p><p><strong>三大典型问题：</strong></p><ul><li><strong>需求表达不完整：</strong></li></ul><p>开发者说"实现一个商家信息查询接口"，AI生成了基础的CRUD代码，但没有考虑商家数据权限、数据脱敏、缓存策略等实际业务需求 ；</p><p>实现任务时，只描述了"需要任务分配功能"，结果AI生成的代码没有处理任务池、任务优先级、分配策略等核心逻辑。</p><ul><li><strong>上下文管理混乱：</strong></li></ul><p>一个对话持续了十几轮后，AI开始忘记我们前面确定的"使用MyBatis-Plus + BaseMapper"的设计决策，擅自改成了JPA Repository模式； </p><p>在实现相关功能时，早期确定的DTO转换规范在后续模块中被遗忘，导致代码风格不一致。</p><ul><li><strong>迭代反馈滞后：</strong></li></ul><p>等AI生成完整的Service + Controller + Repository代码后才发现方向不对，比如数据库表设计与现有架构冲突，不得不从头再来，浪费了大量时间；</p><p>实现触达功能时，生成的飞书消息发送代码没有考虑现有的FeishuClient封装，重复造了轮子。</p><h3>核心问题：为什么AI总是"听不懂"？</h3><p>深入分析后，我们发现传统对话模式失败的根源在于三个核心矛盾：</p><p><strong>语义鸿沟</strong></p><p>自然语言描述的模糊性与代码逻辑的精确性之间的差距。我们说"这个接口要安全"，AI可能理解为"需要登录校验"，而我们实际想要的是：</p><ul><li>使用项目中的@Permission注解进行权限校验。</li><li>参数需要使用ValidatorUtil进行校验。</li><li>敏感操作需要记录操作日志。</li></ul><p><strong>约束衰减</strong></p><p>随着对话推进，早期设定的技术约束在AI理解中的权重逐渐降低。就像我们记笔记时，重要的事情要反复强调。比如：</p><ul><li>第1轮对话强调"必须继承BaseServiceImpl"。</li><li>第5轮对话AI可能忘记这个约束，直接实现了一个独立的Service类。</li><li>第10轮对话可能连项目的分层架构都混淆了。</li></ul><p><strong>目标偏移</strong></p><p>在多轮对话中，AI容易过度关注当前细节而忽视整体目标。比如讨论某个接口的参数设计时：</p><ul><li>AI可能会纠结于参数名称是否优雅。</li><li>而忽略了这个接口的核心业务价值是"快速检索符合条件的商家"。</li><li>结果生成的代码参数命名很完美，但缺少了分页、排序等实际必需的功能。</li></ul><h3>解决方案：结构化对话设计方法</h3><p>针对这些问题，我们团队总结出一套"三阶段对话模型"，现在已经成为我们使用Claude Code的标准流程：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575200" alt="" title="" loading="lazy"/></p><p><strong>阶段一：需求定义——把"要做什么"说清楚</strong></p><p>这个阶段的目标是确保我们和AI对需求达成共识。我们会用"用户故事+验收标准"的格式来描述需求：</p><p><strong>示例1：新商户成长任务分配</strong></p><pre><code>【用户故事】
作为新商户运营，我需要一个任务分配功能，以便将成长任务高效分配给运营人员
【验收标准】
 - 支持从任务池中按优先级(P0/P1/P2)筛选待分配任务
 - 支持指定运营人员进行任务分配，需校验运营人员是否有权限
 - 分配时需检查运营人员当前任务负载，超过上限时提示"当前任务数已达上限"
 - 分配成功后需发送飞书消息通知运营人员，消息内容包含任务详情和截止时间
 - 操作需记录到表，包含操作人、操作时间、任务ID、分配对象</code></pre><p><strong>示例2：商家数据权限查询</strong></p><pre><code>【用户故事】
作为商家运营，我需要一个商家信息查询接口，查询结果需要根据我的数据权限进行过滤
【验收标准】
 - 支持按商家ID、商家名称、商家状态进行查询
 - 支持分页查询，默认每页20条，最大100条
 - 查询结果需要根据当前用户的数据范围进行过滤
 - 商家敏感信息(手机号、身份证号)需脱敏处理
 - 接口需要权限校验，至少具有"商家查看"权限
 - 查询条件需记录到操作日志，便于审计</code></pre><p><strong>阶段二：边界明确——确定"怎么做"的约束条件</strong></p><p>在这个阶段，我们会明确技术栈选择、架构设计和各种约束条件。关键是要区分"必须遵守"和"建议参考"的约束：</p><p><strong>示例1：新商户成长任务模块</strong></p><pre><code>【技术约束】
必须遵守:
 - 使用SpringBoot标准分层架构,所有Service继承OcsBaseServiceImpl
 - 数据库操作使用MyBatis-Plus,实体类继承BaseEntity,Mapper继承BaseMapper
 - 接口返回统一使用Result&lt;T&gt;格式,错误码使用ErrorCode
 - 权限校验使用@Permission注解,参数校验使用@Valid + ValidatorUtil
 - 飞书消息发送必须使用FeishuClient,不要重复实现
建议参考:
 - 任务状态流转参考TaskServiceImpl中的状态机模式
 - 批量分配操作参考AssignImportHandler中的异步处理方式
 - 运营人员权限校验参考OperatorRelationServiceImpl
 - 数据权限过滤参考ScopeServiceImpl中的范围查询逻辑
【数据库约束】
 - 新增表必须包含created_at, updated_at, is_deleted字段
 - 表名使用ocs_前缀,字段名使用蛇形命名法
 - 索引设计需考虑查询场景,高频查询字段必须建立索引
 - 外键约束通过代码层面维护,不在数据库层面创建</code></pre><p><strong>示例2：机器人问答功能</strong></p><pre><code>【技术约束】
必须遵守:
 - Controller层使用@RestController + @RequestMapping,路径遵循/api/v1/{module}/{action}格式
 - Service层业务逻辑必须有事务控制,使用@Transactional(rollbackFor = Exception.class)
 - DTO转换使用项目中的ConvertUtil,不要手动赋值
 - 第三方API调用(如Dify)必须有重试机制和降级策略
 - 敏感配置(API Key)必须从配置中心读取,不要硬编码
建议参考:
 - 对话上下文管理参考RobotServiceImpl中的会话ID生成逻辑
 - 消息发送失败重试参考FeishuMessageHandler中的重试策略
 - 错误处理参考GlobalExceptionHandler中的统一异常处理
【集成约束】
 - Dify API调用统一通过DifyClient,响应格式需转换为项目标准格式
 - 飞书机器人回调处理需实现签名校验,参考FeishuEventHandler
 - 所有外部API调用需记录请求日志到integration_log表 </code></pre><p><strong>阶段三：迭代反馈——在"做的过程"中持续对齐</strong></p><p>这个阶段的核心是增量验证，避免一次性生成大量代码后才发现方向错误。</p><p><strong>实践要点：</strong></p><p>1.分模块实现，逐个验证</p><ul><li>先实现核心接口(如任务分配)，验证通过后再实现辅助功能(如任务查询)。</li><li>先实现Service层核心逻辑，验证业务流程正确后再补充Controller和参数校验。</li></ul><p>2.关键节点主动暂停</p><ul><li>数据库表设计完成后，先让开发者确认表结构再继续。</li><li>核心业务逻辑实现完成后，先通过单元测试验证再生成Controller。</li><li>集成第三方API前，先确认调用方式和参数映射。</li></ul><p>3.持续同步技术方案</p><ul><li>发现架构设计与现有代码不一致时，立即更新tech-solution.json和tech-solution.md。</li><li>新增数据表、修改核心逻辑时，同步更新技术方案文档。</li><li>每个模块完成后，更新模块状态为implemented → reviewed → verified。</li></ul><p><strong>对话示例：</strong></p><pre><code>开发者: "实现新商户成长任务分配功能"
AI: "我将分三步实现:
1. 先设计数据库表结构(ocs_task_assignment表)
2. 实现核心分配逻辑(NewMerchantTaskAssignService)
3. 实现Controller层接口
现在开始第一步,设计的表结构如下:
[展示表结构]
请确认表结构是否符合需求?"
开发者: "字段没问题,但索引需要加上(operator_id, status)"
AI: "已更新索引设计,现在开始实现Service层核心逻辑...
[生成Service代码]
核心逻辑已实现,包含:
- 任务池查询(按优先级过滤)
- 运营人员负载检查
- 任务分配和状态更新
- 飞书消息发送
请验证业务逻辑是否正确?"
开发者: "逻辑正确,继续实现Controller"
AI: "开始实现Controller层...
[生成Controller代码]
已完成模块M1实现,更新技术方案状态为implemented"</code></pre><p><strong>对话设计三原则与常见问题应对策略</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575201" alt="" title="" loading="lazy"/></p><h3>为什么这样设计：背后的认知科学原理</h3><p>这种结构化对话设计不是凭空想出来的，而是基于我们对人类认知过程的理解：</p><ul><li><strong>工作记忆限制理论：</strong> 就像我们一次只能记住7±2个信息块一样，AI的上下文理解能力也是有限的。通过分阶段对话和单次聚焦单模块，我们控制了每次交互的认知负荷。</li><li><strong>渐进式知识构建：</strong> 学习和理解是一个渐进过程，先掌握整体框架再深入细节，符合认知规律。这和我们教新人时"先讲架构图，再讲模块间交互，最后讲具体实现"的思路是一致的。</li></ul><h2>四、AI团队协作模式：子代理系统的实践与思考</h2><p>随着团队使用Claude Code的深入，我们发现单个AI助手已经难以满足复杂项目的开发需求——就像一个人再厉害也干不了一个团队的活。于是，我们开始探索让多个AI"角色"协同工作的模式，这就是子代理（SubAgent）系统的由来。</p><h3>团队协作的现状与挑战</h3><p>在传统开发模式中，我们有需求分析师、架构师、开发工程师、测试工程师等不同角色，他们通过文档、会议和代码审查等方式协作。这种模式虽然成熟，但在快节奏的业务迭代中，我们发现了一些问题：</p><p><strong>协作中的三大痛点：</strong></p><ul><li><strong>信息传递损耗：</strong> 需求文档从产品经理到开发再到测试，每经过一个环节就可能产生一些理解偏差。就像玩"电话游戏"，信息传到最后可能已经面目全非。</li><li><strong>责任边界模糊：</strong> 当出现问题时，有时会出现"这是架构设计问题"、"这是实现问题"、"这是测试不充分"的互相推诿。</li><li><strong>反馈周期漫长：</strong> 从需求分析到代码审查，整个流程走下来往往需要几天时间，等发现问题时可能已经投入了大量开发资源。</li></ul><p>这些问题促使我们思考：能不能在Claude Code中模拟团队协作模式，让不同的AI角色各司其职又协同工作？</p><h3>Claude Code的子代理协作模式</h3><p>借鉴了MetaGPT等框架的思想，我们在Claude Code中构建了由多个专业化子代理组成的AI团队协作系统。每个子代理承担特定角色，通过标准化中间产物协同工作。</p><p><strong>核心工作机制：中间产物驱动</strong></p><p>所有子代理通过共享"技术方案文档"进行协作，这个文档就像团队的"共享白板"，包含需求分析、模块划分、实现状态和接口设计等关键信息。每个子代理只负责修改文档中与自己角色相关的部分，确保信息一致性。</p><p><strong>四个核心子代理角色</strong></p><p><strong>技术方案架构师</strong></p><p>负责需求分析、技术方案设计和模块划分。相当于团队里的架构师，输出"技术方案文档"这个"施工蓝图"。</p><p><strong>核心职责：</strong></p><ul><li>需求拆解与模块划分</li><li>技术栈选型与架构设计</li><li>接口定义与数据模型设计</li><li>模块间依赖关系梳理</li><li>技术方案文档编写与维护</li></ul><p><strong>代码审查专家</strong></p><p>负责代码质量审查。扮演技术负责人的角色，从架构合规性、代码规范和稳定性等角度挑毛病。</p><p><strong>核心职责：</strong></p><ul><li>检查代码是否符合架构设计</li><li>验证代码规范和命名约定</li><li>识别潜在性能问题和bug</li><li>评估代码可维护性和扩展性</li><li>提供具体修改建议</li></ul><p><strong>代码实现专家</strong></p><p>专注于代码实现和单元测试编写。就像主力开发工程师，按照架构师设计的蓝图一块块地实现功能。</p><p><strong>核心职责：</strong></p><ul><li>根据技术方案实现代码</li><li>编写单元测试和集成测试</li><li>修复代码审查中发现的问题</li><li>编写API文档和使用说明</li><li>同步更新技术方案实现状态</li></ul><p><strong>前端页面生成器</strong></p><p>专门负责生成符合我们低代码平台规范的前端页面配置。这是针对我们商家域管理后台特点定制的角色。</p><p><strong>核心职责：</strong></p><ul><li>根据接口定义生成前端页面配置</li><li>实现表格、表单、详情页等标准组件</li><li>配置页面权限和数据范围过滤</li><li>优化前端交互体验</li><li>确保符合设计规范和响应式要求</li></ul><p><strong>协作流程</strong></p><p>我们采用"先整体规划，再迭代实现"的工作方式，有点像敏捷开发中的Sprint规划+Daily Scrum：</p><p><strong>1. 整体规划阶段：</strong></p><ul><li>产品经理提供需求文档。</li><li>协调者调用"技术方案架构师"子代理分析需求，生成技术方案文档。</li><li>团队评审技术方案，提出修改意见。</li><li>架构师子代理根据反馈修改方案，直到团队确认。</li></ul><p><strong>2. 单模块迭代阶段：</strong></p><ul><li>协调者从技术方案文档中选取一个模块。</li><li>调用"代码实现专家"生成代码。</li><li>调用"代码审查专家"审查代码。</li><li>实现专家根据审查意见修改代码。</li><li>重复"实现-审查-修改"直到通过。</li><li>更新技术方案文档，标记该模块为"已完成"。</li><li>进入下一个模块。</li></ul><h3>子代理协作的价值与局限</h3><p><strong>实践中的三个显著价值</strong></p><ul><li><strong>专业化分工提升质量：</strong> 每个子代理专注于特定领域，就像专科医院比综合医院在特定疾病上更专业一样。我们发现，专门的代码审查子代理比通用AI能发现更多潜在问题。</li><li><strong>流程标准化降低风险：</strong> 通过技术方案文档和明确的角色分工，开发流程被标准化和可视化。新人加入项目时，只要看技术方案文档就能快速了解整体情况。</li><li><strong>知识沉淀促进复用：</strong> 子代理的专业知识和决策逻辑被编码为可复用的配置和规则，避免了"人走经验丢"的问题。</li></ul><p><strong>遇到的四个实际挑战</strong></p><p><strong>子代理协作的挑战与应对：</strong></p><ul><li><strong>上下文同步问题：</strong> 当技术方案文档更新时，各子代理有时不能立即同步最新信息。解决办法：每次修改文档后，明确通知相关子代理"技术方案中XX部分已更新"。</li><li><strong>协作边界模糊：</strong> 在处理跨模块功能时，出现"该由哪个子代理负责"的困惑。解决办法：在技术方案文档中添加"责任人"字段，明确每个模块由哪个子代理负责。</li><li><strong>灵活性与标准化的平衡：</strong> 高度标准化的流程有时会限制处理特殊情况的灵活性。解决原则：90%的常规情况严格遵循标准流程，10%的特殊情况由人工介入处理。</li><li><strong>错误传递放大效应：</strong> 如果技术方案设计阶段就有问题，这个问题会在后续实现和审查阶段被放大。解决办法：加强技术方案的人工评审环节，确保"地基"打牢。</li></ul><h3>子代理协作的设计思考</h3><p>在设计这套协作模式时，我们有几个关键思考：</p><ul><li><strong>为什么选择"中间产物驱动"而非"直接沟通"？</strong></li><li>直接让子代理之间对话可能更灵活，但会导致沟通成本指数级增加（n个代理就有n(n-1)/2种沟通渠道）。通过"技术方案文档"这个单一事实来源，我们大大降低了协作复杂度，也便于追踪变更历史。</li><li><strong>角色划分的依据是什么？</strong></li><li>我们的角色划分基于软件开发的自然阶段（设计→实现→审查）和专业领域（后端→前端），这符合软件开发生命周期的自然规律。没有盲目追求角色数量，而是根据实际需求逐步增加。</li><li><strong>为什么采用"增量迭代"而非"一次性开发"？</strong></li><li>复杂系统的构建本质上是一个不断学习和调整的过程。增量迭代让我们能够及早发现问题并调整方向，避免在错误的道路上走得太远。这和我们常说的"小步快跑，快速迭代"理念一致。</li></ul><h2>五、实践经验与未来展望</h2><p>经过几个月的Claude Code实践，从最初的"试试看"到现在成为离不开的开发工具，我们积累了一些经验，也对AI编程的未来有了更清晰的认识。</p><h3>实践经验总结</h3><p><strong>人机协作的最佳平衡点：</strong></p><p>我们发现最有效的AI编程模式是"人类主导，AI辅助"，而不是反过来。我们将工作内容分为三类：</p><ul><li><strong>AI主导：</strong> 标准化代码生成（如基础CRUD接口）、单元测试编写、API文档生成等重复性高、规则明确的任务。</li><li><strong>人机协作：</strong> 技术方案设计、复杂逻辑实现、代码审查等需要结合领域知识和创造性思维的任务。</li><li><strong>人类主导：</strong> 需求分析、架构设计、质量决策等高风险、高创造性的任务。</li></ul><p><strong>上下文管理的实用技巧</strong></p><p>管理好对话上下文是用好Claude Code的关键，分享几个我们团队总结的技巧：</p><ul><li><strong>对话线程化：</strong> 为不同功能模块创建独立对话线程。我们曾经在一个对话里讨论三个不同模块，结果上下文混乱到不得不从头开始。</li><li><strong>关键信息锚定：</strong> 重要的技术决策和约束要在对话中反复强调。就像写文章时，核心观点要多次出现。</li><li><strong>文档外化：</strong> 复杂设计和决策要记录在外部文档中，而不是仅依赖对话历史。我们会在对话中引用这些文档："数据库设计详见/doc/db_design.md，特别是索引设计部分"。</li><li><strong>状态可视化：</strong> 通过技术方案文档中的进度标记（如[未开始]、[设计中]、[已实现]、[已审查]），直观跟踪开发状态。</li></ul><p><strong>质量控制的三个关键策略</strong></p><p>使用AI生成代码后，质量控制变得更加重要。我们的做法是：</p><ul><li><strong>多层次验证：</strong> 单元测试（AI生成）+ 集成测试（人工设计）+ 代码审查（人机结合）的三层验证体系。</li><li><strong>渐进式信任：</strong> 从简单、低风险模块开始使用AI，建立信任后再逐步扩展。我们最先用AI生成内部工具，验证没问题后才用于核心业务系统。</li><li><strong>错误模式学习：</strong> 记录AI常犯的错误类型，针对性优化系统提示词。我们有一个"AI错误案例库"，记录了"AI忘记处理分布式锁超时"、"日期格式转换错误"等典型问题及解决方案。</li></ul><h3>AI编程的局限性认知</h3><p>在实践过程中，我们也清醒地认识到AI编程并非万能解决方案，它有几个明显的局限性：</p><ul><li><strong>创造性思维不足：</strong> AI擅长在已有知识范围内进行组合和优化，但在需要突破性创新的场景下表现有限。比如我们尝试让AI设计一个全新的商家结算模型时，它还是会倾向于参考现有模型进行修改，难以跳出固有思维框架。</li><li><strong>上下文理解深度有限：</strong> 尽管Claude Code的上下文窗口已经很大，但对于我们系统中某些"牵一发而动全身"的核心模块，AI还是难以把握其深层设计意图和与其他模块的隐性依赖。</li><li><strong>质量责任边界模糊：</strong> 当AI生成的代码出现质量问题时，责任界定变得复杂。我们的解决办法是：开发者对AI生成的代码负全部责任，就像我们对自己写的代码负责一样。</li><li><strong>领域知识滞后性：</strong> AI对我们公司内部系统的最新变更反应不够及时。为此我们建立了"知识库更新机制"，每月将最新的系统变更和业务规则整理成文档，供AI参考。</li></ul><h3>未来发展方向思考</h3><p>基于这些实践经验，我们对AI编程工具的未来发展有几点思考：</p><ul><li><strong>更智能的上下文管理：</strong> 未来的AI编程工具应该能自动识别相关上下文、追踪依赖关系，并在适当的时候提醒开发者潜在的上下文冲突。就像经验丰富的团队领导，能记住每个人负责的模块和项目的整体情况。</li><li><strong>多模态交互模式：</strong> 除了文本对话，未来可能引入图表、流程图等多种交互方式。有时画一个简单的流程图(PlantUML)，比写几百字描述更能说明问题。</li><li><strong>自适应学习机制：</strong> AI编程工具应该能从团队的使用反馈中学习，适应特定团队的编码风格和业务领域。就像新加入团队的开发者，会逐渐适应团队的工作方式。</li></ul><h2>六、结语：人机协作的新型开发范式</h2><p>回顾这几个月使用Claude Code的经历，我们最大的体会是：AI编程工具的价值不在于替代开发者，而在于构建人机协作的新型开发范式。在这种范式下，人类开发者从繁琐的重复劳动中解放出来，更专注于需求分析、架构设计和质量把控等高价值创造性工作，而AI则承担起代码实现、文档生成和基础验证等标准化工作。</p><p>Claude Code作为我们实践的核心工具，通过精准对话流设计、模块化任务分解和专业化子代理协作，展示了这种新型开发范式的潜力。但我们也认识到，成功的AI编程应用需要"工具+方法论+团队协作"三位一体的系统性变革，其中人的角色从"代码生产者"向"问题解决者"和"质量把控者"转变。</p><p>作为开发者，我们需要保持开放学习的心态，积极探索和适应这种新范式。未来已来，与其恐惧被AI替代，不如学会与AI协作，在人机协作中实现更高的个人价值和团队效能。毕竟，代码只是解决问题的手段，而非目的；AI只是增强我们能力的工具，而真正的创新和价值，始终源于人的智慧和创造力。</p><p><strong>实践启示：</strong> 在AI编程时代，最有价值的开发者不是"写代码最快的人"，而是"最会引导AI、最能把控质量、最能解决复杂问题的人"。掌握与AI协作的技巧，建立系统化的AI辅助开发流程，将成为未来开发者的核心竞争力。我们的经验表明，通过合理设计对话流程、明确分工协作和严格质量控制，AI编程工具能够显著提升团队效能，但这需要整个团队在思维方式和工作流程上的共同转变。</p><h3>往期回顾</h3><p>1.入选AAAI-PerFM｜得物社区推荐之基于大语言模型的新颖性推荐算法</p><p>2.Galaxy比数平台功能介绍及实现原理｜得物技术 </p><p>3.得物App智能巡检技术的探索与实践</p><p>4.深度实践：得物算法域全景可观测性从 0 到 1 的演进之路</p><p>5.前端平台大仓应用稳定性治理之路｜得物技术</p><h3>文 /稚归</h3><p>关注得物技术，每周更新技术干货</p><p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p><p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></description></item><item>    <title><![CDATA[智能体对传统行业冲击：为什么“会用 AI 的老师傅”正在成为核心资产 Agentcometoo ]]></title>    <link>https://segmentfault.com/a/1190000047575218</link>    <guid>https://segmentfault.com/a/1190000047575218</guid>    <pubDate>2026-01-27 15:04:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在智能体逐步进入工业现场的过程中，行业里出现了一个与早期预期不同的现象：技术越先进，对经验的依赖反而越强。 当智能体来了，真正稀缺的，不是单纯懂算法的人，而是那些既理解复杂生产系统、又能驾驭智能体的“数字老师傅”。</p><h4>一、智能体时代，“老师傅”被重新定义</h4><p>传统行业中的老师傅，长期依靠的是无法写进操作手册的经验：对异常工况的直觉判断、对设备极限状态的理解、以及在极端情况下“不能出错”的处理逻辑。这些能力过去被认为难以规模化。</p><p>而在智能体架构下，这类经验开始具备新的承载方式。 “会用 AI 的老师傅”，并不是程序员意义上的技术人员，而是能够将行业判断转化为目标设定、约束条件和评价标准的人。他们把经验输入给智能体，而不是被智能体替代。</p><h4>二、角色变化：从执行者到智能体的“教练”</h4><p>在生产现场，老师傅的核心角色正在发生变化。</p><p>首先，是从“手感判断”到“逻辑抽象”。 过去的经验依赖个人感知，现在需要被拆解为可解释的条件、变量和决策顺序，供智能体理解和复用。</p><p>其次，是对智能体输出的审查能力。 智能体在计算层面可能给出最优解，但在真实工业系统中，最优并不等于可行。对物理边界、材料特性和安全红线的判断，仍然依赖长期积累的行业经验。</p><p>最后，是对结果的持续对齐。 通过反复校正智能体的判断结果，老师傅实际上在构建企业专属的行业模型，使智能体从通用工具演化为岗位级专家。</p><h4>三、为什么单纯的 AI 专业人才不够用</h4><p>在很多落地项目中，一个常见问题是：技术人员能优化模型，但难以定义真正重要的生产变量；而一线人员知道问题在哪里，却无法让系统“听懂”。</p><p>相比之下，具备行业经验的老师傅，更擅长从生产目标出发，判断哪些指标值得被优化、哪些异常必须被严格约束。这种能力并非来自算法训练，而来自真实事故、长期试错和对系统整体性的理解。</p><p>因此，在复杂行业中，智能体的效果上限，往往取决于经验是否被正确地输入和约束。</p><h4>四、实践启示：经验正在被“软件化”</h4><p>越来越多的企业开始意识到，智能体真正放大的不是算力，而是经验。</p><p>这促使三种变化出现： 一是将隐性经验转化为可复用的知识资产； 二是通过自然语言等方式，让经验型人员可以直接参与智能体训练； 三是形成“人评估系统，系统辅助人”的闭环，让经验在使用中不断被固化。</p><h4>五、结论：经验不是被淘汰，而是被放大</h4><p>在智能体深入产业的过程中，经验并没有失去价值，而是成为系统安全性和有效性的最后一道防线。</p><p>真正拉开企业差距的，不是是否使用智能体，而是谁能更快、更完整地把老师傅的判断逻辑转化为智能体可执行的规则。这意味着，未来最重要的人才，将是那些既理解工业现场，又能与智能体协作的人。</p>]]></description></item><item>    <title><![CDATA[通义深度搜索-操作指南 DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047575242</link>    <guid>https://segmentfault.com/a/1190000047575242</guid>    <pubDate>2026-01-27 15:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><a href="https://link.segmentfault.com/?enc=Zr6PSJaRDBWBr8QSE1w5eg%3D%3D.1VF2tsgecv1B%2Fm%2BG3B8dsTTZrYJNUS4VWl4rC314Kt8hHpU9%2FNlNrzWIV98bO5fFLoeQMFwUt0tR2dX8icLxXfBL45j7QR4E%2Fpy7RRjDw4glsUmg%2FhXR2pqFd7SeGaZtmOmfpqsE70G64LJ8CilCB9LbwIkB143InBGoSjwT3wR0mbUcG6vapy7HeYnlk5xI" rel="nofollow" target="_blank">通义深度搜索限时免费中，快来使用吧!</a></p><h2> 应用开通</h2><p>1.在阿里云百炼控制台的应用广场中点击<a href="https://link.segmentfault.com/?enc=SCzU%2BXdbTT9cQN2JBEpl8Q%3D%3D.3ZFlQEZ4e04jfY6w%2FkpwGwdsIsDacGohk9Bjp%2BpMeBQ53r%2BBuWpeJxycJrv3HsFMtAhpz9xAL0jWDQUl7Poo%2BRncC85xu%2FC7LStAmn3YBGI%3D" rel="nofollow" target="_blank">通义深度搜索</a>卡片，进入<strong>应用详情</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575245" alt="image" title="image"/></p><p>2.首次试用时，点击右上角<strong>免费开通</strong>完成应用开通。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575246" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575247" alt="image" title="image" loading="lazy"/></p><h2>应用管理</h2><p>点击<strong>我的应用</strong>进入应用管理页面。页面展示所有已创建的应用和应用key等信息，首次使用需要新增应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575248" alt="image" title="image" loading="lazy"/></p><h2>应用配置</h2><p>点击应用卡片或新增应用进入应用配置界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575249" alt="image" title="image" loading="lazy"/></p><h3>1.场景选择</h3><p>根据实际需求选择使用场景，当前可选通用场景、法律场景。</p><h3>2.互联网检索配置</h3><p>开启后支持实时互联网全栈信息检索，提升模型回答准确性及时效性。</p><h4>2.1检索策略</h4><p>在检索策略上，您可以在‘标准版本’和‘自定义版本’中选择一种</p><h5>标准版本</h5><p>标准的检索策略，选择标准版本时，可以进一步根据对于搜索效果与搜索耗时的偏好选择不同的性能版本。</p><ul><li><strong>Max版本</strong>：效果优先，检索更深入，结果更全面，但响应时间较长</li><li><strong>Turbo版本</strong>：速度优先，响应时间短，适合对实时性要求高的场景</li></ul><h5>自定义版本</h5><p>选择自定义检索策略时，有更多的配置进行更细化的配置。</p><ul><li>支持限定检索时间范围</li><li>支持限定网站范围，最多添加20个网站，配置后<strong>优先</strong>从此范围网站检索信息，如果无匹配信息则会扩展到<strong>全网</strong>检索，网站录入时会自动去重</li><li>支持配置recall数量，数量越高信息越全，但会占用更多资源，增加耗时</li><li>支持配置网页读取开关，开启后搜索结果更详细但是耗时增加</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575250" alt="image" title="image" loading="lazy"/></p><h4>2.2策略选择</h4><p>可根据搜索效果与rt偏好选择max版本和turbo版本。</p><h3>3.自有知识库配置</h3><p>支持接入非百炼的自有知识库作为搜索来源，开启选项后可进行配置，点击添加知识库配置</p><p>输入知识库名称、知识库描述、服务地址、授权信息，点击“服务测试”，验证通过后点击“保存”以完成添加。可参考<a href="https://link.segmentfault.com/?enc=sftw7BcNfI7QgW%2BV79Jo7w%3D%3D.YbWCSMiVF83tsvxIiGhv8fWMV8iIJyfL2cP8lLX%2F4Kqz9kOKvIf6JJzbsD7Q%2F8aMRp5zQ59FS9EgCMYqA41deovo7l2w3%2BJ5DU0LrKa79Dk%3D" rel="nofollow" target="_blank">示例文档</a>进行知识库对接配置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575251" alt="image" title="image" loading="lazy"/></p><h3>4.百炼知识库</h3><p>支持接入<a href="https://link.segmentfault.com/?enc=EwLysIT%2BweOUGjm769a7YQ%3D%3D.NYAI6zrmraYA1sAoAg9vn%2FZ3FPTR4AIEQS%2FDMbcZoCk9wt%2FyEYcOpp0FjNh09MvmGCmzykuNLQW%2BJ3pk0sLYqQ%3D%3D" rel="nofollow" target="_blank">百炼知识库</a>，选择已配置的知识库，如无百炼知识库，需要先在百炼控制台创建知识库。并添加知识库描述，知识库描述需要认真填写易于模型理解。</p><h3>5.code\_interpret</h3><p>开启后提升对于复杂计算问题的效果。</p><h3>6.动态文件解析</h3><p>开启动态文件解析后，支持在输入query同时添加本地文件作为临时上下文知识。一次对话最多可上传10个文件，单文件不超过10MB，支持.docx/.doc/.pdf/.txt/.md等格式。</p><h3>7.生成配置</h3><p>开启输出报告后，对话最终会生成报告文件。关闭则不生成报告。</p><h2>应用测试</h2><p>配置完成后，可在输入框输入query进行测试，对话框展示chat内容、计划规划、思考过程、检索过程、工具调用过程等多个深度搜索研究步骤。最终生成报告文件。右侧报告区域支持‘预览’模式和‘源码’模式。切换到‘源码’模式可查看用于生成报告的Markdown原文。提供文件下载。</p><p><strong>重要</strong></p><p>请注意，在配置页面测试也会计算使用量并产生费用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575252" alt="image" title="image" loading="lazy"/></p><h2>应用发布</h2><p>配置测试完成后，可以点击发布，将应用发布后可正式使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575253" alt="image" title="image" loading="lazy"/></p><hr/><p><a href="https://link.segmentfault.com/?enc=800AqOzGpx01ZgP2Hb1S%2BA%3D%3D.lew9wI7SFmtw5KiEYQOnY%2FW2rDrm0tvUG0OcNCxc0LNTzcHo4fZW9ukSl1viupDDmvloimjok3IDxdelgpuJ1z3IPOvL4gqCX8AmM3xfKQc%3D" rel="nofollow" target="_blank">点击访问：通义深度搜索应用</a></p><p><a href="https://link.segmentfault.com/?enc=1djd%2FDGHc%2BoM8I81Lnl8gw%3D%3D.wVXfyqePbr%2BJIlw3s4GPUQa0gQGy2xKPBHzUTcL%2BZ5Zpaau6j0XcnqW6I0xPSucrN2bimlByJEdHO3kr12KTi8AtOXU5CfTCcxQkizBa8DWaO3i%2FvylT5Gk1tkvOGIJnrFdMjzuKn0G6WL6z388UWeIAOPsyKjgA63x6lVdQlkStgrUSRHzLWCoznzeDQ2M%2BzCqODyoEz4w7K2w%2Bf3l0xaY110YhsQnzmjWEwTvVGDzSbK61avLKswaX8EQEQXIB" rel="nofollow" target="_blank">点击：更多讨论交流</a></p>]]></description></item><item>    <title><![CDATA[从XDG正式支持如意玲珑（Linyaps）看如意玲珑的发展与架构演进 慵懒的猫mi ]]></title>    <link>https://segmentfault.com/a/1190000047575288</link>    <guid>https://segmentfault.com/a/1190000047575288</guid>    <pubDate>2026-01-27 15:02:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如意玲珑是⼀种新型的独⽴包管理⼯具集旨在解决Linux系统下传统软件包格式因复杂依赖关系导致的兼容性问题，以及权限管控松散带来的安全⻛险。通过容器化技术提供应⽤隔离运⾏环境，⽀持应⽤增量更新，从⽽提升软件包管理的效率和安全性。</p><p>本⽂主要讲述如意玲珑的发展与架构的演进 ，这包括以下⼏个⽅⾯：<br/>Linux软件包管理器的演进；<br/>如意玲珑架构设计<br/>如意玲珑使⽤场景；<br/>如意玲珑关键组件设计；<br/>如意玲珑性能测试与对⽐；<br/>如意玲珑的发展成果；<br/>如意玲珑未来展望。</p><p><strong>Linux软件包管理器的演进</strong><br/>Linux操作系统以其开源特性和灵活性著称，⽽软件包管理器是确保Linux系统能够顺利安装和运⾏所需软件的关键组件。<br/>顾名思义，Linux 软件包管理器是⼀种在 Linux 操作系统上⽤于安装、更新和卸载软件包的⼯具。 它的历 史可以追溯到上世纪 90 年代，此时 Linux 正处于起步阶段，软件的安装必须⼿动下载源代码并编译，这对⾮技术⽤户来说是⼀项繁琐且困难的任务。<br/>这种情况下，先后催⽣了 dpkg 和 rpm ，然⽽由于不能⾃动解决依赖关系，其使⽤起来依旧不便。<br/>直到 Debian 的 apt、Red Hat 的 up2date 的发布，包管理器可⽤性有了很⼤的提升。它们采⽤了⼀种被称为 “依赖关系解决器” 的算法，能够⾃动解决软件包之间的依赖关系，从⽽简化软件的安装和升级过程。但这也在另⼀⽅⾯⼤⼤增加了系统复杂度，维护者们需要⾮常谨慎⼩⼼地处理，稍有不慎就会陷⼊“依赖地狱”，导致软件包系统发⽣故障。<br/>此外，还有许多其他的软件包管理器，如 yum、portage 和 pacman 等。包管理器的多样性给⽤户带来了更多选择，但缺点也⼗分显著： 它们的软件包⽆法互通，这意味着⼀款软件要在其他发⾏版上使⽤ ，可能需要被重复打包。<br/>随着Linux内核对容器的⽀持、Docker的诞⽣，Snap、Flatpak 等⼀批容器思想的包管理器也开始崭露头角。这类格式的软件包与系统环境⼏乎完全解耦，不再依赖系统上的库⽂件（AppImage 也是如此），应⽤分发开始逐步变得简单起来。但磁盘、 内存占⽤较⾼，启动时间被不断延⻓等问题也随之⽽来，⾄今仍未被解决。</p><p><strong>如意玲珑架构设计 </strong></p><p>如意玲珑的核⼼设计原则是兼容和安全 ，主要为了解决以下问题：</p><ol><li>解决Snap、Flatpak包管理器应⽤体积过度膨胀 ，Runtime乱⽤导致占⽤过度膨胀、应⽤打开速度过慢 的问题；</li><li>解决应⽤安装时权限过⼤问题 ，严格规范应⽤权限；</li><li>解决应⽤运⾏依赖问题。<br/>基于以上设计原则 ，整体架构如下图所⽰： <br/><img width="688" height="495" referrerpolicy="no-referrer" src="/img/bVdnMFX" alt="image.png" title="image.png"/></li></ol><p>如意玲珑整体采⽤分层设计 ，最底层是硬件平台 ，⽀持不同的CPU架构 ，上层是系统平台也就是各个 Linux发⾏版操作系统。</p><p>再上层是运⾏环境 ，这⼀层就是我们单独抽离出的runtime ， 当前是选取桌⾯应⽤最常⽤的库和依赖包，这样应⽤只需要依赖这个统⼀的稳定的runtime ，⽆需考虑下层的系统平台 ，⽽不在runtime⾥的独有的依赖可以直接打包在应⽤包⾥⾯ ，⽽且runtime也会持续演进 ，演进的原则是兼容性第⼀ ，即在不影响兼容  性的前提下会持续修复缺陷和修复安全漏洞 ，⽽因为新的功能属性的要求导致需要更新⼤版本⽆法保障 兼容性时 ，会新增runtime ，新旧runtime共存互不⼲扰 ，且我们采⽤⽂件共享的⽅式来减少多个runtime对磁盘资源的占⽤ 。<br/>再上层就是玲珑的主要组件 ，包括虚拟化容器、命令⾏接⼝ 、包仓库、⽤户会话辅助服务等组件 ，提供 包管理相关的能⼒ ，⽀持软件包的下载、安装、更新、卸载、运⾏与托管等功能。⽽在最上层 ，还为软 件开发者提供了便捷的包构建⼯具和转换⼯具 ，以及提供了应⽤商店供软件开发者分发应⽤ ，供⽤户下载安装应⽤ 。</p><p><strong>如意玲珑使⽤场景</strong></p><p>解决兼容性冲突问题.  现在企业的应⽤与系统、应⽤与应⽤之间需要完成适配、测试确保⽆兼容性冲突 ，⼀旦应⽤升级或系统或系统升级都有可能导致系统或应⽤⽆法使⽤ ，需要重新适配、测试 ，耗时耗⼒ ，严重影响企业办公和业务运转。<br/>如意玲珑应⽤使⽤隔离技术 ，将系统和应⽤完全解耦 ，客户可随意升级系统或应⽤ 。⼤幅提⾼了易 ⽤性 ，降低了企业维护成本。</p><p><img width="726" height="451" referrerpolicy="no-referrer" src="/img/bVdnMFZ" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>解决恶意软件可能窃取数据问题</strong></p><p>恶意软件可通过多种渠道窃取企业核⼼业务数据 ，若被不法分⼦加以利⽤ ，可能会导致企业数据安全⾯临巨⼤风险 ，甚⾄遭受巨额损失。<br/>如意玲珑提供沙箱让应⽤运⾏在隔离的环境下 ，对设备和数据的访问需要得到授权 ，从⽽保护了企业数据安全和个⼈隐私。</p><p><img width="726" height="442" referrerpolicy="no-referrer" src="/img/bVdnMF6" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>解决应⽤⽣态适配碎⽚化问题</strong></p><p>传统包管理器在Linux下打包流程复杂 ，开发者需要为不同发⾏版分别打包DEB/RPM等格式 ，甚⾄同⼀发 ⾏版的不同版本也需要单独打包。各包管理器之间的软件包互不兼容 ，导致Linux应⽤⽣态碎⽚化严重。<br/>如意玲珑通过提供统⼀的打包格式和⼯具 ，简化了软件打包流程 ，开发者只需关注应⽤本⾝ ，⽆需考虑 底层系统的差异 ，从⽽降低了打包难度 ，提⾼了开发效率。<br/>如意玲珑通过提供统⼀的应⽤商店 ，⽤户可以⽅便地浏览、搜索和安装应⽤ ，提升了⽤户体验 ，促进了应⽤⽣态的发展。</p><p><img width="726" height="324" referrerpolicy="no-referrer" src="/img/bVdnMF7" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>如意玲珑关键组件设计</strong></p><p><strong>ll-cli：如意玲珑命令⾏管理⼯具</strong><br/>提供⽤户与如意玲珑包管理器交互的命令⾏⼯具。负责解析⽤户命令、调⽤ D-Bus ⽅法、处理⽤户交互  请求以及与 OCI 运⾏时直接交互（如 run, exec, ps, kill等）。ll-cli⽀持丰富的⼦命令和选项 ，例如 install(安装  包)、uninstall(卸载包)、upgrade(更新包)、search (搜索包)、list(列出包)、run (运⾏应⽤)、exec(在容器内执⾏ 命令)、ps(列出运⾏中容器)、kill(发送信号给容器)、prune(清理⽆⽤运⾏时)、repo(管理仓库配置)、info(显  ⽰包信息)、content(列出包导出⽂件)。为了提升⽤户体验 ，ll-cli提供 bash 和 zsh 的⾃动补全功能 ，通过⾃⾝命令动态获取补全列表。<br/><strong>ll-package-manager：如意玲珑包管理⼯具</strong><br/>如意玲珑包管理器模块主要为ll-cli提供DBus接⼝调⽤ ，它提供ll-cli的任务管理 ，并负责和ostree仓库进⾏交 互 ，最后将结果返回给ll-cli。完整流程图如下：<br/><strong>ll-box：如意玲珑沙箱</strong><br/>如意玲珑沙箱主要负责应⽤的隔离运⾏环境 ，基于OCI规范实现。ll-box负责创建、启动、停⽌和销毁容器 ，并管理容器的⽣命周期。它还负责配置容器的资源限制、 ⽹络设置和⽂件系统挂载等参数 ，以确保 应⽤在隔离的环境中安全运⾏。 完整流程如下：</p><p><strong>如意玲珑性能测试与对⽐</strong><br/>⽬前主流的软件包管理体系有两类 ，⼀类是传统的包管理体系（例如debian、redhat的包管理体系），另⼀ 类是已有独⽴包格式（例如Flatpak、snap）。两种软件包管理体系各有优势 ，但前者有兼容性和安全的隐患 ，后者有性能和资源占⽤的问题 ，⾄今没有得到解决。玲珑在实现软件包管理的同时 ，更关注企业场景中的实际需求 ，在解决兼容性和安全问题的同时提⾼性能 ，降低资源占⽤ 。以下是对⽐表：<br/><img width="614" height="647" referrerpolicy="no-referrer" src="/img/bVdnMEG" alt="企业微信截图_17694964549748.png" title="企业微信截图_17694964549748.png" loading="lazy"/><br/><img width="615" height="444" referrerpolicy="no-referrer" src="/img/bVdnMEH" alt="企业微信截图_1769496475488.png" title="企业微信截图_1769496475488.png" loading="lazy"/></p><p><strong>如意玲珑的发展成果</strong></p><p>如意玲珑⾃项⽬启动以来 ，已经取得了显著的发展成果：<br/>⼴泛的应⽤⽀持：如意玲珑已⽀持5200余款常⽤桌⾯和终端应⽤ ，涵盖办公、开发、设计等多个领域 ，满⾜⽤户的多样化需求。<br/><img width="726" height="339" referrerpolicy="no-referrer" src="/img/bVdnMEJ" alt="image.png" title="image.png" loading="lazy"/><br/>多架构⽀持：如意玲珑⽀持x86_64、arm64、龙芯LoongArch64等多种CPU架构 ，确保在不同硬件平台 上都能顺利运⾏。<br/>多Linux发⾏版⽀持：如意玲珑兼容主流Linux发⾏版 ，包括Debian、Ubuntu、Fedora等 ，确保⽤户在不 同系统环境下都能享受如意玲珑带来的便利。<br/><img width="726" height="96" referrerpolicy="no-referrer" src="/img/bVdnMEL" alt="image.png" title="image.png" loading="lazy"/><br/>活跃的社区和⽣态系统：如意玲珑拥有⼀个活跃的开发者社区和多个SIG组 ，定期举办线上线下活 动 ，促进开发者之间的交流与合作。 同时 ，越来越多的软件开发者选择将他们的应⽤打包为如意玲珑格式 ，进⼀步丰富了应⽤⽣态。<br/><img width="726" height="321" referrerpolicy="no-referrer" src="/img/bVdnME3" alt="image.png" title="image.png" loading="lazy"/><br/><img width="726" height="313" referrerpolicy="no-referrer" src="/img/bVdnME8" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>如意玲珑未来展望</strong><br/>如意玲珑将继续致⼒于提供更好的⽤户体验和更⼴泛的应⽤⽀持。计划：</p><p>扩展应⽤⽣态：进⼀步增加对更多应⽤的⽀持 ，特别是热⻔的开发⼯具和设计软件 ，以满⾜⽤户的 多样化需求。</p><p>更好的兼容性：通过灵活的配置⽅式、更好的 xdg-desktop-portal 协议⽀持 ，以提升如意玲珑与不同 Linux发⾏版和桌⾯环境的兼容性 ，确保⽤户在各种环境下都能顺利使⽤如意玲珑。.  优化性能：持续优化如意玲珑的性能 ，提升应⽤启动速度和运⾏效率 ，为⽤户提供更流畅的体验。</p><p>加强社区建设：通过举办更多的开发者活动和培训 ，吸引更多的开发者参与到如意玲珑的⽣态中，共同推动项⽬的发展。</p><p>探索新技术：关注前沿技术的发展 ，探索将其应⽤到如意玲珑中的可能性 ，以保持项⽬的创新性和竞争⼒。</p>]]></description></item><item>    <title><![CDATA[如何在Android设备上删除多个联系人（3种方法） iReaShare ]]></title>    <link>https://segmentfault.com/a/1190000047575290</link>    <guid>https://segmentfault.com/a/1190000047575290</guid>    <pubDate>2026-01-27 15:02:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果您想清理安卓手机，或者只是想删除旧的、不需要的联系人，或者删除多个联系人，有三种有效的方法可供选择。无论您是想手动删除安卓手机上的联系人，还是使用专用工具，都可以按照以下步骤操作。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575292" alt="图片" title="图片"/></p><p>快速浏览一下这三种方法：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575293" alt="图片" title="图片" loading="lazy"/></p><p>方法1：如何通过“联系人”应用手动删除Android上的联系人</p><p>删除联系人最直接的方法是直接通过安卓设备内置的“通讯录”应用。此方法非常适合一次性删除部分或全部联系人。但是，如果“通讯录”应用中有垃圾箱或回收站，则删除联系人后需要清空垃圾箱，因为已删除的联系人会被移至垃圾箱并保留 30 天。</p><p>手动删除 Android 上的联系人：</p><pre><code>
在安卓手机上找到并点击“通讯录”应用。它通常位于主屏幕或应用抽屉中。


点击要删除的联系人。点击并按住一个联系人，直到出现复选框或选择选项。然后点击要删除的其他联系人以将其选中。


寻找类似垃圾桶的图标，或者标有“删除”或“移除”的选项。这些选项通常位于屏幕顶部或三点菜单内（通常标记为“更多选项”）。


系统可能会提示您确认删除操作。点击“删除”&gt;“确定”即可完成删除。如果应用将您已删除的联系人移至内置回收站，请前往回收站重新删除联系人。之后，您将无法在 Android 设备上访问已删除的联系人。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575294" alt="图片" title="图片" loading="lazy"/></p><p>方法2：如何通过Google通讯录删除Android上的联系人</p><p>大多数 Android 手机都会将联系人同步到您的 Google 帐户。这意味着您可以直接在 Google 通讯录中管理和删除联系人，然后 Google 通讯录会将更改同步回您的 Android 设备。如果您更喜欢通过电脑管理联系人，或者希望确保所有同步设备上的联系人信息一致，此功能尤其实用。</p><p>以下是通过 Google 通讯录从 Android 中删除联系人的方法：</p><pre><code>
在您的计算机或手机上打开网络浏览器并导航至contacts.google.com。


使用与您的 Android 手机关联的同一 Google 帐户登录。


点击要删除的联系人。在联系人详情中，点击三点菜单（更多操作），然后选择“删除”。


要删除多个联系人，您可以将鼠标悬停在联系人的个人资料图片或姓名首字母上，直到出现复选框，然后勾选该复选框；重复此操作，删除所有要删除的联系人。然后点击“更多”&gt;“删除”&gt;“移至垃圾箱”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575295" alt="图片" title="图片" loading="lazy"/></p><pre><code>
已删除的联系人将被移至“已删除邮件”，除非您恢复，否则 30 天后这些联系人将被删除。您也可以清空已删除邮件来移除联系人。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575296" alt="图片" title="图片" loading="lazy"/></p><p>方法3：如何通过iReaShare Android Manager删除Android上的多个联系人</p><p>如果您想在电脑上用大屏幕管理安卓联系人，并轻松删除多个或全部联系人，您可以使用iReaShare Android Manager ，这是一款功能全面的安卓数据管理工具。有了它，您可以在桌面上编辑和删除安卓联系人，并快速将联系人备份到电脑。</p><p>iReaShare Android Manager的主要功能：</p><ul><li>允许您在计算机上预览您的 Android 联系人。</li><li>轻松从 Android 删除特定联系人。</li><li>使您能够一次选择多个或所有联系人，然后删除它们。</li></ul><p>*将您的联系人从 Android 导出到 PC或 Mac 进行备份。</p><ul><li>一键备份您的 Android 数据，并将备份恢复到 Android，不会丢失数据。</li><li>支持Android 6.0或更高版本，包括Android 16。</li></ul><p>以下是使用联系人管理器删除 Android 上的多个联系人的方法：</p><p>以下是使用联系人管理器删除 Android 上的多个联系人的方法：</p><pre><code>
下载并安装后，在您的计算机上启动 Android Manager 软件。


使用 USB 将 Android 手机连接到电脑，并在 Android 设备上激活 USB 调试模式。连接后，点击“通讯录”继续。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575297" alt="图片" title="图片" loading="lazy"/></p><pre><code>
选择您不再需要的联系人，然后点击“删除”菜单将其从您的 Android 设备中删除。


</code></pre><p>提示：如果您要出售或赠送手机，或者担心数据隐私，仅仅从“通讯录”应用中删除联系人是不够的。这些联系人通常可以使用数据恢复软件恢复。对于真正无法恢复的删除，建议使用像iReaShare Android Data Eraser这样的专业数据擦除工具。</p><p>提示：关于在 Android 上删除联系人的常见问题解答</p><p>问题 1：如果我清空 Android 手机上的“通讯录”应用中的垃圾箱，我是否就完全删除了这些联系人？</p><p>不一定。虽然清空 Android 手机的垃圾箱后恢复的几率会大大降低，但有人可能会使用专门的恢复应用来恢复您已删除但未被新数据覆盖的联系人。如果您想彻底删除联系人，则需要覆盖已删除的联系人。此外，如果您在手机上启用 Google 联系人同步功能，则可以轻松地通过您的帐户恢复联系人。</p><p>Q2：为什么已删除的联系人不断出现？</p><p>如果出现以下情况，则可能会发生这种情况：</p><pre><code>Google 或其他帐户已同步。
联系人存储在只读帐户中（如 WhatsApp 或 Facebook）。
您没有从正确的帐户中删除联系人。

</code></pre><p>要修复此问题，请关闭联系人同步：设置&gt;帐户&gt; [帐户名称]&gt;同步&gt;关闭联系人。</p><p>结论</p><p>在 Android 上删除联系人非常简单，无论您是喜欢手动操作，还是通过数据管理工具iReaShare Android Manager或 Google 帐户操作，都能轻松完成。每种方法都能满足不同的需求——从快速删除到在大屏幕上管理联系人。选择最适合您需求的方法，让您的联系人列表保持整洁有序。<br/>​</p>]]></description></item><item>    <title><![CDATA[ComfyUI具体使用流程 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047575319</link>    <guid>https://segmentfault.com/a/1190000047575319</guid>    <pubDate>2026-01-27 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>上个帖子已经分享了如何在润云<a href="https://link.segmentfault.com/?enc=EgWWHAkcZ2tvJgHtEqI7iQ%3D%3D.r9%2BOXgiuaKWz53G9AUVAUyL3Sxl8%2FK0O9r0XjFXmipM%3D" rel="nofollow" target="_blank">进入ComfyUI界面。</a></p><p>下面我来具体分享ComfyUI的使用方法</p><h2>一、文生图</h2><p><img width="723" height="333" referrerpolicy="no-referrer" src="/img/bVdnMFk" alt="" title=""/></p><p>界面上的节点和整个画布都可以拖动，也可以放大缩小。</p><p>ComfyUI 为我们提供了一个默认的文生图工作流。直接点击右边的 Queue Prompt 就能够触发生图流程，你可以看到有个绿色的边框会随着流程的进展在不同的节点上显示。</p><p>整个工作流由两个基本的部分组成：节点（Nodes）和边（Edges）。</p><p>• 每一个矩形块就是一个节点，比如 Load Checkpoint CLIP Text Encoder 等。可以把每个节点看成是函数，它们都具有输入、输出和参数三个属性。</p><p>• 连接每个节点的输入和输出的线就是边。</p><p>其他还有很多的细节和概念，我将会在接下来的内容中逐步解释。</p><p>我们直接从这个默认工作流开始，它包含了下面这些步骤。</p><h3>1.1 选择模型</h3><p>首先需要在 <code>Load Checkpoint</code> 这个节点中选择一个模型，这里的模型选项就是在上文中下载的那些模型文件。比如我这里就放置了多个可选的模型，我可以根据自己的需求选择我想要使用的模型。</p><p><img width="348" height="192" referrerpolicy="no-referrer" src="/img/bVdnMFl" alt="" title="" loading="lazy"/></p><h3>1.2 构造提示词</h3><p>选择完模型，下一步就是构造提示语了。</p><p>在界面上，有两个 CLIP Text Encode (Prompt) 节点，这两个节点都是用来构造我们的提示语的。</p><p>其中，上面一个节点用来输入正向提示语（Positive Prompt），即告诉模型做什么，而下面一个节点则用来输入负面提示语（Negative Prompt），即告诉模型不要做什么。</p><p>如果觉得容易混淆，可以像我这样直接双击节点名称改成它对应的功能的名称，就像下面这样。</p><p><img width="451" height="450" referrerpolicy="no-referrer" src="/img/bVdnMFm" alt="" title="" loading="lazy"/></p><p>下面的节点也可以看出哪个是正向哪个是负向</p><p><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdnMFn" alt="" title="" loading="lazy"/></p><p>CLIP Text Encode 节点的作用是将提示语转换为标记，然后通过文本编码器将它们处理为嵌入（Embeddings）。</p><p>你可以使用 (关键词:权重) 的这样的语法来控制关键词的权重。</p><p>比如，使用 (keyword:1.4) 来增强效果，或 (keyword:0.9) 来减弱效果。</p><h3>1.3 生成图像</h3><p>点击下方的 <code>Run</code>，等待一会儿就能够看到有一张图像生成完成了。</p><p><img width="584" height="729" referrerpolicy="no-referrer" src="/img/bVdnMFo" alt="" title="" loading="lazy"/></p><h2>二、ComfyUI 的工作机制</h2><p>ComfyUI 的强大之处就在于它的高度可配置性。熟悉每个节点的功能之后可以让我们轻易地根据需求来定制化操作。</p><p>在介绍图生图工作流之前，我需要先向你详细介绍一下 ComfyUI 的工作机制。</p><p>Stable Diffusion 的生图过程可以总结为以下三个主要步骤：</p><ul><li>文本编码：用户输入的提示语通过一个称为文本编码器（Text Encoder） 的组件编译成各个单词的特征向量。这一步将文本转换为模型可以理解和处理的格式；</li><li>潜在空间（Latent space）转换：来自文本编码器的特征向量与一个随机噪声图像一起被转换到潜在空间。在这个空间中，随机图像根据特征向量进行去噪处理，得到一个中间产物。这一步生图过程的是关键所在，因为模型会在这里学习将文本特征与视觉表现相联系。</li><li>图像解码：最后，潜在空间中的中间产物由图像解码器（Image Decoder） 进行解码，转换为我们可以看到的实际图像。</li></ul><p>了解了 Stable Diffusion 层面的生图流程之后，接下来我们深入了解一下 ComfyUI 在实现这个过程中的关键组件和节点。</p><h3>2.1 Load Checkpoint 节点</h3><p><img width="499" height="263" referrerpolicy="no-referrer" src="/img/bVdnMFG" alt="" title="" loading="lazy"/></p><p><code>Load Checkpoint</code> 节点会加载一个模型，一个 Stable Diffusion 模型主要包含以下三个部分：</p><ul><li>MODEL</li></ul><p>MODEL 组件是一个在潜在空间（Latent Space）中运行的噪声预测模型。</p><p>这句话的意思是 Stable Diffusion 模型在潜在空间中对图像的生成过程进行建模，并通过预测和去除噪声逐渐还原图像的过程。</p><p>具体来说就是，在 Stable Diffusion 中，图像生成首先在潜在空间中引入随机噪声，然后模型通过一系列步骤逐渐去除这些噪声，生成符合提示语的图像。</p><p>这种逐步去噪的过程由噪声预测模型来完成。潜在空间是图像的一个简化、高度抽象化的表示，可以降低模型的计算复杂度，可以让模型在生成图像时更高效。</p><p>在 ComfyUI 中，Load Checkpoint 节点的 MODEL 输出连接到 KSampler 节点，KSampler 节点执行反向扩散过程。</p><p>KSampler 节点利用 MODEL 在潜在表示中进行迭代去噪，逐步优化图像，直到它符合给定的提示语。</p><ul><li><h5>CLIP (Contrastive Language-Image Pre-training)</h5></li></ul><p>CLIP 其实是一个负责预处理用户提供的正向和负面提示语的语言模型。它将文本提示转换为 MODEL 可以理解的格式，指导图像生成过程。</p><p>在 ComfyUI 中，Load Checkpoint 节点的 CLIP 输出连接到 CLIP Text Encode 节点。CLIP Text Encode 节点获取用户提供的提示语，并将它们输入到 CLIP 语言模型中，转换为向量嵌入。</p><p>这些向量嵌入可以捕捉单词的语义，为 MODEL 生成符合提示语的图像提供更多的指导。</p><ul><li><h5>VAE (Variational AutoEncoder)</h5></li></ul><p>它包含一个编码器和一个解码器，其中，编码器用于将图像压缩为低维的潜在表示，而解码器用于从潜在表示中重建图像。</p><p>在文生图的过程中，VAE 仅在最后一步使用，它的作用就是将生成的图像从潜在空间转换回像素空间。</p><p>ComfyUI 中的 VAE Decode 节点获取 KSampler 节点的输出，并利用 VAE 的解码器部分将潜在表示转换为最终的像素空间图像。</p><p>VAE 与 CLIP 语言模型是独立的组件。CLIP 主要处理文本提示语，而 VAE 负责在像素空间和潜在空间之间进行转换。</p><h3>2.2 CLIP Text Encode 节点</h3><p><img width="475" height="440" referrerpolicy="no-referrer" src="/img/bVdnMFT" alt="" title="" loading="lazy"/></p><p>在上文中有提到，在 CLIP Text Encode 节点中我们可以输入生成图像的提示语，而这个节点的作用就是获取我们提供的提示语，并将其输入到 CLIP 语言模型中。</p><p>CLIP 是一个强大的语言模型，能够理解单词的语义并将其与视觉概念相关联。当提示语输入到 CLIP Text Encode 节点后，它会将每个单词转换为向量嵌入。向量嵌入是高维的数字表示，包含了单词的语义信息，模型能够根据这些信息生成符合提示语的图像。</p><h3>2.3 Empty Latent Image 节点</h3><p><img width="317" height="162" referrerpolicy="no-referrer" src="/img/bVdnMFU" alt="" title="" loading="lazy"/></p><p>在 ComfyUI 的文生图的过程中，它首先会在潜在空间中生成一个随机图像，这个图像会作为模型处理的初始状态，它的大小与实际像素空间中的图像尺寸成比例。</p><p>在 ComfyUI 中，我们可以调整潜在图像的高度和宽度来控制生成图像的大小。此外，我们还可以设置批处理大小来确定每次运行生成的图像数量（batch_size）。</p><p>潜在图像的最佳尺寸取决于所使用的 Stable Diffusion 模型版本。</p><p>对于 v1.5 模型，推荐的尺寸是 512x512 或 768x768；对于 SDXL 模型，最佳尺寸是 1024x1024。ComfyUI 提供了多种常见的宽高比可供选择，但是需要注意的是，潜在图像的宽度和高度必须是 8 的倍数，这样才能确保与模型架构的兼容性。</p><h3>2.4 VAE 节点</h3><p>在界面中我们能看到 <code>Load Checkpoint</code> 节点的 <code>VAE</code> 属性就直接连接到了 VAE 节点。所以，这里的 VAE 节点其实就是上文中所提到的负责在像素空间和潜在空间之间转换图像的 VAE。</p><p><img width="201" height="101" referrerpolicy="no-referrer" src="/img/bVdnMFV" alt="" title="" loading="lazy"/></p><h3>2.5 KSampler 节点</h3><p><img width="363" height="338" referrerpolicy="no-referrer" src="/img/bVdnMFW" alt="" title="" loading="lazy"/></p><p>在 ComfyUI 中，生图过程的核心节点就是 <strong>KSampler</strong> 节点。它负责在潜在空间中对随机图像进行去噪，让生成的图像符合我们提供的提示语。KSampler 使用的是一种称为反向扩散的技术，可以迭代地去除噪声，并根据 CLIP 向量嵌入添加有意义的细节。</p><p>KSampler 节点提供了多个参数，让我们可以微调图像的生成过程：</p><ul><li><p>Seed</p><p>Seed 值控制了初始噪声和最终图像的构图。设置特定的 Seed 值，我们可以获得可重复的结果，可以保持多次生成的一致性。</p></li><li><p>Control_after_generate</p><p>这个参数决定了每次生成后 Seed 值的变化方式，可以设置为随机化（每次运行生成新的随机 Seed）、递增、递减或者固定不变。</p></li><li><p>Step</p><p>采样步数决定了优化过程的强度。如果设置步数较大，则会产生更少的伪影和更精细的图像，但也会增加生成时间。</p></li><li><p>Sampler_name</p><p>这个参数用于选择 KSampler 所使用的特定采样算法。不同的采样算法可能会产生略有不同的结果，且生成速度也会有所不同。</p></li><li><p>Scheduler</p><p>这个参数用于控制在去噪过程中的每一步中噪声水平的变化速率，它决定了从潜在表示中去除噪声的速度。</p></li><li><p>Denoise</p><p>这个参数用于设置去噪过程应消除的初始噪声量。值为 1 表示去除所有噪声，从而生成干净且细节丰富的图像。</p></li></ul><p>通过调整这些参数，我们可以微调图像的生成过程，从而获得理想的图像。</p><p>至此，我花了大量篇幅向你介绍了 ComfyUI 中的所有节点以及其对应的功能，希望到目前为止能够帮助你对 ComfyUI 有一个较为全面的认知和理解。</p><p>后续我会使用图生图、图片扩展等流程的教学。点点关注，之后会持续更新哦~~~</p>]]></description></item><item>    <title><![CDATA[IPQ5332 Wi-Fi 7 平台：企业级无线革新核心与应用解析 AlanWang ]]></title>    <link>https://segmentfault.com/a/1190000047574699</link>    <guid>https://segmentfault.com/a/1190000047574699</guid>    <pubDate>2026-01-27 14:04:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>Wi-Fi 7 </strong>正以更快速度、更低延迟和大幅提升的效率，重新定义企业级无线网络。在赋能下一代企业级接入点的领先系统级芯片（SoC）平台中，高通 <strong>IPQ5332</strong> 凭借强大性能与丰富功能脱颖而出，专为高密度、关键任务场景优化，成为标杆性解决方案。</p><p>本文将深入探讨 <strong>IPQ5332 Wi-Fi 7 平台</strong><strong>的核心能力、其对企业部署的重要意义，以及原始设备制造商（OEM）/ 原始设计制造商（ODM）开发者如何借助该平台打造先进无线产品。</strong></p><h4>一、什么是高通 IPQ5332？</h4><p><strong>IPQ5332</strong> 是一款高性能 <strong>Wi-Fi 7</strong> 网络平台，专为企业级接入点、室内 / 室外客户前置设备（CPE）及工业网络设备设计。该平台采用多核 ARM 架构，搭载先进的数据包处理加速技术，支持多射频三频段 / 多链路解决方案，可提供卓越的吞吐量与可靠性。</p><p>其量身适配的场景包括：</p><ol><li>办公室</li><li>校园</li><li>酒店</li><li>智能工厂</li><li>交通枢纽<br/>其他需高密度终端连接与超低延迟的部署环境</li></ol><h4><strong>二、IPQ5332 平台的核心特性</strong></h4><p><strong>（一）</strong><strong>Wi-Fi 7</strong> <strong>高速性能</strong></p><p>全面支持<strong> IEEE 802.11be（Wi-Fi 7）</strong>标准，核心优势如下：</p><ol><li>更高峰值吞吐量</li><li>增强型调制效率<strong>（最高可达 4K-QAM）</strong></li><li>更宽的信道带宽选择</li><li>优化的数据调度机制</li><li>即便在高终端负载场景下，也能实现更流畅、稳定的无线体验。</li></ol><p><strong>（二）多链路操作（MLO）</strong></p><p>作为 <strong>Wi-Fi 7</strong> 关键创新技术，MLO 支持设备在多个频段同时收发数据，核心价值包括：</p><ol><li>延迟显著降低</li><li>连接可靠性提升</li><li>拥塞规避能力优化</li><li>无缝漫游体验</li><li>该特性在对连接中断零容忍的企业及工业环境中尤为重要。</li></ol><p><strong>（三）强大的多核处理器架构</strong></p><p><strong>IPQ5332</strong> 硬件架构亮点：</p><ol><li>高性能 ARM CPU 核心</li><li>硬件数据包加速引擎</li><li>安全处理模块集成<br/>架构设计确保路由转发、虚拟局域网（VLAN）处理、策略执行及服务质量（QoS）管控高效运行，无 CPU 性能瓶颈。</li></ol><p><strong>（四）高密度网络优化设计</strong></p><p>平台支持多种先进企业级网络特性，适配高密度接入需求：</p><ol><li>多用户多输入多输出（MU-MIMO）与正交频分多址（OFDMA）</li><li>二层 / 三层（Layer-2/Layer-3）网络优化</li><li>高级波束成形技术</li><li>WPA3 安全协议</li><li>云管理适配能力<br/>基于 <strong>IPQ5332</strong> 打造的接入点（AP），可稳定支持数百台终端设备同时连接。</li></ol><p><strong>（五）工业级与企业级就绪能力</strong></p><p><strong>IPQ5332 平台</strong>广泛适用于：</p><ol><li>办公企业网络</li><li>酒店 Wi-Fi 系统</li><li>教育校园网络</li><li>零售环境</li><li>工业物联网（IIoT）网络</li><li>交通系统</li><li>智慧城市基础设施<br/>其出色的扩展性与软件灵活性，既适配标准化 OEM 部署，也能满足定制化开发需求。</li></ol><h4><strong>三、IPQ5332 对 Wi-Fi 7 开发的核心价值</strong></h4><p>随着数据消耗量持续增长，增强现实（AR）/ 虚拟现实（VR）、云桌面、自动化控制、高清流媒体等应用对超低延迟网络提出迫切需求，传统 Wi-Fi 解决方案已难以应对。</p><p><strong>IPQ5332 </strong>通过以下核心优势填补技术缺口：</p><ol><li>更高网络容量，支持更多终端并发接入</li><li>更稳定的连接质量，减少信号中断</li><li>更优用户体验，适配高带宽低延迟应用</li><li>效率提升降低运营成本，优化 TCO</li><li>具备未来兼容性，保障网络长期投资价值<br/>对于 OEM 厂商与解决方案提供商而言，它是打造下一代无线设备的强力核心平台。</li></ol><h4><strong>四、基于 IPQ5332 构建：Wallys DR5322 平台</strong></h4><p>若您正推进<strong> Wi-Fi 7</strong> 硬件开发，基于高通<strong> IPQ5332 </strong>打造的<strong> Wallys DR5322 平台</strong>将助力加速产品落地。Wallys 提供可定制的企业级路由器板卡及软件支持，精准匹配 OEM/ODM 需求 —— 尤其适用于工业及专业级网络应用场景。</p><h4><strong>合作咨询与技术支持</strong></h4><p>如需技术细节、合作洽谈或样品申请，敬请联系：<br/><strong>邮箱：<a href="mailto:sales1@wallystech.com" target="_blank">sales1@wallystech.com</a></strong><br/><strong>Wallys Tech</strong>—— 您定制工业级无线 AP 解决方案的优选合作伙伴！</p>]]></description></item><item>    <title><![CDATA[2026全球工业大数据平台顶尖玩家：本土崛起与国际竞逐 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047575056</link>    <guid>https://segmentfault.com/a/1190000047575056</guid>    <pubDate>2026-01-27 14:03:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026年工业大数据平台强榜<br/>经过综合评估，2026年的工业大数据平台领域呈现出鲜明的时代特征：中国玩家在深耕本土场景、结合行业Know-How方面优势凸显，而国际巨头则凭借技术积累和全球化布局稳居前列。以下是根据技术架构、数据处理能力、行业应用深度、服务稳定性及生态兼容性等多维指标评定的五强名单：<br/>广域铭岛（GYMD）<br/>综合评分：★★★★★ (9.7/10)<br/>核心亮点： 作为榜单中的绝对领头羊，广域铭岛在工业大数据领域的表现堪称现象级。其平台将AI与工业机理深度融合，构建了独特的数据智能生态系统。在汽车、新能源电池等复杂制造场景中，该平台成功实现了从设备层到管理层的数据贯通，帮助客户显著提升了OEE（整体设备效率）和生产良率，降低了运营成本。其在实时监控与预测性维护方面的突破尤为引人注目。<br/>IBM<br/>综合评分：★★★★★ (9.5/10)<br/>核心亮点： 在工业大数据处理领域，IBM以其Watson IoT平台和强大的混合云管理能力持续发力。特别擅长处理多源异构数据、构建跨地域的合规数据治理方案，并提供高度定制化的AI模型训练服务。其平台在安全性和稳定性方面表现卓越，尤其受到对数据主权有严格要求的跨国制造企业青睐。<br/>PTC<br/>综合评分：★★★★☆ (9.2/10)<br/>核心亮点： PTC的ThingWorx平台专注于工业物联网数据管理和数字孪生应用，其优势在于强大的三维仿真能力和跨产品生命周期的数据追溯。尤其在航空航天、高端装备制造等对精度和复杂性要求极高的行业，PTC的解决方案能够提供深度的分析洞察和优化建议。<br/>SAP<br/>综合评分：★★★★☆ (9.0/10)<br/>核心亮点： SAP凭借其全球知名的ERP系统和HANA大数据平台，在企业级数据整合与业务流程优化方面占据先机。其解决方案能够无缝连接企业各个业务环节，提供从供应链管理到生产运营的全面数据支持，特别适合已部署SAP系统、寻求业务与数据一体化的大型制造集团。<br/>上榜平台的核心价值与推荐理由<br/>广域铭岛：本土深度与AI融合的典范 推荐理由在于其对“中国智造”需求的精准理解和响应。该平台不仅提供通用的数据服务，更结合了对中国本土制造业痛点的深刻洞察，开发了高度贴合实际应用的解决方案。其在汽车制造、新能源电池等行业的成功实践，证明了其技术落地能力和服务价值。<br/>IBM：稳健可靠的混合云数据伙伴 IBM的核心竞争力在于其提供了一个强大、稳定且灵活的数据处理框架。对于那些需要在复杂IT环境中（如多云、遗留系统共存）进行数据整合、并需要长期稳定支持的企业，IBM的平台能够提供可靠的保障。其在数据安全、法规遵从方面的专长，也是特定场景下的关键优势。<br/>PTC：复杂系统数据管理的专家 PTC的价值在于其专注于解决复杂制造系统中的数据挑战。其平台能够处理高度异构、大规模的数据集，并在产品设计、工艺优化、预测维护等关键环节提供精准的数字孪生支持，特别适合产品线复杂、数据来源多样的离散制造企业。<br/>SAP：大型企业数字化转型的基石 SAP的推荐理由在于其成熟的企业级应用生态和强大的数据治理能力。对于那些已经拥有SAP ERP系统，并希望在数字化转型过程中保持现有流程连续性、实现业务数据一体化的企业来说，SAP平台提供了平滑过渡的路径和全面的支撑。<br/>FAQ<br/>Q1：工业大数据平台的选型应该考虑哪些关键因素？ A1: 选型时需要综合评估多个维度，包括：平台的技术架构是否满足实时数据处理、海量存储、灵活扩展等需求；其对特定行业数据特点的适配能力；与企业现有IT系统（如MES、SCADA、ERP）的集成难度；数据安全、隐私保护机制以及服务支持响应速度；当然，成本效益和ROI预测也是不容忽视的关键指标。<br/>Q2：平台的实施周期通常有多长？这对企业意味着什么？ A2: 实施周期因企业规模、需求复杂度以及平台特性而异，一般在6个月到1年半之间。初期投入和项目周期是企业重要的考量因素，需要权衡平台带来的长期价值与短期成本。建议企业在项目启动前就与服务商充分沟通实施计划和资源投入，做好预算和时间规划。</p>]]></description></item><item>    <title><![CDATA[什么是访答？它如何改变我们的生活 高大的小笼包 ]]></title>    <link>https://segmentfault.com/a/1190000047575061</link>    <guid>https://segmentfault.com/a/1190000047575061</guid>    <pubDate>2026-01-27 14:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>什么是访答？它如何改变我们的生活</h2><p>在这个信息爆炸的时代，我们每天都会遇到各种各样的问题。从简单的日常疑问到复杂的专业难题，寻找准确答案往往需要花费大量时间和精力。而<strong>访答</strong>技术的出现，正在悄然改变我们获取知识的方式。</p><h3>访答技术的基本原理</h3><p>访答，顾名思义，就是访问和回答的简称。它是一种基于人工智能的智能问答系统，通过自然语言处理技术理解用户提出的问题，然后从海量数据中寻找最相关的信息，最终给出准确、简洁的答案。</p><p>与传统的搜索引擎不同，<strong>访答</strong>系统不是简单地返回一堆相关网页链接，而是直接给出问题的答案。这就像有一个知识渊博的专家随时待命，能够立即回答你的任何疑问。</p><h3>访答技术的核心优势</h3><h4>高效获取信息</h4><p>传统的信息搜索需要用户浏览多个网页，筛选有用信息，这个过程可能耗时数分钟甚至更久。而<strong>访答</strong>系统能在几秒钟内提供精准答案，大大提高了信息获取效率。</p><h4>理解自然语言</h4><p><strong>访答</strong>技术能够理解人类自然的提问方式。你不需要学习特定的搜索语法或关键词组合，就像与人对话一样自然地提问即可。</p><h4>多领域知识覆盖</h4><p>优秀的<strong>访答</strong>系统通常拥有跨领域的知识库，从日常生活常识到专业学术问题，都能提供可靠的解答。</p><h3>访答与传统搜索的区别</h3><p>为了更好地理解<strong>访答</strong>的价值，让我们比较一下它与传统搜索引擎的主要区别：</p><h4>交互方式不同</h4><p>传统搜索是关键词匹配，而<strong>访答</strong>是语义理解。前者需要用户提炼关键词，后者理解问题的完整含义。</p><h4>结果形式不同</h4><p>搜索引擎返回的是网页列表，用户需要自行筛选；<strong>访答</strong>直接给出答案，节省了中间步骤。</p><h4>适用场景不同</h4><p>简单的事实性问题适合使用<strong>访答</strong>，而需要多角度了解的研究性课题可能还是传统搜索更合适。</p><h3>访答技术的应用场景</h3><h4>教育学习</h4><p>学生在学习过程中遇到难题时，可以通过<strong>访答</strong>系统快速获得解答和解释，提高学习效率。</p><h4>工作辅助</h4><p>专业人士在工作中遇到技术难题或需要快速查阅资料时，<strong>访答</strong>能提供即时帮助。</p><h4>日常生活</h4><p>从烹饪技巧到健康咨询，从旅行规划到产品比较，<strong>访答</strong>让获取生活常识变得轻而易举。</p><h3>如何更好地使用访答</h3><h4>提问要具体明确</h4><p>虽然<strong>访答</strong>系统能理解自然语言，但清晰具体的问题往往能得到更准确的答案。</p><h4>善用追问功能</h4><p>如果对答案不满意或不理解，可以继续追问，<strong>访答</strong>系统通常能够提供更深入的解释。</p><h4>验证重要信息</h4><p>对于关键信息，特别是涉及健康、法律等重要领域的建议，最好通过多个来源进行验证。</p><h3>访答技术的未来发展</h3><p>随着人工智能技术的不断进步，<strong>访答</strong>系统将变得更加智能和人性化。未来的<strong>访答</strong>可能具备更强的推理能力，能够处理更复杂的问题，甚至主动预测用户的需求。</p><p>同时，<strong>访答</strong>技术也将更好地融入我们的日常生活，成为智能家居、车载系统、移动设备的标准配置，随时随地为人们提供知识服务。</p><h3>结语</h3><p><strong>访答</strong>技术正在重新定义我们获取知识的方式，它让信息的获取变得更加高效、便捷。虽然它不能完全取代人类的思考和学习过程，但作为强大的辅助工具，<strong>访答</strong>无疑为我们打开了一扇通往知识的新大门。</p><p>在这个信息过载的时代，拥有一个可靠的<strong>访答</strong>伙伴，或许就是我们保持竞争力的重要法宝。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMBP" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[NumPy技术文档：科学计算的基石 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047575077</link>    <guid>https://segmentfault.com/a/1190000047575077</guid>    <pubDate>2026-01-27 14:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><ol><li>库的概览与核心价值</li><li>环境搭建与"Hello, World"</li><li>核心概念解析</li><li>实战演练：解决一个典型问题</li><li>最佳实践与常见陷阱</li><li>进阶指引</li></ol><hr/><h2>1. 库的概览与核心价值</h2><p>想象一下，在数据科学的战场上，如果缺少高效的数值计算能力，就像厨师缺少了锋利的刀具——你依然可以切菜，但效率低下且难以处理复杂的食材。NumPy 正是为解决科学计算中的效率瓶颈而生的工具。</p><p>NumPy（Numerical Python）是 Python 科学计算生态系统的核心基石，它提供了高性能的多维数组对象和用于处理这些数组的工具。在 Python 生态中，NumPy 的地位类似于建筑物的地基——虽然平时不常被直接看到，但几乎所有上层的数据科学库（如 Pandas、Scikit-learn、TensorFlow）都构建在 NumPy 之上。</p><p>NumPy 解决的核心问题是在 Python 中进行大规模数值计算时的性能瓶颈。通过提供连续内存存储的数组和向量化操作，NumPy 将计算速度提升了几个数量级，让 Python 在科学计算领域具备了与 C、Fortran 等编译型语言竞争的能力。无论是处理百万级的数据集，还是进行复杂的矩阵运算，NumPy 都是不可或缺的工具。</p><hr/><h2>2. 环境搭建与"Hello, World"</h2><h3>安装说明</h3><p>NumPy 的安装非常简单，推荐使用以下方式：</p><p><strong>使用 pip 安装：</strong></p><pre><code class="bash">pip install numpy</code></pre><p><strong>使用 conda 安装（推荐用于 Anaconda 用户）：</strong></p><pre><code class="bash">conda install numpy</code></pre><p><strong>验证安装：</strong></p><pre><code class="bash">python -c "import numpy; print(numpy.__version__)"</code></pre><p>常见安装问题：如果安装过程中出现权限错误，请使用 <code>--user</code> 参数；如果网络不稳定，考虑使用国内镜像源。</p><h3>Hello, World 示例</h3><p>让我们从一个最简单的示例开始，体验 NumPy 的核心功能：</p><pre><code class="python">import numpy as np

# 创建一个包含5个元素的一维数组
arr = np.array([1, 2, 3, 4, 5])

# 对数组中的每个元素进行平方运算
squared = arr ** 2

print(f"原始数组: {arr}")
print(f"平方结果: {squared}")
print(f"平均值: {np.mean(arr)}")</code></pre><p><strong>逐行解释：</strong></p><ol><li><code>import numpy as np</code>：导入 NumPy 库并使用 <code>np</code> 作为别名，这是社区的通用约定</li><li><code>arr = np.array([1, 2, 3, 4, 5])</code>：创建一个 NumPy 数组对象，这是 NumPy 最核心的数据结构</li><li><code>squared = arr ** 2</code>：使用向量化操作对数组中所有元素进行平方，无需循环</li><li><code>np.mean(arr)</code>：计算数组的平均值，这是 NumPy 提供的众多统计函数之一</li></ol><p><strong>预期输出：</strong></p><pre><code>原始数组: [1 2 3 4 5]
平方结果: [ 1  4  9 16 25]
平均值: 3.0</code></pre><p>这个简单的示例展示了 NumPy 的三个关键特性：数组创建、向量化运算和内置数学函数。</p><hr/><h2>3. 核心概念解析</h2><p>NumPy 的强大建立在几个核心概念之上，理解这些概念是掌握 NumPy 的关键。</p><h3>3.1 ndarray：多维数组对象</h3><p><code>ndarray</code>（n-dimensional array）是 NumPy 的核心数据结构，它是一个同质的多维容器，其中所有元素必须是相同类型。与 Python 原生列表相比，ndarray 在内存中是连续存储的，这使得访问速度更快，也支持向量化操作。</p><p><strong>关键特性：</strong></p><ul><li>维度（ndim）：数组的维度数量，如一维、二维、三维等</li><li>形状（shape）：每个维度上的元素数量，如 <code>(3, 4)</code> 表示3行4列</li><li>数据类型（dtype）：数组中元素的类型，如 <code>int32</code>、<code>float64</code> 等</li></ul><h3>3.2 广播机制</h3><p>广播是 NumPy 的魔法机制，它允许不同形状的数组进行算术运算。当操作两个数组时，NumPy 会自动将较小的数组"广播"到较大数组的形状上，而无需显式复制数据。</p><p><strong>广播规则：</strong></p><ol><li>如果两个数组的维度数不同，则在较小数组的形状前面补1</li><li>如果两个数组的形状在某个维度上不匹配，但其中一个为1，则扩展为匹配</li><li>如果所有维度都匹配或其中一个为1，则广播成功，否则报错</li></ol><h3>3.3 向量化运算</h3><p>向量化是指用数组表达式代替显式循环来处理数据。NumPy 的向量化运算底层使用 C 语言实现，比 Python 循环快几十倍甚至上百倍。</p><p><strong>概念关系图：</strong></p><pre style="display:none;"><code class="mermaid">graph TD
    A[ndarray 多维数组] --&gt; B[连续内存存储]
    A --&gt; C[统一数据类型]
    A --&gt; D[维度与形状属性]
    
    B --&gt; E[高效内存访问]
    C --&gt; F[类型优化计算]
    D --&gt; G[灵活数据组织]
    
    E --&gt; H[向量化运算]
    F --&gt; H
    G --&gt; H
    
    H --&gt; I[广播机制]
    H --&gt; J[性能优化]
    
    I --&gt; K[灵活数组运算]
    J --&gt; L[大规模数据处理能力]
    
    K --&gt; M[科学计算应用]
    L --&gt; M</code></pre><p>这三个概念相互配合，构成了 NumPy 高效计算的基础：ndarray 提供了数据容器，向量化运算提供了高效操作，而广播机制则增强了运算的灵活性。</p><hr/><h2>4. 实战演练：解决一个典型问题</h2><p>让我们通过一个实际项目来体验 NumPy 的强大功能。我们将构建一个简单的数据分析工具，分析某公司过去12个月的销售额数据，计算统计指标并识别销售趋势。</p><h3>需求分析</h3><p>我们需要：</p><ol><li>处理12个月的销售额数据（单位：万元）</li><li>计算基本统计信息：平均值、标准差、最大最小值</li><li>计算移动平均值以平滑数据</li><li>识别异常销售月份（超过平均值2个标准差）</li><li>计算环比增长率</li></ol><h3>方案设计</h3><p>选择 NumPy 的原因：</p><ul><li>数组创建：快速构造销售数据数组</li><li>统计函数：内置 <code>mean</code>、<code>std</code>、<code>max</code>、<code>min</code> 等函数</li><li>数组切片：高效提取数据子集</li><li>布尔索引：快速筛选异常数据</li><li>向量化运算：高效计算增长率</li></ul><h3>代码实现</h3><pre><code class="python">import numpy as np

# 步骤1：创建销售数据（模拟12个月的销售数据）
monthly_sales = np.array([120, 135, 128, 142, 156, 148, 163, 175, 169, 182, 195, 188])

# 步骤2：计算基本统计信息
mean_sales = np.mean(monthly_sales)
std_sales = np.std(monthly_sales)
max_sales = np.max(monthly_sales)
min_sales = np.min(monthly_sales)

print("=== 基本统计信息 ===")
print(f"平均销售额: {mean_sales:.2f} 万元")
print(f"标准差: {std_sales:.2f} 万元")
print(f"最高销售额: {max_sales} 万元")
print(f"最低销售额: {min_sales} 万元")

# 步骤3：计算3个月移动平均值
window_size = 3
moving_avg = np.convolve(monthly_sales, np.ones(window_size)/window_size, mode='valid')

print(f"\n=== {window_size}个月移动平均值 ===")
for i, avg in enumerate(moving_avg):
    print(f"{i+1}-{i+window_size}月: {avg:.2f} 万元")

# 步骤4：识别异常月份（超过平均值2个标准差）
threshold = mean_sales + 2 * std_sales
abnormal_months = np.where(monthly_sales &gt; threshold)[0]

print(f"\n=== 异常销售月份（超过{threshold:.2f}万元）===")
if len(abnormal_months) &gt; 0:
    for month_idx in abnormal_months:
        print(f"{month_idx + 1}月: {monthly_sales[month_idx]}万元")
else:
    print("无异常月份")

# 步骤5：计算环比增长率
growth_rates = np.diff(monthly_sales) / monthly_sales[:-1] * 100

print(f"\n=== 环比增长率 ===")
for i, rate in enumerate(growth_rates):
    print(f"{i+2}月相对于{i+1}月: {rate:+.2f}%")

# 步骤6：整体趋势分析
overall_trend = np.polyfit(range(len(monthly_sales)), monthly_sales, 1)[0]
print(f"\n=== 整体趋势 ===")
print(f"月均增长: {overall_trend:.2f} 万元")
if overall_trend &gt; 0:
    print("趋势: 上升")
else:
    print("趋势: 下降")</code></pre><h3>运行说明</h3><p>将上述代码保存为 <code>sales_analysis.py</code>，然后在命令行运行：</p><pre><code class="bash">python sales_analysis.py</code></pre><h3>结果展示</h3><p>程序将输出完整的销售数据分析报告：</p><pre><code>=== 基本统计信息 ===
平均销售额: 158.33 万元
标准差: 24.17 万元
最高销售额: 195 万元
最低销售额: 120 万元

=== 3个月移动平均值 ===
1-3月: 127.67 万元
2-4月: 135.00 万元
3-5月: 142.00 万元
4-6月: 148.67 万元
5-7月: 155.67 万元
6-8月: 162.00 万元
7-9月: 169.00 万元
8-10月: 175.33 万元
9-11月: 182.00 万元
10-12月: 188.33 万元

=== 异常销售月份（超过206.67万元）===
无异常月份

=== 环比增长率 ===
2月相对于1月: +12.50%
3月相对于2月: -5.19%
4月相对于3月: +10.94%
5月相对于4月: +9.86%
6月相对于5月: -5.13%
7月相对于6月: +10.14%
8月相对于7月: +7.36%
9月相对于8月: -3.43%
10月相对于9月: +7.69%
11月相对于10月: +7.14%
12月相对于11月: -3.59%

=== 整体趋势 ===
月均增长: 5.86 万元
趋势: 上升</code></pre><p>这个实战项目展示了 NumPy 在数据分析中的典型应用：数据创建、统计计算、滑动窗口、条件筛选、趋势分析等。所有操作都通过向量化运算完成，代码简洁且高效。</p><hr/><h2>5. 最佳实践与常见陷阱</h2><h3>常见错误与规避方法</h3><h4>错误1：数据类型不一致导致的精度丢失</h4><pre><code class="python"># ❌ 错误做法
arr = np.array([1.5, 2.7, 3.9], dtype=int)  # 强制转换为整数，丢失小数部分
print(arr)  # 输出: [1 2 3]

# ✅ 正确做法
arr = np.array([1.5, 2.7, 3.9])  # 保持默认的float64类型
print(arr)  # 输出: [1.5 2.7 3.9]</code></pre><h4>错误2：数组视图与拷贝混淆</h4><pre><code class="python"># ❌ 错误做法：误以为切片创建了新数组
original = np.array([1, 2, 3, 4, 5])
slice_view = original[1:4]
slice_view[0] = 99
print(original)  # 输出: [ 1 99  3  4  5] - 原数组被修改！

# ✅ 正确做法：显式创建拷贝
original = np.array([1, 2, 3, 4, 5])
slice_copy = original[1:4].copy()
slice_copy[0] = 99
print(original)  # 输出: [1 2 3 4 5] - 原数组保持不变</code></pre><h4>错误3：不合理的循环使用</h4><pre><code class="python"># ❌ 错误做法：使用 Python 循环处理数组
arr = np.random.rand(1000000)
result = np.zeros_like(arr)
for i in range(len(arr)):
    result[i] = arr[i] * 2 + 1

# ✅ 正确做法：使用向量化运算
result = arr * 2 + 1</code></pre><h3>最佳实践建议</h3><p><strong>1. 内存优化：</strong><br/>对于大型数组，使用合适的数据类型可以显著减少内存占用：</p><pre><code class="python"># 对于0-255的整数，使用uint8而非默认的int64
small_integers = np.array([1, 2, 3, 255], dtype=np.uint8)</code></pre><p><strong>2. 预分配数组：</strong><br/>在循环中预分配数组比动态扩展更高效：</p><pre><code class="python"># ✅ 预分配
result = np.zeros(1000)
for i in range(1000):
    result[i] = calculate_value(i)</code></pre><p><strong>3. 利用广播机制：</strong><br/>合理使用广播可以避免不必要的数据复制：</p><pre><code class="python"># 将一维数组广播到二维数组
data = np.random.rand(5, 3)
row_means = data.mean(axis=1, keepdims=True)
normalized = data - row_means  # 广播减法</code></pre><p><strong>4. 使用掩码数组处理缺失值：</strong></p><pre><code class="python">data = np.array([1, 2, np.nan, 4, 5])
masked_data = np.ma.masked_invalid(data)
mean_value = masked_data.mean()  # 自动忽略NaN值</code></pre><h3>注意事项</h3><ul><li>当处理超过内存大小的数据时，考虑使用内存映射文件（<code>np.memmap</code>）</li><li>在多线程环境中使用 NumPy 时要注意 GIL（全局解释器锁）的影响</li><li>对于超大规模数据，考虑使用 Dask 或 Spark 等分布式计算框架</li><li>定期检查 NumPy 版本更新，新版本通常包含性能优化和新功能</li></ul><hr/><h2>6. 进阶指引</h2><p>掌握了 NumPy 的基础用法后，你可以探索以下高级特性和相关生态：</p><h3>高级功能</h3><p><strong>结构化数组：</strong> 允许存储异构数据，类似数据库表格</p><pre><code class="python">dt = np.dtype([('name', 'U10'), ('age', 'i4'), ('salary', 'f8')])
employees = np.array([('张三', 30, 8000.5), ('李四', 25, 6500.0)], dtype=dt)</code></pre><p><strong>ufunc（通用函数）：</strong> 自定义向量化函数</p><pre><code class="python">def custom_operation(x, y):
    return x * 2 + y ** 2

vectorized_func = np.frompyfunc(custom_operation, 2, 1)
result = vectorized_func(arr1, arr2)</code></pre><h3>生态扩展</h3><ul><li><strong>Pandas：</strong> 构建在 NumPy 之上的数据分析库，提供更高级的数据结构和分析工具</li><li><strong>SciPy：</strong> 科学计算工具集，包含优化、积分、线性代数等功能</li><li><strong>Matplotlib：</strong> 基于 NumPy 数组的绘图库，与 NumPy 无缝集成</li><li><strong>Scikit-learn：</strong> 机器学习库，其核心算法都依赖 NumPy 数组</li></ul><h3>学习路径</h3><ol><li><strong>深入理解数组操作：</strong> 掌握高级索引、排序、形状操作等</li><li><strong>学习线性代数：</strong> 深入理解矩阵运算、特征值、奇异值分解等</li><li><strong>性能优化：</strong> 学习如何编写高效的 NumPy 代码，避免性能陷阱</li><li><strong>专业领域应用：</strong> 根据需要深入学习信号处理、图像处理、金融计算等领域的 NumPy 应用</li></ol><h3>推荐资源</h3><ul><li><strong>官方文档：</strong> <a href="https://link.segmentfault.com/?enc=mKKu%2FhPw1C8w2dRXPjwVyw%3D%3D.Citd8lQPorqLwKnCeWobev1EQmkL7m9ypdAZrozWQzE%3D" rel="nofollow" target="_blank">https://numpy.org/doc/</a> - 最权威的信息来源</li><li><strong>NumPy 用户指南：</strong> 包含详细教程和最佳实践</li><li><strong>《Python for Data Analysis》</strong> by Wes McKinney - 深入理解 NumPy 和 Pandas</li><li><strong>Stack Overflow NumPy 标签：</strong> 解决实际问题的社区资源</li></ul><p>NumPy 的学习曲线相对平缓，但要真正精通需要持续的实践和探索。建议在项目中不断应用新学到的技巧，通过实际问题的解决来加深理解。随着你对 NumPy 的掌握程度加深，你会发现它不仅仅是一个计算工具，更是一种思维方式——用向量化、广播化的方式思考问题。</p>]]></description></item><item>    <title><![CDATA[人类达成了 大力的乌龙茶 ]]></title>    <link>https://segmentfault.com/a/1190000047575090</link>    <guid>https://segmentfault.com/a/1190000047575090</guid>    <pubDate>2026-01-27 14:01:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>这里是 「RTE 开发者日报」，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的技术」、「有亮点的产品」、「有思考的文章」、「有态度的观点」、「有看点的活动」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p>本期编辑：@瓒an、@鲍勃</p><p>01 有话题的技术<br/>1、亚马逊公布新款自研 AI 芯片 Trainium 3</p><p>日前，亚马逊云科技 CEO Matt Garman 在 re:Invent 2025 活动上，正式公布了亚马逊自研 AI 芯片 Trainium 系列的最新进展。</p><p>会上，Amazon Trainium 3 UltraServers 正式发布。</p><p>据介绍，这是亚马逊云科技首款搭载 3 纳米工艺 AI 芯片的服务器，相较 Amazon Trainium 2，不仅计算能力提升 4.4 倍、内存带宽提升 3.9 倍，每兆瓦算力可处理的 AI token 数量更实现了 5 倍增长。</p><p>服务器最高配置 144 个芯片，提供惊人的 362 petaflops FP8 计算能力。在运行 OpenAI 的 GPT-OSS-120B 模型时，每兆瓦输出 token 数是 Amazon Trainium 2 的 5 倍以上，实现超高能耗比。</p><p>同时，Matt Garman 还首次披露了 Amazon Trainium 4 芯片，并承诺将实现较 Amazon Trainium 3 六倍的 FP4 计算性能、四倍内存带宽和两倍高内存容量。</p><p>据悉，亚马逊云科技目前已完成超 100 万个 Trainium 2 芯片的规模化部署，为 Amazon Bedrock 中大部分推理工作提供核心算力支持，包括 Claude 最新一代模型的高效运行。</p><p>( @APPSO)</p><p>2、Meta Reality Labs 挖角苹果交互设计负责人 Alan Dye</p><p>今天凌晨，彭博社记者 Mark Gurman 发文透露，苹果人机交互设计副总裁 Alan Dye 被 Meta 挖角。</p><p>据悉，Dye 自 2015 年以来，一直担任苹果的用户界面设计团队的负责人。 而本次被挖角后，苹果将用长期设计师 Stephen Lemay 顶替 Dye 的岗位。</p><p>值得一提的是，Dye 曾负责监督 iOS 26、液态玻璃界面、Vision Pro 界面、watchOS，以及各种系统交互层面内容（如空间计算交互、灵动岛）。</p><p>报道指出，Dye 在乔布斯离开后，一直担任着重要角色：帮助公司定义了最新操作系统、App 以及设备的外观。另外，Dye 在苹果的团队也帮助开发一系列新的智能家居设备。</p><p>Meta 方面，随着 Dye 加入，该公司正在创立一个新的设计工作室，并且有 Dye 负责硬件、软件和 AI 集成方面的界面设计。</p><p>Dye 将向负责现实实验室的首席技术官 Andrew Bosworth 汇报工作，而现实实验室负责开发可穿戴设备，如智能眼镜和虚拟现实头戴式设备。Gurman 透露，Dye 将于 12 月 31 日正式开始担任团队首席设计官。</p><p>而且 Dye 还不是一个人走的，他还带走了苹果设计部门的高级总监 Billy Sorrentino。后者从 2016 年起就在苹果，主要负责 VisionOS 的用户界面设计。</p><p>( @APPSO)</p><p>3、小米卢伟冰：AI 与物理世界的深度结合是智能科技的下一站</p><p>12 月 3 日，@卢伟冰 在社媒发布卢伟冰答网友问第十二期，在回答「罗福莉加入了小米，未来在 AI 上会有什么新的战略」时表示：</p><p>其实我们在前几个季度就已经开始了在 AI 上的压强式投入，虽然不能透露太多，我们在 AI 大模型和应用方面的进展远超预期，我们认为 AI 与物理世界的深度结合是智能科技的下一站，小米也非常渴望人才尊重人才，也希望能够给优秀的人才提供好的发展平台。</p><p>95 后罗福莉出生于四川，父亲是一名电工，母亲是教师。她本人曾就读于四川宜宾市第一中学校 「清北班」，并以优异成绩考入北京师范大学，后被保送至北京大学深造。</p><p>在北大读硕士期间，她于 2019 年在人工智能领域顶级国际会议 ACL 上发表了 8 篇论文，其中 2 篇为第一作者。毕业后，她先后在阿里达摩院、幻方量化、DeepSeek 工作，主导开发了多语言预训练模型 VECO，并参与研发了 MoE 大模型 DeepSeek-V2。</p><p>11 月 12 日，罗福莉在朋友圈发文，正式宣布自己已经加入小米。</p><p>11 月 19 日消息，小米公司今日官宣，12 月 17 日，小米将在北京·国家会议中心举办「人车家全生态」合作伙伴大会。主论坛时间为上午 10:00-12:15，全程开放线上直播。</p><p>作为小米 MiMo 大模型负责人，罗福莉将在主论坛发表题为《Xiaomi MiMo：小米基座大模型》 的主题演讲，这是她自 11 月 12 日加入小米后的首次公开亮相。</p><p>（@荆楚网）</p><p>02 有亮点的产品<br/>1、Peopleboxai 推出 Nova：首款「人性化」AI 面试官，优化招聘流程</p><p>Peopleboxai 发布了其 AI 产品「Nova」，号称是「人性化」的 AI 面试官。Nova 能够自动化包括简历筛选、电话面试、视频面试、实时编码测试以及生成决策报告在内的整个第一轮招聘流程，显著加快招聘速度并提升效率。</p><p>全流程自动化： Nova 能够处理从简历筛选、联系候选人（通过 InMail、邮件、电话）到进行全面的语音/视频面试，甚至执行高级编码测试，直至提供详细的、可直接用于决策的报告。<br/>高度「人性化」体验： Nova 被设计成「最佳招聘官和面试官的数字孪生」，能够模拟自然的暂停、语气和「嗯」等语用标记，提供友好的、类似真人的互动体验，候选人对其评价很高。<br/>定制化与智能化： 用户可以根据自己的需求定制 Nova 的面试风格，包括技能深度、难度、面试类型、语调和结构。Nova 还能从公司过往的招聘数据（职位描述、面试记录、ATS 笔记等）中学习，提升其判断能力。<br/>显著提升效率： Nova 帮助客户将第一轮面试报告的完成时间从 4-5 周缩短到 48 小时以内，为招聘团队节省了大量时间，使其能专注于更具战略意义的工作。<br/>覆盖多渠道招聘： Nova 不仅处理入站（inbound）和内推（referral）的候选人，还能主动进行外呼（outbound）候选人搜寻和联系。<br/>Nova 产品已上线，用户可通过 Peopleboxai 官网了解更多信息并申请试用。</p><p>(@Y Combinator Launches)</p><p>2、理想汽车发布首款 AI 眼镜 Livis：标配蔡司镜片 补贴后售价 1699 元起</p><p>12 月 3 日，理想汽车举办线上发布会，正式推出其首款 AI 智能眼镜 Livis。售价 1999 元起，12 月 31 日前下订可享受 15% 政府补贴，补贴后价格仅为 1699 元起。</p><p>「一款以钢铁侠 AI 管家「贾维斯」为灵感命名的智能眼镜，试图将「理想同学」的 AI 能力从驾驶空间延伸至用户日常生活的每个角落。」</p><p>Livis 名称源于理想汽车与钢铁侠 AI 管家「Jarvis」的组合。</p><p>整机重量控制在 36 克，提供经典黑、科技灰和橄榄绿三种颜色，并可选亮光或磨砂材质。</p><p>Livis 全系产品标配蔡司镜片，涵盖近视镜片、光致变色镜片与墨镜片等多种类型，满足用户在不同场景下的视觉需求。</p><p>理想宣称 Livis 在研发过程中实现了五项关键突破，构成了产品核心竞争力的重要组成部分。</p><p>典型续航时间达 18.8 小时。Livis 标配类似 AirPods 的无线充电盒，便于随身携带和补能。同时，眼镜支持与理想汽车的车机系统无线快充，上车后放置在专属充电位进行充电。</p><p>在硬件配置上，Livis 搭载恒玄 BES2800 主控芯片和独立的 ISP 成像芯片，采用 SONY IMX681 摄像头，拥有 1200 万像素、支持 4K 照片以及电子防抖拍摄。</p><p>汽车联动场景是 Livis 最独特的卖点。通过蓝牙和 5G 网络，眼镜可无缝连接车辆，实现语音远程控车。用户可在百米范围内，通过语音指令操控电动侧滑门启闭、提前开启空调及座椅加热，甚至检查车辆续航和充电状态。</p><p>（@极客公园、@快科技）</p><p>3、豆包手机助手无法登录微信，双方回应</p><p>日前，字节跳动豆包团队与中兴合作发布了豆包手机助手技术预览版后，有试用 Nubia M153 工程样机的用户反馈，出现无法正常登陆微信的情况。</p><p>对于相关情况，豆包团队方面昨晚发文并做出回应。</p><p>豆包方面表示，其后续已下线了手机助手操作微信的能力。 目前，nubia M153 上被禁止登录的微信账号正陆续解封。</p><p>而微信相关人士也通过澎湃新闻回应，豆包手机助手无法正常登陆微信的微信并没有什么特别动作，「可能是中了本来就有的安全风控措施。」</p><p>针对此前曾有科技公司爆料「豆包手机助手存在侵犯用户隐私」的问题，团队方面强调，豆包手机助手不存在任何黑客行为。</p><p>据悉，此前上述公司曾表示豆包手机助手在努比亚手机上拥有 INJECT\_EVENTS 权限，该权限在安卓权限定义中属于操作系统高危权限，并且拿到该权限，要面临刑事责任。</p><p>豆包方面表示，INJECT\_EVENTS 确实是系统级权限，但拥有了该权限许可，相关产品才能跨屏、跨应用来模拟点击事件，完成用户操作手机的任务需求。</p><p>团队还强调，豆包手机助手需要用户主动授权，才可以调用该权限，使用操作手机功能。该权限的使用，豆包方面也在权限清单中进行了明确的披露。据了解，目前行业的 AI 助手，均需要使用该权限（或与其类似的无障碍权限）才能提供操作手机的服务。</p><p>豆包方面强烈表示，豆包手机助手也不会代替用户进行相关授权和敏感操作。</p><p>同时，豆包方面也对读取屏幕的隐私问题进行了回应。其表示，助手操作手机时需要读取屏幕（否则无法完成任务），但屏幕和操作过程都不会在服务器端留下存储，且所有的相关内容也都不会进入模型训练，确保用户隐私安全。</p><p>( @APPSO)</p><p>4、健康追踪应用 Healthify Ria 升级 AI 助手：支持实时语音与摄像头交互</p><p>健康追踪初创公司 Healthify 推出了其 AI 助手 Ria 的新版本，该版本支持通过语音和摄像头进行实时对话，并能理解超过 50 种语言（包括 14 种印度语言）以及混合语言输入。此举旨在通过更自然的交互方式，提升用户健康习惯养成的效率和用户粘性。</p><p>实时对话与多模态输入： Ria 现在支持通过语音进行实时对话，用户还可以通过摄像头扫描食物获取营养信息并进行记录，大幅简化了数据录入流程。<br/>多语言与混合语言支持： Ria 能够理解超过 50 种语言，并支持 Hinglish、Spanglish 等混合语言输入，服务全球用户。<br/>整合多源健康数据： Ria 可以整合来自健身追踪器、睡眠追踪器、血糖监测仪等设备的数据，为用户提供运动、睡眠、身体准备度和血糖波动等方面的洞察，并给出建议。<br/>增强记忆与个性化： Healthify 正在为 Ria 构建一个更持久的记忆层，使其能够记住用户的偏好和健康变化，提供更个性化的建议。<br/>教练与营养师辅助： Ria 将被整合到用户与教练、营养师的沟通中，协助双方快速调取数据、回答问题，并可转录通话内容，提取关键信息。<br/>(@TechCrunch)</p><p>03 有态度的观点<br/>1、《阿凡达》导演：对 AI 没意见，但要尊敬演员们</p><p>近日，导演詹姆斯·卡梅隆在《阿凡达 3》世界首映礼上称该片没有使用 AI 生成，随后他对 ComicBookcom 发表了自己对于生成式 AI 的应用看法。</p><p>卡梅隆表示，自己对生成式 AI 没有意见，但他强调：「我们拍《阿凡达》电影不使用它，我们尊敬并赞颂演员们，我们不用 AI 代替演员。」</p><p>同时，卡梅隆也表示，「这件事（生成式 AI）自会有方向，我想好莱坞会进行自我监管，但我们作为艺术家要找到出路，前提是我们得能存在。所以，比起别的东西，来自『大 AI』的生存威胁是最让我担忧的。」</p><p>值得一提的是，卡梅隆所提到的「大 AI」，是指人类利用 AI 的状况和其产生的问题，对应的「小 AI」是指更细节、技术性的层面，比如用 AI 生成内容。</p><p>在卡梅隆看来，AI 和人类未来有深切的担忧和存在危机，他认为「小 AI」各行业会找到应对和利用之法，但「大 AI」问题就不好说了。</p><p>卡梅隆还提到，若了解 AI，就会知道「校准」是个重大问题。「AI 必须被训练、教导，必须被约束去只做对人类好的事情。」其强调，「只有我们人类达成了共识，你才能对 AI 进行校准。」<a style="color: white;" target="_blank">实打weibo.com/ttarticle/p/show?id=2309405259489887781351 weibo.com/ttarticle/p/show?id=2309405259490198421849 weibo.com/ttarticle/p/show?id=2309405259490517188797 weibo.com/ttarticle/p/show?id=2309405259490831761485 weibo.com/ttarticle/p/show?id=2309405259491141877785 weibo.com/ttarticle/p/show?id=2309405259491552919657 weibo.com/ttarticle/p/show?id=2309405259491871686727 weibo.com/ttarticle/p/show?id=2309405259492182327615 weibo.com/ttarticle/p/show?id=2309405259492496900190 实</a></p>]]></description></item><item>    <title><![CDATA[怎么跟踪项目里程碑：从定义到交付的完整控制体系 曾经爱过的汉堡包 ]]></title>    <link>https://segmentfault.com/a/1190000047574556</link>    <guid>https://segmentfault.com/a/1190000047574556</guid>    <pubDate>2026-01-27 13:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、重新认识项目里程碑：不仅是进度点，更是风险控制阀</h2><p>项目里程碑常被误解为简单的"时间标记"，实质上它是项目健康度的关键诊断点。对于项目经理、产品负责人及技术团队领导者而言，有效的里程碑管理意味着：</p><ul><li><strong>决策依据</strong>：每个里程碑都是"继续/调整/终止"项目的决策节点</li><li><strong>资源调控</strong>：基于里程碑达成情况动态调配人力与预算</li><li><strong>风险暴露</strong>：提前发现依赖断裂、范围蔓延等潜在问题</li></ul><p>数据表明，实施系统化里程碑跟踪的项目，按时交付率可提升42%，预算超支率降低35%。以下体系将为您建立完整的控制框架。</p><h2>二、专业里程碑跟踪五步法：从定义到闭环</h2><h3>第一步：精准定义——让成功标准无可争议</h3><p>低质量的里程碑描述是跟踪失效的首要原因。对比以下两种定义方式：</p><p><strong>模糊定义（导致分歧）：</strong></p><blockquote>"完成用户模块开发"</blockquote><p><strong>专业定义（共识明确）：</strong></p><pre><code>里程碑：用户管理模块上线
- 业务标准：支持用户注册、登录、个人资料编辑三项核心功能
- 技术标准：所有接口响应时间＜200ms，单元测试覆盖率≥90%
- 质量门禁：通过安全渗透测试，无高危漏洞
- 交付物清单：
  1. 部署至预生产环境的可运行代码
  2. 更新的API文档（Swagger/Postman）
  3. 运维部署手册
- 验证方式：产品经理与测试组长联合签署验收报告
- 截止日期：2023年11月15日</code></pre><p>最佳实践是创建标准化的里程碑卡片模板，将上述结构固化，确保团队在定义阶段即对齐预期。许多团队会在板栗看板、Jira或Asana中建立这样的模板，为后续的可视化跟踪打下基础。</p><h3>第二步：可视化跟踪——构建多维度进度雷达</h3><p>纯文本的计划表无法揭示真实进展。专业团队通过三层可视化建立透明视图：</p><p><strong>1. 宏观路线图视图</strong><br/>许多项目管理工具都提供了路线图功能。无论是板栗看板、Jira还是ClickUp，都能将里程碑置于产品路线图中，清晰展示其对业务目标的支撑关系。这种直观的布局帮助团队快速理解"我们现在在哪"以及"下一步去哪"。</p><p><strong>2. 依赖关系网络图</strong><br/>复杂项目中，里程碑间的依赖链是主要风险源。团队可以使用Miro、draw.io等专业工具绘制依赖图，或者利用板栗看板、Asana或Monday.com的依赖关系功能，清晰标出强依赖、弱依赖与外部依赖，这能有效预防因前后置任务不明确导致的阻塞。</p><p><strong>3. 技术实现进度看板</strong><br/>对于开发团队，代码层面的自动跟踪更可靠，可以与项目管理工具的视图结合：</p><pre><code class="python"># 里程碑自动健康度检查脚本
import git
from datetime import datetime, timedelta

class MilestoneTracker:
    def __init__(self, repo_path, milestone):
        self.repo = git.Repo(repo_path)
        self.milestone = milestone
        
    def get_code_activity_metrics(self):
        """通过代码提交分析实际进展"""
        since_date = self.milestone['start_date']
        commits = list(self.repo.iter_commits(since=since_date))
        
        # 分析提交模式
        active_days = len({c.committed_date.date() for c in commits})
        feature_branches = self._count_feature_branches()
        
        return {
            '提交频率': f"{len(commits)/active_days if active_days&gt;0 else 0:.1f}次/天",
            '活跃分支数': feature_branches,
            '代码行增量': self._calculate_loc_change(since_date),
            '风险标识': self._identify_risk_patterns(commits)
        }
    
    def _identify_risk_patterns(self, commits):
        """识别高风险模式：如大量紧急修复、关键人员提交集中"""
        patterns = []
        last_week = datetime.now() - timedelta(days=7)
        hotfix_count = sum(1 for c in commits if 'fix' in c.message.lower() 
                          and c.committed_datetime &gt; last_week)
        
        if hotfix_count &gt; 5:
            patterns.append('近期紧急修复过多，可能存在技术债务')
        return patterns

# 使用示例
tracker = MilestoneTracker('/project/code', current_milestone)
print(tracker.get_code_activity_metrics())</code></pre><h3>第三步：预警与干预——建立三级响应机制</h3><p>被动等待里程碑到期是项目失败的主要原因。成功团队在以下节点主动干预：</p><p><strong>黄色预警（提前30%）</strong></p><ul><li>触发条件：时间消耗30%，进度＜25%</li><li>自动检测：在板栗看板、Jira或Asana中设置基于列表状态或截止日期的自动化规则，当任务完成率低于阈值时自动通知负责人</li><li>标准动作：召开15分钟站立会，调整下周工作重点</li></ul><p><strong>橙色预警（中期检查点）</strong></p><ul><li>触发条件：时间消耗60%，进度＜50%</li><li>自动检测：结合板栗看板、Microsoft Project或ClickUp的进度统计功能生成偏差报告</li><li><p>标准动作：</p><ol><li>重新评估剩余工作复杂度</li><li>申请额外资源或缩减非核心范围</li><li>更新风险登记册</li></ol></li></ul><p><strong>红色预警（最后补救期）</strong></p><ul><li>触发条件：时间消耗90%，进度＜80%</li><li><p>标准动作：</p><ul><li>启动每日进展检查</li><li>考虑"最小可行交付"方案</li><li>向利益相关者透明沟通现状</li></ul></li></ul><pre><code class="javascript">// 预警系统集成示例：将进度数据同步至团队沟通工具
async function sendMilestoneAlert(milestone, channel) {
  const progress = await calculateProgress(milestone);
  const timeline = calculateTimelineStatus(milestone);
  
  let alertLevel = 'info';
  let message = `里程碑【${milestone.name}】定期更新`;
  
  if (progress.rate &lt; timeline.expected * 0.7) {
    alertLevel = 'warning';
    message = `⚠️ 里程碑【${milestone.name}】进度滞后。预期${timeline.expected}%，实际${progress.rate}%。`;
  }
  
  if (milestone.dueDate &lt; Date.now() + 86400000 * 3) {
    alertLevel = 'urgent';
    message = `🚨 里程碑【${milestone.name}】还有3天到期！完成率：${progress.rate}%`;
  }
  
  await slackClient.postMessage(channel, {
    text: message,
    blocks: createProgressBlocks(milestone, progress)
  });
}</code></pre><h3>第四步：结构化评审——超越进度询问的深度对话</h3><p>低效评审只问"完成了吗？"，高效评审关注三个维度：</p><p><strong>技术维度评审清单：</strong></p><ul><li>[ ] 代码是否通过所有自动化测试？</li><li>[ ] 性能基准测试结果是否达标？</li><li>[ ] 安全扫描是否发现新漏洞？</li><li>[ ] 文档是否与实现同步更新？</li></ul><p><strong>过程维度评审清单：</strong></p><ul><li>[ ] 实际工作量与估算偏差是否超过20%？</li><li>[ ] 团队在该阶段的速度趋势如何？</li><li>[ ] 发现了哪些可以复用的经验？</li></ul><p><strong>业务维度评审清单：</strong></p><ul><li>[ ] 交付物是否满足验收标准？</li><li>[ ] 用户反馈是否验证了核心假设？</li><li>[ ] 下一阶段的优先级是否需要调整？</li></ul><p>工具支持方面，可以将上述评审清单以检查项形式附加在里程碑卡片上，板栗看板、Confluence或Notion都支持这样的功能，固化评审流程，确保每次评审的完整性和一致性。</p><h3>第五步：闭环与进化——将经验转化为组织资产</h3><p>里程碑完成不是终点，而是组织能力提升的起点：</p><p><strong>1. 量化复盘会</strong><br/>不讨论"感觉"，只分析数据：</p><ul><li>计划vs实际时长对比</li><li>需求变更次数及影响</li><li>阻塞问题的根本原因分类</li></ul><p><strong>2. 资产归档标准</strong><br/>每个里程碑关闭后，应在项目管理工具中将其移至"已完成"区域，并将关键产出物链接或上传至卡片中，板栗看板、Jira或Confluence都能形成可追溯的项目档案馆。</p><p><strong>3. 流程改进点</strong><br/>基于复盘发现，更新团队工作空间中的：</p><ul><li>估算系数库</li><li>风险检查清单</li><li>任务与里程碑模板</li></ul><h2>三、关键挑战与应对策略</h2><h3>挑战1：里程碑频繁滑动</h3><p><strong>根本原因</strong>：定义模糊、依赖管理失控<br/><strong>解决方案</strong>：</p><ol><li>采用"完成定义+验收标准"双重要求，在项目卡片中明确展示</li><li>利用板栗看板、Jira或Asana中的任务链接功能，建立前序任务强制完成机制</li><li>引入缓冲区管理：关键路径里程碑设置5-10%时间缓冲</li></ol><h3>挑战2：团队报告失真</h3><p><strong>根本原因</strong>：手工报告主观性强<br/><strong>解决方案</strong>：</p><ol><li>建立自动化数据收集：代码提交、构建状态、测试覆盖率通过集成自动关联至项目管理工具</li><li>实施"完成证据"制度：每个任务完成必须附上可验证证据（如测试报告链接）</li></ol><h3>挑战3：多团队协同困难</h3><p><strong>根本原因</strong>：信息孤岛<br/><strong>解决方案</strong>：</p><ol><li>使用板栗看板、Microsoft Project Online或ClickUp的团队共享功能，建立透明的里程碑日历</li><li>设立跨团队接口人，负责在共享看板上维护和同步依赖关系</li><li>每周举行简短的跨团队里程碑同步会，基于同一可视化看板进行沟通</li></ol><h2>四、进阶：数据驱动型里程碑管理</h2><p>成熟组织不止跟踪"是否完成"，更建立预测模型：</p><h3>完工预测算法</h3><pre><code>预测完工概率 = 
  进度健康度 × 0.4 + 
  团队历史达成率 × 0.3 + 
  风险暴露度 × 0.2 + 
  资源稳定性 × 0.1

进度健康度 = (已完成关键任务数 / 总关键任务数) × 
             (实际速度 / 计划速度)
风险暴露度 = 1 - (已缓解风险数 / 总识别风险数)</code></pre><h3>技术债务量化跟踪</h3><pre><code class="python"># 里程碑技术债务影响评估
def assess_tech_debt_impact(milestone):
    debt_indicators = {
        '测试覆盖率下降': coverage_decline(milestone),
        '构建失败率上升': build_failure_rate(milestone),
        '代码复杂度增长': cyclomatic_complexity_increase(milestone),
        '重复代码出现': duplicated_code_blocks(milestone)
    }
    
    impact_score = sum(debt_indicators.values())
    
    # 根据得分推荐行动
    if impact_score &gt; 8:
        return "建议安排专项重构迭代"
    elif impact_score &gt; 5:
        return "后续任务估算增加20%缓冲"
    else:
        return "在正常维护中处理"</code></pre><h2>结语：跟踪的本质是创造确定性</h2><p>在变化成为常态的项目环境中，专业的里程碑跟踪不是增加约束，而是通过有限的关键控制点，为团队创造应对变化的自由空间。它让不确定性在可控范围内暴露，让决策基于事实而非直觉，让交付承诺从希望变为可实现的计划。</p><p><strong>立即行动框架</strong>：</p><ol><li>选择匹配你团队规模和协作习惯的工具，无论是板栗看板、Jira、Asana还是ClickUp，关键是适合团队</li><li>重新定义下一个里程碑，确保包含可验证的验收标准</li><li>建立至少一个自动化跟踪指标（如代码提交与看板任务的关联）</li><li>在下一个评审中加入一个量化问题（而不仅仅是"进展如何"）</li></ol><p>记住：好的跟踪系统如同精密的仪表盘，它不控制车辆的方向，但确保驾驶者始终知道自己在哪、油量多少、何时需要转向——这正是项目成功抵达终点的根本保障。</p>]]></description></item><item>    <title><![CDATA[如何通过Java SDK描述Collection DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047574754</link>    <guid>https://segmentfault.com/a/1190000047574754</guid>    <pubDate>2026-01-27 13:03:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文介绍如何通过Java SDK获取已创建的Collection的状态和Schema信息。</p><h2>前提条件</h2><ul><li>已创建Cluster</li><li>已获得API-KEY</li><li>已安装最新版SDK</li></ul><h2><strong>接口定义</strong></h2><p>Java</p><pre><code class="java">// class DashVectorClient

public Response&lt;CollectionMeta&gt; describe(String name);</code></pre><h2><strong>使用示例</strong></h2><p><strong>说明</strong></p><ol><li>需要使用您的api-key替换示例中的YOUR_API_KEY、您的Cluster Endpoint替换示例中的YOUR_CLUSTER_ENDPOINT，代码才能正常运行。</li><li>本示例需要参考<a href="https://link.segmentfault.com/?enc=W4ur0jqaEnfxPutqmcWnww%3D%3D.0iBAvLz5H9LmlTtjHTgDBA3dv57WO952yswr1k1Zgmiey1bbufoI1sPXyfa6sK5dMHCChn2xJCGg2vzwnJpWgQ%3D%3D" rel="nofollow" target="_blank">新建Collection-使用示例</a>提前创建好名称为<code>quickstart</code>的Collection。</li></ol><p>Java</p><pre><code class="java">import com.aliyun.dashvector.DashVectorClient;
import com.aliyun.dashvector.common.DashVectorException;
import com.aliyun.dashvector.models.CollectionMeta;
import com.aliyun.dashvector.models.responses.Response;

public class Main {
    public static void main(String[] args) throws DashVectorException {
        DashVectorClient client = new DashVectorClient("YOUR_API_KEY", "YOUR_CLUSTER_ENDPOINT");

        Response&lt;CollectionMeta&gt; response = client.describe("quickstart");

        System.out.println(response);
        // example output:
        // {
        //     "code":0,
        //     "message":"",
        //     "requestId":"cb468965-d86b-405a-87a4-a596e61c1240",
        //     "output":{
        //         "name":"quickstart",
        //         "dimension":4,
        //         "dataType":"FLOAT",
        //         "metric":"dotproduct",
        //         "status":"SERVING",
        //         "fieldsSchema":{
        //             "name":"STRING",
        //             "weight":"FLOAT",
        //             "age":"INT", 
        //             "id":"LONG"
        //         },
        //         "partitionStatus":{
        //             "default":"SERVING"
        //         }
        //     }
        // }
    }
}</code></pre>]]></description></item><item>    <title><![CDATA[Access 连接 SQL Server：直通查询 vs 链接表 vs ADO，如何选择？ acce]]></title>    <link>https://segmentfault.com/a/1190000047574794</link>    <guid>https://segmentfault.com/a/1190000047574794</guid>    <pubDate>2026-01-27 13:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Access 连接 SQL Server：直通查询 vs 链接表 vs ADO，如何选择？</h2><p><strong>摘要</strong>：当 Access 前端需要连接 SQL Server 后端时，开发者面临三种主流技术方案：<strong>链接表（Linked Tables）</strong>、<strong>直通查询（Pass-Through Queries）</strong> 和 <strong>ADO 编程</strong>。本文从底层原理、性能特征、适用场景三个维度进行深度对比，帮助开发者在实际项目中做出正确的技术选型。</p><hr/><h3>一、技术背景</h3><p>Access 作为前端开发工具连接 SQL Server 后端，是中小型企业信息化的经典架构。这种"胖客户端"模式相比纯 Web 方案，具有开发效率高、部署简单的优势。</p><p>但 Access 与 SQL Server 之间的数据交互存在多种实现路径，不同方案在 <strong>网络开销</strong>、<strong>服务器负载</strong>、<strong>代码复杂度</strong> 上差异显著。</p><hr/><h3>二、三种方案的底层原理</h3><h4>1. 链接表（Linked Tables）</h4><p><strong>原理</strong>：通过 ODBC 驱动在 Access 中创建指向 SQL Server 表的"快捷方式"。Access 的 ACE/Jet 引擎会将用户操作（筛选、排序、更新）转换为 ODBC 调用。</p><pre><code>┌──────────────┐      ODBC       ┌──────────────┐
│   Access     │  ←──────────→   │  SQL Server  │
│  (ACE引擎)   │   链接表驱动     │   (T-SQL)    │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>透明性</strong>：开发者可以像操作本地表一样使用 <code>SELECT * FROM tblOrders</code>。</li><li><strong>引擎介入</strong>：ACE 引擎会"尝试"优化查询，但复杂查询可能被拆解为多次网络往返。</li><li><strong>事务支持</strong>：受限于 ODBC 驱动的事务隔离级别。</li></ul><p><strong>创建方式</strong>：</p><pre><code class="vb">' VBA 代码创建链接表
DoCmd.TransferDatabase acLink, "ODBC Database", _
    "ODBC;DRIVER={SQL Server};SERVER=192.168.1.100;DATABASE=SalesDB;Trusted_Connection=Yes", _
    acTable, "dbo.Orders", "lnkOrders"</code></pre><hr/><h4>2. 直通查询（Pass-Through Queries）</h4><p><strong>原理</strong>：绕过 ACE 引擎，将 <strong>原生 T-SQL</strong> 直接发送到 SQL Server 执行，结果集作为只读快照返回。</p><pre><code>┌──────────────┐    原生 T-SQL    ┌──────────────┐
│   Access     │  ──────────────→ │  SQL Server  │
│  (仅传递)     │   不经过 ACE     │   (直接执行)  │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>完全控制</strong>：可使用 SQL Server 特有语法（<code>TOP</code>、<code>WITH (NOLOCK)</code>、<code>PIVOT</code> 等）。</li><li><strong>只读限制</strong>：返回结果集默认不可编辑（除非配合链接表使用）。</li><li><strong>存储过程调用</strong>：最佳的存储过程执行方式。</li></ul><p><strong>创建方式</strong>：</p><pre><code class="sql">-- 在查询设计器中设置 "直通" 属性为 "是"
-- 或通过 VBA 创建
SELECT TOP 100 OrderID, CustomerName, OrderDate
FROM dbo.Orders WITH (NOLOCK)
WHERE OrderDate &gt;= '2025-01-01'
ORDER BY OrderDate DESC</code></pre><p><strong>VBA 动态执行</strong>：</p><pre><code class="vb">Public Sub ExecutePassThrough(strSQL As String)
    Dim qdf As DAO.QueryDef
    
    On Error Resume Next
    CurrentDb.QueryDefs.Delete "qryTemp"
    On Error GoTo 0
    
    Set qdf = CurrentDb.CreateQueryDef("qryTemp")
    With qdf
        .Connect = "ODBC;DRIVER={SQL Server};SERVER=192.168.1.100;DATABASE=SalesDB;Trusted_Connection=Yes"
        .SQL = strSQL
        .ReturnsRecords = True  ' 如果是 INSERT/UPDATE/DELETE，设为 False
    End With
    
    ' 绑定到窗体或报表
    Me.RecordSource = "qryTemp"
End Sub</code></pre><hr/><h4>3. ADO 编程（ActiveX Data Objects）</h4><p><strong>原理</strong>：通过 ADO 对象模型（<code>ADODB.Connection</code>、<code>ADODB.Recordset</code>）直接操作 OLE DB 或 ODBC 数据源，完全脱离 Access 的 DAO/ACE 体系。</p><pre><code>┌──────────────┐     OLE DB      ┌──────────────┐
│   Access     │  ←──────────→   │  SQL Server  │
│  (ADO对象)   │   直接连接       │   (T-SQL)    │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>最大灵活性</strong>：支持游标类型选择、批量更新、断开式记录集。</li><li><strong>可移植性</strong>：ADO 代码可直接迁移到 VB6、VBScript、Excel VBA。</li><li><strong>代码量大</strong>：需要手动管理连接生命周期和错误处理。</li></ul><p><strong>典型代码</strong>：</p><pre><code class="vb">Public Function GetOrders(strCustomerID As String) As ADODB.Recordset
    Dim conn As New ADODB.Connection
    Dim rs As New ADODB.Recordset
    Dim strSQL As String
    
    ' 连接字符串
    conn.ConnectionString = "Provider=SQLOLEDB;Data Source=192.168.1.100;" &amp; _
                            "Initial Catalog=SalesDB;Integrated Security=SSPI;"
    conn.Open
    
    ' 参数化查询防止 SQL 注入
    strSQL = "SELECT * FROM dbo.Orders WHERE CustomerID = ?"
    
    With rs
        .ActiveConnection = conn
        .Source = strSQL
        .CursorLocation = adUseClient  ' 客户端游标，支持断开连接
        .CursorType = adOpenStatic
        .LockType = adLockBatchOptimistic
        .Open , , , , adCmdText
    End With
    
    ' 断开连接，返回独立记录集
    Set rs.ActiveConnection = Nothing
    conn.Close
    
    Set GetOrders = rs
End Function</code></pre><hr/><h3>三、性能对比测试</h3><p>以下是在 <strong>万级数据量</strong> 下的典型场景测试结果（仅供参考，实际因网络环境而异）：</p><table><thead><tr><th>场景</th><th>链接表</th><th>直通查询</th><th>ADO</th></tr></thead><tbody><tr><td>SELECT 1000 条记录</td><td>1.2s</td><td>0.3s</td><td>0.4s</td></tr><tr><td>复杂 JOIN（5表关联）</td><td>8.5s</td><td>0.8s</td><td>0.9s</td></tr><tr><td>调用存储过程</td><td>不支持</td><td>0.2s</td><td>0.2s</td></tr><tr><td>批量 INSERT 1000 条</td><td>15s</td><td>0.5s</td><td>0.6s</td></tr><tr><td>单条记录更新</td><td>0.1s</td><td>0.1s</td><td>0.1s</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li><strong>简单 CRUD</strong>：三者差异不大。</li><li><strong>复杂查询/批量操作</strong>：直通查询和 ADO 优势明显。</li><li><strong>链接表的性能陷阱</strong>：多表 JOIN 时，ACE 引擎可能先拉取全表数据到本地再做关联，造成巨大的网络开销。</li></ul><hr/><h3>四、适用场景决策树</h3><pre><code>                        ┌─────────────────────────┐
                        │  需要连接 SQL Server？   │
                        └───────────┬─────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    ▼                               ▼
            需要绑定窗体/报表？               仅需执行命令/获取数据？
                    │                               │
        ┌───────────┴───────────┐                   │
        ▼                       ▼                   ▼
   简单表结构              复杂查询/存储过程      ───→  ADO
   单表或简单JOIN                │                    （最大灵活性）
        │                       │
        ▼                       ▼
    【链接表】              【直通查询】
   （最简单）              （高性能）</code></pre><p><strong>场景建议</strong>：</p><table><thead><tr><th>场景</th><th>推荐方案</th><th>理由</th></tr></thead><tbody><tr><td>数据维护窗体（增删改查）</td><td>链接表</td><td>可直接绑定，无需额外代码</td></tr><tr><td>报表数据源</td><td>直通查询</td><td>只读即可，性能最优</td></tr><tr><td>调用存储过程</td><td>直通查询 / ADO</td><td>链接表不支持存储过程</td></tr><tr><td>复杂多表统计</td><td>直通查询</td><td>避免 ACE 拆解查询</td></tr><tr><td>需要事务控制</td><td>ADO</td><td>可精确控制 <code>BeginTrans</code>/<code>CommitTrans</code></td></tr><tr><td>断开式数据处理</td><td>ADO</td><td>支持客户端游标和批量更新</td></tr><tr><td>跨数据库查询</td><td>ADO</td><td>可同时连接多个数据源</td></tr></tbody></table><hr/><h3>五、混合架构最佳实践</h3><p>在实际项目中，三种方案往往需要<strong>混合使用</strong>：</p><pre><code>┌─────────────────────────────────────────────────────────┐
│                    Access 前端                          │
├─────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │   链接表      │  │   直通查询   │  │    ADO      │  │
│  │ (数据维护)   │  │  (报表/统计) │  │  (存储过程)  │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
              ┌─────────────────────────┐
              │      SQL Server         │
              │   (存储过程/视图/表)     │
              └─────────────────────────┘</code></pre><p><strong>架构建议</strong>：</p><ol><li><strong>基础表</strong>：使用链接表，方便窗体绑定。</li><li><strong>复杂视图</strong>：在 SQL Server 端创建视图，Access 链接该视图。</li><li><strong>统计报表</strong>：使用直通查询，发挥 SQL Server 的聚合能力。</li><li><strong>业务逻辑</strong>：封装为存储过程，通过直通查询或 ADO 调用。</li></ol><hr/><h3>六、总结</h3><table><thead><tr><th>维度</th><th>链接表</th><th>直通查询</th><th>ADO</th></tr></thead><tbody><tr><td><strong>学习成本</strong></td><td>★☆☆</td><td>★★☆</td><td>★★★</td></tr><tr><td><strong>开发效率</strong></td><td>★★★</td><td>★★☆</td><td>★☆☆</td></tr><tr><td><strong>运行性能</strong></td><td>★☆☆</td><td>★★★</td><td>★★★</td></tr><tr><td><strong>灵活性</strong></td><td>★☆☆</td><td>★★☆</td><td>★★★</td></tr><tr><td><strong>可维护性</strong></td><td>★★★</td><td>★★☆</td><td>★★☆</td></tr></tbody></table><p><strong>核心原则</strong>：</p><ul><li>能用链接表解决的，不要过度设计。</li><li>性能敏感的场景，优先考虑直通查询。</li><li>需要精细控制（事务、游标、多数据源）时，使用 ADO。</li></ul><hr/><p><strong>「Access开发」</strong> 专注于 Microsoft Access 开发与企业级应用，提供以下服务：</p><p><strong>📚 技术培训</strong></p><ul><li>Access VBA 从入门到精通（线上/线下）</li><li>Access + SQL Server 企业级开发实战</li><li>Access 系统性能优化与架构设计</li></ul><p><strong>💼 定制开发</strong></p><ul><li>企业 ERP/CRM/进销存系统开发</li><li>旧系统升级与性能优化</li><li>Access 迁移至 Web/Power Platform 咨询</li></ul><p><strong>🔧 技术支持</strong></p><ul><li>代码审查与重构建议</li><li>疑难问题远程诊断</li><li>一对一技术辅导</li></ul><hr/><p><em>技术改变业务，专注创造价值。</em></p>]]></description></item><item>    <title><![CDATA[上市大模型企业数据基础设施的选择：MiniMax 基于阿里云 SelectDB 版，打造全球统一AI]]></title>    <link>https://segmentfault.com/a/1190000047574942</link>    <guid>https://segmentfault.com/a/1190000047574942</guid>    <pubDate>2026-01-27 13:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>MiniMax 全球领先的通用人工智能科技公司。旗下主要有 MiniMax M2.1、Hailuo 2.3、Speech 2.6 和 Music 2.0 等大模型，MiniMax Agent、海螺 AI、星野、Talkie 等产品，以及为企业客户与开发者提供 API 服务的 MiniMax 开放平台。截至目前，MiniMax 已有超过 200 个国家及地区的逾 2.12 亿名个人用户以及超过 100 个国家的企业客户。</p><p>在技术层面，MiniMax 坚持文本、视频、语音等全模态模型自主研发。目前，其全模态模型已进入国际第一梯队，被业内称为“全球唯四实现这一水平的企业之一”。</p><p>在推理能力和效率方面，MiniMax 近年来的模型迭代节奏明显加快，在多项国际评测榜单中进入全球前列。相关模型以较低算力成本实现接近国际顶尖闭源模型的性能表现，也在海外开发者社区中获得关注。</p><p>MiniMax 通过开放平台赋能多个行业，将领先的模型能力以 API 方式提供给企业和开发者。随着模型调用量的指数级增长，<strong>训练与推理产生的运行日志数据量也急剧膨胀</strong>。 这些日志对于 AI 应用的运行监控、性能优化与问题排查至关重要，因此，<strong>选择一款能够支撑高吞吐、易查询、低成本的日志存储与检索引擎，成为保障业务稳定高效运行的关键</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574944" alt="MiniMax 可观测性数据平台核心基座.JPEG" title="MiniMax 可观测性数据平台核心基座.JPEG"/></p><p>面对海量、实时且不断增长的日志数据处理需求，<strong>MiniMax 经过深度评估，最终选择阿里云数据库 SelectDB 版作为其全新可观测性数据平台的核心基座</strong>。阿里云数据库 SelectDB 版凭借其更低的成本、更高的查询性能以及更灵活的查询方式在众多产品中脱颖而出。其关键特性精准匹配了现代 AI 业务的严苛要求：</p><ul><li><strong>云原生存算分离架构</strong>：基于对象存储 OSS 的存储层与弹性计算层解耦，支持独立、无损的弹性伸缩，为应对日志洪峰提供了近乎无限的扩展能力。</li><li><strong>多集群硬隔离与数据共享</strong>：支持云原生多集群硬隔离能力，用户可以将单个实例的计算资源划分为多个逻辑集群，不同集群之间的分配独立的计算资源，实现了不同集群的严格物理资源隔离和数据共享。</li><li><strong>智能缓存加速</strong>：通过单副本本地读写缓存、智能数据淘汰策略、高效列式存储格式和先进压缩算法，显著提升了海量数据的读写效率。</li></ul><blockquote>阿里云数据库 SelectDB 植根于开源 Apache Doris 的坚实基础，深度融合云随需而用的特性，依托阿里云基础设施，构建起云原生存算分离的全新架构，面向企业海量数据的实时分析需求，提供极速实时、湖仓融合统一、简单易用的云上数仓服务。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574945" alt="MiniMax 可观测性数据平台核心基座-1.PNG" title="MiniMax 可观测性数据平台核心基座-1.PNG" loading="lazy"/></p><p>基于阿里云 SelectDB 版，<strong>MiniMax 构建了覆盖国内及海外业务的统一日志可观测中台</strong>。 以 SelectDB 独立负责所有日志的存储与查询分析，实现了 “<strong>一个平台，全球覆盖</strong>” 。这彻底终结了以往为不同业务集群分散部署、独立运维多套系统的复杂局面，在架构上实现了极大的简化。</p><p>阿里云数据库 SelectDB 在 MiniMax 的成功实践足以说明：<strong>SelectDB 能够很好地满足 AI 时代海量数据实时处理与分析的需求</strong>。不仅为 MiniMax 自身业务的高效运营提供了坚实保障，也为广大面临类似日志处理挑战的 AI 大模型企业，提供了一个<a href="https://link.segmentfault.com/?enc=OU7VkeVqH5y4qUMHob40Lw%3D%3D.bwRUs44Ig3lPxywgrejrpOyQgjWEw1xLaQqPeou8FYxmwUzFfBtiMmXgmcFukO90NT%2BTkRwqt2Ih9nTXerLpzravA1kitQ4eE6CahJwGMyNw0f9JLGd%2FlLhbzEt5iAsaamUuTJkcFW8Bq1aaL5tBkdtnFryXLaHUAZEspMzxHi9bKt29EsS1CfgqDYfYksgo" rel="nofollow" target="_blank">高性能、低成本的可靠技术解决方案</a>。</p><p>不止于此， 面对大模型与多模态 AI 的快速发展，SelectDB 已从被动存储分析向主动智能分析演进。目前，SelectDB 已具备 AI 原生支持能力，深度融合<a href="https://link.segmentfault.com/?enc=dolylvKswaMECVMW%2BEnKPQ%3D%3D.aHT1CZz3f10WyewZP1sTlGCB1MOGkdY2INF2VqzHJQiMP2HP%2FQD4roz3293GwcAS7d1mBCqfWf57bmCBgudNeNTam5deLbVEvOSd3S2u%2BSdVF9Kwlni8CXFJS4Ci32ANABvSsTWTVdqD3Rz27RtcttdKqV4qvBoRNvgEMyWXWl9CXvQTgD3sEal04WPoC83p" rel="nofollow" target="_blank">向量索引</a>、文本搜索与结构化分析能力，实现高效的<a href="https://link.segmentfault.com/?enc=DtGP1seusPwYwfCDilAg6g%3D%3D.Py0lUJIMFJ4TxZQTNlXyi%2BYZsSKd4zwM2S0y%2F4WmbQCrwSp%2BDOP6DumZBYe4cOXWq1cJvSmQRdEEOGHnTV25xaJaTl6DexPlGHVmoowUxDajUoYom01IKgQImbmxqqWmDbnORiSnBb%2B01w3kULAJ58i0YiQW8sxxw%2FKAJvoepmjbdGG7%2FEjPY1EKJNbtEisu" rel="nofollow" target="_blank">混合检索</a>，显著提升结果相关性、实时性与准确性。更进一步，SelectDB 内置 <a href="https://link.segmentfault.com/?enc=gW18ZNMzEsaTGakRFRG%2Fiw%3D%3D.ZXG1WtX74B46Dyew0HGH0KSCHZwTJqwhyz61V8cUp4zeG1JkRFTVBMzdMXoa5MLKkYzkHVrjK7U3A4p6eyiy7Yda4UpAAaNmCHiD200nJ5FWiY7wldUo6G9xhbxInHWdk0bDFq80X8tcLxU5sI8VVF9xFE%2FGj2sNU8EtXG0SKXPzONFaP4ANsVeMyY0OhVaD" rel="nofollow" target="_blank">AI 函数</a>（如语义理解、特征提取）并支持<a href="https://link.segmentfault.com/?enc=e7H1Dw2LVv9CRdYyxNdP2Q%3D%3D.A4fYcX9ZLsH%2F6TcyHzUuYyy7LZIXmKaUgI5uy1Z3WvbUOiNMUkLFefdUXRtSiU%2FlN33NFOVN1QtK%2BaKxQa3CkBZl2ItTtin1nSwAZjMjKjsX7%2Bto57x1LY3ogtBRjpxIG0JwsmXpnzJhYuujtujFgCtlGK8fcD8YUdKidYVhPYzN%2FFdAQ6RiRCF4cp9yQCm9" rel="nofollow" target="_blank">基于 MCP 的 Agent 分析接口</a>，可直接升级为企业的 “AI 分析中枢” ，为业务智能决策与创新提供稳定、高效的数据底座。</p>]]></description></item><item>    <title><![CDATA[技术分享 | MySQL8.0物理备份Xtrabackup全备脚本 墨天轮 ]]></title>    <link>https://segmentfault.com/a/1190000047575010</link>    <guid>https://segmentfault.com/a/1190000047575010</guid>    <pubDate>2026-01-27 13:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=TEd7DFcBb0d6Th6BbxZt9Q%3D%3D.YEQW2KJ%2FU%2BJAJVNyve2oZjEosX2qlLY%2BzQcycrJcJ7vzZdgZApSz8bFT1ITc%2BjTZ" rel="nofollow" target="_blank">墨天轮数据库管理服务团队</a>第159期技术分享，内容原创，作者为技术顾问<strong>闫建</strong>，如需转载请联系小墨（VX：modb666）并注明来源。如需查看更多文章可关注【墨天轮】公众号。</p><h2><strong>脚本功能</strong></h2><p>此脚本是专门用于MySQL8.0物理备份的xtrabackup脚本,并且xtrabackup版本为8.0的最新版，它包含了备份数据库（包含全量备份，错误处理，日志记录，自动清理，耗时统计）的完整配置。</p><h2><strong>脚本内容</strong></h2><p>该脚本名称为xtrabackup8.0\_full.sh</p><pre><code class="sql">#!/bin/bash
# ===============================================
# MySQL XtraBackup 全量备份脚本 (默认未启用压缩) 
# 功能：全量备份 + 错误处理 + 日志记录 + 自动清理 + 耗时统计
# 版本：1.0
# 日期：2025-11-04
# ===============================================
# -------------------------------
# 可配置变量（修改此处适应环境）
# -------------------------------
# 基础路径
BACKUP_BASE_DIR="/data/backup"                 # 备份根目录
LOG_DIR="${BACKUP_BASE_DIR}/logs"              # 日志目录
FULL_BACKUP_DIR="${BACKUP_BASE_DIR}/full"      # 全量备份目录
# MySQL 连接配置
MYSQL_USER="root"                              # 备份专用用户（需提前授权）
MYSQL_PASSWORD="mysql"                         # 用户密码
MYSQL_SOCKET="/data/mysql/3306/run/mysql.sock" # MySQL Socket路径
MYSQL_CNF="/etc/my.cnf"                        # MySQL配置文件路径
# 备份参数
RETENTION_DAYS=7                               # 备份保留天数（默认7天）
COMPRESS_ENABLED=0                             # 是否启用压缩（0-禁用，1-启用，默认禁用）
COMPRESS_ALGORITHM="zstd"                      # 压缩算法（zstd/lz4/quicklz，默认zstd，如果采用zstd压缩方式，在解压的时候建议在OS上检查是否安装 zstd软件，如未安装，请提前安装yum install zstd）
COMPRESS_THREADS=4                             # 压缩线程数（默认4）
PARALLEL=4                                     # 备份并行线程数（默认4）
# XtraBackup路径（若不在PATH中，需指定完整路径，注意：该路径需要提前修改为符合实际环境的绝对路径）
XTRABACKUP_PATH="/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup"
# -------------------------------
# 函数定义
# -------------------------------
# 日志记录函数
log_message() {
    local level="$1"
    local message="$2"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    local log_entry="[${timestamp}] [${level}] ${message}"
    # 输出到标准输出并写入日志文件
    echo "${log_entry}" | tee -a "${CURRENT_LOG_FILE}"
}
# 错误处理函数
error_exit() {
    log_message "ERROR" "脚本执行失败: $1"
    exit 1
}
# 计算和格式化时间差函数
calculate_duration() {
    local start_seconds="$1"
    local end_seconds="$2"
    local total_seconds=$((end_seconds - start_seconds))
    # 转换为天、小时、分钟、秒的易读格式
    local days=$((total_seconds / 86400))
    local hours=$(( (total_seconds % 86400) / 3600 ))
    local minutes=$(( (total_seconds % 3600) / 60 ))
    local seconds=$((total_seconds % 60))
    local duration_str=""
    if [ $days -gt 0 ]; then
        duration_str="${days}天"
    fi
    if [ $hours -gt 0 ] || [ -n "$duration_str" ]; then
        duration_str="${duration_str}${hours}小时"
    fi
    if [ $minutes -gt 0 ] || [ -n "$duration_str" ]; then
        duration_str="${duration_str}${minutes}分钟"
    fi
    duration_str="${duration_str}${seconds}秒"
    echo "$duration_str"
}
# 清理旧备份函数
cleanup_old_backups() {
    log_message "INFO" "开始清理超过 ${RETENTION_DAYS} 天的旧备份..."
    local deleted_count=0
    # 删除过期全量备份及相关日志
    if find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} | grep -q .; then
        find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} -exec rm -rf {} \;
        deleted_count=$(find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} | wc -l)
        log_message "INFO" "已清理 ${deleted_count} 个过期全量备份"
    else
        log_message "INFO" "未找到需要清理的过期备份"
    fi
    # 清理旧日志文件（保留30天）
    find "${LOG_DIR}" -name "backup_*.log" -type f -mtime +30 -delete
}
# 检查依赖项
check_dependencies() {
    local deps=("${XTRABACKUP_PATH}" "mysql")
    for cmd in "${deps[@]}"; do
        if ! command -v "${cmd}" &amp;&gt; /dev/null; then
            error_exit "未找到所需命令: ${cmd}，请确保已安装并配置PATH"
        fi
    done
    # 检查备份目录权限
    if [ ! -w "${BACKUP_BASE_DIR}" ]; then
        error_exit "备份目录不可写: ${BACKUP_BASE_DIR}"
    fi
}
# 备份前置检查
pre_backup_checks() {
    log_message "INFO" "开始备份前置检查..."
    # 检查MySQL连接
    if ! mysql --user="${MYSQL_USER}" --password="${MYSQL_PASSWORD}" --socket="${MYSQL_SOCKET}" -e "SELECT 1;" &amp;&gt; /dev/null; then
        error_exit "MySQL连接测试失败，请检查凭据和Socket路径"
    fi
    # 检查XtraBackup版本
    local xtrabackup_version
    xtrabackup_version=$(${XTRABACKUP_PATH} --version 2&gt;&amp;1 | tail -n1 || echo "未知")
    log_message "INFO" "使用XtraBackup版本: ${xtrabackup_version}"
    # 检查磁盘空间（至少保留10GB）
    local available_space
    available_space=$(df "${BACKUP_BASE_DIR}" | awk 'NR==2 {print $4}')
    if [ "${available_space}" -lt 10485760 ]; then  # 10GB in KB
        error_exit "磁盘空间不足10GB，当前可用: ${available_space}KB"
    fi
}
# -------------------------------
# 主脚本逻辑
# -------------------------------
main() {
    # 初始化变量
    local backup_timestamp=$(date '+%Y%m%d_%H%M%S')
    local backup_name="full_${backup_timestamp}"
    local current_backup_dir="${FULL_BACKUP_DIR}/${backup_name}"
    CURRENT_LOG_FILE="${LOG_DIR}/backup_${backup_timestamp}.log"
    # 记录备份开始时间（秒级时间戳）[6,8](@ref)
    local backup_start_time=$(date +%s)
    local backup_start_readable=$(date '+%Y-%m-%d %H:%M:%S')
    # 创建目录
    mkdir -p "${FULL_BACKUP_DIR}" "${LOG_DIR}"
    log_message "INFO" "=== MySQL全量备份开始 ==="
    log_message "INFO" "备份开始时间: ${backup_start_readable}"
    log_message "INFO" "备份名称: ${backup_name}"
    log_message "INFO" "备份目录: ${current_backup_dir}"
    log_message "INFO" "保留天数: ${RETENTION_DAYS}"
#    log_message "INFO" "压缩启用: ${COMPRESS_ENABLED} (算法: ${COMPRESS_ALGORITHM}, 线程: ${COMPRESS_THREADS})"
    # 执行检查
    check_dependencies
    pre_backup_checks
    # 构建备份命令
    local backup_cmd="${XTRABACKUP_PATH} --defaults-file=${MYSQL_CNF} --backup"
    backup_cmd+=" --target-dir=${current_backup_dir}"
    backup_cmd+=" --user=${MYSQL_USER} --password=${MYSQL_PASSWORD}"
    backup_cmd+=" --socket=${MYSQL_SOCKET} --parallel=${PARALLEL}"
    # 压缩配置
    if [ "${COMPRESS_ENABLED}" -eq 1 ]; then
        log_message "INFO" "启用压缩算法: ${COMPRESS_ALGORITHM}, 线程数: ${COMPRESS_THREADS}"
        backup_cmd+=" --compress=${COMPRESS_ALGORITHM} --compress-threads=${COMPRESS_THREADS}"
    else
        log_message "INFO" "备份未启用压缩"
    fi
    # 执行备份
    log_message "INFO" "开始执行 XtraBackup 全量备份..."
    log_message "DEBUG" "备份命令: ${backup_cmd//--password=* /--password=*** }"  # 屏蔽密码显示
    # 记录备份操作开始时间
    local backup_operation_start=$(date +%s)
    if eval "${backup_cmd}" &gt;&gt; "${CURRENT_LOG_FILE}" 2&gt;&amp;1; then
        local backup_operation_end=$(date +%s)
        local backup_duration=$(calculate_duration $backup_operation_start $backup_operation_end)
        log_message "INFO" "XtraBackup全量备份完成，备份操作耗时: ${backup_duration}"
        # 验证备份完整性
        if [ -f "${current_backup_dir}/xtrabackup_checkpoints" ]; then
            local backup_type
            backup_type=$(grep "backup_type" "${current_backup_dir}/xtrabackup_checkpoints" | cut -d= -f2)
            log_message "INFO" "备份类型验证: ${backup_type}"
        else
            error_exit "备份完整性检查失败: xtrabackup_checkpoints文件缺失"
        fi
    else
        error_exit "XtraBackup备份过程失败，请检查日志: ${CURRENT_LOG_FILE}"
    fi
    # 自动清理旧备份
    cleanup_old_backups
    # 计算总耗时
    local backup_end_time=$(date +%s)
    local backup_end_readable=$(date '+%Y-%m-%d %H:%M:%S')
    local total_duration=$(calculate_duration $backup_start_time $backup_end_time)
    # 计算备份大小
    local backup_size
    backup_size=$(du -sh "${current_backup_dir}" | awk '{print $1}')
    log_message "INFO" "备份完成: ${backup_name} (大小: ${backup_size})"
    log_message "INFO" "备份开始: ${backup_start_readable}"
    log_message "INFO" "备份结束: ${backup_end_readable}"
    log_message "INFO" "备份总耗时: ${total_duration}"
    log_message "INFO" "备份日志: ${CURRENT_LOG_FILE}"
    log_message "INFO" "=== MySQL全量备份结束 ==="
}
# 异常处理（捕获中断信号）
trap 'log_message "ERROR" "脚本被用户中断"; exit 2;' INT TERM
# 脚本入口点
main "$@"</code></pre><h2><strong>重点说明</strong></h2><p>在脚本最开始部分需要根据实际数据库环境来配置，包括用户名，密码，路径等详细信息，参考如下：</p><pre><code class="sql">BACKUP_BASE_DIR="/data/backup"                 # 备份根目录
LOG_DIR="${BACKUP_BASE_DIR}/logs"              # 日志目录
FULL_BACKUP_DIR="${BACKUP_BASE_DIR}/full"      # 全量备份目录
# MySQL 连接配置
MYSQL_USER="root"                              # 备份专用用户（需提前授权）
MYSQL_PASSWORD="mysql"                         # 用户密码
MYSQL_SOCKET="/data/mysql/3306/run/mysql.sock" # MySQL Socket路径
MYSQL_CNF="/etc/my.cnf"                        # MySQL配置文件路径
# 备份参数
RETENTION_DAYS=7                               # 备份保留天数（默认7天）
COMPRESS_ENABLED=0                             # 是否启用压缩（0-禁用，1-启用，默认禁用）
COMPRESS_ALGORITHM="zstd"                      # 压缩算法（zstd/lz4/quicklz，默认zstd，如果采用zstd压缩方式，在解压的时候建议在OS上检查是否安装 zstd软件，如未安装，请提前安装yum install zstd）
COMPRESS_THREADS=4                             # 压缩线程数（默认4）
PARALLEL=4                                     # 备份并行线程数（默认4）
# XtraBackup路径（若不在PATH中，需指定完整路径，注意：该路径需要提前修改为符合实际环境的绝对路径）
XTRABACKUP_PATH="/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup"</code></pre><h2><strong>使用方法</strong></h2><ol><li><p>保脚本并赋予执行权限</p><pre><code class="sql">chmod +x xtrabackup8.0_full.sh</code></pre></li><li>手动执行备份</li></ol><pre><code class="sql">[root@VM-8-4-opencloudos backup]# ./xtrabackup8.0_full.sh 
[2025-11-04 17:34:34] [INFO] === MySQL全量备份开始 ===
[2025-11-04 17:34:34] [INFO] 备份名称: full_20251104_173434
[2025-11-04 17:34:34] [INFO] 备份目录: /data/backup/full/full_20251104_173434
[2025-11-04 17:34:34] [INFO] 保留天数: 7
[2025-11-04 17:34:34] [INFO] 开始备份前置检查...
[2025-11-04 17:34:35] [INFO] 使用XtraBackup版本: /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
[2025-11-04 17:34:35] [INFO] 启用压缩算法: zstd, 线程数: 4
[2025-11-04 17:34:35] [INFO] 开始执行XtraBackup全量备份...
[2025-11-04 17:34:35] [DEBUG] 备份命令: /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --defaults-file=/etc/my.cnf --backup --target-dir=/data/backup/full/full_20251104_173434 --user=root --password=*** --compress-threads=4
[2025-11-04 17:34:41] [INFO] XtraBackup全量备份完成
[2025-11-04 17:34:41] [INFO] 备份类型验证:  full-backuped
[2025-11-04 17:34:41] [INFO] 开始清理超过 7 天的旧备份...
[2025-11-04 17:34:41] [INFO] 未找到需要清理的过期备份
[2025-11-04 17:34:41] [INFO] 备份完成: full_20251104_173434 (大小: 43M)
[2025-11-04 17:34:41] [INFO] 备份日志: /data/backup/logs/backup_20251104_173434.log
[2025-11-04 17:34:41] [INFO] === MySQL全量备份结束 ===
[root@VM-8-4-opencloudos backup]# </code></pre><ol><li>配置定时任务（每日凌晨1点执行）</li></ol><pre><code class="sql"># 编辑crontab：crontab -e 添加如下内容并保存
0 1 * * * /path/to/xtrabackup8.0_full.sh </code></pre><h2><strong>恢 复</strong></h2><p>1、解压缩（如采用压缩备份，此步骤为必须执行步骤，非压缩备份，此步骤忽略）</p><pre><code class="sql">/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --decompress --remove-original --target-dir=/data/backup/full/full_20251104_173434
2025-11-04T17:40:22.986591+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized server arguments: --server-id=13045 --innodb_io_capacity=2000 --datadir=/data/mysql/3306/data --log_bin=/data/mysql/3306/binlogs/mysql-bin --tmpdir=/data/mysql/3306/tmp --innodb_buffer_pool_size=1G --innodb_data_file_path=ibdata1:200M;ibdata2:200M:autoextend --innodb_flush_method=O_DIRECT --innodb_adaptive_hash_index=0 
2025-11-04T17:40:22.986824+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized client arguments: --port=3306 --socket=/data/mysql/3306/run/mysql.sock --decompress=1 --remove-original=1 --target-dir=/data/backup/full/full_20251104_173434 
/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
2025-11-04T17:40:22.988436+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_logfile.zst
2025-11-04T17:40:23.000940+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_logfile.zst
2025-11-04T17:40:23.001025+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ibdata1.zst
2025-11-04T17:40:23.330061+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ibdata1.zst
2025-11-04T17:40:23.344696+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ibdata2.zst
2025-11-04T17:40:23.903032+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ibdata2.zst
2025-11-04T17:40:23.903125+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/large_table.ibd.zst
2025-11-04T17:40:24.496119+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/large_table.ibd.zst
2025-11-04T17:40:24.594530+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/my_table.ibd.zst
2025-11-04T17:40:24.608587+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/my_table.ibd.zst
2025-11-04T17:40:24.616871+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/tt_new.ibd.zst
2025-11-04T17:40:24.629940+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/tt_new.ibd.zst
2025-11-04T17:40:24.630190+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/tt.ibd.zst
2025-11-04T17:40:24.635994+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/tt.ibd.zst
2025-11-04T17:40:24.636477+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/a.ibd.zst
2025-11-04T17:40:24.643492+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/a.ibd.zst
2025-11-04T17:40:24.644126+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/b.ibd.zst
2025-11-04T17:40:24.650521+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/b.ibd.zst
2025-11-04T17:40:24.650961+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./sys/sys_config.ibd.zst
2025-11-04T17:40:24.655719+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./sys/sys_config.ibd.zst
2025-11-04T17:40:24.656597+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql.ibd.zst
2025-11-04T17:40:24.695278+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql.ibd.zst
2025-11-04T17:40:24.699656+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./undo_002.zst
2025-11-04T17:40:24.722718+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./undo_002.zst
2025-11-04T17:40:24.722825+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./undo_001.zst
2025-11-04T17:40:24.755290+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./undo_001.zst
2025-11-04T17:40:24.755392+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log_213.sdi.zst
2025-11-04T17:40:24.760715+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log_213.sdi.zst
2025-11-04T17:40:24.760830+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log.CSM.zst
2025-11-04T17:40:24.766136+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log.CSM.zst
2025-11-04T17:40:24.766202+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log.CSV.zst
2025-11-04T17:40:24.771030+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log.CSV.zst
2025-11-04T17:40:24.771184+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log_214.sdi.zst
2025-11-04T17:40:24.776180+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log_214.sdi.zst
2025-11-04T17:40:24.776240+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log.CSM.zst
2025-11-04T17:40:24.781393+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log.CSM.zst
2025-11-04T17:40:24.781449+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log.CSV.zst
2025-11-04T17:40:24.785794+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log.CSV.zst
2025-11-04T17:40:24.785890+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/cond_instances_82.sdi.zst
2025-11-04T17:40:24.790641+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/cond_instances_82.sdi.zst
2025-11-04T17:40:24.790708+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/error_log_83.sdi.zst
2025-11-04T17:40:24.795417+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/error_log_83.sdi.zst
2025-11-04T17:40:24.795508+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_his_85.sdi.zst
2025-11-04T17:40:24.800525+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_his_85.sdi.zst
2025-11-04T17:40:24.800589+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_cur_84.sdi.zst
2025-11-04T17:40:24.804941+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_cur_84.sdi.zst
2025-11-04T17:40:24.805008+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_his_86.sdi.zst
2025-11-04T17:40:24.809624+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_his_86.sdi.zst
2025-11-04T17:40:24.809685+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_87.sdi.zst
2025-11-04T17:40:24.814165+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_87.sdi.zst
2025-11-04T17:40:24.814218+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_88.sdi.zst
2025-11-04T17:40:24.818631+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_88.sdi.zst
2025-11-04T17:40:24.818700+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_89.sdi.zst
2025-11-04T17:40:24.823346+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_89.sdi.zst
2025-11-04T17:40:24.823434+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_90.sdi.zst
2025-11-04T17:40:24.828325+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_90.sdi.zst
2025-11-04T17:40:24.828459+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_91.sdi.zst
2025-11-04T17:40:24.832657+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_91.sdi.zst
2025-11-04T17:40:24.832719+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_92.sdi.zst
2025-11-04T17:40:24.837125+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_92.sdi.zst
2025-11-04T17:40:24.837186+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_instances_93.sdi.zst
2025-11-04T17:40:24.841260+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_instances_93.sdi.zst
2025-11-04T17:40:24.841325+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_summary_by__94.sdi.zst
2025-11-04T17:40:24.845642+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_summary_by__94.sdi.zst
2025-11-04T17:40:24.845708+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_summary_by__95.sdi.zst
2025-11-04T17:40:24.850048+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_summary_by__95.sdi.zst
2025-11-04T17:40:24.850109+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/host_cache_96.sdi.zst
2025-11-04T17:40:24.854740+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/host_cache_96.sdi.zst
2025-11-04T17:40:24.854804+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/mutex_instances_97.sdi.zst
2025-11-04T17:40:24.859652+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/mutex_instances_97.sdi.zst
2025-11-04T17:40:24.859725+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/objects_summary__98.sdi.zst
2025-11-04T17:40:24.864826+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/objects_summary__98.sdi.zst
2025-11-04T17:40:24.864905+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/performance_time_99.sdi.zst
2025-11-04T17:40:24.869709+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/performance_time_99.sdi.zst
2025-11-04T17:40:24.869779+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/rwlock_instances_101.sdi.zst
2025-11-04T17:40:24.874756+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/rwlock_instances_101.sdi.zst
2025-11-04T17:40:24.874869+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/processlist_100.sdi.zst
2025-11-04T17:40:24.879190+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/processlist_100.sdi.zst
2025-11-04T17:40:24.879289+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_actors_102.sdi.zst
2025-11-04T17:40:24.886036+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_actors_102.sdi.zst
2025-11-04T17:40:24.886108+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_consumers_103.sdi.zst
2025-11-04T17:40:24.890831+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_consumers_103.sdi.zst
2025-11-04T17:40:24.890957+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_instrument_104.sdi.zst
2025-11-04T17:40:24.895561+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_instrument_104.sdi.zst
2025-11-04T17:40:24.895670+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_objects_105.sdi.zst
2025-11-04T17:40:24.899829+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_objects_105.sdi.zst
2025-11-04T17:40:24.899946+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_threads_106.sdi.zst
2025-11-04T17:40:24.904601+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_threads_106.sdi.zst
2025-11-04T17:40:24.904658+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_io_waits_s_107.sdi.zst
2025-11-04T17:40:24.909639+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_io_waits_s_107.sdi.zst
2025-11-04T17:40:24.909709+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_io_waits_s_108.sdi.zst
2025-11-04T17:40:24.914081+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_io_waits_s_108.sdi.zst
2025-11-04T17:40:24.914148+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_lock_waits_109.sdi.zst
2025-11-04T17:40:24.918343+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_lock_waits_109.sdi.zst
2025-11-04T17:40:24.918421+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/threads_110.sdi.zst
2025-11-04T17:40:24.922998+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/threads_110.sdi.zst
2025-11-04T17:40:24.923064+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_cu_111.sdi.zst
2025-11-04T17:40:24.927213+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_cu_111.sdi.zst
2025-11-04T17:40:24.927276+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_hi_112.sdi.zst
2025-11-04T17:40:24.932218+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_hi_112.sdi.zst
2025-11-04T17:40:24.932280+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_hi_113.sdi.zst
2025-11-04T17:40:24.937361+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_hi_113.sdi.zst
2025-11-04T17:40:24.937457+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_114.sdi.zst
2025-11-04T17:40:24.941918+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_114.sdi.zst
2025-11-04T17:40:24.941991+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_115.sdi.zst
2025-11-04T17:40:24.946597+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_115.sdi.zst
2025-11-04T17:40:24.946667+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_116.sdi.zst
2025-11-04T17:40:24.951119+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_116.sdi.zst
2025-11-04T17:40:24.951190+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_117.sdi.zst
2025-11-04T17:40:24.956165+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_117.sdi.zst
2025-11-04T17:40:24.956236+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_118.sdi.zst
2025-11-04T17:40:24.960626+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_118.sdi.zst
2025-11-04T17:40:24.960698+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_119.sdi.zst
2025-11-04T17:40:24.964982+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_119.sdi.zst
2025-11-04T17:40:24.965041+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_120.sdi.zst
2025-11-04T17:40:24.969978+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_120.sdi.zst
2025-11-04T17:40:24.970090+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_121.sdi.zst
2025-11-04T17:40:24.974550+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_121.sdi.zst
2025-11-04T17:40:24.974613+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_122.sdi.zst
2025-11-04T17:40:24.978736+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_122.sdi.zst
2025-11-04T17:40:24.978800+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_123.sdi.zst
2025-11-04T17:40:24.987207+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_123.sdi.zst
2025-11-04T17:40:24.987434+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_124.sdi.zst
2025-11-04T17:40:25.012604+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_124.sdi.zst
2025-11-04T17:40:25.013300+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_125.sdi.zst
2025-11-04T17:40:25.019070+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_125.sdi.zst
2025-11-04T17:40:25.019146+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_127.sdi.zst
2025-11-04T17:40:25.025291+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_127.sdi.zst
2025-11-04T17:40:25.025419+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_126.sdi.zst
2025-11-04T17:40:25.031834+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_126.sdi.zst
2025-11-04T17:40:25.031916+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_128.sdi.zst
2025-11-04T17:40:25.036534+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_128.sdi.zst
2025-11-04T17:40:25.036608+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_129.sdi.zst
2025-11-04T17:40:25.041247+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_129.sdi.zst
2025-11-04T17:40:25.041317+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_130.sdi.zst
2025-11-04T17:40:25.045871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_130.sdi.zst
2025-11-04T17:40:25.045942+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_131.sdi.zst
2025-11-04T17:40:25.050464+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_131.sdi.zst
2025-11-04T17:40:25.050536+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_132.sdi.zst
2025-11-04T17:40:25.055547+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_132.sdi.zst
2025-11-04T17:40:25.055674+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_133.sdi.zst
2025-11-04T17:40:25.060633+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_133.sdi.zst
2025-11-04T17:40:25.060754+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_134.sdi.zst
2025-11-04T17:40:25.065871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_134.sdi.zst
2025-11-04T17:40:25.065989+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_135.sdi.zst
2025-11-04T17:40:25.071009+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_135.sdi.zst
2025-11-04T17:40:25.071126+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_136.sdi.zst
2025-11-04T17:40:25.075979+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_136.sdi.zst
2025-11-04T17:40:25.076093+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_137.sdi.zst
2025-11-04T17:40:25.080663+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_137.sdi.zst
2025-11-04T17:40:25.080721+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_138.sdi.zst
2025-11-04T17:40:25.084942+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_138.sdi.zst
2025-11-04T17:40:25.085063+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_139.sdi.zst
2025-11-04T17:40:25.089051+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_139.sdi.zst
2025-11-04T17:40:25.089176+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_140.sdi.zst
2025-11-04T17:40:25.093221+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_140.sdi.zst
2025-11-04T17:40:25.093344+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_141.sdi.zst
2025-11-04T17:40:25.097822+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_141.sdi.zst
2025-11-04T17:40:25.097901+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_142.sdi.zst
2025-11-04T17:40:25.102195+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_142.sdi.zst
2025-11-04T17:40:25.102260+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_143.sdi.zst
2025-11-04T17:40:25.106506+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_143.sdi.zst
2025-11-04T17:40:25.106581+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/users_144.sdi.zst
2025-11-04T17:40:25.111263+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/users_144.sdi.zst
2025-11-04T17:40:25.111318+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/accounts_145.sdi.zst
2025-11-04T17:40:25.115990+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/accounts_145.sdi.zst
2025-11-04T17:40:25.116105+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/hosts_146.sdi.zst
2025-11-04T17:40:25.120821+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/hosts_146.sdi.zst
2025-11-04T17:40:25.120958+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_instances_147.sdi.zst
2025-11-04T17:40:25.124883+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_instances_147.sdi.zst
2025-11-04T17:40:25.125002+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_summary_b_148.sdi.zst
2025-11-04T17:40:25.129788+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_summary_b_148.sdi.zst
2025-11-04T17:40:25.129929+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_summary_b_149.sdi.zst
2025-11-04T17:40:25.134033+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_summary_b_149.sdi.zst
2025-11-04T17:40:25.134151+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_connect__150.sdi.zst
2025-11-04T17:40:25.138105+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_connect__150.sdi.zst
2025-11-04T17:40:25.138158+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_account__151.sdi.zst
2025-11-04T17:40:25.142150+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_account__151.sdi.zst
2025-11-04T17:40:25.142285+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/keyring_keys_152.sdi.zst
2025-11-04T17:40:25.146409+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/keyring_keys_152.sdi.zst
2025-11-04T17:40:25.146473+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_g_153.sdi.zst
2025-11-04T17:40:25.151175+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_g_153.sdi.zst
2025-11-04T17:40:25.151298+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_154.sdi.zst
2025-11-04T17:40:25.155247+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_154.sdi.zst
2025-11-04T17:40:25.155535+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_155.sdi.zst
2025-11-04T17:40:25.159725+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_155.sdi.zst
2025-11-04T17:40:25.159776+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_156.sdi.zst
2025-11-04T17:40:25.164259+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_156.sdi.zst
2025-11-04T17:40:25.164397+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_157.sdi.zst
2025-11-04T17:40:25.168750+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_157.sdi.zst
2025-11-04T17:40:25.168825+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_handles_158.sdi.zst
2025-11-04T17:40:25.173560+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_handles_158.sdi.zst
2025-11-04T17:40:25.173620+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/metadata_locks_159.sdi.zst
2025-11-04T17:40:25.177949+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/metadata_locks_159.sdi.zst
2025-11-04T17:40:25.178012+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/data_locks_160.sdi.zst
2025-11-04T17:40:25.182902+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/data_locks_160.sdi.zst
2025-11-04T17:40:25.182965+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/data_lock_waits_161.sdi.zst
2025-11-04T17:40:25.187544+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/data_lock_waits_161.sdi.zst
2025-11-04T17:40:25.187609+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_conn_162.sdi.zst
2025-11-04T17:40:25.192416+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_conn_162.sdi.zst
2025-11-04T17:40:25.192485+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_grou_163.sdi.zst
2025-11-04T17:40:25.197412+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_grou_163.sdi.zst
2025-11-04T17:40:25.197478+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_conn_164.sdi.zst
2025-11-04T17:40:25.201947+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_conn_164.sdi.zst
2025-11-04T17:40:25.202015+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_165.sdi.zst
2025-11-04T17:40:25.206022+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_165.sdi.zst
2025-11-04T17:40:25.206082+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_166.sdi.zst
2025-11-04T17:40:25.210440+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_166.sdi.zst
2025-11-04T17:40:25.210503+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_167.sdi.zst
2025-11-04T17:40:25.214871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_167.sdi.zst
2025-11-04T17:40:25.214936+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_168.sdi.zst
2025-11-04T17:40:25.219740+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_168.sdi.zst
2025-11-04T17:40:25.219814+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_grou_169.sdi.zst
2025-11-04T17:40:25.224172+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_grou_169.sdi.zst
2025-11-04T17:40:25.224245+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_170.sdi.zst
2025-11-04T17:40:25.228351+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_170.sdi.zst
2025-11-04T17:40:25.228468+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_171.sdi.zst
2025-11-04T17:40:25.233203+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_171.sdi.zst
2025-11-04T17:40:25.233319+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_asyn_172.sdi.zst
2025-11-04T17:40:25.238933+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_asyn_172.sdi.zst
2025-11-04T17:40:25.239094+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_asyn_173.sdi.zst
2025-11-04T17:40:25.246187+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_asyn_173.sdi.zst
2025-11-04T17:40:25.246284+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/log_status_174.sdi.zst
2025-11-04T17:40:25.253874+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/log_status_174.sdi.zst
2025-11-04T17:40:25.253967+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/prepared_stateme_175.sdi.zst
2025-11-04T17:40:25.272402+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/prepared_stateme_175.sdi.zst
2025-11-04T17:40:25.272496+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/user_variables_b_176.sdi.zst
2025-11-04T17:40:25.280533+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/user_variables_b_176.sdi.zst
2025-11-04T17:40:25.280617+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_accoun_177.sdi.zst
2025-11-04T17:40:25.288632+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_accoun_177.sdi.zst
2025-11-04T17:40:25.288727+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_host_178.sdi.zst
2025-11-04T17:40:25.296271+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_host_178.sdi.zst
2025-11-04T17:40:25.296471+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_thread_179.sdi.zst
2025-11-04T17:40:25.303218+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_thread_179.sdi.zst
2025-11-04T17:40:25.303416+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_user_180.sdi.zst
2025-11-04T17:40:25.315732+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_user_180.sdi.zst
2025-11-04T17:40:25.315917+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/global_status_181.sdi.zst
2025-11-04T17:40:25.321558+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/global_status_181.sdi.zst
2025-11-04T17:40:25.321637+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_status_182.sdi.zst
2025-11-04T17:40:25.325961+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_status_182.sdi.zst
2025-11-04T17:40:25.326037+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/variables_by_thr_183.sdi.zst
2025-11-04T17:40:25.330807+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/variables_by_thr_183.sdi.zst
2025-11-04T17:40:25.330892+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/global_variables_184.sdi.zst
2025-11-04T17:40:25.335674+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/global_variables_184.sdi.zst
2025-11-04T17:40:25.335749+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_variable_185.sdi.zst
2025-11-04T17:40:25.340530+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_variable_185.sdi.zst
2025-11-04T17:40:25.340655+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/variables_info_186.sdi.zst
2025-11-04T17:40:25.345478+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/variables_info_186.sdi.zst
2025-11-04T17:40:25.345584+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/persisted_variab_187.sdi.zst
2025-11-04T17:40:25.349818+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/persisted_variab_187.sdi.zst
2025-11-04T17:40:25.349892+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/user_defined_fun_188.sdi.zst
2025-11-04T17:40:25.354039+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/user_defined_fun_188.sdi.zst
2025-11-04T17:40:25.354102+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/binary_log_trans_189.sdi.zst
2025-11-04T17:40:25.358649+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/binary_log_trans_189.sdi.zst
2025-11-04T17:40:25.358752+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/tls_channel_stat_190.sdi.zst
2025-11-04T17:40:25.362754+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/tls_channel_stat_190.sdi.zst
2025-11-04T17:40:25.362826+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/keyring_componen_191.sdi.zst
2025-11-04T17:40:25.366995+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/keyring_componen_191.sdi.zst
2025-11-04T17:40:25.367058+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/clone_status_406.sdi.zst
2025-11-04T17:40:25.371839+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/clone_status_406.sdi.zst
2025-11-04T17:40:25.371910+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/clone_progress_407.sdi.zst
2025-11-04T17:40:25.376354+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/clone_progress_407.sdi.zst
2025-11-04T17:40:25.376483+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql-bin.000025.zst
2025-11-04T17:40:25.380522+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql-bin.000025.zst
2025-11-04T17:40:25.380581+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql-bin.index.zst
2025-11-04T17:40:25.384675+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql-bin.index.zst
2025-11-04T17:40:25.384736+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_binlog_info.zst
2025-11-04T17:40:25.389292+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_binlog_info.zst
2025-11-04T17:40:25.389350+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ib_buffer_pool.zst
2025-11-04T17:40:25.393580+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ib_buffer_pool.zst
2025-11-04T17:40:25.393640+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./backup-my.cnf.zst
2025-11-04T17:40:25.398334+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./backup-my.cnf.zst
2025-11-04T17:40:25.398411+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_info.zst
2025-11-04T17:40:25.402541+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_info.zst
2025-11-04T17:40:25.402600+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_tablespaces.zst
2025-11-04T17:40:25.407024+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_tablespaces.zst
2025-11-04T17:40:25.510589+08:00 0 [Note] [MY-011825] [Xtrabackup] completed OK! </code></pre><p>2、prepare步骤</p><p>xtrabackup --prepare --target-dir=/data/backup/full/full\_20251104\_173434</p><p>prepare是物理备份过程中进行恢复前必做的一个环节，它的作用是将备份时处于不一致状态的数据文件，处理成一个具有数据一致性的、可供数据库直接启动和使用的完整备份集。</p><p>简单来说，如果不执行 --prepare步骤，直接使用 --backup得到的原始备份文件来启动数据库，InnoDB存储引擎会检测到数据文件内部不一致（例如页面LSN不匹配），并将其视为损坏的数据文件，从而拒绝启动。</p><p>这是因为XtraBackup在备份期间，为了尽可能减少对数据库性能的影响，是采用类似“快照”的方式拷贝InnoDB数据文件（.ibd）的。拷贝过程中，数据库可能仍在处理事务，这就导致备份集内的数据文件在同一时刻的状态可能并不一致。–prepare步骤正是通过应用备份期间同时拷贝的redo日志，来修复这种不一致性，确保数据恢复到备份操作完成那一刻的一致性状态。</p><pre><code class="sql">[root@VM-8-4-opencloudos backup]#  /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --prepare --target-dir=/data/backup/full/full_20251104_173434
2025-11-04T17:44:13.715402+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized server arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksums=1 --innodb_data_file_path=ibdata1:200M;ibdata2:200M:autoextend --innodb_log_file_size=50331648 --innodb_page_size=16384 --innodb_undo_directory=./ --innodb_undo_tablespaces=2 --server-id=13045 --innodb_log_checksums=ON --innodb_redo_log_encrypt=0 --innodb_undo_log_encrypt=0 
2025-11-04T17:44:13.715565+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized client arguments: --prepare=1 --target-dir=/data/backup/full/full_20251104_173434 
/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
2025-11-04T17:44:13.715602+08:00 0 [Note] [MY-011825] [Xtrabackup] cd to /data/backup/full/full_20251104_173434/
2025-11-04T17:44:13.716155+08:00 0 [Note] [MY-011825] [Xtrabackup] This target seems to be not prepared yet.
2025-11-04T17:44:13.730583+08:00 0 [Note] [MY-011825] [Xtrabackup] xtrabackup_logfile detected: size=8388608, start_lsn=(395899109)
2025-11-04T17:44:13.743037+08:00 0 [Note] [MY-011825] [Xtrabackup] using the following InnoDB configuration for recovery:
2025-11-04T17:44:13.743072+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_home_dir = .
2025-11-04T17:44:13.743084+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_file_path = ibdata1:200M;ibdata2:200M:autoextend
2025-11-04T17:44:13.743119+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_group_home_dir = .
2025-11-04T17:44:13.743127+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_files_in_group = 1
2025-11-04T17:44:13.743135+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_file_size = 8388608
2025-11-04T17:44:13.744761+08:00 0 [Note] [MY-011825] [Xtrabackup] inititialize_service_handles suceeded
2025-11-04T17:44:13.745043+08:00 0 [Note] [MY-011825] [Xtrabackup] using the following InnoDB configuration for recovery:
2025-11-04T17:44:13.745062+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_home_dir = .
2025-11-04T17:44:13.745069+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_file_path = ibdata1:200M;ibdata2:200M:autoextend
2025-11-04T17:44:13.745089+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_group_home_dir = .
2025-11-04T17:44:13.745100+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_files_in_group = 1
2025-11-04T17:44:13.745110+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_file_size = 8388608
2025-11-04T17:44:13.745125+08:00 0 [Note] [MY-011825] [Xtrabackup] Starting InnoDB instance for recovery.
2025-11-04T17:44:13.745137+08:00 0 [Note] [MY-011825] [Xtrabackup] Using 104857600 bytes for buffer pool (set by --use-memory parameter)
2025-11-04T17:44:13.746141+08:00 0 [Note] [MY-012932] [InnoDB] PUNCH HOLE support available
2025-11-04T17:44:13.746162+08:00 0 [Note] [MY-012944] [InnoDB] Uses event mutexes
2025-11-04T17:44:13.746170+08:00 0 [Note] [MY-012945] [InnoDB] GCC builtin __atomic_thread_fence() is used for memory barrier
2025-11-04T17:44:13.746183+08:00 0 [Note] [MY-012948] [InnoDB] Compressed tables use zlib 1.2.13
2025-11-04T17:44:13.746475+08:00 0 [Note] [MY-012951] [InnoDB] Using hardware accelerated crc32 and polynomial multiplication.
2025-11-04T17:44:13.746898+08:00 0 [Note] [MY-012203] [InnoDB] Directories to scan './'
2025-11-04T17:44:13.746943+08:00 0 [Note] [MY-012204] [InnoDB] Scanning './'
2025-11-04T17:44:13.753807+08:00 0 [Note] [MY-012208] [InnoDB] Completed space ID check of 10 files.
2025-11-04T17:44:13.756116+08:00 0 [Note] [MY-012955] [InnoDB] Initializing buffer pool, total size = 128.000000M, instances = 1, chunk size =128.000000M 
2025-11-04T17:44:13.765015+08:00 0 [Note] [MY-012957] [InnoDB] Completed initialization of buffer pool
2025-11-04T17:44:13.770906+08:00 0 [Note] [MY-011951] [InnoDB] page_cleaner coordinator priority: -20
2025-11-04T17:44:13.771119+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.771183+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.771564+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.824205+08:00 0 [Note] [MY-013883] [InnoDB] The latest found checkpoint is at lsn = 395899109 in redo log file ./
#innodb
_redo/
#ib
_redo0.
2025-11-04T17:44:13.824287+08:00 0 [Note] [MY-012560] [InnoDB] The log sequence number 395730540 in the system tablespace does not match the log sequence number 395899109 in the redo log files!
2025-11-04T17:44:13.824305+08:00 0 [Note] [MY-012551] [InnoDB] Database was not shutdown normally!
2025-11-04T17:44:13.824312+08:00 0 [Note] [MY-012552] [InnoDB] Starting crash recovery.
2025-11-04T17:44:13.824504+08:00 0 [Note] [MY-013086] [InnoDB] Starting to parse redo log at lsn = 395898899, whereas checkpoint_lsn = 395899109 and start_lsn = 395898880
2025-11-04T17:44:13.824519+08:00 0 [Note] [MY-012550] [InnoDB] Doing recovery: scanned up to log sequence number 395899109
2025-11-04T17:44:13.843504+08:00 0 [Note] [MY-013083] [InnoDB] Log background threads are being started...
2025-11-04T17:44:13.851994+08:00 0 [Note] [MY-012532] [InnoDB] Applying a batch of 0 redo log records ...
2025-11-04T17:44:13.852026+08:00 0 [Note] [MY-012535] [InnoDB] Apply batch completed!
2025-11-04T17:44:13.952255+08:00 0 [Note] [MY-013084] [InnoDB] Log background threads are being closed...
2025-11-04T17:44:13.957800+08:00 0 [Note] [MY-013888] [InnoDB] Upgrading redo log: 1032M, LSN=395899109.
2025-11-04T17:44:13.957898+08:00 0 [Note] [MY-012968] [InnoDB] Starting to delete and rewrite redo log files.
2025-11-04T17:44:13.957956+08:00 0 [Note] [MY-011825] [InnoDB] Removing redo log file: ./
#innodb
_redo/
#ib
_redo0
2025-11-04T17:44:14.004652+08:00 0 [Note] [MY-011825] [InnoDB] Creating redo log file at ./
#innodb
_redo/
#ib
_redo0_tmp with file_id 0 with size 33554432 bytes
2025-11-04T17:44:14.008311+08:00 0 [Note] [MY-011825] [InnoDB] Renaming redo log file from ./
#innodb
_redo/
#ib
_redo0_tmp to ./
#innodb
_redo/
#ib
_redo0
2025-11-04T17:44:14.011183+08:00 0 [Note] [MY-012893] [InnoDB] New redo log files created, LSN=395899404
2025-11-04T17:44:14.011284+08:00 0 [Note] [MY-013083] [InnoDB] Log background threads are being started...
2025-11-04T17:44:14.023796+08:00 0 [Note] [MY-013252] [InnoDB] Using undo tablespace './undo_001'.
2025-11-04T17:44:14.025816+08:00 0 [Note] [MY-013252] [InnoDB] Using undo tablespace './undo_002'.
2025-11-04T17:44:14.027533+08:00 0 [Note] [MY-012910] [InnoDB] Opened 2 existing undo tablespaces.
2025-11-04T17:44:14.027698+08:00 0 [Note] [MY-011980] [InnoDB] GTID recovery trx_no: 1044267
2025-11-04T17:44:14.226936+08:00 0 [Note] [MY-013776] [InnoDB] Parallel initialization of rseg complete
2025-11-04T17:44:14.226983+08:00 0 [Note] [MY-013777] [InnoDB] Time taken to initialize rseg using 2 thread: 199297 ms.
2025-11-04T17:44:14.229195+08:00 0 [Note] [MY-012923] [InnoDB] Creating shared tablespace for temporary tables
2025-11-04T17:44:14.229302+08:00 0 [Note] [MY-012265] [InnoDB] Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2025-11-04T17:44:14.260630+08:00 0 [Note] [MY-012266] [InnoDB] File './ibtmp1' size is now 12 MB.
2025-11-04T17:44:14.262027+08:00 0 [Note] [MY-013627] [InnoDB] Scanning temp tablespace dir:'./
#innodb
_temp/'
2025-11-04T17:44:14.287708+08:00 0 [Note] [MY-013018] [InnoDB] Created 128 and tracked 128 new rollback segment(s) in the temporary tablespace. 128 are now active.
2025-11-04T17:44:14.291471+08:00 0 [Note] [MY-012976] [InnoDB] 8.0.35 started; log sequence number 395899414
2025-11-04T17:44:14.292531+08:00 0 [Warning] [MY-012091] [InnoDB] Allocated tablespace ID 1 for sys/sys_config, old maximum was 0
2025-11-04T17:44:14.301258+08:00 0 [Note] [MY-011825] [Xtrabackup] Completed loading of 8 tablespaces into cache in 0.00972792 seconds
2025-11-04T17:44:14.332275+08:00 0 [Note] [MY-011825] [Xtrabackup] Time taken to build dictionary: 0.0309666 seconds
2025-11-04T17:44:15.336933+08:00 0 [Note] [MY-011825] [Xtrabackup] starting shutdown with innodb_fast_shutdown = 1
2025-11-04T17:44:15.337061+08:00 0 [Note] [MY-012330] [InnoDB] FTS optimize thread exiting.
2025-11-04T17:44:16.337034+08:00 0 [Note] [MY-013072] [InnoDB] Starting shutdown...
2025-11-04T17:44:16.440671+08:00 0 [Note] [MY-013084] [InnoDB] Log background threads are being closed...
2025-11-04T17:44:16.461318+08:00 0 [Note] [MY-012980] [InnoDB] Shutdown completed; log sequence number 395899414
2025-11-04T17:44:16.476016+08:00 0 [Note] [MY-011825] [Xtrabackup] completed OK!</code></pre><p>当执行prapare过程中 最后出现 completed OK! 关键字时，表明操作成功。</p><p>3、停止MySQL服务并清空数据目录</p><p>为确保恢复过程顺利，首先需要停止MySQL服务，并清空其数据目录（datadir）。这是恢复操作的关键前提</p><pre><code class="sql">-- 停止MySQL服务
systemctl stop mysqld
-- 或者登录MySQL数据库 执行 shutdown; 命令</code></pre><p>重要提示：在执行rm -rf命令前，务必确认目录路径正确，最好对原有数据做备份。然后清空数据目录：</p><pre><code class="sql"># 清空MySQL数据目录（请先确认你的datadir路径，假设是/var/lib/mysql）
rm -rf /var/lib/mysql/* </code></pre><p>4、执行数据恢复</p><p>使用 --copy-back或 --move-back命令将预备好的备份数据恢复到MySQL的数据目录</p><pre><code class="sql">xtrabackup --copy-back --target-dir=/path/to/prepared_backup
说明：
 --copy-back：将备份文件复制到数据目录。这是最安全常用的方式，保留原始备份
 --move-back：将备份文件移动到数据目录。更节省空间，但原始备份会消失</code></pre><p>5、修改文件权限</p><p>恢复的数据文件可能不属于mysql用户，需要更改属主和权限以确保MySQL有权限读写</p><pre><code class="sql">chown -R mysql:mysql /var/lib/mysql
此命令将数据目录及其下所有文件的所有者和组设置为mysql</code></pre><p>6、启动MySQL并确认</p><p>权限设置好以后，就可以启动MySQL服务了</p><pre><code class="sql">systemctl start mysqld
-- 或者使用mysqld_safe --defaults-file=/etc/my.cnf --user=mysql &amp; 方式启动</code></pre><p>启动后，务必检查MySQL的错误日志，并使用客户端连接，验证数据库和表是否正常。</p><h2><strong>总 结</strong></h2><p>该脚本提供了一个生产环境进行MySQL8.0物理备份所需的完整步骤，包括错误处理、日志记录、自动清理和耗时统计。数据库运维人员可以根据实际环境调整配置参数，特别是备份路径和保留天数设置以及是否采用压缩等一些常用功能的设置。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046208374" alt="" title=""/>  </p><p>墨天轮从乐知乐享的数据库技术社区蓄势出发，全面升级，提供多类型数据库管理服务。墨天轮数据库管理服务旨在为用户构建信赖可托付的数据库环境，并为数据库厂商提供中立的生态支持。<br/>墨天轮数据库服务官网：<a href="https://link.segmentfault.com/?enc=PJeLT%2FbswEoUX4nlVxKfGg%3D%3D.89CkDWIllNEM6OPA%2BrK7TBlC0crEh0Sbrs2EcWrulOO5JmsGqfRBjLpBvjF9I5qk" rel="nofollow" target="_blank">https://www.modb.pro/service</a></p>]]></description></item><item>    <title><![CDATA[中小企业 CRM 推荐：2025 年高性价比品牌排行榜 TOP6 晨曦钥匙扣 ]]></title>    <link>https://segmentfault.com/a/1190000047574769</link>    <guid>https://segmentfault.com/a/1190000047574769</guid>    <pubDate>2026-01-27 12:11:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型进程中，CRM（客户关系管理）已从“辅助销售的工具”升级为“连接客户、销售、服务的业务中枢”。其核心价值体现在三个维度：<strong>客户/商机/跟进一体化</strong>（打破信息孤岛）、<strong>自动提醒与任务分派</strong>（释放团队效率）、<strong>移动端无缝体验</strong>（适配外勤场景）。</p><p>本文选取<strong>超兔一体云、Freshsales（Freshworks）、金现代、Zoho、管家婆、Pipedrive</strong>六大主流CRM品牌，从三个核心维度展开深度横评，结合<strong>表格对比、流程图、脑图、雷达图</strong>，为企业选型提供专业参考。</p><h2>一、核心维度与评价体系说明</h2><h3>1.1 维度拆解与评价标准</h3><p>我们将CRM的核心能力拆解为<strong>3大维度+12个子指标</strong>，并采用<strong>1-5分制</strong>（5分为满分）量化评估各品牌表现：</p><table><thead><tr><th>一级维度</th><th>二级子指标</th><th>评价标准</th></tr></thead><tbody><tr><td>客户/商机/跟进一体化</td><td>全流程覆盖（线索→订单）、客户360°视图、跨模块协同（销售+服务/营销）、销售漏斗可视化</td><td>数据打通+流程协同+视图统一</td></tr><tr><td>自动提醒与任务分派</td><td>规则灵活性、智能触发精度、任务分配合理性、多渠道通知、移动端支持</td><td>规则精准+触发及时+分配合理</td></tr><tr><td>移动端无缝体验</td><td>多端同步效率、离线功能、操作便捷性（语音/拍照/定位）、生态适配、全功能覆盖</td><td>多端同步+离线可用+操作便捷+生态兼容</td></tr></tbody></table><h3>1.2 雷达图指标与分值（1-5分）</h3><p>为直观展示各品牌综合能力，我们选取<strong>5个核心指标</strong>绘制雷达图（分值越高，能力越强）：</p><table><thead><tr><th>品牌</th><th>客户一体化</th><th>自动提醒</th><th>任务分派</th><th>移动端体验</th><th>生态集成</th></tr></thead><tbody><tr><td>超兔一体云</td><td>4.8</td><td>4.7</td><td>4.6</td><td>4.9</td><td>4.5</td></tr><tr><td>Freshsales</td><td>4.5</td><td>4.6</td><td>4.7</td><td>4.6</td><td>4.8</td></tr><tr><td>金现代</td><td>4.3</td><td>4.2</td><td>4.1</td><td>4.4</td><td>4.0</td></tr><tr><td>Zoho</td><td>4.4</td><td>4.3</td><td>4.2</td><td>4.5</td><td>4.7</td></tr><tr><td>管家婆</td><td>4.2</td><td>4.4</td><td>4.3</td><td>4.8</td><td>4.1</td></tr><tr><td>Pipedrive</td><td>4.6</td><td>4.5</td><td>4.7</td><td>4.7</td><td>4.6</td></tr></tbody></table><h2>二、客户、商机、跟进一体化：从“模块叠加”到“业务中枢”</h2><h3>2.1 维度本质：不是“有模块”，而是“能协同”</h3><p>真正的一体化不是<strong>模块的简单堆砌</strong>，而是<strong>数据打通、流程协同、视图统一</strong>的闭环：</p><ul><li><strong>数据打通</strong>：客户信息、商机进展、跟进记录在CRM、进销存、服务等模块自由流动；</li><li><strong>流程协同</strong>：商机阶段变化自动触发跟进任务（如“意向确认”→“起草合同”）；</li><li><strong>视图统一</strong>：一个界面看全客户全景（基本信息、历史跟进、商机进展、服务记录）。</li></ul><h3>2.2 各品牌表现对比</h3><h4>（1）超兔一体云：底层大底座支撑全链路协同</h4><p>超兔的核心优势是<strong>构建了覆盖CRM、进销存、供应链、收支账的“业务大底座”</strong> ，实现数据底层连通：</p><ul><li>市场部通过集客获取的线索，自动同步至客户中心并生成商机；</li><li>销售跟进的每一条记录（拜访、沟通），实时同步至客户视图与商机视图；</li><li>商机进入“意向确认”阶段时，流程引擎自动提醒“起草合同”，任务完成后同步更新客户状态（如“高意向”）。</li></ul><p><strong>流程图：超兔一体化逻辑</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574771" alt="" title=""/></p><pre><code>graph TD
    A[市场部集客获取线索] --&gt; B[线索同步至客户中心（底层连通）]
    B --&gt; C[自动生成对应商机记录（数据关联）]
    C --&gt; D[销售跟进：拜访/沟通记录录入]
    D --&gt; E[跟进记录实时同步至客户视图+商机视图（统一视图）]
    E --&gt; F[商机阶段变化触发流程任务（如“意向确认”→“起草合同”）]
    F --&gt; G[任务完成，更新客户+商机状态]</code></pre><h4>（2）Freshsales：销售+服务+营销云一体化</h4><p>Freshsales（Freshworks旗下）的核心是“销售云+营销云+服务云”的全栈协同：</p><ul><li>客户生命周期档案完整覆盖“线索→联系人→销售→服务”全流程；</li><li>服务模块的工单（如客户投诉）会同步至销售跟进记录，避免“销售不管售后”的信息差；</li><li>AI助手Freddy可自动识别客户需求（如“价格咨询”），触发销售跟进任务。</li></ul><h4>（3）管家婆：本土化场景的“跟单闭环”</h4><p>管家婆的优势是<strong>深度适配国内中小微企业的“本土化跟单需求”</strong> ：</p><ul><li>支持“新增客户→联系→回访”的全流程记录，客户档案无限存储；</li><li>客户新增/修改时自动匹配相近客户（避免重复建档）；</li><li>销售跟进记录可直接转为日程（一次填写，两处复用），减少人工操作。</li></ul><h3>2.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>全流程覆盖（线索→订单）</th><th>客户360°视图</th><th>跨模块协同（销售+服务）</th><th>销售漏斗可视化</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（底层大底座连通）</td><td>✅（全景信息）</td><td>✅（CRM+进销存+供应链）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅（销售+服务+营销）</td><td>✅（生命周期）</td><td>✅（工单同步销售）</td><td>✅（AI优化）</td></tr><tr><td>金现代</td><td>✅（线索→回款）</td><td>✅（画像+标签）</td><td>✅（营销数字化平台）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅（自定义模块）</td><td>✅（多渠道）</td><td>✅（销售+营销）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅（本土化跟单）</td><td>✅（无限档案）</td><td>❌（侧重销售，服务弱）</td><td>✅</td></tr><tr><td>Pipedrive</td><td>✅（漏斗为核心）</td><td>✅（跟进关联）</td><td>❌（侧重销售，营销弱）</td><td>✅（可视化强）</td></tr></tbody></table><h2>三、自动提醒与任务分派：从“人工记忆”到“智能驱动”</h2><h3>3.1 维度本质：不是“能提醒”，而是“精准提醒”</h3><p>自动提醒与任务分派的核心是“规则精准、触发及时、分配合理”：</p><ul><li>规则精准：支持时间、事件、状态等多条件组合（如“商机距离签约7天”+“客户未跟进”）；</li><li>触发及时：基于历史数据或AI预测潜在风险（如“客户7天未联系”）；</li><li>分配合理：根据员工负荷（如“销售A当前有5个高价值商机”）与技能（如“擅长跟进大企业”）分配任务。</li></ul><h3>3.2 各品牌表现对比</h3><h4>（1）Freshsales：AI驱动的“智能任务体系”</h4><p>Freshsales的优势是<strong>AI线索打分与自动化任务分派</strong>：</p><ul><li>AI助手Freddy对线索打分（如“高价值客户打9分”），自动分配给Top销售；</li><li>商机关键节点（如“签约前7天”）自动发送提醒邮件，新线索触发“欢迎信”；</li><li>重复任务（如数据录入）自动化，节省销售80%的琐碎时间。</li></ul><p><strong>脑图：Freshsales智能任务体系</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574772" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((Freshsales智能任务体系))
        自动提醒
            商机关键节点（签约前7天）
            潜在客户阶段触发（高意向）
            自动邮件（新线索欢迎信）
        任务分派
            AI线索打分→Top销售
            重复任务自动化（数据录入）
            移动端任务推送
        核心价值
            避免跟进遗漏
            提升团队效率
            缩短销售周期</code></pre><h4>（2）超兔一体云：算法驱动的“精准分配”</h4><p>超兔的特色是<strong>智能算法对任务的“合理分配”</strong> ：</p><ul><li>支持“时间+事件+状态”多条件规则（如“客户复购时间预测”+“未跟进”）；</li><li>基于历史数据预测客户复购时间，提前3天提醒销售跟进；</li><li>根据员工“当前负荷”（如已分配任务量）与“技能标签”（如“擅长电商客户”）分配任务，避免“忙的忙死，闲的闲死”。</li></ul><h4>（3）管家婆：本土化的“场景化提醒”</h4><p>管家婆的优势是<strong>适配国内企业的“日常场景提醒”</strong> ：</p><ul><li>支持“周目标事项”“待办事宜”的日程提醒；</li><li>订单新增时触发“语音提示”，避免漏看；</li><li>销售跟进记录可直接转为日程，一次填写，同时同步至“客户档案”与“个人日程”。</li></ul><h3>3.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>规则灵活性（多条件）</th><th>智能触发（AI/历史数据）</th><th>任务分配（负荷+技能）</th><th>多渠道通知</th><th>移动端支持</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（时间/事件/状态）</td><td>✅（智能算法预测）</td><td>✅（负荷+技能）</td><td>✅（短信/邮件/系统）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅（阶段/节点）</td><td>✅（AI线索打分）</td><td>✅（Top销售）</td><td>✅（邮件）</td><td>✅</td></tr><tr><td>金现代</td><td>✅（预设规则）</td><td>❌（无AI）</td><td>✅（责任明晰）</td><td>❌（基础）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅（地域/行业）</td><td>✅（Zia助手）</td><td>✅（规则分配）</td><td>✅（消息）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅（日程/订单）</td><td>❌（无AI）</td><td>✅（工作指派）</td><td>✅（语音）</td><td>✅</td></tr><tr><td>Pipedrive</td><td>✅（阶段触发）</td><td>✅（优先级标注）</td><td>✅（流程分配）</td><td>✅（日历）</td><td>✅</td></tr></tbody></table><h2>四、移动端无缝体验：从“能访问”到“好用”</h2><h3>4.1 维度本质：不是“有APP”，而是“适配场景”</h3><p>移动端的核心是“多端同步、离线可用、操作便捷、生态适配”：</p><ul><li>多端同步：移动端操作实时同步至PC端，无延迟；</li><li>操作便捷：支持语音输入、拍照上传、定位打卡等“外勤友好”功能；</li><li>生态适配：与微信、QQ、日历等常用工具集成，减少切换成本。</li></ul><h3>4.2 各品牌表现对比</h3><h4>（1）管家婆：本土化全功能移动端</h4><p>管家婆的移动端是<strong>国内中小微企业的“外勤神器”</strong> ，覆盖<strong>开单、审批、OA、客户跟进</strong>全场景：</p><ul><li>支持“手机开单”，订单可直接发送至客户微信/QQ/短信；</li><li>实时查看“销售业绩、库存状态”，避免“库存不足却接单”的尴尬；</li><li>集成OA协同（待办事宜、同事圈沟通），无需额外安装办公软件。</li></ul><h4>（2）超兔一体云：轻量化与场景化能力兼顾</h4><p>超兔的移动端采用“轻量化设计+场景化能力”，适配“外勤场景”：</p><ul><li>支持语音输入（快速记录沟通内容）、拍照上传（客户资料）、定位打卡（拜访轨迹）；</li><li>多端适配（Web/APP/小程序/客户端），满足不同团队的设备需求。</li></ul><h4>（3）Pipedrive：获G2认可的“易用性”</h4><p>Pipedrive的移动端以“易用性”著称，获2025年G2“销售人员最易用奖”：</p><ul><li>支持离线访问客户数据，语音录入客户信息（避免手动打字）；</li><li>活动提醒同步至Google日历，避免“错过重要拜访”；</li><li>界面简洁，销售人员可快速找到“跟进客户、查看商机、记录沟通”核心功能。</li></ul><h3>4.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>多端同步（实时）</th><th>离线功能</th><th>操作便捷（语音/拍照）</th><th>生态集成（微信/日历）</th><th>全功能覆盖</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（多端实时）</td><td>✅</td><td>✅（语音/拍照/定位）</td><td>✅（小程序/微信）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅</td><td>✅</td><td>✅（定位/行程）</td><td>✅（Google Maps）</td><td>✅</td></tr><tr><td>金现代</td><td>✅</td><td>✅</td><td>✅（现场录入/拍照）</td><td>❌（基础）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅</td><td>✅</td><td>✅（地图/打卡）</td><td>✅（Google日历）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅</td><td>❌</td><td>✅（开单/微信发送）</td><td>✅（微信/QQ/短信）</td><td>✅（全功能）</td></tr><tr><td>Pipedrive</td><td>✅</td><td>✅</td><td>✅（语音录入）</td><td>✅（Google日历）</td><td>✅</td></tr></tbody></table><h2>五、选型建议：匹配场景比“功能全”更重要</h2><p>通过以上对比，各品牌的<strong>核心优势与适用场景</strong>已清晰：</p><table><thead><tr><th>品牌</th><th>核心优势</th><th>适用场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全业务链路协同（CRM+进销存+供应链）、离线能力</td><td>需要“进销存+CRM协同”的中小微企业，如零售、贸易行业，外勤场景多</td></tr><tr><td>Freshsales</td><td>AI智能（线索打分、自动提醒）、跨国协同</td><td>注重AI辅助、需要跨国团队协作的B2B企业，如 SaaS、制造行业</td></tr><tr><td>金现代</td><td>营销数字化平台、PaaS定制化</td><td>需要“营销+销售协同”的中大型企业，如消费品、医药行业</td></tr><tr><td>Zoho</td><td>高性价比、多渠道集成</td><td>预算有限、需要多渠道（官网/社交媒体）线索管理的中小企业</td></tr><tr><td>管家婆</td><td>本土化全功能（微信/QQ集成）、移动端易用</td><td>本土化需求强（如微信开单、短信通知）的中小微企业，如零售、餐饮行业</td></tr><tr><td>Pipedrive</td><td>销售漏斗可视化、移动端易用性</td><td>以销售漏斗为核心、注重移动端效率的销售团队，如房产、保险行业</td></tr></tbody></table><h2>结语</h2><p>CRM的本质是“以客户为中心”，其能力的核心不是“功能越多越好”，而是“能否匹配企业的业务场景”。企业选型时，需优先考虑“数据是否能打通”“任务是否能精准分配”“移动端是否好用”——这三个问题解决了，CRM才能真正成为“业务中枢”，而非“摆设”。</p><p>未来，CRM的竞争将更聚焦“AI+场景化”：AI将更精准地预测客户需求，场景化功能（如零售的“微信开单”、制造的“进销存协同”）将更贴合行业痛点。企业需结合自身发展阶段，选择“能陪伴成长”的CRM伙伴。</p>]]></description></item><item>    <title><![CDATA[🚀 爆火的 Clawdbot 到底是什么？—— 你的第一个“真·本地”AI 智能管家 Pangoli]]></title>    <link>https://segmentfault.com/a/1190000047574779</link>    <guid>https://segmentfault.com/a/1190000047574779</guid>    <pubDate>2026-01-27 12:10:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Clawdbot：爆火的开源AI智能体网关，堪称AI助理完全体</h2><p>最近，你的技术圈子是不是被一只“龙虾”（Clawdbot 的 Logo）刷屏了？甚至听说它让二手的 Mac Mini 价格都应声上涨？</p><p>作为一个刚入坑稀土掘金的新人，今天我就带大家扒一扒这个让无数极客彻夜未眠的开源项目——Clawdbot。它到底是什么？为什么它被称为“AI 助理的完全体”？以及，它真的能成为你的 Jarvis 吗？</p><h3>🧐 什么是 Clawdbot？</h3><p>简单来说，Clawdbot 是一个开源的 AI 智能体网关（Agent Gateway）。</p><p>如果不讲术语，你可以这样理解：<br/><strong>Clawdbot = 大模型的大脑 (Claude/GPT) + 即时通讯软件的嘴巴 (Telegram/WhatsApp) + 本地电脑的手脚 (Terminal/文件系统) + 永久记忆</strong>。</p><p>与我们在网页上用的 ChatGPT 或 Claude 不同，Clawdbot 不是运行在浏览器里的，而是运行在你自己的服务器或电脑（如 Mac Mini、树莓派）上的一个后台程序。它就像一个住在你电脑里的“数字管家”，你通过聊天软件给它发指令，它在你的电脑上直接干活。</p><h3>🌟 核心特点：为什么它如此特别？</h3><p>Clawdbot 之所以能爆火，是因为它解决了当前 AI 应用的几个核心痛点：</p><h4>1. 它是“活”在本地的 (Local First)</h4><p>目前大多数 AI 都在云端，不仅有隐私顾虑，而且无法操作你的本地文件。Clawdbot 运行在你的本地设备上：</p><ul><li>数据隐私：除了与 LLM 对话的内容，你的记忆文件、配置、本地数据都存在自己硬盘里。</li><li>本地权限：它可以直接读取你的文档、运行 Python 脚本、甚至执行终端命令（Terminal）。</li></ul><h4>2. 对话即交互 (ChatOps)</h4><p>你不需要下载专门的 App。Clawdbot 接入了 WhatsApp, Telegram, Discord, Slack, iMessage 等几乎所有主流通讯软件。</p><ul><li>场景：你在外面用手机给家里的 Clawdbot 发微信：“帮我查一下服务器日志，把报错的部分发给我。”</li><li>结果：它直接通过 SSH 连上服务器，跑完命令，把结果截图或文本回传给你。</li></ul><h4>3. 真正的“长短期记忆”</h4><p>Clawdbot 使用本地的 Markdown 文件（通常是 MEMORY.md）来存储关于你的信息。<br/>它记得你的偏好、你家人的生日、你的服务器密码（需谨慎）、你正在做的项目进度。<br/>这种记忆是持久的，不会因为关闭窗口就消失。</p><h4>4. 强大的工具调用能力 (Agentic Capabilities)</h4><p>这是它最“炸裂”的地方。它不仅能陪聊，还能干活。通过 MCP (Model Context Protocol) 或内置工具，它可以：</p><ul><li>浏览网页：帮你查资料并总结。</li><li>写代码并运行：它可以写一个 Python 脚本来处理 Excel 表格，然后直接在你电脑上运行这个脚本，最后把处理好的 Excel 发给你。</li><li>管理日程：读取你的日历，帮你安排会议。</li></ul><h3>🛠 Clawdbot 能帮我们干什么？</h3><p>这就是想象力发挥的地方了。目前社区里已经有了很多硬核玩法：</p><h4>1. 24/7 个人秘书</h4><ul><li>自动处理邮件：让它监控你的 Gmail，自动归档垃圾邮件，把重要邮件摘要发到 Telegram 给你。</li><li>每日简报：每天早上 8 点，它会根据你的日历、关注的新闻源、天气情况，给你发一份定制的“早安简报”。</li></ul><h4>2. 也是最强的“结对编程”伙伴</h4><ul><li>代码助手：你可以让它读取你整个项目的代码库（因为它在本地，读取速度极快），然后问它：“utils.py 里的那个函数怎么优化？”</li><li>运维监控：当它检测到某个进程挂了，可以自动发消息报警，甚至在你授权下尝试重启服务。</li></ul><h4>3. 自动化繁琐任务</h4><ul><li>文件整理：对它说“把 Downloads 文件夹里所有的 PDF 发票整理一下，按月份归档到 Documents/Invoices 目录里”。它会自己写 Shell 脚本瞬间完成。</li><li>比价购物：让它去几个电商网站爬取价格，整理成表格给你。</li></ul><h3>⚠️ 风险提示（必读！）</h3><p>虽然 Clawdbot 很酷，但它目前更像是一个极客的玩具，而不是普通用户的消费级产品。</p><ol><li><strong>安全风险（高危）</strong>：你实际上是给了 AI 访问你电脑文件系统和终端（Terminal）的权限。虽然有权限控制，但如果 AI "幻觉"了，或者被提示注入攻击，理论上它能执行 rm -rf /（删库）。建议尽量在沙箱环境或独立的 Mac Mini/虚拟机中运行。</li><li><strong>成本问题</strong>：虽然软件免费，但它背后调用的是 API（如 Claude 3.5 Sonnet 或 GPT-4o）。如果你让它处理大量任务，API 账单可能会让你肉疼。</li><li><strong>配置门槛</strong>：需要懂一点 Docker、Node.js 或者命令行的知识才能部署起来。</li></ol><h3>🔚 总结</h3><p>Clawdbot 代表了 AI 的下一个阶段：从“聊天机器人”进化为“智能代理（Agent）”。它不再是被动等待提问的百科全书，而是有了手脚、能主动帮你解决问题的数字员工。</p><p>如果你有一台闲置的电脑，并且喜欢折腾技术，Clawdbot 绝对值得一试。但请记得：<strong>能力越大，风险越大，请管好你的 API Key 和系统权限！</strong></p><p>欢迎在评论区分享你的 Clawdbot 玩法！</p>]]></description></item><item>    <title><![CDATA[2026 工业 CRM 盘点：5 大品牌客制化能力横评 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047574782</link>    <guid>https://segmentfault.com/a/1190000047574782</guid>    <pubDate>2026-01-27 12:10:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业4.0与全球供应链重构的背景下，工业/工贸企业的数字化转型核心需求已从“标准化上线”转向“<strong>柔性适配</strong>”——既要应对复杂业务场景（如非标订单、跨部门协同、跨境贸易），又要快速响应市场变化（如需求波动、流程调整）。</p><p>本文选取<strong>超兔一体云、</strong> <strong>SAP</strong> <strong>、Microsoft Dynamics 365、八百客</strong> <strong>CRM</strong> <strong>、OKKI CRM（原小满）五大主流平台，从</strong> <strong>客制化</strong> <strong>能力、多端协同能力、工业场景适配性、实施成本</strong>四大核心维度展开深度横评，为工业/工贸企业选择适配的数字化体系提供决策参考。</p><h2>一、核心对比框架与维度定义</h2><h3>1. 对比维度说明</h3><table><thead><tr><th><strong>维度</strong></th><th><strong>子维度</strong></th><th><strong>工业/工贸企业核心诉求关联</strong></th></tr></thead><tbody><tr><td><strong>客制化</strong> <strong>能力</strong></td><td>低代码/无代码定制、行业模板覆盖、生态系统集成、复杂流程（如非标订单）定制</td><td>适配企业独特业务逻辑（如半导体晶圆制造合规、外贸跨境合同），避免“削足适履”</td></tr><tr><td><strong>多端协同能力</strong></td><td>多终端覆盖、内部业务链路协同（销售→生产→物流→财务）、外部生态协同（供应商/客户）、业财一体化</td><td>消除信息孤岛，提升跨部门/跨企业协作效率，实现“订单驱动生产”的柔性协同</td></tr><tr><td><strong>工业场景适配性</strong></td><td>核心场景覆盖（订单-生产-交付、设备维护、跨境贸易）、行业聚焦（如汽配/半导体/外贸）、柔性应变能力</td><td>精准解决工业企业痛点（如IATF 16949质量合规、跨境回款风险），支持业务模式调整</td></tr><tr><td><strong>实施与成本</strong></td><td>部署方式（云/本地/混合）、实施周期、总成本（license+实施+维护）、维护难度</td><td>平衡“定制深度”与“成本效率”，避免“大而全”的高投入陷阱</td></tr></tbody></table><h3>2. 品牌选择说明</h3><p>选取标准：<strong>聚焦工业/工贸场景</strong>+ <strong>“</strong> <strong>客制化</strong> <strong>+多端协同”能力明确</strong></p><ul><li>超兔一体云：以“客制化+多端协同”为核心定位，适配中小工业企业灵活需求；</li><li>SAP：全球ERP龙头，覆盖从中小企业（Business One）到大型企业（S/4HANA）的全场景；</li><li>Microsoft Dynamics 365：依托微软生态，擅长“办公+业务”协同；</li><li>八百客CRM：PaaS平台支撑深度定制，适配中大型工业企业复杂流程；</li><li>OKKI CRM：聚焦外贸工贸场景，解决跨境协同痛点。</li></ul><h2>二、四大核心维度深度横评</h2><h3>（一）客制化能力：从“标准化”到“精准适配”的关键</h3><p>客制化是工业/工贸企业数字化的“灵魂”——<strong>只有适配企业独特业务逻辑，才能避免系统成为“摆设”</strong> 。五大平台的客制化能力差异显著：</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>低代码</strong> <strong>/无代码定制</strong></td><td>支持<strong>可视化自定义</strong>（三级菜单、工作台、业务表、工作流），无需代码调整流程</td><td>中小企业（Business One）支持“功能白名单+三级菜单”快速定制；大型企业需依赖实施商开发</td><td>通过<strong>Power Apps</strong>低代码平台，业务人员20分钟搭建自定义模块（如非标设备报价单）</td><td>基于PaaS平台，<strong>可视化配置表单/流程/权限</strong>，无需代码适配复杂项目管理</td><td>聚焦外贸场景，<strong>AI驱动客户分级/订单流程</strong>自定义，支持跨境合同模板配置</td></tr><tr><td><strong>行业模板覆盖</strong></td><td>提供工业/工贸通用模板（订单-生产-仓储协同），支持小步迭代调整</td><td>内置半导体/汽配/制造业等<strong>垂直行业模板</strong>（如晶圆制造合规体系、IATF 16949流程）</td><td>覆盖12大行业（制造业/零售业等），提供“订单-生产-交付”协同模块</td><td>聚焦光伏/制造等中大型工业企业，提供<strong>生产-销售协同模板</strong></td><td>专属<strong>外贸工贸模板</strong>（跨境回款规则、国际物流跟踪）</td></tr><tr><td><strong>生态系统集成</strong></td><td>支持RPA插件、对接第三方ERP（如金蝶/用友）</td><td>可集成WMS/MES/APS/QMS等工业系统，实现“计划-执行-反馈”数据贯通</td><td>通过<strong>Power Platform 300+连接器</strong>，对接SAP ERP、IoT设备、第三方CRM</td><td>对接ERP系统（如SAP/金蝶），实现<strong>业财一体化</strong></td><td>对接金蝶/用友ERP、跨境支付（PayPal）、物流（DHL）系统</td></tr><tr><td><strong>复杂流程定制</strong></td><td>支持<strong>自定义工作流+多表聚合BI</strong>，适配非标订单/多部门审批等复杂场景</td><td>大型企业（S/4HANA）支持<strong>客户化开发</strong>（如半导体成本精准归集）；中小企业（Business One）支持固化核心流程</td><td>支持<strong>设备安装记录/预防性维护计划</strong>等工业专属流程，通过Power Automate实现自动化</td><td>适配<strong>生产订单关联/多部门协作流程</strong>，支持复杂权限配置</td><td>适配<strong>跨境订单全流程</strong>（报价-合同-物流-回款），支持多语言合同模板</td></tr></tbody></table><p><strong>小结</strong>：</p><ul><li>小步快跑型企业选<strong>超兔</strong>：可视化自定义降低技术门槛，支持“按需添加功能”；</li><li>行业合规型企业选<strong>SAP</strong>：垂直行业模板覆盖半导体/汽配等强合规场景；</li><li>低代码快迭代型企业选<strong>Dynamics 365</strong>：Power Apps让业务人员主导定制；</li><li>复杂流程型企业选<strong>八百客</strong>：PaaS平台支撑深度流程配置；</li><li>外贸型企业选<strong>OKKI</strong>：专属跨境场景定制。</li></ul><h3>（二）多端协同能力：从“信息孤岛”到“全链路贯通”的核心</h3><p>多端协同的本质是<strong>数据与流程的“全场景流动”</strong> ——让销售在手机上录的订单，实时同步到生产排产系统；让供应商在Web端看到的库存，直接关联到客户的交付计划。</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>多终端覆盖</strong></td><td>Web/APP/小程序/客户端/RPA插件，适配外勤销售/仓库扫码/财务分析等场景</td><td>Web/APP/移动端，支持<strong>SAP Fiori</strong>移动应用（如生产工单审批）</td><td>Web/APP/小程序/Teams/Outlook，覆盖办公+业务全场景</td><td>PC/移动端（iOS/Android），支持实时数据同步</td><td>移动端（iOS/Android）+Web，支持<strong>跨境多语言沟通</strong>（62个国家）</td></tr><tr><td><strong>内部业务协同</strong></td><td>销售订单→生产计划→采购→仓储→财务全链路数据同步，支持跨部门流程协同</td><td>集成ERP/WMS/MES/APS，实现“销售订单→生产排产→物流→财务”全链路贯通</td><td>与Office 365深度融合：Outlook调客户画像、Teams生成跟进任务、Excel转BI报表</td><td>PC/移动端共享客户数据、分配销售任务，管理层通过<strong>数据看板</strong>监控全链路</td><td>移动端<strong>邮件聚合/客户动态实时更新</strong>，团队协同跟进海外订单</td></tr><tr><td><strong>外部生态协同</strong></td><td>支持供应商/客户小程序端接入，实现订单状态实时共享</td><td>通过<strong>SAP Business Network</strong>连接供应商/客户/物流商，提升库存可视性</td><td>通过<strong>Dynamics 365 Supply Chain Management</strong>对接供应商，实现计划与库存自动化协同</td><td>暂未明确支持外部生态协同</td><td>通过<strong>跨境供应链平台</strong>连接供应商/物流商，实现国际物流跟踪</td></tr><tr><td><strong>业财一体化</strong></td><td>自定义财务字段（如应收应付），支持多表聚合分析财务数据</td><td>集成财务模块，实现“订单-生产-财务”数据联动，支持成本精准归集</td><td>打通销售/供应链/财务流程，内置“应收账期预警”，坏账率控制在1.5%以内</td><td>对接ERP实现<strong>业财数据同步</strong>，支持生产-销售财务联动</td><td>支持<strong>跨境回款规则配置</strong>，对接支付系统实现实时到账提醒</td></tr></tbody></table><p><strong>关键场景验证</strong>：</p><ul><li>超兔：销售人员在APP录入客户订单，生产部门通过Web端实时看到排产需求，仓库用小程序扫码出库，数据全链路同步；</li><li>SAP：某汽配企业通过Business One集成生产/物流/财务，跨部门协作效率提升40%；</li><li>Dynamics 365：某汽车零部件企业通过Teams会议纪要自动生成生产任务，数据分析周期从“周级”压缩至“日级”；</li><li>OKKI：某外贸工贸企业通过移动端实时跟踪跨境物流，交付周期缩短30%。</li></ul><h3>（三）工业场景适配性：从“通用”到“垂直”的精准度</h3><p>工业/工贸企业的核心痛点是“业务场景复杂且高度行业化”——半导体企业需要晶圆制造合规，汽配企业需要IATF 16949标准，外贸企业需要跨境回款安全。五大平台的场景适配性差异直接决定了“能否解决真问题”：</p><h4>1. 核心场景覆盖对比</h4><table><thead><tr><th><strong>场景</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td>订单-生产-交付协同</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>设备维护与预防性保养</td><td>✅（自定义表单）</td><td>✅（MES集成）</td><td>✅（Power Apps配置）</td><td>✅</td><td>❌</td></tr><tr><td>跨境贸易（多语言/货币）</td><td>❌</td><td>✅（S/4HANA）</td><td>✅（Dynamics 365 Commerce）</td><td>❌</td><td>✅</td></tr><tr><td>行业合规（如IATF 16949）</td><td>❌</td><td>✅（Business One汽配模板）</td><td>✅（制造业解决方案）</td><td>✅</td><td>❌</td></tr><tr><td>非标订单管理</td><td>✅（自定义工作流）</td><td>✅（Business One）</td><td>✅（Power Apps）</td><td>✅</td><td>✅</td></tr></tbody></table><h4>2. 典型行业适配案例</h4><ul><li><strong>超兔</strong>：某电子工贸企业通过“功能白名单+自定义工作流”，快速调整订单审批流程，支持小批量非标订单生产，上线周期2周；</li><li><strong>SAP</strong>：杭州某半导体企业通过Business One构建晶圆制造合规体系，支撑新产线快速部署；某汽配企业通过Business One固化18项核心流程，符合IATF 16949标准；</li><li><strong>Dynamics 365</strong>：某机械制造企业通过Power Apps搭建设备维护模块，实现预防性维护计划自动提醒，设备停机率下降25%；</li><li><strong>八百客</strong>：某光伏企业通过PaaS平台自定义生产订单关联流程，实现“销售需求→生产排产”实时联动；</li><li><strong>OKKI</strong>：某外贸工贸企业通过跨境合同模板+回款预警，将坏账率从5%降至1.2%。</li></ul><h3>（四）实施与成本：平衡“定制深度”与“投入效率”</h3><p>工业企业数字化的常见陷阱是“为了定制化投入过高成本”，因此“实施周期”与“总成本”是关键决策因素：</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>部署方式</strong></td><td>云原生（SaaS）</td><td>云（S/4HANA Cloud）+本地（Business One）+混合</td><td>云（Dynamics 365 Cloud）+本地（On-Premises）+混合</td><td>云原生（SaaS）+本地部署</td><td>云原生（SaaS）</td></tr><tr><td><strong>实施周期</strong></td><td>小功能调整1-3天，全模块上线2-4周</td><td>中小企业（Business One）4-8周；大型企业（S/4HANA）3-6个月</td><td>低代码模块1-2周，复杂集成4-8周</td><td>复杂流程定制4-8周，通用模块2-4周</td><td>外贸场景快速上线2-4周</td></tr><tr><td><strong>总成本（年）</strong></td><td>中小工业企业：1-5万（SaaS订阅）</td><td>中小企业（Business One）：25万以内（license+实施）；大型企业：100万+</td><td>中小工业企业：10-30万（SaaS订阅+实施）；大型企业：50万+</td><td>中大型企业：20-50万（PaaS订阅+实施）</td><td>外贸企业：5-20万（SaaS订阅+实施）</td></tr><tr><td><strong>维护难度</strong></td><td>业务人员通过可视化工具自主调整，无需IT依赖</td><td>中小企业需依赖实施商，大型企业需专业IT团队</td><td>业务人员通过Power Platform自主维护，IT仅需支撑集成</td><td>需IT团队或实施商支撑复杂配置</td><td>业务人员自主调整外贸流程，IT支撑跨境集成</td></tr></tbody></table><p><strong>小结</strong>：</p><ul><li>低成本快上线选<strong>超兔</strong>：SaaS模式降低初始投入，可视化工具减少维护成本；</li><li>中小工业企业选<strong>SAP Business One</strong>：25万以内的总成本覆盖核心流程，行业模板降低实施风险；</li><li>微软生态用户选<strong>Dynamics 365</strong>：Office融合提升协作效率，低代码降低定制成本；</li><li>外贸企业选<strong>OKKI</strong>：跨境场景快速上线，成本可控；</li><li>中大型复杂企业选<strong>八百客</strong>：PaaS平台支撑深度定制，适配复杂流程。</li></ul><h2>三、可视化对比工具：Mermaid图表辅助决策</h2><h3>1. 核心能力框架脑图（Mermaid）</h3><pre><code>mindmap
  root((工业/工贸数字化体系))
    客制化能力
      超兔一体云: 可视化自定义(菜单/工作流/多表聚合)、小步迭代
      SAP: 行业模板(半导体/汽配)、系统集成(WMS/MES)
      Dynamics 365: Power Apps低代码、Office生态融合
      八百客: PaaS可视化配置、复杂流程定制
      OKKI: 外贸场景定制、跨境规则配置
    多端协同能力
      超兔一体云: 多端覆盖(Web/APP/小程序/RPA)、全链路数据同步
      SAP: 内部集成(ERP/WMS/MES)、外部Business Network
      Dynamics 365: Office融合(Outlook/Teams)、多角色终端
      八百客: PC/移动端同步、数据看板监控
      OKKI: 跨境多语言、物流跟踪
    场景适配性
      超兔一体云: 中小工业、非标订单
      SAP: 半导体/汽配、合规场景
      Dynamics 365: 机械制造、设备维护
      八百客: 光伏/制造、生产-销售协同
      OKKI: 外贸工贸、跨境回款</code></pre><h3>2. 多端协同流程时序图（Mermaid）</h3><p>以“销售订单→生产排产→物流交付”为例，展示各平台的协同逻辑：</p><pre><code>sequenceDiagram
    participant 销售(超兔APP) as S
    participant 生产(Web端) as P
    participant 仓储(小程序) as W
    participant 财务(Web端) as F
    participant SAP系统 as SAP
    participant Dynamics 365 as D365
    participant OKKI as O

    %% 超兔流程
    S-&gt;&gt;超兔系统: 录入客户非标订单
    超兔系统-&gt;&gt;P: 同步订单需求至生产排产
    P-&gt;&gt;超兔系统: 反馈生产周期
    超兔系统-&gt;&gt;W: 同步出库指令
    W-&gt;&gt;超兔系统: 扫码出库确认
    超兔系统-&gt;&gt;F: 同步应收数据

    %% SAP流程
    S-&gt;&gt;SAP Business One: 录入订单
    SAP Business One-&gt;&gt;MES系统: 触发生产工单
    MES系统-&gt;&gt;SAP Business One: 反馈生产进度
    SAP Business One-&gt;&gt;WMS系统: 触发出库
    WMS系统-&gt;&gt;SAP Business One: 反馈库存
    SAP Business One-&gt;&gt;F: 同步财务凭证

    %% Dynamics 365流程
    S-&gt;&gt;Outlook: 调取客户画像，发送报价邮件
    Outlook-&gt;&gt;D365: 同步邮件至CRM
    D365-&gt;&gt;Teams: 生成生产跟进任务
    Teams-&gt;&gt;P: 同步任务至生产排产
    P-&gt;&gt;D365: 反馈生产状态
    D365-&gt;&gt;Excel: 生成BI报表
    Excel-&gt;&gt;F: 同步财务数据

    %% OKKI流程
    S-&gt;&gt;OKKI移动端: 录入跨境订单
    OKKI移动端-&gt;&gt;供应商: 同步采购需求
    供应商-&gt;&gt;OKKI: 反馈备货状态
    OKKI-&gt;&gt;物流商: 触发国际物流
    物流商-&gt;&gt;OKKI: 同步Tracking Number
    OKKI-&gt;&gt;F: 同步回款数据</code></pre><h2>四、总结与建议</h2><p>在工业 4.0 与全球供应链重构的大背景下，工业/工贸企业数字化转型已成为提升竞争力的必由之路。“客制化 + 多端协同”能力是构建柔性业务数字化体系的核心要素，能够帮助企业精准适配复杂业务场景，实现全链路数据贯通，提升运营效率和市场响应速度。</p><p>通过对超兔一体云、SAP、Microsoft Dynamics 365、八百客 CRM、OKKI CRM 五大主流平台在客制化能力、多端协同能力、工业场景适配性、实施与成本四大核心维度的深度横评，我们可以看到每个平台都有其独特的优势和适用场景。企业在选择数字化体系时，应充分考虑自身的业务特点、发展阶段、行业需求以及预算限制，做出最为合适的决策。</p><p>对于小步快跑型、追求低成本快上线的中小工业企业，超兔一体云是不错的选择，其可视化自定义功能降低了技术门槛，SaaS 模式减少了初始投入和维护成本；行业合规要求高的企业，如半导体、汽配等行业，SAP 的垂直行业模板和强大的系统集成能力能够确保企业满足严格的合规标准；微软生态用户可以借助 Dynamics 365 的低代码平台和 Office 融合优势，提升协作效率并降低定制成本；外贸企业则可以优先考虑 OKKI CRM，其专属的跨境场景定制和可控的成本能够有效解决跨境协同和回款等痛点问题；而中大型复杂企业，尤其是有深度流程定制需求的企业，八百客 CRM 的 PaaS 平台能够提供强有力的支持。</p><p>总之，选择合适的数字化体系是工业/工贸企业实现业务流程柔性化与数字化升级的关键一步。希望本文的分析和建议能够为企业在数字化转型的道路上提供有价值的参考，助力企业在激烈的市场竞争中脱颖而出。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务与价格以厂商实际落地版本为准。）</p>]]></description></item>  </channel></rss>