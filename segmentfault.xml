<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[Linux再添一员猛将，操作完全不输Windows！ CodeSheep ]]></title>    <link>https://segmentfault.com/a/1190000047525754</link>    <guid>https://segmentfault.com/a/1190000047525754</guid>    <pubDate>2026-01-07 09:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>提到 <strong>Zorin OS</strong> 这个操作系统，可能不少喜欢折腾 Linux 系统的小伙伴之前有尝试过。</p><p>作为一款以 UI 交互和颜值著称的 Linux 发行版系统，Zorin OS 也曾一度被广大爱好者们称为 <strong>Windows 系统的开源替代方案</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525756" alt="" title=""/></p><p>Zorin OS 旨在简单易用，用户无需学习任何新知识即可上手，同时 Zorin OS 作为一款 Linux 发行版系统，<strong>专为从 Windows 迁移的用户设计</strong>，提供类似 Windows 的图形界面与操作逻辑，并且<strong>支持一键切换为 Windows 系统风格</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525757" alt="" title="" loading="lazy"/></p><p>前段时间，Zorin OS 团队在其官博正式宣布，最新的 Zorin OS 18 已经正式突破了 100 万次下载。</p><p>并且据官博数据显示，这些下载中<strong>有超过 78% 是来自于 Windows 系统的用户</strong>，这也再次印证了其可以满足从 Windows 桌面系统迁移到 Linux 发行版的用户需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525758" alt="" title="" loading="lazy"/></p><p>作为一个长期关注 Linux 桌面系统的博主，其实这次 Zorin OS 18 大版本更新刚出来那会我就关注了，不过一直没有抽出时间来写文章、来梳理，所以今天这篇文章正好把这件事情给安排了！</p><p>总体来讲，这次的 Zorin OS 18 是以 Ubuntu 24.04 LTS 为基础并由 Linux 6.14 内核提供支持。</p><p>并且这次的 Zorin OS 18 是继之前 17 版本以来的一次大版本迭代，带来了诸多新特性和改进。</p><p>所以接下来我们也来梳理一下这次 Zorin OS 18 所带来的一些重点更新和变化。</p><h2>视觉与交互进化</h2><p>众所周知，Zorin OS 一直以来都以其独特的个性和简约的美学设计风格而著称。</p><p>那这次更新后的新外观给人最直观的感受就是圆润和通透。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525759" alt="" title="" loading="lazy"/></p><p>任务栏这一次采用了全新的悬浮圆角面板设计，不再是死板地贴在屏幕边缘，而是像 macOS 的控制中心一样有一种轻盈的漂浮感。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525760" alt="" title="" loading="lazy"/></p><p>另外这一次大版本还推出了新主题颜色，新增了黄色和棕色两种主题色，视觉层次更加丰富。</p><p>选中元素的色调更加淡雅，背景和侧边栏颜色更深，长时间盯着屏幕写代码或处理文档，眼睛会舒服很多。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525761" alt="" title="" loading="lazy"/></p><p>另外 Pro 版里还提供了更多可切换的桌面布局。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525762" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525763" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525764" alt="" title="" loading="lazy"/></p><p>除此之外，很多经常使用的日常应用也进行了诸多设计调整和改进。</p><p>比如<strong>文件管理器</strong>的侧边栏重新设计了，操作控件更直观，搜索功能支持了全文搜索，找文件效率大增。</p><p><strong>日历</strong>应用增加了侧边栏，月份和事件视图也一目了然。</p><p><strong>相机</strong>应用也做了更新，新相机应用界面简洁，支持多摄像头切换，这对于现在动不动就开视频会议的环境非常友好。</p><h2>Web 应用深度集成</h2><p>对于用户来说，最大的痛点往往不是系统本身，而是数据迁移和应用生态，那 Zorin OS 18 在这方面下了不少功夫。</p><p>首先就是与 Web 应用程序无缝集成。</p><p>众所周知，现在很多应用都构建在云端，这些渐进式 Web 应用与原生应用之间的用户体验正逐渐融合。</p><p>这次 Zorin OS 18 全新内置的「Web Apps」工具非常强大，它可以将 Web 应用转换为桌面应用，用户的 Web 应用将可以显示在开始菜单中，使用起来与原生应用无异。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525765" alt="" title="" loading="lazy"/></p><p>「Web Apps」工具可以作为后端与各种热门 Web 浏览器集成，同时也允许用户自定义对应 Web 应用内的体验。</p><h2>多任务处理：原生窗口平铺</h2><p>这次 Zorin OS 18 的多任务处理变得好用多了。</p><p>Zorin OS 18 引入了一款功能强大的窗口平铺管理器，它能帮助用户更高效地工作，同时上手起来也十分简单。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525766" alt="" title="" loading="lazy"/></p><p>用户只需要把窗口拖到屏幕顶部，系统就会自动弹出布局选择器。</p><p>预设布局支持左右分屏、三栏布局、角落停靠等，同时在智能建议这块，系统也可以根据用户当前所打开的窗口，智能推荐最佳的排列组合。</p><p>除此之外它还支持高度自定义，创建用户自己的平铺布局。</p><p>这个新特性无论对新手还是资深玩家都非常直观易用，从而定制和提升每个用户的生产力。</p><h2>迁移神器：Windows 应用支持</h2><p>用户可以从内置的软件商店发现适用于 Zorin OS 系统的各类应用，这是在 Zorin OS 中安装应用的推荐方式。</p><p>其软件商店可让用户开箱即用地从 Zorin OS 与 Ubuntu APT 仓库、Flathub 以及 Snap Store 安装应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525767" alt="" title="" loading="lazy"/></p><p>而如果用户是刚从 Windows 转过来，看到满硬盘的 .exe 安装包肯定会头疼。</p><p>Zorin OS 18 的处理方式非常聪明。</p><p>系统内置了一个庞大的软件数据库（覆盖超过 170 款软件），当用户双击一个 Windows 安装包（如 setup.exe）时，系统不会直接报错，而是弹出一个友好的对话框。</p><p>如果有 Linux 原生版本，它就会引导你安装原生版本应用；如果没有原生版的话，它就会推荐你使用 Web 版，或者利用兼容层运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525768" alt="" title="" loading="lazy"/></p><p>在兼容层优化这一块，Zorin OS 18 深度集成了 Wine，对于一些必须在 Windows 下运行的行业软件或游戏，它提供了一个“Windows 应用支持”层。虽然不能保证 100% 兼容，但对于很多老旧的 .exe 工具，它能让你在不装虚拟机的情况下应急使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525769" alt="" title="" loading="lazy"/></p><h2>性能与硬件支持</h2><p>Zorin OS 18 基于 Ubuntu LTS 版本打造，同时它将获得直到 2029 年的稳定安全更新。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525770" alt="" title="" loading="lazy"/></p><p>同时官方宣称它甚至可以在十几年前的古董机上流畅运行。最低配置仅需 1GHz 双核 CPU、2GB 内存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525771" alt="" title="" loading="lazy"/></p><p>同时从用户安装的实际表现来看，在现代硬件上，它的动画流畅度非常高，即便在老机器上，它运行起来也比 Windows 系统更加轻快。</p><h2>写在最后</h2><p>那以上就是关于此次 Zorin OS 18 大版本更新的一些梳理和总结，感兴趣的小伙伴也可以去体验一波。</p><p>总的来看，这次的 Zorin OS 18 不仅仅是一个 Linux 发行版，也像极了一个操作系统迁移解决方案。</p><p>另外这次 Zorin OS 18 的发布，也使得 Linux 桌面系统的易用性又向前迈进了一步。</p><p>文章的最后也期待 Linux 桌面系统在未来能百花齐放，发展得越来越好。</p><p>好了，那以上就是今天的内容分享了，希望能对大家有所帮助，我们下篇见。</p><blockquote>注：本文在GitHub开源仓库「编程之路」 <a href="https://link.segmentfault.com/?enc=PCgLJyqi5O%2BTtLTgm1n%2FyA%3D%3D.IQaXZ73LMOvOMvGkY4035I4lHaZXZXNIysGlvskbc6soFJJPzCruaeoWl3MIhJCD" rel="nofollow" target="_blank">https://github.com/rd2coding/Road2Coding</a> 中已经收录，里面有我整理的6大编程方向(岗位)的自学路线+知识点大梳理、面试考点、我的简历、几本硬核pdf笔记，以及程序员生活和感悟，欢迎star。</blockquote>]]></description></item><item>    <title><![CDATA[剑指offer-60、将⼆叉树打印成多⾏ SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047516587</link>    <guid>https://segmentfault.com/a/1190000047516587</guid>    <pubDate>2026-01-07 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>题⽬描述</h2><p>从上到下按层打印⼆叉树，同⼀层结点从左⾄右输出。每⼀层输出⼀⾏。</p><p>给定的⼆叉树是 {1,2,3,#,#,4,5} :</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047516589" alt="" title=""/></p><p>该⼆叉树多⾏打印层序遍历的结果是：</p><pre><code class="text">[
[1],
[2,3],
[4,5]
]</code></pre><p>示例1<br/>输⼊：{8,6,10,5,7,9,11}<br/>返回值：[[8],[6,10],[5,7,9,11]]</p><h2>思路及解答</h2><p>59题的缩减版</p><h3>迭代法BFS（广度优先搜索）</h3><pre><code class="java">public class Solution {
    ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) {
        //层次打印遍历树
        ArrayList&lt;ArrayList&lt;Integer&gt; &gt; lists = new ArrayList&lt;&gt;();
        if(pRoot == null) return lists;
        Queue&lt;TreeNode&gt; q = new LinkedList&lt;&gt;();
        q.offer(pRoot);
        while(!q.isEmpty()){
            int size = q.size();
            ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();
            for(int i = 0; i &lt; size; i++){
                TreeNode temp = q.poll();
                list.add(temp.val);
                if(temp.left != null) q.offer(temp.left);
                if(temp.right != null) q.offer(temp.right);
            }
            lists.add(list);
        }
        return lists;
    }
    
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，每个节点恰好入队和出队各一次</li><li><strong>空间复杂度</strong>：O(n)，队列中最多存储n个节</li></ul><h3>递归DFS（深度优先搜索）</h3><p>虽然层序遍历通常用BFS，但也可以用DFS通过递归隐式维护层级信息来实现</p><pre><code class="java">import java.util.*;

public class Solution {

    public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; levelOrder(TreeNode root) {
        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;();
        if (root == null) return result;
        
        dfs(root, 0, result);
        return result;
    }
    
    private void dfs(TreeNode node, int depth, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result) {
        if (node == null) return;
        
        // 如果当前深度对应的列表不存在，创建新列表
        if (depth &gt;= result.size()) {
            result.add(new ArrayList&lt;&gt;());
        }
        
        // 将当前节点值加入对应深度的列表
        result.get(depth).add(node.val);
        
        // 递归处理左右子树，深度+1
        dfs(node.left, depth + 1, result);
        dfs(node.right, depth + 1, result);
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，每个节点访问一次</li><li><strong>空间复杂度</strong>：O(h)，递归栈深度等于树高，最坏情况O(n)</li></ul>]]></description></item><item>    <title><![CDATA[1956-2026：人类与机器智能的七十年对话 RTE开发者社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525614</link>    <guid>https://segmentfault.com/a/1190000047525614</guid>    <pubDate>2026-01-07 01:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>1956年夏天，当约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Lee Minsky）等先驱在达特茅斯学院首次提出“人工智能”这个概念时，他们乐观地预言：十年内机器将具备人类级别的推理能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525616" alt="" title=""/></p><p>七十年过去了，这个预言虽未完全实现，但AI的演进轨迹却远比当初设想的更加波澜壮阔——从符号推理的黄金时代到“AI寒冬”的沉寂，从机器学习的复兴到深度学习的爆发，再到2026年AI全面融入产业基础设施的当下。</p><p>这七十年的历史揭示了一个关键规律：AI的每一次突破，都源于思想碰撞、跨界融合与全球协作。1997年深蓝战胜卡斯帕罗夫，标志着暴力计算与精妙算法的混合突破；2010年后深度学习革命的爆发，则得益于大数据、GPU算力与神经网络架构创新的三重汇聚。而站在2026年这个新起点，当生成式AI、AI4S、具身智能等前沿趋势加速涌现，<strong>当国际竞合格局重塑全球创新生态，我们比以往任何时候都更需要一个能够汇聚顶级思想、链接全球资源、激发跨界创新的高浓度平台。</strong></p><p>“那么，东方为这场持续七十年的对话，按下了哪些关键按钮？”答案写在黄浦江畔。</p><h2>01 七十年的回响：浦江答卷</h2><p>上海的AI实践为这场全球对话提供了丰富的东方注脚。</p><p>这座城市不仅培育了像MiniMax、阶跃星辰这样的基础大模型先锋，在垂直领域，联影医疗将AI融入医疗影像诊断，松鼠AI打造个性化教育系统，小i机器人深耕政务智能化。</p><p>消费级市场则涌现出珞博智能与华为联合打造的“智能憨憨”情感陪伴硬件，探索人与AI之间超越工具性的“养成”关系；XREAL与Google合作深耕轻量级AR生态，推动AR设备从显示工具向日常化的空间计算终端演进。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525617" alt="" title="" loading="lazy"/></p><p>更令人瞩目的是，沪产机器人“智元”打破了“人形机器人行走最远距离”的吉尼斯世界纪录，标志着中国在具身智能领域的硬核突破。这些看似分散的创新成果，共同勾勒出上海作为全球AI重要节点的完整生态图景。</p><p>然而，在AI这场高度依赖算力、数据与资本的长期竞赛中，任何单一创新节点都面临着资源整合与全球链接的挑战。特别是对于寻求国际化发展的AI企业而言，如何高效对接全球市场、资本与人才，成为必须跨越的门槛。</p><h2>02 七十年的回响：双城新篇章</h2><p>当上海的AI产业积累需要更广阔的国际化舞台时，中国香港以其独特的“超级联系人”角色进入了视野。</p><p>这座城市正在迅速崛起为亚洲AI枢纽，目前已汇聚约500个AI相关组织、290家AI企业及180家投资机构，形成了高密度的创新生态。</p><p>香港的优势远不止于数字。其资本市场在2025年以2860亿港元的IPO募资额位居全球第一，成为科技与AI企业上市的首选地之一。与此同时，香港政府宣布投入30亿港元设立人工智能专项资助计划，建设AI研发院与超算中心。</p><p>一边是扎实的产业“底座”，一边是强大的国际“接口”，两者的历史性握手，只差一个契机。这个契机，随着维多利亚港的海风如期而至。2026年AI领域首场高浓度思想盛宴<strong>“WAIC UP!全球年终盛会”</strong>来到香港。这不仅是世界人工智能大会（WAIC）首次在港举办年度会议，更是上海AI产业实践与香港国际枢纽功能的一次历史性握手。</p><p>会议汇聚了WAIC旗下五大生态品牌——<strong>创新孵化引擎WAIC Future Tech、产业对接枢纽WAIC CONNECT、思想启迪窗口WAIC UP!、青年科教阵地WAIC Young以及全球合作舞台AI GRAVITY</strong>。这种多维度、立体化的平台设计，确保了不同背景的参会者都能找到自己的价值定位。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525618" alt="" title="" loading="lazy"/></p><p>对于上海及内地的AI创业者而言，<strong>这场盛会提供的不仅是展示舞台，更铺设了一张更广阔的全球AI协作网络。</strong> 从港交所的上市通道，到香港投资推广署的落地支持，从科学园的研发设施，到数码港的孵化生态，这些以往需要数月才能打通的环节，如今在一天之内就能建立初步联系。然而，链接资源只是第一步。在范式转换的临界点上，比资源更稀缺的是洞察未来的“思想地图”。</p><h2>03 WAIC UP!连接下一个七十年</h2><p><strong>国际级讲者阵容带来的历史纵深与前沿视野</strong></p><p>如今，我们正站在另一个范式转换的临界点：从单模态到多模态，从云端到边缘，从工具到伙伴……这些AI趋势的把握，不能仅靠闭门研发，更需要站在巨人肩膀上的思想启迪。上午场 <strong>“WAKE思想觉醒”</strong> 正是为此而设。</p><ul><li><strong>皮埃罗·斯加鲁菲（Piero Scaruffi）</strong>，作为硅谷人工智能研究院院长与硅谷精神布道师，见证了从专家系统兴衰到深度学习爆发的完整周期。他对AI演进周期性规律的洞察，将帮助参会者避免重蹈历史覆辙，在泡沫与实质之间保持清醒判断。</li><li><strong>史蒂夫·霍夫曼（Steve Hoffman）</strong>，作为Founders Space创始人与硅谷创投教父，孵化过数百家AI创业公司。他将分享从实验室到市场的转化密码，揭示哪些技术趋势真正具备商业化潜力——这正是避免1980年代专家系统式“虚假繁荣”的关键。</li><li><strong>朱晓波</strong>教授，作为“祖冲之号”量子计算总师，代表着AI算力革命的下一个前沿。量子计算与AI的结合，可能重现2010年GPU为深度学习带来的颠覆性加速，这是理解未来十年AI发展的战略制高点。</li><li>……</li></ul><p>如AI历史所示，1997年深蓝的胜利不仅是技术突破，更重塑了人们对“智能”的理解；2023年ChatGPT的爆发不仅是产品成功，更引发了对AGI路径的全球讨论。这些思想领袖的价值不仅在于传递信息，更在于提供思维框架；参会者将获得的不是碎片化的技术细节，而是构建AI时代世界观的思想基石。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525619" alt="" title="" loading="lazy"/></p><p><strong>产业全链条的立体化资源网络</strong></p><p>低代码/无代码AI平台的普及，使得非技术专家也能构建智能应用。但真正的挑战在于：如何将技术能力转化为产业价值？如何在垂直领域找到AI的最佳应用场景？<strong>下午场“UP拓维跃迁”</strong>精准回应这一需求，通过垂直应用案例、商业实战与出海战略的三维透视，构建从技术到商业的完整闭环。</p><ul><li><strong>技术供给侧：</strong> 商汤科技、科大讯飞等头部AI企业展示最新解决方案，从计算机视觉到语音交互，覆盖AI技术的全栈能力。这些企业经历了从研发到规模化部署的完整历程，其经验教训价值千金。</li><li><strong>产业需求侧：</strong> 中国移动国际、神州数码、易鑫集团等传统行业巨头分享数字化转型实践。他们的痛点与需求，正是AI创业者与技术提供商的机遇所在。</li><li><strong>基础设施层：</strong> 算丰等算力提供商、RTE开发者社区等技术生态，构成AI应用的底层支撑。正如深度学习革命依赖GPU算力突破，下一代AI应用同样需要新型基础设施的支撑。</li><li><strong>资本催化剂：</strong> 孚腾资本、Atma Capital等投资机构带来资本视角。他们对赛道的判断、对商业模式的洞察，能够帮助创业者避免方向性错误，加速从0到1的突破。</li></ul><p>这种产、投、创、研的四维聚合，令参会者可以在一天内完成通常需要数月的生态链接：上午吸收前沿思想，下午对接产业资源，实现从“知道”到“做到”的跨越。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525620" alt="" title="" loading="lazy"/></p><p><strong>AI时代的“达特茅斯时刻”</strong></p><p>达特茅斯会议的真正价值，不仅在于定义了“人工智能”这个术语，更在于它创造了一个跨学科思想自由碰撞的场域。数学家、工程师、心理学家、语言学家齐聚一堂，在非正式讨论中激发出影响后世七十年的核心概念。<strong>夜晚场“MORE灵感迸发”</strong> 正是要重现这种魔力。当正式议程结束，当西装革履卸下，当不同代际、不同领域、不同文化背景的参与者在轻松氛围中交流，往往会产生最意想不到的化学反应。</p><ul><li><strong>跨代际对话的独特价值：</strong> 青年科学家带来未被传统范式束缚的新鲜视角；资深专家提供历史纵深与战略判断；初创团队展现颠覆式创新的勇气；企业决策者贡献产业落地的实战智慧。</li><li><strong>跨领域融合的创新源泉：</strong> 夜晚场汇聚的多元群体——从机器人工程师到AGENT开发者，从科技媒体到社区运营者，从BIM国际青年联盟到WAYtoAGI社区——构成了一个思想熔炉。</li></ul><p>历史反复证明，AI的突破往往发生在学科交叉点。一个机器人工程师与一个内容创作者的对话，可能催生下一个爆款AI应用；一个出海企业家与一个国际组织代表的邂逅，可能开启跨境合作的新篇章。这种“计划外的收获”往往比正式议程更有价值。</p><h2>04 人类与未来的永恒对话</h2><p>以1956年达特茅斯会议正式定名“人工智能”为起点，AI的发展虽然历程尚短，却以远超预期的速度穿越了一个又一个技术关口，从逻辑推理、统计学习，到今天的大模型与多模态系统，智能以持续涌现的方式重塑着现实世界的运行节奏。</p><p><strong>而愈是演进加速的时刻，愈需要重新思考人的位置。</strong></p><p>算法可以更快，模型可以更大，但人的判断、情感与责任，从未可被替代。在碳基生命与硅基智能在文明进化的路口相遇时，我们亟需重新建立认知框架，回望我们想成为什么样的人类。</p><p>站在2026年的起点，我们更能体会这场演进的复杂与惊奇，每一次技术跃迁，像是文明的回响。正是在这样的时代拐点上，WAIC作为全球AI的重要思想交流平台，正持续拓展其“科技×人文”的边界，推动议题，凝聚共识。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525621" alt="" title="" loading="lazy"/></p><p>无论是思想刊物《WAIC UP!》，还是连接创业者、产业方、政策制定者、青年一代的多维平台，WAIC一直以来都承载着一个宏大的命题：我们愿意与AI共同走向怎样的未来。而即将到来的“WAIC UP!全球年终盛会”，也将为新一年的探索翻开新的篇章。</p><p>回顾这七十年，一个有趣的对比是：1956年的达特茅斯会议只有几十位参与者，而今天的WAIC将连接成千上万的全球头脑。规模的变化背后，是AI从学术课题到文明议题的演进。</p><p>我们的使命无比清晰，只要答案未至，步履永远不停。</p><p><strong>即刻锁票，与下一个70年对话</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525622" alt="" title="" loading="lazy"/></p><p>参考资料：</p><p>[1] AI养成系潮玩受资本追捧，“不看好早期具身智能”的朱啸虎也出手了，每日经济新闻，202506.</p><p>[2] Hong Kong Cements AI Hub Status with 500 Organizations, 23% IPO Surge, and AI-for-Finance Ecosystemic Leadership，香港金融发展局，202511.</p><p>[3] 打造香港成全球AI重要枢纽，文汇网，202511.</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525623" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525624" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=7drcaJ7LVpL4%2Bz%2F5L%2Bi3ag%3D%3D.2dpIsiRN94j0XvDyuvuhLEoXY4lvbKql6Nrw4iA3QP8%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525625" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[时代的眼泪，nameko 和 eventlet 停止维护后的项目自救，升级和替代之路 rabbitc]]></title>    <link>https://segmentfault.com/a/1190000047525550</link>    <guid>https://segmentfault.com/a/1190000047525550</guid>    <pubDate>2026-01-07 00:02:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="268" referrerpolicy="no-referrer" src="/img/bVdnzJh" alt="图片.png" title="图片.png"/></p><p>nameko 已经凉了, 最后一次 commit 停留在了 2023.11.3 。<a href="https://segmentfault.com/a/1190000045427395" target="_blank">如何将您的 Eventlet 项目迁移到 Asyncio</a></p><p>eventlet 也是几乎停止了维护，已经无法支持 cpython3.13+ 的版本了。<a href="https://segmentfault.com/a/1190000047525496" target="_blank">nameko 无法适配新版的python3.14，eventlet 停止维护导致的失效</a></p><p><a href="https://link.segmentfault.com/?enc=DDl0O7wwJZJ4fXO%2B1L5Cyg%3D%3D.z%2BJ%2Fykz%2BtA0nHJYu2rPZJk%2BbTTt88YTejeooFKKxRU3DBQWavBX15Nux%2FV9LgueRurTS7jTshsL09X8BlwdszQ%3D%3D" rel="nofollow" target="_blank">https://github.com/eventlet/eventlet/issues/1075</a></p>]]></description></item><item>    <title><![CDATA[Wispr 曝光内部项目：不仅转录文本还执行任务；苹果将推送 LLM 架构 Siri：支持屏幕感知与]]></title>    <link>https://segmentfault.com/a/1190000047525555</link>    <guid>https://segmentfault.com/a/1190000047525555</guid>    <pubDate>2026-01-07 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525557" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、NVIDIA 发布 Nemotron Speech ASR：缓存感知架构实现 24ms 极低延迟与 3 倍并发提升</strong></p><p>NVIDIA 发布开源模型 Nemotron Speech ASR，引入缓存感知流式技术替代传统的重叠缓冲推理。该架构通过仅处理音频增量并复用历史计算状态，解决了高并发环境下的延迟漂移问题，将单卡并发能力提升了 3 倍，为实时语音智能体提供了高性能的基础设施。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525558" alt="" title="" loading="lazy"/></p><ul><li><strong>缓存感知流式架构</strong>：弃用滑动窗口的重叠计算模式。通过在编码器层维护内部缓存状态，确保每帧音频仅被处理一次，实现了内存消耗的线性扩展，彻底消除计算冗余。</li><li><strong>8x 下采样 FastConformer 架构</strong>：模型规模 600M 参数，采用深度可分离卷积实现 8 倍下采样。相比行业主流的 4 倍下采样方案，该架构大幅减少了每秒处理的 Token 数量，显著降低 VRAM 占用。</li><li><strong>24ms 中值最终转录延迟</strong>：在实测中，该模型的 Time-To-Final（最终转录延迟）中值仅为 24ms，且性能不随语音长度增加而衰减。对比之下，同类本地模型延迟约为 90ms，主流 API 方案则通常超过 200ms。</li><li><strong>运行时动态延迟配置</strong>：支持在推理阶段实时切换 80ms、160ms、560ms 及 1.12s 等不同延迟模式。开发者无需重新训练模型，即可根据业务场景在响应速度与识别准确率之间取得平衡。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525559" alt="" title="" loading="lazy"/></p><ul><li><strong>高并发吞吐表现</strong>：单张 H100 GPU 可同时支持 560 个并发流（320ms 块大小），吞吐量较前代方案提升 300%。在 RTX A5000 等工作站级 GPU 上，并发能力提升可达 5 倍。</li></ul><p>模型已在 Hugging Face 开源，支持通过 NVIDIA NeMo 部署。</p><p>Hugging Face: </p><p><a href="https://link.segmentfault.com/?enc=lmx1xobecFhW8QFivgQqTQ%3D%3D.1rEiHmYnuXU%2FO7giLVLrK4CZFjnXMd3ygFfnGxyeLmEYku4lwkzxgomnbe%2BDIaLNEIoafqlmt%2Fu9NZsDeZ4cXA%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b</a></p><p>( @Huggingface)</p><p><strong>2、Boston Dynamics 联合 Google DeepMind：将 Gemini 基础模型集成至新一代 Atlas，开发 VLA 视觉-语言-动作模型</strong></p><p>Boston Dynamics 与 Google DeepMind 宣布达成战略合作，将 Gemini Robotics 基础模型引入新一代全电动「Atlas」机器人。该计划旨在利用大规模多模态模型提升人形机器人的感知推理与灵巧操作能力，首批应用目标锁定为汽车制造业的工业任务。</p><ul><li><strong>集成 Gemini Robotics 基础模型</strong>：基于 Google 的多模态「Gemini」大模型，为机器人提供视觉感知、逻辑推理及工具使用能力，使其能理解并执行复杂的跨模态指令。</li><li><strong>构建视觉-语言-动作（VLA）模型</strong>：双方将共同开发针对人形机器人的 VLA 模型，致力于将非结构化的环境信息直接映射为高维度的执行动作，提升机器人在复杂工业场景下的泛化能力。</li><li><strong>全电动「Atlas」机队部署</strong>：此次合作将完全基于 Boston Dynamics 最新的全电动版 Atlas 平台，利用其超越人类极限的关节活动范围（ROM）验证基础模型在端到端控制上的表现。</li><li><strong>工业级任务对齐</strong>：研发重心处于从「运动智能」向「通用智能」的跨越，重点解决汽车生产线等高动态环境下的灵巧操作与人机协作安全性。</li></ul><p>联合研究计划于 2026 年内正式启动，初期成果将率先在现代汽车工厂进行测试，暂未披露 API 开放计划或具体商用定价。</p><p>( @Boston Dynamics Blog)</p><h2>02 有亮点的产品</h2><p><strong>1、Symbolic Software 发布 Magicall：端到端加密视频通话，支持 SAS 验证与 EU 节点托管</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525560" alt="" title="" loading="lazy"/></p><p>加密咨询公司 Symbolic Software 推出「Magicall」Alpha 版，这是一款强调隐私的浏览器原生视频会议工具。该产品通过端到端加密技术提供无需客户端的即时通讯，旨在通过欧盟本地化托管和无 AI 训练政策解决企业协作中的数据主权与隐私安全问题。</p><ul><li><strong>端到端加密（E2EE）与 SAS 身份验证</strong>：音视频及聊天数据在浏览器端完成加密后再传输；引入「短验证字符串」（Short Authentication Strings， SAS）机制，允许用户通过比对校验码验证参与者身份，防范中间人攻击。</li><li><strong>Zero-Download 架构与固定 URL</strong>：采用 Web 浏览器原生运行，支持 Chrome、Firefox、Safari 和 Edge；用户可申领永久固定的房间链接，访客端无需注册账号或下载任何插件。</li><li><strong>欧盟本土化托管与数据主权</strong>：服务器节点全部位于欧盟境内，由总部位于巴黎的厂商开发，完全符合 GDPR 规范；官方明确承诺不使用通话数据进行 AI 模型训练，且不包含任何广告追踪插件。</li><li><strong>高标准安全背书</strong>：由曾为 Coinbase、1Password、Bitwarden、Zoom 等提供过 250 余项安全审计的 Symbolic Software 团队研发，底层协议基于开放标准构建，强调低延迟与高音频清晰度。</li></ul><p>当前处于 Alpha 测试阶段，提供 Free 永久免费版（单次会议限 5 人、30 分钟，支持无限次重启），用户可通过邮箱注册申领房间名。</p><p>相关链接：</p><p><a href="https://link.segmentfault.com/?enc=xltNwLWkMClnoMN7gq3YcQ%3D%3D.Ce%2BVpU5RQkt%2BaAtwshPd03ixeO3KjNJOQAqXj6uJXU8%3D" rel="nofollow" target="_blank">https://magicall.online/</a></p><p>( @Magicall)</p><p><strong>2、Apple Vision Pro 联合 Spectrum 推出湖人队沉浸式赛事直播：150 Mbps 码率、7 处视角及 3D 悬浮 UI</strong></p><p>Apple 与「Spectrum」宣布将于 2026 年 1 月 9 日起在「Apple Vision Pro」上推出「Spectrum Front Row」直播服务。该服务通过 Apple Immersive 视频技术直播洛杉矶湖人队赛事，旨在通过高带宽流媒体和空间交互技术提供原生虚拟现实观赛体验。</p><ul><li><strong>高吞吐量视频流与 180° 沉浸感</strong>：直播源提供最高 150 Mbps 码率的 Apple Immersive 视频，覆盖 7 个特制拍摄机位，包括记录台、篮架下方、球员通道及解说席。</li><li><strong>3D 空间实时图形渲染</strong>：计分板、球员名单及 24 秒计时器等动态数据以 3D 元素呈现，利用 visionOS 的空间计算能力悬浮于现实环境中。</li><li><strong>Ambisonic 空间音频技术</strong>：利用球场部署的多维麦克风捕捉环境音，通过「Spatial Audio」算法还原球鞋摩擦声、篮网入网声及现场观众的方位感。</li><li><strong>硬件与系统协同</strong>：该功能仅支持搭载 M2 或 M5 芯片的 「Apple Vision Pro」，且系统版本需更新至「visionOS 26」或更高版本。</li><li><strong>分阶段播控策略</strong>：直播期间，暂停、半场休息及球员入场环节将保持实时传输，不切换为传统商业广告广告位，维持全流程场内临场感。</li></ul><p>2026 年 1 月 9 日首播；直播覆盖美国南加州等湖人队转播区，全球其他地区（含日、新、韩等）支持通过「NBA」App 观看部分直播或赛后 24 小时回放。</p><p>( @Apple Newsroom)</p><p><strong>3、Apple 拟于 iOS 26.4 推送 LLM 架构 Siri：支持屏幕感知与 App Intents</strong></p><p>Apple 计划在 iOS 26.4 更新中正式上线基于 LLM 架构的新版 Siri。通过彻底替换运行多年的底层架构，新版 Siri 将具备类 ChatGPT/Gemini 的逻辑理解能力，并实现对系统全局任务的深度接管。</p><ul><li><strong>底层架构重构</strong>：弃用传统的规则/模板匹配系统，转向以 LLM 为核心的推理引擎，旨在提升复杂指令的解析精度与对话连贯性。</li><li><strong>App Intents 实现系统级操作</strong>：通过强化的智能体能力，Siri 可直接调用应用程序内的特定功能，实现全自动的「免提计算」。</li><li><strong>屏幕感知</strong>：Siri 将具备理解当前显示内容的能力，能够基于屏幕上的文本、图像或上下文信息直接执行后续指令。</li><li><strong>个人语境感知</strong>：整合跨 App 的用户数据，使 Siri 能够理解涉及个人日程、偏好及历史交互的私有化指令。</li><li><strong>发布周期预测</strong>：参考 iOS 18.4 与 16.4 的发布节奏（均为 3 月下旬），iOS 26.4 预计于 2026 年 1 月底进入 Beta 测试，3 月正式推送。</li></ul><p>( @9to5Mac\@X)</p><p><strong>4、Amazon 发布 Alexa.com：Alexa+ 全面转向「智能体」架构，支持 Web 端交互与个人数据集成</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525561" alt="" title="" loading="lazy"/></p><p>Amazon 在 CES 2026 上宣布推出 Alexa.com，将基于 LLM 的「Alexa+」服务正式从硬件端延伸至 Web 浏览器。此举通过「智能体化」的 UI 重构与跨平台集成，试图将 Alexa 从单一的语音工具转型为覆盖全平台的个人/家庭自动化中心。</p><ul><li><strong>「智能体」优先的架构重塑</strong>：Alexa 移动端及 Web 端 UI 全面转向聊天机器人界面，将原本的功能图标入口降权，优先通过自然语言交互触发底层服务。</li><li><strong>非原生数据集成能力</strong>：针对缺乏自有办公套件的劣势，Alexa+ 新增邮件、日历及个人文档（如 PDF、照片）的转发与上传接口，允许用户通过文件投喂建立家庭私有知识库，支持检索疫苗记录、学校行程等非结构化信息。</li><li><strong>第三方服务深度接入</strong>：新增 Angi、Expedia、Square 及 Yelp 等 API 集成，配合已有的 Uber、OpenTable 和 Ticketmaster 接口，支持通过智能体直接完成餐厅预订、行程规划及家政预约。</li><li><strong>硬件生态无缝兼容</strong>：Alexa+ 已适配 97% 的现有设备（约 6 亿台 Echo 系列），支持旧款硬件调用新版模型能力，通过后端云端更新实现向后兼容。</li><li><strong>高频交互数据验证</strong>：Early Access 数据显示，转向 Alexa+ 后，用户对话频率提升 2-3 倍，购物行为增长 3 倍，食谱与智能家居控制等高阶功能的使用率分别提升 500% 和 50%。</li></ul><p>已向 Alexa+ Early Access 计划的活跃用户开放，需通过 Amazon 账号登录使用。</p><p>( @TechCrunch)</p><p><strong>5、能帮你做家务的机器人 LG CLOiD 首次亮相 CES</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525562" alt="" title="" loading="lazy"/></p><p>据 The Verge 报道，LG 在 CES 主题演讲中正式展示了其家务机器人 CLOiD 的实际运行效果，LG 将其定位为打造「零劳动家庭」的重要组成部分。</p><p>CLOiD 在舞台上以双手挥动的方式亮相，随后在 LG 家电事业部销售副总裁 Brandt Varner 的指令下，示范了将一条湿毛巾放入洗衣机的完整流程。</p><p>洗衣机门自动打开后，机器人伸出左臂，将毛巾放入滚筒。整个过程耗时约 30 秒，展示了其具备基础家务执行能力，但效率仍有提升空间。</p><p>在演讲后半段，CLOiD 再次登场，为 LG HVAC 事业部高级副总裁 Steve Scarbrough 递上水瓶，并根据其语气判断需求后主动提供帮助，甚至完成了拳碰动作，强调其具备一定的情感交互能力。</p><p>LG 此前已预告该机器人具备多项家务能力，包括从冰箱取牛奶、在烤箱中烤可颂、叠放衣物等。此次演示进一步展示了其在家庭场景中的潜在应用。不过，LG 仍未公布 CLOiD 的上市时间或是否会真正面向消费者销售。</p><p>( @APPSO)</p><h2>03 有态度的观点</h2><p><strong>1、Wispr 创始人：内部项目「Wispr Actions」不仅生成文本，还能直接执行任务</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525563" alt="" title="" loading="lazy"/></p><p>Wispr 首席执行官 Tanay Kothari 近日表示，尽管让办公族放弃键盘是一场「艰苦的战斗」，但 AI 的普及正成为变革的催化剂。目前 Wispr 估值约 7 亿美元，月收入及用户数环比增长达 50%。</p><p>Kothari 认为：<strong>「AI 工具是人们开始使用 Flow 的『gateway drug』。他们下载它，在 ChatGPT 或 Cursor 中使用，到了第二或第三周，他们就会意识到，『为什么我不随处都使用它呢？』然后他们就开始在所有的 Slack 消息和电子邮件中使用它。」</strong></p><p>数据显示，该工具已让深度用户的每日打字时间从 5 小时减至 3 小时，且使用五个月后，72% 的电脑活动均通过语音完成。</p><p>与传统逐字转录工具不同，Flow 侧重于理解语境与意图。Kothari 强调，用户需要的是符合逻辑的书面表达：<strong>「其他模型会逐字转录你所说的一切，但那不是人们想要的——你说的话与你写的字非常不同，所以输出应该反映你实际会写出的样子。」</strong></p><p>通过结合 Llama 3.1 等模型，Flow 实现了高精度输出并降低了在办公室发声的「社交门槛」。在安全性上，Wispr 凭借「零数据留存」模式成功打入严监管领域。</p><p>Kothari 透露，仅约 25% 至 30% 的用户选择共享数据用于训练，这帮助公司：<strong>「获得了一些规模最大、最严格的金融机构的青睐……我们即将在欧洲最大的银行之一进行部署。身处欧洲又是银行——我还没遇到过比这要求更高的地方。」</strong></p><p>展望未来，Kothari 致力于打造现实版 J.A.R.V.I.S。，将人类从屏幕束缚中解放。他感性地表示：「我不希望我的孩子在成长过程中整天盯着手机看。对我来说，那太……令人沮丧了。我希望他们昂首挺胸地走路，而不是被屏幕所束缚。实现这一目标的唯一方法是开发一个人们真正信任的语音界面。」</p><p>其内部称为「Wispr Actions」的项目被列为今年的重点关注内容，语音交互有望从单纯的文本生成，跨越至代为执行复杂任务的新阶段。</p><p>( @Computerworld)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525564" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525565" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=%2F6biZmq5vKYkqowk95cCRQ%3D%3D.79KZq2I%2Fcb2Ymualr83%2Bm53wjysgg5UcJZgZfziwSe8%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525566" alt="" title="" loading="lazy"/></p><p>作者提示：个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[Wget安装教程：Windows命令行下载工具部署步骤（附环境变量配置方法） 读书笔记 ]]></title>    <link>https://segmentfault.com/a/1190000047525224</link>    <guid>https://segmentfault.com/a/1190000047525224</guid>    <pubDate>2026-01-06 23:07:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><h3><strong>1. 先搞清楚：Wget 是什么？</strong> ​</h3><p>Wget 是个命令行下载工具，能从网上下载文件（支持 HTTP/HTTPS/FTP），Linux 自带，但 Windows 没有，得自己装。装完后，在 cmd 里输 <code>wget + 网址</code>就能直接下载，比浏览器右键另存为方便多了。</p><h4><strong>2. 下载 Wget.exe 安装包</strong>​</h4><p>不用找复杂安装程序，直接下<strong>单文件版</strong>最省事：</p><ul><li>选对应系统版本：64位系统下 <code>wget.exe</code>（通常文件名带 <code>x64</code>或直接叫 <code>wget.exe</code>），32位下 <code>wget-i686.exe</code>；</li></ul><h4><strong>3. 安装？其实就是“放对位置”</strong> ​</h4><p>Windows 装 Wget 不用双击安装，两步搞定：</p><ol><li><strong>找个固定文件夹</strong>：比如在 D 盘建个 <code>Tools\Wget</code>文件夹（路径别带中文，比如 <code>D:\Tools\Wget</code>）；</li><li><strong>复制 wget.exe 进去</strong>：把下载好的 <code>wget.exe</code>粘贴到这个文件夹里，完事儿！</li></ol><h4><strong>4. 配置环境变量（关键！否则 cmd 找不到命令）</strong> ​</h4><p>想直接在 cmd 里输 <code>wget</code>就用，得把 Wget 的路径加到系统环境变量里：</p><ol><li>右键“此电脑”→“属性”→“高级系统设置”→“环境变量”；</li><li>在“系统变量”里找到 <code>Path</code>，双击它；</li><li>点“新建”，输入你放 <code>wget.exe</code>的文件夹路径（比如 <code>D:\Tools\Wget</code>），点“确定”保存。</li></ol><h4><strong>5. 验证是否装好</strong>​</h4><p>按 <code>Win+R</code>输 <code>cmd</code>打开命令提示符，输：</p><pre><code>wget --version</code></pre><p>如果显示版本信息（比如 <code>GNU Wget 1.21.3</code>），就成功了！</p><h4><strong>6. 试试用 Wget 下载文件</strong>​</h4><p>比如下载个图片试试：</p><pre><code>wget https://example.com/test.jpg</code></pre><p>当前 cmd 所在文件夹就会出现 <code>test.jpg</code>，说明能正常用。</p><p>​</p>]]></description></item><item>    <title><![CDATA[Web 平台开发日记 - 可观测性实践 天天向尚 ]]></title>    <link>https://segmentfault.com/a/1190000047525329</link>    <guid>https://segmentfault.com/a/1190000047525329</guid>    <pubDate>2026-01-06 23:06:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Web 平台开发日记 - 可观测性实践</h2><blockquote><strong>核心内容</strong>: Prometheus 监控集成、健康检查、请求追踪、结构化日志、可观测性体系<br/><strong>技术栈</strong>: Go + Gin + Prometheus + Correlation ID + Structured Logging</blockquote><hr/><h3>📋 目录</h3><ol><li><a href="#目标" target="_blank">目标</a></li><li><a href="#可观测性架构" target="_blank">可观测性架构</a></li><li><a href="#prometheus-指标集成" target="_blank">Prometheus 指标集成</a></li><li><a href="#健康检查实现" target="_blank">健康检查实现</a></li><li><a href="#correlation-id-请求追踪" target="_blank">Correlation ID 请求追踪</a></li><li><a href="#结构化日志系统" target="_blank">结构化日志系统</a></li></ol><hr/><h3>🎯 目标</h3><ul><li>[x] Prometheus 指标收集与暴露</li><li>[x] Health/Readiness 探针实现</li><li>[x] Correlation ID 请求追踪</li><li>[x] 结构化日志（JSON 格式）</li><li>[x] 完整的验收测试体系</li><li>[x] 监控栈配置（Prometheus + Grafana）</li></ul><p><strong>核心价值</strong>：</p><ol><li><strong>可观测性</strong> - 实时掌握系统运行状态</li><li><strong>故障诊断</strong> - 快速定位和排查问题</li><li><strong>请求追踪</strong> - 跨服务的端到端追踪</li><li><strong>生产就绪</strong> - 符合企业级运维标准</li></ol><p>项目 GitHub 地址：<a href="https://link.segmentfault.com/?enc=2CL%2FDUV1qMk1RS4%2FOo16sw%3D%3D.lN2MvXDdQaZijei4omusMFXKA9OKRHCpNsbMj4Z1HVqPzp4s8KUKzMp%2Ff7bhMMRD" rel="nofollow" target="_blank">https://github.com/Mythetic/web_platform</a></p><hr/><h3>🏗️ 可观测性架构</h3><h4>三大支柱</h4><p>现代应用的可观测性（Observability）由三大支柱构成：</p><pre><code>┌─────────────────────────────────────────────────────────────┐
│                     可观测性三大支柱                          │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  📊 Metrics (指标)          📝 Logs (日志)         🔍 Traces (追踪)  │
│  ────────────────          ───────────────         ────────────────  │
│  • 系统性能指标            • 应用运行日志           • 请求调用链路    │
│  • HTTP 请求计数           • 错误详细信息           • 跨服务追踪      │
│  • 响应时间分布            • 业务操作记录           • 性能瓶颈定位    │
│  • 资源使用率              • 结构化输出             • 依赖关系分析    │
│                                                               │
│  工具: Prometheus          工具: ELK/Loki           工具: Jaeger     │
│                                                               │
└─────────────────────────────────────────────────────────────┘</code></pre><h4>本章实现架构</h4><pre><code>┌────────────────────────────────────────────────────────────┐
│                        用户请求                             │
└──────────────────────┬─────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                   Gin 中间件层                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │ CorrelationID│→ │StructuredLog │→ │PrometheusMetrics│   │
│  │  生成请求ID   │  │  JSON日志    │  │   收集指标    │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└─────────────────────────────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                    业务处理层                                │
│  • API Handlers                                             │
│  • Business Logic                                           │
│  • Database Access                                          │
└─────────────────────────────────────────────────────────────┘
                       │
        ┌──────────────┼──────────────┐
        ▼              ▼              ▼
┌──────────────┐ ┌──────────┐ ┌─────────────┐
│ /metrics     │ │ /health  │ │ server.log  │
│ (Prometheus) │ │ (K8s)    │ │ (JSON)      │
└──────┬───────┘ └────┬─────┘ └──────┬──────┘
       │              │               │
       ▼              ▼               ▼
┌──────────────┐ ┌──────────┐ ┌─────────────┐
│ Prometheus   │ │ LoadBalancer│ │ LogAggregator│
│   Server     │ │ HealthCheck │ │  (ELK/Loki) │
└──────┬───────┘ └──────────┘ └─────────────┘
       │
       ▼
┌──────────────┐
│   Grafana    │
│  Dashboard   │
└──────────────┘</code></pre><h4>数据流向</h4><pre><code>用户请求 → CorrelationID中间件（生成UUID）
         ↓
         StructuredLogger中间件（记录请求信息）
         ↓
         PrometheusMetrics中间件（开始计时、增加并发计数）
         ↓
         业务Handler处理
         ↓
         PrometheusMetrics中间件（记录延迟、状态码、递减并发）
         ↓
         StructuredLogger中间件（记录响应信息）
         ↓
         返回响应（携带 X-Request-ID header）</code></pre><hr/><h3>📊 Prometheus 指标集成</h3><h4>为什么需要 Prometheus？</h4><p><strong>问题场景</strong>：</p><ul><li>❓ 系统现在有多少并发请求？</li><li>❓ API 响应时间是否正常？</li><li>❓ 哪些接口最慢？</li><li>❓ 错误率是否在增加？</li></ul><p><strong>Prometheus 的答案</strong>：</p><ul><li>✅ 实时采集应用指标</li><li>✅ 时间序列数据存储</li><li>✅ 强大的查询语言（PromQL）</li><li>✅ 图形化展示（Grafana）</li></ul><h4>指标类型设计</h4><p>在 <code>server/middleware/metrics.go</code> 中定义了三类核心指标：</p><h5>1. HTTP 请求计数（Counter）</h5><pre><code class="go">var httpRequestsTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "http_requests_total",
        Help: "Total number of HTTP requests",
    },
    []string{"method", "path", "status"},
)</code></pre><p><strong>用途</strong>：统计每个接口的总请求次数，按 HTTP 方法、路径、状态码分类。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看所有接口的请求总数
sum(http_requests_total)

# 查看错误请求（5xx）
sum(http_requests_total{status=~"5.."})

# 查看登录接口的成功率
rate(http_requests_total{path="/base/login",status="200"}[5m])</code></pre><h5>2. HTTP 请求延迟（Histogram）</h5><pre><code class="go">var httpRequestDuration = promauto.NewHistogramVec(
    prometheus.HistogramOpts{
        Name:    "http_request_duration_seconds",
        Help:    "HTTP request latency in seconds",
        Buckets: prometheus.DefBuckets, // [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
    },
    []string{"method", "path"},
)</code></pre><p><strong>用途</strong>：记录接口响应时间的分布情况，支持百分位数计算（P50、P95、P99）。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看 API 的 P95 延迟（95% 的请求在这个时间内完成）
histogram_quantile(0.95, http_request_duration_seconds_bucket)

# 查看平均响应时间
rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])

# 查看慢接口（&gt;1秒）
histogram_quantile(0.99, http_request_duration_seconds_bucket{path="/api/some-slow-endpoint"})</code></pre><h5>3. HTTP 并发请求数（Gauge）</h5><pre><code class="go">var httpRequestsInFlight = promauto.NewGauge(
    prometheus.GaugeOpts{
        Name: "http_requests_in_flight",
        Help: "Current number of HTTP requests being served",
    },
)</code></pre><p><strong>用途</strong>：实时显示当前正在处理的请求数量。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看当前并发数
http_requests_in_flight

# 查看最近 5 分钟的最大并发数
max_over_time(http_requests_in_flight[5m])</code></pre><h4>中间件实现</h4><pre><code class="go">func PrometheusMetrics() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 跳过 metrics 端点本身（避免递归）
        if c.Request.URL.Path == "/metrics" {
            c.Next()
            return
        }
        
        // 1. 增加并发计数
        httpRequestsInFlight.Inc()
        defer httpRequestsInFlight.Dec()
        
        // 2. 记录开始时间
        start := time.Now()
        
        // 3. 执行业务逻辑
        c.Next()
        
        // 4. 计算请求耗时
        duration := time.Since(start).Seconds()
        
        // 5. 收集指标
        status := strconv.Itoa(c.Writer.Status())
        method := c.Request.Method
        path := c.FullPath() // 使用路由路径而不是原始URL（避免高基数）
        
        httpRequestsTotal.WithLabelValues(method, path, status).Inc()
        httpRequestDuration.WithLabelValues(method, path).Observe(duration)
    }
}</code></pre><p><strong>关键设计考虑</strong>：</p><ol><li><p><strong>避免高基数问题</strong>：</p><ul><li>✅ 使用 <code>c.FullPath()</code> 而不是 <code>c.Request.URL.Path</code></li><li>原因：路由路径固定（如 <code>/api/user/:id</code>），而实际 URL 可能有无数个（<code>/api/user/1</code>, <code>/api/user/2</code>, ...）</li><li>高基数会导致 Prometheus 内存暴涨</li></ul></li><li><p><strong>跳过 /metrics 端点</strong>：</p><ul><li>避免 Prometheus 抓取自身指标时产生递归记录</li><li>减少无意义的指标数据</li></ul></li><li><p><strong>使用 <code>defer</code> 确保计数正确</strong>：</p><ul><li>即使请求 panic，并发计数也会正确递减</li></ul></li></ol><h4>Metrics 端点暴露</h4><pre><code class="go">// server/api/v1/system/metrics.go
type MetricsApi struct{}

func (m *MetricsApi) GetMetrics(c *gin.Context) {
    handler := promhttp.Handler()
    handler.ServeHTTP(c.Writer, c.Request)
}

// server/initialize/router.go
metricsApi := &amp;system.MetricsApi{}
router.GET("/metrics", metricsApi.GetMetrics)</code></pre><p>访问 <a href="https://link.segmentfault.com/?enc=vFhZs7vrLZ%2FJK4nNGa8P%2Fg%3D%3D.wT%2By6KJL20UCy9ppos%2Bp4B2mFSKbu5QZj%2FOi%2Frcobs8%3D" rel="nofollow" target="_blank">http://localhost:8888/metrics</a> 可以看到：</p><pre><code># HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",path="/api/health",status="200"} 145
http_requests_total{method="POST",path="/base/login",status="200"} 23
http_requests_total{method="GET",path="/api/user/getList",status="200"} 67

# HELP http_request_duration_seconds HTTP request latency in seconds
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="0.005"} 142
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="0.01"} 145
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="+Inf"} 145
http_request_duration_seconds_sum{method="GET",path="/api/health"} 0.523
http_request_duration_seconds_count{method="GET",path="/api/health"} 145

# HELP http_requests_in_flight Current number of HTTP requests being served
# TYPE http_requests_in_flight gauge
http_requests_in_flight 2</code></pre><h4>Prometheus 配置</h4><p>在 <code>deploy/monitoring/prometheus.yml</code> 中配置抓取任务：</p><pre><code class="yaml">scrape_configs:
  - job_name: 'ewp-backend'
    static_configs:
      - targets: ['host.containers.internal:8888']
    metrics_path: '/metrics'
    scrape_interval: 15s  # 每 15 秒抓取一次</code></pre><ul><li><code>host.containers.internal</code> 是 Podman 访问宿主机的特殊域名</li><li>容器内的 Prometheus 通过这个域名连接到宿主机的 8888 端口</li><li>在生产环境中，应该使用服务发现（Kubernetes Service、Consul 等）</li></ul><hr/><h3>🏥 健康检查实现</h3><h4>为什么需要健康检查？</h4><p><strong>场景</strong>：</p><ul><li>Kubernetes 需要知道 Pod 是否存活（Liveness）</li><li>负载均衡器需要知道实例是否就绪（Readiness）</li><li>运维人员需要快速判断服务状态</li></ul><h4>Liveness Probe - 存活探针</h4><p><strong>用途</strong>：判断应用进程是否存活，如果失败，Kubernetes 会重启 Pod。</p><pre><code class="go">// server/api/v1/system/health.go
func (h *HealthApi) GetHealth(c *gin.Context) {
    response.OkWithData(gin.H{
        "status":    "ok",
        "timestamp": time.Now().Format(time.RFC3339),
    }, c)
}</code></pre><p><strong>API 返回</strong>：</p><pre><code class="bash">GET /api/health

{
  "code": 0,
  "data": {
    "status": "ok",
    "timestamp": "2026-01-05T10:15:30Z"
  },
  "msg": "success"
}</code></pre><p><strong>Kubernetes 配置示例</strong>：</p><pre><code class="yaml">livenessProbe:
  httpGet:
    path: /api/health
    port: 8888
  initialDelaySeconds: 30  # 启动后 30 秒开始检查
  periodSeconds: 10        # 每 10 秒检查一次
  timeoutSeconds: 5        # 超时时间 5 秒
  failureThreshold: 3      # 连续失败 3 次才重启</code></pre><h4>Readiness Probe - 就绪探针</h4><p><strong>用途</strong>：判断应用是否准备好接收流量，如果失败，负载均衡器会摘除这个实例。</p><pre><code class="go">func (h *HealthApi) GetReadiness(c *gin.Context) {
    checks := make(map[string]string)
    allHealthy := true

    // 1. 检查 MySQL 连接
    if err := checkMySQLConnection(); err != nil {
        checks["mysql"] = "error: " + err.Error()
        allHealthy = false
    } else {
        checks["mysql"] = "ok"
    }

    // 2. 检查 Redis 连接
    if err := checkRedisConnection(); err != nil {
        checks["redis"] = "error: " + err.Error()
        allHealthy = false
    } else {
        checks["redis"] = "ok"
    }

    // 3. 返回结果
    if allHealthy {
        response.OkWithData(gin.H{
            "status": "ready",
            "checks": checks,
        }, c)
    } else {
        c.JSON(503, gin.H{
            "code":   503,
            "status": "not ready",
            "checks": checks,
        })
    }
}</code></pre><p><strong>API 返回示例</strong>：</p><p>成功时（HTTP 200）：</p><pre><code class="json">{
  "code": 0,
  "data": {
    "status": "ready",
    "checks": {
      "mysql": "ok",
      "redis": "ok"
    }
  }
}</code></pre><p>失败时（HTTP 503）：</p><pre><code class="json">{
  "code": 503,
  "status": "not ready",
  "checks": {
    "mysql": "error: connection refused",
    "redis": "ok"
  }
}</code></pre><p><strong>Kubernetes 配置示例</strong>：</p><pre><code class="yaml">readinessProbe:
  httpGet:
    path: /api/ready
    port: 8888
  initialDelaySeconds: 10   # 启动后 10 秒开始检查
  periodSeconds: 5          # 每 5 秒检查一次
  timeoutSeconds: 3         # 超时时间 3 秒
  successThreshold: 1       # 成功 1 次即认为就绪
  failureThreshold: 3       # 连续失败 3 次才摘除</code></pre><h4>健康检查实现细节</h4><pre><code class="go">// 检查 MySQL 连接
func checkMySQLConnection() error {
    if global.EWP_DB == nil {
        return fmt.Errorf("Database connection not initialized")
    }
    
    sqlDB, err := global.EWP_DB.DB()
    if err != nil {
        return err
    }
    
    // 执行一个简单的查询来验证连接
    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
    defer cancel()
    
    return sqlDB.PingContext(ctx)
}

// 检查 Redis 连接
func checkRedisConnection() error {
    if global.EWP_REDIS == nil {
        return fmt.Errorf("Redis connection not initialized")
    }
    
    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
    defer cancel()
    
    return global.EWP_REDIS.Ping(ctx).Err()
}</code></pre><p><strong>关键设计</strong>：</p><ol><li><strong>超时控制</strong>：每个检查都设置 2 秒超时，避免阻塞</li><li><strong>依赖检查</strong>：只有所有依赖都健康，才返回就绪状态</li><li><strong>详细反馈</strong>：返回每个依赖的具体状态，方便排查</li></ol><hr/><h3>🔍 Correlation ID 请求追踪</h3><h4>为什么需要 Correlation ID？</h4><p><strong>问题场景</strong>：</p><ul><li>用户报告"登录失败"，但日志里有成千上万条记录，如何找到这个用户的请求？</li><li>一个请求经过了多个微服务，如何追踪完整的调用链路？</li><li>如何将前端错误、后端日志、数据库慢查询关联起来？</li></ul><p><strong>Correlation ID 的答案</strong>：</p><ul><li>为每个请求分配唯一的 UUID</li><li>贯穿请求的整个生命周期</li><li>记录在日志、响应头、调用链中</li><li>支持分布式追踪</li></ul><h4>实现方式</h4><pre><code class="go">// server/middleware/correlation.go
const CorrelationIDKey = "X-Request-ID"

func CorrelationID() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 1. 尝试从请求头获取 Correlation ID
        correlationID := c.GetHeader(CorrelationIDKey)
        
        // 2. 如果没有，生成新的 UUID
        if correlationID == "" {
            correlationID = uuid.New().String()
        }
        
        // 3. 存储到 Gin Context（供其他中间件使用）
        c.Set(CorrelationIDKey, correlationID)
        
        // 4. 设置响应头（返回给客户端）
        c.Writer.Header().Set(CorrelationIDKey, correlationID)
        
        c.Next()
    }
}</code></pre><h4>使用场景</h4><h5>场景 1：单次请求追踪</h5><pre><code class="bash"># 客户端发起请求（不带 Request ID）
curl -i http://localhost:8888/api/health

# 响应头包含自动生成的 Request ID
HTTP/1.1 200 OK
X-Request-ID: 3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c
Content-Type: application/json
...</code></pre><p>后端日志中可以看到：</p><pre><code class="json">{
  "correlation_id": "3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c",
  "method": "GET",
  "path": "/api/health",
  "status": 200,
  "duration": "2.5ms"
}</code></pre><h5>场景 2：请求链传播</h5><pre><code class="bash"># 客户端主动带上 Request ID（用于追踪）
curl -H "X-Request-ID: my-custom-request-id" http://localhost:8888/api/user/getList

# 响应会保持相同的 Request ID
HTTP/1.1 200 OK
X-Request-ID: my-custom-request-id
...</code></pre><p><strong>分布式场景</strong>：</p><pre><code>前端 (Request ID: ABC123)
  ↓
API Gateway (透传 ABC123)
  ↓
User Service (使用 ABC123 记录日志)
  ↓ 调用数据库时在 SQL 注释中包含 ABC123
  ↓
MySQL Slow Query Log (/* RequestID: ABC123 */ SELECT ...)</code></pre><h5>场景 3：日志聚合与搜索</h5><p>在 ELK/Loki 中搜索：</p><pre><code># 搜索某个请求的所有日志
correlation_id:"3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c"

# 结果：
# [Service A] 接收请求
# [Service A] 调用 Service B
# [Service B] 查询数据库
# [Service B] 返回结果
# [Service A] 返回响应</code></pre><ol><li><strong>客户端支持</strong>：前端应该在重试、长轮询时保持相同的 Request ID</li><li><strong>下游传播</strong>：调用其他服务时，必须传递 Correlation ID</li><li><strong>数据库注释</strong>：在 SQL 查询中添加注释 <code>/* RequestID: xxx */</code></li><li><strong>错误报告</strong>：错误信息中包含 Correlation ID，方便用户反馈时快速定位</li></ol><hr/><h3>📝 结构化日志系统</h3><h4>为什么需要结构化日志？</h4><p><strong>传统文本日志的问题</strong>：</p><pre><code>2026-01-05 10:15:30 [INFO] User login from IP 192.168.1.100
2026-01-05 10:15:31 [INFO] API /api/user/getList took 45ms, status=200</code></pre><ul><li>❌ 难以解析和搜索</li><li>❌ 没有统一格式</li><li>❌ 缺少关键信息（如 Request ID）</li><li>❌ 无法高效聚合分析</li></ul><p><strong>结构化日志（JSON）的优势</strong>：</p><pre><code class="json">{
  "timestamp": "2026-01-05T10:15:30Z",
  "level": "info",
  "correlation_id": "3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c",
  "method": "GET",
  "path": "/api/user/getList",
  "status": 200,
  "duration": "45ms",
  "duration_ms": 45,
  "ip": "192.168.1.100",
  "user_agent": "Mozilla/5.0...",
  "user_id": "123"
}</code></pre><ul><li>✅ 机器可读，易于解析</li><li>✅ 字段统一，便于搜索</li><li>✅ 包含完整上下文</li><li>✅ 支持高效聚合查询</li></ul><h4>实现方式</h4><pre><code class="go">// server/middleware/logger.go
func StructuredLogger() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 1. 记录开始时间
        start := time.Now()
        
        // 2. 执行业务逻辑
        c.Next()
        
        // 3. 计算请求耗时
        duration := time.Since(start)
        
        // 4. 获取 Correlation ID
        correlationID, _ := c.Get(CorrelationIDKey)
        
        // 5. 获取用户信息（如果已认证）
        userID := ""
        if claims, exists := c.Get("claims"); exists {
            if jwtClaims, ok := claims.(*systemReq.CustomClaims); ok {
                userID = strconv.Itoa(int(jwtClaims.BaseClaims.ID))
            }
        }
        
        // 6. 构造结构化日志
        logData := map[string]interface{}{
            "timestamp":      time.Now().Format(time.RFC3339),
            "correlation_id": correlationID,
            "method":         c.Request.Method,
            "path":           c.Request.URL.Path,
            "status":         c.Writer.Status(),
            "duration":       duration.String(),
            "duration_ms":    duration.Milliseconds(),
            "ip":             c.ClientIP(),
            "user_agent":     c.Request.UserAgent(),
        }
        
        if userID != "" {
            logData["user_id"] = userID
        }
        
        // 7. 输出 JSON 日志
        logJSON, _ := json.Marshal(logData)
        global.EWP_LOG.Info(string(logJSON))
    }
}</code></pre><h4>日志字段说明</h4><table><thead><tr><th>字段</th><th>类型</th><th>说明</th><th>示例</th></tr></thead><tbody><tr><td><code>timestamp</code></td><td>string</td><td>日志时间（ISO 8601）</td><td><code>2026-01-05T10:15:30Z</code></td></tr><tr><td><code>correlation_id</code></td><td>string</td><td>请求追踪 ID</td><td><code>3c5f6a8b-1e2d-4f9a...</code></td></tr><tr><td><code>method</code></td><td>string</td><td>HTTP 方法</td><td><code>GET</code>, <code>POST</code></td></tr><tr><td><code>path</code></td><td>string</td><td>请求路径</td><td><code>/api/user/getList</code></td></tr><tr><td><code>status</code></td><td>int</td><td>HTTP 状态码</td><td><code>200</code>, <code>404</code>, <code>500</code></td></tr><tr><td><code>duration</code></td><td>string</td><td>人类可读的耗时</td><td><code>45ms</code>, <code>1.2s</code></td></tr><tr><td><code>duration_ms</code></td><td>int</td><td>毫秒数（便于聚合）</td><td><code>45</code>, <code>1200</code></td></tr><tr><td><code>ip</code></td><td>string</td><td>客户端 IP</td><td><code>192.168.1.100</code></td></tr><tr><td><code>user_agent</code></td><td>string</td><td>浏览器标识</td><td><code>Mozilla/5.0...</code></td></tr><tr><td><code>user_id</code></td><td>string</td><td>用户 ID（如已登录）</td><td><code>123</code></td></tr></tbody></table><h4>日志查询示例</h4><p><strong>在 ELK 中查询</strong>：</p><pre><code class="javascript">// 查询某个用户的所有请求
user_id:"123"

// 查询慢请求（&gt;1秒）
duration_ms:&gt;1000

// 查询错误请求
status:&gt;=500

// 查询某个时间段的请求
timestamp:[2026-01-05T10:00:00Z TO 2026-01-05T11:00:00Z]

// 聚合分析：统计各状态码的数量
{
  "aggs": {
    "status_codes": {
      "terms": { "field": "status" }
    }
  }
}</code></pre><p><strong>在 Loki 中查询</strong>：</p><pre><code class="logql"># 查询某个 Request ID 的所有日志
{job="ewp-backend"} | json | correlation_id="3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c"

# 统计每分钟的请求数
sum(rate({job="ewp-backend"}[1m]))

# 查询 P99 响应时间
histogram_quantile(0.99, sum(rate({job="ewp-backend"} | json | __error__="" | unwrap duration_ms [5m])) by (le))</code></pre><h4>日志级别规范</h4><pre><code class="go">// 不同场景使用不同日志级别
global.EWP_LOG.Debug(logJSON)   // 调试信息（生产环境不输出）
global.EWP_LOG.Info(logJSON)    // 正常请求（我们的选择）
global.EWP_LOG.Warn(logJSON)    // 警告信息（如慢查询）
global.EWP_LOG.Error(logJSON)   // 错误信息（如 5xx）
global.EWP_LOG.Fatal(logJSON)   // 致命错误（进程退出）</code></pre><p><strong>日志级别选择</strong>：</p><ul><li><code>Info</code>：正常的 HTTP 请求（200, 201, 204）</li><li><code>Warn</code>：可能有问题的请求（401, 403, 404, 请求超时）</li><li><code>Error</code>：服务器错误（500, 502, 503, panic）</li></ul><hr/><h4>后续优化方向</h4><h5>1. 监控告警</h5><pre><code class="yaml"># Prometheus 告警规则示例
groups:
  - name: ewp_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) &gt; 0.05
        for: 5m
        annotations:
          summary: "High error rate detected"
          
      - alert: HighLatency
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) &gt; 1
        for: 5m
        annotations:
          summary: "API latency P95 &gt; 1s"</code></pre><h5>2. 分布式追踪</h5><p>集成 Jaeger 实现完整的分布式追踪：</p><pre><code class="go">// 使用 OpenTelemetry 标准
import "go.opentelemetry.io/otel"

func TracingMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        ctx, span := tracer.Start(c.Request.Context(), c.FullPath())
        defer span.End()
        
        // 传播 Trace Context
        c.Request = c.Request.WithContext(ctx)
        c.Next()
    }
}</code></pre><h5>3. 日志聚合</h5><p>将日志发送到 ELK 或 Loki：</p><pre><code class="yaml"># Promtail 配置（Loki）
clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: ewp-backend
    static_configs:
      - targets:
          - localhost
        labels:
          job: ewp-backend
          __path__: /path/to/server.log</code></pre><hr/><h3>📚 相关文档</h3><h4>技术文档</h4><ul><li><a href="https://link.segmentfault.com/?enc=Yz4uS7F8lG6EwiushlaSsw%3D%3D.qsiiKDIHnE9rx0I4lZVwr5S6Ps24yjSG1J27ZAwCSz8texgm%2BDs4%2Fg%2FQ7vAO9rl8Mh9rNP6VS2te7Iaso7ehvQ%3D%3D" rel="nofollow" target="_blank">Prometheus 官方文档</a> - 指标收集与监控</li><li><a href="https://link.segmentfault.com/?enc=Z9ywPQsuBT1gpD5oveGegA%3D%3D.2rom8yt40ga4zD%2BwsEF8VWDhCFCSew0arhIW4A4R4kj0XuoxHXaqYRqDXUaNxV%2FK" rel="nofollow" target="_blank">Prometheus 最佳实践</a> - 指标命名规范</li><li><a href="https://link.segmentfault.com/?enc=%2FMuR5nY8aRaXIhB7Vqctsg%3D%3D.1FyRQSJ53OtO9Y4%2Fs89YXG7XPCYKa9fH6FH%2FEPc6zcM23H1Txy2r1XWgrTVW8KB0hIWlptDV4uwj7SM8%2FGougg%3D%3D" rel="nofollow" target="_blank">OpenTelemetry Go SDK</a> - 分布式追踪标准</li><li><a href="https://link.segmentfault.com/?enc=MAmhXiTtHyZ1OLmT%2FBfDzQ%3D%3D.VNvN%2BYS7re7kbNXj1hGWAqA5AyGpaN6n7qKN5wp9I7c%3D" rel="nofollow" target="_blank">Structured Logging in Go</a> - Zap 日志库</li></ul><h4>Kubernetes 健康检查</h4><ul><li><a href="https://link.segmentfault.com/?enc=6j%2BCvyS98BOHXA59C3CHqg%3D%3D.bUVj%2ByoR6Jp2bEm0t7NhvYZR1rmuI%2B8iDIGoQKZOAy7XM%2FnH%2F9V3pCXA4eaci1AnGOD8DBwf29VPGodu1VktKe7UzQGuYaOF2ErfDTUC9ZZ4oois8mEU7zBCT4%2BE4TNMF2jHDvVg8a2YU8xY6InJRg%3D%3D" rel="nofollow" target="_blank">Configure Liveness, Readiness Probes</a> - K8s 探针配置</li><li><a href="https://link.segmentfault.com/?enc=cAuH4tOPf9rx4kr%2Bf%2FAPqA%3D%3D.hqc3OtqjlKxovDfDHDxmSy%2F0FFiawv8chgL9AsdTpNcugrLDuU5soFobkAZasi8Ii3y6z0R613394Ogu6Yf5rqnkf3LTo3dnQoudFRvexIkyHS%2Bi8j0MkrJRqEwga%2FM21Y5%2BfqhPizhBv8fhNSZL4tqBgqYBL%2Bkh%2BNqrXWs9dpVqEFF4m3fBLToC6i6yZaZx3a5F4Ws6cd1XlPL%2FknTGcg%3D%3D" rel="nofollow" target="_blank">Health Check Best Practices</a> - Google 最佳实践</li></ul><h4>可观测性理论</h4><ul><li><a href="https://link.segmentfault.com/?enc=qMy%2FXzpVcHckJgsvXVg%2BOQ%3D%3D.zIfqqLnzOKn03VXUUVa9Qa%2FIU5WykPQW0XQBZJdLL6hDNH4wT45khUmBZzOCCot1qP3PYAhUjkT0N6c3HNiUn2rhpVfel96n5iuZHKPRWJsE582pbTWPKya4UKNd26Ld" rel="nofollow" target="_blank">The Three Pillars of Observability</a> - O'Reilly 可观测性理论</li><li><a href="https://link.segmentfault.com/?enc=l8qSGClQefDWb3JoMi%2Fv2w%3D%3D.TFEngsQGHJxraCzNO3NGEzGfjDnq%2BtmLg1PH%2FktrUVIjyo%2BislMRV6A%2Fq5cQFCXNWtUW5kRsLByTEDOx1xxNHBCoYC0kCWDtr%2FmTo1gARKg%3D" rel="nofollow" target="_blank">Logs vs Metrics vs Traces</a> - 三者的区别与联系</li></ul><h3>🔗 项目地址</h3><ul><li><strong>GitHub</strong>: <a href="https://link.segmentfault.com/?enc=MgCJsioo07iqjAODRxOjmg%3D%3D.FFdnSqFPmML%2Brsg0IrMA19upaggki00XCVwNmQ59iPDWhto2oqa13CLKE4G6D8WR" rel="nofollow" target="_blank">https://github.com/Mythetic/web_platform</a></li></ul>]]></description></item><item>    <title><![CDATA[【赵渝强老师】OceanBase的配置文件与配置项 赵渝强老师 ]]></title>    <link>https://segmentfault.com/a/1190000047525398</link>    <guid>https://segmentfault.com/a/1190000047525398</guid>    <pubDate>2026-01-06 23:05:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在OceanBase集群中，OBServer节点工作目录下通常有audit、bin、etc、etc2、etc3、log、run、store等目录，但这些目录并非都是安装必须的。在启动OBServer节点前需要保证etc、log、run、store这4的目录存在，同时store下应该有clog、slog、sstable这3个目录。etc2、etc3是备份配置文件用的，由OBServer节点创建。audit下存放的是审计日志，也由OBServer节点创建。bin目录用于存放observer二进制文件。如下所示：</p><pre><code class="powershell">[root@node11 ~]# tree observer/ -d -L 1
observer/
├── admin
├── audit
├── bin
├── etc                        
├── etc2                    
├── etc3                    
├── lib
├── log                        
├── log_obshell
├── run
└── store -&gt; /root/obdata

11 directories</code></pre><p>etc、etc2、etc3都是配置文件目录。这三个目录里的内容是完全一致的，区别是后两个目录是OBServer节点创建的，第一个目录是启动前需要准备的。etc2和etc3是配置文件额外保存的目录，由配置项config_additional_dir控制。当配置修改以后，除了会写标准的etc/observer.config.bin以外，还会额外在这些目录创建配置项文件。server启动不会读取额外目录的配置项文件，只是作为额外备份。额外目录如果有权限会自动创建，没有权限则日志中报ERROR。</p><p>视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV1oyipB7Ezh/?aid=115840217258517&amp;cid=35196570069" target="_blank">https://www.bilibili.com/video/BV1oyipB7Ezh/?aid=115840217258...</a></p><p>OceanBase数据库的配置项分为集群级配置项和租户级配置项。OBServer节点会将所有的配置项序列化后保存到工作目录下的配置文件etc/observer.config.bin中，之后在这个工作目录下启动OBServer节点都会读取这个配置文件。</p><ul><li>普通租户使用SHOW PARAMETERS语句查看本租户级配置项信息的SQL语句如下：</li></ul><pre><code class="powershell">SHOW PARAMETERS [SHOW_PARAM_OPTS]</code></pre><ul><li>系统租户可以使用SHOW PARAMETERS语句查看集群级配置项和租户级配置项信息。并可通过增加TENANT关键字信息查看指定租户的配置项。</li></ul><pre><code class="powershell">SHOW PARAMETERS [SHOW_PARAM_OPTS] TENANT = tenant_name</code></pre><p>例如：</p><pre><code class="powershell"># 查询所有的配置信息
ob&gt; show parameters;
...+-------------------------+-----------+-------+....
...| name                    | data_type | value |....
...+-------------------------+-----------+-------+....
...| utl_file_open_max       | INT       | 50    |....
...| _use_odps_jni_connector | BOOL      | True  |....
...| ob_java_connector_path  | STRING    |       |....
...| ob_java_opts            | STRING    |       |....
...| ob_java_home            | STRING    |       |....
......

# 执行模糊查询
ob&gt; show parameters like 'datafile%';
...+--------------------------+-----------+-------+...
...| name                     | data_type | value |...
...+--------------------------+-----------+-------+...
...| datafile_disk_percentage | INT       | 0     |...
...| datafile_maxsize         | CAPACITY  | 12G   |...
...| datafile_next            | CAPACITY  | 1G    |...
...| datafile_size            | CAPACITY  | 1G    |...
...+--------------------------+-----------+-------+...
4 rows in set (0.015 sec)</code></pre>]]></description></item><item>    <title><![CDATA[某知名it培训班前端三阶段vue相关面试题 Nedpill ]]></title>    <link>https://segmentfault.com/a/1190000047525408</link>    <guid>https://segmentfault.com/a/1190000047525408</guid>    <pubDate>2026-01-06 23:04:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>1. Vue 的核心是什么？</h4><p>Vue 的核心主要包含两点：</p><ul><li>​<strong>数据驱动（Data-Driven）</strong>​：视图由数据状态决定，数据变更自动更新 DOM，无需手动操作 DOM；</li><li>​<strong>组件化（Component-Based）</strong>​：将页面拆分为独立、可复用的组件，降低耦合度，提升开发效率；</li><li>补充：核心还包括响应式系统、虚拟 DOM 等底层支撑能力。</li></ul><h4>2. 请简述你对 Vue 的理解</h4><p>Vue 是一套​<strong>渐进式 JavaScript 框架</strong>​，核心定位是“渐进式”——可以按需使用核心功能（如响应式、组件），也可结合路由（Vue Router）、状态管理（Vuex/Pinia）等生态扩展复杂应用；</p><ul><li>设计理念：轻量、易用、高效，兼顾开发体验和运行性能；</li><li>核心特性：响应式数据绑定、组件化、指令系统、虚拟 DOM、生命周期等；</li><li>应用场景：从简单的表单页面到复杂的单页应用（SPA）均可覆盖，是前端主流框架之一。</li></ul><h4>3. 请简述 Vue 的单向数据流</h4><p>Vue 的单向数据流核心规则：​<strong>数据只能从父组件流向子组件，子组件不能直接修改父组件传递的 props</strong>​；</p><ul><li><p>具体表现：</p><ul><li>父组件通过 props 向子组件传值，子组件只读 props，不能直接修改；</li><li>若子组件需修改数据，需通过触发父组件的自定义事件，由父组件修改源数据，再反向更新子组件 props；</li></ul></li><li>目的：保证数据流向可追溯，避免多个组件随意修改数据导致状态混乱，符合“单向绑定”的设计思想。</li></ul><h4>4. Vue 常用的修饰符有哪些</h4><p>Vue 的修饰符按用途可分为三类，核心常用如下：</p><table><thead><tr><th>类型</th><th>常用修饰符</th><th>作用举例</th></tr></thead><tbody><tr><td>事件修饰符</td><td>.stop、.prevent、.once</td><td>.stop 阻止事件冒泡，.prevent 阻止默认行为，.once 只触发一次</td></tr><tr><td>按键修饰符</td><td>.enter、.esc、.tab</td><td>监听特定按键触发事件（如 @keyup.enter）</td></tr><tr><td>表单修饰符</td><td>.trim、.number、.lazy</td><td>.trim 去除输入首尾空格，.number 转为数字，.lazy 失去焦点后更新数据</td></tr><tr><td>鼠标修饰符</td><td>.left、.right、.middle</td><td>监听鼠标特定按键（左键/右键/中键）</td></tr></tbody></table><h4>5. v-text 与 {{}}的区别</h4><p>两者均用于渲染文本，核心区别：</p><ul><li><p>​<strong>{{}}（插值表达式）</strong>​：</p><ul><li>可嵌入 HTML 标签内（如 <code>&lt;div&gt;姓名：{{name}}&lt;/div&gt;</code>）；</li><li>存在“闪烁问题”（页面加载时可能先显示 <code>{{name}}</code> 再渲染值，可通过 <code>v-cloak</code> 解决）；</li><li>支持简单表达式（如 <code>{{age + 1}}</code>）；</li></ul></li><li><p>​<strong>v-text</strong>​：</p><ul><li>是指令，需直接绑定在标签上（如 <code>&lt;div v-text="name"&gt;&lt;/div&gt;</code>）；</li><li>无闪烁问题，覆盖标签内所有内容（包括子节点）；</li><li>不支持复杂表达式，仅接收变量/简单值；</li></ul></li><li>补充：两者均会转义 HTML（若需渲染 HTML 用 <code>v-html</code>）。</li></ul><h4>6. v-on 可以绑定多个方法吗？</h4><p>可以，有两种实现方式：</p><ul><li><p>方式 1：绑定一个方法数组（Vue 2.4+ 支持）</p><pre><code class="Plain">&lt;button @click="[handleClick1, handleClick2]()"&gt;点击触发多个方法&lt;/button&gt;</code></pre></li><li><p>方式 2：绑定一个统一方法，内部调用多个子方法</p><pre><code class="Plain">&lt;button @click="handleAll"&gt;点击触发多个方法&lt;/button&gt;
&lt;script&gt;
export default {
  methods: {
    handleAll() {
      this.handleClick1();
      this.handleClick2();
    },
    handleClick1() { /* 逻辑1 */ },
    handleClick2() { /* 逻辑2 */ }
  }
}
&lt;/script&gt;</code></pre></li><li>注意：数组方式中方法需加 <code>()</code> 执行，否则仅定义不触发。</li></ul><h4>7. Vue 循环的 key 作用</h4><p><code>key</code> 是 Vue 列表渲染的核心属性，作用：</p><ul><li>​<strong>唯一标识节点</strong>​：Vue 根据 key 判断节点是否为同一节点，避免复用错误（如输入框值错乱）；</li><li>​<strong>提升更新效率</strong>​：当列表数据变化时，Vue 通过 key 精准定位需要更新的节点，而非重新渲染整个列表；</li><li><p>注意：</p><ul><li>key 需用唯一值（如 id），避免用 index（index 会随数据顺序变化，失去标识意义）；</li><li>无 key 时 Vue 会采用“就地更新”策略，可能导致 DOM 复用异常。</li></ul></li></ul><h4>8. 什么是计算属性？</h4><p>计算属性（computed）是 Vue 用于处理<strong>派生数据</strong>的特性，基于依赖数据动态计算值：</p><ul><li><p>核心特性：</p><ul><li>​<strong>缓存性</strong>​：依赖数据不变时，多次访问计算属性只会执行一次计算，提升性能；</li><li>​<strong>响应式</strong>​：依赖数据变化时，计算属性自动重新计算并更新视图；</li><li>支持 get/set（默认 get，set 可手动修改依赖数据）；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  data() {
    return { a: 1, b: 2 };
  },
  computed: {
    sum() { // 只读计算属性
      return this.a + this.b;
    },
    fullName: { // 可读写计算属性
      get() { return this.firstName + ' ' + this.lastName; },
      set(val) { const [first, last] = val.split(' '); this.firstName = first; this.lastName = last; }
    }
  }
}
&lt;/script&gt;</code></pre></li></ul><h4>9. Vue 单页面（SPA）的优缺点</h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>1. 无页面刷新，体验接近原生 App</td><td>1. 首屏加载慢（需加载整包 JS/CSS）</td></tr><tr><td>2. 组件化复用性高</td><td>2. SEO 不友好（页面内容动态渲染）</td></tr><tr><td>3. 前后端分离，开发效率高</td><td>3. 路由切换需手动处理缓存/滚动</td></tr><tr><td>4. 数据管理更集中</td><td>4. 打包体积大，需按需加载优化</td></tr></tbody></table><h4>10. Vuex 是什么？怎么使用？在哪些场景下使用？</h4><ul><li>​<strong>定义</strong>​：Vuex 是 Vue 官方的​<strong>集中式状态管理库</strong>​，用于管理多组件共享的状态（如用户信息、全局配置）；</li><li><p>​<strong>使用步骤</strong>​：</p><ul><li>安装：<code>npm install vuex --save</code>；</li><li><p>创建 store：</p><pre><code class="JavaScript">import Vue from 'vue';
import Vuex from 'vuex';
Vue.use(Vuex);
export default new Vuex.Store({
  state: { count: 0 }, // 状态
  mutations: { increment(state) { state.count++ } }, // 同步修改状态
  actions: { asyncIncrement({ commit }) { setTimeout(() =&gt; commit('increment'), 1000) } }, // 异步操作
  getters: { doubleCount(state) { return state.count * 2 } }, // 派生状态
  modules: { /* 模块拆分 */ }
});</code></pre></li><li>挂载到 Vue 实例：<code>new Vue({ store, ... })</code>；</li><li><p>组件中使用：</p><ol><li>读取 state：<code>this.$store.state.count</code> 或 <code>mapState</code> 辅助函数；</li><li>修改 state：<code>this.$store.commit('increment')</code>（同步）/ <code>this.$store.dispatch('asyncIncrement')</code>（异步）；</li></ol></li></ul></li><li><p>​<strong>使用场景</strong>​：</p><ul><li>多组件共享同一状态（如购物车、用户登录状态）；</li><li>组件层级深，props/emit 传值繁琐；</li><li>需要追踪状态变更（Vuex 可记录状态修改日志）。</li></ul></li></ul><h4>11. Vuex 与 Pinia 的区别</h4><p>Pinia 是 Vue 3 推荐的状态管理库，替代 Vuex 4，核心区别：</p><table><thead><tr><th>维度</th><th>Vuex</th><th>Pinia</th></tr></thead><tbody><tr><td>核心结构</td><td>分 state/mutations/actions/getters/modules</td><td>仅 state/actions/getters（无 mutations/modules）</td></tr><tr><td>模块化</td><td>需通过 modules 嵌套，命名空间复杂</td><td>每个 store 独立，天然模块化，无需命名空间</td></tr><tr><td>TypeScript</td><td>支持差，需手动类型声明</td><td>原生支持 TS，类型推断更友好</td></tr><tr><td>代码简洁度</td><td>冗余（如 mutations 必须同步）</td><td>简洁（actions 可同步/异步，无需 commit）</td></tr><tr><td>Vue 版本支持</td><td>Vue 2/3（Vuex 3 对应 Vue 2，Vuex 4 对应 Vue 3）</td><td>主要支持 Vue 3（也可兼容 Vue 2）</td></tr><tr><td>调试</td><td>依赖 Vue Devtools，需配置</td><td>原生集成 Vue Devtools，调试更友好</td></tr></tbody></table><h4>12. Vue 路由的跳转方式</h4><p>Vue Router 的跳转分两类：声明式（模板）和编程式（JS）：</p><ul><li><p>​<strong>声明式（&lt;router-link&gt;）</strong>​：</p><pre><code class="Plain">&lt;!-- 基础跳转 --&gt;
&lt;router-link to="/home"&gt;首页&lt;/router-link&gt;
&lt;!-- 带参数 --&gt;
&lt;router-link :to="{ path: '/user', query: { id: 1 } }"&gt;用户页&lt;/router-link&gt;
&lt;router-link :to="{ name: 'User', params: { id: 1 } }"&gt;用户页&lt;/router-link&gt;</code></pre></li><li><p>​<strong>编程式（&amp;dollar;router.push/replace/go）</strong>​：</p><pre><code class="JavaScript">// 基础跳转
this.$router.push('/home');
// 带query参数（路径拼接，如/user?id=1）
this.$router.push({ path: '/user', query: { id: 1 } });
// 带params参数（需路由配置name，如/user/1）
this.$router.push({ name: 'User', params: { id: 1 } });
// 替换当前历史记录（不新增历史）
this.$router.replace('/home');
// 前进/后退
this.$router.go(-1); // 后退一页</code></pre></li></ul><h4>13. 跨域的解决方式</h4><p>跨域是浏览器同源策略限制（协议、域名、端口任一不同即跨域），前端常用解决方案：</p><ol><li><p>​<strong>Vue CLI 代理（开发环境）</strong>​：</p><pre><code class="JavaScript">// vue.config.js
module.exports = {
  devServer: {
    proxy: {
      '/api': {
        target: 'http://localhost:3000', // 后端接口地址
        changeOrigin: true, // 开启跨域
        pathRewrite: { '^/api': '' } // 重写路径
      }
    }
  }
}</code></pre></li><li>​<strong>后端 CORS（跨域资源共享）</strong>​：后端设置响应头 <code>Access-Control-Allow-Origin: *</code>（或指定域名）；</li><li>​<strong>JSONP</strong>​：仅支持 GET 请求，通过动态创建 <code>&lt;script&gt;</code> 标签请求；</li><li>​<strong>Nginx 反向代理</strong>​：生产环境通过 Nginx 转发请求，统一域名；</li><li>​<strong>WebSocket</strong>​：基于 TCP 协议，无跨域限制。</li></ol><h4>14. Vue 生命周期请简述</h4><p>Vue 生命周期是组件从<strong>创建到销毁</strong>的全过程，分 8 个核心阶段（Vue 2）：</p><ol><li>​<strong>创建阶段</strong>​：beforeCreate（实例初始化，数据/方法未挂载）→ created（数据/方法挂载完成，DOM 未生成）；</li><li>​<strong>挂载阶段</strong>​：beforeMount（编译模板，即将挂载 DOM）→ mounted（DOM 挂载完成，可操作 DOM）；</li><li>​<strong>更新阶段</strong>​：beforeUpdate（数据更新，DOM 未重新渲染）→ updated（DOM 重新渲染完成）；</li><li>​<strong>销毁阶段</strong>​：beforeDestroy（实例即将销毁，数据/方法仍可用）→ destroyed（实例销毁，所有监听/绑定解除）；</li></ol><ul><li>Vue 3 补充：组合式 API 中用 <code>onMounted</code>/<code>onUpdated</code> 等钩子替代选项式，新增 <code>setup</code>（替代 beforeCreate/created）。</li></ul><h4>15. Vue 生命周期的作用</h4><p>生命周期钩子允许开发者在组件不同阶段插入自定义逻辑，核心作用：</p><ol><li>​<strong>初始化逻辑</strong>​：created 中请求数据、初始化变量；</li><li>​<strong>DOM 操作</strong>​：mounted 中操作 DOM（如初始化第三方插件）；</li><li>​<strong>数据更新处理</strong>​：updated 中处理 DOM 更新后的逻辑；</li><li>​<strong>资源清理</strong>​：beforeDestroy 中清除定时器、取消事件监听，避免内存泄漏；</li><li>​<strong>性能优化</strong>​：按需执行逻辑，避免无效代码（如仅在挂载后请求数据）。</li></ol><h4>16. DOM 渲染在哪个生命周期阶段内完成</h4><ul><li>核心结论：<strong>mounted 阶段</strong>完成 DOM 渲染；</li><li><p>细节：</p><ul><li>beforeMount：模板已编译，但未挂载到 DOM（&amp;dollar;el 为虚拟 DOM）；</li><li>mounted：真实 DOM 挂载完成，&amp;dollar;el 指向真实 DOM 节点，可安全操作 DOM；</li><li>若组件包含子组件，mounted 仅表示当前组件 DOM 挂载完成，子组件可能仍在挂载中。</li></ul></li></ul><h4>17. Vue 路由的实现</h4><p>Vue Router 的核心实现依赖​<strong>前端路由原理</strong>​，分两步：</p><ol><li><p>​<strong>路由匹配</strong>​：</p><ol><li>定义路由规则（routes 数组），每个规则包含 path、component 等；</li><li>Vue Router 监听 URL 变化，匹配对应路由规则；</li></ol></li><li><p>​<strong>视图渲染</strong>​：</p><ol><li>通过 <code>&lt;router-view&gt;</code> 组件作为路由出口，匹配到的组件渲染到该位置；</li><li>底层依赖 Vue 的组件系统，通过动态组件切换实现视图更新；</li></ol></li></ol><ul><li>补充：路由模式（hash/history）决定 URL 的表现形式，底层分别基于 hashchange 事件和 History API。</li></ul><h4>18. 简述 Vue 路由模式 hash 和 history</h4><table><thead><tr><th>维度</th><th>hash 模式（默认）</th><th>history 模式</th></tr></thead><tbody><tr><td>URL 表现</td><td>带#（如 <a href="https://link.segmentfault.com/?enc=r8a5yCwecMsdUbkO%2FzY3Aw%3D%3D.WYI5QhkGMWgBMaubelBKEqvIZxWNZhezbDstpVHrtmw%3D" rel="nofollow" target="_blank">http://xxx/#/home</a>）</td><td>无#（如 <a href="https://link.segmentfault.com/?enc=7yiSjxLa2IEmjjMcSB7gqg%3D%3D.O1%2BrtyfMKPz3PompIG4ybQ%3D%3D" rel="nofollow" target="_blank">http://xxx/home</a>）</td></tr><tr><td>底层原理</td><td>基于 hashchange 事件，#后的内容不会发送到服务器</td><td>基于 HTML5 History API（pushState/replaceState）</td></tr><tr><td>服务器配置</td><td>无需配置，刷新页面不会 404</td><td>需配置后端，刷新页面需重定向到 index.html（否则 404）</td></tr><tr><td>兼容性</td><td>兼容所有浏览器（包括 IE）</td><td>仅支持 HTML5 浏览器</td></tr><tr><td>SEO</td><td>部分搜索引擎不识别#后内容</td><td>更友好，SEO 效果更好</td></tr></tbody></table><h4>19. Vue 路由传参方式，params 与 query 方式和区别</h4><ul><li>​<strong>传参方式</strong>​：分 query 和 params 两种核心方式，均支持声明式/编程式；</li><li>​<strong>核心区别</strong>​：</li></ul><table><thead><tr><th>维度</th><th>query 参数</th><th>params 参数</th></tr></thead><tbody><tr><td>URL 表现</td><td>拼接在路径后（?key=value）</td><td>嵌入路径中（/user/1）</td></tr><tr><td>路由配置</td><td>无需特殊配置</td><td>需在路由 path 中定义（如/user/:id）</td></tr><tr><td>刷新页面</td><td>参数不会丢失</td><td>若路由未定义参数，刷新后丢失</td></tr><tr><td>取值方式</td><td>this.&amp;dollar;route.query.key</td><td>this.&amp;dollar;route.params.key</td></tr><tr><td>可选性</td><td>可传可不传</td><td>路由定义的参数必须传（否则跳转失败）</td></tr></tbody></table><ul><li><p>示例：</p><pre><code class="JavaScript">// query传参
this.$router.push({ path: '/user', query: { id: 1 } }); // URL: /user?id=1
// params传参（需路由name）
this.$router.push({ name: 'User', params: { id: 1 } }); // URL: /user/1</code></pre></li></ul><h4>20. Vue 数据绑定的几种方式</h4><p>Vue 数据绑定分三类，核心是响应式绑定：</p><ol><li><p>​<strong>单向绑定</strong>​：</p><ol><li>插值表达式 <code>{{}}</code>：渲染文本；</li><li><code>v-bind</code>（简写 <code>:</code>）：绑定属性（如 <code>:src="imgUrl"</code>、<code>:class="className"</code>）；</li></ol></li><li><p>​<strong>双向绑定</strong>​：</p><ol><li><code>v-model</code>：主要用于表单元素（如 <code>&lt;input v-model="value"&gt;</code>），本质是 <code>v-bind</code>+<code>v-on</code> 的语法糖；</li></ol></li><li><p>​<strong>一次性绑定</strong>​：</p><ol><li><code>v-once</code>：绑定后数据变化不再更新视图（如 <code>&lt;div v-once&gt;{{name}}&lt;/div&gt;</code>）。</li></ol></li></ol><h4>21. Vue 注册一个全局组件</h4><p>全局组件注册后，所有 Vue 实例/组件均可直接使用，步骤：</p><pre><code class="JavaScript">// 1. 定义组件
const MyComponent = {
  template: `&lt;div&gt;{{msg}}&lt;/div&gt;`,
  data() {
    return { msg: '全局组件' };
  }
};
// 2. 注册全局组件（Vue 2）
import Vue from 'vue';
Vue.component('MyComponent', MyComponent);

// Vue 3（createApp方式）
import { createApp } from 'vue';
const app = createApp({});
app.component('MyComponent', MyComponent);
app.mount('#app');</code></pre><ul><li>注意：全局组件需在 Vue 实例创建前注册，否则无法使用。</li></ul><h4>22. Vue 的路由钩子/路由守卫有哪些</h4><p>Vue Router 的路由守卫分三类，用于控制路由跳转权限：</p><ol><li><p>​<strong>全局守卫</strong>​（所有路由生效）：</p><ol><li><code>router.beforeEach</code>：路由跳转前触发（常用作登录验证）；</li><li><code>router.afterEach</code>：路由跳转后触发（无权限控制）；</li><li><code>router.beforeResolve</code>：所有组件内守卫和异步路由解析完成后触发；</li></ol></li><li><p>​<strong>路由独享守卫</strong>​（单个路由生效）：</p><pre><code class="JavaScript">const routes = [
  {
    path: '/user',
    component: User,
    beforeEnter: (to, from, next) =&gt; { // 仅/user路由生效
      if (/* 验证 */) next();
      else next('/login');
    }
  }
];</code></pre></li><li><p>​<strong>组件内守卫</strong>​（组件内生效）：</p><ol><li><code>beforeRouteEnter</code>：进入组件前触发（无法访问 this，需通过 next 回调）；</li><li><code>beforeRouteUpdate</code>：组件复用（如动态路由）时触发；</li><li><code>beforeRouteLeave</code>：离开组件前触发（如提示未保存）。</li></ol></li></ol><h4>23. Vue 中如何进行动态路由设置？有哪些方式？怎么获取传递过来的参数？</h4><ul><li>​<strong>动态路由定义</strong>​：在路由 path 中用 <code>:参数名</code> 定义，匹配任意值；</li><li><p>​<strong>定义方式</strong>​：</p><pre><code class="JavaScript">// 1. 基础动态路由
const routes = [
  { path: '/user/:id', name: 'User', component: User }
];
// 2. 可选参数（加?）
{ path: '/user/:id?', component: User }
// 3. 通配符（匹配所有）
{ path: '*', component: NotFound }</code></pre></li><li><p>​<strong>获取参数</strong>​：</p><ul><li>组件内通过 <code>this.$route.params</code> 获取（如 <code>this.$route.params.id</code>）；</li><li><p>组合式 API 中用 <code>useRoute</code>：</p><pre><code class="JavaScript">import { useRoute } from 'vue-router';
const route = useRoute();
console.log(route.params.id);</code></pre></li></ul></li></ul><h4>24. Element UI 中常用组件有哪些？请简述并说下他们的属性有哪些</h4><p>Element UI 是 Vue 2 主流 UI 库，核心常用组件及属性：</p><table><thead><tr><th>组件</th><th>用途</th><th>核心属性</th></tr></thead><tbody><tr><td>Button</td><td>按钮</td><td>type（primary/success）、size（small/medium）、disabled、icon</td></tr><tr><td>Input</td><td>输入框</td><td>v-model、placeholder、disabled、clearable、type（text/password）</td></tr><tr><td>Table</td><td>表格</td><td>data（数据源）、columns（列配置）、pagination（分页）、border、height</td></tr><tr><td>Form</td><td>表单</td><td>model（表单数据）、rules（校验规则）、label-width、inline</td></tr><tr><td>Dialog</td><td>弹窗</td><td>visible（显示/隐藏）、title、width、modal（遮罩）、close-on-click-modal</td></tr><tr><td>Select</td><td>下拉选择</td><td>v-model、options（选项）、multiple（多选）、disabled</td></tr><tr><td>Pagination</td><td>分页</td><td>total（总条数）、page-size（每页条数）、current-page（当前页）、layout（布局）</td></tr></tbody></table><h4>25. Vue CLI 中如何自定义指令</h4><p>自定义指令用于扩展 DOM 操作，分全局/局部指令：</p><ul><li><p>​<strong>全局自定义指令</strong>​（main.js）：</p><pre><code class="JavaScript">// Vue 2
import Vue from 'vue';
// 注册v-focus指令（自动聚焦输入框）
Vue.directive('focus', {
  inserted(el) { // 指令绑定到元素并插入DOM时触发
    el.focus();
  }
});

// Vue 3
import { createApp } from 'vue';
const app = createApp({});
app.directive('focus', {
  mounted(el) { el.focus(); }
});</code></pre></li><li><p>​<strong>局部自定义指令</strong>​（组件内）：</p><pre><code class="Plain">&lt;script&gt;
export default {
  directives: {
    focus: {
      inserted(el) { el.focus(); }
    }
  }
}
&lt;/script&gt;</code></pre></li><li>指令钩子：bind（绑定）、inserted（插入 DOM）、update（更新）等（Vue 3 调整为 created/mounted/updated）。</li></ul><h4>26. Vue 中指令有哪些</h4><p>Vue 指令分<strong>内置指令</strong>和​<strong>自定义指令</strong>​，核心内置指令：</p><table><thead><tr><th>类别</th><th>指令</th><th>用途</th></tr></thead><tbody><tr><td>数据绑定</td><td>v-text、v-html、v-bind</td><td>渲染文本/HTML、绑定属性</td></tr><tr><td>事件绑定</td><td>v-on</td><td>绑定事件（简写 @）</td></tr><tr><td>双向绑定</td><td>v-model</td><td>表单数据双向绑定</td></tr><tr><td>条件渲染</td><td>v-if、v-else、v-show</td><td>条件显示/隐藏 DOM</td></tr><tr><td>列表渲染</td><td>v-for</td><td>循环渲染列表</td></tr><tr><td>其他</td><td>v-once、v-cloak、v-pre</td><td>一次性绑定、解决闪烁、跳过编译</td></tr></tbody></table><h4>27. Vue 如何定义一个过滤器</h4><p>过滤器用于格式化数据（Vue 3 已移除，推荐用计算属性/方法替代），Vue 2 定义方式：</p><ul><li><p>​<strong>全局过滤器</strong>​（main.js）：</p><pre><code class="JavaScript">import Vue from 'vue';
// 注册全局过滤器（格式化时间）
Vue.filter('formatTime', (value) =&gt; {
  return new Date(value).toLocaleString();
});</code></pre></li><li><p>​<strong>局部过滤器</strong>​（组件内）：</p><pre><code class="Plain">&lt;script&gt;
export default {
  filters: {
    formatTime(value) {
      return new Date(value).toLocaleString();
    }
  }
}
&lt;/script&gt;</code></pre></li><li>使用：<code>{{ time | formatTime }}</code> 或 <code>v-bind:title="time | formatTime"</code>。</li></ul><h4>28. 对 Vue 中 keep-alive 的理解</h4><p><code>keep-alive</code> 是 Vue 的内置组件，用于​<strong>缓存组件实例</strong>​，避免重复创建/销毁：</p><ul><li><p>核心特性：</p><ul><li>包裹动态组件时，缓存不活动的组件，而非销毁；</li><li>触发组件的 <code>activated</code>（激活）/<code>deactivated</code>（失活）钩子；</li></ul></li><li><p>常用属性：</p><ul><li><code>include</code>：仅缓存指定组件（如 <code>include="User,Home"</code>）；</li><li><code>exclude</code>：排除指定组件；</li><li><code>max</code>：最大缓存数量（超出则销毁最久未使用的组件）；</li></ul></li><li>应用场景：路由切换时缓存表单数据、列表滚动位置等（如 <code>&lt;keep-alive&gt;&lt;router-view&gt;&lt;/router-view&gt;&lt;/keep-alive&gt;</code>）。</li></ul><h4>29. 如何让组件中的 CSS 在当前组件生效</h4><p>通过<strong>样式隔离</strong>实现，核心方式：</p><ol><li><p>​<strong>scoped 属性</strong>​（推荐）：</p><pre><code class="Plain">&lt;style scoped&gt;
.box { color: red; } // 仅当前组件生效
&lt;/style&gt;</code></pre><ol><li>原理：Vue 为组件 DOM 添加唯一属性（如 <code>data-v-xxx</code>），CSS 自动添加属性选择器，实现隔离；</li></ol></li><li><p>​<strong>CSS Modules</strong>​：</p><pre><code class="Plain">&lt;style module&gt;
.box { color: red; }
&lt;/style&gt;
&lt;template&gt;
  &lt;div :class="$style.box"&gt;内容&lt;/div&gt;
&lt;/template&gt;</code></pre><ol><li>原理：类名被编译为唯一哈希值，避免冲突；</li></ol></li><li><p>​<strong>深度选择器</strong>​（如需修改子组件样式）：</p><pre><code class="Plain">&lt;style scoped&gt;
::v-deep .child-box { color: blue; } // Vue 2
:deep(.child-box) { color: blue; } // Vue 3
&lt;/style&gt;</code></pre></li></ol><h4>30. Vue 生命周期一共有几个阶段</h4><ul><li><p>​<strong>Vue 2</strong>​：分 4 个大阶段，8 个核心钩子：</p><ul><li>创建阶段（2 个）：beforeCreate、created；</li><li>挂载阶段（2 个）：beforeMount、mounted；</li><li>更新阶段（2 个）：beforeUpdate、updated；</li><li>销毁阶段（2 个）：beforeDestroy、destroyed；</li><li>补充：还有 activated/deactivated（keep-alive 组件）、errorCaptured（错误捕获）等钩子；</li></ul></li><li>​<strong>Vue 3</strong>​：组合式 API 中钩子更细分，核心阶段一致，钩子名调整为 onXxx（如 onMounted），新增 setup（替代 beforeCreate/created）。</li></ul><h4>31. MVVM 和 MVC 的区别</h4><p>两者均为软件架构模式，核心区别：</p><table><thead><tr><th>维度</th><th>MVC（Model-View-Controller）</th><th>MVVM（Model-View-ViewModel）</th></tr></thead><tbody><tr><td>核心角色</td><td>Model（数据）、View（视图）、Controller（控制器，连接 M/V）</td><td>Model（数据）、View（视图）、ViewModel（桥梁，双向绑定）</td></tr><tr><td>数据流向</td><td>单向（Model→Controller→View）</td><td>双向（View←→ViewModel←→Model）</td></tr><tr><td>耦合度</td><td>View 和 Model 需通过 Controller 通信，耦合较高</td><td>View 和 Model 完全解耦，由 ViewModel 中转</td></tr><tr><td>核心特性</td><td>手动更新视图（需 Controller 操作 DOM）</td><td>自动更新视图（数据驱动，ViewModel 实现响应式）</td></tr><tr><td>应用框架</td><td>jQuery、Backbone.js</td><td>Vue、React（类 MVVM）、Angular</td></tr></tbody></table><h4>32. Vue 组件中的 data 为什么是函数</h4><p>核心原因：​<strong>保证组件实例的独立性</strong>​，避免多个组件实例共享同一数据对象；</p><ul><li><p>原理：</p><ul><li>若 data 是对象，所有组件实例会引用同一个对象，修改一个实例的 data 会影响其他实例；</li><li>若 data 是函数，每次创建组件实例时，函数返回一个新的对象，各实例数据独立；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  // 正确：函数返回新对象
  data() {
    return { count: 0 };
  },
  // 错误：所有实例共享count
  // data: { count: 0 }
}
&lt;/script&gt;</code></pre></li><li>补充：根实例（new Vue({})）的 data 可以是对象（仅一个实例，无共享问题）。</li></ul><h4>33. Vue 双向绑定原理</h4><p>Vue 2 基于​<strong>Object.defineProperty</strong>​，Vue 3 基于​<strong>Proxy</strong>​，核心流程：</p><ul><li><p>​<strong>Vue 2</strong>​：</p><ul><li>数据劫持：通过 <code>Object.defineProperty</code> 监听 data 中所有属性的 get/set；</li><li>依赖收集：模板编译时，访问属性触发 get，收集依赖（Watcher）；</li><li>派发更新：修改属性触发 set，通知 Watcher 更新视图；</li><li>缺陷：无法监听数组下标/长度变化、对象新增属性；</li></ul></li><li><p>​<strong>Vue 3</strong>​：</p><ul><li>数据代理：通过 <code>Proxy</code> 代理整个 data 对象，支持监听数组/对象所有变化；</li><li>依赖收集/派发更新逻辑与 Vue 2 类似，但效率更高；</li></ul></li><li>核心公式：<code>MVVM = 数据劫持 + 发布-订阅模式</code>。</li></ul><h4>34. Vue 组件中的传值方式</h4><p>组件传值分 7 种核心场景，覆盖父子/兄弟/跨级：</p><table><thead><tr><th>场景</th><th>传值方式</th><th>示例</th></tr></thead><tbody><tr><td>父 → 子</td><td>props</td><td>子组件定义 props，父组件 <code>:prop="value"</code></td></tr><tr><td>子 → 父</td><td>自定义事件（&amp;dollar;emit）</td><td>子组件 <code>this.$emit('event', data)</code>，父组件 <code>@event="handle"</code></td></tr><tr><td>兄弟组件</td><td>事件总线/Vuex/Pinia</td><td>事件总线：<code>Vue.prototype.$bus = new Vue()</code>，<code>$bus.$emit/$on</code></td></tr><tr><td>跨级组件</td><td>provide/inject</td><td>父组件 provide 提供数据，子组件 inject 注入</td></tr><tr><td>任意组件</td><td>Vuex/Pinia</td><td>全局状态管理，直接读取/修改共享数据</td></tr><tr><td>路由传参</td><td>query/params</td><td>跳转路由时携带参数</td></tr><tr><td>本地存储</td><td>localStorage/sessionStorage</td><td>持久化传值（非响应式）</td></tr></tbody></table><h4>35. Bootstrap 的原理</h4><p>Bootstrap 是前端 UI 框架，核心原理：</p><ol><li>​<strong>栅格系统</strong>​：基于 Flex/Grid 布局，将页面分为 12 列，通过 <code>col-xs-*</code>/<code>col-md-*</code> 等类实现响应式布局；</li><li>​<strong>响应式设计</strong>​：通过媒体查询（@media）适配不同屏幕尺寸（移动端/平板/PC）；</li><li>​<strong>预定义样式</strong>​：提供按钮、表单、导航等组件的 CSS 样式，直接复用；</li><li>​<strong>jQuery 插件</strong>​：内置轮播、弹窗、下拉菜单等交互插件（Bootstrap 5 移除 jQuery，改用原生 JS）；</li><li>​<strong>变量/混合器</strong>​（Sass 版本）：支持自定义主题，通过变量覆盖默认样式。</li></ol><h4>36. Vue 兄弟组件传值</h4><p>兄弟组件无直接传值通道，常用 3 种方式：</p><ol><li><p>​<strong>事件总线（Vue 2）</strong>​：</p><pre><code class="JavaScript">// 1. 全局注册总线
import Vue from 'vue';
Vue.prototype.$bus = new Vue();
// 2. 组件A发送事件
this.$bus.$emit('sendData', data);
// 3. 组件B接收事件（mounted中）
this.$bus.$on('sendData', (data) =&gt; { /* 处理数据 */ });
// 4. 销毁时解绑（避免内存泄漏）
beforeDestroy() {
  this.$bus.$off('sendData');
}</code></pre></li><li>​<strong>Vuex/Pinia</strong>​：将共享数据存入全局状态，兄弟组件直接读取/修改；</li><li>​<strong>父组件中转</strong>​：组件 A→ 父组件（&amp;dollar;emit）→ 组件 B（props）。</li></ol><h4>37. 如果一个组件需要在多个项目中使用怎么办</h4><p>核心方案：​<strong>组件封装并发布为 npm 包</strong>​，步骤：</p><ol><li><p>​<strong>组件封装</strong>​：</p><ol><li>抽离组件的通用逻辑，参数通过 props 暴露，事件通过&amp;dollar;emit 触发；</li><li>避免硬编码，支持自定义样式/配置；</li></ol></li><li><p>​<strong>打包发布</strong>​：</p><ol><li>用 Vue CLI/lib 模式打包：<code>vue-cli-service build --target lib --name my-component src/index.js</code>；</li><li>配置 package.json（main 指向打包后的文件，指定版本、依赖等）；</li><li>发布到 npm（<code>npm publish</code>）；</li></ol></li><li><p>​<strong>其他方案</strong>​：</p><ol><li>搭建私有 npm 仓库（如 Verdaccio），存放内部组件；</li><li>通过 Git submodule 引入组件源码（适合频繁修改的场景）；</li><li>使用 Monorepo 管理多项目共享组件（如 pnpm workspace）。</li></ol></li></ol><h4>38. 简述槽口（Slot）</h4><p>Slot（插槽）是 Vue 组件的内容分发机制，允许父组件向子组件插入自定义内容：</p><ul><li><p>​<strong>核心类型</strong>​：</p><ul><li>​<strong>默认插槽</strong>​：子组件 <code>&lt;slot&gt;&lt;/slot&gt;</code>，父组件直接写内容；</li><li>​<strong>具名插槽</strong>​：子组件 <code>&lt;slot name="header"&gt;&lt;/slot&gt;</code>，父组件 <code>&lt;template v-slot:header&gt;内容&lt;/template&gt;</code>（简写 <code>#header</code>）；</li><li><p>​<strong>作用域插槽</strong>​：子组件向父组件传递数据，父组件自定义渲染逻辑：</p><pre><code class="Plain">&lt;!-- 子组件 --&gt;
&lt;slot :user="user"&gt;&lt;/slot&gt;
&lt;!-- 父组件 --&gt;
&lt;template v-slot:default="slotProps"&gt;
  {{ slotProps.user.name }}
&lt;/template&gt;</code></pre></li></ul></li><li>作用：提升组件灵活性，实现组件内容的自定义渲染。</li></ul><h4>39. 简述 watch</h4><p>watch 是 Vue 的​<strong>侦听器</strong>​，用于监听数据变化并执行自定义逻辑：</p><ul><li><p>核心特性：</p><ul><li>监听单个/多个数据（如 data、props、计算属性）；</li><li>支持深度监听（deep: true）、立即执行（immediate: true）；</li><li>可监听对象属性（如 <code>'user.name'</code>）；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  data() {
    return { user: { name: '张三' }, count: 0 };
  },
  watch: {
    // 监听基本类型
    count(newVal, oldVal) {
      console.log('count变化：', newVal, oldVal);
    },
    // 监听对象（深度监听）
    user: {
      handler(newVal) {
        console.log('user变化：', newVal);
      },
      deep: true,
      immediate: true // 初始化时执行一次
    },
    // 监听对象单个属性
    'user.name'(newVal) {
      console.log('姓名变化：', newVal);
    }
  }
}
&lt;/script&gt;</code></pre></li></ul><h4>40. 简述 Vant UI</h4><p>Vant UI 是有赞开源的​<strong>移动端 Vue UI 组件库</strong>​，核心特点：</p><ol><li>​<strong>适配场景</strong>​：专注移动端（H5/小程序），适配各种屏幕尺寸；</li><li>​<strong>版本支持</strong>​：Vant 2 支持 Vue 2，Vant 3/4 支持 Vue 3；</li><li>​<strong>核心组件</strong>​：Button、Cell、List、PullRefresh、Swipe、Dialog、Toast 等；</li><li><p>​<strong>特性</strong>​：</p><ol><li>轻量：按需引入，减少打包体积；</li><li>易用：API 简洁，文档完善；</li><li>兼容：支持小程序（微信/支付宝）、H5、App（通过 uni-app）；</li></ol></li><li><p>​<strong>使用方式</strong>​：</p><pre><code class="Bash">npm i vant
# 按需引入（需配置babel-plugin-import）
import { Button } from 'vant';
Vue.use(Button);</code></pre></li></ol><h4>41. 计算属性与 watch 的区别</h4><table><thead><tr><th>维度</th><th>计算属性（computed）</th><th>侦听器（watch）</th></tr></thead><tbody><tr><td>核心用途</td><td>派生数据（如 a+b）</td><td>监听数据变化执行副作用（如请求、修改 DOM）</td></tr><tr><td>缓存性</td><td>有缓存，依赖不变则不重新计算</td><td>无缓存，数据变化即触发</td></tr><tr><td>返回值</td><td>必须有返回值</td><td>无需返回值</td></tr><tr><td>语法</td><td>声明式（类似变量）</td><td>命令式（函数）</td></tr><tr><td>适用场景</td><td>简单的同步数据计算</td><td>异步操作、复杂的逻辑处理</td></tr><tr><td>深度监听</td><td>自动深度监听对象属性</td><td>需手动设置 deep: true</td></tr></tbody></table><ul><li><p>示例对比：</p><pre><code class="JavaScript">// 计算属性：适合简单计算
computed: { fullName() { return this.first + ' ' + this.last; } }
// watch：适合异步逻辑
watch: { firstName(newVal) { this.$axios.get('/api', { params: { name: newVal } }); } }</code></pre></li></ul><h4>42. MVVM 框架是什么？它和其他框架的区别是什么？哪些场景适合？</h4><ul><li>​<strong>MVVM 定义</strong>​：MVVM（Model-View-ViewModel）是前端架构模式，核心是 ViewModel 作为 View 和 Model 的桥梁，实现数据与视图的双向绑定；</li><li><p>​<strong>与其他框架的区别</strong>​：</p><ul><li>与 jQuery（无架构）：MVVM 数据驱动，无需手动操作 DOM；jQuery 需手动选择 DOM、修改内容；</li><li>与 React（类 MVVM）：React 核心是单向数据流（State→View），需手动 setState 更新；Vue（MVVM）原生支持双向绑定；</li><li>与 Angular（全量 MVVM）：Vue 更轻量、易用，Angular 功能全但学习成本高；</li></ul></li><li><p>​<strong>适用场景</strong>​：</p><ul><li>中大型单页应用（SPA）：数据交互频繁，需高效管理状态；</li><li>表单类应用：双向绑定简化表单处理；</li><li>移动端/H5 应用：轻量、高性能，适配移动端；</li><li>不适用场景：简单静态页面（如纯展示页），用 jQuery/原生 JS 更高效。</li></ul></li></ul><h4>43. Vue 首屏加载慢的原因，怎么解决的，白屏时间怎么检测，怎么解决白屏问题</h4><h5>（1）首屏加载慢的原因</h5><ol><li>打包体积大：未按需引入组件/库、未压缩代码、包含无用依赖；</li><li>网络问题：请求资源过大、网络延迟高；</li><li>渲染阻塞：JS 执行时间长，阻塞 DOM 渲染；</li><li>服务器响应慢：接口请求耗时久。</li></ol><h5>（2）解决方法</h5><ol><li><p>​<strong>优化打包体积</strong>​：</p><ol><li>按需引入（如 Element UI/Vant）；</li><li>路由懒加载：<code>const Home = () =&gt; import('./Home.vue')</code>；</li><li>压缩代码（Vue CLI 默认开启）、移除 console；</li><li>CDN 引入第三方库（如 Vue、Vue Router），减少打包体积；</li></ol></li><li><p>​<strong>网络优化</strong>​：</p><ol><li>开启 Gzip 压缩（Nginx 配置）；</li><li>使用 HTTP/2、静态资源 CDN；</li><li>预加载/预取（<code>&lt;link rel="preload"&gt;</code>）；</li></ol></li><li><p>​<strong>渲染优化</strong>​：</p><ol><li>首屏骨架屏（Skeleton）；</li><li>异步组件、懒加载图片；</li><li>服务端渲染（SSR）/静态站点生成（SSG）。</li></ol></li></ol><h5>（3）白屏时间检测</h5><ol><li><p>​<strong>浏览器 Performance 面板</strong>​：</p><ol><li>记录首屏时间（First Contentful Paint, FCP）、最大内容绘制（LCP）；</li><li>查看 JS 执行、资源加载耗时；</li></ol></li><li><p>​<strong>代码埋点</strong>​：</p><pre><code class="JavaScript">// 监听DOM加载完成
document.addEventListener('DOMContentLoaded', () =&gt; {
  console.log('DOM加载完成时间：', Date.now() - performance.timing.navigationStart);
});
// 监听首屏绘制
new PerformanceObserver((entryList) =&gt; {
  const entry = entryList.getEntries()[0];
  console.log('首屏时间：', entry.startTime);
}).observe({ type: 'paint', buffered: true });</code></pre></li></ol><h5>（4）解决白屏问题</h5><ol><li>骨架屏：首屏加载时显示占位骨架，替代空白；</li><li>预加载关键资源：优先加载首屏所需 CSS/JS；</li><li>服务端渲染（SSR）：服务端生成首屏 HTML，直接返回；</li><li>减小首屏 JS 体积：路由懒加载、按需引入，只加载首屏必要代码。</li></ol><h4>44. Vue 双向数据绑定中，怎么实现一侧数据改变之后通知另一侧</h4><p>核心是​<strong>发布-订阅模式</strong>​，分两步：</p><ol><li><p>​<strong>数据劫持/代理</strong>​：</p><ol><li>Vue 2 用 <code>Object.defineProperty</code> 监听数据的 setter，Vue 3 用 <code>Proxy</code> 监听对象变化；</li></ol></li><li><p>​<strong>依赖收集与派发更新</strong>​：</p><ol><li>当视图渲染访问数据时（getter），收集依赖（Watcher，关联视图）；</li><li>当数据修改时（setter），触发派发更新，通知所有相关 Watcher 执行更新逻辑，重新渲染视图；</li><li>反向（视图 → 数据）：<code>v-model</code> 监听输入事件（input/change），修改对应数据，完成双向绑定。</li></ol></li></ol><h4>45. Vuex 流程</h4><p>Vuex 的核心数据流向是​<strong>单向循环</strong>​：</p><ol><li>组件通过 <code>dispatch</code> 触发​<strong>Action</strong>​（可执行异步操作）；</li><li>Action 通过 <code>commit</code> 提交​<strong>Mutation</strong>​；</li><li>Mutation 修改​<strong>State</strong>​（唯一能修改 State 的方式）；</li><li>State 变化触发​<strong>Getter</strong>​（可选，派生数据）；</li><li>组件监听 State/Getter 变化，更新视图；</li></ol><ul><li>简化流程：<code>组件 → Action → Mutation → State → 组件</code>（同步操作可直接 <code>commit</code> Mutation）。</li></ul><h4>46. Vuex 怎么请求异步数据</h4><p>Vuex 中异步数据请求需在 <strong>Action</strong> 中执行，步骤：</p><pre><code class="JavaScript">// 1. 定义Action
const store = new Vuex.Store({
  state: { userList: [] },
  mutations: {
    SET_USER_LIST(state, data) {
      state.userList = data;
    }
  },
  actions: {
    // 异步请求数据
    async fetchUserList({ commit }) {
      try {
        const res = await this.$axios.get('/api/user/list');
        commit('SET_USER_LIST', res.data); // 提交Mutation修改State
      } catch (err) {
        console.error('请求失败：', err);
      }
    }
  }
});
// 2. 组件中触发Action
this.$store.dispatch('fetchUserList');</code></pre><ul><li>注意：Mutation 只能执行同步操作，异步操作必须放在 Action 中。</li></ul><h4>47. Vuex 中 Action 如何提交给 Mutation</h4><p>Action 通过 <code>commit</code> 方法提交 Mutation，有两种方式：</p><ol><li><p>​<strong>解构 context 对象</strong>​（推荐）：</p><pre><code class="JavaScript">actions: {
  increment({ commit }) { // 解构commit
    commit('INCREMENT'); // 提交Mutation
  }
}</code></pre></li><li><p>​<strong>完整 context 对象</strong>​：</p><pre><code class="JavaScript">actions: {
  increment(context) {
    context.commit('INCREMENT'); // context包含commit/dispatch/state等
  }
}</code></pre></li></ol><ul><li>带参数提交：<code>commit('INCREMENT', payload)</code>（payload 为任意类型数据）。</li></ul><h4>48. route 与 router 的区别</h4><table><thead><tr><th>维度</th><th>&amp;dollar;route</th><th>&amp;dollar;router</th></tr></thead><tbody><tr><td>核心含义</td><td>当前路由信息对象</td><td>路由实例（导航控制器）</td></tr><tr><td>包含内容</td><td>path、params、query、name 等</td><td>push、replace、go 等导航方法</td></tr><tr><td>用途</td><td>读取当前路由参数/信息</td><td>触发路由跳转</td></tr><tr><td>示例</td><td><code>this.$route.params.id</code></td><td><code>this.$router.push('/home')</code></td></tr></tbody></table><h4>49. Vuex 有哪几种状态和属性</h4><p>Vuex 的核心属性（5 个）：</p><ol><li>​<strong>state</strong>​：存储全局状态（唯一数据源）；</li><li>​<strong>mutations</strong>​：同步修改 state 的方法（唯一入口）；</li><li>​<strong>actions</strong>​：异步操作，提交 mutation 修改 state；</li><li>​<strong>getters</strong>​：派生状态（类似计算属性，基于 state 计算）；</li><li>​<strong>modules</strong>​：模块化拆分 state，解决单一状态树体积过大问题。</li></ol><h4>50. Vuex 的 state 特性是？</h4><ol><li>​<strong>唯一性</strong>​：整个应用只有一个 state（单一状态树）；</li><li>​<strong>响应式</strong>​：state 中的数据是响应式的，修改后视图自动更新；</li><li>​<strong>只读性</strong>​：不能直接修改 state，必须通过 mutation；</li><li>​<strong>可模块化</strong>​：通过 modules 拆分 state，每个 module 有独立的 state/mutations 等；</li><li>​<strong>组件访问</strong>​：通过 <code>this.$store.state</code> 或 <code>mapState</code> 辅助函数访问。</li></ol><h4>51. Vuex 的 getter 特性是？</h4><ol><li>​<strong>缓存性</strong>​：依赖的 state 不变时，多次访问 getter 不会重新计算；</li><li>​<strong>派生状态</strong>​：基于 state 计算新值（如过滤列表、计算总数）；</li><li>​<strong>只读性</strong>​：不能直接修改 getter，需修改依赖的 state；</li><li>​<strong>可传参</strong>​：通过返回函数实现传参（如 <code>getters.getUserById(state) =&gt; (id) =&gt; state.users.find(u =&gt; u.id === id)</code>）；</li><li>​<strong>组件访问</strong>​：通过 <code>this.$store.getters</code> 或 <code>mapGetters</code> 辅助函数访问。</li></ol><h4>52. Vuex 的 mutation 特性是？</h4><ol><li>​<strong>同步性</strong>​：必须是同步函数（异步操作会导致状态变更无法追踪）；</li><li>​<strong>唯一修改入口</strong>​：只能通过 mutation 修改 state；</li><li>​<strong>参数</strong>​：第一个参数是 state，第二个是 payload（可选，传递数据）；</li><li>​<strong>可追踪</strong>​：Vue Devtools 可记录 mutation 的调用记录，便于调试；</li><li>​<strong>调用方式</strong>​：通过 <code>store.commit('mutationName', payload)</code>，不能直接调用。</li></ol><h4>53. Vuex 的 action 特性是？</h4><ol><li>​<strong>异步性</strong>​：支持异步操作（如请求数据、定时器）；</li><li>​<strong>不直接修改 state</strong>​：需提交 mutation 修改 state；</li><li>​<strong>参数</strong>​：第一个参数是 context 对象（包含 commit/dispatch/state/getters）；</li><li>​<strong>支持 Promise</strong>​：action 可返回 Promise，便于链式调用；</li><li>​<strong>调用方式</strong>​：通过 <code>store.dispatch('actionName', payload)</code>，组件中可通过 <code>async/await</code> 等待执行完成。</li></ol><h4>54. Vuex 的优势</h4><ol><li>​<strong>集中式管理</strong>​：多组件共享状态统一存储，避免状态分散；</li><li>​<strong>可追踪性</strong>​：所有状态修改通过 mutation，便于调试和日志记录；</li><li>​<strong>单向数据流</strong>​：状态变更流程清晰，降低维护成本；</li><li>​<strong>模块化</strong>​：支持 modules 拆分状态，适配大型应用；</li><li>​<strong>生态集成</strong>​：与 Vue Devtools 深度集成，可视化调试；</li><li>​<strong>复用性</strong>​：公共逻辑（如数据请求）可封装在 action 中，多组件复用。</li></ol><h4>55. 简述 Vue 路由懒加载</h4><p>路由懒加载（按需加载）是<strong>代码分割</strong>的一种方式，核心是将路由组件拆分为独立的 JS 包，只有访问该路由时才加载对应的包：</p><ul><li><p>​<strong>实现方式</strong>​：</p><pre><code class="JavaScript">// 基础懒加载
const Home = () =&gt; import('./views/Home.vue');
// 带分包命名（webpackChunkName），便于打包后识别
const User = () =&gt; import(/* webpackChunkName: "user" */ './views/User.vue');
const routes = [
  { path: '/home', component: Home },
  { path: '/user', component: User }
];</code></pre></li><li><p>​<strong>优势</strong>​：</p><ul><li>减小首屏 JS 包体积，提升首屏加载速度；</li><li>按需加载，节省带宽和资源；</li></ul></li><li>​<strong>原理</strong>​：基于 ES6 的动态 import 语法，webpack 打包时自动拆分代码块。</li></ul><h4>56. v-for 和 v-if 的区别</h4><table><thead><tr><th>维度</th><th>v-for</th><th>v-if</th></tr></thead><tbody><tr><td>核心用途</td><td>循环渲染列表</td><td>条件渲染 DOM</td></tr><tr><td>优先级</td><td>更高（Vue 2）</td><td>更低（Vue 2）</td></tr><tr><td>执行时机</td><td>每次渲染都循环所有数据</td><td>条件为 true 时才渲染 DOM</td></tr><tr><td>性能</td><td>循环所有数据，性能开销大</td><td>仅渲染满足条件的 DOM</td></tr><tr><td>结合使用</td><td>不推荐直接结合（Vue 2 中 v-for 优先级高，会先循环再判断，性能差）</td><td>推荐用 computed 过滤数据后再循环</td></tr></tbody></table><ul><li><p>优化建议：</p><pre><code class="JavaScript">// 错误：v-for和v-if同节点
&lt;div v-for="item in list" v-if="item.visible"&gt;{{item.name}}&lt;/div&gt;
// 正确：先过滤数据
computed: {
  filteredList() {
    return this.list.filter(item =&gt; item.visible);
  }
}
&lt;div v-for="item in filteredList" :key="item.id"&gt;{{item.name}}&lt;/div&gt;</code></pre></li><li>Vue 3 调整：v-if 优先级高于 v-for，同节点使用会报错，强制开发者先过滤数据。</li></ul>]]></description></item><item>    <title><![CDATA[大模型常见量化方法简介 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047525433</link>    <guid>https://segmentfault.com/a/1190000047525433</guid>    <pubDate>2026-01-06 23:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、引言</h2><p>随着大型语言模型（LLM）在具身智能等领域的广泛应用，接下来就该思考如何在有限硬件资源下部署这些模型，量化是其中必不可少的步骤。</p><p>模型量化（Model Quantization）作为一种有效的模型压缩技术，通过将模型中的浮点数参数转换为低比特宽度的整数表示，显著减少了模型的存储和计算需求，同时尽量保持模型的性能。量化的基础知识相信大家都不会陌生，例如必然要介绍两种量化方式：PTQ/QAT。</p><p>QAT 是一种深度融合量化需求与模型训练流程的技术，核心是在模型训练阶段主动嵌入 “伪量化算子”—— 不实际将参数转换为低比特，而是模拟量化过程中的数值截断误差与舍入误差，让模型在学习任务知识的同时，同步适应量化带来的精度损耗。训练中，伪量化算子会实时统计各层输入输出的数据分布（如激活值的极值、权重的方差），动态优化量化参数（如缩放因子、零点），确保量化逻辑与模型参数更新形成配合。这种 “边训练边适配量化” 的特性，能让大语言模型（LLM）在低精度表示（如 4bit、2bit）下，依然保留接近原始浮点模型的性能。</p><p>PTQ 是在 LLM 完全训练完成后执行的量化方案，无需修改模型训练流程，仅需使用少量校准数据（通常 100-1000 条代表性样本）统计模型权重与激活值的分布特征，即可确定量化参数并完成低比特转换。其核心优势在于 “轻量高效”：无需重新训练模型，量化过程仅需数分钟至数小时，无需大规模计算资源；同时无需改动 LLM 架构，可直接适配各类推理框架，兼顾易用性与部署效率。不过，PTQ 的精度天花板相对较低，目前在 4bit 及以下量化时易出现明显精度损失，更适合对精度要求不高、追求快速部署的场景。</p><p>本文重点不是上述两种量化方式，而是将重点介绍四种主流的大模型量化方法：GPTQ、SmoothQuant、AWQ 和旋转量化，下面来分别看一下。</p><h2>二、GPTQ</h2><p>GPTQ（Gradient-based Post-training Quantization）是一种后训练量化方法，其核心思想是利用梯度信息来指导量化过程，从而最小化量化带来的性能损失。论文中仅进行权重量化，权重被量化为 int4 类型，激活值为 float16。</p><p>GPTQ（GPT Quantization）是一种基于最优量化误差最小化的单轮权重量化方法，其核心创新点在于：</p><ul><li>顺序压缩算法：按重要性对模型层进行排序，优先量化对输出影响较小的层</li><li>误差补偿机制：通过梯度下降优化量化参数，减少精度损失</li><li>分组量化策略：将权重矩阵分为小组（Group Size）独立量化，平衡压缩率与精度</li></ul><p>工作流程图如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525435" alt="" title=""/></p><p>GPTQ 通过优化量化误差的目标函数，使得量化后的模型输出尽可能接近原始模型的输出。其优化目标可以表示为：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525436" alt="" title="" loading="lazy"/></p><p>其中，f（⋅） 为模型的前向传播函数，W 为原始浮点权重，Wq 为量化后的权重。</p><p>GPTQ 已被广泛应用于 HuggingFace 等平台，具有完善的工具链和生态支持。但 GPTQ 主要针对模型的权重进行量化，对激活值的处理相对有限，且在量化过程中需要计算梯度信息，会增加额外的计算开销。</p><h2>三、SmoothQuant</h2><p>SmoothQuant 是一种训练后量化方法，旨在解决激活值量化困难的问题。核心理念在于平衡激活值和权重的量化难度。在大模型量化中，激活值通常包含大量离群点，这些离群点会显著拉伸量化范围，增加量化误差。SmoothQuant 提出了一种基于平滑因子的逐通道缩放变换方法，对每个通道的激活值进行缩放以平滑其分布，同时对权重施加反向缩放，确保模型计算的等价性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525437" alt="" title="" loading="lazy"/></p><p>其中，s 为平滑因子，x 为激活值，W 为权重，x′ 和 W′ 分别为经过平滑处理后的激活值和权重。通过这种方式，SmoothQuant 将激活值的量化难度转移到权重上，从而实现更高效的量化。</p><p>SmoothQuant 可以同时对权重和激活值进行 8 位量化，减少模型的存储需求，通过将激活值的异常值减少，SmoothQuant 可以提高推理的效率，但平滑因子 s 的选择对量化效果有较大影响，且不同的模型可能需要不同的平滑因子，需要通过实验进行调优。</p><h2>四、AWQ</h2><p>AWQ（Activation-aware Weight Quantization）是一种自适应权重量化方法，旨在根据激活值的重要性来指导权重的量化过程。其核心思想是识别出对模型输出影响较大的激活值，并根据这些激活值的重要性来调整权重的量化精度。具体而言，AWQ 通过计算激活值的方差来评估其重要性，然后根据重要性为权重分配不同的量化精度：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525438" alt="" title="" loading="lazy"/></p><p>其中，</p><p>$$
\alpha_i
$$</p><p>为第 i 个权重的激活感知度，&amp;dollar;&amp;dollar;b\_i&amp;dollar;&amp;dollar; 为其对应的量化比特数。</p><p>AWQ 方法源于"权重对于 LLM 的性能并不同等重要"的观察，存在约（0.1%-1%）显著权重对大模型性能影响太大，通过跳过这 1% 的重要权重不进行量化，可以大大减少量化误差。根据激活值的重要性动态调整权重的量化精度，这需要计算激活值的方差，实现相对复杂。</p><h2>五、SpinQuant</h2><p>旋转量化（SpinQuant）是一种通过旋转矩阵变换数据空间来实现量化的方法。其核心思想是通过引入旋转矩阵 RRR 将数据映射到新的空间，然后在新的空间中进行量化，从而使得量化误差在数据空间中更加均匀地分布，减少量化误差对模型性能的影响。具体而言，旋转量化的过程可以表示为：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525439" alt="" title="" loading="lazy"/></p><p>其中，w 为原始权重，w′ 为经过旋转变换后的权重，&amp;dollar;&amp;dollar;w'\_q&amp;dollar;&amp;dollar; 为量化后的权重，&amp;dollar;&amp;dollar;w\_q&amp;dollar;&amp;dollar; 为最终的量化权重。</p><p>通过旋转变换数据空间，可以使得量化误差在数据空间中更加均匀地分布，适应不同的模型结构和数据分布，减少量化误差。这个过程需要计算旋转矩阵，并进行矩阵变换，会引入一些计算开销。</p>]]></description></item><item>    <title><![CDATA[linux 常见稳定性问题分析方法 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047525446</link>    <guid>https://segmentfault.com/a/1190000047525446</guid>    <pubDate>2026-01-06 23:03:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.概述</h2><p>稳定性对项目交付、用户体验有着非常重要的影响，一般定义的稳定性问题是遇到了系统异常重启或者系统卡死等，即无法按照预期为客户继续提供功能和服务。地平线 SoC 平台提供了多种调试手段，去分析系统遇到的稳定性问题。</p><p>首先我们需要了解征程系列的软硬件方案及异常 reset 路径，通过了解异常路径定位发生异常的节点和步骤，定位到问题方向。</p><p>其次，我们需要对发生问题节点提取的调试信息，包括抓取 log、抓取 ramdump 等，对于复杂问题，可能需要不断的迭代 patch 以获取更多调试信息，以缩小问题的范围。</p><p>对于复杂问题，可能需要使用特定的工具去分析问题，如 crash-utility，T32，ftrace 等。</p><p>所以我们将稳定性问题的概述指导分为下面几个章节进行介绍。</p><h2>2. 常见问题</h2><h4>2.1.kernel panic</h4><ul><li>kernel panic 是最常见的系统异常，在 reset\_reason.txt 中显示为 kpanic；</li><li>一般通过 pstore log 就能看到 panic 时的栈和寄存器信息，通过分析上下文配合符号表及 gdb 等工具经常能够直接定位问题；</li><li>对于复杂问题，需要开启 ramdump，抓取 dump 后进行分析。</li></ul><p>一个典型的 kpanic 如下：</p><pre><code class="markdown">&lt;4&gt;[86758.651597] NMI backtrace for cpu 2
&lt;4&gt;[86758.651598] CPU: 2 PID: 105 Comm: khungtaskd Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.651601] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.651602] Call trace:
&lt;4&gt;[86758.651817] NMI backtrace for cpu 4
&lt;4&gt;[86758.651821] CPU: 4 PID: 5110 Comm: glmark2-es2-drm Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.651825] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.651826] pstate: 20400009 (nzCv daif +PAN -UAO -TCO -DIT -SSBS BTYPE=--)
&lt;4&gt;[86758.651829] pc : mas_empty_area_rev+0x2f4/0x574
&lt;4&gt;[86758.651835] lr : mas_empty_area_rev+0x24c/0x574
&lt;4&gt;[86758.651838] sp : ffff800045eafab0
&lt;4&gt;[86758.651839] pmr_save: 000000e0
&lt;4&gt;[86758.651840] x29: ffff800045eafab0 x28: 00000000001fffff x27: 00000000002a0000
&lt;4&gt;[86758.651843] x26: 0000ffdf00000000 x25: 0000000000000018 x24: ffff00040c9d3000
&lt;4&gt;[86758.651846] x23: 0000000000200000 x22: ffff800008d30a28 x21: ffff00041a0cde0c
&lt;4&gt;[86758.651848] x20: 00000000002a0000 x19: ffff800045eafb48 x18: ffffffffffffffc2
&lt;4&gt;[86758.651850] x17: 0000ffdeff1fffff x16: 0000ffdf00000000 x15: 0000ffdeffffffff
&lt;4&gt;[86758.651852] x14: 0000000000200000 x13: 0000ffdeffffffff x12: ffff00041a0cde00
&lt;4&gt;[86758.651854] x11: ffff00041a0cde80 x10: 000000000029ffff x9 : ffffffffffffc005
&lt;4&gt;[86758.651856] x8 : 1fffe00083419bc1 x7 : 0000000000000000 x6 : 0000000000000000
&lt;4&gt;[86758.651858] x5 : 00000000003d0000 x4 : 0000ffdefee30000 x3 : ffff00041a0cde08
&lt;4&gt;[86758.651861] x2 : 0000000000000000 x1 : 0000000000000000 x0 : ffff00041a0cde0c
&lt;4&gt;[86758.651863] Call trace:
&lt;4&gt;[86758.651864]  mas_empty_area_rev+0x2f4/0x574
&lt;4&gt;[86758.651867]  kbase_unmapped_area_topdown.constprop.0+0x150/0x27c [mali_kbase]
&lt;4&gt;[86758.651889]  kbase_context_get_unmapped_area+0x2b0/0x3b0 [mali_kbase]
&lt;4&gt;[86758.651904]  kbase_get_unmapped_area+0x4c/0x7c [mali_kbase]
&lt;4&gt;[86758.651920]  get_unmapped_area+0x60/0xf0
&lt;4&gt;[86758.651925]  do_mmap+0xe4/0x4fc
&lt;4&gt;[86758.651926]  vm_mmap_pgoff+0xf8/0x190
&lt;4&gt;[86758.651929]  ksys_mmap_pgoff+0xb8/0x10c
&lt;4&gt;[86758.651933]  __arm64_sys_mmap+0x38/0x50
&lt;4&gt;[86758.651934]  invoke_syscall+0x50/0x120
&lt;4&gt;[86758.651938]  el0_svc_common.constprop.0+0x58/0x190
&lt;4&gt;[86758.651941]  do_el0_svc+0x34/0xd0
&lt;4&gt;[86758.651944]  el0_svc+0x28/0xb0
&lt;4&gt;[86758.651946]  el0t_64_sync_handler+0xf4/0x120
&lt;4&gt;[86758.651949]  el0t_64_sync+0x19c/0x1a0
&lt;0&gt;[86758.652637] Kernel panic - not syncing: hung_task: blocked tasks
&lt;4&gt;[86758.652639] CPU: 2 PID: 105 Comm: khungtaskd Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.652641] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.652641] Call trace:
&lt;4&gt;[86758.652642]  dump_backtrace+0xe4/0x140
&lt;4&gt;[86758.652644]  show_stack+0x20/0x30
&lt;4&gt;[86758.652645]  dump_stack_lvl+0x64/0x80
&lt;4&gt;[86758.652647]  dump_stack+0x18/0x34
&lt;4&gt;[86758.652649]  panic+0x198/0x394
&lt;4&gt;[86758.652653]  watchdog+0x2d0/0x510
&lt;4&gt;[86758.652655]  kthread+0x138/0x140
&lt;4&gt;[86758.652657]  ret_from_fork+0x10/0x20
&lt;2&gt;[86758.760353] SMP: stopping secondary CPUs
&lt;0&gt;[86758.760362] Kernel Offset: disabled
&lt;0&gt;[86758.760362] CPU features: 0x00000,000700a4,675072ab
&lt;0&gt;[86758.760364] Memory Limit: none</code></pre><p>另外，一些其他的原因也会导致 kernel panic，详细会在 <a href="https://link.segmentfault.com/?enc=WMAY7%2FjM5L2V7RlxgBrvxg%3D%3D.Bd6RJE1d1eygXoDvjAzuqCVrylvuwWEL6EMQFZvYnsVD81zYQnJpJgQf6%2BxyDERxnKmiZxF%2FHcWu4u5ZPhGU%2BKin3kMRMEsqwsa9X5wzdzDrrFd15VH%2F0PkdvyDIAl7QJSoRkJHixZh1g8S9PLr2V8LhfFlyzyNIlOfvMQ0KZC8%3D" rel="nofollow" target="_blank">Kernel panic</a> 进行介绍。</p><h4>2.2. Memory corruption</h4><p>Memory corruption 类问题一般表现也是 kpanic，但是最明显的标志是问题的随机性和不可解释性，出现这类情况，一般要考虑是内存使用上出现了 UAF（Use-After-Free），OOB（Out-of-Bounds）。</p><p>这类问题的难点在于，系统出现异常 crash 时，已经是前面时间发生踩踏的结果，所以需要定位到踩踏发生的位置，才能正向解决这类问题，一般在系统中存在下述两类踩踏问题：</p><p>a&gt; Linux 内核发生踩踏：</p><ul><li>对于 Acore 中运行的 Linux 系统，KASAN 是目前检查内存访问越界（Out-Of-Bound）和释放后访问（Use-After-Free）问题最有效的工具，KASAN 依赖编译器支持，当前 GCC-12.2 可以支持全功能的 KASAN，但是 KASAN 对性能和内存损耗非常严重，对于 slub 内存的使用问题，轻量级的 LUB\_DEBUG 往往也能提供帮助，但功能比较受限且提供的信息也比较有限。</li></ul><p>b&gt; SoC 子系统间的内存踩踏：</p><ul><li>在 征程 6X SoC 拥有 Acore、BPU、VDSP、Secure World（EL3）的多子系统 SoC， DDR 内存空间根据需求划分给不同子系统，如果子系统间内存出现踩踏，整机系统可能会出现各种随机异常；</li><li>征程 6X SoC 中使用 firewall 对 SoC 各子系统间的内存越界踩踏进行检测，当发生踩踏时由 EL3 触发 crash。</li></ul><h4>2.3. Watchdog</h4><ul><li>征程 6X SoC 中有 2 路 watchdog，目前使用 wdt0（监控 linux irq），wdt1（监控 linux 优先级为 50 的 rt kthread）。从/log/reset\_reason.txt 中可以看到 wdt 的 reason：</li></ul><blockquote>2025-06-13-13-35-25: mwdt               xxx@e9e8fadb9907 debug 20250610-193933   1100</blockquote><ul><li>wdt 导致重启后，系统会保存 pstore log，通过检查 pstore 获取异常信息；</li><li>wdt 狗咬中断/事件由 MCU 域处理，MCU 域进行重启。</li><li>wdt0：一般是 kernel 中某个 CPU 处于长时间无法响应中断的状态，征程 6X 平台上在 wdt1 狗咬时间（IRQ\_WDT\_TIMEOUT）到后会触发一个 gic 中断，在这个中断中会使用 NMI 将所有 cpu 的调用栈打印；</li><li>wdt1：一般是 kernel 中某个 CPU 处于长时间无法调度优先级为 50 的 rt kthread 的状态，征程 6X 平台上在 wdt2 狗咬时间（IRQ\_WDT\_TIMEOUT）到后会触发一个 gic 中断，在这个中断中会使用 NMI 将所有 cpu 的调用栈打印。</li></ul><h4><strong>2.4 firewall</strong></h4><p><strong>在 征程 6X SoC 拥有 Acore、BPU、GPU、VDSP0、Secure World（EL3）等多个子系统，DDR 内存空间根据需求划分给不同子系统。</strong></p><p>如果子系统间内存/寄存器空间出现踩踏，整机系统可能会出现各种随机异常。firewall 是 征程 6X SoC 上的硬件单元，功能就是根据配置捕获子系统间的内存越界踩踏，当发生踩踏时由 EL3 触发 crash。</p><p>在 MCU 域的 log 中会输出发生越界访问的地址信息和 master 信息，输出示例如下：ID 为 0xd0 的 master 尝试读地址为 0x80000000 的内存空间。</p><p>MCU 域主动触发 firewall 违例：</p><pre><code class="markdown">horizon:/$ regread 0x80000000</code></pre><p>MCU 域的违例信息 log：</p><pre><code class="markdown">[016.346991 0]firewall module 136 read violation master ID:d0
[016.347654 0]violation port 0[016.348004 0]violation addr high:0, low:80000000
[016.348656 0]Reg 0x80000000 value is 0x12345678

horizon:/$ [016.369156 0][M][time_1: 000016 s, 346 ms] Fchm Info occur (34, 30, 2552, 136)[016.370239 0][M][time_2: 000016 s, 348 ms] 136-6-CF Occur (34, 30, 2552), Payload(00-00-136-00 00-00-00-128 00-00-00-00 208-00-00-00)[016.371715 0]Customer handle(63, 1, 2)[016.421368 0][M][time_1: 000016 s, 348 ms] Fchm Info occur (36, 3, 2553, 349)[016.422437 0][M][time_2: 000016 s, 348 ms] 349-6-CF Occur (36, 3, 2553), Payload(00-00-00-00 00-00-00-00 00-00-00-00 00-00-00-00)[016.423894 0]Customer handle(51, 1, 2)</code></pre><p>征程 6X 部分内存的 firewall 权限设置：</p><p>Master ID 定义，详见：hbbin\_j6p/boot/j6p/bl31-dts/include/hobot\_firewall.h。</p>]]></description></item><item>    <title><![CDATA[B站即时通讯IM消息系统的新架构升级实践 JackJiang ]]></title>    <link>https://segmentfault.com/a/1190000047525462</link>    <guid>https://segmentfault.com/a/1190000047525462</guid>    <pubDate>2026-01-06 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文由B站技术团队比奇堡、Xd、三木森分享，有修订和重新排版。</p><h2>1、引言</h2><p>本文要分享的是B站IM消息系统的新架构升级实践总结，内容包括原架构的问题分析，新架构的整体设计以及具体的升级实现等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525464" alt="图片" title="图片"/><br/>B站技术团队的其它技术文章：B站千万级长连接实时消息系统的架构设计与实践B站实时视频直播技术实践和音视频知识入门B站基于微服务的API网关从0到1的演进之路</p><h2>2、消息系统业务解读</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525465" alt="图片" title="图片" loading="lazy"/><br/>按业务全域现状，在服务端角度分成客服系统、系统通知、互动通知和私信4个业务线，每个业务线内按现状标识了服务分层。私信内分为用户单聊、bToC的批量私信、群聊和应援团小助手四类，这四类细分私信没有技术解耦，单聊和批量私信比较接近系统天花板。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525466" alt="图片" title="图片" loading="lazy"/><br/>私信单聊发送到触达的pv转化和uv转化不足10%，有明显通过业务优化提升触达率的潜力。</p><h2>3、消息系统中的私信业务</h2><p>私信域内的几个概念解释：1）会话列表：按聊天人排序的列表。即B站首页右上角信封一跳后看到的历史聊天人列表，以及点击未关注人等折叠会话看到的同属一类的聊天人列表。传达对方账号、最新私信和未读数的信息。点击一个会话后看到的是对聊历史，也称会话历史。2）会话详情：描述和一个聊天人会话状态的原子概念，包括接收人uid、发送人uid、未读数、会话状态、会话排序位置等。3）会话历史：按时间线对发送内容排序的列表。一份单聊会话历史既属于自己，也属于另一个和自己的聊天的人。群聊的会话历史属于该群，不属于某个成员。会话历史是收件箱和消息内容合并后的结果。4）收件箱：将一次发送的时序位置映射到发送内容唯一id的kv存储，可以让服务端按时间序读取一批发送内容唯一id。5）私信内容：一个包括发送内容唯一id、原始输入内容、消息状态的原子概念。批量私信把同一个发送内容唯一id写入每个收信人的收件箱里。6）timeline模型：时间轴的抽象模型，模型包括消息体、已读位点、最大位点、生产者、消费者等基本模块，可以用于基于时间轴的数据同步、存储和索引。私信涉及timeline模型的包括会话列表和会话历史。7）读扩散：pull模式。群聊每条私信只往群收件箱写一次，让成百上千的群成员在自己的设备都看到，是典型的读扩散。8）写扩散：push模式。单聊每条私信既更新接收人会话也更新发送人会话，是轻微的写扩散，无系统压力。群聊有另一个不一样的特点，就是当群成员发送消息后，需要通过长链接通知其他群成员的在线设备，以及发送人其他的在线设备，这是一个写扩散的技术模型，但是这个写扩散是通知后即时销毁的，并且具有过期时间，所以仅临时占用资源，并不对存储造成压力，且能有较好的并发量。私信核心概念关系表达：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525467" alt="图片" title="图片" loading="lazy"/></p><h2>4、消息系统问题1：会话慢</h2><p>查询当会话缓存过期时，Mysql是唯一回源，Mysql能承载的瞬时QPS受当时应用总连接数和sql平均响应速度的影响，连接数打满时会给前端返回空会话列表。虽然可以增加POD数量、增大akso proxy连接数、优化sql和索引来作为短线方案，来提升瞬时请求Mysql容量，但是这种短线方案无法加快单次响应速度，mysql响应越来越慢的的问题依然在。另外增加POD数量也会降低发版速度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525468" alt="图片" title="图片" loading="lazy"/><br/>会话Mysql使用用户uid%1000/100分库，用户uid%100分表，table总量是1000。单表会话量在1kw-3.2kw。单个大up的会话积累了10W条以上，会话量最大的用户有0.2亿条会话。单个Up的会话会落到一张表中，每张表都有比较严重的数据倾斜。如果考虑增加分库分表的方案，sql查找条件依然需要用户uid，所以相当于倾斜数据要转移到新的单表，问题没有解决。另外，重新分库分表过程中新旧table增量同步和迁移业务读写流量的复杂度也很大，有比较大的业务风险。Mysql的规格是48C 128G和32C 64G。由于会话数据量大，Mysql buffer_pool有限，数据比较容易从内存淘汰，然后mysql需要进行磁盘扫描并将需要的数据加载到内存进行运算，加之比较多的磁盘扫描数据，这时的响应一般在秒级别，接口会给前端返回超时错误，会话列表页空白。为了适配业务发展，Mysql 会话表 已经添加了9个非聚集索引，如果通过增加索引使用业务需要，需要更大的Mysql资源，且解决不了冷数据慢查询的问题。增加更多索引也会让Mysql写入更慢。</p><h2>5、消息系统问题2：私信内容单表空间和写性能接近天花板</h2><p>每条私信内容都绑定私信自己的发号器生成的msgkey，即私信内容唯一id，该msgkey包含私信发送时的时间戳（消息ID生成可参阅读《微信的海量IM聊天消息序列号生成实践》）。读写私信内容Mysql之前先从msgkey解析出时间，用这个时间路由分库分表。私信内容库按季度分库，分库内按月度分表，单表数据量数亿，数据量最大的用户日增私信351.9W条。按照曲率预测，25年全年数据量有近百亿，如果继续按照月度分表，分表规则不适应增长。当前该Mysql最大写qps 790，特别活动时写qps峰值预计是20k，但是为了保障Mysql服务整体的可靠，单库写流量我们需要控制在3000qps以下，无法满足写入量峰值时的需要。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525469" alt="图片" title="图片" loading="lazy"/><br/>此外，消息内容表结构包含了群聊、单聊和应援团小助手全部的属性，增加业务使用难度。绝大部分私信内容是单聊的。</p><h2>6、消息系统问题3：服务端代码耦合</h2><p>B站的四类私信包括：1）单聊；2）群聊；3）B端批量私信；4）应援团小助手。这些私信都需要实现发送和触达两条核心链路，四种私信核心链路的代码逻辑和存储耦合在一起，代码复杂度随着业务功能上线而不断增加，熵增需要得到控制。从微服务这方面来说，实例和存储耦合会带来资源随机竞争，当一方流量上涨，可能给对方的业务性能带来不必要的影响，也会带来不必要的变更传导。</p><h2>7、消息系统新架构的升级路径</h2><p>基于对私信现状的论述，可以确定我们要优化的是一个数据密集型 &gt;&gt; 计算密集型，读多写少（首页未读数）、读少写多（会话）场景兼具的系统。同时需要拥有热门C端产品的稳定性、扩展性和好的业务域解耦。针对读多写少和读少写多制定了针对的技术方案。具体的实施情况请继续往下阅读。</p><h2>8、新架构的整体设计</h2><p>结合B站业务现状，我觉得比较合理的架构：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525470" alt="图片" title="图片" loading="lazy"/><br/>一个兼顾复杂列表查询架构和IM架构的消息域框架，整体分四层：1）接入层：即toC的BFF和服务端网关；2）业务层：按复杂查询设计系统，用于各种业务形态的支撑；3）平台层：按IM架构设计系统，目标是实时、有序的触达用户，平台层可扩展；4）触达层：对接长链和push。</p><h2>9、新架构具体升级1：端上本地缓存降级</h2><p>端上应该支持部分数据缓存，以确保极端情况下用户端可展示，可以是仅核心场景，比如支付小助手、官号通知，用户在任何情况下打开消息页都不应该白屏。</p><h2>10、新架构具体升级2：BFF架构升级</h2><p>BFF网关吸收上浮的业务逻辑，控制需求向核心领域传导。服务端基于业务领域的能力边界，抽象出单聊、群聊、系统通知、互动通知和消息设置共五个新服务，提升微服务健康度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525471" alt="图片" title="图片" loading="lazy"/><br/>新服务剥离了历史包袱，也解决一些在老服务难解的功能case，优化了用户体验，比如消息页不同类型消息的功能一致性；重新设计会话缓存结构和更新机制，优化Mysql索引，优化Mysql查询语句，减少了一个量级的慢查询。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525472" alt="图片" title="图片" loading="lazy"/></p><h2>11、新架构具体升级3：服务端可用性升级</h2><p>11.1 概述服务端按四层拆分后，集中精力优化业务层和平台层。业务层：按复杂查询设计系统，用于各种业务形态的支撑1）冷热分离：多级缓存 redis(核心数据有过期)+taishan(有限明细数据)+mysql(全部数据)；2）读写分离：95%以上复杂查询可以迁移到从库读。平台层：按IM架构设计系统，目标是实时、有序的触达用户，平台层可扩展1）Timeline模型：依赖雪花发号器，成熟方案；2）读写扩散：单聊-写扩散，群聊-读扩散。11.2 单聊会话1）缓存主动预热：用户在首页获取未读数是一个业务域内可以捕捉的事件，通过异步消费这个事件通知服务端创建会话缓存，提高用户查看会话的缓存命中率。鉴于大部分人打开B站并不会进私信，此处可以仅大UP预热。大UP的uid集合可以在数平离线分析会话数据后写入泰山表，这个泰山表更新时效是T+1。监控UP会话数量实时热点，触发突增阈值时，通过异步链路自动为热点用户主动预热会话列表缓存。对预热成功率添加监控，并在数平离线任务失败或者预热失败时做出业务告警，及时排查原因，避免功能失效。2）泰山和Mysql双持久化：增加泰山存储用户有限会话明细，作为redis未命中后的第一回源选择，Mysql作为泰山之后的次选。基于用户翻页长度分析后确定泰山存储的有限会话的量级。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525473" alt="图片" title="图片" loading="lazy"/><br/>redis 存储24小时数据，taishan 存储 600条/用户（20页），预设到的极端情况才会回源mysql从库。对于ZSET和KV两种数据结构，评估了各自读写性能的可靠性，符合业务预期。业务如果新增会话类型，可以跟本次新增泰山有限明细一样，基于会话类型的具体规则新增泰山Key。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525474" alt="图片" title="图片" loading="lazy"/><br/>3）泰山长尾优化：查询redis未命中时会优先回源泰山，考虑到泰山99分位线在50ms以下，而且Mysql多从实例都能承受来自C端的读请求，所以采用比泰山报错后降级Mysql稍微激进的对冲回源策略。在泰山出现“长尾”请求时，取得比较好的耗时优化效果。可以使用大仓提供的error group结合quit channel实现该回源策略，同时能避免协程泄漏。整个处理过程在业务响应和资源开销中维持中间的平衡，等待泰山的时间可以灵活调整。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525475" alt="图片" title="图片" loading="lazy"/><br/>泰山最初没有数据，可以在泰山未命中时进行被动加载，保证用户回访时能命中。4）一致性保证：虽然我们重构了新服务，但是老服务也需要保留，用来处理未接入BFF的移动端老版本和web端请求，这些前端在更新会话时（比如ACK）请求到了老服务，新服务需要通过订阅会话Mysql binlog异步更新本服务的redis和泰山。为了避免分区倾斜，订阅binlog的dts任务使用id分区，这样方便的是一条会话在topic的分区是固定的。为了避免两次请求分别命中泰山和Mysql时给用户返回的数据不一样，需要解决三大问题：a. 当出现分区rebalance需要避免重复消费；b. 当Mysql一条会话记录在短时间内（秒级）多次更新，要保证binlog处理器不会逆时间序消费同一个会话的binlog，即跳过较早版本的binlog；c. 保证泰山写入正确并且从Mysql低延迟同步。这三个问题都要保证最终一致性，具体解决方案是用redis lua脚本实现compare and swap，lua脚本具有原生的原子性优势。dts每同步一条binlog都会携带毫秒级mtime，当binlog被采用时，mtime被记入redis10分钟，如果下一条binlog的mtime大于redis记录的mtime，这条binlog被采用，否则被丢弃。这个过程可以考虑使用gtid代替mtime，但这个存在的问题是每个从实例单独维护自己的gtid，当特殊情况发生mysql主从切换，或者dts订阅的从节点发生变更，gtid在CAS计算中变得不再可靠，所以我们选择了使用mtime作为Mysql会话记录的版本。通过消费路线高性能设计保证泰山异步更新的延迟在1秒以内，并在特殊情况延迟突破1s时有效告警。高性能消费路线中，每个库的binlog分片到50个partition，业务提供不低于50个消费pod，单pod配置100并发数，按照写泰山999分位线20ms计算，每秒可以消费 50<em>100</em>(1000/20)=250000 条，大约线上峰值8.3倍，考虑dts本身的max延迟在600~700毫秒，同步泰山和redis的延迟会在700毫秒至1秒以内，符合业务预期。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525476" alt="图片" title="图片" loading="lazy"/><br/>11.3 收件箱BFF已经从业务层和平台层将单聊读收件箱独立出来，本次升级主要是从存储做增量解耦 ，存量单聊收件箱的读流量可以访问旧表。 单聊新收件箱存储采用redis+泰山的模式，redis提供热数据，泰山提供全部数据并采用RANDOM读模式，让主副本都能分担读流量。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525477" alt="图片" title="图片" loading="lazy"/><br/> 11.4 私信内容本次升级主要如下：1）单聊增量数据独立存储，按照单聊业务设计表结构，和群聊、应援团小助手彻底解耦。2）写Mysql升级为异步化操作，提高写性能天花板，这种异步写Mysql改造不会影响读消息内容的可用性和设计。3）单聊分库规则升级为月度分库，单库内分表为100张。 群聊、应援团小助手和历史单聊依然使用旧的分库分表规则读写Mysql。业务需要对增量单聊私信路由分库分表时，先从msgkey先解析出时间戳，找到用时间戳对应的月份分库，然后用msgkey对100取余找到分表。这种方案能达到按时间纬度的冷热数据的分离，同时由于msgkey取余的结果具有随机性，平衡了每张表的读写流量。这样预计2025年单表数据量能从9亿下降到900万。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525478" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525479" alt="图片" title="图片" loading="lazy"/><br/>11.5 批量私信日常通道：日常批量私信任务共用通道，共用配额。高优通道：主要通过将链路上topic partition扩容、消费POD扩容、POD内消费通道数扩容、缓存扩容、akso proxy连接数扩容，把平均发送速度从3500 人/秒提高到30000人/秒。这个通道可以特殊时期开给特殊业务使用。</p><h2>12、本文小结</h2><p>我们逐步发现技术升级不是一蹴而就的，它是一个逐步优化的过程。设计技术方案前设立合适和有一些挑战的目标，但这个目标要控制成本，做好可行性。设计技术方案的时候，需要清楚现有架构与理想架构的差距和具体差异点，做多个方案选型，并确定一个，这个更多从技术团队考虑。其次要保证功能在新老架构平稳过渡，保证业务的稳定性。后面持续关注新老架构的技术数据，持续优化，老架构要持续关注它的收敛替换。IM系统是一个老生常谈的话题，也是融合众多有趣技术难点的地方，欢迎感兴趣的同行交流研讨。</p><h2>13、参考资料</h2><p>[1] 浅谈IM系统的架构设计[2] 简述移动端IM开发的那些坑：架构设计、通信协议和客户端[3] 一套海量在线用户的移动端IM架构设计实践分享(含详细图文)[4] 一套原创分布式即时通讯(IM)系统理论架构方案[5] 从零到卓越：京东客服即时通讯系统的技术架构演进历程[6] 蘑菇街即时通讯/IM服务器开发之架构选择[7] 微信技术总监谈架构：微信之道——大道至简(演讲全文)[8] 现代IM系统中聊天消息的同步和存储方案探讨[9] 子弹短信光鲜的背后：网易云信首席架构师分享亿级IM平台的技术实践[10] 一套高可用、易伸缩、高并发的IM群聊、单聊架构方案设计实践[11] 从游击队到正规军(一)：马蜂窝旅游网的IM系统架构演进之路[12] 瓜子IM智能客服系统的数据架构设计（整理自现场演讲，有配套PPT）[13] 阿里钉钉技术分享：企业级IM王者——钉钉在后端架构上的过人之处[14] 阿里技术分享：电商IM消息平台，在群聊、直播场景下的技术实践[15] 一套亿级用户的IM架构技术干货(上篇)：整体架构、服务拆分等[16] 从新手到专家：如何设计一套亿级消息量的分布式IM系统[17] 企业微信的IM架构设计揭秘：消息模型、万人群、已读回执、消息撤回等[18] 融云技术分享：全面揭秘亿级IM消息的可靠投递机制[19] 阿里IM技术分享(三)：闲鱼亿级IM消息系统的架构演进之路[20] 基于实践：一套百万消息量小规模IM系统技术要点总结[21] 跟着源码学IM(十)：基于Netty，搭建高性能IM集群（含技术思路+源码）[22] 一套十万级TPS的IM综合消息系统的架构实践与思考[23] 得物从0到1自研客服IM系统的技术实践之路[24] 一套分布式IM即时通讯系统的技术选型和架构设计[25] 微信团队分享：来看看微信十年前的IM消息收发架构，你做到了吗[26] 转转平台IM系统架构设计与实践(一)：整体架构设计[27] 支持百万人超大群聊的Web端IM架构设计与实践[28] 转转客服IM聊天系统背后的技术挑战和实践分享</p><p><strong>即时通讯技术学习：</strong></p><ul><li>移动端IM开发入门文章：《新手入门一篇就够：从零开发移动端IM》</li><li>开源IM框架源码：<a href="https://link.segmentfault.com/?enc=ScdcVz39O%2FPkp0EjAwxI6w%3D%3D.qT%2BIM5jJk2%2FZ4iLlwlwuA2Aet42UAZ4cdIJphZMC%2BYt9d6gEn6dHIu%2BO9YCi%2BF03" rel="nofollow" target="_blank">https://github.com/JackJiang2011/MobileIMSDK</a>（备用地址点此）</li></ul><p>（本文已同步发布于：<a href="https://link.segmentfault.com/?enc=k82NF9Q2%2FcGv0tQ9tv7aCQ%3D%3D.aSoAsnbk036t1FVuzYZT7dOW%2FFkw5l5IxiZipI6O9py6fTw1MkyFM9LkWIpTiDC5" rel="nofollow" target="_blank">http://www.52im.net/thread-4886-1-1.html</a>）</p>]]></description></item><item>    <title><![CDATA[nameko 无法适配新版的python3.14，eventlet 停止维护导致的失效 rabbit]]></title>    <link>https://segmentfault.com/a/1190000047525496</link>    <guid>https://segmentfault.com/a/1190000047525496</guid>    <pubDate>2026-01-06 23:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>nameko 无法适配新版的python3.14，eventlet 停止维护导致的 1 RLock(s) were not greened 报错</p><hr/><p>直接把 nameko2.14.1 用到 python3.14 上会报错 <code>ModuleNotFoundError: No module named 'pkg_resources'</code></p><p>修复之后还是会有警告</p><pre><code class="shell">╰─➤  nameko run --config config.yaml run_services
1 RLock(s) were not greened, to fix this error make sure you run eventlet.monkey_patch() before importing any other modules.
starting services: greeting_service
Connected to amqp://ponponon:**@192.168.31.245:5672//</code></pre><p>这个 <code>1 RLock(s) were not greened</code> 不是因为 nameko 的问题，而是来自 eventlet 本身的问题：<a href="https://link.segmentfault.com/?enc=YWttCbT%2BXH6ikZQpzs%2BQwg%3D%3D.kTpPXaqHCZIflTXH8pJkss0ZJ2a6EYW0NoPnkBq3xD9dDJPsnSn7hg1nkILwRqE5" rel="nofollow" target="_blank">https://github.com/eventlet/eventlet</a></p><p>我通过降低 cpython 版本做测试，发现 cpython3.13 也有这个问题；直到降级到 cpython3.12 则没有这个问题了</p>]]></description></item><item>    <title><![CDATA[Agentic RAG：用LangGraph打造会自动修正检索错误的 RAG 系统 本文系转载，阅读]]></title>    <link>https://segmentfault.com/a/1190000047525372</link>    <guid>https://segmentfault.com/a/1190000047525372</guid>    <pubDate>2026-01-06 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>标准 RAG 流水线有个根本性的毛病：检索到的文档一旦与用户意图对不上号，模型照样能面不改色地输出一堆看似合理的胡话，既没有反馈机制也谈不上什么纠错能力。</p><p>而Agentic RAG 的思路截然不同，它不急着从检索结果里硬挤答案，而是先判断一下拿回来的东西到底有没有用，如果没用则会重写查询再来一轮。这套机制实际上构建了一条具备自我修复能力的检索链路，面对边界情况也不至于直接崩掉。</p><p>本文要做的就是用 LangGraph 做流程编排、Redis 做向量存储，搭一个生产可用的 Agentic RAG 系统。涉及整体架构设计、决策逻辑实现，以及状态机的具体接线方式。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525374" alt="" title=""/></p><h2>传统 RAG 的"一锤子买卖"</h2><p>假设知识库里有一篇《大语言模型的参数高效训练方法》，用户问的是"怎么微调 LLM 效果最好"。</p><p>语义相似度确实存在但不够强。检索器拉回来的可能是模型架构相关的内容虽然沾边但答非所问，LLM 本身没法意识到上下文是错的，照样能生成一段貌似专业实则离题万里的回答。</p><p>传统 RAG 对这种失败模式完全没有办法。查询文档、生成答案，整个过程是单向的没有任何质量把关环节。</p><p>Agentic RAG 的解法是在流程中插入检查点：智能体先判断要不要检索；检索完了有评分环节确认相关性；不相关就重写查询再试；如此循环直到拿到合格的上下文，或者把重试次数耗尽为止。</p><h2>系统架构拆解</h2><p>整个系统拆成六个模块：</p><p>配置层负责环境变量和 API 客户端的初始化工作。Redis 连接串、OpenAI 密钥、模型名称全部归拢到这里统一管理。</p><p>检索器模块承担文档摄取的全套流程，文档经过</p><pre><code>WebBaseLoader</code></pre><p>加载后用</p><pre><code>RecursiveCharacterTextSplitter</code></pre><p>切块，再通过 OpenAI Embedding 向量化，最后存进</p><pre><code>RedisVectorStore</code></pre><p>。检索器本身会被包装成 LangChain 工具供智能体调用。</p><p>智能体节点是决策入口。拿到用户问题后先做判断：这个问题需要查资料还是直接能答？需要查就调检索器，不需要就直出答案。</p><p>评分（Grade Edge）决定检索结果的去向。相关性够就往生成环节走；不够就触发重写。这是整个系统里最关键的质量关卡。</p><p>重写节点把原始问题改写成更适合检索的形式，用户表述太口语化、缺少关键词，这些问题都在这里修正。</p><p>生成节点只有在评分环节确认上下文合格后才会执行，基于检索到的文档产出最终答案。</p><h2>流程图和代码</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525375" alt="" title="" loading="lazy"/></p><p>关键在于从"重写"回到"智能体"这条反馈路径。系统不会因为一次检索失败就直接给出一个牵强附会的答案，它会调整策略重新尝试。</p><pre><code> src/  
├── config/  
│   ├── settings.py      # 环境变量  
│   └── openai.py        # 模型名称和 API 客户端  
├── retriever.py         # 文档摄取和 Redis 向量存储  
├── agents/  
│   ├── nodes.py         # 智能体、重写和生成函数  
│   ├── edges.py         # 文档评分逻辑  
│   └── graph.py         # LangGraph 状态机  
 └── main.py              # 入口点</code></pre><p>职责划分很清晰：配置归</p><pre><code>config/</code></pre><p>，智能体相关的都在</p><pre><code>agents/</code></pre><p>，向量存储操作全在</p><pre><code>retriever.py</code></pre><p>。这种结构调试起来方便，单测也好写。</p><h2>配置模块设计</h2><p>配置层解决两个问题：环境变量加载和 API 客户端复用。</p><pre><code>settings.py</code></pre><p>集中读取 Redis 连接信息、OpenAI API Key、索引名称，不用满项目找配置。</p><pre><code>openai.py</code></pre><p>负责实例化 Embedding 模型和 LLM 客户端。切换到别的模型、调整 Embedding 维度等等配置也只要一处</p><p>这个设计在生产环境里很实用，因为模型会迭代、Key 会轮换、服务商可能换掉，集中管理意味着改动成本可控。</p><h2>检索器实现</h2><p>检索器负责整条数据摄取链路：抓文档、切块、向量化、入库。</p><p>语料选的是 Lilian Weng 关于 Agent 和 Prompt Engineering 的博客文章。</p><pre><code>WebBaseLoader</code></pre><p>负责抓取，</p><pre><code>RecursiveCharacterTextSplitter</code></pre><p>切分成适当大小的块，OpenAI Embedding 完成向量化。</p><p>向量存储用</p><pre><code>RedisVectorStore</code></pre><p>。检索器通过</p><pre><code>create_retriever_tool</code></pre><p>封装成 LangChain 工具形态。这一步的意义在于让智能体能够"调用"检索而不是被动触发，意味着它有权决定什么时候需要查资料、什么时候直接回答。</p><p>为什么用Redis？因为够快，够简单。向量相似度搜索本身 Redis 就能做，不用额外引入专门的向量数据库。对于已经跑着 Redis 的技术栈来说，加 RAG 能力几乎零额外运维负担。</p><h2>智能体节点</h2><pre><code>nodes.py</code></pre><p>里有三个核心函数。</p><p>智能体函数接收当前状态（用户问题、历史对话等），判断下一步怎么走。它能调用包括检索器在内的工具集。问题需要外部知识就调检索，不需要就直接生成回答。</p><p>重写函数处理那些被评分环节打回来的查询。它会让 LLM 把原始问题改写成检索友好的形式，用词更精准、关键信息更突出。改写后的查询再交回智能体重新发起检索。</p><p>生成函数产出最终答案。输入是原始问题加上已确认相关的文档，输出是基于这些上下文的回答。</p><p>三个函数都是无状态的。状态走图，不走函数内部变量。这对测试和排查问题都有好处。</p><h2>文档评分逻辑</h2><pre><code>edges.py</code></pre><p>里的</p><pre><code>grade_documents</code></pre><p>是整个 Agentic 机制的核心。</p><p>检索完成后它会逐个审视返回的文档：这东西跟用户问的相关不相关？能不能帮上忙？</p><p>评分本身是通过一次 LLM 调用完成的，Prompt 设计成要求模型返回二元判断——相关或者不相关。</p><p>判定相关就返回</p><pre><code>"generate"</code></pre><p>，流程走向答案生成；判定不相关则返回</p><pre><code>"rewrite"</code></pre><p>，触发查询改写。</p><p>这个环节的价值在于拦截那些本会导致标准 RAG 胡说八道的情况，与其硬着头皮从不靠谱的上下文里编答案，不如给系统一次修正查询的机会。</p><h2>状态机接线</h2><pre><code>graph.py</code></pre><p>用 LangGraph 的状态机原语把所有节点串起来。</p><p>图结构定义了节点（智能体、检索、生成、重写）和边（节点间的连接关系，包括基于评分结果的条件路由）。</p><p>接线逻辑如下：查询先到智能体节点，智能体决定调检索器的话流程就到检索节点，检索完进评分，评分过了走生成，没过走重写，重写完的查询再回智能体重新来过。生成节点执行完流程结束。</p><p>LangGraph 接管状态流转的细节。每个节点只管接收当前状态、返回状态更新，具体消息怎么路由由图引擎根据边的条件逻辑处理。</p><h2>运行时流程</h2><pre><code>main.py</code></pre><p>是入口，做三件事：构建图、接收问题、流式输出结果。</p><pre><code>build_graph()</code></pre><p>在启动时执行一次，完成 LangGraph 状态机的构建和检索器工具的初始化.</p><p>问题进来之后的流转过程：智能体接收问题决定调检索 → Redis 返回文档 → 评分环节判断相关性 → 相关就生成答案，不相关就重写查询继续循环。</p><p>脚本会把各节点的输出实时打到控制台，方便观察决策过程——什么时候触发了检索、评分结果如何、有没有走到重写环节，一目了然。</p><h2>架构的优势</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525376" alt="" title="" loading="lazy"/></p><p><strong>自校正能力</strong>：检索质量差能发现并修复，不会闷头输出一个基于垃圾上下文的错误答案然后假装没事发生。</p><p><strong>决策透明</strong>：状态机让每个分支点都是显式的。路由决策可以全量记录，想排查为什么系统选择了重写而不是直接生成，日志里全有。</p><p><strong>模块解耦</strong>：每个组件职责单一。想把 Redis 换成 Pinecone？改检索模块。想把 OpenAI 换成 Anthropic？改配置层。其他部分不受影响。</p><h2>总结</h2><p>标准 RAG 把检索当黑盒，查询丢进去、文档出来，至于相不相关全凭运气。Agentic RAG 打开这个黑盒在关键位置加了质量控制。</p><p>LangGraph 加 Redis 的组合提供了一个可以直接上生产的骨架。流程编排的复杂度 LangGraph 消化掉了，向量检索的性能 Redis 兜住了，剩下的评分和重写逻辑负责兜底那些简单系统搞不定的边角案例。</p><p>代码：</p><p><a href="https://link.segmentfault.com/?enc=rtFWx%2B6oN7e8eoOn49TJVg%3D%3D.SC4CAqwoFwohttxer1hArTT0oDfUeQKrtmIThSU3hI2jXjeVOcrAwVmOHczwWJYBHlm7jZ%2BM9mmJCY%2BK0hdV8g%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/a45e19af576a4826a605807d8fcfe298</a></p><p>作者：Kushal Banda</p>]]></description></item><item>    <title><![CDATA[千峰嵌入式2023-完整版 技术站999it点top ]]></title>    <link>https://segmentfault.com/a/1190000047525246</link>    <guid>https://segmentfault.com/a/1190000047525246</guid>    <pubDate>2026-01-06 21:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>《2023 千峰嵌入式开发实战指南：MCU 编程・外设驱动・工业级项目开发精讲》<br/>——从教育公平、科技自立、人文关怀与产业升级多维视角看嵌入式人才的培养价值</p><p>在“万物智联”成为现实的今天，嵌入式系统早已悄然渗透进我们生活的每个角落：从智能家电、车载电子，到工业机器人、医疗设备，再到国家电网、航空航天等关键基础设施，其背后都离不开微控制器（MCU）与底层驱动的精密协同。《2023 千峰嵌入式开发实战指南》以“MCU 编程—外设驱动—工业级项目”为脉络，不仅传授技术细节，更折射出一场关乎国家科技根基、教育转型与产业未来的深层变革。</p><p>教育维度：打破“重应用、轻底层”的失衡格局，重塑工程教育根基<br/>长期以来，高校计算机教育过度聚焦于 Web 开发、移动应用等上层软件，导致大量毕业生对硬件交互、内存管理、中断处理等底层机制缺乏基本认知。这种“空中楼阁”式的培养模式，难以支撑高端制造、芯片设计、工业自动化等国家战略领域的人才需求。</p><p>《千峰嵌入式开发实战指南》以系统化、阶梯式的内容设计，引导学习者从寄存器操作、时钟配置、GPIO 控制等基础入手，逐步掌握 UART、I2C、SPI、ADC 等外设驱动开发，并最终完成如智能温控系统、工业数据采集终端等贴近真实场景的项目。这种“从硅片到系统”的全链路训练，重建了软硬结合的工程思维，为高校教育提供了可借鉴的实践范本，也为自学群体打开了通往硬科技领域的大门。</p><p>科技维度：夯实国产芯片生态，助力关键技术自主可控<br/>当前，全球半导体产业链竞争白热化，MCU 作为“芯片中的芯片”，广泛应用于消费电子与工业控制领域。然而，国内大量嵌入式开发仍依赖国外芯片平台（如 STM32）及配套工具链，存在供应链风险与技术黑盒问题。</p><p>本指南虽以通用原理为主，但其强调的“理解芯片手册、掌握驱动抽象、适配不同硬件平台”的能力，正是构建国产芯片生态适配力的关键。当开发者具备扎实的底层开发功底，便能快速迁移至国产 MCU（如兆易创新、华大半导体、乐鑫等）平台，参与国产芯片的验证、优化与生态建设。从这个角度看，一本嵌入式教材，实则是培育国产半导体“土壤”的重要一环。</p><p>人文发展维度：技术应服务于人的安全、尊严与可持续生活<br/>嵌入式系统不同于普通软件，其失效可能直接导致物理世界的安全事故——如医疗设备误判、工业机械失控、汽车刹车失灵。因此，嵌入式开发天然带有高度的责任伦理。</p><p>《实战指南》在工业级项目讲解中，反复强调实时性保障、异常处理机制、电源管理策略与电磁兼容设计等工程规范，传递出一种严谨、敬畏、以人为本的技术价值观。它提醒开发者：你写的每一行初始化代码，都可能关系到一个工人的安全、一位患者的健康，或一个家庭的用电稳定。这种将技术精度与人文关怀相融合的教育理念，正是培养“负责任工程师”的核心所在。</p><p>经济维度：赋能制造业升级，催生“新蓝领”技术岗位<br/>中国正从“制造大国”迈向“智造强国”，而智能制造的核心在于“感知—决策—执行”的闭环，这正是嵌入式系统的主战场。无论是工厂的 PLC 控制器、物流 AGV 小车，还是农业物联网传感器、新能源充电桩，都急需大量懂硬件、会编程、能调试的复合型嵌入式人才。</p><p>本指南通过工业级项目实战，帮助学习者掌握企业真正需要的技能：如何读芯片 datasheet？如何用示波器调试通信协议？如何在资源受限环境下优化代码？这些能力使学习者能快速胜任嵌入式软件工程师、FAE（现场应用工程师）、测试验证工程师等高价值岗位。更重要的是，它为传统制造业工人、职校学生提供了向“数字新蓝领”转型的可行路径，推动劳动力结构向高技能、高附加值方向演进。</p>]]></description></item><item>    <title><![CDATA[AI 的“性格旋钮”——什么是大模型的温度？ blossom ]]></title>    <link>https://segmentfault.com/a/1190000047525315</link>    <guid>https://segmentfault.com/a/1190000047525315</guid>    <pubDate>2026-01-06 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>你有没有发现：有时候 AI 像个严谨的老教授，回答滴水不漏；有时候它又像个天马行空的艺术家，能编出一堆意想不到的情节？</p><p>这背后往往藏着一个关键参数：<strong>温度（Temperature）</strong>。</p><p>别担心，调高温度并不会让电脑“发烫”，也不是让 AI 发烧。这里的温度，更像一个控制 AI <strong>“有多敢冒险”</strong>的性格旋钮：</p><ul><li>温度低 → 更稳、更像标准答案</li><li>温度高 → 更发散、更有创意，但也更容易跑偏</li></ul><hr/><h2>一、为什么需要温度？（AI 的“填空游戏”）</h2><p>要理解温度，先看大模型是怎么说话的。</p><p>大模型生成文本的过程，近似于一种“逐字填空”的游戏：每输出一个词（token），它都会对“下一步可能出现的候选词”打分。</p><p>比如当 AI 写到：</p><blockquote>“今天天气真——”</blockquote><p>它脑内可能有这样一张“候选词打分表”（通常称为 <strong>logits</strong>）：</p><ul><li><strong>好</strong>：90 分（最稳妥）</li><li><strong>热</strong>：50 分（也合理）</li><li><strong>怪</strong>：5 分（少见但勉强能通）</li><li><strong>紫色</strong>：0 分（基本不通顺）</li></ul><p>如果 AI 每次都只选分数最高的那个词（比如永远选“好”），输出会非常稳定，但也容易变得<strong>模板化</strong>：句子没错，却缺少惊喜，像“复读机”。</p><p>于是，我们需要一种机制：在“稳妥”之外，给 AI 一点点“跳出常规”的空间——这就是温度登场的原因。</p><hr/><h2>二、温度到底做了什么？（神奇的蛋糕分法）</h2><p>在真正选词之前，模型会先把“分数”转换成“概率”，常见做法叫 <strong>Softmax</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525317" alt="" title=""/><br/>你可以把它想象成：</p><blockquote><strong>把一块蛋糕分给候选词：分数越高，分到的蛋糕越多，被选中的概率越大。</strong></blockquote><p>而<strong>温度</strong>，就像影响“怎么切蛋糕”的那把刀——它决定蛋糕分配得<strong>更偏心</strong>还是<strong>更平均</strong>。</p><h3>1）低温（T &lt; 1）：偏心切法（更保守）</h3><p>温度调低后，分配会变得更“极端”：<br/>第一名会拿走绝大多数蛋糕，其他词只剩零头。</p><ul><li><strong>结果</strong>：AI 更倾向选“最常见、最稳”的词</li><li><strong>体验</strong>：更严谨、更稳定，但也更容易“千篇一律”</li></ul><h3>2）高温（T &gt; 1）：均匀切法（更发散）</h3><p>温度调高后，蛋糕切得更平均：<br/>第一名仍然是大头，但第二、第三名也能分到明显份额。</p><ul><li><strong>结果</strong>：AI 更可能选到不那么“标准”的词</li><li><strong>体验</strong>：更有创意、更有变化，但也更容易跑题或胡编</li></ul><hr/><h2>三、温度怎么设置？（三个常见场景）</h2><p>可以把不同温度下的 AI，想象成三种不同“人格”。</p><h3>1）冰块模式（低温：0 ~ 0.3）</h3><ul><li><strong>像谁</strong>：严肃的科学家 / 数学老师</li><li><strong>适合</strong>：做数学题、写代码、严谨问答、总结归纳</li><li><strong>原因</strong>：这类任务追求确定性，“1+1=2”不需要创意</li></ul><h3>2）常温模式（中温：0.5 ~ 0.9）</h3><ul><li><strong>像谁</strong>：正常可靠的聊天伙伴</li><li><strong>适合</strong>：日常对话、写邮件、写周报、写解释说明</li><li><strong>原因</strong>：稳定之余也有一点自然变化，是最常用的平衡区间</li></ul><h3>3）火焰模式（高温：0.9 ~ 1.5 或更高）</h3><ul><li><strong>像谁</strong>：灵感爆棚的艺术家 / 头脑风暴搭子</li><li><strong>适合</strong>：写故事、写诗、创意发想、广告文案、脑洞类任务</li><li><strong>提醒</strong>：温度太高（例如 &gt;1.5）时，输出可能开始发散到不受控，甚至出现“看起来很像话但其实不太对”的内容</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525318" alt="" title="" loading="lazy"/></p><hr/><h2>四、补充：温度 vs Top-k / Top-p（它们到底有什么区别？）</h2><p>温度之外，你可能还见过两个常用的“采样参数”：<strong>Top-k</strong> 和 <strong>Top-p</strong>。它们和温度一样，都是在控制 AI 输出的随机性，但“动手的方式”不同。</p><p>你可以把它们理解成：<strong>温度在“调形状”，Top-k/Top-p 在“划范围”。</strong></p><h3>1）温度（Temperature）：调“整体概率分布”的陡峭程度</h3><ul><li><strong>温度低</strong>：概率分布更“尖”，第一名更容易被选中（更稳）</li><li><strong>温度高</strong>：概率分布更“平”，冷门词也更容易被抽到（更发散）</li></ul><p>👉 它不会删掉任何候选词，只是让“大家的概率差距”变大或变小。</p><h3>2）Top-k：只在“前 k 名”里抽</h3><p>Top-k 的规则很直白：</p><blockquote>只保留概率最高的 <strong>k 个候选词</strong>，其余一律不考虑，然后再在这 k 个里按概率抽。</blockquote><ul><li><strong>优点</strong>：简单、能防止特别离谱的词混进来</li><li><strong>缺点</strong>：k 是固定的——有时候候选词很集中，有时候很分散，固定 k 可能不够灵活</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525319" alt="" title="" loading="lazy"/></p><h3>3）Top-p（Nucleus Sampling）：只在“累计概率达到 p 的那一撮”里抽</h3><p>Top-p 更像“动态的 Top-k”：</p><blockquote>从最高概率开始往下加，直到累计概率达到 <strong>p</strong>（比如 0.9），只在这一小撮里抽。</blockquote><ul><li><strong>优点</strong>：更自适应：模型很确定时范围会自动变小；模型不确定时范围会自动变大</li><li><strong>缺点</strong>：需要理解“累计概率”的概念，但用起来通常更顺手</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525320" alt="" title="" loading="lazy"/></p><h3>怎么搭配最实用？</h3><p>很多实际系统里最常见的是：<strong>温度 + Top-p</strong></p><ul><li><strong>温度</strong>负责“敢不敢跳出最优解”</li><li><strong>Top-p</strong>负责“别跳得太离谱”</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525321" alt="" title="" loading="lazy"/></p><p>一句话记忆：</p><blockquote><strong>温度让你更有变化，Top-p/Top-k 帮你把变化圈在合理范围内。</strong></blockquote><hr/><h2>总结：掌握那个旋钮</h2><p><strong>温度不会让 AI 更聪明</strong>，它改变的是：AI 在“下一步选哪个词”时的<strong>胆量</strong>和<strong>随机性</strong>。</p><ul><li>想要更像“标准答案”？→ <strong>把温度调低</strong></li><li>想要更多惊喜和创意？→ <strong>把温度调高</strong></li></ul><p>下次你可以试试对 AI 说：</p><blockquote>“请把温度设为 1.2，给我讲一个更疯狂、更有画面感的故事。”</blockquote><p>看看它会不会带你去一趟意想不到的冒险。</p><p>本文由<a href="https://link.segmentfault.com/?enc=4BE2hBJL9ERnFowPc2xSug%3D%3D.b7tYCBoRRMPWsc1xdkZF%2FoIe8Pv8N2M65EVUglwbZVY%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Android 车机与 BLE 设备交互 philadelphia ]]></title>    <link>https://segmentfault.com/a/1190000047525180</link>    <guid>https://segmentfault.com/a/1190000047525180</guid>    <pubDate>2026-01-06 20:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Android 车机与 BLE 设备交互全链路实践指南</h2><p>从广播地址、配对绑定到隐私机制的完整理解</p><p>最近在开发车机系统与无屏 BLE 设备（比如智能冰箱）的连接功能，过程中遇到了一连串看似独立、实则紧密关联的问题：为什么扫描到的地址和配对时一样？RPA 地址到底能不能看到？解绑为什么没有公开 API？回连时直接用连接成功时保存的 MAC 行不行？ADB 怎么清空配对列表？</p><p>这些问题背后其实是一套完整的 BLE 安全与隐私机制。这篇文章把整个探索过程串起来，记录下验证过的方法和踩过的坑，希望能帮到正在做类似工作的你。</p><hr/><h3>一、设备广播地址：你以为的“MAC”可能不是真 MAC</h3><p>一开始我在连接成功后将设备的MAC保存到SP中，后续APP启动或者蓝牙开启后直接根据整个mac直接去连接设备，</p><p>大致流程如图：</p><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDj" alt="connect_process" title="connect_process"/></p><p>但是有的设备是OK的，有的设备不行，后来发现不行的设备是开启了BLE Privacy机制，无法直接根据连接时返回的mac直接连接</p><p>后来发现这涉及到 <strong>BLE（Bluetooth Low Energy）隐私保护机制</strong> 的核心设计 —— <strong>RPA（Resolvable Private Address，可解析私有地址）</strong></p><p>首先看下BLE设备的广播类型</p><h4><strong>BLE 广播地址的类型</strong></h4><p>根据 BLE 规范（Core Spec Vol 6, Part B, Section 1.3），广播包中的 <strong>AdvA（Advertiser Address）</strong> 可以是以下之一：</p><table><thead><tr><th>地址类型</th><th>是否可变</th><th>是否可被解析</th><th>说明</th></tr></thead><tbody><tr><td><strong>Public Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>厂商烧录的 MAC</td></tr><tr><td><strong>Static Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>设备自定义的静态随机地址</td></tr><tr><td><strong>Resolvable Private Address (RPA)</strong></td><td>✅ 动态（如每15分钟换）</td><td>✅ 仅对有 IRK 的设备</td><td>隐私保护，可被配对设备解析</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>✅ 动态</td><td>❌ 否</td><td>完全匿名，无法追踪也无法连接（通常用于 beacon）</td></tr></tbody></table><blockquote>📌 大多数支持配对的智能设备（如你的冰箱）会使用 <strong>RPA</strong> 来广播，以保护用户隐私。</blockquote><h3>传统 MAC 地址的问题</h3><p>早期 BLE 设备使用 <strong>静态公开地址（Public Static Address）</strong>，比如：</p><pre><code>AA:BB:CC:11:22:33</code></pre><p>这个地址是固定的、全球唯一（理论上），但也带来严重隐私问题：</p><blockquote>🕵️‍♂️ 攻击者可以在商场、地铁等公共场所通过扫描 BLE 广播包，追踪某个设备（比如你的手机或冰箱）的行踪。</blockquote><p>为了解决这个问题，BLE 4.0 引入了 <strong>Privacy Feature（隐私特性）</strong>，允许设备使用 RPA <strong>随机变化的地址</strong>，而不是固定 MAC。</p><hr/><h3>🛡️ RPA（Resolvable Private Address）是什么？</h3><p>RPA 是一种 <strong>动态变化但可被“信任设备”识别</strong> 的地址机制。</p><h4>工作原理简述：</h4><ol><li><strong>配对（Bonding）时</strong>，双方交换一个密钥：<strong>IRK（Identity Resolving Key）</strong>。</li><li>此后，设备会定期（如每 15 分钟）生成一个新的 <strong>随机地址（RPA）</strong>，该地址由 IRK + 随机数加密生成。</li><li>只有拥有相同 IRK 的设备（即已配对设备）才能 <strong>“解析”这个 RPA，确认它来自同一个物理设备</strong>。</li><li>对未配对的第三方来说，看到的只是一个不断变化的随机地址，无法追踪。</li></ol><p>✅ <strong>优点</strong>：既保护隐私，又不影响已配对设备之间的通信</p><hr/><h3>什么是Identify Address</h3><ul><li><p>Identity Address</p><p>是设备在配对（Pairing + Bonding）过程中交换的“真实身份”，格式为：</p><ul><li>Public Address（如厂商烧录的 MAC）</li><li>或 Static Random Address（由设备制造商设定，固定不变）</li></ul></li><li>这个地址 <strong>在设备生命周期内是固定的</strong>，也是 Android 系统在 <code>BluetoothDevice.getAddress()</code> 中返回的值（对于 bonded 设备）。</li><li><strong>IRK（Identity Resolving Key）就是用来将 RPA 映射回这个 Identity Address 的。</strong></li></ul><hr/><h3>📱 Android 如何处理 RPA？</h3><ul><li><p>当你和一个支持隐私特性的 BLE 设备（如现代智能冰箱）完成 <strong>配对（bonding）</strong> 后：</p><ul><li>Android 系统会 <strong>自动保存该设备的 IRK</strong>；</li><li>即使冰箱下次广播的是一个全新的 RPA（比如 <code>D4:E5:F6:77:88:99</code>），Android 也能通过 IRK 识别出：“这是之前配对过的那台冰箱”。</li></ul></li><li><p>此时，你在代码中调用：</p><pre><code>BluetoothAdapter.getDefaultAdapter().getBondedDevices()</code></pre><p>返回的 <code>BluetoothDevice</code> 对象的 <code>.getAddress()</code> <strong>始终是配对时的“身份地址”（Identity Address）</strong>，通常是 Public 或 Static 地址（如 <code>AA:BB:CC:11:22:33</code>），<strong>而不是当前广播的 RPA</strong>。</p></li></ul><blockquote>✅ 所以：<strong>系统内部已经帮你完成了 RPA → Identity Address 的映射</strong>。</blockquote><hr/><p>这就回到了为什么直接根据扫描到的mac地址直接回连设备会失败的问题了</p><h3>❌ 为什么不要直接用存储的 MAC 字符串调用 <code>getRemoteDevice(mac)</code>？</h3><p>假设你把扫描到的 MAC（如 <code>"AA:BB:CC:11:22:33"</code>）存到 SharedPreferences，下次直接：</p><pre><code>String savedMac = prefs.getString("fridge_mac", null);
BluetoothDevice dev = adapter.getRemoteDevice(savedMac); 
dev.connectGatt(...);</code></pre><ol><li><code>getRemoteDevice(mac)</code> 仅根据地址字符串返回一个设备引用，它不保证能访问到该设备的绑定上下文（如 IRK）。如果传入的地址不是已配对设备的 Iden<code>t</code>ity Address（例如是一个 RPA），即使物理设备已绑定，系统也无法自动解析隐私地址</li><li>如果此时冰箱正在使用 RPA（比如广播地址是 <code>D4:E5:F6:77:88:99</code>），而你传入的是旧的 Identity Address（<code>AA:BB:CC:...</code>）；</li><li>Android <strong>不会自动用 IRK 去解析或关联这个 RPA</strong>，因为 <code>getRemoteDevice()</code> 不知道这个设备是否已配对；</li><li>结果：<strong>连接失败（GATT ERROR 133 或 timeout）</strong>，即使物理设备就在旁边！</li></ol><blockquote>💡 换句话说：<code>getRemoteDevice()</code> 绕过了系统的 bonding 数据库和 IRK 解析机制。</blockquote><hr/><h3>✅ 正确做法：从 <code>getBondedDevices()</code> 中查找</h3><pre><code>String savedIdentityAddress = "AA:BB:CC:11:22:33"; // 这是你配对时记录的身份地址

BluetoothAdapter adapter = BluetoothAdapter.getDefaultAdapter();
for (BluetoothDevice device : adapter.getBondedDevices()) {
    if (device.getAddress().equals(savedIdentityAddress)) {
        // ✅ 这个 device 对象是系统管理的 bonded 设备
        // 即使冰箱当前用 RPA 广播，系统也会自动解析并建立连接
        device.connectGatt(context, false, gattCallback);
        break;
    }
}</code></pre><p>这样做的好处：</p><ul><li>利用了 Android 内置的 <strong>IRK 解析能力</strong>；</li><li>无论冰箱当前使用什么 RPA，系统都能正确路由到物理设备；</li><li>连接成功率高，符合 BLE 规范。</li></ul><hr/><h3>🔧 补充建议</h3><ul><li>要记录配对时的mac地址而不是扫描到的mac地址，因为扫描的到mac可能是PRA，但是绑定时的mac一定是Identify Address</li><li>在配对完成后，<strong>记录的是 <code>device.getAddress()</code>（即 Identity Address）</strong>，这个地址在 bonding 生命周期内是稳定的；</li><li>不要尝试自己解析 RPA（除非你实现完整的 BLE Host 层，不推荐）；</li><li>如果目标设备 <strong>不支持 Privacy（即始终用 Public Address）</strong>，那么 <code>getRemoteDevice()</code> 也能工作，但为了兼容性和未来升级，仍建议走 bonded devices 路径。</li></ul><hr/><h4>✅ 总结</h4><table><thead><tr><th>方式</th><th>是否推荐</th><th>原因</th></tr></thead><tbody><tr><td><code>getRemoteDevice(savedMac)</code></td><td>❌ 不推荐</td><td>无法利用 IRK 解析 RPA，连接可能失败</td></tr><tr><td>从 <code>getBondedDevices()</code> 查找匹配地址</td><td>✅ 强烈推荐</td><td>系统自动处理 RPA，连接可靠</td></tr></tbody></table><p>所以，<strong>永远优先使用系统提供的 bonded device 对象来发起连接</strong>，而不是自己构造设备对象。这不仅是最佳实践，也是应对现代 BLE 隐私机制的必要手段。</p><p><strong>Android 在设备绑定后，所有 API 返回的都是 Identity Address（身份地址），而不是设备当前广播的地址</strong>。</p><p>BLE 设备可以广播四种地址：</p><ul><li><strong>Public Address</strong>：芯片的固定 MAC；</li><li><strong>Static Random Address</strong>：开机生成、运行期间不变的随机地址；</li><li><strong>Non-Resolvable Private Address (NRPA)</strong>：频繁变化、无法追踪的临时地址；</li><li><strong>Resolvable Private Address (RPA)</strong>：频繁变化，但持有 IRK 的设备能解析回 Identity Address。</li></ul><p>真正的 Privacy 机制 = <strong>使用 RPA 广播 + IRK 解析</strong>。</p><p>当你在车机上调用 <code>device.getAddress()</code>，如果设备已绑定，Android 会自动用 IRK 把 RPA “翻译” 成 Identity Address 再返回给你。所以你永远看不到那个变化的 RPA —— 这不是 bug，而是 Privacy 正常工作的表现。</p><blockquote>✅ 验证方法：用手机装 nRF Connect，在<strong>未绑定状态</strong>下扫描设备，隔 15 分钟看地址是否变化。如果变了，说明 Privacy 生效了。</blockquote><hr/><h3>二、RPA 地址能看到吗？怎么获取原始广播地址？</h3><p>既然系统把 RPA 隐藏了，那我们还能不能拿到真实的广播地址？</p><p>答案是：<strong>只有在未绑定状态下才可能看到</strong>。</p><p>当你调用 <code>BluetoothLeScanner.startScan()</code>，如果设备还没配对，且它广播的是 RPA，那么 <code>ScanResult.device.getAddress()</code> 返回的就是这个原始 RPA（比如 <code>D3:A1:F5:09:88:77</code>）。但一旦你完成绑定，下次再扫，系统就会直接给你 Identity Address。</p><p>所以，<strong>不要试图在绑定后获取 RPA</strong>——你不需要它。Identity Address 才是稳定的设备标识，RPA 只是空中传输的“马甲”。</p><p>如果你是在调试固件，建议用 nRF Connect 或蓝牙嗅探器抓包；如果是开发车机 App，请完全忽略 RPA 的存在，只认 <code>getBondedDevices()</code> 里的地址。</p><hr/><h3>三、回连时直接用保存的 MAC 地址行不行？</h3><p>早期我们图省事，配对成功后把设备地址存下来，下次启动直接用：</p><pre><code class="kotlin">val device = adapter.getRemoteDevice(savedMac)
device.connectGatt(...)</code></pre><p>结果某天测试新固件（启用了 RPA）时，连接直接超时失败。</p><p>原因很简单：<code>getRemoteDevice()</code> 创建的是一个“裸设备对象”，它没有 IRK，也不知道这个地址对应的是谁。而冰箱此刻广播的是 RPA，根本不在 <code>savedMac</code> 这个地址上。</p><p>正确做法是：</p><pre><code class="kotlin">val device = adapter.bondedDevices.find { it.address == savedMac }
device?.connectGatt(...)</code></pre><p>因为 <code>bondedDevices</code> 里的设备对象带着完整的绑定上下文（包括 IRK），系统能自动把 RPA 解析出来并建立连接。</p><blockquote>📌 记住：<strong>地址字符串相同 ≠ 设备对象等价</strong>。安全上下文才是关键。</blockquote><hr/><h4>2. <strong>Android 如何处理这个地址？</strong></h4><ul><li>在 <code>onLeScan()</code> 中，Android <strong>直接把广播包里的 AdvA 字段原样封装成 <code>BluetoothDevice</code> 对象的地址</strong>；</li><li>此时系统 <strong>还不知道这个设备是否已配对</strong>，也没有尝试用 IRK 去解析它（因为还没建立 bonding 上下文）；</li><li>所以：<strong><code>device.getAddress()</code> 就是原始广播地址（raw advertising address）</strong>。</li></ul><p>✅ 举例：</p><ul><li>冰箱的 Identity Address 是 <code>AA:BB:CC:11:22:33</code>（Public）；</li><li>当前广播使用 RPA：<code>D4:E5:F6:77:88:99</code>；</li><li>你在 <code>onLeScan()</code> 中拿到的 <code>device.getAddress()</code> 就是 <code>"D4:E5:F6:77:88:99"</code>；</li><li>即使你之前已经和这台冰箱配对过，<strong>扫描回调仍然返回 RPA</strong>，因为这是物理层看到的内容。</li></ul><blockquote>⚠️ 这就是为什么不能在扫描阶段存储这个地址作为设备唯一标识！</blockquote><hr/><h4>3. <strong>那系统怎么知道这是“老朋友”？</strong></h4><ul><li><p>当你调用</p><pre><code>device.createBond() 
或
device.connectGatt(...)</code></pre><p>时，Android 会：</p><ol><li>检查本地是否有该 <strong>Identity Address 对应的 IRK</strong>（即是否已配对）；</li><li>如果有，就尝试用 IRK 解析当前 RPA；</li><li>如果解析成功（RPA 能还原出已知 Identity Address），就走快速重连流程（无需重新配对）；</li><li>连接成功后，<code>BluetoothDevice.getAddress()</code> 在后续 API 调用中（如 GATT 回调、bonded devices 列表）会返回 <strong>Identity Address</strong>。</li></ol></li><li><h3>public Address和identify Address 是一回事吗?</h3></li><li><blockquote><strong>Public Address 是 Identity Address 的一种，但 Identity Address 不一定是 Public Address。</strong></blockquote><p>换句话说：</p><ul><li><strong>Identity Address（身份地址）是一个逻辑概念</strong>，用于唯一标识一个 BLE 设备；</li><li><p>它可以是以下两种之一：</p><ol><li><strong>Public Device Address</strong>（公开地址，即传统 MAC 地址）</li><li><strong>Static Random Address</strong>（静态随机地址）</li></ol></li></ul><p>所以：<br/> ✅ 所有 Public Address 都是 Identity Address，<br/> ❌ 但不是所有 Identity Address 都是 Public Address。</p><hr/><h4>1. <strong>什么是 Identity Address（身份地址）？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 核心规范）</strong>：</p><blockquote><p>The <strong>Identity Address</strong> is the address used to identify a device during the pairing and bonding process. It is either:</p><ul><li>A <strong>Public Device Address</strong>, or</li><li>A <strong>Static Random Address</strong></li></ul></blockquote><p>这个地址在设备的整个生命周期中是 <strong>固定不变的</strong>，并且会和 <strong>IRK（Identity Resolving Key）</strong> 一起在配对时交换，用于后续解析 RPA（Resolvable Private Address）。</p><hr/><h4>2. <strong>Public Device Address（公开地址）</strong></h4><ul><li>就是我们熟悉的 <strong>48-bit IEEE MAC 地址</strong>，如 <code>AA:BB:CC:11:22:33</code>；</li><li>由厂商烧录，全球唯一（理论上）；</li><li>地址的 <strong>最高有效位（MSB）为 0</strong>（即“公共地址”标志）；</li><li><strong>需要向 IEEE（通过 SIG 或直接）购买</strong></li><li>示例：<code>D0:CF:5E:xx:xx:xx</code>（很多手机/模块使用）。</li></ul><p>✅ 特点：固定、可识别、无隐私保护。</p><hr/><h4>3. <strong>Static Random Address（静态随机地址）</strong></h4><ul><li>由设备制造商或开发者设定的一个 <strong>随机生成但永不改变</strong> 的地址；</li><li><p>必须满足：</p><ul><li>最高两位为 <code>11</code>（表示是静态随机地址）；</li><li>不能是全 0 或全 1；</li></ul></li><li>示例：<code>DE:AD:BE:EF:CA:FE</code>（只要符合格式且固定即可）。</li></ul><p>✅ 特点：固定、不依赖 IEEE 分配、有一定匿名性，但仍可作为身份标识。</p><blockquote>📌 很多 IoT 设备（如低成本 BLE 模块）没有 Public Address，就用 Static Random Address 作为 Identity Address。</blockquote><hr/><h4>4. <strong>为什么需要区分？</strong></h4><p>因为 BLE 隐私机制（RPA）依赖于 <strong>Identity Address + IRK</strong> 的组合：</p><ul><li>当设备启用隐私功能时，它会用 IRK 生成 RPA 来广播；</li><li>配对设备收到 RPA 后，用本地存储的 IRK 尝试还原出 <strong>Identity Address</strong>；</li><li>如果匹配成功，就知道“这是之前配对过的那台设备”。</li></ul><p>所以，无论 Identity Address 是 Public 还是 Static Random，只要它是固定的，就能作为“身份锚点”。</p><hr/></li></ul><blockquote>💡 实际开发中，你不需要关心它是 Public 还是 Static Random —— 只需知道：<strong>这是该设备的唯一身份标识，配对后稳定不变，应该存储它。</strong></blockquote><p>------</p><p>## ✅ 总结表</p><table><thead><tr><th>概念</th><th>是否固定</th><th>是否用于配对</th><th>是否可用于长期标识</th><th>备注</th></tr></thead><tbody><tr><td><strong>Public Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>传统 MAC，IEEE 分配</td></tr><tr><td><strong>Static Random Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>设备自定义，高位为 <code>11</code></td></tr><tr><td><strong>Identity Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>= Public 或 Static Random</td></tr><tr><td><strong>RPA（Resolvable Private Address）</strong></td><td>❌ 否（动态）</td><td>❌ 否</td><td>❌ 否</td><td>用于广播，保护隐私</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>❌ 否</td><td>❌ 否</td><td>❌ 否</td><td>完全匿名，通常不可连接</td></tr></tbody></table><p>------</p><p>### 🎯 开发建议</p><ul><li><strong>不要尝试解析地址类型</strong>，只需在 <code>BOND_BONDED</code> 时存储 <code>device.getAddress()</code>；</li><li>这个地址就是系统认可的 <strong>Identity Address</strong>，无论底层是 Public 还是 Static Random；</li><li>后续通过 <code>getBondedDevices()</code> 匹配该地址即可可靠连接。</li></ul><h3>四、配对弹窗是怎么来的？</h3><p>我们从来没调 <code>createBond()</code>，为什么也会弹出系统配对窗口？</p><p>后来发现，<strong>触发配对的不是你的代码，而是 GATT 特征的安全属性</strong>。</p><p>如果你的冰箱声明某个特征需要“认证后才能读写”（比如设置了 <code>AUTHEN</code> 权限），而当前连接还没加密，那 Android 在收到 <code>Insufficient Authentication</code> 错误后，会自动启动配对流程。</p><p>所以，配对弹窗其实是系统在帮你补安全课。你只需要在配对成功后重试 GATT 操作即可。</p><hr/><h3>五、配对流程</h3><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDV" alt="tongyi-mermaid-2026-01-06-194500.png" title="tongyi-mermaid-2026-01-06-194500.png" loading="lazy"/></p><p><strong>在标准 BLE 通信模型中，配对（Pairing）流程是由 Central（车机）发起的，但实际触发时机往往由 Peripheral（BLE设备，如冰箱）的安全需求间接驱动</strong></p><h4><strong>一、协议层面：谁“发起”配对？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 协议规范）</strong>：</p><ul><li><strong>Central（主设备，如车机）</strong> 负责发送 <code>Pairing Request</code>；</li><li><strong>Peripheral（从设备，如冰箱）</strong> 回复 <code>Pairing Response</code>；</li><li>后续密钥交换、确认值计算等均由 Central 主导。</li></ul><p>✅ 所以从<strong>协议动作</strong>看，<strong>配对是由 Central（车机）主动发起的</strong>。</p><hr/><h4><strong>二、应用层面：谁“触发”配对？</strong></h4><p>虽然 Central 发起配对请求，但它通常<strong>不是凭空发起</strong>，而是因为：</p><blockquote><strong>Peripheral 在 GATT 层拒绝了未加密的访问请求，从而迫使 Central 启动配对。</strong></blockquote><p>典型流程如下：</p><ol><li>车机（Central）连接冰箱（Peripheral）；</li><li>车机尝试读取一个被标记为 <strong>“需要认证”</strong> 的特征（例如 <code>read authen</code>）；</li><li>冰箱返回错误：<code>Insufficient Authentication (0x05)</code>；</li><li><strong>Android 系统检测到该错误，自动调用 <code>createBond()</code> 并发送 <code>Pairing Request</code></strong>；</li><li>配对流程启动，弹出系统弹窗或走 Just Works 模式。</li></ol><p>✅ 所以从<strong>触发原因</strong>看，<strong>是 Peripheral 的安全策略“迫使” Central 发起配对</strong>。</p><p>既然是车机发起的配对请求，那车机-Android系统怎么还能收到配对请求广播呢？</p><p>车机（App）收到的 <code>ACTION_PAIRING_REQUEST</code> 广播，<strong>不是来自远端设备的“请求”</strong>，而是 <strong>Android 系统在自己发起配对前，向 App 发出的一个“协商/干预”通知</strong>。</p><p>这个广播的目的是：让 App 有机会干预配对行为</p><p>例如：</p><ul><li>自动确认 Just Works 配对（免弹窗）；</li><li>填入预共享的 PIN 码；</li><li>拒绝某些设备的配对。</li></ul><h4>Pairing 和 Bonding 区别？</h4><p>这两个概念经常混用，但职责完全不同：</p><ul><li><strong>Pairing（配对）</strong>：协商密钥的过程（生成 LTK、IRK 等），确保本次连接加密；</li><li><strong>Bonding（绑定）</strong>：把配对成果（密钥 + 身份）存到本地，供以后复用。</li></ul><p>你可以只配对不绑定（每次连都重新确认），但不能只绑定不配对。在 Android 里，配对成功默认就会绑定，设备进入 <code>getBondedDevices()</code> 列表。</p><hr/><h3>六、怎么用代码解绑设备？</h3><p>Android 没有公开 <code>removeBond()</code> 方法，但它确实存在，只是被标为 <code>@hide</code>。通过反射调用是行业通用做法：</p><pre><code class="kotlin">fun removeBond(device: BluetoothDevice): Boolean {
    return try {
        val method = device::class.java.getMethod("removeBond")
        method.invoke(device) as Boolean
    } catch (e: Exception) {
        false
    }
}</code></pre><p>注意：</p><ul><li>需要 <code>BLUETOOTH_ADMIN</code> 权限；</li><li>解绑是异步的，必须监听 <code>ACTION_BOND_STATE_CHANGED</code> 广播，等状态变成 <code>BOND_NONE</code>；</li><li>最好加兜底：如果反射失败，引导用户去系统设置手动解绑。</li></ul><p>至于为什么不公开？主要是怕 App 滥用——比如偷偷删掉用户的耳机配对。但又不能完全禁掉，毕竟车机、IoT 设备确实需要程序化解绑。所以成了“能用，但不鼓励”的状态。</p><hr/><h3>总结：几个关键原则</h3><ol><li><strong>地址不变是正常的</strong>：那是 Identity Address，不是广播地址；</li><li><strong>RPA 不需要你关心</strong>：系统会自动处理，你只认 bonded 列表；</li><li><strong>回连必须从 <code>getBondedDevices()</code> 取设备</strong>，否则 Privacy 一开就断连；</li><li><strong>配对由 GATT 安全需求驱动</strong>，不是靠你调不调 <code>createBond()</code>；</li><li><strong>解绑用反射没问题</strong>，但要做好兼容和用户引导；</li><li><strong>Privacy 是好东西</strong>，但前提是两端都正确实现——设备要真启用 RPA，车机要会解析。</li></ol><p>BLE 看似简单，但安全和隐私细节很多。理解这套机制，才能做出既好用又安全的产品。希望这篇总结能帮你少走点弯路。</p>]]></description></item><item>    <title><![CDATA[AI智能体元年：IT服务管理行业的拐点已至 ITIL先锋论坛 ]]></title>    <link>https://segmentfault.com/a/1190000047525207</link>    <guid>https://segmentfault.com/a/1190000047525207</guid>    <pubDate>2026-01-06 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>宏观视角下的行业变革信号</strong></p><p>2025年12月13日，广州天河区的一场百人规模Meetup，或许将成为IT服务管理行业转折的标志性事件。这不是一场普通的技术分享会,而是一次行业集体焦虑的集中释放,更是一场关于未来方向的深度探讨。</p><p>当100余位来自大湾区的IT精英齐聚一堂,当四位深耕行业多年的专家倾囊相授,当"AI智能体"这个概念从PPT走向实战演练,我们看到的不仅是技术的演进,更是一个行业在时代巨变前夜的集体转身。</p><p>从更宏观的视角观察,这场活动折射出IT服务管理行业正在经历的三大深刻变革:技术范式的迁移、人才结构的重组、商业模式的重构。而这三大变革的交汇点,正是AI智能体技术的大规模应用。</p><p><img width="723" height="419" referrerpolicy="no-referrer" src="/img/bVdnvsC" alt="" title=""/></p><p><strong>行业痛点:67%的从业者还未真正触碰AI</strong></p><p>长河在现场的调研数据揭示了一个令人警醒的现实:使用AI超过100小时的IT从业者仅占33%,这意味着超过三分之二的行业从业者仍处于AI技术的观望期。</p><p><strong>这个数据背后隐藏着更深层的行业问题:</strong><br/><strong>第一,认知滞后与技术加速的矛盾。</strong><br/>当大语言模型以月为单位迭代升级,当AI智能体技术从概念走向落地,大部分IT从业者的认知仍停留在"AI是高级搜索引擎"的层面。这种认知与现实的错位,正在成为行业人才发展的最大障碍。<br/><strong>第二,技能结构与市场需求的错配。</strong><br/>传统IT服务管理强调的是系统运维、故障处理、流程优化等执行层面的技能。但AI时代需要的是提示词工程、智能体开发、人机协同设计等新型能力。这种技能代际的断层,导致大量经验丰富的从业者面临转型困境。<br/><strong>第三,投入产出与风险收益的博弈。</strong><br/>企业层面对AI技术持谨慎态度:投入巨大但效果未知,试点成功但推广困难,短期收益不明显但长期不投入又可能落后。这种矛盾心态导致行业整体的技术应用进程缓慢。</p><p>从行业发展周期理论看,IT服务管理正处于从"成熟期"向"变革期"过渡的关键节点。这个节点的特征是:传统业务模式增长乏力,新兴技术尚未形成主流,行业参与者普遍焦虑且方向不明。广州这场Meetup的火爆,恰恰说明了行业对方向指引的强烈渴求。</p><p><strong>技术演进:从自动化到智能化的质变</strong></p><p>丁振兴展示的五层智能体架构,标志着IT运维从"自动化"向"智能化"的质的飞跃。这不仅是技术层面的进步,更代表着行业对运维本质认知的深化。<br/>传统自动化解决的是"怎么做"的问题——给定明确的规则和流程,系统按部就班执行。但这种模式的局限在于:面对复杂多变的IT环境,不可能为每一种场景都预设规则。<br/>智能化解决的是"如何判断"和"如何学习"的问题——系统通过感知环境、回忆经验、规划策略、执行行动、持续优化,形成闭环。这种能力的本质是:将运维专家的认知过程数字化。</p><p><strong>从行业发展趋势看,智能运维市场正在经历三个阶段:</strong><br/>1.0阶段:工具集成时代(2010-2018)。市场以监控工具、自动化脚本、ITIL流程管理为主,各厂商提供独立产品,集成度低。<br/>2.0阶段:平台化时代(2018-2023)。市场出现一体化运维平台,打通监控、告警、处置全流程,但仍以规则驱动为主。<br/>3.0阶段:智能化时代(2024-)。市场进入AI驱动阶段,平台具备自主学习、智能决策能力,从"被动响应"转向"主动预防"。</p><p>丁振兴提到的"80%陷阱"是行业当前阶段的真实写照。这不是技术的失败,而是技术成熟度的客观反映。从Gartner技术成熟度曲线看,AI运维正处于"期望膨胀期"向"泡沫破裂期"过渡的阶段。行业需要的是理性认知,而非盲目追捧或全盘否定。</p><p>值得注意的是,乐维软件支持500+厂商、8000+设备型号、100000+指标体系的技术积累,揭示了智能运维的行业门槛:这不是一个可以靠短期投入快速突破的领域,而是需要长期技术沉淀和数据积累的系统工程。这也解释了为什么该领域至今仍是少数头部厂商主导,新进入者难以撼动的市场格局。</p><p><strong>商业模式:从人力密集到智能体驱动</strong></p><p>罗小军展示的企业业务智能体矩阵,预示着IT服务行业商业模式的根本性变革。<br/>传统IT服务的商业模式是人力密集型的:企业需要大量人员提供技术支持、系统运维、项目实施等服务,收入与人力规模直接相关。这种模式的天花板很明显:利润率受制于人力成本,规模扩张受制于人才供给。<br/>智能体驱动的商业模式是技术密集型的:企业投入研发构建智能体平台,通过智能体提供标准化服务,收入与技术能力相关而非人力规模。这种模式的想象空间更大:边际成本递减,规模效应明显,可以实现指数级增长。</p><p><strong>从行业竞争格局看,这种模式转变将带来三个层面的影响:</strong><br/>企业层面:马太效应加剧。拥有技术能力、数据积累、资金实力的头部企业将加速智能体布局,中小企业面临技术门槛高、投入产出不确定的困境,行业集中度可能进一步提升。<br/>项目层面:交付模式重构。传统的"人月成本"定价模式将被"按效果付费"模式取代。智能体处理的任务越多,单位成本越低,但前期研发投入巨大。这要求企业具备长期投入能力和风险承受能力。<br/>人才层面:需求结构变化。对执行型人才的需求下降,对架构型、创新型、复合型人才的需求上升。初级工程师岗位减少,高级架构师岗位增加,行业人才结构呈现"哑铃型"。<br/>罗小军提到的"方案撰写效率提升60倍"案例,在行业引发了广泛讨论。支持者认为这代表了生产力的革命性提升,质疑者认为这种极端案例不具普遍性。<br/>从行业实践看,效率提升的真实情况可能是:对于高度结构化、模板化的工作,效率提升可达10-50倍;对于需要创造性、判断力的工作,效率提升可能只有1.5-3倍。关键是要识别哪些工作适合用AI,哪些工作仍需人工主导。<br/>更深层的问题是:当效率大幅提升后,市场需求能否同步增长?如果需求相对固定,效率提升的结果就是人力需求下降。这是行业必须直面的结构性挑战。</p><p><strong>数据集成:老问题遇上新技术</strong></p><p>王晨光提出的集成中台方案,触及了企业数字化转型的核心痛点。系统孤岛、数据沉睡、重复劳动——这些问题存在了十几年,为何至今未能解决?<br/>从技术演进史看,每隔几年就会出现号称能解决集成问题的新技术:</p><ul><li>2000年代:企业服务总线(ESB)承诺统一集成</li><li>2010年代:微服务架构承诺松耦合集成</li><li>2020年代:集成中台+AI承诺智能化集成<br/>技术在进步,但问题仍在。根本原因在于:集成问题的本质不是技术问题,而是组织问题。<br/>不同系统背后是不同部门,不同部门有不同利益诉求。数据打通意味着权力边界模糊,流程优化意味着责任重新划分。这些组织层面的阻力,远大于技术层面的难度。</li></ul><p>AI在集成方案中的真正价值,不在于技术实现的突破,而在于降低了使用门槛。当业务人员可以用自然语言查询数据,不再依赖IT部门编写SQL,数据的流动就更加顺畅。当数据异常可以被AI自动识别和修复,数据治理的成本就大幅下降。</p><p><strong>从行业发展趋势看,集成中台市场正在从"项目制"向"产品制"转变:</strong><br/>项目制时代:每个企业的集成需求都不同,需要大量定制开发,交付周期长、成本高、可复用性低。<br/>产品制时代:通过零代码配置、智能适配、自学习优化,大部分集成场景可以通过标准产品实现,只有少数个性化需求才需要定制。<br/>这种转变的商业意义在于:集成服务从"一次性项目收入"变为"持续性订阅收入",从"劳动密集"变为"技术密集",商业模式更加健康。<br/>但挑战在于:标准产品能否真正满足企业的个性化需求?零代码配置的灵活性是否足够?AI的智能化水平能否支撑复杂场景?这些问题的答案,将决定集成中台市场的未来格局。</p><p><strong>人才市场:30%-50%岗位影响的深层解读</strong><br/>圆桌讨论中,专家们给出的"未来3-5年AI将影响30%-50%岗位"判断,在行业引发了强烈反响。这个数字是危言耸听还是客观预测?<br/>从劳动经济学角度分析,技术对就业的影响包含三个效应:<br/>替代效应:AI直接替代人工完成某些任务,导致岗位需求下降。这在重复性高、规则明确的岗位上表现明显,如初级运维工程师、基础开发人员、文档撰写人员。<br/>互补效应:AI提升人工效率,使得同样人力可以完成更多工作,进而刺激需求增长。这在咨询、架构设计、创新研发等岗位上表现明显。<br/>创造效应:AI催生新岗位、新业务、新行业,创造就业机会。如AI训练师、提示词工程师、智能体架构师等新兴岗位。</p><p>IT服务管理行业的现实情况是:替代效应在短期内更显著,创造效应在长期才能体现。这就导致了一个过渡期的阵痛:旧岗位快速消失,新岗位缓慢出现,人才供需出现结构性错配。</p><p><strong>从行业数据看,这种影响已经开始显现:</strong><br/>招聘需求变化:2024年初级运维工程师岗位需求同比下降15%,AI相关岗位需求同比上升60%。但绝对数量上,减少的岗位远多于新增的岗位。<br/>薪资结构变化:掌握AI技能的工程师薪资溢价20%-40%,传统技能工程师薪资增长停滞甚至下降。行业内部的薪资分化加剧。<br/>年龄结构变化:35岁以上的从业者转型难度更大,面临的就业压力更明显。年轻从业者因学习能力强、心态开放而适应更快。<br/>长河提出的"六个月转型路线图",在行业引发了两极分化的评价。乐观者认为这是可行的快速转型方案,悲观者认为这过于理想化。<br/>从行业人才培养实践看,六个月确实可以完成从"不懂AI"到"会用AI工具"的跨越,但要成为真正的"AI架构师",可能需要1-2年的持续实践。关键在于:</p><ol><li>明确"转型"的定义。是掌握基本工具使用,还是具备架构设计能力,还是能独立交付项目?不同层级的要求,时间投入差异巨大。</li><li>识别个人的起点。有编程基础的工程师转型更快,纯运维背景的从业者需要补充更多基础知识。</li><li>找到合适的路径。自学、培训、项目实践各有优劣,需要根据个人情况选择。</li></ol><p><strong>市场格局:巨头布局与创业机会并存</strong><br/>从更宏观的市场竞争格局看,AI智能体在IT服务管理领域的应用,正在重塑行业的竞争版图。<br/>巨头企业的布局策略:<br/>国际厂商如IBM、微软、ServiceNow,国内厂商如华为、阿里云、腾讯云,都在加速AI与IT服务管理的融合。它们的优势在于:技术积累深厚、数据资源丰富、客户基础广泛、资金实力雄厚。<br/>但巨头的劣势也很明显:组织庞大决策慢、产品标准化难以满足个性需求、对细分场景的理解不够深入。<br/>创业公司的机会空间:<br/>像乐维软件这样的专业厂商,像猛犸世纪这样的创新企业,在垂直领域、细分场景、特定行业仍有很大机会。它们的优势在于:对客户需求理解深刻、产品迭代速度快、服务响应及时、性价比高。<br/>从行业发展规律看,技术变革期往往是市场格局重塑的窗口期。那些能抓住新技术、切中真需求、建立壁垒的企业,有可能实现弯道超车。</p><p><strong>值得关注的几个趋势:</strong></p><ol><li>垂直化深耕:不追求大而全,而是在某个细分领域(如金融、医疗、制造)做深做透,建立行业壁垒。</li><li>平台生态化:不只是提供工具,而是构建开放平台,让合作伙伴、客户都能参与智能体开发,形成生态效应。</li><li>服务订阅化:从一次性项目收入转向持续订阅收入,提高客户粘性和企业估值。</li><li>开源社区化:通过开源部分核心技术,吸引开发者社区,形成技术影响力和人才聚集效应。</li></ol><p><strong>政策环境:监管与发展的平衡</strong><br/>AI技术的快速发展,也引发了监管层面的关注。虽然本次Meetup未直接涉及政策话题,但这是行业发展不可回避的外部环境。<br/>国家层面的政策导向:<br/>2023年《生成式人工智能服务管理暂行办法》出台,对AI应用提出了明确要求。2024年各部委密集发布AI相关政策,鼓励创新应用的同时,也强化了安全监管。<br/>对IT服务管理行业而言,政策影响主要体现在:<br/>数据安全:智能体训练和运行需要大量数据,如何确保数据不泄露、不滥用,是合规的首要问题。<br/>算法透明:AI决策过程的可解释性要求,对智能运维、智能诊断等应用提出了挑战。<br/>责任界定:当AI做出错误决策导致系统故障,责任如何划分?这涉及法律和保险层面的安排。<br/>行业自律:各行业协会正在制定AI应用的行业标准和最佳实践,参与标准制定将成为企业的竞争优势。<br/>从国际经验看,监管政策对行业发展是把"双刃剑":过严会抑制创新,过松会带来风险。找到平衡点需要监管部门、行业企业、技术专家的共同努力。</p><p><strong>未来展望:三年内的行业图景</strong><br/>基于当前趋势,我们可以对未来3年IT服务管理行业的发展做出以下预判:<br/>2025年:试点探索期</p><ul><li>大型企业启动AI智能体试点项目,聚焦高价值场景</li><li>专业厂商推出成熟度更高的智能体产品</li><li>行业培训和认证体系逐步建立</li><li>初级岗位需求开始明显下降<br/>2026年:规模应用期</li><li>AI智能体从试点走向规模部署</li><li>人机协同的工作模式成为主流</li><li>行业人才结构调整加速</li><li>新商业模式开始产生规模化收入<br/>2027年:深度融合期</li><li>AI成为IT服务管理的基础设施</li><li>行业竞争格局基本稳定</li><li>新一代技术人才成为市场主力</li><li>技术标准和监管框架基本完善<br/>这个演进过程不会一帆风顺,必然伴随着:技术迭代的不确定性、商业模式的试错成本、人才转型的阵痛期、组织变革的阻力。<br/>但历史的车轮不会停止。就像云计算取代传统IDC、移动互联网颠覆PC互联网一样,AI对IT服务管理的重塑已是不可逆转的趋势。</li></ul><p><strong>拐点已至,选择在你</strong><br/>广州这场Meetup的意义,不在于提供了多少技术细节,而在于它标志着行业集体意识的觉醒。<br/>当100多位IT精英主动牺牲周末时间来学习AI,当四位专家不遗余力地分享经验和洞察,当参会者全神贯注地进行实战演练,我们看到的是一个行业在变革前夜的集体行动。<br/>这种行动本身就是信号:IT服务管理行业的拐点已经到来。<br/>站在这个拐点上,每个从业者、每家企业、每个投资机构都面临选择:<br/>是主动拥抱变化,还是被动等待淘汰?<br/>是投入资源转型升级,还是固守传统模式?<br/>是培养新型人才,还是继续依赖旧有能力?<br/>历史告诉我们,在技术变革的拐点上,选择比努力更重要,方向比速度更关键。<br/>2025年,AI智能体元年。IT服务管理行业的新篇章,正在开启。<br/>而你,准备好了吗?</p>]]></description></item><item>    <title><![CDATA[智驾大模型的「隐形战场」：当GPU堆不动了，行业拼什么？ 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524975</link>    <guid>https://segmentfault.com/a/1190000047524975</guid>    <pubDate>2026-01-06 19:10:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会（OpenAnolis Conference，简称 2025 龙蜥大会）于北京圆满结束。同时，由阿里云智能集团编译器技术总监李三红，龙蜥社区运营委员会副主席、龙腾计划生态负责人金美琴联合出品的“数据×模型×软件”分论坛也圆满举办。来自阿里云、安谋科技、HiEV大蒜粒车研所、中兴通讯以及清华大学、澳门大学等企业和高校的 12 位大咖，从操作系统与上下游生态协同的视角出发，与参会嘉宾一起探讨了如何通过技术协作加速智能驾驶的进步，分享了各自在自动驾驶技术栈中的前沿实践与生态思考。以下文章转自 HiEV 大蒜粒车研所公众号：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524977" alt="图片" title="图片"/></p><p>过去两年，随着大模型的发展，智驾行业行业似乎进入一场“军备竞赛”。从大规模装车量产，采集数据喂养模型迭代，“算力”成为一段时间内主机厂们关注的焦点，行业甚至有「千卡是门槛，万卡是入场券」的说法。</p><p>从 BEV+Transformer 到端到端，再到如今大热的 VLA（视觉-语言-动作）模型，参数量指数级膨胀，让整个行业陷入了一种“囤卡狂热”。</p><p>仿佛只要堆砌了足够的 H100 或 H800，L3 甚至 L4 级别的自动驾驶能力就会在 Scaling Law 的魔法下，自动涌现。</p><p>在前不久的 2025 龙蜥操作系统大会“数据×模型×软件”分论坛上，我们听到了一些冷静得近乎“泼冷水”的声音。 </p><p>主持人在圆桌讨论的时候提到一个很有意思的事情： </p><p>之前微软 CEO 萨提亚·纳德拉在接受采访的时候就感慨过，即便拥有大量的 GPU，也面临着缺乏足够的物理基础设施（如机柜与电力环境）来安置它们的尴尬境地。</p><p>这也折射出了智驾行业一个被长期掩盖的痛点：单纯依靠堆砌 GPU，想“大力出奇迹”的模式，正在撞上一堵「物理现实与经济成本」的墙。</p><p>当行业的焦点都集中在英伟达、华为昇腾这些台前的“算力卡”上时，一场关于操作系统、基础软件与异构计算的“隐形战争”早已在水面下打响。</p><p>阿里云副总裁李俊平在开场致辞中提出了一个公式：AI 的效能 = 数据（燃料）× 模型（引擎）× 软件（油门和方向盘）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524978" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云副总裁李俊平）</p><p>今天的智驾竞争，正在从单一的模型之争，演变为这三者乘积效应的系统工程对抗。</p><h3>这届智驾，被“数据搬运”卡脖子</h3><p>“谈卡伤感情，没卡没感情。”这是前两年智驾圈的真实写照。但到了 2025 年，很多车企发现，即便斥巨资买来了卡，训练效率却并没有线性增长。</p><p>问题出在哪？GPU 在“偷懒”。</p><p>这其实不是什么硬件故障，而是数据“喂”得不够快。</p><p>智驾研发并非只有模型训练这一个环节，它是一个包含数据采集、清洗、标注、挖掘、训练、仿真到端侧部署的一条长长的数据闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524979" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团高级架构师张先国分享了一组数据：智驾研发团队，云端存储的数据总量通常已达到 400PB 到 800PB，日增量在 1PB 以上。一个智驾企业同时进行多个模型训练，消耗的算力经常需要万卡以上。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524980" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团高级架构师张先国）</p><p>想象一下，GPU 就像是一台拥有 F1 引擎的赛车，但如果给它输油的管子（I/O带宽）只有吸管那么细，引擎空转就在所难免。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524981" alt="图片" title="图片" loading="lazy"/></p><p>在 2025 龙蜥大会的现场，多位专家指出了“数据闭环”中存在的隐形关卡： 一个是数据加载的问题。训练开始前，海量的小文件（图片、标注信息）需要从存储层搬运到计算层。另一个是预处理可能遭受的瓶颈：视频需要抽帧、解码、清洗，训练集群就在那里，但数据卡在缓存层过不来，GPU 只能闲置等待。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524982" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团产品专家钱君在演讲中提到，为了解决这个问题，行业正在把目光投向存储与操作系统的底层优化。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524983" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团产品专家钱君）</p><p>例如，龙蜥操作系统（OpenAnolis）给出的方案是全链路的“疏通”：针对 CPFS（并行文件系统），龙蜥在 OS 层面进行了深度适配。缓存写场景下的性能可以直接提升 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短：以前需要几小时，现在几十分钟就能搞定。 这种“看不见”的基础设施优化，虽然没有新开发一个大模型那么性感，但它决定了生产智能的效率和成本，是让万卡集群真正跑满的关键。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524984" alt="图片" title="图片" loading="lazy"/></p><h3>CPU：被忽视的“异构协同”</h3><p>在智驾的模型训练中，公众通常认为关键的算力在于 GPU；但在本届大会上，“CPU 的挖掘”成为当下的新共识。</p><p>“不能只关注 GPU，CPU 在数据预处理、存储 I/O 及逻辑控制中扮演着关键角色。” 中兴通讯操作系统产品副总经理胡冲在圆桌讨论中直言。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524985" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统产品副总经理胡冲）</p><p>事实上，在视频转图片（抽帧）、数据清洗、以及 Spark 大数据分析环节，CPU 才是主力军。而且，随着架构的演进，Arm 架构的服务器 CPU（例如如阿里云倚天 710 ）正在展现出独特的优势。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524986" alt="图片" title="图片" loading="lazy"/><br/>（图/安谋科技（Arm China）云人工智能事业部总监侯科鑫）<br/>安谋科技（Arm China）云人工智能事业部总监侯科鑫女士，在演讲中向现场观众展示了数据中心架构的演进逻辑：随着 NVIDIA  Grace Hopper 异构加速平台的推出，CPU 与 GPU 的“紧密协同处理”已成为行业明确的发展方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524987" alt="图片" title="图片" loading="lazy"/></p><p>为什么要协同？是为了打破“内存墙”。</p><p>“视频处理并不是简单的计算，它对高负载下算力要求极高。”张先国指出。</p><p>智驾训练需要把每秒视频抽帧为 8-32 张图片，在视频解码计算（如 H.264/H.265 格式）的高并发场景下，传统的 x86 架构，由于睿频（超线程）机制和功耗墙的存在，在高负载下往往会降频。</p><p>而张先国分享的实测数据显示，Arm 架构处理器凭借更多的物理核和大缓存（L1/L2 Cache），在智驾数据处理场景下表现惊人： </p><p>首先是视频抽帧，性能比传统 x86 提升约 20%，成本却降低了 20%-30%； </p><p>大数据清洗方面，由于拥有更大的 Cache（缓存），数据 Miss 率极低，这意味着 CPU 不需要频繁地去内存“搬砖”，从而使端到端性能提升了 30%，在部分场景下甚至实现了翻倍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524988" alt="图片" title="图片" loading="lazy"/></p><p>一个高效的智驾云端底座，必须是 CPU 与 GPU “各司其职、紧密抱团”的异构系统。</p><p>侯科鑫还从更宏观的维度讲述了硬件底座的变迁。她指出，为了打破“内存墙”和功耗瓶颈，数据中心正在从通用服务器向“定制化 SoC + Chiplet”演进。</p><p>NVIDIA 的 Grace Hopper 平台就是典型案例——通过将 Arm 架构 CPU 与 Hopper  GPU 紧密互联，实现内存共享，极大降低了数据搬运的延迟。这种 CPU 与 GPU 紧密协作的架构，正是为了解决单一算力无法应对复杂数据流的困境。Arm 推出的 Total Design 生态和 Neoverse CSS，正是以推动异构计算规模化落地为核心目标，让芯片设计公司能节省大量工程投入，快速构建这种异构计算的「高速公路」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524989" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><h3>基础软件的魔法：不堆卡也能让训练变得更快</h3><p>摩尔定律在放缓，硬件的红利正在吃紧。这时候，软件工程的价值就被进一步放大了。阿里云智能集团编译器技术总监李三红在圆桌环节提到了一个非常典型的矛盾：模型开发者的“爽”和底层工程师的“痛”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524990" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团编译器技术总监李三红）</p><p>算法工程师喜欢用 PyTorch 的 Eager 模式，因为这样写代码像写 Python 一样灵活，所见即所得；但这种模式对底层硬件极其不友好，运行效率低。而底层工程师希望用 Compile 模式，把代码编译成极致优化的机器码，但这又要求上层改代码，门槛极高。</p><p>“上层的模型开发者追求开发效率（Eager Mode），底层的 Infra 追求成本和性能，这中间的 Gap（鸿沟），就是基础软件的机会。” 阿里云智能集团编译器技术总监李三红在圆桌讨论中一针见血地指出。</p><p>针对如何填补这一鸿沟的问题，阿里云智能集团产品专家钱君与高级架构师张先国在随后的演讲中展示了龙蜥操作系统（OpenAnolis）如何通过全链路优化，在不改变硬件的情况下“白捡”性能：</p><ul><li>存储加速（IO 吞吐）： 针对 CPFS（并行文件系统），系统在 OS 层面进行了深度适配。钱君披露的数据显示，在缓存写场景下，性能提升了惊人的 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短，断点续训不再是噩梦。</li><li>网络加速（打破 TCP 限制）： 张先国指出，通过部署自研的 eRDMA 协议，相比传统 TCP，延迟降低 3 倍，带宽提升 4 倍（实测可达 18GB/s）。这让数据在节点间的跳跃如同在本地总线般顺滑。</li><li>编译器优化（榨干每一滴算力）： 针对 PyTorch 等框架的运行效率痛点，利用 AI Compiler 进行算子融合。据钱君介绍，这套方案在部分通用模型上带来了接近 100% 的性能提升，有效地解决了开发灵活性与运行效率不可兼得的难题。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524991" alt="图片" title="图片" loading="lazy"/><br/>效果有多明显？ </p><p>地平线和小鹏汽车的案例显示，通过这一套“操作系统+编译器+调度”的组合拳，部分场景下的性能提升可达 30% 甚至 100%，而成本却能下降 20%-60%。</p><p>在「降本增效」成为汽车产业主旋律的 2025 年，这种来自基础软件的“软实力”，比盲目堆更多的卡，更有性价比。</p><h3>眺望未来：世界模型与“合成数据”</h3><p>如果说当下智驾行业发展的痛点是“效率”，那么未来的挑战可能会是“认知”。</p><p>清华大学人工智能研究院视觉智能研究中心主任邓志东教授在圆桌论坛上抛出了一个前瞻性观点：智驾模型正在从单纯的感知，向世界模型（World Model）演进。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524992" alt="图片" title="图片" loading="lazy"/><br/>图片来源：CVPR 2024 Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</p><p>目前的端到端大模型，虽然能处理很多场景，但面对极端的 Corner Case（长尾场景），靠实车采集的数据永远是不够的。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524993" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><p>“路是跑不完的，但世界是可以被模拟的。”</p><p>但这种演进这种演进对基础设施提出了更苛刻的要求：</p><ul><li>算力需求的指数级爆炸： 世界模型极重，不仅需要理解物理世界，还要生成虚拟物理世界。这可能需要数百亿甚至更高的算力支撑，甚至触及到供电能力的边界。</li><li>合成数据的崛起： 真实路采数据的效率太低且稀缺。未来，大量的训练数据将来自“虚拟物理世界”的高效生成。这对 GPU 的渲染能力和 CPU 的逻辑模拟能力提出了双重挑战。</li><li>软件定义的灵活性：正如中兴操作系统产品线副总经理胡冲在圆桌中所感慨的，算法迭代极快——“去年可能还是 BEV，今年就是 VLA 了”。而阿里云李三红也证实，一线技术团队确实清晰感知到了模型向 VLA 及世界模型演进的趋势。这种软件层面的极速狂奔，与硬件芯片较长的迭代周期形成了鲜明对比。这就要求编译器和操作系统必须具备极强的适应性，通过软件定义来抹平硬件迭代的时间差。AI 不仅要“看懂”视频，还要能“生成”视频，甚至要理解牛顿定律。</li></ul><p>邓教授指出，这需要底层算力支持极其复杂的“虚实迁移”。这意味着，未来的操作系统不仅要调度计算，还要调度“物理世界的规则”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524994" alt="图片" title="图片" loading="lazy"/><br/>（图/清华大学教授、清华大学人工智能研究院视觉智能研究中心主任邓志东）</p><p>这也解释了为什么像龙蜥这样的开源社区，开始在这个阶段强调“ AI 原生操作系统”的概念——因为旧的底座，真的撑不住新的世界了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524995" alt="图片" title="图片" loading="lazy"/></p><h3>开源底座的长期主义</h3><p>从 2025 龙蜥大会的这场分论坛中，我们看到了汽车科技行业的一个明显转折：</p><p>大家不再盲目迷信硬件的堆砌，开始回归计算机科学的常识——系统协同。</p><p>面对 Arm、x86、RISC-V 等复杂的芯片架构，面对日新月异的模型算法，车企和智驾公司不可能每一家都去从零手搓一套底层软件。</p><p>而龙蜥社区的存在，就是为了提供一个标准化的技术底座，屏蔽底层异构硬件（不同架构的 CPU、GPU、NPU）的差异，让车企和智驾公司能够专注于上层模型和算法的创新。正如 Arm 通过 Arm Total Design 联合产业链一样，软件层面也需要这样一个“连接器”来降低全行业的试错成本。</p><p>正如胡冲所言：“通过社区共建、共享，降低车企的研发门槛与成本，是解决算力荒的另一种路径。”</p><p>在算力资源有限、成本高企、模型日趋复杂的背景下，谁能更高效地榨干每一 Tops 算力的价值，谁能以更低的成本完成数据的闭环流转，谁就能在 L3+ 的量产前夜活下来。</p><p>数据是资产，模型是能力，而软件与操作系统，是这一切的根基。</p><p>自动驾驶的下半场，不再是单点技术的突破，而是“数据-模型-软件”全链路的生态战争。在这个战场上，那个由 CPU、操作系统、编译器、文件系统构成的庞大“新基座”，正在成为决定胜负的隐形力量。</p><p>对中国的自动驾驶产业而言，建立一个自主、可控、高效的基础软件生态，其战略意义或许丝毫不亚于拥有几万张显卡。</p><p>因为只有根扎得够深，智能的树才能长得够高够稳。</p><p>本次分论坛回顾已上线，欢迎点击下方链接查看回放：<a href="https://link.segmentfault.com/?enc=Ad0vvxxozxD8A3cfJDxYVg%3D%3D.KaKY6pQ2Dh7iI5rJZ8a12KlhpFFqCT4%2BFD6fTofl9ZOwG4ruzgEki2alw41JoyB5" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[优秀学子获颁证书，开放原子校源行Meetup活动（中南大学站）圆满举办 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525016</link>    <guid>https://segmentfault.com/a/1190000047525016</guid>    <pubDate>2026-01-06 19:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，由浪潮信息联合龙蜥社区、中南大学信息与网络中心、电子信息学院共同举办的开放原子校源行 Meetup 活动（中南大学站）顺利举行。本次活动吸引了 70 余名中南大学本科生和研究生的积极参与，现场气氛热烈，同学们和与会嘉宾深入交流开源文化与技术应用，收获颇丰。</p><p>活动伊始，中南大学电子信息学院特聘副教授、博士生导师施鹤远主持开场环节。施教授对参与活动的师生、技术专家表示欢迎，并简要介绍了活动背景和目的。他强调，本次活动旨在为同学们搭建一个学习和实践开源技术的平台，帮助大家更好地了解开源文化、技术应用以及未来发展方向。他鼓励同学们积极参与今天的活动，主动与嘉宾交流，积极探索开源技术在实际应用中的价值，为未来的职业发展和个人成长积累宝贵经验。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525018" alt="图片" title="图片"/><br/>（图/中南大学电子信息学院特聘副教授、博士生导师 施鹤远）</p><p>中南大学电子信息学院教授、博士生导师、副院长石金晶为本次活动致辞。石院长在致辞中表示，开源技术不仅是当今科技发展的前沿趋势，更是同学们提升自身竞争力的重要途径。开源社区汇聚了全球最优秀的技术人才和创新项目，同学们在这里可以接触到最前沿的技术理念和实践经验。石院长强调，学院将全力支持同学们的开源实践，为大家提供更多的资源和平台，希望同学们能够珍惜这次机会，积极参与开源活动，为自己的未来职业发展打下坚实的基础，同时也为电子信息学院的学科发展贡献自己的力量。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525019" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、副院长 石金晶）</p><p>中南大学电子信息学院教授、博士生导师、教学实验中心主任胡超为本次活动致辞。胡主任在致辞中表示，开源技术为同学们提供了一个绝佳的实践平台，能够让大家在实践中快速成长。他指出，开源项目不仅能够提升同学们的专业技能，还能培养大家的创新思维和团队合作精神。胡主任鼓励同学们积极参与开源社区，主动探索和学习，勇于挑战自己。他强调，学院将为同学们提供全方位的支持，包括技术指导、实验资源等，帮助大家更好地参与到开源项目中。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525020" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、教学实验中心主任 胡超）</p><p>活动中还举行了“浪潮信息 - 龙蜥技术认证证书颁发仪式”。石院长与胡主任为在技术认证中表现优异的同学颁发了工程师证书，表彰他们在开源技术领域的突出成绩，激励更多同学投身开源实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525021" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息 - 龙蜥技术认证证书颁发仪式）</p><p>中南大学计算中心实验师徐海坤在活动中作了题为《AI 赋能科学计算》的分享。他介绍了中南大学计算平台的发展历程，包括其在 2020 年建成的千万亿次级计算平台，以及该平台在全球和中国相关领域排行榜中的优异表现。报告重点阐述了 AI 技术在科学计算中的应用，包括 AI 能力平台的建设、基于 AI 的智能运维和作业调度优化等内容，强调了开源技术在提升平台性能和运维效率中的重要作用，展示了中南大学在 AI 赋能科学计算领域的创新实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525022" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学计算中心实验师 徐海坤）</p><p>阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦以《让校园代码长出 AI 翅膀：与龙蜥共探下一代开源智能》为题进行分享。他介绍了龙蜥社区的 Anolis OS 23 操作系统，重点阐述其在 AI 场景下的创新应用，包括 AI 辅助开发、系统构建优化和运维智能化。他还展示了 OS Copilot 智能助手的功能，强调其在降低 Linux 使用门槛和提升运维效率方面的优势，并鼓励同学们积极参与开源社区活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525023" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦）</p><p>浪潮信息高级产品经理 Viki Wang在活动中分享了《AI 时代开源操作系统应用及生态创新实践》。他表示，浪潮信息作为龙蜥社区副理事长单位，在开源领域持续创新，其开源贡献排名位居前列，并在多个领域取得显著成果。他还分享了浪潮信息在 AI 时代的系统优化成果，如 GPU 和 CPU 异构算力协同、大模型推理性能提升以及兼容性测试基准的建立等，展示了浪潮信息在开源操作系统领域的技术实力和生态建设成果。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525024" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息高级产品经理 Viki Wang）</p><p>龙蜥社区 CXL SIG Maintainer 李伟在活动中分享了《携手龙蜥 共创芯生态》。他介绍了其团队在开源领域的全面布局和深度合作，特别是在龙蜥社区的积极参与。通过贡献内核优化、虚拟化支持和安全技术，推动了开源生态的发展。李伟强调，其团队致力于通过开源合作，共同打造开放、共赢的芯片与操作系统生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525025" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区CXL SIG Maintainer 李伟）</p><p>本次开放原子校源行 Meetup 活动（中南大学站）在热烈的氛围中圆满落幕。通过本次活动，浪潮信息携手开放原子、龙蜥社区及中南大学，成功搭建了一个前沿技术交流与实践的平台，为同学们开启了通往开源世界的大门。未来，浪潮信息将继续践行“龙蜥+”合作模式，深化与高校的合作，助力更多学子在开源领域成长成才，为开源生态的繁荣发展持续贡献力量。</p><p>龙蜥技术认证学习中心：<a href="https://link.segmentfault.com/?enc=Y2JFwazv9c2RpeTAkxdl%2BA%3D%3D.D%2B8TjMa%2FZo3B02zmnK3x3lKmicqpZuvd0Lny6tggDws%3D" rel="nofollow" target="_blank">https://openanolis.cn/course</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[专访 | 软硬协同、开源共建：英特尔与龙蜥携手打造 AI 时代的可信计算底座 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525040</link>    <guid>https://segmentfault.com/a/1190000047525040</guid>    <pubDate>2026-01-06 19:09:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>编者按：近日，2025 龙蜥操作系统大会（OpenAnolis Conference）在北京圆满召开。当智能驾舱厂商训练自动驾驶 AI 模型、金融机构运行 AI 风控系统时，普遍面临相同困境：数据敏感不敢上云，本地算力又难以支撑大模型需求。AI大爆发后，“算力效率”与“数据安全”的矛盾愈发突出。2025 龙蜥操作系统大会前夕，InfoQ 对话英特尔技术专家与阿里云技术专家，揭秘双方如何通过第六代至强处理器与龙蜥操作系统的深度协同，破解这一行业难题。以下为采访全文：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525042" alt="图片" title="图片"/></p><h3>合作底层逻辑：为什么是“英特尔硬件+龙蜥开源”？</h3><p>AI 时代的算力释放，早已不是“硬件单枪匹马”能实现的。“过去硬件做芯片、软件写系统的分工模式已失效，AI 模型扩大与数据敏感化，要求硬件与软件必须深度协同。”英特尔技术专家强调，“2020 年我们成为龙蜥首批理事单位，正是看中龙蜥操作系统在云原生和 AI 领域的开源属性——它能快速承接硬件新特性，而英特尔的芯片技术，就是要为这个开源生态打下坚实的算力底座。”</p><p>过去一年，双方形成了固定的合作节奏：硬件首发时与龙蜥操作系统同步适配，避免企业“空有硬件用不了”；针对硬件暴露的软件问题联合优化，成果反哺龙蜥及 Linux 上游社区；英特尔提供测试资源，龙蜥联动客户推动技术落地。总结来说，英特尔负责造算力、锁安全，龙蜥负责用算力、传价值，这是双方合作的核心逻辑。</p><p>阿里云技术专家补充道：“阿里云作为云平台方，刚好承接这种协同成果——把英特尔的硬件能力和龙蜥操作系统的系统优势，打包成企业能直接用的云服务，这也是我们作为社区理事长单位的价值。”</p><h3>技术共建：从芯片到工具链，让 AI 算力“跑满效能”</h3><p>AI 算力的高效释放，需要“芯片发动机”“系统公路”与“工具链交通规则”的协同。过去一年，英特尔与龙蜥的技术共建集中在这三大方向：</p><h4>第六代至强适配：不止能用，更要“榨干”性能</h4><p>2025 年发布的第六代至强处理器，针对不同 AI 场景做了细分设计：Granite Rapids 主打高密度计算，适配金融风控、科学计算等强性能需求；Sierra Forest 聚焦云原生大规模部署，优化能效比以降低云厂商运营成本。“我们的芯片设计贴合场景化需求，而龙蜥操作系统能精准匹配这种特性，让硬件能力不浪费。”英特尔技术专家说。</p><h4>为让硬件性能充分释放，双方完成了两项关键优化：</h4><ul><li>全链路适配：覆盖龙蜥操作系统 5.10 长期支持版与 6.6 最新特性版，同时完成主流虚拟化平台定制，为 Granite Rapids 开发“大内存调度补丁”，支持 2TB 以上内存以满足 AI 训练需求；</li><li>突破多核瓶颈：针对新一代处理器近 128 核的硬件特性，重构龙蜥操作系统多核调度算法，通过“专属缓存分配”减少核心资源争抢，优化内存页表管理实现有序读写。</li></ul><p>这些优化最终让 Granite Rapids 在多线程任务中性能较上代提升显著。“性能提升是企业能切实感受到的变化，这是软硬件协同的价值。”英特尔技术专家表示。</p><h4>下一代硬件预研：提前3个月适配，消除“空窗期”</h4><p>为解决企业“硬件到位、系统未就绪”的痛点，双方采用“提前布局”策略。英特尔下一代至强 6 Plus 服务器（代号 Clearwater Forest）尚未上市，2025 年 Q2 已联合龙蜥启动适配。“企业采购硬件投入大，我们把适配周期提前，就是要让客户拿到硬件就能开机测试，这符合龙蜥社区‘开箱即用’的理念。”英特尔技术专家表示。</p><h4>异构工具链：oneAPI+OpenVINO，降低开发门槛</h4><p>AI开发者常受困于“硬件异构”——为 CPU 写的代码无法直接在 GPU、NPU 上运行，重复适配耗费大量精力。英特尔与龙蜥的解法是构建“统一工具链”。“开发者的核心价值是优化 AI 模型精度，不是做硬件适配的‘翻译官’。”英特尔技术专家直言，“oneAPI 和 OpenVINO 的融合，就是要把硬件差异藏在工具链里，让一套代码跑通所有设备。”</p><ul><li>oneAPI 统一开发框架：基于 LLVM 扩展异构编译能力，搭建设备抽象层，一套代码可调用不同硬件能力；该平台支持多种编程语言，包括 C++、Python、Fortran 等，使得 AI 模型的训练和推理能够在不同计算架构上高效执行。</li><li>OpenVINO 工具链即插即用：与龙蜥操作系统深度集成，简单命令即可部署，为云端和边缘计算环境中的 AI 推理任务提供优化方案，进一步降低 AI 部署的计算成本，提高 AI 模型的执行效率。</li></ul><p>“我们的目标是让开发者聚焦模型优化，而非硬件适配。”英特尔技术专家表示，这正是“软件定义、硬件赋能”的核心体现。</p><h3>生态共建：让算力生态“活起来”</h3><p>技术落地离不开生态支撑。作为龙蜥社区副理事长单位，英特尔从社区治理、资源支持、国际化联动三方面推动生态发展：</p><p>首先是深度参与社区治理。英特尔并非单纯的“硬件供应商”，而是深度参与龙蜥社区底层建设：如主导 X86 架构优化的 Arch SIG 项目，制定至强处理器在龙蜥操作系统上的性能基准测试体系；参与《国产服务器操作系统发展报告（2025）》中核心章节的撰写；推动龙蜥社区加速国际化等。“开源社区要靠核心厂商带头做实事，这是我们作为副理事长单位的责任，也是为了让龙蜥生态更有技术厚度。”英特尔技术专家说。</p><p>其次是开放资源、降低参与门槛。为解决中小企业“缺硬件、缺技术”的问题，英特尔向龙蜥社区开放测试硬件，开展联合测试并输出技术文档、联合报告。阿里云技术专家补充：“我们会把测试成果转化为云平台最佳实践，帮客户少走弯路。”</p><p>最后是国际化经验反哺。依托在 Linux Foundation、CNCF 的经验，英特尔帮助龙蜥优化内核补丁流程，深度参与 X86 架构补丁审核；并推动龙蜥开发者参加全球开源峰会，加强国际化交流等。</p><h3>机密计算：用“硬件锁+开源钥匙”守护数据安全</h3><p>AI 时代的核心矛盾是“数据需流动产生价值，却怕流动中泄露”。英特尔与龙蜥的解法，是从硬件隔离到开源方案的全链路防护。</p><p>英特尔从硬件层面构建安全底座，核心依赖两大技术，其原理均通过硬件隔离实现数据防护。“机密计算的核心是‘硬件可信’，软件再安全，硬件被突破就没用。”英特尔技术专家解释，“TDX 和 SGX 就是从芯片层面给数据加‘锁’，让安全成为硬件原生能力。”</p><ul><li>TDX（可信域扩展）：在至强芯片中创建“隔离执行域（TD）”，即使系统内核被攻击，TD内的内容也无法被访问，内存数据通过内存控制器实时加密，仅硬件才能解密；</li><li>SGX（软件防护扩展）：针对轻量级场景在内存中划分“加密区（Enclave）”，仅授权代码可访问，其他进程即使获取内存地址，看到的也只是乱码。</li></ul><p>为让龙蜥操作系统适配这些能力，英特尔在系统内核中集成 TDX 和 SGX 驱动并由硬件实现 AES-GCM 加密协议，确保安全防护不影响性能。</p><p>此外，基于硬件底座，阿里云在龙蜥社区推出 Confidential AI 开源方案，整合 TDX 安全能力、远程证明服务与密态存储/网络能力，降低企业使用 TDX 机密计算的门槛。“英特尔的硬件是‘安全地基’，我们的工作是在地基上搭好‘房子’，让企业不用自己打地基就能用。”阿里云技术专家说，“目前龙蜥社区 Confidential AI 开源方案已落地阿里云异构机密计算实例，并正与消费电子、智能驾舱客户合作。”</p><p>为推动行业规范，双方在标准化工作上已取得明确进展。阿里云技术专家具体介绍：“以 Confidential AI 为技术基础，阿里云已联合 30 多家合作伙伴牵头编写 CCSA AI 数据安全的标准化架构与技术实现方案，重点覆盖 AI 推理和训练两大核心场景，目前这项工作已进入实质推进阶段。”</p><p>国际标准的布局也在同步展开，他补充道：“我们计划以 CCSA 的标准化成果为基础，在 12 月初日内瓦举行的国际电信联盟 ITU-T SG17 分论坛上，推动该标准的国际版本立项。这将形成一个良性闭环——开源解决方案为标准化提供了可落地的技术参考，而标准化规范又能为开源方案的合理性和通用性提供背书。”</p><p>在他看来，这种联动恰好体现了核心价值：“商业需求驱动开源方案迭代，开源方案支撑标准化落地，标准化又反过来赋能商业推广，三者不是孤立的，而是循环共生的关系。”</p><h3>未来方向：AI 原生时代的多元协同</h3><p>在 2025 龙蜥大会上，双方集中展示了第六代至强与龙蜥操作系统的性能优化成果、Confidential AI 落地进展及 Clearwater Forest 适配情况，同时释放 AI 原生时代的算力发展方向。</p><p>“AI 原生算力靠‘芯-OS-云-AI’协同。”英特尔技术专家透露规划：硬件上，下一代至强将进一步强化 AI 与安全能力；系统上，将联合龙蜥开发 AI 任务调度器，优化资源分配效率；场景上，将针对隐私敏感场景推出通用部署方案。</p><p>对企业而言，这意味着未来使用 AI 将更简单：依托“英特尔硬件+龙蜥操作系统”的组合，无需自行解决适配、优化与安全问题，即可直接获得高效且安全的算力支撑。随着龙蜥大会的召开，这套“硬件底座+开源生态”的方案，将成为企业 AI 落地的核心选择。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[不止于用，更在于创！龙蜥社区点燃高校开源火种 | 龙蜥五周年征文精选 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525045</link>    <guid>https://segmentfault.com/a/1190000047525045</guid>    <pubDate>2026-01-06 19:08:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>各位小伙伴，龙蜥社区已启动主题为「5 周年，你与龙蜥的故事」征文活动！征文内容包括但不限于以下：</p><p>故事文章：文字记录你的经历、感悟或技术心得，包括：龙蜥操作系统 Anolis OS 的使用体验、相关技术特性解读、经验分享等。</p><p>故事视频：用镜头讲述你与龙蜥的点滴瞬间，参与某次大会/ MeetUp 的拍摄视频等（3 分钟内即可）。</p><p>优质文章将获得龙蜥社区官网及公众号推荐展出，还可获得神秘礼品。欢迎各位龙蜥社区朋友来稿~</p></blockquote><p>本期征文故事主角：施刚，龙蜥社区 2025 年度优秀贡献者奖获得者、成都东软学院计算机与软件学院教授、中国自动化学会边缘计算专业委员会委员，从事计算机系统研究与操作系统相关课程教学工作。2022 年，通过教育部产学合作协同育人项目开始与龙蜥社区开展深度合作。</p><h3>初识龙蜥社区</h3><p>在高校计算机相关专业中，操作系统相关的专业课程占据着重要的地位。多年来，基于 UNIX 和 Linux 系统来进行相关课程的讲授与学习，是高校计算机相关专业师生的普遍选择。早期，CentOS 系统作为 RHEL 系统的完全功能兼容版且开源免费的特性，是高校学习操作系统及中小 IT 企业用于部署各类应用服务器的首先系统。但随着 2021 年末，CentOS 系统停止更新退出市场后，不仅商业市场，在高校教学领域也必须加紧填补其留下的技术空白。</p><p>近年来，在国产系统走进课堂的大背景下，作为面向智算时代的国产开源操作系统，龙蜥操作系统 Anolis OS 已成为 CentOS 的优秀继承者，它不仅完全兼容 RHEL，更针对云原生、高性能计算进行了优化，是高校在操作系统相关课程尤其是 Linux 相关课程中的最优选择。2021 年底，我开始关注 CentOS 停服后的国产操作系统替代方案，并从此开始了解龙蜥社区。2022 年，我通过申请教育部产学合作协同育人项目（龙蜥社区理事长单位-阿里云发布 Anolis OS 项目），与龙蜥社区正式确立了合作关系，并把龙蜥社区正式引入了成都东软学院计算机相关专业的教学实践工作中来。同年注册龙蜥社区，下载安装或通过龙蜥实验室使用龙蜥操作系统的师生就超 1500 人次。至今，成都东软学院已有累计超 5000 人次师生学习和使用龙蜥操作系统。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525047" alt="图片" title="图片"/></p><p>基于 Anolis OS 8，我们也产出了一套完整的《Linux 系统管理与服务器配置》教材，用于日常教学。社区成员也可在龙蜥文档中心获取：<a href="https://link.segmentfault.com/?enc=ptZx1WglRHCOfgYTNCnpKw%3D%3D.1Lg7dpPFr3sqejFcRRI%2FUgtzwcj67OrF2GiUfZapyCQHGy96Mf94hC1IG9hDCdqVMd0VtwuEuiF2unAPfowhTQ%3D%3D" rel="nofollow" target="_blank">https://docs.openanolis.cn/document/detail/rxli6fw9</a></p><h3>龙蜥实验室：优秀的在线实践与学习平台</h3><p>对高校的计算机类实践教学活动来说，拥有一个优秀的在线实践平台是非常重要的。依靠高校自身打造基于操作系统的服务性平台所需的软硬件资源非常大，国内高校鲜有独立建立的相关实践教学平台。而龙蜥社区提供的龙蜥实验室帮助我完美解决了此问题。作为学生，只需要一台联网的计算机，就可在“龙蜥实验室”申请一台机器并在其进行各类实践和实验活动。我在使用龙蜥实验室的过程中，还不断通过向龙蜥社区后台技术人员反馈使用情况并提出改进意见，使得龙蜥实验室的申请和使用流程越来越优化和便捷。以我讲授的 Linux 基础课程为例，除日常的各类实践类作业，该课程所涉及的 8 个实验项目中有 6 个都可通过龙蜥实验室在线完成。 </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525048" alt="图片" title="图片" loading="lazy"/></p><h3>龙蜥技术认证：为高校学生提供企业认可的职业技能认证</h3><p>对高校学生来说，毕业求职时能提供更多的 IT 职业认证技能证书更能得到对口求职单位的认可。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525049" alt="图片" title="图片" loading="lazy"/></p><p>在 Linux 的认证领域，传统的 RHEL 认证已发展了近二十年，在 RHEL 占据国内高端服务器操作系统的时代其认证证书的含金量是很高的。但随着 CentOS 的退出及以龙蜥操作系统为代表的国产服务器操作系统的强势崛起，国内 IT 企业对龙蜥操作系统的认可度也越来越高。根据2025 龙蜥操作系统大会数据显示，当前龙蜥操作系统的装机量已突破 1000 万套，市场占有率接近 50%，国内 IT 市场对相关运维人才的需要也越来越多。2023 年开始，龙蜥社区联合相关企业开始推出龙蜥技术认证的活动，我在成都东软学院计算机学院配合了该认证活动的推广和实施。</p><p>首次认证是基于龙蜥-统信联合开展的，有 600 多名同学报名，这些同学都经过了一学期的 Linux 课程学习，大部分同学都顺利通过了考试获得了龙蜥社区和统信的双认证证书。在首次活动顺利开展的基础上，2024 年度和 2025 年度，我在龙蜥社区的协助下，又继续开展了两次集中式的认证考试组织，报名同学所覆盖的专业由成都东软学院的计算机科学与技术拓展到了网络空间安全、网络工程、大数据等多个计算机相关专业，报名人数也逐年提升。为提高同学们的认证考试通过率，龙蜥社区还联合浪潮信息和统信软件的技术专家在后续两次考试前为报名同学进行了统一在线培训。</p><p>截止 2025 年下半年，成都东软学院已和龙蜥社区配合共同开展了 3 次龙蜥技术认证考试，累计参与学生 2000 余人次，通过率接近 80%。在推进龙蜥认证活动的过程中，我还通过申请将认证证书与 Linux 相关课程考核成绩相关联的方式，更好地激发和推动了学生参加龙蜥认证活动的热情。随着龙蜥中级认证活动在 2025 年的推出，后续龙蜥认证在我校的推进将更加深入开展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525050" alt="图片" title="图片" loading="lazy"/><br/>（图/技术认证现场）</p><h3>深度合作：龙蜥社区与高校计算机专业教学的发展方向</h3><p>操作系统类课程是计算机专业的核心课程也是我国计算机技术发展的重要基石，高校是为我国计算机行业提供后备人才的最重要基地。在当前国产软件国产系统进校园进课堂的大趋势和大背景下，龙蜥社区在与高校合作领域面临着非常好的发展前景。以我本人为例，当前我已将龙蜥操作系统作为操作系统类课程学习和实践的主要平台，并将龙蜥实验室作为相关课程实践和实验活动开展的主要场景，通过将龙蜥认证活动与相关课程的考核深度绑定。同时，龙蜥社区还提供了丰富的在线学习视频，可作为 Linux 相关课程内容学习的强力辅助。我作为一名高校教师，深感龙蜥社区在专业度上与计算机相关课程教学大纲和教学内容是高度契合的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525051" alt="图片" title="图片" loading="lazy"/><br/>最后，高校与社区间是可以持续开展深度合作互惠互利的关系，龙蜥操作系统作为国产操作系统的优秀代表，高校的土壤可以为龙蜥操作系统培养源源不断的学习和使用者，同时也为 IT 企业输送合格的龙蜥操作系统开发和运维人员。随着我国在操作系统技术领域的不断发展，国产操作系统必将逐步取代 RHEL 和 Windows 为代表的各类非国产操作系统在国内各领域的地位和市场。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[关于 AI 编程的思考 edagarli ]]></title>    <link>https://segmentfault.com/a/1190000047525058</link>    <guid>https://segmentfault.com/a/1190000047525058</guid>    <pubDate>2026-01-06 19:07:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在低代码开发与AI商家提效领域深耕多年后，我对AI编程的核心认知逐渐聚焦于“人机协同的价值放大”——它并非替代开发者，而是通过技术工具将开发者从重复编码中解放，转向更核心的业务拆解、架构设计与质量把控。</p><p>AI编程的核心优势在于对标准化场景的效率提升，以我之前的“AI商家工作台看板生成”项目为例，通过结构化Prompt将商家的业务规范（如销量/库存/毛利等核心指标定义）、技术约束（UI组件库规范、接口对接标准）、权限规则等嵌入交互逻辑，AI能快速生成可直接运行的前端代码，将原本1-2天的开发周期压缩至小时级。但这背后离不开两个关键前提：一是精准的Prompt工程，需要将模糊的业务需求转化为AI可理解的技术语言，这要求开发者既懂业务又懂AI的“认知逻辑”；二是对生成结果的校验能力，AI可能存在边界case遗漏、性能优化不足等问题，开发者需凭借技术经验进行兜底，尤其企业级应用中，安全性、规范性与业务贴合度的校验不可或缺。</p><p>从行业发展来看，AI编程正朝着“领域定制化”方向演进。通用型AI编程工具已无法满足企业级场景需求，针对电商采销、供应链管理等垂直领域的定制化AI模型，通过训练行业专属知识库，能大幅提升代码生成的精准度。同时，开发者的角色也在迭代：从单纯的编码者转变为“需求拆解师+Prompt工程师+架构设计师”，需要更强的业务抽象能力与跨领域整合能力——只有将业务逻辑、技术架构与AI工具特性深度融合，才能让AI真正成为业务提效的催化剂。</p><p>归根结底，AI编程的本质是“技术工具对生产力的重构”，但其价值上限始终由开发者的业务理解深度与技术把控能力决定。未来，人机协同的核心将是“人定义价值、AI落地执行”，开发者需聚焦于更具创造性的工作，让AI成为串联需求与实现的高效桥梁，最终实现技术服务于业务增长的核心目标。</p>]]></description></item><item>    <title><![CDATA[2025全球量子计算产业发展展望报告：技术路线、市场规模与应用落地|附200+份报告PDF、数据、可]]></title>    <link>https://segmentfault.com/a/1190000047525072</link>    <guid>https://segmentfault.com/a/1190000047525072</guid>    <pubDate>2026-01-06 19:07:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>原文链接：<a href="https://link.segmentfault.com/?enc=%2BgYqhnlaAiIvokMsNg5M%2Bw%3D%3D.3MfwCGmj%2Fq0s63W5OmSPga4%2B50b64sCkQ6G1JnFX4l8%3D" rel="nofollow" title="https://tecdat.cn/?p=44713" target="_blank">https://tecdat.cn/?p=44713</a>  <br/>原文出处：拓端抖音号@拓端tecdat</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面"/></p><h3><a name="t0" target="_blank"/>引言</h3><p>当谷歌Willow芯片实现量子纠错关键突破，中国“祖冲之三号”刷新超导量子计算性能基准，全球量子计算产业已从“实验室小众探索”迈入中美双极竞速的战略博弈新阶段。20余年技术演进，让量子计算从理论构想成为重塑全球科技版图的核心变量——一个拥有100个量子比特的系统，理论上可并行处理2^100种可能状态，这一特性让它在密码破解、药物研发、材料设计等经典计算“束手无策”的领域具备颠覆性潜力，也让中美欧等主要经济体展开了围绕技术路线、产业生态、标准制定的全方位竞争。  <br/>本报告洞察基于《发布机构：光子盒研究院：2025全球量子计算产业发展展望》和文末<strong>200+份</strong>量子计算与量子技术行业研究报告及数据，系统梳理全球技术路线竞争格局、市场规模增长逻辑、应用场景落地潜力与风险挑战，为创业者、技术决策者、投资者提供可落地的行业洞察。</p><p>本文完整报告数据图表和文末最新参考报告合集已分享在交流群，阅读原文查看、进群咨询，定制数据、报告和800+行业人士共同交流和成长。</p><p>值得警惕的是，中国在量子计算领域虽实现单点突破，但在技术路线收敛性、产业生态完整性、专利布局广度上仍与美国存在显著差距。技术路线分野、成本高企、应用场景模糊等问题，再叠加美国的技术封锁与生态壁垒，正考验着所有中国参与者的战略定力——稍有迟疑，便可能在这场关乎未来算力主权的竞赛中被系统性甩开。</p><h3><a name="t1" target="_blank"/>一、技术路线竞速：中美主导的路线之争，谁能笑到最后？</h3><p>量子计算的技术路线之争从未停歇，超导、离子阱、光量子、中性原子等路线各有优劣，但竞争的核心已从“多路线并存”转向“中美主导的生态卡位”。以下核心指标直观呈现不同路线的性能差异，更揭示了中美企业的实力差距：</p><h4><a name="t2" target="_blank"/>1. 量子比特数量对比</h4><p><strong>规模扩张的核心竞赛，中美已形成第一梯队</strong>  <br/>量子比特数是衡量量子计算机算力的基础指标，直接决定并行处理能力的上限，也是中美企业的核心竞争点。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525075" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院《2025全球量子计算产业发展展望》，包含超导、离子阱、光量子、中性原子四大主流技术路线核心设备量子比特数统计，量子比特数均指物理的、可用于计算的，逻辑的、耦合的不考虑在内。  <br/>量子比特数量对比图1数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：超导路线在比特数上暂时领先，中国“祖冲之三号”与谷歌Willow均突破105比特，但美国QuEra的中性原子路线已实现256比特规模，中国中科酷原“汉原一号”仅达到100+比特，规模化差距明显。  <br/>对应人群行动建议：创业者可优先关注超导路线的商业化机会，依托中国在该路线的单点优势快速落地；技术团队需警惕“比特数陷阱”——单纯追求数量而忽视保真度无实际意义，同时需紧盯美国中性原子路线的规模化进展，避免技术代差扩大。</p><h4><a name="t3" target="_blank"/>2. 量子门保真度对比</h4><p><strong>计算准确性的生命线，美国企业仍占绝对优势</strong>  <br/>保真度直接影响计算结果的可靠性，是量子算法落地的核心前提，美国在高精度操控技术上的积累已形成壁垒。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525076" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K技术监测数据，统计时间为2024年全年，涵盖全球15家主流量子计算硬件厂商核心产品的单比特门与双比特门保真度测试结果。  <br/>量子门保真度对比图2数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线保真度最高，美国Quantinuum的单比特门保真度超99.99%，中国华翊量子HYQ-A37约为99.9%；超导路线中，谷歌Willow双比特门保真度达99.5%，中国“祖冲之三号”约为99.0%，差距虽小但在高精度场景影响显著。  <br/>对应人群行动建议：金融、医药等对计算准确性要求极高的行业，短期可优先布局美国离子阱路线应用；中国企业需加大量子测控技术研发，缩小保真度差距，避免在核心场景被替代。</p><h4><a name="t4" target="_blank"/>3. 量子相干时间对比</h4><p><strong>量子态稳定的关键，中美路线各有优劣但差距明显</strong>  <br/>相干时间决定量子比特能保持量子态的时长，直接影响计算深度，中国在极低温环境维持技术上仍依赖进口设备。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525077" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算硬件性能基准测试平台（2024年度报告），相干时间测试环境为各技术路线标准运行环境（超导10mK、离子阱超高真空等）。  <br/>量子相干时间对比图3数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线相干时间最长（毫秒级），美国IonQ达到10毫秒，中国华翊量子约为2毫秒；超导路线中，谷歌Willow在自主研发的稀释制冷机支持下，相干时间达500微秒，中国“祖冲之三号”约为300微秒，设备依赖导致差距难以快速缩小。  <br/>对应人群行动建议：需要长计算周期的量子模拟场景（如材料研发）可选择美国离子阱路线，短期快速计算任务（如组合优化）中国超导路线更具优势；国内企业需加速稀释制冷机等核心设备的国产替代，从底层突破相干时间瓶颈。</p><h4><a name="t5" target="_blank"/>4. 量子门操作时间对比</h4><p><strong>计算效率的核心保障，中国超导路线具备局部优势</strong>  <br/>操作时间决定量子门执行速度，影响整体计算效率，中国在超导路线的门操作速度上实现局部反超，但应用场景有限。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525078" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院与清华大学量子信息研究中心联合测试数据，操作时间为单量子门平均执行时间，气泡大小对应技术路线成熟度评分。  <br/>量子门操作时间对比图4数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国超导路线操作时间最短（30纳秒），略优于谷歌Willow的40纳秒；但离子阱路线中，美国Quantinuum虽操作时间较长（100微秒），但可通过高保真度弥补效率差距，应用场景更广泛。  <br/>对应人群行动建议：高频计算场景（如实时风控）优先选择中国超导路线，对效率要求不高的科研场景可接受美国离子阱路线的速度trade-off；中国企业需扩大超导路线的应用场景覆盖，将局部优势转化为生态优势。</p><h4><a name="t6" target="_blank"/>技术路线对比表</h4><table><thead><tr><th>核心结论</th><th>数据差异</th><th>原因分析</th></tr></thead><tbody><tr><td>超导路线成当前主流，中美双雄争霸</td><td>美国比特数105-176、保真度99.5%-99.9%；中国比特数105-176、保真度99.0%-99.5%</td><td>美国与半导体工艺兼容性高，生态成熟；中国技术单点突破快，但设备依赖进口</td></tr><tr><td>离子阱路线美国稳扎稳打，中国追赶中</td><td>美国比特数32-100、保真度99.8%-99.99%；中国比特数37-100、保真度99.0%-99.9%</td><td>美国量子比特天然全同技术积累深；中国依托高校研发快速突破，但激光系统仍依赖进口</td></tr><tr><td>中性原子路线美国潜力巨大，中国起步晚</td><td>美国比特数200-256、保真度99.0%-99.5%；中国比特数100+、保真度98.5%-99.0%</td><td>美国规模化扩展成本低，可构建多维阵列；中国测控难度高，技术成熟度不足</td></tr><tr><td>光量子路线中美进展均缓慢，中国略占优</td><td>美国比特数216、保真度98.0%-99.0%；中国比特数255、保真度98.5%-99.0%</td><td>中国“九章三号”实现光子数突破，但光子纠缠操控难度大，退相干快的问题未解决</td></tr></tbody></table><p>不同路线的竞争本质是“中美生态卡位战”——没有完美的技术，只有适配的场景。当前行业共识是：短期内超导路线将主导商业落地，中国可依托该路线实现局部突破；中长期中性原子路线可能成为美国拉开差距的关键，中国需加速技术攻关；离子阱路线则成为美国巩固高精度场景优势的核心，中国需在细分领域建立差异化壁垒。</p><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047525079" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>2025人工智能AI研究报告：算力、应用、风险与就业|附1000+份报告PDF、数据、可视化模板汇总下载</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=Wu18l8G51gy0HdhvU9zVfA%3D%3D.BtNMBYglGHPlbuEbx8X9M9B5LTrJ0oEzbtfOLi0S3e8%3D" rel="nofollow" title="https://tecdat.cn/?p=44642" target="_blank">https://tecdat.cn/?p=44642</a></p><h3><a name="t8" target="_blank"/>二、市场规模与投融资：资本押注的未来，中美差距正在拉大</h3><p>量子计算市场正呈现“指数级增长”态势，但资本布局的结构性差异已凸显中美产业发展的深层差距——美国聚焦生态构建，中国仍停留在单点技术突破。</p><h4><a name="t9" target="_blank"/>1. 全球量子计算市场规模预测</h4><p><strong>从十亿到万亿的跨越，中国占比仍处弱势</strong>  <br/>量子计算产业规模将在2030年后迎来爆发式增长，但中国市场占比提升缓慢，难以撼动美国主导地位。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525080" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院产业预测模型，基于2024年全球50.37亿美元市场规模，结合技术迭代速度、政策支持力度、应用落地进度综合测算。  <br/>全球量子计算市场规模预测图5数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球市场规模50.37亿美元，美国占比62.7%，中国仅25.3%；2030年全球将达2199.78亿美元，中国占比预计提升至27.96%，仍落后美国30个百分点以上；2035年全球突破8000亿美元，中国占比29.49%，差距仍未缩小。  <br/>对应人群行动建议：投资者可重点布局美国上游核心器件（稀释制冷机、量子芯片）和中游整机厂商，同时关注中国国产替代机会；中国企业需加强产业链协同，避免单点作战，依托政策支持构建自主生态。</p><h4><a name="t10" target="_blank"/>2. 全球量子计算产业规模预测</h4><p><strong>中国市场自主化驱动的增长，难掩生态短板</strong>  <br/>中国量子计算市场在自主化突破下快速增长，但产业生态不完整导致增长质量不高。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525081" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K全球区域市场分析报告（2024），中国市场规模统计包含硬件整机、软件算法、云平台及下游应用四大板块。  <br/>全球量子计算产业规模预测图6数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国市场规模占全球25.3%，其中硬件整机占比超60%，软件算法仅占15%；而美国软件算法占比达40%，生态完整性远超中国，产业抗风险能力更强。  <br/>对应人群行动建议：国内创业者可依托政策支持，聚焦上游国产替代机会（如稀释制冷机、测控系统），同时加大软件算法研发投入，补全生态短板；海外企业可寻求与国内科研机构的合作切入点，共享中国硬件增长红利。</p><h4><a name="t11" target="_blank"/>3. 中国量子计算融资规模</h4><p><strong>本土资本的谨慎布局，单笔体量远逊美国</strong>  <br/>中国量子计算融资活跃度位列全球第二，但单笔体量偏小，反映本土资本对生态构建的信心不足。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525082" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为PitchBook 2024量子技术投资报告及光子盒研究院投融资监测数据，融资规模统计包含种子轮、天使轮、A/B/C轮及政府资助。  <br/>中国量子计算融资规模图7数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国融资规模0.47亿美元，交易笔数6笔，单笔平均0.078亿美元；美国融资规模12.60亿美元，交易笔数17笔，单笔平均0.741亿美元，单笔体量是中国的9.5倍。  <br/>对应人群行动建议：国内初创企业需突出技术差异化与国产替代价值争取融资，避免单纯追求比特数突破；政府引导基金可加大对中游整机厂商的长期投入，同时设立专项基金支持软件生态建设，改变“重硬件、轻软件”的融资格局。</p><h4><a name="t12" target="_blank"/>4. 量子计算企业融资额</h4><p><strong>全球融资头部集中效应显著，美国企业垄断核心资源</strong>  <br/>全球融资向技术成熟、具备生态构建能力的企业集中，美国企业占据绝对主导地位，中国企业难获大额融资。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525083" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算投融资数据库（2024），统计范围为全球量子计算硬件、软件、云平台相关企业公开融资事件。  <br/>量子计算企业融资额图8数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球融资20.15亿美元，美国企业占比62.7%，PsiQuantum以6.246亿美元获最大轮融资，Quantinuum、Q-CTRL紧随其后；中国最大单笔融资仅0.1亿美元，且集中在硬件领域，软件企业融资困难。  <br/>对应人群行动建议：中国初创企业需聚焦细分技术痛点（如低温测控、量子纠错）建立壁垒，避免与美国巨头正面竞争；投资者可关注“硬件+软件”一体化布局的中国企业，降低单一环节风险，同时警惕纯硬件企业的技术迭代风险。</p><h4><a name="t13" target="_blank"/>5. 量子技术专利数量</h4><p><strong>知识产权的全球博弈，中国基础专利差距明显</strong>  <br/>专利数量反映国家技术积累，美国仍占据绝对优势，中国在基础专利上的短板可能制约长期发展。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525084" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为Patsnap量子技术专利分析报告（截至2024年12月），统计范围为全球量子计算核心技术相关授权专利。  <br/>量子技术专利数量图9数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：截至2024年，美国量子技术专利18649件，基础专利占比超40%；中国7601件，基础专利仅占15%，多为应用层专利；日本、德国分别以9400件、8500件位列第二、三位，基础专利布局均优于中国。  <br/>对应人群行动建议：国内企业需加强核心技术专利布局，避免陷入“低端专利陷阱”，重点突破量子芯片、量子纠错等基础领域专利；科研机构可聚焦基础理论与核心器件专利突破，提升行业话语权，减少对美国基础专利的依赖。</p><h3><a name="t14" target="_blank"/>三、应用场景落地：从实验室到产业的跨越，中美应用深度差距显著</h3><p>量子计算的终极价值在于产业赋能，当前已在多个领域展现出落地潜力，但美国在应用深度与广度上已形成优势，中国仍处于试点阶段。</p><h4><a name="t15" target="_blank"/>1. 计算加速倍数</h4><p><strong>效率革命的开始，美国应用场景更广泛</strong>  <br/>量子计算在特定场景实现指数级加速，但美国已在多领域形成规模化应用，中国仍以科研试点为主。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525085" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为麦肯锡《The state of AI in 2025: Agents, innovation, and transformation》，加速倍数为量子计算与经典超级计算机在相同任务下的效率对比。  <br/>计算加速倍数图10数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：基因组组装加速1000倍，美国已应用于生物医药企业的新药研发；AI模型训练加速15.2倍，美国谷歌、微软已用于大模型优化；电力故障定位加速1.34倍，中国仅在个别电力企业试点，应用范围有限。  <br/>对应人群行动建议：中国生物医药企业可优先布局基因组组装、药物研发场景，依托“九章三号”光量子计算机的局部优势快速验证价值；能源企业可聚焦电力优化等轻量级应用，逐步探索深度融合，避免盲目跟风美国的大规模应用。</p><h4><a name="t16" target="_blank"/>2. 预测准确率提升</h4><p><strong>决策质量的提升，中国在核心场景应用滞后</strong>  <br/>量子算法显著提升预测准确率，但中国在金融、医药等核心场景的应用滞后美国3-5年。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525086" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为量子前哨《量子技术赋能金融风控与定价管理白皮书》，测试场景包含金融风控、生物制药分子对接等核心应用领域。  <br/>预测准确率提升图11数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：金融风控准确率提升18.5%，美国摩根大通、花旗已用于信贷风控和衍生品定价；生物制药分子对接提升14.3%，美国辉瑞、阿斯利康已纳入药物研发流程；中国仅个别头部企业开展POC测试，尚未规模化应用。  <br/>对应人群行动建议：中国金融机构可先在信贷风控、衍生品定价场景试点，依托量子云平台降低投入成本；医药企业可与量子计算公司合作开展药物分子模拟，快速验证价值，避免在核心场景被美国企业拉开代差。</p><h4><a name="t17" target="_blank"/>3. 后量子密码市场规模</h4><p><strong>应对量子安全威胁，中国PQC迁移进展缓慢</strong>  <br/>量子计算的发展带来密码安全风险，后量子密码（PQC）成为刚需，但中国PQC迁移进展滞后于美国和欧盟。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525087" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为朗空量子《全球抗量子迁移战略白皮书（2025）》，市场规模预测基于全球关键基础设施PQC迁移需求测算。  <br/>后量子密码市场规模图12数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：后量子密码市场规模将随量子计算成熟度同步增长，2030年将突破百亿规模；美国已完成关键基础设施PQC试点，欧盟2030年将全面完成迁移，中国仍处于标准制定阶段，迁移进度滞后2-3年。  <br/>对应人群行动建议：中国政府、金融、电信等关键基础设施行业需加快PQC迁移规划，避免“量子威胁”冲击；企业可先开展密码系统风险评估，依托国内PQC技术企业开展试点，降低对国外算法的依赖。</p><h4><a name="t18" target="_blank"/>4. NIST算法公钥长度比较</h4><p><strong>技术适配的关键，中国算法适配能力不足</strong>  <br/>不同PQC算法公钥长度差异显著，影响设备适配性与传输效率，中国在算法适配与设备兼容上仍落后。  </p><p>注释：数据来源为NIST后量子密码标准化项目（Round 4）测试数据，公钥长度为各算法标准实现的平均长度。  <br/>NIST算法公钥长度比较图13数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：NIST标准化PQC算法中，美国企业已实现全场景适配，支持从物联网设备到金融核心系统的全覆盖；中国仅能适配部分场景，短公钥长度算法在物联网设备的兼容性不足，长公钥算法在金融系统的传输效率问题未解决。  <br/>对应人群行动建议：中国物联网企业可优先选择短公钥长度算法（如SLH-DSA），与国内PQC企业联合优化兼容性；金融机构等核心场景可选用高安全性算法（如CRYSTALS-Kyber），同时加大传输效率优化投入，平衡安全与效率。</p><h4><a name="t19" target="_blank"/>5. 迁移政策时间线</h4><p><strong>全球PQC迁移协同推进，中国需加快节奏</strong>  <br/>各国明确PQC迁移时间表，形成全球协同防控量子安全风险的格局，中国迁移目标虽与美国一致，但执行力度需加强。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525088" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为信通院《量子信息领域的国家战略布局与研发趋势分析》，迁移目标完成年份为各国官方发布的量子安全相关政策明确时间。  <br/>迁移政策时间线图14数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：美国、中国计划2035年完成PQC迁移，欧盟提前至2030年；美国已出台分阶段迁移细则，明确各行业责任主体；中国虽明确目标，但缺乏具体执行方案，关键基础设施行业的迁移动力不足。  <br/>对应人群行动建议：中国跨国企业需按区域政策要求制定分阶段迁移计划，避免因合规问题影响海外业务；国内企业可依托国家级专项推进自主PQC技术落地，积极参与国际标准制定，提升话语权。</p><h3><a name="t20" target="_blank"/>四、风险提示与应对方案</h3><p>量子计算产业看似前景光明，但暗藏多重“陷阱”，中国企业需比美国更谨慎应对，避免在技术迭代与生态竞争中被淘汰：</p><h4><a name="t21" target="_blank"/>1. 技术路线收敛风险</h4><p><strong>风险描述</strong>：当前技术路线未收敛，美国已形成“超导+中性原子”双主线布局，中国企业多押注单一路线，可能面临“满盘皆输”的风险，如早期光量子路线企业因技术瓶颈陷入发展困境。  <br/><strong>具体应对方案</strong>：采用“主线+支线”布局策略，核心业务聚焦主流路线（如超导），同时小比例投入潜力路线（如中性原子）；加入行业联盟，实时跟踪美国技术路线演进动态，每半年评估一次路线优先级，避免与美国主流路线偏离。  <br/><strong>社群支持</strong>：交流群定期分享美国技术路线进展报告，组织技术选型闭门会，邀请光子盒研究院专家提供定制化建议，对接上下游企业资源，帮助中国企业快速调整技术方向。</p><h4><a name="t22" target="_blank"/>2. 成本高企风险</h4><p><strong>风险描述</strong>：量子计算机硬件成本动辄数亿元，运维成本（如极低温环境）高昂，美国企业可通过生态协同分摊成本，中国中小企业难以承受，单台超导量子计算机年运维成本超千万元，远超美国企业的600万元。  <br/><strong>具体应对方案</strong>：优先采用量子云平台（如中电信“天衍”平台）按需付费，避免重资产投入；聚焦细分场景的轻量化应用，降低算力需求，控制初期投入规模；联合高校、科研机构共享设备资源，分摊运维成本。  <br/><strong>社群支持</strong>：整理全球量子云平台对比手册，重点标注中美平台成本差异，提供中国平台优惠资源对接，组织中小企业量子计算应用试点对接会，帮助企业降低试点成本。</p><h4><a name="t23" target="_blank"/>3. 政策合规风险</h4><p><strong>风险描述</strong>：量子技术涉及国家安全，各国政策限制（如禁运、出口管制）日益严格，欧美已将稀释制冷机、量子芯片等纳入禁运清单，中国企业核心器件进口难度加大，技术迭代受阻。  <br/><strong>具体应对方案</strong>：国内企业加强自主化研发，重点突破稀释制冷机、测控系统等“卡脖子”环节，减少核心器件进口依赖；建立供应链风险预警机制，提前储备替代资源；避免违规合作，优先选择政策友好区域布局海外业务。  <br/><strong>社群支持</strong>：及时更新全球量子技术政策数据库，重点标注美国对华技术封锁清单，提供合规咨询对接服务，组织政策解读直播，帮助企业把握政策导向与机遇，规避合规风险。</p><h3><a name="t24" target="_blank"/>五、可落地的3件事</h3><ol><li>开展“量子就绪”评估：梳理企业核心业务中的计算瓶颈，对比中美应用场景差异，判断是否适合量子计算赋能，优先选择美国已验证、中国有技术基础的场景（如金融风控、药物研发），形成《量子应用潜力评估报告》，避免盲目跟风。</li><li>小步试点验证价值：与国内量子云平台合作开展POC（概念验证），投入少量资源测试量子算法效果，比如金融企业可试点量子组合优化算法优化投资组合，医药企业可测试量子模拟加速药物分子筛选，重点验证国产技术的可行性，避免过度依赖美国平台。</li><li>储备量子人才与专利：招聘具备量子计算基础的技术人员，或对现有团队开展量子技术培训（如参加量旋科技“量子计算实训营”），建立人才护城河；同时对接高校量子信息专业，搭建校企人才输送通道，加强核心技术专利布局，尤其是基础专利，减少对美国专利的依赖。</li></ol><h3><a name="t25" target="_blank"/>六、核心数据表格</h3><h4><a name="t26" target="_blank"/>1. 主要技术路线核心性能指标表</h4><table><thead><tr><th>技术路线</th><th>量子比特数</th><th>单比特门保真度</th><th>相干时间</th><th>单量子门操作时间</th><th>代表企业/设备（美国）</th><th>代表企业/设备（中国）</th></tr></thead><tbody><tr><td>超导</td><td>105-176</td><td>99.5%-99.9%</td><td>400-500微秒</td><td>40纳秒</td><td>谷歌Willow</td><td>中国“祖冲之三号”</td></tr><tr><td>离子阱</td><td>32-100</td><td>99.8%-99.99%</td><td>5-10毫秒</td><td>100微秒</td><td>Quantinuum H2-1、IonQ Forte</td><td>华翊量子HYQ-A37、幺正量子UQM1</td></tr><tr><td>光量子</td><td>216</td><td>98.0%-99.0%</td><td>50-100微秒</td><td>10微秒</td><td>Xanadu Borealis</td><td>中国“九章三号”</td></tr><tr><td>中性原子</td><td>200-256</td><td>99.0%-99.5%</td><td>800微秒-1秒</td><td>1微秒</td><td>QuEra Aquila</td><td>中科酷原“汉原一号”</td></tr></tbody></table><h4><a name="t27" target="_blank"/>2. 全球量子计算市场规模预测表（单位：亿美元）</h4><table><thead><tr><th>年份</th><th>全球市场规模</th><th>美国市场规模</th><th>中国市场规模</th><th>美国占比</th><th>中国占比</th><th>中国年复合增长率</th></tr></thead><tbody><tr><td>2024</td><td>50.37</td><td>31.58</td><td>12.74</td><td>62.70%</td><td>25.30%</td><td>-</td></tr><tr><td>2027</td><td>111.75</td><td>69.90</td><td>30.00</td><td>62.55%</td><td>26.84%</td><td>29.5%</td></tr><tr><td>2030</td><td>2199.78</td><td>1389.26</td><td>615.00</td><td>63.16%</td><td>27.96%</td><td>173.2%</td></tr><tr><td>2035</td><td>8077.50</td><td>5153.00</td><td>2382.00</td><td>63.80%</td><td>29.49%</td><td>29.8%</td></tr></tbody></table><h4><a name="t28" target="_blank"/>3. 量子计算应用场景价值表</h4><table><thead><tr><th>应用场景</th><th>加速倍数</th><th>准确率提升</th><th>落地周期</th><th>美国落地状态</th><th>中国落地状态</th><th>产业估值（2035年，亿美元）</th></tr></thead><tbody><tr><td>基因组组装</td><td>1000倍</td><td>-</td><td>3-5年</td><td>规模化应用</td><td>科研试点</td><td>-</td></tr><tr><td>AI模型训练</td><td>15.2倍</td><td>-</td><td>5-8年</td><td>企业试点</td><td>实验室阶段</td><td>-</td></tr><tr><td>电力故障定位</td><td>1.34倍</td><td>-</td><td>2-3年</td><td>行业应用</td><td>个别试点</td><td>-</td></tr><tr><td>金融风控</td><td>-</td><td>18.5%</td><td>3-5年</td><td>规模化应用</td><td>POC测试</td><td>7000（乐观估值）</td></tr><tr><td>生物制药分子对接</td><td>-</td><td>14.3%</td><td>5-8年</td><td>企业应用</td><td>科研合作</td><td>1830（乐观估值）</td></tr></tbody></table><h4><a name="t29" target="_blank"/>4. 主要经济体PQC迁移政策表</h4><table><thead><tr><th>经济体</th><th>迁移目标完成年份</th><th>核心要求</th><th>重点领域</th><th>执行进度</th></tr></thead><tbody><tr><td>美国</td><td>2035年</td><td>禁用传统密码算法，强制采用NIST标准化PQC算法</td><td>国防、金融、电信</td><td>分阶段执行中</td></tr><tr><td>中国</td><td>2035年</td><td>自主PQC技术落地，关键基础设施率先完成迁移</td><td>金融、能源、政务</td><td>标准制定阶段</td></tr><tr><td>欧盟</td><td>2030年</td><td>关键基础设施完成PQC升级，建立跨境互认机制</td><td>能源、交通、医疗</td><td>全面推进中</td></tr></tbody></table><h3><a name="t30" target="_blank"/>七、数据图表列表</h3><ol><li>量子比特数量对比图1.pdf</li><li>量子门保真度对比图2.pdf</li><li>量子相干时间对比图3.pdf</li><li>量子门操作时间对比图4.pdf</li><li>全球量子计算市场规模预测图5.pdf</li><li>中国量子计算融资规模图6.pdf</li><li>全球量子计算产业规模预测图7.pdf</li><li>量子计算企业融资额图8.pdf</li><li>量子技术专利数量图9.pdf</li><li>计算加速倍数图10.pdf</li><li>预测准确率提升图11.pdf</li><li>后量子密码市场规模图12.pdf</li><li>NIST算法公钥长度比较图13.pdf</li><li>迁移政策时间线图14.pdf</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t31" target="_blank"/>本专题内的参考报告（PDF）目录</h3><p>⦁    PQC-X实验室：全球金融银行业后量子安全迁移白皮书（2025）.pdf  <br/>⦁    2026-01-05 20:34  <br/>⦁    2025年量子技术：健康与医疗保健领导者的战略要务报告.pdf  <br/>⦁    2026-01-03 10:50  <br/>⦁    朗空量子：全球抗量子迁移战略白皮书（2025）.pdf  <br/>⦁    2025-12-31 15:52  <br/>⦁    2025年全球量子生态全景洞察报告：基于创新、企业、投资、技能、贸易及政策数据的综合研究（英文版）.pdf  <br/>⦁    2025-12-29 15:59  <br/>⦁    量子计算行业深度：行业概况、发展趋势、产业链及相关公司深度梳理.pdf  <br/>⦁    2025-12-29 15:53  <br/>⦁    软件与服务行业量子信息技术专题研究报告（二）：科技巨头加速布局，量子产业前景可期.pdf  <br/>⦁    2025-12-24 15:30  <br/>⦁    量子位；2025年度AI十大趋势报告.pdf  <br/>⦁    2025-12-17 16:16  <br/>⦁    量子信息技术发展与应用研究报告（2025年）-中国信通院.pdf  <br/>⦁    2025-12-16 16:29  <br/>⦁    长江证券：量子计算：从“量子优越性”到产业优越性.pdf  <br/>⦁    2025-12-04 16:45  <br/>⦁    ATARC：2025年揭秘当今与未来量子技术能力白皮书汇编（英文版）.pdf  <br/>⦁    2025-11-30 09:17  <br/>⦁    量子科技行业深度报告：量子科技驱动产业变革，激活经济增长新引擎.pdf  <br/>⦁    2025-11-30 09:11  <br/>⦁    2025版量子计算 生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-11-24 15:06  <br/>⦁    2025量子计算+生物制药产业与技术发展研究报告.pdf  <br/>⦁    2025-11-19 15:28  <br/>⦁    2025量子技术：先进制造和供应链的关键机遇白皮书（英文版）.pdf  <br/>⦁    2025-11-08 17:47  <br/>⦁    量子科技行业深度报告：量子革命，量子科技的现状与未来.pdf  <br/>⦁    2025-10-30 15:17  <br/>⦁    2025年量子计算驱动的电力系统弹性提升-探索与展望报告.pdf  <br/>⦁    2025-10-29 16:27  <br/>⦁    海通国际：量子科技行业深度报告：量子革命：量子科技的现状与未来.pdf  <br/>⦁    2025-10-28 16:17  <br/>⦁    电子行业深度报告：量子深潜-计算篇：从比特到Qubit的范式转移.pdf  <br/>⦁    2025-10-26 08:49  <br/>⦁    量子计算硬件深度报告：行业奇点将至，硬件破局当时.pdf  <br/>⦁    2025-10-16 15:09  <br/>⦁    中航证券：量子信息：引领未来全球科技变革之关键力量.pdf  <br/>⦁    2025-10-15 15:17  <br/>⦁    中移智库：移动网络中量子计算应用能力评估模型（2025年）.pdf  <br/>⦁    2025-09-30 16:39  <br/>⦁    中国信通院：量子计算发展态势研究报告（2025年）.pdf  <br/>⦁    2025-09-26 14:27  <br/>⦁    2025量子信息行业研究报告.pdf  <br/>⦁    2025-09-21 17:17  <br/>⦁    计算机行业深度研究：后量子密码技术：应对量子计算威胁的关键防线.pdf  <br/>⦁    2025-09-17 16:27  <br/>⦁    后量子密码学（PQC）测试研究白皮书.pdf  <br/>⦁    2025-09-12 16:36  <br/>⦁    麻省理工学院：2025年量子指数报告（英文版）.pdf  <br/>⦁    2025-09-03 16:55  <br/>⦁    量子计算专题：下一代计算革命，关注核心设备环节.pdf  <br/>⦁    2025-09-01 16:24  <br/>⦁    AI Coding玩家图谱【量子位智库】.pdf  <br/>⦁    2025-08-31 17:47  <br/>⦁    未来网络发展大会：2025量子互联网与算网协同体系架构白皮书.pdf  <br/>⦁    2025-08-23 17:17  <br/>⦁    2025中国量子计算产业市场现状及发展前景研究报告.pdf  <br/>⦁    2025-08-16 16:49  <br/>⦁    2025年全球科技行业：量子计算将如何影响AI发展？（英文版）.pdf  <br/>⦁    2025-08-05 15:31  <br/>⦁    量子位智库：2025上半年AI核心成果及趋势报告.pdf  <br/>⦁    2025-08-02 16:21  <br/>⦁    2025年全球量子计算新进展深度分析报告.pdf  <br/>⦁    2025-08-02 16:16  <br/>⦁    2025年中国联通后量子密码白皮书-中国联通.pdf  <br/>⦁    2025-07-30 16:14  <br/>⦁    量子位智库：2025年AI+游戏产业变革研究报告.pdf  <br/>⦁    2025-07-17 15:53  <br/>⦁    麦肯锡：量子之年：从2025年从概念到现实报告（英文版）.pdf  <br/>⦁    2025-07-13 08:36  <br/>⦁    后量子密码技术白皮书（2025）-东进技术.pdf  <br/>⦁    2025-07-07 16:52  <br/>⦁    2024年量子技术在金融通信安全领域的应用研究报告.pdf  <br/>⦁    2025-07-02 16:38  <br/>⦁    浙商证券-量子科技行业深度报告：超越经典，面向未来.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    政策与战略专题报告：量子科技：产业革命核心赛道，投资风口将至.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    2024年量子传感在位置、导航和定时应用中的案例（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    2024年量子技术在金融消息传递中的应用报告（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    应对量子威胁：SIM体系抗量子密码迁移白皮书（2025年）.pdf  <br/>⦁    2025-06-19 16:02  <br/>⦁    量子算法在金融风控与定价管理领域的应用研究.pdf  <br/>⦁    2025-06-18 15:27  <br/>⦁    2024年量子计算在交通运输与物流领域的应用研究报告（英文版）.pdf  <br/>⦁    2025-06-12 15:35  <br/>⦁    通信行业动态报告：量子计算光量子技术路线进展加速，未来大有可为.pdf  <br/>⦁    2025-06-12 15:34  <br/>⦁    量子位智库：2025大模型架构创新研究报告.pdf  <br/>⦁    2025-06-06 15:38  <br/>⦁    量子位智库：2025年AI眼镜「预选赛」格局报告.pdf  <br/>⦁    2025-06-05 16:09  <br/>⦁    鼎帷咨询：2025年美国量子技术发展研究报告.pdf  <br/>⦁    2025-06-02 08:57  <br/>⦁    2025年全球量子计算用同轴电缆市场分析报告-光子盒研究院.pdf  <br/>⦁    2025-05-26 16:58  <br/>⦁    量子位智库：2025年AI智能助手的SEO策略变革研究报告.pdf  <br/>⦁    2025-05-24 16:38  <br/>⦁    2025美韩科技合作报告：电池、生物技术与量子技术（英文）.pdf  <br/>⦁    2025-05-20 17:05  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年营养健康食品专题报告.pdf  <br/>⦁    2025-05-17 16:13  <br/>⦁    2025年量子技术与未来学习研究报告（英文版）.pdf  <br/>⦁    2025-05-14 16:34  <br/>⦁    Globant：2024年科技趋势报告-人工智能、量子技术、机器人等将如何塑造未来一年（英文版）.pdf  <br/>⦁    2025-05-01 17:54  <br/>⦁    量子计算：打破维度瓶颈，开启化学的“算力革命”.pdf  <br/>⦁    2025-04-29 15:55  <br/>⦁    量子位智库：2025年空间智能研究报告.pdf  <br/>⦁    2025-04-28 17:23  <br/>⦁    量子信息网络产业联盟：2025年光量子计算技术产业研究报告.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    量子信息网络产业联盟：量子计算云平台接口研究报告（2024）.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    2025年量子密钥无线分发技术研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年量子计算应用能力指标与测评研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年经典计算与多制式量子计算异构融合研究报告.pdf  <br/>⦁    2025-04-26 14:27  <br/>⦁    量子信息技术应用案例集（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    量子信息技术产业发展研究报告（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    2025版量子计算+生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-04-21 10:06  <br/>⦁    量子位智库：2025年中国AIGC应用全景图谱报告..pdf  <br/>⦁    2025-04-19 14:49  <br/>⦁    量子计算行业深度：市场现状、发展趋势、产业链及相关企业深度梳理.pdf  <br/>⦁    2025-03-26 15:33  <br/>⦁    2025年全球量子技术专利态势分析白皮书（英文版）.pdf  <br/>⦁    2025-03-13 17:11  <br/>⦁    光子盒：2025年全球量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-12 15:49  <br/>⦁    光子盒：2025年量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-05 15:24  <br/>⦁    中国在量子领域有多大创新性？.pdf  <br/>⦁    2025-03-04 16:09  <br/>⦁    光子盒：2025年全球量子传感产业发展展望报告.pdf  <br/>⦁    2025-03-01 16:55  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:38  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:37  <br/>⦁    光子盒：2025年全球量子计算产业发展展望报告.pdf  <br/>⦁    2025-02-27 14:57  <br/>⦁    2024年量子安全威胁及其对国内金融行业的影响研究报告.pdf  <br/>⦁    2025-02-18 15:53  <br/>⦁    2025年拥抱量子经济：企业领袖的前进之路洞察报告（英文版）.pdf  <br/>⦁    2025-01-22 16:12  <br/>⦁    量子位智库：智能驾驶2024年度报告.pdf  <br/>⦁    2025-01-17 13:14  <br/>⦁    量子信息技术国内外标准化进展报告（2024）.pdf  <br/>⦁    2025-01-15 15:49  <br/>⦁    ITIF：2023年美国的量子政策方针研究报告（英文版）.pdf  <br/>⦁    2025-01-13 10:17  <br/>⦁    2024年量子计算性能评估基准研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    2024年基于量子安全的分布式容错云存储应用场景研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    量子信息技术发展与应用研究报告（2024年）.pdf  <br/>⦁    2024-12-28 16:56  <br/>⦁    移动网络中量子计算应用能力评测白皮书1.0（2024 年）.pdf  <br/>⦁    2024-12-26 15:46  <br/>⦁    2024年量子技术研究报告：投资于拐点（英文版）.pdf  <br/>⦁    2024-12-24 17:14  <br/>⦁    2024年度AI十大趋势报告-量子位.pdf  <br/>⦁    2024-12-14 15:10  <br/>⦁    量子安全技术蓝皮书2024.pdf  <br/>⦁    2024-12-09 16:56  <br/>⦁    量子位智库：2024年大模型落地与前沿趋势研究报告.pdf  <br/>⦁    2024-12-08 16:33  <br/>⦁    2024年量子计算与人工智能：无声的革命报告.pdf  <br/>⦁    2024-12-01 20:55  <br/>⦁    量子位智库：Robotaxi2024年度格局报告.pdf  <br/>⦁    2024-11-30 20:19  <br/>⦁    2023全球量子政策研究报告-光子盒.pdf  <br/>⦁    2024-11-12 16:46  <br/>⦁    量子技术助力社会_实现可持续发展目标.pdf  <br/>⦁    2024-10-19 16:36  <br/>⦁    世界经济论坛：2024年量子技术助力社会：实现可持续发展目标报告（英文版）.pdf  <br/>⦁    2024-10-19 16:30  <br/>⦁    2024中国量子计算应用潜力洞察报告.pdf  <br/>⦁    2024-10-10 15:21  <br/>⦁    2024年AI大模型创业格局报告-量子位智库.pdf  <br/>⦁    2024-10-06 15:17  <br/>⦁    AI教育硬件全景报告【量子位智库】.pdf  <br/>⦁    2024-09-30 15:14  <br/>⦁    量子计算发展态势研究报告（2024年）-中国信通院.pdf  <br/>⦁    2024-09-27 15:55  <br/>⦁    三未信安：抗量子密码技术与应用白皮书（2024）.pdf  <br/>⦁    2024-09-15 15:20  <br/>⦁    光子盒：2024上半年全球量子计算产业发展展望报告.pdf  <br/>⦁    2024-09-14 16:42  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年市场白皮书.pdf  <br/>⦁    2024-09-14 16:39  <br/>⦁    欧洲专利局：2023年量子计算洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:21  <br/>⦁    欧洲专利局：2023年量子模拟洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:20  <br/>⦁    2024量子计算技术全景报告-星河智源.pdf  <br/>⦁    2024-09-05 16:36  <br/>⦁    利亚德&amp;赛富乐斯半导体：2024年T003-量子点（QD-mLED）直显解决方案白皮书.pdf  <br/>⦁    2024-08-31 17:26  <br/>⦁    甲子大脑全球首发：以量子人工智能重新定义智库.pdf  <br/>⦁    2024-08-30 17:46  <br/>⦁    麦肯锡数字量子技术监测.pdf  <br/>⦁    2024-08-27 16:28  <br/>⦁    iCV TA&amp;K：2024年全球量子独角兽企业发展概览报告（英文版）.pdf  <br/>⦁    2024-08-27 16:18  <br/>⦁    光子盒：2024全球量子产业发展现状及展望报告.pdf  <br/>⦁    2024-08-18 17:30  <br/>⦁    尺度定律科普报告【量子位智库】 .pdf  <br/>⦁    2024-08-04 20:05  <br/>⦁    AI视频生成研究报告（2024年）-量子位.pdf  <br/>⦁    2024-07-30 16:30  <br/>⦁    2024中国具身智能创投报告-量子位智库.pdf  <br/>⦁    2024-07-27 17:08  <br/>⦁    AI音乐应用产业报告【量子位智库】.pdf  <br/>⦁    2024-07-22 16:40  <br/>⦁    计算机行业量子科技：见微知著、革故鼎新-国投证券.pdf  <br/>⦁    2024-07-17 10:39  <br/>⦁    光子盒：2024争夺量子优势的芬兰-国家量子战略的政策建议报告.pdf  <br/>⦁    2024-07-10 11:25  <br/>⦁    数据创新中心：2023美国的量子政策报告（英文版）.pdf  <br/>⦁    2024-07-04 11:00  <br/>⦁    赛迪报告：电子信息研究2024年第1期（总第95期）《量子产业发展白皮书》.pdf  <br/>⦁    2024-07-01 09:31  <br/>⦁    ...】2023年中国中老年市场白皮书-中老年服务及产品 “人-货-场”三维解析-CIC灼识咨询&amp;量子之歌.pdf  <br/>⦁    2024-06-28 10:40  <br/>⦁    头豹研究院-企业竞争图谱：2024年量子计算 头豹词条报告系列.pdf  <br/>⦁    2024-06-28 10:39  <br/>⦁    后量子密码迁移白皮书（2024）-西电广研院&amp;LRINF-.pdf  <br/>⦁    2024-06-27 11:20  <br/>⦁    国信证券-海外铜企专题3-第一量子-FM.TO-：高成长性的铜矿公司.pdf  <br/>⦁    2024-06-18 12:51  <br/>⦁    光子盒-量子准备：向后量子密码迁移.pdf  <br/>⦁    2024-06-15 11:02  <br/>⦁    2024上海量子科技产业发展白皮书.pdf  <br/>⦁    2024-06-08 13:03  <br/>⦁    后量子密码应用研究报告（2023年) .pdf  <br/>⦁    2024-06-07 10:11  <br/>⦁    “十五五”时期我国量子产业发展形势研判及思路建议.pdf  <br/>⦁    2024-06-05 10:15  <br/>⦁    西南证券-量子科技专题：量子应用逐步落地，关注政策支持.pdf  <br/>⦁    2024-05-31 14:53  <br/>⦁    量子科技专题系列一：逐梦量子，星辰大海.pdf  <br/>⦁    2024-05-13 13:31  <br/>⦁    通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-05-05 17:55  <br/>⦁    解读新质生产力：量子计算：打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-05-05 17:54  <br/>⦁    计算机：量子加密，一片新蓝海.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    计算机行业深度研究：抢先布局量子信息技术革命.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    2024全球6G技术大会-面向6G时代前沿技术初探：量子信息技术-英文.pdf  <br/>⦁    2024-05-01 11:47  <br/>⦁    面向6G时代前沿技术初探：量子信息技术2024白皮书-29页.pdf  <br/>⦁    2024-05-01 11:44  <br/>⦁    2024年面向6G时代前沿技术初探量子信息技白皮书-全球6G技术大会.pdf  <br/>⦁    2024-04-30 14:38  <br/>⦁    2024量子加密，一片新蓝海.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    2024解读新质生产力：量子计算，打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    中国银河-通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-04-29 12:42  <br/>⦁    抢先布局量子信息技术革命.PDF  <br/>⦁    2024-04-27 10:25  <br/>⦁    量子位：2024中国AIGC应用全景报告.pdf  <br/>⦁    2024-04-26 11:21  <br/>⦁    华鑫证券-量子信息技术行业专题报告：优化运算法则，重塑安全格局.pdf  <br/>⦁    2024-04-20 12:06  <br/>⦁    量子化学方法的开发及其在能源环境材料研究中的应用-赵焱.pdf  <br/>⦁    2024-04-17 10:11  <br/>⦁    量子通信金融应用研究报告.pdf  <br/>⦁    2024-04-11 10:53  <br/>⦁    计算机行业深度报告：量子信息：下一场信息革命.pdf  <br/>⦁    2024-04-07 10:10  <br/>⦁    计算机行业深度研究-量子计算-人工智能与新质生产力的“未来引擎”-民生证券.pdf  <br/>⦁    2024-03-25 14:44  <br/>⦁    计算机行业深度研究：量子计算：人工智能与新质生产力的“未来引擎”.pdf  <br/>⦁    2024-03-24 10:43  <br/>⦁    量子精密测量行业赋能白皮书.pdf  <br/>⦁    2024-03-18 11:08  <br/>⦁    2024量子精密测量产业发展展望.pdf  <br/>⦁    2024-03-06 14:33  <br/>⦁    2024全球量子通信与安全产业发展展望报告-光子盒.pdf  <br/>⦁    2024-02-29 15:40  <br/>⦁    量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-28 11:37  <br/>⦁    北京金融科技产业联盟：2024量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-26 16:13  <br/>⦁    2024全球量子计算产业发展展望.pdf  <br/>⦁    2024-02-22 11:00  <br/>⦁    量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子信息技术产业发展报告（2023年）.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    AI制药深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    存算一体芯片深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    隐私计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    计算生物深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    脑机接口深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    虚拟人深度产业报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    类脑计算神经拟态计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    卫星互联网深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    2021十大前沿科技趋势报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    量子信息网络产业联盟：2024量子人工智能技术白皮书.pdf  <br/>⦁    2024-02-07 15:02  <br/>⦁    量子信息网络产业联盟：2024量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-06 15:05  <br/>⦁    量子信息网络产业联盟：量子信息技术应用案例集（2023年）.pdf  <br/>⦁    2024-02-06 15:04  <br/>⦁    量子信息网络产业联盟：2024量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-05 16:04  <br/>⦁    2023年ARMR技术深度产业报告-量子位智库.pdf  <br/>⦁    2024-01-26 15:18  <br/>⦁    量子位：2024中国AIGC广告营销产业全景报告.pdf  <br/>⦁    2024-01-25 15:09  <br/>⦁    量子十年-2024量子计算未来趋势展望报告第四版-英文版-IBM商业价值研究院.pdf  <br/>⦁    2024-01-24 14:34  <br/>⦁    中国信通院：量子计算发展态势研究报告（2023年）.pdf  <br/>⦁    2024-01-02 14:36  <br/>⦁    量子测量技术发展蓝皮书.pdf  <br/>⦁    2023-12-30 10:11  <br/>⦁    中国信通院：量子信息技术发展与应用研究报告（2023年）.pdf  <br/>⦁    2023-12-29 14:43  <br/>⦁    欧洲量子技术关键绩效指标（2023年9月）（英文版）.pdf  <br/>⦁    2023-12-23 09:52  <br/>⦁    量子位：2023中国AIGC数据标注产业全景报告.pdf  <br/>⦁    2023-12-20 15:12  <br/>⦁    欧洲量子技术关键绩效指标（2023 年 9 月）-英.pdf  <br/>⦁    2023-12-16 15:12  <br/>⦁    赛迪前瞻：应对量子计算挑战需积极推进后量子密码研发和迁移.pdf  <br/>⦁    2023-12-04 15:21  <br/>⦁    应对量子计算挑战需积极推进后量子密码研发和迁移2023-赛迪前瞻.pdf  <br/>⦁    2023-11-24 07:24  <br/>⦁    QIIA：量子计算金融应用白皮书.pdf  <br/>⦁    2023-11-22 18:56  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-21 14:33  <br/>⦁    2023年中国AIGC产业全景报告-量子位.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    2023西班牙量子产业报告英文-Ametic.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    量子信息技术标准化图景(2022)- QIIA.pdf  <br/>⦁    2023-11-10 16:02  <br/>⦁    量子位：2023年中国AIGC产业全景报告.pdf  <br/>⦁    2023-11-10 09:55  <br/>⦁    Y2Q2023量子安全加密之旅报告英文-凯捷.pdf  <br/>⦁    2023-11-09 10:16  <br/>⦁    量子计算金融应用白皮书-QIIA.pdf  <br/>⦁    2023-11-09 10:13  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 16:01  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 11:26  <br/>⦁    Capgemini-Y2Q：量子安全密码之旅【英文版】-2023.pdf  <br/>⦁    2023-11-02 10:25  <br/>⦁    量子计算概念、现状和国会考虑（英）.pdf  <br/>⦁    2023-09-16 09:58  <br/>⦁    中国仿生机器人产业全景报告-量子位智库.pdf  <br/>⦁    2023-08-16 21:47  <br/>⦁    十大AI商业落地趋势-量子位智库.pdf  <br/>⦁    2023-08-16 07:15  <br/>⦁    量子位智库：十大AI商业落地趋势.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    量子位智库：中国仿生机器人产业全景报告.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    2023 AIGC算力全景与趋势报告-量子位.pdf  <br/>⦁    2023-07-26 06:37  <br/>⦁    ChatGPT 实用指南（精编版）（2023）-量子论.pdf  <br/>⦁    2023-04-17 14:37  <br/>⦁    2023全球量子精密测量产业发展展望-量子盒.pdf  <br/>⦁    2023-04-03 10:27  <br/>⦁    2023全球量子通信与安全产业发展展望-光子盒.pdf  <br/>⦁    2023-03-13 17:26  <br/>⦁    2023全球量子精密测量产业发展展望（中）-103页.pdf  <br/>⦁    2023-03-10 09:16  <br/>⦁    AIGC深度产业报告 量子位智库-34页.pdf  <br/>⦁    2023-03-09 10:46  <br/>⦁    量子位2022十大前沿科技报告.pdf  <br/>⦁    2023-03-08 09:52</p>]]></description></item><item>    <title><![CDATA[活动回顾：Arm 龙蜥齐携手，共筑 AI 时代开源 OS 新生态 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525104</link>    <guid>https://segmentfault.com/a/1190000047525104</guid>    <pubDate>2026-01-06 19:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>12 月 11 日下午，在一个温暖和煦的冬日，“龙蜥社区走进系列”之走进 Arm Meetup 在上海成功举办。本次活动吸引了来自云计算、互联网、半导体等领域的众多专家学者齐聚一堂，围绕 Arm 生态、开源社区、AI 基础设施及大模型推理等领域的技术突破，共同探讨了开源操作系统与 Arm Neoverse 平台在人工智能（AI）时代的深度融合与创新实践。</p><p>现场通过一系列主题分享，集中展示了 Arm Neoverse 平台、AI 性能分析工具、异构推理框架及优化等技术成果。这些成果不仅体现了产业链上下游协同创新与开源共建的精神，也加速了 AI 与云计算在 Arm 架构上的落地，为开源操作系统行业提供了更高效、可靠的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525106" alt="图片" title="图片"/><br/>（图/活动现场嘉宾合影）</p><p>活动伊始，阿里云智能集团编译器技术总监、Java 标准委员会委员(JCP-EC)，Java Champion、龙蜥社区 Java 语言与虚拟机 SIG Maintainer 李三红做开场致辞。发言聚焦阿里云在倚天 Arm 架构上的深耕，首先以全栈自研为核心，从芯片、操作系统到编译器全面优化，提升云原生场景的性能与性价比。其次重申对开源的长期投入，积极参与龙蜥社区建设，持续贡献稳定高效的操作系统能力。最后强调通过社区、理事单位和生态伙伴的合作，携手 Arm 共建完整生态，共同推动企业更好地使用 Arm 技术。与此同时，他也鼓励大家在活动中积极交流，共促行业发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525107" alt="图片" title="图片" loading="lazy"/><br/>（图/李三红）</p><p>接下来，来自阿里云、龙蜥社区、趋境科技、鸿钧微电子以及安谋科技的技术和市场专家针对基于 Arm Neoverse 平台，围绕开源龙蜥操作系统 Anolis OS 的各个层面在 AI 浪潮中的变革和演进，进行了精彩的分享与思想碰撞。</p><p>首先由安谋科技云人工智能事业部总监侯科鑫和阿里云智能集团弹性计算高级架构师张先国带来了题为《Arm 基础设施加速云计算智能驾驶》的联合演讲。</p><p>侯科鑫女士回顾了 AI 浪潮中若干行业趋势，强调 AI 的快速发展正在推动基础设施技术的重大转型，Arm 参与其中并重新定义计算。Arm 帮助合作伙伴在通用计算以及智能计算定制化平台上取得了不菲的成绩。Arm Neoverse 已成为这一转型中多个关键领域的首选平台。演讲深入介绍了 Arm Neoverse CSS，以及 Arm 的技术创新是如何帮助合作伙伴在 AI 时代加速产品上市。此外 Arm 在软件生态系统上也持续投入，通过与阿里云等合作伙伴的深度协作，Arm 平台不断推动 AI 基础设施创新，满足智能驾驶行业对高可靠性、弹性扩展和绿色算力的迫切需求，助力企业快速应对 AI 时代的挑战与机遇。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525108" alt="图片" title="图片" loading="lazy"/><br/>（图/侯科鑫）</p><p>张先国则针对智能驾驶场景，重点展示了 Arm 基础设施在智能驾驶场景中的云计算加速能力，围绕智能驾驶全流程——从数据采集、存储、标注，到模型训练、仿真验证及端侧集成，深入剖析行业痛点，包括数据规模庞大、算力消耗高、模型迭代快、训练成本高。针对这些挑战，提出了阿里云弹性计算解决方案，包括高性能、弹性伸缩的 AI 基础设施，支持十万核级资源快速调度，结合容器化算力、Serverless 调度、GPU 切分等创新技术，显著降低运维成本并提升性能（Spark场景优化 10% 以上，MRACC 算子优化 35%以上）。此外，方案还涵盖视频抽帧、点云处理、分布式训练及大规模仿真，为智能驾驶业务提供端到端的高效云端加速能力，助力行业实现 高可靠、低成本、快速迭代的目标。<br/><img width="469" height="313" referrerpolicy="no-referrer" src="/img/bVdnzB5" alt="image.png" title="image.png" loading="lazy"/><br/>（图/张先国）</p><p>安谋科技主任软件工程师方方明做了《RTP-LLM：Arm平台全面支持》的主题演讲。Arm 同龙蜥社区、阿里巴巴在很多技术领域都有深度合作，包括推理引擎。在 AI 时代，基于 Arm CPU，团队实现了对阿里巴巴大模型推理引擎 RTP-LLM 的全面支持，通过与生态伙伴的深度合作，RTP-LLM 不仅实现了对倚天等云端 CPU 的高效适配，还支持主流大模型（如 Qwen、Llama、DeepSeek、Bert 等）及多种量化格式（FP32、FP16、INT8、INT4、GPTQ 等），并集成了 Arm KleidiAI 等高性能 AI 内核库，极大提升了推理效率和灵活性。详细的技术亮点包括：1. 利用 Arm 的加速指令对算子的极致性能优化；2. 全面支持 MoE 架构（如 DeepSeek V3、Qwen3），使用专家融合提高并行计算能力，MoE 推理性能最高提升 4 倍；3. 多种量化与矩阵乘优化方案，显著降低内存占用并提升算力利用率；4. 端到端测试与高效部署，支持云到边多场景落地；5. 性能对比显示，RTP-LLM 在 Arm 平台上推理速度和资源效率均优于同类方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525109" alt="图片" title="图片" loading="lazy"/><br/>（图/方方明）</p><p>随着 AI 时代的到来，Java 在 AI 相关负载中扮演越来越重要的角色。阿里云智能集团高级 JVM 工程师邢其正做了《阿里巴巴 Dragonwell JDK：为 AI 时代而生》的主题演讲。阿里云推出的 Dragonwell 21 AI 增强版，包含 Native 加速、热代码重排和 JTune 三大核心技术。具体来说：</p><ul><li>Native 加速：通过高度优化的原生实现，显著提升 AI 相关计算性能，远超传统 JNI 方案，助力 Elasticsearch 向量搜索、Spark 等场景性能提升 18%-60%。</li><li>热代码重排：智能管理 JVM Code Cache，提升 JIT 编译代码的命中率和运行效率。</li><li>智能调优：AI驱动的自动调优框架，降低JVM参数复杂度，实现更高效的资源利用和运维自动化。</li></ul><p>Dragonwell JDK 不仅让 Java 在 RAG、大数据、智能驾驶等 AI 场景下实现性能飞跃，还兼顾企业级系统集成与运维需求，推动 Java 成为 AI 时代的主流生产力工具。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525110" alt="图片" title="图片" loading="lazy"/><br/>（图/邢其正）</p><p>ModelSight 是龙蜥社区自研的 AI 性能分析工具，基于 eBPF 实现 GPU、CPU、框架事件一体化观测，实现 AI 场景下端到端的性能诊断。阿里云智能集团技术专家、龙蜥社区智算基础设施联盟委员王鹏和常怀鑫联合带来了题为《ModelSight：端到端 AI 性能分析框架》的演讲。两位嘉宾分享了如何利用 ModelSight 对 235B 参数的 Qwen3 推理链路进行压测、热点定位与瓶颈可视化，并结合 TP/PP/EP 并行策略在 SGLang 框架中的落地，最终实现 2.12 倍性能提升。通过优化 SGLang overlap schedule，TTFT（首 Token 响应时间）平均提升 20%+。ModelSight 让 AI 性能分析更智能、更高效，助力企业迈向 AI 时代算力极致优化！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525111" alt="图片" title="图片" loading="lazy"/></p><p>鸿钧微电子产品营销经理吴喆就《鸿钧微电子开源社区实践》话题进行了深度分享。他介绍了鸿钧微电子基于 Arm Neoverse 平台的服务器 CPU 产品的规格特点与主要适配的应用场景，并详细阐述其开源社区策略：积极拥抱开源、服务社区；取之于社区，也回馈社区。基于龙蜥操作系统进行芯片验证、驱动适配与性能优化，并向 Linux kernel、Qemu 等社区贡献多项驱动与功能补丁。在应用层面，鸿旻处理器在内存数据库 (Redis，Memcached)、视频编解码 (X265)、大数据 (Spark、Flink) 等场景展现出卓越的性能与能效优势，助力 Arm 架构服务器在云计算、AI、大数据等领域实现突破。基于高效能 Arm Neoverse 平台的服务器 CPU，鸿钧微电子将持续推动开源协作与生态繁荣。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525112" alt="图片" title="图片" loading="lazy"/><br/>（图/吴喆）</p><p>KTransformers 专注于大语言模型的高效推理和微调，通过 CPU-GPU 异构计算实现资源受限环境下的大模型部署，探索 Arm CPU+GPU 平台下的本地极致推理和个性化微调方案。趋境科技技术专家、KTransformers 核心开发人员袁子为带来了题为《KTransformers：在 Arm CPU 上实现大模型异构推理》的主题演讲，就 KTransformers 的以下创新亮点进行了细致探讨：支持 DeepSeek、Qwen、GLM、LLaMa 等主流大模型，灵活适配多种硬件平台；创新“Expert Deferral”机制，推理吞吐提升 45%，精度无损；针对 Arm 架构深度优化，NUMA 亲和、SVE/SME 指令集加速，矩阵运算性能提升 1.5倍；支持 LoRA 微调，已集成进 Llama-Factory 和 SGLang，便于本地微调与多 GPU 加速。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525113" alt="图片" title="图片" loading="lazy"/><br/>（图/袁子为）</p><p>最后，安谋科技主任软件工程师刘亮亮就话题《llama.cpp 跨 NUMA 节点部署优化实践》展开详细探讨。刘亮亮介绍了 llama.cpp 在 Arm 架构服务器部署中跨 NUMA 节点的性能问题及优化方案。主要通过以下两种优化手段：通过“分治”优化 GGML barrier 大幅度减少跨 NUMA 节点原子操作；性能瓶颈 MUL_MAT 算子通过 dst_tensor=src0_tensor * wdata_tensor 进行 Tensor 的乘积操作，为了实现 MUL_MAT 算子的 NUMA 感知内存访问，避免跨 NUMA 内存访问。对其中的 src0 Tensor 以及 dst Tensor 进行内存分割，实现处在一个 NUMA 节点中的线程只访问本地 NUMA 内存。而 wdata Tensor 是通过在量化的时候同时在不同的 NUMA 节点计算成基于 NUMA 节点的多副本。该方案已在 Arm Neoverse N2 平台实测，实现了 S_TG t/s提升 55%，S t/s 提升 53.2%，内存带宽分布也更为均衡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525114" alt="图片" title="图片" loading="lazy"/><br/>（图/刘亮亮）</p><p>最后，感谢本次活动各位嘉宾的精彩演讲，也感谢龙蜥社区伙伴及 Arm 工作人员：刘捷、蔡佳丽、吴永霞、倪俊雄（以上排名不分先后）等人的组织与配合，使得本次走进 Arm MeetUp 活动圆满结束。未来，期待龙蜥社区与 Arm 持续深化合作，在 AI 浪潮中共筑开放、创新、可持续的开源操作系统新生态！</p><p>本次 MeetUp 回顾视频及 PPT 后续会陆续上传至龙蜥官网，欢迎大家持续关注。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[迈向云+数据中心的国产 CPU 新引擎，龙蜥大会 RISC-V 分论坛回顾一览 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525125</link>    <guid>https://segmentfault.com/a/1190000047525125</guid>    <pubDate>2026-01-06 19:05:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会在京顺利落幕，由中兴通讯操作系统产品规划总工徐立锋，中国科学院软件研究所工程师、如意 RISC-V 社区运营丁欣，阿里巴巴达摩院高级合作伙伴运营专家朱祯贞，龙蜥社区运营委员会副主席、龙蜥智算基础设施联盟秘书处负责人金美琴联合出品的 RISC-V 分论坛也圆满举办。本论坛汇聚 RISC-V 芯片厂商、云服务提供商及顶尖科研机构代表，以 “软硬协同” 为核心主线，围绕系统层适配优化、芯片与软件协同创新、开源生态标准化三大维度展开深度研讨。通过前沿技术案例分享、行业大咖圆桌对话及 Anolis OS RISC-V 版本特性展示，本论坛成为高效的技术交流与合作平台，充分展示了 RISC-V 架构在服务器、边缘计算等高性能场景的商业化落地，助力构建开放共赢的算力新生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525127" alt="图片" title="图片"/></p><p>会议伊始，阿里云智能集团研发副总裁、龙蜥社区理事长马涛，中兴通讯副总裁赵志勇，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕依次开场致辞。马涛认为 RISC-V 是国产 CPU 自主创新与协同发展的关键战略方向，尽管 RISC-V 已在嵌入式、边缘计算领域取得进展，但在云计算、数据中心方面尚处于初步探索阶段。未来，社区将联合合作伙伴，持续完善 RISC-V 生态和技术标准化，加速其在数据中心和云计算场景的产业化落地，推动 RISC-V 成为高效节能云计算基础设施的关键力量。赵志勇指出 RISC-V 作为开放指令集架构的代表，以及模块化可扩展的天然优势，在服务器、AI、工业控制等核心领域展现出了巨大的潜力。龙蜥社区作为开源生态的重要载体，汇聚了众多的产学研用各种各方的力量，为 RISC-V 技术落地搭建了宝贵的协作平台。这既是开源生态众人拾柴火焰高的生动体现，更是响应国家战略，凝聚产业合力的具体实践。于佳耕表示，中国科学院软件研究所自 2019 年以来，持续深耕 RISC-V 架构的基础软件与操作系统生态建设，通过成立“如意 RISC-V 社区”，聚焦操作系统内核、编译器等关键基础软件的架构适配与优化，有效推动了 RISC-V 基础软件生态的快速成长与可持续发展，也期待与龙蜥社区等合作伙伴携手，共同推进 RISC-V 基础软件生态的繁荣与成熟。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525128" alt="图片" title="图片" loading="lazy"/><br/>从左至右：马涛、赵志勇、于佳耕</p><p>阿里巴巴达摩院高级技术专家王云龙分享了《共建 RISC-V 通用高性能平台标准》，他重点介绍了 RISC-V 架构进军高性能服务器领域所面临的标准化挑战与共建之路。当前，RISC-V 在技术指标上已快速接近主流水平，产业落地加速，但整体生态仍处起步阶段。关键挑战在于：软硬件标准接口标准不够完善、测试标准与测试套件缺失、系统软件存在碎片化风险。为破局，业界正联合推动软硬件兼容性标准的制定，并构建配套测试体系，旨在以玄铁 C930 等重点产品为突破，协同研发与标准，目标在 2026 年推出标杆服务器产品，实现从技术到生态的闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525129" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家王云龙）</p><p>中兴通讯操作系统架构师谈虎分享了《操作系统 RISC-V 生态实践》，本次分享聚焦 RISC-V在通用计算与服务器领域的软件生态建设与实践进展。核心在于依托 RVA23 关键规范，系统推进从操作系统、编译器、基础库到上层应用的全栈支持与优化。目前，龙蜥等社区已推出 RVA23 预览镜像，内核及虚拟化支持持续增强，并在编译器与基础库层面实现了显著的性能提升。同时，基于 OpenStack、Kubernetes 的云平台及 PyTorch 等关键 AI 组件已完成验证，贯通了主流业务场景。展望未来，行业将协同完善 Server Platform 标准与内核特性，并推动 Ubuntu 等主流系统的长期支持，加速 RISC-V 的产业应用落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525130" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统架构师谈虎）</p><p>中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕分享了《RISC-V 高性能基础软件共建生态》。他重点探讨了如何构建开放共赢的 RISC-V 高性能基础软件生态。RISC-V 凭借其开放与模块化特性，正通过 RVA23 等标准增强对 AI、云计算等高性能场景的支持。然而，硬件多样性给软件兼容与优化带来挑战。为应对此，国内正通过中电标协工委会与“如意 RISC-V 社区”等平台，推动标准、测试与应用生态建设，并构建集成关键扩展的操作系统参考版。现场介绍了 Sapling 评估框架，当前生态成熟度得分为 55.72。展望未来，业界计划借鉴成功模式，通过共建技术规范、CI/CD 体系与示范软件栈，并利用硬件试验场加速软件合规与交付，最终推动 RISC-V 生态实现规模化发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525131" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕）</p><p>龙蜥社区系统安全 SIG Owner 徐峥分享了《RISC-V 可信计算技术实践》。他介绍了可信计算的发展及其在 RISC-V 生态中的关键进展。可信计算历经从概念萌芽到与机密计算、AI 安全融合的数个发展阶段，其核心技术包括信任根、可信度量链、计算平台、软件栈与远程证明。当前，RISC-V 正积极融入这一体系，不仅制定了相关安全模型与服务器规范，还支持 dTPM、fTPM 等多种信任根实现方案。现场以 KOS 的 KTrusted 组件为例，展示了其在 RISC-V 平台上成功实现的动态度量、国密支持与远程认证，验证了可信计算技术从理论到实践的闭环。未来，RISC-V 生态将继续深化可信计算标准的落地与应用协同。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525132" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区系统安全 SIG Owner 徐峥）</p><p>中兴通讯 RISC-V 生态技术专家申林分享了《多架构源代码向 RISC-V 的迁移工具研发与迁移优化实践》。本次分享聚焦于 x86/Arm 软件向 RISC-V 架构迁移所面临的挑战与智能化解决方案。随着 RISC-V 生态扩展，多语言代码迁移因底层架构差异、依赖复杂及人才稀缺而成本高昂。为应对此，中兴通讯开发了智能迁移工具，其核心是“扫描-建议-生成-评测”的四层架构，并创新性地结合结构化扫描、Agent 增强与知识库匹配，以精准识别和转换架构相关代码。目前已成功应用于 OpenSSL 等开源项目，推动其原生支持 RISC-V 向量指令。展望未来，我们将持续扩展工具能力，深化 AI 迁移支持并优化性能，以加速整个软件生态向 RISC-V 的高效过渡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525133" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯 RISC-V 生态技术专家申林）</p><p>阿里巴巴达摩院高级技术专家童琪杰分享了《玄铁 RISC-V 在 OpenAnolis 上的进展》。童琪杰介绍了玄铁在高性能 RISC-V 软件生态建设上的全面布局与实践。面向 RISC-V 在高性能计算领域的广阔前景，玄铁正系统性地构建从编译器、虚拟化、固件到操作系统与应用的全栈软件能力。其在虚拟化、IOMMU 支持及高性能诊断技术等方面取得关键突破，并通过对 ISA-L、X264/X265 等关键组件的深度优化，实现了数倍的性能提升。在实际云服务场景中，Redis、MySQL 等应用也获得了显著的性能增益。玄铁已向 Linux 及龙蜥等主流社区贡献了数百个补丁，持续推动内核与生态成熟，致力于实现高性能 RISC-V 的软硬件协同与规模化落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525134" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家童琪杰）</p><p>中国科学院软件研究所工程师戴希铨分享了《RISC-V 架构上 AI 应用实践与探索》。戴希铨介绍了软件所/如意社区在 AI 应用方面的研究进展。他表示，RISC-V 作为开源指令集架构，正以年均 40% 的增速迅猛发展，全球出货量已超 130 亿颗，成为国产芯片突围的关键路径。其开源、模块化与可扩展特性，为 AI 算力创新提供坚实基础，尤其在端侧AI与异构计算中表现突出。通过玄铁 C920 CPU+NPU/TPU 协同，如意 OS 平台已成功移植 PyTorch、vLLM 等框架，支撑 DeepSeek、Qwen 等大模型在 AI PC、AI 教育等场景本地化运行，实现数据不出端、隐私有保障。典型应用如 AI 公文写作、多模态助手、智能教学等，验证了 RISC-V 在高性能 AI 推理上的可行性，正加速构建 RISC-V 的 AI 软硬件生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525135" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所工程师戴希铨）</p><p>中国电信研究院先进计算中心研究员崔恩放分享了《RISC-V 云计算：AI 智能体算力基础设施新路径》。他系统介绍了中国电信在 RISC-V 云计算领域的完整实践路径。自 2022 年启动研究以来，团队先后实现了云原生虚拟机、操作系统适配等关键突破，并于 2024 年建成了拥有数千核心的“北海”RISC-V 云计算平台。目前，该平台已支撑视频转码、AI 大模型等多元场景的生态建设。在技术层面，通过定制 TeleVM 轻量虚拟机，实现了大幅度的内存与启动开销优化；提出的 AgentDNS 服务发现系统更获得了国际认可。这些成果标志着 RISC-V 正从技术验证走向规模化的云服务商用部署。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525136" alt="图片" title="图片" loading="lazy"/><br/>（图/中国电信研究院先进计算中心研究员崔恩放）</p><p>会上，由阿里云智能集团高级开发工程师、龙蜥社区 RISC-V SIG Maintainer 田瑞冬主持，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕，阿里巴巴达摩院高级技术专家王云龙，中兴通讯操作系统架构师谈虎，中国电信研究院先进计算中心研究员崔恩放共同参与了主题为“高性能 RISC-V 算力生态展望”的圆桌讨论，围绕 RISC-V 架构在高性能计算和 AI 领域的未来发展方向、面临的挑战与机遇展开深度探讨。中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕表示，尽管面临生态不成熟、软件兼容性、定制化成本等挑战，但开源社区、学术界和产业界的合作被视为加速RISC-V生态发展的关键。阿里巴巴达摩院高级技术专家王云龙聚焦在通用高性能（服务器场景）和 AI 两大领域展开分析了 RISC-V 的性能优势、生态支持以及面临的挑战，并表示，如何有效利用 RISC-V 的优势，特别是在 AI 计算方面，仍需探索和创新技术方案。中兴通讯操作系统架构师谈虎则从操作系统角度来看， RISC-V 在高性能方面已取得显著进展，但在实际应用中，尤其是在虚拟化扩展性能测试中的细节问题，与主流架构相比仍存在差距。此外，谈虎呼吁商业软件、硬件公司，尤其是 AI GPU 厂商，共同参与到 RISC-V 生态建设中来。中国电信研究院先进计算中心研究员崔恩放表示， RISC-V 面临的主要挑战在于如何选择合适的场景引入服务器厂商在适配软件方面的问题，并强调了成本优势对于推动新技术采用的重要性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525137" alt="图片" title="图片" loading="lazy"/><br/>（图/圆桌讨论现场）</p><p>感谢本论坛的出品团队：胡捷、徐立锋、陈盛德、丁欣、朱祯贞、林洛卉、李康雅等。</p><p>附本论坛的精彩集锦：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525138" alt="图片" title="图片" loading="lazy"/><br/>视频回放链接：<a href="https://link.segmentfault.com/?enc=NpjaWlihyhAJeIAns2ha1A%3D%3D.A8eCC42G1bUbGWENa3CgYL58VGT1oqw1b5PrdGRfunq7DE2h6nS0vQE82Wgrj%2Fsd" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a> —— 完 ——</p>]]></description></item><item>    <title><![CDATA[龙蜥社区荣膺 InfoQ “2025 中国技术力量榜单”两大奖项 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525152</link>    <guid>https://segmentfault.com/a/1190000047525152</guid>    <pubDate>2026-01-06 19:04:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，InfoQ 极客传媒携手模力工场发起的 “2025 中国技术力量榜单” 评选结果正式揭晓。龙蜥社区与合作伙伴联合提报的 “AI Serving Stack：面向大模型时代的云原生推理服务全栈解决方案”，凭借创新的 RBG 重新定义推理编排、智能调度实现差异化负载优化、以 KVCache 为中心的 PD 分离和以存换算架构等方面的突出创新，斩获了 “2025 年度 AI 工程与部署卓越奖”。同时，“SysOM AI 基础设施运维解决方案”凭借在大规模 AI 训推集群的“分钟级发现、小时级定界”能力、及持续性能剖析与资源效能提升上的领先实践，荣获“‘人工智能+’行业最佳解决/落地方案”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525154" alt="图片" title="图片"/></p><p>在“人工智能 +”行动规划的推动下，人工智能已从前沿技术逐渐演变为驱动产业升级与经济转型的关键力量。2025 年，中国 AI 技术落地与产业应用进入加速期。龙蜥社区积极布局 AI 基础软件栈，在操作系统内核、工具链与运维体系等关键环节持续创新，构建起支撑大模型高效训练与推理的坚实底座。</p><p>大模型推理正演变为"最昂贵的微服务"——既需 HPC 集群的极致性能，又要求云原生的敏捷弹性。AI Serving Stack 是由龙蜥社区与 SGLang 社区、Mooncake 社区、清华大学、南京大学、小红书、算秩未来、科大讯飞和阿里云联合打造，坚持全栈开源理念，采用开放治理模式，100% 开源架构让企业可零成本快速落地。作为大模型生产级”最后一公里”难题的解决方案，AI Serving Stack 填补开源社区在“生产级 LLM 推理编排”领域的空白，提供了从部署管理、智能路由、弹性伸缩、深度可观测的一体化能力，助力企业无论处于 AI 应用初期还是已运行大规模推理业务，都能轻松驾驭复杂的云原生 AI 推理场景。依托领先的 RBG 部署编排机制、智能负载调度策略，以及以 KVCache 为中心的 PD 分离架构和以存换算架构，AI Serving Stack 实现数倍性能提升，显著降低推理成本。未来，AI Serving Stack 将以更加开放的生态聚合产学研智慧，为产业提供从“能跑通”到“高可用、高吞吐、高弹性”的质变路径。</p><p>SysOM（System Operation&amp;Maintenance）是龙蜥社区系统运维 SIG 打造的一站式操作系统运维平台，通过监控、诊断、持续性能剖析等一体化解决方案，具备常态化、无侵入、低开销、可视化分析等特点，广泛应用于教育、医疗、电商、智驾等场景的性能诊断和分析优化。SysOM AI 基础设施运维解决方案是围绕训练及推理场景“MTTR （平均修复时间）长、无效训练时间长”等痛点，以“1 分钟极速发现、5 分钟快速定界定位问题”为目标，显著提升整体训推效率。目前，龙蜥社区系统运维联盟成员单位阿里云已基于 SysOM 项目发布了操作系统控制台，操作系统控制台为用户提供全面的系统资源监控、问题分析和故障解决能力，旨在优化系统性能，显著提升运维效率和业务稳定性。未来，SysOM 将会继续帮助提升训推业务场景万卡集群规模的 GPU 利用率等问题而努力。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=6yPhoU9r6pOT%2Fle9vWWtnw%3D%3D.u%2BcE3ZhUTnDYyLDVsl5%2F%2FGo48Itp1HE5prBmDl5GjPiCGbcwJPHTRTv4eRjGTz%2FO" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p>此前，龙蜥社区及龙蜥操作系统也获得了业界的广泛认可，荣获 OS2ATC 2025 “最具影响力开源创新贡献奖”、龙蜥操作系统通过工信部电子标准院首批开源项目成熟度评估，唯一获得“卓越级”（最高等级）的开源项目等 40+ 行业奖项。未来，龙蜥社区将持续深耕 AI 与操作系统融合创新，携手更多合作伙伴，共建开放、高效的下一代智能基础设施底座。</p><p>完整榜单见链接：<a href="https://link.segmentfault.com/?enc=c3D3E5PLKj5CstdZ2hmgWg%3D%3D.prz5JcPya7deRXMLFocsnLvU0BqgER%2BNoDN09c01dsdIUu0tvZQBGjs4Hs4Yo0FZ" rel="nofollow" target="_blank">https://www.infoq.cn/zones/chinatechawards2025/</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[聚力同行！这 13 家企业荣获“2025 龙蜥社区最佳合作伙伴奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525157</link>    <guid>https://segmentfault.com/a/1190000047525157</guid>    <pubDate>2026-01-06 19:03:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，龙蜥社区正式揭晓 2025 年度“最佳合作伙伴”获奖名单，并于 11 月落幕的龙蜥操作系统大会上举行颁奖仪式。现场，龙蜥社区理事代表、凝思软件副总经理彭志航为阿里云、浪潮信息、intel、海光信息、AMD、Arm、Tenable 等 13 家获奖企业代表颁奖。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525159" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区年度最佳合作伙伴颁奖现场）</p><p>本次获奖企业是与龙蜥社区有实质性合作落地，在产品共建、技术创新、商业实践及社区运营等多个维度综合评估的基础上，结合龙蜥贡献平台的实际数据，经过社区运营委员会与技术委员会评审，并由理事会公示后最终确认。恭喜这些单位！</p><p>在龙蜥社区生态发展计划--“龙腾计划 2.0”的战略牵引下，2025 年龙蜥社区携手整机、云服务、芯片、安全等全产业链伙伴，依托安全联盟、系统运维联盟与智算联盟，深入开展全栈协同，推动装机量达 1000 万套。在龙蜥社区运营目标牵引下，年度案例、活动与贡献度同比增长最高达 200%。本次获奖企业正是深度融入这一开放生态，积极践行“自愿、平等、开放、协作”理念的优秀代表。</p><p>其中，理事长单位与副理事长单位在社区生态共建上做了良好示范。阿里云作为社区理事长单位，积极引领社区整体规划和技术发展方向。通过推出三大合作计划，有效促进芯片、云服务、基础软件等多方成员的深度协同；通过社区治理和生态协同，有效推动关键项目落地，凝聚生态力量促进龙蜥蓬勃发展。浪潮信息全面参与安全联盟、智算联盟及委员会的工作，并积极参与社区规划和组织工作，有力支撑了龙蜥操作系统 Anolis OS 在 AI、云计算及关键基础设施等领域的生态拓展与规模化落地 。海光信息将新一代处理器的关键计算与安全特性深度融入龙蜥操作系统，实现高性能芯片与开源底座的高效协同。通过生态活动与社区共治机制，积极推动国产芯片与龙蜥在行业场景中的融合落地。intel 在技术适配、AI 生态融合等方面与社区深度共建，推动 Anolis OS 在 intel 平台及异构计算场景的优化落地，积极参与 SIG 协作与国际技术资源整合，为社区注入了产业动能与全球视野。中兴通讯为龙蜥社区提供 SW 与 RISC-V 架构的完整工具链及关键组件适配，降低新架构操作系统构建门槛，并持续维护软件供应链安全稳定；同时，其贡献的 NDE 桌面环境在兼容主流生态基础上，丰富了龙蜥桌面生态多样性。</p><p>在技术研发和社区运营方面，诸多伙伴单位也在积极并深度参与龙蜥社区。AMD 在积极推进 EPYC 处理器在龙蜥操作系统上的适配与优化的同时，还长期支持并参与社区组织的技术交流与生态活动，以实际行动助力开源基础软件生态的发展。Arm 深度投入社区 SIG 建设、参与内核及基础软件栈的技术共建，同时通过联合举办技术活动、推动工具链完善等方式，助力龙蜥生态的多元发展和产业落地。如意社区与社区携手加速 RISC-V 在数据中心、云计算等高性能场景的落地，助力开源芯片生态与操作系统深度融合。达摩院依托在操作系统、AI 与系统软件领域的深厚积累，为龙蜥社区在内核优化、智能调度及云原生工具链等方向提供关键技术协助。</p><p>安全联盟、系统运维联盟和智算联盟的成员单位，也通过技术合作、产品适配等参与龙蜥社区共建。安恒信息基于 Anolis OS 23 完成了其安全防护平台的兼容性验证与性能调优，为关键行业用户提供高可靠的安全运行环境；沐曦高效完成其曦云系列加速产品对 Anolis OS 23 的全面适配，充分释放国产 GPU 在龙蜥操作系统上的计算潜能；Tenable 作为国际安全厂商加入安全联盟， 已启动对 Anolis OS 的适配工作，助力用户构建与国际接轨的系统安全防护体系；信通院则发挥其在 ICT 领域标准与评测方面的权威优势，联合龙蜥社区推进操作系统相关技术标准、兼容性认证及开源治理规范的建设。</p><p>最佳合作伙伴生动诠释了各成员单位开源协作与生态共赢的初心与实践。龙蜥社区理事长马涛表示：“开源不是独行，而是众行。龙蜥的蓬勃活力源于每一位合作伙伴的长期投入与多元共创。未来，我们愿与更多伙伴一道，以开放聚合力，以协同筑根基，共赴基础软件的长远未来。”</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=0TiWPsEZ8VHSzT5657EtYw%3D%3D.vvX3WH%2FGOib7YKzdE0GYC0rarmfa5GrKKwQ64G6%2FNE0%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p>]]></description></item><item>    <title><![CDATA[五年同行，共铸基石！11 位杰出贡献者荣获“龙蜥社区五周年特别贡献奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525162</link>    <guid>https://segmentfault.com/a/1190000047525162</guid>    <pubDate>2026-01-06 19:02:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在第三届龙蜥操作系统大会上，龙蜥社区“五周年特别贡献奖”正式公布，由社区高级顾问、凝思软件董事长宫敏与特约顾问和中国开源软件推进联盟副主席兼秘书长刘澎，为本次 11 位获奖的贡献者颁奖。五周年奖项是为激励那些在龙蜥社区成立至今，不仅坚持个人长期参与并持续牵引所在企业投入关键资源，在社区中产生较大影响力的理事或委员。这 11 位获奖者为：马涛、张磊、高翔、龚文、顾剑、陈绪、杨勇、金美琴、张金利、王洪虎、陈鲍孜。</p><p>这些获奖者不仅是龙蜥社区五年发展的亲历者，更是其关键推动者。在他们的持续投入与引领下，龙蜥社区从零起步，逐步构建起覆盖操作系统、芯片、整机及配件、云服务等产业链上下游核心环节的开源生态；龙蜥操作系统也由此建立起完整的技术体系，Anolis OS 累计发布多个稳定版本，广泛支撑千行百业落地应用——装机量突破 1000 万，社区合作伙伴超 1000 家，生态影响力持续扩大。他们的工作，实实在在地塑造了龙蜥今天坚实的技术底座与蓬勃发展的产业格局。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525164" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区“五周年特别贡献奖”现场合照）</p><p>在开源社区的演进过程中，清晰的技术方向与生态愿景离不开顶层的战略引领与坚定投入。龙蜥社区的发展之所以能够稳步前行、持续突破，离不开以理事长、副理事长及理事为代表的高层治理者们的远见与担当。他们不仅锚定社区长期发展方向，更通过资源协同、组织推动和关键决策，将战略转化为行动，为龙蜥构建坚实的技术底座、繁荣的产业生态和可持续的开源机制提供了根本保障。</p><p>阿里云智能集团研发副总裁马涛作为龙蜥社区理事长和掌舵人，五年来始终身先士卒、躬身入局，引领社区从初创走向成熟。他持续坚定投入阿里云的技术与工程和运营资源，加速操作系统关键技术攻坚，推动龙蜥社区夯实基础、扩大影响，成长为国内领先的开源操作系统阵地。他的远见与坚持，不仅为龙蜥社区的蓬勃发展提供了核心动力，也为中国操作系统产业的进步和开源生态建设作出了重要贡献。</p><p>统信软件高级副总经理、龙蜥社区副理事长张磊深耕社区建设，积极推动统信软件深度参与龙蜥建设，成功促成 DDE 桌面环境的移植，并首创分层分类的开源操作系统架构方法，为系统模块化与可维护性奠定基础。同时，他带领团队积极参与开源安全、Rust 改造、机密计算等关键工作组，持续筑牢社区的安全技术底座，为完善龙蜥产品版图与提升用户体验提供了核心支撑。</p><p>龙芯中科副总裁、龙蜥社区理事高翔作为首批加入社区的理事代表，始终带领龙芯团队深度参与并积极推动社区各项建设。特别是在 Anolis OS 8.4 LoongArch 版本发布和 LoongArch SIG 组建过程中，依托龙芯深厚的技术积累与大量工程投入，他有力推动了龙芯开源发行版的构建、软硬件技术的高效协同，以及应用生态的繁荣发展，为龙蜥社区自主架构领域的拓展作出了重要贡献。</p><p>中科方德高级副总裁、龙蜥社区理事龚文自社区成立之初就亲自带领团队深度参与龙蜥操作系统研发工作，成功发布基于 Anolis OS 的中科方德服务器操作系统，并持续推进技术适配与性能优化，在加强系统的安全性与兼容性方面取得成果，为龙蜥技术演进和生态完善作出了切实的贡献。</p><p>飞腾软件技术方案部高级总监、龙蜥社区理事顾剑积极推动飞腾产品与龙蜥在技术及生态层面的深度协同，不仅推动适配飞腾系列 CPU 平台的龙蜥社区发行版共计 17 项、覆盖腾云 S25000、S5000C 等核心产品，同时牵头在飞腾官网设立龙蜥专区，联合共建生态影响力，在加速龙蜥操作系统与国产化软硬件融合落地过程中发挥了关键作用。</p><p>在整体发展方向锚定之后，要将愿景转化为现实，离不开系统化的组织规划与强有力的治理机制。龙蜥社区以技术委员会和运营委员会承担了这个职责，其关键成员作为社区治理的核心支柱构建起兼具技术前瞻性与执行落地力的治理体系。</p><p>龙蜥社区技术委员会主席杨勇主导制定了龙蜥操作系统整体的技术战略与发展路线，构建了清晰的架构蓝图，以社区“三大技术合作计划”为载体，有效牵引并协同推进九大关键技术方向的持续演进。同时，通过建立开放、规范的技术治理机制，为龙蜥打造高可靠、高性能且可持续演进的操作系统底座提供了坚实引领，是龙蜥不可或缺的技术领航人。</p><p>龙蜥社区运营委员会主席陈绪始终聚焦社区发展顶层架构与运营体系的建设，全面指导并参与运营委员会各项工作。凭借 30 年的开源社区实践经验，参与创立了龙蜥社区，为社区的组织设计与机制完善提供了前瞻性引导，有效连接技术、商业与生态多方力量，持续强化社区联盟与组织韧性，为龙蜥社区的高端心智打造、实现规模化成长和可持续健康发展注入了关键的推动力。</p><p>龙蜥社区运营委员会副主席金美琴参与龙蜥社区的早期建设和筹备工作，自 2020 年 6 月开始，领导并推动了包括社区官网的创建、组织制度的建设、生态合作机制的运行以及重要联盟组织和关键项目的建立等，为社区的运营体系建设和生态发展发挥了至关重要的作用，是龙蜥五周年特别致谢的功勋个人。</p><p>版本研发是龙蜥社区工作的重中之重，在有了清晰的技术路线指引、规范的运营和组织建设后，技术的演进则需要更多核心贡献者的参与，他们就是推动 Anolis OS 逐步走向成熟、稳定与广泛应用的关键支撑力量。</p><p>龙蜥社区技术委员张金利作为龙蜥操作系统版本研发负责人，主导了 Anolis OS 所有重大版本的规划与发布，成功构建了一套稳定、安全且高性能的发行版体系。在此基础上，他持续推进内核优化、提升硬件兼容性，并加强自动化测试能力建设，有效保障了版本的高质量迭代。同时，他还积极协同社区力量，不断完善研发流程与发布机制，为龙蜥操作系统在金融、通信等关键行业的广泛应用提供了坚实可靠的版本研发支撑。</p><p>龙芯中科操作系统研发总监、龙蜥社区技术委员王洪虎带领龙芯研发团队与社区紧密协作，牵头成立 LoongArch SIG 工作组，统筹各方力量，向社区累计提交补丁覆盖 800 余个软件包，贡献代码超 30 万行。他主导完成了 Anolis OS 9 个大版本的移植、同源异构适配、验证与发布工作，使龙蜥操作系统成为首个支持龙芯 3C5000 与 3C6000 系列服务器芯片的社区发行版。</p><p>飞腾公司开源操作系统技术负责人、龙蜥社区技术委员陈鲍孜主导推动飞腾处理器系统软件开源生态的逐步成熟与持续演进。他积极推动飞腾平台在内核、驱动及基础软件等关键领域的深度投入，并持续向龙蜥社区贡献技术成果。同时，他协同社区伙伴共同制定内核 kAPI 与基础配置的统一规范，有效助力构建更加开放、兼容、安全且高效的操作系统生态。</p><p>一人行快，众人行远。过去五年，龙蜥社区之所以能从零起步、稳步成长，离不开这些坚定的贡献者和广大开发者的持续投入——他们的努力不仅体现在一行行代码和一个个版本中，更实实在在地落在千行百业的真实场景里。站在新起点，龙蜥将继续坚持共建、共治、共享的理念，诚邀更多企业和开发者加入，一起夯实基础软件底座，共筑中国开源的未来。</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=7%2BluN7rKbZr2dL8NJp278w%3D%3D.hl4b%2FUEytrxFU6fsCDCUUkONpU0x7J%2F0dXWXVsv1flo%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[直播预告｜智算时代，龙蜥携手玄铁共探开源驱动的高性能 RISC-V 新范式 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525169</link>    <guid>https://segmentfault.com/a/1190000047525169</guid>    <pubDate>2026-01-06 19:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>龙蜥社区早在成立之初就已积极布局并持续投入 RISC-V 生态建设。过去一年中，龙蜥社区在 RISC-V 领域取得双突破：软件生态方面，联合达摩院、中兴通讯、浪潮信息、中科院软件所等机构制定 RISC-V SIG 2.0 规划，并携手中兴通讯、达摩院、浪潮信息、如意社区等伙伴首次发布支持 RVA23 高性能扩展的 Anolis 23 RISC-V 预览版。国际标准化方面，社区专家在 RISC-V 国际基金会担任主席/副主席要职，主导 Data Center SIG 运作，推进 RAS/PMU 云方案增强、AIOE 扩展及虚拟化标准制定，参与全球标准建设。</p><p>1 月 8 日（周四）晚 7 点玄铁【智算系列】专题技术沙龙第二期沙龙聚焦玄铁与龙蜥社区合作最新进展，特邀 RISC-V 国际基金会 Datacenter SIG chair、龙蜥社区 RISC-V SIG co-maintainer 宋卓，阿里云技术专家薛帅，阿里巴巴达摩院玄铁 RISC-V 计算库技术专家周云飞等多位专家，系统分享玄铁 RISC-V 在数据中心、高性能计算及云原生场景下的最新技术突破与实践成果。欢迎大家扫码观看直播。</p><p><img width="723" height="1680" referrerpolicy="no-referrer" src="/img/bVdnzC6" alt="image.png" title="image.png"/></p>]]></description></item><item>    <title><![CDATA[2026招聘分水岭：AI重构决策型招聘新逻辑 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047525175</link>    <guid>https://segmentfault.com/a/1190000047525175</guid>    <pubDate>2026-01-06 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026招聘分水岭：AI重构决策型招聘新逻辑<br/>2026年之前，招聘或许还能依赖经验支撑；但2026年之后，仅靠经验驱动招聘，终将陷入被动。2025年，不少企业已开启AI招聘的初步尝试，从生成招聘文案、自动回复候选人到简化流程，确实为HR减轻了部分负担。但随着实践深入，企业逐渐意识到：浅层的AI工具仅能提升表层效率，已无明显边际价值。真正的变革正在发生——AI不再是单纯的效率辅助工具，而是深度渗透到组织能力核心层，开始重构招聘的底层逻辑。<br/>进入2026年，AI的价值早已超越“提速”，转而介入三个过去高度依赖经验与直觉的关键命题：人才该如何被精准定义、能力该如何被科学评估、招聘决策是否还能仅凭主观感觉。在人力资源领域，这一转变指向一个核心结论：唯有实现评估的精准度，才能真正掌握招聘决策的主动权。</p><p>招聘质变：从“工具辅助”到“决策支撑”<br/>过去一年，AI面试已在企业中广泛落地，但多数应用仍停留在基础层面：能与候选人对话、能提出常规问题、能生成评估报告，却无法直接为招聘决策提供有效支撑。核心症结并非AI的智能化程度不足，而是评估打分缺乏精准度、标准缺乏稳定性、结果缺乏清晰解释，难以支撑决策落地。<br/>招聘的本质是对候选人的综合判断，而判断的核心终究要落到精准评估上。真正能融入招聘全流程的AI工具，必然以“精准度”为核心能力，其评估结果不再是仅供参考的建议，而是可直接纳入决策链路的有效依据。这需要满足三重条件：通过人机“背靠背”对比实验验证效果，经受效标效度与重测稳定信度两大心理学指标检验，形成可复现、可验证的评估体系。这一突破，标志着招聘正式从“经验型判断”转向“数据驱动的规模化决策”。<br/>精准评估：贯穿招聘全流程的核心能力<br/>AI招聘工具的精准度，并非单一环节的能力体现，而是渗透到面试全流程的系统性优势，具体表现为四大维度：<br/>•一问多能：单道问题可同步评估多项胜任力，实现HR初筛与技术复试的无缝衔接，整体评估效率提升50%以上；<br/>•智能追问：借鉴资深面试官的提问逻辑，根据候选人回答即时生成针对性问题，精准捕捉核心信息，避免遗漏关键能力点；<br/>•简历深度挖掘：自动解析简历中的关键信息与模糊表述，生成递进式提问，既能有效降低候选人造假风险，也能避免因人工疏忽错失优质人才；<br/>•全维度覆盖：既可以评估沟通、协作等通用胜任力，也能针对编程、算法、工程、财务等专业领域精准设计考题，同时解放HR与专业面试官的精力。<br/>体验赋能：候选人体验成雇主品牌新抓手<br/>AI招聘的价值，既要兼顾企业端的评估效率，也要保障候选人的体验感——在竞争激烈的人才市场中，面试体验本身就是雇主品牌的重要组成部分。优质的AI面试工具，必然以“拟人化交互”为核心设计逻辑，打造有温度、有尊重感的面试场景：<br/>•情绪感知交互：精准捕捉候选人的语速、情绪与潜台词，通过引导式沟通帮助候选人缓解紧张，充分发挥真实水平；<br/>•无断点流畅体验：无需手动操作启停，系统自动识别回答状态并衔接问题，模拟真人对话节奏，避免机械感；<br/>•沉浸式视觉呈现：实现语音与口型的精准同步，打破传统AI面试的“纸片人”疏离感，提升交互真实度；<br/>•实时答疑解惑：支持候选人随时提问，针对岗位详情、企业福利等问题即时回应，强化候选人对企业的认知与入职意愿。<br/>全流程自动化：开启招聘“无人驾驶”新阶段<br/>如果说AI面试解决了“选得准”的问题，那么AI人才寻访工具则攻克了“找得快”的痛点，将招聘初筛阶段的机械劳动彻底自动化，推动招聘进入“无人驾驶”模式。这类工具并非简单的批量操作工具，而是具备独立判断与执行能力的全流程解决方案：<br/>•快速启动值守：30-60秒即可完成配置启用，全程无需人工值守，大幅降低操作门槛；<br/>•智能筛选匹配：自动按学历、年龄、薪资预期等预设条件筛选简历，精准锁定目标候选人；<br/>•拟人化动态沟通：模拟人类沟通逻辑发起对话，根据交互结果判断适配性，不合适则智能终止沟通，提升转化效率；<br/>•全链路信息闭环：自动遍历未读消息并个性化回复，主动索取候选人简历，同步模拟人类打字节奏提升真实感，最终将简历自动同步至ATS系统并生成候选人档案，实现信息流转无缝衔接。<br/>借助AI大模型能力，招聘彻底从“经验驱动”升级为“数据驱动”，不仅实现效率的几何级提升，更将HR从海量重复劳动中解放，聚焦于人才洞察、战略规划等更具价值的工作，让招聘成为支撑企业核心竞争力的重要环节。2026年的招聘分水岭已然显现，唯有主动拥抱AI重构的决策型招聘逻辑，才能在人才竞争中占据主动。</p>]]></description></item><item>    <title><![CDATA[汽车制造质量大数据分析如何助力企业创新发展？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047524715</link>    <guid>https://segmentfault.com/a/1190000047524715</guid>    <pubDate>2026-01-06 18:13:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在汽车制造业转型升级的关键阶段，质量大数据分析正成为企业提升核心竞争力的重要抓手。随着市场竞争日趋激烈，消费者对汽车产品的要求不断提高，传统的质量管理方法已经难以满足现代生产的需求。质量大数据分析通过整合多源异构数据，构建智能化分析模型，为企业提供了全新的质量管控视角和决策支持手段。<br/>一、质量大数据分析的技术路径与价值实现<br/>汽车制造过程中的质量数据具有多维度、多阶段、多系统的特征，涵盖了从原材料进厂到整车出厂的全生命周期。这些数据如果能够得到有效挖掘和分析，将为企业带来显著的竞争优势。当前，主流的质量大数据分析技术包括数据采集与预处理、特征工程、机器学习建模和深度学习算法等多个环节。<br/>在数据采集方面，现代汽车制造企业普遍采用多种技术手段实现数据的全面采集。例如，通过部署在生产线上的各类传感器，实时采集关键工艺参数；通过连接MES系统，获取生产过程的详细记录；通过对接QMS系统，收集质量检验数据和反馈信息。这些数据经过清洗、转换和标准化处理后，才能为后续分析提供可靠基础。<br/>特征工程是质量大数据分析的核心环节。通过对原始数据进行降维、变换和特征提取，可以发现数据中隐藏的质量规律。常用的特征工程方法包括主成分分析、小波变换、时间序列分析等，这些方法能够将复杂的数据转化为可解释性强的特征指标。<br/>在模型构建层面，企业可以根据实际需求选择不同的算法。传统统计方法如回归分析、方差分析仍然有其价值，但随着数据规模的扩大和复杂度的提高，机器学习算法如随机森林、支持向量机等逐渐成为主流。近年来，深度学习技术在质量预测领域的应用也取得了显著进展。<br/>质量大数据分析的价值不仅体现在质量问题的解决上，更重要的是它能够为企业的创新活动提供数据支撑。通过对质量数据的深度挖掘，企业可以发现产品设计、工艺改进和质量控制的创新点，从而开发出更具竞争力的产品。<br/>二、实施质量大数据分析的关键要素与挑战<br/>实施质量大数据分析项目需要综合考虑技术、管理、人才等多个因素。首先，企业需要建立完善的数据采集体系，确保数据的全面性和准确性。这包括部署各类传感器、建立统一的数据标准、完善数据传输机制等。<br/>数据治理是实施过程中的重要环节。企业需要制定数据质量标准，建立数据清洗流程，完善数据安全机制。特别是在汽车行业，质量数据往往涉及商业机密，如何在保护数据安全的同时实现数据价值，是一个需要认真考虑的问题。<br/>技术平台的建设同样至关重要。企业需要根据自身需求选择合适的大数据分析工具和算法。目前市场上有多种解决方案可供选择，如阿里云Quick BI、华为FusionInsight、浪潮云洲工业互联网平台等。这些平台的功能特点、适用场景各不相同，需要结合企业实际情况进行评估和选择。<br/>在组织保障方面，企业需要调整现有的管理架构和工作流程。质量大数据分析往往需要跨部门协作，这就要求企业打破传统的部门壁垒，建立以数据为中心的协同工作机制。<br/>三、行业应用案例：标杆企业的实践<br/>广域铭岛的质量管理系统（QMS）在汽车制造领域实现了多项突破。该系统通过整合工业互联网技术，实现了质量数据的实时采集、传输和分析。系统架构包括数据采集层、传输层、存储层和应用层四个主要部分，各层之间通过标准化接口实现无缝对接。<br/>在应用效果方面，广域铭岛的QMS系统为某知名汽车零部件企业提供了全面的质量管控解决方案。该项目实现了：<br/>200多个关键质量指标的实时监控<br/>检测效率提升40%，检测时间从原来的数小时缩短到半小时以内<br/>质量预警准确率达到95%以上<br/>年均质量成本降低25%<br/>客户投诉率下降30%<br/>另一个值得关注的案例是某德系豪华品牌汽车制造商。该企业实施了基于大数据的智能质量控制系统，实现了从质量检测到工艺优化的全流程数字化管理。系统采用了实时数据可视化技术，将质量数据以直观的方式呈现给相关人员。通过机器学习算法的持续优化，系统能够自动识别质量异常，并给出预警和建议。<br/>国内领先的新能源汽车制造商也在质量大数据分析方面进行了创新探索。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造需要引入工业智能体进行全链路管理？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524723</link>    <guid>https://segmentfault.com/a/1190000047524723</guid>    <pubDate>2026-01-06 18:12:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在新一轮制造业智能化浪潮中，工业智能体正成为推动产业变革的核心力量，尤其在汽车制造领域，其价值已从概念验证迈向规模化落地。作为融合大语言模型、工业机理与多源数据的新型认知智能系统，工业智能体不再局限于单一任务的自动化执行，而是像一位具备经验与判断力的“数字工匠”，能够自主感知产线状态、动态优化工艺参数、协同调度资源，并持续学习进化，真正实现从“人控设备”到“系统自驱”的跃迁。<br/>在汽车制造这一高度复杂、流程密集的行业中，工业智能体的应用已深入全价值链。以广域铭岛为代表的领先企业，通过其Geega工业超级智能体平台，成功将多年沉淀的工艺经验转化为可复用、可迭代的AI能力。在焊装环节，智能体可直接理解工程师的自然语言指令，自动调整焊缝参数并完成编程，使工艺优化周期缩短60%，缺陷率下降45%；在冲压与涂装工序中，它实时融合设备振动、温度、压力与视觉数据，动态调节运行参数，使零部件精度提升15%，废品率降低18%。更令人瞩目的是，其“设计智能体”能根据需求描述自动生成轻量化零部件方案，将新车研发周期压缩60%，彻底打破传统依赖人工经验的设计瓶颈。<br/>在生产运营层面，工业智能体构建了覆盖排产、质量、物流、运维的全链路协同网络。在汽车工厂中，调度智能体可在1小时内完成原本需6小时的人工排产，每周节省超15小时；质量检测智能体借助多模态感知，实现微米级缺陷识别，效率较人工提升200倍；而当供应链突发中断时，广域铭岛平台上的12类专业智能体可在5分钟内协同生成替代方案并验证可行性，将损失降低80%，极大增强了制造系统的韧性与响应速度。其“黑灯仓库”系统更联动AGV与AMR机器人，实现无人化拣选与缺件预警，打通了从订单到交付的智能闭环。<br/>广域铭岛的突破不仅在于技术应用，更在于其构建了“数据—机理—反馈”三位一体的能力基座。通过边缘计算实时捕捉毫秒级设备信号，结合工业Know-How形成“懂行AI”，并依托低代码平台让非技术工程师也能“搭积木”式开发专属智能应用，大幅降低了部署门槛。这种模式有效破解了工业领域长期存在的“数据孤岛”与“经验失传”难题，将老师傅的隐性知识转化为可传承、可扩展的数字资产。<br/>当前，工业智能体正从“辅助工具”加速演变为汽车制造的“决策中枢”。随着数字孪生、联邦学习等技术的融合，未来智能体将不仅优化单点效率，更将重构整车研发、生产、服务的全生命周期体系。广域铭岛的实践表明，唯有将智能体深度扎根于真实汽车制造场景，构建开放、协同、自进化的智能生态，才能真正释放其作为新型生产力的潜能，推动中国汽车产业从“规模驱动”迈向“认知驱动”的高质量发展新阶段。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造企业需要引入模具智能管理系统？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524727</link>    <guid>https://segmentfault.com/a/1190000047524727</guid>    <pubDate>2026-01-06 18:12:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业4.0深度演进的背景下，模具智能管理正成为制造业提质增效的核心引擎，尤其在汽车这一高度精密、高节奏的产业中，其价值尤为凸显。传统依赖人工经验、固定周期保养的模具管理模式，早已难以应对现代汽车生产对稳定性、一致性与响应速度的严苛要求——停机频发、备件冗余、故障难溯，成为制约产能释放的隐形瓶颈。<br/>广域铭岛数字科技有限公司以“数据驱动、AI赋能”为理念，率先构建起一套面向汽车制造的模具智能管理闭环体系，彻底重塑了模具在生产链中的角色：从被动消耗的“工具”，转变为可感知、可预测、可优化的智能资产。其核心依托于Geega工业AI应用平台与工业智造超级智能体，通过在模具与压机上部署高精度传感器网络，实时采集冲压次数、温度、振动、压力等多维数据，结合材料特性、历史维修知识图谱与生产排程信息，动态生成每副模具的“设备健康指数”（EHI）。这一指数不再是抽象报表，而是模具的“生命体征”——当某副用于高光件生产的模具因表面易划伤导致EHI升高，系统自动缩短保养周期；当高强度钢模具因应力累积预警导柱磨损，系统即推送“更换导柱+优化润滑”的精准干预方案，实现从“定时体检”到“精准诊疗”的跃迁。<br/>在领克汽车成都工厂的实践中，这套系统展现出惊人的实战效能：模具相关停机率下降65%，故障响应时间从2小时压缩至15分钟，润滑剂消耗减少18%，备件库存周转率提升40%，设备故障预测准确率突破95%。更重要的是，系统打通了MES、ERP与库存调度系统，形成全链路协同——当某模具即将达维护阈值，系统可提前48小时自动切换订单至健康模具，避免突发停线；一旦异常发生，15分钟内自动生成包含设备切换与参数调整的应急方案，真正实现“未病先防、小病早治”。<br/>这一模式的价值远不止于降本增效。广域铭岛的解决方案将每副模具的全生命周期数据完整沉淀，质量问题可精准溯源至某次保养中的润滑不足或导柱更换延迟，企业经验不再随技师离职而流失，而是转化为可复用、可迭代的数字资产。在芯片短缺等供应链危机中，系统甚至能基于3000组模具实时状态数据，智能分配稀缺资源至故障风险最低的产线，保障核心车型交付，展现出强大的韧性与战略价值。<br/>如今，模具智能管理已从汽车制造的“关键支撑”升级为“核心竞争力”。广域铭岛所推动的，不仅是技术工具的升级，更是一场工业文明的范式革命——它让沉默的钢铁学会“说话”，让混沌的生产重获秩序。未来，随着5G、边缘计算与数字孪生技术的深度融合，模具的“数字分身”将在虚拟空间中模拟百万次冲压，AI智能体通过“自我对弈”持续优化策略，实现“一处学习，全网受益”的群体智能。在汽车工业迈向高端化、柔性化、智能化的征途中，广域铭岛正以模具智能管理为支点，撬动整个制造体系的进化。</p>]]></description></item><item>    <title><![CDATA[团队协作冲突怎么处理？项目管理常见冲突原因与解决流程 项目管理小胡 ]]></title>    <link>https://segmentfault.com/a/1190000047524731</link>    <guid>https://segmentfault.com/a/1190000047524731</guid>    <pubDate>2026-01-06 18:11:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我从市场转岗做项目经理后，最先卡住的不是排计划，而是团队协作里的冲突：一句“又改需求？”就能把讨论从对事变成对人。我也试过圆场、私聊硬扛，结果问题没少，关系更累。后来我才明白：冲突不可怕，可怕的是没流程、没规则。下面我用自己的踩坑经历，给你一套新人也能照着用的团队协作冲突解决流程。</p><p>读完本文你会得到：</p><ul><li>5类常见冲突根因（目标/范围/资源/边界/信息）怎么识别</li><li>一套“冲突处理5步流程”（含话术 + 纪要模板）</li><li>需求变更、跨部门拉扯、资源抢占时怎么拍板与升级</li></ul><h2>明明想把事做好，为什么越团队协作越拧巴</h2><p>我印象最深的一次，是需求评审会的后半段。业务说按钮颜色得改，不然转化会掉。研发同事直接就是一个问号：“又改“？测试补了一句：“那我这轮回归是不是白做了”？</p><p>我当时脑子里只有一个念头：别吵起来。于是我快速圆场：“先按原计划走，改动我会私下再对齐”。会开完我分别私聊三方，像个客服一样安抚情绪。结果一周后需求照改、返工照来、抱怨更大。</p><p>后来我反思了一下，我总怕场面难堪，觉得“和气”比“结论”重要。后来才发现，回避一次，问题会在延期、返工或下一次更大的争执里补回来。</p><p>我开始把争论改写成一句“可决策的话”：要不要进本期？代价是什么？谁拍板？验收标准是什么？当问题变成“怎么做选择”，团队协作才有机会回到同一张桌子上。</p><p>真正让局面变稳的，是固定住几件事：事实对齐、影响量化、拍板机制、书面结论、行动项追踪。下面我会先讲“冲突根因怎么识别”，再给你完整的“5步解决流程”。</p><h2>团队协作冲突最常见的5个原因（以及你怎么识别它）</h2><p>下面五类，我会配上“项目现场常见句式”，你会更容易判断自己遇到的是哪一种。</p><p><strong>1）目标不一致：大家都在为“正确的事”努力，但方向不在一条线上</strong></p><p>典型句式：业务：“先上再说，错过窗口期就没机会了”；研发：“不把隐患收掉，上线就是埋雷”。</p><p>识别要点：一方在谈“速度/机会”，一方在谈“风险/稳定”。</p><p>我的补救：把目标拆成“本期必须守住的三件事”（比如合规、核心质量、关键路径），其余作为后续迭代。目标一旦写清，争论会自动收敛。</p><p><strong>2）范围不清/需求频变：冲突表面是情绪，背后是变更控制缺失</strong></p><p>典型句式：“这不是小改吗？” vs “你们根本不知道影响面。”<br/>识别要点：一方在谈“功能点”，另一方在谈“系统影响/回归成本”。<br/>我的补救：建立最小变更闭环：变更内容 → 影响面 → 成本/风险 → 决策人 → 验收口径。只要闭环跑起来，需求变更就不再等于“临时加塞”。</p><p><strong>3）角色与边界模糊：很多冲突不是讨论不出来，是没人有权拍板</strong></p><p>典型句式：“这个你问产品。”<br/>“这个要业务确认。”<br/>“那你们决定吧。”<br/>识别要点：决策权漂移，责任也漂移。<br/>我的补救：至少明确三件事：谁拍板、谁负责落地、谁负责验收。不用一上来就做完整RACI，但要让“决策出口”存在。</p><p><strong>4）资源稀缺：你以为在争优先级，其实在争“风险与代价谁来背”</strong></p><p>典型句式：“我们也很忙，你这个插进来我那边就要延期。”<br/>识别要点：争论持续围绕“谁先做”，却没人把“代价”写清。<br/>我的补救：把资源冲突显性化：关键人同时承担哪些任务？每个任务的代价是什么？让优先级回到“业务价值 × 风险”，而不是“谁声音大”。</p><p><strong>5）信息不对称：一方看见客户压力，一方只看见工期压缩</strong></p><p>典型句式：“客户已经在催了。” vs “那也不能不讲基本规律。”<br/>识别要点：双方讲的不是同一套语言。<br/>我的补救：练习“翻译”：业务把客户压力翻译成风险等级；研发把技术影响翻译成成本、失败概率与兜底方案。<br/>识别了原因，你就能更从容地进入“处理流程”。接下来这套 5 步，我自己反复用，尤其适合新人 PM 不被情绪带跑。</p><h2>方法与实践：我现在用的“冲突处理5步流程”</h2><p>我把它当成一个“小型可复制流程”。每次冲突来了，我就按步骤走，自己也不容易被情绪裹挟。</p><p><strong>Step 0：先“降温”，再“推进”（30秒把会议从对人拉回对事）</strong></p><p>我常用两句开场：</p><ul><li>“我先确认一下：我们目标是一致的——按期交付、风险可控，对吗？”</li><li>“我们先对齐事实，再讨论方案，最后明确谁拍板。”</li></ul><p>这不是装冷静，而是在给团队协作一个“共同底盘”：我们不是来赢对方，我们是来解决问题。</p><p><strong>Step 1：用“事实—影响—待决策”清单，结束空转争论</strong></p><p>我会把白板（或在线文档）写成三段：</p><ul><li>事实（可验证）：发生了什么？版本/时间/承诺/数据是什么？</li><li>影响（可量化）：对范围/进度/质量/风险的影响分别是什么？</li><li>待决策（一句话）：我们现在要决定什么？（进不进本期？延期？降级？拆分？）</li></ul><p>可复制模板（你直接粘贴到纪要里）：</p><ul><li>事实：</li><li>影响（范围/进度/质量/风险）：</li><li>待决策：</li><li>拍板人：</li><li>结论：</li><li>行动项（Owner/DDL/验收口径）：</li></ul><p>示例（拿“颜色变更”举例）：</p><ul><li>事实：提出变更发生在联调后；影响页面 6 个；涉及埋点 2 个。</li><li>影响：回归增加 1.5 人日；本期上线风险 +1。若不改，业务预计转化下降 X%（先标记为“假设”，待数据验证）。</li><li>待决策：是否进入本期？若进入，是否接受回归工期延长？验收口径是什么？</li></ul><p>小技巧：把“观点”先标为“假设”，再决定要不要验证。这样团队协作会更少陷入“我觉得”。</p><p><strong>Step 2：从“立场”转到“利益”（问两句就够用）</strong></p><p>冲突里最耗人的，是大家都在捍卫立场：</p><ul><li>业务立场：“必须改。”</li><li>研发立场：“不能改。”</li></ul><p>我会刻意追问“利益/担忧”：</p><ul><li>“你最担心的是什么风险？”</li><li>“如果只能保一个，你想保住什么？”</li></ul><p>这就是我借鉴的“原则式谈判”思路：先把深层诉求拉出来，方案空间才会出现。</p><p><strong>Step 3：选对冲突处理方式：不是永远“协作”，而是看情境</strong></p><p>这里我会用 TKI 的两条轴提醒自己，从而形成五种方式：竞争、回避、迁就、妥协、协作。</p><ul><li>坚持度（assertiveness）：我多想满足自己的诉求</li><li>合作度（cooperativeness）：我多想满足对方的诉求</li></ul><p>我给新人一个更“好用”的判断：</p><ul><li>合规/安全/关键质量：可以更“强制/直接”，但必须讲清依据与责任边界；</li><li>信息不足：先“延后”，补数据，不要在不确定里硬拍；</li><li>双方诉求都合理：优先“协作”，找拆分、找阶段性方案；</li><li>必须快速推进：选择“妥协”，但把代价写进记录里（别让代价消失在空气中）。</li></ul><p><strong>Step 4：开一场15分钟“问题解决会”，把结论落地到行动项</strong></p><p>我现在处理冲突，会尽量快速拉一个短会（人越少越好，但必须包含拍板人或授权人）：</p><ul><li>5分钟：事实&amp;影响对齐</li><li>5分钟：列方案（至少2个）+评估标准（进度/风险/收益/成本）</li><li>5分钟：决策 + 行动项（负责人/截止时间/验收口径/风险兜底）</li></ul><p>我常用的“拍板句”：</p><ul><li>“我们现在不是在找完美解，是在找‘可交付的最优解’。”</li><li>“我把代价写进纪要：我们接受 A，意味着 B 风险增加；谁确认/谁承担/怎么兜底？”</li></ul><p><strong>Step 5：把冲突变成机制：写进“工作约定”，减少下次重复爆炸</strong></p><p>如果同一类冲突反复出现，我会在复盘后补一条“工作约定”，比如：</p><ul><li>需求变更三问：改什么？影响什么？谁拍板？</li><li>优先级原则：以业务价值×风险为准（并把评估标准写出来）</li><li>升级路径：卡住超过 24 小时 → @相关Owner；超过 48 小时 → 升级到负责人；所有升级必须带“事实—影响—建议方案”。</li></ul><p>流程能止血，机制能治本。把冲突写进规则里，你的团队协作会越来越省心。</p><h2>启发与建议：我从实践里提炼的5条心得</h2><ol><li>先对齐目标，再讨论方案：目标是方向盘，方案只是路线图。</li><li>冲突不是人不好，多半是规则不清：范围、边界、信息、决策出口，缺哪补哪。</li><li>把结论写下来：没有记录的共识，下一次冲突会把你带回原点。</li><li>让“说真话”变安全：心理安全感常被定义为团队成员相信“表达观点、提出问题、承认错误不会被惩罚或羞辱”。这也是健康团队协作的重要底层条件。</li><li>把冲突当成体检报告：每次冲突都在告诉你——哪条规则缺失、哪条流程漏风、哪个角色没被授权。</li></ol><h2>常见问题 FAQ：</h2><p><strong>Q1：团队协作冲突出现时，PM 是不是应该立刻站队？</strong><br/>A：通常不要。先把冲突从“立场对抗”拉回“事实与影响”。当事实清晰、代价明确时，站队会变成“基于原则的决策”，而不是“情绪偏好”。</p><p><strong>Q2：需求变更引发冲突，最关键的一句话是什么？</strong><br/>A：把争论写成“可决策的问题”：“要不要进本期？代价是什么？谁拍板？验收是什么？”——这句话能把团队协作从互怼拉回协作。</p><p><strong>Q3：跨部门协作总卡住，怎么推动？</strong><br/>A：用“升级路径 + 书面化影响”推动。升级不是告状，而是把“卡住的代价”摆在台面上，让组织机制来做取舍。</p><p><strong>Q4：冲突解决后还要做什么？</strong><br/>A：一定做一次“轻量复盘”：本次冲突的根因属于目标/范围/资源/边界/信息哪一类？把对应规则补上，否则还会复发。</p><p>我到现在也不敢说自己“很会处理冲突”。但我越来越确定：项目经理的成长，不是把所有人都哄开心，而是把复杂的团队协作问题，变成大家都能理解、都能执行的流程与规则。</p><p>如果你也是跨岗位转型、刚做 PM，遇到冲突别急着怀疑自己——很多时候不是你不行，而是你还在学习“从沟通者走向协调者”。愿我们都能在一次次复盘里，把拉扯变成对齐，把争论变成决策，把冲突变成更稳的协作机制。</p>]]></description></item><item>    <title><![CDATA[这篇一定要看，观测云 2026 产品路线图全公开 观测云 ]]></title>    <link>https://segmentfault.com/a/1190000047524757</link>    <guid>https://segmentfault.com/a/1190000047524757</guid>    <pubDate>2026-01-06 18:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>序言：奇点临近，可观测性的代际跨越</h2><p>站在 2026 年的时间节点回望，我们正处于 IT 基础设施历史上最深刻的变革之中。这不仅是云计算的延续，更是一场由人工智能（AI）主导的“认知革命”。如果说云原生（Cloud Native）时代解决了资源的弹性问题，那么 AI 原生（AI Native）时代则致力于解决决策的自主性问题。</p><p>Gartner 的战略预测早已指出，到 2026 年底，由于缺乏足够的 AI 风险护栏，甚至可能出现数千起因 AI 决策失误导致的法律索赔案件。这一预测不仅揭示了 AI 技术的双刃剑效应，更深刻地指出了当前技术栈中最大的空白——对于自主智能体（Autonomous Agents）的深度可观测性与治理能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524759" alt="图片" title="图片"/></p><p>在 2026 年的企业环境中，由于 Agentic AI 的普及，软件不再仅仅是执行预定义代码的静态指令集，而是变成了具有推理、规划和执行能力的“数字员工”。这些智能体像 F1 赛车的维修团队一样协作，以模块化的方式处理复杂的业务逻辑。然而，这种自主性带来了前所未有的不确定性：一个简单的用户请求可能触发成百上千次非确定性的模型推理、工具调用和数据库交互。传统的应用性能监控（APM）工具，基于确定性的堆栈跟踪和静态的拓扑图，已无法完全解释这种动态生成的行为链路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524760" alt="图片" title="图片" loading="lazy"/></p><p>与此同时，数据重力的法则依然生效且愈发严苛。随着生成式 AI 和多模态交互的爆发，企业产生的数据量呈指数级增长，但 IT 预算的增长却远远滞后。如何在数据爆炸的背景下，既保持对所有信号的敏锐捕捉，又严格控制存储成本，成为了 SRE 和 CIO 面临的头号难题。传统的“索引一切”（Index Everything）的日志管理模式在经济上已然破产，市场迫切呼唤一种全新的、基于存算分离架构的数据底座。<br/>本文将作为观测云（Guance）2026 年的产品技术展望，深入剖析在这一大变革背景下，我们如何通过产品演进解决测试、业务、数分、SRE 等多角色的核心痛点。我们将沿着“从上层业务应用到底层基础设施”的逻辑脉络，抽丝剥茧，呈现一个全栈可观测的 2026 图景。</p><h2>1. 市场趋势：驱动变革的四股力量</h2><p>在展开产品细节之前，我们需要厘清推动 2026 年可观测性技术变革的宏观力量。</p><h3>1.1 AI Agent 的崛起与黑盒治理危机</h3><p>2026 年，AI 不再是辅助工具，而是核心生产力。Gartner 指出，AI 原生开发平台正在让自主 Agent 协作完成复杂任务。然而，Agent 的引入带来了全新的不可预测性：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524761" alt="图片" title="图片" loading="lazy"/></p><ul><li>非确定性路径：Agent 的决策逻辑是动态生成的，传统的基于固定代码路径的 APM 难以追踪其思维链。</li><li>Token 经济学：每一次 API 调用都对应着真金白银。监控系统的核心指标从 CPU 使用率转向了“Token 消耗率”与“任务完成成本”。</li><li>黑盒风险：当 Agent 陷入死循环或产生幻觉时，传统的监控告警往往滞后，导致巨额的 API 费用浪费。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524762" alt="图片" title="图片" loading="lazy"/></p><h3>1.2 数据引力与存算分离的必然</h3><p>随着数字化转型的深入，企业数据量正以每年 180 EB 的速度增长。传统的基于本地磁盘（SSD/HDD）的存储架构（存算耦合）面临巨大的成本压力：</p><ul><li>扩容困境：为了增加存储空间，不得不增加计算节点，导致计算资源闲置浪费。</li><li>冷热数据鸿沟：90% 的查询集中在最近 24 小时的数据，但为了合规，企业必须存储数年的历史数据。将所有数据都放在昂贵的块存储上在经济上已不可行。</li><li>解决方案：市场正全面转向基于对象存储（S3/OSS）的存算分离架构，这也是 GuanceDB 演进的必然方向。</li></ul><h3>1.3 平台工程（Platform Engineering）与左移</h3><p>DevOps 正在进化为平台工程。开发者不再满足于被动接收告警，他们需要自服务的、可编程的观测能力。可观测性正在“左移”进入 CI/CD 流水线，开发者要求能够通过代码（Monitoring as Code）定义监控规则，并通过 API 触发自动化修复流程。</p><h3>1.4 FinOps 与数据主权的博弈</h3><p>随着全球数据法规（GDPR 等）的收紧，大型企业越来越倾向于“控制面与数据面分离”的架构。他们希望利用 SaaS 厂商提供的先进 AI 分析能力（控制面），但要求原始遥测数据保留在自己的云账号下的对象存储桶中（数据面），即 BYOS（Bring Your Own Storage）模式。</p><h2>2. 观测云新的产品功能：蓝图 (Blueprint)</h2><p>—— 可观测性编排与自动化引擎</p><p>在观测云 2026 的规划中，“蓝图”（Blueprint）不是一张静态的架构图或一套预设的 Dashboard 模板。基于最新的用户需求与 UI 设计，蓝图被重新定义为“官方组件支持计划”的核心载体，是一个低代码/无代码的可观测性编排与自动化引擎。</p><p>它通过可视化工作流（DAG - 有向无环图）将分散的观测能力串联起来，形成从 数据查询 -&gt; 逻辑转换 -&gt; AI 分析 -&gt; 行动 的完整闭环。</p><h3>2.1 蓝图的核心架构：可视化 DAG 工作流</h3><p>传统的监控告警是离散的：一个阈值触发一封邮件。而 2026 年的蓝图引擎引入了状态机与流式处理的概念。蓝图工作流由以下四类核心节点构成，支持用户通过拖拽方式构建复杂的运维逻辑：</p><h4>2.1.1 数据查询节点（Input / Sensor）</h4><ul><li><p>DQL (Data Query Language) 驱动：支持复杂的查询逻辑，包含了简单的指标阈值（如 CPU &gt; 80%），更加支持跨数据源的关联查询。</p><ul><li>示例：“查询最近 5 分钟支付接口的 P99 延迟，且仅当该延迟不仅超过阈值，同时伴随错误日志激增时触发。”</li></ul></li><li>多源异构：支持 Metrics、Logs、Traces、RUM（用户体验数据）的混合查询。</li></ul><h4>2.1.2 转换与逻辑节点（Processor / Logic）</h4><ul><li>低代码处理：支持 JavaScript/TypeScript 片段或表达式语言（Expression Language）。</li><li>上下文丰富：原始告警往往缺乏上下文。转换节点可以调用外部 CMDB 或 K8s API，为告警数据打上“业务线”、“负责人”、“部署版本”等标签。</li><li>价值：解决“告警疲劳”的核心手段。通过逻辑判断（如去重、抑制、时间窗聚合），将 100 条原始告警压缩为 1 条高价值根因分析。</li></ul><h4>2.1.3 AI 分析节点（Intelligence / Obsy AI）</h4><ul><li>ObsyAI 智能体介入：这是蓝图的智能核心。当逻辑节点检测到异常后，自动唤起 ObsyAI 进行根因分析。</li><li>能力：自动关联该时间段内的变更事件（Change Events）、错误日志聚类（Log Patterns）和异常链路等等。</li><li>输出：一段自然语言描述的诊断建议：“检测到支付服务延迟升高，关联到 3 分钟前 payment-service 的 v2.1 发布，且 DB 连接池报错激增。”</li></ul><h4>2.1.4 行动节点（Action / Actuator）</h4><ul><li>OpenAPI 闭环：这是蓝图与传统监控的最大区别。它通过 OpenAPI 与外部系统对接，执行实质性操作。</li><li><p>场景覆盖：</p><ul><li>通知：发送富文本消息到 Slack/钉钉/企业微信（包含 AI 诊断结果）等任意communication channel。</li><li>监控器管理：自动静默非核心服务的告警，或在流量高峰期动态调整阈值。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524763" alt="图片" title="图片" loading="lazy"/></p><h2>3. 更加全面的变更观测 (Change Observability)</h2><p>—— 根因分析的时间维度</p><h3>3.1 变更：系统熵增的核心问题</h3><p>根据 SRE 的经验法则，80% 的生产事故是由变更（Change）引起的。无论是代码发布、配置文件的修改、Feature Flag 的切换，还是基础设施的扩缩容操作，都是打破系统稳态的潜在因素。然而，传统的监控工具往往只记录了“结果”（Metrics 的突变、Logs 的报错），却丢失了“原因”（谁、在什么时候、做了什么变更）。</p><p>观测云 2026 将“变更”提升为与 Logs、Metrics、Traces 同等的一级数据公民（First-Class Citizen），构建了全维度的 变更观测（Change Observability） 体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524764" alt="图片" title="图片" loading="lazy"/></p><h3>3.2 变更数据的全栈采集与关联</h3><h4>3.2.1 统一变更数据模型</h4><p>为了捕捉系统中的每一次变化，观测云 2026 建立了一套标准化的变更数据模型：</p><ul><li>应用层：深度集成 Jenkins、GitLab、GitHub Actions 等 CI/CD 工具，自动捕获部署事件（Deployment）、Commit 信息、Artifact 版本。</li><li>基础设施层：监听 Kubernetes Events（如 Pod Killing, Scaling）、云厂商审计日志（如 AWS CloudTrail、阿里云 ActionTrail），捕获资源的创建、销毁和规格变更。</li><li>配置层：对接 Nacos、Apollo、Consul 等配置中心，实时记录配置项的 Diff。记录配置变了，还记录从什么变成了什么。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524765" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.2 变更叠加分析（Change Overlay）</h4><p>变更观测的核心价值在于上下文的融合。在观测云的所有时序图表（Metric Charts）上，系统会自动叠加变更事件的标记（Annotations）。</p><ul><li><p>场景示例：</p><ul><li>传统视图：看到 API 错误率曲线在 14:00 突然飙升，SRE 开始排查日志。</li><li>变更观测视图：看到错误率飙升的同时，时间轴上显示 13:59 分有一个“支付服务 v3.2 Canary 发布”的标记。鼠标悬停即可看到该发布的 Commit Message 和变更人。</li></ul></li></ul><p>这种直观的视觉关联，能够将 MTTR（平均修复时间）从小时级缩短至分钟级。运维人员不再需要去各个聊天群里询问“刚才谁动了线上环境？”，变更观测直接给出了答案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524766" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.3 变更风险评分与智能门禁</h4><p>结合 Arbiter 引擎的历史分析能力，系统能对每一次变更进行风险评分。如果某次代码提交修改了核心链路的关键文件，且缺乏足够的测试覆盖率，或者历史数据显示该开发者的变更回滚率较高，系统将在变更发生前发出预警，甚至联动 CI/CD 流水线进行阻断。</p><h2>4. Obsy AI SRE Agent 推出：可交互的根因分析侦探</h2><p>观测云 2026 颠覆了传统人找数据的排查模式，推出了一套基于 动态假设树（Dynamic Hypothesis Tree） 的交互式排查界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524767" alt="图片" title="图片" loading="lazy"/></p><h3>4.1 触发与情境感知</h3><p>当监控器发现异常（例如 flight-query-api 接口响应时间 P99 &gt; 2s），系统将直接启动 Obsy AI SRE Agent。在观测云的 Console 中，用户会看到一个关联了错误上下文（Error Trace、Latency Chart）的交互式卡片。</p><h3>4.2 动态假设引擎（Dynamic Hypothesis Engine）</h3><p>AI Agent 不会盲目列出所有指标，而是像一位经验丰富的 SRE 工程师一样进行逻辑推演。它会基于当前的异常特征，生成多条排查路径（Investigative Plans），并依据历史数据和专家知识库计算出每一条路径的置信度概率：</p><ul><li>Plan A (概率：高)：假设为数据库超时（DB Connection Block / Slow SQL）。</li><li>Plan B (概率：低)：假设为上游依赖服务响应变慢。</li><li>Plan C (概率：低)：假设为网络网关故障。</li></ul><h3>4.3 交互式思维导图与递归诊断</h3><p>用户点击高概率的 Plan A，界面将展开一个可视化的排查思维导图。这不仅仅是静态图表，而是 AI 正在执行的逻辑动作流：</p><ul><li>节点展开：Agent 自动检查 "RDS 资源水位" -&gt; "数据库连接池状态" -&gt; "慢查询日志分析"。</li><li>执行验证：每个节点会显示执行状态（Check Passed / Failed）。例如，AI 发现连接池正常，但捕获到了一条全表扫描的慢 SQL。</li><li>根因锁定：当 AI 找到确凿证据（如：flight_no 字段缺失索引导致全表扫描），它会标记为“Root Cause Identified”，并生成自然语言的结论报告。</li></ul><h3>4.4 闭环与反馈</h3><ul><li>对话式追问：在锁定根因后，用户可以直接与 Agent 对话：“如何修复这个问题？”Agent 会根据知识库提供 Runbook 建议（如：添加索引的 SQL 语句）。</li><li>多路径回溯：如果 Plan A 的排查结果显示一切正常（Negative Result），Agent 会智能建议用户切换至 Plan B 或 Plan C。系统会自动保留已排查过的路径记录，避免重复工作，直到递归找到真正的问题源头。</li><li>人工接管：整个 UI 包含清晰的 "Abort/Take Over" 按钮，允许工程师随时打断 AI 的自动化逻辑，手动介入排查。</li></ul><p>这套设计融合了现代工程美学与 AI 智能，将原本黑盒的 AI 思考过程透明化（White-box），让 SRE 既能享受 AI 的效率，又能保持对排查逻辑的掌控。</p><h2>5. GuanceDB 演进策略：云原生内核的重构</h2><p>GuanceDB 3.0 是观测云强健的心脏。现有的数据库架构大多基于本地磁盘（Shared-Nothing 架构），在面对 PB 级数据时，扩展成本高昂且缺乏弹性。GuanceDB 3.0 的核心目标是演进为基于对象存储（S3-Native）的存算分离架构。在这一演进过程中，我们必须正视目前与行业标杆的技术差距，并提出针对性的优化策略。</p><h3>5.1 关键演进挑战与探索方向</h3><p>GuanceDB 的演进要解决对象存储带来的物理限制：高延迟与元数据管理。</p><h4>5.1.1 挑战一：海量小文件元数据瓶颈 (Metadata Bottleneck)</h4><ul><li>痛点：在实时写入场景下（如 IoT），会产生数以亿计的小文件（Objects）。如果 GuanceDB 3.0 的元数据层不够强大，查询时的“列出文件”操作就会成为瓶颈，导致查询超时。</li><li><p>演进方向：分布式元数据架构</p><ul><li>探索：不再依赖单体 SQL 数据库存储元数据。探索分布式 Key-Value 存储来构建元数据层。</li><li>目标：支持每秒数十万次的元数据读写，确保即使底层有百亿个 S3 对象，查询规划器也能在毫秒级定位到需要扫描的文件。</li></ul></li></ul><h4>5.1.2 挑战二：存算分离后的查询延迟 (Cold Start Latency)</h4><ul><li>痛点：S3 的首字节延迟（TTFB）通常在几十到几百毫秒。对于“老板看数”的实时 Dashboard 场景，这种延迟是不可接受的。</li><li><p>演进方向：智能分层与分布式缓存 (Smart Tiering &amp; Caching)</p><ul><li>热数据 (Hot)：近期的数据查询直接走本地内存/磁盘，速度极快。</li><li>温数据 (Warm)：引入分布式缓存层。对于经常访问的“昨天”或“上周”的数据，在计算节点的 SSD 上进行 LRU 缓存。</li><li>冷数据 (Cold)：完全沉淀在 S3。查询时按需拉取，接受秒级延迟，换取极致成本。</li><li>价值：实现“像 SSD 一样快，像 S3 一样便宜”。</li></ul></li></ul><h4>5.1.3 挑战三：Compaction (压缩) 策略与写放大</h4><ul><li>痛点：为了优化查询，必须将 S3 上的小文件合并为大文件（Compaction）。但 S3 的 PUT 操作是收费的，且消耗网络带宽。</li><li><p>演进方向：成本感知的智能 Compaction</p><ul><li>策略：不盲目压缩。引入基于“查询热度”和“S3 计费模型”的代价函数。</li><li>探索：利用 Spot Instances（竞价实例）在云厂商的闲时运行 Compaction 任务，将小文件合并为列式存储（Parquet/ORC 变体），同时构建布隆过滤器（Bloom Filters）和 Min/Max 索引，以减少未来的扫描量。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524768" alt="图片" title="图片" loading="lazy"/></p><h2>6. 落地 Targeting Needs：场景化痛点的精准打击</h2><p>技术必须服务于业务。不同的大客户场景对数据平台的需求是截然不同的，甚至是互斥的。我们不能用一套参数满足所有人，而是提供灵活，可以满足特种需求的的数据引擎。</p><h3>6.1  场景一：实时查询（Real-Time Query）—— 老板看数</h3><ul><li>用户：CIO、CTO、NOC 监控大屏。</li><li>痛点：Dashboard 需要秒级刷新。读多写少，并发高。传统的 OLAP 引擎在处理聚合查询时延迟较高，且并发能力受限。</li><li><p>观测云 2026 解决方案：流式聚合。</p><ul><li>原理：GuanceDB 不再每次刷新都扫描原始日志。在数据摄取（Ingest）阶段，通过流式预聚合引擎（Pre-aggregation Engine）自动维护常用指标（如 Global_Error_Rate）。</li><li>效果：Dashboard 查询实际上是在读取一张极小的预计算表，无论原始数据量是 1TB 还是 1PB，大屏刷新始终保持在亚秒级。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524769" alt="图片" title="图片" loading="lazy"/></p><h3>6.2 场景二：批量报表与数据挖掘 —— 分析师的深潜</h3><ul><li>用户：SRE 专家、安全分析师、运营人员。</li><li>痛点：读少，但 IO 极重。需要扫描过去 30 天的海量日志进行根因分析或生成月度运营报告。容易导致数据库 OOM (Out of Memory) 或查询超时。</li><li><p>观测云 2026 解决方案：向量化执行引擎 + Serverless 扫描。</p><ul><li>原理：利用存算分离架构，当检测到此类大查询时，GuanceDB 动态弹出一组 Serverless 计算节点（Worker），并行扫描 S3 上的数据块。利用 SIMD 指令集和向量化执行（Vectorized Execution）加速过滤。</li><li>开放性：支持通过 DQL 导出数据到 Notebook 或外部数仓，满足深度挖掘需求。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524770" alt="图片" title="图片" loading="lazy"/></p><h3>6.3 场景三：高并发写入 —— IoT 与车联网数据海啸</h3><ul><li>用户：车企（V2X）、智能制造、IoT 架构师。</li><li>痛点：写多读少。Tag（标签）基数极高（High Cardinality）。例如，百万辆车，每辆车有唯一的 VehicleID，传统时序数据库的倒排索引会因此膨胀爆炸，导致内存溢出。</li><li><p>观测云 2026 解决方案：稀疏索引与列式存储优化。</p><ul><li>原理：放弃对高基数 Tag 建立全量倒排索引。GuanceDB 借鉴先进的架构设计，采用 稀疏索引（Sparse Indexing）和数据分区（Micro-partitions） 技术。</li><li>效果：将 VehicleID 作为排序列，通过 Min/Max 索引快速跳过无关数据块。在不牺牲写入性能的前提下，支持对高基数标签的高效过滤，彻底解决“索引爆炸”问题。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524771" alt="图片" title="图片" loading="lazy"/></p><h3>6.4 场景四：AI/LLM 可观测 —— Agent 行为治理</h3><ul><li>用户：AI 平台工程师、大模型应用开发者。</li><li>痛点：Agent 行为具有不确定性（幻觉、死循环），且 Token 成本昂贵。传统的 CPU/内存监控无法反映 AI 业务的健康度。</li><li><p>观测云 2026 解决方案：Model Telemetry 与成本归因。</p><ul><li>数据模型：引入专用的数据类型追踪 Prompt 和 Completion 的 Token 消耗、延迟、模型版本。</li><li>蓝图集成：通过蓝图实时监控 Token 消耗速率。一旦发现某个 Agent 陷入死循环（Token 消耗斜率异常），立即触发熔断机制（Action 节点），并通知开发者。</li><li>价值：进阶到 AI 业务治理，为企业节省真金白银的算力成本。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524772" alt="图片" title="图片" loading="lazy"/></p><h3>6.5 场景五：日志成本黑洞 —— 拒绝存不起，查不到</h3><ul><li>用户： 运维总监、合规审计部门、FinOps 负责人。</li><li>痛点： 日志数据量呈指数级增长（每天几十 TB），但 99% 的日志通常都用不上，只有故障时才需要回溯。传统方案要么全量索引导致存储成本天价，要么为了省钱只存 3 天导致关键数据丢失。</li><li><p>观测云 2026 解决方案： 冷热分层（Tiered Storage）+ Schema-on-Read（读时建模）。</p><ul><li>原理： GuanceDB 引入智能分层策略。热数据（最近 3 天）存高性能 SSD 并建立全索引；温/冷数据（3 天 - 3 年）自动下沉至对象存储（S3/OSS），不建立繁重倒排索引。当需要查询冷数据时，利用算子下推（Pushdown）临时扫描目标块。</li><li>效果： 将日志的长期存储成本降低 80% 以上。让企业存得起海量日志，还能在需要审计时，无需数据迁移即可直接查询历史归档。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524773" alt="图片" title="图片" loading="lazy"/></p><h3>6.6 场景六：微服务风暴 —— 抓住百万分之一的异常</h3><ul><li>用户： 架构师、中台研发负责人。</li><li>痛点： 在成百上千个微服务的调用链中，每天产生数亿条 Trace 数据。传统 APM 采用头部采样（Head-based Sampling）（如只采 1%），容易导致“关键的报错请求正好被丢弃了”，无法还原故障现场。</li><li><p>观测云 2026 解决方案： 100% 全量摄取 + 尾部采样（Tail-based Sampling）。</p><ul><li>原理： 数据进入系统时不做丢弃，先在内存缓冲区暂存。通过流式引擎实时分析整条链路的尾部状态（是否报错、是否高延迟）。只有有问题或高价值的链路才会被持久化存储，正常的无用链路自动丢弃。</li><li>价值： 在不增加存储预算的前提下，实现100% 的异常捕获率。不再靠运气抓 Bug，而是靠精准的算法。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524774" alt="图片" title="图片" loading="lazy"/></p><p>当然以上仅是冰山一角。观测云的统一数据底座已打破场景壁垒，无论是日志降本还是链路追踪，皆能以一套架构，从容应对万千需求。</p><h2>结语：观测云的2026</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524775" alt="图片" title="图片" loading="lazy"/></p><p>观测云 2026 的产品预告是对未来观测形态的一次预判与押注。</p><ul><li>市场在变：AI Agent 带来了复杂性，FinOps 带来了成本压力，数据主权带来了架构约束。</li><li>产品在变：蓝图将会成为企业的自动化中枢；GuanceDB 拥抱 S3，打破存储的物理边界，用云原生的架构解决云时代的规模问题。</li><li>价值在变：我们针对不同角色（CIO、SRE、IoT 架构师、AI 工程师等等）提供不同场景都可用的灵活解决方案。</li></ul><p>对于 CTO 和 CIO 而言，选择观测云 2026，不仅是选择了一个监控平台，更是选择了一套能够驾驭 AI 时代不确定性、从容应对数据洪流的系统。请查收这份产品路线图。</p>]]></description></item><item>    <title><![CDATA[从“救火”到“预见”：汽车行业操作系统智能运维解决方案 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524793</link>    <guid>https://segmentfault.com/a/1190000047524793</guid>    <pubDate>2026-01-06 18:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>汽车行业趋势与核心挑战</h3><p>近年来，新能源汽车加速普及，智能座舱、车联网和智能辅助驾驶等技术已成为整车厂商竞争的关键。这些功能基于端云协同架构，云端基础设施至关重要——无论是用户在车上点播音乐、远程控制车辆，还是智能车联网系统上传传感器数据，背后都离不开稳定、高效的基础设施云平台支持。</p><p>随着车辆联网率的提升以及 AI 模型能力的增强，汽车行业IT系统的数据吞吐量与计算负载呈指数级增长。一辆具备智能辅助驾驶能力的测试车，单日即可产生数 TB 的原始数据；一次面向百万用户的 OTA 升级，也可能在短时间内引发流量洪峰。在此业务特点下，云端基础设施的稳定性已成为直接影响用户体验甚至行车安全的核心环节。</p><h4>汽车行业的基础设施面临的四大核心运维挑战</h4><p>在上述业务压力下，支撑汽车场景的基础设施频繁遭遇以下四类典型问题，传统的运维手段往往难以有效应对：</p><p>1、周期性高峰业务-资源超载与系统夯机<br/>在 OTA 推送或早晚高峰、节假日远程控制集中触发时，服务器内存和 CPU 瞬时过载，系统进入“假死”状态——进程无法调度、命令无响应，即使未完全宕机，业务也已不可用。</p><p>2、出行服务下的资源超卖-内存失控与服务中断<br/>内存泄漏、缓存膨胀或显存异常增长等问题隐蔽性强，初期不易察觉，但会逐步耗尽系统资源，最终触发OOM（Out-Of-Memory）导致关键进程被强制终止，服务中断。</p><p>3、车联网服务响应迟滞-性能抖动与偶发卡顿<br/>系统在多数时间运行正常，却偶尔出现毫秒级延迟突增，且无法稳定复现。这类问题通常源于锁竞争、高频系统调用或 I/O 瓶颈，传统监控指标难以捕捉根因。</p><p>4、智能驾驶业务-智算可观测能力缺失<br/>在 GPU 集群中，显存使用异常、NCCL 通信失败、任务卡死等问题频发，但缺乏从应用层到硬件层的全栈观测能力，导致排查周期长、依赖人工经验，严重影响模型训练与推理效率。</p><p>这些问题共同指向一个核心诉求：汽车行业需要一套能够贯通“应用—操作系统—硬件”的智能运维体系，实现故障的提前预警、精准定位与自动恢复，而非被动响应。</p><h3>通过操作系统管理平台一站式解决 OS 运维卡点</h3><h4>操作系统管理平台介绍</h4><p>操作系统控制台是阿里云自研的操作系统管理平台，覆盖主流 Linux 操作系统，旨在为客户提供便捷易用、高效、专业的操作系统生命周期管理能力，包括运维管理、操作系统智能助手 OS Copilot、订阅等功能，支持通过界面、OpenAPI、MCP、CLI 等多种方式提供服务。致力于降低操作系统的技术门槛，通过系统解决客户应用与云平台运维信息不对称等问题，提升用户的云上体验。操作系统控制台智能运维可以让用户摆脱冗长的运维垂直栈和分析链，让平台更懂用户业务的异常根因，懂资源的消耗。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=vmTNRifaTQOsj1de9lMeow%3D%3D.uYj4k8BtGkFAkkSovLqFxFlnyZL96ZPP%2FZ%2FvAgkmMfOIAvB7mEJWlvfr%2FDOs6kjW" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524795" alt="图片" title="图片"/></p><h4>解决方案概述</h4><p>面对智能座舱与自动驾驶业务对云端基础设施提出的高并发、低延迟、强稳定等严苛要求，传统运维手段已难以应对资源超载、内存失控、性能抖动和 AI 任务异常等复杂问题。操作系统控制台作为面向汽车行业的综合运维平台，致力于打通“应用—操作系统—硬件”全栈链路运维能力。</p><h4>场景化解决方案与核心能力</h4><p>针对智能座舱、自动驾驶等业务以上提到的汽车行业四大典型运维痛点，操作系统控制台推出对应的诊断及观测能力，在常见的夯机、OOM、抖动及 AI 观测都给出了对应的解决方案，弥补汽车行业的企业在基础设施可观测性的能力短板。</p><p>应对资源超载与系统夯机 —— 主动内存保护</p><p>核心收益：解决用车周期性高峰业务场景，资源的夯机问题，减少业务卡顿及异常。</p><p>适用场景：OTA 大规模推送、远程控制指令洪峰、AI 模型高并发推理等瞬时高负载场景。</p><p>在高峰期，系统常因内存迅速耗尽而进入“near-OOM”状态，传统 Linux OOM 机制响应滞后，往往在系统已卡死或无响应后才触发进程终止，且易误杀缓存型或 I/O 密集型进程，进一步加剧磁盘压力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524796" alt="图片" title="图片" loading="lazy"/></p><p>通过以下机制实现主动防护：</p><ul><li>堆内存精准评分：不再依赖 RSS（常驻内存），而是聚焦可回收的堆内存使用量，更准确识别真正造成内存压力的进程。</li><li>批量终止策略：单次释放不足以缓解压力时，可同时终止多个高内存占用进程，快速释放大量内存。</li><li>多级压力响应：支持低、中、高三档灵敏度配置，适配不同业务对延迟的容忍度。</li><li>关键进程白名单：通过进程名或命令行参数显式保护车控、推理等关键服务，避免误杀。</li></ul><p>在内存压力上升初期即介入干预，有效防止系统夯机，保障远程控制、OTA 下发等关键指令的可达性和执行时效。</p><p>破解内存黑盒 —— 内存全景分析</p><p>核心收益：解决出行出行服务下的资源超卖所引起的服务中断问题，提升业务连续性。</p><p>适用场景：内存使用率持续飙升、频繁触发 OOM、缓存占用异常、GPU 显存增长不明等复杂内存问题。</p><p>传统运维难以回答“内存到底被谁用了”——是应用泄漏？文件缓存堆积？还是驱动或 GPU 隐性占用？内存全景分析提供统一、细粒度的内存视图。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524797" alt="图片" title="图片" loading="lazy"/></p><ul><li>一键生成全链路报告：无需登录机器，控制台点击即可输出包含进程、容器、缓存、驱动、GPU 显存的完整内存分布。</li><li>穿透应用堆内存：支持对 Java、Python、C++ 等语言进程的堆内对象进行二次拆解，定位具体泄漏点。</li><li>关联缓存与原始文件：如识别出“/ota/firmware_v2.1.bin”占用了 8GB page cache，便于优化预加载或清理策略。</li><li>纳入 GPU 与网卡内存：将 RDMA 缓冲区、GPU 显存映射等“不可见”内存纳入监控范围，消除盲区。</li></ul><p>内存全景从“猜测谁吃内存”转变为“秒级定位泄漏源”，显著缩短故障排查时间，支撑容量规划与资源优化。</p><p>消除性能抖动 —— 进程热点分析</p><p>核心收益：解决车联网服务响应迟滞问题，提升用户体验。</p><p>适用场景：偶发性卡顿、CPU 或 I/O 突发飙升、毫秒级延迟突增等难以复现的性能问题。</p><p>这类问题往往无固定复现路径，传统监控无法捕获瞬时调用栈，导致根因长期悬而未决。</p><p>进程热点分析基于 eBPF 实现轻量、持续追踪，该功能具有以下特点：</p><ul><li>小于 3% 性能开销：无侵入采集函数调用栈、上下文切换、系统调用等数据，适用于生产环境长期运行。</li><li>火焰图 + Diff 对比：直观展示 CPU 热点路径，并支持抖动前后或版本升级前后的性能差异比对，自动高亮退化点。</li><li>智能识别：结合大模型语义理解，识别高频/proc 访问、锁竞争、阻塞 I/O 等常见性能陷阱，并给出优化建议。</li><li>秒级回溯抖动时刻：系统持续缓存轻量调用栈，问题发生时可立即锁定瞬时高负载进程及其热点函数。</li></ul><p>进程热点分析解决了“无法复现”的性能难题，让偶发卡顿变得可追踪、可解释、可修复。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524798" alt="图片" title="图片" loading="lazy"/></p><p>保障智算基础设施的稳定 —— GPU 持续追踪时序图</p><p>核心收益：解决智能驾驶业务在 GPU 场景运维难的问题，提升训推效率，节省成本。</p><p>适用场景：自动驾驶模型训练、vLLM 等大模型推理、多 GPU 通信任务等 AI 密集型负载。</p><p>AI 任务对 GPU 资源稳定性高度敏感，但显存泄漏、XID 错误、通信瓶颈等问题往往隐蔽且难定位。</p><p>操作系统控制台构建基于内核的持续追踪体系，它具有以下特点：</p><ul><li>分钟级异常告警：实时监控显存、SM 利用率、温度、XID 错误码等，及时发现GPU掉卡、硬件报错或任务卡死。</li><li>小时级问题定界：支持慢节点识别、NCCL 通信延迟分析、单卡/整机资源瓶颈判断，快速缩小排查范围。</li><li>函数级根因剖析：通过 GPU 火焰图和 Timeline Profiling，将 Python 层调用、框架算子与 CUDA Kernel 关联，可视化算子执行序列与等待时间。</li><li>让 AI 任务“看得见、说得清、改得准”，避免因底层资源异常导致训练中断或推理延迟，提升 AI 基础设施可靠性。</li></ul><h3>行业成功案例分享</h3><h4>案例一：车机服务高峰期无响应</h4><p>案例背景</p><p>某头部物流行业用户节假日出现业务无响应、登录实例也十分卡顿。通过监控发现客户实例使用的内存在某个时间点开始徒增，接近系统的总内存（即 available 非常低），但没有超过系统总内存。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524799" alt="图片" title="图片" loading="lazy"/></p><p>通过 top 命令可以看到系统的 CPU sys 利用率和 iowait 利用率和系统负载都持续飙高，kswapd0 线程占用非常高的 CPU 进行内存回收。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524800" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>通过配置开启节点级别的 FastOOM 功能，由于业务是实验较为敏感的业务，内存压力选择中，且设置业务程序（以 python 启动，进程名包含 python 子串）为避免被 OOM 进程且设置无关的日志程序优先杀死。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524801" alt="图片" title="图片" loading="lazy"/></p><p>开启后，当节点内存水位处于 near-OOM 状态时，用户态提前介入，根据配置杀死了如下进程，从而释放了部分内存避免系统进入了夯机状态。通过操作系统控制台的系统概览可以看到 FastOOM 介入的相关记录。</p><p>如下图所示，由于 kube-rbac-proxy 和 node_exporter 等进程 oom_score_adj 被设置为接近 999，FastOOM 会匹配内核策略优先杀死这些进程，但是由于杀死这些进程后释放内存较小，仍处于 near-OOM；因此 FastOOM 杀死了配置优先杀死的 logcollect 进程。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524802" alt="图片" title="图片" loading="lazy"/><br/>由于用户态及时介入杀死进程释放出内存，使系统避免进入了near-OOM的抖动状态。</p><h4>案例二：AI 推理场景显存异常增长</h4><p>案例背景<br/>某头部自动驾驶方案公司部署的 vLLM 线上推理服务：KV-Cache 利用率并未打满，但通过 GPU 监控（DCGM）观察到有显存明显增长。vLLM 启动时使用显存预分配机制，在 KV-Cache 利用率未满情况下理论显存值不应上涨。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524803" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>对在线业务应用进行 continuous Profiling，在 TimeLine 上找到显存申请的 cudaMalloc 调用，打上标记线，即可找到具体的 Python 调用，进一步定位到导致显存额外申请的调用栈如下所示，结合 decorate_context() 实现可以判断出显存增长的原因 Torch 的缓存管理机制，可以通过调整 vLLM 显存预占或 Torch 缓存的显存占用环境变量来进行相应的问题规避。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524804" alt="图片" title="图片" loading="lazy"/></p><h4>案例三：智能汽车全球发布会——高并发实时交互下的零卡顿保障</h4><p>案例背景<br/>2025 年春季，某智能电动汽车品牌在全球同步发布其旗舰车型，并启动大规模整合营销活动。由于发布会覆盖全球多个时区，且关键的价格公布环节引发高度关注，价格揭晓后 5 分钟内，App 总访问量突破 800 万，商城相关接口请求峰值高达 12 万 QPS，整体流量达日常水平的 200 倍以上。在此极端并发场景下，系统面临严峻挑战：核心交互接口的端到端响应必须控制在 30ms 以内，任何毫秒级的延迟都可能导致 APP 白屏、操作无响应或直播卡顿，严重影响用户体验并威胁品牌形象。与此同时，瞬时流量洪峰、极致的体验敏感性以及秒级故障定位与恢复的严苛要求，使得传统依赖日志回溯的运维排查方式完全失效，系统稳定性与实时可观测性面临前所未有的考验。</p><p>解决方案<br/>依托操作系统控制台构建“三位一体”保障体系：</p><p>1.高并发资源防护 —— 主动内存保护 + 关键进程隔离 提前识别车控指令服务、视频流网关、身份认证微服务为 关键路径组件，加入 FastOOM 白名单； 配置中等灵敏度内存压力策略，在系统进入 near-OOM 前主动释放低优先级进程内存，避免 kswapd 抢占 CPU 导致 API 延迟飙升，实现发布会全程实现 “零白屏、零卡顿、零交互失败”。</p><p>2.实时性能监控 —— 进程热点分析持续追踪 全链路启用 eBPF 驱动的进程热点分析，持续采集函数调用栈； 当某区域用户集中反馈“点击无反应”时，系统秒级回溯到问题时刻； 结合大模型辅助诊断，自动建议“缓存网络指标”而非实时读取，热修复后延迟 P99 从 50ms 降至 30ms。</p><h3>展望</h3><p>随着智能电动汽车的持续发展，车载系统与云端基础设施的耦合将更加紧密。未来，汽车不仅是交通工具，更是移动的计算终端和数据节点。这要求云平台不仅具备更强的弹性、更低的延迟和更高的可靠性，还需在资源调度、故障自愈和性能优化等方面实现更深层次的智能化。</p><p>操作系统控制台将持续围绕汽车行业核心场景打磨能力。一方面，我们将进一步强化对高并发、高实时性业务的支持，优化 FastOOM、内存全景分析、进程热点追踪等能力在 OTA 洪峰、自动驾驶训练推理等典型负载下的表现；另一方面，我们将探索 AI 驱动的智能运维（AIOps）路径，结合大模型与实时可观测数据，构建具备预测、诊断、决策和执行能力的 AI Agent 运维体系。</p><p>联系我们 </p><p>您在使用操作系统控制台的过程中，有任何疑问和建议，可以搜索群号：94405014449 加入钉钉群反馈，欢迎大家扫码加入交流。</p>]]></description></item><item>    <title><![CDATA[专访 | 不仅是适配，更是定义标准：中兴新支点在龙蜥大会交出的“2025答卷” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524830</link>    <guid>https://segmentfault.com/a/1190000047524830</guid>    <pubDate>2026-01-06 18:09:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>编者按：近日，2025 龙蜥操作系统大会（OpenAnolis Conference）在北京圆满召开，主题为“生态共融·智驱未来”。现场汇聚了逾千位全球技术领袖、业界精英和行业开发者，共同探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。作为龙蜥社区的核心理事单位及龙蜥大会的承办方之一，中兴通讯走向台前，分享了其在 OS 架构创新与场景拓展方面的深度思考，并设立专门的分论坛与展区，集中展示新支点操作系统在 RISC-V 及工业领域的最新核心成果。大会前夕，广东中兴新支点技术有限公司总经理王健雄在与 InfoQ 的深度对话中描绘了一幅全新的战略版图：2025 年，中兴新支点操作系统已经不仅仅满足于传统的架构适配，而是向“AI 原生”与“RISC-V 深水区”发起全面冲击。以下为采访全文：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524832" alt="图片" title="图片"/></p><p>过去几年，中兴新支点操作系统常常被贴上“工业级”、“安全稳定”、“电信场景专家”的标签。但在 2025 年，中兴在操作系统上显然有着更大的雄心。</p><h3>V7 全面适配主流大模型，RISC-V 全栈技术得以攻坚</h3><p>王健雄表示，中兴通讯一直都高度重视和开源社区的互动与合作，目前已组建起专业的团队，深度参与龙蜥社区的 AI 生态建设，在技术贡献、组织协同和生态共建方面都在持续投入。</p><p>他还强调，2025 年中兴新支点发布的新支点服务器操作系统 V7 作为面向 AI 时代的新型操作系统，全面适配了主流的 AI 硬件大模型及计算框架，具备内生智能特性，加速了用户在 AI 场景的落地进程。</p><p>面对 AI 算力爆发与大模型落地的行业浪潮，操作系统作为承上启下的核心软件底座，必须完成从“管理资源”到“加速智能”的进化。</p><p>王健雄在采访中透露，2025 年，“新支点服务器操作系统 V7”版本的发布，也并不仅仅是一次版本号的迭代，更是一次产品形态的重塑。V7 版本被定义为“面向 AI 时代的新型操作系统”，其核心战略价值体现在两个维度：</p><ul><li>具备“内生智能”特性：区别于过去“打补丁”式的 AI 支持，V7 版本在内核层构建了智能特性，能够全面适配主流的 AI 硬件（GPU/NPU/DPU）、大模型（如DeepSeek）以及主流计算框架。</li><li>加速场景落地：这一新版本将不仅服务于中兴自身的通信产品体系，更致力于显著加速用户在 AI 场景下的落地进程，解决软硬协同难、部署效率低的问题。</li></ul><p>如果说 AI 是应用层的风口，那么 RISC-V 则是芯片架构层的“未来变量”。在多架构支持方面，中兴新支点在 2025 年也展现出“All in”的姿态，尤其在 RISC-V 领域，其投入力度和技术深度均超出传统的适配范畴。</p><p>王健雄详细拆解了中兴在 RISC-V 领域的四大核心动作，标志着其工作重心正从“基础适配”转向“性能进取”：</p><ul><li>全量组件构建与社区首发：中兴推动成立了 RISC-V SIG 2.0 协作组，计划在2025年 完成全量 2000+ 组件的构建，并发布社区首个 RISC-V 同源预览版本。这意味着 RISC-V 在龙蜥生态中将拥有与 x86/Arm 同等的核心地位。</li><li>底层性能的极致优化：针对RISC-V 基础库的性能，中兴进行了向量化（Vectorization）及汇编级的优化。这表明中兴已经不满足于软件“能跑”，而是要让软件在 RISC-V 硬件上“跑得快”。</li><li>关键特性支持（RVA23）：基于 Linux6.6 内核基线，新支点操作系统将实现对 RISC-V 服务器关键特性的支持，包括 AIA（高级中断架构）、IOMMU（输入输出内存管理单元）以及最新的 RVA23 指令集。这些硬核技术的突破，是 RISC-V 能够真正走进高性能计算和数据中心场景的前提。</li><li>通过 V7 版本的发布与 RISC-V 全栈技术的攻坚，中兴新支点操作系统在 2025 年的战略意图十分清晰：在龙蜥社区的多架构生态中，构建一个既能承载传统通信业务高稳定性，又能适应 AI 时代异构算力挑战的“双模”底座。</li></ul><h3>填补全球空白：首个电信级“AI 试金石”</h3><p>在通用 AI 模型“卷”参数量的当下，垂直行业的 AI 落地却面临着一个隐蔽的痛点：缺乏针对特定工业场景的标准化评测基准。对于通信行业而言，这一问题尤为突出——5G 网络切片、6G 信道建模等场景极其复杂，通用数据集难以覆盖，导致电信级 AI 模型的训练与调优往往面临“无尺可量”的窘境。</p><p>2025 年，中兴新支点打破了这一僵局，将其在通信领域深耕多年的“独门秘籍”——TFCE 数据集贡献到开源社区，试图为行业建立一套全新的 AI 评测标准。</p><p>王健雄在采访中强调，TFCE 数据集并非普通的行业数据包，而是“全球首个面向电信场景的函数级、调用级 AI 评测标准”。这一成果的开源，填补了国际上在电信 AI 测评领域的空白。</p><p>TFCE 的核心价值在于其“广度”与“精度”。根据中兴披露的数据，该数据集包含了超过 20 万条通信设备的函数调用样本，覆盖了 5G 核心网、计算、智能运维、传输等八大典型电信场景。</p><p>通过与龙蜥社区的深度协同，TFCE 不再仅仅是静态的数据，它提供了一套标准化的接口协议与性能指标体系（涵盖时延、准确率、资源占用等维度）。这意味着，未来开发者在龙蜥操作系统上开发通信类 AI 应用时，将拥有一把“标准尺”，彻底解决了电信 AI 模型训练中“缺乏行业基准（Benchmark）”的痛点。</p><p>数据标准的建立，最终是为了反哺系统性能。中兴新支点操作系统团队并没有止步于数据集的开源，而是将 TFCE 的评测工具链与操作系统进行了深度融合。</p><p>这种“操作系统+行业数据”的协同效应，直接转化为惊人的性能数据。王健雄列举了两组极具说服力的成果：</p><ul><li>硬件加速效率提升：通过 TFCE工 具链优化通信函数调用在底层硬件上的执行路径，新支点操作系统成功将 DPU（数据处理器）和 NPU（神经网络处理器）的资源利用率提升了 40%。</li><li>运维推理速度翻倍：在基站故障日常运维这一高频场景中，基于 TFCE 优化的智能运维推理引擎，将推理效率提升了 60%。</li></ul><p>从中可以看出，中兴在 2025 年的打法非常务实：利用独有的行业数据资产（TFCE），驱动操作系统内核与 AI 硬件的深度磨合。这不仅提升了新支点操作系统的行业竞争力，更为龙蜥社区在工业、通信等垂直领域的 AI 落地提供了教科书级别的范例。</p><h3>攻坚深水区——解决“算力与通信融合”的四大痛点</h3><p>当 AI 的浪潮涌入 5G/6G 通信网络，“云网融合”不再是一个概念，而是具体的工程挑战。王健雄直言，在 AI 与通信技术深度融合的当下，服务器操作系统正面临着前所未有的“异构算力调度”压力。</p><p>传统的操作系统设计以 CPU 为中心，而现在的智算场景需要 CPU、GPU、DPU、NPU 等多芯协同。这种架构的错位，直接导致了行业内一个尴尬的现状：AI服务器的算力利用率普遍仅在 30%-40% 左右徘徊。</p><p>为了打破这一瓶颈，中兴新支点将 2025 年的技术攻坚重点锁定在“四大核心痛点”上，并给出了基于龙蜥生态的破局之道。</p><h4>直面四大“拦路虎”</h4><p>王健雄详细剖析了当前阻碍 AI 在通信场景落地的技术壁垒：</p><p>算力调度与适配难题：传统 OS 难以高效管理异构资源，且不同硬件厂家的驱动接口差异巨大，导致操作系统如同“盲人摸象”，难以进行全局最优调度。</p><p>低时延与高稳定性的双重挤压：AI 运维需要毫秒级响应，智能体调度要求指令实时下发。但传统 OS 的内存访问、中断处理等环节往往会引入不可控的时延抖动，这对于要求“确定性网络”的通信业务是致命的。</p><p>全栈生态的“变速齿轮”：上层 AI 框架（如PyTorch、TensorFlow）迭代周期缩短至季度级，而底层的通信协议（5G/6G 接口）极其复杂且严谨。操作系统夹在中间，若无法动态适配这种“快慢齿轮”的差异，将直接卡死业务落地。</p><p>运维智能化的缺失：面对大规模异构集群，传统 OS 缺乏全链路感知能力，难以发现 GPU 算力瓶颈或网络微拥塞，导致“有故障难定界”。</p><h4>系统级解法：构建“确定性”与“弹性”兼备的底座</h4><p>面对上述挑战，中兴新支点并没有选择“头痛医头”，而是联合龙蜥社区，从内核与架构层面进行了深度的重构与优化。</p><p>智能算力调度（解决利用率低）：中兴作为发起人之一，加入龙蜥智算基础设施联盟，推进算力调度标准的制定。在最新的 V7 版本中，新支点操作系统内置了智能资源调度优化算法。这一算法能够像交通指挥官一样，动态感知 AI 训推任务的负载波动，弹性匹配最优算力组合。结合龙蜥社区的 AI 增强组件，成功打破了“以 CPU 为中心”的调度桎梏，显著提升了异构算力的利用效率。</p><p>电信级低时延优化（解决抖动）：发挥中兴在通信领域的“看家本领”，新支点操作系统对高性能网络协议栈和内存访问效率进行了深度改造。通过内核级的优化，支持控制面与用户面分离、实时任务优先调度等关键特性，为 AI 通信场景构建了一个“高稳定、低抖动”的操作系统底座，确保关键指令“即发即达”。</p><p>通过这一系列技术攻坚，中兴新支点证明了：在智算时代，操作系统不再是底层的“透明管家”，而是决定算力能否转化为生产力的“核心调度引擎”。</p><h3>从“独行”到“受众”，中兴引领制定行业标准</h3><p>技术的领先往往始于单点的突破，但生态的繁荣必须依赖标准的统一。</p><p>在展望未来时，王健雄传递出一个明确的信号：中兴新支点正在试图将其在电信、金融、电力等关键行业积累的“专属经验”，转化为龙蜥社区的“公共财富”。</p><p>2025 年，中兴新支点操作系统的迁移方案凭借在电信 NFV（网络功能虚拟化）场景下的优异表现，成功入选“鼎新杯”典型案例。但这在中兴看来，仅仅是一个开始。</p><p>王健雄表示，中兴计划利用其作为龙蜥社区副理事长单位的影响力，将这些经过大规模商用验证的高可用、低时延、实时虚拟化性能优化经验，沉淀为“社区共性技术需求”。</p><p>更具战略意义的是，中兴计划联合产业链伙伴，共同制定“电信级操作系统适配标准”。这一标准将对操作系统的时延抖动、系统可靠性、安全性与资源调度能力提出量化要求。</p><p>这意味着，未来龙蜥社区的操作系统在进入关键基础设施领域时，将拥有一套清晰的“准入体检单”，这将极大地推动国产操作系统从“能用”向“好用、耐用”跨越。</p><p>如果把目光投向更远的未来，中兴新支点也已经开始 6G 场景的前瞻性布局。</p><p>不同于 5G，未来的 6G 网络将呈现“连接+计算+智能”的高度融合特征。王健雄透露，新支点操作系统将重点强化分布式组网、数据平面与计算平面的协同、以及服务化架构的能力。其目标是构建一个能够支撑 6G 网络“可编程”与“原生 AI”特性的底座，同时依然保持工业级的高安全与高实时性。</p><p>此外，在 RISC-V 全场景支持与智算操作系统方向，中兴也将持续投入。通过支持异构算力的统一纳管，为边缘计算、物联网等泛在智能场景提供坚实基础。</p><p>从深耕 RISC-V 架构到开源 TFCE 行业数据，从解决异构算力调度难题到制定电信级适配标准，中兴新支点在 2025 年的进化路径，清晰地折射出国产操作系统行业的发展缩影：</p><p>它正在走出单纯的“国产替代”逻辑，迈向以 AI 为核心、以多架构为两翼、以行业标准为护城河的“技术深水区”。在龙蜥社区这片开源沃土上，中兴新支点正在努力证明，一个懂通信、懂 AI 的操作系统，将如何成为支撑数字经济未来的坚实脊梁。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[前端性能革命：200 行 JavaScript 代码实现 Streaming JSON 冴羽 ]]></title>    <link>https://segmentfault.com/a/1190000047524856</link>    <guid>https://segmentfault.com/a/1190000047524856</guid>    <pubDate>2026-01-06 18:08:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 前言</h2><p>5 月的时候，React 的核心开发者 Dan 发表了一篇名为<a href="https://link.segmentfault.com/?enc=6%2By9R1cb4IXTtPS1F6BJ5A%3D%3D.iG%2BidHMQAd1SVrpZrVvadpFowx3fKWPvBA7435xziX6ZxLq6rNrphjHF5VFT%2FKQb" rel="nofollow" target="_blank">《Progressive JSON》</a> 的文章，介绍了一种将 JSON 数据从服务器流式传输到客户端的技术，允许客户端在接收到全部数据之前就开始渲染部分数据。</p><p><strong>这可以显著提升用户体验，尤其是处理大型数据集时。</strong></p><p>让我们以“获取用户文章”这个场景为例。</p><p>这是一个完整的数据结构：</p><pre><code class="json">{
  "user": {
    "id": 1,
    "name": "John Doe",
    "posts": [
      { "id": 101, "title": "First Post", "content": "..." },
      { "id": 102, "title": "Second Post", "content": "..." }
    ]
  }
}</code></pre><p>假设我们能够很快获取用户信息，但文章数据还需要一段时间从数据库获取。</p><p>与其等待数据完全加载完毕，不如先发送一个占位符表示文章字段：</p><pre><code class="json">{
  "user": {
    "id": 1,
    "name": "John Doe",
    "posts": "_$1"
  }
}</code></pre><p>客户端收到数据后，先将用户信息渲染出来。</p><p>然后，当文章数据准备完毕后，我们将文章数据作为一个单独的 chunk 发送：</p><pre><code class="json">{
  "_$1": [
    { "id": 101, "title": "First Post", "content": "..." },
    { "id": 102, "title": "Second Post", "content": "..." }
  ]
}</code></pre><p>客户端收到数据后，最后将文章数据渲染出来。</p><p>要实现这样一个功能，客户端需要具备处理这些占位符的能力，并在最终数据到达时替换为实际数据。</p><p>如果要实现这样一个单独的功能需要多少代码呢？</p><p><strong>200 行就可以！</strong></p><p>本篇文章和大家介绍下实现思路，供大家学习和思考使用。</p><h2>2. 服务端实现</h2><p>让我们来看下服务器端实现。</p><p>首先是服务端函数。</p><pre><code class="javascript">function serve(res, data) {
  res.setHeader("Content-Type", "application/x-ndjson; charset=utf-8");
  res.setHeader("Transfer-Encoding", "chunked");

  // 向客户端发送 chunks
  res.write(JSON.stringify(...) + "\n");
  res.write(JSON.stringify(...) + "\n");

  // 当完成的时候
  res.end();
}</code></pre><p>这里有 2 点值得注意：</p><ol><li><strong>我们使用了 <code>application/x-ndjson</code>内容类型。</strong></li></ol><p>NDJSON，全拼 Newline Delimited JSON，其实就是一种换行符分割的 JSON，其中每一行都是一个有效的 JSON 对象。这允许我们在单个响应中发送多个 JSON 对象，并以换行符分隔。</p><ol start="2"><li><strong>我们使用了 <code>Transfer-Encoding: chunked</code>响应头。</strong></li></ol><p>使用该响应头，可以通知客户端，响应将分块发送。在调用 <code>res.end()</code>之前，请保持连接活跃状态。</p><p>其次，我们需要对数据进行分块。</p><p>实现方式也很简单，遍历数据对象，并用占位符替代那些暂时没有准备好的部分。</p><p>当遇到需要稍后发送的部分（一个 Promise）时，我们将其存储到队列中，并在准备就绪后，将其作为单独的数据块发送。</p><p>函数如下：</p><pre><code class="javascript">function normalize(value) {
  function walk(node) {
    if (isPromise(node)) {
      const id = getId();
      registerPromise(node, id);
      return id;
    }
    if (Array.isArray(node)) {
      return node.map((item) =&gt; walk(item));
    }
    if (node &amp;&amp; typeof node === "object") {
      const out = {};
      for (const [key, val] of Object.entries(node)) {
        out[key] = walk(val);
      }
      return out;
    }
    return node;
  }
  return walk(value);
}</code></pre><p>函数递归遍历数据对象。</p><p>当遇到 Promise 时，它会生成一个唯一的占位符 ID，注册该 Promise 以便稍后解析，并返回该占位符。</p><p>对于数组和对象，它会递归处理它们的元素或属性。原始值将按原样返回。</p><p>这是注册 Promise 的代码：</p><pre><code class="javascript">let promises = [];

function registerPromise(promise, id) {
  promises.push({ promise, id });
  promise.then((value) =&gt; {
    send(id, value);
  }).catch((err) =&gt; {
    console.error("Error resolving promise for path", err);
    send(id, { error: "promise error", timeoutMs: TIMEOUT });
  });</code></pre><p>这是 <code>send</code> 的代码，<code>send</code>函数负责将解析后的数据发送给客户端：</p><pre><code class="javascript">function send(id, value) {
  res.write(JSON.stringify({ i: id, c: normalize(value) }) + "\n");
  promises = promises.filter((p) =&gt; p.id !== id);
  if (promises.length === 0) res.end();
}</code></pre><p>该 <code>send</code> 函数会向响应中写入一个新的数据块，其中包括占位符 ID 和 normalize 后的值。然后它会从队列中移除已经 resolve 的 Promise。如果没有其他要处理的 Promise，它就会结束响应，从而关闭与客户端的连接。</p><p>完整的实现代码<a href="https://link.segmentfault.com/?enc=lx8v9k5RX8%2F4x5MN1UxhWA%3D%3D.s6XmYz0Pk2mMxHG6N6KhfFtm9Ll7Vo8rHPxdMx%2B8Y5JB89lVXiUJlRLzvoL0lh8wGdzMXdsJyPpcysczWSqZe41aMtvgtVWgqT6tf6sP5H5EbmSOguYSMSnuz5K2WVFy" rel="nofollow" target="_blank">点击这里</a>。</p><p>最后，我们举一个从服务端发送的对象示例：</p><pre><code class="javascript">const data = {
  user: {
    id: 1,
    name: "John Doe",
    posts: fetchPostsFromDatabase(), // 返回一个 promise
  },
};

async function fetchPostsFromDatabase() {
  const posts = await database.query("SELECT * FROM posts WHERE userId = 1");
  return posts.map((post) =&gt; ({
    id: post.id,
    title: post.title,
    content: post.content,
    comments: fetchCommentsForPost(post.id), // 返回一个 promise
  }));
}</code></pre><p>每篇文章还有一个评论字段（comments），该字段是一个 Promise 对象。意味着评论数据将在文章数据发送后，作为单独的片段发送。</p><h2>3. 客户端实现</h2><p>那客户端该如何实现呢？</p><p>在客户端，我们处理传入的数据块，并将占位符替换为实际数据。</p><p>我们可以使用 Fetch API 向服务器发送请求，并将响应读取为流。每当遇到占位符时，我们都会将其替换为一个 Promise，该 Promise 将在实际数据到达时解析。</p><p>核心逻辑如下：</p><pre><code class="javascript">try {
    const res = await fetch(endpoint);
    const reader = res.body.getReader();
    const decoder = new TextDecoder();

    async function process() {
      let done = false;
      while (!done) {
        const { value, done: readerDone } = await reader.read();
        done = readerDone;
        if (value) {
          try {
            const chunk = JSON.parse(decoder.decode(value, { stream: true }));
            chunk.c = walk(chunk.c);
            if (promises.has(chunk.i)) {
              promises.get(chunk.i)(chunk.c);
              promises.delete(chunk.i);
            }
          } catch (e) {
            console.error(`Error parsing chunk.`, e);
          }
        }
      }
    }
    process();
  } catch (e) {
    console.error(e);
    throw new Error(`Failed to fetch data from Streamson endpoint ${endpoint}`);
  }
}</code></pre><p>对流的处理，你可能感到陌生，可以拓展阅读我的这篇文章：<a href="https://link.segmentfault.com/?enc=DjtitRg6QB7Nk%2BT3bOQLrg%3D%3D.KDDZFgP2wnvEeradAokQz7iit9hXJJiq%2BiUem48q4rN2KVMrTBNQxYdnv082qQg1" rel="nofollow" target="_blank">《如何用 Next.js v14 实现一个 Streaming 接口？》</a></p><p><code>process</code> 函数逐块读取响应流。每个数据块都被解析为 JSON，并调用 <code>walk</code> 函数将占位符替换为 Promise。</p><p>如果数据块包含先前注册的占位符 ID ，则相应的 Promise 会被解析为接收到的数据。关键在于 <code>await reader.read()</code>，它允许我们等待新数据到来。</p><p><code>walk</code>函数用于将占位符替换为 Promise：</p><pre><code class="javascript">function walk(node) {
  if (isPromisePlaceholder(node)) {
    return new Promise((done) =&gt; {
      promises.set(node, done);
    });
  }
  if (Array.isArray(node)) {
    return node.map((item) =&gt; walk(item));
  }
  if (node &amp;&amp; typeof node === "object") {
    const out = {};
    for (const [key, val] of Object.entries(node)) {
      out[key] = walk(val);
    }
    return out;
  }
  return node;
}
function isPromisePlaceholder(val) {
  return typeof val === "string" &amp;&amp; val.match(/^_\$(\d)/);
}</code></pre><p>类似于服务端的 <code>normalize</code> 函数。当遇到占位符的时候，它会返回一个新的 Promise，该 Promise 将在实际数据到达时解析。对于数组和对象，它会递归处理它们的元素或属性。原始值则直接返回。当然，ID 必须与服务器端生成的 ID 匹配。</p><p>完整的实现代码<a href="https://link.segmentfault.com/?enc=RLEZ%2FwEjDelIoAsCVgJ1oQ%3D%3D.B6PwLLcs1ud2JL3kGcW05RFVRIMkBOnRZKQYp%2BYqT%2FeAV2YcAyhjm9QNg6N0po4hlwOioXHfu%2B0%2BmNkIJdTaD4sR3NngQ7M4ifUKAT4YatBNBbCyYZowmalJbSzp0DOs" rel="nofollow" target="_blank">点击这里</a>。两个文件加起来一共 155 行代码。</p><h2>4. NPM 包</h2><p>本篇文章整理翻译自 <a href="https://link.segmentfault.com/?enc=tlrSktL70aUsFByoUB5vyQ%3D%3D.wEhaubUq%2Fk6fbYhO5zPdylfPUJg9fVuoAyiwXkauB4QQ5kCp6D4uc4EQr1wc%2BkLpkzubTfoR%2FX03DZY%2FkMAD%2FXG3%2FfsrGpFjlov%2BSa%2BBLvFdlgYQPgbayKxC1RICBujL" rel="nofollow" target="_blank">Streaming JSON in just 200 lines of JavaScript</a>。</p><p>作者还将代码整理成了一个 NPM 包：<a href="https://link.segmentfault.com/?enc=07yxWKdv7CyN01nfQb5Oug%3D%3D.%2BT5VNTqFJZgn9MBWcnxa4JGebICeEmN8wvgg6pBqioFIQzMSWeKdkx4ZrpVUs3K6" rel="nofollow" target="_blank">Streamson</a>。</p><p>通过 npm 安装：<code>npm intall streamson</code></p><p>服务端上使用：</p><pre><code class="javascript">import { serve } from "streamson";
import express from "express";

const app = express();
const port = 5009;

app.get("/data", async (req, res) =&gt; {
  const myData = {
    title: "My Blog",
    description: "A simple blog example using Streamson",
    posts: getBlogPosts(), // this returns a Promise
  };
  serve(res, myData);
});

app.listen(port, () =&gt; {
  console.log(`Example app listening on port ${port}`);
});</code></pre><p>客户端是一个 1KB 的 JavaScript 文件，地址：<a href="https://link.segmentfault.com/?enc=9IzGiUoGgY9EN8tavz2JzQ%3D%3D.rUGgm8DVmH1Hd1ze2ZDYRBpmRrLCEO%2BtOtYuQO0jRnMEJomK%2BgLISdnaacuUuzc4M7VSJJr27l4DP%2FyeabvyJw%3D%3D" rel="nofollow" target="_blank">https://unpkg.com/streamson@latest/dist/streamson.min.js</a></p><p>客户端使用如下：</p><pre><code class="javascript">const request = Streamson("/data");

const data = await request.get();
console.log(data.title); // "My Blog"

const posts = await request.get("posts");
console.log(posts); // Array of blog posts</code></pre><h2>5. 最后</h2><p>作为准前端开发专家的你，第一时间获取前端资讯、技术干货、AI 课程，那不得关注下我的公众号「冴羽」。</p><p>流式传输 JSON 数据是一种提升 Web 应用感知性能的有效方法，尤其适用于处理大型数据集或动态生成数据。</p><p>通过在数据可用时立即发送部分数据，我们可以让客户端更早地开始渲染内容，从而带来更佳的用户体验。</p>]]></description></item><item>    <title><![CDATA[基于 YOLOv8 的学生课堂行为检测(举手、看书、写作业、玩手机)-完整项目源码 南瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047524870</link>    <guid>https://segmentfault.com/a/1190000047524870</guid>    <pubDate>2026-01-06 18:07:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>基于 YOLOv8 的学生课堂行为检测-完整项目源码</h2><h3>一、问题背景：为什么要做“课堂行为识别”</h3><p>在智慧校园和数字化教学逐步落地的过程中，<strong>课堂行为数据</strong>正在从“不可量化”走向“可分析、可追溯、可评估”。</p><p>在真实教学场景中，教师和管理者往往关注以下问题：</p><ul><li>学生是否专注听讲？</li><li>是否存在频繁低头、趴桌、玩手机等行为？</li><li>课堂互动（举手、回答问题）是否足够积极？</li><li>不同时间段、不同课程的学习状态差异如何？</li></ul><p>传统方式主要依赖<strong>人工巡视或事后主观评价</strong>，存在明显局限：</p><table><thead><tr><th>方式</th><th>问题</th></tr></thead><tbody><tr><td>人工观察</td><td>成本高、主观性强、难以量化</td></tr><tr><td>问卷反馈</td><td>滞后、失真、难以持续</td></tr><tr><td>简单视频回放</td><td>无结构化信息、分析效率低</td></tr></tbody></table><p>因此，<strong>基于计算机视觉的课堂行为识别系统</strong>成为一个极具实际价值的工程方向。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524872" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>源码下载与效果演示</h3><p>哔哩哔哩视频下方观看：<br/><a href="https://www.bilibili.com/video/BV1m7KJzNEQ2/" target="_blank">https://www.bilibili.com/video/BV1m7KJzNEQ2/</a></p><p>包含：</p><p>📦完整项目源码</p><p>📦 预训练模型权重</p><p>🗂️ 数据集地址（含标注脚本<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524873" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>二、整体技术路线设计</h3><p>本项目的目标不是“只跑一个模型 Demo”，而是构建一个<strong>可直接使用的完整系统</strong>。因此在设计之初，整体架构就围绕以下三点展开：</p><ol><li><strong>模型必须实时可用</strong></li><li><strong>系统必须非算法人员也能操作</strong></li><li><strong>工程结构支持后续扩展</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524874" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ol><h4>2.1 系统总体架构</h4><p>整体采用典型的三层结构：</p><pre><code>数据层 → 模型层 → 应用层</code></pre><ul><li><strong>数据层</strong>：YOLO 格式行为数据集（图片 + 标签）</li><li><strong>模型层</strong>：YOLOv8 Detection 模型（PyTorch）</li><li><strong>应用层</strong>：PyQt5 桌面 GUI + 多输入推理模块</li></ul><pre><code>摄像头 / 视频 / 图片
        ↓
   YOLOv8 行为检测
        ↓
   行为类别 + 置信度
        ↓
   GUI 实时展示 / 保存结果</code></pre><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524875" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524876" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>三、为什么选择 YOLOv8 做课堂行为识别</h3><p>在行为识别领域，常见技术路线包括：</p><ul><li><strong>CNN + 分类</strong>（仅判断整张图）</li><li><strong>CNN + 时序模型（LSTM / Transformer）</strong></li><li><strong>目标检测 + 行为标签</strong></li></ul><p>在课堂场景中，我们更关心的是：</p><blockquote><strong>“谁”在“做什么行为”</strong></blockquote><p>因此，<strong>目标检测模型</strong>比单纯分类模型更合适。</p><h4>3.1 YOLOv8 的工程优势</h4><p>YOLOv8 相比早期 YOLO 版本，具备明显工程优势：</p><ul><li><strong>Anchor-Free 设计</strong><br/>不再依赖复杂 Anchor 调参，对新场景友好</li><li><strong>端到端训练流程简化</strong></li><li><strong>推理速度快，适合实时摄像头</strong></li><li><strong>Ultralytics 官方生态成熟</strong></li><li><strong>原生支持 ONNX / TensorRT 导出</strong></li></ul><p>对于“课堂实时监测”这种 <strong>FPS 和稳定性同等重要</strong> 的任务，YOLOv8 是非常理性的选择。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524877" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>四、行为数据集构建：比模型更重要的一步</h3><p>在实际项目中，<strong>数据质量往往决定上限</strong>。</p><h4>4.1 行为类别设计原则</h4><p>本项目中的课堂行为类别遵循三个原则：</p><ol><li><strong>视觉上可区分</strong></li><li><strong>教学场景有明确意义</strong></li><li><strong>避免过细导致标注困难</strong></li></ol><p>示例类别包括：</p><ul><li>举手</li><li>看书</li><li>写作业</li><li>听讲</li><li>趴桌</li><li>玩手机</li></ul><p>这些行为都可以通过<strong>单帧图像 + 空间特征</strong>进行判别，而无需复杂时序建模。</p><hr/><h4>4.2 数据集结构（YOLO 标准）</h4><pre><code class="text">dataset/
├── images/
│   ├── train/
│   └── val/
├── labels/
│   ├── train/
│   └── val/</code></pre><p>标签采用 YOLO 标准格式：</p><pre><code>class_id x_center y_center width height</code></pre><p>例如：</p><pre><code class="txt">3 0.51 0.36 0.39 0.32</code></pre><blockquote>所有坐标均为 <strong>相对比例</strong>，方便多分辨率训练。</blockquote><hr/><h3>五、模型训练与参数配置经验</h3><h4>5.1 训练策略</h4><p>在课堂场景中，模型训练重点并不是追求极限精度，而是：</p><ul><li><strong>稳定收敛</strong></li><li><strong>类别区分度清晰</strong></li><li><strong>推理速度可控</strong></li></ul><p>示例训练命令：</p><pre><code class="bash">yolo detect train \
  data=dataset/classroom.yaml \
  model=yolov8n.pt \
  epochs=100 \
  batch=16 \
  imgsz=640</code></pre><h4>5.2 关键指标解读</h4><p>训练完成后，重点关注：</p><ul><li><strong>mAP@0.5</strong>：是否稳定在 90% 左右</li><li><strong>混淆矩阵</strong>：是否存在行为间严重混淆</li><li><strong>box_loss / cls_loss 收敛情况</strong></li></ul><p>课堂行为中，“看书 / 写作业”是最容易混淆的类别，通常需要通过 <strong>数据平衡和样本增强</strong> 来改善。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524878" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524879" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>六、推理系统设计：从模型到可用软件</h3><p>如果说模型是“发动机”，那么 <strong>GUI 系统</strong>就是“驾驶舱”。</p><h4>6.1 多输入推理设计</h4><p>系统支持以下输入形式：</p><ul><li>单张图片检测</li><li>文件夹批量检测</li><li>视频文件检测</li><li>摄像头实时检测</li></ul><p>其核心思想是：</p><blockquote><strong>统一推理接口，不同输入仅影响数据读取方式</strong></blockquote><pre><code class="python">results = model(frame, conf=0.25)</code></pre><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524880" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>6.2 PyQt5 界面设计要点</h4><p>GUI 设计遵循三个工程原则：</p><ol><li><strong>功能按钮逻辑清晰</strong></li><li><strong>推理与界面解耦</strong></li><li><strong>避免阻塞主线程</strong></li></ol><p>常见功能包括：</p><ul><li>模型加载</li><li>输入源选择</li><li>实时结果显示</li><li>结果保存开关</li></ul><p>这种设计使得<strong>非算法人员也能直接运行系统</strong>。</p><hr/><h3>七、实际应用价值分析</h3><p>在真实教学场景中，该系统可用于：</p><ul><li><strong>课堂状态统计分析</strong></li><li><strong>教学质量评估辅助</strong></li><li><strong>学生行为数据可视化</strong></li><li><strong>智慧教室系统模块化集成</strong></li></ul><p>相比“单纯监控”，该系统更强调：</p><blockquote><strong>结构化行为数据的自动生成</strong></blockquote><hr/><h3>八、可扩展方向与进阶思路</h3><p>该项目并非终点，而是一个可持续扩展的工程起点。</p><p>可扩展方向包括：</p><ul><li><strong>引入姿态估计（Keypoints）</strong></li><li><strong>多摄像头联动分析</strong></li><li><strong>行为时间序列建模</strong></li><li><strong>行为频次 / 趋势统计</strong></li><li><strong>与教学管理系统对接</strong></li></ul><p>未来可从“检测行为”升级为：</p><blockquote><strong>理解课堂状态</strong></blockquote><hr/><h3>九、总结</h3><p>本文从工程视角出发，完整复盘了一个 <strong>基于 YOLOv8 的学生课堂行为识别系统</strong> 从需求分析、模型选择、数据构建、训练评估到 GUI 应用落地的全过程。</p><p>该项目的核心价值不在于“某一个模型指标”，而在于：</p><ul><li>模型可实时运行</li><li>系统可直接部署</li><li>工程结构可持续演进</li></ul><p>对于希望将 <strong>计算机视觉真正落地到教育场景</strong> 的开发者而言，这是一个非常具有实践意义的方向。</p><p>本文从工程实践角度系统性地梳理了一个基于 YOLOv8 的学生课堂行为识别系统的完整落地过程，涵盖需求背景、模型选型、数据集构建、训练评估以及 PyQt5 图形化应用封装等关键环节。实践表明，目标检测模型在课堂行为分析这一复杂场景中具备良好的实时性与可扩展性，能够有效将“不可量化的课堂状态”转化为结构化、可分析的数据资产。相比单一算法实验，本项目更强调模型与系统的协同设计，使 AI 能够真正服务于教学管理与教学分析。未来，随着姿态估计、多模态数据与行为统计分析的引入，该类系统有望从行为识别进一步升级为课堂状态理解与教学决策支持，为智慧校园建设提供更加可靠的技术基础。</p>]]></description></item><item>    <title><![CDATA[开发者如何集成IP查询功能？主流IP离线库全面解读与性能对比 科技块儿 ]]></title>    <link>https://segmentfault.com/a/1190000047524895</link>    <guid>https://segmentfault.com/a/1190000047524895</guid>    <pubDate>2026-01-06 18:06:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>IP查询是开发者在反欺诈、精准营销、网络防护等场景的核心需求，离线库因无网络依赖、响应更快，成为企业级应用的首选。本文筛选5款主流IP离线库（IP数据云、IPnews、IPinfo、IP2Location、DB-IP），从技术指标与实用场景切入测评，为开发者集成决策提供参考。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524897" alt="开发者如何集成IP查询功能主流IP离线库全面解读与性能对比.png" title="开发者如何集成IP查询功能主流IP离线库全面解读与性能对比.png"/></p><h2>一、主流IP离线库核心参数对比</h2><table><thead><tr><th>对比维度</th><th>IP数据云</th><th>IPnews</th><th>IPinfo</th><th>IP2Location</th><th>DB-IP</th></tr></thead><tbody><tr><td>全球IP覆盖</td><td>全球全量覆盖</td><td>全球覆盖</td><td>全球覆盖</td><td>全球覆盖</td><td>全球覆盖</td></tr><tr><td>定位精度</td><td>街道级</td><td>城市级</td><td>城市级</td><td>城市级</td><td>城市级</td></tr><tr><td>支持协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td></tr><tr><td>字段维度</td><td>20+（含风险画像、场景）</td><td>20+（含代理及风险检测）</td><td>20+（含ISP信息）</td><td>20+（含运营商信息）</td><td>20+（基础地理+ISP）</td></tr><tr><td>风险识别能力</td><td>支持（薅羊毛/垃圾注册等）</td><td>支持</td><td>支持风险标记</td><td>支持</td><td>支持</td></tr><tr><td>适用场景</td><td>金融反欺诈、政企安全</td><td>普通网站定位</td><td>中小企业营销</td><td>网络防护、欺诈防范和广告等</td><td>通用信息查询</td></tr></tbody></table><p>这五款IP离线库均实现全球IP覆盖与IPv4/IPv6双协议支持，字段维度均达20 +且具备风险识别相关能力；核心差异集中在定位精度与适用场景，其中IP数据云以街道级定位精度凸显优势，适配金融反欺诈、政企安全等高精度需求场景，其余四款均为城市级定位，分别对应普通网站定位、中小企业营销、网络防护及通用信息查询等不同业务场景。</p><h2>二、开发者集成关键步骤</h2><p>1.选型适配：根据业务场景选择库；<br/>2.离线部署：下载对应库的离线数据包，支持本地服务器部署，规避网络波动；<br/>3.接口调用：各服务商大多通过SDK或API接口接入，无需复杂开发（其中IP数据云提供Java/Python/Go等多语言SDK）；<br/>4.数据更新：定期同步官方离线包。</p><h2>三、总结</h2><p>开发者集成IP查询功能，需平衡精度、性能与功能适配性。IP数据云等IP离线库服务商在定位精度、响应速度、风险识别等核心维度表现突出，尤其适合金融、政企等对安全性和精度要求严苛的场景。选择离线库时，建议优先考量业务核心需求，结合表格对比结果选型，提升集成效率与应用效果。</p>]]></description></item><item>    <title><![CDATA[解锁 PDF 内容：如何用 Python 从 PDF 中快速提取文本 宇文成都 ]]></title>    <link>https://segmentfault.com/a/1190000047524909</link>    <guid>https://segmentfault.com/a/1190000047524909</guid>    <pubDate>2026-01-06 18:06:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代办公环境中，PDF 文件作为一种通用的文档格式被广泛使用。无论是合同、报告还是电子书，很多重要信息都储存于 PDF 文件中。因此，从 PDF 文件中提取文本数据的需求也逐渐增加。本文将为大家介绍如何使用 Spire.PDF for Python 来实现这一功能，具体包括从某一页和从指定区域提取文本。</p><h2>1. 环境准备</h2><p>首先，确保你已经安装了 Python 和 Spire.PDF 的相关库。你可以通过以下命令安装 Spire.PDF：</p><pre><code class="bash">pip install Spire.PDF</code></pre><h2>2. 从指定页面提取文本</h2><h3>2.1 代码示例</h3><p>以下代码展示了如何从 PDF 文档中的特定页（例如第2页）提取文本：</p><pre><code class="python">from spire.pdf.common import *
from spire.pdf import *

# 创建一个 PdfDocument 对象
doc = PdfDocument()

# 加载 PDF 文档
doc.LoadFromFile('C:/Users/Administrator/Desktop/Terms of service.pdf')

# 创建 PdfTextExtractOptions 对象并启用全文本提取
extractOptions = PdfTextExtractOptions()
# 提取所有文本，包括空格
extractOptions.IsExtractAllText = True

# 获取特定的页面（例如，第2页）
page = doc.Pages.get_Item(1)

# 创建 PdfTextExtractor 对象
textExtractor = PdfTextExtractor(page)

# 从页面中提取文本
text = textExtractor.ExtractText(extractOptions)

# 使用 UTF-8 编码将提取的文本写入文件
withopen('output/TextOfPage.txt', 'w', encoding='utf-8') as file:
    file.write(text)</code></pre><h3>2.2 代码解析</h3><ol><li><strong>创建 <code>PdfDocument</code> 对象</strong> ：这一步是加载 PDF 文件的第一步。</li><li><strong>加载 PDF 文档</strong> ：使用指定路径加载你要处理的 PDF 文件。</li><li><strong>配置提取选项</strong> ：通过设置 <code>IsExtractAllText</code> 为 True，确保提取所有文本，包括空格。</li><li><strong>获取特定页面</strong> ：<code>doc.Pages.get_Item(1)</code> 获取的是 PDF 的第二页（索引从0开始）。</li><li><strong>创建文本提取器并提取文本</strong> ：使用 <code>PdfTextExtractor</code> 对象来提取文本。</li><li><strong>将提取的文本保存为文件</strong> ：最终将文本内容保存到指定路径的文件中。</li></ol><h2>3. 从指定区域提取文本</h2><p>有时候，仅提取 PDF 中的某一特定区域的文本更加有效。这可以通过定义一个矩形区域来实现。</p><h3>3.1 代码示例</h3><p>以下代码将展示如何从 PDF 的指定区域提取文本：</p><pre><code class="python">from spire.pdf.common import *
from spire.pdf import *

# 创建一个 PdfDocument 对象
doc = PdfDocument()

# 加载 PDF 文档
doc.LoadFromFile('C:/Users/Administrator/Desktop/Terms of service.pdf')

# 获取特定的页面（例如，第2页）
page = doc.Pages.get_Item(1)

# 创建 PdfTextExtractor 对象
textExtractor = PdfTextExtractor(page)

# 创建 PdfTextExtractOptions 对象
extractOptions = PdfTextExtractOptions()

# 定义提取的矩形区域
# RectangleF(left, top, width, height)
extractOptions.ExtractArea = RectangleF(0.0, 100.0, 890.0, 80.0)

# 从指定区域提取文本，保留空格
text = textExtractor.ExtractText(extractOptions)

# 使用 UTF-8 编码将提取的文本写入文件
withopen('output/TextOfRectangle.txt', 'w', encoding='utf-8') as file:
    file.write(text)</code></pre><h3>3.2 代码解析</h3><ol><li><strong>加载 PDF 文件</strong> ：与之前相同，首先加载 PDF 文档。</li><li><strong>获取特定页面</strong> ：依然使用 <code>doc.Pages.get_Item(1)</code> 来获取第2页。</li><li><strong>定义提取区域</strong> ：通过 <code>RectangleF</code> 类来定义一个矩形区域，该区域的左上角坐标为 <code>(0, 100)</code>，宽度为 <code>890</code>，高度为 <code>80</code>。</li><li><strong>执行文本提取</strong> ：然后使用 <code>ExtractText</code> 方法从指定区域提取文本。</li><li><strong>保存文本</strong> ：最后，提取的文本同样保存为 UTF-8 编码的文件。</li></ol><h2>结论</h2><p>通过以上方法，我们可以方便地从 PDF 文档中提取所需的文本信息。Spire.PDF for Python 提供的 API 简洁高效，能够满足多种文本提取需求。不论是从全页提取还是从特定区域提取，在实际工作中都能显著提高效率，尤其对于需要处理大量 PDF 文件的场合，使用此工具将使你事半功倍。</p><p>希望这篇博客能够帮助你更好地理解如何使用 Python 提取 PDF 文本，让你的工作更轻松高效！</p>]]></description></item><item>    <title><![CDATA[上传图片时交互来回闪现的情况优化 freeman_Tian ]]></title>    <link>https://segmentfault.com/a/1190000047524907</link>    <guid>https://segmentfault.com/a/1190000047524907</guid>    <pubDate>2026-01-06 18:05:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <pre><code>&lt;template&gt;
  &lt;div class="car-upload"&gt;
    &lt;el-upload
      ref="upload"
      action="#"
      :file-list="internalFileList"
      :before-upload="beforeUpload"
      :http-request="handleUpload"
      :on-preview="handlePreview"
      :multiple="false"
      list-type="picture-card"
      :on-remove="handleRemove"
      :class="{ 'hide-upload': hideUpload }"
      :disabled="uploading"
    &gt;
      &lt;template v-if="uploading"&gt;
        &lt;div class="uploading-mask"&gt;
          &lt;div class="custom-loading-icon" /&gt;
          &lt;span class="upload-text"&gt;{{ $t('common.uploading') }}&lt;/span&gt;
        &lt;/div&gt;
      &lt;/template&gt;
      &lt;template v-else&gt;
        &lt;i class="el-icon-plus" /&gt;
      &lt;/template&gt;

      &lt;div slot="tip" class="tip"&gt;
        {{ $t('externalModel.uploadimgTip') }}
      &lt;/div&gt;
    &lt;/el-upload&gt;

    &lt;PreviewModal :visible.sync="previewVisible" :image-url="previewImageUrl" /&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
import { getToken } from '@/utils/auth'
import PreviewModal from './PreviewModal.vue'
import { imagesUpload } from '@/api/type/external'

export default {
  name: 'CarImageUpload',
  components: {
    PreviewModal
  },
  props: {
    value: {
      type: [String, Array],
      default: () =&gt; []
    },
    maxSize: { type: Number, default: 2 },
    accept: {
      type: Array,
      default: () =&gt; ['image/bmp', 'image/png', 'image/jpeg', 'image/gif']
    },
    uploadUrl: { type: String, required: false, default: '' },
    fixedSize: { type: Object, required: false, default: null },
    limit: {
      type: Number,
      default: 1
    }
  },
  data() {
    return {
      internalFileList: [],
      previewUrl: '',
      headers: { Authorization: 'Bearer ' + getToken() },
      previewVisible: false,
      previewImageUrl: '',
      uploading: false,
      currentFile: null,
      // 添加缓存用于存储原始文件的UID
      fileUidMap: new Map()
    }
  },
  computed: {
    hideUpload() {
      return this.internalFileList.length &gt;= this.limit || this.uploading
    }
  },
  watch: {
    value: {
      immediate: true,
      handler(newVal) {
        // 优化：避免不必要的重渲染，只有在真正变化时更新
        const currentUrls = this.internalFileList.map(item =&gt; item.url).filter(Boolean)
        const newUrls = Array.isArray(newVal) ? newVal : (newVal ? [newVal] : [])

        if (JSON.stringify(currentUrls) !== JSON.stringify(newUrls)) {
          this.syncFileList(newVal)
        }
      }
    }
  },
  methods: {
    // 优化文件列表同步方法
    syncFileList(newVal) {
      if (Array.isArray(newVal)) {
        this.internalFileList = newVal.map((url, index) =&gt; ({
          url,
          status: 'success',
          // 保持UID一致性，避免重新生成
          uid: this.fileUidMap.get(url) || this.generateUid(url, index)
        }))
      } else if (newVal) {
        this.internalFileList = [{
          url: newVal,
          status: 'success',
          uid: this.fileUidMap.get(newVal) || this.generateUid(newVal, 0)
        }]
      } else {
        this.internalFileList = []
      }
    },

    // 生成基于URL的稳定UID
    generateUid(url, index) {
      // 基于URL生成稳定UID，避免每次重新生成不同的UID
      const uid = `file_${btoa(url).substr(0, 10)}_${index}`
      this.fileUidMap.set(url, uid)
      return uid
    },

    async beforeUpload(file) {
      if (this.uploading) {
        this.$message.warning('正在上传中，请稍候...')
        return false
      }

      const isImage = this.accept.includes(file.type)
      const isLt10M = file.size / 1024 / 1024 &lt; 10

      if (!isImage) {
        this.$message.error(this.$t('externalModel.uploadimgTip'))
        return false
      }
      if (!isLt10M) {
        this.$message.error(this.$t('externalModel.uploadimgTip'))
        return false
      }

      if (this.fixedSize || this.aspectRatio) {
        const isDimensionValid = await this.validateImageSize(file)
        if (!isDimensionValid) {
          const msg = this.fixedSize
            ? `图片尺寸必须为 ${this.fixedSize.width}×${this.fixedSize.height}`
            : `宽高比需为 ${this.aspectRatio}:1`
          this.$message.error(msg)
          return false
        }
      }

      this.currentFile = file
      return true
    },

    // 优化上传方法，避免闪动[1,2](@ref)
    async handleUpload({ file }) {
      this.uploading = true

      // 保存原始文件的UID[2](@ref)
      const originalUid = file.uid

      try {
        const formData = new FormData()
        formData.append('file', this.currentFile || file)

        const res = await imagesUpload(formData)

        if (res.code === '000000') {
          const url = res.body.url

          // 优化：直接更新现有文件对象，而不是创建新对象[5](@ref)
          const existingFileIndex = this.internalFileList.findIndex(f =&gt; f.uid === originalUid)
          if (existingFileIndex &gt; -1) {
            // 保持UID不变，只更新URL[2](@ref)
            this.internalFileList[existingFileIndex].url = url
            this.internalFileList[existingFileIndex].status = 'success'
          } else {
            // 如果找不到现有文件，添加新文件但保持UID
            this.internalFileList.push({
              url,
              status: 'success',
              uid: originalUid
            })
          }

          this.updateModelValue(url)
          this.$message.success('上传成功')
        } else {
          throw new Error(res.message || '上传失败')
        }
      } catch (error) {
        console.error('上传失败:', error)
        this.$message.error('上传失败，请重试')

        // 上传失败时更新状态而不是移除文件[1](@ref)
        const failedFileIndex = this.internalFileList.findIndex(f =&gt; f.uid === originalUid)
        if (failedFileIndex &gt; -1) {
          this.internalFileList[failedFileIndex].status = 'failed'
        }
      } finally {
        this.uploading = false
        this.currentFile = null
      }
    },

    validateImageSize(file) {
      return new Promise((resolve) =&gt; {
        const img = new Image()
        img.src = URL.createObjectURL(file)
        img.onload = () =&gt; {
          let isValid = true
          if (this.fixedSize) {
            isValid = img.width === this.fixedSize.width &amp;&amp; img.height === this.fixedSize.height
          } else if (this.aspectRatio) {
            const ratio = (img.width / img.height).toFixed(2)
            isValid = ratio === this.aspectRatio.toFixed(2)
          }
          URL.revokeObjectURL(img.src)
          resolve(isValid)
        }
        img.onerror = () =&gt; resolve(false)
      })
    },

    // 优化模型值更新[5](@ref)
    updateModelValue(newUrl) {
      // 避免直接赋值导致的重新渲染
      this.$nextTick(() =&gt; {
        const currentValue = this.value
        let newValue

        if (this.limit &gt; 1) {
          const currentUrls = Array.isArray(currentValue) ? currentValue : []
          // 去重并过滤空值
          newValue = [...new Set([...currentUrls, newUrl].filter(url =&gt; url))]
        } else {
          newValue = newUrl
        }

        // 只有值真正改变时才触发更新
        if (JSON.stringify(currentValue) !== JSON.stringify(newValue)) {
          this.$emit('input', newValue)
        }
      })
    },

    handleRemove(file) {
      // 从UID映射中移除
      this.fileUidMap.delete(file.url)

      const newList = this.internalFileList.filter(f =&gt; f.uid !== file.uid)
      const newValue = this.limit &gt; 1 ? newList.map(f =&gt; f.url) : ''
      this.$emit('input', newValue)
    },

    handlePreview(file) {
      // 添加时间戳防止缓存问题[4](@ref)
      const timestamp = new Date().getTime()
      this.previewImageUrl = file.url + (file.url.includes('?') ? '&amp;' : '?') + `t=${timestamp}`
      this.previewVisible = true
    }
  }
}
&lt;/script&gt;

&lt;style lang="scss" scoped&gt;
.car-upload {
  height: 200px;
  position: relative;
}

.hide-upload {
  ::v-deep .el-upload--picture-card {
    display: none;
    /* 添加过渡效果减少视觉突兀感 */
    transition: opacity 0.3s ease;
  }
}

.tip {
  font-size: 12px;
  color: #909399;
  padding-top: 8px;
}

.uploading-mask {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  color: #409EFF;
}

.upload-text {
  margin-top: 8px;
  font-size: 12px;
}

.custom-loading-icon {
  width: 20px;
  height: 20px;
  border: 2px solid #f3f3f3;
  border-top: 2px solid #409EFF;
  border-radius: 50%;
  animation: rotating 2s linear infinite;
}

@keyframes rotating {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/* 添加文件列表项过渡效果 */
::v-deep .el-upload-list__item {
  transition: all 0.3s ease;
}

/* 确保图片加载平滑 */
::v-deep .el-upload-list__item-thumbnail {
  object-fit: contain;
  transition: opacity 0.3s ease;
}
&lt;/style&gt;
</code></pre><p>上传组件的闪动问题是由于双阶段渲染导致的，选择文件后先用本地Blob URL预览，上传完成后再用服务器URL替换，这个替换过程会导致重新加载和闪动。文中建议保持使用本地预览不替换URL，或者优化上传流程。都指出Element UI中el-upload组件的闪动问题与uid变化有关，上传成功后如果file-list被重新赋值且uid发生变化，就会导致组件重新渲染和图片闪烁。解决方案是在回调中保持uid不变。提到避免使用computed或watch监听fileList，这可能导致不必要的重新渲染。<br/>UID不一致​</p><p>文件上传后新对象的UID与原始文件不同，导致组件重新渲染</p><p>保持UID一致性，避免重新创建文件对象</p><p>URL替换闪动​</p><p>从本地Blob URL切换到服务器HTTP URL时浏览器重新加载</p><p>优化URL替换时机或使用统一URL格式</p><p>文件列表重建​</p><p>使用watch监听value导致整个internalFileList重建</p><p>优化数据更新策略，避免不必要的重渲染</p>]]></description></item><item>    <title><![CDATA[节点小宝4.0性能测试：远程文件管理体验全面升级 节点小宝 ]]></title>    <link>https://segmentfault.com/a/1190000047524911</link>    <guid>https://segmentfault.com/a/1190000047524911</guid>    <pubDate>2026-01-06 18:04:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在远程工具的使用体验中，性能表现一直是用户关注的重点。近期我们对节点小宝4.0版本进行了系统性的性能测试，重点评估了其在远程文件管理方面的表现。</p><h3>测试环境与方法</h3><p>测试采用统一的硬件配置：红米K60设备通过4G热点连接，访问群晖DS220+ NAS设备。通过对比3.0与4.0版本的核心指标，我们获得了以下数据：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524913" alt="图片" title="图片"/></p><p>测试结果显示，4.0版本在多个关键指标上均有显著提升：<br/>冷启动到显示NAS根目录的时间从11.7秒缩短至2.3秒<br/>大文件缩略图加载时间从6.4秒减少到0.9秒<br/>批量文件列表加载效率提升约5.8倍<br/>4G网络下文件传输速度提升4倍</p><h4>技术优化亮点</h4><p>预加载算法改进<br/>新版本采用智能预读技术，自动加载可视区域前后30%的数据内容。缩略图经过优化压缩，在保证观感的同时节省了70%的流量消耗。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524914" alt="图片" title="图片" loading="lazy"/></p><h4>连接稳定性提升</h4><p>P2P直连技术在复杂网络环境下的成功率提升至92%，当主连接失败时，系统能在0.8秒内切换到备用通道，确保服务连续性。</p><h4>资源占用优化</h4><p>即使在处理万级文件批量操作时，CPU占用率也能控制在30%以内，保证了系统的流畅运行。</p><h4>实际使用体验文件浏览流畅度</h4><p>测试中在NAS存入2万张图片，高速滑动浏览时缩略图加载延迟低于0.1秒，在高刷新率设备上也能保持流畅的视觉效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524915" alt="图片" title="图片" loading="lazy"/></p><h4>断点续传功能</h4><p>在模拟弱网环境下测试文件传输，网络中断后重新连接时，系统能在3秒内自动恢复传输，并能从断点处准确继续，避免了重复传输。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524916" alt="图片" title="图片" loading="lazy"/></p><h3>功能改进对比</h3><h4>操作路径简化</h4><p>相比3.0版本需要多步操作才能访问远程文件，4.0版本实现了"一键直达"的体验。用户只需在首页点击"远程文件"即可直接访问NAS内容。</p><h4>协议支持扩展</h4><p>新版本增强了对SMB协议的支持，使得大文件编辑操作更加流畅。测试中打开45MB的RAW格式图片文件，曝光调整的响应延迟仅为120ms。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524917" alt="图片" title="图片" loading="lazy"/></p><h4>附加功能特性</h4><p>4.0版本还包含了一些实用的附加功能：<br/>WebDAV挂载支持，允许用户在系统文件管理器中直接访问远程存储<br/>快捷传输功能，通过快捷方式快速访问常用设备<br/>多标签页管理，支持同时控制多个远程设备</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524918" alt="图片" title="图片" loading="lazy"/></p><p>节点小宝4.0版本在远程文件管理方面实现了显著的技术突破。通过优化算法、提升连接稳定性和简化操作流程，为用户提供了接近本地操作的使用体验。这些改进使得远程文件管理变得更加高效和便捷，为需要频繁访问远程存储的用户提供了实用的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524919" alt="图片" title="图片" loading="lazy"/></p><p>该版本现已正式发布，欢迎技术爱好者体验并提供反馈意见。对于远程访问技术有深入探讨需求的用户，可以在技术社区继续交流相关实现细节。</p>]]></description></item><item>    <title><![CDATA[企业落地 ChatBI，如何构建可信可靠的数据底座？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047524928</link>    <guid>https://segmentfault.com/a/1190000047524928</guid>    <pubDate>2026-01-06 18:04:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业 ChatBI 落地过程中，数据底座的技术路线选择直接决定了数据可信度、维护成本和业务响应速度。传统宽表架构在数据口径一致性、维护成本和灵活性上已难以支撑企业级 ChatBI 的规模化应用，而基于 NoETL 明细语义层的方案正成为新一代数据底座的主流选择。</p><h2>企业落地 ChatBI 痛点：为什么传统宽表越来越难用？</h2><p><strong>痛点一：数据口径碎片化，业务不敢信</strong><br/>● 不同宽表、不同报表对同一指标定义不一致：同一“销售额”指标在营销宽表、财务宽表中可能包含不同的业务口径（如是否含税、是否含退货），导致业务人员无法判断哪个数据可信。<br/>● 业务与 IT 对指标理解偏差，导致“问 A 得 B”：业务人员理解的“活跃用户”与宽表字段逻辑存在差异，取数结果与预期不符，反复沟通成本高。<br/>● 数据口径不透明，结果难解释，决策依赖“拍脑袋”：宽表背后复杂的 ETL 逻辑缺乏文档沉淀，业务人员无法追溯计算过程，只能凭经验决策，数据驱动决策沦为口号。</p><p><strong>痛点二：维护成本高，IT 排期长</strong><br/>● 宽表数量随业务需求线性增长，开发与运维成本失控：每新增一个分析维度或业务场景，就需要新建一张宽表，导致数仓中宽表数量激增，数据冗余严重，存储和计算成本持续攀升。<br/>● 业务需求变更需重建宽表，响应周期长：当业务口径调整（如“高净值客户”定义变化）时，需要重新设计宽表、开发 ETL 任务并重新上线，响应周期通常以周为单位。<br/>● 数据工程师疲于应付宽表开发，难以沉淀数据资产：工程师长期陷入“接需求—建宽表—改宽表”的循环，无法将精力投入到数据资产治理和业务价值挖掘中。</p><p><strong>痛点三：分析灵活性差，难以下钻明细</strong><br/>● 宽表预聚合导致数据粒度固化，无法满足灵活分析需求：宽表通常按固定维度（如“日期+区域+品类”）预聚合，当业务需要按“渠道+门店”分析时，只能新建宽表或放弃分析。<br/>● 跨表分析需新建宽表，无法动态组合维度和指标：不同宽表之间缺乏统一的语义关联，跨表分析需要重新建模，无法实现“任意维度+任意指标”的动态组合查询。<br/>● 明细数据被汇总后丢失，归因分析只能靠人工猜测：宽表只保留汇总结果，原始明细数据被丢弃，当出现数据异常时，无法下钻到明细交易进行根因分析，只能依赖人工经验猜测。</p><h2>NoETL 明细语义层——ChatBI 数据底座的核心</h2><p>● 基于明细层数据模型进行语义抽象，覆盖完整分析场景：明细语义层直接对接企业数仓 DWD 层的明细模型，沉淀所有明细级语义，支持从宏观汇总到明细下钻的全场景问数需求。<br/>● 指标和维度一次定义，多处使用，确保口径一致：通过可视化配置指标逻辑（组合度量/维度/限定），自动生成无歧义 SQL，指标逻辑全局唯一，下游应用直接调用，避免重复开发与口径分歧。<br/>● 支持原子指标、派生指标、衍生指标的统一管理：原子指标（如“销售额”“客单价”）和维度（如“时间”“地区”）在语义层标准化定义，派生指标和衍生指标基于原子指标动态生成，无需预先固化所有分析路径。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmNuE" alt="" title=""/></p><h2>Aloudata Agent：基于 NoETL 明细语义层的分析决策智能体</h2><p>借助于 NoETL 明细语义层和 NL2MQL2SQL 的技术路径，Aloudata Agent 可以将自然语言查询转换为对指标语义层的精准查询请求，再由语义引擎生成准确、可执行的 SQL，有效避免了语义歧义与数据幻觉现象。该功能还支持复杂的智能归因分析，如维度归因和因子归因，并能自动生成结构化报告。</p><p>面对复杂的分析任务，Aloudata Agent 提供的多 Agent 协同架构能够自动进行拆解与协同处理。以“Q2 利润下滑”分析为例，系统可自动将其分解为收入分析、成本分析、异常交易检测等子任务，并分别调用相应的指标查询、归因分析和报告生成等子智能体，最终交付一个包含数据查询结果、关键异常发现及具体行动建议的完整结构化报告。</p><p>此外，Aloudata Agent 提供场景化的分析助手功能，以沉淀和复用业务知识。支持根据不同业务职能创建个性化助手，如门店运营助手或财务分析助手。每个助手可配置独立的资源管理与访问权限，有效避免跨业务间的数据干扰。同时，用户可在使用中维护个人术语知识和分析思路，促进业务知识的持续积累与沉淀。<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdmOdO" alt="" title="" loading="lazy"/></p><p>最后，为确保数据安全与合规，NoETL 明细语义层可支持行级和列级的数据权限，确保用户仅能访问其权限范围内的数据，如客户经理仅能看到所负责客户的销售数据。同时，通过多租户隔离机制，满足不同业务部门或子公司的独立使用需求，并严格管控数据访问，以符合金融、医疗等行业对数据安全与合规的严格要求。</p><h2>FAQ: 常见疑问解答</h2><p><strong>Q1: 语义层方案是否会影响查询性能？​</strong><br/>不会。基于 NoETL 明细语义层的方案通过智能物化加速和查询改写优化，能够保障亿级数据秒级响应。语义层负责逻辑抽象，底层通过数据虚拟化引擎和物化策略实现性能优化，相比宽表方案在灵活性和性能上取得更好平衡。</p><p><strong>Q2: 语义层如何解决数据口径一致性问题？​</strong><br/>语义层通过统一指标定义和指标血缘管理，确保所有分析场景消费相同的指标口径。业务规则迭代只需在语义层一次修改，全链路查询自动同步更新，避免了宽表方案中口径碎片化的问题。</p><p><strong>Q3: 语义层方案是否支持跨表分析？​</strong><br/>支持。基于明细语义层的方案突破分析维度和数据粒度固化，支持任意维度和指标的灵活组合，实现跨表动态查询。相比宽表预聚合方案，语义层在分析灵活性上具有明显优势。</p>]]></description></item><item>    <title><![CDATA[英伟达旗下芯片 咕噜云服务器晚晚 ]]></title>    <link>https://segmentfault.com/a/1190000047524932</link>    <guid>https://segmentfault.com/a/1190000047524932</guid>    <pubDate>2026-01-06 18:03:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>英伟达作为全球领先的半导体设计公司，其芯片产品以卓越的性能和广泛的应用场景引领着计算技术的变革。以下从产品体系、技术优势、应用领域及市场影响四个维度，对英伟达旗下芯片进行全面解析。<br/>一、产品体系：覆盖多场景的芯片矩阵<br/>英伟达芯片产品布局呈现“通用计算+专用加速”双轮驱动特征，核心产品线包括：</p><ol><li><p><strong>GPU（图形处理器）</strong></p><ul><li><strong>GeForce系列</strong>：面向消费级市场，如RTX 4090/4080采用Ada Lovelace架构，集成DLSS 3技术，支持光线追踪和AI渲染，为游戏、内容创作提供超高清画质与实时交互体验。</li><li><strong>Quadro（现更名为RTX A系列）</strong>：专业图形卡，针对CAD设计、影视特效等领域，如RTX A6000搭载48GB GDDR6显存，支持多屏输出和GPU加速计算，满足工业级渲染需求。</li><li><strong>Tesla系列（已逐步整合至数据中心产品线）</strong>：早期数据中心GPU，为云计算、科学计算提供算力支持，后续迭代为A100/H100等型号。</li></ul></li><li><p><strong>数据中心芯片</strong></p><ul><li><strong>Hopper架构H100</strong>：基于台积电4nm工艺，集成4nm CoWoS封装技术，支持HBM3显存（带宽达5TB/s）和PCIe 5.0，单芯片FP8算力达4PetaFLOPS，是AI训练与超算的核心引擎，被广泛应用于ChatGPT等大模型训练。</li><li><strong>Ampere架构A100</strong>：前一代数据中心旗舰，采用5nm工艺，支持多实例GPU（MIG）技术，可分割为7个独立计算单元，兼顾算力与资源利用率，已部署于全球超50%的TOP500超级计算机。</li><li><strong>Grace CPU</strong>：首款基于ARM架构的高性能CPU，与Grace Hopper Superchip结合GPU与CPU优势，内存带宽达1TB/s，专为AI和HPC workload优化，目标替代传统x86服务器芯片。</li></ul></li><li><p><strong>专用加速芯片</strong></p><ul><li><strong>DPU（数据处理单元）</strong>：如BlueField系列，集成ARM CPU、可编程网络加速引擎和安全协处理器，卸载服务器CPU的网络、存储与安全任务，提升数据中心整体效率，已与微软、AWS达成深度合作。</li><li><strong>AGX Orin</strong>：面向边缘计算与自动驾驶，集成12核ARM Cortex-A78AE CPU和2048核Ampere GPU，AI算力达200TOPS，支持L4级自动驾驶功能，是特斯拉、蔚来等车企的自动驾驶域控制器核心芯片。  <br/>二、技术优势：架构创新与生态壁垒<br/>英伟达芯片的核心竞争力源于持续的架构突破与全栈生态构建：</li></ul></li><li><strong>架构设计</strong>：从Fermi架构引入CUDA核心，到Volta架构的Tensor Core（专为深度学习优化），再到Hopper架构的Transformer Engine（加速大语言模型训练），每代架构均针对AI与并行计算需求升级。例如，Tensor Core支持混合精度计算，将FP16与FP32结合，在精度损失极小的情况下提升AI算力2-4倍。</li><li><strong>软件生态</strong>：CUDA平台作为全球最成熟的GPU编程模型，拥有超过400万开发者和2000+应用程序支持，形成“硬件-软件-开发者”正循环。此外，TensorRT（推理优化工具）、cuDNN（深度学习库）等工具链进一步降低AI开发门槛，巩固英伟达在AI软件生态的垄断地位。</li><li><strong>制程与封装技术</strong>：与台积电深度合作，率先采用4nm、5nm先进制程，并主导CoWoS（Chip on Wafer on Substrate）封装技术，实现多芯片异构集成（如H100集成GPU、HBM显存和IO die），突破物理性能瓶颈。  <br/>三、应用领域：从消费电子到产业变革<br/>英伟达芯片已渗透至几乎所有计算密集型领域：</li><li><strong>AI与深度学习</strong>：凭借H100/A100的算力优势，占据全球AI加速芯片市场80%以上份额，客户涵盖OpenAI、谷歌、Meta等科技巨头，以及高校科研机构。其推出的NeMo框架和NGC（NVIDIA GPU Cloud）平台，为开发者提供从模型训练到部署的全流程支持。</li><li><strong>自动驾驶</strong>：AGX Orin已成为自动驾驶域控制器的“标配”，除特斯拉外，小鹏、理想、奔驰等车企均采用其方案，支持激光雷达点云处理、实时路径规划等关键功能，推动L3/L4级自动驾驶商业化落地。</li><li><strong>元宇宙与数字孪生</strong>：GeForce RTX系列与Omniverse平台结合，支持实时物理模拟和3D渲染，助力企业构建虚拟工厂、数字城市等场景。例如，宝马利用Omniverse和RTX GPU构建虚拟生产线，将新车研发周期缩短30%。</li><li><strong>医疗与科学计算</strong>：在蛋白质结构预测（如AlphaFold）、癌症药物研发、气候模拟等领域，英伟达GPU加速了科研进程。美国橡树岭国家实验室的Summit超算（搭载V100 GPU）曾实现0.1秒内完成一次全球气候模型模拟。  <br/>四、市场影响与挑战<br/>英伟达芯片已成为全球数字经济的基础设施，2023年数据中心业务营收达470亿美元，同比增长279%，占总营收比重超70%，远超传统PC GPU业务。其市值一度突破2万亿美元，成为全球市值最高的半导体公司。  <br/>然而，挑战亦随之而来：</li><li><strong>供应链依赖</strong>：高度依赖台积电先进制程，地缘政治风险可能导致产能受限；</li><li><strong>竞争加剧</strong>：AMD推出MI300X GPU、Intel加速Xeon Max与Habana Labs AI芯片布局，AWS、谷歌等云厂商自研AI芯片（如Trainium/TPU），试图降低对英伟达的依赖；</li><li><strong>技术瓶颈</strong>：摩尔定律逼近极限，单芯片算力提升放缓，需通过Chiplet、3D封装等技术突破物理限制。  <br/>总结<br/>英伟达通过“GPU+CUDA”生态构建、架构持续创新与场景深度绑定，已从图形芯片供应商跃升为全球计算技术的引领者。其芯片不仅是AI革命的核心驱动力，更在重塑数据中心、自动驾驶、科学研究等产业格局。面对未来，如何在保持技术领先的同时应对供应链与竞争压力，将是英伟达持续增长的关键命题。</li></ol>]]></description></item><item>    <title><![CDATA[数字孪生中的 流渲染技术 与 大规模场景实践 数字孪生进化论 ]]></title>    <link>https://segmentfault.com/a/1190000047524943</link>    <guid>https://segmentfault.com/a/1190000047524943</guid>    <pubDate>2026-01-06 18:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当“数字孪生”遭遇“卡在75%”的尴尬时刻，你是否曾遇到过这样的场景：</p><ul><li><em>在智慧城市平台上，试图拉近查看某个街道的实时交通状况，画面却永远卡在75%的加载进度？</em></li><li><em>在工厂数字孪生系统中，想要查看一台关键设备的内部结构，浏览器却因为模型过重而崩溃？</em></li><li><em>好不容易加载完一个园区的模型，旋转视角时却像看PPT一样一顿一顿？</em></li></ul><p>这些，都是数字孪生（Digital Twin）从概念走向大规模落地时，必须面对的“渲染之痛”。</p><p>数字孪生作为物理世界在数字空间的全息映射，正从单体设备走向城市级、园区级的大规模场景。然而，场景规模的增长速度，远远超过了硬件和网络性能的提升速度。一个现代智慧城市的数字孪生，可能需要处理数千平方公里的地理数据、数万栋建筑的 BIM 模型、数百万个物联网传感器的实时数据——这已经远远超出了传统三维渲染技术的处理能力。<br/><img width="723" height="437" referrerpolicy="no-referrer" src="/img/bVdnzyG" alt="" title=""/><br/>当场景数据量从 GB 级跃升至 TB 级乃至 PB 级时，基于“全量下载、本地渲染”的模式已无法满足实时性、可访问性与协同性要求。流渲染（Streaming Rendering）技术由此成为支撑大规模数字孪生应用的核心基础设施，通过数据调度、传输与呈现方式的根本性变革，实现了海量三维数据的可访问与可操作。</p><h2>一、为什么传统渲染在大规模数字孪生中“失灵”了？</h2><h3>1）数字孪生渲染的三大“不可能三角”：</h3><p>在理想状态下，我们希望数字孪生渲染同时满足：</p><ul><li><strong>高质量</strong>：高精度模型、真实材质、复杂光照</li><li><strong>大规模</strong>：城市级、工厂级的海量数据承载</li><li><strong>低延迟</strong>：实时交互、快速响应</li></ul><p>但传统渲染方式（尤其是基于 WebGL 的本地渲染）在面对大规模场景时，陷入了典型的“不可能三角”——三者不可兼得。</p><h3>2）数据量的“指数级爆炸”</h3><p>让我们看一组真实的数据对比：</p><ul><li><strong>单体设备</strong>：一个高精度泵阀模型，约50-100MB</li><li><strong>中型工厂</strong>：包含1000台设备，约50-100GB</li><li><strong>智慧城市</strong>：200km²区域，包含建筑、道路、植被，约10-20TB</li></ul><p>当数据量从GB级跃升到TB级，传统的“全量下载+本地渲染”模式彻底失效。即便使用最先进的网络（千兆光纤），下载10TB数据也需要超过24小时——这显然是不可接受的。</p><h3>3）终端设备的“性能天花板”</h3><p>数字孪生的用户终端千差万别：</p><ul><li>高性能工作站（专业GPU，32GB+内存）</li><li>普通办公电脑（集成显卡，8GB内存）</li><li>平板电脑、手机（移动端GPU，有限的内存）</li></ul><p>如果按照最高性能设备来设计，低端设备无法运行；如果按照最低性能设计，高端设备的潜力无法发挥。流渲染正是通过“按需分配”解决了这一矛盾。</p><h2>二、流渲染技术核心：按需调度与协同计算</h2><p>流渲染的本质是一种数据供给范式的革新。它不要求终端设备完整拥有或处理整个庞大的数据集，而是建立一个智能的调度系统，确保用户在交互过程中，能够实时获得其视野内必要的数据切片，并在云端或边缘完成大部分繁重的计算。<br/><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdnzyU" alt="" title="" loading="lazy"/><br/>传统三维渲染依赖于终端设备的完整数据加载与本地图形计算能力，其瓶颈在于：</p><ul><li><strong>数据承载极限</strong>：终端存储与内存无法容纳城市级BIM+GIS融合数据</li><li><strong>计算能力不均</strong>：用户设备性能差异导致体验割裂</li><li><strong>网络传输低效</strong>：海量数据下载耗时无法满足实时需求</li></ul><p>流渲染通过将渲染管线解耦为“云端计算-网络传输-终端呈现”三个环节，实现了：</p><ul><li><strong>计算卸载</strong>：复杂渲染任务在云端高性能集群完成</li><li><strong>按需传输</strong>：仅传输视域范围内的必要数据</li><li><strong>终端适配</strong>：根据设备能力自动调整数据质量与呈现方式</li></ul><p>其核心在于：<strong>以网络和云端算力换取终端的普适性与体验的一致性</strong>。</p><h3>1) 空间分块与动态加载</h3><p>面对城市级模型，流渲染引擎会将整个三维场景依据地理坐标或逻辑结构，划分为众多可独立管理的数据块。当用户浏览时，系统持续进行视锥体裁切计算，仅请求并加载当前及邻近可视区域的数据块。远处的、不可见的部分则被暂时搁置，从而将单次处理的负载降至最低。<br/><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnzzp" alt="" title="" loading="lazy"/></p><h3>2) 多层次细节自适应</h3><p>为平衡画质与性能，同一地理对象会预先生成多个细节层级的版本。当用户从高空俯瞰时，引擎调用最低层级的简化模型以呈现宏观格局；当用户逐步拉近视角，系统则无缝切换至更精细的模型层级，展示建筑立面、窗户乃至设备纹理。这一过程是动态自适应的，依据屏幕像素误差、网络状况和设备性能自动决策，确保流畅交互。<br/><img width="723" height="401" referrerpolicy="no-referrer" src="/img/bVdnzzq" alt="" title="" loading="lazy"/></p><h3>3) 静态模型与动态数据的实时融合</h3><p>数字孪生的价值在于映射物理世界的状态。流渲染架构不仅传输静态三维网格，更建立了与实时数据流的通道。物联网传感器的读数、业务系统的状态更新、模拟分析的结果，都能作为属性或动画实时“绑定”到对应的三维实体上，将静态场景转化为一个持续跳动、反映现实状态的动态孪生体。</p><h2>三、技术冷静期：正视流渲染的挑战与局限</h2><p>在充分认识流渲染技术巨大优势的同时，我们必须清醒地认识到其固有的技术局限与应用挑战。任何技术决策都是利弊权衡的结果，全面理解流渲染的弊端对于架构设计与项目成功至关重要。</p><p>流渲染的根本前提是稳定可靠的网络连接，这在实际应用中构成了显著的脆弱性：</p><h3>1）连接敏感性问题</h3><ul><li><strong>网络抖动的影响</strong>：即使平均带宽足够，网络延迟的波动也会导致画面卡顿、加载中断；</li><li><strong>网络盲区限制</strong>：地下空间、偏远厂区、船舶等网络覆盖不足环境难以应用；</li><li><strong>带宽成本限制</strong>：高清画质流式传输的带宽消耗可观，大规模应用时网络成本显著。</li></ul><h3>2）成本结构的不确定性</h3><p>流渲染服务的使用成本是：固定基础设施成本+随用户数、使用时长、画质等级线性增长的变动成本，突发流量可能导致成本超预期增长（如突发事件期间大量并发访问）。</p><h3>3）技术碎片化现状</h3><ul><li><strong>技术栈选择困境</strong>：各家云服务商提供互不兼容的流渲染方案，不同终端平台（Web、移动、XR）需要不同适配方案；</li><li><strong>集成复杂度高</strong>：与现有业务系统（ERP、MES、SCADA）集成缺乏标准接口，多源数据融合（BIM+GIS+IoT）的流式化处理缺乏成熟方案。</li></ul><h2>四、技术演进趋势：下一代流渲染架构</h2><p>随着数字孪生应用向更广泛领域和更深层次发展，流渲染技术正在经历从单一方案向多元融合的演进。下一代流渲染架构将不再局限于传统的云端渲染或纯 WebGL 方案，而是呈现出多技术路径融合、智能协同的新特征。</p><p><img width="668" height="213" referrerpolicy="no-referrer" src="/img/bVdnzzr" alt="" title="" loading="lazy"/><br/>随着数字孪生应用场景的多样化，渲染架构并未收敛于单一方案，而是形成了 “<strong>端渲染（WebGL）</strong>”与“<strong>流渲染</strong>”<strong>两条清晰且长期共存的技术路径</strong>。下一代架构的核心演进方向并非强制性融合，而是实现更深刻的 “<strong>兼容性</strong>”——即系统平台能够支撑两种模式，并允许用户或部署者根据具体场景的约束与需求，做出最务实的选择。关键在于理解“何时选择端”与“何时选择流”，并在架构上使二者能灵活部署甚至并存。</p><h2>五、渲染不是终点，体验与价值才是</h2><p>数字孪生的终极目标并非仅仅是“渲染得更好”，而是“用得更好”。流渲染作为一种关键的基础设施技术，其意义在于让大规模、高保真的三维场景能够被广泛、实时、协同地访问与操作，从而支撑起真正的业务洞察与决策优化。</p><p>未来，随着5G/6G网络普及、算力成本持续下探、渲染与 AI 技术进一步融合，流渲染将更加智能、自适应与无缝。但无论技术如何演进，衡量其成功的标准始终是：是否让数字孪生从“可看”走向“可用”，从“展示”走向“驱动”。</p><p>关注 “数字孪生进化论” ，与我们共同探索数字孪生技术前沿与落地实践。</p>]]></description></item><item>    <title><![CDATA[如何利用生产调度分析实现汽车生产的实时决策与优化？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524952</link>    <guid>https://segmentfault.com/a/1190000047524952</guid>    <pubDate>2026-01-06 18:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代汽车制造业加速向智能化、柔性化与绿色化转型的背景下，生产调度分析已从传统的排产工具，演变为驱动整车制造效率跃升的核心决策中枢。面对多品种、小批量、快交付的市场需求，传统依赖人工经验与静态计划的调度方式难以应对复杂的产线协同与供应链波动，而以数据驱动、算法赋能和人机协同为特征的新一代生产调度分析系统，正重塑汽车生产的响应逻辑与价值创造模式。<br/>广域铭岛的Geega工业互联网平台正是这一变革的典型实践者。在汽车制造场景中，Geega系统通过实时采集焊装、涂装、总装等关键工序的300余项工艺参数，结合Few-Shot Learning等先进算法，将老师傅对工序交叉操作的隐性经验——如特定车型换线时的工位衔接节奏、物料批次异常时的应急处理策略——转化为可计算、可复用的智能模型。这使得系统不仅能提前12小时预判物料齐套风险，还能在焊装线因夹具切换导致停机前自动调整排程，避免产线空转，显著提升设备综合效率（OEE）。<br/>在汽车生产中，时间损失、速度损失与质量缺陷是影响交付与成本的三大核心瓶颈。Geega平台通过对OEE的精细化钻取分析——精准识别设备故障、换型时间、速度降速与缺陷率等关键因子——帮助企业定位效率洼地。例如，在某新能源汽车厂商的产线部署中，系统通过动态优化电池包装配节拍与物流配送节奏，将单台车的装配周期缩短8%，同时将因物料错配导致的返工率降低40%。更进一步，系统还与供应链端联动，当预判到某关键电子元件到货延迟时，自动触发供应商预警，并智能重组后续车型的生产顺序，确保高价值订单优先交付。<br/>此外，生产调度分析在汽车制造中的价值不仅体现在技术层面，更推动了组织形态的升级。借助FineBI等商业智能工具，生产、物流、质量等部门可共享同一套数据视图，实现“厂级—车间—工段”三级联动决策。管理者不再依赖周报与会议做判断，而是通过可视化仪表盘实时掌握产线健康度，实现分钟级策略迭代。这种“协同式治理”模式，让原本割裂的部门协同成为常态，真正实现了“以数据说话、用算法决策”。<br/>对于中小型汽车零部件供应商而言，广域铭岛提供的低代码开发界面与预置行业模型库，大幅降低了系统落地门槛。无论是冲压线的预测性维护，还是注塑工艺的良率优化，企业均可通过拖拽式配置快速构建专属AI调度应用，实现低成本、敏捷化部署。<br/>综上所述，生产调度分析在汽车制造领域已超越“排产”功能，成为连接人、机、料、法、环的智能神经中枢。它以数据为基因、算法为引擎、人机协同为纽带，不仅提升了产线效率与交付韧性，更重构了汽车企业的运营逻辑。广域铭岛的实践表明，唯有将先进的调度分析系统与制造流程深度耦合，汽车企业才能在激烈竞争与供应链波动中，实现柔性生产、绿色低碳与客户满意三者的统一，真正迈向智能制造的新纪元。</p>]]></description></item><item>    <title><![CDATA[基于 SGlang RBG + Mooncake 打造生产级云原生大模型推理平台 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524957</link>    <guid>https://segmentfault.com/a/1190000047524957</guid>    <pubDate>2026-01-06 18:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者 | 玖宇（SGLang 社区 &amp; 阿里云），杨彦波（SGLang 社区 &amp; 科大讯飞），孙伟祥（SGLang 社区 &amp; 小红书），宋阳 （SGLang 社区 &amp; 小红书），雨杨（Mooncake &amp; 阿里云）</p><h3>背景</h3><p>大语言模型（LLM）推理服务正迅速成为企业级应用的核心基础设施。生产级落地的关键在于性能、稳定性与成本三者的平衡，而本文聚焦于如何构建稳定的高性能推理系统。</p><p>当前，LLM 推理架构正从单体模式向分布式演进，主流路径包括 Prefill-Decode（PD）分离、Attention-FFN（AF）分离以及 KVCache 外置。这一演进的根本动因是模型规模扩张导致的显存压力：在长上下文或高并发场景下，KVCache 显存占用常超 70%，单纯依赖 GPU HBM 与 CPU DRAM 已难以为继。将 KVCache 解耦外置，不仅能突破存储容量瓶颈，更能实现跨请求缓存共享、弹性伸缩与故障隔离等关键能力。尤其在 RAG、AI Agent、长文本生成等机器驱动消费 Token 的场景中，提示词模板化与可复用性成为常态，外置 KVCache 已成为保障低延迟、高吞吐与成本效益的必选项。</p><p>Mooncake 作为业界主流的分布式 KVCache 存储引擎，正是为应对这一挑战而生。它通过专用缓存集群为 SGLang 等推理框架提供高吞吐、低延迟的 KVCache 分布式服务。</p><p>然而，在生产环境中管理 Mooncake 这类分布式 KVCache 系统，以实现稳定的高性能仍面临新挑战：</p><ol><li>部署与运维复杂度高： 推理服务不限于单一 Pod，还可能是由 Prefill/Decode 计算节点与 Mooncake 缓存节点 构成的分布式系统。两者需在拓扑亲和、生命周期、扩缩容策略上深度协同，而 Kubernetes 原生 Workload（Deployment/StatefulSet）难以表达这种多角色强协同语义，导致配置繁琐、资源浪费或性能劣化。</li><li>滚动升级稳定性风险：Prefill 与 Mooncake 实例在升级过程中缓存丢失，迫使活跃会话的Prefill阶段需要重新计算，引发 P99 延迟毛刺与吞吐量断崖，严重影响服务稳定性。</li></ol><p>为根治这些痛点，RoleBasedGroup（RBG）应运而生。作为面向 AI 推理的 Kubernetes 原生 API，RBG 通过多角色协同编排，将 Mooncake 缓存与 SGLang 推理节点视为同一服务的不同角色，统一管理其部署、升级与弹性。借助 RBG 的原地升级与拓扑感知能力，既能尽可能避免缓存丢失，又能确保计算与缓存升级、调度和伸缩策略上的一致性，从而在性能最大化的同时，保障生产环境的稳定性与可运维性。</p><p>本文旨在阐明如何将 Mooncake Store 作为 RBG 编排下 SGLang PD 分离推理服务的补充角色，系统化实现生产级 KVCache 外置能力。</p><h3>Mooncake：面向大模型推理的分布式 KVCache 存储引擎</h3><p>项目地址：<a href="https://link.segmentfault.com/?enc=wXp3FXH3lp8dM%2Fz6gSlxSg%3D%3D.qLZCaoRhW0Y%2Fqo%2F%2FOMCoMVw8gkjbz4Y1NMCmYTiWIEl488BXlysKyrR11fSFwb8C" rel="nofollow" target="_blank">https://github.com/kvcache-ai/Mooncake</a></p><p>Mooncake 是 SGLang HiCache（层级缓存）的高性能分布式 L3 存储后端，通过 RDMA 实现跨机 KVCache 共享，突破单机 GPU/CPU 缓存容量瓶颈。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524959" alt="图片" title="图片"/></p><p>核心组件：</p><ul><li>Master Service： 管理集群存储池、元数据与节点生命周期</li><li>Store Service： 提供分布式缓存存储，支持多副本、条带化传输与热点负载均衡</li></ul><p>核心特性：</p><ul><li>RDMA 加速 + 零拷贝机制，实现高带宽、低延迟数据访问</li><li>智能预取与 GPU 直传，最大化 I/O 效率</li><li>支持 PD 分离架构，提升大规模集群 Token 吞吐量</li></ul><p>快速预览：</p><pre><code># 启动 Master
mooncake_master --http_metadata_server_port=9080
# 启动 Store 服务（配置 RDMA 设备与内存池）
python -m mooncake.mooncake_store_service --config=config.json
# 启动 SGLang（启用 Mooncake 后端）
python -m sglang.launch_server \
    --enable-hierarchical-cache \
    --hicache-storage-backend mooncake \
    --model-path &lt;model_path&gt;</code></pre><h3>RoleBasedGroup (RBG)：面向大模型推理的弹性角色编排引擎</h3><p>项目地址：<a href="https://link.segmentfault.com/?enc=wR222p%2FqWFRSrqxpjECARA%3D%3D.1WrS%2F8ZrnHTzVY%2FUyKc1azD0unioBT1RbQot9z0EZgtBw7vcdjp9tGbEAIqh99P5" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg</a></p><h4>3.1 核心问题：大模型推理生产落地的五大挑战</h4><p>大模型推理正演变为"最昂贵的微服务"——既需 HPC 集群的极致性能，又要求云原生的敏捷弹性。当前生产环境面临五大根本性挑战：</p><ol><li>快速架构迭代： 分离式大模型推理架构（如 Prefill/Decode 解耦、多级 Router/Gateway 等）演进极快，传统依赖固定抽象的平台难以及时适配新架构。</li><li>性能敏感：TTFT、TPOT 等关键性能指标对 GPU 拓扑（NVLink / PCIe）、RDMA 亲和性等因素有亚毫秒级敏感度，随意迁移或不当调度都会放大首响、尾响时延。</li><li>组件强依赖：关键角色之间存在强依赖关系（如 Prefill 与 Decode 等角色需要 1:1、N:1 等强绑定关系），版本升级、回滚必须在多个角色之间保持原子性，否则容易导致请求失败或数据不一致。</li><li>运维效率低：现有平台在重启、扩缩容、故障迁移等运维操作上缺乏对多角色整体的统一视角，日均高达 5% 的时间消耗于重启扩容升级中的手动协调，导致 GPU 资源空置浪费。</li><li>资源潮汐显著与利用率不足：线上流量峰谷差常超 10 倍，但静态配置的推理服务 GPU 平均利用率长期低于 30%，性能与成本难以兼得。</li></ol><p>根本矛盾：传统微服务面向无状态、弱拓扑场景，而大模型推理是强状态、拓扑感知、极致性能的有状态应用。</p><h4>3.2 RBG 设计理念：角色即一等公民，角色协同即核心场景</h4><p>RBG 源自 SGLang 社区，由小红书，算秩未来，科大讯飞、阿里云和南京大学等联合贡献。其核心目标，是在兼顾性能与稳定性的前提下，以"角色（Role）"作为调度编排的原子单元，构建贴合 LLM 推理特性的管理范式。</p><p>RBG 将一次推理服务视为拓扑化、有状态、可协同的"角色有机体"，而非孤立的 Deployment 集合。基于此理念，RBG 提出面向生产环境的 SCOPE 核心能力框架：</p><ul><li>S – Stable：面向拓扑感知的确定性运维</li><li>C – Coordination：跨角色协同策略引擎</li><li>O – Orchestration：有编排语义的角色与服务发现</li><li>P – Performance：拓扑感知的高性能调度</li><li>E – Extensible：面向未来的声明式抽象</li></ul><h4>3.3 SCOPE 核心能力解析</h4><p>3.3.1 Stable (稳定)：面向拓扑感知的确定性运维</p><p>稳定性是 RBG 的基石。通过为每个 Pod 注入全局唯一 RoleID，并遵循 "最小替换域" 原则，RBG 确保运维操作在原有 GPU-NVLink 域、NUMA 节点等硬件拓扑范围内完成，尽量避免拓扑漂移导致的性能抖动。</p><pre><code>roles:
- name: prefill
  replicas: 3
  rolloutStrategy:
    rollingUpdate:
      type: InplaceIfPossible
      maxUnavailable: 1</code></pre><p>3.3.2 Coordination (协同)：跨角色协同策略引擎</p><p>RBG 内置声明式协同引擎，通过Coordination 机制精确定义角色间依赖关系：</p><ul><li>部署协同：例如 Prefill 与 Decode 以特定比例成对调度、成组就绪；</li><li>升级协同：支持“比例协议”式升级，确保多角色版本一致性，避免部分升级导致协议不兼容；</li><li>故障协同：预定义联动策略，某个角色故障时触发关联角色的自动补救或迁移；</li><li>伸缩协同：在扩缩容时按照角色关系配比成组调整实例，保持吞吐与延迟表现稳定。</li></ul><p>这种精细化协同能力，将复杂分布式推理服务作为统一生命周期的整体进行管理，极大降低运维复杂度。</p><pre><code># 示例：PD 分离架构中 Prefill 和 Decode 角色协作升级
coordination:
- name: prefill-decode-co-update
type: RollingUpdate
  roles:
  - prefill
  - decode
  strategy:
    maxUnavailable: 5%
    maxSkew: 1% # 两个角色在升级的过程中新版本比例的最大偏差
    partition: 20%
roles:
- name: prefill
  replicas: 200
  template: ...
- name: decode
  replicas: 100
  template: ...</code></pre><p>3.3.3 Orchestration (编排)：编排化的角色与服务发现</p><p>RBG 显式定义角色依赖与精确启动顺序，实现编排化管理。更关键的是，它提供拓扑自感知的内建服务发现，在 Pod 启动时将完整拓扑信息（各角色 IP、属性、关系等）注入环境变量或配置文件。</p><p>推理引擎（SGLang、vLLM 等）可直接从本地配置读取拓扑视图，无需依赖 etcd、Consul 等外部服务发现系统，使服务跨环境迁移更自包含，显著降低集成复杂度。</p><p>3.3.4 Performance (性能)：拓扑感知的高性能调度</p><p>单次请求的延迟与吞吐高度依赖硬件拓扑与资源亲和性。RBG 引入拓扑感知的装箱策略，支持多维度性能优化：</p><ul><li>GPU 拓扑优先级（如 GPU-NVLink &gt; PCIe &gt; RDMA &gt; VPC）</li><li>角色之间的亲和与反亲和约束</li><li>同角色实例的布局均衡性</li><li>部署完成后的短路读优化</li></ul><p>通过这些约束与策略，RBG 在大规模部署时，能够在不牺牲稳定性的前提下，尽可能贴合最优的硬件拓扑，从而保障 TTFT、TPOT 等关键性能指标。</p><p>3.3.5 Extensible (可扩展)：面向变化的部署抽象</p><p>RBG 通过声明式 API（RBG、RBGs、EngineRuntimeProile等）与插件化机制，将 "角色关系定义"与"部署 / 模型管理 / 弹性策略"解耦 。</p><p>当社区演进出新架构（如新路由层形态、分离式架构等时），无需修改 RBG 核心代码，只需通过 YAML 定义新角色模板与关系，即可快速落地。这种"声明式 API + 插件机制"的平台化设计，将新架构的投产周期显著缩短。</p><pre><code># 示例：PD 分离架构角色定义
roles:
- name: prefill
  replicas: 2
  engineRuntimes:
  - profileName: custom-engine-runtime
  template:
  ...
- name: decode
  replicas: 1
  engineRuntimes:
  - profileName: custom-engine-runtime
  template:
  ...</code></pre><p>RBG 通过 Kubernetes 原生 API ，为大模型推理服务提供了一套稳定（Stable）、协同（Coordination）、可编排（Orchestration）、高性能（Performance）、可演进（Extensible）的统一承载层，是面向现代 LLM 推理工作负载的一种新型部署与运维抽象。</p><h3>基于RBG部署PD分离架构+Mooncake 推理服务</h3><p>4.1. 部署架构<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524960" alt="图片" title="图片" loading="lazy"/></p><p>通过 RoleBasedGroup 可部署高可用、弹性的 SGLang PD 分离推理系统，核心组件如下：</p><p>整个系统由以下核心角色构成：</p><ul><li>SGLang Router： 作为统一的请求入口与流量调度器，负责接收用户推理请求，根据负载状态、上下文长度和模型配置，智能为请求选择合适的Prefill 和 Decode 节点进行处理。</li><li>Prefill Serving Backend： 专用于处理提示词（prompt）的前向计算，生成初始 KVCache；通常为计算密集型，对显存带宽敏感。</li><li>Decode Serving Backend： 专注于自回归生成阶段的 token 逐个解码，依赖已生成的 KVCache 进行高效推理；对缓存访问延迟极为敏感。</li><li>Mooncake Master/Store： 作为独立的 KVCache 外置存储角色，提供高吞吐、低延迟的分布式缓存服务，持久化存储所有推理会话的 Key-Value Cache。它不仅突破了单 GPU HBM 和 CPU DRAM 的容量限制，还支持跨请求缓存复用以及细粒度缓存淘汰策略（如 LRU + 高水位驱逐）。</li></ul><p>这些角色并非孤立运行，而是通过 RBG 提供的原生多角色协同能力紧密集成。此外，EngineRuntime 作为 RBG 注入给引擎服务 Pod 的 Sidecar，成为推理引擎与上层编排系统的桥梁，提供了服务注册与元数据上报、动态 LoRA 加载 / 卸载、流量状态控制和可观测性集成的关键的运行时能力。</p><h4>4.2. 通过 RBG 部署 Mooncake + SGLang PD 分离推理服务</h4><ul><li>安装 RBG：<br/><a href="https://link.segmentfault.com/?enc=TuMJiWrLMIIf058PFduYZw%3D%3D.hs4kzMlbnrHFgbCpDQPKW%2FD4Y9gOVlyEoDLpt02nFHR1ee4R7Yzmq6%2FdOn86gebo9Awug3fvQTa%2B5eX80e3oJA%3D%3D" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/doc/install.md</a></li><li>镜像准备见附录 8.1</li><li>服务部署</li></ul><p>准备好容器镜像后，使用下面的 yaml，可以基于 RBG 部署带有 KVCache Offloading 能力的 SGLang PD 分离推理服务：<br/><a href="https://link.segmentfault.com/?enc=V82mF4XvZLciM7YEqjTpOQ%3D%3D.yGiTN629X%2BnAI82rtYbwo0%2FfiA5czqgD2QnMWkceOoaLj6wK7i8ATrR1T6nRKa1iLh9zrT4jzizk0IIL4%2FJHaW8%2FxtlWdrCGkBJ34b3A7ErrWRsqBZY%2FiyS75EqMUB29aHMBf576Yn3ohRlH358qOw%3D%3D" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/examples/mooncake/pd-disaggregated-with-mooncake.yaml</a> </p><p>yaml 中涉及的环境变量说明可以参考：<br/><a href="https://link.segmentfault.com/?enc=kS7Nb7lKuR2k5amX5juKeg%3D%3D.3LTA8haw3H1hUjZmcxaeYzeWRNKWjVIPSrV5hN5mdSnaKvxUwFa%2BPnZjHksY%2F9nB8a9xwftQVCb5aNPu6Q7gEENdA3nocVFfqG%2BN6FULhDA%3D" rel="nofollow" target="_blank">https://github.com/kvcache-ai/Mooncake/blob/main/doc/zh/mooncake-store.md</a></p><ul><li>查看部署结果：</li></ul><pre><code>kubectl get pods -l rolebasedgroup.workloads.x-k8s.io/name=sglang-pd-with-mooncake-demo
sglang-pd-with-mooncake-demo-router-0               1/1     Running   0          71s
sglang-pd-with-mooncake-demo-prefill-0              1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-decode-0               1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-master-0      1/1     Running   0          4m2s
sglang-pd-with-mooncake-demo-mooncake-store-bh9xs   1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-store-dsrv4   1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-store-tqjvt   1/1     Running   0          3m42s</code></pre><ul><li>查看 Mooncake Store 角色其中一个实例的网络和 location 信息：</li></ul><pre><code>kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.spec.nodeName}'
kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.status.podIP}'</code></pre><h4>4.3. Benchmark 测试结果：多级缓存加速显著</h4><ul><li>Baseline（仅 GPU 显存）：缓存命中率低，平均 TTFT 5.91s，P90 12.16s，系统吞吐受限，InputToken 吞吐仅为 6576.85 token/s。</li><li>L2DRAMHiCache： 命中率提升至 40.62%，平均 TTFT 降至 3.77s（↓36.2%），P90 降至 10.88s，InputToken 吞吐提升至 10054.21 token/s（↑52.89%）。</li><li>L3 Mooncake 缓存：命中率进一步跃升，平均 TTFT 降至 2.58s（↓56.3%），P90 大幅改善至 6.97s（↓42.7%），InputToken 吞吐提升至 15022.80 token/s（↑49.41%）。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524961" alt="图片" title="图片" loading="lazy"/><br/>（图/多轮对话测试场景下服务整体吞吐指标）</p><p><img width="723" height="420" referrerpolicy="no-referrer" src="/img/bVdnzzG" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524962" alt="图片" title="图片" loading="lazy"/><br/>（图/多轮对话测试场景下 KVCache 命中率及对应 TTFT 指标）</p><p><em>测试细节详见附录 8.2。</em></p><h3>通过原地升级能力实现Mooncake 版本平滑升级</h3><p>由于 Mooncake 内置的 transfer-engine 与 SGLang Serving Backend（Prefill/Decode）中的 transfer-engine 需保持严格版本一致，以确保 KVCache 传输协议的兼容性，因此在推理引擎升级时，Mooncake 需要同步进行版本更新。</p><p>然而，Mooncake 作为有状态的缓存服务，其 KVCache 数据通常仅驻留在内存中。在传统 Kubernetes 滚动升级（Rolling Update）过程中，旧 Pod 被终止时，其内存中的缓存数据会立即丢失；而新 Pod 启动后需要经历重新调度、重新创建的过程。这导致所有依赖该节点缓存的活跃推理会话被迫中断，必须重新执行完整的 Prefill 计算——这一过程不仅计算开销巨大，还会引发：</p><ul><li>P99 首 Token 延迟显著毛刺（从秒级飙升至数十秒）；</li><li>因大量请求排队等待 Prefill，导致的系统吞吐量断崖式下跌；</li><li>用户体验剧烈抖动，破坏生产环境的服务稳定性。</li></ul><p>解决方案：Mooncake 缓存本地持久化 + RBG 原地升级：</p><ul><li>Mooncake 缓存本地持久化：在 Mooncake 社区的 PR#1031 中，mooncake 支持在节点 ShareMemory 和本地磁盘（或高性能 NVMe）上将 KVCache 元数据与热数据快照持久化，确保进程重启后可快速恢复缓存状态，避免缓存失效导致的 Prefill 重计算；</li><li>RBG 原地升级：通过 RBG 的精细化角色控制能力，在升级 Mooncake 角色时避免重建 Pod，而是原地替换容器镜像并复用节点的本地盘或共享内存，从而保留已持久化的缓存数据，实现“无缝”版本切换。</li></ul><p>二者结合，使得在 Serving Backend 与 Mooncake 联合升级过程中，KVCache 状态得以延续，活跃会话无需回退到 Prefill 阶段，从而有效规避了延迟毛刺与吞吐下跌，保障了大模型推理服务在版本迭代期间的端到端稳定性与高可用性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524963" alt="图片" title="图片" loading="lazy"/></p><p>换言之，RBG 不仅解决了多角色协同部署的复杂性，更通过原地升级，将“有状态缓存服务的平滑演进”这一行业难题转化为标准化、可自动化的运维能力，真正实现了 “升级无感、服务不抖” 的生产级目标。</p><p>我们对刚刚部署的服务进行引擎版本的更新，由 v0.5.5 版本更新至 v0.5.6。</p><pre><code>kubectl patch rolebasedgroup sglang-pd-with-mooncake-demo \
  --type='json' \
  -p='[{"op": "replace", "path": "/spec/roles/1/template/spec/containers/0/image", "value": "lmsysorg/sglang:v0.5.6"}]'</code></pre><p>通过查看 Pod 状态能发现，在 Mooncake Store 角色镜像版本更新后仅发生了一次容器的重启。</p><pre><code>kubectl get pods -l rolebasedgroup.workloads.x-k8s.io/name=sglang-pd-with-mooncake-demo -owide
NAME                                                READY   STATUS             RESTARTS   AGE
sglang-pd-with-mooncake-demo-decode-0               1/1     Running            0          7m4s
sglang-pd-with-mooncake-demo-mooncake-master-0      1/1     Running            0          7m24s
sglang-pd-with-mooncake-demo-mooncake-store-bh9xs   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-mooncake-store-dsrv4   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-mooncake-store-tqjvt   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-prefill-0              1/1     Running            0          7m4s
sglang-pd-with-mooncake-demo-router-0               1/1     Running            0          4m33s</code></pre><p>可以通过查看 Pod 的事件确认重新原因：</p><pre><code>kubectl describe pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4
Events:
  Type     Reason          Age                  From               Message
  ----     ------          ----                 ----               -------
  Normal   Scheduled       27m                  default-scheduler  Successfully assigned default/sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 to cn-beijing.10.134.xxx.xxx
  Normal   AllocIPSucceed  27m                  terway-daemon      Alloc IP 10.134.25.238/16 took 584.019653ms
  Normal   Created         27m                  kubelet            Created container: store
  Normal   Pulled          27m                  kubelet            Container image "lmsysorg/sglang:v0.5.5" already present on machine
  Normal   Started         27m                  kubelet            Started container store
  Normal   Killing         21m                  kubelet            Container store definition changed, will be restarted</code></pre><p>确认重启的 Mooncake 实例状态可以发现，在原地升级后 Pod 的网络和拓扑信息并没有发生改变，配合 Mooncake 提供的缓存持久化能力，可以保证重启前的 KVCache 缓存并没有发生丢失，在原地升级后预期地完成了恢复。</p><pre><code>kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.spec.nodeName}'
 kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.status.podIP}'</code></pre><h3>总结和展望</h3><p>本文系统阐述了如何通过 RoleBasedGroup（RBG） 与 Mooncake 的协同设计，构建生产级的稳定高性能 PD 分离推理服务。结论如下：</p><ul><li>RBG 重新定义了 LLM 推理服务的编排范式：通过将多角色协同（PD 分离、Mooncake 缓存）与拓扑感知调度作为一等公民，RBG 不仅解决了分布式部署的复杂性，更通过原地升级能力攻克了"有状态缓存服务平滑演进"这一行业难题，实现了升级无感、服务不抖的生产级目标。</li><li>Mooncake 解锁了 KVCache 的无限可能：作为 L3 缓存层，Mooncake 通过分布式内存池与 RDMA 加速，使缓存命中率跃升，TTFT 降低 56.3%，P90 延迟改善 42.7%，同时将 GPU 平均利用率从不足 30% 提升至可持续弹性伸缩的水平，真正平衡了性能与成本。</li><li>分级缓存架构是长上下文推理的必由之路：从 GPU HBM → DRAM → Mooncake 的三级缓存体系，在 Benchmark 中证明了其有效性，尤其在多轮对话、RAG、AI Agent 等机器驱动场景中，缓存复用带来的边际成本递减效应将愈发显著。</li></ul><p>RBG + Mooncake 的实践表明，只有将高性能系统设计与云原生运维能力深度融合，才能让大模型推理真正从"能用"走向"好用"，从"实验室"走向"生产级"。 我们期待与社区共同推进这一范式，为下一代 AI 基础设施奠定基础。</p><p>Acknowledgment</p><ul><li>小红书：孙伟祥、宋阳、熊峰</li><li>科大讯飞：杨彦波</li><li>趋境科技：杨珂</li><li>Mooncake：马腾、蔡尚铭</li><li>阿里云：一斋、柏存、东伝</li></ul><h4>附录</h4><p>8.1 镜像构建</p><p>此本文所使用部署样例中，我们可以直接使用 SGLang社区的官方容器镜像 lmsysorg/sglang:v0.5.5（mooncake-transfer-engine &gt;= 0.3.7），该镜像已经默认包含了 Mooncake 相关依赖。如果有定制化需求，可以参考链接中提供的 Dockerfile 自行构建特定版本的 Mooncake 镜像：<a href="https://link.segmentfault.com/?enc=QuSr0WjXf4EYMqIT3hwGBw%3D%3D.lVaUJjMoE9aaLIsylK9UMkdDfZIq%2BYiqUsOVx6Yrp6lDhEjeIkUtINDhE%2FYw7GmhyuF0DnhYTN%2BE7fuwaPHskUYHWsuCFaVtMW8xrm2R1Ut%2BfL9NENvLsk1ZWbwKTqlz" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/examples/mooncake/Dockerfile.mooncake</a></p><p>8.2 Benchmark 测试<br/>8.2.1 环境配置<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524964" alt="图片" title="图片" loading="lazy"/><br/>8.2.2 测试方法<br/>通过 HiCache 提供的多轮对话压测工具模拟多轮对话场景，测试 KVCache 可重用场景下开启了 L3 Mooncake + L2 Hicache 的推理服务，相对于仅开启了 L2 Hicache 和不开启 Hicache 的推理服务，在吞吐指标和 SLO 指标上的收益情况。</p><ul><li>测试对象<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524965" alt="图片" title="图片" loading="lazy"/></li><li>测试命令</li></ul><pre><code>python3 benchmark/hicache/bench_multiturn.py \
--model-path /models/Qwen3-235B/Qwen3-235B-A22B \
--dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
--disable-random-sample \
--output-length 1 \
--request-length 2048 \
--num-clients 150 \
--num-rounds 10 \
--max-parallel 4 \
--request-rate 16 \
--ready-queue-policy random \
--disable-auto-run \
--enable-round-barrier</code></pre><ul><li>分组记录：</li></ul><p><img width="723" height="179" referrerpolicy="no-referrer" src="/img/bVdnzzI" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item>  </channel></rss>