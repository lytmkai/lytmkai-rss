<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[Microsoft Office LTSC 2024 for Mac 16.106 - 文档、电子表]]></title>    <link>https://segmentfault.com/a/1190000047608698</link>    <guid>https://segmentfault.com/a/1190000047608698</guid>    <pubDate>2026-02-13 09:01:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Microsoft Office LTSC 2024 for Mac (Microsoft 365) 16.106 - 文档、电子表格、演示文稿和电子邮件</p><p>Office LTSC 2024 for Mac (Word, Excel, PowerPoint, Outlook + OneNote, OneDrive)</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=HS%2BoY9AO5L%2BY6Fvf5ahlcw%3D%3D.PQG2n03mG3jzeeeZbpRNj%2FeztbNmtY5O%2BP2fUsl9u7eehG5xm7LxdEe%2BfUILKBTD" rel="nofollow" target="_blank">https://sysin.org/blog/office-2024-for-mac/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=hTjFlERie53sy0mRFSbvAw%3D%3D.F3PqT3lKJFUAOk%2B9lWTcBEuEck67wrTsOeeUwMF3GNk%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>Office for Mac 2024 2026 年 2 月份月度更新来袭！</p><h2>Office for Mac 2024 组件和发行版</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608476" alt="Office LTSC" title="Office LTSC"/></p><p>宣布推出适用于 Windows 和 Mac 的 Microsoft Office LTSC 2024 正式版</p><p>2024 年 9 月 16 日，Microsoft Office LTSC 2024 的正式版现已适用于 Windows 和 Mac。</p><p>如果您有 Office 2016、Office 2019 或 Office LTSC 2021 并且想要预览 Office LTSC 2024，请访问<a href="https://link.segmentfault.com/?enc=%2B9gZ5YHQQT%2FCDrGXHEpkeQ%3D%3D.B8CwkO%2Foq7Q7B%2FSmax9mS6q6eGZ7UX8jLHtvhmeQwKE1K2b1xkEQKZZxeW7xLvoQ" rel="nofollow" target="_blank">安装 Office LTSC 正式版</a>，了解如何在 Windows 设备上安装和使用。</p><p>Office for Mac 包含以下组件：</p><ul><li>Microsoft <strong>Excel</strong>：电子表格和数据分析</li><li>Microsoft <strong>Outlook</strong>：电子邮件和日历</li><li>Microsoft <strong>PowerPoint</strong>：创建吸引人的演示文稿</li><li>Microsoft <strong>Word</strong>：创建、编辑和分享文档</li><li>Microsoft <strong>OneNote</strong>：记录笔记、创意和备忘录</li></ul><p>Office for Mac 有以下两种发行版（详见下文描述）：</p><ul><li>Office for Mac (Office 365) pkg</li><li>Office LTSC for Mac DMG VL</li></ul><h2>Office for Mac 2024 (Microsoft 365) pkg</h2><p>⚠️：<strong>请慎用此版本，需要 root 权限才能运行，安装一堆无用文件，强制自动更新。</strong></p><p>参看：<a href="https://link.segmentfault.com/?enc=tln1mXPp3MAeUZA1SlBRJA%3D%3D.miHvDgBEmxI8VrsHV%2Bbttz5aq%2FrwOoPlYOEJwLxeYA0MTxzL%2FSXelfzicudvszqZQ13eVnok5V%2BnVNlF8WjzcQ%3D%3D" rel="nofollow" target="_blank">如何卸载 Office for Mac</a></p><p>此版本的唯一优点是开放下载，各大网站通常提供的也是此版本。</p><p><strong>Microsoft Office for Mac 2024 (Office 365) 16.106 Universal</strong>（2026-01-13）</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=Aa5f%2FMsn7vKZy5msfuea7w%3D%3D.W6OCXINk4jLkdVnNunhxwAVBmmPHJaTjH9HyFnM02sMTpf4RydCFaZwX5o3f18aq" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li><li>系统要求：从 16.101 开始，要求 macOS Sonoma 14.0 及以上版本。</li></ul><p>从 16.84 开始，Office 2024 和 Office 2022 是共用安装文件 (sysin)，通过许可证激活不同的版本，主要体现在界面风格上有较为明显差异，另外 2024 版有一些新增功能。</p><p>Office 365 (现在称为 Microsoft 365 Apps) 是一种订阅模式，永久许可版即 Office LTSC for Mac。</p><h2>Office LTSC 2024 for Mac DMG VL</h2><p>该产品符合 Apple 平台设计规范，无需 root 权限安装，只需要拖拽到应用程序下即可，无需登录，没有自动更新程序，也不会提示过期。</p><ul><li>无需 root 权限，拖拽即可安装</li><li>无需登录账号（无需注册，支持离线使用）</li><li>无自动更新程序</li><li>不会提示过期</li><li>可以仅安装单个组件</li></ul><p>包含 Excel、Outlook、PowerPoint 和 Word 四个核心组件，可独立运行单个组件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608477" alt="Microsoft Excel" title="Microsoft Excel" loading="lazy"/></p><p>Microsoft <strong>Excel</strong>：电子表格和数据分析</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608478" alt="Microsoft Outlook" title="Microsoft Outlook" loading="lazy"/></p><p>Microsoft <strong>Outlook</strong>：电子邮件和日历</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608479" alt="Microsoft PowerPoint" title="Microsoft PowerPoint" loading="lazy"/></p><p>Microsoft <strong>PowerPoint</strong>：创建吸引人的演示文稿</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608480" alt="Microsoft Outlook" title="Microsoft Outlook" loading="lazy"/></p><p>Microsoft <strong>Word</strong>：创建、编辑和分享文档</p><p>备注：OneNote 免费，需要登录。</p><p><strong>Microsoft Office LTSC 2024 for Mac DMG VL v16.89 (Final version)</strong> for macOS Montery 12</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=cG9JYvzqTeJ5wk9zNwQwug%3D%3D.%2FKQejFLrzvDQdWPnBV%2BITyEWx7Q%2FqEEimPivtteQD%2FV33nQ1DRIMYfr1KbAGrQxF" rel="nofollow" target="_blank">https://sysin.org/blog/office-2024-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC 2024 for Mac DMG VL v16.100 (Final version)</strong> for macOS Ventura 13</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=T7aduepF8abL8f2O9JVeMg%3D%3D.QBPdZ605v3gJTval7B4OF8ljlFIBw8NrwXk0Rov%2BQ44CqI%2FfjqeI%2F1RaJfBcag29" rel="nofollow" target="_blank">https://sysin.org/blog/office-2024-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC 2024 for Mac DMG VL v16.106</strong> for macOS Sonoma 14 or later</p><ul><li>支持 macOS Sonoma 14、macOS Sequoia 15 和 macOS Tahoe 26</li><li>请访问：<a href="https://link.segmentfault.com/?enc=3Cze20pXPKf6E9Uj%2F7BdLA%3D%3D.McQKgGj8U%2BU8cjQ3erK9eRPi5KmqooYe39U47vUymKaOwBXhmDoHtwJVeiAL2cyD" rel="nofollow" target="_blank">https://sysin.org/blog/office-2024-for-mac/</a></li></ul><hr/><p>更多：<a href="https://link.segmentfault.com/?enc=aIFQLnheHtSZ3M7UxFrZFw%3D%3D.36maCiuGq66dbxztGJUc8eJ%2FqPxRHHAo14C2INjbJUU%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[LockSupport深度解析：线程阻塞与唤醒的底层实现原理 SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047598590</link>    <guid>https://segmentfault.com/a/1190000047598590</guid>    <pubDate>2026-02-13 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>LockSupport简介</h2><p>LockSupprot 用来阻塞和唤醒线程，底层实现依赖于 <a href="https://link.segmentfault.com/?enc=2jYChfnH9ZWiDDMFfuabdw%3D%3D.iThJ3ygEeLy5ommch6CuKm1B6%2FnXyYPaTWUBgfcLwbZgLB4u3vRHrLReS0ysgNJ3LPiD%2BRG13ADe7%2BA4djCeNA%3D%3D" rel="nofollow" target="_blank">Unsafe 类</a>。</p><p>LockSupport用来创建锁和其他同步类的基本线程阻塞原语。简而言之，当调用LockSupport.park时，表示当前线程将会等待，直至获得许可，当调用LockSupport.unpark时，必须把等待获得许可的线程作为参数进行传递，好让此线程继续运行。在AQS中大量使用，AQS最终都是使用LockSupport来阻塞线程的。</p><p>该类包含一组用于阻塞和唤醒线程的静态方法，这些方法主要是围绕 park 和 unpark 展开，话不多说，直接来看一个简单的例子吧。</p><pre><code class="java">public class LockSupportDemo1 {
    public static void main(String[] args) {
        Thread mainThread = Thread.currentThread();

        // 创建一个线程从1数到1000
        Thread counterThread = new Thread(() -&gt; {
            for (int i = 1; i &lt;= 1000; i++) {
                System.out.println(i);
                if (i == 500) {
                    // 当数到500时，唤醒主线程
                    LockSupport.unpark(mainThread);
                }
            }
        });

        counterThread.start();

        // 主线程调用park
        LockSupport.park();
        System.out.println("Main thread was unparked.");
    }
}</code></pre><p>上面的代码中，当 counterThread 数到 500 时，它会唤醒 mainThread。而 mainThread 在调用 park 方法时会被阻塞，直到被 unpark。</p><p>LockSupport 中的方法不多，这里将这些方法做一个总结：</p><h3>阻塞线程</h3><ol><li><code>void park()</code>：阻塞当前线程，如果调用 unpark 方法或线程被中断，则该线程将变得可运行。请注意，park 不会抛出 InterruptedException，因此线程必须单独检查其中断状态。</li><li><code>void park(Object blocker)</code>：功能同方法 1，入参增加一个 Object 对象，用来记录导致线程阻塞的对象，方便问题排查。</li><li><code>void parkNanos(long nanos)</code>：阻塞当前线程一定的纳秒时间，或直到被 unpark 调用，或线程被中断。</li><li><code>void parkNanos(Object blocker, long nanos)</code>：功能同方法 3，入参增加一个 Object 对象，用来记录导致线程阻塞的对象，方便问题排查。</li><li><code>void parkUntil(long deadline)</code>：阻塞当前线程直到某个指定的截止时间（以毫秒为单位），或直到被 unpark 调用，或线程被中断。</li><li><code>void parkUntil(Object blocker, long deadline)</code>：功能同方法 5，入参增加一个 Object 对象，用来记录导致线程阻塞的对象，方便问题排查。</li></ol><h3>唤醒线程</h3><p><code>void unpark(Thread thread)</code>：唤醒一个由 park 方法阻塞的线程。如果该线程未被阻塞，那么下一次调用 park 时将立即返回。这允许“先发制人”式的唤醒机制。</p><p>实际上，LockSupport 阻塞和唤醒线程的功能依赖于 <code>sun.misc.Unsafe</code>，这是一个很底层的类，比如 LockSupport 的 park 方法是通过 <code>unsafe.park()</code> 方法实现的。</p><h2>LockSupport源码分析</h2><h3>类的属性</h3><pre><code class="java">public class LockSupport {
    // Hotspot implementation via intrinsics API
    private static final sun.misc.Unsafe UNSAFE;
    // 表示内存偏移地址
    private static final long parkBlockerOffset;
    // 表示内存偏移地址
    private static final long SEED;
    // 表示内存偏移地址
    private static final long PROBE;
    // 表示内存偏移地址
    private static final long SECONDARY;
    
    static {
        try {
            // 获取Unsafe实例
            UNSAFE = sun.misc.Unsafe.getUnsafe();
            // 线程类类型
            Class&lt;?&gt; tk = Thread.class;
            // 获取Thread的parkBlocker字段的内存偏移地址
            parkBlockerOffset = UNSAFE.objectFieldOffset
                (tk.getDeclaredField("parkBlocker"));
            // 获取Thread的threadLocalRandomSeed字段的内存偏移地址
            SEED = UNSAFE.objectFieldOffset
                (tk.getDeclaredField("threadLocalRandomSeed"));
            // 获取Thread的threadLocalRandomProbe字段的内存偏移地址
            PROBE = UNSAFE.objectFieldOffset
                (tk.getDeclaredField("threadLocalRandomProbe"));
            // 获取Thread的threadLocalRandomSecondarySeed字段的内存偏移地址
            SECONDARY = UNSAFE.objectFieldOffset
                (tk.getDeclaredField("threadLocalRandomSecondarySeed"));
        } catch (Exception ex) { throw new Error(ex); }
    }
}</code></pre><p>说明: UNSAFE字段表示<a href="https://link.segmentfault.com/?enc=TeLuQsSYqfvr4NkrAdCKKA%3D%3D.rfQII56OH2a4euquJhQKHm64Rlyxfwm2Ork2KiOvQYr9%2FaMnPeDy9xj82Zps8dqjBlAGh3w20bA8cL9%2BkkMqgA%3D%3D" rel="nofollow" target="_blank">sun.misc.Unsafe</a>类，一般程序中不允许直接调用，而long型的表示实例对象相应字段在内存中的偏移地址，可以通过该偏移地址获取或者设置该字段的值。</p><h3>类的构造函数</h3><pre><code class="java">// 私有构造函数，无法被实例化
private LockSupport() {}</code></pre><p>说明: LockSupport只有一个私有构造函数，无法被实例化。</p><h3>核心函数分析</h3><p>在分析LockSupport函数之前，先引入sun.misc.Unsafe类中的park和unpark函数，因为LockSupport的核心函数都是基于Unsafe类中定义的park和unpark函数，下面给出两个函数的定义:</p><pre><code class="java">public native void park(boolean isAbsolute, long time);
public native void unpark(Thread thread);</code></pre><p>说明: 对两个函数的说明如下:</p><ul><li><p>park函数，阻塞线程，并且该线程在下列情况发生之前都会被阻塞:</p><ul><li>调用unpark函数，释放该线程的许可。</li><li>该线程被中断。</li><li>设置的时间到了。并且，当time为绝对时间时，isAbsolute为true，否则，isAbsolute为false。当time为0时，表示无限等待，直到unpark发生。</li></ul></li><li>unpark函数，释放线程的许可，即激活调用park后阻塞的线程。这个函数不是安全的，调用这个函数时要确保线程依旧存活。</li></ul><h4>park函数</h4><p>park函数有两个重载版本，方法摘要如下</p><pre><code class="java">public static void park()；
public static void park(Object blocker)；</code></pre><p>说明: 两个函数的区别在于park()函数没有没有blocker，即没有设置线程的parkBlocker字段。park(Object)型函数如下。</p><pre><code class="java">public static void park(Object blocker) {
    // 获取当前线程
    Thread t = Thread.currentThread();
    // 设置Blocker
    setBlocker(t, blocker);
    // 获取许可
    UNSAFE.park(false, 0L);
    // 重新可运行后再此设置Blocker
    setBlocker(t, null);
}</code></pre><p>说明: 调用park函数时，首先获取当前线程，然后设置当前线程的parkBlocker字段，即调用setBlocker函数，之后调用Unsafe类的park函数，之后再调用setBlocker函数。那么问题来了，为什么要在此park函数中要调用两次setBlocker函数呢? 原因其实很简单，调用park函数时，当前线程首先设置好parkBlocker字段，然后再调用Unsafe的park函数，此后，当前线程就已经阻塞了，等待该线程的unpark函数被调用，所以后面的一个setBlocker函数无法运行，unpark函数被调用，该线程获得许可后，就可以继续运行了，也就运行第二个setBlocker，把该线程的parkBlocker字段设置为null，这样就完成了整个park函数的逻辑。如果没有第二个setBlocker，那么之后没有调用park(Object blocker)，而直接调用getBlocker函数，得到的还是前一个park(Object blocker)设置的blocker，显然是不符合逻辑的。总之，必须要保证在park(Object blocker)整个函数执行完后，该线程的parkBlocker字段又恢复为null。所以，park(Object)型函数里必须要调用setBlocker函数两次。setBlocker方法如下。</p><pre><code class="java">private static void setBlocker(Thread t, Object arg) {
    // 设置线程t的parkBlocker字段的值为arg
    UNSAFE.putObject(t, parkBlockerOffset, arg);
}</code></pre><p>说明: 此方法用于设置线程t的parkBlocker字段的值为arg。</p><p>另外一个无参重载版本，park()函数如下。</p><pre><code class="java">public static void park() {
    // 获取许可，设置时间为无限长，直到可以获取许可
    UNSAFE.park(false, 0L);
}</code></pre><p>说明: 调用了park函数后，会禁用当前线程，除非许可可用。在以下三种情况之一发生之前，当前线程都将处于休眠状态，即下列情况发生时，当前线程会获取许可，可以继续运行。</p><ul><li>其他某个线程将当前线程作为目标调用 unpark。</li><li>其他某个线程中断当前线程。</li><li>该调用不合逻辑地(即毫无理由地)返回。</li></ul><h4>parkNanos函数</h4><p>此函数表示在许可可用前禁用当前线程，并最多等待指定的等待时间。具体函数如下。</p><pre><code class="java">public static void parkNanos(Object blocker, long nanos) {
    if (nanos &gt; 0) { // 时间大于0
        // 获取当前线程
        Thread t = Thread.currentThread();
        // 设置Blocker
        setBlocker(t, blocker);
        // 获取许可，并设置了时间
        UNSAFE.park(false, nanos);
        // 设置许可
        setBlocker(t, null);
    }
}</code></pre><p>说明: 该函数也是调用了两次setBlocker函数，nanos参数表示相对时间，表示等待多长时间。</p><h4>parkUntil函数</h4><p>此函数表示在指定的时限前禁用当前线程，除非许可可用, 具体函数如下:</p><pre><code class="java">public static void parkUntil(Object blocker, long deadline) {
    // 获取当前线程
    Thread t = Thread.currentThread();
    // 设置Blocker
    setBlocker(t, blocker);
    UNSAFE.park(true, deadline);
    // 设置Blocker为null
    setBlocker(t, null);
}</code></pre><p>说明: 该函数也调用了两次setBlocker函数，deadline参数表示绝对时间，表示指定的时间。</p><h4>unpark函数</h4><p>此函数表示如果给定线程的许可尚不可用，则使其可用。如果线程在 park 上受阻塞，则它将解除其阻塞状态。否则，保证下一次调用 park 不会受阻塞。如果给定线程尚未启动，则无法保证此操作有任何效果。具体函数如下:</p><pre><code class="java">public static void unpark(Thread thread) {
    if (thread != null) // 线程为不空
        UNSAFE.unpark(thread); // 释放该线程许可
}</code></pre><p>说明: 释放许可，指定线程可以继续运行。</p><h2>更深入的理解</h2><h3>与 synchronzed 的区别</h3><p><a href="https://link.segmentfault.com/?enc=jo3YUxDP9f05SJ%2B15isffA%3D%3D.kWBILxLBAbRfHmK0EBJKAVlwmJ51kCRfGUQ%2BLPrRjGlL3tYjrnfYERVI7hOaXM1wy7rGfAHk7YcztPdSICOItmxZOUurB4Is5DMcvymOr%2Fs%3D" rel="nofollow" target="_blank">synchronzed</a> 会使线程阻塞，线程会进入 BLOCKED 状态，而调用 LockSupprt 方法阻塞线程会使线程进入到 WAITING 状态。</p><h3>Thread.sleep()和Object.wait()的区别</h3><p>首先，我们先来看看Thread.sleep()和Object.wait()的区别，这是一个烂大街的题目了，大家应该都能说上来两点。</p><ul><li>Thread.sleep()不会释放占有的锁，Object.wait()会释放占有的锁；</li><li>Thread.sleep()必须传入时间，Object.wait()可传可不传，不传表示一直阻塞下去；</li><li>Thread.sleep()到时间了会自动唤醒，然后继续执行；</li><li>Object.wait()不带时间的，需要另一个线程使用Object.notify()唤醒；</li><li>Object.wait()带时间的，假如没有被notify，到时间了会自动唤醒，这时又分好两种情况，一是立即获取到了锁，线程自然会继续执行；二是没有立即获取锁，线程进入同步队列等待获取锁；</li></ul><p>其实，他们俩最大的区别就是Thread.sleep()不会释放锁资源，Object.wait()会释放锁资源。</p><h3>Object.wait()和Condition.await()的区别</h3><p>Object.wait()和Condition.await()的原理是基本一致的，不同的是Condition.await()底层是调用LockSupport.park()来实现阻塞当前线程的。</p><p>实际上，它在阻塞当前线程之前还干了两件事，一是把当前线程添加到条件队列中，二是“完全”释放锁，也就是让state状态变量变为0，然后才是调用LockSupport.park()阻塞当前线程。</p><h3>Thread.sleep()和LockSupport.park()的区别</h3><p>LockSupport.park()还有几个兄弟方法——parkNanos()、parkUtil()等，我们这里说的park()方法统称这一类方法。</p><ul><li>从功能上来说，Thread.sleep()和LockSupport.park()方法类似，都是阻塞当前线程的执行，且都不会释放当前线程占有的锁资源；</li><li>Thread.sleep()没法从外部唤醒，只能自己醒过来；</li><li>LockSupport.park()方法可以被另一个线程调用LockSupport.unpark()方法唤醒；</li><li>Thread.sleep()方法声明上抛出了InterruptedException中断异常，所以调用者需要捕获这个异常或者再抛出；</li><li>LockSupport.park()方法不需要捕获中断异常；</li><li>Thread.sleep()本身就是一个native方法；</li><li>LockSupport.park()底层是调用的Unsafe的native方法；</li></ul><h3>Object.wait()和LockSupport.park()的区别</h3><p>二者都会阻塞当前线程的运行，他们有什么区别呢? 经过上面的分析相信你一定很清楚了，真的吗? 往下看！</p><ul><li>Object.wait()方法需要在synchronized块中执行；</li><li>LockSupport.park()可以在任意地方执行；</li><li>Object.wait()方法声明抛出了中断异常，调用者需要捕获或者再抛出；</li><li>LockSupport.park()不需要捕获中断异常；</li><li>Object.wait()不带超时的，需要另一个线程执行notify()来唤醒，但不一定继续执行后续内容；</li><li>LockSupport.park()不带超时的，需要另一个线程执行unpark()来唤醒，一定会继续执行后续内容；</li></ul><p>park()/unpark()底层的原理是“二元信号量”，你可以把它相像成只有一个许可证的Semaphore，只不过这个信号量在重复执行unpark()的时候也不会再增加许可证，最多只有一个许可证。</p><h3>如果在wait()之前执行了notify()会怎样?</h3><p>如果当前的线程不是此对象锁的所有者，却调用该对象的notify()或wait()方法时抛出IllegalMonitorStateException异常；</p><p>如果当前线程是此对象锁的所有者，wait()将一直阻塞，因为后续将没有其它notify()唤醒它。</p><h3>如果在park()之前执行了unpark()会怎样?</h3><p>线程不会被阻塞，直接跳过park()，继续执行后续内容</p><h3>LockSupport.park()会释放锁资源吗?</h3><p>不会，它只负责阻塞当前线程，释放锁资源实际上是在Condition的await()方法中实现的。</p>]]></description></item><item>    <title><![CDATA[蓝易云cdn:在conda中如何查看安装的pytorch版本 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047608564</link>    <guid>https://segmentfault.com/a/1190000047608564</guid>    <pubDate>2026-02-13 00:02:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>蓝易云CDN：在 Conda 中如何查看安装的 PyTorch 版本 🧠</h2><p>在多环境开发场景中，确认 PyTorch 版本至关重要，尤其涉及 CUDA 兼容性与模型推理稳定性。以下提供&lt;span style="color:red"&gt;标准且可靠的三种方法&lt;/span&gt;，适用于当前主流 Conda 环境管理方式。</p><hr/><h2>一、方法一：使用 conda list 查询（推荐）🚀</h2><h3>1️⃣ 先激活目标环境</h3><pre><code class="bash">conda activate 环境名</code></pre><h4>命令解释：</h4><table><thead><tr><th>组件</th><th>作用</th></tr></thead><tbody><tr><td>conda</td><td>Conda 包管理工具</td></tr><tr><td>activate</td><td>激活指定虚拟环境</td></tr><tr><td>环境名</td><td>例如 pytorch_env</td></tr></tbody></table><p>如果不激活环境，查询到的是 base 环境内容。</p><hr/><h3>2️⃣ 查询 PyTorch 版本</h3><pre><code class="bash">conda list pytorch</code></pre><h4>命令解释：</h4><table><thead><tr><th>组件</th><th>作用</th></tr></thead><tbody><tr><td>conda list</td><td>查看当前环境已安装包</td></tr><tr><td>pytorch</td><td>过滤指定包</td></tr></tbody></table><p>示例输出：</p><pre><code>pytorch 2.2.1 py3.10_cuda11.8_cudnn8.7</code></pre><p>说明：</p><ul><li>版本号：2.2.1</li><li>Python版本：3.10</li><li>CUDA版本：11.8</li></ul><p>&lt;span style="color:red"&gt;这是最标准且准确的查询方式&lt;/span&gt;。</p><hr/><h2>二、方法二：通过 Python 内部查看 🐍</h2><p>在激活环境后执行：</p><pre><code class="bash">python</code></pre><p>进入交互模式后输入：</p><pre><code class="python">import torch
print(torch.__version__)</code></pre><h4>代码解释：</h4><table><thead><tr><th>代码</th><th>说明</th></tr></thead><tbody><tr><td>import torch</td><td>导入 PyTorch 模块</td></tr><tr><td>torch.<strong>version</strong></td><td>输出当前版本</td></tr></tbody></table><p>若输出：</p><pre><code>2.2.1+cu118</code></pre><p>表示：</p><ul><li>主版本 2.2.1</li><li>CUDA 11.8</li></ul><p>若无 CUDA 后缀，说明为 CPU 版本。</p><hr/><h2>三、方法三：查看所有相关组件版本 ⚙️</h2><pre><code class="bash">conda list | grep torch</code></pre><p>作用：</p><ul><li>同时查看 torchvision</li><li>torchtext</li><li>torchaudio</li></ul><p>示例：</p><pre><code>torchvision 0.17.1
torchaudio 2.2.1</code></pre><hr/><h2>四、完整判断流程图 🧩</h2><pre style="display:none;"><code class="mermaid">graph TD
A[激活环境] --&gt; B[conda list pytorch]
B --&gt; C{是否显示版本}
C --&gt;|是| D[确认CUDA版本]
C --&gt;|否| E[检查是否安装]
D --&gt; F[验证 python import torch]</code></pre><hr/><h2>五、版本与CUDA匹配原则 📊</h2><table><thead><tr><th>PyTorch版本</th><th>推荐CUDA版本</th></tr></thead><tbody><tr><td>2.2.x</td><td>11.8 / 12.1</td></tr><tr><td>2.1.x</td><td>11.8</td></tr><tr><td>1.13.x</td><td>11.6</td></tr></tbody></table><p>⚠️ 注意：</p><p>&lt;span style="color:red"&gt;PyTorch 的 CUDA 版本必须与系统驱动兼容&lt;/span&gt;。</p><p>可检查驱动：</p><pre><code class="bash">nvidia-smi</code></pre><p>解释：</p><ul><li>查看 GPU 驱动版本</li><li>判断是否支持当前 CUDA</li></ul><hr/><h2>六、核心总结 🎯</h2><p>在 Conda 中查看 PyTorch 版本的关键步骤：</p><ol><li>&lt;span style="color:red"&gt;激活正确环境&lt;/span&gt;</li><li>使用 &lt;span style="color:red"&gt;conda list pytorch&lt;/span&gt;</li><li>通过 &lt;span style="color:red"&gt;python + torch.<strong>version</strong> 双重验证&lt;/span&gt;</li></ol><p>开发中最常见错误不是版本太低，而是：</p><ul><li>查错环境</li><li>CUDA 与驱动不匹配</li></ul><p>严谨的做法永远是：<br/>环境确认 → 版本确认 → CUDA确认 → 驱动确认。</p><p>并发算力与框架稳定性，本质来自版本匹配，而不是版本最新。 🔍</p>]]></description></item><item>    <title><![CDATA[蓝易云cdn:dockers搭建基本服务 蓝易云 ]]></title>    <link>https://segmentfault.com/a/1190000047608566</link>    <guid>https://segmentfault.com/a/1190000047608566</guid>    <pubDate>2026-02-13 00:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>蓝易云CDN：Docker 搭建基本服务完整指南 🚀</h2><p>在现代运维体系中，Docker 已成为部署基础服务的标准工具。其核心优势是：&lt;span style="color:red"&gt;环境隔离、快速部署、可移植性强&lt;/span&gt;。下面给出标准化搭建流程与示例。</p><hr/><h2>一、安装 Docker（以 Ubuntu 为例）⚙️</h2><h3>1️⃣ 更新系统</h3><pre><code class="bash">sudo apt update</code></pre><p>解释：</p><ul><li><code>apt update</code>：同步软件仓库索引</li><li>确保后续安装使用最新包信息</li></ul><hr/><h3>2️⃣ 安装 Docker</h3><pre><code class="bash">sudo apt install -y docker.io</code></pre><p>解释：</p><ul><li><code>docker.io</code>：Ubuntu 官方仓库中的 Docker 引擎</li><li><code>-y</code>：自动确认安装</li></ul><hr/><h3>3️⃣ 启动并设置开机自启</h3><pre><code class="bash">sudo systemctl enable docker
sudo systemctl start docker</code></pre><p>解释：</p><table><thead><tr><th>命令</th><th>作用</th></tr></thead><tbody><tr><td>enable</td><td>开机自动启动</td></tr><tr><td>start</td><td>立即启动服务</td></tr></tbody></table><p>验证：</p><pre><code class="bash">docker --version</code></pre><p>若显示版本号说明安装成功。</p><hr/><h2>二、Docker 基本概念说明 🧠</h2><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td>Image</td><td>镜像，运行模板</td></tr><tr><td>Container</td><td>容器，运行实例</td></tr><tr><td>Volume</td><td>数据卷，持久存储</td></tr><tr><td>Network</td><td>网络模式</td></tr></tbody></table><p>简单模型公式：</p><pre><code>容器 = 镜像 + 运行参数 + 资源限制</code></pre><hr/><h2>三、搭建 Nginx 服务示例 🌐</h2><h3>1️⃣ 拉取镜像</h3><pre><code class="bash">docker pull nginx</code></pre><p>解释：</p><ul><li>从官方镜像仓库下载 nginx 镜像</li></ul><hr/><h3>2️⃣ 运行容器</h3><pre><code class="bash">docker run -d -p 80:80 --name mynginx nginx</code></pre><p>解释：</p><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>-d</td><td>后台运行</td></tr><tr><td>-p 80:80</td><td>宿主机80映射容器80</td></tr><tr><td>--name</td><td>容器名称</td></tr><tr><td>nginx</td><td>使用的镜像</td></tr></tbody></table><p>访问服务器IP即可看到默认页面。</p><hr/><h2>四、搭建 MySQL 服务示例 🗄</h2><pre><code class="bash">docker run -d \
-p 3306:3306 \
--name mysql8 \
-e MYSQL_ROOT_PASSWORD=123456 \
-v /data/mysql:/var/lib/mysql \
mysql:8</code></pre><p>解释：</p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>-e</td><td>设置环境变量</td></tr><tr><td>MYSQL_ROOT_PASSWORD</td><td>root密码</td></tr><tr><td>-v</td><td>数据持久化映射</td></tr><tr><td>mysql:8</td><td>指定版本</td></tr></tbody></table><p>&lt;span style="color:red"&gt;数据必须挂载数据卷，否则删除容器数据会丢失&lt;/span&gt;。</p><hr/><h2>五、推荐使用 docker-compose 管理多服务 🧩</h2><p>创建 <code>docker-compose.yml</code>：</p><pre><code class="yaml">version: '3'
services:
  nginx:
    image: nginx
    ports:
      - "80:80"
  mysql:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: 123456
    volumes:
      - ./mysql:/var/lib/mysql</code></pre><p>启动：</p><pre><code class="bash">docker compose up -d</code></pre><p>解释：</p><ul><li><code>compose up</code>：创建并启动服务</li><li><code>-d</code>：后台运行</li></ul><hr/><h2>六、部署流程图 📊</h2><pre style="display:none;"><code class="mermaid">graph TD
A[安装Docker] --&gt; B[拉取镜像]
B --&gt; C[运行容器]
C --&gt; D[配置端口映射]
D --&gt; E[配置数据卷]
E --&gt; F[服务上线]</code></pre><hr/><h2>七、基础服务推荐结构 🔐</h2><table><thead><tr><th>服务类型</th><th>建议部署方式</th></tr></thead><tbody><tr><td>Web服务</td><td>Nginx容器</td></tr><tr><td>数据库</td><td>独立容器 + 数据卷</td></tr><tr><td>缓存</td><td>Redis容器</td></tr><tr><td>API</td><td>应用镜像</td></tr></tbody></table><hr/><h2>八、生产环境注意事项 ⚠️</h2><ol><li>&lt;span style="color:red"&gt;不要使用 latest 标签&lt;/span&gt;</li><li>必须挂载数据卷</li><li>限制容器资源：</li></ol><pre><code class="bash">--memory="1g" --cpus="1.0"</code></pre><p>解释：</p><ul><li>限制容器最大内存</li><li>限制CPU核心使用率</li></ul><hr/><h2>九、总结 🎯</h2><p>Docker 搭建基本服务的核心逻辑是：</p><ul><li>安装引擎</li><li>拉取镜像</li><li>配置端口</li><li>持久化数据</li><li>合理限制资源</li></ul><p>容器不是虚拟机，而是进程级隔离。<br/>真正的稳定部署来自：</p><p>&lt;span style="color:red"&gt;版本固定 + 数据持久化 + 资源限制&lt;/span&gt;。</p><p>基础架构稳，业务才稳。 🚀</p>]]></description></item><item>    <title><![CDATA[【MATLAB源码】6G：XL-MIMO 混合场信道估计仿真平台 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047608372</link>    <guid>https://segmentfault.com/a/1190000047608372</guid>    <pubDate>2026-02-12 22:05:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>&lt;p align="center"&gt;<br/>  &lt;h1 align="center"&gt;Hybrid-Field XL-MIMO 混合场信道估计仿真平台&lt;/h1&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;strong&gt;完整的混合场信道估计实现：建模 → 网格内恢复 → 离网细化 → 结果可视化&lt;/strong&gt;</code></pre><p>&lt;/p&gt;<br/>&lt;/p&gt;</p><hr/><h2>🚀 为什么选择本仿真平台？</h2><table><thead><tr><th align="left">痛点</th><th align="left">本平台解决方案</th></tr></thead><tbody><tr><td align="left">XL-MIMO 远近场共存，建模容易失配</td><td align="left">✅ <strong>远场 DFT + 近场极域联合字典</strong>，统一建模混合传播机理</td></tr><tr><td align="left">远场占比 ($\gamma$) 先验难以准确给定</td><td align="left">✅ 提供 <strong>无 ($\gamma$) 比例搜索</strong>，自动完成远近场路径分配</td></tr><tr><td align="left">离网优化容易震荡或发散</td><td align="left">✅ <code>SIGW</code> 内置 <strong>单调下降 + 回溯线搜索 + 坐标回退 + 岭正则</strong> 稳定机制</td></tr><tr><td align="left">只看均值曲线难以评估稳健性</td><td align="left">✅ 内置 <strong>CDF / Pareto / 相图 / 支撑图</strong> 四类强相关演示</td></tr><tr><td align="left">复现实验路径分散</td><td align="left">✅ 提供 <code>main_all_experiments</code> 与图集脚本，支持一键复现</td></tr></tbody></table><hr/><h2>🌟 核心价值</h2><table>
<tr>
<td width="50%">

### 📘 学术研究价值

- 混合场（远场+近场）统一信道建模
- 无先验 (\gamma) 的支撑搜索机制验证
- 网格内估计与离网细化协同流程完整复现
- 精度、复杂度、运行时间三维对比评估

</td>
<td width="50%">

### 🛠️ 工程应用价值

- 单天线与多天线两套实验链路
- 快速模式与完整模式双配置
- 自动保存图像与结果数据（不带日期命名）
- 中文详细注释，便于二次开发与教学演示

</td>
</tr>
</table><hr/><h2>⚡ 技术亮点</h2><h3>1) Hybrid-Field 估计系统架构</h3><pre><code class="text">┌───────────────────────────────────────────────────────────────────────────────┐
│                Hybrid-Field XL-MIMO 信道估计与可视化链路                     │
├───────────────────────────────────────────────────────────────────────────────┤
│                                                                               │
│  混合场信道生成 ──► 加噪观测 y ──► 联合字典构建 D=[Af, An]                   │
│      │                  │                   │                                  │
│  Far/Near/LoS       SNR 控制           远场 DFT + 近场极域                    │
│                                                                               │
│                 ┌──────────── 网格内恢复（On-grid）────────────┐              │
│                 │  Hybrid OMP / Hybrid SGP / 无γ比例搜索        │              │
│                 └────────────────────────────────────────────────┘              │
│                                       │                                       │
│                                       ▼                                       │
│                 ┌──────────── 离网细化（Off-grid SIGW）──────────┐            │
│                 │  数值梯度 + 回溯线搜索 + 坐标回退 + 岭回归      │            │
│                 └────────────────────────────────────────────────┘            │
│                                       │                                       │
│                                       ▼                                       │
│                 NMSE / SE / 复杂度 / CDF / Pareto / 相图 / 支撑图            │
└───────────────────────────────────────────────────────────────────────────────┘</code></pre><h3>2) 性能指标（本地 quick 配置实测，2026-02-12）</h3><table><thead><tr><th align="left">场景</th><th align="left">指标定义</th><th align="left">实测结果</th></tr></thead><tbody><tr><td align="left">单天线 SNR=0 dB</td><td align="left"><code>HF-SGP(no-γ)</code> 对比 <code>Off-grid HF-SGP(no-γ)</code></td><td align="left"><strong>+3.79 dB</strong> NMSE 增益（-4.16 dB → -7.95 dB）</td></tr><tr><td align="left">单天线 SNR=4 dB</td><td align="left"><code>HF-SGP(no-γ)</code> 对比 <code>Off-grid HF-SGP(no-γ)</code></td><td align="left"><strong>+6.01 dB</strong> NMSE 增益（-3.66 dB → -9.67 dB）</td></tr><tr><td align="left"><code>demo_polar_support_map</code></td><td align="left">单样本离网收益</td><td align="left"><strong>+3.95 dB</strong></td></tr><tr><td align="left"><code>demo_sigw_convergence</code></td><td align="left">单样本离网收益</td><td align="left"><strong>+4.39 dB</strong></td></tr><tr><td align="left"><code>demo_snr_gamma_phase_map</code></td><td align="left">离网增益为正的网格占比</td><td align="left"><strong>100%</strong></td></tr><tr><td align="left"><code>demo_nmse_cdf_pareto</code></td><td align="left">Off-grid 相对 HF-SGP 的均值收益/时延</td><td align="left"><strong>+1.56 dB</strong> / <strong>+21.04 ms</strong></td></tr></tbody></table><blockquote>📌 说明：以上数据来自项目当前代码在本机快速配置下的直接运行结果，用于展示方法趋势与工程可复现性。</blockquote><hr/><h2>🖥️ 运行环境</h2><h3>最低要求</h3><table><thead><tr><th align="left">项目</th><th align="left">要求</th></tr></thead><tbody><tr><td align="left"><strong>MATLAB 版本</strong></td><td align="left">R2021b 或更高</td></tr><tr><td align="left"><strong>必需工具箱</strong></td><td align="left">基础 MATLAB 即可（推荐安装常用信号处理相关工具箱）</td></tr><tr><td align="left"><strong>操作系统</strong></td><td align="left">Windows 10/11、macOS、Linux</td></tr><tr><td align="left"><strong>内存</strong></td><td align="left">8 GB+（大规模 Monte-Carlo 建议 16 GB+）</td></tr></tbody></table><h3>快速验证</h3><pre><code class="matlab">% 进入项目根目录后
run_smoke_test

% 一键运行强相关图集
run_related_figure_gallery(true)</code></pre><hr/><h2>📐 算法原理（项目对应版）</h2><h3>1) 混合场信道模型</h3><p>$$
\mathbf{h}=\sum_{\ell=1}^{L_f} \beta_{f,\ell}\,\mathbf{a}_f(\theta_{f,\ell})
+\sum_{\ell=1}^{L_n} \beta_{n,\ell}\,\mathbf{a}_n(r_{\ell},\theta_{n,\ell})
+\mathbf{h}_{\mathrm{LoS}}.
$$</p><h3>2) 联合字典建模</h3><p>$$
\mathbf{D}=[\mathbf{A}_f,\mathbf{A}_n],
\qquad
\min_{\mathbf{g}}\|\mathbf{y}-\mathbf{D}\mathbf{g}\|_2^2
\;\text{s.t.}\;\|\mathbf{g}\|_0\le K.
$$</p><h3>3) 无 ($\gamma$) 比例搜索</h3><p>$$
\hat{\gamma}
=\arg\min_{\gamma\in\Gamma}
\left(
\min_{\mathbf{g}:\operatorname{supp}(\mathbf{g})\in\mathcal{S}(\gamma)}
\|\mathbf{y}-\mathbf{D}\mathbf{g}\|_2^2
\right),
\quad
\Gamma=\left\{\frac{L-1}{L},\frac{L-2}{L},\dots,0\right\}.
$$</p><h3>4) SIGW 离网细化目标</h3><p>$$
J(\Theta,\mathbf{g})=\|\mathbf{y}-\mathbf{A}(\Theta)\mathbf{g}\|_2^2 + \lambda\|\mathbf{g}\|_2^2,
$$</p><p>$$
\mathbf{g}^*(\Theta)=\left(\mathbf{A}^H\mathbf{A}+\lambda\mathbf{I}\right)^{-1}\mathbf{A}^H\mathbf{y}.
$$</p><p>通过“回溯线搜索 + 坐标回退”保证优化过程稳定，缓解高 SNR 区域的网格失配误差。</p><hr/><h2>📁 项目结构</h2><pre><code class="text">hmimo ce/
├── main_all_experiments.m             # 一键总入口（主实验+演示）
│
├── src/
│   ├── common/                        # 配置、字典、信道、流形、路径、存图
│   │   ├── hf_default_config.m
│   │   ├── hf_build_dictionaries_single.m
│   │   ├── hf_build_dictionaries_multi.m
│   │   ├── hf_qua_codebook.m
│   │   ├── hf_generate_hybrid_channel_single.m
│   │   ├── hf_generate_hybrid_channel_multi.m
│   │   └── ...
│   │
│   ├── estimators/                    # OMP / SGP / Hybrid / SIGW
│   │   ├── hf_hybrid_omp.m
│   │   ├── hf_hybrid_omp_nogamma.m
│   │   ├── hf_hybrid_sgp.m
│   │   ├── hf_hybrid_sgp_nogamma.m
│   │   ├── hf_sigw_single.m
│   │   ├── hf_sigw_multi.m
│   │   └── ...
│   │
│   └── metrics/                       # NMSE / SE / 复杂度
│       ├── hf_compute_complexity.m
│       └── hf_compute_se_mr.m
│
├── experiments/                       # 主实验脚本
│   ├── run_single_snr_experiment.m
│   ├── run_multi_snr_experiment.m
│   ├── run_multi_se_experiment.m
│   ├── run_complexity_experiment.m
│   └── ...
│
├── demos/                             # 强相关演示图
│   ├── demo_polar_support_map.m
│   ├── demo_sigw_convergence.m
│   ├── demo_nmse_cdf_pareto.m
│   ├── demo_snr_gamma_phase_map.m
│   └── run_related_figure_gallery.m
│
├── docs/
│   ├── 算法文档.md
│   ├── 代码文档.md
│
└── results/
    ├── data/                          # .mat 结果文件
    └── figures/                       # 自动保存图像（无日期命名）</code></pre><p><strong>代码统计（当前工程）</strong>：</p><ul><li><code>40</code> 个 <code>.m</code> 文件</li><li>约 <code>4085</code> 行 MATLAB 代码</li><li>核心模块全部中文详细注释</li></ul><hr/><h2>🧪 仿真演示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608374" alt="multi_se_vs_snr.png" title="multi_se_vs_snr.png"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608375" alt="single_nmse_vs_paths.png" title="single_nmse_vs_paths.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608376" alt="single_nmse_vs_snr.png" title="single_nmse_vs_snr.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608377" alt="demo_nmse_cdf.png" title="demo_nmse_cdf.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608378" alt="demo_pareto_tradeoff.png" title="demo_pareto_tradeoff.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608379" alt="demo_polar_support_dual.png" title="demo_polar_support_dual.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608380" alt="demo_far_support_compare.png" title="demo_far_support_compare.png" loading="lazy"/></p><hr/><h2>✅ 您将获得</h2><table><thead><tr><th align="left">内容</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><strong>完整混合场源码</strong></td><td align="left">远场/近场字典、信道生成、OMP/SGP、离网细化全覆盖</td></tr><tr><td align="left"><strong>双层文档体系</strong></td><td align="left"><code>算法文档.md/.docx</code> + <code>代码文档.md</code> + 本 <code>项目文档.md</code></td></tr><tr><td align="left"><strong>强相关演示图集</strong></td><td align="left">支撑图、收敛图、CDF/Pareto、SNR-γ 相图</td></tr><tr><td align="left"><strong>可复现实验脚本</strong></td><td align="left">单天线、多天线、SE、复杂度、一键总入口</td></tr><tr><td align="left"><strong>工程化输出机制</strong></td><td align="left">自动存图、自动存 <code>mat</code>、命名稳定（无日期）</td></tr><tr><td align="left"><strong>可扩展开发骨架</strong></td><td align="left">新算法、新配置、新图表可按现有接口平滑扩展</td></tr></tbody></table><hr/><h2>▶️ 一键运行建议</h2><pre><code class="matlab">% 1) 基础冒烟验证
run_smoke_test

% 2) 单天线核心性能
run_single_snr_experiment(false)

% 3) 多天线核心性能
run_multi_snr_experiment(false)

% 4) 频谱效率与复杂度
run_multi_se_experiment(false)
run_complexity_experiment

% 5) 强相关图集
run_related_figure_gallery(true)

% 6) 全部任务一键执行
main_all_experiments</code></pre><hr/><h2>🛒 获取方式</h2><p>本文代码仅为核心片段，完整版工程已整理好。 关注公众号 【<strong>3GPP仿真实验室</strong>】进行获取。</p>]]></description></item><item>    <title><![CDATA[macOS Tahoe 26.3 (25D125) 正式版 ISO、IPSW、PKG 下载 sysi]]></title>    <link>https://segmentfault.com/a/1190000047608392</link>    <guid>https://segmentfault.com/a/1190000047608392</guid>    <pubDate>2026-02-12 22:04:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>macOS Tahoe 26.3 (25D125) 正式版 ISO、IPSW、PKG 下载</p><p>Liquid Glass 惊艳新设计亮相，电话 app 和实时活动丰富连续互通体验，聚焦搜索迎来最大更新</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=zoxEb0GLtG0I7nIlPI%2FPJA%3D%3D.fRQmoElu23JRTMEPt6w1fFuWjrWddVelxDYi9juRl3%2B3NMLv8WKmUkjvIauyniyn" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=q61BZJTWVeHyzbL15gNtBg%3D%3D.bv3J8YCdFUvKYp9rPGYzj%2Fh0pttu7jSGzWOanWLT3K0%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>2026 年 2 月 12 日凌晨，Apple 发布 iOS/iPadOS/macOS/watchOS/tvOS/visonOS 全平台 26.3 版本更新。macOS Tahoe 26.3 此次为常规问题修复和安全更新。</p><h2>macOS Tahoe 让 Mac 更强大 更高效 更智能</h2><p>惊艳新设计亮相，电话 app 和实时活动丰富连续互通体验，聚焦搜索迎来最大更新</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624380" alt="16 英寸 MacBook Pro、iMac 和 13 英寸 MacBook Air。" title="16 英寸 MacBook Pro、iMac 和 13 英寸 MacBook Air。"/></p><p>macOS Tahoe 26 推出精美新设计、丰富的连续互通体验及更多功能，强势助推生产力。</p><p><strong>加利福尼亚州，库比提诺</strong> Tim Cook 领导的 Apple 今日预览了新一代 macOS——macOS Tahoe 26，推出惊艳新设计和诸多强大功能，赋能用户完成更多任务。macOS 的新设计让桌面、程序坞、app  内导览和工具栏等经典元素更加灵动活泼、赏心悦目且契合用户个性需求，同时延续了原有的熟悉感。用户可使用更新版控制中心和文件夹、app  图标与小组件的新色彩选项，进一步打造个性化体验。随着 Mac 版电话 app  的推出，连续互通功能进一步提升，用户可轻松使用最近通话、通讯录和语音留言等 iPhone 版电话 app  的全部功能，以及通话筛选和通话保留助理等新功能 (sysin)。依托 iPhone 实时活动，用户可直接在 Mac  上实时掌握正在进行的活动，如航班信息等。聚焦搜索迎来迄今最大更新，用户现可直接执行数百项操作，如发送电子邮件或创建备忘录等，并利用全新浏览体验更快捷地访问内容。</p><p>“macOS 是 Mac 的核心与灵魂，Tahoe 则将深受用户喜爱的功能发扬光大。无论资深用户还是 Mac  新手，都能借助更多功能提高效率，更顺畅地利用 Mac 和 iPhone 协同工作。”Apple 软件工程高级副总裁 Craig  Federighi 表示，“令人惊艳的新设计、奇妙的连续互通体验、聚焦搜索的强大提升、更多智能快捷指令和 Apple 智能的更新让 Mac  体验更胜以往。”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624381" alt="一台 iMac 上显示着设计一新的主屏幕。" title="一台 iMac 上显示着设计一新的主屏幕。" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624382" alt="一台 MacBook Pro 上显示着深色调的新设计。" title="一台 MacBook Pro 上显示着深色调的新设计。" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624383" alt="一台 MacBook Pro 上显示着设计一新的主屏幕。 " title="一台 MacBook Pro 上显示着设计一新的主屏幕。 " loading="lazy"/></p><p>图：新设计解锁了个性化设置 Mac 的更多方式。</p><h2>macOS Tahoe 硬件兼容性列表</h2><p>笔者提示：“Apple Intelligence” 及相关功能要求 <a href="https://link.segmentfault.com/?enc=oGQmdWvHavaJD91Ro767Qw%3D%3D.WfejsiKv9WE6p4S3x9ww8MYjxvtCU5up7QF9dRkqNWfmAfwHLHOkVL4xybOTLGdp" rel="nofollow" target="_blank">搭载 Apple 芯片的 Mac 电脑</a>。</p><p>看看你的 Mac 是否能用 macOS Tahoe</p><p><a href="https://link.segmentfault.com/?enc=3hYwQlyQb79o28s%2F0CFxqA%3D%3D.Soxo8Qiqz8LSuFp8xpegU0vnTEMpf2%2BnBN0QwdsNjF0%3D" rel="nofollow" target="_blank">进一步了解 Mac&gt;</a></p><ul><li><strong>MacBook Air</strong> with Apple silicon 2020 and later <a href="https://link.segmentfault.com/?enc=qahyKhdMiXSdNzB2t5dCCg%3D%3D.WcgLFNZZXPkolOYEEKrVlqY4uyy7XAyH%2Fms7ieLAaK2AWjlDq6y%2F3Afpt9uyrpVp" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> with Apple silicon (2020 and later) <a href="https://link.segmentfault.com/?enc=hVoBV%2BpTQK1sXZJSZz5Tnw%3D%3D.iRVolvyb2krOvkxViYLZ2eCjFuRZRnO67MIDG4glmwcnXILloOIEiGXD9vvIJSAO" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> 2019 <a href="https://link.segmentfault.com/?enc=eg6NGDa5YL%2FaCe2%2BIzhojQ%3D%3D.o9mhCORlrvkxohOTVNgY54fLP84lTZSv%2F9FzC9KntchCGkSj285DZ4VEy1V7SLLW" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> (13‑inch, 2020, Four Thunderbolt 3 ports) <a href="https://link.segmentfault.com/?enc=TslWxEiRjYKwEJvxAi3KEQ%3D%3D.tKgD3ACEedLQnia7Kgm9yK%2FedPGkvnUhJZ0LbsKd9oLYuVsTjybOcnW5jrSJvnuA" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac mini</strong> 2020 and later <a href="https://link.segmentfault.com/?enc=WaxuyYdIBF8pSp95B3UunA%3D%3D.FuIdxCvXBXncaghdT13mdSn3VHuUzIA7c19YfYM%2FQUue8oSFwDp7zoO8jh6jB0Ya" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac Studio</strong> 2022 <a href="https://link.segmentfault.com/?enc=s9BMOWWhSgtgI5J8vf48LA%3D%3D.NI0PyxNWpQHVevB%2BcA318tjgFI4TUlKgfGefS48dyMfcO0Vh1fI6w9NRnkSkocKQ" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac Pro</strong> 2019 and later <a href="https://link.segmentfault.com/?enc=vSTako4FVLzciwAruYaY4A%3D%3D.fctK6fNdpozgbHkck8dLHbU2AlMoksGzzi9PF%2BWoik3gCMV9exND9JimxGAhOEcy" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>iMac</strong> 2020 and later <a href="https://link.segmentfault.com/?enc=e9WV8iCrjXCtO%2FQX8gGpJA%3D%3D.ZXJhCmQrVt591R6Z4hm%2BiGMBVwUCsORsxUDEiPzhM8yRrAs2lvfHss8p1oKyVyGC" rel="nofollow" target="_blank">进一步了解 &gt;</a></li></ul><p>如果你的 Mac 不在兼容性列表，参看：<a href="https://link.segmentfault.com/?enc=Vn9SgV0l20KEVoKB92qNtQ%3D%3D.tFlHcceEhIO6ogY4EW5rf75qjP4enGAvUe4g4ZFA%2BCgIEkZA%2FH%2FF5sojc3P6d4GNz%2Bsm7hqb0OlohKV9UlxCqg%3D%3D" rel="nofollow" target="_blank">在不受支持的 Mac 上安装 macOS Tahoe 26</a></p><h2>macOS Tahoe 版本历史</h2><p>Software Releases</p><ul><li>macOS Tahoe 26.3 (25D125) - 2026.02.11</li><li>macOS Tahoe 26.2 (25C56) - 2025.12.12</li><li>macOS Tahoe 26.1 (25B78) - 2025.11.03</li><li>macOS Tahoe 26.0.1 (25A362) - 2025.09.30</li><li>macOS Tahoe 26 (25A354) - 2025.09.15</li><li>macOS 26 RC (25A353) - 2025.09.09</li><li>macOS 26 beta 9 (25A5351b) - 2025.09.03</li><li>macOS 26 beta 8 (25A5349a) - 2025.08.25</li><li>macOS 26 beta 7 (25A5346a) - 2025.08.19</li><li>macOS 26 beta 6 (25A5338b) - 2025.08.12</li><li>macOS 26 beta 5 (25A5327h) - 2025.08.06</li><li>macOS 26 beta 4 (25A5316i) - 2025.07.22</li><li>macOS 26 beta 3 (25A5306g) - 2025.07.08</li><li>macOS 26 beta 2 (25A5295e) - 2025.06.23</li><li>macOS 26 beta (25A5279m) - 2025.06.09</li></ul><h2>下载 macOS Tahoe</h2><p>💡 <a href="https://link.segmentfault.com/?enc=ehJjVymVUfZspur%2Bu%2FV7Pw%3D%3D.2zezN2x8tGH8xZjxcHs4FLDb%2FEAcM8I0%2BFt9ELxfnwQ%3D" rel="nofollow" target="_blank">如何校验本站下载的文件的完整性</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047305036" alt="macOS Tahoe" title="macOS Tahoe" loading="lazy"/></p><h3>(1) ISO 格式软件包 (推荐)</h3><blockquote><p>本站原创可引导映像，可以在当前系统中安装或者升级，可以通过 USB 存储引导安装，也可以用于虚拟机安装。</p><p>此版本更多介绍请参看：<a href="https://link.segmentfault.com/?enc=1W3SRIHHQMfCGaXYrtnp%2FA%3D%3D.4rilMuIrWN3qvODQXCfOu9tv4F%2Fgihx5apIddwE8YiWiROwZ2zGoRG66qxpfBFRJ" rel="nofollow" target="_blank">macOS Tahoe 26 Boot ISO 原版可引导映像下载</a></p></blockquote><ul><li>macOS Tahoe 26.3 (25D125) - 2026.02.11</li><li>macOS Tahoe 26.2 (25C56) - 2025.12.12</li><li>macOS Tahoe 26.1 (25B78) - 2025.11.03</li><li>macOS Tahoe 26.0.1 (25A362) - 2025.09.30</li><li><p>macOS Tahoe 26 (25A354) - 2025.09.15</p><ul><li>下载地址：<a href="https://link.segmentfault.com/?enc=MITygpzXcQdkjyCyQHKzVQ%3D%3D.vmnJiieurov0UxJncun8t9CMhvJ1krOSRURYnqO%2Bf16LGdPDg5vTTgsS5bp6q1Ic" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe/</a></li></ul></li></ul><p>参看：<a href="https://link.segmentfault.com/?enc=5y47AE41xm25LxwBeJUpsA%3D%3D.yFXNyZNsOg2JHZsCNSXVqi3kQfl7u4luhXnb2EeocNaAm7%2F9gUy6ERYVc8%2FxoC85" rel="nofollow" target="_blank">如何在 Mac 和虚拟机上安装 macOS Sequoia、macOS Sonoma 和 macOS Ventura</a></p><h3>(2) PKG 格式软件包</h3><blockquote>该格式软件包双击运行后将自动安装在 <code>/Applications</code> 下。</blockquote><ul><li>macOS Tahoe 26.3 (25D125) - 2026.02.11</li><li>macOS Tahoe 26.2 (25C56) - 2025.12.12</li><li>macOS Tahoe 26.1 (25B78) - 2025.11.03</li><li>macOS Tahoe 26.0.1 (25A362) - 2025.09.30</li><li><p>macOS Tahoe 26 (25A354) - 2025.09.15</p><ul><li>下载地址：<a href="https://link.segmentfault.com/?enc=4haWcB%2Fpx9rIbO4CBhfhEg%3D%3D.%2Fmh%2BEQ2CDtWWnZJUDfTcp38z2S1qJDXx0BTYjDEgy9cysjRFbcXh81RK7mCPj%2FKA" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe/</a></li></ul></li></ul><h3>(3) IPSW 固件 (Apple 芯片 Mac 专用)</h3><blockquote>IPSW 格式为搭载 Apple 芯片的 Mac 专用映像，其他格式通用。</blockquote><p>适用于：<a href="https://link.segmentfault.com/?enc=B5tKBfmu%2FN2Zlbvfd99Cvg%3D%3D.OWW4genhdsoMyCGtw6yqPkcCBLOdCTWuuvNcmyR9D%2B35%2FosRMh0Q6pxWpgNOr5Vh" rel="nofollow" target="_blank">搭载 Apple 芯片的 Mac 电脑</a></p><p>参看：<a href="https://link.segmentfault.com/?enc=uyL3lQ%2Fmn%2FyCztJiUJ%2Bhww%3D%3D.GRAKeBO%2BQ10B7rJ5XFIc72EO6UzZQHR37xED7z7kGvYzYzM5AAKYAgVN9%2BeOwE4B" rel="nofollow" target="_blank">使用 DFU 模式修复或恢复 Mac 固件</a></p><ul><li>macOS Tahoe 26.3 (25D125) - 2026.02.11</li><li>macOS Tahoe 26.2 (25C56) - 2025.12.12</li><li>macOS Tahoe 26.1 (25B78) - 2025.11.03</li><li>macOS Tahoe 26.0.1 (25A362) - 2025.09.30</li><li><p>macOS Tahoe 26 (25A354) - 2025.09.15</p><ul><li>下载地址：<a href="https://link.segmentfault.com/?enc=n0Rdlici51SjcrhAe14ZEg%3D%3D.Q2UFa2SF2U%2B%2F2C4gjk8JLtit%2FnRZmrDk6ot58Etlk2Oq2rPjfKBXt9D8x6Kz6JCK" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe/</a></li></ul></li></ul><h3>(4) App Store 链接</h3><p><code>待上架</code></p><p>或者打开 App Store 搜索 “macOS Tahoe” 即可下载（下载的是当前最新版）。</p><h2>适用的 VMware 软件下载链接</h2><p>建议在以下版本的 VMware 软件中运行（Linux OVF 无需本站定制版可以正常运行，macOS 虚拟化如果不是 Mac 必须使用定制版才能运行，Windows OVF 需要定制版才能启用完整功能）：</p><ul><li><p>VMware vSphere：</p><ul><li>VMware <a href="https://link.segmentfault.com/?enc=edtuzNRXmYvfkd4tfSGIkg%3D%3D.5AWfi74BZCe0C0tZ1qLpnpC5Yp7iEbnureZBxjKp1WdEsxclVQkbZBXH34CVetWo" rel="nofollow" target="_blank">ESXi 9</a> or <a href="https://link.segmentfault.com/?enc=3VW3IN0QS4IjsqxpNYqFmA%3D%3D.MYGtYDsHwjzac1k4SJ5UuNPzTkQIEXRrlz8803DmGNUVm0eu%2B2kGamQTB8j1W6Io" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=JOHmlyVV5ks5LQ0YWv392w%3D%3D.7GSGK3D3IPstOj6gPthdbbBFKZc6gM3R4WBoR393XZfocTpIoVRZARDJakBMGth%2F" rel="nofollow" target="_blank">vCenter Server 9</a> &amp; <a href="https://link.segmentfault.com/?enc=SuDzyw9R9Lzm5ZgzMBHJeQ%3D%3D.LEOLhxErpU0llER1XlBR8%2FxMzwiDpLC1%2BBBY%2BushYvy8ULmlfGvJRLTEaIakhDbdkwZJ%2BwHQh4yYwsk%2F7nsp%2Bg%3D%3D" rel="nofollow" target="_blank">VCF Operations 9</a></li><li>VMware <a href="https://link.segmentfault.com/?enc=494mo6YvMFtt8La6JhU79g%3D%3D.IOfqOQVnTElW1iaWOtMAFf8oWYE39hcOKfnFDu0CdfedHNSKpUQpEht7roZy4io1" rel="nofollow" target="_blank">ESXi 8</a> or <a href="https://link.segmentfault.com/?enc=pTbX7RqXg59Pz4wKCDcB0Q%3D%3D.Ntmzu6ntIS7%2Bw6B0jyRu6inkEGZGhB0rwVErVqTxPuSwf5YVtqZV4DW3vIdOlTSd" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=fE2%2BXVrLfrdDojrO1IsRPw%3D%3D.e4r%2BJhvqBXTHnyOrk7OR1tuxyOfefqfgK1HC76uuOc1eRRxC%2FXS0d4bmpQr3joxX" rel="nofollow" target="_blank">vCenter Server 8</a></li><li>VMware <a href="https://link.segmentfault.com/?enc=EKB1nawXGZwtaINHHJE9Bw%3D%3D.9qHfRtbq9iixQTorhGtDCHnsMIcwivamFEljKMBZjKVMXK3E2iNc88NhSM7g0DHR" rel="nofollow" target="_blank">ESXi 7</a> or <a href="https://link.segmentfault.com/?enc=uAjtCbm%2FHPRmTZFQNXShDw%3D%3D.ThJC0W6aUK%2FKNvhohXWMlxkHemfRgyUq3rsht0eeyCr6bSEZft4USCP2cM1NP%2BD%2B" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=4FgcQoiF%2BjMFt1gtMCbVpw%3D%3D.Z8piBShQ%2FTa1pql1EbSVcR2yIMvr19Bnj8%2Bb%2BOssx2cJYfTV9tLw%2FI7D8acoCWU9" rel="nofollow" target="_blank">vCenter Server 7</a></li></ul></li><li>macOS：<a href="https://link.segmentfault.com/?enc=PrJnCpAC%2BdvABrxKI5gBWQ%3D%3D.aby%2BPkBMG2aOENDEe75GCfYBSP9KxTvYVO3xdebnW%2FOIMCPlnAhkN3jPF5JCAeGG" rel="nofollow" target="_blank">VMware Fusion</a></li><li>Linux：<a href="https://link.segmentfault.com/?enc=Il4pW1UfG0CN1c9zFgzDTA%3D%3D.VnwCudzWbEWfNgAmDq0JDWFUmIu1He20%2Fj1FPXQWs9AZS2Io1goaYqqfk2Jw2J0AuWRnpRL1qn1T8dM6dHEetg%3D%3D" rel="nofollow" target="_blank">VMware Workstation for Linux</a></li><li>Windows：<a href="https://link.segmentfault.com/?enc=GXUoZv16x61UukbRinvW8g%3D%3D.Kw9Vx1BB6Z%2FbqrRSFjJfBgWei5HuOgmJcp6I%2BWQxW81FMWHze6I9jon5QKyAnEtx8R%2BTa7DnaU08Yy%2FlRtXRUg%3D%3D" rel="nofollow" target="_blank">VMware Workstation for Windows</a></li></ul><p>macOS Tahoe 虚拟化解决方案，请参看：<a href="https://link.segmentfault.com/?enc=Or3CmUbeyRVK9DjnifZsTQ%3D%3D.u5%2FvMsM7oJGos3K8St%2BT3Egn4K%2FhcjXM1HBvmdLDFlAyzmklYe%2Fz0Qu9S1qIFBUh" rel="nofollow" target="_blank">macOS 26 Blank OVF - macOS Tahoe 虚拟化解决方案</a></p><h2>如何创建可引导的 macOS 安装器</h2><p>请访问：<a href="https://link.segmentfault.com/?enc=sIe8uS2HPJVRex0TLzZ2bg%3D%3D.6aMaow3v%2BVqPQblsUABSm8A7hwkrnunMGJH%2FIkyJP3S4dW9y5ckdXTyMLepmWSufCbVceuW2HO0qZ%2Ba53kyUbQ%3D%3D" rel="nofollow" target="_blank">如何创建可引导的 macOS 安装介质</a></p><p>更多：<a href="https://link.segmentfault.com/?enc=CUs3BbqRHeZ3wRtwVEbwMg%3D%3D.lpqEPgdNhBLFssM5wHqY7vTEK%2FFNAHcbxY4CWQBJegA%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[macOS Tahoe 26.3 (25D125) Boot ISO 原版可引导映像下载 sysin]]></title>    <link>https://segmentfault.com/a/1190000047608395</link>    <guid>https://segmentfault.com/a/1190000047608395</guid>    <pubDate>2026-02-12 22:04:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>macOS Tahoe 26.3 (25D125) Boot ISO 原版可引导映像下载</p><p>Liquid Glass 惊艳新设计亮相，电话 app 和实时活动丰富连续互通体验，聚焦搜索迎来最大更新</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=Q6HvaYV70FEmmCNE5DHcaA%3D%3D.3i8jVpNnlZogpeaNbdfDkQbS8gBZCSUczojSerKtvyu%2FzDszdQ5AooYeJp%2B%2Bqgs2" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe-boot-iso/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=AKvmOfWKqlg8VTcQiup8JA%3D%3D.PspjVxsTO9%2FKczHJvJZ8UE39pvJqOlyTY%2FzX6clzQSo%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>2026 年 2 月 12 日凌晨，Apple 发布 iOS/iPadOS/macOS/watchOS/tvOS/visonOS 全平台 26.3 版本更新。macOS Tahoe 26.3 此次为常规问题修复和安全更新。</p><p>macOS Tahoe 26 让 Mac 更强大、更高效、更智能</p><p>惊艳新设计亮相，电话 app 和实时活动丰富连续互通体验，聚焦搜索迎来最大更新</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624380" alt="16 英寸 MacBook Pro、iMac 和 13 英寸 MacBook Air。" title="16 英寸 MacBook Pro、iMac 和 13 英寸 MacBook Air。"/></p><p>macOS Tahoe 26 推出精美新设计、丰富的连续互通体验及更多功能，强势助推生产力。</p><p><strong>加利福尼亚州，库比提诺</strong> Tim Cook 领导的 Apple 今日预览了新一代 macOS——macOS Tahoe 26，推出惊艳新设计和诸多强大功能，赋能用户完成更多任务。macOS 的新设计让桌面、程序坞、app  内导览和工具栏等经典元素更加灵动活泼、赏心悦目且契合用户个性需求，同时延续了原有的熟悉感。用户可使用更新版控制中心和文件夹、app  图标与小组件的新色彩选项，进一步打造个性化体验。随着 Mac 版电话 app  的推出，连续互通功能进一步提升，用户可轻松使用最近通话、通讯录和语音留言等 iPhone 版电话 app  的全部功能，以及通话筛选和通话保留助理等新功能 (sysin)。依托 iPhone 实时活动，用户可直接在 Mac  上实时掌握正在进行的活动，如航班信息等。聚焦搜索迎来迄今最大更新，用户现可直接执行数百项操作，如发送电子邮件或创建备忘录等，并利用全新浏览体验更快捷地访问内容。</p><p>“macOS 是 Mac 的核心与灵魂，Tahoe 则将深受用户喜爱的功能发扬光大。无论资深用户还是 Mac  新手，都能借助更多功能提高效率，更顺畅地利用 Mac 和 iPhone 协同工作。”Apple 软件工程高级副总裁 Craig  Federighi 表示，“令人惊艳的新设计、奇妙的连续互通体验、聚焦搜索的强大提升、更多智能快捷指令和 Apple 智能的更新让 Mac  体验更胜以往。”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046624381" alt="一台 iMac 上显示着设计一新的主屏幕。" title="一台 iMac 上显示着设计一新的主屏幕。" loading="lazy"/></p><p>图：新设计解锁了个性化设置 Mac 的更多方式。</p><h2>ISO 映像的优势</h2><p>相对于官方发布 PKG 映像（另有 IPSW 映像，但仅适用于 Apple 芯片），以及第三方制作的 DMG 映像，ISO 格式具有以下优势：</p><ul><li>可以直接拖拽到 Applications（应用程序）目录下（无需管理员权限），进行升级安装</li><li>可以直接双击挂载，执行命令写入 USB 存储设备或者其他卷，然后启动全新安装（无需拖拽到“应用程序”目录下）</li><li>可以直接启动虚拟机安装，介质本身为可引导映像</li><li>可以在 Windows 和 Linux 下写入 USB 存储设备，创建 USB 引导安装介质</li><li>跨平台支持，可以在任意操作系统中使用，其他格式仅限 macOS 专用</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046878483" alt="macOS Tahoe in VMware" title="macOS Tahoe in VMware" loading="lazy"/></p><p>图：macOS Tahoe 运行在 Fusion 25H2 中，并开启了 Metal GPU 加速。</p><h2>下载 macOS Tahoe ISO</h2><p>💡 <a href="https://link.segmentfault.com/?enc=15WiPhMIHiPhAArXlHjz7A%3D%3D.9OtZdFLLoWflmKMM3n1g4Tdo10%2F%2FBh0WvtAPFPb0csI%3D" rel="nofollow" target="_blank">如何校验本站下载的文件的完整性</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047305036" alt="macOS Tahoe" title="macOS Tahoe" loading="lazy"/></p><blockquote>本站原创可引导映像，可以在当前系统中安装或者升级，可以通过 USB 存储引导安装，也可以用于虚拟机安装。</blockquote><ul><li>macOS Tahoe 26.3 (25D125) - 2026.02.11</li><li>macOS Tahoe 26.2 (25C56) - 2025.12.12</li><li>macOS Tahoe 26.1 (25B78) - 2025.11.03</li><li>macOS Tahoe 26.0.1 (25A362) - 2025.09.30</li><li><p>macOS Tahoe 26 (25A354) - 2025.09.15</p><ul><li>下载地址：<a href="https://link.segmentfault.com/?enc=QrcNtaZ2NlEOgsz5q9FYnA%3D%3D.lzv%2B6qp3pqEkildtJCqZIBleS4Ie3bDiHzhHdjE3udFNVZlvAJ15f3ORcWhsy9B2" rel="nofollow" target="_blank">https://sysin.org/blog/macos-tahoe-boot-iso/</a></li></ul></li></ul><p>参看：<a href="https://link.segmentfault.com/?enc=QCFCZhmi6RW7OAUaImQFnw%3D%3D.vH%2FhISQAMEk4wne0Ki4EB%2BupWS38RphII0RoSZMinLlLWdbcOCdPKg2rVsy8Gy1h" rel="nofollow" target="_blank">如何在 Mac 和虚拟机上安装 macOS Sequoia、macOS Sonoma 和 macOS Ventura</a></p><p>这里列出 ISO 启动映像下载链接，更多格式请访问以下地址：</p><ul><li><a href="https://link.segmentfault.com/?enc=fSWxh%2Bp2ZMYyK3v9piTByA%3D%3D.jbETOstrb4WhAYGuHoiVhmCZ0SxaYjYhn8sKV5kha7ptbrbrOycZQnsOCh9ZluqT" rel="nofollow" target="_blank">macOS Tahoe 26 ISO、IPSW、PKG 下载</a></li></ul><h2>macOS Tahoe 硬件兼容性列表</h2><p>笔者提示：“Apple Intelligence” 及相关功能要求 <a href="https://link.segmentfault.com/?enc=bu4qEh91X9sfgFMYv87SsA%3D%3D.W5OI0eGLxufpirQT6s8kH%2BHrS2A2RRg0T6d0A25veS5BhvEAyY8s65vb%2FlcM0v%2BE" rel="nofollow" target="_blank">搭载 Apple 芯片的 Mac 电脑</a>。</p><p>看看你的 Mac 是否能用 macOS Tahoe</p><p><a href="https://link.segmentfault.com/?enc=UM5zVgSCyuqF%2Bdh49d4TgA%3D%3D.%2FJjGRjpKnX9Lm8BTzWUi7t%2BO7dm%2FMKv6%2FygFHdawM%2Fk%3D" rel="nofollow" target="_blank">进一步了解 Mac&gt;</a></p><ul><li><strong>MacBook Air</strong> with Apple silicon 2020 and later <a href="https://link.segmentfault.com/?enc=fvddrb5PgOkcZQShafvUkQ%3D%3D.CnIcJBfNCIY8eFDvo8vBFEWMLiPMYCshCT4ZK3Ba5zn29l6L0PdM52WsdsjtWJQ2" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> with Apple silicon (2020 and later) <a href="https://link.segmentfault.com/?enc=foHu%2Bomgj%2FUTedLlVcMv5A%3D%3D.1hF0Qz6nRcW3rhMKwuc6J4wPbCiEw51OXmNSBjYFi3l4n2q1NkyMCyvz%2F1xA6opn" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> 2019 <a href="https://link.segmentfault.com/?enc=7QxXmTP02MMt4IFFe7jJoA%3D%3D.zfU09ZaKA2R3%2BjfpgLANZZVAWA2m8IeMY%2FO0xpmeZaEOEO7W830GMdKwqHZ1HtBM" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>MacBook Pro</strong> (13‑inch, 2020, Four Thunderbolt 3 ports) <a href="https://link.segmentfault.com/?enc=Muh2ySRn4iISovyFkpMgzA%3D%3D.p7egdTj4Wi%2BQyQIDbp8MmNZy1LSwOLmEfb9FzkJ9hdQq6FUZ64gYOgmK50DAEMc0" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac mini</strong> 2020 and later <a href="https://link.segmentfault.com/?enc=Zqtf%2F%2F34Oo0SNlP85tp2bg%3D%3D.9roZKhPdWX4eGrnZ76bAkeWBrGhsqhIefiFp9LcZSCcgoBHeBR3QdSZxR354oTFM" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac Studio</strong> 2022 <a href="https://link.segmentfault.com/?enc=pWDrMOq03IEVHWr5AmeCWA%3D%3D.pPrmm%2Bmqx5h6%2Bqzcwm1Jo6GlU8auAm9PmzHoxXOcyTnz2UDxd2oogBIBn5jXGDO5" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>Mac Pro</strong> 2019 and later <a href="https://link.segmentfault.com/?enc=PZunyAz%2BFA7mt3CcntyN%2BA%3D%3D.hd7V%2BA7i%2F2go75VCQjQPatxI%2F4ubJob0oE4ezbFscb%2F4CBv2vFHKCXEXVtmStxcM" rel="nofollow" target="_blank">进一步了解 &gt;</a></li><li><strong>iMac</strong> 2020 and later <a href="https://link.segmentfault.com/?enc=SrGnXsBefMrxPeK41%2BL6Fw%3D%3D.yRYmxmC4my9zWhx3QaoO83pFB29f1kdXL74NYvItEozYs8ntDU5urRwf6zkKwa%2BY" rel="nofollow" target="_blank">进一步了解 &gt;</a></li></ul><p>如果你的 Mac 不在兼容性列表，参看：<a href="https://link.segmentfault.com/?enc=leFF%2Fy%2FdsBrTeeKgkN650Q%3D%3D.69omGzqs6tpoF15SWStHixl1m5YSXCEkX1EPowvpgeA0hu4H2JfwLDdN75XMS4reIThWyctH2wlGB2ij7s71IQ%3D%3D" rel="nofollow" target="_blank">在不受支持的 Mac 上安装 macOS Tahoe 26</a></p><h2>适用的 VMware 软件下载链接</h2><p>建议在以下版本的 VMware 软件中运行（Linux OVF 无需本站定制版可以正常运行，macOS 虚拟化如果不是 Mac 必须使用定制版才能运行，Windows OVF 需要定制版才能启用完整功能）：</p><ul><li><p>VMware vSphere：</p><ul><li>VMware <a href="https://link.segmentfault.com/?enc=NXse2hkFM9fRFX%2FoWx5E3Q%3D%3D.AX7ZEbad8miHt5so6qYQg88IBKPR5DEezA2C1fLJrT%2B1Vl9QkMQg0LjIFYE0DNFX" rel="nofollow" target="_blank">ESXi 9</a> or <a href="https://link.segmentfault.com/?enc=gTZcN5fHmMsFoXKglDQ5Ug%3D%3D.%2BsP9NJUZua%2FuxLGUqG2yXU7H2kmxqk6sXftqbdeiebIVkXmGbeMvC47cgF34Ad1o" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=5KQrhKm5dEasjShnscfRNw%3D%3D.4n0VzdOQLwNSRizI1X7Ml45FfaBucME6ULecHrBcRF3172ZDmDJ%2FcUyuck8RgOdq" rel="nofollow" target="_blank">vCenter Server 9</a> &amp; <a href="https://link.segmentfault.com/?enc=S54v%2Fv9VHEwgbnU08fIe2g%3D%3D.Jm%2FuCgc7rmYFJzNEdcXFovN4M6Kw%2F3VkJxBYItvy%2BaOYN0u7ameFdz8UEnsWtQTPGHnGCQbXyfZryMQT5VzgAg%3D%3D" rel="nofollow" target="_blank">VCF Operations 9</a></li><li>VMware <a href="https://link.segmentfault.com/?enc=aKTcQYOIPPcyjmDlRJrkyA%3D%3D.QdJJCKL6Ar3LOpjHSeO49qNOjcYE%2FZkUl08IA5wjLwPixXllPokcFYmrvarD1HD5" rel="nofollow" target="_blank">ESXi 8</a> or <a href="https://link.segmentfault.com/?enc=lqhtH23YtlkL1%2FZEaiW0Vg%3D%3D.qr8JlEe5JHjjRlvO5JRpD83veU2URk%2Fpq2Z1mubT1sve%2Bnz254hWhlWPKxqtClLF" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=G29Hg5bpFQ9oE6ncML9iuw%3D%3D.%2BRG38uZsOi9U0WcpbSrUlXinZRgGZ8HVv2Zcws3BxdKsU3KOau0F0swB7OEkrTq0" rel="nofollow" target="_blank">vCenter Server 8</a></li><li>VMware <a href="https://link.segmentfault.com/?enc=3T3VeyeBJ7ECfuZA2Td7Mg%3D%3D.XYeofG%2FVeMqj%2BeMQ5DdpcfI%2BSWAeW7nTvbQImTBmXzPY5yxsBczEGPd7OeoLPMra" rel="nofollow" target="_blank">ESXi 7</a> or <a href="https://link.segmentfault.com/?enc=wtiPZtiHvxk1WIQAdXoImQ%3D%3D.akSmLFZaLfmXJiGVv3DO%2Bhdxk7Fl%2FrroUeLqQ4B6RHSSEds3FL54dSzIlqJAzocW" rel="nofollow" target="_blank">with driver</a> &amp; <a href="https://link.segmentfault.com/?enc=PtfFAmF3FbGJjYX5y0Xk1g%3D%3D.aj2pgcaLtx8IxgEtWS5My%2FUxzKCUR2w9ObvgfGu9P3Ev02xq0nvAcflWMua%2BG7xq" rel="nofollow" target="_blank">vCenter Server 7</a></li></ul></li><li>macOS：<a href="https://link.segmentfault.com/?enc=MgOp0ciRS322QE%2FNq%2BgYsw%3D%3D.SYkHEaN%2BNARsErZTgP6le8JQ7wOsxbm9pOYt32QSihXTvDDDNBAcikI6nGhmTTND" rel="nofollow" target="_blank">VMware Fusion</a></li><li>Linux：<a href="https://link.segmentfault.com/?enc=lCAq6adWVXDBsqFjxrvgUA%3D%3D.%2FeR9GvPf7BU5sYAcjQJI8MXNXTnLn6BfHKTbWcyBW%2BWzJtWXffIE4UqwrVKjQoyxOCL91Yrf721FVNZifq6bTQ%3D%3D" rel="nofollow" target="_blank">VMware Workstation for Linux</a></li><li>Windows：<a href="https://link.segmentfault.com/?enc=oPH09YSWmjOXzlQgvF1faA%3D%3D.jPcIuE%2FB1TGGR5XvohDJu3i0M5sgk7tyIu60o%2Bz9rggTwCnmEYmmbzFRwz7KvJIsUBk7eAqQ1EE2EFF6Z4XzhA%3D%3D" rel="nofollow" target="_blank">VMware Workstation for Windows</a></li></ul><p>macOS Tahoe 虚拟化解决方案，请参看：<a href="https://link.segmentfault.com/?enc=c3NqB5bG3XX%2FxkO4I2gvrA%3D%3D.aDfFFWLHnX0llW7phak6rH4hZ2TpSE%2FXSv5defdXdvIqsv9s5cg%2FbW%2BA54GtUGHQ" rel="nofollow" target="_blank">macOS 26 Blank OVF - macOS Tahoe 虚拟化解决方案</a></p><h2>如何创建可引导的 macOS 安装器</h2><p>请访问：<a href="https://link.segmentfault.com/?enc=vCAFKYbEnTjEC%2B%2FMW%2BwF4A%3D%3D.K%2FEUnm3DIapf11M8MD5T6IXg7wBBPb97kverD%2BdDmbw17XJ6smttyKlU6ZtPl10XgCszKz8Lt8%2FxD35ENDZT3g%3D%3D" rel="nofollow" target="_blank">如何创建可引导的 macOS 安装介质</a></p><p>更多：<a href="https://link.segmentfault.com/?enc=rVgIAup57Wjduy98O%2FPsvw%3D%3D.PB4KqDi6Lo8B%2BYQrx23LKT48kmm3eIrApZ45Qps3Mj0%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[6G 物理层变天AFDM：与其在 OFDM 的死胡同里撞墙，不如换个坐标系“折叠”世界 3GPP仿真]]></title>    <link>https://segmentfault.com/a/1190000047608399</link>    <guid>https://segmentfault.com/a/1190000047608399</guid>    <pubDate>2026-02-12 22:03:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我们先承认一个尴尬的事实：</p><p>面对 6G 提出的 <strong>1000 km/h（超高铁）</strong> 和 <strong>28000 km/h（低轨卫星）</strong> 愿景，统治了通信界二十年的王者——OFDM，已经尽力了。</p><p>依靠缩短符号时间、加大子载波间隔（SCS），这只是在物理极限的边缘疯狂试探。我们就像在暴风雨中修补一艘漏水的船，补丁打得越多，船越重（CP 开销大、频谱效率低）。</p><p><strong>是时候换一艘船了。</strong></p><p>今天，我们来聊聊物理层的一场“降维打击”： ​<strong>AFDM 仿射频分复用</strong>​ 。</p><hr/><h3>01. 完美的代价：OFDM 的基因缺陷</h3><p>一切悲剧的根源，早在我们选择 OFDM 的那一刻就注定了。</p><p>为了追求频谱效率的极致，我们在频域选择了​<strong>Sinc 函数</strong>​（$sin(x)/x$）作为子载波。</p><p>它长得并不像一根完美的针，而是一个带着无数“拖油瓶”的波形：</p><ul><li><strong>主瓣：</strong> 高耸入云，承载有用信息。</li><li><strong>旁瓣 (Side-lobes)：</strong> 像波纹一样向两边扩散，且衰减极其缓慢。</li></ul><p>OFDM 利用数学上的<strong>“正交性”</strong>，巧妙地让每一个子载波的<strong>峰值</strong>，精准地踩在其他所有子载波的<strong>零点 (Zero Crossing)</strong> 上。</p><p>这是一场​<strong>刀尖上的舞蹈</strong>​。</p><p>虽然旁瓣拖得很长，但在采样点那一瞬间，大家互不干扰。只要大家都不动，这个平衡就是完美的。</p><blockquote><strong>但在 6G 的世界里，“不动”成了一种奢望。</strong></blockquote><hr/><h3>02. 速度的诅咒：从 350km/h 到 7.6km/s</h3><p>当你在 350km/h 的高铁上，或者在 7.6km/s 的卫星下，物理世界开始对这个脆弱的数学平衡下手了。</p><p>大家通常认为多普勒只是​<strong>频率平移</strong>​。但在 OFDM 的眼里，这简直就是一场 <strong>“旁瓣的屠杀”</strong> 。</p><p>设想一下，当整个频谱发生微小的偏移（哪怕只是子载波间隔的 ​<strong>3%</strong> ​）：</p><ol><li><strong>零点错位：</strong> 接收机做 FFT 采样时，原本应该采到“0”的地方，现在采到了隔壁子载波的​<strong>旁瓣能量</strong>​。</li><li><strong>能量海啸：</strong> 由于 Sinc 函数的旁瓣拖得很长，<strong>远处的子载波</strong>也会把能量“泼”过来。</li><li><strong>ICI 爆发：</strong> 成千上万个子载波的干扰叠加在一起，形成了恐怖的 ​<strong>ICI（载波间干扰）</strong> ​。</li></ol><p><strong>(建议配图：OFDM 子载波正交性被破坏的示意图，展示波峰对不准零点)</strong></p><p>更绝望的是​<strong>低轨卫星（LEO）场景</strong>​。</p><p>当速度达到 ​<strong>7.6 km/s</strong>​，多普勒频移轻松突破 ​<strong>500 kHz</strong>​。</p><p>这直接导致​<strong>相干时间（Coherence Time）崩塌</strong>​。</p><p>这意味着：<strong>你的导频（Pilot）刚测完信道，还没来得及发数据，信道已经变了。</strong></p><p>传统的信道估计逻辑彻底断裂。</p><p>这时候，无论你把基站功率开多大，都没用了。因为干扰来自信号内部，信噪比（SINR）被锁死在一个 <strong>“地板”</strong> 上。</p><p><strong>网速瞬间从“千兆级”掉回“3G 时代”。</strong></p><hr/><h3>03. 第一性原理：把“正弦波”扔进垃圾桶</h3><p>OFDM 为什么怕多普勒？</p><p>因为它用的基底是 ​<strong>正弦波</strong>​——$e^{j2pi ft}$。</p><p>正弦波是静态的、永恒的。它唯一的弱点就是 <strong>“频率必须精准”</strong> 。</p><p>面对 6G 的超高动态，物理层先锋们做了一个违背祖宗的决定：</p><p><strong>抛弃正弦波，改用 Chirp（线性调频信号）。</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608401" alt="demo_afdm_basics.png" title="demo_afdm_basics.png"/></p><p>想象一下：</p><ul><li><strong>OFDM 的子载波</strong> 像是一排排<strong>垂直竖立</strong>的栅栏。风（多普勒）一吹，栅栏就歪了，互相碰撞。</li><li><strong>AFDM 的子载波</strong> 像是<strong>倾斜的</strong>多米诺骨牌。<br/>它的频率本身就是随时间线性变化的：$e^{j2pi (ft + frac{c}{2}t^2)}$。</li></ul><p>这里的 $c$ (Chirp Rate)，就是我们手中的​<strong>魔法钥匙</strong>​。</p><hr/><h3>04. DAFT：上帝的扭曲力场</h3><p>有了 Chirp 信号，我们如何调制数据？</p><p>欢迎来到数学的无人区—— ​<strong>DAFT (离散仿射傅里叶变换)</strong> ​。</p><p>别被名字吓到。它的物理本质极其性感：</p><p><strong>它在对时频平面（Time-Frequency Plane）进行“剪切” (Shearing) 和“旋转”。</strong></p><ul><li><strong>传统 FFT</strong> 是正正方方的网格。</li><li><strong>DAFT</strong> 通过调整参数 $c$，把网格<strong>扭曲</strong>成平行四边形，使其斜率与信道的<strong>多普勒频移斜率</strong>完美对齐。</li></ul><p><strong>见证奇迹的时刻：</strong></p><p>当信道的最大多普勒频移为 $f_{max}$ 时，我们只需要设置 Chirp 参数 $c = 2f_{max}/T$。</p><p>此时，原本在这个星球上狂暴变化的信道，在 DAFT 变换后的域里，竟然奇迹般地​<strong>变成了一条直线（时不变信道）</strong> ​！</p><blockquote><strong>我们没有消除多普勒，我们只是通过扭曲坐标系，把它“骗”过去了。</strong></blockquote><hr/><h3>05. 降维打击：全分集 (Full Diversity) 的暴力美学</h3><p>AFDM 最让通信人上瘾的，是它的抗衰落能力。</p><p>在 OFDM 中，如果一个子载波掉进深衰落（Deep Fade）的坑里，上面的数据就死定了。</p><p>但在 AFDM 中，<strong>每一个数据符号都“弥散”在整个带宽和时隙上。</strong></p><p>这就好比：</p><ul><li><strong>OFDM</strong> 是把鸡蛋放在 1000 个篮子里。摔了一个篮子，就碎一个鸡蛋。</li><li><strong>AFDM</strong> 是把鸡蛋打散，均匀地涂在 1000 个篮子上。摔碎几个篮子？无所谓，把剩下的拼起来，鸡蛋还是完整的。</li></ul><p><strong>结论炸裂：</strong></p><p>多普勒越大，多径越复杂，AFDM 的性能反而越好（分集阶数越高）。</p><p><strong>这是物理层对恶劣环境的最强嘲讽。</strong></p><hr/><h3>06. 终极杀手锏：它不再只是通信</h3><p>如果你以为 AFDM 只是为了让网速快一点，那你就把格局想小了。</p><p>AFDM 真正让 6G 颤抖的，是它的 <strong>“双重身份”</strong> 。</p><p>请回想一下，AFDM 的核心波形是什么？<strong>是 Chirp。</strong></p><p>在通信人眼里，这是新波形；但在<strong>雷达人</strong>眼里，这是 <strong>“老祖宗”</strong> ！</p><p><strong>一个惊人的宿命出现了：</strong></p><p>当我们在 6G 基站上发射 AFDM 波形时，我们实际上是在发射​<strong>雷达波</strong>​。</p><ul><li><strong>OFDM 是“盲人”：</strong> 它只能以此岸传到彼岸，不知道中间经历了什么。</li><li><strong>AFDM 是“睁眼玩家”：</strong> 它的波形天然具备​<strong>探测能力</strong>​。它在传输数据的同时，顺便把周围环境的 <strong>距离（Delay）</strong> 和 <strong>速度（Doppler）</strong> 扫描了一遍。</li></ul><p><strong>这就是 6G 的圣杯——通感一体化 (ISAC)。</strong></p><p>未来的基站，不需要你发导频告诉它你在哪。通过 AFDM 的回波，基站直接 <strong>“看”</strong> 到了你。</p><p>它知道这辆车在以 120km/h 变道，它知道那颗卫星在以 7.6km/s 靠近。</p><p>因为我看清了你，所以我能完美地调节坐标系来适应你。</p><p><strong>通信与感知，在 AFDM 的时延-多普勒域里，完成了物理层上的“灵肉合一”。</strong></p><hr/><h3>结语</h3><p>OFDM 统治了二十年，它把“静态”做到了极致。</p><p><strong>但 AFDM 的出现，标志着我们终于有勇气去拥抱“动态”。</strong></p><p>在 7.6km/s 的星链上，在 1000km/h 的真空管道里，正弦波的时代正在落幕。</p><p>那个属于 Chirp，属于 DAFT，属于 <strong>“御风而行”</strong> 的时代，才刚刚开始。</p><hr/><blockquote><p>“欢迎关注公众号 <strong>3GPP仿真实验室</strong>！这里是通信算法工程师的加油站。</p><p>我们不搬运新闻，只输出<strong>可运行的代码</strong>和<strong>深度标准解读</strong>。</p><p>👇 <strong>新人见面礼（后台回复关键词获取）：</strong></p><p>回复【LDPC】：获取 5G NR LDPC 编解码 MATLAB 代码（含注释）。<br/>回复【工具】：通信人减负神器：5G NR 帧结构与频点一键生成器（Python+Excel+Web三版）。<br/>回复【Pytorch】：获取 5G NR OFDM 链路 Pytorch 教学代码（含注释），助力人工智能 + 通信</p><p>让我们一起探索 6G 的无限可能。</p></blockquote>]]></description></item><item>    <title><![CDATA[【Matlab源码】6G候选波形：MIMO-OFDM-IM 增强仿真平台 3GPP仿真实验室 ]]></title>    <link>https://segmentfault.com/a/1190000047608413</link>    <guid>https://segmentfault.com/a/1190000047608413</guid>    <pubDate>2026-02-12 22:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>&lt;p align="center"&gt;<br/>  &lt;h1 align="center"&gt;📡 MIMO-OFDM-IM 空间扩展仿真平台&lt;/h1&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;strong&gt;空间+频率双域索引调制，MIMO 分集与复用增益的完美结合&lt;/strong&gt;</code></pre><p>&lt;/p&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;img src="https://img.shields.io/badge/MATLAB-R2021b+-blue?style=flat-square&amp;logo=mathworks" alt="MATLAB"/&gt;
&lt;img src="https://img.shields.io/badge/MIMO-8x8-green?style=flat-square" alt="MIMO"/&gt;
&lt;img src="https://img.shields.io/badge/Spatial-IM-orange?style=flat-square" alt="SIM"/&gt;
&lt;img src="https://img.shields.io/badge/Diversity-Order-red?style=flat-square" alt="Diversity"/&gt;</code></pre><p>&lt;/p&gt;<br/>&lt;/p&gt;</p><hr/><h2>📌 为什么选择本仿真平台？</h2><table><thead><tr><th align="left">痛点</th><th align="left">本平台解决方案</th></tr></thead><tbody><tr><td align="left">📚 SISO 分集阶数有限</td><td align="left">✅ <strong>MIMO 空间分集</strong>：多天线提供 Nr × Nt 分集阶数</td></tr><tr><td align="left">🔧 空间复用与索引调制难结合</td><td align="left">✅ <strong>空频双域索引</strong>：天线选择 + 子载波选择叠加</td></tr><tr><td align="left">📊 MIMO 检测复杂度高</td><td align="left">✅ <strong>分离检测算法</strong>：先空间后频域，复杂度大幅降低</td></tr><tr><td align="left">⚡ 信道模型单一</td><td align="left">✅ <strong>MIMO 瑞利信道</strong>：独立衰落建模，真实场景验证</td></tr><tr><td align="left">📡 缺乏分集增益量化</td><td align="left">✅ <strong>BER 曲线斜率分析</strong>，直观展示分集阶数提升</td></tr></tbody></table><hr/><h2>🎯 核心价值</h2><table>
<tr>
<td width="50%">

### 🔬 学术研究价值

- MIMO-IM 空频联合调制理论验证
- 空间分集与频率分集叠加效应
- ML/分离/迭代检测算法对比
- 大规模 MIMO 性能边界探索

</td>
<td width="50%">

### 💼 工程应用价值

- 支持 2×2 到 8×8 天线配置
- 可配置激活天线数量
- 适用于 5G/6G 多天线系统
- 完整的 MIMO 收发链路

</td>
</tr>
</table><hr/><h2>⚡ 技术亮点</h2><h3>🌊 MIMO-OFDM-IM 双域架构</h3><pre><code class="text">┌─────────────────────────────────────────────────────────────────┐
│                 MIMO-OFDM-IM 空频双域索引调制                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   【空间域索引】        【频率域索引】        【联合传输】       │
│                                                                 │
│   ┌─ Tx1 ★ ─┐          ┌─ f1 ● ─┐                              │
│   │  Tx2 ○  │          │  f2 ○  │                              │
│   │  Tx3 ○  │    +     │  f3 ● ─┼──► X[Nt, Nf] 空频符号矩阵    │
│   └─ Tx4 ★ ─┘          └─ f4 ○  ┘                              │
│                                                                 │
│   C(Nt,Na) 空间         C(n,k) 频域          联合索引比特        │
│   索引模式             索引模式                                  │
│                                                                 │
│         ┌──────── MIMO 瑞利信道 H[Nr×Nt] ────────┐              │
│         │      各天线对独立衰落                    │              │
│         └─────────────────────────────────────────┘              │
│                                                                 │
│   【接收端】 Y = HX + N ──► [空间检测] ──► [频域检测] ──► 恢复   │
└─────────────────────────────────────────────────────────────────┘</code></pre><h3>📊 分集增益对比 (n=4, k=2, QPSK)</h3><table><thead><tr><th align="center">配置</th><th align="center">空间分集</th><th align="center">频率分集</th><th align="center">总分集阶数</th><th align="center">BER@15dB</th></tr></thead><tbody><tr><td align="center">SISO-IM</td><td align="center">1</td><td align="center">n-k+1=3</td><td align="center"><strong>3</strong></td><td align="center">2.5e-3</td></tr><tr><td align="center">2×2 MIMO-IM</td><td align="center">4</td><td align="center">3</td><td align="center"><strong>12</strong></td><td align="center">1.8e-5</td></tr><tr><td align="center">4×4 MIMO-IM</td><td align="center">16</td><td align="center">3</td><td align="center"><strong>48</strong></td><td align="center">&lt; 1e-6</td></tr></tbody></table><blockquote>💡 <strong>分集倍增</strong>：MIMO-IM 总分集阶数 = Nr × Nt × (n-k+1)，相比 SISO 呈倍数增长。</blockquote><hr/><h2>🖥️ 运行环境</h2><h3>最低要求</h3><table><thead><tr><th align="left">项目</th><th align="left">要求</th></tr></thead><tbody><tr><td align="left"><strong>MATLAB版本</strong></td><td align="left">R2021b 或更高</td></tr><tr><td align="left"><strong>必需工具箱</strong></td><td align="left">Communications Toolbox</td></tr><tr><td align="left"><strong>基础依赖</strong></td><td align="left">P1 基础包</td></tr><tr><td align="left"><strong>内存</strong></td><td align="left">8 GB+ (大规模 MIMO 建议 16GB)</td></tr></tbody></table><h3>快速验证</h3><pre><code class="matlab">&gt;&gt; cd packages/P4_空间扩展包
&gt;&gt; setup_path
&gt;&gt; generate_ber_plots</code></pre><hr/><h2>🧠 算法原理</h2><h3>MIMO-IM 系统模型</h3><p><strong>发射端</strong>：</p><p>$$
\mathbf{X}[N_t \times N_f] = \text{SpatialMapper}(\mathbf{s}_{spatial}) \odot \text{FreqMapper}(\mathbf{s}_{freq})
$$</p><p><strong>接收端</strong>：</p><p>$$
\mathbf{Y} = \mathbf{H}\mathbf{X} + \mathbf{N}
$$</p><h3>空间索引比特</h3><p>$$
p_{spatial} = \lfloor \log_2 C(N_t, N_a) \rfloor
$$</p><h3>总比特数</h3><p>$$
p_{total} = p_{spatial} + G \cdot (p_1 + p_2)
$$</p><p>其中 G 为频域子块数。</p><h3>分集阶数分析</h3><p><strong>SISO-IM</strong>: $d = n - k + 1$</p><p><strong>MIMO-IM</strong>: $d = N_r \cdot N_t \cdot (n - k + 1)$</p><hr/><h2>📁 项目结构</h2><pre><code class="text">P4_空间扩展包/
├── 📂 mimo/                         # MIMO 索引调制
│   ├── mimo_im_modulator.m          #   🚀 MIMO-IM 调制器
│   ├── mimo_im_demodulator.m        #   🚀 MIMO-IM 解调器
│   ├── spatial_index_mapper.m       #   空间索引映射
│   └── spatial_index_demapper.m     #   空间索引解映射
│
├── 📂 channels/                     # MIMO 信道模型
│   ├── mimo_rayleigh_channel.m      #   MIMO 瑞利衰落信道
│   └── mimo_awgn_channel.m          #   MIMO AWGN 信道
│
├── 📂 core/                         # 继承 P1 核心模块
├── 📂 config/                       # 配置 (扩展 MIMO 参数)
│
├── 📂 docs/                         # 文档
│   ├── 算法文档.md                   #   📘 MIMO-IM 原理推导
│   ├── 代码文档.md                   #   📒 接口说明
│   └── 项目文档.md                   #   📗 本文档
│
├── generate_plots.m                 # 📊 基础 BER 曲线
└── generate_ber_plots.m             # 📊 MIMO vs SISO 分集对比</code></pre><p><strong>代码统计</strong>：</p><ul><li>📄 20+ 个核心 MATLAB 文件</li><li>📝 2000+ 行精炼代码</li><li>💬 100% 中文详细注释</li></ul><hr/><h2>🎬 仿真演示</h2><h3>一键运行</h3><pre><code class="matlab">&gt;&gt; cd packages/P4_空间扩展包
&gt;&gt; setup_path
&gt;&gt; generate_ber_plots</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608415" alt="p4_mimo_vs_siso.png" title="p4_mimo_vs_siso.png"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608416" alt="p4_spatial_heatmap.png" title="p4_spatial_heatmap.png" loading="lazy"/></p><hr/><h2>📦 您将获得</h2><table><thead><tr><th align="left">内容</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">📁 <strong>完整源码</strong></td><td align="left">MIMO-IM 空频双域调制解调</td></tr><tr><td align="left">📖 <strong>原理文档</strong></td><td align="left">空间索引、分集增益数学推导</td></tr><tr><td align="left">🚀 <strong>双域索引</strong></td><td align="left">空间+频率联合索引调制</td></tr><tr><td align="left">📊 <strong>分集验证</strong></td><td align="left">SISO vs MIMO 分集增益对比</td></tr><tr><td align="left">🔧 <strong>灵活天线</strong></td><td align="left">支持 2×2 到 8×8 配置</td></tr><tr><td align="left">📡 <strong>MIMO 信道</strong></td><td align="left">独立瑞利衰落信道建模</td></tr></tbody></table><hr/><h2>🎯 典型应用场景</h2><table><thead><tr><th align="left">场景</th><th align="left">推荐配置</th><th align="left">优势</th></tr></thead><tbody><tr><td align="left">低功耗 IoT</td><td align="left">2×2, Na=1</td><td align="left">分集增益 + 能效</td></tr><tr><td align="left">移动终端</td><td align="left">4×4, Na=2</td><td align="left">平衡性能与复杂度</td></tr><tr><td align="left">5G 基站</td><td align="left">8×8, Na=4</td><td align="left">最大分集增益</td></tr></tbody></table><hr/><h2>🛒 获取方式</h2><p>本文代码仅为核心片段，完整版工程已整理好。 关注公众号 【<strong>3GPP仿真实验室</strong>】进行获取。</p><h2>📚 参考文献</h2><ol><li><strong>E. Başar et al.</strong> (2013): "OFDM with Index Modulation for MIMO Systems." <em>IEEE Trans. Signal Process.</em>, vol. 61, no. 22.</li><li><strong>J. Crawford et al.</strong> (2017): "MIMO Spatial Modulation with Index Modulation." <em>IEEE Trans. Veh. Technol.</em>, vol. 66, no. 3.</li><li><strong>Y. Xiao et al.</strong> (2018): "OFDM with Flexible Space-Frequency Index Modulation." <em>IEEE Trans. Wireless Commun.</em>, vol. 17, no. 7.</li><li><strong>R. Mesleh et al.</strong> (2008): "Spatial Modulation." <em>IEEE Trans. Veh. Technol.</em>, vol. 57, no. 4.</li></ol>]]></description></item><item>    <title><![CDATA[【Matlab源码】6G候选波形：OFDM-IM 增强仿真平台 GIM、MM、IQ 3GPP仿真实验]]></title>    <link>https://segmentfault.com/a/1190000047608420</link>    <guid>https://segmentfault.com/a/1190000047608420</guid>    <pubDate>2026-02-12 22:02:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>&lt;p align="center"&gt;<br/>  &lt;h1 align="center"&gt;📈 OFDM-IM 容量增强仿真平台&lt;/h1&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;strong&gt;GIM + MM + IQ 三大容量扩展技术，突破传统索引调制比特率极限&lt;/strong&gt;</code></pre><p>&lt;/p&gt;<br/>  &lt;p align="center"&gt;</p><pre><code>&lt;img src="https://img.shields.io/badge/MATLAB-R2021b+-blue?style=flat-square&amp;logo=mathworks" alt="MATLAB"/&gt;
&lt;img src="https://img.shields.io/badge/GIM-Variable_k-green?style=flat-square" alt="GIM"/&gt;
&lt;img src="https://img.shields.io/badge/MM-Multi_Mode-orange?style=flat-square" alt="MM"/&gt;
&lt;img src="https://img.shields.io/badge/IQ-Independent-red?style=flat-square" alt="IQ"/&gt;</code></pre><p>&lt;/p&gt;<br/>&lt;/p&gt;</p><hr/><h2>📌 为什么选择本仿真平台？</h2><table><thead><tr><th align="left">痛点</th><th align="left">本平台解决方案</th></tr></thead><tbody><tr><td align="left">📚 固定激活数限制索引空间</td><td align="left">✅ <strong>GIM 广义索引</strong>：可变激活数，索引空间扩大 2-4 倍</td></tr><tr><td align="left">🔧 调制阶数浪费额外比特</td><td align="left">✅ <strong>MM 多模调制</strong>：每个激活子载波携带模式选择比特</td></tr><tr><td align="left">📊 I/Q 资源耦合利用</td><td align="left">✅ <strong>IQ 独立索引</strong>：实部虚部分别索引，并行双倍效率</td></tr><tr><td align="left">⚡ 容量提升难以量化</td><td align="left">✅ 内置 <strong>容量分析对比工具</strong>，直观展示各技术增益</td></tr><tr><td align="left">📡 检测算法对应难</td><td align="left">✅ 每种技术配套 <strong>专用解调器</strong>，算法一一对应</td></tr></tbody></table><hr/><h2>🎯 核心价值</h2><table>
<tr>
<td width="50%">

### 🔬 学术研究价值

- 三大容量扩展技术完整实现
- 索引空间理论上界研究
- 频谱效率 vs 复杂度权衡分析
- 不同场景最优技术选择

</td>
<td width="50%">

### 💼 工程应用价值

- 高频谱效率需求场景首选
- 灵活的参数配置空间
- 结构化的技术对比框架
- 完整的收发链路验证

</td>
</tr>
</table><hr/><h2>⚡ 技术亮点</h2><h3>🌊 三大容量增强技术对比</h3><pre><code class="text">┌─────────────────────────────────────────────────────────────────┐
│                    容量增强技术对比                              │
├──────────────┬──────────────┬──────────────┬──────────────────┤
│   基础 IM    │     GIM      │      MM      │       IQ         │
├──────────────┼──────────────┼──────────────┼──────────────────┤
│ 固定 k 个    │ k ∈ [k1,k2]  │ 每个子载波   │ I/Q 独立索引     │
│ 激活子载波   │ 可变激活数   │ 独立选模式   │ 并行处理         │
├──────────────┼──────────────┼──────────────┼──────────────────┤
│   C(n,k)     │  Σ C(n,ki)  │ k × Nm 模式  │   2 × C(n,k)     │
│   索引数     │  索引空间大  │  比特倍增    │   双倍索引       │
└──────────────┴──────────────┴──────────────┴──────────────────┘</code></pre><h3>📊 容量对比 (n=4, k=2, M=4)</h3><table><thead><tr><th align="center">技术</th><th align="center">索引比特 p1</th><th align="center">数据比特 p2</th><th align="center">总比特/子块</th><th align="center">vs 基础 IM</th></tr></thead><tbody><tr><td align="center">基础 IM</td><td align="center">2</td><td align="center">4</td><td align="center"><strong>6</strong></td><td align="center">基准</td></tr><tr><td align="center">GIM (k∈[1,4])</td><td align="center">4</td><td align="center">~5</td><td align="center"><strong>9</strong></td><td align="center">+50%</td></tr><tr><td align="center">MM (4模式)</td><td align="center">2</td><td align="center">8</td><td align="center"><strong>10</strong></td><td align="center">+67%</td></tr><tr><td align="center">IQ</td><td align="center">4</td><td align="center">4</td><td align="center"><strong>8</strong></td><td align="center">+33%</td></tr></tbody></table><blockquote>💡 <strong>MM 最强容量</strong>：通过模式选择比特，每个激活子载波额外携带 log₂(Nm) 比特。</blockquote><hr/><h2>🖥️ 运行环境</h2><h3>最低要求</h3><table><thead><tr><th align="left">项目</th><th align="left">要求</th></tr></thead><tbody><tr><td align="left"><strong>MATLAB版本</strong></td><td align="left">R2021b 或更高</td></tr><tr><td align="left"><strong>必需工具箱</strong></td><td align="left">Communications Toolbox</td></tr><tr><td align="left"><strong>基础依赖</strong></td><td align="left">P1 基础包</td></tr><tr><td align="left"><strong>内存</strong></td><td align="left">4 GB+</td></tr></tbody></table><h3>快速验证</h3><pre><code class="matlab">&gt;&gt; cd packages/P3_容量增强包
&gt;&gt; setup_path
&gt;&gt; generate_plots_enhanced</code></pre><hr/><h2>🧠 算法原理</h2><h3>GIM 广义索引调制</h3><p><strong>核心思想</strong>：允许激活子载波数量可变，融合多种 C(n,k) 组合。</p><p><strong>索引空间</strong>：</p><p>$$
|\mathcal{S}_{GIM}| = \sum_{k=k_{min}}^{k_{max}} C(n,k)
$$</p><p><strong>索引比特</strong>：</p><p>$$
p_{1,GIM} = \lfloor \log_2 |\mathcal{S}_{GIM}| \rfloor
$$</p><h3>MM 多模索引调制</h3><p><strong>核心思想</strong>：每个激活子载波独立选择调制模式。</p><p><strong>总比特数</strong>：</p><p>$$
p_{MM} = p_1 + k \cdot (\log_2 N_m + \bar{m})
$$</p><p>其中 $\bar{m}$ 为各模式的平均数据比特。</p><h3>IQ 独立索引调制</h3><p><strong>核心思想</strong>：I 和 Q 分量使用独立的激活模式。</p><p><strong>索引比特</strong>：</p><p>$$
p_{1,IQ} = 2 \times \lfloor \log_2 C(n,k) \rfloor
$$</p><hr/><h2>📁 项目结构</h2><pre><code class="text">P3_容量增强包/
├── 📂 gim/                     # GIM 广义索引
│   ├── gim_modulator.m         #   🚀 GIM 调制器
│   ├── gim_demodulator.m       #   GIM 解调器
│   └── gim_table.m             #   可变激活索引表
│
├── 📂 mm/                      # MM 多模调制
│   ├── mm_modulator.m          #   🚀 MM 调制器
│   └── mm_demodulator.m        #   MM 解调器
│
├── 📂 iq/                      # IQ 独立索引
│   ├── iq_modulator.m          #   🚀 IQ 调制器
│   └── iq_demodulator.m        #   IQ 解调器
│
├── 📂 core/                    # 继承 P1 核心模块
├── 📂 config/                  # 配置 (扩展 GIM/MM/IQ 参数)
│
├── 📂 docs/                    # 文档
│   ├── 算法文档.md              #   📘 三技术原理推导
│   ├── 代码文档.md              #   📒 接口说明
│   └── 项目文档.md              #   📗 本文档
│
├── generate_plots.m            # 📊 容量对比曲线
└── generate_plots_enhanced.m   # 📊 MM 多模星座图</code></pre><p><strong>代码统计</strong>：</p><ul><li>📄 25+ 个核心 MATLAB 文件</li><li>📝 2500+ 行精炼代码</li><li>💬 100% 中文详细注释</li></ul><hr/><h2>🎬 仿真演示</h2><h3>一键运行</h3><pre><code class="matlab">&gt;&gt; cd packages/P3_容量增强包
&gt;&gt; setup_path
&gt;&gt; generate_ber_plots  % 三技术 BER 对比</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608422" alt="p3_capacity_compare.png" title="p3_capacity_compare.png"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608423" alt="p3_gim_hist.png" title="p3_gim_hist.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608424" alt="p3_mm_constellation.png" title="p3_mm_constellation.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608425" alt="p3_variants_ber.png" title="p3_variants_ber.png" loading="lazy"/></p><hr/><h2>📦 您将获得</h2><table><thead><tr><th align="left">内容</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">📁 <strong>完整源码</strong></td><td align="left">GIM + MM + IQ 三技术完整实现</td></tr><tr><td align="left">📖 <strong>原理文档</strong></td><td align="left">索引空间扩展数学推导</td></tr><tr><td align="left">🚀 <strong>容量提升</strong></td><td align="left">最高 67% 频谱效率增益</td></tr><tr><td align="left">📊 <strong>对比工具</strong></td><td align="left">一键生成三技术性能对比</td></tr><tr><td align="left">🔧 <strong>灵活配置</strong></td><td align="left">可变 k 范围、模式数、调制阶数</td></tr><tr><td align="left">📡 <strong>可视化</strong></td><td align="left">多模星座图、容量对比曲线</td></tr></tbody></table><hr/><h2>🛒 获取方式</h2><p>本文代码仅为核心片段，完整版工程已整理好。 关注公众号 【<strong>3GPP仿真实验室</strong>】进行获取。</p><h2>📚 参考文献</h2><ol><li><strong>M. Wen et al.</strong> (2017): "Generalized Index Modulation Aided OFDM." <em>IEEE Trans. Wireless Commun.</em>, vol. 16, no. 3.</li><li><strong>B. Zheng et al.</strong> (2019): "Multiple-Mode OFDM with Index Modulation." <em>IEEE Trans. Signal Process.</em>, vol. 67, no. 9.</li><li><strong>E. Başar et al.</strong> (2017): "OFDM with Index Modulation Using In-Phase and Quadrature Indices." <em>IEEE Trans. Veh. Technol.</em>, vol. 66, no. 5.</li></ol>]]></description></item><item>    <title><![CDATA[Microsoft Office LTSC 2021 for Mac 16.106 - 文档、电子表]]></title>    <link>https://segmentfault.com/a/1190000047608474</link>    <guid>https://segmentfault.com/a/1190000047608474</guid>    <pubDate>2026-02-12 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Microsoft Office LTSC 2021 for Mac (Microsoft 365) 16.106 - 文档、电子表格、演示文稿和电子邮件</p><p>Office LTSC 2021 for Mac (Word, Excel, PowerPoint, Outlook + OneNote, OneDrive)</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=yf9Qwr2KLb%2BTYScDMwyFVA%3D%3D.ITt4QRJBTiJwD8dFlax%2BddAO2%2FzXSDcQgZJeA%2BHaUIg2nEEYtnhsPmjuoeK8%2BE2Y" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=fk6M%2FzUUvhXZa5RCd1tbwQ%3D%3D.R7GLNJqYl2dB%2FOnrX4neb43l5rqVcdGrq8MTN2cJmgE%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>Office for Mac 2021 2026 年 2 月份月度更新来袭！</p><h2>Office for Mac 2021 组件和发行版</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608476" alt="Office LTSC" title="Office LTSC"/></p><p>2021.09.16，微软正式发布了 Office LTSC 2021，当然也包括 for Mac 版本，Office for Mac 2021 首个版本号为 16.53，与 Office 365 和 Office 2019 共享安装介质，通过许可的不同而区分版本和功能。请参看 <a href="https://link.segmentfault.com/?enc=2rcc4aathr15h1ELqJiKnA%3D%3D.Bhu1WGvfNS%2BzTYutqAuaTQiHRmlbw9Vq8idZtRwCiKW%2FECD2KUB3TKSNhbp6vtXKhI1vYCN4vDDgF4jw5bvM44mUG3yV459ofXOTlkp2PQH8W059ht485EREFB%2FA4SEcbOtELZf4sd5ETvE0ZAoWHA%3D%3D" rel="nofollow" target="_blank">Office 2021 for Mac 新增功能</a>。</p><p>Office for Mac 包含以下组件：</p><ul><li>Microsoft <strong>Excel</strong>：电子表格和数据分析</li><li>Microsoft <strong>Outlook</strong>：电子邮件和日历</li><li>Microsoft <strong>PowerPoint</strong>：创建吸引人的演示文稿</li><li>Microsoft <strong>Word</strong>：创建、编辑和分享文档</li><li>Microsoft <strong>OneNote</strong>：记录笔记、创意和备忘录</li></ul><p>Office for Mac 有以下两种发行版（详见下文描述）：</p><ul><li>Office for Mac (Office 365) pkg</li><li>Office LTSC for Mac DMG VL</li></ul><h2>Office for Mac 2021 (Office 365) pkg</h2><p>⚠️：<strong>请慎用此版本，需要 root 权限才能运行，安装一堆无用文件，强制自动更新。</strong></p><p>参看：<a href="https://link.segmentfault.com/?enc=L%2FQhAod1hhisJtmnUODXmg%3D%3D.1r58AIxqcshibjPzu2covEtBJ0d4P6zYl0GQvDBgaJsYvHafha2Y6CQ7KfzUwDPpix9uhFt%2BSVI0BoxCwVQerw%3D%3D" rel="nofollow" target="_blank">如何卸载 Office for Mac</a></p><p>此版本的唯一优点是开放下载，各大网站通常提供的也是此版本。</p><p><strong>Microsoft Office for Mac 2021 (Office 365) 16.106 Universal</strong>（2026-01-13）</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=Qd%2BaJ6Qx3N2jL8gNT0cXtw%3D%3D.8wg5WI1%2B4g3SWc%2BKAfU3U2b9VcyaT0aufNVrYxON97hjZWk6ck1X84v%2BxmZizQju" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li><li>系统要求：从 16.101 开始，要求 macOS Sonoma 14.0 及以上版本。</li></ul><p>从 16.53 开始，Office 2021 和 Office 2019 是共用安装文件，通过许可证激活不同的版本，主要体现在界面风格上有较为明显差异，另外 2021 版有一些新增功能。</p><p>Office 365 是一种订阅模式，永久许可版即 Office LTSC for Mac。</p><h2>Office LTSC 2021 for Mac DMG VL</h2><p>该产品符合 Apple 平台设计规范，无需 root 权限安装，只需要拖拽到应用程序下即可，无需登录，没有自动更新程序，也不会提示过期。</p><ul><li>无需 root 权限，拖拽即可安装</li><li>无需登录账号（无需注册，支持离线使用）</li><li>无自动更新程序</li><li>不会提示过期</li><li>可以仅安装单个组件</li></ul><p>包含 Excel、Outlook、PowerPoint 和 Word 四个核心组件，可独立运行单个组件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608477" alt="Microsoft Excel" title="Microsoft Excel" loading="lazy"/></p><p>Microsoft <strong>Excel</strong>：电子表格和数据分析</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608478" alt="Microsoft Outlook" title="Microsoft Outlook" loading="lazy"/></p><p>Microsoft <strong>Outlook</strong>：电子邮件和日历</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608479" alt="Microsoft PowerPoint" title="Microsoft PowerPoint" loading="lazy"/></p><p>Microsoft <strong>PowerPoint</strong>：创建吸引人的演示文稿</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608480" alt="Microsoft Outlook" title="Microsoft Outlook" loading="lazy"/></p><p>Microsoft <strong>Word</strong>：创建、编辑和分享文档</p><p>备注：OneNote 免费，需要登录。</p><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.54 (Final version)</strong> for macOS Mojave 10.14</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=filDvKZEKMSECicv3%2FPYgg%3D%3D.LX6qRbXtdzlGjjNtEGoPn3ranRorxuGknYzMRvdBLpJToADxwpXI7z3vusrP8RgR" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.66 (Final version)</strong> for macOS Catalina 10.15</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=asvQNeLu0Pe%2FbGOybmvRgw%3D%3D.UAtFT9X8iRYgYRC9dTrxnCHrf7TH%2Bmfol5tHKbpQHuziRFA6viHm8dOmdoXbw%2BbB" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.77 (Final version)</strong> for macOS Big Sur 11</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=QvUmz9gnis6X5BwMVouJVg%3D%3D.VD2BEQfxN3O2pBfqOFIH9ooG1oWdOJgZf9cCYVvX3rjmwi2ZhHs5c%2FFUiSiuAGyF" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.89 (Final version)</strong> for macOS Montery 12</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=Qirr7wV6YnAaWiUEFkH%2FzQ%3D%3D.%2F0zvLrAc8wFgufg1sJ2R9dVa6DhazfxMrmTKsV3f3n4Bff8J2ECtY1%2BmLbgBys0P" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.100 (Final version)</strong> for macOS Ventura 13</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=1jBUMZW4iQJgfBZk9arbvw%3D%3D.p5Q0eauLY7Z9aVYSyPzo9h0s5pc3PDK%2B%2B8klnd5VT23nLNAUXx0VHb%2FXZIxT8OVf" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><p><strong>Microsoft Office LTSC for Mac 2021 DMG VL v16.106</strong> for macOS Sonoma 14 or later</p><ul><li>支持 macOS Sonoma 14、macOS Sequoia 15 和 macOS Tahoe 26</li><li>请访问：<a href="https://link.segmentfault.com/?enc=cDSQGF9N7UBzIoVEjB2brg%3D%3D.SzopchXroB7djR9A8fqXb2gGYrYJyGZY5TAzPniKPVJ6itm38C%2B1Qzf1Vcrk75v3" rel="nofollow" target="_blank">https://sysin.org/blog/office-2021-for-mac/</a></li></ul><hr/><p>新版链接：</p><ul><li><a href="https://link.segmentfault.com/?enc=s3y2Nv18dPUECBbZ8A2h8A%3D%3D.btTP8tgmdbmqLeTUjJI0ufHcYP9ewLWuwEWgqzWGQSJ85olpYjepL%2FNmAOaHfKS4" rel="nofollow" target="_blank">Microsoft Office LTSC 2024 for Mac (Microsoft 365) 16.106 - 文档、电子表格、演示文稿和电子邮件</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=nxR7aB42vAbeeD3rOqEChw%3D%3D.ycr9Lp4mTzVukyJLgPBEcNSeOCblNS5aowpu%2FJAU4lA%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[Xampp集成环境包 安装步骤详解（附Apache、MySQL启动与本地网站搭建） 小童童 ]]></title>    <link>https://segmentfault.com/a/1190000047608327</link>    <guid>https://segmentfault.com/a/1190000047608327</guid>    <pubDate>2026-02-12 21:03:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><p><code>Xampp</code>是 <strong>XAMPP 集成环境包</strong>​ 的安装程序，把 Apache（网页服务器）、MySQL（数据库）、PHP（编程语言解释器）还有 Perl 打包到一起，装完就能在本地跑网站、做 PHP 开发或测试。</p><h2>一、准备工作</h2><ol><li><p><strong>下载安装包</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=E5Iyyv8UCY6m32x7P%2BnttA%3D%3D.Kl8XTfBy2mWETiYiynbPQxETMCxMrDDYyETZuzFTsbHpQiI%2FZLNMUTFb1TquZ4%2BL" rel="nofollow" title="https://pan.quark.cn/s/2e6ba302b1eb" target="_blank">https://pan.quark.cn/s/2e6ba302b1eb</a></p></li><li><p><strong>用管理员身份运行（推荐）</strong> ​</p><ul><li>右键 <code>Xampp.exe</code>→ 选“以管理员身份运行”，避免权限不足导致端口占用或服务启动失败。</li></ul></li></ol><h2>二、安装步骤</h2><ol><li>双击 <code>Xampp.exe</code>打开安装程序。</li><li>弹出语言选择 → 选  <strong>“English”</strong> ​ 或  <strong>“中文(简体)”</strong> （看版本支持）→ 点  <strong>“OK”</strong> 。</li><li>欢迎界面 → 点  <strong>“Next”</strong> 。</li><li><p>选择组件：</p><ul><li>默认是全选（Apache、MySQL、PHP、Perl、phpMyAdmin 等），新手直接保持默认，点  <strong>“Next”</strong> 。</li></ul></li><li><p>选安装路径：</p><ul><li>默认 <code>C:\xampp`，可点 “Browse” 改到其他盘，比如</code>D:\xampp`，然后点  <strong>“Next”</strong> 。</li></ul></li><li><p>Bitnami 提示：</p><ul><li>这个是可选的应用安装向导，不想用就取消勾选，点  <strong>“Next”</strong> 。</li></ul></li><li>点  <strong>“Install”</strong> ​ 开始安装，等进度条走完（几分钟）。</li><li>安装中会问是否装到开始菜单和桌面快捷方式 → 根据需要勾选 → 继续直到完成。</li><li>完成后会提示是否立刻运行 XAMPP 控制面板 → 勾上 → 点  <strong>“Finish”</strong> 。</li></ol><h2>三、首次运行与基本使用</h2><ol><li>打开 XAMPP 控制面板（桌面或开始菜单里找）。</li><li>左侧列表里找到 <strong>Apache</strong>​ 和 <strong>MySQL</strong>​ → 分别点  <strong>“Start”</strong> ​ 启动，绿灯亮表示正常运行。</li><li>浏览器输入 <code>http://localhost</code>或 <code>http://127.0.0.1</code>，能看到 XAMPP 欢迎页说明成功。</li><li><p><strong>放网站文件</strong>：</p><ul><li>默认网站根目录在 <code>C:\xampp\htdocs</code>（或你改的路径下的 htdocs 文件夹），把 PHP 文件或项目丢进去即可访问。</li></ul></li><li><p><strong>管理数据库</strong>：</p><ul><li>浏览器访问 <code>http://localhost/phpmyadmin</code>，用默认账号 <code>root</code>，密码为空登录，就能建库、导数据。</li></ul></li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[《2025年度OpenAtom openKylin社区全景案例集》正式发布 openKylin ]]></title>    <link>https://segmentfault.com/a/1190000047608345</link>    <guid>https://segmentfault.com/a/1190000047608345</guid>    <pubDate>2026-02-12 21:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本次发布的《2025年度OpenAtom openKylin社区全景案例集》（以下简称‘案例集’）由openKylin社区牵头编撰，众多产业领域优势企业、知名院校及杰出开发者共同参与。在2024版的基础上，新增收录了社区成员们在2025年的杰出技术成果和行业应用案例共40余项。通过这些案例，读者可以深入了解openKylin社区在技术创新、AI智能融合、应用生态拓展、行业应用等方面的最新进展，为广大技术爱好者、生态伙伴及行业从业者提供宝贵的参考资料，进一步推动开源技术生态的繁荣发展！</p><p><strong>案例集内容概览</strong><br/>1.社区简介及共建情况主要介绍openKylin社区从成立至今的发展历程、贡献者参与、上下游贡献成果、社区活动风采等内容，帮助大家快速了解社区、参与社区。<br/>2.根社区基础能力主要介绍openKylin作为开源操作系统根社区所具备的基础能力，包括核心组件选型维护能力、“可控开源”体系等，欢迎更多人参与到根社区的建设中来。<br/>3.技术创新项目主要介绍openKylin社区目前孵化的优秀技术创新项目，涵盖底层技术、桌面场景、生态技术、开发工具、安全能力、智能融合六大技术创新领域，帮助大家快速了解社区最新技术创新成果。4.生态适配案例主要介绍openKylin社区上下游生态伙伴主导的行业生态适配优秀案例，包括xPU硬件、整机、应用软件等方面，帮助大家快速了解社区生态适配工作，吸引更多行业生态加入openKylin社区，共建繁荣。<br/>5.行业应用案例主要介绍openKylin系操作系统（包括商业发行版、用户自用版以及社区版）在各行业领域中的应用实践案例，帮助解决行业核心场景中痛点问题，满足典型场景需求，为行业用户提供有示范效应的解决方案。<br/>6.社区爱好者构建成果主要介绍openKylin社区优秀开发者和爱好者在社区参与的桌面环境移植构建和内核构建成果，帮助有兴趣参与社区的个人开发者或爱好者找到适合自己的贡献方向。</p><ol start="7"><li>社区基础设施平台建设成果主要介绍openKylin社区当前基础设施平台建设成果，包括基础服务相关平台、一站式编译构建相关平台、学习成长平台、AI融合平台，帮助大家了解社区基础设施平台体系架构和目前可以支撑的能力，提升用户参与社区的体验和效率。</li></ol><p>点击链接下载案例集：<a href="https://link.segmentfault.com/?enc=LX9BeOEuDXTF4ouW9ydzdQ%3D%3D.Y5EBwouL8fe12Yj2ERB9IGIhehOWf4w8Q0XpmdVlRmOxKA64MpbOGAc5rSLyM70q2YB5pwdPI%2B0lD0gKJ%2Brodxfuq3nGlxtweF4YfnolMOzYeKK96Y8jVIh3qiWs2Ntp4PjWysiVvZceYjR%2BM%2FcQ7Q%3D%3D" rel="nofollow" target="_blank">https://www.openkylin.top/public/pdf/OpenAtom_openKylin_Commu...</a></p>]]></description></item><item>    <title><![CDATA[phpwind_UTF8_8.5部署步骤详解（含环境准备+安装教程） 无邪的课本 ]]></title>    <link>https://segmentfault.com/a/1190000047608353</link>    <guid>https://segmentfault.com/a/1190000047608353</guid>    <pubDate>2026-02-12 21:02:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><h2>一、先准备点东西（必看！）</h2><ol><li><strong>环境得有</strong>：本地或服务器得装好 PHP+MySQL+Apache/Nginx（比如用宝塔面板的话，直接一键装这仨就行；没面板就自己手动搭，新手建议用集成环境像phpStudy/WAMP，省事儿）。</li><li><strong>下载安装包</strong>：<strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=zJkLIKX5JJCMiwhbtMRdsw%3D%3D.9fiKPOcP11pcRWgFsmn3Ll3uPlEsy3xLoo1vdv%2FNabFZLeaO1f2cH3KOb9j4vxPV" rel="nofollow" title="https://pan.quark.cn/s/3c18c722e10a" target="_blank">https://pan.quark.cn/s/3c18c722e10a</a>   ，把 <code>phpwind_UTF8_8.5.zip</code>下到电脑/服务器上。</li></ol><h2>二、解压+扔到网站目录</h2><ol><li>先把 zip 包解压了，里面会有个类似 <code>phpwind</code>的文件夹（具体看压缩包里的内容，别整错）。</li><li>把这个文件夹里的<strong>所有文件</strong>，复制到你的网站根目录（比如 Apache 默认是 <code>htdocs</code>，Nginx 可能是 <code>www</code>或你自己设的目录，宝塔面板里就是“网站”对应的根目录，直接上传进去就行）。</li></ol><h2>三、给目录开权限（不然可能报错）</h2><p>找到网站根目录下的两个文件夹：</p><ul><li><code>data</code>（存数据用的）</li><li><code>upload</code>（存上传图片/文件的）</li></ul><p>右键这两个文件夹 → 属性（或权限设置）→ 把“写入权限”勾上（Linux服务器一般设 <code>755</code>或 <code>777</code>，Windows服务器直接给“完全控制”也行，新手别纠结数字，能写就行）。</p><h2>四、浏览器访问安装页面</h2><p>打开浏览器，输入你的网站地址（比如 <code>http://localhost</code>或你的域名），会自动跳转到 phpwind 的安装页面（如果没跳转，手动输 <code>http://你的域名/install.php</code>，一般在根目录下）。</p><h2>五、跟着安装向导走（傻瓜式操作）</h2><h3>1. 同意协议，下一步</h3><p>看到许可协议，拉到最下面点“我同意”，然后点“下一步”。</p><h3>2. 检查环境（有问题会标红，先解决再继续）</h3><p>这里会检测 PHP版本、MySQL扩展、文件夹权限这些。如果有标红的“失败”项：</p><ul><li>比如“PHP版本太低”：升级PHP（宝塔里直接在软件商店点升级）；</li><li>比如“data目录不可写”：回到第三步重新设权限；</li><li>都绿了（显示“成功”）再点“下一步”。</li></ul><h3>3. 填数据库信息（重点！别乱填）</h3><p>这里需要提前在 MySQL 里建一个<strong>空数据库</strong>（比如叫 <code>phpwind_db</code>，字符集选 utf8 或 utf8mb4，避免乱码）：</p><ul><li>数据库名：填刚才建的空库名（比如 <code>phpwind_db</code>）；</li><li>数据库用户名：一般是 <code>root</code>（如果你单独给phpwind建了个MySQL用户，就填那个用户名）；</li><li>数据库密码：你MySQL的 root 密码（忘了就去查配置文件，宝塔里在“数据库”页能看到）；</li><li>数据库主机：默认 <code>localhost</code>（不用改，除非你的数据库不在本地）；</li><li>表前缀：默认 <code>pw_</code>就行，不改也没事。</li></ul><p>填完点“测试数据库连接”，提示“连接成功”就点“下一步”。</p><h3>4. 设管理员账号（记好！别忘密码）</h3><ul><li>管理员账号：自己设个登录名（比如 <code>admin</code>）；</li><li>密码：设复杂点（字母+数字+符号），一定要记住！</li><li>邮箱：填个能收邮件的（找回密码用）。</li></ul><p>填完点“下一步”，等它跑完进度条。</p><h3>5. 安装完成！删安装文件（重要！防被黑）</h3><p>看到“安装成功”页面后，<strong>务必删掉或重命名根目录下的 <code>install.php</code>文件</strong>（或者整个 <code>install</code>文件夹，有的压缩包里有这个文件夹），不然别人可能通过它重复安装搞破坏。</p><h2>六、登录后台玩去吧</h2><p>安装完会自动跳转到首页，或者手动访问 <code>http://你的域名/admin.php</code>，用刚才设的管理员账号密码登录，就能进后台管理论坛了（发帖、设置板块啥的都在里面）。</p><p>​</p>]]></description></item><item>    <title><![CDATA[LLM创造力可以被度量吗？一个基于提示词变更的探索性实验 本文系转载，阅读原文
https://av]]></title>    <link>https://segmentfault.com/a/1190000047608356</link>    <guid>https://segmentfault.com/a/1190000047608356</guid>    <pubDate>2026-02-12 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大语言模型在demo阶段总是看起来很惊艳。但一旦进入到生产环境很多问题就暴露了：不稳定、不可预测，甚至直接不可用。</p><p>从实践来看核心问题很少出在模型本身。更多时候是在于如何设计、评估和迭代应用模型的提示词。LLM应用的输入提示词必须适配具体任务，才能让模型在期望的输出范围内工作。</p><p>提示词工程在今天基本还是被当作一种"艺术"。这篇文章要讨论的就是为什么这是个问题，以及怎么把它变成一门可度量的工程学科。</p><blockquote>提示词工程仍然是猜测</blockquote><p>大多数团队的提示词改进流程其实很粗糙：有人写（或重写）提示词，跑几个例子，主观觉得"感觉好了一些"，然后就上线了。</p><p>没有度量标准，没有基线，也没有对"更好"的明确定义。</p><p>这带来的直接后果是：提示词质量难以对比，评估基本靠外部响应来判断，回归问题不容易察觉，很多故障等到上线后才被发现。</p><p>提示词工程本质上极度主观，如果目标是构建可靠的AI系统，这就成了一个严重的瓶颈。</p><h2>实际LLM使用中的两个对立问题</h2><p>在生产环境里跑LLM，我发现有两个反复出现的问题。</p><blockquote>不一致性：同一个提示词，不同的答案</blockquote><p>同一条提示词跑多次会产生明显不同的输出。这不只是烦人的问题，而是对数据流水线、自动化决策系统、评估框架来说，这是实打实的可靠性风险。</p><p>高方差在这类场景下是bug不是feature。模型要么表现出确定性行为，要么至少得在可控范围内运行。</p><blockquote>缺乏多样性：模型不够有创造力</blockquote><p>反过来，有好几个实际项目中碰到了相反的困境：做创意生成、探索性分析、创意制作这类任务时，模型产出的内容彼此过于相似，概念覆盖面非常窄。一旦规模化，创造力就丢得干干净净。</p><p>这时候确定性就从优势变成了束缚。</p><h2>一个简单的假设</h2><p>提示词质量应该是可衡量的。</p><p>有些任务需要最小化输出方差，有些任务需要最大化多样性，而提示词的变更应该能推动结果朝可度量的方向移动。不同类型的任务也可以选择不同的度量标准。</p><p>既然模型行为可以衡量，提示词行为为什么不能？</p><p>为了验证这个想法，我选了模型行为的一个切面来入手：响应多样性，把它当作创造力的代理指标。</p><p>目标不是找到完美的度量方式，而是回答两个问题：提示词变更能不能转化为一致的数值差异？单次任务上的创造力/确定性到底取决于提示词还是仅取决于温度？</p><h2>实验设置</h2><p>实验规模不大，设计如下：</p><p>提示词</p><p>提示词A：</p><p>"Create 5 ideas of creative banners for performance marketing of an AI benchmarking platform."</p><p>提示词B在A的基础上加了一条指令：</p><p>"Create 5 ideas of creative banners for performance marketing of an AI benchmarking platform. Be as creative as possible."</p><p>模型和采样</p><p>采用单次生成模式，测试了多个LLM（具体型号这里略过），温度分别设为0 × max、0.5 × max和1 × max。每个（提示词、模型、温度）组合跑10次。</p><p>测试集选了4个主流模型家族的13个模型：OpenAI的GPT系列、Google的Gemini系列、Antropic的Claude系列，以及Deepseek。</p><p>通过Embedding衡量多样性</p><p>每条生成结果都计算了4096维的embedding向量。然后对每个实验集（固定提示词、模型和温度），取集合内embedding的最大成对距离作为响应多样性的度量。</p><p>逻辑很简单：距离小说明行为高度确定，距离大说明输出多样且有创造力。最终得到一个数值，描述模型响应的"分散程度"。</p><h2>结果</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608358" alt="" title=""/></p><p>汇总表，创意提示词版本导致了更显著的分散。同时温度并不总起作用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608359" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608360" alt="" title="" loading="lazy"/></p><p>基础提示词和创意提示词在模型-温度切片上的比较图。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608361" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608362" alt="" title="" loading="lazy"/><br/>每个模型在不同温度水平上的响应分散图</p><p>结果比预期要清晰得多。</p><p>跨模型来看有三个明显趋势：在提示词中加入明确的创造力指令，曲线一致上移；提高温度在一定程度上增大了响应多样性，但受限于小样本，这个结论还需谨慎看待；各模型对温度变化的响应方式差异很大没有统一规律。</p><p>提示词变更带来的是可预测的数值效果，而非随机噪声。</p><p>这说明两件事：提示词迭代不必完全依赖直觉，输出创造力是可量化的；这一假设有可能推广到更大的样本和不同的应用场景。</p><p>这套方法的实际意义在于：提示词可以通过数值做A/B测试，温度调优有了度量依据而不是靠猜，模型选择可以由任务需求驱动而非跟风。</p><p>它让团队能在提示词变更上线之前就对效果做出推断。</p><h2>局限性</h2><p>结果虽然是正向的但有几个局限</p><blockquote>度量标准的任务特定性</blockquote><p>这里定义的"创造力"严格来说是任务相关的。用embedding距离衡量的响应多样性，在创意生成、营销创意、探索性任务上作为创造力的代理指标还算合理，但在事实性问答、代码生成、结构化数据提取这些场景下可能毫无意义，甚至会产生误导。</p><p>不能把它当成模型质量的通用指标。目前我也在测试其他面向不同任务的度量标准。</p><blockquote>对Embedding空间的依赖</blockquote><p>所有测量都建立在特定embedding模型和距离度量之上。换用不同的embedding模型、向量归一化方式或距离函数，绝对值也是会变的，所以模型间的相对排名也可能有所不同。</p><p>但本实验中观察到的趋势是稳定的，所以结果应当按相对值来解读，不宜绝对化。</p><blockquote>有限的样本量</blockquote><p>每个配置只跑了有限次数。趋势虽然一致，但要减少方差、估计置信区间、得出统计上站得住的结论，样本量还远远不够。当前的发现更多是探索性的，不是定论。</p><blockquote>提示词和领域偏差</blockquote><p>实验只用了一种任务表述和一个窄领域（效果营销创意）。换到其他领域或提示词风格，效果可能更弱、更强，也可能呈现完全不同的行为模式。把这些结论向创意任务之外推广需要格外谨慎。</p><blockquote>创造力与实用性的权衡</blockquote><p>响应多样性高不等于结果好。高度多样化的输出里可能混着不相关的想法、低质量的建议和不连贯的回复。这个实验测的是方差，不是实用性更不是商业价值。实际应用中创造力度量必须和质量过滤或下游评估配合使用。</p><blockquote>LLM的非平稳性</blockquote><p>大语言模型会被提供商持续更新，所以绝对分数可能随时间漂移，分数可能在提示词没改的情况下发生变化，可复现性也可能下降。任何长期的基准测试工作都必须把这种非平稳性纳入考量。</p><blockquote>相关性不意味着因果性</blockquote><p>最后要说的是，温度、提示词指令和响应多样性之间虽然有明确的相关性，但这不代表对模型行为有了完整的因果理解。实验证明的是"提示词变更可以被衡量"，而不是创造力可以被这套度量标准完全解释。</p><h2>总结</h2><p>这只是一系列研究的第一个实验，后续结果会在接下来的文章中陆续呈现。下一步计划：增加样本量，尝试不同的提示词，实验如何降低创造力，为其他类型任务定义新的度量标准，以及构建一个定期更新的模型排行榜来覆盖各项指标。</p><p><a href="https://link.segmentfault.com/?enc=24Jm%2BX7XSUR0lBw7fE%2BY9Q%3D%3D.EUeMMtmrp8O7ZIvXhPHv0LUM5mohaBOvUuQVkRO6LdkEJ8lMR1g8Is70%2BaNvi4LtT4b3MCXvFcF54%2ByOAiqXMQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/e84eee36d7bc4263b9fd5dfe564e21d9</a></p><p>作者：Alexey Konoshenkov</p>]]></description></item><item>    <title><![CDATA[枫琳 (Fenglin) 人机共生智能协作平台 鸿枫 ]]></title>    <link>https://segmentfault.com/a/1190000047608204</link>    <guid>https://segmentfault.com/a/1190000047608204</guid>    <pubDate>2026-02-12 20:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>🍁 枫琳 (Fenglin)</h2><blockquote>人机共生智能协作平台 - 让智能自然融入生活</blockquote><hr/><h3>一、产品简介</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608206" alt="" title=""/></p><p><strong>枫琳</strong> 是一款实现人与智能体（OpenClaw、OpenCode 等）和谐相处、共同交流、生活、工作的智能协作平台。支持接入钉钉、企业微信等办公软件，覆盖朋友圈、点赞、评论、私信等社区场景，实现人与机器人共生，共同进步，就像人与自然和谐共生一样。</p><h4>品牌理念</h4><table><thead><tr><th>字</th><th>含义</th><th>象征</th></tr></thead><tbody><tr><td><strong>枫</strong></td><td>四季流转，顺应自然</td><td>人与 AI 和谐适应、共同成长</td></tr><tr><td><strong>琳</strong></td><td>美玉相击，清音回响</td><td>思想交流、心灵共鸣</td></tr></tbody></table><blockquote><em>"枫叶由绿转红，是成长的印记；人机由陌生到默契，是共生的旅程"</em></blockquote><hr/><h3>二、核心功能</h3><h4>1. 🤖 智能体协作</h4><blockquote><em>枫叶随风起舞，人与 AI 自然共处</em></blockquote><ul><li><strong>多智能体协同工作</strong> - 支持 OpenClaw、OpenCode 等多种智能体</li><li><strong>智能任务分配</strong> - AI 自动分析并分配最优任务</li><li><strong>实时沟通反馈</strong> - 人机无缝对话，即时响应</li></ul><h4>2. 🏢 企业级集成</h4><blockquote><em>枫枝相连，生态融合</em></blockquote><ul><li><strong>钉钉深度集成</strong> - 无缝对接企业钉钉工作流</li><li><strong>企业微信对接</strong> - 支持企业微信生态</li><li><strong>自定义工作流</strong> - 灵活配置企业专属流程</li></ul><h4>3. 💬 社区生态</h4><blockquote><em>枫叶飘落，信息传递；琳玉相击，思想共鸣</em></blockquote><ul><li><strong>朋友圈动态分享</strong> - 发布图文、视频动态</li><li><strong>实时互动评论</strong> - 点赞、评论、转发</li><li><strong>安全私密对话</strong> - 端到端加密私信</li></ul><h4>4. ✨ 智慧共生</h4><blockquote><em>枫叶四季蜕变，持续成长</em></blockquote><ul><li><strong>知识共享沉淀</strong> - 构建团队知识库</li><li><strong>能力互补提升</strong> - 人机优势互补</li><li><strong>共同成长轨迹</strong> - 记录每一步进步</li></ul><hr/><h3>三、应用场景</h3><table><thead><tr><th>场景</th><th>描述</th></tr></thead><tbody><tr><td><strong>协同办公</strong></td><td>智能体助手帮你处理日常事务，如枫叶随风，自然流畅</td></tr><tr><td><strong>团队协作</strong></td><td>人机混合团队高效配合，如枫林成片，协作共生</td></tr><tr><td><strong>社交互动</strong></td><td>与 AI 伙伴分享生活，如枫语私语，获得理解与陪伴</td></tr><tr><td><strong>知识共创</strong></td><td>人类智慧与 AI 能力融合，如秋日枫林，收获满满</td></tr></tbody></table><hr/><h3>四、产品优势</h3><table><thead><tr><th>优势</th><th>说明</th></tr></thead><tbody><tr><td>🛡️ <strong>安全可靠</strong></td><td>企业级数据安全保障，如枫根深扎</td></tr><tr><td>⏰ <strong>全天候服务</strong></td><td>7×24 小时智能体在线，如枫叶常伴</td></tr><tr><td>⚡ <strong>高效智能</strong></td><td>AI 驱动的工作流，效率提升 300%</td></tr><tr><td>🌐 <strong>开放生态</strong></td><td>支持多种智能体接入，如枫林开放</td></tr></tbody></table><hr/><h3>五、品牌色彩体系</h3><table><thead><tr><th>颜色</th><th>色值</th><th>含义</th></tr></thead><tbody><tr><td>🔴 枫叶红</td><td><code>#C41E3A</code></td><td>温暖、活力、信任</td></tr><tr><td>🟡 秋金黄</td><td><code>#D4A017</code></td><td>收获、价值、希望</td></tr><tr><td>🟢 自然绿</td><td><code>#228B22</code></td><td>成长、生命、和谐</td></tr></tbody></table><hr/><h3>六、品牌 Slogan</h3><p><strong>主 Slogan：</strong></p><blockquote>枫琳，让智能自然融入生活</blockquote><p><strong>场景 Slogan：</strong></p><table><thead><tr><th>场景</th><th>Slogan</th></tr></thead><tbody><tr><td>品牌宣传</td><td>枫琳 — 人机共生，自然之道</td></tr><tr><td>产品介绍</td><td>枫琳，你的 AI 协作伙伴</td></tr><tr><td>社交场景</td><td>在枫琳，与 AI 成为朋友</td></tr><tr><td>办公场景</td><td>枫琳协创，工作更自然</td></tr></tbody></table><hr/><h3>七、开源项目</h3><p>本产品为开源项目，欢迎参与贡献：</p><p>🔗 <strong>Gitee 仓库：</strong> <a href="https://link.segmentfault.com/?enc=gFvMdIRPobQeTNyjakNgDQ%3D%3D.1HTXCw3mTbGYAYJgxDuZHeaiOTHNd1fuRCooZbXqNIuLm3e%2FQinyHvQfDUccs1QmBFb2BM73sDLe5LQ1xsdpeA%3D%3D" rel="nofollow" target="_blank">https://gitee.com/hongmaple/openclaw-dindin-chart</a></p><hr/><h3>八、快速开始</h3><h4>1. 注册账号</h4><p>访问枫琳官网，点击"免费试用"按钮</p><h4>2. 创建工作空间</h4><p>选择你的行业场景，系统将为你推荐合适的智能体</p><h4>3. 开始协作</h4><p>与智能体开始对话，让它成为你的得力助手</p><hr/><h3>九、联系我们</h3><table><thead><tr><th>渠道</th><th>信息</th></tr></thead><tbody><tr><td>📧 Email</td><td><a href="mailto:296155694@qq.com" target="_blank">296155694@qq.com</a></td></tr><tr><td>💬 微信</td><td>mapleCx330</td></tr><tr><td>🌐 官网</td><td>www.fenlin.ai</td></tr><tr><td>📦 开源项目</td><td><a href="https://link.segmentfault.com/?enc=isKs3OdkjoRz2LrZpO5Pqg%3D%3D.1XTIQR3xt6Hdy4IKwdNaEg7qu%2FtBL5z2Y5XtKVf32YJr%2BY%2Fn3I1oBPPX8CirF9RRolMiH0kiQUpaBDPUSKCDuw%3D%3D" rel="nofollow" target="_blank">https://gitee.com/hongmaple/openclaw-dindin-chart</a></td></tr></tbody></table><hr/><h3>十、相关链接</h3><table><thead><tr><th>项目</th><th>地址</th></tr></thead><tbody><tr><td>枫琳落地页源码</td><td><a href="https://link.segmentfault.com/?enc=bJBZ9lb9ID%2FLW9H8eE%2B2jw%3D%3D.Rns5mEMu5hbBPHg40JWLB4vlIf6dnk1LCmR3W%2Bnuv9oXdzj6vC9ME%2FA68rh2qQiF" rel="nofollow" target="_blank">https://gitee.com/hongmaple/fenlin-landing</a></td></tr><tr><td>枫琳 chat 开源项目</td><td><a href="https://link.segmentfault.com/?enc=PxBIuEK33RbLcykndB5k6Q%3D%3D.lnKZEEivtpEDAjnVcgAKten6Tza7CbNyILUk3JxZk6pYhCHlUwOSw2P7DRz0NykN0nTo%2Bo7Lg%2F5ox2utBh7jsA%3D%3D" rel="nofollow" target="_blank">https://gitee.com/hongmaple/openclaw-dindin-chart</a></td></tr></tbody></table><hr/><p><strong>🍁 枫琳 - 人机共生，自然之道</strong></p><p><em>让 AI 如枫叶般自然融入你的工作与生活</em></p><p>本文由<a href="https://link.segmentfault.com/?enc=29mm8%2B92F5TecmKbaRWbig%3D%3D.nKiP1N%2BNMKU74Omzrrd5zUieDMir%2BGRNApl5cj%2BLOCE%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[用 Go 实现一个可长期运行的 GitHub Webhook 服务实践 苏琢玉 ]]></title>    <link>https://segmentfault.com/a/1190000047608290</link>    <guid>https://segmentfault.com/a/1190000047608290</guid>    <pubDate>2026-02-12 20:02:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>前段时间我写过一篇文章，<a href="https://link.segmentfault.com/?enc=sFyUet9O2S9U7h9SFq5gQA%3D%3D.e8uq1bW1jZqjtmW2WWz8x9FJhm3MRUiGW2tW4QIJqKvslhzJA8IMROYBtd6fG0lw" rel="nofollow" target="_blank">记录自己作为一名 PHP 开发者自学 Go 的过程</a></p><p>那篇更多是学习阶段的整理。这次则是一次完整实践的复盘。</p><p>单点知识和系统能力之间始终存在差距。</p><p>理解一个概念并不难，但要把多个能力组合起来，形成一个可以长期运行的系统，往往需要真实项目去反复打磨。很多看似基础的东西，只有亲手做过，理解才会真正扎实。</p><p>最近我完成了一个小工具：<strong>github-webhook-listener</strong></p><p>一个用 Go 实现的 GitHub Webhook 接收服务，可以根据规则执行 Shell 命令，并内置一个简单的 Vue 面板，用于查看运行状态和执行记录。</p><p>功能本身并不复杂，AI 也完全可以在较短时间内生成类似的实现。但在实际开发过程中，我更在意的并不是功能本身，而是一些基础层面的设计问题：项目结构如何划分，依赖如何组织，边界如何定义，以及构建与部署如何简化。</p><p>这些内容未必新鲜，但当它们被组合到一个完整系统中时，体会是不同的。</p><p>项目地址我放在文章末尾，感兴趣可以自行查看下载使用。</p><p><img width="723" height="378" referrerpolicy="no-referrer" src="/img/bVdnVeJ" alt="" title=""/></p><p><img width="723" height="378" referrerpolicy="no-referrer" src="/img/bVdnVeK" alt="" title="" loading="lazy"/></p><p><img width="723" height="378" referrerpolicy="no-referrer" src="/img/bVdnVeL" alt="" title="" loading="lazy"/></p><p>下面我会从结构设计、并发模型以及构建方式三个方面，做一次相对完整的技术复盘。</p><hr/><h2>项目结构与职责划分</h2><p>项目核心代码放在 <code>internal</code> 目录：</p><pre><code>internal/
├── bootstrap
├── handler
├── service
├── repository
├── model
├── dto</code></pre><p>这种结构并不追求“标准答案”，重点在于依赖方向清晰。</p><h3>repository</h3><ul><li>只负责数据库操作</li><li>不包含业务判断</li><li>不依赖 HTTP</li></ul><h3>service</h3><ul><li>负责业务逻辑</li><li>调用 repository</li><li>不处理 HTTP 细节</li></ul><h3>handler</h3><ul><li>只做参数解析与响应封装</li><li>调用 service</li><li>不包含核心逻辑</li></ul><p>在功能简单时，这种分层似乎有些“多余”。</p><p>但当涉及到任务调度、执行记录、重试机制时，结构边界开始体现价值。</p><p>边界明确之后，功能扩展基本是“局部修改”，而不是结构性调整。</p><hr/><h2>在 bootstrap 中组织依赖关系</h2><p>所有初始化逻辑集中在 <code>bootstrap</code> 包中完成：</p><ol><li>初始化数据库</li><li>创建 repository</li><li>注入到 service</li><li>注入到 handler</li><li>注册路由</li></ol><p>依赖关系在入口处完全展开，而不是在各个文件中隐式创建。</p><p>这种方式带来的最大好处是：</p><ul><li>对象生命周期清晰</li><li>依赖方向可控</li><li>替换实现时改动集中</li></ul><p>在没有使用任何 DI 框架的情况下，通过显式构造函数完成依赖注入，本身就是对依赖关系的一种约束。</p><p>当项目规模不大时，这种方式反而比自动注入更透明。</p><hr/><h2>双队列 Worker Pool 的并发调度模型</h2><p>这个项目的核心之一，是执行 Shell 命令并控制并发数量。</p><p>我实现的是一个“双队列 Worker Pool”结构，主要包含三个核心组件：</p><ol><li><strong>任务生产者（Producer）</strong></li><li><strong>集中式调度器 + Worker Goroutine</strong></li><li><strong>结果处理器（Result Processor）</strong></li></ol><h3>第一层：任务生产者</h3><p>当 Webhook 触发或 Web 面板手动触发任务时，任务被封装为一个结构体，发送到调度队列。</p><p>这一层只负责“生成任务”，不关心执行细节。</p><hr/><h3>第二层：集中式调度器 + Worker Pool</h3><p>调度器内部维护：</p><ul><li>一个任务输入队列</li><li>一个固定数量的 worker goroutine</li></ul><p>调度流程：</p><ul><li>调度器从任务队列中取出任务</li><li>分发给空闲 worker</li><li>worker 执行 Shell 命令</li><li>将执行结果发送到结果队列</li></ul><p>worker 数量可控，因此系统并发是有上限的。</p><p>这种结构的优点：</p><ul><li>并发可控</li><li>不会因为 Webhook 高频触发而无限创建 goroutine</li><li>任务调度逻辑集中管理</li></ul><p>相比“每来一个请求直接开 goroutine 执行”的写法，这种结构在可控性和可扩展性上更好。</p><hr/><h3>第三层：结果处理器</h3><p>worker 不直接写数据库，而是把结果推送到结果队列。</p><p>结果处理器负责：</p><ul><li>更新执行记录</li><li>写入数据库</li><li>处理重试逻辑（如果有）</li></ul><p>这样做的目的，是进一步解耦：</p><ul><li>执行逻辑专注执行</li><li>持久化逻辑专注记录</li></ul><p>这就是“双队列”的意义：</p><ul><li>队列一：任务调度</li><li>队列二：结果处理</li></ul><p>这种分离在系统规模变大时尤为重要，因为执行耗时和持久化耗时是两个不同维度的问题。</p><hr/><h2>Makefile 作为构建入口</h2><p>项目使用 Makefile 统一管理：</p><ul><li>后端构建</li><li>前端构建</li><li>交叉编译</li><li>发布打包</li></ul><p>Makefile 在这里的意义并不是“少打几行命令”，而是：</p><ul><li>所有构建流程被显式记录</li><li>新环境下可直接复现</li><li>发布步骤标准化</li></ul><p>当一个项目开始涉及前后端协作、交叉编译和发布时，构建流程本身就成为项目的一部分。</p><hr/><h2>使用 embed 将前端资源打包进二进制</h2><p>这是我在这个项目中感受最明显的“Go 工程优势”。</p><p>前端使用 Vue 构建完成后，静态资源通过 <code>embed</code> 打包进 Go 二进制中。</p><p>然后通过：</p><pre><code class="go">http.FileServer(http.FS(...))</code></pre><p>直接提供访问。</p><p>最终效果是：</p><ul><li>只有一个可执行文件</li><li>不需要 Node 环境</li><li>不需要单独部署前端</li><li>不依赖外部静态文件目录</li></ul><p>从架构上看，它仍然是前后端分离：</p><ul><li>前端独立开发</li><li>后端提供 API</li></ul><p>但从交付形态看，它又像是传统单体应用：</p><ul><li>单文件分发</li><li>直接运行</li></ul><p>这种组合非常适合工具型项目和内部服务。</p><p>Go 在这一点上确实有明显优势：编译后就是完整产物，不需要运行时环境，不依赖包管理器，不依赖额外解释器。</p><p>分发成本几乎为零。</p><hr/><h2>写在最后</h2><p>这个项目没有刻意追求复杂设计，也没有引入额外框架。</p><p>它更像是一次完整的工程实践：把分层、依赖组织、并发控制、构建管理这些已经学过的能力组合在一起，形成一个可长期运行的系统。</p><p>我自己已经在实际环境中持续使用它，用来自动化部署和执行脚本，稳定性和可维护性都符合预期。对我来说，它已经从“练手项目”变成了日常工具。</p><p>如果你刚好也需要一个简单的 GitHub Webhook 执行工具，可以直接拿去用；</p><p>如果你正在学习 Go，想找一个结构完整、但复杂度可控的小项目作为参考，也可以看看实现细节。</p><p>GitHub 仓库地址：<a href="https://link.segmentfault.com/?enc=ZWyF5brWdytMPOmG5kvQjw%3D%3D.BqtnrQ%2FzXUZZOY098ej7iTsv3%2FCVKKLZn60p0Vpe3tG7%2Fr%2B4dTXn116JOe0iwFOH1GxoR7eQauoj6PvnxdxMJQ%3D%3D" rel="nofollow" target="_blank">点击查看</a></p><p>有问题或者想法，也欢迎直接在 GitHub 上交流。</p>]]></description></item><item>    <title><![CDATA[【节点】[Ambient节点]原理解析与实际应用 SmalBox ]]></title>    <link>https://segmentfault.com/a/1190000047608295</link>    <guid>https://segmentfault.com/a/1190000047608295</guid>    <pubDate>2026-02-12 20:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=BzrvDuW4Q4cBpdzm5NzZyg%3D%3D.G130zWMTp7SVzM6Dcej7799yCu7kouSaz1rV9fxmiNMXqw8fPVesPUEoi%2FO0uqxEkXndNfTUaFeEmUE46f0vXHH3HVUHEpbbxVUSG0O%2BL6eISQ7%2F6A%2BbEZaXVk4z5rI9oqKK88f2wa2IEma8ADsp18ITXRe1NxJHFCw%2FH4dbtpvpnPtlp1kjTYCxVArvtreepjjMy6vDKelRYF8yFmLveX9MtzUcx6uH6Vo2hOcFovs%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong></blockquote><p>在Unity的Shader Graph中，Ambient节点是一个重要的环境光照访问工具，它允许着色器获取场景中的环境光照信息。环境光照是全局照明的重要组成部分，能够为场景中的物体提供基础照明，模拟间接光照效果，增强场景的真实感和深度。</p><p>Ambient节点的核心功能是提供对Unity场景环境光照设置的访问。在Unity中，环境光照可以通过Window &gt; Rendering &gt; Lighting &gt; Environment面板进行配置。Ambient节点将这些设置暴露给Shader Graph，使得着色器能够根据场景的环境光照设置动态调整材质的外观。</p><h2>描述</h2><p>Ambient节点的主要作用是允许着色器访问场景的环境颜色值。这个节点的行为取决于Unity Lighting窗口中的Environment Lighting Source设置。当Environment Lighting Source设置为Gradient时，节点的Color/Sky端口将返回Sky Color值；当设置为Color时，Color/Sky端口将返回Ambient Color值。</p><p>无论Environment Lighting Source设置为何值，Equator和Ground端口都会始终分别返回Equator Color和Ground Color值。这种设计使得着色器能够灵活地适应不同的环境光照配置，同时保持对特定环境颜色成分的访问。</p><p>需要注意的是，Ambient节点的值更新时机是有限的。仅当进入运行模式或保存当前场景/项目时，才会更新此节点的值。这意味着在编辑模式下修改环境光照设置时，Shader Graph中的Ambient节点可能不会立即反映这些变化，直到执行上述操作之一。</p><p>另一个重要注意事项是，此节点的行为未在全局范围内统一定义。Shader Graph本身并不定义此节点的具体函数实现，而是由每个渲染管线为此节点定义要执行的HLSL代码。这意味着不同的渲染管线可能会产生不同的结果，这是在使用Ambient节点时需要特别注意的。</p><h3>环境光照源类型详解</h3><p>Unity中的环境光照源主要有两种配置方式，每种方式都会影响Ambient节点的输出结果：</p><ul><li><strong>Color模式</strong>：当Environment Lighting Source设置为Color时，环境光照使用单一颜色值。这种模式下，Ambient节点的Color/Sky端口将返回在Lighting窗口中设置的Ambient Color值。这种配置适用于需要简单、统一环境照明的场景，或者风格化渲染中。</li><li><strong>Gradient模式</strong>：当选择Gradient模式时，环境光照使用三种颜色组成的渐变：Sky Color（天空颜色）、Equator Color（赤道颜色）和Ground Color（地面颜色）。这种模式下，Ambient节点的Color/Sky端口返回Sky Color，而Equator和Ground端口分别返回对应的颜色值。这种配置能够创建更加自然的环境光照效果，模拟从天空到地面的颜色过渡。</li></ul><h3>使用限制与注意事项</h3><p>Ambient节点在使用中有几个重要的限制需要了解：</p><ul><li><strong>值更新时机</strong>：Ambient节点的值不会实时更新。只有在进入运行模式或保存场景/项目时，节点才会更新其输出值。这意味着在编辑模式下调整环境光照设置时，需要执行这些操作之一才能看到更新后的效果。</li><li><strong>渲染管线依赖性</strong>：此节点的行为完全依赖于所使用的渲染管线。不同的渲染管线可能实现不同的环境光照计算方式，导致相同的着色器在不同管线中产生不同的视觉效果。</li><li><strong>跨管线兼容性</strong>：如果计划构建需要在多个渲染管线中使用的着色器，务必在实际应用前在两个管线中都进行检查测试。某些节点可能在一个渲染管线中已定义，而在另一个中未定义。</li><li><strong>未定义行为处理</strong>：如果Ambient节点在某个渲染管线中未定义，它将返回0（黑色）。这可能导致着色器显示异常，因此在跨管线开发时需要特别注意。</li></ul><h2>支持的渲染管线</h2><p>Ambient节点的支持情况因渲染管线而异：</p><ul><li><strong>通用渲染管线（URP）</strong>：完全支持Ambient节点。在URP中，Ambient节点能够正确访问场景的环境光照设置，并根据Environment Lighting Source配置返回相应的颜色值。</li><li><strong>高清渲染管线（HDRP）</strong>：不支持Ambient节点。HDRP使用不同的环境光照系统，因此需要采用其他方法访问环境光照信息。在HDRP中，通常使用HDRI天空或物理天空系统，并通过不同的节点或方式访问环境光照。</li><li><strong>内置渲染管线</strong>：在传统的内置渲染管线中，Ambient节点通常能够正常工作，但具体行为可能因Unity版本而异。</li></ul><p>了解所在渲染管线对Ambient节点的支持情况至关重要，特别是在进行跨管线项目开发或着色器资源迁移时。如果需要在HDRP中实现类似环境光照访问的功能，通常需要探索HDRP特定的节点和光照访问方法。</p><h2>端口</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608297" alt="" title=""/></p><p>Ambient节点提供三个输出端口，每个端口都输出Vector 3类型的三维向量，表示RGB颜色值。这些端口使着色器能够访问环境光照的不同组成部分，为材质提供丰富的环境光照信息。</p><h3>Color/Sky 端口</h3><p>Color/Sky端口是Ambient节点的主要输出端口，其行为随Environment Lighting Source设置而变化：</p><ul><li>当Environment Lighting Source设置为Color时，此端口返回Ambient Color值</li><li>当Environment Lighting Source设置为Gradient时，此端口返回Sky Color值</li><li>输出类型为Vector 3，包含RGB颜色分量</li><li>这是最常用的环境光照访问端口，通常用于提供材质的基础环境照明</li></ul><h3>Equator 端口</h3><p>Equator端口提供对环境光照中赤道颜色成分的访问：</p><ul><li>无论Environment Lighting Source设置为何值，此端口始终返回Equator Color值</li><li>在Gradient模式下，Equator Color表示天空与地面之间的中间颜色</li><li>在Color模式下，Equator Color仍然可用，但通常与Ambient Color相同或类似</li><li>输出类型为Vector 3，可用于创建更复杂的环境光照响应效果</li></ul><h3>Ground 端口</h3><p>Ground端口专门用于访问环境光照中的地面颜色：</p><ul><li>无论Environment Lighting Source设置为何值，此端口始终返回Ground Color值</li><li>在Gradient模式下，Ground Color表示场景底部的环境颜色，模拟地面反射的光照</li><li>在Color模式下，Ground Color仍然可用，但通常与Ambient Color相同或类似</li><li>输出类型为Vector 3，适用于需要区分上下表面环境照明的材质</li></ul><h3>端口使用策略</h3><p>理解这些端口的特性和行为对于有效使用Ambient节点至关重要：</p><ul><li><strong>动态行为</strong>：Color/Sky端口的动态特性使其能够适应不同的环境光照配置，但这也意味着着色器在不同配置下可能产生不同的视觉效果</li><li><strong>一致性保证</strong>：Equator和Ground端口的一致行为使得着色器能够可靠地访问这些特定的环境颜色成分，无论整体环境光照如何配置</li><li><strong>数据绑定</strong>：这些端口均无特定绑定，直接输出颜色值，可以连接到任何接受Vector 3输入的节点，如颜色混合、光照计算或材质参数</li></ul><h2>环境光照配置与Ambient节点的关系</h2><p>要充分利用Ambient节点，需要深入理解Unity环境光照系统的工作原理及其与节点的交互方式。环境光照不仅影响场景的整体亮度，还极大地影响材质的视觉表现和场景的氛围。</p><h3>Environment Lighting Source配置</h3><p>Environment Lighting Source是控制环境光照行为的核心设置，位于Lighting窗口的Environment部分。这一设置直接影响Ambient节点的输出：</p><ul><li><p><strong>Color模式配置</strong>：</p><ul><li>设置单一的Ambient Color，影响整个场景的环境光照</li><li>Ambient Intensity控制环境光的强度</li><li>在这种模式下，Ambient节点的Color/Sky端口直接返回Ambient Color值</li><li>适用于风格化场景或性能要求较高的项目</li></ul></li><li><p><strong>Gradient模式配置</strong>：</p><ul><li>设置三个颜色值：Sky、Equator和Ground</li><li>创建从天空到地面的颜色渐变，模拟更自然的环境光照</li><li>Ambient节点的三个端口分别对应这三个颜色值</li><li>Intensity控制整体环境光强度</li><li>适用于追求真实照明的场景</li></ul></li><li><p><strong>Skybox模式</strong>：</p><ul><li>使用指定的天空盒材质提供环境光照</li><li>环境颜色从天空盒动态采样计算</li><li>Ambient节点在这种模式下的行为可能因渲染管线而异</li><li>提供最真实的环境光照效果，但计算成本较高</li></ul></li></ul><h3>环境反射与环境光照</h3><p>除了直接的环境光照，Unity还提供了环境反射设置，与环境光照协同工作：</p><ul><li><strong>Source设置</strong>：可以选择Skybox或Custom提供环境反射</li><li><p><strong>Resolution</strong>：控制环境反射贴图的分辨率</p><ul><li><strong>Compression</strong>：设置环境反射贴图的压缩方式</li><li><strong>Intensity</strong>：控制环境反射的强度，影响材质的反射效果</li></ul></li></ul><p>环境反射与环境光照共同作用，决定了材质如何响应场景的全局照明。Ambient节点主要关注环境光照（直接照明），而环境反射通常通过反射探头或天空盒单独处理。</p><h3>实时更新与烘焙考虑</h3><p>环境光照的设置还与光照烘焙方式相关：</p><ul><li><strong>Realtime环境光照</strong>：动态变化的环境光照会实时影响Ambient节点的输出</li><li><strong>Baked环境光照</strong>：烘焙到光照贴图的环境光照在运行时不变，Ambient节点输出相应固定值</li><li><strong>Mixed光照</strong>：结合实时和烘焙特性，Ambient节点可能需要特殊处理</li></ul><p>理解这些光照模式对于预测Ambient节点在不同场景中的行为非常重要，特别是在涉及动态光照变化或昼夜循环的项目中。</p><h2>实际应用示例</h2><p>Ambient节点在Shader Graph中有多种实际应用，从简单的颜色调整到复杂的环境响应效果。以下是一些常见的应用场景和实现方法。</p><h3>基础环境光照应用</h3><p>最基本的应用是将环境光照直接应用于材质：</p><ul><li>创建Unlit Master节点，将Ambient节点的Color/Sky端口直接连接到Base Color输入</li><li>这样材质将完全由环境光照着色，随着环境光照设置的变化而改变外观</li><li>适用于需要完全环境照明的物体，如全息投影或发光体</li></ul><h3>环境敏感材质</h3><p>创建根据环境光照改变外观的智能材质：</p><ul><li>使用Ambient节点的输出控制材质的颜色、亮度或反射率</li><li>例如，将环境光照强度与材质发射强度相乘，创建在明亮环境中较暗、在黑暗环境中较亮的自发光材质</li><li>可以使用 Separate RGB 节点分离环境颜色分量，分别控制材质的不同属性</li></ul><h3>三色环境混合</h3><p>利用Ambient节点的三个输出端口创建复杂的环境响应：</p><ul><li>根据表面法线方向在Sky、Equator和Ground颜色之间混合</li><li>使用Normal Vector节点获取表面法线，通过Dot Product计算法线与世界空间向上方向的点积</li><li>根据点积结果使用Lerp节点在三色之间混合，创建与方向相关的环境着色</li></ul><h3>环境遮蔽增强</h3><p>结合环境遮蔽贴图增强环境光照效果：</p><ul><li>将Ambient节点输出与AO贴图相乘，创建更加真实的环境光照响应</li><li>在凹处和遮蔽区域减少环境光照影响，增强场景的深度感和立体感</li><li>可以使用Multiply节点简单混合，或使用更复杂的混合函数实现特定效果</li></ul><h3>动态材质调整</h3><p>通过脚本动态调整环境光照，并观察材质响应：</p><ul><li>在运行时通过Lighting API修改环境光照设置</li><li>观察材质如何实时响应这些变化（注意Ambient节点的更新限制）</li><li>适用于需要程序化控制场景氛围或实现昼夜循环的项目</li></ul><h2>生成的代码示例</h2><p>Ambient节点在生成的着色器代码中对应特定的HLSL宏或变量。理解这些生成的代码有助于深入理解节点的行为，并在需要时进行手动调整或优化。</p><h3>标准生成代码</h3><p>典型的Ambient节点生成代码如下：</p><pre><code>float3 _Ambient_ColorSky = SHADERGRAPH_AMBIENT_SKY;
float3 _Ambient_Equator = SHADERGRAPH_AMBIENT_EQUATOR;
float3 _Ambient_Ground = SHADERGRAPH_AMBIENT_GROUND;</code></pre><p>这段代码声明了三个float3变量，分别对应Ambient节点的三个输出端口。这些变量通过特定的宏（SHADERGRAPH_AMBIENT_SKY等）获取实际的环境光照值。</p><h3>宏定义与渲染管线差异</h3><p>不同渲染管线为这些环境光照宏提供了不同的实现：</p><ul><li><strong>通用渲染管线（URP）</strong>：这些宏通常指向URP着色器库中定义的环境光照变量</li><li><strong>内置渲染管线</strong>：可能使用Unity内置的着色器变量，如UNITY_LIGHTMODEL_AMBIENT</li><li><strong>自定义实现</strong>：在某些情况下，可能需要手动定义这些宏以提供自定义环境光照行为</li></ul><h3>代码集成示例</h3><p>在实际着色器中，Ambient节点生成的代码会与其他着色器代码集成：</p><pre><code>// Ambient节点生成的变量
float3 _Ambient_ColorSky = SHADERGRAPH_AMBIENT_SKY;
float3 _Ambient_Equator = SHADERGRAPH_AMBIENT_EQUATOR;
float3 _Ambient_Ground = SHADERGRAPH_AMBIENT_GROUND;

// 表面着色器函数
void SurfaceFunction_float(float3 Normal, out float3 Out)
{
    // 基于法线方向混合环境颜色
    float skyFactor = saturate(dot(Normal, float3(0, 1, 0)));
    float groundFactor = saturate(dot(Normal, float3(0, -1, 0)));
    float equatorFactor = 1.0 - skyFactor - groundFactor;

    // 混合环境颜色
    Out = _Ambient_ColorSky * skyFactor +
          _Ambient_Equator * equatorFactor +
          _Ambient_Ground * groundFactor;
}</code></pre><p>这个示例展示了如何利用Ambient节点生成的变量创建基于法线方向的环境颜色混合效果。</p><h2>故障排除与最佳实践</h2><p>使用Ambient节点时可能会遇到各种问题，了解常见问题及其解决方案非常重要。同时，遵循一些最佳实践可以确保环境光照在着色器中的正确应用。</p><h3>常见问题与解决方案</h3><ul><li><p><strong>问题：Ambient节点返回黑色</strong></p><ul><li>可能原因：渲染管线不支持Ambient节点</li><li>解决方案：检查当前渲染管线，考虑使用替代方案或切换至支持的管线</li><li>可能原因：环境光照未正确设置</li><li>解决方案：检查Lighting窗口中的环境光照设置，确保已配置有效的环境颜色或渐变</li></ul></li><li><p><strong>问题：环境光照不更新</strong></p><ul><li>可能原因：Ambient节点值更新限制</li><li>解决方案：进入运行模式或保存场景/项目以更新节点值</li><li>可能原因：环境光照设置为Baked且未重新烘焙</li><li>解决方案：重新烘焙光照或切换至Realtime环境光照</li></ul></li><li><p><strong>问题：不同平台表现不一致</strong></p><ul><li>可能原因：不同平台对环境光照的支持差异</li><li>解决方案：在所有目标平台上测试着色器，必要时添加平台特定处理</li><li>可能原因：移动设备性能限制导致环境光照简化</li><li>解决方案：为移动设备使用简化的环境光照模型</li></ul></li></ul><h3>性能优化建议</h3><p>环境光照访问通常性能开销较低，但在某些情况下仍需注意优化：</p><ul><li>避免在片段着色器中频繁进行复杂的环境光照计算</li><li>考虑在顶点着色器中计算环境光照，并通过插值传递到片段着色器</li><li>对于静态物体，可以考虑将环境光照烘焙到顶点颜色或光照贴图中</li><li>在性能敏感的平台（如移动设备）上，使用简化的环境光照模型</li></ul><h3>跨管线兼容性策略</h3><p>确保着色器在多个渲染管线中正常工作：</p><ul><li>在目标渲染管线中早期测试Ambient节点的行为</li><li>使用Shader Graph的Node Library功能检查节点在不同管线中的可用性</li><li>考虑为不支持Ambient节点的管线提供回退实现</li><li>使用Custom Function节点编写特定于管线的环境光照代码</li></ul><h3>版本兼容性注意事项</h3><p>不同Unity版本可能对环境光照系统和Ambient节点有所改变：</p><ul><li>在升级Unity版本时，检查环境光照相关的新功能或变更</li><li>注意不同版本间渲染管线的更新可能影响Ambient节点的行为</li><li>定期查看Unity官方文档和更新日志，了解相关变更</li></ul><h2>高级应用技巧</h2><p>一旦掌握了Ambient节点的基本原理，可以探索一些高级应用技巧，创建更加复杂和有趣的环境响应效果。</p><h3>动态环境响应</h3><p>创建根据环境条件动态调整的材质：</p><ul><li>使用Time节点结合环境光照创建脉动或呼吸效果</li><li>根据环境亮度自动调整材质的发射强度或反射率</li><li>使用场景中的光源信息与环境光照结合，创建更加真实的照明响应</li></ul><h3>风格化环境着色</h3><p>利用环境光照创建非真实感渲染效果：</p><ul><li>将环境颜色转换为灰度，用于卡通着色中的阴影区域</li><li>使用Posterize节点量化环境光照，创建色块化效果</li><li>通过自定义曲线重新映射环境光照强度，实现特定的艺术风格</li></ul><h3>环境光照遮罩</h3><p>创建只影响特定区域的环境光照效果：</p><ul><li>使用贴图或程序化生成的遮罩控制环境光照的应用区域</li><li>结合顶点颜色或UV坐标创建复杂的环境光照分布</li><li>使用世界空间位置驱动环境光照强度，模拟局部环境效果</li></ul><h3>多环境系统集成</h3><p>将Ambient节点与其他环境系统结合：</p><ul><li>与环境反射探头结合，创建完整的环境响应材质</li><li>与光照探头代理体积（LPPV）集成，实现动态环境光照</li><li>结合全局光照系统，创建更加真实的材质外观</li></ul><hr/><blockquote><a href="https://link.segmentfault.com/?enc=UQxH5G5yzeXGIyeGOqcc1A%3D%3D.9qCaYo7PcFMg3ffX1MJBtY3XCzDFHU86LlP2xu%2Bg%2BEq%2FUF4Jb8mOqI527tBY8y3x6fPwMS8LeiJhlzQcAyEQZ%2FXu7ecY0AU6l1fXMJqf6jkkspTnvua6KSsTMGfPfmKstd%2Fu6spOTpA24RF9jle1S402FrQFZ1whs8XR40ANnfaA%2Bd3fCwY%2BMYS38CBeVOrCp9DrlUJ9CNXlg4bZ6V%2BzmTKzax7xDOiLxGf8HtmG2sg%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[SQLAlchemy 技术入门指南 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047608306</link>    <guid>https://segmentfault.com/a/1190000047608306</guid>    <pubDate>2026-02-12 20:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 库的概览与核心价值</h2><p>想象一下，在编写 Web 应用或数据处理程序时，如果需要直接使用 SQL 语句与数据库交互，就像在高速公路上骑着自行车——虽然能够到达目的地，但不仅效率低下，而且随时可能因为写错一个关键字而导致整个程序崩溃。<code>SQLAlchemy</code> 正是为解决这个痛点而生的 Python 数据库工具包。</p><p><strong>SQLAlchemy</strong> 是 Python 中最流行的 SQL 工具包和对象关系映射器（ORM），它在 Python 生态系统中占据着不可替代的地位。简单来说，SQLAlchemy 提供了两种使用方式：</p><ul><li><strong>Core（核心层）</strong>：一个灵活的 SQL 表达式语言，允许你用 Python 代码构造 SQL 语句</li><li><strong>ORM（对象关系映射）</strong>：将数据库表映射为 Python 类，让你可以像操作普通对象一样操作数据库</li></ul><p>SQLAlchemy 的核心价值在于：</p><ul><li><strong>桥接 Python 和 SQL 的鸿沟</strong>：用 Python 的思维来操作数据库，无需编写复杂的 SQL</li><li><strong>数据库无关性</strong>：支持 MySQL、PostgreSQL、SQLite、Oracle 等多种数据库，代码无需修改即可切换</li><li><strong>企业级特性</strong>：提供连接池、事务管理、关系映射等高级功能</li><li><strong>渐进式学习</strong>：可以从简单的 Core 开始，逐步学习强大的 ORM 功能</li></ul><h2>2. 环境搭建与 "Hello, World"</h2><h3>安装说明</h3><p>安装 SQLAlchemy 非常简单，使用 pip 即可：</p><pre><code class="bash"># 安装最新稳定版
pip install SQLAlchemy

# 安装指定版本（如 2.0.x）
pip install SQLAlchemy==2.0.45

# 安装预发布版本（用于测试新特性）
pip install --pre SQLAlchemy</code></pre><p>如果需要支持特定数据库，还需要安装对应的 DBAPI（以 PostgreSQL 为例）：</p><pre><code class="bash">pip install psycopg2-binary  # PostgreSQL
pip install pymysql          # MySQL
pip install cx-Oracle        # Oracle</code></pre><h3>最简示例</h3><p>让我们通过一个完整的 "Hello World" 示例来快速了解 SQLAlchemy 的基本用法：</p><pre><code class="python">from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import DeclarativeBase, Session

# 1. 定义基础类
class Base(DeclarativeBase):
    pass

# 2. 定义模型（映射到数据库表）
class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    name = Column(String(50), nullable=False)
    email = Column(String(100), unique=True)

# 3. 创建数据库引擎（使用 SQLite）
engine = create_engine('sqlite:///example.db', echo=True)

# 4. 创建表结构
Base.metadata.create_all(engine)

# 5. 插入数据
with Session(engine) as session:
    # 创建新用户对象
    new_user = User(name='张三', email='zhangsan@example.com')
    
    # 添加到会话
    session.add(new_user)
    
    # 提交到数据库
    session.commit()
    
    # 查询数据
    users = session.query(User).all()
    for user in users:
        print(f'ID: {user.id}, 姓名: {user.name}, 邮箱: {user.email}')</code></pre><h3>代码逐行解释</h3><ul><li><strong>第 1-3 行</strong>：导入必要的模块。<code>create_engine</code> 用于建立数据库连接，<code>Column</code> 等用于定义表结构</li><li><strong>第 6-7 行</strong>：定义 <code>Base</code> 类，这是所有 ORM 模型的基类</li><li><strong>第 10-16 行</strong>：定义 <code>User</code> 类，它对应数据库中的 <code>users</code> 表。<code>__tablename__</code> 指定表名，各个 <code>Column</code> 定义表字段</li><li><strong>第 19 行</strong>：创建数据库引擎，<code>sqlite:///example.db</code> 表示使用 SQLite 数据库（文件名：example.db），<code>echo=True</code> 会打印执行的 SQL 语句，便于调试</li><li><strong>第 22 行</strong>：根据模型定义创建所有表（如果表已存在则跳过）</li><li><strong>第 25 行</strong>：创建会话（Session），会话是 ORM 与数据库交互的桥梁</li><li><strong>第 28 行</strong>：创建一个 User 对象，相当于创建了一条记录</li><li><strong>第 30 行</strong>：将对象添加到会话的"待提交区"</li><li><strong>第 32 行</strong>：提交事务，将数据真正写入数据库</li><li><strong>第 35-37 行</strong>：查询所有用户并打印</li></ul><p><strong>运行结果</strong>：</p><pre><code>ID: 1, 姓名: 张三, 邮箱: zhangsan@example.com</code></pre><p><strong>常见安装问题</strong>：</p><ul><li>如果安装失败，尝试使用国内镜像：<code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simple SQLAlchemy</code></li><li>Windows 用户安装某些数据库驱动可能需要 Visual C++ 运行时库</li></ul><h2>3. 核心概念解析</h2><p>SQLAlchemy 的架构清晰，核心概念主要包括以下几个部分：</p><h3>3.1 Engine（引擎）</h3><p><code>Engine</code> 是 SQLAlchemy 的心脏，负责管理与数据库的连接池和实际通信。</p><pre><code class="python">from sqlalchemy import create_engine

# 创建引擎
engine = create_engine('postgresql://user:password@localhost/mydatabase')

# 引擎本身不直接连接数据库，而是在需要时从连接池中获取连接</code></pre><p><strong>关键点</strong>：</p><ul><li>Engine 是线程安全的，一个应用程序通常只需要一个 Engine 实例</li><li>它维护一个连接池，自动管理数据库连接的创建和复用</li></ul><h3>3.2 Session（会话）</h3><p><code>Session</code> 是 ORM 的核心，实现了工作单元（Unit of Work）模式。它负责：</p><ul><li>跟踪所有被加载或创建的对象</li><li>记录这些对象的状态变化</li><li>在提交时将所有变更一次性同步到数据库</li></ul><pre><code class="python">from sqlalchemy.orm import Session

# 创建会话
with Session(engine) as session:
    # 会话内部的工作
    pass</code></pre><h3>3.3 Model（模型）</h3><p>模型（或称为映射类）是数据库表的 Python 表示。每个模型类都继承自 <code>DeclarativeBase</code>（SQLAlchemy 2.0+）或使用 <code>declarative_base</code>（旧版本）。</p><h3>3.4 概念关系图</h3><pre style="display:none;"><code class="mermaid">graph TD
    A[Engine 引擎] --&gt;|管理连接池| B[Connection 连接]
    B --&gt;|执行SQL| C[Database 数据库]
    
    A --&gt;|创建| D[Session 会话]
    D --&gt;|操作| E[Model 模型]
    
    D --&gt;|跟踪状态变化| F[Unit of Work 工作单元]
    F --&gt;|提交事务| B
    
    E --&gt;|映射到| G[Table 表结构]
    G --&gt;|包含| H[Column 列]
    H --&gt;|可关联| I[Relationship 关系]
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style E fill:#ffe1e1</code></pre><p><strong>概念间的关系</strong>：</p><ol><li><strong>Engine → Session</strong>：Session 基于 Engine 创建，使用 Engine 的连接池</li><li><strong>Session → Model</strong>：Session 负责管理 Model 对象的生命周期</li><li><strong>Model → Table</strong>：Model 类通过声明式映射到数据库表</li><li><strong>Session → Unit of Work</strong>：Session 内部实现了工作单元模式，自动跟踪对象变化</li></ol><p><strong>工作流程</strong>：</p><ol><li>创建 Engine（连接数据库）</li><li>定义 Model（定义表结构）</li><li>创建 Session（交互桥梁）</li><li>操作 Model 对象（CRUD 操作）</li><li>Session.commit()（提交事务）</li></ol><h2>4. 实战演练：构建博客系统的文章管理</h2><p>让我们通过一个完整的实战项目——博客文章管理系统——来综合运用 SQLAlchemy 的核心功能。</p><h3>需求分析</h3><p>我们需要实现一个简单的博客系统，具备以下功能：</p><ul><li>存储文章信息（标题、内容、发布时间）</li><li>存储作者信息（用户名、邮箱）</li><li>每篇文章关联一个作者（一对多关系）</li><li>支持增删改查（CRUD）操作</li></ul><h3>方案设计</h3><p>我们将使用 SQLAlchemy 的 ORM 功能：</p><ul><li>创建两个模型：<code>Author</code>（作者）和 <code>Article</code>（文章）</li><li>使用 <code>relationship</code> 建立一对多关系</li><li>使用 <code>Session</code> 完成数据持久化和查询</li></ul><h3>代码实现</h3><pre><code class="python">from sqlalchemy import create_engine, Column, Integer, String, DateTime, ForeignKey, func
from sqlalchemy.orm import DeclarativeBase, Session, relationship
from datetime import datetime

# 1. 定义基础类
class Base(DeclarativeBase):
    pass

# 2. 定义 Author 模型（作者表）
class Author(Base):
    __tablename__ = 'authors'
    
    id = Column(Integer, primary_key=True)
    username = Column(String(50), nullable=False, unique=True)
    email = Column(String(100), nullable=False, unique=True)
    
    # 一对多关系：一个作者可以有多篇文章
    articles = relationship('Article', back_populates='author', cascade='all, delete-orphan')

# 3. 定义 Article 模型（文章表）
class Article(Base):
    __tablename__ = 'articles'
    
    id = Column(Integer, primary_key=True)
    title = Column(String(200), nullable=False)
    content = Column(String, nullable=False)
    publish_time = Column(DateTime, default=func.now())
    author_id = Column(Integer, ForeignKey('authors.id'), nullable=False)
    
    # 反向关系：文章属于一个作者
    author = relationship('Author', back_populates='articles')

# 4. 创建数据库引擎
engine = create_engine('sqlite:///blog.db', echo=False)

# 5. 创建表结构
Base.metadata.create_all(engine)

# 6. 实战操作：完整的 CRUD 流程
with Session(engine) as session:
    
    # ========== 创建（Create）==========
    print("=== 创建作者和文章 ===")
    
    # 创建作者
    author1 = Author(username='小明', email='xiaoming@example.com')
    author2 = Author(username='小红', email='xiaohong@example.com')
    
    # 创建文章并关联作者
    article1 = Article(
        title='SQLAlchemy 入门指南',
        content='SQLAlchemy 是一个强大的 Python ORM 框架...',
        author=author1
    )
    article2 = Article(
        title='Python 高级编程技巧',
        content='装饰器、生成器、上下文管理器...',
        author=author1
    )
    article3 = Article(
        title='Web 开发最佳实践',
        content='RESTful API 设计原则...',
        author=author2
    )
    
    # 批量添加到会话
    session.add_all([author1, author2, article1, article2, article3])
    session.commit()
    
    print(f"✓ 已创建 2 位作者和 3 篇文章\n")
    
    # ========== 读取（Read）==========
    print("=== 查询文章 ===")
    
    # 查询所有文章
    all_articles = session.query(Article).all()
    for article in all_articles:
        print(f"文章: {article.title}")
        print(f"  作者: {article.author.username}")
        print(f"  发布时间: {article.publish_time}")
        print()
    
    # 查询某位作者的所有文章
    print("=== 查询小明的所有文章 ===")
    ming_articles = session.query(Article).join(Author).filter(Author.username == '小明').all()
    for article in ming_articles:
        print(f"- {article.title}")
    print()
    
    # ========== 更新（Update）==========
    print("=== 更新文章 ===")
    
    # 找到要更新的文章
    article_to_update = session.query(Article).filter(Article.title == 'SQLAlchemy 入门指南').first()
    article_to_update.title = 'SQLAlchemy 2.0 完全指南（更新版）'
    article_to_update.content = '本文将深入介绍 SQLAlchemy 2.0 的新特性...'
    session.commit()
    
    print(f"✓ 已更新文章标题为：{article_to_update.title}\n")
    
    # ========== 删除（Delete）==========
    print("=== 删除文章 ===")
    
    # 删除某篇文章
    article_to_delete = session.query(Article).filter(Article.title == 'Web 开发最佳实践').first()
    session.delete(article_to_delete)
    session.commit()
    
    print("✓ 已删除文章：Web 开发最佳实践\n")
    
    # ========== 最终统计 ==========
    print("=== 最终统计 ===")
    print(f"作者数量: {session.query(Author).count()}")
    print(f"文章数量: {session.query(Article).count()}")
    print(f"小明的文章数: {len(ming_articles)}")</code></pre><h3>运行说明</h3><ol><li>将上述代码保存为 <code>blog_demo.py</code></li><li>确保已安装 SQLAlchemy：<code>pip install SQLAlchemy</code></li><li>运行程序：<code>python blog_demo.py</code></li></ol><h3>运行结果</h3><pre><code>=== 创建作者和文章 ===
✓ 已创建 2 位作者和 3 篇文章

=== 查询文章 ===
文章: SQLAlchemy 入门指南
  作者: 小明
  发布时间: 2026-02-03 11:05:58

文章: Python 高级编程技巧
  作者: 小明
  发布时间: 2026-02-03 11:05:58

文章: Web 开发最佳实践
  作者: 小红
  发布时间: 2026-02-03 11:05:58

=== 查询小明的所有文章 ===
- SQLAlchemy 入门指南
- Python 高级编程技巧

=== 更新文章 ===
✓ 已更新文章标题为：SQLAlchemy 2.0 完全指南（更新版）

=== 删除文章 ===
✓ 已删除文章：Web 开发最佳实践

=== 最终统计 ===
作者数量: 2
文章数量: 2
小明的文章数: 2</code></pre><h3>关键知识点</h3><ol><li><strong>关系映射</strong>：使用 <code>relationship</code> 定义模型间的关系，<code>back_populates</code> 实现双向关联</li><li><strong>级联删除</strong>：<code>cascade='all, delete-orphan'</code> 表示删除作者时自动删除其所有文章</li><li><strong>默认值</strong>：<code>default=func.now()</code> 使用数据库函数自动填充发布时间</li><li><strong>外键约束</strong>：<code>ForeignKey</code> 确保数据完整性</li><li><strong>链式查询</strong>：<code>session.query().join().filter()</code> 构建复杂查询</li></ol><h2>5. 最佳实践与常见陷阱</h2><h3>常见错误与规避方法</h3><h4>错误 1：忘记提交事务</h4><pre><code class="python"># ❌ 错误做法
with Session(engine) as session:
    new_user = User(name='张三')
    session.add(new_user)
    # 忘记 session.commit()，数据不会写入数据库！

# ✅ 正确做法
with Session(engine) as session:
    new_user = User(name='张三')
    session.add(new_user)
    session.commit()  # 必须提交！</code></pre><p><strong>原因</strong>：SQLAlchemy 默认开启事务，不调用 <code>commit()</code> 就不会真正写入数据库。</p><h4>错误 2：N+1 查询问题</h4><pre><code class="python"># ❌ 错误做法（会触发 N+1 次查询）
users = session.query(User).all()
for user in users:
    print(user.name, user.orders)  # 每次访问 orders 都会触发一次新查询

# ✅ 正确做法（使用 eager loading 一次性加载）
from sqlalchemy.orm import selectinload
users = session.query(User).options(selectinload(User.orders)).all()
for user in users:
    print(user.name, user.orders)  # 不再触发额外查询</code></pre><p><strong>原因</strong>：懒加载会导致循环中频繁查询数据库，性能极差。</p><h4>错误 3：跨 Session 使用对象</h4><pre><code class="python"># ❌ 错误做法
with Session(engine) as session1:
    user = session1.query(User).first()

with Session(engine) as session2:
    # user 属于 session1，不能在 session2 中使用
    user.name = '新名字'  # 可能报错或无法保存
    session2.commit()

# ✅ 正确做法
with Session(engine) as session1:
    user = session1.query(User).first()
    user_id = user.id

with Session(engine) as session2:
    user = session2.query(User).get(user_id)
    user.name = '新名字'
    session2.commit()</code></pre><p><strong>原因</strong>：每个 Session 维护自己的对象缓存，对象不能跨 Session 使用。</p><h3>最佳实践建议</h3><ol><li><p><strong>使用上下文管理器</strong></p><pre><code class="python"># 推荐
with Session(engine) as session:
    # 操作
    pass
# 自动关闭 session</code></pre></li><li><p><strong>合理设置连接池大小</strong></p><pre><code class="python"># 根据应用并发量调整
engine = create_engine('postgresql://...', pool_size=10, max_overflow=20)</code></pre></li><li><p><strong>使用环境变量存储敏感信息</strong></p><pre><code class="python">import os
DATABASE_URL = os.getenv('DATABASE_URL', 'sqlite:///default.db')
engine = create_engine(DATABASE_URL)</code></pre></li><li><p><strong>添加索引优化查询</strong></p><pre><code class="python">class User(Base):
    email = Column(String(100), unique=True, index=True)  # 为常用查询字段添加索引</code></pre></li><li><p><strong>使用类型提示提高代码质量</strong>（SQLAlchemy 2.0+）</p><pre><code class="python">from typing import Optional
from sqlalchemy.orm import Mapped, mapped_column

class User(Base):
    name: Mapped[str] = mapped_column(String(50))
    age: Mapped[Optional[int]] = mapped_column()  # 可空字段</code></pre></li></ol><h2>6. 进阶指引</h2><h3>高级功能</h3><ul><li><strong>异步支持</strong>：SQLAlchemy 2.0+ 全面支持 <code>asyncio</code>，可使用 <code>AsyncSession</code> 进行异步数据库操作</li><li><strong>混合属性</strong>：结合 Python 属性和 SQL 表达式，定义可计算的字段</li><li><strong>事件监听</strong>：监听对象的创建、修改、删除等事件，实现业务逻辑解耦</li><li><strong>批量操作</strong>：使用 <code>bulk_insert_mappings</code> 等方法进行高性能批量插入</li></ul><h3>生态扩展</h3><ul><li><strong>Alembic</strong>：SQLAlchemy 官方的数据库迁移工具，用于管理数据库 schema 变更</li><li><strong>Flask-SQLAlchemy</strong>：Flask 框架的 SQLAlchemy 集成，简化 Web 开发</li><li><strong>GeoAlchemy2</strong>：支持地理空间数据类型和查询</li></ul><h3>学习路径</h3><ol><li><strong>巩固基础</strong>：熟练掌握 Core 和 ORM 的基本用法</li><li><strong>深入学习关系</strong>：理解各种关系模式（一对一、一对多、多对多）和加载策略</li><li><strong>性能优化</strong>：学习索引、连接池、批量操作等性能优化技巧</li><li><strong>架构设计</strong>：掌握复杂业务场景下的数据模型设计</li><li><strong>源码阅读</strong>：深入理解 SQLAlchemy 的实现原理</li></ol><h3>推荐资源</h3><ul><li><strong>官方文档</strong>：<a href="https://link.segmentfault.com/?enc=2gWbCHGgAFNWhOpNySnDkg%3D%3D.0%2F3Zf30BdzG1BoLfzEMr71IEZyb%2BCE3FrR9vbgEB%2FXQ%3D" rel="nofollow" target="_blank">https://docs.sqlalchemy.org/</a></li><li><strong>SQLAlchemy 2.0 教程</strong>：<a href="https://link.segmentfault.com/?enc=YYdm%2FsMZYYbOEpV36IVSXg%3D%3D.R%2FrQOxw7gYwSOf6n0C8uIf7Qb4R9WwuSBRj3lgcHnN%2Bur7bvp%2BnPnUc5Qtq%2FPEI%2B" rel="nofollow" target="_blank">https://docs.sqlalchemy.org/en/20/tutorial/</a></li><li><strong>中文社区</strong>：<a href="https://link.segmentfault.com/?enc=tXYgTM1MS2shEzA04MUqIg%3D%3D.BrBJ3wL49T8WLUEPewPzuQkt4%2BIgjuK4C5jPVoIV8XgGJedqiroVwV3taqPmHh0w" rel="nofollow" target="_blank">https://docs.sqlalchemy.org.cn/zh_CN/20/</a></li><li><strong>GitHub 示例代码</strong>：<a href="https://link.segmentfault.com/?enc=AbCFS8WIK06kasgzFnt8eA%3D%3D.xA3k3WCYiqxgG7j%2FbXknMhNqKmqYY9VfW3ehRDESQovf8DXIfF%2BZDzEbVFGKlEHece47MrQMxR7ls5JgA6NVrQ%3D%3D" rel="nofollow" target="_blank">https://github.com/sqlalchemy/sqlalchemy/tree/main/examples</a></li></ul><hr/><p>SQLAlchemy 是一个功能强大且设计优雅的 Python 数据库工具包。掌握它不仅能提升开发效率，还能帮助你更好地理解数据库和对象关系映射的原理。建议读者结合实际项目多加练习，逐步深入理解其高级特性。祝学习愉快！</p>]]></description></item><item>    <title><![CDATA[从0到1到100：中小学学科答题小程序的设计与实现 CC同学呀 ]]></title>    <link>https://segmentfault.com/a/1190000047608128</link>    <guid>https://segmentfault.com/a/1190000047608128</guid>    <pubDate>2026-02-12 19:03:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>研究背景</h2><p>在教育信息化 2.0 行动计划的推动下，数字化学习工具逐渐融入中小学的日常教学与课后辅导中。中小学生的注意力持续时间较短，传统的刷题模式形式单一、趣味性不足，难以调动学生的学习积极性，而游戏化学习模式将学习与互动、竞赛相结合，能够有效提升学生的参与度和学习效率。<br/>微信作为国内用户量最大的社交平台，其小程序生态日趋完善，具有无需下载安装、即用即走、开发成本低、传播便捷等特点，非常适合开发轻量化的教育类应用。同时，腾讯微信云开发技术为小程序提供了一站式的云端开发解决方案，无需开发者搭建独立的服务器和域名，降低了小程序的开发和部署门槛，为教育类小程序的快速开发提供了技术支撑。在此背景下，设计并实现一款基于微信云开发的中小学学科答题小程序，能够满足校方、教师的教学辅助需求和家长的课后辅导需求，助力中小学教育的数字化转型。</p><h2>主要研究内容</h2><p>（1）通过调研中小学教学和课后辅导的实际需求，结合游戏化学习理论，完成小程序的功能性和非功能性需求分析，并进行技术、经济、操作可行性分析；<br/>（2）确定小程序的设计原则，设计基于微信云开发的系统架构，将小程序划分为前端用户模块和后台管理模块，并对各模块的功能进行详细设计；<br/>（3）根据需求分析和总体设计，完成小程序的数据库设计，设计用户表、题库表、答题记录表、竞赛表等核心数据表的结构；<br/>（4）选择微信小程序平台和腾讯云开发技术作为核心技术栈，完成小程序前端页面的开发、云函数的编写和后台管理功能的实现；<br/>（5）制定详细的系统部署步骤，完成小程序的本地部署和云端发布，并设计功能测试用例，对小程序的各项功能进行测试，验证系统的稳定性和实用性；<br/>（6）总结本次研究的成果和不足，对小程序的后续优化和功能拓展进行展望。</p><h2>系统需求分析</h2><p>需求分析是系统开发的基础，通过调研校方、教师、家长和学生的实际需求，明确系统的功能和性能要求，为后续的系统设计和实现提供依据。本次调研采用问卷调查和访谈相结合的方式，调研对象包括中小学教师、家长和学生，共发放问卷 300 份，回收有效问卷 286 份，访谈中小学教师 20 名、家长 30 名。</p><h2>功能性需求</h2><p>根据调研结果，将中小学学科答题小程序的用户分为普通用户（学生）、<strong> 管理用户（教师 / 家长 / 校方管理员）</strong> 两类，不同用户的功能性需求不同，同时小程序需满足基础的系统管理需求，具体功能性需求如下：<br/>2.1.1 普通用户（学生）需求<br/>普通用户为中小学学生，核心需求是通过小程序进行学科知识点的练习和答题竞赛，具体需求包括：<br/>（1）用户登录：支持通过微信授权快速登录，无需单独注册账号，登录后可查看个人信息和答题记录；<br/>（2）题库选择：支持按照学科（数学、语文、英语等）和知识点对题库进行分类筛选，方便学生选择针对性的知识点进行练习；<br/>（3）随机抽题：支持选择特定学科或知识点，由系统随机生成题目进行练习，抽题数量可灵活选择；<br/>（4）答题练习：答题过程中显示题目序号、题干和答题选项，支持选择题、判断题等常见题型，答题完成后即时显示答题结果；<br/>（5）解析详解：每道题目答题完成后，提供详细的解答和解题思路解析，帮助学生理解错题原因，巩固知识点；<br/>（6）答题竞赛：支持参与模拟竞赛，竞赛前可查看竞赛规则（答题时间、题目数量），竞赛过程中倒计时显示，答题完成后即时显示竞赛成绩；<br/>（7）排行榜查看：支持查看答题竞赛的积分排行榜，按积分从高到低排序，显示用户昵称、积分和排名，提升学习的竞争性；<br/>（8）个人中心：支持查看个人答题记录（答题次数、正确率、错题集）、竞赛记录（竞赛次数、最高成绩、平均成绩），可修改个人昵称等基础信息。</p><h2>系统基础需求</h2><p>（1）权限管理：实现超级管理员和普通管理员的权限分离，超级管理员拥有所有操作权限，普通管理员仅拥有题库管理、答题参数设置等部分权限；<br/>（2）数据备份与恢复：支持对题库、用户信息等核心数据进行备份，防止数据丢失；支持在数据异常时进行数据恢复；<br/>（3）缓存清理：支持清理小程序的本地缓存，提升小程序的运行速度。</p><h2>性能需求</h2><p>（1）响应速度：小程序前端页面的加载时间不超过 3 秒，随机抽题、答题结果展示、排行榜查看等操作的响应时间不超过 1 秒；<br/>（2）并发处理：支持至少 100 名用户同时参与答题竞赛，系统无卡顿、无延迟；<br/>（3）数据处理：支持 Excel 文件批量导入题库，5000 条数据的导入时间不超过 30 秒。</p><h2>技术可行性</h2><p>本小程序基于微信小程序平台和腾讯微信云开发技术进行开发，相关技术均为腾讯官方推出，技术文档完善、社区支持活跃，开发门槛较低。<br/>（1）微信小程序的开发框架提供了丰富的组件和 API，支持前端页面的快速开发，且兼容 iOS 和 Android 两大移动操作系统；<br/>（2）微信云开发技术提供了云函数、云数据库、云存储、定时器等一站式云端服务，无需开发者搭建独立的服务器和域名，降低了后端开发的复杂度；<br/>（3）开发所需的辅助技术如 Node.js、NPM 均为开源技术，学习资源丰富，开发者可快速掌握；<br/>（4）微信开发者工具为小程序的开发、调试、预览、部署提供了一体化的支持，支持本地调试和真机测试，方便开发过程中的问题排查。</p><h2>经济可行性</h2><p>本项目的开发和部署成本较低，后期维护成本几乎为零，具有良好的经济可行性：<br/>（1）开发成本：项目基于开源技术和腾讯免费的云开发基础配额进行开发，开发过程中无需支付软件授权费、服务器租赁费、域名注册费等费用；<br/>（2）部署成本：微信云开发的资源配额价格低廉，基础版配额可满足中小学校方的日常使用需求，即使后续需要提升资源配额，费用也远低于传统的服务器部署；<br/>（3）维护成本：微信云开发由腾讯官方进行维护，无需开发者进行服务器的运维、升级、安全防护等工作，节省了大量的维护人力和物力成本；<br/>（4）使用成本：小程序对用户完全免费，用户无需支付任何费用即可使用所有功能，易于推广和使用。</p><h2>系统总体设计</h2><p>模块化设计原则：将系统划分为多个独立的功能模块，每个模块实现特定的功能，模块之间通过统一的接口进行交互，降低模块之间的耦合度，方便后续的功能扩展和 bug 修复；<br/>用户体验优先原则：结合中小学生和管理员的使用习惯，进行前端界面和操作流程的设计，保证界面友好、操作简单，提升用户的使用体验；<br/>轻量化设计原则：基于微信小程序 “即用即走” 的特性，简化前端页面的代码和资源，降低小程序的代码包大小，提升页面的加载速度；<br/>云原生设计原则：充分利用微信云开发的技术优势，将业务逻辑、数据存储、文件存储均部署在云端，实现前后端的解耦，降低本地开发的复杂度；<br/>权限最小化原则：针对不同的管理员角色分配最小的操作权限，超级管理员拥有所有权限，普通管理员仅拥有必要的操作权限，保证系统的数据安全；<br/>可扩展设计原则：在系统架构和数据库设计中，预留扩展接口和字段，方便后续新增功能和扩展数据类型，适应不同用户的个性化需求。</p><h2>数据库设计</h2><p>本小程序采用微信云开发的云数据库进行数据存储，云数据库是一种非关系型数据库（NoSQL），以集合（Collection）为单位存储数据，每个集合包含多个文档（Document），文档采用 JSON 格式存储数据，具有灵活的结构，适合小程序的轻量化开发需求。根据系统的功能需求，设计用户集合、题库集合、答题记录集合、竞赛记录集合、管理员集合、答题参数集合、系统日志集合七个核心集合<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608130" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>前端功能实现</h2><p>小程序前端是用户与系统的交互界面，采用 WXML、WXSS、JavaScript 进行开发，遵循微信小程序的页面 - 逻辑 - 配置开发模式，每个页面由.wxml（页面结构）、.wxss（页面样式）、.js（页面逻辑）、.json（页面配置）四个文件组成。前端功能实现的核心是通过微信云开发的 API 与云端进行交互，将用户的操作请求发送至云函数，接收云函数返回的处理结果，并完成页面的动态渲染。</p><h2>云函数实现</h2><p>云函数是本小程序的业务逻辑处理核心，基于 Node.js 开发，核心云函数为mcloud，包含用户管理、题库管理、答题处理、竞赛管理、参数设置、数据统计六大业务模块的处理逻辑。云函数的开发遵循模块化原则，将不同的业务逻辑拆分为不同的函数，通过action参数区分前端的请求类型，实现对不同请求的处理。</p><pre><code class="c">// 得分统计
    async statAnswer(userId) { 
        let where = {
            ANSWER_USER_ID: userId,
            ANSWER_TYPE: 1
        }
        let cnt = await AnswerModel.count(where);
        let score = await AnswerModel.sum(where, 'ANSWER_SCORE');

        let data = {
            USER_ANSWER_CNT: cnt,
            USER_ANSWER_SCORE: score
        }
        await UserModel.edit({ USER_MINI_OPENID: userId }, data);
    }

    // 每日可答题次数校验
    async isAnswerTimes(userId, cateId) {
        let dayCnt = 100;
        let setup = await setupUtil.get('answer');
        if (setup) {
            setup = dataUtil.dbForms2Obj(setup);
            dayCnt = Number(setup.daycnt);

            if (setup.open != true) {
                return '竞赛尚未开始!';
            }
        }

        let where = {
            ANSWER_CATE_ID: String(cateId),
            ANSWER_USER_ID: userId,
            ANSWER_TYPE: 1,
            ANSWER_DAY: timeUtil.time('Y-M-D')
        }
        let cnt = await AnswerModel.count(where);
        if (cnt &gt;= dayCnt) {
            return '每日竞赛答题最多' + dayCnt + '次，请明日再来！';
        }

        return '';
    }

    async saveMyAnswer(userId,
     ) { 
     
    }

    // 随机N条记录，生成本次题库
    async genQuestion(userId, type, cateId) { 

        return { questionList: [], maxTime:10 };
    }


    async getMyAnswerList(userId, {
        search, // 搜索条件
        sortType, // 搜索菜单
        sortVal, // 搜索菜单
        orderBy, // 排序 
        page,
        size,
        isTotal = true,
        oldTotal
    }) {

        orderBy = orderBy || {
            'ANSWER_ADD_TIME': 'desc'
        };
        let fields = 'ANSWER_SCORE,ANSWER_CATE_NAME,ANSWER_TYPE,ANSWER_ADD_TIME,ANSWER_CNT,ANSWER_PER,ANSWER_SUCC_CNT,ANSWER_DURATION,ANSWER_START,ANSWER_END';

        let where = {};
        where.and = {
            ANSWER_USER_ID: userId,
            _pid: this.getProjectId() //复杂的查询在此处标注PID
        };

        if (util.isDefined(search) &amp;&amp; search) {
            where.or = [

            ];
        } else if (sortType &amp;&amp; util.isDefined(sortVal)) {
            // 搜索菜单
            switch (sortType) {
                case 'type': {
                    where.and.ANSWER_TYPE = Number(sortVal);
                    break;
                }
                case 'cateId': {
                    where.and.ANSWER_CATE_ID = String(sortVal);
                    break;
                }
                case 'sort': {
                    orderBy = this.fmtOrderBySort(sortVal, 'ANSWER_ADD_TIME');
                    break;
                }
            }
        }

        return await AnswerModel.getList(where, fields, orderBy, page, size, isTotal, oldTotal);
    }</code></pre><h2>git代码</h2><p><a href="https://link.segmentfault.com/?enc=V92CkP8G%2BWcBS%2BKMkoFfWQ%3D%3D.bR%2F3FP4EO%2FSx0mVt%2FH6HqcCGaWYeUerfvHoyxrcOQbcldhPjTBy0Wp%2FVUOi7%2B3FL" rel="nofollow" target="_blank">点击下载</a></p>]]></description></item><item>    <title><![CDATA[函数计算 AgentRun 重磅上线知识库功能，赋能智能体更“懂”你 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047608137</link>    <guid>https://segmentfault.com/a/1190000047608137</guid>    <pubDate>2026-02-12 19:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：靖苏</p><p>阿里云函数计算 <strong>AgentRun</strong> 正式推出全新<strong>知识库功能</strong>，为智能体（Agent）注入更强的语义理解与上下文感知能力。通过深度集成<strong>百炼知识库</strong>与 <strong>RAGFlow 知识库</strong>，AgentRun 让开发者能够轻松构建具备“知识”的智能应用，真正实现“更懂用户、更贴场景、更高效响应”。</p><h2>为什么需要知识库？</h2><p>在传统智能体开发中，模型往往依赖通用训练数据，缺乏对特定业务、私有文档或实时信息的理解能力。这导致其在面对专业领域问题、企业内部知识或个性化需求时表现受限。</p><p>AgentRun 的知识库功能正是为解决这一痛点而生——它将外部知识源无缝接入智能体运行流程，通过<strong>检索增强生成（RAG）</strong> 技术，让智能体在回答问题、执行任务时，能动态调用相关知识，大幅提升准确性、专业性与可信度。</p><h2>双引擎支持：百炼+RAGFlow，覆盖多元知识形态</h2><h3>百炼知识库绑定</h3><p>函数计算 AgentRun 可以绑定您账号下已经创建好的阿里云百炼知识库 <strong>[</strong> <strong>1]</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608139" alt="image" title="image"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“百炼”，可以多选绑定您账号下已经在阿里云百炼控制台创建好的多个知识库。填写检索配置后，点击创建知识库，即可将您的阿里云百炼知识库绑定至 AgentRun 平台。</p><h3>RAGFlow 知识库绑定</h3><p>函数计算 AgentRun 可以绑定您账号下已经创建好的 RAGFlow 知识库。如果您没有 RAGFlow 知识库，可以点击此链接（<a href="https://link.segmentfault.com/?enc=iUpFBqMSTnskNUwGWsrSsA%3D%3D.MW%2FyadxC%2FEOH8RAW%2FuQVEiMezpwMwpRlk6e2IkyMBe%2Fwed60OUcYytUyJEAvtKXrPNc2eLtUIFawWFKSPGJZaU7xtxvcyssOjz7kM8PZ2DYM75KE5QVyBGFl83ayIn6HtEXK2ZTA95wV1GNFSywgPObaruyWmaoWvqUMA1a10qa0a8u%2BUh8%2F6dw712oYEoowXMtzvqEuM%2B34Tj379bCtS7SKi8SL36auZUtCUAnS%2FZM4LZQ0qfcedr9mD4UDP9A93mEVgbEOdtpPP935BbyOLg%3D%3D" rel="nofollow" target="_blank">https://saenext.console.aliyun.com/cn-hangzhou/scene-market/m...</a> ），一键在 SAE 上创建 RAGFlow。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608140" alt="image" title="image" loading="lazy"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“RAGFlow”，填写您已部署的 RAGFlow 的 BaseURL、Dataset IDs 和 API-KEY（将其保存在凭证中）。填写检索配置后，点击创建知识库，即可将您自建的 RAGFlow 知识库绑定至 AgentRun 平台。</p><p>RAGFlow 知识库详细配置获取方式，可参考此文档：<a href="https://link.segmentfault.com/?enc=fv9Df6vGBIXkxrsEY0iTXQ%3D%3D.l7hIce0MNnwe%2F8pM0kIvM39A7xVmJ92YWj139WmYbfxKpWPzWmQLlBLr7Twqs4N41df1CbSOs5uWZO5tcAVlihXGSQmscvYPrnb1wIJPwhw%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/functioncompute/fc/knowledge-base-...</a>。</p><h2>三大集成方式，灵活适配各类开发场景</h2><p>函数计算 AgentRun 知识库功能支持快速创建集成、代码集成和 MCP 集成三种方式，满足不同技术栈和开发习惯。</p><h3>快速创建Agent集成知识库功能</h3><p>对于希望快速验证想法或加速产品迭代的团队，AgentRun 提供了<strong>低代码、可视化</strong>的知识库绑定能力。开发者只需登录 AgentRun 控制台，选择已创建的百炼或 RAGFlow 知识库，将其关联到目标智能体，并配置简单的检索参数（如返回结果数量、相似度阈值等），即可完成集成——全程无需编写一行代码。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608141" alt="image" title="image" loading="lazy"/></p><p>这一模式极大降低了技术门槛，让产品经理、运营人员甚至非技术背景的创新者也能参与智能体的构建与优化。无论是搭建内部知识问答机器人、客户自助服务助手，还是快速验证某个垂直领域的 AI 应用场景，都能在<strong>几分钟内完成部署并上线试用</strong>。</p><p><strong>代码集成知识库查询能力</strong>对于追求极致灵活性与控制力的开发者，AgentRun 提供了<strong>原生代码级知识库接入能力</strong>。您可以在代码逻辑中，调用 AgentRun SDK 的知识库检索接口，根据业务上下文动态发起检索请求，精准筛选并注入最相关的信息片段到智能体的推理流程中。您可以使用 AgentRun SDK，调用以下封装的接口，进行单知识库查询或多知识库查询。</p><pre><code>fromagentrun.knowledgebaseimportKnowledgeBase
## 获取单知识库，进行查询
knowledgebase=KnowledgeBase.get_by_name("ragflow-test")
single_kb_retrieve_result=knowledgebase.retrieve("&lt;your-query&gt;")
print(single_kb_retrieve_result)
## 获取多知识库，进行查询，支持跨供应商知识库类型检索
multi_kb_retrieve_result=KnowledgeBase.multi_retrieve(
    query="&lt;your-query&gt;",
    knowledge_base_names=["ragflow-test","&lt;your-knowledge-base-name-2&gt;"],
)
print(multi_kb_retrieve_result)</code></pre><p>同样，您可以集成 LangChain 框架，将知识库的查询能力集成在工具或上下文中。</p><pre><code>"""AgentRun 知识库智能体集成代码示例
使用前，请参考https://docs.agent.run/docs/tutorial/quick-start 配置好相应认证信息和环境变量
curl http://127.0.0.1:9000/openai/v1/chat/completions -X POST \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "什么是Serverless?"}], "stream": true}'
"""
import json
import os
from typing import Any
from langchain.agents import create_agent
import pydash
from agentrun import Config
from agentrun.integration.langchain import model
from agentrun.integration.langchain import knowledgebase_toolset
from agentrun.integration.langgraph.agent_converter import AgentRunConverter
from agentrun.knowledgebase import KnowledgeBase
from agentrun.server import AgentRequest, AgentRunServer
from agentrun.server.model import ServerConfig
from agentrun.utils.log import logger
# 请替换为您已经创建的 模型 名称
AGENTRUN_MODEL_SERVICE = os.getenv("AGENTRUN_MODEL_SERVICE", "&lt;your-model-service&gt;")
AGENTRUN_MODEL_NAME = os.getenv("AGENTRUN_MODEL_NAME", "&lt;your-model-name&gt;")
KNOWLEDGE_BASES = os.getenv("AGENTRUN_KNOWLEDGE_BASES", "ragflow-test").split(",")
if AGENTRUN_MODEL_NAME.startswith("&lt;") or not AGENTRUN_MODEL_NAME:
    raise ValueError("请将 MODEL_NAME 替换为您已经创建的模型名称")
## 加载知识库工具，知识库可以以工具的方式供Agent进行调用
knowledgebase_tools = []
if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
    knowledgebase_tools = knowledgebase_toolset(
        knowledge_base_names=KNOWLEDGE_BASES,
    )
else:
    logger.warning("KNOWLEDGE_BASES 未设置或未替换，跳过加载知识库工具。")
agent = create_agent(
    model=model(AGENTRUN_MODEL_SERVICE, model=AGENTRUN_MODEL_NAME, config=Config(timeout=180)),
    tools=[
        *knowledgebase_tools,   ## 通过工具集成知识库查询能力
    ],
    system_prompt="你是一个 AgentRun 的 AI 专家，可以通过查询知识库文档来回答用户的问题。",
)
async def invoke_agent(request: AgentRequest):
    messages = [
        {"role": msg.role, "content": msg.content}
        for msg in request.messages
    ]
    # 如果配置了知识库，查询知识库并将结果添加到上下文
    if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
        # 获取用户最新的消息内容作为查询
        user_query = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break
        if user_query:
            try:
                retrieve_result = await KnowledgeBase.multi_retrieve_async(
                    query=user_query,
                    knowledge_base_names=KNOWLEDGE_BASES,
                )
                # 直接将检索结果添加到上下文
                if retrieve_result:
                    messages.append({
                        "role": "assistant",
                        "content": json.dumps(retrieve_result, ensure_ascii=False),
                    })
            except Exception as e:
                logger.warning(f"知识库检索失败: {e}")
    input: Any = {"messages": messages}
    converter = AgentRunConverter()
    if request.stream:
        async def async_generator():
            async for event in agent.astream(input, stream_mode="updates"):
                for item in converter.convert(event):
                    yield item
        return async_generator()
    else:
        result = await agent.ainvoke(input)
        return pydash.get(result, "messages[-1].content", "")
AgentRunServer(
    invoke_agent=invoke_agent,
    config=ServerConfig(
        cors_origins=[
            "*"
        ]
    ),
).start()</code></pre><p>注意⚠️：如果您选择了 RAGFlow 的知识库，<strong>需要确保您的 Agent 运行环境和 RAGFlow 的 BaseURL 的地址处于同一网络环境下，否则 AgentRun SDK 将无法调用 RAGFlow 的 API 实现查询能力。</strong></p><p>通过代码集成，AgentRun 赋予开发者“全栈可控”的能力——既享受函数计算的弹性与免运维优势，又保留对智能体认知过程的深度掌控，真正实现“知识为我所用，逻辑由我定义”。</p><h3>MCP 集成：将知识库检索作为 Agent 的工具调用</h3><p>AgentRun 知识库率先实现“Agentic RAG”（智能体 RAG）模式——将传统静态检索升级为动态、可编程的智能体工具调用。具体而言，用户可一键将知识库发布为 MCP，使其成为大语言模型（LLM）可主动调用的工具之一。在此模式下，LLM 不再被动接收上下文，而是具备“工具使用能力”，在推理过程中自主判断何时调用 RAG、数据库查询、库存检查等工具，并基于返回结果进行多步推理与任务分解。这种机制使 RAG 从单一检索功能转变为智能体工具箱中的灵活组件，与其他工具并列协作，显著提升复杂任务的处理能力。其工作方式更贴近人类“思考—行动—反思”的认知流程：模型先分析问题，制定计划，再按需调用多个工具获取信息，最终整合结果生成答案。</p><p>进入其他 &gt;&gt; 工具管理 &gt;&gt; 工具市场，可以搜索到 <strong>“AgentRun 知识库 MCP”</strong> 工具模板，点击安装后，填写知识库名称和类型，即可将知识库的查询能力一件发布成 MCP 工具给大模型进行调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608142" alt="image" title="image" loading="lazy"/></p><p>创建完毕后，点击工具详情，即可看到集成调用的工具地址：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608143" alt="image" title="image" loading="lazy"/></p><p>基于 MCP 工具标准协议，AgentRun 支持以标准化方式对接知识库服务，实现跨平台、跨模型的上下文注入能力，保障架构的开放性与可扩展性。</p><h2>结语：从“能回答”到“真理解”，智能体正在拥有“知识之眼”</h2><p>AgentRun 知识库功能的上线，不仅是一次技术能力的升级，更标志着智能体发展迈入新阶段——从依赖通用语料的“泛化应答”，转向基于专属知识的“情境理解”。当智能体能够随时调用企业文档、行业规范、用户历史甚至实时数据，它便不再只是一个语言模型的接口，而成为一个<strong>具备领域认知、上下文记忆与决策依据的数字协作者</strong>。</p><p>未来，随着知识库的持续进化——支持多模态内容、动态更新、跨源推理——AgentRun 将进一步降低构建“有知识、有逻辑、有温度”智能体的门槛。</p><p>我们相信，真正的智能，不在于模型有多大，而在于是否“懂你所需、知你所问、信你所依”。</p><p><strong>AgentRun，正让每一个智能体，学会思考，更学会理解。</strong></p><p><strong>相关链接：</strong></p><p>[1] 阿里云百炼知识库</p><p><a href="https://link.segmentfault.com/?enc=lxliN5coN6z41szdnERlCg%3D%3D.Oa26KHsEek%2F7AykJ%2BdQYc%2BzRNh9MYL2yzdaln%2B%2FEFUnV5wEw5o6uOgeGHZiPCSNsZGlw9DB3AgrnNY1UDN4fQBnLMwYUVeU9glN%2BBwM7Ep4%3D" rel="nofollow" target="_blank">https://bailian.console.aliyun.com/cn-beijing/?admin=1&amp;tab=ap...</a></p>]]></description></item><item>    <title><![CDATA[巨人网络《超自然行动组》携手阿里云打造云原生游戏新范式 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047608151</link>    <guid>https://segmentfault.com/a/1190000047608151</guid>    <pubDate>2026-02-12 19:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>从开服第一天起，就跑在云上；</p><p>上线一年，DAU 已经突破 1000 万；</p><p>高峰期百万玩家同时在线，零重大故障。</p><p>这不是科幻，而是巨人网络与阿里云共书写的云原生实战。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608153" alt="image" title="image"/></p><h2>《超自然行动组》的云原生架构先行战略</h2><p>2025 年 1 月，巨人网络推出多人组队欢乐冒险游戏《超自然行动组》，凭借创新的“中式微恐+多人合作"的独特玩法，迅速成为现象级产品。最近，《超自然行动组》宣布 DAU 突破 1000 万，更攀升至 iOS 游戏畅销榜第四。尤为值得一提的是，自开服第一天起，<strong>这款游戏从未部署在任何物理机或传统虚拟机上——它从第一天起，就运行在云原生架构之上</strong> <strong>。</strong></p><p>对于大多数游戏公司而言，“上线即爆款” 是甜蜜的烦恼——流量洪峰来得快、退得慢，而传统架构却“笨重”：</p><ul><li>游戏服（如战斗服、房间服）部署在固定服务器，扩容需数天；</li><li>为应对峰值需长期预留资源，空闲时浪费严重；</li><li>版本更新靠脚本，灰度发布难，一出错就“全服回滚”；</li><li>日志分散、监控割裂，故障定位动辄几小时；</li><li>安全防护薄弱，易受 DDoS 攻击；</li><li>数据层瓶颈突出：战斗结算延迟、排行榜卡顿、玩家数据丢失等问题频发。</li></ul><p>《超自然行动组》团队深知：若沿用旧模式，很可能“倒在成功的路上”。</p><p>于是，他们选择了一条更难但更远的路——<strong>全面拥抱云原生</strong>。</p><p>通过 ACK（容器服务）、ESS（弹性伸缩）、网络型负载均衡 NLB、OpenKruiseGame（OKG）、SLS（日志服务）、ARMS（应用实时监控服务）、阿里云原生防护（Native Protection），以及云原生数据库 polardb 和 Redis 的深度协同，巨人网络构建了一套高弹性、高可用、低成本、智能化、高安全且高性能数据处理能力的新一代游戏基础设施，为行业树立了云原生落地的标杆。如今，随着日活跃用户（DAU）突破千万大关，这套技术体系，已经成为游戏行业“云原生转型”的标杆案例。</p><h2>高弹性×低延迟×零故障：解码&lt;超自然行动组&gt;的云原生底座</h2><p>《超自然行动组》基于阿里云 ACK 与 OpenKruiseGame（OKG）构建了业界领先的云原生游戏服架构：通过蓝绿发布与原地升级实现零停机、无感交付；通过 OKG+多 NLB 资源池，全面覆盖 BGP、电信、联通、移动等主流线路，实现多运营商网络自动化映射。结合 HPA 智能扩缩容与 OKG 优雅下线机制，在成本与用户体验间取得平衡；通过 ACK Koordinator 组件，实现 CPU Burst 与 QoS 精细化调度，显著提升集群资源利用率；并通过基础设施与业务状态的双向感知，构建起“业务语义驱动”的自动化运维闭环——真正实现了高弹性、高可用、高性能、高安全的新一代游戏后端体系。在显著降低运维压力的同时，实现了机制化、可持续的成本优化。</p><p>在网络层面，作为一款对延迟极度敏感的竞技手游，《超自然行动组》依托阿里云打造了“云边协同、三网通吃、弹性集约”的新一代云网络架构：通过 OKG 与 NLB 实现电信、联通、移动、BGP 四线并发接入，全国玩家自动匹配最优链路，并以“静态网络+动态计算”创新模式达成 50 节点/分钟的极速扩容，15 分钟内可拉起数千战斗服，彻底告别排队；同时，借助阿里云高速通道，将本地机房的账号、支付等核心系统与上海 VPC 内网直连，构建毫秒级同步、金融级安全的混合云中枢；并通过共享带宽包统一聚合公网出口，在简化运维的同时显著降本，为玩家交互与高频状态同步提供弹性“带宽蓄水池”，真正实现千万玩家同场竞技零卡顿、零等待的极致体验。</p><p>在数据层面，云原生 polardb 和 Tair（兼容 Redis）构建了弹性，稳定的玩家存档方案，支持千万级玩家高并发登录和读写，基于 polardb 云原生数据库的存算分离和弹性能力，支持游戏在活动期间自动扩展弹性，并且支持玩家数据的秒级备份和回档，大幅降低了数据库的运维成本，并且 PolarDB Serverless 支持自动扩容和缩容，能够根据用户访问量的实时变化，秒级调整计算资源。在高峰时期自动增加资源，低谷时期自动减少资源，确保社区始终运行在最佳状态。基于阿里云 Tair（兼容 Redis）支持玩家超高并发的访问，作为实时排行榜、战斗状态缓存和匹配池的核心，依托多线程与持久内存优化，单实例 QPS 超百万，实现毫秒级排名刷新、瞬时结算与断线无缝恢复。</p><p>当数百万玩家涌入《超自然行动组》，DDoS 攻击成为影响体验的关键风险。为此，巨人网络联合阿里云，基于云原生安全架构打造了一套高性能、智能化的防护体系。该方案依托阿里云原生高防能力，无需架构改造，一键接入即可实现 TB 级 DDoS 攻击的毫秒级识别与精准清洗，防护能力行业领先。即便在版本更新或大型赛事等高并发场景下，系统仍保障 99.99% 以上服务可用性，真正做到“攻击零感知、切换无中断”。面对突发流量洪峰，系统支持防御带宽自动弹性伸缩，动态调配资源，避免因容量不足导致服务中断。同时，通过集成安全事件中心，运营团队可实时监控攻击事件，分析攻击类型与特征，并结合 AI 驱动的策略建议，快速部署定制化游戏协议防护规则，显著提升响应效率与防御精准度。从高效清洗到智能决策，阿里云以“稳定、高效、安全”为核心，为《超自然行动组》构筑起坚不可摧的数字护盾，在保障千万玩家流畅竞技的同时，也为游戏行业树立了云原生安全新标杆。</p><p>对于《超自然行动组》这款主打实时互动的竞技游戏，“能跑” 只是起点，“看得清、查得准” 才是保障千万玩家流畅体验的关键。运维团队摒弃传统分散监控工具，基于阿里云日志服务 SLS 、云监控 CMS 的 Prometheus 服务、Grafana 服务，搭建起轻量、标准、深度集成的可观测体系：</p><ul><li>依托 Prometheus 实时采集百万级 PCU 下的资源水位与在线人数、匹配时长等核心业务指标，确保高并发下监控精准不丢点；</li><li>通过 SLS 统一汇聚全链路日志，支持按 RequestID / 玩家 ID 秒级还原行为路径，结合 SQL 分析与自定义规则，实现地图报错统计、异常操作追踪；</li><li>借助 Grafana 打造统一全景大盘，融合展示指标与日志数据，告警时可一键跳转 SLS 查看关联日志，实现 “指标发现问题、日志定位根因” 的闭环，将故障响应时间从小时级压缩至分钟级，充分发挥云原生可观测与协同优势。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608154" alt="image" title="image" loading="lazy"/></p><p><em>超自然云原生架构</em></p><h2>从“能跑”到“跑赢”：OKG 重塑游戏后端新范式</h2><p>当一款游戏从“能跑”走向“跑得快、跑得省、跑得稳”，背后一定有一套先进的技术底座在支撑。《超自然行动组》的故事，源于巨人网络，也属于所有正在思考“如何用云原生重构游戏后端”的开发者。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608155" alt="image" title="image" loading="lazy"/></p><p>面对全球游戏市场对高并发、低延迟及快速迭代的极致追求，OpenKruiseGame (OKG) 作为阿里云打造的“为游戏而生”的云原生游戏服管理方案，正成为推动行业架构平滑升级的核心引擎。针对游戏业务特有的异构性管理难题，OKG 提供了从精细化配置、自动化网络接入到业务状态感知的一站式管理体系。它不仅极大降低了游戏厂商的云原生转型门槛，更通过全球多地域一致性交付能力，助力开发者突破地域限制，实现业务的快速敏捷部署与全球化扩张。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608156" alt="image" title="image" loading="lazy"/></p><p>云原生，已不再是互联网应用的专属，而是下一代游戏基础设施的必然选择。</p>]]></description></item><item>    <title><![CDATA[Apache Doris 4.0.3 版本正式发布 SelectDB技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047608164</link>    <guid>https://segmentfault.com/a/1190000047608164</guid>    <pubDate>2026-02-12 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>亲爱的社区小伙伴们，<strong>Apache Doris 4.0.3 版本已正式发布。</strong>此版本新增了在 AI &amp; Search、湖仓一体、查询引擎等方面的能力，并同步进行了多项优化改进及问题修复，欢迎下载体验！</p><ul><li>GitHub 下载：<a href="https://link.segmentfault.com/?enc=x%2B76iEQfzUlDK4tMwlVjkw%3D%3D.qQenp6u67c1y9pgPJRwosQEuTX07HU%2B83Aa8171D1QLbHozOYMtKzTXZZt%2FEtlNg" rel="nofollow" target="_blank">https://github.com/apache/doris/releases</a></li><li>官网下载：<a href="https://link.segmentfault.com/?enc=yvZWOmjIS8NWVT55MOku7A%3D%3D.lISNSqnU8TiXI7c2uxFSUySg8ShLeUKgX6NSPm4HCY6GNZw6RQyb2ehZOTTnfT1b" rel="nofollow" target="_blank">https://doris.apache.org/download</a></li></ul><h2>新增功能</h2><h3>AI &amp; Search</h3><ul><li>添加倒排索引 NORMALIZER 支持</li><li>实现类似 ES 的布尔查询</li><li>为搜索函数引入 lucene 布尔模式</li></ul><h3>湖仓一体</h3><ul><li>支持通过 AwsCredentialsProviderChain 加载 Catalog 凭证</li><li>支持使用 OSSHDFS 存储的 Paimon DLF Catalog</li><li>为 Iceberg 表添加 manifest 级别缓存</li></ul><h3>查询引擎</h3><ul><li>支持 INTERVAL 函数并修复 EXPORT_SET</li><li>支持 TIME_FORMAT 函数</li><li>支持 QUANTILE_STATE_TO/FROM_BASE64 函数</li></ul><h2>优化改进</h2><ul><li>引入加载作业系统表</li><li>使视图、物化视图、生成列和别名函数能够持久化会话变量</li><li>将表查询计划操作接收的 SQL 添加到审计日志</li><li>启用流式加载记录到审计日志系统表</li><li>通过列裁剪优化复杂类型列读取</li><li>兼容 MySQL MOD 语法</li><li>为 sql_digest 生成添加动态配置</li><li>使用 Youngs-Cramer 算法实现 REGR_SLOPE/INTERCEPT 以与 PG 对齐</li></ul><h2>问题修复</h2><ul><li>修复 JdbcConnector 关闭时的 JNI 全局引用泄漏</li><li>修复由于 BE 统计信息上传不及时导致 CBO 无法稳定选择同步物化视图的问题</li><li>用默认的 JSONB null 值替换无效的 JSONB</li><li>修复由于并发删除后端导致的 OlapTableSink.createPaloNodesInfo 空指针异常</li><li>修复 FROM DUAL 错误匹配以 dual 开头的表名</li><li>修复 BE 宕机时预热取消失败的问题</li><li>修复当物化视图被 LimitAggToTopNAgg 重写但查询未被重写时物化视图重写失败的问题</li><li>修复刷新时 lastUpdateTime 未更新的问题并添加定时刷新日志</li><li>修复 hll_from_base64 输入无效时的崩溃问题</li><li>修复带表达式的加载列映射的敏感性问题</li><li>修复删除表时未删除约束相关信息的问题</li><li>修复 parquet topn 延迟物化复杂数据错误结果</li><li>始终创建数据和索引页缓存以避免空指针</li><li>修改 tablet cooldownConfLock 以减少内存占用</li><li>修复读取 parquet footer 时缺失 profile 的问题</li><li>修复 Exception::to_string 中潜在的释放后使用问题</li><li>修复浮点字段 to_string 问题</li><li>修复读取 hudi parquet 导致 BE 崩溃的问题</li><li>修复 Kerberos 认证配置检测</li><li>修复空表下的同步失败问题</li><li>修复 parquet 类型未处理 float16 的问题</li><li>修复 BM25 LENGTH_TABLE 范数解码问题</li><li>避免某些日期类函数的误报</li></ul>]]></description></item><item>    <title><![CDATA[拒绝“Demo 级”架构：基于 SAE × SLS 构建 Dify 高可用生产底座 Serverle]]></title>    <link>https://segmentfault.com/a/1190000047608003</link>    <guid>https://segmentfault.com/a/1190000047608003</guid>    <pubDate>2026-02-12 18:04:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>导读：</p><blockquote><p>在上一篇<a href="https://link.segmentfault.com/?enc=aBoL1B0q0hVMJzjmzQLP4A%3D%3D.YO79WjNtq%2FguM227YmdpHW%2FS3CdfesVxha3l%2Bj1IpzEWjMz%2B2ESlwzVboIMBwQPeHMMYnfI40P%2BMF9hnxBlUWci59DtjuHEsv3Ina%2BMwRIZ2wL1EfMNyaG3BPzdYpeOIXr%2FAcqUfqAv%2BwrtO6xKwJkiXOQGJ0Cmzftah%2BzVK7r%2B1tO02l92vK899Uv3JcuBEYcYPnxiboLET9jtXqDWduzRijJ5A4rfuvtqTHOvNzL4%3D" rel="nofollow" target="_blank">《告别数据库“膨胀”：Dify x SLS 构建高可用生产级 AI 架构》</a>中，我们深度剖析了 Dify 在大规模生产场景下数据库因日志写入而面临的性能瓶颈，并介绍了通过 SLS 插件实现“存算分离”的硬核改造方案。</p><p>然而，解决“数据存储”只是跨过了生产落地的第一道坎。面对复杂的微服务运维、波动的 AI 潮汐流量，如何构建一个弹性、免运维的“计算底座”同样关键。本文作为系列的第二篇，将视野从单一的数据架构扩展至全栈基础设施，为您介绍基于 阿里云 SAE × SLS 的终极生产级解决方案。</p><p>随着 LLM 应用的爆发式增长，Dify 以其强大的工作流编排与友好的可视化界面，正成为企业构建 AI 应用的首选。然而，当应用从本地 Demo 迈向大规模生产时，开发者常会遭遇两大“隐形”挑战：运维复杂度的剧增与数据架构的性能瓶颈。</p><p>本文将深度解析这一架构瓶颈，并介绍基于阿里云 <strong>SAE（Serverless 应用引擎）</strong> 与 <strong>SLS（日志服务）</strong> 的联合解决方案。通过“全托管算力”与“存算分离”的双轮驱动，打造一个高弹性、低成本、且具备深度数据洞察力的生产级 Dify 环境。</p></blockquote><h2>一、现状与挑战：Dify 规模化落地的架构瓶颈</h2><p>在单机 Demo 阶段，使用 Docker Compose 部署配合默认的 PostgreSQL 存储方案完全够用。但一旦进入生产环境，这两项基础设施往往最先成为性能与扩展性的瓶颈。</p><h4>运维管理复杂</h4><p>Dify 是一个由 API 服务、Worker、Web 前端、KV缓存、关系型数据库、向量数据库等多个组件构成的微服务架构。在生产环境中，这种架构给运维带来了很大挑战：</p><ul><li><strong>资源缺乏弹性</strong>：AI 应用通常具有明显的流量波峰波谷特征。若采用自建 Kubernetes 或 ECS 集群，扩容响应滞后，高峰期用户排队等待，低谷期又造成大量资源闲置，推高成本。</li><li><strong>维护成本高昂</strong>：保障高可用、配置负载均衡、处理节点故障、执行蓝绿/灰度发布等基础设施工作，不仅技术门槛高，还会大量挤占开发团队本应用于业务创新的精力。</li><li><strong>性能瓶颈明显</strong>：默认部署方式下的 QPS 能力有限，难以支撑高并发场景，尤其在推理密集型任务下容易成为系统瓶颈。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608005" alt="" title=""/></p><h4>数据库容量爆炸</h4><p>Dify 默认将所有数据（包括业务元数据和运行日志）存储在 PostgreSQL 中。随着业务量增长，“数据特征”与“存储引擎”的错配问题日益凸显：</p><ul><li><strong>日志“撑爆”数据库</strong>：Workflow 的每一次节点执行，都会完整记录输入输出、Prompt、推理过程及 Token 统计等详细信息。在生产级高并发场景下，这些数据占据了数据库绝大部分资源，导致表空间迅速膨胀。</li><li><strong>拖慢核心业务：</strong> 高频、高吞吐的日志写入会大量占用数据库连接池和 I/O 资源，严重干扰核心业务操作（如创建应用、知识库检索、对话上下文管理等），导致响应延迟、超时甚至服务不可用。</li></ul><h2><strong>二、协同赋能：SAE 与 SLS 的核心优势</strong></h2><p>为解决上述瓶颈，SAE 与 SLS 协同发力——SAE 聚焦弹性算力调度，SLS 专攻海量日志存储，共同构建高性能、高可用的 Dify 运行底座。</p><h4>SAE：极致弹性的 Dify 全托管运行环境</h4><p>SAE 不仅负责 Dify 核心微服务（API、Worker、Sandbox）的编排，更通过一键化模板集成了 Dify 运行所需的完整云生态。</p><ul><li><strong>一键全栈交付：</strong> 开发者无需手动搭建复杂环境。通过预置模板，可一键部署完整的微服务集群，并自动创建和集成连通日志服务SLS（工作流日志存储）、表格存储Tablestore（向量存储）、云数据库 Redis 版（缓存）及 RDS for PostgreSQL（元数据存储）等阿里云服务，无需逐个购买和配置，实现“开箱即是生产级”的交付体验。</li><li><strong>企业级高可用保障：</strong> 实例自动分布于多可用区，配合健康检查与自愈机制规避单点故障。支持金丝雀发布，确保在工作流频繁迭代时，流量切换平滑无感。</li><li><strong>秒级算力弹性</strong>：完美适配 AI 业务的“潮汐特征”。SAE 支持按 CPU/内存利用率或 QPS 指标进行自动扩缩容。在推理高峰期，秒级拉起 Worker 实例抗压；在业务低谷期，自动释放闲置资源，将算力成本严格控制在“有效使用”范围内。</li><li><strong>深度性能调优</strong>：SAE 对 Dify 实施了穿透代码与架构的“立体调优”，不仅在底层修补了 Redis 集群适配与慢 SQL 短板，更精准调优了运行参数并对齐了资源规格。这一全链路改造驱动吞吐量实现从 10 QPS 到 500 QPS 的 50 倍跃迁，确保 AI 响应如丝般顺滑。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608006" alt="" title="" loading="lazy"/></p><h4>SLS：支撑海量数据的“存算分离”方案</h4><p>SLS 并非简单的数据库替代品，而是专为日志场景设计的云原生基础设施。相比于 PostgreSQL，SLS 在 Dify 场景下实现了四个维度的架构升级：</p><ul><li><strong>极致存储弹性：</strong> 不同于数据库需按“峰值”预置资源，SLS 作为 Saas 化服务，天然支持秒级弹性伸缩。无论是深夜的低谷还是突发的推理洪峰，都能自适应承载，无需关心分片或容量上限。</li><li><strong>架构解耦负载隔离：</strong> 相利用追加写入特性，避免了数据库常见的随机 I/O 与锁竞争，轻松支撑万级 TPS 吞吐。同时通过将日志负载彻底剥离至云端，确保海量日志写入不影响 Dify 核心业务的响应速度。</li><li><strong>分层存储低成本留存</strong>：依托高压缩比技术，热数据实时分析，冷数据自动沉降至归档存储，可以远低于数据库 SSD 的成本满足长周期的审计与回溯需求。</li><li><strong>开箱即用的业务洞察：</strong> 内置的 OLAP 分析引擎支持 SQL 实时查询、可视化报表与告警监控，帮助开发者将沉睡的日志数据转化为直观的业务洞察。</li></ul><h2><strong>三、极简部署：1 分钟定义生产级底座</strong></h2><p>SAE 应用中心内置了深度优化的 Dify 生产级模板，通过简单的参数配置，即可一键交付一套高可用就绪的运行环境，告别繁琐的 YAML 编写与环境调试。</p><p><strong>Step 1：选择部署模板</strong></p><p>登录 SAE 控制台，进入应用中心，选择 <strong>“Dify 社区版 - Serverless 部署”</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608007" alt="" title="" loading="lazy"/></p><p><strong>Step 2：配置参数与规格选型</strong></p><p>目前提供了 Dify 高性能版、Dify 高可用版、Dify 测试版 三种模板。</p><p>如果是应对高并发生产场景，建议优先选择 <strong>Dify 高性能版</strong>，该版本专门针对 <code>api</code> 镜像以及 <code>plugin-daemon</code> 镜像做了深度优化，运行效率更高。配置过程极为精简，只需手动填写各云服务的密码并选定所属的 VPC 与子网（VSW），系统便会针对选定的云资源给出一份总预估价格，确保成本清晰透明。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608008" alt="" title="" loading="lazy"/></p><p><strong>Step 3：提交并访问服务</strong></p><p>点击提交后，系统会自动完成核心服务的部署与云资源关联。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608009" alt="" title="" loading="lazy"/></p><p>部署完成后，直接在浏览器输入控制台提供的服务地址 <code>${EXTERNAL-IP}:${PORT}</code>，即可开始您的 Dify 应用编排之旅。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608010" alt="" title="" loading="lazy"/></p><blockquote>注：当Dify启动并运行之后，SLS插件会自动创建相关的logstore和索引配置。无须手动干预，直接从SLS控制台进入对应的project，即可工作流日志进行实时的查询和分析。</blockquote><h2>四、50 倍性能跃迁：SAE 从 10 QPS 到 500 QPS 的突破之路</h2><p>Dify 社区版的默认配置仅能支撑 10 QPS，但这仅仅是起步。从“尝鲜”到 500 QPS 的生产级扩容，并非简单的堆砌服务器资源，而是一场步步惊心的“闯关游戏”。每当你试图提升吞吐量时，总会撞上新的隐形天花板——从基础的参数限制到深层的架构瓶颈。SAE 团队通过全链路压测，为您提前探明并攻克了这条晋级之路上的两大核心关卡，让高性能部署变得有迹可循。</p><h4>瓶颈一：突破 10 QPS 限制——组件并发与数据库连接的协同调优</h4><ol><li><strong>为什么默认配置只有 10 QPS？</strong></li></ol><p>Dify 社区版默认配置更多是为了方便开发者快速试用，而非为大规模生产环境设计。其核心组件 dify-api 的默认参数极为保守：</p><pre><code class="plain">SERVER_WORKER_AMOUNT（工作进程数）：1
SERVER_WORKER_CONNECTIONS（单进程最大连接数）：10</code></pre><p>这两个参数直接锁死了单节点的吞吐上限。但在生产环境中，我们不能简单地将这些参数“调大十倍”，因为应用层的并发能力提升，立即会引发下游数据库的连锁反应。</p><ol start="2"><li><strong>牵一发而动全身的“连接池”难题</strong></li></ol><p>随着 QPS 的增长，dify-api 和 dify-plugin-daemon 等组件会向 PostgreSQL 发起海量连接。如果缺乏全链路的参数协同，系统极易陷入瘫痪：</p><ul><li>连接数被打满：PostgreSQL 的总连接数是有限的，盲目增加组件并发，会导致数据库连接迅速耗尽，后续请求直接报错。</li><li>组件间的连接争抢：SQLAlchemy 连接池有“懒加载”机制，且空闲连接在过期前不会释放。如果配置不当，会出现非核心组件占用大量空闲连接，而核心组件因拿不到连接而“饥饿”的情况。</li></ul><p><strong>解决方案：经过实战验证的“生产级配置矩阵”</strong></p><p>为了避免用户陷入繁琐的参数试错循环，SAE 团队在真实生产环境下进行了多轮全链路压测。 摸索出了不同流量档位下API 并发度、数据库连接池大小与各组件资源规格之间的<strong>生产级配置清单</strong>。用户无需关心具体的参数计算，只需根据预估流量选择对应的规格档位，确保每一份算力都能转化为实际的业务吞吐量。</p><blockquote>注：压测场景并不包含代码执行（Code Sandbox）链路。dify-sandbox 组件的规格与数量请根据实际业务中代码运行的复杂度自行评估调整。</blockquote><p><a href="https://link.segmentfault.com/?enc=AMoF2WRCiWPjx57lrQK8pw%3D%3D.OzN3KaTtTh0OwyvOexYn70RMcRhRlrZgTu6dttllPaZ6j3VDCrsZKASyTIKYBpJIJl3KML1kqssxaesJuSgOJA%3D%3D" rel="nofollow" target="_blank">Dify性能优化</a></p><h4>瓶颈二：从 200 QPS 到 500 QPS —— Redis 单点瓶颈与读写分离</h4><ol><li><strong>集成 ARMS 链路追踪定位性能瓶颈</strong></li></ol><p>在将数据库连接优化、QPS 稳定提升至 200 后，系统吞吐量难以进一步提高。为定位瓶颈，SAE 团队通过 SAE 平台深度集成的 ARMS 应用监控，对 dify-plugin-daemon 组件进行链路分析——在 SAE 控制台的应用详情页点击“应用监控”，即可查看耗时最长的调用链路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608011" alt="" title="" loading="lazy"/></p><p>Trace 数据显示，下游 Redis 的 SET/DEL 操作频繁失败。SAE 团队尝试将 Redis 实例垂直扩容至最大规格（64 核），但效果甚微：QPS 仅小幅提升，SET/DEL 操作延迟却急剧升高，CPU 利用率迅速打满。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608012" alt="" title="" loading="lazy"/></p><ol start="2"><li><strong>Dify-plugin-daemon 高频读写 Redis 引发单点拥堵</strong></li></ol><p>通过代码分析发现，这是 Dify 业务逻辑与 Redis 单点架构冲突的结果：</p><ul><li>dify-plugin-daemon 在处理每次数据链路请求时，都会生成一个新的 Session ID 并写入 Redis。这种高频的写入逻辑导致 Redis 请求量居高不下。</li><li>原生架构中，所有的 Session 读写请求都全部集中在同一个 Redis 节点上。在 200+ QPS 的高并发冲击下，Redis 单线程处理能力达到极限，导致 set 和 del 等基础操作的耗时急剧增大，从而阻塞了整个请求链路。</li></ul><p><strong>解决方案：集群化改造实现读写分离</strong></p><p>为了突破单机架构限制，SAE 团队深入组件底层，对 dify-plugin-daemon 进行了集群化适配：</p><ul><li>补全集群协议：针对原生组件不支持 Redis Cluster 的问题，SAE 团队修改了底层代码，使其完整支持 Redis Cluster 协议。</li><li>实现读写分离：通过架构升级，将原本集中在单机上的海量请求分发到集群。利用集群的多节点特性，实现了流量的负载分担与读写分离。</li></ul><p>这一改造彻底解决了单点瓶颈，成功支撑业务吞吐量从 200 QPS 平滑提升至 500 QPS。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608013" alt="" title="" loading="lazy"/></p><h2>五、激活全链路数据价值：SLS 从“黑盒运行”到“深度洞察”的透视之眼</h2><p>Dify 上线后，如何评估模型的成本和性能？如何分析业务的走势？依托 SLS 强大的 OLAP 分析引擎，我们无需预先定义表结构，即可对 Dify 的工作流日志进行深度挖掘，构建覆盖“技术指标”与“业务指标”的全景仪表盘。</p><h4><strong>基础设施视角：透视 LLM 成本与性能</strong></h4><p>对于Dify的LLM节点，workflow_node_execution日志中的process_data字段中详细记录了模型的调用情况，可以用来对模型调用情况进行秒级多维分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608014" alt="" title="" loading="lazy"/></p><p><strong>场景 A：Token 消耗与成本审计</strong> 实时监控 Token 的消耗趋势，是控制 AI 成本的关键。我们可以统计输入（prompt_tokens）、输出（completion_tokens）及总 Token 数随时间的变化曲线，精准识别异常流量。</p><p>分析 SQL 示例：</p><pre><code class="plain">node_type:llm | select
  sum(
    json_extract_long(process_data, '$.usage.prompt_tokens')
  ) prompt_tokens,
  sum("process_data.usage.completion_tokens") completion_tokens,
  sum("process_data.usage.total_tokens") total_tokens,
  date_trunc('minute', __time__) t
group by
  t
order by
  t
limit
  all</code></pre><p>注：json中的字段可以在SQL中直接用json_extract_xxx函数进行提取分析，如<code>json_extract_long(process_data, '$.usage.prompt_tokens')</code>。对于常用的字段建议额外建立json子索引，然后在SQL中就可以引用对应的列名，如<code>"process_data.usage.completion_tokens"</code>，便于进行更高效的统计分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608015" alt="" title="" loading="lazy"/></p><p><strong>场景 B：首字延迟（TTFT）性能分位分析</strong> LLM 的响应速度直接影响用户体验。通过分析 <code>time_to_first_token</code> 的 P50、P90、P99 分位值，可以客观评估模型在不同负载下的响应稳定性，为模型路由或推理加速提供数据支撑。</p><p>分析 SQL 示例：</p><pre><code class="plain">node_type:llm | select
  date_format(__time__-__time__ % 60, '%m-%d %H:%i') as time,
  approx_percentile("process_data.usage.time_to_first_token", 0.25) as Latency_p25,
  approx_percentile("process_data.usage.time_to_first_token", 0.50) as Latency_p50,
  approx_percentile("process_data.usage.time_to_first_token", 0.75) as Latency_p75,
  approx_percentile("process_data.usage.time_to_first_token", 0.99) as Latency_p99,
  min("process_data.usage.time_to_first_token") as Latency_min
group by
  time
order by
  time
limit
  all</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608016" alt="" title="" loading="lazy"/></p><h4>业务运营视角：洞察用户意图与转化</h4><p>除了底层的模型指标，SLS 还能帮助我们深入理解业务逻辑。以一个“电商智能客服助手”的Dify应用为例，我们可以利用 SQL 拆解工作流节点的输入输出，辅助运营决策。</p><p><strong>场景 A：用户意图分布趋势</strong> 通过分析工作流中“意图识别”节点的输出结果，我们可以量化统计用户咨询的高频类目（如：退换货、物流查询、优惠券），并观察这些需求随时间的变化趋势，从而指导知识库的优化方向。</p><p>分析 SQL 示例：</p><pre><code class="plain">* and title: 用户意图识别 | select
  json_extract(outputs, '$.text') as "用户意图",
  count(1) as pv
group by
  "用户意图"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608017" alt="" title="" loading="lazy"/></p><p><strong>场景 B：异常诊断与漏斗分析</strong> 通过统计特定节点的错误率或特定意图的后续流转情况，构建漏斗图，快速定位导致用户流失的节点。例如，分析“商品检索”节点的空结果率，以判断是否需要扩充商品知识库。</p><p>可以通过漏斗图，分析观察工作流哪些中间节点出现异常失败的比率较高。</p><p>分析 SQL 示例：</p><pre><code class="plain">status:succeeded | select
  title,
  count(distinct workflow_run_id) cnt
group by
  title
order by
  cnt desc</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608018" alt="" title="" loading="lazy"/></p><h2>六、结语：让 AI 应用回归业务本质</h2><p>从“能用”到“好用”，Dify 的生产级落地需要坚实的基础设施支撑。SAE 与 SLS 的联合方案，不仅仅是两个云产品的简单叠加，而是通过“算力托管”与“存储解耦”的深度协同，为 Dify 带来了全栈 Serverless 化的架构质变：</p><ul><li><strong>全栈弹性：</strong> 计算层随流量秒级伸缩，存储层无惧突发吞吐，完美适配 AI 业务的“潮汐特征”。</li><li><strong>结构性降本：</strong> 彻底消除闲置资源浪费，用低成本的分层存储替代昂贵的数据库扩容，最大化 ROI。</li><li><strong>极致稳定：</strong> 全托管免运维底座配合 I/O 物理隔离，彻底消除单点故障风险与数据库性能黑洞。</li><li><strong>深度洞察：</strong> 打通从基础设施监控到业务数据分析的“黑盒”，用 Token 成本与用户意图数据反哺业务进化。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608019" alt="" title="" loading="lazy"/></p><p>通过 SAE 联合 SLS 发布的这一解决方案，Dify开发者无需再为底层的资源和架构操心，只需一次简单的配置，即可拥有一个高可用、高性能、低成本的 AI 应用环境，从而真正专注于业务创新与 Prompt 调优。</p><p><strong>立即体验：</strong> 欢迎登录<a href="https://link.segmentfault.com/?enc=kOOHvCzGOPUs0ue0w0B9jQ%3D%3D.pm6EwFMHAJgqsayWjn1MIWvpfNLL7H6jtHAnUNmvSirNg6xLn4VoM5WgrKirAjPh" rel="nofollow" target="_blank">阿里云 SAE 控制台</a>，进入应用中心，搜索 Dify 模板，勾选Dify高性能版，开启您的一键托管之旅。</p><h3>了解 Serverless 应用引擎 SAE</h3><p>阿里云 Serverless 应用引擎 SAE 是面向 AI 时代的一站式容器化应用托管平台，以“托底传统应用、加速 AI 创新”为核心理念。它简化运维、保障稳定、闲置特性降低 75% 成本，并通过 AI 智能助手提升运维效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047415441" alt="" title="" loading="lazy"/>面向 AI，SAE 集成 Dify 等主流框架，支持一键部署与弹性伸缩，在 Dify 场景中实现性能<strong>提升 50 倍、成本优化 30% 以上</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047415442" alt="" title="" loading="lazy"/></p><h4>产品优势</h4><p>凭借八年技术沉淀，SAE 入选 2025 年 Gartner 云原生魔力象限全球领导者，亚洲第一，助力企业零节点管理、专注业务创新。SAE 既是传统应用现代化的“托举平台”，也是 AI 应用规模化落地的“加速引擎”。</p><ol><li><strong>传统应用运维的“简、稳、省”优化之道</strong></li></ol><ul><li>简：零运维心智，专注业务创新</li><li>稳：企业级高可用，内置全方位保障</li><li>省：极致弹性，将成本降至可度量</li></ul><p><strong>2.加速 AI 创新：从快速探索到高效落地</strong></p><ul><li>快探索：内置 Dify、RAGFlow、OpenManus 等 热门 AI 应用模板，开箱即用，分钟级启动 POC；</li><li>稳落地：提供生产级 AI 运行时，性能优化（如 Dify 性能提升 50 倍）、无感升级、多版本管理，确保企业级可靠交付；</li><li>易集成：深度打通网关、ARMS、计量、审计等能力，助力传统应用智能化升级。</li></ul><h4>适合谁？</h4><p>✅ 创业团队：没有专职运维，需要快速上线<br/>✅ 中小企业：想降本增效，拥抱云原生<br/>✅ 大型企业：需要企业级稳定性和合规性<br/>✅ 出海企业：需要中国区 + 全球部署<br/>✅ AI创新团队：想快速落地AI应用</p><h4>了解更多</h4><p>产品详情页地址：<a href="https://link.segmentfault.com/?enc=92A0EY7XQghGuN%2Bwj8NO4w%3D%3D.i0UBIhU6hSZfiKBKnG84NvySTapuGEUTtUygB3w4zwOcOANJx5pOQK8vwY%2BJNIrB" rel="nofollow" target="_blank">https://www.aliyun.com/product/sae</a></p>]]></description></item><item>    <title><![CDATA[3天工作量压缩至30分钟，重构我的Go后端开发逻辑 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047608041</link>    <guid>https://segmentfault.com/a/1190000047608041</guid>    <pubDate>2026-02-12 18:04:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>不会吧，你不会现在写代码还是靠拼时长吧，不会吧？</p><p>我曾经也认为，优秀的后端工程师就是写得快、写得多。直到我发现，我把大量时间浪费在了配置环境、用Print查Bug、以及手动排查内存泄漏上。</p><p>真正的效率提升，不仅仅是手速变快了，今天分享8个彻底改变我开发逻辑的工具，它们把我的焦虑变成了生产力。</p><h2>ServBay：我再也不想在本地环境上浪费一秒钟</h2><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnVbJ" alt="image.png" title="image.png"/></p><p>说实话，每次接手新项目或者维护老项目，最让我头疼的不是代码逻辑，而是 <a href="https://link.segmentfault.com/?enc=8JZCaPwZS9E36xiS%2FmXG8w%3D%3D.RUgTilgmolDJ5z6TXjhDMsGBQjGkzULF75x0PSKxDwNI1B6gRFJBUdZQwHvBmKoa" rel="nofollow" target="_blank">Go 环境配置</a>。</p><p>以前为了跑一个老项目，我得去改 <code>.bash_profile</code>，去整 <code>GOPATH</code>，甚至因为版本冲突把本地环境搞得一团糟。如果是混合技术栈，比如还得跑个Java服务，那简直是灾难。</p><p>而 ServBay，它就是拯救我于水深火热中的。它能够了一键安装，多版本共存。我可以同时保留 Go 1.11 和最新的 Go 1.24。</p><p>现在，环境配置对我来说就是点一下鼠标的事。这种隔离且并存的能力，让我工作效率蹭蹭的。</p><h2>Delve：求求大家，别再用Print调试了</h2><p>曾几何时，我也是Print党。遇到Bug，就在代码里疯狂塞 <code>fmt.Println("111")</code>、<code>fmt.Println("here")</code>。</p><p>但在Go的高并发场景下，这种做法是一时爽，事后火葬场。Goroutine一多，控制台输出乱成一锅粥，根本看不出谁先谁后。</p><p><strong>Delve</strong> 能够仔细剖析正在运行的程序。</p><p>不需要修改代码，直接启动调试：</p><pre><code class="bash">dlv debug main.go</code></pre><p>遇到死锁或者逻辑诡异的地方，打个断点，直接看内存里的变量状态。它能清晰地展示出每一个Goroutine停在哪里。自从用了Delve，我解决并发Bug的时间从半天缩短到了几分钟。</p><h2>Cobra：写出让人想用的CLI工具</h2><p><img width="723" height="307" referrerpolicy="no-referrer" src="/img/bVdnVbK" alt="image.png" title="image.png" loading="lazy"/></p><p>以前写内部脚本，我总是偷懒直接解析 <code>os.Args</code>。结果就是，过了一个月，连我自己都忘了参数顺序是怎样的，同事用起来更是怨声载道。</p><p>后来我强制自己用 <strong>Cobra，</strong> 它是Kubernetes都在用的库。用它写出来的工具，天生就带有规范的帮助文档（--help）和子命令结构。</p><p>看看这个架子，写出来就显得很专业：</p><pre><code class="go">package main

import (
        "fmt"
        "github.com/spf13/cobra"
)

func main() {
        var rootCmd = &amp;cobra.Command{
                Use:   "deploy",
                Short: "一键部署工具",
                Run: func(cmd *cobra.Command, args []string) {
                        fmt.Println("正在执行部署逻辑...")
                },
        }
        // 哪怕只是个内部工具，也要像模像样
        rootCmd.Execute()
}</code></pre><p>把烂脚本变成正规军，Cobra是门槛最低的选择。</p><h2>GoVet：编译通过不代表逻辑正确</h2><p>编译器只能告诉我们语法没错，但它不管逻辑是不是弱智。</p><p>我有一次在 <code>if</code> 条件里把 <code>==</code> 写成了 <code>=</code>（虽然Go通常会报错，但在某些特定构造下容易混淆），或者在循环里错误地使用了闭包变量，导致线上数据全错。</p><p><strong>GoVet</strong> 就是为了拦截这种低级但致命的错误存在的。</p><pre><code class="bash">go vet ./...</code></pre><p>它专门扫描那些“看起来对，但执行起来会炸”的代码。比如 <code>Printf</code> 的参数类型不对，或者不可达的代码块。现在我把它做进了提交前的钩子（Pre-commit hook），不通过Vet检查的代码，根本不允许提交。</p><h2>Golangci-lint：我的全自动代码洁癖管家</h2><p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnVbL" alt="image.png" title="image.png" loading="lazy"/></p><p>团队协作最怕什么？怕每个人的代码都有自己的想法。</p><p>与其在Code Review时因为“花括号换不换行”或者“变量名太短”吵架，不如直接上 <strong>Golangci-lint</strong>。</p><p>它不是一个工具，它是一个聚合器，并行跑了50多个检查器。</p><pre><code class="bash">golangci-lint run</code></pre><p>配置好 <code>.golangci.yml</code> 后，它就是就是一个无情的检查机器。未使用的变量、过高的圈复杂度、拼写错误，它全能抓出来。它让Code Review回归到了关注业务逻辑本身，而不是纠结语法细节。</p><h2>Pprof：甚至能看清内存的毛细血管</h2><p>服务上线后CPU突然飙高，或者内存缓慢泄漏，这时候看日志是没用的。以前我只能靠猜，现在我靠 <strong>Pprof</strong>。</p><p>只需要在代码里加一行副作用引入：</p><pre><code class="go">import _ "net/http/pprof"</code></pre><p>然后启动个HTTP服务，就能通过浏览器看到程序的X光片。</p><p>我也经常用命令行来生成火焰图：</p><pre><code class="bash">go tool pprof -http=:8080 http://localhost:6060/debug/pprof/profile</code></pre><p>哪一行代码占用了最多的CPU，哪个对象在这个瞬间分配了最多的内存，一目了然。不夸张地说，Pprof 给了我一种“上帝视角”。</p><h2>Godotenv：别把秘密写在代码里</h2><p>有些初级事故是因为把数据库密码或者AWS Key直接硬编码在代码里，然后推到了Git仓库。</p><p><strong>Godotenv</strong> 是我所有项目的标配。</p><p>开发时，我只需要在本地建一个 <code>.env</code> 文件：</p><pre><code class="plain">DB_SECRET=123456
DEBUG_MODE=true</code></pre><p>代码里直接读：</p><pre><code class="go">import "github.com/joho/godotenv"

func init() {
    // 自动加载，从此告别硬编码
    _ = godotenv.Load() 
}</code></pre><p>这样既方便本地调试，又彻底杜绝了泄密风险。</p><h2>Gosec：上线前的最后一道防线</h2><p><img width="723" height="262" referrerpolicy="no-referrer" src="/img/bVdnVbM" alt="image.png" title="image.png" loading="lazy"/></p><p>即便有了前面的工具，安全漏洞依然防不胜防。比如随机数生成器用得不安全，或者TLS配置太弱。</p><p>人工审查很难发现这些隐患，但 <strong>Gosec</strong> 可以。</p><p>它会扫描代码的抽象语法树（AST），专门寻找安全漏洞。</p><pre><code class="bash">gosec ./...</code></pre><p>它会直接甩一份报告给我，告诉我哪一行代码可能导致SQL注入，哪里的文件权限设置太宽泛。对于金融类或者对安全性要求高的项目，这是必须要跑的流程。</p><hr/><h3>低效是一种选择，而你本可以拒绝</h3><p>开发者的黄金时间极其有限。是用这仅有的精力去和环境配置搏斗、去肉眼查错，还是把它们交给工具，自己专注于构建复杂的系统逻辑？</p><p>这不只是工具的差异，这是职业生涯的加速度差异。</p><p>从今天开始，选两个装上，别让重复劳动毁了你的创造力。</p>]]></description></item><item>    <title><![CDATA[ArkUI框架运行原理与常见性能优化方案 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047608043</link>    <guid>https://segmentfault.com/a/1190000047608043</guid>    <pubDate>2026-02-12 18:03:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、ArkUI框架概述</h2><p>ArkUI是OpenHarmony生态中核心的UI渲染框架，采用声明式开发范式，支持多设备（手机、平板、PC等）多端统一开发。开发者通过ArkTS语言描述界面，框架负责组件树构建、布局测量、渲染绘制及事件处理。底层由方舟运行时引擎驱动，协同无障碍、国际化等系统能力，保障高性能与良好用户体验。</p><h2>二、开发范式与执行机制</h2><ol><li>开发范式<br/>当前ArkUI中主流的开发范式采用ArkTS声明式范式，支持多端统一UI描述。在一些需要更高性能的场景下，可以采用Native API进行开发。</li><li><p>代码执行流程<br/>ETS源码经IDE编译生成ABC中间指令文件，打包成HAP安装包。应用启动时，原能力子系统启动对应应用进程，ArkUI子系统负责组件创建与渲染，最终由图形侧执行渲染指令，完成界面展示。<br/><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdnVaP" alt="image.png" title="image.png"/></p><h2>三、渲染核心流程与状态管理</h2></li><li>组件树构建<br/>框架在运行时维护组件树的压栈与出栈，动态构建UI组件树结构。<br/><img width="208" height="312" referrerpolicy="no-referrer" src="/img/bVdnVaQ" alt="image.png" title="image.png" loading="lazy"/></li><li>布局测量与渲染绘制<br/>父节点传递约束条件，子节点自底向上计算尺寸和位置，完成布局测量。随后根据信息发送渲染指令，执行绘制操作，生成最终界面。<br/><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdnVaR" alt="image.png" title="image.png" loading="lazy"/></li><li>差异更新机制<br/>通过装饰器（如@State、@Provider）实现状态观察，观察过程识别“脏”组件，即需要更新的组件。<br/>ArkUI区分两类“脏”状态：</li><li>布局脏：影响尺寸和位置，需重新测量布局，以及判定影响范围。</li><li><p>绘制脏：仅影响样式，重绘但不重新布局。<br/><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdnVaS" alt="image.png" title="image.png" loading="lazy"/><br/>状态变更触发依赖收集，精准标记相关组件为脏，在布局过程只更新需要刷新的组件，避免造成组件树的重建。</p><h2>四、ArkUI应用开发性能优化方案</h2><p>1、创建过程优化<br/>方案1：使用组件懒加载机制，减少创建数量，提升响应速度。在滚动过程中进行数据读取和加载，使用LazyForEach仅渲染可视区域项，避免一次性数据加载过多，解决页面加载耗时长问题，关于长列表优化可以参考<a href="https://link.segmentfault.com/?enc=zWEXykSOrJ1MMbeDHplFKw%3D%3D.67%2FCyeohJ1Zpv3lE1l4ILTnSlZaOLiGGO2%2FPzLs4BBBnRGaBH14DkcthT4Jdjg0EQtVHekRCGpn1gKFTf9fT82%2FLI16%2BGI7GY4Z95oece5htSEUZ95MsLbwFWN5iyozfM7U%2Fg08j7KBBNh9D5hzku0N1hLpV8Hxy9vLhAS7%2BRhA%3D" rel="nofollow" target="_blank">长列表加载丢帧优化。</a><br/><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdnVaT" alt="image.png" title="image.png" loading="lazy"/><br/>方案2：高负载场景分帧渲染，将本来一帧内加载的数据分成多帧加载，但是分帧渲染需要开发者计算每帧中加载多少数据，操作复杂，因此在必要的情况下才推荐使用。<a href="https://link.segmentfault.com/?enc=%2FNOnEvHq229sFGBknGSvGw%3D%3D.78mVGRmEWfk9mL7N%2B3DjrNsbLGSjPWN2JpgA2pZ%2Fyoi0Pqf8Q%2FeUS0l%2BN1rS7YcaPkgyudPzQatwdTETvKnsjxl2PtwOnTAdoBwxOh6y1SOjNrjAe%2BrndKwlJ8Ls%2Bfa96RJ6HHpKhNRbMFlLweBqPAtvCovA%2B4pwD8S7rstq7zCc11HXnn%2BOSSJAjkDO53TA" rel="nofollow" target="_blank">详请可点击查看</a>。<br/><img width="723" height="519" referrerpolicy="no-referrer" src="/img/bVdnVaU" alt="image.png" title="image.png" loading="lazy"/></p></li></ol><p>2、布局过程优化<br/>方案1：精简组件数量，使用扁平化布局组件（如RelativeContainer、Grid）替代多层Column/Row嵌套，减少中间节点数量。<br/><img width="723" height="95" referrerpolicy="no-referrer" src="/img/bVdnVaV" alt="image.png" title="image.png" loading="lazy"/><br/>方案2：利用布局边界减少布局计算<br/>①对固定尺寸组件设置具体宽高，限制布局影响范围。<br/>②优先使用无状态组件@Builder替代@Component，减少状态依赖。<br/><img width="723" height="450" referrerpolicy="no-referrer" src="/img/bVdnVaW" alt="image.png" title="image.png" loading="lazy"/></p><p>3、更新过程优化<br/>复用替代重建, 利用组件复用机制，减少滑动过程中组件创建、布局开销，提升帧率。<br/><img width="408" height="352" referrerpolicy="no-referrer" src="/img/bVdnVaX" alt="image.png" title="image.png" loading="lazy"/></p><p>4、状态管理优化<br/>可以采用状态管理V2进行开发，状态管理V2相对于状态管理V1优化了更新方式，由V1的对象级观察，优化为属性级观察，可以降低状态更新时带来的开销。详细内容参考：<a href="https://link.segmentfault.com/?enc=qyA230qJNl4NzOAgD8B9hg%3D%3D.YTX30JTm%2FZmmuIkL8FyRMTp9VVsdjDBsA%2BOfwoA4eC517fM03X90xEEAdr19Q1u3bdyJUgT9474NMATOs%2BC%2FQgT5Z8uOOc48L8cZeSf1JMtdOTEQg%2B0Uiro6vHw8oHRTRr2ZS3SlEU8y1XgovgY5d%2F13G9fCeWmdlizyiWDYQ8c%3D" rel="nofollow" target="_blank">状态管理V2。</a></p><h2>五、工具链支持与性能分析</h2><p>推荐使用DevEco Studio内置工具：</p><ul><li>AppAnalyzer：实现“体检-报告-修复”一体化流程，快速定位布局耗时及性能瓶颈。</li><li>通过工具量化指标，结合业务场景，精准实施优化策略。<br/><img width="723" height="128" referrerpolicy="no-referrer" src="/img/bVdnVaY" alt="image.png" title="image.png" loading="lazy"/></li><li>ArkUI Inspector：用于可视化的展示UI组件树，分析UI的布局层次和参数。使用方法可以参考ArkUI Inspector使用说明</li><li>CPU Profiler：Profiler：用于在运行过程中抓取trace和调用栈对耗时点进行分析，使用方法可以参考CPU Profiler的使用指导分析的思路可以参考常用Trace的含义。</li></ul><h2>六、性能标准与实践建议</h2><ul><li>帧率要求：120fps设备单帧耗时≤8ms，90fps设备单帧耗时≤12ms。</li><li>响应速度：页面跳转及交互反馈延迟需低于用户感知阈值，保证流畅体验。<br/>实践中应结合懒加载、分帧渲染、组件复用、扁平化布局及状态管理优化等多种手段，综合提升应用性能和用户体验。</li></ul><h2>七、总结</h2><p>ArkUI框架通过声明式开发范式和高效的状态管理机制，实现了灵活且高性能的UI渲染。性能优化需基于框架运行机制，结合具体业务场景，重点控制组件数量、优化更新粒度、合理利用复用与懒加载策略。借助DevEco Studio提供的丰富工具链，开发者可快速定位性能瓶颈，持续提升鸿蒙应用的流畅度和响应速度。</p><h2>八、更多参考</h2><p>1、界面渲染性能优化<br/>2、<a href="https://link.segmentfault.com/?enc=32fu7o%2B4u6Hy5b0ktVem8A%3D%3D.2G1xYHP4pg4WCEb4uqIkhP6qdllpGyu0yVyz%2FFBiwXgOgQInDok7PPUjjMWr%2Btuxh4D6tyl4L2%2B1Z9zR%2Bn%2BM3cqF1Jp04g6y3Hqo0mGZoY51Rcwmkc2shoX%2ByfRRoFuMI6ypdm5fHbqo%2BukzSbvZPjwcQcJS0P3M2UuRy2mIi%2Fw%3D" rel="nofollow" target="_blank">AppAnalyzer</a></p><p>所有人【华为专家面对面01期】ArkUI框架运行原理与常见性能优化方案 </p><p>了解ArkUI渲染的基本流程，探索通过节点优化、懒加载、预加载、组件复用等技术手段,提升列表场景下应用的流畅度，打造极致流畅的界面体验。<br/>➡️ <a href="https://link.segmentfault.com/?enc=Li57llIRbSSPSrRIEoisrw%3D%3D.se3J789EsNUFhQ%2BimpDDetFagzrn0tIz4bl7XkG%2F3l0nFYghNvnu2jXCtl84jyGm7kuLl279TuGBJKE4nS%2B65Bi3911UHhlQ0W8GqkxAllWPxYqE6SRYNUsj8D3Jo5Fy8y%2Bb98oVDfQpMoPcUn1Lgw%3D%3D" rel="nofollow" target="_blank">详情点击</a></p>]]></description></item><item>    <title><![CDATA[函数计算AgentRun重磅上线知识库功能，赋能智能体更“懂”你 Serverless ]]></title>    <link>https://segmentfault.com/a/1190000047608089</link>    <guid>https://segmentfault.com/a/1190000047608089</guid>    <pubDate>2026-02-12 18:02:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>阿里云函数计算 <strong>AgentRun</strong> 正式推出全新 <strong>知识库功能</strong>，为智能体（Agent）注入更强的语义理解与上下文感知能力。通过深度集成 <strong>百炼知识库</strong> 与 <strong>RAGFlow 知识库</strong>，AgentRun 让开发者能够轻松构建具备“知识”的智能应用，真正实现“更懂用户、更贴场景、更高效响应”。</p><h2>为什么需要知识库？</h2><p>在传统智能体开发中，模型往往依赖通用训练数据，缺乏对特定业务、私有文档或实时信息的理解能力。这导致其在面对专业领域问题、企业内部知识或个性化需求时表现受限。</p><p>AgentRun 的知识库功能正是为解决这一痛点而生——它将外部知识源无缝接入智能体运行流程，通过 <strong>检索增强生成（RAG）</strong> 技术，让智能体在回答问题、执行任务时，能动态调用相关知识，大幅提升准确性、专业性与可信度。</p><h2>双引擎支持：百炼 + RAGFlow，覆盖多元知识形态</h2><h3>百炼知识库绑定</h3><p>函数计算AgentRun可以绑定您账号下已经创建好的<a href="https://link.segmentfault.com/?enc=6GpFDXaDlCfD6eE3L2fTOA%3D%3D.MZ8cb3Qd38PjjJTpvAU6%2FVzjzPggSs2mU957a%2BVh9IR3LEZ9oXJU%2B09dxqjPb%2B8yJUyAnPn0YuM8HZkLjTuiAALOfuLpiZMP6vxtcb3JZyg%3D" rel="nofollow" target="_blank">阿里云百炼知识库</a>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608091" alt="" title=""/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“百炼”，可以多选绑定您账号下已经在阿里云百炼控制台创建好的多个知识库。填写检索配置后，点击创建知识库，即可将您的阿里云百炼知识库绑定至AgentRun平台。</p><h3>RAGFlow知识库绑定</h3><p>函数计算AgentRun可以绑定您账号下已经创建好的RAGFlow知识库。如果您没有RAGFlow知识库，可以点击<a href="https://link.segmentfault.com/?enc=T%2F8vBWl5yupaSXzBytGr8A%3D%3D.2NcMueMjf2QCDf%2BdSRN%2FFnn0OpNJt935Uucr95Kw11VzZVOJtlZ7tOWBMXyTrD5kAE2rSC5aUtO4UG%2Foor7jQuM6PpsH9Upj2Mgh28XSJLm2Xe5z2RKYBLSZ07tsvTsJY4sofyHCyvIThye8mvTMtBEmq7Cwp9K8TKFgTaXIZuJ6bJCnQYpp%2Bc025ORVbDCO8vxPVhtXUykByZ4Ka4ZTtuCD1XlUqvWMhtp1kgZwY%2FiFWgOcV9iOUrjUtNakQlXisszI2%2BocBhFyhPO3C%2BtjQQ%3D%3D" rel="nofollow" target="_blank">此链接</a>，一键在SAE上创建RAGFlow。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608092" alt="" title="" loading="lazy"/></p><p>进入到创建页面，输入知识库名称、描述，选择知识库类型为“RAGFlow”，填写您已部署的RAGFlow的BaseURL、Dataset IDs和API-KEY（将其保存在凭证中）。填写检索配置后，点击创建知识库，即可将您自建的RAGFlow知识库绑定至AgentRun平台。</p><blockquote>RAGFlow知识库详细配置获取方式，可参考<a href="https://link.segmentfault.com/?enc=Wo4ewzBVe%2FaNyL839deYZw%3D%3D.QXRqhftrXzqbmowvHzcWYe1Eh2RixUoTE8%2BHzNFWtd74LJiTr7cM6QaVqJCyCtMsVoXsYsRnTYs9waQY0g4iX%2FchkJvTFvpsXEzU4n5gXCakOo%2BwkJtbI8A8OawN30326ekqaqwIFKYpekfBu9VDdg8QxG7RrCYo%2BM%2Fc55vtEdprFupvvou%2Fw%2Bg7J2oBdkfEdIMuilA4W0ns1aiA8jdjIA%3D%3D" rel="nofollow" target="_blank">此文档</a>。</blockquote><h2>三大集成方式，灵活适配各类开发场景</h2><p>函数计算AgentRun 知识库功能支持快速创建集成、代码集成和MCP集成三种方式，满足不同技术栈和开发习惯。</p><h3>快速创建Agent集成知识库功能</h3><p>对于希望快速验证想法或加速产品迭代的团队，AgentRun 提供了<strong>低代码、可视化</strong>的知识库绑定能力。开发者只需登录 AgentRun 控制台，选择已创建的百炼或 RAGFlow 知识库，将其关联到目标智能体，并配置简单的检索参数（如返回结果数量、相似度阈值等），即可完成集成——全程无需编写一行代码。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608093" alt="" title="" loading="lazy"/></p><p>这一模式极大降低了技术门槛，让产品经理、运营人员甚至非技术背景的创新者也能参与智能体的构建与优化。无论是搭建内部知识问答机器人、客户自助服务助手，还是快速验证某个垂直领域的 AI 应用场景，都能在<strong>几分钟内完成部署并上线试用</strong>。</p><p><strong>代码集成知识库查询能力</strong>对于追求极致灵活性与控制力的开发者，AgentRun 提供了<strong>原生代码级知识库接入能力</strong>。您可以在代码逻辑中，调用<a href="https://link.segmentfault.com/?enc=E4g0Wh71iKkmsCISQgOuFA%3D%3D.hS04OIGCDwEamgFjv6%2B6VAGdzt1Hqu72Bcwkw1DE62DmsTbm7yVwGqmTszaq98Z4B85IcK%2FTIAYMgxZzHfxAgQ%3D%3D" rel="nofollow" target="_blank">AgentRun SDK</a>的知识库检索接口，根据业务上下文动态发起检索请求，精准筛选并注入最相关的信息片段到智能体的推理流程中。您可以使用<a href="https://link.segmentfault.com/?enc=auaNfMU51K%2B%2BFzfSGTYWjA%3D%3D.%2FeQMs6w2HfP7cGPmR6siFs3jAFoa5fgx9%2Fm1BIS6O7tbKV6v1ojieoGO3qSFpiy%2B1dJvh%2BIRdyxGSM%2FceR%2FWLA%3D%3D" rel="nofollow" target="_blank">AgentRun SDK</a>，调用以下封装的接口，进行单知识库查询或多知识库查询。</p><pre><code class="python">fromagentrun.knowledgebaseimportKnowledgeBase
## 获取单知识库，进行查询
knowledgebase=KnowledgeBase.get_by_name("ragflow-test")
single_kb_retrieve_result=knowledgebase.retrieve("&lt;your-query&gt;")
print(single_kb_retrieve_result)
## 获取多知识库，进行查询，支持跨供应商知识库类型检索
multi_kb_retrieve_result=KnowledgeBase.multi_retrieve(
    query="&lt;your-query&gt;",
    knowledge_base_names=["ragflow-test","&lt;your-knowledge-base-name-2&gt;"],
)
print(multi_kb_retrieve_result)</code></pre><p>同样，您可以集成LangChain框架，将知识库的查询能力集成在工具或上下文中。</p><pre><code class="python">"""AgentRun 知识库智能体集成代码示例

使用前，请参考https://docs.agent.run/docs/tutorial/quick-start 配置好相应认证信息和环境变量

curl http://127.0.0.1:9000/openai/v1/chat/completions -X POST \
    -H "Content-Type: application/json" \
    -d '{"messages": [{"role": "user", "content": "什么是Serverless?"}], "stream": true}'
"""

import json
import os
from typing import Any

from langchain.agents import create_agent
import pydash

from agentrun import Config
from agentrun.integration.langchain import model
from agentrun.integration.langchain import knowledgebase_toolset
from agentrun.integration.langgraph.agent_converter import AgentRunConverter
from agentrun.knowledgebase import KnowledgeBase
from agentrun.server import AgentRequest, AgentRunServer
from agentrun.server.model import ServerConfig
from agentrun.utils.log import logger

# 请替换为您已经创建的 模型 名称
AGENTRUN_MODEL_SERVICE = os.getenv("AGENTRUN_MODEL_SERVICE", "&lt;your-model-service&gt;")
AGENTRUN_MODEL_NAME = os.getenv("AGENTRUN_MODEL_NAME", "&lt;your-model-name&gt;")
KNOWLEDGE_BASES = os.getenv("AGENTRUN_KNOWLEDGE_BASES", "ragflow-test").split(",")

if AGENTRUN_MODEL_NAME.startswith("&lt;") or not AGENTRUN_MODEL_NAME:
    raise ValueError("请将 MODEL_NAME 替换为您已经创建的模型名称")

## 加载知识库工具，知识库可以以工具的方式供Agent进行调用
knowledgebase_tools = []
if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
    knowledgebase_tools = knowledgebase_toolset(
        knowledge_base_names=KNOWLEDGE_BASES,
    )
else:
    logger.warning("KNOWLEDGE_BASES 未设置或未替换，跳过加载知识库工具。")

agent = create_agent(
    model=model(AGENTRUN_MODEL_SERVICE, model=AGENTRUN_MODEL_NAME, config=Config(timeout=180)),
    tools=[
        *knowledgebase_tools,   ## 通过工具集成知识库查询能力
    ],
    system_prompt="你是一个 AgentRun 的 AI 专家，可以通过查询知识库文档来回答用户的问题。",
)


async def invoke_agent(request: AgentRequest):
    messages = [
        {"role": msg.role, "content": msg.content}
        for msg in request.messages
    ]

    # 如果配置了知识库，查询知识库并将结果添加到上下文
    if KNOWLEDGE_BASES and not KNOWLEDGE_BASES[0].startswith("&lt;"):
        # 获取用户最新的消息内容作为查询
        user_query = None
        for msg in reversed(request.messages):
            if msg.role == "user":
                user_query = msg.content
                break

        if user_query:
            try:
                retrieve_result = await KnowledgeBase.multi_retrieve_async(
                    query=user_query,
                    knowledge_base_names=KNOWLEDGE_BASES,
                )
                # 直接将检索结果添加到上下文
                if retrieve_result:
                    messages.append({
                        "role": "assistant",
                        "content": json.dumps(retrieve_result, ensure_ascii=False),
                    })
            except Exception as e:
                logger.warning(f"知识库检索失败: {e}")

    input: Any = {"messages": messages}

    converter = AgentRunConverter()
    if request.stream:

        async def async_generator():
            async for event in agent.astream(input, stream_mode="updates"):
                for item in converter.convert(event):
                    yield item

        return async_generator()
    else:
        result = await agent.ainvoke(input)
        return pydash.get(result, "messages[-1].content", "")


AgentRunServer(
    invoke_agent=invoke_agent,
    config=ServerConfig(
        cors_origins=[
            "*"
        ]
    ),
).start()</code></pre><blockquote>注意⚠️：如果您选择了RAGFlow的知识库，<strong>需要确保您的Agent运行环境和RAGFlow的BaseURL的地址处于同一网络环境下，否则AgentRun SDK将无法调用RAGFlow的API实现查询能力。</strong></blockquote><p>通过代码集成，AgentRun 赋予开发者“全栈可控”的能力——既享受函数计算的弹性与免运维优势，又保留对智能体认知过程的深度掌控，真正实现“知识为我所用，逻辑由我定义”。</p><h3>MCP集成：将知识库检索作为Agent的工具调用</h3><p>AgentRun知识库率先实现“Agentic RAG”（智能体RAG）模式——将传统静态检索升级为动态、可编程的智能体工具调用。具体而言，用户可一键将知识库发布为MCP，使其成为大语言模型（LLM）可主动调用的工具之一。在此模式下，LLM不再被动接收上下文，而是具备“工具使用能力”，在推理过程中自主判断何时调用RAG、数据库查询、库存检查等工具，并基于返回结果进行多步推理与任务分解。这种机制使RAG从单一检索功能转变为智能体工具箱中的灵活组件，与其他工具并列协作，显著提升复杂任务的处理能力。其工作方式更贴近人类“思考—行动—反思”的认知流程：模型先分析问题，制定计划，再按需调用多个工具获取信息，最终整合结果生成答案。</p><p>进入其他 &gt;&gt; 工具管理 &gt;&gt; 工具市场，可以搜索到“<strong>AgentRun知识库MCP</strong>” 工具模板，点击安装后，填写知识库名称和类型，即可将知识库的查询能力一件发布成MCP工具给大模型进行调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608094" alt="" title="" loading="lazy"/></p><p>创建完毕后，点击工具详情，即可看到集成调用的工具地址：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047608095" alt="" title="" loading="lazy"/></p><p>基于MCP工具标准协议，AgentRun 支持以标准化方式对接知识库服务，实现跨平台、跨模型的上下文注入能力，保障架构的开放性与可扩展性。</p><h2>结语：从“能回答”到“真理解”，智能体正在拥有“知识之眼”</h2><p>AgentRun 知识库功能的上线，不仅是一次技术能力的升级，更标志着智能体发展迈入新阶段——从依赖通用语料的“泛化应答”，转向基于专属知识的“情境理解”。当智能体能够随时调用企业文档、行业规范、用户历史甚至实时数据，它便不再只是一个语言模型的接口，而成为一个<strong>具备领域认知、上下文记忆与决策依据的数字协作者</strong>。</p><p>未来，随着知识库的持续进化——支持多模态内容、动态更新、跨源推理——AgentRun 将进一步降低构建“有知识、有逻辑、有温度”智能体的门槛。</p><p>我们相信，真正的智能，不在于模型有多大，而在于是否“懂你所需、知你所问、信你所依”。</p><p><strong>AgentRun，正让每一个智能体，学会思考，更学会理解。</strong></p>]]></description></item><item>    <title><![CDATA[日志成本降低 83%：云上 Elasticsearch 和 SelectDB 的基准测试及成本分析 ]]></title>    <link>https://segmentfault.com/a/1190000047608105</link>    <guid>https://segmentfault.com/a/1190000047608105</guid>    <pubDate>2026-02-12 18:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在可观测性场景中，Elasticsearch 常受限于写入性能与高昂成本。在《可观测性方案怎么选？SelectDB vs Elasticsearch vs ClickHouse》一文中提到， 在云上日志服务中，SelectDB 相比 Elasticsearch 展现出明显的性能和成本优势。为进一步探索，本文通过基准测试对比二者表现，验证 SelectDB 在日志场景下性能与成本上的显著优势。1、基准目标和方法本次测试的目的是在可观测性场景下公平比较 SelectDB 和 Elasticsearch 的实际性能和成本，并为用户提供参考数据。为尽可能做到真实和公平，我们设计了如下对比测试：测试环境：使用 腾讯云 Elasticsearch 和 SelectDB Cloud  进行测试，未进行任何针对性调优。测试数据：使用 Elasticsearch 的官方性能测试集http logs，以确保测试中立性（实际更偏向 Elasticsearch）。测试内容：写入性能、查询性能、存储空间和成本的比较，这些是可观测性场景中用户最关心的指标。测试方法：第一阶段比较相同资源下的性能第二阶段比较支持相同负载所需的成本。第二阶段超越了传统的性能测试，以验证性能优势是否能在实际用户需求中转化为成本优势，而不仅仅是一种推断。2、相同资源下的性能比较在测试的第一阶段，比较相同配置下 Elasticsearch 和 SelectDB 的性能和成本。第一步：Elasticsearch 和 SelectDB 分别购买具有相同配置（48vCPU、348GB RAM）的集群，成本分别为 18.83 元/小时 和  16.95 元/小时。（1）腾讯云 Elasticsearch（48c） 费用：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608107" alt="图片" title="图片"/><br/>（2）SelectDB Cloud（48c） 费用：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608108" alt="图片" title="图片" loading="lazy"/><br/>第二步：在 Elasticsearch 中创建索引，并在 SelectDB Cloud 中创建表。为确保公平性，两个系统使用相同的模式，包括字段类型、索引类型、共享/分片数量等。需要注意的是，Elasticsearch 的索引大致对应于 SelectDB 的表。第三步：将相同的 HTTP 日志数据集导入到 Elasticsearch 和 SelectDB Cloud。Elasticsearch 耗时 225 秒，而 SelectDB Cloud 仅需 69 秒。SelectDB Cloud 比 Elasticsearch 快 3.3 倍。第四步：分别在 Elasticsearch 和 SelectDB Cloud 中运行 HTTP 日志测试集的查询。Elasticsearch 中的首次运行（冷查询）耗时 2.049 秒，第二次运行（热启动）耗时 1.691 秒，SelectDB Cloud 中的首次运行（冷查询）耗时 0.599 秒，第二次运行（热启动）耗时 0.52 秒。SelectDB Cloud 在冷查询和热启动时的速度均比 Elasticsearch 快 3  倍以上。第五步：分别获取 Elasticsearch 和 SelectDB Cloud 的存储空间使用情况。Elasticsearch 的存储空间使用量为 12.8GB，而 SelectDB Cloud 的存储空间使用量为 3.3GB。与 Elasticsearch 相比，SelectDB Cloud 的存储空间减少了 75%。通过本次测试可以看出，在相同配置下，SelectDB Cloud 的数据导入性能比 Elasticsearch 快 3.3 倍，查询性能快 3 倍以上，存储空间减少 75%。这意味着，在相同配置下，SelectDB Cloud 的用户将比使用 Elasticsearch 的用户获得数倍的性能提升。在可观测性场景下，用户更关心相同负载和性能下能否真正降低成本。因此，接下来的测试将验证 SelectDB Cloud 的性能优势能转化为多大的实际成本优势。3、成本突破：从性能领先到真正的成本降低在测试的第二阶段，SelectDB Cloud 将缩小至其原始规模的 1/6，与使用 6 倍资源的 Elasticsearch 进行性能比较。第一步：将 SelectDB Cloud 48vCPU 的集群规模缩减至 8vCPU，成本也大幅降低至 2.93 元/小时。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047608109" alt="图片" title="图片" loading="lazy"/><br/>第二步：在仅有 8vCPU 的 SelectDB Cloud 集群中创建相同的表。第三步：将相同的 HTTP 日志数据集导入到具有 8vCPU 的 SelectDB Cloud 集群中。这一过程耗时 140 秒，速度仍比 48vCPU 的 Elasticsearch 云集群快 1.6 倍。第四步：在 8vCPU 的 SelectDB Cloud 集群中运行来自 HTTP 日志测试集的查询。第一次运行（冷查询）耗时 1.389 秒，第二次运行（热启动）耗时 1.246 秒。8 vCPU 的 SelectDB Cloud 在冷查询时比 48vCPU 的 Elasticsearch 还快 47.5%。在第五步中，获取 8vCPU SelectDB Cloud 集群中的存储空间使用情况。SelectDB Cloud 的存储空间使用量仍为 3.3GB，比 Elasticsearch 低 75%。通过本次测试可以看出，在将 SelectDB Cloud 的资源缩减至 Elasticsearch 的 1/6 后，成本仅为 2.93 元/小时，比 Elasticsearch 的 18.83 元/小时 节省了 85% 的费用。尽管成本大幅降低，但性能仍保持显著优势：数据导入性能快 1.6 倍，冷查询性能快 47.5%，存储空间减少 75%。这意味着，对于从 Elasticsearch 切换到 SelectDB Cloud 以支持相同负载的用户来说，SelectDB Cloud 将实现实打实的 83% 成本降低，并提供更好的性能。4、为什么 SelectDB 能如此显著地降低成本SelectDB Cloud 出色的性能和成本优势得益于针对可观测性场景进行的广泛优化。SelectDB 针对日志场景优化倒排索引降低空间占用，数据和索引均采用列式存储，并使用 ZSTD 压缩算法，实现了高压缩率，可大幅减少存储空间。此外，SelectDB 将所有数据存储在低成本的对象存储中，热数据在 SSD 等本地磁盘上进行缓存和加速，利用可观测性数据冷热分层的特点降低存储空间单价。这些特性使 SelectDB 的存储成本比 Elasticsearch 降低了接近一个数量级。 SelectDB 采用存储与计算分离的架构。在写入数据时，计算层仅存在一次计算消耗，避免了 Elasticsearch 存储与计算一体化架构所需的多副本。此外，SelectDB 为日志和追踪等时间序列数据设计了时间序列压缩策略，将后台数据合并的写入放大从 3 降低到 1，大幅节省计算和 IO 资源消耗。SelectDB 专为实时分析而设计，这意味着它支持高性能聚合操作，这些操作常用于可观测性领域。在搜索查询方面，SelectDB 以一种针对日志搜索和topn查询（如SELECT * FROM log WHERE message MATCH 'error' ORDER BY time DESC LIMIT 100）进行优化的方式实现了倒排索引。结果是，SelectDB 在搜索查询方面速度快 2 倍，在聚合查询方面速度快 10 倍。结论在 HTTP 日志基准工作负载下，SelectDB Cloud 与 Elasticsearch 相比实现了 83%的成本降低。在实际生产环境中，许多用户已经在 PB 规模下用 SelectDB 或 Apache Doris 取代了 Elasticsearch，实现了显著的成本节约。您可以阅读来自网易、MiniMax、领创集团、中信信用卡中心的用户故事来了解更多。我们建议您根据实际业务场景设计测试，亲自验证 SelectDB 在成本与性能上的表现。欢迎申请 SelectDB Cloud 试用验证。</p>]]></description></item><item>    <title><![CDATA[【鲲苍提效】应用链路全景透视，让性能问题无处可藏 汉得数字平台 ]]></title>    <link>https://segmentfault.com/a/1190000047607766</link>    <guid>https://segmentfault.com/a/1190000047607766</guid>    <pubDate>2026-02-12 17:05:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607769" alt="" title=""/><br/>汉得鲲苍基础架构管理平台的核心目标是为企业的异构系统提供简单高效的一站式统一闭环管理能力，包括统一资源（集群、主机、存储等）管理、统一应用及部署管理、统一监控管理、统一服务治理，帮助企业实现更快、更好、更全面的异构系统管理。</p><p>接下来我们将会提供一系列推文，介绍鲲苍平台的使用，帮助您快速了解本平台，给您更好的使用体验。</p><p>本文为系列推文的第三十一讲，将介绍如何通过鲲苍监控应用性能，在分布式系统中快速定位性能问题，大大缩短故障排查时间，高效解决性能问题！</p><h2>本篇概述</h2><p>在分布式架构时代，一次用户请求的背后，可能历经数十个服务的流转，如何快速洞察系统性能、精准定位性能瓶颈？鲲苍平台「应用性能监控（APM）」能力，为您提供从全局拓扑到代码堆栈的全链路可观测方案，让应用性能问题无处遁形。</p><h2>功能亮点：应用性能监控接入</h2><h3>1. 新建数据源</h3><p>服务可观测性/监控数据源配置：新增Skywalking类型数据源。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607770" alt="" title="" loading="lazy"/></p><h3>2. 新建应用性能监控集群</h3><p>应用性能监控/应用性能监控集群配置：关联数据源。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607771" alt="" title="" loading="lazy"/></p><h3>3. 应用性能监控接入</h3><p>查看 接入指南 ，按步骤操作：<img referrerpolicy="no-referrer" src="/img/remote/1460000047607772" alt="" title="" loading="lazy"/><br/>部署前端应用时，开启 isTrace ，例如：</p><pre><code class="bash">`ClientMonitor.register({
  accessTokenUrl： http://1.2.3.4:8080/oauth/oauth/token,
  collector:${HOPS_CLUSTER_URL}/v3/segments,
  isAjax: true,
  isTrace: true,
  namespace: '',
  clientId: '';
  clientSecret: '';
})`</code></pre><p>部署后端应用时，通过 javaagent 接入应用性能监控，例如：</p><pre><code class="bash">## 应用启动需要添加以下启动参数
-Xms1024m -Xmx1536m -javaagent:agent_path/skywalking-agent.jar -Dskywalking.agent.namespace=hops-dev -Dskywalking.agent.service_name=hops-dev:hzero-product -Dskywalking.collector.backend_service=127.0.0.1:11800</code></pre><h2>应用性能监控分析</h2><h3>1. 全景拓扑，一眼看懂服务关系</h3><p>基于真实的调用链路数据，自动绘制实时服务依赖关系图。节点颜色动态反映服务健康状态，直观呈现系统架构全貌，依赖关系一目了然。</p><ul><li>边上可查看服务间平均响应延迟，点击可查看详细的平均响应时间、平均吞吐量、平均SLA、响应时间分布等指标</li><li><p>服务实例上点击可查看服务应用性能指数（APDEX）、响应时间、吞吐量、SLA、响应时间分布等指标<img referrerpolicy="no-referrer" src="/img/remote/1460000047607773" alt="" title="" loading="lazy"/></p><h3>2. 链路追踪，穿透每一个调用环节</h3><p>从入口到数据库，完整记录请求在分布式系统中的流转路径。支持查看每个环节的耗时、状态、异常详情、SQL语句，支持多种视图灵活切换，轻松定位慢调用与异常节点。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607774" alt="" title="" loading="lazy"/></p><h3>3. 多维监控，关键指标实时掌控</h3><p><strong>全局概览</strong>：掌握集群整体响应延迟分布、吞吐量排行、慢服务/慢端点排行等。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607775" alt="" title="" loading="lazy"/></p></li></ul><p><strong>服务维度</strong>：深入查看单服务响应时间、吞吐量、SLA、Apdex满意度指数等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607776" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607777" alt="" title="" loading="lazy"/></p><p><strong>服务端点及数据库分析</strong>：分析接口性能与数据库慢查询，全面覆盖应用层到数据层。<img referrerpolicy="no-referrer" src="/img/remote/1460000047607778" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607779" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607780" alt="" title="" loading="lazy"/></p><h3>4. 深度剖析，直击性能根源</h3><p><strong>JVM&amp;实例级深度分析</strong></p><p>针对 Java 服务，鲲苍提供实例级 JVM 健康洞察，从“现象”到“根因”，不再依赖经验猜测：</p><ul><li>CPU 使用率、GC 耗时与次数、线程状态、线程堆栈</li><li>堆内存使用情况与对象分布</li><li>MBean 详情、系统属性与运行环境信息等<img referrerpolicy="no-referrer" src="/img/remote/1460000047607781" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607782" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607783" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607784" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607785" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607786" alt="" title="" loading="lazy"/></li></ul><p><strong>服务链路性能剖析</strong><br/>通过采样跟踪与性能剖析任务，鲲苍可对指定 API 在一段时间内进行方法级堆栈分析，并以火焰图形式呈现调用链。宽而平的“平顶”函数，往往就是性能瓶颈所在，问题定位更直接、更高效。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607787" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607788" alt="" title="" loading="lazy"/></p><h3>5. 应用性能告警，防患于未然</h3><p>基于响应时间、成功率、吞吐量等核心指标，灵活配置告警规则与生效范围，实现应用性能的主动感知与提前预警，助您提前发现风险，保障系统持续稳定运行。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607789" alt="" title="" loading="lazy"/></p><h2>联系我们：</h2><ul><li>如果您想了解鲲苍更详细的功能介绍和产品信息，请登录开放平台查阅我们的产品文档</li><li>如果您有疑问，可以通过开放平台进行工单反馈，问题分类请选择【产品/汉得基础架构管理平台】</li><li>相关产品咨询或更多信息了解，欢迎联系我们。<br/>邮箱：<a href="mailto:openhand@vip.hand" target="_blank">openhand@vip.hand</a>-china.com</li></ul>]]></description></item><item>    <title><![CDATA[[开源] myclaw：2000 行 Go 平替 43 万行的 OpenClaw 荀彧9527 ]]></title>    <link>https://segmentfault.com/a/1190000047607856</link>    <guid>https://segmentfault.com/a/1190000047607856</guid>    <pubDate>2026-02-12 17:04:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>推荐API服务：<a href="https://link.segmentfault.com/?enc=hv9gh9CmlWCICWLvRHyKjQ%3D%3D.dvsRT0uATnTz2P1W4XOU0%2BXHIOfGCYkmpGm1hjvSFfU%3D" rel="nofollow" target="_blank">https://nicecode.cc/</a></p><h3>AI Agent Gateway 赛道的现状</h3><p>2026 年初，AI Agent 领域最火的项目非 OpenClaw 莫属。这个前身为 Clawdbot 🦞（后改名 Moltbot，最终定名 OpenClaw）的项目，在 GitHub 上已经积累了超过 17 万 Star。它的核心理念很直接：给 LLM 一双"手"，让 AI 能操作你的本地系统——执行命令、读写文件、控制浏览器。</p><p>OpenClaw 的架构确实强大：</p><p>• Gateway + Pi Agent：Gateway 是 Node.js WebSocket 服务（默认绑 ws://127.0.0.1:18789），内嵌 Pi（Mario Zechner 写的开源 Coding Agent）通过 JSON-RPC over stdio 做推理和工具调用<br/>• 多模型支持：通过 Pi 的统一 LLM API 接 Anthropic、OpenAI、Google、Ollama 等多家 Provider<br/>• 支持 WhatsApp、Telegram、Discord、iMessage、Slack、Signal 等消息通道<br/>• 沙箱模式、设备配对审批、加密凭据存储<br/>但它也有明显的代价：43 万行 TypeScript 代码，Node.js 运行时，以及相当复杂的依赖链。</p><p>对于只想自托管一个 AI 助手的个人开发者来说，这个体量太重了。myclaw 想做的事情很简单——用 Go 写一个够用的轻量替代。</p><h3>myclaw 是什么</h3><p>myclaw 是一个 Go 编写的自托管 AI Agent Gateway。设计目标三条：</p><ol><li>轻量：核心代码约 2000 行，单二进制部署，无运行时依赖</li><li>实用：覆盖日常场景——Telegram 和飞书双通道、定时任务、记忆持久化</li><li>可扩展：模块化架构，Channel 接口抽象清晰，加新通道写一个 struct 就行<br/>架构上借鉴了 OpenClaw 的 Gateway 模式，但实现上砍掉了所有我用不到的东西。</li></ol><p>myclaw 的整体架构可以用一句话概括：消息总线驱动的服务编排。<br/><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdnU8F" alt="image.png" title="image.png"/><br/>核心组件包括：</p><ol><li>Message Bus（消息总线）<br/>消息总线是 myclaw 的中枢。两种消息类型：</li></ol><p>• InboundMessage：从通道流入，携带 Channel、SenderID、ChatID、Content、Timestamp 等字段<br/>• OutboundMessage：从 Agent 流出，携带 Channel、ChatID、Content、ReplyTo 等字段<br/>通过 Pub/Sub 模式（SubscribeOutbound / DispatchOutbound），各服务之间实现松耦合的事件路由。缓冲区默认 100 条消息，Goroutine 安全。</p><ol start="2"><li>Gateway（网关编排器）<br/>Gateway 是顶层编排器，负责：</li></ol><p>• 组装系统 Prompt（从 AGENTS.md + SOUL.md + 记忆上下文拼接）<br/>• 处理入站消息，调用 Agent 运行时（支持 Anthropic 和 OpenAI 两种 Provider）<br/>• 将 Agent 输出路由到对应的消息通道<br/>• 处理 SIGINT / SIGTERM 优雅关闭<br/>Provider 切换的逻辑很直接——配置里 provider.type 写 openai 就走 OpenAI，其他情况默认 Anthropic。不搞什么抽象工厂，一个 switch 解决。</p><ol start="3"><li>Channel（消息通道）<br/>Channel 接口定义了四个方法：Name()、Start()、Stop()、Send()。目前实现了两个通道：</li></ol><p>Telegram 通道：</p><p>• 基于 telegram-bot-api/v5 长轮询<br/>• Markdown → Telegram HTML 格式转换<br/>• 消息分片（4096 字符限制）<br/>• 发送者白名单过滤<br/>• 代理配置支持（方便国内网络环境）<br/>飞书通道：</p><p>• Webhook 模式，启动一个 HTTP Server 监听 /feishu/webhook（默认端口 9876）<br/>• Tenant Access Token 管理，带缓存和双重检查锁<br/>• URL Verification Challenge 自动应答<br/>• 事件驱动的消息接收（im.message.receive_v1）<br/>• 发送者白名单过滤（基于 open_id）<br/>• Verification Token 校验<br/>飞书通道需要一个公网可达的 Webhook URL。本地开发可以用 Cloudflared 临时隧道，生产环境建议配 DNS。</p><ol start="4"><li>Memory（记忆系统）<br/>记忆系统分为两层：</li></ol><p>• 长期记忆（MEMORY.md）：持久化的知识库<br/>• 每日日记（YYYY-MM-DD.md）：按日期归档的交互记录<br/>提供 ReadLongTerm()、WriteLongTerm()、ReadToday()、AppendToday() 和 GetRecentMemories(days) 方法。默认取最近 7 天的日记，和长期记忆一起组装进 LLM 的系统 Prompt。</p><p>文件就是 Markdown，想手动改也行。</p><ol start="5"><li>Cron（定时任务）<br/>支持三种调度模式：</li></ol><p>• cron：标准 Cron 表达式（基于 robfig/cron/v3）<br/>• every：固定间隔（毫秒级）<br/>• at：一次性定时执行<br/>任务持久化为 JSON（存在 ~/.myclaw/data/cron/jobs.json），支持状态追踪（LastRunAtMs、LastStatus、LastError）和执行后自动删除。任务的执行结果可以通过 deliver 字段指定是否推送到某个消息通道。</p><ol start="6"><li><p>Heartbeat（心跳服务）<br/>定期读取 HEARTBEAT.md 文件内容，触发 Agent 处理。Agent 返回 HEARTBEAT_OK 表示无需进一步操作。默认间隔 30 分钟，适合做周期性自检或主动提醒。</p><h3>为什么用 Go</h3><p>选 Go 不是为了赶时髦，是几个实际的考量：</p></li><li>单二进制部署：go build 产出一个可执行文件，不需要 Node.js 运行时或 Python 虚拟环境。scp 到服务器直接跑</li><li>并发原语：Goroutine + Channel 天然适合消息总线架构。每个通道、每个定时任务、Webhook Server 都是独立的 Goroutine，代码写起来比 async/await 回调链清爽</li><li>内存占用：Go 运行时的内存开销远低于 Node.js / Python，一个长期驻留的 Gateway 进程，这点差别会累积</li><li>交叉编译：GOOS=linux GOARCH=arm64 go build 一行命令编译到任意平台</li></ol><h3>快速开始</h3><p>安装</p><pre><code>go install github.com/stellarlinkco/myclaw/cmd/myclaw@latest</code></pre><p>初始化<br/><code>myclaw onboard</code><br/>这会在 ~/.myclaw/ 下创建配置文件和工作空间：</p><pre><code>~/.myclaw/
├── config.json          # 主配置
├── workspace/
│   ├── AGENTS.md        # Agent 角色定义
│   ├── SOUL.md          # 人格特质
│   ├── HEARTBEAT.md     # 心跳任务提示词
│   └── memory/
│       └── MEMORY.md    # 长期记忆
└── data/
    └── cron/
        └── jobs.json    # 定时任务持久化</code></pre><p>配置<br/>编辑 ~/.myclaw/config.json：</p><pre><code>{
  "agent": {
    "model": "claude-sonnet-4-5-20250929",
    "maxTokens": 8192,
    "temperature": 0.7,
    "maxToolIterations": 20
  },
  "provider": {
    "type": "anthropic",
    "apiKey": "sk-ant-..."
  },
  "channels": {
    "telegram": {
      "enabled": true,
      "token": "your-bot-token",
      "allowFrom": ["123456789"],
      "proxy": ""
    },
    "feishu": {
      "enabled": true,
      "appId": "cli_xxx",
      "appSecret": "xxx",
      "verificationToken": "xxx",
      "port": 9876,
      "allowFrom": ["ou_xxx"]
    }
  },
  "gateway": {
    "host": "0.0.0.0",
    "port": 18790
  }
}</code></pre><p>想用 OpenAI 兼容的 API？把 provider.type 改成 "openai"，填上对应的 Key 和 Base URL 就行。</p><p>也支持环境变量覆盖：<br/><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdnU8H" alt="image.png" title="image.png" loading="lazy"/><br/>一个细节：如果只设了 OPENAI_API_KEY 而没有配 provider.type，myclaw 会自动把 Provider 切到 OpenAI。少一步配置。<br/><strong>运行</strong></p><pre><code># REPL 模式（命令行交互）
myclaw agent

# 单条消息模式
myclaw agent -m "今天的任务清单"

# 完整 Gateway 模式（启动所有服务）
myclaw gateway

# 查看状态
myclaw status</code></pre><h3>部署</h3><p><strong>Docker</strong><br/>myclaw 提供了多阶段 Dockerfile（golang:1.24-alpine 构建，alpine:3.21 运行），编译产物约 10MB。</p><pre><code># 构建并启动
docker compose up -d --build

# 如果需要飞书 Webhook 的公网隧道
docker compose --profile tunnel up -d --build</code></pre><p>Docker Compose 里包含一个可选的 Cloudflared 隧道服务，通过 --profile tunnel 激活。它会自动把飞书 Webhook 端口暴露到公网，省去自己配 Nginx 反向代理的麻烦。</p><p>本地开发也可以直接用 Make：</p><p><code>make tunnel  # 启动 cloudflared 临时隧道</code><br/>拿到 *.trycloudflare.com 的 URL 后填到飞书开放平台的事件订阅里就行。<br/><strong>裸机部署</strong></p><pre><code># 交叉编译
GOOS=linux GOARCH=amd64 go build -ldflags="-s -w" -o myclaw ./cmd/myclaw

# 丢到服务器上
scp myclaw user@server:/usr/local/bin/
ssh user@server "myclaw onboard &amp;&amp; myclaw gateway"</code></pre><h3>人格定制</h3><p>myclaw 的一个有趣设计是通过 Markdown 文件定义 Agent 的"灵魂"。</p><p>AGENTS.md 定义角色和行为准则——你是谁、你能做什么、你的边界在哪里。SOUL.md 定义人格特质——语气、偏好、思维方式。这两个文件会被 Gateway 拼接到系统 Prompt 中。</p><p>这意味着你可以通过编辑两个 Markdown 文件来完全自定义 AI 助手的行为，不需要改任何代码。想要一个严肃的工作助手？改 SOUL.md。想要一个幽默的聊天伙伴？也是改 SOUL.md。</p><h3>与 OpenClaw 的对比</h3><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnU8I" alt="image.png" title="image.png" loading="lazy"/><br/>myclaw 不试图替代 OpenClaw。如果你需要多平台消息通道、完整的沙箱安全模型、Pi Agent 的 Skills 扩展体系，OpenClaw 是更好的选择。myclaw 的定位是：你只需要一个能通过 Telegram 或飞书控制的、带记忆的、能跑定时任务的 AI 助手，并且希望它是一个 10MB 的二进制文件而不是一个 Node.js 项目。</p><h3>测试</h3><p>myclaw 的测试覆盖率在 82%-100% 之间，核心模块都有单元测试：</p><p>• bus_test.go：消息总线的发布/订阅<br/>• channel_test.go：通道接口、Telegram 适配和飞书 Webhook 处理<br/>• config_test.go：配置加载和环境变量覆盖<br/>• cron_test.go：三种调度模式<br/>• gateway_test.go：服务编排和优雅关闭（90.2% 覆盖）<br/>• heartbeat_test.go：心跳触发逻辑<br/>• memory_test.go：记忆读写和上下文组装<br/>• main_test.go：CLI 命令注册<br/>使用依赖注入的 Factory 模式，测试时替换外部依赖。RuntimeFactory、BotFactory、FeishuClientFactory 这些接口让你不需要真实的 Telegram Bot 或 Anthropic API 也能跑完所有测试。</p><pre><code>make test          # 跑全部测试
make test-race     # 带竞态检测
make test-cover    # 生成覆盖率报告</code></pre><h3>安全考量</h3><p>AI Agent Gateway 的安全性不容忽视。OpenClaw 社区已经多次讨论过"投毒网页"导致的 Prompt 注入攻击问题。myclaw 采取了几个基本措施：</p><p>• 发送者白名单：Telegram 和飞书通道都支持 allowFrom 配置，只有白名单中的用户才能触发 Agent<br/>• 工具迭代上限：maxToolIterations 限制单次对话中的工具调用次数，防止 Agent 失控循环<br/>• 工作空间隔离：tools.restrictToWorkspace 默认开启，Agent 的文件操作限制在工作空间目录内<br/>• Webhook 验证：飞书通道支持 Verification Token 校验，防止伪造请求<br/>对于生产环境，建议配合 Docker 容器运行以提供进程级隔离。</p><h3>关键依赖</h3><p>myclaw 的外部依赖保持精简，直接依赖只有 4 个：</p><p>• agentsdk-go（v0.8.0）：Agent 运行时，底层包了 Anthropic SDK 和 OpenAI SDK，处理 ReAct 循环和工具调用<br/>• telegram-bot-api/v5：Telegram Bot API 客户端<br/>• robfig/cron/v3：Cron 表达式解析和调度<br/>• spf13/cobra：CLI 框架<br/>间接依赖包括 anthropic-sdk-go、openai-go、go-sdk（MCP）和 OpenTelemetry 相关的 Tracing 库。go.sum 里条目不少，但运行时真正加载的东西不多。</p><h3>我的看法</h3><p>myclaw 证明了一件事：构建一个实用的 AI Agent Gateway 不需要 43 万行代码。2000 行 Go，两个消息通道，一套记忆系统，一个 Cron 调度器——日常够用了。</p><p>当然它也有明显的不足。没有 Web UI，没有多用户会话隔离，飞书通道目前只支持纯文本消息。如果你的场景需要这些，OpenClaw 或者自己加功能。</p><p>Go 的单二进制部署和低内存占用让它特别适合丢在一台小 VPS 上长期跑着。如果你认同"能用 2000 行解决的问题不要用 43 万行"这个理念，可以试试。</p>]]></description></item><item>    <title><![CDATA[Dragonfly 论文入选 IEEE TON：AI 领域海量镜像与大模型分发的解决方案 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047607891</link>    <guid>https://segmentfault.com/a/1190000047607891</guid>    <pubDate>2026-02-12 17:04:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着生成式人工智能（AIGC）等技术不断演进，海量镜像与大模型的分发成为 AI 领域的一项关键挑战。这些挑战包括：海量分片（数百万个）、高并发拉取需求、严格的延迟要求，以及动态的网络环境等。如何在兼容 OCI 等主流格式，并且无需侵入性的实现动态、高效、可扩展的大规模镜像与模型文件分发系统，已是云原生应用与 AI 服务的迫切需求。</p><p>为了解决这些问题，蚂蚁集团与大连理工大学合作设计了一套动态、高效、可扩展的大规模镜像与模型文件分发系统。近日，由蚂蚁集团与大连理工大学共同撰写关于该系统的论文被 IEEE Transactions on Networking (TON) 期刊录用。TON 是由 IEEE 认可的高影响力学术期刊，在网络与系统领域具有重要影响力。本论文的录用标志着研究成果对行业发展具有前瞻性和创新性。 </p><h2>论文简介</h2><p>论文设计构建了一个高效、可扩展的 P2P 模型分发系统，该系统是对 CNCF 孵化项目 Dragonfly 的增强，通过多层次设计实现了资源优化与数据同步的有机结合，旨在解决传统 P2P 文件分发系统在面对 AI 大模型（如千亿参数模型）分发的特定挑战时表现不佳的问题。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607893" alt="图片" title="图片"/><br/>论文链接：<a href="https://link.segmentfault.com/?enc=3pheekG8exl8sQ59UwJ3uw%3D%3D.6wJ7RJ3%2BiKHd3EUeAWF81npkxJdMSa98hLYZRx4uYvlvTkMNfgkMSVJL3GYLEa9c" rel="nofollow" target="_blank">https://ieeexplore.ieee.org/document/11152005</a><br/>项目官网：<a href="https://link.segmentfault.com/?enc=t9OBZ%2FjMZomC2VmNyY7hxA%3D%3D.3ZQ11q3NvjcKIzn%2F18T08g%3D%3D" rel="nofollow" target="_blank">https://d7y.io</a></p><h2>技术方案与创新方法</h2><p>传统的集中式镜像/模型中心（Container/Model Registry）在并发下载高峰期常遭遇单点带宽瓶颈，导致拉取速度下降、任务延迟增加。另一方面，单纯依赖内容分发网络（CDN）或私有链路虽能缓解部分热点问题，却无法充分利用集群内部节点的空闲带宽资源，同时引入额外的成本开销。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607894" alt="图片" title="图片" loading="lazy"/><br/> 图 1: 文件分发系统架构图</p><p>应对这些问题，本方案引入了该方案引入了三个关键设计：首先，引入轻量级的网络测量机制，通过主动探测网络延迟和推断带宽，实时预测网络信息。其次，设计了可扩展的调度框架，通过将推理与调度解耦，提升了调度系统的资源利用率和响应速度。最后，Trainer 模块采用异步模型训练与推理方法，结合图学习算法，实现了基于突发性任务的增量学习。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607895" alt="图片" title="图片" loading="lazy"/><br/>图 2: 三个关键设计的调度算法</p><p>如图 2 所示，轻量级的网络测量机制确保在有限的可用网络资源下对集群中的每个节点进行高效探测。可扩展的调度框架确保足够的可用资源执行调度任务。异步模型训练和推理方法让算法结合节点特性参数进行聚合，以捕捉集群内的相似性，从而提升带宽预测效果。</p><h2>性能成果</h2><p>性能评估表明，相较于主流系统和算法，本系统在总加载完成时间上实现了至少 10% 的缩减，同时将节点平均带宽利用率提升约 20%。此外，所提出的轻量级探测机制通过减少探测频率和计算复杂度，相比现有网络探测方法有效降低了资源开销。该系统不仅能满足 AI 对大规模模型分发的高并发、低延迟需求，还能更高效地利用集群资源，希望可以为行业提供参考。</p><h2>关于我们</h2><p>蚂蚁集团容器镜像与存储团队，主要参与<br/>Dragonfly(<a href="https://link.segmentfault.com/?enc=mSRreybcrae6rT8iRQ1clg%3D%3D.4LllGiyZVV%2F%2BAzApiUvTsSQK3UOWlVrGW4E5SqTyJBitfh6MK7GMB%2F410NQgSs9J" rel="nofollow" target="_blank">https://github.com/dragonflyoss/dragonfly</a>)、<br/>Nydus(<a href="https://link.segmentfault.com/?enc=2MDMSkZNtNr2onK3oxRz3g%3D%3D.ai7%2FOpjHFygnmPdyP%2B4ZkfGgEVDx%2Bp9VxBWZ4XhJWLQGjaRcPm7%2FtBQTOTzK%2BUNq" rel="nofollow" target="_blank">https://github.com/dragonflyoss/nydus</a>)、<br/>Harbor(<a href="https://link.segmentfault.com/?enc=a%2B5SDMzVo%2B2syWBcavu6QA%3D%3D.JSplnywtnDvnfPdeoIfuz8UMOMa2%2FjI%2BKJGW66zAeKdmH7ApVo4L1pFOZTOsup%2BD" rel="nofollow" target="_blank">https://github.com/goharbor/harbor</a>)和 <br/>ModelPack(<a href="https://link.segmentfault.com/?enc=NmTuv1ZwCYMXQyaeC%2FdbTw%3D%3D.wSWqIhPRw6sXtjFq3Y1UiHRu479W9IDanaokarqMeASPnO0u5hmFVflG9mlNP%2BZS" rel="nofollow" target="_blank">https://github.com/modelpack/model-spec</a>) 等开源项目在内部的开发落地和上游项目的维护。我们致力于打造业内顶尖的容器镜像服务，并推动云原生场景下 AI 模型和镜像分发的社区标准化。</p>]]></description></item><item>    <title><![CDATA[向量数据库选型指南：Pinecone vs Weaviate vs Chroma 深度对比 ꯭꯭听꯭]]></title>    <link>https://segmentfault.com/a/1190000047607897</link>    <guid>https://segmentfault.com/a/1190000047607897</guid>    <pubDate>2026-02-12 17:03:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在AI应用开发的浪潮中，向量数据库已经成为构建智能检索、推荐系统和RAG（检索增强生成）应用的基础设施。当你的应用需要处理嵌入向量、进行语义搜索时，选择合适的向量数据库就显得至关重要。今天我们就来深入对比三款主流向量数据库：Pinecone、Weaviate和Chroma，帮助你做出明智的技术选型。</p><h2>什么是向量数据库？为什么需要它？</h2><p>在传统数据库中，我们通过精确匹配来查询数据——比如查找"张三"的订单记录。但在AI时代，我们面临的是另一个挑战：如何找到"语义相似"的内容？当用户问"如何提升工作效率"时，系统需要找到所有关于时间管理、任务规划、工具使用的相关文档，即便这些文档中并没有出现"工作效率"这四个字。</p><p>这就是向量数据库的用武之地。它将文本、图像等数据转换为高维向量（通常由嵌入模型生成），然后通过计算向量之间的距离（相似度）来实现语义搜索。想象一下，每个概念都是空间中的一个点，相似的概念会聚集在一起，向量数据库就是帮你在这个高维空间中快速找到最近邻居的工具。</p><h2>三款数据库的基因与定位</h2><h3>Pinecone：云原生的性能之选</h3><p>Pinecone诞生于2019年，是一个完全托管的云服务，它的核心理念是"让开发者专注于应用，而不是基础设施"。Pinecone团队在设计之初就瞄准了企业级性能和规模化需求，它采用了专有的优化算法，在处理数十亿级别的向量检索时依然能保持毫秒级的响应速度。</p><p>如果你正在构建一个需要处理海量数据的生产系统，比如全网商品的相似推荐、大规模文档库的智能检索，Pinecone的稳定性和性能表现会让你印象深刻。不过，这种高性能是有代价的——它是一个纯云服务，你无法在本地部署，而且定价相对较高。</p><h3>Weaviate：开源的全能战士</h3><p>Weaviate从2019年开始开源，它的野心更大——不仅仅是一个向量数据库，而是一个完整的AI原生数据库。除了向量搜索，Weaviate还支持传统的CRUD操作、复杂的过滤条件、多模态数据（文本、图像等）的混合查询。</p><p>这款数据库的架构非常灵活，你可以选择云托管，也可以在自己的服务器上部署。它内置了多种向量化模型，甚至支持在查询时实时生成嵌入向量，这对于快速原型开发非常友好。如果你的应用场景复杂，需要结合传统数据库功能和向量搜索，Weaviate会是一个理想的选择。</p><h3>Chroma：轻量级的开发者最爱</h3><p>Chroma是三者中最年轻的，2022年才推出，但它迅速在开发者社区中走红。原因很简单：它足够轻量、足够简单。Chroma的设计哲学是"嵌入即数据库"——你可以用几行Python代码就启动一个向量数据库，无需复杂的配置和部署。</p><p>对于AI应用的原型开发、小规模项目或者本地实验，Chroma简直是完美的工具。它默认使用本地持久化，也支持客户端-服务器模式。虽然在企业级功能和性能上不如前两者，但它的简洁性和易用性让初学者和独立开发者爱不释手。</p><h2>核心能力对比：谁更适合你的场景？</h2><h3>性能与规模</h3><p>在处理亿级向量的场景下，Pinecone展现出了明显的优势。它采用的近似最近邻（ANN）算法经过深度优化，查询延迟可以控制在10-50毫秒之间。Weaviate的性能也相当不错，特别是在合理配置HNSW索引参数后，可以达到相近的水平。Chroma在小规模数据（百万级）下表现良好，但当数据量突破千万级别时，性能会有明显下降。</p><p>一个典型的例子：如果你在构建一个服务百万用户的推荐系统，每天处理上亿次查询，Pinecone的稳定性和性能会给你更多信心。但如果是一个企业内部的知识库检索系统，用户基数有限，Weaviate或Chroma都能胜任。</p><h3>功能丰富度</h3><p>Weaviate在功能上是最全面的。它支持混合搜索（结合关键词和向量）、多租户隔离、复杂的GraphQL查询、自动向量化等。这意味着你可以在一个系统中同时满足传统数据库和向量数据库的需求。</p><p>Pinecone则专注于向量搜索这一核心功能，提供了元数据过滤、命名空间隔离等实用特性，但不会有传统数据库的CRUD操作。Chroma介于两者之间，提供了基础的元数据过滤和简单的查询接口，足够日常使用但不够企业级。</p><h3>部署与运维</h3><p>这是三者差异最大的地方。Pinecone是纯云服务，你无需关心任何基础设施，一个API密钥就能开始使用。这既是优势也是限制——优势在于零运维成本，限制在于你必须依赖外部服务，数据存储在第三方云上。</p><p>Weaviate提供了最大的灵活性：你可以使用官方云服务Weaviate Cloud Services（WCS），也可以通过Docker、Kubernetes自行部署。对于有数据主权要求或需要定制化配置的企业，这种灵活性至关重要。</p><p>Chroma默认是一个嵌入式数据库，可以直接在应用中启动，也支持独立的服务器模式。它的部署极其简单，甚至不需要Docker，一个Python环境就够了。</p><h3>成本考量</h3><p>成本不仅是金钱，还包括学习成本和维护成本。Pinecone按照向量存储量和查询次数计费，对于中大型应用，月度费用可能从数百到数千美元不等。但你节省了运维时间和基础设施成本。</p><p>Weaviate和Chroma都是开源的，如果自行部署，只需要承担服务器成本。Weaviate的云服务定价比Pinecone略低，而且有免费额度。Chroma完全免费，但你需要自己处理扩展性和高可用问题。</p><h2>实际应用场景建议</h2><h3>选择Pinecone，如果你：</h3><ul><li>需要处理数千万甚至数亿级别的向量数据</li><li>对查询延迟有严格要求（如实时推荐系统）</li><li>希望最小化运维工作，专注于业务逻辑</li><li>有充足的预算，愿意为性能和稳定性付费</li></ul><h3>选择Weaviate，如果你：</h3><ul><li>需要结合传统数据库功能和向量搜索</li><li>有数据隐私或本地化部署的要求</li><li>应用场景复杂，需要灵活的查询能力</li><li>希望在开源生态和企业支持之间取得平衡</li></ul><h3>选择Chroma，如果你：</h3><ul><li>正在进行原型开发或概念验证</li><li>数据规模在百万级别以内</li><li>团队规模较小，需要快速上手</li><li>预算有限，或者偏好简单的技术栈</li></ul><h2>技术演进与未来趋势</h2><p>向量数据库领域还很年轻，技术演进非常迅速。Pinecone最近推出了Serverless架构，进一步降低了使用门槛。Weaviate在多模态搜索和AI集成方面持续发力，最新版本已经支持了生成式AI模块。Chroma则在不断优化性能，缩小与成熟产品的差距。</p><p>值得注意的是，传统数据库巨头也在入场。PostgreSQL的pgvector插件、Elasticsearch的向量搜索功能都在快速成熟。选型时也可以考虑这些"混合型"方案，特别是当你已经在使用这些数据库时。</p><h2>结语</h2><p>向量数据库的选型没有绝对的对错，关键是匹配你的实际需求。如果你追求极致性能且预算充足，Pinecone是最省心的选择；如果需要功能全面且部署灵活，Weaviate值得深入研究；如果想要快速启动或控制成本，Chroma会是理想的起点。</p><p>技术选型永远是一个权衡的过程——性能、成本、灵活性、易用性，你需要在这些维度之间找到最适合自己的平衡点。建议在做最终决定前，针对你的真实数据和查询模式做一些基准测试，数据会给你最直观的答案。</p><p>记住，最好的数据库不是功能最多的，也不是性能最强的，而是最适合你的业务场景、团队能力和发展阶段的那一个。</p><hr/><h2>快速对比表格</h2><table><thead><tr><th>特性</th><th>Pinecone</th><th>Weaviate</th><th>Chroma</th></tr></thead><tbody><tr><td><strong>类型</strong></td><td>托管云服务</td><td>开源（可托管）</td><td>开源嵌入式</td></tr><tr><td><strong>推出时间</strong></td><td>2019</td><td>2019</td><td>2022</td></tr><tr><td><strong>适用规模</strong></td><td>亿级+</td><td>千万-亿级</td><td>百万-千万级</td></tr><tr><td><strong>部署方式</strong></td><td>仅云端</td><td>云端/自托管</td><td>嵌入式/服务器</td></tr><tr><td><strong>查询延迟</strong></td><td>10-50ms</td><td>20-100ms</td><td>50-200ms</td></tr><tr><td><strong>功能丰富度</strong></td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td><td>⭐⭐⭐</td></tr><tr><td><strong>易用性</strong></td><td>⭐⭐⭐⭐</td><td>⭐⭐⭐</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td><strong>成本</strong></td><td>高</td><td>中</td><td>低/免费</td></tr><tr><td><strong>企业支持</strong></td><td>是</td><td>是</td><td>社区</td></tr></tbody></table><hr/><p><strong>作者注：</strong> 本文基于2026年2月的技术现状撰写，向量数据库技术发展迅速，建议查阅各产品官方文档获取最新信息。</p><p><strong>相关资源：</strong></p><ul><li>Pinecone官网：<a href="https://link.segmentfault.com/?enc=NmB%2Bvdp%2BdQSuESM0Inq7Mg%3D%3D.j5KX%2Fqd5SXdjQRTkrgMd2OqtLlpFlqtzTLZ8BOy2Ms8%3D" rel="nofollow" target="_blank">https://www.pinecone.io</a></li><li>Weaviate官网：<a href="https://link.segmentfault.com/?enc=b8XBqqsZTteiw5%2FZ%2BCpieg%3D%3D.eouo8sv%2BtQ1UtTpTgonpGgx5sm1RGwi0lJDTo3IlyjA%3D" rel="nofollow" target="_blank">https://weaviate.io</a></li><li>Chroma官网：<a href="https://link.segmentfault.com/?enc=ul0CUjIW%2B6dBdiMKj9BitQ%3D%3D.GeCutvxvlVi3SwQR%2FbsqBzmv7ZnLfn3f8dO0ushq1G8%3D" rel="nofollow" target="_blank">https://www.trychroma.com</a></li></ul>]]></description></item><item>    <title><![CDATA[『NAS』将魂斗罗马里奥塞进NAS里 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047607908</link>    <guid>https://segmentfault.com/a/1190000047607908</guid>    <pubDate>2026-02-12 17:02:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=OIbxR326Hl964XzLpvB60A%3D%3D.MPdd3tnoyNZC3vzjW04vnrP9keghPCQZzcwb2%2Bb7WPZJS2pF%2BpeYk4IP25ZdS4rEqIJeaMHr5f2s2wF3lxqEGiBx79OncNeq6BLdv5zCvzluM0OJuXmeNZAhW4nBL2dU2Yw9iYIJXwMBjHRNz8IE3C9z7QVou2kh4JzWXfI26vU%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>JSNES 是一款怀旧游戏模拟器，无需安装任何客户端，仅通过浏览器即可运行，支持超级马里奥、魂斗罗等海量经典游戏。可部署到 NAS、服务器等设备打造本地怀旧游戏中心，完全免费无广告，轻松重温童年游戏乐趣。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607910" alt="" title=""/></p><p>本次使用飞牛 NAS 部署 JSNES，其他品牌的 NAS 部署流程也是差不多的。</p><p>在“文件管理”找到“docker”文件夹，在里面创建一个“jsnes”文件夹。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607911" alt="" title="" loading="lazy"/></p><p>打开“Docker”，切换到「Compose」面板，创建一个项目。</p><p>项目名称填 <code>jsnes</code>。</p><p>路径选择刚刚在“文件管理”里创建的 <code>/docker/jsnes</code>，具体目录根据你的 NAS 情况来填。</p><p>来源选择“创建docker-compose.yml”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607912" alt="" title="" loading="lazy"/></p><p>输入以下代码：</p><pre><code>services:
  jsnes:
    image: docker.1ms.run/wangz2019/jsnes:1.0.0
    container_name: jsnes
    ports:
      - 3456:80
    restart: always</code></pre><p>我给它配置了 <code>3456</code> 端口，你可以自定义。</p><p>等 jsnes 下载并构建完成后，切换到「容器」面板，找到 jsnes 点击这个“链接”按钮就可以在浏览器打开 jsnes 了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607913" alt="" title="" loading="lazy"/></p><p>支持键盘按键操作。 </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607914" alt="" title="" loading="lazy"/></p><p>在手机也可以玩的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607915" alt="" title="" loading="lazy"/></p><p>除了马里奥和魂斗罗之外，还有淘金者、功夫、坦克大战等众多经典游戏。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607916" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=X8nFGUn4xlbeQ0GIpX0fjQ%3D%3D.SYtNdgI8Y6CHkXVMScGnv2f4yAnjuNnyY5Xd9bbeyxrCdhXM%2FomYx254Lz6ydLkuZ7SYd7kyrmZAbPsmnaGdBs3vao7B3dI2h6VqkN%2FgNs16m%2FRW5Olz2wX0xry6b1rMs68jqhq5h0ZAFguwjMzjp6NfBk%2BsVyRJGOu77Sn%2FS20%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[拆开一看才明白：Codex 这种“本地AI写代码”到底怎么跑起来的？ 吾日三省吾码 ]]></title>    <link>https://segmentfault.com/a/1190000047607960</link>    <guid>https://segmentfault.com/a/1190000047607960</guid>    <pubDate>2026-02-12 17:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你也在折腾“让大模型在本地改代码”的 Agent，八成遇到过这种崩溃瞬间：模型说要执行命令、工具跑完回传一大坨、上下文越滚越长、性能忽高忽低、最后还把权限问题搞成“你敢让我删库我就敢执行”……😅</p><p>OpenAI 的 Codex CLI（本地跨平台软件代理）把这套流程摊开讲得很直白：核心就是 <strong>agent loop（代理循环）</strong>——一个负责“用户 ↔ 模型 ↔ 工具”编排的 harness。把 loop 设计对了，Agent 才不会像一只喝了三杯咖啡的无头苍蝇。</p><p>下面就用更接地气（但依然严谨）的方式，把 Codex 的 agent loop 彻底“拆开看看”，顺便给做 Java/Python 工程化的同学一些能直接落地的套路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607962" alt="image" title="image"/></p><h2>1）所谓 Agent Loop：其实就是“反复问、反复干、反复追加”的流水线</h2><p>Codex 这套 loop 的节奏非常规律：</p><ol><li><strong>收用户输入</strong>：把用户的话塞进要发给模型的 prompt（注意：真实 prompt 不是一段字符串，而是“多条消息/多种 item 的列表”）。</li><li><strong>模型推理（inference）</strong>：把 prompt 送到模型，让模型输出。</li><li><p><strong>分支</strong>：模型输出要么是</p><ul><li><strong>最终回复（assistant message）</strong>：这回合结束；</li><li><strong>工具调用（tool call）</strong>：比如让 agent 执行 <code>ls</code>、读文件、跑测试等。</li></ul></li><li><strong>执行工具 + 追加结果</strong>：agent 执行工具，把工具输出追加回 prompt，再请求模型下一轮推理。</li><li><strong>循环直到停止调用工具</strong>：最后必须以 assistant message 收尾（哪怕主要产出是“本地代码改动”）。</li></ol><p>一回合（turn）里可能有很多次“推理↔工具”迭代；多回合（multi-turn）则会把历史对话都带上，prompt 越滚越长：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607963" alt="image" title="image" loading="lazy"/></p><p>这也解释了为什么 Agent 工程化最容易踩的坑，永远是这两座大山：</p><ul><li><strong>性能</strong>（请求体越来越大，推理越来越贵，缓存还经常失效）</li><li><strong>上下文窗口</strong>（context window）不够用（尤其单回合工具调用特别多的时候）</li></ul><h2>2）Codex 如何“组装 prompt”：不是你以为的一段文本，而是一串分角色的 item</h2><p>Codex CLI 用的是 <a href="https://link.segmentfault.com/?enc=efFWgMQPCLDWE7U8UzagkA%3D%3D.RBZKltXaBq195AM5UHHz5wr2%2FF1QbYfClVSlE%2FZTUrLuwF7%2FOHjPywIENzkCcteX7HvyOCYjKD%2FKDAPxFMl15w%3D%3D" rel="nofollow" target="_blank">Responses API</a>，而不是让用户直接手搓 prompt。用户提交的 JSON 里最关键的三块是：</p><ul><li><code>instructions</code>：系统/开发者指令（Codex 既支持用户配置，也有模型内置 base instructions）</li><li><code>tools</code>：可调用的工具定义列表（Codex 内置 shell、plan 等，也可接 MCP 工具，甚至用 web_search）</li><li><code>input</code>：多条 item 的数组（消息、文件、图片、推理结果、工具调用/输出等都在这里）</li></ul><p>Codex 会先往 <code>input</code> 里插入一堆“铺垫项”，再追加用户的真实提问。典型的插入顺序包含：</p><ul><li><strong>developer 消息：权限/沙箱说明</strong>（只约束 Codex 自带的 shell 工具）</li><li><strong>developer 消息：用户自定义 developer_instructions（可选）</strong></li><li><strong>user 消息：用户指令聚合（可选）</strong>（例如 AGENTS.md/AGENTS.override.md、skills 等）</li><li><strong>user 消息：环境上下文</strong>（cwd、shell 等）</li></ul><p>示例（权限/沙箱说明）：</p><pre><code class="txt">&lt;permissions instructions&gt;
  - description of the sandbox explaining file permissions and network access
  - instructions for when to ask the user for permissions to run a shell command
  - list of folders writable by Codex, if any
&lt;/permissions instructions&gt;</code></pre><p>示例（环境上下文）：</p><pre><code class="txt">&lt;environment_context&gt;
  &lt;cwd&gt;/Users/mbolin/code/codex5&lt;/cwd&gt;
  &lt;shell&gt;zsh&lt;/shell&gt;
&lt;/environment_context&gt;</code></pre><p>而真正发到 Responses API 的 <code>input</code> item，是这种结构：</p><pre><code class="json">{
  "type": "message",
  "role": "user",
  "content": [
    {
      "type": "input_text",
      "text": "Add an architecture diagram to the README.md"
    }
  ]
}</code></pre><p>另外，Codex 还会把工具定义塞到 <code>tools</code> 里，长得大概这样（里面同时包含 shell、plan、web_search、以及 MCP 工具）：</p><pre><code class="javascript">[
  // Codex's default shell tool for spawning new processes locally.
  {
    "type": "function",
    "name": "shell",
    "description": "Runs a shell command and returns its output...",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {
        "command": {"type": "array", "description": "The command to execute"},
        "workdir": {"description": "The working directory..."},
        "timeout_ms": {"description": "The timeout for the command..."}
      },
      "required": ["command"]
    }
  },

  // Codex's built-in plan tool.
  {
    "type": "function",
    "name": "update_plan",
    "description": "Updates the task plan...",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {"plan": "...", "explanation": "..."},
      "required": ["plan"]
    }
  },

  // Web search tool provided by the Responses API.
  { "type": "web_search", "external_web_access": false },

  // MCP server tool (example).
  {
    "type": "function",
    "name": "mcp__weather__get-forecast",
    "description": "Get weather alerts for a US state",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {"latitude": {}, "longitude": {}},
      "required": ["latitude", "longitude"]
    }
  }
]</code></pre><p>Codex 文章里还有几张“prompt 快照”的图，直观看出服务器会把 system、tools、instructions、input 组装成最终 prompt：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607964" alt="image" title="image" loading="lazy"/></p><h2>3）流式推理 + 工具回填：SSE 事件才是“真实对话记录”</h2><p>Codex 发起一次推理，Responses API 会用 SSE（Server-Sent Events）流式返回事件；这些事件不仅用来 UI 实时输出，还会被 Codex 转成内部对象，并追加回 <code>input</code>，供下一轮推理继续使用。</p><p>示例（SSE 事件流片段）：</p><pre><code class="txt">data: {"type":"response.reasoning_summary_text.delta","delta":"ah ", ...}
data: {"type":"response.reasoning_summary_text.delta","delta":"ha!", ...}
data: {"type":"response.reasoning_summary_text.done", "item_id":...}
data: {"type":"response.output_item.added", "item":{...}}
data: {"type":"response.output_text.delta", "delta":"forty-", ...}
data: {"type":"response.output_text.delta", "delta":"two!", ...}
data: {"type":"response.completed","response":{...}}</code></pre><p>如果模型输出了 <code>function_call</code>，Codex 执行工具后会把 <strong>推理摘要、函数调用、函数输出</strong> 一股脑追加回下一次请求的 <code>input</code>，示例：</p><pre><code class="javascript">[
  /* ... original items ... */
  {
    "type": "reasoning",
    "summary": [
      {"type": "summary_text", "text": "**Adding an architecture diagram for README.md**\n\nI need to..."}
    ],
    "encrypted_content": "gAAAAABpaDWNMxMeLw..."
  },
  {
    "type": "function_call",
    "name": "shell",
    "arguments": "{\"command\":\"cat README.md\",\"workdir\":\"/Users/mbolin/code/codex5\"}",
    "call_id": "call_8675309..."
  },
  {
    "type": "function_call_output",
    "call_id": "call_8675309...",
    "output": "&lt;p align=\"center\"&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;..."
  }
]</code></pre><p>对应的第二张快照图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607965" alt="image" title="image" loading="lazy"/></p><p>当模型终于输出 assistant message（不再请求工具）时，这个 turn 才算结束：</p><pre><code class="txt">data: {"type":"response.output_text.done","text":"I added a diagram to explain...", ...}
data: {"type":"response.completed","response":{...}}</code></pre><p>用户再发一句话，就进入下一 turn：需要把上一次 assistant message 和本次 user message 一起追加进去：</p><pre><code class="javascript">[
  /* ... all items from the last request ... */
  {
    "type": "message",
    "role": "assistant",
    "content": [{ "type": "output_text", "text": "I added a diagram to explain the client/server architecture." }]
  },
  {
    "type": "message",
    "role": "user",
    "content": [{ "type": "input_text", "text": "That's not bad, but the diagram is missing the bike shed." }]
  }
]</code></pre><p>第三张快照图更能说明：这玩意儿会一直长一直长……</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607966" alt="image" title="image" loading="lazy"/></p><h2>4）性能：请求体“看似二次方”，但缓存命中能救命</h2><p>很多同学看到“每次把所有历史 input 都带上”第一反应：<strong>这不就是网络传输二次方增长吗？</strong> 没错。</p><p>Responses API 支持 <code>previous_response_id</code> 来减少重复传输但Codex 当前选择不用它，主要为了两点：</p><ul><li><strong>保持请求无状态</strong>：对 API 提供方更友好。</li><li><strong>支持 ZDR（Zero Data Retention）</strong>：不在服务端存历史数据，避免与“零数据保留”冲突；同时 reasoning 的 <code>encrypted_content</code> 允许服务端解密以保留“模型理解”，但不持久化用户数据本身。</li></ul><p>真正的性能关键在于：<a href="https://link.segmentfault.com/?enc=p%2FOe5ynECiM%2B385ByZthSw%3D%3D.f3hqcwGx1q3dt61B4ETyC2%2F7s84stjSJXxFsbfdAPWv4FjfR3F33VNkTLkEVddT%2FPjAPMwPxF5Vx2KU4BvjJ9qXZ89mxW72pTwwc%2FANvRSQ%3D" rel="nofollow" target="_blank"><strong>prompt caching</strong></a>。缓存命中要求非常苛刻：<strong>必须是精确的前缀匹配</strong>。因此缓存命中要求非常苛刻：<strong>必须是精确的前缀匹配</strong>。因此) Codex 特别强调“旧 prompt 是新 prompt 的 exact prefix”，这不是强迫症，是省钱省到骨子里了😂。</p><p>哪些变化会导致 cache miss？</p><ul><li>中途改 <code>tools</code>（甚至工具顺序不稳定都会炸）</li><li>改 <code>model</code>（模型内置指令变了）</li><li>改沙箱配置、审批策略、cwd 等</li></ul><p>Codex 的一个经典教训：早期接入 MCP 工具时，因为 <strong>工具枚举顺序不一致</strong> 导致缓存频繁失效（工具数量一多，直接“性能心电图”📉）。</p><p>为了尽量保住前缀，Codex 对“中途配置变更”的处理是：<strong>追加新消息</strong>，而不是修改旧消息：</p><ul><li>沙箱配置/审批模式变化：追加新的 developer <code>&lt;permissions instructions&gt;</code></li><li>cwd 变化：追加新的 user <code>&lt;environment_context&gt;</code></li></ul><p>这招非常值得抄作业：你改的是“环境状态”，但你不能动 prompt 的“历史前缀”。</p><h2>5）上下文窗口：不够用怎么办？压缩（compaction）才是正解</h2><p>Agent 聊着聊着就爆 context window，这事不是“会不会发生”，而是“什么时候发生”。Codex 的策略是超过阈值就 <strong>compact</strong>：把冗长的 <code>input</code> 替换成一个更短、但能代表历史的 item 列表，让后续推理还能“记得发生过啥”。</p><p>早期需要手动 <code>/compact</code>；后来 Responses API 提供了专门的 compaction endpoint：<br/><a href="https://link.segmentfault.com/?enc=YF0ywUSlzB%2F283ua1y8Wfw%3D%3D.XVgEZ4niHzRNynK7Z6k7GJMJjLNK8ptYB8bgajWz3aToT60dq1J8PWKrQhO5rqCS2xj3Ss2Ya8g%2BhjRAtexnMBQ0O6iE78y6FCD%2Bd7ftM8E%3D" rel="nofollow" target="_blank">https://platform.openai.com/docs/guides/conversation-state#co...</a></p><p>它会返回一组可直接替代原 <code>input</code> 的 items，其中包含 <code>type=compaction</code> 以及不透明的 <code>encrypted_content</code>，用于保留模型的“潜在理解”。Codex 现在会在超过 <code>auto_compact_limit</code> 后自动触发这件事。</p><hr/><h2>6）给做 Java/Python 工程化 Agent 的“抄作业清单”✅</h2><p>把 Codex 的设计拆完，可以总结出几条非常硬核、非常工程的结论：</p><ol><li><strong>把 prompt 当“不可变日志”来设计</strong><br/>追加新事件，不修改旧事件。你改旧的，缓存就没了；你改多了，定位就疯了。</li><li><strong>工具列表要稳定排序 + 版本可控</strong><br/>工具顺序不稳定 ≈ 主动放弃缓存。生产环境别玩“每次启动动态扫描全部工具再随机排列”的刺激游戏。</li><li><strong>权限/沙箱必须写进 prompt</strong><br/>这不是文档，这是模型行为约束的一部分。尤其是 shell 这类高危工具，不写清楚“哪些目录可写、何时需要用户确认”，迟早出事故。</li><li><strong>流式事件要落地成结构化记录</strong><br/>SSE 不只是展示给用户看的“打字效果”，而是下一轮推理的输入材料。要能回放、能审计、能复现。</li><li><strong>上下文管理要有自动化机制</strong><br/>compaction 不是锦上添花，是续命针。阈值触发、摘要策略、加密理解保留，都是工程必须项。</li></ol><hr/><p>当 Agent loop 真正做对了，你会发现“让模型写代码”不再是玄学，更像一套可控的流水线：<strong>可追溯、可审计、可缓存、可压缩、可扩展</strong>。</p><p>而 Codex 把这套“写代码的 AI 代理”最核心的一环——loop——摊开讲清楚了：所谓智能体，很多时候并不是模型多聪明，而是 harness 多靠谱。</p><p>（Codex 开源仓库： <a href="https://link.segmentfault.com/?enc=94A8hvv6OvvNV9c9IvyDfw%3D%3D.pxrN4EG3eqtWKpGlku%2F14O8affAlxU8d7I1NO%2BL1wLw%3D" rel="nofollow" target="_blank">https://github.com/openai/codex</a> ）</p><hr/><p><strong>喜欢就奖励一个“👍”和“在看”呗~</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047106529" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[数据库数据恢复—ASM故障后Oracle数据如何起死回生？ 北亚数据恢复 ]]></title>    <link>https://segmentfault.com/a/1190000047607992</link>    <guid>https://segmentfault.com/a/1190000047607992</guid>    <pubDate>2026-02-12 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>一、Oracle数据库故障描述</strong><br/>一个Oracle数据库故障表现为ASM磁盘组掉线，ASM实例无法挂载（mount）。数据库管理员自行进行简单修复，未能成功，随后联系北亚数据恢复中心恢复数据。</p><p><strong>二、Oracle数据库故障分析方法</strong><br/>北亚企安数据恢复工程师先对底层磁盘展开分析，从组成ASM磁盘组的磁盘中提取ASM元数据作进一步研究。经分析发现，ASM存储元数据已损坏，这就是diskgroup无法挂载的原因。接着，北亚企安数据恢复工程师重组ASM存储空间，导出其中的数据库文件，再对导出的文件进行检测与恢复。若检测显示数据文件完整，后续可直接用其启动数据库；若文件也损坏，则需对底层文件进行解析和恢复。</p><p><strong>三、Oracle数据库数据恢复过程</strong><br/>1、按上述方法分析和提取底层数据，得到ASM元数据，借助其重组出ASM存储空间。<br/>2、得到ASM存储空间后，使用北亚自主开发的ASM解析工具（也可用其他常见工具或自编脚本）解析ASM结构，目的是获取ASM中的数据文件。<br/><img width="723" height="445" referrerpolicy="no-referrer" src="/img/bVc6FHj" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复"/><br/>3、对提取的Oracle数据库文件进行检测。<br/>检测结果：<br/><img width="723" height="417" referrerpolicy="no-referrer" src="/img/bVc6FHk" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/><br/>4、利用北亚自主开发的oracle数据库解析工具，解析所有数据文件中的数据记录，然后按用户需求导入到新数据库中。<br/><img width="718" height="590" referrerpolicy="no-referrer" src="/img/bVc6FHl" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/></p><p><strong>四、Oracle数据库数据恢复成功</strong><br/>通过重组ASM存储空间、对ASM磁盘底层解析，导出恢复后的数据库文件，并进一步对这些文件进行底层解析，再按用户要求将数据导入新数据库。北亚企安数据恢复工程师抽查数据表验证恢复数据，未发现异常，随后通知用户方进行全面数据验证，结果显示数据恢复完整，本次Oracle数据库数据恢复成功。<br/><img width="723" height="323" referrerpolicy="no-referrer" src="/img/bVc6FHm" alt="北亚企安数据恢复—oracle数据恢复" title="北亚企安数据恢复—oracle数据恢复" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[首款 NL2GeoSQL 的测试基准和数据集来了！ 爱可生开源社区 ]]></title>    <link>https://segmentfault.com/a/1190000047607608</link>    <guid>https://segmentfault.com/a/1190000047607608</guid>    <pubDate>2026-02-12 16:09:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>专题介绍</h2><p>当我们对 AI4SQL/AI4DB/DB4AI 类产品进行研究时，我们发现 SQL 领域应用能力的提升很大程度上依赖于高质量的数据集。</p><p>还需要在此基础上进行数据合成，生成针对特定问题的训练集和评估集。为了帮助更多开发者快速获取资源，我们将近年来公开的 Text2SQL/NL2SQL 数据集进行了整理清单，持续分享给大家！</p><p>本期为系列文章的第六期，将介绍 <strong>大模型在地理空间查询 SQL 生成</strong> 和 <strong>提高 NL2SQL 精准度</strong> 方面的两款数据集：<a href="https://link.segmentfault.com/?enc=qNudtB%2BvK2GrN%2Fev2yN8RA%3D%3D.Wj2NtBKfA7Y1CowgWB45CtalYW8xZfQPN%2FVIZhSXEdGVsGSnNpf0F13op%2B4yJf75" rel="nofollow" title="GeoSQL-Eval 论文" target="_blank">GeoSQL-Eval</a> 与 <a href="https://link.segmentfault.com/?enc=5vHWvc99jJW0bFZG7gdxrg%3D%3D.Ytvwh%2BmKn8RXWhM4LTILBOi22X%2BspJZSM7aiNF2zHlOXu3SiOt4zeTDdfq0AePddCoxgBg67V233vTDBKQ6xzA%3D%3D" rel="nofollow" title="DeKeyNLU 论文" target="_blank">DeKeyNLU</a>。</p><h2>GeoSQL-Eval / GeoSQL-Bench</h2><p><a href="https://link.segmentfault.com/?enc=Nirdu%2FV188ots%2BZMMQ%2F2qw%3D%3D.xK8jU%2F4eKMwsaka5aSlMrNHVNe15YGs894xm9326nmuxJ0JBrQQVEAeOP1tR1r5rNlwbzaU%2FbljEODof7OrkuA%3D%3D" rel="nofollow" title="GeoSQL-Eval 排行榜" target="_blank">GeoSQL-Eval</a> 是首个面向 PostGIS 环境的端到端自动化评估框架，旨在衡量大型语言模型在 <strong>地理空间</strong> 数据库查询生成（GeoSQL）方面的性能。</p><p>该研究还包括发布 <strong>GeoSQL-Bench 基准测试数据集</strong>，其中包含 14,178 个实例、340 个 PostGIS 函数和 82 个专题数据库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607610" alt="image2026-2-2 10_41_56.png" title="image2026-2-2 10_41_56.png"/></p><h3>论文意图</h3><p>本文主要针对现有大型语言模型在生成 PostGIS 空间查询（GeoSQL）方面的性能评估难题，探讨如何系统地衡量这些模型的性能，因为目前 <strong>缺乏专门的评估基准和框架</strong>。传统的 NL2SQL 基准测试无法涵盖空间数据类型、函数和坐标系等复杂元素，导致在实际应用场景中出现函数错觉和参数误用等错误。</p><p>为了解决这一问题，论文提出了：</p><ul><li><strong>GeoSQL-Bench 基准测试</strong></li><li><strong>GeoSQL-Eval 评估框架</strong></li></ul><p>这些框架旨在为 <strong>NL2GeoSQL</strong> 任务建立一个标准化、多层次且可执行的评估系统，支持模型能力诊断和优化，并降低不同领域用户使用空间数据库的门槛。</p><h3>数据集分析</h3><p><strong>GeoSQL-Bench 数据集</strong> 采用多源结构化方法构建，涵盖三种类型的任务：</p><ol><li><strong>多项选择题和判断题</strong>（2380 道），基于 PostGIS 3.5 官方手册，测试函数功能、参数顺序、返回类型以及是否符合规范；</li><li><strong>语法级 SQL 生成题</strong>（3744 道），源自手册示例，包含显式提示和欠规范提示，验证模型生成可执行查询的能力；</li><li><strong>表结构检索题</strong>（2155 道），基于使用联合国全球地理信息管理 (UN GGIM) 主题和 ISO 19115 分类构建的包含 82 个真实场景的空间数据库，要求模型使用表结构生成复杂查询。</li></ol><p>所有任务均在 GPT-4o 的辅助下生成，并经过领域专家的三重审核，以确保准确性、多样性和真实性。</p><h2>小结</h2><p>本研究使用 <strong>GeoSQL-Eval 框架</strong> 系统地评估了六大类共 24 个主流模型。</p><p>实验表明，推理增强型模型（例如 GPT-5 和 o4-mini）在复杂的空间查询和多轮查询生成方面表现出色，尤其是在几何任务中展现出显著的准确率优势。通用非推理模型（例如 Claude3.7-Sonnet）在执行效率和语法正确性方面表现更佳。然而，函数调用和参数匹配错误仍然是核心瓶颈，约占 70%，而表结构检索任务由于多表连接逻辑的复杂性而面临最大挑战。</p><p>这项工作建立了首个针对 NL2GeoSQL 任务的标准化评估系统，为自然语言与空间数据库的交互提供了关键的基准和优化方向。</p><h2>DeKeyNLU</h2><p><a href="https://link.segmentfault.com/?enc=aRdRLtcbNQ4l0%2FmLlkvhhw%3D%3D.kDLiI7aS1VOPf51mwdw%2BJb%2BKMZ8Zqt3RcwVqYG5L3Yat215hrgcEl%2Bv30Eb7%2Fv8q" rel="nofollow" title="DeKeyNLU 数据集" target="_blank">DeKeyNLU</a> 通过三层人工交叉验证，实现了任务分解和关键词提取的联合细粒度标注。在此基础上，DeKeySQL 框架创新性地将一个专门的理解模块深度集成到 RAG（结果生成）过程中，建立了一种 “<strong>优先考虑精确语义解析</strong>” 的新范式，<strong>显著提高了复杂查询 SQL 生成的准确性和领域适应性。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607611" alt="image2026-2-3 14_41_56 (1).png" title="image2026-2-3 14_41_56 (1).png" loading="lazy"/></p><h3>论文意图</h3><p>本文旨在解决当前 RAG（检索增强生成）和 CoT（思维链）技术在 NL2SQL（自然语言 SQL 生成）任务中遇到的主要瓶颈：</p><p><strong>通用大模型在任务分解和关键词提取方面的准确性不足。</strong></p><p>现有的数据集在任务分解方面往往过于碎片化，且缺乏特定领域的关键词标注。为了解决这些问题，作者提出了 <strong>DeKeyNLU 数据集</strong> 和 <strong>DeKeySQL 流程</strong>（包含三个模块：用户问题理解、实体检索和生成）。通过对模型进行微调以优化问题理解阶段，最终生成的 SQL 语句的准确性得到了提升。</p><h3>数据集分析</h3><p><strong>DeKeyNLU 数据集</strong> 包含 1500 个高质量标注的问答对，数据来源于 BIRD 基准数据集，涵盖金融、教育等多个领域的真实数据库场景，数据集按 <strong>7:2:1</strong> 的比例划分为训练集、验证集和测试集。</p><p>数据合成采用 “<strong>LLM 预标注 + 人工润色</strong>” 的混合工作流程：</p><ul><li>第一步：使用 GPT-4o 自动生成每个问题的初步任务分解（主任务/子任务）和关键词提取（对象/实现）；</li><li>第二步：三位专家标注员进行三轮交叉验证和修订确保标注质量。</li></ul><h3>小结</h3><p>论文通过引入 <strong>DeKeyNLU 数据集</strong> 和 <strong>DeKeySQL 框架</strong>，证明了 <strong>针对性的任务分解和关键词提取训练能够有效提升 NL2SQL 的性能。</strong></p><p>实验结果表明，利用 DeKeyNLU 对 “用户问题理解” 模块进行微调后，模型在 BIRD 开发集上的准确率从 62.31% 提升至 69.10%，在 Spider 开发集上的准确率从 84.2% 提升至 88.7%。</p><p>在 NL2SQL 流程中，实体检索被认为是影响整体准确率的最关键环节，其次是用户问题理解和修正机制。这些发现凸显了以数据集为中心的方法和精心设计的流程对于提升 NL2SQL 系统能力的重要价值，并为用户实现直观、准确的数据交互铺平了道路。</p>]]></description></item><item>    <title><![CDATA[我把大模型装进了电脑里：Ollama 本地部署全攻略 程序员小崔日记 ]]></title>    <link>https://segmentfault.com/a/1190000047607635</link>    <guid>https://segmentfault.com/a/1190000047607635</guid>    <pubDate>2026-02-12 16:08:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>本地大模型神器来了！Ollama 一键部署 30B 模型实战指南</h2><hr/><h3>一、认识这只"羊驼"</h3><p>如果你最近在研究本地大模型，那你一定绕不开它。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607637" alt="" title=""/></p><p>它叫 <strong>Ollama</strong>。</p><p>官网地址：\<br/><a href="https://link.segmentfault.com/?enc=WB9UYYkvxDztgn%2FUzt5aEQ%3D%3D.IlXYgBhFuFicAXQrZOr21tBmAV9LmipXmZqQbjNzbVc%3D" rel="nofollow" target="_blank">https://ollama.com</a></p><p>一句话总结：</p><blockquote><strong>Ollama = 本地大模型运行与管理工具</strong></blockquote><p>它的核心目标非常简单：</p><p>让你在自己的电脑上，像用 Docker 一样管理和运行大语言模型。</p><hr/><h3>二、为什么 Ollama 这么受欢迎？</h3><p>以前部署大模型通常有三种方式：</p><ul><li>调用 API（长期成本高）</li><li>自己编译部署（流程复杂）</li><li>各种依赖冲突（容易踩坑）</li></ul><p>Ollama 做了一件非常关键的事情：</p><blockquote>把复杂的模型部署，变成一行命令。</blockquote><p>例如：</p><pre><code class="bash">ollama run qwen3:8b</code></pre><p>自动下载\<br/>自动加载\<br/>直接进入对话</p><p>对开发者来说，体验非常流畅。</p><hr/><h3>三、安装与使用</h3><h4>1. 下载安装</h4><p>访问官网下载安装即可。</p><p>支持系统：</p><ul><li>Windows</li><li>macOS</li><li>Linux</li></ul><p>安装完成后即可开始运行模型。</p><hr/><h4>2. 第一次下载模型的注意事项</h4><p>首次运行模型时会自动下载。</p><p>强烈建议：</p><blockquote>在设置中将模型下载目录改到 D 盘或其他大容量磁盘。</blockquote><p>原因：</p><ul><li><code>qwen3:30b</code> 等模型体积较大</li><li>下载后可能占用十几 G 甚至几十 G 空间</li><li>默认路径在 C 盘容易导致磁盘爆满</li></ul><p>提前规划好存储路径非常重要。</p><hr/><h3>四、模型区别与推荐</h3><h4>1. GPT-OSS 系列</h4><p>包含：</p><ul><li>gpt-oss:120b</li><li>gpt-oss:20b</li></ul><p>特点：</p><ul><li>通用对话模型</li><li>适合写作、问答、知识整理</li></ul><p>推荐建议：</p><ul><li>16GB 内存以下建议选择 20b</li><li>高性能设备可以尝试 120b</li></ul><hr/><h4>2. DeepSeek 系列</h4><p>包含：</p><ul><li>deepseek-v3.1:671b-cloud</li><li>deepseek-r1:8b</li></ul><p>特点：</p><ul><li>推理能力较强</li><li>数学与逻辑能力表现不错</li></ul><p>说明：</p><ul><li>671b 为云端模型</li><li>本地可选择 r1:8b 体验推理能力</li></ul><p>适合对逻辑思考要求较高的场景。</p><hr/><h4>3. Qwen3 系列（当前主流推荐）</h4><p>包含：</p><ul><li>qwen3:4b / 8b / 30b</li><li>qwen3-coder:30b / 480b-cloud</li><li>qwen3-vl:4b / 8b / 30b / 235b-cloud</li></ul><h5>（1）qwen3 ------ 通用模型</h5><p>适合：</p><ul><li>日常聊天</li><li>写文章</li><li>知识问答</li><li>代码辅助</li></ul><p>推荐配置参考：</p><ul><li>8GB 内存 → 4b</li><li>16GB 内存 → 8b</li><li>32GB 内存以上 → 30b</li></ul><hr/><h5>（2）qwen3-coder ------ 专业代码模型</h5><p>专为程序员优化：</p><ul><li>代码生成</li><li>代码补全</li><li>Bug 修复</li><li>项目结构生成</li></ul><p>推荐：</p><ul><li>本地优先选择 30b</li><li>480b 为云端版本</li></ul><p>如果你是开发者，这个系列非常值得长期使用。</p><hr/><h5>（3）qwen3-vl ------ 视觉语言模型</h5><p>VL = Vision + Language</p><p>可以实现：</p><ul><li>图片识别</li><li>图文问答</li><li>图片分析</li></ul><p>推荐：</p><ul><li>8b 起步</li><li>追求更好效果可选择 30b</li></ul><hr/><h4>4. Gemma3 系列（Google 系）</h4><p>包含：</p><ul><li>gemma3:1b / 4b / 12b / 27b</li></ul><p>特点：</p><ul><li>体积小</li><li>运行速度快</li><li>资源占用较低</li></ul><p>适合：</p><ul><li>轻量电脑</li><li>老设备</li><li>快速测试</li></ul><p>推荐：</p><ul><li>4b 或 12b 更均衡</li></ul><hr/><h3>五、如果只推荐三个模型</h3><p>综合考虑性能与实用性，建议优先尝试：</p><ul><li>日常聊天：qwen3:8b</li><li>写代码：qwen3-coder:30b</li><li>轻量体验：gemma3:4b</li></ul><p>如果你的机器配置较高：</p><blockquote>可以直接尝试 qwen3:30b。</blockquote><hr/><h3>六、一个必须说明的事实</h3><p>蒸馏模型并不是满血模型。</p><p>参数规模不等于能力等同于顶级闭源模型。</p><p>实际表现取决于：</p><ul><li>CPU / GPU 性能</li><li>显存大小</li><li>内存容量</li><li>是否开启量化</li></ul><p>同一个模型，在不同设备上的表现差距可能非常明显。</p><p>因此建议多尝试不同模型，找到最适合自己机器的版本。</p><hr/><h3>七、本地部署真正的意义</h3><p>本地运行大模型，并不是为了与顶级闭源模型直接竞争。</p><p>它的真正价值在于：</p><ul><li>数据隐私</li><li>零 API 成本</li><li>企业内网部署</li><li>本地知识库整合</li><li>可深度定制</li></ul><p>对于开发者而言，这是可控、可扩展的能力。</p><hr/><h3>结语</h3><p>当你第一次在本地成功运行一个 30B 模型时，那种掌控感非常真实。</p><p>Ollama 的出现，让本地大模型真正进入"普通开发者可用"阶段。</p><p>如果你正在探索 AI 工具链，本地部署值得认真体验一次。</p><hr/><p><strong>作者：程序员小崔日记</strong></p><p>本文由<a href="https://link.segmentfault.com/?enc=pW58eKHO8eOIwvWpTmbzAA%3D%3D.eQovV5FjCz%2FFu%2F5ja5Cej84zHiUThHenNFotzu5Opyw%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[只有243多行的AI训练代码，中文注释版 CRStudio ]]></title>    <link>https://segmentfault.com/a/1190000047607639</link>    <guid>https://segmentfault.com/a/1190000047607639</guid>    <pubDate>2026-02-12 16:07:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Andrej Karpathy刚发布了一个仅用约 250 行纯 Python 代码就实现了 GPT 训练和推理全过程的演示，非常适合用来理解大型语言模型底层的数学原理。</p><blockquote><p>Andrej Karpathy：“新的艺术项目。<br/>用243行纯粹的、无依赖的Python代码实现GPT的训练与推理。这包含了所需内容的完整算法部分，其余的一切都只是为了提升效率。我已无法再进一步简化。</p><p>其工作原理是将完整的LLM架构和损失函数彻底分解为构成它的最基本数学运算（+、<em>、</em>*、log、exp），然后通过一个微小的标量自动求导引擎（micrograd）来计算梯度，优化器使用Adam。<br/>”</p></blockquote><p><img width="690" height="553" referrerpolicy="no-referrer" src="/img/bVdnU4F" alt="image.png" title="image.png"/></p><p>源文件在这里：<a href="https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95" target="_blank">https://gist.github.com/karpathy/8627fe009c40f57531cb18360106ce95</a></p><h4>我用AI添加了注释并手动整理了一下：</h4><pre><code class="python"># 导入所需依赖库
import torch # PyTorch核心库，用于张量计算和神经网络构建
import torch.nn as nn # PyTorch的神经网络模块，包含层、损失函数等
from torch.nn import functional as F # PyTorch的优化器模块，用于模型参数更新
import torch.optim as optim # GPT2的分词器，用于文本的编码和解码
from transformers import GPT2Tokenizer # 从huggingface/transformers导入GPT2分词器，适配英文文本

# 定义全局超参数，控制模型训练和结构
batch_size = 16 # 每次训练的样本数，小批量梯度下降用
block_size = 32 # 上下文窗口大小，模型能看到的最大文本长度
max_iters = 5000 # 训练的最大迭代次数
eval_interval = 100 # 每多少轮迭代评估一次模型性能
learning_rate = 1e-3 # 优化器的学习率，控制参数更新步长
device = 'cuda' if torch.cuda.is_available() else 'cpu' # 模型运行的设备，优先使用GPU(cuda)，无则用CPU
eval_iters = 200 # 每次评估时的迭代次数，取平均减少波动
n_embd = 64 # 嵌入层的维度，模型中隐藏层的特征维度
n_head = 4 # 注意力头的数量，实现多头自注意力
n_layer = 4 # Transformer解码器的层数

# 固定随机种子，保证实验结果可复现
torch.manual_seed(1337)

# 加载GPT2分词器，设置为不使用填充（padding）和未知词（unk）的特殊处理
# GPT2分词器基于BPE(字节对编码)，适配英文自然语言，词表大小约50k
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
# 禁用填充token，因为本微型GPT不做批量填充对齐
tokenizer.pad_token = None
# 禁用未知token，遇到未登录词时直接拆分为子词
tokenizer.unk_token = None

# 加载训练数据：这里使用经典的莎士比亚文本作为训练语料
# 从github拉取原始文本文件，读取为字符串格式
with open('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# 对原始文本进行分词编码，将字符串转换为模型可处理的整数张量
# return_tensors='pt'：返回PyTorch张量；truncation=True：超长文本截断
# max_length=None：不限制单条文本长度，后续按block_size切分
encoded_text = tokenizer(text, return_tensors='pt', truncation=True, max_length=None)
# 提取编码后的输入id，展平为一维张量（shape: [总词数]）
data = encoded_text.input_ids.flatten()

# 划分训练集和验证集：90%数据用于训练，10%用于验证
n = int(0.9 * len(data))
train_data = data[:n]
val_data = data[n:]

# 数据加载函数：随机生成一批训练/验证样本
# split：指定数据集（'train'/'val'），控制加载训练集还是验证集
def get_batch(split):
    # 根据split选择对应的数据集
    data = train_data if split == 'train' else val_data
    # 随机生成batch_size个起始索引，范围：[0, 数据长度-block_size)，保证能取到连续的block_size个词
    ix = torch.randint(len(data) - block_size, (batch_size,))
    # 构造输入张量x：取每个起始索引后连续的block_size个词，shape: [batch_size, block_size]
    x = torch.stack([data[i:i+block_size] for i in ix])
    # 构造目标张量y：取每个起始索引后偏移1的block_size个词（语言模型的预测目标是下一个词），shape: [batch_size, block_size]
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    # 将张量移到指定设备（GPU/CPU）
    x, y = x.to(device), y.to(device)
    return x, y

# 定义评估函数：计算模型在训练/验证集上的平均损失（无梯度计算，提升效率）
# model：待评估的模型实例
@torch.no_grad()  # 装饰器，禁用梯度计算，减少内存占用
def estimate_loss(model):
    # 初始化损失字典，存储训练集和验证集的损失
    out = {}
    # 将模型设为评估模式，关闭Dropout等训练特有的层
    model.eval()
    # 遍历训练集和验证集
    for split in ['train', 'val']:
        # 初始化损失数组，存储每次迭代的损失
        losses = torch.zeros(eval_iters)
        # 循环eval_iters次，计算平均损失
        for k in range(eval_iters):
            # 获取一批样本
            X, Y = get_batch(split)
            # 前向传播，得到模型输出和损失
            logits, loss = model(X, Y)
            # 记录当前迭代的损失
            losses[k] = loss.item()
        # 计算该数据集的平均损失，存入字典
        out[split] = losses.mean()
    # 将模型恢复为训练模式，开启Dropout等层
    model.train()
    return out

# 定义单头自注意力层：实现自注意力的核心逻辑（缩放点积注意力）
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        # 定义查询（q）、键（k）、值（v）的线性投影层，将n_embd维映射到head_size维
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        # 注册三角掩码张量，用于遮蔽未来的词（自回归语言模型，不能看到未来信息）
        # tril：下三角矩阵，upper三角部分填充为-∞，后续softmax后为0
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
        # 定义Dropout层，防止过拟合，随机失活20%的神经元
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # x：输入张量，shape: [batch_size, block_size, n_embd]
        B, T, C = x.shape
        # 线性投影得到q、k、v，shape均为[batch_size, block_size, head_size]
        k = self.key(x)
        q = self.query(x)
        v = self.value(x)

        # 计算注意力权重：q @ k.T / sqrt(head_size)（缩放点积）
        # wei shape: [batch_size, block_size, block_size]
        wei = q @ k.transpose(-2, -1) * C**-0.5
        # 应用掩码：将上三角部分设为-∞，遮蔽未来的词
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))
        # softmax归一化，得到注意力权重（每行和为1）
        wei = F.softmax(wei, dim=-1)
        # 应用Dropout，随机失活部分注意力权重
        wei = self.dropout(wei)
        # 注意力加权求和：权重 @ v，得到输出，shape: [batch_size, block_size, head_size]
        out = wei @ v
        return out

# 定义多头自注意力层：将多个单头注意力的输出拼接，实现多维度的特征提取
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        # 创建num_heads个单头注意力层，存入nn.ModuleList（可被PyTorch识别的层列表）
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        # 线性投影层，将拼接后的特征映射回n_embd维
        self.proj = nn.Linear(head_size * num_heads, n_embd)
        # Dropout层，防止过拟合
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        # 拼接所有单头注意力的输出，dim=-1表示在最后一维拼接
        # 输出shape: [batch_size, block_size, head_size*num_heads]
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        # 线性投影+Dropout，完成多头注意力的输出变换
        out = self.dropout(self.proj(out))
        return out

# 定义前馈网络层：Transformer解码器中的全连接层，实现特征的非线性变换
class FeedFoward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        # 两层线性层+ReLU激活+Dropout，隐藏层维度设为n_embd*4（Transformer原论文设定）
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # 升维
            nn.ReLU(),  # 非线性激活
            nn.Linear(4 * n_embd, n_embd),  # 降维回原维度
            nn.Dropout(0.2),  # 随机失活，防止过拟合
        )

    def forward(self, x):
        # 前向传播，输入输出shape均为[batch_size, block_size, n_embd]
        return self.net(x)

# 定义Transformer解码器块：由「多头自注意力 + 前馈网络」组成，带残差连接和层归一化
class Block(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        # 计算每个注意力头的维度：总嵌入维 / 头数
        head_size = n_embd // n_head
        # 多头自注意力层
        self.sa = MultiHeadAttention(n_head, head_size)
        # 前馈网络层
        self.ffwd = FeedFoward(n_embd)
        # 层归一化层（Pre-LN架构，Transformer原论文是Post-LN，Pre-LN更易训练）
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        # 残差连接 + 层归一化 + 多头自注意力：x = x + sa(ln1(x))
        # 残差连接缓解深度网络的梯度消失问题
        x = x + self.sa(self.ln1(x))
        # 残差连接 + 层归一化 + 前馈网络：x = x + ffwd(ln2(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# 定义微型GPT模型核心类，继承自nn.Module（PyTorch所有网络的基类）
class GPTLanguageModel(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        # 词嵌入层：将词的整数ID映射为n_embd维的向量，vocab_size为词表大小
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        # 位置嵌入层：将位置索引映射为n_embd维的向量，捕捉文本的位置信息
        # 因为Transformer是并行计算，无内置位置信息，需手动加入
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        # Transformer解码器块序列：n_layer个Block堆叠
        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
        # 最后的层归一化层
        self.ln_f = nn.LayerNorm(n_embd)
        # 输出线性层：将n_embd维的特征映射回词表大小，用于预测下一个词的概率
        self.lm_head = nn.Linear(n_embd, vocab_size)

        # 初始化模型参数：使用自定义的初始化方式，提升训练稳定性
        self.apply(self._init_weights)

    # 模型参数初始化函数
    def _init_weights(self, module):
        # 如果是线性层，初始化权重为正态分布（均值0，标准差0.02），偏置为0
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        # 如果是嵌入层，初始化权重为正态分布（均值0，标准差0.02）
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    # 模型前向传播函数：输入x（词ID），y（目标词ID，可选），返回预测logits和损失
    def forward(self, idx, targets=None):
        # idx shape: [batch_size, block_size]
        # targets shape: [batch_size, block_size]
        B, T = idx.shape

        # 词嵌入 + 位置嵌入，shape均为[batch_size, block_size, n_embd]
        tok_emb = self.token_embedding_table(idx)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device))
        # 嵌入层输出：词嵌入+位置嵌入，捕捉词的语义和位置信息
        x = tok_emb + pos_emb

        # 经过Transformer解码器块序列，输出shape不变：[batch_size, block_size, n_embd]
        x = self.blocks(x)
        # 最后的层归一化
        x = self.ln_f(x)
        # 输出线性层，得到logits（未归一化的概率），shape: [batch_size, block_size, vocab_size]
        logits = self.lm_head(x)

        # 如果没有目标值，仅返回logits（用于生成文本）
        if targets is None:
            loss = None
        else:
            # 重塑logits和targets，适配交叉熵损失的输入格式
            # cross_entropy要求输入为[B*T, vocab_size]，目标为[B*T]
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            # 计算交叉熵损失（语言模型的核心损失，预测下一个词的概率）
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    # 文本生成函数：基于当前输入idx，生成后续max_new_tokens个词
    # 自回归生成：每次预测一个词，拼接到输入后，继续预测下一个
    def generate(self, idx, max_new_tokens):
        # idx: 初始输入张量，shape: [batch_size, block_size]
        for _ in range(max_new_tokens):
            # 截取最后block_size个词，保证输入长度不超过模型的上下文窗口
            idx_cond = idx[:, -block_size:]
            # 前向传播，得到logits（无目标值，loss=None）
            logits, loss = self(idx_cond)
            # 取最后一个时间步的logits（预测下一个词的logits），shape: [batch_size, vocab_size]
            logits = logits[:, -1, :]
            # softmax归一化，得到下一个词的概率分布
            probs = F.softmax(logits, dim=-1)
            # 根据概率分布随机采样一个词ID（也可以用argmax取最可能的词，即贪心生成）
            idx_next = torch.multinomial(probs, num_samples=1)
            # 将采样的词ID拼接到输入后，更新输入张量
            idx = torch.cat((idx, idx_next), dim=1)
        # 返回生成后的完整词ID张量
        return idx

# 主程序入口：实例化模型、优化器，开始训练和生成
if __name__ == "__main__":
    # 获取GPT2分词器的词表大小，作为模型的vocab_size
    vocab_size = tokenizer.vocab_size
    # 实例化微型GPT模型，移到指定设备
    model = GPTLanguageModel(vocab_size).to(device)
    # 打印模型参数量，查看模型规模
    print(f"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M")

    # 定义优化器：使用AdamW（Adam的改进版，带权重衰减，防止过拟合）
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    # 训练循环：迭代max_iters次
    for iter in range(max_iters):
        # 每隔eval_interval次迭代，评估模型损失并打印
        if iter % eval_interval == 0:
            losses = estimate_loss(model)
            print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

        # 获取一批训练样本
        xb, yb = get_batch('train')
        # 前向传播，得到logits和损失
        logits, loss = model(xb, yb)
        # 梯度清零：PyTorch梯度会累加，每次迭代前需清零
        optimizer.zero_grad(set_to_none=True)
        # 反向传播：计算损失对模型参数的梯度
        loss.backward()
        # 优化器步骤：更新模型参数
        optimizer.step()

    # 训练完成后，进行文本生成
    # 初始化输入：&lt;|endoftext|&gt;是GPT2的特殊起始token，编码为张量并移到设备
    # shape: [1, 1]（batch_size=1, block_size=1）
    start_idx = tokenizer.encode("&lt;|endoftext|&gt;", return_tensors='pt').to(device)
    # 生成max_new_tokens=500个词
    generated_ids = model.generate(start_idx, max_new_tokens=500)
    # 将生成的词ID解码为字符串，跳过特殊token
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    # 打印生成的文本
    print("\nGenerated text:\n")
    print(generated_text)</code></pre>]]></description></item><item>    <title><![CDATA[【划重点】HarmonyOS 应用市场审核 3.5 驳回“十大高频问题”全解析 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047607645</link>    <guid>https://segmentfault.com/a/1190000047607645</guid>    <pubDate>2026-02-12 16:07:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在 HarmonyOS 生态蓬勃发展的今天，应用上架是开发者面临的关键一环。不少作品因触碰 3.5 条款（应用价值与独特性） 被拒，常见的困惑包括：为何被判“功能单一”或“缺乏实质服务”？<br/>本专题复盘近期审核数据，深度解析 3.5 条款 Top 10 驳回问题，为您提供：</p><ul><li>问题预警： 详解高频被拒问题，提前预警</li><li>调优路径： 给出应用从纯展示向强交互的实操路径</li><li>通关范本： 参考标杆案例，让优化有据可依<br/>读懂 3.5，上架更有数。优化应用实用性，助您的作品顺利开启鸿蒙之旅。</li></ul><p><strong>问题1：不收录应用功能与手机系统自带的功能重复，缺乏独特价值。如简易计算器、桌面时钟、加密备忘录、天气、手电筒、指南针、镜子、日历、计时类等。</strong></p><p><strong>【改进建议】</strong><br/>明确应用的独特价值，确保其核心功能与系统功能非完全重叠，强化在垂直领域的专业能力；交互设计与用户体验优化，通过清晰的界面布局与突出的重点功能，降低用户认知与操作成本，确保其价值能被用户快速感知和顺畅使用。</p><p><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVi" alt="image.png" title="image.png"/></p><p><strong>【优化后成功上架案例】</strong><br/>优化前：与系统备忘录功能相似，只能简单记录。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVk" alt="image.png" title="image.png" loading="lazy"/><br/>优化后：丰富应用核心功能，优化首页打卡统计展示，及新增“膳食食材、养生知识”等养生内容及“组队打卡、隔空传送”等趣味内容。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVl" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题2：应用为有限的信息内容罗列不收录，如信息介绍，生活指南、法律案例展示、书籍推荐、诗词罗列等。</strong><br/><strong>【改进建议】</strong><br/>聚焦应用核心功能，避免信息的简单堆砌与罗列，需打造差异化内容，增强交互设计与闭环体验，提升用户使用体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVm" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>【优化后成功上架案例】</strong><br/>优化前：仅少量菜谱信息罗列，UX设计略显欠缺。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVn" alt="image.png" title="image.png" loading="lazy"/></p><p>优化后：丰富应用核心功能，新增“做饭计划、上传菜谱、菜篮子”等模块，满足多种用户使用场景，优化整体界面设计展示，凸显菜品，使应用具有实用性的同时更加美观。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVp" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题3：单一界面的恶作剧恶搞类应用不收录，如屏幕破裂、屏幕鬼魂、模拟打嗝等。</strong><br/><strong>【改进建议】</strong><br/>通过整合多种恶作剧类型，增强应用场景适配性与趣味性。结合创意互动与交互创新，在丰富功能的同时保持界面简洁，提升娱乐性。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVq" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题4：单一H5/Web页面应用不收录，如一张图片，一首音乐、一本书、一个主题、单一影视剧集类、单一非官方游戏攻略类等。</strong><br/><strong>【改进建议】</strong><br/>聚焦应用核心功能，构建复合型功能体系，避免功能单一化；深化内容价值，丰富交互场景与功能闭环，持续提升用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVr" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题5：不收录企业黄页类应用，仅展示文字信息、图片介绍，不提供用户服务。</strong><br/><strong>【改进建议】</strong><br/>强化功能深度，增加功能实用性，提供实际可使用有价值的功能，不能仅企业文字信息、图片介绍，未提供实际的用户服务。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVI" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题6：不收录应用内容均为各种广告推广。</strong><br/><strong>【改进建议】</strong><br/>删除广告推广内容<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVU" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题7：不收录应用页面仅提供简单的预约功能，但未对预约内容进行详细说明，未体现功能的实质价值，未给到用户清晰、准确的服务预期。</strong><br/><strong>【改进建议】</strong><br/>优化功能设计，确保功能闭环完整，消除使用断点，提供清晰明确的功能内容。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVV" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题8：不收录应用无实质功能，仅记录想对自己说的话。</strong><br/><strong>【改进建议】</strong><br/>围绕核心功能深化与拓展实用场景，突出差异化设计理念，强化技术业务协同能力，优化数据存储机制，提升操作便捷性与界面友好度，打造流畅舒适的用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVW" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题9：不收录应用无实质功能和真实用户使用场景，如纯虚构内容。</strong><br/><strong>【改进建议】</strong><br/>围绕真实用户的实际使用场景进行功能规划与设计，提供满足实际使用需求的可用功能；深度优化应用的核心使用场景体验，打磨并升级用户界面的设计质感与交互流畅度。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUVX" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>问题10：不收录资讯类应用内容老旧、过时，最新资讯模块展示过时的新闻报道。</strong><br/><strong>【改进建议】</strong><br/>建立资讯内容定期更新机制，定期对资讯内容进行动态更新与优化，确保信息时效性，保持内容新鲜度与吸引力，避免陈旧内容影响用户体验。<br/><strong>【驳回示例】</strong><br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnUV0" alt="image.png" title="image.png" loading="lazy"/></p><p>➡️ <a href="https://link.segmentfault.com/?enc=PCPVXYhvZhx3xqG2LS05%2Bg%3D%3D.FdWwQ0am8T22aSsdcOICRTF20ZJi1ahPSsgXPPH9new5CLCyfyjCFlU8hwx6EKVSiXcym5pVPajTBpwm39JxVodg1qa6l7V7Cqdi6iNeNI5P3rIUfLoSKvMaRFsLC2k%2FzMCKa9aeBEuqz258YIM0sA%3D%3D" rel="nofollow" target="_blank">原贴指路</a></p>]]></description></item><item>    <title><![CDATA[sizeof与strlen BlackQid ]]></title>    <link>https://segmentfault.com/a/1190000047607650</link>    <guid>https://segmentfault.com/a/1190000047607650</guid>    <pubDate>2026-02-12 16:06:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><code>sizeof</code>计算变量所占内存空间大小，单位是字节，如果操作数是类型，计算的是使用类型创建的变量所占内存空间的大小。<code>sizeof</code>只关注占用内存空间的大小，不在乎内存中存放什么数据。</p><p><code>strlen</code>是C语言库函数，功能是求字符串长度。函数原型如下：</p><pre><code class="c">size_t strlen ( const char * str );</code></pre><p>统计的是从<code>strlen</code>函数的参数<code>str</code>中这个地址开始向后，<code>\0</code>之前字符串中字符的个数。<code>strlen</code>函数会一直向后找<code>\0</code>字符，直到找到为止，所以可能存在越界查找。 <code>strlen</code>关注到了字符串中具体的内容。</p><table><thead><tr><th>sizeof</th><th>strlen</th></tr></thead><tbody><tr><td>1. 是操作符 &lt;br/&gt;2. 计算操作数所占内存的大小，单位是字节&lt;br/&gt;3. 不关注内存中存放什么数据</td><td>1. 是库函数，使用需要包含头文件string.h&lt;br/&gt;2. 是求字符串长度的，统计的是\0之前字符的个数&lt;br/&gt;3. 关注内存中是否有\0，如果没有\0，就会持续往后找，可能会越界</td></tr></tbody></table><p>数组名的意义：</p><ol><li><code>sizeof(数组名)</code>，这里的数组名表示整个数组，计算的是整个数组的大小。</li><li><code>&amp;数组名</code>，这里的数组名表示整个数组，取出的是整个数组的地址。</li><li>除此之外所有的数组名都表示首元素的地址。（二维数组的首元素是一个一维数组）</li></ol><hr/><p>小试牛刀：</p><pre><code class="c">#define _CRT_SECURE_NO_WARNINGS 1
#include &lt;stdio.h&gt;
int main()
{
    char* c[] = { "ENTER","NEW","POINT","FIRST" };
    char** cp[] = { c + 3,c + 2,c + 1,c };
    char*** cpp = cp;
    printf("%s\n", **++cpp);
    printf("%s\n", *--*++cpp+3);
    printf("%s\n", *cpp[-2]+3);
    printf("%s\n", cpp[-1][-1]+1);
    return 0;
}</code></pre>]]></description></item><item>    <title><![CDATA[无需修改内核即可为 PostgreSQL 数据库对象添加自定义属性 IvorySQL ]]></title>    <link>https://segmentfault.com/a/1190000047607656</link>    <guid>https://segmentfault.com/a/1190000047607656</guid>    <pubDate>2026-02-12 16:05:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在开发实践中，经常会遇到一个问题：如何在不修改 PostgreSQL 内核代码的前提下，为数据库对象附加自定义元数据。本文展示了一种基于 PostgreSQL SECURITY LABELS 机制的可行方案，用于实现自定义属性。这种方式具备事务性、与数据库对象强关联，并且能够与标准 PostgreSQL 操作良好协同。</p><h2>问题背景：复制冲突的管理</h2><p>在一个典型的两主节点向第三节点复制数据的架构中，UPDATE/UPDATE 冲突较为常见。复制冲突的处理本身较为复杂，且在多数场景下并无通用解法，但某些列类型可以采用更简单的处理思路。</p><p>例如，在银行业务场景中，账户余额字段通常只允许增减操作。此时可以采用基于增量（delta）的方式：不在冲突时选择某个绝对值，而是分别计算两次更新的变化量并进行叠加。该方法仅需进行基础校验（如溢出检查、余额不小于 0），即可确保所有更新均被正确计入。</p><p>难点在于：如果 PostgreSQL 本身不支持此类机制，如何标记特定列以启用基于 delta 的冲突解决策略？理想状态下，可以直接通过类似以下语法完成：</p><pre><code>ALTER TABLE accounts ALTER COLUMN balance SET delta_apply = 'true';</code></pre><p>但这种深度集成 SQL 语法的方式实现难度较高，且对可移植性意义有限。更现实的需求是通过扩展接口完成设置，例如：</p><pre><code>SELECT my_extension.set_delta_apply('accounts', 'balance', true);</code></pre><p>社区中曾多次讨论为数据库对象引入自定义属性的提案，也曾提交过内核补丁，但实现代码规模较大，而应用场景相对有限，合入内核的可行性存疑。此外，有时需要为索引、大对象或数据类型等非表对象附加属性，这进一步增加了复杂度。更重要的是，即便补丁被接受，也只能影响未来版本，而现实需求往往是“当下可用”。</p><h2>功能需求</h2><p>实现该功能前需明确相关需求：</p><ul><li><strong>对象生命周期绑定</strong>：属性必须与数据库对象建立内部依赖关系。例如，在执行 <code>DROP … CASCADE</code> 删除父对象时，属性也应随之自动删除。</li><li><strong>扩展关联性</strong>：属性需要与扩展建立明确关联，以便在扩展被卸载时由数据库系统正确处理。</li><li><strong>事务性行为</strong>：对象属性需满足事务规则与可见性约束，并行事务修改时，新属性值仅在当前事务内可见，提交前回滚则恢复原值。</li><li><strong>升级与迁移支持</strong>：在 <code>pg_upgrade</code> 以及 dump/restore 过程中，属性应随数据库对象一并正确迁移。</li></ul><p>理想情况下可实现类似 PostgreSQL GUC 的会话级特性，但实现难度显著提升。</p><h2>方案评析</h2><p>以对象 OID 为键的简易全局哈希表无法满足需求，属性值可为变长类型（如字符串），对象与属性的关联实现复杂，且无法保障事务特性与 MVCC 机制。</p><p>另一种思路是在扩展中创建一张 &lt;<code>key</code>, <code>value</code>&gt; 表存储属性，由扩展在运行时查询该表。这在理论上可行，但实践中问题较多：涉及升级、dump/restore、复制一致性等一系列复杂问题，同时还需持续校验对象是否存在，并引入额外的查询开销，整体可靠性较差。</p><h2>实现思路</h2><p>具体目标是为任意表的列定义一个 <code>delta_apply</code> 属性，用于逻辑复制场景下的 UPDATE/UPDATE 冲突处理。当该属性启用时，订阅端不采用传统冲突解决策略，而是计算新旧值之间的差量，并将其累加到订阅端当前值中。</p><p>在 PostgreSQL 中，唯一同时满足上述全部需求的机制是 SECURITY LABELS。尽管该机制最初用于安全模块，但并未限制其仅用于安全相关元数据。需要注意以下事项：</p><ul><li>面向安全的工具可能会检查或校验标签内容。</li><li>需要使用独立且唯一的 provider 名称，以避免冲突。</li></ul><p>它是如何工作的？让我们看看这张图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607658" alt="extension_side.jpg" title="extension_side.jpg"/></p><p>实现的核心依赖于系统表 pg_seclabel。该表中的记录与数据库对象天然绑定，对其的增删改通过 SECURITY LABEL … 工具命令完成。扩展可以通过 utility hook 监听这一过程。</p><p>SECURITY LABEL 支持多种对象类型，包括视图、函数等，基本覆盖常见需求。每条标签记录包含对象的 OID 及对象类型（表、列、函数等），并通过文本字段存储任意自定义数据，同时通过 provider 字段区分不同模块生成的标签。</p><p>由于每次访问都直接查询 pg_seclabel 成本较高，且目前尚无系统级缓存，引入本地哈希缓存以降低开销：</p><pre><code>typedef struct PropertyCacheEntry {
    Oid classId;
    Oid objectId;
    char *propertyValue;
    bool valid;
} PropertyCacheEntry;
static HTAB *property_cache = NULL;</code></pre><p>通过注册 RelcacheCallback 实现缓存失效管理，对象执行 ALTER TABLE 操作时标记对应缓存条目无效。缓存填充策略可根据使用场景调整。例如，在核心 hook 中访问对象时加载，或在扩展提供的用户接口函数中主动加载。</p><h2>属性增删的实现细节</h2><p>为了保持各后端缓存一致性，每次属性变更都需要向其他进程发送失效通知。对象本身发生变化时，可通过 RelcacheCallback 处理；但属性变更本质上只是对 <code>pg_seclabel</code> 的 DML 操作，如何通知其他后端成为问题。</p><p>PostgreSQL 提供了 <a href="https://link.segmentfault.com/?enc=jpN6mfe%2FqbX98U3AvVuYeA%3D%3D.uxhUdfTNyJrTl9gfcAjGohsUKlVboRdomgKVZ2OyV%2BZx3jZazfayyKJyb0Vq92M2emDJ9MjAmSV8TK6M4sOPapHTl4FBEmvKsG%2FRCn1fIphT%2BlZCmaiIaGKxS%2BfnWEIe0DxGoDQAdYDXm4TuxQm6GQ%3D%3D" rel="nofollow" target="_blank">CacheInvalidateRelcacheByRelid</a>，但仅适用于 <code>pg_class</code> 中的对象，对于数据类型等对象无效。因此，在实际实现中，属性变更时会触发一次对象自身的“无实质变更更新”，以借此触发相应的失效回调，从而刷新扩展内部缓存。</p><p>扩展通过 <code>set_property()</code> 接口向用户暴露能力，用于为指定对象设置 SECURITY LABEL。标签文本中描述属性值，例如 <code>delta_apply: true</code>。在扩展中实现的 <code>seclabel_provider</code> 回调负责校验对象类型及属性合法性。</p><p>标签文本字段具备高度灵活性，允许存储复杂结构，例如以 JSON 形式描述属性逻辑。</p><p>通过该机制，客户端与扩展之间建立了一种相对原生的通信方式。扩展可在运行时判断属性是否存在，并据此调整行为。</p><p>在 <code>delta_apply</code> 的具体实现中，逻辑复制订阅端在处理 UPDATE 记录时，会同时查询对应表的属性缓存。若存在标记为增量属性的列，则计算新旧值差量并累加到订阅端当前值。即便在冲突解决策略（如 last-update-wins）下决定拒绝整条更新，增量列的变更仍会被应用，从而降低冲突概率并确保增量更新不丢失。</p><h2>外部干扰处理</h2><p>pg_seclabel 仍然是系统目录表，具备足够权限的用户（如 DBA）可以直接修改其内容。为降低风险，可在扩展中引入内部 GUC，例如 <code>myextension.call_guard</code>。在扩展 UI 函数执行前将其置为 true，结束后重置为 <code>false</code>，并在关键路径中校验其状态是否符合预期。</p><p>理论上，超级用户仍可能通过手段绕过该限制。虽然可以进一步为该 GUC 设置 hook 进行防护，但实现复杂度显著提高，容易演变为过度设计。</p><h2>总结</h2><p>PostgreSQL SECURITY LABELS 机制提供了一种可靠、事务安全的方式，用于在不修改内核的情况下为数据库对象添加自定义属性。尽管该机制最初面向安全模块，但同样适用于扩展级元数据管理，且具备良好的生命周期管理与 MVCC 支持。</p><p>该方案支持多种对象类型，具备事务一致性，并可正确参与 dump/restore 与升级流程。</p><p>原文链接：</p><p><a href="https://link.segmentfault.com/?enc=8z0pNgTWbVKGk08cYoYSAQ%3D%3D.eXB7s3jpsiRxiXqcmAWKa1ohSV2TJeiEVhP3LAKsleIKw%2Fl1302vZ0SNObLsRBJxARZScIwm5KUBy0ehyQ6nr8wIldChdBvRbR2%2FZTtVg7ha92wIANUbIpazi9E%2Bm32fOZ5QKWejaOJ%2FMgOwwBRasA%3D%3D" rel="nofollow" target="_blank">https://www.pgedge.com/blog/custom-properties-for-postgresql-...</a></p><p>作者：Andrei Lepikhov</p><hr/><h2><a href="https://link.segmentfault.com/?enc=%2BOFpBSEcm9rv1T8AMmVgbg%3D%3D.2KBCHSpNCB1Toqj%2FmB2IPAUZelHFhYK5P1J0K0wPKFs%3D" rel="nofollow" target="_blank">HOW 2026 议题招募中</a></h2><p>2026 年 4 月 27-28 日，由 IvorySQL 社区联合 PGEU（欧洲 PG 社区）、PGAsia（亚洲 PG 社区）共同打造的 HOW 2026（IvorySQL &amp; PostgreSQL 技术峰会） 将再度落地济南。届时，PostgreSQL 联合创始人 Bruce Momjian 等顶级大师将亲临现场。</p><p>自开启征集以来，HOW 2026 筹备组已感受到来自全球 PostgreSQL 爱好者的澎湃热情。为了确保大会议题的深度与广度，我们诚邀您在 2026 年 2 月 27 日截止日期前，提交您的技术见解。</p><p>投递链接：<a href="https://link.segmentfault.com/?enc=ohAI8pG9ezBwcQ2OU%2BRvoQ%3D%3D.5JbJDzO1etB%2BruJdZhPuC74k6zbB3BJUUeyXTUqUR%2Bk%3D" rel="nofollow" target="_blank">https://jsj.top/f/uebqBc</a></p>]]></description></item><item>    <title><![CDATA[企业级指标中台 API/JDBC 架构选型四步法 Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047607662</link>    <guid>https://segmentfault.com/a/1190000047607662</guid>    <pubDate>2026-02-12 16:04:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>本文首发于 Aloudata 官方技术博客：<a href="https://link.segmentfault.com/?enc=eIVXIJmKksG7oeyLIEadiA%3D%3D.IkewO0FaHKxJ6H62ywWK%2FbIX5l%2BnNDAovvjz3WTKZtOPhLifYDwkJSclIllKDlESePGgW%2Fj8CBJ0edrjWHedhs2rHUfejXQxzgCcXtjZoTVXPfpYRyYxeyidNZb6T7qz" rel="nofollow" target="_blank">《指标中台全场景消费选型：Aloudata CAN API/JDBC 架构的适配性与扩展性》</a>转载请注明出处。</blockquote><p>摘要：本文面向数据工程团队，提供一套四步评估框架，用于选型指标中台的 API/JDBC 架构。核心聚焦于技术适配性、性能扩展性、生态治理扩展性及安全运维扩展性，并以 Aloudata CAN NoETL 指标平台为例，详解如何通过语义层、计算存储解耦与智能物化加速，构建高并发、全场景的统一指标服务出口。</p><p>在 BI 报表、AI 分析、业务系统等多端消费场景下，企业数据服务接口面临适配性与扩展性的双重挑战。传统 API 架构常因计算存储紧耦合而引发性能瓶颈与安全风险。本文面向数据架构师与工程团队，提供一套聚焦“适配性”与“扩展性”的四步选型评估框架，并以 Aloudata CAN NoETL 指标平台为例，详解如何通过其语义引擎、智能物化加速与开放化服务架构，构建稳定、高效且面向未来的统一指标服务出口。</p><h2>引言：全场景指标消费对 API/JDBC 架构的严苛挑战</h2><p>企业数据消费场景正以前所未有的速度演进：管理层需要通过 BI 报表实时监控业务健康度，分析师需要灵活下钻探查数据波动原因，业务系统需要调用精准的指标数据进行自动化决策，AI 大模型则需要安全、准确地“理解”企业数据以提供智能洞察。这些多样化的消费端，无一例外地依赖于底层数据服务接口——通常是 RESTful API 或 JDBC 连接。</p><p>然而，传统的“数仓+ETL+宽表”模式下的数据服务接口，正面临严峻考验：</p><ul><li>扩展瓶颈：查询负载直接冲击承载宽表的数据库，高并发下 CPU、内存迅速耗尽，导致查询超时甚至服务雪崩。</li><li>安全风险：直接暴露数据库或宽表接口，权限管控粗放，存在数据泄露风险，尤其在对接 AI 应用时更为突出。</li><li>适配困难：新消费端（如新 BI 工具、AI 应用）接入需要复杂的定制开发，形成“架构孤岛”。</li></ul><p>因此，为指标中台选择一套具备强大适配性与扩展性的 API/JDBC 底层架构，已成为企业数据工程团队的核心决策之一。以下四步评估框架，旨在提供一套清晰、可执行的选型方法论。</p><h2>第一步：技术适配性——能否无缝融入现有技术栈？</h2><p>评估一套 API/JDBC 架构的首要标准是其“开箱即用”的适配能力。它必须能够无缝融入企业现有的技术生态，避免产生高昂的改造成本和新的“架构孤岛”。正如行业专家在选型实践中指出的，开放性与灵活性是关键考量点。</p><h3>向下适配：对接现有数据湖仓，无需大改造</h3><p>优秀的架构不应要求企业推翻重来。其核心在于能够直接对接企业现有的 DWD（明细数据层），通过声明式策略在逻辑层面构建虚拟业务事实网络，而非强制要求建设或改造大量的物理宽表（DWS/ADS 层）。</p><ul><li>理想特征：平台作为语义层与指标计算引擎，构建在现有数据湖仓之上。通过配置化的方式声明业务实体间的逻辑关联（Join），形成“虚拟明细大宽表”，从而直接基于明细数据定义和计算指标。</li><li>价值体现：这实现了 “做轻数仓” ，保护了历史投资，避免了因引入新平台而引发的数仓大改造。企业可以采用 “存量挂载、增量原生、存量替旧” 的三步走策略，实现架构的平滑演进。</li></ul><h3>向上适配：标准接口，支持多前端消费</h3><p>架构必须提供广泛兼容的标准接口，确保各类消费端能够“即插即用”。</p><p>理想特征：</p><ul><li>标准 API/JDBC：提供完善的 RESTful API 和 JDBC 驱动。</li><li>生态深度融合：与 FineBI、Quick BI 等主流 BI 工具实现无缝集成；通过 JDBC 支持 Tableau、Power BI 等其他工具。</li><li>多元消费支持：除 BI 工具外，API 应能直接支持自研业务系统、AI 大模型调用，以及通过 WPS 插件在办公表格中直接分析。</li></ul><p>权威背书：在某全球连锁餐饮巨头的实践中，该架构日均支撑百万级 API 调用，验证了其标准接口在高并发、多场景下的稳定服务能力。</p><h2>第二步：性能扩展性——如何支撑高并发与大数据量？</h2><p>当“双十一”大促或月度财报日来临，指标查询量可能瞬间激增数十倍。架构的横向扩展能力是应对此类业务峰值的生命线。其核心在于计算与存储的解耦以及智能的预计算加速机制。</p><h3>计算存储解耦与无状态横向扩展</h3><p>紧耦合的架构（查询直接在存储数据的数据库上执行）是性能扩展的天花板。现代指标平台应采用计算层与底层数据存储分离的架构。</p><ul><li>理想特征：计算节点设计为无状态，可以像云原生应用一样，根据查询并发压力快速进行弹性伸缩（Scale-Out）。流量高峰时快速扩容实例，低谷时自动缩容，从而高效利用资源并确保服务 SLA。</li><li>价值体现：从根本上避免了因单个数据库实例性能瓶颈导致的整个数据服务集群雪崩的风险。</li></ul><h3>智能物化加速：以空间换时间，保障查询稳定性</h3><p>应对高并发和大数据量查询，不能仅依赖底层数据库的“硬扛”，更需要智能的“空间换时间”策略。</p><ul><li>理想特征：基于声明式物化策略。用户可配置需要对哪些指标和维度组合进行预计算及更新时效，系统据此自动编排和维护多层级的物化加速表（明细加速、汇总加速、结果加速）。查询时，语义引擎 自动进行 SQL 改写和智能路由，透明地命中最优的物化结果。</li><li>权威背书：同样在上述餐饮巨头案例中，面对 百亿级数据规模，该平台实现了 P90 查询响应时间小于 1 秒 的极致性能。这证明了智能物化加速引擎在将不可预测的复杂查询，转化为可预测的快速数据检索方面的核心价值。</li></ul><h2>第三步：生态与治理扩展性——能否支持未来演进？</h2><p>选型不仅要满足当下，更要具备面向未来的扩展性。这包括对 AI 等新技术的原生适配能力，以及在指标体系膨胀过程中内嵌的治理能力。</p><h3>AI-Ready：提供根治幻觉的语义层，而非裸数据接口</h3><p>直接向 AI 大模型开放数据库查询权限，是极其危险的做法。理想的架构应充当安全、语义化的代理。</p><ul><li>理想特征：采用 NL2MQL2SQL 架构。AI 负责将自然语言问题转换为结构化的指标查询语言（MQL），然后由平台的语义引擎将其翻译为优化、安全的 SQL。这相当于将“写代码”的开放题，变成了“选指标”的选择题，极大收敛搜索空间，从根源上杜绝“幻觉”。</li><li>安全增强：所有 AI 发起的查询，必须经过语义层的统一鉴权（行列级权限），实现 “先安检，后执行” ，确保每一次 AI 交互都是合规、可控的。</li></ul><h3>治理内嵌：定义即治理，保障指标资产持续健康</h3><p>随着业务发展，企业指标数量可能呈指数级增长。缺乏治理的指标体系将迅速失控，重回“口径混乱”的老路。</p><ul><li>理想特征：实现 “定义即治理” 。在指标定义环节，系统自动进行全局判重、逻辑校验和影响分析。结合完整的指标生命周期管理（创建、发布、变更、下线）、版本控制和权责体系，确保每一个进入指标库的资产都是规范、可信的。</li><li>价值体现：将治理流程内嵌于生产流程，变被动的事后治理为主动的事前预防，保障了指标体系在持续扩展过程中的健康度与一致性。</li></ul><h2>第四步：安全与运维扩展性——如何降低长期 TCO？</h2><p>总拥有成本（TCO）是选型的关键经济指标。优秀的架构应通过内置的安全稳定机制和低摩擦的运维模式，有效降低长期投入。</p><h3>内置安全与稳定性机制</h3><p>安全与稳定不应是事后补救的功能，而应是架构的固有属性。</p><p>理想特征：</p><ul><li>精细化权限：支持基于用户、角色、组织的行列级数据权限管控。</li><li>熔断与隔离：具备查询级熔断机制，异常慢查询或恶意请求会被自动隔离，防止其耗尽资源、拖垮整个集群，有效防范“链式雪崩”。</li></ul><p>价值体现：从架构层面为企业数据资产建立了主动防御体系，降低了数据泄露和系统宕机的风险与成本。</p><h3>运维复杂度与平滑演进路径</h3><p>平台落地和升级的复杂度直接关系到项目成败与 ROI。</p><p>理想特征：支持 渐进式落地策略。企业无需一次性迁移所有历史资产，而是可以：</p><ol><li>存量挂载：将现有稳定宽表逻辑接入，统一服务出口。</li><li>增量原生：所有新需求直接基于平台 NoETL 方式开发。</li><li>存量替旧：逐步优化或下线维护成本高的旧宽表。</li></ol><p>价值体现：极大降低了实施阻力，保护了既有 IT 投资，使企业能够在业务连续的前提下，平稳、可控地向现代化数据架构演进。</p><h2>综合评估清单与行动建议</h2><p>将上述四个维度转化为可执行的评估清单，有助于在选型过程中进行客观对比。</p><table><thead><tr><th>评估维度</th><th>关键问题</th><th>理想特征 (以 Aloudata CAN 为例)</th></tr></thead><tbody><tr><td>技术适配性</td><td>是否需要改造现有数仓？能否连接现有 BI 工具？</td><td>直接对接 DWD，标准 API/JDBC，与 FineBI/Quick BI 等深度集成，开箱即用。</td></tr><tr><td>性能扩展性</td><td>如何应对突发高并发？大数据量查询能否稳定在秒级？</td><td>计算存储解耦，无状态横向扩展；声明式智能物化加速，百亿数据 P90&lt;1s。</td></tr><tr><td>生态扩展性</td><td>是否支持 AI 智能问数？能否表达复杂业务指标？</td><td>NL2MQL2SQL 架构根治幻觉；支持指标转标签、跨表聚合等复杂业务逻辑。</td></tr><tr><td>安全运维扩展性</td><td>权限管控是否精细？架构升级是否必须推翻重来？</td><td>行列级权限，内置查询熔断机制；支持“存量挂载、增量原生、存量替旧”平滑演进。</td></tr></tbody></table><p>行动建议：</p><ul><li>初创/快速发展期企业：应优先考虑技术适配性和平滑演进路径，选择能够快速上线、不绑架技术栈的平台，为未来留足扩展空间。</li><li>成熟/高并发企业：需重点评估性能扩展性和内置安全机制，通过 POC 严格测试其在高并发压力下的稳定性（P99 延迟）和资源消耗。</li><li>普遍原则：要求供应商提供与自身业务场景相近的客户案例验证数据（如日均 API 调用量、查询性能指标），并评估其产品在行业标准制定中的参与度（如信通院标准起草单位），作为技术先进性与可靠性的重要佐证。</li></ul><h2>常见问题（FAQ）</h2><h4>Q1: Aloudata CAN 的 API 和 JDBC 接口，与直接查询数据库有什么区别？</h4><p>本质区别在于“语义层”。直接查库暴露的是原始表字段和复杂 SQL 逻辑，而 CAN 的接口提供的是经过治理的、业务友好的“指标”和“维度”，屏蔽底层复杂性，保障口径一致性与查询安全，并通过智能物化加速获得更优性能。</p><h4>Q2: 如果我们已经有很多基于宽表开发的 FineBI 报表，迁移到 Aloudata CAN 的 API/JDBC 接口工作量有多大？</h4><p>工作量可控。首先，无需重做报表，只需将 BI 工具的数据源切换至 Aloudata CAN 的 JDBC。其次，通过“存量挂载”功能将现有宽表逻辑接入，统一口径。后续新需求采用“增量原生”方式开发，逐步优化底层架构，实现平滑演进。</p><h4>Q3: 在高并发场景下，Aloudata CAN 的 API 服务如何保证稳定性和不超时？</h4><p>主要通过三层机制：1) 无状态计算层支持横向扩展应对流量峰值；2) 智能物化路由使查询自动命中预计算结果，避免冲击源库；3) 内置熔断与隔离防止异常查询拖垮集群。这在某餐饮巨头日均百万级 API 调用的场景中已得到验证。</p><h4>Q4: 想要让大模型使用我们的指标数据，通过 Aloudata CAN 的 API 接入是否安全？</h4><p>相比直接开放数据库更安全。Aloudata CAN 充当“安全代理”和“语义翻译”。所有查询需经语义层行列级权限校验。AI 通过 NL2MQL2SQL 调用标准指标接口，而非编写任意 SQL，极大限制操作范围，杜绝越权访问和数据泄露。</p><h2>核心要点</h2><ol><li>适配性是基础：优秀的指标中台 API 架构必须能 开箱即用，无缝对接企业现有数据湖仓和 BI 工具，避免产生“架构孤岛”。</li><li>解耦是扩展的关键：计算与存储解耦 是实现无状态横向扩展、应对高并发流量的前提，而 智能物化加速 是保障大数据量下稳定秒级响应的核心引擎。</li><li>治理必须内嵌：通过 “定义即治理” 和 NL2MQL2SQL 架构，在指标生产源头和 AI 消费入口嵌入管控，确保指标体系在扩展中的健康度与安全性。</li><li>平滑演进降低 TCO：支持 “存量挂载、增量原生、存量替旧” 的渐进式策略，能最大程度保护历史投资，降低项目风险与长期运维成本，实现架构的可持续升级。</li></ol><ul><li><ul><li>*</li></ul></li></ul><p>本文详细内容及高清架构图，请访问 Aloudata 官方技术博客原文：<a href="https://link.segmentfault.com/?enc=NKfgmQ1sNSofNfAvIQM88A%3D%3D.1v5wmxVA1Cdbvv2wAdfBzRNljCPmu%2FxocoNNgkJm2RUf%2F38n6hSjjPlrot%2FJ2aZ3PGip%2BUIn0wjdDjOIGGaerooy0ofFq%2B1oUulhCaAVbNc1jPSYYMSydmGTWPpjRgug" rel="nofollow" target="_blank">https://ai.noetl.cn/knowledge-base/metric-middleware-aloudata...</a></p>]]></description></item><item>    <title><![CDATA[“马”住这份攻略：1个智能体，N种专属用法！ Smartbi ]]></title>    <link>https://segmentfault.com/a/1190000047607664</link>    <guid>https://segmentfault.com/a/1190000047607664</guid>    <pubDate>2026-02-12 16:03:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>年关将至，此时此刻，不同部门的战壕里，大家都在为了“圆满收尾”而全力冲刺：</p><p>财务部需要严谨核算：查凭证、平账目、出年报，容不得半点差错；</p><p>市场部正在灵活复盘：看转化、析渠道、定策略，急需洞察明年的先机。</p><p>如何让一个智能体，既能满足财务的严谨，又能适应市场的灵活？白泽SmartBI近期上线的“智能体快捷方式”功能，为您提供了一种极简解法。快来“马”住这份攻略，让您新的一年“马”力全开！</p><h2>01 业务痛点：不同部门个性化需求如何满足？</h2><p>在实际业务中，不同角色的关注点截然不同，通用配置的智能体往往难以解决个性化的提效需求：</p><h4>财务部门：</h4><p>日常聚焦于查凭证、看余额、审报表，高频使用“财务会计模型”和“资金管理模型”，需要一个专注财务领域的纯净入口，告别手动切换的繁琐。</p><h4>市场部门：</h4><p>核心关注客户价值与渠道转化，依赖“客户生命周期价值模型”和“渠道引流模型”来支撑决策，渴望一个能够直达业务核心、快速获得答案的智能体。</p><p>业务人员都希望能对智能体做轻量化调整，以满足自己个性化需求（如预设常用模型、简化交互流程），又需要一直保持智能体最新版本，享受升级带来的新功能。</p><p>如何能够同时满足统一维护与个性体验？白泽SmartBI给出了解决方案。</p><h2>02 解决方案：快捷方式 + 参数配置</h2><p>现在，通过“快捷方式 + 参数配置”的组合，您可以将一个核心智能体，快速为不同部门“分发”专属入口。它既保留了“开箱即用”的便捷，又能实时同步底层的每一次能力升级。真正实现了 “千人千面，常用常新”。</p><h4>快捷方式</h4><p>低成本实现一个智能体、N个“分身”</p><ul><li>它是指向原智能体的链接，不是复制粘贴，低成本实现千人千面；</li><li>原智能体有任何升级优化，所有快捷方式自动同步，功能永远保持最新；</li><li>支持基于同一智能体创建多个不同快捷方式，实现“一对多”个性化配置。</li></ul><h4>参数配置</h4><p>给每个“分身”设定专属性格</p><ul><li>每个快捷方式都可以独立配置运行参数（如限定数据模型范围、调整反问规则、联网等设置）。</li><li>通过提前预设，就能让同一个智能体在不同部门面前，展现出完全不同的“专属性格”，同时满足企业权限管控的需求。</li></ul><p>接下来我们从实际业务场景出发，三步带您解锁智能体专属用法。</p><h2>03 实操流程：3步打造专属智能体入口</h2><h4>Step1：创建快捷方式</h4><p>在“智能助理”下，创建两个快捷方式，命名为：智能助理_财务、智能助理_市场。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607666" alt="图片" title="图片"/></p><h4>Step2：配置专属参数</h4><p>智能助理_财务：数据模型列表设为财务常用的【财务会计模型】和【资金管理模型】，并开启自动选择数据模型。</p><p>智能助理_市场：数据模型列表设为市场常用的【客户生命周期价值模型】和【渠道引流模型】，并开启自动选择数据模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607667" alt="图片" title="图片" loading="lazy"/></p><p>同时还可以对联网、文件上传、反问等多项参数进行自定义设置。</p><h4>Step3：设置权限</h4><p>完成配置后即刻启用，将“智能助理_财务”权限开放给财务部，“智能助理_市场”权限开放给市场部。</p><h4>效果：开箱即用，马上见效！</h4><p>财务小王和市场小林登录后，只需选择自己的专属入口，直接输入问题即可。所有模型切换自动完成，他们得到的是最精准、最直接的答案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607668" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607669" alt="图片" title="图片" loading="lazy"/></p><h2>04 体验升维：提效立竿见影</h2><p>通过“快捷方式+参数配置”，我们解决的不仅是模型切换的操作痛点，更构建了一种可持续的个性化能力：各业务部门能获得量身定制的智能体验，而所有定制入口都能随原智能体同步进化，保障了企业整体迭代的效率与一致性，效果立竿见影。</p><p>SmartBI在不断拓展智能体能力边界的同时，也从未停止对“交互体验”的微调与打磨。这不仅是产品从可用迈向易用的关键，更是SmartBI能够承载大型企业复杂需求、交付可靠服务的最佳证明。</p><p>欢迎免费试用白泽SmartBI！开启您的专属智能之旅！</p><p>​</p>]]></description></item><item>    <title><![CDATA[鸿蒙架构师修炼之道-如何成为团队的架构师 waylau ]]></title>    <link>https://segmentfault.com/a/1190000047607851</link>    <guid>https://segmentfault.com/a/1190000047607851</guid>    <pubDate>2026-02-12 16:03:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>要成为鸿蒙开发团队的架构师，需要从知识储备、技能提升、经验积累、职业素养培养等多个方面进行努力，以下是具体的建议。</p><h3>扎实的知识储备</h3><p>鸿蒙开发团队的架构师需要具备扎实的知识储备。</p><ul><li><strong>操作系统知识</strong>：深入掌握操作系统的基本原理，包括进程管理、内存管理、文件系统、网络协议栈等。了解Linux内核的相关知识也很有帮助，因为鸿蒙系统与Linux有一定的渊源。</li><li><strong>鸿蒙系统知识</strong>：全面学习鸿蒙系统的架构、特性、开发框架和工具。熟悉鸿蒙的分布式技术、HarmonyOS应用开发语言（如ArkTS、仓颉编程语言等）、应用开发框架以及系统服务等内容。</li><li><strong>编程语言</strong>：熟练掌握至少一种鸿蒙应用开发语言，如ArkTS、C++、仓颉等，同时要对JavaScript、TypeScript等前端语言有一定的了解，以便进行跨平台开发和与Web技术的交互。</li><li><strong>硬件知识</strong>：了解硬件体系结构、芯片原理、传感器原理等硬件基础知识，有助于更好地理解鸿蒙系统与硬件的交互，以及在不同硬件平台上进行系统优化。</li></ul><h3>丰富的提升技能</h3><p>鸿蒙开发团队的架构师需要具备丰富的提升技能。</p><ul><li><strong>架构设计能力</strong>：通过学习架构设计模式和原则，如微服务架构、分层架构等，提升系统架构设计能力。能够根据业务需求，设计出合理、高效、可扩展的鸿蒙系统架构方案。</li><li><strong>开发与调试能力</strong>：具备熟练的鸿蒙应用开发能力，能够独立完成应用的编码、调试和测试工作。掌握调试工具和技巧，能够快速定位和解决开发过程中出现的问题。</li><li><strong>性能优化能力</strong>：学习性能优化的方法和技术，如代码优化、算法优化、资源管理优化等。能够对鸿蒙系统和应用进行性能分析和调优，提高系统的运行效率和响应速度。</li><li><strong>安全与隐私保护能力</strong>：了解安全与隐私保护的相关知识和技术，如数据加密、身份认证、访问控制等。能够在鸿蒙系统和应用的设计和开发中，充分考虑安全与隐私问题，确保用户数据的安全。</li></ul><h3>经验的积累</h3><p>鸿蒙开发团队的架构师需要具备丰富的项目经验。</p><ul><li><strong>项目实践</strong>：积极参与鸿蒙相关的项目开发，从简单的应用项目开始，逐步积累经验。在项目中，承担不同的角色和任务，如模块开发、架构设计、项目管理等，全面提升自己的能力。</li><li><strong>社区贡献</strong>：参与开源鸿蒙社区的开发和维护工作，贡献自己的代码和技术方案。通过与社区中的其他开发者交流和合作，学习先进的技术和经验，提高自己的知名度和影响力。</li><li><strong>技术分享与交流</strong>：积极参加鸿蒙技术相关的研讨会、讲座、线上论坛等活动，与同行进行技术分享和交流。了解行业的最新动态和技术趋势，拓宽自己的技术视野。</li></ul><h3>职业素养的培养</h3><p>鸿蒙开发团队的架构师需要注重职业素养的培养。</p><ul><li><strong>学习能力</strong>：鸿蒙技术在不断发展和更新，需要具备良好的学习能力，能够快速掌握新的技术和知识。保持学习的热情和好奇心，不断提升自己的技术水平。</li><li><strong>沟通能力</strong>：作为架构师，需要与团队成员、产品经理、其他部门等进行频繁的沟通和协作。具备良好的沟通能力，能够清晰地表达自己的想法和观点，倾听他人的意见和建议，推动项目的顺利进行。</li><li><strong>团队合作精神</strong>：在团队中，要能够与不同背景和专业的人员合作，发挥自己的技术优势，共同完成项目目标。具备团队合作精神，能够关心和帮助团队成员，营造良好的团队氛围。</li><li><strong>问题解决能力</strong>：在项目开发过程中，会遇到各种各样的问题和挑战。具备较强的问题解决能力，能够迅速分析问题的本质，提出有效的解决方案，确保项目的顺利进行。</li></ul><p>综上，要成为团队的架构师，“打铁还需自身硬”，除了下苦功夫，还需要针对性的对自身能力进行不断打磨。</p><p>这里推荐 <a href="https://link.segmentfault.com/?enc=9cdJbWmq565u9HULOCCriQ%3D%3D.nPFPM2P84cUwgaDvjK6dCgfO9Oxt8wJAvy8T4iSsmV9k6ldK5HTtBBYGiiZvXz1KiXoK1EZ7r%2Faz9cmR%2FaThITizkSjzHUlANTfxJ6%2FZIqk%3D" rel="nofollow" target="_blank">《鸿蒙架构师修炼之道》</a>（北京大学出版社）这本书。本书不但通过真实案例讲解架构设计流程和经验，还总结了丰富的鸿蒙架构师工作原则和技巧，读者可以对照本书内容进行查漏补缺，提升自身能力，早日踏上鸿蒙架构师修炼之道。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607853" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[LLaDA2.1 正式开源，可纠错编辑机制让 100B 扩散模型突破 892 TPS 速度极限 蚂蚁]]></title>    <link>https://segmentfault.com/a/1190000047607860</link>    <guid>https://segmentfault.com/a/1190000047607860</guid>    <pubDate>2026-02-12 16:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在大语言模型的技术版图中，自回归（AR）架构长期占据主导地位，而扩散模型则被视作一条充满挑战的“非共识”路线。LLaDA2.0 已经成功证明了扩散语言模型（dLLM）规模化至 100B 参数的可行性，但生成速度与生成质量的平衡始终是横亘在扩散模型面前的核心难题。</p><p>2月11日，我们正式发布 LLaDA2.1，通过可纠错编辑机制，首次让扩散语言模型在保持高质量的同时，将推理速度推至 892 TPS的新高度，让扩散语言模型从“研究探索”向“真正可用”迈进了一大步。</p><ul><li>模型：<br/><a href="https://link.segmentfault.com/?enc=F6METJ5skJDDSvadfWvScw%3D%3D.o7EhjYTjm6X%2FpQ96oOfIdGe9AmBFuw1TrzINvStSnhQ%2FLK7n3JT%2BgL6YWI2XHbJMPSir0Lvm9WBqc6DSYtBnyQ%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/collections/inclusionAI/llada21</a>；<a href="https://link.segmentfault.com/?enc=RC7PyZOO4msgxH9YrXsBfA%3D%3D.EUpNFZHi3rWWzqRoyDg2AOPvBzV%2FkBbaOsP03Ti7hVe9EXwwtOIiUjo0IFbACkHl9VsvAE6zL6r%2FrElstG4UDQ%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/collections/inclusionAI/LLaDA21</a></li><li>GitHub：<br/><a href="https://link.segmentfault.com/?enc=5IWZLcT5e5%2FxKadYkGMwbQ%3D%3D.0Lmd0HzFwlmUx5ov8sSLIV86gRAVwsTQvixxmpuX0JCkQX9SzKF2DUS6q7uprfnt" rel="nofollow" target="_blank">https://github.com/inclusionAI/LLaDA2.X</a></li><li>技术报告：<br/><a href="https://link.segmentfault.com/?enc=pbJ73v7DXo5Vojwm9xnnnQ%3D%3D.nWfzSvl8Y9r2zxUsemlCwLS2l9wc%2BwwFdwo1SvZ5ahLyNAar84RtZYtmqyeqCri9" rel="nofollow" target="_blank">https://huggingface.co/papers/2602.08676</a><br/>从“学术研究”到真正可用、甚至效率更优的强大工具，这一飞跃，源于以下三大技术亮点 ——</li></ul><h2>创新“可纠错编辑”机制  Error-Correcting Editable, ECE</h2><p>作为实现飞跃的最核心创新，它赋予了扩散模型一种前所未有的“智慧”——像人类专家一样“起草-编辑”。</p><p>传统自回归模型像是一个不允许带草稿纸、不允许带提纲的考生，它下笔无悔，不允许修改自己写好的答案。LLaDA2.1 从根本上重构了这一范式。我们提出了 Token-to-Token（T2T）编辑机制，让模型具备「起草-编辑」的双重能力，在毫秒级的闪电采样中完成“草稿”到“正卷”的转身：</p><ul><li>起草阶段：模型以较低的置信度阈值快速并行生成初始草稿；</li><li>编辑阶段：模型启动自我纠错，对已生成的 Token 进行回溯检查和迭代修正。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607862" alt="图片" title="图片"/><br/>图 1：传统吸收态范式 vs LLaDA2.1 可纠错编辑机制</li></ul><p>这种设计让扩散模型首次拥有了类似人类的「修改草稿」能力，解决了并行生成的误差累积问题，为高速解码奠定了理论基础。</p><h2>灵活“双模式”设计 Speedy Mode vs Quality Mode</h2><p>基于可纠错编辑机制，LLaDA2.1 提供了两种截然不同的运行模式，将速度与生成的选择权交还给用户：<br/>| Speedy Mode（极速模式）<br/>采用激进的低阈值策略进行 M2T 解码，以最大化并行度生成初始草稿，随后依赖 T2T 编辑机制进行精炼修正。这一模式实现了快速的推理速度，在代码生成等结构化任务中，仅带来可接受的性能折损。</p><p>| Quality Mode（质量模式）<br/>采用保守的高阈值策略，优先保证解码的精确性。在这一模式下，LLaDA2.1 在 33 项基准测试上全面超越 LLaDA2.0，并超越同类型扩散语言模型：<br/>代码能力：HumanEval+ 89.63%，CRUXEval-O 87.50%<br/>数学推理：AIME 2025 63.33%，GSM-Plus 89.69%<br/>智能体任务：BFCL v3 75.61%，IFEval 83.55%</p><p>双模式设计让用户真正成为速度与质量的决策者 —— 需要实时响应时选择 Speedy Mode，需要精确输出时则可以切换 Quality Mode，满足不同场景下的真实需求。</p><h2>业界首个 dLLM 大规模 RL 框架</h2><p>如果说“可纠错编辑”让模型变得“可用”，那么强化学习则让模型变得更“聪明”、更“可靠”，体感更强。LLaDA2.1 实现了首个专为 dLLM 设计的大规模强化学习框架。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607863" alt="图片" title="图片" loading="lazy"/><br/>图 2：LLaDA2.1 训练与推理框架概览</p><p>扩散模型的策略优化面临一个根本性障碍：序列级对数似然无法直接计算。在 100B 规模的扩散模型上跑通 RL 绝非易事。它不仅需要极强的工程底层支撑，要求我们从块状扩散（Block-diffusion）的条件概率转移视角，提出稳定的梯度估计算法，即 EBPO（ELBO-based Block-level Policy Optimization）：</p><ul><li>使用 Evidence Lower Bound (ELBO) 作为似然的合理代理；</li><li>结合 Vectorized Likelihood Estimation，实现边界估计的并行计算；</li></ul><p>EBPO 不仅提升了训练效率，更为 dLLM 的后训练优化提供了稳定、可扩展的解决方案。这一突破让强化学习首次能够稳定地扩展到扩散语言模型的后训练阶段，显著提升了模型的指令遵循能力和人类意图对齐度。</p><p>性能表现<br/>LLaDA2.1 开源两个版本：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607864" alt="图片" title="图片" loading="lazy"/></p><p>LLaDA2.1 在保持高质量生成的同时，实现了突破性的推理速度 —— 892 TPS，是传统自回归模型的数倍。在多个生成场景中，它都能以闪电般的速度完成；尤其是在代码领域，平均达到了 600-700 的 TPS，让用户体验如丝般流畅。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607865" alt="图片" title="图片" loading="lazy"/><br/>图 3：LLaDA2.1 在 Mini 和 Flash 系列上的吞吐量对比</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607866" alt="图片" title="图片" loading="lazy"/><br/>表 1：在不同场景上，LLaDA2.1 在 Mini 和 Flash 系列上的吞吐量</p><p>892 TPS 意味着什么？相当于每秒生成近 900 个 token，足以支撑实时交互、大规模部署等工业级应用场景。<br/>这一速度飞跃的背后，正是可纠错编辑机制的支撑 —— 正因为模型具备自我修正的能力，才敢在初始阶段采用激进的低阈值策略快速生成，而不必担心错误累积导致质量崩塌。</p><h2>结语</h2><p>LLaDA2.1 的意义不仅在于 892 TPS 的速度数字，更在于它证明了：通过技术创新，扩散语言模型完全可以在保持并行生成优势的同时，克服质量与速度的传统权衡。</p><p>可纠错编辑机制的引入，让 dLLM 第一次拥有了“自我修正”的智慧；双模式设计让用户真正成为速度与质量的决策者；强化学习框架则为扩散模型的后训练开辟了新的可能性。</p><p>我们诚挚邀请社区开发者体验 LLaDA2.1，也欢迎有志于探索 LLaDA 模型的同学加入我们，共同探索扩散语言模型的边界。蚂蚁技术研究院招聘火热进行中！多个研究课题，等你挑战！</p>]]></description></item><item>    <title><![CDATA[『NAS』在绿联部署一个白板工具-tldraw 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047607873</link>    <guid>https://segmentfault.com/a/1190000047607873</guid>    <pubDate>2026-02-12 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=g2lDP1u1YI3iRRH3tJ4eKQ%3D%3D.CaHRbvQ%2BnPzzXgsuIxgKyoR4SY83nLF6D6zgR82wAcRG%2FujGBKThMg7y6K6oOq6OYkrGBpBA1to117x%2F%2F05yQ6%2FbjQLpw3LLSQ4kgmw69AQ%2FM1QZi8YAMZ3mjopyIwDtnd43K3ebK7lZ7OBzmqs6iMUXhdXC0KsKd4wGEKIriVs%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>tldraw 是一款开源免费的轻量级协作白板工具，NAS 可以通过 Docker 就能一键部署，群晖、绿联、极空间等主流 NAS 全适配，无需复杂配置。它拥有无限手绘画布，支持画笔、形状、连线、文本便签等实用功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607875" alt="" title=""/></p><p>这次我用绿联 NAS 部署，其他品牌的 NAS 操作步骤也是差不多的。</p><p>打开“Docker”应用，在“镜像”页面搜索“tldraw”，下载红框选中的那个 <code>ratneo/tldraw</code>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607876" alt="" title="" loading="lazy"/></p><p>下载完成后，切换到“本地镜像”页面，点击 <code>ratneo/tldraw</code> 旁边的加号，创建容器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607877" alt="" title="" loading="lazy"/></p><p>在创建容器这页，容器名称可以自定义，我就不改了。</p><p>自动重启可以勾选上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607878" alt="" title="" loading="lazy"/></p><p>往下滑，”NAS端口“这项可以自定义，我这里选的是 <code>39445</code> 这个端口。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607879" alt="" title="" loading="lazy"/></p><p>部署完成后，浏览器输入 <code>NAS的IP:39445</code> 就能使用 tldraw 了。</p><p>它支持将画布内容导出为 <code>SVG</code>、<code>PNG</code> 和 <code>JSON</code> 这几种格式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607880" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=hAkveRjgWmbmqXYQbQz3aQ%3D%3D.WsWKrO0gBnmzmK5KbLu3INODiat1XzjPXdJBuYAO4C9xX86acregm9lTL5B4Gk2Ne5TwbznYrA2sYc8MgzQcO3%2BK8bAe130WYy9LDwRxvtCrCcs9FMmne%2BBEFM3CBSMOGrOUuoDfOITXLAj6ltPDII5E44Len0Rr0Dt1soB7iDY%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[千问app崩了！背后的技术困局值得所有AI产品警惕 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047607431</link>    <guid>https://segmentfault.com/a/1190000047607431</guid>    <pubDate>2026-02-12 15:12:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026年2月6日，不少网友的手机屏幕上都出现了熟悉又令人无奈的提示——千问APP加载失败、页面卡住、订单无法提交，甚至直接闪退。短短几小时内，“阿里巴巴千问崩了”冲上热搜，有人吐槽“蹲了半天奶茶福利，结果被崩到怀疑人生”，也有人调侃“运维小哥怕是要连夜搬救兵”。这场看似偶然的“崩溃事件”，表面是30亿奶茶福利引发的流量狂欢，实则暴露了AI产品在规模化落地中，从流量承载到工程化部署的一系列技术短板，值得整个行业深思。</p><p>不同于普通APP的崩溃，千问作为阿里旗下核心AI助手，其崩溃并非单一环节故障，而是“流量洪峰+AI负载”双重压力下，多技术环节协同失灵的集中爆发。结合官方回应及AI产品部署的共性问题，我们可以从四个核心维度，拆解此次崩溃背后的技术真相。</p><h2>一、流量突袭：超出预估的并发，压垮前端与网关</h2><p>此次千问崩溃的直接导火索，是其推出的“春节30亿大免单”活动——更新APP即送25元无门槛奶茶免单卡，邀请新用户可叠加福利，实实在在的优惠让用户蜂拥而至。数据显示，活动上线不到4小时，下单量就突破200万单，短时间内大量用户集中点击、分享、下单，形成了远超系统预估的瞬时流量洪峰，这成为压垮系统的第一道防线。</p><p>从技术层面看，这种突发高并发对AI产品的考验，远高于普通电商或社交APP。普通APP的请求多为简单的读写操作，而千问的用户在参与活动的同时，仍会使用其核心AI功能——问答、图像生成、实时翻译等，这意味着服务器要同时承载“活动下单”的高频轻请求和“AI推理”的高负载重请求，双重压力下，前端与网关率先失守。</p><p>前端层面，大量并发请求导致页面资源加载超时、接口调用失败，部分用户出现“点击无响应”的ANR（应用无响应）现象，甚至触发闪退——这并非单纯的前端优化不足，更在于未针对突发流量设计弹性适配机制，比如未设置请求排队、限流熔断，也未对活动页面进行静态资源缓存，导致每一次用户刷新都要向服务器发送新的请求，进一步加剧拥堵。</p><p>网关层面，作为所有请求的“入口闸门”，千问的网关可能未做好动态扩容准备。当瞬时请求量超出网关的承载阈值，就会出现请求堆积、路由失败的情况，部分请求无法正常转发至后端服务，进而表现为APP加载失败、页面空白。更值得注意的是，此次活动的社交裂变模式，原本是为了扩大传播，但微信对分享链接的屏蔽，让用户只能通过复制口令、跳转浏览器访问，反而让流量更加集中，相当于“几十万人同时挤一座窄桥”，网关的压力呈指数级上升。</p><h2>二、核心瓶颈：AI推理的“算力陷阱”，显存与并发的双重制约</h2><p>如果说突发流量是“导火索”，那么AI推理本身的技术特性，就是此次崩溃的“核心诱因”。不同于普通APP，千问的核心服务是大模型推理，而大模型在高并发场景下的算力消耗、显存占用，往往是技术部署的“重灾区”，也是很多AI产品容易忽视的短板。</p><p>此次崩溃的关键技术问题之一，很可能是高并发下的显存溢出（OOM）——这是大模型部署中最常见的“致命问题”。AI大模型本身占用大量内存，尤其是在加载模型或进行推理时，若未做好内存管理，极易引发崩溃。千问的大模型参数规模可观，每一次用户请求都需要占用一定的显存进行推理计算，而活动期间的高并发请求，会让多个推理任务同时进行，显存占用瞬间飙升。</p><p>更隐蔽的“显存杀手”是KV Cache的膨胀。大模型推理时，会缓存键值对（KV Cache）以加速自注意力计算，提升响应速度，但在高并发场景下，多个请求的KV Cache会叠加，尤其是当用户进行长文本问答、多轮交互时，缓存会随着交互长度线性增长，快速耗尽显存资源。如果千问未采用动态批处理、PagedAttention等先进技术，仍使用固定批处理模式，当大量请求同时涌入时，系统会尝试将它们拼成一个超大批次，显存瞬间被撑爆，进而导致服务崩溃——这也是很多大模型从Demo走向生产环境时，最容易踩的“工程化大坑”。</p><p>除此之外，算力分配不均也加剧了崩溃。千问的服务器需要同时兼顾活动板块的业务请求和AI推理的算力需求，当活动流量暴涨时，若未做好算力动态调度，大量算力会被活动的高频请求占用，导致AI推理任务排队、阻塞，进而引发整个系统的响应延迟、服务超时，甚至出现“牵一发而动全身”的连锁反应——虽然官方回应称“核心AI功能基本正常”，但从用户反馈来看，部分用户在活动期间使用AI问答时，也出现了响应变慢、加载失败的情况，本质就是算力分配失衡导致的。</p><h2>三、协同失灵：线程管理与依赖兼容的隐性隐患</h2><p>此次千问崩溃，还暴露了AI产品在线程管理、依赖库兼容等细节层面的技术漏洞，这些看似微小的问题，在高并发压力下，会被无限放大，成为系统崩溃的“压垮骆驼的最后一根稻草”。</p><p>线程阻塞与并发管理不当，是其中一个重要原因。AI推理任务通常需要在主线程之外的后台线程执行，若未合理管理线程，就可能导致ANR或闪退。比如，若千问的活动页面在处理下单请求时，未使用异步线程，而是在主线程中执行耗时操作，就会导致主线程阻塞，用户点击无响应，甚至触发APP闪退——这也是很多AI应用在移动端部署时的常见问题，尤其在高并发场景下，线程管理的漏洞会被快速暴露。</p><p>依赖库缺失或版本冲突，则可能导致部分设备出现闪退。千问作为一款跨平台AI应用，需要依赖多个第三方库（如TensorFlow Lite、OpenCV等）实现AI推理、图像处理等功能，若依赖管理不当，就可能出现兼容性问题。比如，部分低版本手机设备可能不支持某一依赖库的版本，或缺少相关的.so文件，在高并发请求的触发下，就会出现“UnsatisfiedLinkError”等异常，导致APP闪退；而不同设备对AI模型的兼容性差异，也可能让部分用户在加载模型时失败，进一步扩大崩溃的影响范围。</p><p>此外，数据同步延迟也成为用户吐槽的焦点。不少用户反馈，“邀请新用户后，免单次数没显示”“下单后订单状态迟迟不更新”，这看似是业务层面的问题，本质是技术层面的数据同步机制不完善。活动期间，大量用户同时进行邀请、下单操作，产生的海量数据需要实时同步至服务器，但高并发导致数据写入、同步延迟，缓存更新不及时，进而出现数据不一致的情况——这不仅影响用户体验，也会增加服务器的负担，进一步加剧系统拥堵。</p><h2>四、可观测性不足：故障定位滞后，应急响应受限</h2><p>一场成熟的技术故障应对，不仅需要快速的应急扩容，更需要精准的故障定位——而千问此次崩溃，也暴露了AI产品在可观测性建设上的不足，这也是导致故障初期无法快速缓解的重要原因。</p><p>对于大模型服务而言，可观测性并非简单的CPU、内存监控，而是需要构建专属的监控体系，涵盖吞吐量、推理延迟、显存利用率等核心指标。比如，需要监控Tokens per second（TPS）吞吐量，判断系统的处理能力；监控Time To First Token（TTFT）、Time Per Output Token（TPOT），掌握AI推理的响应速度；监控GPU利用率、显存利用率，及时发现显存溢出、算力不足的隐患。</p><p>但从此次事件来看，千问可能未完全构建起完善的大模型可观测性体系。一方面，对长尾延迟的监控不足——平均响应时间看似正常，但部分慢请求会拖累整个批次的处理速度，在高并发场景下，这种长尾延迟会快速扩散，导致系统卡顿、崩溃；另一方面，链路追踪能力不足，当故障发生时，无法快速定位是前端请求、网关转发、AI推理还是数据同步环节出现问题，只能采取“紧急扩容”这种粗放式的应对方式，导致故障缓解速度不及用户预期。</p><p>官方回应中提到“技术团队已紧急调配资源，扩容服务器”，这也从侧面说明，此次故障的应急响应更多依赖于“扩容”，而非精准的故障定位与修复——这并非千问个例，而是很多AI产品的共性问题：过度关注大模型的算法性能，却忽视了工程化部署中的可观测性建设，最终导致“故障来了，不知道问题在哪；修复故障，只能靠盲目扩容”。</p><h2>结语：AI产品的成熟，从来不止于算法</h2><p>此次千问APP崩溃，看似是一场“幸福的烦恼”——因活动太火爆导致流量超出预估，但背后折射出的，是整个AI行业在工程化部署中的普遍困境：当大模型从实验室的Demo，走向亿级用户的生产环境，算法的先进性只是基础，流量承载、算力调度、线程管理、可观测性建设等工程化能力，才是决定产品稳定性的关键。</p><p>随着春节临近，各大AI产品纷纷推出营销活动，试图抢占用户心智，但千问的教训提醒我们：AI产品的竞争，从来不止于算法的比拼，更在于工程化能力的较量。对于所有AI产品而言，想要避免“崩溃式翻车”，不仅要做好活动前的流量预估、系统扩容，更要补齐技术短板——采用动态批处理、PagedAttention等技术优化显存利用，构建完善的线程管理与依赖兼容体系，搭建专属的大模型可观测性平台，让故障能够被提前预警、快速定位、及时修复。</p><p>毕竟，对于用户而言，再强大的AI功能，再丰厚的福利，都抵不过一次流畅的使用体验。千问此次的崩溃，既是一次技术预警，也是整个AI行业走向成熟的必经之路——唯有正视工程化部署中的技术隐患，补齐短板，才能让AI产品真正走进用户的日常生活，实现技术价值与用户体验的双赢。而对于千问而言，如何在此次故障后优化技术架构、完善应急机制，避免类似问题再次发生，才是接下来最需要解决的问题。</p>]]></description></item><item>    <title><![CDATA[当CRM系统不堪重负，重药集团如何通过OceanBase实现“实时精准营销”？ OceanBase技]]></title>    <link>https://segmentfault.com/a/1190000047607447</link>    <guid>https://segmentfault.com/a/1190000047607447</guid>    <pubDate>2026-02-12 15:11:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：张红霞，青岛雨诺网络信息股份有限公司新零售产品部总监</p><p><strong>摘要：</strong><br/><strong><em>随着CRM会员系统的使用时间拉长，重药集团底层的传统数据库逐渐难以满足复杂数据的高效处理需求。面对海量交易和多维度行为数据的汇聚，需采用具备高可用、强一致、可扩展特性的数据库。其选择OceanBase，最终实现系统稳定运行、复杂场景实时分析、查询效率提升25倍、存储空间节约60%。</em></strong></p><p>当前，医药零售企业已不再满足于“卖药”，而是致力于成为“健康管理伙伴”。通过构建以 CRM 会员系统为核心、线上与线下深度融合的全渠道服务架构，企业实现了服务时间与空间的无限延展、会员数据的集中管理与智能应用、营销活动的精准触达与高效转化。</p><p>作为医药零售的头部企业，重庆医药（集团）股份有限公司（简称“重药集团”）前身是成立于1950年的中国医药公司西南区公司，服务于医药全产业链，同时从事医药研发（MAH）、医疗器械生产，并投资参与医药工业。重药集团拥有全级次分、子公司200余家，正在从传统的配送商业企业向“互联网+医药”融合型现代医药企业转型。</p><p>随着CRM会员系统的使用时间拉长，其底层的传统数据库逐渐难以满足复杂数据的高效处理需求。面对海量交易和多维度行为数据的汇聚，重药集团CRM会员系统亟需采用具备高可用、强一致、可扩展特性的数据库。经过对比三款国产分布式数据库，重药集团选择OceanBase，最终实现系统稳定运行、复杂场景实时分析、查询效率提升25倍、存储空间节约60%。</p><p>此次重药集团CRM系统的数据库升级不仅提升了用户体验与品牌忠诚度，也为后续集团构建高性能、高可用的“集团级数字化运营中枢”提供了明确的业务需求与数据基础，构建可扩展、可复制、可监管的集团化运营体系。</p><h2>医药零售商业模式变革，CRM系统实现全渠道协同</h2><p>随着消费者行为的数字化转型和健康需求的持续升级，医药零售行业正经历深刻的商业模式变革。传统药店“有啥卖啥”的经营逻辑，逐步向“顾客需要什么”的逻辑转变，除了提供到店服务外，还支持线上服务，比如通过企业微信、公众号等渠道建立长期沟通机制。微商城代客下单、在线解答疑问等。</p><p>为构建以专业化服务为基础的顾客信任体系，医药企业建立了完整的会员服务体系——CRM 会员系统，以实现绑定多重会员信息、建立精准的会员标签画像，为会员提供更多的服务和营销。通过数据驱动决策的专业化服务能力提升来提高企业在行业内的竞争力，实现增收。</p><p>如图1 所示，CRM 会员系统可以实现线上、线下全渠道协同，支持会员档案统一、标签体系完善、自动触发机制、店员触达赋能、社群营销等关键功能。完成顾客到店/线上购药 → 完成交易 → 数据沉淀至 CRM → 触发服务与营销 → 二次消费 → 再次触达，实现“交易—服务—再交易”的正向循环。<br/><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnU13" alt="" title=""/><br/>图1 CRM会员系统实现线上、线下全渠道协同</p><h2>为实现一体化管理需求，构建CRM会员系统</h2><p>重药集团CRM会员系统的搭建背景，源自于其各子公司会员管理分散，系统缺乏统一规划，导致数据难沉淀、服务差异大、运营难复制，且缺乏实时监控，难以支撑决策。</p><p>为实现一体化管理，重药集团CRM会员系统分阶段建设。第一阶段完成会员营销平台的底座建设，打造集团化、标准化、数据化运营基础，核心目标如下。</p><p>搭建集团化会员运营平台。实现集团—子公司—门店的一体化管理，打通组织架构与业务链路，确保会员在不同层级和渠道中都能获得一致的服务体验。</p><p>统一的会员运营服务体系。构建覆盖会员管理、营销活动、服务交付的标准化流程，减少分散运作带来的效率损耗，提升整体运营协同能力。</p><p>可快速复制标准化服务能力。形成可落地的服务模板和运营机制，帮助新业务和子公司快速复制成熟经验，缩短建设周期，提升推广效率。</p><p>实现经营数据统一分析。沉淀完整的数据资产，打破信息孤岛，实现对会员、门店、区域的多维度统一分析，为企业战略决策与合规审计提供有力支撑。</p><p>在上述目标指导下，我们做了三个核心举措：</p><p>联合集团会员中心，推进一体化进程。覆盖集团全品牌及线上会员，实现线上和线下会员统一运营和全域价值管理（见图2）。</p><p>构建多层级组织架构视角报表。支持集团、品牌、门店的权限管理，权限灵活配置，便于集团总部进行跨品牌的数据报表分析。</p><p>集团统一下达任务。集团可向各品牌下发销售任务、患者教育活动任务及拉新任务，实现集团任务统一管理与执行监督。<br/><img width="723" height="634" referrerpolicy="no-referrer" src="/img/bVdnU14" alt="" title="" loading="lazy"/><br/>图2 集团会员同意运营架构</p><p>我们计划以集团内个别区域公司为试点，试行以上举措，若成功，则进行全面推广。推广成功后，重药集团会员运营平台将实现从“单一业务系统”向“集团级数字化运营中枢”演进。依托统一的技术底座与标准化流程，平台不仅实现对多家子公司、多个品牌的全面接入，更构建起可扩展、可复制、可监管的集团化运营体系。</p><p>此外，为实现全渠道会员统一运营，平台通过整合分散在各系统中的数据，构建统一、动态、多维度的会员标签画像体系（见图3），支撑精细化运营决策。<br/><img width="723" height="338" referrerpolicy="no-referrer" src="/img/bVdnU15" alt="" title="" loading="lazy"/><br/>图3 多维度会员标签画像体系</p><p>通过会员系统精准化的服务来反哺我们的线上和线下的会员营销和服务，实现线上精准营销、个性化推荐、好物推送、会员关怀，线下关联用药建议、慢病管理提醒、店员主动触达等，提升营销转化率，增强客户粘性，实现“数据驱动服务”的闭环。</p><h2>精细化会员服务，带来海量数据的查询、存储难题</h2><p>然而，随着集团化会员运营平台的推进，精细化服务模式持续深化，导致用户数据规模呈指数级增长，显著提升了系统的查询与存储复杂性。</p><p>会员量：突破千万级，覆盖多个品牌及区域公司。<br/>交易数据量：达到亿级，涵盖线上线下购药、用券、复购等行为。<br/>用户行为类数据：包括商品浏览、搜索、加购等，总量亦达千万级以上。</p><p>这些数据来源于线上商城、私域平台、公众号等多个渠道，经标签体系整合后，用于构建立体化的会员画像，支撑精准营销与双向引流。</p><p>但数据体量大、类型多样、实时性要求高，对数据库的高并发读写能力、存储扩展性与查询性能提出严峻考验。面对千万级会员、亿级交易和多维度行为数据的汇聚，传统数据库难以满足高效处理需求，亟需采用具备高可用、强一致、可扩展特性的分布式数据库系统进行支撑。</p><h2>CRM会员系统数据库升级，应对千万级数据处理难题</h2><p><strong>传统数据库的技术瓶颈制约业务发展</strong></p><p>重药集团会员服务平台的规模化发展，使系统数据总量迅速增长至千万级、数十 TB 存储规模，传统关系型数据库在支撑精细化会员运营场景时，暴露出四大核心挑战。</p><p>性能：百万大表 InnoDB 在高并发读写及复杂查询场景下，性能显著下降，无法满足业务需求，且有事务访问，无法通过拆分提升性能。同时，业务强依赖事务一致性，无法通过拆分提升性能。</p><p>效率：核心归档由于业务需求，需要保留大量数据（数十 TB），会造成 DDL 周期长，延迟业务上线时间。</p><p>成本：随着企业数量增多、历年数据累积，存储成本将越来越高。</p><p>及时性：在各种场景下，对应数据处理的及时性需求越来越强。</p><p>上述技术挑战不乏真实业务案例。</p><p><strong>例 1：某大型连锁店，以满足信创要求为前提进行性能保障</strong></p><p>如今国家对信息技术应用创新（简称“信创”）的要求日益严格，特别是在国有企业中，系统必须符合相关标准才能上线。为了响应这一趋势，我们严格按照信创目录选择数据库产品，并对其进行了全面的业务场景适配与性能验证。</p><p>数据准备：会员卡 9950万+、订单 1 亿 9980万+。<br/>验证数据库：OceanBase 数据库、某数据库1、某数据库2。<br/>验证功能：报表 14 项内容、高级筛选 8 项内容。<br/>参考标准：报表查询小于 20s、静态化数据小于 60s、高级筛选小于 15s。</p><p>测试结果如图4所示。OceanBase 在所有测试项中均显著优于其他两个国产数据库，在报表查询、高级筛选、静态化数据三个场景的性能表现都远超预期：</p><p>报表查询小于 7s，平均提速 78 倍以上。<br/>高级筛选响应高级筛选小于 1s，速度提升 200–700 倍。<br/>静态化数据静态化数据小于 46s，效率提升 6.7 倍以上。<br/><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdnU16" alt="" title="" loading="lazy"/><br/>图4 OceanBase 数据库、某数据库1、某数据库2的测试结果</p><p>在严格遵循国家信创要求的前提下，OceanBase 不仅完全满足合规性准入条件，更在百亿级数据规模下的复杂查询与批量处理场景中展现出卓越性能，远超同类国产数据库产品。基于此，我们总结了三个数据库的性能数据，向客户提交了一份详细的分析报告。</p><p><strong>例 2：连锁会员、订单交易数据量增长迅速，实时性查询瓶颈</strong></p><p>除了信创需求外，客户对业务的实时性、及时性要求也越来越高。过去，企业主要依赖 BI 工具进行周期性报表生成，可容忍数小时甚至数天的数据延迟。然而，随着营销策略向精准触达和即时响应演进，业务人员需要在高价值客户识别、复购提醒触发、定向营销投放、健康知识推荐等场景中获取近实时数据支持。为实现精准服务，运营人员经常需要基于会员信息、会员属性、历史消费、会员标签、商品集合等多个维度进行多维组合筛选，由于关联维度过多，可能会出现查询失败、查询时间过长、范围跨度受限、复杂查询无法支持等问题，显然，这些问题是我们服务的客户无法接受的。</p><p><strong>例 3：海量业务数据，系统可用性与存储成本难平衡</strong></p><p>连锁医药企业会员体系的不断扩展和数字化运营的深入，必然会带来业务数据量的指数级增长，海量数据带来的高存储成本成为制约系统可持续发展的关键瓶颈之一。</p><p>用户数据：累计会员数量突破千万级（&gt;1000万）。<br/>交易流水：日均订单量达百万级，历史累计超过亿级（&gt;1亿条）。<br/>用户行为数据：包括浏览、搜索、加购、收藏等行为记录，总量亦达千万级以上。</p><p>单个业务数据库实例空间占用已达到 N 个 TB 级别，且随时间推移呈线性增长。随着客户数量增加和业务持续扩张，业务数据库实例的空间占用迅速攀升至数十TB甚至上百TB级别，这些数据不仅用于支撑日常业务运行，还需长期保留以满足合规审计、精准营销、客户画像构建等需求。企业面临保障性能与可用性的前提下降低存储成本的难题。</p><p>因此，引入具备高效数据压缩、自动冷热分层、弹性扩展能力的新一代分布式数据库，是实现“数据价值最大化、存储成本最小化”的必然选择。</p><h2>数据库技术引入，支撑海量交易数据的高效处理</h2><p>综合业务需求与传统数据库的技术瓶颈考虑，我们需要替换传统数据库，升级为高性能、稳定性强、成本低、 HTAP 一体化的分布式数据库。</p><p>自 2023 年起，我们开始系统性地评估并引入 OceanBase，历经技术认知、多轮测试、工具链验证、SaaS 级试点上线等关键阶段（见图5），最终成功应用于重药集团会员管理平台。<br/><img width="723" height="150" referrerpolicy="no-referrer" src="/img/bVdnU17" alt="" title="" loading="lazy"/><br/>图5 上线OceanBase的关键阶段</p><p><strong>1.技术引入与评估阶段（2023年）</strong></p><p>测试重点包括三部分。</p><p>其一，日常抖动测试。在对 OceanBase 初期测试时，我们首先进行了业务压力测试。低峰期业务配合100%模拟线上流量直接发压，高达4轮的压力测试，每次持续 3 小时以上。</p><p>其二，扩容/缩容测试。在业务流量低时进行相关操作验证。为了验证是否存在小概率事件，进行了为期一周的脚本自动扩、缩容操作以观察其稳定性。</p><p>其三，Add Index 测试。与扩容、缩容相仿，基于业务流量对1T大表进行多达几十次的add index操作，观察延迟情况。</p><p><strong>2.SaaS 产品试点上线（2023 年 12 月）</strong></p><p>在完成全面技术验证后，我司将 OceanBase 应用于内部 SaaS 类产品中，作为首个生产级试点场景。该阶段实现了：</p><p>数据库稳定运行于真实业务环境中。<br/>验证了迁移、运维、监控等全生命周期管理能力。<br/>积累了宝贵的实战经验，为后续客户项目打下坚实基础。</p><p><strong>3.重药集团项目正式上线（2025 年 4 月）</strong></p><p>基于前期充分验证与试点成果，我们于 2025 年 4 月正式启动重药集团会员管理平台项目，OceanBase 正式投入生产使用，支撑海量交易数据的高效处理。</p><h2>会员服务平台“新面貌”：稳定、高性能、低成本</h2><p><strong>构建标准化数据链路，稳定、高效处理海量数据</strong></p><p>目前，OceanBase 主要支撑重药集团会员服务平台的分析型业务场景，支撑高并发、多维度的会员数据查询、标签计算、报表生成及精准营销决策。其核心价值体现在：高效处理海量历史数据、支持复杂实时分析、保障查询性能与系统稳定性。</p><p>整个数据链路遵循“源系统 → CRM 中转清洗 → OceanBase 分析库”的三层架构，如图6所示。<br/><img width="723" height="313" referrerpolicy="no-referrer" src="/img/bVdnU18" alt="" title="" loading="lazy"/><br/>图6 会员服务平台的数据分析链路</p><p>数据来源（源系统）包括POS 订单数据、各渠道会员信息、组织人员数据、会员标签数据、档案测量数据、全部商品主数据。</p><p>中转与清洗层（CRM 系统）：所有原始数据通过定时抽取或实时接入方式进入 CRM 系统，进行统一的数据清洗、去重、合并与标准化处理。关键处理策略包括历史数据清洗、订单数据合并、积分逻辑处理、会员标签动态更新、消费行为计算、活跃度模型计算。</p><p>目标存储与分析层（OceanBase 分析库）：清洗后的数据通过同步机制实时或定时写入 OceanBase 分析库；并分为原始数据表、静态化处理表、日表/月表、报表中间表。</p><p>通过构建“源数据 → CRM 清洗 →  OceanBase 分析库”的标准化数据链路，实现了多源异构数据的统一整合、复杂分析场景的高性能响应、业务数据的长期留存与高效利用。</p><p><strong>会员精准筛选复杂场景，查询效率提升 25.7 倍</strong></p><p>在重药集团会员服务平台的实际运营中，多维度组合筛选（见图7）是支撑精细化营销与客户管理的核心功能。对于数据库而言，该功能是典型的复杂查询场景，用户需同时基于多个维度进行精确匹配，查询通常涉及多表关联、大量过滤条件和聚合计算，非常考验数据库的执行效率。我们通过开启 OceanBase 的列存模式（Columnar Storage），将原本传统数据库MySQL 的响应时间从 18 秒缩短至 0.7 秒，性能提升达 25.7 倍，满足业务对“实时圈选、即时触达”的严苛需求，显著提升了系统整体吞吐量与用户体验。<br/><img width="723" height="386" referrerpolicy="no-referrer" src="/img/bVdnU2a" alt="" title="" loading="lazy"/><br/>图7 会员服务平台多维度组合筛选</p><p><strong>数据存储空间省 60%，有效降低存储成本压力</strong></p><p>OceanBase 将全量数据划分为两个部分进行管理：一是增量数据（Memtable），即实时写入内存中的热数据，支持快速读写；二是基线数据（静态数据），即经过合并与持久化后的冷数据，存储于磁盘。</p><p>对于静态数据，OceanBase 采用高效的压缩算法，对列式存储的数据进行深度压缩，显著减少磁盘 I/O 和存储开销。例如，当原始数据总量为 4TB 时，MySQL 需要完整保留所有数据，存储空间占用为 4TB；而 OceanBase 通过对静态数据进行高压缩处理，仅需 1.5TB 即可承载相同规模的数据。</p><p>在重药集团会员服务平台的实际部署中，OceanBase 通过其先进的列式存储引擎与高效压缩算法，显著降低了数据存储空间占用，在同等业务数据规模下实现了 60% 以上的存储空间节约，有效缓解了海量数据带来的存储成本压力。</p><h2>面向未来，持续推进 OceanBase 的深度集成与价值释放</h2><p>随着 OceanBase 在重药集团会员服务平台的成功落地，我们对其在更广泛业务领域和客户群体中的应用充满信心。面向 2026 年及未来，我们将围绕场景拓展、客户推广、技术融合与产品适配四大方向，持续推进 OceanBase 的深度集成与价值释放。</p><p><strong>应用于更多业务场景与产品</strong></p><p>当前，OceanBase 已稳定支撑重药集团会员管理平台的复杂分析型业务（如精准筛选、标签计算、报表生成）。订单处理中心和运营诊断产品也在生产环境开始使用OceanBase，下一步，我们将推动其全面融入日常运营服务场景，包括：实时会员服务、营销活动执行、AI 智能推荐等业务场景。</p><p>另外，我们将逐步将 OceanBase 适配至更多内部产品，包括商品主数据管理、患者健康管理平台、智能补货与供应链协同系统，构建以 OceanBase 为核心的统一、弹性、智能的企业级数据基础设施。</p><p><strong>向业内客户推荐</strong></p><p>在国家信创政策与企业降本增效双重驱动下，我们已将 OceanBase 作为高并发、大数据量、强一致性要求场景下的首选数据库，并向行业客户积极推广。截至目前，已在以下大型医药企业成功落地：扬子江药业集团、鹭燕医学、重药集团、上海医药、国大药房。未来，我们将继续优先推荐 OceanBase 作为会员服务、订单中心等关键系统的数据库底座，助力更多企业完成安全、高效、低成本的国产化替代。</p><p><strong>交流开发，沉淀运维经验</strong></p><p>为持续提升团队与客户的 OceanBase 应用能力，我们计划定期组织专题培训、参与社区技术沙龙、共建问题解决机制、定期组织数据库培训及实战分享会议，探讨并解决遇到的问题，争取打造一支“懂业务、精技术、能落地”的复合型数据库应用团队。</p><p>未来，我们将携手更多合作伙伴，共同探索“数据库 + AI + 行业场景”的创新路径，为医药健康行业的高质量发展注入新动能。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=BbXh2tVImkhLkkP%2FBIYVSA%3D%3D.DYHnzQkGrG%2BS1%2FXzLBMyzWy1cIFksp%2FuKqnFk5aN1Wo%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[Python用SentenceTransformer、OLS、集成学习、模型蒸馏情感分类金融新闻文本]]></title>    <link>https://segmentfault.com/a/1190000047607449</link>    <guid>https://segmentfault.com/a/1190000047607449</guid>    <pubDate>2026-02-12 15:11:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=KU5xrBr5LvdyLZuH2qnWTQ%3D%3D.LWQDJa5M70rX3YRcW4%2FjkhPRWKs5ntv5MvPa1by8waQ%3D" rel="nofollow" title="https://tecdat.cn/?p=44976" target="_blank">https://tecdat.cn/?p=44976</a>  <br/>原文出处：拓端数据部落公众号  <br/> </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607451" alt="封面" title="封面"/></p><h3><a name="t2" target="_blank"/>引言</h3><p>在企业级AI真正落地的过程中，大语言模型（LLM）已经成为文本理解任务的核心工具，无论是金融新闻情感判断、行业舆情分析，还是内容风险识别，GPT-4、Gemini等模型都展现出极强的能力。但在规模化落地时，几乎所有数据科学家都会遇到同一个瓶颈：<strong>成本高、速度慢、难以实时处理、无法本地化部署</strong>。当数据量从万级跃升到千万级甚至亿级时，单纯依赖API调用不再具备工程可行性。  <br/>我们团队在长期为金融、资讯类客户提供AI咨询方案时，反复遇到这类问题：如何在不损失精度的前提下，把重型大模型“轻量化”，让普通服务器甚至单机就能跑高性能NLP任务。本文正是基于这类真实项目的技术沉淀，系统介绍一套<strong>集成-蒸馏</strong>的工程化范式：先用多个强模型集成得到超高精度“教师模型”，再通过句子嵌入与线性回归训练“学生模型”，最终实现<strong>轻量模型在金融新闻情感分类上超越GPT-4</strong>。整套方案兼顾精度、速度、成本与可扩展性，已在真实业务中稳定运行。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，该<strong>项目完整代码与数据</strong>已分享至交流社群。阅读原文进群，可与800+行业人士交流成长。  <br/> </p><h4><a name="t3" target="_blank"/>整体流程</h4><ol><li>构建代表性金融新闻数据集</li><li>多LLM标注 → 集成构建教师模型</li><li>句子Transformer生成文本嵌入</li><li>OLS线性回归学习教师输出</li><li>蒸馏得到轻量学生模型</li><li>规模化部署、低成本实时推理</li></ol><hr/><h2><a name="t4" target="_blank"/>背景与问题：大模型好用，但难落地</h2><p>在金融文本分析中，情感分类直接影响投研信号、舆情监控、风险预警等核心业务。过去我们对比过TextBlob、VADER、FinBERT等传统模型，也实测过PaLM-2、GPT-3.5、GPT-4、Gemini-Pro等大模型。结果显示，LLM在情感理解上显著优于传统模型，与人工标注一致性极高。  <br/>但在规模化场景中，问题立刻显现：</p><ul><li>千万级文本调用API成本极高</li><li>处理耗时长达数天甚至数周</li><li>无法满足近实时业务需求</li><li><p>模型不可控、难以本地化部署  <br/>这也是我们提出<strong>集成+蒸馏</strong>模式的初衷：<strong>保留大模型能力，剥离大模型负担，把能力装进轻量模型里</strong>。  </p><p>我们构建了覆盖多年、多行业、多新闻类型的新闻文本集，总计超万条样本，并在这批数据上系统评测了各类模型。以与人工标注一致率为指标，排名靠前的模型包括：Unicorn、GPT-4、Gemini-Pro等。</p></li></ul><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047607452" alt="" title="" loading="lazy"/></p><h2><a name="t5" target="_blank"/>Python用langchain、OpenAI大语言模型LLM情感分析AAPL股票新闻数据及提示工程优化应用</h2><h3><a name="t6" target="_blank"/>原文链接：<a href="https://link.segmentfault.com/?enc=9fq2BdZRnQQTcqbulrEXNw%3D%3D.rnap0KMKpMecKntXlreEM%2BTJaqYm6fGNNFp9gxtp3fw%3D" rel="nofollow" title="https://tecdat.cn/?p=39614" target="_blank">https://tecdat.cn/?p=39614</a></h3><hr/><h2><a name="t7" target="_blank"/>教师模型构建：多模型集成，精度再升级</h2><p>集成学习的核心思想是：<strong>组合多个模型，得到比单一模型更稳、更强的输出</strong>。但并非随便组合就能提升，只有满足以下条件，集成才有意义：</p><ul><li>单个模型效果优于随机</li><li>模型正确时预测相近</li><li>模型出错时互不相关  <br/>我们采用<strong>迭代增量集成策略</strong>，从最优单模型开始，逐步加入能带来正向增益的模型，最终构建出最优教师集成系统。  <br/>经过逐次测试，最优组合为：  <br/>Unicorn + GPT-3.5 + GPT-4 + SigmaFSA + Bison  <br/>最终集成模型与人工标注一致率达到 <strong>90.4%</strong>，比最优单一模型再提升近6个百分点。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607453" alt="" title="" loading="lazy"/>  <br/>我们通过阈值规则将集成输出转为离散情感标签：</li><li>大于1 → 正向</li><li>小于-1 → 负向</li><li>中间 → 中性  <br/>这套教师模型将作为后续蒸馏学习的“标准答案”。</li></ul><h2><a name="t8" target="_blank"/>学生模型构建：基于句子嵌入与OLS线性回归</h2><p>为了让轻量模型学会教师模型的能力，我们选择<strong>句子嵌入（Sentence Embedding）+ 普通最小二乘回归（OLS）</strong>作为学生模型主体。  <br/>句子嵌入可以把不定长文本转为固定维度向量，高质量嵌入能保留文本语义信息。我们选用主流Sentence Transformer模型，包括微软E5系列、BGE系列、GTE系列、Sentence-T5系列等。  <br/>回归模型我们选择最简单的<strong>线性回归</strong>，原因很明确：</p><ul><li>结构简单、推理极快、部署成本极低</li><li>可解释性强，适合金融等合规敏感场景</li><li>在优质特征下效果远超预期</li><li>支持单机、边缘设备、高并发场景  <br/>模型的核心思路是：  <br/><strong>用文本嵌入拟合教师模型输出 → 蒸馏得到轻量可部署模型</strong></li></ul><h2><a name="t9" target="_blank"/>核心实现代码</h2><p>以下为项目核心代码和数据，变量、结构、注释均已重构，保留关键逻辑，省略部分配置与模型列表代码，便于学习与复现。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607454" alt="" title="" loading="lazy"/></p><pre><code>import datetimeimport jsonimport numpy as npimport pandas as pdfrom sentence_transformers import SentenceTransformerfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import train_test_split# 存储实验结果exp_result = {}# 加载新闻数据与标注数据with open("news_data.json", "r", encoding="utf-8") as f: news_content = json.load(f)with open("news_labels.json", "r", encoding="utf-8") as f: news_label_info = json.load(f)# 构建教师模型集成分数for doc_id, label_info in news_label_info.items(): total_score = sum([ label_info["Text-Unicorn"], label_info["GPT-3.5-Turbo"], label_info["GPT-4-Original"], label_info["Sigma"], label_info["Text-Bison"], ]) news_content[doc_id]["teacher_score"] = total_score# 构建输入文本sentence_list = []for doc_id, content in news_content.items(): sentence_list.append(f"{content['Headline']}. {content['Description']}")# 选择需要测试的句子嵌入模型（部分省略）emb_model_list = [ "intfloat/e5-small-v2", "intfloat/e5-base-v2", "intfloat/e5-large-v2", "BAAI/bge-large-en-v1.5", "sentence-transformers/sentence-t5-large", ...... # 此处省略其余模型列表]# 遍历模型训练与评估for idx, model_name in enumerate(emb_model_list): # 加载模型 encoder = SentenceTransformer(model_name, trust_remote_code=True) # 计算参数量 param_count = sum(p.numel() for p in encoder.parameters()) start_time = datetime.datetime.utcnow() # 生成嵌入 embeddings = encoder.encode(sentence_list, show_progress_bar=False) dim_num = embeddings.shape[1] time_cost = (datetime.datetime.utcnow() - start_time).total_seconds() # 构建训练数据 out_df = pd.DataFrame.from_dict(news_content, orient="index") in_df = pd.DataFrame(embeddings, index=out_df.index) # 划分训练集测试集 x_tr, x_te, y_tr, y_te = train_test_split( in_df, out_df, test_size=0.25, random_state=42 ) # 构建教师标签 teacher_val = y_te["teacher_score"].values teacher_class = np.zeros(len(teacher_val)) teacher_class[teacher_val &lt;= -1] = -1 teacher_class[teacher_val &gt;= 1] = 1 # 训练线性回归模型 lr_model = LinearRegression() lr_model.fit(x_tr.values, y_tr["teacher_score"].values) # 预测与评估 y_pred = lr_model.predict(x_te.values) y_pred = y_pred.flatten() y_pred_class = np.zeros(len(y_pred)) y_pred_class[y_pred &lt;= -1] = -1 y_pred_class[y_pred &gt;= 1] = 1 match_num = np.sum(y_pred_class == teacher_class) acc = match_num / len(teacher_class) * 100 print(f"模型 {model_name} 准确率: {acc:.2f}%") exp_result[f"LR_{model_name.split('/')[-1]}"] = { "params": param_count, "time": time_cost, "dim": dim_num, "acc": acc }# 保存结果result_df = pd.DataFrame.from_dict(exp_result, orient="index")result_df.to_csv("model_compare_result.csv", index_label="model")</code></pre><h2><a name="t10" target="_blank"/>结果对比与分析</h2><p>我们一共评测了38个模型，包括教师模型、主流LLM以及30个基于不同句子嵌入的轻量学生模型。整体结论非常明确：<strong>优质嵌入+简单回归，即可在金融情感任务上超过GPT-4</strong>。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047607455" alt="" title="" loading="lazy"/>  <br/>微软E5系列在各参数量级中表现都极为突出：</p><ul><li>小于1亿参数：e5-small-v2 最优</li><li>1–2亿参数：e5-base-v2 最优</li><li>3–4亿参数：e5-large-v2 最优</li></ul><h3><a name="t11" target="_blank"/>384维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607456" alt="" title="" loading="lazy"/></p><ul><li>LLM参考：Unicorn 89.04%，GPT-4 79.90%</li><li>最优轻量模型：e5-small-v2 76.54%</li></ul><h3><a name="t12" target="_blank"/>768维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607457" alt="" title="" loading="lazy"/></p><ul><li>最优：sentence-t5-large 80.40% <strong>（超过GPT-4）</strong></li></ul><h3><a name="t13" target="_blank"/>1024维向量模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607458" alt="" title="" loading="lazy"/></p><ul><li>最优：e5-large-v2 80.21% <strong>（超过GPT-4）</strong></li></ul><h3><a name="t14" target="_blank"/>多语言模型结果</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607459" alt="" title="" loading="lazy"/></p><h2><a name="t15" target="_blank"/>核心价值：为什么这套方案值得企业落地</h2><p>我们从真实业务视角总结这套<strong>集成+蒸馏</strong>模式的四大优势：</p><h3><a name="t16" target="_blank"/>1. 效果更强</h3><p>sentence-t5-large、e5-large-v2等轻量模型在金融新闻情感分类任务上<strong>准确率超过GPT-4</strong>，同时逼近90%+级别的教师集成模型。</p><h3><a name="t17" target="_blank"/>2. 成本极低</h3><p>大模型API调用千万级文本成本极高，而蒸馏后的轻量模型<strong>推理边际成本几乎为0</strong>，一次性训练，终身复用。</p><h3><a name="t18" target="_blank"/>3. 速度极快</h3><p>GPT-4单条推理约0.5秒，千万级文本需数十天；  <br/>轻量模型单条推理约0.005秒，千万级<strong>一天内即可完成</strong>。</p><h3><a name="t19" target="_blank"/>4. 可复用、可扩展</h3><p>这套集成-蒸馏模式不局限于情感分类，可直接迁移到：</p><ul><li>时态判断</li><li>新闻主题分类</li><li>品牌安全检测</li><li>文本风险分级</li><li>政治倾向识别</li><li>前瞻性陈述判断</li><li>上千类网站内容分类</li></ul><h2><a name="t21" target="_blank"/>总结</h2><p>本文提出一套<strong>LLM集成+句子嵌入+线性回归蒸馏</strong>的文本理解轻量化方案，在金融新闻情感分类任务上实现了<strong>轻量模型超越GPT-4</strong>的效果。整套方法工程化程度高、成本低、速度快、稳定可靠，适合大规模文本实时处理场景，是大模型从“演示可用”到“规模化可用”的关键桥梁。  <br/> </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047607451" alt="封面" title="封面" loading="lazy"/></p>]]></description></item>  </channel></rss>