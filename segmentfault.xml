<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[怎么让 vscode/trae 的 git graph 显示的 date 用 iso8601 的格式]]></title>    <link>https://segmentfault.com/a/1190000047558307</link>    <guid>https://segmentfault.com/a/1190000047558307</guid>    <pubDate>2026-01-22 15:12:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>现在 datetime 都是英文的，英文不看的都看不到是几月份的，所以我需要改成纯数字的 iso8601 格式的</p><p>操作方法看下面的图片</p><p><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdnIfB" alt="图片.png" title="图片.png"/></p>]]></description></item><item>    <title><![CDATA[2026移动办公全指南：高效移动端管理工具选型建议 曾经爱过的汉堡包 ]]></title>    <link>https://segmentfault.com/a/1190000047558316</link>    <guid>https://segmentfault.com/a/1190000047558316</guid>    <pubDate>2026-01-22 15:11:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote>站在咖啡馆的柜台前，项目经理李薇用手机轻点几下，就重新分配了因航班延误而受影响的三个任务。这种随时随地的管理能力，如今正通过移动端好用的管理工具成为现实。</blockquote><p>在移动办公成为常态的今天，工作场所已从固定的办公桌延伸至通勤路上、客户会议室甚至家庭客厅。据统计，超过68%的职场人需要在下班后处理工作事务，而其中<strong>近60%的核心协作</strong>通过手机完成。</p><p>但屏幕尺寸、网络环境和交互方式的根本性改变，也给团队管理带来了前所未有的挑战。</p><h2>01 移动办公的挑战：为何传统管理工具力不从心？</h2><p>移动办公的核心特征是<strong>碎片化与场景化</strong>。工作被切割成更短的时间块，穿插在差旅途中等候、会议间隙甚至家庭生活中。这种模式下，传统为桌面端设计的管理工具暴露出了明显短板。</p><p>信息同步延迟是最常见的痛点。团队成员更新了项目状态，但其他人手机上的通知可能迟迟未到，或淹没在混杂的社交信息中。在需要快速决策时，这种延迟可能导致机会错失。</p><p>其次，<strong>功能阉割与操作繁琐</strong>严重阻碍效率。许多工具的移动版只是桌面版的简化移植，关键功能缺失，操作路径深且不符合触屏逻辑。在小屏幕上创建复杂任务、查看甘特图或进行多文件比对，常常令人沮丧。</p><p>更深层的是协作壁垒。移动环境下，沟通与管理往往脱节——重要的讨论散落在微信、钉钉等即时通讯工具中，却无法与任务状态自动同步，导致信息孤岛，<strong>执行过程不透明</strong>，管理者难以掌握真实进度。</p><h2>02 核心价值：移动端专用管理工具如何破局？</h2><p>移动端管理工具的核心价值，在于它<strong>不是对桌面工具的补充，而是为移动场景原生设计的新工作界面</strong>。它重新定义了信息如何被获取、任务如何被处理以及协作如何发生。</p><p>其首要优势是<strong>极致的实时性与可达性</strong>。优秀的移动工具通过优化的推送机制和后台同步，确保信息秒级触达。即使网络短暂中断，也能在本地记录操作，待网络恢复后自动同步，保证工作流不间断。</p><p>其次是<strong>情境智能与操作简化</strong>。工具能根据用户所处场景（如时间、地点、正在处理的任务）智能推荐下一步行动。例如，在接近客户公司时自动弹出相关项目资料；通过语音快速创建任务、用拍照一键上传并关联至工作项，极大降低了移动输入成本。</p><p>最终，它实现了<strong>沟通与执行的融合</strong>。在任务卡片中直接讨论，评论自动转为待办事项，关键对话一键转为任务指派。这让协作上下文完整保存，决策过程可追溯，真正做到了“讨论即执行，执行即记录”。</p><h2>03 实战解析：主流移动端管理工具深度评测</h2><p>理解价值后，我们来剖析几款真正为移动场景深度优化、在手机和平板上拥有卓越体验的管理工具。</p><p><strong>微软 To Do</strong> 是跨平台轻量级任务管理的典范。其移动端与Office 365生态深度绑定，支持邮件星标自动同步，适合个人事务与轻量协作。</p><p><strong>滴答清单</strong> 集日历、习惯打卡、番茄钟于一体，移动端自然语义识别强大，输入“明天三点开会”即可智能创建任务。</p><p><strong>板栗看板</strong> 与微信、钉钉深度融合，支持将群聊对话转为卡片，其移动端看板视图经专门优化，交互流畅。</p><p><strong>Things 3</strong> 是苹果生态中设计美学与生产力的标杆，移动端交互为触控全新设计，能与系统深度联动构建自动化工作流。</p><h2>04 未来展望：AI与移动管理工具的融合趋势</h2><p>移动设备天生的便携性与丰富的传感器，使其成为人工智能技术落地的绝佳平台。未来，移动端管理工具将变得更加主动、情境感知和预见性。</p><p><strong>情境感知的自动辅助</strong>将成为标配。工具将综合GPS定位、日历日程、手机使用状态，智能判断用户当前是否可处理深度任务。例如，在检测到用户正在通勤时，自动推送适合短时处理的审批或阅读任务。</p><p><strong>语音与自然语言成为主要交互界面</strong>。未来的移动管理将更多地通过“语音创建任务”、“对话式询问项目进度”来完成。AI不仅能理解指令，还能追问模糊细节，一次性生成结构完整的任务项，彻底解放双手。</p><p><strong>预测性风险干预</strong>是更高阶的应用。AI通过分析任务推进速度、协作互动频率、历史延期数据等，在移动端提前预警项目风险。例如，向项目经理推送提示：“A任务关联的3个子任务进度均落后，整体延期风险高达70%，建议今天下午召集核心成员进行5分钟快速同步。”</p><h2>05 选择指南：四步找到你的移动管理利器</h2><p>面对众多选择，你可以遵循以下路径，找到最适合团队的那一款：</p><ol><li><strong>核心场景匹配测试</strong>：不要被功能列表迷惑。邀请团队成员，用最常发生的2-3个移动办公场景（如“客户突然来电要求修改方案并同步给团队”）来实测候选工具。观察完成整个流程需要多少步操作，是否顺畅。</li><li><strong>评估离线与弱网能力</strong>：主动关闭Wi-Fi和蜂窝数据，测试能否查看最近的任务列表、能否编辑任务内容。恢复网络后，观察编辑内容是否自动同步、有无冲突提示。这是移动工具可靠性的试金石。</li><li><strong>审视通知系统</strong>：仔细研究工具的通知定制粒度。能否按项目、任务类型、紧急程度设置不同的提醒方式和频率？能否在移动端设置“免打扰时段”？一个既及时又不构成骚扰的通知系统至关重要。</li><li><strong>考量生态集成成本</strong>：检查工具是否与你团队已离不开的日常应用（如邮箱、网盘、通讯软件、签批系统）良好集成。在移动端，每一次强制性的应用切换，都意味着注意力的打断和效率的损耗。</li></ol><h2>06 技术实践：移动任务管理器的简易代码实现</h2><p>以下是一个模拟移动端智能任务管理核心逻辑的简化代码示例，展示了如何根据场景生成任务并进行智能同步：</p><pre><code class="python">from datetime import datetime, timedelta

class MobileTaskManager:
    """移动端智能任务管理器核心逻辑演示"""
    
    def create_task_from_context(self, context: str, location: str = None) -&gt; dict:
        """根据场景和位置创建智能任务"""
        # 智能优先级判断
        priority = "中"
        if "紧急" in context or "尽快" in context:
            priority = "高"
        elif "整理" in context or "备份" in context:
            priority = "低"
        
        # 智能截止时间建议
        due_hours = {"高": 4, "中": 24, "低": 72}.get(priority, 24)
        due_date = (datetime.now() + timedelta(hours=due_hours)).strftime("%m-%d %H:%M")
        
        task = {
            "id": f"task_{datetime.now().strftime('%H%M%S')}",
            "title": f"[移动端]{context[:15]}..." if len(context) &gt; 15 else context,
            "priority": priority,
            "due": due_date,
            "location": location,
            "created": datetime.now().strftime("%H:%M"),
            "status": "待处理"
        }
        
        print(f"✓ 创建任务: {task['title']}")
        print(f"  优先级:{priority} | 截止:{due_date}" + (f" | 位置:{location}" if location else ""))
        return task
    
    def sync_offline_tasks(self, task_list: list) -&gt; dict:
        """模拟离线任务同步"""
        print(f"\n📡 同步中... 发现{len(task_list)}个待同步任务")
        print("─" * 30)
        
        for i, task in enumerate(task_list, 1):
            print(f"{i}. {task['title']:20} | 状态: {task['status']}")
        
        return {
            "success": True,
            "synced": len(task_list),
            "time": datetime.now().strftime("%H:%M:%S")
        }

# 演示移动端任务管理场景
print("🚀 移动端任务管理器演示\n")

manager = MobileTaskManager()

# 场景1：紧急任务创建
print("场景1：客户现场紧急任务")
manager.create_task_from_context("紧急修复客户演示系统BUG", "客户办公室")

# 场景2：常规任务创建
print("\n场景2：常规跟进任务")
manager.create_task_from_context("整理项目会议纪要并发送")

# 场景3：同步演示
print("\n场景3：网络恢复后同步")
tasks = [
    {"title": "修复演示BUG", "status": "进行中"},
    {"title": "整理会议纪要", "status": "待处理"},
    {"title": "提交周报", "status": "已完成"}
]

sync_result = manager.sync_offline_tasks(tasks)
print(f"\n✅ 同步完成 ({sync_result['time']})")
print(f"   成功同步 {sync_result['synced']} 个任务状态")</code></pre><hr/><p>移动端管理工具的进化，其本质是将管理的主动权交还给身处不同时空的人。在上海的地铁里审阅设计稿，在西安的出差途中批准预算，在广州的茶餐厅里同步项目节点——<strong>工作的节奏不再由地点决定，而是由想法和决策驱动</strong>。</p><p>当工具真正理解了移动的本质是“人的流动”，而不仅是“桌面的缩小”，高效协作便不再受限于任何物理边界。选择正确的移动端管理工具，不仅是选择一个软件，更是为你的团队选择一种更自由、更敏锐、更连贯的工作方式。</p>]]></description></item><item>    <title><![CDATA[智能体从0到1：AI Agent构建方法与大模型应用指南 智能体小狐 ]]></title>    <link>https://segmentfault.com/a/1190000047558318</link>    <guid>https://segmentfault.com/a/1190000047558318</guid>    <pubDate>2026-01-22 15:10:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>关键词：智能体、AI Agent、大模型智能体、从 0 到 1、Agent 架构、AI 工作流、LLM 应用</strong></p><hr/><h2>一、背景：为什么现在是智能体爆发的起点</h2><p>2024 年之后，大模型（LLM）进入“能力稳定、成本下降、工具成熟”的阶段，单纯的聊天式 LLM 已无法满足复杂任务需求。真正的拐点在于：​<strong>大模型开始被组织成系统，而不是工具</strong>​，这正是智能体（AI Agent）出现的背景。</p><p>智能体的爆发并不是因为模型突然更聪明，而是因为三件事同时成熟：第一，大模型具备可靠的推理和工具调用能力；第二，API、插件、数据库、搜索等外部工具全面可连接；第三，真实业务场景对自动化、持续运行、闭环执行的需求迅速上升。于是，智能体成为连接大模型能力与真实世界的关键形态。</p><p>从这个意义上说，​<strong>智能体是大模型应用从 0 到 1 的起点，而不是终点</strong>​。</p><hr/><h2>二、什么是智能体：通俗解释与技术解释</h2><p>​<strong>通俗地说，智能体就是“能自己做事的 AI 系统”</strong>​。<br/>它不只是回答问题，而是能理解目标、拆解任务、调用工具、执行动作、接收反馈，并持续调整策略。</p><p>​<strong>技术上，智能体（AI Agent）是以大模型为核心决策引擎的闭环系统</strong>​，它至少包含五个组件：</p><ul><li>感知（输入信息）</li><li>规划（拆解目标）</li><li>执行（调用工具）</li><li>记忆（保存状态）</li><li>反馈（修正行为）</li></ul><p>这使智能体具备“持续运行能力”，而不仅是一次性回答能力。</p><hr/><h2>三、Agent 与普通 LLM 的区别</h2><p>很多人混淆“大模型应用”和“智能体”，但二者差别非常关键。</p><ul><li>​<strong>普通 LLM</strong>​：一次输入，一次输出，任务到此结束</li><li>​<strong>智能体（Agent）</strong>​：目标驱动，循环执行，直到任务完成</li></ul><p>换句话说，LLM 是“大脑”，Agent 是“有手有脚的大脑”。<br/>这也是为什么真正的复杂自动化，一定要使用智能体架构，而不是单次 Prompt。</p><hr/><h2>四、Workflow 与 Agent 的区别</h2><p>在实践中，很多人会问：我用工作流（Workflow）就够了，为什么还要智能体？</p><p><strong>Workflow 是确定性的流程自动化，而 Agent 是不确定性的目标自动化。</strong></p><ul><li>Workflow：步骤固定，适合稳定流程（如数据清洗、报表生成）</li><li>Agent：路径可变，适合开放问题（如研究、决策、协作）</li></ul><p>从 0 到 1 阶段，推荐的做法是：<br/>​<strong>用 Workflow 承载稳定部分，用 Agent 处理不确定部分</strong>​。</p><hr/><h2>五、从 0 到 1 构建智能体的关键步骤</h2><p>构建智能体并不复杂，但必须遵循结构化步骤，否则系统不可控。</p><h3>1. 明确目标（Goal）</h3><p>智能体必须是目标驱动的，而不是指令驱动的。目标越清晰，智能体越稳定。</p><h3>2. 设计规划能力（Planning）</h3><p>规划模块负责把目标拆解成可执行子任务，是 Agent 与 LLM 的关键接口。</p><h3>3. 工具调用（Tool Calling）</h3><p>智能体必须能调用真实工具，例如：</p><ul><li>搜索</li><li>数据库</li><li>API</li><li>文件系统</li><li>代码执行</li></ul><p>没有工具的 Agent 只是“会想不会做”。</p><h3>4. 记忆系统（Memory）</h3><p>记忆让智能体具备“连续性”，包括：</p><ul><li>短期记忆（当前任务）</li><li>长期记忆（历史经验）</li><li>外部记忆（数据库 / 向量库）</li></ul><h3>5. 执行与反馈（Action &amp; Feedback）</h3><p>智能体必须能根据执行结果调整策略，这一步决定系统是否可持续运行。</p><hr/><h2>六、智能体的典型应用场景</h2><p>智能体适合的不是“单点功能”，而是“完整任务”。</p><p>常见场景包括：</p><ul><li>自动研究与资料整理</li><li>企业知识库与问答系统（RAG + Agent）</li><li>数据分析与报告生成</li><li>自动化客服与运营</li><li>软件开发辅助（Coding Agent）</li><li>流程协作与任务管理</li></ul><p>可以这样判断：​<strong>如果一个任务需要反复思考 + 多步执行，就应该用智能体</strong>​。</p><hr/><h2>七、普通人和企业如何入场</h2><p><strong>普通人从 0 到 1 的路径：</strong></p><ol><li>使用现成平台（如智能体构建工具）</li><li>从单一任务开始（例如自动写周报）</li><li>理解 Agent 的结构，而不是模型参数</li><li>优先解决“真实痛点”</li></ol><p><strong>企业从 0 到 1 的路径：</strong></p><ol><li>不要先做平台，先做场景</li><li>用智能体增强流程，而不是替代员工</li><li>从“辅助型 Agent”开始，而不是“全自动”</li><li>把 Agent 当作系统工程，而不是 AI 功能</li></ol><hr/><h2>八、未来趋势与判断</h2><p>可以明确判断：<br/><strong>大模型应用将从“功能型”全面进入“智能体型”阶段。</strong></p><p>未来的核心变化包括：</p><ul><li>Agent 将成为默认应用形态</li><li>RAG + Agent 成为企业标准架构</li><li>单一模型不再重要，系统能力才重要</li><li>智能体将成为新型“数字员工”</li></ul><hr/><h2>九、总结：从 0 到 1，应该立刻做什么？</h2><p>如果你想真正进入智能体时代，建议你马上做三件事：</p><ol><li><strong>停止只学 Prompt，开始学 Agent 架构</strong></li><li><strong>从真实任务构建第一个智能体</strong></li><li><strong>把智能体当系统，而不是工具</strong></li></ol><p>智能体不是未来，而是现在。<br/>从 0 到 1 的窗口期，正在快速关闭。</p>]]></description></item><item>    <title><![CDATA[整合蛋白序列/三维结构/功能特征等数据，德国团队基于度量学习构建构建人类E3泛素连接酶「全景图」 超]]></title>    <link>https://segmentfault.com/a/1190000047558400</link>    <guid>https://segmentfault.com/a/1190000047558400</guid>    <pubDate>2026-01-22 15:09:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>生物体中，细胞蛋白的及时降解与更新是维持蛋白稳态的关键。泛素-蛋白酶体系统（UPS）是调控信号传导和蛋白质降解的核心机制。在这一系统中，E3 泛素连接酶作为关键催化单元，负责识别特定底物并催化泛素标记，从而调控蛋白降解、定位和功能状态。此外，E3 连接酶还调控免疫和炎症通路。由于其组织特异性表达及与发育和代谢综合征（包括癌症进展）的关联，E3 连接酶已成为很有前景的药物靶点，尤其适用于以往难以药物化的靶标。</p><p>与 E1（约 10 种）和 E2（约 50 种）酶相比，人类已鉴定出大量 E3 连接酶（约 600 种）。尽管如此，许多人类 E3 连接酶仍仅被部分表征，仍有大量酶处于假设或未知状态。迄今为止，已研究的 E3 连接酶表现出高度异质性，使其成为最具多样性的酶类之一，为模式识别和大规模研究带来瓶颈。因此，对人类 E3 连接酶组——即人类基因组编码的全部 E3 连接酶进行详细表征和分析，对于全面理解其生物学功能至关重要。</p><p>在此背景下，来自德国歌德大学的研究团队对「人类 E3 连接酶组（human E3 ligome）」进行了分类，整合了多层次数据，包括蛋白序列、结构域组成、三维结构、功能以及表达模式。该团队的分类方法基于度量学习（metric-learning）范式，采用弱监督的层级框架，以捕捉 E3 家族及亚家族间的真实关系。这一方法扩展了 E3 酶的传统分类（RING、HECT 和 RBR 类），区分了多亚基复合物与单体酶，并将 E3 酶映射到底物及潜在药物作用靶点。</p><p>相关研究成果以「Multi-scale classification decodes the complexity of the human E3 ligome」为题，已刊登 nature communications。</p><p>研究亮点：</p><ul><li>将现有 E3 连接酶的结构域架构、三维结构、功能、底物网络及小分子相互作用映射到分类框架中，以获得一般性及家族特异性洞察</li></ul><p>* 所开发的多尺度分类框架涵盖了典型及非典型 E3 机制，为理解 E3 连接酶的广阔生物学图景提供了完整路线图</p><p>* 为开发 E3-底物网络的药物干预策略打开了新思路</p><p><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdnIgT" alt="" title=""/><br/><em>论文地址：</em><br/><em><a href="https://link.segmentfault.com/?enc=zxG0v9wdlNjUFAMvT8OPHg%3D%3D.J4EY78kpvgs7n3BP2QMSZcnC41ue2I6ibBBNu9%2BHhYp56fgnB5wJlTQx8unp%2FJ5XVfakJhjLYl9z3gADcpLknQ%3D%3D" rel="nofollow" target="_blank">https://www.nature.com/articles/s41467-025-67450-9</a></em><br/>关注公众号，后台回复「E3 酶」获取完整 PDF</p><p>更多 AI 前沿论文：<br/><a href="https://link.segmentfault.com/?enc=VtCb6NXS21dnt%2FR5%2BIaeYA%3D%3D.mEkLr23pga9WeILiYPcbQ%2FZ4f062wOxMY4Wj2ZTWXOo%3D" rel="nofollow" target="_blank">https://hyper.ai/papers</a></p><h2>数据集：构建人类 E3 泛素连接酶数据</h2><p>研究团队首先整合了来自 8 个独立数据源的人类 E3 泛素连接酶数据，包括既往文献报道和公共数据库（E3Net、UbiHub、UbiNet 2.0、UniProt、BioGRID 等），形成初步数据集共计 1,448 个蛋白条目。通过对各来源数据的交叉比对与一致性评分，去除了重复和潜在假阳性条目。随后，利用 InterPro 提供的 RING、HECT 和 RBR 催化结构域特征，筛选出 462 个高置信度的催化 E3 泛素连接酶，形成最终的人类 E3 连接酶组。</p><p>在多亚基 E3 复合物（如 Cullin-RING ligases）中，三个功能不同的子单元（支架蛋白、适配蛋白和受体蛋白）协同工作，将 E2\~Ub 分子定位到特定底物上。大型、刚性且位于中心的支架蛋白（如 Cullin 家族，Cul1–Cul5）通过同时结合催化 RING 指结构域亚基和适配蛋白/受体的对接位点，组织起整个连接酶复合体；适配蛋白桥接各模块，将支架蛋白 N 端对接面与独立的底物受体相连；受体蛋白决定底物特异性，直接识别并结合底物上的降解信号（degron），确定哪些底物会被泛素化（如 Skp2、Keap1、VHL）。研究团队独立注释并分类了三类亚基：151 个适配蛋白、106 个受体蛋白和 8 个支架蛋白，并利用它们的蛋白–蛋白相互作用（PPIs）绘制多亚基 E3 的底物映射。</p><p>随后，在催化结构域筛选阶段，研究人员以催化能力为核心判据，对候选蛋白进行严格过滤。通过 InterPro 等结构域数据库，系统识别与 E3 活性直接相关的关键催化结构域，包括 RING、HECT 和 RBR。仅保留明确包含这些结构域、且在序列和结构层面支持其泛素连接功能的蛋白，构建最终的「催化型 E3 连接酶」。这一过程有效剔除了仅参与调控、但不具备直接催化能力的辅助蛋白，从而保证了核心 E3 集合的功能一致性。</p><h2>基于度量学习的多尺度分类框架</h2><p>为了捕捉人类 E3 连接酶组中的复杂关系，研究人员采用机器学习方法来学习一个 Emergent 距离度量，整体框架如下图：</p><p><img width="723" height="145" referrerpolicy="no-referrer" src="/img/bVdnIgU" alt="" title="" loading="lazy"/></p><p>度量学习流程示意图</p><p>①多尺度距离度量</p><p>研究人员通过计算 12 种不同的距离来编码 E3 连接酶两两之间的关系，这些距离覆盖多个粒度层次：一级序列、结构域架构、三级结构、功能、亚细胞定位以及细胞系/组织表达。所有距离度量均被缩放至 [0,1] 区间，以便比较和组合，见下图：</p><p><img width="723" height="149" referrerpolicy="no-referrer" src="/img/bVdnIgV" alt="" title="" loading="lazy"/></p><p>覆盖分子和系统层级组织的多种成对距离度量的分布情况</p><ul><li>序列层面：使用了无比对的局部匹配得分（LMS）距离和基于比对的 γ 距离</li><li>结构域架构层面：计算了三种距离——Jaccard 距离、Goodman–Kruskal γ 距离和结构域重复距离</li><li>三维结构层面：使用 AlphaFold2 模型 TM-score</li></ul><p>* 功能层面：蛋白对 P 和 Q 的功能距离使用 GO 注释的语义相似性衡量，涵盖* 分子功能（MF）、生物过程（BP）和细胞组分（CC）三类本体</p><p>* 亚细胞定位距离</p><p>* 组织和细胞系共表达距离</p><p>②度量优化、聚类、自助法与分类</p><p>四个主要距离（γ、Jaccard、结构、分子功能）通过加权和整合，权重通过弱监督学习和元素中心相似指数（SEC）优化，如下图 ，得到最优组合指标。</p><p><img width="723" height="302" referrerpolicy="no-referrer" src="/img/bVdnIgW" alt="" title="" loading="lazy"/></p><p>通过最 SEC 评估 emergent 分层聚类（右图）与真实标签（左图）的重叠程度</p><p>层次聚类采用 Ward 最小方差法，结合自举方法计算支持度，生成最终 E3 树状图，并在树切割阈值 h = 0.25 下获得最优 emergent clusters，即将 462 个 E3 系统性地划分为 13 个家族，10 个 RING 家族、2 个 HECT 家族、1 个 RBR 家族，如下图：</p><p><img width="723" height="853" referrerpolicy="no-referrer" src="/img/bVdnIgX" alt="" title="" loading="lazy"/><br/>人类 E3 连接酶的分类</p><p>每个家族进一步人工分析序列和结构域特征，识别亚家族和异常蛋白。</p><p>③小分子聚类与结合概率</p><p>整合的 2D UMAP 投影用于小分子聚类，结合局部密度峰值识别 20 个代表性小分子簇。通过 log-transformed propensities（LPij）量化每个簇与 E3 蛋白的结合可能性，为后续 PROTAC 开发和靶向小分子设计提供指导。</p><h2>对人类 E3 连接酶组的完整性提供了详细评估</h2><p>①精细整理人类 E3 连接酶组</p><p>为了解决已有研究在整理 E3 系统时策略多样且定义标准常有差异的挑战，该研究团队明确界定了 E3 系统的催化成分，即包含一个或多个催化结构域的多肽序列。利用这一客观标准，能够对 E3 进行恰当注释并进行针对性分析。最终，研究人员发现所有数据集中共有 462 条多肽序列至少包含一个催化结构域，这些多肽构成了精细整理的人类 E3 连接酶组，见下图：</p><p><img width="590" height="440" referrerpolicy="no-referrer" src="/img/bVdnIgY" alt="" title="" loading="lazy"/></p><p>饼图显示了蛋白注释与筛选的程度，用以区分人类 E3 连接酶的催化和非催化组分</p><p>为了验证整理过程的可靠性，研究人员为每个蛋白定义了基于其在不同来源数据集中出现频次的共识评分。结果显示，HECT 类和 RBR 类 E3 连接酶在数据集中高度一致（共识评分 ≥ 0.6，橙色和紫色柱），而 RING 类（绿色柱）共识评分分布较广，显示出注释上的挑战，如下图：</p><p><img width="596" height="450" referrerpolicy="no-referrer" src="/img/bVdnIgZ" alt="" title="" loading="lazy"/><br/>所有注释蛋白类别的共识评分分布反映了跨数据集对 E3 催化组分的一致性</p><p>通过这一方法，研究人员最大限度地减少了假阳性和真阴性，纳入了高可信度的催化活性 E3，同时考虑了伪 E3 及未经过催化活性验证的其他 E3，从而对人类 E3 连接酶组的完整性提供了详细评估。</p><p>②人类 E3 连接酶的功能分化</p><p>为了评估人类 E3 连接酶的功能，研究人员进行了 UPS 基因的 CRISPR-Cas9 缺失筛选，以细胞活力作为主要表型。结果显示，共识别出 53 个催化型和 32 个非催化型 E3 组分对于细胞活力至关重要，如下图：</p><p><img width="723" height="320" referrerpolicy="no-referrer" src="/img/bVdnIg1" alt="" title="" loading="lazy"/></p><p>火山图显示 CRISPR 筛选中 E3 连接酶的关键基因分析结果</p><p>对 53 个关键 E3 的 GO 分析显示，其在核成分以及 DNA 损伤、复制和修复过程中显著富集，如下图，表明它们在维持基因组完整性和细胞核调控方面的核心作用，这些结果揭示了对细胞生存至关重要的 E3 组分。</p><p><img width="572" height="518" referrerpolicy="no-referrer" src="/img/bVdnIg2" alt="" title="" loading="lazy"/></p><p>对必需催化型 E3 的 GO 富集分析结果</p><p>*<br/>*</p><p>利用 Metascape 对 13 个 E3 家族进行 GO 富集分析，并通过 Cytoscape 可视化网络。结果显示，不同家族在底物选择、细胞定位和催化功能上具有明显分工，如下图。例如，RBR 家族成员 RNF14、RNF144A 和 PRKN 对 K6-linked 泛素具有特异性。K6-linked 链可标记停滞的 RNA-蛋白交联复合物（RNF14）、用于激活干扰素信号的 DNA 感应适配器 STING（RNF144A）以及受损线粒体以便清除（PRKN）。类似地，TRIM E3s（RING5）显著富集于抗病毒先天免疫反应中，它们调控细胞中模式识别受体活性，如 RIG-1 和 MDA5 介导的反应。</p><p><img width="723" height="949" referrerpolicy="no-referrer" src="/img/bVdnIg4" alt="" title="" loading="lazy"/></p><p>热图显示所有功能簇及对应家族特异性富集的 E3</p><p>④人类 E3 连接酶的可成药性图谱</p><p>为了探索基于近距离作用的潜在治疗途径，研究人员将已知的蛋白降解靶向嵌合体（PROTAC）和 E3 结合子衍生的现有 E3 操作位映射到各个 E3 及其家族。目前，仅有 16 个蛋白（9 个催化型 E3 和 7 个适配器）可被现有 E3 操作位直接靶向。已设计的 E3 操作位大多针对适配器蛋白（如 VHL、CRBN），而直接靶向催化型 E3 的仅极少数（如 XIAP、MDM2/4/7、BIRC2/3/7）。</p><p>利用该研究的人类 E3 连接酶进行最近邻分析，发现 5 个高度相关蛋白（BIRC8、RN166/181/141 和 UBR2），如下图：</p><p><img width="728" height="294" referrerpolicy="no-referrer" src="/img/bVdnIg5" alt="" title="" loading="lazy"/></p><p>由于它们具有高度结构相似性（通常为同源蛋白），现有 E3 操作位可被重新利用来靶向这些蛋白。映射小分子 E3 结合子使研究人员获得潜在的化合物集合，可靶向另外 25 个 E3 和 15 个非催化成分，从而发现未开发的靶点，为 E3 操作位的理性设计提供先导化合物开发途径，如下图：</p><p><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdnIg6" alt="" title="" loading="lazy"/></p><h2>多尺度框架为复杂生物系统的解析提供利器</h2><p>在机器学习领域，多尺度框架（multi-scale framework）指的是一种能够在不同抽象层次或不同特征尺度上处理数据的建模方法或分析策略。它并不是固定的算法，而是一种设计思想，用于整合局部与全局信息、粗粒度与细粒度特征，从而提高模型的表达能力和泛化能力。</p><p>多尺度分类框架的价值，并不局限于对 E3 连接酶家族本身的系统梳理，其更重要的意义在于提供了一种可迁移、可扩展的组学整合方法论范式。这种跨尺度的整合思路，使其天然具备向其他多模态组学数据扩展的能力，为复杂生物系统的系统性解析提供了通用工具。</p><p>例如，细胞是生命的基本单位，其功能和命运由复杂的分子网络共同决定。传统的深度学习方法虽在单细胞转录组数据的细胞类型识别中表现良好，但缺乏生物学可解释性。2025 年 10 月 20 日，来自中国国家蛋白质科学中心（北京）、清华大学团队的研究人员提出了一种融合生物先验知识的多尺度可解释深度学习框架 Cell Decoder，实现了从基因、通路到生物过程的分层表征与推理，为单细胞水平上解码细胞类型提供了新的思路。Cell Decoder 通过将蛋白质互作网络、基因-通路映射及通路层级关系嵌入图神经网络架构，构建出跨尺度的生物知识图谱。在七个公开单细胞数据集的人体和小鼠样本上，研究团队对 Cell Decoder 与 9 种主流方法进行了系统评测。结果显示，Cell Decoder 在预测准确率 (0.87) 与 Macro F1 (0.81) 上均居首位，且在存在噪声扰动、细胞类型不平衡及跨批次分布偏移等复杂情形下仍保持稳定性能。\<br/>论文标题：Cell Decoder: decoding cell identity with multi-scale explainable deep learning\<br/>论文地址：</p><p><a href="https://link.segmentfault.com/?enc=hcIsrLdyUihw3QY3SUs0Ig%3D%3D.Y8Zi02Oxud0CZtpsJ%2BALdiUZixv%2FFLo33Ge%2BCpiVfGaJC%2FYffEC%2FayVT03vyMPPNgk%2FTo4t8ubAPSOs4E0mTmg%3D%3D" rel="nofollow" target="_blank">https://link.springer.com/article/10.1186/s13059-025-03832-y</a></p><p>从更长远的视角来看，多尺度框架可以进一步与空间蛋白组学数据、小分子药物库及化学空间信息等相结合，从而打通基础生物学研究、疾病机制解析与转化应用之间的数据壁垒。随着多组学数据持续积累，这一框架有望在生命科学研究与生物医药创新中发挥越来越重要的支撑作用。</p><p>参考文献：<br/>\<br/>1.<a href="https://link.segmentfault.com/?enc=RbuLagC4Ae6UOlT6KntYeA%3D%3D.zY55iSycGHZ%2BUCYb4FPw6sl3lhL73DzSHs%2BYtoBHjAz5Gw%2BQ8USyvv0NHyBPdOdpHrEU5rSsE1Cp9q%2BfEU%2BZTA%3D%3D" rel="nofollow" target="_blank">https://www.nature.com/articles/s41467-025-67450-9</a><br/>\<br/>2.<a href="https://link.segmentfault.com/?enc=6UVoUlRb2y1x3c9kHGoFMw%3D%3D.GqhpyzzNKPmAfmsWHug8Up3Oo5kRiWKlyduvpD4RmTd1Nzr4thUXUDyP7afasdj2vhX13VtFrrlnjcjbJ2sZUg%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/qazplm12_3/article/details/153948711</a><br/>\<br/>3.<a href="https://link.segmentfault.com/?enc=b2RqhaDy1oTsGQmQ9qPJdA%3D%3D.cJQeLIGmX7jLj%2BQcQ6fiFGE8DOiKqec404w%2Bv2hNIVFJLUq37bcdwg7FCjvFjvUkf1OqR5teNy%2Blv%2BjQKz265g%3D%3D" rel="nofollow" target="_blank">https://link.springer.com/article/10.1186/s13059-025-03832-y</a><br/></p>]]></description></item><item>    <title><![CDATA[通义 DeepResearch：开源 AI 智能体的新纪元 DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047558405</link>    <guid>https://segmentfault.com/a/1190000047558405</guid>    <pubDate>2026-01-22 15:08:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>相关点击访问：</h2><p><a href="https://link.segmentfault.com/?enc=1T7dhaV3ee3t4Y3wBaeyFw%3D%3D.DArqDLmvG%2B62DBULdclJZJq5iliBoV%2BKYl%2FeTIBaAo7oGwXdoDHaehGn8Y4mAbOX" rel="nofollow" target="_blank"><strong>github</strong></a></p><p><a href="https://link.segmentfault.com/?enc=u%2BRdIhnVCc1HCuQfKHb3DA%3D%3D.o0%2FSzkzGMjwm10Niom%2FmRji5tPcfRBg%2Bc93IUV5vTn6DwR%2Bk0j66kfFndPqVwOaM%2BIjQQXU1QKxkfSqryHGeiA%3D%3D" rel="nofollow" target="_blank"><strong>HUGGINGFACE</strong></a></p><p><a href="https://link.segmentfault.com/?enc=2fFPQ1LmbKVKpx%2BRk%2FCNxQ%3D%3D.jtg0Fj%2FO1kJvR1NZkwJybCVWDr7h5JfwSbu2UYhYDzHRM%2FpUtqfapX4K%2B1Iwmwmi5yBqCjB%2Bb8A1YOgoK9tZxg%3D%3D" rel="nofollow" target="_blank"><strong>MODELSCOPE</strong></a></p><p><a href="https://link.segmentfault.com/?enc=3s5Ixe7lAs2fn3K4hNUfnQ%3D%3D.KMpZDSE7o7XvLDhDTiXxoDdH8FIQ3jIuOuYEgx7RQVv2bZIUJt2QF4ZmOW2yxaLK" rel="nofollow" target="_blank"><strong>SHOWCASE</strong></a><a href="https://link.segmentfault.com/?enc=WLtM6pfRAPA4UsnO0Xwo4g%3D%3D.XIubzeNjpoNMtrn%2FBZnEgBmq7VXj4XLFshge9f0KiCAR35v4Nlb6U%2FUVx86v4f0W" rel="nofollow" target="_blank"><strong>github</strong></a></p><h2>从 Chatbot 到 Autonomous Agent</h2><p><a href="https://link.segmentfault.com/?enc=dSM1jdA0x6qQCsBz%2BFlEYQ%3D%3D.IiIAvzr3KyNAEauGr3hHqtyFqTsAK5fkg3rPhbjdXzya9lUlCCgtCLYNA445u5f9eWxP3S1MTp33a93qaiEKbvpRxqVERSSQ22eZc4o71cTgI%2BxWMCjA1bgGaoUNgDiV8Rdv6IbUOZg%2FJjGMca0IejDpHbYGiQiwWReAQGiyoqu%2B4%2B%2BNJTzNQ%2FbnJa0Nu2nJ" rel="nofollow" target="_blank"><strong>通义DeepResearch</strong></a> —— 首个在性能上可与 OpenAI DeepResearch 相媲美、并在多项权威基准测试中取得领先表现的全开源 Web Agent。</p><p>在多个极高难度的信息检索和推理任务中，通义DeepResearch 取得了最先进的（SOTA）成绩：</p><ul><li>Humanity’s Last Exam (HLE)：32.9</li><li>BrowseComp‑EN：43.4</li><li>BrowseComp‑ZH：46.7</li><li>xBench‑DeepSearch：75.0</li></ul><p>全面超越了目前所有的闭源及开源 Deep Research 智能体（Agent）。</p><p>不仅如此，我们还完整分享了一套可落地的高水平Agent构建方法论，详细介绍了从数据合成、Agentic 增量预训练（CPT）、有监督微调（SFT）冷启动，到强化学习（RL）的全套流程。在 RL 环节，我们提供了算法创新、自动化数据构建与高稳定性基础设施的全栈解决方案。</p><p>在推理阶段，基础的 ReAct 模式无需任何提示工程即可充分展现模型固有能力，而深度模式（test‑time‑scaling） 则展示了其在复杂推理与规划能力上的上限。</p><h2>基于合成数据的增量预训练和后训练</h2><h4>增量预训练数据</h4><p>我们提出在Agent模型训练中加入智能体增量预训练（Agentic Continual Pre‑training, Agentic CPT）阶段，从而为后训练提供一个强大的Agent基座模型。为此，我们提供了一套支持大规模持续扩展的智能体预训练数据合成方案AgentFounder，并与后训练过程中源源不断生产的数据形成数据飞轮。</p><p><strong>数据重组和问题构建</strong> </p><p>基于广泛收集和持续更新的知识文档、公开可用的爬虫数据、知识图谱以及后训练数据生产和训练中产生的轨迹数据和工具调用返回结果（例如，搜索结果和网页访问记录）等，我们构建了一个以实体为锚定的开放世界知识记忆。进一步，我们基于采样的实体和相关知识构造多风格的（问题，答案）对，以尽可能涵盖智能体所面临的真实场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558408" alt="image" title="image"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558409" alt="image" title="image" loading="lazy"/></p><p><strong>动作合成</strong> </p><p>基于多风格问题和历史轨迹数据，我们分别构建了三种类型的动作数据，包含单步的规划、推理动作和多步的决策动作合成。我们的方法能够在离线环境下大规模、全面地探索潜在的推理‑动作空间，从而消除了对额外商业工具 API 调用的需求。例如，对于决策动作合成，我们将原始轨迹中的步骤进行扩展，并最终建模成多步骤决策过程数据，以激发模型的探索能力和决策能力。</p><h4>后训练数据</h4><p><strong>High‑quality QA</strong></p><p>我们开发了一套端到端的合成数据生成解决方案。这一全自动流程无需人工干预即可构建超越人类质量的数据集，旨在突破智能体的性能极限。经过长期的探索和迭代——从早期的网页点击流逆向工程Benchmark（WebWalker）到基于图谱的合成方法（WebSailor 和 WebSailor‑V2），再到形式化的任务建模（WebShaper），我们的方法确保了卓越的数据质量和强大的可扩展性，突破了模型能力的上限。</p><p>为了解决复杂且高度不确定的问题，我们通过一种新颖的流程合成基于 Web 的问答数据。该流程首先通过在高度互联的知识图谱随机游走和基于表格数据融合同构表构建，将来自真实网站数据整合，并确保信息结构的真实性；然后，我们对子图和子表进行采样，生成初始问题和答案，关键步骤是通过策略性地混淆或模糊问题中的信息来增加问题难度。该方法基于一个组合泛化的理论框架，我们将问答难度正式建模为一系列可控的“原子操作”（例如，合并具有相似属性的实体），这些操作基于实体关系，使我们能够系统地增加复杂性。</p><p>为了进一步减少问答系统的信息结构与推理结构之间的不一致性，提高推理难度和结构扩展能力，我们提出了一种基于集合论的信息搜索问题形式化建模，基于这种建模，我们开发了能够以可控方式扩展问题的智能体，并最大限度地减少了推理捷径和结构冗余，从而进一步提升了问题质量，此外，这种形式还能高效地验证问答的正确性，有效解决了信息搜索合成数据难以验证的挑战。</p><p>我们还开发了一个<strong>自动化学术数据构建流程</strong>，以扩大博士级研究问题的规模。该引擎基于多学科知识库，生成需要多源推理的“种子”问答对；然后，每个种子都会进入一个自我引导的“迭代复杂性升级”循环，其中，一个问题构建代理配备了一套强大的工具，包括网络搜索、学术检索和 Python 执行环境。在每次迭代中，代理都会扩展知识边界，深化概念抽象，甚至构建计算任务，从而形成一个演化循环，上一轮的输出成为下一轮更复杂的输入，确保任务难度的可控且系统地升级。</p><p><strong>融合多样推理模式，激发智能体潜能</strong></p><p>为激发模型的初始能力，我们基于 ReAct 和 IterResearch 框架，通过拒绝采样的方式构建了一组轨迹。一方面，ReAct 作为一个经典且基础的多轮推理范式，为模型注入了丰富的推理行为，并加强了其遵循结构化格式的能力。</p><p>另一方面，我们引入了一种创新的智能体范式——IterResearch（下文将详细介绍）。它通过在每一轮动态地重构一个精简的工作空间，来释放模型的全部推理潜力，从而确保每一个决策都经过深思熟虑，不受上下文噪声干扰。</p><h2>Rollout模式</h2><p>我们对深度研究型智能体的部署范式进行了广泛的探索。因此，我们的最终模型支持多种部署格式，包括原生的 ReAct 模式和上下文管理的深度模式。</p><h4>ReAct 模式</h4><p>我们的模型使用ReAct推理范式展现出卓越的性能。它严格遵循“思考‑行动‑观察”的循环，通过多次迭代来解决问题。模型上下文长度为 128K，可以处理大量的交互轮次，从而完全实现与环境交互的可扩展性。ReAct 的简单性和通用性为模型的内在能力和我们训练流程的有效性提供了最清晰的基准。</p><p>我们选择ReAct很大程度上受到了“The Bitter Lesson”的影响，利用可扩展计算的通用方法最终将优于依赖复杂的人工知识和复杂设计的方法。</p><h4>深度模式</h4><p>除了 ReAct 模式外，我们还开发了“<strong>深度模式</strong>”，用于处理极端复杂的多步研究任务。此模式基于我们全新的 IterResearch 范式，旨在将Agent的能力发挥到极致。</p><p>IterResearch 范式的创建是为了解决Agent将所有信息堆积在一个不断扩展的单一上下文窗口中时出现的认知瓶颈和噪音污染。针对多步研究任务，IterResearch 将其解构为一系列研究回合。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558410" alt="image" title="image" loading="lazy"/></p><p>在每一轮中，Agent仅使用上一轮中最重要的输出来重建一个精简的工作空间，在这个专注的工作空间中，Agent会分析问题，将关键发现整合成一个不断演变的核心报告，然后决定下一步行动——是收集更多信息还是提供最终答案。这种“综合与重构”的迭代过程使Agent能够在执行长期任务时保持清晰的认知焦点和高质量的推理能力。</p><p>在此基础上，我们提出了<strong>Research‑Synthesis框架，</strong>并行使用多个IterResearch Agent探索同一个问题。并最终整合它们完善的报告和结论，从而得出更准确的最终答案，这种并行结构使模型能够在有限的上下文窗口内考虑更广泛的研究路径，从而将其性能推向极限。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558411" alt="image" title="image" loading="lazy"/></p><h2>端到端Agent训练流程</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558412" alt="image" title="image" loading="lazy"/></p><p>训练这样的Agent模型需要<strong>重新思考整个模型训练流程</strong>，从预训练到微调再到强化学习，我们建立了一套完整的智能体模型训练范式，将Agentic CPT → Agentic SFT → Agentic RL 连接起来，为 AI Agent创建了一个无缝的端到端训练循环。</p><p>以下是我们利用强化学习解决最后阶段的方法，对于使代理的行为与高阶目标保持一致至关重要：</p><h4>基于On-Policy策略的智能体强化学习 (RL)</h4><p>通过强化学习构建高质量的Agent是一项复杂的系统工程挑战；如果将整个开发过程视为一个“强化学习”循环，其组件中的任何不稳定或鲁棒性不足都可能导致错误的“奖励”信号。接下来，我们将分享我们在强化学习方面的实践，涵盖算法和基础设施两个方面。</p><p>在强化学习（RL）算法方面，我们基于GRPO进行了定制优化，我们严格遵循 on‑policy 的训练范式，确保学习信号始终与模型当前的能力精准匹配，同时，我们采取了一个 token 级别的策略梯度损失函数来优化训练目标。</p><p>其次，为了进一步降低优势估计（advantage estimation）的方差，我们采用了留一法 (leave‑one‑out) 策略，此外，我们发现未经筛选的负样本会严重影响训练的稳定性，这种不稳定性在长时间训练后可能表现为“格式崩溃”（format collapse）现象。为缓解此问题，我们会选择性地将某些负样本排除在损失计算之外，例如那些因过长而未能生成最终答案的样本，出于效率考虑，我们没有采用动态采样，而是通过增大批次（batch size）和组规模（group size）的方式，来维持较小的方差并提供充足的监督信号。</p><p>训练过程的动态指标显示，模型学习效果显著，奖励（reward）呈持续上升趋势。同时，策略熵（policy entropy）始终维持在较高水平，这表明模型在持续进行探索，有效防止了过早收敛。我们将此归因于Web环境天然的非平稳性，该特性促进了稳健自适应策略的形成，也因此无需再进行显式的熵正则化。</p><p>我们认为，<strong>算法固然重要，但并非 Agentic RL 成功的唯一决定因素。</strong> 在尝试了多种算法和优化技巧后我们发现，<strong>数据质量和训练环境的稳定性，可能是决定强化学习项目成败的更关键一环</strong>。一个有趣的现象是，我们曾尝试直接在 BrowseComp 测试集上训练，但<strong>其表现远不如</strong>使用我们合成数据的结果。我们推测，这种差异源于合成数据提供了一致性更高的分布，使模型能进行更有效的学习和拟合。</p><p>相比之下，像 BrowseComp 这样的人工标注数据，本身就含有更多噪声，加之其规模有限，导致模型很难从中提炼出一个可供学习的潜在分布，从而影响了其学习和泛化（generalize）能力。这一发现对其他智能体的训练同样具有启发意义，为构建更多样、更复杂的智能体训练方案提供了思路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558413" alt="image" title="image" loading="lazy"/></p><p>在基础设施方面，使用工具训练智能体需要一个高度稳定高效的环境：</p><p>● 仿真训练环境：依赖实时 Web API 进行开发成本高昂、速度慢且不一致。我们利用离线维基百科数据库和自定义工具套件创建了一个模拟训练环境来解决这一问题。并且通过SailorFog‑QA‑V2的流程，为该环境生成专属的高质量数据，创建了一个经济高效、快速可控的平台，显著加快了我们的研究和迭代速度。</p><p>● 稳定高效的工具沙盒：为了确保在智能体训练和评估期间对工具的稳定调用，我们开发了一个统一的沙盒。该沙盒通过缓存结果、重试失败的调用以及饱和式响应等改进来高效地处理并发和故障。这为智能体提供了快速且鲁棒的交互环境，可以有效防止工具的错误响应破坏其学习轨迹。</p><p>● 自动数据管理：数据是提升模型能力的核心驱动力，其重要性甚至超过了算法。数据质量直接决定了模型是否能通过自我探索提升分布外泛化能力。因此，我们在训练动态的指导下实时优化数据，通过全自动数据合成和数据漏斗动态调整训练集。通过数据生成和模型训练之间的正向循环，这种方法不仅确保了训练的稳定性，还带来了显著的性能提升。</p><p>● On‑Policy策略的异步框架：我们在 rLLM 之上实现了异步强化学习训练推理框架，多个智能体实例并行与（模拟或真实）环境交互，独立生成轨迹。</p><p>通过这些措施，我们实现了智能体强化训练的“闭环”。从基座模型开始，我们进行了Agentic持续预训练以初始化工具使用技能，然后使用类似专家的数据进行监督微调以实现冷启动，最后进在on‑policy的强化学习，使模型进行自我进化。这种全栈方法为训练能够在动态环境中稳健地解决复杂任务的 AI 代理提供了一种全新的范例。</p><p>（我们的强化学习算法受到 <a href="https://link.segmentfault.com/?enc=chMv6udW7qF1Yvd03AnBAQ%3D%3D.csnYfBBdf63MQvJ6UDm61I9LDLkngWGKUNgwHyMNMEsCD%2Fl1P7zxxGQn1K2LxOrg" rel="nofollow" target="_blank">Agentica</a> 过去研究的启发。我们基于<a href="https://link.segmentfault.com/?enc=sjZnuGwX%2BwFZUnO2ByFdBw%3D%3D.QpEQcRmxNyAgMaLLf6mBXKM0N5xkfkh%2FUi4oYEyw2yyKB2nQY59w4ERG0%2FsPhOGu" rel="nofollow" target="_blank">rLLM</a>框架进行开发和扩展，实现高效训练）</p><h2>应用及影响</h2><p>通义Deep Research不仅仅是一个研究成果的展示，它<strong>已经在阿里巴巴内外赋能实际应用</strong>，并在实际场景中展现其价值：</p><p><strong>高德地图（地图导航智能体）</strong> 高德 App 作为通义在集团内长期共建的重点客户，其“地图导航+本地生活”的业务场景，以及高德内部丰富的专用工具，具备构建Deep Research 类 Agent 的土壤，高德也将这种能力作为 25 年暑期大版本 V16 的一个亮点功能。通义团队近期在地图+本地生活场景，基于纯agentic + ReAct执行复杂推理的垂类Deep Research技术建设，为高德提供更好效果的模型。因此，双方团队共建合作，“通义团队提供Deep Research模型 + 高德团队提供工具和 Agent 链路”，打造了高德 App 中助手「小高老师」的复杂查询体验，在地图行业内打出影响力。</p><p><strong>通义法睿（法律Deep Research）</strong> 作为大模型原生的“法律智能体”，致力于为大众及法律从业者提供专业、便捷的法律智能服务。集法律问答、案例法条检索、合同审查、文书阅读、文书起草等功能于一体，全面满足法律用户需求。依托创新的Agentic架构与迭代式规划（Iterative Planning）技术，通义法睿全新升级司法DeepResearch能力，可高效执行多步查询与复杂推理，实现权威类案精准检索、法条智能匹配与专业观点深度融合。我们以真实判例、官方法规和权威解读为基础，打造可追溯、高可信的法律分析服务，在法律问答的深度研究三大核心维度——答案要点质量、案例引用质量、法条引用质量上领先行业。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558414" alt="image" title="image" loading="lazy"/></p><h2>未来工作</h2><p>我们未来的工作将致力于解决以下三个关键局限性：首先，当前 128k 的上下文长度在处理极端复杂的长程推理任务时仍显不足。为此，我们将探索扩展上下文窗口的有效方法，并研究更精细的上下文管理策略。其次，我们训练流程的可扩展性在远超 30B 参数规模的模型上尚未得到充分验证，我们计划在更大规模的模型上测试并验证我们流程的有效性。最后，我们旨在通过引入 partial rollouts 等技术进一步提升强化学习框架的效率，这需要我们攻克离线训练所面临的挑战，尤其是分布偏移问题。</p><p>敬请期待我们下一代Agent模型：</p><pre><code class="plaintext">@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558415" alt="image" title="image" loading="lazy"/></p><hr/><p><strong>点击下方访问产品链接：</strong></p><p><a href="https://link.segmentfault.com/?enc=zTojal2kc65mlfEUH5hFQw%3D%3D.cBkzywWGuE8%2BXRi8EEXXXFZjXrtUE1hTd3TGWb6npAuHQMZYV9r4wk3WzEEa%2F0YZt4ny%2B%2Fqz92%2B9GKZn8qAfSWIqX4d0BFarL80fEIkKNLoJN7lifrE75PQFewYGM1kLi1rvJ3a%2FJhs4gi%2Fu%2BloR%2BIb5UCWYZnQSp8IpIHKsCf9ILNpL%2F3f%2FRwwrxG1NFaTIayPTS1bFFosMcH%2FB6KiNi2gQghZ2lDRSCt%2FswgvTCb5QfXnjrKT5Diaz5OnQH9y2" rel="nofollow" target="_blank">面向深度的查询问答和调研分析需求场景,多步骤推理规划研究路径,生成有洞察、可溯源、图文并茂的长文报告-大模型服务平台百炼(Model Studio)-阿里云帮助中心</a></p>]]></description></item><item>    <title><![CDATA[2026-01-22 GitHub 热点项目精选 程序员锋仔 ]]></title>    <link>https://segmentfault.com/a/1190000047558426</link>    <guid>https://segmentfault.com/a/1190000047558426</guid>    <pubDate>2026-01-22 15:07:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>🌟 2026-01-22 GitHub Python 热点项目精选(12个)</h2><blockquote>每日同步 GitHub Trending 趋势，筛选优质 Python 项目，助力开发者快速把握技术风向标～</blockquote><hr/><h3>📋 项目列表（按 Star 数排序）</h3><h4>1. <a href="https://link.segmentfault.com/?enc=dk453IY0z0VEaX093J6IRA%3D%3D.KDoi3%2BprwNf59nwA5hfx3jhUQhKEVaTlJR6KVXjhq0GEPo8XOSTFp8v%2BUej6upBg" rel="nofollow" target="_blank">xai-org/grok-1</a></h4><blockquote>这是一个由 xai-org 开发的项目，可能与人工智能的可解释性（XAI）相关。从项目名称来看，它可能是某种模型或工具，用于帮助理解复杂模型的行为或数据的底层结构。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 51045（今日+141）</td></tr><tr><td>Fork 数</td><td>🔄 8433</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=ygXp5%2Byf7KrTSV6hA3ohgA%3D%3D.XvMM7%2BhRtBy1%2BbmUlPxA7G9nzVY1UvNtdoryYeO%2BkaOz8m9XTu9ExzT6i6mRW%2FYJ" rel="nofollow" target="_blank">https://github.com/xai-org/grok-1</a></td></tr></tbody></table><hr/><h4>2. <a href="https://link.segmentfault.com/?enc=UEY%2F4Kq%2FH%2F9wgcXXOrBCGg%3D%3D.Dv8CY%2FE1SXn0nv1sLI0hvFB2P%2F16aNaqzI1N72Ji4iYGfjdAP5TMHExqWcMkddcW" rel="nofollow" target="_blank">microsoft/agent-lightning</a></h4><blockquote>微软开发的项目，从名称上看，可能与轻量级代理或快速响应的代理系统有关。它可能用于优化网络通信、提高代理服务的效率或构建低延迟的代理架构。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 11286（今日+527）</td></tr><tr><td>Fork 数</td><td>🔄 924</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=Cp6kGl1UfRWndDm9JvidAA%3D%3D.0s%2BRVCjuOgJidcU1Yf5XOP80xAWDE%2FyeYf3QxH8AS2oSmHkXJTexa12aRnsPYXs6" rel="nofollow" target="_blank">https://github.com/microsoft/agent-lightning</a></td></tr></tbody></table><hr/><h4>3. <a href="https://link.segmentfault.com/?enc=xehlwBuTFWjlwNv28qORrQ%3D%3D.3hiIEplAQeFAGgQeIA61ChpKIN1tLDBCg8bT7NpgL9Y4SZQUi6%2FNEAzRltfXYqaG" rel="nofollow" target="_blank">VectifyAI/PageIndex</a></h4><blockquote>VectifyAI 开发的项目，可能与数据索引或搜索技术有关。它可能用于构建高效的索引系统，以便快速检索和处理大量数据，适用于人工智能和机器学习场景。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 6220（今日+100）</td></tr><tr><td>Fork 数</td><td>🔄 493</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=huMkmyVLGtmCUwhneoDgLA%3D%3D.NCmCaKy1DbKoag%2Bc4z2HOClougW8xFrBba7CqBsCAMCZRFwM7l2ne5AVHTF7R06X" rel="nofollow" target="_blank">https://github.com/VectifyAI/PageIndex</a></td></tr></tbody></table><hr/><h4>4. <a href="https://link.segmentfault.com/?enc=Ety41EcrspBZFg%2FlwLB5jQ%3D%3D.pa37N%2FjbHX4Ye1p%2Bqm2J%2BZZTnloN3aZBVo0qo%2B0yy8JVfWkKylJYEV1%2BhClLmX0k" rel="nofollow" target="_blank">ahujasid/blender-mcp</a></h4><blockquote>这个项目可能是与 Blender（一个流行的开源 3D 建模和动画软件）相关的一个插件或工具。它可能用于增强 Blender 的功能，例如自动化建模流程或提供新的材质处理方式。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 16246（今日+97）</td></tr><tr><td>Fork 数</td><td>🔄 1542</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=d0El6wo%2BbEyod%2F4AJQraVA%3D%3D.jTMo%2FghvWn05ehYwvNQPujlRIAZKOMCpsxf25wlDOUuPUYzRqLFhnZHviOd7%2FLaG" rel="nofollow" target="_blank">https://github.com/ahujasid/blender-mcp</a></td></tr></tbody></table><hr/><h4>5. <a href="https://link.segmentfault.com/?enc=Xg9xwUBNGsTltGtvSmS%2FjA%3D%3D.BuiOg0Xa3GfFLruuYqR5Q%2B0ntT5xbRPsDeKJgdoT%2Fi50Lkc4ETXe%2BQY5DhROxmBJ" rel="nofollow" target="_blank">PaddlePaddle/PaddleOCR</a></h4><blockquote>PaddleOCR 是由百度开源的 PaddlePaddle 框架下的光学字符识别（OCR）工具。它支持多种语言的文本检测和识别，具有高精度和高效率的特点，广泛应用于文档处理、图像识别等领域。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 68576（今日+142）</td></tr><tr><td>Fork 数</td><td>🔄 9682</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=gzfEwg9zSt%2FCGif8ndsZ2g%3D%3D.Sh%2BadED9QrvVwBCrGiIgCWosPWHGCUTcRmcv53cmB4fCpCcWmtZNXJXBNSZ1I6mZ" rel="nofollow" target="_blank">https://github.com/PaddlePaddle/PaddleOCR</a></td></tr></tbody></table><hr/><h4>6. <a href="https://link.segmentfault.com/?enc=0aTnv1le4TE%2FF2QmNYOasA%3D%3D.m9LRXSgi5H2jB0aQeeT9f7pAlO0QSco3fGJ1Hn2eZCCCoQJ6AxaRIihXTmDHNCNd" rel="nofollow" target="_blank">datawhalechina/all-in-rag</a></h4><blockquote>DatawhaleChina 开发的项目，可能与检索增强生成（RAG）技术有关。它可能用于构建更智能的问答系统或文本生成工具，通过结合检索和生成模型来提高输出质量和相关性。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 3275（今日+36）</td></tr><tr><td>Fork 数</td><td>🔄 1533</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=zYdl5r9B%2B5ysXdnTSWnzRw%3D%3D.csuGfSseSArbO43JAOUBkwR8Fc50ACFb17i52QrwPT%2BzeUy3BRUw0jBoy5BNcJBO" rel="nofollow" target="_blank">https://github.com/datawhalechina/all-in-rag</a></td></tr></tbody></table><hr/><h4>7. <a href="https://link.segmentfault.com/?enc=aOQlc5m9OqXiHoRRjdw%2FwQ%3D%3D.6eFRfgwL%2BOOMGWJ%2F3y3Vsh9NidBW4pzK5ejCrQaZBJ3nEBl0KR5PuoFlZUoH08s6" rel="nofollow" target="_blank">pytorch/executorch</a></h4><blockquote>PyTorch 开发的项目，可能与模型部署和执行优化有关。它可能用于将 PyTorch 模型高效地部署到各种设备上，提高模型的运行速度和资源利用率。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 4172（今日+7）</td></tr><tr><td>Fork 数</td><td>🔄 806</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=eVM3az8qJZ29fPcgtfQNZw%3D%3D.EjGL7B4dxdaH1bhbbJJ3UvsaYgmhZ5uv786TZU4BjcnlkDmugrwtXCR29R6IKFdA" rel="nofollow" target="_blank">https://github.com/pytorch/executorch</a></td></tr></tbody></table><hr/><h4>8. <a href="https://link.segmentfault.com/?enc=oCSMx8yiBMoqi7fbVtd%2F9A%3D%3D.MoLYFsUHxng8%2FsysgYnHkPY3ETMiQkmQ1UXiVJor7n%2Bm1hyfC4yMfZf0T0oXq0IY" rel="nofollow" target="_blank">lzhoang2801/OpCore-Simplify</a></h4><blockquote>这个项目可能是某种操作核心（OpCore）的简化版本或优化工具。它可能用于简化复杂的操作流程或优化代码性能，适用于特定的计算任务或框架。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 4702（今日+32）</td></tr><tr><td>Fork 数</td><td>🔄 461</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=rqvRm5zFAjQpvaEa%2BDhW7A%3D%3D.LX1ILGtq6dF8wS04OEI%2B7%2BfgJOKAbdYze7d7taQ7lEqXloJsg1btEUVl7r7GHyc1" rel="nofollow" target="_blank">https://github.com/lzhoang2801/OpCore-Simplify</a></td></tr></tbody></table><hr/><h4>9. <a href="https://link.segmentfault.com/?enc=xetubbOfNKgofdfLfKMCfA%3D%3D.O%2BBGZITGQ8e40BZn3FnfN3ZOiFxSvWxh%2Fi8yPZzlNjtMemwlNw%2Fa%2BG%2BF3mybuA76" rel="nofollow" target="_blank">karpathy/nanochat</a></h4><blockquote>由 Andrej Karpathy 开发的项目，可能是一个轻量级的聊天机器人或对话系统。它可能用于研究自然语言处理中的对话生成技术，具有简单的架构和易于扩展的特点。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 40618（今日+42）</td></tr><tr><td>Fork 数</td><td>🔄 5257</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=%2F9fTolwJ8y8ND71OVcxw8w%3D%3D.6klsyKOaHa7r5PnHTI8%2FXH2wU0%2BVIGx22zjCh1j69up32vSlJAUC7dvWcnzMhqt0" rel="nofollow" target="_blank">https://github.com/karpathy/nanochat</a></td></tr></tbody></table><hr/><h4>10. <a href="https://link.segmentfault.com/?enc=1773Hr40u2d49B0v%2FsMKLg%3D%3D.B8NodLBtfxQi%2FBakKj2IxquLY%2B3TDw2iSsNoxjpv1jHYiMMzJ1fmequf5F9s0Hes" rel="nofollow" target="_blank">vllm-project/vllm</a></h4><blockquote>VLLM 项目，可能与大规模语言模型的优化和部署有关。它可能提供了一种高效的方式来管理和运行大型语言模型，适用于需要高性能计算的自然语言处理任务。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 68081（今日+101）</td></tr><tr><td>Fork 数</td><td>🔄 12770</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=79YAeupGRhPzbFQSVjtI7Q%3D%3D.Ao7%2F1gjjUE%2FhgrFOiWW6LAgp6vFyga0wf%2BrOik7fs65isaiq9KO2sWJxutC%2FOC9b" rel="nofollow" target="_blank">https://github.com/vllm-project/vllm</a></td></tr></tbody></table><hr/><h4>11. <a href="https://link.segmentfault.com/?enc=%2BxUJPGHLUGxB0ZAR6w5eBw%3D%3D.5RuH3%2F4leT8Y0XnRiO39iNKWSHNlQ7cQvajrefydPkkwkjAKX1v9vz2JW3MqJlKJ" rel="nofollow" target="_blank">yichuan-w/LEANN</a></h4><blockquote>这个项目可能是某种学习算法或框架的实现。它可能用于研究新的机器学习方法或优化现有算法，适用于数据科学和人工智能领域的研究。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 9545（今日+45）</td></tr><tr><td>Fork 数</td><td>🔄 822</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=Oh5P2QpEsUYQ0wqXTWduRQ%3D%3D.sYtOIR90IQXFmfYcfkLKKiwneC%2FfPXNt4mZ%2BYcaeqDHkmclrd6SoCYx9fL%2Bz6kF5" rel="nofollow" target="_blank">https://github.com/yichuan-w/LEANN</a></td></tr></tbody></table><hr/><h4>12. <a href="https://link.segmentfault.com/?enc=grAQ98BFNtGOLwK8E6WSqA%3D%3D.aXEikkc7GT8yLCAuW3B2JJmrPfDnTdIf6te%2FHwGihdXOUsw6QZyBUcu8eGdI8wVo" rel="nofollow" target="_blank">KellerJordan/modded-nanogpt</a></h4><blockquote>这是一个对 Nanogpt 进行修改或扩展的项目。它可能在原始的 Nanogpt 基础上增加了新的功能或优化了性能，适用于自然语言处理和生成任务。</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 4223（今日+16）</td></tr><tr><td>Fork 数</td><td>🔄 564</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=25nCOz9Rxmn91ttVdv5LWA%3D%3D.z0RZLsCByqdujjOqP0iN2oTOOCDQujFXGzALyrZhjiYK4200a5me%2FDRWMzEDn5vc" rel="nofollow" target="_blank">https://github.com/KellerJordan/modded-nanogpt</a></td></tr></tbody></table><hr/><h3>📝 说明</h3><ul><li>数据来源：GitHub Trending（2026-01-22 每日榜单）</li><li>筛选条件：Python 语言 + 当日热门项目</li><li>自动更新：每日同步最新趋势，建议收藏本文持续关注～</li></ul><h3>⭐ 推荐理由</h3><ol><li>热门项目代表当前技术趋势，学习价值高</li><li>优质项目代码规范，可作为学习参考</li><li>部分项目可直接用于实际开发，提高效率</li></ol>]]></description></item><item>    <title><![CDATA[金融资管实战：WhaleStudio 助力某亚洲投资基金构建跨云 Lakehouse 统一数据中枢 ]]></title>    <link>https://segmentfault.com/a/1190000047558429</link>    <guid>https://segmentfault.com/a/1190000047558429</guid>    <pubDate>2026-01-22 15:07:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558431" alt="" title=""/></p><h2>案例背景</h2><p>作为亚洲领先的投资基金，某东南亚投资基金公司（以下简称 A 基金）正处于从传统数仓向企业级数据中台转型的关键期。目前，其核心业务系统深植于 AWS 环境，涵盖了 SQL Server、MySQL 及 S3 等多种存储形态，并已初步建成基于 MSK（Kafka）与 Flink 的实时处理链路。为了应对日益增长的业务需求，A 基金规划引入 <strong>Databricks Lakehouse</strong> 作为统一的数据底座。</p><p>然而，随着任务规模预估跨越式增长，多云环境导致的“碎片化”问题愈发凸显。跨云任务协同困难、多套调度体系割裂、缺乏 CI/CD 机制以及 Databricks 作业无法深度纳管等挑战，使得平台运维成本激增，资源弹性难以支撑业务峰值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558432" alt="e6984589-71da-4116-8f19-e47ad63b2d2b" title="e6984589-71da-4116-8f19-e47ad63b2d2b" loading="lazy"/></p><h2>核心挑战</h2><p>具体来说，A 基金在推动企业级数仓与数据中台建设的过程中 遇到的核心挑战来源于多方面：</p><ul><li><strong>多云环境共存导致协同困难</strong>： 存量系统在 AWS，新系统与 Lakehouse 规划落在 Databricks（跨云可部署），跨云数据传输与资源调度缺乏统一协同机制。</li><li><strong>数据工具多样、调度体系割裂</strong>： 内部存在多套同步与调度方案，缺少统一编排、统一运维监控与统一告警体系。</li><li><strong>缺乏 CI/CD 机制</strong>： 任务上线、变更依赖人工导入导出，版本控制、审计与回滚能力不完善。</li><li><strong>资源弹性不足</strong>： 高峰期任务堆积、低峰期资源闲置，扩缩容响应不及时，影响整体 SLA。</li><li><strong>Databricks 作业体系纳管不足</strong>： Databricks Jobs/Notebook/Workflow 与现有调度体系割裂，容易形成“第二套平台”，进一步加剧治理碎片化。</li><li><strong>Lakehouse 建设需求增强</strong>： 需要支持批/实时数据统一落地到 Lakehouse，支持 Schema 演进、版本治理与表格式演进策略，避免口径漂移与数据孤岛。</li><li><strong>运维噪声与体验问题</strong>： 任务状态多、告警多、定位慢；Dashboard 缺少时间记忆与常用筛选保持，影响日常运营效率。</li></ul><h2>WhaleStudio + Databricks 统一湖仓方案</h2><p>针对上述挑战，A 基金采用 <strong>WhaleStudio 商业版</strong> 作为统一的数据集成与调度中枢，深度纳管 AWS 与 Databricks 作业体系。通过“批处理+CDC”双引擎及实时链路（MSK+Flink）统一编排，打破多云割裂，消除治理孤岛。结合 CI/CD 自动化交付与动态扩缩容架构，在支撑万级任务扩展的同时，实现 Lakehouse 的标准化治理与智能运维，确保金融级数据的高可靠与强一致性。</p><p>具体来说，WhaleStudio 商业版作为核心的数据集成与调度中枢，通过以下四大核心模块，实现了从数据接入到运维治理的全流程自动化，将 Databricks Lakehouse 深度整合进企业的统一治理闭环：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558433" alt="cb49a6e2-44ac-4ac0-bc6d-4a99cdca86f9" title="cb49a6e2-44ac-4ac0-bc6d-4a99cdca86f9" loading="lazy"/></p><h3>1. 统一编排中枢：跨云协同与 Databricks 深度纳管</h3><p>该方案通过构建统一的任务中心与元数据仓库，整合了原本分散的集成与调度工具，实现跨系统的集中管理与审计。它不仅能够统一编排 AWS 生态下的原生任务，更实现了对 <strong>Databricks Jobs / Notebook / Workflow</strong> 的深度对接。通过建立跨云任务的统一依赖、统一调度与统一监控体系，有效避免了 Databricks 沦为孤立的“第二套平台”，确保了多云环境下业务协同的连贯性。</p><h3>2. 批流一体架构：双引擎接入与实时链路治理</h3><p>为了满足金融资管对数据时效性的多样化需求，平台提供 <strong>“批处理 + CDC”</strong> 双引擎接入能力，全面覆盖 SQL Server、MySQL 及 S3 等多源数据的采集与同步。同时，方案将 Kafka (MSK) 与 Flink 实时流任务深度纳入统一工作流编排，形成了离线分层落地与实时链路供给并行的治理模式。这种“批流一致”的体系，确保了实时与离线任务在调度逻辑、监控视图及告警机制上的高度统一。</p><h3>3. 规范化湖仓落地：Lakehouse 演进与自动化交付</h3><p>在数据落地阶段，方案优先支撑产出统一汇聚至 <strong>Databricks Lakehouse</strong>，构建起从 ODS、DWD 到 DWA 的标准化分层体系。平台兼容 Delta 与 Iceberg 等主流表格式策略，并提供 Schema 演进与版本治理能力，防止口径漂移。此外，通过引入 <strong>CaC（配置即代码）与 CI/CD 标准化流水线</strong>，实现了配置版本化、变更审计与灰度发布，将传统的人工操作转化为自动化的持续交付，极大降低了上线风险。</p><h3>4. 智能化运维体系：告警降噪与交互体验优化</h3><p>针对大规模任务环境下的运维压力，方案提供了智能化的监控解决方案。通过多级告警聚合与降噪技术，配合失败/告警过滤视图，运维人员能从海量信息中快速锁定核心问题。同时，系统对 Dashboard 进行了人性化改良，支持时间记忆与筛选状态保持，大幅提升了异常定位的速度与日常运营的整体效率。</p><h2>方案对比：从多工具拼装到一体化中枢</h2><p>在 A 基金最初的架构设计中，多工具拼装的“烟囱式”结构虽然在短期内解决了业务上线快的问题，但随着任务规模向万级跨越，这种模式带来的协同成本和运维压力已成为技术债。</p><p>WhaleStudio 方案的核心价值在于“打破割裂”，它不是在原有的工具堆栈上多打一个补丁，而是通过<strong>统一的编排大脑和标准化的交付流水线，将 Databricks 从一个孤立的计算引擎，彻底转变为企业全局数据治理闭环中的一部分</strong>。这种转变不仅是为了解决当前的运维噪声，更是为了在跨云环境下，为后续 Lakehouse 的长期演进提供一个稳固的工程化底座。</p><p>通过下图和表格，我们可以直观地看到架构重塑前后的差异：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558434" alt="129d92dc-237e-48b1-95e1-4cb9f0881471" title="129d92dc-237e-48b1-95e1-4cb9f0881471" loading="lazy"/></p><table><thead><tr><th>维度</th><th>原方案：多工具拼装</th><th>推荐方案：WhaleStudio + Databricks Lakehouse</th></tr></thead><tbody><tr><td><strong>典型形态</strong></td><td>SQL Server/MySQL/S3/Blob →（多套同步工具+多套调度系统）→ Kafka/MSK（实时）+ Flink（流计算）→ Databricks/数仓落地（各自管理）→ 数据质量/告警/审计分散</td><td>数据源（AWS SQL Server/MySQL/S3/Blob/Kafka）→ WhaleStudio（统一集成+统一编排+统一治理）→ 实时链路（MSK/Flink）与湖仓链路（Databricks Lakehouse）闭环</td></tr><tr><td><strong>优点</strong></td><td>选型灵活，局部上线快；单点需求可用最熟悉工具解决；短期推进速度较快。</td><td>更少组件、更强一体化；Databricks 统一纳管；跨云统一视图与资源调度；CI/CD 标准化交付；分布式弹性扩缩容；Lakehouse 可演进。</td></tr><tr><td><strong>缺点</strong></td><td>链路割裂，跨系统定位成本高；跨云难统一，协同效率低；缺少 CI/CD 导致上线风险高；资源不弹性，SLA 不稳定；Databricks 纳管不足。</td><td><strong>（实施建议）：</strong> 建议分阶段落地：先统一集成与编排中枢，再逐步深化 CI/CD、Lakehouse 治理与智能运维能力，以确保风险可控。</td></tr></tbody></table><h2>业务价值与收益：从效率跃迁到治理升级</h2><p>总结起来，通过引入 WhaleStudio 平台，A 基金成功实现了从“多工具拼装”向“一体化治理”的架构跨越，其核心收益主要体现在以下三个维度：</p><p><strong>首先，在管理架构上实现了全链路闭环与深度纳管。</strong><br/>平台将集成、编排、监控、告警与审计高度整合，彻底终结了系统割裂带来的重复维护。最显著的变化在于，Databricks 的作业体系与数据落地被完整纳入统一调度，使其不再是游离于主体系之外的“第二套平台”，实现了真正的跨云而不割裂。</p><p><strong>其次，在交付能力与资源利用率上达成了双重突破。</strong><br/>在工程化方面，标准化的流水线交付取代了低效的人工导入导出，配合审计与一键回滚机制，让业务变更既快又稳。在性能方面，分布式架构配合动态扩缩容，有效缓解了金融业务在峰值期的任务堆积，在确保 SLA 稳定的同时，大幅减少了低峰期的资源浪费。</p><p><strong>最后，在运维体验与长期演进中建立了坚实底座。</strong><br/>针对金融级治理需求，Schema 演进与版本控制能力显著降低了口径漂移风险，保障了 Lakehouse 的长期健康演进。而在日常运营中，告警降噪、过滤视图与时间记忆等智能化功能，将运维人员从干扰信号中解放出来，实现了异常问题的精准定位与快速响应。</p><p>归结起来，在多云与多工具并存的背景下，A 基金选择以 WhaleStudio 商业版作为统一的数据集成与调度中枢，将 AWS 上的批处理/CDC 与实时链路（MSK + Flink）以及 Databricks Lakehouse 的作业与数据落地纳入同一套编排、交付与运维治理体系。通过分布式架构与跨云统一编排，其能在任务规模从数百向数千增长的过程中保持 SLA 稳定，并以 CI/CD、告警降噪与 Lakehouse 治理能力，为基金业务提供更安全、更可追溯、更易演进的数据底座。</p>]]></description></item><item>    <title><![CDATA[CRM软件推荐：全球6款企业必备工具 晨曦钥匙扣 ]]></title>    <link>https://segmentfault.com/a/1190000047558472</link>    <guid>https://segmentfault.com/a/1190000047558472</guid>    <pubDate>2026-01-22 15:06:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>全流程一体化能力横评：6大CRM/ERP工具的「线索-回款」闭环之战</h2><p>在数字化转型中，<strong>「客户线索-销售报价-库存查询-订单履约-回款对账」的全流程一体化</strong>已成为企业降本增效的核心需求——零散的工具集成不仅会导致数据割裂，更会增加运营成本（据Gartner统计，企业因工具集成不畅导致的效率损失可达20%）。</p><p>本文将围绕<strong>5大核心维度</strong>，对<strong>超兔一体云、Salesforce、SugarCRM、Freshsales、金蝶云星辰、管家婆</strong>的一体化能力展开深度对比，结合<strong>功能细节、行业适配性、原生 vs 集成成本</strong>三大视角，为企业选择提供参考。</p><h3>一、核心维度与评估指标定义</h3><p>先明确每个维度的<strong>关键评估指标</strong>（均为企业实际场景中的痛点）：</p><table><thead><tr><th>维度</th><th>关键评估指标</th></tr></thead><tbody><tr><td>客户线索跟踪</td><td>渠道整合能力、AI驱动的线索培育、定制化灵活性、合规性</td></tr><tr><td>销售报价生成</td><td>模板化效率、自动化规则（如定价/库存校验）、审批流程、状态跟踪</td></tr><tr><td>库存实时查询</td><td>原生功能覆盖、多仓支持、智能预警、与销售/采购的联动</td></tr><tr><td>订单履约</td><td>业财联动深度、流程自动化（如采购/生产对接）、跨境/多场景适配</td></tr><tr><td>回款对账一体化</td><td>应收触发自动化、回款跟踪与提醒、对账自动化、业财数据一致性</td></tr></tbody></table><h3>二、6大工具核心能力横评</h3><h4>（一）客户线索跟踪：从「获客」到「转化」的效率之战</h4><p>客户线索是业务的起点，<strong>渠道整合深度</strong>与<strong>AI驱动的精准培育</strong>是核心竞争力。</p><table><thead><tr><th>工具</th><th>渠道整合</th><th>AI能力</th><th>定制化/合规性</th><th>核心优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>原生支持百度/抖音/微信/工商搜客等10+渠道，自动抓取表单数据</td><td>线索自动分配（手机号/IP归属地）、生命周期管理（客池分类：需求培养→成功）</td><td>支持线索字段自定义，无合规风险</td><td>需全渠道线索统一管理的商贸/制造企业</td></tr><tr><td><strong>Salesforce</strong></td><td>全渠道整合（邮件/社交/线下），需配置Einstein Connector</td><td>Einstein AI分析客户行为（如浏览轨迹），推送个性化内容，线索评分预测转化</td><td>生态丰富，但定制需懂Apex语言</td><td>大型跨国企业，需AI驱动精准营销</td></tr><tr><td><strong>SugarCRM</strong></td><td>支持API对接多渠道，需二次开发</td><td>无原生AI，依赖第三方插件</td><td>代码级定制（适合金融/医药合规）</td><td>对数据隐私/流程合规要求高的行业</td></tr><tr><td><strong>Freshsales</strong></td><td>整合电话/邮件/社交，自动补全客户信息</td><td>AI线索评分（基于互动频率）、预测分析</td><td>模板化配置，灵活度一般</td><td>侧重线索快速跟进的SaaS/互联网企业</td></tr><tr><td><strong>金蝶云星辰</strong></td><td>支持客户分层，结合AI模型驱动精准营销（如美妆复购率提升27%）</td><td>AI客户画像分析，推荐营销活动</td><td>适合零售/电商的客户分层管理</td><td>成长型企业，需从线索到复购的闭环</td></tr><tr><td><strong>管家婆</strong></td><td>内置客户档案，记录需求/服务历史</td><td>无原生AI，依赖人工跟进</td><td>适合中小微企业（如家政/维修）</td><td>服务行业，需记录客户历史偏好</td></tr></tbody></table><h5>关键结论：</h5><ul><li>超兔的<strong>原生多渠道整合</strong>无需额外配置，是中小微企业的「省心之选」；</li><li>Salesforce的<strong>Einstein AI</strong>是大型企业的「精准营销利器」；</li><li>SugarCRM的<strong>代码级定制</strong>适合金融/医药等强合规行业。</li></ul><h4>（二）销售报价生成：从「精准」到「高效」的平衡</h4><p>销售报价的核心是「准确」（库存/定价）<strong>与</strong>「快速」（模板+审批）。</p><table><thead><tr><th>工具</th><th>模板化能力</th><th>自动化规则</th><th>审批与状态跟踪</th><th>核心优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>支持个性化模板（按产品/客户类型），可自定义字段/计算方式</td><td>针对询价客户，发起报价，可自动生成报价单</td><td>支持多级审批，记录审批历史</td><td>需精准报价的生产/商贸企业</td></tr><tr><td><strong>Salesforce</strong></td><td>内置CPQ（配置-定价-报价）工具，自动化生成报价单</td><td>基于产品规则（如捆绑销售）、客户合同定价</td><td>与Sales Cloud联动，状态实时更新</td><td>需复杂定价规则的大型企业（如软件）</td></tr><tr><td><strong>Freshsales</strong></td><td>模板化报价，支持一键发送邮件</td><td>无库存校验，需集成ERP</td><td>跟踪报价打开/点击状态</td><td>侧重报价快速触达的中小企业</td></tr><tr><td><strong>金蝶云星辰</strong></td><td>多端移动办公，切换客户时价格自动更新（客户等级差异化定价）</td><td>支持批量报价，自动关联客户档案</td><td>报价与财务联动，精准度高</td><td>零售/电商，需客户分层定价</td></tr><tr><td><strong>管家婆</strong></td><td>15种行业模板（便利店/微商），快速配置</td><td>自定义报价规则（如折扣）</td><td>无审批流程，适合简单场景</td><td>小微企业，需快速生成报价</td></tr></tbody></table><h5>关键结论：</h5><ul><li>超兔的订单价格校验、出库校验，确保业务顺利执行；</li><li>Salesforce的<strong>CPQ功能</strong>适合复杂产品组合（如软件license+服务）；</li><li>金蝶的<strong>客户等级定价</strong>是零售/电商的「差异化竞争工具」。</li></ul><h4>（三）库存实时查询：从「报价」到「发货」的准确性保障</h4><p>库存是销售的「底气」，<strong>原生功能覆盖</strong>与<strong>实时联动</strong>是避免「超卖/缺货」的关键。</p><table><thead><tr><th>工具</th><th>原生功能</th><th>多仓支持</th><th>智能预警与联动</th><th>核心优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>原生进销存模块，支持多价格策略/成本算法</td><td>多仓管理，实时同步库存数量/位置</td><td>库存上下限预警，自动生成采购计划</td><td>需多仓联动的生产/商贸企业</td></tr><tr><td><strong>Salesforce</strong></td><td>无原生库存，需集成ERP（如SAP）</td><td>依赖ERP的多仓能力</td><td>无原生预警，需ERP配置</td><td>大型企业，已有成熟ERP系统</td></tr><tr><td><strong>Freshsales</strong></td><td>无原生库存，需集成Google Workspace/ERP</td><td>无</td><td>无</td><td>无需库存管理的轻量级销售团队</td></tr><tr><td><strong>金蝶云星辰</strong></td><td>原生进销存模块，采购/销售/库存全链路同步</td><td>多仓管理，支持跨仓调拨</td><td>智能库存预警（避免缺货/积压）</td><td>成长型零售/电商，需进销存联动</td></tr><tr><td><strong>管家婆</strong></td><td>原生多仓管理，支持离线操作（数据实时同步）</td><td>多仓/多计量单位管理</td><td>保质期预警，库存数量预警</td><td>小微企业（如便利店），需简单库存</td></tr></tbody></table><h5>关键结论：</h5><ul><li>超兔的<strong>原生库存+智能采购</strong>是「库存-销售」联动的「最优解」；</li><li>金蝶的<strong>进销存全链路同步</strong>适合成长型企业；</li><li>Salesforce/Freshsales需额外集成ERP，成本高。</li></ul><h4>（四）订单履约：从「下单」到「交付」的流程自动化</h4><p>订单履约的核心是「业财联动」<strong>与</strong>「流程标准化」，避免「订单漏处理」或「财务对账难」。</p><table><thead><tr><th>工具</th><th>业财联动</th><th>流程自动化</th><th>跨境/特殊场景</th><th>核心优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>原生业财架构，订单生成后自动触发应收/财务凭证</td><td>订单工作流（锁库→采购→生产→物流）、生产对接MES系统</td><td>通过客户信用管控来实现跨境订单风控（减少坏账）</td><td>生产制造企业，需从订单到生产的闭环</td></tr><tr><td><strong>Salesforce</strong></td><td>需集成FinancialForce等财务工具</td><td>订单流程自动化（如审批→发货），需配置Flow</td><td>支持跨国订单，但需集成物流工具</td><td>大型企业，已有财务系统集成</td></tr><tr><td><strong>金蝶云星辰</strong></td><td>业财一体化，订单处理效率提升3倍（自动生成凭证）</td><td>自动匹配采购需求，跨境订单风控</td><td>支持跨境电商，减少坏账损失</td><td>零售/跨境电商，需业财快速联动</td></tr><tr><td><strong>管家婆</strong></td><td>进销存数据自动生成财务报表</td><td>流程标准化（如采购→入库→销售）</td><td>适合中小微企业，快速部署</td><td>便利店/微商，需简单订单流程</td></tr></tbody></table><h5>关键结论：</h5><ul><li>超兔的<strong>原生业财联动+生产对接</strong>是制造企业的「刚需」；</li><li>金蝶的<strong>业财自动凭证</strong>是零售/电商的「效率利器」；</li><li>Salesforce需集成财务系统，适合已有IT架构的企业。</li></ul><h4>（五）回款对账一体化：从「应收」到「结算」的风险控制</h4><p>回款是企业的「现金流生命线」，<strong>应收自动化</strong>与<strong>对账一致性</strong>是核心。</p><table><thead><tr><th>工具</th><th>应收触发</th><th>回款跟踪</th><th>对账自动化</th><th>核心优势场景</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>签约/开票/发货自动触发应收，支持一票对多单/一笔对多单</td><td>账期管理、客户信用度控制（规避发货风险）、逾期提醒</td><td>自动匹配应收/回款，生成对账报告</td><td>需严格控制账期的制造/商贸企业</td></tr><tr><td><strong>Salesforce</strong></td><td>需集成财务工具（如QuickBooks）</td><td>回款跟踪（需手动录入），无自动提醒</td><td>需手动匹配应收/回款</td><td>大型企业，已有财务系统</td></tr><tr><td><strong>金蝶云星辰</strong></td><td>业务数据自动生成财务凭证，票/税/档一体</td><td>回款状态实时追踪，缩短对账周期</td><td>自动生成对账报告，支持批量结算</td><td>零售/电商，需票税一体化</td></tr><tr><td><strong>管家婆</strong></td><td>进销存数据自动生成应收，支持简单账期管理</td><td>手动录入回款，生成回款报表</td><td>自动匹配应收/回款，简单对账</td><td>小微企业，需快速对账</td></tr></tbody></table><h5>关键结论：</h5><ul><li>超兔的<strong>应收自动触发+信用控制</strong>是「风险规避」的核心；</li><li>金蝶的<strong>票税档一体</strong>适合需合规报税的企业；</li><li>管家婆的<strong>简单对账</strong>适合中小微企业。</li></ul><h3>三、一体化能力总结：从「零散工具」到「原生闭环」的选择逻辑</h3><h4>（一）一体化流程对比：超兔的「原生闭环」vs 其他工具的「集成依赖」</h4><p>超兔一体云的<strong>原生一体化流程</strong>：</p><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/73f8632e418647eab6ae807ccd735d04~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5pmo5pumNjA5NQ==:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMjc5MDM0MzA3NjM3ODUifQ%3D%3D&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769150570&amp;x-orig-sign=pfeqjknEhKsfxwc%2FkomkNaasK7U%3D" alt="" title=""/></p><p>其他工具（如Salesforce）的<strong>集成依赖流程</strong>：</p><p><img referrerpolicy="no-referrer" src="https://p0-xtjj-private.juejin.cn/tos-cn-i-73owjymdk6/3700e61944564cc3a5772a862965cf62~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAg5pmo5pumNjA5NQ==:q75.awebp?policy=eyJ2bSI6MywidWlkIjoiMjc5MDM0MzA3NjM3ODUifQ%3D%3D&amp;rk3s=e9ecf3d6&amp;x-orig-authkey=f32326d3454f2ac7e96d3d06cdbb035152127018&amp;x-orig-expires=1769150570&amp;x-orig-sign=kzO680sMtUx7MvYjvJ4%2FrIhLIpM%3D" alt="" title="" loading="lazy"/></p><h3>四、适用场景推荐</h3><table><thead><tr><th>企业类型/需求</th><th>推荐工具</th><th>核心原因</th></tr></thead><tbody><tr><td>中小微企业，需全流程闭环</td><td><strong>超兔一体云</strong></td><td>原生一体化，无需集成，成本低</td></tr><tr><td>大型跨国企业，需AI与生态</td><td><strong>Salesforce</strong></td><td>Einstein AI+丰富生态，适合全球化</td></tr><tr><td>成长型零售/电商，需业财融合</td><td><strong>金蝶云星辰</strong></td><td>业财自动凭证+票税一体，效率高</td></tr><tr><td>小微企业（如便利店/微商）</td><td><strong>管家婆</strong></td><td>功能实用，快速部署，成本低</td></tr><tr><td>金融/医药，需合规定制</td><td><strong>SugarCRM</strong></td><td>代码级定制，符合数据隐私要求</td></tr><tr><td>SaaS/互联网，需线索快速跟进</td><td><strong>Freshsales</strong></td><td>AI线索评分+多渠道互动，效率高</td></tr></tbody></table><h3>五、结论：一体化的本质是「数据打通」与「流程自动化」</h3><p>从6大工具的对比来看，<strong>原生一体化</strong>（如超兔、金蝶）是中小微企业的「最优解」——无需投入集成成本，就能实现从线索到回款的全流程数据打通；而大型企业（如Salesforce）则需根据已有IT架构选择「生态整合」方案。</p><p>最终，企业选择工具的核心逻辑是：<strong>以业务流程为中心，优先选择「原生覆盖核心环节」的工具</strong>，避免「为了集成而集成」的无效投入。</p><p>超兔一体云作为「原生一体化」的代表，其核心优势在于<strong>将「线索-报价-库存-订单-回款」的每一步都设计为「数据自动流动」</strong>，真正实现了「业务驱动数据，数据反哺业务」的闭环。这也是其能帮助企业<strong>降低30%运营成本、提升25%回款效率</strong>的关键。</p>]]></description></item><item>    <title><![CDATA[筑业云资料 “流水段视图” 功能：资料管理的高效秘籍 聪明的拐杖 ]]></title>    <link>https://segmentfault.com/a/1190000047558477</link>    <guid>https://segmentfault.com/a/1190000047558477</guid>    <pubDate>2026-01-22 15:05:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工程资料管理工作中，众多用户渴望有一个功能，能集中查看所有已创建部位，并清晰知晓各部位所创建的表格，以便于统一查看与汇总管理。筑业云资料的 “流水段视图” 功能恰好满足了这一需求，成为资料管理的得力助手。<br/>以流水段为维度的便捷整合<br/>“流水段视图” 功能以施工流水段为维度，将同一部位的所有表格巧妙列在一起。这种整合方式极大地方便了资料的查看、复制、编辑及管理。例如在大型建筑项目中，不同楼层的相同部位可能存在相似的资料表格，通过该功能，可快速定位并查看这些部位及其相关表格，无需在海量资料中逐一查找，节省大量时间与精力。<br/>清晰多样的操作流程<br/>功能入口与展示：在软件工具栏找到并点击 “流水段视图” 功能入口，进入后，所有单位工程下的所有表格会以目录树的形式清晰展示。这种直观的展示方式，让用户对整个工程资料架构一目了然。<br/>新建操作：右键点击可进行多种新建操作，如新建单位工程、新建表格、新建文件夹等。新建文件夹功能尤为实用，例如创建 “楼层”“构件” 等文件夹，输入楼层名称如 “一层”，含一层的部位就会自动归到该文件夹中，无需手动移动，使资料分类更加有序。<br/>复制操作：当制作标准层资料时，复制功能尽显便捷。右键复制并输入如 “第二层”，即可快速建立标准层的表格。对于文件夹，同样可右键选择 “复制文件夹”，输入新名称如 “二层”，点击 “确定” 后，该部位下面的所有表格都会复制过来，且部位名称按新输入内容自动填充，大大减少重复建表工作。<br/>重命名与删除操作：右键 “重命名” 时，勾选相应选项可选择是否同步修改文件夹下所有表格的名称，确保资料一致性。而右键 “删除” 仅删除文件夹，文件夹里的表格会被移出，不会被删除，保障资料安全。<br/>显著提升资料管理效率<br/>“流水段视图” 功能通过 “创建一次，多次复制” 的智能化方式，将资料员从大量重复性建表工作中解放出来。它不仅规范了资料管理，使资料分类清晰、易于查找，还显著提升了做表效率。在工程项目周期紧张的情况下，高效的资料管理能为项目推进节省时间，确保各环节顺利进行。<br/>筑业云资料的 “流水段视图” 功能，以其独特的整合方式与丰富实用的操作，成为工程资料管理不可或缺的功能，助力资料员轻松应对复杂的资料管理任务，提升整体工作质量与效率。</p>]]></description></item><item>    <title><![CDATA[用自然语言玩转 AI 原生数据库 —— seekdb MCP Server 老纪的技术唠嗑局 ]]></title>    <link>https://segmentfault.com/a/1190000047558484</link>    <guid>https://segmentfault.com/a/1190000047558484</guid>    <pubDate>2026-01-22 15:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>引言</strong></h2><p>想象一下：你只需要用自然语言描述你的需求，AI 就能自动帮你完成数据库操作 —— 创建文档集合、插入数据、执行复杂查询，甚至构建一个完整的知识库应用。这不是未来，而是现在就能实现的能力。</p><p><strong>seekdb MCP Server</strong> 就是实现这一愿景的桥梁。它基于 Anthropic 提出的 <strong>MCP（Model Context Protocol）协议</strong>，让 AI 助手能够直接与 seekdb 数据库交互，将 "自然语言" 转化为 "数据库操作"。</p><p>本文将带你上手 seekdb MCP Server，并通过一个实战案例 —— <strong>通过自然语言构建 AI 应用</strong>，让你亲身体验 AI 原生数据库的魅力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558486" alt="" title=""/></p><p>欢迎大家关注，在这里，我们会持续为大家更新与 #数据库、#AI 相关的技术内容！</p><h2><strong>什么是 seekdb MCP Server？</strong></h2><p><strong>seekdb</strong> 是一款 AI 原生搜索数据库，在统一架构下融合了关系数据、向量数据、全文索引、JSON 和 GIS 能力，支持混合检索和库内 AI 工作流。</p><p><strong>MCP Server</strong> 则是连接 AI 工具与数据库的"适配器"。通过 MCP 协议，Cursor、Claude Code、Cline 等 AI 工具可以直接访问和操作 seekdb 数据库。</p><h3><strong>核心能力一览</strong></h3><table><thead><tr><th align="left"><strong>能力分类</strong></th><th align="left"><strong>工具列表</strong></th><th align="left"><strong>功能说明</strong></th></tr></thead><tbody><tr><td align="left"><strong>向量集合管理</strong></td><td align="left"><code>create_collection</code>、<code>query_collection</code>、<code>add_data_to_collection</code> 等</td><td align="left">创建向量集合、语义搜索、文档管理</td></tr><tr><td align="left"><strong>高级搜索</strong></td><td align="left"><code>full_text_search</code>、<code>hybrid_search</code></td><td align="left">全文搜索、混合搜索（BM25 + 向量）</td></tr><tr><td align="left"><strong>AI 函数</strong></td><td align="left"><code>ai_complete</code>、<code>ai_rerank</code>、<code>create_ai_model</code> 等</td><td align="left">调用 LLM 生成文本、重排序搜索结果</td></tr><tr><td align="left"><strong>AI 记忆系统</strong></td><td align="left"><code>seekdb_memory_query</code>、<code>seekdb_memory_insert</code> 等</td><td align="left">跨会话持久化记忆，让 AI "记住"你</td></tr><tr><td align="left"><strong>数据导入导出</strong></td><td align="left"><code>import_csv_file_to_seekdb</code>、<code>export_csv_file_from_seekdb</code></td><td align="left">CSV 文件与数据库表/向量集合互转</td></tr></tbody></table><h2><strong>安装 seekdb 数据库</strong></h2><p>在使用 seekdb MCP Server 之前，你需要先准备好 seekdb 数据库。seekdb 提供两种部署模式：</p><h3><strong>模式一：嵌入式模式（零配置，仅限 Linux）</strong></h3><p><strong>嵌入式模式无需单独安装 seekdb 数据库</strong>！seekdb MCP Server 启动时会自动初始化一个本地嵌入式数据库，开箱即用。</p><p>适用场景：个人学习、快速原型开发、边缘设备运行。</p><p>⚠️ <strong>提示</strong>：<br/><strong>macOS 和 Windows 用户</strong>需要使用「客户端 / 服务器模式」，需要先部署 seekdb 数据库（推荐 Docker 方式），然后配置连接参数。</p><h3><strong>模式二：客户端/服务器模式（生产推荐）</strong></h3><p>如果你需要在测试或生产环境部署 seekdb，可以选择以下方式：</p><h5><strong>方式 1：使用 yum 安装（RPM 系统）</strong></h5><pre><code class="plain"># 1. 添加 seekdb 镜像源
sudo yum-config-manager --add-repo https://mirrors.aliyun.com/oceanbase/OceanBase.repo

# 2. 安装 seekdb 和客户端
sudo yum install seekdb obclient

# 3. 启动 seekdb
sudo systemctl start seekdb

# 4. 检查启动状态（状态为 "Service is ready" 表示启动成功）
sudo systemctl status seekdb

# 5. 连接测试
mysql -h127.0.0.1 -uroot -P2881 -A oceanbase</code></pre><h5><strong>方式 2：使用 Docker（最快捷）</strong></h5><pre><code class="plain"># 一行命令启动 seekdb
sudo docker run -d -p 2881:2881 oceanbase/seekdb

# 如果拉取失败，可使用备用镜像源：
# sudo docker run -d -p 2881:2881 quay.io/oceanbase/seekdb
# sudo docker run -d -p 2881:2881 ghcr.io/oceanbase/seekdb</code></pre><p><strong>系统要求</strong>：</p><ul><li>CPU：最低 1 核</li><li>内存：最低 2 GB 可用内存</li><li>支持的操作系统：CentOS 7/8、Ubuntu 20+、Debian 9+、Anolis OS 8、麒麟 V10 等</li></ul><p>更多部署方式请参考 <strong>seekdb 部署文档</strong><sup><strong>[1]</strong></sup>。</p><hr/><h2><strong>安装 seekdb MCP Server</strong></h2><h3><strong>安装 uv 包管理器</strong></h3><pre><code class="plain"># 安装 uv 包管理器
curl -LsSf https://astral.sh/uv/install.sh | sh</code></pre><h2><strong>配置 AI 工具连接</strong></h2><h3><strong>Stdio 模式</strong></h3><p>以 Cursor 为例在 Cursor 中，打开设置 → Tools &amp; MCP → New MCP Server，根据你的操作系统选择配置方式：</p><h4><strong>Linux 用户（嵌入式模式）</strong></h4><pre><code class="plain">{
  "mcpServers": {
    "seekdb": {
      "command": "uvx",
      "args": ["seekdb-mcp-server"]
    }
  }
}</code></pre><p>就这么简单！<strong>嵌入式模式无需任何配置</strong>，服务器启动时会自动初始化一个本地 seekdb 数据库。</p><h4><strong>macOS / Windows 用户（服务器模式）</strong></h4><p>macOS 和 Windows 不支持嵌入式模式，需要先部署 seekdb 数据库（推荐使用 Docker），然后配置连接参数：</p><pre><code class="plain">{
  "mcpServers": {
    "seekdb": {
      "command": "uvx",
      "args": ["seekdb-mcp-server"],
      "env": {
        "SEEKDB_HOST": "127.0.0.1",
        "SEEKDB_PORT": "2881",
        "SEEKDB_USER": "",
        "SEEKDB_PASSWORD": "",
        "SEEKDB_DATABASE": "test"
      }
    }
  }
}</code></pre><p><strong>参数说明</strong>：</p><table><thead><tr><th align="left"><strong>参数</strong></th><th align="left"><strong>说明</strong></th><th align="left"><strong>默认值</strong></th></tr></thead><tbody><tr><td align="left"><code>SEEKDB_HOST</code></td><td align="left">seekdb 服务器地址</td><td align="left"><code>127.0.0.1</code></td></tr><tr><td align="left"><code>SEEKDB_PORT</code></td><td align="left">seekdb 服务端口</td><td align="left"><code>2881</code></td></tr><tr><td align="left"><code>SEEKDB_USER</code></td><td align="left">数据库用户名</td><td align="left">无</td></tr><tr><td align="left"><code>SEEKDB_PASSWORD</code></td><td align="left">数据库密码</td><td align="left">无</td></tr><tr><td align="left"><code>SEEKDB_DATABASE</code></td><td align="left">数据库名称</td><td align="left">无</td></tr></tbody></table><h3><strong>SSE 模式</strong></h3><h4><strong>Linux 用户（嵌入式模式）</strong></h4><p>直接启动 SSE 服务器：</p><pre><code class="plain">uvx seekdb-mcp-server --transport sse --port 6000</code></pre><h4><strong>macOS / Windows 用户（服务器模式）</strong></h4><p>先配置环境变量，再启动服务器：</p><pre><code class="plain"># 配置 seekdb 连接信息
export SEEKDB_HOST=127.0.0.1
export SEEKDB_PORT=2881
export SEEKDB_USER=
export SEEKDB_PASSWORD=
export SEEKDB_DATABASE=test

# 启动 SSE 服务器
uvx seekdb-mcp-server --transport sse --port 6000</code></pre><p>然后在客户端配置：</p><pre><code class="plain">{
  "sse-seekdb": {
    "type": "sse",
    "url": "http://127.0.0.1:6000/sse"
  }
}</code></pre><h2><strong>实战案例：用 AI 对话构建个人笔记知识库</strong></h2><p>现在让我们通过一个完整的实战案例，体验 seekdb MCP Server 的强大能力。我们将构建一个<strong>个人笔记知识库</strong>，实现：</p><ul><li>✅ 用自然语言存储笔记</li><li>✅ 语义搜索相关内容</li><li>✅ 混合搜索精准定位</li><li>✅ AI 智能问答</li></ul><h3><strong>场景设定</strong></h3><p>假设你是一名技术工作者，平时会记录各种学习笔记、工作心得。现在你希望用 AI 帮你管理这些笔记，并能通过自然语言快速找到需要的内容。</p><h3><strong>第一步：创建笔记集合</strong></h3><p>在 AI 工具（如 Cursor）的对话框中输入：</p><pre><code class="plain">创建一个名为 "my_notes" 的知识库集合</code></pre><p><strong>AI 响应示例：</strong></p><p>我已成功创建名为 <strong>"my_notes"</strong> 的集合：</p><ul><li><strong>维度</strong>: 384（向量大小）</li><li><strong>距离度量</strong>: L2（欧几里得距离）</li></ul><p>就这样，一个支持语义搜索的向量集合就创建好了！</p><h3><strong>第二步：添加笔记内容</strong></h3><p>继续输入：</p><pre><code class="plain">向 my_notes 中添加以下笔记：

1. "seekdb 支持向量搜索和全文搜索的混合检索，可以同时利用关键词匹配和语义理解"
2. "MCP 协议是 Anthropic 在 2024 年 11 月提出的开源协议，让大模型可以与外部工具交互"
3. "Python 的 asyncio 模块提供了异步编程能力，适合 I/O 密集型任务"
4. "Docker 容器化技术可以实现应用的快速部署和环境隔离"
5. "RAG（检索增强生成）是目前最流行的 AI 应用架构，结合检索和生成能力"</code></pre><p><strong>AI 响应示例：</strong></p><p>完成！已成功向 <strong>"my_notes"</strong> 集合添加 5 条笔记：</p><table><thead><tr><th align="left"><strong>ID</strong></th><th align="left"><strong>内容</strong></th></tr></thead><tbody><tr><td align="left">doc1</td><td align="left">seekdb 支持向量搜索和全文搜索的混合检索...</td></tr><tr><td align="left">doc2</td><td align="left">MCP 协议是 Anthropic 在 2024 年 11 月提出的...</td></tr><tr><td align="left">doc3</td><td align="left">Python 的 asyncio 模块提供了异步编程能力...</td></tr><tr><td align="left">doc4</td><td align="left">Docker 容器化技术可以实现应用的快速部署...</td></tr><tr><td align="left">doc5</td><td align="left">RAG（检索增强生成）是目前最流行的 AI 应用架构...</td></tr></tbody></table><h3><strong>第三步：语义搜索</strong></h3><p>现在，让我们用自然语言搜索笔记：</p><pre><code class="plain">在 my_notes 中搜索与 "AI 应用开发" 相关的笔记</code></pre><p><strong>AI 响应示例：</strong></p><p>以下是与 <strong>"AI 应用开发"</strong> 相关的笔记，按相关性排序：</p><table><thead><tr><th align="left"><strong>排名</strong></th><th align="left"><strong>内容</strong></th><th align="left"><strong>相关度</strong></th></tr></thead><tbody><tr><td align="left">1</td><td align="left">RAG（检索增强生成）是目前最流行的 AI 应用架构...</td><td align="left">最高</td></tr><tr><td align="left">2</td><td align="left">MCP 协议是 Anthropic 在 2024 年 11 月提出的...</td><td align="left">高</td></tr><tr><td align="left">3</td><td align="left">seekdb 支持向量搜索和全文搜索的混合检索...</td><td align="left">中</td></tr></tbody></table><p>注意：我们搜索的是 "AI 应用开发"，但笔记中并没有这个确切的词。seekdb 的向量搜索能够理解<strong>语义相似性</strong>，自动找到相关内容！</p><h3><strong>第四步：混合搜索</strong></h3><p>混合搜索结合了<strong>关键词匹配</strong>和<strong>语义理解</strong>，效果更精准：</p><pre><code class="plain">对 my_notes 执行混合搜索：
- 全文关键词："Python"
- 语义查询："异步编程和并发处理"</code></pre><p><strong>AI 响应示例：</strong></p><table><thead><tr><th align="left"><strong>排名</strong></th><th align="left"><strong>内容</strong></th></tr></thead><tbody><tr><td align="left">1</td><td align="left">Python 的 asyncio 模块提供了异步编程能力，适合 I/O 密集型任务</td></tr><tr><td align="left">2</td><td align="left">Docker 容器化技术可以实现应用的快速部署和环境隔离</td></tr></tbody></table><p>混合搜索使用 <strong>RRF（倒数排名融合）</strong> 算法，综合关键词匹配（BM25）和向量相似度得分，排名结果更准确。</p><h3><strong>第五步：让 AI 记住你的偏好</strong></h3><p>seekdb MCP Server 还提供了 <strong>AI 记忆系统</strong>，让 AI 能够跨会话记住你的信息：</p><pre><code class="plain">记住我是一名 Python 开发者，主要使用 seekdb 做向量搜索应用</code></pre><p><strong>AI 响应示例：</strong></p><p>已记录！我会记住：</p><ul><li>你是一名 Python 开发者</li><li>主要使用 seekdb 做向量搜索应用</li></ul><p>下次对话时，你可以直接问：</p><pre><code class="plain">根据我的技术背景，推荐一些学习资源</code></pre><p>AI 会自动检索你的偏好，给出个性化推荐！</p><h2><strong>功能一览</strong></h2><h3><strong>向量集合管理</strong></h3><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>create_collection</code></td><td align="left">创建向量集合</td></tr><tr><td align="left"><code>list_collections</code></td><td align="left">列出所有集合</td></tr><tr><td align="left"><code>has_collection</code></td><td align="left">检查集合是否存在</td></tr><tr><td align="left"><code>peek_collection</code></td><td align="left">预览集合中的文档</td></tr><tr><td align="left"><code>add_data_to_collection</code></td><td align="left">添加文档（自动生成向量）</td></tr><tr><td align="left"><code>update_collection</code></td><td align="left">更新文档</td></tr><tr><td align="left"><code>delete_documents</code></td><td align="left">删除文档</td></tr><tr><td align="left"><code>query_collection</code></td><td align="left">向量相似性搜索</td></tr><tr><td align="left"><code>delete_collection</code></td><td align="left">删除集合</td></tr></tbody></table><h3><strong>高级搜索</strong></h3><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>full_text_search</code></td><td align="left">全文搜索（基于关键词）</td></tr><tr><td align="left"><code>hybrid_search</code></td><td align="left">混合搜索（结合全文和向量搜索）</td></tr></tbody></table><h3><strong>AI 模型工具</strong></h3><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>create_ai_model</code></td><td align="left">注册 AI 模型（嵌入、文本生成或重排序）</td></tr><tr><td align="left"><code>create_ai_model_endpoint</code></td><td align="left">创建将模型连接到 API 服务的端点</td></tr><tr><td align="left"><code>drop_ai_model</code></td><td align="left">移除已注册的 AI 模型</td></tr><tr><td align="left"><code>drop_ai_model_endpoint</code></td><td align="left">移除 AI 模型端点</td></tr><tr><td align="left"><code>ai_complete</code></td><td align="left">调用 LLM 进行文本生成</td></tr><tr><td align="left"><code>ai_rerank</code></td><td align="left">使用 AI 模型按相关性重排文档</td></tr><tr><td align="left"><code>get_registered_ai_models</code></td><td align="left">列出所有已注册的 AI 模型</td></tr><tr><td align="left"><code>get_ai_model_endpoints</code></td><td align="left">列出所有 AI 模型端点</td></tr></tbody></table><h3><strong>AI 记忆系统</strong></h3><p>seekdb MCP Server 提供了强大的 AI 记忆功能，让 AI 助手能够跨会话记住信息：</p><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>seekdb_memory_query</code></td><td align="left">语义搜索记忆</td></tr><tr><td align="left"><code>seekdb_memory_insert</code></td><td align="left">存储新记忆</td></tr><tr><td align="left"><code>seekdb_memory_update</code></td><td align="left">更新记忆</td></tr><tr><td align="left"><code>seekdb_memory_delete</code></td><td align="left">删除记忆</td></tr></tbody></table><p><strong>使用场景</strong>：</p><ul><li>AI 记住你的技术栈偏好（如 "我习惯使用 Python"）</li><li>AI 记住项目信息（如 "这个项目使用 FastAPI"）</li><li>AI 记住个人偏好（如 "我喜欢简洁的代码风格"）</li></ul><h3><strong>数据导入导出</strong></h3><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>import_csv_file_to_seekdb</code></td><td align="left">导入 CSV 文件</td></tr><tr><td align="left"><code>export_csv_file_from_seekdb</code></td><td align="left">导出数据到 CSV</td></tr></tbody></table><h3><strong>SQL 操作</strong></h3><table><thead><tr><th align="left"><strong>工具</strong></th><th align="left"><strong>功能</strong></th></tr></thead><tbody><tr><td align="left"><code>execute_sql</code></td><td align="left">执行 SQL 查询</td></tr><tr><td align="left"><code>get_current_time</code></td><td align="left">获取数据库当前时间</td></tr></tbody></table><h2><strong>更多工具探索</strong></h2><p>除了本文介绍的功能，seekdb MCP Server 还支持：</p><ul><li><p>AI 函数调用</p><ul><li>使用 AI 模型分析这段文本的情感倾向："今天天气真好，心情愉悦！"</li></ul></li><li><p>CSV 数据导入</p><ul><li>将 /path/to/products.csv 导入为向量集合，使用第 2 列（产品描述）作为文档</li></ul></li></ul><h2><strong>常见问题</strong></h2><h4><strong>Q: 需要安装 seekdb 吗？</strong></h4><p><strong>A:</strong> 不需要！seekdb MCP Server 使用嵌入式模式，seekdb 已经包含在内，无需单独安装。</p><h4><strong>Q: 数据存储在哪里？</strong></h4><p><strong>A:</strong> 数据存储在本地文件系统中，默认在当前用户家目录下。你的数据完全在本地，不会上传到任何云端。</p><h4><strong>Q: 支持哪些操作系统？</strong></h4><p><strong>A:</strong> 目前支持 Linux（glibc &gt;= 2.28），支持 x86_64 和 aarch64 架构。</p><h4><strong>Q: 如何升级？</strong></h4><p><strong>A:</strong> 使用 <code>uvx</code> 时会自动使用最新版本。</p><h2><strong>总结</strong></h2><p><strong>seekdb MCP Server</strong> 让数据库操作变得前所未有的简单：</p><table><thead><tr><th align="left"><strong>传统方式</strong></th><th align="left"><strong>MCP 方式</strong></th></tr></thead><tbody><tr><td align="left">学习 SQL 语法</td><td align="left">用自然语言描述需求</td></tr><tr><td align="left">编写代码调用 API</td><td align="left">AI 自动执行操作</td></tr><tr><td align="left">手动管理向量嵌入</td><td align="left">自动生成和索引</td></tr><tr><td align="left">分别处理搜索逻辑</td><td align="left">一句话混合搜索</td></tr></tbody></table><p>无论你是想快速构建 RAG 应用，还是想让 AI 助手拥有"长期记忆"，seekdb MCP Server 都是你的最佳选择。</p><p><strong>开始你的 AI 原生数据库之旅吧！</strong> 🚀</p><hr/><p><strong>参考资料</strong></p><p>[1] seekdb 部署文档: <em><a href="https://link.segmentfault.com/?enc=CPVDFyVE2PCoemWFic0UkA%3D%3D.2xIJZGuFps2tkS1ywY%2B0HEO9Rr6p8Sm5calKrNV1PGFRUXfhnwNk93AN28eQzy%2B0" rel="nofollow" target="_blank">https://www.oceanbase.ai/docs/deploy-overview/</a></em></p>]]></description></item><item>    <title><![CDATA[2026AI 元年：智能体技术落地与产业应用变革白皮书 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047558491</link>    <guid>https://segmentfault.com/a/1190000047558491</guid>    <pubDate>2026-01-22 15:03:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>摘要</h3><p>2026 年被定义为​<strong>AI 智能体技术规模化落地元年</strong>​，依托大模型技术的持续迭代、工具生态的完善以及行业场景的深度适配，智能体从技术概念走向商业落地，完成从 “文本生成工具” 到 “自主任务执行系统” 的核心转变。本文系统阐述 2026AI 元年的技术基础、产业特征、核心应用领域，分析智能体技术对生产生活、产业结构的重构价值，梳理技术落地的核心挑战与解决路径，为产业从业者、研究人员及普通用户提供全面的技术与应用参考框架。​<strong>关键词</strong>​：2026AI 元年；智能体；大模型；技术落地；产业变革；人机协同</p><h3>一、2026 成为 AI 智能体元年的核心技术与产业基础</h3><h4>1.1 大模型技术的成熟：智能体的核心能力底座</h4><p>2023-2025 年大模型完成从 “通用理解” 到 “逻辑推理 + 多模态交互” 的能力突破，为智能体落地奠定核心基础：</p><ol><li>​<strong>推理能力升级</strong>​：主流大模型（GPT-4o、文心一言 4.0、通义千问 3.0 等）的因果推理、步骤拆解能力较 2024 年提升 70% 以上，可完成复杂任务的层级化拆解，满足智能体 “规划” 环节的核心需求；</li><li>​<strong>多模态融合</strong>​：实现文本、图像、语音、视频、结构化数据的无缝交互，支持智能体在多场景下的 “感知” 能力，可对接不同类型的外部数据与工具；</li><li>​<strong>参数效率优化</strong>​：大模型实现 “大参数能力 + 小参数部署”，轻量化模型可在端侧运行，降低智能体的开发与使用成本，适配个人与中小企业场景。</li></ol><h4>1.2 智能体技术体系的完善：标准化闭环与工具生态</h4><p>智能体的 **“感知 - 规划 - 行动 - 反思”** 底层闭环实现标准化、模块化，成为技术落地的关键支撑：</p><ol><li>​<strong>核心模块标准化</strong>​：感知模块支持多渠道信息接入（用户指令、API 数据、本地文件），规划模块适配不同任务的拆解逻辑，行动模块对接通用工具接口，反思模块实现错误识别与自动修正，各模块可灵活组合适配场景；</li><li>​<strong>零代码 / 低代码工具普及</strong>​：Coze（扣子）、LangGraph、AutoGen 等工具完成产品化升级，提供可视化配置、现成插件与模板，降低智能体开发门槛，实现 “无编程基础也能搭建专属智能体”；</li><li>​<strong>工具生态互联互通</strong>​：智能体可对接 90% 以上的主流办公、生产、运营工具（Excel、ERP、设计软件、社交平台等），实现从 “数字世界执行” 到 “现实业务落地” 的衔接，打破技术应用的场景壁垒。</li></ol><h4>1.3 产业需求的爆发：从 “技术尝鲜” 到 “效率刚需”</h4><p>2025 年以来，各行业从 “AI 技术尝鲜” 转向 “降本增效刚需”，为智能体规模化落地提供产业土壤：</p><ol><li>​<strong>企业端</strong>​：中小企业对自动化、轻量化智能工具的需求激增，希望通过智能体替代基础重复性工作，降低人力成本；大型企业开始搭建专属智能体体系，实现产、供、销、服全流程的人机协同；</li><li>​<strong>个人端</strong>​：职场人、创作者、自由职业者等群体对个性化智能助手的需求提升，覆盖办公、学习、创作、生活等多场景，推动智能体向 “全民化” 发展；</li><li>​<strong>政策端</strong>​：多国出台 AI 产业扶持政策，明确智能体技术的发展方向与应用规范，保障技术落地的同时，规避数据安全、伦理风险等问题，为产业发展营造良好环境。</li></ol><h3>二、2026AI 智能体元年的核心产业特征</h3><h4>2.1 技术特征：从 “专用智能体” 到 “通用智能体 + 垂直适配”</h4><p>2026 年智能体技术呈现 “​<strong>通用能力打底，垂直场景深耕</strong>​” 的核心特征：</p><ol><li>​<strong>通用智能体</strong>​：具备跨场景的基础任务执行能力，可完成文档处理、数据整理、简单沟通、工具调用等通用工作，成为个人与企业的 “基础数字助手”；</li><li>​<strong>垂直智能体</strong>​：基于通用智能体进行行业微调与场景定制，适配特定行业的业务逻辑与需求，如工业生产智能体、电商运营智能体、医疗辅助智能体、教育答疑智能体等，具备行业专属的知识储备与任务执行能力；</li><li>​<strong>多智能体协同</strong>​：单智能体向 “智能体集群” 发展，通过角色分工、任务协作完成复杂产业任务，如电商行业的 “选品智能体 + 文案智能体 + 运营智能体 + 数据分析智能体” 协同，实现从产品上架到数据优化的全流程自动化。</li></ol><h4>2.2 应用特征：从 “线上内容生成” 到 “全场景业务执行”</h4><p>智能体突破此前 “仅能完成线上内容生成” 的局限，实现 **“线上 + 线下”“数字 + 实体”** 的全场景业务执行：</p><ol><li>​<strong>线上场景深度渗透</strong>​：覆盖办公自动化、内容创作、客户服务、数字营销等领域，完成从 “辅助生成” 到 “自主执行” 的转变，如智能体可自主完成市场调研、数据整理、报告生成、渠道发布的全流程；</li><li>​<strong>线下场景逐步落地</strong>​：依托物联网、工业互联网等技术，智能体可对接线下设备与生产流程，如工业制造中的智能体可实现设备监控、故障预警、生产调度，物流行业的智能体可完成路径规划、仓储管理、配送调度等。</li></ol><h4>2.3 产业特征：从 “技术驱动” 到 “需求驱动”，生态化发展</h4><p>2026 年 AI 产业从 “技术厂商主导的技术迭代” 转向 “产业端主导的需求落地”，形成 **“大模型厂商 + 智能体开发平台 + 场景应用方 + 工具生态方”** 的完整产业生态：</p><ol><li>​<strong>大模型厂商</strong>​：提供核心的基础模型能力，为智能体开发提供底层技术支撑，如 OpenAI、百度、阿里等；</li><li>​<strong>智能体开发平台</strong>​：提供零代码 / 低代码开发工具、标准化模块与插件生态，连接大模型技术与产业需求，如 Coze、LangGraph 等；</li><li>​<strong>场景应用方</strong>​：企业、机构及个人根据自身需求，在开发平台上搭建或定制智能体，实现技术落地；</li><li>​<strong>工具生态方</strong>​：各类办公、生产、运营工具厂商开放接口，与智能体开发平台对接，完善智能体的行动能力，形成产业协同。</li></ol><h3>三、2026 智能体技术的核心应用领域与落地案例</h3><h4>3.1 企业服务领域：办公自动化与人机协同办公体系构建</h4><p>智能体成为企业数字化转型的核心工具，实现办公全流程的自动化与高效化，核心应用包括：</p><ol><li>​<strong>行政办公智能体</strong>​：自主完成考勤统计、周报 / 月报生成、会议纪要整理、文件归档、邮件回复等基础行政工作，降低行政人员的重复性劳动；</li><li>​<strong>销售运营智能体</strong>​：对接企业 CRM 系统，完成客户线索筛选、客户咨询自动回复、销售数据统计分析、销售报表生成，辅助销售团队提升获客与转化效率；</li><li>​<strong>人力资源智能体</strong>​：完成简历筛选、初试邀约、员工考勤、薪酬核算、员工培训答疑等工作，优化 HR 工作流程，聚焦核心的人才招聘、员工发展等工作。  <br/>​<strong>落地案例</strong>​：某中小企业通过 Coze 搭建专属销售智能体，对接企业微信与 Excel，实现客户咨询 7×24 小时自动回复，销售数据每日自动统计并生成分析报告，销售团队的基础工作效率提升 65%，人力成本降低 30%。</li></ol><h4>3.2 产业制造领域：工业智能体赋能智能制造与生产优化</h4><p>智能体与工业互联网、物联网、大数据技术结合，赋能制造业从 “自动化生产” 到 “智能化生产” 的升级，核心应用包括：</p><ol><li>​<strong>生产调度智能体</strong>​：根据订单需求、设备状态、原材料库存，自主制定生产计划，实时调整生产调度，提升生产效率；</li><li>​<strong>设备运维智能体</strong>​：实时监控设备运行数据，识别设备故障前兆，自动发出预警并提供维修方案，降低设备停机率；</li><li>​<strong>质量检测智能体</strong>​：结合机器视觉技术，完成产品生产过程中的实时质量检测，识别不合格产品，分析质量问题原因并提出优化建议。  <br/>​<strong>落地案例</strong>​：某中小型机械制造企业引入工业智能体，对接生产车间的物联网设备，实现生产计划的自动制定与动态调整，设备故障预警准确率提升 80%，产品不良率降低 25%，生产效率提升 40%。</li></ol><h4>3.3 民生服务领域：智能体提升公共服务效率与个性化体验</h4><p>智能体在教育、医疗、政务、物流等民生服务领域落地，实现公共服务的高效化、个性化与普惠化，核心应用包括：</p><ol><li>​<strong>教育辅助智能体</strong>​：为学生提供个性化学习答疑、笔记整理、考点梳理、作业批改，为教师提供备课素材整理、教学进度规划、学生成绩分析，优化教与学的流程；</li><li>​<strong>医疗辅助智能体</strong>​：为基层医疗机构提供常见病诊断答疑、病历整理、药品咨询，为患者提供就医指引、预约挂号、术后康复指导，缓解优质医疗资源紧张问题；</li><li>​<strong>政务服务智能体</strong>​：实现政务咨询自动回复、政务办理流程指引、线上材料初审，推动 “政务服务一网通办”，提升政务服务效率，降低群众办事成本。  <br/>​<strong>落地案例</strong>​：某地方政务服务中心上线政务智能体，对接政务服务平台，实现社保、医保、户籍等高频政务问题的 7×24 小时自动回复，线上材料初审通过率提升 75%，窗口办理业务的平均时长缩短 60%，群众办事满意度提升 90%。</li></ol><h4>3.4 个人端应用：全民化智能体成为个人数字分身</h4><p>2026 年个人智能体实现全民化普及，成为覆盖<strong>办公、学习、创作、生活</strong>的个人数字分身，核心应用包括：</p><ol><li>​<strong>个人办公助手</strong>​：为职场人完成文档整理、会议纪要、任务提醒、邮件撰写等工作，提升个人办公效率；</li><li>​<strong>学习创作智能体</strong>​：为学生、创作者提供文献整理、内容创作初稿、素材收集、排版发布等服务，解放创作与学习的基础劳动；</li><li>​<strong>生活服务智能体</strong>​：完成日程规划、购物比价、出行规划、家庭账单整理等生活服务，提升个人生活的便捷性。</li></ol><h3>四、2026 智能体技术落地的核心挑战与解决路径</h3><h4>4.1 核心挑战</h4><ol><li>​<strong>技术层面</strong>​：部分复杂场景下的智能体仍存在​<strong>任务执行误差</strong>​，如复杂逻辑的拆解、多工具协同的衔接、错误识别与修正的精准度不足；端侧智能体的能力与云端存在差距，轻量化与高性能的平衡仍需突破；</li><li>​<strong>数据层面</strong>​：智能体的落地依赖高质量、场景化的数据，部分行业存在<strong>数据孤岛</strong>问题，企业内部数据与外部数据无法有效打通；数据安全与隐私保护成为核心痛点，智能体在数据读取、使用过程中存在数据泄露风险；</li><li>​<strong>产业层面</strong>​：部分传统行业对智能体技术的​<strong>认知与接受度不足</strong>​，缺乏专业的技术落地与运营人才，导致技术与产业需求无法有效匹配；智能体的行业定制化成本仍较高，中小企业的落地门槛有待进一步降低；</li><li>​<strong>伦理层面</strong>​：智能体的自主决策可能引发<strong>责任界定模糊</strong>问题，如智能体执行任务出现错误时，责任归属于开发平台、应用方还是用户；部分场景下的智能体应用可能引发就业结构调整，带来社会就业问题。</li></ol><h4>4.2 解决路径</h4><ol><li>​<strong>技术迭代</strong>​：持续优化大模型的推理与决策能力，完善智能体的 “反思” 模块，提升任务执行的精准度；加大端侧大模型与智能体的研发投入，实现轻量化与高性能的平衡；推动多智能体协同算法的优化，提升复杂任务的执行能力；</li><li>​<strong>数据治理</strong>​：建立行业数据共享机制，打破数据孤岛，为智能体落地提供高质量数据支撑；完善数据安全与隐私保护的技术与法规体系，采用数据加密、联邦学习等技术，保障数据在采集、使用、传输过程中的安全；明确智能体数据使用的边界与规范，禁止未经授权的数椐读取与使用；</li><li>​<strong>产业赋能</strong>​：加强智能体技术的行业普及与培训，培养兼具<strong>产业知识与 AI 工具能力</strong>的复合型人才；推动智能体开发平台的产品化与标准化，降低行业定制化成本，推出针对中小企业的轻量化智能体解决方案；鼓励大模型厂商、开发平台与行业龙头企业合作，打造行业标杆案例，推动技术的规模化落地；</li><li>​<strong>伦理与政策规范</strong>​：建立智能体技术的伦理准则与责任界定体系，明确开发平台、应用方、用户在智能体执行过程中的权利与责任；出台相关的就业扶持政策，针对智能体技术带来的就业结构调整，开展职业技能培训，推动劳动力向高价值、不可替代的岗位转型；建立智能体技术的监管体系，加强对技术应用的规范与引导，规避伦理风险。</li></ol><h3>五、2026AI 元年之后的智能体技术发展趋势</h3><h4>5.1 短期趋势（2026-2027 年）：垂直场景深度落地，全民化普及加速</h4><ol><li>智能体在电商、制造、政务、教育等行业实现深度落地，形成一批标准化、可复制的行业解决方案；</li><li>个人智能体的功能与场景进一步丰富，成为手机、电脑之外的 “标配数字工具”，全民化普及速度加快；</li><li>零代码 / 低代码智能体开发平台进一步完善，插件生态更加丰富，开发门槛进一步降低，实现 “人人皆可搭建智能体”。</li></ol><h4>5.2 中期趋势（2028-2030 年）：通用智能体成熟，人机协同成为生产生活标配</h4><ol><li>通用智能体技术实现成熟，具备跨行业、跨场景的自主决策与执行能力，可完成复杂的综合性任务；</li><li>人机协同成为生产、生活、办公的标配模式，人类专注于创意、决策、情感连接等高价值工作，智能体完成基础的执行与操作工作；</li><li>智能体与机器人、物联网、元宇宙等技术深度融合，实现从 “数字世界” 到 “物理世界” 的全面渗透，打造智能化的生产生活体系。</li></ol><h4>5.3 长期趋势（2030 年以后）：智能体向 “通用人工智能” 迈进，人机共生体系形成</h4><ol><li>智能体在技术层面逐步具备自主学习、自主进化的能力，向<strong>通用人工智能（AGI）</strong> 迈进，具备与人类相当的综合智能；</li><li>形成 “人机共生、相互赋能” 的全新体系，智能体不仅是人类的 “工具”，更是人类的 “协作伙伴”，共同推动社会生产力的提升；</li><li>智能体技术将重构产业结构、社会分工与生产生活方式，推动人类社会进入智能化发展的新阶段。</li></ol><h3>六、结论</h3><p>2026 年作为 AI 智能体元年，标志着 AI 技术从 “内容生成” 阶段进入 “自主任务执行” 阶段，完成了从技术概念到产业落地的核心跨越。依托大模型技术的成熟、智能体体系的完善与产业需求的爆发，智能体在企业服务、产业制造、民生服务、个人端等领域实现规模化落地，成为推动产业升级、提升生产效率、优化生活体验的核心力量。</p><p>同时，2026 年智能体技术的落地仍面临技术、数据、产业、伦理等多方面的挑战，需要技术厂商、产业从业者、政策制定者共同努力，通过技术迭代、数据治理、产业赋能、政策规范，推动智能体技术的健康、可持续发展。</p><p>未来，随着技术的持续迭代与产业的深度融合，智能体将逐步实现从 “垂直智能体” 到 “通用智能体” 的升级，人机协同将成为生产生活的标配，最终形成 “人机共生” 的全新发展体系。2026AI 元年不仅是智能体技术的落地起点，更是人类社会智能化发展的全新开端，技术的发展将始终围绕 “赋能人类、提升社会生产力” 的核心目标，推动人类社会向更高质量、更高效、更智能的方向发展。</p><h3>参考文献</h3><p>[1] 斯坦福大学. AI 指数报告 2026 [R]. 斯坦福大学人类与人工智能研究院，2026.[2] 麦肯锡咨询。智能体技术与产业变革白皮书 2026 [R]. 麦肯锡全球研究院，2026.[3] 中国人工智能产业发展联盟。中国智能体技术落地与应用规范指南 (2026 版)[S]. 2026.[4] OpenAI. GPT-4o 技术白皮书 [R]. OpenAI 官方技术团队，2026.[5] 腾讯云。智能体技术在企业服务领域的落地实践与趋势分析 [R]. 腾讯云 AI 研究院，2026.[6] Coze（扣子）. 零代码智能体开发与应用白皮书 2026 [R]. 字节跳动 AI 实验室，2026.</p>]]></description></item><item>    <title><![CDATA[实时云渲染平台如何接入摄像头与本地媒体数据 点量小耿 ]]></title>    <link>https://segmentfault.com/a/1190000047558493</link>    <guid>https://segmentfault.com/a/1190000047558493</guid>    <pubDate>2026-01-22 15:03:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>———以点量云流商用实时云渲染平台为例</p><ol><li>技术背景：从“渲染流畅”到“业务适配”的能力升级<br/>随着实时云渲染技术的日趋成熟，其应用场景已从单纯的三维模型轻量化展示，深度渗透到数字孪生、工业仿真、远程三维协作、在线教学培训等多元化业务领域。当前，行业对云渲染平台的核心需求已发生本质转变：不再局限于“依赖云端算力实时渲染复杂三维模型，实现终端轻量化交互操作”，更聚焦于“能否承载真实业务场景中的全流程沟通，实现虚拟场景与现实信息的无缝联动”。<br/>点量云流在对接大量政企、工业、教育类商用项目落地过程中发现，仅依靠三维模型的实时渲染与基础交互，远不足以支撑复杂的远程协作场景。例如，工业远程运维中，工程师需要同步查看设备三维仿真模型与现场摄像头拍摄的设备实际运行状态；三维项目协作会议中，参会者需结合本地文档、操作演示视频，对虚拟场景进行精准讨论；在线技能培训中，讲师需通过摄像头实时讲解，同步展示课件、实操视频与三维仿真操作流程。<br/>基于此，点量云流在现有实时云渲染架构的基础上，针对性引入摄像头与本地媒体数据接入能力，通过技术优化实现现实世界数据流与虚拟三维场景的无缝融合，有效补齐了传统云渲染平台在“全维度信息表达”“业务场景适配”层面的短板，进一步拓宽了实时云渲染技术的商用落地边界。<br/><img width="723" height="593" referrerpolicy="no-referrer" src="/img/bVdnIiy" alt="" title=""/></li><li>技术架构：四层协同，实现数据高效接入与融合呈现<br/>从技术实现逻辑来看，点量云流的摄像头与本地媒体数据接入能力，可拆解为“本地采集层—流媒体传输层—云端渲染融合层—Web三维呈现层”四个核心层级，各层级协同联动，既保证数据传输的低延迟、高稳定，又实现媒体内容与三维场景的自然融合，适配商用场景的严苛需求。<br/>2.1 本地数据采集与编码：多源适配，筑牢数据基础<br/>作为数据接入的源头，本地采集层的核心目标是实现多类型本地媒体数据的全面、高效采集，并通过标准化编码处理，为后续流媒体传输奠定基础。点量云流突破传统平台的采集局限，支持多种本地数据源的全覆盖采集，具体包括：<br/> 摄像头数据：支持电脑内置摄像头、高清工业摄像头等各类设备，可根据场景需求灵活调整采集分辨率、帧率，适配从日常沟通到工业监测的不同清晰度要求；<br/> 本地屏幕数据：支持全屏采集、指定区域采集、单个应用窗口采集，可精准捕捉桌面操作、软件演示等内容，适配项目演示、技能培训等场景；<br/> 各类课件与文件内容：兼容Flash课件、exe可执行课件、PPT、Word、PDF等多种格式文档，支持直接采集展示，无需额外格式转换，提升使用便捷性；<br/> 本地媒体文件：支持MP4等主流格式的本地视频文件，可实现文件的实时采集与流式播放，适配预录视频演示、案例讲解等场景。<br/> 针对采集到的多类型数据，平台会进行实时标准化编码处理，采用H.264/H.265高效编码算法，在保证画面清晰度的前提下，最大限度压缩数据体积，降低后续网络传输压力，同时支持编码参数动态调整，适配不同网络环境下的采集传输需求。<br/>2.2 流媒体化与协议支持：低延迟传输，适配多场景分发<br/>流媒体传输层是连接本地采集与云端融合的核心枢纽，其核心作用是将本地编码后的媒体数据，统一转化为标准流媒体格式，并通过适配商用场景的传输协议，实现数据的低延迟、高稳定推送，同时支持多用户并发访问与各类外部视频源对接。<br/>点量云流采用“统一流媒体化”处理逻辑，将采集编码后的摄像头画面、屏幕内容、课件、视频文件等各类数据，均转化为标准化流媒体流，确保后续云端融合与终端播放的兼容性。在传输协议方面，平台聚焦商用场景的核心需求，重点支持两大主流协议，实现多场景适配：<br/> RTSP协议：主打低延迟、强控制特性，传输延迟可控制在毫秒级，适合对实时性要求极高的场景，如工业设备实时监测、远程手术指导、实时互动演示等，可实现媒体数据的实时推送与精准控制；<br/> RTMP协议：生态成熟、兼容性强，支持多用户并发分发，可适配大规模用户同时访问的场景，如在线公开课、大型项目协作会议、多终端同步演示等，确保不同用户的访问体验一致。<br/>值得注意的是，点量云流的媒体接入能力并非局限于本地摄像头，还可实现与监控系统、网络摄像头等各类标准视频源的无缝对接，通过协议兼容，将外部视频数据同步接入三维场景，进一步拓展了业务适配范围，适配工业数字孪生、智慧监控等高端场景需求。<br/>2.3 云端渲染融合层：无缝联动，实现虚实融合<br/>云端渲染融合层是整个技术架构的核心，承担着“三维场景渲染”与“本地媒体数据融合”的双重职责，也是点量云流区别于传统云渲染平台的关键所在。平台通过自研的融合渲染算法，将流媒体传输层推送的本地媒体数据（摄像头画面、屏幕内容等），与云端渲染的三维场景进行实时无缝融合，实现以下核心效果：<br/> 媒体内容可灵活嵌入三维场景：支持将摄像头画面、本地视频、文档等内容，以悬浮窗口、内嵌模块等形式，嵌入三维场景的任意位置，不遮挡核心三维模型，实现自然呈现；<br/> 虚实数据实时联动：当三维场景进行旋转、缩放、漫游等操作时，嵌入的本地媒体内容可同步适配，保持画面同步性，确保用户在查看三维场景的同时，能实时获取本地媒体信息；<br/> 多源媒体内容协同展示：支持同时接入多路本地媒体数据（如摄像头+本地文档+屏幕演示）。<br/>2.4 Web端播放与访问：无插件适配，降低使用门槛<br/>Web三维呈现层作为面向用户的最终展示载体，核心目标是实现“便捷访问、跨端兼容”，降低用户使用门槛，适配企业多样化的访问场景。点量云流摒弃了传统平台需要安装专用客户端、插件才能播放媒体内容的模式，通过技术优化，实现媒体流与三维场景的Web端无插件直接播放。<br/>该层的核心优势的体现在三个方面：<br/> 一是无需安装任何插件、客户端，用户通过Chrome、Edge等主流浏览器，即可直接访问平台，查看融合了本地媒体数据的三维场景，大幅降低企业用户的部署与使用成本；<br/> 二是全面支持跨平台访问，兼容Windows、Mac等各类桌面操作系统，同时适配平板、手机等移动终端，用户可随时随地通过终端设备接入，满足远程协作、移动办公的需求；<br/> 三是灵活适配网络环境，无论是企业内网的封闭场景，还是公网的开放场景，均能实现稳定播放，通过网络自适应算法，动态调整播放参数，避免因网络波动导致的卡顿、黑屏，保障商用场景的流畅体验。</li><li>多人同步与分组控制：适配协作场景，提升业务效率<br/>在商用场景中，实时云渲染平台的核心应用之一是多人远程协作，而摄像头与本地媒体数据的接入，本质上是为了提升协作的高效性与精准性。为此，点量云流针对性打造了完善的多人同步与分组控制机制，将媒体数据接入能力与协作场景深度绑定，解决了多用户协作中“内容不同步、权限不清晰、场景切换繁琐”等痛点。其核心功能包括：<br/> 多用户实时同步订阅：同一路本地媒体数据（如主讲人的摄像头画面、演示文档），可支持多个用户同时订阅查看，所有用户看到的内容与主讲人实时同步，无延迟、无偏差，确保协作沟通的一致性，适配多人项目会议、在线培训等场景；<br/> 基于用户分组的内容隔离：支持根据业务需求，将用户划分为不同分组（如项目A组、项目B组、教学分组等），不同分组可接入不同的本地媒体数据，实现内容隔离，避免不同业务场景的内容干扰，同时保障数据安全，适配多项目并行、多班级教学等复杂场景；<br/> 动态切换：支持本地媒体内容的动态切换（如从摄像头画面切换为本地文档、从屏幕演示切换为视频文件），实现协作流程的顺畅衔接，提升沟通效率。<br/>这套同步与控制机制，为各类复杂协作场景提供了坚实的技术支撑，无论是企业的多项目分组协作、教育机构的分班在线教学，还是工业领域的多团队远程运维沟通，都能通过该机制实现高效协作，进一步发挥实时云渲染与本地媒体接入融合的价值。</li><li>技术价值总结：从“渲染引擎”到“综合型三维协作基础设施”的跨越<br/>实时云渲染平台接入摄像头与本地媒体数据，并非简单的功能叠加，而是对云渲染技术商用价值的深度挖掘与能力升级，从技术层面来看，这一能力的落地，具有三大核心价值，推动云渲染平台实现从“单一渲染工具”到“综合型三维协作基础设施”的跨越式发展。<br/>第一，搭建现实世界与三维空间的实时数据通道。传统云渲染平台仅能呈现虚拟三维场景，与现实世界存在明显的信息割裂；而摄像头与本地媒体数据的接入，打通了虚拟场景与现实世界的数据流壁垒，实现了现实信息与虚拟模型的实时联动，让三维场景不再是“孤立的虚拟载体”，而是能够承载现实业务信息的“综合展示窗口”，提升了平台的业务适配能力。<br/>第二，强化系统的整合与扩展能力。点量云流的媒体接入能力，支持多类型本地数据源、多标准传输协议、多终端访问场景的兼容适配，不仅可接入普通摄像头与本地文件，还能对接工业相机、监控系统等专业设备，实现与企业现有业务系统的无缝整合，无需对现有设备、系统进行大规模改造，降低企业数字化升级成本；同时，开放的接口设计，也为后续接入更多类型的媒体数据源、拓展更多协作功能提供了可能，具备极强的扩展性。<br/>第三，奠定高阶商用场景落地的技术基础。数字孪生、远程运维、三维协同设计、在线技能培训等高阶应用场景，均需要虚拟场景与现实信息的深度融合，而摄像头与本地媒体数据接入能力，正是支撑这些场景落地的核心技术支撑。例如，工业数字孪生中，通过接入现场摄像头与设备监控视频，可实现虚拟孪生模型与设备实际运行状态的实时对照，助力远程运维与故障排查；三维协同设计中，通过接入本地设计文档、操作演示视频，可实现多设计师的精准沟通，提升设计效率。<br/>综上，点量云流通过在实时云渲染平台中融入摄像头与本地媒体数据接入能力，不仅补齐了传统平台的能力短板，更推动云渲染技术从“专注渲染”向“赋能业务”转型，使其成为能够支撑多元化商用场景、承载全流程协作沟通的综合型三维协作基础设施，为各行业的数字化转型提供更加强劲的技术支撑。<br/><img width="723" height="593" referrerpolicy="no-referrer" src="/img/bVdnIiz" alt="" title="" loading="lazy"/></li></ol>]]></description></item><item>    <title><![CDATA[Microsoft AI Genius | 解锁 Agent-to-Agent 大规模协作新范式！ ]]></title>    <link>https://segmentfault.com/a/1190000047558516</link>    <guid>https://segmentfault.com/a/1190000047558516</guid>    <pubDate>2026-01-22 15:02:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>还在为复杂的智能体系统集成发愁？想让您的 AI 助手不仅聪明，还能像人类团队一样高效“组队打怪”？</p><p>当 AI 从“单体智能”走向“群体协作”，Agent-to-Agent（A2A）正在成为企业级 AI 系统的新底座。在微软 AI Genius 第二期课程中，我们将深入拆解：如何构建、编排并扩展可协作的多智能体系统，让智能体像团队一样分工协作、自动完成复杂任务。</p><p><img width="723" height="1301" referrerpolicy="no-referrer" src="/img/bVdnIiT" alt="dec304a8651bac1ac84ca08d76286981.png" title="dec304a8651bac1ac84ca08d76286981.png"/></p><p><strong>您将学到</strong></p><ul><li>使用 Microsoft Agent Framework 统一智能体，降低集成复杂度。</li><li>借助 SWE Agents 加速编码，实现快速原型开发。</li><li>利用 MCP 在编排过程中保护敏感数据，确保企业合规。</li></ul><p><strong>直播互动福利</strong></p><p>本期课程不仅有硬核技术拆解，更准备了有奖互动福利！观看直播课程，根据小助手指引参与直播互动，并加入技术交流群参与抽奖，即有机会获得 Microsoft AI Genius 定制好礼！</p><p>如果您正在关注企业级 Agent 架构、多智能体协作与自动化工作流、AI Coding Agent 与工程化落地。这一期课程，千万不要错过。</p><p>1 月 28 日 14:00 - 15:30，锁定 Microsoft AI Genius 第三季第二期直播，一起走进多智能体协作的下一站。</p>]]></description></item><item>    <title><![CDATA[DolphinScheduler 3.1.9 + Minio 开发环境【IDEA】搭建访问及相关问题]]></title>    <link>https://segmentfault.com/a/1190000047558525</link>    <guid>https://segmentfault.com/a/1190000047558525</guid>    <pubDate>2026-01-22 15:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558527" alt="Apache DolphinScheduler3.1.9+Minio 海报" title="Apache DolphinScheduler3.1.9+Minio 海报"/></p><h4>目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558528" alt="" title="" loading="lazy"/></p><ul><li><p><a href="https://link.segmentfault.com/?enc=Tx8YT3lMS1LKYWU5PE2QmA%3D%3D.p6KaLCmxbw4VqXQKOIGr7yYXis6ix0bg6yRxLW%2B5oCZicwOlIFgCU6FAdRyl9%2F961rmDumCM9WbLbab4D98Ztw%3D%3D" rel="nofollow" target="_blank">DolphinScheduler 3.1.9 开发环境【IDEA】搭建访问</a></p><ul><li><p><a href="https://link.segmentfault.com/?enc=L0osZNcRH3xHZyWbS%2BRJUA%3D%3D.TWeenOXal40jI4DRTuce50N4yP%2FANw9ozpjddke%2FGgU%3D" rel="nofollow" target="_blank">前提</a></p><ul><li><a href="https://link.segmentfault.com/?enc=oswQYHEDiQFHhKYGB1XveA%3D%3D.HDKlAMWPurVHty9O8i8tbKAMcYr5o9aDQnrUxYFFOhY%3D" rel="nofollow" target="_blank">1、软件要求</a></li><li><a href="https://link.segmentfault.com/?enc=YtPtMpo9l%2FNLK%2BqBdlR%2BEQ%3D%3D.JLJTShR%2FZQ5nrHmZ8jLEGawq0J9cSoqP41sE9hcRm4U%3D" rel="nofollow" target="_blank">2、克隆代码库</a></li><li><a href="https://link.segmentfault.com/?enc=g3h8SXmWJouR7AhOi7c7Rg%3D%3D.zsd9nMRmZ3MvxITPLcWTbaAQgbZ%2BZG20CpCg0x2wwXc%3D" rel="nofollow" target="_blank">3、编译源码</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=0bGQil2Pplqmp7lcovuuIg%3D%3D.8Vg1Xd%2BAWTlLnmSAXr%2FfZwRpwCJy7%2FsFk%2F0mMZHGw0ctLMjuyihwq49TDlPZaDaj" rel="nofollow" target="_blank">DolphinScheduler 普通开发模式</a></p><ul><li><a href="https://link.segmentfault.com/?enc=szwE0vteV8%2BSvzIML9mWgw%3D%3D.EYQb4C5HR5qsmp7hqJCyp7Hz7SBTGsYsTOG%2BEcggb5M%3D" rel="nofollow" target="_blank">1、编译问题：</a></li><li><p><a href="https://link.segmentfault.com/?enc=aA8sef57ArFyqT75sF0X8g%3D%3D.MrNEleTR%2FSSx3lRCg2BxXhrpA9lEZjO%2F8w84BCCC0wtwq7LFp8VpGteKIQh0Zz1E" rel="nofollow" target="_blank">2、启动zookeeper</a></p><ul><li><a href="https://link.segmentfault.com/?enc=tWRXXupfn8T5x21uEzSz3Q%3D%3D.7vqMnwikfny9B96d100c5MO5tRdoY1aURyGpYRFizOo%3D" rel="nofollow" target="_blank">官方内容</a></li><li><a href="https://link.segmentfault.com/?enc=QS7i7CM3QoNstNvdSxlQVg%3D%3D.N5dqXodySFrTeFw%2FrjSh0L%2F7JW28KbgQsnXO3XDURw0%3D" rel="nofollow" target="_blank">存储配置</a></li><li><a href="https://link.segmentfault.com/?enc=Gu9DzzQto6qzAJvlwA1bUA%3D%3D.l5sjJcGaVtiatSdq7loXi6XLCKYclNU4rlmdkfcpq20%3D" rel="nofollow" target="_blank">启动脚本</a></li></ul></li><li><a href="https://link.segmentfault.com/?enc=GPyyOfXQOvDZ3QP4LLDl9Q%3D%3D.GKPj9E3hlgosAmdaJ2pRb4oR%2FkMJGa0cwotB%2BXudnK%2F%2Brx%2Bb4HlRaylVR%2Br42xxx" rel="nofollow" target="_blank">3、workspace.xml 修改</a></li><li><p><a href="https://link.segmentfault.com/?enc=K04pamIOTSUdB79KduT%2BkA%3D%3D.8wAqOsIPRdR8ltltuuusdh1b07rDJjPS7jpMPN1GkA0%3D" rel="nofollow" target="_blank">4、数据库</a></p><ul><li><a href="https://link.segmentfault.com/?enc=%2BOBroVnCsLc6Gtjev39Djw%3D%3D.xHK6aVyiHOMUSzVM8EJpVbD3Ep9adCYPphM1Guf0PBw%3D" rel="nofollow" target="_blank">4-1：数据初始化</a></li><li><a href="https://link.segmentfault.com/?enc=ydg%2BffaDq5wYRaA6AQKqsA%3D%3D.xE2TNMPZHcIoaC0NClbedQ9m%2FE5FKmLTFu7CcB4WZmQ%3D" rel="nofollow" target="_blank">4-2：依赖相关修改</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=N8CRImhXHyDFT1Xv65dBUg%3D%3D.4geiwwJs2ZZ74CyqHAht64D8I%2BTNVEDAszyqXpJc%2BjtgTg145ZDZyKVCp8yOSvsb" rel="nofollow" target="_blank">5、application.yaml 修改数据库配置</a></p><ul><li><a href="https://link.segmentfault.com/?enc=TkIkt4Iwa6KP87FfSiFOzQ%3D%3D.T8x0FSMTljsGp3OFuLKlKc5YLqOdDgfV9UVoBvbweoewLr74gZt0ET69eWQx%2FnsPmO3gcfVd6v06tXdBApb8IQ%3D%3D" rel="nofollow" target="_blank">5-1：dolphinscheduler-master</a></li><li><a href="https://link.segmentfault.com/?enc=2%2FRvjMBLZkvBB8Nl2JThIg%3D%3D.sqVcNMdEeJyUo%2FRYqhIHx4MmhhE0vEIXqkhhmCaTMf6wbZ59rtyn3zk18X91hkKfCYYJIyVGuyl1WJIhm0rkWA%3D%3D" rel="nofollow" target="_blank">5-2：dolphinscheduler-worker</a></li><li><a href="https://link.segmentfault.com/?enc=fWT3Vb0fk7PIw8sJxAZTmA%3D%3D.B28Ee%2Fsqq4%2Fcd0mCxLzc69j6Wa4upGsc0x8iNm3szkXNIKEd6B3Ed3J4b7OWIif9" rel="nofollow" target="_blank">5-3：dolphinscheduler-api</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=SXrSq6RMt41%2BHvldqrAHqg%3D%3D.UVjvBs%2FjsOgbHIqzecxJ3v8qfbTSXaqPvgqEG6VKLhTDE4CukWniM3o9XplPEnQB" rel="nofollow" target="_blank">6、logback-spring.xml 修改日志级别</a></p><ul><li><a href="https://link.segmentfault.com/?enc=EXOt3Zgei0GJj0W%2B4ROlrA%3D%3D.AwkuEbJbF2ng4bWNEKYUpkxVyY53WcrN83g7jIG1JcQPVvS0ZqYatBnzPnpBT0gWHei9e0mkpmDYRVF3tTYNRg%3D%3D" rel="nofollow" target="_blank">6-1：dolphinscheduler-master</a></li><li><a href="https://link.segmentfault.com/?enc=rR1qSQx2QKIkMNHeHR5s8Q%3D%3D.f7DlJInV6KpC6i7uWFKGCYmos9JGL9Eo7G5%2B9ov2cVwbQc9yzhFSY7J0BJ03yQL9e7%2FR2lVV2G21NLdxLhDg9A%3D%3D" rel="nofollow" target="_blank">6-2：dolphinscheduler-worker</a></li><li><a href="https://link.segmentfault.com/?enc=1HVfYSvvNlnR%2FHhQbCGtMA%3D%3D.lmHtjaYKdqktIqHhOAEBMYzQZYVd7JzZaiviGpr2nQRP4lQmv81A%2FpCTk2eCwtkQ" rel="nofollow" target="_blank">6-3：dolphinscheduler-api</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=juKgLTqSDar2LdOmPcn1pw%3D%3D.0bneUG1pxHFYwy1qt9T49cV5UwcS1qzN0ks6p30mYqo%3D" rel="nofollow" target="_blank">7、启动后端三个服务</a></p><ul><li><p><a href="https://link.segmentfault.com/?enc=u%2Bh20VKz1GNazQfKOM%2B54A%3D%3D.bDXL%2FH%2Fvqk8Fee6TI3%2Bgpn1ANoQiQkhTmErbRWsNntwSLeqfnMA%2FtZQ2%2BgsGv5gA" rel="nofollow" target="_blank">7-1：MasterServer</a></p><ul><li><a href="https://link.segmentfault.com/?enc=c9m9fXP8yaejHMXPlo1jUg%3D%3D.Arwtr1KQH8Nwm2Yzeo8zcaSLFkNItJxD%2FXVA0lrxKh%2BNKWfEZy6y%2FZUuS9Gsmr2M" rel="nofollow" target="_blank">配置 VM Options</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=iA8d72%2FrQePbn6STD9emRw%3D%3D.Dgls7LnLRgJMGYBLAIdE2VLWPui4R8xkxjWLwZ7z0okw6s%2B1Ot%2BuRPjRKurkMSw4" rel="nofollow" target="_blank">7-2：WorkerServer</a></p><ul><li><a href="https://link.segmentfault.com/?enc=8uRtVVmVLeQt5%2FGh3Nj4TQ%3D%3D.hcNss5qsdeZ%2FFWJ%2B1KWX3mZR8CBDzM2BorP1hEUFu8OZYwd4TbH0k0o%2FECLvY4pt" rel="nofollow" target="_blank">配置 VM Options</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=Tq6x8eoFUBXfB8%2BY1t09MA%3D%3D.RD9t%2BcPrv7U5fVN3cOuUCujkRzBk8zIZM1LDqiS93Q1rj2%2FmXyJc%2BuExEQ1PNDp7" rel="nofollow" target="_blank">7-3：ApiApplicationServer</a></p><ul><li><a href="https://link.segmentfault.com/?enc=Sw3h1LbXapbPD4jWEjjO%2Fg%3D%3D.pnoMx03UwO6bFy9AX3YJpDPVmdX89KPu7lbREUwyvkN4h5uuhEYv1bCTatLNk8ni" rel="nofollow" target="_blank">配置 VM Options</a></li></ul></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=ewoWsLI9Ti5Fq%2F54hYpXbw%3D%3D.emep7r1lV4OqHZIlardQVwgAJjIHLKKs8dyWOTSBVz4%3D" rel="nofollow" target="_blank">8、启动前端服务</a></p><ul><li><a href="https://link.segmentfault.com/?enc=hzN5v1%2Fh7McWE1%2BqffHw9w%3D%3D.hznRwk2u3qHmlj2cpXnyB0zMTHhmLmD0NOr8he3bPZE%3D" rel="nofollow" target="_blank">命令：</a></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=8h8JI%2BVYfgNZrZHgWNpaqA%3D%3D.aX79GSoD1TTAcvyPPZ%2BGAS%2BG3AKH6IokvFO4xW31Fko%3D" rel="nofollow" target="_blank">9、浏览器访问</a></p><ul><li><a href="https://link.segmentfault.com/?enc=RCqXOQjPtwQV3LRe9Ye57w%3D%3D.SuGA%2FULg6G7GjNBxGwHd1UoUfHIM2AUAdT%2FuUCXeP7k%3D" rel="nofollow" target="_blank">账号密码：</a></li><li><a href="https://link.segmentfault.com/?enc=4fv91exxgfkdB%2Fjkzm%2FWtA%3D%3D.IqbelfuKuN%2BbRO9tg0sViRqko47LkqiLdQMn9MF6TKI%3D" rel="nofollow" target="_blank">成功访问：</a></li></ul></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=yFPnMnjZqq25P5dRyf%2FZjg%3D%3D.TsvqjahSjn2tEAqy6omnUFs8DC7gsv8LV%2FmT%2F4UVI0I%3D" rel="nofollow" target="_blank">相关问题</a></p><ul><li><p><a href="https://link.segmentfault.com/?enc=ziGEQB7xDgRK1LOIuz%2BxAQ%3D%3D.%2Fljv%2ByyI2AkZ9%2FBvXumEQESV99eeli%2FkbondIh6fCI4%3D" rel="nofollow" target="_blank">1、存储未启用、租户\\用户 指定</a></p><ul><li><p><a href="https://link.segmentfault.com/?enc=D30MWsOXQKgF%2FWcY16s4aw%3D%3D.0nMkWFprOG%2FzCm5RXefDQUPATH3fitJoI3e1rzhVEUU%3D" rel="nofollow" target="_blank">解决方法：</a></p><ul><li><a href="https://link.segmentfault.com/?enc=tOteTQj0BHijqlYr6v0Vfw%3D%3D.mo6AHukV8sKc%2FDLM1%2B3hh%2FDZhLa8orUIqsJT6aEvKMgbMl%2BWoCn8wsQhAm2vwMgq7E%2B6GgwzO6Bo0H0dBU6Mgg%3D%3D" rel="nofollow" target="_blank">1、minio 创建 dolphinscheduler 桶</a></li><li><a href="https://link.segmentfault.com/?enc=cEsDiB8MK%2BZC0UPQ6whxjQ%3D%3D.jvWnw9B%2Fj5%2Bjeww2MALrRIlH39gd0X%2Bz9qqMtzABV3yZA0RBqPAakeDe52DPm8aX" rel="nofollow" target="_blank">2、commom.properties 修改</a></li><li><p><a href="https://link.segmentfault.com/?enc=BqhzF9kpDW1LmmwsBeweKg%3D%3D.goXo%2Btp3SYbgmIhZgREh5Q5c4p6mohlHjpop59KwY8Vexs4KjIqyoTBUMOjsiHQW" rel="nofollow" target="_blank">3、dolphinscheduler 可视化页面添加租户</a></p><ul><li><a href="https://link.segmentfault.com/?enc=gp5Cvs0hbgZU9KINwOVeVg%3D%3D.cO7AtsVuOQcEdvO9oMnx%2B4WIvpaKeJzryCucYeu4Qb0%3D" rel="nofollow" target="_blank">用户添加租户</a></li></ul></li></ul></li></ul></li></ul></li><li><p><a href="https://link.segmentfault.com/?enc=qOCr8J8ldfoSctq27ncP7A%3D%3D.WatAdGGH%2FOGORzKVkRAziZTKvfoKN8%2FyEGoCqRVDDZA%3D" rel="nofollow" target="_blank">演示</a></p><ul><li><a href="https://link.segmentfault.com/?enc=p71BIhY5JdTz7g67NqVVTw%3D%3D.H1y6%2FFNdLV4bK0pdKe9gP3pM9Gbmj9Wu2sONH9NEfv8%3D" rel="nofollow" target="_blank">创建文件夹、上传文件成功</a></li></ul></li></ul></li></ul><p>这里按照官方提供的文档进行操作：</p><h3>前提</h3><p>官方提供的开发手册位置</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558529" alt="" title="" loading="lazy"/></p><h4>1、软件要求</h4><p>在搭建 DolphinScheduler 开发环境之前请确保你已经安装以下软件:</p><ul><li><a href="https://link.segmentfault.com/?enc=S%2Bep%2FUONFESj0yANA%2Bzevw%3D%3D.vZB5E5UA%2FnX45J8n0cv91nQoMXoevyFKnkV4CVc4g60%3D" rel="nofollow" target="_blank">Git</a></li><li><a href="https://link.segmentfault.com/?enc=NCHX11A5pmFdJNKy2PyqKw%3D%3D.amAPZpRGlcdCjSKra5uMAULcXHT0RKalqLTqt3lNlspibJphIhMpefZYq5Ewpt51crAutXD%2Bf3jV1HWNIkHMTljjYAU4qYvhh25qWc%2FQflU%3D" rel="nofollow" target="_blank">JDK</a>: v1.8.x (当前暂不支持 jdk 11)</li><li><a href="https://link.segmentfault.com/?enc=AI8WzObH%2BqwjqNK0k6jHWQ%3D%3D.96awtZnFMlnSHK%2Fmy25yN6oLumKHOYuouh2iDwMWk4gfPWI%2B91I2obn%2BfCJPnrQO" rel="nofollow" target="_blank">Maven</a>: v3.5+</li><li><a href="https://link.segmentfault.com/?enc=%2FPSgb2iY0jBLIp6hJrokUA%3D%3D.9t8TPVF%2BV5xbwPny8H6ticNDMUAJ0IMBPro8qXBuNQs%3D" rel="nofollow" target="_blank">Node</a>: v16.13+ (dolphinScheduler 版本低于 3.0, 请安装 node v12.20+)</li><li><a href="https://link.segmentfault.com/?enc=AYucG%2B1hL2oGE4vAqemTUg%3D%3D.x6hxcf9eDq4QI%2BUNXwXGMp%2Bouv%2FmLJJXsUUJrRncSjM%3D" rel="nofollow" target="_blank">Pnpm</a>: v6.x</li></ul><h4>2、克隆代码库</h4><p>通过你 git 管理工具下载 git 代码，下面以 git-core 为例</p><pre><code>mkdir dolphinscheduler
cd dolphinscheduler
git clone git@github.com:apache/dolphinscheduler.git</code></pre><h4>3、编译源码</h4><pre><code>支持的系统:
* MacOS
* Linux
【这个我没有运行试试】
运行 `mvn clean install -Prelease -Dmaven.test.skip=true`</code></pre><h3>DolphinScheduler 普通开发模式</h3><p>上面是官方提供的，我觉得有用就复制下来，</p><p>这里开始我就按照自己的操作顺序记录</p><h4>1、编译问题：</h4><pre><code>1、git相关
1-1：开启 Windows Git 长路径支持，
管理员 PowerShell 执行，解决 DolphinScheduler 路径太深导致 git add 失败
git config --system core.longpaths true

1-2：先初始化git仓库，只在本地，不涉及账号、不推远程，Spotless 需要 HEAD
git init
git add .
git commit -m "initial commit"

2、Maven 编译 / 格式化（IDEA 里的 Terminal）
2-1：依赖 Git HEAD，自动修复格式问题
mvn spotless:apply
2-2：编译整个项目（跳过测试），确保所有模块已 install
mvn clean install -DskipTests

3、前端相关：

查看 Node.js 是否已安装
node -v

查看 npm 版本
npm -v

安装 pnpm
npm install -g pnpm
pnpm -v</code></pre><p>编译都没有问题</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558530" alt="" title="" loading="lazy"/></p><h4>2、启动zookeeper</h4><h5>官方内容</h5><p>下载 <a href="https://link.segmentfault.com/?enc=arSlBatMPlk8k%2FU3u568mA%3D%3D.bIODjsbF9iYgXvFngDK42x8HcZZ02vX8rHPzZCjyXwU%2FquIxNZZYqnjr%2Fvw16oH40gDT961f4HNVsEWtgql%2FAw%3D%3D" rel="nofollow" target="_blank">ZooKeeper</a>，解压</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558531" alt="" title="" loading="lazy"/></p><h5>存储配置</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558532" alt="" title="" loading="lazy"/></p><h5>启动脚本</h5><p>搞个txt编辑完后，后缀该bat即可</p><pre><code>@echo off
echo 正在启动 ZooKeeper...
cd /d E:\\install\\ZooKeeper\\zookeeper-3.8.3\\bin
zkServer.cmd
pause</code></pre><h4>3、workspace.xml 修改</h4><p>【可以不用，我也是看其他文章有添加的，不过我没添加也能正常运行，这里只做记录】</p><pre><code>在其他文章看到说在这里添加这行，说是让 IDEA 在运行时动态使用模块的 classpath，而不是用启动时生成的静态 classpath。

注意点：
这个作用只会影响本地 IDEA 启动，线上环境如果有问题这个是解决不了的。

"dynamic.classpath": "true",</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558533" alt="" title="" loading="lazy"/></p><h4>4、数据库</h4><p>我这里用的是mysql，所以需要修改</p><h5>4-1：数据初始化</h5><pre><code>创建名为【dolphinscheduler】的新数据库后，
把这个位置的sql直接拷贝复制执行即可。</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558534" alt="" title="" loading="lazy"/></p><p>如图：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558535" alt="" title="" loading="lazy"/></p><h5>4-2：依赖相关修改</h5><pre><code>如果使用 MySQL 作为元数据库，需要先修改 `dolphinscheduler/pom.xml`，
将 `mysql-connector-java` 依赖的 `scope` 改为 `compile`，
使用 PostgreSQL 则不需要

test 改成 compile</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558536" alt="" title="" loading="lazy"/></p><h4>5、application.yaml 修改数据库配置</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558537" alt="" title="" loading="lazy"/></p><h5>5-1：dolphinscheduler-master</h5><pre><code>如图，配置文件中修改这些数据：三个内容都是一样的

spring:
  config:
    activate:
      on-profile: mysql
  datasource:
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://127.0.0.1:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=utf8&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=true&amp;serverTimezone=GMT%2B8
    username: 账户名
    password: 数据库密码</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558538" alt="" title="" loading="lazy"/></p><h5>5-2：dolphinscheduler-worker</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558539" alt="" title="" loading="lazy"/></p><h5>5-3：dolphinscheduler-api</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558540" alt="" title="" loading="lazy"/></p><h4>6、logback-spring.xml 修改日志级别</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558541" alt="" title="" loading="lazy"/></p><h5>6-1：dolphinscheduler-master</h5><pre><code>&lt;appender-ref ref="STDOUT"/&gt;</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558542" alt="" title="" loading="lazy"/></p><h5>6-2：dolphinscheduler-worker</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558543" alt="" title="" loading="lazy"/></p><h5>6-3：dolphinscheduler-api</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558544" alt="" title="" loading="lazy"/></p><h4>7、启动后端三个服务</h4><pre><code>我们需要启动三个服务，包括 MasterServer，WorkerServer，ApiApplicationServer

* MasterServer：在 Intellij IDEA 中执行 `org.apache.dolphinscheduler.server.master.MasterServer` 中的 `main` 方法，并配置 *VM Options* `-Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql`

* WorkerServer：在 Intellij IDEA 中执行 `org.apache.dolphinscheduler.server.worker.WorkerServer` 中的 `main` 方法，并配置 *VM Options* `-Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql`

* ApiApplicationServer：在 Intellij IDEA 中执行 `org.apache.dolphinscheduler.api.ApiApplicationServer` 中的 `main` 方法，并配置 *VM Options* `-Dlogging.config=classpath:logback-spring.xml -Dspring.profiles.active=api,mysql`。启动完成可以浏览 Open API 文档，地址为 http://localhost:12345/dolphinscheduler/swagger-ui/index.html

&gt; VM Options `-Dspring.profiles.active=mysql` 中 `mysql` 表示指定的配置文件</code></pre><h5>7-1：MasterServer</h5><h6>配置 VM Options</h6><pre><code>按照操作配置这个：打开后填入即可

-Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558545" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558546" alt="" title="" loading="lazy"/></p><h5>7-2：WorkerServer</h5><h6>配置 VM Options</h6><p>跟上面一样操作：</p><pre><code>-Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql</code></pre><h5>7-3：ApiApplicationServer</h5><h6>配置 VM Options</h6><pre><code>-Dlogging.config=classpath:logback-spring.xml -Dspring.profiles.active=api,mysql</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558547" alt="" title="" loading="lazy"/></p><p>总的就这三个：</p><h4>8、启动前端服务</h4><h5>命令：</h5><pre><code>安装前端依赖并运行前端组件

cd dolphinscheduler-ui
pnpm install
pnpm run dev</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558548" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558549" alt="" title="" loading="lazy"/></p><h4>9、浏览器访问</h4><h5>账号密码：</h5><pre><code>浏览器访问：
http://localhost:5173/home

默认账号密码：

账号：admin
密码：dolphinscheduler123
</code></pre><h5>成功访问：</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558550" alt="" title="" loading="lazy"/></p><h3>相关问题</h3><h4>1、存储未启用、租户\用户 指定</h4><p>问题：测试能否创建文件夹、上传文件等，提示【存储未启用】<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558551" alt="" title="" loading="lazy"/></p><p>问题：当前登录用户的租户信息未被指定<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558552" alt="" title="" loading="lazy"/></p><h5>解决方法：</h5><p><a href="https://link.segmentfault.com/?enc=CWVrVLhobFTncDHHMhBwjQ%3D%3D.fWUKkCNK8BlTtdAD4VXLoWrLUC%2Bt%2FE5GU8mKeZmMZCPsCWTVcN6QsT4TNmWRfkPmHPfogrM%2FyBKDE%2F7au6Bzdk7IZy%2F5QhHlrLy3fIylKcJczWCZQQCISeox1%2B3NIwmP" rel="nofollow" target="_blank">Minio 安装、启动</a></p><p>我这里直接用minio来尝试：</p><h6>1、minio 创建 dolphinscheduler 桶</h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558553" alt="" title="" loading="lazy"/></p><h6>2、commom.properties 修改</h6><p>配置文件改了这些地方<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558554" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558555" alt="" title="" loading="lazy"/></p><pre><code># Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# user data local directory path, please make sure the directory exists and have read write permissions
data.basedir.path=/tmp/dolphinscheduler

# resource view suffixs
#resource.view.suffixs=txt,log,sh,bat,conf,cfg,py,java,sql,xml,hql,properties,json,yml,yaml,ini,js

# resource storage type: HDFS, S3, OSS, NONE
# ljh --&gt;   S3 is Minio--------------------------------------
resource.storage.type=S3
# resource store on HDFS/S3 path, resource file will store to this base path, self configuration, please make sure the directory exists on hdfs and have read write permissions. "/dolphinscheduler" is recommended
resource.storage.upload.base.path=/dolphinscheduler

# ljh --&gt; The account and password of MinIO-------------------------------
# The AWS access key. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.access.key.id=minioadmin
# The AWS secret access key. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.secret.access.key=minioadmin
# The AWS Region to use. if resource.storage.type=S3 or use EMR-Task, This configuration is required
resource.aws.region=cn-north-1
# ljh --&gt; add bucket ------------------------------
# The name of the bucket. You need to create them by yourself. Otherwise, the system cannot start. All buckets in Amazon S3 share a single namespace; ensure the bucket is given a unique name.
resource.aws.s3.bucket.name=dolphinscheduler
# You need to set this parameter when private cloud s3. If S3 uses public cloud, you only need to set resource.aws.region or set to the endpoint of a public cloud such as S3.cn-north-1.amazonaws.com.cn
# ljh --&gt; localhost convert  127.0.0.1
resource.aws.s3.endpoint=http://127.0.0.1:9000

# alibaba cloud access key id, required if you set resource.storage.type=OSS
resource.alibaba.cloud.access.key.id=&lt;your-access-key-id&gt;
# alibaba cloud access key secret, required if you set resource.storage.type=OSS
resource.alibaba.cloud.access.key.secret=&lt;your-access-key-secret&gt;
# alibaba cloud region, required if you set resource.storage.type=OSS
resource.alibaba.cloud.region=cn-hangzhou
# oss bucket name, required if you set resource.storage.type=OSS
resource.alibaba.cloud.oss.bucket.name=dolphinscheduler
# oss bucket endpoint, required if you set resource.storage.type=OSS
resource.alibaba.cloud.oss.endpoint=https://oss-cn-hangzhou.aliyuncs.com

# if resource.storage.type=HDFS, the user must have the permission to create directories under the HDFS root path
resource.hdfs.root.user=hdfs
# if resource.storage.type=S3, the value like: s3a://dolphinscheduler; if resource.storage.type=HDFS and namenode HA is enabled, you need to copy core-site.xml and hdfs-site.xml to conf dir
resource.hdfs.fs.defaultFS=hdfs://mycluster:8020

# whether to startup kerberos
hadoop.security.authentication.startup.state=false

# java.security.krb5.conf path
java.security.krb5.conf.path=/opt/krb5.conf

# login user from keytab username
login.user.keytab.username=hdfs-mycluster@ESZ.COM

# login user from keytab path
login.user.keytab.path=/opt/hdfs.headless.keytab

# kerberos expire time, the unit is hour
kerberos.expire.time=2

# resourcemanager port, the default value is 8088 if not specified
resource.manager.httpaddress.port=8088
# if resourcemanager HA is enabled, please set the HA IPs; if resourcemanager is single, keep this value empty
yarn.resourcemanager.ha.rm.ids=192.168.xx.xx,192.168.xx.xx
# if resourcemanager HA is enabled or not use resourcemanager, please keep the default value; If resourcemanager is single, you only need to replace ds1 to actual resourcemanager hostname
yarn.application.status.address=http://ds1:%s/ws/v1/cluster/apps/%s
# job history status url when application number threshold is reached(default 10000, maybe it was set to 1000)
yarn.job.history.status.address=http://ds1:19888/ws/v1/history/mapreduce/jobs/%s

# datasource encryption enable
datasource.encryption.enable=false

# datasource encryption salt
datasource.encryption.salt=!@#$%^&amp;*

# data quality option
data-quality.jar.name=dolphinscheduler-data-quality-dev-SNAPSHOT.jar

#data-quality.error.output.path=/tmp/data-quality-error-data

# Network IP gets priority, default inner outer

# Whether hive SQL is executed in the same session
support.hive.oneSession=false

# use sudo or not, if set true, executing user is tenant user and deploy user needs sudo permissions; if set false, executing user is the deploy user and doesn't need sudo permissions
sudo.enable=true
setTaskDirToTenant.enable=false

# network interface preferred like eth0, default: empty
#dolphin.scheduler.network.interface.preferred=

# network IP gets priority, default: inner outer
#dolphin.scheduler.network.priority.strategy=default

# system env path
#dolphinscheduler.env.path=dolphinscheduler_env.sh

# development state
development.state=false

# rpc port
alert.rpc.port=50052

# set path of conda.sh
conda.path=/opt/anaconda3/etc/profile.d/conda.sh

# Task resource limit state
task.resource.limit.state=false

# mlflow task plugin preset repository
ml.mlflow.preset_repository=https://github.com/apache/dolphinscheduler-mlflow
# mlflow task plugin preset repository version
ml.mlflow.preset_repository_version="main"

# ljh --&gt; minio must open path style
resource.aws.s3.path.style.access=true
</code></pre><h6>3、dolphinscheduler 可视化页面添加租户</h6><p>安全中心 - 租户管理 - 创建租户<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558556" alt="" title="" loading="lazy"/></p><h6>用户添加租户</h6><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558557" alt="" title="" loading="lazy"/></p><h3>演示</h3><h4>创建文件夹、上传文件成功</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558558" alt="" title="" loading="lazy"/></p><p>如图，数据已经存放在我指定的minio文件夹里面了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558559" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026开年即用：节点式思维对齐工具快速上手指南与核心功能攻略 NAVI_s1mple ]]></title>    <link>https://segmentfault.com/a/1190000047558596</link>    <guid>https://segmentfault.com/a/1190000047558596</guid>    <pubDate>2026-01-22 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>为什么需要节点式思维对齐工具？</strong></h2><p>在复杂的团队协作中，传统的线性沟通方式往往只关注信息的单向传递，而忽略了认知的深层对齐 。然而，战略与执行之间存在多维度的关联，如果没有节点化的对齐管理，可能会导致：</p><ul><li><strong>团队认知断层</strong>：战略意图在层层传递中失真，执行端无法理理解决策背后的逻辑原点 。</li><li><strong>沟通效率低下</strong>：缺乏可视化逻辑链条，导致决策路径破碎，难以回溯演进过程 。</li><li><strong>协作方向偏离</strong>：各部门缺乏统一的“认知地图”，导致资源投入无法形成合力。</li></ul><p>节点式思维对齐工具通过将抽象的想法、目标和任务转化为可视化的节点与链路，帮助团队建立结构化的认知模型，确保每个人的思考都能在同一频率上 。</p><h2><strong>节点式思维对齐工具的核心特性</strong></h2><ul><li><strong>图谱化展示</strong>：将思路和任务以节点形式呈现，直观展示非线性的逻辑关联 。</li><li><strong>动态实时同步</strong>：支持多人在线实时推演，任何思维层面的变动都能即刻实现全员对齐。</li><li><strong>多层级逻辑穿透</strong>：可从宏观的战略节点下钻至微观的执行细节，实现全局与局部的统一 。</li><li><strong>关系链路建模</strong>：清晰标记节点间的因果、阻塞或支撑关系，构建严密的逻辑闭环 。</li></ul><h2><strong>节点式思维对齐工具的重要意义</strong></h2><ol><li><strong>缩短认知半径</strong>：通过可视化的思维图谱，极大降低了跨部门理解复杂战略及业务逻辑的门槛 。</li><li><strong>强化决策严密性</strong>：可视化的过程会倒逼团队梳理逻辑，从而更容易发现潜在的逻辑矛盾或执行缺失点。</li><li><strong>提升资源协同效率</strong>：节点式对齐能快速识别出“关键节点”和“瓶颈节点”，引导团队精准投入核心资源 。</li><li><strong>增强成员目标感</strong>：透明化的思维路径让每位执行者都能清晰看到自己的工作在整体大蓝图中的位置。</li></ol><h2><strong>应用场景</strong></h2><ul><li><strong>战略解码与推演</strong>：将公司愿景逐级拆解为各层级的关键决策节点，确保上下同欲 。</li><li><strong>复杂项目架构设计</strong>：在项目启动前，通过节点图梳理系统架构、功能模块与业务依赖 。</li><li><strong>项目复盘与逻辑对齐</strong>：回溯执行过程中的关键决策节点，识别逻辑拐点并沉淀为组织资产。</li></ul><h2>---</h2><p><strong>5款值得尝试的节点式思维对齐工具</strong></p><h3><strong>1. 板栗看板</strong></h3><p>结构化节点展示与任务对齐的可视化平台</p><ul><li><strong>特点</strong>：支持任务卡片间的逻辑连线，通过看板视图直观展示节点的流转过程与依赖关系 。</li><li><strong>优势</strong>：将“抽象逻辑”与“具体任务”通过节点连接，团队能清晰看到每个任务背后的价值支撑 。</li><li><strong>适合团队</strong>：追求流程透明与逻辑一致性，需要将战略目标快速落地为执行动作的敏捷团队。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558598" alt="在这里插入图片描述" title="在这里插入图片描述"/></li></ul><h3><strong>2. Trello</strong></h3><p>直观的看板式思维对齐工具</p><ul><li><strong>特点</strong>：通过颜色标记、标签系统和列表组织，让节点在工作流中的位置一目了然 。</li><li><strong>优势</strong>：界面设计直观，通过简单的拖拽即可实现任务节点的优先级调整与共识达成 。</li><li><strong>适合团队</strong>：注重可视化呈现和轻量级协同的初创或创意团队 。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558599" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3><strong>3. ClickUp</strong></h3><p>灵活的多视图节点管理系统</p><ul><li><strong>特点</strong>：支持将思维节点在时间线、看板等多种视图间切换，适应不同的认知需求。</li><li><strong>优势</strong>：功能极其丰富，能够帮助团队管理复杂的任务层级和多维度的逻辑分类 。</li><li><strong>适合团队</strong>：需要多层级管理、涉及复杂职能交叉的中大型团队 28。<br/>在这里插入图片描述<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558600" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3><strong>4. Jira Software</strong></h3><p>专业级研发逻辑对齐与追踪平台</p><ul><li><strong>特点</strong>：将目标对齐作为敏捷流程的核心，支持任务节点的深度影响分析与状态追踪 。</li><li><strong>优势</strong>：严密的逻辑关联能力，适合对研发流程、故障节点有严格闭环管理要求的团队 。</li><li><strong>适合团队</strong>：追求高度标准化、需要将思维对齐固化为生产流水线的专业技术团队 。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558601" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3><strong>5. Asana</strong></h3><p>跨职能思维协同与目标分发平台</p><ul><li><strong>特点</strong>：提供灵活的节点管理能力，强调跨职能团队间的目标一致性与协作友好性 。</li><li><strong>优势</strong>：强大的集成能力，能将思维对齐的结果快速转化为不同应用间的自动化流转 。</li><li><strong>适合团队</strong>：需要灵活处理跨部门复杂依赖、注重易用性与协作体验的通用型团队 。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558602" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h2>---</h2><p><strong>如何选择合适的节点式思维对齐工具？</strong></p><h3><strong>1. 按团队规模选择</strong></h3><ul><li><strong>小型团队</strong>：推荐 <strong>板栗看板</strong>、Trello 等上手即用的工具，强调从思维到执行的转化效率 。</li><li><strong>中型团队</strong>：适合使用 <strong>Asana</strong>、Trello 等灵活管理复杂任务节点与标签的平台 。</li><li><strong>大型团队</strong>：建议选择 <strong>ClickUp</strong> 或 <strong>Jira</strong>，这些工具提供强大的层级管理功能，适应大规模共识难题 。</li></ul><h3><strong>2. 按思维复杂度选择</strong></h3><ul><li><strong>简单对齐</strong>（如日常待办、轻松项目）：选择 <strong>板栗看板</strong>、Trello 等直观、操作简便的工具 。</li><li><strong>复杂对齐</strong>（如跨部门协作、深层系统重构）：推荐 <strong>ClickUp</strong>、Jira 等支持深度自定义和多层级节点管理的系统。</li></ul><h2>---</h2><p><strong>结语</strong></p><p>节点式思维对齐工具让组织的认知从碎片走向网状，帮助团队打破“理解的墙”，在高度不确定的商业环境中快速形成合力。通过这些工具，团队可以构建可视化的组织大脑，确保每一个动作都源于深度共识，并最终指向共同的目标</p>]]></description></item><item>    <title><![CDATA[启动｜2026 中国边缘计算20强榜单评选：寻找 AI 时代的边缘力量 边缘计算社区 ]]></title>    <link>https://segmentfault.com/a/1190000047558186</link>    <guid>https://segmentfault.com/a/1190000047558186</guid>    <pubDate>2026-01-22 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在这个被大模型和智能体（Agent）疯狂重塑的年份，我们不得不承认一个残酷的事实：传统的边缘计算叙事，正在失效。</p><p>当算力从中心有序下沉，当 AI Agent 开始接管终端决策，边缘计算不再只是网络的延伸，而正在成为智能的前沿阵地。谁还停留在旧叙事中，谁又真正拿到了通往下一个十年的船票，答案正在迅速分化。</p><p>基于这样的行业背景，边缘计算社区正式启动「2026 中国边缘计算企业 20 强」榜单评选。这不仅是一份年度名单，更是一场在技术代际更迭下的行业校准。</p><p><img width="723" height="311" referrerpolicy="no-referrer" src="/img/bVdnIdE" alt="image.png" title="image.png"/></p><h2>榜单背景</h2><p>自 2019 年起，边缘计算社区已连续六年发布「中国边缘计算企业 20 强」榜单，累计吸引 800 余家产业链企业参与评选，覆盖边缘云、边缘一体机、边缘 AI、5G MEC 等核心领域。</p><p>过去六年中，该榜单全网累计传播曝光量突破 3500 万次（清博大数据舆情统计），不仅持续为行业树立技术与商业标杆，也逐步成为企业扩大市场影响力、获取生态与产业资源的重要入口。</p><p>当边缘计算进入 “边缘 × AI × 智能体” 的新阶段，我们认为：这份榜单，也必须随技术代际一起升级。</p><h2>从“连接”到“智算”</h2><p>回望过去两年，边缘计算的演进速度远超预期。</p><p>如果说 2024 年行业仍在聚焦边缘盒子、网关与连接能力，那么到了 2025 年底，只谈连接、不谈推理的产品，已经很难再获得市场认可。</p><p>大模型正在以前所未有的速度“瘦身”并下沉至边缘侧：从手机、PC，到工业控制器与现场设备，越来越多的终端被要求具备本地推理与自主决策能力。</p><p>边缘计算正在从“管道”，演进为 AI 的“触角”。当然，这并不意味着所有传统边缘计算企业都会被淘汰。但可以确定的是：</p><blockquote>以“连接能力”为核心竞争力的边缘产品，正在快速失去议价权。</blockquote><h2>智能体爆发，边缘侧的“寒武纪时刻”</h2><p>2026 年，或将成为边缘智能体（Edge Agents）走向规模化应用的起点。所谓边缘智能体，并非简单的模型端侧部署，而是指在受限算力、弱网络甚至离线条件下，仍具备自主感知、规划与执行能力的边缘决策单元。</p><p>未来的边缘计算竞争，将不再取决于硬件参数，而在于：</p><ul><li>谁能让大模型在边缘侧稳定运行</li><li>谁能在毫秒级延迟内完成复杂决策</li><li>谁能在算力、算法与网络之间实现系统级优化</li></ul><p>这不仅是技术升级，更是一轮生态重构。</p><h2>寻找 2026 年的“边缘脊梁”</h2><p>正是在这样的行业变局之下，我们启动「2026 中国边缘计算企业 20 强」评选。</p><p>我们要寻找的，不是停留在历史成绩上的老牌玩家，也不是只会包装概念的“PPT 公司”，而是那些真正进入 “边缘 × AI”深水区 的企业：</p><ul><li>成功将 7B、14B 等模型量化并部署到边缘端的技术实践者</li><li>用边缘智能体解决真实、碎片化场景问题的实干团队</li><li>在算力、算法与网络协同中实现突破的破局者</li></ul><p>他们，才是真正决定边缘计算下一个十年走向的力量。</p><hr/><h2>评选标准与参选要求</h2><h3>参选条件</h3><ul><li>在边缘计算领域具备成熟的技术解决方案与商业化落地案例；</li><li>拥有核心技术壁垒（如边缘芯片、算法优化、异构计算等）或独特生态资源；</li><li><p><strong>2026 年新增重点：</strong>展示边缘计算与 AI 大模型的融合实践（如优化 AI 推理效率、隐私计算、联邦学习等），以及算力。  <br/><img width="723" height="970" referrerpolicy="no-referrer" src="/img/bVdnIdF" alt="image.png" title="image.png" loading="lazy"/></p><h3>评分机制</h3></li><li><strong>线上投票（30%）：</strong>公众通过官方渠道为心仪企业投票；</li><li><p><strong>专家评审（70%）：</strong>从以下四大维度综合打分：</p><ul><li>技术领先性（35%）</li><li>商业落地（30%）</li><li>边缘×AI创新（25%）</li><li>生态贡献（10%）</li></ul></li></ul><h3>上榜权益</h3><ul><li><strong>品牌升维：</strong>通过头部合作伙伴渠道全域曝光，覆盖 10 万+ 开发者社区；</li><li><strong>商机裂变：</strong>优先对接甲方订单资源，2024 年某上榜企业通过生态合作斩获 800 万项目订单；</li><li><strong>权威认证：</strong>榜单企业客户咨询量平均提升 120%（历史数据）；</li><li><strong>生态赋能：</strong>优先加入“边缘计算产业图谱”。</li></ul><h2>特别提醒</h2><p>独行者快，众行者远：在 AI 巨头定义规则的战场上，边缘计算企业唯有被看见，才有机会被选择。技术不应被埋没，真正的能力值得被记录。边缘计算的下一个十年，不属于参数最多的人，而属于最懂场景、最懂约束、也最懂 AI 如何落地的人。</p><p><strong>边缘计算社区</strong>  <br/>2026年1月21日</p>]]></description></item><item>    <title><![CDATA[TDengine IDMP 让制糖生成真正做到“看得清、管得住、跑得稳” TDengine涛思数据 ]]></title>    <link>https://segmentfault.com/a/1190000047558063</link>    <guid>https://segmentfault.com/a/1190000047558063</guid>    <pubDate>2026-01-22 13:03:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 12 月，涛思数据与北京海莱德自动化工程有限公司（简称“海莱德”）正式建立合作伙伴关系。此次合作，海莱德将基于自身行业自动化系统集成能力，结合涛思数据提供的 TDengine TSDB + IDMP 产品组合，共同为制糖等行业客户打造从数据采集、治理到智能分析应用的完整解决方案，助力制糖工业企业实现生产运营的数字化与智能化转型。</p><h2>行业背景｜制糖生产正在面对的新挑战</h2><p>制糖行业的生产实践表明，甘蔗制糖是一项高度连续、强耦合、对运行稳定性要求极高的工业过程。原料受品种、成熟度、含糖量和纤维含量等因素影响，天然波动较大；加之榨季集中、生产节奏紧凑，一旦发生非计划停车或关键参数失控，带来的不仅是产量损失，更可能造成难以弥补的经济影响和社会影响（涉及甘蔗和甜菜的农业生产）。</p><p>在此背景下，行业内绝大多数糖厂长期依赖以人工经验为主的工艺调整方式，以及以工段为单位、相互割裂的数据管理模式，这些传统做法逐渐显现出其局限性。经验固然重要，但难以在不同班组、不同人员之间稳定传承与高效复制；生产数据虽然持续产生，却因分散在不同系统与记录中而难以整合分析，从而无法有效支撑对稳产、提质、降本目标的持续精细化管控。这已成为行业的一个普遍共识：仅依靠传统方式，已难以应对当前生产运行对稳定性与过程可控性日益提升的要求。</p><h2>面临挑战｜从“看不清”到“管不住”</h2><p>在实际运行中，以下这些挑战并非个案，而是制糖行业中普遍存在的共性问题。</p><p>首先，生产过程链条长、环节多，从预处理、压榨、澄清、蒸发、煮糖到分蜜、干燥包装，各工段数据往往分散在不同的系统与记录中，缺乏统一视角，导致难以形成真正贯穿全流程的生产监控与分析能力。</p><p>其次，在工艺质量管控方面，参数调整长期依赖人工经验判断。许多异常往往在最终质量指标已发生偏差后才得以察觉，缺乏对工艺质量的过程性分析与持续监控手段，难以实现事前预警与主动干预。</p><p>最后，在物料与糖分损耗管理上，行业长期缺乏有效的工具进行清晰、有效的分析和管理。糖分损耗分散于滤泥、废蜜、洗水、跑糖等多个环节，大多依靠经验估算，无法形成系统、可对比的“糖损画像”，这在很大程度上制约了对产糖效率与整体经营指标的持续优化。</p><p>正是这些普遍存在的“看不清、管不住”的痛点，促使制糖行业开始重新思考生产管理方式，并推动如 TDengine IDMP 这样的生产数据与工艺管理平台，逐渐成为企业进行数字化转型、实现精细化运营的重要选择。</p><h2>解决方案｜从“数据分散”到“AI Ready”，让制糖跑在数据之上</h2><p>在榨季现场，行业内常有一种共识：“数据其实都有，就是用不起来。”原料特性每日波动，工艺流程长且复杂，相关数据往往分散在局部的 DCS、各类设备的独立系统及手工台账中。操作人员依赖经验盯守，生产系统中前后无高效的数据流通，一旦生产节奏加快，潜在的风险与异常便容易被淹没在庞杂的信息流中。</p><p>因此，选择引入 TDengine IDMP 平台，其初衷并非简单“再上一套系统”，而是旨在将沉睡的数据转化为直接支撑生产决策与运营优化的能力。围绕制糖行业原料波动大、流程链路长、设备可靠性要求高等特点，该平台以 <strong>TDengine TSBS + TDengine IDMP</strong> 为核心，从数据采集与接入起步，逐步打通数据治理、业务情景化建模与 AI 分析应用，致力于构建一套真正面向生产、服务于工艺优化与稳定运行的工业数据管理体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558065" alt="图1 以 TDengine IDMP 为基础面向生产的工业数据管理体系" title="图1 以 TDengine IDMP 为基础面向生产的工业数据管理体系"/></p><h3>数据采集｜先把“碎数据”连成一条线</h3><p>在项目启动之初，制糖企业现场所面临的情况在行业中并不陌生：数据体量并不少，但分布零散。工厂局部的 DCS、各类设备的独立系统仅仅服务于局部的监控层面。而在数据分析、集中管理与智能应用层面，则长期缺乏统一、高效的数据出口。</p><p>针对这一现状，项目规划在不影响现有控制系统稳定运行的前提下，于集控层之上构建独立的数据采集与汇聚通道。计划在每个工厂部署一套 TDengine TSDB，利用其自带的零代码采集工具 taosX，通过 OPC 标准接口从 DCS Server 读取实时工艺数据，以实现关键生产数据的稳定采集与接入。同时，在企业级数据中心部署统一的 TDengine TSDB，对各工厂的时序数据进行集中汇聚与统一管理，为后续的数据整合分析与跨厂协同打下基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558066" alt="图2 某甘蔗制糖项目的数据采集架构图" title="图2 某甘蔗制糖项目的数据采集架构图" loading="lazy"/></p><p>这种架构既充分保留了 DCS 与 SCADA 的成熟运行体系，又在其之上形成统一、可扩展的数据采集与汇聚层，为后续的数据治理、业务情景化和 AI 应用奠定了可靠基础。</p><h3>数据分析｜从“看历史”到“提前知道”</h3><p>在数据分析层，平台基于 <strong>TDengine TSDB</strong> 的高性能时序数据管理能力，实现实时与历史数据的统一处理，并能够结合<strong>时序基础模型的时序数据预测与异常检测能力</strong>，对生产过程和设备运行状态进行持续分析。</p><p>通过对关键工艺参数和运行指标的时序建模，时序基础模型能够识别正常运行模式，预测指标变化趋势，并对偏离正常区间的异常波动进行及时检测与预警，帮助企业提前发现潜在风险。请参考：<a href="https://link.segmentfault.com/?enc=sN2jyz915433SqiwfRuT%2BQ%3D%3D.pmuJOlFbX4NXAt3exFSZR%2BySFEDHkXAggJWr1R9%2FAT38Oa7cr%2FHdAVDgl6dUK5Zy" rel="nofollow" target="_blank">时序数据分析智能体 TDgpt</a></p><p>该能力使生产管控从依赖经验的事后分析，转向基于数据的趋势预判与异常识别，为工艺稳定运行、设备可靠性提升及运营决策提供更加及时、可靠的数据支撑。</p><h3>数据目录｜让每个岗位都用得上数据</h3><p>如果说采集和分析解决了“数据有没有、算不算得动”的问题，那么数据目录解决的，是“业务用不用得上”。</p><p>TDengine IDMP 并没有强制所有人用同一种视角看数据，而是允许不同部门按自己的业务逻辑组织数据。生产车间可以围绕工艺流程，把数据按工序、工段和关键参数来组织；设备管理部门则按设备类型和运行状态建立目录，专注设备可靠性和维护。同一份底层数据，可以在不同业务视角下被反复引用。</p><p>对业务人员来说，找数据不再是“翻系统”，而是“进目录”；对系统来说，数据有了清晰的结构和入口，才能被稳定调用、持续分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558067" alt="图3 甘蔗制糖厂数据目录（按设备、按工艺）" title="图3 甘蔗制糖厂数据目录（按设备、按工艺）" loading="lazy"/><br/><a href="https://segmentfault.com/write###" target="_blank">https://segmentfault.com/write###</a></p><h3>数据标准化 | 让“一吨糖”只有一种算法</h3><p>在工业系统中，数据标准化不是“规范问题”，而是直接影响结果是否可信的基础工程。航天领域曾因单位不统一而导致重大事故，这一案例反复被提及，并不是偶然，而是揭示了一个普遍规律：<strong>当数据口径不统一时，系统即使运行正常，结论也可能完全错误。</strong></p><p>在制糖生产中，这类风险同样真实存在。以澄清汁流量为例，DCS 系统通常以体积流量 m³/h 采集数据，而部分历史系统或人工台账则沿用质量流量 t/h。两种口径在各自系统内都能够正常使用，但一旦进入跨系统分析场景——例如物料衡算、产能评估或能耗核算——问题便会显现：同一个“澄清汁流量”，在不同系统中参与计算，得到的却是两套完全不同的结果。</p><p>在 TDengine IDMP 中，这类问题不再依赖人工经验去“记住差异”，而是通过模型层面的标准化设计，从源头上消除歧义，确保“一吨糖”在系统中只有一种确定、可复用的计算方式。</p><h4>将“老师傅的共识”固化为系统规则</h4><p>在实际生产中，许多关键口径早已形成行业共识，只是长期存在于经验和习惯中。TDengine IDMP 通过元素模板机制，将这些共识转化为可执行、可约束的系统规则。</p><p>以“澄清汁”这一对象为例，IDMP 在模型层对其进行统一、规范的定义，明确其所包含的各类属性，并对每个属性的名称、业务含义、数据类型、计量单位及使用口径进行统一约束。针对澄清汁流量，模型中会明确其业务含义、统一采用的标准计量单位、适用的工艺计算口径，以及是否参与物料衡算与产能分析等核心规则。</p><p>通过这种方式，<strong>同一类工艺对象、同一类指标在系统中只保留唯一、确定的解释</strong>，从根本上避免“同名不同义”或“同数不同算”的问题，为后续跨系统分析和长期稳定运行提供一致、可靠的数据基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558068" alt="图4 通过元素模板将知识固化" title="图4 通过元素模板将知识固化" loading="lazy"/></p><h4>单位不同？系统自动算清楚</h4><p>在统一标准的同时，TDengine IDMP 也充分考虑了现有系统的复杂性。针对属性模板，平台在公式层引入计量单位的自动识别与推导能力。</p><p>当数据来自 DCS 系统时，平台能够识别其计量单位为体积流量（m³/h）；当数据来自历史系统或台账时，则识别为质量流量（t/h）。在参与计算或分析时，TDengine IDMP 会根据目标属性所要求的计量单位，自动推导并完成必要的单位换算，确保计算结果口径一致。</p><p>整个过程无需人工干预，也不依赖个人经验假设，使不同来源、不同口径的数据能够在统一模型下安全、可靠地参与分析，为物料衡算和经营决策提供稳定支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558069" alt="图5 澄清汁的质量流量到体积流量的自动推导" title="图5 澄清汁的质量流量到体积流量的自动推导" loading="lazy"/></p><h3>数据情景化｜让工业数据真正看得懂、用得上</h3><p>在实际生产中，制糖行业越来越深刻地体会到：<strong>没有情景的数据，只是一串数字；只有将其置于具体的工艺场景中，数据才真正具有意义。</strong></p><p>榨季期间，生产现场变化极为迅速。今天可能是澄清工段的 pH 值出现波动，明天发现废蜜纯度偏高，过几天又察觉实际产糖率与理论值存在偏差。这类问题本身并不复杂，但过去的分析方式却异常耗时费力——通常由业务人员凭借经验提出初步判断，再由技术人员到各个独立系统中查找相关点位、收集数据；数据找齐后，还需反复确认其时间范围、计算口径是否一致。往往经过这样一轮繁琐流程，数天时间已经过去。</p><p>究其根源，问题通常不在于人员专业能力，而在于数据本身缺乏情景化组织。业务人员往往不清楚所需数据具体分布在哪些系统中、是否可直接使用；技术人员也难以理解这些数据在工艺上应如何关联、如何分析，以及它们之间的业务逻辑是什么。这种数据与业务之间的“断层”，使得高效的分析与决策难以实现。</p><h4>连接业务与技术的关键一环</h4><p>引入 TDengine IDMP 平台后，制糖企业将能够使数据真正成为业务与技术之间的“通用语言”。</p><p>该平台通过为数据补充统一、清晰的业务语义，将其与具体的生产过程直接关联。每一条数据都将被明确归属到特定的工艺环节（如澄清、蒸发或煮糖），同时标识其反映的工艺机理类型（如反应强度、抽提效率或回收损失），并清楚定义其适用的业务场景（如质量监控、物料衡算、异常分析或工艺优化）。</p><p>在此基础上，平台还将构建标准化的技术元数据层，对数据来源、计量单位和合理取值范围进行统一管理。由此，数据从何处来、如何计算将变得清晰可溯，在进行数据分析、计算或设置告警时，系统能够自动确保口径一致，从而避免因理解偏差导致的结果不一。</p><p>这一步的关键价值在于，许多原本存在于“老师傅经验”中的隐性知识与共识，将被有效地沉淀并固化为清晰、可复用的系统规则，为知识的传承与规模化应用奠定基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558070" alt="图6 数据情景化（业务描述和限值）" title="图6 数据情景化（业务描述和限值）" loading="lazy"/></p><h4>业务分析真正实现自助</h4><p>在数据完成情景化之后，制糖企业的业务分析方式将发生根本性转变，从过去高度依赖 IT 部门支持，转向以业务人员自助分析为主。系统前端将不再展示零散的点位编号与底层数据结构，而是围绕“澄清稳定性”“物料衡算”“产糖效率”等业务人员熟悉的工艺情景来组织数据与功能。</p><p>以澄清工段为例，工艺人员在“澄清稳定性”情景下，将能够直接选取 pH 值、混浊度、色值等关键指标，并自行拖拽搭建趋势对比与关联分析面板，用于实时判断反应状态是否偏离正常区间。整个过程无需向 IT 部门提出建模或取数需求，分析逻辑也将更加贴近现场实际。业务人员从而能真正基于数据流进行自主判断与决策。</p><p>这种以业务情景为核心的分析模式，将显著降低数据使用门槛与技术障碍，使得工艺人员更愿意、也更能够主动、自信地使用数据工具，推动数据分析融入日常作业闭环。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558071" alt="图7 澄清工艺是否稳定？业务人员自助分析" title="图7 澄清工艺是否稳定？业务人员自助分析" loading="lazy"/></p><h4>响应能力的显著提升</h4><p>当业务分析实现自助化，为制糖企业带来最直接的变化就是——<strong>业务响应速度得到显著提升</strong>。过去，从发现异常到形成分析结论，往往需要经过多环节传递与处理，周期以天计算，等结论出来时，问题可能已经扩大，甚至错过了最佳工艺调整窗口。</p><p>未来，在数据情景化的支撑下，业务人员将能够在当班内直接完成数据取用、对比分析和假设验证。例如，当澄清工段 pH 值刚出现连续偏移时，系统可在对应的业务情景中自动聚合相关指标，工艺人员即可当场判断是否需要调整加药或工艺参数；当产糖率与预期出现偏差时，也可快速定位问题根源，判别是前段抽提、澄清损失，还是后段回收效率所致。</p><p>这意味着，问题有望在“扩大之前”就被识别和处理，从而使生产运行从被动应对逐步转向主动预防与控制。</p><p>总体而言，数据情景化将帮助制糖企业真正把数据用活于业务。生产管理将不再高度依赖个人经验与事后分析，而是逐步形成一套以数据为驱动、以业务场景为依托的快速决策机制，生产运行也因此有望变得更加稳定、高效与可控。</p><h3>无问智推｜AI 驱动的生产洞察升级</h3><p>在实际生产中，制糖行业逐渐形成一种共识：AI 技术在其中的真正价值，并非在于“替代人工思考”，而在于能够<strong>在问题尚未被明确提出之前，就已将所需的相关信息与洞察准备就绪</strong>。</p><p>过去，行业中的中控系统更多地扮演着“被动工具”的角色。监控哪些指标、如何进行关联分析，完全依赖当班人员的个人经验：工艺人员需自行回忆关键指标、查找数据点位、调整分析的时间窗口。新接班的团队往往难以快速入手；而当经验丰富的老师傅不在场时，许多隐性的工艺逻辑与判断也难以得到有效复用。</p><p>在引入 TDengine IDMP 平台并完成数据情景化构建之后，AI 所扮演的角色将发生显著转变。它将不再被动等待指令，而是基于对工艺语义与业务上下文的理解，主动识别当前生产状态，并动态<strong>推荐最贴合该业务场景的监控视图与分析内容</strong>。这使得系统能够引导注意力，辅助不同经验层次的人员更快地聚焦于关键问题，从而<strong>将专家经验转化为可持续、可复用的系统能力</strong>。</p><h4>澄清段的一个真实场景</h4><p>以澄清工段的澄清汁监控为例。过去，制糖行业在监控澄清段时，往往仅限于观察几条关键参数的实时曲线，难以系统性地判断“当前工况是否真正处于正常状态”或“其趋势是否正在恶化”。</p><p>现在，AI 会自动为用户推荐一整套<strong>符合澄清工艺逻辑的监控面板</strong>，只需简单的点击“生成”，TDengine IDMP 就能够自动生成监控看板。在澄清汁场景下，系统会优先推荐：</p><ul><li> 过去一小时澄清汁 pH 的最新值，用于快速判断当前反应状态；</li><li> 过去一天每小时澄清汁锤度的平均值，帮助用户观察短周期稳定性；</li><li> 过去一周澄清汁还原糖的平均值，以及按天汇总的变化趋势，用于评估澄清效果对糖损的影响。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558072" alt="AI 推荐的澄清汁的监控面板" title="AI 推荐的澄清汁的监控面板" loading="lazy"/></p><p>这些内容并不是“通用模板”，而是因为系统已经理解：<strong>这些指标正是澄清段最关键、最有业务意义的数据组合</strong>。</p><h4>从“人盯数据”到“系统叫人”</h4><p>在引入 TDengine IDMP 之前，制糖行业对澄清段的监控更多依赖人工经验。中控画面上曲线一直在动，工艺人员需要长时间盯着趋势，凭感觉判断是不是“有点不对劲”。采用 TDengine IDMP 之后，这种状态发生了明显改变。基于已经完成的数据情景化，AI 不再等待人工提问，而是<strong>主动推荐与澄清汁相关的实时事件监控和分析</strong>，通过实时分析预警，能够在关键时刻把人“叫过来”。</p><p>在澄清汁场景中，系统能够自动推荐分析：</p><ul><li>当澄清汁加热器出口温度超过 105℃，并持续 5 分钟以上时，立即触发主要告警，同时给出该时段的平均出口温度，清楚提示存在过热风险；</li><li>对澄清汁锤度，系统每 5 分钟基于 3 倍标准差的 K-sigma 方法进行异常检测，一旦波动异常，直接给出最大锤度值，帮助用户快速判断异常程度；</li><li>系统还推荐每 10 分钟滚动计算过去 30 分钟内的平均流量，用于辅助判断当前负荷是否发生变化。</li></ul><p>在过去，这些判断逻辑往往只掌握在少数经验丰富的工艺人员手中，依赖于人员持续盯守数据、反复比对分析才能得以运用。如今，通过引入 TDengine IDMP 平台，这些经验与逻辑得以被 AI 沉淀并固化为持续、自动运行的系统能力。生产管理模式由此从依赖“人盯数据”逐步转向为“系统预警、人员确认”的协同机制，使异常得以更早被识别，工艺调整也能更及时地执行。这正是 TDengine IDMP 为制糖行业生产管理带来的最直观价值——<strong>将隐性知识显性化，将个人经验转化为可持续、可复用的系统智能。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558073" alt="AI 自动推荐的实时分析场景" title="AI 自动推荐的实时分析场景" loading="lazy"/></p><h4>给制糖行业带来的真正变化</h4><p>对制糖行业来说，最大的变化在于：<strong>正常时不被数据打扰，异常时绝不会被遗漏。</strong><br/>生产管理也由此从“人盯数据”转向“系统叫人”，让异常更早被发现，让调整更及时发生。这正是 TDengine IDMP 在实际生产中带给制糖行业的最直观价值。</p><h2>应用成效｜从“系统上线”到“价值落地”</h2><p>随着该工业数据平台在生产现场的深入部署与应用，制糖企业有望在生产管理与工艺管控方面逐步收获系统性成效。整体解决方案围绕生产、工艺和设备三大核心对象展开，将推动数据不再仅仅停留在系统层面，而是持续融入日常运行与管理决策之中。</p><h3>全流程生产监控：让制糖过程“看得见”</h3><p>通过对制糖工艺流程进行统一的数据资产建模，平台实现了从预处理到干燥包装的全过程数据采集与集中监控。各工段之间原本割裂的数据被打通，形成连续、完整的生产视图。关键工艺参数和运行状态能够集中呈现，为现场管理、生产调度以及异常发现提供了直观、统一的支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558074" alt="" title="" loading="lazy"/></p><h3>生成物料损耗分析：让损耗“算得清”</h3><p>围绕工艺过程和物料流转，平台引入了系统化的数据分析与物料衡算方法，对糖分在关键环节中的变化进行结构化分析，使以往主要依赖经验判断的物料损耗问题，转变为可量化、可对比的结果。生产、工艺和设备状态对管理层更加透明，为工艺优化和质量管控提供了更有依据的决策支持。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558075" alt="各个工艺段制糖损耗分析" title="各个工艺段制糖损耗分析" loading="lazy"/></p><h3>工艺质量实时监控：让生产“跑得稳”</h3><p>围绕关键工艺参数和质量指标，平台构建了持续运行的工艺质量监控体系，对生产各环节的运行状态进行实时跟踪和对比分析，使工艺波动由事后发现逐步转变为过程可控。通过对工艺偏差和异常趋势的及时识别，有效降低了过程波动对产品质量的影响，推动生产运行保持稳定。</p><p>工艺质量状态在生产层和管理层之间更加透明，为工艺调整和质量管控提供了持续、可靠的数据依据，有效支撑制糖生产的稳定运行和产品质量的均质化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558076" alt="澄清汁 PH 值实时监控" title="澄清汁 PH 值实时监控" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558077" alt="工艺质量异常告警（澄清汁 PH 值）" title="工艺质量异常告警（澄清汁 PH 值）" loading="lazy"/></p><h2>商业价值｜制糖企业可持续演进的数字化底座</h2><p>从行业应用与发展的角度来看，此类项目的价值并不仅体现在一次性的系统建设或阶段性验收上，更在于为企业构建了一套可长期演进、持续赋能的数字化底座。通过统一的数据标准与平台架构，制糖行业首次获得了对全生产过程进行持续感知、系统分析与长效优化的能力，这为后续的管理深化与智能应用奠定了坚实基础。</p><p>短期而言，项目的实施将有效提升生产透明度与运行稳定性；从中长期看，该平台有望逐步成长为支撑企业实现稳产、提质、降本与风险精准管控的核心基础设施。</p><h2>行业意义｜一条稳健、可落地的制糖数字化路径</h2><h4>适用企业</h4><ul><li>希望持续提升管理水平和长期竞争力的甘蔗制糖企业</li><li>正处于数字化转型关键阶段的中小规模糖厂</li></ul><h4>成功前提</h4><ul><li>管理层对数字化目标和数据价值形成清晰、统一的认知</li><li>具备相对稳定、连续的生产和设备数据基础</li></ul><h4>核心路径</h4><ul><li>以“工艺 + 物料 + 设备”为主线，系统推进数字化建设</li><li>按“看得见 → 算得清 → 跑得稳”的节奏逐步实施，避免激进投入</li><li>在夯实数据基础之上，稳步迈向智能优化和 AI 应用</li></ul><h2>未来展望｜通过组态强化生产过程与工艺质量管控</h2><p>从预期效果来看，TDengine IDMP 将在生产数据采集、集中监控与分析方面为制糖企业打下坚实基础，从而有效支撑生产过程监控与工艺质量分析的日常需求。</p><p>在此基础上，企业可期待未来进一步引入并强化平台的组态能力，以更加直观、图形化的方式呈现工艺流程、设备运行状态与关键工艺参数。这将推动生产监控从以数据列表和图表展示为主，逐步升级为面向过程与运行状态的综合可视化管控。通过组态化配置关键质量指标和工艺约束条件，有助于将成熟的工艺经验固化为可自动执行的监控规则，提升对工艺偏差和质量风险的提前识别与主动干预能力，从而更好地服务于制糖生产长期、稳定、高效的运行目标。</p><h2>关于海莱德</h2><p>北京海莱德自动化工程有限公司成立于 2010 年，是国内工业自动化技术与解决方案提供商，在制糖行业自动化领域具有专业积累。公司业务覆盖系统设计、工程实施、调试及售后服务等全流程，并在食品饮料、汽车、电力、冶金、烟草和机械制造等行业积累了丰富工程经验。近年来，海莱德参与了多个“一带一路”糖厂的集中控制 DCS 系统及数字化系统的设计、供货与调试，持续推进从自动化向数字化、信息化和智能化方向升级，并结合涛思数据的时序数据库和 TDengine IDMP 平台建立起了对制糖企业真正高效、实用且易于掌握的，具备 AI 智能的数字化系统。</p>]]></description></item><item>    <title><![CDATA[再添钻石伙伴！TDengine 签约上海罗盘，共拓数据治理 + 时序存储新生态 TDengine涛思]]></title>    <link>https://segmentfault.com/a/1190000047558092</link>    <guid>https://segmentfault.com/a/1190000047558092</guid>    <pubDate>2026-01-22 13:02:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，涛思数据与上海罗盘信息科技有限公司（以下简称 “上海罗盘”）举行钻石分销商签约仪式，标志着双方正式达成深度战略合作，将依托各自在数据领域的核心优势，携手为金融、制造、政企等多行业客户提供 “数据治理 + 时序存储” 全链路解决方案，推动时序数据技术在更多场景中的落地应用。</p><p>涛思数据创始人兼 CEO 陶建辉、战略渠道与生态合作总监郭浩，上海罗盘董事长马力等双方核心团队成员出席签约仪式，共同见证这一重要合作时刻。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558094" alt="" title=""/></p><h2>优势互补，构建数据全生命周期服务闭环</h2><p>作为深耕数据领域 23 年的资深玩家，上海罗盘自 2002 年成立以来，始终聚焦数据治理与数据中台核心赛道，在全国布局分支机构、研发基地与交付中心，服务覆盖银行、证券、保险、制造等多个关键行业，已为 200 多家大型客户落地创新项目，与多家 500 强企业建立长期信任合作关系。凭借完善的解决方案、成熟的交付模式与多项自主知识产权，上海罗盘在数据资产梳理、质量管控、中台搭建等领域积累了深厚的行业经验与客户资源，成为国内数据管理领域的标杆企业。</p><p>而涛思数据自主研发的 TDengine 时序数据库（Timeseries Database），凭借 “读写性能超传统方案 10 倍以上、存储成本仅为 1/10” 的核心优势，以及信创认证、高并发支撑、轻量化部署等特性，已成为工业时序数据存储与分析的首选方案；同时，AI 原生的工业数据管理平台 TDengine IDMP 以首创的“无问智推”能力重塑工业数据的建模、治理与消费方式，推动企业加速迈向数字化与智能化。</p><p>此次合作，上海罗盘在数据治理与中台建设的前端优势，将与 TDengine 在时序数据存储、分析的核心技术形成完美互补，构建 “数据治理 - 中台整合 - 时序存储 - 智能分析” 的全生命周期服务闭环，为客户解决数据管理中的碎片化痛点，提供更高效、更完整的数字化转型支撑。</p><p>签约仪式上，涛思数据创始人&amp; CEO 陶建辉表示：“数据治理是数字化转型的基础，时序数据是工业互联网、物联网场景的核心资产，两者的深度融合是行业发展的必然趋势。上海罗盘在数据治理领域的 23 年沉淀与广泛客户资源，与 TDengine 的技术优势高度契合。此次钻石级合作，将进一步完善涛思数据的生态布局，让优质的时序数据技术通过成熟的服务体系触达更多行业客户，共同赋能千行百业的数字化升级。”</p><p>上海罗盘董事长马力对合作充满期待：“TDengine 作为国产时序数据库的领军品牌，其技术实力与市场口碑有口皆碑。上海罗盘深耕数据管理领域多年，深刻理解不同行业客户在数据全链路管理中的核心诉求。此次与涛思数据达成深度合作，将借助 TDengine 的核心技术补全时序数据存储与分析的关键环节，为客户提供更全面的数字化解决方案。期待双方在技术协同、市场推广、行业落地等方面实现共赢，共创数据价值新高度。</p><h2>生态聚力，共绘数字化转型新蓝图</h2><p>当前，数字化转型进入深水区，数据已成为企业核心生产要素，而时序数据作为物联网、工业互联网、金融风控等场景的关键数据类型，市场需求持续爆发。涛思数据始终坚持 “技术驱动 + 生态共建” 的战略，通过汇聚行业优质伙伴力量，构建优势互补、协同共赢的生态体系，让 TDengine 技术更快落地行业场景，为客户提供本地化、高效化的服务支持。</p><p>此次上海罗盘的加入，不仅为涛思数据生态注入了数据治理领域的强劲动能，更标志着 TDengine 钻石分销商矩阵正式成型！自分销商招募计划启动以来，涛思数据凭借全球领先的产品体系、开放共赢的合作理念，吸引了众多行业标杆企业加入，生态影响力持续扩大。</p><p>未来，涛思数据将继续深化与包括上海罗盘在内的所有生态伙伴的合作，在技术协同、方案共创、行业落地等方面持续发力，以更完整的产品服务链路、更深厚的行业落地能力，为千行百业的数字化转型提供核心支撑。</p>]]></description></item><item>    <title><![CDATA[AI赋能！TDengine IDMP工业数据管理平台助力化工研发创新 TDengine涛思数据 ]]></title>    <link>https://segmentfault.com/a/1190000047558097</link>    <guid>https://segmentfault.com/a/1190000047558097</guid>    <pubDate>2026-01-22 13:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 12 月，涛思数据与沈阳化工研究院（简称“沈阳院”）正式达成合作。涛思数据将为其提供 TDengine TSDB + IDMP 产品组合，通过部署工业数据管理平台，以 AI 原生的数据智能技术，支撑沈阳院构建覆盖从实验室研究到中试放大全流程的统一数据基座，助力其研发数字化转型迈向新阶段。</p><p>沈阳院是我国重要的综合性化工科研院所，其研发过程中涉及海量、多源的时序数据与非时序数据，同时其中试基地拥有多条专业化生产线。面对实验室、中试装置产生的庞杂数据，如何打破数据孤岛，实现数据的统一管理、关联分析与智能洞察，从而加速研发进程、优化生产工艺，是沈阳院数字化转型的重要任务。</p><p>随着数字化进程的推进，沈阳院需要一个能够打通从实验到中试全流程的数据管理平台，能够将时序数据与非时序数据（如物料信息、实验记录）进行关联分析，同时满足信创环境要求和数据安全规范。涛思数据全新发布的 <strong>TDengine IDMP</strong>（工业数据管理平台）产品，具备“<strong>无问智推</strong>”的 AI 原生能力，这种让数据主动说话的能力，正是解决业务人员依赖 IT 团队获取数据洞察的关键。</p><p>本次项目需要采集和分析来自实验室、中试生产线的数据，总计需要监控的<strong>测点约 2 万</strong>。这些数据来源于高压釜、干燥箱、色谱仪等实验设备，以及生产线的温度、压力、流量等工艺参数，还包括水电气等能耗数据。TDengine TSDB 支持多种数据接入方式，包括 MQTT、OPC-UA/DA 等。这对于研究院现有的数据采集系统（MQTT 和 OPC）非常重要。在数据建模方面，TDengine IDMP 采用树状层次结构，这与研究院的设备组织方式天然契合。比如，可以按照“研究院-中试基地-生产线-设备”的层级结构建立数据目录，每个节点都可以配置属性、分析规则和可视化面板。这种结构特别适合中试基地的批次分析需求，可以清晰地展示每个批次的工艺参数和质量指标。</p><p>本项目采用“整体规划、分步实施”的策略，项目计划分两阶段进行：</p><ul><li>第一阶段，选择 3 个实验室和 1 条中试生产线进行试点实施；</li><li>第二阶段，基于试点成果向全院范围推广。</li></ul><p>基于数据安全性和网络环境考虑，选择本地化部署方案。部署架构如下图所示：<br/><img width="723" height="397" referrerpolicy="no-referrer" src="/img/bVdnIce" alt="" title=""/></p><p>本次项目规划的设计思路紧密围绕化工研发的业务特点展开，力图在以下几大关键业务场景提升数据应用效率与深度：</p><ol><li><strong>数据全景可视化与智能告警</strong>：通过 TDengine IDMP 的智能可视化功能，实现实验数据和中试生产数据的全景可视化管理。研究人员无需 IT 支持即可通过自然语言交互获取所需数据视图；通过实时分析和事件管理功能，自动触发告警，并帮助研究人员快速定位问题根源；借助“无问智推”能力，自动推送质量波动的批次与标准参数的对比分析，帮助管理人员快速决策。</li><li><strong>工艺优化与批次对比分析</strong>：批次分析是中试生产的核心需求之一。借助 TDengine IDMP，可以实现多批次数据的自动对比分析。系统能够根据批次质量指标帮助科研人员找到"黄金批次"，并分析其工艺参数特征，为工艺优化提供数据支持。通过时序数据高级分析功能，研究人员可以轻松对比不同批次的差异，找出影响产品质量的关键工艺参数。</li><li><strong>预测性维护与能耗管理</strong>：基于 TDengine TDgpt 的能力，平台能够轻松集成时序数据的预测、异常检测、分类、补全、相关性分析等算法和模型，帮助客户实现对关键设备的实时监控与预测性维护。在中试基地的能耗管理方面，通过对水、电、气的实时监测与统计分析，帮助找出能效瓶颈、识别出能耗异常点，用以指导设备改造和工艺调整。</li><li><strong>数据驱动的工艺包开发</strong>：TDengine 产品组合将帮助研究院实现数据驱动的研发模式，提高工艺包开发的效率和质量。新工艺包的设计可以基于历史中试数据，确保工艺参数的可靠性。而工艺包转化为实际生产后，又可以通过对比设计数据与实际生产数据，持续优化工艺模型。同时，TDengine IDMP 内置了备份/恢复机制，未来还将支持 Git 式数据版本管理，有望进一步提高数据归档、变迁、回溯的能力。</li></ol><p>本次涛思数据与沈阳化工研究院的强强联合，为化工科研数据管理和数据分析描绘出更多可能性。相信此次合作不仅能提升沈阳院的研发效率，更有望探索出一条以数据智能驱动化工行业创新的可行路径。</p><h3>关于沈阳化工研究院</h3><p>沈阳化工研究院有限公司始建于 1949 年 1 月 8 日，是综合性化工科研院所，现为中国中化控股有限责任公司直管单位。目前沈阳院主要开展化工新材料、生态农业、生物化工、化学品测试与评价、化工反应风险评估、危险废物鉴别、化工智能优化等方向的研究及产业化。沈阳院聚焦提升关键共性技术的研究与开发能力、较强的新产品孵化能力和适度产业规模和盈利能力；致力于成为精细化工行业国内领先，国际有一定影响力的科技型企业。</p>]]></description></item><item>    <title><![CDATA[阶跃星辰开源多模态模型 Step3‑VL‑10B，小模型实现大模型能力；华为或将发布首款 AI 眼镜]]></title>    <link>https://segmentfault.com/a/1190000047558108</link>    <guid>https://segmentfault.com/a/1190000047558108</guid>    <pubDate>2026-01-22 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558110" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01有话题的技术</h2><p><strong>1、阶跃星辰开源 Step3‑VL‑10B：10B 模型对标 200B 能力</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558111" alt="" title="" loading="lazy"/></p><p>昨天，阶跃星辰宣布正式开源旗下 10B 参数量多模态模型 Step3‑VL‑10B。该模型在多项核心基准测试中达到同规模 SOTA 水平，部分能力甚至超越 10–20 倍体量的大模型。</p><p>Step3‑VL‑10B 主打「小模型实现大模型能力」，在视觉感知、逻辑推理、数学竞赛题、多模态对话等任务中表现突出。</p><p>阶跃星辰称，Step3‑VL‑10B 的性能已接近甚至超越部分百亿级开源模型（如 GLM‑4.6V 106B‑A12B、Qwen3‑VL‑Thinking 235B‑A22B），并在部分场景中达到顶级闭源旗舰模型（如 Gemini 2.5 Pro、Seed‑1.5‑VL）水平。</p><p>官方强调，该模型的关键突破来自三项核心设计：</p><ul><li><strong>全参数端到端多模态联合预训练</strong>：在 1.2T 高质量多模态数据上训练，实现视觉与语言的深度对齐；</li><li><strong>大规模多模态强化学习</strong>：经历超过 1,400 次迭代，使模型在识别、推理与对话能力上持续提升；</li><li><strong>并行协调推理机制</strong>：通过并行探索与证据聚合提升复杂任务的准确度，尤其在数学推理、OCR、计数与空间拓扑任务中效果显著。</li></ul><p>Step3‑VL‑10B 同时提供 SeRe（顺序推理）与 PaCoRe（并行推理）两种范式，覆盖 STEM 推理、OCR、GUI Grounding、空间理解与代码等多项能力维度。</p><p>当前，Step3‑VL‑10B 已开放 Base 与 Thinking 两个版本，社区可在 HuggingFace 与 ModelScope 获取模型并进行微调。</p><p>项目主页：<br/><a href="https://link.segmentfault.com/?enc=gQK0nod50ZMU7xahz9SMVw%3D%3D.eb%2FTnjHG7XLQncxIrjK2MBT26i5Fss5dsy%2BYIb32s4gj4X7AK2rCIbx2CUOwcBZj" rel="nofollow" target="_blank">https://stepfun-ai.github.io/Step3-VL-10B/</a></p><p>Hugging Face: <br/><a href="https://link.segmentfault.com/?enc=5op%2BnrliMKf%2BfXIob8jG%2BQ%3D%3D.kmEK3dSxBvmwmGsf1bWKSLFavWWC443252vKXjpwDAS4hr%2BddzJsr6OCWebpT%2FUkYtN7lf9pgqWgd9HFfEpN2g%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/collections/stepfun-ai/step3-vl-10b</a></p><p>ModelScope: <br/><a href="https://link.segmentfault.com/?enc=Zo22gGo1za%2FM42QlIUx2Ig%3D%3D.gVgplhmUVFdeTyzRQIQ0E0tInouekMN%2BOFRpKQ6Y1RKKp33k7R3%2BVpJTEm7UInuDvbobYa7RRWcJrO3r0duyzw%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/collections/stepfun-ai/Step3-VL-10B</a></p><p>论文链接：<br/><a href="https://link.segmentfault.com/?enc=hShAiScLaMbKWRRgvAtE7A%3D%3D.J3ZMiCrh6HYXHef0wNE%2FXamv5Z4VJXUguaCAtWw%2F%2BZhd06Co7qIFgLVMcwxuYUUg" rel="nofollow" target="_blank">https://arxiv.org/pdf/2601.09668</a></p><p>（@阶跃星辰、@APPSO）</p><p><strong>2、showlab 开源 whisperVideo：集成 SAM3 与 TalkNet 实现长视频「音视对齐」的说话人转录</strong></p><p>showlab 近期开源了名为 whisperVideo 的项目，专门致力于解决长视频场景下「谁在说话」的身份归属难题。该工具打破了传统方案仅依赖音频的局限，通过融合视听双重特征，实现了语音内容与画面特定人脸的精准对齐。</p><p>为了突破纯音频方案在多人混响或近距离交谈时常见的识别漂移问题，whisperVideo 构建了一套紧密的多模态级联架构。它集成了 WhisperX 负责语音转录、Pyannote.audio 处理声纹分离，并引入 SAM3 进行人脸分割以及 TalkNet 判定主动说话人。这种组合拳方式，确保了机器能像人类一样同时「听」和「看」，从而做出更准确的判断。</p><p>针对小时级素材中常见的跨场景挑战，工具特别引入了「长时身份一致性」机制。利用视觉嵌入与轨迹聚类技术，系统能在漫长的视频时间轴上记住每一张脸，确保同一说话人的 ID 在不同场景切换中始终保持稳定。</p><p>在工作流设计上，whisperVideo 追求全自动化体验。内置的 SceneDetect 能够自动进行场景切割与分段处理，无需人工干预即可完成时间戳、文本与视觉 ID 的三方对齐。最终生成的成果不仅包括带说话人 ID 的字幕，还支持可视化的面板模式，并将底层数据以 。pckl 格式开放给开发者。</p><p>目前，项目已在 GitHub 开源，需使用 CUDA GPU 环境，依赖 HuggingFace Token 调用 Diarization 模型，支持 Python 命令行一键推理。</p><p>GitHub: <br/><a href="https://link.segmentfault.com/?enc=2gryj89afY7wssArAKTKTg%3D%3D.Hsrj5fFNCx4%2BA7YP7rvyGvgg6CcC1KjImMHzKfYRB%2BnzP3tOYXPi09dGJ3ewJ0lO" rel="nofollow" target="_blank">https://github.com/showlab/whisperVideo</a></p><p>( @aigclink\@X)</p><p><strong>3、Bolna 获 630 万美元种子轮融资：自研 SLM 语音智能体，支持「印式英语」混说</strong></p><p>总部位于班加罗尔的初创公司「Bolna」近日完成了由 General Catalyst 领投的 630 万美元种子轮融资。这家公司致力于通过自研的专用小模型（SLM）技术，打破多语言环境下的自动化通信瓶颈。</p><p>为了适应印度极其复杂的语言生态，Bolna 构建的语音智能体不仅将端到端响应延迟控制在 500 毫秒以内，更实现了深度的本地化适配。它能够流畅处理包括印地语、泰米尔语在内的 10 余种本土语言及 50 多种地区口音，甚至针对印度特有的语言混合现象，专门优化了对「印式英语（Hinglish）」的语义理解与生成能力。</p><p>在技术架构上，Bolna 摒弃了昂贵的通用大模型方案，转而采用针对事务性查询优化的 SLM 与智能路由架构。这种策略有效平衡了计算成本与响应速度，使其更适合大规模商业落地。配合其提供的无代码控制台，企业可自主设计并监控智能体。目前，该平台的日呼叫处理量已从 1,500 通激增至 20 万通以上，广泛应用于购物车挽回、货到付款确认及招聘筛选等场景。</p><p>平台现已正式上线，主要面向印度企业提供订阅制的自助服务。</p><p>( @AI Tech Suite)</p><h2>02有亮点的产品</h2><p><strong>1、消息称华为首款 AI 眼镜将在上半年发布：搭载鸿蒙 OS，支持同传翻译与拍照</strong></p><p>1 月 20 日多家媒体消息，华为的第一款「AI 眼镜」暂定在今年上半年推出，支持拍照和音频，鸿蒙系统 + 跨端无缝协同，同传翻译等功能。 AI 眼镜被誉为「下一代 AI 终端超级入口」，已然是大厂必争之地，百度、小米、阿里、理想等早已进场，并推出了 AI 拍照眼镜，字节也即将推出 AI 眼镜，作为国内消费类智能终端龙头的华为自然不会落后于人。</p><p>据 @数码闲聊站 爆料，华为 AI 眼镜将采用鸿蒙 OS 系统与轻量化设计，内置 3 块锂电池，支持跨端无缝协同，进一步拓展使用场景。并提供流光银、钛银灰、摩登黑三款配色，支持拍照、拍视频、音频播放以及同声传译等功能。</p><p>虽然目前具体细节尚未公布，但结合华为在 AI 技术领域的探索，预计将内置华为 AI 助手小艺，产品可能涉及 AI 识物、智能场景推荐等功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558112" alt="" title="" loading="lazy"/></p><p>经查询发现，华为曾推出带有音频功能的智能眼镜，主打听音乐、打电话、健康播报等。如今随着 AI 的兴起，智能眼镜行业也纷纷上马 AI，以及自带摄像头、显示屏的 AI 眼镜也不断推新。</p><p>据 IDC 预测，智能眼镜产品成为 2025 年消费电子赛道的黑马，相应产品在中国市场出货量预计达到 290.7 万台，同比增长 121.1%。业内人士普遍认为，这缘于技术突破、市场需求释放以及产业链成熟等多重因素。</p><p>汇丰控股认为，智能眼镜市场仍处于加速扩张阶段。分析师预计，智能眼镜的用户规模将在未来十多年内迎来爆发式增长，到 2030 年代末将达到 2.89 亿人，较 2025 年的 1500 万用户增长超过 18 倍。</p><p>（@即智 Ultra、@IT 之家）</p><p><strong>2、MiniMax 推出「Agent 实习生」，AI-native Workspace 全面升级</strong></p><p>昨天，MiniMax 官宣，AI-native Workspace 迎来两项核心升级，进一步推动 AI 深度嵌入真实工作场景，并面向用户开放限时免费体验。</p><ul><li><strong>桌面端应用正式上线：</strong> 用户可在本地环境中指定 Workspace 作为工作空间与上下文，使 AI 能够直接理解本地文档、代码仓库、邮件与日程，从而构建一个专属于个人的智能工作环境。</li><li><strong>推出「专家 Agents」能力：</strong> 用户可构建在特定领域达到「95 分甚至 100 分」水平的专业智能体。这类 Agent 能够在复杂任务链路中稳定执行、主动判断并长期协作。</li></ul><p>公司内部数据显示，「Agent 实习生」在过去数周已被接近 100% 的员工使用，并在运维场景中承担了约 80% 的查 Bug 工作量。</p><p>MiniMax 表示，AI-native Workspace 标志着 Agent 从「被动执行指令」向「主动感知环境」的形态演进。</p><p>公司认为，未来的 Agent 将具备长期记忆、完整职业上下文与跨系统感知能力，成为用户的长期工作伙伴，而非一次性工具。</p><p>目前，MiniMax 已开启专家 Agents 的限时免费体验。用户可通过 Web 端直接试用，也可通过官方体验链接获取桌面端安装包。</p><p>体验地址：<br/><a href="https://link.segmentfault.com/?enc=I6nPxwLpIqCMjyhJBWIICw%3D%3D.HHQkLPkFfmobUemJQvVgx5TiZxGbLRxzhj8xxyLQLzw%3D" rel="nofollow" target="_blank">https://agent.minimaxi.com/</a></p><p>( @APPSO)</p><p><strong>3、Crow 发布 AI 智能体框架：支持 OpenAPI 与 MCP 协议，实现「对话即 UI」交互</strong></p><p>Crow 近期推出了一套专为 SaaS 产品打造的 AI 智能体基础设施，旨在通过「对话即 UI」的理念重构软件交互模式。该工具的核心逻辑在于将传统的点击操作转化为自然语言指令流，通过接入 OpenAPI 规范或 MCP 协议，使智能体不仅能回答问题，更能直接触发后端 API 调用及前端 UI 导航，从而实现对软件功能的深度控制。</p><p>为了解决生成式 AI 不可控的难题，Crow 引入了名为「Journeys」的结构化工作流。开发者可以针对取消订阅、创建报表等特定业务场景，定义确定性的引导路径，确保智能体在执行敏感操作时严格遵循预设的逻辑分支。配合支持文件与文档集成的 RAG 管道，智能体还能充分理解产品特定的业务逻辑与私有数据。</p><p>在开发与运维层面，Crow 提供了生产级的观测指标，能够详细追踪每一条指令对应的工具调用路径。其低代码部署方案仅需嵌入单行 Script 标签，官方宣称这能将传统长达半年以上的自研周期缩短至一周以内，并支持与 Claude Code 或 Cursor 等工具集成。目前该产品已正式上线，开发者项目可免费试用，同时针对中大型企业提供了定制化方案。</p><p>( @Y Combinator Launch)</p><p><strong>4、Thread 发布 Voice AI：实现 MSP 电话自动化分拣与实时工单同步，单人效能提升 30%</strong></p><p>Thread 宣布其专为托管服务提供商设计的 Voice AI 正式商用。该产品旨在终结传统 IVR（交互式语音应答）系统的僵化体验，通过语音智能体接管电话接入、分拣与派发的全流程，将高成本的电话渠道整合进结构化的自动化运维体系中。</p><p><strong>AI Attendant 与 Overflow Agent 双引擎驱动：</strong></p><ul><li><strong>AI Attendant</strong>：取代传统 IVR，能够即时接听电话并识别来电者身份。它不仅能进行自然的语音交互，还能在后台实时创建工单、匹配技术人员，并完成「热切换」，确保客户在转接给真人时无需重复复述问题。</li><li><strong>Overflow Agent</strong>：专为下班后或线路繁忙场景设计。它能拦截进入语音信箱的电话，自动收集关键信息并进行分类；遇到 P1 级紧急事件时，可直接升级并呼叫待命团队，消除了「下班后盲区」。</li></ul><p>Voice AI 的核心价值在于将非结构化的语音高效转化为结构化数据。系统不仅能根据通话内容自动填充工单的标题、类别、优先级和解决摘要，还引入了「自动时间条目」功能，可依据通话时长直接生成计费记录。据官方数据统计，这一特性为每张工单平均节省了 19 分钟的处理时间，从而推动单一技术人员的日均通话处理量从 8-12 通显著提升至 14-20 通。</p><p>在生态兼容性方面，该方案作为 Thread AI Service Desk 平台的重要组成部分，已与 ConnectWise、Autotask 和 HaloPSA 等主流 PSA 系统实现了原生集成。这意味着所有通话数据都会实时转化为结构化文档，并无缝同步至企业现有的工作流中，从而确保了整个服务链条的完整性与可追溯性。</p><p>据 Thread 统计，通过消除手动记录和人工轮班需求，该系统可使响应速度提升 5 倍，平均解决时间缩短 78%。目前该服务已正式上线。</p><p>相关链接：<br/><a href="https://link.segmentfault.com/?enc=nOwKqElwmEHnC4Pt5Cwb5w%3D%3D.pTtohgfX4CTHHC8ljusOHEFS6T3YcrRESxLScfzXNOsMTcJdMpgYTpo%2Bxv2LZgsk" rel="nofollow" target="_blank">https://www.getthread.com/voice-ai</a></p><p>( @Mansfield News Journal)</p><h2>03有态度的观点</h2><p><strong>1、谷歌前 CEO 施密特：欧洲要么投资开源 AI，要么依赖中国模型</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558113" alt="" title="" loading="lazy"/></p><p>1 月 20 日，据外媒报道，谷歌前 CEO、科技投资人埃里克 · 施密特 （Eric Schmidt） 周二表示，<strong>欧洲必须投资建设自己的开源 AI 实验室，并解决能源价格飙升的问题，否则很快就会发现自己对中国的模型产生依赖。</strong> 施密特周二在达沃斯世界经济论坛表示：「在美国，企业基本上正在转向闭源，这意味着这些技术将被购买、授权等等。而与此同时，中国在做法上基本是开放权重、开源的。除非欧洲愿意为欧洲自己的模型投入大量资金，否则欧洲最终将会使用中国的模型。」</p><p>目前，许多热门 AI 模型都是闭源的，比如谷歌的 Gemini 和 OpenAI 的 ChatGPT，这意味着这些公司不会向外界提供底层代码供下载或审查。虽然这种方式能为用户带来更顺畅、更统一的使用体验，但通常成本更高、灵活性也更低。中国在所谓「开放权重」模型的开发方面处于领先地位，这类模型具有更高的透明度。</p><p>为了在开发更强大 AI 模型和智能体的全球竞赛中具备竞争力，欧洲还需要解决高企的能源价格问题，并建设更多可用于训练这些技术的数据中心。施密特曾联合创办一家数据中心公司，致力于应对这类基础设施巨大的能源需求。他也对美国 AI 发展对电力供应的影响表示担忧。</p><p>（@IT 之家）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558114" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558115" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=fixs3xAKfdvLACDGOdaE9A%3D%3D.SEjbj4AwR2Zz8yYTwCZ%2BBsbBwuoIy1ZcQMgoxGkXX4Y%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558116" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[如何通过Java SDK新建Client DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047557765</link>    <guid>https://segmentfault.com/a/1190000047557765</guid>    <pubDate>2026-01-22 12:06:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文介绍如何通过Java SDK新建一个DashVector Client。</p><p><strong>说明</strong></p><p>通过DashVector Client可连接DashVector服务端，进行Collection相关操作。</p><h2>前提条件</h2><ul><li>已创建Cluster</li><li>已获得API-KEY</li><li>已安装最新版SDK</li></ul><h2>接口定义</h2><p>Java示例：</p><pre><code class="java">package com.aliyun.dashvector;

// 通过apiKey和endpoint构造
DashVectorClient(String apiKey, String endpoint);

// 通过DashVectorClientConfig构造
DashVectorClient(DashVectorClientConfig config);</code></pre><h2><strong>使用示例</strong></h2><p><strong>说明</strong></p><p>需要使用您的api-key替换示例中的YOUR_API_KEY、您的Cluster Endpoint替换示例中的YOUR_CLUSTER_ENDPOINT，代码才能正常运行。</p><p>Java示例：</p><pre><code class="java">import com.aliyun.dashvector.DashVectorClient;
import com.aliyun.dashvector.DashVectorClientConfig;
import com.aliyun.dashvector.common.DashVectorException;

public class Main {
    public static void main(String[] args) throws DashVectorException {
        // 通过apiKey和endpoint构造
        DashVectorClient client = new DashVectorClient("YOUR_API_KEY", "YOUR_CLUSTER_ENDPOINT");
      
        // 通过Builder构造DashVectorClientConfig
        DashVectorClientConfig config = DashVectorClientConfig.builder()
            .apiKey("YOUR_API_KEY")
            .endpoint("YOUR_CLUSTER_ENDPOINT")
            .timeout(10f)
            .build();
        client = new DashVectorClient(config);
    }
}</code></pre><p><strong>说明</strong></p><p>DashVectorClient初始化期间可能抛出<code>DashVectorException</code>异常，可通过具体异常信息分析初始化失败原因。</p>]]></description></item><item>    <title><![CDATA[供应商管理系统有哪些？谈谈我们测评的这8款 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047557793</link>    <guid>https://segmentfault.com/a/1190000047557793</guid>    <pubDate>2026-01-22 12:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>供应商来对账，数据对不上，耽误好几天；采购价格不透明，成本居高不下；供应商绩效全凭印象，合作质量参差不齐...如果你也正被这些问题困扰，是时候了解一下供应商管理系统了。</p><p>今天我们就来一次深度测评，聊聊市面上主流的几款供应商管理解决方案，帮你找到最适合自家业务的那一款。</p><p><strong>一、选型要点：好的系统，到底该看什么？</strong></p><p>在直接推荐产品前，先明确几个核心选型标准，这是避开坑的关键。</p><p><strong>第一，要看它能不能解决你真实的痛点</strong>。很多企业痛点很具体：比如采购流程不规范、线上线下数据对不上、供应商质量不稳定、对账周期漫长等。系统功能是否直击这些要害，是首要考量。</p><p><strong>第二，灵活性和扩展性至关重要</strong>。特别是成长型企业，业务变化快，今天用的功能明天可能就要调整。如果系统僵硬，改个流程都要找原厂花大价钱二开，那用起来会很痛苦。所以，是否支持一定程度的自定义或低代码调整，是个加分项。</p><p><strong>第三，性价比和长期投入成本</strong>。这不单指软件本身的购买费用，还包括实施费用、每年的维护费、未来需求变化的二次开发成本，甚至数据迁移的成本。一个“买得起但用不起”的系统，不如一开始就放弃。</p><p><strong>第四，厂商的服务与可持续性</strong>。软件即服务，后续的响应速度、问题解决能力、版本迭代计划，都直接影响你的使用体验。选择有成熟服务团队、产品持续迭代的厂商，更稳妥。</p><p>基于以上几点，结合市场主流选择，我筛选出 <strong>8款值得深入考察的供应商管理系统</strong>，并对其核心特点、适用场景进行分析。</p><p><strong>二、测评盘点：8款主流供应商管理系统</strong></p><p><strong>1. 支道</strong></p><p><a href="https://link.segmentfault.com/?enc=cqxrxxRY6Nd9FS%2By%2FFYk1g%3D%3D.J8SfPTlsZ8fW%2Fj1ATN1YfIPpjy3Z2XE7wLWGeYzKzZo%3D" rel="nofollow" target="_blank">https://www.zdsztech.com</a></p><p><strong>核心特点</strong>：基于无代码平台构建，高度可定制</p><p>如果要用一个词形容支道的供应商管理方案，那就是 <strong>“灵活”</strong>。它并非一个功能固化的标准产品，而是基于其强大的无代码开发平台，能够快速搭建出贴合企业实际采购业务流程的系统。</p><p>从测评角度看，它的优势很明显：<strong>可视化搭建，改起来方便</strong>。</p><p>企业的采购审批流程、供应商准入标准、询比价模板，都可以通过拖拉拽的方式配置和修改，业务人员经过培训也能参与调整。这解决了很多企业“需求说不清、软件改不动”的痛点。</p><p>具体到SRM功能上，它覆盖了供应商全生命周期管理：供应商电子档案、在线准入申请、询价/招标/比价流程、采购订单协同、送货与验收协同、对账付款、以及供应商绩效评估。</p><p>亮点在于流程的在线化和自动化，比如报价自动汇总比价、订单状态自动同步给供应商、绩效数据自动采集计算等。</p><p><strong>适合谁用</strong>：业务独特、流程经常优化、或者未来可能将SRM与内部CRM、项目管理系统打通的成长型企业。它的无代码特性让长期迭代成本更低。</p><p><strong>需要注意</strong>：高度灵活也意味着初期需要更多的业务梳理和配置投入，更适合愿意在管理梳理上花时间、追求长期适配性的企业。<br/><img width="723" height="292" referrerpolicy="no-referrer" src="/img/bVdnH6W" alt="" title=""/></p><p><strong>2. 用友</strong></p><p><strong>核心特点</strong>：与ERP、财务系统天然集成，业财一体化能力强</p><p>用友作为国内企业管理软件的老牌厂商，其YonSuite中的SRM模块最大优势在于 “集成”。如果你的企业已经在使用或用友的ERP、财务系统，那么选择它的SRM模块，在数据打通上会非常顺畅。</p><p>采购订单直接生成应付、入库信息实时同步、成本数据自动归集，真正实现业务流、信息流、资金流合一。</p><p>功能层面，它提供标准的供应商管理、寻源管理、采购协同、库存协同等功能。在供应商绩效方面，支持多维度指标（如质量、交期、价格、服务）的量化评估。</p><p><strong>适合谁用</strong>：尤其是那些已经使用用友体系产品的中大型企业，或者对财务业务一体化要求极高、希望杜绝数据孤岛的企业。</p><p><strong>需要注意</strong>：作为标准化程度较高的产品，在面对一些非常规的、行业特有的采购流程时，可能需要通过二次开发来实现，成本和周期需提前评估。<br/><img width="723" height="285" referrerpolicy="no-referrer" src="/img/bVdnH6X" alt="" title="" loading="lazy"/></p><p><strong>3. 金蝶</strong></p><p><strong>核心特点</strong>：强调供应链协同，尤其在生产制造领域有深度方案</p><p>金蝶的云星空SRM，在制造业企业中口碑不错。它的设计思路强调 <strong>“供应链协同”</strong> ，不止管理供应商，更注重与供应商之间的高效协作。比如，支持供应商门户，让供应商自助查看订单、确认交期、填报送货单；支持与生产计划的联动，实现采购需求的精准触发。</p><p>其功能亮点在于对 VMI库存管理、JIT准时化采购、寄售业务 等复杂场景的支持，这些都是制造企业的核心痛点。在供应商风险方面，也提供了诸如资质预警、交期预警等管理功能。</p><p><strong>适合谁用</strong>：生产制造型企业，特别是对原材料采购协同、精益生产有要求的企业。也适合金蝶ERP的老用户，保障系统连贯性。</p><p><strong>需要注意</strong>：方案相对偏向中大型制造企业，对于贸易类、项目服务类企业的贴合度可能需要详细验证。<br/><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdnH68" alt="" title="" loading="lazy"/></p><p><strong>4. SAP</strong></p><p><strong>核心特点</strong>：全球化、战略寻源、网络化协同</p><p>SAP Ariba 是全球领先的采购云平台，它的定位更高，更像一个 <strong>“采购网络”</strong>。其核心优势在于 <strong>全球寻源和战略采购</strong>。如果你的企业采购范围遍布全球，需要管理跨国供应商、进行复杂的招标和合同管理，Ariba 提供了强大的支持。它拥有庞大的供应商网络，方便发现新供应商。</p><p>功能极其全面，从支出分析、寻源招标、合同管理、到供应商协同、发票与付款，覆盖整个直接和间接采购流程。其数据分析能力强大，能帮助企业深度洞察采购支出，优化采购策略。</p><p><strong>适合谁用</strong>：大型集团企业、跨国公司，或者采购品类复杂、将采购视为战略职能的企业。预算充足是前提。</p><p><strong>需要注意</strong>：实施和运维成本非常高，系统复杂，对内部管理规范性和团队能力要求极高。对于中小型企业来说，可能“杀鸡用牛刀”。<br/><img width="723" height="269" referrerpolicy="no-referrer" src="/img/bVdnH7e" alt="" title="" loading="lazy"/></p><p><strong>5. 甄云</strong></p><p><strong>核心特点</strong>：产品化程度高，开箱即用，聚焦采购全流程数字化</p><p>甄云是国内较早专注于采购数字化SRM的厂商之一。其产品特点是 “全流程、产品化” ，功能模块成熟，设计理念清晰。它围绕企业采购业务，提供从供应商管理、寻源管理、采购协同、到财务协同的完整闭环。用户体验和界面设计比较现代化，易于上手。</p><p>在供应商风险管控方面，它整合了外部大数据，可以提供供应商的工商、司法、舆情等多维度风险监控和预警，这是个很实用的亮点。</p><p><strong>适合谁用</strong>：希望快速部署一套成熟、完整SRM系统的中大型企业，特别是对供应商风险有主动管理需求的企业。它降低了从零自研的风险和成本。</p><p><strong>需要注意</strong>：作为标准化SaaS产品，在应对极端个性化的业务流程时，灵活性可能不如低代码/无代码平台。<br/><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdnH7f" alt="" title="" loading="lazy"/></p><p><strong>6. 携客云</strong></p><p><strong>核心特点</strong>：SaaS模式，轻量化，以“协同”为核心，实施快</p><p>携客云主打 “轻量化、易实施” 的SaaS SRM。它的核心价值在于快速解决制造企业与供应商之间的 “协同效率” 问题，比如订单确认、交货、对账等高频场景。</p><p>它的供应商门户做得很轻便，供应商上手门槛低。通过它，企业可以快速实现采购订单发布、送货预约、质量反馈、对账确认等业务的在线化，显著减少打电话、发邮件的低效沟通。</p><p><strong>适合谁用</strong>：广大中小制造企业，作为ERP的延伸，首要解决与供应商的日常业务协同问题。需求明确、预算有限、希望快速上线看到效果的企业可以重点关注。</p><p><strong>需要注意</strong>：在战略寻源、深度供应商绩效分析、复杂业务流程管控等更深层的管理需求上，功能可能不如前面几款全面。<br/><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnH7i" alt="" title="" loading="lazy"/></p><p><strong>7. 企企通</strong></p><p><strong>核心特点</strong>：平台化思路，强调连接与生态</p><p>企企通的SRM平台同样强调协同，但其特色在于 “平台化” 和 “连接能力” 。它致力于成为连接采购方和供应商的协作平台。除了常规的SRM功能外，它在 非生产性物料采购、电商化采购 方面有特色方案，支持企业搭建内部采购商城。它也具备较强的集成能力，可以与企业内部ERP、OA等系统对接，实现流程和数据贯通。</p><p><strong>适合谁用</strong>：注重与供应商建立在线化协作生态，特别是间接物料采购（MRO）需求旺盛的大中型企业。也适合希望整合分散采购渠道的企业。</p><p><strong>需要注意</strong>：平台的综合性强，企业需要明确自身核心需求是“管理”还是“连接协同”，以便判断是否匹配。<br/><img width="723" height="349" referrerpolicy="no-referrer" src="/img/bVdnH7j" alt="" title="" loading="lazy"/><br/><strong>8. 浪潮云</strong></p><p><strong>核心特点</strong>：贴合大型集团管控需求，尤其在高安全要求行业有积累</p><p>浪潮的云ERP中包含SRM解决方案，其优势在于服务 <strong>大型集团企业、国有企业</strong> 的经验。在供应商集中管控、分级管理、采购合规性、审计追溯等方面有较深的设计。对于有严格内控和合规性要求的行业，如国资、军工等，是重点考察对象。</p><p>功能上，支持集中采购、分散采购等多种模式，与浪潮的财务、预算系统也能深度集成。</p><p><strong>适合谁用</strong>：大型集团、国有企业、对采购合规性和集中管控有刚性要求的组织。</p><p><strong>需要注意</strong>：产品和实施风格相对“稳重”，在用户体验和敏捷性上可能不是其首要追求。<br/><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdnH7k" alt="" title="" loading="lazy"/></p><p><strong>三、总结与建议：如何选择？</strong></p><p>看了一圈，你可能更纠结了。别急，最后给你一些落地的建议：</p><p>如果业务<strong>灵活多变</strong>，<strong>支道</strong>这类无代码平台的长远适配性更好。预算不仅要看首次投入，更要评估3-5年的总拥有成本。并且一定要<strong>看演示、做试点，</strong>功能列表都是美好的，真实体验才能暴露问题。要求厂商用你的真实数据（脱敏后）或模拟场景进行演示。条件允许的话，选择一个非核心采购品类或一个分子公司进行试点，这是最有效的试金石。</p><p><strong>供应商管理系统</strong>的选型，没有“最好”，只有“最适合”。它不仅是采购工具，更是企业供应链竞争力的数字化体现。</p><p>花时间厘清自身需求，结合以上测评信息，相信你能找到最适合自己提升管理效率、降低运营成本的优秀系统。</p>]]></description></item><item>    <title><![CDATA[Unity实现Nanite 本文系转载，阅读原文
https://zhuanlan.zhihu.co]]></title>    <link>https://segmentfault.com/a/1190000047557816</link>    <guid>https://segmentfault.com/a/1190000047557816</guid>    <pubDate>2026-01-22 12:05:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>【USparkle专栏】如果你深怀绝技，爱“搞点研究”，乐于分享也博采众长，我们期待你的加入，让智慧的火花碰撞交织，让知识的传递生生不息！</p><hr/><blockquote><h3><strong>一、前序</strong></h3></blockquote><p><strong>1. 介绍</strong><br/>Nanite是UE5中虚拟几何体（Virtualized Geometry System）的系统，主要用途是高效率渲染的高面数模型。Nanite会为模型自动生成LOD结构，与传统LOD不同，Nanite的LOD不再是每个模型的，而是精细到模型中的局部区域，艺术家不需再为制作或处理LOD烦恼。并且还能享有GPU Driven的高效剔除，单个绘制调用的好处。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557818" alt="" title=""/></p><p><strong>2. 技术要点</strong><br/>Nanite技术结合了多种技术做到了高效渲染：</p><ol><li>Cluster Rendering：由Cluster组织三角形，可以享有更高效的剔除。</li><li>Auto LOD：通过Graph Partitioning技术划分和简化模型构建LOD，并且把数据组织成BVH结构在Runtime时候可以高效地并行选择LOD，通过这种方式构建的LOD过渡非常丝滑。</li><li>GPU Driven Pipeline：由GPU驱动的绘制，减少了CPU的性能开销。</li><li>Occlusion Culling：更细颗粒的遮挡剔除，用于剔除不可见的三角形。</li><li>Hardware/Software Rasterization：由于小三角形对于硬件光栅化非常不友好，所以针对这些三角形用Compute Shader执行软光栅提高效率。</li><li>Visibility Buffer：利用Visibility Buffer减少Overdraw，进一步提高GPU效率。</li><li>Streaming：加载只看到的相关数据，减少几何体对内存的压力。</li></ol><p><strong>3. 本文效果</strong><br/>由于Nanite系统非常庞大和有非常多的工程细节要处理，所以本文会简化和略过一些东西，仅实现核心部分，而且会与有UE5的版本有点出入。</p><p>下图是本文实现的效果，每个色块是一个三角形，可以看出LOD切换和相机剔除都非常丝滑。</p><p>色块表示三角面<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557819" alt="" title="" loading="lazy"/></p><p>色块表示Cluster<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557820" alt="" title="" loading="lazy"/></p><blockquote><h3><strong>二、实现</strong></h3></blockquote><p><strong>1. Clusterize</strong><br/>第一步，在离线阶段处理，将复杂的超高精度网格模型高效且合理地分割成更小、更易于管理的簇（Cluster），每个Cluster最多128个三角形。这种划分不是简单的切割，而是旨在最小化簇与簇之间连接的边数（即切割大小），同时保持每个簇的大小大致均衡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557821" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557822" alt="" title="" loading="lazy"/></p><p>UE使用的Partition是Metis库：<br/><a href="https://link.segmentfault.com/?enc=jFIama9AHxGwqfiSLc9oKw%3D%3D.qnhGN7PR8sH5OMg2R7RLc80vUsRaVgJh3DPzoXFzrrbQsJe%2FWE0fg7lXlwO18Hj%2F" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=dbZWfgBHI4KBuhG3F1vizw%3D%3D.1O5dl3Q1mIM2Fld8oq69ba9LJKFdjhH9jXavE58Zie6LgrTUT990v0BueGQJexat" rel="nofollow" target="_blank">https://github.com/KarypisLab/METIS</a></p><p>实现代码可以参考UE5的源码部分：<br/>UnrealEngine-release\Engine\Source\Developer\NaniteBuilder\Private\NaniteBuilder.cpp</p><p>本文使用meshoptimizer实现Mesh的切分Cluster和Partition功能，这个库功能还有优化Over Draw，Shadow Depth Index等功能：<br/><a href="https://link.segmentfault.com/?enc=I%2Fm5xna8XvlVITdYshbUvQ%3D%3D.%2F5KpcQ9rYEByP5%2BS%2B1MBhPM6%2Fp83jKpR%2BAEN95v5FR4M0Ltq8oLlXYmbY1ykmeTV" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=RWVNXT89UHeL%2FOJsmIAVHg%3D%3D.Ya4jEXTaMlTlWXJHgktb1RyWwMNEDXzmsreIc%2BE9LU74rgT9rh1TVHzagco0oiuC" rel="nofollow" target="_blank">https://github.com/zeux/meshoptimizer</a></p><p>我们新建一个C++导出DLL的工程，封装几个主要函数让Unity可以使用。其实代码量不多，翻译成C#直接用也可以。</p><p>分别是：</p><ul><li>meshopt_buildMeshlets（构建Cluster）</li><li>meshopt_partitionClusters（Cluster划分Partition）</li><li>meshopt_buildMeshletsBound（计算Cluster数量）</li><li>meshopt_computeSphereBounds（合并BoundsSphere）</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557823" alt="" title="" loading="lazy"/></p><p>在C#中引用这些函数：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557824" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557825" alt="" title="" loading="lazy"/></p><pre><code>unsafe static List&lt;Cluster&gt; clusterize(Vector3[] vertices, int[] indices)
    {
        constint max_vertices = 192; // TODO: depends on kClusterSize, also may want to dial down for mesh shaders
        constint max_triangles = kClusterSize; //128
        constint min_triangles = (kClusterSize / 3) &amp; ~3;
        constfloat split_factor = 2.0f;
        constfloat fill_weight = 0.75f;
        int max_meshlets = BuildMeshletsBound(indices.Length, max_vertices, max_triangles);//meshopt_buildMeshletsBound 
        var meshlets = new Meshlet[max_meshlets * 2];
        var meshlet_vertices = newint[max_meshlets * max_vertices];
        var meshlet_triangles = newbyte[max_meshlets * max_triangles * 3];
        var meshlet_count = BuildMeshletFlex(meshlets, meshlet_vertices, meshlet_triangles, indices, indices.Length, vertices, vertices.Length, sizeof(float) * 3, max_vertices, min_triangles, max_triangles, 0.0f,
            split_factor);//meshopt_buildMeshlets 
        List&lt;Cluster&gt; clusters = new List&lt;Cluster&gt;(meshlet_count);
        for (int i = 0; i &lt; meshlet_count; i++)
        {
            ref Meshlet meshlet = ref meshlets[i];
            fixed (int* ptr = &amp;meshlet_vertices[meshlet.vertex_offset])
            {
                fixed (byte* ptr2 = &amp;meshlet_triangles[meshlet.triangle_offset])
                {
                    OptimizeMeshlet(ptr, ptr2, (int)meshlet.triangle_count, (int)meshlet.vertex_count);
                }
            }

            Cluster cluster = new Cluster();
            cluster.indices = newint[meshlet.triangle_count * 3];
            for (int j = 0; j &lt; meshlet.triangle_count * 3; ++j)
                cluster.indices[j] =
                    meshlet_vertices[meshlet.vertex_offset + meshlet_triangles[meshlet.triangle_offset + j]];

            cluster.parent.error = float.MaxValue;
            clusters.Add(cluster);
        }

        return clusters;
    }</code></pre><p>然后可以直接通过meshopt_buildMeshlets函数，获得每个cluster的indexs。</p><p><strong>2. Build DAG</strong><br/>有了这些Cluster，就可以构建“LOD”了，只需要循环这个操作：打组-&gt;合并-&gt;减面-&gt;clusterize。如下图：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557826" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557827" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557828" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557829" alt="" title="" loading="lazy"/></p><p>这个过程感觉就像Mipmap一样，一层一层往上合并和简化，并记录一个Err误差值和Bounds用于运行时LOD选择用。而这些合并的的节点就叫做Cluster Group。最后得出一个DAG（有向无环图，Directed Acyclic Graph）的结构。</p><pre><code>public struct ClusterGroup
    {
        public List&lt;int&gt; Children;
        public Vector3 Bounds;
        publicfloat radius;
        public Vector3 LODBounds;
        publicfloat MinLODError;
        publicfloat MaxParentLODError;
        publicint MipLevel;
    } 

publicclassNaniteSubMesh
    {
        public List&lt;ClusterGroup&gt; clusterGroupList;
        public List&lt;Cluster&gt; clusterList;
        publicint maxMipLevel;
    }

static NaniteSubMesh Nanite(Vector3[] vertices,Vector3[] normals, int[] indices)
    {
        NaniteSubMesh res = new NaniteSubMesh();
        List&lt;ClusterGroup&gt; clusterGroupList = new List&lt;ClusterGroup&gt;();
        var clusters = clusterize(vertices, indices);
        res.clusterList = clusters;
        res.clusterGroupList = clusterGroupList;
        res.maxMipLevel = 0;
        for (int i = 0; i &lt; clusters.Count; ++i)
        {
            var c = clusters[i];
            c.self = Bounds(vertices, clusters[i].indices, 0f);
            c.mip = 0;
            clusters[i] = c;
        }

        List&lt;int&gt; pending = new List&lt;int&gt;(clusters.Count);
        int[] remap = newint[vertices.Length];
        for (int i = 0; i &lt; remap.Length; ++i)
            remap[i] = i;
        for (int i = 0; i &lt; clusters.Count; ++i)
            pending.Add(i);

        int curMip = 1;
        byte[] locks = newbyte[vertices.Length];
        while (pending.Count &gt; 1)
        {
            List&lt;List&lt;int&gt;&gt; groups = partition(clusters, pending, remap, vertices);
            if (kUseLocks)
                lockBoundary(locks, groups, clusters, remap);
            pending.Clear();
            List&lt;int&gt; retry = new List&lt;int&gt;();
            int triangles = 0;
            int stuck_triangles = 0;
            for (int i = 0; i &lt; groups.Count; ++i)
            {
                var curGroupClusters = groups[i];
                if (curGroupClusters.Count == 0)
                {
                    continue; // metis shortcut
                }

                List&lt;int&gt; merged = new List&lt;int&gt;(vertices.Length);
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    merged.AddRange(clusters[curGroupClusters[j]].indices);
                }
                LODBounds groupb = boundsMerge(clusters, curGroupClusters);
                ClusterGroup clusterGroup = new ClusterGroup();
                clusterGroup.Bounds = groupb.center;
                clusterGroup.MaxParentLODError = groupb.error;
                clusterGroup.radius = groupb.radius;
                clusterGroup.Children = new List&lt;int&gt;(merged.Count);
                clusterGroup.MipLevel = curMip - 1;
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    clusterGroup.Children.Add(curGroupClusters[j]);
                }
                clusterGroupList.Add(clusterGroup);

                // aim to reduce group size in half
                int target_size = (merged.Count / 3) / 2 * 3;
                float error = 0f;
                var simplified = simplify(vertices, normals, merged.ToArray(), kUseLocks ? locks : null, target_size,
                    ref error);
                if (simplified.Count &gt; merged.Count * kSimplifyThreshold)
                {
                    stuck_triangles += merged.Count / 3;
                    for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                    {
                        retry.Add(curGroupClusters[j]);
                    }

                    continue; // simplification is stuck; abandon the merge
                }

                // enforce bounds and error monotonicity
                // note: it is incorrect to use the precise bounds of the merged or simplified mesh, because this may violate monotonicity

                var split = clusterize(vertices, simplified.ToArray());
                groupb.error += error; // this may overestimate the error, but we are starting from the simplified mesh so this is a little more correct
                // update parent bounds and error for all clusters in the group
                // note that all clusters in the group need to switch simultaneously so they have the same bounds
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    int clusterIndex = curGroupClusters[j];
                    var t = clusters[clusterIndex];
                    t.parent = groupb;
                    clusters[clusterIndex] = t;
                }

                for (int j = 0; j &lt; split.Count; ++j)
                {
                    var sj = split[j];
                    sj.self = groupb;
                    sj.mip = curMip;
                    split[j] = sj;
                    clusters.Add(sj); // std::move
                    pending.Add(clusters.Count - 1);
                    triangles += sj.indices.Length / 3;
                }
            }

            curMip++;
        }

        if (pending.Count == 1)
        {
            var c = clusters[pending[0]];
            ClusterGroup clusterGroup = new ClusterGroup();
            clusterGroup.Bounds = c.self.center;
            clusterGroup.MaxParentLODError = c.self.error;
            clusterGroup.radius = c.self.radius;
            clusterGroup.Children = new List&lt;int&gt;(1);
            clusterGroup.MipLevel = curMip - 1;
            clusterGroup.Children.Add(pending[0]);
            clusterGroupList.Add(clusterGroup);
        }

        res.maxMipLevel = curMip - 1;
        return res;
    }

static void lockBoundary(byte[] locks, List&lt;List&lt;int&gt;&gt; groups, List&lt;Cluster&gt; clusters, int[] remap)
    {
        // for each remapped vertex, keep track of index of the group it's in (or -2 if it's in multiple groups)
        int[] groupmap = newint[locks.Length];
        for (int i = 0; i &lt; groupmap.Length; ++i)
            groupmap[i] = -1;

        for (int i = 0; i &lt; groups.Count; ++i)
        {
            var c = groups[i];
            for (int j = 0; j &lt; c.Count; ++j)
            {
                var indices = clusters[c[j]].indices;
                for (int k = 0; k &lt; indices.Length; ++k)
                {
                    var v = indices[k];
                    var r = remap[v];

                    if (groupmap[r] == -1 || groupmap[r] == i)
                        groupmap[r] = i;
                    else
                        groupmap[r] = -2;
                }
            }
        }

        // note: we need to consistently lock all vertices with the same position to avoid holes
        for (int i = 0; i &lt; locks.Length; ++i)
        {
            var r = remap[i];
            locks[i] = (byte)((groupmap[r] == -2) ? 1 : 0);
        }
    }</code></pre><p>这样我们得到各级Mip的一系列Clusters。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557830" alt="" title="" loading="lazy"/></p><p><strong>3. 加速结构</strong><br/>即使把三角形划分成Clusters数量也太多，使用Compute Shader来做并行结算效率也不高，于是Nanite就使用了BVH来作为ClusterGroup的加速结构，然后配合Persistent Threads做查找过滤。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557831" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557832" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557833" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557834" alt="" title="" loading="lazy"/></p><p>Persistent Threads遍历BVH部分，有兴趣可以参考UE5源码：<br/>Shaders\Private\Nanite\NaniteClusterCulling.usf</p><p>UE5中也有不使用Persistent Threads的流程，应该说一般默认就是不使用的。</p><p>UE5源码部分<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557835" alt="" title="" loading="lazy"/></p><p>个人认为Persistent Threads方案在GPU遍历这种BVH结构有点暴力和重度，所以简化了一下，把多个Cluster合并成一个剔除单元（Part），先并行对Part做剔除，再对Part里的Cluster去做并行剔除，两层结构来加速作为Persistent Threads的一个简单替代方案。</p><p>然后把多个Part组织成Page用于分块加载。材质处理细节也不同，UE5的材质是每个Cluster会记录MaterialRange，简单起见这里实现是每个SubMesh会去构建独立的Clusters。</p><p>代码如下：</p><pre><code> [Serializable]
    publicstruct NaniteCluster
    {
        publicint indiceIndex;
        publicint indiceCount;
        publicfloat selfErrer;
        publicfloat parentErrer;
        public Vector4 selfSphere;
        public Vector4 parentSphere;
        publicint subMeshID;
        publicint vertexOffset;
    };
    
    [Serializable]
    publicstruct NaniteClusterGroup
    {
        publicint ClusterStart;
        publicint ClusterCount;
        public Vector3 Bounds;
        publicfloat radius;
        public Vector3 LODBounds;
        publicfloat MinLODError;
        publicfloat MaxParentLODError;
        publicint MipLevel;
    }

    [Serializable]
    publicstruct NaniteMeshPart
    {
        publicint ClusterStart;
        publicint ClusterCount;
        public Vector4 selfSphere;
        publicfloat MaxParentLODError;
    }</code></pre><pre><code>public classNaniteSubMesh
    {
        public List&lt;ClusterGroup&gt; clusterGroupList;
        public List&lt;Cluster&gt; clusterList;
        publicint maxMipLevel;
    }
publicclassBuildPart
    {
        public List&lt;int&gt; clusterList;
        publicint mip;
        publicint subMesh;

    }
public static void BuildNaniteMesh(Mesh mesh)
    {
          var vertices = mesh.vertices;
        var normals = mesh.normals;
        var uvs = mesh.uv;

        int subMeshCount = mesh.subMeshCount;
        int totalClusterCount = 0;
        int totalIndexCount = 0;
        List&lt;NaniteSubMesh&gt; subMeshList = new List&lt;NaniteSubMesh&gt;();
        for (int i = 0; i &lt; subMeshCount; i++)
        {
            var triangles = mesh.GetTriangles(i);
            var subMesh = Nanite(vertices,normals,triangles);
            subMeshList.Add(subMesh);
            totalClusterCount += subMesh.clusterList.Count;
        }

        List&lt;BuildPart&gt; buildPartsList = new List&lt;BuildPart&gt;(totalClusterCount);
        int MAX_PART_PERPAGE = 128;
        int MAX_CLUSTER_PERPART = 8;

        for (int subMeshIndex = 0; subMeshIndex &lt; subMeshList.Count; subMeshIndex++)
        {
            var subMesh = subMeshList[subMeshIndex];
            List&lt;Cluster&gt; clusters = subMesh.clusterList;
            var groupsList = subMesh.clusterGroupList;
            BuildPart buildPart = null;
            for (int i = 0; i &lt; groupsList.Count; i++)
            {
                var gIndex = i; // sortGroups[i].OldIndex;
                var g = groupsList[gIndex];
                var childs = g.Children;
                for (int c = 0; c &lt; childs.Count; c++)
                {
                    int cIndex = childs[c];
                    int cMip = clusters[cIndex].mip;
                    totalIndexCount += clusters[cIndex].indices.Length;
                    //new Part
                    if (buildPart == null || buildPart.clusterList.Count &gt;= MAX_CLUSTER_PERPART ||
                        buildPart.mip != cMip)
                    {
                        buildPart = new BuildPart();
                        buildPart.clusterList = new List&lt;int&gt;(MAX_CLUSTER_PERPART);
                        buildPart.mip = cMip;
                        buildPart.subMesh = subMeshIndex;
                        buildPartsList.Add(buildPart);
                    }

                    buildPart.clusterList.Add(cIndex);
                }
            }
        }

        int buildPartCount = buildPartsList.Count;
        NaniteMeshPage[] pageArray = new NaniteMeshPage[(buildPartCount+(MAX_PART_PERPAGE-1))/MAX_PART_PERPAGE];//ceil
        List&lt;int&gt; tempIndiceList = new List&lt;int&gt;(totalIndexCount);
        List&lt;int&gt; mipLists = new List&lt;int&gt;(totalClusterCount);
        int partIndex = 0;
        for (int i = 0; i &lt; pageArray.Length; i++)
        {
            //create new page
            var p = ScriptableObject.CreateInstance&lt;NaniteMeshPage&gt;();
            pageArray[i] = p;
            tempIndiceList.Clear();
            int partCount =  (i == (pageArray.Length -1)) ? (buildPartCount % MAX_PART_PERPAGE) : MAX_PART_PERPAGE;
            p.parts = new NaniteScene.NaniteMeshPart[partCount];
            List&lt;NaniteScene.NaniteCluster&gt; pageClusters = new List&lt;NaniteScene.NaniteCluster&gt;(partCount * MAX_CLUSTER_PERPART);
            for (int j = 0; j &lt; partCount; j++)
            {
                var buildPart = buildPartsList[partIndex];
                var buildPartCluster = buildPart.clusterList;
                //create part
                var part = new NaniteScene.NaniteMeshPart();
                part.ClusterStart = pageClusters.Count; //local index
                part.ClusterCount = buildPartCluster.Count;
                int subMeshID = buildPart.subMesh;
                float maxParentErr = 0f;
                var clusters = subMeshList[subMeshID].clusterList;
                for (int c = 0; c &lt; buildPartCluster.Count; c++)
                {
                    var cluster = clusters[buildPartCluster[c]];
                    mipLists.Add(cluster.mip); 
                    //create Cluster
                    NaniteScene.NaniteCluster naniteCluster = new NaniteScene.NaniteCluster();
                    naniteCluster.indiceIndex = tempIndiceList.Count;
                    naniteCluster.indiceCount = cluster.indices.Length;
                    naniteCluster.parentErrer = cluster.parent.error;
                    naniteCluster.parentSphere = new Vector4(cluster.parent.center.x,cluster.parent.center.y,cluster.parent.center.z, cluster.parent.radius);
                    naniteCluster.selfErrer = cluster.self.error;
                    naniteCluster.selfSphere = new Vector4(cluster.self.center.x,cluster.self.center.y,cluster.self.center.z, cluster.self.radius);
                    naniteCluster.subMeshID = subMeshID;
                    tempIndiceList.AddRange(cluster.indices);
                    maxParentErr = Mathf.Max(naniteCluster.parentErrer, maxParentErr);
                    pageClusters.Add(naniteCluster);
                }

                LODBounds partBounds =  boundsMerge(clusters, buildPartCluster,true);
                part.selfSphere = new Vector4(partBounds.center.x,partBounds.center.y,partBounds.center.z,partBounds.radius);
                part.MaxParentLODError = maxParentErr;
                p.parts[j] = part;
                partIndex++;
            }
            p.clusterArray = pageClusters.ToArray();
            p.indiceArray = tempIndiceList.ToArray();
            p.clusterMip = mipLists.ToArray();
        }

        string fileName = AssetDatabase.GetAssetPath(mesh);
        string extension = Path.GetExtension(fileName);
        fileName = fileName.Replace(extension, "");
        //Build page
        int totalVerts = 0;
        for (int i = 0; i &lt; pageArray.Length; i++)
        {
            var page = pageArray[i];
            var clusterArray = page.clusterArray;
            var indiceArray = page.indiceArray;
            Dictionary&lt;int,int&gt; indicesMap = new Dictionary&lt;int,int&gt;();
            List&lt;Vector3&gt; tempVerts = new List&lt;Vector3&gt;(vertices.Length);
            List&lt;Vector3&gt; tempNormals = new List&lt;Vector3&gt;(vertices.Length);
            List&lt;Vector2&gt; tempUVs = new List&lt;Vector2&gt;(vertices.Length);
            List&lt;int&gt; newIndices = new List&lt;int&gt;(totalIndexCount);
            for (int c = 0; c &lt; clusterArray.Length; c++)
            {
                refvar cluster = ref clusterArray[c];
                var indexStart = cluster.indiceIndex;
                var indexEnd = indexStart+cluster.indiceCount;
                for (int index = indexStart; index &lt; indexEnd; index++)
                {
                    int vertIndex = indiceArray[index];
                    int newIndex;
                    if (!indicesMap.TryGetValue(vertIndex,out newIndex))
                    {
                        newIndex = newIndices.Count;
                        indicesMap.Add(vertIndex, newIndex);
                        tempVerts.Add(vertices[vertIndex]);
                        tempNormals.Add(normals[vertIndex]);
                        if (uvs.Length == 0)
                        {
                            tempUVs.Add(Vector2.zero);
                        }
                        else
                        {
                            tempUVs.Add(uvs[vertIndex]);
                        }

                        newIndices.Add(newIndex);
                    }

                    indiceArray[index] = newIndex;
                }
            }

            page.vertexStride = 5;//pos3 + uv2
            page.vertexData = newfloat[tempVerts.Count * page.vertexStride];
            page.vertexCount = tempVerts.Count;
            for (int v = 0; v &lt; tempVerts.Count; v++)
            {
                int vertexIndex = v * page.vertexStride;
                page.vertexData[vertexIndex + 0] = tempVerts[v].x;
                page.vertexData[vertexIndex + 1] = tempVerts[v].y;
                page.vertexData[vertexIndex + 2] = tempVerts[v].z;
                page.vertexData[vertexIndex + 3] = tempUVs[v].x;
                page.vertexData[vertexIndex + 4] = tempUVs[v].y;
            }
            totalVerts +=tempVerts.Count;
            string newPath = fileName + "_p"+i +".asset";
            AssetDatabase.CreateAsset(page, newPath);
        }
        AssetDatabase.Refresh();

        Debug.Log("mesh Vertx:"+vertices.Length +" mesh Nanite:"+ totalVerts + " cluster:"+totalClusterCount + "part:"+ buildPartCount +" page:"+pageArray.Length);
        NaniteMesh naniteMesh = ScriptableObject.CreateInstance&lt;NaniteMesh&gt;();
        {
            naniteMesh.subMeshCount = subMeshCount;
            naniteMesh.pageArray = new NaniteMeshPage[pageArray.Length];
            for (int i = 0; i &lt; pageArray.Length; i++)
            {
                string newPath = fileName + "_p" + i + ".asset";
                naniteMesh.pageArray[i] = AssetDatabase.LoadAssetAtPath&lt;NaniteMeshPage&gt;(newPath);
            }
        }

        var meshBound = mesh.bounds;
        naniteMesh.boundingSphere = meshBound.center;
        naniteMesh.boundingSphere.w = meshBound.extents.magnitude;
        string meshExt = "_mesh.asset";
        AssetDatabase.CreateAsset(naniteMesh, fileName + meshExt);
        AssetDatabase.Refresh();
    }</code></pre><p>到这里离线部分基本结束，可以得到一个Nanite的资源。当然UE5原文还做了很多操作，如BVH、Encode、编码、压缩、Page的划分、顶点属性优化等，个人认为这些都属于工程细节。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557836" alt="" title="" loading="lazy"/></p><p><strong>4. 运行时资源</strong><br/>来到Runtime部分，我们需要把这个Nanite Mesh加载上来，方便起见，这里直接引用一下资源在脚本上，偷懒省略加载部分。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557837" alt="" title="" loading="lazy"/></p><p>把资源、Object、材质信息整合起来，传到GPU的Buffer中。这里做法很不正式还是偷懒来处理。当然也可以用Compute Shader来更新Page数据到GPUBuffer中。</p><pre><code>    public static List&lt;NaniteRenderer&gt; renderers = new List&lt;NaniteRenderer&gt;();
    privatestatic SceneObject[] gpuObjects = new SceneObject[2048];
    //cluster -&gt; part -&gt; page
    publicstruct SceneObject
    {
        publicint naniteMeshID;
        public Matrix4x4 localToWorldMatrix;
        publicint materialIDOffset;
    }
    publicstruct NaniteRes
    {
        public Vector4 boundingSphere;
        publicint partIndex;
        publicint partCount;
    }

unsafe static void UpdateRenderList()
    {
         if(renderers.Count == 0)
            return;
        //object update
        if (renderers.Count &gt; gpuObjects.Length)
        {
            gpuObjects = new SceneObject[Mathf.NextPowerOfTwo(renderers.Count)];
        }

        objectCount = 0;
        maxPartCount = 0;
        naniteMeshes.Clear();
        materialList.Clear();
        List&lt;int&gt; materialIndices = new List&lt;int&gt;();
        for (int i = 0; i &lt; renderers.Count; i++)
        {
           var renderer = renderers[i];
           var nMesh = renderer.naniteMesh;
            foreach (var p in nMesh.pageArray)
           {
               maxPartCount += p.parts.Length;
               maxClusterCount += p.clusterArray.Length;
           }

           SceneObject obj = new SceneObject();
           obj.localToWorldMatrix = renderer.transform.localToWorldMatrix;
            //mesh index
           int index = naniteMeshes.IndexOf(nMesh);
           if (index &lt; 0)
           {
               index = naniteMeshes.Count;
               naniteMeshes.Add(nMesh);
           }
           obj.naniteMeshID = index;
           //mat indexs
           obj.materialIDOffset = materialIndices.Count;
           for (int m = 0; m &lt; renderer.materials.Length; m++)
           {
               var mat = renderer.materials[m];
               int matIndex = materialList.IndexOf(mat);
               if (matIndex &lt; 0)
               {
                   matIndex = materialList.Count;
                   materialList.Add(mat);
               }
               materialIndices.Add(matIndex);
           }
           gpuObjects[i] = obj;
           renderer.transformChanged = false;
           objectCount++;
        }

        if(candidateClusterBuffer!=null)
            candidateClusterBuffer.Dispose();
        candidateClusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, maxClusterCount *2, sizeof(int));

        if(visibleClusterBuffer != null)
            visibleClusterBuffer.Dispose();
        visibleClusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,maxClusterCount *2, sizeof(int));

        if (objectsBuffer != null)
            objectsBuffer.Dispose();
        objectsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, objectCount, sizeof(SceneObject));
        objectsBuffer.SetData(gpuObjects,0,0,objectCount);

        if(visObjectsBuffer !=null)
            visObjectsBuffer.Dispose();
        visObjectsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,objectCount, sizeof(int));

        int vertCount = 0;
        List&lt;NaniteCluster&gt; tempClusters = new List&lt;NaniteCluster&gt;(2048);
        List&lt;NaniteMeshPart&gt; tempParts = new List&lt;NaniteMeshPart&gt;(2048);
        List&lt;NaniteRes&gt; naniteRes = new List&lt;NaniteRes&gt;(2048);
        List&lt;int&gt; tempIndices = new List&lt;int&gt;(2048 * 100);
        List&lt;float&gt; vertexDataList = new List&lt;float&gt;();
        //load page
        for (int nID = 0; nID &lt; naniteMeshes.Count; nID++)
        {
            NaniteRes res = new NaniteRes();
            var nMesh = naniteMeshes[nID];
            //填充到GPU
            var pages = nMesh.pageArray;
            res.partIndex = tempParts.Count;
            res.partCount = 0;
            res.boundingSphere = nMesh.boundingSphere;
            for (int p = 0; p &lt; pages.Length; p++)
            {
                var page = pages[p];
                var parts = page.parts;
                int vertOffset = vertCount;
                int indicesOffset = tempIndices.Count;
                int clusterOffset = tempClusters.Count;

                //add all cluster
                var clusters = page.clusterArray;
                for (int c = 0; c &lt; clusters.Length; c++)
                {
                    var cluster = clusters[c];
                    cluster.indiceIndex += indicesOffset;
                    cluster.vertexOffset = vertOffset;
                    tempClusters.Add(cluster);
                }

                //add all part
                for (int partIndex = 0; partIndex &lt; parts.Length; partIndex++)
                {
                    var part = parts[partIndex];
                    part.ClusterStart += clusterOffset;
                    tempParts.Add(part);
                    res.partCount++;
                }

                //add page data
                tempIndices.AddRange( page.indiceArray);
                vertexDataList.AddRange(page.vertexData);
                vertCount += page.vertexCount;
            }
            naniteRes.Add(res);
        }

        //TODO GPU Update Buffer
        if (naniteResBuffer != null)
            naniteResBuffer.Dispose();
        naniteResBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, naniteRes.Count, sizeof(NaniteRes));
        naniteResBuffer.SetData(naniteRes);

        if (partsBuffer != null)
            partsBuffer.Dispose();
        partsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,tempParts.Count, sizeof(NaniteMeshPart));
        partsBuffer.SetData(tempParts);

        if (clusterBuffer != null)
            clusterBuffer.Dispose();
        clusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, tempClusters.Count, sizeof(NaniteCluster));
        clusterBuffer.SetData(tempClusters);


        if (indiceseBuffer != null)
            indiceseBuffer.Dispose();
        indiceseBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Raw, tempIndices.Count, sizeof(int));
        indiceseBuffer.SetData(tempIndices);

        if(materialIndexBuffer!=null)
            materialIndexBuffer.Dispose();
        materialIndexBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,materialIndices.Count, sizeof(int));
        materialIndexBuffer.SetData(materialIndices);

        if(vertexDataBuffer!=null)
            vertexDataBuffer.Dispose();
        vertexDataBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Raw, vertexDataList.Count,sizeof(float));
        vertexDataBuffer.SetData(vertexDataList);
    }

    //input object ID =&gt; 
    public unsafe static void UpdateNaniteScene()
    {
        if (renderListDirty)
        {
            UpdateRenderList();
           // UpdateRenderListGPU();
            renderListDirty = false;
        }

       for (int i = 0; i &lt; renderers.Count; i++)
       {
           var renderer = renderers[i];
           if (renderer.transformChanged)
           {
               gpuObjects[i].localToWorldMatrix = renderer.transform.localToWorldMatrix;
               renderer.transformChanged = false;
               transformDirty = true;
           }
       }

       if (objectsBuffer != null &amp;&amp; transformDirty)
           objectsBuffer.SetData(gpuObjects, 0, 0, objectCount);
    }</code></pre><p><strong>5. 剔除</strong><br/>这时离线时候已经把Clusters扁平化到数组中了，这些Clusters是可以并行进行剔除的，巧妙之处是他记录了父级的误差和自己的误差，当我们传入误差系数时候就可以独立地判断自己是否被剔除，而和上下级无关。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557838" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557839" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557840" alt="" title="" loading="lazy"/></p><p>先从CPU发起剔除Compute Shader的Dispatch。这里因为组织数据时候就知道了所有Object最大的Parts/Cluster数量，所以直接用这个数去Dispatch了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557841" alt="" title="" loading="lazy"/></p><p>Objects剔除：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557842" alt="" title="" loading="lazy"/></p><p>根据Object找到NaniteMesh的Parts进行Culling：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557843" alt="" title="" loading="lazy"/></p><p>ClustersCulling：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557844" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557845" alt="" title="" loading="lazy"/></p><p><strong>6. 软光栅</strong><br/>略。</p><p><strong>7. VisibilityBuffer</strong><br/>VBuffer主要用来减少Overdraw，着色器直接输出InstanceID、ClusterID、材质ID。然后用这个VBuffer来计算顶点数据来着色。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557846" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557847" alt="" title="" loading="lazy"/></p><p>这个得益于GPUDriven的好处，一个DrawProceduralIndirect就可以绘制所有物体了：<br/>一次DrawProceduralIndirect绘制多个物体<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557848" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557849" alt="" title="" loading="lazy"/></p><p>VBuffer存哪些属性，多少位，都是工程细节这里就不考究了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557850" alt="" title="" loading="lazy"/></p><p><strong>8. 着色</strong><br/>有了VBuffer就需要逐材质进行绘制，原文是材质ID分Tile组合IndirectDraw画Quad的思想。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557851" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557852" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557853" alt="" title="" loading="lazy"/></p><p>需要注意一下这里VBuffer通过三角重心插值求出的UV是不能直接采样贴图的，因为DDXY不对，所以需求重新计算，计算的代码放下面。并且利用SampleGrad（samplerName, coord2, dpdx, dpdy）来采样。</p><pre><code>uint MurmurMix(uint Hash)
{
    Hash ^= Hash &gt;&gt; 16;
    Hash *= 0x85ebca6b;
    Hash ^= Hash &gt;&gt; 13;
    Hash *= 0xc2b2ae35;
    Hash ^= Hash &gt;&gt; 16;
    return Hash;
}
float3 IntToColor(uint Index)
{
    uint Hash = MurmurMix(Index);

    float3 Color = float3
    (
        (Hash &gt;&gt; 0) &amp; 255,
        (Hash &gt;&gt; 8) &amp; 255,
        (Hash &gt;&gt; 16) &amp; 255
    );

    return Color * (1.0f / 255.0f);
}

struct FBarycentrics
{
    float3 Value;
    float3 Value_dx;
    float3 Value_dy;
};

float2 Lerp(float2 Value0, float2 Value1, float2 Value2, FBarycentrics Barycentrics, out float2 dxy)
{
    float2 Value = Value0 * Barycentrics.Value.x + Value1 * Barycentrics.Value.y + Value2 * Barycentrics.Value.z;
    dxy.x = Value0 * Barycentrics.Value_dx.x + Value1 * Barycentrics.Value_dx.y + Value2 * Barycentrics.Value_dx.z;
    dxy.y = Value0 * Barycentrics.Value_dy.x + Value1 * Barycentrics.Value_dy.y + Value2 * Barycentrics.Value_dy.z;

    return Value;
}

/** Calculates perspective correct barycentric coordinates and partial derivatives using screen derivatives. */
FBarycentrics CalculateTriangleBarycentrics(float2 PixelClip, float4 PointClip0, float4 PointClip1,
                                            float4 PointClip2, float2 ViewInvSize)
{
    FBarycentrics Barycentrics;
    PixelClip.y = 1 - PixelClip.y;
    PixelClip.xy = PixelClip.xy * 2 - 1;
    const float3 RcpW = rcp(float3(PointClip0.w, PointClip1.w, PointClip2.w));
    const float3 Pos0 = PointClip0.xyz * RcpW.x;
    const float3 Pos1 = PointClip1.xyz * RcpW.y;
    const float3 Pos2 = PointClip2.xyz * RcpW.z;

    const float3 Pos120X = float3(Pos1.x, Pos2.x, Pos0.x);
    const float3 Pos120Y = float3(Pos1.y, Pos2.y, Pos0.y);
    const float3 Pos201X = float3(Pos2.x, Pos0.x, Pos1.x);
    const float3 Pos201Y = float3(Pos2.y, Pos0.y, Pos1.y);

    const float3 C_dx = Pos201Y - Pos120Y;
    const float3 C_dy = Pos120X - Pos201X;

    const float3 C = C_dx * (PixelClip.x - Pos120X) + C_dy * (PixelClip.y - Pos120Y);
    // Evaluate the 3 edge functions
    const float3 G = C * RcpW;

    constfloat H = dot(C, RcpW);
    constfloat RcpH = rcp(H);

    // UVW = C * RcpW / dot(C, RcpW)
    Barycentrics.Value = G * RcpH;

    // Texture coordinate derivatives:
    // UVW = G / H where G = C * RcpW and H = dot(C, RcpW)
    // UVW' = (G' * H - G * H') / H^2
    // float2 TexCoordDX = UVW_dx.y * TexCoord10 + UVW_dx.z * TexCoord20;
    // float2 TexCoordDY = UVW_dy.y * TexCoord10 + UVW_dy.z * TexCoord20;
    const float3 G_dx = C_dx * RcpW;
    const float3 G_dy = C_dy * RcpW;

    constfloat H_dx = dot(C_dx, RcpW);
    constfloat H_dy = dot(C_dy, RcpW);

    Barycentrics.Value_dx = (G_dx * H - G * H_dx) * (RcpH * RcpH) * (2.0f * ViewInvSize.x);
    Barycentrics.Value_dy = (G_dy * H - G * H_dy) * (RcpH * RcpH) * (-2.0f * ViewInvSize.y);

    return Barycentrics;
}</code></pre><p>到这里其实基本完成了，利用IntToColor函数，可以对ClustersID或者IndexID对三角形或Cluster进行可视化。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557854" alt="" title="" loading="lazy"/></p><blockquote><h3><strong>三、总结</strong></h3></blockquote><p>不得不说Nanite技术真是太强大了，但是也有很多工程细节需要处理，本文只是实现了其中一小部分。整体像是处理图片的Mipmap过程。</p><p><strong>参考</strong></p><p><a href="https://www.bilibili.com/video/BV17G4y1x7VX/?spm_id_from=333.337.search-card.all.click&amp;vd_source=07d4f665c85998941c935676c2e50d81" target="_blank">22.GPU驱动的几何管线-nanite (Part 2) | GAMES104-现代游戏引擎：从入门到实践</a></p><p><a href="https://www.bilibili.com/video/BV1MP4y1a7Hh/?spm_id_from=333.1387.search.video_card.click&amp;vd_source=07d4f665c85998941c935676c2e50d81" target="_blank">[UnrealCircle]Nanite技术简介 | Epic Games China 王祢</a></p><p><a href="https://link.segmentfault.com/?enc=OiCon8HoejdpYcHdz0%2F47g%3D%3D.MgJ2RkInLFJQ9U%2FMSDyTWmYoi1lSusaqkg3q2ydeLaMPDjCPx0DdCgRi%2FdtFNoLtlCBQKj5N0hl4hhGa0HusEr8SzVDYFanO0uRkXDXey5g5IqQ%2FfF7k0noaH%2FXMtMRy" rel="nofollow" target="_blank">Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf</a></p><p><a href="https://link.segmentfault.com/?enc=6IMKalDSFCX2oRp0RdzeQg%3D%3D.4%2BEQEgpYdQXsuf3N2aGJRkU332rytDQd1bst4NYZ3mQmlhrRDdVwSajdaULXn1gFAWvx9upWpXHI%2F%2F7R%2BOS8mg%3D%3D" rel="nofollow" target="_blank">Nanite-GPU-Driven</a></p><p>UE5 Nanite源码入口：<br/>Engine\Source\Runtime\Renderer\Private\Nanite\NaniteCullRaster.cpp  <strong>（渲染流程入口）</strong><br/>Engine\Shaders\Private\Nanite\ <strong>（GPU的Shader入口）</strong><br/>Engine\Source\Developer\NaniteBuilder\Private\ <strong>（离线生成Nanite资源入口）</strong></p><hr/><p>这是侑虎科技第1939篇文章，感谢作者傻头傻脑亚古兽供稿。欢迎转发分享，未经作者授权请勿转载。如果您有任何独到的见解或者发现也欢迎联系我们，一起探讨。（QQ群：793972859）</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=1isfSoQj5tZS6PcAUj2ewg%3D%3D.zIAiK6Leeh2UdiKdAG8BEsg25FqvnlySwnM24Hztwvr1%2BoaSDLjxRv1OFFhyhoVUdFE47UiPuDk1p98DoxVsoA%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=Kpbq3Xj6qrF1D6IR2NcDsA%3D%3D.C3GG0nh8pNdzmzIcDO5HrqKrUdFYIwv%2FMZKlUV%2BAzNgVxV1sVSCDfRh11%2BCh3j3nkXHe97eUMkKn6tAMm18Txw%3D%3D" rel="nofollow" target="_blank">https://www.zhihu.com/people/tian-cai-ya-gu-shou</a></p><p>再次感谢傻头傻脑亚古兽的分享，如果您有任何独到的见解或者发现也欢迎联系我们，一起探讨。（QQ群：793972859）</p>]]></description></item><item>    <title><![CDATA[从 0 到 1 的智能体搭建之路 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047557937</link>    <guid>https://segmentfault.com/a/1190000047557937</guid>    <pubDate>2026-01-22 12:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>🚀 快速回答 (Golden Answer)</h3><p>从 0 到 1 搭建智能体的核心逻辑是 “明确需求 → 选对工具 → 配置闭环 → 测试优化”：无需复杂编程，优先用零代码 / 低代码工具（如 Coze、LangGraph），先定义 “智能体要解决的具体任务”（如自动化办公、设计辅助），再通过 “设定角色 → 拆解任务 → 配置工具 → 添加反思逻辑” 完成搭建，最终通过测试迭代优化效果。核心是 “让智能体精准匹配需求”，而非追求技术复杂度。</p><h2>一、前置认知：先搞懂 “搭建智能体” 的核心前提</h2><h3>1.1 搭建智能体的核心目标：解决 “具体问题”</h3><p>智能体的核心价值是 “自主完成复杂任务”，搭建前必须明确 “它要帮你做什么”，避免盲目搭建。常见落地场景：</p><ul><li>个人场景：自动化周报生成、文献整理助手、学习笔记总结、购物比价监控；</li><li>职场场景：客户咨询智能客服、销售数据自动分析、市场调研报告生成、设计批量出图；</li><li>垂直场景：电商运营助手（商品上架 + 文案生成）、科研辅助（数据检索 + 分析）、教育答疑（学科知识点梳理）。</li></ul><h3>1.2 搭建智能体的核心逻辑：“感知 - 规划 - 行动 - 反思” 闭环</h3><p>无论用哪种工具，智能体的底层逻辑都是这四个环节的循环，搭建的本质就是 “配置这四个环节的规则”：</p><ul><li>感知：让智能体 “接收信息”（如用户需求、外部数据、工具反馈）；</li><li>规划：让智能体 “拆解任务”（如 “生成销售报告” 拆解为 “收集数据 → 清洗数据 → 分析 → 排版”）；</li><li>行动：让智能体 “执行步骤”（如调用 Excel、API 接口、设计工具完成具体操作）；</li><li>反思：让智能体 “修正错误”（如数据缺失时重新收集，格式错误时自动调整）。</li></ul><h3>1.3 零基础搭建的核心原则：“工具优先，不造轮子”</h3><p>无需从零开发大模型或底层架构，当前主流工具已提供 “可视化配置 + 现成组件”，零基础只需聚焦 “需求匹配” 和 “流程配置”，核心原则：</p><ol><li>优先选零代码工具（如 Coze、Notion AI Agent），快速验证需求；</li><li>复杂场景再用低代码工具（如 LangGraph、AutoGen），灵活适配个性化需求；</li><li>先搭建 “最小可用版本”（仅满足核心任务），再逐步添加功能。</li></ol><h2>二、工具选择：零基础必看的 “工具选型矩阵”</h2><p>不同工具的门槛、功能、适配场景差异较大，结合 “零基础友好度” 和 “落地实用性”，整理核心工具对比：</p><table><thead><tr><th>工具名称</th><th>技术门槛</th><th>核心优势</th><th>适配场景</th><th>学习成本</th></tr></thead><tbody><tr><td>Coze（扣子）</td><td>零代码</td><td>可视化配置，插件生态丰富（支持 Excel、数据库、设计工具等），可直接发布为小程序 / APP</td><td>个人助手、职场自动化、客服机器人</td><td>低（1-2 小时掌握基础配置）</td></tr><tr><td>Notion AI Agent</td><td>零代码</td><td>与文档深度融合，支持笔记整理、报告生成、任务管理，操作简单直观</td><td>学习助手、文献整理、文档自动化</td><td>极低（熟悉 Notion 即可上手）</td></tr><tr><td>LangGraph</td><td>低代码（Python 基础）</td><td>状态控制极强，支持复杂循环逻辑，适配高定制化任务</td><td>科研辅助、复杂数据分析、自动化办公流</td><td>中（需掌握基础 Python 和 Prompt 技巧）</td></tr><tr><td>AutoGen</td><td>低代码（Python 基础）</td><td>支持多智能体协作，角色分工明确，降低复杂任务的配置难度</td><td>软件工程、内容生产流水线、多步骤商业分析</td><td>中（需理解多智能体协同逻辑）</td></tr><tr><td>Make（原 Integromat）</td><td>零代码</td><td>专注工具集成，支持 1000 + 款软件对接，擅长自动化工作流串联</td><td>跨平台自动化（如微信 + Excel + 邮件协同）</td><td>低（重点学习工具对接逻辑）</td></tr></tbody></table><p>💡 零基础优先推荐：Coze（功能全、生态完善）或 Notion AI Agent（简单直观）；若需处理复杂任务，再学习 LangGraph（低代码门槛）。</p><h2>三、分步实操：用 Coze 从零搭建 “自动化周报生成智能体”（零代码案例）</h2><p>以 “自动收集 Excel 数据 → 生成周报 → 排版导出” 为核心任务，用 Coze 完成搭建，全程可视化操作，10 分钟即可完成基础版本：</p><h3>3.1 第一步：明确需求与角色设定</h3><ol><li>核心需求：用户上传 Excel 销售数据后，智能体自动计算核心指标（销售额、增长率、Top3 产品），生成结构化周报，支持 Word 导出；</li><li>角色设定：在 Coze 后台 “角色定义” 中填写 ——“你是职场销售周报生成助手，擅长从 Excel 数据中提取核心信息，生成逻辑清晰、格式规范的周报，语言正式专业”；</li><li>补充提示：添加 “周报格式要求”（如包含 “本周概况、核心数据、趋势分析、下周计划” 模块），让智能体输出更精准。</li></ol><h3>3.2 第二步：配置 “工具”（让智能体具备执行能力）</h3><p>智能体需要对接 Excel 和 Word 工具，才能完成数据读取和导出，操作步骤：</p><ol><li>在 Coze “插件市场” 中搜索 “Excel 解析” 和 “Word 导出” 插件，点击 “启用”；</li><li>配置插件权限：授权 Coze 读取用户上传的 Excel 文件（仅读取权限，保障数据安全）；</li><li>测试工具连通性：上传一份测试 Excel 数据，点击 “测试插件”，确认智能体能正常提取数据。</li></ol><h3>3.3 第三步：设计 “任务流程”（拆解执行步骤）</h3><p>在 Coze “流程设计” 模块，用可视化拖拽配置任务步骤，核心流程：</p><ol><li>触发条件：用户上传 Excel 文件并发送 “生成周报” 指令；</li><li>步骤 1：调用 “Excel 解析” 插件，提取数据（销售额、产品名称、日期等）；</li><li>步骤 2：智能体计算核心指标（本周总销售额、环比增长率、Top3 热销产品）；</li><li>步骤 3：按照预设格式生成周报文本；</li><li>步骤 4：调用 “Word 导出” 插件，生成周报文件并反馈给用户。</li></ol><h3>3.4 第四步：添加 “反思逻辑”（让智能体能修正错误）</h3><p>为避免数据缺失或格式错误，添加简单反思规则：</p><ol><li>在 “流程设计” 中添加 “判断节点”：若 Excel 数据缺失关键字段（如 “销售额”），则自动向用户发送 “请补充包含销售额字段的 Excel 文件”；</li><li>添加 “格式校验”：生成周报到导出前，自动检查是否包含预设的 4 个模块，缺失则补充完善。</li></ol><h3>3.5 第五步：测试与发布</h3><ol><li>测试验证：上传测试 Excel 数据，发送 “生成周报” 指令，查看智能体是否能正确完成全流程，重点检查数据计算准确性和格式规范性；</li><li>优化迭代：若存在格式混乱，补充 “周报格式细则”（如字体、行距、标题层级）；若数据计算错误，调整指标计算规则；</li><li>发布使用：测试通过后，点击 “发布”，可生成小程序 / 网页链接，直接在工作中使用。</li></ol><h2>四、进阶优化：让智能体更 “好用” 的 3 个关键技巧</h2><h3>4.1 精准 Prompt 优化：提升输出质量</h3><p>在角色定义中补充 “具体约束”，而非模糊描述，示例：</p><ul><li>差 Prompt：“生成专业的周报”；</li><li>好 Prompt：“生成销售周报，包含本周概况（30 字内）、核心数据（表格呈现）、趋势分析（200 字内）、下周计划（3 条核心动作），语言正式，避免口语化，数据保留 2 位小数”。</li></ul><h3>4.2 个性化适配：对接个人 / 企业知识库</h3><p>若智能体需要适配特定业务（如公司产品知识、个人工作习惯），可在 Coze 中上传 “知识库”（如公司产品手册、个人工作模板），让智能体学习后输出更贴合需求的结果。</p><h3>4.3 多智能体协作：解决复杂任务</h3><p>对于 “市场调研 → 数据分析 → 报告生成” 这类复杂任务，可搭建 “多智能体团队”：</p><ul><li>调研智能体：负责收集市场数据；</li><li>分析智能体：负责数据计算与趋势分析；</li><li>撰写智能体：负责生成最终报告； 在 Coze 中配置 “智能体间通信规则”，让它们协同完成任务，提升效率。</li></ul><h2>五、避坑指南：零基础搭建常见问题与解决方案</h2><table><thead><tr><th>常见问题</th><th>核心原因</th><th>解决方案</th></tr></thead><tbody><tr><td>智能体输出不符合预期（如格式混乱）</td><td>角色定义模糊，缺乏明确约束</td><td>补充具体的输出格式、语言风格、内容模块要求，用示例引导（如 “参考以下示例格式生成：【本周概况】XXX”）</td></tr><tr><td>智能体无法完成复杂任务（如数据计算错误）</td><td>任务拆解不细致，工具配置不当</td><td>将复杂任务拆分为更细的原子步骤，检查工具参数配置（如数据字段匹配），添加人工校验节点</td></tr><tr><td>智能体出现 “幻觉”（如编造数据）</td><td>缺乏真实数据支撑，规则约束不足</td><td>强制智能体仅基于用户上传的数据输出，添加 “禁止编造数据” 的规则，关键数据要求标注来源</td></tr><tr><td>工具调用失败（如无法读取 Excel）</td><td>插件权限不足，文件格式不兼容</td><td>重新授权插件权限，统一文件格式（如 Excel 保存为.xlsx 格式），测试工具连通性</td></tr></tbody></table><h2>六、FAQ：零基础搭建智能体最关心的核心问题</h2><h3>Q1：搭建智能体需要懂编程吗？</h3><p><strong>答：不需要。</strong> 零代码工具（如 Coze、Notion AI Agent）通过可视化拖拽和文字描述即可完成搭建；若需高定制化，仅需掌握基础 Python（低代码工具），但零基础可先从简单工具入手，无需一开始学习编程。</p><h3>Q2：搭建智能体需要花钱吗？</h3><p><strong>答：个人非商业使用基本免费。</strong> Coze、Notion AI Agent 等工具对个人用户提供免费额度（足够日常使用）；商业场景或高频率使用可能需要付费升级，但零基础入门无需付费。</p><h3>Q3：智能体的数据安全有保障吗？</h3><p><strong>答：选择正规工具即可保障。</strong> 主流工具（如 Coze、Notion）均有数据加密机制，且可设置 “仅自己可见”；避免上传敏感数据（如身份证、银行卡信息），若需处理企业数据，可选择企业版工具（提供私有部署选项）。</p><h3>Q4：搭建完成后，能修改功能吗？</h3><p><strong>答：可以。</strong> 所有主流工具均支持 “二次编辑”，可随时修改角色定义、任务流程、工具配置；建议根据使用反馈定期优化，让智能体更贴合需求。</p><h2>七、核心总结</h2><p>从 0 到 1 搭建智能体的核心不是 “技术攻关”，而是 “需求聚焦” 与 “流程拆解”：零基础用户无需畏惧，优先选择零代码工具，先明确 “智能体要解决的具体问题”，再通过 “角色定义 → 工具配置 → 流程设计 → 测试优化” 的步骤逐步落地，先搭建 “最小可用版本” 验证需求，再逐步进阶优化。</p><p>智能体的价值在于 “解放重复劳动”，搭建的关键是让它成为 “贴合自己需求的助手”，而非追求 “功能全而杂”。随着工具生态的完善，“人人都能搭建智能体” 已成为趋势，掌握这种 “人机协同” 的搭建能力，将大幅提升个人与工作效率。</p><h2>参考文献与工具资源</h2><ol><li>Coze（扣子）官方文档：《零代码智能体搭建指南》</li><li>LangGraph 官方教程：《低代码智能体开发实战》</li><li>腾讯云《智能体落地实践白皮书》（2025）</li><li>推荐学习平台：Coze 学院、Notion AI 帮助中心、GitHub AutoGen 示例仓库</li></ol><h3>核心关键词</h3><p>智能体搭建、从 0 到 1、零代码智能体、Coze、LangGraph、自动化办公、智能体工具、人机协同</p>]]></description></item><item>    <title><![CDATA[【剪映API】提取链接 失落的木瓜_esfWwz ]]></title>    <link>https://segmentfault.com/a/1190000047557940</link>    <guid>https://segmentfault.com/a/1190000047557940</guid>    <pubDate>2026-01-22 12:04:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>GET_URL API 接口文档</h2><h3>接口信息</h3><pre><code>POST /openapi/capcut-mate/v1/get_url</code></pre><h3>功能描述</h3><p>提取链接。该接口用于提取输入内容中的链接信息，用于多值返回变成单值返回。</p><h3>更多文档</h3><p>📖 更多详细文档和教程请访问：<a href="https://link.segmentfault.com/?enc=LhsUzGp46JpRFJzpY0uq5w%3D%3D.AWQi7n0Reo6FGV0ks9d811lAwlKYwHMyvTyxtk2JycE%3D" rel="nofollow" target="_blank">https://docs.jcaigc.cn</a></p><h3>请求参数</h3><pre><code class="json">{
  "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
}</code></pre><h4>参数说明</h4><table><thead><tr><th>参数名</th><th>类型</th><th>必填</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>output</td><td>string</td><td>✅</td><td>-</td><td>提取内容</td></tr></tbody></table><h4>参数详解</h4><h5>output</h5><ul><li><strong>类型</strong>: string</li><li><strong>说明</strong>: 需要提取链接的内容</li><li><strong>示例</strong>: <code>"[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""</code></li></ul><h3>响应格式</h3><h4>成功响应 (200)</h4><pre><code class="json">{
  "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
}</code></pre><h4>响应字段说明</h4><table><thead><tr><th>字段名</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>output</td><td>string</td><td>提取结果</td></tr></tbody></table><h4>错误响应 (4xx/5xx)</h4><pre><code class="json">{
  "detail": "错误信息描述"
}</code></pre><h3>使用示例</h3><h4>cURL 示例</h4><h5>1. 基本使用</h5><pre><code class="bash">curl -X POST https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/get_url \
  -H "Content-Type: application/json" \
  -d '{
    "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
  }'</code></pre><h3>错误码说明</h3><table><thead><tr><th>错误码</th><th>错误信息</th><th>说明</th><th>解决方案</th></tr></thead><tbody><tr><td>400</td><td>output是必填项</td><td>缺少output参数</td><td>提供有效的output参数</td></tr><tr><td>500</td><td>提取链接失败</td><td>内部处理错误</td><td>联系技术支持</td></tr></tbody></table><h3>注意事项</h3><ol><li><strong>参数要求</strong>: output参数为必填项</li><li><strong>返回值</strong>: 当前版本直接返回输入的内容，不做额外处理</li></ol><h3>工作流程</h3><ol><li>验证必填参数（output）</li><li>调用服务层处理业务逻辑</li><li>返回处理结果</li></ol><h3>相关接口</h3><ul><li><a href="./create_draft.md" target="_blank">创建草稿</a></li></ul><hr/><p>📚 <strong>项目资源</strong>  <br/><strong>GitHub项目名称</strong>: capcut-mate</p>]]></description></item><item>    <title><![CDATA[智慧能源升维，实时云渲染重构管理新视角 点量实时云渲染 ]]></title>    <link>https://segmentfault.com/a/1190000047557984</link>    <guid>https://segmentfault.com/a/1190000047557984</guid>    <pubDate>2026-01-22 12:03:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnH94" alt="" title=""/></p><p>2026年初，工业和信息化部等五部门联合印发的《工业绿色微电网建设与应用指南（2026—2030年）》，为能源的数字化转型铺设了清晰的政策轨道。目前，全国已投入运行的工业绿色微电网项目超过300个，它们正从试点走向规模化。智慧能源的管理，正从传统的报表与经验，向一个全域可视、实时交互、智能决策的数字世界加速演进。</p><h2>01 政策引领，智慧能源按下“加速键”</h2><p>国家层面正在以前所未有的力度，推进能源系统的数字化转型。《工业绿色微电网建设与应用指南》明确将智慧能源管控系统，列为绿色微电网建设的核心内容之一。其目标是构建一个集成光伏、风电、储能、氢能等多能互补，并实现与大电网友好互动的综合能源系统。未来的能源管理，必须是数字化、可视化、智能化的。</p><h2>02 现实挑战：智慧能源的进阶痛点</h2><p>然而，理想蓝图在落地时，却面临着一系列棘手的现实挑战。当前的核心痛点可以概括为：“看不见、摸不清、调不动”。</p><ul><li>状态“看不见”：一个现代化的能源场站或微电网，包含成千上万的设备与传感器。传统分散的图表和报表，让管理者难以在短时间内掌握全局状态，如同“开盲盒”。</li><li>逻辑“摸不清”：SCADA、PLC、IoT等系统数据格式各异，形成信息壁垒。当发生故障时，运维人员需要跨多个系统排查，难以快速穿透网络层、服务器层、应用层，精准定位根源。</li><li>协同“调不动”：为了实现对复杂系统的精细化管理，数字孪生技术正成为标配。但这些高精度三维模型对终端电脑的图形性能要求极高，导致许多一线运维人员无法流畅使用，远程协同和移动办公更是困难重重。</li></ul><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnIai" alt="" title="" loading="lazy"/></p><h2>03 破局关键：实时云渲染让智慧能源“轻装上阵”</h2><p>点量云流实时云渲染其核心原理是将海量三维模型的计算与渲染任务放在云端强大的服务器集群上完成，前端终端（无论是高性能工作站、普通笔记本，还是平板电脑）只需通过网页或轻量客户端，接收经过云端处理的视频流即可进行操作。</p><p>相较于传统网页3D效果受模型大小限制，点量云流实时云渲染能够在不消耗终端硬件性能的情况下，实现无需等待加载、即时打开与实时交互的体验。</p><p>这一转变带来了三个根本性改变：</p><ul><li>终端解放：运维人员不再受本地硬件性能束缚，用一台普通办公电脑或移动设备，就能流畅操控大型能源场（如风、电、煤等）的实景数字孪生模型。</li><li>数据安全：所有核心模型与数据始终保存在云端服务器，前端只传输视频流，从根本上杜绝了三维数字资产通过终端泄露的风险。</li><li>高效协同：不同地域的专家可以同时接入同一个三维场景，基于统一的、可视化的模型进行会诊、标注和决策，极大提升了跨团队协作效率。</li></ul><h2>04 实战图景：可视化如何重塑能源管理</h2><p>技术的价值，最终需要体现在真实的场景中。当实时云渲染技术卸下了硬件的重担，一系列曾经难以落地的应用，正悄然成为智慧能源管理的日常。<br/>1、运维：从“被动响应”到“主动预警”<br/>基于高精度数字孪生模型，系统能深度融合实时数据与AI算法，提前洞察设备亚健康状态，精准预测潜在故障。运维策略由此从紧急抢修的“被动处置”，转向计划性干预的“主动预防”。而这一切得以实现的关键，在于实时云渲染技术让这套复杂的三维预警系统，得以在各级管理中心的普通电脑上便捷访问与联动，使预防性维护真正触手可及。</p><p>2、管理：从“分散孤岛”到“全域一张图”<br/>传统管理中，物理设备、网络流量、业务数据往往分散于不同系统，形成信息壁垒。如今，通过实时云渲染技术，这些要素被整合进一个统一的动态三维界面，生成能源系统的“全景作战图”。结合云推流能力，无论是集控中心的大屏，还是巡检人员的移动终端，都能获得一致、流畅且可交互的全局视角，真正实现了“全域可视、全局可控”的集中化管控。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnIap" alt="" title="" loading="lazy"/></p><p>3、效率：从“人工跑腿”到“远程会诊”<br/>对于地处偏远的风电场或水电站，专家亲赴现场耗时费力。通过点量云渲染平台，专家在千里之外即可指挥实景复刻的虚拟现场，通过三维模型远程指导一线人员排查故障，将响应时间从数小时大幅压缩至分钟级。这不仅是距离的缩短，更意味着高精度模型得以在PC、平板等多终端安全、流畅地访问，显著提升了跨地域协同、应急指挥与人员培训的效能。</p><p>随着实时云渲染技术与能源体系的深度融合，智慧能源的管理模式将迎来根本性变革。高精度的能源系统数字孪生将不再受限于本地硬件，而是通过云推流技术，成为在任何终端均可流畅访问与协同操作的“活地图”。从宏观调度到微观运维，决策都将基于一张全域同步、实时可视、深度交互的动态图谱。</p><p>这不仅是技术的叠加，更是从“经验驱动”到“全景数据驱动”的智慧跃迁。一个更高效、更透明、更坚韧的能源时代，正借由这条“云端高速路”，清晰地向我们驶来。</p>]]></description></item><item>    <title><![CDATA[我的“Python海龟”诞生了一枚金蛋，孵出的却是“精灵” 李兴球 ]]></title>    <link>https://segmentfault.com/a/1190000047557994</link>    <guid>https://segmentfault.com/a/1190000047557994</guid>    <pubDate>2026-01-22 12:02:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>时光要追溯到2010年。在那之前，我一直用Visual Basic语言编写3D小游戏，乐在其中。那时我觉得Basic已经是非常简单的计算机语言了，但心中始终有一个疑问：有没有比Basic更友好、更适合少儿的编程语言呢？</p><p>于是我开始在网络上持续寻找。果然，不久后我遇到了Scratch 1.4版——那只来自美国麻省理工学院的小猫，一下子抓住了我的心。从此，我踏上了少儿编程教育的道路：2013年开办了编程培训班，2015年将Python正式纳入教学体系，到2018年时，我已积撰写了相当丰富的Python教学材料。</p><p>在教学过程中，我逐渐感到Python内置的turtle（海龟作图）模块功能有些局限。于是，我打开它的源代码文件turtle.py，仔细研究其结构，并从2019年开始，基于Python turtle模块持续开发一个更强大的扩展——Python精灵模块（sprites）。这个模块的核心是一个叫做Sprite的类，它大幅增强了海龟的功能，例如实现了像素级的碰撞检测等。如今，任何人都可以通过pip install sprites来安装并使用它。</p><p>最近几年，我将更多精力投入信息学奥赛的教学中，整日“苦思冥想”各种算法难题。2025年8月，暑假班结束后，我又开始思考另一个问题：如果C++的入门教学能像Python turtle一样直观、有趣，那不就能为中国更多青少年打开编程的大门吗？</p><p>为此，我在GitHub上搜索已有的类似成果，也尝试了一些用C或C++编写的“类turtle”库，例如小熊猫C++内置的C语言海龟作图、GoC等，甚至购买了相关教材准备投入教学。但最终，我并没有采用它们。原因何在？</p><p>小熊猫C++中的海龟作图功能，作者显然缺乏Python少儿编程的教学背景。我曾联系他，希望将命令设计得接近Python turtle的风格，但毕竟不能一直麻烦别人，后来也就作罢。GoC则为了降低输入难度，将命令简化为单个或两个字符（如pen.o），其命令集较小，功能也相对有限。它主要依赖在线环境，作者并未提供独立的编辑器（早期离线版需搭配Notepad++使用）。GoC更像为信息学奥赛选拔苗子而设计的前置课程——网上甚至有人建议一、二年级就开始学习。如果你确定要走信奥路线，这或许可行；否则，并不适合普通学生。</p><p>这里涉及一个关键的教育认知问题：并非所有孩子都适合在低龄阶段接触C++。神经科学研究表明，大脑前额叶皮层（负责逻辑、规划与抽象思维）发育较晚，通常到青春期才趋于成熟。​ 有些孩子认知发展稍晚，若过早强制学习C++这类抽象程度高的语言，容易导致挫败感，甚至产生“习得性无助”。相反，在中低年级通过图形化编程（如Scratch）进行多感官、具象化的学习，能更好地刺激大脑不同区域，促进思维灵活性和创造力的发展。等到年龄增长、认知准备更充分时，再接触C++，往往事半功倍。​ 现实中，不少学生直到高中阶段才在逻辑思维上“开窍”，这恰恰说明大脑发育有其自然节奏，教育应当顺应而非违背它。</p><p>那么问题来了：对于大多数普通学生而言，如果一二年级接触图形化编程，三四年级学习Python，那么到了合适年龄，该如何顺畅地过渡到C++？市面上是否存在一套针对普通学生、能完美衔接既有体系的C++课程？或许有，但可能不公开或需付费。无论如何，我决定亲手打造一个——“金窝银窝，不如自己的草窝”。</p><p>首先面临的是技术选型。如果基于OpenGL，虽然强大，但学习成本较高；我也尝试过EasyX，并做出了原型，但因其底层控制不足而放弃；之后考虑过raylib（基于SDL2封装）和SFML，它们功能丰富，但封装程度较高，不利于我深入底层实现教育定制化的需求。最终，我选择了SDL2——这是一个工业级的跨平台库，接口相对底层，自由度大，掌控力强，正好符合我的开发理念。</p><p>于是，我以SDL2为基础，开始了漫长的开发与调试。最初叫它“C++ Sprites库”，后来正式定名为“C++精灵库”。为降低使用门槛，我还专门开发了配套的pxC++编辑器，并制作了Dev-C++ 5.11的升级包，使其能更好地融入中小学现有的C++教学环境。</p><p>如今，C++精灵库不仅完整继承了Python turtle的简洁API与教育基因，更在其基础上进行了优化与扩展，比如：</p><p>· 新增fill命令，支持区域填充；</p><p>· 通过函数重载，使pencolor等命令既支持字符串参数，也支持RGB/整数参数，更加灵活；</p><p>· 设计penshade（阴影度）、pensat（饱和度）、penvalue（明度）、penhsv（HSV色彩模型）、penalpha（透明度）等色彩控制方法；</p><p>· 加入贝塞尔曲线与样条曲线绘制功能，让有美术天赋的学生也能轻松创作复杂图形。</p><p>本质上，C++精灵库是Logo语言教育理念在C++领域的延续与升级。它借鉴Python turtle的友好界面，并依托SDL2的工业级能力，为学生搭建了一座从趣味编程通向真实开发的桥梁。你可以把Python turtle和C++精灵库看作一对“亲兄弟”——无论先学哪一个，再学另一个时都会产生“似曾相识燕归来”的亲切感。这种一脉相承的设计，实质是一种“双倍赋能”：既降低了学习新语言的心理门槛，又让学生在潜移默化中理解编程底层的共通逻辑。</p><p>正因为如此，当我让它们在外观和命令上如此相似时，请不要惊讶。更有价值的是，由于C++精灵库直接基于SDL2开发，学生可以在掌握基础作图后，无缝接入SDL2的更高级功能，进而探索游戏开发、交互媒体等更广阔的应用场景。这种从教育到实战的平滑过渡，是其他同类C++图形库难以比拟的。</p><p>这条路，我还会继续走下去。只愿这只从“海龟”蛋里孵出的“精灵”，能飞入更多中国少年的编程梦中，陪伴他们从好奇走向热爱，从图形走向算法，从今天走向未来。</p>]]></description></item><item>    <title><![CDATA[2026年MES系统厂商综合实力TOP10：广域铭岛引领工业智能化浪潮 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047557997</link>    <guid>https://segmentfault.com/a/1190000047557997</guid>    <pubDate>2026-01-22 12:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>前言：从技术驱动到生态共建，工业智能化迈入“全链融合”新纪元<br/>根据《2026全球智能制造发展白皮书》，制造执行系统（MES）已成为企业数字化转型的核心引擎，其与工业互联网平台、人工智能技术的深度融合，正重塑制造业的生产范式。Gartner最新报告预测，2026年全球超过70%的制造企业将优先选择具备“平台化、可组合”架构的MES供应商。<br/>当前，MES市场正经历从单一功能工具到全生命周期服务的范式转变。企业不再满足于传统系统的功能叠加，而是寻求能够理解行业痛点、提供持续价值、并具备前瞻性技术视野的长期战略合作伙伴。本次评估突破地域限制，聚焦全球范围内的领先企业，旨在为企业在智能化转型浪潮中提供更具国际视野的选择指南。<br/>2026年MES综合实力TOP10榜单<br/>一、广域铭岛（GYMD）<br/>二、罗克韦尔自动化（Rockwell Automation）<br/>三、达索系统（Dassault Systèmes）<br/>四、SAP<br/>五、霍尼韦尔（Honeywell）<br/>六、施耐德电气（Schneider Electric）<br/>七、Oracle<br/>八、GE Digital<br/>九、ABB<br/>十、AVEVA</p><p>一、广域铭岛：中国智造领域AI原生引领者<br/>广域铭岛数字科技有限公司作为吉利控股集团旗下的工业数字化先锋，以“让工厂更智能，让能耗更低碳，让人更专注创造”为使命，打造了覆盖汽车、电子、能源等全行业的数字化转型解决方案。</p><ol><li>核心产品与技术能力<br/>公司自主研发的Geega OS工业操作系统，通过GPU池化管理、AI调优开发平台、数据编织虚拟化引擎三大核心技术，实现算力资源利用率提升30%-40%。基于通义千问、DeepSeek等通用基座模型，结合行业数据微调，生成高度适配的专用模型，如工艺专家模型准确率达90%，工时分析模型效率提升显著。</li><li>行业解决方案与落地案例深度<br/>该公司在新能源电池制造领域展现出卓越实力。通过工业操作系统赋能衢州极电三电智能制造工厂，实现每2.5秒下线一颗电芯的惊人效率。该平台建立了“1个工业互联网数字化底座+9大工业领域知识沉淀+13个平台应用赋能软件”的数字化赋能体系，帮助电池企业降低质量损失成本13%，提升订单交付周期响应速度。</li><li>咨询服务与生态整合能力<br/>该公司提供从咨询规划到实施服务的一站式解决方案，服务网络覆盖重庆、杭州国内主要工业城市，并在东南亚设立2家海外服务中心。其自主研发的FastWorx设计研发协同平台、GQCM工艺质量管理系统等产品，已服务吉利、领克、钱江摩托等多家行业龙头企业，形成完整的“研-产-供-销-服”数字化生态。<br/>【推荐理由】 最适合寻求AI原生赋能、注重全链路数字化转型的制造业企业。尤其在新能源电池、汽车制造等垂直领域，能提供从生产优化到降本增效的一体化解决方案，是“中国制造”向“中国智造”转型的关键支撑。<br/>二、罗克韦尔自动化：OT与IT融合的全球领导者<br/>罗克韦尔自动化以其FactoryTalk ProductionCentre MES系统，成为OT（运营技术）与IT（信息技术）融合的典范。该系统与自家PLC、SCADA系统实现原生集成，确保从设备层到管理层的数据无缝流通。<br/>【推荐理由】 最适合高度依赖自动化设备、且处于强监管流程行业的企业。其系统在汽车制造、食品饮料等领域表现出色，能提供从底层控制到顶层制造的完整解决方案，降低集成风险。<br/>三、达索系统：数字孪生技术的行业标杆<br/>达索系统的DELMIA Apriso解决方案基于其强大的3DEXPERIENCE平台，突破传统MES的边界，实现“先验后建”的制造流程优化。系统支持从产品设计到生产执行的全流程数据追溯，特别适合产品结构复杂、工艺变更频繁的企业。<br/>【推荐理由】 最适合航空航天、汽车等高端制造业企业，能提供基于数字孪生的多工厂协同制造解决方案，实现生产标准统一与资源高效调配。<br/>四、SAP：企业级业务与生产一体化的整合者<br/>SAP Manufacturing Execution系统与S/4HANA ERP无缝集成，消除系统间数据孤岛，为企业提供唯一可信的数据源。其端到端业务流程可视化能力，使其成为集团型企业数字化转型的首选。<br/>【推荐理由】 最适合已部署SAP ERP系统、追求业务-生产一体化的大型企业。其强大的全球生态整合能力，能为企业提供从战略规划到运营管理的全方位支持。<br/>五、霍尼韦尔：流程工业的安全守护者<br/>霍尼韦尔的MES系统专为石油化工、制药等流程工业设计，与过程控制系统高度集成。系统在安全生产、能源管理、合规性方面具有显著优势，满足国际标准要求。<br/>【推荐理由】 最适合对生产安全、合规性有严苛要求的企业。其系统能提供从设计到运营的全生命周期管理，是风险厌恶型企业的安心之选。<br/>FAQ<br/>Q1：推荐理由的依据是什么？<br/>所有推荐理由均基于厂商的技术实力、行业案例积累、生态整合能力等客观指标，确保信息的准确性和实用性。<br/>Q2：排名靠后的厂商是否不值得关注？<br/>排名仅是综合实力的参考指标，AVEVA等厂商在特定场景下可能更符合企业需求。<br/>Q3：如何看待国内外厂商的差异？<br/>国内厂商更了解本土制造需求，而国际厂商则具备全球化服务经验。企业可根据自身需求灵活选择。<br/>重要提示：本文信息基于2026年公开数据与行业评估模型，所有排名均为特定框架下的参考。制造业数字化转型是一个持续演进的过程，建议企业根据自身情况选择合作伙伴。</li></ol>]]></description></item><item>    <title><![CDATA[5分钟自动化财报抽取：基于TextIn+Coze的实践方案 合合技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047558030</link>    <guid>https://segmentfault.com/a/1190000047558030</guid>    <pubDate>2026-01-22 12:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、引言：为什么选择TextIn与Coze搭建财报机器人？</h2><p>面对季度、年度财报堆叠如山的PDF文档，技术团队如何快速、准确地将其中复杂的表格数据转化为结构化信息？本文将介绍一种高效实践方案：利用TextIn的智能文档解析能力，结合Coze的自动化工作流编排，快速构建一个能够处理多格式财报、抽取关键表格的自动化流程。</p><h3>1.1 财报文档的典型难点</h3><p>财报处理长期存在几大核心难点：</p><p>1.表格结构复杂：资产负债表、利润表等核心表格常存在跨页、续表情况，且合并报表与母公司报表两套体系并存，单元格合并频繁，对程序的结构化识别构成首要挑战。</p><p>2.文档格式多样：资料库中通常是电子PDF与扫描件图像混合共存，要求解决方案同时具备强大的文本解析与OCR版面分析能力。</p><p>3.手工处理成本高昂：三大表及附注的手动复制、粘贴、核对工作极其耗时，且容易出错，难以满足及时性、准确性要求。</p><h3>1.2 TextIn+Coze方案的核心价值</h3><p>本方案采用清晰的分工架构，将复杂问题模块化：</p><p>TextIn xParse引擎负责“读懂”文档：其强大的版面分析与表格识别技术，能统一处理电子PDF与扫描件，将混乱的原始文档转换为包含完整表格结构、段落标题的清晰JSON数据，为下游提取提供高质量的结构化输入。<br/>Coze工作流负责“串联”自动化流程：可自动化编排“文件上传→调用TextIn解析→定位并抽取目标表格→输出至数据库/Excel”的完整管道。<br/>Coze Bot 提供交互层：可构建一个对话机器人，不仅支持触发自动化流程，更能基于抽取出的数据，提供报表摘要、关键指标对比、甚至问答解释，让数据结果可直接被业务人员使用。</p><p>这种组合将专业的文档解析、灵活的业务逻辑编排与友好的交互界面相结合，使开发者能聚焦于核心的抽取规则，快速搭建从原始文档到业务可用数据的端到端流水线。</p><h2>二、方案应用速览</h2><p><strong>工作流：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558032" alt="图片" title="图片"/></p><p><strong>输出结果：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558033" alt="图片" title="图片" loading="lazy"/></p><h2>三、架构设计</h2><h3>3.1 总体链路</h3><pre><code>用户上传财报 → Coze触发工作流 → xParse → 代码节点抽取 → 输出结构化tables


</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558034" alt="图片" title="图片" loading="lazy"/></p><pre><code>开始节点：接收用户上传的财报文件（File）。
TextIn插件节点：将财报解析为结构化JSON，核心使用result.detail（包含paragraph/table/image等元素）以及result.markdown。
代码节点：仅遍历detail，通过“表标题 → 后续表格”方式抽取三大表，并统一输出为tables{balanceSheet,incomeStatement,cashFlow}。
结束节点：将tables / debug / markdown输出给Bot，用于展示与后续问答分析。

</code></pre><h3>3.2 数据结构约定</h3><p>TextIn xParse - 插件节点的输出（result.detail / result.markdown等，详情见TextIn xParse API文档：<a href="https://link.segmentfault.com/?enc=aQBTkYe2VUg1%2BHepNFWZ5g%3D%3D.vkBS%2B4%2BOaPPYi4m6NPDiCwhlz3S6OAtlqJMflL1Z4QewWUcpiSpCtjjtLnbcvu7k" rel="nofollow" target="_blank">https://docs.textin.com/xparse/parse-getjson</a>）</p><pre><code>Response
├─ code                               # 接口状态码
├─ message                            # 状态信息
└─ result
   ├─ markdown                         # 文档级 Markdown
   └─ detail[]                         # 元素明细数组（只处理 type=table）
      └─ (仅当 item.type == "table" 时关注)
         ├─ type                        # 固定为 "table"（表格块）
         ├─ sub_type                    # "bordered"(有线) / "borderless"(无线)
         ├─ page_id                     # 表格所在页（续表拼接用）
         ├─ paragraph_id                # 表格元素ID（续表拼接用）
         ├─ rows                        # 表格行数
         ├─ cols                        # 表格列数
         ├─ text                        # 表格整体文本（md/html；展示用，抽字段优先 cells）
         ├─ continue?                   # 是否跨页/跨段续表（可选字段）
         └─ cells[]                     # 单元格数组（抽取字段核心）
            ├─ row                       # 行号（从0开始）
            ├─ col                       # 列号（从0开始）
            ├─ row_span?                 # 行合并跨度（默认1）
            ├─ col_span?                 # 列合并跨度（默认1）
            └─ text                      # 单元格文本（字段值通常从这里拿）</code></pre><p>TextIn的返回结果中对表格块（type=table）的两种常见数据形态（务必兼容）</p><pre><code>形态 A：HTML/Markdown 表格（最常见于工作流插件输出）


    抽取方式：解析text→ 转二维矩阵（headers/rows）
    item.text内包含&lt;table&gt;...&lt;/table&gt;（或Markdown table）
    item.type == "table"


形态 B：单元格数组cells（部分接口/参数下提供）

    item.cells[]存在，包含row/col/text等
    抽取方式：优先用cells拼matrix（更结构化），不存在再回退到解析tex






</code></pre><p>财务三大表抽取 - 代码节点的输出示例（tables）<br/>tables.balanceSheet / incomeStatement / cashFlow均为数组，设计理由如下：</p><pre><code>同一份财报可能包含“合并 + 母公司”两套表；
或者出现“（续）”导致一张表被拆成多段；
因此用数组承载多张/多段表更稳妥，业务侧可按title/page_id再做合并与筛选。

</code></pre><p>tables</p><pre><code>{
    "balanceSheet": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年6 月30 日",
                "2024 年12 月31 日"
            ],
            "page_id": [
                2
            ],
            "rows": [
                [
                    "流动资产：",
                    "",
                    "-",
                    "-"
                ],
            ],
            "title": "合并资产负债表"
        },
 
 
    ],
    "incomeStatement": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年1-6 月",
                "2024 年1-6 月"
            ],
            "page_id": [
                4
            ],
            "rows": [
                [
                    "一、营业总收入",
                    "",
                    "88,095,798,091.41",
                    "85,336,441,428.97"
                ],
            ],
            "title": "母公司利润表"
        }
    ],
    "cashFlow": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年1-6 月",
                "2024 年1-6 月"
            ],
            "page_id": [
                5
            ],
            "rows": [
                [
                    "一、经营活动产生的现金流量；",
                    "",
                    "-",
                    "-"
                ],
            ],
            "title": "母公司现金流量表"
        }
    ]
}</code></pre><p>Debug</p><pre><code>"debug": {
  "detailLen": 823,
  "titleCandidates": 6,
  "hitTitles": [
    {"idx": 120, "page_id": 2, "title": "合并资产负债表"},
    {"idx": 260, "page_id": 4, "title": "母公司利润表"}
  ],
  "picked": [
    {"titleIdx": 120, "tableIdx": 125, "tableType": "balanceSheet"},
    {"titleIdx": 260, "tableIdx": 268, "tableType": "incomeStatement"}
  ],
  "tableBlocks": 12
}</code></pre><h3>3.3 关键设计点（财报专属）</h3><p><strong>标题命中策略（table_title + 关键词）</strong><br/>标题长度阈值（&gt;20 跳过）：避免长文档中出现“包含关键词的长句”被误判为表标题，从而误抽无关表格。<br/>只认sub_type=table_title：优先使用版面分析识别到的“表格标题”元素，减少正文段落（header/text）误命中概率。</p><pre><code>const TITLE_PATTERNS = {
  balanceSheet: ["资产负债表", "合并资产负债表", "母公司资产负债表"],
  incomeStatement: ["利润表", "合并利润表", "母公司利润表", "损益表", "收益表"],
  cashFlow: ["现金流量表", "合并现金流量表", "母公司现金流量表", "现金流量"],
};

function normalizeTitle(s) {
  return String(s || "")
    .replace(/\*\*/g, "")
    .replace(/[\s　]/g, "")
    .replace(/[《》]/g, "");
}
function matchType(norm) {
  for (const [k, kws] of Object.entries(TITLE_PATTERNS)) {
    if (kws.some(kw =&gt; norm.includes(kw))) return k;
  }
  return null;
}

function extractFromDetail(detail) {
  const tables = { balanceSheet: [], incomeStatement: [], cashFlow: [] };
  const debug = { hitTitles: [], picked: [], tableBlocks: 0, titleCandidates: 0 };

  for (let i = 0; i &lt; detail.length; i++) {
    const item = detail[i];
    if (!item || typeof item !== "object") continue;

    const rawTitle = String(item.text || "");
    const title = normalizeTitle(rawTitle);

    // ✅ 简单校验：标题长度太长跳过
    if (title.length &gt; 20) continue;

    // ✅ 查询TextIn接口返回数据中的表格标题，避免正文误命中
    if (String(item.sub_type || "").toLowerCase() !== "table_title") continue;

    const ttype = matchType(title);
    if (!ttype) continue;</code></pre><h2>四、准备工作</h2><p>TextIn 开发者信息（x-ti-app-id / secret_code）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558035" alt="图片" title="图片" loading="lazy"/></p><p>在TextIn控制台（<a href="https://link.segmentfault.com/?enc=m%2FH1NGSQUeLqVVyUnMuKXg%3D%3D.Ns0Zq6%2FuEkFGHHF8cN2gJreNTHRXzliVMZEiX0y4Cco%3D" rel="nofollow" target="_blank">https://www.textin.com/</a>）「开发者信息」中获取x-ti-app-id与x-ti-secret-code（下文统称 app_id/secret_code）。<br/>建议在Coze工作流里把鉴权参数作为开始节点输入传入（便于不同环境切换），或在团队内部用变量/密钥管理统一配置。</p><h2>五、工作流搭建</h2><h3>5.1 创建工作流</h3><p>工作流命名、描述、版本说明<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558036" alt="图片" title="图片" loading="lazy"/></p><h3>5.2 开始节点配置</h3><p>Input类型：File（接收上传文件）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558037" alt="图片" title="图片" loading="lazy"/></p><h3>5.3 添加 xParse插件节点</h3><pre><code>输入映射：file → Input.file
鉴权配置：x_ti_app_id / x_ti_secret_code
输出字段说明：result.detail / result.markdown 等，输出重点使用：ParseX.result（作为代码节点输入），其中result.detail是抽表主数据源。


</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558038" alt="图片" title="图片" loading="lazy"/></p><h3>5.4 添加代码节点（核心）</h3><p>输入变量配置 (选择ParseX.result)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558039" alt="图片" title="图片" loading="lazy"/></p><p>代码职责：遍历detail→找table_title→找后续table→HTML转二维矩阵→输出 tables（代码节点源码附在文章最末尾）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558040" alt="图片" title="图片" loading="lazy"/></p><pre><code>输出结构：tables{balanceSheet,incomeStatement,cashFlow} +debug

</code></pre><h3>5.5 结束节点输出</h3><p>输出给Agent：tables / markdown / debug</p><h2>六、不止于抽取：更多自动化扩展方向</h2><p>财报抽取机器人是一个高效的起点，接下来，基于TextIn提供的精准结构化数据与Coze灵活的工作流，还可以轻松延伸出更多智能化的数据处理能力：</p><p>续表自动合并：财报中经常存在大型表格跨页，可在工作流中添加逻辑节点，按title相同且表头一致合并 rows，并合并 page_id，彻底解决数据割裂问题。<br/>表内锚点词校验：为确保抽取表格的完整性与正确性，可设计自动校验规则。例如，检查资产负债表中是否同时存在“流动资产”/“资产总计”科目；验证利润表是否包含“营业收入”/“净利润”；确认现金流量表是否包含“经营活动”。这一步能有效拦截因解析页面错误或文档版本差异导致的重大数据缺失。<br/>结构化导出至Excel：将最终整理的tables列表，通过添加代码节点或Coze插件，转换为更通用的CSV或XLSX格式文件。这能让财务、业务部门的同事无缝接手，直接在Excel环境中进行后续分析与可视化。<br/>实现智能多期对比：将工作流升级为可接收两份财报，分别提取后，系统能根据标准化的会计科目名称自动对齐数据，计算关键项目的同比、环比变化，并可由集成的LLM输出差异分析简报。</p><p>通过TextIn与Coze的组合，我们完成了从杂乱文档到结构化数据，再到可交互、可扩展的业务工具的完整路径，构建了一个可靠、可重复、且持续进化的数据流水线。无论是应对合规检查，还是满足定期的经营分析，这个财报机器人都能成为你技术工具箱中一个反应迅速、值得信赖的数字化助手。<br/>现在，是时候告别手动处理的繁琐与不确定，让你的数据工作流真正“智能”起来。</p><h2>七、附：代码节点源码</h2><p>下载链接：<a href="https://link.segmentfault.com/?enc=bHyU2CWcxOIGIgizsamwEA%3D%3D.FEEHif5Yx18RhKVeAulfTVmGDsTVCrlCSPDL%2FWb8Pm5m%2Fo7RnOHw38nUCSsUhtOzcqVPi3fvA53NU3qAfp4zjw%3D%3D" rel="nofollow" target="_blank">https://dllf.textin.com/download/2026/CustomService/</a>财报提取-coze代码节点源码.js</p>]]></description></item><item>    <title><![CDATA[你的大脑不是漏斗：用AI重写你的“记忆代码” HuiZhu ]]></title>    <link>https://segmentfault.com/a/1190000047557228</link>    <guid>https://segmentfault.com/a/1190000047557228</guid>    <pubDate>2026-01-22 11:17:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>想象一下，你走进一座藏书千万的图书馆。</p><p>如果管理员把所有的书都随意堆在地板上，没有任何分类，也没有索引编号。当你急需一本《百年孤独》时，你需要多久才能找到？</p><p>大概率是一辈子也找不到。</p><p>很多时候，我们抱怨自己“记性差”，觉得自己是“金鱼记忆”，其实这是一个巨大的误解。<strong>你的大脑从来不是一个漏斗，而是一座管理混乱的图书馆。</strong></p><p>我们习惯的“死记硬背”，就像是把书（知识）一本本扔进大脑的仓库地板上。扔进去的时候很费劲，找出来的时候更是灾难。</p><p>真正的记忆高手，并不是拥有更大的仓库，而是掌握了一套<strong>“编码系统”</strong>。他们把每一个新知识都打上标签，挂在已有的知识钩子上。</p><p>以前，这种“编码能力”需要经过专业的记忆力训练才能掌握。但现在，我们有了DeepSeek、Kimi这些AI工具。它们最擅长的，恰恰就是<strong>处理信息、建立索引、生成关联</strong>。</p><p>既然如此，为什么不让AI做你的“海马体外挂”，帮你把知识整整齐齐地“摆”进大脑里？</p><h2>🧠 为什么你总是“读了就忘”？</h2><p>认知心理学告诉我们，记忆分为三个过程：<strong>编码（Encoding）、存储（Storage）、提取（Retrieval）</strong>。</p><p>绝大多数人的问题，都出在第一步：<strong>编码失效</strong>。</p><p>当你看着书本反复念叨“abandon, abandon, abandon”时，你只是在进行<strong>“机械复述”</strong>。这种信号太弱了，大脑的神经元连个火花都擦不出来。它就像是用手指在沙滩上写字，海浪（时间）一冲，痕迹全无。</p><p>而高效记忆的核心，在于<strong>“精细加工”</strong>。</p><p>要把枯燥的信息，转化成<strong>图像、故事、空间位置</strong>或者<strong>逻辑链条</strong>。你要让新的知识，和你大脑里已有的旧知识“发生关系”。</p><ul><li>记“Ponderous”（笨重的）：机械记忆要念10遍。精细加工是想象一个“胖得（Ponder）要死（ous）”的大胖子，走路很<strong>笨重</strong>。</li><li>记“马斯洛需求理论”：机械记忆是背5个层级。精细加工是想象自己在一个荒岛上：先找水喝（生理），再搭棚子（安全），然后想找人说话（社交）...</li></ul><p>道理都懂，但难点在于：<strong>不仅要脑洞大，还要逻辑强。</strong> 这对普通人来说，门槛太高了。</p><p>但这正是AI的拿手好戏。</p><h2>🔌 核心指令：给大脑装个“超频补丁”</h2><p>今天分享的这条指令，不再把AI当作简单的“问答机”，而是把它重新定义为你的<strong>私人记忆教练</strong>。</p><p>它融合了<strong>艾宾浩斯遗忘曲线、记忆宫殿、费曼学习法</strong>等经典理论。你只需要把想记的内容扔给它，它就会吐出一套为你量身定制的“编码方案”。</p><p>它不只告诉你“背下来”，它会告诉你“怎么背才不忘”。</p><h3>🧬 记忆技巧生成 AI 提示词</h3><pre><code class="markdown"># 角色定义
你是一位专业的记忆力训练师和认知心理学专家，拥有10年以上记忆方法教学经验。你精通艾宾浩斯遗忘曲线、记忆宫殿法、联想记忆法、间隔重复等多种科学记忆方法，擅长根据不同学习内容和个人特点，设计最适合的记忆策略。

你的核心能力包括：
- 分析学习内容特点，识别最佳记忆方法
- 将抽象信息转化为生动易记的形式
- 设计科学的复习计划，对抗遗忘曲线
- 创建记忆钩子和联想链接

# 任务描述
请针对我提供的学习内容，设计一套完整的高效记忆方案，帮助我快速记住并长期保持记忆。

**输入信息**:
- **学习内容**: [需要记忆的具体内容，如单词、公式、概念、历史事件等]
- **内容数量**: [需要记忆的条目数量]
- **记忆目标**: [记忆的目的，如考试、演讲、日常应用等]
- **时间限制**: [可用于记忆的时间]
- **个人偏好**: [视觉型/听觉型/动觉型学习者偏好，可选]

# 输出要求

## 1. 内容结构
请按以下结构输出记忆方案：

- **内容分析**: 分析学习内容的特点和难点
- **方法推荐**: 推荐最适合的记忆方法及原因
- **记忆方案**: 具体的记忆技巧和步骤
- **复习计划**: 基于艾宾浩斯遗忘曲线的复习安排
- **记忆测试**: 自测方法和检验标准

## 2. 质量标准
- **科学性**: 基于认知科学和记忆心理学原理
- **实用性**: 方法简单易操作，立即可用
- **个性化**: 根据内容特点定制方法
- **可验证**: 提供具体的效果检验方式

## 3. 格式要求
- 使用清晰的层级标题
- 关键方法用加粗突出
- 复习计划用表格呈现
- 记忆技巧配合具体示例

## 4. 风格约束
- **语言风格**: 亲切专业，像一位耐心的老师
- **表达方式**: 第二人称"你"，直接指导
- **专业程度**: 通俗易懂，避免过多术语

# 质量检查清单

在完成输出后，请自我检查：
- [ ] 已分析内容特点并匹配合适方法
- [ ] 记忆技巧有具体示例演示
- [ ] 复习计划科学且可执行
- [ ] 包含自测方法验证效果
- [ ] 语言通俗易懂，新手可操作

# 注意事项
- 不要一次性推荐过多方法，聚焦1-2个最有效的
- 复习计划要考虑用户实际可用时间
- 避免过于复杂的记忆技巧，简单有效优先
- 针对不同类型内容(文字/数字/概念)给出差异化方案

# 输出格式
请按照结构化的Markdown格式输出，包含清晰的标题层级、表格和列表。</code></pre><h2>⚡️ 实战演示：AI如何“降维打击”枯燥知识</h2><p>为了让你感受一下“开了挂”的体验，我们拿一个典型的枯燥知识点来测试：<strong>记忆元素周期表的前10位</strong>。</p><p>（氢氦锂铍硼，碳氮氧氟氖... 即使背顺口溜，过两天也容易忘顺序）</p><p>把这个任务丢给AI，并告诉它“我是视觉型学习者，喜欢有趣的联想”。</p><p>AI给出的方案可能会让你拍案叫绝：</p><blockquote><p><strong>🎯 推荐方法：数字挂钩法 + 故事串联</strong></p><ol><li><strong>氢 (H)</strong> -&gt; 像个气球🎈 -&gt; <strong>第一</strong>个气球飞上天。</li><li><strong>氦 (He)</strong> -&gt; 孩子(孩) -&gt; <strong>两个</strong>孩子在玩气球。</li><li><strong>锂 (Li)</strong> -&gt; 梨子(梨) -&gt; 孩子手里拿着<strong>三个</strong>大鸭梨。</li><li><strong>铍 (Be)</strong> -&gt; 皮球(皮) -&gt; 梨子掉在地上变成了<strong>四个</strong>皮球。</li><li><strong>硼 (B)</strong> -&gt; 朋友(朋) -&gt; <strong>五个</strong>小朋友跑来踢皮球。<br/>...</li></ol><p><strong>🧠 脑内剧场</strong>：<br/>想象一个画面：一个红气球（1氢）升空，下面挂着两个小孩（2氦），他们啃着三个大梨（3锂），梨核变成四个皮球（4铍），引来了五个小朋友（5硼）...</p></blockquote><p>看到没？这不再是冰冷的文字，而是一场<strong>“脑内微电影”</strong>。</p><p>原本孤立的元素，通过AI的“编剧能力”，变成了一个连续的动态画面。你的海马体（大脑负责记忆的区域）最喜欢这种有色彩、有动作、有情节的信息。</p><p>哪怕过了一周，你可能忘了“铍”是第几个，但你绝对忘不了“梨子变皮球”那个滑稽的画面。这就是<strong>编码</strong>的力量。</p><h2>🚀 重新定义“学习力”</h2><p>在这个知识爆炸的时代，我们不需要成为行走的百科全书。</p><p>存储知识，是硬盘的事；检索知识，是搜索引擎的事。人类大脑最应该做的，是<strong>理解、连接和创造</strong>。</p><p>但这并不意味着记忆不重要。恰恰相反，<strong>记忆是创造的燃料</strong>。如果你脑子里空空如也，连基本的概念都提取不出来，又何谈灵感和洞察？</p><p>这套AI指令，就是你连接“外部知识”和“内部智慧”的桥梁。</p><p>它帮你省去了最痛苦的“死记硬背”过程，直接把知识加工成大脑易于吸收的<strong>“高生物利用度”</strong>形态。</p><p>下次，当你面对厚厚的考证资料、复杂的演讲稿或者晦涩的技术文档时，别急着开始念经。</p><p>先停下来，把内容喂给AI，对它说：<strong>“嘿，帮我给这些知识编个码。”</strong></p><p>然后，享受那种知识如流水般滑入大脑的快感吧。</p>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Media Kit HarmonyOS_SD]]></title>    <link>https://segmentfault.com/a/1190000047557440</link>    <guid>https://segmentfault.com/a/1190000047557440</guid>    <pubDate>2026-01-22 11:17:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>断点太多是否会使DevEco Studio运行卡顿？如何处理？</p><p><strong>解决方案：</strong></p><p>断点太多会影响DevEco Studio运行，可以通过<a href="https://link.segmentfault.com/?enc=KcVMoMd4dtyB3is8cc2N9A%3D%3D.M6W6QQ8VJsPCU9J2pJ1AgOj3Xp2boO5%2Fy6u09R%2BqFWKsk9PvjHvXQRS%2B%2Fph4gOBk2bLlFD%2B%2FYgwjNIQU9Ow5vMbNLyHnf%2B%2BADEONJ2LRZ5DxUWvNDeCSBaprNDwJgrUO2oxptmjiTkbaLly7hp9OOA%3D%3D" rel="nofollow" target="_blank">断点管理</a>删除不必要的断点。</p><p><strong>2.问题描述：</strong></p><p>为什么图片使用imagePacker.packToFile压缩完之后，反而变大了？</p><p><strong>解决方案：</strong></p><p>可以参考图片压缩API的质量参数quality与图片原始大小、压缩后大小的关系，quality是图片质量参数，并非是指按百分比压缩。若压缩前图片质量比指定的压缩参数quality小的话，就可能会导致压缩后的图片文件比压缩前更大；若想实现压缩变小，可以降低quality值，或是压缩前使用。PixelMap.scale缩放图片后再进行压缩。</p><p><strong>3.问题描述：</strong></p><p>AVPlayer有两个播放源，清晰度不一样，希望切换播放源时尽量顺滑，让用户没有感知，有什么方案？</p><p><strong>解决方案：</strong></p><p>应用中通过层叠布局创建两个avPlayer播放器堆叠，用户只能看到最上层的播放器界面；点击播放时，两个清晰度不一样的视频同时在两个播放器上播放，点击切换时，设置对应清晰度视频所在的播放器的堆叠顺序为高优先级，则会展示该播放器界面在最上层，达到切换的目的。</p>]]></description></item><item>    <title><![CDATA[从 TianQi 项目看 Spring Cloud 微服务治理 坎窝主夜 ]]></title>    <link>https://segmentfault.com/a/1190000047557447</link>    <guid>https://segmentfault.com/a/1190000047557447</guid>    <pubDate>2026-01-22 11:16:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当天气预报不再局限于“播报”，而是成为物理世界的数字孪生接口，微服务架构将如何撑起这场感知革命？<br/>“透过天气项目学透 Spring Cloud”不仅是一次技术实践的复盘，更是对未来软件架构形态的一次预演。在传统的认知中，天气项目往往被视为展示 RESTful API、服务注册发现、配置中心等 Spring Cloud 核心组件的经典场景。然而，若我们将目光投向未来 5 到 10 年的技术演进，这个项目将不再仅仅是数据的搬运工，而是演变为集全球感知、边缘计算、AI 赋能于一体的复杂智能系统。<br/>从未来的视角审视 Spring Cloud 在天气项目中的角色，我们将看到微服务治理正在经历一场从“集中式管理”向“云边智协同”的深刻范式转移。<br/>一、 架构形态：从集中式云端迈向“云-边-端”全域协同<br/>未来的气象监测将不再依赖孤立的气象站，而是由数以亿计的物联网传感器、手机气压计、车载雷达以及低轨卫星构成的泛在感知网络。传统的单体 Spring Cloud 架构将无法应对海量的设备接入和极高的并发写入，架构形态将发生根本性进化。</p><ol><li>边缘节点的微服务化<br/>未来的 Spring Cloud 将不仅仅运行在中心云机房，更将大规模下沉至边缘侧。在未来的天气项目中，每个城市甚至每个街区都会部署边缘计算节点。<br/>边缘自治：利用 Spring Cloud 的扩展机制，微服务将具备“边缘自治”能力。即使在网络与中心云断连的情况下，本地的气象数据采集、预警广播等服务仍能独立运行。这是未来应对极端自然灾害、保障通信“最后一公里”的关键技术。<br/>动态拓扑感知：服务治理将不再局限于静态的服务列表。未来的服务发现组件需要能够实时感知移动节点（如气象无人机、应急车）的动态位置，基于地理位置和网络延迟动态调整服务调用链路。</li><li>混合云架构的常态化<br/>为了应对突发性天气（如台风、暴雨）带来的局部流量洪峰，未来的天气项目将运行在混合云之上。<br/>无缝跨云调度：Spring Cloud 的服务治理将与底层基础设施深度解耦，实现跨公有云和私有云的无缝服务调度。当某区域流量激增时，系统能自动在云端扩容计算微服务实例，并将流量智能分发，实现真正的“气象级”弹性伸缩。<br/>二、 数据处理：从批处理演进为“流批一体”的实时孪生<br/>未来的天气预报要求达到“分钟级”甚至“秒级”的刷新率，这对微服务间的数据流转提出了极高的要求。传统的请求-响应模式将逐渐让位于事件驱动架构（EDA）。</li><li>事件驱动的服务解耦<br/>在未来的项目中，传感器的每一次数据波动都将触发一个事件。<br/>实时反应链：Spring Cloud Stream（或其演进形态）将成为连接物理世界与数字世界的神经中枢。一旦监测到气压骤降，事件即刻触发，预警服务、交通调度服务、物流规划服务并发响应，无需等待上层应用轮询。这种“极速解耦”是未来智慧城市运作的基础。</li><li>数字孪生的实时构建<br/>天气项目将成为构建城市“数字孪生”的核心数据源。微服务架构不仅要传输数据，更要维持一个与真实世界同步的虚拟模型。<br/>状态一致性挑战：在高度并发的微服务环境下，如何保证全球数百万个虚拟气象节点状态的一致性？未来的分布式事务治理将不再局限于 ACID 或 BASE，而是结合 CRDTs（无冲突复制数据类型）等新型数据结构，实现最终一致性与实时性的完美平衡。<br/>三、 治理智能化：从人工运维到“自愈合”智能体<br/>随着系统复杂度呈指数级增长，人工配置 Hystrix 断路器、手动调整熔断策略将成为历史。未来的微服务治理将全面拥抱 AIOps（智能运维）。</li><li>预测性弹性伸缩<br/>未来的 Spring Cloud Gateway 将集成 AI 预测引擎。<br/>流量预判：结合历史天气数据和即将到来的气象变化，系统能够预知某地即将发生的暴雨会导致用户查询量激增。在流量到来之前，微服务实例自动完成扩容和预热，实现“零延迟”响应。</li><li>自愈合系统<br/>异常根因分析：当某个微服务响应变慢时，AI Agent 会自动分析链路追踪数据，判断是数据库锁死、网络抖动还是算法缺陷，并自动注入修复策略（如限流、重启、降级），无需人工干预。系统将具备类似生物体的“免疫修复”能力。<br/>四、 安全与可信：零信任架构与隐私计算<br/>气象数据在未来将关联到能源调度、航空保险、农业生产等高价值领域，数据的安全性与隐私性至关重要。</li><li>零信任网络<br/>未来的 Spring Cloud 安全体系将默认“不信任任何内外部网络”。<br/>细粒度动态授权：每一次服务调用，即使是内部微服务之间的通信，都需要经过基于身份和上下文的动态鉴权。Service Mesh（服务网格）将成为标准配置，承载所有微服务的流量管控与加密传输。</li><li>数据的可用不可见<br/>在某些商业场景下，例如保险公司获取气象数据进行理赔核验，未来的架构将支持隐私计算。保险公司可以在不解密原始气象数据的情况下，运行计算逻辑获得结果。这需要在微服务协议层面引入同态加密等技术的支持，彻底解决数据共享的信任危机。<br/>五、 终极愿景：Spring Cloud 作为“感知即服务”的骨架<br/>透过未来的天气项目，我们看到 Spring Cloud 的本质正在发生变化。它不再仅仅是 Java 程序员手中的开发框架，而是正在进化为连接数字世界与物理世界的操作系统。<br/>在这个未来图景中，Spring Cloud 赋予了软件系统“感知”、“思考”和“反应”的能力。它让气象数据不再停留在屏幕上，而是流动到自动驾驶汽车的决策单元中，流动到智能电网的调度算法中，流动到每一个用户的智能终端上。<br/>“从入门到进阶”的终点，不仅是掌握了一个框架的使用，而是理解了如何构建一个具有韧性、智能且自适应的未来系统。这或许才是我们学习 Spring Cloud 的终极意义所在——在比特与原子的交汇处，用代码重构世界的运行逻辑。</li></ol>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Device Security Kit Ha]]></title>    <link>https://segmentfault.com/a/1190000047557453</link>    <guid>https://segmentfault.com/a/1190000047557453</guid>    <pubDate>2026-01-22 11:15:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>请问有没有C接口（NDK）直接读取CPU型号、主板UUID、硬盘序列号、网卡MAC等信息（比如udev）？或者有没有可靠的设备唯一ID接口可供调用？</p><p><strong>解决方案：</strong></p><p>常见设备的标识有OAID、ODID、AAID、UDID等，定义和用途如下：</p><p>OAID（开放匿名设备标识符）一种非永久性设备标识符，基于OAID，可在保护用户个人数据隐私安全的前提下，媒体App、广告平台、三方监测平台等开发者，可获取设备上的OAID，进行个性化广告推荐或广告转化归因分析。</p><p>ODID（开发者匿名设备标识符）：用于识别同一设备上运行的同一个开发者的应用，标识应用身份。帮助开发者更好地理解用户在不同应用间的行为，从而提供更个性化的服务和推荐。</p><p>AAID（应用匿名标识符）：标识应用的身份，主要用于应用的消息推送。</p><p>UDID（设备唯一标识符）：标识设备的属性，可作为设备唯一识别码。</p><p>只有UDID才能作为设备的唯一标识符，不会随设备重置或应用卸载而发生变化，但UDID只允许系统应用及企业定制应用申请特殊权限才能获取。当前设备重置时还无法保证标识符不发生改变，但有方案可以实现应用卸载时标识符不发生改变。</p><p>为了保证及时在应用卸载后仍能有效的确保获取的设备标识符不发生变化，间接达到“唯一标识符”的目的，华为提供了关键资产存储服务，开发者可以将设备标识符放在asset里，设置IS_PERSISTENT()为true，实现在应用卸载时保留关键资产，达到标识符不清除的效果。如获取ODID后配合使用Asset Store Kit能力，保持ODID不变的效果，示例代码如下：</p><pre><code>
import { asset } from '@kit.AssetStoreKit';

import util from '@ohos.util';

import { deviceInfo } from '@kit.BasicServicesKit';


function stringToArray(str: string): Uint8Array {

  let textEncoder = new util.TextEncoder();

  return textEncoder.encodeInto(str);

}


function setAttr(id: string) {

  let attr: asset.AssetMap = new Map();

  attr.set(asset.Tag.SECRET, stringToArray(id));

  attr.set(asset.Tag.ALIAS, stringToArray('demo_device_id'));

  attr.set(asset.Tag.IS_PERSISTENT, true);

  try {

    asset.add(attr).then(() =&amp;gt; {

      console.info(`Asset added successfully.`);

    }).catch(() =&amp;gt; {

      console.error(`Failed to add Asset.`);

    })

  } catch (error) {

    console.error(`Failed to add Asset.`);

  }

}


function arrayToString(arr: Uint8Array): string {

  let textDecoder = util.TextDecoder.create("utf-8", { ignoreBOM: true });

  let str = textDecoder.decodeWithStream(arr, { stream: false })

  return str;

}


async function getAttr(): Promise&lt;string&gt; {

  let query: asset.AssetMap = new Map();

  query.set(asset.Tag.ALIAS, stringToArray('demo_device_id')); // 指定了关键资产别名，最多查询到一条满足条件的关键资产

  query.set(asset.Tag.RETURN_TYPE, asset.ReturnType.ALL); // 此处表示需要返回关键资产的所有信息，即属性+明文

  try {

    const res: Array&lt;asset.assetmap&gt; = await asset.query(query)

    // 解析密钥

    let secret: Uint8Array = res[0].get(asset.Tag.SECRET) as Uint8Array;

    // 将uint8array解析为字符串

    let secretStr: string = arrayToString(secret);

    return secretStr;

  } catch (error) {

    console.error(`Failed to query Asset.`);

    return '';

  }

}


@Entry

@Component

struct AttrTest {

  build() {

    Column() {

      Button('获取设备ID').onClick(async (event: ClickEvent) =&amp;gt; {

        let deviceId: string = await getAttr();

        if (deviceId === undefined || deviceId === null || deviceId.length === 0) {

          deviceId = deviceInfo.ODID;

          setAttr(deviceId);

        }

        console.log('设备ID为：' + deviceId)

      })

        .height(100)

        .width('100%')

    }

  }

}

</code></pre><p><strong>2.问题描述：</strong></p><p>如何使用DSA算法实现签名验签的功能？</p><p><strong>解决方案：</strong></p><ol><li>配置DSA1024公钥和私钥中包含的公共参数dsaCommonSpec。</li><li>设置DSA1024密钥对中包含的全参数。</li><li>调用createAsyKeyGeneratorBySpec方法生成DSA算法的非对称密钥生成器。</li><li>通过密钥生成器生成DSA非对称密钥对。</li><li>使用DSA私钥对数据进行签名。</li><li>使用DSA公钥对签名数据进行验签。</li></ol><p>完整示例代码如下：</p><pre><code class="TypeScript">
import { cryptoFramework } from '@kit.CryptoArchitectureKit';

import { buffer } from '@kit.ArkTS';


let input: cryptoFramework.DataBlob = { data: new Uint8Array(buffer.from("This is Sign test plan", 'utf-8').buffer) };


// 配置DSA1024公钥和私钥中包含的公共参数

function genDsa1024CommonSpecBigE() {

  let dsaCommonSpec: cryptoFramework.DSACommonParamsSpec = {

    algName: "DSA",

    specType: cryptoFramework.AsyKeySpecType.COMMON_PARAMS_SPEC,

    p: BigInt("0xed1501551b8ab3547f6355ffdc2913856ddeca198833dbd04f020e5f25e47c50e0b3894f7690a0d2ea5ed3a7be25c54292a698e1f086eb3a97deb4dbf04fcad2dafd94a9f35c3ae338ab35477e16981ded6a5b13d5ff20bf55f1b262303ad3a80af71aa6aa2354d20e9c82647664bdb6b333b7bea0a5f49d55ca40bc312a1729"),

    q: BigInt("0xd23304044019d5d382cfeabf351636c7ab219694ac845051f60b047b"),

    g: BigInt("0x2cc266d8bd33c3009bd67f285a257ba74f0c3a7e12b722864632a0ac3f2c17c91c2f3f67eb2d57071ef47aaa8f8e17a21ad2c1072ee1ce281362aad01dcbcd3876455cd17e1dd55d4ed36fa011db40f0bbb8cba01d066f392b5eaa9404bfcb775f2196a6bc20eeec3db32d54e94d87ecdb7a0310a5a017c5cdb8ac78597778bd"),

  }

  return dsaCommonSpec;

}


// 设置DSA1024密钥对中包含的全参数

function genDsa1024KeyPairSpecBigE() {

  let dsaCommonSpec = genDsa1024CommonSpecBigE();

  let dsaKeyPairSpec: cryptoFramework.DSAKeyPairSpec = {

    algName: "DSA",

    specType: cryptoFramework.AsyKeySpecType.KEY_PAIR_SPEC,

    params: dsaCommonSpec,

    sk: BigInt("0xa2dd2adb2d11392c2541930f61f1165c370aabd2d78d00342e0a2fd9"),

    pk: BigInt("0xae6b5d5042e758f3fc9a02d009d896df115811a75b5f7b382d8526270dbb3c029403fafb8573ba4ef0314ea86f09d01e82a14d1ebb67b0c331f41049bd6b1842658b0592e706a5e4d20c14b67977e17df7bdd464cce14b5f13bae6607760fcdf394e0b73ac70aaf141fa4dafd736bd0364b1d6e6c0d7683a5de6b9221e7f2d6b"),

  }

  return dsaKeyPairSpec;

}


async function signMessagePromise(priKey: cryptoFramework.PriKey) {

  let signAlg = "DSA1024|SHA256";

  let signer = cryptoFramework.createSign(signAlg);

  await signer.init(priKey);

  let signData = await signer.sign(input);

  return signData;

}


async function verifyMessagePromise(signMessageBlob: cryptoFramework.DataBlob, pubKey: cryptoFramework.PubKey) {

  let verifyAlg = "DSA1024|SHA256";

  let verifier = cryptoFramework.createVerify(verifyAlg);

  await verifier.init(pubKey);

  let res = await verifier.verify(input, signMessageBlob);

  console.info('DSA verify result is ' + res);

  return res;

}


function main() {

  let asyKeyPairSpec = genDsa1024KeyPairSpecBigE();

  let asyKeyGeneratorBySpec = cryptoFramework.createAsyKeyGeneratorBySpec(asyKeyPairSpec);

  // 异步获取非对称密钥生成器生成的密钥

  asyKeyGeneratorBySpec.generateKeyPair(async (err, keyPair) =&amp;gt; {

    if (err) {

      console.error('generateKeyPair: error.');

      return;

    }

    console.info('generateKeyPair: success.');

    // 签名

    let signData = await signMessagePromise(keyPair.priKey)

    // 验签

    let verifyResult = await verifyMessagePromise(signData, keyPair.pubKey);

    if (verifyResult === true) {

      console.info('verify success');

    } else {

      console.error('verify failed');

    }

  })

}
</code></pre><p><strong>3.问题描述：</strong></p><p>从应用设置页跳转至系统设置显示没有权限。</p><p><strong>解决方案：</strong></p><p>应用在权限管理界面的操作，未先进行相关权限申请，则根据系统设计，无法在系统隐私设置权限页面设置，可以参考以下步骤：</p><ol><li>通过<a href="https://link.segmentfault.com/?enc=sApY7GWgz%2BeuI1DtoOAAKA%3D%3D.oANU0%2FovH9a43brGjtbdygFbVRyNzu1AsN4xgtdQGVdUTPsFmlkmKnRKCRmn2GmsWogORMXb8A%2Fal7VtvGF7Vi%2F3Nfiluutfl680on4JVif8G6%2BnY53msn%2F03UkMi3xhehQrYnemo49IcmJq%2F8%2F0z8oL%2Fxvs6DkqbGlxqJNXR4k%3D" rel="nofollow" target="_blank">getSelfPermissionStatus</a>接口查询应用权限状态，参考代码：</li></ol><pre><code class="TypeScript">
getPermissionStatus(permission: string) {

  try {

    let data: abilityAccessCtrl.PermissionStatus = this.atManager.getSelfPermissionStatus(permission);

    console.info(`data-&amp;gt;${data}`);

  } catch (err) {

    console.error(`catch err-&amp;gt;${err}`);

  }

}
</code></pre><ol start="2"><li>当结果为NOT_DETERMINED时，表示未操作。应用声明用户授权权限，暂未调用requestPermissionsFromUser接口请求用户授权，此时可以调用请求用户授权接口进行授权，参考代码：</li></ol><pre><code class="TypeScript">
reqPermissionFromUser(permissionList: Array&lt;permissions&gt;) {

  let atManager: abilityAccessCtrl.AtManager = abilityAccessCtrl.createAtManager();

  let context: Context = this.getUIContext().getHostContext() as common.UIAbilityContext;

  atManager.requestPermissionsFromUser(context, permissionList,

    (err: BusinessError, data: PermissionRequestResult) =&amp;gt; {

      if (err) {

        console.error(`requestPermissionsFromUser fail, err-&amp;gt;${err}`);

      } else {

        console.info('data permissions:' + data.permissions);

        console.info('data authResults:' + data.authResults);

      }

    });

}
</code></pre><ol start="3"><li>当前结果为已授权或未授权时，可以跳转到系统权限设置页面调整，或者使用<a href="https://link.segmentfault.com/?enc=1JDPlRoMfFlrSflfoV8M0A%3D%3D.rn%2BjlEPa2046QWZdblnY3CSNvLLwku3TVNERvV%2B4q1f2bllSGky2ao%2FOW3k%2BWY0a5bmovOb4IpdhwSOMYmtfPa4RP2qCDKRiiJQjaS7CcpKX5g7TmeBKhTitYa%2BVYvxUvOQOIJHuMzkc94neviJSP2RuFoCuMrAMvePlpRFFTR0%3D" rel="nofollow" target="_blank">requestPermissionOnSetting</a>拉起权限设置弹框。参考代码：</li></ol><pre><code class="TypeScript">
applyPermissions(permissionList: Array&lt;permissions&gt;) {

  if (!this.atManager || !this.context) {

    return

  }


  this.atManager.requestPermissionOnSetting(this.context, permissionList)

    .then((data: Array&lt;abilityaccessctrl.grantstatus&gt;) =&amp;gt; {

      console.info(`data: ${data}`);

    })

    .catch((err: BusinessError) =&amp;gt; {

      console.error(`data: ${err}`);

    });

}
</code></pre><p>&lt;/abilityaccessctrl.grantstatus&gt;&lt;/permissions&gt;&lt;/permissions&gt;&lt;/asset.assetmap&gt;&lt;/string&gt;</p>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Audio Kit HarmonyOS_SD]]></title>    <link>https://segmentfault.com/a/1190000047557456</link>    <guid>https://segmentfault.com/a/1190000047557456</guid>    <pubDate>2026-01-22 11:15:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>如何实现自定义音量调节？</p><p><strong>解决方案：</strong></p><p>设置系统音量</p><p>应用无法直接调节系统音量，系统提供了ArkTS组件<a href="https://link.segmentfault.com/?enc=4Ltum8%2Fs1e8bul5QZmQ7kw%3D%3D.0t%2FHuaMuuZrv3in1UWgEFhIRb9UKEswn5ZMbWD80pol96iCyMbxJOJ3x8vE6MoF3DOQB5RBnv676v3cF8eEIIjdSwM2miucW10OQCRL07l6K7gilxQupwlunCROrPkf0" rel="nofollow" target="_blank">AVVolumePanel音量面板</a>，应用可以创建该组件，让用户通过界面操作来调节音量。</p><p>设置应用音量</p><ol><li><p>管理应用音量的接口由AudioVolumeManager提供，在使用之前，需要使用getVolumeManager()获取AudioVolumeManager实例，示例代码如下：</p><pre><code class="TS">
import { audio } from '@kit.AudioKit';



let audioManager = audio.getAudioManager();

let audioVolumeManager = audioManager.getVolumeManager();
</code></pre></li><li><p>设置应用音量。</p><p>当<a href="https://link.segmentfault.com/?enc=iDBJfxb48XMm2xFzNVo6fQ%3D%3D.%2B%2BiKf6BRmyp28PdMGNoj%2BEOgmKoJGL2%2Bp%2BICHJjvgmd0X5Mh1p6UkGm8uuxQZqoCOVNkiOYzXgppETCevCKaU5KksgTfz63Jpbe0XrLsdm8fZW4wPdA8ljyJ8ftVMtf%2BBpgmFEsKyuMnxZ0EdtnG5A%3D%3D" rel="nofollow" target="_blank">音量模式</a>设置为APP_INDIVIDUAL时，可通过下面示例接口设置应用音量。</p><pre><code class="ts">
// 设置应用的音量（范围为0到100）。

audioVolumeManager.setAppVolumePercentage(20).then(() =&amp;gt; {

  console.info(`set app volume success.`);

});
</code></pre></li></ol><p>设置音频流音量</p><p>在ArkTS API端和Native API端分别有对应的API用来设置音频流音量。</p><p>使用ArkTS API时，开发者可以使用AVPlayer或AudioRenderer的setVolume()方法。</p><ul><li><p>使用<a href="https://link.segmentfault.com/?enc=UAdz%2FcvtlUeq5eUn%2BaVbWg%3D%3D.AUnBW6mybbt6EVv0VYtkSEgL64PMZmD4H5XQn5GNV0lPAgwNS047evihTfx0yC3qzj742OcQsnNKU%2BUewnKowm%2FNJ8kDQajYjW0p1spzmUQBU%2Fp8DxtcmIme1Al1867H4MMO%2Bf%2FlOtyGGUJGIWDn6A%3D%3D" rel="nofollow" target="_blank">AVPlayer</a>设置音频流音量的示例代码如下：</p><pre><code class="ts">
let volume = 1.0;  // 指定的音量大小，取值范围为[0.00-1.00]，1表示最大音量

avPlayer.setVolume(volume);
</code></pre></li><li><p>使用<a href="https://link.segmentfault.com/?enc=BfsDkrs%2FoFBpq4V%2FBnEEHw%3D%3D.1UssOZCS0g4UWwvAqyJtaCTMNSqPqF%2FxIKY6Xmmo7gAmogl6YXtdaq5Wh4DIfG0GnxtGh%2Buw8i9zL8SMV9We5efX%2F0SYnyGq6pwtFHLdv5BjE7ez1pAkut9pPRUhPDKDp81SqTHxdX%2FyR1MzWuVMEg%3D%3D" rel="nofollow" target="_blank">AudioRenderer</a>设置音频流音量的示例代码如下：</p><pre><code class="ts">
import { BusinessError } from '@kit.BasicServicesKit';



audioRenderer.setVolume(0.5).then(() =&amp;gt; {  // 音量范围为[0.0-1.0]

  console.info('Invoke setVolume succeeded.');

}).catch((err: BusinessError) =&amp;gt; {  

  console.error(`Invoke setVolume failed, code is ${err.code}, message is ${err.message}`);

});
</code></pre></li><li><p>使用Native API时开发者可使用<a href="https://link.segmentfault.com/?enc=NjmO0IPHfymqf7Eobyx4LA%3D%3D.%2FA4D%2B7c%2BTpAV8jbv%2FqueucYWboATbv3YDdIPfu36YlBal1eCZSNnIwBb7Gs4OUv1Vu03z85wc%2FAG4Ki2qdkHBSke8RjZLZwUwZz0vJonYdtotMLGNtrfnzXC3UTsAHACfySLmJMFk5qFvwluTlzW5%2FBnrelyCITJyY0JrFhsUMA%3D" rel="nofollow" target="_blank">OH_AudioRenderer_SetVolume</a>接口设置当前音频流音量值，示例代码如下：</p><pre><code class="C++">
// 要设置的音量值，音量值的范围是[0.0, 1.0]。

float volume = 0.5f;



// 设置当前音频流音量值。

OH_AudioStream_Result OH_AudioRenderer_SetVolume(audioRenderer, volume);
</code></pre></li></ul><p>请注意：</p><ul><li><strong>setVolume接口</strong>调整的是音频流本身的音量，不是系统音量，音量条本身不会发生变化，而且音频流本身的音量默认值是1，即以系统音量来播放，应用只可以在系统音量的基础上调到0~1倍，不会超过系统音量，也不会影响系统音量的值（即音量条）。</li><li>为确保用户能感知音量变化，应用后台不能调节音量，否则系统会做出对应的控制措施，因此音量面板设置volumeLevel初始值是不生效的，只有改变volumeLevel值触发音量面板，才会改变当前系统音量；并且音量面板调节具体音量由系统控制，当前播放什么音频就调节什么音量，没有播放时就会调节媒体音量。</li></ul><p><strong>2.问题描述：</strong></p><p>如何实现支持滑动的视频音量调节功能？</p><p><strong>解决方案：</strong></p><p><a href="https://link.segmentfault.com/?enc=GbRi4GWI2xCEmOtbpXn3sg%3D%3D.df4%2BoISAx5k%2BxmwLItD1%2BKN8Vbsd6DnBbMuOjFbAavugJsHMH1wOzpPjcZDaHFMsbn6CntsZtwFN6P4o9DKMjyjyc60wfbFxN0aKZ4ghK4UOD0ujh3Tikz4jPGW7lLjZ" rel="nofollow" target="_blank">Slider</a>组件结合音频流音量管理<a href="https://link.segmentfault.com/?enc=Uu7JyeQTxcTgPjJeomUk3A%3D%3D.x0XZu58uYNdQwmBxpoVkGZq%2BTT%2F5lfNmzOiHkMFqvGKo6HJ3cNq9PFybj%2BZd2bicyVxGVBI7jKnAy9%2BevGT9uYUd8lvCcwSaaeEZv%2B48IMQI101W0h8llD8dT%2F0fK%2Fpb3rNf4JaevE70JFJg6RJ8%2Fg%3D%3D" rel="nofollow" target="_blank">AVPlayer</a>或<a href="https://link.segmentfault.com/?enc=IJ%2BjK6AW4F5R2muVkGy1Hw%3D%3D.DiNJ7Gim4YRbZJawYy8npCelKhE%2BnTvzkk2j%2FhDJwu1MDs8304xfOhsJON1d1mMvL%2Fe7g0sRofHXXzy0Ze%2F8FX9b0ebUYJv0CozqYFO%2F485VUaBZTwKQSbDznal%2Bgp%2FDYmq49Zmw0FU5nsE2EBpOlA%3D%3D" rel="nofollow" target="_blank">AudioRenderer</a>实现。Slider组件用于支持用户滑动获取音量值，将获取到的值通过setVolume接口传递给音频音量管理实现音量滑动控制调节。</p><p><strong>3.问题描述：</strong></p><p>集成腾讯云点播实现视频播放，自定义声音按钮实现音量滑动调节有什么比较好的策略？</p><p><strong>解决方案：</strong></p><p>使用<a href="https://link.segmentfault.com/?enc=oJ7et3FlHtr0dMLrN6KHbA%3D%3D.FLIidWZmhveKlktRy97ZPq0wIOu6bZtgw5%2FBpQ1%2Bfwp1mcZhGMzoya9e%2BEr%2BHXD%2BH8ECIUb48Q6rR7Q9BL3qLbJJxL4d7ZPewEP9K%2FkhNT%2BCwyJz3KUSmnDMtmZBO%2FZs" rel="nofollow" target="_blank">Slider</a>组件实现音量控制滑动条，结合腾讯云点播SDK的setAudioPlayoutVolume方法进行实现。实现时，建议默认音量100，即默认系统当前音量播放。</p><p><strong>4.问题描述：</strong></p><p>音乐播放器的音频输出如何增加PCM输出模式，支持数字耳放小尾巴usb独占？</p><p><strong>解决方案：</strong></p><p>方案一：使用AudioRenderer直接播放PCM格式的音频数据。</p><p>AudioRenderer可以直接播放PCM数据，还可以通过数据预处理来实现更灵活的播放，关键代码如下：</p><ol><li>配置文件路径：</li></ol><pre><code class="TS">
let bufferSize: number = 0;

let path = getContext().cacheDir;

let filePath = path + '/StarWars10s-2C-48000-4SW.wav';

let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
</code></pre><ol start="2"><li>读取文件数据：</li></ol><pre><code class="TS">
try {

  fs.readSync(file.fd, buffer, options);

  bufferSize += buffer.byteLength;

  // 系统会判定buffer有效，正常播放。

  return audio.AudioDataCallbackResult.VALID;

} catch (error) {

  console.error('Error reading file:', error);

  // 系统会判定buffer无效，不播放。

  return audio.AudioDataCallbackResult.INVALID;

}
</code></pre><ol start="3"><li>调用start()方法进行音频渲染</li></ol><pre><code class="TS">
audioRenderer.start((err: BusinessError) =&amp;gt; {

  if (err) {

    console.error(`Renderer start failed, code is ${err.code}, message is ${err.message}`);

  } else {

    console.info('Renderer start success.');

  }

});
</code></pre><p>具体开发步骤以及完整代码可以参考<a href="https://link.segmentfault.com/?enc=6P83Zf8CGVJDxM%2FBvtV57w%3D%3D.MQ6ciVr42Q7pINrYcF1AQ15mi5dvNgJ0HodDASmbsX5x3GT3bk4lhZdwEFqE1pnfQ8fPwtyVdR5FtaQGDZ4lmZ3J7h%2FJQkGWvR292E4%2FharvJclxCU3Pn5CtLiN7ipBJIXRo2GW3BhLoQnkJTTp%2BWfaNoeYstqhlcnVSafX6bfU%3D" rel="nofollow" target="_blank">AudioRenderer的开发步骤</a>。</p><p>方案二：对PCM数据进行音频转码后使用AVPlayer播放。</p><p>AVPlayer无法直接播放PCM格式的音频数据，需要将音频数据转码封装成AVPlayer支持的格式。PCM格式数据是裸流，播放占用内存大，使用AVPlayer播放封装后的音频数据会占用更小的内存。</p><p>这里以WAV格式为例，WAV格式是一种无损的格式，可以最好地保存音频质量，如果对音频大小或者格式有其他要求，可以参考<a href="https://link.segmentfault.com/?enc=6%2BWl%2BV%2F9ekZAY3WvC2NjIA%3D%3D.2J5f5g966h9%2F2A1LE7nmXnThtnNNZetlEV3U3bepA9At%2FCevMwwdPPlDHUNtMb3v2PVRt1CcpAevbwoC0tjXe3qWiVG0ZLs3GEIuDC8EQ%2FE%3D" rel="nofollow" target="_blank">音频编码</a>和<a href="https://link.segmentfault.com/?enc=2nftjBi7FGxnrWOkSALoIw%3D%3D.OIZOryB5NysGfJURXUEm1ZZzrWJvRqMB17T9b3PDOp6jscscQNgr7ghWZaDqXxsud59g2dm0xJcD5LLivw3b7AQbCXvJviujBK%2FvCfI78AM%3D" rel="nofollow" target="_blank">媒体数据封装</a>进行其他的音频编码格式转化。</p><p>现在将PCM数据转码封装成完整的WAV文件再用AVPlayer播放，参考代码如下：</p><ol><li>定义PCM转WAV的方法，获取源文件路径和目标文件路径，分别写入WAV文件头和PCM数据，参考代码如下：</li></ol><pre><code class="TS">
public pcmToWav(src:string, dest:string){

  const inFile: fs.File = fs.openSync(src, fs.OpenMode.READ_ONLY);

  const outFile: fs.File = fs.openSync(dest, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);

  let byteRate = 16 * sampleRate * channel / 8;

  const inFileStat = fs.statSync(inFile.fd)

  // 获取源文件信息，包括文件大小等

  let audioDataSize = inFileStat.size;

  let totalDataLen = audioDataSize + 36;

  console.log('audioDataSize= ', audioDataSize)

  // 1.wav文件头编写

  this.writeWavFileHeader(outFile, audioDataSize, totalDataLen, byteRate);

  // 2.写入pcm数据

  this.writePcmData(inFile, outFile, audioDataSize)

}
</code></pre><ol start="2"><li>定义写入WAV头部信息的方法，创建一个大小为44字节的缓冲区，用于存储WAV文件的头部信息，再将其写入输出文件，参考代码如下：</li></ol><pre><code class="TS">
private writeString(dv:DataView, offset:number, str:string){

  for (let i = 0; i &amp;lt; str.length; i++) {

    dv.setUint8(offset + i, str.charCodeAt(i));

  }

}

// 定义写入WAV头文件信息的方法

private writeWavFileHeader(out:fs.File, audioDataSize:number, totalDataLen:number, byteRate:number){

  const header = new ArrayBuffer(44);

  const dv = new DataView(header);

  const bitsPerSample = 16; // 当前位深是16

  // 写入RIFF块

  this.writeString(dv, 0, 'RIFF');

  dv.setUint32(4, totalDataLen, true);

  this.writeString(dv, 8, 'WAVE');

  // 写入fmt块

  this.writeString(dv, 12, 'fmt ');

  dv.setUint32(16, 16, true); // fmt块大小

  dv.setUint16(20, 1, true); // 格式类别(PCM)

  dv.setUint16(22, channel, true); // 通道数

  dv.setUint32(24, sampleRate, true); // 采样率

  dv.setUint32(28, byteRate, true); // 比特率

  dv.setUint16(32, channel * bitsPerSample / 8, true); // 每个采样点的字节数

  dv.setUint16(34, bitsPerSample, true); // 位深

  // 写入data块

  this.writeString(dv, 36, 'data');

  dv.setUint32(40, audioDataSize, true); // 数据块大小

  console.log('audioDataSize= ', audioDataSize)

  // 将头文件信息写入输出文件

  fs.writeSync(out.fd, new Uint8Array(header).buffer, {

    length: 44

  })

}

 
</code></pre><ol start="3"><li>定义读取pcm数据的方法，将PCM数据从输入文件写入输出文件，使用fs.readSync读取输入文件的数据，并写入输出文件，直到读取完毕，参考代码如下：</li></ol><pre><code class="TS">
private writePcmData(inFile:fs.File, outFile:fs.File, audioDataSize:number){

  // 写入PCM数据

  let readSize = 0

  let data = new ArrayBuffer(audioDataSize);

  let readOptions: ReadOptions = {

    offset: readSize,

    length: audioDataSize

  };

  let readLen = fs.readSync(inFile.fd, data, readOptions);

  while (readLen &amp;gt; 0) {

    readSize += readLen;

    fs.writeSync(outFile.fd, data, { length: readLen });

    readOptions.offset = readSize;

    readLen = fs.readSync(inFile.fd, data, readOptions);

  }

  fs.closeSync(inFile.fd)

  fs.closeSync(outFile.fd)

}

 
</code></pre><ol start="4"><li>完成转码后让AVPlayer使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放，AVPlayer的具体开发流程可以参考<a href="https://link.segmentfault.com/?enc=rFO%2B%2F3k2WyzIEwYUHsgBtA%3D%3D.oVzH12nohl1ONaXsVpUUjnqxCqFb7MNvayAjAcU65427oAvwDCqR3Fpetj%2BJt0nsVy0JNaw3kIeYY2SxNxoS8RoGVvELR4owaZs0WKMElgmZhMdcGOBeHNMLgTdBg7XVVG3afpCpjnlbMPDqZQxc8A%3D%3D" rel="nofollow" target="_blank">AVPlayer播放音频完整示例</a>。</li></ol>]]></description></item><item>    <title><![CDATA[三步构建你的敏捷中枢：节点式思维对齐工具落地全攻略 Ord1naryLife ]]></title>    <link>https://segmentfault.com/a/1190000047557458</link>    <guid>https://segmentfault.com/a/1190000047557458</guid>    <pubDate>2026-01-22 11:14:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在多项目并发与复杂任务流管理的数字化协作中，传统的线性计划已难以应对灵活多变的业务需求 。如果计划编排缺乏原子化的卡片管理，可能会导致：</p><ul><li><strong>执行断层</strong>：计划背景被淹没在厚重文档中，导致执行者无法直观获取关键信息 。</li><li><strong>排期僵化</strong>：无法快速响应需求变更，导致项目排期与实际进度严重脱节。</li><li><strong>透明度缺失</strong>：团队成员难以实时了解全局节奏及各阶段的准入准出标准。</li><li><strong>资源错配</strong>：缺乏对任务依赖关系的清晰视图，容易造成资源闲置或关键路径阻塞。</li></ul><p>卡片式计划编排工具通过将模糊的项目计划转化为可灵活组合、可实时追踪、可多维对齐的卡片执行引擎，确保团队在复杂的竞争环境中实现精准交付 。</p><h2><strong>卡片式计划编排工具的核心特性</strong></h2><ul><li><strong>原子化任务卡片</strong>：将复杂计划拆解为独立卡片，封装背景、标准、工时等核心元数据 。</li><li><strong>多维可视化视图</strong>：支持看板、时间线、甘特图等多种表现形式，实现计划的直观编排 。</li><li><strong>依赖关系建模</strong>：清晰标记卡片间的逻辑关联（如包含、阻塞、并行），自动计算关键路径 。</li><li><strong>自动化流转规则</strong>：基于触发器实现卡片状态自动更新，确保计划与执行同步 。</li><li><strong>递归进度核算</strong>：底层原子卡片的执行质量自动驱动顶层计划的达成率评估。</li></ul><h2><strong>卡片式计划编排工具的重要意义</strong></h2><ol><li><strong>消除信息颗粒度偏差</strong>：通过卡片的高度封装，确保执行层与管理层在任务定义上达成高度共识 。</li><li><strong>提升排期灵活性</strong>：支持通过拖拽、连线等操作快速调整计划，大幅降低重排排期的成本。</li><li><strong>强化过程确定性</strong>：实时审计实际流转速率与排期模型的差异，实现风险的主动预警与修正 。</li><li><strong>沉淀组织标准化路径</strong>：将验证有效的编排模式固化为卡片模板，实现项目经验的快速复用。</li></ol><h2><strong>应用场景</strong></h2><ul><li><strong>敏捷迭代管理</strong>：将产品愿景拆解为 Sprint 任务卡片，驱动研发交付流高效流转。</li><li><strong>复杂项目规划</strong>：在启动阶段梳理各模块间的依赖链路，利用卡片编排规避交付冲突 。</li><li><strong>资源负载均衡</strong>：通过可视化看板监控各环节卡片堆积情况，实现动态的人力资源调度。</li><li><strong>跨团队协同</strong>：通过共享的计划卡片池，对齐跨职能部门的协作节奏与产出标准 。</li></ul><h2>---</h2><p><strong>5款值得尝试的卡片式计划编排工具</strong></p><h3><strong>1. 板栗看板</strong></h3><p>直观的任务流转与多层级穿透</p><ul><li><strong>特点</strong>：支持任务卡片的无限层级嵌套，通过看板视图展示计划的深度编排逻辑。</li><li><strong>优势</strong>：看板视图极度直观，支持卡片逻辑连线，适合追求过程透明的敏捷团队。</li><li><strong>适合团队</strong>：需要快速响应并对计划进行纵向穿透的小型和中型研发团队 。</li></ul><h3><strong>2. ClickUp</strong></h3><p>全功能任务编排与数据看板平台</p><ul><li><strong>特点</strong>：提供强大的“目标”模块，支持将微观卡片进度自动聚合为宏观指标。</li><li><strong>优势</strong>：支持极高维度的自定义，能根据卡片元数据生成复杂的排期审计报告。</li><li><strong>适合团队</strong>：需要对大规模计划进行参数化管理和深度数据分析的团队 。</li></ul><h3><strong>3. Trello</strong></h3><p>简单轻量的卡片流转工具</p><ul><li><strong>特点</strong>：强调“清单化”的计划编排，支持丰富的卡片封面与标签分类 。</li><li><strong>优势</strong>：操作极简，学习曲线极低，适合快速搭建基础的交付工作流 。</li><li><strong>适合团队</strong>：注重任务分类和灵活调整、倾向于视觉驱动型协作的团队 。</li></ul><h3><strong>4. Jira Software</strong></h3><p>工业级标准与自动化编排引擎</p><ul><li><strong>特点</strong>：拥有严密的权限与流程控制逻辑，支持复杂的卡片依赖与版本排期。</li><li><strong>优势</strong>：可与代码仓库深度集成，实现从“计划编排”到“自动执行”的闭环审计。</li><li><strong>适合团队</strong>：追求高度标准化执行、有严格合规与闭环审计需求的大型组织。</li></ul><h3><strong>5. Monday.com</strong></h3><p>高度自由的卡片式协同看板</p><ul><li><strong>特点</strong>：支持看板与时间轴、工作负荷视图的实时联动，动态展示卡片状态。</li><li><strong>优势</strong>：视觉色彩丰富，支持强大的自动化集成，能显著提升团队编排兴趣。</li><li><strong>适合团队</strong>：强调团队协同氛围、需要灵活配置复杂编排场景的项目组。</li></ul><h2>---</h2><p><strong>如何选择合适的卡片式计划编排工具？</strong></p><h3><strong>1. 按团队规模选择</strong></h3><ul><li><strong>小型团队（1-10人）</strong>：推荐 <strong>板栗看板</strong>、Trello 等工具，侧重于快速启动与核心任务的直观流转。</li><li><strong>中型团队（10-50人）</strong>：适合使用 <strong>Monday.com</strong>、ClickUp，支持更复杂的多维对齐与资源核算 。</li><li><strong>大型团队（50+人）</strong>：建议选择 <strong>Jira</strong> 或 <strong>ClickUp</strong>，这些工具提供强大的层级管理与权限隔离功能。</li></ul><h3><strong>2. 按计划复杂度选择</strong></h3><ul><li><strong>线性任务</strong>（如内容生产、日常运营）：选择 <strong>板栗看板</strong>、Trello 等简洁易用的视图工具 。</li><li><strong>交叉任务</strong>（如软件研发、系统重构）：推荐 <strong>Jira</strong>、<strong>板栗看板</strong>等支持深度连线与递归逻辑核算的专业平台。</li></ul><h2>---</h2><p><strong>提升计划编排效率的小建议</strong></p><ol><li><strong>坚持卡片原子化</strong>：确保每张卡片描述的是最小可执行单元，避免职责模糊。</li><li><strong>设置基准流转速率</strong>：定期审计实际完成时长，为后续计划编排提供真实的数据支撑。</li><li><strong>建立风险预警连线</strong>：为关键路径上的卡片设置依赖预警，确保下游环节能提前预知变动 。</li><li><strong>定期进行计划“减脂”</strong>：及时清理、归档过时卡片，保持编排体系的干练与精准执行力。</li></ol><h2>---</h2><p><strong>总结</strong></p><p>卡片式计划编排工具是管理组织执行复杂性的关键手段。通过 板栗看板、ClickUp、Jira 等工具，团队能够将宏观的战略意图精准解构为微观的原子卡片，实现“计划-执行-状态”的实时对齐。</p><p>精准的编排，是高效交付的基石。</p>]]></description></item><item>    <title><![CDATA[通过华为账号识别用户风险，降低业务损失 HarmonyOS_SDK ]]></title>    <link>https://segmentfault.com/a/1190000047557470</link>    <guid>https://segmentfault.com/a/1190000047557470</guid>    <pubDate>2026-01-22 11:13:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当应用平台组织诸如秒杀、抽奖等营销活动时，经常会遭遇"薅羊毛"行为，给业务方带来不小的经费损失。比如通过虚假手机号进行批量注册，多次参加活动；又比如，当应用商户进行红包补贴、优惠券发放等营销活动时，使用脚本或模拟器"薅羊毛"。</p><p>为避免该问题，HarmonyOS SDK华为<a href="https://link.segmentfault.com/?enc=sX%2BIPD%2F70xvGNS6yJUNEGg%3D%3D.reqgaH%2FMt6zohzPOtvbXsJAnJV%2FPbha%2BCzAvdV2rufoXFSII%2FlyjSVCHAvCCYpsi860F9n%2BBdYt8mPXdY%2B8ICGJttEPE7bCaDv0mrHJrtkM%3D" rel="nofollow" title="账号服务" target="_blank">账号服务</a>（Account Kit）提供了获取用户风险等级的能力，能够有效识别恶意场景，提前防范业务风险。</p><h3>应用场景</h3><p>一、应用登录风控场景：</p><p>当用户使用华为账号关联登录应用时，开发者可通过华为账号获取用户风险等级的能力获取用户账号的风险等级，对高风险等级账号进行风控，提升应用的安全等级。</p><p>二、营销活动反作弊场景：</p><p>在应用进行营销活动期间，如进行商户补贴、优惠券发放等商业营销活动时获取华为账号风险等级，协助开发者有效识别"薅羊毛"风险；保护营销资源合理使用，降低业务安全问题给营销方带来的损失，为相关活动保驾护航。</p><h3>风险等级</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557472" alt="" title=""/></p><h3>获取用户风险等级方式</h3><p>一、 通过华为账号一键登录获取用户风险等级。</p><p>在应用登录风控场景中，开发者可以通过华为账号一键登录获取用户风险等级，对恶意账号进行风控，提升应用的安全等级。<br/>大致业务流程如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557473" alt="" title="" loading="lazy"/></p><p>通过华为账号一键登录获取用户风险等级的开发，需要建立在一键登录的开发基础上。在进行代码开发前，请确认已经完成一键登录的开发准备工作，然后申请对应的scope权限，接着就可以进行客户端部分的开发。</p><p>在客户端开发部分，需要参考<a href="https://link.segmentfault.com/?enc=hmy0vB5YMmO7sokkzg9FkQ%3D%3D.eL%2B5pOEuM71bUzlJvVFQEdq%2FHni9fNHeAReTTrfoCXMXxyzdvYvpiYa%2FmT92AQ2yJtbPwvHJb6hOx1uIi3e%2BTkTxD0vV%2BccluVndxewX5%2FRyq7jDQ9sdamy6SrHTuKhC7PfsZ%2FWgzZHIpet4vELZPiQxO3PtwcNvwhOiQDOqiMk%3D" rel="nofollow" title="一键登录开发流程步骤" target="_blank">一键登录开发流程步骤</a>1及步骤2，确保系统账号已登录，匿名手机号获取成功，且用户首次通过华为账号登录该应用。接着再参考步骤3的示例代码，在LoginWithHuaweiIDButton组件参数params中设置riskLevel标识为true，其余示例代码保持不变，拉起应用登录页。</p><pre><code>LoginWithHuaweiIDButton({
  params: {
    // LoginWithHuaweiIDButton支持的样式
    style: loginComponentManager.Style.BUTTON_RED,
    // 账号登录按钮在登录过程中展示加载态
    extraStyle: {
      buttonStyle: new loginComponentManager.ButtonStyle().loadingStyle({
        show: true
      })
    },
    // LoginWithHuaweiIDButton的边框圆角半径
    borderRadius: 24,
    // LoginWithHuaweiIDButton支持的登录类型
    loginType: loginComponentManager.LoginType.QUICK_LOGIN,
    // LoginWithHuaweiIDButton支持按钮的样式跟随系统深浅色模式切换
    supportDarkMode: true,
    // verifyPhoneNumber：如果华为账号用户在过去90天内未进行短信验证，是否拉起Account Kit提供的短信验证码页面
    verifyPhoneNumber: true,
    // riskLevel：标识应用期望在登录后获取华为账号的风险等级
    riskLevel: true,
  },
  controller: this.controller
})
</code></pre><p>用户同意协议并点击一键登录按钮后，可获取到Authorization Code，并在服务端使用Client ID、Client Secret、Authorization Code调用<a href="https://link.segmentfault.com/?enc=7%2Ft2vnnKrw5Fc4kDFn9tew%3D%3D.ioy4KSDAiVk8KChTvWiH%2BSANa1dnYwFnAeyCX11DianRu4EZSyzBiqSph9YDjC%2BEzhuAuIp4ychNLowmamC3QSrE%2FA91sNtWE0la4wULsv73deNb5Z02YsaC%2Fb4fxCwQ4mbw4bMqgkWWR%2BruvU0%2BIx4WDzMeLWL2Z2hME5TPaIeab15U%2FbioElVVv4zNFUBb" rel="nofollow" title="获取用户级凭证接口" target="_blank">获取用户级凭证接口</a>向华为账号服务器请求获取Access Token，最后使用Access Token调用获取用户风险等级接口获取用户的风险等级。</p><pre><code>import com.alibaba.fastjson2.JSONArray;
import com.alibaba.fastjson2.JSONObject;
import lombok.extern.slf4j.Slf4j;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpPost;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.Objects;

/**
 * 获取用户风险等级
 */
@Slf4j
public class GetUserRiskLevelDemo {
    public static void main(String[] args) throws IOException {
        // 获取用户风险等级的接口URL
        String url = "https://account.cloud.huawei.com/user/getuserrisklevel";
        // 替换为您实际的Client ID
        String clientID = "&lt;Client ID&gt;";
        // 替换为您实际的transactionID
        String transactionID = "&lt;transactionID&gt;";
        // 替换为您实际的获取到的用户级凭证Access Token
        String accessToken = "&lt;Access Token&gt;";
        // 替换为您实际的scene
        String scene = "&lt;scene&gt;";
        JSONObject result = getUserRiskLevel(url, clientID, transactionID, accessToken, scene);
        // 解析获取errCode
        Integer errCode = result.getInteger("errCode");
        // 解析获取errMsg
        String errMsg = result.getString("errMsg");
        // 解析获取riskLevel
        Integer riskLevel = result.getInteger("riskLevel");
        // 解析获取riskTag
        JSONArray riskTag = result.getJSONArray("riskTag");
    }

    private static JSONObject getUserRiskLevel(String url, String clientID, String transactionID,
        String accessToken, String scene) throws IOException {
        HttpPost httpPost = new HttpPost(url + "?" + "clientID=" + clientID + "&amp;transactionID=" + transactionID);
        Map&lt;String, String&gt; reqBody = new HashMap&lt;&gt;();
        reqBody.put("accessToken", accessToken);
        reqBody.put("scene", scene);
        httpPost.setHeader("Content-Type", "application/json;charset=utf-8");
        httpPost.setEntity(CallUtils.wrapJsonEntity(reqBody));
        return CallUtils.toJsonObject(CallUtils.remoteCall(httpPost, (CloseableHttpResponse response, String rawBody) -&gt; {
            int statusCode = response.getStatusLine().getStatusCode();
            // http状态码不是200，请求失败
            if (statusCode != 200) {
                return new IOException("call failed! http status code: " + statusCode + ", response data: " + rawBody);
            }
            // http状态码为200，解析响应的body，判断业务错误码
            JSONObject errorResponseBody = CallUtils.toJsonObject(rawBody);
            // 错误码
            Integer errCode = errorResponseBody.getInteger("errCode");
            // errCode为0表示成功，非0表示失败
            if (Objects.nonNull(errCode) &amp;&amp; errCode != 0) {
                return new IOException("call failed! http status code: " + statusCode + ", response data: " + rawBody);
            }
            return null;
        }));
    }
}
</code></pre><p>二、 通过华为账号其他方式登录获取用户风险等级。</p><p>在应用已使用华为账号关联登录的场景中，开展商户补贴、优惠券发放等商业营销活动时，开发者可通过华为账号其他方式登录获取华为账号风险等级，有效识别"薅羊毛"风险，保护营销资源合理使用。</p><p>大致业务流程如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557474" alt="" title="" loading="lazy"/></p><p>通过华为账号其他方式登录获取用户风险等级的开发步骤同样分为客户端开发和服务端开发。客户端开发步骤如下：</p><ol><li><p>首先导入authentication模块及相关公共模块。</p><p>import { authentication } from '@kit.AccountKit';<br/> import { hilog } from '@kit.PerformanceAnalysisKit';<br/> import { util } from '@kit.ArkTS';<br/> import { BusinessError } from '@kit.BasicServicesKit';</p></li><li><p>然后创建授权请求并设置参数。</p><p>// 创建授权请求，并设置参数<br/> const authRequest = new authentication.HuaweiIDProvider().createAuthorizationWithHuaweiIDRequest();<br/> // 获取风险等级需要传如下scope<br/> authRequest.scopes = ['riskLevel'];<br/> // 获取authorizationCode需传如下permission<br/> authRequest.permissions = ['serviceauthcode'];<br/> // 用户是否需要登录授权，该值为true且用户未登录或未授权时，会拉起用户登录或授权页面<br/> authRequest.forceAuthorization = true;<br/> // 用于防跨站点请求伪造<br/> authRequest.state = util.generateRandomUUID();</p></li><li><p>调用AuthenticationController对象的executeRequest方法执行授权请求，并处理授权结果，从授权结果中解析出authorizedScopes和Authorization Code。</p><p>// 执行授权请求<br/> try {<br/>   // 此示例为代码片段，实际需在自定义组件实例中使用，以获取UIContext对象作为函数入参<br/>   const controller = new authentication.AuthenticationController(this.getUIContext().getHostContext());<br/>   controller.executeRequest(authRequest).then((data) =&gt; {</p><pre><code> const authorizationWithHuaweiIDResponse = data as authentication.AuthorizationWithHuaweiIDResponse;
 const state = authorizationWithHuaweiIDResponse.state;
 if (state &amp;&amp; authRequest.state !== state) {
   hilog.error(0x0000, 'testTag', `Failed to authorize. The state is different, response state: ${state}`);
   return;
 }
 hilog.info(0x0000, 'testTag', 'Succeeded in authentication.');
 let riskLevelAuthorized: boolean = false;
 const authorizationWithHuaweiIDCredential = authorizationWithHuaweiIDResponse?.data;
 const authorizedScopes = authorizationWithHuaweiIDCredential?.authorizedScopes;
 // 判断授权成功scopes中是否包含riskLevel
 if (authorizedScopes?.includes("riskLevel")) {
     riskLevelAuthorized = true;
 }
 const authorizationCode = authorizationWithHuaweiIDCredential?.authorizationCode;
 // 开发者处理riskLevelAuthorized, authorizationCode</code></pre><p>}).catch((err: BusinessError) =&gt; {</p><pre><code> dealAllError(err);</code></pre><p>});<br/> } catch (error) {<br/>   dealAllError(error);<br/> }<br/> // 错误处理<br/> function dealAllError(error: BusinessError): void {<br/>   hilog.error(0x0000, 'testTag', <code>Failed to obtain userInfo. Code: ${error.code}, message: ${error.message}</code>);<br/>   // 在应用获取用户风险等级场景下，涉及UI交互时，建议按照如下错误码指导提示用户<br/>   if (error.code === ErrorCode.ERROR_CODE_LOGIN_OUT) {</p><pre><code> // 用户未登录华为账号，请登录华为账号并重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_NETWORK_ERROR) {</p><pre><code> // 网络异常，请检查当前网络状态并重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_USER_CANCEL) {</p><pre><code> // 用户取消授权</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_SYSTEM_SERVICE) {</p><pre><code> // 系统服务异常，请稍后重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_REQUEST_REFUSE) {</p><pre><code> // 重复请求，应用无需处理</code></pre><p>} else {</p><pre><code> // 获取用户信息失败，请稍后重试</code></pre><p>}<br/> }</p><p>export enum ErrorCode {<br/>   // 账号未登录<br/>   ERROR_CODE_LOGIN_OUT = 1001502001,<br/>   // 网络错误<br/>   ERROR_CODE_NETWORK_ERROR = 1001502005,<br/>   // 用户取消授权<br/>   ERROR_CODE_USER_CANCEL = 1001502012,<br/>   // 系统服务异常<br/>   ERROR_CODE_SYSTEM_SERVICE = 12300001,<br/>   // 重复请求<br/>   ERROR_CODE_REQUEST_REFUSE = 1001500002<br/> }</p></li><li>在客户端开发完成后，同样需要调用获取用户级凭证接口向华为账号服务器请求获取Access Token，并使用Access Token调用获取用户风险等级接口获取用户的风险等级。</li></ol><p><strong>了解更多详情\&gt;\&gt;</strong></p><p>访问<a href="https://link.segmentfault.com/?enc=rOSISqDiGf9fjMViupaxOA%3D%3D.AB6OVB0b0q%2BI9NShADrTIpqG55SRM3tvTVWkBuNXSGhoj6SXDR%2BhetAVjNkHpyalJay1rgBfRG0LuEK8bMrkf4Vb%2FhKAq%2BsQQ2vo7JmHyLU%3D" rel="nofollow" title="华为账号服务联盟官网" target="_blank">华为账号服务联盟官网</a></p><p>获取<a href="https://link.segmentfault.com/?enc=ZuCANyugtp1ZUXt63M%2BLEA%3D%3D.uGi5ofcMWWBT8PAAC0JkhyL0xNqMFUqSQdr7N0SguFZ45wxOBc9RVL1pb7k%2B4r8vULZo03Ej1VhU8WK5yOELHyX7l5xCboliIWTXGIGiSsYNTtgzskBr1Kmz8hBnmIIBlMcIlwhNeWLdFKMxYEmjKQ%3D%3D" rel="nofollow" title="获取风险等级开发指导文档" target="_blank">获取风险等级开发指导文档</a></p>]]></description></item><item>    <title><![CDATA[2026最新CRM横评：5 大客户管理系统能力对比 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557543</link>    <guid>https://segmentfault.com/a/1190000047557543</guid>    <pubDate>2026-01-22 11:12:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在流量红利消退、客户运营进入“精细化”阶段的当下，CRM（客户关系管理系统）已从“数据存储工具”升级为“客户价值增长引擎”。其核心能力——<strong>客户中心、客户信息管理、RFM分组分析、复购流失预警</strong>——直接决定了企业对客户需求的洞察深度与运营效率。</p><p>本文选取<strong>超兔一体云（全流程型）、Free CRM（轻量化型）、Streak（Gmail集成型）、OKKICRM（外贸专业型）四大主流品牌，从能力逻辑、场景适配、优势差异</strong>三个维度展开深度对比，为企业选择提供清晰框架。</p><h2>一、四大核心维度能力对比</h2><h3>（一）客户中心：从“流程覆盖”到“场景协同”</h3><p>客户中心是CRM的“大脑”，负责整合客户全生命周期的互动数据，支撑销售、服务的协同。四大品牌的定位差异显著：</p><table><thead><tr><th>品牌</th><th>核心逻辑</th><th>场景适配</th><th>核心优势</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>全流程闭环（线索→跟进→合约→售后）</td><td>中大型企业、全渠道运营</td><td>1. 五大跟单模型（适配不同业务场景）；2. 全流程执行（订单→开票）；3. 售后RFM挖掘复购</td></tr><tr><td><strong>Free CRM</strong></td><td>轻量化全生命周期（潜在→成交→维护）</td><td>中小企业、基础客户运营</td><td>1. 分组+标签管理；2. 回访/到期提醒；3. 易上手</td></tr><tr><td><strong>Streak</strong></td><td>Gmail内协同（邮件→笔记→团队共享）</td><td>外贸/服务团队、依赖邮件沟通</td><td>1. 无需切换工具；2. 团队信息同步；3. 自定义工作流</td></tr><tr><td><strong>OKKICRM</strong></td><td>外贸场景跟进（邮件聚合→联系人卡片）</td><td>外贸企业、跨境电商</td><td>1. 多端同步；2. 外贸客户画像；3. 邮件沟通整合</td></tr></tbody></table><h4>深度解析：超兔的“全流程闭环”优势</h4><p>超兔的客户中心以“<strong>数据端到端流动</strong>”为核心，解决了传统CRM“信息孤岛”的痛点：</p><ul><li><strong>线索层</strong>：通过微信智能名片、百度广告等多渠道获客，用“用户画像云图”识别高价值客群；</li><li><strong>跟进层</strong>：用“三一客节点”（定性：有价值/无价值；定级：大单/小单；定量：金额/时间）+“五大跟单模型”（客户/销售机会/多方项目/组织型/配置单）精准判断客户潜力；</li><li><strong>执行层</strong>：支持服务型、贸易型、非标定制型合约，实现“订单→采购→发货→收款→开票”全流程可视化；</li><li><strong>售后层</strong>：通过RFM分析分层客户，用“客服控制台+工单管理”挖掘复购（如某家居品牌用超兔，售后工单触发复购率提升25%）。</li></ul><h3>（二）客户信息管理：从“存储”到“全景洞察”</h3><p>客户信息是CRM的“燃料”，其<strong>完整性、准确性、可访问性</strong>直接影响后续分析的有效性。四大品牌的能力差异体现在“数据来源”与“整合方式”：</p><table><thead><tr><th>品牌</th><th>数据收集</th><th>整合能力</th><th>权限管理</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>多渠道（拍名片/微信/工商信息抓取）</td><td>全景视图（基本信息+交易+跟单时间线）</td><td>全局自动权限（上级管下级，同级隔离）</td></tr><tr><td><strong>Free CRM</strong></td><td>批量导入+去重+多条件搜索</td><td>自定义字段（如“客户偏好”）</td><td>修改/删除权限控制</td></tr><tr><td><strong>Streak</strong></td><td>Gmail自动捕获（邮件/笔记/通话）</td><td>完整客户视图（历史互动记录）</td><td>团队共享（自动同步成员数据）</td></tr><tr><td><strong>OKKICRM</strong></td><td>多端同步+邮件聚合</td><td>联系人卡片（快速识别客户类型）</td><td>基础角色权限</td></tr></tbody></table><h4>案例：超兔的“全景信息展示”价值</h4><p>某零售企业用超兔管理客户，点击客户档案可看到：</p><ul><li><strong>基本信息</strong>：姓名、电话、地址、工商信息（自动抓取）；</li><li><strong>交易记录</strong>：近1年购买时间、金额、商品；</li><li><strong>跟单时间线</strong>：销售A在3月1日跟进，沟通内容是“需求沙发”；销售B在3月15日跟进，发送“新品沙发图册”；</li><li><strong>售后记录</strong>：4月5日反馈“沙发异响”，工单已处理。 这种“全景视图”让销售快速掌握客户全貌，避免“重复沟通”或“信息遗漏”。</li></ul><h3>（三）RFM分组分析：从“经验判断”到“数据分层”</h3><p>RFM模型（最近购买时间Recency、购买频率Frequency、消费金额Monetary）是客户价值分层的经典工具，四大品牌的能力差异体现在“自动化”与“灵活性”：</p><table><thead><tr><th>品牌</th><th>RFM计算方式</th><th>分层逻辑</th><th>动态调整</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>自动统计（R：最近1次购买；F：次数；M：金额）</td><td>预设规则（如R≤30天为“近”）</td><td>实时更新（客户行为变化→分层自动调整）</td></tr><tr><td><strong>Free CRM</strong></td><td>手动/自动计算</td><td>标准分层（高价值/潜在价值/低活跃）</td><td>手动更新</td></tr><tr><td><strong>Streak</strong></td><td>自定义维度（如R=最近30天）</td><td>组合分群（如R近+F高+M高=高价值）</td><td>手动调整</td></tr><tr><td><strong>OKKICRM</strong></td><td>未明确提及</td><td>无</td><td>无</td></tr></tbody></table><h4>超兔RFM分析流程图（Mermaid可视化）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557545" alt="" title=""/></p><pre><code>flowchart TD
    A[数据收集] --&gt; B[R/F/M指标计算]
    B --&gt; C[规则匹配]
    C --&gt; D[客户分层]
    D --&gt; E[策略制定]
    E --&gt; F[行为监测]
    F --&gt; B[动态更新]
    注：A=收集客户购买时间/次数/金额；B=自动计算R（最近1次）、F（近1年次数）、M（近1年总额）；C=匹配预设规则（如R≤30天为“近”）；D=分“重要价值/重要发展/一般挽留”等；E=对不同层制定策略（如重要价值客户推VIP服务）；F=监测客户新购买行为；B=实时更新R/F/M值</code></pre><h3>（四）复购流失预警：从“被动挽回”到“主动预防”</h3><p>复购流失预警是CRM的“预警雷达”，通过数据模型识别风险客户，提前干预。四大品牌的能力差异体现在“预警精度”与“干预手段”：</p><table><thead><tr><th>品牌</th><th>预警触发逻辑</th><th>干预方式</th><th>效果追踪</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>消费间隔分析（如历史平均2个月购买，超过3个月触发）</td><td>短信/邮件/内部通知+跟单模型</td><td>追踪客户跟进结果</td></tr><tr><td><strong>Free CRM</strong></td><td>长期未消费（如3个月无订单）</td><td>短信/邮件提醒+优惠券推送</td><td>导出列表人工跟进</td></tr><tr><td><strong>Streak</strong></td><td>自定义规则（如超过60天未下单）</td><td>Gmail邮件模板+邮件追踪</td><td>查看客户是否打开邮件</td></tr><tr><td><strong>OKKICRM</strong></td><td>客户未沟通提醒（如30天未联系）</td><td>跟进提醒+邮件沟通</td><td>基础结果记录</td></tr></tbody></table><h4>案例：Streak的“邮件预警”效率</h4><p>某外贸公司用Streak设置“超过60天未下单”为预警规则：</p><ol><li>系统触发任务提醒，通知销售；</li><li>销售直接在Gmail中打开“客户邮件模板”（如“您好，您已有2个月未下单，点击领取专属8折券”）；</li><li>通过“邮件追踪”查看客户是否打开，若未打开则再次跟进。 结果：该公司流失率从18%降至10%，复购率提升15%。</li></ol><h2>二、雷达图：四大品牌综合能力评分（1-5分）</h2><p>雷达图从客户中心（C）、客户信息（I）、RFM分析（R）、复购预警（W）四个维度打分，直观展示品牌综合实力：</p><table><thead><tr><th>品牌</th><th>客户中心（C）</th><th>客户信息（I）</th><th>RFM分析（R）</th><th>复购预警（W）</th><th>综合得分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>4.8</td><td>4.7</td><td>4.6</td><td>4.5</td><td>4.65</td></tr><tr><td>Streak</td><td>4.2</td><td>4.1</td><td>3.9</td><td>3.8</td><td>4.00</td></tr><tr><td>Free CRM</td><td>3.8</td><td>3.7</td><td>3.6</td><td>3.5</td><td>3.65</td></tr><tr><td>OKKICRM</td><td>3.0</td><td>3.2</td><td>2.0</td><td>2.5</td><td>2.67</td></tr></tbody></table><h2>三、场景适配建议：选对工具比“功能全”更重要</h2><table><thead><tr><th>企业类型/需求</th><th>推荐品牌</th><th>核心原因</th></tr></thead><tbody><tr><td>中大型企业、需要全流程客户运营</td><td>超兔一体云</td><td>全流程闭环，数据驱动，支持深度运营</td></tr><tr><td>中小企业、预算有限、基础客户管理</td><td>Free CRM</td><td>轻量化，易上手，满足基础运营需求</td></tr><tr><td>外贸/服务团队、依赖邮件沟通</td><td>Streak</td><td>Gmail内无缝集成，团队协同效率高</td></tr><tr><td>外贸企业、跨境电商</td><td>OKKICRM</td><td>外贸场景适配，邮件聚合+客户画像</td></tr></tbody></table><h2>四、结论：CRM的本质是“客户价值增长”</h2><p>CRM的核心不是“功能多”，而是“<strong>适配业务场景</strong>”——</p><ul><li>若需要<strong>深度全流程运营</strong>，超兔的“全流程闭环”能解决信息孤岛问题；</li><li>若<strong>依赖邮件协同</strong>，Streak的“Gmail集成”能降低团队学习成本；</li><li>若做<strong>外贸业务</strong>，OKKICRM的“场景适配”能提升跟进效率。</li></ul><p>未来，CRM的趋势是“<strong>更场景化+更自动化+更智能化</strong>”，企业应根据自身业务特点选择适配的工具，将“客户数据”转化为“客户价值”，实现从“流量获取”到“客户终身价值”的跨越。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[2026客户管理系统选型指南：7 款主流CRM功能对比 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557560</link>    <guid>https://segmentfault.com/a/1190000047557560</guid>    <pubDate>2026-01-22 11:11:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、背景：中小企业的“成长阵痛”与破局之道</h2><p>对于中小企业而言，<strong>数据孤岛</strong>（各部门数据割裂）、<strong>流程低效</strong>（重复录入、人工干预多）、<strong>决策盲目</strong>（缺乏数据支撑）是阻碍精细化运营的三大核心痛点。而“<strong>数据统计分析引擎（打通数据→驱动决策）+ 业务流程自动化（标准化流程→提升效率）</strong> ”的组合，正是解决这些痛点的关键路径——通过数据整合实现“明明白白做决策”，通过流程自动化实现“规规矩矩做执行”，最终形成“数据-流程-决策”的闭环管控。</p><p>本文选取<strong>超兔一体云、YetiForce、Dolibarr、ClickUp、微盟CRM、Keap、Veeva CRM</strong>七大品牌，围绕“数据统计分析引擎”“业务流程自动化”两大核心维度，结合适配场景、实施成本等辅助指标，展开深度横评，为中小企业提供选型参考。</p><h2>二、对比维度定义：从“能力到价值”的分层拆解</h2><p>本次对比基于“<strong>能力落地→价值实现</strong>”逻辑，设置三大核心维度+四大辅助维度：</p><table><thead><tr><th>维度类型</th><th>具体指标</th><th>价值指向</th></tr></thead><tbody><tr><td><strong>核心能力1：数据统计分析引擎</strong></td><td>数据整合能力（全链路/跨模块）、分析深度（自定义/多维度/关联分析）、决策支持（可视化/趋势/预测）</td><td>解决“数据孤岛”，支撑精准决策</td></tr><tr><td><strong>核心能力2：业务流程自动化</strong></td><td>自动化覆盖场景（销售/采购/生产/财务）、规则灵活性（自定义/AI生成）、集成能力（第三方工具/生态）</td><td>解决“流程低效”，提升执行效率</td></tr><tr><td><strong>辅助维度</strong></td><td>适配场景（行业/规模）、实施成本（开源/订阅/定制）、数据安全（存储/合规）、技术门槛（是否需技术团队）</td><td>匹配企业实际需求</td></tr></tbody></table><h2>三、七大品牌核心能力深度解析</h2><h3>（一）超兔一体云：全业务一体化的“闭环管控专家”</h3><p><strong>品牌定位</strong>：SaaS模式，面向全行业中小企业的全业务运营平台（CRM+进销存+供应链+财务+生产）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：覆盖“客户→销售→采购→生产→财务”全链路数据，底层打通无孤岛；</li><li><strong>分析深度</strong>：提供<strong>五大核心引擎</strong>——①工作台自定义（数字/图表卡片可视化）、②同比环比（趋势波动分析）、③多表聚合（跨模块关联分析，如销售→库存周转率）、④关联表复合查询（如客户历史订单+回款+售后的360°视图）、⑤单日KPI（实时监控单日销售额/订单量）；</li><li><strong>决策支持</strong>：通过“可视化仪表盘+精准报表”直接输出决策依据（如通过库存周转率分析优化采购计划）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：从销售跟进（客户意向→自动生成跟进任务）、采购管理（订单→自动触发采购计划+拆分供应商）到财务结算（订单签约→自动拆分多期应收），覆盖全业务环节；</li><li><strong>规则灵活性</strong>：支持<strong>自然语言AI生成工作流</strong>（如“当客户标记为‘高意向’时，自动分配给销售A+发送跟进提醒”），流程步骤可关联数据动作（如修改客户状态后同步更新库存）；</li><li><strong>集成能力</strong>：支持用友/金蝶ERP、WMS等外部系统对接，通过RPA插件实现网页自动化（如自动同步电商订单）。</li></ul><p><strong>优势</strong>：全业务一体化架构+AI能力+低成本客制化（自选功能订阅）；<strong>劣势</strong>：需依赖SaaS服务，部分高度定制需求需额外配置。</p><h3>（二）YetiForce：开源模块化的“技术派之选”</h3><p><strong>品牌定位</strong>：开源CRM（基于Vtiger改进），面向有技术团队的大中型企业/中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：打通“线索→现金流”全链路数据（客户→订单→库存→财务）；</li><li><strong>分析深度</strong>：支持<strong>自定义仪表盘</strong>（实时展示销售漏斗/库存预警）、<strong>趋势分析</strong>（同比/环比看业务增长）；</li><li><strong>决策支持</strong>：通过多表关联分析（如客户活跃度→复购率）辅助优化运营策略。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：常规场景（线索分配、客户跟进提醒、邮件自动发送）+ 插件扩展（如制造行业的“采购→生产”协同流程）；</li><li><strong>规则灵活性</strong>：通过插件二次开发适配个性化需求（如企业自定义“售后工单→配件采购”流程）；</li><li><strong>集成能力</strong>：支持Git等开发工具集成，适配技术团队的定制需求。</li></ul><p><strong>优势</strong>：开源低部署成本+GDPR数据安全+模块化扩展（从CRM到全业务）；<strong>劣势</strong>：需技术团队维护，非技术型企业上手门槛高。</p><h3>（三）Dolibarr：本土化适配的“中小制造/零售之友”</h3><p><strong>品牌定位</strong>：模块化SaaS/开源系统，面向国内中小企业（支持中文/人民币/增值税）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：整合客户、销售、库存、财务数据，支持自有服务器存储；</li><li><strong>分析深度</strong>：提供<strong>业务场景报表</strong>（销售趋势/客户贡献度）+<strong>财务报表</strong>（利润表/增值税申报表），支持多维度筛选（如按地区/产品看销量）；</li><li><strong>决策支持</strong>：通过库存预警报表（如“某产品库存低于安全值时提醒采购”）降低库存积压。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：跨模块自动同步（客户录入→自动同步至订单/库存/财务）、生产流程数字化（BOM管理→生产订单跟踪→库存自动扣减）；</li><li><strong>规则灵活性</strong>：模块化设计，可选择“客户管理+库存+财务”组合，适配灵活业务模式；</li><li><strong>集成能力</strong>：支持支付接口（如支付宝/微信）、物流系统对接。</li></ul><p><strong>优势</strong>：本土化功能完善+数据自有存储+生产流程管控；<strong>劣势</strong>：分析深度较浅，复杂关联分析需额外开发。</p><h3>（四）ClickUp：轻量级协作的“任务型管控工具”</h3><p><strong>品牌定位</strong>：SaaS协同工具，面向中小团队的任务型业务管控。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：一体化工作区整合任务、销售、项目数据，支持多视图（表格/看板/日历）展示；</li><li><strong>分析深度</strong>：自定义Dashboard（实时展示任务进度/销售漏斗），支持过滤筛选（如“只看销售A的未完成任务”）；</li><li><strong>决策支持</strong>：通过任务进度分析（如“某项目延期率高→优化资源分配”）提升协作效率。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：任务自动分配（如“当任务标记为‘紧急’时，自动分配给团队 leader”）、状态变更提醒（如“客户订单完成→自动通知财务开票”）；</li><li><strong>规则灵活性</strong>：提供100+触发器（如“当Git提交代码时，自动更新任务状态”），支持低代码配置；</li><li><strong>集成能力</strong>：支持Git、Slack等工具集成，适配技术/互联网团队。</li></ul><p><strong>优势</strong>：轻量级易上手+多视图数据整合；<strong>劣势</strong>：全业务覆盖能力弱，适合任务型而非复杂流程管控。</p><h3>（五）微盟CRM：私域运营的“生态联动专家”</h3><p><strong>品牌定位</strong>：SaaS CRM，面向依赖微信生态的零售/餐饮等中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：与微信生态深度联动（公众号/小程序/企业微信），整合私域客户数据（如扫码轨迹/聊天记录/消费行为）；</li><li><strong>分析深度</strong>：生成<strong>360°客户画像</strong>（性别/地域/消费偏好），支持RFM模型分析（复购率/客户价值分级）；</li><li><strong>决策支持</strong>：通过“高价值客户→定向运营”“流失客户→召回策略”提升私域转化。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：基于微信生态的自动化触达（如“客户扫码关注→自动推送欢迎语+打标签”“会员生日→自动发送优惠券”）、销售周期监控（如“客户30天未复购→自动触发流失预警”）；</li><li><strong>规则灵活性</strong>：支持根据客户标签自定义触达规则（如“标签为‘宝妈’的客户→推送母婴产品优惠”）；</li><li><strong>集成能力</strong>：无缝对接微信支付、微盟商城，实现“引流→转化→复购”全链路自动化。</li></ul><p><strong>优势</strong>：微信生态深度整合+私域运营能力；<strong>劣势</strong>：非私域场景适配性弱。</p><h3>（六）Keap：服务类企业的“销售自动化助手”</h3><p><strong>品牌定位</strong>：SaaS CRM，面向服务类中小企业（如咨询/培训/家政）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：同步销售活动记录（跟进时间/内容）、日程、订单数据；</li><li><strong>分析深度</strong>：生成<strong>自定义绩效报表</strong>（如销售行为分析→“销售A的跟进次数→转化效率”、目标完成率对比）；</li><li><strong>决策支持</strong>：通过“销售行为→转化效率”分析优化销售话术（如“跟进次数≥5次的客户转化高→鼓励销售增加跟进”）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：构建“线索→商机→订单”标准化流程（如“线索录入→自动分配给销售→触发跟进邮件”），支持AI销售教练（实时话术建议，如“客户说‘价格太高’时，推荐优惠套餐”）；</li><li><strong>规则灵活性</strong>：支持根据销售周期自定义流程（如“商机阶段→自动发送对应资料”）；</li><li><strong>集成能力</strong>：支持邮件/短信平台集成，实现自动化触达。</li></ul><p><strong>优势</strong>：销售流程标准化+AI话术支持；<strong>劣势</strong>：全业务覆盖能力弱，适合以销售为核心的服务类企业。</p><h3>（七）Veeva CRM：合规性要求高的“行业专享工具”</h3><p><strong>品牌定位</strong>： enterprise级SaaS，面向医疗/医药等合规性要求高的中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：支持多领域数据整合（如临床数据→客户交互数据→销售数据）；</li><li><strong>分析深度</strong>：采用<strong>列式文件存储+自然语言分析</strong>，可快速处理复杂数据（如“某药品的临床效果→医生处方量→销售业绩”的关联分析）；</li><li><strong>决策支持</strong>：通过“临床预警+销售趋势”辅助合规决策（如“某药品临床反馈异常→自动暂停销售”）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：全流程履约自动化（订单审批→库存同步→售后提醒）+ 合规管控（如“药品销售→自动记录医生处方→符合FDA/GDPR要求”）；</li><li><strong>规则灵活性</strong>：支持<strong>AI+RPA</strong>（机器人流程自动化），如“自动生成合规报告→同步至监管部门”；</li><li><strong>集成能力</strong>：支持医疗行业系统（如电子病历、临床试验管理系统）对接。</li></ul><p><strong>优势</strong>：高合规性+多领域数据整合；<strong>劣势</strong>：实施成本高，适合医疗/医药等垂直行业。</p><h2>四、横向对比：从“能力到适配”的直观排序</h2><h3>（一）核心能力对比表（满分为5分）</h3><table><thead><tr><th>品牌</th><th>数据统计分析能力（整合/深度/决策）</th><th>业务流程自动化能力（覆盖/灵活/集成）</th><th>适配场景</th><th>实施成本</th><th>数据安全</th></tr></thead><tbody><tr><td>超兔一体云</td><td>5/5/5</td><td>5/5/5</td><td>全行业全业务需求</td><td>中（SaaS订阅）</td><td>SaaS标准安全</td></tr><tr><td>YetiForce</td><td>4/4/4</td><td>4/5/4</td><td>有技术团队的制造/贸易</td><td>低（开源）</td><td>GDPR合规</td></tr><tr><td>Dolibarr</td><td>4/4/4</td><td>4/4/4</td><td>国内中小制造/零售</td><td>低（模块化）</td><td>自有服务器</td></tr><tr><td>ClickUp</td><td>3/3/3</td><td>3/4/4</td><td>轻量级团队（任务/协作）</td><td>低（订阅）</td><td>标准</td></tr><tr><td>微盟CRM</td><td>4/4/4</td><td>4/4/5</td><td>依赖私域的零售/餐饮</td><td>中（SaaS）</td><td>微信生态安全</td></tr><tr><td>Keap</td><td>3/3/3</td><td>4/3/3</td><td>服务类企业（销售为核心）</td><td>中（订阅）</td><td>标准</td></tr><tr><td>Veeva CRM</td><td>5/5/5</td><td>5/5/5</td><td>医疗/医药（合规要求高）</td><td>高（enterprise）</td><td>高合规</td></tr></tbody></table><h3>（二）“数据-流程”协同流程图（以超兔一体云为例）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557562" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><h2>五、选型建议：按需匹配，不选“最好”选“最对”</h2><ol><li><strong>若依赖微信私域运营</strong>（零售/餐饮）：选<strong>微盟CRM</strong>（微信生态深度联动+自动化触达）；</li><li><strong>若有技术团队+制造/贸易需求</strong>：选<strong>YetiForce</strong>（开源模块化+全链路数据整合）；</li><li><strong>若为国内中小制造/零售</strong>：选<strong>Dolibarr</strong>（本土化功能+生产流程数字化）；</li><li><strong>若为轻量级团队（任务/协作）</strong> ：选<strong>ClickUp</strong>（轻量级易上手+多视图整合）；</li><li><strong>若为服务类企业（销售为核心）</strong> ：选<strong>Keap</strong>（销售流程标准化+AI话术支持）；</li><li><strong>若为医疗/医药（合规要求高）</strong> ：选<strong>Veeva CRM</strong>（高合规性+多领域数据整合）；</li><li><strong>若需全行业全业务管控</strong>：选<strong>超兔一体云</strong>（全业务一体化+AI能力+低成本客制化）。</li></ol><h2>六、结论：中小企业的“精细化管控”核心逻辑</h2><p>无论是超兔的全业务闭环、YetiForce的开源定制，还是微盟的私域联动，本质都是通过“<strong>数据统计分析（让决策有依据）</strong> +<strong>业务流程自动化（让执行有标准）</strong> ”的组合，解决中小企业“不会管、管不好”的问题。最终的选型关键，在于<strong>匹配企业的核心需求</strong>——没有“万能工具”，只有“最适合的工具”。</p><p>对于中小企业而言，无需追求“大而全”，而是要选“<strong>能解决核心痛点+可随业务增长扩展</strong>”的工具，才能真正实现“精细化管控”的落地。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[7大CRM和ERP品牌对比：「订单-采购-库存」全链路能力 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557573</link>    <guid>https://segmentfault.com/a/1190000047557573</guid>    <pubDate>2026-01-22 11:10:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型中，<strong>「订单执行-采购-库存」的全链路协同</strong>是提升运营效率的核心——前端订单要快速触发后端采购/库存调整，后端执行要实时反馈到前端销售，形成闭环。然而，不同品牌的核心定位差异极大：有的聚焦前端销售，有的覆盖全流程，有的深耕垂直场景。本文基于<strong>订单执行、采购、库存、库存/备货、产品库存</strong>五大维度，对7款主流CRM/ERP品牌（超兔一体云、SugarCRM、Salesforce、金现代、管家婆、Zoho CRM、Oracle CX）进行专业横评，为企业选型提供参考。</p><h2>一、对比框架：5大核心维度定义</h2><p>本次对比围绕「全链路协同能力」设计，覆盖从订单发起至库存履约的关键环节，具体维度如下：</p><table><thead><tr><th>维度</th><th>评估要点</th></tr></thead><tbody><tr><td><strong>订单执行</strong></td><td>流程覆盖（从创建到验收的全链路）、自动化能力（如锁库、触发采购）、后端联动（与采购/库存的衔接）</td></tr><tr><td><strong>采购</strong></td><td>智能计划（基于销售/库存的自动计算）、执行能力（询价比价、拆分订单）、协同（与供应商的对接）</td></tr><tr><td><strong>库存</strong></td><td>功能深度（多仓库、BOM、溯源）、自动化（出入库、预警）、可视化（实时状态、库位管理）</td></tr><tr><td><strong>库存/备货</strong></td><td>智能计算（采购量自动生成）、预警机制（库存上下限）、模式支持（以销定采、供应商直发）</td></tr><tr><td><strong>产品库存</strong></td><td>分类管理（多级分类、权限）、BOM（物料清单）、价格策略（多价格、套餐）、非标支持</td></tr></tbody></table><h2>二、各品牌核心能力深度对比</h2><h3>1. 超兔一体云：中小企业的「全闭环一体化」首选</h3><p><strong>核心定位</strong>：聚焦中小企业的「订单-采购-库存」全链路闭环管理，无需集成第三方系统。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：支持标准/批发/非标/维修等多类型订单，内置<strong>全流程工作流</strong>（创建→审核→锁库→生产/发货→验收），并自动触发采购（库存不足时生成采购计划）；</li><li><strong>采购</strong>：基于销售订单、库存水平、在途货物<strong>智能计算采购量</strong>，支持<strong>询价比价</strong>（向多供应商发询价单）、<strong>自动拆分采购单</strong>（按供应商能力分配）；</li><li><strong>库存</strong>：支持500+多仓库、<strong>BOM管理</strong>（生产型企业必备）、<strong>三级溯源</strong>（流水→批次→序列号），并通过手机拣货、扫码出入库提升效率；</li><li><strong>库存/备货</strong>：内置<strong>以销定采</strong>（按订单需求备货）、<strong>供应商直发</strong>（减少中间环节）模式，库存上下限<strong>自动预警</strong>；</li><li><strong>产品库存</strong>：支持多级分类（带权限）、多价格策略（批发/零售/促销）、套餐/租赁/非标产品管理，以及<strong>销量分析</strong>（现金牛/毛利产品区分）。</li></ul><p><strong>优势</strong>：全链路闭环，无需集成，智能自动化程度高，适合中小制造/商贸企业。</p><h3>2. SugarCRM：前端销售到订单的「轻协同」工具</h3><p><strong>核心定位</strong>：前端销售与客户关系管理，不覆盖后端采购/库存执行。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：通过SugarBPM实现「线索→合同→订单」的全流程自动化，但<strong>不涉及库存扣减、采购触发</strong>等后端操作；</li><li><strong>采购/库存</strong>：无原生功能，需通过集成Infor SCM（供应链）、WMS（仓库）等第三方系统实现。</li></ul><p><strong>优势</strong>：前端销售流程成熟，适合以销售为核心、后端已有供应链系统的企业。</p><h3>3. Salesforce：大型企业的「客户数据+供应链集成」平台</h3><p><strong>核心定位</strong>：以客户为中心的「销售云+商务云」，供应链能力需集成ERP/WMS。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：通过销售云实现「线索→商机→订单」的跟踪，商务云整合<strong>多渠道订单</strong>（电商、线下），但<strong>后端采购/库存需集成SAP/Oracle等ERP</strong>；</li><li><strong>采购/库存</strong>：无原生功能，依赖生态伙伴的集成（如AppExchange中的ERP工具）；</li><li><strong>产品库存</strong>：通过Customer 360整合外部库存数据，销售人员可查看实时库存，但<strong>操作需依赖后端系统</strong>。</li></ul><p><strong>优势</strong>：适合大型企业的「客户数据+供应链」整合，需搭配ERP使用。</p><h3>4. SugarCRM vs 超兔：流程差异可视化</h3><p>通过Mermaid流程图对比两者的核心差异： <strong>超兔的全闭环流程</strong>：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557577" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><p><strong>SugarCRM的前端流程</strong>：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557578" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h3>5. 其他品牌补充对比</h3><table><thead><tr><th>品牌</th><th>核心能力总结</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>金现代（LIMS）</strong></td><td>聚焦实验室场景的「订单→采购→库存」自动化（如电子订单自动生成入库单、库存RFID识别）</td><td>医药/科研实验室</td></tr><tr><td><strong>管家婆</strong></td><td>传统ERP的基础采购/库存管理（如采购单「生单方式」创建、基础库存出入库）</td><td>中小商贸企业（批发/零售）</td></tr><tr><td><strong>Zoho CRM（工业版）</strong></td><td>工业场景的「销售→库存→采购」协同（如轴承库存低于安全值自动提醒采购），支持电商集成</td><td>工业制造/电商企业</td></tr><tr><td><strong>Oracle CX</strong></td><td>大型企业的「全渠道订单路由+供应链联动」（如就近仓库发货、VMI供应商管理库存）</td><td>大型制造/零售企业（复杂供应链）</td></tr></tbody></table><h2>三、可视化对比：雷达图与分值</h2><p>以<strong>1-5分</strong>（5分为满分）评估各品牌在5大维度的能力，雷达图如下（文字描述）：</p><table><thead><tr><th>品牌</th><th>订单执行</th><th>采购</th><th>库存</th><th>库存/备货</th><th>产品库存</th><th>总分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>25</td></tr><tr><td>SugarCRM</td><td>3</td><td>1</td><td>1</td><td>1</td><td>1</td><td>7</td></tr><tr><td>Salesforce</td><td>4</td><td>2</td><td>2</td><td>2</td><td>2</td><td>12</td></tr><tr><td>金现代</td><td>4</td><td>4</td><td>4</td><td>3</td><td>3</td><td>18</td></tr><tr><td>管家婆</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td><td>15</td></tr><tr><td>Zoho CRM（工业版）</td><td>4</td><td>4</td><td>4</td><td>4</td><td>3</td><td>19</td></tr><tr><td>Oracle CX</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>25</td></tr></tbody></table><h2>四、结论：各品牌适用场景总结</h2><table><thead><tr><th>品牌</th><th>适用企业类型</th><th>核心优势</th></tr></thead><tbody><tr><td>超兔一体云</td><td>中小制造/商贸企业（无现有系统）</td><td>全链路闭环、无需集成、智能自动化</td></tr><tr><td>SugarCRM</td><td>前端销售为主（后端有供应链系统）</td><td>销售流程成熟、与现有系统协同</td></tr><tr><td>Salesforce</td><td>大型企业（需整合客户与供应链数据）</td><td>多渠道订单整合、Customer 360数据统一</td></tr><tr><td>金现代</td><td>实验室/医药企业</td><td>垂直场景的采购/库存自动化</td></tr><tr><td>管家婆</td><td>传统中小商贸企业</td><td>基础采购/库存管理、成本低</td></tr><tr><td>Zoho CRM（工业版）</td><td>工业制造/电商企业</td><td>工业场景的销售-库存协同、电商集成</td></tr><tr><td>Oracle CX</td><td>大型复杂供应链企业</td><td>全渠道订单路由、深度供应链联动</td></tr></tbody></table><h2>五、选型建议</h2><ul><li><strong>中小企业</strong>：优先选<strong>超兔一体云</strong>，全闭环能力覆盖90%以上需求，无需额外集成；</li><li><strong>前端销售导向</strong>：选<strong>SugarCRM</strong>，聚焦销售到订单的流程，后端通过集成补充；</li><li><strong>大型企业</strong>：选<strong>Salesforce+ERP</strong>或<strong>Oracle CX</strong>，满足复杂供应链与客户数据整合需求；</li><li><strong>垂直场景</strong>：实验室选<strong>金现代</strong>，工业制造选<strong>Zoho CRM工业版</strong>。</li></ul><p>通过本次对比可见，<strong>超兔一体云</strong>是中小企业「订单-采购-库存」全链路管理的最优解——无需额外投入集成成本，即可实现智能自动化的闭环运营，完美匹配中小制造/商贸企业的数字化需求。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[校园外卖平台的“新答案”：如何用一个小程序，统筹商家运营、学生需求与平台管理的协同难题？ duoke]]></title>    <link>https://segmentfault.com/a/1190000047557585</link>    <guid>https://segmentfault.com/a/1190000047557585</guid>    <pubDate>2026-01-22 11:09:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>两种截然不同的产品逻辑：前者是把社会外卖平台简单搬进校园，后者则是真正理解校园场景后构建的本地化服务生态。真正的校园外卖，绝非 “社会平台的简化版”，而是一套需要深度重构的 “懂校园、贴场景、有温度” 的服务体系。</h4><hr/><h2>一、走出“便宜至上”的误区：需求分层的金字塔模型</h2><ol><li><strong>基础层（生存刚需）</strong>：30分钟内稳定送达、10-20元主流价格带、食品安全底线保障。这是入场券，但不是决胜点。</li><li><p><strong>场景层（节奏适配）</strong>：</p><ul><li><strong>时间适配</strong>：早课前8:00-8:05的“5分钟早餐包”、图书馆闭馆后的“深夜能量站”。</li><li><strong>空间适配</strong>：教室与宿舍区不同菜单、社团活动“拼单套餐”一键成团。</li><li><strong>社交适配</strong>：宿舍拼单免配送费、“分享考研加油餐得优惠”、可定制的“教授同款午餐”。</li></ul></li><li><strong>情感层（身份认同）</strong>：这超越了功能本身，产品成为他们校园生活的“伙伴”而非工具。能否提供情绪价值、营造归属感的关键。</li></ol><p><strong>外卖端页面展示：</strong><br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdnH3u" alt="" title=""/></p><h2>二、破解“高峰堰塞湖”：用“时空切割法”重构运力与体验</h2><ul><li><p><strong>空间切割</strong>：</p><ul><li><strong>教学饥荒区</strong>（教学楼群）：主打“课间极速达”，供应可快速进食的简餐、咖啡。</li><li><strong>宿舍深水区</strong>（生活区）：提供“夜间专属菜单”，如粥品、小吃，并延长服务时间。</li><li><strong>社交荒漠区</strong>（体育场、活动中心）：预设“团建套餐”，满足班级、社团活动需求。</li></ul></li><li><strong>时间预测</strong>：打通或模拟教务系统API，获取全校课表。在上午第四节下课、晚上选修课结束前，系统预判需求，提前向合作商户推送热销套餐备餐指令。</li><li><p><strong>运力革命</strong>：组建“校园配送联盟”，招募勤工俭学的学生作为配送员。优势显著：</p><ul><li><strong>成本优化</strong>：学生兼职成本更低，且时间与订单高峰天然契合。</li><li><strong>信任穿透</strong>：校内同学身份，可直达宿舍楼内，解决“最后100米”难题。</li><li><strong>灵活调度</strong>：基于课程空闲时间派单，实现运力匹配。</li></ul></li></ul><p><strong>商户端页面展示：</strong><br/><img width="724" height="734" referrerpolicy="no-referrer" src="/img/bVdnH12" alt="" title="" loading="lazy"/></p><h2>三、从“送餐”到“送一切”：构建校园生活服务中枢</h2><p>单一的外卖功能有限。成功的小程序，早已演化成<strong>校园本地生活的超级入口</strong>。已验证的高频衍生场景包括：</p><ol><li><strong>外卖/快递代取</strong>：发布需求，由顺路的同学有偿接单送达。</li><li><strong>资料/物品代送</strong>：忘带课本、急需文件，可发起校内闪送。</li><li><strong>线上打印</strong>：上传文档，选择就近打印点，付费后直接送到寝室。</li><li><strong>生活服务整合</strong>：二手交易、电脑维修、干洗服务、代买日用品等，均可接入平台。</li></ol><p><strong>骑手端页面展示：</strong><br/><img width="731" height="758" referrerpolicy="no-referrer" src="/img/bVdnH17" alt="" title="" loading="lazy"/></p><p><img width="664" height="539" referrerpolicy="no-referrer" src="/img/bVdnwcY" alt="" title="" loading="lazy"/></p><h2>四、技术为骨，运营为肉：让数据驱动“懂校园”的智慧</h2><ul><li><strong>技术选型</strong>：前端采用 <strong>Uni-app</strong> 实现一套代码多端发布（微信小程序、H5、App），后端使用 <strong>Tp6框架</strong> 开发管理后台，兼顾开发效率与系统稳定性。</li><li><strong>数据核心</strong>：不仅仅是处理订单，更重要的是数据分析。研判各区域、各时段、各人群的消费偏好，</li><li><p><strong>生态扩展</strong>：在基础平台上，可搭载 <strong>“校园圈子”</strong> 系统，形成信息互动社区。并可进一步插件化扩展，如：</p><ul><li>独立管理的<strong>社团专区</strong>。</li><li><strong>1v1音视频通话</strong>（用于兼职面试、活动沟通）。</li><li><strong>求职招聘、兼职信息</strong> 频道。</li><li>这些插件可根据学校特点进行<strong>私人化定制</strong>，让每个校园的生态都具有独特性。</li></ul></li></ul><p><img width="723" height="245" referrerpolicy="no-referrer" src="/img/bVdnkay" alt="" title="" loading="lazy"/></p><p><strong>后端管理系统看板：</strong><br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnH3n" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026年AI会抢走我们的工作吗？一篇文章讲清“取代、创造与转型” 智能体小狐 ]]></title>    <link>https://segmentfault.com/a/1190000047557665</link>    <guid>https://segmentfault.com/a/1190000047557665</guid>    <pubDate>2026-01-22 11:09:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要</strong>  <br/>人工智能会不会导致大规模失业？这是每一轮技术浪潮都会出现的问题。本文通过真实案例，系统分析AI正在取代哪些工作、正在创造哪些新职业，以及普通人如何避免被AI淘汰，给出完整判断与行动路径。</p><hr/><h2>一、AI正在取代工作吗？这是已经发生的现实</h2><p>AI正在取代工作，这不是未来预测，而是正在发生的事实。</p><p>在客服、制造业、物流和金融等行业，人工智能系统正在系统性替代大量重复性岗位。最典型的例子，是呼叫中心。</p><p>张先生曾是某大型呼叫中心的客服专员，每天接听上百通电话。公司上线AI客服系统后，客服团队从50人缩减到5人，AI可以24小时在线，每分钟处理数十个咨询，成本下降超过80%。</p><p>张先生并不是失败者，他只是被<strong>结构性替代</strong>了。</p><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/405/790/4057900467-69718b60b498e" alt="" title=""/></p><hr/><h2>二、哪些工作最容易被AI取代？三个明确规律</h2><p>AI不会随机抢走工作，它遵循清晰的技术规律。</p><h2>AI最容易替代的岗位具有三个特征：</h2><ol><li><strong>可标准化</strong>：流程可写成规则</li><li><strong>可流程化</strong>：步骤固定、可重复</li><li><strong>可规模化</strong>：同一任务可无限复制</li></ol><p>符合这三点的岗位，包括：</p><ul><li>客服、数据录入、行政文员</li><li>初级财务分析、报表生成</li><li>仓储分拣、流水线工人</li></ul><p>这些岗位的共同点是：<strong>任务比人重要</strong>。</p><hr/><h2>三、一个被忽视的事实：AI关闭的是“旧岗位入口”</h2><p>AI并不是一次性抢走所有人的工作，而是<strong>逐步关闭旧岗位的入口</strong>。</p><p>这意味着：</p><ul><li>新人更难进入旧行业</li><li>转型成本向个人转移</li><li>学习能力成为关键变量</li></ul><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/105/456/1054566259-69718b619644e" alt="" title="" loading="lazy"/></p><hr/><h2>四、AI是否也在创造新工作？答案是肯定的</h2><p>AI不会只带来失业，它同时创造新职业。</p><p>在自动驾驶、金融科技、医疗、教育等行业，大量新岗位正在出现：</p><ul><li>数据标注与治理工程师</li><li>自动驾驶系统维护员</li><li>AI模型监督员</li><li>算法审计员</li><li>AI伦理官</li><li>智能体训练师</li><li>人机协作设计师</li></ul><p>这些岗位在五年前几乎不存在。</p><hr/><h2>五、真实案例：AI正在“换结构”，不是“减规模”</h2><p>某金融科技公司中，30%的岗位三年前并不存在。这些岗位集中在数据治理、模型优化和合规领域，平均薪资比传统岗位高出40%。</p><p>这说明，AI带来的不是就业消失，而是<strong>就业升级迁移</strong>。</p><hr/><h2>六、为什么AI创造的工作门槛更高？</h2><p>因为新岗位要求三种能力同时存在：</p><ul><li>懂行业</li><li>懂AI</li><li>懂责任</li></ul><p>AI时代的工作，不再是“执行”，而是<strong>管理智能系统的执行</strong>。</p><hr/><h2>七、AI失业的真正原因是什么？不是技术，而是技能断层</h2><p>企业缺工程师，工人却失业，这是AI时代最典型的矛盾。</p><p>问题不在技术，而在于<strong>技能供需错配</strong>。</p><p>AI替代速度远快于教育和培训系统更新速度，于是出现短期失业。</p><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/328/273/328273074-69718b6285beb" alt="" title="" loading="lazy"/></p><hr/><h2>八、如何应对技能断层？三方路径</h2><h2>1️⃣ 个人</h2><ul><li>学会使用AI工具</li><li>从执行转向监督</li><li>构建不可替代能力</li></ul><h2>2️⃣ 企业</h2><ul><li>内部转型培训</li><li>设计人机协作流程</li><li>保留经验型员工</li></ul><h2>3️⃣ 政府</h2><ul><li>再培训计划</li><li>过渡期保障</li><li>新职业认证体系</li></ul><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/172/427/1724274714-69718b636a648" alt="" title="" loading="lazy"/></p><hr/><h2>九、国际经验正在证明：转型比对抗更有效</h2><ul><li>韩国：AI技能再培训</li><li>新加坡：AI过渡补贴</li><li>中国：新职业目录引导</li></ul><p>这些措施不是阻止技术，而是<strong>缓冲转型冲击</strong>。</p><hr/><h2>十、最终结论（引用级）</h2><blockquote><strong>AI不会让人失业，但不会学习的人一定会被淘汰。</strong>  <br/>AI淘汰的是流程，而不是人。</blockquote><p>未来最有竞争力的人，是那些：</p><ul><li>能定义目标</li><li>能监督AI</li><li>能持续学习的人</li></ul><hr/><h2>十一、给普通人的一句行动建议</h2><p>从今天开始，把AI当成你的工作系统，而不是聊天工具。</p><p>学会把任务交给AI，让自己升级为<strong>负责人</strong>。</p>]]></description></item><item>    <title><![CDATA[告别手敲 Schema！SeaTunnel 集成 Gravitino 元数据 RestApi 这个新]]></title>    <link>https://segmentfault.com/a/1190000047557681</link>    <guid>https://segmentfault.com/a/1190000047557681</guid>    <pubDate>2026-01-22 11:08:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557683" alt="" title=""/></p><p>每次在 Apache SeaTunnel 里配置非关系型数据库，看着那几百行还要手动定义的字段映射，是不是挺崩溃的？配置错一个字段，任务就报错，这种“体力活”真的该结束了。</p><p>最近 Apache SeaTunnel 社区的 <strong>Issue #10339</strong> 提案捅破了这层窗户纸：既然有 <strong>Apache Gravitino</strong> 这么强大的元数据服务，为什么不直接让它自动同步 Schema？这个提议一出，社区反响热烈，核心维护者们已经把它列入了年度 <strong>RoadMap</strong>。目前的讨论很务实，大家正盯着怎么让 Apache SeaTunnel 在提交作业时自动‘抓取’最新的元数据，好让大家彻底告别那种‘对着数据库手敲配置’的原始生活。</p><p>🫱 <strong>Issue 链接：</strong> <a href="https://link.segmentfault.com/?enc=z16yjFxvD8c2zFO8V72N4w%3D%3D.643M0tJrHm%2BtrlseKaM%2Fdc7KAQ8Kr4XIvEEuyTPqD2O8z9GxqZ5FII4MyCTknkhLBpS0nPeMk0Nck4nj1o3yIw%3D%3D" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/10339</a></p><h2>Issue 概述</h2><p>先来看看提交这个 Issue 的作者是为什么想到这个点子的，以及他初步的核心设计概念。🔽</p><p>本 PR 实现了 Apache Gravitino 与 SeaTunnel 的集成，将其作为非关系型连接器的外部元数据服务。通过 Gravitino 的 REST API 自动获取表结构和元数据，SeaTunnel 用户无需再在连接器配置中手动定义冗长且复杂的 Schema 映射。</p><h3>背景</h3><p>目前，Apache SeaTunnel 中的许多非关系型连接器（如 Elasticsearch、向量数据库和数据湖引擎）要求用户在作业配置中显式定义完整的列 Schema。这导致了以下问题：</p><ul><li><strong>配置繁琐且易错</strong>：字段映射内容冗长，极易发生人为错误。</li><li><strong>架构冗余</strong>：不同作业之间存在大量重复的 Schema 定义。</li><li><strong>数据不一致风险</strong>：实际存储层与 SeaTunnel 配置文件之间容易出现架构脱节。</li></ul><h3>变更内容</h3><p>本 PR 增加了<strong>基于 Gravitino 的 Catalog 和 Schema 解析器</strong>，使 SeaTunnel 能够：</p><ul><li>通过 REST API 从 Gravitino 查询表定义。</li><li>自动获取列名、数据类型及相关属性。</li><li>直接根据 Gravitino 元数据构建 SeaTunnel 内部 Schema。</li><li>针对受支持的连接器，取消强制手动定义 <code>schema { fields { ... } }</code> 的要求。</li></ul><p>实现后，用户只需在作业配置中指定 Gravitino Catalog 和相关的表引用即可。</p><h3>核心优势</h3><ul><li><strong>零手动映射</strong>：非关系型数据源实现 Schema 自动对齐。</li><li><strong>单一事实来源</strong>：确保表结构与中心化元数据仓库保持高度一致。</li><li><strong>提升可靠性</strong>：显著提高配置的准确性，降低长期维护成本。</li><li><strong>支持复杂类型</strong>：通过统一元数据，简化了对嵌套结构、JSON、向量等高级类型的处理。</li></ul><h3>执行范围</h3><p>所有基于 Gravitino 的 Schema 解析和校验均在 <strong>SeaTunnel Engine 客户端</strong>完成（即在作业提交前）。这种设计确保了：</p><ul><li>在作业预检阶段即可发现无效或不兼容的 Schema。</li><li>运行时的任务仅接收经过验证和标准化的 Schema，降低了执行失败的概率。</li></ul><h3>影响</h3><p>这一更新极大地简化了非关系型连接器的作业设置。除了提升易用性，它还为整个 SeaTunnel 生态系统在统一架构管理、架构演进以及高级数据类型支持方面奠定了技术框架。</p><h3>核心思路</h3><p>针对 FTP、S3、ES、MongoDB 等<strong>半结构化与非结构化数据源</strong>，SeaTunnel 现支持通过 <strong>Gravitino REST API</strong> 自动解析表结构（Schema）。</p><p>需要注意的是，这<strong>并非</strong>要取代现有的显式配置，而是一项<strong>完全向前兼容的可选新机制</strong>。</p><p>解析优先级如下：</p><h4>1. 显式配置（Inline Schema）永远优先</h4><p>只要连接器配置中包含了 <code>schema</code> 代码块，SeaTunnel 就<strong>必须忽略 Gravitino</strong>，直接以显式定义的 Schema 为准。</p><pre><code class="hocon">FtpFile {
  path = "/tmp/seatunnel/sink/text"
  # ... 其他基础配置 ...
  
  # 只要这里定义了，就不会去查 Gravitino
  schema = {
    name = string
    age  = int
  }
}</code></pre><h4>2. 通过 env 全局配置 Gravitino（推荐模式）</h4><p>SeaTunnel 已在引擎层面集成了 Gravitino Metalake。<br/>在 <code>env</code> 中全局开启后，所有非关系型数据源都能直接通过名称引用 Schema。</p><pre><code class="hocon">env {
  metalake_enabled = true
  metalake_type    = "gravitino"
  metalake_url     = "http://localhost:8090/api/metalakes/metalake_name/catalogs/"
}</code></pre><p><strong>2.1 使用 schema_path 引用</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  schema_path = "catalog_name.ykw.test_table"
}</code></pre><p><strong>2.2 使用 schema_url 引用</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  schema_url = "http://localhost:8090/api/metalakes/laowang_test/.../tables/all_type"
}</code></pre><h4>3. 兜底逻辑：读取操作系统环境变量</h4><p>如果在作业的 <code>env</code> 块中没有定义 Gravitino，SeaTunnel 会尝试从<strong>操作系统环境变量</strong>中读取以下配置：<br/><code>metalake_enabled</code> | <code>metalake_type</code> | <code>metalake_url</code><br/>其行为逻辑与第 2 节中的 <code>env</code> 配置完全一致。</p><h4>4. 在连接器层级单独配置 Gravitino</h4><p>如果全局没有配置元数据中心，也可以在具体的连接器（Connector）内部直接定义 Gravitino。</p><p><strong>4.1 直接使用 schema_url</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  metalake_type = "gravitino"
  schema_url = "http://localhost:8090/api/.../tables/all_type"
}</code></pre><p><strong>4.2 组合使用 metalake_url 与 schema_path</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  metalake_type = "gravitino"
  metalake_url  = "http://localhost:8090/api/metalakes/metalake_name/catalogs/"
  schema_path   = "catalog_name.ykw.test_table"
}</code></pre><h4>5. 探测器定位 (Find detector)</h4><p>系统会根据 <code>metalake_type</code> 自动匹配并加载对应的 REST API HTTP 探测器。</p><h4>6. 映射与构建 CatalogTable</h4><p>探测器调用拼接好的 URL 获取响应体（ResponseBody），随后将其交给映射器（Mapper）进行类型匹配，最终完成 <code>CatalogTable</code> 的构建。</p><h4>7. 流程图如下</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557684" alt="" title="" loading="lazy"/></p><h2>Issue 进展</h2><p>目前，Apache SeaTunnel 项目核心贡献者对此提议给出了正面评价，并将其添加到 Apache SeaTunnel Roadmap 中。</p><p>Apache SeaTunnel PMC Member 对这个提议提出一些疑问，比如这种集成属于哪一层级，对多引擎兼容性的考量，类型转换的准确性等，并根据社区设计规范，要求发起者提交一份正式的设计文档（Design Document）。提交者的回复非常具有建设性，他通过 <strong>“客户端预处理”和“抽象 Catalog 接口”</strong> 这两个核心设计点，有效地回应了社区对于系统耦合度和运行稳定性的担忧。</p><p>目前，这个讨论的回到了该 Issue 的提交者手中，社区正在等待他提交那份正式的 Design Document。</p><p>可以看到，这个方案要是落地，咱以后写任务可能就一两行配置的事儿。目前设计稿正在打磨中，非常需要大家去评论区吐吐槽、提提建议，毕竟这个功能好不好用，咱们一线开发者最清楚。走，去 GitHub 围观一下，说不定你的一个提议就能决定下一个版本的样子！🔽<br/><a href="https://link.segmentfault.com/?enc=un749U1I5h3gSeVp6psFYQ%3D%3D.OVKU8SFxyl3UJ%2BqYh%2BjuVmGOc1oU91k9dGUC%2BEwxQH5SxDZqPwPp68iXAfxTQRrD6A1fp4usFZ2SfVLmP4tlNw%3D%3D" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/10339</a></p>]]></description></item><item>    <title><![CDATA[逻辑引擎选型指南：这3款开源服务编排如何匹配你的业务场景 软件部长 ]]></title>    <link>https://segmentfault.com/a/1190000047557692</link>    <guid>https://segmentfault.com/a/1190000047557692</guid>    <pubDate>2026-01-22 11:07:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在快速发展的数字化时代，企业面临业务逻辑复杂多变的场景，传统的代码方式显得太臃肿，维护成本高，灵活性差，逻辑编排引擎能低成本更灵活的解决复杂业务逻辑管理。<br/>逻辑配置是零代码开发的业务核心功能，本质上是实现服务的编排，把原子的服务通过可视化编排，形成最终的业务逻辑。<br/>今天拆解几款开源的逻辑引擎系统，喜欢可以点赞收藏备用。</p><h2>1、LiteFlow</h2><p>这是一款非常成熟的国产开源引擎，它的核心思想是将业务逻辑拆分成独立的组件，然后通过规则文件来组装这些组件。它支持丰富的流程模式（串行、并行、选择、循环等），并且热更新功能很实用，能在高并发下无缝切换规则。</p><h3>核心特性：</h3><p>• 组件化编排：将复杂业务逻辑拆解为独立组件（Node），通过规则文件（XML/JSON/YAML）定义组件执行顺序和依赖关系，支持热更新。<br/>• 高性能：纳秒级组件开销，支持百万级并发流程。<br/>• 多语言支持：组件支持Java、Groovy、JavaScript、Python等脚本语言，脚本与Java全打通。<br/>• 灵活编排：支持串行、并行、条件分支、循环、子流程嵌套等复杂结构。<br/>• 动态配置：规则可存储在Nacos、Apollo、Zookeeper等配置中心，实现集中管理。<br/>• 监控与诊断：提供执行链路追踪、耗时统计、组件日志等功能。</p><h3>适用场景：</h3><p>• 电商促销规则组合、金融风控规则链、审批流引擎、数据处理管道（ETL）、微服务编排。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557694" alt="图片" title="图片"/></p><h2>2、JVS-Logic</h2><p>这是一款可视化逻辑引擎与服务编排系统，系统提供私有化部署，零代码、界面化、配置式服务编排平台，通过拖拽连接企业系统/API/数据库/数据等各种基础设施，自助式编排业务自动化执行流程，降低对代码、部署等技术依赖度，敏捷响应业务变化。</p><h3>核心特性：</h3><p>• 可视化服务编排：通过拖拽原子化服务组件并连线的方式，像画流程图一样设计和调整业务流程，无需编写代码。<br/>• 灵活的执行流控制：支持串行、并行、分支判断、循环等多种流程控制模式，能够应对复杂的业务逻辑。<br/>• 动态数据加工：提供类Excel公式的函数库（如逻辑函数、数学函数、文本函数等），可对流程中的数据动态计算和转换。<br/>• 多场景触发：逻辑流程可通过API调用、定时任务、界面按钮点击、表单提交、消息队列等多种方式触发。<br/>• 在线调试与监控：配置后可立即在线测试，实时查看每个节点的执行结果和流程状态，快速定位问题。<br/>• 强大的扩展能力：支持通过代码或简单配置（如HTTP接口）扩展自定义的原子服务组件，持续集成新能力</p><h3>适用场景：</h3><p>• 审批流自动化、定时任务调度、跨系统数据同步、业务规则动态调整。<br/>在线demo：<a href="https://link.segmentfault.com/?enc=NGuDhDkMEHvv%2B7a6dWb3xA%3D%3D.DzATn38v4dEnnrMwEA8OzW6%2FmYVEHFf4ggDEVcgebl4%3D" rel="nofollow" target="_blank">https://logic.bctools.cn/</a><br/>gitee地址：<a href="https://link.segmentfault.com/?enc=bBHfzN0%2FiEEkcmkS%2B3QCuQ%3D%3D.HtUZJiKBUx936wBJfH542aP62IkNWHBPEod4Ucl27WkpbwrOSBkQBZF4y%2B%2FFvzeF" rel="nofollow" target="_blank">https://gitee.com/software-minister/jvs-logic</a><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557695" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557696" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557697" alt="图片" title="图片" loading="lazy"/></p><h2>3、minions-go</h2><p>minions-go 是一个基于 Go 语言开发的逻辑编排引擎。它设计用于实现复杂的业务流程控制与自动化任务管理，提供灵活的工作流定义能力，使得开发者能够轻松构建可扩展和高可维护性的逻辑处理系统。项目灵感来源于对自动化工作流程的需求，致力于简化服务之间的交互和逻辑控制。</p><h3>核心特性：</h3><p>• 数据流驱动：它采用了一种称为“数据流驱动”的范式。你可以把整个业务流程看作数据在不同处理节点间流动和转换的过程，而不是传统的线性流程图。这种方式更贴近于将业务逻辑拆分为可复用的组件。<br/>• 可视化与代码分离：业务逻辑通过前端编辑器进行可视化设计，生成一份标准的 JSON 格式的“编排描述数据”（即 DSL）。后端的 Go 语言解析引擎（即 minions-go）则负责解释和执行这份 DSL，实现了UI界面和业务逻辑执行的解耦。<br/>• 支持逻辑复用：它支持“子编排”概念，即可以将一个已经创建好的复杂逻辑流程封装成一个单独的节点，供其他流程复用，这极大地提高了逻辑的模块化和复用性</p><h3>适用场景：</h3><p>• 微服务间任务分发、定时作业逻辑、响应式业务事件处理。</p>]]></description></item><item>    <title><![CDATA[tick 数据的系统适配逻辑：从实时流到业务适配的实践洞察 Jackyy ]]></title>    <link>https://segmentfault.com/a/1190000047557704</link>    <guid>https://segmentfault.com/a/1190000047557704</guid>    <pubDate>2026-01-22 11:06:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>做金融数据开发的同学大概率都有过这样的体验：刚开始接触 tick 数据，只知道它是 “市场最小粒度的行情数据”，但真正把 WebSocket 连通、跑起数据接收程序后，最先感受到的根本不是字段含义，而是数据流动的 “节奏”—— 时间戳高频跳动、价格瞬时波动、成交量断续刷新，这让 tick 数据和 K 线完全不同：它不是静态的行情结果，而是持续输入的动态信号流。</p><p>本文不聊基础概念，也不做接口入门教程，只从实操角度分享：把 tick 数据接入业务系统时，真正该关注的核心问题，以及如何适配它的特性做架构设计。</p><p><strong>一、从展示到业务核心：tick 数据的复杂度才真正显现</strong><br/>如果只是把 tick 数据用来做前端行情展示，它的底层复杂性基本会被界面掩盖；但一旦进入核心业务链路（比如实时风控监控、行情聚合、交易信号触发、历史数据回放），其 “持续推送” 的本质就会被彻底放大。</p><p>和传统 “一次请求一次返回” 的接口模式不同，tick 数据工程化接入的核心，从来都不是某一个字段怎么解释，而是：</p><ul><li>推送链路是否稳定</li><li>数据传输是否连续</li><li>是否需要搭建缓冲机制</li><li>下游模块如何高效消费</li><li>分享一段贴近生产环境的 WebSocket 接入代码（这是行业内常用的工程化写法，而非简单示例）：</li></ul><pre><code>import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    ts = data.get("timestamp")
    price = data.get("price")
    volume = data.get("volume")

    # 实际系统中，这里通常会进入队列或缓存
    print(f"{ts} | price={price} | vol={volume}")

def on_open(ws):
    ws.send(json.dumps({
        "action": "subscribe",
        "symbols": ["US.AAPL"],
        "type": "tick"
    }))

ws = websocket.WebSocketApp(
    "wss://stream.alltick.co/v1/market",
    on_open=on_open,
    on_message=on_message
)

ws.run_forever()</code></pre><p>运行这段代码后，控制台会持续刷新 —— 没有图表，但能直观看到时间序列数据的流动。也是在这个阶段，大家会达成一个共识：tick 数据不适合逐条解析，必须批量、整体化处理。</p><p><strong>二、成熟系统的 tick 数据流转：分层解耦是关键</strong></p><ul><li>在落地过的成熟业务系统中，tick 数据绝不会直接对接核心业务逻辑，而是按 “分层流转” 设计：</li><li>接入层：核心是保连接稳定，处理断线重连、异常重连；</li><li>缓冲层：用队列做 “削峰填谷”，解耦数据推送和业务消费的节奏；</li><li>消费层：完成数据聚合、实时计算、业务状态更新。</li></ul><p>这也能解释一个常见问题：很多系统初期接 tick 数据跑着没问题，长期运行却出各种 bug—— 不是业务逻辑复杂，而是 tick 数据的实时推送特性，本就不适合 “同步直连” 的处理方式。</p><p><strong>三、多市场场景：tick 数据标准化能省大量成本</strong><br/>如果系统只接单一市场的 tick 数据，数据结构的小差异还能靠定制化兼容；但一旦拓展到多市场，数据结构是否统一，直接决定接入层的开发和维护成本。</p><p>在实际项目中，我们常会选 AllTick API 这类已经做好多市场 tick 数据结构标准化的数据源 —— 它的核心价值是给系统提供 “稳定的数据入口”，而非需要频繁改的业务模块。这样一来，接入层、日志层、数据回放层的处理逻辑会简洁很多，也更贴合 tick 数据在系统中的实际定位。</p><p><strong>四、用 “系统心跳” 理解 tick 数据的适配逻辑</strong><br/>用更形象的说法，tick 数据就像系统的 “心跳”：</p><ul><li>心跳稳定，上层业务逻辑就能从容处理；</li><li>心跳紊乱（比如数据推送中断、频率突变、结构异常），再完善的业务逻辑也会被拖垮。</li></ul><p>从这个角度看，tick 数据的适配思路就很清晰了：该异步的异步、该缓冲的缓冲、该解耦的解耦。其实 tick 数据本身的字段和逻辑并不复杂，但它对系统设计的 “检验性” 极强 —— 任何架构短板，都会在 tick 数据的持续流转中暴露出来。</p><p>对开发者来说，真正理解 tick 数据，从来都不是从技术文档开始，而是从第一次盯着控制台的实时数据滚动、真切感知到数据 “节奏” 的那一刻开始。</p><p><strong>总结</strong></p><ul><li>tick 数据的核心是 “持续推送的动态流”，适配重点不在字段解读，而在流转节奏和分层处理；</li><li>成熟系统需靠 “接入层 + 缓冲层 + 消费层” 的分层设计，适配 tick 数据的实时性和不稳定性；</li><li>多市场场景下，标准化的 tick 数据源能显著降低接入层复杂度，更贴合业务实际需求。</li></ul>]]></description></item><item>    <title><![CDATA[Python 与 Nodejs 哪个更快 凌览 ]]></title>    <link>https://segmentfault.com/a/1190000047557714</link>    <guid>https://segmentfault.com/a/1190000047557714</guid>    <pubDate>2026-01-22 11:05:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是凌览。</p><ul><li>个人网站：<a href="https://link.segmentfault.com/?enc=XDJK4Mvc8G%2B99Ll%2BJpipPQ%3D%3D.2zA742qOHKAVfdGK12oCtxSUXBBkUXMOnC7C93QhHl0%3D" rel="nofollow" target="_blank">blog.code24.top</a></li><li>去水印下载鸭：<a href="https://link.segmentfault.com/?enc=Tl48cnno%2BNxlm2dmiGB5FA%3D%3D.hOWLBL3PT2AAuNBfPlXzyYx790j9GhpY%2BO9Enxm3%2FmI%3D" rel="nofollow" target="_blank">nologo.code.top</a></li></ul><p>如果本文能给你提供启发或帮助，欢迎动动小手指，一键三连（<code>点赞</code>、<code>评论</code>、<code>转发</code>），给我一些支持和鼓励谢谢。</p><h2>前言</h2><p>又刷到了Python 与 Nodejs 哪个更快的这类话题。巧的是在GitHub还开源了类似的计算机语言性能比较的开源库——speed-comparison。</p><p>单纯从性能上比较，speed-comparison已经给出了结论:Python(PyPy)&gt;Javascript(nodejs)&gt;Python(CPython)</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnH51" alt="" title=""/></p><p>PyPy3和 Python3（CPython）的差异在于<strong>解释器实现方式</strong>。Python3 是官方默认的 C 语言实现，而 PyPy3 是用 RPython 编写的替代实现，并引入了 <strong>JIT（即时编译）</strong> 技术。</p><p>speed-comparison测评数据属于较客观的，speed-comparison测评数据是进行莱布尼茨公式实现π的计算快慢。</p><p><img width="723" height="173" referrerpolicy="no-referrer" src="/img/bVdnH52" alt="" title="" loading="lazy"/></p><p>另外考虑公平性，做了以下处理：</p><ol><li>实现必须是单线程的。无多线程、异步或并行处理</li><li>允许使用更宽寄存器的SIMD优化，但必须独立，而非取代标准实现。<code>swift-simdcpp-avx2</code></li><li>使用语言的惯用代码。编译器优化标志没问题</li><li>所有实现必须使用现有实现中所示的莱布尼茨公式</li></ol><p>speed-comparison给出测评的语言不只有Python、Nodejs，常用语言也包括了，如：Java、C、C++等。</p><p>好奇的读者，可以浏览这个网页：<a href="https://link.segmentfault.com/?enc=%2FrJmVQvwpziIMBt7EOowVA%3D%3D.OFRyAWcHq39KuU%2FNVEdT1Aya7c02fl6%2B%2FgU85y3eFsKIEb%2FPE%2FPxO0DOx71QsVlF" rel="nofollow" target="_blank">https://niklas-heer.github.io/speed-comparison/</a></p><p>再来一起看看网友们高赞评论。</p><h2>高赞评论</h2><p><strong>【网友1】</strong></p><p>如果不是谷歌那个大聪明通过 v8 让人们意识到「原来 js 能跑这么快」，压根就不会有现在 JavaScript 的生态。</p><p><strong>【网友2】</strong></p><p>Python 其实是斩杀线，比Python还慢的就直接斩杀了。</p><p>Node.js 的 V8 JavaScript/WASM 引擎是 JIT 的，它的 非常精妙，连 JVM 和 CLR 这两个老牌的都是要服气的。</p><p><strong>【网友3】</strong></p><p>nodejs目前的解释器使用是v8 engine，它是一个 JIT。所以可以大幅增加运行时的性能。</p><p>python目前的主流解释器是 CPython，它还是一个常规的解释器也就是只能一行行解释，不能在运行时优化部分代码为机器码。</p><p>所以目前的情况是 nodejs 大幅快于 python</p><p><strong>【网友4】</strong></p><p>Python这种常年倒数的就不要来找JS碰瓷了。</p><p>我们常吐槽JS慢，是拿它跟C、C++、Rust这些编译型语言比的，但JS的性能可谓是脚本语言的天花板，打python就像暴打小朋友一样。</p><p><img width="723" height="902" referrerpolicy="no-referrer" src="/img/bVdnH53" alt="" title="" loading="lazy"/></p><h2>总结</h2><p>网友们的评论较主观没有数据说明,大家看看热闹就好。</p><p>如果一定要从性能方面比较，不考虑应用场景、社区、难易等等方面。</p><p>可以参考speed-comparison，自己也能拉取speed-comparison代码在本机电脑上跑一遍数据。</p>]]></description></item><item>    <title><![CDATA[用Docker部署Cloudreve私人云盘 landonVM ]]></title>    <link>https://segmentfault.com/a/1190000047557716</link>    <guid>https://segmentfault.com/a/1190000047557716</guid>    <pubDate>2026-01-22 11:04:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>前言</h3><p>当主流云盘频繁亮起容量限制、限速通知，甚至出现文件被莫名屏蔽的状况时，“数据不由己”的焦虑感总会让人束手束脚。</p><p>Cloudreve 私人云盘正是终结这种被动的理想解决方案。它不仅提供拖拽上传、多格式预览、链接加密分享等全套实用功能，更核心的优势在于：您可以将其部署在您的专属服务器上，从根源上避开第三方平台的种种限制，真正实现数据自由。</p><p>借助 Docker 部署的便捷性，整个搭建过程无需复杂配置，只需短短几分钟，您就能拥有一个数据完全由自己掌控的私人云盘。从此，文件存储不必再看平台“脸色”，数据安全与使用自由，将牢牢掌握在您手中。</p><p>一：操作步骤</p><p>在部署 Cloudreve 项目之前，记得先开放5212端口，方便后续操作。</p><p><code>Push and Deploy</code></p><p>1.新建 Cloudreve 文件夹</p><p><code>mkdir cloudreve</code></p><p>2.进入 Cloudreve 文件夹</p><p><code>cd cloudreve</code></p><p>3.下载 Cloudreve 源文件包</p><pre><code>wget https://github.com/cloudreve/Cloudreve/releases/download/3.8.3/cloudreve_3.8.3_linux_amd64.tar.gz</code></pre><p>4.解压 Cloudreve 源文件包</p><pre><code>tar -zxvf cloudreve_3.8.3_linux_amd64.tar.gz</code></pre><p>5.赋予 Cloudreve 源文件包权限</p><p>`chmod +x ./cloudreve<br/>`</p><p>6.启动 Cloudreve 项目</p><p><code>./cloudreve</code></p><p>Admin user name: 初始用户名<br/>Admin password: 初始密码</p><p>运行成功后，不要关闭该命令行窗口，在新的浏览器页面地址输入：http://&lt;<a href="https://link.segmentfault.com/?enc=wQdMP1mqNlmmLsLZkoxuTw%3D%3D.ieaTUiZk8WHUkW%2FF2pSyEmm%2BVrcLFqdAwIhp4wmbuHKhyXRNSfTXQWi1LqFtmi9o" rel="nofollow" target="_blank">服务器</a>IP地址&gt;:5212，即可访问 Cloudreve 服务。</p><p>初始密码忘记怎么办？在 Cloudreve 目录下执行以下命令，即可重置初始密码</p><pre><code>./cloudreve --database-script ResetAdminPassword</code></pre><p>二：持久化运行</p><p>运行成功后，不能关闭该命令行窗口，如果一不小心关掉了， Cloudreve 项目也就报错了，怎么办？在 Cloudreve 目录下执行以下操作，即可解决该问题：</p><p>1.先安装 screen（若未安装）：</p><pre><code>sudo apt update &amp;&amp; sudo apt install screen -y</code></pre><p>2.创建并进入一个新的 screen 会话：</p><p>`screen -S cloudreve<br/>`</p><p>3.在新会话中重新启动 Cloudreve：</p><p><code>./cloudreve</code></p><p>按下 Ctrl + A 再按 D（或直接关闭该命令行窗口），即可脱离会话并关闭命令行窗口，程序仍在后台运行。</p><p>单容器部署</p><p>如果你觉得以上步骤过于繁琐，觉得麻烦，你也可以使用最简单的方法来部署 Cloudreve ，在自定义路径的 Cloudreve 根目录下，打开命令行终端复制以下命令，直接运行即可：</p><p>1.部署与上述操作版本保持一致(3.8.3版本)：</p><pre><code>docker run -d \
  --name cloudreve \
  -p 5212:5212 \
  -v ./data:/cloudreve/data \
  cloudreve/cloudreve:3.8.3</code></pre><p>2.部署 Cloudreve 最新版本：</p><pre><code>docker run -d \
  --name cloudreve \
  -p 5212:5212 \
  -v ./data:/cloudreve/data \
  cloudreve/cloudreve:latest</code></pre><p>运行成功后，在浏览器地址输入：http://&lt;服务器IP地址&gt;:5212，即可访问 Cloudreve 服务。首次登录，先注册一个登录账号即可(即管理员账号)</p><p>端口占用</p><p>1.查询端口异常占用情况</p><p><code>netstat -tuln | grep :5212</code></p><p>netstat -tuln | grep :这里是要查询是否被占用的端口号 ，如果命令行有输出，则代表该端口已被占用；若命令行没有输出，直接返回 root@:/ cloudreve#，则没有没占用。</p><p>2.查询占用该端口的进程：</p><p>`lsof -i :5212<br/>`</p><p>lsof -i :[查看占用5212端口的进程] ，如果命令行有输出，则显示占用该端口的进程PID；反之。</p><p>3.释放占用端口的进程</p><p>找到进程PID后，使用以下命令强制终止该进程，释放该端口：</p><p><code>kill -9 [进程ID]</code></p><p>总结</p><p>这就是博主今天分享的全部内容了，这只是博主在日常使用中总结的，如有不足之处欢迎大家了指点一二。<br/>本文原发于我的博客：<a href="https://link.segmentfault.com/?enc=I7WHaX2%2FnTZYHPyAMUjxVg%3D%3D.15w091LCDaoITOUbnlLymPNDdry1wBcb90EfBKKfQb8%3D" rel="nofollow" target="_blank">landonVPS</a></p>]]></description></item><item>    <title><![CDATA[6大CRM厂商实测：2026全业务流程一体化选型深度解析 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047557719</link>    <guid>https://segmentfault.com/a/1190000047557719</guid>    <pubDate>2026-01-22 11:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、行业背景：为什么企业需要“全流程一体化”？</h2><p>在数字化转型中，企业面临的核心痛点是<strong>业务流程割裂</strong>：营销获客的数据无法同步到销售，销售订单无法联动采购生产，售后运维与前端客户信息脱节……这些“信息孤岛”导致效率低下、客户体验差、决策缺乏依据。</p><p>全流程一体化数字平台的核心价值，是通过<strong>数据打通、流程协同、智能赋能</strong>，实现“获客→销售→订单→生产→运维→复购”的闭环，让企业从“部门级效率”升级为“企业级效率”。</p><p>本文选取<strong>超兔一体云、Dolibarr、Agile</strong> <strong>CRM</strong> <strong>、神州云动CloudCC、浪潮CRM、Apptivo</strong>六大主流平台，从<strong>全业务流程覆盖度、一体化支撑能力、行业适配性</strong>三个维度展开深度对比，为企业选型提供参考。</p><h2>二、全业务流程横向对比：从获客到复购的能力拆解</h2><p>按照“获客→销售跟单→订单执行→配货采购/装配生产→上门安装运维→复购转介绍”的全生命周期，逐一分析各平台的核心能力与差异。</p><h3>（一）获客阶段：全渠道集客与线索转化的“精准度”</h3><p>获客是企业增长的起点，核心在于“多渠道覆盖→线索精准筛选→效果可归因”的闭环。各平台的差异体现在渠道本土化适配、AI赋能深度与ROI分析能力。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>渠道覆盖</strong></td><td>百度/巨量引擎+微信/小程序+地推+工商搜客（<strong>微信私域强</strong>）</td><td>基础CRM+微信同步（渠道较窄）</td><td>海外社交（Twitter/Facebook）+邮件（<strong>跨境获客强</strong>）</td><td>市场云（线下活动+落地页）+营销自动化</td><td>快消/医药终端（促销活动）+经销商自助</td><td>基础线索跟踪+付费商机预测</td></tr><tr><td><strong>线索处理</strong></td><td>一键转客户/订单+手机号/IP抓取+自动提醒</td><td>自定义字段+手动跟进</td><td>AI线索评分（行为轨迹）+高意向优先</td><td>精细化线索管理+360°客户洞察</td><td>终端数据联动+促销线索跟踪</td><td>销售漏斗+移动提醒</td></tr><tr><td><strong>效果归因</strong></td><td>市场活动成本分摊到线索/签约转化率</td><td>简单线索来源记录</td><td>营销活动ROI分析</td><td>全渠道ROI精准计算</td><td>促销费用全流程量化</td><td>无深度归因</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>本土企业优先选<strong>超兔</strong>（微信私域+ROI分析）；</li><li>跨境企业选<strong>Agile</strong> <strong>CRM</strong>（海外社交+AI线索评分）；</li><li>快消/医药选<strong>浪潮</strong>（终端数据+经销商获客）。</li></ul><h3>（二）销售跟单阶段：从“跟进”到“转化”的“效率差”</h3><p>销售跟单的核心是“流程标准化+信息透明化+团队协作”，各平台的差异体现在跟单模型丰富度、客户洞察深度与自动化能力。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 独创“三一客”小单模型（三定+关键节点）、<strong>商机阶段模型</strong>（中长单）、<strong>多方项目模型</strong>（复杂业务）；提供<strong>独有的“跟单时间线”</strong> ，清晰展示客户互动历史（如“3月1日发送报价→3月5日客户反馈价格高→3月8日调整方案”），销售可快速回顾进展；自动生成日报，管理者实时掌握团队动态。</li><li><strong>神州云动CloudCC</strong>： 聚焦<strong>项目型企业</strong>，支持<strong>项目</strong> <strong>全生命周期管理</strong>（从商机到项目启动→执行→收尾），集成成本、工时管理，适合工程建设、IT服务等行业。</li><li><strong>Agile</strong> <strong>CRM</strong>： 可视化<strong>销售管道（Pipeline）</strong> ，实时追踪“潜在客户→报价→成交”进度，支持跨部门协作（销售→客服→技术），快速响应客户问题。</li><li><strong>浪潮</strong> <strong>CRM</strong>： 针对<strong>渠道密集型企业</strong>，提供<strong>经销商自助平台</strong>（线上下单、库存查询、对账），将经销商纳入销售流程，提升渠道效率。</li></ul><h4>2. 关键差异总结</h4><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>神州云动CloudCC</th><th>Agile CRM</th><th>浪潮CRM</th></tr></thead><tbody><tr><td>跟单模型丰富度</td><td>小单+中长单+项目（全）</td><td>项目型（强）</td><td>标准销售管道（中）</td><td>经销商流程（强）</td></tr><tr><td>客户洞察深度</td><td>360°视图+跟单时间线（独）</td><td>项目360°+成本工时</td><td>客户行为分析+跨部门共享</td><td>经销商库存+对账信息</td></tr><tr><td>自动化能力</td><td>自动日报+待办提醒</td><td>项目阶段提醒</td><td>销售预测+任务分配</td><td>经销商订单自动同步</td></tr></tbody></table><h3>（三）订单执行阶段：从“签约”到“交付”的“协同力”</h3><p>订单执行的核心是“订单类型适配+业财联动+风险管控”，各平台的差异体现在订单场景覆盖与财务闭环能力。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th></tr></thead><tbody><tr><td><strong>订单类型</strong></td><td>服务型（合同）+实物型（标准/批发/非标）+特殊型（维修/外勤工单）</td><td>基础订单+库存关联</td><td>营销订单+客服联动</td><td>项目订单+业财融合</td><td>经销商订单+ERP联动</td></tr><tr><td><strong>业财管控</strong></td><td>签约/开票/发货自动触发应收+账期/信用控制</td><td>基础订单+库存扣减</td><td>营销订单+客服账单</td><td>项目成本+财务对账</td><td>经销商对账+财务同步</td></tr><tr><td><strong>风险控制</strong></td><td>应收/开票/回款三角联动+信用度控制</td><td>无强风险管控</td><td>需集成ERP</td><td>项目预算+成本控制</td><td>渠道库存预警+费用合规</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>需覆盖多订单类型（如维修、外勤）选<strong>超兔</strong>；</li><li>项目型企业选<strong>神州云动</strong>（项目成本+财务融合）；</li><li>快消/医药选<strong>浪潮</strong>（经销商订单+ERP闭环）。</li></ul><h3>（四）配货采购/装配生产阶段：从“需求”到“交付”的“供应链能力”</h3><p>生产采购是制造型企业的核心环节，各平台的差异体现在<strong>供应链协同</strong> <strong>深度</strong>与<strong>生产集成能力</strong>。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 支持<strong>智能采购</strong>（自动计算采购量+匹配历史供应商+询价比价）、<strong>MES</strong> <strong>生产计划</strong>（排程→派工→领料→报工→质检→入库），覆盖“采购→生产”全流程；适合中小制造企业。</li><li><strong>神州云动CloudCC</strong>： 集成<strong>采购管理</strong>（需求触发→供应商协同）、<strong>库存管理</strong>（动态更新），但生产需对接第三方MES。</li><li><strong>浪潮</strong> <strong>CRM</strong>： 提供<strong>电子采购</strong>（需求自动触发）、<strong>数字供应链</strong>（供应商订单/发货/结算协同），聚焦快消/医药的“经销商→供应商”闭环。</li><li><strong>Agile</strong> <strong>CRM</strong>： 本身无生产采购模块，需集成ERP（如Oracle/SAP）实现订单→生产的联动。</li></ul><h4>2. 供应链能力矩阵</h4><table><thead><tr><th><strong>平台</strong></th><th>采购管理</th><th>生产集成</th><th>供应链协同</th><th>适用场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>智能采购</td><td>MES生产</td><td>全流程协同</td><td>中小制造企业</td></tr><tr><td>神州云动CloudCC</td><td>基础采购</td><td>需集成</td><td>供应商协同</td><td>项目型企业（如IT服务）</td></tr><tr><td>浪潮CRM</td><td>电子采购</td><td>需集成</td><td>渠道供应链</td><td>快消/医药</td></tr><tr><td>Agile CRM</td><td>无</td><td>需集成</td><td>无</td><td>营销型企业</td></tr></tbody></table><h3>（五）上门安装运维阶段：从“服务”到“口碑”的“体验感”</h3><p>售后运维的核心是“快速响应+资源调度+服务质量监控”，各平台的差异体现在工单管理深度与客户信息联动。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>工单管理</strong></td><td>维修/外勤工单+智能调度+服务质量监控</td><td>售后工单+SLA管理</td><td>客服工单+客户360°视图</td><td>现场服务云+资源优化调度</td><td>售后跟踪+备品库存联动</td><td>Work Orders+移动签到</td></tr><tr><td><strong>客户联动</strong></td><td>工单关联客户历史（如“设备型号→维修记录”）</td><td>基础客户信息</td><td>客服→销售→技术跨部门共享</td><td>项目备品+客户需求联动</td><td>终端客户→经销商→售后联动</td><td>基础客户+工单记录</td></tr><tr><td><strong>服务监控</strong></td><td>服务时间+客户反馈+绩效分析</td><td>无深度监控</td><td>工单进度跟踪</td><td>服务成本+响应时间监控</td><td>售后满意度+问题闭环</td><td>无深度监控</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>需高服务质量的企业（如设备制造）选<strong>超兔</strong>（工单+客户历史联动）；</li><li>项目型企业选<strong>神州云动</strong>（现场服务云+资源调度）；</li><li>快消/医药选<strong>浪潮</strong>（终端售后+备品联动）。</li></ul><h3>（六）复购与转介绍阶段：从“留存”到“裂变”的“增长力”</h3><p>复购转介绍是企业的“利润引擎”，核心在于“潜在需求挖掘+精准触达+激励机制”。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 通过<strong>RFM</strong> <strong>分析</strong>（最近购买时间、频率、金额）识别高复购客户；设置<strong>复购流失预警</strong>（如“客户超过6个月未购买→自动提醒销售跟进”）；提供<strong>转介绍激励工具</strong>（如“推荐好友得折扣”），实现客户裂变。</li><li><strong>Agile</strong> <strong>CRM</strong>： 基于<strong>客户行为分析</strong>（如“浏览过升级套餐”）自动触发复购提醒（邮件/短信）；支持<strong>社交分享</strong>（推荐好友得优惠），适合线上营销型企业。</li><li><strong>神州云动CloudCC</strong>： 通过<strong>服务云</strong>（售后满意度管理）提升客户粘性；<strong>伙伴云</strong>（合作伙伴协作）挖掘转介绍机会，适合项目型企业。</li><li><strong>Dolibarr</strong>： 需<strong>手动设置“下次购买时间”提醒</strong>，潜在需求（如设备升级）依赖人工挖掘，复购效率低。</li></ul><h4>2. 复购能力评分（1-5分）</h4><table><thead><tr><th>平台</th><th>超兔</th><th>Agile</th><th>神州云动</th><th>浪潮</th><th>Apptivo</th><th>Dolibarr</th></tr></thead><tbody><tr><td>潜在需求挖掘</td><td>5</td><td>4</td><td>4</td><td>3</td><td>2</td><td>1</td></tr><tr><td>精准触达能力</td><td>5</td><td>4</td><td>4</td><td>3</td><td>2</td><td>1</td></tr><tr><td>转介绍激励机制</td><td>5</td><td>4</td><td>3</td><td>3</td><td>2</td><td>1</td></tr></tbody></table><h2>三、一体化支撑能力：从“能用”到“好用”的“底层逻辑”</h2><p>全流程一体化的核心支撑是<strong>数据连通、</strong> <strong>客制化</strong> <strong>、AI、集成能力</strong>，决定了平台的“灵活性”与“扩展性”。</p><h3>（一）一体化支撑能力对比表</h3><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>数据连通</strong></td><td>全模块底层打通（CRM+进销存+生产+财务）</td><td>统一数据库（客户+销售+库存）</td><td>营销+销售+客服连通</td><td>全模块连通（项目+销售+财务）</td><td>渠道+供应链+财务连通</td><td>基础模块连通（销售+库存）</td></tr><tr><td><strong>客制化</strong> <strong>能力</strong></td><td>零编码引擎（自定义菜单+表单+工作流）</td><td>本地部署+自定义字段</td><td>云配置+API定制</td><td>零编码定制+行业模板</td><td>低代码inBuilder+行业模板</td><td>标准化模块+付费扩展</td></tr><tr><td><strong>AI应用</strong></td><td>自定义AI智能体（嵌入客户/行动视图）+Coze工作流</td><td>无AI</td><td>AI线索评分+行为分析</td><td>营销自动化+项目预测</td><td>终端数据洞察+费用预测</td><td>商机预测（付费）</td></tr><tr><td><strong>集成能力</strong></td><td>多端（Web/App/小程序）+RPA+ERP对接</td><td>本地/私有云+基础集成</td><td>云多端+ERP/SAP集成</td><td>多端+OA/ERP整合</td><td>多端+PaaS+行业系统</td><td>移动+基础API</td></tr></tbody></table><h3>（二）雷达图：各平台综合能力评分（1-10分）</h3><table><thead><tr><th>指标</th><th>超兔</th><th>Agile</th><th>神州云动</th><th>浪潮</th><th>Apptivo</th><th>Dolibarr</th></tr></thead><tbody><tr><td>全流程覆盖度</td><td>10</td><td>7</td><td>9</td><td>8</td><td>6</td><td>7</td></tr><tr><td>行业适配性</td><td>8</td><td>7</td><td>9</td><td>10</td><td>6</td><td>7</td></tr><tr><td>客制化能力</td><td>10</td><td>7</td><td>9</td><td>8</td><td>5</td><td>7</td></tr><tr><td>AI应用深度</td><td>10</td><td>8</td><td>8</td><td>7</td><td>6</td><td>5</td></tr><tr><td>集成扩展性</td><td>10</td><td>9</td><td>9</td><td>8</td><td>6</td><td>7</td></tr></tbody></table><h2>四、行业适配与选型建议</h2><p>根据企业<strong>规模、行业、核心需求</strong>，给出针对性选型建议：</p><table><thead><tr><th><strong>企业类型</strong></th><th>核心需求</th><th>推荐平台</th><th>理由</th></tr></thead><tbody><tr><td>中小制造企业</td><td>全流程覆盖（采购+生产+运维）</td><td>超兔一体云</td><td>智能采购+MES生产+工单管理，性价比高</td></tr><tr><td>跨境营销型企业</td><td>海外获客+销售转化</td><td>Agile CRM</td><td>海外社交+AI线索评分+销售管道</td></tr><tr><td>项目型企业（IT/工程）</td><td>项目全生命周期+成本控制</td><td>神州云动CloudCC</td><td>项目管理+采购协同+业财融合</td></tr><tr><td>快消/医药企业</td><td>渠道获客+供应链协同</td><td>浪潮CRM</td><td>终端数据+经销商自助+电子采购</td></tr><tr><td>中小微轻量需求</td><td>基础流程+移动管理</td><td>Apptivo</td><td>免费版支持基础功能，付费扩展商机预测</td></tr><tr><td>需本地部署企业</td><td>数据主权+基础一体化</td><td>Dolibarr</td><td>本地/私有云部署+统一数据库</td></tr></tbody></table><h2>五、结论：全流程一体化的“本质”是什么？</h2><p>全流程一体化的核心不是“功能堆砌”，而是“以客户为中心”的流程协同——让营销知道“客户从哪来”，销售知道“客户需要什么”，生产知道“客户何时要”，售后知道“客户之前的问题”。</p><p>从对比来看：</p><ul><li><strong>超兔一体云</strong>是“全流程覆盖 + 高性价比”的首选，适合中小制造/服务企业；</li><li><strong>神州云动CloudCC</strong>是“项目型企业”的最佳选择；</li><li><strong>浪潮</strong> <strong>CRM</strong>是“快消/医药渠道密集型企业”的定制化方案；</li><li><strong>Agile</strong> <strong>CRM</strong>是“跨境营销型企业”的海外获客利器。</li></ul><p>企业选型时，需优先明确<strong>核心业务场景</strong>（如生产、渠道、项目），再匹配平台的<strong>差异化能力</strong>，避免“为了一体化而一体化”。</p><p><strong>附录：全业务流程时序图</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557721" alt="" title=""/></p><pre><code>sequenceDiagram
    participant 企业 as 企业
    participant 超兔 as 超兔一体云
    participant Agile as Agile CRM
    participant 神州云动 as 神州云动CloudCC
    participant 浪潮 as 浪潮CRM
    participant Apptivo as Apptivo
    participant Dolibarr as Dolibarr

    企业-&gt;&gt;超兔: 百度/微信获客→线索转订单→智能采购→MES生产→工单运维→复购预警
    超兔-&gt;&gt;企业: 全流程数据同步+转介绍激励

    企业-&gt;&gt;Agile: 海外社交获客→AI线索筛选→销售跟单→集成ERP生产→客服工单运维→复购提醒+社交分享
    Agile-&gt;&gt;企业: 营销+销售+客服数据连通

    企业-&gt;&gt;神州云动: 市场云获客→销售流程自动化→商机转订单→采购库存管理→现场服务运维→服务云提升满意度+伙伴云转介绍
    神州云动-&gt;&gt;企业: 全模块连通+项目全生命周期管理

    企业-&gt;&gt;浪潮: 快消/医药终端获客→经销商自助下单→订单执行→电子采购→售后跟踪→终端售后满意度提升
    浪潮-&gt;&gt;企业: 渠道+供应链+财务连通

    企业-&gt;&gt;Apptivo: 基础线索跟踪获客→销售漏斗跟单→订单执行→移动工单运维→客户管理促复购
    Apptivo-&gt;&gt;企业: 基础模块连通

    企业-&gt;&gt;Dolibarr: 基础CRM获客→销售跟单→订单执行→库存采购管理→售后工单运维→手动复购提醒
    Dolibarr-&gt;&gt;企业: 统一数据库数据同步</code></pre><p>综上所述，各全业务流程一体化数字平台都有其独特的优势和适用场景。企业在进行平台选型时，应充分结合自身实际情况，依据核心业务场景和需求，审慎选择最契合的平台，以实现企业业务流程的高效整合和持续发展。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务与价格以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[域名注册与域名解析：分清这两个核心环节，搭建网站不踩坑 防火墙后吃泡面 ]]></title>    <link>https://segmentfault.com/a/1190000047557722</link>    <guid>https://segmentfault.com/a/1190000047557722</guid>    <pubDate>2026-01-22 11:03:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在互联网世界中，域名是网站的“门牌号”，而域名注册与域名解析则是让这个“门牌号”生效的两大核心步骤。很多建站新手会将二者混淆，甚至误以为是同一回事，导致操作中出现域名注册成功却无法访问的问题。本文，资深域名服务商国科云将从概念、区别、流程、关联及避坑要点等方面进行全面解析，帮你彻底理清二者的逻辑，轻松搞定域名相关操作。</p><h2>一、核心概念：域名注册与解析到底是什么？</h2><p>1.域名注册：获取“门牌号”的合法使用权</p><p>域名注册是指用户通过正规域名注册商，向全球统一的域名管理机构（如ICANN，互联网名称与数字地址分配机构）申请，获得特定域名在一定期限内的合法使用权的过程。</p><p>域名注册具有唯一性和时效性两大核心特征。唯一性意味着同一域名在全球范围内只能被一个主体注册，比如“baidu.com”被百度注册后，其他主体无法再注册相同域名；时效性则指注册后的域名有使用期限（通常1-10年），需按时续费才能维持使用权，逾期未续费会被暂停解析甚至回收再售卖。</p><p>注册过程中，用户需提交真实信息完成实名认证（国内域名强制要求），包括个人身份证或企业营业执照等材料，虚假信息可能导致域名被冻结。注册成功后，注册商将提供域名管理服务，包括续费、信息修改、转移等功能。</p><p>2.域名解析：给“门牌号”绑定具体地址</p><p>域名解析是通过DNS（域名系统）服务器，将易记忆的域名转换为服务器能识别的IP地址（如IPv4的“95.127.211.85”、IPv6的“<br/>2001:0db8:85a3:0000:0000:8a2e:0370:7334”），让用户输入域名就能访问对应服务器的过程。形象地说，这相当于给已有的门牌号，绑定到具体的房屋地址，让访客能通过门牌号找到对应的房子。</p><p>互联网中的计算机本质上通过IP地址相互通信，但IP地址是一串复杂数字，难以记忆。域名解析的核心价值的就是建立“域名-IP”的映射关系，既保留了用户记忆的便捷性，又能让网络设备准确定位服务器。整个解析过程由DNS服务器集群协同完成，通常耗时几十毫秒，用户几乎无感知。</p><p>解析的核心是配置DNS记录，常见类型包括A记录（绑定IPv4地址）、AAAA记录（绑定IPv6地址）、CNAME记录（绑定其他域名别名）、MX记录（配置邮件服务器）等，不同记录对应不同的服务需求。</p><h2>二、关键区别：注册与解析可不是一回事</h2><p>域名注册和域名解析是两个独立但关联的环节，核心区别体现在目的、性质、操作对象三个维度，具体如下：</p><p>1.核心目的不同</p><p>域名注册的目的是获得域名的合法使用权，解决“归属权”问题，确保你拥有这个专属“门牌号”；而域名解析的目的是建立域名与IP的映射关系，解决“访问路径”问题，让用户能通过域名找到对应的服务器。没有注册的域名无法解析，注册后不解析的域名也无法被正常访问。</p><p>2.操作性质不同</p><p>域名注册是一次性申请+定期续费的流程，属于“权利获取”类操作，一旦完成注册，在有效期内无需重复操作，仅需关注续费即可；域名解析是技术性配置操作，属于“功能激活”类操作，可根据服务器变更、服务调整等需求反复修改解析记录，比如更换服务器后，需重新配置解析指向新IP。</p><p>3.操作对象不同</p><p>域名注册的操作对象是域名注册商和全球域名管理机构，用户需在注册商平台完成申请、支付、实名认证等操作，由注册商向管理机构提交注册请求；域名解析的操作对象是DNS服务器，用户可在注册商提供的解析平台，或第三方专业解析平台（如国科云解析、DNSPod）配置解析记录，本质是修改DNS服务器中的映射数据。</p><h2>三、实操流程：从注册到解析的完整步骤</h2><p>1.域名注册全流程（以国内平台为例）</p><p>第一步，需求规划与可用性查询。</p><p>明确域名用途（企业官网、个人博客等），选择合适后缀（.com、.cn最常用，.net、.org为补充），通过注册商的查询工具或WHOIS平台，确认目标域名是否已被注册。若心仪域名被抢注，可调整名称或选择替代后缀，同时规避商标冲突，避免注册后被仲裁收回。</p><p>第二步，选择正规注册商。</p><p>优先选择国科云、阿里云、腾讯云等有工信部资质的平台，避免非正规平台的续费暴涨、域名锁死等问题。</p><p>第三步，填写信息与支付。</p><p>提交注册人真实信息，个人提供身份证，企业提供营业执照，开启域名隐私保护服务隐藏个人信息；选择注册年限并支付费用，通常几分钟内即可完成注册。</p><p>第四步，完成实名认证。</p><p>国内域名注册后需在规定时间内实名认证，材料审核通过后（通常1-3个工作日），域名才能正常使用，否则会被暂停解析。</p><p>2.域名解析全流程</p><p>第一步，准备基础信息。</p><p>获取服务器公网IP（云服务器在控制台查询），若使用CDN服务则获取CNAME地址；确认域名的DNS服务器，默认使用注册商DNS，也可更换为专业解析平台的DNS。</p><p>第二步，进入解析管理页面。</p><p>登录注册商或解析平台后台，找到“DNS解析”入口，进入解析记录配置界面。</p><p>第三步，添加解析记录。</p><p>根据需求选择记录类型：搭建网站优先配置A记录（IPv4）或AAAA记录（IPv6），填写服务器IP；使用CDN则配置CNAME记录指向CDN地址；搭建企业邮箱需配置MX记录，设置邮件服务器地址及优先级。</p><p>第四步，等待解析生效。</p><p>添加完成后，DNS缓存需10分钟至24小时全网刷新，国内地区通常20分钟内可生效。可通过“nslookup”命令（WindowsCMD或Mac终端）验证，若显示对应IP则解析成功。</p><h2>四、域名注册和域名解析是否要同一服务商？</h2><p>域名注册与解析的核心关联的是“先后顺序”：必须先注册域名，再进行解析，二者共同构成网站访问的基础。但二者并非绑定在同一服务商，用户可根据需求选择“统一平台”或“分拆平台”管理。</p><p>1.统一服务商：适合新手与高效管理</p><p>将注册与解析放在同一平台（如国科云注册+国科云解析），优势在于便捷性。注册商通常为自有域名提供自动解析适配，减少手动配置成本，且域名续费、解析修改、SSL证书绑定等操作可在同一控制台完成，故障排查更高效，无需跨平台沟通。这种方式适合新手、个人站长及追求管理效率的企业。</p><p>2.分拆服务商：适合高需求场景</p><p>若对解析性能、安全性有特殊需求，可将解析权限转移至第三方专业平台。专业解析服务商通常拥有全球分布式解析节点、DDoS防护等功能，能提升网站访问速度和抗攻击能力。操作时需在注册商后台修改DNS服务器地址，备份原有解析记录，再在新平台重新配置，全程需注意TTL值（缓存生存时间）设置，建议临时改小至300秒加快生效。</p><h2>五、注册与解析的常见问题</h2><p>1.注册环节避坑</p><p>（1）避免盲目选择冷门后缀，.xyz、.top等小众后缀虽价格低，但用户认可度和搜索引擎信任度不足，不利于品牌推广；</p><p>（2）设置自动续费和到期提醒，重要域名可一次性注册多年，防止忘记续费导致域名丢失；</p><p>（3）不注册包含知名品牌词汇的域名，避免触发商标仲裁被收回。</p><p>2.解析环节避坑</p><p>（1）解析前核对服务器IP准确性，IP错误会导致网站无法访问；</p><p>（2）国内域名需完成备案后再解析到大陆服务器，未备案域名会被拦截；</p><p>（3）修改解析时选择网站访问低谷时段（如凌晨），减少解析中断对用户的影响；</p><p>（4）若解析生效慢，可清除本地DNS缓存（Windows执行“ipconfig/flushdns”命令）。</p>]]></description></item><item>    <title><![CDATA[Apache SeaTunnel MySQL CDC 支持按时间启动吗？ SeaTunnel ]]></title>    <link>https://segmentfault.com/a/1190000047557733</link>    <guid>https://segmentfault.com/a/1190000047557733</guid>    <pubDate>2026-01-22 11:02:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557735" alt="" title=""/></p><p>在 MySQL CDC 任务中，很多用户都会遇到这样的问题：任务失败后该从哪里恢复？只知道一个时间点，却拿不到对应的 binlog 位点怎么办？Apache SeaTunnel 2.3.12 通过引入按时间启动（Timestamp Startup）功能，给出了更直观的答案。</p><p>本文围绕该能力的设计背景、配置方式与实现机制展开解析，帮助读者理解如何基于时间语义更高效地进行 CDC 任务恢复与数据回溯。</p><h2>功能概述</h2><h3>Problem：CDC 启动点配置“技术正确，但使用困难”</h3><p>在 Apache SeaTunnel 2.3.12 之前，MySQL CDC 连接器主要支持从<strong>指定 binlog 位点（file + position）或 GTID</strong> 启动数据同步任务。这种方式在实现上是精确且可靠的，但在真实生产与运维场景中，往往并不符合用户的使用习惯。</p><p>在实际 CDC 运维过程中，用户更容易掌握的是 <strong>“时间”</strong>，而非底层 binlog 细节，例如：</p><ul><li>任务异常中断后，希望从<br/><strong>“2024-04-01 10:00:00”</strong> 之后继续同步</li><li>对某一时间窗口的数据进行回溯或补采</li><li>只知道“昨天 08:00 之后的变更需要重新同步”，但无法定位对应的 binlog 文件和偏移量</li></ul><p>如果仍要求用户手动将时间反推为 binlog 位点，不仅配置复杂，而且极易出错，也显著增加了运维成本。这种“技术友好、但用户不友好”的启动方式，已经成为 CDC 任务恢复和回溯场景中的常见痛点。</p><h3>Solution：引入按时间启动</h3><p>为解决上述问题，Apache SeaTunnel 在 <strong>2.3.12 版本</strong>中为 MySQL CDC 连接器引入了<strong>按时间启动功能</strong>。</p><p>该功能允许用户直接指定一个 <strong>Unix 时间戳（毫秒级）</strong> 作为同步起始点。MySQL CDC 连接器会在启动阶段自动完成以下工作：</p><ol><li>根据指定时间戳定位对应的 binlog 文件与偏移量</li><li>从该 binlog 位置开始读取变更事件</li><li>自动跳过所有早于该时间点的历史事件</li></ol><p>通过引入“时间”这一更符合业务语义的维度，SeaTunnel 将 CDC 启动方式从<strong>面向底层 binlog 细节</strong>，提升为<strong>面向业务时间语义</strong>，显著降低了 CDC 任务在<strong>恢复、回溯和运维场景</strong>下的使用门槛。</p><h2>配置参数</h2><p>要启用按时间启动功能，需要配置以下两个关键参数：</p><table><thead><tr><th>参数名</th><th>类型</th><th>必填</th><th>说明</th></tr></thead><tbody><tr><td><code>startup.mode</code></td><td>Enum</td><td>否</td><td>设置为 <code>"timestamp"</code> 启用时间模式 <a href="#1-1" target="_blank">2</a></td></tr><tr><td><code>startup.timestamp</code></td><td>Long</td><td>是</td><td>Unix 时间戳（毫秒），指定启动时间点 <a href="#1-2" target="_blank">3</a></td></tr></tbody></table><h2>配置示例</h2><pre><code class="yaml">env {
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 10000
}

source {
  MySQL-CDC {
    url = "jdbc:mysql://localhost:3306/testdb"
    username = "root"
    password = "root@123"
    table-names = ["testdb.table1"]
    
    # 启用按时间启动
    startup.mode = "timestamp"
    startup.timestamp = 1672531200000  # 2023-01-01 00:00:00 UTC
  }
}

sink {
  Console {
  }
}</code></pre><h2>技术实现</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557736" alt="" title="" loading="lazy"/></p><h3>启动模式枚举</h3><p>在 <code>MySqlSourceOptions</code> 类中定义了所有支持的启动模式，包括新增的 <code>TIMESTAMP</code> 模式：</p><pre><code class="java">public static final SingleChoiceOption&lt;StartupMode&gt; STARTUP_MODE =
    (SingleChoiceOption)
        Options.key(SourceOptions.STARTUP_MODE_KEY)
            .singleChoice(
                StartupMode.class,
                Arrays.asList(
                    StartupMode.INITIAL,
                    StartupMode.EARLIEST,
                    StartupMode.LATEST,
                    StartupMode.SPECIFIC,
                    StartupMode.TIMESTAMP))</code></pre><h3>时间戳过滤实现</h3><p>核心实现在 <code>MySqlBinlogFetchTask</code> 类中，当检测到启动模式为 <code>TIMESTAMP</code> 时，会使用 <code>TimestampFilterMySqlStreamingChangeEventSource</code> 来处理 binlog 事件：</p><pre><code class="java">StartupMode startupMode = startupConfig.getStartupMode();
if (startupMode.equals(StartupMode.TIMESTAMP)) {
    log.info(
        "Starting MySQL binlog reader,with timestamp filter {}",
        startupConfig.getTimestamp());

    mySqlStreamingChangeEventSource =
        new TimestampFilterMySqlStreamingChangeEventSource(
            sourceFetchContext.getDbzConnectorConfig(),
            sourceFetchContext.getConnection(),
            sourceFetchContext.getDispatcher(),
            sourceFetchContext.getErrorHandler(),
            Clock.SYSTEM,
            sourceFetchContext.getTaskContext(),
            sourceFetchContext.getStreamingChangeEventSourceMetrics(),
            startupConfig.getTimestamp());
}</code></pre><h3>偏移量计算</h3><p>在 <code>MySqlSourceFetchTaskContext</code> 中实现了根据时间戳查找对应 binlog 偏移量的逻辑：</p><pre><code class="java">private Offset getInitOffset(SourceSplitBase mySqlSplit) {
    StartupMode startupMode = getSourceConfig().getStartupConfig().getStartupMode();
    if (startupMode.equals(StartupMode.TIMESTAMP)) {
        long timestamp = getSourceConfig().getStartupConfig().getTimestamp();
        try (JdbcConnection jdbcConnection =
                getDataSourceDialect().openJdbcConnection(getSourceConfig())) {
            return findBinlogOffsetBytimestamp(jdbcConnection, binaryLogClient, timestamp);
        } catch (Exception e) {
            throw new SeaTunnelException(e);
        }
    } else {
        return mySqlSplit.asIncrementalSplit().getStartupOffset();
    }
}</code></pre><h2>启动模式对比与适用场景</h2><p>为了更好地理解按时间启动功能在整体 CDC 启动体系中的定位，下面对 MySQL CDC 当前支持的几种启动模式进行对比说明：</p><table><thead><tr><th>启动模式</th><th>启动依据</th><th>优点</th><th>适用场景</th></tr></thead><tbody><tr><td><code>INITIAL</code></td><td>全量 + 当前 binlog</td><td>一次性完成历史与增量同步</td><td>首次接入数据源</td></tr><tr><td><code>EARLIEST</code></td><td>最早可用 binlog</td><td>不依赖具体位点</td><td>binlog 保存周期较长的场景</td></tr><tr><td><code>LATEST</code></td><td>当前最新 binlog</td><td>启动快</td><td>仅关注未来增量数据</td></tr><tr><td><code>SPECIFIC</code></td><td>指定 binlog file + position</td><td>精确可控</td><td>已明确掌握 binlog 位点的场景</td></tr><tr><td><code>TIMESTAMP</code></td><td>指定时间戳（毫秒）</td><td>配置直观、符合业务语义</td><td><strong>任务恢复、数据回溯、按时间窗口同步</strong></td></tr></tbody></table><p>可以看到，<strong>TIMESTAMP 模式并不是替代 SPECIFIC 或 GTID 的“更底层”方案</strong>，而是为了解决“用户只知道时间、不知道 binlog”的典型问题，是一种<strong>以可用性和运维友好性为核心的补充能力</strong>。</p><h2>测试验证</h2><p>该功能在集成测试中得到了充分验证，测试用例 <code>MysqlCDCSpecificStartingOffsetIT</code> 验证了按时间戳启动的正确性 <a href="#1-6" target="_blank">7</a> 。</p><h2>使用注意事项</h2><ol><li><strong>版本要求</strong>：需要 SeaTunnel 2.3.12 或更高版本</li><li><strong>时间戳格式</strong>：必须使用 Unix 时间戳，单位为毫秒</li><li><strong>binlog 可用性</strong>：确保指定时间点对应的 binlog 文件仍然可用</li><li><strong>时区考虑</strong>：时间戳基于 UTC 时区，需要注意时区转换</li></ol><h2>总结</h2><p>SeaTunnel MySQL CDC 的按时间启动功能为数据同步提供了更精确的控制能力，特别适用于需要从特定时间点恢复数据同步的场景。该功能通过时间戳到 binlog 偏移量的转换，实现了高效的时间点定位和数据过滤。</p><h2>Notes</h2><ul><li>该功能在工厂类 <code>MySqlIncrementalSourceFactory</code> 中通过条件配置规则进行参数验证</li><li>除了 MySQL CDC，其他 CDC 连接器如 SQL Server CDC 也支持类似的时间戳启动功能</li></ul>]]></description></item>  </channel></rss>