<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[TDengine IDMP 让制糖生成真正做到“看得清、管得住、跑得稳” TDengine涛思数据 ]]></title>    <link>https://segmentfault.com/a/1190000047558063</link>    <guid>https://segmentfault.com/a/1190000047558063</guid>    <pubDate>2026-01-22 13:03:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 12 月，涛思数据与北京海莱德自动化工程有限公司（简称“海莱德”）正式建立合作伙伴关系。此次合作，海莱德将基于自身行业自动化系统集成能力，结合涛思数据提供的 TDengine TSDB + IDMP 产品组合，共同为制糖等行业客户打造从数据采集、治理到智能分析应用的完整解决方案，助力制糖工业企业实现生产运营的数字化与智能化转型。</p><h2>行业背景｜制糖生产正在面对的新挑战</h2><p>制糖行业的生产实践表明，甘蔗制糖是一项高度连续、强耦合、对运行稳定性要求极高的工业过程。原料受品种、成熟度、含糖量和纤维含量等因素影响，天然波动较大；加之榨季集中、生产节奏紧凑，一旦发生非计划停车或关键参数失控，带来的不仅是产量损失，更可能造成难以弥补的经济影响和社会影响（涉及甘蔗和甜菜的农业生产）。</p><p>在此背景下，行业内绝大多数糖厂长期依赖以人工经验为主的工艺调整方式，以及以工段为单位、相互割裂的数据管理模式，这些传统做法逐渐显现出其局限性。经验固然重要，但难以在不同班组、不同人员之间稳定传承与高效复制；生产数据虽然持续产生，却因分散在不同系统与记录中而难以整合分析，从而无法有效支撑对稳产、提质、降本目标的持续精细化管控。这已成为行业的一个普遍共识：仅依靠传统方式，已难以应对当前生产运行对稳定性与过程可控性日益提升的要求。</p><h2>面临挑战｜从“看不清”到“管不住”</h2><p>在实际运行中，以下这些挑战并非个案，而是制糖行业中普遍存在的共性问题。</p><p>首先，生产过程链条长、环节多，从预处理、压榨、澄清、蒸发、煮糖到分蜜、干燥包装，各工段数据往往分散在不同的系统与记录中，缺乏统一视角，导致难以形成真正贯穿全流程的生产监控与分析能力。</p><p>其次，在工艺质量管控方面，参数调整长期依赖人工经验判断。许多异常往往在最终质量指标已发生偏差后才得以察觉，缺乏对工艺质量的过程性分析与持续监控手段，难以实现事前预警与主动干预。</p><p>最后，在物料与糖分损耗管理上，行业长期缺乏有效的工具进行清晰、有效的分析和管理。糖分损耗分散于滤泥、废蜜、洗水、跑糖等多个环节，大多依靠经验估算，无法形成系统、可对比的“糖损画像”，这在很大程度上制约了对产糖效率与整体经营指标的持续优化。</p><p>正是这些普遍存在的“看不清、管不住”的痛点，促使制糖行业开始重新思考生产管理方式，并推动如 TDengine IDMP 这样的生产数据与工艺管理平台，逐渐成为企业进行数字化转型、实现精细化运营的重要选择。</p><h2>解决方案｜从“数据分散”到“AI Ready”，让制糖跑在数据之上</h2><p>在榨季现场，行业内常有一种共识：“数据其实都有，就是用不起来。”原料特性每日波动，工艺流程长且复杂，相关数据往往分散在局部的 DCS、各类设备的独立系统及手工台账中。操作人员依赖经验盯守，生产系统中前后无高效的数据流通，一旦生产节奏加快，潜在的风险与异常便容易被淹没在庞杂的信息流中。</p><p>因此，选择引入 TDengine IDMP 平台，其初衷并非简单“再上一套系统”，而是旨在将沉睡的数据转化为直接支撑生产决策与运营优化的能力。围绕制糖行业原料波动大、流程链路长、设备可靠性要求高等特点，该平台以 <strong>TDengine TSBS + TDengine IDMP</strong> 为核心，从数据采集与接入起步，逐步打通数据治理、业务情景化建模与 AI 分析应用，致力于构建一套真正面向生产、服务于工艺优化与稳定运行的工业数据管理体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558065" alt="图1 以 TDengine IDMP 为基础面向生产的工业数据管理体系" title="图1 以 TDengine IDMP 为基础面向生产的工业数据管理体系"/></p><h3>数据采集｜先把“碎数据”连成一条线</h3><p>在项目启动之初，制糖企业现场所面临的情况在行业中并不陌生：数据体量并不少，但分布零散。工厂局部的 DCS、各类设备的独立系统仅仅服务于局部的监控层面。而在数据分析、集中管理与智能应用层面，则长期缺乏统一、高效的数据出口。</p><p>针对这一现状，项目规划在不影响现有控制系统稳定运行的前提下，于集控层之上构建独立的数据采集与汇聚通道。计划在每个工厂部署一套 TDengine TSDB，利用其自带的零代码采集工具 taosX，通过 OPC 标准接口从 DCS Server 读取实时工艺数据，以实现关键生产数据的稳定采集与接入。同时，在企业级数据中心部署统一的 TDengine TSDB，对各工厂的时序数据进行集中汇聚与统一管理，为后续的数据整合分析与跨厂协同打下基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558066" alt="图2 某甘蔗制糖项目的数据采集架构图" title="图2 某甘蔗制糖项目的数据采集架构图" loading="lazy"/></p><p>这种架构既充分保留了 DCS 与 SCADA 的成熟运行体系，又在其之上形成统一、可扩展的数据采集与汇聚层，为后续的数据治理、业务情景化和 AI 应用奠定了可靠基础。</p><h3>数据分析｜从“看历史”到“提前知道”</h3><p>在数据分析层，平台基于 <strong>TDengine TSDB</strong> 的高性能时序数据管理能力，实现实时与历史数据的统一处理，并能够结合<strong>时序基础模型的时序数据预测与异常检测能力</strong>，对生产过程和设备运行状态进行持续分析。</p><p>通过对关键工艺参数和运行指标的时序建模，时序基础模型能够识别正常运行模式，预测指标变化趋势，并对偏离正常区间的异常波动进行及时检测与预警，帮助企业提前发现潜在风险。请参考：<a href="https://link.segmentfault.com/?enc=sN2jyz915433SqiwfRuT%2BQ%3D%3D.pmuJOlFbX4NXAt3exFSZR%2BySFEDHkXAggJWr1R9%2FAT38Oa7cr%2FHdAVDgl6dUK5Zy" rel="nofollow" target="_blank">时序数据分析智能体 TDgpt</a></p><p>该能力使生产管控从依赖经验的事后分析，转向基于数据的趋势预判与异常识别，为工艺稳定运行、设备可靠性提升及运营决策提供更加及时、可靠的数据支撑。</p><h3>数据目录｜让每个岗位都用得上数据</h3><p>如果说采集和分析解决了“数据有没有、算不算得动”的问题，那么数据目录解决的，是“业务用不用得上”。</p><p>TDengine IDMP 并没有强制所有人用同一种视角看数据，而是允许不同部门按自己的业务逻辑组织数据。生产车间可以围绕工艺流程，把数据按工序、工段和关键参数来组织；设备管理部门则按设备类型和运行状态建立目录，专注设备可靠性和维护。同一份底层数据，可以在不同业务视角下被反复引用。</p><p>对业务人员来说，找数据不再是“翻系统”，而是“进目录”；对系统来说，数据有了清晰的结构和入口，才能被稳定调用、持续分析。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558067" alt="图3 甘蔗制糖厂数据目录（按设备、按工艺）" title="图3 甘蔗制糖厂数据目录（按设备、按工艺）" loading="lazy"/><br/><a href="https://segmentfault.com/write###" target="_blank">https://segmentfault.com/write###</a></p><h3>数据标准化 | 让“一吨糖”只有一种算法</h3><p>在工业系统中，数据标准化不是“规范问题”，而是直接影响结果是否可信的基础工程。航天领域曾因单位不统一而导致重大事故，这一案例反复被提及，并不是偶然，而是揭示了一个普遍规律：<strong>当数据口径不统一时，系统即使运行正常，结论也可能完全错误。</strong></p><p>在制糖生产中，这类风险同样真实存在。以澄清汁流量为例，DCS 系统通常以体积流量 m³/h 采集数据，而部分历史系统或人工台账则沿用质量流量 t/h。两种口径在各自系统内都能够正常使用，但一旦进入跨系统分析场景——例如物料衡算、产能评估或能耗核算——问题便会显现：同一个“澄清汁流量”，在不同系统中参与计算，得到的却是两套完全不同的结果。</p><p>在 TDengine IDMP 中，这类问题不再依赖人工经验去“记住差异”，而是通过模型层面的标准化设计，从源头上消除歧义，确保“一吨糖”在系统中只有一种确定、可复用的计算方式。</p><h4>将“老师傅的共识”固化为系统规则</h4><p>在实际生产中，许多关键口径早已形成行业共识，只是长期存在于经验和习惯中。TDengine IDMP 通过元素模板机制，将这些共识转化为可执行、可约束的系统规则。</p><p>以“澄清汁”这一对象为例，IDMP 在模型层对其进行统一、规范的定义，明确其所包含的各类属性，并对每个属性的名称、业务含义、数据类型、计量单位及使用口径进行统一约束。针对澄清汁流量，模型中会明确其业务含义、统一采用的标准计量单位、适用的工艺计算口径，以及是否参与物料衡算与产能分析等核心规则。</p><p>通过这种方式，<strong>同一类工艺对象、同一类指标在系统中只保留唯一、确定的解释</strong>，从根本上避免“同名不同义”或“同数不同算”的问题，为后续跨系统分析和长期稳定运行提供一致、可靠的数据基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558068" alt="图4 通过元素模板将知识固化" title="图4 通过元素模板将知识固化" loading="lazy"/></p><h4>单位不同？系统自动算清楚</h4><p>在统一标准的同时，TDengine IDMP 也充分考虑了现有系统的复杂性。针对属性模板，平台在公式层引入计量单位的自动识别与推导能力。</p><p>当数据来自 DCS 系统时，平台能够识别其计量单位为体积流量（m³/h）；当数据来自历史系统或台账时，则识别为质量流量（t/h）。在参与计算或分析时，TDengine IDMP 会根据目标属性所要求的计量单位，自动推导并完成必要的单位换算，确保计算结果口径一致。</p><p>整个过程无需人工干预，也不依赖个人经验假设，使不同来源、不同口径的数据能够在统一模型下安全、可靠地参与分析，为物料衡算和经营决策提供稳定支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558069" alt="图5 澄清汁的质量流量到体积流量的自动推导" title="图5 澄清汁的质量流量到体积流量的自动推导" loading="lazy"/></p><h3>数据情景化｜让工业数据真正看得懂、用得上</h3><p>在实际生产中，制糖行业越来越深刻地体会到：<strong>没有情景的数据，只是一串数字；只有将其置于具体的工艺场景中，数据才真正具有意义。</strong></p><p>榨季期间，生产现场变化极为迅速。今天可能是澄清工段的 pH 值出现波动，明天发现废蜜纯度偏高，过几天又察觉实际产糖率与理论值存在偏差。这类问题本身并不复杂，但过去的分析方式却异常耗时费力——通常由业务人员凭借经验提出初步判断，再由技术人员到各个独立系统中查找相关点位、收集数据；数据找齐后，还需反复确认其时间范围、计算口径是否一致。往往经过这样一轮繁琐流程，数天时间已经过去。</p><p>究其根源，问题通常不在于人员专业能力，而在于数据本身缺乏情景化组织。业务人员往往不清楚所需数据具体分布在哪些系统中、是否可直接使用；技术人员也难以理解这些数据在工艺上应如何关联、如何分析，以及它们之间的业务逻辑是什么。这种数据与业务之间的“断层”，使得高效的分析与决策难以实现。</p><h4>连接业务与技术的关键一环</h4><p>引入 TDengine IDMP 平台后，制糖企业将能够使数据真正成为业务与技术之间的“通用语言”。</p><p>该平台通过为数据补充统一、清晰的业务语义，将其与具体的生产过程直接关联。每一条数据都将被明确归属到特定的工艺环节（如澄清、蒸发或煮糖），同时标识其反映的工艺机理类型（如反应强度、抽提效率或回收损失），并清楚定义其适用的业务场景（如质量监控、物料衡算、异常分析或工艺优化）。</p><p>在此基础上，平台还将构建标准化的技术元数据层，对数据来源、计量单位和合理取值范围进行统一管理。由此，数据从何处来、如何计算将变得清晰可溯，在进行数据分析、计算或设置告警时，系统能够自动确保口径一致，从而避免因理解偏差导致的结果不一。</p><p>这一步的关键价值在于，许多原本存在于“老师傅经验”中的隐性知识与共识，将被有效地沉淀并固化为清晰、可复用的系统规则，为知识的传承与规模化应用奠定基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558070" alt="图6 数据情景化（业务描述和限值）" title="图6 数据情景化（业务描述和限值）" loading="lazy"/></p><h4>业务分析真正实现自助</h4><p>在数据完成情景化之后，制糖企业的业务分析方式将发生根本性转变，从过去高度依赖 IT 部门支持，转向以业务人员自助分析为主。系统前端将不再展示零散的点位编号与底层数据结构，而是围绕“澄清稳定性”“物料衡算”“产糖效率”等业务人员熟悉的工艺情景来组织数据与功能。</p><p>以澄清工段为例，工艺人员在“澄清稳定性”情景下，将能够直接选取 pH 值、混浊度、色值等关键指标，并自行拖拽搭建趋势对比与关联分析面板，用于实时判断反应状态是否偏离正常区间。整个过程无需向 IT 部门提出建模或取数需求，分析逻辑也将更加贴近现场实际。业务人员从而能真正基于数据流进行自主判断与决策。</p><p>这种以业务情景为核心的分析模式，将显著降低数据使用门槛与技术障碍，使得工艺人员更愿意、也更能够主动、自信地使用数据工具，推动数据分析融入日常作业闭环。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558071" alt="图7 澄清工艺是否稳定？业务人员自助分析" title="图7 澄清工艺是否稳定？业务人员自助分析" loading="lazy"/></p><h4>响应能力的显著提升</h4><p>当业务分析实现自助化，为制糖企业带来最直接的变化就是——<strong>业务响应速度得到显著提升</strong>。过去，从发现异常到形成分析结论，往往需要经过多环节传递与处理，周期以天计算，等结论出来时，问题可能已经扩大，甚至错过了最佳工艺调整窗口。</p><p>未来，在数据情景化的支撑下，业务人员将能够在当班内直接完成数据取用、对比分析和假设验证。例如，当澄清工段 pH 值刚出现连续偏移时，系统可在对应的业务情景中自动聚合相关指标，工艺人员即可当场判断是否需要调整加药或工艺参数；当产糖率与预期出现偏差时，也可快速定位问题根源，判别是前段抽提、澄清损失，还是后段回收效率所致。</p><p>这意味着，问题有望在“扩大之前”就被识别和处理，从而使生产运行从被动应对逐步转向主动预防与控制。</p><p>总体而言，数据情景化将帮助制糖企业真正把数据用活于业务。生产管理将不再高度依赖个人经验与事后分析，而是逐步形成一套以数据为驱动、以业务场景为依托的快速决策机制，生产运行也因此有望变得更加稳定、高效与可控。</p><h3>无问智推｜AI 驱动的生产洞察升级</h3><p>在实际生产中，制糖行业逐渐形成一种共识：AI 技术在其中的真正价值，并非在于“替代人工思考”，而在于能够<strong>在问题尚未被明确提出之前，就已将所需的相关信息与洞察准备就绪</strong>。</p><p>过去，行业中的中控系统更多地扮演着“被动工具”的角色。监控哪些指标、如何进行关联分析，完全依赖当班人员的个人经验：工艺人员需自行回忆关键指标、查找数据点位、调整分析的时间窗口。新接班的团队往往难以快速入手；而当经验丰富的老师傅不在场时，许多隐性的工艺逻辑与判断也难以得到有效复用。</p><p>在引入 TDengine IDMP 平台并完成数据情景化构建之后，AI 所扮演的角色将发生显著转变。它将不再被动等待指令，而是基于对工艺语义与业务上下文的理解，主动识别当前生产状态，并动态<strong>推荐最贴合该业务场景的监控视图与分析内容</strong>。这使得系统能够引导注意力，辅助不同经验层次的人员更快地聚焦于关键问题，从而<strong>将专家经验转化为可持续、可复用的系统能力</strong>。</p><h4>澄清段的一个真实场景</h4><p>以澄清工段的澄清汁监控为例。过去，制糖行业在监控澄清段时，往往仅限于观察几条关键参数的实时曲线，难以系统性地判断“当前工况是否真正处于正常状态”或“其趋势是否正在恶化”。</p><p>现在，AI 会自动为用户推荐一整套<strong>符合澄清工艺逻辑的监控面板</strong>，只需简单的点击“生成”，TDengine IDMP 就能够自动生成监控看板。在澄清汁场景下，系统会优先推荐：</p><ul><li> 过去一小时澄清汁 pH 的最新值，用于快速判断当前反应状态；</li><li> 过去一天每小时澄清汁锤度的平均值，帮助用户观察短周期稳定性；</li><li> 过去一周澄清汁还原糖的平均值，以及按天汇总的变化趋势，用于评估澄清效果对糖损的影响。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558072" alt="AI 推荐的澄清汁的监控面板" title="AI 推荐的澄清汁的监控面板" loading="lazy"/></p><p>这些内容并不是“通用模板”，而是因为系统已经理解：<strong>这些指标正是澄清段最关键、最有业务意义的数据组合</strong>。</p><h4>从“人盯数据”到“系统叫人”</h4><p>在引入 TDengine IDMP 之前，制糖行业对澄清段的监控更多依赖人工经验。中控画面上曲线一直在动，工艺人员需要长时间盯着趋势，凭感觉判断是不是“有点不对劲”。采用 TDengine IDMP 之后，这种状态发生了明显改变。基于已经完成的数据情景化，AI 不再等待人工提问，而是<strong>主动推荐与澄清汁相关的实时事件监控和分析</strong>，通过实时分析预警，能够在关键时刻把人“叫过来”。</p><p>在澄清汁场景中，系统能够自动推荐分析：</p><ul><li>当澄清汁加热器出口温度超过 105℃，并持续 5 分钟以上时，立即触发主要告警，同时给出该时段的平均出口温度，清楚提示存在过热风险；</li><li>对澄清汁锤度，系统每 5 分钟基于 3 倍标准差的 K-sigma 方法进行异常检测，一旦波动异常，直接给出最大锤度值，帮助用户快速判断异常程度；</li><li>系统还推荐每 10 分钟滚动计算过去 30 分钟内的平均流量，用于辅助判断当前负荷是否发生变化。</li></ul><p>在过去，这些判断逻辑往往只掌握在少数经验丰富的工艺人员手中，依赖于人员持续盯守数据、反复比对分析才能得以运用。如今，通过引入 TDengine IDMP 平台，这些经验与逻辑得以被 AI 沉淀并固化为持续、自动运行的系统能力。生产管理模式由此从依赖“人盯数据”逐步转向为“系统预警、人员确认”的协同机制，使异常得以更早被识别，工艺调整也能更及时地执行。这正是 TDengine IDMP 为制糖行业生产管理带来的最直观价值——<strong>将隐性知识显性化，将个人经验转化为可持续、可复用的系统智能。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558073" alt="AI 自动推荐的实时分析场景" title="AI 自动推荐的实时分析场景" loading="lazy"/></p><h4>给制糖行业带来的真正变化</h4><p>对制糖行业来说，最大的变化在于：<strong>正常时不被数据打扰，异常时绝不会被遗漏。</strong><br/>生产管理也由此从“人盯数据”转向“系统叫人”，让异常更早被发现，让调整更及时发生。这正是 TDengine IDMP 在实际生产中带给制糖行业的最直观价值。</p><h2>应用成效｜从“系统上线”到“价值落地”</h2><p>随着该工业数据平台在生产现场的深入部署与应用，制糖企业有望在生产管理与工艺管控方面逐步收获系统性成效。整体解决方案围绕生产、工艺和设备三大核心对象展开，将推动数据不再仅仅停留在系统层面，而是持续融入日常运行与管理决策之中。</p><h3>全流程生产监控：让制糖过程“看得见”</h3><p>通过对制糖工艺流程进行统一的数据资产建模，平台实现了从预处理到干燥包装的全过程数据采集与集中监控。各工段之间原本割裂的数据被打通，形成连续、完整的生产视图。关键工艺参数和运行状态能够集中呈现，为现场管理、生产调度以及异常发现提供了直观、统一的支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558074" alt="" title="" loading="lazy"/></p><h3>生成物料损耗分析：让损耗“算得清”</h3><p>围绕工艺过程和物料流转，平台引入了系统化的数据分析与物料衡算方法，对糖分在关键环节中的变化进行结构化分析，使以往主要依赖经验判断的物料损耗问题，转变为可量化、可对比的结果。生产、工艺和设备状态对管理层更加透明，为工艺优化和质量管控提供了更有依据的决策支持。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558075" alt="各个工艺段制糖损耗分析" title="各个工艺段制糖损耗分析" loading="lazy"/></p><h3>工艺质量实时监控：让生产“跑得稳”</h3><p>围绕关键工艺参数和质量指标，平台构建了持续运行的工艺质量监控体系，对生产各环节的运行状态进行实时跟踪和对比分析，使工艺波动由事后发现逐步转变为过程可控。通过对工艺偏差和异常趋势的及时识别，有效降低了过程波动对产品质量的影响，推动生产运行保持稳定。</p><p>工艺质量状态在生产层和管理层之间更加透明，为工艺调整和质量管控提供了持续、可靠的数据依据，有效支撑制糖生产的稳定运行和产品质量的均质化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558076" alt="澄清汁 PH 值实时监控" title="澄清汁 PH 值实时监控" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558077" alt="工艺质量异常告警（澄清汁 PH 值）" title="工艺质量异常告警（澄清汁 PH 值）" loading="lazy"/></p><h2>商业价值｜制糖企业可持续演进的数字化底座</h2><p>从行业应用与发展的角度来看，此类项目的价值并不仅体现在一次性的系统建设或阶段性验收上，更在于为企业构建了一套可长期演进、持续赋能的数字化底座。通过统一的数据标准与平台架构，制糖行业首次获得了对全生产过程进行持续感知、系统分析与长效优化的能力，这为后续的管理深化与智能应用奠定了坚实基础。</p><p>短期而言，项目的实施将有效提升生产透明度与运行稳定性；从中长期看，该平台有望逐步成长为支撑企业实现稳产、提质、降本与风险精准管控的核心基础设施。</p><h2>行业意义｜一条稳健、可落地的制糖数字化路径</h2><h4>适用企业</h4><ul><li>希望持续提升管理水平和长期竞争力的甘蔗制糖企业</li><li>正处于数字化转型关键阶段的中小规模糖厂</li></ul><h4>成功前提</h4><ul><li>管理层对数字化目标和数据价值形成清晰、统一的认知</li><li>具备相对稳定、连续的生产和设备数据基础</li></ul><h4>核心路径</h4><ul><li>以“工艺 + 物料 + 设备”为主线，系统推进数字化建设</li><li>按“看得见 → 算得清 → 跑得稳”的节奏逐步实施，避免激进投入</li><li>在夯实数据基础之上，稳步迈向智能优化和 AI 应用</li></ul><h2>未来展望｜通过组态强化生产过程与工艺质量管控</h2><p>从预期效果来看，TDengine IDMP 将在生产数据采集、集中监控与分析方面为制糖企业打下坚实基础，从而有效支撑生产过程监控与工艺质量分析的日常需求。</p><p>在此基础上，企业可期待未来进一步引入并强化平台的组态能力，以更加直观、图形化的方式呈现工艺流程、设备运行状态与关键工艺参数。这将推动生产监控从以数据列表和图表展示为主，逐步升级为面向过程与运行状态的综合可视化管控。通过组态化配置关键质量指标和工艺约束条件，有助于将成熟的工艺经验固化为可自动执行的监控规则，提升对工艺偏差和质量风险的提前识别与主动干预能力，从而更好地服务于制糖生产长期、稳定、高效的运行目标。</p><h2>关于海莱德</h2><p>北京海莱德自动化工程有限公司成立于 2010 年，是国内工业自动化技术与解决方案提供商，在制糖行业自动化领域具有专业积累。公司业务覆盖系统设计、工程实施、调试及售后服务等全流程，并在食品饮料、汽车、电力、冶金、烟草和机械制造等行业积累了丰富工程经验。近年来，海莱德参与了多个“一带一路”糖厂的集中控制 DCS 系统及数字化系统的设计、供货与调试，持续推进从自动化向数字化、信息化和智能化方向升级，并结合涛思数据的时序数据库和 TDengine IDMP 平台建立起了对制糖企业真正高效、实用且易于掌握的，具备 AI 智能的数字化系统。</p>]]></description></item><item>    <title><![CDATA[再添钻石伙伴！TDengine 签约上海罗盘，共拓数据治理 + 时序存储新生态 TDengine涛思]]></title>    <link>https://segmentfault.com/a/1190000047558092</link>    <guid>https://segmentfault.com/a/1190000047558092</guid>    <pubDate>2026-01-22 13:02:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，涛思数据与上海罗盘信息科技有限公司（以下简称 “上海罗盘”）举行钻石分销商签约仪式，标志着双方正式达成深度战略合作，将依托各自在数据领域的核心优势，携手为金融、制造、政企等多行业客户提供 “数据治理 + 时序存储” 全链路解决方案，推动时序数据技术在更多场景中的落地应用。</p><p>涛思数据创始人兼 CEO 陶建辉、战略渠道与生态合作总监郭浩，上海罗盘董事长马力等双方核心团队成员出席签约仪式，共同见证这一重要合作时刻。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558094" alt="" title=""/></p><h2>优势互补，构建数据全生命周期服务闭环</h2><p>作为深耕数据领域 23 年的资深玩家，上海罗盘自 2002 年成立以来，始终聚焦数据治理与数据中台核心赛道，在全国布局分支机构、研发基地与交付中心，服务覆盖银行、证券、保险、制造等多个关键行业，已为 200 多家大型客户落地创新项目，与多家 500 强企业建立长期信任合作关系。凭借完善的解决方案、成熟的交付模式与多项自主知识产权，上海罗盘在数据资产梳理、质量管控、中台搭建等领域积累了深厚的行业经验与客户资源，成为国内数据管理领域的标杆企业。</p><p>而涛思数据自主研发的 TDengine 时序数据库（Timeseries Database），凭借 “读写性能超传统方案 10 倍以上、存储成本仅为 1/10” 的核心优势，以及信创认证、高并发支撑、轻量化部署等特性，已成为工业时序数据存储与分析的首选方案；同时，AI 原生的工业数据管理平台 TDengine IDMP 以首创的“无问智推”能力重塑工业数据的建模、治理与消费方式，推动企业加速迈向数字化与智能化。</p><p>此次合作，上海罗盘在数据治理与中台建设的前端优势，将与 TDengine 在时序数据存储、分析的核心技术形成完美互补，构建 “数据治理 - 中台整合 - 时序存储 - 智能分析” 的全生命周期服务闭环，为客户解决数据管理中的碎片化痛点，提供更高效、更完整的数字化转型支撑。</p><p>签约仪式上，涛思数据创始人&amp; CEO 陶建辉表示：“数据治理是数字化转型的基础，时序数据是工业互联网、物联网场景的核心资产，两者的深度融合是行业发展的必然趋势。上海罗盘在数据治理领域的 23 年沉淀与广泛客户资源，与 TDengine 的技术优势高度契合。此次钻石级合作，将进一步完善涛思数据的生态布局，让优质的时序数据技术通过成熟的服务体系触达更多行业客户，共同赋能千行百业的数字化升级。”</p><p>上海罗盘董事长马力对合作充满期待：“TDengine 作为国产时序数据库的领军品牌，其技术实力与市场口碑有口皆碑。上海罗盘深耕数据管理领域多年，深刻理解不同行业客户在数据全链路管理中的核心诉求。此次与涛思数据达成深度合作，将借助 TDengine 的核心技术补全时序数据存储与分析的关键环节，为客户提供更全面的数字化解决方案。期待双方在技术协同、市场推广、行业落地等方面实现共赢，共创数据价值新高度。</p><h2>生态聚力，共绘数字化转型新蓝图</h2><p>当前，数字化转型进入深水区，数据已成为企业核心生产要素，而时序数据作为物联网、工业互联网、金融风控等场景的关键数据类型，市场需求持续爆发。涛思数据始终坚持 “技术驱动 + 生态共建” 的战略，通过汇聚行业优质伙伴力量，构建优势互补、协同共赢的生态体系，让 TDengine 技术更快落地行业场景，为客户提供本地化、高效化的服务支持。</p><p>此次上海罗盘的加入，不仅为涛思数据生态注入了数据治理领域的强劲动能，更标志着 TDengine 钻石分销商矩阵正式成型！自分销商招募计划启动以来，涛思数据凭借全球领先的产品体系、开放共赢的合作理念，吸引了众多行业标杆企业加入，生态影响力持续扩大。</p><p>未来，涛思数据将继续深化与包括上海罗盘在内的所有生态伙伴的合作，在技术协同、方案共创、行业落地等方面持续发力，以更完整的产品服务链路、更深厚的行业落地能力，为千行百业的数字化转型提供核心支撑。</p>]]></description></item><item>    <title><![CDATA[AI赋能！TDengine IDMP工业数据管理平台助力化工研发创新 TDengine涛思数据 ]]></title>    <link>https://segmentfault.com/a/1190000047558097</link>    <guid>https://segmentfault.com/a/1190000047558097</guid>    <pubDate>2026-01-22 13:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 12 月，涛思数据与沈阳化工研究院（简称“沈阳院”）正式达成合作。涛思数据将为其提供 TDengine TSDB + IDMP 产品组合，通过部署工业数据管理平台，以 AI 原生的数据智能技术，支撑沈阳院构建覆盖从实验室研究到中试放大全流程的统一数据基座，助力其研发数字化转型迈向新阶段。</p><p>沈阳院是我国重要的综合性化工科研院所，其研发过程中涉及海量、多源的时序数据与非时序数据，同时其中试基地拥有多条专业化生产线。面对实验室、中试装置产生的庞杂数据，如何打破数据孤岛，实现数据的统一管理、关联分析与智能洞察，从而加速研发进程、优化生产工艺，是沈阳院数字化转型的重要任务。</p><p>随着数字化进程的推进，沈阳院需要一个能够打通从实验到中试全流程的数据管理平台，能够将时序数据与非时序数据（如物料信息、实验记录）进行关联分析，同时满足信创环境要求和数据安全规范。涛思数据全新发布的 <strong>TDengine IDMP</strong>（工业数据管理平台）产品，具备“<strong>无问智推</strong>”的 AI 原生能力，这种让数据主动说话的能力，正是解决业务人员依赖 IT 团队获取数据洞察的关键。</p><p>本次项目需要采集和分析来自实验室、中试生产线的数据，总计需要监控的<strong>测点约 2 万</strong>。这些数据来源于高压釜、干燥箱、色谱仪等实验设备，以及生产线的温度、压力、流量等工艺参数，还包括水电气等能耗数据。TDengine TSDB 支持多种数据接入方式，包括 MQTT、OPC-UA/DA 等。这对于研究院现有的数据采集系统（MQTT 和 OPC）非常重要。在数据建模方面，TDengine IDMP 采用树状层次结构，这与研究院的设备组织方式天然契合。比如，可以按照“研究院-中试基地-生产线-设备”的层级结构建立数据目录，每个节点都可以配置属性、分析规则和可视化面板。这种结构特别适合中试基地的批次分析需求，可以清晰地展示每个批次的工艺参数和质量指标。</p><p>本项目采用“整体规划、分步实施”的策略，项目计划分两阶段进行：</p><ul><li>第一阶段，选择 3 个实验室和 1 条中试生产线进行试点实施；</li><li>第二阶段，基于试点成果向全院范围推广。</li></ul><p>基于数据安全性和网络环境考虑，选择本地化部署方案。部署架构如下图所示：<br/><img width="723" height="397" referrerpolicy="no-referrer" src="/img/bVdnIce" alt="" title=""/></p><p>本次项目规划的设计思路紧密围绕化工研发的业务特点展开，力图在以下几大关键业务场景提升数据应用效率与深度：</p><ol><li><strong>数据全景可视化与智能告警</strong>：通过 TDengine IDMP 的智能可视化功能，实现实验数据和中试生产数据的全景可视化管理。研究人员无需 IT 支持即可通过自然语言交互获取所需数据视图；通过实时分析和事件管理功能，自动触发告警，并帮助研究人员快速定位问题根源；借助“无问智推”能力，自动推送质量波动的批次与标准参数的对比分析，帮助管理人员快速决策。</li><li><strong>工艺优化与批次对比分析</strong>：批次分析是中试生产的核心需求之一。借助 TDengine IDMP，可以实现多批次数据的自动对比分析。系统能够根据批次质量指标帮助科研人员找到"黄金批次"，并分析其工艺参数特征，为工艺优化提供数据支持。通过时序数据高级分析功能，研究人员可以轻松对比不同批次的差异，找出影响产品质量的关键工艺参数。</li><li><strong>预测性维护与能耗管理</strong>：基于 TDengine TDgpt 的能力，平台能够轻松集成时序数据的预测、异常检测、分类、补全、相关性分析等算法和模型，帮助客户实现对关键设备的实时监控与预测性维护。在中试基地的能耗管理方面，通过对水、电、气的实时监测与统计分析，帮助找出能效瓶颈、识别出能耗异常点，用以指导设备改造和工艺调整。</li><li><strong>数据驱动的工艺包开发</strong>：TDengine 产品组合将帮助研究院实现数据驱动的研发模式，提高工艺包开发的效率和质量。新工艺包的设计可以基于历史中试数据，确保工艺参数的可靠性。而工艺包转化为实际生产后，又可以通过对比设计数据与实际生产数据，持续优化工艺模型。同时，TDengine IDMP 内置了备份/恢复机制，未来还将支持 Git 式数据版本管理，有望进一步提高数据归档、变迁、回溯的能力。</li></ol><p>本次涛思数据与沈阳化工研究院的强强联合，为化工科研数据管理和数据分析描绘出更多可能性。相信此次合作不仅能提升沈阳院的研发效率，更有望探索出一条以数据智能驱动化工行业创新的可行路径。</p><h3>关于沈阳化工研究院</h3><p>沈阳化工研究院有限公司始建于 1949 年 1 月 8 日，是综合性化工科研院所，现为中国中化控股有限责任公司直管单位。目前沈阳院主要开展化工新材料、生态农业、生物化工、化学品测试与评价、化工反应风险评估、危险废物鉴别、化工智能优化等方向的研究及产业化。沈阳院聚焦提升关键共性技术的研究与开发能力、较强的新产品孵化能力和适度产业规模和盈利能力；致力于成为精细化工行业国内领先，国际有一定影响力的科技型企业。</p>]]></description></item><item>    <title><![CDATA[阶跃星辰开源多模态模型 Step3‑VL‑10B，小模型实现大模型能力；华为或将发布首款 AI 眼镜]]></title>    <link>https://segmentfault.com/a/1190000047558108</link>    <guid>https://segmentfault.com/a/1190000047558108</guid>    <pubDate>2026-01-22 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558110" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01有话题的技术</h2><p><strong>1、阶跃星辰开源 Step3‑VL‑10B：10B 模型对标 200B 能力</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558111" alt="" title="" loading="lazy"/></p><p>昨天，阶跃星辰宣布正式开源旗下 10B 参数量多模态模型 Step3‑VL‑10B。该模型在多项核心基准测试中达到同规模 SOTA 水平，部分能力甚至超越 10–20 倍体量的大模型。</p><p>Step3‑VL‑10B 主打「小模型实现大模型能力」，在视觉感知、逻辑推理、数学竞赛题、多模态对话等任务中表现突出。</p><p>阶跃星辰称，Step3‑VL‑10B 的性能已接近甚至超越部分百亿级开源模型（如 GLM‑4.6V 106B‑A12B、Qwen3‑VL‑Thinking 235B‑A22B），并在部分场景中达到顶级闭源旗舰模型（如 Gemini 2.5 Pro、Seed‑1.5‑VL）水平。</p><p>官方强调，该模型的关键突破来自三项核心设计：</p><ul><li><strong>全参数端到端多模态联合预训练</strong>：在 1.2T 高质量多模态数据上训练，实现视觉与语言的深度对齐；</li><li><strong>大规模多模态强化学习</strong>：经历超过 1,400 次迭代，使模型在识别、推理与对话能力上持续提升；</li><li><strong>并行协调推理机制</strong>：通过并行探索与证据聚合提升复杂任务的准确度，尤其在数学推理、OCR、计数与空间拓扑任务中效果显著。</li></ul><p>Step3‑VL‑10B 同时提供 SeRe（顺序推理）与 PaCoRe（并行推理）两种范式，覆盖 STEM 推理、OCR、GUI Grounding、空间理解与代码等多项能力维度。</p><p>当前，Step3‑VL‑10B 已开放 Base 与 Thinking 两个版本，社区可在 HuggingFace 与 ModelScope 获取模型并进行微调。</p><p>项目主页：<br/><a href="https://link.segmentfault.com/?enc=gQK0nod50ZMU7xahz9SMVw%3D%3D.eb%2FTnjHG7XLQncxIrjK2MBT26i5Fss5dsy%2BYIb32s4gj4X7AK2rCIbx2CUOwcBZj" rel="nofollow" target="_blank">https://stepfun-ai.github.io/Step3-VL-10B/</a></p><p>Hugging Face: <br/><a href="https://link.segmentfault.com/?enc=5op%2BnrliMKf%2BfXIob8jG%2BQ%3D%3D.kmEK3dSxBvmwmGsf1bWKSLFavWWC443252vKXjpwDAS4hr%2BddzJsr6OCWebpT%2FUkYtN7lf9pgqWgd9HFfEpN2g%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/collections/stepfun-ai/step3-vl-10b</a></p><p>ModelScope: <br/><a href="https://link.segmentfault.com/?enc=Zo22gGo1za%2FM42QlIUx2Ig%3D%3D.gVgplhmUVFdeTyzRQIQ0E0tInouekMN%2BOFRpKQ6Y1RKKp33k7R3%2BVpJTEm7UInuDvbobYa7RRWcJrO3r0duyzw%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/collections/stepfun-ai/Step3-VL-10B</a></p><p>论文链接：<br/><a href="https://link.segmentfault.com/?enc=hShAiScLaMbKWRRgvAtE7A%3D%3D.J3ZMiCrh6HYXHef0wNE%2FXamv5Z4VJXUguaCAtWw%2F%2BZhd06Co7qIFgLVMcwxuYUUg" rel="nofollow" target="_blank">https://arxiv.org/pdf/2601.09668</a></p><p>（@阶跃星辰、@APPSO）</p><p><strong>2、showlab 开源 whisperVideo：集成 SAM3 与 TalkNet 实现长视频「音视对齐」的说话人转录</strong></p><p>showlab 近期开源了名为 whisperVideo 的项目，专门致力于解决长视频场景下「谁在说话」的身份归属难题。该工具打破了传统方案仅依赖音频的局限，通过融合视听双重特征，实现了语音内容与画面特定人脸的精准对齐。</p><p>为了突破纯音频方案在多人混响或近距离交谈时常见的识别漂移问题，whisperVideo 构建了一套紧密的多模态级联架构。它集成了 WhisperX 负责语音转录、Pyannote.audio 处理声纹分离，并引入 SAM3 进行人脸分割以及 TalkNet 判定主动说话人。这种组合拳方式，确保了机器能像人类一样同时「听」和「看」，从而做出更准确的判断。</p><p>针对小时级素材中常见的跨场景挑战，工具特别引入了「长时身份一致性」机制。利用视觉嵌入与轨迹聚类技术，系统能在漫长的视频时间轴上记住每一张脸，确保同一说话人的 ID 在不同场景切换中始终保持稳定。</p><p>在工作流设计上，whisperVideo 追求全自动化体验。内置的 SceneDetect 能够自动进行场景切割与分段处理，无需人工干预即可完成时间戳、文本与视觉 ID 的三方对齐。最终生成的成果不仅包括带说话人 ID 的字幕，还支持可视化的面板模式，并将底层数据以 。pckl 格式开放给开发者。</p><p>目前，项目已在 GitHub 开源，需使用 CUDA GPU 环境，依赖 HuggingFace Token 调用 Diarization 模型，支持 Python 命令行一键推理。</p><p>GitHub: <br/><a href="https://link.segmentfault.com/?enc=2gryj89afY7wssArAKTKTg%3D%3D.Hsrj5fFNCx4%2BA7YP7rvyGvgg6CcC1KjImMHzKfYRB%2BnzP3tOYXPi09dGJ3ewJ0lO" rel="nofollow" target="_blank">https://github.com/showlab/whisperVideo</a></p><p>( @aigclink\@X)</p><p><strong>3、Bolna 获 630 万美元种子轮融资：自研 SLM 语音智能体，支持「印式英语」混说</strong></p><p>总部位于班加罗尔的初创公司「Bolna」近日完成了由 General Catalyst 领投的 630 万美元种子轮融资。这家公司致力于通过自研的专用小模型（SLM）技术，打破多语言环境下的自动化通信瓶颈。</p><p>为了适应印度极其复杂的语言生态，Bolna 构建的语音智能体不仅将端到端响应延迟控制在 500 毫秒以内，更实现了深度的本地化适配。它能够流畅处理包括印地语、泰米尔语在内的 10 余种本土语言及 50 多种地区口音，甚至针对印度特有的语言混合现象，专门优化了对「印式英语（Hinglish）」的语义理解与生成能力。</p><p>在技术架构上，Bolna 摒弃了昂贵的通用大模型方案，转而采用针对事务性查询优化的 SLM 与智能路由架构。这种策略有效平衡了计算成本与响应速度，使其更适合大规模商业落地。配合其提供的无代码控制台，企业可自主设计并监控智能体。目前，该平台的日呼叫处理量已从 1,500 通激增至 20 万通以上，广泛应用于购物车挽回、货到付款确认及招聘筛选等场景。</p><p>平台现已正式上线，主要面向印度企业提供订阅制的自助服务。</p><p>( @AI Tech Suite)</p><h2>02有亮点的产品</h2><p><strong>1、消息称华为首款 AI 眼镜将在上半年发布：搭载鸿蒙 OS，支持同传翻译与拍照</strong></p><p>1 月 20 日多家媒体消息，华为的第一款「AI 眼镜」暂定在今年上半年推出，支持拍照和音频，鸿蒙系统 + 跨端无缝协同，同传翻译等功能。 AI 眼镜被誉为「下一代 AI 终端超级入口」，已然是大厂必争之地，百度、小米、阿里、理想等早已进场，并推出了 AI 拍照眼镜，字节也即将推出 AI 眼镜，作为国内消费类智能终端龙头的华为自然不会落后于人。</p><p>据 @数码闲聊站 爆料，华为 AI 眼镜将采用鸿蒙 OS 系统与轻量化设计，内置 3 块锂电池，支持跨端无缝协同，进一步拓展使用场景。并提供流光银、钛银灰、摩登黑三款配色，支持拍照、拍视频、音频播放以及同声传译等功能。</p><p>虽然目前具体细节尚未公布，但结合华为在 AI 技术领域的探索，预计将内置华为 AI 助手小艺，产品可能涉及 AI 识物、智能场景推荐等功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558112" alt="" title="" loading="lazy"/></p><p>经查询发现，华为曾推出带有音频功能的智能眼镜，主打听音乐、打电话、健康播报等。如今随着 AI 的兴起，智能眼镜行业也纷纷上马 AI，以及自带摄像头、显示屏的 AI 眼镜也不断推新。</p><p>据 IDC 预测，智能眼镜产品成为 2025 年消费电子赛道的黑马，相应产品在中国市场出货量预计达到 290.7 万台，同比增长 121.1%。业内人士普遍认为，这缘于技术突破、市场需求释放以及产业链成熟等多重因素。</p><p>汇丰控股认为，智能眼镜市场仍处于加速扩张阶段。分析师预计，智能眼镜的用户规模将在未来十多年内迎来爆发式增长，到 2030 年代末将达到 2.89 亿人，较 2025 年的 1500 万用户增长超过 18 倍。</p><p>（@即智 Ultra、@IT 之家）</p><p><strong>2、MiniMax 推出「Agent 实习生」，AI-native Workspace 全面升级</strong></p><p>昨天，MiniMax 官宣，AI-native Workspace 迎来两项核心升级，进一步推动 AI 深度嵌入真实工作场景，并面向用户开放限时免费体验。</p><ul><li><strong>桌面端应用正式上线：</strong> 用户可在本地环境中指定 Workspace 作为工作空间与上下文，使 AI 能够直接理解本地文档、代码仓库、邮件与日程，从而构建一个专属于个人的智能工作环境。</li><li><strong>推出「专家 Agents」能力：</strong> 用户可构建在特定领域达到「95 分甚至 100 分」水平的专业智能体。这类 Agent 能够在复杂任务链路中稳定执行、主动判断并长期协作。</li></ul><p>公司内部数据显示，「Agent 实习生」在过去数周已被接近 100% 的员工使用，并在运维场景中承担了约 80% 的查 Bug 工作量。</p><p>MiniMax 表示，AI-native Workspace 标志着 Agent 从「被动执行指令」向「主动感知环境」的形态演进。</p><p>公司认为，未来的 Agent 将具备长期记忆、完整职业上下文与跨系统感知能力，成为用户的长期工作伙伴，而非一次性工具。</p><p>目前，MiniMax 已开启专家 Agents 的限时免费体验。用户可通过 Web 端直接试用，也可通过官方体验链接获取桌面端安装包。</p><p>体验地址：<br/><a href="https://link.segmentfault.com/?enc=I6nPxwLpIqCMjyhJBWIICw%3D%3D.HHQkLPkFfmobUemJQvVgx5TiZxGbLRxzhj8xxyLQLzw%3D" rel="nofollow" target="_blank">https://agent.minimaxi.com/</a></p><p>( @APPSO)</p><p><strong>3、Crow 发布 AI 智能体框架：支持 OpenAPI 与 MCP 协议，实现「对话即 UI」交互</strong></p><p>Crow 近期推出了一套专为 SaaS 产品打造的 AI 智能体基础设施，旨在通过「对话即 UI」的理念重构软件交互模式。该工具的核心逻辑在于将传统的点击操作转化为自然语言指令流，通过接入 OpenAPI 规范或 MCP 协议，使智能体不仅能回答问题，更能直接触发后端 API 调用及前端 UI 导航，从而实现对软件功能的深度控制。</p><p>为了解决生成式 AI 不可控的难题，Crow 引入了名为「Journeys」的结构化工作流。开发者可以针对取消订阅、创建报表等特定业务场景，定义确定性的引导路径，确保智能体在执行敏感操作时严格遵循预设的逻辑分支。配合支持文件与文档集成的 RAG 管道，智能体还能充分理解产品特定的业务逻辑与私有数据。</p><p>在开发与运维层面，Crow 提供了生产级的观测指标，能够详细追踪每一条指令对应的工具调用路径。其低代码部署方案仅需嵌入单行 Script 标签，官方宣称这能将传统长达半年以上的自研周期缩短至一周以内，并支持与 Claude Code 或 Cursor 等工具集成。目前该产品已正式上线，开发者项目可免费试用，同时针对中大型企业提供了定制化方案。</p><p>( @Y Combinator Launch)</p><p><strong>4、Thread 发布 Voice AI：实现 MSP 电话自动化分拣与实时工单同步，单人效能提升 30%</strong></p><p>Thread 宣布其专为托管服务提供商设计的 Voice AI 正式商用。该产品旨在终结传统 IVR（交互式语音应答）系统的僵化体验，通过语音智能体接管电话接入、分拣与派发的全流程，将高成本的电话渠道整合进结构化的自动化运维体系中。</p><p><strong>AI Attendant 与 Overflow Agent 双引擎驱动：</strong></p><ul><li><strong>AI Attendant</strong>：取代传统 IVR，能够即时接听电话并识别来电者身份。它不仅能进行自然的语音交互，还能在后台实时创建工单、匹配技术人员，并完成「热切换」，确保客户在转接给真人时无需重复复述问题。</li><li><strong>Overflow Agent</strong>：专为下班后或线路繁忙场景设计。它能拦截进入语音信箱的电话，自动收集关键信息并进行分类；遇到 P1 级紧急事件时，可直接升级并呼叫待命团队，消除了「下班后盲区」。</li></ul><p>Voice AI 的核心价值在于将非结构化的语音高效转化为结构化数据。系统不仅能根据通话内容自动填充工单的标题、类别、优先级和解决摘要，还引入了「自动时间条目」功能，可依据通话时长直接生成计费记录。据官方数据统计，这一特性为每张工单平均节省了 19 分钟的处理时间，从而推动单一技术人员的日均通话处理量从 8-12 通显著提升至 14-20 通。</p><p>在生态兼容性方面，该方案作为 Thread AI Service Desk 平台的重要组成部分，已与 ConnectWise、Autotask 和 HaloPSA 等主流 PSA 系统实现了原生集成。这意味着所有通话数据都会实时转化为结构化文档，并无缝同步至企业现有的工作流中，从而确保了整个服务链条的完整性与可追溯性。</p><p>据 Thread 统计，通过消除手动记录和人工轮班需求，该系统可使响应速度提升 5 倍，平均解决时间缩短 78%。目前该服务已正式上线。</p><p>相关链接：<br/><a href="https://link.segmentfault.com/?enc=nOwKqElwmEHnC4Pt5Cwb5w%3D%3D.pTtohgfX4CTHHC8ljusOHEFS6T3YcrRESxLScfzXNOsMTcJdMpgYTpo%2Bxv2LZgsk" rel="nofollow" target="_blank">https://www.getthread.com/voice-ai</a></p><p>( @Mansfield News Journal)</p><h2>03有态度的观点</h2><p><strong>1、谷歌前 CEO 施密特：欧洲要么投资开源 AI，要么依赖中国模型</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558113" alt="" title="" loading="lazy"/></p><p>1 月 20 日，据外媒报道，谷歌前 CEO、科技投资人埃里克 · 施密特 （Eric Schmidt） 周二表示，<strong>欧洲必须投资建设自己的开源 AI 实验室，并解决能源价格飙升的问题，否则很快就会发现自己对中国的模型产生依赖。</strong> 施密特周二在达沃斯世界经济论坛表示：「在美国，企业基本上正在转向闭源，这意味着这些技术将被购买、授权等等。而与此同时，中国在做法上基本是开放权重、开源的。除非欧洲愿意为欧洲自己的模型投入大量资金，否则欧洲最终将会使用中国的模型。」</p><p>目前，许多热门 AI 模型都是闭源的，比如谷歌的 Gemini 和 OpenAI 的 ChatGPT，这意味着这些公司不会向外界提供底层代码供下载或审查。虽然这种方式能为用户带来更顺畅、更统一的使用体验，但通常成本更高、灵活性也更低。中国在所谓「开放权重」模型的开发方面处于领先地位，这类模型具有更高的透明度。</p><p>为了在开发更强大 AI 模型和智能体的全球竞赛中具备竞争力，欧洲还需要解决高企的能源价格问题，并建设更多可用于训练这些技术的数据中心。施密特曾联合创办一家数据中心公司，致力于应对这类基础设施巨大的能源需求。他也对美国 AI 发展对电力供应的影响表示担忧。</p><p>（@IT 之家）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558114" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558115" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=fixs3xAKfdvLACDGOdaE9A%3D%3D.SEjbj4AwR2Zz8yYTwCZ%2BBsbBwuoIy1ZcQMgoxGkXX4Y%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558116" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[如何通过Java SDK新建Client DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047557765</link>    <guid>https://segmentfault.com/a/1190000047557765</guid>    <pubDate>2026-01-22 12:06:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文介绍如何通过Java SDK新建一个DashVector Client。</p><p><strong>说明</strong></p><p>通过DashVector Client可连接DashVector服务端，进行Collection相关操作。</p><h2>前提条件</h2><ul><li>已创建Cluster</li><li>已获得API-KEY</li><li>已安装最新版SDK</li></ul><h2>接口定义</h2><p>Java示例：</p><pre><code class="java">package com.aliyun.dashvector;

// 通过apiKey和endpoint构造
DashVectorClient(String apiKey, String endpoint);

// 通过DashVectorClientConfig构造
DashVectorClient(DashVectorClientConfig config);</code></pre><h2><strong>使用示例</strong></h2><p><strong>说明</strong></p><p>需要使用您的api-key替换示例中的YOUR_API_KEY、您的Cluster Endpoint替换示例中的YOUR_CLUSTER_ENDPOINT，代码才能正常运行。</p><p>Java示例：</p><pre><code class="java">import com.aliyun.dashvector.DashVectorClient;
import com.aliyun.dashvector.DashVectorClientConfig;
import com.aliyun.dashvector.common.DashVectorException;

public class Main {
    public static void main(String[] args) throws DashVectorException {
        // 通过apiKey和endpoint构造
        DashVectorClient client = new DashVectorClient("YOUR_API_KEY", "YOUR_CLUSTER_ENDPOINT");
      
        // 通过Builder构造DashVectorClientConfig
        DashVectorClientConfig config = DashVectorClientConfig.builder()
            .apiKey("YOUR_API_KEY")
            .endpoint("YOUR_CLUSTER_ENDPOINT")
            .timeout(10f)
            .build();
        client = new DashVectorClient(config);
    }
}</code></pre><p><strong>说明</strong></p><p>DashVectorClient初始化期间可能抛出<code>DashVectorException</code>异常，可通过具体异常信息分析初始化失败原因。</p>]]></description></item><item>    <title><![CDATA[供应商管理系统有哪些？谈谈我们测评的这8款 SaaS圈老马 ]]></title>    <link>https://segmentfault.com/a/1190000047557793</link>    <guid>https://segmentfault.com/a/1190000047557793</guid>    <pubDate>2026-01-22 12:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>供应商来对账，数据对不上，耽误好几天；采购价格不透明，成本居高不下；供应商绩效全凭印象，合作质量参差不齐...如果你也正被这些问题困扰，是时候了解一下供应商管理系统了。</p><p>今天我们就来一次深度测评，聊聊市面上主流的几款供应商管理解决方案，帮你找到最适合自家业务的那一款。</p><p><strong>一、选型要点：好的系统，到底该看什么？</strong></p><p>在直接推荐产品前，先明确几个核心选型标准，这是避开坑的关键。</p><p><strong>第一，要看它能不能解决你真实的痛点</strong>。很多企业痛点很具体：比如采购流程不规范、线上线下数据对不上、供应商质量不稳定、对账周期漫长等。系统功能是否直击这些要害，是首要考量。</p><p><strong>第二，灵活性和扩展性至关重要</strong>。特别是成长型企业，业务变化快，今天用的功能明天可能就要调整。如果系统僵硬，改个流程都要找原厂花大价钱二开，那用起来会很痛苦。所以，是否支持一定程度的自定义或低代码调整，是个加分项。</p><p><strong>第三，性价比和长期投入成本</strong>。这不单指软件本身的购买费用，还包括实施费用、每年的维护费、未来需求变化的二次开发成本，甚至数据迁移的成本。一个“买得起但用不起”的系统，不如一开始就放弃。</p><p><strong>第四，厂商的服务与可持续性</strong>。软件即服务，后续的响应速度、问题解决能力、版本迭代计划，都直接影响你的使用体验。选择有成熟服务团队、产品持续迭代的厂商，更稳妥。</p><p>基于以上几点，结合市场主流选择，我筛选出 <strong>8款值得深入考察的供应商管理系统</strong>，并对其核心特点、适用场景进行分析。</p><p><strong>二、测评盘点：8款主流供应商管理系统</strong></p><p><strong>1. 支道</strong></p><p><a href="https://link.segmentfault.com/?enc=cqxrxxRY6Nd9FS%2By%2FFYk1g%3D%3D.J8SfPTlsZ8fW%2Fj1ATN1YfIPpjy3Z2XE7wLWGeYzKzZo%3D" rel="nofollow" target="_blank">https://www.zdsztech.com</a></p><p><strong>核心特点</strong>：基于无代码平台构建，高度可定制</p><p>如果要用一个词形容支道的供应商管理方案，那就是 <strong>“灵活”</strong>。它并非一个功能固化的标准产品，而是基于其强大的无代码开发平台，能够快速搭建出贴合企业实际采购业务流程的系统。</p><p>从测评角度看，它的优势很明显：<strong>可视化搭建，改起来方便</strong>。</p><p>企业的采购审批流程、供应商准入标准、询比价模板，都可以通过拖拉拽的方式配置和修改，业务人员经过培训也能参与调整。这解决了很多企业“需求说不清、软件改不动”的痛点。</p><p>具体到SRM功能上，它覆盖了供应商全生命周期管理：供应商电子档案、在线准入申请、询价/招标/比价流程、采购订单协同、送货与验收协同、对账付款、以及供应商绩效评估。</p><p>亮点在于流程的在线化和自动化，比如报价自动汇总比价、订单状态自动同步给供应商、绩效数据自动采集计算等。</p><p><strong>适合谁用</strong>：业务独特、流程经常优化、或者未来可能将SRM与内部CRM、项目管理系统打通的成长型企业。它的无代码特性让长期迭代成本更低。</p><p><strong>需要注意</strong>：高度灵活也意味着初期需要更多的业务梳理和配置投入，更适合愿意在管理梳理上花时间、追求长期适配性的企业。<br/><img width="723" height="292" referrerpolicy="no-referrer" src="/img/bVdnH6W" alt="" title=""/></p><p><strong>2. 用友</strong></p><p><strong>核心特点</strong>：与ERP、财务系统天然集成，业财一体化能力强</p><p>用友作为国内企业管理软件的老牌厂商，其YonSuite中的SRM模块最大优势在于 “集成”。如果你的企业已经在使用或用友的ERP、财务系统，那么选择它的SRM模块，在数据打通上会非常顺畅。</p><p>采购订单直接生成应付、入库信息实时同步、成本数据自动归集，真正实现业务流、信息流、资金流合一。</p><p>功能层面，它提供标准的供应商管理、寻源管理、采购协同、库存协同等功能。在供应商绩效方面，支持多维度指标（如质量、交期、价格、服务）的量化评估。</p><p><strong>适合谁用</strong>：尤其是那些已经使用用友体系产品的中大型企业，或者对财务业务一体化要求极高、希望杜绝数据孤岛的企业。</p><p><strong>需要注意</strong>：作为标准化程度较高的产品，在面对一些非常规的、行业特有的采购流程时，可能需要通过二次开发来实现，成本和周期需提前评估。<br/><img width="723" height="285" referrerpolicy="no-referrer" src="/img/bVdnH6X" alt="" title="" loading="lazy"/></p><p><strong>3. 金蝶</strong></p><p><strong>核心特点</strong>：强调供应链协同，尤其在生产制造领域有深度方案</p><p>金蝶的云星空SRM，在制造业企业中口碑不错。它的设计思路强调 <strong>“供应链协同”</strong> ，不止管理供应商，更注重与供应商之间的高效协作。比如，支持供应商门户，让供应商自助查看订单、确认交期、填报送货单；支持与生产计划的联动，实现采购需求的精准触发。</p><p>其功能亮点在于对 VMI库存管理、JIT准时化采购、寄售业务 等复杂场景的支持，这些都是制造企业的核心痛点。在供应商风险方面，也提供了诸如资质预警、交期预警等管理功能。</p><p><strong>适合谁用</strong>：生产制造型企业，特别是对原材料采购协同、精益生产有要求的企业。也适合金蝶ERP的老用户，保障系统连贯性。</p><p><strong>需要注意</strong>：方案相对偏向中大型制造企业，对于贸易类、项目服务类企业的贴合度可能需要详细验证。<br/><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdnH68" alt="" title="" loading="lazy"/></p><p><strong>4. SAP</strong></p><p><strong>核心特点</strong>：全球化、战略寻源、网络化协同</p><p>SAP Ariba 是全球领先的采购云平台，它的定位更高，更像一个 <strong>“采购网络”</strong>。其核心优势在于 <strong>全球寻源和战略采购</strong>。如果你的企业采购范围遍布全球，需要管理跨国供应商、进行复杂的招标和合同管理，Ariba 提供了强大的支持。它拥有庞大的供应商网络，方便发现新供应商。</p><p>功能极其全面，从支出分析、寻源招标、合同管理、到供应商协同、发票与付款，覆盖整个直接和间接采购流程。其数据分析能力强大，能帮助企业深度洞察采购支出，优化采购策略。</p><p><strong>适合谁用</strong>：大型集团企业、跨国公司，或者采购品类复杂、将采购视为战略职能的企业。预算充足是前提。</p><p><strong>需要注意</strong>：实施和运维成本非常高，系统复杂，对内部管理规范性和团队能力要求极高。对于中小型企业来说，可能“杀鸡用牛刀”。<br/><img width="723" height="269" referrerpolicy="no-referrer" src="/img/bVdnH7e" alt="" title="" loading="lazy"/></p><p><strong>5. 甄云</strong></p><p><strong>核心特点</strong>：产品化程度高，开箱即用，聚焦采购全流程数字化</p><p>甄云是国内较早专注于采购数字化SRM的厂商之一。其产品特点是 “全流程、产品化” ，功能模块成熟，设计理念清晰。它围绕企业采购业务，提供从供应商管理、寻源管理、采购协同、到财务协同的完整闭环。用户体验和界面设计比较现代化，易于上手。</p><p>在供应商风险管控方面，它整合了外部大数据，可以提供供应商的工商、司法、舆情等多维度风险监控和预警，这是个很实用的亮点。</p><p><strong>适合谁用</strong>：希望快速部署一套成熟、完整SRM系统的中大型企业，特别是对供应商风险有主动管理需求的企业。它降低了从零自研的风险和成本。</p><p><strong>需要注意</strong>：作为标准化SaaS产品，在应对极端个性化的业务流程时，灵活性可能不如低代码/无代码平台。<br/><img width="723" height="364" referrerpolicy="no-referrer" src="/img/bVdnH7f" alt="" title="" loading="lazy"/></p><p><strong>6. 携客云</strong></p><p><strong>核心特点</strong>：SaaS模式，轻量化，以“协同”为核心，实施快</p><p>携客云主打 “轻量化、易实施” 的SaaS SRM。它的核心价值在于快速解决制造企业与供应商之间的 “协同效率” 问题，比如订单确认、交货、对账等高频场景。</p><p>它的供应商门户做得很轻便，供应商上手门槛低。通过它，企业可以快速实现采购订单发布、送货预约、质量反馈、对账确认等业务的在线化，显著减少打电话、发邮件的低效沟通。</p><p><strong>适合谁用</strong>：广大中小制造企业，作为ERP的延伸，首要解决与供应商的日常业务协同问题。需求明确、预算有限、希望快速上线看到效果的企业可以重点关注。</p><p><strong>需要注意</strong>：在战略寻源、深度供应商绩效分析、复杂业务流程管控等更深层的管理需求上，功能可能不如前面几款全面。<br/><img width="723" height="368" referrerpolicy="no-referrer" src="/img/bVdnH7i" alt="" title="" loading="lazy"/></p><p><strong>7. 企企通</strong></p><p><strong>核心特点</strong>：平台化思路，强调连接与生态</p><p>企企通的SRM平台同样强调协同，但其特色在于 “平台化” 和 “连接能力” 。它致力于成为连接采购方和供应商的协作平台。除了常规的SRM功能外，它在 非生产性物料采购、电商化采购 方面有特色方案，支持企业搭建内部采购商城。它也具备较强的集成能力，可以与企业内部ERP、OA等系统对接，实现流程和数据贯通。</p><p><strong>适合谁用</strong>：注重与供应商建立在线化协作生态，特别是间接物料采购（MRO）需求旺盛的大中型企业。也适合希望整合分散采购渠道的企业。</p><p><strong>需要注意</strong>：平台的综合性强，企业需要明确自身核心需求是“管理”还是“连接协同”，以便判断是否匹配。<br/><img width="723" height="349" referrerpolicy="no-referrer" src="/img/bVdnH7j" alt="" title="" loading="lazy"/><br/><strong>8. 浪潮云</strong></p><p><strong>核心特点</strong>：贴合大型集团管控需求，尤其在高安全要求行业有积累</p><p>浪潮的云ERP中包含SRM解决方案，其优势在于服务 <strong>大型集团企业、国有企业</strong> 的经验。在供应商集中管控、分级管理、采购合规性、审计追溯等方面有较深的设计。对于有严格内控和合规性要求的行业，如国资、军工等，是重点考察对象。</p><p>功能上，支持集中采购、分散采购等多种模式，与浪潮的财务、预算系统也能深度集成。</p><p><strong>适合谁用</strong>：大型集团、国有企业、对采购合规性和集中管控有刚性要求的组织。</p><p><strong>需要注意</strong>：产品和实施风格相对“稳重”，在用户体验和敏捷性上可能不是其首要追求。<br/><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdnH7k" alt="" title="" loading="lazy"/></p><p><strong>三、总结与建议：如何选择？</strong></p><p>看了一圈，你可能更纠结了。别急，最后给你一些落地的建议：</p><p>如果业务<strong>灵活多变</strong>，<strong>支道</strong>这类无代码平台的长远适配性更好。预算不仅要看首次投入，更要评估3-5年的总拥有成本。并且一定要<strong>看演示、做试点，</strong>功能列表都是美好的，真实体验才能暴露问题。要求厂商用你的真实数据（脱敏后）或模拟场景进行演示。条件允许的话，选择一个非核心采购品类或一个分子公司进行试点，这是最有效的试金石。</p><p><strong>供应商管理系统</strong>的选型，没有“最好”，只有“最适合”。它不仅是采购工具，更是企业供应链竞争力的数字化体现。</p><p>花时间厘清自身需求，结合以上测评信息，相信你能找到最适合自己提升管理效率、降低运营成本的优秀系统。</p>]]></description></item><item>    <title><![CDATA[Unity实现Nanite 本文系转载，阅读原文
https://zhuanlan.zhihu.co]]></title>    <link>https://segmentfault.com/a/1190000047557816</link>    <guid>https://segmentfault.com/a/1190000047557816</guid>    <pubDate>2026-01-22 12:05:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>【USparkle专栏】如果你深怀绝技，爱“搞点研究”，乐于分享也博采众长，我们期待你的加入，让智慧的火花碰撞交织，让知识的传递生生不息！</p><hr/><blockquote><h3><strong>一、前序</strong></h3></blockquote><p><strong>1. 介绍</strong><br/>Nanite是UE5中虚拟几何体（Virtualized Geometry System）的系统，主要用途是高效率渲染的高面数模型。Nanite会为模型自动生成LOD结构，与传统LOD不同，Nanite的LOD不再是每个模型的，而是精细到模型中的局部区域，艺术家不需再为制作或处理LOD烦恼。并且还能享有GPU Driven的高效剔除，单个绘制调用的好处。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557818" alt="" title=""/></p><p><strong>2. 技术要点</strong><br/>Nanite技术结合了多种技术做到了高效渲染：</p><ol><li>Cluster Rendering：由Cluster组织三角形，可以享有更高效的剔除。</li><li>Auto LOD：通过Graph Partitioning技术划分和简化模型构建LOD，并且把数据组织成BVH结构在Runtime时候可以高效地并行选择LOD，通过这种方式构建的LOD过渡非常丝滑。</li><li>GPU Driven Pipeline：由GPU驱动的绘制，减少了CPU的性能开销。</li><li>Occlusion Culling：更细颗粒的遮挡剔除，用于剔除不可见的三角形。</li><li>Hardware/Software Rasterization：由于小三角形对于硬件光栅化非常不友好，所以针对这些三角形用Compute Shader执行软光栅提高效率。</li><li>Visibility Buffer：利用Visibility Buffer减少Overdraw，进一步提高GPU效率。</li><li>Streaming：加载只看到的相关数据，减少几何体对内存的压力。</li></ol><p><strong>3. 本文效果</strong><br/>由于Nanite系统非常庞大和有非常多的工程细节要处理，所以本文会简化和略过一些东西，仅实现核心部分，而且会与有UE5的版本有点出入。</p><p>下图是本文实现的效果，每个色块是一个三角形，可以看出LOD切换和相机剔除都非常丝滑。</p><p>色块表示三角面<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557819" alt="" title="" loading="lazy"/></p><p>色块表示Cluster<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557820" alt="" title="" loading="lazy"/></p><blockquote><h3><strong>二、实现</strong></h3></blockquote><p><strong>1. Clusterize</strong><br/>第一步，在离线阶段处理，将复杂的超高精度网格模型高效且合理地分割成更小、更易于管理的簇（Cluster），每个Cluster最多128个三角形。这种划分不是简单的切割，而是旨在最小化簇与簇之间连接的边数（即切割大小），同时保持每个簇的大小大致均衡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557821" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557822" alt="" title="" loading="lazy"/></p><p>UE使用的Partition是Metis库：<br/><a href="https://link.segmentfault.com/?enc=jFIama9AHxGwqfiSLc9oKw%3D%3D.qnhGN7PR8sH5OMg2R7RLc80vUsRaVgJh3DPzoXFzrrbQsJe%2FWE0fg7lXlwO18Hj%2F" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=dbZWfgBHI4KBuhG3F1vizw%3D%3D.1O5dl3Q1mIM2Fld8oq69ba9LJKFdjhH9jXavE58Zie6LgrTUT990v0BueGQJexat" rel="nofollow" target="_blank">https://github.com/KarypisLab/METIS</a></p><p>实现代码可以参考UE5的源码部分：<br/>UnrealEngine-release\Engine\Source\Developer\NaniteBuilder\Private\NaniteBuilder.cpp</p><p>本文使用meshoptimizer实现Mesh的切分Cluster和Partition功能，这个库功能还有优化Over Draw，Shadow Depth Index等功能：<br/><a href="https://link.segmentfault.com/?enc=I%2Fm5xna8XvlVITdYshbUvQ%3D%3D.%2F5KpcQ9rYEByP5%2BS%2B1MBhPM6%2Fp83jKpR%2BAEN95v5FR4M0Ltq8oLlXYmbY1ykmeTV" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=RWVNXT89UHeL%2FOJsmIAVHg%3D%3D.Ya4jEXTaMlTlWXJHgktb1RyWwMNEDXzmsreIc%2BE9LU74rgT9rh1TVHzagco0oiuC" rel="nofollow" target="_blank">https://github.com/zeux/meshoptimizer</a></p><p>我们新建一个C++导出DLL的工程，封装几个主要函数让Unity可以使用。其实代码量不多，翻译成C#直接用也可以。</p><p>分别是：</p><ul><li>meshopt_buildMeshlets（构建Cluster）</li><li>meshopt_partitionClusters（Cluster划分Partition）</li><li>meshopt_buildMeshletsBound（计算Cluster数量）</li><li>meshopt_computeSphereBounds（合并BoundsSphere）</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557823" alt="" title="" loading="lazy"/></p><p>在C#中引用这些函数：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557824" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557825" alt="" title="" loading="lazy"/></p><pre><code>unsafe static List&lt;Cluster&gt; clusterize(Vector3[] vertices, int[] indices)
    {
        constint max_vertices = 192; // TODO: depends on kClusterSize, also may want to dial down for mesh shaders
        constint max_triangles = kClusterSize; //128
        constint min_triangles = (kClusterSize / 3) &amp; ~3;
        constfloat split_factor = 2.0f;
        constfloat fill_weight = 0.75f;
        int max_meshlets = BuildMeshletsBound(indices.Length, max_vertices, max_triangles);//meshopt_buildMeshletsBound 
        var meshlets = new Meshlet[max_meshlets * 2];
        var meshlet_vertices = newint[max_meshlets * max_vertices];
        var meshlet_triangles = newbyte[max_meshlets * max_triangles * 3];
        var meshlet_count = BuildMeshletFlex(meshlets, meshlet_vertices, meshlet_triangles, indices, indices.Length, vertices, vertices.Length, sizeof(float) * 3, max_vertices, min_triangles, max_triangles, 0.0f,
            split_factor);//meshopt_buildMeshlets 
        List&lt;Cluster&gt; clusters = new List&lt;Cluster&gt;(meshlet_count);
        for (int i = 0; i &lt; meshlet_count; i++)
        {
            ref Meshlet meshlet = ref meshlets[i];
            fixed (int* ptr = &amp;meshlet_vertices[meshlet.vertex_offset])
            {
                fixed (byte* ptr2 = &amp;meshlet_triangles[meshlet.triangle_offset])
                {
                    OptimizeMeshlet(ptr, ptr2, (int)meshlet.triangle_count, (int)meshlet.vertex_count);
                }
            }

            Cluster cluster = new Cluster();
            cluster.indices = newint[meshlet.triangle_count * 3];
            for (int j = 0; j &lt; meshlet.triangle_count * 3; ++j)
                cluster.indices[j] =
                    meshlet_vertices[meshlet.vertex_offset + meshlet_triangles[meshlet.triangle_offset + j]];

            cluster.parent.error = float.MaxValue;
            clusters.Add(cluster);
        }

        return clusters;
    }</code></pre><p>然后可以直接通过meshopt_buildMeshlets函数，获得每个cluster的indexs。</p><p><strong>2. Build DAG</strong><br/>有了这些Cluster，就可以构建“LOD”了，只需要循环这个操作：打组-&gt;合并-&gt;减面-&gt;clusterize。如下图：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557826" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557827" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557828" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557829" alt="" title="" loading="lazy"/></p><p>这个过程感觉就像Mipmap一样，一层一层往上合并和简化，并记录一个Err误差值和Bounds用于运行时LOD选择用。而这些合并的的节点就叫做Cluster Group。最后得出一个DAG（有向无环图，Directed Acyclic Graph）的结构。</p><pre><code>public struct ClusterGroup
    {
        public List&lt;int&gt; Children;
        public Vector3 Bounds;
        publicfloat radius;
        public Vector3 LODBounds;
        publicfloat MinLODError;
        publicfloat MaxParentLODError;
        publicint MipLevel;
    } 

publicclassNaniteSubMesh
    {
        public List&lt;ClusterGroup&gt; clusterGroupList;
        public List&lt;Cluster&gt; clusterList;
        publicint maxMipLevel;
    }

static NaniteSubMesh Nanite(Vector3[] vertices,Vector3[] normals, int[] indices)
    {
        NaniteSubMesh res = new NaniteSubMesh();
        List&lt;ClusterGroup&gt; clusterGroupList = new List&lt;ClusterGroup&gt;();
        var clusters = clusterize(vertices, indices);
        res.clusterList = clusters;
        res.clusterGroupList = clusterGroupList;
        res.maxMipLevel = 0;
        for (int i = 0; i &lt; clusters.Count; ++i)
        {
            var c = clusters[i];
            c.self = Bounds(vertices, clusters[i].indices, 0f);
            c.mip = 0;
            clusters[i] = c;
        }

        List&lt;int&gt; pending = new List&lt;int&gt;(clusters.Count);
        int[] remap = newint[vertices.Length];
        for (int i = 0; i &lt; remap.Length; ++i)
            remap[i] = i;
        for (int i = 0; i &lt; clusters.Count; ++i)
            pending.Add(i);

        int curMip = 1;
        byte[] locks = newbyte[vertices.Length];
        while (pending.Count &gt; 1)
        {
            List&lt;List&lt;int&gt;&gt; groups = partition(clusters, pending, remap, vertices);
            if (kUseLocks)
                lockBoundary(locks, groups, clusters, remap);
            pending.Clear();
            List&lt;int&gt; retry = new List&lt;int&gt;();
            int triangles = 0;
            int stuck_triangles = 0;
            for (int i = 0; i &lt; groups.Count; ++i)
            {
                var curGroupClusters = groups[i];
                if (curGroupClusters.Count == 0)
                {
                    continue; // metis shortcut
                }

                List&lt;int&gt; merged = new List&lt;int&gt;(vertices.Length);
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    merged.AddRange(clusters[curGroupClusters[j]].indices);
                }
                LODBounds groupb = boundsMerge(clusters, curGroupClusters);
                ClusterGroup clusterGroup = new ClusterGroup();
                clusterGroup.Bounds = groupb.center;
                clusterGroup.MaxParentLODError = groupb.error;
                clusterGroup.radius = groupb.radius;
                clusterGroup.Children = new List&lt;int&gt;(merged.Count);
                clusterGroup.MipLevel = curMip - 1;
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    clusterGroup.Children.Add(curGroupClusters[j]);
                }
                clusterGroupList.Add(clusterGroup);

                // aim to reduce group size in half
                int target_size = (merged.Count / 3) / 2 * 3;
                float error = 0f;
                var simplified = simplify(vertices, normals, merged.ToArray(), kUseLocks ? locks : null, target_size,
                    ref error);
                if (simplified.Count &gt; merged.Count * kSimplifyThreshold)
                {
                    stuck_triangles += merged.Count / 3;
                    for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                    {
                        retry.Add(curGroupClusters[j]);
                    }

                    continue; // simplification is stuck; abandon the merge
                }

                // enforce bounds and error monotonicity
                // note: it is incorrect to use the precise bounds of the merged or simplified mesh, because this may violate monotonicity

                var split = clusterize(vertices, simplified.ToArray());
                groupb.error += error; // this may overestimate the error, but we are starting from the simplified mesh so this is a little more correct
                // update parent bounds and error for all clusters in the group
                // note that all clusters in the group need to switch simultaneously so they have the same bounds
                for (int j = 0; j &lt; curGroupClusters.Count; ++j)
                {
                    int clusterIndex = curGroupClusters[j];
                    var t = clusters[clusterIndex];
                    t.parent = groupb;
                    clusters[clusterIndex] = t;
                }

                for (int j = 0; j &lt; split.Count; ++j)
                {
                    var sj = split[j];
                    sj.self = groupb;
                    sj.mip = curMip;
                    split[j] = sj;
                    clusters.Add(sj); // std::move
                    pending.Add(clusters.Count - 1);
                    triangles += sj.indices.Length / 3;
                }
            }

            curMip++;
        }

        if (pending.Count == 1)
        {
            var c = clusters[pending[0]];
            ClusterGroup clusterGroup = new ClusterGroup();
            clusterGroup.Bounds = c.self.center;
            clusterGroup.MaxParentLODError = c.self.error;
            clusterGroup.radius = c.self.radius;
            clusterGroup.Children = new List&lt;int&gt;(1);
            clusterGroup.MipLevel = curMip - 1;
            clusterGroup.Children.Add(pending[0]);
            clusterGroupList.Add(clusterGroup);
        }

        res.maxMipLevel = curMip - 1;
        return res;
    }

static void lockBoundary(byte[] locks, List&lt;List&lt;int&gt;&gt; groups, List&lt;Cluster&gt; clusters, int[] remap)
    {
        // for each remapped vertex, keep track of index of the group it's in (or -2 if it's in multiple groups)
        int[] groupmap = newint[locks.Length];
        for (int i = 0; i &lt; groupmap.Length; ++i)
            groupmap[i] = -1;

        for (int i = 0; i &lt; groups.Count; ++i)
        {
            var c = groups[i];
            for (int j = 0; j &lt; c.Count; ++j)
            {
                var indices = clusters[c[j]].indices;
                for (int k = 0; k &lt; indices.Length; ++k)
                {
                    var v = indices[k];
                    var r = remap[v];

                    if (groupmap[r] == -1 || groupmap[r] == i)
                        groupmap[r] = i;
                    else
                        groupmap[r] = -2;
                }
            }
        }

        // note: we need to consistently lock all vertices with the same position to avoid holes
        for (int i = 0; i &lt; locks.Length; ++i)
        {
            var r = remap[i];
            locks[i] = (byte)((groupmap[r] == -2) ? 1 : 0);
        }
    }</code></pre><p>这样我们得到各级Mip的一系列Clusters。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557830" alt="" title="" loading="lazy"/></p><p><strong>3. 加速结构</strong><br/>即使把三角形划分成Clusters数量也太多，使用Compute Shader来做并行结算效率也不高，于是Nanite就使用了BVH来作为ClusterGroup的加速结构，然后配合Persistent Threads做查找过滤。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557831" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557832" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557833" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557834" alt="" title="" loading="lazy"/></p><p>Persistent Threads遍历BVH部分，有兴趣可以参考UE5源码：<br/>Shaders\Private\Nanite\NaniteClusterCulling.usf</p><p>UE5中也有不使用Persistent Threads的流程，应该说一般默认就是不使用的。</p><p>UE5源码部分<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557835" alt="" title="" loading="lazy"/></p><p>个人认为Persistent Threads方案在GPU遍历这种BVH结构有点暴力和重度，所以简化了一下，把多个Cluster合并成一个剔除单元（Part），先并行对Part做剔除，再对Part里的Cluster去做并行剔除，两层结构来加速作为Persistent Threads的一个简单替代方案。</p><p>然后把多个Part组织成Page用于分块加载。材质处理细节也不同，UE5的材质是每个Cluster会记录MaterialRange，简单起见这里实现是每个SubMesh会去构建独立的Clusters。</p><p>代码如下：</p><pre><code> [Serializable]
    publicstruct NaniteCluster
    {
        publicint indiceIndex;
        publicint indiceCount;
        publicfloat selfErrer;
        publicfloat parentErrer;
        public Vector4 selfSphere;
        public Vector4 parentSphere;
        publicint subMeshID;
        publicint vertexOffset;
    };
    
    [Serializable]
    publicstruct NaniteClusterGroup
    {
        publicint ClusterStart;
        publicint ClusterCount;
        public Vector3 Bounds;
        publicfloat radius;
        public Vector3 LODBounds;
        publicfloat MinLODError;
        publicfloat MaxParentLODError;
        publicint MipLevel;
    }

    [Serializable]
    publicstruct NaniteMeshPart
    {
        publicint ClusterStart;
        publicint ClusterCount;
        public Vector4 selfSphere;
        publicfloat MaxParentLODError;
    }</code></pre><pre><code>public classNaniteSubMesh
    {
        public List&lt;ClusterGroup&gt; clusterGroupList;
        public List&lt;Cluster&gt; clusterList;
        publicint maxMipLevel;
    }
publicclassBuildPart
    {
        public List&lt;int&gt; clusterList;
        publicint mip;
        publicint subMesh;

    }
public static void BuildNaniteMesh(Mesh mesh)
    {
          var vertices = mesh.vertices;
        var normals = mesh.normals;
        var uvs = mesh.uv;

        int subMeshCount = mesh.subMeshCount;
        int totalClusterCount = 0;
        int totalIndexCount = 0;
        List&lt;NaniteSubMesh&gt; subMeshList = new List&lt;NaniteSubMesh&gt;();
        for (int i = 0; i &lt; subMeshCount; i++)
        {
            var triangles = mesh.GetTriangles(i);
            var subMesh = Nanite(vertices,normals,triangles);
            subMeshList.Add(subMesh);
            totalClusterCount += subMesh.clusterList.Count;
        }

        List&lt;BuildPart&gt; buildPartsList = new List&lt;BuildPart&gt;(totalClusterCount);
        int MAX_PART_PERPAGE = 128;
        int MAX_CLUSTER_PERPART = 8;

        for (int subMeshIndex = 0; subMeshIndex &lt; subMeshList.Count; subMeshIndex++)
        {
            var subMesh = subMeshList[subMeshIndex];
            List&lt;Cluster&gt; clusters = subMesh.clusterList;
            var groupsList = subMesh.clusterGroupList;
            BuildPart buildPart = null;
            for (int i = 0; i &lt; groupsList.Count; i++)
            {
                var gIndex = i; // sortGroups[i].OldIndex;
                var g = groupsList[gIndex];
                var childs = g.Children;
                for (int c = 0; c &lt; childs.Count; c++)
                {
                    int cIndex = childs[c];
                    int cMip = clusters[cIndex].mip;
                    totalIndexCount += clusters[cIndex].indices.Length;
                    //new Part
                    if (buildPart == null || buildPart.clusterList.Count &gt;= MAX_CLUSTER_PERPART ||
                        buildPart.mip != cMip)
                    {
                        buildPart = new BuildPart();
                        buildPart.clusterList = new List&lt;int&gt;(MAX_CLUSTER_PERPART);
                        buildPart.mip = cMip;
                        buildPart.subMesh = subMeshIndex;
                        buildPartsList.Add(buildPart);
                    }

                    buildPart.clusterList.Add(cIndex);
                }
            }
        }

        int buildPartCount = buildPartsList.Count;
        NaniteMeshPage[] pageArray = new NaniteMeshPage[(buildPartCount+(MAX_PART_PERPAGE-1))/MAX_PART_PERPAGE];//ceil
        List&lt;int&gt; tempIndiceList = new List&lt;int&gt;(totalIndexCount);
        List&lt;int&gt; mipLists = new List&lt;int&gt;(totalClusterCount);
        int partIndex = 0;
        for (int i = 0; i &lt; pageArray.Length; i++)
        {
            //create new page
            var p = ScriptableObject.CreateInstance&lt;NaniteMeshPage&gt;();
            pageArray[i] = p;
            tempIndiceList.Clear();
            int partCount =  (i == (pageArray.Length -1)) ? (buildPartCount % MAX_PART_PERPAGE) : MAX_PART_PERPAGE;
            p.parts = new NaniteScene.NaniteMeshPart[partCount];
            List&lt;NaniteScene.NaniteCluster&gt; pageClusters = new List&lt;NaniteScene.NaniteCluster&gt;(partCount * MAX_CLUSTER_PERPART);
            for (int j = 0; j &lt; partCount; j++)
            {
                var buildPart = buildPartsList[partIndex];
                var buildPartCluster = buildPart.clusterList;
                //create part
                var part = new NaniteScene.NaniteMeshPart();
                part.ClusterStart = pageClusters.Count; //local index
                part.ClusterCount = buildPartCluster.Count;
                int subMeshID = buildPart.subMesh;
                float maxParentErr = 0f;
                var clusters = subMeshList[subMeshID].clusterList;
                for (int c = 0; c &lt; buildPartCluster.Count; c++)
                {
                    var cluster = clusters[buildPartCluster[c]];
                    mipLists.Add(cluster.mip); 
                    //create Cluster
                    NaniteScene.NaniteCluster naniteCluster = new NaniteScene.NaniteCluster();
                    naniteCluster.indiceIndex = tempIndiceList.Count;
                    naniteCluster.indiceCount = cluster.indices.Length;
                    naniteCluster.parentErrer = cluster.parent.error;
                    naniteCluster.parentSphere = new Vector4(cluster.parent.center.x,cluster.parent.center.y,cluster.parent.center.z, cluster.parent.radius);
                    naniteCluster.selfErrer = cluster.self.error;
                    naniteCluster.selfSphere = new Vector4(cluster.self.center.x,cluster.self.center.y,cluster.self.center.z, cluster.self.radius);
                    naniteCluster.subMeshID = subMeshID;
                    tempIndiceList.AddRange(cluster.indices);
                    maxParentErr = Mathf.Max(naniteCluster.parentErrer, maxParentErr);
                    pageClusters.Add(naniteCluster);
                }

                LODBounds partBounds =  boundsMerge(clusters, buildPartCluster,true);
                part.selfSphere = new Vector4(partBounds.center.x,partBounds.center.y,partBounds.center.z,partBounds.radius);
                part.MaxParentLODError = maxParentErr;
                p.parts[j] = part;
                partIndex++;
            }
            p.clusterArray = pageClusters.ToArray();
            p.indiceArray = tempIndiceList.ToArray();
            p.clusterMip = mipLists.ToArray();
        }

        string fileName = AssetDatabase.GetAssetPath(mesh);
        string extension = Path.GetExtension(fileName);
        fileName = fileName.Replace(extension, "");
        //Build page
        int totalVerts = 0;
        for (int i = 0; i &lt; pageArray.Length; i++)
        {
            var page = pageArray[i];
            var clusterArray = page.clusterArray;
            var indiceArray = page.indiceArray;
            Dictionary&lt;int,int&gt; indicesMap = new Dictionary&lt;int,int&gt;();
            List&lt;Vector3&gt; tempVerts = new List&lt;Vector3&gt;(vertices.Length);
            List&lt;Vector3&gt; tempNormals = new List&lt;Vector3&gt;(vertices.Length);
            List&lt;Vector2&gt; tempUVs = new List&lt;Vector2&gt;(vertices.Length);
            List&lt;int&gt; newIndices = new List&lt;int&gt;(totalIndexCount);
            for (int c = 0; c &lt; clusterArray.Length; c++)
            {
                refvar cluster = ref clusterArray[c];
                var indexStart = cluster.indiceIndex;
                var indexEnd = indexStart+cluster.indiceCount;
                for (int index = indexStart; index &lt; indexEnd; index++)
                {
                    int vertIndex = indiceArray[index];
                    int newIndex;
                    if (!indicesMap.TryGetValue(vertIndex,out newIndex))
                    {
                        newIndex = newIndices.Count;
                        indicesMap.Add(vertIndex, newIndex);
                        tempVerts.Add(vertices[vertIndex]);
                        tempNormals.Add(normals[vertIndex]);
                        if (uvs.Length == 0)
                        {
                            tempUVs.Add(Vector2.zero);
                        }
                        else
                        {
                            tempUVs.Add(uvs[vertIndex]);
                        }

                        newIndices.Add(newIndex);
                    }

                    indiceArray[index] = newIndex;
                }
            }

            page.vertexStride = 5;//pos3 + uv2
            page.vertexData = newfloat[tempVerts.Count * page.vertexStride];
            page.vertexCount = tempVerts.Count;
            for (int v = 0; v &lt; tempVerts.Count; v++)
            {
                int vertexIndex = v * page.vertexStride;
                page.vertexData[vertexIndex + 0] = tempVerts[v].x;
                page.vertexData[vertexIndex + 1] = tempVerts[v].y;
                page.vertexData[vertexIndex + 2] = tempVerts[v].z;
                page.vertexData[vertexIndex + 3] = tempUVs[v].x;
                page.vertexData[vertexIndex + 4] = tempUVs[v].y;
            }
            totalVerts +=tempVerts.Count;
            string newPath = fileName + "_p"+i +".asset";
            AssetDatabase.CreateAsset(page, newPath);
        }
        AssetDatabase.Refresh();

        Debug.Log("mesh Vertx:"+vertices.Length +" mesh Nanite:"+ totalVerts + " cluster:"+totalClusterCount + "part:"+ buildPartCount +" page:"+pageArray.Length);
        NaniteMesh naniteMesh = ScriptableObject.CreateInstance&lt;NaniteMesh&gt;();
        {
            naniteMesh.subMeshCount = subMeshCount;
            naniteMesh.pageArray = new NaniteMeshPage[pageArray.Length];
            for (int i = 0; i &lt; pageArray.Length; i++)
            {
                string newPath = fileName + "_p" + i + ".asset";
                naniteMesh.pageArray[i] = AssetDatabase.LoadAssetAtPath&lt;NaniteMeshPage&gt;(newPath);
            }
        }

        var meshBound = mesh.bounds;
        naniteMesh.boundingSphere = meshBound.center;
        naniteMesh.boundingSphere.w = meshBound.extents.magnitude;
        string meshExt = "_mesh.asset";
        AssetDatabase.CreateAsset(naniteMesh, fileName + meshExt);
        AssetDatabase.Refresh();
    }</code></pre><p>到这里离线部分基本结束，可以得到一个Nanite的资源。当然UE5原文还做了很多操作，如BVH、Encode、编码、压缩、Page的划分、顶点属性优化等，个人认为这些都属于工程细节。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557836" alt="" title="" loading="lazy"/></p><p><strong>4. 运行时资源</strong><br/>来到Runtime部分，我们需要把这个Nanite Mesh加载上来，方便起见，这里直接引用一下资源在脚本上，偷懒省略加载部分。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557837" alt="" title="" loading="lazy"/></p><p>把资源、Object、材质信息整合起来，传到GPU的Buffer中。这里做法很不正式还是偷懒来处理。当然也可以用Compute Shader来更新Page数据到GPUBuffer中。</p><pre><code>    public static List&lt;NaniteRenderer&gt; renderers = new List&lt;NaniteRenderer&gt;();
    privatestatic SceneObject[] gpuObjects = new SceneObject[2048];
    //cluster -&gt; part -&gt; page
    publicstruct SceneObject
    {
        publicint naniteMeshID;
        public Matrix4x4 localToWorldMatrix;
        publicint materialIDOffset;
    }
    publicstruct NaniteRes
    {
        public Vector4 boundingSphere;
        publicint partIndex;
        publicint partCount;
    }

unsafe static void UpdateRenderList()
    {
         if(renderers.Count == 0)
            return;
        //object update
        if (renderers.Count &gt; gpuObjects.Length)
        {
            gpuObjects = new SceneObject[Mathf.NextPowerOfTwo(renderers.Count)];
        }

        objectCount = 0;
        maxPartCount = 0;
        naniteMeshes.Clear();
        materialList.Clear();
        List&lt;int&gt; materialIndices = new List&lt;int&gt;();
        for (int i = 0; i &lt; renderers.Count; i++)
        {
           var renderer = renderers[i];
           var nMesh = renderer.naniteMesh;
            foreach (var p in nMesh.pageArray)
           {
               maxPartCount += p.parts.Length;
               maxClusterCount += p.clusterArray.Length;
           }

           SceneObject obj = new SceneObject();
           obj.localToWorldMatrix = renderer.transform.localToWorldMatrix;
            //mesh index
           int index = naniteMeshes.IndexOf(nMesh);
           if (index &lt; 0)
           {
               index = naniteMeshes.Count;
               naniteMeshes.Add(nMesh);
           }
           obj.naniteMeshID = index;
           //mat indexs
           obj.materialIDOffset = materialIndices.Count;
           for (int m = 0; m &lt; renderer.materials.Length; m++)
           {
               var mat = renderer.materials[m];
               int matIndex = materialList.IndexOf(mat);
               if (matIndex &lt; 0)
               {
                   matIndex = materialList.Count;
                   materialList.Add(mat);
               }
               materialIndices.Add(matIndex);
           }
           gpuObjects[i] = obj;
           renderer.transformChanged = false;
           objectCount++;
        }

        if(candidateClusterBuffer!=null)
            candidateClusterBuffer.Dispose();
        candidateClusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, maxClusterCount *2, sizeof(int));

        if(visibleClusterBuffer != null)
            visibleClusterBuffer.Dispose();
        visibleClusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,maxClusterCount *2, sizeof(int));

        if (objectsBuffer != null)
            objectsBuffer.Dispose();
        objectsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, objectCount, sizeof(SceneObject));
        objectsBuffer.SetData(gpuObjects,0,0,objectCount);

        if(visObjectsBuffer !=null)
            visObjectsBuffer.Dispose();
        visObjectsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,objectCount, sizeof(int));

        int vertCount = 0;
        List&lt;NaniteCluster&gt; tempClusters = new List&lt;NaniteCluster&gt;(2048);
        List&lt;NaniteMeshPart&gt; tempParts = new List&lt;NaniteMeshPart&gt;(2048);
        List&lt;NaniteRes&gt; naniteRes = new List&lt;NaniteRes&gt;(2048);
        List&lt;int&gt; tempIndices = new List&lt;int&gt;(2048 * 100);
        List&lt;float&gt; vertexDataList = new List&lt;float&gt;();
        //load page
        for (int nID = 0; nID &lt; naniteMeshes.Count; nID++)
        {
            NaniteRes res = new NaniteRes();
            var nMesh = naniteMeshes[nID];
            //填充到GPU
            var pages = nMesh.pageArray;
            res.partIndex = tempParts.Count;
            res.partCount = 0;
            res.boundingSphere = nMesh.boundingSphere;
            for (int p = 0; p &lt; pages.Length; p++)
            {
                var page = pages[p];
                var parts = page.parts;
                int vertOffset = vertCount;
                int indicesOffset = tempIndices.Count;
                int clusterOffset = tempClusters.Count;

                //add all cluster
                var clusters = page.clusterArray;
                for (int c = 0; c &lt; clusters.Length; c++)
                {
                    var cluster = clusters[c];
                    cluster.indiceIndex += indicesOffset;
                    cluster.vertexOffset = vertOffset;
                    tempClusters.Add(cluster);
                }

                //add all part
                for (int partIndex = 0; partIndex &lt; parts.Length; partIndex++)
                {
                    var part = parts[partIndex];
                    part.ClusterStart += clusterOffset;
                    tempParts.Add(part);
                    res.partCount++;
                }

                //add page data
                tempIndices.AddRange( page.indiceArray);
                vertexDataList.AddRange(page.vertexData);
                vertCount += page.vertexCount;
            }
            naniteRes.Add(res);
        }

        //TODO GPU Update Buffer
        if (naniteResBuffer != null)
            naniteResBuffer.Dispose();
        naniteResBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, naniteRes.Count, sizeof(NaniteRes));
        naniteResBuffer.SetData(naniteRes);

        if (partsBuffer != null)
            partsBuffer.Dispose();
        partsBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,tempParts.Count, sizeof(NaniteMeshPart));
        partsBuffer.SetData(tempParts);

        if (clusterBuffer != null)
            clusterBuffer.Dispose();
        clusterBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured, tempClusters.Count, sizeof(NaniteCluster));
        clusterBuffer.SetData(tempClusters);


        if (indiceseBuffer != null)
            indiceseBuffer.Dispose();
        indiceseBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Raw, tempIndices.Count, sizeof(int));
        indiceseBuffer.SetData(tempIndices);

        if(materialIndexBuffer!=null)
            materialIndexBuffer.Dispose();
        materialIndexBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Structured,materialIndices.Count, sizeof(int));
        materialIndexBuffer.SetData(materialIndices);

        if(vertexDataBuffer!=null)
            vertexDataBuffer.Dispose();
        vertexDataBuffer = new GraphicsBuffer(GraphicsBuffer.Target.Raw, vertexDataList.Count,sizeof(float));
        vertexDataBuffer.SetData(vertexDataList);
    }

    //input object ID =&gt; 
    public unsafe static void UpdateNaniteScene()
    {
        if (renderListDirty)
        {
            UpdateRenderList();
           // UpdateRenderListGPU();
            renderListDirty = false;
        }

       for (int i = 0; i &lt; renderers.Count; i++)
       {
           var renderer = renderers[i];
           if (renderer.transformChanged)
           {
               gpuObjects[i].localToWorldMatrix = renderer.transform.localToWorldMatrix;
               renderer.transformChanged = false;
               transformDirty = true;
           }
       }

       if (objectsBuffer != null &amp;&amp; transformDirty)
           objectsBuffer.SetData(gpuObjects, 0, 0, objectCount);
    }</code></pre><p><strong>5. 剔除</strong><br/>这时离线时候已经把Clusters扁平化到数组中了，这些Clusters是可以并行进行剔除的，巧妙之处是他记录了父级的误差和自己的误差，当我们传入误差系数时候就可以独立地判断自己是否被剔除，而和上下级无关。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557838" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557839" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557840" alt="" title="" loading="lazy"/></p><p>先从CPU发起剔除Compute Shader的Dispatch。这里因为组织数据时候就知道了所有Object最大的Parts/Cluster数量，所以直接用这个数去Dispatch了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557841" alt="" title="" loading="lazy"/></p><p>Objects剔除：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557842" alt="" title="" loading="lazy"/></p><p>根据Object找到NaniteMesh的Parts进行Culling：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557843" alt="" title="" loading="lazy"/></p><p>ClustersCulling：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557844" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557845" alt="" title="" loading="lazy"/></p><p><strong>6. 软光栅</strong><br/>略。</p><p><strong>7. VisibilityBuffer</strong><br/>VBuffer主要用来减少Overdraw，着色器直接输出InstanceID、ClusterID、材质ID。然后用这个VBuffer来计算顶点数据来着色。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557846" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557847" alt="" title="" loading="lazy"/></p><p>这个得益于GPUDriven的好处，一个DrawProceduralIndirect就可以绘制所有物体了：<br/>一次DrawProceduralIndirect绘制多个物体<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557848" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557849" alt="" title="" loading="lazy"/></p><p>VBuffer存哪些属性，多少位，都是工程细节这里就不考究了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557850" alt="" title="" loading="lazy"/></p><p><strong>8. 着色</strong><br/>有了VBuffer就需要逐材质进行绘制，原文是材质ID分Tile组合IndirectDraw画Quad的思想。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557851" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557852" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557853" alt="" title="" loading="lazy"/></p><p>需要注意一下这里VBuffer通过三角重心插值求出的UV是不能直接采样贴图的，因为DDXY不对，所以需求重新计算，计算的代码放下面。并且利用SampleGrad（samplerName, coord2, dpdx, dpdy）来采样。</p><pre><code>uint MurmurMix(uint Hash)
{
    Hash ^= Hash &gt;&gt; 16;
    Hash *= 0x85ebca6b;
    Hash ^= Hash &gt;&gt; 13;
    Hash *= 0xc2b2ae35;
    Hash ^= Hash &gt;&gt; 16;
    return Hash;
}
float3 IntToColor(uint Index)
{
    uint Hash = MurmurMix(Index);

    float3 Color = float3
    (
        (Hash &gt;&gt; 0) &amp; 255,
        (Hash &gt;&gt; 8) &amp; 255,
        (Hash &gt;&gt; 16) &amp; 255
    );

    return Color * (1.0f / 255.0f);
}

struct FBarycentrics
{
    float3 Value;
    float3 Value_dx;
    float3 Value_dy;
};

float2 Lerp(float2 Value0, float2 Value1, float2 Value2, FBarycentrics Barycentrics, out float2 dxy)
{
    float2 Value = Value0 * Barycentrics.Value.x + Value1 * Barycentrics.Value.y + Value2 * Barycentrics.Value.z;
    dxy.x = Value0 * Barycentrics.Value_dx.x + Value1 * Barycentrics.Value_dx.y + Value2 * Barycentrics.Value_dx.z;
    dxy.y = Value0 * Barycentrics.Value_dy.x + Value1 * Barycentrics.Value_dy.y + Value2 * Barycentrics.Value_dy.z;

    return Value;
}

/** Calculates perspective correct barycentric coordinates and partial derivatives using screen derivatives. */
FBarycentrics CalculateTriangleBarycentrics(float2 PixelClip, float4 PointClip0, float4 PointClip1,
                                            float4 PointClip2, float2 ViewInvSize)
{
    FBarycentrics Barycentrics;
    PixelClip.y = 1 - PixelClip.y;
    PixelClip.xy = PixelClip.xy * 2 - 1;
    const float3 RcpW = rcp(float3(PointClip0.w, PointClip1.w, PointClip2.w));
    const float3 Pos0 = PointClip0.xyz * RcpW.x;
    const float3 Pos1 = PointClip1.xyz * RcpW.y;
    const float3 Pos2 = PointClip2.xyz * RcpW.z;

    const float3 Pos120X = float3(Pos1.x, Pos2.x, Pos0.x);
    const float3 Pos120Y = float3(Pos1.y, Pos2.y, Pos0.y);
    const float3 Pos201X = float3(Pos2.x, Pos0.x, Pos1.x);
    const float3 Pos201Y = float3(Pos2.y, Pos0.y, Pos1.y);

    const float3 C_dx = Pos201Y - Pos120Y;
    const float3 C_dy = Pos120X - Pos201X;

    const float3 C = C_dx * (PixelClip.x - Pos120X) + C_dy * (PixelClip.y - Pos120Y);
    // Evaluate the 3 edge functions
    const float3 G = C * RcpW;

    constfloat H = dot(C, RcpW);
    constfloat RcpH = rcp(H);

    // UVW = C * RcpW / dot(C, RcpW)
    Barycentrics.Value = G * RcpH;

    // Texture coordinate derivatives:
    // UVW = G / H where G = C * RcpW and H = dot(C, RcpW)
    // UVW' = (G' * H - G * H') / H^2
    // float2 TexCoordDX = UVW_dx.y * TexCoord10 + UVW_dx.z * TexCoord20;
    // float2 TexCoordDY = UVW_dy.y * TexCoord10 + UVW_dy.z * TexCoord20;
    const float3 G_dx = C_dx * RcpW;
    const float3 G_dy = C_dy * RcpW;

    constfloat H_dx = dot(C_dx, RcpW);
    constfloat H_dy = dot(C_dy, RcpW);

    Barycentrics.Value_dx = (G_dx * H - G * H_dx) * (RcpH * RcpH) * (2.0f * ViewInvSize.x);
    Barycentrics.Value_dy = (G_dy * H - G * H_dy) * (RcpH * RcpH) * (-2.0f * ViewInvSize.y);

    return Barycentrics;
}</code></pre><p>到这里其实基本完成了，利用IntToColor函数，可以对ClustersID或者IndexID对三角形或Cluster进行可视化。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557854" alt="" title="" loading="lazy"/></p><blockquote><h3><strong>三、总结</strong></h3></blockquote><p>不得不说Nanite技术真是太强大了，但是也有很多工程细节需要处理，本文只是实现了其中一小部分。整体像是处理图片的Mipmap过程。</p><p><strong>参考</strong></p><p><a href="https://www.bilibili.com/video/BV17G4y1x7VX/?spm_id_from=333.337.search-card.all.click&amp;vd_source=07d4f665c85998941c935676c2e50d81" target="_blank">22.GPU驱动的几何管线-nanite (Part 2) | GAMES104-现代游戏引擎：从入门到实践</a></p><p><a href="https://www.bilibili.com/video/BV1MP4y1a7Hh/?spm_id_from=333.1387.search.video_card.click&amp;vd_source=07d4f665c85998941c935676c2e50d81" target="_blank">[UnrealCircle]Nanite技术简介 | Epic Games China 王祢</a></p><p><a href="https://link.segmentfault.com/?enc=OiCon8HoejdpYcHdz0%2F47g%3D%3D.MgJ2RkInLFJQ9U%2FMSDyTWmYoi1lSusaqkg3q2ydeLaMPDjCPx0DdCgRi%2FdtFNoLtlCBQKj5N0hl4hhGa0HusEr8SzVDYFanO0uRkXDXey5g5IqQ%2FfF7k0noaH%2FXMtMRy" rel="nofollow" target="_blank">Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf</a></p><p><a href="https://link.segmentfault.com/?enc=6IMKalDSFCX2oRp0RdzeQg%3D%3D.4%2BEQEgpYdQXsuf3N2aGJRkU332rytDQd1bst4NYZ3mQmlhrRDdVwSajdaULXn1gFAWvx9upWpXHI%2F%2F7R%2BOS8mg%3D%3D" rel="nofollow" target="_blank">Nanite-GPU-Driven</a></p><p>UE5 Nanite源码入口：<br/>Engine\Source\Runtime\Renderer\Private\Nanite\NaniteCullRaster.cpp  <strong>（渲染流程入口）</strong><br/>Engine\Shaders\Private\Nanite\ <strong>（GPU的Shader入口）</strong><br/>Engine\Source\Developer\NaniteBuilder\Private\ <strong>（离线生成Nanite资源入口）</strong></p><hr/><p>这是侑虎科技第1939篇文章，感谢作者傻头傻脑亚古兽供稿。欢迎转发分享，未经作者授权请勿转载。如果您有任何独到的见解或者发现也欢迎联系我们，一起探讨。（QQ群：793972859）</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=1isfSoQj5tZS6PcAUj2ewg%3D%3D.zIAiK6Leeh2UdiKdAG8BEsg25FqvnlySwnM24Hztwvr1%2BoaSDLjxRv1OFFhyhoVUdFE47UiPuDk1p98DoxVsoA%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=Kpbq3Xj6qrF1D6IR2NcDsA%3D%3D.C3GG0nh8pNdzmzIcDO5HrqKrUdFYIwv%2FMZKlUV%2BAzNgVxV1sVSCDfRh11%2BCh3j3nkXHe97eUMkKn6tAMm18Txw%3D%3D" rel="nofollow" target="_blank">https://www.zhihu.com/people/tian-cai-ya-gu-shou</a></p><p>再次感谢傻头傻脑亚古兽的分享，如果您有任何独到的见解或者发现也欢迎联系我们，一起探讨。（QQ群：793972859）</p>]]></description></item><item>    <title><![CDATA[从 0 到 1 的智能体搭建之路 智能猫 ]]></title>    <link>https://segmentfault.com/a/1190000047557937</link>    <guid>https://segmentfault.com/a/1190000047557937</guid>    <pubDate>2026-01-22 12:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>🚀 快速回答 (Golden Answer)</h3><p>从 0 到 1 搭建智能体的核心逻辑是 “明确需求 → 选对工具 → 配置闭环 → 测试优化”：无需复杂编程，优先用零代码 / 低代码工具（如 Coze、LangGraph），先定义 “智能体要解决的具体任务”（如自动化办公、设计辅助），再通过 “设定角色 → 拆解任务 → 配置工具 → 添加反思逻辑” 完成搭建，最终通过测试迭代优化效果。核心是 “让智能体精准匹配需求”，而非追求技术复杂度。</p><h2>一、前置认知：先搞懂 “搭建智能体” 的核心前提</h2><h3>1.1 搭建智能体的核心目标：解决 “具体问题”</h3><p>智能体的核心价值是 “自主完成复杂任务”，搭建前必须明确 “它要帮你做什么”，避免盲目搭建。常见落地场景：</p><ul><li>个人场景：自动化周报生成、文献整理助手、学习笔记总结、购物比价监控；</li><li>职场场景：客户咨询智能客服、销售数据自动分析、市场调研报告生成、设计批量出图；</li><li>垂直场景：电商运营助手（商品上架 + 文案生成）、科研辅助（数据检索 + 分析）、教育答疑（学科知识点梳理）。</li></ul><h3>1.2 搭建智能体的核心逻辑：“感知 - 规划 - 行动 - 反思” 闭环</h3><p>无论用哪种工具，智能体的底层逻辑都是这四个环节的循环，搭建的本质就是 “配置这四个环节的规则”：</p><ul><li>感知：让智能体 “接收信息”（如用户需求、外部数据、工具反馈）；</li><li>规划：让智能体 “拆解任务”（如 “生成销售报告” 拆解为 “收集数据 → 清洗数据 → 分析 → 排版”）；</li><li>行动：让智能体 “执行步骤”（如调用 Excel、API 接口、设计工具完成具体操作）；</li><li>反思：让智能体 “修正错误”（如数据缺失时重新收集，格式错误时自动调整）。</li></ul><h3>1.3 零基础搭建的核心原则：“工具优先，不造轮子”</h3><p>无需从零开发大模型或底层架构，当前主流工具已提供 “可视化配置 + 现成组件”，零基础只需聚焦 “需求匹配” 和 “流程配置”，核心原则：</p><ol><li>优先选零代码工具（如 Coze、Notion AI Agent），快速验证需求；</li><li>复杂场景再用低代码工具（如 LangGraph、AutoGen），灵活适配个性化需求；</li><li>先搭建 “最小可用版本”（仅满足核心任务），再逐步添加功能。</li></ol><h2>二、工具选择：零基础必看的 “工具选型矩阵”</h2><p>不同工具的门槛、功能、适配场景差异较大，结合 “零基础友好度” 和 “落地实用性”，整理核心工具对比：</p><table><thead><tr><th>工具名称</th><th>技术门槛</th><th>核心优势</th><th>适配场景</th><th>学习成本</th></tr></thead><tbody><tr><td>Coze（扣子）</td><td>零代码</td><td>可视化配置，插件生态丰富（支持 Excel、数据库、设计工具等），可直接发布为小程序 / APP</td><td>个人助手、职场自动化、客服机器人</td><td>低（1-2 小时掌握基础配置）</td></tr><tr><td>Notion AI Agent</td><td>零代码</td><td>与文档深度融合，支持笔记整理、报告生成、任务管理，操作简单直观</td><td>学习助手、文献整理、文档自动化</td><td>极低（熟悉 Notion 即可上手）</td></tr><tr><td>LangGraph</td><td>低代码（Python 基础）</td><td>状态控制极强，支持复杂循环逻辑，适配高定制化任务</td><td>科研辅助、复杂数据分析、自动化办公流</td><td>中（需掌握基础 Python 和 Prompt 技巧）</td></tr><tr><td>AutoGen</td><td>低代码（Python 基础）</td><td>支持多智能体协作，角色分工明确，降低复杂任务的配置难度</td><td>软件工程、内容生产流水线、多步骤商业分析</td><td>中（需理解多智能体协同逻辑）</td></tr><tr><td>Make（原 Integromat）</td><td>零代码</td><td>专注工具集成，支持 1000 + 款软件对接，擅长自动化工作流串联</td><td>跨平台自动化（如微信 + Excel + 邮件协同）</td><td>低（重点学习工具对接逻辑）</td></tr></tbody></table><p>💡 零基础优先推荐：Coze（功能全、生态完善）或 Notion AI Agent（简单直观）；若需处理复杂任务，再学习 LangGraph（低代码门槛）。</p><h2>三、分步实操：用 Coze 从零搭建 “自动化周报生成智能体”（零代码案例）</h2><p>以 “自动收集 Excel 数据 → 生成周报 → 排版导出” 为核心任务，用 Coze 完成搭建，全程可视化操作，10 分钟即可完成基础版本：</p><h3>3.1 第一步：明确需求与角色设定</h3><ol><li>核心需求：用户上传 Excel 销售数据后，智能体自动计算核心指标（销售额、增长率、Top3 产品），生成结构化周报，支持 Word 导出；</li><li>角色设定：在 Coze 后台 “角色定义” 中填写 ——“你是职场销售周报生成助手，擅长从 Excel 数据中提取核心信息，生成逻辑清晰、格式规范的周报，语言正式专业”；</li><li>补充提示：添加 “周报格式要求”（如包含 “本周概况、核心数据、趋势分析、下周计划” 模块），让智能体输出更精准。</li></ol><h3>3.2 第二步：配置 “工具”（让智能体具备执行能力）</h3><p>智能体需要对接 Excel 和 Word 工具，才能完成数据读取和导出，操作步骤：</p><ol><li>在 Coze “插件市场” 中搜索 “Excel 解析” 和 “Word 导出” 插件，点击 “启用”；</li><li>配置插件权限：授权 Coze 读取用户上传的 Excel 文件（仅读取权限，保障数据安全）；</li><li>测试工具连通性：上传一份测试 Excel 数据，点击 “测试插件”，确认智能体能正常提取数据。</li></ol><h3>3.3 第三步：设计 “任务流程”（拆解执行步骤）</h3><p>在 Coze “流程设计” 模块，用可视化拖拽配置任务步骤，核心流程：</p><ol><li>触发条件：用户上传 Excel 文件并发送 “生成周报” 指令；</li><li>步骤 1：调用 “Excel 解析” 插件，提取数据（销售额、产品名称、日期等）；</li><li>步骤 2：智能体计算核心指标（本周总销售额、环比增长率、Top3 热销产品）；</li><li>步骤 3：按照预设格式生成周报文本；</li><li>步骤 4：调用 “Word 导出” 插件，生成周报文件并反馈给用户。</li></ol><h3>3.4 第四步：添加 “反思逻辑”（让智能体能修正错误）</h3><p>为避免数据缺失或格式错误，添加简单反思规则：</p><ol><li>在 “流程设计” 中添加 “判断节点”：若 Excel 数据缺失关键字段（如 “销售额”），则自动向用户发送 “请补充包含销售额字段的 Excel 文件”；</li><li>添加 “格式校验”：生成周报到导出前，自动检查是否包含预设的 4 个模块，缺失则补充完善。</li></ol><h3>3.5 第五步：测试与发布</h3><ol><li>测试验证：上传测试 Excel 数据，发送 “生成周报” 指令，查看智能体是否能正确完成全流程，重点检查数据计算准确性和格式规范性；</li><li>优化迭代：若存在格式混乱，补充 “周报格式细则”（如字体、行距、标题层级）；若数据计算错误，调整指标计算规则；</li><li>发布使用：测试通过后，点击 “发布”，可生成小程序 / 网页链接，直接在工作中使用。</li></ol><h2>四、进阶优化：让智能体更 “好用” 的 3 个关键技巧</h2><h3>4.1 精准 Prompt 优化：提升输出质量</h3><p>在角色定义中补充 “具体约束”，而非模糊描述，示例：</p><ul><li>差 Prompt：“生成专业的周报”；</li><li>好 Prompt：“生成销售周报，包含本周概况（30 字内）、核心数据（表格呈现）、趋势分析（200 字内）、下周计划（3 条核心动作），语言正式，避免口语化，数据保留 2 位小数”。</li></ul><h3>4.2 个性化适配：对接个人 / 企业知识库</h3><p>若智能体需要适配特定业务（如公司产品知识、个人工作习惯），可在 Coze 中上传 “知识库”（如公司产品手册、个人工作模板），让智能体学习后输出更贴合需求的结果。</p><h3>4.3 多智能体协作：解决复杂任务</h3><p>对于 “市场调研 → 数据分析 → 报告生成” 这类复杂任务，可搭建 “多智能体团队”：</p><ul><li>调研智能体：负责收集市场数据；</li><li>分析智能体：负责数据计算与趋势分析；</li><li>撰写智能体：负责生成最终报告； 在 Coze 中配置 “智能体间通信规则”，让它们协同完成任务，提升效率。</li></ul><h2>五、避坑指南：零基础搭建常见问题与解决方案</h2><table><thead><tr><th>常见问题</th><th>核心原因</th><th>解决方案</th></tr></thead><tbody><tr><td>智能体输出不符合预期（如格式混乱）</td><td>角色定义模糊，缺乏明确约束</td><td>补充具体的输出格式、语言风格、内容模块要求，用示例引导（如 “参考以下示例格式生成：【本周概况】XXX”）</td></tr><tr><td>智能体无法完成复杂任务（如数据计算错误）</td><td>任务拆解不细致，工具配置不当</td><td>将复杂任务拆分为更细的原子步骤，检查工具参数配置（如数据字段匹配），添加人工校验节点</td></tr><tr><td>智能体出现 “幻觉”（如编造数据）</td><td>缺乏真实数据支撑，规则约束不足</td><td>强制智能体仅基于用户上传的数据输出，添加 “禁止编造数据” 的规则，关键数据要求标注来源</td></tr><tr><td>工具调用失败（如无法读取 Excel）</td><td>插件权限不足，文件格式不兼容</td><td>重新授权插件权限，统一文件格式（如 Excel 保存为.xlsx 格式），测试工具连通性</td></tr></tbody></table><h2>六、FAQ：零基础搭建智能体最关心的核心问题</h2><h3>Q1：搭建智能体需要懂编程吗？</h3><p><strong>答：不需要。</strong> 零代码工具（如 Coze、Notion AI Agent）通过可视化拖拽和文字描述即可完成搭建；若需高定制化，仅需掌握基础 Python（低代码工具），但零基础可先从简单工具入手，无需一开始学习编程。</p><h3>Q2：搭建智能体需要花钱吗？</h3><p><strong>答：个人非商业使用基本免费。</strong> Coze、Notion AI Agent 等工具对个人用户提供免费额度（足够日常使用）；商业场景或高频率使用可能需要付费升级，但零基础入门无需付费。</p><h3>Q3：智能体的数据安全有保障吗？</h3><p><strong>答：选择正规工具即可保障。</strong> 主流工具（如 Coze、Notion）均有数据加密机制，且可设置 “仅自己可见”；避免上传敏感数据（如身份证、银行卡信息），若需处理企业数据，可选择企业版工具（提供私有部署选项）。</p><h3>Q4：搭建完成后，能修改功能吗？</h3><p><strong>答：可以。</strong> 所有主流工具均支持 “二次编辑”，可随时修改角色定义、任务流程、工具配置；建议根据使用反馈定期优化，让智能体更贴合需求。</p><h2>七、核心总结</h2><p>从 0 到 1 搭建智能体的核心不是 “技术攻关”，而是 “需求聚焦” 与 “流程拆解”：零基础用户无需畏惧，优先选择零代码工具，先明确 “智能体要解决的具体问题”，再通过 “角色定义 → 工具配置 → 流程设计 → 测试优化” 的步骤逐步落地，先搭建 “最小可用版本” 验证需求，再逐步进阶优化。</p><p>智能体的价值在于 “解放重复劳动”，搭建的关键是让它成为 “贴合自己需求的助手”，而非追求 “功能全而杂”。随着工具生态的完善，“人人都能搭建智能体” 已成为趋势，掌握这种 “人机协同” 的搭建能力，将大幅提升个人与工作效率。</p><h2>参考文献与工具资源</h2><ol><li>Coze（扣子）官方文档：《零代码智能体搭建指南》</li><li>LangGraph 官方教程：《低代码智能体开发实战》</li><li>腾讯云《智能体落地实践白皮书》（2025）</li><li>推荐学习平台：Coze 学院、Notion AI 帮助中心、GitHub AutoGen 示例仓库</li></ol><h3>核心关键词</h3><p>智能体搭建、从 0 到 1、零代码智能体、Coze、LangGraph、自动化办公、智能体工具、人机协同</p>]]></description></item><item>    <title><![CDATA[【剪映API】提取链接 失落的木瓜_esfWwz ]]></title>    <link>https://segmentfault.com/a/1190000047557940</link>    <guid>https://segmentfault.com/a/1190000047557940</guid>    <pubDate>2026-01-22 12:04:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>GET_URL API 接口文档</h2><h3>接口信息</h3><pre><code>POST /openapi/capcut-mate/v1/get_url</code></pre><h3>功能描述</h3><p>提取链接。该接口用于提取输入内容中的链接信息，用于多值返回变成单值返回。</p><h3>更多文档</h3><p>📖 更多详细文档和教程请访问：<a href="https://link.segmentfault.com/?enc=LhsUzGp46JpRFJzpY0uq5w%3D%3D.AWQi7n0Reo6FGV0ks9d811lAwlKYwHMyvTyxtk2JycE%3D" rel="nofollow" target="_blank">https://docs.jcaigc.cn</a></p><h3>请求参数</h3><pre><code class="json">{
  "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
}</code></pre><h4>参数说明</h4><table><thead><tr><th>参数名</th><th>类型</th><th>必填</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>output</td><td>string</td><td>✅</td><td>-</td><td>提取内容</td></tr></tbody></table><h4>参数详解</h4><h5>output</h5><ul><li><strong>类型</strong>: string</li><li><strong>说明</strong>: 需要提取链接的内容</li><li><strong>示例</strong>: <code>"[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""</code></li></ul><h3>响应格式</h3><h4>成功响应 (200)</h4><pre><code class="json">{
  "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
}</code></pre><h4>响应字段说明</h4><table><thead><tr><th>字段名</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>output</td><td>string</td><td>提取结果</td></tr></tbody></table><h4>错误响应 (4xx/5xx)</h4><pre><code class="json">{
  "detail": "错误信息描述"
}</code></pre><h3>使用示例</h3><h4>cURL 示例</h4><h5>1. 基本使用</h5><pre><code class="bash">curl -X POST https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/get_url \
  -H "Content-Type: application/json" \
  -d '{
    "output": "[魂牵梦萦https://sf.com；中国人https://jcaigc.cn],\"[]\""
  }'</code></pre><h3>错误码说明</h3><table><thead><tr><th>错误码</th><th>错误信息</th><th>说明</th><th>解决方案</th></tr></thead><tbody><tr><td>400</td><td>output是必填项</td><td>缺少output参数</td><td>提供有效的output参数</td></tr><tr><td>500</td><td>提取链接失败</td><td>内部处理错误</td><td>联系技术支持</td></tr></tbody></table><h3>注意事项</h3><ol><li><strong>参数要求</strong>: output参数为必填项</li><li><strong>返回值</strong>: 当前版本直接返回输入的内容，不做额外处理</li></ol><h3>工作流程</h3><ol><li>验证必填参数（output）</li><li>调用服务层处理业务逻辑</li><li>返回处理结果</li></ol><h3>相关接口</h3><ul><li><a href="./create_draft.md" target="_blank">创建草稿</a></li></ul><hr/><p>📚 <strong>项目资源</strong>  <br/><strong>GitHub项目名称</strong>: capcut-mate</p>]]></description></item><item>    <title><![CDATA[智慧能源升维，实时云渲染重构管理新视角 点量实时云渲染 ]]></title>    <link>https://segmentfault.com/a/1190000047557984</link>    <guid>https://segmentfault.com/a/1190000047557984</guid>    <pubDate>2026-01-22 12:03:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnH94" alt="" title=""/></p><p>2026年初，工业和信息化部等五部门联合印发的《工业绿色微电网建设与应用指南（2026—2030年）》，为能源的数字化转型铺设了清晰的政策轨道。目前，全国已投入运行的工业绿色微电网项目超过300个，它们正从试点走向规模化。智慧能源的管理，正从传统的报表与经验，向一个全域可视、实时交互、智能决策的数字世界加速演进。</p><h2>01 政策引领，智慧能源按下“加速键”</h2><p>国家层面正在以前所未有的力度，推进能源系统的数字化转型。《工业绿色微电网建设与应用指南》明确将智慧能源管控系统，列为绿色微电网建设的核心内容之一。其目标是构建一个集成光伏、风电、储能、氢能等多能互补，并实现与大电网友好互动的综合能源系统。未来的能源管理，必须是数字化、可视化、智能化的。</p><h2>02 现实挑战：智慧能源的进阶痛点</h2><p>然而，理想蓝图在落地时，却面临着一系列棘手的现实挑战。当前的核心痛点可以概括为：“看不见、摸不清、调不动”。</p><ul><li>状态“看不见”：一个现代化的能源场站或微电网，包含成千上万的设备与传感器。传统分散的图表和报表，让管理者难以在短时间内掌握全局状态，如同“开盲盒”。</li><li>逻辑“摸不清”：SCADA、PLC、IoT等系统数据格式各异，形成信息壁垒。当发生故障时，运维人员需要跨多个系统排查，难以快速穿透网络层、服务器层、应用层，精准定位根源。</li><li>协同“调不动”：为了实现对复杂系统的精细化管理，数字孪生技术正成为标配。但这些高精度三维模型对终端电脑的图形性能要求极高，导致许多一线运维人员无法流畅使用，远程协同和移动办公更是困难重重。</li></ul><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnIai" alt="" title="" loading="lazy"/></p><h2>03 破局关键：实时云渲染让智慧能源“轻装上阵”</h2><p>点量云流实时云渲染其核心原理是将海量三维模型的计算与渲染任务放在云端强大的服务器集群上完成，前端终端（无论是高性能工作站、普通笔记本，还是平板电脑）只需通过网页或轻量客户端，接收经过云端处理的视频流即可进行操作。</p><p>相较于传统网页3D效果受模型大小限制，点量云流实时云渲染能够在不消耗终端硬件性能的情况下，实现无需等待加载、即时打开与实时交互的体验。</p><p>这一转变带来了三个根本性改变：</p><ul><li>终端解放：运维人员不再受本地硬件性能束缚，用一台普通办公电脑或移动设备，就能流畅操控大型能源场（如风、电、煤等）的实景数字孪生模型。</li><li>数据安全：所有核心模型与数据始终保存在云端服务器，前端只传输视频流，从根本上杜绝了三维数字资产通过终端泄露的风险。</li><li>高效协同：不同地域的专家可以同时接入同一个三维场景，基于统一的、可视化的模型进行会诊、标注和决策，极大提升了跨团队协作效率。</li></ul><h2>04 实战图景：可视化如何重塑能源管理</h2><p>技术的价值，最终需要体现在真实的场景中。当实时云渲染技术卸下了硬件的重担，一系列曾经难以落地的应用，正悄然成为智慧能源管理的日常。<br/>1、运维：从“被动响应”到“主动预警”<br/>基于高精度数字孪生模型，系统能深度融合实时数据与AI算法，提前洞察设备亚健康状态，精准预测潜在故障。运维策略由此从紧急抢修的“被动处置”，转向计划性干预的“主动预防”。而这一切得以实现的关键，在于实时云渲染技术让这套复杂的三维预警系统，得以在各级管理中心的普通电脑上便捷访问与联动，使预防性维护真正触手可及。</p><p>2、管理：从“分散孤岛”到“全域一张图”<br/>传统管理中，物理设备、网络流量、业务数据往往分散于不同系统，形成信息壁垒。如今，通过实时云渲染技术，这些要素被整合进一个统一的动态三维界面，生成能源系统的“全景作战图”。结合云推流能力，无论是集控中心的大屏，还是巡检人员的移动终端，都能获得一致、流畅且可交互的全局视角，真正实现了“全域可视、全局可控”的集中化管控。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnIap" alt="" title="" loading="lazy"/></p><p>3、效率：从“人工跑腿”到“远程会诊”<br/>对于地处偏远的风电场或水电站，专家亲赴现场耗时费力。通过点量云渲染平台，专家在千里之外即可指挥实景复刻的虚拟现场，通过三维模型远程指导一线人员排查故障，将响应时间从数小时大幅压缩至分钟级。这不仅是距离的缩短，更意味着高精度模型得以在PC、平板等多终端安全、流畅地访问，显著提升了跨地域协同、应急指挥与人员培训的效能。</p><p>随着实时云渲染技术与能源体系的深度融合，智慧能源的管理模式将迎来根本性变革。高精度的能源系统数字孪生将不再受限于本地硬件，而是通过云推流技术，成为在任何终端均可流畅访问与协同操作的“活地图”。从宏观调度到微观运维，决策都将基于一张全域同步、实时可视、深度交互的动态图谱。</p><p>这不仅是技术的叠加，更是从“经验驱动”到“全景数据驱动”的智慧跃迁。一个更高效、更透明、更坚韧的能源时代，正借由这条“云端高速路”，清晰地向我们驶来。</p>]]></description></item><item>    <title><![CDATA[我的“Python海龟”诞生了一枚金蛋，孵出的却是“精灵” 李兴球 ]]></title>    <link>https://segmentfault.com/a/1190000047557994</link>    <guid>https://segmentfault.com/a/1190000047557994</guid>    <pubDate>2026-01-22 12:02:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>时光要追溯到2010年。在那之前，我一直用Visual Basic语言编写3D小游戏，乐在其中。那时我觉得Basic已经是非常简单的计算机语言了，但心中始终有一个疑问：有没有比Basic更友好、更适合少儿的编程语言呢？</p><p>于是我开始在网络上持续寻找。果然，不久后我遇到了Scratch 1.4版——那只来自美国麻省理工学院的小猫，一下子抓住了我的心。从此，我踏上了少儿编程教育的道路：2013年开办了编程培训班，2015年将Python正式纳入教学体系，到2018年时，我已积撰写了相当丰富的Python教学材料。</p><p>在教学过程中，我逐渐感到Python内置的turtle（海龟作图）模块功能有些局限。于是，我打开它的源代码文件turtle.py，仔细研究其结构，并从2019年开始，基于Python turtle模块持续开发一个更强大的扩展——Python精灵模块（sprites）。这个模块的核心是一个叫做Sprite的类，它大幅增强了海龟的功能，例如实现了像素级的碰撞检测等。如今，任何人都可以通过pip install sprites来安装并使用它。</p><p>最近几年，我将更多精力投入信息学奥赛的教学中，整日“苦思冥想”各种算法难题。2025年8月，暑假班结束后，我又开始思考另一个问题：如果C++的入门教学能像Python turtle一样直观、有趣，那不就能为中国更多青少年打开编程的大门吗？</p><p>为此，我在GitHub上搜索已有的类似成果，也尝试了一些用C或C++编写的“类turtle”库，例如小熊猫C++内置的C语言海龟作图、GoC等，甚至购买了相关教材准备投入教学。但最终，我并没有采用它们。原因何在？</p><p>小熊猫C++中的海龟作图功能，作者显然缺乏Python少儿编程的教学背景。我曾联系他，希望将命令设计得接近Python turtle的风格，但毕竟不能一直麻烦别人，后来也就作罢。GoC则为了降低输入难度，将命令简化为单个或两个字符（如pen.o），其命令集较小，功能也相对有限。它主要依赖在线环境，作者并未提供独立的编辑器（早期离线版需搭配Notepad++使用）。GoC更像为信息学奥赛选拔苗子而设计的前置课程——网上甚至有人建议一、二年级就开始学习。如果你确定要走信奥路线，这或许可行；否则，并不适合普通学生。</p><p>这里涉及一个关键的教育认知问题：并非所有孩子都适合在低龄阶段接触C++。神经科学研究表明，大脑前额叶皮层（负责逻辑、规划与抽象思维）发育较晚，通常到青春期才趋于成熟。​ 有些孩子认知发展稍晚，若过早强制学习C++这类抽象程度高的语言，容易导致挫败感，甚至产生“习得性无助”。相反，在中低年级通过图形化编程（如Scratch）进行多感官、具象化的学习，能更好地刺激大脑不同区域，促进思维灵活性和创造力的发展。等到年龄增长、认知准备更充分时，再接触C++，往往事半功倍。​ 现实中，不少学生直到高中阶段才在逻辑思维上“开窍”，这恰恰说明大脑发育有其自然节奏，教育应当顺应而非违背它。</p><p>那么问题来了：对于大多数普通学生而言，如果一二年级接触图形化编程，三四年级学习Python，那么到了合适年龄，该如何顺畅地过渡到C++？市面上是否存在一套针对普通学生、能完美衔接既有体系的C++课程？或许有，但可能不公开或需付费。无论如何，我决定亲手打造一个——“金窝银窝，不如自己的草窝”。</p><p>首先面临的是技术选型。如果基于OpenGL，虽然强大，但学习成本较高；我也尝试过EasyX，并做出了原型，但因其底层控制不足而放弃；之后考虑过raylib（基于SDL2封装）和SFML，它们功能丰富，但封装程度较高，不利于我深入底层实现教育定制化的需求。最终，我选择了SDL2——这是一个工业级的跨平台库，接口相对底层，自由度大，掌控力强，正好符合我的开发理念。</p><p>于是，我以SDL2为基础，开始了漫长的开发与调试。最初叫它“C++ Sprites库”，后来正式定名为“C++精灵库”。为降低使用门槛，我还专门开发了配套的pxC++编辑器，并制作了Dev-C++ 5.11的升级包，使其能更好地融入中小学现有的C++教学环境。</p><p>如今，C++精灵库不仅完整继承了Python turtle的简洁API与教育基因，更在其基础上进行了优化与扩展，比如：</p><p>· 新增fill命令，支持区域填充；</p><p>· 通过函数重载，使pencolor等命令既支持字符串参数，也支持RGB/整数参数，更加灵活；</p><p>· 设计penshade（阴影度）、pensat（饱和度）、penvalue（明度）、penhsv（HSV色彩模型）、penalpha（透明度）等色彩控制方法；</p><p>· 加入贝塞尔曲线与样条曲线绘制功能，让有美术天赋的学生也能轻松创作复杂图形。</p><p>本质上，C++精灵库是Logo语言教育理念在C++领域的延续与升级。它借鉴Python turtle的友好界面，并依托SDL2的工业级能力，为学生搭建了一座从趣味编程通向真实开发的桥梁。你可以把Python turtle和C++精灵库看作一对“亲兄弟”——无论先学哪一个，再学另一个时都会产生“似曾相识燕归来”的亲切感。这种一脉相承的设计，实质是一种“双倍赋能”：既降低了学习新语言的心理门槛，又让学生在潜移默化中理解编程底层的共通逻辑。</p><p>正因为如此，当我让它们在外观和命令上如此相似时，请不要惊讶。更有价值的是，由于C++精灵库直接基于SDL2开发，学生可以在掌握基础作图后，无缝接入SDL2的更高级功能，进而探索游戏开发、交互媒体等更广阔的应用场景。这种从教育到实战的平滑过渡，是其他同类C++图形库难以比拟的。</p><p>这条路，我还会继续走下去。只愿这只从“海龟”蛋里孵出的“精灵”，能飞入更多中国少年的编程梦中，陪伴他们从好奇走向热爱，从图形走向算法，从今天走向未来。</p>]]></description></item><item>    <title><![CDATA[2026年MES系统厂商综合实力TOP10：广域铭岛引领工业智能化浪潮 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047557997</link>    <guid>https://segmentfault.com/a/1190000047557997</guid>    <pubDate>2026-01-22 12:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>前言：从技术驱动到生态共建，工业智能化迈入“全链融合”新纪元<br/>根据《2026全球智能制造发展白皮书》，制造执行系统（MES）已成为企业数字化转型的核心引擎，其与工业互联网平台、人工智能技术的深度融合，正重塑制造业的生产范式。Gartner最新报告预测，2026年全球超过70%的制造企业将优先选择具备“平台化、可组合”架构的MES供应商。<br/>当前，MES市场正经历从单一功能工具到全生命周期服务的范式转变。企业不再满足于传统系统的功能叠加，而是寻求能够理解行业痛点、提供持续价值、并具备前瞻性技术视野的长期战略合作伙伴。本次评估突破地域限制，聚焦全球范围内的领先企业，旨在为企业在智能化转型浪潮中提供更具国际视野的选择指南。<br/>2026年MES综合实力TOP10榜单<br/>一、广域铭岛（GYMD）<br/>二、罗克韦尔自动化（Rockwell Automation）<br/>三、达索系统（Dassault Systèmes）<br/>四、SAP<br/>五、霍尼韦尔（Honeywell）<br/>六、施耐德电气（Schneider Electric）<br/>七、Oracle<br/>八、GE Digital<br/>九、ABB<br/>十、AVEVA</p><p>一、广域铭岛：中国智造领域AI原生引领者<br/>广域铭岛数字科技有限公司作为吉利控股集团旗下的工业数字化先锋，以“让工厂更智能，让能耗更低碳，让人更专注创造”为使命，打造了覆盖汽车、电子、能源等全行业的数字化转型解决方案。</p><ol><li>核心产品与技术能力<br/>公司自主研发的Geega OS工业操作系统，通过GPU池化管理、AI调优开发平台、数据编织虚拟化引擎三大核心技术，实现算力资源利用率提升30%-40%。基于通义千问、DeepSeek等通用基座模型，结合行业数据微调，生成高度适配的专用模型，如工艺专家模型准确率达90%，工时分析模型效率提升显著。</li><li>行业解决方案与落地案例深度<br/>该公司在新能源电池制造领域展现出卓越实力。通过工业操作系统赋能衢州极电三电智能制造工厂，实现每2.5秒下线一颗电芯的惊人效率。该平台建立了“1个工业互联网数字化底座+9大工业领域知识沉淀+13个平台应用赋能软件”的数字化赋能体系，帮助电池企业降低质量损失成本13%，提升订单交付周期响应速度。</li><li>咨询服务与生态整合能力<br/>该公司提供从咨询规划到实施服务的一站式解决方案，服务网络覆盖重庆、杭州国内主要工业城市，并在东南亚设立2家海外服务中心。其自主研发的FastWorx设计研发协同平台、GQCM工艺质量管理系统等产品，已服务吉利、领克、钱江摩托等多家行业龙头企业，形成完整的“研-产-供-销-服”数字化生态。<br/>【推荐理由】 最适合寻求AI原生赋能、注重全链路数字化转型的制造业企业。尤其在新能源电池、汽车制造等垂直领域，能提供从生产优化到降本增效的一体化解决方案，是“中国制造”向“中国智造”转型的关键支撑。<br/>二、罗克韦尔自动化：OT与IT融合的全球领导者<br/>罗克韦尔自动化以其FactoryTalk ProductionCentre MES系统，成为OT（运营技术）与IT（信息技术）融合的典范。该系统与自家PLC、SCADA系统实现原生集成，确保从设备层到管理层的数据无缝流通。<br/>【推荐理由】 最适合高度依赖自动化设备、且处于强监管流程行业的企业。其系统在汽车制造、食品饮料等领域表现出色，能提供从底层控制到顶层制造的完整解决方案，降低集成风险。<br/>三、达索系统：数字孪生技术的行业标杆<br/>达索系统的DELMIA Apriso解决方案基于其强大的3DEXPERIENCE平台，突破传统MES的边界，实现“先验后建”的制造流程优化。系统支持从产品设计到生产执行的全流程数据追溯，特别适合产品结构复杂、工艺变更频繁的企业。<br/>【推荐理由】 最适合航空航天、汽车等高端制造业企业，能提供基于数字孪生的多工厂协同制造解决方案，实现生产标准统一与资源高效调配。<br/>四、SAP：企业级业务与生产一体化的整合者<br/>SAP Manufacturing Execution系统与S/4HANA ERP无缝集成，消除系统间数据孤岛，为企业提供唯一可信的数据源。其端到端业务流程可视化能力，使其成为集团型企业数字化转型的首选。<br/>【推荐理由】 最适合已部署SAP ERP系统、追求业务-生产一体化的大型企业。其强大的全球生态整合能力，能为企业提供从战略规划到运营管理的全方位支持。<br/>五、霍尼韦尔：流程工业的安全守护者<br/>霍尼韦尔的MES系统专为石油化工、制药等流程工业设计，与过程控制系统高度集成。系统在安全生产、能源管理、合规性方面具有显著优势，满足国际标准要求。<br/>【推荐理由】 最适合对生产安全、合规性有严苛要求的企业。其系统能提供从设计到运营的全生命周期管理，是风险厌恶型企业的安心之选。<br/>FAQ<br/>Q1：推荐理由的依据是什么？<br/>所有推荐理由均基于厂商的技术实力、行业案例积累、生态整合能力等客观指标，确保信息的准确性和实用性。<br/>Q2：排名靠后的厂商是否不值得关注？<br/>排名仅是综合实力的参考指标，AVEVA等厂商在特定场景下可能更符合企业需求。<br/>Q3：如何看待国内外厂商的差异？<br/>国内厂商更了解本土制造需求，而国际厂商则具备全球化服务经验。企业可根据自身需求灵活选择。<br/>重要提示：本文信息基于2026年公开数据与行业评估模型，所有排名均为特定框架下的参考。制造业数字化转型是一个持续演进的过程，建议企业根据自身情况选择合作伙伴。</li></ol>]]></description></item><item>    <title><![CDATA[5分钟自动化财报抽取：基于TextIn+Coze的实践方案 合合技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047558030</link>    <guid>https://segmentfault.com/a/1190000047558030</guid>    <pubDate>2026-01-22 12:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、引言：为什么选择TextIn与Coze搭建财报机器人？</h2><p>面对季度、年度财报堆叠如山的PDF文档，技术团队如何快速、准确地将其中复杂的表格数据转化为结构化信息？本文将介绍一种高效实践方案：利用TextIn的智能文档解析能力，结合Coze的自动化工作流编排，快速构建一个能够处理多格式财报、抽取关键表格的自动化流程。</p><h3>1.1 财报文档的典型难点</h3><p>财报处理长期存在几大核心难点：</p><p>1.表格结构复杂：资产负债表、利润表等核心表格常存在跨页、续表情况，且合并报表与母公司报表两套体系并存，单元格合并频繁，对程序的结构化识别构成首要挑战。</p><p>2.文档格式多样：资料库中通常是电子PDF与扫描件图像混合共存，要求解决方案同时具备强大的文本解析与OCR版面分析能力。</p><p>3.手工处理成本高昂：三大表及附注的手动复制、粘贴、核对工作极其耗时，且容易出错，难以满足及时性、准确性要求。</p><h3>1.2 TextIn+Coze方案的核心价值</h3><p>本方案采用清晰的分工架构，将复杂问题模块化：</p><p>TextIn xParse引擎负责“读懂”文档：其强大的版面分析与表格识别技术，能统一处理电子PDF与扫描件，将混乱的原始文档转换为包含完整表格结构、段落标题的清晰JSON数据，为下游提取提供高质量的结构化输入。<br/>Coze工作流负责“串联”自动化流程：可自动化编排“文件上传→调用TextIn解析→定位并抽取目标表格→输出至数据库/Excel”的完整管道。<br/>Coze Bot 提供交互层：可构建一个对话机器人，不仅支持触发自动化流程，更能基于抽取出的数据，提供报表摘要、关键指标对比、甚至问答解释，让数据结果可直接被业务人员使用。</p><p>这种组合将专业的文档解析、灵活的业务逻辑编排与友好的交互界面相结合，使开发者能聚焦于核心的抽取规则，快速搭建从原始文档到业务可用数据的端到端流水线。</p><h2>二、方案应用速览</h2><p><strong>工作流：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558032" alt="图片" title="图片"/></p><p><strong>输出结果：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558033" alt="图片" title="图片" loading="lazy"/></p><h2>三、架构设计</h2><h3>3.1 总体链路</h3><pre><code>用户上传财报 → Coze触发工作流 → xParse → 代码节点抽取 → 输出结构化tables


</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558034" alt="图片" title="图片" loading="lazy"/></p><pre><code>开始节点：接收用户上传的财报文件（File）。
TextIn插件节点：将财报解析为结构化JSON，核心使用result.detail（包含paragraph/table/image等元素）以及result.markdown。
代码节点：仅遍历detail，通过“表标题 → 后续表格”方式抽取三大表，并统一输出为tables{balanceSheet,incomeStatement,cashFlow}。
结束节点：将tables / debug / markdown输出给Bot，用于展示与后续问答分析。

</code></pre><h3>3.2 数据结构约定</h3><p>TextIn xParse - 插件节点的输出（result.detail / result.markdown等，详情见TextIn xParse API文档：<a href="https://link.segmentfault.com/?enc=aQBTkYe2VUg1%2BHepNFWZ5g%3D%3D.vkBS%2B4%2BOaPPYi4m6NPDiCwhlz3S6OAtlqJMflL1Z4QewWUcpiSpCtjjtLnbcvu7k" rel="nofollow" target="_blank">https://docs.textin.com/xparse/parse-getjson</a>）</p><pre><code>Response
├─ code                               # 接口状态码
├─ message                            # 状态信息
└─ result
   ├─ markdown                         # 文档级 Markdown
   └─ detail[]                         # 元素明细数组（只处理 type=table）
      └─ (仅当 item.type == "table" 时关注)
         ├─ type                        # 固定为 "table"（表格块）
         ├─ sub_type                    # "bordered"(有线) / "borderless"(无线)
         ├─ page_id                     # 表格所在页（续表拼接用）
         ├─ paragraph_id                # 表格元素ID（续表拼接用）
         ├─ rows                        # 表格行数
         ├─ cols                        # 表格列数
         ├─ text                        # 表格整体文本（md/html；展示用，抽字段优先 cells）
         ├─ continue?                   # 是否跨页/跨段续表（可选字段）
         └─ cells[]                     # 单元格数组（抽取字段核心）
            ├─ row                       # 行号（从0开始）
            ├─ col                       # 列号（从0开始）
            ├─ row_span?                 # 行合并跨度（默认1）
            ├─ col_span?                 # 列合并跨度（默认1）
            └─ text                      # 单元格文本（字段值通常从这里拿）</code></pre><p>TextIn的返回结果中对表格块（type=table）的两种常见数据形态（务必兼容）</p><pre><code>形态 A：HTML/Markdown 表格（最常见于工作流插件输出）


    抽取方式：解析text→ 转二维矩阵（headers/rows）
    item.text内包含&lt;table&gt;...&lt;/table&gt;（或Markdown table）
    item.type == "table"


形态 B：单元格数组cells（部分接口/参数下提供）

    item.cells[]存在，包含row/col/text等
    抽取方式：优先用cells拼matrix（更结构化），不存在再回退到解析tex






</code></pre><p>财务三大表抽取 - 代码节点的输出示例（tables）<br/>tables.balanceSheet / incomeStatement / cashFlow均为数组，设计理由如下：</p><pre><code>同一份财报可能包含“合并 + 母公司”两套表；
或者出现“（续）”导致一张表被拆成多段；
因此用数组承载多张/多段表更稳妥，业务侧可按title/page_id再做合并与筛选。

</code></pre><p>tables</p><pre><code>{
    "balanceSheet": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年6 月30 日",
                "2024 年12 月31 日"
            ],
            "page_id": [
                2
            ],
            "rows": [
                [
                    "流动资产：",
                    "",
                    "-",
                    "-"
                ],
            ],
            "title": "合并资产负债表"
        },
 
 
    ],
    "incomeStatement": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年1-6 月",
                "2024 年1-6 月"
            ],
            "page_id": [
                4
            ],
            "rows": [
                [
                    "一、营业总收入",
                    "",
                    "88,095,798,091.41",
                    "85,336,441,428.97"
                ],
            ],
            "title": "母公司利润表"
        }
    ],
    "cashFlow": [
        {
            "headers": [
                "项 目",
                "附注",
                "2025 年1-6 月",
                "2024 年1-6 月"
            ],
            "page_id": [
                5
            ],
            "rows": [
                [
                    "一、经营活动产生的现金流量；",
                    "",
                    "-",
                    "-"
                ],
            ],
            "title": "母公司现金流量表"
        }
    ]
}</code></pre><p>Debug</p><pre><code>"debug": {
  "detailLen": 823,
  "titleCandidates": 6,
  "hitTitles": [
    {"idx": 120, "page_id": 2, "title": "合并资产负债表"},
    {"idx": 260, "page_id": 4, "title": "母公司利润表"}
  ],
  "picked": [
    {"titleIdx": 120, "tableIdx": 125, "tableType": "balanceSheet"},
    {"titleIdx": 260, "tableIdx": 268, "tableType": "incomeStatement"}
  ],
  "tableBlocks": 12
}</code></pre><h3>3.3 关键设计点（财报专属）</h3><p><strong>标题命中策略（table_title + 关键词）</strong><br/>标题长度阈值（&gt;20 跳过）：避免长文档中出现“包含关键词的长句”被误判为表标题，从而误抽无关表格。<br/>只认sub_type=table_title：优先使用版面分析识别到的“表格标题”元素，减少正文段落（header/text）误命中概率。</p><pre><code>const TITLE_PATTERNS = {
  balanceSheet: ["资产负债表", "合并资产负债表", "母公司资产负债表"],
  incomeStatement: ["利润表", "合并利润表", "母公司利润表", "损益表", "收益表"],
  cashFlow: ["现金流量表", "合并现金流量表", "母公司现金流量表", "现金流量"],
};

function normalizeTitle(s) {
  return String(s || "")
    .replace(/\*\*/g, "")
    .replace(/[\s　]/g, "")
    .replace(/[《》]/g, "");
}
function matchType(norm) {
  for (const [k, kws] of Object.entries(TITLE_PATTERNS)) {
    if (kws.some(kw =&gt; norm.includes(kw))) return k;
  }
  return null;
}

function extractFromDetail(detail) {
  const tables = { balanceSheet: [], incomeStatement: [], cashFlow: [] };
  const debug = { hitTitles: [], picked: [], tableBlocks: 0, titleCandidates: 0 };

  for (let i = 0; i &lt; detail.length; i++) {
    const item = detail[i];
    if (!item || typeof item !== "object") continue;

    const rawTitle = String(item.text || "");
    const title = normalizeTitle(rawTitle);

    // ✅ 简单校验：标题长度太长跳过
    if (title.length &gt; 20) continue;

    // ✅ 查询TextIn接口返回数据中的表格标题，避免正文误命中
    if (String(item.sub_type || "").toLowerCase() !== "table_title") continue;

    const ttype = matchType(title);
    if (!ttype) continue;</code></pre><h2>四、准备工作</h2><p>TextIn 开发者信息（x-ti-app-id / secret_code）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558035" alt="图片" title="图片" loading="lazy"/></p><p>在TextIn控制台（<a href="https://link.segmentfault.com/?enc=m%2FH1NGSQUeLqVVyUnMuKXg%3D%3D.Ns0Zq6%2FuEkFGHHF8cN2gJreNTHRXzliVMZEiX0y4Cco%3D" rel="nofollow" target="_blank">https://www.textin.com/</a>）「开发者信息」中获取x-ti-app-id与x-ti-secret-code（下文统称 app_id/secret_code）。<br/>建议在Coze工作流里把鉴权参数作为开始节点输入传入（便于不同环境切换），或在团队内部用变量/密钥管理统一配置。</p><h2>五、工作流搭建</h2><h3>5.1 创建工作流</h3><p>工作流命名、描述、版本说明<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047558036" alt="图片" title="图片" loading="lazy"/></p><h3>5.2 开始节点配置</h3><p>Input类型：File（接收上传文件）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558037" alt="图片" title="图片" loading="lazy"/></p><h3>5.3 添加 xParse插件节点</h3><pre><code>输入映射：file → Input.file
鉴权配置：x_ti_app_id / x_ti_secret_code
输出字段说明：result.detail / result.markdown 等，输出重点使用：ParseX.result（作为代码节点输入），其中result.detail是抽表主数据源。


</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558038" alt="图片" title="图片" loading="lazy"/></p><h3>5.4 添加代码节点（核心）</h3><p>输入变量配置 (选择ParseX.result)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558039" alt="图片" title="图片" loading="lazy"/></p><p>代码职责：遍历detail→找table_title→找后续table→HTML转二维矩阵→输出 tables（代码节点源码附在文章最末尾）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047558040" alt="图片" title="图片" loading="lazy"/></p><pre><code>输出结构：tables{balanceSheet,incomeStatement,cashFlow} +debug

</code></pre><h3>5.5 结束节点输出</h3><p>输出给Agent：tables / markdown / debug</p><h2>六、不止于抽取：更多自动化扩展方向</h2><p>财报抽取机器人是一个高效的起点，接下来，基于TextIn提供的精准结构化数据与Coze灵活的工作流，还可以轻松延伸出更多智能化的数据处理能力：</p><p>续表自动合并：财报中经常存在大型表格跨页，可在工作流中添加逻辑节点，按title相同且表头一致合并 rows，并合并 page_id，彻底解决数据割裂问题。<br/>表内锚点词校验：为确保抽取表格的完整性与正确性，可设计自动校验规则。例如，检查资产负债表中是否同时存在“流动资产”/“资产总计”科目；验证利润表是否包含“营业收入”/“净利润”；确认现金流量表是否包含“经营活动”。这一步能有效拦截因解析页面错误或文档版本差异导致的重大数据缺失。<br/>结构化导出至Excel：将最终整理的tables列表，通过添加代码节点或Coze插件，转换为更通用的CSV或XLSX格式文件。这能让财务、业务部门的同事无缝接手，直接在Excel环境中进行后续分析与可视化。<br/>实现智能多期对比：将工作流升级为可接收两份财报，分别提取后，系统能根据标准化的会计科目名称自动对齐数据，计算关键项目的同比、环比变化，并可由集成的LLM输出差异分析简报。</p><p>通过TextIn与Coze的组合，我们完成了从杂乱文档到结构化数据，再到可交互、可扩展的业务工具的完整路径，构建了一个可靠、可重复、且持续进化的数据流水线。无论是应对合规检查，还是满足定期的经营分析，这个财报机器人都能成为你技术工具箱中一个反应迅速、值得信赖的数字化助手。<br/>现在，是时候告别手动处理的繁琐与不确定，让你的数据工作流真正“智能”起来。</p><h2>七、附：代码节点源码</h2><p>下载链接：<a href="https://link.segmentfault.com/?enc=bHyU2CWcxOIGIgizsamwEA%3D%3D.FEEHif5Yx18RhKVeAulfTVmGDsTVCrlCSPDL%2FWb8Pm5m%2Fo7RnOHw38nUCSsUhtOzcqVPi3fvA53NU3qAfp4zjw%3D%3D" rel="nofollow" target="_blank">https://dllf.textin.com/download/2026/CustomService/</a>财报提取-coze代码节点源码.js</p>]]></description></item><item>    <title><![CDATA[你的大脑不是漏斗：用AI重写你的“记忆代码” HuiZhu ]]></title>    <link>https://segmentfault.com/a/1190000047557228</link>    <guid>https://segmentfault.com/a/1190000047557228</guid>    <pubDate>2026-01-22 11:17:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>想象一下，你走进一座藏书千万的图书馆。</p><p>如果管理员把所有的书都随意堆在地板上，没有任何分类，也没有索引编号。当你急需一本《百年孤独》时，你需要多久才能找到？</p><p>大概率是一辈子也找不到。</p><p>很多时候，我们抱怨自己“记性差”，觉得自己是“金鱼记忆”，其实这是一个巨大的误解。<strong>你的大脑从来不是一个漏斗，而是一座管理混乱的图书馆。</strong></p><p>我们习惯的“死记硬背”，就像是把书（知识）一本本扔进大脑的仓库地板上。扔进去的时候很费劲，找出来的时候更是灾难。</p><p>真正的记忆高手，并不是拥有更大的仓库，而是掌握了一套<strong>“编码系统”</strong>。他们把每一个新知识都打上标签，挂在已有的知识钩子上。</p><p>以前，这种“编码能力”需要经过专业的记忆力训练才能掌握。但现在，我们有了DeepSeek、Kimi这些AI工具。它们最擅长的，恰恰就是<strong>处理信息、建立索引、生成关联</strong>。</p><p>既然如此，为什么不让AI做你的“海马体外挂”，帮你把知识整整齐齐地“摆”进大脑里？</p><h2>🧠 为什么你总是“读了就忘”？</h2><p>认知心理学告诉我们，记忆分为三个过程：<strong>编码（Encoding）、存储（Storage）、提取（Retrieval）</strong>。</p><p>绝大多数人的问题，都出在第一步：<strong>编码失效</strong>。</p><p>当你看着书本反复念叨“abandon, abandon, abandon”时，你只是在进行<strong>“机械复述”</strong>。这种信号太弱了，大脑的神经元连个火花都擦不出来。它就像是用手指在沙滩上写字，海浪（时间）一冲，痕迹全无。</p><p>而高效记忆的核心，在于<strong>“精细加工”</strong>。</p><p>要把枯燥的信息，转化成<strong>图像、故事、空间位置</strong>或者<strong>逻辑链条</strong>。你要让新的知识，和你大脑里已有的旧知识“发生关系”。</p><ul><li>记“Ponderous”（笨重的）：机械记忆要念10遍。精细加工是想象一个“胖得（Ponder）要死（ous）”的大胖子，走路很<strong>笨重</strong>。</li><li>记“马斯洛需求理论”：机械记忆是背5个层级。精细加工是想象自己在一个荒岛上：先找水喝（生理），再搭棚子（安全），然后想找人说话（社交）...</li></ul><p>道理都懂，但难点在于：<strong>不仅要脑洞大，还要逻辑强。</strong> 这对普通人来说，门槛太高了。</p><p>但这正是AI的拿手好戏。</p><h2>🔌 核心指令：给大脑装个“超频补丁”</h2><p>今天分享的这条指令，不再把AI当作简单的“问答机”，而是把它重新定义为你的<strong>私人记忆教练</strong>。</p><p>它融合了<strong>艾宾浩斯遗忘曲线、记忆宫殿、费曼学习法</strong>等经典理论。你只需要把想记的内容扔给它，它就会吐出一套为你量身定制的“编码方案”。</p><p>它不只告诉你“背下来”，它会告诉你“怎么背才不忘”。</p><h3>🧬 记忆技巧生成 AI 提示词</h3><pre><code class="markdown"># 角色定义
你是一位专业的记忆力训练师和认知心理学专家，拥有10年以上记忆方法教学经验。你精通艾宾浩斯遗忘曲线、记忆宫殿法、联想记忆法、间隔重复等多种科学记忆方法，擅长根据不同学习内容和个人特点，设计最适合的记忆策略。

你的核心能力包括：
- 分析学习内容特点，识别最佳记忆方法
- 将抽象信息转化为生动易记的形式
- 设计科学的复习计划，对抗遗忘曲线
- 创建记忆钩子和联想链接

# 任务描述
请针对我提供的学习内容，设计一套完整的高效记忆方案，帮助我快速记住并长期保持记忆。

**输入信息**:
- **学习内容**: [需要记忆的具体内容，如单词、公式、概念、历史事件等]
- **内容数量**: [需要记忆的条目数量]
- **记忆目标**: [记忆的目的，如考试、演讲、日常应用等]
- **时间限制**: [可用于记忆的时间]
- **个人偏好**: [视觉型/听觉型/动觉型学习者偏好，可选]

# 输出要求

## 1. 内容结构
请按以下结构输出记忆方案：

- **内容分析**: 分析学习内容的特点和难点
- **方法推荐**: 推荐最适合的记忆方法及原因
- **记忆方案**: 具体的记忆技巧和步骤
- **复习计划**: 基于艾宾浩斯遗忘曲线的复习安排
- **记忆测试**: 自测方法和检验标准

## 2. 质量标准
- **科学性**: 基于认知科学和记忆心理学原理
- **实用性**: 方法简单易操作，立即可用
- **个性化**: 根据内容特点定制方法
- **可验证**: 提供具体的效果检验方式

## 3. 格式要求
- 使用清晰的层级标题
- 关键方法用加粗突出
- 复习计划用表格呈现
- 记忆技巧配合具体示例

## 4. 风格约束
- **语言风格**: 亲切专业，像一位耐心的老师
- **表达方式**: 第二人称"你"，直接指导
- **专业程度**: 通俗易懂，避免过多术语

# 质量检查清单

在完成输出后，请自我检查：
- [ ] 已分析内容特点并匹配合适方法
- [ ] 记忆技巧有具体示例演示
- [ ] 复习计划科学且可执行
- [ ] 包含自测方法验证效果
- [ ] 语言通俗易懂，新手可操作

# 注意事项
- 不要一次性推荐过多方法，聚焦1-2个最有效的
- 复习计划要考虑用户实际可用时间
- 避免过于复杂的记忆技巧，简单有效优先
- 针对不同类型内容(文字/数字/概念)给出差异化方案

# 输出格式
请按照结构化的Markdown格式输出，包含清晰的标题层级、表格和列表。</code></pre><h2>⚡️ 实战演示：AI如何“降维打击”枯燥知识</h2><p>为了让你感受一下“开了挂”的体验，我们拿一个典型的枯燥知识点来测试：<strong>记忆元素周期表的前10位</strong>。</p><p>（氢氦锂铍硼，碳氮氧氟氖... 即使背顺口溜，过两天也容易忘顺序）</p><p>把这个任务丢给AI，并告诉它“我是视觉型学习者，喜欢有趣的联想”。</p><p>AI给出的方案可能会让你拍案叫绝：</p><blockquote><p><strong>🎯 推荐方法：数字挂钩法 + 故事串联</strong></p><ol><li><strong>氢 (H)</strong> -&gt; 像个气球🎈 -&gt; <strong>第一</strong>个气球飞上天。</li><li><strong>氦 (He)</strong> -&gt; 孩子(孩) -&gt; <strong>两个</strong>孩子在玩气球。</li><li><strong>锂 (Li)</strong> -&gt; 梨子(梨) -&gt; 孩子手里拿着<strong>三个</strong>大鸭梨。</li><li><strong>铍 (Be)</strong> -&gt; 皮球(皮) -&gt; 梨子掉在地上变成了<strong>四个</strong>皮球。</li><li><strong>硼 (B)</strong> -&gt; 朋友(朋) -&gt; <strong>五个</strong>小朋友跑来踢皮球。<br/>...</li></ol><p><strong>🧠 脑内剧场</strong>：<br/>想象一个画面：一个红气球（1氢）升空，下面挂着两个小孩（2氦），他们啃着三个大梨（3锂），梨核变成四个皮球（4铍），引来了五个小朋友（5硼）...</p></blockquote><p>看到没？这不再是冰冷的文字，而是一场<strong>“脑内微电影”</strong>。</p><p>原本孤立的元素，通过AI的“编剧能力”，变成了一个连续的动态画面。你的海马体（大脑负责记忆的区域）最喜欢这种有色彩、有动作、有情节的信息。</p><p>哪怕过了一周，你可能忘了“铍”是第几个，但你绝对忘不了“梨子变皮球”那个滑稽的画面。这就是<strong>编码</strong>的力量。</p><h2>🚀 重新定义“学习力”</h2><p>在这个知识爆炸的时代，我们不需要成为行走的百科全书。</p><p>存储知识，是硬盘的事；检索知识，是搜索引擎的事。人类大脑最应该做的，是<strong>理解、连接和创造</strong>。</p><p>但这并不意味着记忆不重要。恰恰相反，<strong>记忆是创造的燃料</strong>。如果你脑子里空空如也，连基本的概念都提取不出来，又何谈灵感和洞察？</p><p>这套AI指令，就是你连接“外部知识”和“内部智慧”的桥梁。</p><p>它帮你省去了最痛苦的“死记硬背”过程，直接把知识加工成大脑易于吸收的<strong>“高生物利用度”</strong>形态。</p><p>下次，当你面对厚厚的考证资料、复杂的演讲稿或者晦涩的技术文档时，别急着开始念经。</p><p>先停下来，把内容喂给AI，对它说：<strong>“嘿，帮我给这些知识编个码。”</strong></p><p>然后，享受那种知识如流水般滑入大脑的快感吧。</p>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Media Kit HarmonyOS_SD]]></title>    <link>https://segmentfault.com/a/1190000047557440</link>    <guid>https://segmentfault.com/a/1190000047557440</guid>    <pubDate>2026-01-22 11:17:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>断点太多是否会使DevEco Studio运行卡顿？如何处理？</p><p><strong>解决方案：</strong></p><p>断点太多会影响DevEco Studio运行，可以通过<a href="https://link.segmentfault.com/?enc=KcVMoMd4dtyB3is8cc2N9A%3D%3D.M6W6QQ8VJsPCU9J2pJ1AgOj3Xp2boO5%2Fy6u09R%2BqFWKsk9PvjHvXQRS%2B%2Fph4gOBk2bLlFD%2B%2FYgwjNIQU9Ow5vMbNLyHnf%2B%2BADEONJ2LRZ5DxUWvNDeCSBaprNDwJgrUO2oxptmjiTkbaLly7hp9OOA%3D%3D" rel="nofollow" target="_blank">断点管理</a>删除不必要的断点。</p><p><strong>2.问题描述：</strong></p><p>为什么图片使用imagePacker.packToFile压缩完之后，反而变大了？</p><p><strong>解决方案：</strong></p><p>可以参考图片压缩API的质量参数quality与图片原始大小、压缩后大小的关系，quality是图片质量参数，并非是指按百分比压缩。若压缩前图片质量比指定的压缩参数quality小的话，就可能会导致压缩后的图片文件比压缩前更大；若想实现压缩变小，可以降低quality值，或是压缩前使用。PixelMap.scale缩放图片后再进行压缩。</p><p><strong>3.问题描述：</strong></p><p>AVPlayer有两个播放源，清晰度不一样，希望切换播放源时尽量顺滑，让用户没有感知，有什么方案？</p><p><strong>解决方案：</strong></p><p>应用中通过层叠布局创建两个avPlayer播放器堆叠，用户只能看到最上层的播放器界面；点击播放时，两个清晰度不一样的视频同时在两个播放器上播放，点击切换时，设置对应清晰度视频所在的播放器的堆叠顺序为高优先级，则会展示该播放器界面在最上层，达到切换的目的。</p>]]></description></item><item>    <title><![CDATA[从 TianQi 项目看 Spring Cloud 微服务治理 坎窝主夜 ]]></title>    <link>https://segmentfault.com/a/1190000047557447</link>    <guid>https://segmentfault.com/a/1190000047557447</guid>    <pubDate>2026-01-22 11:16:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当天气预报不再局限于“播报”，而是成为物理世界的数字孪生接口，微服务架构将如何撑起这场感知革命？<br/>“透过天气项目学透 Spring Cloud”不仅是一次技术实践的复盘，更是对未来软件架构形态的一次预演。在传统的认知中，天气项目往往被视为展示 RESTful API、服务注册发现、配置中心等 Spring Cloud 核心组件的经典场景。然而，若我们将目光投向未来 5 到 10 年的技术演进，这个项目将不再仅仅是数据的搬运工，而是演变为集全球感知、边缘计算、AI 赋能于一体的复杂智能系统。<br/>从未来的视角审视 Spring Cloud 在天气项目中的角色，我们将看到微服务治理正在经历一场从“集中式管理”向“云边智协同”的深刻范式转移。<br/>一、 架构形态：从集中式云端迈向“云-边-端”全域协同<br/>未来的气象监测将不再依赖孤立的气象站，而是由数以亿计的物联网传感器、手机气压计、车载雷达以及低轨卫星构成的泛在感知网络。传统的单体 Spring Cloud 架构将无法应对海量的设备接入和极高的并发写入，架构形态将发生根本性进化。</p><ol><li>边缘节点的微服务化<br/>未来的 Spring Cloud 将不仅仅运行在中心云机房，更将大规模下沉至边缘侧。在未来的天气项目中，每个城市甚至每个街区都会部署边缘计算节点。<br/>边缘自治：利用 Spring Cloud 的扩展机制，微服务将具备“边缘自治”能力。即使在网络与中心云断连的情况下，本地的气象数据采集、预警广播等服务仍能独立运行。这是未来应对极端自然灾害、保障通信“最后一公里”的关键技术。<br/>动态拓扑感知：服务治理将不再局限于静态的服务列表。未来的服务发现组件需要能够实时感知移动节点（如气象无人机、应急车）的动态位置，基于地理位置和网络延迟动态调整服务调用链路。</li><li>混合云架构的常态化<br/>为了应对突发性天气（如台风、暴雨）带来的局部流量洪峰，未来的天气项目将运行在混合云之上。<br/>无缝跨云调度：Spring Cloud 的服务治理将与底层基础设施深度解耦，实现跨公有云和私有云的无缝服务调度。当某区域流量激增时，系统能自动在云端扩容计算微服务实例，并将流量智能分发，实现真正的“气象级”弹性伸缩。<br/>二、 数据处理：从批处理演进为“流批一体”的实时孪生<br/>未来的天气预报要求达到“分钟级”甚至“秒级”的刷新率，这对微服务间的数据流转提出了极高的要求。传统的请求-响应模式将逐渐让位于事件驱动架构（EDA）。</li><li>事件驱动的服务解耦<br/>在未来的项目中，传感器的每一次数据波动都将触发一个事件。<br/>实时反应链：Spring Cloud Stream（或其演进形态）将成为连接物理世界与数字世界的神经中枢。一旦监测到气压骤降，事件即刻触发，预警服务、交通调度服务、物流规划服务并发响应，无需等待上层应用轮询。这种“极速解耦”是未来智慧城市运作的基础。</li><li>数字孪生的实时构建<br/>天气项目将成为构建城市“数字孪生”的核心数据源。微服务架构不仅要传输数据，更要维持一个与真实世界同步的虚拟模型。<br/>状态一致性挑战：在高度并发的微服务环境下，如何保证全球数百万个虚拟气象节点状态的一致性？未来的分布式事务治理将不再局限于 ACID 或 BASE，而是结合 CRDTs（无冲突复制数据类型）等新型数据结构，实现最终一致性与实时性的完美平衡。<br/>三、 治理智能化：从人工运维到“自愈合”智能体<br/>随着系统复杂度呈指数级增长，人工配置 Hystrix 断路器、手动调整熔断策略将成为历史。未来的微服务治理将全面拥抱 AIOps（智能运维）。</li><li>预测性弹性伸缩<br/>未来的 Spring Cloud Gateway 将集成 AI 预测引擎。<br/>流量预判：结合历史天气数据和即将到来的气象变化，系统能够预知某地即将发生的暴雨会导致用户查询量激增。在流量到来之前，微服务实例自动完成扩容和预热，实现“零延迟”响应。</li><li>自愈合系统<br/>异常根因分析：当某个微服务响应变慢时，AI Agent 会自动分析链路追踪数据，判断是数据库锁死、网络抖动还是算法缺陷，并自动注入修复策略（如限流、重启、降级），无需人工干预。系统将具备类似生物体的“免疫修复”能力。<br/>四、 安全与可信：零信任架构与隐私计算<br/>气象数据在未来将关联到能源调度、航空保险、农业生产等高价值领域，数据的安全性与隐私性至关重要。</li><li>零信任网络<br/>未来的 Spring Cloud 安全体系将默认“不信任任何内外部网络”。<br/>细粒度动态授权：每一次服务调用，即使是内部微服务之间的通信，都需要经过基于身份和上下文的动态鉴权。Service Mesh（服务网格）将成为标准配置，承载所有微服务的流量管控与加密传输。</li><li>数据的可用不可见<br/>在某些商业场景下，例如保险公司获取气象数据进行理赔核验，未来的架构将支持隐私计算。保险公司可以在不解密原始气象数据的情况下，运行计算逻辑获得结果。这需要在微服务协议层面引入同态加密等技术的支持，彻底解决数据共享的信任危机。<br/>五、 终极愿景：Spring Cloud 作为“感知即服务”的骨架<br/>透过未来的天气项目，我们看到 Spring Cloud 的本质正在发生变化。它不再仅仅是 Java 程序员手中的开发框架，而是正在进化为连接数字世界与物理世界的操作系统。<br/>在这个未来图景中，Spring Cloud 赋予了软件系统“感知”、“思考”和“反应”的能力。它让气象数据不再停留在屏幕上，而是流动到自动驾驶汽车的决策单元中，流动到智能电网的调度算法中，流动到每一个用户的智能终端上。<br/>“从入门到进阶”的终点，不仅是掌握了一个框架的使用，而是理解了如何构建一个具有韧性、智能且自适应的未来系统。这或许才是我们学习 Spring Cloud 的终极意义所在——在比特与原子的交汇处，用代码重构世界的运行逻辑。</li></ol>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Device Security Kit Ha]]></title>    <link>https://segmentfault.com/a/1190000047557453</link>    <guid>https://segmentfault.com/a/1190000047557453</guid>    <pubDate>2026-01-22 11:15:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>请问有没有C接口（NDK）直接读取CPU型号、主板UUID、硬盘序列号、网卡MAC等信息（比如udev）？或者有没有可靠的设备唯一ID接口可供调用？</p><p><strong>解决方案：</strong></p><p>常见设备的标识有OAID、ODID、AAID、UDID等，定义和用途如下：</p><p>OAID（开放匿名设备标识符）一种非永久性设备标识符，基于OAID，可在保护用户个人数据隐私安全的前提下，媒体App、广告平台、三方监测平台等开发者，可获取设备上的OAID，进行个性化广告推荐或广告转化归因分析。</p><p>ODID（开发者匿名设备标识符）：用于识别同一设备上运行的同一个开发者的应用，标识应用身份。帮助开发者更好地理解用户在不同应用间的行为，从而提供更个性化的服务和推荐。</p><p>AAID（应用匿名标识符）：标识应用的身份，主要用于应用的消息推送。</p><p>UDID（设备唯一标识符）：标识设备的属性，可作为设备唯一识别码。</p><p>只有UDID才能作为设备的唯一标识符，不会随设备重置或应用卸载而发生变化，但UDID只允许系统应用及企业定制应用申请特殊权限才能获取。当前设备重置时还无法保证标识符不发生改变，但有方案可以实现应用卸载时标识符不发生改变。</p><p>为了保证及时在应用卸载后仍能有效的确保获取的设备标识符不发生变化，间接达到“唯一标识符”的目的，华为提供了关键资产存储服务，开发者可以将设备标识符放在asset里，设置IS_PERSISTENT()为true，实现在应用卸载时保留关键资产，达到标识符不清除的效果。如获取ODID后配合使用Asset Store Kit能力，保持ODID不变的效果，示例代码如下：</p><pre><code>
import { asset } from '@kit.AssetStoreKit';

import util from '@ohos.util';

import { deviceInfo } from '@kit.BasicServicesKit';


function stringToArray(str: string): Uint8Array {

  let textEncoder = new util.TextEncoder();

  return textEncoder.encodeInto(str);

}


function setAttr(id: string) {

  let attr: asset.AssetMap = new Map();

  attr.set(asset.Tag.SECRET, stringToArray(id));

  attr.set(asset.Tag.ALIAS, stringToArray('demo_device_id'));

  attr.set(asset.Tag.IS_PERSISTENT, true);

  try {

    asset.add(attr).then(() =&amp;gt; {

      console.info(`Asset added successfully.`);

    }).catch(() =&amp;gt; {

      console.error(`Failed to add Asset.`);

    })

  } catch (error) {

    console.error(`Failed to add Asset.`);

  }

}


function arrayToString(arr: Uint8Array): string {

  let textDecoder = util.TextDecoder.create("utf-8", { ignoreBOM: true });

  let str = textDecoder.decodeWithStream(arr, { stream: false })

  return str;

}


async function getAttr(): Promise&lt;string&gt; {

  let query: asset.AssetMap = new Map();

  query.set(asset.Tag.ALIAS, stringToArray('demo_device_id')); // 指定了关键资产别名，最多查询到一条满足条件的关键资产

  query.set(asset.Tag.RETURN_TYPE, asset.ReturnType.ALL); // 此处表示需要返回关键资产的所有信息，即属性+明文

  try {

    const res: Array&lt;asset.assetmap&gt; = await asset.query(query)

    // 解析密钥

    let secret: Uint8Array = res[0].get(asset.Tag.SECRET) as Uint8Array;

    // 将uint8array解析为字符串

    let secretStr: string = arrayToString(secret);

    return secretStr;

  } catch (error) {

    console.error(`Failed to query Asset.`);

    return '';

  }

}


@Entry

@Component

struct AttrTest {

  build() {

    Column() {

      Button('获取设备ID').onClick(async (event: ClickEvent) =&amp;gt; {

        let deviceId: string = await getAttr();

        if (deviceId === undefined || deviceId === null || deviceId.length === 0) {

          deviceId = deviceInfo.ODID;

          setAttr(deviceId);

        }

        console.log('设备ID为：' + deviceId)

      })

        .height(100)

        .width('100%')

    }

  }

}

</code></pre><p><strong>2.问题描述：</strong></p><p>如何使用DSA算法实现签名验签的功能？</p><p><strong>解决方案：</strong></p><ol><li>配置DSA1024公钥和私钥中包含的公共参数dsaCommonSpec。</li><li>设置DSA1024密钥对中包含的全参数。</li><li>调用createAsyKeyGeneratorBySpec方法生成DSA算法的非对称密钥生成器。</li><li>通过密钥生成器生成DSA非对称密钥对。</li><li>使用DSA私钥对数据进行签名。</li><li>使用DSA公钥对签名数据进行验签。</li></ol><p>完整示例代码如下：</p><pre><code class="TypeScript">
import { cryptoFramework } from '@kit.CryptoArchitectureKit';

import { buffer } from '@kit.ArkTS';


let input: cryptoFramework.DataBlob = { data: new Uint8Array(buffer.from("This is Sign test plan", 'utf-8').buffer) };


// 配置DSA1024公钥和私钥中包含的公共参数

function genDsa1024CommonSpecBigE() {

  let dsaCommonSpec: cryptoFramework.DSACommonParamsSpec = {

    algName: "DSA",

    specType: cryptoFramework.AsyKeySpecType.COMMON_PARAMS_SPEC,

    p: BigInt("0xed1501551b8ab3547f6355ffdc2913856ddeca198833dbd04f020e5f25e47c50e0b3894f7690a0d2ea5ed3a7be25c54292a698e1f086eb3a97deb4dbf04fcad2dafd94a9f35c3ae338ab35477e16981ded6a5b13d5ff20bf55f1b262303ad3a80af71aa6aa2354d20e9c82647664bdb6b333b7bea0a5f49d55ca40bc312a1729"),

    q: BigInt("0xd23304044019d5d382cfeabf351636c7ab219694ac845051f60b047b"),

    g: BigInt("0x2cc266d8bd33c3009bd67f285a257ba74f0c3a7e12b722864632a0ac3f2c17c91c2f3f67eb2d57071ef47aaa8f8e17a21ad2c1072ee1ce281362aad01dcbcd3876455cd17e1dd55d4ed36fa011db40f0bbb8cba01d066f392b5eaa9404bfcb775f2196a6bc20eeec3db32d54e94d87ecdb7a0310a5a017c5cdb8ac78597778bd"),

  }

  return dsaCommonSpec;

}


// 设置DSA1024密钥对中包含的全参数

function genDsa1024KeyPairSpecBigE() {

  let dsaCommonSpec = genDsa1024CommonSpecBigE();

  let dsaKeyPairSpec: cryptoFramework.DSAKeyPairSpec = {

    algName: "DSA",

    specType: cryptoFramework.AsyKeySpecType.KEY_PAIR_SPEC,

    params: dsaCommonSpec,

    sk: BigInt("0xa2dd2adb2d11392c2541930f61f1165c370aabd2d78d00342e0a2fd9"),

    pk: BigInt("0xae6b5d5042e758f3fc9a02d009d896df115811a75b5f7b382d8526270dbb3c029403fafb8573ba4ef0314ea86f09d01e82a14d1ebb67b0c331f41049bd6b1842658b0592e706a5e4d20c14b67977e17df7bdd464cce14b5f13bae6607760fcdf394e0b73ac70aaf141fa4dafd736bd0364b1d6e6c0d7683a5de6b9221e7f2d6b"),

  }

  return dsaKeyPairSpec;

}


async function signMessagePromise(priKey: cryptoFramework.PriKey) {

  let signAlg = "DSA1024|SHA256";

  let signer = cryptoFramework.createSign(signAlg);

  await signer.init(priKey);

  let signData = await signer.sign(input);

  return signData;

}


async function verifyMessagePromise(signMessageBlob: cryptoFramework.DataBlob, pubKey: cryptoFramework.PubKey) {

  let verifyAlg = "DSA1024|SHA256";

  let verifier = cryptoFramework.createVerify(verifyAlg);

  await verifier.init(pubKey);

  let res = await verifier.verify(input, signMessageBlob);

  console.info('DSA verify result is ' + res);

  return res;

}


function main() {

  let asyKeyPairSpec = genDsa1024KeyPairSpecBigE();

  let asyKeyGeneratorBySpec = cryptoFramework.createAsyKeyGeneratorBySpec(asyKeyPairSpec);

  // 异步获取非对称密钥生成器生成的密钥

  asyKeyGeneratorBySpec.generateKeyPair(async (err, keyPair) =&amp;gt; {

    if (err) {

      console.error('generateKeyPair: error.');

      return;

    }

    console.info('generateKeyPair: success.');

    // 签名

    let signData = await signMessagePromise(keyPair.priKey)

    // 验签

    let verifyResult = await verifyMessagePromise(signData, keyPair.pubKey);

    if (verifyResult === true) {

      console.info('verify success');

    } else {

      console.error('verify failed');

    }

  })

}
</code></pre><p><strong>3.问题描述：</strong></p><p>从应用设置页跳转至系统设置显示没有权限。</p><p><strong>解决方案：</strong></p><p>应用在权限管理界面的操作，未先进行相关权限申请，则根据系统设计，无法在系统隐私设置权限页面设置，可以参考以下步骤：</p><ol><li>通过<a href="https://link.segmentfault.com/?enc=sApY7GWgz%2BeuI1DtoOAAKA%3D%3D.oANU0%2FovH9a43brGjtbdygFbVRyNzu1AsN4xgtdQGVdUTPsFmlkmKnRKCRmn2GmsWogORMXb8A%2Fal7VtvGF7Vi%2F3Nfiluutfl680on4JVif8G6%2BnY53msn%2F03UkMi3xhehQrYnemo49IcmJq%2F8%2F0z8oL%2Fxvs6DkqbGlxqJNXR4k%3D" rel="nofollow" target="_blank">getSelfPermissionStatus</a>接口查询应用权限状态，参考代码：</li></ol><pre><code class="TypeScript">
getPermissionStatus(permission: string) {

  try {

    let data: abilityAccessCtrl.PermissionStatus = this.atManager.getSelfPermissionStatus(permission);

    console.info(`data-&amp;gt;${data}`);

  } catch (err) {

    console.error(`catch err-&amp;gt;${err}`);

  }

}
</code></pre><ol start="2"><li>当结果为NOT_DETERMINED时，表示未操作。应用声明用户授权权限，暂未调用requestPermissionsFromUser接口请求用户授权，此时可以调用请求用户授权接口进行授权，参考代码：</li></ol><pre><code class="TypeScript">
reqPermissionFromUser(permissionList: Array&lt;permissions&gt;) {

  let atManager: abilityAccessCtrl.AtManager = abilityAccessCtrl.createAtManager();

  let context: Context = this.getUIContext().getHostContext() as common.UIAbilityContext;

  atManager.requestPermissionsFromUser(context, permissionList,

    (err: BusinessError, data: PermissionRequestResult) =&amp;gt; {

      if (err) {

        console.error(`requestPermissionsFromUser fail, err-&amp;gt;${err}`);

      } else {

        console.info('data permissions:' + data.permissions);

        console.info('data authResults:' + data.authResults);

      }

    });

}
</code></pre><ol start="3"><li>当前结果为已授权或未授权时，可以跳转到系统权限设置页面调整，或者使用<a href="https://link.segmentfault.com/?enc=1JDPlRoMfFlrSflfoV8M0A%3D%3D.rn%2BjlEPa2046QWZdblnY3CSNvLLwku3TVNERvV%2B4q1f2bllSGky2ao%2FOW3k%2BWY0a5bmovOb4IpdhwSOMYmtfPa4RP2qCDKRiiJQjaS7CcpKX5g7TmeBKhTitYa%2BVYvxUvOQOIJHuMzkc94neviJSP2RuFoCuMrAMvePlpRFFTR0%3D" rel="nofollow" target="_blank">requestPermissionOnSetting</a>拉起权限设置弹框。参考代码：</li></ol><pre><code class="TypeScript">
applyPermissions(permissionList: Array&lt;permissions&gt;) {

  if (!this.atManager || !this.context) {

    return

  }


  this.atManager.requestPermissionOnSetting(this.context, permissionList)

    .then((data: Array&lt;abilityaccessctrl.grantstatus&gt;) =&amp;gt; {

      console.info(`data: ${data}`);

    })

    .catch((err: BusinessError) =&amp;gt; {

      console.error(`data: ${err}`);

    });

}
</code></pre><p>&lt;/abilityaccessctrl.grantstatus&gt;&lt;/permissions&gt;&lt;/permissions&gt;&lt;/asset.assetmap&gt;&lt;/string&gt;</p>]]></description></item><item>    <title><![CDATA[【FAQ】HarmonyOS SDK 闭源开放能力 — Audio Kit HarmonyOS_SD]]></title>    <link>https://segmentfault.com/a/1190000047557456</link>    <guid>https://segmentfault.com/a/1190000047557456</guid>    <pubDate>2026-01-22 11:15:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>1.问题描述：</strong></p><p>如何实现自定义音量调节？</p><p><strong>解决方案：</strong></p><p>设置系统音量</p><p>应用无法直接调节系统音量，系统提供了ArkTS组件<a href="https://link.segmentfault.com/?enc=4Ltum8%2Fs1e8bul5QZmQ7kw%3D%3D.0t%2FHuaMuuZrv3in1UWgEFhIRb9UKEswn5ZMbWD80pol96iCyMbxJOJ3x8vE6MoF3DOQB5RBnv676v3cF8eEIIjdSwM2miucW10OQCRL07l6K7gilxQupwlunCROrPkf0" rel="nofollow" target="_blank">AVVolumePanel音量面板</a>，应用可以创建该组件，让用户通过界面操作来调节音量。</p><p>设置应用音量</p><ol><li><p>管理应用音量的接口由AudioVolumeManager提供，在使用之前，需要使用getVolumeManager()获取AudioVolumeManager实例，示例代码如下：</p><pre><code class="TS">
import { audio } from '@kit.AudioKit';



let audioManager = audio.getAudioManager();

let audioVolumeManager = audioManager.getVolumeManager();
</code></pre></li><li><p>设置应用音量。</p><p>当<a href="https://link.segmentfault.com/?enc=iDBJfxb48XMm2xFzNVo6fQ%3D%3D.%2B%2BiKf6BRmyp28PdMGNoj%2BEOgmKoJGL2%2Bp%2BICHJjvgmd0X5Mh1p6UkGm8uuxQZqoCOVNkiOYzXgppETCevCKaU5KksgTfz63Jpbe0XrLsdm8fZW4wPdA8ljyJ8ftVMtf%2BBpgmFEsKyuMnxZ0EdtnG5A%3D%3D" rel="nofollow" target="_blank">音量模式</a>设置为APP_INDIVIDUAL时，可通过下面示例接口设置应用音量。</p><pre><code class="ts">
// 设置应用的音量（范围为0到100）。

audioVolumeManager.setAppVolumePercentage(20).then(() =&amp;gt; {

  console.info(`set app volume success.`);

});
</code></pre></li></ol><p>设置音频流音量</p><p>在ArkTS API端和Native API端分别有对应的API用来设置音频流音量。</p><p>使用ArkTS API时，开发者可以使用AVPlayer或AudioRenderer的setVolume()方法。</p><ul><li><p>使用<a href="https://link.segmentfault.com/?enc=UAdz%2FcvtlUeq5eUn%2BaVbWg%3D%3D.AUnBW6mybbt6EVv0VYtkSEgL64PMZmD4H5XQn5GNV0lPAgwNS047evihTfx0yC3qzj742OcQsnNKU%2BUewnKowm%2FNJ8kDQajYjW0p1spzmUQBU%2Fp8DxtcmIme1Al1867H4MMO%2Bf%2FlOtyGGUJGIWDn6A%3D%3D" rel="nofollow" target="_blank">AVPlayer</a>设置音频流音量的示例代码如下：</p><pre><code class="ts">
let volume = 1.0;  // 指定的音量大小，取值范围为[0.00-1.00]，1表示最大音量

avPlayer.setVolume(volume);
</code></pre></li><li><p>使用<a href="https://link.segmentfault.com/?enc=BfsDkrs%2FoFBpq4V%2FBnEEHw%3D%3D.1UssOZCS0g4UWwvAqyJtaCTMNSqPqF%2FxIKY6Xmmo7gAmogl6YXtdaq5Wh4DIfG0GnxtGh%2Buw8i9zL8SMV9We5efX%2F0SYnyGq6pwtFHLdv5BjE7ez1pAkut9pPRUhPDKDp81SqTHxdX%2FyR1MzWuVMEg%3D%3D" rel="nofollow" target="_blank">AudioRenderer</a>设置音频流音量的示例代码如下：</p><pre><code class="ts">
import { BusinessError } from '@kit.BasicServicesKit';



audioRenderer.setVolume(0.5).then(() =&amp;gt; {  // 音量范围为[0.0-1.0]

  console.info('Invoke setVolume succeeded.');

}).catch((err: BusinessError) =&amp;gt; {  

  console.error(`Invoke setVolume failed, code is ${err.code}, message is ${err.message}`);

});
</code></pre></li><li><p>使用Native API时开发者可使用<a href="https://link.segmentfault.com/?enc=NjmO0IPHfymqf7Eobyx4LA%3D%3D.%2FA4D%2B7c%2BTpAV8jbv%2FqueucYWboATbv3YDdIPfu36YlBal1eCZSNnIwBb7Gs4OUv1Vu03z85wc%2FAG4Ki2qdkHBSke8RjZLZwUwZz0vJonYdtotMLGNtrfnzXC3UTsAHACfySLmJMFk5qFvwluTlzW5%2FBnrelyCITJyY0JrFhsUMA%3D" rel="nofollow" target="_blank">OH_AudioRenderer_SetVolume</a>接口设置当前音频流音量值，示例代码如下：</p><pre><code class="C++">
// 要设置的音量值，音量值的范围是[0.0, 1.0]。

float volume = 0.5f;



// 设置当前音频流音量值。

OH_AudioStream_Result OH_AudioRenderer_SetVolume(audioRenderer, volume);
</code></pre></li></ul><p>请注意：</p><ul><li><strong>setVolume接口</strong>调整的是音频流本身的音量，不是系统音量，音量条本身不会发生变化，而且音频流本身的音量默认值是1，即以系统音量来播放，应用只可以在系统音量的基础上调到0~1倍，不会超过系统音量，也不会影响系统音量的值（即音量条）。</li><li>为确保用户能感知音量变化，应用后台不能调节音量，否则系统会做出对应的控制措施，因此音量面板设置volumeLevel初始值是不生效的，只有改变volumeLevel值触发音量面板，才会改变当前系统音量；并且音量面板调节具体音量由系统控制，当前播放什么音频就调节什么音量，没有播放时就会调节媒体音量。</li></ul><p><strong>2.问题描述：</strong></p><p>如何实现支持滑动的视频音量调节功能？</p><p><strong>解决方案：</strong></p><p><a href="https://link.segmentfault.com/?enc=GbRi4GWI2xCEmOtbpXn3sg%3D%3D.df4%2BoISAx5k%2BxmwLItD1%2BKN8Vbsd6DnBbMuOjFbAavugJsHMH1wOzpPjcZDaHFMsbn6CntsZtwFN6P4o9DKMjyjyc60wfbFxN0aKZ4ghK4UOD0ujh3Tikz4jPGW7lLjZ" rel="nofollow" target="_blank">Slider</a>组件结合音频流音量管理<a href="https://link.segmentfault.com/?enc=Uu7JyeQTxcTgPjJeomUk3A%3D%3D.x0XZu58uYNdQwmBxpoVkGZq%2BTT%2F5lfNmzOiHkMFqvGKo6HJ3cNq9PFybj%2BZd2bicyVxGVBI7jKnAy9%2BevGT9uYUd8lvCcwSaaeEZv%2B48IMQI101W0h8llD8dT%2F0fK%2Fpb3rNf4JaevE70JFJg6RJ8%2Fg%3D%3D" rel="nofollow" target="_blank">AVPlayer</a>或<a href="https://link.segmentfault.com/?enc=IJ%2BjK6AW4F5R2muVkGy1Hw%3D%3D.DiNJ7Gim4YRbZJawYy8npCelKhE%2BnTvzkk2j%2FhDJwu1MDs8304xfOhsJON1d1mMvL%2Fe7g0sRofHXXzy0Ze%2F8FX9b0ebUYJv0CozqYFO%2F485VUaBZTwKQSbDznal%2Bgp%2FDYmq49Zmw0FU5nsE2EBpOlA%3D%3D" rel="nofollow" target="_blank">AudioRenderer</a>实现。Slider组件用于支持用户滑动获取音量值，将获取到的值通过setVolume接口传递给音频音量管理实现音量滑动控制调节。</p><p><strong>3.问题描述：</strong></p><p>集成腾讯云点播实现视频播放，自定义声音按钮实现音量滑动调节有什么比较好的策略？</p><p><strong>解决方案：</strong></p><p>使用<a href="https://link.segmentfault.com/?enc=oJ7et3FlHtr0dMLrN6KHbA%3D%3D.FLIidWZmhveKlktRy97ZPq0wIOu6bZtgw5%2FBpQ1%2Bfwp1mcZhGMzoya9e%2BEr%2BHXD%2BH8ECIUb48Q6rR7Q9BL3qLbJJxL4d7ZPewEP9K%2FkhNT%2BCwyJz3KUSmnDMtmZBO%2FZs" rel="nofollow" target="_blank">Slider</a>组件实现音量控制滑动条，结合腾讯云点播SDK的setAudioPlayoutVolume方法进行实现。实现时，建议默认音量100，即默认系统当前音量播放。</p><p><strong>4.问题描述：</strong></p><p>音乐播放器的音频输出如何增加PCM输出模式，支持数字耳放小尾巴usb独占？</p><p><strong>解决方案：</strong></p><p>方案一：使用AudioRenderer直接播放PCM格式的音频数据。</p><p>AudioRenderer可以直接播放PCM数据，还可以通过数据预处理来实现更灵活的播放，关键代码如下：</p><ol><li>配置文件路径：</li></ol><pre><code class="TS">
let bufferSize: number = 0;

let path = getContext().cacheDir;

let filePath = path + '/StarWars10s-2C-48000-4SW.wav';

let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
</code></pre><ol start="2"><li>读取文件数据：</li></ol><pre><code class="TS">
try {

  fs.readSync(file.fd, buffer, options);

  bufferSize += buffer.byteLength;

  // 系统会判定buffer有效，正常播放。

  return audio.AudioDataCallbackResult.VALID;

} catch (error) {

  console.error('Error reading file:', error);

  // 系统会判定buffer无效，不播放。

  return audio.AudioDataCallbackResult.INVALID;

}
</code></pre><ol start="3"><li>调用start()方法进行音频渲染</li></ol><pre><code class="TS">
audioRenderer.start((err: BusinessError) =&amp;gt; {

  if (err) {

    console.error(`Renderer start failed, code is ${err.code}, message is ${err.message}`);

  } else {

    console.info('Renderer start success.');

  }

});
</code></pre><p>具体开发步骤以及完整代码可以参考<a href="https://link.segmentfault.com/?enc=6P83Zf8CGVJDxM%2FBvtV57w%3D%3D.MQ6ciVr42Q7pINrYcF1AQ15mi5dvNgJ0HodDASmbsX5x3GT3bk4lhZdwEFqE1pnfQ8fPwtyVdR5FtaQGDZ4lmZ3J7h%2FJQkGWvR292E4%2FharvJclxCU3Pn5CtLiN7ipBJIXRo2GW3BhLoQnkJTTp%2BWfaNoeYstqhlcnVSafX6bfU%3D" rel="nofollow" target="_blank">AudioRenderer的开发步骤</a>。</p><p>方案二：对PCM数据进行音频转码后使用AVPlayer播放。</p><p>AVPlayer无法直接播放PCM格式的音频数据，需要将音频数据转码封装成AVPlayer支持的格式。PCM格式数据是裸流，播放占用内存大，使用AVPlayer播放封装后的音频数据会占用更小的内存。</p><p>这里以WAV格式为例，WAV格式是一种无损的格式，可以最好地保存音频质量，如果对音频大小或者格式有其他要求，可以参考<a href="https://link.segmentfault.com/?enc=6%2BWl%2BV%2F9ekZAY3WvC2NjIA%3D%3D.2J5f5g966h9%2F2A1LE7nmXnThtnNNZetlEV3U3bepA9At%2FCevMwwdPPlDHUNtMb3v2PVRt1CcpAevbwoC0tjXe3qWiVG0ZLs3GEIuDC8EQ%2FE%3D" rel="nofollow" target="_blank">音频编码</a>和<a href="https://link.segmentfault.com/?enc=2nftjBi7FGxnrWOkSALoIw%3D%3D.OIZOryB5NysGfJURXUEm1ZZzrWJvRqMB17T9b3PDOp6jscscQNgr7ghWZaDqXxsud59g2dm0xJcD5LLivw3b7AQbCXvJviujBK%2FvCfI78AM%3D" rel="nofollow" target="_blank">媒体数据封装</a>进行其他的音频编码格式转化。</p><p>现在将PCM数据转码封装成完整的WAV文件再用AVPlayer播放，参考代码如下：</p><ol><li>定义PCM转WAV的方法，获取源文件路径和目标文件路径，分别写入WAV文件头和PCM数据，参考代码如下：</li></ol><pre><code class="TS">
public pcmToWav(src:string, dest:string){

  const inFile: fs.File = fs.openSync(src, fs.OpenMode.READ_ONLY);

  const outFile: fs.File = fs.openSync(dest, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);

  let byteRate = 16 * sampleRate * channel / 8;

  const inFileStat = fs.statSync(inFile.fd)

  // 获取源文件信息，包括文件大小等

  let audioDataSize = inFileStat.size;

  let totalDataLen = audioDataSize + 36;

  console.log('audioDataSize= ', audioDataSize)

  // 1.wav文件头编写

  this.writeWavFileHeader(outFile, audioDataSize, totalDataLen, byteRate);

  // 2.写入pcm数据

  this.writePcmData(inFile, outFile, audioDataSize)

}
</code></pre><ol start="2"><li>定义写入WAV头部信息的方法，创建一个大小为44字节的缓冲区，用于存储WAV文件的头部信息，再将其写入输出文件，参考代码如下：</li></ol><pre><code class="TS">
private writeString(dv:DataView, offset:number, str:string){

  for (let i = 0; i &amp;lt; str.length; i++) {

    dv.setUint8(offset + i, str.charCodeAt(i));

  }

}

// 定义写入WAV头文件信息的方法

private writeWavFileHeader(out:fs.File, audioDataSize:number, totalDataLen:number, byteRate:number){

  const header = new ArrayBuffer(44);

  const dv = new DataView(header);

  const bitsPerSample = 16; // 当前位深是16

  // 写入RIFF块

  this.writeString(dv, 0, 'RIFF');

  dv.setUint32(4, totalDataLen, true);

  this.writeString(dv, 8, 'WAVE');

  // 写入fmt块

  this.writeString(dv, 12, 'fmt ');

  dv.setUint32(16, 16, true); // fmt块大小

  dv.setUint16(20, 1, true); // 格式类别(PCM)

  dv.setUint16(22, channel, true); // 通道数

  dv.setUint32(24, sampleRate, true); // 采样率

  dv.setUint32(28, byteRate, true); // 比特率

  dv.setUint16(32, channel * bitsPerSample / 8, true); // 每个采样点的字节数

  dv.setUint16(34, bitsPerSample, true); // 位深

  // 写入data块

  this.writeString(dv, 36, 'data');

  dv.setUint32(40, audioDataSize, true); // 数据块大小

  console.log('audioDataSize= ', audioDataSize)

  // 将头文件信息写入输出文件

  fs.writeSync(out.fd, new Uint8Array(header).buffer, {

    length: 44

  })

}

 
</code></pre><ol start="3"><li>定义读取pcm数据的方法，将PCM数据从输入文件写入输出文件，使用fs.readSync读取输入文件的数据，并写入输出文件，直到读取完毕，参考代码如下：</li></ol><pre><code class="TS">
private writePcmData(inFile:fs.File, outFile:fs.File, audioDataSize:number){

  // 写入PCM数据

  let readSize = 0

  let data = new ArrayBuffer(audioDataSize);

  let readOptions: ReadOptions = {

    offset: readSize,

    length: audioDataSize

  };

  let readLen = fs.readSync(inFile.fd, data, readOptions);

  while (readLen &amp;gt; 0) {

    readSize += readLen;

    fs.writeSync(outFile.fd, data, { length: readLen });

    readOptions.offset = readSize;

    readLen = fs.readSync(inFile.fd, data, readOptions);

  }

  fs.closeSync(inFile.fd)

  fs.closeSync(outFile.fd)

}

 
</code></pre><ol start="4"><li>完成转码后让AVPlayer使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放，AVPlayer的具体开发流程可以参考<a href="https://link.segmentfault.com/?enc=rFO%2B%2F3k2WyzIEwYUHsgBtA%3D%3D.oVzH12nohl1ONaXsVpUUjnqxCqFb7MNvayAjAcU65427oAvwDCqR3Fpetj%2BJt0nsVy0JNaw3kIeYY2SxNxoS8RoGVvELR4owaZs0WKMElgmZhMdcGOBeHNMLgTdBg7XVVG3afpCpjnlbMPDqZQxc8A%3D%3D" rel="nofollow" target="_blank">AVPlayer播放音频完整示例</a>。</li></ol>]]></description></item><item>    <title><![CDATA[三步构建你的敏捷中枢：节点式思维对齐工具落地全攻略 Ord1naryLife ]]></title>    <link>https://segmentfault.com/a/1190000047557458</link>    <guid>https://segmentfault.com/a/1190000047557458</guid>    <pubDate>2026-01-22 11:14:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在多项目并发与复杂任务流管理的数字化协作中，传统的线性计划已难以应对灵活多变的业务需求 。如果计划编排缺乏原子化的卡片管理，可能会导致：</p><ul><li><strong>执行断层</strong>：计划背景被淹没在厚重文档中，导致执行者无法直观获取关键信息 。</li><li><strong>排期僵化</strong>：无法快速响应需求变更，导致项目排期与实际进度严重脱节。</li><li><strong>透明度缺失</strong>：团队成员难以实时了解全局节奏及各阶段的准入准出标准。</li><li><strong>资源错配</strong>：缺乏对任务依赖关系的清晰视图，容易造成资源闲置或关键路径阻塞。</li></ul><p>卡片式计划编排工具通过将模糊的项目计划转化为可灵活组合、可实时追踪、可多维对齐的卡片执行引擎，确保团队在复杂的竞争环境中实现精准交付 。</p><h2><strong>卡片式计划编排工具的核心特性</strong></h2><ul><li><strong>原子化任务卡片</strong>：将复杂计划拆解为独立卡片，封装背景、标准、工时等核心元数据 。</li><li><strong>多维可视化视图</strong>：支持看板、时间线、甘特图等多种表现形式，实现计划的直观编排 。</li><li><strong>依赖关系建模</strong>：清晰标记卡片间的逻辑关联（如包含、阻塞、并行），自动计算关键路径 。</li><li><strong>自动化流转规则</strong>：基于触发器实现卡片状态自动更新，确保计划与执行同步 。</li><li><strong>递归进度核算</strong>：底层原子卡片的执行质量自动驱动顶层计划的达成率评估。</li></ul><h2><strong>卡片式计划编排工具的重要意义</strong></h2><ol><li><strong>消除信息颗粒度偏差</strong>：通过卡片的高度封装，确保执行层与管理层在任务定义上达成高度共识 。</li><li><strong>提升排期灵活性</strong>：支持通过拖拽、连线等操作快速调整计划，大幅降低重排排期的成本。</li><li><strong>强化过程确定性</strong>：实时审计实际流转速率与排期模型的差异，实现风险的主动预警与修正 。</li><li><strong>沉淀组织标准化路径</strong>：将验证有效的编排模式固化为卡片模板，实现项目经验的快速复用。</li></ol><h2><strong>应用场景</strong></h2><ul><li><strong>敏捷迭代管理</strong>：将产品愿景拆解为 Sprint 任务卡片，驱动研发交付流高效流转。</li><li><strong>复杂项目规划</strong>：在启动阶段梳理各模块间的依赖链路，利用卡片编排规避交付冲突 。</li><li><strong>资源负载均衡</strong>：通过可视化看板监控各环节卡片堆积情况，实现动态的人力资源调度。</li><li><strong>跨团队协同</strong>：通过共享的计划卡片池，对齐跨职能部门的协作节奏与产出标准 。</li></ul><h2>---</h2><p><strong>5款值得尝试的卡片式计划编排工具</strong></p><h3><strong>1. 板栗看板</strong></h3><p>直观的任务流转与多层级穿透</p><ul><li><strong>特点</strong>：支持任务卡片的无限层级嵌套，通过看板视图展示计划的深度编排逻辑。</li><li><strong>优势</strong>：看板视图极度直观，支持卡片逻辑连线，适合追求过程透明的敏捷团队。</li><li><strong>适合团队</strong>：需要快速响应并对计划进行纵向穿透的小型和中型研发团队 。</li></ul><h3><strong>2. ClickUp</strong></h3><p>全功能任务编排与数据看板平台</p><ul><li><strong>特点</strong>：提供强大的“目标”模块，支持将微观卡片进度自动聚合为宏观指标。</li><li><strong>优势</strong>：支持极高维度的自定义，能根据卡片元数据生成复杂的排期审计报告。</li><li><strong>适合团队</strong>：需要对大规模计划进行参数化管理和深度数据分析的团队 。</li></ul><h3><strong>3. Trello</strong></h3><p>简单轻量的卡片流转工具</p><ul><li><strong>特点</strong>：强调“清单化”的计划编排，支持丰富的卡片封面与标签分类 。</li><li><strong>优势</strong>：操作极简，学习曲线极低，适合快速搭建基础的交付工作流 。</li><li><strong>适合团队</strong>：注重任务分类和灵活调整、倾向于视觉驱动型协作的团队 。</li></ul><h3><strong>4. Jira Software</strong></h3><p>工业级标准与自动化编排引擎</p><ul><li><strong>特点</strong>：拥有严密的权限与流程控制逻辑，支持复杂的卡片依赖与版本排期。</li><li><strong>优势</strong>：可与代码仓库深度集成，实现从“计划编排”到“自动执行”的闭环审计。</li><li><strong>适合团队</strong>：追求高度标准化执行、有严格合规与闭环审计需求的大型组织。</li></ul><h3><strong>5. Monday.com</strong></h3><p>高度自由的卡片式协同看板</p><ul><li><strong>特点</strong>：支持看板与时间轴、工作负荷视图的实时联动，动态展示卡片状态。</li><li><strong>优势</strong>：视觉色彩丰富，支持强大的自动化集成，能显著提升团队编排兴趣。</li><li><strong>适合团队</strong>：强调团队协同氛围、需要灵活配置复杂编排场景的项目组。</li></ul><h2>---</h2><p><strong>如何选择合适的卡片式计划编排工具？</strong></p><h3><strong>1. 按团队规模选择</strong></h3><ul><li><strong>小型团队（1-10人）</strong>：推荐 <strong>板栗看板</strong>、Trello 等工具，侧重于快速启动与核心任务的直观流转。</li><li><strong>中型团队（10-50人）</strong>：适合使用 <strong>Monday.com</strong>、ClickUp，支持更复杂的多维对齐与资源核算 。</li><li><strong>大型团队（50+人）</strong>：建议选择 <strong>Jira</strong> 或 <strong>ClickUp</strong>，这些工具提供强大的层级管理与权限隔离功能。</li></ul><h3><strong>2. 按计划复杂度选择</strong></h3><ul><li><strong>线性任务</strong>（如内容生产、日常运营）：选择 <strong>板栗看板</strong>、Trello 等简洁易用的视图工具 。</li><li><strong>交叉任务</strong>（如软件研发、系统重构）：推荐 <strong>Jira</strong>、<strong>板栗看板</strong>等支持深度连线与递归逻辑核算的专业平台。</li></ul><h2>---</h2><p><strong>提升计划编排效率的小建议</strong></p><ol><li><strong>坚持卡片原子化</strong>：确保每张卡片描述的是最小可执行单元，避免职责模糊。</li><li><strong>设置基准流转速率</strong>：定期审计实际完成时长，为后续计划编排提供真实的数据支撑。</li><li><strong>建立风险预警连线</strong>：为关键路径上的卡片设置依赖预警，确保下游环节能提前预知变动 。</li><li><strong>定期进行计划“减脂”</strong>：及时清理、归档过时卡片，保持编排体系的干练与精准执行力。</li></ol><h2>---</h2><p><strong>总结</strong></p><p>卡片式计划编排工具是管理组织执行复杂性的关键手段。通过 板栗看板、ClickUp、Jira 等工具，团队能够将宏观的战略意图精准解构为微观的原子卡片，实现“计划-执行-状态”的实时对齐。</p><p>精准的编排，是高效交付的基石。</p>]]></description></item><item>    <title><![CDATA[通过华为账号识别用户风险，降低业务损失 HarmonyOS_SDK ]]></title>    <link>https://segmentfault.com/a/1190000047557470</link>    <guid>https://segmentfault.com/a/1190000047557470</guid>    <pubDate>2026-01-22 11:13:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当应用平台组织诸如秒杀、抽奖等营销活动时，经常会遭遇"薅羊毛"行为，给业务方带来不小的经费损失。比如通过虚假手机号进行批量注册，多次参加活动；又比如，当应用商户进行红包补贴、优惠券发放等营销活动时，使用脚本或模拟器"薅羊毛"。</p><p>为避免该问题，HarmonyOS SDK华为<a href="https://link.segmentfault.com/?enc=sX%2BIPD%2F70xvGNS6yJUNEGg%3D%3D.reqgaH%2FMt6zohzPOtvbXsJAnJV%2FPbha%2BCzAvdV2rufoXFSII%2FlyjSVCHAvCCYpsi860F9n%2BBdYt8mPXdY%2B8ICGJttEPE7bCaDv0mrHJrtkM%3D" rel="nofollow" title="账号服务" target="_blank">账号服务</a>（Account Kit）提供了获取用户风险等级的能力，能够有效识别恶意场景，提前防范业务风险。</p><h3>应用场景</h3><p>一、应用登录风控场景：</p><p>当用户使用华为账号关联登录应用时，开发者可通过华为账号获取用户风险等级的能力获取用户账号的风险等级，对高风险等级账号进行风控，提升应用的安全等级。</p><p>二、营销活动反作弊场景：</p><p>在应用进行营销活动期间，如进行商户补贴、优惠券发放等商业营销活动时获取华为账号风险等级，协助开发者有效识别"薅羊毛"风险；保护营销资源合理使用，降低业务安全问题给营销方带来的损失，为相关活动保驾护航。</p><h3>风险等级</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557472" alt="" title=""/></p><h3>获取用户风险等级方式</h3><p>一、 通过华为账号一键登录获取用户风险等级。</p><p>在应用登录风控场景中，开发者可以通过华为账号一键登录获取用户风险等级，对恶意账号进行风控，提升应用的安全等级。<br/>大致业务流程如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557473" alt="" title="" loading="lazy"/></p><p>通过华为账号一键登录获取用户风险等级的开发，需要建立在一键登录的开发基础上。在进行代码开发前，请确认已经完成一键登录的开发准备工作，然后申请对应的scope权限，接着就可以进行客户端部分的开发。</p><p>在客户端开发部分，需要参考<a href="https://link.segmentfault.com/?enc=hmy0vB5YMmO7sokkzg9FkQ%3D%3D.eL%2B5pOEuM71bUzlJvVFQEdq%2FHni9fNHeAReTTrfoCXMXxyzdvYvpiYa%2FmT92AQ2yJtbPwvHJb6hOx1uIi3e%2BTkTxD0vV%2BccluVndxewX5%2FRyq7jDQ9sdamy6SrHTuKhC7PfsZ%2FWgzZHIpet4vELZPiQxO3PtwcNvwhOiQDOqiMk%3D" rel="nofollow" title="一键登录开发流程步骤" target="_blank">一键登录开发流程步骤</a>1及步骤2，确保系统账号已登录，匿名手机号获取成功，且用户首次通过华为账号登录该应用。接着再参考步骤3的示例代码，在LoginWithHuaweiIDButton组件参数params中设置riskLevel标识为true，其余示例代码保持不变，拉起应用登录页。</p><pre><code>LoginWithHuaweiIDButton({
  params: {
    // LoginWithHuaweiIDButton支持的样式
    style: loginComponentManager.Style.BUTTON_RED,
    // 账号登录按钮在登录过程中展示加载态
    extraStyle: {
      buttonStyle: new loginComponentManager.ButtonStyle().loadingStyle({
        show: true
      })
    },
    // LoginWithHuaweiIDButton的边框圆角半径
    borderRadius: 24,
    // LoginWithHuaweiIDButton支持的登录类型
    loginType: loginComponentManager.LoginType.QUICK_LOGIN,
    // LoginWithHuaweiIDButton支持按钮的样式跟随系统深浅色模式切换
    supportDarkMode: true,
    // verifyPhoneNumber：如果华为账号用户在过去90天内未进行短信验证，是否拉起Account Kit提供的短信验证码页面
    verifyPhoneNumber: true,
    // riskLevel：标识应用期望在登录后获取华为账号的风险等级
    riskLevel: true,
  },
  controller: this.controller
})
</code></pre><p>用户同意协议并点击一键登录按钮后，可获取到Authorization Code，并在服务端使用Client ID、Client Secret、Authorization Code调用<a href="https://link.segmentfault.com/?enc=7%2Ft2vnnKrw5Fc4kDFn9tew%3D%3D.ioy4KSDAiVk8KChTvWiH%2BSANa1dnYwFnAeyCX11DianRu4EZSyzBiqSph9YDjC%2BEzhuAuIp4ychNLowmamC3QSrE%2FA91sNtWE0la4wULsv73deNb5Z02YsaC%2Fb4fxCwQ4mbw4bMqgkWWR%2BruvU0%2BIx4WDzMeLWL2Z2hME5TPaIeab15U%2FbioElVVv4zNFUBb" rel="nofollow" title="获取用户级凭证接口" target="_blank">获取用户级凭证接口</a>向华为账号服务器请求获取Access Token，最后使用Access Token调用获取用户风险等级接口获取用户的风险等级。</p><pre><code>import com.alibaba.fastjson2.JSONArray;
import com.alibaba.fastjson2.JSONObject;
import lombok.extern.slf4j.Slf4j;
import org.apache.http.client.methods.CloseableHttpResponse;
import org.apache.http.client.methods.HttpPost;
import java.io.IOException;
import java.util.HashMap;
import java.util.Map;
import java.util.Objects;

/**
 * 获取用户风险等级
 */
@Slf4j
public class GetUserRiskLevelDemo {
    public static void main(String[] args) throws IOException {
        // 获取用户风险等级的接口URL
        String url = "https://account.cloud.huawei.com/user/getuserrisklevel";
        // 替换为您实际的Client ID
        String clientID = "&lt;Client ID&gt;";
        // 替换为您实际的transactionID
        String transactionID = "&lt;transactionID&gt;";
        // 替换为您实际的获取到的用户级凭证Access Token
        String accessToken = "&lt;Access Token&gt;";
        // 替换为您实际的scene
        String scene = "&lt;scene&gt;";
        JSONObject result = getUserRiskLevel(url, clientID, transactionID, accessToken, scene);
        // 解析获取errCode
        Integer errCode = result.getInteger("errCode");
        // 解析获取errMsg
        String errMsg = result.getString("errMsg");
        // 解析获取riskLevel
        Integer riskLevel = result.getInteger("riskLevel");
        // 解析获取riskTag
        JSONArray riskTag = result.getJSONArray("riskTag");
    }

    private static JSONObject getUserRiskLevel(String url, String clientID, String transactionID,
        String accessToken, String scene) throws IOException {
        HttpPost httpPost = new HttpPost(url + "?" + "clientID=" + clientID + "&amp;transactionID=" + transactionID);
        Map&lt;String, String&gt; reqBody = new HashMap&lt;&gt;();
        reqBody.put("accessToken", accessToken);
        reqBody.put("scene", scene);
        httpPost.setHeader("Content-Type", "application/json;charset=utf-8");
        httpPost.setEntity(CallUtils.wrapJsonEntity(reqBody));
        return CallUtils.toJsonObject(CallUtils.remoteCall(httpPost, (CloseableHttpResponse response, String rawBody) -&gt; {
            int statusCode = response.getStatusLine().getStatusCode();
            // http状态码不是200，请求失败
            if (statusCode != 200) {
                return new IOException("call failed! http status code: " + statusCode + ", response data: " + rawBody);
            }
            // http状态码为200，解析响应的body，判断业务错误码
            JSONObject errorResponseBody = CallUtils.toJsonObject(rawBody);
            // 错误码
            Integer errCode = errorResponseBody.getInteger("errCode");
            // errCode为0表示成功，非0表示失败
            if (Objects.nonNull(errCode) &amp;&amp; errCode != 0) {
                return new IOException("call failed! http status code: " + statusCode + ", response data: " + rawBody);
            }
            return null;
        }));
    }
}
</code></pre><p>二、 通过华为账号其他方式登录获取用户风险等级。</p><p>在应用已使用华为账号关联登录的场景中，开展商户补贴、优惠券发放等商业营销活动时，开发者可通过华为账号其他方式登录获取华为账号风险等级，有效识别"薅羊毛"风险，保护营销资源合理使用。</p><p>大致业务流程如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557474" alt="" title="" loading="lazy"/></p><p>通过华为账号其他方式登录获取用户风险等级的开发步骤同样分为客户端开发和服务端开发。客户端开发步骤如下：</p><ol><li><p>首先导入authentication模块及相关公共模块。</p><p>import { authentication } from '@kit.AccountKit';<br/> import { hilog } from '@kit.PerformanceAnalysisKit';<br/> import { util } from '@kit.ArkTS';<br/> import { BusinessError } from '@kit.BasicServicesKit';</p></li><li><p>然后创建授权请求并设置参数。</p><p>// 创建授权请求，并设置参数<br/> const authRequest = new authentication.HuaweiIDProvider().createAuthorizationWithHuaweiIDRequest();<br/> // 获取风险等级需要传如下scope<br/> authRequest.scopes = ['riskLevel'];<br/> // 获取authorizationCode需传如下permission<br/> authRequest.permissions = ['serviceauthcode'];<br/> // 用户是否需要登录授权，该值为true且用户未登录或未授权时，会拉起用户登录或授权页面<br/> authRequest.forceAuthorization = true;<br/> // 用于防跨站点请求伪造<br/> authRequest.state = util.generateRandomUUID();</p></li><li><p>调用AuthenticationController对象的executeRequest方法执行授权请求，并处理授权结果，从授权结果中解析出authorizedScopes和Authorization Code。</p><p>// 执行授权请求<br/> try {<br/>   // 此示例为代码片段，实际需在自定义组件实例中使用，以获取UIContext对象作为函数入参<br/>   const controller = new authentication.AuthenticationController(this.getUIContext().getHostContext());<br/>   controller.executeRequest(authRequest).then((data) =&gt; {</p><pre><code> const authorizationWithHuaweiIDResponse = data as authentication.AuthorizationWithHuaweiIDResponse;
 const state = authorizationWithHuaweiIDResponse.state;
 if (state &amp;&amp; authRequest.state !== state) {
   hilog.error(0x0000, 'testTag', `Failed to authorize. The state is different, response state: ${state}`);
   return;
 }
 hilog.info(0x0000, 'testTag', 'Succeeded in authentication.');
 let riskLevelAuthorized: boolean = false;
 const authorizationWithHuaweiIDCredential = authorizationWithHuaweiIDResponse?.data;
 const authorizedScopes = authorizationWithHuaweiIDCredential?.authorizedScopes;
 // 判断授权成功scopes中是否包含riskLevel
 if (authorizedScopes?.includes("riskLevel")) {
     riskLevelAuthorized = true;
 }
 const authorizationCode = authorizationWithHuaweiIDCredential?.authorizationCode;
 // 开发者处理riskLevelAuthorized, authorizationCode</code></pre><p>}).catch((err: BusinessError) =&gt; {</p><pre><code> dealAllError(err);</code></pre><p>});<br/> } catch (error) {<br/>   dealAllError(error);<br/> }<br/> // 错误处理<br/> function dealAllError(error: BusinessError): void {<br/>   hilog.error(0x0000, 'testTag', <code>Failed to obtain userInfo. Code: ${error.code}, message: ${error.message}</code>);<br/>   // 在应用获取用户风险等级场景下，涉及UI交互时，建议按照如下错误码指导提示用户<br/>   if (error.code === ErrorCode.ERROR_CODE_LOGIN_OUT) {</p><pre><code> // 用户未登录华为账号，请登录华为账号并重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_NETWORK_ERROR) {</p><pre><code> // 网络异常，请检查当前网络状态并重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_USER_CANCEL) {</p><pre><code> // 用户取消授权</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_SYSTEM_SERVICE) {</p><pre><code> // 系统服务异常，请稍后重试</code></pre><p>} else if (error.code === ErrorCode.ERROR_CODE_REQUEST_REFUSE) {</p><pre><code> // 重复请求，应用无需处理</code></pre><p>} else {</p><pre><code> // 获取用户信息失败，请稍后重试</code></pre><p>}<br/> }</p><p>export enum ErrorCode {<br/>   // 账号未登录<br/>   ERROR_CODE_LOGIN_OUT = 1001502001,<br/>   // 网络错误<br/>   ERROR_CODE_NETWORK_ERROR = 1001502005,<br/>   // 用户取消授权<br/>   ERROR_CODE_USER_CANCEL = 1001502012,<br/>   // 系统服务异常<br/>   ERROR_CODE_SYSTEM_SERVICE = 12300001,<br/>   // 重复请求<br/>   ERROR_CODE_REQUEST_REFUSE = 1001500002<br/> }</p></li><li>在客户端开发完成后，同样需要调用获取用户级凭证接口向华为账号服务器请求获取Access Token，并使用Access Token调用获取用户风险等级接口获取用户的风险等级。</li></ol><p><strong>了解更多详情\&gt;\&gt;</strong></p><p>访问<a href="https://link.segmentfault.com/?enc=rOSISqDiGf9fjMViupaxOA%3D%3D.AB6OVB0b0q%2BI9NShADrTIpqG55SRM3tvTVWkBuNXSGhoj6SXDR%2BhetAVjNkHpyalJay1rgBfRG0LuEK8bMrkf4Vb%2FhKAq%2BsQQ2vo7JmHyLU%3D" rel="nofollow" title="华为账号服务联盟官网" target="_blank">华为账号服务联盟官网</a></p><p>获取<a href="https://link.segmentfault.com/?enc=ZuCANyugtp1ZUXt63M%2BLEA%3D%3D.uGi5ofcMWWBT8PAAC0JkhyL0xNqMFUqSQdr7N0SguFZ45wxOBc9RVL1pb7k%2B4r8vULZo03Ej1VhU8WK5yOELHyX7l5xCboliIWTXGIGiSsYNTtgzskBr1Kmz8hBnmIIBlMcIlwhNeWLdFKMxYEmjKQ%3D%3D" rel="nofollow" title="获取风险等级开发指导文档" target="_blank">获取风险等级开发指导文档</a></p>]]></description></item><item>    <title><![CDATA[2026最新CRM横评：5 大客户管理系统能力对比 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557543</link>    <guid>https://segmentfault.com/a/1190000047557543</guid>    <pubDate>2026-01-22 11:12:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在流量红利消退、客户运营进入“精细化”阶段的当下，CRM（客户关系管理系统）已从“数据存储工具”升级为“客户价值增长引擎”。其核心能力——<strong>客户中心、客户信息管理、RFM分组分析、复购流失预警</strong>——直接决定了企业对客户需求的洞察深度与运营效率。</p><p>本文选取<strong>超兔一体云（全流程型）、Free CRM（轻量化型）、Streak（Gmail集成型）、OKKICRM（外贸专业型）四大主流品牌，从能力逻辑、场景适配、优势差异</strong>三个维度展开深度对比，为企业选择提供清晰框架。</p><h2>一、四大核心维度能力对比</h2><h3>（一）客户中心：从“流程覆盖”到“场景协同”</h3><p>客户中心是CRM的“大脑”，负责整合客户全生命周期的互动数据，支撑销售、服务的协同。四大品牌的定位差异显著：</p><table><thead><tr><th>品牌</th><th>核心逻辑</th><th>场景适配</th><th>核心优势</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>全流程闭环（线索→跟进→合约→售后）</td><td>中大型企业、全渠道运营</td><td>1. 五大跟单模型（适配不同业务场景）；2. 全流程执行（订单→开票）；3. 售后RFM挖掘复购</td></tr><tr><td><strong>Free CRM</strong></td><td>轻量化全生命周期（潜在→成交→维护）</td><td>中小企业、基础客户运营</td><td>1. 分组+标签管理；2. 回访/到期提醒；3. 易上手</td></tr><tr><td><strong>Streak</strong></td><td>Gmail内协同（邮件→笔记→团队共享）</td><td>外贸/服务团队、依赖邮件沟通</td><td>1. 无需切换工具；2. 团队信息同步；3. 自定义工作流</td></tr><tr><td><strong>OKKICRM</strong></td><td>外贸场景跟进（邮件聚合→联系人卡片）</td><td>外贸企业、跨境电商</td><td>1. 多端同步；2. 外贸客户画像；3. 邮件沟通整合</td></tr></tbody></table><h4>深度解析：超兔的“全流程闭环”优势</h4><p>超兔的客户中心以“<strong>数据端到端流动</strong>”为核心，解决了传统CRM“信息孤岛”的痛点：</p><ul><li><strong>线索层</strong>：通过微信智能名片、百度广告等多渠道获客，用“用户画像云图”识别高价值客群；</li><li><strong>跟进层</strong>：用“三一客节点”（定性：有价值/无价值；定级：大单/小单；定量：金额/时间）+“五大跟单模型”（客户/销售机会/多方项目/组织型/配置单）精准判断客户潜力；</li><li><strong>执行层</strong>：支持服务型、贸易型、非标定制型合约，实现“订单→采购→发货→收款→开票”全流程可视化；</li><li><strong>售后层</strong>：通过RFM分析分层客户，用“客服控制台+工单管理”挖掘复购（如某家居品牌用超兔，售后工单触发复购率提升25%）。</li></ul><h3>（二）客户信息管理：从“存储”到“全景洞察”</h3><p>客户信息是CRM的“燃料”，其<strong>完整性、准确性、可访问性</strong>直接影响后续分析的有效性。四大品牌的能力差异体现在“数据来源”与“整合方式”：</p><table><thead><tr><th>品牌</th><th>数据收集</th><th>整合能力</th><th>权限管理</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>多渠道（拍名片/微信/工商信息抓取）</td><td>全景视图（基本信息+交易+跟单时间线）</td><td>全局自动权限（上级管下级，同级隔离）</td></tr><tr><td><strong>Free CRM</strong></td><td>批量导入+去重+多条件搜索</td><td>自定义字段（如“客户偏好”）</td><td>修改/删除权限控制</td></tr><tr><td><strong>Streak</strong></td><td>Gmail自动捕获（邮件/笔记/通话）</td><td>完整客户视图（历史互动记录）</td><td>团队共享（自动同步成员数据）</td></tr><tr><td><strong>OKKICRM</strong></td><td>多端同步+邮件聚合</td><td>联系人卡片（快速识别客户类型）</td><td>基础角色权限</td></tr></tbody></table><h4>案例：超兔的“全景信息展示”价值</h4><p>某零售企业用超兔管理客户，点击客户档案可看到：</p><ul><li><strong>基本信息</strong>：姓名、电话、地址、工商信息（自动抓取）；</li><li><strong>交易记录</strong>：近1年购买时间、金额、商品；</li><li><strong>跟单时间线</strong>：销售A在3月1日跟进，沟通内容是“需求沙发”；销售B在3月15日跟进，发送“新品沙发图册”；</li><li><strong>售后记录</strong>：4月5日反馈“沙发异响”，工单已处理。 这种“全景视图”让销售快速掌握客户全貌，避免“重复沟通”或“信息遗漏”。</li></ul><h3>（三）RFM分组分析：从“经验判断”到“数据分层”</h3><p>RFM模型（最近购买时间Recency、购买频率Frequency、消费金额Monetary）是客户价值分层的经典工具，四大品牌的能力差异体现在“自动化”与“灵活性”：</p><table><thead><tr><th>品牌</th><th>RFM计算方式</th><th>分层逻辑</th><th>动态调整</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>自动统计（R：最近1次购买；F：次数；M：金额）</td><td>预设规则（如R≤30天为“近”）</td><td>实时更新（客户行为变化→分层自动调整）</td></tr><tr><td><strong>Free CRM</strong></td><td>手动/自动计算</td><td>标准分层（高价值/潜在价值/低活跃）</td><td>手动更新</td></tr><tr><td><strong>Streak</strong></td><td>自定义维度（如R=最近30天）</td><td>组合分群（如R近+F高+M高=高价值）</td><td>手动调整</td></tr><tr><td><strong>OKKICRM</strong></td><td>未明确提及</td><td>无</td><td>无</td></tr></tbody></table><h4>超兔RFM分析流程图（Mermaid可视化）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557545" alt="" title=""/></p><pre><code>flowchart TD
    A[数据收集] --&gt; B[R/F/M指标计算]
    B --&gt; C[规则匹配]
    C --&gt; D[客户分层]
    D --&gt; E[策略制定]
    E --&gt; F[行为监测]
    F --&gt; B[动态更新]
    注：A=收集客户购买时间/次数/金额；B=自动计算R（最近1次）、F（近1年次数）、M（近1年总额）；C=匹配预设规则（如R≤30天为“近”）；D=分“重要价值/重要发展/一般挽留”等；E=对不同层制定策略（如重要价值客户推VIP服务）；F=监测客户新购买行为；B=实时更新R/F/M值</code></pre><h3>（四）复购流失预警：从“被动挽回”到“主动预防”</h3><p>复购流失预警是CRM的“预警雷达”，通过数据模型识别风险客户，提前干预。四大品牌的能力差异体现在“预警精度”与“干预手段”：</p><table><thead><tr><th>品牌</th><th>预警触发逻辑</th><th>干预方式</th><th>效果追踪</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>消费间隔分析（如历史平均2个月购买，超过3个月触发）</td><td>短信/邮件/内部通知+跟单模型</td><td>追踪客户跟进结果</td></tr><tr><td><strong>Free CRM</strong></td><td>长期未消费（如3个月无订单）</td><td>短信/邮件提醒+优惠券推送</td><td>导出列表人工跟进</td></tr><tr><td><strong>Streak</strong></td><td>自定义规则（如超过60天未下单）</td><td>Gmail邮件模板+邮件追踪</td><td>查看客户是否打开邮件</td></tr><tr><td><strong>OKKICRM</strong></td><td>客户未沟通提醒（如30天未联系）</td><td>跟进提醒+邮件沟通</td><td>基础结果记录</td></tr></tbody></table><h4>案例：Streak的“邮件预警”效率</h4><p>某外贸公司用Streak设置“超过60天未下单”为预警规则：</p><ol><li>系统触发任务提醒，通知销售；</li><li>销售直接在Gmail中打开“客户邮件模板”（如“您好，您已有2个月未下单，点击领取专属8折券”）；</li><li>通过“邮件追踪”查看客户是否打开，若未打开则再次跟进。 结果：该公司流失率从18%降至10%，复购率提升15%。</li></ol><h2>二、雷达图：四大品牌综合能力评分（1-5分）</h2><p>雷达图从客户中心（C）、客户信息（I）、RFM分析（R）、复购预警（W）四个维度打分，直观展示品牌综合实力：</p><table><thead><tr><th>品牌</th><th>客户中心（C）</th><th>客户信息（I）</th><th>RFM分析（R）</th><th>复购预警（W）</th><th>综合得分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>4.8</td><td>4.7</td><td>4.6</td><td>4.5</td><td>4.65</td></tr><tr><td>Streak</td><td>4.2</td><td>4.1</td><td>3.9</td><td>3.8</td><td>4.00</td></tr><tr><td>Free CRM</td><td>3.8</td><td>3.7</td><td>3.6</td><td>3.5</td><td>3.65</td></tr><tr><td>OKKICRM</td><td>3.0</td><td>3.2</td><td>2.0</td><td>2.5</td><td>2.67</td></tr></tbody></table><h2>三、场景适配建议：选对工具比“功能全”更重要</h2><table><thead><tr><th>企业类型/需求</th><th>推荐品牌</th><th>核心原因</th></tr></thead><tbody><tr><td>中大型企业、需要全流程客户运营</td><td>超兔一体云</td><td>全流程闭环，数据驱动，支持深度运营</td></tr><tr><td>中小企业、预算有限、基础客户管理</td><td>Free CRM</td><td>轻量化，易上手，满足基础运营需求</td></tr><tr><td>外贸/服务团队、依赖邮件沟通</td><td>Streak</td><td>Gmail内无缝集成，团队协同效率高</td></tr><tr><td>外贸企业、跨境电商</td><td>OKKICRM</td><td>外贸场景适配，邮件聚合+客户画像</td></tr></tbody></table><h2>四、结论：CRM的本质是“客户价值增长”</h2><p>CRM的核心不是“功能多”，而是“<strong>适配业务场景</strong>”——</p><ul><li>若需要<strong>深度全流程运营</strong>，超兔的“全流程闭环”能解决信息孤岛问题；</li><li>若<strong>依赖邮件协同</strong>，Streak的“Gmail集成”能降低团队学习成本；</li><li>若做<strong>外贸业务</strong>，OKKICRM的“场景适配”能提升跟进效率。</li></ul><p>未来，CRM的趋势是“<strong>更场景化+更自动化+更智能化</strong>”，企业应根据自身业务特点选择适配的工具，将“客户数据”转化为“客户价值”，实现从“流量获取”到“客户终身价值”的跨越。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[2026客户管理系统选型指南：7 款主流CRM功能对比 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557560</link>    <guid>https://segmentfault.com/a/1190000047557560</guid>    <pubDate>2026-01-22 11:11:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、背景：中小企业的“成长阵痛”与破局之道</h2><p>对于中小企业而言，<strong>数据孤岛</strong>（各部门数据割裂）、<strong>流程低效</strong>（重复录入、人工干预多）、<strong>决策盲目</strong>（缺乏数据支撑）是阻碍精细化运营的三大核心痛点。而“<strong>数据统计分析引擎（打通数据→驱动决策）+ 业务流程自动化（标准化流程→提升效率）</strong> ”的组合，正是解决这些痛点的关键路径——通过数据整合实现“明明白白做决策”，通过流程自动化实现“规规矩矩做执行”，最终形成“数据-流程-决策”的闭环管控。</p><p>本文选取<strong>超兔一体云、YetiForce、Dolibarr、ClickUp、微盟CRM、Keap、Veeva CRM</strong>七大品牌，围绕“数据统计分析引擎”“业务流程自动化”两大核心维度，结合适配场景、实施成本等辅助指标，展开深度横评，为中小企业提供选型参考。</p><h2>二、对比维度定义：从“能力到价值”的分层拆解</h2><p>本次对比基于“<strong>能力落地→价值实现</strong>”逻辑，设置三大核心维度+四大辅助维度：</p><table><thead><tr><th>维度类型</th><th>具体指标</th><th>价值指向</th></tr></thead><tbody><tr><td><strong>核心能力1：数据统计分析引擎</strong></td><td>数据整合能力（全链路/跨模块）、分析深度（自定义/多维度/关联分析）、决策支持（可视化/趋势/预测）</td><td>解决“数据孤岛”，支撑精准决策</td></tr><tr><td><strong>核心能力2：业务流程自动化</strong></td><td>自动化覆盖场景（销售/采购/生产/财务）、规则灵活性（自定义/AI生成）、集成能力（第三方工具/生态）</td><td>解决“流程低效”，提升执行效率</td></tr><tr><td><strong>辅助维度</strong></td><td>适配场景（行业/规模）、实施成本（开源/订阅/定制）、数据安全（存储/合规）、技术门槛（是否需技术团队）</td><td>匹配企业实际需求</td></tr></tbody></table><h2>三、七大品牌核心能力深度解析</h2><h3>（一）超兔一体云：全业务一体化的“闭环管控专家”</h3><p><strong>品牌定位</strong>：SaaS模式，面向全行业中小企业的全业务运营平台（CRM+进销存+供应链+财务+生产）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：覆盖“客户→销售→采购→生产→财务”全链路数据，底层打通无孤岛；</li><li><strong>分析深度</strong>：提供<strong>五大核心引擎</strong>——①工作台自定义（数字/图表卡片可视化）、②同比环比（趋势波动分析）、③多表聚合（跨模块关联分析，如销售→库存周转率）、④关联表复合查询（如客户历史订单+回款+售后的360°视图）、⑤单日KPI（实时监控单日销售额/订单量）；</li><li><strong>决策支持</strong>：通过“可视化仪表盘+精准报表”直接输出决策依据（如通过库存周转率分析优化采购计划）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：从销售跟进（客户意向→自动生成跟进任务）、采购管理（订单→自动触发采购计划+拆分供应商）到财务结算（订单签约→自动拆分多期应收），覆盖全业务环节；</li><li><strong>规则灵活性</strong>：支持<strong>自然语言AI生成工作流</strong>（如“当客户标记为‘高意向’时，自动分配给销售A+发送跟进提醒”），流程步骤可关联数据动作（如修改客户状态后同步更新库存）；</li><li><strong>集成能力</strong>：支持用友/金蝶ERP、WMS等外部系统对接，通过RPA插件实现网页自动化（如自动同步电商订单）。</li></ul><p><strong>优势</strong>：全业务一体化架构+AI能力+低成本客制化（自选功能订阅）；<strong>劣势</strong>：需依赖SaaS服务，部分高度定制需求需额外配置。</p><h3>（二）YetiForce：开源模块化的“技术派之选”</h3><p><strong>品牌定位</strong>：开源CRM（基于Vtiger改进），面向有技术团队的大中型企业/中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：打通“线索→现金流”全链路数据（客户→订单→库存→财务）；</li><li><strong>分析深度</strong>：支持<strong>自定义仪表盘</strong>（实时展示销售漏斗/库存预警）、<strong>趋势分析</strong>（同比/环比看业务增长）；</li><li><strong>决策支持</strong>：通过多表关联分析（如客户活跃度→复购率）辅助优化运营策略。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：常规场景（线索分配、客户跟进提醒、邮件自动发送）+ 插件扩展（如制造行业的“采购→生产”协同流程）；</li><li><strong>规则灵活性</strong>：通过插件二次开发适配个性化需求（如企业自定义“售后工单→配件采购”流程）；</li><li><strong>集成能力</strong>：支持Git等开发工具集成，适配技术团队的定制需求。</li></ul><p><strong>优势</strong>：开源低部署成本+GDPR数据安全+模块化扩展（从CRM到全业务）；<strong>劣势</strong>：需技术团队维护，非技术型企业上手门槛高。</p><h3>（三）Dolibarr：本土化适配的“中小制造/零售之友”</h3><p><strong>品牌定位</strong>：模块化SaaS/开源系统，面向国内中小企业（支持中文/人民币/增值税）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：整合客户、销售、库存、财务数据，支持自有服务器存储；</li><li><strong>分析深度</strong>：提供<strong>业务场景报表</strong>（销售趋势/客户贡献度）+<strong>财务报表</strong>（利润表/增值税申报表），支持多维度筛选（如按地区/产品看销量）；</li><li><strong>决策支持</strong>：通过库存预警报表（如“某产品库存低于安全值时提醒采购”）降低库存积压。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：跨模块自动同步（客户录入→自动同步至订单/库存/财务）、生产流程数字化（BOM管理→生产订单跟踪→库存自动扣减）；</li><li><strong>规则灵活性</strong>：模块化设计，可选择“客户管理+库存+财务”组合，适配灵活业务模式；</li><li><strong>集成能力</strong>：支持支付接口（如支付宝/微信）、物流系统对接。</li></ul><p><strong>优势</strong>：本土化功能完善+数据自有存储+生产流程管控；<strong>劣势</strong>：分析深度较浅，复杂关联分析需额外开发。</p><h3>（四）ClickUp：轻量级协作的“任务型管控工具”</h3><p><strong>品牌定位</strong>：SaaS协同工具，面向中小团队的任务型业务管控。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：一体化工作区整合任务、销售、项目数据，支持多视图（表格/看板/日历）展示；</li><li><strong>分析深度</strong>：自定义Dashboard（实时展示任务进度/销售漏斗），支持过滤筛选（如“只看销售A的未完成任务”）；</li><li><strong>决策支持</strong>：通过任务进度分析（如“某项目延期率高→优化资源分配”）提升协作效率。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：任务自动分配（如“当任务标记为‘紧急’时，自动分配给团队 leader”）、状态变更提醒（如“客户订单完成→自动通知财务开票”）；</li><li><strong>规则灵活性</strong>：提供100+触发器（如“当Git提交代码时，自动更新任务状态”），支持低代码配置；</li><li><strong>集成能力</strong>：支持Git、Slack等工具集成，适配技术/互联网团队。</li></ul><p><strong>优势</strong>：轻量级易上手+多视图数据整合；<strong>劣势</strong>：全业务覆盖能力弱，适合任务型而非复杂流程管控。</p><h3>（五）微盟CRM：私域运营的“生态联动专家”</h3><p><strong>品牌定位</strong>：SaaS CRM，面向依赖微信生态的零售/餐饮等中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：与微信生态深度联动（公众号/小程序/企业微信），整合私域客户数据（如扫码轨迹/聊天记录/消费行为）；</li><li><strong>分析深度</strong>：生成<strong>360°客户画像</strong>（性别/地域/消费偏好），支持RFM模型分析（复购率/客户价值分级）；</li><li><strong>决策支持</strong>：通过“高价值客户→定向运营”“流失客户→召回策略”提升私域转化。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：基于微信生态的自动化触达（如“客户扫码关注→自动推送欢迎语+打标签”“会员生日→自动发送优惠券”）、销售周期监控（如“客户30天未复购→自动触发流失预警”）；</li><li><strong>规则灵活性</strong>：支持根据客户标签自定义触达规则（如“标签为‘宝妈’的客户→推送母婴产品优惠”）；</li><li><strong>集成能力</strong>：无缝对接微信支付、微盟商城，实现“引流→转化→复购”全链路自动化。</li></ul><p><strong>优势</strong>：微信生态深度整合+私域运营能力；<strong>劣势</strong>：非私域场景适配性弱。</p><h3>（六）Keap：服务类企业的“销售自动化助手”</h3><p><strong>品牌定位</strong>：SaaS CRM，面向服务类中小企业（如咨询/培训/家政）。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：同步销售活动记录（跟进时间/内容）、日程、订单数据；</li><li><strong>分析深度</strong>：生成<strong>自定义绩效报表</strong>（如销售行为分析→“销售A的跟进次数→转化效率”、目标完成率对比）；</li><li><strong>决策支持</strong>：通过“销售行为→转化效率”分析优化销售话术（如“跟进次数≥5次的客户转化高→鼓励销售增加跟进”）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：构建“线索→商机→订单”标准化流程（如“线索录入→自动分配给销售→触发跟进邮件”），支持AI销售教练（实时话术建议，如“客户说‘价格太高’时，推荐优惠套餐”）；</li><li><strong>规则灵活性</strong>：支持根据销售周期自定义流程（如“商机阶段→自动发送对应资料”）；</li><li><strong>集成能力</strong>：支持邮件/短信平台集成，实现自动化触达。</li></ul><p><strong>优势</strong>：销售流程标准化+AI话术支持；<strong>劣势</strong>：全业务覆盖能力弱，适合以销售为核心的服务类企业。</p><h3>（七）Veeva CRM：合规性要求高的“行业专享工具”</h3><p><strong>品牌定位</strong>： enterprise级SaaS，面向医疗/医药等合规性要求高的中小企业。 <strong>核心能力1：数据统计分析引擎</strong></p><ul><li><strong>数据整合</strong>：支持多领域数据整合（如临床数据→客户交互数据→销售数据）；</li><li><strong>分析深度</strong>：采用<strong>列式文件存储+自然语言分析</strong>，可快速处理复杂数据（如“某药品的临床效果→医生处方量→销售业绩”的关联分析）；</li><li><strong>决策支持</strong>：通过“临床预警+销售趋势”辅助合规决策（如“某药品临床反馈异常→自动暂停销售”）。</li></ul><p><strong>核心能力2：业务流程自动化</strong></p><ul><li><strong>覆盖场景</strong>：全流程履约自动化（订单审批→库存同步→售后提醒）+ 合规管控（如“药品销售→自动记录医生处方→符合FDA/GDPR要求”）；</li><li><strong>规则灵活性</strong>：支持<strong>AI+RPA</strong>（机器人流程自动化），如“自动生成合规报告→同步至监管部门”；</li><li><strong>集成能力</strong>：支持医疗行业系统（如电子病历、临床试验管理系统）对接。</li></ul><p><strong>优势</strong>：高合规性+多领域数据整合；<strong>劣势</strong>：实施成本高，适合医疗/医药等垂直行业。</p><h2>四、横向对比：从“能力到适配”的直观排序</h2><h3>（一）核心能力对比表（满分为5分）</h3><table><thead><tr><th>品牌</th><th>数据统计分析能力（整合/深度/决策）</th><th>业务流程自动化能力（覆盖/灵活/集成）</th><th>适配场景</th><th>实施成本</th><th>数据安全</th></tr></thead><tbody><tr><td>超兔一体云</td><td>5/5/5</td><td>5/5/5</td><td>全行业全业务需求</td><td>中（SaaS订阅）</td><td>SaaS标准安全</td></tr><tr><td>YetiForce</td><td>4/4/4</td><td>4/5/4</td><td>有技术团队的制造/贸易</td><td>低（开源）</td><td>GDPR合规</td></tr><tr><td>Dolibarr</td><td>4/4/4</td><td>4/4/4</td><td>国内中小制造/零售</td><td>低（模块化）</td><td>自有服务器</td></tr><tr><td>ClickUp</td><td>3/3/3</td><td>3/4/4</td><td>轻量级团队（任务/协作）</td><td>低（订阅）</td><td>标准</td></tr><tr><td>微盟CRM</td><td>4/4/4</td><td>4/4/5</td><td>依赖私域的零售/餐饮</td><td>中（SaaS）</td><td>微信生态安全</td></tr><tr><td>Keap</td><td>3/3/3</td><td>4/3/3</td><td>服务类企业（销售为核心）</td><td>中（订阅）</td><td>标准</td></tr><tr><td>Veeva CRM</td><td>5/5/5</td><td>5/5/5</td><td>医疗/医药（合规要求高）</td><td>高（enterprise）</td><td>高合规</td></tr></tbody></table><h3>（二）“数据-流程”协同流程图（以超兔一体云为例）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557562" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><h2>五、选型建议：按需匹配，不选“最好”选“最对”</h2><ol><li><strong>若依赖微信私域运营</strong>（零售/餐饮）：选<strong>微盟CRM</strong>（微信生态深度联动+自动化触达）；</li><li><strong>若有技术团队+制造/贸易需求</strong>：选<strong>YetiForce</strong>（开源模块化+全链路数据整合）；</li><li><strong>若为国内中小制造/零售</strong>：选<strong>Dolibarr</strong>（本土化功能+生产流程数字化）；</li><li><strong>若为轻量级团队（任务/协作）</strong> ：选<strong>ClickUp</strong>（轻量级易上手+多视图整合）；</li><li><strong>若为服务类企业（销售为核心）</strong> ：选<strong>Keap</strong>（销售流程标准化+AI话术支持）；</li><li><strong>若为医疗/医药（合规要求高）</strong> ：选<strong>Veeva CRM</strong>（高合规性+多领域数据整合）；</li><li><strong>若需全行业全业务管控</strong>：选<strong>超兔一体云</strong>（全业务一体化+AI能力+低成本客制化）。</li></ol><h2>六、结论：中小企业的“精细化管控”核心逻辑</h2><p>无论是超兔的全业务闭环、YetiForce的开源定制，还是微盟的私域联动，本质都是通过“<strong>数据统计分析（让决策有依据）</strong> +<strong>业务流程自动化（让执行有标准）</strong> ”的组合，解决中小企业“不会管、管不好”的问题。最终的选型关键，在于<strong>匹配企业的核心需求</strong>——没有“万能工具”，只有“最适合的工具”。</p><p>对于中小企业而言，无需追求“大而全”，而是要选“<strong>能解决核心痛点+可随业务增长扩展</strong>”的工具，才能真正实现“精细化管控”的落地。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[7大CRM和ERP品牌对比：「订单-采购-库存」全链路能力 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047557573</link>    <guid>https://segmentfault.com/a/1190000047557573</guid>    <pubDate>2026-01-22 11:10:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型中，<strong>「订单执行-采购-库存」的全链路协同</strong>是提升运营效率的核心——前端订单要快速触发后端采购/库存调整，后端执行要实时反馈到前端销售，形成闭环。然而，不同品牌的核心定位差异极大：有的聚焦前端销售，有的覆盖全流程，有的深耕垂直场景。本文基于<strong>订单执行、采购、库存、库存/备货、产品库存</strong>五大维度，对7款主流CRM/ERP品牌（超兔一体云、SugarCRM、Salesforce、金现代、管家婆、Zoho CRM、Oracle CX）进行专业横评，为企业选型提供参考。</p><h2>一、对比框架：5大核心维度定义</h2><p>本次对比围绕「全链路协同能力」设计，覆盖从订单发起至库存履约的关键环节，具体维度如下：</p><table><thead><tr><th>维度</th><th>评估要点</th></tr></thead><tbody><tr><td><strong>订单执行</strong></td><td>流程覆盖（从创建到验收的全链路）、自动化能力（如锁库、触发采购）、后端联动（与采购/库存的衔接）</td></tr><tr><td><strong>采购</strong></td><td>智能计划（基于销售/库存的自动计算）、执行能力（询价比价、拆分订单）、协同（与供应商的对接）</td></tr><tr><td><strong>库存</strong></td><td>功能深度（多仓库、BOM、溯源）、自动化（出入库、预警）、可视化（实时状态、库位管理）</td></tr><tr><td><strong>库存/备货</strong></td><td>智能计算（采购量自动生成）、预警机制（库存上下限）、模式支持（以销定采、供应商直发）</td></tr><tr><td><strong>产品库存</strong></td><td>分类管理（多级分类、权限）、BOM（物料清单）、价格策略（多价格、套餐）、非标支持</td></tr></tbody></table><h2>二、各品牌核心能力深度对比</h2><h3>1. 超兔一体云：中小企业的「全闭环一体化」首选</h3><p><strong>核心定位</strong>：聚焦中小企业的「订单-采购-库存」全链路闭环管理，无需集成第三方系统。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：支持标准/批发/非标/维修等多类型订单，内置<strong>全流程工作流</strong>（创建→审核→锁库→生产/发货→验收），并自动触发采购（库存不足时生成采购计划）；</li><li><strong>采购</strong>：基于销售订单、库存水平、在途货物<strong>智能计算采购量</strong>，支持<strong>询价比价</strong>（向多供应商发询价单）、<strong>自动拆分采购单</strong>（按供应商能力分配）；</li><li><strong>库存</strong>：支持500+多仓库、<strong>BOM管理</strong>（生产型企业必备）、<strong>三级溯源</strong>（流水→批次→序列号），并通过手机拣货、扫码出入库提升效率；</li><li><strong>库存/备货</strong>：内置<strong>以销定采</strong>（按订单需求备货）、<strong>供应商直发</strong>（减少中间环节）模式，库存上下限<strong>自动预警</strong>；</li><li><strong>产品库存</strong>：支持多级分类（带权限）、多价格策略（批发/零售/促销）、套餐/租赁/非标产品管理，以及<strong>销量分析</strong>（现金牛/毛利产品区分）。</li></ul><p><strong>优势</strong>：全链路闭环，无需集成，智能自动化程度高，适合中小制造/商贸企业。</p><h3>2. SugarCRM：前端销售到订单的「轻协同」工具</h3><p><strong>核心定位</strong>：前端销售与客户关系管理，不覆盖后端采购/库存执行。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：通过SugarBPM实现「线索→合同→订单」的全流程自动化，但<strong>不涉及库存扣减、采购触发</strong>等后端操作；</li><li><strong>采购/库存</strong>：无原生功能，需通过集成Infor SCM（供应链）、WMS（仓库）等第三方系统实现。</li></ul><p><strong>优势</strong>：前端销售流程成熟，适合以销售为核心、后端已有供应链系统的企业。</p><h3>3. Salesforce：大型企业的「客户数据+供应链集成」平台</h3><p><strong>核心定位</strong>：以客户为中心的「销售云+商务云」，供应链能力需集成ERP/WMS。 <strong>关键能力</strong>：</p><ul><li><strong>订单执行</strong>：通过销售云实现「线索→商机→订单」的跟踪，商务云整合<strong>多渠道订单</strong>（电商、线下），但<strong>后端采购/库存需集成SAP/Oracle等ERP</strong>；</li><li><strong>采购/库存</strong>：无原生功能，依赖生态伙伴的集成（如AppExchange中的ERP工具）；</li><li><strong>产品库存</strong>：通过Customer 360整合外部库存数据，销售人员可查看实时库存，但<strong>操作需依赖后端系统</strong>。</li></ul><p><strong>优势</strong>：适合大型企业的「客户数据+供应链」整合，需搭配ERP使用。</p><h3>4. SugarCRM vs 超兔：流程差异可视化</h3><p>通过Mermaid流程图对比两者的核心差异： <strong>超兔的全闭环流程</strong>：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557577" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><p><strong>SugarCRM的前端流程</strong>：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557578" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h3>5. 其他品牌补充对比</h3><table><thead><tr><th>品牌</th><th>核心能力总结</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>金现代（LIMS）</strong></td><td>聚焦实验室场景的「订单→采购→库存」自动化（如电子订单自动生成入库单、库存RFID识别）</td><td>医药/科研实验室</td></tr><tr><td><strong>管家婆</strong></td><td>传统ERP的基础采购/库存管理（如采购单「生单方式」创建、基础库存出入库）</td><td>中小商贸企业（批发/零售）</td></tr><tr><td><strong>Zoho CRM（工业版）</strong></td><td>工业场景的「销售→库存→采购」协同（如轴承库存低于安全值自动提醒采购），支持电商集成</td><td>工业制造/电商企业</td></tr><tr><td><strong>Oracle CX</strong></td><td>大型企业的「全渠道订单路由+供应链联动」（如就近仓库发货、VMI供应商管理库存）</td><td>大型制造/零售企业（复杂供应链）</td></tr></tbody></table><h2>三、可视化对比：雷达图与分值</h2><p>以<strong>1-5分</strong>（5分为满分）评估各品牌在5大维度的能力，雷达图如下（文字描述）：</p><table><thead><tr><th>品牌</th><th>订单执行</th><th>采购</th><th>库存</th><th>库存/备货</th><th>产品库存</th><th>总分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>25</td></tr><tr><td>SugarCRM</td><td>3</td><td>1</td><td>1</td><td>1</td><td>1</td><td>7</td></tr><tr><td>Salesforce</td><td>4</td><td>2</td><td>2</td><td>2</td><td>2</td><td>12</td></tr><tr><td>金现代</td><td>4</td><td>4</td><td>4</td><td>3</td><td>3</td><td>18</td></tr><tr><td>管家婆</td><td>3</td><td>3</td><td>3</td><td>3</td><td>3</td><td>15</td></tr><tr><td>Zoho CRM（工业版）</td><td>4</td><td>4</td><td>4</td><td>4</td><td>3</td><td>19</td></tr><tr><td>Oracle CX</td><td>5</td><td>5</td><td>5</td><td>5</td><td>5</td><td>25</td></tr></tbody></table><h2>四、结论：各品牌适用场景总结</h2><table><thead><tr><th>品牌</th><th>适用企业类型</th><th>核心优势</th></tr></thead><tbody><tr><td>超兔一体云</td><td>中小制造/商贸企业（无现有系统）</td><td>全链路闭环、无需集成、智能自动化</td></tr><tr><td>SugarCRM</td><td>前端销售为主（后端有供应链系统）</td><td>销售流程成熟、与现有系统协同</td></tr><tr><td>Salesforce</td><td>大型企业（需整合客户与供应链数据）</td><td>多渠道订单整合、Customer 360数据统一</td></tr><tr><td>金现代</td><td>实验室/医药企业</td><td>垂直场景的采购/库存自动化</td></tr><tr><td>管家婆</td><td>传统中小商贸企业</td><td>基础采购/库存管理、成本低</td></tr><tr><td>Zoho CRM（工业版）</td><td>工业制造/电商企业</td><td>工业场景的销售-库存协同、电商集成</td></tr><tr><td>Oracle CX</td><td>大型复杂供应链企业</td><td>全渠道订单路由、深度供应链联动</td></tr></tbody></table><h2>五、选型建议</h2><ul><li><strong>中小企业</strong>：优先选<strong>超兔一体云</strong>，全闭环能力覆盖90%以上需求，无需额外集成；</li><li><strong>前端销售导向</strong>：选<strong>SugarCRM</strong>，聚焦销售到订单的流程，后端通过集成补充；</li><li><strong>大型企业</strong>：选<strong>Salesforce+ERP</strong>或<strong>Oracle CX</strong>，满足复杂供应链与客户数据整合需求；</li><li><strong>垂直场景</strong>：实验室选<strong>金现代</strong>，工业制造选<strong>Zoho CRM工业版</strong>。</li></ul><p>通过本次对比可见，<strong>超兔一体云</strong>是中小企业「订单-采购-库存」全链路管理的最优解——无需额外投入集成成本，即可实现智能自动化的闭环运营，完美匹配中小制造/商贸企业的数字化需求。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[校园外卖平台的“新答案”：如何用一个小程序，统筹商家运营、学生需求与平台管理的协同难题？ duoke]]></title>    <link>https://segmentfault.com/a/1190000047557585</link>    <guid>https://segmentfault.com/a/1190000047557585</guid>    <pubDate>2026-01-22 11:09:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>两种截然不同的产品逻辑：前者是把社会外卖平台简单搬进校园，后者则是真正理解校园场景后构建的本地化服务生态。真正的校园外卖，绝非 “社会平台的简化版”，而是一套需要深度重构的 “懂校园、贴场景、有温度” 的服务体系。</h4><hr/><h2>一、走出“便宜至上”的误区：需求分层的金字塔模型</h2><ol><li><strong>基础层（生存刚需）</strong>：30分钟内稳定送达、10-20元主流价格带、食品安全底线保障。这是入场券，但不是决胜点。</li><li><p><strong>场景层（节奏适配）</strong>：</p><ul><li><strong>时间适配</strong>：早课前8:00-8:05的“5分钟早餐包”、图书馆闭馆后的“深夜能量站”。</li><li><strong>空间适配</strong>：教室与宿舍区不同菜单、社团活动“拼单套餐”一键成团。</li><li><strong>社交适配</strong>：宿舍拼单免配送费、“分享考研加油餐得优惠”、可定制的“教授同款午餐”。</li></ul></li><li><strong>情感层（身份认同）</strong>：这超越了功能本身，产品成为他们校园生活的“伙伴”而非工具。能否提供情绪价值、营造归属感的关键。</li></ol><p><strong>外卖端页面展示：</strong><br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdnH3u" alt="" title=""/></p><h2>二、破解“高峰堰塞湖”：用“时空切割法”重构运力与体验</h2><ul><li><p><strong>空间切割</strong>：</p><ul><li><strong>教学饥荒区</strong>（教学楼群）：主打“课间极速达”，供应可快速进食的简餐、咖啡。</li><li><strong>宿舍深水区</strong>（生活区）：提供“夜间专属菜单”，如粥品、小吃，并延长服务时间。</li><li><strong>社交荒漠区</strong>（体育场、活动中心）：预设“团建套餐”，满足班级、社团活动需求。</li></ul></li><li><strong>时间预测</strong>：打通或模拟教务系统API，获取全校课表。在上午第四节下课、晚上选修课结束前，系统预判需求，提前向合作商户推送热销套餐备餐指令。</li><li><p><strong>运力革命</strong>：组建“校园配送联盟”，招募勤工俭学的学生作为配送员。优势显著：</p><ul><li><strong>成本优化</strong>：学生兼职成本更低，且时间与订单高峰天然契合。</li><li><strong>信任穿透</strong>：校内同学身份，可直达宿舍楼内，解决“最后100米”难题。</li><li><strong>灵活调度</strong>：基于课程空闲时间派单，实现运力匹配。</li></ul></li></ul><p><strong>商户端页面展示：</strong><br/><img width="724" height="734" referrerpolicy="no-referrer" src="/img/bVdnH12" alt="" title="" loading="lazy"/></p><h2>三、从“送餐”到“送一切”：构建校园生活服务中枢</h2><p>单一的外卖功能有限。成功的小程序，早已演化成<strong>校园本地生活的超级入口</strong>。已验证的高频衍生场景包括：</p><ol><li><strong>外卖/快递代取</strong>：发布需求，由顺路的同学有偿接单送达。</li><li><strong>资料/物品代送</strong>：忘带课本、急需文件，可发起校内闪送。</li><li><strong>线上打印</strong>：上传文档，选择就近打印点，付费后直接送到寝室。</li><li><strong>生活服务整合</strong>：二手交易、电脑维修、干洗服务、代买日用品等，均可接入平台。</li></ol><p><strong>骑手端页面展示：</strong><br/><img width="731" height="758" referrerpolicy="no-referrer" src="/img/bVdnH17" alt="" title="" loading="lazy"/></p><p><img width="664" height="539" referrerpolicy="no-referrer" src="/img/bVdnwcY" alt="" title="" loading="lazy"/></p><h2>四、技术为骨，运营为肉：让数据驱动“懂校园”的智慧</h2><ul><li><strong>技术选型</strong>：前端采用 <strong>Uni-app</strong> 实现一套代码多端发布（微信小程序、H5、App），后端使用 <strong>Tp6框架</strong> 开发管理后台，兼顾开发效率与系统稳定性。</li><li><strong>数据核心</strong>：不仅仅是处理订单，更重要的是数据分析。研判各区域、各时段、各人群的消费偏好，</li><li><p><strong>生态扩展</strong>：在基础平台上，可搭载 <strong>“校园圈子”</strong> 系统，形成信息互动社区。并可进一步插件化扩展，如：</p><ul><li>独立管理的<strong>社团专区</strong>。</li><li><strong>1v1音视频通话</strong>（用于兼职面试、活动沟通）。</li><li><strong>求职招聘、兼职信息</strong> 频道。</li><li>这些插件可根据学校特点进行<strong>私人化定制</strong>，让每个校园的生态都具有独特性。</li></ul></li></ul><p><img width="723" height="245" referrerpolicy="no-referrer" src="/img/bVdnkay" alt="" title="" loading="lazy"/></p><p><strong>后端管理系统看板：</strong><br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnH3n" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026年AI会抢走我们的工作吗？一篇文章讲清“取代、创造与转型” 智能体小狐 ]]></title>    <link>https://segmentfault.com/a/1190000047557665</link>    <guid>https://segmentfault.com/a/1190000047557665</guid>    <pubDate>2026-01-22 11:09:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要</strong>  <br/>人工智能会不会导致大规模失业？这是每一轮技术浪潮都会出现的问题。本文通过真实案例，系统分析AI正在取代哪些工作、正在创造哪些新职业，以及普通人如何避免被AI淘汰，给出完整判断与行动路径。</p><hr/><h2>一、AI正在取代工作吗？这是已经发生的现实</h2><p>AI正在取代工作，这不是未来预测，而是正在发生的事实。</p><p>在客服、制造业、物流和金融等行业，人工智能系统正在系统性替代大量重复性岗位。最典型的例子，是呼叫中心。</p><p>张先生曾是某大型呼叫中心的客服专员，每天接听上百通电话。公司上线AI客服系统后，客服团队从50人缩减到5人，AI可以24小时在线，每分钟处理数十个咨询，成本下降超过80%。</p><p>张先生并不是失败者，他只是被<strong>结构性替代</strong>了。</p><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/405/790/4057900467-69718b60b498e" alt="" title=""/></p><hr/><h2>二、哪些工作最容易被AI取代？三个明确规律</h2><p>AI不会随机抢走工作，它遵循清晰的技术规律。</p><h2>AI最容易替代的岗位具有三个特征：</h2><ol><li><strong>可标准化</strong>：流程可写成规则</li><li><strong>可流程化</strong>：步骤固定、可重复</li><li><strong>可规模化</strong>：同一任务可无限复制</li></ol><p>符合这三点的岗位，包括：</p><ul><li>客服、数据录入、行政文员</li><li>初级财务分析、报表生成</li><li>仓储分拣、流水线工人</li></ul><p>这些岗位的共同点是：<strong>任务比人重要</strong>。</p><hr/><h2>三、一个被忽视的事实：AI关闭的是“旧岗位入口”</h2><p>AI并不是一次性抢走所有人的工作，而是<strong>逐步关闭旧岗位的入口</strong>。</p><p>这意味着：</p><ul><li>新人更难进入旧行业</li><li>转型成本向个人转移</li><li>学习能力成为关键变量</li></ul><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/105/456/1054566259-69718b619644e" alt="" title="" loading="lazy"/></p><hr/><h2>四、AI是否也在创造新工作？答案是肯定的</h2><p>AI不会只带来失业，它同时创造新职业。</p><p>在自动驾驶、金融科技、医疗、教育等行业，大量新岗位正在出现：</p><ul><li>数据标注与治理工程师</li><li>自动驾驶系统维护员</li><li>AI模型监督员</li><li>算法审计员</li><li>AI伦理官</li><li>智能体训练师</li><li>人机协作设计师</li></ul><p>这些岗位在五年前几乎不存在。</p><hr/><h2>五、真实案例：AI正在“换结构”，不是“减规模”</h2><p>某金融科技公司中，30%的岗位三年前并不存在。这些岗位集中在数据治理、模型优化和合规领域，平均薪资比传统岗位高出40%。</p><p>这说明，AI带来的不是就业消失，而是<strong>就业升级迁移</strong>。</p><hr/><h2>六、为什么AI创造的工作门槛更高？</h2><p>因为新岗位要求三种能力同时存在：</p><ul><li>懂行业</li><li>懂AI</li><li>懂责任</li></ul><p>AI时代的工作，不再是“执行”，而是<strong>管理智能系统的执行</strong>。</p><hr/><h2>七、AI失业的真正原因是什么？不是技术，而是技能断层</h2><p>企业缺工程师，工人却失业，这是AI时代最典型的矛盾。</p><p>问题不在技术，而在于<strong>技能供需错配</strong>。</p><p>AI替代速度远快于教育和培训系统更新速度，于是出现短期失业。</p><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/328/273/328273074-69718b6285beb" alt="" title="" loading="lazy"/></p><hr/><h2>八、如何应对技能断层？三方路径</h2><h2>1️⃣ 个人</h2><ul><li>学会使用AI工具</li><li>从执行转向监督</li><li>构建不可替代能力</li></ul><h2>2️⃣ 企业</h2><ul><li>内部转型培训</li><li>设计人机协作流程</li><li>保留经验型员工</li></ul><h2>3️⃣ 政府</h2><ul><li>再培训计划</li><li>过渡期保障</li><li>新职业认证体系</li></ul><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/172/427/1724274714-69718b636a648" alt="" title="" loading="lazy"/></p><hr/><h2>九、国际经验正在证明：转型比对抗更有效</h2><ul><li>韩国：AI技能再培训</li><li>新加坡：AI过渡补贴</li><li>中国：新职业目录引导</li></ul><p>这些措施不是阻止技术，而是<strong>缓冲转型冲击</strong>。</p><hr/><h2>十、最终结论（引用级）</h2><blockquote><strong>AI不会让人失业，但不会学习的人一定会被淘汰。</strong>  <br/>AI淘汰的是流程，而不是人。</blockquote><p>未来最有竞争力的人，是那些：</p><ul><li>能定义目标</li><li>能监督AI</li><li>能持续学习的人</li></ul><hr/><h2>十一、给普通人的一句行动建议</h2><p>从今天开始，把AI当成你的工作系统，而不是聊天工具。</p><p>学会把任务交给AI，让自己升级为<strong>负责人</strong>。</p>]]></description></item><item>    <title><![CDATA[告别手敲 Schema！SeaTunnel 集成 Gravitino 元数据 RestApi 这个新]]></title>    <link>https://segmentfault.com/a/1190000047557681</link>    <guid>https://segmentfault.com/a/1190000047557681</guid>    <pubDate>2026-01-22 11:08:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557683" alt="" title=""/></p><p>每次在 Apache SeaTunnel 里配置非关系型数据库，看着那几百行还要手动定义的字段映射，是不是挺崩溃的？配置错一个字段，任务就报错，这种“体力活”真的该结束了。</p><p>最近 Apache SeaTunnel 社区的 <strong>Issue #10339</strong> 提案捅破了这层窗户纸：既然有 <strong>Apache Gravitino</strong> 这么强大的元数据服务，为什么不直接让它自动同步 Schema？这个提议一出，社区反响热烈，核心维护者们已经把它列入了年度 <strong>RoadMap</strong>。目前的讨论很务实，大家正盯着怎么让 Apache SeaTunnel 在提交作业时自动‘抓取’最新的元数据，好让大家彻底告别那种‘对着数据库手敲配置’的原始生活。</p><p>🫱 <strong>Issue 链接：</strong> <a href="https://link.segmentfault.com/?enc=z16yjFxvD8c2zFO8V72N4w%3D%3D.643M0tJrHm%2BtrlseKaM%2Fdc7KAQ8Kr4XIvEEuyTPqD2O8z9GxqZ5FII4MyCTknkhLBpS0nPeMk0Nck4nj1o3yIw%3D%3D" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/10339</a></p><h2>Issue 概述</h2><p>先来看看提交这个 Issue 的作者是为什么想到这个点子的，以及他初步的核心设计概念。🔽</p><p>本 PR 实现了 Apache Gravitino 与 SeaTunnel 的集成，将其作为非关系型连接器的外部元数据服务。通过 Gravitino 的 REST API 自动获取表结构和元数据，SeaTunnel 用户无需再在连接器配置中手动定义冗长且复杂的 Schema 映射。</p><h3>背景</h3><p>目前，Apache SeaTunnel 中的许多非关系型连接器（如 Elasticsearch、向量数据库和数据湖引擎）要求用户在作业配置中显式定义完整的列 Schema。这导致了以下问题：</p><ul><li><strong>配置繁琐且易错</strong>：字段映射内容冗长，极易发生人为错误。</li><li><strong>架构冗余</strong>：不同作业之间存在大量重复的 Schema 定义。</li><li><strong>数据不一致风险</strong>：实际存储层与 SeaTunnel 配置文件之间容易出现架构脱节。</li></ul><h3>变更内容</h3><p>本 PR 增加了<strong>基于 Gravitino 的 Catalog 和 Schema 解析器</strong>，使 SeaTunnel 能够：</p><ul><li>通过 REST API 从 Gravitino 查询表定义。</li><li>自动获取列名、数据类型及相关属性。</li><li>直接根据 Gravitino 元数据构建 SeaTunnel 内部 Schema。</li><li>针对受支持的连接器，取消强制手动定义 <code>schema { fields { ... } }</code> 的要求。</li></ul><p>实现后，用户只需在作业配置中指定 Gravitino Catalog 和相关的表引用即可。</p><h3>核心优势</h3><ul><li><strong>零手动映射</strong>：非关系型数据源实现 Schema 自动对齐。</li><li><strong>单一事实来源</strong>：确保表结构与中心化元数据仓库保持高度一致。</li><li><strong>提升可靠性</strong>：显著提高配置的准确性，降低长期维护成本。</li><li><strong>支持复杂类型</strong>：通过统一元数据，简化了对嵌套结构、JSON、向量等高级类型的处理。</li></ul><h3>执行范围</h3><p>所有基于 Gravitino 的 Schema 解析和校验均在 <strong>SeaTunnel Engine 客户端</strong>完成（即在作业提交前）。这种设计确保了：</p><ul><li>在作业预检阶段即可发现无效或不兼容的 Schema。</li><li>运行时的任务仅接收经过验证和标准化的 Schema，降低了执行失败的概率。</li></ul><h3>影响</h3><p>这一更新极大地简化了非关系型连接器的作业设置。除了提升易用性，它还为整个 SeaTunnel 生态系统在统一架构管理、架构演进以及高级数据类型支持方面奠定了技术框架。</p><h3>核心思路</h3><p>针对 FTP、S3、ES、MongoDB 等<strong>半结构化与非结构化数据源</strong>，SeaTunnel 现支持通过 <strong>Gravitino REST API</strong> 自动解析表结构（Schema）。</p><p>需要注意的是，这<strong>并非</strong>要取代现有的显式配置，而是一项<strong>完全向前兼容的可选新机制</strong>。</p><p>解析优先级如下：</p><h4>1. 显式配置（Inline Schema）永远优先</h4><p>只要连接器配置中包含了 <code>schema</code> 代码块，SeaTunnel 就<strong>必须忽略 Gravitino</strong>，直接以显式定义的 Schema 为准。</p><pre><code class="hocon">FtpFile {
  path = "/tmp/seatunnel/sink/text"
  # ... 其他基础配置 ...
  
  # 只要这里定义了，就不会去查 Gravitino
  schema = {
    name = string
    age  = int
  }
}</code></pre><h4>2. 通过 env 全局配置 Gravitino（推荐模式）</h4><p>SeaTunnel 已在引擎层面集成了 Gravitino Metalake。<br/>在 <code>env</code> 中全局开启后，所有非关系型数据源都能直接通过名称引用 Schema。</p><pre><code class="hocon">env {
  metalake_enabled = true
  metalake_type    = "gravitino"
  metalake_url     = "http://localhost:8090/api/metalakes/metalake_name/catalogs/"
}</code></pre><p><strong>2.1 使用 schema_path 引用</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  schema_path = "catalog_name.ykw.test_table"
}</code></pre><p><strong>2.2 使用 schema_url 引用</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  schema_url = "http://localhost:8090/api/metalakes/laowang_test/.../tables/all_type"
}</code></pre><h4>3. 兜底逻辑：读取操作系统环境变量</h4><p>如果在作业的 <code>env</code> 块中没有定义 Gravitino，SeaTunnel 会尝试从<strong>操作系统环境变量</strong>中读取以下配置：<br/><code>metalake_enabled</code> | <code>metalake_type</code> | <code>metalake_url</code><br/>其行为逻辑与第 2 节中的 <code>env</code> 配置完全一致。</p><h4>4. 在连接器层级单独配置 Gravitino</h4><p>如果全局没有配置元数据中心，也可以在具体的连接器（Connector）内部直接定义 Gravitino。</p><p><strong>4.1 直接使用 schema_url</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  metalake_type = "gravitino"
  schema_url = "http://localhost:8090/api/.../tables/all_type"
}</code></pre><p><strong>4.2 组合使用 metalake_url 与 schema_path</strong></p><pre><code class="hocon">FtpFile {
  # ... 基础配置 ...
  metalake_type = "gravitino"
  metalake_url  = "http://localhost:8090/api/metalakes/metalake_name/catalogs/"
  schema_path   = "catalog_name.ykw.test_table"
}</code></pre><h4>5. 探测器定位 (Find detector)</h4><p>系统会根据 <code>metalake_type</code> 自动匹配并加载对应的 REST API HTTP 探测器。</p><h4>6. 映射与构建 CatalogTable</h4><p>探测器调用拼接好的 URL 获取响应体（ResponseBody），随后将其交给映射器（Mapper）进行类型匹配，最终完成 <code>CatalogTable</code> 的构建。</p><h4>7. 流程图如下</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557684" alt="" title="" loading="lazy"/></p><h2>Issue 进展</h2><p>目前，Apache SeaTunnel 项目核心贡献者对此提议给出了正面评价，并将其添加到 Apache SeaTunnel Roadmap 中。</p><p>Apache SeaTunnel PMC Member 对这个提议提出一些疑问，比如这种集成属于哪一层级，对多引擎兼容性的考量，类型转换的准确性等，并根据社区设计规范，要求发起者提交一份正式的设计文档（Design Document）。提交者的回复非常具有建设性，他通过 <strong>“客户端预处理”和“抽象 Catalog 接口”</strong> 这两个核心设计点，有效地回应了社区对于系统耦合度和运行稳定性的担忧。</p><p>目前，这个讨论的回到了该 Issue 的提交者手中，社区正在等待他提交那份正式的 Design Document。</p><p>可以看到，这个方案要是落地，咱以后写任务可能就一两行配置的事儿。目前设计稿正在打磨中，非常需要大家去评论区吐吐槽、提提建议，毕竟这个功能好不好用，咱们一线开发者最清楚。走，去 GitHub 围观一下，说不定你的一个提议就能决定下一个版本的样子！🔽<br/><a href="https://link.segmentfault.com/?enc=un749U1I5h3gSeVp6psFYQ%3D%3D.OVKU8SFxyl3UJ%2BqYh%2BjuVmGOc1oU91k9dGUC%2BEwxQH5SxDZqPwPp68iXAfxTQRrD6A1fp4usFZ2SfVLmP4tlNw%3D%3D" rel="nofollow" target="_blank">https://github.com/apache/seatunnel/issues/10339</a></p>]]></description></item><item>    <title><![CDATA[逻辑引擎选型指南：这3款开源服务编排如何匹配你的业务场景 软件部长 ]]></title>    <link>https://segmentfault.com/a/1190000047557692</link>    <guid>https://segmentfault.com/a/1190000047557692</guid>    <pubDate>2026-01-22 11:07:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在快速发展的数字化时代，企业面临业务逻辑复杂多变的场景，传统的代码方式显得太臃肿，维护成本高，灵活性差，逻辑编排引擎能低成本更灵活的解决复杂业务逻辑管理。<br/>逻辑配置是零代码开发的业务核心功能，本质上是实现服务的编排，把原子的服务通过可视化编排，形成最终的业务逻辑。<br/>今天拆解几款开源的逻辑引擎系统，喜欢可以点赞收藏备用。</p><h2>1、LiteFlow</h2><p>这是一款非常成熟的国产开源引擎，它的核心思想是将业务逻辑拆分成独立的组件，然后通过规则文件来组装这些组件。它支持丰富的流程模式（串行、并行、选择、循环等），并且热更新功能很实用，能在高并发下无缝切换规则。</p><h3>核心特性：</h3><p>• 组件化编排：将复杂业务逻辑拆解为独立组件（Node），通过规则文件（XML/JSON/YAML）定义组件执行顺序和依赖关系，支持热更新。<br/>• 高性能：纳秒级组件开销，支持百万级并发流程。<br/>• 多语言支持：组件支持Java、Groovy、JavaScript、Python等脚本语言，脚本与Java全打通。<br/>• 灵活编排：支持串行、并行、条件分支、循环、子流程嵌套等复杂结构。<br/>• 动态配置：规则可存储在Nacos、Apollo、Zookeeper等配置中心，实现集中管理。<br/>• 监控与诊断：提供执行链路追踪、耗时统计、组件日志等功能。</p><h3>适用场景：</h3><p>• 电商促销规则组合、金融风控规则链、审批流引擎、数据处理管道（ETL）、微服务编排。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557694" alt="图片" title="图片"/></p><h2>2、JVS-Logic</h2><p>这是一款可视化逻辑引擎与服务编排系统，系统提供私有化部署，零代码、界面化、配置式服务编排平台，通过拖拽连接企业系统/API/数据库/数据等各种基础设施，自助式编排业务自动化执行流程，降低对代码、部署等技术依赖度，敏捷响应业务变化。</p><h3>核心特性：</h3><p>• 可视化服务编排：通过拖拽原子化服务组件并连线的方式，像画流程图一样设计和调整业务流程，无需编写代码。<br/>• 灵活的执行流控制：支持串行、并行、分支判断、循环等多种流程控制模式，能够应对复杂的业务逻辑。<br/>• 动态数据加工：提供类Excel公式的函数库（如逻辑函数、数学函数、文本函数等），可对流程中的数据动态计算和转换。<br/>• 多场景触发：逻辑流程可通过API调用、定时任务、界面按钮点击、表单提交、消息队列等多种方式触发。<br/>• 在线调试与监控：配置后可立即在线测试，实时查看每个节点的执行结果和流程状态，快速定位问题。<br/>• 强大的扩展能力：支持通过代码或简单配置（如HTTP接口）扩展自定义的原子服务组件，持续集成新能力</p><h3>适用场景：</h3><p>• 审批流自动化、定时任务调度、跨系统数据同步、业务规则动态调整。<br/>在线demo：<a href="https://link.segmentfault.com/?enc=NGuDhDkMEHvv%2B7a6dWb3xA%3D%3D.DzATn38v4dEnnrMwEA8OzW6%2FmYVEHFf4ggDEVcgebl4%3D" rel="nofollow" target="_blank">https://logic.bctools.cn/</a><br/>gitee地址：<a href="https://link.segmentfault.com/?enc=bBHfzN0%2FiEEkcmkS%2B3QCuQ%3D%3D.HtUZJiKBUx936wBJfH542aP62IkNWHBPEod4Ucl27WkpbwrOSBkQBZF4y%2B%2FFvzeF" rel="nofollow" target="_blank">https://gitee.com/software-minister/jvs-logic</a><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557695" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557696" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557697" alt="图片" title="图片" loading="lazy"/></p><h2>3、minions-go</h2><p>minions-go 是一个基于 Go 语言开发的逻辑编排引擎。它设计用于实现复杂的业务流程控制与自动化任务管理，提供灵活的工作流定义能力，使得开发者能够轻松构建可扩展和高可维护性的逻辑处理系统。项目灵感来源于对自动化工作流程的需求，致力于简化服务之间的交互和逻辑控制。</p><h3>核心特性：</h3><p>• 数据流驱动：它采用了一种称为“数据流驱动”的范式。你可以把整个业务流程看作数据在不同处理节点间流动和转换的过程，而不是传统的线性流程图。这种方式更贴近于将业务逻辑拆分为可复用的组件。<br/>• 可视化与代码分离：业务逻辑通过前端编辑器进行可视化设计，生成一份标准的 JSON 格式的“编排描述数据”（即 DSL）。后端的 Go 语言解析引擎（即 minions-go）则负责解释和执行这份 DSL，实现了UI界面和业务逻辑执行的解耦。<br/>• 支持逻辑复用：它支持“子编排”概念，即可以将一个已经创建好的复杂逻辑流程封装成一个单独的节点，供其他流程复用，这极大地提高了逻辑的模块化和复用性</p><h3>适用场景：</h3><p>• 微服务间任务分发、定时作业逻辑、响应式业务事件处理。</p>]]></description></item><item>    <title><![CDATA[tick 数据的系统适配逻辑：从实时流到业务适配的实践洞察 Jackyy ]]></title>    <link>https://segmentfault.com/a/1190000047557704</link>    <guid>https://segmentfault.com/a/1190000047557704</guid>    <pubDate>2026-01-22 11:06:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>做金融数据开发的同学大概率都有过这样的体验：刚开始接触 tick 数据，只知道它是 “市场最小粒度的行情数据”，但真正把 WebSocket 连通、跑起数据接收程序后，最先感受到的根本不是字段含义，而是数据流动的 “节奏”—— 时间戳高频跳动、价格瞬时波动、成交量断续刷新，这让 tick 数据和 K 线完全不同：它不是静态的行情结果，而是持续输入的动态信号流。</p><p>本文不聊基础概念，也不做接口入门教程，只从实操角度分享：把 tick 数据接入业务系统时，真正该关注的核心问题，以及如何适配它的特性做架构设计。</p><p><strong>一、从展示到业务核心：tick 数据的复杂度才真正显现</strong><br/>如果只是把 tick 数据用来做前端行情展示，它的底层复杂性基本会被界面掩盖；但一旦进入核心业务链路（比如实时风控监控、行情聚合、交易信号触发、历史数据回放），其 “持续推送” 的本质就会被彻底放大。</p><p>和传统 “一次请求一次返回” 的接口模式不同，tick 数据工程化接入的核心，从来都不是某一个字段怎么解释，而是：</p><ul><li>推送链路是否稳定</li><li>数据传输是否连续</li><li>是否需要搭建缓冲机制</li><li>下游模块如何高效消费</li><li>分享一段贴近生产环境的 WebSocket 接入代码（这是行业内常用的工程化写法，而非简单示例）：</li></ul><pre><code>import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    ts = data.get("timestamp")
    price = data.get("price")
    volume = data.get("volume")

    # 实际系统中，这里通常会进入队列或缓存
    print(f"{ts} | price={price} | vol={volume}")

def on_open(ws):
    ws.send(json.dumps({
        "action": "subscribe",
        "symbols": ["US.AAPL"],
        "type": "tick"
    }))

ws = websocket.WebSocketApp(
    "wss://stream.alltick.co/v1/market",
    on_open=on_open,
    on_message=on_message
)

ws.run_forever()</code></pre><p>运行这段代码后，控制台会持续刷新 —— 没有图表，但能直观看到时间序列数据的流动。也是在这个阶段，大家会达成一个共识：tick 数据不适合逐条解析，必须批量、整体化处理。</p><p><strong>二、成熟系统的 tick 数据流转：分层解耦是关键</strong></p><ul><li>在落地过的成熟业务系统中，tick 数据绝不会直接对接核心业务逻辑，而是按 “分层流转” 设计：</li><li>接入层：核心是保连接稳定，处理断线重连、异常重连；</li><li>缓冲层：用队列做 “削峰填谷”，解耦数据推送和业务消费的节奏；</li><li>消费层：完成数据聚合、实时计算、业务状态更新。</li></ul><p>这也能解释一个常见问题：很多系统初期接 tick 数据跑着没问题，长期运行却出各种 bug—— 不是业务逻辑复杂，而是 tick 数据的实时推送特性，本就不适合 “同步直连” 的处理方式。</p><p><strong>三、多市场场景：tick 数据标准化能省大量成本</strong><br/>如果系统只接单一市场的 tick 数据，数据结构的小差异还能靠定制化兼容；但一旦拓展到多市场，数据结构是否统一，直接决定接入层的开发和维护成本。</p><p>在实际项目中，我们常会选 AllTick API 这类已经做好多市场 tick 数据结构标准化的数据源 —— 它的核心价值是给系统提供 “稳定的数据入口”，而非需要频繁改的业务模块。这样一来，接入层、日志层、数据回放层的处理逻辑会简洁很多，也更贴合 tick 数据在系统中的实际定位。</p><p><strong>四、用 “系统心跳” 理解 tick 数据的适配逻辑</strong><br/>用更形象的说法，tick 数据就像系统的 “心跳”：</p><ul><li>心跳稳定，上层业务逻辑就能从容处理；</li><li>心跳紊乱（比如数据推送中断、频率突变、结构异常），再完善的业务逻辑也会被拖垮。</li></ul><p>从这个角度看，tick 数据的适配思路就很清晰了：该异步的异步、该缓冲的缓冲、该解耦的解耦。其实 tick 数据本身的字段和逻辑并不复杂，但它对系统设计的 “检验性” 极强 —— 任何架构短板，都会在 tick 数据的持续流转中暴露出来。</p><p>对开发者来说，真正理解 tick 数据，从来都不是从技术文档开始，而是从第一次盯着控制台的实时数据滚动、真切感知到数据 “节奏” 的那一刻开始。</p><p><strong>总结</strong></p><ul><li>tick 数据的核心是 “持续推送的动态流”，适配重点不在字段解读，而在流转节奏和分层处理；</li><li>成熟系统需靠 “接入层 + 缓冲层 + 消费层” 的分层设计，适配 tick 数据的实时性和不稳定性；</li><li>多市场场景下，标准化的 tick 数据源能显著降低接入层复杂度，更贴合业务实际需求。</li></ul>]]></description></item><item>    <title><![CDATA[Python 与 Nodejs 哪个更快 凌览 ]]></title>    <link>https://segmentfault.com/a/1190000047557714</link>    <guid>https://segmentfault.com/a/1190000047557714</guid>    <pubDate>2026-01-22 11:05:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是凌览。</p><ul><li>个人网站：<a href="https://link.segmentfault.com/?enc=XDJK4Mvc8G%2B99Ll%2BJpipPQ%3D%3D.2zA742qOHKAVfdGK12oCtxSUXBBkUXMOnC7C93QhHl0%3D" rel="nofollow" target="_blank">blog.code24.top</a></li><li>去水印下载鸭：<a href="https://link.segmentfault.com/?enc=Tl48cnno%2BNxlm2dmiGB5FA%3D%3D.hOWLBL3PT2AAuNBfPlXzyYx790j9GhpY%2BO9Enxm3%2FmI%3D" rel="nofollow" target="_blank">nologo.code.top</a></li></ul><p>如果本文能给你提供启发或帮助，欢迎动动小手指，一键三连（<code>点赞</code>、<code>评论</code>、<code>转发</code>），给我一些支持和鼓励谢谢。</p><h2>前言</h2><p>又刷到了Python 与 Nodejs 哪个更快的这类话题。巧的是在GitHub还开源了类似的计算机语言性能比较的开源库——speed-comparison。</p><p>单纯从性能上比较，speed-comparison已经给出了结论:Python(PyPy)&gt;Javascript(nodejs)&gt;Python(CPython)</p><p><img width="723" height="446" referrerpolicy="no-referrer" src="/img/bVdnH51" alt="" title=""/></p><p>PyPy3和 Python3（CPython）的差异在于<strong>解释器实现方式</strong>。Python3 是官方默认的 C 语言实现，而 PyPy3 是用 RPython 编写的替代实现，并引入了 <strong>JIT（即时编译）</strong> 技术。</p><p>speed-comparison测评数据属于较客观的，speed-comparison测评数据是进行莱布尼茨公式实现π的计算快慢。</p><p><img width="723" height="173" referrerpolicy="no-referrer" src="/img/bVdnH52" alt="" title="" loading="lazy"/></p><p>另外考虑公平性，做了以下处理：</p><ol><li>实现必须是单线程的。无多线程、异步或并行处理</li><li>允许使用更宽寄存器的SIMD优化，但必须独立，而非取代标准实现。<code>swift-simdcpp-avx2</code></li><li>使用语言的惯用代码。编译器优化标志没问题</li><li>所有实现必须使用现有实现中所示的莱布尼茨公式</li></ol><p>speed-comparison给出测评的语言不只有Python、Nodejs，常用语言也包括了，如：Java、C、C++等。</p><p>好奇的读者，可以浏览这个网页：<a href="https://link.segmentfault.com/?enc=%2FrJmVQvwpziIMBt7EOowVA%3D%3D.OFRyAWcHq39KuU%2FNVEdT1Aya7c02fl6%2B%2FgU85y3eFsKIEb%2FPE%2FPxO0DOx71QsVlF" rel="nofollow" target="_blank">https://niklas-heer.github.io/speed-comparison/</a></p><p>再来一起看看网友们高赞评论。</p><h2>高赞评论</h2><p><strong>【网友1】</strong></p><p>如果不是谷歌那个大聪明通过 v8 让人们意识到「原来 js 能跑这么快」，压根就不会有现在 JavaScript 的生态。</p><p><strong>【网友2】</strong></p><p>Python 其实是斩杀线，比Python还慢的就直接斩杀了。</p><p>Node.js 的 V8 JavaScript/WASM 引擎是 JIT 的，它的 非常精妙，连 JVM 和 CLR 这两个老牌的都是要服气的。</p><p><strong>【网友3】</strong></p><p>nodejs目前的解释器使用是v8 engine，它是一个 JIT。所以可以大幅增加运行时的性能。</p><p>python目前的主流解释器是 CPython，它还是一个常规的解释器也就是只能一行行解释，不能在运行时优化部分代码为机器码。</p><p>所以目前的情况是 nodejs 大幅快于 python</p><p><strong>【网友4】</strong></p><p>Python这种常年倒数的就不要来找JS碰瓷了。</p><p>我们常吐槽JS慢，是拿它跟C、C++、Rust这些编译型语言比的，但JS的性能可谓是脚本语言的天花板，打python就像暴打小朋友一样。</p><p><img width="723" height="902" referrerpolicy="no-referrer" src="/img/bVdnH53" alt="" title="" loading="lazy"/></p><h2>总结</h2><p>网友们的评论较主观没有数据说明,大家看看热闹就好。</p><p>如果一定要从性能方面比较，不考虑应用场景、社区、难易等等方面。</p><p>可以参考speed-comparison，自己也能拉取speed-comparison代码在本机电脑上跑一遍数据。</p>]]></description></item><item>    <title><![CDATA[用Docker部署Cloudreve私人云盘 landonVM ]]></title>    <link>https://segmentfault.com/a/1190000047557716</link>    <guid>https://segmentfault.com/a/1190000047557716</guid>    <pubDate>2026-01-22 11:04:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>前言</h3><p>当主流云盘频繁亮起容量限制、限速通知，甚至出现文件被莫名屏蔽的状况时，“数据不由己”的焦虑感总会让人束手束脚。</p><p>Cloudreve 私人云盘正是终结这种被动的理想解决方案。它不仅提供拖拽上传、多格式预览、链接加密分享等全套实用功能，更核心的优势在于：您可以将其部署在您的专属服务器上，从根源上避开第三方平台的种种限制，真正实现数据自由。</p><p>借助 Docker 部署的便捷性，整个搭建过程无需复杂配置，只需短短几分钟，您就能拥有一个数据完全由自己掌控的私人云盘。从此，文件存储不必再看平台“脸色”，数据安全与使用自由，将牢牢掌握在您手中。</p><p>一：操作步骤</p><p>在部署 Cloudreve 项目之前，记得先开放5212端口，方便后续操作。</p><p><code>Push and Deploy</code></p><p>1.新建 Cloudreve 文件夹</p><p><code>mkdir cloudreve</code></p><p>2.进入 Cloudreve 文件夹</p><p><code>cd cloudreve</code></p><p>3.下载 Cloudreve 源文件包</p><pre><code>wget https://github.com/cloudreve/Cloudreve/releases/download/3.8.3/cloudreve_3.8.3_linux_amd64.tar.gz</code></pre><p>4.解压 Cloudreve 源文件包</p><pre><code>tar -zxvf cloudreve_3.8.3_linux_amd64.tar.gz</code></pre><p>5.赋予 Cloudreve 源文件包权限</p><p>`chmod +x ./cloudreve<br/>`</p><p>6.启动 Cloudreve 项目</p><p><code>./cloudreve</code></p><p>Admin user name: 初始用户名<br/>Admin password: 初始密码</p><p>运行成功后，不要关闭该命令行窗口，在新的浏览器页面地址输入：http://&lt;<a href="https://link.segmentfault.com/?enc=wQdMP1mqNlmmLsLZkoxuTw%3D%3D.ieaTUiZk8WHUkW%2FF2pSyEmm%2BVrcLFqdAwIhp4wmbuHKhyXRNSfTXQWi1LqFtmi9o" rel="nofollow" target="_blank">服务器</a>IP地址&gt;:5212，即可访问 Cloudreve 服务。</p><p>初始密码忘记怎么办？在 Cloudreve 目录下执行以下命令，即可重置初始密码</p><pre><code>./cloudreve --database-script ResetAdminPassword</code></pre><p>二：持久化运行</p><p>运行成功后，不能关闭该命令行窗口，如果一不小心关掉了， Cloudreve 项目也就报错了，怎么办？在 Cloudreve 目录下执行以下操作，即可解决该问题：</p><p>1.先安装 screen（若未安装）：</p><pre><code>sudo apt update &amp;&amp; sudo apt install screen -y</code></pre><p>2.创建并进入一个新的 screen 会话：</p><p>`screen -S cloudreve<br/>`</p><p>3.在新会话中重新启动 Cloudreve：</p><p><code>./cloudreve</code></p><p>按下 Ctrl + A 再按 D（或直接关闭该命令行窗口），即可脱离会话并关闭命令行窗口，程序仍在后台运行。</p><p>单容器部署</p><p>如果你觉得以上步骤过于繁琐，觉得麻烦，你也可以使用最简单的方法来部署 Cloudreve ，在自定义路径的 Cloudreve 根目录下，打开命令行终端复制以下命令，直接运行即可：</p><p>1.部署与上述操作版本保持一致(3.8.3版本)：</p><pre><code>docker run -d \
  --name cloudreve \
  -p 5212:5212 \
  -v ./data:/cloudreve/data \
  cloudreve/cloudreve:3.8.3</code></pre><p>2.部署 Cloudreve 最新版本：</p><pre><code>docker run -d \
  --name cloudreve \
  -p 5212:5212 \
  -v ./data:/cloudreve/data \
  cloudreve/cloudreve:latest</code></pre><p>运行成功后，在浏览器地址输入：http://&lt;服务器IP地址&gt;:5212，即可访问 Cloudreve 服务。首次登录，先注册一个登录账号即可(即管理员账号)</p><p>端口占用</p><p>1.查询端口异常占用情况</p><p><code>netstat -tuln | grep :5212</code></p><p>netstat -tuln | grep :这里是要查询是否被占用的端口号 ，如果命令行有输出，则代表该端口已被占用；若命令行没有输出，直接返回 root@:/ cloudreve#，则没有没占用。</p><p>2.查询占用该端口的进程：</p><p>`lsof -i :5212<br/>`</p><p>lsof -i :[查看占用5212端口的进程] ，如果命令行有输出，则显示占用该端口的进程PID；反之。</p><p>3.释放占用端口的进程</p><p>找到进程PID后，使用以下命令强制终止该进程，释放该端口：</p><p><code>kill -9 [进程ID]</code></p><p>总结</p><p>这就是博主今天分享的全部内容了，这只是博主在日常使用中总结的，如有不足之处欢迎大家了指点一二。<br/>本文原发于我的博客：<a href="https://link.segmentfault.com/?enc=I7WHaX2%2FnTZYHPyAMUjxVg%3D%3D.15w091LCDaoITOUbnlLymPNDdry1wBcb90EfBKKfQb8%3D" rel="nofollow" target="_blank">landonVPS</a></p>]]></description></item><item>    <title><![CDATA[6大CRM厂商实测：2026全业务流程一体化选型深度解析 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047557719</link>    <guid>https://segmentfault.com/a/1190000047557719</guid>    <pubDate>2026-01-22 11:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、行业背景：为什么企业需要“全流程一体化”？</h2><p>在数字化转型中，企业面临的核心痛点是<strong>业务流程割裂</strong>：营销获客的数据无法同步到销售，销售订单无法联动采购生产，售后运维与前端客户信息脱节……这些“信息孤岛”导致效率低下、客户体验差、决策缺乏依据。</p><p>全流程一体化数字平台的核心价值，是通过<strong>数据打通、流程协同、智能赋能</strong>，实现“获客→销售→订单→生产→运维→复购”的闭环，让企业从“部门级效率”升级为“企业级效率”。</p><p>本文选取<strong>超兔一体云、Dolibarr、Agile</strong> <strong>CRM</strong> <strong>、神州云动CloudCC、浪潮CRM、Apptivo</strong>六大主流平台，从<strong>全业务流程覆盖度、一体化支撑能力、行业适配性</strong>三个维度展开深度对比，为企业选型提供参考。</p><h2>二、全业务流程横向对比：从获客到复购的能力拆解</h2><p>按照“获客→销售跟单→订单执行→配货采购/装配生产→上门安装运维→复购转介绍”的全生命周期，逐一分析各平台的核心能力与差异。</p><h3>（一）获客阶段：全渠道集客与线索转化的“精准度”</h3><p>获客是企业增长的起点，核心在于“多渠道覆盖→线索精准筛选→效果可归因”的闭环。各平台的差异体现在渠道本土化适配、AI赋能深度与ROI分析能力。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>渠道覆盖</strong></td><td>百度/巨量引擎+微信/小程序+地推+工商搜客（<strong>微信私域强</strong>）</td><td>基础CRM+微信同步（渠道较窄）</td><td>海外社交（Twitter/Facebook）+邮件（<strong>跨境获客强</strong>）</td><td>市场云（线下活动+落地页）+营销自动化</td><td>快消/医药终端（促销活动）+经销商自助</td><td>基础线索跟踪+付费商机预测</td></tr><tr><td><strong>线索处理</strong></td><td>一键转客户/订单+手机号/IP抓取+自动提醒</td><td>自定义字段+手动跟进</td><td>AI线索评分（行为轨迹）+高意向优先</td><td>精细化线索管理+360°客户洞察</td><td>终端数据联动+促销线索跟踪</td><td>销售漏斗+移动提醒</td></tr><tr><td><strong>效果归因</strong></td><td>市场活动成本分摊到线索/签约转化率</td><td>简单线索来源记录</td><td>营销活动ROI分析</td><td>全渠道ROI精准计算</td><td>促销费用全流程量化</td><td>无深度归因</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>本土企业优先选<strong>超兔</strong>（微信私域+ROI分析）；</li><li>跨境企业选<strong>Agile</strong> <strong>CRM</strong>（海外社交+AI线索评分）；</li><li>快消/医药选<strong>浪潮</strong>（终端数据+经销商获客）。</li></ul><h3>（二）销售跟单阶段：从“跟进”到“转化”的“效率差”</h3><p>销售跟单的核心是“流程标准化+信息透明化+团队协作”，各平台的差异体现在跟单模型丰富度、客户洞察深度与自动化能力。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 独创“三一客”小单模型（三定+关键节点）、<strong>商机阶段模型</strong>（中长单）、<strong>多方项目模型</strong>（复杂业务）；提供<strong>独有的“跟单时间线”</strong> ，清晰展示客户互动历史（如“3月1日发送报价→3月5日客户反馈价格高→3月8日调整方案”），销售可快速回顾进展；自动生成日报，管理者实时掌握团队动态。</li><li><strong>神州云动CloudCC</strong>： 聚焦<strong>项目型企业</strong>，支持<strong>项目</strong> <strong>全生命周期管理</strong>（从商机到项目启动→执行→收尾），集成成本、工时管理，适合工程建设、IT服务等行业。</li><li><strong>Agile</strong> <strong>CRM</strong>： 可视化<strong>销售管道（Pipeline）</strong> ，实时追踪“潜在客户→报价→成交”进度，支持跨部门协作（销售→客服→技术），快速响应客户问题。</li><li><strong>浪潮</strong> <strong>CRM</strong>： 针对<strong>渠道密集型企业</strong>，提供<strong>经销商自助平台</strong>（线上下单、库存查询、对账），将经销商纳入销售流程，提升渠道效率。</li></ul><h4>2. 关键差异总结</h4><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>神州云动CloudCC</th><th>Agile CRM</th><th>浪潮CRM</th></tr></thead><tbody><tr><td>跟单模型丰富度</td><td>小单+中长单+项目（全）</td><td>项目型（强）</td><td>标准销售管道（中）</td><td>经销商流程（强）</td></tr><tr><td>客户洞察深度</td><td>360°视图+跟单时间线（独）</td><td>项目360°+成本工时</td><td>客户行为分析+跨部门共享</td><td>经销商库存+对账信息</td></tr><tr><td>自动化能力</td><td>自动日报+待办提醒</td><td>项目阶段提醒</td><td>销售预测+任务分配</td><td>经销商订单自动同步</td></tr></tbody></table><h3>（三）订单执行阶段：从“签约”到“交付”的“协同力”</h3><p>订单执行的核心是“订单类型适配+业财联动+风险管控”，各平台的差异体现在订单场景覆盖与财务闭环能力。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th></tr></thead><tbody><tr><td><strong>订单类型</strong></td><td>服务型（合同）+实物型（标准/批发/非标）+特殊型（维修/外勤工单）</td><td>基础订单+库存关联</td><td>营销订单+客服联动</td><td>项目订单+业财融合</td><td>经销商订单+ERP联动</td></tr><tr><td><strong>业财管控</strong></td><td>签约/开票/发货自动触发应收+账期/信用控制</td><td>基础订单+库存扣减</td><td>营销订单+客服账单</td><td>项目成本+财务对账</td><td>经销商对账+财务同步</td></tr><tr><td><strong>风险控制</strong></td><td>应收/开票/回款三角联动+信用度控制</td><td>无强风险管控</td><td>需集成ERP</td><td>项目预算+成本控制</td><td>渠道库存预警+费用合规</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>需覆盖多订单类型（如维修、外勤）选<strong>超兔</strong>；</li><li>项目型企业选<strong>神州云动</strong>（项目成本+财务融合）；</li><li>快消/医药选<strong>浪潮</strong>（经销商订单+ERP闭环）。</li></ul><h3>（四）配货采购/装配生产阶段：从“需求”到“交付”的“供应链能力”</h3><p>生产采购是制造型企业的核心环节，各平台的差异体现在<strong>供应链协同</strong> <strong>深度</strong>与<strong>生产集成能力</strong>。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 支持<strong>智能采购</strong>（自动计算采购量+匹配历史供应商+询价比价）、<strong>MES</strong> <strong>生产计划</strong>（排程→派工→领料→报工→质检→入库），覆盖“采购→生产”全流程；适合中小制造企业。</li><li><strong>神州云动CloudCC</strong>： 集成<strong>采购管理</strong>（需求触发→供应商协同）、<strong>库存管理</strong>（动态更新），但生产需对接第三方MES。</li><li><strong>浪潮</strong> <strong>CRM</strong>： 提供<strong>电子采购</strong>（需求自动触发）、<strong>数字供应链</strong>（供应商订单/发货/结算协同），聚焦快消/医药的“经销商→供应商”闭环。</li><li><strong>Agile</strong> <strong>CRM</strong>： 本身无生产采购模块，需集成ERP（如Oracle/SAP）实现订单→生产的联动。</li></ul><h4>2. 供应链能力矩阵</h4><table><thead><tr><th><strong>平台</strong></th><th>采购管理</th><th>生产集成</th><th>供应链协同</th><th>适用场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>智能采购</td><td>MES生产</td><td>全流程协同</td><td>中小制造企业</td></tr><tr><td>神州云动CloudCC</td><td>基础采购</td><td>需集成</td><td>供应商协同</td><td>项目型企业（如IT服务）</td></tr><tr><td>浪潮CRM</td><td>电子采购</td><td>需集成</td><td>渠道供应链</td><td>快消/医药</td></tr><tr><td>Agile CRM</td><td>无</td><td>需集成</td><td>无</td><td>营销型企业</td></tr></tbody></table><h3>（五）上门安装运维阶段：从“服务”到“口碑”的“体验感”</h3><p>售后运维的核心是“快速响应+资源调度+服务质量监控”，各平台的差异体现在工单管理深度与客户信息联动。</p><table><thead><tr><th><strong>能力维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>工单管理</strong></td><td>维修/外勤工单+智能调度+服务质量监控</td><td>售后工单+SLA管理</td><td>客服工单+客户360°视图</td><td>现场服务云+资源优化调度</td><td>售后跟踪+备品库存联动</td><td>Work Orders+移动签到</td></tr><tr><td><strong>客户联动</strong></td><td>工单关联客户历史（如“设备型号→维修记录”）</td><td>基础客户信息</td><td>客服→销售→技术跨部门共享</td><td>项目备品+客户需求联动</td><td>终端客户→经销商→售后联动</td><td>基础客户+工单记录</td></tr><tr><td><strong>服务监控</strong></td><td>服务时间+客户反馈+绩效分析</td><td>无深度监控</td><td>工单进度跟踪</td><td>服务成本+响应时间监控</td><td>售后满意度+问题闭环</td><td>无深度监控</td></tr></tbody></table><p><strong>总结</strong>：</p><ul><li>需高服务质量的企业（如设备制造）选<strong>超兔</strong>（工单+客户历史联动）；</li><li>项目型企业选<strong>神州云动</strong>（现场服务云+资源调度）；</li><li>快消/医药选<strong>浪潮</strong>（终端售后+备品联动）。</li></ul><h3>（六）复购与转介绍阶段：从“留存”到“裂变”的“增长力”</h3><p>复购转介绍是企业的“利润引擎”，核心在于“潜在需求挖掘+精准触达+激励机制”。</p><h4>1. 核心能力对比</h4><ul><li><strong>超兔一体云</strong>： 通过<strong>RFM</strong> <strong>分析</strong>（最近购买时间、频率、金额）识别高复购客户；设置<strong>复购流失预警</strong>（如“客户超过6个月未购买→自动提醒销售跟进”）；提供<strong>转介绍激励工具</strong>（如“推荐好友得折扣”），实现客户裂变。</li><li><strong>Agile</strong> <strong>CRM</strong>： 基于<strong>客户行为分析</strong>（如“浏览过升级套餐”）自动触发复购提醒（邮件/短信）；支持<strong>社交分享</strong>（推荐好友得优惠），适合线上营销型企业。</li><li><strong>神州云动CloudCC</strong>： 通过<strong>服务云</strong>（售后满意度管理）提升客户粘性；<strong>伙伴云</strong>（合作伙伴协作）挖掘转介绍机会，适合项目型企业。</li><li><strong>Dolibarr</strong>： 需<strong>手动设置“下次购买时间”提醒</strong>，潜在需求（如设备升级）依赖人工挖掘，复购效率低。</li></ul><h4>2. 复购能力评分（1-5分）</h4><table><thead><tr><th>平台</th><th>超兔</th><th>Agile</th><th>神州云动</th><th>浪潮</th><th>Apptivo</th><th>Dolibarr</th></tr></thead><tbody><tr><td>潜在需求挖掘</td><td>5</td><td>4</td><td>4</td><td>3</td><td>2</td><td>1</td></tr><tr><td>精准触达能力</td><td>5</td><td>4</td><td>4</td><td>3</td><td>2</td><td>1</td></tr><tr><td>转介绍激励机制</td><td>5</td><td>4</td><td>3</td><td>3</td><td>2</td><td>1</td></tr></tbody></table><h2>三、一体化支撑能力：从“能用”到“好用”的“底层逻辑”</h2><p>全流程一体化的核心支撑是<strong>数据连通、</strong> <strong>客制化</strong> <strong>、AI、集成能力</strong>，决定了平台的“灵活性”与“扩展性”。</p><h3>（一）一体化支撑能力对比表</h3><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>Dolibarr</th><th>Agile CRM</th><th>神州云动CloudCC</th><th>浪潮CRM</th><th>Apptivo</th></tr></thead><tbody><tr><td><strong>数据连通</strong></td><td>全模块底层打通（CRM+进销存+生产+财务）</td><td>统一数据库（客户+销售+库存）</td><td>营销+销售+客服连通</td><td>全模块连通（项目+销售+财务）</td><td>渠道+供应链+财务连通</td><td>基础模块连通（销售+库存）</td></tr><tr><td><strong>客制化</strong> <strong>能力</strong></td><td>零编码引擎（自定义菜单+表单+工作流）</td><td>本地部署+自定义字段</td><td>云配置+API定制</td><td>零编码定制+行业模板</td><td>低代码inBuilder+行业模板</td><td>标准化模块+付费扩展</td></tr><tr><td><strong>AI应用</strong></td><td>自定义AI智能体（嵌入客户/行动视图）+Coze工作流</td><td>无AI</td><td>AI线索评分+行为分析</td><td>营销自动化+项目预测</td><td>终端数据洞察+费用预测</td><td>商机预测（付费）</td></tr><tr><td><strong>集成能力</strong></td><td>多端（Web/App/小程序）+RPA+ERP对接</td><td>本地/私有云+基础集成</td><td>云多端+ERP/SAP集成</td><td>多端+OA/ERP整合</td><td>多端+PaaS+行业系统</td><td>移动+基础API</td></tr></tbody></table><h3>（二）雷达图：各平台综合能力评分（1-10分）</h3><table><thead><tr><th>指标</th><th>超兔</th><th>Agile</th><th>神州云动</th><th>浪潮</th><th>Apptivo</th><th>Dolibarr</th></tr></thead><tbody><tr><td>全流程覆盖度</td><td>10</td><td>7</td><td>9</td><td>8</td><td>6</td><td>7</td></tr><tr><td>行业适配性</td><td>8</td><td>7</td><td>9</td><td>10</td><td>6</td><td>7</td></tr><tr><td>客制化能力</td><td>10</td><td>7</td><td>9</td><td>8</td><td>5</td><td>7</td></tr><tr><td>AI应用深度</td><td>10</td><td>8</td><td>8</td><td>7</td><td>6</td><td>5</td></tr><tr><td>集成扩展性</td><td>10</td><td>9</td><td>9</td><td>8</td><td>6</td><td>7</td></tr></tbody></table><h2>四、行业适配与选型建议</h2><p>根据企业<strong>规模、行业、核心需求</strong>，给出针对性选型建议：</p><table><thead><tr><th><strong>企业类型</strong></th><th>核心需求</th><th>推荐平台</th><th>理由</th></tr></thead><tbody><tr><td>中小制造企业</td><td>全流程覆盖（采购+生产+运维）</td><td>超兔一体云</td><td>智能采购+MES生产+工单管理，性价比高</td></tr><tr><td>跨境营销型企业</td><td>海外获客+销售转化</td><td>Agile CRM</td><td>海外社交+AI线索评分+销售管道</td></tr><tr><td>项目型企业（IT/工程）</td><td>项目全生命周期+成本控制</td><td>神州云动CloudCC</td><td>项目管理+采购协同+业财融合</td></tr><tr><td>快消/医药企业</td><td>渠道获客+供应链协同</td><td>浪潮CRM</td><td>终端数据+经销商自助+电子采购</td></tr><tr><td>中小微轻量需求</td><td>基础流程+移动管理</td><td>Apptivo</td><td>免费版支持基础功能，付费扩展商机预测</td></tr><tr><td>需本地部署企业</td><td>数据主权+基础一体化</td><td>Dolibarr</td><td>本地/私有云部署+统一数据库</td></tr></tbody></table><h2>五、结论：全流程一体化的“本质”是什么？</h2><p>全流程一体化的核心不是“功能堆砌”，而是“以客户为中心”的流程协同——让营销知道“客户从哪来”，销售知道“客户需要什么”，生产知道“客户何时要”，售后知道“客户之前的问题”。</p><p>从对比来看：</p><ul><li><strong>超兔一体云</strong>是“全流程覆盖 + 高性价比”的首选，适合中小制造/服务企业；</li><li><strong>神州云动CloudCC</strong>是“项目型企业”的最佳选择；</li><li><strong>浪潮</strong> <strong>CRM</strong>是“快消/医药渠道密集型企业”的定制化方案；</li><li><strong>Agile</strong> <strong>CRM</strong>是“跨境营销型企业”的海外获客利器。</li></ul><p>企业选型时，需优先明确<strong>核心业务场景</strong>（如生产、渠道、项目），再匹配平台的<strong>差异化能力</strong>，避免“为了一体化而一体化”。</p><p><strong>附录：全业务流程时序图</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557721" alt="" title=""/></p><pre><code>sequenceDiagram
    participant 企业 as 企业
    participant 超兔 as 超兔一体云
    participant Agile as Agile CRM
    participant 神州云动 as 神州云动CloudCC
    participant 浪潮 as 浪潮CRM
    participant Apptivo as Apptivo
    participant Dolibarr as Dolibarr

    企业-&gt;&gt;超兔: 百度/微信获客→线索转订单→智能采购→MES生产→工单运维→复购预警
    超兔-&gt;&gt;企业: 全流程数据同步+转介绍激励

    企业-&gt;&gt;Agile: 海外社交获客→AI线索筛选→销售跟单→集成ERP生产→客服工单运维→复购提醒+社交分享
    Agile-&gt;&gt;企业: 营销+销售+客服数据连通

    企业-&gt;&gt;神州云动: 市场云获客→销售流程自动化→商机转订单→采购库存管理→现场服务运维→服务云提升满意度+伙伴云转介绍
    神州云动-&gt;&gt;企业: 全模块连通+项目全生命周期管理

    企业-&gt;&gt;浪潮: 快消/医药终端获客→经销商自助下单→订单执行→电子采购→售后跟踪→终端售后满意度提升
    浪潮-&gt;&gt;企业: 渠道+供应链+财务连通

    企业-&gt;&gt;Apptivo: 基础线索跟踪获客→销售漏斗跟单→订单执行→移动工单运维→客户管理促复购
    Apptivo-&gt;&gt;企业: 基础模块连通

    企业-&gt;&gt;Dolibarr: 基础CRM获客→销售跟单→订单执行→库存采购管理→售后工单运维→手动复购提醒
    Dolibarr-&gt;&gt;企业: 统一数据库数据同步</code></pre><p>综上所述，各全业务流程一体化数字平台都有其独特的优势和适用场景。企业在进行平台选型时，应充分结合自身实际情况，依据核心业务场景和需求，审慎选择最契合的平台，以实现企业业务流程的高效整合和持续发展。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务与价格以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[域名注册与域名解析：分清这两个核心环节，搭建网站不踩坑 防火墙后吃泡面 ]]></title>    <link>https://segmentfault.com/a/1190000047557722</link>    <guid>https://segmentfault.com/a/1190000047557722</guid>    <pubDate>2026-01-22 11:03:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在互联网世界中，域名是网站的“门牌号”，而域名注册与域名解析则是让这个“门牌号”生效的两大核心步骤。很多建站新手会将二者混淆，甚至误以为是同一回事，导致操作中出现域名注册成功却无法访问的问题。本文，资深域名服务商国科云将从概念、区别、流程、关联及避坑要点等方面进行全面解析，帮你彻底理清二者的逻辑，轻松搞定域名相关操作。</p><h2>一、核心概念：域名注册与解析到底是什么？</h2><p>1.域名注册：获取“门牌号”的合法使用权</p><p>域名注册是指用户通过正规域名注册商，向全球统一的域名管理机构（如ICANN，互联网名称与数字地址分配机构）申请，获得特定域名在一定期限内的合法使用权的过程。</p><p>域名注册具有唯一性和时效性两大核心特征。唯一性意味着同一域名在全球范围内只能被一个主体注册，比如“baidu.com”被百度注册后，其他主体无法再注册相同域名；时效性则指注册后的域名有使用期限（通常1-10年），需按时续费才能维持使用权，逾期未续费会被暂停解析甚至回收再售卖。</p><p>注册过程中，用户需提交真实信息完成实名认证（国内域名强制要求），包括个人身份证或企业营业执照等材料，虚假信息可能导致域名被冻结。注册成功后，注册商将提供域名管理服务，包括续费、信息修改、转移等功能。</p><p>2.域名解析：给“门牌号”绑定具体地址</p><p>域名解析是通过DNS（域名系统）服务器，将易记忆的域名转换为服务器能识别的IP地址（如IPv4的“95.127.211.85”、IPv6的“<br/>2001:0db8:85a3:0000:0000:8a2e:0370:7334”），让用户输入域名就能访问对应服务器的过程。形象地说，这相当于给已有的门牌号，绑定到具体的房屋地址，让访客能通过门牌号找到对应的房子。</p><p>互联网中的计算机本质上通过IP地址相互通信，但IP地址是一串复杂数字，难以记忆。域名解析的核心价值的就是建立“域名-IP”的映射关系，既保留了用户记忆的便捷性，又能让网络设备准确定位服务器。整个解析过程由DNS服务器集群协同完成，通常耗时几十毫秒，用户几乎无感知。</p><p>解析的核心是配置DNS记录，常见类型包括A记录（绑定IPv4地址）、AAAA记录（绑定IPv6地址）、CNAME记录（绑定其他域名别名）、MX记录（配置邮件服务器）等，不同记录对应不同的服务需求。</p><h2>二、关键区别：注册与解析可不是一回事</h2><p>域名注册和域名解析是两个独立但关联的环节，核心区别体现在目的、性质、操作对象三个维度，具体如下：</p><p>1.核心目的不同</p><p>域名注册的目的是获得域名的合法使用权，解决“归属权”问题，确保你拥有这个专属“门牌号”；而域名解析的目的是建立域名与IP的映射关系，解决“访问路径”问题，让用户能通过域名找到对应的服务器。没有注册的域名无法解析，注册后不解析的域名也无法被正常访问。</p><p>2.操作性质不同</p><p>域名注册是一次性申请+定期续费的流程，属于“权利获取”类操作，一旦完成注册，在有效期内无需重复操作，仅需关注续费即可；域名解析是技术性配置操作，属于“功能激活”类操作，可根据服务器变更、服务调整等需求反复修改解析记录，比如更换服务器后，需重新配置解析指向新IP。</p><p>3.操作对象不同</p><p>域名注册的操作对象是域名注册商和全球域名管理机构，用户需在注册商平台完成申请、支付、实名认证等操作，由注册商向管理机构提交注册请求；域名解析的操作对象是DNS服务器，用户可在注册商提供的解析平台，或第三方专业解析平台（如国科云解析、DNSPod）配置解析记录，本质是修改DNS服务器中的映射数据。</p><h2>三、实操流程：从注册到解析的完整步骤</h2><p>1.域名注册全流程（以国内平台为例）</p><p>第一步，需求规划与可用性查询。</p><p>明确域名用途（企业官网、个人博客等），选择合适后缀（.com、.cn最常用，.net、.org为补充），通过注册商的查询工具或WHOIS平台，确认目标域名是否已被注册。若心仪域名被抢注，可调整名称或选择替代后缀，同时规避商标冲突，避免注册后被仲裁收回。</p><p>第二步，选择正规注册商。</p><p>优先选择国科云、阿里云、腾讯云等有工信部资质的平台，避免非正规平台的续费暴涨、域名锁死等问题。</p><p>第三步，填写信息与支付。</p><p>提交注册人真实信息，个人提供身份证，企业提供营业执照，开启域名隐私保护服务隐藏个人信息；选择注册年限并支付费用，通常几分钟内即可完成注册。</p><p>第四步，完成实名认证。</p><p>国内域名注册后需在规定时间内实名认证，材料审核通过后（通常1-3个工作日），域名才能正常使用，否则会被暂停解析。</p><p>2.域名解析全流程</p><p>第一步，准备基础信息。</p><p>获取服务器公网IP（云服务器在控制台查询），若使用CDN服务则获取CNAME地址；确认域名的DNS服务器，默认使用注册商DNS，也可更换为专业解析平台的DNS。</p><p>第二步，进入解析管理页面。</p><p>登录注册商或解析平台后台，找到“DNS解析”入口，进入解析记录配置界面。</p><p>第三步，添加解析记录。</p><p>根据需求选择记录类型：搭建网站优先配置A记录（IPv4）或AAAA记录（IPv6），填写服务器IP；使用CDN则配置CNAME记录指向CDN地址；搭建企业邮箱需配置MX记录，设置邮件服务器地址及优先级。</p><p>第四步，等待解析生效。</p><p>添加完成后，DNS缓存需10分钟至24小时全网刷新，国内地区通常20分钟内可生效。可通过“nslookup”命令（WindowsCMD或Mac终端）验证，若显示对应IP则解析成功。</p><h2>四、域名注册和域名解析是否要同一服务商？</h2><p>域名注册与解析的核心关联的是“先后顺序”：必须先注册域名，再进行解析，二者共同构成网站访问的基础。但二者并非绑定在同一服务商，用户可根据需求选择“统一平台”或“分拆平台”管理。</p><p>1.统一服务商：适合新手与高效管理</p><p>将注册与解析放在同一平台（如国科云注册+国科云解析），优势在于便捷性。注册商通常为自有域名提供自动解析适配，减少手动配置成本，且域名续费、解析修改、SSL证书绑定等操作可在同一控制台完成，故障排查更高效，无需跨平台沟通。这种方式适合新手、个人站长及追求管理效率的企业。</p><p>2.分拆服务商：适合高需求场景</p><p>若对解析性能、安全性有特殊需求，可将解析权限转移至第三方专业平台。专业解析服务商通常拥有全球分布式解析节点、DDoS防护等功能，能提升网站访问速度和抗攻击能力。操作时需在注册商后台修改DNS服务器地址，备份原有解析记录，再在新平台重新配置，全程需注意TTL值（缓存生存时间）设置，建议临时改小至300秒加快生效。</p><h2>五、注册与解析的常见问题</h2><p>1.注册环节避坑</p><p>（1）避免盲目选择冷门后缀，.xyz、.top等小众后缀虽价格低，但用户认可度和搜索引擎信任度不足，不利于品牌推广；</p><p>（2）设置自动续费和到期提醒，重要域名可一次性注册多年，防止忘记续费导致域名丢失；</p><p>（3）不注册包含知名品牌词汇的域名，避免触发商标仲裁被收回。</p><p>2.解析环节避坑</p><p>（1）解析前核对服务器IP准确性，IP错误会导致网站无法访问；</p><p>（2）国内域名需完成备案后再解析到大陆服务器，未备案域名会被拦截；</p><p>（3）修改解析时选择网站访问低谷时段（如凌晨），减少解析中断对用户的影响；</p><p>（4）若解析生效慢，可清除本地DNS缓存（Windows执行“ipconfig/flushdns”命令）。</p>]]></description></item><item>    <title><![CDATA[Apache SeaTunnel MySQL CDC 支持按时间启动吗？ SeaTunnel ]]></title>    <link>https://segmentfault.com/a/1190000047557733</link>    <guid>https://segmentfault.com/a/1190000047557733</guid>    <pubDate>2026-01-22 11:02:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557735" alt="" title=""/></p><p>在 MySQL CDC 任务中，很多用户都会遇到这样的问题：任务失败后该从哪里恢复？只知道一个时间点，却拿不到对应的 binlog 位点怎么办？Apache SeaTunnel 2.3.12 通过引入按时间启动（Timestamp Startup）功能，给出了更直观的答案。</p><p>本文围绕该能力的设计背景、配置方式与实现机制展开解析，帮助读者理解如何基于时间语义更高效地进行 CDC 任务恢复与数据回溯。</p><h2>功能概述</h2><h3>Problem：CDC 启动点配置“技术正确，但使用困难”</h3><p>在 Apache SeaTunnel 2.3.12 之前，MySQL CDC 连接器主要支持从<strong>指定 binlog 位点（file + position）或 GTID</strong> 启动数据同步任务。这种方式在实现上是精确且可靠的，但在真实生产与运维场景中，往往并不符合用户的使用习惯。</p><p>在实际 CDC 运维过程中，用户更容易掌握的是 <strong>“时间”</strong>，而非底层 binlog 细节，例如：</p><ul><li>任务异常中断后，希望从<br/><strong>“2024-04-01 10:00:00”</strong> 之后继续同步</li><li>对某一时间窗口的数据进行回溯或补采</li><li>只知道“昨天 08:00 之后的变更需要重新同步”，但无法定位对应的 binlog 文件和偏移量</li></ul><p>如果仍要求用户手动将时间反推为 binlog 位点，不仅配置复杂，而且极易出错，也显著增加了运维成本。这种“技术友好、但用户不友好”的启动方式，已经成为 CDC 任务恢复和回溯场景中的常见痛点。</p><h3>Solution：引入按时间启动</h3><p>为解决上述问题，Apache SeaTunnel 在 <strong>2.3.12 版本</strong>中为 MySQL CDC 连接器引入了<strong>按时间启动功能</strong>。</p><p>该功能允许用户直接指定一个 <strong>Unix 时间戳（毫秒级）</strong> 作为同步起始点。MySQL CDC 连接器会在启动阶段自动完成以下工作：</p><ol><li>根据指定时间戳定位对应的 binlog 文件与偏移量</li><li>从该 binlog 位置开始读取变更事件</li><li>自动跳过所有早于该时间点的历史事件</li></ol><p>通过引入“时间”这一更符合业务语义的维度，SeaTunnel 将 CDC 启动方式从<strong>面向底层 binlog 细节</strong>，提升为<strong>面向业务时间语义</strong>，显著降低了 CDC 任务在<strong>恢复、回溯和运维场景</strong>下的使用门槛。</p><h2>配置参数</h2><p>要启用按时间启动功能，需要配置以下两个关键参数：</p><table><thead><tr><th>参数名</th><th>类型</th><th>必填</th><th>说明</th></tr></thead><tbody><tr><td><code>startup.mode</code></td><td>Enum</td><td>否</td><td>设置为 <code>"timestamp"</code> 启用时间模式 <a href="#1-1" target="_blank">2</a></td></tr><tr><td><code>startup.timestamp</code></td><td>Long</td><td>是</td><td>Unix 时间戳（毫秒），指定启动时间点 <a href="#1-2" target="_blank">3</a></td></tr></tbody></table><h2>配置示例</h2><pre><code class="yaml">env {
  parallelism = 1
  job.mode = "STREAMING"
  checkpoint.interval = 10000
}

source {
  MySQL-CDC {
    url = "jdbc:mysql://localhost:3306/testdb"
    username = "root"
    password = "root@123"
    table-names = ["testdb.table1"]
    
    # 启用按时间启动
    startup.mode = "timestamp"
    startup.timestamp = 1672531200000  # 2023-01-01 00:00:00 UTC
  }
}

sink {
  Console {
  }
}</code></pre><h2>技术实现</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557736" alt="" title="" loading="lazy"/></p><h3>启动模式枚举</h3><p>在 <code>MySqlSourceOptions</code> 类中定义了所有支持的启动模式，包括新增的 <code>TIMESTAMP</code> 模式：</p><pre><code class="java">public static final SingleChoiceOption&lt;StartupMode&gt; STARTUP_MODE =
    (SingleChoiceOption)
        Options.key(SourceOptions.STARTUP_MODE_KEY)
            .singleChoice(
                StartupMode.class,
                Arrays.asList(
                    StartupMode.INITIAL,
                    StartupMode.EARLIEST,
                    StartupMode.LATEST,
                    StartupMode.SPECIFIC,
                    StartupMode.TIMESTAMP))</code></pre><h3>时间戳过滤实现</h3><p>核心实现在 <code>MySqlBinlogFetchTask</code> 类中，当检测到启动模式为 <code>TIMESTAMP</code> 时，会使用 <code>TimestampFilterMySqlStreamingChangeEventSource</code> 来处理 binlog 事件：</p><pre><code class="java">StartupMode startupMode = startupConfig.getStartupMode();
if (startupMode.equals(StartupMode.TIMESTAMP)) {
    log.info(
        "Starting MySQL binlog reader,with timestamp filter {}",
        startupConfig.getTimestamp());

    mySqlStreamingChangeEventSource =
        new TimestampFilterMySqlStreamingChangeEventSource(
            sourceFetchContext.getDbzConnectorConfig(),
            sourceFetchContext.getConnection(),
            sourceFetchContext.getDispatcher(),
            sourceFetchContext.getErrorHandler(),
            Clock.SYSTEM,
            sourceFetchContext.getTaskContext(),
            sourceFetchContext.getStreamingChangeEventSourceMetrics(),
            startupConfig.getTimestamp());
}</code></pre><h3>偏移量计算</h3><p>在 <code>MySqlSourceFetchTaskContext</code> 中实现了根据时间戳查找对应 binlog 偏移量的逻辑：</p><pre><code class="java">private Offset getInitOffset(SourceSplitBase mySqlSplit) {
    StartupMode startupMode = getSourceConfig().getStartupConfig().getStartupMode();
    if (startupMode.equals(StartupMode.TIMESTAMP)) {
        long timestamp = getSourceConfig().getStartupConfig().getTimestamp();
        try (JdbcConnection jdbcConnection =
                getDataSourceDialect().openJdbcConnection(getSourceConfig())) {
            return findBinlogOffsetBytimestamp(jdbcConnection, binaryLogClient, timestamp);
        } catch (Exception e) {
            throw new SeaTunnelException(e);
        }
    } else {
        return mySqlSplit.asIncrementalSplit().getStartupOffset();
    }
}</code></pre><h2>启动模式对比与适用场景</h2><p>为了更好地理解按时间启动功能在整体 CDC 启动体系中的定位，下面对 MySQL CDC 当前支持的几种启动模式进行对比说明：</p><table><thead><tr><th>启动模式</th><th>启动依据</th><th>优点</th><th>适用场景</th></tr></thead><tbody><tr><td><code>INITIAL</code></td><td>全量 + 当前 binlog</td><td>一次性完成历史与增量同步</td><td>首次接入数据源</td></tr><tr><td><code>EARLIEST</code></td><td>最早可用 binlog</td><td>不依赖具体位点</td><td>binlog 保存周期较长的场景</td></tr><tr><td><code>LATEST</code></td><td>当前最新 binlog</td><td>启动快</td><td>仅关注未来增量数据</td></tr><tr><td><code>SPECIFIC</code></td><td>指定 binlog file + position</td><td>精确可控</td><td>已明确掌握 binlog 位点的场景</td></tr><tr><td><code>TIMESTAMP</code></td><td>指定时间戳（毫秒）</td><td>配置直观、符合业务语义</td><td><strong>任务恢复、数据回溯、按时间窗口同步</strong></td></tr></tbody></table><p>可以看到，<strong>TIMESTAMP 模式并不是替代 SPECIFIC 或 GTID 的“更底层”方案</strong>，而是为了解决“用户只知道时间、不知道 binlog”的典型问题，是一种<strong>以可用性和运维友好性为核心的补充能力</strong>。</p><h2>测试验证</h2><p>该功能在集成测试中得到了充分验证，测试用例 <code>MysqlCDCSpecificStartingOffsetIT</code> 验证了按时间戳启动的正确性 <a href="#1-6" target="_blank">7</a> 。</p><h2>使用注意事项</h2><ol><li><strong>版本要求</strong>：需要 SeaTunnel 2.3.12 或更高版本</li><li><strong>时间戳格式</strong>：必须使用 Unix 时间戳，单位为毫秒</li><li><strong>binlog 可用性</strong>：确保指定时间点对应的 binlog 文件仍然可用</li><li><strong>时区考虑</strong>：时间戳基于 UTC 时区，需要注意时区转换</li></ol><h2>总结</h2><p>SeaTunnel MySQL CDC 的按时间启动功能为数据同步提供了更精确的控制能力，特别适用于需要从特定时间点恢复数据同步的场景。该功能通过时间戳到 binlog 偏移量的转换，实现了高效的时间点定位和数据过滤。</p><h2>Notes</h2><ul><li>该功能在工厂类 <code>MySqlIncrementalSourceFactory</code> 中通过条件配置规则进行参数验证</li><li>除了 MySQL CDC，其他 CDC 连接器如 SQL Server CDC 也支持类似的时间戳启动功能</li></ul>]]></description></item><item>    <title><![CDATA[如何判断三种基本放大电路（共射、共集、共基） 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047557744</link>    <guid>https://segmentfault.com/a/1190000047557744</guid>    <pubDate>2026-01-22 11:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许</p><p>在模拟电路设计中，三极管放大电路是最基础也是最重要的电路单元。</p><p>无论是在音频放大、信号调理还是在嵌入式系统的模拟前端电路中，我们都会遇到共射、共集、共基这三种基本放大电路。</p><p>作为一名嵌入式工程师，虽然我们日常工作更多接触数字电路和软件开发，但在做硬件调试、电路分析时，准确判断这三种放大电路的类型是非常必要的技能。</p><p>今天我就来详细讲解如何快速准确地判断这三种基本放大电路。</p><h2>1. 三种基本放大电路的核心概念</h2><h3>1.1 什么是"共"</h3><p>在开始判断之前，我们首先要理解"共"这个字的含义。</p><p>这里的"共"指的是输入信号和输出信号的公共端，也就是交流接地点。</p><p>三极管有三个极:发射极(E)、基极(B)、集电极(C)。哪个极作为输入和输出的公共端，就叫做"共某极"放大电路。</p><p>需要注意的是，这里说的"公共端"是针对交流信号而言的，不是直流电源的地。</p><p>在实际电路中，某个极可能通过电容接地，对交流信号来说就是接地，但直流上并不接地。</p><p>这是初学者最容易混淆的地方。</p><h3>1.2 三种电路的基本特征</h3><p>共射放大电路(Common Emitter):发射极作为公共端，信号从基极输入，从集电极输出。这是应用最广泛的放大电路，具有电压放大和电流放大能力。</p><p>共集放大电路(Common Collector):集电极作为公共端，信号从基极输入，从发射极输出。这种电路也叫射极跟随器，主要用于阻抗变换和缓冲。</p><p>共基放大电路(Common Base):基极作为公共端，信号从发射极输入，从集电极输出。这种电路常用于高频放大和宽带放大。</p><h2>2. 判断方法详解</h2><h3>2.1 第一步:找出交流接地点</h3><p>判断放大电路类型的关键是找出哪个极是交流接地的。具体方法如下:</p><p><strong>直接接地法</strong>:如果某个极通过导线直接连接到地(GND)，那么这个极就是交流接地点。这是最简单直接的情况。</p><p><strong>电容接地法</strong>:如果某个极通过一个较大容量的电容(通常是电解电容，几微法到几百微法)连接到地，由于电容对交流信号相当于短路，所以这个极对交流信号来说也是接地的。这种电容我们称为旁路电容。</p><p><strong>电源接地法</strong>:对于交流信号来说，电源(<em>V<sub>CC</sub></em> 或 <em>V<sub>EE</sub></em>)也相当于地。因为电源内阻很小，并且通常会并联大容量的滤波电容。所以如果某个极直接连接到电源，对交流信号来说也是接地的。</p><p>举个实际例子，在我之前做的一个音频放大项目中，发射极通过一个 100<em>μF</em> 的电解电容接地，这个电容的作用就是让发射极对音频信号(交流)接地，同时保持直流偏置电压不变。</p><h3>2.2 第二步:确定信号输入输出端</h3><p>找到交流接地点后，剩下的两个极中，一个是信号输入端，一个是信号输出端。</p><p><strong>输入端的判断</strong>:输入端通常会有以下特征:</p><ul><li>连接有耦合电容，用于隔直流通交流</li><li>可能有分压电阻网络，用于提供直流偏置</li><li>在实际电路图中，信号源(如传感器输出、前级电路输出)会连接到这里</li></ul><p><strong>输出端的判断</strong>:输出端通常会有以下特征:</p><ul><li>连接有负载电阻(集电极电阻或发射极电阻)</li><li>可能有耦合电容连接到下一级电路</li><li>在实际电路图中，会连接到后级电路或负载</li></ul><h3>2.3 第三步:综合判断电路类型</h3><p>根据前两步的分析结果，我们可以得出结论:</p><p>如果发射极是交流接地点，基极输入、集电极输出，就是<strong>共射放大电路</strong>。</p><p>如果集电极是交流接地点，基极输入、发射极输出，就是<strong>共集放大电路</strong>。</p><p>如果基极是交流接地点，发射极输入、集电极输出，就是<strong>共基放大电路</strong>。</p><h2>3. 典型电路分析实例</h2><h3>3.1 共射放大电路实例</h3><p>让我们看一个典型的共射放大电路:</p><pre><code>VCC (+12V)
 |
 |
 Rc (集电极电阻， 2kΩ)
 |
 |----输出(Vout)
 |
 C (集电极)
    /
   /  NPN三极管
  /
 B----Rb2----输入(Vin)
 |
 Rb1
 |
GND
​
发射极 E
 |
 Re (发射极电阻， 1kΩ)
 |
 Ce (旁路电容， 100μF)
 |
GND</code></pre><p>在这个电路中:</p><ul><li>发射极通过旁路电容 <em>C<sub>e</sub></em> 接地，所以发射极是交流接地点</li><li>信号从基极输入(通过 <em>R<sub>b2</sub></em>)</li><li>信号从集电极输出(通过 <em>R<sub>c</sub></em>)</li><li>因此这是典型的<strong>共射放大电路</strong></li></ul><p>这种电路的特点是电压放大倍数约为 <em>A<sub>v</sub></em>=−(<em>R<sub>c</sub>/</em>r<sub>be</sub>)，其中 <em>r<sub>be</sub></em> 是三极管的输入电阻。负号表示输出信号与输入信号反相。</p><p>在实际应用中，我曾经用这种电路做过一个温度传感器的信号放大。</p><p>传感器输出的微弱电压信号(几十毫伏)通过共射放大电路放大到几伏，然后送入 STM32 的 ADC 进行采集。</p><p>代码示例如下:</p><pre><code>// STM32 HAL库ADC采集代码示例
void ReadAmplifiedSignal(void)
{
    uint32_t adcValue;
    float voltage;
    float temperature;
    
    // 启动ADC转换
    HAL_ADC_Start(&amp;hadc1);
    
    // 等待转换完成
    if(HAL_ADC_PollForConversion(&amp;hadc1， 100) == HAL_OK)
    {
        // 读取ADC值
        adcValue = HAL_ADC_GetValue(&amp;hadc1);
        
        // 转换为电压值(假设参考电压3.3V， 12位ADC)
        voltage = (adcValue * 3.3) / 4095.0;
        
        // 根据放大倍数反推原始信号
        // 假设放大倍数为50倍
        float originalVoltage = voltage / 50.0;
        
        // 转换为温度(假设传感器灵敏度10mV/℃)
        temperature = originalVoltage / 0.01;
        
        printf("Temperature: %.2f °C\n"， temperature);
    }
    
    HAL_ADC_Stop(&amp;hadc1);
}</code></pre><h3>3.2 共集放大电路实例</h3><p>共集放大电路的典型结构如下:</p><pre><code>VCC (+12V)
 |
 |----C (集电极，直接接电源)
    /
   /  NPN三极管
  /
 B----Rb----输入(Vin)
 |
 Rb1
 |
GND
​
发射极 E
 |
 |----输出(Vout)
 |
 Re (发射极电阻， 1kΩ)
 |
GND</code></pre><p>在这个电路中:</p><ul><li>集电极直接连接到 <em>V<sub>CC</sub></em>，对交流信号来说相当于接地</li><li>信号从基极输入</li><li>信号从发射极输出</li><li>因此这是<strong>共集放大电路</strong></li></ul><p>共集放大电路的电压放大倍数接近 1(<em>A<sub>v</sub></em>≈1)，但电流放大倍数很高(A<em><sub>i</sub></em>=1+<em>β</em>)。</p><p>输出电压跟随输入电压变化，所以也叫射极跟随器。</p><p>我在做一个 CAN 总线驱动电路时，就使用了共集放大电路作为缓冲级。</p><p>因为前级电路输出阻抗较高，直接驱动 CAN 收发器会导致信号失真，通过射极跟随器进行阻抗变换，可以有效解决这个问题。</p><pre><code>// CAN总线发送数据示例
void CAN_SendMessage(uint32_t id， uint8_t *data， uint8_t len)
{
    CAN_TxHeaderTypeDef txHeader;
    uint32_t txMailbox;
    
    // 配置发送帧
    txHeader.StdId = id;
    txHeader.IDE = CAN_ID_STD;
    txHeader.RTR = CAN_RTR_DATA;
    txHeader.DLC = len;
    
    // 发送数据
    // 射极跟随器确保信号完整性
    if(HAL_CAN_AddTxMessage(&amp;hcan1， &amp;txHeader， data， &amp;txMailbox) != HAL_OK)
    {
        Error_Handler();
    }
    
    // 等待发送完成
    while(HAL_CAN_IsTxMessagePending(&amp;hcan1， txMailbox));
}</code></pre><h3>3.3 共基放大电路实例</h3><p>共基放大电路的典型结构:</p><pre><code>VCC (+12V)
 |
 |
 Rc (集电极电阻， 2kΩ)
 |
 |----输出(Vout)
 |
 C (集电极)
    /
   /  NPN三极管
  /
 B----Rb----Cb(旁路电容)----GND
 |
 Rb1
 |
GND
​
发射极 E
 |
 |----输入(Vin)
 |
 Re (发射极电阻， 1kΩ)
 |
GND</code></pre><p>在这个电路中:</p><ul><li>基极通过旁路电容 <em>C<sub>b</sub></em> 接地，所以基极是交流接地点</li><li>信号从发射极输入</li><li>信号从集电极输出</li><li>因此这是<strong>共基放大电路</strong></li></ul><p>共基放大电路的特点是输入阻抗低，输出阻抗高，电压放大倍数较高，而且输入输出同相。</p><p>它特别适合用于高频放大，因为没有密勒效应的影响。</p><p>在射频电路设计中，共基放大电路应用很广。</p><p>我在做一个 433MHz 无线模块的项目时，就使用了共基放大电路作为射频前端的第一级放大。</p><h2>4. 快速判断技巧总结</h2><h3>4.1 口诀记忆法</h3><p>为了方便记忆，我总结了一个口诀:</p><p><strong>"地在哪，共哪极;入出剩，定类型"</strong></p><p>意思是:先找交流地在哪个极，那就是共哪个极;然后在剩下的两个极中确定输入和输出，就能确定电路类型。</p><h3>4.2 特征对比表</h3><table><thead><tr><th>电路类型</th><th>交流接地极</th><th>输入极</th><th>输出极</th><th>电压放大</th><th>电流放大</th><th>输入阻抗</th><th>输出阻抗</th><th>相位关系</th></tr></thead><tbody><tr><td>共射</td><td>E</td><td>B</td><td>C</td><td>高</td><td>高</td><td>中</td><td>中</td><td>反相</td></tr><tr><td>共集</td><td>C</td><td>B</td><td>E</td><td>≈1</td><td>高</td><td>高</td><td>低</td><td>同相</td></tr><tr><td>共基</td><td>B</td><td>E</td><td>C</td><td>高</td><td>≈1</td><td>低</td><td>高</td><td>同相</td></tr></tbody></table><h3>4.3 实用判断流程</h3><p>在实际工作中，我总结了一个快速判断流程:</p><p><strong>步骤 1</strong>:观察三个极的连接情况，找出哪个极通过电容接地或直接接地或接电源。</p><p><strong>步骤 2</strong>:如果不明显，可以用万用表测量各极对地的交流阻抗，阻抗最小的就是交流接地点。</p><p><strong>步骤 3</strong>:在电路板上追踪信号走向，看信号从哪里来，到哪里去。</p><p><strong>步骤 4</strong>:结合电路的功能需求，验证判断结果是否合理。比如需要阻抗变换的地方通常用共集，需要高增益的地方通常用共射。</p><h2>5. 常见误区和注意事项</h2><h3>5.1 直流地与交流地的混淆</h3><p>这是最常见的错误。</p><p>有些同学看到发射极通过电阻接地，就认为是共射电路，但如果这个电阻没有并联旁路电容，那么对交流信号来说发射极并不是接地的，这时候要重新分析。</p><p>例如，如果发射极电阻 <em>R<sub>e</sub></em> 没有并联电容，那么这个电阻会引入负反馈，改变电路的性能，但电路类型的判断方法不变，仍然要看交流接地点在哪里。</p><h3>5.2 PNP 与 NPN 三极管的区别</h3><p>前面的例子都是用 NPN 三极管，如果是 PNP 三极管，电源极性相反，但判断方法完全一样。</p><p>关键还是看哪个极是交流接地点。</p><h3>5.3 复合电路的判断</h3><p>在实际电路中，经常会遇到多级放大电路，每一级可能是不同类型的放大电路。</p><p>这时候要逐级分析，不能混为一谈。</p><p>比如常见的组合是:共射-共集级联，第一级提供电压放大，第二级提供阻抗变换。</p><h2>6. 工程应用建议</h2><p>作为嵌入式工程师，虽然我们主要做软件开发，但理解这些基本的模拟电路对我们的工作很有帮助。</p><p>在实际项目中:</p><p><strong>硬件调试时</strong>:当遇到信号异常，我们需要能够快速判断电路类型，分析可能的故障点。比如共射电路输出信号反相，如果发现输出没有反相，可能是电路类型判断错误或者电路有问题。</p><p><strong>电路设计时</strong>:选择合适的放大电路类型。需要高增益用共射，需要阻抗匹配用共集，需要高频响应用共基。</p><p><strong>与硬件工程师沟通时</strong>:能够准确理解电路原理图，提出合理的修改建议。我在项目中经常需要和硬件工程师讨论 ADC 前端电路的设计，准确判断放大电路类型是有效沟通的基础。</p><p>掌握这三种基本放大电路的判断方法，不仅能帮助我们更好地理解模拟电路，也能提升我们作为嵌入式工程师的综合能力。</p><p>希望这篇文章能对大家有所帮助，在实际工作中遇到相关问题时，能够快速准确地做出判断。</p>]]></description></item><item>    <title><![CDATA[Claude Code 支持重磅扩展 Skills —— 用最新 API 构建更靠谱的 AI 项目 ]]></title>    <link>https://segmentfault.com/a/1190000047557753</link>    <guid>https://segmentfault.com/a/1190000047557753</guid>    <pubDate>2026-01-22 11:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在上一篇《<strong>Claude Code × 智谱 BigModel 实战集成指南</strong>》中，我们已经完成了一次完整的项目实战。项目<strong>可以正常运行</strong>，但在后续代码 Review 时，一个问题逐渐暴露出来：</p><blockquote><strong>生成的代码虽然能跑，但大量 API 和用法已经过时，与最新官方文档存在明显偏差。</strong></blockquote><p>这在 AI 辅助开发中其实非常常见——模型的训练数据更新速度，往往赶不上框架和 SDK 的迭代速度。</p><p>正巧这时，一位朋友向我推荐了 <strong>Anthropic 最新发布的 Agent Skills</strong>，通过 <em>plugins</em> 的方式，让 Claude 在生成代码时 <strong>动态读取最新官方文档和工具能力</strong>，从而显著降低“写得像，但跑不通”的概率。</p><p>本文就是这次探索的完整记录。</p><hr/><h2>一、Agent Skills 是什么？</h2><p>官方仓库地址：</p><blockquote><a href="https://link.segmentfault.com/?enc=e9qho45XYItt7%2F82uJwz%2BA%3D%3D.VZV%2BH7jpFeZUesFi4QqoswuWLrANZ0DKG3R6yU5bW1fiohQ5PeVTi48oggSJLKNa" rel="nofollow" target="_blank">https://github.com/anthropics/skills</a></blockquote><p><strong>Agent Skills</strong> 可以理解为：</p><blockquote>一套可插拔的“能力模块”，用于教会 Claude <strong>如何用正确的方法、最新的工具、可重复的流程</strong> 来完成特定任务。</blockquote><p>在技术层面上：</p><ul><li>每个 Skill 本质上是一个文件夹</li><li><p>内部包含：</p><ul><li>指令（instructions）</li><li>脚本（scripts）</li><li>资源文件（resources）</li></ul></li><li>Claude Code 会在运行时动态加载这些 Skills</li></ul><h3>它能解决什么问题？</h3><p>Agent Skills 的核心价值在于 <strong>“降低幻觉 + 提高一致性”</strong>，典型应用场景包括：</p><ul><li>按公司/团队的编码规范生成代码</li><li>按最新官方文档调用 API（而不是靠模型记忆）</li><li>执行固定的工程化流程（初始化项目、生成目录结构、部署脚本等）</li><li>自动化个人或组织级任务</li></ul><p>简单来说：</p><blockquote><strong>Skills 不是让模型更聪明，而是让模型更“守规矩”。</strong></blockquote><hr/><h2>二、在 Claude Code 中安装 Agent Skills</h2><p>在 Claude Code 命令行中执行：</p><pre><code class="bash">/plugin marketplace add anthropics/skills</code></pre><p>安装完成后，你就已经具备了使用官方 Skills 的能力。</p><blockquote>这一步相当于为 Claude Code 打开了“官方增强模式”。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557755" alt="PixPin_2026-01-22_10-07-25.png" title="PixPin_2026-01-22_10-07-25.png"/></p><hr/><h2>三、安装 context7 插件（关键步骤）</h2><p>接下来是本文的重点：<strong>context7</strong>。</p><h3>1️⃣ 打开插件管理</h3><p>在 Claude Code 中输入：</p><pre><code class="shell">/plugins</code></pre><p>然后使用键盘 ➡️ 进入 <strong>Discover</strong>。</p><h3>2️⃣ 搜索并安装 context7</h3><p>在搜索框中输入 <code>context7</code>，完成安装。</p><blockquote>context7 本质上是一个 MCP（Model Context Protocol）插件，<br/>能让 Claude <strong>直接参考并对齐最新的官方文档内容</strong>。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557756" alt="PixPin_2026-01-22_10-09-22.png" title="PixPin_2026-01-22_10-09-22.png" loading="lazy"/></p><hr/><h2>四、使用 context7 生成项目代码</h2><p>安装完成后，就可以在 Prompt 中显式声明使用 <code>context7</code>。</p><h3>示例 Prompt</h3><pre><code class="markdown">---
name: context7
description: 使用 Context7，基于框架最新的官方文档
---

# context7

## 指南
已使用以下技术栈生成企业级项目：
- 使用 Context7，基于最新的官方文档
- FastAPI 0.128.0，带 Token 认证
  - 使用 sqlite 生成 token
  - 不使用 JWT，仅做 Token 校验
- langchain 1.2.6，使用 create_agent
- langchain-ollama 1.0.1
  - model：qwen3-vl:32b
  - embedding：qwen3-embedding:8b
- langgraph 1.0.6
- Milvus（pymilvus）2.6.6
- langfuse 3.12.0</code></pre><p>通过这种方式，你是在<strong>明确告诉 Claude</strong>：</p><blockquote>不要靠“印象”写代码，而是<strong>以当前官方文档为准</strong>。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557757" alt="PixPin_2026-01-22_10-29-45.png" title="PixPin_2026-01-22_10-29-45.png" loading="lazy"/></p><hr/><h2>五、实际体验与问题分析</h2><h3>真实结论只有一句话：</h3><blockquote><strong>效果明显提升，但依然不能“一次生成直接可用”。</strong></blockquote><h3>优点</h3><ul><li>API 使用明显更接近最新文档</li><li>过时参数、废弃方法显著减少</li><li>工程结构更合理，思路更偏向“真实项目”</li></ul><h3>仍然存在的问题</h3><ul><li>复杂技术栈组合（LangChain + LangGraph + Milvus + Langfuse）</li><li>仍然需要 <strong>多轮调试才能完全跑通</strong></li><li>某些边界用法依然存在偏差</li></ul><h3>我的判断</h3><blockquote><strong>并不是 context7 不行，而是模型生成速度，依然落后于框架演进速度。</strong></blockquote><p>context7 做到的是：</p><ul><li>让 Claude <em>看得到</em> 最新文档</li><li>但最终“怎么拼起来”，仍然依赖模型本身的推理与代码能力</li></ul><hr/><h2>六、总结</h2><p>如果你正在使用 Claude Code 做偏工程化、偏企业级的项目开发，我的建议是：</p><p>✅ <strong>一定要上 Agent Skills</strong></p><p>✅ <strong>能用 context7 就用 context7</strong></p><p>❌ 不要再完全相信“模型记忆里的 API”</p><p>但同时也要有一个清醒认知：</p><blockquote><strong>AI 辅助开发 = 更快的起点，而不是免调试的终点。</strong></blockquote><p>在当前阶段，最理想的模式依然是：</p><blockquote><strong>AI 生成 + 人类 Review + 多轮修正</strong></blockquote><p>后续我也会继续记录 Claude Code + MCP + 多模型协作 的实践经验，欢迎关注。</p>]]></description></item><item>    <title><![CDATA[如何申请OV SSL证书 逼格高的仙人掌 ]]></title>    <link>https://segmentfault.com/a/1190000047557328</link>    <guid>https://segmentfault.com/a/1190000047557328</guid>    <pubDate>2026-01-22 10:05:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OV SSL证书即组织验证型SSL证书，与域名验证型DV SSL证书相比，OV证书在验证过程中除了确认域名所有权外，还要求对申请的公司进行严格的审核，以确保公司的真实性和合法性。可以在浏览器地址栏绿色的小锁中显示单位名称，来增强用户信任，深受单位官网的喜爱。</p><h3>谁应该使用OVSSL证书？</h3><ul><li><strong>企业官方网站</strong>：展示公司形象，与客户建立信任。</li><li><strong>电子商务网站</strong>：处理在线支付和客户个人信息。</li><li><strong>会员登录/用户门户网站</strong>：保护用户登录凭证和个人数据。</li><li><strong>API 服务接口</strong>：确保服务器到服务器通信的身份真实性。</li><li><strong>需要提交敏感信息的网站</strong>（如医疗、教育、B2B平台）。</li><li><strong>追求比DV证书更高信任等级的所有商业网站。</strong></li></ul><p><img width="690" height="294" referrerpolicy="no-referrer" src="/img/bVdnDcV" alt="" title=""/></p><h3><a href="https://link.segmentfault.com/?enc=4qgDat%2F6UC5djhHNm5Eywg%3D%3D.84pzJ98AWEcW4OYT2NLyuYhGTyreOA7WAQFzrprj9IrPBUeYi0Zx4UB2PdwfwLFMSPq6t3t8ly2MnNwfHFEHZg%3D%3D" rel="nofollow" target="_blank"><strong>OV SSL证书申请流程：</strong></a></h3><p><strong>一、注册账号</strong></p><p>访问<strong>JoySSL</strong>官方网站，在右上角找到“注册”按钮并点击。填写相关信息，创建一个证书管理账号。注册过程中，务必填写特定的注册码<strong>230970</strong>，这样才可以获得渠道低价和全程技术支持。</p><p><strong>二、选择证书类型与年限</strong></p><p>登录账号后，进入SSL证书栏，找到“OV证书”选项。根据自身需求，选择OV单域名、OV通配符、或者OV IP地址等SSL证书后，点击“下单”，并通过在线支付或公对公转账的方式完成支付。</p><p><strong>三、申请证书</strong></p><p>在申请页面，需要填写一系列信息，包括域名、单位名称、联系人、联系方式、邮箱等。这些信息将用于验证单位的真实性，所以务必确保准确无误。</p><p><strong>四、验证域名或IP的管理权</strong></p><p>提交申请后，并要求验证域名或者IP地址的所有权。按照系统提示的操作步骤进行验证，包括域名DNS解析认证或者服务器文件验证，操作完成后提交。</p><p><strong>五、组织信息审核</strong></p><p>JoySSL会对企业的组织信息进行验证，通常会通过电话、电子邮件等方式确认公司信息的真实性，审核过程一般在1到3个工作日左右完成。</p><p><strong>六、部署证书</strong></p><p>一旦审核通过后，JoySSL将签发OV SSL证书。下载已经签发的证书，根据JoySSL提供的安装指南或服务器文档，将证书安装到服务器上。</p><p><strong>七、测试证书</strong></p><p>使用浏览器访问网站，检查HTTPS访问是否正常工作，并且浏览器没有任何安全警告。同时，查看浏览器地址栏是否显示安全锁标志以及点开小锁是否有单位名称，这表示OV SSL证书已成功部署并生效。</p>]]></description></item><item>    <title><![CDATA[烟草行业科技创新为什么需要“既懂技术，更懂行业”的中烟创新 中烟创新 ]]></title>    <link>https://segmentfault.com/a/1190000047557390</link>    <guid>https://segmentfault.com/a/1190000047557390</guid>    <pubDate>2026-01-22 10:04:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当人工智能与实体经济的融合步入深水区，烟草作为国民经济的重要支柱产业，其智能化进程已超越单一的生产自动化，全面渗透至管理、采购、专卖、物流、营销与服务等全价值链环节。打破数据壁垒、构建端到端的协同智能运营体系，成为推动行业向高质量发展的核心课题。</p><p>在此背景下，北京中烟创新科技有限公司（简称：中烟创新）作为专注于烟草行业的AI解决方案提供商，经过三年扎实深耕，已将人工智能解决方案植入烟草商业公司23个部门的68个场景及工业公司19个部门的56个场景。实践印证，AI不仅是提升效率的工具，更是驱动业务模式重构、重塑行业竞争力的战略引擎。</p><p>一、直面行业痛点，响应深度智能化需求在宏观经济、行业政策与企业内部治理的多重驱动下，烟草企业对智能化的需求日益深化。尤其在综合办公、合规风控、管理决策等领域，传统模式普遍面临效率低下、标准不一、合规风险高等挑战。要破解这些难题，仅依靠技术远远不够，唯有对行业运作逻辑的深刻理解，才能使解决方案真正落地生根。中烟创新坚信：技术是基础，而对业务的洞察才是价值实现的钥匙。 </p><p>三年来，团队深入多家烟草企业的财务、法规、专卖、营销、物流等核心部门，与业务人员共同作业，精准把握行业特有的业务流程、合规要求与管理难点。这种深度共创构建了公司独特的“行业知识壁垒”，并具体体现于每一个优化细节中：在采购管理中，精准理解“一项一卷”评查要求，破解采购全流程的合规难题；在专卖执法中，依据行政处罚案卷规范，实现法条自动匹配、文书一键生成；在财务审核中，平衡效率与风控，通过AI实现发票自动识别与智能审验，大幅提升自动化率。</p><p>二、“懂行业”的AI：从技术赋能到业务融合基于对业务的深入理解，中烟创新的AI应用不是简单的技术叠加，而是与业务深度咬合的“智能伙伴”。例如，为济南市烟草专卖局打造的招标文件查重系统，以及为行业客户累计开发的128个业务场景解决方案，均证明了“行业知识+人工智能”的融合价值。烟草行业的科技创新，是一场以"懂行业"为前提、以"真落地"为标准、以"创价值"为目标的深度数字化变革，选择"既懂技术更懂烟草"的深耕型合作伙伴成为制胜关键。</p><p>三、沉淀与积累：以实力构建信任基石凭借坚实的技术积累与持续的创新实践，中烟创新已构建起体系化的资质与荣誉矩阵：被认定国家高新技术企业，连续入选北京软件核心竞争力企业，并认定为科技型中小企业、创新型中小企业；连续三年获评诚信企业认证，企业社会责任治理AA级；累计拥有发明专利、软件著作权等知识产权百余项，荣获国家、行业及省市级荣誉上百项。</p><p>公司不仅提供技术，更注重为每一项创新项目注入可持续的软实力支撑，围绕软件著作权、高水平论文、发明专利等进行全方位支持，全面塑造项目的长期影响力与行业创新价值链。四、携手共创智能化新格局历经三年打磨，中烟创新已在财务管理、专卖监管、采购合规等领域形成成熟的产品矩阵与解决方案。每一套方案都源于真实的业务场景，经过多家烟草企业的实践验证，具备快速部署、敏捷响应的特点。数字化转型浪潮奔涌向前。</p><p>中烟创新将继续坚持以客户为中心，加大研发投入，深化行业理解，以“AI+行业知识”的双重驱动，为烟草行业提供更智能、更可靠、更贴身的解决方案。选择中烟创新，不仅是选择一个技术伙伴，更是选择一位懂行业、通业务、可持续赋能的智能化升级同行者。</p>]]></description></item><item>    <title><![CDATA[dbeaver 社区版的 ai 只能配置 openai 或者 copilot 吗？如何配置智谱免费模]]></title>    <link>https://segmentfault.com/a/1190000047557394</link>    <guid>https://segmentfault.com/a/1190000047557394</guid>    <pubDate>2026-01-22 10:03:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>dbeaver 社区版的 ai 只能配置 openai 或者 copilot 吗？如何配置智谱、硅基流动等等免费模型</p><p><a href="https://link.segmentfault.com/?enc=%2FOhGqc%2BDBhWvjxb%2BZpaAmA%3D%3D.nkSe4FFfTxOz47AH%2BXJ8auZa%2FymXm4fA47P%2BbmtlGTT2U%2BCpz3c7bf9rTgbq1DvxFgg8EfvyQbHbt2Md%2BKZ2MQ%3D%3D" rel="nofollow" target="_blank">https://docs.bigmodel.cn/cn/guide/models/free/glm-4.7-flash</a></p><p>API base URL 改成：<a href="https://link.segmentfault.com/?enc=3AtTccpp2pLcaMj7w7DTmg%3D%3D.7VnjgNRlJWotAtP1taM3el156Ps%2B4zubrGPLG7%2FO9zdZRyQAmjxEzwbIwh4RUIHa" rel="nofollow" target="_blank">https://open.bigmodel.cn/api/paas/v4/</a></p><blockquote>参考：<a href="https://link.segmentfault.com/?enc=g4GnFaSMse85HDlM4kKKCw%3D%3D.Gbj4MQOSHEjZs5VMkZD915ZH7VLZJxOfbNKzikyHN0lr4mhssFglLXnS2HtTlFTM4BGLIyeLBy1ZO5X44Yr1fA%3D%3D" rel="nofollow" target="_blank">https://docs.bigmodel.cn/cn/guide/develop/openai/introduction</a></blockquote><p>Model 改成 glm-4.7-flash</p><p>登录 bigmodel.cn 得到你的 API token</p><p><img width="723" height="504" referrerpolicy="no-referrer" src="/img/bVdnH0S" alt="图片.png" title="图片.png"/></p>]]></description></item><item>    <title><![CDATA[SpreadJS V19.0 新特性解密：报表导出黑科技，公式逻辑全保留 葡萄城技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047557396</link>    <guid>https://segmentfault.com/a/1190000047557396</guid>    <pubDate>2026-01-22 10:02:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着企业数字化转型的深入，报表不仅是数据的展示工具，更是业务逻辑的载体。在与众多开发者的交流中，我们发现了一个长期存在的痛点：<strong>“为什么我精心设计的报表导出到 Excel 后，动态的公式都变成了死板的数值？”</strong></p><p>在即将发布的 <strong>SpreadJS V19.0</strong> 中，我们针对报表插件（ReportSheet）带来了一项重量级更新——“<strong>导出预览报表到 Excel 时保留公式</strong>”功能。今天，我就带大家深度解密这项特性，看它如何打破数据与逻辑之间的壁垒。</p><h3>一、 痛点回顾：消失的“计算逻辑”</h3><p>在过去，开发者在报表模板中定义的公式，在导出为 Excel 文件时，往往会被计算引擎处理并转化为<strong>静态值</strong>。</p><p>这意味着，当终端用户拿到导出的 Excel 文件并试图修改其中的基础数据时，报表中的小计、总计等关键指标并不会随之更新。用户不得不手动重新输入 Excel 公式，这不仅降低了工作效率，也让报表失去了原本的动态交互灵魂。</p><h3>二、 核心能力：让 Excel 报表“动”起来</h3><p>SpreadJS V19.0 引入的“保留公式导出（Preserve Formula in Export）”功能，允许用户在将报表导出为 Excel 文件时，<strong>完整保留单元格中的计算逻辑</strong>。</p><h4>1. 核心价值总结</h4><ul><li><strong>逻辑无缝延续</strong>：导出后的 Excel 依然拥有动态计算能力，而非固定数值。</li><li><strong>自由编辑体验</strong>：终端用户修改 Excel 单元格内容后，相关公式会自动重算，保持与原始系统一致的交互体验。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557398" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>三、 深度解析：它是如何实现的？</h3><p>为了兼顾各种复杂的报表场景，我们针对不同的公式类型和布局制定了严密的导出策略。</p><h4>1. 标准 Excel 函数处理</h4><ul><li><strong>连续区域引用</strong>：如果报表展开后的单元格区域是连续的，导出时将作为单一区域引用。</li><li><strong>不连续区域引用</strong>：对于 SUM、AVERAGE、MIN、MAX 等聚合函数，即使报表生成的区域不连续，SpreadJS 也会智能地将其导出为多个区域的组合引用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557399" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557400" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>2. R.V（报表变量/视觉）公式的智能转换</h4><p>R.V 公式是 SpreadJS 报表中的特色功能。在 V19.0 中：</p><ul><li>如果公式在预览模式下可解析，导出时会精准转换为 <strong>Excel 实际单元格引用</strong>。</li><li>对于表达式中部分可解析的情况，我们会使用 <code>SJS.EMPTY_CELL</code>（值为 0）进行占位，确保公式结构的完整性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557401" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557402" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>3. 报表专用公式的保留</h4><p>对于如 <code>R.Index</code>、<code>R.Rank</code>、<code>R.YoY</code>（同比）等 SpreadJS 专有的报表函数，导出时会保留其函数名和引用。虽然 Excel 原生不支持这些函数（会显示为 <code>#NAME?</code>），但这为二次开发或后续回导提供了珍贵的元数据信息。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557403" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>四、 开发者友好：配置只需一个属性</h3><p>在 SpreadJS V19.0 中，启用这项功能非常简单。</p><h4>方式一：API 配置</h4><p>在设置 <code>StaticCell</code> 类型的模板单元格时，只需指定 <code>preserveFormulaInExport</code> 属性：</p><pre><code class="JavaScript">// 代码示例
export type StaticCell = {
    type: 'Static',
    preserveFormulaInExport?: boolean; // 设为 true 即可开启
    // ... 其他属性
};</code></pre><h4>方式二：设计器直观操作</h4><p>如果您使用的是 SpreadJS 设计器，完全无需编写代码。在“报表单元格”属性面板中，勾选“<strong>导出 Excel 时保留公式</strong>”选项即可一键开启。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557404" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>五、 结语</h3><p>“保留公式导出”特性的加入，标志着 SpreadJS 报表插件在“<strong>所见即所得</strong>”的基础上，进一步实现了“<strong>所获即所用</strong>”。它不仅是导出格式的改进，更是对数据生命周期的深度赋能。</p><p>SpreadJS V19.0 还有更多关于 <strong>AI 插件增强、协同插件正式版、WebWorker 增量计算</strong>等重磅特性蓄势待发。</p><p><strong>道阻且长，行则将至。</strong> 让我们共同期待 V19.0 带来的生产力变革！</p><p><em>注：具体技术文档请以正式发布版本为准。</em></p>]]></description></item><item>    <title><![CDATA[UTM 5.0.1 发布 - 基于 QEMU 的 macOS 虚拟机与模拟器应用 sysin ]]></title>    <link>https://segmentfault.com/a/1190000047557425</link>    <guid>https://segmentfault.com/a/1190000047557425</guid>    <pubDate>2026-01-22 10:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>UTM 5.0.1 发布 - 基于 QEMU 的 macOS 虚拟机与模拟器应用</p><p>在 iPhone 和 iPad 中虚拟化 Windows、Linux 和 Unix，如此简单！</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=eMLlWIU3njaWHzY2wqbkAg%3D%3D.gZKXE3oFnhSLRjXrQepRaUeskPmDFQSqBFToGKMHvzY%3D" rel="nofollow" target="_blank">https://sysin.org/blog/utm-5/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=q6uGAP1RJJIGS%2FQ2Z2048Q%3D%3D.xjazUyUn4FeTiwrEP47fdiTjtDryEppvdtl%2F6Jmm7lU%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>UTM for Mac 是一款基于 QEMU 的 macOS 虚拟机与模拟器应用，用于在 Mac 上运行和测试 Windows、Linux 等多种操作系统。</p><h2>UTM 是什么？</h2><p>UTM 是一款面向 iOS 和 macOS 的全功能系统模拟器和虚拟机宿主，基于 QEMU。简而言之，它可以让你在 Mac、iPhone 和 iPad 上运行 Windows、Linux 等多种操作系统。</p><p>QEMU（Quick Emulator）是一款开源的机器模拟器和虚拟化软件，由 Fabrice Bellard 于 2003 年创建。它通过动态二进制转换技术实现跨平台虚拟化，支持 x86、ARM、MIPS 等多种处理器架构。QEMU 既可以作为独立虚拟机运行完整的操作系统，也可与 KVM（基于内核的虚拟机）配合使用实现硬件加速虚拟化 (sysin)，这种组合方案能够提供接近原生性能的虚拟化体验。</p><p>QEMU 核心特点是跨平台虚拟化支持，支持 x86、ARM、RISC-V、PowerPC 等多种处理器架构，可在 Windows、Linux、macOS 等操作系统上运行，这意味着在 iOS 上也可以运行 Windows x86 系统。</p><p>UTM 和 QEMU 的关系可以概括为：UTM 是基于 QEMU 构建的图形化虚拟化与系统模拟工具。</p><h2>快速入门</h2><p><img referrerpolicy="no-referrer" src="https://sysin.org/blog/utm-5/utm-getting-started-1.webp" alt="快速入门 - 步骤一" title="快速入门 - 步骤一"/></p><p>1、使用 “+” 按钮打开新的虚拟机向导。</p><p>2、 选择一个虚拟机以显示其详细信息；对虚拟机进行右键点击或使用 Force Touch 可打开 <strong>操作菜单</strong> (actions menu)。</p><p>3、使用启动按钮快速启动虚拟机。</p><p><img referrerpolicy="no-referrer" src="https://sysin.org/blog/utm-5/utm-getting-started-2.webp" alt="快速入门 - 步骤二" title="快速入门 - 步骤二" loading="lazy"/></p><p>4、通过工具栏最右侧的图标打开 <strong>设置</strong> (settings)。</p><p>5、在详情视图中，点击 “浏览…” 按钮选择一个 <strong>共享目录</strong> ( shared directory)。</p><p>6、在详情视图中打开驱动器菜单，选择要挂载（或弹出）的可移动磁盘映像</p><h2>UTM 5 新增功能</h2><p>✅ <strong>亮点</strong></p><ul><li><strong>改进了 Linux 的图形加速</strong>：在 Linux 客户机中，通过 Mesa 的 VirtIO Venus 驱动现已支持 Vulkan 1.3；在 macOS（仅限）上，借助全新的 Apple Core OpenGL 后端支持 OpenGL 4.1。</li></ul><p>✅ <strong>已知问题</strong></p><ul><li>（macOS）Apple CoreGL 后端不支持 Vulkan。</li><li>由于缺少相关特性，DXVK 无法工作（#7575）。</li><li>KosmicKrisp 目前以 WIP（进行中）形式提供，但上游当前版本尚不完整，因此推荐使用 MoltenVK 驱动。</li><li>（macOS 26）由于 <code>HV_UNSUPPORTED</code> 错误，虚拟机无法启动。这是一个构建问题，将在下一个版本中修复。临时解决方法：在设置中禁用 Vulkan（#7579）。</li></ul><p>✅ <strong>更改内容（v5.0.1）</strong></p><ul><li>CocoaSpice：重构了 Metal 渲染器，以提升性能并降低延迟</li><li>修复了当 BIOS 文件名中包含逗号时无法加载的问题</li><li>默认 FPS 现在将设置为 macOS 和 iPadOS 显示器的最大刷新率（配备 ProMotion 的 iPhone 默认仍为 60Hz，但可在设置中覆盖为 120Hz）</li><li>（macOS）修复了在启用 Vulkan 时启动出现 HV_UNSUPPORTED 的问题（#7579）</li><li>（iOS）修复了外接显示器的分辨率问题（#6040）</li><li>（iOS）双指缩放将自动关闭来自来宾系统的缩放自动更新（可通过调整大小按钮重置）</li><li>（iOS）修复了外接显示器菜单未更新的问题 (sysin)</li><li>（iOS）修复了外接显示器缩放比例不正确的问题，并且在连接外接显示器时将自动缩放以适配屏幕</li><li>（iOS）修复了关闭虚拟机电源后会显示主屏幕但不会在后台终止 UTM 的问题</li></ul><h2>下载地址</h2><p><strong>UTM v5.0.1</strong> Release 2026-01-20</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=6rHiDUJDQwZQhogm0yYZyA%3D%3D.ybhDso4PkIZ0HUYgnmGg2%2FhGA9UezQnisIrjxg8QJNI%3D" rel="nofollow" target="_blank">https://sysin.org/blog/utm-5/</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=4ZLyDRMDoEFTU6BwmX%2BVzg%3D%3D.N1PBKv1FCgUeH9Xwx2JIP7SrCfT2H2YU1j%2F2p%2BCezK4%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[2026开年即用：卡片式计划编排工具快速上手指南与核心功能攻略 NAVI_s1mple ]]></title>    <link>https://segmentfault.com/a/1190000047557428</link>    <guid>https://segmentfault.com/a/1190000047557428</guid>    <pubDate>2026-01-22 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在多项目并发与复杂任务流管理的数字化协作中，传统的线性计划已难以应对灵活多变的业务需求 。如果计划编排缺乏原子化的卡片管理，可能会导致：</p><ul><li><strong>执行断层</strong>：计划背景被淹没在厚重文档中，导致执行者无法直观获取关键信息 。</li><li><strong>排期僵化</strong>：无法快速响应需求变更，导致项目排期与实际进度严重脱节。</li><li><strong>透明度缺失</strong>：团队成员难以实时了解全局节奏及各阶段的准入准出标准。</li><li><strong>资源错配</strong>：缺乏对任务依赖关系的清晰视图，容易造成资源闲置或关键路径阻塞。</li></ul><p>卡片式计划编排工具通过将模糊的项目计划转化为可灵活组合、可实时追踪、可多维对齐的卡片执行引擎，确保团队在复杂的竞争环境中实现精准交付 。</p><h2><strong>卡片式计划编排工具的核心特性</strong></h2><ul><li><strong>原子化任务卡片</strong>：将复杂计划拆解为独立卡片，封装背景、标准、工时等核心元数据 。</li><li><strong>多维可视化视图</strong>：支持看板、时间线、甘特图等多种表现形式，实现计划的直观编排 。</li><li><strong>依赖关系建模</strong>：清晰标记卡片间的逻辑关联（如包含、阻塞、并行），自动计算关键路径 。</li><li><strong>自动化流转规则</strong>：基于触发器实现卡片状态自动更新，确保计划与执行同步 。</li><li><strong>递归进度核算</strong>：底层原子卡片的执行质量自动驱动顶层计划的达成率评估。</li></ul><h2><strong>卡片式计划编排工具的重要意义</strong></h2><ol><li><strong>消除信息颗粒度偏差</strong>：通过卡片的高度封装，确保执行层与管理层在任务定义上达成高度共识 。</li><li><strong>提升排期灵活性</strong>：支持通过拖拽、连线等操作快速调整计划，大幅降低重排排期的成本。</li><li><strong>强化过程确定性</strong>：实时审计实际流转速率与排期模型的差异，实现风险的主动预警与修正 。</li><li><strong>沉淀组织标准化路径</strong>：将验证有效的编排模式固化为卡片模板，实现项目经验的快速复用。</li></ol><h2><strong>应用场景</strong></h2><ul><li><strong>敏捷迭代管理</strong>：将产品愿景拆解为 Sprint 任务卡片，驱动研发交付流高效流转。</li><li><strong>复杂项目规划</strong>：在启动阶段梳理各模块间的依赖链路，利用卡片编排规避交付冲突 。</li><li><strong>资源负载均衡</strong>：通过可视化看板监控各环节卡片堆积情况，实现动态的人力资源调度。</li><li><strong>跨团队协同</strong>：通过共享的计划卡片池，对齐跨职能部门的协作节奏与产出标准 。</li></ul><h2>---</h2><p><strong>5款值得尝试的卡片式计划编排工具</strong></p><h3><strong>1. 板栗看板</strong></h3><p>直观的任务流转与多层级穿透</p><ul><li><strong>特点</strong>：支持任务卡片的无限层级嵌套，通过看板视图展示计划的深度编排逻辑。</li><li><strong>优势</strong>：看板视图极度直观，支持卡片逻辑连线，适合追求过程透明的敏捷团队。</li><li><strong>适合团队</strong>：需要快速响应并对计划进行纵向穿透的小型和中型研发团队 。</li></ul><h3><strong>2. ClickUp</strong></h3><p>全功能任务编排与数据看板平台</p><ul><li><strong>特点</strong>：提供强大的“目标”模块，支持将微观卡片进度自动聚合为宏观指标。</li><li><strong>优势</strong>：支持极高维度的自定义，能根据卡片元数据生成复杂的排期审计报告。</li><li><strong>适合团队</strong>：需要对大规模计划进行参数化管理和深度数据分析的团队 。</li></ul><h3><strong>3. Trello</strong></h3><p>简单轻量的卡片流转工具</p><ul><li><strong>特点</strong>：强调“清单化”的计划编排，支持丰富的卡片封面与标签分类 。</li><li><strong>优势</strong>：操作极简，学习曲线极低，适合快速搭建基础的交付工作流 。</li><li><strong>适合团队</strong>：注重任务分类和灵活调整、倾向于视觉驱动型协作的团队 。</li></ul><h3><strong>4. Jira Software</strong></h3><p>工业级标准与自动化编排引擎</p><ul><li><strong>特点</strong>：拥有严密的权限与流程控制逻辑，支持复杂的卡片依赖与版本排期。</li><li><strong>优势</strong>：可与代码仓库深度集成，实现从“计划编排”到“自动执行”的闭环审计。</li><li><strong>适合团队</strong>：追求高度标准化执行、有严格合规与闭环审计需求的大型组织。</li></ul><h3><strong>5. Monday.com</strong></h3><p>高度自由的卡片式协同看板</p><ul><li><strong>特点</strong>：支持看板与时间轴、工作负荷视图的实时联动，动态展示卡片状态。</li><li><strong>优势</strong>：视觉色彩丰富，支持强大的自动化集成，能显著提升团队编排兴趣。</li><li><strong>适合团队</strong>：强调团队协同氛围、需要灵活配置复杂编排场景的项目组。</li></ul><h2>---</h2><p><strong>如何选择合适的卡片式计划编排工具？</strong></p><h3><strong>1. 按团队规模选择</strong></h3><ul><li><strong>小型团队（1-10人）</strong>：推荐 <strong>板栗看板</strong>、Trello 等工具，侧重于快速启动与核心任务的直观流转。</li><li><strong>中型团队（10-50人）</strong>：适合使用 <strong>Monday.com</strong>、ClickUp，支持更复杂的多维对齐与资源核算 。</li><li><strong>大型团队（50+人）</strong>：建议选择 <strong>Jira</strong> 或 <strong>ClickUp</strong>，这些工具提供强大的层级管理与权限隔离功能。</li></ul><h3><strong>2. 按计划复杂度选择</strong></h3><ul><li><strong>线性任务</strong>（如内容生产、日常运营）：选择 <strong>板栗看板</strong>、Trello 等简洁易用的视图工具 。</li><li><strong>交叉任务</strong>（如软件研发、系统重构）：推荐 <strong>Jira</strong>、<strong>板栗看板</strong>等支持深度连线与递归逻辑核算的专业平台。</li></ul><h2>---</h2><p><strong>提升计划编排效率的小建议</strong></p><ol><li><strong>坚持卡片原子化</strong>：确保每张卡片描述的是最小可执行单元，避免职责模糊。</li><li><strong>设置基准流转速率</strong>：定期审计实际完成时长，为后续计划编排提供真实的数据支撑。</li><li><strong>建立风险预警连线</strong>：为关键路径上的卡片设置依赖预警，确保下游环节能提前预知变动 。</li><li><strong>定期进行计划“减脂”</strong>：及时清理、归档过时卡片，保持编排体系的干练与精准执行力。</li></ol><h2>---</h2><p><strong>总结</strong></p><p>卡片式计划编排工具是管理组织执行复杂性的关键手段。通过 板栗看板、ClickUp、Jira 等工具，团队能够将宏观的战略意图精准解构为微观的原子卡片，实现“计划-执行-状态”的实时对齐。</p><p>精准的编排，是高效交付的基石。</p>]]></description></item><item>    <title><![CDATA[剑指offer-67、剪绳⼦ SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047548771</link>    <guid>https://segmentfault.com/a/1190000047548771</guid>    <pubDate>2026-01-22 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>题目描述</h2><p>给你⼀根⻓度为n 的绳⼦，请把绳⼦剪成整数⻓的m 段（ m 、n 都是整数， n&gt;1 并 且m&gt;1 ， m&lt;=n ），每段绳⼦的⻓度记为k[1],...,k[m]。请问k[1]x...xk[m] 可能的最⼤乘积是多少？例如，当绳⼦的⻓度是8 时，我们把它剪成⻓度分别为2 、3 、3 的三段，此时得到的最⼤乘积是18`。</p><p>输⼊描述:输⼊⼀个数n，意义⻅题⾯。（2 &lt;= n &lt;= 60）</p><p>返回值描述:输出答案。</p><p>示例1<br/>输⼊：8<br/>返回值：18</p><h2>思路及解答</h2><h3>备忘录</h3><p>本题的解答思路就是每个⻓度的绳⼦，要么最⻓的情况是不剪开（⻓度是本身），要么⻓度是剪开两段的乘积。因此每个⻓度 length 都需要遍历两个相加之后等于 length 的乘积，取最⼤值。</p><p>初始化值⻓度为 1 的值为 1 ，从⻓度为 2 开始，每⼀种⻓度都需要遍历两个⼦⻓度的乘积。</p><p>显然，为了避免多次重复计算，可以写个备忘录</p><pre><code class="java">public class Solution {
    public int cutRope(int target) {
        if (target &lt;= 1) {
            return target;
        }
        int[] nums = new int[target + 1];
        nums[1] = 1;
        nums[0] = 1;
        for (int i = 2; i &lt;= target; i++) {
            int max = i;
            for(int j=0;j&lt;=i/2;j++){
                int temp = nums[j] * nums[i-j];
                if(temp &gt; max){
                    max = temp;
                }
            }
            nums[i]=max;
        }
        return nums[target];
    }
}</code></pre><h3>动态规划</h3><p>⽤动态规划的思维来做，假设绳⼦⻓度为 n 的 最⼤的⻓度为 f(n) ，那你说 f(n) 怎么计算得来呢？</p><ol><li>f(n) 可能是 n(不切分)</li><li>也可能是 f(n-1) 和 f(1) 的乘积</li><li>也可能是 f(n-2) 和 f(2) 的乘积</li><li>......</li></ol><p>那么也就是想要求 f( n ) 我们必须先把 f(n-1) ， f(n-2) ...之类的前⾯的值先求出来， f(1)=1 这是初始化值。</p><pre><code class="java">public class Solution {
    public int cutRope(int target) {
        int[] dp = new int[target + 1];
        dp[1] = 1;
        for (int i = 2; i &lt;= target; i++) {
            for (int j = 1; j &lt; i; j++) {
                dp[i] = Math.max(dp[i], (Math.max(j, dp[j])) * (Math.max(i - j, dp[i - j])));
            }
        }
        return dp[target];
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n²)，外层循环n-3次，内层循环i/2次</li><li><strong>空间复杂度</strong>：O(n)，需要dp数组存储中间结果</li></ul><h3>贪心算法（最优解）</h3><p>基于数学推导的贪心策略，优先剪出长度为3的段。当n≥5时，优先剪出长度为3的段；剩余4时剪成2×2</p><p><strong>为什么选择3？</strong></p><ol><li><strong>数学证明</strong>：当n ≥ 5时，3(n-3) ≥ 2(n-2) &gt; n</li><li><strong>接近自然底数e</strong>：最优分段长度应接近e ≈ 2.718，3是最接近的整数</li><li><strong>4的特殊处理</strong>：2×2 &gt; 3×1，所以剩余4时剪成2×2而不是3×1</li></ol><pre><code class="java">public class Solution {
    public int cutRope(int n) {
        // 特殊情况处理
        if (n &lt;= 3) return n - 1;
        
        // 计算可以剪出多少段长度为3的绳子
        int countOf3 = n / 3;
        
        // 处理剩余部分：当剩余长度为1时，调整策略
        if (n - countOf3 * 3 == 1) {
            countOf3--; // 减少一段3，与剩余的1组成4
        }
        
        // 计算剩余部分能剪出多少段长度为2的绳子
        int countOf2 = (n - countOf3 * 3) / 2;
        
        // 计算结果：3的countOf3次方 × 2的countOf2次方
        return (int)(Math.pow(3, countOf3)) * (int)(Math.pow(2, countOf2));
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(1)，只有常数次操作</li><li><strong>空间复杂度</strong>：O(1)，只使用固定变量</li></ul><h3>数学公式法（理论最优）</h3><p>根据n除以3的余数直接套用公式</p><pre><code class="java">public class Solution {
    public int cutRope(int n) {
        if (n &lt;= 3) return n - 1;
        
        int countOf3 = n / 3;
        int remainder = n % 3;
        
        // 根据余数直接返回结果
        if (remainder == 0) {
            return (int) Math.pow(3, countOf3);
        } else if (remainder == 1) {
            return (int) Math.pow(3, countOf3 - 1) * 4;
        } else { // remainder == 2
            return (int) Math.pow(3, countOf3) * 2;
        }
    }
}</code></pre><ul><li>时间复杂度：O(1)</li><li>空间复杂度：O(1)</li></ul>]]></description></item><item>    <title><![CDATA[鸿蒙 RTL 适配踩坑记录：为什么你的布局在阿拉伯语下一定会翻车 前端视界 ]]></title>    <link>https://segmentfault.com/a/1190000047557193</link>    <guid>https://segmentfault.com/a/1190000047557193</guid>    <pubDate>2026-01-22 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557195" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>摘要</h2><p>随着鸿蒙应用逐步走向国际化，应用不再只面对中文和英文用户。<br/>在 <strong>中东、北非</strong> 等地区，<strong>阿拉伯语、希伯来语</strong> 这类 <strong>从右到左（RTL）语言</strong> 是主流，如果应用在这些语言环境下：</p><ul><li>布局顺序是反的</li><li>返回按钮方向不对</li><li>文字对齐看着很别扭</li></ul><p>那基本可以直接劝退用户。</p><p>好消息是：<br/><strong>鸿蒙系统对 RTL 是原生支持的，而且大部分情况下是“自动完成”的。</strong><br/>坏消息是：<br/><strong>一旦你写了不该写的代码，系统也救不了你。</strong></p><p>这篇文章就从<strong>真实开发角度</strong>，聊清楚鸿蒙里 RTL 适配到底该怎么做、哪些地方最容易踩坑，以及在真实页面里该怎么写。</p><h2>引言</h2><p>在早期做 Android / Web 国际化时，RTL 基本属于“高级需求”，很多项目甚至直接忽略。<br/>但在鸿蒙生态里，<strong>国际化是默认要考虑的事情</strong>，尤其是：</p><ul><li>智能设备出海</li><li>海外 ROM</li><li>多语言系统级应用</li></ul><p>在这些场景下，RTL 不再是“锦上添花”，而是<strong>基础能力</strong>。</p><p>鸿蒙的设计理念其实很明确：</p><blockquote>系统帮你做方向适配，你只要别把方向写死。</blockquote><p>问题就在于：<br/>很多开发者在不知不觉中，把方向写死了。</p><h2>鸿蒙对 RTL 的整体支持机制</h2><h3>系统层是自动感知的</h3><p>当系统语言切换为 RTL 语言时，鸿蒙会自动做这些事情：</p><ul><li>整体布局方向切换为 RTL</li><li>文本阅读方向切换</li><li><code>Row / Flex</code> 子组件顺序镜像</li><li>列表、导航组件交互方向变化</li></ul><p>前提只有一个：<br/><strong>你的代码要写得“语义化”。</strong></p><h2>布局方向适配的核心原则</h2><h3>永远不要写死 left / right</h3><p>这是 RTL 适配里<strong>最常见、也是最致命的问题</strong>。</p><h4>错误示例（真实项目里经常看到）</h4><pre><code class="ts">Text('返回')
  .margin({ left: 16 })</code></pre><p>这段代码在中文、英文环境下完全正常，<br/>但在 RTL 环境下：</p><ul><li>系统已经整体翻转</li><li>你又强行加了 left</li><li>结果就是布局看起来“很怪”</li></ul><h4>正确示例（推荐写法）</h4><pre><code class="ts">Text('返回')
  .margin({ start: 16 })</code></pre><p>这里的 <code>start</code> 是一个<strong>语义方向</strong>：</p><ul><li>LTR 语言下等价于 left</li><li>RTL 语言下等价于 right</li></ul><p>你不用管语言，系统会帮你算。</p><h3>Demo：基础 RTL 自适应 Row</h3><p>下面是一个<strong>可以直接运行的 Demo</strong>，你只需要切换系统语言就能看到效果。</p><pre><code class="ts">@Entry
@Component
struct RtlBaseDemo {
  build() {
    Row() {
      Image($r('app.media.arrow'))
        .width(24)
        .height(24)

      Text('返回')
        .margin({ start: 8 })
    }
    .padding({ start: 16, end: 16 })
  }
}</code></pre><p>这个 Demo 的特点：</p><ul><li>没有写 left / right</li><li>没有强制方向</li><li>图标和文字顺序会自动镜像</li></ul><p>在阿拉伯语系统下，你会发现：</p><ul><li>箭头跑到了右侧</li><li>文本在左</li><li>间距依然正确</li></ul><h2>文本方向与对齐的正确方式</h2><h3>文本不要写 Left / Right 对齐</h3><p>很多人习惯性这样写：</p><pre><code class="ts">Text('مرحبا')
  .textAlign(TextAlign.Left)</code></pre><p>问题是：<br/><strong>Left 在 RTL 里并不是“阅读起点”。</strong></p><p>正确的写法是：</p><pre><code class="ts">Text('مرحبا')
  .textAlign(TextAlign.Start)</code></pre><p>系统会自动判断：</p><ul><li>英文 → 左对齐</li><li>阿拉伯语 → 右对齐</li></ul><h3>Demo：多语言文本展示</h3><pre><code class="ts">@Entry
@Component
struct TextAlignDemo {
  build() {
    Column() {
      Text('Hello HarmonyOS')
        .textAlign(TextAlign.Start)
        .fontSize(18)

      Text('مرحبا هارموني')
        .textAlign(TextAlign.Start)
        .fontSize(18)
    }
    .padding(16)
  }
}</code></pre><p>这个 Demo 非常适合用来<strong>自测</strong>：<br/>切换系统语言，你能直观看到对齐方向变化。</p><h2>结合真实业务场景的 RTL 适配实践</h2><h3>场景一：应用顶部导航栏</h3><p>这是 RTL 最容易翻车的地方。</p><h4>典型需求</h4><ul><li>返回按钮</li><li>页面标题</li></ul><h4>正确实现方式</h4><pre><code class="ts">@Component
struct TitleBar {
  build() {
    Row() {
      Image($r('app.media.back'))
        .width(24)
        .height(24)

      Text('设置')
        .margin({ start: 12 })
        .fontSize(20)
    }
    .padding(16)
  }
}</code></pre><p>这里的关键点：</p><ul><li>不指定 <code>FlexDirection</code></li><li>使用 <code>start</code> 间距</li><li>图标自动镜像</li></ul><p>系统语言一换，整个标题栏方向自然就对了。</p><h3>场景二：设置页列表项</h3><p>设置页通常是左右结构，比如：</p><ul><li>左边是标题</li><li>右边是开关或箭头</li></ul><h4>推荐写法</h4><pre><code class="ts">@Component
struct SettingItem {
  build() {
    Row() {
      Text('通知')
        .layoutWeight(1)

      Image($r('app.media.arrow'))
        .width(16)
    }
    .padding({ start: 16, end: 16, top: 12, bottom: 12 })
  }
}</code></pre><p>在 RTL 下：</p><ul><li>文本会靠右</li><li>箭头会跑到左侧</li><li>整体阅读顺序符合习惯</li></ul><p>你不需要为 RTL 单独写一套 UI。</p><h3>场景三：列表页面与滑动方向</h3><p>鸿蒙的 <code>List</code> 在 RTL 下：</p><ul><li>排列顺序自动调整</li><li>滑动方向符合阅读习惯</li></ul><h4>示例代码</h4><pre><code class="ts">@Entry
@Component
struct ListDemo {
  build() {
    List() {
      ForEach(['Item A', 'Item B', 'Item C'], (item: string) =&gt; {
        ListItem() {
          Text(item)
            .padding(16)
            .textAlign(TextAlign.Start)
        }
      })
    }
  }
}</code></pre><p>只要你不去强制对齐方向，列表在 RTL 下基本是“零成本适配”。</p><h2>QA：开发中常见问题</h2><h3>Q1：需要手动判断当前是不是 RTL 吗？</h3><p>一般不需要。<br/><strong>90% 的页面交给系统就够了。</strong></p><p>只有在：</p><ul><li>自定义绘制</li><li>特殊动画</li><li>非标准交互</li></ul><p>这些场景下，才需要手动处理。</p><h3>Q2：图片什么时候需要手动镜像？</h3><ul><li>返回箭头</li><li>方向性极强的图标</li></ul><p>可以使用：</p><pre><code class="ts">Image($r('app.media.arrow'))
  .mirror(true)</code></pre><p>普通装饰性图片不建议镜像。</p><h3>Q3：为什么我写了 start / end 还是不生效？</h3><p>通常是因为：</p><ul><li>强制写了 <code>FlexDirection.Row</code></li><li>写死了 <code>Alignment.Left</code></li><li>在父容器里破坏了方向规则</li></ul><p>RTL 出问题，<strong>优先回头检查是不是哪一层写死了方向</strong>。</p><h2>总结</h2><p>鸿蒙里的 RTL 适配，其实不是“多写代码”，而是“少犯错误”。</p><p>一句话经验总结：</p><ul><li>用 <code>start / end</code></li><li>用 <code>TextAlign.Start</code></li><li>不强制方向</li><li>相信系统</li></ul><p>只要遵守这几条规则，<br/><strong>绝大多数 RTL 问题都会在你“什么都没做”的情况下自动解决。</strong></p>]]></description></item><item>    <title><![CDATA[借助gh-ost，对MySQL大表进行表结构的变更 好文收藏 ]]></title>    <link>https://segmentfault.com/a/1190000047557175</link>    <guid>https://segmentfault.com/a/1190000047557175</guid>    <pubDate>2026-01-21 23:02:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>关于 gh-ost</h2><p>gh-ost 是 GitHub 开发的一个 MySQL 在线表结构变更工具(online schema migration tool)。它的全称是 "GitHub's Online Schema Translator"。</p><p>gh-ost 现在已经是大型互联网公司进行数据库运维的重要工具。</p><h3>主要作用</h3><p>gh-ost 允许在不锁表、不影响业务的情况下,对 MySQL 数据库表进行结构变更(如添加列、修改索引等)。</p><h3>核心特点</h3><ol><li><strong>无触发器设计</strong> - 不像传统工具使用触发器来同步数据,gh-ost 通过解析 binlog 来捕获变更</li><li><strong>可暂停/恢复</strong> - 可以随时暂停迁移过程,对生产环境更友好</li><li><strong>可测试</strong> - 支持在从库上测试变更,确认无误后再应用到主库</li><li><strong>动态调整</strong> - 可以实时调整迁移速度,避免影响线上服务</li></ol><h3>工作原理</h3><ol><li>创建一个与原表结构相同的"影子表"(ghost table)</li><li>在影子表上执行 DDL 变更</li><li>通过 binlog 将原表的增量数据同步到影子表</li><li>数据同步完成后,快速切换表名完成迁移</li></ol><h3>使用方法</h3><ol><li><strong>安装</strong>：<br/><code>gh-ost</code> 可以直接从最新的 <a href="https://link.segmentfault.com/?enc=5VXGnl1U8LdIBYisXgjt0w%3D%3D.o6qphFAYMZXnetdAWZxikfb6nMBHsNq%2FUGPl8eSRod%2B9kg9u3jX2jZo83q1sRX7%2F" rel="nofollow" title="发布页面" target="_blank">发布页面</a> 下载二进制文件，支持 Linux 和 macOS。</li><li><p><strong>基本命令</strong>：</p><ul><li><p><strong>测试迁移</strong>：</p><pre><code class="bash">gh-ost --test-on-replica --database=mydb --table=mytable --alter="ADD COLUMN new_col INT" --execute</code></pre></li><li><p><strong>真实迁移</strong>：</p><pre><code class="bash">gh-ost --database=mydb --table=mytable --alter="ADD COLUMN new_col INT" --execute</code></pre></li></ul></li></ol><h3>实际例子</h3><p>假设你有一个用户表需要添加新字段:</p><pre><code class="bash">gh-ost \
  --host=localhost \
  --user=root \
  --password=password \
  --database=mydb \
  --table=users \
  --alter="ADD COLUMN age INT DEFAULT 0" \
  --execute</code></pre><p><strong>场景说明</strong>:</p><ul><li>原表 <code>users</code> 有 1000 万条数据</li><li>使用传统 <code>ALTER TABLE</code> 可能需要锁表数小时</li><li>使用 gh-ost 可以在后台逐步完成变更,期间用户可以正常读写数据</li><li>最后只需要几秒钟的短暂切换时间即可完成迁移</li></ul><h3>适用场景</h3><ul><li>大表的结构变更(百万级以上数据)</li><li>需要保证高可用性的生产环境</li><li>需要精确控制数据库负载的情况</li></ul><h3>数据库支持范围</h3><p><strong>gh-ost 目前只适用于 MySQL</strong>(包括 Percona Server 和 MariaDB)。它依赖 MySQL 的 binlog 机制,因此不支持 PostgreSQL、Oracle 等其他数据库。</p><h3>常见的坑</h3><h4>1. <strong>外键约束问题</strong></h4><p>gh-ost <strong>不支持有外键的表</strong>。如果表有外键关系,迁移会失败。</p><pre><code>解决办法: 需要先删除外键,迁移完成后再重新添加</code></pre><h4>2. <strong>binlog 格式要求</strong></h4><p>必须使用 <strong>ROW 格式</strong>的 binlog,STATEMENT 或 MIXED 格式不支持。</p><pre><code class="sql">-- 检查 binlog 格式
SHOW VARIABLES LIKE 'binlog_format';

-- 如果不是 ROW,需要修改配置
SET GLOBAL binlog_format = 'ROW';</code></pre><h4>3. <strong>主键要求</strong></h4><p>表<strong>必须有主键</strong>或唯一索引,否则 gh-ost 无法正常工作。</p><h4>4. <strong>磁盘空间</strong></h4><p>会创建影子表,需要<strong>额外的磁盘空间</strong>(大约是原表的大小)。如果磁盘空间不足,迁移会失败。</p><h4>5. <strong>复制延迟</strong></h4><p>如果主从复制本身就有延迟,gh-ost 的迁移会进一步加重延迟。需要监控 <code>--max-lag-millis</code> 参数。</p><h4>6. <strong>触发器冲突</strong></h4><p>虽然 gh-ost 本身不用触发器,但如果原表上<strong>已有触发器</strong>,可能会导致数据不一致。</p><h4>7. <strong>字符集问题</strong></h4><p>影子表的字符集需要与原表一致,否则可能出现乱码或数据截断。</p><h4>8. <strong>长时间迁移中断</strong></h4><p>如果迁移过程很长(几天),期间 MySQL 重启或 binlog 被清理,会导致迁移失败需要重新开始。</p><h3>实践建议</h3><pre><code class="bash"># 先在从库测试
gh-ost \
  --host=slave-host \
  --test-on-replica \
  --migrate-on-replica \
  --database=mydb \
  --table=users \
  --alter="ADD COLUMN age INT" \
  --execute

# 设置合理的限流参数
gh-ost \
  --max-load=Threads_running=25 \
  --critical-load=Threads_running=100 \
  --chunk-size=1000 \
  --throttle-query="SELECT HOUR(NOW()) BETWEEN 2 AND 6" \
  --execute</code></pre><h3>替代方案</h3><p>如果 gh-ost 不适用,可以考虑:</p><ul><li><strong>pt-online-schema-change</strong> (Percona Toolkit)</li><li><strong>原生 Online DDL</strong> (MySQL 5.6+ 支持部分操作)</li><li>对于其他数据库,PostgreSQL 可以用 pg_repack</li></ul>]]></description></item><item>    <title><![CDATA[鸿蒙系统 IO 性能优化实战：从应用卡顿到 OTA 升级的完整解决方案 前端视界 ]]></title>    <link>https://segmentfault.com/a/1190000047557180</link>    <guid>https://segmentfault.com/a/1190000047557180</guid>    <pubDate>2026-01-21 23:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047554512" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>摘要</h2><p>在鸿蒙（HarmonyOS / OpenHarmony）应用和系统开发中，IO 操作几乎无处不在，比如文件读写、配置加载、日志输出、数据库访问以及 OTA 升级等。很多性能问题表面上看是应用卡顿、启动慢、耗电高，实际上根源都指向 IO 使用不当。本文结合当前鸿蒙系统的实际开发现状，从应用层和系统层两个角度，系统梳理 IO 性能优化的常见思路，并通过可运行的 Demo 代码，讲清楚这些优化在真实项目中该怎么落地。</p><p>文章整体偏向实战，语言尽量贴近日常开发交流，适合正在做鸿蒙应用、系统服务或设备升级相关开发的同学参考。</p><h2>引言</h2><p>随着鸿蒙生态逐渐完善，应用形态从早期的简单页面，发展到现在的多端协同、分布式能力、设备级应用，IO 压力明显变大。一方面，应用启动阶段要加载更多配置和资源；另一方面，系统服务、后台任务、设备升级都会产生大量读写操作。</p><p>在实际项目中，经常能看到下面这些情况：</p><ul><li>页面一打开就卡，结果发现主线程在读文件</li><li>日志一多，设备开始明显发热</li><li>OTA 升级时间很长，写盘阶段占了一大半</li><li>分布式数据一同步，前台体验明显下降</li></ul><p>这些问题并不是鸿蒙系统本身性能不行，而是 IO 的使用方式不够合理。下面我们就从最常见、也最容易优化的地方开始讲。</p><h2>鸿蒙 IO 性能瓶颈从哪来</h2><p>在多数项目中，IO 性能问题通常集中在下面几个点：</p><ul><li>频繁进行小文件读写</li><li>同步 IO 放在主线程执行</li><li>每次用文件都重新 open 和 close</li><li>没有任何缓存策略</li><li>用文件存 KV 数据</li><li>日志输出不受控制</li></ul><p>只要命中其中一两条，性能基本都会出问题。</p><h2>应用层 IO 优化（最常用）</h2><h3>IO 一定不要放在主线程</h3><p>这是最基础，也是最容易踩坑的一点。ArkTS 中如果直接使用同步文件接口，UI 线程就会被直接卡住。</p><h4>错误示例</h4><pre><code class="ts">import fs from '@ohos.file.fs';

let text = fs.readTextSync('/data/storage/test.txt');</code></pre><p>这种写法在数据量稍微大一点时，页面就会出现明显卡顿。</p><h4>推荐写法（异步 IO Demo）</h4><pre><code class="ts">import fs from '@ohos.file.fs';

export async function readFileAsync(path: string): Promise&lt;string&gt; {
  let file = await fs.open(path, fs.OpenMode.READ_ONLY);
  let buffer = new ArrayBuffer(4096);
  let result = '';

  let readLen = await fs.read(file.fd, buffer);
  if (readLen &gt; 0) {
    result = String.fromCharCode(...new Uint8Array(buffer, 0, readLen));
  }

  await fs.close(file);
  return result;
}</code></pre><h4>代码说明</h4><ul><li>使用 async/await，把 IO 操作放到异步任务中</li><li>读取完成后再返回结果，不阻塞 UI</li><li>真实项目中可以配合 taskpool 使用</li></ul><h3>合并小 IO，减少系统调用</h3><p>很多性能问题不是数据量大，而是 IO 次数太多。</p><h4>不推荐的写法</h4><pre><code class="ts">for (let i = 0; i &lt; list.length; i++) {
  fs.writeSync(fd, list[i]);
}</code></pre><h4>推荐写法</h4><pre><code class="ts">let content = list.join('');
fs.writeSync(fd, content);</code></pre><h4>实际效果</h4><ul><li>系统调用次数明显减少</li><li>写盘效率更高</li><li>对 Flash 存储更友好</li></ul><h3>引入内存缓存，避免重复读文件</h3><p>配置文件、初始化数据非常适合放进内存缓存。</p><pre><code class="ts">let configCache: string | null = null;

export async function getConfig(path: string): Promise&lt;string&gt; {
  if (configCache !== null) {
    return configCache;
  }
  configCache = await readFileAsync(path);
  return configCache;
}</code></pre><h4>使用场景</h4><ul><li>应用启动配置</li><li>JSON 静态数据</li><li>权限或状态信息</li></ul><h3>能用 Preferences 就别用文件</h3><p>对于少量 KV 数据，文件 IO 的性价比非常低。</p><h4>Preferences Demo</h4><pre><code class="ts">import preferences from '@ohos.data.preferences';

export async function saveUserInfo(context, userId: string) {
  let pref = await preferences.getPreferences(context, 'user_config');
  await pref.put('userId', userId);
  await pref.flush();
}</code></pre><h4>优点</h4><ul><li>内部自带缓存</li><li>自动批量落盘</li><li>使用简单，性能稳定</li></ul><h2>系统层 IO 优化（Native / 服务侧）</h2><h3>使用缓冲 IO</h3><p>在系统服务或 Native 模块中，直接写裸 IO 往往效率不高。</p><pre><code class="cpp">#include &lt;stdio.h&gt;

void writeFile(const char* path, const char* data, size_t len) {
    FILE* fp = fopen(path, "w");
    if (!fp) return;

    setvbuf(fp, nullptr, _IOFBF, 8 * 1024);
    fwrite(data, 1, len, fp);
    fclose(fp);
}</code></pre><h4>说明</h4><ul><li>设置 8KB 缓冲区</li><li>减少实际写盘次数</li><li>适合大量顺序写场景</li></ul><h3>顺序 IO 优于随机 IO</h3><pre><code class="cpp">off_t offset = 0;
pread(fd, buffer, size, offset);
offset += size;</code></pre><p>尽量避免频繁 seek 和交叉读写多个文件。</p><h3>控制日志 IO</h3><p>日志在调试阶段很有用，但在正式环境中是 IO 隐形杀手。</p><pre><code class="ts">if (__DEV__) {
  console.info('debug log');
}</code></pre><p>建议：</p><ul><li>发布版本关闭 debug 和 info</li><li>避免循环内打印日志</li><li>合并日志输出</li></ul><h2>典型应用场景分析</h2><h3>场景一：应用启动阶段加载配置</h3><h4>问题</h4><p>启动慢，页面白屏时间长。</p><h4>解决方案</h4><ul><li>异步读取配置</li><li>内存缓存</li></ul><pre><code class="ts">await getConfig('/data/storage/app_config.json');</code></pre><h3>场景二：OTA 升级文件写入</h3><h4>问题</h4><p>升级包大，写盘耗时长。</p><h4>优化思路</h4><ul><li>分块下载</li><li>分块写入</li><li>写完再统一校验</li></ul><pre><code class="ts">async function writeChunk(fd: number, data: Uint8Array) {
  await fs.write(fd, data.buffer);
}</code></pre><h3>场景三：日志过多导致设备发热</h3><h4>问题</h4><p>设备运行一段时间后发热、掉帧。</p><h4>解决方案</h4><ul><li>控制日志级别</li><li>关闭非必要日志</li></ul><h2>常见问题 QA</h2><p><strong>Q：异步 IO 一定比同步快吗？</strong><br/>A：不一定，但一定不会卡 UI。</p><p><strong>Q：缓存会不会导致数据不一致？</strong><br/>A：需要设计好更新策略，配置类数据问题不大。</p><p><strong>Q：文件和 RDB 怎么选？</strong><br/>A：结构化数据选 RDB，大文件选文件。</p><h2>总结</h2><p>IO 性能优化并不复杂，关键在于使用方式是否合理。大多数性能问题，并不是因为设备性能不足，而是 IO 用得太随意。</p><p>简单总结几句话：</p><ul><li>IO 不要放主线程</li><li>少做小 IO，多做批量 IO</li><li>能缓存就缓存</li><li>能不用文件就不用文件</li><li>日志一定要克制</li></ul><p>这些原则在应用层、系统层、OTA 场景中都是通用的。如果你正在做鸿蒙系统相关开发，把 IO 优化当成基本功，会少踩很多坑。</p>]]></description></item><item>    <title><![CDATA[鸿蒙系统中地区特定内容实现实战：从资源适配到业务控制 前端视界 ]]></title>    <link>https://segmentfault.com/a/1190000047557182</link>    <guid>https://segmentfault.com/a/1190000047557182</guid>    <pubDate>2026-01-21 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557184" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>摘要（背景与现状）</h2><p>随着鸿蒙系统在手机、平板、穿戴设备以及 IoT 场景中的逐步落地，同一套应用需要面向<strong>不同国家、不同地区、不同语言和政策环境</strong>已经成为常态。<br/>在实际项目中，我们经常会遇到这些问题：</p><ul><li>不同地区展示的文案不一样</li><li>某些功能在特定地区不能上线</li><li>活动内容、公告、支付方式存在地区差异</li></ul><p>如果地区适配逻辑处理得不好，就很容易出现<strong>代码混乱、维护成本高、后期改动困难</strong>的问题。</p><p>本文结合鸿蒙系统（HarmonyOS / OpenHarmony）的实际开发方式，从<strong>系统能力、资源机制和业务逻辑</strong>三个层面，总结一套<strong>可落地、好维护</strong>的地区特定内容实现方案。</p><h2>引言（发展情况与应用场景）</h2><p>从早期 Android / iOS 开发经验来看，地区适配往往依赖大量 <code>if-else</code> 判断，代码里到处是国家缩写，后期维护非常痛苦。<br/>鸿蒙在设计之初，就在<strong>国际化与地区适配</strong>方面做了比较完整的能力封装，比如：</p><ul><li>系统级语言和地区识别</li><li>资源文件按地区自动匹配</li><li>ArkUI 对多语言、多地区资源的天然支持</li></ul><p>在真实项目中，大多数地区定制需求并不复杂，核心思路其实只有一句话：</p><p><strong>先交给系统做资源适配，实在不行再写判断逻辑。</strong></p><p>下面我们一步一步来看具体实现方式。</p><h2>鸿蒙地区特定内容的整体实现思路</h2><p>在鸿蒙系统中，地区定制通常可以拆分为三个层次：</p><ol><li>系统层：获取当前设备的语言和地区信息</li><li>资源层：根据地区自动加载不同资源</li><li>业务层：在运行时根据地区控制功能和内容</li></ol><p>这三层并不是互斥的，而是经常组合使用。</p><h2>通过系统语言和地区识别用户环境</h2><h3>获取系统地区信息</h3><p>鸿蒙提供了 i18n 模块用于国际化相关能力，获取系统地区非常简单。</p><pre><code class="ts">import i18n from '@ohos.i18n';

const locale: string = i18n.getSystemLocale();
console.info(`当前系统地区为: ${locale}`);</code></pre><p>常见返回值包括：</p><ul><li>zh-CN：中国大陆</li><li>zh-HK：香港地区</li><li>en-US：美国</li><li>ja-JP：日本</li></ul><p>这个值通常在应用启动时获取一次即可。</p><h3>基于地区进行基础内容控制</h3><pre><code class="ts">let isChinaRegion: boolean = false;

if (locale.startsWith('zh-CN')) {
  isChinaRegion = true;
}</code></pre><p>在 ArkUI 页面中直接使用：</p><pre><code class="ts">if (isChinaRegion) {
  Text('中国地区专属内容')
    .fontSize(16)
}</code></pre><p>这种方式比较直观，适合少量差异控制，但不建议大量使用在文案层面。</p><h2>通过资源文件实现地区内容自动适配</h2><h3>资源目录结构设计</h3><p>这是鸿蒙中<strong>最推荐、维护成本最低</strong>的方式。</p><pre><code class="text">resources/
 ├─ base/
 │   └─ element/
 │       └─ string.json
 ├─ zh_CN/
 │   └─ element/
 │       └─ string.json
 ├─ en_US/
 │   └─ element/
 │       └─ string.json</code></pre><h3>不同地区资源内容示例</h3><p>base 目录作为兜底资源：</p><pre><code class="json">{
  "welcome_text": "Welcome"
}</code></pre><p>中国地区资源：</p><pre><code class="json">{
  "welcome_text": "欢迎使用（中国地区）"
}</code></pre><p>美国地区资源：</p><pre><code class="json">{
  "welcome_text": "Welcome (US Version)"
}</code></pre><h3>ArkUI 中直接使用资源</h3><pre><code class="ts">Text($r('app.string.welcome_text'))
  .fontSize(18)</code></pre><p>系统会根据当前设备地区自动匹配资源，不需要任何额外判断。</p><p>如果没有对应地区资源，就自动回退到 base。</p><h2>结合运行时逻辑实现地区功能差异</h2><p>在真实项目中，地区差异不仅体现在文案上，功能层面的限制更常见。</p><h3>地区功能开关示例</h3><pre><code class="ts">let enablePayment: boolean = true;

if (!locale.startsWith('zh-CN')) {
  enablePayment = false;
}</code></pre><p>ArkUI 中控制按钮展示：</p><pre><code class="ts">if (enablePayment) {
  Button('立即支付')
    .width(200)
}</code></pre><h3>代码逻辑说明</h3><ul><li>地区判断逻辑集中在一个地方</li><li>UI 只关心布尔状态，不直接判断地区</li><li>后期调整地区规则只改一处代码</li></ul><p>这种写法在中大型项目中特别重要。</p><h2>结合实际业务场景的应用示例</h2><h3>场景一：地区公告与活动内容展示</h3><p>不同地区活动内容变化频繁，适合服务端下发。</p><pre><code class="ts">let requestParam = {
  locale: locale
};</code></pre><p>服务器返回内容：</p><pre><code class="json">{
  "notice": "日本地区限定活动"
}</code></pre><p>客户端展示：</p><pre><code class="ts">Text(serverData.notice)</code></pre><p>这种方式运营改内容不需要重新发版。</p><h3>场景二：支付方式地区限制</h3><pre><code class="ts">function isPaymentSupported(locale: string): boolean {
  return locale.startsWith('zh-CN');
}</code></pre><pre><code class="ts">if (isPaymentSupported(locale)) {
  Button('使用本地支付')
}</code></pre><p>清晰区分业务规则和 UI。</p><h3>场景三：隐私协议与合规文案差异</h3><p>通过资源文件区分不同地区隐私条款：</p><pre><code class="ts">Text($r('app.string.privacy_policy'))</code></pre><p>不同地区加载不同内容，避免代码层面处理复杂文本。</p><h2>常见问题 QA</h2><h3>Q1：可以只用代码判断不做资源适配吗？</h3><p>可以，但不推荐。<br/>代码判断适合控制功能，不适合承载大量文案。</p><h3>Q2：地区和语言一定是一一对应的吗？</h3><p>不一定。<br/>比如香港地区可能使用中文或英文，建议优先按语言，再结合地区判断。</p><h3>Q3：地区变化时需要重启应用吗？</h3><p>一般不需要，重新加载页面即可。<br/>资源匹配通常在页面创建时生效。</p><h2>总结</h2><p>在鸿蒙系统中实现地区特定内容，其实并不复杂，关键在于<strong>合理分层</strong>：</p><ul><li>文案和静态内容优先使用资源适配</li><li>功能和业务规则使用少量逻辑判断</li><li>活动和运营内容交给服务端</li></ul><p>一句话概括就是：</p><p><strong>资源适配解决大部分问题，代码只处理真正的差异逻辑。</strong></p>]]></description></item><item>    <title><![CDATA[如何系统性打造高浏览量视频号内容 blossom ]]></title>    <link>https://segmentfault.com/a/1190000047557133</link>    <guid>https://segmentfault.com/a/1190000047557133</guid>    <pubDate>2026-01-21 22:03:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>从「抄作业」到 AI 自动生成视频的完整方法论</h2><p>很多创作者在做视频号时都会遇到同一个问题：<br/><strong>为什么看起来很努力，却始终没有稳定的高播放？</strong></p><p>原因往往不在执行力，而在起点就错了——<br/><strong>从“原创灵感”开始，而不是从“成功案例”开始。</strong></p><p>事实证明，当前阶段最容易跑通的方式不是凭空创作，而是：</p><blockquote><strong>先抄作业，再用 AI 把成功经验规模化复制。</strong></blockquote><p>下面是一套已经被反复验证、且非常适合短视频平台的完整方法。</p><hr/><h2>一、核心思路：不是抄内容，而是抄「爆款结构」</h2><p>这里的“抄作业”并不是搬运视频，而是<strong>反向工程爆款</strong>：</p><ul><li>不关心某条视频讲了什么</li><li>只关心它<strong>为什么能火</strong></li><li>把“感觉”拆成可复用的结构</li></ul><p>整个流程可以拆成四个关键词：</p><blockquote><strong>采样 → 归纳 → 再创作 → 自动生成</strong></blockquote><hr/><h2>二、为什么这个方法能跑通？</h2><h3>1️⃣ 爆款不是偶然，而是可重复的结构结果</h3><p>绝大多数高播放视频，并不是随机出现的，而是满足了以下条件：</p><ul><li>前几秒有强烈视觉或行为异常</li><li>中段存在明确冲突或失控</li><li>结尾有情绪释放或反转</li><li>风格高度统一，利于算法识别</li></ul><p>单个视频看不出规律，但<strong>同一 channel 的 Top 视频几乎一定有共性</strong>。</p><hr/><h3>2️⃣ 从 YouTube 入手，是最稳妥的起点</h3><p>YouTube 的优势在于：</p><ul><li>样本量大</li><li>数据透明</li><li>爆款生命周期长</li></ul><p>选择一个已经跑通的 YouTube channel，本质是在复用：</p><ul><li>已验证的受众偏好</li><li>已适配的平台算法</li><li>已成熟的内容节奏</li></ul><hr/><h3>3️⃣ NotebookLM 的价值：把隐性经验变成显性规则</h3><p>NotebookLM 的核心作用并不是“写文案”，而是：</p><blockquote><strong>从多个成功样本中，提炼共性模式。</strong></blockquote><p>例如：</p><ul><li>开头平均在第几秒出现刺激点</li><li>冲突是否围绕“规则 / 强迫 / 对抗”</li><li>情绪是逐步升级还是瞬间爆发</li><li>是否存在固定角色关系（支配 / 反抗）</li></ul><p>这一步完成后，爆款不再是“感觉”，而是<strong>结构模板</strong>。</p><hr/><h3>4️⃣ 文本转视频，是 AI 当前最成熟的短视频应用场景</h3><p>当前 AI 在短视频领域的优势集中在：</p><ul><li>夸张动作</li><li>强对比画面</li><li>明确情绪</li><li>简单故事线</li></ul><p>当“创意结构”已经由 NotebookLM 给出，<br/>AI 更适合承担的是<strong>从创意到画面的执行过程</strong>。</p><hr/><h2>三、完整可执行流程（SOP）</h2><h3>Step 1：查找 YouTube 火爆 Channel</h3><p>筛选标准：</p><ul><li>同一类型内容</li><li>至少 3–5 条百万播放</li><li>风格高度统一</li></ul><hr/><h3>Step 2：选取 Top 10 爆款视频</h3><p>重点关注：</p><ul><li>播放量</li><li>明显被算法推荐的迹象</li><li>评论区情绪密度</li></ul><hr/><h3>Step 3：将视频链接输入 NotebookLM 分析</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557135" alt="" title=""/></p><p>分析重点放在结构层面：</p><ul><li>前 3 秒发生了什么</li><li>冲突第一次出现的时间点</li><li>情绪如何被放大</li><li>是否存在“规则被打破”的瞬间</li></ul><p>最终得到的是一个<strong>可复用的爆款结构模型</strong>。</p><hr/><h3>Step 4：让 NotebookLM 生成“类似结构”的新创意</h3><p>在结构不变的前提下，替换：</p><ul><li>场景</li><li>道具</li><li>主题设定</li></ul><p>NotebookLM 在这一阶段输出的，是<strong>已经符合爆款结构的新视频创意</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047557136" alt="" title="" loading="lazy"/></p><hr/><h2>四、演示案例：厨房灾难——机器“闹鬼”事件</h2><p><strong>根据前述步骤，选择一个由 NotebookLM 生成的视频创意，用于展示从创意到视频生成的全过程。</strong></p><h3>创意名称</h3><p><strong>厨房灾难：机器“闹鬼”事件（The Haunted Mixer Prank）</strong></p><h3>创意概念</h3><p>在制作节日甜点的过程中，人为制造厨房设备故障，形成短暂混乱，再用反转完成喜剧闭环。</p><h3>核心情节点</h3><ul><li>设备失控</li><li>人物恐慌</li><li>荒诞解释</li><li>快速反转恢复秩序</li></ul><hr/><h2>五、让 AI 根据该创意生成文本转视频 Prompt</h2><p>在演示中，并不直接人工编写提示词，而是：</p><blockquote><strong>将该创意输入给视频生成模型或多模态 AI，要求其根据创意自动生成文本转视频 Prompt。</strong></blockquote><p>并对 AI 提出明确约束：</p><ul><li>视频总时长：20 秒</li><li>镜头数量：4 个</li><li>每个镜头 1 个核心事件</li><li>强调视觉、动作和情绪变化</li></ul><h3>🎬 AI 生成的 Text-to-Video Prompt（20 秒）</h3><pre><code>A 20-second comedic kitchen prank video.

Scene 1 (0–4s):
Bright home kitchen.
A cheerful female character is happily making holiday desserts.
She overloads a stand mixer with too many ingredients.
The mixer begins shaking violently.

Scene 2 (4–9s):
The mixer malfunctions.
Smoke rises dramatically.
Ingredients splatter everywhere.
The character panics, shouting:
“Unplug it! Unplug it now!”

Scene 3 (9–14s):
The mixer stops.
Close-up of the burnt mixer head.
She stares at it and asks nervously:
“Did I summon a ghost?”

Scene 4 (14–20s):
Comedic reversal.
She pulls out a brand-new mixer.
Smiles calmly and continues cooking as if nothing happened.
Bright, cheerful ending.

Style:
Fast-paced, exaggerated comedy.
Strong facial expressions.
Short-form video style.
No subtitles, no text overlays, no watermarks.</code></pre><p>然后选一个文本转视频的模型将提示词输入。</p><hr/><h2>六、为什么这个演示案例具有代表性？</h2><ul><li>创意来源于结构分析，而非灵感碰运气</li><li>Prompt 由 AI 基于创意自动生成</li><li>冲突、节奏、反转完整可复用</li><li>非常适合短视频平台算法偏好</li></ul><p>这说明：<br/><strong>当结构正确时，AI 的执行能力已经足够支撑内容生产。</strong></p><hr/><h2>七、结语：内容创作正在进入「工程化时代」</h2><p>当内容生产开始遵循：</p><ul><li>用数据筛选方向</li><li>用模型总结结构</li><li>用 AI 生成与执行</li><li>用批量测试验证结果</li></ul><p>创作就不再是玄学，而是一套<strong>可以被复用和放大的系统</strong>。</p><p>在这个体系中，“抄作业”不是捷径，而是<strong>最低成本、最高成功率的起点</strong>。<br/>当结构被掌握，所谓的“原创”，自然会不断出现。</p><p>本文由<a href="https://link.segmentfault.com/?enc=WP7hk88PHk5X5oUEOVKYRw%3D%3D.yBTGhtIVVrIqlacOzJwfmCFCfeXvXHqLeEi4k37Ivdc%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Andrej Karpathy：过去一年大模型的六个关键转折 卡尔AI工坊 ]]></title>    <link>https://segmentfault.com/a/1190000047557148</link>    <guid>https://segmentfault.com/a/1190000047557148</guid>    <pubDate>2026-01-21 22:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>Andrej Karpathy：过去一年大模型的六个关键转折</strong></p><p><img width="554" height="554" referrerpolicy="no-referrer" src="/img/bVdnHWN" alt="" title=""/></p><p>本文共 2836 字，阅读预计需要 4 分钟。</p><p>一边是模型光靠"多想一会儿"就能解出奥数题，另一边是刷爆排行榜的选手被用户吐槽"中看不中用"。</p><p>2025年的AI圈，弥漫着一股诡异的气息：</p><p><strong>参数规模不再是唯一的军备竞赛指标，但模型能力却在某些维度上狂飙突进。</strong></p><p>这到底发生了什么？</p><p>Andrej Karpathy——前OpenAI研究总监、曾掌舵特斯拉AI团队的技术大牛——在年终复盘中抛出了一个判断：</p><p><strong>2025年LLM的真正突破，不在于模型变大，而在于我们"驯养"它的方式、理解它的视角、以及使用它的姿势，都发生了根本性的变化。</strong></p><p>这篇文章，我会带你拆解Karpathy眼中的六个范式转变，聊聊它们对普通人意味着什么，以及有哪些坑是你现在就该绕开的。</p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWO" alt="" title="" loading="lazy"/></p><p><strong>一、RLVR：训练范式的静默换代</strong></p><p>2024年之前，大模型训练三板斧：预训练、监督微调、RLHF。但RLHF的瓶颈很明显——<strong>依赖人工标注，成本高、速度慢</strong>。</p><p>2025年，RLVR（基于可验证奖励的强化学习）开始上位。核心逻辑很简单：<strong>用有标准答案的任务来训练</strong>。数学题对不对？代码能不能跑？机器自己就能验证。</p><p>打个比方：RLHF像请老师批改作文，标准不一；RLVR像做数学卷子，对就是对、错就是错。</p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWP" alt="" title="" loading="lazy"/></p><p>RLVR还解锁了一个调节旋钮：<strong>让模型"多想一会儿"</strong></p><p>生成更长的推理链，就能换来更强能力。OpenAI的o1到o3，DeepSeek的R1，都是这条路线的产物。</p><p>以前比谁模型参数多，现在比谁的强化学习跑得久。</p><p><strong>二、召唤幽灵，而非驯养动物</strong></p><p>Karpathy用了一个隐喻：<strong>我们不是在"培育动物"，而是在"召唤幽灵"</strong>。</p><p>动物智能是进化塑造的，能力配合天衣无缝。</p><p>但LLM的"大脑"是为了预测下一个词、在数学题里拿分——<strong>这些目标和生存没关系</strong>。</p><p>结果就是"锯齿状智能"：<strong>某些任务碾压专家，另一些任务犯低级错误。</strong></p><p>它能写出逻辑严密的报告，但是转头就被越狱提示词骗了。</p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWQ" alt="" title="" loading="lazy"/></p><p>实际后果是：<strong>别迷信基准测试。</strong> LLM团队为了刷榜，围绕测试题大量生成训练数据，榜单漂亮，实际用起来翻车。</p><p>幽灵的能力是尖刺的、不可预测的。用的时候，得时刻警惕。</p><p><strong>三、Cursor与新应用层：上下文工程的价值爆发</strong></p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWR" alt="" title="" loading="lazy"/></p><p>2025年，Cursor没有自己训练模型，但估值从4亿飙到99亿美元。它做对了什么？</p><p>答案是<strong>上下文工程</strong>——在调用大模型时，精心设计给它的信息环境：提示词怎么写、代码库怎么索引、多次调用怎么编排。</p><p>Karpathy的观点是：<strong>LLM实验室培养"通才大学生"，应用层把他们培养成"垂直专家"</strong>。桥梁就是上下文工程。</p><p>直接问ChatGPT和用Cursor写代码，体验天差地别。Cursor自动索引代码仓库，理解文件依赖，提问时自动塞入相关上下文。这不是模型能力差距，是<strong>信息组织方式的差距</strong>。</p><p>启示很清晰：<strong>模型会迭代，但上下文工程能力可以沉淀，能无缝迁移到下一代模型。</strong></p><p>这也是我一直以来坚持上下文工程优先的原因。</p><p><strong>四、Claude Code：AI从"网站"变成"室友"</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnHWS" alt="" title="" loading="lazy"/></p><p>Claude Code是Anthropic推出的命令行工具，特别之处在于：<strong>直接运行在本地电脑上</strong>，访问你的文件、配置、密钥。后续Copilot等工具也相继推出了这样的开发模式。</p><p>Karpathy说：<strong>它不再是需要打开浏览器的网站，而是"寄居"在电脑里的小精灵</strong>。</p><p>本地运行的好处：AI直接读取电脑上的上下文——装了哪些软件、项目代码长什么样，不需要手动复制粘贴。</p><p>更重要的是<strong>延迟和隐私</strong>——云端来回几百毫秒，敏感数据发到第三方合规部门不同意。</p><p>当然也有隐患：一个能操作本地文件的AI，权限边界怎么划定？</p><p><strong>五、Vibe Coding：代码正在变得廉价</strong></p><p>Karpathy造了个词叫"Vibe Coding"——氛围编程。</p><p><strong>用自然语言描述需求，AI帮你写代码，你甚至不需要"懂"代码</strong>。</p><p>2025年这事跨过了临界点。之前AI写代码问题多，需要人debug。现在很多简单项目，从想法到可运行程序，一气呵成。</p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWT" alt="" title="" loading="lazy"/></p><p>Karpathy自己用它写了Rust版tokenizer（不需要学Rust）、做了好几个小应用原型、甚至写过临时应用定位bug——用完就扔。</p><p>他的原话是：<strong>代码变得廉价、短暂、可塑、用完即弃。</strong></p><p>对普通人意味着什么？"我有想法但不会代码"这个门槛，正在消失。</p><p><strong>六、Nano Banana：LLM的GUI时代前奏</strong></p><p>Google的Gemini Nano Banana让Karpathy特别兴奋。</p><p>核心不是图像生成能力，而是<strong>文本、图像与世界知识在模型权重中的深度融合</strong>。</p><p>现在"跟LLM对话"像1980年代敲命令。文本是机器原生语言，但人更喜欢视觉化呈现——这正是GUI被发明的原因。</p><p><strong>LLM也需要自己的GUI</strong>——用图片、信息图、动画跟我们沟通。Nano Banana就是这个方向的早期预演。</p><p><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnHWU" alt="" title="" loading="lazy"/></p><p><strong>写在最后：可立即落地的三个建议</strong></p><p>拉回来说说，这六个范式转变对你意味着什么。</p><p><strong>如果你是创业者</strong>，最重要的启示是：模型能力会继续涨，但涨的方式变了。与其追着模型跑，不如在上下文工程上建立壁垒。Cursor的成功已经证明了这条路。</p><p><strong>如果你是开发者</strong>，Vibe Coding值得你认真对待。不是说它会取代你，而是说它能让你的生产力翻倍。把重复性的代码工作交给AI，把精力放在架构设计和业务逻辑上。</p><p><strong>如果你是普通用户</strong>，最重要的是调整预期。AI既不是全能的神，也不是彻底的废物——它是一个能力极度不均匀的"幽灵"。用好它的尖刺能力，同时对它的盲区保持警惕。</p><p>三个行动建议，作为结束：</p><p><strong>投资上下文工程能力</strong>。学会设计提示词、组织RAG检索、编排多步调用，这是当下性价比最高的AI技能。</p><p><strong>用Vibe Coding降低创意落地门槛</strong>。你脑子里的想法，别再等"等我学会编程再说"，现在就可以试着让AI帮你实现。</p><p><strong>理解锯齿状智能，设置人工校验</strong>。在享受AI效率提升的同时，别忘了在关键环节保留人工把关。</p><p>2025年是LLM的分水岭。规则变了，玩法也得跟着变。</p><p>2026年，又会有什么新的成果出现呢？评论区聊聊你的看法</p><p>既然看到这了，如果觉得不错，随手点个赞、收藏、转发三连吧～</p><p>我是Carl，大厂研发裸辞的AI创业者，只讲能落地的AI干货。</p><p>关注我，更多AI趋势与实战，我们下期再见！</p><p><img width="723" height="311" referrerpolicy="no-referrer" src="/img/bVdnuIt" alt="" title="" loading="lazy"/></p><p><strong>数据来源</strong></p><p>Karpathy 2025年终复盘原文 [数据|2025|<a href="https://link.segmentfault.com/?enc=c2aH28NQkYu%2BcDvNPeoQFQ%3D%3D.cBJT5Qlno%2FVvwqRdXEIhQOID4q08ShHOgztPNALCPoHkrn8nswrZ9dlnEAn1LWa7jZoWT%2BCdKTbCgua%2F0FSYNQ%3D%3D" rel="nofollow" target="_blank">https://karpathy.bearblog.dev/year-in-review-2025/</a>]</p><p>RLVR训练范式说明：基于可验证奖励的强化学习 [数据|2025|Karpathy原文]</p><p>DeepSeek R1推理能力展示 [数据|2025|DeepSeek R1论文]</p><p>Cursor估值变化：$400M(2024.8) → $9.9B(2025.6) [数据|2024-2025|<a href="https://link.segmentfault.com/?enc=j61Od%2BzxlkO%2F7NE3BKUYrQ%3D%3D.jfmxt5aRCtGTrr0gF3ZNSyU3xfgOr24PaB5LdjllOEep3wZVI8sEZ1h43MCOMsd0" rel="nofollow" target="_blank">https://techcrunch.com/tag/cursor/</a>]</p><p>OpenAI o1/o3推理模型发布 [数据|2024-2025|OpenAI官方]</p><p>Claude Code产品发布与功能说明 [数据|2025|Anthropic官方]</p><p>Vibe Coding概念由Karpathy在Twitter提出 [数据|2025|<a href="https://link.segmentfault.com/?enc=OaT%2BThX4ur9CJctXs%2BDpxQ%3D%3D.g3XlWXPEXvZHZfsSzYL%2BHq5CY2IlD2a6bD%2FDUhJOjYnvPTqjpfncWqaryT0NS1eyQDjRLEE7gh3fRnv51QK63g%3D%3D" rel="nofollow" target="_blank">https://x.com/karpathy/status/1886192184808149383</a>]</p><p>Google Gemini Nano Banana多模态融合能力 [数据|2025|Google官方]</p>]]></description></item><item>    <title><![CDATA[对抗样本：20行Python代码让95%准确率的图像分类器彻底失效 本文系转载，阅读原文
https]]></title>    <link>https://segmentfault.com/a/1190000047557152</link>    <guid>https://segmentfault.com/a/1190000047557152</guid>    <pubDate>2026-01-21 22:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>下图展示了一个有趣的现象：在法国斗牛犬的图像上添加一小块对抗性补丁后，VGG分类器竟然以极高的置信度将其判定为足球。Grad-CAM可视化清楚地显示，模型的注意力完全从狗身上转移到了那块补丁——一个精心构造的小扰动就足以劫持整个决策过程。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557154" alt="" title=""/></p><h2>95%准确率的模型可能不堪一击</h2><p>ResNet、VGG、EfficientNet这些主流架构在ImageNet上动辄90%以上的准确率，看起来已经相当可靠。但这些模型隐藏着一个被多数工程师忽视的致命缺陷：它们极易被对抗样本愚弄。</p><p>改变一个像素，可能肉眼完全看不出区别，但分类器会彻底崩溃。本文会用FGSM（快速梯度符号法）演示如何制作对抗样本，并解释神经网络为何如此脆弱。</p><h2>对抗样本到底是什么</h2><p>简单说，对抗样本就是专门设计来欺骗模型的输入。和随机噪声不同，这种扰动是经过精确计算的——目标是在人眼察觉不到的前提下，最大化模型的预测误差。</p><p>这里存在一个悖论：模型可以正确识别成千上万张图片，但只要加上一点经过数学优化的噪声（像素值变化不到1%），它就会完全判断失误。</p><p>对抗攻击绝非学术界的自娱自乐。自动驾驶汽车可能把停车标志识别成限速标志；人脸识别系统可能被绕过；放射科AI可能给出错误诊断；有害内容可能躲过审核系统的检测。</p><p>问题的根源在于：分类器学到的是统计层面的捷径，而非真正的语义理解。高准确率和高安全性是两回事。</p><h2>FGSM：简单却致命的攻击方法</h2><p>Ian Goodfellow等人在2015年提出的FGSM至今仍是最经典的对抗攻击之一。它的原理出奇地简单，但恰恰暴露了深度神经网络的根本弱点。</p><h3>数学原理</h3><p>给定分类器和输入图像，FGSM计算一个扰动把图像推向错误分类的方向。具体做法是沿着损失函数梯度的方向移动每个像素，用epsilon参数控制扰动幅度，确保改动在视觉上不可察觉。</p><h3>FGSM为何有效</h3><p>深度网络虽然有非线性激活函数但在局部表现出近似线性的特性。每个像素上的微小变化会在高维空间中累积，最终在输出空间产生巨大偏移。梯度恰好指明了这个最有效的攻击方向——随机噪声做不到的事情，梯度对齐的噪声可以轻松做到。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047557155" alt="" title="" loading="lazy"/><br/>上图就是是Goodfellow等人最初展示的结果：在熊猫图像上叠加梯度符号计算得到的微小扰动，模型就会以极高置信度将其误判为长臂猿。两张图片在人眼看来毫无差别，但神经网络的判断却天差地别。</p><h2>Python实战：构建你的第一个对抗样本</h2><p>下面用PyTorch和预训练的ResNet-50从零实现一个对抗样本。</p><p>先安装依赖：</p><pre><code> pip install torch torchvision matplotlib numpy pillow</code></pre><p>导入必要的库：</p><pre><code> import torch  
 import torch.nn.functional as F  
 import torchvision.models as models  
 import torchvision.transforms as transforms  
 import matplotlib.pyplot as plt  
 import numpy as np  
 from PIL import Image</code></pre><p>第一步：加载分类器</p><p>用ResNet-50作为目标模型。这个架构在生产环境中很常见，而且支持梯度计算：</p><pre><code> model=models.resnet50(pretrained=True)  
 model.eval()</code></pre><p>第二步：准备图像</p><p>按ImageNet标准预处理输入图像：</p><pre><code> transform=transforms.Compose([  
    transforms.Resize((224, 224)),  
    transforms.ToTensor(),  
])

img=Image.open("your_image.jpg").convert("RGB")  
x=transform(img).unsqueeze(0)  
 x.requires_grad=True</code></pre><p>注意</p><pre><code>requires_grad=True</code></pre><p>这行。没有它就无法计算梯度，对抗攻击也就无从谈起。</p><p>第三步：获取原始预测</p><p>跑一次前向传播，看看模型本来会给出什么分类：</p><pre><code> logits=model(x)  
 pred=logits.argmax(dim=1)  
 print(f"Original prediction: {pred.item()}")</code></pre><p>正常情况下模型应该能正确分类。</p><p>第四步：FGSM攻击</p><p>核心代码如下：</p><pre><code> label = pred  
loss = F.cross_entropy(logits, label)  
loss.backward()

epsilon = 0.01  # perturbation budget
perturbation = epsilon * x.grad.sign()  
x_adv = x + perturbation  
 x_adv = torch.clamp(x_adv, 0, 1)</code></pre><p>这段代码做了什么？计算损失对输入像素的梯度，取符号得到方向，乘以epsilon控制幅度，加到原图上就得到对抗样本。最后用clamp保证像素值在合法范围内。</p><p>第五步：检验效果</p><p>用同一个模型测试对抗图像：</p><pre><code> logits_adv=model(x_adv)  
 pred_adv=logits_adv.argmax(dim=1)  
 print(f"Adversarial prediction: {pred_adv.item()}")</code></pre><p>大多数情况下预测结果会完全不同。图像看起来一样，分类却天壤之别。</p><p>第六步：可视化</p><p>把原图、对抗图、噪声模式放在一起对比：</p><pre><code> def show_adversarial_attack(original, adversarial, perturbation):  
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))  
      
    axes[0].imshow(original)  
    axes[0].set_title("Original Image")  
    axes[0].axis("off")  
      
    axes[1].imshow(adversarial)  
    axes[1].set_title("Adversarial Image")  
    axes[1].axis("off")  
      
    axes[2].imshow(perturbation, cmap="gray")  
    axes[2].set_title("Noise Pattern (10x Amplified)")  
    axes[2].axis("off")  
      
    plt.tight_layout()  
    plt.show()

orig_np = x.detach().squeeze().permute(1, 2, 0).numpy()  
adv_np = x_adv.detach().squeeze().permute(1, 2, 0).numpy()  
noise_np = (adv_np - orig_np) * 10
 show_adversarial_attack(orig_np, adv_np, noise_np)</code></pre><p>噪声模式放大10倍后看起来像电视雪花。人眼根本分辨不出两张图的区别，但神经网络却认为它们是完全不同的物体。</p><h2>神经网络为何如此脆弱</h2><p>理解这个问题需要从三个角度切入。</p><p>高维几何：一张224×224的RGB图像有150,528个维度。在这么高的维度里每个维度上的微小扰动累加起来就是巨大的距离。</p><p>局部线性：尽管激活函数是非线性的，深度网络在数据点附近的小邻域内表现得非常线性，这让基于梯度的攻击特别有效。</p><p>非泛化特征：研究发现模型大量依赖那些与标签相关、但与人类感知无关的统计模式。对抗样本正是在利用这些"捷径特征"。</p><p>一个令人不安的事实：深度学习模型优化的目标是训练集上的准确率，而不是对扰动的泛化性。</p><h2>一些限制需要说明</h2><p>FGSM只是单步攻击算比较弱的。迭代方法如PGD和Carlini-Wagner攻击力更强也更难防御。</p><p>本文的演示假设攻击者能拿到模型权重和梯度，属于白盒场景。现实中攻击者可能只能观察模型输出，需要用黑盒攻击技术或者利用对抗样本的迁移性。</p><p>数字扰动只是一种形式。物理世界的对抗样本——比如贴在物体上的特制贴纸——可以在不同光照和角度下持续欺骗视觉系统。</p><p>防御手段确实存在：对抗训练、输入预处理、集成方法、认证防御等等。但这些方法往往要牺牲准确率，而且没有哪个能提供完全的保护。</p><h2>防御策略</h2><p>几种主流防御思路：</p><p>对抗训练把对抗样本混入训练数据，让模型学会应对扰动。输入变换用JPEG压缩、随机缩放、降低位深等预处理来破坏对抗扰动。集成防御结合多个模型的预测或引入随机性来增加攻击难度。认证防御用随机平滑等技术在一定范围内提供数学上的泛化性保证。检测方法则训练专门的模型来识别对抗样本。</p><p>每种方法都有代价，在泛化性、准确率、计算开销之间做权衡。</p><h2>总结</h2><p>对抗样本揭示的是统计优化和人类感知之间的根本鸿沟。深度学习擅长模式匹配，但它并不理解图像的语义。</p><p>对抗样本不会消失。这不是可以修复的bug而是当前深度学习架构的内在属性。随着AI在关键基础设施中的应用越来越广，理解和缓解对抗脆弱性变得愈发重要。</p><p>泛化性应该和准确率、公平性、效率一样，成为一等公民级别的工程需求。否则，高准确率带来的只是虚假的安全感。</p><p><a href="https://link.segmentfault.com/?enc=iPNKIfJj2O4Wurp3m79zlg%3D%3D.L2R3eQYgd5RQfJakjiGiPcXB8tvZAu0IY81UK0CScl0qgQ5Wvk8mx1AFeEw3evwVPOLMbezTM1B%2FqmCLI1noJQ%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/935d5167003748db859452026a44b056</a></p><p>作者: Sarthakvyadav</p>]]></description></item>  </channel></rss>