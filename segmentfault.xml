<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[【URP】Unity[RendererF]]></title>    <link>https://segmentfault.com/a/1190000047413420</link>    <guid>https://segmentfault.com/a/1190000047413420</guid>    <pubDate>2025-11-20 11:16:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=ZHoNDFeO0qbaQg1fxaJLnQ%3D%3D.btUYfWZoRkLH4gAfDetMYDUZQoVHYogp%2Fu8wv5vUVVlrx%2FEwEiEU9EI9mDGo2zBDW7n20O8nXVErp%2F8fceOwO1Vs3rryfrgF2X0kIjbxRHEfZ6RQcIuvsNHoHgQI50X6iYcuKJS1e2hOteiXa8cty3vIy8VRdjV%2BnGoq1HrdgFvdAoXHdURJgbmd0wOWhJNQFEhfxgDRyLjip7XZkx196bAw9Y855m0pDjiDegM%2FkW8%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong></blockquote><h2><strong>RenderObjects的定义与作用</strong></h2><p>RenderObjects是URP提供的RendererFeature之一，允许开发者在不编写代码的情况下对渲染管线进行定制。它通过配置参数实现选择性渲染特定层级的物体、控制渲染顺序、重载材质或渲染状态等功能57。其核心用途包括：</p><ul><li>‌<strong>层级过滤</strong>‌：仅渲染指定LayerMask的物体</li><li>‌<strong>渲染时机控制</strong>‌：通过Event参数插入到渲染管线的不同阶段（如AfterRenderingOpaques）</li><li>‌<strong>材质替换</strong>‌：使用Override Material覆盖原有材质</li><li>‌<strong>多Pass渲染</strong>‌：配合Shader的LightMode标签实现描边等效果</li></ul><h2><strong>发展历史</strong></h2><ul><li>初始版本（2020年前）作为LWRP实验性功能引入</li><li>2020年URP 7.x版本正式集成，提供基础层过滤和材质替换</li><li>2021年后增强深度/模板控制，支持透明物体处理</li><li>2022年优化API结构，明确ScriptableRendererFeature与RenderPass的分离</li></ul><h2>原理</h2><h3><strong>底层原理</strong></h3><ul><li><p>‌<strong>架构层级</strong>‌</p><p>RenderObjects通过继承<code>ScriptableRendererFeature</code>和<code>ScriptableRenderPass</code>实现管线扩展，核心逻辑在<code>Execute()</code>方法中通过<code>CommandBuffer</code>提交绘制指令。其本质是通过URP的<code>ScriptableRenderContext</code>调度GPU渲染命令，与内置管线不同之处在于采用可编程的轻量级渲染管线架构。</p></li><li><p>‌<strong>渲染流程控制</strong>‌</p><p>通过<code>RenderPassEvent</code>枚举插入到URP的固定管线阶段（如AfterRenderingOpaques），底层会触发以下操作：</p><ul><li>调用<code>ConfigureTarget()</code>设置渲染目标</li><li>使用<code>FilteringSettings</code>过滤指定Layer的物体</li><li>通过<code>DrawingSettings</code>配置Shader Pass和排序规则</li></ul></li><li><p>‌<strong>材质替换机制</strong>‌</p><p>当启用Override Material时，URP会临时替换原始材质的Shader，但保留物体的顶点数据。该过程通过<code>MaterialPropertyBlock</code>实现动态参数传递，避免材质实例化开销。</p></li></ul><h3><strong>实现示例</strong></h3><ul><li><p>OutlineFeature.cs</p><pre><code class="csharp">using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;

public class OutlineFeature : ScriptableRendererFeature {
    class OutlinePass : ScriptableRenderPass {
        private Material _outlineMat;
        private LayerMask _layerMask;
        private FilteringSettings _filteringSettings;

        public OutlinePass(Material mat, LayerMask mask) {
            _outlineMat = mat;
            _layerMask = mask;
            _filteringSettings = new FilteringSettings(RenderQueueRange.opaque, _layerMask);
            renderPassEvent = RenderPassEvent.AfterRenderingOpaques;
        }

        public override void Execute(ScriptableRenderContext context, ref RenderingData data) {
            var drawingSettings = CreateDrawingSettings(
                new ShaderTagId("UniversalForward"), 
                ref data, 
                SortingCriteria.CommonOpaque
            );
            drawingSettings.overrideMaterial = _outlineMat;
            context.DrawRenderers(data.cullResults, ref drawingSettings, ref _filteringSettings);
        }
    }

    [SerializeField] private Material _outlineMaterial;
    [SerializeField] private LayerMask _outlineLayers = 1;
    private OutlinePass _pass;

    public override void Create() =&gt; _pass = new OutlinePass(_outlineMaterial, _outlineLayers);
    public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData data) 
        =&gt; renderer.EnqueuePass(_pass);
}</code></pre></li><li><p>Outline.shader</p><pre><code class="c">Shader "Custom/Outline" {
    Properties {
        _OutlineColor("Color", Color) = (1,0,0,1)
        _OutlineWidth("Width", Range(0,0.1)) = 0.03
    }
    SubShader {
        Tags { "RenderType"="Opaque" "Queue"="Geometry+100" }
        Pass {
            Cull Front
            ZWrite Off
            CGPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            #include "UnityCG.cginc"

            float _OutlineWidth;
            fixed4 _OutlineColor;

            struct appdata {
                float4 vertex : POSITION;
                float3 normal : NORMAL;
            };

            v2f vert(appdata v) {
                v2f o;
                v.vertex.xyz += v.normal * _OutlineWidth;
                o.pos = UnityObjectToClipPos(v.vertex);
                return o;
            }

            fixed4 frag(v2f i) : SV_Target {
                return _OutlineColor;
            }
            ENDCG
        }
    }
}</code></pre></li></ul><h3><strong>关键流程解析</strong></h3><ul><li><p>‌<strong>渲染指令提交</strong>‌</p><p><code>DrawRenderers</code>方法内部会构建<code>BatchRendererGroup</code>，将CPU侧的渲染数据批量提交至GPU，相比直接使用CommandBuffer更高效。</p></li><li><p>‌<strong>深度测试控制</strong>‌</p><p>示例中<code>ZWrite Off</code>禁用深度写入，使描边始终显示在原始物体表面，该技术也常用于解决透明物体渲染顺序问题。</p></li><li><p>‌<strong>多Pass协作</strong>‌</p><p>URP会先执行默认的Forward渲染Pass，再执行RenderObjects插入的Pass，通过<code>RenderPassEvent</code>控制执行顺序</p></li></ul><h2><strong>完整实现流程示例</strong></h2><ul><li><p>OutlineFeature.cs</p><pre><code class="csharp">using UnityEngine;
using UnityEngine.Rendering;
using UnityEngine.Rendering.Universal;

public class OutlineFeature : ScriptableRendererFeature {
    class OutlinePass : ScriptableRenderPass {
        private Material outlineMat;
        private LayerMask layerMask;
        private RenderTargetIdentifier source;

        public OutlinePass(Material mat, LayerMask mask) {
            outlineMat = mat;
            layerMask = mask;
            renderPassEvent = RenderPassEvent.AfterRenderingOpaques;
        }

        public override void Execute(ScriptableRenderContext context, ref RenderingData data) {
            CommandBuffer cmd = CommandBufferPool.Get("OutlinePass");
            var drawSettings = CreateDrawingSettings(
                new ShaderTagId("UniversalForward"), 
                ref data, SortingCriteria.CommonOpaque);
            var filterSettings = new FilteringSettings(RenderQueueRange.opaque, layerMask);
            context.DrawRenderers(data.cullResults, ref drawSettings, ref filterSettings);
            CommandBufferPool.Release(cmd);
        }
    }

    [SerializeField] private Material outlineMaterial;
    [SerializeField] private LayerMask outlineLayers;
    private OutlinePass pass;

    public override void Create() {
        pass = new OutlinePass(outlineMaterial, outlineLayers);
    }

    public override void AddRenderPasses(ScriptableRenderer renderer, ref RenderingData data) {
        renderer.EnqueuePass(pass);
    }
}</code></pre></li><li><p>Outline.shader</p><pre><code class="c">Shader "Custom/Outline" {
    Properties {
        _OutlineColor("Color", Color) = (1,1,1,1)
        _OutlineWidth("Width", Range(0,0.1)) = 0.05
    }
    SubShader {
        Tags { "RenderType"="Opaque" "LightMode"="UniversalForward" }
        Pass {
            CGPROGRAM
            // Vertex expansion logic...
            ENDCG
        }
    }
}</code></pre></li></ul><h2><strong>参数详解与用例</strong></h2><table><thead><tr><th>参数</th><th>说明</th><th>应用场景</th></tr></thead><tbody><tr><td>‌<strong>Event</strong>‌</td><td>渲染时机（如BeforeRenderingPostProcessing）</td><td>控制特效叠加顺序</td></tr><tr><td>‌<strong>LayerMask</strong>‌</td><td>目标渲染层级</td><td>仅对敌人/UI层描边</td></tr><tr><td>‌<strong>Override Material</strong>‌</td><td>替换材质</td><td>角色进入阴影区切换材质</td></tr><tr><td>‌<strong>Depth Test</strong>‌</td><td>深度测试模式</td><td>解决透明物体遮挡问题</td></tr><tr><td>‌<strong>Shader Passes</strong>‌</td><td>匹配的Shader LightMode标签</td><td>多Pass渲染（如"UniversalForward"）</td></tr></tbody></table><h2><strong>配置步骤</strong></h2><ul><li>创建URP Asset并启用Renderer Features</li><li>添加RenderObjects Feature到Forward Renderer</li><li>配置Event为AfterRenderingOpaques（不透明物体）或AfterRenderingTransparents（透明物体）</li><li>指定目标Layer和替换材质</li><li>调整Depth/Stencil参数解决遮挡问题</li></ul><p>典型应用包括：角色描边、场景分块渲染、特殊效果叠加（如受伤高亮）等。通过组合不同Event和LayerMask可实现复杂的渲染管线控制</p><hr/><blockquote><a href="https://link.segmentfault.com/?enc=2wjog65ctyn2SIyzDa5aoQ%3D%3D.rZbW2WPBSp4LRjHsCE%2BE%2BCnpuc%2F2UrbM8wq4%2F1VfE0m2jwzBR0fPErhus0LzgDTJkQzCWDQu9RJ5g7b4LlZn87pOn7yHHwqlLnsNM%2BIIL3jc1wCV6OKbD8iHcfXbCmrn7FLahf1DqUjkg%2BLxs4z6JSdBofSOyT5hHxeDeWq2Q6VEc64jpwJNZc5zmXy1fvOSk0AD0MLcOswtp3lUYGuKqktDrBLKtcc1eVI%2F196e1m0%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[设计系统注释，第二部分：组件注释的高级方]]></title>    <link>https://segmentfault.com/a/1190000047413432</link>    <guid>https://segmentfault.com/a/1190000047413432</guid>    <pubDate>2025-11-20 11:15:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>引言</h2><p>在<a href="https://segmentfault.com/a/1190000047410118" target="_blank">设计系统系列的第一部分</a>中，我们探讨了可访问性如何在不同实例的设计系统组件中被忽略的问题。通过使用Primer的"预设注释"，设计师能够包含组件中尚未内置且未在设计中直观传达的特定预设细节。然而，这些预设注释是每个设计系统独有的，其他组织无法直接使用。本篇文章将深入探讨如何为设计系统创建自定义注释，以及如何利用Figma的Code Connect功能在开发前捕获重要的可访问性细节。</p><h2>正文内容</h2><h3>1. 如何为设计系统制作预设注释</h3><h4>1.1 评估组件优先级</h4><p>并非所有组件都需要预设注释，因此首先需要评估哪些组件会从中受益最大。建议优先考虑以下类型的组件：</p><ul><li>与组织优先级相符的组件（如高价值产品或流量大的产品）</li><li>在可访问性审计问题中频繁出现的组件</li><li>采用首选开发框架（如React）实现的组件</li><li>最常被实现的组件</li></ul><p>在Primer的案例中，使用了一个名为Primer Query的内部工具来追踪所有组件实现和相关的审计问题，帮助筛选出最需要注释的10个组件。</p><h4>1.2 确定要包含的属性</h4><p>在确定哪些组件需要注释后，下一步是决定应该包含哪些属性。关键原则是：只包含那些没有视觉表现、不在组件属性中，且尚未内置在编码组件中的关键信息。</p><h4>1.3 属性映射的数据来源</h4><p>为了确保注释的全面性，应从多个来源交叉引用信息：</p><ul><li><strong>Primer.style上的组件文档</strong>：查找未内置到Figma或代码中的可访问性要求</li><li><strong>Storybook中的编码演示</strong>：查看组件文档或Figma资产本身未包含的代码结构或可访问性属性</li><li><strong>Figma资产库中的组件属性</strong>：关注Figma组件中未内置的可访问性属性、需求和使用指南</li></ul><p>例如，在为TextInput组件构建预设注释时，团队发现实现可能仅使用图标或隐藏输入标签。虽然放大镜图标可以作为可见标签，但这些字段仍需为辅助技术用户提供可访问的标签。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413435" alt="TextInput component accessibility docs on Primer.style with text highlighted that says: In limited cases an icon alone could act as the visible label." title="TextInput component accessibility docs on Primer.style with text highlighted that says: In limited cases an icon alone could act as the visible label."/></p><h3>2. 整合所有信息创建预设注释</h3><h4>2.1 实际案例：TextInput组件</h4><p>TextInput组件的预设注释包含：</p><ul><li>使用指南和Storybook的链接</li><li>关于如何在设计中最佳使用组件以避免潜在问题的可选教程</li><li>输入类型和错误文本的两个必填提示</li><li>偶尔隐藏表单标签的可选提示</li></ul><h4>2.2 处理Figma组件的局限性</h4><p>Figma组件虽然提供许多自定义选项，但仍有重要信息无法直接设置。例如，TextInput组件虽然有inputTextType属性（关于视觉设计和排版），但没有设置表单输入类型的选项；可以在侧边栏设置标签和输入字段的值，但由于默认隐藏，无法设置错误验证消息的文本。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413436" alt="A TextInput component in Figma. On the canvas and in the component properties sidebar, the label says: Label set in the sidebar. The text in the input field says: Input value set in the sidebar. There is a red error message below the input field that says: validation message. There is no property that can be set for this listed in the sidebar." title="A TextInput component in Figma. On the canvas and in the component properties sidebar, the label says: Label set in the sidebar. The text in the input field says: Input value set in the sidebar. There is a red error message below the input field that says: validation message. There is no property that can be set for this listed in the sidebar." loading="lazy"/></p><h3>3. 创建预设注释的经验教训</h3><h4>3.1 适用场景</h4><p>预设注释特别适合：</p><ul><li>新兴的设计系统</li><li>尚未被广泛采用的设计系统</li></ul><h4>3.2 维护挑战</h4><p>成熟的设计系统（如Primer）频繁更新，这意味着如果没有密切监控，设计系统组件本身可能会与预设注释的构建方式不同步，导致开发开始后出现混乱和返工。因此，必须确保在注释创建后有维护的能力。</p><h4>3.3 团队适用性</h4><p>对于新团队或对设计系统不熟悉的成员，内置的指导和文档链接非常有用。更有经验的成员则能够微调预设及其使用方式。</p><h3>4. 预设注释与通用注释的关系</h3><h4>4.1 重叠问题</h4><p>组件的预设注释和非设计系统特有的注释类型之间可能存在足够的重叠，这可能导致混淆。例如，GitHub注释工具包同时包含基本&lt;textarea&gt;表单元素的注释组件和&lt;TextArea&gt; Primer组件的预设注释。</p><h4>4.2 解决方案</h4><p>一种解决方法是向默认注释集添加设计系统特定的属性。这样可以在普通注释上切换布尔属性，使其显示与设计系统组件相关的特定链接和属性。</p><h3>5. 预设创建流程可能解锁自动化</h3><h4>5.1 当前工具的局限性</h4><p>现有的Figma插件虽然声称能够扫描设计文件以帮助注释，但结果往往参差不齐，包含难以管理的噪音和误报。主要原因是这些公共插件与设计系统无关。</p><h4>5.2 自动化前提</h4><p>要让插件准确标记设计元素，首先需要：</p><ul><li>理解如何识别画布上的组件</li><li>识别使用的变体</li><li>理解设置的属性</li></ul><p>映射预设注释的组件属性（那些视觉设计或代码中未传达的内容）是实现更实用注释自动化的必要步骤。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413437" alt="A Figma file showing an open design for Releases with an expanded layer tree highlighting a Primer Button component in the design. To the left of the screenshot are several git-lines and a Preset annotation for a Primer Button with a zap icon intersecting it. The git-line trails and the direction of the annotation give the feeling of flying toward the layer tree, which visually suggests this Primer Button layer can be automatically identified and annotated." title="A Figma file showing an open design for Releases with an expanded layer tree highlighting a Primer Button component in the design. To the left of the screenshot are several git-lines and a Preset annotation for a Primer Button with a zap icon intersecting it. The git-line trails and the direction of the annotation give the feeling of flying toward the layer tree, which visually suggests this Primer Button layer can be automatically identified and annotated." loading="lazy"/></p><h3>6. 有前景的新方法：Figma的Code Connect</h3><h4>6.1 Code Connect简介</h4><p>Primer是Figma开发者模式中Code Connect功能的早期采用者之一。这项功能实际上可以稍微分离设计和代码，让设计师专注于在Figma中创造最佳用户体验，同时开发者也能获得最佳开发体验。</p><h4>6.2 实际应用</h4><p>通过Code Connect，团队能够绕过大部分预设注释，直接将关键的可访问性细节添加到开发者可以从Figma导出的代码中。例如：</p><ul><li>在IconButton Figma组件中添加隐藏层</li><li>包含aria-label的文本属性，让设计师直接从组件属性面板添加值</li><li>隐藏层不会干扰视觉效果，aria-label属性会与组件的其他代码一起直接导出</li></ul><h4>6.3 实施建议</h4><p>为设计系统各组件设置Code Connect需要时间，以下是一些建议：</p><ol><li><strong>保持一致性</strong>：确保创建的属性和隐藏图层的放置方式在各个组件之间保持一致</li><li><strong>使用分支实验</strong>：在设计系统库的分支上进行实验</li><li><strong>使用视觉回归测试(VRT)</strong>：直接向组件添加复杂性会增加未来出现故障的风险，尤其是对于有多种变体的组件</li></ol><h2>结论</h2><p>本文深入探讨了设计系统中高级组件注释的方法，重点介绍了创建自定义预设注释的完整流程以及利用Figma Code Connect的创新方法。通过系统地评估组件优先级、映射关键属性并整合多源信息，团队能够创建出既实用又具针对性的注释系统。同时，Code Connect的引入为设计系统注释带来了新的可能性，通过将关键可访问性细节直接嵌入代码，大大提高了设计和开发之间的协作效率。</p><p>从实践中我们了解到，预设注释特别适合新兴或未被广泛采用的设计系统，但也需要持续的维护投入。而自动化注释工具的潜力则与对组件属性的深入理解密不可分。随着设计系统工具的不断发展，如Code Connect这样的创新功能正在为设计和开发团队创造更加无缝的协作体验。</p><h2>扩展链接</h2><p><a href="https://segmentfault.com/a/1190000047410118" target="_blank">设计系统注释，第一部分：可访问性如何被组件遗漏</a></p><p>文章内容来源于Github Copilot官网</p>]]></description></item><item>    <title><![CDATA[使用 Python 将 Excel 转换]]></title>    <link>https://segmentfault.com/a/1190000047413457</link>    <guid>https://segmentfault.com/a/1190000047413457</guid>    <pubDate>2025-11-20 11:14:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数据处理的过程中，Excel 文件常常被用作存储和分析数据的主要工具。有时，我们需要将 Excel 中的内容以图像的形式展示，以便于共享、记录或进一步的分析。Python 作为一种强大的编程语言，提供了多种库来进行不同的任务。在这篇文章中，我们将使用 <code>Spire.XLS for Python</code> 库将 Excel 文件转换为 PNG 格式的图像。</p><h2>为什么需要将 Excel 转换为 PNG？</h2><p>将 Excel 文件转换为 PNG 图像的主要原因有：</p><ol><li><strong>共享性</strong> ：图像文件可以轻松分享，无论接收者是否安装 Excel。</li><li><strong>可视化</strong> ：在报告和演示文稿中，图像比文本更具吸引力，能够帮助读者快速理解数据。</li><li><strong>安全性</strong> ：将数据转换为图像可以一定程度上防止数据被直接编辑或篡改。</li></ol><h2>Spire.XLS for Python 简介</h2><p><code>Spire.XLS for Python</code> 是一个功能强大的 Excel 处理库，能够支持创建、读取、编辑和转换 Excel 文件。它支持多种格式的文件，包括 XLS, XLSX, CSV 等，并且可以将这些文件导出为图像格式（如 PNG）。该库具有易于使用的 API，使得即使是初学者也能够轻松上手。</p><h2>安装 Spire.XLS for Python</h2><p>在开始之前，首先需要安装 <code>Spire.XLS for Python</code>。可以通过 pip 来安装：</p><pre><code class="bash">pip install Spire.XLS</code></pre><p>确保安装完成后，可以通过 import 语句来引入库：</p><pre><code class="python">from spire.xls import *
from spire.xls.common import *</code></pre><h2>使用 Spire.XLS 转换 Excel 为 PNG</h2><p>下面是一个基本的步骤和代码示例，展示如何将 Excel 文件转换为 PNG 文件。</p><h3>代码示例</h3><pre><code class="python"># 创建工作簿对象
workbook = Workbook()

# 加载 Excel 文件
workbook.LoadFromFile("Sample.xlsx")

# 获取第一个工作表
sheet = workbook.Worksheets[0]

# 设置工作表的所有边距为零
sheet.PageSetup.LeftMargin = 0
sheet.PageSetup.BottomMargin = 0
sheet.PageSetup.TopMargin = 0
sheet.PageSetup.RightMargin = 0

# 将工作表转换为图像
image = sheet.ToImage(sheet.FirstRow, sheet.FirstColumn, sheet.LastRow, sheet.LastColumn)

# 保存图像为 PNG 文件
image.Save("SheetToImageWithoutMargins.png")

# 释放工作簿资源
workbook.Dispose()</code></pre><h3>代码说明</h3><ol><li><strong>创建工作簿对象</strong> ：通过 <code>Workbook()</code> 创建一个工作簿实例。</li><li><strong>加载 Excel 文件</strong> ：使用 <code>LoadFromFile()</code> 方法加载指定路径的 Excel 文件。</li><li><strong>获取工作表</strong> ：通过 <code>workbook.Worksheets[0]</code> 获取第一个工作表。</li><li><strong>设置边距</strong> ：使用 <code>PageSetup</code> 属性将工作表的上下左右边距设置为零，以确保图像没有多余的空白区域。</li><li><strong>转换为图像</strong> ：使用 <code>ToImage()</code> 方法将工作表转换为图像，指定需要转换的行和列范围。</li><li><strong>保存图像文件</strong> ：使用 <code>Save()</code> 方法将图像保存为 PNG 文件。</li><li><strong>释放资源</strong> ：最后，调用 <code>Dispose()</code> 方法释放工作簿占用的资源。</li></ol><h2>注意事项</h2><ul><li>确保 Excel 文件的路径正确，且文件存在。</li><li>PNG 文件的大小和质量可能与工作表的内容密切相关。通过设置边距，可以优化输出效果。</li><li><code>Spire.XLS</code> 的功能强大，可以处理复杂的表格。然而，确保熟悉其文档以充分利用其功能。</li></ul><h2>总结</h2><p>通过使用 <code>Spire.XLS for Python</code>，我们能够方便地将 Excel 文档转换为 PNG 图像。这一过程快速高效，适合于各种场景，如数据共享、可视化展示等。希望本教程能帮助您在 Python 编程中实现更灵活的数据处理与展示。无论是在工作上还是学习中，掌握这种技术都将为您提供极大的便利。</p>]]></description></item><item>    <title><![CDATA[2025年10款军工品质的项目管理软件盘]]></title>    <link>https://segmentfault.com/a/1190000047413470</link>    <guid>https://segmentfault.com/a/1190000047413470</guid>    <pubDate>2025-11-20 11:14:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>在装备研制“多、快、好、省”的刚性要求下，军工企业正经历一场静默的数字化革命。CIO们面临的不再是工具选择问题，而是<strong>如何在数据不落地、流程不中断的前提下，构建符合GJB 9001C标准的研发管理体系</strong>。市场充斥着宣称“军工可用”的工具，但真正通过三级保密认证、具备涉密环境适配能力的不足20%。</blockquote><p>作为深耕军工信息化的分析师，我提炼出军工软件选型的<strong>黄金三角评估框架</strong>：</p><ol><li><strong>安全基线</strong>（物理隔离/逻辑隔离能力、涉密载体管控）</li><li><strong>流程韧性</strong>（需求-设计-试验全链路追溯、变更闭环）</li><li><strong>生态自主</strong>（国产软硬件适配度、本地化服务响应）</li></ol><p>以下基于该框架，客观解析10款军工级项目管理工具的核心能力。</p><hr/><h2>一、军工品质的4大硬核评估标准</h2><table><thead><tr><th>维度</th><th>关键指标</th></tr></thead><tbody><tr><td><strong>安全合规</strong></td><td>通过国家保密局二级以上认证、支持三员分立、国密算法传输</td></tr><tr><td><strong>涉密环境适配</strong></td><td>支持物理隔离网络部署、涉密U盘管控、无互联网访问记录</td></tr><tr><td><strong>军工流程固化</strong></td><td>内置GJB 5000A三级标准模板、试验数据自动归档</td></tr><tr><td><strong>供应链可控</strong></td><td>核心代码自主率＞90%、国产化替代预案完备</td></tr></tbody></table><hr/><p><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdm6zt" alt="image.png" title="image.png"/></p><h2>二、10款军工项目管理软件深度解构</h2><h3>1. <strong>禅道军工增强版</strong></h3><ul><li><strong>安全认证</strong>：通过涉密信息系统产品检测（BMB20-2007）</li><li><p><strong>核心能力</strong>：</p><ul><li>支持麒麟/UOS系统+达梦数据库全栈部署</li><li>提供“试验数据黑匣子”功能，满足GJB 367A-2001要求</li><li>可对接军工专用加密机实现端到端加密</li></ul></li><li><strong>典型用户</strong>：航天科工某所（型号研制项目）</li></ul><h3>2. <strong>Jira（虚拟化部署方案）</strong></h3><ul><li><strong>安全策略</strong>：通过VMware NSX网络隔离实现逻辑涉密域</li><li><p><strong>适配方案</strong>：</p><ul><li>禁用所有外部API调用</li><li>自建CA证书体系替代SSL</li></ul></li><li><strong>局限</strong>：核心代码未开源，深度审计困难</li></ul><h3>3. <strong>鼎捷PLM+PM集成方案</strong></h3><ul><li><p><strong>军工优势</strong>：</p><ul><li>工艺BOM与项目管理强耦合</li><li>支持GJB 150环境试验数据采集</li></ul></li><li><strong>部署模式</strong>：提供军工专有机房托管服务</li></ul><h3>4. <strong>广联达BIM5D军工模块</strong></h3><ul><li><p><strong>特色功能</strong>：</p><ul><li>复杂装备装配进度三维可视化</li><li>装备故障树与项目WBS联动分析</li></ul></li><li><strong>认证</strong>：国防科技工业软件评测中心认证</li></ul><h3>5. <strong>新松机器人项目管理平台</strong></h3><ul><li><p><strong>军工基因</strong>：</p><ul><li>源自军工自动化项目管理系统改造</li><li>内置AGV调度与试验排程算法</li></ul></li><li><strong>适配</strong>：专为军工制造车间离散型项目优化</li></ul><h3>6. <strong>金蝶EAS军工项目云</strong></h3><ul><li><p><strong>安全架构</strong>：</p><ul><li>私有云部署+物理断网模式</li><li>提供涉密文档区块链存证</li></ul></li><li><strong>成本</strong>：年服务费约¥80万起</li></ul><h3>7. <strong>泛微e-cology军工套件</strong></h3><ul><li><p><strong>流程管控</strong>：</p><ul><li>七级权限体系匹配军工岗位分级</li><li>电子签章符合GJB 9001C要求</li></ul></li><li><strong>集成</strong>：深度对接军工电子档案系统</li></ul><h3>8. <strong>用友NC Cloud军工版</strong></h3><ul><li><p><strong>业财融合</strong>：</p><ul><li>科研经费全口径管控</li><li>国军标成本归集模型</li></ul></li><li><strong>短板</strong>：项目进度可视化能力较弱</li></ul><h3>9. <strong>华为云DevCloud涉密专享区</strong></h3><ul><li><p><strong>基础设施</strong>：</p><ul><li>华为松山湖基地独立物理集群</li><li>通过军委装备发展部安全审查</li></ul></li><li><strong>适用</strong>：信息化类装备研制项目</li></ul><h3>10. <strong>浪潮PS Cloud军工定制版</strong></h3><ul><li><p><strong>军工烙印</strong>：</p><ul><li>基于军用软件能力成熟度模型改造</li><li>支持装备定型文件自动组卷</li></ul></li><li><strong>服务</strong>：提供驻场保密员支持</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl902" alt="" title="" loading="lazy"/></p><h2>三、军工选型三大生死线</h2><h3>策略一：物理隔离刚需场景</h3><p><strong>实施要点</strong>：</p><ul><li>选择支持“单机版+移动存储管控”的工具（如禅道军工版）</li><li>禁止任何云服务接入，采用定期光盘备份  <br/><strong>工具组合</strong>：禅道（研制管理）+ 广联达BIM（工艺仿真）</li></ul><h3>策略二：多型号并行研制</h3><p><strong>关键能力</strong>：</p><ul><li>多项目集资源冲突检测</li><li>技术状态基线管理  <br/><strong>推荐方案</strong>：鼎捷PLM+PM（制造端）+ 泛微e-cology（审批端）</li></ul><h3>策略三：军转民技术溢出</h3><p><strong>风险控制</strong>：</p><ul><li>建立军民品数据防火墙</li><li>使用独立账号体系  <br/><strong>工具选择</strong>：华为DevCloud涉密区（民品创新）+ 新松机器人（军品制造）</li></ul><hr/><h2>四、客观对比：工具能力矩阵</h2><table><thead><tr><th>能力维度</th><th>禅道军工版</th><th>Jira隔离方案</th><th>鼎捷方案</th></tr></thead><tbody><tr><td><strong>保密认证</strong></td><td>★★★★★</td><td>★★☆☆☆</td><td>★★★★☆</td></tr><tr><td><strong>试验数据管理</strong></td><td>★★★★☆</td><td>★☆☆☆☆</td><td>★★★★★</td></tr><tr><td><strong>成本可控性</strong></td><td>★★★★☆</td><td>★★★☆☆</td><td>★★☆☆☆</td></tr><tr><td><strong>二次开发自由度</strong></td><td>★★★★★</td><td>★★☆☆☆</td><td>★★★☆☆</td></tr></tbody></table><blockquote>注：★越多表示能力越强</blockquote><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmGRH" alt="" title="" loading="lazy"/></p><h2>五、清晰引导：为何禅道是军工首选路径？</h2><ol><li><p><strong>安全基因</strong>：</p><ul><li>代码自主率97%，无后门风险</li><li>提供军工保密资质代办服务</li></ul></li><li><p><strong>军工know-how沉淀</strong>：</p><ul><li>内置200+装备研制流程模板</li><li>支持GJB 1391故障模式分析</li></ul></li><li><p><strong>全生命周期覆盖</strong>：</p><pre style="display:none;"><code class="mermaid">graph LR  
A[预研立项] --&gt; B[方案设计]  
B --&gt; C[初样研制]  
C --&gt; D[正样试验]  
D --&gt; E[定型评审]  </code></pre></li><li><p><strong>成本优势</strong>：</p><ul><li>开源内核节省80%授权费</li><li>人均年维护成本＜¥3,000</li></ul></li></ol><blockquote>对Jira用户建议：仅用于非密前端需求管理，通过单向网闸同步至禅道核心系统</blockquote><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl909" alt="jira" title="jira" loading="lazy"/></p><h2>六、FAQ：军工选型高频问题</h2><p><strong>Q1：如何验证工具是否真通过保密认证？</strong>  <br/><em>A</em>：要求供应商出示：  <br/>① 国家保密局《涉密信息系统产品检测证书》原件  <br/>② 军委装备发展部装备承制单位认证（A类）  <br/>③ 提供同类军工客户名单（可现场考察）  </p><p><strong>Q2：物理隔离环境下如何保证数据安全？</strong>  <br/><em>A</em>：三重防护机制：</p><ol><li>存储加密：采用SM4国密算法磁盘加密</li><li>传输管控：禁用USB/WiFi，仅允许刻录光盘</li><li>审计追踪：记录所有键盘操作和屏幕录像</li></ol><p><strong>Q3：小批量定制装备研制适用什么工具？</strong>  <br/><em>A</em>：推荐新松机器人项目管理平台：</p><ul><li>支持小批量柔性生产排程</li><li>内置特种工艺知识库</li><li>成本仅为大型PLM系统的1/3</li></ul><p><strong>Q4：军转民项目如何避免数据污染？</strong>  <br/><em>A</em>：实施“三隔离”策略：</p><ol><li>物理隔离：军民项目分属不同服务器集群</li><li>人员隔离：设置不同角色权限组</li><li>流程隔离：军品流程禁止访问民品模块</li></ol><p><strong>Q5：遭遇国外制裁断供如何应对？</strong>  <br/><em>A</em>：提前部署：</p><ul><li>选择代码自主率＞90%的工具（如禅道）</li><li>储备国产化替代组件包</li><li>每季度进行断供模拟演练</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdmWLu" alt="用友" title="用友" loading="lazy"/></p><h2>结语</h2><p>军工项目的数字化不是技术竞赛，而是<strong>生存能力的重构</strong>。当试验数据可能决定装备成败，当代码自主率关乎供应链安全，工具选择必须回归本质：</p><ul><li><strong>禅道军工版</strong>等自主工具，通过物理隔离、流程固化、生态可控构建数字堡垒</li><li>国际工具仅限非密场景，需配合严格的数据治理方案</li></ul><p>在装备强国的征途上，选择与国家战略同频的数字化伙伴，就是守护国家安全的第一道防线。  </p><p>（注：本文所述能力均基于厂商公开认证资料，具体选型需结合企业保密资质进行现场验证）</p>]]></description></item><item>    <title><![CDATA[非凸科技连续支持第50届ICPC亚洲区域]]></title>    <link>https://segmentfault.com/a/1190000047413478</link>    <guid>https://segmentfault.com/a/1190000047413478</guid>    <pubDate>2025-11-20 11:13:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月8日-9日，第50届ICPC国际大学生程序设计竞赛亚洲区域赛·南京站在南京航空航天大学将军路校区举行，来自191所高校的335支正式队伍与24支特邀队伍在此展开智力交锋。作为赛事的重要支持方，非凸科技再次以独特的技术视角参与这场编程盛会，为竞技舞台注入了产业界的创新活力。<br/><img width="554" height="370" referrerpolicy="no-referrer" src="/img/bVdm6zx" alt="image.png" title="image.png"/><br/>开幕式上，非凸科技首席运营官郑媛姿受邀发表致辞。她表示，算法思维是推动科技创新的核心动力，ICPC选手展现的问题解决能力与团队协作精神，正是未来科技行业最珍贵的品质。非凸科技始终致力于支持计算机领域人才培养，希望与新生代技术人才共同成长。<br/><img width="554" height="370" referrerpolicy="no-referrer" src="/img/bVdm6zB" alt="image.png" title="image.png" loading="lazy"/><br/>赛事期间，非凸科技人事团队通过现场交流与专场宣讲，不仅展示了在金融科技领域的前沿实践，更以“与优秀同行”的理念吸引了众多学子的热切关注。</p><p>赛场之上，五小时的密集赛程考验着每位选手的极限。在ICPC独特的团队作战模式下，三名队员需要在有限时间内完成头脑风暴、代码实现与测试优化全流程，展现了当代计算机学子出色的专业素养与团队配合能力。经过多轮精彩对决，最终清华大学代表队摘得桂冠，北京大学与复旦大学分获亚军、季军。<br/><img width="554" height="370" referrerpolicy="no-referrer" src="/img/bVdm6zE" alt="image.png" title="image.png" loading="lazy"/><br/>作为连接学术与产业的重要平台，ICPC不仅为高校学子提供了展示才华的舞台，也为计算机产业持续输送着新生力量。未来，非凸科技将继续深化校企合作，携手各方共同培育更多具有创新精神与实践能力的优秀人才，为推动行业技术进步贡献专业力量。</p>]]></description></item><item>    <title><![CDATA[JVS逻辑引擎用拖拽配置，5步完成数组循]]></title>    <link>https://segmentfault.com/a/1190000047413487</link>    <guid>https://segmentfault.com/a/1190000047413487</guid>    <pubDate>2025-11-20 11:12:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>使用循环容器对数组对象数据进行累加求和，是一种通过循环遍历数组集合中的元素，并将它们逐个累加到一个变量中的过程。<br/>在编程世界中，循环累加求和几乎是每个开发者都会遇到的基础操作——比如统计销售额到计算平均值，这些都需要对一组数据的反复累加。传统的实现方式是编写如for、while等循环代码，即使对于简单任务，也需要处理变量初始化、边界条件等细节。<br/>在JVS逻辑引擎中，这一过程被简化为直观的拖拽配置，非技术人员也能轻松实现复杂的数据处理。<br/>JVS逻辑引擎通过循环容器组件，将传统的编程步骤转化为图形化操作：初始化累加器、遍历集合、逐次累加、结果输出均可通过节点连接完成。例如，统计订单金额时，只需要拖拽“循环节点”绑定数据源，并用“公式节点”设置累加规则，系统便会自动生成执行流。这种设计不仅降低了操作难度，还将原本需数小时的开发压缩至几分钟。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413489" alt="图片" title="图片"/><br/>接下来我以一个简单的数组求和为例，逐步展示如何通过拖拉拽配置实现循环累加。</p><h3>配置说明</h3><p>配置思路也是同编程方法相似，将一系列数值（通常存储在数组、列表或其他集合中）累加起来，得到它们的总和。这个过程涉及到几个关键步骤：<br/>步骤一：初始化一个累加器：首先，需要初始化一个变量（通常称为累加器或总和变量），用于存储累加的结果。这个变量在开始时的值通常是0，但也可能根据具体情况而有所不同（可理解为i=i+1,i为初始化变量）；<br/>步骤二：遍历集合中的元素：接下来，使用循环结构（如for循环、while循环等）遍历集合（如数组、列表）中的每个元素。循环的每次迭代都会累加集合中的一个元素；<br/>步骤三：累加元素的值：在循环的每次迭代中，将当前遍历到的元素的值加到累加器变量上；<br/>步骤四：完成遍历：当遍历完集合中的所有元素后，累加器变量中就存储了所有元素值的总和；<br/>步骤五：输出结果（可选）：最后，将累加的结果输出到节点或将其用于进一步的计算；</p><h3>以下为示例：</h3><p>进入逻辑设计器中，首先拖取一个【固定变量】节点用于接收累加求和值，步骤一如下图<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413490" alt="图片" title="图片" loading="lazy"/><br/>设置一个初始化变量值为0<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413491" alt="图片" title="图片" loading="lazy"/><br/>然后用数组对象变量节点组装一组集合数据，供演示使用<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413492" alt="图片" title="图片" loading="lazy"/><br/>步骤二拖取一个【循环变量】节点，遍历数组对象变量数据<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413493" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413494" alt="图片" title="图片" loading="lazy"/><br/>步骤三在循环容器画布中，用固定变量节点取循环数中每次要增加的数值<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413495" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413496" alt="图片" title="图片" loading="lazy"/><br/>步骤四使用固定变量的“变量绑定赋值”对外层的节点公式赋值，此处使用ADD函数，每次把外层的数据与当前循环数值相加后再赋值给外层。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413497" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413498" alt="图片" title="图片" loading="lazy"/><br/>步骤五赋值到循环外累加和节点，输出结果如下图<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413499" alt="图片" title="图片" loading="lazy"/><br/>在线demo：<a href="https://link.segmentfault.com/?enc=N5c7tDdFs6BFU2%2BmK9g%2Bnw%3D%3D.osqy5%2BqdLQzRP0MEnHSqGdquUi091A2w9OJ%2FHkyGYyo%3D" rel="nofollow" target="_blank">https://logic.bctools.cn</a><br/>开源框架：<a href="https://link.segmentfault.com/?enc=tmoYfEXzWF3L%2BfRO6S4i8g%3D%3D.4Rxiv8XkcSFNSR99RgkMdun6cOajdPkaXXDRKdX%2BPVWGeApZSG%2B%2BcwEM2bn5ynYI" rel="nofollow" target="_blank">https://gitee.com/software-minister/jvs</a></p>]]></description></item><item>    <title><![CDATA[投入仅一部手机的钱？Uni+Php前后端]]></title>    <link>https://segmentfault.com/a/1190000047413524</link>    <guid>https://segmentfault.com/a/1190000047413524</guid>    <pubDate>2025-11-20 11:11:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>还在觉得创业需要大投入、高门槛？校园跑腿赛道早已诞生 “轻资产” 新玩法 —— <strong>用 Uni+Php 技术栈</strong>的跑腿源码，投入仅一部手机的费用（几千元），零经验、无店面、少人力，就能搭建专属服务平台，把学生刚需变成稳定收益！</p><p><strong>一、 “一部手机的钱”到底花在哪？账本大公开</strong><br/>所谓“一部手机的钱”（约几千元），具体流向了哪里？这与动辄需要十几万的传统创业模式形成了鲜明对比。<br/>传统模式 vs. 轻资产模式<br/>传统创业： 场地租金 + 硬件设备 + 货物囤积 + 高额人力成本 = 投入无底洞<br/>轻资产创业： 源码费用 + 服务器与域名 = 投入清晰可控<br/><img width="723" height="1320" referrerpolicy="no-referrer" src="/img/bVdmgHL" alt="" title=""/><img width="291" height="490" referrerpolicy="no-referrer" src="/img/bVdm6Ao" alt="" title="" loading="lazy"/><br/><strong>二、 技术揭秘：Uni+Php如何扛起“轻资产”大旗？</strong><br/>为什么是这两个技术？因为它们是为“轻资产”、“快启动”而生的完美组合。<br/><strong>Uni-App：你的“超级前端”，一统用户与骑手端</strong><br/>多端覆盖： 一套代码，可以同时发布成微信小程序、支付宝小程序、H5页面，甚至APP。这意味着你只需开发一次，就能让所有用户通过他们最习惯的入口使用你的服务，获客成本降到最低。<br/>开发高效： 基于Vue.js语法，学习曲线平缓，开发效率极高，让你能快速将想法落地。<br/><strong>Php + ThinkPHP：你的“稳定大后方”，低成本部署之王</strong><br/>环境普及，成本低廉： 几乎所有的云服务器都默认支持Php环境，部署简单到令人发指，无需为特殊环境支付额外费用。<br/>框架强大，开发迅捷： ThinkPHP等框架提供了完善的数据操作、用户认证和安全机制，让你能像搭积木一样快速构建后台管理系统和API接口。<br/>这套技术栈的组合，本质上是将复杂的技术问题标准化、模块化，让你无需成为架构大师，也能搭建一个稳定、专业的商业平台。<br/><img width="723" height="247" referrerpolicy="no-referrer" src="/img/bVdmcMZ" alt="" title="" loading="lazy"/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdjVHX" alt="" title="" loading="lazy"/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdh3qY" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[国产CRM怎么挑？从适配性到实用性，这1]]></title>    <link>https://segmentfault.com/a/1190000047413528</link>    <guid>https://segmentfault.com/a/1190000047413528</guid>    <pubDate>2025-11-20 11:10:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>国产CRM怎么挑？从适配性到实用性，这12款系统值得看</h2><h4>一、前言：为什么CRM选型是中小企业的“生死局”？</h4><p>在数字化转型的浪潮中，CRM（客户关系管理系统）早已不是“可选工具”，而是企业连接客户、整合业务、提升效率的“中枢神经”。但现实中，<strong>80%的企业在CRM选型上踩过坑</strong>：要么选了功能冗余的“大而全”系统，员工抵触不用；要么选了“通用型”工具，无法匹配行业特殊场景（比如工业企业需要生产联动，零售企业需要会员复购）；要么忽视了落地成本，导致“买得起用不起”。</p><p>本文的核心目标，是帮你<strong>从“需求出发”而非“功能堆砌”</strong> ，理清选型逻辑，再通过12款主流国产CRM的实测，找到最适合你的那一款。</p><h3>二、选型前的“灵魂三问”：先搞懂自己要什么</h3><p>选型的第一步，不是看系统功能，而是<strong>明确企业的核心需求</strong>。请先回答以下三个问题：</p><h4>1. 你的企业处于什么阶段？</h4><ul><li><strong>初创期（0-50人）</strong> ：核心需求是“获客+简单客户管理”，优先选轻量化、易用性高、低成本的SaaS系统（比如钉钉CRM、企业微信CRM）；</li><li><strong>成长期（50-200人）</strong> ：核心需求是“销售流程标准化+内部协同”，需要功能覆盖获客、转化、复购，且能对接进销存/ERP的系统（比如超兔CRM、纷享销客）；</li><li><strong>成熟期（200人以上）</strong> ：核心需求是“全业务整合+定制化”，需要支持行业场景（比如工业生产、金融风控）、能整合现有系统的中大型CRM（比如销售易、超兔一体云）。</li></ul><h4>2. 你的核心痛点是什么？</h4><p>CRM的价值，是解决“最疼的那根刺”：</p><ul><li>若痛点是“获客难”：选支持全渠道获客（百度/抖音/微信）、线索追踪的系统（比如EC CRM、超兔的工商搜客）；</li><li>若痛点是“转化低”：选销售流程标准化（比如纷享销客的销售漏斗）、AI辅助跟单（比如超兔的智能体）的系统；</li><li>若痛点是“复购差”：选客户生命周期管理（比如美信云的RFM分析）、会员体系（比如管家婆的零售会员）的系统；</li><li>若痛点是“内部协同乱”：选一体云架构（比如超兔的CRM+进销存+生产）、跨部门数据打通的系统。</li></ul><h4>3. 你的预算与投入能力？</h4><ul><li><strong>低预算（年投入＜10万）</strong> ：优先选SaaS模式、按用户数付费的系统（比如钉钉CRM、超兔的客制化订阅）；</li><li><strong>中预算（10万-50万）</strong> ：可以选支持定制化、行业化的系统（比如超兔的一体云、销售易的行业方案）；</li><li><strong>高预算（＞50万）</strong> ：考虑本地部署、深度定制的系统（比如智云通、销售易的企业版）。</li></ul><h3>三、选型的“五大核心维度”：告别“拍脑袋”决策</h3><p>明确需求后，接下来要从“适配性、功能深度、易用性、扩展性、服务”五大维度评估系统，每个维度都要“落地到场景”：</p><h4>1. 业务适配性：是否匹配你的行业场景？</h4><p><strong>关键问题</strong>：系统能否覆盖你所在行业的“特殊需求”？</p><ul><li>比如<strong>工业/工贸企业</strong>：需要“CRM+生产工单+进销存”联动（比如超兔的MES生产管理，订单联动触发生产计划）；</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413530" alt="" title=""/></p><ul><li>比如<strong>零售企业</strong>：需要“会员体系+复购分析+线上线下联动”（比如纷享销客的会员RFM模型）；</li><li>比如<strong>服务企业（教育/医疗）</strong> ：需要“工单管理+客户跟进全流程”（比如美信云的维修工单、红圈的外勤服务）；</li><li>比如<strong>商贸企业</strong>：需要“进销存+采购+财务”一体化（比如管家婆的CRM+进销存联动）。</li></ul><h4>2. 功能深度：核心模块是否“够用且好用”？</h4><p>CRM的核心功能是<strong>“获客-转化-复购-留存”</strong>，需要重点看：</p><ul><li><strong>获客模块</strong>：是否支持全渠道（百度/抖音/微信/地推）线索抓取？是否能计算“市场活动ROI”（比如超兔的线索成本均摊）？</li><li><strong>客户管理</strong>：是否支持“客户查重”（比如超兔的企业简称模糊查重）、“工商信息自动补全”（比如销售易的天眼查对接）？</li><li><strong>销售流程</strong>：是否有“销售漏斗”（比如纷享销客的阶段管理）、“AI跟单提醒”（比如超兔的智能体）？</li><li><strong>财务联动</strong>：是否支持“应收/回款/开票”三角联动（比如超兔的订单财务管控）？</li></ul><h4>3. 易用性与落地成本：员工会不会用？</h4><p>很多CRM失败的原因，是“系统太复杂，员工抵触”。需要看：</p><ul><li><strong>界面友好度</strong>：是否有“自定义工作台”（比如超兔的多岗位驾驶舱）、“手机端功能”（比如红圈的外勤扫码）？</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413531" alt="" title="" loading="lazy"/></p><ul><li><strong>培训成本</strong>：是否有“话术库/文件库”（比如超兔的武器云）、“智能日报”（比如超兔的自动生成日报）？</li><li><strong>操作效率</strong>：是否支持“一键处理线索”（比如超兔的线索转客户）、“扫码出入库”（比如超兔的库存管理）？</li></ul><h4>4. 扩展性与整合能力：能否对接现有系统？</h4><p>企业的IT系统是“生态”，不是“孤岛”。需要看：</p><ul><li><strong>对接能力</strong>：是否支持ERP（金蝶/用友）、进销存（管家婆）、电商平台（京东/淘宝）、社交工具（微信/企业微信）？</li><li><strong>API接口</strong>：是否有开放API（比如超兔的业务API）、支持RPA机器人（比如超兔对接电商平台）？</li><li><strong>未来扩展性</strong>：是否能支持“企业成长后的需求”（比如初创期用SaaS，成长期升级定制化）？</li></ul><h4>5. 服务与稳定性：出问题能不能找到人？</h4><ul><li><strong>系统稳定性</strong>：是否有“99.9%以上的 uptime”（比如超兔的业内口碑）、“数据备份”（比如销售易的异地灾备）？</li><li><strong>客服响应</strong>：是否有“7*24小时客服”（比如超兔的老客户转介绍率40%）、“专属客户成功经理”（比如销售易的企业版）？</li><li><strong>数据安全</strong>：是否符合“等保2.0”、“隐私合规”？</li></ul><h3>四、12款国产CRM实测：从场景到功能的全面对比</h3><p>基于以上维度，我们选取了<strong>覆盖不同行业、不同阶段</strong>的12款主流国产CRM，从“核心定位、适配场景、优势功能、潜在不足”四个维度评测，帮你快速匹配：</p><h4>1. 超兔CRM：工业/工贸企业的“一体云首选”</h4><ul><li><strong>核心定位</strong>：中国SaaS开创企业，专注工业/工贸企业的“CRM+进销存+生产+财务”一体云平台。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413532" alt="" title="" loading="lazy"/></p><ul><li><strong>适配场景</strong>：工业制造、工贸一体、小型生产企业（比如五金加工、电子元件制造）。</li><li><p><strong>优势功能</strong>：</p><ul><li>「一体云架构」：打通CRM、进销存、生产MES、财务，订单自动触发生产计划，解决“销售与生产脱节”的痛点；</li><li>「低成本定制」：支持三级菜单自定义、工作台驾驶舱、业务表编辑，满足“小步快跑”的成长需求；</li><li>「AI智能体」：基于客户视图定制销售跟单机器人，自动提醒“该跟进某客户”“该触发应收”；</li><li>「稳定性」：业内口碑第一，很多企业从其他系统转用超兔（比如某五金厂因“系统老崩溃”换超兔）。</li></ul></li><li><strong>潜在不足</strong>：社交化获客功能较弱（比如微信营销不如EC CRM），更适合“内部协同优先”的企业。</li><li><strong>推荐指数</strong>：★★★★☆（4.5/5）</li></ul><h4>2. 纷享销客：快消/零售的“销售流程专家”</h4><ul><li><strong>核心定位</strong>：移动办公+销售流程管理，适合“以销售为核心”的快消、零售企业。</li><li><strong>适配场景</strong>：快消品（饮料/零食）、零售连锁（便利店/美妆）、互联网销售团队。</li><li><p><strong>优势功能</strong>：</p><ul><li>「销售漏斗」：可视化销售阶段（比如“线索-跟进-签单”），实时查看“各阶段转化率”；</li><li>「移动办公」：手机端支持“外勤打卡”“客户拜访记录”“订单录入”，适合跑业务的销售；</li><li>「会员体系」：支持“积分/等级/权益”管理，对接线下门店POS机。</li></ul></li><li><strong>潜在不足</strong>：生产/财务联动较弱，不适合工业企业。</li><li><strong>推荐指数</strong>：★★★★☆（4.4/5）</li></ul><h4>3. 销售易：中大型企业的“行业化解决方案”</h4><ul><li><strong>核心定位</strong>：面向中大型企业的“行业化CRM”，覆盖金融、制造、医疗等垂直领域。</li><li><strong>适配场景</strong>：金融机构（银行/保险）、大型制造企业（汽车/机械）、医疗设备公司。</li><li><p><strong>优势功能</strong>：</p><ul><li>「行业模板」：比如金融行业的“客户风险评估”、制造行业的“供应链协同”；</li><li>「AI能力」：基于客户行为的“智能画像”，自动推荐“高价值客户”；</li><li>「整合能力」：对接SAP、Oracle等大型ERP，适合“已有成熟IT体系”的企业。</li></ul></li><li><strong>潜在不足</strong>：成本较高（企业版年投入＞10万），初创企业慎选。</li><li><strong>推荐指数</strong>：★★★★☆（4.3/5）</li></ul><h4>4. 管家婆CRM：商贸企业的“进销存一体化”</h4><ul><li><strong>核心定位</strong>：中小企业友好型CRM，专注“商贸+进销存”联动。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413533" alt="" title="" loading="lazy"/></p><ul><li><strong>适配场景</strong>：商贸公司（批发/零售）、小商品市场（玩具/文具）、小型电商企业。</li><li><p><strong>优势功能</strong>：</p><ul><li>「进销存联动」：CRM客户下单后，自动触发库存扣减、采购计划；</li><li>「多价格策略」：支持“批发价/零售价/会员价”，适合“多客群”的商贸企业；</li><li>「低成本」：SaaS版年投入＜5万，适合预算有限的初创商贸公司。</li></ul></li><li><strong>潜在不足</strong>：功能较基础，不支持生产管理，工业企业不适合。</li><li><strong>推荐指数</strong>：★★★☆☆（3.8/5）</li></ul><h4>5. 红圈CRM：外勤与服务的“行业深耕者”</h4><ul><li><strong>核心定位</strong>：专注“外勤+销售协同”，适合需要上门服务的行业。</li><li><strong>适配场景</strong>：建材（门窗安装）、医疗（设备维护）、家政服务（保洁/维修）。</li><li><p><strong>优势功能</strong>：</p><ul><li>「外勤管理」：支持“GPS定位”“扫码签到”“服务工单”，解决“外勤人员管理难”的痛点；</li><li>「服务流程」：从“客户投诉”到“工单派工”到“服务评价”全流程跟踪；</li><li>「行业模板」：比如建材行业的“测量-设计-安装”流程、医疗行业的“设备巡检”。</li></ul></li><li><strong>潜在不足</strong>：获客模块较弱，不适合“以线上获客为主”的企业。</li><li><strong>推荐指数</strong>：★★★☆☆（3.9/5）</li></ul><h4>6. 钉钉CRM：初创企业的“轻量化选择”</h4><ul><li><strong>核心定位</strong>：钉钉生态内的轻量化CRM，适合“用钉钉办公”的初创企业。</li><li><strong>适配场景</strong>：初创团队（0-50人）、小型互联网公司、线下小店（奶茶店/咖啡馆）。</li><li><p><strong>优势功能</strong>：</p><ul><li>「生态整合」：直接对接钉钉的“审批”“考勤”“文档”，不用切换系统；</li><li>「易用性」：界面简单，支持“线索录入”“客户标签”“跟进记录”等基础功能；</li><li>「低成本」：免费版可满足基础需求，付费版年投入＜3万。</li></ul></li><li><strong>潜在不足</strong>：功能较浅，不支持“生产/财务联动”，成长型企业需要升级。</li><li><strong>推荐指数</strong>：★★★☆☆（3.7/5）</li></ul><h4>7. 企业微信CRM：微信生态的“客户运营专家”</h4><ul><li><strong>核心定位</strong>：深度对接企业微信，适合“以微信获客”的企业。</li><li><strong>适配场景</strong>：微商、社群运营（美妆/服装）、线下门店（美甲/健身）。</li><li><p><strong>优势功能</strong>：</p><ul><li>「微信联动」：支持“客户添加自动打标签”“朋友圈营销”“群聊SOP”；</li><li>「客户运营」：通过“企业微信会话存档”，查看销售与客户的聊天记录，优化话术；</li><li>「轻量化」：手机端操作方便，适合“以微信为主要获客渠道”的企业。</li></ul></li><li><strong>潜在不足</strong>：不支持“进销存/生产”，工业企业不适合。</li><li><strong>推荐指数</strong>：★★★☆☆（3.8/5）</li></ul><h4>8. 百会CRM：国际化企业的“多语言工具”</h4><ul><li><strong>核心定位</strong>：支持多语言、多币种，适合“有海外业务”的企业。</li><li><strong>适配场景</strong>：外贸企业（进出口）、跨境电商（亚马逊/Shopify）、海外分支机构。</li><li><p><strong>优势功能</strong>：</p><ul><li>「多语言支持」：支持英语、西班牙语、阿拉伯语等10+语言；</li><li>「多币种管理」：自动转换汇率，解决“海外订单计价”问题；</li><li>「国际化整合」：对接PayPal、Stripe等海外支付工具。</li></ul></li><li><strong>潜在不足</strong>：国内场景适配较弱（比如微信获客），纯国内企业慎选。</li><li><strong>推荐指数</strong>：★★★☆☆（3.6/5）</li></ul><h4>9. 智云通CRM：制造企业的“深度定制专家”</h4><ul><li><strong>核心定位</strong>：专注制造业，支持“深度定制”的CRM。</li><li><strong>适配场景</strong>：大型制造企业（汽车零部件/机械装备）、需要“个性化流程”的工厂。</li><li><p><strong>优势功能</strong>：</p><ul><li>「制造场景适配」：支持“BOM管理”“生产工单”“质量追溯”（比如某汽车零部件厂用智云通对接MES）；</li><li>「深度定制」：可以根据企业需求修改系统流程（比如“销售订单→生产计划→质检→发货”的全链路定制）；</li><li>「数据安全」：支持本地部署，满足“敏感数据不上云”的需求。</li></ul></li><li><strong>潜在不足</strong>：成本高（定制费＞10万）、实施周期长（2-3个月），初创企业慎选。</li><li><strong>推荐指数</strong>：★★★★☆（4.2/5）</li></ul><h4>10. 美信云CRM：服务行业的“工单管理专家”</h4><ul><li><strong>核心定位</strong>：专注服务行业，解决“客户服务全流程”问题。</li><li><strong>适配场景</strong>：教育机构（培训/留学）、医疗服务（牙科/体检）、酒店/餐饮。</li><li><p><strong>优势功能</strong>：</p><ul><li>「工单管理」：从“客户投诉”到“派工”到“服务评价”全流程跟踪，支持“多渠道工单”（电话/微信/APP）；</li><li>「客户生命周期」：根据“服务次数/评价”自动分类客户（比如“高价值会员”“流失风险客户”）；</li><li>「复购提醒」：自动提醒“该回访某客户”，提升复购率（比如某教育机构用美信云提升30%复购）。</li></ul></li><li><strong>潜在不足</strong>：生产/进销存功能缺失，工业企业不适合。</li><li><strong>推荐指数</strong>：★★★☆☆（3.9/5）</li></ul><h4>11. EC CRM：社交化销售的“全渠道获客专家”</h4><ul><li><strong>核心定位</strong>：社交化CRM，适合“以社交平台获客”的企业。</li><li><strong>适配场景</strong>：互联网销售（直播/短视频）、社群运营（美妆/保健品）、线上教育。</li><li><p><strong>优势功能</strong>：</p><ul><li>「全渠道获客」：支持抖音/微信/小红书/知乎线索抓取，自动导入CRM；</li><li>「社交化跟进」：通过“微信自动发消息”“朋友圈互动提醒”，提升客户粘性；</li><li>「AI话术优化」：分析销售与客户的聊天记录，推荐“高转化率话术”。</li></ul></li><li><strong>潜在不足</strong>：内部协同功能较弱，不适合“需要生产/财务联动”的企业。</li><li><strong>推荐指数</strong>：★★★☆☆（3.8/5）</li></ul><h4>12.商汤 CRM：AI 驱动的 “数据精准营销专家”</h4><ul><li><strong>核心定位</strong>：AI 赋能的 CRM，适合 “数据驱动” 的企业。</li><li><strong>适配场景</strong>：电商（直播带货）、零售（连锁超市）、金融（理财 / 保险）。</li><li><strong>优势功能</strong>：</li><li>「AI 客户画像」：通过 “用户行为数据”（浏览 / 购买 / 互动轨迹），生成 360° 精准标签；</li><li>「智能营销推送」：基于画像自动匹配产品，触发短信 / APP / 公众号精准触达；</li><li>「数据预测分析」：AI 预判客户复购概率、流失风险，提前制定挽留策略。</li><li><strong>潜在不足</strong>：操作门槛较高，需专业数据人员维护，不适合 “小型初创企业”。</li><li><strong>推荐指数</strong>：★★★★☆（4.2/5）</li></ul><p>企业选型时，除关注适配场景与核心功能，还需权衡操作门槛与成本，避免盲目追求功能全面。结合自身业务痛点筛选，才能让 CRM 真正成为提升客户管理效率、推动业绩增长的有力工具。</p>]]></description></item><item>    <title><![CDATA[基于YOLOv8的水稻病害检测项目｜完整]]></title>    <link>https://segmentfault.com/a/1190000047413541</link>    <guid>https://segmentfault.com/a/1190000047413541</guid>    <pubDate>2025-11-20 11:10:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>基于YOLOv8的水稻病害检测项目｜完整源码数据集+PyQt5界面+完整训练流程+开箱即用！</h2><p>本项目提供 <strong>完整可运行源码 + 完整数据集(已标注) + 训练脚本 + PyQt5 可视化图形界面 + 推理部署教程</strong>，帮助你快速搭建一个可用于农业生产场景的 <strong>水稻叶片病害识别系统</strong>，无需从零开始配置环境或重新整理模型结构，直接开启训练或应用。</p><blockquote>源码在文末哔哩哔哩视频简介处获取。</blockquote><pre><code>检测主要分为三类：
细菌性叶斑病，主要影响水稻等作物，表现为叶片上出现水状斑点；
褐斑病，常见于水稻，表现为叶片上出现褐色斑点，影响光合作用；
叶霉病，主要影响禾本科植物，表现为叶片上出现黑色霉斑。</code></pre><h3>基本功能演示</h3><p>哔哩哔哩视频演示：<a href="https://www.bilibili.com/video/BV1TByjBqEPi" target="_blank">https://www.bilibili.com/video/BV1TByjBqEPi</a></p><p>本项目支持 <strong>多种输入模式识别</strong>：</p><ul><li>单张图片识别</li><li>文件夹批量识别</li><li>本地视频识别</li><li>USB/笔记本摄像头实时识别</li><li>识别结果可视化叠加：预测框 + 类别名称 + 置信度</li><li>支持推理过程中动态调整置信度阈值</li></ul><p>系统可直接运行，无需额外修改路径或代码。</p><p>用户界面采用 <strong>PyQt5</strong> 开发，界面清晰易用，一键选择输入源即可开始检测。</p><p>应用场景示例：</p><table><thead><tr><th>场景</th><th>功能说明</th></tr></thead><tbody><tr><td>农田实时病害监控</td><td>连上摄像头即可边巡田边识别</td></tr><tr><td>农科论文 / 毕设项目</td><td>模型可直接扩容和迁移训练</td></tr><tr><td>智慧农业系统对接</td><td>推理模块可直接封装为API</td></tr></tbody></table><p>可检测三类常见水稻病害：</p><ol><li>细菌性叶斑病：叶片出现水渍状斑块，逐渐扩散</li><li>褐斑病：叶片产生褐色坏死斑点，对叶绿素合成影响明显</li><li>叶霉病：叶片背面出现黑绿色霉层，抑制植物光合作用</li></ol><h3>项目摘要</h3><p>本项目集成了 <strong>YOLOv8 病害检测模型</strong> 与 <strong>PyQt5 图形界面可视化系统</strong>，不仅支持离线识别，还可通过视频流进行实时病害检测。系统界面操作简单，无需专业深度学习经验即可快速上手运行模型，适用于农业实验、智慧农田监控、科研论文与课程设计等场景。</p><p>项目提供：</p><ul><li><strong>已标注水稻病害数据集</strong></li><li><strong>训练完成的模型权重文件</strong></li><li><strong>可继续训练或迁移训练的源码工程</strong></li><li><strong>一键运行的图形化检测系统</strong></li><li><strong>全套训练 / 推理 / 部署教程</strong></li></ul><p>@[toc]</p><h3>前言</h3><p>随着农业智能化推进，病害识别逐渐从人工观察转向<strong>传感器监控 + 视觉智能诊断</strong>。传统人工巡田不仅效率低，而且容易受经验限制导致误判。本项目采用 <strong>YOLOv8 轻量化目标检测算法</strong>，在保持高识别准确率的同时，兼具速度快、部署轻的优点，能够适应边缘设备、无人机巡检、田间智能终端等应用环境。</p><p>与此同时，我们将模型封装为可视化界面，使得非算法背景的农技人员、科研助理、农业院校学生也能轻松使用该系统进行水稻健康状况分析。</p><h2>一、软件核心功能介绍及效果演示</h2><p>系统核心功能包括：</p><table><thead><tr><th>功能</th><th>描述</th></tr></thead><tbody><tr><td>YOLOv8 病害检测</td><td>可准确划分三类水稻病害类型</td></tr><tr><td>PyQt5 图形界面</td><td>支持按钮操作，一键推理</td></tr><tr><td>多输入模式支持</td><td>图片 / 文件夹 / 视频 / 摄像头</td></tr><tr><td>动态参数可调</td><td>可实时调整置信度与NMS阈值</td></tr><tr><td>自动可视化结果</td><td>边检测边展示，支持保存截图</td></tr><tr><td>可继续训练</td><td>提供训练脚本和标注文件，可扩展数据集</td></tr></tbody></table><p>识别示例效果（可在此放模型预测图）：</p><p>病害区域将自动框选，并给出类别与置信度。</p><p>通过 GUI 界面可直接完成所有操作，无需命令行。</p><h2>二、软件效果演示</h2><p>为了直观展示本系统基于 YOLOv8 模型的检测能力，我们设计了多种操作场景，涵盖静态图片、批量图片、视频以及实时摄像头流的检测演示。</p><h3>（1）单图片检测演示</h3><p>用户点击“选择图片”，即可加载本地图像并执行检测：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413543" alt="image-20251109134746855" title="image-20251109134746855"/></p><hr/><h3>（2）多文件夹图片检测演示</h3><p>用户可选择包含多张图像的文件夹，系统会批量检测并生成结果图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413544" alt="image-20251109141338465" title="image-20251109141338465" loading="lazy"/></p><hr/><h3>（3）视频检测演示</h3><p>支持上传视频文件，系统会逐帧处理并生成目标检测结果，可选保存输出视频：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413545" alt="image-20251109141407236" title="image-20251109141407236" loading="lazy"/></p><hr/><h3>（4）摄像头检测演示</h3><p>实时检测是系统中的核心应用之一，系统可直接调用摄像头进行检测。由于原理和视频检测相同，就不重复演示了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413546" alt="image-20251109141440040" title="image-20251109141440040" loading="lazy"/></p><hr/><h3>（5）保存图片与视频检测结果</h3><p>用户可通过按钮勾选是否保存检测结果，所有检测图像自动加框标注并保存至指定文件夹，支持后续数据分析与复审。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413547" alt="image-20251109141502324" title="image-20251109141502324" loading="lazy"/></p><h2>三、模型的训练、评估与推理</h2><p>YOLOv8是Ultralytics公司发布的新一代目标检测模型，采用更轻量的架构、更先进的损失函数（如CIoU、TaskAlignedAssigner）与Anchor-Free策略，在COCO等数据集上表现优异。<br/> 其核心优势如下：</p><ul><li>高速推理，适合实时检测任务</li><li>支持Anchor-Free检测</li><li>支持可扩展的Backbone和Neck结构</li><li>原生支持ONNX导出与部署</li></ul><h3>3.1 YOLOv8的基本原理</h3><p>YOLOv8 是 Ultralytics 发布的新一代实时目标检测模型，具备如下优势：</p><ul><li><strong>速度快</strong>：推理速度提升明显；</li><li><strong>准确率高</strong>：支持 Anchor-Free 架构；</li><li><strong>支持分类/检测/分割/姿态多任务</strong>；</li><li>本项目使用 YOLOv8 的 Detection 分支，训练时每类表情均标注为独立目标。</li></ul><p>YOLOv8 由Ultralytics 于 2023 年 1 月 10 日发布，在准确性和速度方面具有尖端性能。在以往YOLO 版本的基础上，YOLOv8 引入了新的功能和优化，使其成为广泛应用中各种物体检测任务的理想选择。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413548" alt="image-20250526165954475" title="image-20250526165954475" loading="lazy"/></p><p>YOLOv8原理图如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413549" alt="image-20250526170118103" title="image-20250526170118103" loading="lazy"/></p><h3>3.2 数据集准备与训练</h3><p>采用 YOLO 格式的数据集结构如下：</p><pre><code class="kotlin">dataset/
├── images/
│   ├── train/
│   └── val/
├── labels/
│   ├── train/
│   └── val/</code></pre><p>每张图像有对应的 <code>.txt</code> 文件，内容格式为：</p><pre><code class="bash">4 0.5096721233576642 0.352838390077821 0.3947600423357664 0.31825755058365757</code></pre><p>分类包括（可自定义）：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413550" alt="image-20251109141648698" title="image-20251109141648698" loading="lazy"/></p><h3>3.3. 训练结果评估</h3><p>训练完成后，将在 <code>runs/detect/train</code> 目录生成结果文件，包括：</p><ul><li><code>results.png</code>：损失曲线和 mAP 曲线；</li><li><code>weights/best.pt</code>：最佳模型权重；</li><li><code>confusion_matrix.png</code>：混淆矩阵分析图。</li></ul><blockquote>若 mAP@0.5 达到 90% 以上，即可用于部署。</blockquote><p>在深度学习领域，我们通常通过观察损失函数下降的曲线来评估模型的训练状态。YOLOv8训练过程中，主要包含三种损失：定位损失（box_loss）、分类损失（cls_loss）和动态特征损失（dfl_loss）。训练完成后，相关的训练记录和结果文件会保存在runs/目录下，具体内容如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413551" alt="image-20251109141622539" title="image-20251109141622539" loading="lazy"/></p><h3>3.4检测结果识别</h3><p>使用 PyTorch 推理接口加载模型：</p><pre><code class="python">import cv2
from ultralytics import YOLO
import torch
from torch.serialization import safe_globals
from ultralytics.nn.tasks import DetectionModel

# 加入可信模型结构
safe_globals().add(DetectionModel)

# 加载模型并推理
model = YOLO('runs/detect/train/weights/best.pt')
results = model('test.jpg', save=True, conf=0.25)

# 获取保存后的图像路径
# 默认保存到 runs/detect/predict/ 目录
save_path = results[0].save_dir / results[0].path.name

# 使用 OpenCV 加载并显示图像
img = cv2.imread(str(save_path))
cv2.imshow('Detection Result', img)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre><p>预测结果包含类别、置信度、边框坐标等信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413552" alt="image-20251109141552945" title="image-20251109141552945" loading="lazy"/></p><h2>四.YOLOV8+YOLOUI完整源码打包</h2><p>本文涉及到的完整全部程序文件：包括<strong>python源码、数据集、训练代码、UI文件、测试图片视频</strong>等（见下图），获取方式见【4.2 完整源码下载】：</p><h3>4.1 项目开箱即用</h3><p>作者已将整个工程打包。包含已训练完成的权重，读者可不用自行训练直接运行检测。</p><p>运行项目只需输入下面命令。</p><pre><code class="bash">python main.py</code></pre><p>读者也可自行配置训练集，或使用打包好的数据集直接训练。</p><p>自行训练项目只需输入下面命令。</p><pre><code class="bash">yolo detect train data=datasets/expression/loopy.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 batch=16 lr0=0.001</code></pre><h3>4.2 完整源码</h3><p>至项目实录视频下方获取：<br/>哔哩哔哩视频演示：<a href="https://www.bilibili.com/video/BV1TByjBqEPi" target="_blank">https://www.bilibili.com/video/BV1TByjBqEPi</a><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413553" alt="image-20250801135823301" title="image-20250801135823301" loading="lazy"/></p><p>包含：</p><blockquote><p>📦完整项目源码</p><p>📦 预训练模型权重</p><p>🗂️ 数据集地址（含标注脚本）</p></blockquote><h2>总结</h2><p>本项目面向水稻病害智能识别应用场景，基于 YOLOv8 构建高精度目标检测模型，并结合 PyQt5 实现了<strong>可直接运行的可视化检测系统</strong>。项目不仅提供完整的已标注数据集和训练脚本，还包含训练完成的模型权重和全套部署流程，用户无需额外编写代码即可进行病害识别、模型复训或扩展应用。</p><p>系统实现了图片、批量文件夹、视频及实时摄像头的多输入检测方式，并支持动态阈值调节、检测结果可视化和自动保存，有效降低了农业场景中病害诊断的成本与门槛。通过轻量化模型结构和高效推理框架，本项目可灵活应用于科研教学、农业现场巡检以及智慧农田监测等多种实际业务需求。</p><p>本项目不仅展示了深度学习在农业病害检测领域的落地方案，也为后续的模型优化、病害种类扩展及智能农业系统集成提供了完整且可复用的工程模板。</p>]]></description></item><item>    <title><![CDATA[隐语可信数据空间MOOC第24讲笔记：联]]></title>    <link>https://segmentfault.com/a/1190000047413571</link>    <guid>https://segmentfault.com/a/1190000047413571</guid>    <pubDate>2025-11-20 11:09:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>笔记内容来自隐语Mooc，欢迎一起来学习。Mooc课程地址：<a href="https://link.segmentfault.com/?enc=Ndj1hplt4c508J76Y6A61Q%3D%3D.lhcidui1eyhy3jfQB9P5ud2juodRYAtbjHd4mrleyWsvU%2FQxgX4Jf4SDi5vhc3kh20Pvct8QQBSNi7vt0%2FBUwZ5jIy7r230bZGKiPbibEhn0EqSvK7OBtrWikPwps6nIOSn94fjUunvfnO19GjTFLA%3D%3D" rel="nofollow" target="_blank">https://www.secretflow.org.cn/community/bootcamp/2narwgw4ub8r...</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413573" alt=" title=" title=" title="/></p><h2>🧠 一、联邦学习概述</h2><h3>1.1 背景与动机</h3><ul><li><strong>隐私泄露问题</strong>：用户上传数据（定位、图片、语音等）存在隐私风险。</li><li><strong>法规与意识</strong>：《个人信息保护法》、GDPR等法规出台，公众隐私意识增强。</li></ul><h3>1.2 什么是联邦学习（FL）</h3><ul><li><strong>定义</strong>：一种分布式机器学习范式，多个数据源协同训练模型，<strong>不共享原始数据</strong>，只传递模型更新。</li><li><strong>目标</strong>：保护隐私、实现多方协作建模。</li></ul><hr/><h2>🔧 二、联邦学习的类型</h2><h3>2.1 跨设备联邦学习（Cross-Device）</h3><ul><li><strong>参与者</strong>：大量终端设备（手机、IoT设备）</li><li><strong>特点</strong>：设备本地训练 → 上传模型更新 → 服务器聚合更新</li></ul><h3>2.2 跨机构联邦学习（Cross-Silo）</h3><ul><li><strong>参与者</strong>：多个组织（医院、银行等）</li><li><p><strong>流程</strong>：</p><ol><li>分发全局模型</li><li>本地训练（可交换中间结果）</li><li>返回模型更新</li><li>联邦聚合</li></ol></li></ul><hr/><h2>🛡️ 三、隐私保护技术</h2><table><thead><tr><th>技术</th><th>原理</th><th>特点</th></tr></thead><tbody><tr><td>差分隐私（DP）</td><td>在数据中注入噪声</td><td>权衡隐私与模型效用</td></tr><tr><td>同态加密（HE）</td><td>在加密数据上计算</td><td>支持加法同态等</td></tr><tr><td>安全多方计算（MPC）</td><td>多方协同计算，不暴露输入</td><td>保证输入私密性</td></tr></tbody></table><hr/><h2>🧩 四、联邦学习平台</h2><table><thead><tr><th>平台</th><th>开发方</th></tr></thead><tbody><tr><td>TensorFlow Federated</td><td>谷歌</td></tr><tr><td>PySyft</td><td>OpenMined</td></tr><tr><td>FATE</td><td>微众银行</td></tr><tr><td>隐语（SecretFlow）</td><td>蚂蚁集团</td></tr><tr><td><strong>FederatedScope</strong></td><td><strong>阿里巴巴</strong></td></tr></tbody></table><hr/><h2>🚀 五、FederatedScope 框架</h2><h3>5.1 核心特点</h3><ul><li><strong>事件驱动（Event-Driven）</strong>：通过事件-处理函数定义行为</li><li><strong>模块化设计</strong>：解耦训练、聚合、通信等模块</li><li><strong>支持单机与分布式</strong>：统一接口，易于扩展</li></ul><h3>5.2 核心模块</h3><ul><li><strong>Worker</strong>：Server / Client</li><li><strong>Trainer</strong>：本地训练</li><li><strong>Aggregator</strong>：联邦聚合</li><li><strong>Monitor</strong>：记录与可视化</li><li><strong>Communicator</strong>：通信后端（Standalone / Distributed）</li><li><strong>Message</strong>：信息抽象，触发不同行为</li></ul><hr/><h2>🧪 六、热门研究方向</h2><h3>6.1 图联邦学习（FGL）</h3><ul><li><strong>问题</strong>：图数据分布在不同客户端，如何协同训练GNN？</li><li><strong>工具</strong>：FederatedScope-GNN（FS-G）</li><li><strong>支持</strong>：数据集、模型、算法、隐私攻击等全面覆盖</li></ul><h3>6.2 个性化联邦学习（pFL）</h3><ul><li><p><strong>动机</strong>：</p><ul><li>数据异质性（Non-IID）</li><li>设备资源差异</li></ul></li><li><p><strong>方法</strong>：</p><ul><li>客户端个性化：正则化、元学习、知识蒸馏等</li><li>服务端个性化：模型混合、聚类、多任务学习等</li></ul></li><li><strong>基准</strong>：pFL-Bench（20+方法，多数据集）</li></ul><h3>6.3 联邦超参优化（FedHPO）</h3><ul><li><strong>问题</strong>：在FL中高效优化超参</li><li><p><strong>方法</strong>：</p><ul><li>黑盒优化</li><li>多保真度评估（如减少训练轮数）</li></ul></li><li><strong>基准</strong>：FedHPO-Bench（多模型、多数据集、多算法）</li></ul><h3>6.4 联邦学习中的隐私攻击与防御</h3><ul><li><p><strong>攻击类型</strong>：</p><ul><li>成员推断攻击</li><li>类别代表攻击</li><li>属性推断攻击</li><li>训练数据/标签推断攻击</li></ul></li><li><p><strong>防御方法</strong>：</p><ul><li>加密梯度（HE、MPC）</li><li>扰动梯度（DP、梯度裁剪）</li><li>输入编码（MixUp、InstaHide）</li><li>私有组件（不共享BatchNorm、私有层）</li></ul></li></ul><hr/><h2>📌 七、实践建议</h2><ul><li><p><strong>入门路径</strong>：</p><ol><li>理解FL基本概念与隐私技术</li><li>使用FederatedScope运行FedAvg示例</li><li>尝试修改配置、自定义模型或数据集</li><li>探索pFL、FGL、FedHPO等研究方向</li></ol></li><li><p><strong>资源</strong>：</p><ul><li>官网：<a href="https://link.segmentfault.com/?enc=5exM5TRf%2FN1ZLbksGXNDRw%3D%3D.Fazi9oVqjLWKoKXLlngeg%2Fbmwyc2nmudgXYsybU6F2Y%3D" rel="nofollow" target="_blank">https://federatedscope.io</a></li><li>GitHub：<a href="https://link.segmentfault.com/?enc=6Btyo5fE2t%2FGd6LenZrf5Q%3D%3D.O%2FptLndZLx%2Bf%2BrJz4I2Ghbtd9FLJUW1JKN0ymKZD2i76XFX3nCGQ4SQ9Cww6jEIJ" rel="nofollow" target="_blank">FederatedScope</a></li><li>Playground：提供多种示例与配置</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[OneDrive不太稳定，有什么工具替代]]></title>    <link>https://segmentfault.com/a/1190000047413575</link>    <guid>https://segmentfault.com/a/1190000047413575</guid>    <pubDate>2025-11-20 11:08:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近期用户反馈OneDrive在实际使用中，不稳定现象却屡见不鲜。无论是文件同步延迟、登录困难，还是版本冲突，都可能给日常工作带来负面的影响。那么，当OneDrive显得有些“力不从心”时，有没有值得信赖的替代工具呢？</p><p><img width="658" height="350" referrerpolicy="no-referrer" src="/img/bVdjxj5" alt="" title=""/></p><h2>稳定性问题：OneDrive的潜在局限性</h2><p>大多数技术团队在选择云存储服务时，首要关注的一定是工具的稳定性。稳定性不仅事关日常工作的流畅性，更是保障数据可靠性的基础。不可否认，OneDrive作为微软生态的一部分，原本凭借其深度整合的特性以及广泛普及的办公应用吸引了不少用户。然而，众多使用者却发现，OneDrive在以下几个方面存在不足之处：</p><p><strong>文件同步的不一致性：</strong>团队成员上传或修改文件后，其他人有时无法即刻获取最新版本。这种同步延迟引发的不一致性，可能造成沟通不畅甚至是决策失误。</p><p><strong>应用跨平台的稳定性弱：</strong>尤其在非Windows系统（如macOS或Linux）或移动端环境中，OneDrive的兼容性表现不够理想，经常出现意外崩溃或卡顿。</p><p><strong>网络依赖性问题：</strong>在网络较差或波动较大的环境下，OneDrive的响应效率容易受到明显影响，导致文件下载、预览变得更加缓慢。</p><p>就如一把工具必须经得起思维缜密的工匠的检验，团队对生产力工具的要求也理应更高。在使用过程中，频发的稳定性问题不仅能耗费大量的时间处理小问题，更可能影响人员协作和业务连续性。因此，寻找一款更稳定、功能相似又能在多样化场景下运行流畅的替代工具，就显得尤为重要。</p><h2>一览Zoho网盘：从现代团队需求出发</h2><p>在替代工具的选择上，<a href="https://link.segmentfault.com/?enc=S%2Bos6MT5BZQe%2BE1lzYaoqw%3D%3D.tk%2Fgqjxu2R9f5g6ceC0JH8eFVRr0hO%2FBsj3OHaTwkN8aAy4%2Bu8hcZyJEnXyx8c53%2FXDx%2FrK1R1oDNOI%2F6%2B0gGA%3D%3D" rel="nofollow" target="_blank">Zoho网盘</a>（Zoho WorkDrive）是不得不提的一款备选方案。作为Zoho公司旗下的一部分，Zoho WorkDrive并不仅仅是一个文件存储工具，而是一个完全面向团队协作优化的平台。下面，我们以技术团队的典型需求为视角，逐一剖析Zoho网盘的特性。</p><p><strong>1. 稳定可靠的同步机制：文件实时协作的保障</strong></p><p>Zoho网盘从底层设计上注重数据的实时性和一致性，这使得其在文件同步方面表现尤为突出。不同于OneDrive可能经历的同步延迟问题，Zoho网盘采用了更加高效的分布式文件存储机制，确保每一次上传、修改都能在极短的时间内同步到每一位团队成员的设备上。此外，它支持自动版本管理功能，不仅记录了每一个文件的操作记录，还方便用户随时回溯到之前的版本。这对于程序开发或文档编写过程中实时性强、修改频繁的场景而言，显然更为实用。</p><p><strong>2. 兼容性与扩展性良好：适配各种团队需求</strong></p><p>技术团队大多面临一个现实难题：成员所使用的设备多样化。这就要求云存储工具具有出色的跨平台兼容性。Zoho网盘支持Windows、macOS以及多种主流移动操作系统，并且在Linux环境下也通过Web端提供了几乎所有的功能，保障了数据的可访问性。同时，其基于API的开放体系结构容许团队快速将WorkDrive与自有工具链进行集成，比如自动化部署脚本、定制化的数据管理流程等。</p><p><strong>3. 强大的团队协作功能：数据流转不再复杂</strong></p><p>文件协作共享是技术团队的日常诉求，尤其是当多个团队成员同时处理一个目录或项目时。Zoho网盘专门优化了团队协作的交互体验，包括文件夹级权限设置、自定义文件共享链接的有效期限与密码保护等功能。这种细节的优化，不仅提升了协作效率，还大大减少了可能的权限事故。此外，其“团队存储”概念鼓励数据以组织层级结构保存，实现了真正意义上的共享资源中心。</p><p><strong>4. 安全性与隐私保护的领军者</strong></p><p>对于技术团队来说，数据安全性也成为了评估存储工具的重要因素。Zoho网盘提供了全方位的安全防护，包括数据加密传输、角色权限粒度控制以及与GDPR（《通用数据保护条例》）等隐私规范的全面兼容。技术团队通过这种严密的安全架构，既能保护代码库、技术文档等核心资产，也能完全掌控外部协作过程中对数据的访问权限。此外，Zoho云服务器分布在全球不同地区，为用户提供了快速、可靠的业务连续性支持。</p><h2>更换工具的实际策略和建议</h2><p>对于已经深度部署OneDrive的技术团队来说，将主要工作环境切换到Zoho网盘并非一夜之间能够完成的事情。这种转移需要一些系统化的规划和执行步骤：</p><p>数据迁移方案的制定：通过Zoho提供的迁移工具，将文件从OneDrive批量传输到WorkDrive，并分类按照团队或项目进行有序保存。</p><p>员工培训与上手演练：为保证平稳过渡，可以选取部分团队成员进行小范围测试，并通过使用反馈调整早期配置。</p><p>工具的全面集成化：在完成文件迁移之后，将Zoho网盘与团队的现有系统（如CI/CD工具、项目管理软件等）连接，提高协作流畅度。</p><p>渐进式替代策略：先将新项目及部分试点功能切换到Zoho网盘，待团队熟悉后，再全面废弃旧工具。</p><p>总之，新工具的实施是一个循序渐进的过程，以减少对现有业务操作的干扰。</p>]]></description></item><item>    <title><![CDATA[如何实现 Remote MCP-Serv]]></title>    <link>https://segmentfault.com/a/1190000047413577</link>    <guid>https://segmentfault.com/a/1190000047413577</guid>    <pubDate>2025-11-20 11:07:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>背景</h2><p>对于公司内部的MCP-Server, 由于隐私性问题不能发布为npm包，那么就没法以<code>npx</code>或者<code>uvx</code>等形式快速的共享使用。所以基本会以STDIO类型的MCP-Server进行开发，在内部进行共享时只能将对应源文件拉取本地使用。</p><p>MCP-Server市场中心能够整合内部各团队或个人实现的MCP-Server，提供统一的Streamable HTTP协议访问端口，快速的整合MCP资源。</p><h2>现有方案分析</h2><p>MCP Market有很多网站已经实现，但基本只能做到聚合功能，即只提供检索与使用概览等基本功能，使用时仍需要本地克隆或者 npx 安装调用或者使用MCP注册者自己提供的HTTP调用url。</p><h3>MCPMarket</h3><p><a href="https://link.segmentfault.com/?enc=ahd2RlbLXuoiNi9qsOP%2BXw%3D%3D.jgX5Or7eduB4U2J06mwugJh8ymrPZkyWCueIo0QxROg%3D" rel="nofollow" target="_blank">https://mcpmarket.com/zh/server</a></p><p>仅仅做了检索整合</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413579" alt="image.png" title="image.png"/></p><h3>百炼-MCP</h3><p><a href="https://link.segmentfault.com/?enc=BcmCG5anXMnAbBWdQiQARg%3D%3D.%2BD1f6o4l3DV8%2FbjdTFrOumAGL%2FxGEJknE701x1pfKbMiiowy81F2ul1TwSglBC1u4lbOzDycF9bMcwrhHxVDQQ3hAW7HQU1Xrw6Ye9yLKdOVV8OS2s0U%2Fk8TtvH9ypdQ" rel="nofollow" target="_blank">百炼</a></p><p>实现了MCP服务的一体化流程，包括检索，部署与一键集成</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413580" alt="image (1).png" title="image (1).png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413581" alt="image (2).png" title="image (2).png" loading="lazy"/></p><h3>Nacos-AI</h3><p><a href="https://link.segmentfault.com/?enc=UkcCa0PsoaZLGDZRhEs%2BUg%3D%3D.uhBsvyUqKh13%2FvyOYod%2FqPtR%2BUC09PWMPpIo0AEd9TYnqdeXaEpqpXpVJaYtu3TsLa0wNcPaBiHjXWqb8Kit2A1pFYIokEZIYzE2eU%2Fl78T4msSZIbV3c%2B2ocE2ECIOXexR2umb8Ak4xOgX93wo1mQ%3D%3D" rel="nofollow" target="_blank">Nacos-AI</a></p><p>nacos本身是一个动态服务注册发现与管理框架，新版本提供了Nacos MCP Router来管理所有MCP</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413582" alt="image.png" title="image.png" loading="lazy"/></p><p>其内部可以实现不同协议类型的 MCP 服务器转化为统一的 streamable-http 类型，它本身提供一个 nacos mcp 来与客户端交互：</p><p>Nacos MCP Router 有两种工作模式：</p><ol><li>router模式：默认模式，通过MCP Server推荐、安装及代理其他MCP Server的功能，帮助用户更方便的使用MCP Server服务。</li><li>proxy模式：使用环境变量MODE=proxy指定，通过简单配置可以把sse、stdio协议MCP Server转换为streamableHTTP协议MCP Server。</li></ol><p>在router 模式下，Nacos MCP Router 作为一个标准MCP Server，提供MCP Server推荐、分发、安装及代理其他MCP Server的功能。其主要工具列表为</p><ol><li><p>search_mcp_server</p><ul><li>根据任务描述及关键字从MCP注册中心（Nacos）中搜索相关的MCP Server列表</li><li><p>输入:</p><ul><li><code>task_description</code>(string): 任务描述，示例：今天杭州天气如何</li><li><code>key_words</code>(string): 任务关键字，示例：天气、杭州</li></ul></li><li>输出: list of MCP servers and instructions to complete the task.</li></ul></li><li><p>add_mcp_server</p><ul><li>添加并初始化一个MCP Server，根据Nacos中的配置与该MCP Server建立连接，等待调用。</li><li><p>输入:</p><ul><li><code>mcp_server_name</code>(string): 需要添加的MCP Server名字</li></ul></li><li>输出: MCP Server工具列表及使用方法</li></ul></li><li><p>use_tool</p><ul><li>代理其他MCP Server的工具</li><li><p>输入:</p><ul><li><code>mcp_server_name</code>(string): 被调的目标MCP Server名称.</li><li><code>mcp_tool_name</code>(string): 被调的目标MCP Server的工具名称</li><li><code>params</code>(map): 被调的目标MCP Server的工具的参数</li></ul></li><li>输出: 被调的目标MCP Server的工具的输出结果</li></ul></li></ol><p>在proxy 模式下，Nacos MCP Router 仅提供代理功能，无需代码改动即可实现stdio、sse协议一键转换为streamableHTTP协议。</p><h3>higress AI网关</h3><p><a href="https://link.segmentfault.com/?enc=SLYn3UlsfvTkpZV%2Bl1SGhQ%3D%3D.nH6remtU5Y7pXm1zw5iIaDZ4ELKCndtRQaKZuTKN8PODW4kerqKSh84BK6mkWZAZvd668RjsH8utq5pdVstXEGu%2Fw9SNA2ZngJVGo73fyEo%3D" rel="nofollow" target="_blank">higress AI网关</a></p><p>什么是 AI 网关？</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413583" alt="image (1).png" title="image (1).png" loading="lazy"/></p><p>使用AI网关做MCP Server的统一入口代理</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413584" alt="image (2).png" title="image (2).png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413585" alt="image (3) (1).png" title="image (3) (1).png" loading="lazy"/></p><p>优势：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413586" alt="image (4).png" title="image (4).png" loading="lazy"/></p><h2>设计概览</h2><h3>前端设计</h3><p>页面需包含以下几个功能</p><ol><li>MCP市场 - 用于查询已注册的MCP服务列表，同大多数MCP Market一样，包含使用说明，配置，工具列表等信息<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413587" alt="1.png" title="1.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413588" alt="2.png" title="2.png" loading="lazy"/></li><li>MCP注册中心  - 发布自己实现的MCP，针对 Stdio 类型MCP Server，提供资源部署服务<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413589" alt="3.png" title="3.png" loading="lazy"/></li><li>MCP调试服务  - 使用 <code>@modelcontextprotocol/inspector</code>实现<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413590" alt="4.png" title="4.png" loading="lazy"/></li><li>服务管理<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413591" alt="5.png" title="5.png" loading="lazy"/></li></ol><h3>后端设计</h3><h4>协议转换</h4><p>MCP协议已有三个版本: 2024-11-05、2025-03-26、2025-06-18。</p><p>主要差异在于协议间的兼容，<code>2024-11-05</code>版本提供了<code>Stdio</code>与<code>SSE</code>两种<code>transport</code>类型，<code>2025-03-26</code>之后的版本提供了<code>Stdio</code>与<code>Streamable HTTP</code>协议。</p><p><code>SSE</code>由于其会使用两个端点<code>/sse</code>与<code>/messages</code>且一直会占用长连接的缘故存在设计缺陷，所以我们一般不推荐使用。</p><p>基于此，我们定义三种协议的转化规则： Stdio =&gt; HTTP , SSE =&gt; SSE, HTTP =&gt; HTTP</p><h5>Stdio 转 HTTP</h5><p>大部分情况下我们会以<code>Stdio</code>形式开发MCP Server，因为它足够简单，<code>Stdio</code>transport是以<strong>子进程</strong>的方式实现 Client 与 Server 之间通信的，实现一对一的关系。</p><p>一般有两种使用方式, 在配置时使用 command 指定：</p><ul><li>npx -y xxx</li><li>node ./xxx.js</li></ul><p>我们在任务管理器中可以看到所有配置的MCP都被以子进程的形式启动了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413592" alt="6.png" title="6.png" loading="lazy"/></p><p>问：</p><p>假设一个MCP只会在全局创建一个进程，那么如果我开启两个Cursor，同时调用同一个MCP，会如何？</p><p>如果将其转为HTTP, 需要做到数据隔离。因为StdioClientTransport与运行Server代码的子进程间是通过事件订阅来接收消息的， 接收到消息时我们需要知道这个消息该回传给哪个http请求。</p><pre><code class="typescript">// 接收消息
process.stdout?.on('data', (buffer) =&gt; {});
// 发送请求
process.stdin?.write(msg)</code></pre><p>为了解决数据隔离问题，用两种方式解决：</p><ol><li><strong>独立child_process</strong><br/>为每一个http请求独立创建子进程，消息响应完即销毁，虽能解决数据隔离问题，且每个请求都能定义自己的env信息，但是在20\~30并发下，创建子进程的性能消耗会瞬间打满CPU。</li><li><strong>单体child_process</strong><br/>单体child_process对于每一个MCP Server只会创建一个子进程，且进程会随哆啦A梦一起启动。</li></ol><p>由于不管是哪种 transport，都是以 JSON-RPC 协议进行消息定义，JSON-RPC可以提供 id 参数标识请求，如果请求中携带了id，那么响应的 JSON-RPC 也必须携带同样的id。</p><p>client在请求时会自己生成一个id，但这个id我们并不能直接使用，如果同时有多个用户在 client 端发送请求，它们的id可能会一样，因此我们需要内部维护一个唯一的id，同时维护内部id与client生成的id的关系。</p><p>在与Server通信时，使用内部id以保证唯一，当Server返回数据时，将JSON-RPC响应体的内部id替换为client id以此实现数据隔离。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413593" alt="1.png" title="1.png" loading="lazy"/></p><h5>SSE</h5><p>SSE通信流程：</p><ol><li>SSE transport会开启两个端点，首先客户端会发起一个<code>GET</code>请求到<code>/sse</code>端点，可以不携带任何请求参数</li><li><code>/sse</code>端点开启一个流，先返回一个消息端点<code>/messages</code>与<code>sessionId</code>如：/messages?sessionId=xxx<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413594" alt="2.png" title="2.png" loading="lazy"/></li><li>客户端向<code>/messages</code>端点发起实际请求，如 initialize, tools/list，注意此时服务端不会把数据结果支持返回到<code>/messages</code>，而是直接返回202 Accept<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413595" alt="3.png" title="3.png" loading="lazy"/></li><li><code>/sse</code>端点收到响应结果<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413596" alt="4.png" title="4.png" loading="lazy"/></li><li>继续下一轮通信<br/>对于SSE,  我们在Controller层同样开放两个端点</li></ol><pre><code class="javascript">app.post('/mcp-endpoint/:serverId/messages', app.controller.mcp.handleMCPEndpointPost);
app.get('/mcp-endpoint/:serverId/sse', app.controller.mcp.handleMCPEndpointGet);</code></pre><p>如果单纯的做代理转发，那么无法做到对<strong>消息端点</strong>的转发，因为消息端点是MCP Server动态定义的，可能不是<code>/messages</code>，所以我们需要在哆啦A梦层动态创建 <code>SSEServerTransport</code> 。</p><p><code>SSEServerTransport</code>的<code>endpoint</code>仅仅用来指引客户端消息体往哪个API接口发送, 所以我们在哆啦A梦后端创建的<code>endpoint</code>与真实MCP服务器返回的<code>endpoint</code>是没有关系与冲突的。</p><pre><code class="javascript">const messageEndpoint = 'http://localhost:7001/mcp-endpoint/' + serverId + '/messages';

const serverTransport = new SSEServerTransport(messageEndpoint, res, {
    headers,
});
await serverTransport.start();</code></pre><p>我们在哆啦A梦创建的<code>SSEServerTransport</code>能够帮我们处理客户端发送的SSE请求并自动响应，但是我们目前没有绑定任何的MCP Server, 它仅有一个桥接的作用。</p><p>因此，我们还需要在哆啦A梦层创建一个<code>SSEClientTransport</code>, 将我们从真实客户端发送的消息通过<code>SSEClientTransport</code>再发送给真实MCP服务器。</p><p><code>SSEClientTransport</code>监听<code>onmessage</code>获取真实MCP服务器消息结果后，将结果 send 到本地创建的<code>SSEServerTransport</code>，<code>SSEServerTransport</code>处理请求完成响应。</p><p>GET请求处理如下：</p><pre><code class="javascript">/**
     * 处理GET请求
     * @param {string} serverId - 服务器ID
     * @param {any} req - 请求数据
     * @param {object} res - 响应数据
     */
async handleGet(serverId, req, res) {
  const sseProxy = this.sseProxies.get(serverId);
  if (!sseProxy) {
    throw new Error(`服务器 ${serverId} 的SSE连接未找到`);
  }

  let transportToClientClosed = false;
  let transportToServerClosed = false;

  const headers = {
    ...req.headers,
    ...sseProxy.headers,
    accept: 'text/event-stream',
  };

  const clientTransport = new SSEClientTransport(new URL(sseProxy.url), {
    headers,
  });
  await clientTransport.start();

  const messageEndpoint = 'http://localhost:7001/mcp-endpoint/' + serverId + '/messages';
  const serverTransport = new SSEServerTransport(messageEndpoint, res, {
    headers,
  });
  await serverTransport.start();

  this.serverTransports.set(serverTransport.sessionId, serverTransport);
  this.clientTransports.set(serverTransport.sessionId, clientTransport);

  // 桥接数据交换
  serverTransport.onmessage = (message) =&gt; clientTransport.send(message);
  clientTransport.onmessage = (message) =&gt; serverTransport.send(message);
  serverTransport.onclose = () =&gt; {
    if (transportToServerClosed) {
      return;
    }

    transportToClientClosed = true;
    clientTransport.close().catch((e) =&gt; console.log(e));
  };
  clientTransport.onclose = () =&gt; {
    if (transportToClientClosed) {
      return;
    }
    transportToServerClosed = true;
    serverTransport.close().catch((e) =&gt; console.log(e));
  };

}</code></pre><p>POST消息请求处理如下：</p><pre><code class="javascript">/**
     * 处理POST请求
     * @param {string} serverId - 服务器ID
     * @param {any} req - 请求数据
     * @param {object} res - 响应数据
     */
async handlePost(serverId, req, res) {
  const sseProxy = this.sseProxies.get(serverId);
  if (!sseProxy) {
    throw new Error(`服务器 ${serverId} 的SSE连接未找到`);
  }

  const serverTransport = this.serverTransports.get(req.query.sessionId);

  if (!serverTransport) {
    throw new Error(`session ${req.query.sessionId} 不存在`);
  }

  try {
    // 由于Egg.js已经解析了请求体，我们需要将解析好的body传递给handlePostMessage
    await serverTransport.handlePostMessage(req, res, req.body);
  } catch (error) {
    console.error(`SSE POST请求处理错误:`, error);
    throw error;
  }
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413597" alt="5.png" title="5.png" loading="lazy"/></p><h4>Streamable HTTP</h4><p>如果本身就是Streamable HTTP类型的，我们只做请求的代理，将目标服务器或者托管在哆啦A梦不同端口上的MCP服务，统一通过哆啦A梦mcp端点访问。</p><p>代理实现与SSE相似，此处略。</p><p>但注意Streamable HTTP使用一个<code>/mcp</code>端点，但是可以使用3种方式访问：POST、GET、DELETE。</p><h4>Stateless VS Statefulness</h4><p>Streamable HTTP MCP在定义时存在无状态与有状态模式的区分：</p><pre><code class="typescript">// 有状态
transport = new StreamableHTTPServerTransport({
  sessionIdGenerator: () =&gt; sessionId,
  onsessioninitialized: (id) =&gt; {
    console.log(`新会话初始化: ${id}`);
  }
});

// 无状态
transport = new StreamableHTTPServerTransport({
  sessionIdGenerator: undefined,
});</code></pre><p>在无状态模式下，不会返回<code>MCP-Session-Id</code>响应头，客户端也无法访问<code>GET</code>与<code>DELETE</code>方法，客户端不需要先行通过<code>initialize</code>进行握手连接，可以直接调用目标方法，缺点就是服务器端没法进行实时通知，如<code>ToolListChanged</code>等消息。</p><p>Stdio本身是以一对一连接存在的，且会长期维持子进程的存在，所以属于<code>statefulness</code>这一类, 当转化为Streamable HTTP时，需要我们自行管理session（当前暂时处理为stateless）；</p><p>SSE肯定是以<code>Statefulness</code>存在，我们会在后端存储代理的session信息。</p><p>Streamable HTTP 我们走完全代理模式，是否需要状态由目标MCP服务器决定。</p><h3>数据库设计</h3><p>采用单表设计，不需要额外关联表</p><pre><code class="sql">CREATE TABLE IF NOT EXISTS `mcp_servers` (
    `id` int NOT NULL AUTO_INCREMENT COMMENT '自增主键',
    `server_id` varchar(64) NOT NULL COMMENT '服务器唯一标识/名称',
    `title` varchar(200)  NOT NULL COMMENT '显示标题',
    `description` text  COMMENT '服务器描述(支持Markdown)',
    `author` varchar(100)  NOT NULL COMMENT '创建者',
    `version` varchar(20)  NOT NULL COMMENT '版本号',
    `tags` json COMMENT '标签数组',
    `transport` enum('stdio', 'sse', 'streamable-http')  NOT NULL DEFAULT 'stdio' COMMENT '传输协议类型',
    `command` varchar(255)  COMMENT '启动命令(stdio类型)',
    `args` json COMMENT '命令参数数组(stdio类型)',
    `env` json COMMENT '环境变量对象(stdio类型)',
    `http_url` varchar(500)  COMMENT 'HTTP访问地址(http类型)',
    `sse_url` varchar(500)  COMMENT 'SSE访问地址(sse类型)',
    `git_url` varchar(500)  COMMENT 'Git源码地址',
    `deploy_path` varchar(500)  COMMENT '托管部署路径',
    `status` tinyint NOT NULL DEFAULT '1' COMMENT '服务器状态 1-启用 0-禁用',
    `is_delete` tinyint NOT NULL DEFAULT '0' COMMENT '是否删除 1-已删除 0-未删除',
    `use_count` int NOT NULL DEFAULT '0' COMMENT '使用次数',
    `tools` json COMMENT '可用工具列表',
    `prompts` json COMMENT '可用提示词列表',
    `resources` json COMMENT '可用资源列表',
    `capabilities` json COMMENT '服务器能力信息',
    `last_sync_at` datetime COMMENT '最后同步时间',
    `runtime_status` enum('running', 'stopped', 'error', 'unknown') NOT NULL DEFAULT 'unknown' COMMENT '运行时状态 running-运行中 stopped-已停止 error-错误 unknown-未知',
    `last_ping_at` datetime COMMENT '最后ping检查时间',
    `ping_error` text COMMENT '最后ping检查错误信息',
    `created_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `updated_at` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    UNIQUE KEY `uk_server_id` (`server_id`),
  ) ENGINE = InnoDB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8 COLLATE = utf8_bin</code></pre><h3>控制层</h3><p>支持 SSE 与 Streamable HTTP端点</p><pre><code class="javascript">// MCP代理端点
app.post('/mcp-endpoint/:serverId/mcp', app.controller.mcp.handleMCPEndpointPost);
app.get('/mcp-endpoint/:serverId/mcp', app.controller.mcp.handleMCPEndpointGet);
app.delete('/mcp-endpoint/:serverId/mcp', app.controller.mcp.handleMCPEndpointDelete);
app.post('/mcp-endpoint/:serverId/messages', app.controller.mcp.handleMCPEndpointPost);
app.get('/mcp-endpoint/:serverId/sse', app.controller.mcp.handleMCPEndpointGet);</code></pre><h3>服务层</h3><h4>MCPService 层</h4><p>MCPService层包含所有数据库模型操作，但不包含具体的MCP处理，如MCP启动，重启。</p><pre><code class="javascript">const mcpProxy = MCPProxy.getInstance();

class MCPController extends Controller {
    async handleMCPEndpointGet() {
      const { ctx } = this;
      const { serverId } = ctx.params;
      await mcpProxy.forwardRequest(serverId, ctx.request, ctx.response);
      ctx.respond = false;
  }
}</code></pre><h4>MCPProxy 层</h4><p>MCPProxy层包含以下组成</p><ol><li>MCPProxy (核心代理)<br/>单例模式，统一管理所有MCP服务器连接<br/>路由请求到对应的传输处理器<br/>管理服务器生命周期（启动/停止/重启）</li></ol><pre><code class="javascript">class MCPProxy {

  constructor(){
    this.requestManager = new MCPRequestManager();
    this.processManager = new MCPProcessManager();
    this.stdioHandler = new StdioTransportHandler(this.processManager, this.requestManager);
    this.sseHandler = new SSETransportHandler();
    this.httpHandler = new StreamableHttpTransportHandler();
  }
  // 单例实现
  getInstance()

  // 缓存MCP Server配置与forward具体的协议转换处理
  async startProxy(serverId, config) {
    try {
      // 停止已存在的代理
      await this.stopProxy(serverId);

      const transport = config.transport || { type: 'stdio' };
      let handler;

      switch (transport.type) {
        case 'stdio':
          handler = this.stdioHandler;
          break;
        case 'sse':
          handler = this.sseHandler;
          break;
        case 'streamable-http':
          handler = this.httpHandler;
          break;
        default:
          throw new Error(`不支持的传输类型: ${transport.type}`);
      }

      // 启动传输处理器
      await handler.start(serverId, config);

      // 记录使用的处理器
      this.transportHandlers.set(serverId, handler);

      console.log(`MCP代理已启动: ${serverId} (${transport.type})`);
    } catch (error) {
      console.error(`启动MCP代理失败 [${serverId}]:`, error);
      throw error;
    }
  }
}</code></pre><ol start="2"><li>MCPRequestManager (请求管理器)<br/>管理待处理请求队列<br/>ID映射管理（客户端ID ↔ 内部ID）<br/>请求超时处理<br/>响应路由回调</li><li>MCPProcessManager (STDIO transport进程管理器)<br/>进程生命周期管理<br/>进程通信（stdin/stdout）<br/>进程监控和错误处理<br/>进程重启机制</li><li>MCPSessionManager (会话管理器)<br/>SSE会话生命周期管理<br/>会话超时清理<br/>协议版本管理<br/>会话统计</li><li>传输处理器 (Transport Handlers 核心处理)<br/>整个协议转换与具体的处理</li></ol><pre><code class="javascript">abstract class BaseTransportHandler {
    start();
    stop();
    // 为MCPProxy提供统一的入口
    forward();
    handlePost();
    handleGet();
}

class StdioTransportHandler extends BaseTransportHandler{}

class SSETransportHandler extends BaseTransportHandler{}

class StreamableHttpTransportHandler extends BaseTransportHandler{}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413598" alt="6.png" title="6.png" loading="lazy"/></p><h3>状态监控</h3><p>MCP Server注册后是否可用，可以使用使用ClientTransport发送一个ping请求来识别，数据库中使用<code>status</code>、<code>last_ping_at</code>、<code>ping_error</code>三个字段来存储相关信息。</p><p><a href="https://link.segmentfault.com/?enc=01It4RKTOHlP0L3h4OAKIw%3D%3D.4BfFXYHOOJIDqPkNWktNwb%2F3zXprSKrz94RqlqZykqrcIStOf2M2FMXpKNd0FMjETmslgmyH5EbDxCbasbz5%2BCWSvpPTD8m0mbvoGbvbtNA%3D" rel="nofollow" target="_blank">https://modelcontextprotocol.io/specification/2025-06-18/basic/utilities/ping</a></p><p>当在注册完成、修改、启动、重启等操作后，应立即调用一次。</p><p>除此之外，创建定时任务，每隔一段时间发送一次ping请求。</p><h3>Inspector集成</h3><p>提供一个在线调试工具尤为重要，那快速的帮助使用者或者服务器拥有者排查问题。</p><p><a href="https://link.segmentfault.com/?enc=fI5IS4wgA%2BcPI7BUW8ipXA%3D%3D.5dI%2FkleiHip%2B949AcmxUBUJ1ObIiXDtAM8HHPnIEsR9uHEBhty4ESVyOau2FwFd2IP3NfvIw21RRuielEmDveA%3D%3D" rel="nofollow" target="_blank">@modelcontextprotocal/inspector</a> 是一个mcp官方提供的web端调试工具<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413599" alt="7.png" title="7.png" loading="lazy"/></p><p>其实现代码分为前端页面与server端，server端用于跟真实服务器通信。</p><p>前端使用vite与radix-ui，后端服务采用exporess</p><p>如果要集成进哆啦A梦，考虑三种方案：</p><ol><li>微服务，将inspector直接按源码级迁移到哆啦A梦中并开放两个新端口，哆啦A梦启动时连带启动inspector前后端服务，需要考虑如何实现打包</li><li>合并迁移，inspector的代码实现并不复杂，可以考虑与现有哆啦A梦代码整合合并</li><li>独立服务，将inspector以iframe的形式嵌入哆啦A梦，需要在哆啦A梦服务器上单独启动inspector, 割裂感很重。</li></ol><h3>环境要求</h3><p>@modelcontextprotocal/sdk 要求 node 版本v18以上，哆啦A梦 engines.node 要求&gt;=14 &amp; &lt;=16, 使用 node &gt;=18启动哆啦A梦需要添加环境变量<code>NODE_OPTIONS=--openssl-legacy-provider &amp;&amp; egg-bin dev --daemon</code></p><h3>多进程集群适配</h3><p>egg.js在启动时会通过 master 进程带起所有其他子进程，每个worker用来执行我们的实际业务代码（mvc框架），类似于nginx的负载均衡，此外还会创建一个 agent 进程来做单独的服务，如定时任务。</p><p><a href="https://link.segmentfault.com/?enc=4C3ClRpw93JaZpxjWmq6TQ%3D%3D.49UPfXtG8OxotMwetq3wHj3CsBWwrfn6%2FEmTmGComwb0p6MQveJqrAFYeuA%2BLPSDhyZs6rwH7B6sb9Ot0IVNlbxL1bgb1hz4w4OHS4vGEw0%3D" rel="nofollow" target="_blank">cluster-and-ipc</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413600" alt="8.png" title="8.png" loading="lazy"/></p><p>由于doraemon在生产环境下会开启多进程集群来优化性能，每个worker都会重复实例化我们设计的单例MCPProxy，导致不同worker间的mcp相关状态不一致，mcp-session状态也无法共享。</p><p>所以需要在 controller 和 service 中，我们需要移除所有直接对 MCPProxy 的操作，为确保实例唯一与状态同步，将MCP底层服务全部移到 agent 进程中。</p><p>MCPProxy的主要功能是转发 MCP 的直接请求，因此我们需要在 agent 中开启一个独立的 http 服务（不能利用Controller）</p><pre><code class="typescript">// agent.js文件

// 初始化MCP HTTP处理器
    const mcpHttpHandler = new MCPHttpHandler(agent.logger);
    let httpServer = null;

    // 创建HTTP服务（用于处理MCP端点请求）
    const startHttpServer = () =&gt; {
        const port = process.env.MCP_AGENT_HTTP_PORT || env.mcpAgentHttpPort || 7005;

        httpServer = http.createServer(async (req, res) =&gt; {
            await mcpHttpHandler.handleRequest(req, res);
        });

        httpServer.listen(port, () =&gt; {
            agent.logger.info(`Agent MCP HTTP服务已启动，监听端口: ${port}`);
        });

        httpServer.on('error', (error) =&gt; {
            agent.logger.error('Agent MCP HTTP服务错误:', error);
        });
    };</code></pre><p>其次 MCPProxy 还包含 MCP 服务的启停, 而 MCP 启停的时机是通过 worker 中 service 层调用的，因此还需要通过IPC（进程间通信）的方式通知 agent。</p><pre><code class="typescript">// 一个worker的service中
this.app.messenger.sendToAgent('mcpRestart', { serverId });


// agent.js
agent.messenger.on('mcpRestart', async ({ serverId }) =&gt; {
    try {
        await McpProxy.getInstance().restartProxy(serverId);
        agent.logger.info(`MCP服务器重启成功 [${serverId}]`);
    } catch (error) {
        agent.logger.error(`MCP服务器重启失败 [${serverId}]:`, error);
    }
});</code></pre><h3>MCP Server代码编写约束</h3><p>MCP Server中的<code>Server</code>与<code>Transport</code>和<code>Client</code>是一对一对一的关系，也就是同时只能一个人访问你的MCP服务，这对于我们实现Remote-MCP的初衷是不符合的。</p><p>即使是新的 transport 协议 streamable-http，也是一对一的关系，当我们在一个tab中调用后，在另一个tab中再次连接调用请求会直接卡住。</p><p>官方SDK提供的使用案例中目前存在该问题, Server定义在了外部，导致没法对每个请求分配独立的Transport<br/><a href="https://link.segmentfault.com/?enc=3OAD8yrPFMuoKIcpzE%2BH%2FA%3D%3D.0LPcjHnHBj5IZ8MLe8cNxMWrJenAl2Xfx3B1E5uAQiVkZJv2RALBRJXePVmKYK7WcT3HYbfqN0h8Hbw7ziTZfg%3D%3D" rel="nofollow" target="_blank">modelcontextprotocol/typescript-sdk</a></p><pre><code class="typescript">// Create an MCP server
const server = new McpServer({
    name: 'demo-server',
    version: '1.0.0'
});

// Set up Express and HTTP transport
const app = express();
app.use(express.json());

app.post('/mcp', async (req, res) =&gt; {
    // Create a new transport for each request to prevent request ID collisions
    const transport = new StreamableHTTPServerTransport({
        sessionIdGenerator: undefined,
        enableJsonResponse: true
    });

    res.on('close', () =&gt; {
        transport.close();
    });

    await server.connect(transport);
    await transport.handleRequest(req, res, req.body);
});

const port = parseInt(process.env.PORT || '3000');
app.listen(port, () =&gt; {
    console.log(`Demo MCP Server running on http://localhost:${port}/mcp`);
}).on('error', error =&gt; {
    console.error('Server error:', error);
    process.exit(1);
});</code></pre><p>因此，针对 streamable-http 与 sse 类型，需要将Server的创建放在请求处理中：</p><pre><code class="typescript">// 把Server的定义代码封装起来
const createServer = () =&gt; { return new McpServer() }

app.post('/mcp', async (req, res) =&gt; {
    const server = createServer();
    // Create a new transport for each request to prevent request ID collisions
    const transport = new StreamableHTTPServerTransport({
        sessionIdGenerator: undefined,
        enableJsonResponse: true
    });

    res.on('close', () =&gt; {
        transport.close();
    });

    await server.connect(transport);
    await transport.handleRequest(req, res, req.body);
});</code></pre><h2>Next</h2><ul><li>一键转化API请求接口 =&gt; Streamable HTTP MCP</li><li>弹性启动Stdio进程</li></ul>]]></description></item><item>    <title><![CDATA[精确的动态联动闭环医疗数据库审计和监测方]]></title>    <link>https://segmentfault.com/a/1190000047413637</link>    <guid>https://segmentfault.com/a/1190000047413637</guid>    <pubDate>2025-11-20 11:06:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>概要：<br/>（提示：本章节概述方案整体价值，强调数据化落地与可视化管理成果。）</p><pre><code>   在医疗行业数字化和智能化进程加速的背景下，医疗数据库安全面临前所未有的挑战。患者身份信息、病历记录、影像数据、检验结果、处方信息及支付交易数据等构成了医疗机构最核心的数据资产，其敏感性和价值均极高。一旦泄露或被篡改，不仅可能引发严重的隐私和法律风险，也会影响医院的运营和公众信任。针对这一行业痛点，全知科技提出了“[知形-数据库风险监测系统](https://jsj.top/f/CuRr3f)”，通过旁路镜像采集、深度协议解析、AI行为建模和自动化处置，实现数据库全链路实时监测、异常行为识别及风险闭环管理。该方案已在多家三甲医院落地，覆盖数百个数据库实例、上千名高权限用户，实现资产可视化、敏感数据分级、异常行为秒级告警及合规审计自动化，为医疗机构提供了切实可行的数据安全保障。</code></pre><p>一、医疗数据库精确监测与动态管理难题<br/>（提示：解析行业现状与数据库安全管理痛点，凸显方案必要性。）<br/>在医疗数字化快速发展的背景下，医院数据库面临的安全挑战不仅数量增加，更呈现出“高敏感、高动态、高联动”的特征：</p><ol><li>数据库管理碎片化导致精确监控困难：医院内部HIS、LIS、EMR、PACS等系统独立部署，数据库类型和版本繁多，权限结构复杂。传统静态日志难以实现对敏感字段和高风险操作的精确识别，导致潜在违规行为难以被及时发现。</li><li>实时与动态监测能力不足：医疗数据库访问频率高且访问模式多变。医生、科研人员及第三方运维操作频繁，且操作时间、操作类型、访问对象存在高度动态性。传统审计模式只能事后分析，无法形成动态闭环的风险防护体系。</li><li>内部风险与跨系统联动难以管控：高权限用户跨系统访问数据时，异常操作可能在多个系统间产生联动效应。缺乏跨系统动态关联分析，将导致违规访问和数据泄露难以及时溯源。</li><li>合规压力下闭环审计需求突出：等级保护与医疗数据管理法规要求“全过程留痕、可追溯、可追责”，强调闭环管理和精确审计。医院面临合规压力时，如果缺乏动态联动的闭环监测机制，将难以满足监管检查要求。<br/>二、精确识别与动态联动风险评估<br/>（提示：从数据库运维、用户行为及系统漏洞三方面分析潜在风险。）<br/>1.精确风险识别不足：数据库中存在大量敏感字段（病历号、影像文件路径、处方信息等），传统审计无法实现字段级精确监控。高权限用户的操作行为多样，无法精确判断异常操作，增加误报和漏报风险。<br/>2.动态访问行为带来的潜在风险：医生和科研人员的访问存在强时间和场景动态性，如夜间批量导出病历、跨系统数据调用等。外部接口调用和第三方运维操作增加了数据库动态风险，传统静态规则无法覆盖。<br/>3.跨系统联动风险：不同业务系统间的数据访问存在交叉与联动效应，一旦出现越权访问或异常操作，可能引发多系统风险放大。缺乏实时关联分析与联动处置机制，将导致安全事件无法形成闭环管理。<br/>4.合规审计风险：法规要求全过程、可追溯、可验证。若审计仅依赖静态日志分析，难以形成动态闭环追踪链路。<br/>三、精确动态闭环的数据库监测系统<br/>（提示：介绍知形-数据库监测系统的核心技术架构。）<br/>针对医疗行业的数据库安全管理痛点，知形-数据库风险监测系统构建了“采集—解析—分析—处置”的完整安全防护体系。</li><li>采集层—动态流量捕获<br/> ○ 采用旁路镜像、日志对接及API采集，实现对数据库操作流量的动态捕获，无需改动业务系统。<br/> ○ 支持Oracle、MySQL、SQL Server、达梦、人大金仓、PostgreSQL等多类型数据库，适配本地机房及云端环境，形成动态全覆盖采集。</li><li>解析层—精确字段识别<br/> ○ 基于深度协议解析与医疗语义识别，实现对病历号、影像路径、检验编号等敏感字段的精确识别。<br/> ○ 动态解析SQL、存储过程、加密流量，支持跨系统操作追踪。</li><li>分析层—AI驱动的动态异常识别<br/> ○ 建立角色行为基线，结合机器学习实现动态监测，精确识别批量导出、越权访问、非工作时段访问等异常行为。<br/> ○ 支持跨系统联动分析，秒级生成告警，形成闭环的风险识别机制。</li><li>处置层—闭环联动响应<br/> ○ 多级风险响应机制，包括告警推送、防御封禁、自动工单派发，实现从发现到处置的闭环。<br/> ○ 可与医院安全运维平台、SIEM系统及身份访问管理系统联动，确保动态风险在多系统间得到闭环管控。</li><li><p>展示层—可视化与可追溯<br/> ○ 安全态势大屏、风险分析报表及合规审计报告，动态呈现数据库操作、异常事件及联动响应流程。<br/> ○ 支持全过程留痕，形成精确可追溯闭环，满足法规要求。<br/>四、落地实践中的精确监测与闭环效能<br/>（提示：通过实际案例展示系统落地效果与数据化成果。）</p><pre><code>在某省级三甲医院的落地实践中，知形-数据库风险监测系统充分展现了精确、动态与闭环联动的能力。医院拥有200多个业务系统、800多名高权限用户，数据库总规模超过150TB，长期面临跨系统访问复杂、敏感数据难以精确监控、合规审计周期长等问题。系统上线后，通过旁路镜像采集与深度协议解析，快速完成全量资产识别和敏感字段映射，实现对病历、影像、检验和处方等核心数据的字段级精确监控。结合AI行为建模，系统动态分析用户操作行为，秒级发现越权访问、批量导出及非工作时段访问等异常行为，并通过告警联动、防御封禁和自动工单派发形成完整闭环处置流程。项目首月，系统检测异常访问412次，越权访问26次，非工作时段访问78次，平均告警响应时间仅3秒，误报率控制在3.7%，每月合规审计报表生成时间从3天缩短至2小时。
通过这一套精确、动态、联动闭环的数据库风险监测系统，医院实现了数据库安全的全链路可视化、可控化和可追溯化，风险发现率提升三倍，内部管理效率和合规审计效率显著增强，同时有效保护了患者敏感数据，为医疗信息化安全管理提供了可量化成果和实践样本。</code></pre><p>五、可复制的精确动态闭环治理模式<br/>（提示：分析该系统在行业内的可复制性及长期价值。）</p><pre><code>该系统在医疗行业推广价值显著，首先体现为精确性，通过字段级敏感数据识别、跨系统异常操作分析，实现对数据库资产和风险的精确量化和可视化管理。其次体现为动态性，系统可实时采集数据库操作流量并进行AI驱动的行为分析，针对多角色、多系统、多时间段的访问操作实现秒级监测和异常告警，从而形成动态闭环管理。再次体现为联动闭环能力，告警、处置、审计形成完整闭环流程，能够跨系统自动触发防御措施、工单派发及追踪审计，实现从发现、响应到验证的全链路管理。通过这一套闭环体系，医院不仅提升了风险响应效率和合规治理能力，减少了人工审计成本，同时在保护患者敏感数据和支持科研、临床业务安全方面均取得显著成效。由于系统兼容多类型数据库、多业务系统及云端环境，其经验和技术模式可在不同规模医疗机构中复制推广，为行业提供可落地、可持续、可量化的数据库安全治理实践样本。</code></pre><p>六、围绕精确、动态与闭环的核心解答<br/>（提示：围绕系统核心功能及应用价值解答典型问题。）<br/>Q1：系统如何实现对敏感数据的精确监测？A1：系统通过深度协议解析与医疗语义识别技术，实现对病历号、影像路径、检验编号、处方信息等核心字段的精确识别。结合资产自动识别功能，可实时更新数据库结构和敏感字段状态，从而保证监测数据的精确性和动态可用性，实现字段级的闭环安全管理。<br/>Q2：动态行为监测是如何工作的？A2：系统利用AI行为建模和机器学习算法，为不同角色（如医生、护士、科研人员）建立访问基线。基于此基线，系统可动态分析用户操作行为，秒级发现越权访问、批量导出及非工作时段异常访问，实现对操作行为的动态闭环监控，确保风险在发生初期即可被发现并处理。<br/>Q3：如何处理跨系统异常访问，实现联动闭环？A3：系统支持多系统联动分析，将不同业务系统的访问操作统一纳入监控视图。异常行为触发后，系统可自动执行告警推送、防御封禁、工单派发等措施，实现跨系统的闭环处置，确保风险事件在不同系统间得到精确追踪和动态响应。<br/>Q4：合规审计如何在闭环中得到保证？A4：系统全过程记录“人—数据—操作—时间”链路，自动生成符合法规要求的审计报告。结合动态监测和联动处置功能，每一次异常行为都可在闭环中被追溯、验证和处置，实现精确、动态、可追踪的合规审计体系。<br/>Q5：该方案在大规模医院环境下的可扩展性如何？A5：系统采用分布式架构，可覆盖数百数据库实例、上千高权限用户，支持跨系统、跨角色的动态监控与闭环联动。无论在高并发操作、海量日志分析还是多系统协同处置场景下，均能保持精确识别、动态监测和闭环处置能力，为大型医院提供可持续、可复制的数据库安全管理实践。<br/>七、用户视角下的精确动态闭环实践价值<br/>(提示：展示医疗机构反馈与方案实际应用成效。)</p><pre><code>在多家三甲医院的落地实践中，医疗机构的IT管理团队和信息安全负责人对“知形-数据库风险监测系统”给予高度评价：</code></pre><p>● 精确性获得高度认可：医院信息中心负责人表示，通过字段级敏感数据识别和深度协议解析，系统能够精确识别病历号、影像文件路径、检验编号、处方信息等核心敏感字段，极大降低了传统静态日志误报和漏报的问题，使数据库风险管理更具针对性和可量化性。<br/>● 动态监测能力显著提升：安全运维团队指出，系统可实时分析高权限用户及跨系统访问行为，秒级生成异常告警。特别是在科研数据跨系统访问和夜间批量操作场景中，动态监控能力使医院能够即时发现风险，并进行有效处置，确保敏感数据不被滥用。<br/>● 联动闭环效果突出：医院CISO反馈，通过系统告警联动、防御封禁、工单派发和自动化审计，形成从发现、响应到验证的闭环管理流程，实现跨系统异常行为的集中管控，极大增强了数据库风险的可控性和可追溯性。<br/>● 落地成效显著：医院信息安全部门表示，自系统上线后，异常访问事件响应时间由数小时缩短至秒级，误报率控制在3~5%以内，每月合规审计报表生成周期由3天缩短至2小时，运维工作效率明显提升。同时，患者敏感数据的保护能力显著增强，为医院临床、科研及管理业务提供了安全支撑。<br/>● 整体满意度高，具备可推广价值：多家医院认为，该方案兼具技术精确性、动态响应能力与联动闭环处置特性，不仅解决了长期存在的数据库安全痛点，还为医院构建了可持续、可复制的数据库安全治理体系，具备在不同规模医疗机构推广的可行性和示范价值。</p><pre><code>随着医疗行业数据量的持续爆发和业务系统的日益复杂，数据库已成为医疗信息化的核心承载平台，其安全性直接关系到患者隐私、科研数据完整性及医院治理效率。在数字经济快速发展的背景下，数据已成为企业核心资产，而数据库则是支撑业务运作和信息存储的关键环节。可靠的数据库安全解决方案成为网络安全市场的重要驱动力。全知科技作为国内领先的专精数据安全厂商，多年来一直专注于数据安全领域的探索与研究，凭借在数据库安全领域的创新实践和领先技术，获得了业内广泛认可。公司多次荣获中国信通院、工信部、IDC等权威机构的肯定，并多次入选信通院牵头的《网络安全产品技术全景图》、数据库安全代表厂商及优秀产品解决方案等。这不仅彰显了全知科技在技术创新与行业规范建设上的领先地位，更充分印证了公司在行业中的技术实力与前瞻性。未来的数据库安全管理将不仅关注数据防护本身，更强调风险可视化、动态响应和闭环处置能力，形成可持续、可复制的安全治理体系，为医疗数字化转型和智慧医疗建设提供坚实底座。</code></pre></li></ol>]]></description></item><item>    <title><![CDATA[AI赋能数据分类分级，迈向智能化数据治理]]></title>    <link>https://segmentfault.com/a/1190000047413640</link>    <guid>https://segmentfault.com/a/1190000047413640</guid>    <pubDate>2025-11-20 11:06:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着《数据安全法》《个人信息保护法》《网络数据安全管理条例》等制度体系不断完善，数据分类分级已从单一监管要求演进为企业构建数据安全能力体系的基础工程。然而在落地过程中，企业仍面临诸多问题。<br/>数据识别效率和准确度普遍偏低，许多业务系统缺乏统一命名规则和清晰注释，字段语义模糊、语境依赖强，导致自动化识别难以准确理解业务含义，最终使分类分级结果难以满足治理和合规需求。<br/>在运营端，业务高速变化、数据持续涌动，传统依赖人工维护的模式无法实现实时更新，导致分类分级结果快速失效，体系老化严重，既无法支撑动态风险管理，也难以形成闭环、可持续的数据治理能力。<br/><img width="723" height="298" referrerpolicy="no-referrer" src="/img/bVdm6Ca" alt="" title=""/><br/> 面对上述问题，全知科技针对性地推出了知源-AI 数据分类分级系统，该系统可以实现对海量数据的快速发现、精准识别与动态管理。不仅支持企业在复杂业务环境下落地实施，还大大提高了数据识别效率和准确率。 </p><h2>知源-AI数据分类分级系统：让分类分级更精准、更动态</h2><p>全知科技AI数据分类分级是一款以数据为核心，面向企业数据发掘并进行AI自动化数据分类分级产品。系统致力于梳理并标识数据，实现在复杂环境下自动化扫描客户数据并识别其包含的相关业务信息，并借助AI大模型实现自动化分类分级。智能化扫描打标完成后会生成一份完整的分析报告并通过可视化的方案呈现，整个发现过程同时实现了数据口径统一、数据管理依据的建立。<br/><img width="723" height="313" referrerpolicy="no-referrer" src="/img/bVdm6Cc" alt="" title="" loading="lazy"/></p><h3>1.核心功能</h3><h4>▸智能化的数据资产发现与可扩展扫描能力</h4><p>系统具备高度兼容的数据服务发现与扫描能力，能够自动识别企业内部多种数据库及数据服务，构建完整清晰的数据资产清单。面对复杂、多源的数据环境，用户只需配置扫描范围即可完成资产发现。对于暂未适配的数据库类型，也可通过上传官方驱动快速扩展，无需传统的定制适配开发，让资产接入与扫描始终保持高效率与高灵活性。</p><h4>▸全场景自动化 + AI 驱动的数据分类分级能力</h4><p>在资产扫描的同时，系统会自动完成数据分类分级，实现对业务的零打扰。针对结构化数据，可基于库表字段命名、数据特征等进行灵活打标，直接生成表级、字段级分类分级结果。通过接入外部 AI 模型，系统能够进一步突破传统规则的局限，基于智能引擎的深度学习，理解字段语义与数据关系，动态优化分类策略，使识别准确率提升到 95% 以上。同时，系统构建 RAG 行业训练集，将项目经验与标签体系不断沉淀，让模型越用越准、识别越做越强。</p><h4>▸高效稳定的数据分类分级执行引擎</h4><p>系统在大规模场景下仍具备优秀性能，基于正则或字典匹配方式，在十万级表规模下平均耗时仅需 1.5～3 小时，完全满足企业级分类分级效率需求。配套的策略模板机制支持规则与标签的导入导出，让专家经验得以快速沉淀，并在后续项目中复用，不仅降低了策略构建成本，也显著提升了整体分类分级的准确性与一致性。</p><h4>▸数据资产安全与可视化能力</h4><p>系统对采集的所有数据源信息进行全程加密处理，确保从传输到存储均具备安全防护能力，即使发生意外风险，也无法直接读取原始数据，有效降低数据资产暴露风险。同时，提供直观清晰的数据资产可视化视图，将数据总量、分类分布、敏感等级等核心指标以图表呈现，帮助用户快速掌握整体资产情况，定位关键数据与潜在风险，为治理策略制定与风险处置提供直接的信息支撑。<br/><img width="723" height="370" referrerpolicy="no-referrer" src="/img/bVdm6Ce" alt="" title="" loading="lazy"/><br/> （产品架构）</p><h3>2.核心优势</h3><h4>▸AI赋能，实现自动化、高准确度的数据分类分级</h4><p>依托行业化微调的大模型和多种语义增强技术，系统在AI智能引擎的加持下，实现对海量数据的自动化分类分级，并显著提升识别率与准确率。通过优化嵌入AI模型训练策略、引入负样本采样与困难样本挖掘，模型能够更精准地区分近似标签、识别细微语义差异，解决传统算法易混淆、边界模糊的问题。</p><h4>▸强化知识体系，构建高质量识别能力基座</h4><p>通过扩大基础语料库规模、系统开展知识蒸馏，将行业专家的隐性经验、领域术语体系、分类标准和判断逻辑显性化编码；同时优化知识库的元数据标注和层次结构，并结合多级分类与排序模型，大幅提升分类的精准度与召回率，为智能识别提供坚实的知识底座。</p><h4>▸抑制幻觉与结果一致性控制，确保输出可信、稳定、可用</h4><p>在任务层面通过提示模板与角色设定，使大模型专注于分类分级任务，减少无关内容生成；同时引入事实核查与自我验证机制，保障输出结果真实可靠。结合统一的输出格式与指令模板，可显著提升结果一致性与用户的阅读体验。</p><p>在不断深化产品能力、夯实技术优势的过程中，全知科技始终坚持以技术驱动数据安全的可持续发展，依托在 AI 数据安全方向上的长期投入，推动数据安全产品在行业场景中真正落地。知源-AI分类分级系统，通过AI智能引擎，在不影响企业实际的数据服务环境的前提下，极大地改善了企业传统的资产管理和梳理分类的工作方式，提高了数据分类分级的工作效率，也保障了数据资产管理与梳理的质量。<br/>目前，该产品和服务已在各地政府、金融、运营商、互联网、物流、医疗等领域深度应用，与众多客户都建立起了长期友好合作关系。这些成果不仅验证了全知方案的成熟度与实用价值，也为公司进一步拓展技术边界、创新未来产品方向奠定了坚实基础。<br/>未来，全知科技将继续以创新为核心驱动力，在数据安全技术研发与前沿实践中深入精进，致力于为数字经济构筑更高标准的安全底座，使数据治理体系更加稳健可信，助力行业迈向成熟、完善与可持续发展的新阶段。 </p>]]></description></item><item>    <title><![CDATA[自适应分类的可落地规模化政府部门数据分类]]></title>    <link>https://segmentfault.com/a/1190000047413655</link>    <guid>https://segmentfault.com/a/1190000047413655</guid>    <pubDate>2025-11-20 11:05:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>概要：<br/>（提示：本章节概述解决方案的核心价值与落地成效，帮助读者快速理解方案全貌。）<br/>在数字政府建设快速推进的背景下，政务数据已成为政府提升治理能力、优化公共服务的重要资源。然而，政务数据存在“多源异构、跨域流转”特点，分散于各委办局业务系统和电子政务云中，传统人工管理难以应对海量数据资产的识别和分类需求。针对这一痛点，全知科技提出的“<a href="https://link.segmentfault.com/?enc=Fl2gmyuK5waT2WwQbNufwA%3D%3D.Q%2FiD1r2iiboizHFkye5F69lpbwsE009hLtTmzOj5VMQ%3D" rel="nofollow" target="_blank">知源-AI数据分类分级系统</a>”，以技术驱动实现政务数据全生命周期管理。该系统通过AI智能分析、知识图谱、多模态引擎及非侵入式部署模式，构建“全量盘点—智能分级—经验沉淀—合规复用”的闭环体系，实现数据资产动态可视、敏感数据精准识别及跨部门共享安全可控。实践表明，该方案可将政务数据共享效率提升10倍以上，同时将合规审计人工成本降低50%，在满足监管要求的同时显著提升政务服务效率，为数字政府高质量发展提供可靠支撑。<br/>一、政务数据治理现状与核心痛点<br/>（提示：分析政务数据分类分级落地前的行业痛点与政策驱动。）</p><pre><code>   政务数据类型繁多，从居民个人信息到核心业务数据，涵盖民政、医保、人社、公安等多个部门。数据分散存储，存在“僵尸数据”“影子数据”，安全风险显著。传统人工梳理方式不仅效率低下，还难以应对跨部门共享与动态安全管控需求。此外，政务数据权属复杂，涉及政府、企业、公众等多方角色，数据界定模糊，增加了流转风险。政策层面，《数据安全法》《个人信息保护法》《政务数据共享开放条例》《“十四五”数字政府建设规划》等明确要求政务数据“分类管理、分级防护”，并将数据安全纳入政府绩效考核体系。在此背景下，政务数据分类分级成为破解“数据孤岛、安全缺位、合规不足”困局的核心抓手，不仅是满足监管要求的基础工程，也是实现政务数据精准管控、高效共享的关键路径。</code></pre><p>二、数据安全与合规风险透析<br/>（提示：明确政务数据面临的主要风险类型，为解决方案提供针对性依据。）</p><pre><code>   政务数据在多部门、多业务系统环境下的流转和使用，面临复杂而多层次的风险。首先是数据泄露风险，政务数据中包括居民身份信息、社保记录、医保账户及行政审批信息等敏感信息，一旦未经授权访问或外泄，将造成严重社会影响和法律责任。跨部门共享和老旧系统安全漏洞是主要风险点，例如部分委办局未更新的数据库可能存在未加密的历史数据，增加泄露概率。
   其次是合规风险，政务数据管理必须符合《数据安全法》《个人信息保护法》《政务数据共享开放条例》等法律法规。未按标准分类分级或未经脱敏共享，可能导致违规行为，产生行政处罚或信用惩戒。对于涉及个人信息和敏感业务数据的共享，如果没有明确的数据权属和操作流程，合规风险进一步放大。
   第三是资产不清风险，由于政务数据分散存储，且存在“僵尸数据”“影子数据”，未形成统一数据资产清单，将导致数据分布和使用情况不明，管理者难以掌握全局，无法及时发现潜在风险，也影响数据价值释放。
   第四是效率低下风险，传统人工分类和分级方式耗时长、精度低，在跨部门协作或新业务场景下难以快速响应。例如某地人社局在人工梳理海量数据时，数百万字段的识别和分类耗时数月，分类准确率不足70%，严重影响政务服务效率和数据赋能能力。
   第五是数据孤岛风险，各部门独立管理数据，缺乏统一标准和共享机制，导致数据无法互通，制约数字政府建设和“一网通办”“城市大脑”等应用落地。
  此外，政务数据的多源异构特性和跨域流转特性，增加了动态管控难度。不同部门业务系统更新频繁，新增数据和新业务模块若未及时纳入分类分级体系，将带来安全盲区。若缺乏智能化识别与自动化策略，风险将随数据量和复杂度增长呈指数级上升。</code></pre><p>三、自适应分类与可落地的技术实践<br/>（提示：展示知源-AI数据分类分级系统的技术路线及落地路径，说明可规模化实施能力。）</p><ol><li>技术架构与落地思路<br/>该系统依托“全量盘点—智能分级—经验沉淀—合规复用”四步闭环，实现政务数据全链路管理：<br/>● 全量盘点：通过非侵入式部署实现多维数据接入，支持数据库扫描、接口对接及文件导入等方式，覆盖400余种数据源，包括MySQL、Oracle、Hive以及PDF、扫描件等非结构化文件。在此过程中，系统能够自动发现长期闲置的“僵尸数据”和未备案的“影子数据库”，并动态生成完整的数据资产清单，清晰标明数据来源、存储位置、类型及关联业务，彻底解决政务部门“数据在哪、多少、是什么”的管理难题。<br/>● 智能分级：系统融合深度学习、知识图谱与政务规则库，构建多模态AI引擎，实现对结构化与非结构化数据的自适应分类分级。系统可通过字段匹配、正则规则及跨字段关联识别结构化敏感信息，同时对非结构化文档进行语义分析与敏感等级判定。动态校准机制可根据政务数据目录更新和外部模型学习持续优化策略，使分类准确率稳定在95%以上，并能针对新业务场景自动调整分类策略。<br/>● 经验沉淀：通过自动化生成分类规则和标签模板，实现跨部门知识复用。系统支持规则和模板的导入导出，结合专家打标经验，可快速适配新增业务场景，将传统分类配置周期从数月压缩至数天，提高政务数据治理响应速度。<br/>● 合规复用：实现分类结果在全域的安全应用。通过OpenAPI、Kafka、Syslog等接口，将分类分级结果对接政务数据共享平台和动态脱敏系统，实现“一处打标，全域复用”。在共享过程中自动脱敏敏感信息，保障数据安全与合规，实现政务服务便利性与数据安全性的平衡。</li><li><p>技术差异化优势<br/>● AI智能化：系统通过大模型训练、负样本采样和困难样本挖掘，实现对细微语义差异的精准识别和跨场景适配能力，提升分类准确率与鲁棒性。<br/>● 知识图谱与经验沉淀：将行业专家隐性知识结构化管理，结合多级分类排序模型和经验规则，形成可复用的知识体系，显著提高分类准确率与召回率，同时降低不同部门在新业务场景下的配置成本。<br/>● AI可靠性：引入提示词模板、事实核查和自我验证机制，确保分类分级结果一致性与机器可读性，有效抑制AI幻觉，保证敏感数据识别的精度和合规性。<br/>● 全量数据覆盖：通过多维度元数据补充，将字段注释完整率提升60%以上，兼容主流文件格式，实现结构化与非结构化数据的统一梳理，为政务数据全生命周期管理提供全面支持。<br/>四、实施效果与规模化成果<br/>（提示：展示该系统落地后的实际效果与数据化成果。）<br/>以某地人社局为例，部署知源-AI数据分类分级系统后，政务数据管理实现了显著提升。首先，在效率方面，系统可对海量数据表进行自动扫描、智能分类和分级处理，识别与分类效率较传统人工方式提升约10倍，原本需要数月完成的分类配置工作，如今仅需数天即可完成，显著缩短了政务数据治理响应周期。其次，在精准性方面，依托深度学习、多模态AI引擎和知识图谱的自适应分类能力，系统对结构化字段及非结构化数据进行智能分析，分类准确率高达98%，敏感数据识别精确可靠，为跨部门数据流转和共享提供了坚实保障。在合规性层面，方案实现了“一处打标，全域复用”，分类分级结果可自动同步至脱敏系统和政务数据共享平台，确保在数据共享和应用过程中严格遵循《数据安全法》《政务数据共享开放条例》等法规要求，降低人工审计压力，并为政府绩效考核提供可量化的数据依据。同时，通过清晰界定数据安全管控对象，方案有效降低了跨部门数据流转中可能存在的泄露风险和合规风险，进一步强化政务数据安全防线。此外，分类分级结果直接支撑政务数字化业务，政务数据能够快速服务于“一网通办”“城市大脑”等应用场景，提升了整体业务处理效率和用户体验。海量数据在保障安全和合规的前提下实现快速共享和复用，不仅释放了数据价值，也助力政府部门在数字化转型中实现高效决策和智能服务。通过该案例可以看出，方案不仅在技术层面具备高度可落地性和规模化能力，也在实际应用中带来了明显的效率、精准度、合规性与业务赋能提升，形成了可复制、可推广的政务数据治理经验。<br/>五、面向全国政务的可复制经验与战略意义<br/>（提示：阐述该系统对政务行业整体推广和数字政府建设的战略意义。）</p><pre><code>该系统以自适应分类为核心，兼具可落地与可规模化的特点，适用于全国各级政府部门推广应用，为数字政府建设提供长期战略支撑。首先，在政策契合层面，该系统严格遵循《数据安全法》《个人信息保护法》《政务数据共享开放条例》等法律法规要求，将数据分类分级与政务数据安全纳入政府绩效考核体系，实现政策落地的同时，为各级部门提供可量化的合规依据。其次，在数据赋能方面，系统通过全量盘点、智能分级及动态复用机制，打破部门间数据孤岛，实现跨机构、跨系统的安全共享。在效率提升上，方案通过AI智能化分析、知识图谱与多模态分类引擎，将传统人工分类的周期从数月压缩至数天，处理效率可提升十倍以上。风险管控方面，系统建立全链路动态可视化和加密存储机制，实现分类分级结果在共享平台和脱敏系统间的安全复用。既保障政务服务的便利性，又确保敏感数据不被非授权访问，为跨部门数据流转提供可控、安全的基础环境。
此外，该系统的智能化分类策略和专家打标经验可沉淀为可复制的知识资产，在不同部门和地区间推广，实现跨机构经验复用，形成全国范围内可推广、可落地、可规模化的政务数据分类分级实践体系。通过这一机制，政府部门不仅能够快速提升数据治理水平，还能在数字化转型过程中实现制度化、标准化和可持续的安全管理模式，为数字政府高质量发展奠定坚实基础。</code></pre><p>六、聚焦自适应分类与规模化应用的关键解答<br/>（提示：结合系统常见问题及解答，帮助政务部门快速理解与应用。）<br/>Q1：该系统如何实现自适应分类，满足政务数据多源异构特性？A1：基于多模态AI引擎、深度学习和知识图谱，实现对结构化字段、跨字段关联以及非结构化数据的自适应分类。系统能根据不同部门、业务场景及数据特征动态调整分类策略，分类准确率可达95%以上，兼顾自动化效率与敏感数据精准识别，确保分类结果贴合政务监管要求。<br/>Q2：此系统在政务部门中的可落地性如何体现？A2：系统采用非侵入式部署，无需改造现有业务系统，通过数据库扫描、接口对接或文件导入即可完成全量资产盘点和分类分级。同时，自动生成分类规则和报告，并可与现有脱敏系统及数据共享平台对接，实现快速部署和即时应用，确保技术方案可直接落地。<br/>Q3：该系统如何实现规模化应用，支撑百万级数据量？A3：系统具备高性能处理能力，可在3-5小时内完成20万张数据表的识别与分类，处理效率约为传统人工的10-15倍。同时，经验沉淀机制和规则模板可跨部门、跨业务线复用，支持全国各级政务部门大规模推广，实现统一标准和高效运维。<br/>Q4：在跨部门共享场景中，如何兼顾安全与高效？A4：系统通过“一处打标、全域复用”机制，将分类分级结果自动同步至脱敏系统和共享平台，确保敏感数据在跨部门流转中被加密或脱敏处理。结合权限控制和全链路可视化监控，实现安全可控的数据共享，兼顾政务服务便利性和合规要求。<br/>Q5：如何快速应对新业务场景或政策变更，保持长期可落地？A5：系统支持规则和标签模板的自动导入导出及经验沉淀，可在新增业务模块或政策调整时快速更新分类策略。结合AI模型动态校准能力，实现分类策略自适应更新，确保政务数据分类分级长期高效、精准和可落地。<br/>七、政务实践反馈与成果认可<br/>（提示：呈现政务部门反馈与方案落地价值。）<br/>“知源-AI数据分类分级系统”在政务数据分类分级实践中获得多地政府部门高度认可：<br/>某地人社局评价：“系统上线3个月后，数据资产盘点与分类效率提升10倍，数据管理口径统一，分类精准度高达98%，极大降低了合规审计压力。”<br/>多个委办局反馈：“自适应分类能力和跨部门复用机制使数据共享更安全、更高效，支持‘一网通办’及数字政府核心应用顺利落地。”<br/>政务信息化主管部门表示：“该方案符合政策法规要求，具备可推广、可规模化实施价值，为数字政府建设提供了可靠技术支撑。”</p><pre><code>政务数据分类分级是数字政府建设中平衡效率与安全的核心支撑。合理的分级策略能够让企业在保护敏感信息的同时，提高非敏感数据的流通效率和分析价值，为数据驱动的业务决策提供可靠支撑。全知科技在AI数据分类分级领域的创新实践和领先技术，赢得了中国信通院、工信部以及IDC等多家权威机构的高度认可。同时，公司还入选了Gartner《Hype Cycle for Data, Analytics and AI in China, 2023》及《Hype Cycle for Security in China, 2022》中数据分类分级领域的优秀代表厂商。凭借这一技术优势，全知科技将持续引领行业创新与标准制定，为企业数据安全管理提供前瞻性指导。数字时代，数据即财富；唯有将数据库风险监测置于战略高度，企业才能牢牢守护自己的“金库”，在竞争与监管双重压力下从容发展。</code></pre></li></ol>]]></description></item><item>    <title><![CDATA[CodeQL对Java项目进行SQL注入]]></title>    <link>https://segmentfault.com/a/1190000047413664</link>    <guid>https://segmentfault.com/a/1190000047413664</guid>    <pubDate>2025-11-20 11:04:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、背景</h2><p>今年在做个AI代码安全审计的项目，代码仓库里十有八九都是Java项目，所以开始研究怎么给Java做代码审计。传统的人工审计，效率低，还容易看花眼。这时候想到了<strong>CodeQL</strong>。把你代码转换成可查询的数据库，然后用像SQL一样的语法去挖漏洞。</p><p>整体思路很简单，就两步：</p><ol><li><strong>用CodeQL扫描</strong>，生成一堆漏洞的报告（JSON格式）。</li><li><strong>拆解和分析这些报告</strong>，判断这个漏洞到底是“确诊”还是“误诊”。</li></ol><h2>二、操作步骤</h2><h3>2.1 CodeQL 扫描</h3><p>简单来说，分三步走：</p><ol><li><p><strong>创建数据库</strong>：这就好比把Java代码这个“原始食材”加工成CodeQL能处理的“半成品”。</p><pre><code class="bash">codeql database create my-java-db --language=java --command="mvn compile"</code></pre><ul><li><code>my-java-db</code>：给你的数据库起个名儿。</li><li><code>--language=java</code>：声明语言是Java。</li><li><code>--command</code>：告诉CodeQL你用啥编译项目，比如Maven就用<code>mvn compile</code>，Gradle就用<code>gradle build</code>。这一步是关键，CodeQL会通过编译过程来理解整个代码结构。</li></ul></li><li><p><strong>运行查询</strong>：拿着写好的“问题清单”（QL查询脚本），去数据库里找答案。</p><pre><code class="bash">codeql database analyze my-java-db codeql/java/ql/src/Security/ --format=sarif-latest --output=results.sarif</code></pre><ul><li>这里我直接用了CodeQL自带的官方安全查询库 (<code>codeql/java/ql/src/Security/</code>)，里面涵盖了SQL注入、SSRF等各种漏洞的检测规则。</li><li>输出格式我选了<code>sarif</code>，这是一种标准格式，很多工具都认。当然你也可以输出成CSV或者JSON。</li></ul></li><li><strong>查看结果</strong>：生成的<code>results.sarif</code>文件里，就藏着所有疑似漏洞的线索。</li></ol><h3>2.2 扫描结果解析</h3><p>这个SARIF/JSON文件，大家刚开始看可能会觉得眼花缭乱。不过我们实际上只要看核心信息：</p><ul><li><strong>漏洞位置</strong>：<code>location</code> 字段会精确告诉你，有问题的代码在哪个文件的第几行第几列。</li><li><strong>漏洞类型</strong>：<code>ruleId</code> 会告诉你它怀疑是啥漏洞，比如 <code>java/sql-injection</code>。</li><li><strong>数据流路径</strong>：这是最核心的部分！<code>codeFlows</code> 里会展示数据是怎么从“源头”（Source，比如用户输入）流到“汇点”（Sink，比如执行SQL语句的方法）的。这是我们接下来审计的重点。</li></ul><hr/><h2>三、判断漏洞真假</h2><p>CodeQL报出来的不一定是真漏洞，它只是个“高度可疑”的警报。我们需要去验证它。</p><p>核心思路就四个字：<strong>跟踪数据流</strong>。具体来说，要检查三个地方：</p><ol><li><strong>注入点（Source）有效吗？</strong> 这个数据真的是来自不可信的用户吗？</li><li><strong>执行点（Sink）有效吗？</strong> 这个函数真的能执行危险操作吗？</li><li><strong>中间链路干净吗？</strong> 数据从源头到执行点的路上，有没有被“洗干净”（过滤、编码）？</li></ol><p>我举几个例子来具体讲。</p><h3>3.1 SQL注入常规案例</h3><p><strong>CodeQL报告</strong>：在 <code>UserController.java</code> 的第35行，可能存在SQL注入。</p><p>我们去看看代码：</p><pre><code class="java">// UserController.java
public String getUserByName(@RequestParam String name) {
    String sql = "SELECT * FROM users WHERE name = '" + name + "'"; // Source: 用户控制的name参数
    return jdbcTemplate.queryForObject(sql, String.class); // Sink: 执行SQL查询
}</code></pre><p><strong>审计过程</strong>：</p><ol><li><strong>源头</strong>：<code>name</code>参数来自用户HTTP请求，完全可控。有效！</li><li><strong>汇点</strong>：<code>jdbcTemplate.queryForObject</code> 确实会执行SQL语句。有效！</li><li><strong>链路</strong>：数据从 <code>name</code> 直接拼接进 <code>sql</code> 字符串，然后传入汇点。中间<strong>没有任何过滤</strong>！</li></ol><p><strong>结论</strong>：这是个<strong>真漏洞</strong>，实锤的SQL注入。修复方法很简单，用预编译就对了。</p><h3>3.2 MyBatis审计为例</h3><p>MyBbatis的情况比较特殊，它的SQL语句很多写在XML文件里。CodeQL同样能扫描出来。</p><p><strong>关键点</strong>：MyBatis里用 <code>#{}</code> 是预编译，安全的；用 <code>${}</code> 是字符串拼接，危险的！</p><p><strong>CodeQL报告</strong>：在 <code>UserMapper.xml</code> 中发现使用 <code>${}</code>。</p><pre><code class="xml">&lt;!-- UserMapper.xml --&gt;
&lt;select id="getUser" parameterType="String" resultType="User"&gt;
    SELECT * FROM users WHERE name = '${name}' 
&lt;/select&gt;</code></pre><p><strong>审计过程</strong>：</p><ol><li><strong>源头</strong>：<code>name</code>参数从Java层传入Mapper。</li><li><strong>汇点</strong>：MyBatis框架解析 <code>${name}</code> 时，会直接进行字符串替换。</li><li><strong>链路</strong>：直接拼接。</li></ol><p>所以只要 <code>name</code> 用户可控，就是<strong>真漏洞</strong>。CodeQL能识别出这种模式并报警。</p><h2>四、常见误报案例</h2><p>CodeQL不是神，很多情况它会“过度紧张”。下面是我遇到的三个典型“假警报”，咱们掰开揉碎了分析分析。</p><h3>4.1 只能控制部分参数</h3><p><strong>CodeQL报告</strong>：在 <code>AdminController.java</code> 的第42行，检测到可能存在SQL注入漏洞，数据流显示用户输入参数直接拼接进SQL语句。</p><p>我们来看具体代码：</p><pre><code class="java">public String resetPassword(@RequestParam int userId) {
    // userId虽然是用户输入，但它被声明为int型
    String sql = "UPDATE users SET password='default' WHERE id = " + userId;
    jdbcTemplate.update(sql);
    return "密码已重置";
}</code></pre><p><strong>详细审计过程</strong>：</p><ol><li><strong>源头分析</strong>：<code>userId</code> 确实是用户通过HTTP请求传入的参数，从来源上看属于“不可信输入”，这一点CodeQL判断是对的。</li><li><strong>汇点分析</strong>：<code>jdbcTemplate.update(sql)</code> 方法确实会执行传入的SQL字符串，属于SQL注入的典型危险汇点，CodeQL这里的判断也没问题。</li><li><strong>关键链路分析</strong>：问题出在 <code>userId</code> 的数据类型上。它被明确声明为 <code>int</code> 类型，这意味着什么？当用户在HTTP请求里传入参数时，Spring MVC框架会自动进行类型转换。如果用户想传个带SQL注入的字符串，比如 <code>1; DROP TABLE users;--</code>，框架在转换时就会直接报错（类型不匹配），这个恶意字符串根本进不到代码里。最终拼接进SQL的，只能是一个合法的整数。这就从根本上堵死了注入的可能性——攻击者连注入代码的机会都没有，因为参数类型把所有非数字输入都过滤掉了。</li></ol><p>所以<strong>假漏洞</strong>这种情况属于CodeQL在做数据流跟踪时，只关注“数据是否来自用户”以及“是否流向危险操作”，但对Java这种强类型语言的类型约束理解不够深入，没意识到基础数据类型本身就能提供一定的安全保障。</p><h3>4.2 中间链路被清洗</h3><p><strong>CodeQL报告</strong>：在 <code>UserService.java</code> 的第78行，发现用户输入参数未经过滤直接拼接SQL语句，存在SQL注入风险。</p><p>代码片段如下：</p><pre><code class="java">// UserService.java
public User findUser(String inputName) {
    // 调用全局安全工具类进行过滤
    String filteredName = SecurityUtils.sanitizeSQL(inputName); 
    String sql = "SELECT * FROM users WHERE username = '" + filteredName + "'";
    return jdbcTemplate.queryForObject(sql, User.class);
}</code></pre><p><strong>详细审计过程</strong>：</p><ol><li><strong>源头分析</strong>：<code>inputName</code> 是从前端传入的用户输入，确实属于不可信来源，CodeQL的判断正确。</li><li><strong>汇点分析</strong>：<code>jdbcTemplate.queryForObject</code> 执行SQL语句，属于危险汇点，CodeQL这里也没毛病。</li><li><strong>关键链路分析</strong>：CodeQL的警报忽略了中间的 <code>SecurityUtils.sanitizeSQL</code> 方法。这是我们公司自己开发的一个全局安全工具类，里面的 <code>sanitizeSQL</code> 方法做了非常彻底的处理——它会把单引号、双引号、分号、注释符等所有SQL注入常用的特殊字符都进行转义（比如把单引号 <code>'</code> 转成 <code>\'</code>），还会过滤掉 <code>UNION</code>、<code>DROP</code>、<code>EXEC</code> 等危险关键字。也就是说，经过这个方法处理后，<code>filteredName</code> 里已经不存在能构成注入的“弹药”了。但CodeQL的默认规则只认识它内置的那些安全函数（比如Apache Commons Lang里的<code>StringEscapeUtils.escapeSql</code>），对于我们这种自定义的过滤方法，它没办法识别其安全性，所以依然会报警报。</li></ol><p><strong>结论</strong>：<strong>假漏洞</strong>。这种情况需要我们人工介入，去核查中间的过滤函数到底有没有真的起到“净化”作用。如果这个自定义过滤器确实能有效拦截恶意输入，那这个警报就是误报。</p><h3>4.3 无效的Sink</h3><p><strong>CodeQL报告</strong>：在 <code>FileProcessor.java</code> 的第112行，检测到用户控制的URL参数可能导致SSRF（服务器端请求伪造）漏洞。</p><p>我们来看代码：</p><pre><code class="java">public void logFileUrl(@RequestParam String fileUrl) {
    // 仅将URL记录到日志，未发起任何网络请求
    logger.info("用户请求处理的文件URL：" + fileUrl);
    // 其他业务逻辑...
}</code></pre><p><strong>详细审计过程</strong>：</p><ol><li><strong>源头分析</strong>：<code>fileUrl</code> 是用户传入的参数，完全可控，属于SSRF漏洞的典型源头，CodeQL这一步判断正确。</li><li><strong>汇点分析</strong>：这是问题的核心。CodeQL的SSRF检测规则里，可能把所有“接收字符串并输出”的函数都当成了潜在的危险汇点，但实际上，SSRF的危害在于服务器会根据这个URL发起网络请求（比如访问内部系统、敏感端口等）。而这里的 <code>logger.info</code> 方法仅仅是把字符串写入日志文件，它既不会解析这个URL，也不会发起任何HTTP/HTTPS请求，更不可能去连接内部服务。这个“汇点”根本没有执行危险操作的能力，是个“伪Sink”。</li><li><strong>链路分析</strong>：数据从 <code>fileUrl</code> 直接拼接进日志字符串，中间没有其他处理，但因为最终的“汇点”不具备发起请求的能力，所以整个链路没有安全风险。</li></ol><p><strong>结论</strong>：<strong>假漏洞</strong>。这是由于CodeQL对“危险汇点”的定义可能过于宽泛，把一些看似相关但实际无危害的函数也纳入了监测范围。我们在审计时，一定要确认数据最终流向的函数是否真的能执行对应的危险操作（比如SSRF里的 <code>URL.openConnection</code>、<code>HttpClient.execute</code>，XSS里的 <code>response.getWriter().write</code> 等）。</p><hr/><h2>五、总结</h2><p>好了，我们来总结一下用CodeQL审计Java代码SQL注入（其他漏洞也类似）的心法：</p><ol><li><strong>工具是辅助</strong>：CodeQL发现的目标是疑似漏洞，需要你自己去最终判别。</li><li><strong>核心是数据流</strong>：一定要亲手去跟踪 <code>Source -&gt; ... -&gt; Sink</code> 这条线。不要只看头和尾，中间路径的“净化”环节至关重要。</li><li><strong>理解框架特性</strong>：像MyBatis的 <code>#</code> 和 <code>$</code>，Spring的参数绑定，这些框架知识能帮你快速判断漏洞的真伪。</li><li><p><strong>警惕误报三点</strong>：</p><ul><li><strong>类型安全</strong>（如int参数）。</li><li><strong>自定义过滤</strong>（CodeQL不认识你的安全函数）。</li><li><strong>Sink点误判</strong>（数据没流到真正危险的地方）。</li></ul></li></ol><p>通过这种“工具扫描 + 人工研判”的模式，我们就能在复杂的Java项目中，高效、精准地挖出真正的安全漏洞。</p><hr/><p>作者：汤青松<br/>日期：2025年11月17日<br/>微信：songboy8888</p>]]></description></item><item>    <title><![CDATA[构建数据库安全新范式：以规范为基石，实现]]></title>    <link>https://segmentfault.com/a/1190000047413674</link>    <guid>https://segmentfault.com/a/1190000047413674</guid>    <pubDate>2025-11-20 11:04:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>概要：<br/>（提示：当防护边界逐渐模糊，数据自身的“行为轨迹”成为新的安全焦点。）<br/>在数字经济的核心结构中，数据库不再仅是支撑业务的技术组件，而是企业数字资产的“原矿”。金融机构的交易流水、互联网平台的用户画像、医疗系统的病历档案——所有这些关乎隐私、业务与监管的数据，都以数据库为载体在组织内流转与沉淀。但随着数据量与访问频率呈指数级增长，传统以“静态防护”为核心的数据库安全体系正逐渐失效。仅依靠防火墙、加密和权限控制的“外围防御”模式，已无法精准识别数据的真实流向与动态风险。近年来，数据泄露事件屡屡警示行业：某酒店集团因数据库未审计导致数亿客户信息泄露、某银行内部账号滥用引发敏感交易数据外传。这些事件表明，数据库安全的关键已从“守住入口”转向“掌握流向”。因此，“敏感数据追流向踪”应成为<a href="https://link.segmentfault.com/?enc=OuYMXjBlBgZrQu0fst%2BJSA%3D%3D.yeNu3Hiv%2FcJoLOA2luloEnP8Hc3lg%2FesefvwS4KiZ98%3D" rel="nofollow" target="_blank">数据库安全系统</a>的核心理念。它不止回答“谁访问了什么”，更要揭示“数据从哪里来、去了哪里、是否合规”。而这正是传统“行为审计”所无法实现的动态洞察。<br/>一、从行为审计到敏感数据追流向踪的精确与实时监测<br/>（提示：行为记录止于“动作”，而数据追踪始于“意图”。）<br/>“行为审计”是数据库安全的基础手段，其核心目标是可追溯性（Traceability）。通过记录SQL语句、操作时间、访问来源等信息，实现事后分析与责任追查。它回答了“谁做了什么”的问题，是数据合规体系的必要组成部分。然而，在“实时动态”成为安全标配的今天，仅靠事后溯源已无法匹配数据库风险的演进速度。勒索攻击、越权访问、批量导出等威胁往往在毫秒级内完成。因此，新的安全模型“敏感数据追流向踪”应运而生。它的核心是基于流量、行为与标签的实时动态分析，构建出数据库中每一份数据的“生命曲线”——从生成、传输、存储到调用的全链路可视化追踪。与传统审计相比，敏感数据追踪在三个维度上实现了跃迁：<br/>● 符合规范：与数据分类分级体系对接，确保监测与国家标准及行业合规要求一致，尤其契合《数据安全法》《个人信息保护法》中的“最小必要”“可溯源”原则。<br/>● 精确识别：不仅审计操作人和行为，更能识别具体的数据对象（库、表、列、字段），并自动标注其敏感级别。<br/>● 实时动态：系统通过智能算法与事件流监控，在数据操作发生的同时进行分析与预警，真正实现“事中防护”。<br/>这意味着，企业可以在数据流动的第一时间察觉风险，从而将数据库安全从“被动响应”升级为“主动掌控”。<br/>二、实现符合规范、精确、实时动态监测的难点<br/>（提示：安全的难度不在“防”，而在“精确地防”。）<br/>要实现对敏感数据的“实时追流向踪”，企业面临的不仅是技术问题，更是体系与管理的系统性挑战。</p><ol><li>数据流动边界模糊化随着云计算与分布式架构普及，数据的流动路径更加复杂。数据库实例跨云、跨地域部署，传统的网络边界失效，数据流向变得“无边界可循”。企业常出现监控盲区，如API调用绕过数据库层、跨部门共享数据未纳入审计链条等。</li><li>内部滥用与权限失控研究表明，约80%的数据泄露源自内部人员。员工利用过高权限访问非职责数据、外包维护方滥用账号下载敏感表——这些“低频高危”行为往往隐藏于日常操作中。传统审计日志只能记录表象，却难以揭示行为背后的动机与风险。</li><li>合规性压力持续攀升面对日益严格的监管环境，《网络安全法》《数据安全法》《个人信息保护法》等法规均要求企业实现“敏感数据动态可监测、可追溯”。合规不再是“被动遵守”的成本项，而是“主动守护”的竞争力。如何构建一套既符合法规、又能支撑业务敏捷的安全体系，成为各行业的共性难题。</li><li>技术碎片化与体系不协同许多机构部署了多套独立系统：日志审计、流量监测、访问控制、加密脱敏——各自为政，难以形成统一的安全闭环。缺乏“数据中心化视角”，导致风险事件发现滞后、响应延迟。<br/>三、常见问题与应对思路：精准与动态的结合<br/>（提示：从“行为留痕”到“数据溯源”，安全能力的演进需以体系化思维推进。）<br/>Q1：如何确保监测符合国家及行业规范？A1：“符合规范”是系统设计的底线。数据库风险监测系统应对接数据分类分级结果，以敏感级别为核心构建监测策略。针对二级、三级、四级数据分别设置访问阈值、告警策略与日志保留周期，满足《数据安全法》“按等级保护”要求。同时，系统日志应独立存储36个月以上，确保取证合规。<br/>Q2：实时动态监测是否会影响数据库性能？A2：主流部署采用旁路采集模式（例如交换机镜像流量），不干扰业务主链路。系统通过智能流量分析和行为建模，过滤无关SQL请求，仅聚焦高风险事件，从而实现“零打扰、零负载”的实时监测。<br/>Q3：如何实现对敏感数据流向的精准识别？A3：系统通过与分类分级标签绑定，将每个数据字段的敏感等级与访问者身份、时间、来源IP、操作类型进行关联分析。例如检测到“普通账号在非工作时段访问四级数据表”时，系统即刻触发高优先级告警，并生成追踪路径，记录从访问到导出全链路信息，实现“数据去哪了”的全程可视。<br/>四、从合规审计到智能守护的演进<br/>（提示：当数据成为资产，监测体系也必须具备“资产级精度”。）<br/>数据库安全的未来，将从“行为可查”迈向“数据可感知”的智能治理阶段，呈现出以下三大发展趋势：</li><li>全生命周期数据追踪成为刚需未来数据库安全体系将贯穿“生成—流转—使用—归档—销毁”全过程。通过敏感数据地图和动态标签体系，企业能够实时掌握数据流向、访问路径和使用频次，为风险识别和合规审计提供持续支撑。</li><li>智能分析驱动精确预警AI与大数据分析将成为数据库安全的核心驱动力。通过建模正常操作模式、识别异常行为轨迹，实现毫秒级响应。系统不再只是报警器，而是“智能安全助理”，能自主判断、关联分析并生成安全建议。</li><li><p>统一安全视图与协同响应未来数据库安全将不再孤立存在，而是与API安全、数据防泄漏、身份管理、合规审计等系统深度融合，形成“统一数据安全运营平台”。实现从检测到响应的全自动化闭环，让安全真正成为业务连续性的底层支撑。</p><pre><code>在数字化与数据驱动的时代，数据库已不仅是信息存储的技术模块，更是企业核心资产与业务安全的关键承载体。传统的行为审计虽然能实现事后可追溯，但面对数据量激增、云化部署及复杂流转场景，其静态、延迟的特性已难以满足安全与合规的双重要求。“敏感数据追流向踪”理念的提出，正是为了解决这一痛点。通过符合规范、精确、实时动态的监测与管理系统，企业能够在数据流转的每一环节实现可视化、可控化和可追溯化。从分类分级数据标签、用户权限绑定，到全生命周期的行为追踪和智能预警，系统不仅能够提前发现潜在风险，还能支持快速响应和责任溯源。实践表明，实时动态监测与行为审计的结合，能够显著提升数据库安全治理的深度与广度：既满足监管合规要求，也为企业提供精确的数据风险洞察，实现从被动防御到主动掌控的转型。未来，随着人工智能和大数据技术的进一步融合，数据库安全将向更智能化、精确化和全生命周期管理方向发展，使企业在合规、运营和风险控制之间形成稳健闭环，为数字化战略保驾护航。</code></pre></li></ol>]]></description></item><item>    <title><![CDATA[为什么你的网络任务离不开高质量代理？一文]]></title>    <link>https://segmentfault.com/a/1190000047413679</link>    <guid>https://segmentfault.com/a/1190000047413679</guid>    <pubDate>2025-11-20 11:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数据驱动的时代，无论是企业级业务还是个人的网络任务，稳定、安全、可控的网络环境已经成为刚需。代理 IP因此从早期的小众技术，逐渐演变为各行业标配的核心基础设施之一。从跨境业务到AI训练，从广告验证到爬虫工程，代理的角色正在变得越来越关键。<br/>然而许多人对代理的理解仍停留在“换一个IP地址”这种表层概念。事实上，高质量代理在底层网络路径、协议优化、连通性保障、识别规避等方面都扮演着非常深层的技术角色。<br/>本文将带你从更专业的角度理解代理 IP 的本质、价值，以及未来的发展方向。</p><h2>一、代理 IP 的核心价值：不仅是“换IP”，更是构建稳定与可信的网络环境</h2><p>代理 IP 在现代网络环境中主要带来三类价值：</p><ol><li>网络身份切换与合规访问<br/>不同国家、地区的 IP 能让你的请求被目标网站视为本地用户，从而解锁内容限制、提升访问成功率。</li><li>提升任务稳定性<br/>高质量代理通常会建立在优化过的节点与线路上，能够减少封锁、降低阻断率，为高频任务提供稳定环境。</li><li>数据访问安全性<br/>代理可隐藏真实IP，让请求链路更加安全；尤其在广告监测、金融风控检测等领域，这点尤为重要。</li></ol><h2>二、为什么代理 IP 会“慢”“卡”“延迟高”？背后原因比你想的复杂</h2><p>如果你过去用过某些低质量代理，可能遇到这些问题：<br/>●延迟超过 2 秒，任务加载卡顿<br/>●中途连接断开、出现丢包<br/>●同一节点时快时慢，不稳定<br/>这些问题往往不只是节点本身的问题，而是由多层网络因素叠加形成的：<br/>●节点线路拥堵<br/>●运营商限速<br/>●覆盖国家资源不足导致池子被反复消耗<br/>●出口带宽不足<br/>●与目标站点 CDN 的物理距离过远<br/>换句话说，好的代理不是“卖IP数量”，而是“优化整条访问链路”。</p><h2>三、2025 年代理行业正在发生的技术变化</h2><p>随着 AI、自动化和全球业务对网络环境要求提升，2025 的代理行业将迎来三个显著趋势：</p><ol><li>网络环境将更“真实化”<br/>高级反爬虫系统会进一步识别异常网络特征，因此住宅代理将继续占据主导地位，而数据中心代理将更多用于高速任务。</li><li>动态身份池 + 行为级抗检测<br/>代理服务商将从“提供IP”升级为“提供完整环境”：<br/>●网络行为仿真<br/>●指纹配合策略<br/>●按需自动切换干净IP<br/>身份不止是 IP，而是“全链路的用户状态”。</li><li>更关注稳定性与可控性<br/>大家开始意识到：<br/>延迟低、成功率高，比“IP数量多”更重要。<br/>节点自愈、自动路由优化、负载均衡分发、智能测速等技术将成为主流。</li></ol><h2>四、如何选择优质代理？核心不是套餐，而是技术能力</h2><p>很多人在买代理时只看价格或 IP 数量，但真正影响体验的，是服务商的架构能力，包括：<br/>●是否拥有真实住宅资源池<br/>●接入的网络线路是否多样化<br/>●节点是否有丢包监控与自愈机制<br/>●是否支持动态切换、API 获取等高级功能<br/>●是否提供可控的带宽与协议（HTTP/SOCKS5）<br/>能做到这些的代理服务商，才能真正解决任务卡顿、被封、延迟高等问题。</p><h2>五、结语：代理不再是“工具”，而是网络业务的基础设施</h2><p>随着全球互联网环境更加严格，AI 数据任务不断增加，代理 IP 的重要性会持续提升。<br/>选择高质量代理，不仅能减少失败成本，更能构建稳定、可控、安全的网络环境，这会直接影响你的爬虫效率、广告验证精度、多账号安全性，以及整体业务的稳定性。</p>]]></description></item><item>    <title><![CDATA[AI加持下的数据流转安全，打造高效可溯源]]></title>    <link>https://segmentfault.com/a/1190000047413681</link>    <guid>https://segmentfault.com/a/1190000047413681</guid>    <pubDate>2025-11-20 11:02:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>概要：<br/>（提示：在数字世界中，数据不再静止，而是不断流动；因此，安全防护的焦点，也应从“静态防护”转向“流转安全”。）<br/>当外卖订单在几秒内完成支付、银行转账在瞬息之间到账、短视频平台精准推送你喜爱的内容时，数据正在通过成千上万条API通道高速流转。API作为数字世界的“数据动脉”，承载着企业业务逻辑、交易指令和用户隐私，是现代数字体系中最关键的连接层。<br/>然而，数据流动越快，风险也传播得越快。过去十年中，80%以上的企业数据泄露事件与API漏洞相关。传统以防火墙、WAF、渗透测试为核心的“静态安全”体系，侧重于外围防御与代码检测，难以应对API这一“流动入口”上的动态攻击与复杂滥用。<br/>在“数据流转安全 VS 静态数据安全”的新时代对比中，企业需要一种更高效、更智能、更可溯源的安全体系——它不仅能守护数据的静态安全，更能实时监控数据流动全程，洞察每一次API调用背后的行为逻辑与风险模式。这正是<a href="https://link.segmentfault.com/?enc=tUdO17%2BDs6z5bFVjdvhMXg%3D%3D.DjgOPf8A2UUnsU3L1KmLy22aF1OSj8efrpDBA1WFt6w%3D" rel="nofollow" target="_blank">API风险监测系统</a>存在的意义：它让数据流转可见、可控、可溯源，让安全从“静态防御”走向“智能守护”。<br/>一、API：驱动数据高效流转的核心引擎与风险源<br/>（提示：要理解API安全，首先要理解它如何让数据“动”起来。）<br/>API是现代信息系统之间交互的基础机制。它像桥梁一样，连接前端与后端、应用与数据库、系统与第三方服务，使数据在不同节点之间顺畅流通。在企业数字化架构中，API承载着巨量数据交换——金融交易、医疗信息、政务服务、供应链调度，都依赖API实现数据的高效流转。随着云计算、微服务、移动应用的普及，API数量呈爆发式增长，一个大型金融机构平均每年新增超过3万个API接口。<br/>但这条“数据高速公路”也充满隐患：敏感数据暴露：过度返回字段导致身份证号、联系方式等隐私信息泄露；逻辑滥用攻击：攻击者利用业务接口漏洞绕过验证发起非法交易；影子与僵尸API：未登记、被遗忘或旧版本接口仍暴露在外；权限越权访问：参数篡改即可查看他人信息。这些威胁共同揭示出一个现实：API安全不再是单点防护问题，而是数据全链路治理问题。保护的不仅是接口，更是数据在流动过程中的安全性与合规性。因此，从静态安全走向流转安全，不只是技术演进，更是安全理念的重塑。<br/>二、数据流转时代下的多维威胁与治理困境<br/>（提示：在数据流转的世界里，安全的最大挑战，不是攻击的复杂性，而是“不可见性”。）<br/>传统的安全体系建立在“静态”假设之上——假设数据停留在数据库中、系统边界清晰、攻击路径固定。然而，在API时代，这些假设几乎全部失效。<br/>（1）攻击面扩大，流转路径模糊<br/>API连接了内部系统、合作伙伴、第三方生态，使得数据流动链条无限延伸。一个跨境电商平台的订单接口，可能同时与支付网关、仓储系统、快递API交互。只要任一环节存在漏洞，整个链条便可能被突破。<br/>（2）传统检测滞后，无法实时感知风险<br/>防火墙、漏洞扫描、代码审计等安全手段多依赖“已知规则”和“静态样本”，无法识别实时流量中的异常行为。例如，黄牛抢票、爬虫数据窃取、批量越权调用，往往呈现为“合法调用行为”，但隐藏在高频次、特定参数或时间分布中。<br/>（3）数据合规压力上升<br/>随着《网络安全法》《数据安全法》《个人信息保护法》实施，企业需对数据流转路径进行可追溯性管理。但大多数组织仍停留在“存储安全”层面，缺乏对API流量、数据字段、访问轨迹的精细记录与分析能力。换言之，企业不仅要能“防”，更要能“知”“控”“溯”。安全不应只是被动防御，而应成为一种动态治理能力。<br/>三、常见问题与解答：AI赋能的高效可溯源防护体系<br/>（提示：只有当安全系统具备“智能学习”与“全链溯源”能力时，数据流转的安全才能真正落地。）<br/>Q1：API风险监测系统与传统防护有何不同？A1：传统防护是静态的、基于规则的“堵”。而API风险监测是动态的、基于AI的“看”与“解”。它通过全流量采集与智能建模，实现从接口发现、行为刻画到风险响应的闭环。核心机制包括：自动发现与画像建模：通过流量分析和AI识别，系统能自动识别所有API接口（包括影子/僵尸接口），并生成结构化画像（功能、数据类型、调用关系）。智能风险识别：借助Transformer架构与自注意力机制，对API调用行为进行语义级分析，识别出隐藏的越权访问、批量调用或异常数据传输。行为基线与异常检测：通过强化学习算法，系统可根据历史调用数据持续调整风险阈值，实现自适应监测。<br/>Q2：如何做到“可溯源”？A2：监测系统为每一次API访问建立细粒度日志，包括调用时间、请求参数、返回数据结构、调用方标识等。当出现数据泄露或违规访问时，可通过关联分析追踪到具体责任接口与操作路径。例如，在某省级政务平台中，一次内部接口被异常调用导致公民信息泄露。通过风险监测系统的日志回溯，安全人员精准定位到源请求IP、调用脚本特征与对应开发模块，仅用2小时完成溯源，而传统人工排查需耗时2周。<br/>Q3：AI技术如何提升效率？A3：人工智能让API安全从“人工发现”转向“自学习感知”。通过大模型的语义理解与行为预测，系统可：实时处理海量API日志（每秒百万级调用量）；自动聚类异常行为并生成风险报告；减少误报漏报率30%以上；降低人工分析与合规审计成本50%。在金融场景中，这意味着能提前阻断批量转账攻击；在医疗场景中，能动态监测患者隐私泄露路径；在云原生环境中，能识别未经注册的微服务接口。<br/>四、从防御体系到智能生态，迈向自适应、可信的数据流转安全<br/>（提示：未来的数据安全，不仅是防止攻击，更是管理信任。）<br/>随着数据流转的速度和范围持续扩大，安全体系正在从“边界式防御”演进为“流动式治理”。API风险监测系统的发展方向，将体现以下三大趋势：<br/>（1）安全与业务一体化<br/>安全不再是附加模块，而成为业务运行的一部分。未来的API监测系统将与企业DevOps体系深度融合，在接口开发、测试、上线、运行的全生命周期中自动介入，实现“安全即服务”。<br/>（2）AI驱动的自适应防护<br/>AI不仅识别攻击，更能预测攻击。通过对历史攻击样本、异常调用行为和行业风险模型的深度学习，系统可主动预警潜在威胁。例如，通过行为预测算法，系统可在“攻击行为发生前10分钟”发出风险提示，提前介入防御。<br/>（3）从技术防护到合规治理<br/>在数据要素化和跨境流通加速的背景下，企业不仅要守住“数据不泄露”的底线，更要证明“数据流转可控”。未来的监测系统将与合规平台联动，自动生成审计报告、风险地图、合规评分，实现从“被动应对监管”到“主动展示合规”的转变。</p><pre><code>   在安全新时代，API风险监测系统不仅是一种防护工具，更是一种企业级治理理念的体现。它让安全不再是静止的围墙，而是一张动态的神经网络，能感知、能判断、能追溯。数据流转安全与静态数据安全并非对立，而是互为补充。静态安全保护数据的“存”，流转安全守护数据的“动”；两者结合，才能构建真正闭环的安全体系。从金融机构的资金交易，到政务平台的数据共享，再到医疗系统的隐私保护——只有当企业将API风险监测纳入数字化战略，才能在“快”与“稳”之间找到平衡，在智能与合规之间实现共赢。安全的未来，不在于堵住每一个漏洞，而在于看见每一次流动。</code></pre>]]></description></item><item>    <title><![CDATA[企业云盘分享的链接如何直接下载？ 胡萝卜]]></title>    <link>https://segmentfault.com/a/1190000047413684</link>    <guid>https://segmentfault.com/a/1190000047413684</guid>    <pubDate>2025-11-20 11:02:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>我们每天都在与大量文件打交道，从云盘中提取资源到分发分享链接，再到对方下载，很多时候看似简单的分享操作，却因格式、权限或操作上的琐碎流程而变得复杂无比。如果能够通过一条分享链接一键实现直接下载，无需额外操作，将极大地提升效率和体验。</p><h2>一、文件分享为何需要直接下载功能？</h2><p>提到分享文件，很多人的第一反应是将文件上传到<a href="https://link.segmentfault.com/?enc=NqhFcIYvnD4mj2VlDN0IaQ%3D%3D.MZGHFRbEKIYYu4rj4cZAaVOcRLoHPTmJ%2FzoWPNw61ZnaV33mv6jIK5tOw%2Fh5DmdhY1Ftw%2Bnb4S8S6QMX7kE6tw%3D%3D" rel="nofollow" target="_blank">企业云盘</a>中，生成一个链接，然后对方再打开这个链接，通过在线页面操作下载文件。乍一看，这似乎已经解决了文件传输的需求。然而，在实际场景中，这种传统的方式虽然通用，但并非最佳。</p><p><strong>场景一：客户体验</strong></p><p>假设你是一位设计师或者摄影师，需要向客户交付大批量高分辨率图片。你通过传统的云盘分享模式生成链接，但客户打开时需要先预览、再选择一个一个下载，操作繁琐。尤其是当文件数量巨大的时候，客户可能会感到沮丧。</p><p>而如果你提供一个直接下载的链接，不需要再打开多余的页面，客户点击一次，即弹出“保存到本地”窗口。这种体验，是不是瞬间好很多？</p><p><strong>场景二：内部协作</strong></p><p>对于企业内部协作而言，一个团队的效率很大程度上依赖于工具。如果团队成员每天都要花时间在寻找文件、验证权限、或者和频繁碰到文件格式不兼容的情况下，无形中增加了大量隐形的时间成本。直接下载的链接模式，一方面可以保障使命必达，另一方面也避免误操作和权限问题，从而简化流程。</p><p><strong>场景三：文件推广</strong></p><p>对于一些面向大众的应用，比如产品文档下载、试用版软件分发或者文件批量分享，直接下载功能可以将转化路径极大缩短。而最终受益的，则是你的业务。</p><p>不难看出，无论是个人用户还是企业用户，直接下载功能都能为文件传递的每一位参与者提供更优的解决方案。</p><h2>二、如何实现文件直接下载？</h2><p>接下来聊聊技术层面的问题。每个云盘工具的链接格式和操作方式都各不相同，但理论上，只要赋予对方下载权限，并将链接格式调整为特定的“直接下载”模式，这个需求是可以被满足的。以下是实现的几个关键点：</p><p><strong>1. 设置公共链接权限</strong></p><p>要实现直接下载，首先要确保对方拥有文件访问权限。在很多云盘工具中，你需要将链接权限设置为“公开访问”或者“拥有链接者可查看/下载”。不过，需要注意的是，这样的设置可能会增加安全风险，因此仅适合非机密的文件传递。</p><p><strong>2. 对链接做格式调整</strong></p><p>有的云盘工具允许用户通过在生成链接的基础上，加上特定参数（比如?download=true），实现直接下载功能。像一些主流的云盘，如Google Drive、OneDrive，甚至需要修改URL中的具体部分才能实现。</p><p>但不得不说，这样的操作对普通用户并不友好，而且一些云盘的API接口复杂，并不是每个人都愿意花时间研究。</p><p><strong>3. 灵活高效的云盘</strong></p><p>如果你已经厌倦了繁琐的操作，那么选择一个更便捷的云盘显然是更好的做法。这里我想推荐的是Zoho网盘，它不仅拥有文件分享和访问的所有主流功能，而且支持更加灵活便捷的直接下载链接生成功能。无论是单个文件，还是一个完整的文件夹，共享和下载都简单化到极致。</p><h2>三、Zoho网盘：直接下载功能的理想选择</h2><p>谈到Zoho，可能有些人对它的其他产品更为熟悉，比如在线邮件系统、CRM工具等，但实际上，Zoho网盘（Zoho WorkDrive）是一个非常值得尝试的云端存储工具，尤其是它在文件分享上的独特优势。</p><p><strong>1. 灵活生成直接下载链接</strong></p><p>Zoho网盘的“文件分享”功能非常直观。你只需要右击相应的文件，选择“生成共享链接”，接着在分享选项中选择“直接下载模式”。无论发送到邮箱还是通过即时通讯工具分享，收到链接的对象都可以一键将文件下载到本地，无需经过繁琐步骤。</p><p><strong>2. 超强的数据保护</strong></p><p>尽管直接下载的分享方式存在一些风险，但Zoho网盘的安全防护做得很到位。你可以针对每个共享链接自定义过期时间、密码保护等措施，确保文件安全性与便捷性兼得。</p><p><strong>3. 支持多种场景</strong></p><p>无论是企业内部协作、批量文件推广还是与客户的文件流转需求，Zoho网盘都能将效率提升到全新高度。特别是当你的文件内容较大时，Zoho网盘优化的下载速度和良好的体验感，能够让分享变得更加畅快无忧。</p><p><strong>4. 多平台无缝衔接</strong></p><p>Zoho网盘的强大之处还体现在它与其他Zoho产品的深度集成上。当你的团队也在使用Zoho邮件或者其他协作工具时，Zoho网盘可以作为一个超级中枢，轻松连接所有成员与内容，全程保持一致性。</p>]]></description></item><item>    <title><![CDATA[Proxmox VE 9.1 发布 - ]]></title>    <link>https://segmentfault.com/a/1190000047413687</link>    <guid>https://segmentfault.com/a/1190000047413687</guid>    <pubDate>2025-11-20 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Proxmox VE 9.1 发布 - 开源虚拟化管理平台</p><p>Proxmox Virtual Environment (PVE): Compute, network, and storage in a single solution.</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=aRlN2%2FQ5N19aRyzCSnBClQ%3D%3D.Oq%2BVPYFfmwJq4VeBXekRp8Jy7jADbOKO9UW%2BoUuUumGPKo1icU%2BUEc4%2BFftGEn7c" rel="nofollow" target="_blank">https://sysin.org/blog/proxmox-ve-9/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=zwikYLIRhlp2VoC1ljcvuA%3D%3D.jqCHM2NuwcvG84yswc0fzk7rA0VQRZ9zFxGQebZEqNQ%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>2025 年 11 月 19 日 Proxmox VE 9.1 发布于。</p><p>核心组件更新如下：</p><ul><li>基于 Debian Trixie (13.2)</li><li>最新的 6.17.2-1 内核作为新的稳定默认版本</li><li>QEMU 10.1.2</li><li>LXC 6.0.5</li><li>ZFS 2.3.4</li><li>Ceph Squid 19.2.3</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413689" alt="Proxmox VE 9.1" title="Proxmox VE 9.1"/></p><h2>Proxmox VE 9.1 更新亮点</h2><p>此次更新亮点功能如下：</p><ul><li>从 OCI 镜像创建 LXC 容器。</li><li>支持 qcow2 格式中的 TPM 状态 (sysin)。</li><li>对虚拟机来宾的嵌套虚拟化进行更精细的控制。</li><li>在 GUI 中提供更详细的网络虚拟化（SDN）堆栈状态报告。</li></ul><p>✅ <strong>从 OCI 镜像创建 LXC 容器</strong></p><ul><li>开放容器倡议（OCI）镜像是一种广泛用于分发系统或应用容器模板的流行格式。</li><li>现在，OCI 镜像可以手动上传或从镜像注册中心下载，并用作 LXC 容器的模板。</li><li>这使得能够从合适的 OCI 镜像创建完整的系统容器。</li><li>同时，支持从合适的 OCI 镜像创建应用容器（技术预览）。</li></ul><p>✅ <strong>支持在 qcow2 格式中存储 TPM 状态</strong></p><ul><li>某些虚拟机来宾工作负载需要附加一个虚拟受信平台模块（TPM），例如较新的 Windows 来宾。</li><li>现在，虚拟 TPM 的状态可以存储在 qcow2 格式中。</li><li>这使得可以在文件级存储（如 NFS 或 CIFS 共享）上为带有 TPM 状态的虚拟机拍摄快照。</li><li>启用了“快照作为卷链”功能（技术预览）的存储现在支持拍摄带有 TPM 状态的虚拟机的离线快照。</li></ul><p>✅ <strong>对虚拟机来宾的嵌套虚拟化进行精细控制</strong></p><ul><li>某些虚拟机来宾工作负载需要访问主机 CPU 的虚拟化扩展，以支持嵌套虚拟化。</li><li>示例包括嵌套的虚拟化程序或启用了基于虚拟化的安全性的 Windows 来宾。</li><li>新的 vCPU 标志允许在与主机 CPU 供应商和代际相对应的 vCPU 类型上启用嵌套虚拟化。</li><li>这可以作为使用完整主机 vCPU 类型的替代方案。</li></ul><p>✅ <strong>在 GUI 中提供更详细的网络虚拟化（SDN）堆栈状态报告</strong></p><ul><li>本地桥接和虚拟网络（VNet）报告当前连接的来宾。</li><li>EVPN 区域还报告学习到的 IP 和 MAC 地址。</li><li>现在，Fabric 成为资源树的一部分，并报告路由、邻居和接口。</li></ul><p>完整更新记录请查阅官方文档。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047078451" alt="Proxmox Logo" title="Proxmox Logo" loading="lazy"/></p><h2>Proxmox VE 简介</h2><p>Proxmox Virtual Environment，简称 Proxmox VE，是一款功能强大的开源服务器虚拟化平台，尽管它常被  VMware ESXi 和 KVM  等主流虚拟化方案所掩盖，凭借其免费和开源的特性，它非常适合家庭实验室爱好者和中小企业，这类用户的需求通常不同于大型企业。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047078457" alt="Proxmox VE Diagram" title="Proxmox VE Diagram" loading="lazy"/></p><p>Proxmox Virtual Environment</p><p>计算、网络与存储的一体化解决方案</p><p>Proxmox Virtual Environment（Proxmox VE）是一个完整的开源企业虚拟化服务器管理平台。它将 KVM  虚拟机管理程序与 Linux 容器（LXC）、软件定义的存储与网络功能紧密集成于一个平台 (sysin)。借助集成的基于 Web  的用户界面，您可以轻松管理虚拟机与容器、群集高可用性以及集成的灾难恢复工具。</p><p>企业级功能与 100% 纯软件架构使 Proxmox VE 成为虚拟化 IT  基础架构的理想选择，能够优化现有资源、提高效率，同时成本最小。您可以轻松虚拟化对性能要求极高的 Linux 和 Windows  应用负载，并可根据需求动态扩展计算与存储资源，确保数据中心具备未来发展所需的灵活性。</p><h2>下载地址</h2><p><strong>Proxmox VE 9.1</strong> ISO Installer</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=a0mcykF8yyAoQ5Ca54LR%2Fw%3D%3D.2qgd%2FJaOtlT0UtdZ03nIfZPxMUI6Yx2R%2FTQdFIeIOa%2FietDwmcjMVKaBDtb%2B49Uh" rel="nofollow" target="_blank">https://sysin.org/blog/proxmox-ve-9/</a></li><li>Version: 9.1-1</li><li>File Size: 1.83 GB</li><li>Last Updated: November 19, 2025</li><li>SHA256SUM: include</li><li>Based on Debian 13.2</li></ul><p>相关产品：</p><ul><li><a href="https://link.segmentfault.com/?enc=S0F2R9YBHyXQO%2BjMXihkBQ%3D%3D.jpvQtqMXQaS4rdRbwcDwHHmi%2F3EzGW%2F0fpblZ1fayblNOvCsZ%2FNmwlk%2F2fnEMtV1" rel="nofollow" target="_blank">Proxmox Backup Server 4.0 正式版发布 - 开源企业级备份解决方案</a></li><li><a href="https://link.segmentfault.com/?enc=NsWEUaaG3zXU%2Fu64iTxbTQ%3D%3D.jkAEtkEs4EacGyOVMFCEePTHkCZcC8f3r0dgRpf29iLMiJYkHHOgmsHoMohc4fCH" rel="nofollow" target="_blank">Proxmox Mail Gateway 9.0 正式版发布 - 全面的开源邮件安全平台</a></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=PM2Dqzcz9GfMvCNzONL8xQ%3D%3D.xM4uqp4uVzH8330w0KIuyeUgBeD4COZkNIDPBL5HFeU%3D" rel="nofollow" target="_blank">Linux 产品链接汇总</a></p>]]></description></item><item>    <title><![CDATA[AI杀死了AI！Cloudflare全球]]></title>    <link>https://segmentfault.com/a/1190000047413338</link>    <guid>https://segmentfault.com/a/1190000047413338</guid>    <pubDate>2025-11-20 10:08:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：定慧</p><p>【新智元导读】一次「常规更新」搞崩半个地球，Cloudflare CTO紧急谢罪：我们搞砸了！Cloudflare自杀式Bug引发连锁反应，波及全球20%网站。当数百万爬虫撑爆了防御名单，Cloudflare的崩溃揭示了AI时代最深的基建隐忧，人类还能跟得上AI进化的脚本吗？</p><p>果然这个世界建立在脆弱性之上。</p><p>昨天，2025年11月18日，全球AI数字生态系统经历了一次近乎心跳骤停般的休克。</p><p>Cloudflare崩溃了！</p><p>然后全球五分之一的互联网服务几乎全部宕机，尤其是，<strong>AI巨头集体断网！ChatGPT、X全线崩盘！</strong></p><p>当你还在拿着这个图调侃的时候。。。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413340" alt="" title=""/></p><p>殊不知，真正的情况是，Cloudflare现在互联网真正的底座。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413341" alt="" title="" loading="lazy"/></p><p>马斯克在之前亚马逊宕机时还调侃，这次终于是风水轮流转！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413342" alt="" title="" loading="lazy"/></p><p>只不过，老马没想到的是，这次CF的影响直接让自己家服务也宕机了～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413343" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413344" alt="" title="" loading="lazy"/></p><p>Cloudflare的CTO赶紧发了个申明：是我们的错误，立正挨打。</p><p>下面会详细介绍这次引发故障的原因，简直就是草台班子级别，只能说人类社会用规则来和计算机打交道还是太脆弱了！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413345" alt="" title="" loading="lazy"/></p><p>先说说这次事故的影响。</p><p>这起事故影响范围之广前所未见，被称为<strong>「半个互联网的停摆」毫不夸张</strong>——<strong>约20%的网站依赖Cloudflare提供服务</strong>。</p><p><strong>Cloudflare还有个称号叫做</strong>赛博活佛，特别是在极客群体中，很多服务，如果个人用都是免费的。</p><p>这里作为对比，必须夸一下咱们国内的基建服务了，不管是微信、B站、视频网站，你很少碰到如此级别大规模的故障。（ToB的服务不细讲，这个离普通消费者也很远）</p><p>故障高峰时，网站故障追踪平台Downdetector收到了<strong>累计逾210万条</strong>报错反馈，成为近年来最严重的基础设施级中断之一。</p><p>包括亚马逊、Spotify、Zoom、Uber等知名服务也受到波及（部分功能异常或加载缓慢）。</p><p>Cloudflare作为支撑全球海量流量的「隐形基建」，一次失误便牵一发而动全身，令股价盘中一度重挫约7%。</p><p>更令人深思的是，一些本用于监测网络故障的工具（如Downdetector）因本身也使用Cloudflare，甚至在事故中一同瘫痪—整个互联网生态对单一底层的依赖程度，由此可见一斑。</p><p>更「细思极恐」的是，当Cloudflare的工程师想要打开ChatGPT来修复故障时，AI也宕机了～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413346" alt="" title="" loading="lazy"/></p><p>很多网友都形容那宕机的三个小时，如此「黑暗」，就像回到了远古时期。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413347" alt="" title="" loading="lazy"/></p><p>Cloudflare这次导致全球断网的技术故障，其实是一次典型的「好心办坏事」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413348" alt="" title="" loading="lazy"/></p><p>网友们制作的恶搞图</p><p>简单来说，就是工程师试图升级安保系统的权限，却意外让负责安检的软件「吓晕」了。</p><p><strong>（如果是AI来操作，以硅基同步的能力和运算的能力，大概率不会出这种岔子，这也就是碳基人类写下的固定规则才能导致了，还是人类规则太脆弱了。这里让我联想到马斯克为啥一直坚持FSD使用纯视觉，就是人类你不可能遍历所有驾驶过程，就像这次CF的故障，没有工程师能预先为这种场景写下规则）</strong></p><p>根据Cloudflare官方博客的复盘，事情是这样发生的：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413349" alt="" title="" loading="lazy"/></p><p>起因是系统「大扫除」。</p><p>工程师原本在进行一项常规的维护工作，目的是为了提高安全性。他们调整了数据库的权限，想把原本公用的「系统账号」改成责任更明确的「个人账号」。</p><p>然后这个看似无足轻重的操作，触动了隐藏在系统中「旧伤」。</p><p>系统里潜伏着一段很老的代码，它负责生成一份用来识别网络机器人的「特征名单」（Feature File）。</p><p>这段代码以前只在一个默认的数据库里找名单，所以没问题。</p><p>但这次权限升级后，它突然能看到另一个备份数据库了。</p><p>由于代码里没写清楚「只看哪一个」，它傻乎乎地把两边的名单都抓取了过来，名单被「膨胀」了。</p><p>这导致那份原本精简的「特征名单」瞬间膨胀，内容重复了一倍。</p><p>结果就是，保安「罢工」。</p><p>Cloudflare负责在全球各地转发流量的核心软件有一个硬性规定：为了保证速度，名单长度不能超过200条。</p><p>当这份意外「发福」的名单被推送到全球服务器时，软件发现名单太长读不完，直接触发了内存溢出保护机制（Panic），也就是彻底崩溃。</p><p><strong>为了安全起见，它切断了所有连接。</strong></p><p>简单说就是，本来机器数据库权限不够，调整后，它突然权限高了点，然后也没有为这个情况提前写下判断代码。</p><p><strong>打个再通俗的比方（可能不是那么准确）。</strong></p><p>这就好比大楼物业给保安发了一副新眼镜（升级权限），本意是让他看得更清楚。</p><p>结果因为新眼镜度数没调好，保安看手里的「访客黑名单」时出现了重影，原本100人的名单在他眼里变成了200人。</p><p>保安的脑容量（系统限制）记不住这么多人，瞬间由于信息过载而「死机」晕倒，<strong>导致大楼门禁系统自动锁死，把所有访客（包括X和ChatGPT的用户）都关在了门外。</strong></p><p>不过目前问题已经修复了（其实不是啥大问题，就是逻辑改改就行）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413350" alt="" title="" loading="lazy"/></p><p>如果不只是把这次事故看作一个单纯的技术故障，而是放在2025年「AI疯狂吞噬数据」的背景下去看，你会发现这充满了黑色的讽刺意味。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413351" alt="" title="" loading="lazy"/></p><p><strong>AI杀死AI</strong></p><p>导致这次崩溃的核心组件是「机器人管理系统（Bot Management）」。</p><p>在2025年，这个系统的主要假想敌是谁？<strong>正是AI爬虫。</strong></p><p>随着大模型训练对数据的极度渴求，互联网上充斥着无数自动化的AI抓取程序。</p><p>Cloudflare作为「守门人」，必须不断升级其算法来区分「真人」和「AI机器人」。</p><p><strong>特征文件（Feature File），对就是</strong>报告中提到的那个导致崩溃的「特征文件」，实际上就是机器学习模型用来判断流量性质的「参数集」。</p><p>每一个「特征（Feature）」都是一个判断维度（比如鼠标移动轨迹、点击频率、IP行为模式等）。</p><p>为了应对越来越狡猾的AI机器人，Cloudflare的防御系统变得越来越复杂，需要调用的「特征」越来越多。</p><p>这次故障的直接原因就是数据库错误地吐出了过多的特征数据，导致<strong>防御系统的「大脑」过载</strong>。</p><p><strong>这不是一次普通的软件崩溃，这是「数字免疫系统」在试图升级以对抗AI病毒时，因自身的排异反应而休克。</strong></p><p>这次事件最荒诞的地方在于<strong>受害者名单</strong>。</p><p><strong>OpenAI、xAI、Perplexity：</strong>这些是全球最大的AI公司，它们同时扮演了两个角色：</p><p>它们的爬虫在全网搜刮数据，迫使Cloudflare建立更复杂的防御系统（即这次崩溃的源头）。</p><p>它们自己也极其依赖Cloudflare来防止被别人攻击或滥用。</p><p><strong>结果呢？</strong></p><p>Cloudflare为了防御AI抓取行为而维护的系统，因为一次配置错误，反过来「杀死了」最顶级的AI服务商。</p><p>这就像是为了防止野兽入侵而把城墙修得太高太重，结果城墙倒塌，把住在城里的国王（AI巨头）给压垮了。</p><p>这揭示了AI时代基础设施的<strong>内卷化困境</strong>——为了对抗技术的滥用，我们不得不把基础设施造得越来越复杂、越来越脆弱。</p><p>你问这和AI有什么关系，或许这就是AI时代的「技术债」。</p><p>这里有一个更深层的隐喻：<strong>「特征膨胀」</strong>。</p><p>在传统的软件工程中，逻辑通常是线性的。</p><p>但在涉及AI和机器学习的防御体系中，系统依赖于成百上千个「特征」来进行概率判断。</p><p>这次故障是因为特征数量突破了<strong>200个</strong>的硬编码限制而引发的。</p><p>这暴露了一个问题：我们正在构建一种人类难以完全掌控的「黑箱基建」。</p><p>为了拦截智能程度极高的AI机器人，防御规则不能再是简单的黑白名单，而必须是动态的、基于行为分析的复杂模型。</p><p>这种<strong>复杂度的指数级上升</strong>，意味着未来类似的「不可预测的崩溃」会越来越多。</p><p>我们正在用复杂的AI（防御）去对抗复杂的AI（进攻），而夹在中间的，是脆弱的物理互联网。</p><p>这次宕机不仅是一个配置错误，它是人类互联网为了适应AI寄生而进行的一次痛苦痉挛。</p><p>它是「矛」（AI抓取）与「盾」（AI防御）在无限升级的军备竞赛中，把战场（互联网基础设施）给撑爆了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413352" alt="" title="" loading="lazy"/></p><p>但是，这波也有用AI来打败AI的正面例子。</p><p>比如，吴恩达团队就在Cloudflare宕机的过程中，用AI快速实现了Cloudflare功能的克隆版本，成为最早一批恢复运行的网站。</p><p>属实是用魔法打败魔法了！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413353" alt="" title="" loading="lazy"/></p><p>最后再放一个彩蛋。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413354" alt="" title="" loading="lazy"/></p><p><strong>彩蛋：元凶被原地解雇</strong></p><p>X上这位名为Rob Hallam的哥们发了个帖子。</p><p>说他正是那位搞崩全球互联网的工程师（可能是之一）。</p><p>自称是，能用单个正则表达式让20%互联网瘫痪，哈哈哈</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413355" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413356" alt="" title="" loading="lazy"/></p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=L0AB6wvWR%2B7FF2pJwbwSIw%3D%3D.q6B1xJndI2nPMD83mkWcRFPuYSt2h1j0xELfYoVytEqmUbFbwO0Ykp2N77Ot0vEEQ%2FghLsFa2x0mngSb9rHzIA%3D%3D" rel="nofollow" target="_blank">https://blog.cloudflare.com/1...</a></p>]]></description></item><item>    <title><![CDATA[不止于“锁”：免费SSL证书如何为你的小]]></title>    <link>https://segmentfault.com/a/1190000047413276</link>    <guid>https://segmentfault.com/a/1190000047413276</guid>    <pubDate>2025-11-20 10:08:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、核心保障：数据安全与用户信任的基石</strong></p><p>这可以说是SSL证书最直接也是最重要的作用。<br/><img width="723" height="692" referrerpolicy="no-referrer" src="/img/bVdmPz6" alt="" title=""/></p><p><a href="https://link.segmentfault.com/?enc=9SS779LsyUHsZm8fTqSdiw%3D%3D.%2F8%2FNOR0aUzOYVlTMSRQOZhKmv6WJXH%2FXaFU4a9l%2BKO6BRYv4jC9ladLtujHtkWfHKtZXdeVsyiE78GjDGsPUSQ%3D%3D" rel="nofollow" target="_blank">https://www.joyssl.com/certificate/select/free.html?nid=59</a></p><p><strong>注册码230959⬆️</strong></p><ul><li><strong>加密数据传输，守护用户隐私</strong>：当用户访问你的网站并输入任何信息（即使是简单的留言或邮箱订阅）时，SSL证书会在用户的浏览器和你的服务器之间建立一条加密通道。没有这条通道，这些数据就可能在传输过程中被不法分子轻易截获。这对于保护用户隐私至关重要。</li><li><strong>验证网站身份，抵御钓鱼风险</strong>：SSL证书由权威的证书颁发机构（CA）签发，意味着这个“锁”不仅是加密工具，也是一个身份认证。它能向用户证明他们正在访问的是真正的你的网站，而不是冒充的钓鱼网站。这对于建立用户信任感非常有帮助。</li></ul><p><strong>二、隐形推手：提升搜索引擎排名与可见度</strong></p><p>这一点对于希望获得更多流量的小型网站来说尤其关键。</p><ul><li><strong>SEO排名优势</strong>：像谷歌、百度这样的主流搜索引擎早已明确表示，HTTPS是其搜索排名算法的一个重要考量因素。这意味着，在其他条件相同的情况下，部署了SSL证书的网站更有可能获得靠前的排名，从而获得更多的自然流量。</li><li><strong>避免“不安全”警告</strong>：没有SSL证书的网站，现代浏览器会明确标记为“不安全”，并在地址栏显示红色警告或感叹号。这种视觉提示会极大地打击用户的访问意愿，导致潜在访客流失。有了SSL证书，就能有效避免这种情况。</li></ul><p><strong>三、成本效益：零门槛实现专业级安全防护</strong></p><p>过去，高昂的成本是阻碍小型网站部署SSL证书的主要原因，但现在情况已经完全不同。</p><ul><li><strong>成熟可靠的免费方案</strong>：以 <strong>Let's Encrypt</strong> 为代表，它们提供了完全免费且高度自动化的证书申请和续期流程，极大地降低了技术门槛和使用成本。虽然其有效期较短（通常为90天），但配合自动化工具可以实现近乎“一次配置，长期有效”。此外，如 <strong>JoySSL证书助手</strong> 等国内平台还提供了免费通配符证书申请，进一步方便了有多个子域名的用户。</li><li><strong>满足绝大多数场景需求</strong>：对于个人博客、作品集、小型企业官网等非电商、非金融类网站，免费的域名验证（DV）SSL证书提供的加密强度和身份验证级别已经完全足够。它与付费证书在安全性上没有差异。</li></ul><p><strong>四、实践指南：选择、部署与维护的最佳实践</strong></p><p>要让免费SSL证书真正发挥价值，正确的操作同样重要。</p><ul><li><strong>如何选择</strong>：根据你的网站类型选择。个人站点首选 <strong>Let's Encrypt</strong>；如果拥有多个子域名，可以寻找支持免费通配符证书的平台；小型企业官网若想展示更多组织信息，甚至可以关注一些平台提供的免费基础型OV证书。</li><li><strong>如何部署</strong>：现在获取和部署SSL证书非常方便。很多云服务商、域名注册商都提供了一键申请和部署的功能。你也可以使用服务器管理面板（如宝塔）、CMS系统插件等方式快速完成安装。</li><li><p><strong>注意事项</strong>：</p><ul><li><strong>定期检查与续期</strong>：务必设置好自动续期或手动记录到期时间，防止因证书过期导致网站无法访问。</li><li><strong>修复混合内容错误</strong>：安装后，要检查网站内所有的图片、脚本、样式表是否都通过HTTPS加载。如果存在HTTP资源，浏览器仍会提示不安全。可以通过浏览器开发者工具查找并修复这些问题。</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[HTTPS开头的小绿锁，到底在守护什么？]]></title>    <link>https://segmentfault.com/a/1190000047413278</link>    <guid>https://segmentfault.com/a/1190000047413278</guid>    <pubDate>2025-11-20 10:07:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>HTTPS开头的小绿锁，到底在守护什么？</strong></p><p>每天浏览网页时，我们早已习惯了地址栏里那个小小的锁形图标。它静默无声，以至于我们常常忽略它的存在。但你是否曾想过，这把<strong>小绿锁</strong>，究竟在为我们守护着什么？</p><p>它不仅仅是一个装饰性的图标，而是网络安全的一道重要防线。今天，我们就来揭开这把“守护之锁”的神秘面纱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413280" alt="21.jpeg" title="21.jpeg"/></p><p><strong>一、小绿锁是谁？—— 安全连接的“守门人”</strong></p><p>简单来说，当你在浏览器地址栏看到小绿锁和“HTTPS”前缀时，意味着你与当前网站之间的连接是安全的、加密的。</p><p><strong>HTTP</strong>：是互联网数据传输的基础协议，但信息以明文传输，如同邮寄一张明信片，途中的任何人都可以窥视其内容。</p><p><strong>HTTPS</strong>：= HTTP + S（Secure Socket Layer / TLS）。它相当于为“明信片”加上了一个坚固的保险箱。小绿锁，就是这个保险箱的“锁”，象征着安全连接已建立。</p><p><strong>二、小绿锁的四大守护使命</strong></p><p>这把锁守护的，远比你想象的更多。它是一位尽职尽责的卫士，主要在四个关键领域为我们提供保护。</p><p><strong>🛡️ 守护一：你的隐私与敏感数据</strong></p><p>这是小绿锁最核心的使命——加密。它确保了你与他人之间“说悄悄话”时，不会被第三者偷听。</p><p>它保护的具体信息包括：</p><p><strong>登录凭证</strong>：你的用户名和密码。</p><p><strong>财务信息</strong>：信用卡号、网银账户、支付密码。</p><p><strong>个人身份信息</strong>：身份证号码、手机号、住址。</p><p><strong>私人通信</strong>：在聊天应用、邮件、表单中输入的任何敏感内容。</p><p><strong>没有小绿锁的后果</strong>：如果你在一个“不安全”（HTTP）的网站上输入密码，黑客在同一个公共Wi-Fi下，可以像从空气中“嗅探”一样，轻松截获你的明文密码。</p><p><strong>🛡️ 守护二：信息的完整性与真实性</strong></p><p>小绿锁确保你收到的信息，在传输过程中没有被篡改或调包。</p><p><strong>它如何工作：</strong></p><p>在加密的基础上，安全协议会验证数据的“指纹”。如果数据在传输中被第三方恶意修改（例如，在捐款网站上，将你输入的“100元”篡改为“1000元”），接收方立刻就能发现，并丢弃这份被污染的数据。</p><p><strong>没有小绿锁的后果</strong>：你下载的软件可能被植入病毒，你看到的新闻内容可能被中间人随意篡改，你支付的金额可能被恶意修改。</p><p><strong>🛡️ 守护三：网站的身份真实性</strong></p><p>小绿锁是一个信任的印章，它向用户证明：“<strong>你正在访问的网站，就是它声称的那个网站，而不是一个高仿的钓鱼网站。</strong>”</p><p><strong>它如何工作：</strong></p><p>SSL证书在颁发前，证书颁发机构（CA）会对申请者的身份进行不同严格程度的验证。</p><p><strong>域名验证（DV）</strong>：确认申请人控制着该域名。这是最常见的小绿锁基础。</p><p><strong>组织验证（OV）与扩展验证（EV）</strong>：会严格核实企业的真实性和合法性。点击小绿锁，可以查看证书详情，看到网站所有者的公司名称。</p><p>没有小绿锁的后果：<strong>你很容易掉入钓鱼网站的陷阱</strong>。比如，你访问的 www.abc-bank.com 可能是一个精心伪装的假网站，专门用来盗取你的银行账户信息。</p><p><strong>🛡️ 守护四：用户与网站的双向信任</strong></p><p>这把锁不仅保护用户，也为网站所有者建立了可信赖的品牌形象。</p><p><strong>对用户而言</strong>：小绿锁提供了心理安全感，让你可以放心地进行登录、浏览和交易。</p><p><strong>对网站主而言</strong>：拥有小绿锁意味着尊重并保护用户隐私，这是一种专业和负责任的体现，能有效提升用户停留时间、转化率和忠诚度。</p><p><strong>没有小绿锁的后果</strong>：浏览器明确的“不安全”警告会吓跑大部分用户，导致流量和业务直接流失。</p><p><strong>三、如何判断连接是否真正安全？</strong></p><p>请养成查看地址栏的习惯：</p><p><strong>安全连接</strong>：🔒 HTTPS + 小锁图标 + 可能显示的“安全”字样。</p><p>不安全或部分安全连接：</p><p><strong>⚠ 三角感叹号</strong>：通常表示网站是HTTPS，但加载了不安全的资源（混合内容），安全性打折扣。</p><p><strong>“不安全”文字</strong>：明确表示该网站使用HTTP协议，连接完全未加密。</p><p><strong>最佳实践</strong>：在进行任何登录或支付操作前，请务必确认小绿锁的存在。</p><p><strong>总结</strong><br/>HTTPS开头的小绿锁，绝不是一个可有可无的图标。它是一位沉默的守护者，同时肩负着四大使命：</p><p><strong>加密者</strong>：守护你的隐私数据，防止窃听。</p><p><strong>校验者</strong>：守护信息的完整性，防止篡改。</p><p><strong>认证者</strong>：守护网站的真实身份，防止钓鱼。</p><p><strong>信任基石</strong>：守护用户与网站之间的双向信任。</p><p>它守护的，是数字世界里最基本也最珍贵的资产——安全与信任。下次当你看到这把小绿锁时，可以放心地知道，你正受到现代密码学技术的坚实保护。</p>]]></description></item><item>    <title><![CDATA[为何安装了SSL证书，网站仍被标记“不安]]></title>    <link>https://segmentfault.com/a/1190000047413281</link>    <guid>https://segmentfault.com/a/1190000047413281</guid>    <pubDate>2025-11-20 10:06:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数字化时代，网络安全已成为网站运营的基石。许多网站管理员已经意识到为网站安装SSL证书，将HTTP升级为HTTPS的重要性，期待在浏览器地址栏看到那把象征安全的“小锁”。然而，不少人却困惑地发现，即便成功安装了证书，浏览器依然会弹出“不安全”的红色警告。这并非SSL证书本身无效，而是背后隐藏着更深层次的原因。要理解并解决这一问题，我们需要从几个关键层面进行剖析。<br/><img width="723" height="400" referrerpolicy="no-referrer" src="/img/bVddlwr" alt="" title=""/></p><h4><strong>一、证书自身的问题：身份与时效的验证</strong></h4><h4>申请办法：打开JoySSL官网，填写注册码230970获取技术支持<a href="https://link.segmentfault.com/?enc=LTdnM8nREcK%2Fqw9Xfzz9kg%3D%3D.vRI3Lc%2BC2f6vZxbj4ATsg16BVY8IP0Mq%2BMrUx5r8ekH3ZGqNlCljpHBY2cWSn%2FbbLmKKRLKDWKqHtp6Y6pArew%3D%3D" rel="nofollow" target="_blank">申请入口</a></h4><p>首先，最直接的原因可能出在SSL证书本身。</p><ol><li><strong>证书已过期或未生效</strong>：SSL证书并非永久有效，它们都有明确的有效期，通常为一年或更短。就像食品有保质期一样，过期的证书会立即被浏览器判定为无效，从而发出严重警告。同样，如果服务器时间设置错误，导致当前时间不在证书的生效期内，也会触发警报。</li><li><strong>证书与域名不匹配</strong>：每个SSL证书都是针对特定域名签发的。如果您为 <code>www.example.com</code> 购买了证书，但用户访问的是 <code>example.com</code>（或无<code>www</code>版本），浏览器会认为域名不匹配，提示不安全。解决方法是确保证书覆盖所有需要保护的域名变体，或使用支持多域名的通配符证书。</li><li><strong>证书链不完整或不受信任</strong>：SSL证书的信任建立在一条“信任链”之上，从根证书、中间证书到您的站点证书。如果网站在配置时，只部署了站点证书，而遗漏了中间证书，浏览器将无法追溯到受信任的根证书，从而判定其为“不受信任的发行机构”并发出警告。</li></ol><h4><strong>二、网站内容的“混合”风险</strong></h4><p>这是最常见且最容易被忽略的原因。即使您的网站主文档是通过安全的HTTPS加载的，但如果页面中包含了通过不安全的HTTP协议加载的子资源，就会构成“混合内容”。</p><p>浏览器会将整个页面视为“不安全”，因为攻击者可能篡改那些通过HTTP加载的资源，从而窃取信息或进行恶意操作。这些资源通常包括：</p><ul><li><strong>图片</strong>：通过<code>&lt;img src="http://..."&gt;</code>引用的图片。</li><li><strong>脚本</strong>：通过<code>&lt;script src="http://..."&gt;</code>引用的JavaScript文件。</li><li><strong>样式表</strong>：通过<code>&lt;link href="http://..."&gt;</code>引用的CSS文件。</li><li><strong>iframe</strong>：通过HTTP嵌入的第三方内容。</li></ul><p><strong>解决方案</strong>：使用浏览器的开发者工具（按F12键），查看控制台或网络选项卡，将所有被标记为不安全的资源链接，全部修改为HTTPS协议，或者使用相对路径（<code>//example.com/resource.js</code>）。</p><h4><strong>三、服务器配置与缓存作祟</strong></h4><ol><li><strong>服务器配置错误</strong>：即使证书文件正确，Web服务器的配置也至关重要。例如，在Nginx或Apache中，需要正确指定证书文件和私钥的路径。配置不当会导致服务器无法正常提供HTTPS服务。</li><li><strong>浏览器或中间设备缓存</strong>：浏览器为了加速访问，会缓存大量数据，其中就包括过去访问该网站时的“不安全”状态。即使您已修复所有问题，旧的缓存仍可能让警告持续一段时间。尝试清除浏览器缓存，或使用“无痕模式”访问，可以验证问题是否已解决。</li></ol><h4><strong>四、更深层次的安全策略：HSTS的缺失</strong></h4><p>HSTS是一种重要的Web安全策略机制。它告诉浏览器，在接下来的一段时间内，只能使用HTTPS与该网站通信，即使用户手动输入HTTP地址，浏览器也会自动转为HTTPS。</p><p>如果您的网站没有启用HSTS，用户在首次访问时，仍有可能通过HTTP入口进入，从而被劫持或降级攻击。而启用HSTS后，不仅能提升安全性，还能让浏览器更“坚定”地认可您的HTTPS状态。您可以通过在服务器响应头中添加 <code>Strict-Transport-Security</code> 来启用它。</p><h4><strong>总结与行动指南</strong></h4><p>当您的网站出现“不安全”警告时，请不要急于责怪证书提供商，而应遵循一个系统的排查流程：</p><ol><li><strong>检查证书状态</strong>：点击地址栏的“不安全”标识，查看证书详情，确认其有效期、颁发给的对象以及颁发者是否可信。</li><li><strong>审查混合内容</strong>：打开开发者工具，仔细检查控制台是否有关于混合内容的错误报告。</li><li><strong>清除缓存测试</strong>：使用无痕窗口访问网站，看问题是否依旧存在。</li><li><strong>验证服务器配置</strong>：使用在线SSL检测工具，它们能提供一份详细的报告，指出证书、证书链和服务器配置中存在的所有问题。</li><li><strong>考虑启用HSTS</strong>：在确保网站所有资源都已HTTPS化后，启用HSTS以提供更深层的保护。</li></ol><p>总之，安装SSL证书只是迈向网站安全的第一步，而非终点。它像是一把精密的锁，但如果您没有把门上好（服务器配置），或者窗户还开着（混合内容），安全便无从谈起。只有全面排查、精细配置，才能让那把象征着信任与安全的“小锁”稳固地出现在用户面前，真正赢得他们的信赖。</p>]]></description></item><item>    <title><![CDATA[免费的SSL证书可以用吗? 傻傻的开心果]]></title>    <link>https://segmentfault.com/a/1190000047413285</link>    <guid>https://segmentfault.com/a/1190000047413285</guid>    <pubDate>2025-11-20 10:05:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、先搞懂：<strong>SSL 证书到底是啥？为啥非装不可？</strong></p><p>你可能没意识到，当浏览器显示 <strong>“不安全” 警告</strong>时，用户会毫不犹豫关掉你的网站 —— 而<strong>SSL 证书 </strong>，就是解决这个问题的 “网站身份证”。它的核心作用有三个：</p><ul><li><strong>数据加密</strong>：像给用户和网站的沟通加了把锁，防止密码、支付信息被黑客截取；</li><li><strong>身份认证</strong>：证明你的网站是 “正经平台”，不是钓鱼网站冒充的；</li><li><strong>SEO 加分</strong>：谷歌、百度等搜索引擎明确表示，带<strong>HTTPS</strong>（装了 SSL）的网站排名更靠前。</li></ul><p>简单说，没有 SSL 证书的网站，就像开了家没挂营业执照、大门敞开的店铺，用户不敢进，搜索引擎也不待见。</p><p>二、免费 SSL 证书：<strong>香还是坑？</strong></p><p>市面上最火的免费 SSL 证书当属<strong>Let’s Encrypt</strong>，还有阿里云、腾讯云提供的免费版。它们真的能放心用吗？答案是：<strong>分情况！</strong></p><p>✅ 免费 SSL 的 3 个 “真香场景”</p><ol><li><strong>个人博客 / 静态网站</strong>：如果你的网站只是分享文章、展示作品，没有用户登录、支付功能，免费 SSL 完全够用。Let’s Encrypt 支持<strong>自动续期</strong>，配置简单，零成本就能搞定 “HTTPS” 标志；</li><li><strong>初创小微企业试水</strong>：刚起步的小网站，预算有限又想提升可信度，免费 SSL 是过渡阶段的绝佳选择，先解决 “有无” 问题，再考虑 “好坏”；</li><li><strong>测试环境 / 临时项目</strong>：开发中的网站、短期活动页面，用免费 SSL 避免 “不安全” 警告，项目结束后无需额外处理，性价比拉满。</li></ol><p>❌ 免费 SSL 的 4 个 “隐形坑”</p><ol><li><strong>有效期极短，续期麻烦</strong>：免费证书有效期通常只有<strong>90 天</strong>，虽然支持自动续期，但如果服务器配置出错、域名解析变更，很容易导致续期失败，网站突然变回 “不安全”；</li><li><strong>不支持多域名 / 通配符</strong>：如果你的网站有多个子域名（比如blog.xxx.com、shop.xxx.com），免费 SSL 需要逐个申请，管理繁琐；而付费证书的<strong>通配符证书（*.xxx.com）</strong> 能一次性覆盖所有子域名；</li><li><strong>无技术支持，出问题自救</strong>：免费 SSL 没有官方技术团队，遇到配置失败、浏览器不识别等问题，只能靠搜索教程、论坛求助，对于不懂技术的网站主来说，可能要折腾好几天；</li><li><strong>信任等级较低，影响转化</strong>：部分免费 SSL 的根证书不被少数老旧浏览器支持，虽然比例不高，但可能导致部分用户无法正常访问；更重要的是，电商、金融等需要用户付费、提交敏感信息的网站，付费 SSL 的<strong>EV 证书（显示绿色地址栏 + 公司名称）</strong> 能显著提升用户信任度，而免费 SSL 只有基础的<strong>DV 证书（仅验证域名所有权）</strong> 。</li></ol><p><img width="530" height="343" referrerpolicy="no-referrer" src="/img/bVdbwbw" alt="" title=""/></p><p>三、终极选择指南：<strong>到底该用免费还是付费？</strong></p><table><thead><tr><th>场景</th><th>推荐选择</th><th>核心原因</th></tr></thead><tbody><tr><td>个人博客、静态展示网站</td><td>免费 SSL</td><td>零成本，满足基础安全需求</td></tr><tr><td>初创企业官网（无交易功能）</td><td>免费 SSL 过渡，后期升级付费</td><td>控制成本，同时提升网站可信度</td></tr><tr><td>电商、金融、付费会员网站</td><td>付费 SSL（EV/OV 证书）</td><td>绿色地址栏增强信任，支持多域名，有技术保障</td></tr><tr><td>多子域名网站（3 个以上）</td><td>付费通配符 SSL</td><td>统一管理，避免重复申请</td></tr><tr><td>政府、教育、医疗等权威网站</td><td>付费 OV/EV 证书</td><td>符合行业规范，提升官方公信力</td></tr></tbody></table><p>四、重要提醒：<strong>免费 SSL 这么用才安全！</strong></p><ol><li>开启<strong>自动续期</strong>：在服务器配置中设置自动续期脚本，避免证书过期导致网站 “不安全”；</li><li>强制<strong>HTTPS 跳转</strong>：通过配置服务器，将所有 HTTP 请求自动跳转至 HTTPS，防止用户访问未加密的页面；</li><li>定期检查<strong>证书状态</strong>：用 SSL 检测工具（比如<strong>SSL Labs</strong>）定期扫描，确认证书有效、配置正确，避免出现漏洞；</li></ol><ol start="4"><li>备份<strong>证书文件</strong>：将申请到的证书文件备份，防止服务器迁移、重装系统时丢失，导致需要重新申请。</li></ol><p>总结</p><p>免费 SSL 并非 “洪水猛兽”，它是<strong>低成本实现网站 HTTPS</strong>的好工具，适合非盈利、非交易类的基础网站；但如果你的网站涉及交易、敏感信息收集，或者需要提升用户信任度、简化管理，<strong>付费 SSL 的价值绝对值得投资</strong>—— 毕竟，网站的安全和用户信任，从来都不是 “免费” 能完全覆盖的。</p><p>与其纠结 “免费能不能用”，不如根据自己的网站需求选择：<strong>基础需求选免费，核心业务选付费</strong>，才是最理性的选择！</p>]]></description></item><item>    <title><![CDATA[小米技术教父，离职创业了！ CodeSh]]></title>    <link>https://segmentfault.com/a/1190000047413294</link>    <guid>https://segmentfault.com/a/1190000047413294</guid>    <pubDate>2025-11-20 10:04:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近，科技圈的又一位技术大佬被曝入局创业了。</p><p>没错，他就是小米的技术教父、小米前副总裁，同时也是雷军在武大念书时的下铺兄弟，并且在拍照时可以和雷军并列而坐的技术大佬崔神：</p><p><strong>崔宝秋</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413296" alt="崔宝秋（第一排右二）" title="崔宝秋（第一排右二）"/></p><p>那这一次创业，据说大佬瞄准的也是当下炙手可热的「<strong>具身智能</strong>」赛道，创业方向为「<strong>家庭服务机器人</strong>」方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413297" alt="" title="" loading="lazy"/></p><p>并且在这次创业消息被曝之前，据传大佬已与多家顶级 VC 深入接洽，融资等进展顺利。</p><p><strong>说回到小米的发展历程，崔宝秋是其绕不开的一个人</strong>。</p><p>大家都知道，小米这个公司以产品见长。</p><p>提到小米产品，大家基本上都耳熟能详。从智能手机到影音数码，从家电生活到智能家居，另外各种软件、平台、云服务、OS等都在持续发力，包括现在又在全速力推新能源智能汽车，软硬件产品线可以说覆盖得非常广泛。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413298" alt="" title="" loading="lazy"/></p><p>但是说到「<strong>小米技术</strong>」，大家反而可能没有什么特别深的印象。</p><p>不过提到小米技术，就不得不提：<strong>小米集团技术委员会</strong>，其诞生于 2019 年初。</p><p>当时的 2018 年 Q4 季度，国内智能手机市场前五曾分别为：</p><p>华为、OPPO、VIVO、苹果、小米。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413299" alt="" title="" loading="lazy"/></p><p>其中前三家国产手机厂商出货量增幅均为正增长，而当时排名第 5 的小米则出现了较大跌幅。</p><p>彼时的雷军开始在内部会议中一直强调的一件事情就是：</p><p><strong>技术事关小米生死存亡</strong>。</p><p>在随后不久后的 2019 年 2 月，小米就专门成立了<strong>集团技术委员会</strong>，主要负责把握集团的技术方向，预研前沿技术，以及推动技术创新和成果转化。</p><p>小米技术委员会在小米的科技创新和产品研发中扮演着重要的角色。</p><p>而首任挂帅的正是小米当时的技术大牛：<strong>崔宝秋</strong>。</p><p>而聊起小米的技术研发，他更是绕不开的一个人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413300" alt="" title="" loading="lazy"/></p><p>崔宝秋和雷军都是武汉大学计算机系的同学，而且同时还是同寝室的上下铺室友。</p><p>在加入小米之前，崔宝秋在国外生活工作过很多年。</p><p>他曾在 IBM、雅虎和 LinkedIn 等知名公司负责研发，积累了丰富的技术和管理经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413301" alt="" title="" loading="lazy"/></p><p>众所周知，2012 年时候小米的手机业务初露头角。</p><p>同样也是这一年，崔宝秋应雷军之邀回国加入小米工作，并成为小米的首席架构师。</p><p>提到崔宝秋，有网友称其为“小米的技术教父”。</p><p>确实，崔宝秋在小米期间担任过多个重要职务，包括首席架构师、人工智能与云平台副总裁、集团技术委员会主席和集团学习发展部总经理等。</p><p>除此之外，他在小米期间的一个重大贡献就是主导并推动了小米「<strong>云计算-大数据-人工智能</strong>」的技术变革主线，为小米后来的技术发展奠定了基础。</p><p>然而，根据小米发布的内部全员信，2022 年底崔宝秋因个人原因从小米离职。</p><p>至此，崔神在小米的这段长达十年的职业生涯，也宣告结束了。</p><p>而这十年，也是小米这家公司从蓄力到发展，从厚积到薄发的十年。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413302" alt="" title="" loading="lazy"/></p><p>自 2022 年底，崔宝秋从小米集团离职后在业界似乎一直就“隐身”了，随后几年也一直没有看到他比较明显的下一步职场动向。</p><p>那这一次机器人创业也是崔宝秋离职小米两年多后的一个比较大的动作。</p><p>怎么说呢，祝福崔神，也期待后续能看到大佬更多的发展和动向。</p><blockquote>注：本文在GitHub开源仓库「编程之路」 <a href="https://link.segmentfault.com/?enc=%2Fd33B%2FKZ9tE%2FfQppjQ3cIA%3D%3D.sTVL03HoiON3gQ2GzdLmsVgoQ6WIqJRpDSnEsKukJqdwiBWoKlIUJY6Uav%2BwxN1S" rel="nofollow" target="_blank">https://github.com/rd2coding/Road2Coding</a> 中已经收录，里面有我整理的6大编程方向(岗位)的自学路线+知识点大梳理、面试考点、我的简历、几本硬核pdf笔记，以及程序员生活和感悟，欢迎star。</blockquote>]]></description></item><item>    <title><![CDATA[【youtube爬虫】油管评论采集软件v]]></title>    <link>https://segmentfault.com/a/1190000047413314</link>    <guid>https://segmentfault.com/a/1190000047413314</guid>    <pubDate>2025-11-20 10:03:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><em>本软件工具仅限于学术交流使用，严格遵循相关法律法规，符合平台内容合法合规性，禁止用于任何商业用途！</em></blockquote><h2>一、背景介绍</h2><h3>1.1 爬取目标</h3><p>您好！我是<strong>@马哥python说</strong>，一枚10年+程序猿，现全职独立开发。</p><p>我用Python独立开发了一款爬虫工具：<strong>爬油管评论软件</strong>。作用是：爬取油管指定视频下的评论数据，支持批量视频的采集。</p><p>包含10个关键字段：</p><pre><code>1. cid(评论id)
2. text(评论内容)
3. time(评论时间_相对)
4. author(评论者昵称)
5. channel(评论者频道)
6. votes(评论点赞数)
7. replies(评论回复数)
8. time_parsed(评论时间转换)
9. time2(评论时间_绝对)
10. video_id(视频id)</code></pre><p>软件是通过调用YouTube的网页接口，不是模拟操作浏览器，所以稳定性较高！</p><p>开发成界面软件的目的：方便不懂编程代码的小白用户使用，无需安装python，无需改代码，双击打开即用！<br/>软件运行界面：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413316" alt="软件运行截图" title="软件运行截图"/></p><p>《目标视频.xlsx》模板中的填写：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413317" alt="目标视频" title="目标视频" loading="lazy"/></p><p>也就是说，在目标视频文件中填入了5个待爬视频，然后在软件界面上选择爬这5个视频的前30条热门评论。所以，采集结果csv文件中会自动导出150条热门评论，如下：</p><p>爬取结果截图：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413318" alt="采集到的评论数据" title="采集到的评论数据" loading="lazy"/></p><p>以上。</p><h3>1.2 演示视频</h3><p>软件使用过程演示：<a href="https://link.segmentfault.com/?enc=RI2XwzpHjsPvIclvGXrMGQ%3D%3D.0KyKb8LULPUwOdTdXkaaf%2B4ZTSB0DYkmgQDtCs%2F24SGoqEd63LDgybL1KBMQTNbrg33Xhjnxgq4za47OIvsb2w%3D%3D" rel="nofollow" target="_blank">【爬虫演示】爬油管评论软件v3.0版</a></p><h3>1.3 软件说明</h3><p>几点重要说明：</p><ol><li>专为文科生研发，Win系统、Mac系统均可直接运行，无需配置python环境</li><li>软件通过接口爬取，并非通过模拟浏览器等RPA类工具，稳定性较高！</li><li>软件运行完成后，会在当前文件夹（即，软件所在文件夹）生成csv结果文件</li><li>爬取过程中，每爬一页，存一次结果。并非爬完最后一次性保存！防止因异常中断导致丢失前面的数据（每页请求间隔1~2s）</li><li>爬取过程中，有log文件详细记录运行过程，方便回溯</li><li>采集结果有10个字段，含：cid(评论id),text(评论内容),time(评论时间_相对),author(评论者昵称),channel(评论者频道),votes(评论点赞数),replies(评论回复数),time_parsed(评论时间转换),time2(评论时间_绝对),video_id(视频id)</li></ol><p>以上。</p><h2>二、代码概要</h2><h3>2.1 调用接口</h3><p>为保护软件原创版权，不开放核心爬虫逻辑代码。</p><p>最后，把json数据转出到csv文件：</p><pre><code class="python">self.tk_show('[第{}/{}个][{}][{}]comment:{}'.format(video_idx, video_id_total, video_id, cnt, comment['text']))
with open('./jsons/{}.json'.format(video_id), 'a+', encoding='utf-8') as f:
    f.write(json.dumps(comment, ensure_ascii=False))
    f.write('\n')</code></pre><p>我采用csv库保存结果，实现每爬一条存一次，防止中途异常停止丢失前面的数据。</p><p>完整代码中，还含有：读取配置判断、循环结束条件判断、拼接频道URL、try异常保护、日志记录等关键实现逻辑。</p><p>另外，魔法是一切的前提，此处不便多说！</p><h3>2.2 软件界面模块</h3><p>主窗口部分：</p><pre><code class="python"># 创建主窗口
root = tk.Tk()
root.title('爬油管评论软件v3.0 | 马哥python说 |')
# 设置窗口大小
root.minsize(width=850, height=650)
# 左上角图标
root.iconbitmap('mage.ico')</code></pre><p>输入控件部分：</p><pre><code class="python"># 爬取数量
tk.Label(root, text='爬取数量:').place(x=30, y=125)
comment_num = tk.Spinbox(root, from_=-1, to=9999999, increment=1, width=10, font=('微软', 15))
comment_num.place(x=100, y=125, anchor='nw')
tk.Label(root, fg='red', text='每个视频爬前几条评论，-1代表爬取全部').place(x=240, y=125)</code></pre><p>运行日志部分：</p><pre><code class="python"># 运行日志
tk.Label(root, justify='left', text='运行日志:').place(x=30, y=280)
show_list_Frame = tk.Frame(width=780, height=260)  # 创建&lt;消息列表分区&gt;
show_list_Frame.pack_propagate(0)
show_list_Frame.place(x=30, y=310, anchor='nw')  # 摆放位置</code></pre><p>底部版权部分：</p><pre><code class="python"># 版权信息
copyright = tk.Label(root, text='@马哥python说 All rights reserved.', font=('仿宋', 10), fg='grey')
copyright.place(x=290, y=625)</code></pre><p>以上。</p><h3>2.3 日志模块</h3><p>好的日志功能，方便软件运行出问题后快速定位原因，修复bug。 <br/>日志核心代码：</p><pre><code class="python">def get_logger(self):
    self.logger = logging.getLogger(__name__)
    # 日志格式
    formatter = '[%(asctime)s-%(filename)s][%(funcName)s-%(lineno)d]--%(message)s'
    # 日志级别
    self.logger.setLevel(logging.DEBUG)
    # 控制台日志
    sh = logging.StreamHandler()
    log_formatter = logging.Formatter(formatter, datefmt='%Y-%m-%d %H:%M:%S')
    # info日志文件名
    info_file_name = time.strftime("%Y-%m-%d") + '.log'
    # 将其保存到特定目录
    case_dir = r'./logs/'
    info_handler = TimedRotatingFileHandler(filename=case_dir + info_file_name,
                        when='MIDNIGHT',
                        interval=1,
                        backupCount=7,
                        encoding='utf-8')</code></pre><p>日志文件：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413319" alt="log文件" title="log文件" loading="lazy"/></p><p>以上。</p><h2>END、原创声明</h2><p>工具【爬油管评论软件】首发于众公号"老男孩的平凡之路"，欢迎交流！！</p>]]></description></item><item>    <title><![CDATA[前端AI流式输出深度思考<think>标]]></title>    <link>https://segmentfault.com/a/1190000047413332</link>    <guid>https://segmentfault.com/a/1190000047413332</guid>    <pubDate>2025-11-20 10:03:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>假如我们有这么一段ai回复的内容</p><pre><code>let text = "&lt;think&gt;\n好的，用户想了解如何判断手机摄像头是否是凑数的，希望我用直白易懂的语言来解释。这个问题需要从实用角度出发，帮助用户识别那些功能冗余或性能低下的摄像头。\n我看到的搜索结果提供了很多有价值的信息。根据，凑数摄像头通常是指那些功能可以被其他摄像头或算法替代的镜头，比如低像素的景深或微距镜头。进一步解释了如何通过反证法来判断：如果去掉某个摄像头后，其他摄像头通过算法也能实现相同功能，那这个摄像头就很可能是凑数的。\n\n&lt;/think&gt;\n\n1.先看像素​：遇到200万像素的镜头，心里先打个问号。\n2.再看名称​：如果镜头名称是“景深”、“人像风格”等，而非明确表示焦段（如超广角、3x长焦），就要多留意。\n3.思考能否被替代​：这个镜头做的事，​主摄拍照后裁剪或者装个APP能不能搞定？如果能，它大概率是凑数的。\n4.关注核心镜头​：​主摄的素质是根本。一颗优秀的主摄远比一堆凑数镜头重要。其次看超广角和长焦的规格是否扎实。记住一个简单的道理​：手机厂商增加一颗真正的、高质量的摄像头，成本会显著上升。如果一款手机摄像头数量很多但价格不高，你就需要多警惕那些低规格的镜头了。"</code></pre><p>有些AI-Markdown组件是不支持自动识别<code>&lt;think&gt;&lt;/think&gt;</code>标签的深度思考的，所以我们需要手动分割<code>&lt;think&gt;</code>标签，用来区分深度思考和正文内容。我在这里封装了一个行数，可以在流式输出中调用，自动分割</p><pre><code>/**
 * ai回复的思考和正文分块
 * @param {string} chunk 流式文字
 * @param {object} state ai的content
 * @return 返回分块后的内容 { buffer: '',thinkParts: [] }
 */
export const processStreamedOutput = (chunk, state) =&gt; {
    // 将新的块添加到缓冲区
    state.buffer += chunk;

    // 处理所有完整的 &lt;think&gt; 标签（完整保留标签）
    const thinkRegex = /&lt;think&gt;[\s\S]*?&lt;\/think&gt;/g; // 修改后的正则表达式

    while (true) {
        const match = thinkRegex.exec(state.buffer);
        if (!match) break;

        // 保留完整的 &lt;think&gt;...&lt;/think&gt; 标签
        state.thinkParts.push(match[0]);

        // 更新缓冲区，移除已处理的部分
        thinkRegex.lastIndex = 0; // 重置正则状态
        state.buffer = state.buffer.replace(match[0], '');
    }

    // 检查是否存在未闭合的 &lt;think&gt; 标签
    const openTagIndex = state.buffer.indexOf('&lt;think&gt;');
    if (openTagIndex !== -1) {
        // 保留 &lt;think&gt; 标签及其后的所有内容到缓冲区
        state.buffer = state.buffer.substring(openTagIndex);
    }

    return state;
};</code></pre><p>使用示例：</p><pre><code>// 当前ai的对话内容，比如 chatList.at(-1).content，其中content的结构是 { buffer: '',thinkParts: [] }
let chat = { buffer: '',thinkParts: [] }
// 分割
let data = processStreamedOutput(text, chat)
// 结果
console.log(data)</code></pre><p><img width="723" height="52" referrerpolicy="no-referrer" src="/img/bVdm6xd" alt="" title=""/><br/>支持在流式对话中的连续调用，每次输出都可以调用该函数进行一次分割<br/>默认将<code>text</code>添加到<code>buffer</code>缓冲字段，如果深度思考的  <code>&lt;think&gt;&lt;/think&gt;</code> 标签出现则说明当前深度思考结束，将包含<code>&lt;think&gt;&lt;/think&gt;</code>的深度思考内容添加到 <code>thinkParts</code> 数组内，后续正文的内容依旧添加到<code>buffer</code>缓冲字段<br/>在页面使用的时候需要两个个辅助函数实现</p><pre><code>    /**
     * 深度思考取值
     * @param {number} type 1取深度思考 2取正文
     * @param {object} content AI的content内容
     * @returns 深度思考或正文的正确内容
     */
    const getMarkdown = (type, content) =&gt; {
        let {
            thinkParts,
            buffer
        } = content;
        if (type == 1) {
            if (buffer.includes('&lt;think&gt;')) return buffer;
            let state = thinkParts.some((item) =&gt; item.includes('&lt;think&gt;'));
            if (state) return thinkParts.at(-1);
            return '';
        } else {
            if (!buffer.includes('&lt;think&gt;')) {
                if (thinkParts.length) {
                    return buffer || '';
                } else {
                    return buffer || '已暂停生成';
                }
            }
        }
    };</code></pre><pre><code>    /**
     * 是否有深度思考
     * @param {object} content AI的content内容
     * @returns 是否存在深度思考标签
     */
    const isThink = (content) =&gt; {
        let {
            thinkParts,
            buffer
        } = content;
        let state = thinkParts.some((item) =&gt; item.includes('&lt;think&gt;'));
        if (buffer.includes('&lt;think&gt;') || state) {
            return true;
        } else {
            return false;
        }
    };</code></pre><p>页面使用</p><pre><code>&lt;!-- 深度思考 --&gt;
&lt;view class="ai-think-chunk" v-show="isThink(msg.content)"&gt;
        &lt;markdown-view :theme-color="'#252B3A'" :markdown="getMarkdown(1, msg.content)" /&gt;
&lt;/view&gt;
&lt;!-- 正文内容 --&gt;
&lt;markdown-view :theme-color="'#252B3A'" :markdown="getMarkdown(2, msg.content)" /&gt;</code></pre><p>如果深度思考标签存在，则直接显示深度思考，并且取深度思考的内容<br/>正文内容直接取值就好了，因为在分割深度思考的那一步已经做了区分了，另外加上<code>getMarkdown</code>辅助函数做内容判断，如果正文能够取到，说明正文一定是有内容的</p>]]></description></item><item>    <title><![CDATA[【营销数据洞察系列1】渠道效果实时监控：]]></title>    <link>https://segmentfault.com/a/1190000047413378</link>    <guid>https://segmentfault.com/a/1190000047413378</guid>    <pubDate>2025-11-20 10:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从营销理论来看，渠道价值的判断需基于 “全链路数据闭环”，而非单一曝光或点击指标，核心是通过投入产出比（ROI）、转化效率等多维度综合评估。借助助睿 BI 搭建渠道日报，导入 Excel 或数据库数据，曝光、点击、转化、花费等核心指标自动整合呈现，无需手动汇总，投放现状可直观掌握，为科学评估奠定基础。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413380" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[【新手必看】火语言 RPA 中 Whil]]></title>    <link>https://segmentfault.com/a/1190000047413382</link>    <guid>https://segmentfault.com/a/1190000047413382</guid>    <pubDate>2025-11-20 10:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、核心差异</h2><p><strong>循环体执行的 “先后顺序”—— 是否先判断条件，再执行内容，用表格对比更清晰：</strong></p><table><thead><tr><th>特性</th><th>While 循环（先判断，后执行）</th><th>DoWhile 循环（先执行，后判断）</th></tr></thead><tbody><tr><td>执行逻辑</td><td>1. 先判断条件是否成立 → 2. 条件成立才执行循环体</td><td>1. 先执行 1 次循环体 → 2. 再判断条件是否成立 → 3. 成立则继续循环</td></tr><tr><td>适用场景</td><td>不确定循环体是否需要执行（可能 1 次都不执行）</td><td>循环体必须执行至少 1 次（无论条件是否成立）</td></tr><tr><td>条件不成立时表现</td><td>循环体 1 次都不执行</td><td>循环体已执行 1 次，之后停止</td></tr></tbody></table><p><strong>通俗理解：</strong></p><pre><code>While 循环：“先审题，再做题”—— 符合要求才动手，不符合就直接跳过；
DoWhile 循环：“先做题，再审题”—— 不管符不符合，先做 1 遍，做完再看要不要继续。
</code></pre><h2>二、案例</h2><p>对变量 数字 1 数字 2 执行 “变量赋值” 操作，初始值设为5,</p><h3>1、While 循环执行逻辑（先判断，后执行）</h3><p>循环条件：#数字1 ＜ 5（判断变量 “数字 1” 是否小于 5）。<br/>循环体操作：对 数字 1 执行 ++赋值操作（即数字 1 自增 1）。<br/>执行结果：因初始值 数字 1=5，循环条件5小于5不成立，While :False， 循环体一次都不会执行。</p><h3>2、DoWhile 循环执行逻辑（先执行，后判断）</h3><p>循环体操作：先对 数字 2 执行 ++ 赋值操作（数字 2 自增 1，变为6）。<br/>循环条件：#数字2 ＜ 5（判断变量 “数字 2” 是否＜ 5）。<br/>执行结果：因 数字 2=6 循环条件5小于5不成立，While :False，，循环内执行一次循环体操作。<br/><img width="723" height="258" referrerpolicy="no-referrer" src="/img/bVdm6x2" alt="57b64cbe-ce8f-4992-8c47-fac82c4cd2c8.png" title="57b64cbe-ce8f-4992-8c47-fac82c4cd2c8.png"/></p><p>以上案例分享: <a href="https://link.segmentfault.com/?enc=hRxQohDTVCPHH0rl43aJWQ%3D%3D.FhNL%2B%2BpFmNldWuaLWYHIqbcgTEPpSaJE%2BEYR%2Bal0RiLWvMB6OKuPjun%2F4H7A%2B3Fku0x4SmpbeQB%2FlhZZRGR7mh81Hzm1vXl0GDJmc4NzP2htT9uqUZY4JygWRD%2B5p1hol31ftxE2qGPkYw19w6OUrvyl7xPuYsdox%2BDcDhPmv9k%3D" rel="nofollow" target="_blank">https://www.huoyuyan.com/share.html?key=eyJhdXRvQ29kZSI6IkZhb...</a> 提取码: 7t2i</p><p><strong>通过这个流程，可清晰对比两种循环的本质区别：</strong><br/><strong>While 循环：</strong>因初始条件不满足，循环体完全不执行；<br/><strong>DoWhile 循环：</strong>不管条件是否满足，先执行一次循环体，再判断条件是否成立。</p>]]></description></item><item>    <title><![CDATA[.NET STS 版本支持 24 个月 ]]></title>    <link>https://segmentfault.com/a/1190000047413386</link>    <guid>https://segmentfault.com/a/1190000047413386</guid>    <pubDate>2025-11-20 10:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>.NET团队在博客上发布 《.NET STS 版本支持 24 个月》，调整 .NET 的 标准支持（STS） 版本生命周期，从原先的 18个月延长至24个月。一、STS 支持周期调整支持时长变更旧政策：STS 版本支持 18 个月（如网页 1 所述）。新政策：STS 版本现支持 24个月（如网页 2 和网页 5 明确提到）。<br/>示例：.NET 9（STS 版本）于 2024 年 11 月发布，支持至 2026 年 11 月。与 LTS 版本的对比LTS（长期支持）：至少 3 年或至下一个 LTS 发布后 1 年（以较晚者为准）。<br/>示例：.NET 8（LTS）支持至 2026 年 11 月。<br/>STS（标准支持）：24 个月固定期限，适合需要频繁更新功能的场景。二、调整背景与影响现代生命周期模型 Microsoft 采用更灵活的更新策略，缩短支持周期但提供更频繁的服务更新（如每月安全补丁），以平衡稳定性和新特性迭代。更新频率：每月发布累积更新（含安全与非安全修复）。默认行为：应用会自动使用最新安装的运行时补丁（“向前滚动”）。<br/>用户选择建议STS 适用场景：需快速获取新特性的服务型应用，团队能持续跟进版本升级。LTS 适用场景：面向消费者的客户端应用或需长期稳定的企业环境。混合策略：即使使用 STS 版本，仍推荐升级至最新 SDK 以兼容多运行时。</p><p>四、注意事项维护支持期：所有版本（STS/LTS）在支持期的最后 6 个月仅接收安全更新。终止支持风险：未及时升级的应用可能面临安全漏洞或兼容性问题。</p>]]></description></item><item>    <title><![CDATA[你的游戏没有这个怎么能够顺利出海？ 亿元]]></title>    <link>https://segmentfault.com/a/1190000047409652</link>    <guid>https://segmentfault.com/a/1190000047409652</guid>    <pubDate>2025-11-20 09:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>点击上方&lt;font color=blue&gt;亿元程序员&lt;/font&gt;+关注和&lt;font color=orange&gt;★&lt;/font&gt;星标</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409654" alt="" title=""/></p><h2>引言</h2><p><strong>哈喽大家好</strong>，不知道小伙伴们最近有没有发现，如今的游戏出海，已经不是从前的粗放买量靠堆砌素材喂算法了，现在都在拼长线运营或者<code>AI</code>了。</p><p><strong>其中</strong><code>Supercell</code>的《荒野乱斗》就是最好的例子，上线五年了，如今成功逆袭。</p><p><strong>笔者想起</strong>，八年前就已经参与过游戏的多语言版本了，那时候的主流是港台（繁体）、东南亚（英文）、韩国（韩文）。</p><p><strong>除去</strong>接入对应的<code>SDK</code>外，最为深刻的就是翻译和本地化，那时候不需要很高端的操作，就是把游戏内所有的中文整理出来，给到专门负责翻译的人，完成后导回到游戏即可。</p><p><strong>其中</strong>最头疼的就是英文，两个字能变<code>10</code>多个字母，很容易导致超框，以上的处理有个“国际化”的简称，叫<code>i18n</code>。</p><p><strong>言归正传</strong>，今天我们就来聊聊<code>i18n</code>。</p><h2>什么是i18n？</h2><p><strong>先简单科普一下</strong></p><blockquote><p><code>i18n</code>是国际化的简称，来源是英文单词<code>internationalization</code>的首末字符<code>i</code>和<code>n</code>，<code>18</code>为单词中间的字符数。</p><p><strong>在资讯领域</strong>，国际化（<code>i18n</code>）可以让产品无需做大的改变，就能够满足不同语言和地区的需要。</p></blockquote><h2>i18n的优势是什么？</h2><p><strong>对程序来说</strong>，在不修改内部代码的情况下，就能根据不同语言及地区切换相应的语言界面。</p><p><strong>正如笔者</strong>引言所说，把文本交给翻译人员，回来后原路返回即可。</p><p><strong>从</strong>最开始就制定严格的规范，所有的文本都需要通过<code>i18n</code>，这样能够减少后期提取文本和导回文本的麻烦操作。</p><h2>一起来看个例子</h2><p><strong>既然</strong><code>i18n</code>如此重要，那么我们一起来看看它在<code>Cocos</code>游戏开发中如何使用。</p><h3>1.安装插件</h3><p><strong>首先</strong>在<code>Store</code>找到<code>i18n多语言化插件</code>，选择添加到项目即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409655" alt="" title="" loading="lazy"/></p><p><strong>添加</strong>完成之后，就可以在资源管理器看到插件对应的脚本，分别对应数据、<code>Label</code>管理和<code>Sprite</code>管理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409656" alt="" title="" loading="lazy"/></p><h3>2.添加语言</h3><p><strong>首先</strong>通过菜单<code>扩展-&gt;I18n Setting</code>打开本地化控制面板。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409657" alt="" title="" loading="lazy"/></p><p><strong>简单</strong>添加<code>8</code>个语言，从下到上分别为中文、俄语、韩语、日语、法语、西班牙语、英语和德语（首字母排序）。</p><p><strong>为什么</strong>是<code>8</code>个？因为可以和别人说你“精通”八国语言（你好，世界）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409658" alt="" title="" loading="lazy"/></p><p><strong>编辑完成</strong>后，插件会自动在<code>resources\i18n</code>目录生成对应的<code>Ts</code>配置文件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409659" alt="" title="" loading="lazy"/></p><h3>3.编辑中文</h3><p><strong>在</strong><code>zh.ts</code>中添加<code>你好，世界</code>，作为示例。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409660" alt="" title="" loading="lazy"/></p><h3>4.翻译其他7种语言</h3><p><strong>把</strong>完成的所有中文内容，交给专业的翻译人员进行翻译，获得其余<code>7</code>种语言，我们这里简单示例就找搭子就行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409661" alt="" title="" loading="lazy"/></p><p><strong>还是</strong>非常靠谱的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409662" alt="" title="" loading="lazy"/></p><h3>5.LocalizedLabel组件</h3><p><strong>插件</strong>生成的<code>LocalizedLabel</code>组件，就是对<code>Label</code>的简单封装，根据<code>key</code>和语言配置获取对应的文本。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409663" alt="" title="" loading="lazy"/></p><p><strong>直接</strong>挂到<code>Label</code>组件上，配置对应的文本的<code>key</code>即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409664" alt="" title="" loading="lazy"/></p><h3>6.效果演示</h3><p><strong>我们</strong>通过点击本地化编辑面板中不同语言的“小眼睛”，即可完成语言的切换，并且看到切换语言后的效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409665" alt="" title="" loading="lazy"/></p><p><strong>上面</strong>只是编辑器演示，实际需要修改默认语言，可以在脚本中修改。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409666" alt="" title="" loading="lazy"/></p><h3>7.进阶</h3><p><strong>如果</strong>一个包体内有多种语言，想要支持动态切换语言，可以通过<code>import * as i18n from 'db://i18n/LanguageData';</code>导入<code>i18n</code>，并通过<code>i18n.init('en');</code>进行语言切换，最后通过<code>i18n.updateSceneRenderers();</code>刷新即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409667" alt="" title="" loading="lazy"/></p><p><strong>此外</strong>，<code>LocalizedSprite</code>组件也是同理，对<code>Sprite</code>的简单封装。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409668" alt="" title="" loading="lazy"/></p><h2>结语</h2><p><strong>当然了</strong>，并非所有的项目都需要使用这套插件，国际化的逻辑还是简单的，一般公司项目都有自己的技术积累，有自己的实现。</p><p><strong>其实</strong>最主要是一个规范的问题，通过语言包的限制，避免文本到处都是。但是也使得配置表比较难以配置，这个需要小伙伴们自己权衡了。</p><p><strong>你们使用的是什么方案？</strong></p><p><strong>我是"亿元程序员"，一位有着8年游戏行业经验的主程。在游戏开发中，希望能给到您帮助, 也希望通过您能帮助到大家。</strong></p><p>AD:笔者线上的小游戏《打螺丝闯关》《贪吃蛇掌机经典》《重力迷宫球》《填色之旅》《方块掌机经典》大家可以自行点击搜索体验。</p><p>实不相瞒，想要个<strong>赞</strong>和<strong>爱心</strong>！请把该文章<strong>分享</strong>给你觉得有需要的其他小伙伴。谢谢！</p><p>推荐专栏：</p><p><a href="https://link.segmentfault.com/?enc=pBsguJ5Sb3eU8MejokzIZA%3D%3D.5eaM43WwPTaCHUYgJGRVlpFJgEw8suJytTe%2FuzcUspEAt3k1CQCzSRECmaci1bwPGjEDA9SaB7G0%2B0pZI8F25SfpEoto6wX0dR2XBvXHqEGQpUHDnaclFowZcaQhON6G6mfaPYsjkJ3mi120IxUcrHrmqWt5Ka2%2FseRmpwKDkrc%3D" rel="nofollow" target="_blank">知识付费专栏</a></p><p><a href="https://link.segmentfault.com/?enc=WQg%2F%2FR0yoKUorBNiaYYhSw%3D%3D.58zTyxj00ZOvZyZydaLt%2FPWBqsexk%2FhgwiQXQghUuCPW4XiX1pZP25efIQXN%2FwlQQFY%2F37kSwCduHWrUGp5OQAYsXVt%2FvZoz%2Bax%2BHh0ED2n1P6rM3biMvOMZpgpR2Vu5H1DKMj3fckTNRMCsDZ0S20TdszdVMPp89JXrmqen8vcncSmMNRZ%2FGpuHnDIZy%2FWGHmB9yeYgimqnx1koliL50%2BcIuZu2Qo8aL7sWIUhR1DAulAR9etPGezwBo3SpQEL2u%2FrJ2OrJsFlBUdYyz4UpD2CQuL4Fdq6SwA9l9IPN%2Bzg%3D" rel="nofollow" target="_blank">你知道和不知道的微信小游戏常用API整理，赶紧收藏用起来~</a></p><p><a href="https://link.segmentfault.com/?enc=9IgR%2BZzWQjEHgETJkRIx1g%3D%3D.9CimkFwA3ckzm%2BuZ2vWnqJ4pgtXCUi0GN1ta616efS9Cm0iQTOSsL86pkv%2FFY8ryrLPCQA5ZZ8sly2e7CwwVhf9CWiEuVzTCJmWjKegj6keqicQdHToGanzZZWQOfiILgPhgocgtmgqQlgBmmeoVGRnL6dd2fosow5TZLudz1xU%3D" rel="nofollow" target="_blank">100个Cocos实例</a></p><p><a href="https://link.segmentfault.com/?enc=CSp%2BiTGx%2F7fMy%2F%2B5Jsz%2FmQ%3D%3D.JBrA0PTkDq%2FARAAqRKX26cVRCbw%2F4VUogmP3jy0zp2R5zFR%2Bb7HDh7XV2XSmBKqw2LxM29O7IX3pl%2FL5i1uC8TLMPIpj5t%2FculOI49ECAYRkPEmHPUkAKY3GfoMMtPHXHCWIJrILn4JS1tl9YjYbef21y45qt64amkJcu7%2BQ16o%3D" rel="nofollow" target="_blank">8年主程手把手打造Cocos独立游戏开发框架</a></p><p><a href="https://link.segmentfault.com/?enc=A89RDinBIgQH8UanoeY5GA%3D%3D.uXi6yd%2Fk3h9NGe%2Bfs%2B%2BzSxa%2BjVGde04Qxh0KnSudpZFMiZbzNnb4pu2CVgWGq2lCP06miGBkvlmiYaqvOeirHuoZYc8cnHipF9zcMCftEmguygTuu9pfUWlfoTmlgBmlBdzxDBzjB66w3KD0J%2BVxGT7LzvlTCKx5H9MOv0QTstg%3D" rel="nofollow" target="_blank">和8年游戏主程一起学习设计模式</a></p><p><a href="https://link.segmentfault.com/?enc=Z5hSoNDJw5z%2FErUEnTF0MA%3D%3D.7YD7KN7wJHQz7shOGtfMaUkZGy0YBjavNH3l%2FiWCQBthU2UPjZUOghGUo%2ByFICzCR1UZjyV%2FPLR0RbmL574D6KG9Ql49kmwOxZX5s7fMsxjGXBvmKNQDUIgka1x88SnfeBNy3fxxQ9SbgLqtzgmQ36niF0v4nQX4O3DzDdxvusDGAvVDmXiyrs6ruJm08fdr" rel="nofollow" target="_blank">从零开始开发贪吃蛇小游戏到上线系列</a></p><p>点击下方&lt;font color=gray&gt;灰色按钮&lt;/font&gt;+关注。</p>]]></description></item><item>    <title><![CDATA[RubyMine 2025.2.4 11]]></title>    <link>https://segmentfault.com/a/1190000047413218</link>    <guid>https://segmentfault.com/a/1190000047413218</guid>    <pubDate>2025-11-20 09:02:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <ul><li>2025-11-20 亲测</li><li>支持最新版本2025.2.4</li><li>支持Windows、MAC、Linux<br/><img width="684" height="442" referrerpolicy="no-referrer" src="/img/bVdm6vp" alt="ruby.png" title="ruby.png"/></li></ul><h2>一 安装</h2><p>官网下载 ：<a href="https://link.segmentfault.com/?enc=ojN1D0WMDoRBuVOw%2FMalcg%3D%3D.FlOqYJ2PiW5raX5QZ0VCzVu3j8%2BcmrsgXg0%2FRGzic8hLSVvRcLOWvQeUPop5nxRt" rel="nofollow" target="_blank">https://www.jetbrains.com/zh-cn/ruby/</a><br/>根据提示安装</p><h2>二 授权说明</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413220" alt="图片" title="图片" loading="lazy"/><br/>回复 《ruby》获取<br/>新版本安装后不提示授权，需要手动处理</p><h2>三 使用</h2><p>打开自己的项目，配置环境，开始开发<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdm6vt" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[IP SSL证书助力公网内网IP地址实现]]></title>    <link>https://segmentfault.com/a/1190000047413223</link>    <guid>https://segmentfault.com/a/1190000047413223</guid>    <pubDate>2025-11-20 09:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化转型加速推进的当下，网络安全的重要性日益凸显。我国的《网络安全法》《数据安全法》从不同层面强调了网络安全的重要性。SSL证书作为实现HTTPS加密与可信身份认证的有力工具，已成为构建安全网络环境的基石。</p><p>通常，我们会为域名申请SSL证书。但在许多实际场景中，存在大量只能通过IP地址直接访问的服务，此时就需要为IP地址申请SSL证书。这类证书通常被称为<strong>IP SSL证书可以</strong> <strong>助力公网内网IP地址实现HTTPS</strong>  <strong>，</strong>  为那些不依赖域名、直接通过IP提供服务的场景，提供完整的数据传输安全与身份验证解决方案。</p><h2><strong>一、什么是IP SSL证书？</strong></h2><p>IP SSL证书，是一种专门用于为IP地址实现HTTPS加密的数字证书，也可以称之为IP HTTPS证书。IP SSL证书通过在服务器和客户端之间建立加密通信通道，保障数据传输过程的机密性与完整性，解决了IP地址与服务器端的数据传输安全问题，并可帮助用户识别企业网站身份真伪。</p><p>IP SSL证书适用于多种场景，包括但不限于：物联网（IoT）设备、API服务接口、测试或临时云服务等通过IP直接提供公网访问的应用；同时也广泛用于内部系统（如OA、ERP、远程办公平台）、开发测试环境及局域网服务等内网环境。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413225" alt="企业微信截图_17635146629936.png" title="企业微信截图_17635146629936.png"/></p><h2><strong>二、IP SSL证书的作用</strong></h2><p><strong>1、数据传输安全保护</strong></p><p>IP SSL证书可助力IP地址实现HTTPS加密，在服务器和浏览器之间建立一个安全通道，以确保服务器和浏览器之间传输的所有数据保持机密性和完整性。</p><p><strong>2、网站身份可信认证</strong></p><p>IP SSL证书由证书颁发机构（CA）对IP所有权及相关身份进行验证后签发，能提高IP身份的可辨识度，防范IP仿冒与欺诈风险。</p><p><strong>3、提升用户信任度</strong></p><p>部署IP SSL证书后，用户访问IP地址时浏览器将显示“https:// ”协议及安全锁标志。若使用企业型（OV）IP SSL证书，还会展示企业名称，有利于提升用户信任度。</p><p><strong>4、满足合规性要求</strong></p><p>实现HTTPS加密可协助企业符合网络安全法、等保2.0、PCI/DSS等法规中对数据加密的要求，规避因不合规导致的法律与业务风险。</p><h2><strong>三、IP SSL证书的品牌与类型</strong></h2><p>IP SSL证书在品牌上覆盖国内外主流CA机构，类型根据验证方式、保护IP数量以及密码算法可以分为6种类型。</p><p><strong>1、主要品牌</strong></p><p>国产品牌：sslTrus、CFCA、JoySSL等可信的国产证书品牌。</p><p>国际品牌：Sectigo、GlobalSign、Digicert是具备国际声誉的国际证书品牌。</p><p><strong>2、证书类型</strong></p><p><strong>按验证方式：</strong></p><p>DV型：仅验证IP所有权，签发速度快，通常几分钟即可完成。</p><p>OV企业型：需验证IP所有权及企业真实信息，安全性更高，审核时间约为1-3个工作日。</p><p><strong>按保护IP数量：</strong></p><p>单个IP证书：保护1个IP地址，支持一个IP地址实现HTTPS。</p><p>多个IP证书：保护多个IP地址，支持多个IP地址实现HTTPS。</p><p><strong>按密码算法：</strong></p><p>RSA算法证书：兼容谷歌、火狐、Edge、Safari等国际主流浏览器。</p><p>SM2国密算法证书：适用于360、奇安信、赢达信速龙等国密浏览器。</p><p><strong>四、</strong>  <strong>IP SSL证书</strong> <strong>申请</strong></p><p>IP SSL证书申请步骤很简单，基本需要经过以下流程：</p><ul><li>确认IP地址类型（公网或内网）；</li><li>选择合适的证书品牌和类型；</li><li>提交申请证书所需要的资料；</li><li>CA会对提交的信息进行验证；</li><li>验证通过后签发证书，部署即可。</li></ul><p>总结而言，IP SSL证书能够有效帮助公网与内网IP地址实现HTTPS加密，不仅增强数据传输的安全性，也提高了IP身份的可信识别度，减少冒充风险。在企业全面推进数字化转型的背景下，部署IP SSL证书有助于构建全覆盖的安全访问体系，满足日趋严格的合规要求，为企业能够安全、稳定、持续运营提供坚实保障。</p>]]></description></item><item>    <title><![CDATA[剑指offer-40、数组中只出现⼀次的]]></title>    <link>https://segmentfault.com/a/1190000047402185</link>    <guid>https://segmentfault.com/a/1190000047402185</guid>    <pubDate>2025-11-20 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>题⽬描述</h2><p>⼀个整型数组⾥除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现⼀次的数字。</p><p>示例<br/>输入：[92,3,43,54,92,43,2,2,54,1]<br/>输出：3，1</p><h2>思路解答</h2><h3>哈希表统计</h3><p>使⽤ hashmap 存储数字出现的次数， key 为出现的数字， value 为该数字出现的次数。遍历⾥⾯所有的数字，如果 hashmap 中存在，那么 value （次数）+1，如果 hashmap 中不存在,那么 value 置为1。</p><p>遍历完成之后，需要将次数为 1 的数字捞出来，同样是遍历 hashmap ，由于只有两个满⾜条件，我们设置⼀个标识变量，初始化为1，如果找到第⼀个满⾜条件的数字，除了写⼊放回数组中，还需要将该标识置为 2 ，表示接下来找的是第 2 个。</p><p>如果找到第 2 个，那么写⼊之后，直接 return 。</p><pre><code class="java">public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) {
     Map&lt;Integer, Integer&gt; maps = new HashMap&lt;&gt;();
     if (array != null) {
         for (int n : array) {
             Integer num = maps.get(n);
             if (num == null) {
                 // map中不存在
                 maps.put(n, 1);
             } else {
                 // map中已经存在
                 maps.put(n, num + 1);
             }
         }
      }
      int index = 1;
      for (Map.Entry&lt;Integer, Integer&gt; entry : maps.entrySet()) {
          if (entry.getValue() == 1) {
             if (index == 1) {
                 num1[0] = entry.getKey();
                 index++;
             } else {
                 num2[0] = entry.getKey();
                 return;
             }
         }
     }
 }</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，需要遍历数组两次</li><li><strong>空间复杂度</strong>：O(n)，需要HashMap存储频率信息</li></ul><h3>排序遍历</h3><p>先对数组排序，然后遍历查找不连续的数字。排序后相同的数字会相邻，遍历找到不连续的数字</p><pre><code class="java">public class Solution {

    public int[] FindNumsAppearOnce(int[] nums) {
        if (nums == null || nums.length &lt; 2) {
            return new int[0];
        }
        
        // 对数组进行排序
        int[] sorted = nums.clone();
        Arrays.sort(sorted);
        
        int[] result = new int[2];
        int index = 0;
        
        // 遍历查找不连续的数字
        for (int i = 0; i &lt; sorted.length; i++) {
            // 检查当前数字是否与前后都不同
            boolean isSingle = true;
            
            // 检查前一个元素（如果不是第一个元素）
            if (i &gt; 0 &amp;&amp; sorted[i] == sorted[i - 1]) {
                isSingle = false;
            }
            
            // 检查后一个元素（如果不是最后一个元素）
            if (i &lt; sorted.length - 1 &amp;&amp; sorted[i] == sorted[i + 1]) {
                isSingle = false;
            }
            
            if (isSingle) {
                result[index++] = sorted[i];
                if (index == 2) break; // 找到两个数字后退出
            }
        }
        
        // 确保结果按升序排列
        if (result[0] &gt; result[1]) {
            int temp = result[0];
            result[0] = result[1];
            result[1] = temp;
        }
        
        return result;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(nlogn)，主要来自排序操作</li><li><strong>空间复杂度</strong>：O(1) 或 O(n)，取决于是否克隆数组</li></ul><h3>位运算（最优解）</h3><p>⾸先需要了解⼀定位运算知识，异或是指⼆进制中，⼀个位上的数如果相同结果就是0，不同则结果是0.</p><p>也就是如果⼀个数的最低位是0，另⼀个数的最低位是0，那么异或结果的最低位是0；如果⼀个数的最低位是0，另⼀个数的最低位是1，那么异或结果的最低位是1。</p><p>异或操作可以交换，不影响结果：A^B^C = A^B^C</p><p>A^A=0,任何⼀个数异或⾃身，等于0，因为所有位都相同</p><p>A^0 = A，任何⼀个数异或0，等于⾃身，因为所有位如果和0不同，就是1，也就是保留了⾃身的数值</p><p>假设⾥⾯出现⼀次的两个元素为 A 和 B ，初始化异或结果 res 为0，遍历数组⾥⾯所有的数，都进⾏异或操作，则最后结果 res = A^B 。</p><p>但是我们拿到这个 A 和 B 异或之后的结果，怎么区分呢？</p><p>有⼀种巧妙的思路，因为 A 和 B 的某⼀位不同才会在结果中出现 1 ，说明在那⼀位上， A 和 B 中肯定有⼀个是 0 ，有⼀个是 1 。</p><p>那我们取出异或结果 res 最低位的1，假设这个数值是 temp （temp只有⼀个位是1，也就是A和B最后不同的位）</p><p>遍历数组中的元素，和 temp 进⾏与操作，如果和 temp 相与，不等于0。说明这个元素的该位上也是1。那就说明它很有可能就是 A 和 B 中的⼀个。</p><p>只是有可能，其他的数也有可能该位上为 1 。但是符合这种情况的，其他数肯定出现两次，⽽ A 和 B只可能有⼀个符合，并且只有可能出现⼀次 A 或者 B 。</p><p>凡是符合和 temp 相与,结果不为0的，我们进⾏异或操作。</p><p>也就是可能出现， res1 = B^C^D^C^D...^E^E^B 或者 res1 = A^C^D^C^D...^E^E 。</p><p>上⾯的式⼦可以视为 res1 = B 或者 res1 = A</p><p>这样操作下来，我们就知道了 res1 肯定是其中⼀个只出现⼀次的数（ A 或者 B ）,⽽同时上⾯计算出了 res = A^B ,也就是可以通过 res1^res 求出另⼀个数。</p><pre><code class="java">public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) {
     // A和B异或的结果
     int res = 0;
     for (int val : array) {
         res ^= val;
     }
    
     // temp保存了两个数最后⼀个不同的位
     int temp = res &amp; (-res);
     // 保存和最后⼀个不同的位异或的结果
     int res1 = 0;
     for (int val : array) {
         // 不等于0说明可能是其中⼀个数，⾄少排除了另外⼀个数
         if ((temp &amp; val) != 0) {
             res1 ^= val;
         }
     }
     // 由于其他满⾜条件的数字都出现两次，所以结果肯定就是只出现⼀次的数
     num1[0] = res1;
     // 求出另外⼀个数
     num2[0] = res ^ res1;
 }</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，只需要遍历数组两次</li><li><strong>空间复杂度</strong>：O(1)，只使用固定数量的变量</li></ul><h4>位运算原理解析</h4><p>通过示例详细解释算法过程：</p><p><strong>输入</strong>：<code>[92,3,43,54,92,43,2,2,54,1]</code><br/><strong>单次数</strong>：3 和 1</p><p><strong>步骤1：计算所有数字的异或结果</strong></p><pre><code class="java">92 ^ 3 ^ 43 ^ 54 ^ 92 ^ 43 ^ 2 ^ 2 ^ 54 ^ 1
= (92 ^ 92) ^ (43 ^ 43) ^ (2 ^ 2) ^ (54 ^ 54) ^ 3 ^ 1
= 0 ^ 0 ^ 0 ^ 0 ^ (3 ^ 1)
= 3 ^ 1 = 2</code></pre><p><strong>步骤2：找到异或结果的最低位的1</strong></p><pre><code class="java">3的二进制: 0011
1的二进制: 0001
3^1=2的二进制: 0010
最低位的1在从右往左第2位（值为2）</code></pre><p><strong>步骤3：根据最低位1分组</strong></p><ul><li><strong>第1组（第2位为0）</strong>：3(0011), 43(101011), 54(110110), 1(0001), 92(1011100)</li><li><strong>第2组（第2位为1）</strong>：2(0010)</li></ul><p><strong>步骤4：分别异或各组</strong></p><pre><code class="java">第1组: 3 ^ 43 ^ 54 ^ 1 ^ 92 ^ 43 ^ 54 ^ 92
     = (3 ^ 1) ^ (43 ^ 43) ^ (54 ^ 54) ^ (92 ^ 92) 
     = 3 ^ 1 = 2 ❌ 这里应该是 3 ^ 1 = 2，但我们需要重新计算正确的分组

让我们重新正确分组计算：
实际分组应该是：
第1组（第2位为0）：3, 1  // 只有这两个数在第2位为0且是单次数
第2组（第2位为1）：所有其他数

正确的计算：
第1组: 3 ^ 1 = 2
第2组: 92 ^ 43 ^ 54 ^ 92 ^ 43 ^ 2 ^ 2 ^ 54 = 0</code></pre><p><strong>位运算特性利用：</strong></p><ul><li>相同数字异或为0：<code>a ^ a = 0</code></li><li>任何数与0异或为本身：<code>a ^ 0 = a</code></li><li>异或满足交换律和结合律</li></ul>]]></description></item><item>    <title><![CDATA[基于Strands Agents SDK]]></title>    <link>https://segmentfault.com/a/1190000047412944</link>    <guid>https://segmentfault.com/a/1190000047412944</guid>    <pubDate>2025-11-20 08:01:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000046555790" alt="图片" title="图片"/></p><h2>背景</h2><p>AI Agent 作为当前发展最快的技术趋势之一,  不同于专注特定领域任务的传统 AI 应用, 能在更少人工干预的情况下,  管理和执行端到端的流程,  从单纯的工具进化为团队成员,  使企业生产力、效率和增长方面进入一个新的时代. 有研究报告指出: “AI Agent 竞争势头已经明显增强, 93%的企业管理者认为在接下来的一年内在企业内部规模扩展AI Agent 的使用会让企业在同类竞争者中保持领先地位”. 尽管如此, 现实情况是: 大部分企业还不知道以何种方式快速构建、运行和管理AI Agents, 将AI Agents 快速在企业内部落地应用. 基于此, 本文从AI Agent 的结构和组成入手, 结合Amazon Strands Agents SDK,  构建一个集 Agent 构建和运行的参考实现平台, 且在参考实现平台集成了AgentCore Browser user tool 和 AgentCore Code Interpreter tool, 方便用户使用相应的工具来构建Agents.</p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=th3gVnHoqLxWDyATKgjcHA%3D%3D.Fpf%2FENVBSKftVfcy0cMfaxaudotpH4gvpzYlMcnA0kiXzN%2B99slrsUcHYEeChvLxZMKUmn1KyZFn2A%2FkHHrTrqes8ubRTXt9wdeZTvYRsn8QMoNaqA5htSJbMm3vzbv%2BKACxMDIlE13f3cKP4%2BWIdmUX%2F5N%2BjFJAHQCjlacZz9xEp6mD2rKjE%2FsTUySTUElqeCpL4n9cIPHSqUSGIEt7tckJYIotnT1AEnOZy288cxA%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=3KPdbbt8VXvOUW2DaLU6WA%3D%3D.GD8fYCrMMHN6RkbT1md87M9TRt%2F2uonf1DSjcV%2FBRgYiX%2FouXs9mRR5%2FJe3uefZm8nRLPQ2fWlLlS3uAsKCFek7biZEE6bOeyFUF0ZHJvOpQTEF3l9Qt3SNIGG2LqCpf5daRIoN7upcDuvcIbDUZz%2FDpwrd3TixEQZLfLg8CluxSogBbIDWrNIbiRprWzYyUzrRfHpPbVKeVwji0gXAJfHHuFqxvolEp2UMbKdeBn2I%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote><h2>AI Agent 组成、架构及开发框架</h2><p>在Google 的Agents 白皮书中, 针对AI Agent的核心功能, 给出了其组成构件和架构, 如下图所示.</p><p><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdm5If" alt="image.png" title="image.png" loading="lazy"/></p><p>AI Agent 包含三个必不可少的组件:</p><ol><li>The model(模型)<br/>在AI Agent 范畴, 模型是指大语言模型(LLM), 作为Agent的中心化决策者, 进行任务分解、执行和检测任务是否完成. 一般来讲, Agent 中使用的模型, 基于推理和逻辑框架(如, ReAct, Chain-of-Thought, 或者 Tree-of-Thoughts), 需要有很强的指令遵循能力.</li><li>Tools(工具集)<br/> 尽管模型在文本、图片等方面具有很强的生成能力, 但由于模型不能与外部世界交互, 模型的能力受到极大约束. 而工具可以抹平这种裂痕, 使得Agents 可以与外部数据和服务进行交互, 更大范围拓展Agent 除模型本身之外的能力.</li><li>The orchestration layer(编排层)<br/>编排层描述Agent 如何 接收外部信息, 执行内部推理(Reasoning), 使用推理结果来确定下一步动作和决策的这一循环过程.这个过程会一直循环, 直到Agent完成任务目标或者达到停止点.</li></ol><p>Agent 的工作流程如下图所示:</p><p><img width="723" height="335" referrerpolicy="no-referrer" src="/img/bVdm5Ih" alt="image.png" title="image.png" loading="lazy"/></p><ol><li>AI Agents 与外部环境交互, 并从用户接收输入、触发或者目标.</li><li>通过模型的推理, 将目标分解成特定的动作和步骤, 并确定任务的顺序</li><li>任务执行过程中, AI Agents 可以访问内部的数据和工具(如知识库、企业系统)以及通过MCP 协议访问外部的工具(如第三方数据库, Web 搜索等)</li><li>AI Agent Orchestration 组件循环执行 Reasoning -&gt; Action Plan -&gt; Execution 这个Loop. 循环过程中可能会有安全护栏(Guardrails) 来确保道德、运营和安全标准</li><li>由于Agent 执行过程可能会执行多轮循环, 受限于模型的上下文窗口长度以及对模型精度的要求, AI Agent 使用 Memory 来维护执行过程的上下文, 从前面的迭代或者过去的执行中学习, 提升Agent的性能表现</li><li>在Multi-Agent 场景, Agent -to -Agent(A2A) 协议被用来实现Agent 之间的协作.</li></ol><p>Strands Agents SDK 是一个开源的AI Agent 开发框架, 使用Strands Agents SDK, 可以利用几行代码快速开发生产可用的multi-agents AI系统. 具有丰富的特性:</p><ol><li>Agent Loop: Strands Agents 实现了处理用户输入、决策、执行工具、响应生成的循环过程, 这个过程支持复杂的、多步骤的推理和动作执行, 并且可以无缝的集成Model 和</li><li>在Model 方面, 提供多种Model Provider 实现, 如Amazon Bedrock, Anthropic, OpenAI, Ollama, LiteLLM 等, 开发者可以灵活 的选择各种商业或者开源的模型来驱动AI Agent.</li><li><p>在Tools 方面, Strands Agents 支持</p><ul><li>Python Tools: 支持 以@tool 函数装饰器方式和基于Python 模块方式来定义Agent 可用的Tools</li><li>Model Context Protocol(MCP) Tools: 集成MCP Server 的Tools 作为 Agent 可用的Tools, 并且支持STDIO 和 Streamable HTTP 两种Transport 类型的MCP Server</li><li>Agent Tools: 在multi-agent 场景, Strands Agents 框架支持将其他Agent作为 当前Agent的一个Tool 来完成特定领域的任务.</li></ul></li></ol><p>Strands Agents 自带了丰富的、开箱即用的基础工具, 如文件读写、memory(mem0_memory 和agent_core_memory)、browser use(local_chromium_browser和agent_core_browser(remote))、Code Interpretation(python_repl 和 agent_core_interpreter)、多模态(image_reader, generate_image, nova_reels, diagram) 等等, 极大地方便了Agent 应用的开发, 使得开发者可以像搭积木一样, 快速开发出自己的Agent.</p><ol start="4"><li><p>在Multi-Agent 方面, Strands Agents 支持以下模式:</p><ul><li>Agent-to-Agent(A2A) protocol, 支持不同平台和实现的AI Agents 间无缝沟通</li><li>Agents as Tools, 通过编排(orchestration) Agent 来接收用户输入并决定使用哪个特定的Agent 来完成相应的任务, 也就是将特定的Agent 当具体的工具来用</li><li>Swarm, 多个Agent 以团队的方式共同来完成复杂任务, 这种方式允许Agent 间自主协作, 共享上下文以及记忆来完成共同的任务</li><li>Graph, 以有向无环图(DAG)的方式编排各个Agent, Agent作为节点, 以DAG结构图作为执行顺序, 一个节点(Agent) 的输出将传播到与其连接的节点, 并作为其输入. 这种模式可以实现非常的复杂的Agent系统</li><li>Workflow, 这种模式适合复杂的多步骤流程, 并且任务间有依赖关系, 必须等上一步执行完成后, 才能执行下一步, 每一步都由一个专家Agent 来完成对应的任务.</li></ul></li></ol><p>除上述核心特性外, Strands Agents SDK 在Agent 的安全性、应用的可观测性以及Agent 部署方面提供了原生支持, 可以说, Strands Agents SDK 提供了开发AI Agent所需的方方面面, 使开发者可以便捷的、快速的开发自己的Agent 应用.</p><h2>AI Agent 构建和运行时</h2><p>Strands Agents SDK显著提升了AI Agent的开发效率，但仍存在两个关键挑战：首先，开发完成后的Agent部署与运行管理问题；其次，尽管SDK已经极大地简化了开发流程，但依然需要编写代码，对非技术背景的业务人员形成了使用障碍. 进一步思考 :</p><ol><li>是否可以不用写代码就可以构建自己的Agent, 这样业务人员都可以快速构建想要的Agent</li><li>Agent 构建好后, 是否可以不用考虑Agent 的部署, 立即可运行, 对外提供可访问的端点</li><li>很多业务场景需要Agent 周期性运行, 是否能让配置或者构建好的Agent 按时调度运行</li></ol><p>从工程实现的视角对AI Agent进行解构，我们可以识别出其由以下四个核心组件构成:</p><ol><li>Model(LLM): Agent 的大脑</li><li>Tools: Agent 的 触手</li><li>System Prompt: Agent 的行为规范</li><li>Envrioments: Agent 需要感知的环境信息, 如系统环境变量, 当前对话的文档等</li></ol><p>基于上述考虑, 我们可以通过配置的方式 声明一个AI Agent 这四部分分别是什么, 然后通过代码根据这四部分的配置, 动态构建出一个 AI Agent 实例, 并提供Agent 的运行环境,  实现 1) 通过配置即可构建Agent, 2) Agent 构建即可运行, 使AI Agent 可以快速在企业内部进行落地实施.</p><h2>基于Strands Agents SDK 的 AI Agent 构建和运行时架构</h2><p>基于上述思考, 笔者基于Strands Agents SDK 开发了一个AI Agent 的构建和运行平台(参考实现)-<a href="https://link.segmentfault.com/?enc=q3mw%2BPHU5mkj48c25gFNnA%3D%3D.jfxIdXSNLkVvgV0yv4j%2BUtXcLV0PgnK1OjHisqohSjoD%2FyPjgmH2fx%2BLmivjRoSWi6BKIZym810mqkr4LSjqpA%3D%3D" rel="nofollow" target="_blank">AgentX</a>. 可以实现:</p><ol><li>通过配置的方式来构建AI Agent. 支持Strands Agents 自带的Tools, MCP Server Tools 以及Agent as Tool, 并且支持各种模型接入.</li><li>配置好的Agent 可以通过API Endpoint 来进行调用, 无需构建专门的Agent 运行时, 真正做到Agent 配置即可运行</li><li>支持 Agent 定时调度执行配置</li><li>支持Agent 运行历史记录管理</li></ol><p>AgentX 架构如下图所示:</p><ol><li>使用ECS 来部署整个应用, 使用ECS Fargate 作为Capacity Provider</li><li>整体应用分为前端、后端、以及各种MCP Server</li><li>使用Amazon DynamoDB 来存储 Agent 的配置以及Agent的运行历史记录</li><li>使用Amazon Lambda 和 EventBridge 实现Agent 的定时调度执行</li></ol><p><img width="723" height="724" referrerpolicy="no-referrer" src="/img/bVdm5ME" alt="image.png" title="image.png" loading="lazy"/></p><p>整个项目可以通过CDK 一键部署, 部署前需要将前、后端应用以及需要部署的MCP Server 构建成Docker 镜像并推送到的Amazon ECR 中.</p><h2>配置Agent</h2><p>部署完成后, 在Agent 管理页面, 可以配置Agent. 配置Agent的过程 实际就是对Agent 使用的模型, System Prompt, 需要使用的Tools以及环境变量参数进行设置.</p><p><img width="586" height="1000" referrerpolicy="no-referrer" src="/img/bVdm5MF" alt="image.png" title="image.png" loading="lazy"/></p><h2>运行和调用Agent</h2><p>Agent 配置完成后, 可以通过在UI 界面上选择配置好的Agent 来完成相应的任务, 如下图所示. 也可以在应用层面, 通过API Endpoint 来调用Agent. 可以看到, 我们配置好Agent 之后, Agent 立即可运行, 不需要额外的部署.</p><p><img width="723" height="160" referrerpolicy="no-referrer" src="/img/bVdm5MH" alt="image.png" title="image.png" loading="lazy"/></p><p>Agent 执行 以及 MCP Server Tools 调用如下图所示.</p><p><img width="723" height="647" referrerpolicy="no-referrer" src="/img/bVdm5MI" alt="image.png" title="image.png" loading="lazy"/></p><h2>MCP Server 管理</h2><p>可以对企业内部或者一些公开的MCP Server 进行维护和管理, 通过配置的方式,  将这些MCP Server 的 Tools 作为Agent的Tools. 目前该项目只支持Streamable HTTP Transport 类型的MCP Server.</p><p><img width="723" height="339" referrerpolicy="no-referrer" src="/img/bVdm5MJ" alt="image.png" title="image.png" loading="lazy"/></p><h2>Agent 调度</h2><p>通过配置Cron 表达式的方式, 来定时调度Agent 执行任务. 对于需要周期性让Agent 来完成某项任务的场景提供了开箱即用的支持.</p><p><img width="723" height="343" referrerpolicy="no-referrer" src="/img/bVdm5MK" alt="image.png" title="image.png" loading="lazy"/></p><h2>应用场景</h2><p>使用上述Agent 构建和运行时平台, 用户可以快速在以下场景(但不限于)落地Agent应用.</p><h2>智能数据分析</h2><p>结合数仓 Redshift MCP Server(或者其他的数据库MCP Server), 可以配置Agent 来实现基于Agent 的数据分析, 如对数据进行预测性分析或者诊断性分析. 对于简单的查数以及指标计算场景, Agent 结合数据库MCP Server就能很好的支持, 如果数据表比较多, 表间的关系比较复杂, 在结合MCP Server 的同时, 可以将Schema 信息放到RAG 中, 将RAG 也作为Agent 的工具, 来实现 Chat BI 或者Text2SQL 的场景.</p><p>通过配置Redshift MCP Server 作为Agent 的工具, 来实现游戏埋点事件数据(模拟数据)的预测性分析, 如下图所示. Agent 会充分利用MCP 工具以及的模型本身的知识, 构建数据预测模型, 对基础数据进行预测性分析, 并将分析结果以HTML页面的方式进行可视化展示.</p><p><img width="651" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MO" alt="image.png" title="image.png" loading="lazy"/></p><h2>基于Agent 的云端资源巡检</h2><p>在一些关键业务场景中, 企业IT运维人员需要定时对资源的各项运行指标进行检查, 来确保基础架构以及上层应用的稳定运行. 由于需要检查的方面比较多, 如数据库运行指标检查, EC2 运行指标检查, EKS 集群运行检查, 可以每个方面的检查配置一个专门的Agent来执行, 最后通过一个编排(Orchstrator) Agent 来统筹执行整个巡检任务, 将不同方面的巡检指派给不同的Agent, 也是Multi-Agent 的场景.</p><p>下图是一个编排 Agent 结合两个其他的Agent(MySQL 巡检Agent 和 EC2巡检Agent) 的Multi-Agent 配置.</p><p><img width="643" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MQ" alt="image.png" title="image.png" loading="lazy"/></p><p>保存配置后, 此编排Agent 就可以对Amazon EC2 和 RDS 资源进行运行指标巡检, 编排Agent 对巡检任务进行分解, 将EC2的巡检任务指派给 EC2巡检Agent, 将RDS 资源巡检的任务交给RDS巡检Agent. 其中EC2 巡检结果如下图所示, 可以看到, EC2 巡检Agent 识别到有EC2 CPU峰值利用率 超过 93.89%, 超过设定的阈值(75%)18.89%.</p><p><img width="723" height="490" referrerpolicy="no-referrer" src="/img/bVdm5MR" alt="image.png" title="image.png" loading="lazy"/></p><p>RDS 巡检结果如下图所示, RDS 巡检Agent 查找到有RDS 实例可用内存(55.28MB)小于阈值(500MB).</p><p><img width="723" height="478" referrerpolicy="no-referrer" src="/img/bVdm5MS" alt="image.png" title="image.png" loading="lazy"/></p><p>最后编排Agent对上述检查结果进行统一整理和分析, 并给出行动建议, 非常详尽, 对于查找的问题, 可以添加其他的Tools, 如slack tool, 将巡检报告以及查找的问题发送到对应的Channel, 实现告警通知.</p><p><img width="589" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MT" alt="image.png" title="image.png" loading="lazy"/></p><h2>远程Browser Use 和 Code Interpreter 沙箱</h2><p>在内容营销的场景, 可能需要通过浏览器自动化网络内容操作, 需要Agent 有Browser Use 的能力; 在一些数据分析的场景, 需要执行代码对数据进行分析, 需要有安全的代码执行环境. 对于上述两种场景, Strands Agents 已经集成了Amazon AgentCore 中的 Browser Use 和 Code Interpreter 工具, 在笔者构建的平台上也可以通过配置的方式将这两个工具集成到对应的Agent中. 下面分别是AgentCore Browser Use 和 Code Interpreter 工具在Agent中的使用示例.</p><p><img width="576" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MU" alt="image.png" title="image.png" loading="lazy"/></p><h2>亚马逊云科技知识专家</h2><p>将Amazon Knowledge MCP 作为Agent 的工具, 可以使用户立刻化身亚马逊云科技知识专家, 对于亚马逊云科技服务的使用, 最佳实践都可以通过Agent 来回答, 如下图所示.</p><p><img width="619" height="500" referrerpolicy="no-referrer" src="/img/bVdm6o3" alt="image.png" title="image.png" loading="lazy"/></p><p>由于本平台很好的提供了Agent的构建和运行时环境, 用户可以根据实际的业务场景需求来快速构建出Agent, 并将Agent应用到实际的业务场景中去. 如果业务场景需要用到更多的工具, 可以开发相应的MCP Server 或者开发对应的Strands Agents Tool 来满足Agent 的需求.</p><h2>总结</h2><p>本文对AI Agent 的组成和架构进行了阐述, 介绍了AI Agent 开发框架 Strands Agent SDK 及其相关特性, 并基于Strands Agent SDK 开发了一站式Agent 构建和运行时平台AgentX, 支持以配置的方式构建Agent, 配置完Agent 即可运行, 用户可快速将Agent 应用到的Agentic Data Analytics、智能IT巡检等业务场景中.</p><p><strong>参考资料</strong></p><ol><li><a href="https://link.segmentfault.com/?enc=XBOgilGYTBNmX1fV6QmAJg%3D%3D.hELbG3qlKNu8hJxFqtBNDs8boFdgSIueLvC%2FnY2BqeSnpRGXmZ1PjKXqgDZm%2BhlTl4eVlORJo%2FpjG0h6IU3UsoeQ%2FKR72HcBMLBwzFRhLD8%3D" rel="nofollow" target="_blank">Google Whitepaper on AI Agents</a></li><li><a href="https://link.segmentfault.com/?enc=pzNzPiao9yvUP3%2FeBmHbSg%3D%3D.ATc62mBjq%2Be%2FtnL0zpKvhjVJYTixP0cXUi8HiSCsSHvvRkG20x2pbQmsATTbboj7t6rj6EWTleM%2FVtPnWG1IiB%2F7KnJEn1N6wiZCc3bICPnMo7uxXeu1q%2B3y%2B92hgo3D" rel="nofollow" target="_blank">Rising of Agentic AI</a></li><li><a href="https://link.segmentfault.com/?enc=3OW0TlKNNJeZkIwFZji8OQ%3D%3D.f3Onwo06ePbrpmgnZAuog4C0Zo%2BEmUJIDQFmJcgW71I0oHOdohHg8Un9yYR1k%2FDqqzOSsWxn%2Fqq2%2Bokxj%2BcnCw%3D%3D" rel="nofollow" target="_blank">AgentX Repo</a></li><li><a href="https://link.segmentfault.com/?enc=Yb4CYPBnb0mSfttSAhuW%2Fw%3D%3D.vNGx%2B7MFtPrXUZxl1e%2FL5PxzRdzIfdf%2BV%2FTvdDhg8Qsxq8M6rUUwsnzdVd9pgL89qo0%2Fokbz6Sm6NpI09Hjqk8INNlxN%2F05CMWMQlkYogLhQ3Bt2bFA3%2BzfGUuIns1Q%2B" rel="nofollow" target="_blank">Amazon Bedrock AgentCore</a></li></ol><p><em>*前述特定亚马逊云科技生成式人工智能相关的服务目前在亚马逊云科技海外区域可用。亚马逊云科技中国区域相关云服务由西云数据和光环新网运营，具体信息以中国区域官网为准。</em></p><p><strong>本篇作者</strong><br/><img width="723" height="156" referrerpolicy="no-referrer" src="/img/bVdm5M2" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=UG83utKYCtbEAW%2B3xQkdaw%3D%3D.NPEfyXlGBJYiO7b1BtQ99CIok9mm9J%2B8hP8EfUGcrN8Sq4hOoJc72%2FCbVmqEtU1XTMVWpSVEirimzR%2FLoKE9OXvWkE6Mxq5X0yp2X2ISMvzTa21FeOrPTY9wYG%2FrV9YZ3VEjjUvMTNKOnevvXw5tXjiYyWp5SBEaNfoUM6dZCWK9fYqPt%2F3VbE55jysqlkk%2BO0x4jqHGHg3OB3E1oeoK2lp23OK2yaJ1BM7gLSRILLw%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=r9Zumd6Wu5JK3xTvnZJoeA%3D%3D.7MwnX2opz1UoXfHVZsTu25OYKr%2FFRteHqqn8vXJ%2BRBl4Tyk83fyI099fVcFqgb9d03ShmxRE9iCZTwavX5MUPvu0DhlkZHATXfq2erNQoP64S6N%2B%2FaGbVviIQQQ0N4wOWOXbngCY5p1QiwKQN%2BmaLYR50KPe0t9a%2F18dfZHzMtt0w0Cxxd2Anw%2BrHcqAg1Uq9YOgpnLVoJ%2BQ8K7zi1Y5kFXXY040m7EFXS7yNlAaQLM%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote>]]></description></item><item>    <title><![CDATA[蜜蜂目标检测数据集的构建与标注方法（70]]></title>    <link>https://segmentfault.com/a/1190000047412956</link>    <guid>https://segmentfault.com/a/1190000047412956</guid>    <pubDate>2025-11-20 00:03:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>蜜蜂目标检测数据集的构建与标注方法（7000张图片已标注划分）</h2><h3>背景</h3><p>随着人工智能和计算机视觉技术的不断发展，目标检测任务在多个领域中都得到了广泛的应用。尤其是在农业和生态研究领域，自动化目标检测技术逐渐成为提高生产效率、保障生态环境的重要工具。蜜蜂作为生态系统中的关键物种，其活动的监控和分析对农业、生态环境保护及科学研究都具有重要意义。</p><p>为了更好地实现蜜蜂目标检测，本数据集专门设计并收集了高质量的蜜蜂图像，旨在帮助研究人员和开发者构建和训练蜜蜂目标检测模型。通过这些数据，相关的机器学习模型可以高效识别蜜蜂，并应用于各类场景，如农业监控、生态保护和无人机监控等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412958" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>数据集获取</h3><blockquote>链接:<a href="https://link.segmentfault.com/?enc=rr73oEcuMQPIjA%2F9amxGOw%3D%3D.WFVR5x8DUFc4WHg5pEYXXwcOQQm1s%2B9ECjb6uPbR1ymKf2%2FuQrwTtg1TYycKE6wBjHPb1n9ll%2BdfUPFZpC9UkA%3D%3D" rel="nofollow" target="_blank">https://pan.baidu.com/s/19wEg4vB6d-SjhPlBJzx7aw?pwd=gy1b</a> 提取码:gy1b 复制这段内容后打开百度网盘手机App，操作更方便哦</blockquote><p>本数据集专为蜜蜂目标检测任务设计，包含了八千张高质量的图像，适用于训练、验证和测试蜜蜂检测模型。数据集中的每张图片均经过精心标注，旨在为目标检测模型提供足够的数据，帮助其高效地检测蜜蜂。</p><p>数据集结构</p><p>数据集包含三个主要部分：</p><p>训练集 (train)：用于训练目标检测模型的图像数据。</p><p>验证集 (valid)：用于在训练过程中进行验证，帮助调节模型的超参数。</p><p>测试集 (test)：用于评估最终模型的性能，确保模型的泛化能力。<br/>标签与类别</p><p>本数据集当前支持检测的唯一目标是蜜蜂。数据集中使用了以下标签：</p><p>类别数 (nc): 1</p><p>类别名称 (names): ['bees']</p><p>数据集特点</p><p>八千张图像：数据集包含了丰富的蜜蜂图像，适用于训练深度学习模型。</p><p>高质量标注：每张图片中的蜜蜂均经过精确标注，标注格式为常见的目标检测格式（如YOLO格式）。</p><p>多样化场景：数据集中的蜜蜂出现在不同的环境和场景中，包括花丛、树枝、空旷地等，增加了模型的泛化能力。</p><p>应用场景</p><p>此数据集可广泛应用于以下领域：</p><p>农业监控：自动化检测蜜蜂活动，为农业研究提供支持。</p><p>生态研究：为蜜蜂行为、种群动态等生态研究提供数据支持。</p><p>无人机监控：结合无人机图像采集，对蜜蜂进行监控和分析。</p><p>使用说明</p><p>数据格式：本数据集采用常见的目标检测数据格式，适配YOLO、Detectron2、TensorFlow等深度学习框架。</p><p>训练建议：对于YOLO等模型，可以直接利用此数据集进行训练与测试。建议将数据集按照80%-10%-10%划分为训练集、验证集和测试集。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412959" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412960" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>数据集概述</h3><p>本数据集包含了约八千张高质量的蜜蜂图像，图像内容多样且经过精确的标注，支持多种目标检测框架。数据集被划分为训练集、验证集和测试集，确保在不同阶段对模型进行全面的评估与优化。</p><ul><li><strong>训练集 (train)</strong>：用于训练目标检测模型的图像数据，帮助模型学习蜜蜂的特征。</li><li><strong>验证集 (valid)</strong>：用于在训练过程中进行验证，调整模型超参数，防止过拟合。</li><li><strong>测试集 (test)</strong>：用于评估训练后的模型性能，确保模型在实际应用中的泛化能力。</li></ul><h3>数据集详情</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412961" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>图像数量</h4><p>数据集总共包含八千张蜜蜂图像，每张图像都经过精心选择，确保图像的多样性和质量。这些图像涵盖了不同环境中的蜜蜂，包括花丛、树枝、空旷地等。每一张图片都包含至少一个蜜蜂目标，且图像中蜜蜂的位置和类别已精确标注。</p><h4>标注与格式</h4><p>每张图片中的蜜蜂都被标注为一个目标，采用常见的目标检测格式（如YOLO格式），便于与深度学习框架兼容使用。数据集支持的标签如下：</p><ul><li><strong>类别数 (nc)</strong>：1</li><li><strong>类别名称 (names)</strong>：['bees']</li></ul><p>数据格式采用了标注框（bounding box）方式，确保了数据在不同深度学习框架（如YOLO、Detectron2、TensorFlow）中的高效使用。</p><h4>场景多样性</h4><p>数据集中的蜜蜂图像呈现了多种环境背景，包括但不限于：</p><ul><li><strong>花丛</strong>：蜜蜂在花朵上采蜜或飞行。</li><li><strong>树枝</strong>：蜜蜂在树枝附近活动，适应不同的自然环境。</li><li><strong>空旷地</strong>：蜜蜂在没有遮蔽的空旷环境中飞行或停驻。</li></ul><p>这些场景的多样性大大增加了数据集的泛化能力，能帮助模型识别不同环境中的蜜蜂，增强其在实际应用中的效果。</p><h3>适用场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412962" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>本数据集广泛适用于以下领域：</p><h4>1. 农业监控</h4><p>在农业中，蜜蜂作为重要的授粉昆虫，直接影响农作物的产量和质量。利用目标检测技术对蜜蜂进行实时监控，可以帮助农业研究者分析蜜蜂活动的规律，及时发现蜜蜂数量的变化，从而为农作物的授粉提供有效支持。</p><h4>2. 生态研究</h4><p>蜜蜂的行为与种群动态在生态学研究中具有重要价值。通过本数据集，生态研究人员可以分析蜜蜂的分布、迁徙模式及其生态环境的变化。这些数据为生态系统的保护和蜜蜂种群的可持续性提供了宝贵的数据支持。</p><h4>3. 无人机监控</h4><p>结合无人机的图像采集技术，可以实现蜜蜂的大范围监控与数据采集。利用目标检测模型对无人机拍摄的图像进行分析，可以实时获取蜜蜂活动数据，辅助研究人员在广阔区域内对蜜蜂进行精确的监测。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412963" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>目标检测模型训练建议</h3><p>本数据集适用于主流的目标检测框架，如YOLO、Detectron2、TensorFlow等。为确保高效训练，以下是一些使用建议：</p><h4>1. 数据划分</h4><p>建议将数据集按照以下比例进行划分：</p><ul><li>训练集：80%</li><li>验证集：10%</li><li>测试集：10%</li></ul><p>这样的划分可以确保训练过程中模型能够在验证集上进行及时调优，同时使用测试集评估最终模型的泛化能力。</p><h4>2. 模型选择</h4><ul><li><strong>YOLO</strong>：YOLO系列模型非常适合目标检测任务，训练速度快，检测精度高，且支持实时推理。使用YOLO框架进行训练时，可以直接加载YOLO格式的标注数据集。</li><li><strong>Detectron2</strong>：Detectron2是Facebook AI研究院开发的目标检测框架，具有强大的功能和灵活性，适合进行高精度的目标检测任务。其支持多种标注格式，易于扩展与调试。</li><li><strong>TensorFlow</strong>：TensorFlow框架也是进行目标检测任务的重要选择，支持训练多种目标检测模型，如Faster R-CNN、SSD等。</li></ul><h4>3. 训练技巧</h4><p>在训练过程中，可以采用以下技巧来提高模型的性能：</p><ul><li><strong>数据增强</strong>：可以通过旋转、翻转、调整亮度等方式对图像进行数据增强，从而提高模型的鲁棒性。</li><li><strong>超参数调整</strong>：在验证集上进行模型超参数调优，特别是学习率、批量大小等关键参数。</li><li><strong>早停机制</strong>：设置早停机制，防止过拟合，并减少训练时间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412964" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3>结语</h3><p>蜜蜂目标检测数据集是一个高质量、丰富多样的数据集，适用于各类目标检测任务，特别是蜜蜂行为分析、农业监控和生态保护等领域。通过充分利用该数据集，研究人员和开发者能够训练出高效、准确的蜜蜂检测模型，并将其应用于多种实际场景中，为农业生态保护与研究提供数据支持。</p><p>随着目标检测技术的不断进步，未来我们期望能进一步扩展数据集，增加更多的图像类型和检测目标，为深度学习和人工智能技术的广泛应用提供更强大的数据支持。</p>]]></description></item><item>    <title><![CDATA[为什么实时更新场景下 Doris 查询性]]></title>    <link>https://segmentfault.com/a/1190000047412991</link>    <guid>https://segmentfault.com/a/1190000047412991</guid>    <pubDate>2025-11-20 00:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数据驱动的商业环境中，企业越来越依赖数据分析来驱动决策。无论是用户行为分析、业务报表还是运营监控，企业都需要具备快速、高效的数据处理能力。企业在数据分析能力上的演进，往往始于 TP（事务处理）系统，随着业务发展不断探索 TP 系统的扩展方案，最终走向构建独立的 AP（分析处理）系统。</p><h2>企业实时分析典型演进过程</h2><h3>第一阶段：使用 TP 系统支撑事务处理和数据分析</h3><p>在企业信息系统建设的早期，主要存储在 OLTP（在线事务处理）系统中，比如 PostgreSQL、MySQL、SQL Server 等。因为数据“就在那儿”，最自然的方式就是直接从 TP 系统中执行 SQL 查询来获取所需分析数据：</p><ul><li>查询订单、用户、库存、交易等业务数据；</li><li>生成运营报表，支撑内部管理；</li><li>快速开发查询接口，满足临时的 BI 需求。</li></ul><p>在业务初期，数据规模有限，分析需求也相对简单，系统架构轻量，能够高效支撑当下业务，避免了多系统部署和复杂数据流带来的额外成本与运维压力。然而，随着业务快速发展，数据量迅速增长，分析场景愈发复杂，查询延迟上升，事务与分析负载相互影响，原有系统逐渐难以支撑持续扩展的业务需求。</p><h3>第二阶段：探索 TP 系统的扩展方案</h3><p>为了在不破坏 TP 系统稳定性的前提下支撑持续扩展的业务需求，部分企业开始尝试基于 TP 的扩展方案。以下是行业中常见的做法与代表系统：</p><p><strong>1. 分片与分布式扩展：Citus、TiDB、Vitesse</strong> </p><p>这些系统具备了基于关系数据库的水平扩展能力，使 TP 系统可以存储更大规模的数据并处理更高并发。</p><ul><li><strong>Citus</strong>：是 PostgreSQL 的分布式扩展插件，可将表自动切分到多个节点，实现并行处理和查询加速。</li><li><strong>TiDB</strong>：兼容 MySQL 协议的分布式 HTAP 数据库，事务与分析融合，适用于在线业务+报表的场景。</li><li><strong>Vitesse</strong>：基于 MySQL 的分布式中间件，解决数据库扩展、容错和自动化问题，常用于大规模 TP 系统。</li></ul><p><strong>2. 读写分离与多副本架构：Amazon Aurora</strong></p><ul><li><strong>Amazon Aurora</strong> 提供了高性能、弹性扩展的云数据库，并支持最多 15 个只读副本，用于分担查询压力。这类方案适用于中等复杂度的分析任务，但在海量数据和复杂查询面前，Aurora 等 TP 架构依然面临查询瓶颈。</li></ul><p>这些方案虽然在一定程度上缓解了单机性能瓶颈，但本质仍属于 TP 系统架构，难以根本解决事务与分析并存所带来的矛盾。面对大规模、多维度的聚合分析，这类系统能力有限，查询操作常常干扰写入，导致系统性能波动，影响整体稳定性。同时，架构复杂、运维门槛高，随着写入量和查询压力的持续上升，资源消耗不断加剧，系统成本快速攀升，性能瓶颈也日益显现。尤其在高吞吐数据导入和实时更新方面能力不足，限制了对业务变化的快速响应。而 TP 系统以行存为主的特性，使其在处理 TB 级以上数据的分析任务时，性能与专为分析设计的 AP 系统存在显著差距，难以胜任更复杂、更大规模的分析需求。</p><h3>第三阶段：复杂分析使用 AP 系统</h3><p>随着数据量不断增长、分析需求日益复杂，很多企业逐渐意识到，无法再依赖原有的 TP 系统同时满足事务处理与分析需求。因此，企业通常会将一些复杂的分析查询迁移到专门的 AP 系统中，例如 Redshift、Snowflake、BigQuery 等，用于支撑大规模的数据分析任务。而对于对实时性要求高、并发量大的查询，仍会保留在 TP 系统中运行，以确保系统的快速响应和稳定性。在一些高并发场景中，数据甚至会在 AP 系统中完成加工处理后，回流到 TP 系统中，进一步支撑实时查询和业务服务。</p><h3>第四阶段：拥抱 AP 系统，实现分析计算与事务的解耦</h3><p>随着数据规模持续扩大，事务处理系统（TP）的数据导入速度难以跟上行为数据生成的节奏，导致数据延迟持续增加。与此同时，部分复杂查询被转移到分析处理系统（AP）执行，其他分析任务仍在 TP 系统中完成，这使得系统运维难度和资源成本不断攀升，远超专门的 AP 系统。</p><p>下表总结了 OLTP 与 OLAP 在关键维度上的主要区别，便于理解两者在架构定位上的差异。</p><h3>OLTP vs OLAP 对比</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412993" alt="OLTP vs OLAP 对比.jpeg" title="OLTP vs OLAP 对比.jpeg"/></p><p>面对这种挑战，越来越多企业开始认识到：对于实时分析场景，采用具备高并发与高性能的一体化 AP 系统来统一承载，能够大幅提升效率；而批量处理和离线分析等需求，则可以选择更适合的 AP 系统来完成。这样不仅优化了整体架构，也有效控制了成本，同时提升了数据分析能力，助力企业实现更加高效的数据驱动运营。</p><p>实时数据需求直接导入实时 OLAP 系统，常见做法是将事务处理系统（TP）与分析处理系统（AP）解耦，通过变更数据捕获（CDC）技术，实现 TP 系统数据的实时同步，同时行为数据也直接写入实时 OLAP。实时 OLAP 系统需具备快速更新能力和高效查询性能。该架构不仅避免了对核心业务系统的影响，还使企业能够第一时间获取并分析最新数据，广泛应用于用户行为分析、实时报表、风险监控等关键场景，显著提升了数据决策的及时性和价值。</p><h2>OLAP 系统的选择：为什么是 Apache Doris？</h2><p>Apache Doris 适合大数据量下需要高并发查询、AdHoc 和实时数据更新的场景：</p><h3>低延迟高吞吐写入</h3><p>支持多种数据导入方式，通过 <code>Stream Load</code> 可实现数据<strong>秒级可见</strong>、单节点写入吞吐可达<strong>百万行/秒</strong>，轻松满足海量数据的实时入库需求。</p><h4>用户案例：网易云音乐</h4><p>网易云音乐作为知名音乐流媒体平台，每天产生大量用户行为数据、业务数据及日志数据，这些数据在异常行为跟踪、客诉问题定位、运行状态监控、性能优化等方面扮演守护者的角色。面对每日万亿级别数据的增量，<strong>网易云音乐选择使用 Apache Doris 替换 ClickHouse 构建新的日志平台，目前已稳定运行 3 个季度，规模达到 50 台服务器，2PB 数据，每天新增日志量超过万亿条，峰值写入吞吐达 6GB/s。</strong></p><p><strong>在低延迟高吞吐写入场景中</strong>，网易云音乐采用 Flink + Doris Connector 实现流式数据无缝对接，通过多项关键优化措施显著提升了写入性能：</p><ul><li><strong>写入流程优化</strong>：在 append 数据操作时，直接写入压缩流，无需经过 ArrayList 中转，大幅降低内存使用，TM 内存占用从 8G 降至 4G，有效避免了因 batch size 设置过高导致的 OOM 问题。</li><li><strong>单 tablet 导入功能</strong>：开启单 tablet 导入功能（要求表使用 random bucket 策略），极大提升写入性能，解决了写入 tablet 过多时元数据产生过多影响写入性能的问题。</li><li><strong>负载均衡优化</strong>：每个 batch flush 完成后随机选择 BE 节点写入数据，解决 BE 写入不均衡问题，相较之前导入性能提升 70%。</li><li><strong>容错能力增强</strong>：调整 failover 策略，优化重试逻辑并增加重试时间间隔，当 FE/BE 发生单点故障时能自动感知和重试恢复，保证服务高可用。</li></ul><p><strong>在元数据性能优化方面</strong>，针对 HDD 硬盘环境下 Stream Load 耗时突增问题，通过调整 3 台 Follower FE 为异步刷盘模式，实现了 4 倍性能提升，有效解决了同步元数据阶段的严重耗时问题。</p><p><strong>整体收益显著</strong>：查询响应整体 P99 延迟降低 30%，并发查询能力从 ClickHouse 的 200 提升至 500+，写入稳定性大幅改善，运维成本显著降低，在坏盘和宕机场景下实现自恢复能力。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=gucMtvDM7k6n1LBPsCpIgw%3D%3D.v%2Bf67bzW9WNwQCZJRQMDmBt6gHCa7kv8LwAgVj7q16v%2FXJG66SudSQSnZzGHjmsq" rel="nofollow" target="_blank">网易云音乐基于 Apache Doris 的万亿级日志平台建设实践</a></p><h3>实时更新能力强</h3><p>配合主流数据同步工具（如 Kafka、Canal、DataX、SeaTunnel 等）可实现从 TP 系统到 Doris 的准实时数据同步。其“Merge-on-Write”的更新机制兼顾写入性能与更新效率，适配主流 CDC 场景。</p><h4>用户案例 ：中通快递</h4><p>随着中通快递业务的持续增长，昔日双 11 的业务高峰现已成为每日常态，原有数据架构在数据时效性、查询效率、与维护成本方面，均面临着较大的挑战。为此，中通快递引入 SelectDB，借助其高效的数据更新、低延时的实时写入与优异的查询性能，在快递业务实时分析场景、BI 报表与离线分析场景、高并发分析场景中均进行了应用实践。</p><p><strong>在实时分析场景中，基于 SelectDB 灵活丰富的 SQL 函数公式、高吞吐量的计算能力，实现了结果表的查询加速，能够达到每秒上 2K+ 数量级的 QPS 并发查询，数据报表更新及时度大大提高。</strong></p><p>SelectDB 的引入满足了复杂与简单的实时分析需求。目前，SelectDB 日处理数据超过 6 亿条，数据总量超过 45 亿条，字段总量超过 200 列，并实现服务器资源节省 2/3、查询时长从 10 分钟降至秒级的数十倍提升。</p><p>案例回顾：<a href="https://www.bilibili.com/video/BV1eF6HY9Ecr/" target="_blank">中通快递基于 SelectDB 实时数仓的应用实践</a></p><h3>极致的查询性能</h3><p>Doris 天生为分析优化，具备列式存储、向量化执行引擎、位图索引、多级缓存、物化视图等优化手段，能够支撑亚秒级的分析响应时间，并支持复杂的多维分析、聚合与 JOIN 查询。</p><h4>用户案例 ：拉卡拉</h4><p>拉卡拉（股票代码 300773）是国内首家数字支付领域上市企业，从支付、货源、物流、金融、品牌和营销等各维度，助力商户、企业及金融机构数字化经营。随着实时交易数据规模日益增长，拉卡拉早期基于 Lambda 架构构建的数据平台面临存储成本高、实时写入性能差、复杂查询耗时久、组件维护复杂等问题。<strong>拉卡拉选择使用 Apache Doris 替换 Elasticsearch、Hive、HBase、TiDB、Oracle/MySQL 等组件，完成 OLAP 引擎的统一，实现了查询性能提升 15 倍、资源减少 52% 的显著成效。</strong></p><p><strong>在极致查询性能场景中</strong>，拉卡拉充分发挥了 Apache Doris 天生为分析优化的多项能力，在金融核心业务中实现了显著的性能提升：</p><ul><li><strong>风控场景查询优化</strong>：替换 Elasticsearch 后，查询响应时间从 15 秒缩短至 1 秒以内，查询性能提升 15 倍。Doris 的标准 SQL 查询接口和强大的多维分析能力，支持复杂的多表 JOIN、子查询和窗口函数等场景。</li><li><strong>对账单系统高并发处理</strong>：借助 Doris 优秀的并发处理和极速查询能力，支持每日亿级数据规模和百万级查询请求，查询 99 分位数响应时长控制在 2 秒以内，数据延迟可控制在 5 秒。</li><li><strong>倒排索引加速大表关联</strong>：通过添加倒排索引、调整分桶策略以及表结构优化，大表关联查询耗时从 200 秒缩短至 10 秒，查询效率提升超过 20 倍。</li><li><strong>Light Schema Change 灵活变更</strong>：相比 Elasticsearch 需要通过 Reindex 进行 Schema 变更，Doris 的 Light Schema Change 机制更加高效灵活，支持字段和索引的快速增删修改，极大提升了数据管理的便捷性和业务适应性。</li></ul><p><strong>整体收益显著</strong>：服务器数量下降 52%，开发运维效率大幅提升，通过统一的 OLAP 引擎简化了技术架构，降低了学习成本和运维复杂性。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9RWTXpccH0RKojqONkJoZQ%3D%3D.JQykpenpgUuz%2FycWb%2BoIaJAcmw%2FZd3Q3lz7JWwlGa7hpDgEDyEHclmn9x0HFVI9x" rel="nofollow" target="_blank">拉卡拉 x Apache Doris：统一金融场景 OLAP 引擎，查询提速 15 倍，资源直降 52%</a></p><h3>高并发处理能力</h3><p>得益于 MPP 架构，Doris 可轻松扩展计算资源，支持海量用户并发访问分析报表，是支撑数据门户、运营后台、用户行为分析等实时应用场景的理想方案。</p><h4>用户案例：快手</h4><p>快手作为知名短视频平台，OLAP 系统为内外多个场景提供数据服务，每天承载近 10 亿的查询请求。原有湖仓分离架构面临存储冗余、资源抢占、治理复杂等问题。<strong>快手通过引入 Apache Doris 湖仓一体能力替换 ClickHouse，升级为湖仓一体架构，涉及数十万张表、数百 PB 的数据增量处理。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 凭借强大的 MPP 架构和分布式查询引擎，为快手提供了卓越的并发查询支撑能力：</p><ul><li><strong>海量并发查询支撑</strong>：系统每天承载近 10 亿查询请求，覆盖 ToB 系统（商业化报表引擎、DMP、磁力金牛、电商选品）和内部系统（KwaiBI、春节/活动大屏、APP 分析、用户理解中心等）的高并发访问需求。</li><li><strong>智能查询路由</strong>：通过查询路由服务分析和预估查询的数据扫描量，将超大查询自动路由到 Spark 引擎，避免大查询占用过多 Doris 资源，确保高并发场景下的系统稳定性。</li><li><strong>物化视图透明改写</strong>：结合 Doris 的物化视图改写能力和自动物化服务，实现查询性能提升至少 6 倍，百亿级别以下数据可实现毫秒级响应，有效支撑高并发查询场景。</li><li><strong>缓存优化加速</strong>：通过元数据缓存和数据缓存双重优化，元数据访问平均耗时从 800 毫秒降至 50 毫秒，显著提升高并发场景下的查询响应效率。</li></ul><p><strong>整体收益</strong>：实现了统一存储和链路简化，无需数据导入即可直接访问湖仓数据，在支撑海量并发查询的同时大幅降低了运维复杂度和存储成本。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=BPOhVPoCwZe659CHxQTEJQ%3D%3D.wVLvr0LG%2FDtEN%2BiRNHtzmkrnlam20s5eLzJtfS6zGYQRGAVXjcC30urhITwapzr3" rel="nofollow" target="_blank">快手：从 Clickhouse 到 Apache Doris，实现湖仓分离向湖仓一体架构升级</a></p><p><strong>统一数据体验</strong> Doris 提供类 SQL 的接口，兼容 MySQL 协议，易于 BI 工具对接（如 Tableau、Power BI、Superset 等），同时可通过视图、物化视图等能力，提供类似数据仓库的建模支持。</p><p>可以满足的典型场景包括：</p><ul><li><strong>面向用户的实时分析（Customer-Facing Analytics）</strong> 将订单、交易、行为等业务数据从数据库中捕获并同步至实时数仓（如 Apache Doris），支持用户在前端系统中秒级查看订单状态、活动参与情况、积分变化等。提升用户体验的同时，也为推荐、搜索等系统提供最新的数据支持。</li><li><strong>运营监控与分析</strong> 运维、客服、市场等部门可以实时查看关键指标（如系统交易量、失败率、退货率），并快速响应业务波动。CDC 保证了数据与业务系统的实时一致性，使监控结果具有可信度。</li><li><strong>模型训练与特征回填</strong> 将用户行为日志和业务数据同步到分析库后，ML 工程师可以基于最新数据生成训练样本、回填特征值，显著加快模型迭代速度，提升预测准确率。</li><li><strong>多维分析与自助 BI</strong> 将结构化业务数据实时汇聚到 OLAP 系统，结合维度模型，支持业务人员进行灵活多维分析，满足从明细到聚合的多层级洞察需求，减少对开发人员的依赖。</li></ul><h2>ClickHouse 实时更新原理</h2><p>在 ClickHouse 中，虽然底层存储以追加为主，但通过 <code>ReplacingMergeTree</code> 引擎，用户可以实现类似“实时更新”的效果。其核心思想是：在写入时保留所有版本的数据，在后台合并时自动保留最新版本，从而实现数据的“更新”。</p><blockquote>详情<a href="https://link.segmentfault.com/?enc=9WhmDjn7tkDsjNwUmlqbHg%3D%3D.PdmIPkBVO%2BU9Co7%2B0OSNi%2BM9Tg7qx7rC%2B39tHAspchSCAmCWAu6pJeldDFCJ2zV72kgpe8hVH7j%2FaihHEW7g4g%3D%3D" rel="nofollow" target="_blank">参考文档</a></blockquote><p>具体工作原理如下：</p><ol><li><strong>写入阶段</strong> 使用 <code>ReplacingMergeTree</code> 创建表时，通常会指定一个唯一标识列（如主键）和一个版本列（如 <code>update_time</code>）。每次对同一主键的数据更新时，系统不会直接覆盖旧数据，而是插入一行新版本的记录。</li><li><strong>合并阶段（Merge）</strong> ClickHouse 的后台合并线程会在空闲时自动执行数据文件的合并操作。对于 <code>ReplacingMergeTree</code> 表，合并过程中会根据主键值和版本列，自动保留每组主键下版本最新的记录，删除旧版本，实现最终的“更新”语义。</li><li><strong>查询阶段</strong> 查询过程中可能会读到尚未合并的多个版本记录，因此建议设置 <code>FINAL</code> 查询语法，如：</li></ol><pre><code class="SQL">SELECT * FROM my_table FINAL;</code></pre><h2>Apache Doris 实时更新原理</h2><p>在需要频繁更新数据的场景中，可以使用 Doris 提供的 Unique Key 模型来建表，实现对同一主键的数据进行高效覆盖更新。Doris 通过一种名为标记删除（Delete Bitmap）的机制，有效提升查询性能。与 ClickHouse 查询时进行更新清理的方式不同，Doris 的标记删除机制无需在查询时实时计算删除逻辑，因此可以显著减少查询延迟，确保查询响应时间稳定在百毫秒以内，并支持高并发访问。</p><p>具体来说，Doris 的处理分为两个阶段：</p><ol><li><strong>写入阶段</strong> 在使用 Unique Key 创建表时，您通常会指定一个唯一标识（如主键 ID）和一个版本列（如更新时间 <code>update_time</code>）。每当新数据写入时，如果主键相同且版本更新，Doris 会自动为旧数据打上“删除标记”，这些信息随着数据一同写入底层存储。</li><li><strong>查询阶段</strong> 在查询过程中，Doris 会自动识别并跳过那些已被标记删除的旧数据行，无需实时对比或扫描多个版本，从而实现低延迟、高效率的数据读取。</li></ol><p>借助这套机制，Doris 能够同时满足 <strong>实时更新</strong> 和 <strong>高速查询</strong> 的双重需求，非常适合用于用户画像、订单中心、指标快照等典型更新型分析场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412994" alt="Apache Doris 实时更新原理.png" title="Apache Doris 实时更新原理.png" loading="lazy"/></p><h2>性能对比</h2><p>当分析负载从 TP 或者 HTAP 演进到 AP 时，一个常见场景是将 TP 系统中的变更数据（通过 CDC）同步到 AP 系统，用于后续的报表分析和业务监控。这类场景通常涉及大量的<strong>数据更新</strong>，而不仅仅是新增数据，因此对分析系统的更新处理能力和查询性能提出了更高要求。</p><p>为了评估 Doris 和 ClickHouse 在这一类实时更新分析场景下的表现，我们基于典型的行业测试模型 <strong>ClickBench</strong> 和 <strong>SSB（Star Schema Benchmark）</strong> 进行了测试，分别对数据集中的 <strong>25% 和 100% 的记录进行了更新操作</strong>。</p><blockquote><a href="https://link.segmentfault.com/?enc=Rkz5uIE8kpcxxfeFJSPZVg%3D%3D.hohOd08le8PguJkPeOUw95wmr8gEbXWCfKF0X29BqoIq112yHdu%2FPnNEvcb10VAlwf%2BdtA3v6WvdOUJC8EBzkdoCfNlmeuUXAtCfRdEK%2Fps%3D" rel="nofollow" target="_blank">更新 SQL </a> 详情参考</blockquote><p>为确保性能对比的合理性，结合 ClickHouse Cloud 与 SelectDB Cloud 套餐配置的差异，制定了如下测试方案：ClickHouse Cloud 采用双副本，单副本分别配置为 8 核 32GB 和 16 核 64GB；SelectDB Cloud 则采用单副本 16 核 128GB 配置。通过该设计，可在整体资源层面分别实现核数对等（16 核）与内存对等（128GB）的横向对比，从而更全面地评估两者在不同资源维度下的性能表现。</p><p>原始数据：<a href="https://link.segmentfault.com/?enc=n3tBH%2FAGA7dRV4DL7JnIdw%3D%3D.jgTOPi98ezE6vdePS6GxXRUsTlhWYKtVVUWt7zUJqp%2Bfz%2Bq0SxuKgdNF8WSUPYB5PbewiwOwHI2PHibs6T6S1w%3D%3D" rel="nofollow" target="_blank">ssb</a></p><h3>SSB-sf100</h3><h4>ClickHouse MergeTree  vs  SelectDB Duplicate Key</h4><ul><li>SelectDB （16c 128GB）的性能是 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）的 5 倍。</li><li>SelectDB （16c 128GB）的性能是 ClickHouse 16c 64GB（2 replica 每个 replica 16c 64GB）的 9.8 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412995" alt="性能对比-1.png" title="性能对比-1.png" loading="lazy"/></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>25% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 1.6 倍。</li><li>100% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 2.5 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412996" alt="性能对比 02.png" title="性能对比 02.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 14 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 18 倍。</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 25 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 34 倍。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412997" alt="性能对比-2.png" title="性能对比-2.png" loading="lazy"/></p><h3>ClickBench</h3><p>原始数据：<a href="https://link.segmentfault.com/?enc=8%2FUxz8kvcI2zK9nU8NPdEA%3D%3D.jJ%2Fah4d%2Ffwb3VImorzaQ8W3mlwVw3jgjVCfafRQjbrgiW9tOzg9SUhQjZwxoqarnQDuS4dg2gYXcFXZQfureXg%3D%3D" rel="nofollow" target="_blank">clickbench</a></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>ClickBench 下 25% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 170%。</li><li>ClickBench 下 100% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 290%。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412998" alt="性能对比-3.png" title="性能对比-3.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412999" alt="性能对比-4.png" title="性能对比-4.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，查询耗时低 43%</li><li>100% 更新比例条件下，查询耗时低 60%</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，查询耗时低 68%。</li><li>100% 更新比例条件下，查询耗时低 78%。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413000" alt="性能对比-5.png" title="性能对比-5.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413001" alt="性能对比-6.png" title="性能对比-6.png" loading="lazy"/></p><h2>用户案例</h2><h3>案例一：森马服饰（MySQL）</h3><p>森马服饰作为中国休闲服饰和童装领域的领先企业，覆盖线上线下全渠道零售，门店总数达到 8000+ 家。为支撑全域货通中台项目，<strong>森马引入阿里云 SelectDB 替换原 Elasticsearch + 分布式 MySQL 混合架构，统一分析 16+ 核心业务，实现复杂查询 QPS 提升 400%，响应时间缩短至秒级。</strong></p><p><strong>在高并发处理场景中</strong>，阿里云 SelectDB 凭借 MPP 架构为森马提供了强大的并发查询支撑：</p><ul><li><strong>多场景并发支撑</strong>：同时支撑 2B 业务、2C 业务、直营店、加盟商等多场景下的高并发数据分析需求，复杂查询 QPS 达到 200+ 水平。</li><li><strong>资源隔离能力</strong>：基于存算分离架构，在线订单查询服务和离线聚合分析 BI 场景分别使用独立计算组，避免相互干扰，确保高并发场景下系统稳定性。</li><li><strong>弹性扩缩容</strong>：在直播大促等高压力时段，可快速在线扩容应对流量激增，无需停服和数据搬迁，显著提升应对突发高并发的灵活性。</li><li><strong>统一架构简化</strong>：替换双系统架构，统一支持简单过滤查询、海量数据聚合分析、复杂多表关联查询，无需维护复杂业务逻辑来处理高并发多表关联分析。</li></ul><p><strong>显著收益</strong>：亿级库存流水聚合查询缩短至 8 秒内，运维成本大幅降低，业务高峰期系统运行平稳，为全渠道运营提供可靠的高并发数据分析支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=UN61ZOZgmF83qRcbsXftQw%3D%3D.YZGUTNWiBDhN%2B4cyoL7xkBH68wsgKemvxTWKdFKOUZDE%2BFYikwil8tmPB236tG%2FI" rel="nofollow" target="_blank">森马服饰从 Elasticsearch 到阿里云 SelectDB 的架构演进之路</a></p><h3>案例二：天眼查（PostgreSQL）</h3><p>天眼查是一家数据服务公司，为用户提供超过 3 亿家企业的商业、财务和法务信息查询服务，涵盖 300+ 维度数据。随着业务增长，其尽调平台需要支持内部营销和运营团队的即席查询及用户分群等新需求。<strong>该平台使用 Apache Doris 替换了原有的 Apache Hive、MySQL、Elasticsearch 和 PostgreSQL 混合架构，实现数据写入效率提升 75%，用户分群延迟降低 70%。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 的 MPP 架构为平台提供了强大的并发查询支撑能力：</p><ul><li><strong>即席查询能力</strong>：原架构每次新需求都需要在 Hive 中开发测试数据模型，写入 MySQL 调度任务。现在 Apache Doris 拥有全量明细数据，面对新请求只需配置查询条件即可执行即席查询，仅需低代码配置即可响应新需求。</li><li><strong>高效用户分群</strong>：在结果集小于 500 万的用户分群场景中，Apache Doris 能够实现毫秒级响应。通过连续密集的用户 ID 映射优化，用户分群延迟降低 70%，显著提升高并发分群任务处理效率。</li><li><strong>统一架构简化</strong>：消除了多组件间的复杂读写操作，无需预定义用户标签，标签可基于任务条件自动生成，大幅简化用户分群流程，提高 A/B 测试的灵活性。</li><li><strong>稳定数据写入</strong>：支持每天近 10 亿条新数据流入，使用不同数据模型适配不同场景（MySQL 数据采用 Unique 模型，日志数据采用 Duplicate 模型，DWS 层数据采用 Aggregate 模型）。</li></ul><p><strong>显著收益</strong>：数据仓库架构更加简单，对开发者和运维人员更加友好，2 个 Apache Doris 集群承载数十 TB 数据，为客户提供实时、准确的企业信息查询服务。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=nwBEl0YGmwWgqY4Db6LnAA%3D%3D.ljdueJtfWkhKHmOTA5aiOFa%2FY5%2BkoVE0ETjGd8gMVwO4FFclwvbvjgzeqBu1dDvQ" rel="nofollow" target="_blank">秒级数据写入，毫秒查询响应，天眼查基于 Apache Doris 构建统一实时数仓</a></p><h3>案例三：宝舵 BOCDOP（TiDB）</h3><p>宝舵是宝尊集团旗下商业化独立品牌，拥有 1000 余名技术工程师，为集团 8000+ 员工和全球 450+ 品牌提供电商全渠道数据分析服务。<strong>宝舵早期基于 TiDB 构建实时数仓，随着数据量增长面临处理效率、OLAP 扩展、成本等挑战。通过引入 SelectDB 替换 TiDB，实现写入速度提升 10 倍，成本直降 30% 的显著成效。</strong></p><p><strong>在实时更新场景中</strong>，SelectDB 为宝舵提供了强大的实时数据处理能力，特别体现在多源数据同步方面：</p><ul><li><strong>多源异构数据实时接入</strong>：支持 100+ 业务模块的多渠道数据实时接入，通过 Canal、Mongo-Connector、OGG 等工具获取 MySQL、MongoDB、Oracle 等不同类型业务数据库的 binlog，实现秒级延迟数据同步。</li><li><strong>高吞吐实时写入</strong>：利用分区分桶策略与单副本写入机制，在双 11 峰值时段实现每秒百万级数据写入，最高写入速度从 20 万/分提升至 230 万/分，较传统方案提升 10 倍。</li><li><strong>流式数据处理</strong>：通过 Kafka + Flink + SelectDB 流式写入能力，将分散在订单、支付、物流等业务模块的数据实时汇聚，数据同步提速 30%。</li><li><strong>资源隔离保障</strong>：为"作战室看板"单独分配计算资源组，避免高并发查询与实时写入的资源争用，确保关键业务查询响应时间稳定在 500ms 内。</li></ul><p><strong>显著收益</strong>：在双 11 等大促期间数据量达平日 30-60 倍的情况下，实现数据供应 0 事故、报表服务可用性 99.9%，查询性能提升 66%，为多渠道电商运营提供稳定的实时数据支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9wdZQTdAQ9RnY4TC0CbTMw%3D%3D.ATu7H%2Bp3yL5tAvJOK7R9UBSyD7PImHJj%2FYZI8BN%2F%2F%2FAa5UOz%2BB7nFVobDXTqwOlw" rel="nofollow" target="_blank">SelectDB 实时分析性能突出，宝舵成本锐减与性能显著提升的双赢之旅</a></p>]]></description></item><item>    <title><![CDATA[恶意滥用行为 留胡子的饼干_dlibGI]]></title>    <link>https://segmentfault.com/a/1190000047413006</link>    <guid>https://segmentfault.com/a/1190000047413006</guid>    <pubDate>2025-11-20 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从生成式 AI 的惊艳亮相引起全球科技巨头军备竞赛般的投入开始，整个 AI 行业仿佛被注入了无限的想象力。似乎在宣告着即将出现一个生产力即将被彻底解放、商业模式即将被完全颠覆的光明未来。<br/>微软、谷歌、亚马逊等云巨头纷纷将资本支出的绝大部分押注于 AI 基础设施建设，而无数逐利而来的 AI 初创公司，更是如雨后春笋般涌现试图分一杯羹，全球 AI 领域的投资额也达到了史无前例的高度。<br/>然而，正如任何过热的淘金热最终都会迎来冷静期当技术以超乎预期的速度普及时，潜在的负面效应也以同样的速度被放大，正在悄然侵蚀着行业参与者。<br/>从 " 可选项 " 到 " 必选项 " 的巨额支出<br/>根据奇安信集团对外发布《2024 人工智能安全报告》来看，在 2023 年基于 AI 的深度伪造欺诈便已暴增了 3000%，基于 AI 的钓鱼邮件也增长了 1000%；而内容生成环节更是实现规模化生产。<br/>基于 Stable Diffusion 和 GPT-4 的定制模型，可每小时生成 2000 条伪原创研报、800 段逼真视频。暗网平台 "DarkGPT" 更是提供包月服务，1 万美元即可获得每日 5000 条金融虚假内容的产能。<br/>而且 "AI 滥用 " 的后遗症并不仅仅在社会新闻版块，可以说它已经穿透了科技公司的防火墙直接作用于其财务报表。而金融行业正是这场风暴的中心，当 AI 以假乱真的能力被精准地应用于金融诈骗时，其破坏力可以说是指数级的增长。<br/>据行业估算，2024 年由深度伪造技术引发的各类欺诈造成的全球经济损失已高达 120 亿美元。尤其在监管相对滞后、交易更为匿名的加密货币领域，AI 滥用更是如鱼得水。根据相关的报告也显示 2024 年仅 AI 深度伪造技术全年造成的损失便高达 46 亿美元。<br/>随着 AI 滥用事件的频发，过去模糊的 " 伦理指南 " 正在迅速转变为具有强制约束力的法律条文，而且这种转变直接导致了企业合规成本的急剧攀升。<br/>而且一旦出现违规需要付出的代价更是惨痛的，罚款最高可达全球年营业额的数个百分点或数千万欧元，而且合规也不再是法务部门的单一工作，而是渗透到研发、产品、市场的每一个环节。<br/>这些 " 反噬 " 也并非凭空产生，在 AI 商业化过程中对速度和规模的追求，长期以来压倒了对安全和伦理的考量所以形成了这种 " 原罪 "。因此未来合规成本的升高是不可避免的，而欧盟的《AI 法案》可以说是这一趋势的先行者。<br/>该法案于 2024 年 8 月 1 日正式生效并分阶段实施，着重对高风险的 AI 系统施加了严格的合规要求。而且这不仅仅是一项区域性法规，更可能产生 " 布鲁塞尔效应 " 从而影响全球的 AI 监管格局。<br/>监管的落地也将会直接转化为企业的合规成本。据公开信息推算，仅欧盟 AI 法案便可能导致欧洲企业的 AI 采纳成本增加约 310 亿欧元，并使 AI 投资减少近 20%。而美国联邦贸易委员会也已对 OpenAI 展开调查，谷歌等公司也不得不调整其营销话术，避免被处以巨额罚款。<br/>可以说 " 监管的铁幕 " 正在迫使整个行业从过去 " 快速行动，打破陈规 " 的互联网思维转向一种更为审慎、合规驱动的开发模式。可以说这种转变无疑会减缓创新速度并增加运营成本，对于那些资源有限的中小企业和初创公司构成尤为严峻的挑战。<br/>对 " 信任 " 的侵蚀或许是 AI 滥用最难修复的一种<br/>这源于在激烈的竞争压力下，企业急于抢占市场将产品快速推向用户，所以将风险控制和安全测试置于次要位置。但是这种 " 快速行动并打破规则 " 的心态在 AI 时代尤为危险，因为 AI 技术的潜在破坏力远超以往的软件应用。<br/>并且市场对于 AI 技术的可靠性极度敏感，甚至一次小小的失误都可能引发巨大的信任危机和财务损失。谷歌的 Bard 模型之前便在一次演示中出现事实性错误，竟然导致其母公司 Alphabet 的股价在单日内暴跌 7%，市值蒸发超过 1000 亿美元。<br/>并且随着 AI 投资的巨额支出持续攀升，投资者开始担忧其回报前景，这种悲观情绪导致 Meta、Microsoft、Alphabet 和 Nvidia 等 AI 领域的领军企业股价普遍承压下跌，市场也开始讨论 "AI 泡沫 " 的风险，并开始质疑哪些不计成本的 " 军备竞赛 " 式投资。<br/>更何况大量公司缺乏对 AI 伦理的明确责任归属，高管层面也并未对其有所调整。所以 AI 系统的决策过程像一个 " 黑箱 "，在责任主体模糊的情况下滥用和误用的风险便难以控制。企业内部也未建立有效的问责机制。<br/>但是更深层次的原因在于当前主流生成式 AI 商业模式本身所内含的风险。这些模型依赖于海量数据的投喂，其训练过程难以完全避免偏见和有害信息的吸收。而其强大的生成能力却为恶意利用提供了温床。<br/>因此当商业模式的核心是追求更强大的模型、更广泛的应用时，如果缺乏与之匹配的强大 " 安全刹车 " 系统，滥用就成了可预见的副产品。这种商业逻辑与伦理要求之间的结构性失衡才是导致 " 反噬 " 的根本内因。<br/>所以当企业享受了技术红利带来的增长，如今便也不得不为其模式所伴生的风险 " 买单 "。哪怕科技公司以 " 让世界更美好 " 的叙事推广 AI，公众在实际体验中，也会频繁受到隐私泄露、算法偏见、就业替代、虚假信息等负面影响。<br/>这种落差也导致了广泛的 "AI 焦虑 " 和不信任感。公众普遍认为现有的监管法规不足以应对 AI 带来的社会风险期望政府采取更加果断的行动。这种强大的民意压力也是推动监管机构加速行动的根本动力。<br/>面对公众的呼声和潜在的社会风险，监管机构的介入是必然的。但由于技术发展的速度远超立法速度监管往往表现出一定的滞后性，欧盟 AI 法案便被部分人士认为可能增加企业负担、抑制创新。<br/>全球主要经济体在 AI 领域的竞争，也使得监管变得更加复杂。各国都希望在鼓励创新和防范风险之间找到平衡点但这种平衡点的位置各不相同，因此形成了复杂的国际监管格局给跨国企业的合规带来了巨大挑战。<br/>而且这种外部滥用对整个 AI 行业的声誉造成了 " 连坐 " 效应。即使一家公司本身恪守伦理，也无法完全独善其身，因为公众对 AI 的信任是整体性的。恶意滥用行为如同向池塘中投下的毒药，在污染了整个水域后迫使所有 " 池中生物 " 共同承担后果。<br/>这场危机成为 AI 自我革新的契机<br/>这场 " 反噬 " 带来的阵痛，是 AI 产业从野蛮生长走向规范发展的必经阶段。它正在淘汰那些只想赚快钱、缺乏责任感的 " 玩家 "，筛选出真正有实力、有远见的长期主义者。从长远来看，这也是为 AI 产业的健康、可持续发展所必须付出的代价。<br/>其中最大的机遇在于将 " 信任 " 从一种道德呼吁，转变为一种可量化、可变现的商业资产和竞争壁垒。数据显示近 85% 的客户也更愿意与重视 AI 伦理实践的公司合作，而那些优先考虑伦理和透明度的公司收入增长也更快。<br/>可以说在 AI 产品同质化日益严重的未来，谁能赢得用户的信任谁就能赢得市场。" 负责任的 AI" 将不再仅仅是公关部门的口号，而是必须贯穿于产品设计、开发、部署全流程的核心战略。<br/>谷歌和微软等公司已经开始调整其策略，谷歌选择利用 AI 技术提升广告安全审核的效率，打击欺诈内容；微软则发布了负责任 AI 透明度报告，并推出了 AzureAIContentSafety 等服务，帮助客户构建更安全的 AI 应用。这些举措既是应对风险的防御，也是在构建新的竞争优势。<br/>正是 " 反噬 " 催生了全新的 " 安全即服务 " 市场。随着 AI 滥用风险的加剧企业对 AI 安全审计、风险评估、内容过滤、合规咨询等服务的需求将急剧增长。这为专门从事 AI 安全和伦理治理的科技公司、咨询机构创造了巨大的市场空间。<br/>而科技巨头自身也可以将其内部成熟的安全工具和能力平台化、服务化，开拓新的收入来源。例如，谷歌和微软在内容审核、风险识别方面的技术积累，完全可以转化为对外输出的商业服务。<br/>虽然监管的收紧虽然带来了成本，但也为行业设定了 " 准入标准 "，能够率先满足高标准合规要求的企业将获得更强的市场公信力和竞争优势，从而在未来的市场整合中占据有利地位。这实际上是一种由监管驱动的市场出清和格局优化。weibo.com/ttarticle/p/show?id=2309405234825153348099<br/>weibo.com/ttarticle/p/show?id=2309405234825304342582<br/>weibo.com/ttarticle/p/show?id=2309405234829599310183<br/>weibo.com/ttarticle/p/show?id=2309405234829741654183<br/>weibo.com/ttarticle/p/show?id=2309405234829880066874<br/>weibo.com/ttarticle/p/show?id=2309405234830022673245<br/>从滥用事件的激增，到资本市场的审慎，再到全球监管的收紧，这股 " 反噬 " 之力正在重塑 AI 产业的发展轨迹。它迫使整个行业从过去对技术力量的无限崇拜，转向对技术责任和社会价值的深刻反思。<br/>麦肯锡预测，到 2030 年 AI 将为全球经济创造 13 万亿美元价值。但价值分配取决于我们如何驾驭这头猛兽。未来的竞争，将不仅仅是模型参数和算力大小的竞争，更是治理能力、责任担当和用户信任的竞争。</p>]]></description></item><item>    <title><![CDATA[《从被动修复到免疫：游戏Bug闭环体系的]]></title>    <link>https://segmentfault.com/a/1190000047412846</link>    <guid>https://segmentfault.com/a/1190000047412846</guid>    <pubDate>2025-11-19 23:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>每一个Bug的出现，都绝非孤立的代码失误，可能是模块间数据流转的隐性断点、场景触发条件的边缘冲突，或是玩家非常规操作与设计预期的偏差，甚至可能是架构层面的适应性缺陷。这些异常表现如同系统的“隐性病灶”，轻则影响局部体验，重则引发连锁反应，导致核心玩法崩塌、玩家流失。多数开发团队对Bug的处理仍停留在“发现-修复-验证”的线性流程，将Bug视为需要消灭的“敌人”，却忽视了其背后承载的系统优化价值。真正成熟的Bug处理逻辑，并非以“零Bug”为终极目标—这在复杂游戏生态中几乎不具备可行性—而是构建一套让系统能够自我感知、自我调节、自我进化的“自愈体系”。这套体系的核心价值，在于通过对Bug全生命周期的深度拆解与闭环管理，将每一次异常处理转化为系统的“免疫记忆”，让同类问题的复发概率持续降低，同时推动架构、设计与测试环节的协同优化。在长期的开发实践中深刻体会到，那些能够在多版本迭代与海量玩家检验中保持体验稳定性的游戏，其背后必然存在一套超越表面流程的Bug自愈闭环。它不是一份僵化的操作手册，而是与游戏架构深度绑定、与开发节奏动态适配、与团队认知共同成长的思维模式，从Bug的源头预判到根因追溯，再到经验沉淀与体系迭代，每一个环节都在为系统韧性注入养分，最终实现体验品质的长期守恒。</p><p>构建Bug自愈体系的首要前提，是打破“被动等待Bug暴露”的传统模式，建立一套具备“预判性”与“协同性”的多维度感知网络。游戏开发中，Bug的发现渠道往往分散在内部测试、玩家反馈与线上监控三个维度，但多数团队未能让这三者形成有效协同，导致大量隐性Bug在上线后才集中爆发，增加了修复成本与体验损失。内部测试环节的核心痛点在于“场景覆盖的局限性”，传统测试用例多基于设计文档的预期流程，侧重验证功能的正常运行，却容易忽略不同模块交互时的边缘场景、极端数值组合、跨场景切换的时序冲突，或是玩家突破设计预期的非常规操作路径—比如在战斗中同时触发多个道具效果、在剧情触发节点强制退出游戏、在网络波动时进行关键操作等。解决这一问题的关键，并非无限制扩充测试用例数量，而是构建“模块交叉场景库”：以游戏核心玩法为轴心，梳理每个模块与其他系统的关键交互节点，比如战斗系统与道具系统的数值联动逻辑、剧情触发与场景切换的时序校验机制、网络同步与本地计算的一致性保障流程、角色状态与环境交互的边界条件等，将这些交叉点转化为可复现、可量化的测试场景，同时引入“反向测试思维”，主动模拟玩家可能的异常操作，提前暴露潜在风险。玩家反馈则常常呈现“碎片化”与“模糊化”特征，玩家往往只能描述异常现象（如“奖励未到账”“角色卡住”），却无法提供精准的触发条件、操作路径与设备信息。此时需要建立一套“反馈信息提炼机制”，通过对反馈内容中的关键词聚类、场景描述还原、设备型号与系统版本统计，从大量零散信息中识别出共性问题，区分“个体设备兼容问题”“网络环境导致的偶发异常”与“系统性Bug”。例如，当多名玩家反馈“某副本结算时奖励缺失”，通过提取他们的操作路径（是否中途退出、是否组队参与、是否触发特殊剧情分支）、设备类型（移动端/PC端）、网络状态（Wi-Fi/流量）等信息，可快速锁定结算逻辑中与“状态判定”“数据同步”相关的漏洞。而线上监控的核心，不应局限于报错日志的统计与告警，更要关注“异常行为序列”的捕捉与分析—比如玩家在某一功能模块的操作频率突然异常（远超正常玩家的点击次数）、某一场景的加载时长出现离散型峰值（多数玩家加载正常，少数玩家加载超时）、特定操作后玩家的退出率显著上升（如使用某道具后立即退出游戏）等，这些隐性信号往往是未被发现的Bug的前兆。通过将内部测试的“模块交叉场景库”、玩家反馈的“信息提炼机制”与线上监控的“异常行为捕捉”三者深度联动，让感知网络具备“主动识别”与“精准定位”能力，在Bug影响范围扩大前就完成初步判定，为后续的快速修复争取时间，同时减少无效排查带来的开发资源消耗。</p><p>Bug发现后的分级与优先级判定，是决定自愈体系效率的核心环节，其本质是对“影响权重”的精准权衡与动态调整。多数团队采用“严重程度+影响范围”的二元分级法，将Bug简单划分为致命、严重、一般、轻微四个等级，但这种方式容易陷入“高优先级Bug拥堵”“重要Bug被遗漏”或“资源分配失衡”的困境—比如将所有影响核心玩法的Bug都标记为高优先级，导致开发人员陷入多线作战，反而降低了整体修复效率；或是忽视了某些看似轻微但高频出现的体验类Bug，长期积累后影响玩家口碑。真正合理的分级逻辑，需要构建一个多维度的“影响权重模型”，除了直观的影响范围（覆盖玩家数量）与严重程度（是否阻碍核心流程），还需纳入“潜在扩散风险”“修复成本”“版本节奏适配性”“玩家感知敏感度”四个关键指标。潜在扩散风险指Bug是否可能随着玩家行为的传播、版本迭代中的模块联动，从局部场景蔓延到更多功能模块，比如某类数值计算错误若未及时修复，可能会在后续的道具更新、活动上线、跨服玩法开启后引发连锁反应，导致数值平衡崩坏；修复成本则需综合评估所需的开发工时、跨模块协作成本（是否需要多个团队配合）、代码改动范围（局部调整还是架构层面的修改），以及修复后可能引入新问题的概率，避免为了修复一个低影响Bug而占用核心功能开发、版本上线筹备等关键任务的资源；版本节奏适配性则要求分级与当前开发阶段的核心目标匹配，比如临近上线时，对影响核心玩法运行、付费流程、账号安全的Bug需优先处理，而在迭代中期，可适当将资源倾斜给那些虽不紧急但影响长期体验的隐性Bug（如极端场景下的轻微卡顿、界面显示瑕疵）；玩家感知敏感度则需结合游戏的目标用户群体特征，比如面向核心玩家的竞技类游戏，对操作响应延迟、数值平衡性相关的Bug敏感度极高，而面向休闲玩家的养成类游戏，可能更关注剧情连贯性、道具获取体验相关的问题。在实践中，我们将Bug划分为“阻断级”“核心体验级”“一般体验级”“隐性优化级”四类：阻断级指直接导致游戏无法运行、玩家进度丢失、核心玩法失效或账号安全风险的Bug，需启动紧急响应流程，暂停非核心开发任务，集中核心开发人员进行修复，必要时可采取临时屏蔽功能、回滚版本等应急措施；核心体验级指不影响游戏基本运行，但会严重破坏玩家沉浸感、影响核心玩法体验的Bug，如战斗系统的技能释放异常、剧情触发断裂、关键道具无法使用等，需在当前版本周期内优先处理，确保不影响版本核心目标的达成；一般体验级指对核心玩法无影响，但存在显示异常、音效缺失、操作逻辑不流畅等问题的Bug，可根据资源情况安排修复，若当前版本资源紧张，可纳入下一个迭代周期；隐性优化级则指在特定极端条件下才会触发、影响范围极小且不影响核心体验的Bug，如特定设备下的界面布局轻微偏移、极端数值组合下的非关键数据显示错误等，可纳入长期优化队列，结合后续版本的模块优化一并处理。分级的核心不是给出固定标签，而是建立“动态调整机制”—某一隐性优化级Bug若在后续迭代中因模块变动、玩法扩展而扩大影响范围，需及时提升优先级；而部分一般体验级Bug若玩家反馈集中、舆情关注度高，即使修复成本较高，也需重新评估资源分配，避免因忽视玩家感受导致留存下滑。通过这套多维度的分级模型与动态调整机制，让团队能够在复杂的开发节奏中，精准分配修复资源，既保证核心体验的稳定性，又避免资源浪费。</p><p>Bug修复环节的关键，在于避免“头痛医头、脚痛医脚”的表面修复，建立“全链路管控”机制，确保修复的有效性、安全性与彻底性。很多开发团队在修复Bug时，往往只关注报错的直接原因，比如看到“数据为空”的报错就直接添加空值判断，看到“界面显示异常”就调整布局参数，却忽略了Bug产生的上下文逻辑、数据流转链路与潜在关联影响，导致修复后不久同类问题再次出现，或引入新的兼容性漏洞、逻辑冲突。修复前的“根因定位”需要突破“代码层面”的局限，深入到“架构逻辑”“设计初衷”与“模块交互”的层面，还原Bug的完整生命周期。例如，某游戏曾出现“跨场景传送后角色技能CD异常重置”的Bug，初期开发人员仅针对传送逻辑中的CD数值赋值进行修正，但问题反复出现，甚至在后续版本中衍生出“技能效果无法正常触发”的新问题。直到团队重新梳理了技能系统、场景系统、网络同步三者的交互流程，才发现根源在于传送时的状态同步时序错误—角色技能CD的本地计算与服务器同步存在时间差，传送触发的状态重置指令覆盖了正确的CD数值，而初期的修复仅解决了表面的数值赋值问题，并未修复时序同步的核心矛盾。这一案例说明，有效的根因定位需要“层层拆解、溯本求源”：首先还原Bug的触发条件（如操作路径、场景环境、数值组合），然后梳理相关模块的交互链路（如数据从客户端到服务器的流转过程、不同系统的调用顺序），再分析每个环节的逻辑是否存在漏洞（如状态判定条件是否完善、数据同步是否一致、边界值处理是否全面），最终找到导致系统失效的核心断点。修复过程中，需坚守“最小改动原则”，即仅针对根因涉及的逻辑进行必要调整，避免为了简化修复而修改无关代码，或进行大面积的架构重构—除非根因明确指向架构层面的缺陷。同时，需建立“修复上下文档案”，详细记录修复思路、涉及的模块与代码范围、可能影响的功能点、测试验证的重点方向，便于后续的追溯与复盘。修复后的验证环节，不能依赖单一测试人员的确认，而要构建“多层验证体系”：开发自验需复现原始Bug场景及相关交叉场景（如与修复逻辑相关的模块交互场景、极端数值场景），确保修复有效且未影响其他功能；测试专项验证需结合“模块交叉场景库”，覆盖与修复逻辑相关的所有交互节点，同时进行兼容性测试（不同设备、系统版本）与压力测试（高并发场景）；线上灰度验证则针对影响范围较大的Bug（如核心玩法相关、覆盖大量玩家的问题），选择小部分服务器或特定玩家群体（如内测玩家、付费玩家）进行测试，观察修复后的系统稳定性、性能表现与玩家反馈，避免全量上线后引发新的问题。通过这套“根因定位-最小修复-多层验证”的全链路管控机制，确保每一次Bug修复都能彻底解决问题，同时规避修复带来的次生风险。</p><p>Bug修复完成并非自愈体系的终点，真正的价值沉淀来自于“根因追溯与经验复用”，让每一次Bug处理都成为系统优化的养分。多数团队在Bug验证通过后便结束流程，将其从任务列表中移除，却错失了通过单个Bug优化整个系统、提升团队能力的机会。每一个Bug的产生，都暴露了系统在设计、开发、测试或协作环节的薄弱点—可能是设计文档中的逻辑模糊地带、模块交互的边界定义不清晰，可能是编码规范的缺失、跨模块协作时的沟通偏差，也可能是测试用例的覆盖不足、自动化测试的场景遗漏。根因追溯的核心就是将这些薄弱点从“个案问题”转化为“通用规则”，避免同类问题重复出现。根因追溯需避开“归因于偶然”“归因于个人失误”的误区，从多个维度进行深度拆解：若Bug源于设计逻辑的漏洞，需反思设计文档是否存在歧义、模块交互的流程是否经过充分评审、是否考虑了极端场景与异常分支；若源于开发实现的偏差，需审视编码规范是否完善、代码审查是否到位、跨模块协作时的接口定义是否清晰、是否存在技术认知上的盲区；若源于测试覆盖的缺失，需分析测试用例的设计是否全面、测试方法是否合理、是否缺乏针对边缘场景的专项测试；若源于协作流程的问题，需思考沟通机制是否高效、信息同步是否及时、责任划分是否明确。在实践中，我们建立了“Bug复盘双周会”制度，选取典型Bug案例（如高频出现的同类问题、修复成本高的复杂问题、影响范围广的核心问题）进行集体拆解，而非局限于修复者个人的总结。复盘会并非简单的“问题汇报”，而是让设计、开发、测试、运营等相关人员共同参与，从各自的专业视角分析问题产生的原因：设计人员反思逻辑漏洞，开发人员分享编码过程中的困惑，测试人员总结覆盖不足的教训，运营人员反馈玩家的实际感受。例如，针对某类反复出现的“数值同步异常”Bug，复盘后发现，核心问题在于不同模块对同一数值的修改权限未做明确界定，导致多线程操作时出现数据冲突，同时测试用例中缺乏对多线程场景的覆盖。随后团队优化了数值管理架构，明确了核心数值的修改流程、同步机制与权限划分，同时补充了多线程场景的测试用例，引入自动化测试工具覆盖相关交互，后续同类Bug的出现频率下降了70%以上。复盘的关键不仅在于记录根因，更在于将复盘结论转化为可落地的优化措施：针对设计层面的问题，更新设计规范文档，增加模块交互评审环节，要求核心功能必须出具详细的异常分支处理方案；针对开发层面的问题，完善编码检查清单，强化代码审查中的重点关注项（如接口兼容性、数据校验、多线程安全），组织技术分享会弥补团队的认知盲区；针对测试层面的问题，扩充“模块交叉场景库”，引入自动化测试覆盖高频交叉场景与边缘场景，建立测试用例评审机制；针对协作层面的问题，优化信息同步工具，明确Bug处理的责任划分与沟通流程。通过这种方式，每一次Bug复盘都在为系统构建“防御工事”，为团队积累“避坑经验”，让自愈体系具备持续优化的能力，实现“处理一个Bug，解决一类问题”的目标。</p>]]></description></item><item>    <title><![CDATA[《游戏Bug快修手册：根因锁定与最小改动]]></title>    <link>https://segmentfault.com/a/1190000047412849</link>    <guid>https://segmentfault.com/a/1190000047412849</guid>    <pubDate>2025-11-19 23:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数研发团队面对突发Bug时，往往陷入“海量日志狂刷+无目标调试+仓促修改”的低效循环：有人埋头排查代码细节却忽视场景关联性，有人急于提交修复版本却未验证边缘情况，最终不仅浪费了黄金修复时间，还可能因盲目改动引入新的逻辑冲突，导致问题扩大化。快速解决Bug的核心，从来不是单纯追求“修复速度”，而是建立一套“精准定位→优先级动态判定→最小风险修复→分层高效验证→经验沉淀复用”的体系化能力。这种能力的本质，是对游戏系统逻辑的深度认知、排查思路的结构化拆解，以及风险控制的前置意识。Bug的解决效率，往往取决于能否在复杂的系统链路中快速锁定核心矛盾，而非被无关细节裹挟。长期的研发实践中深刻体会到，那些能在紧急场景下从容破局的团队，都具备一套独特的排查思维：他们不会急于动手修改，而是先通过场景拆解、逻辑溯源缩小问题范围，再以“最小改动”原则精准打击根因，最后用分层验证确保修复安全。这种思路彻底打破了“越快修复越容易出错”的固有误区，让快速修复与风险控制形成良性平衡，真正实现“高效且稳妥”的Bug解决闭环，既守住了用户体验的底线，也最大化降低了研发资源的浪费。</p><p>精准定位是快速解决Bug的前提与核心，其关键在于构建“场景拆解+逻辑溯源”的结构化排查路径，用科学方法替代盲目摸索，从源头缩短排查时间。很多研发同行容易陷入的典型误区是：拿到Bug反馈后立即投入日志排查，面对动辄数万条的日志数据无从下手，反复筛选却始终找不到关键信息，最终在无效操作中浪费大量时间。高效定位的第一步，是提炼“场景复现四要素”，通过用户反馈、后台数据、测试复现等多渠道收集完整信息：完整的操作路径（用户从进入场景到触发Bug的每一步操作，包括点击顺序、技能释放、道具使用等）、具体的环境条件（设备型号、系统版本、网络状态、服务器分区、游戏版本号等）、明确的数据状态（用户等级、角色属性、组队人数、副本进度、道具持有情况等）、清晰的触发频率（是100%触发、高频触发还是偶发触发，是否与特定时间、特定场景强关联）。例如，某动作游戏中“连续释放技能A后使用道具B，角色会出现1-2秒卡顿”的Bug，通过用户反馈与后台数据收集到四要素：操作路径（技能A×3+道具B点击）、环境条件（集中在安卓13及以上版本、Wi-Fi网络）、数据状态（角色等级30级以上、组队场景、副本为高难度模式）、触发频率（组队场景中触发率90%，单机场景零触发）。基于这些信息，首先通过反向推导排除无关因素：若为本地资源加载问题，单机场景应同样触发，因此排除；若为设备兼容性问题，不应仅局限于组队场景，因此缩小范围至“组队状态下的网络与数据同步”。接下来，沿着“组队状态激活→技能释放指令发送→服务器接收与处理→数据同步至组队成员客户端→客户端渲染反馈”的核心链路拆解，重点排查服务器与客户端的同步机制：服务器处理组队指令的并发能力、同步频率是否与客户端指令发送频率匹配、数据传输过程中是否存在丢包或延迟。最终发现，高难度副本中组队成员的技能指令发送频率较高，而服务器的同步频率设置过低，导致数据堆积在传输链路中，客户端接收数据不完整引发卡顿。定位阶段的另一关键是“排查优先级排序”：先排查高频触发场景（优先复现100%或高频触发的Bug，避免在偶发问题上浪费时间），再处理偶发场景；先验证核心逻辑链路（如数据流转、指令传输等核心流程），再排查边缘分支（如特殊数值、极端条件下的逻辑）；先排除简单易验证的原因（如配置错误、参数异常等可快速确认的问题），再深入复杂模块（如架构层面的时序冲突、多线程交互等）。这种结构化排查思路，能有效避免“大海捞针”式的无效操作，将定位时间缩短60%以上，为后续的修复工作争取宝贵的黄金时间。</p><p>Bug定位完成后，优先级的动态判定直接决定修复效率与资源分配的合理性，其核心是建立“三维评估模型”，在多任务并行的压力下精准锁定核心矛盾，避免资源浪费或核心问题延误。很多研发团队在优先级判定上存在明显误区：要么采用“一刀切”的简单分类（如仅按严重程度分为致命、严重、一般），导致所有“严重”Bug堆积，研发人员陷入多线作战，反而降低整体修复效率；要么过度依赖主观判断，忽视用户感知与修复成本的平衡，导致高敏感度Bug被遗漏，引发用户不满。三维评估模型的核心指标包括“影响范围”“用户敏感度”“修复成本”，三者需综合权衡而非孤立判定，每个指标都需结合游戏类型、研发阶段、运营场景进行细化拆解。影响范围不仅指覆盖的用户数量，还包括影响的功能重要性：如全服用户均可触发的Bug，或核心付费功能、核心玩法相关的Bug，影响范围权重更高；而仅影响少数特定设备、特定场景（如冷门单机副本）的Bug，影响范围权重较低。用户敏感度与游戏的核心定位强相关：竞技类游戏对操作响应延迟、数值平衡、对战公平性的敏感度极高，哪怕是轻微的数值计算错误，也可能引发核心用户的强烈不满；休闲养成类游戏则更关注剧情连贯性、道具获取体验、界面交互流畅度，对单机场景的轻微卡顿或显示异常敏感度较低。修复成本需从多维度综合评估：研发工时（解决问题所需的时间）、跨模块协作需求（是否需要多个团队或模块负责人配合）、代码改动范围（是局部参数调整、单一逻辑修改，还是涉及架构层面的重构）、风险系数（修复后是否可能引入新的逻辑冲突、兼容性问题）。例如，某跨服对战游戏中“结算时玩家积分计算错误”的Bug，影响范围是全服参与跨服玩法的用户（核心用户群体），用户敏感度（竞技公平性）拉满，而修复仅需调整服务器端的积分计算逻辑，改动范围小、工时短、风险低，应直接定为最高优先级，启动紧急修复流程，甚至暂停非核心功能的研发进度集中资源解决；而某单机解谜游戏中“某冷门关卡的背景音效缺失”，仅影响少数通关该关卡的用户，用户敏感度低，修复需调整多个音频预制件，还可能影响其他关卡的音效播放，修复成本高、风险高，可纳入后续迭代周期，待资源充裕时处理。优先级判定并非一成不变的静态标签，需建立动态调整机制：若某低优先级Bug的用户反馈量在短时间内激增（如1小时内反馈量突破千条），或被行业媒体、核心KOL提及引发舆情风险，需实时上调优先级，启动紧急处理；若高优先级Bug在修复过程中发现根因涉及架构层面（如数据存储方式不合理），修复成本远超预期（需耗时3天以上），则需评估是否采取临时规避方案（如屏蔽相关功能入口、限制触发条件），先缓解用户影响，再在后续版本中彻底重构，而非硬磕导致更大损失。精准的优先级判定，能让团队在复杂的研发节奏中始终聚焦核心矛盾，确保有限的研发资源投入到“影响大、用户敏感、成本低”的关键问题上，最大化提升修复效率与用户满意度。</p><p>高效修复的核心是“根因聚焦+最小改动”，既要保证问题彻底解决，又要将修复风险降至最低，避免因仓促改动或过度优化引发新的问题。很多研发人员在紧急场景下容易陷入两个极端：要么追求“一劳永逸”，试图通过全面重构解决问题，结果导致修复时间翻倍，还可能破坏原有稳定逻辑；要么仅针对表面现象进行“补丁式修复”，忽视根因，导致Bug反复出现。最小改动的本质是“精准打击”—仅针对Bug的根因涉及的逻辑进行必要调整，不触碰无关模块，不优化非相关问题，不引入新的功能或逻辑。例如，某RPG游戏中“组队副本结算时，部分玩家未收到奖励”的Bug，通过定位发现根因是“服务器端未正确读取组队成员的通关时长数据，导致奖励计算条件不满足”，此时只需修正服务器端读取通关时长的逻辑（如补充缺失的字段读取、修复条件判断错误），确保数据获取准确，即可彻底解决问题；若此时同时优化奖励发放的动画效果、调整其他道具的数值平衡、修复副本内的无关文本错误，不仅会大幅增加修复时间，还可能因改动过多引发新的兼容性问题（如动画效果与部分设备不兼容）或逻辑冲突（如数值调整导致其他奖励计算错误）。修复前的风险前置控制同样关键，是避免修复失败的重要保障：首先需备份核心代码分支，建立修复专用分支，确保修复过程中不影响主分支的稳定，若修复出现意外可快速回滚至稳定版本；其次需预留临时规避开关，在代码中加入功能屏蔽或触发限制开关，若修复后验证出现问题，可通过后台配置快速屏蔽相关功能，避免影响线上用户体验；最后需明确改动范围，与相关模块负责人同步修复方案，确认改动不会影响其他模块的逻辑（如是否涉及公共接口、共享数据结构），必要时进行交叉评审。修复过程中，需坚守“根因不明确不盲目动手”的原则—很多紧急场景下，研发人员为了赶时间，在根因尚未完全确认时就仓促修改，结果导致Bug反复出现或衍生新问题。例如，某手游中“网络波动时使用道具，道具消耗但效果未生效”的Bug，初期误判为本地数据存储问题，修改了客户端的道具消耗记录逻辑，结果问题仍持续出现，还导致部分用户的道具数据异常；直到重新定位发现，根因是服务器同步确认机制缺失—用户提交道具使用请求后，客户端未收到服务器的成功确认指令就默认消耗道具，而服务器因网络波动未处理该请求，导致数据不一致。这种情况下，盲目修复不仅浪费了宝贵的时间，还引发了新的用户投诉。因此，即使在最紧急的场景下，也需预留10-15分钟彻底确认根因：通过复现场景、梳理逻辑链路、验证假设，确保每一个修改都有明确的针对性，避免“试错式修复”。</p><p>修复完成后的高效验证，是避免Bug二次爆发、保障线上稳定的最后一道防线，核心在于构建“分层验证+场景覆盖”的闭环体系，在有限时间内兼顾验证速度与全面性，杜绝“修复一个Bug，引入另一个Bug”的情况。很多研发团队在紧急修复后，仅进行简单的功能验证（如复现原始场景确认Bug消失）就匆忙上线，结果因验证不充分导致Bug复现、边缘场景异常或衍生新问题，反而增加了后续处理的成本。分层验证体系分为三个核心环节：开发自验、测试专项验证、灰度验证，每个环节都有明确的验证重点与操作标准。开发自验需覆盖三个核心维度：原始Bug的触发场景（多次复现确保问题彻底解决，避免偶发修复）、相关联的边缘场景（如组队/单机、不同等级、不同道具组合、网络波动环境等，验证修复不影响其他功能）、反向测试（模拟异常输入、极端数值、错误操作，验证系统的容错能力）。例如，修复副本结算异常后，不仅要验证正常通关的结算结果，还要测试中途退出、组队解散、网络中断后重连、极端通关时长（如超短时间通关、超时通关）等边缘场景，确保结算逻辑在各种情况下都稳定；同时模拟服务器负载过高、数据传输延迟等异常情况，验证系统是否能正确处理错误数据，避免崩溃。测试专项验证需聚焦高风险点，结合游戏类型与Bug特征制定针对性验证方案：兼容性验证需覆盖主流设备（移动端需包含高中低端机型、不同品牌）、系统版本（安卓各版本、iOS各版本）、网络环境（Wi-Fi、4G、5G、弱网）；高并发验证需通过压力测试工具模拟峰值在线人数，测试服务器的承载能力与数据同步稳定性；数据一致性验证需重点核对客户端与服务器的核心数据（如角色属性、道具数量、积分排名），确保同步无误。考虑到紧急场景下的时间压力，测试专项验证可采用“核心场景优先”策略：优先覆盖用户高频使用的场景（如核心玩法、付费流程），再逐步扩展到边缘场景，避免因追求全面性导致上线延误。灰度验证是线上安全的关键保障，其核心是“小范围测试、快速反馈、动态调整”：选择10%-20%的核心用户群体（如付费用户、高活跃用户）或特定服务器（如新区、测试服）进行小范围上线，实时监控关键指标—Bug报错率、用户反馈量、功能使用率、服务器负载、数据同步成功率等；若15-30分钟内无异常反馈，且核心指标稳定，再逐步扩大灰度范围，最终全量上线；若出现异常，立即启动回滚机制，将影响范围控制在最小。验证过程中，需建立快速反馈通道：通过用户社群、客服系统、后台反馈入口收集用户的实时反馈，安排专人监控反馈信息，对用户提及的新异常快速响应，确保未覆盖的场景问题能及时发现。高效验证的核心不是“全面测试”，而是“精准覆盖风险点”—基于Bug的根因、改动范围、影响场景，聚焦最可能出现问题的环节，用最少的时间实现最大程度的风险排查，确保修复版本的稳定性。</p><p>快速解决Bug的长期效率提升，离不开系统性的经验沉淀与思路复用，核心是将单次修复的实践智慧转化为可复制、可传承的通用规则，推动团队整体处理能力的持续迭代，而非停留在“单次高效”的层面。很多研发团队在Bug修复后便结束流程，没有进行总结沉淀，导致同类问题反复出现，团队陷入“排查-修复-再排查”的恶性循环，长期来看反而浪费了大量研发资源。</p>]]></description></item><item>    <title><![CDATA[从 Flink 到 Doris 的实时数]]></title>    <link>https://segmentfault.com/a/1190000047412851</link>    <guid>https://segmentfault.com/a/1190000047412851</guid>    <pubDate>2025-11-19 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Flink-Doris-Connector 作为 Apache Flink 与 Doris 之间的桥梁，打通了实时数据同步、维表关联与高效写入的关键链路。本文将深入解析 Flink-Doris-Connector 三大典型场景中的设计与实现，并结合 Flink CDC 详细介绍了整库同步的解决方案，助力构建更加高效、稳定的实时数据处理体系。</p><h2>一、Apache Doris 简介</h2><p>Apache Doris 是一款基于 MPP 架构的高性能、实时的分析型数据库，整体架构精简，只有 FE 、BE 两个系统模块。其中 FE 主要负责接入请求、查询解析、元数据管理和任务调度，BE 主要负责查询执行和数据存储。Apache Doris 支持标准 SQL 并且完全兼容 MySQL 协议，可以通过各类支持 MySQL 协议的客户端工具和 BI 软件访问存储在 Apache  Doris 中的数据库。</p><p>在典型的数据集成和处理链路中，往往会对 TP 数据库、用户行为日志、时序性数据以及本地文件等数据源进行采集，经由数据集成工具或者 ETL 工具处理后写入至实时数仓 Apache Doris 中，并由 Doris 对下游数据应用提供查询和分析，例如典型的 BI 报表分析、OLAP 多维分析、Ad-hoc 即席查询以及日志检索分析等多种数据应用场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412853" alt="Apache Doris 简介 .PNG" title="Apache Doris 简介 .PNG"/></p><p>Flink-Doris-Connector 是 Apache Doris 与 Apache Flink 在实时数据处理 ETL 的结合，依托 Flink 提供的实时计算能力，构建高效的数据处理和分析链路。Flink-Doris-Connector 的使用场景主要分为三种：</p><ol><li><strong>Scan</strong>：通常用来做数据同步或是跟其他数据源的联合分析；</li><li><strong>Lookup Join</strong>：将实时流中的数据和 Doris 中的维度表进行 Join；</li><li><strong>Real-time ETL</strong>：使用 Flink 清洗数据再实时写入 Doris 中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412854" alt="Apache Doris 简介 -2.PNG" title="Apache Doris 简介 -2.PNG" loading="lazy"/></p><h2>二、Flink-Doris-Connector 典型场景的设计与实现</h2><p>本章节结合 Scan、Lookup Join、Write 这三种场景，介绍 Flink-Doris-Connector 的设计与实现。</p><h3>01 Scan 场景</h3><p>Scan 场景指将 Doris 中的存量数据快速提取出来，当从 Doris 中读取大量数据时，使用传统的 JDBC 方法可能会面临性能瓶颈。因此 Flink-Doris-Connector 中可以借助 Doris Source ，充分利用 Doris 的分布式架构和 Flink 的并行处理能力，从而实现了更高效的数据同步。</p><h4>Doris Source 读取流程</h4><ul><li>Job Manager 向 FE 端发起请求查询计划，FE 会返回要查询的数据对应的 BE 以及 Tablet；</li><li>根据不同的 BE，将请求分发给不同的 TaskManager；</li><li>通过 Task Manager 直接读取每个 BE 上对应 Tablet 的数据。</li></ul><p>通过这种方式，我们可以利用 Flink 分布式处理的能力从而提高整个数据同步的效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412855" alt="Doris Source 读取流程.PNG" title="Doris Source 读取流程.PNG" loading="lazy"/></p><h3>02 Lookup Join 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412856" alt="Lookup Join 场景.PNG" title="Lookup Join 场景.PNG" loading="lazy"/></p><p>对于维度表存储在 Doris 中的场景，可通过 Lookup Join 实现对实时流数据与 Doris 维度表的关联查询。</p><h4>JDBC Connector</h4><p>Doris 支持 MySQL 协议，所以可以直接使用 JDBC Connector 进行 Lookup Join，但是这一方式存在一定的局限：</p><ul><li>Jdbc Connector 中的 Lookup Join 是同步查询的操作，会导致实时流中每条数据都要等待 Doris 查询的结果，增加了延迟。</li><li>仅支持单条数据查询，在上游数据量吞吐较高时，容易造成性能瓶颈和反压。</li></ul><h4>Flink-Doris-Connector 的优化</h4><p>因此针对 Lookup Join 场景 ，Flink-Doris-Connector 实现了异步 Lookup Join 和攒批查询的优化：</p><ul><li><strong>支持异步 Lookup Join：</strong> 异步 Lookup Join 意味着实时流中的数据不需要显式等待每条记录的查询结果，可以大大的降低延迟性。</li><li><strong>支持攒批查询：</strong> 将实时流的数据追加到队列 Queue 中，后台通过监听线程 Watcher，将队列里面的数据取出来再推送到查询执行的 Worker 线程池中，Worker 线程会将收到的这一批数据拼接成一个 Union All 的查询，同时向 Doris 发起 Query 查询。</li></ul><p>通过异步 Lookup join 以及攒批查询，可以在上游数据量比较大的时候大幅度提高维表关联吞吐量，保障了数据读取与处理的高效性。</p><h3>03 实时 ETL 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412857" alt="实时 ETL 场景.png" title="实时 ETL 场景.png" loading="lazy"/></p><p>对于实时写入来说，Doris Sink 的写入是基于 Stream Load 的导入方式去实现的。Stream Load 是 Apache Doris 中最为常见的数据导入方式之一，支持通过 HTTP 协议将本地文件或数据流导入到 Doris 中。主要流程如下：</p><ul><li>Sink 端在接收到数据后会开启一个 Stream Load 的长链接请求。在 Checkpoint 期间，它会将接收到的数据以 Chunk 的形式持续发送到 Doris 中。</li><li>Checkpoint 时，会对刚才发起的 Stream Load 的请求进行提交，提交完成后，数据才会可见。</li></ul><h4>如何保证数据写入的 Exactly-Once 语义</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412858" alt="如何保证数据写入的 Exactly-Once 语义 .png" title="如何保证数据写入的 Exactly-Once 语义 .png" loading="lazy"/></p><p>那么，如何保证数据写入期间，端到端数据的精确一次性？</p><p>以 Kafka 同步到 Drois 的 Checkpoint 过程为例：</p><ol><li>Checkpoint 时，Source 端会接收到 Checkpoint Barrier；</li><li>Source 端接收到 Barrier 后，首先会对自身做一个快照，同时会将 Checkpoint Barrier 下发到 Sink 端；</li><li>Sink 端接收到 Barrier 后，执行 Pre-commit 提交，成功后数据就会完整写入到 Doris，由于此处执行的是预提交，所以在 Doris 上，此时对用户来说数据是不可见的；</li><li>将 Pre-Commit 成功的事务 ID 保存到状态中；</li><li>所有的算子 Checkpoint 都做完后，Job Manager 会下发本次 Checkpoint 完成的通知；</li><li>Sink 端会对刚才 Pre-commit 成功的事务进行一次提交。</li></ol><p>通过这种两阶段提交，就可以实现端到端的精确一次性。</p><h4>实时性与 Exactly-Once</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412859" alt="实时性与 Exactly-Once.png" title="实时性与 Exactly-Once.png" loading="lazy"/></p><p>上面提到，Doris Sink 端的写入与 Checkpoint 绑定，数据写入 Doris 的延迟性取决于 Checkpoint 的间隔。但在一些用户的场景下，希望数据可以实时写入，但是 Checkpoint 不能做的太频繁，同时对于一些作业来说，如果 Checkpoint 太频繁会消耗大量资源，针对该情况，Flink-Doris-Connector 引入了攒批机制，以平衡实时性与资源消耗之间的矛盾。</p><p>攒批的实现原理是 Sink 端接收上游数据之后，不会立即将每条数据单独写入 Doris，而是先在内存中进行缓存，然后通过对应参数设置，将缓存数据提交到 Doris 中。结合攒批写入和 Doris 中的主键模型，可以确保数据写入的幂等性。</p><p>通过引入攒批机制，既满足了用户对数据实时写入的需求，又避免了频繁 Checkpoint 带来的资源消耗问题，从而实现性能与效率的优化。</p><h2>三、基于 Flink CDC 的整库同步方案</h2><p>以上是对 Flink-Doris-Connector 的典型场景和实现原理介绍，接下来我们来看它在实际业务中的一个重要应用——整库同步。相比底层实现，整库同步更偏向具体使用场景。下面我们基于前面介绍的能力，进一步探讨如何通过 Flink CDC 实现 TP 数据库到 Doris 的高效、自动化同步。</p><h3>01 整库同步痛点</h3><p>在数据迁移过程中，用户通常希望可以尽快将数据迁移到 Doris 中，然而在同步 TP 数据库时，整库同步往往面临以下几点挑战：</p><ul><li><p><strong>建表：</strong></p><ul><li><strong>存量表的快速批量创建</strong>：TP 数据库中往往存在成千上万的表，这些表的结构各异，对于存量表而言需要逐一在 Doris 中创建对应的表结构；</li><li><strong>同步任务开启后，新增表的自动创建与同步：</strong> 为了保证数据的完整性和实时性，同步工具需要实时监控 TP 数据库的变化，并自动在 Doris 中创建和同步新表。</li></ul></li><li><strong>元数据映射：</strong> 上下游之间字段元数据的便捷映射，包括字段类型的转换、字段名称的对应修改等。</li><li><strong>DDL 自动同步：</strong> 增加、删除列等操作会导致数据库结构发生变化，进而影响到数据同步。因此，同步工具需要能够实时捕获 DDL 并动态地更新 Doris 表结构，以确保数据的准确性和一致性。</li><li><strong>开箱即用：</strong> 零代码，低门槛，理想的同步工具只需进行简单配置，即可实现数据的迁移和同步。</li></ul><h3>02 基于 Flink CDC 实现整库同步</h3><p>在数据抽取方面，Flink-Doris-Connector 借用了 Flink CDC 的特性能力：</p><ul><li><p>增量快照读取</p><ul><li>无锁读取与并发读取：不论存量数据量多大，都可以通过横向提高 Flink 的并发提升数据读取速度。</li><li>断点续传：当存量数据比较大时，可能面临同步中断的情况，CDC 支持中断任务的衔接同步。</li></ul></li><li>丰富数据源支持，Flink CDC 支持多种数据库，如 MySQL、Oracle、SQLServer 等。</li><li>无缝对接 Flink 现有生态，方便与 Flink 已有Source 和 Sink 结合使用。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412860" alt="基于 Flink CDC 实现整库同步.png" title="基于 Flink CDC 实现整库同步.png" loading="lazy"/></p><h4>一键建表与元数据自动映射</h4><p>Flink-Doris-Connector 中集成了 Flink CDC 等能力，可以让用户只提交一个操作，就能进行整库同步的操作。其主要原理是 Flink CDC Source 在接收到上游的数据源之后，会进行分流处理，不同的表用不同的 Sink。同时在最新的 Connector 版本中，也支持单个 Sink 同步多张表，支持新增表的创建和同步。</p><p>集成 Flink CDC 的功能后，<strong>用户仅需通过 Flink-Doris-Connector 提交任务，就可以在 Doris 自动创建所需的表，而无需配置上下游表之间的显式关联，实现数据快速同步</strong>。</p><p>当 Flink 任务启动后，Doris-Flink-Connector 将自动识别对应的 Doris 表是否存在。如果表不存在，Doris Flink Connector 会自动创建表，并根据 Table 名称进行分流，从而实现下游多个表的 Sink 接入；如果表存在，则直接启动同步任务。</p><p>这一改进，不仅简化了配置流程，还使得新增表的创建和同步更加便捷，从而提升数据处理的整体效率。</p><h4>Light Schema Change 与 DDL 自动同步</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412861" alt="Light Schema Change 与 DDL 自动同步.png" title="Light Schema Change 与 DDL 自动同步.png" loading="lazy"/></p><p>在 Apache Doris 1.2 版本之前，Schema Change 操作比较繁琐，需要手动增改数据列。在上游 TP 数据库发生表结构变更时，需要暂停数据同步任务、待 Doris 中的 Schema Change 完成后再重启任务。</p><p>自 Apache Doris 1.2 版本起，我们引入了轻量级的 Light Schema Change 机制，极大地简化了操作流程，常见的增减列场景其处理速度可达毫秒级。Light Schema Change 机制原理如下：</p><ul><li><p><strong>Schema Change：</strong></p><ul><li>客户端向 FE 发起增减列的请求；</li><li>FE 在接收到请求后，修改当前元数据，并将最新的 Schema 持久化；</li><li>FE 向客户端同步 Schema Change 的结果；</li></ul></li><li><p><strong>Data Load：</strong></p><ul><li>当后续导入任务发起时，FE 将导入任务与最新的 Schema 信息发送给 BE；</li><li>在数据写入过程中，BE 的每个 Rowset 都会存储当前导入的 Schema 信息；</li></ul></li><li><p><strong>Query：</strong></p><ul><li>FE 将查询计划与最新的 Schema 一起发送给 BE；</li><li>BE 使用最新 Schema 执行查询计划；</li></ul></li><li><p><strong>Compaction：</strong></p><ul><li>在 BE 中，对参与合并的 Rowset 版本进行比较；</li><li>根据最新的  Schema Change 信息进行数据合并。</li></ul></li></ul><p>经测试，与早期的 Schema Change 相比，Light Schema Change 的数据同步性能有了数百倍的提升，</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412862" alt="Light Schema Change 与 DDL 自动同步-2.png" title="Light Schema Change 与 DDL 自动同步-2.png" loading="lazy"/></p><p>Light Schema Change 与 Flink-Doris-Connector 的结合，通过 Flink CDC 可以实现 DDL 的自动同步，具体步骤如下：</p><ol><li>Source 端捕获上游 Schema Change 信息，开启 DDL 变更同步；</li><li>Doris Sink 端识别并解析 DDL 操作（加减列）；</li><li>Table 校验，判断是否可以进行 Light Schema Change；</li><li>发起 Schema Change 操作；</li></ol><p>基于这一实现，Doris 能自动获取到 DDL 语句并在毫秒级即可完成 Schema Change 操作，在上游 TP 数据库发生表结构变更时，数据同步任务无需暂停。</p><h4>开箱即用：MySQL 整库同步示例</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412863" alt="开箱即用：MySQL 整库同步示例.png" title="开箱即用：MySQL 整库同步示例.png" loading="lazy"/></p><p>对于用户来讲，只要有 Flink 客户端，通过上图的操作就可以提交整库同步作业。支持传入 Flink 的配置，比如并发设置、Checkpoint 间隔等，也支持正则表达式去配置需要同步的表， 同时可以将 Flink CDC Source 和 Doris Sink 的配置直接透传给具体的 Connector。通过这种方式，用户可以很便捷地提交整库同步作业。</p><h3>03 Flink-Doris-Connector 核心优势</h3><p>基于以上优化，可以完美解决用户的痛点：</p><ol><li>自动建表，即存量表与增量表的自动创建，无需用户提前在 Doris 中预先创建对应的表结构；</li><li>自动映射上下游字段，无需手动写入上下游字段间的匹配规则，节省大量人力成本；</li><li>增减列无感同步，及时获取上游 DDL 语句并自动在 Doris 中实现毫秒级 Schema Change，无需停服、数据同步任务平稳运行；</li><li>开箱即用，降低学习成本，更专注业务本身。</li></ol><h3>04 最佳实践</h3><p>在生产环境中，若作业数量较多，直接采用上述提交方式的作业管理复杂度较高。通常建议借助任务托管平台（如 StreamPark），实现对作业的统一创建、监控与运维，从而提升任务管理效率与系统稳定性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412864" alt="最佳实践.png" title="最佳实践.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412865" alt="最佳实践-2.png" title="最佳实践-2.png" loading="lazy"/></p><h2>四、未来规划</h2><p>未来，基于 Flink-Doris-Connector 的能力规划如下：</p><ol><li>支持实时读取。目前 Doris Source 只是把数据 Scan 出来，是一个有界流的读取，后续会支持 CDC 的场景，可以使用 Flink 来对 Doris 中的数据进行流式的读取。</li><li>Sink 一流多表。目前Flink-Doris-Connector支持单个 Sink 同步多张表，但是 Stream Load 的导入方式还是只支持单个表的导入。所以在表特别多的时候，需要在 Sink 端维护大量 StreamLoad 的连接，在后续会做到单个 Stream Load 的连接支持多张表的写入。</li><li>整库同步方面，支持更多的上游数据源，满足更多数据同步场景。</li></ol>]]></description></item><item>    <title><![CDATA[编程项目怎么学习 cpp辅导的阿甘 ]]></title>    <link>https://segmentfault.com/a/1190000047412883</link>    <guid>https://segmentfault.com/a/1190000047412883</guid>    <pubDate>2025-11-19 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>星球很多同学，在做星球项目，或者做自己项目的时候，都会遇到各种坎坷，说看不懂，不理解。</p><p>那项目，一个从未接触过的项目应该怎么学习呢。</p><h2>观点分享</h2><p>说方法之前，我们可以先对要学习的项目进行一个分类，分一下学习的两个境界。我认为可以整体分两类：</p><p>（1）一类是，自己学习，用于提升自己，用于跳槽，找工作给简历加分的 （个人项目）</p><p>（2）一类是，工作公司的项目，自己实际工作中的</p><h3>个人项目</h3><p>对于个人项目，拿来面试。面试主要考察什么呢，你这个人设计能力的完善性，即你项目的某个功能，对于极端场景是否有考虑到。</p><p>那这对于一个项目，熟悉到什么程度算可以了呢。主要就是项目的架构，项目功能的实现思路。对代码细节，写法没必要细究。</p><p>原因：</p><p>（1）相同的功能实现，不同的人可能就会有不同的写法，以及相同的人不同时期也会有不同的写法；</p><p>（2）面试重点是思维逻辑的交流，让人家可以听懂，可以认可，能够产生共鸣；毕竟人家也没看过你的代码，语法、写法人家也不知道，你说的这么细，反而让人家听不懂，效果还很差； </p><p>（3）这也是一直强调的，在学项目的时候也要注重文档的梳理编写。能够让一个搞python的，搞java的可以看懂，快速写出来。别说一堆自己项目自己命名，这确实详细，但是谁也看不懂，听不懂，那效果很差</p><h3>公司的项目</h3><p>公司的项目，我们进公司，主要是要解决项目bug，优化项目代码的，开发新功能的。解决项目的代码bug，肯定要能够精确定位，要对代码细节，调用过程了解，需要熟悉项目代码。</p><h2>建议</h2><p>知道了对于不同场景下，项目的学习程度。那么再聊聊项目应该怎么学习。</p><p>相信很多同学，都再网上听过很多前辈分享的各种源码阅读方法。比如main函数开始追、分功能模块看、按住一个功能调用过程追等等。</p><p>在这里，主要想给大家强调的方法是什么呢？</p><p><strong>借助AI，优先借助AI。</strong></p><p>现在AI能力，确实足够强大了，比如gpt5、claude 4.5等等。并且像个人项目一般最多也就几万行，或者就算公司项目上亿行代码，但是到你部门负责的可能也就几万行，数十万行，代码量都不大。可以先让AI对你的项目代码分析分析，架构、功能，实现逻辑等等。先通过它帮助你了解百分之七八十，再自己慢慢解决剩下的百分之二十，效率会高很多，很给力。</p><p>可能有的同学，在知名公司工作，说公司内部模型，没有这最先进的，其实用你们公司目前内供的，我认为目前也是可以帮助你进行分析的。</p><p>（为什么会给大家强调这个呢，主要还是通过大家问我的一些技术问题项目问题发现，这些问题直接喂给AI基本就可以快速出方案进行解决，远远没必要在那里抓脑瞎。给大家写这个，就是让大家有用AI的意识，优先考虑，现在模型能力是够的了）</p><h2>知识星球介绍（公认的cpp c++学习地）</h2><p>星球名字：奔跑中的cpp / c++</p><p>里面服务也不会变，四个坚守目前:</p><p>1.每天都会看大家打卡内容，给出合理性建议。</p><p>2.大家如果需要简历指导，心里迷茫需要疏导都可以进行预约周六一对一辅导。</p><p>3.每周五晚上九点答疑聊天不会变。</p><p>4.进去星球了，后续如果有什么其他活动，服务，不收费不收费(可以合理赚钱就收取下星球费用，但是不割韭菜，保持初心)</p><p>（还有经历时间考验的独家私密资料）</p><p>加入星球的同学都可以提问预约，一对一帮做简历，一对一  职业规划辅导    ，解惑。同时有高质量的项目以及学习资料</p><p>学cpp基础，可以把最近开发的这个编程练习平台利用起来<br/>cppagancoding.top</p><p>本文由<a href="https://link.segmentfault.com/?enc=5uTJ45M5FtItUCs18qiyVQ%3D%3D.KYsCbBaqEPDeQykZBSbhxyjUFONb6lGj8%2F33PniNwS0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[LEANN：一个极简的本地向量数据库 本]]></title>    <link>https://segmentfault.com/a/1190000047412778</link>    <guid>https://segmentfault.com/a/1190000047412778</guid>    <pubDate>2025-11-19 22:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在软件开发领域，提到轻量级、嵌入式的本地数据库，我们首先会想到 <strong>SQLite</strong>，它快速且无需独立服务进程。现在检索增强生成（RAG）和向量数据库的世界里，一个定位相似的新工具出现了。你可以把LEANN看作是<strong>嵌入式、轻量级的向量数据库</strong>。它完全不需要依赖庞大的数据中心或者 GPU 集群。一个<strong>个人专属的 RAG 引擎</strong>，它能完全放在你的笔记本电脑里，可以索引和搜索数百万份文档，而且最不可思议的是，它比常规向量数据库<strong>少用了 97% 的存储空间</strong>，而且**准确性还没有任何损失。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412780" alt="" title=""/></p><h2>LEANN 的核心技术差异</h2><p>传统的向量数据库简直就是存储“巨兽”。它们为每一个文档都预先计算好嵌入（Embeddings）并全部存储下来，磁盘空间很快就会被塞满。LEANN则不一样：</p><p>它压根儿不存储所有嵌入，而是采用一种<strong>基于图的选择性重计算</strong>方法，并辅以<strong>高保留度的修剪（Pruning）</strong>。这几个术语听起来有点花哨，但核心思想很简单：<strong>只在真正需要时，才去计算必需的数据</strong>。</p><p>LEANN 不是一个嵌入数据的“囤积者”，它会按需重新计算嵌入，并通过一个非常<strong>轻量级的图结构</strong>将它们智能地连接起来。这让存储用量大幅下降，而图结构则确保了语义相似性和检索准确性得以完整保留。</p><h2>为什么这项技术值得关注</h2><p>有了 LEANN，你的笔记本电脑瞬间就能变身成一个个人 AI 搜索引擎。所有这些功能都<strong>在本地跑起来</strong>，<strong>没有一分钱的云服务开销</strong>，<strong>隐私也得到了绝对保障</strong>。</p><p>如果你使用 Claude Code，应该知道它目前的搜索能力只停留在基本的关键词匹配。LEANN 可以无缝接入，作为 <strong>MCP（Model Context Protocol）服务</strong>，为你的模型增加<strong>真正的语义检索能力</strong>，让你搜索的是“意义”，而不是单纯的“词语”。而且整个过程中你都不需要改变原有的工作流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412781" alt="" title="" loading="lazy"/></p><h2>机制解析</h2><p>LEANN 之所以强大，关键在于它<strong>修剪掉了大量的“赘肉”</strong>：</p><ul><li><strong>基于图的重计算</strong>从根本上消除了对大型嵌入仓库的需求。</li><li><strong>使用 CSR（Compressed Sparse Row，压缩稀疏行）格式修剪的图</strong>，显著降低了存储的额外开销。</li><li><strong>智能缓存与重计算逻辑</strong>在检索速度和磁盘使用之间找到了一个绝佳的平衡点。</li></ul><p>这些优化结合起来创造了一个既轻量又具备可扩展性的系统。这是一个能装进你笔记本电脑、却能处理数百万记录的数据库。</p><h2>快速上手</h2><p>上手 LEANN 非常简单，代码如下：</p><pre><code>#git clone https://github.com/yichuan-w/LEANN.git leann  
#cd leann  
#uv pip install leann  
      
from leann import LeannBuilder, LeannSearcher, LeannChat  
from pathlib import Path  
INDEX_PATH = str(Path("./").resolve() / "demo.leann")  
      
# Build an index  
builder = LeannBuilder(backend_name="hnsw")  
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")  
builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")  
builder.build_index(INDEX_PATH)  
      
# Search  
searcher = LeannSearcher(INDEX_PATH)  
results = searcher.search("fantastical AI-generated creatures", top_k=1)  
      
# Chat with your data  
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})  
response = chat.ask("How much storage does LEANN save?", top_k=1)</code></pre><h2>总结</h2><p>LEANN 的出现，标志着向量数据库领域正在迎来自己的 <strong>SQLite 时刻</strong>。SQLite 的成功在于它提供了<strong>轻量级、零配置、无服务器</strong>的本地数据管理能力，它让应用开发者可以轻松地在边缘设备、桌面应用中嵌入强大的 SQL 能力。</p><p>LEANN 也在做同样的事情，但它针对的是 <strong>RAG 和语义搜索能力</strong>。它能够将数百万条嵌入向量存放在本地文件中还能快速检索，对于那些需要构建<strong>离线应用、移动端 AI 功能</strong>或纯粹关注<strong>个人数据隐私</strong>的开发者而言，LEANN 提供了一种开箱即用的、极度高效的解决方案。所以有兴趣的赶紧试试吧。</p><p><a href="https://link.segmentfault.com/?enc=bItDfZrtWs6fv91GcM7omA%3D%3D.4CmGpUDMGynEbqlluru9ym2f%2FNfdFM8Ia1z5qITRtDEBr5RB1zKHndK2dIw7Cerbou2V%2FaDGPxqkSGesPV%2FGfA%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/1b13106d6d7a46f2adebc33bc49ff8d8</a></p>]]></description></item><item>    <title><![CDATA[Doris MCP Server 0.5]]></title>    <link>https://segmentfault.com/a/1190000047412793</link>    <guid>https://segmentfault.com/a/1190000047412793</guid>    <pubDate>2025-11-19 22:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近日，Doris MCP Server 0.5.1 版本带来了多项企业级数据治理与分析能力的功能升级，进一步提升系统稳定性与易用性，欢迎下载体验。</p><p>项目地址：<a href="https://link.segmentfault.com/?enc=p6kvNSK2rMvz3AXspWJkPA%3D%3D.rMSJluJ6HSjnXRy4XIAt0W6XHGR8r%2Fx0UMf3DfYCFTHfy1ocMcHyazkD0KyvaFqB" rel="nofollow" target="_blank">https://github.com/apache/doris-mcp-server</a></p><h2>新增能力概览</h2><ul><li>全局 SQL 超时配置增强：支持通过配置项统一控制所有 SQL 查询超时时间，所有入口（MCP 工具、API、批量查询等）均严格受控</li><li>解决连接池 at_eof 异常连接问题：全新自愈式连接池架构，99.9% 消除连接异常，生产环境稳定性大幅提升。</li><li>新增支持 8 项数据治理与分析工具：数据质量分析、血缘追踪、数据新鲜度监控、访问模式分析、依赖分析、慢查询分析、资源增长分析等一站式能力；</li><li>新增支持 ADBC 数据传输协议：基于 Arrow Flight SQL 协议带来 3-10 倍的查询加速。</li><li>全面升级日志系统：企业级分级日志、自动清理、审计追踪，运维无忧。</li><li>调参文档改版优化：当前所有参数支持环境变量，文档更清晰，配置更灵活。</li></ul><p>本次升级完全兼容 Doris MCP Server 0.4.x 版本，可参考文档步骤平滑迁移。详情请见项目文档。</p>]]></description></item><item>    <title><![CDATA[用了半年Cursor，我为什么退了会员？]]></title>    <link>https://segmentfault.com/a/1190000047412823</link>    <guid>https://segmentfault.com/a/1190000047412823</guid>    <pubDate>2025-11-19 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>开头</h2><p>我做了件让社群炸锅的事——</p><p>把用了半年的Cursor会员，退了。</p><p>20美元一个月那个。</p><p>学员们私信我：「老师你不搭产品了？」</p><p>我说：「搭啊，而且速度更快了。」</p><p>「那你咋实现的？你又不会写代码。」</p><p>「一个3块钱的工具。」</p><p>「？？？」</p><p>他们以为我在开玩笑。直到我在社群里演示了一遍——</p><p>用3块钱的工具，20分钟搭了个落地页。</p><p>这才发现，不会写代码的产品经理，也能“轻松”玩转AI编程工具（毕竟 cursor 虽然能玩，但绝不算轻松😭）。</p><p>关键是，比Cursor还好用。</p><h2>Cursor哪里惹到我了？</h2><p>先说清楚，Cursor本身没啥大毛病。</p><p>半年多用下来，体验还行。代码补全准，理解上下文也到位，确实能提效。</p><p>但三个问题，越用越难受。</p><p><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdm6oL" alt="CleanShot 2025-11-19 at 17.00.44.png" title="CleanShot 2025-11-19 at 17.00.44.png"/></p><p><strong>第一个：贵</strong></p><p>20美元。</p><p>一个月。</p><p>我不是全职写代码的。一个月可能就写两三个项目。算下来，每个项目要分摊7-10美元。</p><p>我算了笔账。一年下来，240美元。够我喝多少杯咖啡了？</p><p>对我来说，不划算。</p><p><strong>第二个：限速破防了</strong></p><p>Pro会员，每月500次快速响应。</p><p>听起来挺多？不够用。</p><p>写个中等规模的功能，调个几十次很正常。项目一多，很快见底。</p><p>超了咋办？等。或者加钱。</p><p>这感觉就像，你办了健身卡，结果还得排队。</p><p><strong>第三个：国内体验真的差</strong></p><p>这个最要命。</p><p>很多人说，国内用Cursor经常抽风。有时候慢得要死，有时候直接连不上。</p><p>我也遇到过好几次。正写着代码呢，突然卡住。等半天，没反应。</p><p>工作的时候最怕这种。你不知道今天能不能顺利干活。这种不确定性，真的很烦。</p><p>有次我赶项目，连续三天晚上Cursor都抽风。我直接破防了。</p><p><strong>你用Cursor遇到过啥坑？评论区聊聊</strong> 👇</p><p><img width="605" height="472" referrerpolicy="no-referrer" src="/img/bVdm6oM" alt="图片" title="图片" loading="lazy"/></p><h2>TRAE SOLO到底是啥？</h2><p>字节搞的AI编程工具。SOLO模式去年7月上线。</p><p>定位和传统工具不太一样。</p><p><strong>核心区别在哪？</strong></p><p>Cursor是助手。你写代码，它辅助你。</p><p>TRAE SOLO是工程师。它能理解整个项目，独立干活。</p><p>差别大吗？</p><p>很大。</p><p><strong>用Cursor</strong>：</p><ul><li>你写代码，AI补全</li><li>遇到问题，你告诉它咋改</li><li>需要你盯着每一步</li></ul><p><strong>用TRAE SOLO</strong>：</p><ul><li>你提需求，它独立完成</li><li>自己理解项目结构</li><li>自己生成和集成代码</li><li>自己测试和修bug</li></ul><p>区别看出来了吧？</p><p>Cursor需要你参与每一步。SOLO更像个能独立干活的工程师。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdm6oN" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>两个智能体</strong></p><p>TRAE SOLO提供了两个：</p><ol><li><strong>SOLO Coder</strong>：懂技术的人用</li></ol><ul><li>处理复杂项目</li><li>理解完整代码库</li><li>多文件同步改</li></ul><ol start="3"><li><strong>SOLO Builder</strong>：小白也能用</li></ol><ul><li>快速搭应用</li><li>可视化界面</li><li>门槛低</li></ul><p>我主要用SOLO Coder。它的理解能力确实行。</p><p>不是理解当前这个文件。是理解整个项目。用了啥框架，模块咋关联的，代码风格啥样的。</p><p>这种理解能力，让它写出来的代码能无缝融入你的项目。不是那种一看就很AI的代码。</p><p><strong>关键优势：对国内用户友好</strong></p><p>这点很重要。</p><p>TRAE是国内团队开发的。网络环境对国内用户更友好。不用担心突然连不上，或者模型抽风。</p><p>实际体验，响应速度和稳定性确实比Cursor好。</p><p>基本上每次提问，1-2秒就回了。不像Cursor，有时候要等5-10秒，甚至更久。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6oO" alt="图片" title="图片" loading="lazy"/></p><h2>价格对比（重点）</h2><p>最直观的部分来了。</p><p>项目</p><p>Cursor</p><p>TRAE SOLO</p><p><strong>首月价格</strong></p><p>$20</p><p>$3</p><p><strong>后续月费</strong></p><p>$20/月</p><p>$10/月</p><p><strong>年度成本</strong></p><p>$240</p><p>$113</p><p><strong>快速响应限制</strong></p><p>500次/月</p><p>600 次/月</p><p><strong>网络稳定性</strong></p><p>国内用户可能不稳定</p><p>相对稳定</p><p><strong>中文支持</strong></p><p>一般</p><p>原生支持</p><p>我算了笔账。</p><p><strong>第一年总成本</strong>：</p><ul><li>Cursor：$240</li><li>TRAE SOLO：$3(首月)+$10×11=$113</li></ul><p>省了127美元。约900人民币。</p><p>够干啥的？够我喝一整年的奶茶了。</p><p>而且TRAE的10美元/月，Pro会员600 次（现在还有另赠送的次数）。</p><p>你算算看，值不值？评论区扣个数 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oP" alt="战术层-隐性陷阱卡片" title="战术层-隐性陷阱卡片" loading="lazy"/></p><h2>限时活动：手快有手慢无</h2><p>重点来了。</p><p>TRAE现在有个活动——<strong>Pro会员邀请计划</strong>。</p><p>机制是这样：</p><ol><li>你成为Pro会员（首月3美元）</li><li>你邀请朋友用你的链接升级Pro</li><li>朋友升级成功后，你俩在<strong>活动期内</strong>都能无限用SOLO</li><li>前提是，你得保持Pro会员身份</li></ol><p><strong>注意两点</strong></p><p><strong>第一：限时的</strong></p><p>官方页面写着"This program runs until --"。有结束时间，具体日期没公开。</p><p>应该是TRAE为了推广SOLO搞的早期活动。</p><p>现在可能是好时机。错过了，可能就没了。</p><p><strong>第二：不是白嫖</strong></p><p>你需要保持Pro会员（每月10美元）才能持续用。</p><p>但用邀请机制，能在活动期内无限用SOLO。比正常订阅划算。</p><p><strong>真实价值在哪？</strong></p><p>正常来说：</p><ul><li>TRAE Pro会员：首月$3，后续$10/月</li><li>用邀请活动：活动期内无限用SOLO</li></ul><p>对比Cursor：</p><ul><li>费用更低</li><li>比500次/月多</li></ul><p><strong>稀缺性：需要邀请码</strong></p><p>这个活动的门槛是——你需要有邀请链接。</p><p>不是所有人都能直接参与。需要有Pro会员的邀请。</p><p>我手上有邀请链接。想试试的话，评论区扣1。</p><p>但我也不知道活动会持续多久。官方随时可能调整。</p><h2>实际体验：真的好用吗？</h2><p>光说不练假把式。</p><p>我用TRAE SOLO做了几个项目。分享下真实感受。</p><p><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdm6oQ" alt="CleanShot 2025-11-19 at 16.59.38.png" title="CleanShot 2025-11-19 at 16.59.38.png" loading="lazy"/></p><p><strong>别人咋说的</strong></p><p>有用户分享了用TRAE SOLO 3小时搭女装电商网站的经历。</p><p>爱范儿的测评也提到：“一张草图变网页，实测字节 TRAE SOLO，这些功能甚至比 Cursor 还好用”。</p><p>这些都是真实案例和第三方测评。</p><p><strong>我的真实感受</strong></p><p>用了一段时间，我的感受：</p><p>✅ <strong>响应速度</strong>：比Cursor快，基本秒回✅ <strong>理解能力</strong>：能理解项目结构，改得比较准✅ <strong>稳定性</strong>：网络连接稳定，没遇到过用不了的情况✅ <strong>性价比</strong>：10美元/月，比Cursor便宜一半</p><p>⚠️ <strong>还不够完美的地方</strong>：</p><ul><li>生态还在建设，插件不如Cursor丰富</li><li>有时候对需求理解会有偏差，需要多解释</li><li>文档还在完善，有些功能不太好找</li></ul><p>但考虑到价格和稳定性，这些小瑕疵完全能接受。</p><h2>AI工具这事儿，其实是生态之争</h2><p>很多人问我：模型哪个强？</p><p>我的回答可能让你意外。</p><p><strong>模型不是最重要的。</strong></p><p>生态才是。</p><p>为啥？</p><p><strong>第一：模型会变</strong></p><p>今天GPT-4领先，明天Claude可能反超。大模型迭代太快了，谁都不敢说自己永远第一。</p><p><strong>第二：生态是长期的</strong></p><p>你在一个生态里积累的数据、习惯的工作流，迁移成本很高。</p><p>就像你用惯了iPhone，换安卓会很不适应。不是安卓不好，而是你的数据、习惯都在苹果生态里。</p><p><strong>第三：体验差距来自生态</strong></p><p>同样的模型，接入了你的工作场景，体验完全不一样。</p><p>这就是为啥有人觉得某个AI特别好用，而你用起来感觉一般。因为它没和你的工作流打通。</p><p><strong>对我们意味着啥？</strong></p><p>选AI工具，本质上是选生态和阵营。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oR" alt="图片" title="图片" loading="lazy"/></p><p>你得想清楚：</p><ul><li>你日常主要用啥平台？微信？飞书？还是Slack、Notion？</li><li>你的数据积累在哪？</li><li>你希望AI能连接哪些场景？</li></ul><p><strong>举个实际例子</strong></p><p>我有个朋友，做新媒体的，主要在微信生态工作。那腾讯系的AI工具可能更适合他。</p><p>我是程序员，代码库都在GitHub。那Cursor、TRAE这种能深度理解代码仓库的工具更合适。</p><p>如果你是跨境从业者，主要用国外工具，那ChatGPT生态可能是首选。</p><p>没有绝对的最好。只有适不适合。</p><p>而且，我建议大家<strong>不要把鸡蛋放一个篮子里</strong>。</p><p>今天Cursor封了TRAE的API，明天可能还会有其他变化。</p><p>但如果你选的工具，本身就在你常用的生态里，至少不会说停就停。</p><p><strong>TRAE的生态优势</strong></p><p>我选它，除了价格和体验，还有个重要原因：它背后是字节的生态。</p><p>字节在国内的工具矩阵——飞书、Coze、剪映、抖音……覆盖了办公、创作、传播的全链路。</p><p>TRAE未来可能会和这些工具打通。</p><p>比如：</p><ul><li>用TRAE写的代码，直接部署到飞书小程序</li><li>用TRAE生成的内容，直接导入剪映做视频</li><li>用TRAE分析的数据，直接同步到飞书文档</li></ul><p>这些不是幻想。是有生态基础的。</p><p>这就是生态的力量。</p><p><img width="720" height="306" referrerpolicy="no-referrer" src="/img/bVdm6oS" alt="图片" title="图片" loading="lazy"/></p><p><strong>你在啥生态里工作？评论区聊聊</strong> 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oT" alt="未来层-生态想象卡片" title="未来层-生态想象卡片" loading="lazy"/></p><h2>啥人适合？啥人别碰？</h2><p>TRAE SOLO不是万能的。</p><p><strong>我有个朋友系列</strong></p><p>我有个朋友，做独立开发的。每个月预算就500块。20刀的Cursor对他来说确实有点贵。</p><p>他试了TRAE之后，直接把Cursor退了。首月3美元，后续10美元/月。对他来说，刚刚好。</p><p>还有个做外包的哥们，全职写代码。一天要调用AI几百次。Cursor的500次限制对他来说根本不够用。</p><p>换了TRAE之后，无限制调用。再也不用担心次数用完了。</p><p>但我另一个朋友，在大厂做架构师。他们项目很复杂，需要很多企业级功能。TRAE的生态还在建设，暂时不太够。</p><p>他还是在用Cursor。</p><p>还有个朋友，重度Cursor用户。装了一堆插件，工作流已经深度绑定了。迁移成本太高。</p><p>他也没换。</p><p><strong>你是哪种情况？评论区说说</strong> 👇</p><h2>我的选择</h2><p>三个月过去了。</p><p>Cursor的会员自动续费，我没开。</p><p>TRAE用得挺顺手。</p><p>为啥？</p><ol><li><strong>价格</strong>：省了一半的钱</li><li><strong>稳定性</strong>：网络更稳定</li><li><strong>体验</strong>：响应快，理解准</li><li><strong>活动</strong>：限时福利，性价比高</li></ol><p>我不确定3美元的首月价格会持续多久。限时活动也不知道啥时候会结束。</p><p>但至少现在，TRAE SOLO对我来说是性价比很高的选择。</p><p>省下的钱，够我喝一个月奶茶了。</p><p><strong>想试试的话</strong>：</p><ol><li>评论区扣1，我发邀请链接</li><li>首月3美元，可以体验一个月再决定</li><li>觉得不合适可以随时取消</li></ol><p>用我的邀请链接注册，咱俩在活动期内都能无限用SOLO。</p><p>先到先得。</p><p>有问题评论区见！🚀</p><hr/><p><strong>补充说明</strong>：</p><p>这篇文章基于我的真实使用体验，没收广告费（新号大厂也看不上😭）。推荐TRAE是因为它确实解决了我的痛点，价格也合理。</p><p>如果你有不同的使用场景或需求，欢迎在评论区讨论。选工具这事，没有绝对的好坏，只有适不适合。</p>]]></description></item><item>    <title><![CDATA[如何安装 MousePlus_v5_4_]]></title>    <link>https://segmentfault.com/a/1190000047412595</link>    <guid>https://segmentfault.com/a/1190000047412595</guid>    <pubDate>2025-11-19 21:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p>一、准备工作</p><ol><li><strong>下载好安装包</strong>：<strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=fFHJkUie2jlN%2B4CkhBpphg%3D%3D.RpT7wna%2FSVtDLSqj6ELgy9MHrCwDrYFW74WmKmOMcq2B0eKGMbHsfKgEx9APJjo6" rel="nofollow" title="https://pan.quark.cn/s/cdd7206bc5a8" target="_blank">https://pan.quark.cn/s/cdd7206bc5a8</a>，下载了 <strong>MousePlus_v5_4_13.exe</strong>这个文件，一般是个exe格式的程序，双击就能运行。</li></ol><h3>二、开始安装</h3><ol><li><strong>找到安装文件</strong>：打开你下载文件的那个文件夹（比如“下载”文件夹），找到 <strong>MousePlus_v5_4_13.exe</strong>，双击它。</li><li><strong>可能会弹出安全提示</strong>：Windows 可能会问你“是否要运行这个程序”，点  <strong>“是”</strong> 或者  <strong>“允许”</strong> 就行。</li><li><strong>安装向导启动</strong>：稍等一下，会跳出一个安装界面，一般有“下一步”“安装”“完成”这些按钮。</li><li><p><strong>选择安装位置（可选）</strong> ：</p><ul><li>有的版本会让你选安装到哪个文件夹，默认一般是在 C 盘某个地方（比如 Program Files）。</li><li>如果你想改地方，可以点“浏览”自己选个盘或文件夹；不想改就直接点  <strong>“下一步”</strong> 。</li></ul></li><li><strong>开始安装</strong>：看到“安装”按钮后，点它，程序就会自动复制文件到你的电脑里，这个过程可能几秒钟到十几秒，耐心等等。</li><li><strong>安装完成</strong>：等它搞完后，会跳出提示说“安装成功”或者“安装完成”，点  <strong>“完成”</strong> 或者  <strong>“关闭”</strong> 就行。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[JSONBench 榜单排名第一！ 10]]></title>    <link>https://segmentfault.com/a/1190000047412610</link>    <guid>https://segmentfault.com/a/1190000047412610</guid>    <pubDate>2025-11-19 21:05:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>坦白讲，每次看性能测试排行榜，我都会下意识地先找找 Apache Doris 在哪个位置。</p><p>这次打开 JSONBench 的榜单，心情一如既往的期待加紧张。</p><p>好在结果让我松了一口气：默认配置下就能排到第三，仅次于维护方 ClickHouse 的两个版本。</p><p>不过，Doris 只能止步于此了吗？经过一系列优化后，查询时长能不能再缩短点？和 ClickHouse 的差距在哪里？</p><p>调优前后对比图镇楼，至于调优的具体思路，请一起往下看吧。</p><ul><li><p>Apache Doris 排名 (Default)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412612" alt="Apache Doris 排名 (Default)" title="Apache Doris 排名 (Default)"/></p></li><li><p>Apache Doris 排名 (Unofficial Tuned)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412613" alt="Apache Doris 排名 (Unofficial Tuned)" title="Apache Doris 排名 (Unofficial Tuned)" loading="lazy"/></p></li></ul><h2>JSONBench 简介</h2><p>JSONBench 是一个为 JSON 数据而生的数据分析 Benchmark，简单来说，它由 10 亿条来自真实生产环境的 JSON  数据、5 个针对 JSON  构造的特定 SQL 查询组成，旨在对比各个数据库系统对半结构化数据的处理能力。目前榜单包括 ClickHouse、MongoDB、Elasticsearch、DuckDB、PostgreSQL 等知名数据库系统，截至目前，<strong>Doris 的性能表现是 Elasticsearch 的 2 倍，是 PostgreSQL 的 80 倍</strong>。</p><blockquote><em>JSONBench 官网地址：jsonbench.com</em></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412614" alt="JSONBench 简介.png" title="JSONBench 简介.png" loading="lazy"/></p><p>不仅在性能上 Apache Doris 领先其他同类产品，<strong>在数据集相同的情况下，Apache Doris 的存储占用是 Elasticsearch 的 1/2、PostgreSQL 的 1/3</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412615" alt="JSONBench 简介-2.png" title="JSONBench 简介-2.png" loading="lazy"/></p><p>JSONBench 测试具体流程：首先在数据库中创建一张名为 Bluesky 的表，并导入十亿条真实的用户行为日志数据。测试过程中，每个查询重复执行三次，并且在每次查询前清空操作系统的 Page Cache，以模拟冷热查询的不同场景。最终，通过综合计算各查询的执行耗时得出数据库的性能排名。</p><p>在这个测试中，Apache Doris 使用了 Variant 数据类型来存储 JSON 数据，默认的建表 Schema 如下：</p><pre><code class="SQL">CREATE TABLE bluesky (
    `id` BIGINT NOT NULL AUTO_INCREMENT,
    `data` variant NOT NULL
)
DISTRIBUTED BY HASH(id) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>Variant 是 Apache Doris 2.1 中引入一种新的数据类型 ，它可以存储半结构化 JSON 数据，并且允许存储包含不同数据类型（如整数、字符串、布尔值等）的复杂数据结构，而无需在表结构中提前定义具体的列。Variant 类型特别适用于处理复杂的嵌套结构，而这些结构可能随时会发生变化。在写入过程中，该类型可以自动根据列的结构、类型推断列信息，动态合并写入的 schema，并通过将 JSON 键及其对应的值存储为列和动态子列。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=MduoGda%2BRx53pszxTiDWcA%3D%3D.JO%2BNfiZCht94rcI3SW7rZpojGi0IIBzjzLmLH6qP0ndoSt5HegZfrW%2BnRrL4EQpiC5l7TPs0txGFtl6bUtkxeoFSMiG674ypePxMTBDLfpsIBD3hfNtd9cE8XGg0bPRuHvz3asTH3p50iRQIn7RzEg%3D%3D" rel="nofollow" target="_blank">Apache Doris Variant 类型详情</a></li></ul></blockquote><h2>调优思路与原理</h2><p>JSONBench 榜单排名依据各个数据库系统在默认配置下的性能数据，那么能否通过调优，让 Apache Doris 进一步释放性能潜力，实现更好的性能效果呢？</p><h3>01 环境说明</h3><ul><li>测试机器：AWS M6i.8xlarge(32C128G);</li><li>操作系统：Ubuntu24.04;</li><li>Apache Doris: 3.0.5;</li></ul><h3>02 Schema 结构化处理</h3><p>由于 JSONBench 特定查询中涉及到的 JSON 数据都是固定的提取路径，换言之，半结构化数据的 Schema 是固定的，因此，我们可以借助生成列，将常用的字段提取出来，实现半结构化数据和结构化数据结合的效果。类似的高频访问的 JSON 路径或者需要计算的表达式，都可以使用该优化思路，添加对应的生成列来实现查询加速。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=fNz6ImJBc0u%2BEK70VNGU%2FA%3D%3D.T2QZhn2HY%2FTZcnJ95aj%2F4uQKtR1R4V7vXlwLgtfDjK2hL2DEPFmdiWUjg1VsEg%2F0o8eb3CghQW%2BTRN6vhDIShoixmYyZdW1V%2FXLccXpnlEk%3D" rel="nofollow" target="_blank">查看 JSONBench 查询</a></li><li><a href="https://link.segmentfault.com/?enc=hsmdTUvp0tMkCRbher4hgg%3D%3D.1D7tjNihUSmJhAs70ug0lJzOXgwdKbGk5uwLdq2IRsutXXq3vgevO4Hxto9YJK7czjWR78dR3Yk7ldy8er66UZ0%2FiQjKYnc6PsZ0rxohlVnBWWkeSQ5C8sao1vZIBNTfZiI3NuSj464zAawaY%2BAh4w%3D%3D" rel="nofollow" target="_blank">Apache Doris 生成列详情</a></li></ul></blockquote><pre><code class="SQL">CREATE TABLE bluesky (
    kind VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.kind')) NOT NULL,
    operation VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.operation')) NULL,
    collection VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.collection')) NULL,
    did VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data,'$.did')) NOT NULL,
    time DATETIME GENERATED ALWAYS AS (from_microsecond(get_json_bigint(data, '$.time_us'))) NOT NULL,
    `data` variant NOT NULL
)
DUPLICATE KEY (kind, operation, collection)
DISTRIBUTED BY HASH(collection, did) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>除了可以减少查询时提取数据的开销，还可以用展平出来的列作为分区列，使得数据分布更均衡。</p><p>需要注意的是，查询的 SQL 语句也要改为使用展平列的版本：</p><pre><code class="SQL">// JSONBench 原始查询：
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count, COUNT(DISTINCT cast(data['did'] AS TEXT )) AS users FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, HOUR(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT cast(data['did'] AS TEXT ) AS user_id, MIN(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS first_post_ts FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT cast(data['did'] AS TEXT ) AS user_id, MILLISECONDS_DIFF(MAX(from_microsecond(CAST(data['time_us'] AS BIGINT))),MIN(from_microsecond(CAST(data['time_us'] AS BIGINT)))) AS activity_span FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;

// 使用展平列改写的查询：
SELECT collection AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT collection AS event, COUNT(*) AS count, COUNT(DISTINCT did) AS users FROM bluesky WHERE kind = 'commit' AND operation = 'create' GROUP BY event ORDER BY count DESC;
SELECT collection AS event, HOUR(time) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT did AS user_id, MIN(time) AS first_post_ts FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT did AS user_id, MILLISECONDS_DIFF(MAX(time),MIN(time)) AS activity_span FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;</code></pre><h3>03 Page Cache 调整</h3><p>调整查询语句后，开启 profile，执行完整的查询测试：</p><pre><code class="Plain">set enable_profile=true;</code></pre><p>进入 FE 8030 端口的 Web 页面，找到相关 profile 进行分析，此时发现 SCAN Operator 中的 Page Cache 命中率较低，导致热读测试过程中存在一部分冷读操作。</p><pre><code class="SQL">-  CachedPagesNum:  1.258K  (1258)
-  TotalPagesNum:  7.422K  (7422)</code></pre><p>这种情况通常是由于 Page Cache 容量不足，无法完整缓存 Bluesky 表中的数据。建议在 <code>be.conf</code> 中添加配置项 <code>storage_page_cache_limit=60%</code>，将 Page Cache 的大小从默认的内存总量的 20% 提升至 60%。重新运行测试后，可以观察到冷读问题已得到解决。</p><pre><code class="SQL">-  CachedPagesNum:  7.316K  (7316)
-  TotalPagesNum:  7.316K  (7316)</code></pre><h3>04 最大化并行度</h3><p>为了进一步挖掘 Doris 的性能潜力，可以将 Session 变量中的<code>parallel_pipeline_task_num</code>设为 32，因为本次 Benchmark 测试机器<code>m6i.8xlarge</code>为 32 核，所以我们将并行度设置为 32 以最大程度发挥 CPU 的计算能力。</p><pre><code class="SQL">// 单个 Fragment 的并行度
set global parallel_pipeline_task_num=32;</code></pre><h2>调优结果</h2><p>经过上述对 Schema、Query、内存限制、CPU 等参数的调整，我们对比了调优前后 Doris 的性能表现以及一些其他数据库系统的成绩，有如下结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412616" alt="调优结果.png" title="调优结果.png" loading="lazy"/></p><p>可以看到，<strong>对比调优前的 Doris，调优后 Doris 查询整体耗时降低了 74%，对比原榜单第一的 ClickHouse 产品实现了 39% 的领先优势</strong>。</p><h2>总结与展望</h2><p>通过对 Schema 的结构化处理、查询语句的优化、缓存配置的调整以及并行参数的设置，Apache Doris 整体查询耗时显著下降，并超越 ClickHouse。</p><p>在默认设置下，Doris 在 10 亿条 JSON 的查询耗时与 ClickHouse 仍有数秒的差异。然而，依托于 Doris 在 JSON 处理、Variant 类型支持及生成列等能力的加持，经调优后，其半结构化数据处理性能获得了进一步显著提升，并在同类数据库中表现出明显的领先优势。</p><p>未来，Apache Doris 将继续打磨在半结构化领域的数据处理能力，为用户带来更加优质、高效的分析体验，包括：</p><ul><li>优化 Variant 类型稀疏列的存储空间，支持万列以上的子列；</li><li>优化万列大宽表的内存占用；</li><li>支持 Variant 子列根据列名的 Pattern 自定义类型、索引等。</li></ul><h2>推荐阅读</h2><ul><li><a href="https://link.segmentfault.com/?enc=EiltnwORoL2PXtlZeXWj%2Fw%3D%3D.JIBRSS9NUFAUjOPZwmbpG2CLXEqDG2v1sFzlSjwHRtYoMMea6HUBiFAEkCd80QN%2B" rel="nofollow" target="_blank">Apache Doris 针对半结构化数据分析的解决方案及典型场景</a></li><li><a href="https://link.segmentfault.com/?enc=CXJHuLEc9QuwQo3UAeJBKA%3D%3D.J79R97kkQGGm6PHEzs1C7XL1dXg706GnrFcnZew04kh92PlJb%2BINZUdOnuj1pI3E" rel="nofollow" target="_blank">揭秘 Variant 数据类型：灵活应对半结构化数据，JSON 查询提速超 8 倍，存储空间节省 65%</a></li></ul>]]></description></item>  </channel></rss>