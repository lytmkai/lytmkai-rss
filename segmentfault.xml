<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[# 适合小企业使用的KVM虚拟机管理工具]]></title>    <link>https://segmentfault.com/a/1190000047437261</link>    <guid>https://segmentfault.com/a/1190000047437261</guid>    <pubDate>2025-11-28 21:05:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>背景：</h2><p>我们日常在管理KVM虚拟机主要使用的功能是批量克隆、修改、删除、启动、关闭KVM虚拟机，其中克隆、修改是最为重要的功能，因为其他功能我们用<code>virt-manager</code>也可以方便的操作，比如启动、关闭。虽然<code>virt-manager</code>也可以实现克隆、修改之类的操作，但是你若有一大批需要创建的虚拟机，那么它将是痛苦的，而<code>zzxia-kvm-manage</code>项目就是为了解决这个问题，你只需要通过编辑一个markdown表格文件就可以实现批量虚拟机的克隆和IP地址修改之类的预设操作，在此过程中无需人工干预，省心省力。项目地址：<strong><a href="https://link.segmentfault.com/?enc=BIaIFVTWaoqF57Xws1O8UQ%3D%3D.HxmbAFN3QUM3blI4dpOSi4z6JveQjpN5SB8CeKr5kIU6iQjILbKpLbLBTLtz3vY6" rel="nofollow" target="_blank">https://gitee.com/zhf_sy/zzxia-kvm-manage</a></strong></p><p>[toc]</p><h2>1 介绍</h2><p>批量克隆、修改、删除、启动、自动启动、关闭KVM虚拟机。适合小企业使用。</p><h3>1.1 功能：</h3><ol><li>克隆虚拟机：通过编辑my_vm.list文件定义虚拟机信息，然后运行vm-clone.sh，选择克隆模板，然后按照my_vm.list清单克隆出想要的虚拟机</li><li>修改虚拟机信息：【主机名、IP、IP子网掩码、网关、域名、DNS】，一般主要配合vm-clone.sh使用，也可以单独使用</li><li>批量启动指定虚拟机；批量启动清单中的虚拟机；批量启动选择的虚拟机</li><li>批量设置自动启动指定虚拟机；批量设置自动启动清单中的虚拟机；批量设置自动启动选择的虚拟机</li><li>批量关闭指定虚拟机；批量关闭清单中的虚拟机；批量关闭选择的虚拟机</li><li>批量删除指定虚拟机；批量删除清单中的虚拟机；批量删除选择的虚拟机</li></ol><h3>1.2 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk；<br/>&lt;img src="https://gitee.com/zhf_sy/pic-bed/raw/master/dao.png" alt="打赏" style="zoom:40%;" /&gt;</li></ol><h2>2 软件架构</h2><p>Linux shell</p><h2>3 安装教程</h2><p>克隆到KVM服务器上即可</p><h2>4 使用说明</h2><p>请使用-h|--help参数运行sh脚本即可看到使用帮助<br/>除了kvm，你还需要安装guestfs，在centos7上运行<code>yum install -y  libguestfs-tools</code></p><h3>4.1 环境变量文件<code>kvm.env</code></h3><p>基于<code>kvm.env.sample</code>创建环境变量文件<code>kvm.env</code>，根据你的环境修改相关环境变量，这个非常重要，否则你可能运行出错</p><pre><code class="bash">$ cat kvm.env.sample
#!/bin/bash

# 静默方式
export QUIET='no'     #--- yes|no

# KVM环境参数
export KVM_XML_PATH='/etc/libvirt/qemu'                 #-- KVM虚拟XML配置文件路径（CENTOS下XML的默认路径，如果是UBUNTU，请修改）

# 模板虚拟机参数
/dev/sda1】
export VM_NIC_CONF_FILE='/etc/sysconfig/network-scripts/ifcfg-eth0'   #-- 模板虚拟机CentOS系统内的网卡配置文件

# 新虚拟机默认参数，特殊值可以在【my_vm.list】中指定
export VM_DEFAULT_DNS='192.168.11.3,192.168.11.4'      #-- 默认DNS，最多两个DNS服务器，中间用【,】分隔，不要有空格
export VM_DEFAULT_DOMAIN='zjlh.lan'                    #-- 默认域名
export VM_DEFAULT_DISK_IMG_PATH='/var/lib/libvirt/images'   #-- 虚拟机磁盘文件默认路径</code></pre><h3>4.2 虚拟机列表文件<code>my_vm.list</code></h3><p>基于<code>my_vm.list.sample</code>创建虚拟机列表文件<code>my_vm.list</code>（默认，文件名可以是其他名称），根据自己的需要定制虚拟机信息，以逗号分隔，用#注释掉不需要的行：</p><pre><code class="text">$ cat  my_vm.list.sample
### 虚拟机克隆清单
###.
###   2【名称：NAME】= [自定义]
###     既是虚拟机名称，也是虚拟机主机名
###..
###   3【CPU：CPU】= [自定义数量]
###..
###   4【内存：MEM】= [自定义数量]
###     单位是GB
###..
###   5【网卡：NIC】= [自定义]
###.    KVM网卡名称
###..
###   6【IP地址：IP】= [自定义]
###.    IP地址
###..
###   7【IP掩码：IP_MASK】= [自定义]
###.    IP地址掩码，例如：24、16、8、12
###..
###   8【IP网关：IP_GATEWAY】= [自定义]
###.    IP网关
###..
###   9【DNS：DNS】= &lt; 自定义1 &lt;,自定义2&gt; &gt;
###.    可以定义0~2个，例如：1.1.1.1, 2.2.2.2，或者8.8.8.8
###.
###  10【域名：DOMAIN】= &lt;自定义&gt;
###.    虚拟机的域名
###.
###  11【磁盘IMG路径：IMG_PATH】= &lt;自定义&gt;
###.    虚拟机KVM磁盘文件存放路径，例如：/disk2/images
###.
###  12【备注：NOTE】= [ 自定义 ]
###     说明信息
###.
###
###     暂时不需要的行用'#'注释掉
###
###
#| NAME                   | CPU  | MEM  | NIC  | IP             | IP_MASK | IP_GATEWAY   | DNS                        | DOMAIN   | IMG_PATH                    | NOTE               |
#| **名称**               | CPU  | 内存 | 网卡 | **IP地址**     | IP掩码  | **IP网关**   | **DNS**                    | **域名** | **磁盘IMG路径**             | **备注**           |
#| ---------------------- | ---- | ---- | ---- | -------------- | ------- | ------------ | -------------------------- | -------- | --------------------------- |                    |
| v-192-168-11-190-deploy | 1    | 2    | br1  | 192.168.11.190 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-191-mast   | 4    | 8    | br1  | 192.168.11.191 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-192-node   | 4    | 8    | br1  | 192.168.11.192 | 24      | 192.168.11.1 | 8.8.8.8                    | zj.lan   | /var/lib/libvirt/images22   |                    |
| v-192-168-11-193-node   | 4    | 8    | br1  | 192.168.11.193 | 24      | 192.168.11.1 | 1.1.1.1, 2.2.2.2           | hb.lan   |                             |                    |
| v-192-168-11-194-etcd   | 2    | 4    | br1  | 192.168.11.194 | 24      | 192.168.11.1 |                            |          |                             |                    |
#| v-192-168-11-195-etcd   | 2    | 4    | br1  | 192.168.11.195 | 24      | 192.168.11.1 |                            |          |                             |                    |
#| v-192-168-11-196-etcd   | 2    | 4    | br1  | 192.168.11.196 | 24      | 192.168.11.1 |                            |          |                             |                    |
| v-192-168-11-197-repo   | 2    | 4    | br1  | 192.168.11.197 | 24      | 192.168.11.1 |                            |          | /disk2/images               |                    |</code></pre><h3>4.3 克隆</h3><p><strong>克隆前的建议：</strong></p><ul><li>建议先制作好一个较为完美的模板虚拟机，然后在克隆时选择使用他</li><li>查看KVM环境变量文件<code>kvm.env</code>，看是否与你的实际情况相同，否则请修改它</li></ul><pre><code class="bash">$ ./vm-clone.sh --help

    用途：KVM上虚拟机克隆，并修改相关信息（主机名、IP、IP子网掩码、网关、域名、DNS）
    依赖：
        ./vm-img-modify.sh
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-clone.sh  [-h|--help]
        ./vm-clone.sh  &lt;-f|--file {清单文件}&gt;  &lt; -q|--quiet  [-t|--template {虚拟机模板}] &gt;
        ./vm-clone.sh  &lt;-f|--file {清单文件}&gt;  &lt;-t|--template {虚拟机模板}&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -f|--file      虚拟机清单文件，默认为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -q|--quiet     静默方式
        -t|--templat   指定虚拟机模板
    示例:
        #
        ./vm-clone.sh  -h
        # 一般
        ./vm-clone.sh                       #--- 默认虚拟机清单文件【./my_vm.list】，非静默方式，手动选择模板
        ./vm-clone.sh  -t v-centos-1        #--- 默认虚拟机清单文件【./my_vm.list】，非静默方式，基于模板【v-centos-1】创建
        # 指定vm清单文件
        ./vm-clone.sh  -f xxx.list                      #--- 使用虚拟机清单文件【xxx.list】，非静默方式，手动选择模
        ./vm-clone.sh  -f xxx.list  -t v-centos-1       #--- 使用虚拟机清单文件【xxx.list】，非静默方式，基于模板【v-centos-1】创建
        # 静默方式
        ./vm-clone.sh  -q  -t v-centos-1                #--- 默认虚拟机清单文件【./my_vm.list】，静默方式，基于模板【v-centos-1】创建
        ./vm-clone.sh  -q  -t v-centos-1  -f xxx.list   #--- 使用虚拟机清单文件【xxx.list】，静默方式，基于模板【v-centos-1】创建</code></pre><h3>4.4 修改vm信息</h3><pre><code class="bash">$ ./vm-img-modify.sh 

    用途：修改KVM虚拟机主机名及网卡信息（主机名、IP、IP子网掩码、网关、域名、DNS）
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-img-modify.sh  [-h|--help]
        ./vm-img-modify.sh  &lt;-q|--quiet&gt;  [ {VM_NAME}  {NEW_IP}  {NEW_IP_MASK}  {NEW_GATEWAY} ]  &lt;{NEW_DOMAIN}&gt;  &lt;{NEW_DNS1}&lt;,{NEW_DNS2}&gt;&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -q|--quiet     静默方式
    示例:
        #
        ./vm-img-modify.sh  -h        #--- 帮助
        # 一般
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3,192.168.11.4
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3
        ./vm-img-modify.sh  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1
        # 静默方式
        ./vm-img-modify.sh  -q  v-192-168-1-3-nexxxx  192.168.1.3  24  192.168.11.1  zjlh.lan  192.168.11.3,192.168.11.4</code></pre><h3>4.5 启动（或自动启动）虚拟机</h3><pre><code class="bash">$ ./vm-start.sh -h

    用途：启动虚拟机；设置虚拟机自动启动
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-start.sh  [-h|--help]
        ./vm-start.sh  [-l|--list]
        ./vm-start.sh  [ &lt;-s|--start&gt;  &lt;-a|--autostart&gt; ]  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -s|--start     启动虚拟机
        -a|--autostart 开启自动启动虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
    示例:
        #
        ./vm-start.sh  -h                   #--- 帮助
        ./vm-start.sh  -l                   #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-start.sh  -s                   #--- 启动默认虚拟机清单文件【./my_vm.list】中的虚拟机
        ./vm-start.sh  -s  -a               #--- 启动默认虚拟机清单文件【./my_vm.list】中的虚拟机，并设置为自动启动
        ./vm-start.sh  -a                   #--- 自动启动默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-start.sh  -s  -f xxx.list      #--- 启动虚拟机清单文件【xxx.list】中的虚拟机
        ./vm-start.sh  -a  -f xxx.list      #--- 自动启动虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-start.sh  -s  -S               #--- 启动我选择的虚拟机
        ./vm-start.sh  -a  -S               #--- 自动启动我选择的虚拟机
        # 指定虚拟机
        ./vm-start.sh  -s  -A  vm1 vm2      #--- 启动虚拟机【vm1、vm2】
        ./vm-start.sh  -a  -A  vm1 vm2      #--- 自动启动虚拟机【vm1、vm2】</code></pre><h3>4.6 关闭虚拟机</h3><pre><code class="bash">$ ./vm-shutdown.sh -h

    用途：shutdown虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-shutdown.sh  [-h|--help]
        ./vm-shutdown.sh  [-l|--list]
        ./vm-shutdown.sh  &lt;-q|--quiet&gt;  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
        -q|--quiet     静默方式
    示例:
        #
        ./vm-shutdown.sh  -h               #--- 帮助
        ./vm-shutdown.sh  -l               #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-shutdown.sh                   #--- shutdown默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-shutdown.sh  -f xxx.list      #--- shutdown虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-shutdown.sh  -S               #--- shutdown我选择的虚拟机
        # 指定虚拟机
        ./vm-shutdown.sh  -A  vm1 vm2      #--- shutdown虚拟机【vm1、vm2】
        # 静默方式
        ./vm-shutdown.sh  -q               #--- shutdown默认虚拟机清单文件【./my_vm.list】中的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -f xxx.list  #--- shutdown虚拟机清单文件【xxx.list】中的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -S           #--- shutdown我选择的虚拟机，用静默方式
        ./vm-shutdown.sh  -q  -A  vm1 vm2  #--- shutdown虚拟机【vm1、vm2】，用静默方式</code></pre><h3>4.7 删除虚拟机</h3><pre><code class="bash">$ ./vm-rm.sh -h

    用途：删除虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-rm.sh  [-h|--help]
        ./vm-rm.sh  [-l|--list]
        ./vm-rm.sh  &lt;-q|--quiet&gt;  [ [-f|--file {清单文件}] | [-S|--select] | [-A|--ARG {虚拟机1} {虚拟机2} ... {虚拟机n}] ]
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出KVM上的虚拟机
        -f|--file      从文件选择虚拟机（默认），默认文件为【./my_vm.list】，请基于【my_vm.list.sample】创建
        -S|--select    从KVM中选择虚拟机
        -A|--ARG       从参数获取虚拟机
        -q|--quiet     静默方式
    示例:
        #
        ./vm-rm.sh  -h               #--- 帮助
        ./vm-rm.sh  -l               #--- 列出KVM上的虚拟机
        # 一般（默认从默认文件）
        ./vm-rm.sh                   #--- 删除默认虚拟机清单文件【./my_vm.list】中的虚拟机
        # 从指定文件
        ./vm-rm.sh  -f xxx.list      #--- 删除虚拟机清单文件【xxx.list】中的虚拟机
        # 我选择
        ./vm-rm.sh  -S               #--- 删除我选择的虚拟机
        # 指定虚拟机
        ./vm-rm.sh  -A  vm1 vm2      #--- 删除虚拟机【vm1、vm2】
        # 静默方式
        ./vm-rm.sh  -q               #--- 删除默认虚拟机清单文件【./my_vm.list】中的虚拟机，用静默方式
        ./vm-rm.sh  -q  -f xxx.list  #--- 删除虚拟机清单文件【xxx.list】中的虚拟机，用静默方式
        ./vm-rm.sh  -q  -S           #--- 删除我选择的虚拟机，用静默方式
        ./vm-rm.sh  -q  -A  vm1 vm2  #--- 删除虚拟机【vm1、vm2】，用静默方式</code></pre><h3>4.8 列出已有虚拟机</h3><pre><code class="bash">$ ./vm-list.sh -h

    用途：列出KVM上的虚拟机
    依赖：
    注意：本脚本在centos 7上测试通过
    用法：
        ./vm-list.sh  &lt;-h|--help&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
    示例:
        #
        ./vm-list.sh  -h                   #--- 帮助
        ./vm-list.sh                       #--- 列出KVM上的虚拟机</code></pre><h3>4.8 简单管理虚拟机命令</h3><p>看名字就知道他的用途了</p><pre><code class="bash">./easy-save-all-online-vm-list-to-file.sh
./easy-save-all-vm-list-to-file.sh
./easy-start-spec-vm-list.sh
./easy-shutdown-all-online-vm.sh
./easy-shutdown-spec-vm-list.sh
./easy-force-shutdown-spec-vm-list.sh</code></pre><h2>5 最后</h2><p>好用</p>]]></description></item><item>    <title><![CDATA[以 StarRocks 4.0 为核，引]]></title>    <link>https://segmentfault.com/a/1190000047437269</link>    <guid>https://segmentfault.com/a/1190000047437269</guid>    <pubDate>2025-11-28 21:04:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着人工智能与大数据技术的深度融合，数据分析平台正面临一场深刻的变革。传统的 T+1 批处理模式、孤立的分析系统以及仅面向内部用户的服务模式，已无法满足当今业务对实时性、灵活性和智能化的高度需求。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLg" alt="640.jpg" title="640.jpg"/></p><p>新时代的分析场景，正从高层战略延伸至一线运营，从批处理转向批流一体，服务对象也从内部员工扩展至外部客户乃至 AI Agent。在这一背景下，一个能够支撑极速、实时、统一分析的数据底座，已成为企业在 AI 时代保持竞争力的关键。</p><h4>StarRocks 进化：实时与统一</h4><p>技术演进的根本目标在于解决客户的业务问题。StarRocks 的发展路径正是这一理念的体现，从解决 BI 查询慢，到实现业务实时洞察，再到构建统一的湖仓分析能力。今天，<strong>StarRocks 4.0 聚焦AI实时湖仓”这一场景</strong>，成为了一个为 AI Agent 应用场景做好充分准备的数据引擎。</p><h4>StarRocks 4.0 核心能力：为 AI 湖仓注入强劲动力</h4><p>StarRocks 4.0 带来了多项重大改进，旨在全面提升分析性能和用户体验：</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLj" alt="640 (1).jpg" title="640 (1).jpg" loading="lazy"/></p><ul><li><strong>极致性能，加速洞察</strong>：通过深度的算子优化，StarRocks 将 Iceberg 数据湖上的分析性能提升了 60%，JSON 数据处理速度提升 3-15 倍。这意味着企业的业务团队可以更快地从海量数据中获得洞察，抢占市场先机。</li><li><strong>把握当下，实时加速</strong>：通过优化数据导入机制，<strong>对云存储的 API 调用降低了 70%-90%</strong>，实现了数据的秒级可见与分析。无论是实时风控、动态定价还是即时营销，企业的决策都可以基于最新的业务动态。</li><li><strong>极致统一，简化架构</strong>：StarRocks 提供原生的湖仓一体能力，支持对 Iceberg、Hudi、Paimon 等主流数据湖格式进行高性能读写，并以统一的 Catalog 管理权限，帮助企业告别繁杂的 ETL 和多套组件并存现状，构建一个开放、统一、高效的 One Data 体系。同时，通过全面向量化、CBO、现代化物化视图等上百项优化，为全场景分析提供性能保障。</li></ul><h4>落地实践：StarRocks 在多元场景中的应用</h4><p>StarRocks 已在金融、零售、制造等行业的头部企业中得到广泛应用，并取得了显著成效。</p><p><strong>金融：赋能大型城商行，实现架构现代化</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLk" alt="640 (5).jpg" title="640 (5).jpg" loading="lazy"/></p><p>该行原有的经典Lambda架构组件繁多、维护成本高昂，多套 OLAP 引擎并存导致数据孤岛问题严重。通过引入 StarRocks 作为统一分析引擎，替代了 Impala、HBase、Kylin 等多套系统。利用 StarRocks 直接分析内表与 Hive/Iceberg 外表，并借助物化视图等特性进行全场景加速。数据架构与 Pipeline 极大简化，运维成本显著降低，报表、驾驶舱、探查分析等所有应用均实现了极速响应。</p><p><strong>即时零售：支撑淘宝闪购，赢得即时零售战役</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLl" alt="640 (2).jpg" title="640 (2).jpg" loading="lazy"/></p><p>在竞争激烈的即时零售业务中，对实时决策、高并发查询和复杂多维分析的性能要求极高。淘宝闪购采用“Paimon + StarRocks”的实时湖仓架构，利用 StarRocks 对外表、分层物化视图的透明加速能力，实现了多维分析、UV 统计等复杂场景的极速响应，<strong>分析性能提升 10 倍、存储成本降低 60%</strong>，<strong>并稳定支撑每日超 17 万次的高并发查询。</strong></p><p><strong>电商：助力电商平台，迈向智能 BI 新阶段</strong></p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLm" alt="640 (3).jpg" title="640 (3).jpg" loading="lazy"/></p><p>该电商平台希望从“敏捷 BI”向“智能 BI”（ChatBI）演进，但面临技术栈复杂、多模态数据检索困难等问题。项目最终选用 StarRocks 作为统一数据底座。它不仅承载指标查询，同时凭借其内置的文本与向量检索能力，统一替代了原有的 ES 和 Milvus，简化了平台技术栈。该智能 BI 平台上线一年来，月活用户超百人，月均问答超 3000 次，<strong>准确率高达 90% 以上</strong>，有效降低数据分析门槛。</p><h4>未来：迈向 AI 增强的智能数据分析</h4><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncLn" alt="640 (4).jpg" title="640 (4).jpg" loading="lazy"/></p><p>作为 StarRocks 开源项目在中国的主要贡献者和推广者，镜舟科技深知企业级客户对稳定性、安全性及专业服务的更高要求。通过企业版产品镜舟数据库，在 StarRocks 内核能力的基础上，增加了安全管控、灾备、可视化运维等企业级功能，致力于帮助更多企业构建卓越的数据分析系统，共同拥抱数据驱动的智能未来。</p><p>展望未来，镜舟科技将持续推动 StarRocks 在 AI 增强领域的创新，提供智能建模、AI 增强分析闭环等能力，让数据分析变得更加智能、自动化。</p><p>镜舟科技致力于成为企业在 AI 时代最值得信赖的数据技术伙伴，与客户携手共筑开放、统一、高性能的 AI 实时湖仓，共同迈向数据驱动的智能未来。</p>]]></description></item><item>    <title><![CDATA[Spec Kit 踩坑实录：为什么我严格]]></title>    <link>https://segmentfault.com/a/1190000047437278</link>    <guid>https://segmentfault.com/a/1190000047437278</guid>    <pubDate>2025-11-28 21:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>引言：完美的流程，崩塌的结果</h2><p>最近我在使用 Spec Kit 做需求开发。这套工具宣称通过标准化的 7 个步骤——<strong>Specify（定义）、Clarify（澄清）、Plan（计划）、Tasks（任务）、Analyze（分析）、Checklist（检查）、Implement（实现）</strong>——来生成高质量代码。</p><p>我的初衷是美好的：输入需求，喝杯咖啡，输出代码。</p><p>但现实给了我一记响亮的耳光。当我满怀期待地跑到第 7 步 <code>Implement</code> 时，生成的代码完全不可用：数据库方言错了、重复造了已有的轮子、业务逻辑不仅没对齐甚至还跑偏了。</p><p>这让我意识到一个残酷的真相：<strong>在 AI 辅助编程中，如果缺乏严格的每一步把控，AI 就会陷入“幻觉”，原本的“自动驾驶”会变成“事故现场”。</strong></p><p>今天这篇博客，就是一次血泪复盘。我总结了 Spec Kit 流程中的“避坑指南”，希望大家能少走弯路。</p><hr/><h2>核心复盘：蝴蝶效应与“全自动”陷阱</h2><p>我在这次实践中最大的教训是：<strong>不要相信 AI 的默认假设。</strong></p><p>Spec Kit 的流程是一个链式反应。第 1 步的 <code>Specify</code> 如果有 1% 的偏差，到了第 3 步 <code>Plan</code> 就会变成技术选型的错误，到了第 7 步 <code>Implement</code> 就会变成几百行完全跑不通的垃圾代码。这就是 AI 编程的<strong>蝴蝶效应</strong>。</p><p>以下是具体的踩坑现场与应对策略：</p><h3>1. Specify &amp; Clarify（定义与澄清）：别只顾着答题</h3><p><strong>❌ 踩坑现场：</strong><br/>在第 2 步 <code>Clarify</code> 时，AI 会像面试官一样问我不清楚的地方。我当时只顾着回答它的问题，却忽略了检查它基于第一步生成的 <code>specify.md</code>。<br/><strong>结果：</strong> AI 在文档里悄悄“脑补”了一些我没提到、但它认为合理的逻辑，或者定义了错误的数据结构。</p><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>回溯检查</strong>：不要只看 Question，一定要回头精读 <code>specify.md</code> 的 Summary 和 Description。</li><li><strong>不仅仅是澄清</strong>：除了回答 AI 的问题，还要主动思考：“你生成的文档是否包含了我没说、但我默认你该知道的约束？”如果不包含，立刻补充。</li></ul><h3>2. Plan（技术方案）：张冠李戴的重灾区</h3><p><strong>❌ 踩坑现场：</strong><br/>这是最容易翻车的地方。AI 往往不知道你项目的具体技术栈细节。</p><ul><li><strong>案例 A</strong>：我的项目明明是 <strong>PostgreSQL</strong>，AI 生成的 Plan 里却打算用 <strong>MySQL</strong> 的方言写 SQL，甚至引入 MySQL 的驱动依赖。</li><li><strong>案例 B</strong>：项目里规定用 MyBatis-Plus，AI 却在 Plan 里规划了一套 JPA 的实体类。</li></ul><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>上下文注入</strong>：在这一步，必须强制检查技术栈。</li><li><strong>明确否决</strong>：如果 Plan 里出现了错误的技术选型（例如用错了数据库、ORM 框架），必须立刻打回重做，绝不能想着“生成代码后再改”。<strong>Plan 错了，后面的代码不仅是错的，还是不可挽回的错。</strong></li></ul><h3>3. Tasks（任务拆解）：拒绝重复造轮子</h3><p><strong>❌ 踩坑现场：</strong><br/>AI 作为一个“外来户”，它不知道你项目里有什么。</p><ul><li><strong>现象</strong>：AI 生成的 Tasks 里，赫然列着“配置数据库连接池”、“编写 Redis 工具类”、“搭建日志切面”。</li><li><strong>后果</strong>：这些基础设施（Infrastructure）在现有代码里早就有了！如果照着执行，你的项目里就会出现两个 RedisUtil，两套鉴权逻辑，导致代码极其臃肿甚至冲突。</li></ul><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>做减法</strong>：毫不留情地删除所有“基建类”任务。</li><li><strong>强制复用</strong>：在 Task 描述里明确标注：“使用现有的 <code>com.example.common.RedisUtil</code>，不要新建”。</li></ul><h3>4. Checklist（检查清单）：最后的防线</h3><p><strong>❌ 踩坑现场：</strong><br/>经历了前面几步的折腾，人容易产生疲劳感。到了第 6 步 <code>Checklist</code>，我当时的心态是：“差不多得了，快生成代码吧。” 于是看都没看细则就点了通过。<br/><strong>结果：</strong> AI 生成代码时彻底放飞自我，刚才在 Plan 里没拦住的错误，在这里全部变成了具体的 Bug。</p><p><strong>✅ 避坑策略：</strong></p><ul><li><strong>逐条审计</strong>：这步必须一个一个检查，不要放过任何疑点。</li><li><strong>脑内预演</strong>：看到 Checklist 时，脑子里要预演一遍生成的代码大概长什么样。如果这一步没把控住，AI 就会偏离你的初衷。<strong>只有这一步对了，Implement 才会是你想要的代码。</strong></li></ul><hr/><h2>方法论总结：从“指挥官”转变为“审计员”</h2><p>通过这次踩坑，我总结了一套使用 Spec Kit（或其他 AI 编程 Agent）的标准作业程序（SOP）：</p><h3>1. 角色转换</h3><p>不要把自己当成只会下命令的<strong>指挥官（Commander）</strong>，要把自己当成极其严格的<strong>代码审计员（Auditor）</strong>。AI 是实习生，你是 Tech Lead。</p><h3>2. 前置约束（Pre-Prompt）</h3><p>不要等到 AI 犯错再改。在开始流程前，最好贴一段<strong>“项目上下文声明”</strong>：</p><blockquote><p><strong>Project Context:</strong></p><ul><li><strong>DB</strong>: PostgreSQL (Not MySQL)</li><li><strong>ORM</strong>: MyBatis-Plus</li><li><strong>Infra</strong>: 禁止创建新的 DB/Redis 配置，必须复用 <code>src/common</code> 下的代码。</li></ul></blockquote><h3>3. 零容忍原则</h3><p><strong>每一步都需要严格把控，一个问题都不要放过。</strong><br/>如果在 <code>Plan</code> 阶段发现一个小偏差，不要幻想 AI 在 <code>Implement</code> 阶段会自动修正。相反，这个偏差会被放大 10 倍。</p><h3>4. 持续迭代你的 <code>prompt.md</code> 资产</h3><p>这是最高阶的玩法。<strong>AI 犯过的错，不要让它犯第二次。</strong><br/>Spec Kit 的核心配置通常在于 <code>prompt.md</code> 文件。每当你发现 AI 在某个环节踩坑（比如总是忘记加 <code>@Builder</code> 注解，或者总是想自己写 Util 类），<strong>请立刻将这条规则补充进你的 <code>prompt.md</code> 中。</strong><br/>随着你不断地“喂养”和修缮这个文件，它会变成一份<strong>“不懂你的人绝对用不好的”</strong>核心技术资产。你的 AI Agent 会越来越懂你的代码风格，效率也会呈现指数级上升。</p><h2>结语</h2><p>AI 编程工具确实能提升效率，但它目前还做不到“完全托管”。<strong>它能帮我们省去写样板代码的手速，但无法替代我们对技术方案的判断力。</strong></p><p>如果你也想用 Spec Kit 做出想要的代码，请记住：<strong>时刻保持警惕，把每一步的输出都当成必须要 Code Review 的代码来看待，并不断打磨你的 Prompt 资产。</strong></p><p>只有这样，AI 才能成为你的神队友，而不是那个坑你的实习生。</p><p>本文由<a href="https://link.segmentfault.com/?enc=U6vhj4QhA%2B9qLy%2FdsdM8hg%3D%3D.IJiUxVZjhZfoCAtkWoS9TBMhUmUOMh0gp8VoNWv%2Fpkc%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[# 好用且功能强大的自建CA证书服务器工]]></title>    <link>https://segmentfault.com/a/1190000047437288</link>    <guid>https://segmentfault.com/a/1190000047437288</guid>    <pubDate>2025-11-28 21:03:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>中文名：猪猪侠之CA服务器</p><p><strong>项目地址：<a href="https://link.segmentfault.com/?enc=gAvD%2BqsKB4DZb2Dq0RaGjQ%3D%3D.%2F5qvJRKYf3HSIbqnCxqCE5SWa1%2F4%2B2PEsh0XW%2FJgW3ctE9yGuY%2FAvdTPDGIRhpAkcHnxkdUZqHEQqfJKqzDV%2Fg%3D%3D" rel="nofollow" target="_blank">https://gitee.com/zhf_sy/zzxia-openssl-ca-server</a></strong></p><p>这基于openssl的CA证书服务器。你可以用它搭建自己的专属CA服务器，以方便为用户生成私钥、证书请求、颁发证书、吊销证书、证书续期、证书吊销列表等。它可以生成多种类型的证书，包括且不限于web服务器、代码、计算机、客户端、信任列表、时间戳、IPSec、Email、智能卡登陆及其他OID证书。只需简单在配置文件中指定即可，证书完全兼容与Windows、Linux、Android、iOS等PC及手机系统（自签名不兼容）。项目是产品化的，不用修改代码就可以管理CA服务器整个生命周期，计划未来增加web操作页面，实现用户从网页端申请、下载、续期等证书操作，以及证书吊销列表的分发。</p><p>[toc]</p><h2>1 介绍</h2><h3>1.1 背景</h3><p>由于现在https的盛行，我们经常需要在内网服务器、手机、PC上使用证书（内网域名没法使用免费的Letsencrypt证书），但多数时候大家只会生成自签名证书，不会以CA的方式颁发证书，更不会让用户安装CA证书，造成用户在使用过程中总是提示不安全，浪费时间且体验非常糟糕，再者，颁发证书的相关信息从来不保存，不具延续性，不是正经人的做法，哈哈哈哈哈哈哈哈哈！</p><p>另外：OpenSSL证书相关知识还是有点复杂的（虽然一般用的很简单），特别是一些概念，很多人搞不清用途与区别，所以生成较为复杂的证书就会走一些弯路（有别于简单的自签名证书），希望这个可以帮到你。也可以帮到我自己，免得要用的时候又得折腾，因为长时间不用，容易遗忘，算是知识的固化吧。</p><h3>1.2 功能</h3><ol><li>初始化CA服务器</li><li>一步生成CA服务器私钥及证书</li><li>一步生成用户私钥及证书</li><li>分开步骤，分别为用户生成【私钥、证书请求、证书】</li><li>为第三方证书请求颁发证书</li><li>为证书续期</li><li>吊销证书</li><li>生成CA证书吊销列表</li></ol><h3>1.3 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk； <img referrerpolicy="no-referrer" src="/img/remote/1460000047436584" alt="打赏" title="打赏"/></li></ol><h2>2 软件架构</h2><p>Linux shell</p><h3>2.1 设计理念</h3><ul><li>用Openssl搭建CA服务器</li><li>信任环境：在CA服务器上为用户生成私钥与证书</li><li>非信任环境：用户自己生成私钥与证书请求，将证书请求给到CA服务器，CA服务器根据用户提供的证书请求文件为用户生成证书（私钥一般是需要保密的，把自己假想成了NB的公共证书颁发者了）</li></ul><h3>2.2 目录结构</h3><blockquote>初始化后的目录结构：</blockquote><pre><code class="bash">$ tree
.
├── 0-init_ca.sh
├── 1-generate_CA_key_and_crt.sh
├── blog-自建CA及证书颁发-old.md
├── crlnumber
├── function.sh
├── index.txt
├── key_usage.md
├── LICENSE
├── m-1-generate_user_key.sh
├── m-2-generate_user_csr.sh
├── m-3-generate_user_crt.sh
├── m-3in1-generate_user_key-csr-crt.sh
├── m-x-revoke_user_crt.sh
├── m-x-renew_user_crt.sh
├── m-x-generate_CA_crl.sh
├── my_conf
│   ├── env.sh--CA.sample
│   ├── env.sh--model
│   └── env.sh--test.lan
├── README.md
└── serial

1 directory, 17 files</code></pre><h2>3 安装教程</h2><p>克隆到服务器上即可。 需要安装Linux 软件包<code>expect</code>。 在ubuntu上测试通过，理论上只要是基于Linux内核都行</p><h2>4 使用说明</h2><p>所有脚本都提供了<code>$0 -h|--help</code>参数，查看帮助即可。</p><h3>4.1 搭建CA</h3><ol><li>运行<code>./0-init_ca.sh -y</code>进行初始化</li><li>基于<code>./my_conf/env.sh--CA.sample</code>创建<code>./my_conf/env.sh--CA</code>CA的环境变量文件</li><li>运行<code>1-generate_CA_key_and_crt.sh -y</code>以生成CA服务器私钥与自签名证书</li></ol><blockquote>以上根据自己的信息填写即可</blockquote><h3>4.2 日常使用（为用户生成私钥、证书请求、证书）</h3><blockquote>运行脚本前，请先查看帮助，帮助中有相关脚本的依赖文件、参数说明及示例！ 多数脚本须依赖基于<code>./my_conf/env.sh--model</code>创建的<code>./my_conf/env.sh--证书相关名称</code>环境变量文件，仓库中提供了一个示例（test.lan）<code>./my_conf/env.sh--test.lan</code>供参考。</blockquote><h4>4.2.1 一步为用户生成私钥、证书请求、证书</h4><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;4(证书相关名称.key)
1--&gt;2(env.sh--证书相关名称)
2--&gt;3(openssl.cnf--证书相关名称)
3--&gt;5(证书相关名称.csr)
4--&gt;5
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-3in1-generate_user_key-csr-crt.sh -h

    用途：用于生成用户秘钥与证书
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建
    注意：
    用法:
        ./m-3in1-generate_user_key-csr-crt.sh  [-h|--help]
        ./m-3in1-generate_user_key-csr-crt.sh  [-n|--name {证书名称}]  &lt;-p|--privatekey-bits {私钥长度}&gt;  &lt;-c|--cert-bits {证书长度}&gt;  &lt;-d|--days {证书有效天数}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -p|--privatekey-bits  私钥长度，默认2048
        -c|--cert-bits 证书长度，默认2048
        -d|--days      证书有效期，默认365天
        -q|--quiet     静默方式运行
    示例:
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com
        #
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -d 730
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -p 4096
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -p 4096  -c 2048  -d 730
        ./m-3in1-generate_user_key-csr-crt.sh  -n test.com  -q</code></pre><h4>4.2.2 分步骤为用户生成私钥、证书请求、证书</h4><ol><li>生成私钥：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
1(证书相关名称)
1--&gt;4(证书相关名称.key)</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-1-generate_user_key.sh -h

    用途：用于生成用户秘钥
    依赖：
        ./function.sh
    注意：
    用法:
        ./m-1-generate_user_key.sh  [-h|--help]
        ./m-1-generate_user_key.sh  [-n|--name {证书相关名称}]  &lt;-p|--privatekey-bits {私钥长度}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -p|--privatekey-bits  私钥长度，默认2048
        -q|--quiet     静默方式运行
    示例:
        ./m-1-generate_user_key.sh  -n test.com
        ./m-1-generate_user_key.sh  -p 4096  -n test.com
        ./m-1-generate_user_key.sh  -q  -n test.com</code></pre><ol><li>生成证书请求：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
1(证书相关名称)
1--&gt;2(env.sh--证书相关名称)
2--&gt;3(openssl.cnf--证书相关名称)
3--&gt;5(证书相关名称.csr)
1--&gt;4(证书相关名称.key)
4--&gt;5</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-2-generate_user_csr.sh -h

    用途：用于生成用户证书请求
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建
    注意：
    用法:
        ./m-2-generate_user_csr.sh  [-h|--help]
        ./m-2-generate_user_csr.sh  [-n|--name {证书相关名称}]  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -q|--quiet     静默方式运行
    示例:
        ./m-2-generate_user_csr.sh  -n test.com
        ./m-2-generate_user_csr.sh  -n test.com -q</code></pre><ol><li>颁发证书（证书第一次颁发、证书续期重新颁发）：</li></ol><blockquote>程序流程图：</blockquote><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;3(openssl.cnf--证书相关名称)
1--&gt;5(证书相关名称.csr)
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><p>或者：</p><pre style="display:none;"><code class="mermaid">graph LR;
0(CA私钥)
1(证书相关名称)
1--&gt;3(openssl.cnf--证书相关名称)
5(来自外部.csr)--&gt;3
5--&gt;6(证书相关名称.crt)
3--&gt;6
0--&gt;6</code></pre><blockquote>帮助：</blockquote><pre><code class="bash">$ ./m-3-generate_user_crt.sh -h

    用途：用于颁发或更新用户证书
    依赖：
        ./function.sh
        ./my_conf/env.sh--${NAME}      #--- 此文件须自行基于【./my_conf/env.sh--model】创建，当使用外部证书请求文件时，无须此配置文件
    注意：
    用法:
        ./m-3-generate_user_crt.sh  [-h|--help]
        ./m-3-generate_user_crt.sh  [-n|--name {证书相关名称}]  &lt;-c|--cert-bits {证书长度}&gt;  &lt;-d|--days {证书有效天数}&gt;  &lt;-f|--csr-file {证书请求文件}&gt;  &lt;-q|--quiet&gt;
    参数说明：
        $0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -n|--name      指定名称，用以确定用户证书相关名称前缀及env、cnf文件名称后缀。
                       即：【私钥、证书请求、证书】的文件名称前缀：test.com.key、test.com.csr、test.com.crt
                           【环境变量、配置】文件名的后缀：env.sh--test.com、openssl.cnf--test.com
        -f|--csr-file  指定外部用户证书请求文件。一般只有在用户使用其他工具生成证书请求时使用此项
        -c|--cert-bits 证书长度，默认2048
        -d|--days      证书有效期，默认365天
        -q|--quiet     静默方式运行
    示例:
        ./m-3-generate_user_crt.sh  -n test.com
        #
        ./m-3-generate_user_crt.sh  -c 4096  -n test.com
        ./m-3-generate_user_crt.sh  -d 730   -n test.com
        ./m-3-generate_user_crt.sh  -c 4096  -d 730  -n test.com
        # 第三方证书请求
        ./m-3-generate_user_crt.sh  -f /path/to/xxx.csr  -n xxxxx
        ./m-3-generate_user_crt.sh  -c 4096  -d 730  -f /path/to/xxx.csr  -n xxxxx</code></pre><blockquote>如果曾经颁发的证书过期了，只需再次运行<code>m-3-generate_user_crt.sh</code>就可以了，为了便于用户理解，增加了个软连接名称<code>m-x-renew_user_crt.sh</code>。</blockquote><h3>4.3 其他使用</h3><h4>4.3.1 更新（renew）用户证书</h4><p>等同【4.2.2 - 3】为用户生成证书，请参考</p><h4>4.3.2 吊销（revoke）用户证书</h4><pre><code class="bash">./m-x-revoke_user_crt.sh</code></pre><h4>4.3.3 吊销（revoke）用户证书</h4><pre><code class="bash">./m-x-generate_CA_crl.sh</code></pre><h2>5 最后</h2><p>Enjoy！</p>]]></description></item><item>    <title><![CDATA[K-Mind 行业数智大脑：破解企业 A]]></title>    <link>https://segmentfault.com/a/1190000047437305</link>    <guid>https://segmentfault.com/a/1190000047437305</guid>    <pubDate>2025-11-28 21:02:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>大模型时代的机遇与困境：高成本与低落地率的矛盾</strong></h2><p>在人工智能技术飞速发展的当下，大模型已成为推动产业变革的核心力量，但其高昂的研发成本与落地挑战却让众多企业望而却步。除了资金投入，模型训练还面临数据质量要求高、模态结构复杂、算力消耗大、耗时久等多重难题，这些都成为企业入局大模型的 "高门槛"。</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdncLS" alt="" title=""/></p><p>2016-2023 年部分人工智能模型训练成本估算（图片来源于网络）</p><p>成本压力之下，企业 AI 工程化落地率同样不容乐观。麦肯锡调研数据显示，仅有 1% 的受访企业认为他们的 AI 投资已经达到成熟阶段。深入分析可见，企业在大模型落地过程中面临三重核心挑战：技术层面，模型微调需海量高质量数据，部署时集成兼容难度大，GPU 等异构资源调度效率低；应用层面，通用大模型 "幻觉" 控制难、行业知识薄弱，且评估体系复杂、安全要求高；业务层面，运维可靠性与稳定性难以保障，维护成本居高不下，难以匹配企业个性化业务需求。</p><h2><strong>破局之道：从通用大模型到行业专属模型的转型</strong></h2><p>面对企业 AI 落地困境，浪潮开务提出核心解决方案------推动大模型从 "通用" 向 "行业专属" 演进。通过对比可见，通用大模型虽覆盖范围广，但存在资金与研发投入大、行业适配性弱的劣势；行业垂域大模型依托专用行业引擎，能以更低成本实现行业适配；而企业专属大模型则可贴合企业自身特色，借助客户私有数据实现更敏捷、更快速的落地。</p><p><img width="723" height="355" referrerpolicy="no-referrer" src="/img/bVdncLT" alt="" title="" loading="lazy"/></p><p>通用大模型与行业垂域大模型、企业专属大模型优劣对比</p><p>这一转型的关键在于构建 "小算力 + 海量领域高质量数据" 的模式。浪潮KaiwuDB 以 KaiwuDB 3.0 为数据底座，结合多模态数据治理与合规能力，将分散的设备数据、业务数据、外部接口数据转化为 "AI-ready Data"，为模型训练与微调提供高质量数据源。同时，通过强化模型运营能力、优化资源调度策略、完善监控维护体系，不仅降低了整个模型系统的成本，更确保模型在动态环境中始终保持高效稳定，解决了企业 "如何获取管理训练数据"、"如何应对访问量波动"、"如何快速部署推理服务" 等核心疑问。</p><h2><strong>K-Mind：物联网行业大脑的全链路赋能</strong></h2><h4>K-Mind</h4><p>浪潮开务物联网行业数智大脑(K-Mind)是由时序、语言、视觉、图学习、科学计算、决策优化等六大核心基础模型协同工作的物联网行业大模型有机体。K-Mind 可为上层应用提供"感知-认知-决策-优化"的全链路 AI 能力,最终助力客户在物联网场景,如能源、水利、矿山等关键领域快速、低成本地实现AI工程化落地，并为业务的重构与智能化提供高效赋能。</p><p><img width="723" height="380" referrerpolicy="no-referrer" src="/img/bVdncLU" alt="" title="" loading="lazy"/></p><p>K-Mind 行业数智大脑架构图</p><p>浪潮KaiwuDB 依托 KaiwuDB 与浪潮开务物联网行业数智大脑 K-Mind，以 "数据洞见未来" 为核心，为企业 AI 工程化落地提供了全链路解决方案，助力能源、水利、矿山等关键领域迈入全域智能新时代。</p><p>以油气行业为例，K-Mind 油气行业大脑可接入石油实时产量数据、设备数据、水电气数据、人员数据等多源数据，经过数据清洗、聚合、实时计算等处理环节，结合油田生产经验数据与行业知识，实现产油能效评测、生产异常告警、产量预测等核心应用。</p><p><img width="723" height="281" referrerpolicy="no-referrer" src="/img/bVdncLV" alt="" title="" loading="lazy"/><br/>油田 K-Mind 架构图</p><p>从技术架构来看，K-Mind 采用分层设计，底层依托 KaiwuDB 分布式多模数据库与数据湖仓，实现多模态数据的高效存储与计算；中间层通过多模态数据治理与合规、语义层构建，将原始数据转化为 AI-ready 数据，为模型训练提供高质量数据支撑；上层则基于行业知识库、行业算法和模型库，构建行业智能体，通过 API/SDK 为应用层提供服务。</p><p><img width="723" height="367" referrerpolicy="no-referrer" src="/img/bVdncLW" alt="" title="" loading="lazy"/></p><p>以 K-Mind 为核心的基建平台链路图</p><p>在能源行业，K-Mind 能源业务大脑能够支撑虚拟电厂（VPP）的运行，通过电价、负荷、出力预测，辅助交易决策，实现调度优化与故障诊断。</p><p><img width="723" height="303" referrerpolicy="no-referrer" src="/img/bVdncLX" alt="" title="" loading="lazy"/><br/>能源行业链路示意图</p><h2><strong>快、省、准、深------迈向全域智能的 "高速路"</strong></h2><p>K-Mind 物联网行业大脑之所以能够高效赋能企业 AI 工程化落地，源于其四大核心优势。其一，深厚的行业积累，浪潮在制造、交通、能源等物联网领域沉淀了丰富经验，能精准匹配行业需求；其二，完善的交付服务体系，从数据治理到模型部署，提供全流程支持；其三，强大的生态构建能力，内部可提供多元化产品满足不同用户需求，外部通过 API/SDK 实现灵活集成；其四，严格的安全保障，确保企业数据与业务运营的合规性与安全性。</p><p>K-Mind 正以 "<strong>快、省、准、深</strong>" 为核心，构建企业 AI 工程化落地的 "高速路"------"快" 即模型训练快、业务落地快，大幅缩短数智化转型周期；"省" 即省资源、省人力，通过高效资源调度与自动化运维降低成本；"准" 即行业适配准、经验沉淀准、模型预测准、业务结果准，贴合企业实际需求；"深" 即纵向深耕电力、石油、天然气、水务、矿山、冶金等领域，横向拓展辅助交易决策、故障诊断、智能运维等场景，实现全产业、全链路的智能升级。</p><p>目前，浪潮开务物联网行业数智大脑 K-Mind 已正式发布，企业可通过<strong>扫描下方二维码填写问卷</strong>获取并体验最新技术内容，抢先体验数智化转型的核心动力。未来，浪潮KaiwuDB 将持续以数据为基、以 AI 为翼，与企业携手共赴物联网全域智能的未来，让数据的价值在更多场景中绽放，推动产业高质量发展。</p><p><img width="280" height="280" referrerpolicy="no-referrer" src="/img/bVdncLY" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[《iOS相机/定位的精准适配指南》 程序]]></title>    <link>https://segmentfault.com/a/1190000047437324</link>    <guid>https://segmentfault.com/a/1190000047437324</guid>    <pubDate>2025-11-28 21:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数开发者在推进功能开发时，往往将重心放在镜头捕捉的流畅度、定位数据的实时性等显性指标上，却忽视了权限声明作为系统与应用达成交互共识的底层逻辑—它绝非简单的文本补充，而是应用融入iOS生态的前置承诺，是用户与应用建立信任关系的初始触点。曾在优化一款场景化服务类功能时，投入大量精力打磨定位与影像的协同体验，确保在不同光线、地形条件下的数据稳定性，却在实际验证过程中发现，部分场景下功能虽能启动却无法获取核心数据，既无系统提示，也无异常反馈，这种看似“功能正常却无效”的现象，起初让人陷入困惑，反复排查后才意识到，问题的根源在于权限声明未能精准传递功能的核心意图，导致系统在后台校验时未能建立有效的信任链路。这种经历让我深刻体会到，权限声明的配置绝非技术流程中的次要环节，而是对iOS权限机制设计哲学的深度理解，它要求开发者跳出功能实现的单一维度，站在系统安全架构、用户隐私诉求、生态交互规则的多重角度，重新审视每一处配置的深层意义。</p><p>权限声明的本质，是应用向系统与用户传递行为意图的语义载体，而相机与定位权限的特殊性，在于其直接关联设备硬件的调用权限与用户的核心隐私边界。iOS系统对这类权限的管控逻辑，早已超越了“允许”或“拒绝”的二元判断，而是构建了一套基于场景感知、意图验证、信任累积的动态评估体系。Info.plist中的声明文本，每一个表述都承载着系统对应用行为的预判依据，也影响着用户对权限使用合理性的判断标准，它需要在功能需求与用户认知之间找到精准的平衡点，既不能过于抽象导致系统无法识别场景，也不能过于繁琐引发用户的授权抵触。例如，同样是相机权限的声明，用于证件扫描功能时，需要突出“信息采集的准确性与安全性”，让用户明确权限使用的核心目的是为了快速提取有效信息；用于风景拍摄功能时，则应强调“场景记录的完整性与个性化”，契合用户对生活内容留存的需求；而用于AR互动功能时，需聚焦“虚实融合的沉浸式体验”，让用户理解权限调用对功能实现的必要性。这种细微的语义差异，不仅决定了系统在权限校验时的判定结果，更影响着用户在授权弹窗出现时的心理决策—当声明文本能够精准呼应用户的使用预期，用户的授权意愿会显著提升，反之则可能引发抵触情绪，甚至直接影响对应用的整体信任度。在长期的实践中发现，权限声明的语义精准度，往往与功能的实际使用效果形成隐性关联，那些能够清晰、真诚传递行为意图的声明，不仅能减少系统的隐性拦截，更能让用户在使用过程中感受到被尊重，从而建立起对应用的长期信任。</p><p>深入探究iOS的权限机制设计，会发现Info.plist中的声明配置，实则是系统权限管控链路的起点，它与应用的功能模块设计、用户交互流程、隐私保护策略形成了环环相扣的生态闭环。系统在处理相机或定位权限请求时，并非仅简单校验声明是否存在，而是会结合声明文本的语义指向、功能调用的时机与场景、用户的历史授权行为、应用的整体口碑等多维度信息，进行综合信任评估。这种评估机制的底层逻辑，是iOS对用户隐私保护的极致践行，也是对应用开发规范的刚性约束，它要求应用的每一次权限调用都具备合理的场景支撑与明确的意图说明。在实践中曾遇到过这样的情况：一款应用的相机功能在测试环境中运行稳定，所有权限声明均已按常规配置，但在用户反馈中却出现部分设备无法正常调用的问题，且这类问题集中在特定iOS版本与机型的组合中。经过多轮排查与测试，最终发现问题的根源在于声明文本中存在模糊表述，导致系统在特定版本的权限评估逻辑中，无法将声明意图与实际功能场景精准匹配，从而触发了隐性的权限拦截。这一经历让我深刻认识到，权限声明的配置并非一劳永逸的静态操作，而是需要随着系统版本的迭代、功能场景的拓展、用户需求的变化进行动态优化。开发者需要持续关注iOS系统的更新日志，深入理解每一次权限机制调整的底层逻辑，同时结合应用的实际使用数据，分析用户授权行为的变化趋势，不断优化声明文本的表述方式，确保其始终与系统的评估逻辑同频，与用户的认知预期同步。</p><p>权限声明的优化过程，本质上是开发者对功能场景与用户需求的深度解构与重构，它要求开发者跳出技术实现的思维定式，站在用户的视角审视每一处表述的合理性与真诚度。在进行相机权限声明的优化时，不能简单套用“需要访问相机”这类泛化表述，而应深入挖掘功能的核心价值与用户的真实使用场景—用于文档扫描功能时，需明确“为快速提取文档信息，提升办公效率”；用于美食拍摄功能时，可强调“为记录食材细节，生成个性化烹饪指南”；用于社交分享功能时，则应突出“为捕捉生活瞬间，实现好友间的情感传递”。每一种场景对应的声明文本，都应具备独特的语义指向，让用户在看到授权弹窗的瞬间，就能清晰理解权限使用的必要性与价值所在。定位权限的声明优化更是如此，不同的功能场景对定位精度的要求不同，声明文本也应随之调整：用于本地生活服务推荐时，需说明“为匹配周边优质资源，提供精准的生活建议”；用于运动轨迹记录时，应强调“为完整呈现运动数据，助力科学健身规划”；用于旅行导航功能时，则需明确“为实时规划最优路线，提升出行的便捷性”。这种基于场景的精准声明，不仅能有效降低用户的授权顾虑，提升授权转化率，更能让系统在权限管控过程中，准确把握应用的行为边界，从而减少不必要的拦截与限制。同时，声明文本的表述风格也应贴近用户的日常语言习惯，避免使用过于专业的技术术语，以真诚、简洁的方式传递核心信息，让用户感受到开发者对其隐私的重视与对使用体验的用心。</p><p>在权限声明的实践过程中，系统版本迭代带来的隐性变化是开发者必须关注的核心变量，这种变化往往体现在权限评估逻辑的细微调整上，需要通过持续的测试与总结，捕捉其中的规律与趋势。不同iOS版本对权限声明的语义解析能力、场景匹配精度、用户交互反馈都存在差异，某些在旧版本中能够正常使用的声明文本，在新版本中可能会因为语义模糊、意图不明确而被系统判定为不合理，从而影响功能的正常使用。因此，在完成权限声明配置后，开发者不能仅在单一版本中进行测试，而应覆盖多个主流版本，甚至包括测试版与预览版，通过模拟不同用户的授权行为、功能调用场景、设备使用环境，全面验证声明文本的兼容性与有效性。同时，还应建立完善的用户反馈收集机制，通过应用内反馈渠道、社区讨论、数据分析等多种方式，挖掘与权限声明相关的潜在问题—比如某些用户频繁拒绝授权，可能是因为声明文本未能清晰传递功能价值；某些功能在特定机型上出现调用异常，可能是因为声明文本与该机型的系统权限逻辑存在冲突；某些用户授权后仍无法正常使用功能，可能是因为声明文本与实际功能场景存在偏差。针对这些问题，需要进行针对性的优化调整，不断迭代声明文本的表述方式，确保其始终符合系统要求与用户预期。此外，还可以借鉴行业内的优秀实践，分析同类应用在权限声明上的表述方式与优化路径，但并非简单模仿，而是结合自身应用的功能特性与用户群体，形成具有独特性与适配性的声明方案，让权限声明成为应用差异化竞争的隐性优势。</p><p>权限声明的深层价值，早已超越了单纯的技术配置要求，成为应用生态适配、用户信任构建、品牌口碑沉淀的核心环节。在iOS生态日益强调隐私保护与用户体验的当下，权限声明不再是可有可无的辅助配置，而是应用能否获得系统认可、用户青睐的重要前提。一个精准、清晰、真诚的权限声明，能够让应用在系统权限管控的框架内，最大限度地发挥功能价值，同时也能让用户在使用过程中，感受到开发者对隐私保护的重视与对用户体验的尊重。这种尊重与重视，最终会转化为用户对应用的信任与依赖，成为应用核心竞争力的重要组成部分。在长期的开发实践中，我逐渐意识到，权限声明的优化过程，也是开发者技术认知不断深化、思维模式不断升级的过程，它要求我们跳出代码与功能的表层，站在生态、用户、系统的多重角度，审视每一个开发细节，培养一种全面、系统、动态的思考方式。这种思考方式不仅能解决权限声明相关的具体问题，更能迁移到应用开发的其他环节，帮助我们更好地应对复杂的技术挑战，构建出更符合生态规则、更贴近用户需求、更具核心竞争力的应用产品。</p>]]></description></item><item>    <title><![CDATA[《音频格式优化的底层逻辑：场景拆解与解码]]></title>    <link>https://segmentfault.com/a/1190000047437327</link>    <guid>https://segmentfault.com/a/1190000047437327</guid>    <pubDate>2025-11-28 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数开发者聚焦于播放控制逻辑、音效算法迭代等显性模块，却忽略了格式未优化引发的隐性体验损耗—它并非仅体现为文件体积冗余，而是在设备差异、网络波动、使用场景切换中触发的感知断层。曾在一次多场景音频适配测试中观察到，同一段音频在旗舰机型上呈现出清晰通透的听感，在中端设备上却伴随若隐若现的底噪与播放迟滞，而在弱网环境下加载进度条的缓慢蠕动更让体验断崖式下跌。起初将问题归咎于设备硬件规格或网络带宽限制，经过多轮交叉测试、排除硬件性能瓶颈与网络环境干扰后，才发现核心症结在于音频格式未进行全链路适配优化，编码方案与设备解码逻辑、网络传输特性、场景使用需求形成了隐性错配。这种经历让我深刻体悟到，音频格式优化绝非简单的格式转换或参数调整，而是对编码标准特性、设备硬件能力、用户使用场景三者协同关系的深度解构与重构，它要求开发者跳出“能播放即合格”的表层认知，站在生态兼容与感知体验的双重维度，重新定义优化的核心价值与实践路径。</p><p>音频格式未优化的本质，是编码逻辑与使用场景、设备能力的三重适配失衡，这种失衡往往被“正常播放”的表象所掩盖，通过细微却关键的体验差异影响用户感知。不同音频格式的编码架构、压缩算法、资源占用特性存在显著差异：部分格式以极致压缩比为核心优势，却需消耗更多设备算力进行解码，在性能有限的设备上易引发卡顿；部分格式追求无损音质呈现，却导致文件体积激增，在弱网环境下加载耗时过长，甚至触发加载失败；部分格式兼容性覆盖广泛，却在特定硬件解码芯片上出现适配损耗，导致音质畸变或播放中断。例如，面向专业音乐制作的无损格式，若直接应用于移动端即时语音通讯场景，会因解码延迟累积引发语音同步偏差，破坏实时交互体验；而针对短视频流媒体设计的高压缩格式，用于本地高清音频播放时，会出现高频细节丢失、声场变窄等音质短板，难以满足用户对听感品质的需求。在长期的实践观察中发现，格式未优化带来的问题具有极强的场景依赖性与设备关联性，它不会在单一测试环境中集中暴露，却在用户真实使用的复杂场景中频繁爆发—比如户外移动场景下的弱网加载超时、后台持续播放时的电量快速消耗、多音频连续切换时的瞬时静音、老旧设备上的播放卡顿与音画不同步。这些问题的核心症结，在于开发者未能将音频格式的编码逻辑与使用场景的核心需求、设备的硬件解码能力进行精准匹配，导致格式成为制约体验升维的隐性瓶颈，即便其他功能模块打磨得再完善，也难以实现整体体验的闭环。</p><p>深入探究音频格式的底层逻辑会发现，其优化的核心在于对“编码效率、解码兼容、场景需求”三者动态平衡的精准把握。每一种音频格式的诞生都承载着特定的设计初衷：早期格式多为适配存储设备的容量限制，以高压缩比为核心目标；专业领域的格式聚焦音质无损呈现，牺牲部分存储与解码效率；移动生态的格式则侧重低功耗、快解码特性，适配移动端设备的硬件资源与使用场景。这意味着，音频格式优化并非寻找某一种“万能最优格式”，而是根据具体场景的核心诉求，选择最适配的编码方案与参数组合。在实践中曾遇到这样的典型案例：一款音频类应用上线后，收到大量老旧机型用户反馈播放时音画不同步，经过深度排查发现，应用采用的音频格式解码复杂度较高，而老旧机型的硬件解码芯片算力有限，无法及时处理编码数据，导致解码延迟持续累积，最终引发音画偏差。通过深入研究该格式的编码架构，调整关键压缩参数，在保证可感知音质不受影响的前提下，降低解码时的算力消耗，同时优化音频数据的封装方式，使其更适配老旧设备的解码逻辑，最终成功解决了这一问题。这一经历让我深刻认识到，音频格式优化需要开发者对不同编码标准的底层逻辑有深入理解—比如有损编码的心理声学模型差异、无损编码的熵编码算法特点、自适应编码的动态码率调整机制，只有精准掌握这些核心信息，才能在复杂的场景与设备差异中做出最优选择，实现编码效率、解码兼容与场景需求的动态平衡。</p><p>音频格式优化的实践路径，始于对使用场景的深度拆解，再落地为编码方案的精准选型与参数的精细化调校。场景拆解的核心在于明确三个关键维度：使用环境（网络状态、噪声环境）、设备类型（硬件性能、解码芯片、系统版本）、核心需求（音质优先、流畅优先、低功耗优先、存储友好）。例如，面向离线本地播放的高清音频场景，核心需求是音质还原与存储平衡，可优先选择无损格式或高码率有损格式，在保留音频细节的同时，通过合理的压缩算法控制文件体积；面向移动端在线播放的场景，核心需求是弱网适配与流畅播放，需侧重选择高压缩比、低解码损耗的格式，确保在网络带宽有限的情况下快速加载，同时降低解码时的设备资源占用；面向即时通讯的语音场景，核心需求是低延迟与实时交互，需优先选择低复杂度、快解码的格式，避免解码延迟影响语音同步，同时控制文件体积以提升传输效率；面向户外运动场景的音频应用，核心需求是低功耗与抗噪声干扰，需选择解码功耗低、中高频表现力强的格式，兼顾续航与听感清晰度。在编码方案选型之后，参数调校是实现体验升维的关键环节：调整压缩比时，需通过大量听感测试找到“音质损耗阈值”与“文件体积优化”的平衡点，既不能为追求压缩率而牺牲关键音质细节，也不能因过度追求音质而导致文件体积失控；调整采样率与位深时，需结合设备的音频输出能力与用户的实际听感需求，过高的采样率若超出设备播放极限与人类听觉范围，只会造成资源浪费；调整码率分配策略时，需根据音频内容的特性动态分配码率，对人声、乐器等关键信息分配更高码率，对背景噪声等非关键信息适当降低码率，实现资源的高效利用。在实践过程中，往往需要构建多维度的测试矩阵，在不同价位的设备、不同网络带宽、不同噪声环境下进行反复测试，对比音质表现、加载速度、功耗消耗等关键指标，最终形成适配目标场景的最优参数方案。</p><p>优化过程中，设备兼容性与生态适配是容易被忽视却至关重要的环节，不同设备的硬件解码芯片、系统音频框架、驱动程序对音频格式的支持程度存在显著差异，这种差异直接影响格式优化的实际效果。部分在主流旗舰机型上表现优异的格式，在小众机型或老旧设备上可能出现解码失败、音质畸变、播放卡顿等问题；而某些新推出的高效编码格式，可能因系统版本不支持或硬件解码芯片未适配，无法发挥其优势，甚至出现兼容性故障。因此，在进行音频格式优化时，不能仅针对单一设备或系统版本进行测试，而应构建全面的兼容性测试矩阵，覆盖高中低端不同价位的机型、主流与小众品牌设备、新旧不同系统版本，确保优化方案在各类设备上都能稳定运行。同时，还需深入了解不同平台的音频生态规则与调度机制：例如，部分系统对特定音频格式的解码资源调度优先级更高，部分平台对音频文件的封装格式有特殊要求，部分生态对后台播放时的音频格式功耗控制有明确标准。这些生态规则直接影响音频格式的实际表现，若忽视这些细节，即便编码方案再优化，也可能出现体验问题。在实践中曾遇到这样的案例：一款应用采用了某新型高效音频格式，在多数安卓设备上表现良好，但在部分品牌的设备上出现后台播放时卡顿的问题，排查后发现，该品牌系统对该格式的后台解码资源调度优先级较低，当设备资源紧张时，音频解码会被抢占资源，导致播放卡顿。通过调整音频文件的封装方式，适配该系统的资源调度逻辑，同时优化解码初始化流程，最终解决了这一兼容性问题。这一经历让我深刻认识到，音频格式优化不仅是技术层面的编码调整，更是对整个音频生态规则、设备硬件特性的深度适配，只有实现技术与生态的协同，才能确保优化效果的落地。</p><p>音频格式优化的深层价值，早已超越了单一功能的体验提升，成为应用核心竞争力的隐性组成部分。在用户对多媒体体验要求日益严苛的当下，流畅的播放体验、清晰的音质表现、高效的资源占用、持久的续航能力，这些看似基础的需求，恰恰是区分优秀应用与普通应用的关键维度。一款能够根据场景智能适配音频格式的应用，不仅能在不同环境下保持稳定一致的体验表现，更能让用户感受到开发者对细节的极致追求与对用户需求的深度洞察，这种感知会转化为用户对应用的信任与依赖，成为应用长期发展的核心资产。在长期的开发实践中，我逐渐意识到，音频格式优化的过程，也是开发者技术认知不断深化、思维模式不断升级的过程—它要求我们跳出技术本身的局限，站在用户体验、设备特性、生态规则的多重角度进行系统性思考，培养“场景化思维”与“全链路思维”。这种思维模式的提升，不仅能解决音频格式优化的具体问题，更能迁移到多媒体开发的其他环节，比如视频格式适配、音视频同步优化、多媒体资源加载策略等，帮助我们更好地应对复杂的技术挑战。而对于开发者而言，这种对隐性细节的深耕、对底层逻辑的探究、对实践经验的沉淀，正是建立个人技术品牌、吸引同行关注的核心竞争力。</p>]]></description></item><item>    <title><![CDATA[采集华为云 CCI 日志到观测云最佳实践]]></title>    <link>https://segmentfault.com/a/1190000047437128</link>    <guid>https://segmentfault.com/a/1190000047437128</guid>    <pubDate>2025-11-28 20:03:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、背景与挑战</h2><p>华为云 CCE 提供了云原生日志采集插件，采集了包含 CCE 集群以及弹性到 CCI 的实例的容器内日志，但对观测云来讲，观测云可以基于 DataKit Operator 以及提供一个 DataKit 的 DaemonSet 部署来实现 CCE 各节点的容器内的日志文件采集，但针对于对于 CCI 的这种 serverless 的容器内日志采集，观测云采集思路包含：</p><ul><li>通过观测云的 logforward 的 sidecar 部署来实现日志转发给观测云，这种方式消耗大量的资源，并且要对原有的 CCE 的 Deployment 进行改造注入。</li><li>使用 lambda 函数将 LTS 采集的 OBS 的日志上报到观测云，因 CCE 的同一 Deployment 弹性到 CCI，这种方式基于 OBS 区分不出哪些是 CCI 的日志，哪些是 CCE 的日志。</li><li>华为云 CCE 云原生日志采集插件中包含了 Otel Collector 组件，通过改造 Otel Collector 的 exporter 配置实现 CCI 日志的导出，这种方式减少了日志接入的成本，避免了资源额外消耗的成本，即本篇重点阐述的最佳实践。</li></ul><h2>二、前置条件</h2><ul><li>DataKit：观测云的采集组件，负责 CCE 日志采集与接收 Otel Collector 的 CCI 日志收集导出。</li><li>观测云：统一日志检索、查询分析、仪表盘展示、智能告警等。</li><li>云原生日志采集插件：负责 CCE 日志和 CCI 日志的采集，插件版本要求 1.5.1 版本以上，插件说明如下。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437130" alt="图片" title="图片"/></p><ul><li>业务场景环境：华为 CCE 调度到 CCI 场景。</li></ul><h2>三、采集流程</h2><p>华为云 CCE 集群容器内日志通过观测云标准方案 <a href="https://link.segmentfault.com/?enc=TE2LDdmNblvnqoC38ed9KQ%3D%3D.C6oVuzAYGVO0uFJ3WGZw86dVL%2FQYTvzVSyBnTqjC3QEbWom%2FOjR4tFL5IHqXDhc5WQjakIZ63oG530AKk5QF%2Bw%3D%3D" rel="nofollow" target="_blank">DataKit Operator</a> 的方式采集，而弹性到 CCI 的日志通过云原生插件采集 Otel Collector 并导出到观测云 DataKit 服务，最终展示在观测云控制台，如下流程图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437131" alt="图片" title="图片" loading="lazy"/></p><h2>四、配置步骤</h2><h3>步骤 1：CCE 集群弹性到 CCI Demo 搭建</h3><ul><li>请自行创建 CCE 集群，并创建应用，测试可强制调度到 CCI，如下图：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437132" alt="图片" title="图片" loading="lazy"/></p><p>sp-demo2.yaml</p><pre><code>kind: Deployment

apiVersion: apps/v1

metadata:

  name: sp-demo2

  namespace: default

  uid: 403dd3e0-8591-44d8-bd7f-0c8585acb26d

  resourceVersion: '295573'

  generation: 1

  creationTimestamp: '2025-09-12T12:15:48Z'

  labels:

    appgroup: ''

    version: v1

    virtual-kubelet.io/burst-to-cci: enforce

  annotations:

    deployment.kubernetes.io/revision: '1'

    description: ''

    kubectl.kubernetes.io/last-applied-configuration: &gt;

      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"5","description":"","workload.cce.io/swr-version":"[{\"version\":\"Private

      Edition\"}]"},"labels":{"appgroup":"","version":"v1","virtual-kubelet.io/burst-to-cci":"enforce"},"name":"sp-demo2","namespace":"default"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"sp-demo2","version":"v1"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"labels":{"app":"sp-demo2","version":"v1"}},"spec":{"containers":[{"env":[{"name":"PAAS_APP_NAME","value":"sp-demo2"},{"name":"PAAS_NAMESPACE","value":"default"},{"name":"PAAS_PROJECT_ID","value":"bacc65fb662f435dab3acda49acae0c9"}],"image":"swr.cn-north-4.myhuaweicloud.com/liurui_bj/springboot-server:openj8","imagePullPolicy":"IfNotPresent","name":"container-1","resources":{"limits":{"cpu":"250m","memory":"512Mi"},"requests":{"cpu":"250m","memory":"512Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","imagePullSecrets":[{"name":"default-secret"}],"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":300},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":300}]}}}}

    workload.cce.io/swr-version: '[{"version":"Private Edition"}]'

  managedFields:

    - manager: kubectl-client-side-apply

      operation: Update

      apiVersion: apps/v1

      time: '2025-09-12T12:15:48Z'

      fieldsType: FieldsV1

      fieldsV1:

        f:metadata:

          f:annotations:

            .: {}

            f:description: {}

            f:kubectl.kubernetes.io/last-applied-configuration: {}

            f:workload.cce.io/swr-version: {}

          f:labels:

            .: {}

            f:appgroup: {}

            f:version: {}

            f:virtual-kubelet.io/burst-to-cci: {}

        f:spec:

          f:progressDeadlineSeconds: {}

          f:replicas: {}

          f:revisionHistoryLimit: {}

          f:selector: {}

          f:strategy:

            f:rollingUpdate:

              .: {}

              f:maxSurge: {}

              f:maxUnavailable: {}

            f:type: {}

          f:template:

            f:metadata:

              f:labels:

                .: {}

                f:app: {}

                f:version: {}

            f:spec:

              f:containers:

                k:{"name":"container-1"}:

                  .: {}

                  f:env:

                    .: {}

                    k:{"name":"PAAS_APP_NAME"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                    k:{"name":"PAAS_NAMESPACE"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                    k:{"name":"PAAS_PROJECT_ID"}:

                      .: {}

                      f:name: {}

                      f:value: {}

                  f:image: {}

                  f:imagePullPolicy: {}

                  f:name: {}

                  f:resources:

                    .: {}

                    f:limits:

                      .: {}

                      f:cpu: {}

                      f:memory: {}

                    f:requests:

                      .: {}

                      f:cpu: {}

                      f:memory: {}

                  f:terminationMessagePath: {}

                  f:terminationMessagePolicy: {}

              f:dnsPolicy: {}

              f:imagePullSecrets:

                .: {}

                k:{"name":"default-secret"}: {}

              f:restartPolicy: {}

              f:schedulerName: {}

              f:securityContext: {}

              f:terminationGracePeriodSeconds: {}

              f:tolerations: {}

    - manager: kube-controller-manager

      operation: Update

      apiVersion: apps/v1

      time: '2025-09-12T12:16:19Z'

      fieldsType: FieldsV1

      fieldsV1:

        f:metadata:

          f:annotations:

            f:deployment.kubernetes.io/revision: {}

        f:status:

          f:availableReplicas: {}

          f:conditions:

            .: {}

            k:{"type":"Available"}:

              .: {}

              f:lastTransitionTime: {}

              f:lastUpdateTime: {}

              f:message: {}

              f:reason: {}

              f:status: {}

              f:type: {}

            k:{"type":"Progressing"}:

              .: {}

              f:lastTransitionTime: {}

              f:lastUpdateTime: {}

              f:message: {}

              f:reason: {}

              f:status: {}

              f:type: {}

          f:observedGeneration: {}

          f:readyReplicas: {}

          f:replicas: {}

          f:updatedReplicas: {}

      subresource: status

spec:

  replicas: 1

  selector:

    matchLabels:

      app: sp-demo2

      version: v1

  template:

    metadata:

      creationTimestamp: null

      labels:

        app: sp-demo2

        version: v1

    spec:

      containers:

        - name: container-1

          image: swr.cn-north-4.myhuaweicloud.com/liurui_bj/springboot-server:openj8

          env:

            - name: PAAS_APP_NAME

              value: sp-demo2

            - name: PAAS_NAMESPACE

              value: default

            - name: PAAS_PROJECT_ID

              value: bacc65fb662f435dab3acda49acae0c9

          resources:

            limits:

              cpu: 250m

              memory: 512Mi

            requests:

              cpu: 250m

              memory: 512Mi

          terminationMessagePath: /dev/termination-log

          terminationMessagePolicy: File

          imagePullPolicy: IfNotPresent

      restartPolicy: Always

      terminationGracePeriodSeconds: 30

      dnsPolicy: ClusterFirst

      securityContext: {}

      imagePullSecrets:

        - name: default-secret

      schedulerName: default-scheduler

      tolerations:

        - key: node.kubernetes.io/not-ready

          operator: Exists

          effect: NoExecute

          tolerationSeconds: 300

        - key: node.kubernetes.io/unreachable

          operator: Exists

          effect: NoExecute

          tolerationSeconds: 300

  strategy:

    type: RollingUpdate

    rollingUpdate:

      maxUnavailable: 25%

      maxSurge: 25%

  revisionHistoryLimit: 10

  progressDeadlineSeconds: 600

status:

  observedGeneration: 1

  replicas: 1

  updatedReplicas: 1

  readyReplicas: 1

  availableReplicas: 1

  conditions:

    - type: Available

      status: 'True'

      lastUpdateTime: '2025-09-12T12:16:19Z'

      lastTransitionTime: '2025-09-12T12:16:19Z'

      reason: MinimumReplicasAvailable

      message: Deployment has minimum availability.

    - type: Progressing

      status: 'True'

      lastUpdateTime: '2025-09-12T12:16:19Z'

      lastTransitionTime: '2025-09-12T12:15:48Z'

      reason: NewReplicaSetAvailable

      message: ReplicaSet "sp-demo2-7d9cd96c44" has successfully progressed.</code></pre><ul><li>查看 CCI 节点运行的 pod ：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437133" alt="图片" title="图片" loading="lazy"/></p><ul><li>本次要采集的 CCI 容器内日志为 server.log，目录如下：</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437134" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 2：在 CCE 安装云原生日志采集插件</h3><ul><li>在 CCE 插件中心安装云原生日志采集插件，实例规格自定义配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437135" alt="图片" title="图片" loading="lazy"/></p><ul><li>在日志中心创建 CCI 日志采集策略</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437136" alt="图片" title="图片" loading="lazy"/></p><ul><li>华为云 LTS 日志采集展示</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437137" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 3：在 CCE 集群部署 DataKit</h3><ul><li>通过 kubectl apply -f datakit.yaml 命令实现在华为云 CCE 的的一个 Daemonset 部署，采集器要开启 opentelemetry 采集器，并通过亲和性设置不让 DataKit 调度到虚拟节点</li></ul><p>datakit.yaml</p><pre><code>kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: datakit
  namespace: datakit
  uid: 122c1472-03cd-4ec6-a684-0384e40b011c
  resourceVersion: '5351437'
  generation: 2
  creationTimestamp: '2025-09-16T10:45:45Z'
  labels:
    app: daemonset-datakit
  annotations:
    deprecated.daemonset.template.generation: '2'
    kubectl.kubernetes.io/last-applied-configuration: &gt;
      {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"daemonset-datakit"},"name":"datakit","namespace":"datakit"},"spec":{"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"daemonset-datakit"}},"template":{"metadata":{"labels":{"app":"daemonset-datakit"}},"spec":{"containers":[{"env":[{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"ENV_K8S_NODE_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"ENV_K8S_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"ENV_DATAWAY","value":"https://openway.guance.com?token=tkn_3a0052c9f6d3498c8ce9ca0988fd9c82"},{"name":"ENV_CLUSTER_NAME_K8S","value":"cce"},{"name":"ENV_GLOBAL_HOST_TAGS","value":"host=__datakit_hostname,host_ip=__datakit_ip"},{"name":"ENV_GLOBAL_ELECTION_TAGS","value":""},{"name":"ENV_DEFAULT_ENABLED_INPUTS","value":"statsd,dk,cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container,kubernetesprometheus,logfwdserver,opentelemetry"},{"name":"ENV_ENABLE_ELECTION","value":"enable"},{"name":"ENV_INPUT_CONTAINER_ENABLE_POD_METRIC","value":"true"},{"name":"ENV_HTTP_LISTEN","value":"0.0.0.0:9529"},{"name":"ENV_INPUT_OTEL_GRPC","value":"{\"addr\":
      \"0.0.0.0:4317\"}"},{"name":"HOST_PROC","value":"/rootfs/proc"},{"name":"HOST_SYS","value":"/rootfs/sys"},{"name":"HOST_ETC","value":"/rootfs/etc"},{"name":"HOST_VAR","value":"/rootfs/var"},{"name":"HOST_RUN","value":"/rootfs/run"},{"name":"HOST_DEV","value":"/rootfs/dev"},{"name":"HOST_ROOT","value":"/rootfs"}],"image":"swr.cn-north-4.myhuaweicloud.com/liurui_bj/datakit:1.79.0","imagePullPolicy":"IfNotPresent","name":"datakit","ports":[{"containerPort":9529,"hostPort":9529,"name":"http-port","protocol":"TCP"},{"containerPort":8125,"hostPort":8125,"name":"statsd-port","protocol":"UDP"},{"containerPort":4317,"hostPort":4317,"name":"otel-grpc-port","protocol":"TCP"},{"containerPort":9533,"hostPort":9533,"name":"logfwd-port","protocol":"TCP"}],"resources":{"limits":{"cpu":"500m","memory":"1Gi"},"requests":{"cpu":"200m","memory":"128Mi"}},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/usr/local/datakit/cache","name":"cache","readOnly":false},{"mountPath":"/rootfs","mountPropagation":"HostToContainer","name":"rootfs"},{"mountPath":"/var/run","mountPropagation":"HostToContainer","name":"run"},{"mountPath":"/sys/kernel/debug","name":"debugfs"},{"mountPath":"/var/lib/containerd/container_logs","name":"container-logs"},{"mountPath":"/usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf","name":"datakit-conf","subPath":"kubelet.conf"}],"workingDir":"/usr/local/datakit"}],"dnsPolicy":"ClusterFirstWithHostNet","hostIPC":true,"hostNetwork":true,"hostPID":true,"restartPolicy":"Always","serviceAccount":"datakit","serviceAccountName":"datakit","tolerations":[{"operator":"Exists"}],"volumes":[{"configMap":{"name":"datakit-conf"},"name":"datakit-conf"},{"hostPath":{"path":"/"},"name":"rootfs"},{"hostPath":{"path":"/var/run"},"name":"run"},{"hostPath":{"path":"/sys/kernel/debug"},"name":"debugfs"},{"hostPath":{"path":"/root/datakit_cache"},"name":"cache"},{"hostPath":{"path":"/var/lib/containerd/container_logs"},"name":"container-logs"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":1},"type":"RollingUpdate"}}}
  managedFields:
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-16T10:45:45Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:deprecated.daemonset.template.generation: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
          f:labels:
            .: {}
            f:app: {}
        f:spec:
          f:revisionHistoryLimit: {}
          f:selector: {}
          f:template:
            f:metadata:
              f:labels:
                .: {}
                f:app: {}
            f:spec:
              f:containers:
                k:{"name":"datakit"}:
                  .: {}
                  f:env:
                    .: {}
                    k:{"name":"ENV_CLUSTER_NAME_K8S"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_DATAWAY"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_DEFAULT_ENABLED_INPUTS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_ENABLE_ELECTION"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_GLOBAL_ELECTION_TAGS"}:
                      .: {}
                      f:name: {}
                    k:{"name":"ENV_GLOBAL_HOST_TAGS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_HTTP_LISTEN"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_INPUT_CONTAINER_ENABLE_POD_METRIC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_INPUT_OTEL_GRPC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ENV_K8S_NODE_IP"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"ENV_K8S_NODE_NAME"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"HOST_DEV"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_ETC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_PROC"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_ROOT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_RUN"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_SYS"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"HOST_VAR"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"POD_NAME"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:name: {}
                  f:ports:
                    .: {}
                    k:{"containerPort":4317,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":8125,"protocol":"UDP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":9529,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                    k:{"containerPort":9533,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:hostPort: {}
                      f:name: {}
                      f:protocol: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:securityContext:
                    .: {}
                    f:privileged: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/rootfs"}:
                      .: {}
                      f:mountPath: {}
                      f:mountPropagation: {}
                      f:name: {}
                    k:{"mountPath":"/sys/kernel/debug"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/usr/local/datakit/cache"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:subPath: {}
                    k:{"mountPath":"/var/lib/containerd/container_logs"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                    k:{"mountPath":"/var/run"}:
                      .: {}
                      f:mountPath: {}
                      f:mountPropagation: {}
                      f:name: {}
                  f:workingDir: {}
              f:dnsPolicy: {}
              f:hostIPC: {}
              f:hostNetwork: {}
              f:hostPID: {}
              f:restartPolicy: {}
              f:schedulerName: {}
              f:securityContext: {}
              f:serviceAccount: {}
              f:serviceAccountName: {}
              f:terminationGracePeriodSeconds: {}
              f:tolerations: {}
              f:volumes:
                .: {}
                k:{"name":"cache"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"container-logs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"datakit-conf"}:
                  .: {}
                  f:configMap:
                    .: {}
                    f:defaultMode: {}
                    f:name: {}
                  f:name: {}
                k:{"name":"debugfs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"rootfs"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"run"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
          f:updateStrategy:
            f:rollingUpdate:
              .: {}
              f:maxSurge: {}
              f:maxUnavailable: {}
            f:type: {}
    - manager: cfe-apiserver
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-19T06:28:11Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          f:template:
            f:spec:
              f:affinity:
                .: {}
                f:nodeAffinity:
                  .: {}
                  f:requiredDuringSchedulingIgnoredDuringExecution: {}
    - manager: kube-controller-manager
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-19T06:28:19Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:status:
          f:currentNumberScheduled: {}
          f:desiredNumberScheduled: {}
          f:numberAvailable: {}
          f:numberMisscheduled: {}
          f:numberReady: {}
          f:observedGeneration: {}
          f:updatedNumberScheduled: {}
      subresource: status
spec:
  selector:
    matchLabels:
      app: daemonset-datakit
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: daemonset-datakit
    spec:
      volumes:
        - name: datakit-conf
          configMap:
            name: datakit-conf
            defaultMode: 420
        - name: rootfs
          hostPath:
            path: /
            type: ''
        - name: run
          hostPath:
            path: /var/run
            type: ''
        - name: debugfs
          hostPath:
            path: /sys/kernel/debug
            type: ''
        - name: cache
          hostPath:
            path: /root/datakit_cache
            type: ''
        - name: container-logs
          hostPath:
            path: /var/lib/containerd/container_logs
            type: ''
      containers:
        - name: datakit
          image: swr.cn-north-4.myhuaweicloud.com/liurui_bj/datakit:1.79.0
          workingDir: /usr/local/datakit
          ports:
            - name: http-port
              hostPort: 9529
              containerPort: 9529
              protocol: TCP
            - name: statsd-port
              hostPort: 8125
              containerPort: 8125
              protocol: UDP
            - name: otel-grpc-port
              hostPort: 4317
              containerPort: 4317
              protocol: TCP
            - name: logfwd-port
              hostPort: 9533
              containerPort: 9533
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: ENV_K8S_NODE_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
            - name: ENV_K8S_NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: ENV_DATAWAY
              value: https://openway.guance.com?token=tkn_3a0052c9f6d3498c8ce9ca0988fd9c82
            - name: ENV_CLUSTER_NAME_K8S
              value: cce
            - name: ENV_GLOBAL_HOST_TAGS
              value: host=__datakit_hostname,host_ip=__datakit_ip
            - name: ENV_GLOBAL_ELECTION_TAGS
            - name: ENV_DEFAULT_ENABLED_INPUTS
              value: statsd,dk,cpu,disk,diskio,mem,swap,system,hostobject,net,host_processes,container,kubernetesprometheus,logfwdserver,opentelemetry
            - name: ENV_ENABLE_ELECTION
              value: enable
            - name: ENV_INPUT_CONTAINER_ENABLE_POD_METRIC
              value: 'true'
            - name: ENV_HTTP_LISTEN
              value: 0.0.0.0:9529
            - name: ENV_INPUT_OTEL_GRPC
              value: '{"addr": "0.0.0.0:4317"}'
            - name: HOST_PROC
              value: /rootfs/proc
            - name: HOST_SYS
              value: /rootfs/sys
            - name: HOST_ETC
              value: /rootfs/etc
            - name: HOST_VAR
              value: /rootfs/var
            - name: HOST_RUN
              value: /rootfs/run
            - name: HOST_DEV
              value: /rootfs/dev
            - name: HOST_ROOT
              value: /rootfs
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 128Mi
          volumeMounts:
            - name: cache
              mountPath: /usr/local/datakit/cache
            - name: rootfs
              mountPath: /rootfs
              mountPropagation: HostToContainer
            - name: run
              mountPath: /var/run
              mountPropagation: HostToContainer
            - name: debugfs
              mountPath: /sys/kernel/debug
            - name: container-logs
              mountPath: /var/lib/containerd/container_logs
            - name: datakit-conf
              mountPath: /usr/local/datakit/conf.d/kubernetesprometheus/kubelet.conf
              subPath: kubelet.conf
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirstWithHostNet
      serviceAccountName: datakit
      serviceAccount: datakit
      hostNetwork: true
      hostPID: true
      hostIPC: true
      securityContext: {}
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: bursting.cci.io/node-type
                    operator: NotIn
                    values:
                      - virtual-kubelet
      schedulerName: default-scheduler
      tolerations:
        - operator: Exists
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  revisionHistoryLimit: 10
status:
  currentNumberScheduled: 2
  numberMisscheduled: 0
  desiredNumberScheduled: 2
  numberReady: 2
  observedGeneration: 2
  updatedNumberScheduled: 2
  numberAvailable: 2</code></pre><ul><li>进入 datakit 容器，并执行 datakit monitor 查看 opentelemetry 采集器是否开启</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437138" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 4：重写 Otel Collector 的采集配置</h3><p>log-agent-otel-collector.yaml</p><pre><code>kind: Deployment
apiVersion: apps/v1
metadata:
  name: log-agent-otel-collector
  namespace: monitoring
  uid: c055d466-4287-4860-9ff7-d28cc036ae89
  resourceVersion: '7557223'
  generation: 3
  creationTimestamp: '2025-09-22T07:28:09Z'
  labels:
    app: log-agent-otel-collector
    app.kubernetes.io/managed-by: Helm
    release: cceaddon-log-agent
  annotations:
    deployment.kubernetes.io/revision: '3'
    kubectl.kubernetes.io/last-applied-configuration: &gt;
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"3","meta.helm.sh/release-name":"cceaddon-log-agent","meta.helm.sh/release-namespace":"monitoring"},"creationTimestamp":"2025-09-20T19:02:18Z","generation":3,"labels":{"app":"log-agent-otel-collector","app.kubernetes.io/managed-by":"Helm","release":"cceaddon-log-agent"},"name":"log-agent-otel-collector","namespace":"monitoring","resourceVersion":"7514159","uid":"180806a1-7260-4139-989c-73945d7b1a4c"},"spec":{"minReadySeconds":5,"progressDeadlineSeconds":120,"replicas":2,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"log-agent-otel-collector"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"prometheus.io/path":"/metrics","prometheus.io/port":"8888","prometheus.io/scheme":"http","prometheus.io/scrape":"true","redeploy-timestamp":"1758396245987","scheduler.alpha.kubernetes.io/tolerations":"[{\"key\":
      \"taint.alpha.kubernetes.io/nodedown\",\"value\": \"\",\"effect\": \"NoExecute\",\"operator\":
      \"Exists\"}]"},"creationTimestamp":null,"labels":{"app":"log-agent-otel-collector","release":"cceaddon-log-agent"}},"spec":{"affinity":{"podAntiAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"podAffinityTerm":{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["log-agent-otel-collector"]}]},"topologyKey":"topology.kubernetes.io/zone"},"weight":100}]}},"containers":[{"args":["--config=/var/paas/ot-collector/ot-collector-service.yaml"],"command":["/var/paas/otel-collector/otelcol"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.podIP"}}},{"name":"Region","value":"cn-north-4"},{"name":"ProjectID","value":"9e92837f567145009ad4d230c4ac2c01"},{"name":"ClusterID","value":"74e8b92f-8f80-11f0-afe1-0255ac10026c"},{"name":"ClusterName","value":"cce-cci"},{"name":"WATCH_SECRET","value":"true"},{"name":"INSECURE_SKIP_VERIFY","value":"true"},{"name":"SCENE","value":"HWS"},{"name":"AKSK_SECRET_NAME","value":"paas.elb"},{"name":"WATCH_CLUSTER_CONFIG","value":"true"},{"name":"AOM_ENDPOINT","value":"https://aom.cn-north-4.myhuaweicloud.com"},{"name":"LTS_ACCESS_ENDPOINT","value":"https://lts-access.cn-north-4.myhuaweicloud.com:8102"},{"name":"CRYPTO_ENABLE","value":"true"},{"name":"PAAS_CRYPTO_PATH","value":"/etc/cipher"}],"image":"swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4","imagePullPolicy":"IfNotPresent","livenessProbe":{"exec":{"command":["/bin/bash","-c","exit
      0"]},"failureThreshold":3,"initialDelaySeconds":20,"periodSeconds":20,"successThreshold":1,"timeoutSeconds":10},"name":"otel-collector","ports":[{"containerPort":8006,"protocol":"TCP"},{"containerPort":4317,"protocol":"TCP"},{"containerPort":8888,"name":"metric-port","protocol":"TCP"}],"resources":{"limits":{"cpu":"1","memory":"2Gi"},"requests":{"cpu":"200m","memory":"1Gi"}},"securityContext":{"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsGroup":10000,"runAsUser":10000},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/paas/otel-collector/conf","name":"otel-collector-config-vol","readOnly":true},{"mountPath":"/var/paas/ot-collector/ot-collector-service.yaml","name":"ot-collector-service","readOnly":true,"subPath":"ot-collector-service.yaml"},{"mountPath":"/var/paas/sys/log","name":"logpath"},{"mountPath":"/etc/cipher/root.key","name":"rootkey","readOnly":true},{"mountPath":"/etc/cipher/common_shared.key","name":"commonsharedkey","readOnly":true},{"mountPath":"/var/paas/cert","name":"cert","readOnly":true}]}],"dnsConfig":{"options":[{"name":"ndots","value":"3"}]},"dnsPolicy":"ClusterFirst","initContainers":[{"command":["/bin/sh","-c","mkdir
      -p /var/paas/sys/log/otel \u0026\u0026 chmod 750 /var/paas/sys/log/otel \u0026\u0026 chown -R 10000:10000
      /var/paas/sys/log/otel"],"image":"swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4","imagePullPolicy":"IfNotPresent","name":"init","resources":{"limits":{"cpu":"200m","memory":"200Mi"},"requests":{"cpu":"100m","memory":"100Mi"}},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","volumeMounts":[{"mountPath":"/var/paas/sys/log","name":"logpath"}]}],"priorityClassName":"system-cluster-critical","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{"fsGroup":10000},"serviceAccount":"log-agent-serviceaccount","serviceAccountName":"log-agent-serviceaccount","terminationGracePeriodSeconds":30,"tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/not-ready","operator":"Exists","tolerationSeconds":30},{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":30},{"key":"role","operator":"Exists"},{"effect":"NoSchedule","key":"distribution.io/category","operator":"Equal","value":"IES"}],"volumes":[{"name":"otel-collector-config-vol","secret":{"defaultMode":384,"secretName":"log-agent-otel-collector-config"}},{"configMap":{"defaultMode":420,"items":[{"key":"ot-collector-service.yaml","path":"ot-collector-service.yaml"}],"name":"ot-collector-service"},"name":"ot-collector-service"},{"name":"cert","secret":{"defaultMode":416,"items":[{"key":"caCert","path":"caCert"},{"key":"serverCert","path":"serverCert"},{"key":"serverKey","path":"serverKey"}],"secretName":"log-agent-cert-secret"}},{"hostPath":{"path":"/var/paas/sys/log","type":""},"name":"logpath"},{"hostPath":{"path":"/var/paas/srv/kubernetes/root.key","type":""},"name":"rootkey"},{"hostPath":{"path":"/var/paas/srv/kubernetes/common_shared.key","type":""},"name":"commonsharedkey"}]}}},"status":{"conditions":[{"lastTransitionTime":"2025-09-20T19:02:18Z","lastUpdateTime":"2025-09-20T19:37:31Z","message":"ReplicaSet
      \"log-agent-otel-collector-8fbf8c694\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"},{"lastTransitionTime":"2025-09-22T06:15:38Z","lastUpdateTime":"2025-09-22T06:15:38Z","message":"Deployment does not have minimum availability.","reason":"MinimumReplicasUnavailable","status":"False","type":"Available"}],"observedGeneration":3,"replicas":2,"unavailableReplicas":2,"updatedReplicas":2}}
    meta.helm.sh/release-name: cceaddon-log-agent
    meta.helm.sh/release-namespace: monitoring
  managedFields:
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:28:09Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
            f:meta.helm.sh/release-name: {}
            f:meta.helm.sh/release-namespace: {}
          f:labels:
            .: {}
            f:app: {}
            f:app.kubernetes.io/managed-by: {}
            f:release: {}
        f:spec:
          f:minReadySeconds: {}
          f:progressDeadlineSeconds: {}
          f:replicas: {}
          f:revisionHistoryLimit: {}
          f:selector: {}
          f:strategy:
            f:rollingUpdate:
              .: {}
              f:maxSurge: {}
              f:maxUnavailable: {}
            f:type: {}
          f:template:
            f:metadata:
              f:annotations:
                .: {}
                f:prometheus.io/path: {}
                f:prometheus.io/port: {}
                f:prometheus.io/scheme: {}
                f:prometheus.io/scrape: {}
                f:scheduler.alpha.kubernetes.io/tolerations: {}
              f:labels:
                .: {}
                f:app: {}
                f:release: {}
            f:spec:
              f:affinity:
                .: {}
                f:podAntiAffinity:
                  .: {}
                  f:preferredDuringSchedulingIgnoredDuringExecution: {}
              f:containers:
                k:{"name":"otel-collector"}:
                  .: {}
                  f:args: {}
                  f:command: {}
                  f:env:
                    .: {}
                    k:{"name":"AKSK_SECRET_NAME"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"AOM_ENDPOINT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"CRYPTO_ENABLE"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ClusterID"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"ClusterName"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"INSECURE_SKIP_VERIFY"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"LTS_ACCESS_ENDPOINT"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"PAAS_CRYPTO_PATH"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"POD_IP"}:
                      .: {}
                      f:name: {}
                      f:valueFrom:
                        .: {}
                        f:fieldRef: {}
                    k:{"name":"ProjectID"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"Region"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"SCENE"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"WATCH_CLUSTER_CONFIG"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                    k:{"name":"WATCH_SECRET"}:
                      .: {}
                      f:name: {}
                      f:value: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:livenessProbe:
                    .: {}
                    f:exec:
                      .: {}
                      f:command: {}
                    f:failureThreshold: {}
                    f:initialDelaySeconds: {}
                    f:periodSeconds: {}
                    f:successThreshold: {}
                    f:timeoutSeconds: {}
                  f:name: {}
                  f:ports:
                    .: {}
                    k:{"containerPort":4317,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:protocol: {}
                    k:{"containerPort":8006,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:protocol: {}
                    k:{"containerPort":8888,"protocol":"TCP"}:
                      .: {}
                      f:containerPort: {}
                      f:name: {}
                      f:protocol: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:securityContext:
                    .: {}
                    f:allowPrivilegeEscalation: {}
                    f:readOnlyRootFilesystem: {}
                    f:runAsGroup: {}
                    f:runAsUser: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/etc/cipher/common_shared.key"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/etc/cipher/root.key"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/cert"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/ot-collector/ot-collector-service.yaml"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                      f:subPath: {}
                    k:{"mountPath":"/var/paas/otel-collector/conf"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
                      f:readOnly: {}
                    k:{"mountPath":"/var/paas/sys/log"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
              f:dnsConfig:
                .: {}
                f:options: {}
              f:dnsPolicy: {}
              f:initContainers:
                .: {}
                k:{"name":"init"}:
                  .: {}
                  f:command: {}
                  f:image: {}
                  f:imagePullPolicy: {}
                  f:name: {}
                  f:resources:
                    .: {}
                    f:limits:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                    f:requests:
                      .: {}
                      f:cpu: {}
                      f:memory: {}
                  f:terminationMessagePath: {}
                  f:terminationMessagePolicy: {}
                  f:volumeMounts:
                    .: {}
                    k:{"mountPath":"/var/paas/sys/log"}:
                      .: {}
                      f:mountPath: {}
                      f:name: {}
              f:priorityClassName: {}
              f:restartPolicy: {}
              f:schedulerName: {}
              f:securityContext:
                .: {}
                f:fsGroup: {}
              f:serviceAccount: {}
              f:serviceAccountName: {}
              f:terminationGracePeriodSeconds: {}
              f:tolerations: {}
              f:volumes:
                .: {}
                k:{"name":"cert"}:
                  .: {}
                  f:name: {}
                  f:secret:
                    .: {}
                    f:defaultMode: {}
                    f:items: {}
                    f:secretName: {}
                k:{"name":"commonsharedkey"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"logpath"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
                k:{"name":"ot-collector-service"}:
                  .: {}
                  f:configMap:
                    .: {}
                    f:defaultMode: {}
                    f:items: {}
                    f:name: {}
                  f:name: {}
                k:{"name":"otel-collector-config-vol"}:
                  .: {}
                  f:name: {}
                  f:secret:
                    .: {}
                    f:defaultMode: {}
                    f:secretName: {}
                k:{"name":"rootkey"}:
                  .: {}
                  f:hostPath:
                    .: {}
                    f:path: {}
                    f:type: {}
                  f:name: {}
    - manager: cfe-apiserver
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:40:23Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:spec:
          f:template:
            f:metadata:
              f:annotations:
                f:redeploy-timestamp: {}
    - manager: kube-controller-manager
      operation: Update
      apiVersion: apps/v1
      time: '2025-09-22T07:40:31Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            f:deployment.kubernetes.io/revision: {}
        f:status:
          f:availableReplicas: {}
          f:conditions:
            .: {}
            k:{"type":"Available"}:
              .: {}
              f:lastTransitionTime: {}
              f:lastUpdateTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
            k:{"type":"Progressing"}:
              .: {}
              f:lastTransitionTime: {}
              f:lastUpdateTime: {}
              f:message: {}
              f:reason: {}
              f:status: {}
              f:type: {}
          f:observedGeneration: {}
          f:readyReplicas: {}
          f:replicas: {}
          f:updatedReplicas: {}
      subresource: status
spec:
  replicas: 2
  selector:
    matchLabels:
      app: log-agent-otel-collector
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: log-agent-otel-collector
        release: cceaddon-log-agent
      annotations:
        prometheus.io/path: /metrics
        prometheus.io/port: '8888'
        prometheus.io/scheme: http
        prometheus.io/scrape: 'true'
        redeploy-timestamp: '1758526823089'
        scheduler.alpha.kubernetes.io/tolerations: '[{"key": "taint.alpha.kubernetes.io/nodedown","value": "","effect": "NoExecute","operator": "Exists"}]'
    spec:
      volumes:
        - name: otel-collector-config-vol
          secret:
            secretName: log-agent-otel-collector-config
            defaultMode: 384
        - name: ot-collector-service
          configMap:
            name: ot-collector-service
            items:
              - key: ot-collector-service.yaml
                path: ot-collector-service.yaml
            defaultMode: 420
        - name: cert
          secret:
            secretName: log-agent-cert-secret
            items:
              - key: caCert
                path: caCert
              - key: serverCert
                path: serverCert
              - key: serverKey
                path: serverKey
            defaultMode: 416
        - name: logpath
          hostPath:
            path: /var/paas/sys/log
            type: ''
        - name: rootkey
          hostPath:
            path: /var/paas/srv/kubernetes/root.key
            type: ''
        - name: commonsharedkey
          hostPath:
            path: /var/paas/srv/kubernetes/common_shared.key
            type: ''
      initContainers:
        - name: init
          image: swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4
          command:
            - /bin/sh
            - '-c'
            - mkdir -p /var/paas/sys/log/otel &amp;&amp; chmod 750 /var/paas/sys/log/otel &amp;&amp; chown -R 10000:10000 /var/paas/sys/log/otel
          resources:
            limits:
              cpu: 200m
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - name: logpath
              mountPath: /var/paas/sys/log
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      containers:
        - name: otel-collector
          image: swr.cn-north-4.myhuaweicloud.com/hwofficial/otelcol:1.7.4
          command:
            - /var/paas/otel-collector/otelcol
          args:
            - '--config=/var/paas/ot-collector/ot-collector-service.yaml'
          ports:
            - containerPort: 8006
              protocol: TCP
            - containerPort: 4317
              protocol: TCP
            - name: metric-port
              containerPort: 8888
              protocol: TCP
          env:
            - name: POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: Region
              value: cn-north-4
            - name: ProjectID
              value: 9e92837f567145009ad4d230c4ac2c01
            - name: ClusterID
              value: 74e8b92f-8f80-11f0-afe1-0255ac10026c
            - name: ClusterName
              value: cce-cci
            - name: WATCH_SECRET
              value: 'true'
            - name: INSECURE_SKIP_VERIFY
              value: 'true'
            - name: SCENE
              value: HWS
            - name: AKSK_SECRET_NAME
              value: paas.elb
            - name: WATCH_CLUSTER_CONFIG
              value: 'true'
            - name: AOM_ENDPOINT
              value: https://aom.cn-north-4.myhuaweicloud.com
            - name: LTS_ACCESS_ENDPOINT
              value: https://lts-access.cn-north-4.myhuaweicloud.com:8102
            - name: CRYPTO_ENABLE
              value: 'true'
            - name: PAAS_CRYPTO_PATH
              value: /etc/cipher
          resources:
            limits:
              cpu: '1'
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 1Gi
          volumeMounts:
            - name: otel-collector-config-vol
              readOnly: true
              mountPath: /var/paas/otel-collector/conf
            - name: ot-collector-service
              readOnly: true
              mountPath: /var/paas/ot-collector/ot-collector-service.yaml
              subPath: ot-collector-service.yaml
            - name: logpath
              mountPath: /var/paas/sys/log
            - name: rootkey
              readOnly: true
              mountPath: /etc/cipher/root.key
            - name: commonsharedkey
              readOnly: true
              mountPath: /etc/cipher/common_shared.key
            - name: cert
              readOnly: true
              mountPath: /var/paas/cert
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - '-c'
                - exit 0
            initialDelaySeconds: 20
            timeoutSeconds: 10
            periodSeconds: 20
            successThreshold: 1
            failureThreshold: 3
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 10000
            runAsGroup: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      serviceAccountName: log-agent-serviceaccount
      serviceAccount: log-agent-serviceaccount
      securityContext:
        fsGroup: 10000
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - log-agent-otel-collector
                topologyKey: topology.kubernetes.io/zone
      schedulerName: default-scheduler
      tolerations:
        - key: node.kubernetes.io/not-ready
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 30
        - key: node.kubernetes.io/unreachable
          operator: Exists
          effect: NoExecute
          tolerationSeconds: 30
        - key: role
          operator: Exists
        - key: distribution.io/category
          operator: Equal
          value: IES
          effect: NoSchedule
      priorityClassName: system-cluster-critical
      dnsConfig:
        options:
          - name: ndots
            value: '3'
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  minReadySeconds: 5
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 120
status:
  observedGeneration: 3
  replicas: 2
  updatedReplicas: 2
  readyReplicas: 2
  availableReplicas: 2
  conditions:
    - type: Available
      status: 'True'
      lastUpdateTime: '2025-09-22T07:28:16Z'
      lastTransitionTime: '2025-09-22T07:28:16Z'
      reason: MinimumReplicasAvailable
      message: Deployment has minimum availability.
    - type: Progressing
      status: 'True'
      lastUpdateTime: '2025-09-22T07:40:31Z'
      lastTransitionTime: '2025-09-22T07:28:09Z'
      reason: NewReplicaSetAvailable
      message: ReplicaSet "log-agent-otel-collector-5cfd6f4c7c" has successfully progressed.</code></pre><ul><li>为避免配置覆盖以及确保配置生效，指定 Otel Collector 启动加载生效的配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437139" alt="图片" title="图片" loading="lazy"/></p><ul><li>Otel Collector 挂载新的配置</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437140" alt="图片" title="图片" loading="lazy"/></p><ul><li>关闭健康检查</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437141" alt="图片" title="图片" loading="lazy"/></p><ul><li>若要实现 LTS 和观测云的数据双写，挂载的配置如下：</li></ul><pre><code>exporters:
  aom/default-event-aom:
    endpoint: https://aom.cn-north-4.myhuaweicloud.com
    events:
    - name: DeleteNodeWithNoServer
      name_cn: 废弃节点清理
  ...
  lts/default-stdout:
    compress_type: gzip
    endpoint: https://lts-access.cn-north-4.myhuaweicloud.com:8102
    log_type: log
    lts_group_id: d6b393b8-484f-4835-ba9f-xxxxx
    lts_stream_id: 8e02106f-8aeb-4da5-a5e1-xxxxx
  otlphttp:
    endpoint: http://datakit-service.datakit:9529/otel
    tls:
      insecure: true          
processors:
  batch/default-event:
    send_batch_max_size: 1000
    send_batch_size: 500
    timeout: 1000000000
  ...
  filter/cci-log:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: cci-log
  filter/datakit:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: datakit
service:
  pipelines:
    logs/cci-log:
      exporters:
      - lts/cci-log
      - otlphttp
  ...</code></pre><ul><li>挂载的配置若是只写到观测云，配置如下：</li></ul><pre><code>exporters:
  otlphttp:
    endpoint: http://datakit-service.datakit:9529/otel
    tls:
      insecure: true
processors:
  batch/logs:
    send_batch_max_size: 2000
    send_batch_size: 2000
  filter/cci-log:
    logs:
      exclude: {}
      include:
        match_type: strict
        record_attributes:
        - key: logconfig
          value: cci-log
receivers:
  fluentforward:
    endpoint: ${POD_IP}:8006
    tls:
      cert_file: /var/paas/cert/serverCert
      client_ca_file: /var/paas/cert/caCert
      key_file: /var/paas/cert/serverKey
  k8s_events: {}
service:
  pipelines:
    logs/cci-log:
      exporters:
      - otlphttp
      processors:
      - filter/cci-log
      - batch/logs
      receivers:
      - fluentforward
  telemetry:
    logs: {}
    metrics:
      address: ${POD_IP}:8888
      level: basic</code></pre><h3>步骤 5：容器 demo 发起请求，产生日志</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437142" alt="图片" title="图片" loading="lazy"/></p><h3>步骤 6：在观测云验证日志接入</h3><ul><li>登录观测云控制台 → 日志查看器 ，可以看到相关日志已经被采集到了观测云。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437143" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[AI重塑招聘决策：从“经验判断”到“数据]]></title>    <link>https://segmentfault.com/a/1190000047437213</link>    <guid>https://segmentfault.com/a/1190000047437213</guid>    <pubDate>2025-11-28 20:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI重塑招聘决策：从“经验判断”到“数据支撑”的变革<br/>AI破解招聘隐性损耗：重构面试与寻访的核心逻辑<br/>传统招聘中，企业常将招聘困境归咎于“简历不足”“人才难寻”，但真正消耗HR与业务部门精力的，是面试判断不准、沟通流程低效、候选人体验不佳三大隐性问题。这些问题虽不直接爆发，却在持续侵蚀企业的用人成本、品牌口碑与人才转化率。在AI技术深度渗透人力资源领域的当下，企业需要的已非单纯执行工具操作的AI，而是具备专业判断力、能支撑招聘决策的智能系统。<br/>这类AI招聘系统的核心价值，在于彻底解决传统招聘“面试不准”的痛点，推动招聘决策从“凭感觉”走向“靠数据”，为企业构建更高效、精准的人才选拔体系。</p><p>从“参考辅助”到“决策核心”：AI重构招聘评估标准<br/>对企业而言，AI招聘系统的核心价值判断标准，在于其评分结果是否精准、能否直接支撑招聘决策。当前领先的AI面试系统，已在评估精准度上实现突破性进展，完全能够满足企业的决策需求。<br/>其可靠性源于多维度的科学验证：评分结果可与人工面试官进行一对一“背靠背”对比校准，同时通过了效标效度与重测稳定信度两大心理学指标检验，确保评分的客观性与稳定性。这意味着，AI的评估结果足够可靠，可直接作为招聘决策依据，帮助企业摆脱“凭经验选人”的困境。经过持续迭代，当前顶尖的AI面试系统，已将判断能力提升至国际领先水准。<br/>全环节技术渗透：AI解决招聘“三难”问题<br/>传统AI面试常陷入“只会提问、不会判断”的误区，而领先的AI面试系统将“精准度”落实到面试的每一个细节环节，实现评估质量与效率的双重提升：<br/>•一问多能提升效率：单道题目可同步评估多项胜任力，实现初筛与技术复试的无缝衔接，让招聘评估效率提升50%以上，减少流程冗余。<br/>•自由追问捕捉关键：能够根据候选人的回答即时生成针对性追问，避免遗漏核心信息，打破模板化面试的局限，实现“有思考的互动评估”。<br/>•简历挖掘防范风险：自动识别简历中的模糊信息与潜在疑点，生成递进式提问，既能有效防范信息造假，又能避免HR因主观疏忽错失优质候选人；同时覆盖通用能力与专业能力考察，既能评估沟通、协作等通用素质，也能针对编程、算法、工程、财务等专业领域精准出题，大幅减轻HR与专业面试官的工作负担。<br/>从“机械交互”到“品牌增值”：AI重塑候选人体验<br/>过去，AI面试常因交互冷漠、流程呆板引发候选人投诉，影响雇主品牌形象。而新一代AI面试系统通过拟人化设计，实现了候选人体验的全面升级，让面试成为企业展示品牌价值的窗口：<br/>•情绪感知式沟通：系统能实时捕捉候选人的语速变化、情绪波动，甚至从回答中解读潜台词，用引导式语言帮助候选人缓解紧张，充分展现真实能力，而非机械地“完成问答流程”。<br/>•无干扰流畅体验：无需候选人手动操作“开始答题”“结束录制”，系统可自动识别回答状态，无缝衔接下一问题，交互节奏如同与真人HR面对面交流，消除流程中断带来的不适感。<br/>•沉浸式视觉交互：语音与口型实现高精度同步，彻底告别传统AI“纸片人”般的疏离感，让候选人在更自然的场景中完成面试。<br/>•双向信息互通：候选人可随时提出关于岗位职责、薪酬福利、职业发展路径等疑问，AI能即时给出准确解答，帮助候选人全面了解企业，提升其入职意愿。在优质人才稀缺的当下，良好的面试体验已成为企业吸引人才的重要竞争力。<br/>全流程自动化：AI重构人才寻访链路<br/>AI招聘的能力已突破单一面试环节，延伸至人才寻访全流程，形成一套可独立运行的自动化招聘体系，彻底改变传统人才寻访模式：<br/>•快速启动无延迟：仅需30-60秒完成参数设置，系统即可自动启动服务，无需专人值守，大幅降低人力成本。<br/>•智能筛选精准匹配：根据企业预设的学历、工作经验、技能要求等条件，自动从简历库中识别符合要求的候选人，排除无效信息干扰。<br/>•拟人化动态沟通：模拟人类交流语气发起对话，通过问答式互动进一步了解候选人意向与能力，若发现不匹配则即时终止沟通，避免资源浪费。<br/>•全量消息无遗漏：自动遍历所有未读消息，针对不同候选人的疑问给出个性化回复，确保不错失任何潜在人才。<br/>•数据自动流转：当候选人信息不全时，系统会主动以自然语言索取简历；确认候选人符合条件后，自动下载简历并同步至企业ATS系统，生成完整档案，实现“筛选-沟通-归档”全链路自动化。</p>]]></description></item><item>    <title><![CDATA[# Wireguard服务器管理工具之 ]]></title>    <link>https://segmentfault.com/a/1190000047437247</link>    <guid>https://segmentfault.com/a/1190000047437247</guid>    <pubDate>2025-11-28 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>你知道的，Wireguard服务器是没有图形界面管理工具的，默认只能在命令行编辑配置文件进行添加删除用户，更没有其他高级功能了，这非常不方便，然后就自己开发了这个工具，已经用了好多年了，现在推荐给大家：<code>zzxia-wireguard-manage</code>，项目地址是：<code>https://gitee.com/zhf_sy/zzxia-wg-manage</code>。</p><p>[toc]</p><h2>1 介绍</h2><p>wireguard 服务器管理工具。提供服务器配置、重启、警报、报表等功能；提供用户的列出、添加、删除、导出配置、二维码分享等功能</p><h3>1.1 功能：</h3><ol><li>配置服务器</li><li>列出账号</li><li>添加删除账号</li><li>导出用户配置文本，也可以二维码分享</li><li>重启</li><li>显示用户在线状态</li><li>用户登录发送钉钉消息</li><li>用户离线发送钉钉消息</li><li>每天生成用户报告</li><li>总报告</li></ol><h3>1.2 喜欢她，就满足她：</h3><ol><li>【Star】她，让她看到你是爱她的；</li><li>【Watching】她，时刻感知她的动态；</li><li>【Fork】她，为她增加新功能，修Bug，让她更加卡哇伊；</li><li>【Issue】她，告诉她有哪些小脾气，她会改的，手动小绵羊；</li><li>【打赏】她，为她买jk；<br/>&lt;img src="http://pic-bed.zzxia.vip/pic1/20210429155627295.jpg" alt="打赏" style="zoom:50%;" /&gt;</li></ol><h2>2 软件架构</h2><p>Linux shell</p><h2>3 安装教程</h2><ul><li>克隆下来即可</li><li>wireguard服务器的安装方法请参考官网</li></ul><h2>4 使用说明</h2><p>请使用-h|--help参数运行sh脚本即可看到使用帮助</p><h3>4.1 创建修改环境变量文件</h3><p>基于<code>env.sh.sample</code>创建环境变量文件<code>env.sh</code>，并根据自己的环境修改它：</p><pre><code class="bash">$ cat env.sh 
#!/bin/bash

## ----- 一般不需要修改 -----
# server env
SERVER_CONF_FILE_PATH="/etc/wireguard"             #--- wireguard服务器配置文件路径
WG_IF='wg0'                                        #--- wireguard服务器网卡
IP_PREFIX='172.30.0'                               #--- wireguard服务器网络地址前3节
IP_NETMASK='24'                                    #--- wireguard服务器IP掩码
# run
SERVER_CONF_FILE="${SERVER_CONF_FILE_PATH}/${WG_IF}.conf"
SERVER_PRIVATE_KEY="${SERVER_CONF_FILE_PATH}/private.key"
TODAY_WG_USER_LATEST_LOGIN_FILE="/tmp/wg-user-first-login-today.txt"

## ----- 一般需要修改 -----
# 钉钉
export DINGDING_API_URL_FOR_LOGIN="https://oapi.dingtalk.com/robot/send?access_token=填上你的token在这里"      #-- 用来发送钉钉消息
# server env
SERVER_CONNECT_INFO='服务器IP或域名:端口如51820'            #--- wireguard服务器用以接受用户连接的IP或域名及端口，用来生成用户的wg配置文件
# user env
USER_DNSs='192.168.11.3,192.168.11.4'                       #--- 用户的DNS，用来设置用户的DNS
USER_ALOWED_IPs="${IP_PREFIX}.0/${IP_NETMASK},0.0.0.0/0"    #--- 用户端走Wireguard链路的网络地址范围（用来设置用户端路由）</code></pre><h3>4.2 服务器设置</h3><p>运行<code>0-init-setup.sh</code>用于第一次配置服务器：</p><pre><code class="bash"># ./0-init-setup.sh</code></pre><h3>4.3 服务器管理</h3><pre><code class="bash"># ./wg-manage.sh -h

    用途：用于wireguard的用户管理
    依赖：./env.sh
          qrencode
    注意：
    用法：
        ./wg-manage.sh  [-h|--help]
        ./wg-manage.sh  [-l|--list]
        ./wg-manage.sh  [-a|--add {用户名}]  &lt;{IP第4段}&gt;
        ./wg-manage.sh  [-r|--rm|-o|--output-config  {用户名}]
        ./wg-manage.sh  [-R|--reload]
        ./wg-manage.sh  [-s|--status]
    参数说明：
        \$0   : 代表脚本本身
        []   : 代表是必选项
        &lt;&gt;   : 代表是可选项
        |    : 代表左右选其一
        {}   : 代表参数值，请替换为具体参数值
        %    : 代表通配符，非精确值，可以被包含
        #
        -h|--help      此帮助
        -l|--list      列出现有用户
        -a|--add       添加用户
        -r|--rm        删除用户
        -o|--output-config 输出用户配置文件
        -R|--reload    重启服务器
        -s|--status    服务器状态
    示例:
        #
        ./wg-manage.sh  -l              #--- 列出用户清单
        #
        ./wg-manage.sh  -a 猪猪侠 11    #--- 添加用户【猪猪侠】，IP地址尾号为【11】
        ./wg-manage.sh  -a 猪猪侠       #--- 添加用户【猪猪侠】，IP地址尾号自动分配
        #
        ./wg-manage.sh  -r 猪猪侠       #--- 删除用户【猪猪侠】
        #
        ./wg-manage.sh  -o 猪猪侠       #--- 输出用户【猪猪侠】的配置文件
        #
        ./wg-manage.sh  -R              #--- 重启服务器
        #
        ./wg-manage.sh  -s              #--- 查看服务器状态</code></pre><h3>4.3 用户登录警报级用户登录报告（可选）</h3><p>添加计划任务：</p><pre><code class="bash"># ./1-add-crontab.sh</code></pre><p>查看报告：</p><pre><code class="bash"># cat ./report/wg-report.list
+------------+----------+---------+---------+----------+-------------+----------------+
|日期        |用户名    |总流浪MiB|IN流量MiB|OUT流量MiB|IP           |远程IP          |
+------------+----------+---------+---------+----------+-------------+----------------+
|2021-07-19  |HB清      |.3       |.1       |.2        |172.30.5.23  |113.111.63.250  |
|2021-07-19  |HB颖      |28.9     |3.4      |25.5      |172.30.5.31  |113.111.63.250  |
|2021-07-20  |HB宇      |45.3     |8.3      |37.0      |172.30.5.22  |113.111.63.250  |
|2021-07-20  |HB华      |708.9    |101.6    |607.3     |172.30.5.23  |113.111.63.250  |
+------------+----------+---------+---------+----------+-------------+----------------+</code></pre><p>警报需要钉钉API，方法是建立钉钉群，然后添加一个钉钉机器人，然后把得到的api url写入到<code>env.sh</code>即可。</p><p>测试警报：</p><pre><code class="bash"># ./wg-login-alert-cron.sh</code></pre><h2>5 总结</h2><p>真的好好用！</p>]]></description></item><item>    <title><![CDATA[朝阳永续基于阿里云 Milvus 构建金]]></title>    <link>https://segmentfault.com/a/1190000047436995</link>    <guid>https://segmentfault.com/a/1190000047436995</guid>    <pubDate>2025-11-28 19:08:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、客户简介</h2><p>朝阳永续是先进的金融数据与智能服务提供商，致力于为基金管理公司、证券研究机构及专业投资者提供高质量、精准和全面的数据分析与决策支持工具。依托多年深耕金融行业的数据积累与投研经验，朝阳永续推出其核心产品——AI 小二，一款融合大模型技术的 AI 金融投研智能体。</p><p>AI 小二基于生成式 AI 能力，结合阿里云向量检索服务 Milvus 版（简称阿里云 Milvus），打造了集“智能问答、极速研究、深度分析、主题推荐、报表生成”于一体的智能化投研平台，全面赋能投资研究流程，显著提升用户的研究效率与决策质量。<br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdncwC" alt="image.png" title="image.png"/></p><h2>二、业务背景与诉求</h2><p>在金融投研领域，信息的准确性、权威性和时效性至关重要。AI 小二作为面向专业用户的严谨型 AI 应用，要求所有输出内容必须具备可追溯的信源依据，并确保引用观点来自权威、合规的数据来源。</p><p>然而，在面对日益增长的多源异构金融数据时，传统技术架构面临四大核心挑战：<br/>1. 数据规模庞大且复杂度高<br/>朝阳永续需处理包括上市公司公告、财报、卖方研究报告、新闻资讯、基金数据在内的海量非结构化与半结构化数据，日均新增数据量持续攀升。传统的关键词检索方式难以理解语义关联，无法满足用户对精准匹配的需求。</p><p>2. 检索精度与召回率难以兼顾<br/>用户常以自然语言提出复杂查询需求，例如：“某公司在过去四个财报季中管理层表述的变化趋势”。此类问题涉及跨文档、跨时间维度的语义理解，传统系统因缺乏上下文感知能力，导致漏检率高、误判频发，严重影响研究结果的可靠性。</p><p>3. 系统性能与稳定性压力巨大<br/>金融市场具有极强的实时性要求，尤其在交易时段，任何延迟都可能影响投资决策。原有自建检索系统在数据量达到亿级后，出现查询延迟波动、节点负载不均等问题，部分复杂查询响应时间超过数分钟，已无法满足高频、低延迟的业务需求。</p><p>4. 运维成本高昂，资源管理复杂<br/>随着集群规模扩大，运维团队需投入大量人力进行监控、调优、故障恢复和版本升级。这不仅增加了运营负担，也严重挤占了核心技术研发的时间与资源。</p><h2>三、阿⾥云的解决⽅案</h2><p>为突破上述瓶颈，朝阳永续携手阿里云，引入 阿里云向量检索服务 Milvus 版，构建高性能、高可用的金融级语义检索底座，支撑 AI 小二实现从“数据到洞察”的高效转化。<br/><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdncwD" alt="image.png" title="image.png" loading="lazy"/><br/>系统采用分层架构：<br/>底层：存储汇聚上市公司公告、研报、财报等原始文本数据；<br/>中间层：通过经由 PDF 解析、 Embedding 等处理环节形成向量化数据，并存入阿里云 Milvus；<br/>上层：利用阿里云 Milvus 强大的语义检索与条件过滤能力，实现多模态召回；<br/>接入层：通过标准化接口为 AI 小二提供低延迟、高并发的检索服务。</p><p>整个系统实现了从数据摄入、向量化、存储到检索的全链路自动化，为金融级 AI 应用提供了坚实的技术支撑。</p><h2>四、典型应用场景与效果验证</h2><p><strong>案例一：追踪上市公司“互动易”表述变化（上市公司互动易数据）</strong><br/>上市公司在历史多时期内的互动易表述，往往隐含管理层对未来业绩、行业趋势的前瞻性判断。这类非结构化文本难以通过关键词检索准确提取。</p><p>借助阿里云 Milvus 的高精度语义匹配能力，AI 小二能够快速定位并召回特定时间段内相关问答内容，帮助研究员高效捕捉企业战略动向与情绪变化，显著提升信息挖掘效率。</p><p><strong>案例二：分析卖方分析师观点演变（卖方研报数据）</strong><br/>朝阳永续累计收录超300万份经合规授权的卖方研究报告，每日新增数千篇。投资者常需分析某分析师对某一公司的长期观点演变。</p><p>阿里云 Milvus 支持大规模向量实时写入与高效检索，在财报披露高峰期仍能稳定响应复杂查询，确保用户及时获取最新市场研判，强化投研时效性。<br/><img width="723" height="208" referrerpolicy="no-referrer" src="/img/bVdncwE" alt="image.png" title="image.png" loading="lazy"/><br/><strong>案例三：解读财报中的管理层表述（上市公司财报数据）</strong><br/>财报信息是众多专业投资者关注的重要一手信息，其中，管理层在财报中的经营表述可能蕴含重要的投资信息。AI 小二通过将百万级财报文本向量化，并基于阿里云 Milvus 实现精准语义召回，可快速提取与用户问题相关的段落，再由大模型进行情感分析与趋势提炼，大幅提升专业投资者财报研究效率。</p><h2>五、为什么选择阿⾥云 Milvus</h2><p>在向量数据库选型过程中，朝阳永续曾长期使用开源 PostgreSQL 向量扩展方案，但在实际应用中暴露出性能瓶颈与运维难题。最终全面迁移至 阿里云 Milvus 向量检索服务，主要基于以下三大关键考量：<br/><strong>1. 性能飞跃：查询速度提升十倍以上</strong><br/>经验证：开源 PG 库平均响应速度600ms，阿里云Milvus平均响应速度50ms。已有的上市公司公告、基金公司公告、卖方研究报告，专利及法律文档等，Embedding完成后，向量的数量级已达到十亿量级。开源 PG 库在结合标量条件筛选，并开启向量检索和全文检索的混合检索模式下，平均响应时间已冲到600ms，极端情况长达数分钟。切换至阿里云 Milvus 后，同样规模的数据，类似的检索方式，平均响应速度提升至50ms，提速十倍以上，充分满足金融业务对实时性的严苛要求。</p><p><strong>2. 运维大幅降低，工作量下降80%</strong><br/>开源 PG 方案遵循一切自己动手的原则，大量监控框架均需要搭配其他开源项目进行部署，需要花费运维人员大量的时间调研，且不完全符合运维需要，须定制整合，成本高，运维难度大。</p><p>阿里云 Milvus 具备完善的监控能力，提供包括 CPU 和内存使用率在内的逾百项监控指标，并支持自定义报警规则，可灵活适应多样化的业务需求。这一全方位的监控体系有效帮助朝阳用户技术实现对集群运行状态的精准感知与及时响应。同时，阿里云 Milvus 还支持灵活的资源调整机制，可根据业务负载变化，平滑实现资源的扩容或缩容，保障服务持续稳定运行。</p><p><strong>3. 生产级稳定性：历经百次高峰冲击，零故障运行</strong><br/>在开盘、重大政策发布等流量高峰期间，原有 PG 集群频繁出现CPU打满、服务卡顿甚至宕机现象。</p><p>切换至阿里云 Milvus 后，面对这些投研及投顾领域的热点事件及开盘、收盘时间点的高频应用时间。阿里云 Milvus 历经半年百余次瞬时访问峰值考验，始终保持 0 故障、0 中断，展现出卓越的高可用性与抗压能力，真正达到金融级标准。</p><h2>六、总结</h2><p>通过采用 阿里云向量检索服务 Milvus 版，朝阳永续成功构建了高性能、高可靠的金融语义检索引擎，有效解决了海量非结构化数据下的检索效率、精度与稳定性难题，为“AI 小二”提供了强大的底层支撑，显著提升了智能投研服务的用户体验与商业价值。</p><p>未来，朝阳永续期待与阿里云在大模型、向量检索、Agent 记忆系统等领域深化合作，共同探索其在智能风控、资产配置、合规审查等更多金融场景的创新应用，携手推动中国金融行业的数字化转型与智能化升级。</p>]]></description></item><item>    <title><![CDATA[在 Gemini CLI 中使用 Gem]]></title>    <link>https://segmentfault.com/a/1190000047437007</link>    <guid>https://segmentfault.com/a/1190000047437007</guid>    <pubDate>2025-11-28 19:07:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>现在可以在 Gemini CLI 中用 Gemini 3 Pro 啦。</p><p>这下子终端不仅仅是一个输入指令的窗口，而是变成了一个具备执行力的开发环境，并且还能通过 Agentic Coding（代理编码）处理复杂的工程任务，并通过调用外部工具优化工作流。</p><p>目前 Google AI Ultra 订阅用户或持有付费 Gemini API Key 的用户可以使用，而其他用户可以加入<a href="https://link.segmentfault.com/?enc=%2B8yY1SZFW14zCDDkfMwxzw%3D%3D.I0hAbAaNcPG2IyDgcuvU6HQvS4KjwqyQ97XgPEKc0TRp8%2BCFaN27mgwcOkYR5L%2Bi" rel="nofollow" target="_blank">加入候补名单</a>，等官方开放权限。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncHi" alt="image.png" title="image.png"/></p><h3>如何在 Gemini CLI 使用 Gemini 3 Pro</h3><p>首先要部署好<a href="https://link.segmentfault.com/?enc=VhxgUMSeQ4i6Cgj91USbaQ%3D%3D.kpEtmOXNFhl1iBv%2FeJFfEAGeEcmE37Dc5Jv25aOGF4XfFNez8gY09sJy5zBpWbaD" rel="nofollow" target="_blank">Node.js 20或以上的环境</a>，如果不知道怎么安装。可以使用<a href="https://link.segmentfault.com/?enc=Dwy8QbOE5BdcMOmYNDVp7g%3D%3D.RnbXqf4extGSY%2Bq9VobrmfS9Om3Cn73wfr8L9FKelgM%3D" rel="nofollow" target="_blank">ServBay</a>，一键安装。</p><p>ServBay能够支持不同Node.js版本同时运行，并且能一键切换它们。只需要安装好ServBay后，在左边菜单的「软件包」中找到Node.js，点击安装即可。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdncHj" alt="image.png" title="image.png" loading="lazy"/></p><p>然后输入命令安装好Gemini。</p><pre><code>npm install -g @google/gemini-cli</code></pre><h4><strong>升级</strong> <strong>CLI</strong> <strong>版本</strong></h4><ol><li>所有这些工作准备好之后，输入下面的命令升级。</li></ol><pre><code>npm install -g @google/gemini-cli@latest</code></pre><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHl" alt="image.png" title="image.png" loading="lazy"/></p><ol start="2"><li><strong>开启预览功能</strong></li></ol><p>安装完成后，在终端运行 <code>/settings</code>，将 <code>Preview features</code> 设置为 <code>true</code>。此时，Gemini CLI 将默认使用 Gemini 3 Pro 模型。</p><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHm" alt="image.png" title="image.png" loading="lazy"/></p><p>以下是 4个具体场景，展示如何利用 Gemini 3 Pro 加速开发。</p><hr/><h3>利用 Agentic Coding 在终端直接构建应用</h3><p>Gemini 3 Pro 擅长整合文本、代码和视觉信息，并能遵循极其复杂的指令，一句话就能直接从生成一个可运行的项目骨架。</p><p><strong>实操案例：构建</strong> <strong>高保真</strong> <strong>3D 仿真场景</strong></p><p>传统的 3D 开发需要配置图形库、本地服务器和大量样板代码。现在，可以将创意简报和技术规格合并为一个 Prompt，让 CLI 直接生成结果。</p><p>比如输入以下指令，要求它生成一个金门大桥的 3D 模拟：</p><pre><code>Objective: Build a visually stunning, photorealistic 3D Voxel simulation of the Golden Gate Bridge using Three.js, prioritizing quality and complex visuals (no simple blocks), atmospheric depth and 60FPS performance.

Visuals  &amp;  Atmosphere:
Lighting: Slider (0-24h) controlling sun position, light intensity, sky color, and fog color.
Fog: Volumetric-style fog using sprite particles that drift and bob. Slider 0-100. 0 = True Zero (Crystal Clear). 100 = Dense but realistic (not whiteout).
Water: Custom GLSL shader with waves, specular reflections, and manual distance-based fog blending (exp2) for seamless horizon integration.
Post-Processing: ACESFilmic Tone Mapping and UnrealBloom (optimized for glowing lights at night).

Scene Details:
Bridge: Art Deco towers with concrete piers (anchored to seabed), main span catenary cables, and suspenders.
Terrain: Low-poly Marin Headlands and SF Peninsula.
Skyline: Procedural city blocks on the SF side.
Traffic: Up to 400 cars using InstancedMesh, positioned accurately on top of the deck (ensure vertical alignment prevents clipping into the concrete). Each car features emissive headlights (white) and taillights (red).
Ships: Procedural cargo ships with hull, containers, and functional navigation lights (Port/Starboard/Mast/Cabin) moving along the water.
Nature: Animated flocking birds.
Night Mode: At night, activate city lights, car headlights, ship navigation lights, tower beacons, street lights.

Tech  &amp;  Controls:
Core: Must output only single HTML file golden_gate_bridge.html to be run in a blank Chrome tab. Import Three.js/Addons via CDN map.
Libs: three (Core library) via CDN (ES Modules); three/examples/jsm/... modules via Import Map.
Build: No build step (Vite/Webpack). Pure HTML/JS.
UI: Visually appealing sliders for Time (0-24h), Fog Density (0-100%), Traffic Density (0-100%), and Camera Zoom.
Optimization: InstancedMesh for all repetitive elements (cars, lights, birds).</code></pre><p>Gemini 3 Pro 会理解对光照、GLSL 着色器和性能优化的具体要求，直接生成一个独立的 HTML 文件。</p><p><img width="723" height="427" referrerpolicy="no-referrer" src="/img/bVdncHn" alt="image.png" title="image.png" loading="lazy"/></p><h3>多模态开发：从草图到代码</h3><p>如果想做一个视觉创意，Gemini 3 Pro 的多模态能力可以快速实现 UI 原型。只需将草图拖入终端，配合文字描述，它就能识别布局并生成代码。</p><p><strong>实操案例：还原</strong> <strong>赛博朋克</strong> <strong>风格</strong> <strong>UI</strong></p><p>假设正在设计一个网络安全监控工具，需要独特的视觉风格。将线框图（<code>@wireframe.png</code>）拖入终端，并输入以下指令：</p><pre><code>Build a UI prototype for "CyberSentinel," a real-time network security monitor. The visual style should be gritty Cyberpunk: neon green and hot pink grid lines against a deep void background. Instead of typical charts, visualize data streams as cascading "digital rain" or glitch-art pillars. When hovering over a data node, a holographic, semi-transparent info card should pop up with glitch effects, styled using Tailwind CSS. I have a rough wireframe here to guide the layout: @wireframe.png.</code></pre><p>模型不仅会还原线框图的结构，还会根据开发者对 "Cyberpunk"、"Digital Rain" 和 "Glitch effects" 的描述，编写对应的 CSS 动画和布局逻辑。</p><h3>逆向工程：自动生成项目文档</h3><p>Gemini 3 Pro 能够深入理解代码逻辑，而不仅仅是语法。这使得它非常适合为老旧项目或复杂的开源代码库补充文档。</p><p><strong>实操案例：为无文档代码生成说明书</strong></p><p>如果接手一个没有文档的项目时，可以让 Gemini 分析代码并生成结构化文档：</p><pre><code>This is an undocumented application. Please read through the entire codebase to understand the logic first, then generate user documentation for me. The documentation should include: user interactions (command-line options, authentication, etc.), explanations of core concepts (such as MCP), an architectural overview, and how to contribute to the open-source project. Please ensure the format is clear and easy to read; do not just provide a simple HTML page.</code></pre><h3>跨服务联动：排查云端故障</h3><p>Gemini 3 Pro 支持工具调用（Tool Use），可以根据你的指令制定多步骤计划，联动日志监控、安全扫描和代码库来解决复杂问题。</p><p><strong>实操案例：排查 Cloud Run 服务延迟</strong></p><p>当遇到线上性能问题时，它能化身为一个 SRE 助手：</p><pre><code>Users are reporting that the 'Save Changes' button is slow to respond. Please investigate the status of the 'tech-stack' service.</code></pre><p>它会自动连接 Cloud Run 查看指标，调用 Snyk 等工具扫描潜在问题，并结合代码变更记录，最终给出根因分析甚至修复补丁。</p><h3>总结</h3><p>这些案例只是冰山一角。Gemini 3 Pro 的真正价值在于它能适应开发具体场景，无论是优化一行命令，还是构建一个完整的功能模块。</p><p>感兴趣可以升级体验。</p>]]></description></item><item>    <title><![CDATA[活动推荐丨「实时互动 × 对话式 AI」]]></title>    <link>https://segmentfault.com/a/1190000047437028</link>    <guid>https://segmentfault.com/a/1190000047437028</guid>    <pubDate>2025-11-28 19:06:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437030" alt="" title=""/></p><p>声网博客www.shengwang.cn/blog/ 正式启航啦！这是一个聚焦实时互动（RTE） × 对话式 AI（Conversational AI） 的内容空间。</p><p>我们关注技术背后的Why，探讨应用场景的What if，分享实战经验的How better。在声网博客，我们希望呈现：</p><ul><li>技术解析：从原理到落地的深度剖析。</li><li>应用创新：实时互动与对话式AI 如何重塑场景。</li><li>实战经验：工程师世界里的技术挑战与解法。</li><li>趋势洞察：对未来技术范式的思考。</li></ul><p>博客刚刚开张，内容区还像一个刚初始化、等待被写入的模块组，所以诚邀你成为第一批内容共建者。</p><p>为此，我们全新推出「时空切片」特刊，用以捕捉和记录大家在“实时互动 X 对话式 AI ” 世界里的思想火花、实战经验……并将它们沉淀成一帧帧可被分享的时空标注。</p><h2>活动主题</h2><p><strong>为了让大家更轻松地加入，特刊第一期定了个能快速上手的征文主题：</strong></p><p><strong>《“实时互动 X 对话式AI”工作簿》</strong></p><p>我们想听听你在“RTE × 对话式 AI” 这件事上，看到了什么、做过什么、想过什么。</p><p>无论是技术拆解、实践经验、场景思考，还是灵光一现的洞察，<strong>只要与“实时互动、对话式AI”相关</strong>，欢迎你的“内容投喂”。</p><h2>活动奖励</h2><p>文章一旦入选，就有机会被数万技术er刷到。至于能不能“爆”，就让阅读量这个指标来跑分吧。</p><ul><li><p>阳光奖</p><p>作品入选，即赠 定制 3C 认证充电宝</p></li><li><p>内容战力榜（按单篇阅读量从高到低排序）</p><p>No.1  AirPods</p><p>No.2-3 各 500 元京东卡</p><p>No.4-6 各 200 元京东卡</p></li><li><p>好文助力出圈</p><p>连更 ≥3 篇，为你开设个人专栏</p><p>优秀作品将在小红书、CSDN 、InfoQ 等平台原文署名发布</p></li></ul><h2>活动评选与投稿方式</h2><h3>评选方式</h3><ul><li>初审关卡：博客内容小组会对提交作品进行初审，入选后将发布在博客征文活动页。</li><li>内容战力榜：所有入选作品会按阅读量排名，阅读量=内容战力值（同一作者若多篇冲榜，只取阅读量最高的那篇计名）。</li><li>光荣榜公布：获奖结果将统一公示在博客活动页，人人都能查。</li></ul><h3>活动时间</h3><ul><li><strong>2025 年 11 月 17 日 - 2025 年 12 月 31 日</strong></li></ul><h3>征文要求</h3><ul><li>字数：1000字以上，Word 或 Markdown 都行。</li><li>内容：必须原创，未在其他平台发布，可加图表、代码等。</li><li>主题范围：实时互动、对话式AI、应用场景、实战经验……总之跟“RTE × 对话式AI”沾边的都欢迎！</li></ul><h3>提交方式</h3><ul><li>邮件格式：[声网博客]+ 文章标题 + 作者姓名，记得附上作品哟！</li><li>发送至邮箱：<a href="mailto:blog@shengwang.cn" target="_blank">blog@shengwang.cn</a></li></ul><h2>Q&amp;A</h2><p>1）奖品什么时候发？</p><ul><li>作品入选后，小编会邮件联系你确认信息；奖品将在征文活动结束后两周内发出。</li></ul><p>2）投稿遇到问题？</p><ul><li>对征文活动有任何疑问，统统可以邮件联系我们blog\@shengwang.cn，备注[博客征文]，你的疑问，我们都会认真回复。</li></ul><p>逛博客请查看</p><p><a href="https://link.segmentfault.com/?enc=YZyGYmsRy%2BT3nATjBpTl8g%3D%3D.QWDFFSw40VmLjRZLiDienGnt%2FZnHlMpO8UWsmGWsoaE%3D" rel="nofollow" target="_blank">https://www.shengwang.cn/blog/</a> </p><p>活动详情查看</p><p><a href="https://link.segmentfault.com/?enc=17QIs7rYERUbMZnti3xaQg%3D%3D.hBLknFtKT7uR51tYbIccfmaD3p549WFLnocd8eq8Wn9yKSy6ZOgBd0liS%2FZm7GcLBKV7hTP9HSAzPNuRWG86oQ%3D%3D" rel="nofollow" target="_blank">https://www.shengwang.cn/blog/blogdetail/RTExConvoAI-campaign/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437031" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437032" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=y%2B3BtIckZ8n8BYNHP1QcNA%3D%3D.WtDJwsVXDplwQmB7P9VHjRCSs5cyTSYUY%2Ft%2BQQ4KIFs%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437033" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[用 Python 将 `.py` 文件转]]></title>    <link>https://segmentfault.com/a/1190000047437040</link>    <guid>https://segmentfault.com/a/1190000047437040</guid>    <pubDate>2025-11-28 19:05:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在开发工作中，我们经常需要分享或归档 Python 脚本文件。<code>.py</code> 文件虽然在编辑器中可读性强，但直接发送给他人时，缺乏格式统一、排版美观，且打印体验不佳。尤其是在技术文档、培训资料或项目报告中，将代码转换为 PDF 不仅便于阅读，也能保证跨平台展示效果一致。</p><p>Python 生态中有多种方式可以实现代码转 PDF，而 <strong><a href="https://link.segmentfault.com/?enc=AwWL7ubnvKEutlLm4qv1sw%3D%3D.OsQRc7w6vrqrq0J9tpbK%2Fi2fydSOSqvYynfU8ImnLJRln4saTIki1l1gpVuugx9E68u8f2VcMonOJegRsYp%2Fhg%3D%3D" rel="nofollow" target="_blank">Free Spire.Doc for Python</a></strong> 提供了稳定、易用的方案。本文将介绍两种常见的实现方法：<strong>普通文本方式</strong>和<strong>带语法高亮的 HTML 插入方式</strong>，并详细分析两者的差异与使用场景。</p><hr/><h2>1. 使用 Spire.Doc 将 Python 代码按行写入 PDF</h2><p>最简单的方法是将 <code>.py</code> 文件逐行读取，并将每一行以固定字体插入到 PDF 中。这种方式适合不需要语法高亮，只希望保留原始代码排版的场景。</p><pre><code class="python">from spire.doc import Document, FileFormat, BreakType, Color, LineSpacingRule, LineNumberingRestartMode

# 读取 Python 文件
with open("Python.py", "r", encoding="utf-8") as f:
    python_code = f.read()

# 创建文档对象
doc = Document()
section = doc.AddSection()
paragraph = section.AddParagraph()

# 逐行添加代码
for line_number, line in enumerate(python_code.split("\n")):
    tr = paragraph.AppendText(line)
    tr.CharacterFormat.FontName = "Courier New"  # 设置等宽字体
    tr.CharacterFormat.FontSize = 10.5
    if line_number &lt; len(python_code.split("\n")) - 1:
        paragraph.AppendBreak(BreakType.LineBreak)

# 可选格式设置
paragraph.Format.BackColor = Color.get_WhiteSmoke()  # 背景色
paragraph.Format.LineSpacingRule = LineSpacingRule.Multiple
paragraph.Format.LineSpacing = 14.0

# 行号设置
section.PageSetup.LineNumberingStartValue = 1
section.PageSetup.LineNumberingStep = 1
section.PageSetup.LineNumberingRestartMode = LineNumberingRestartMode.RestartPage
section.PageSetup.LineNumberingDistanceFromText = 12.0

# 保存为 PDF
doc.SaveToFile("output/Python-PDF.pdf", FileFormat.PDF)</code></pre><p>转换结果：</p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdncHF" alt="转换py文件为PDF" title="转换py文件为PDF"/></p><p><strong>说明与优化点</strong>：</p><ul><li><strong>字体选择</strong>：使用等宽字体（如 Courier New）保证代码对齐整齐。</li><li><strong>行间距</strong>：设置多倍行距可提高可读性。</li><li><strong>背景色与行号</strong>：轻微灰色背景搭配行号，更适合打印或阅读。</li></ul><p>这种方式的优势在于实现简单，代码结构完全保留，兼容性高，但缺点是无法提供语法高亮效果，对于较长或复杂代码可读性稍差。</p><hr/><h2>2. 使用 Pygments 生成带语法高亮的 PDF</h2><p>如果希望 PDF 中的代码带颜色区分关键字、注释、字符串等，可以先使用 <strong>Pygments</strong> 将 Python 代码转换为 HTML，再通过 Spire.Doc 将 HTML 插入 PDF。</p><pre><code class="python">from spire.doc import Document, FileFormat
from pygments import highlight
from pygments.lexers import PythonLexer
from pygments.formatters import HtmlFormatter

def py_to_inline_html(py_file_path):
    with open(py_file_path, "r", encoding="utf-8") as f:
        code = f.read()
    # 生成行内 HTML，带行号
    formatter = HtmlFormatter(noclasses=True, linenostart=1, linenos='inline')
    return highlight(code, PythonLexer(), formatter)

html_result = py_to_inline_html("Python.py")

doc = Document()
section = doc.AddSection()
paragraph = section.AddParagraph()
paragraph.AppendHTML(html_result)

# 保存带高亮的 PDF
doc.SaveToFile("output/Python-PDF-Highlighted.pdf", FileFormat.PDF)</code></pre><p>转换结果：</p><p><img width="723" height="330" referrerpolicy="no-referrer" src="/img/bVdncHH" alt="转换py文件为PDF" title="转换py文件为PDF" loading="lazy"/></p><p><strong>关键说明</strong>：</p><ul><li><code>HtmlFormatter(noclasses=True, linenos='inline')</code>：生成内联样式 HTML，并带行号。</li><li><code>AppendHTML</code> 方法可以直接将 HTML 内容插入到 PDF，保留语法高亮效果。</li><li>使用这种方法生成的 PDF 更美观，适合演示文档、教程或培训资料。</li></ul><hr/><h2>3. 两种方法的对比与使用建议</h2><table><thead><tr><th>特性</th><th>按行插入文本</th><th>HTML 语法高亮插入</th></tr></thead><tbody><tr><td>复杂度</td><td>简单</td><td>中等，需要 Pygments</td></tr><tr><td>可读性</td><td>一般</td><td>高，关键字、注释颜色区分明显</td></tr><tr><td>打印效果</td><td>普通</td><td>良好，但颜色需打印机支持</td></tr><tr><td>适用场景</td><td>快速生成、代码归档</td><td>教学文档、演示、报告</td></tr></tbody></table><p>总结来看，如果对语法高亮要求不高，按行插入文本即可；如果希望 PDF 更美观、可读性高，HTML 高亮方式更合适。</p><hr/><h2>4. 扩展说明</h2><ol><li><strong>合并重复操作</strong>：在按行插入的方式中，背景色、行距、字体等可封装为函数，避免重复设置，提高代码复用性。</li><li><strong>代码排版与打印</strong>：PDF 是固定排版格式的文档，将 Python 代码导出后可确保不同环境中显示一致，避免字体或缩进混乱。</li><li><strong>批量处理</strong>：可将以上方法封装为函数，循环处理多个 <code>.py</code> 文件，实现批量生成 PDF，适合团队协作或项目文档归档。</li></ol><hr/><h2>总结</h2><p>本文介绍了两种将 Python 脚本转换为 PDF 的方法：一种是按行插入文本，另一种是通过 HTML 生成语法高亮效果。前者简单高效，适合快速归档；后者美观专业，适合文档和演示场景。通过掌握 <strong>Spire.Doc</strong> 的 <code>AppendText</code> 和 <code>AppendHTML</code> 方法，以及 Pygments 的 HTML 转换能力，可以轻松生成结构清晰、可读性强的 Python PDF 文档。</p><p>无论是个人笔记整理、项目文档归档，还是教学演示，这两种方法都能满足不同需求，提高代码分享和管理的效率，同时保留排版美观性。</p>]]></description></item><item>    <title><![CDATA[共筑AI时代开源OS新生态，龙蜥社区走进]]></title>    <link>https://segmentfault.com/a/1190000047437046</link>    <guid>https://segmentfault.com/a/1190000047437046</guid>    <pubDate>2025-11-28 19:05:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>「龙蜥社区“走进系列”MeetUp」是由龙蜥社区与生态合作伙伴联合主办的系列月度活动，每期走进一家企业，聚焦龙蜥社区和合作伙伴的技术、产品和创新动态，展示硬核技术，共建繁荣生态。</p><p>龙蜥社区“走进系列”MeetUp 第 19 期将携手 Arm，聚焦“共筑 AI 时代开源 OS 新生态”主题，举办一场深度技术研讨会。随着大语言模型（LLM）与生成式 AI 吹响人工智能时代的新号角，本期 MeetUp 将围绕龙蜥操作系统与 Arm Neoverse 平台，全面呈现 AI 进化的全链路实践。内容涵盖硬件 IP、服务器平台、Java 虚拟机、模型推理框架与引擎、优化分析工具，并特别分享在智能驾驶场景中的系统优化与部署经验。诚邀您一同探索 AI 时代下开源操作系统新生态的开放演进之路。</p><p>时间：12 月 11 日 13:00-17:10</p><p>地点：上海漕河泾万丽酒店2楼</p><p>报名链接：<a href="https://link.segmentfault.com/?enc=oS41hP5k0%2FmcFnFH3%2BUTcg%3D%3D.dxZDpMtG%2F75w%2B%2BtypZdXKgRb3RDHTTBPgsVAulMxnuAHRCkpJNK4ueLcRca27bbC" rel="nofollow" target="_blank">https://openanolis.mikecrm.com/3sbZgtt</a></p><p>更多详细议程见下方海报：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437048" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[直播预告：LLM for AIOPS，是]]></title>    <link>https://segmentfault.com/a/1190000047437051</link>    <guid>https://segmentfault.com/a/1190000047437051</guid>    <pubDate>2025-11-28 19:04:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在 AI 与本土化双重浪潮之下，服务器操作系统正迎来历史性变革。由龙蜥社区理事长单位阿里云联合 InfoQ 打造的直播 IP 栏目《AI 进化论：智算时代操作系统的破局之路》，以云、AI、安全等技术与服务器操作系统如何融合演进为主线，聚焦服务器操作系统在智算时代的进化之路，特邀学术权威、行业专家、客户代表围绕原生智能、原生安全、软硬协同等热点议题展开深度对话。截至目前，已直播五期，线上观看人次达 30 万+。</p><p>当大模型浪潮席卷运维领域，LLM Agent 被寄予厚望——它究竟是解决复杂运维难题的“银弹”，还是被过度炒作的“泡沫”？《AI 进化论：智算时代操作系统的破局之路》系列直播第六期将于 12 月 8 日 14:00 开始，特别邀请到云杉网络总裁向阳，阿里云智能集团运维总监、龙蜥社区运维联盟主席冯富秋，InfoQ 极客传媒总经理、总编辑王一鹏三位嘉宾，围绕 “LLM for AIOPS，是泡沫还是银弹？” 这一主题展开深度探讨，聚焦 LLM Agent 在 AIOPS 中的真实价值与落地挑战，深入剖析其在故障诊断、根因分析、自动化响应等场景中的典型应用与常见误区；探讨如何通过提示工程、工具调用与反馈机制有效抑制幻觉，释放大模型在运维领域的潜力。</p><p>更多直播亮点，可点击下方海报了解，欢迎大家打开微信，扫描二维码预约直播：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437053" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[干货推荐：Java内存排查太难？这个「内]]></title>    <link>https://segmentfault.com/a/1190000047437059</link>    <guid>https://segmentfault.com/a/1190000047437059</guid>    <pubDate>2025-11-28 19:03:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>背景</h3><p>随着汽车行业加速智能化转型，从传统线下 IDC 集群向云端迁移并进行容器化改造，经常会遇到关于 Pod 内存异常、Pod发生 OOMKilled 的问题， 这些问题主要的矛盾点在于：<br/>1、Pod（容器）内存占用比 JVM 内存监控（堆内和堆外内存）占用大很多。<br/>2、总是有一部分消失的内存无法找出具体是哪部分占用。<br/>3、同一业务同一 JDK 版本，切换 OS 或容器化改造之后，才出现了 1、2 现象。</p><p>虽然 Java 工具千千万，但是选用什么工具排查起这类 Java 内存问题也是一个头疼的问题；甚至有时候翻遍了工具百宝箱，最后还是没有排查出问题的根因。经历过这些问题的洗礼之后，我们也从中总结了一些排查思路，并沉淀成一个阿里云操作系统控制台的 Java 内存诊断功能，帮助用户结合应用和操作系统的角度，快速揪出 Java 应用内存占用的元凶。</p><h3>消失的 Java 内存</h3><p>Java 内存组成<br/>为了找出消失的内存，我们首先要了解 Java 进程的主要内存组成以及现有工具和监控主要覆盖的部分；如下图所示可分为：</p><p>JVM 内存</p><ul><li>堆内存：可通过 -Xms/-Xmx 参数控制，内存大小可通过 memorymxbean 等获取。</li><li>堆外内存：包括元空间、压缩类空间、代码缓冲区、直接缓冲、线程栈等内存组成；它们分别可以通过-XX:MaxMetaspaceSize（元空间）、-XX:CompressedClassSpaceSize （压缩类空间）、 -XX:ReservedCodeCacheSize（ 代码缓冲区）、-XX:MaxDirectMemorySize (直接缓冲)、-Xss（线程栈）参数限制。</li></ul><p>非 JVM 内存</p><ul><li>JNI本地内存：即通过本地方法接口调用 C、C++ 代码（原生库），并在这部分代码中调用 C 库（malloc）或系统调用（brk、mmap）直接分配的内存。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437061" alt="图片" title="图片"/></p><h3>常见 Java 内存黑洞</h3><p>JNI 内存</p><p>通过对 Java 内存组成的了解，我们其实已经可以揭开第一个容易造成内存黑洞的隐藏 Boss-JNI 内存，因为这部分内存暂时没有工具可以获取其占用大小。</p><p>通常来说，编写相关业务代码的同学会认为代码中没有使用本地方法直接调用 C 库，所以不会存在这些问题，但是代码中引用的各种包却有可能会使用到 JNI 内存，比如说经典的使用 ZLIB 压缩库不当导致的 JNI 泄漏问题[2]。</p><p>LIBC 内存<br/>熟悉 Java 的同学都知道，JVM 是由 C++ 编写的，JVM 调用 malloc、free 申请/释放内存的过程中其实还要经过一个二道贩子 libc 库；以 gibc 中默认的内存分配器 ptmalloc 为例：</p><p>chunk 是 glibc 堆内存分配的最小单位，表示一段连续的内存区域。ptmalloc 会为每一个线程维护一个 Arena，每一个 Arena 中会有一个大 chunk（Top chunk）和 chunk 的缓存集合（bins）；当应用通过 malloc、free 申请/释放内存时、ptmalloc 会优先从 bins 中拿取和释放 chunk，如果没有符合要求的 bins，再从 top chunk 里面获取、再没有就通过 brk、mmap 系统调用从 OS 获取。</p><p>从上面的流程，我们可以发现，libc 作为二道贩子，很有可能多申请、扣留一些内存，从而导致 JVM 内存和进程实际内存的差异；我们也总结一下 libc 常见的一些问题：</p><ul><li>多线程 64M Arena 内存占用，libc 会为每个线程创建一个 64M 大小的 Arena，默认配置下在线程数量较多时会导致一定的内存浪费 [3]。</li><li>Top chunk 由于内存空洞导致无法及时释放回 OS [4]。</li><li>bins 缓存，JVM 释放的内存被缓存在 bins 中，导致内存差异 [4]。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437062" alt="图片" title="图片" loading="lazy"/></p><p>透明大页<br/>到了 OS 层，Linux 中的透明大页（Transparent Huge Page）机制也是造成 JVM 内存和实际内存差异的一大元凶。简单来说，THP 机制就是 OS 会将 4kb 页变成 2M 的大页，从而减少 TLB miss 和缺页中断，提升应用性能，但是也带来了一些内存浪费。如应用申请了一段 2M 的虚拟内存，但实际只用了里面的 4kb，但是由于 THP 机制，OS 已经分配了一个 2M 的页了[5]。</p><h3>通过阿里云操作系统控制台揪出Java内存占用元凶</h3><p>操作系统控制台是阿里云推出的一站式运维管理平台，充分结合了阿里在百万服务器运维领域的丰富经验。集成了监控、系统诊断、持续追踪、AI 可观测、集群健康度和 OS Copilot 等核心功能，专门应对云端高负载、网络延迟抖动、内存泄漏、内存溢出(OOM)、宕机、I/O 流量分析及性能抖动等各种复杂问题。</p><p>下面将以汽车行业客户在从线下 idc 集群迁移至云上 ACK 集群时遇到的由于 JNI 内存泄漏导致 Pod 频繁 OOM 为例，介绍如何通过操作系统控制台的内存全景分析功能[5]来一步步找出 Java 内存占用的元凶。</p><p>背景：</p><p>客户云上多个集群多个服务中的一些 Java 业务 pod 在没有任何服务异常、请求异常、流量异常的迹象下会偶发 OOM，且从 jvm 监控上内存也没有很大的波动（客户设置 5G limit，正常水位在 3G 左右）。</p><p>排查过程：</p><ul><li>尝试在内存高水位时对 Pod 发起内存全景分析，我们可以了解到当 Pod 中容器内存使用已经接近 limit，从诊断结论和容器内存占用分析中，我们可以看到容器内存主要是由于 Java 进程内存占用导致。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437063" alt="图片" title="图片" loading="lazy"/></p><p>对 Java 进程发起内存分析，查看诊断报告。报告展示了 Java 进程所在 Pod 和容器的 rss 和 WorkingSet（工作集）内存信息、进程 Pid、JVM 内存使用量（即 JVM 视角的内存使用量）、Java 进程内存使用量（进程实际占用内存），进程匿名用量以及进程文件内存用量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437064" alt="图片" title="图片" loading="lazy"/></p><p>通过诊断结论和 Java 内存占用饼图我们可以发现，进程实际内存占用比 JVM 监控显示的内存占用大 570M，全都由 JNI 内存所贡献。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437065" alt="图片" title="图片" loading="lazy"/></p><p>开启 JNI（Java Native Interface）内存分配 profiling，报告列出当前 Java 进程 JNI 内存分配调用火焰图，火焰图中为所有分配 JNI 内存的调用路径。（说明：由于是采样采集，火焰图中的内存大小不代表实际分配大小）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047437066" alt="图片" title="图片" loading="lazy"/></p><ul><li>从内存分配火焰图中，我们可以看到主要的内存申请为 C2 compiler 正在进行代码 JIT 预热；</li><li>但是由于诊断的过程中没有发现 pod 有内存突增；所以我们进一步借助可以常态化运行的 Java CPU 热点追踪功能[7]尝试抓取内存升高时的进程热点，并通过热点对比[8]尝试对内存正常时的热点进行对比。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437067" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437068" alt="图片" title="图片" loading="lazy"/></p><ul><li>通过热点栈和热点分析对比，发现内存突增时间点的 CPU 栈也是 c2 compiler 的JIT 栈，且 c2 compiler 热点前有部分业务流量突增，且业务代码使用了大量反射操作（反射操作会导致 c2 compiler 进行新的预热）。</li></ul><p>排查结论：</p><p>C2 compiler JIT 过程申请 JNI 内存，且由于 glibc 内存空洞等原因导致申请内存放大且延时释放。</p><p>缓解方法：</p><p>1.调整 C2 compiler 参数，让其编译策略更保守，可以尝试调整相关参数，观察内存消耗变化。</p><p>2.调整 glibc 环境变量 MALLOC_TRIM_THRESHOLD_，让 glibc 及时将内存释放回操作系统。</p><p>联系我们 </p><p>您在使用操作系统控制台的过程中，有任何疑问和建议，可以搜索群号：94405014449 加入钉钉群反馈，欢迎大家扫码加入交流。</p><p>相关链接：</p><p>【1】阿里云操作系统控制台PC端链接：<a href="https://link.segmentfault.com/?enc=8Tg0g%2FnICAG4WHVYp5BLfw%3D%3D.Xw%2Bcb2tEEhHHKsYvrQIqqC%2B6Iic9eCeFN3x%2BYihqdd5gwHZmoj3XbI6041cJN8HA" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p>【2】java.util.zip内存泄露：<a href="https://link.segmentfault.com/?enc=1hTGkkUR8LNQ6UCng1igww%3D%3D.MiPVJ8IvJeVh7%2FlcaP06w4JPdY0x68DU4gSnnRj3tomzc9fW%2FwVNSDlnOX3Ldt9j" rel="nofollow" target="_blank">https://bugs.openjdk.org/browse/JDK-8257032</a></p><p>【3】glibc 64M arena内存浪费：<a href="https://link.segmentfault.com/?enc=8oJ5IiTuLetkUKRg7qjZYA%3D%3D.lbW5m%2BwSbTOS5cqqdoZz%2B%2FhCGgrxEEIQZHtysOtyZRJGGXnmYHUn6f4rP2UrG0gL" rel="nofollow" target="_blank">https://bugs.openjdk.org/browse/JDK-8193521</a></p><p>【4】glibc top chunk/fast bin内存不回收：<a href="https://link.segmentfault.com/?enc=1YvEEop8mZ1DfwmPjt7Gow%3D%3D.x1c2f9XrwRNFMI1RupovTjEjV2a%2FxKhoUJ0MHiU6ROcSSdZIciptSN%2FxiWr3RvJTolLF7rc1FxvVYTuNZkrJOFs55kFcCt775iXf7H4Ya9MiBEIfeWKkXVJBoxShDWIK" rel="nofollow" target="_blank">https://wenfh2020.com/2021/04/08/glibc-memory-leak/#332-fast-...</a></p><p>【5】go应用由于thp导致内存膨胀：<a href="https://link.segmentfault.com/?enc=m4Rc6t4yochtDoHMRqnR%2FA%3D%3D.BqTMjdFs4krcTZ8gHy2SqxqmuCNrZrMGlRxGrrBdaguBwvLlkGl7DAJ1hdXpJtB8" rel="nofollow" target="_blank">https://github.com/golang/go/issues/64332</a></p><p>【6】操作系统控制台内存全景分析：<a href="https://link.segmentfault.com/?enc=xSd6ffYjgwyHUw3sdKb4UQ%3D%3D.BeG7NA3ihFzjUToD2ylQW6IgAgT1cG7B2mKQaJIbFXyTFJ52sSaXac5b1rUMWhK%2FYCT8YX0%2FpMuJjaJLCKnPR99Fzl651SL46sjq%2FjF4sIoy0JDvOvHh64yhuq8Ud7KMzl1qzQc4P6I7oOdRW%2FmMtKLwAo2Ds%2BV3EQl9NuQZSjE%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/memory-panorama-...</a></p><p>【7】操作系统控制台热点追踪：<a href="https://link.segmentfault.com/?enc=RQaJ3yClBTPPahDcmRYp5Q%3D%3D.YlxKfuI8751UCeBWW1yryzhY%2BKXk%2FomTKb31Asz0wUq3DUWI9MeyaC5DlpBjNROgKT4nESetcPN9zdDUzUSjvGROCopvktWdK2Wr65iD5bYvq5%2F%2BXNwxhIWICBIgjShbtYhOGY%2FDVVJgYYCGoE1BzscCliTJMIPGg8UOzDihhahanc3U%2BuvMRU30q3EOcBmLy7q0mFud9be%2FILv23vav4PCNOn03EThAv8X4Dukl8zaOYLdYUODHnWeAhWicgxdJ" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/process-hotspot-...</a></p><p>【8】操作系统控制台热点对比分析：<a href="https://link.segmentfault.com/?enc=jVo9LBH%2B7SGFyzCxUrCrxQ%3D%3D.kHe5dKr7BEl49I67Kn8FTGQBwXV5FeOcXHKmQ%2Fv58JvQRC1ofe7CxeIE1%2ByFZeVPjkR%2BrWCOPKJXP%2FFnXwWcIgkSL%2F4MD5VNmtvt6FUkVg3Bnv8iI8RT1Xc%2BqE0oXPYVBdeLtIF8acRs136vz7%2FgT0LUbRpxQ1aA6%2FsbH%2FhpBVcuyH1Z9EPzZNsxhsSLEkEERxAy4zt35Ndi0lDp2Mgam%2FnNFPn%2Ffp8ITViCQqzXNlW1CD%2F0y8n%2FkxlI7N2tx%2BZT" rel="nofollow" target="_blank">https://help.aliyun.com/zh/alinux/user-guide/hot-spot-compara...</a></p>]]></description></item><item>    <title><![CDATA[告别「耗时无果」：迭代构建AI知识库，帮]]></title>    <link>https://segmentfault.com/a/1190000047437078</link>    <guid>https://segmentfault.com/a/1190000047437078</guid>    <pubDate>2025-11-28 19:02:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437080" alt="图片" title="图片"/><br/>作者：王传阳<br/>枫清科技（Fabarta）技术合伙人</p><h3>现状与挑战</h3><p>企业在构建AI应用时，通常会由业务部门负责构建相关的业务知识库。业务部门在构建AI知识库时，普遍面临两大核心挑战：其一，对AI知识库与传统知识库的本质差异认知不足，缺乏适配AI语义理解的知识梳理方法，导致知识应用准确率难以达标；其二，存在 “一劳永逸” 的认知误区，倾向于耗时数月构建覆盖全场景的 “大而全” 知识库，既造成资源浪费，又因需求变化导致知识失效，无法快速响应业务需求。</p><h3>企业AI应用的知识库构建核心理念 - 迭代</h3><p><strong>一、迭代理念的核心内涵</strong></p><p>AI 应用语境下的迭代，是指打破 “一次性构建” 的传统思维，通过场景拆分 - 小库验证 - 逐步扩展的路径，实现知识库的动态优化。具体而言，先聚焦单一细分场景，构建最小化可用知识库与配套测试集，在验证准确率达标后，再逐步扩展场景边界、补充知识内容、升级测试标准，最终形成覆盖完整需求的知识体系。这种模式下，场景迭代与知识库迭代深度绑定，知识库的每一次优化都直接服务于应用效果提升，而应用反馈又反向驱动知识完善。</p><p><strong>二、迭代的起点：场景与知识的精准拆分</strong></p><p>迭代的关键起点是“化整为零”：针对计划构建的完整应用，按业务流程、用户需求或知识领域完成场景拆分，同步将全域知识拆解为相互独立的知识模块，选取其中一个高频、低复杂度的场景作为切入点，形成“小应用 + 小知识库” 的初始组合。同时，需围绕该小应用的核心功能，构建包含高频问题、边缘案例、歧义场景的效果测试集，明确准确率衡量标准。</p><p><strong>三、知识库迭代的实施路径</strong></p><p><strong>1.首轮迭代：最小知识库的打磨</strong><br/>业务部门围绕初始场景，收集整理相关知识文档（政策文件、操作手册、FAQ 等）并上传至平台。这一阶段的核心是 “精准适配”：通过多轮优化提升测试集准确率，具体包括：</p><ul><li>内容完善：补充缺失的关键信息，删除冗余无效内容；</li><li>结构优化：按“核心概念 - 操作流程 - 常见问题” 分层组织文档，优化目录层级；</li><li>格式适配：统一文档格式（优先 word/pdf 等可编辑格式），处理图片、表格等多模态内容；</li><li>实践积累：记录每轮优化的有效方法，形成专属最佳实践库。</li></ul><p><strong>2.迭代扩展：从单一场景到全域覆盖</strong><br/>当首轮迭代的准确率达标后，小应用即可上线产生业务价值。随后启动扩展迭代：</p><ul><li>功能扩展：新增关联场景功能（如从“信用卡申请查询” 扩展至 “信用卡还款咨询”）；</li><li>知识扩容：纳入新增场景的相关知识，必要时联动更多业务部门；</li><li>实践复用：将首轮积累的最佳实践直接应用于新场景，降低跨部门协作成本；</li><li>测试升级：扩展测试集覆盖新场景的业务逻辑，保持准确率标准一致性。</li></ul><p><strong>3.全域收敛：形成完整知识体系</strong><br/>经过多轮场景扩展与知识补充，逐步整合所有细分模块，通过统一的知识管理规范（如术语统一、结构对齐）实现全域知识的协同，最终完成完整应用的构建。</p><p><strong>四、迭代式构建的核心优势</strong></p><p>1.降低准入门槛：无需业务部门一次性掌握所有AI知识梳理技巧，在小场景实践中快速积累经验；<br/>2.快速产生价值：小应用可在短期内上线，避免“长期投入无产出” 的困境；<br/>3.提升构建效率：最佳实践的复用，减少跨部门协作中的重复试错，据行业案例验证，可降低 30% 以上的知识整理成本；<br/>4.保障效果稳定：每轮迭代均以测试集准确率为核心目标，避免全域上线后出现大规模准确率问题；<br/>5.适配需求变化：迭代过程中可灵活调整知识内容，应对业务政策、用户需求的动态变化。</p><p><strong>五、金融行业迭代构建案例分享</strong></p><p>某金融企业需构建覆盖“信贷、理财、支付、保险、财富管理”5 大领域的智能问答应用，涉及 5 个业务部门，迭代路径如下：</p><p>1.首轮迭代：聚焦“个人信贷” 单一领域选取高频场景“个人住房贷款申请咨询” 作为起点，由信贷部门独立负责知识库构建：</p><ul><li>知识范围：房贷申请条件、所需材料、审批流程、利率计算 4 类内容；</li><li>最佳实践积累：<br/>格式规范：优先上传可编辑 PDF/word 文档，避免扫描件（OCR 识别误差率降低 40%）；<br/>多模态处理：贷款流程图前后需添加文字说明（如“下图为房贷审批全流程，其中面签环节需携带以下材料：……”）；<br/>业务描述：利率政策需明确“计息周期 + 适用人群 + 调整规则”（如 “首套房年利率 3.6%（按年计息），适用于无逾期记录的刚需购房者，每年 1 月 1 日调整”）；<br/>分段优化：操作流程类文档按 1024 字符分段，核心概念类按 512 字符分段，重叠字符数设置为 100；<br/>提示词配置：添加“严格依据文档内容回答，无相关信息时回复‘暂无对应政策说明’” 的系统指令。</li></ul><p>经过 3 轮优化，测试集准确率从 65% 提升至 92%，“房贷申请咨询” 子应用上线。</p><p>2.二次迭代：扩展至 3 个业务领域新增“理财”“支付” 两大领域，由理财部、支付部加入协作：</p><ul><li>复用实践：直接套用信贷部门积累的格式规范、分段规则、多模态处理方法，理财部快速完成基金产品说明书的结构化处理，支付部顺利整理跨行转账流程文档，未出现“图片无法识别”“分段混乱” 等初期问题；</li><li>协同优化：统一三大领域的术语规范（如“‘年化收益率’统一表述为‘七日年化收益率’”），补充跨领域关联知识（如 “理财赎回资金的支付到账时效”）；</li><li>价值延续：原“房贷咨询” 子应用持续稳定运行，新增的 “理财产品查询”“转账问题咨询” 子应用经 2 轮迭代后准确率达标上线。</li></ul><p>3.三轮迭代：完成 5 大领域全覆盖纳入“保险”“财富管理” 领域，5 个业务部门协同推进：</p><ul><li>实践升级：基于前两轮经验，制定《金融知识 AI 适配统一规范》，明确保险条款需按 “保障范围 - 免责条款 - 理赔流程” 分层，财富管理方案需标注 “风险等级 + 适配人群”；</li><li>知识融合：构建跨领域知识关联（如“保险理赔资金的理财建议”“信贷客户的财富管理方案推荐”）；</li></ul><h3>总结</h3><p>回顾企业业务部门构建 AI 知识库的初始困境：对 “AI 与传统知识库差异” 的认知盲区，让知识梳理陷入 “无方法、低准确率” 僵局；“一步到位建大库” 的误区，导致资源空耗与需求错配。而迭代式构建理念恰恰提供破局路径 —— 通过 “小场景起步 + 实践积累”，业务部门无需初期掌握复杂 AI 技巧，在打磨小知识库中摸清语义理解规律，破解准确率难题；通过 “分阶段扩展 + 快速上线”，避免全场景一次性构建浪费，让小应用快速产生价值，灵活应对需求变化。</p><p>枫清科技企业知识中台产品的功能体系，与迭代式构建的全周期精准适配，为业务部门提供全流程支撑：</p><ul><li>在场景与知识拆分阶段，针对“化整为零” 需求，通过无代码智能体应用工具快速创建小应用，搭配标签管理按业务域 / 场景划分知识模块，同步用问答集管理构建测试集。</li><li>在首轮知识库打磨阶段，围绕多模态适配与准确率验证，提供文档管理（支持 30 + 格式解析，含图文增强处理）、高级切片设置（自定义字符大小 / 重叠度）与在线调试功能。</li><li>在迭代扩展阶段，针对跨部门协作与新场景接入，通过组织管理划分协作单元、知识库关联功能快速对接新领域知识，搭配权限标签实现“理财部仅见基金知识” 等安全隔离需求。</li><li>在全域收敛阶段，依托统一语义层整合多模态知识、词表管理统一跨域术语，结合知识运营监控全域效果。</li></ul><p>枫清科技深度依托多行业客户交付经验持续打磨知识中台产品，通过客户实践反哺产品优化，优化后产品再赋能更多客户，实现产品与客户的共同成长。</p>]]></description></item><item>    <title><![CDATA[全球首个语音 AI 广告平台问世；Sam]]></title>    <link>https://segmentfault.com/a/1190000047437082</link>    <guid>https://segmentfault.com/a/1190000047437082</guid>    <pubDate>2025-11-28 19:02:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437084" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@鲍勃 和 Gemini（尽量不生产 AI Slop）</em></p><h2>01 有话题的技术</h2><p><strong>1、Vision Agents + Gemini + Ultralytics YOLO 构建 AI 语音瑜伽教练</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437085" alt="" title="" loading="lazy"/></p><p><strong>Vision Agents</strong> 发布了一个教程，将 LLM、实时视频分析和语音转语音 API 相结合，创建一个名为「AI 瑜伽教练」的交互式 Python 应用。该教练可以通过用户的摄像头分析瑜伽姿势，并提供实时的语音指导和反馈，旨在革新居家和健身房的锻炼体验。</p><ul><li><strong>实时姿势分析与反馈：</strong> 利用 Ultralytics YOLO 模型实时检测用户姿势，并通过 Gemini LLM 进行分析，提供即时语音反馈以纠正错误和改进动作。</li><li><strong>全栈语音交互：</strong> 集成 Gemini Live API，实现低延迟的语音转语音交互，用户可以通过语音与 AI 教练进行自然对话。</li><li><strong>易于集成的框架：</strong> Vision Agents 提供了一个开源框架，简化了语音和视频 AI 应用的开发，支持多种第三方 AI 模型和服务集成。</li><li><strong>多场景应用潜力：</strong> 该模式不仅限于瑜伽教练，还可以扩展到体育指导、物理治疗、无人机监控等需要实时视频分析和语音交互的领域。</li><li><strong>快速原型开发：</strong> 通过预置的插件和简单的 Python 配置，开发者可以快速构建和部署 AI 语音助手。</li></ul><p>教程：</p><p><a href="https://link.segmentfault.com/?enc=4XZjYJfUITPj%2Fm1GDCKDHg%3D%3D.XlxX8OSqqXukjVK6scFH1LAz44lmqqq2WgcsGn7Hux7Lp70XpmELwPkg6h2fXLhfpvlDR3UsuAhvUm3vt%2FgrSg%3D%3D" rel="nofollow" target="_blank">https://getstream.io/blog/ai-voice-yoga-instructor/</a></p><p>(@Vision Agents Blog)</p><h6><strong>2、巨人与清华、西工大发布 「视频 X 音乐」 多模态生成新进展</strong></h6><p>多模态生成技术在图像、视频、语音等方向的快速突破，使 「视频 × 音乐」 的多模态生成变成新的研究热点。然而在真实业务场景中，仍然存在诸多未被充分解决的技术空白，例如：</p><ul><li>在音乐驱动的视频生成中，仍缺乏对长时序一致性、音画节奏对齐与镜头运动的系统建模；</li><li>歌声转换（SVC）方面，在大量真实歌曲输入下仍面临音色稳定性不足、和声干扰导致破音等业界难题；</li><li>歌声合成（SVS）场景，缺乏能够在零样本条件下稳健适配不同歌词长度与旋律结构的模型。</li></ul><p>在此背景下，巨人网络 AI Lab 继 2024 年发布 YingGame 有声游戏生成模型之后，继续在多模态领域发力，本次联合清华大学与西北工业大学推出三项研究成果：<strong>YingVideo-MV、YingMusic-SVC 与 YingMusic-Singer</strong>，分别面向音乐驱动的视频生成、歌声转换与歌声合成任务，完善了真实业务场景中多项关键能力链路，为 「视频 × 音乐」 的多模态生成方向带来了系统性的技术进展。</p><p>详细介绍：</p><p><a href="https://link.segmentfault.com/?enc=j9MEx9WwgtjDwNInMKptYA%3D%3D.yje4F7uwKu3D%2BI2ngt8V3Ts1EWzuEz6axqhWQ%2FVDYLGlWDqZp5C1tFpo5XzUyqBXKOJNUrkTA%2BNWZJnIHmYZTA%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/r8de9g9tGFbgk466i8-2Gg</a></p><p>（@巨人网络 AI Lab）</p><p><strong>3、All Voice AI 联手 Factory Berlin 推出全球首个语音 AI 广告平台，将 AI 电话变为收入渠道</strong></p><p>U.S. 公司「All Voice AI」与欧洲创投机构「Factory Berlin」合作，推出了全球首个能在实时 AI 语音通话中嵌入广告的平台。这项技术旨在将传统的客户支持电话转变为可直接变现的收入渠道，通过在对话中适时推送相关优惠，为品牌开辟了全新的互动广告模式。</p><ul><li>实时对话内广告：该平台能在用户与 AI 进行语音通话时，根据上下文实时插入相关的促销或优惠信息。所有广告均为用户选择性加入 （opt-in），旨在确保良好的用户体验。</li><li>将成本中心转为收入中心：传统上，语音通话多为客户支持等成本部门。该平台通过广告变现，帮助企业将这一渠道转变为新的收入来源，目前已支持 57 种语言。</li><li>高接受度与市场潜力：「All Voice AI」的调查显示，高达 97% 的受访者表示，如果广告内容与他们的通话目的相关，他们愿意在电话中接收此类优惠信息，这预示着巨大的市场潜力。</li><li>技术生态与专利保护：该平台技术与「OpenAI」、「Twilio」和「AWS」等巨头合作。其核心创新已申请专利保护，并提供技术授权给商业合作伙伴。</li></ul><p>(@PR Newswire)</p><hr/><h2>02 有亮点的产品</h2><p><strong>1、Sam Altman 和 Jony Ive 透露合作硬件：「如湖畔山间小屋般平静」</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437086" alt="" title="" loading="lazy"/></p><p>品玩 11 月 27 日讯，据 TechCrunch 报道，OpenAI CEO Sam Altman 与苹果前首席设计师 Jony Ive 近日在旧金山 Emerson Collective 活动上透露，双方合作的 AI 硬件设备已进入原型阶段，预计两年内面世。</p><p>该设备被描述为「无屏幕、口袋大小」，强调极致简约与宁静体验。Altman 称其愿景是打造一款如「湖畔山间小屋般平静」的产品，能长期理解用户情境、主动过滤干扰，并赢得用户信任。Ive 表示，理想设计应「看似天真简单」，却内含高度智能，让人无负担地自然使用。</p><p>目前 OpenAI 尚未公布具体技术细节。</p><p>（@品玩）</p><p><strong>2、阿里发布 AI 眼镜夸克 S1，双目 AR 光波导+AI 拍摄</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437087" alt="" title="" loading="lazy"/></p><p>继小米、百度之后，国内第三家科技互联网巨头发布 AI 眼镜。</p><p>11 月 27 日，阿里夸克在北京举办「先见之明」新品发布会，正式发布了「夸克 AI 眼镜 S1」。作为阿里 AI 战略中的关键落子，夸克 S1 在技术路径的选择上并未采用保守的 ODM 通用方案，而是展现出了极强的「创新欲」：搭载「千问」对话助手、采用双芯片架构、双目 AR 光波导与 AI 拍摄结合的高集成度方案。</p><p>这种从底层技术逻辑出发的产品定义，让夸克 AI 眼镜 S1 与此前小米、百度等厂商推出的以「拍摄」为主的 AI 眼镜形成了显著差异。它不局限于单一的影像捕捉，而是基于「近眼显示」能力，将阿里庞大的服务生态通过 AI 多模态形式延展至眼镜端。更为难得的是，在堆叠如此复杂硬件的同时，整机重量依然被控制在了 51g（含 0 度近视镜片）。</p><p>(@VR陀螺)</p><p><strong>3、Gloo 收购 XRI Global，开发全球数千种语言 AI 模型</strong></p><p>Gloo 公司，一家技术平台，近日宣布收购 AI 公司 XRI Global。此次战略性举动旨在将其平台 Gloo AI 和 Gloo360 嵌入 XRI Global 先进的多语言和语音 AI 能力，从而大幅拓展 Gloo 的市场潜力，并赋能全球数千种语言的用户。</p><ul><li>战略性技术整合： Gloo 收购 XRI Global，旨在整合其在多语言和语音 AI 领域的尖端技术，特别是针对「低资源语言」的创新。</li><li>弥补 AI 语言鸿沟： XRI Global 专注于开发能够覆盖全球数千种语言（包括缺乏训练数据或书写系统的语言）的 AI 模型，旨在解决目前全球约 6,800 种语言未被现代 AI 创新覆盖的问题。</li><li>独特方法论与成果： XRI Global 拥有一套经过研究验证的方法论，已在过去 18 个月内为 30 多种语言构建了 AI 模型。其团队由来自 Meta 和 Google 的机器学习、计算语言学博士组成。</li></ul><p>(@Gloo)</p><p><strong>4、迪士尼下场做机器人，把《冰雪奇缘》雪宝（Olaf）从电影里硬生生地「拽」了出来！</strong></p><p>（@香港迪士尼乐园度假区、@机器人前瞻）</p><h2>03 有态度的观点</h2><p><strong>1、「AI slop」（AI 劣质内容）当选澳洲 2025 年度词汇</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437088" alt="" title="" loading="lazy"/></p><p>据澳洲九号台新闻 11 月 25 日报道，《Macquarie Dictionary》宣布「AI slop」成为 2025 年度词汇，用以描述由生成式人工智能大量生产的、缺乏意义且充满错误的低质量内容。</p><p>该词语的入选，反映了公众对人工智能技术滥用现象的关注日益增强。</p><p>每年，《Macquarie Dictionary》都会组织特别委员会评选年度词汇。今年的评审成员包括词典编辑团队、广播主持人兼作家 David Astle，以及语言研究专家 Tiger Webb。</p><p>委员会指出，2025 年「我们已理解『slop』的含义——即无意义、无用途的 AI 产物」，并进一步提出：「那些摄取并传播这类内容的人，是否也将被称作『AI sloppers』？」</p><p>除「AI slop」外，今年的荣誉提名还包括「clanker」，用于贬义地称呼取代人类完成任务的 AI 机器人，以及「medical misogyny」，意指医疗和知识体系中，特别是在女性生殖健康领域存在的性别偏见。</p><p>这些新词汇反映出科技发展、社会结构与公共意识在过去一年中的显著变化。</p><p>（@澳洲九号台）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437089" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437090" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=MVb5pC5GCzFkmvZ1lSwS9A%3D%3D.pluk4ChLbsVZWWJ3NLAG2Pa4VIO06Czlja1RzL7%2FISc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437091" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[重磅揭晓！「2025龙蜥社区年度优秀贡献]]></title>    <link>https://segmentfault.com/a/1190000047437117</link>    <guid>https://segmentfault.com/a/1190000047437117</guid>    <pubDate>2025-11-28 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047437119" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[promise 的三个方法 freema]]></title>    <link>https://segmentfault.com/a/1190000047436814</link>    <guid>https://segmentfault.com/a/1190000047436814</guid>    <pubDate>2025-11-28 18:13:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>promise.race\promise.all\promise.allSettled <br/>需要所有操作都成功才能继续时，用 Promise.all。<br/>只关心最快出结果的那个操作（比如设置超时），用 Promise.race。<br/>需要知道每个操作的最终状态（无论成功失败），比如批量处理任务后生成报告，用 Promise.allSettled。</p><pre><code>let params1 = {
        dataType: 19, //高频
        deviceId: this.current.vin,
      };
      let params2 = {
        dataType: 20, //低频
        deviceId: this.current.vin,
      };
      let params3 = {
        dataType: 120, //周期数据
        deviceId: this.current.vin,
      };
      let params4 = {
        dataType: 122, //事件数据
        deviceId: this.current.vin,
      };
      let p1 = this.findAllUploadTradReq(params1, "highFreData");
      let p2 = this.findAllUploadTradReq(params2, "lowFreData");
      let p3 = this.findAllUploadTradReq(params3, "cycleData");
      let p4 = this.findAllUploadTradReq(params4, "eventData");
      Promise.all([p1, p2, p3, p4])
        .then((res) =&gt; {
          let [highFreData, lowFreData, cycleData, eventData] = res;
          // console.log(highFreData,lowFreData,cycleData,eventData);
          if (highFreData || lowFreData || cycleData || eventData) {
            this.tableList[0].lowFreData = "已上传"; // 燃油车检测成功
            if (
              this.current.bleVersion == "2.0" &amp;&amp;
              this.current.bleThreeValueStatus === "0"
            ) {
              this.checkingText = "蓝牙检测中...";
              // this.disconnect();
              this.connect();
              return;
            }
            this.unqualified(2);
            return;
          } else {
            this.tableList[0].highFreData = "未上传";
            this.tableList[0].lowFreData = "未上传";
            this.tableList[0].cycleData = "未上传";
            this.tableList[0].eventData = "未上传";
            let temp = JSON.parse(JSON.stringify(this.tableList[0]));
            this.$set(this.tableList, 0, temp);
            this.unqualified(1);
            this.prolineErrMsg = "燃油车数据检测未通过！";
          }
        })
        .catch((err) =&gt; {
          this.unqualified(1);
          console.log(err);
        });
    },</code></pre>]]></description></item><item>    <title><![CDATA[MES 系统到底管什么？11大核心模块、]]></title>    <link>https://segmentfault.com/a/1190000047436817</link>    <guid>https://segmentfault.com/a/1190000047436817</guid>    <pubDate>2025-11-28 18:12:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>很多朋友都是从《什么是MES，MES系统主要包括哪些功能？》和《为什么MES难以标准化？》这几篇内容关注的我。那么今天我就再来发挥余热，和大家好好聊聊本人对于MES的全新观点。希望能帮助大家更透彻的理解MES。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436819" alt="image.png" title="image.png"/></p><p>在全民工业智能化时代，AI人工智能的兴起，让每个公司都热衷于用不同的方式推进机器换人与智能生产的落地。而在这一进程中，制造执行系统（MES）起到很大的作用。</p><p>谈及MES必须先谈生产（即“生产体系模型”，如下图所示），生产体系模型设计到人、财、物、信息等资源，产、供、销等环节，以及供应商、客户与合作伙伴等多个维度。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436820" alt="image.png" title="image.png" loading="lazy"/></p><p>其中，生产管理是通过对生产系统的战略计划、组织、指挥、实施、协调、控制等活动，实现系统的物质变换、产品生产、价值提升的过程。企业要善于运用“计划—执行—检查—处理”（P-D-C-A）的循环机制，持续推进生产运营战略的设计、实施、控制与优化，从而准确分析问题、追溯根源、制定对策并实现生产过程的持续改善。</p><p>作为企业价值链中的主要环节，生产管理一直是构成企业核心竞争力的关键内容。而MES作为企业资源计划（ERP）系统与底层控制系统之间的桥梁，整合了各类工厂管理系统，是提升企业制造能力与生产管理水平的重要工具。其功能覆盖生产计划与调度、物料平衡与物流管理、库存控制、工艺技术管理、过程监控、质量管理、健康安全环境（HSE）、设备管理、能源管理、成本控制以及绩效管理等多个方面。</p><h2>一、什么是MES？为何要实施MES？</h2><p>核心定义：</p><p>MES（Manufacturing Execution System）即制造企业生产过程执行系统，是一套面向制造企业车间执行层的生产信息化管理系统。MES可以为企业提供包括制造数据管理、计划排产管理、生产调度管理、库存管理、质量管理、人力资源管理、工作中心/设备管理、工具工装管理、采购管理、成本管理、项目看板管理、生产过程控制、底层数据集成分析、上层数据集成分解等管理模块，为企业打造一个扎实、可靠、全面、可行的制造协同管理平台。</p><p>历史争论：</p><p>由于MES一词相对其他信息系统在中国流行较晚。很多企业决策者开始常提到：为何实施MES？外国也有同论：Why MES？因为许多规划是先实施ERP，后实施MES。因此产生了为ERP实施MES的结论。其实不尽然。</p><p>核心价值：</p><p>MES系统是在过程控制层面以上，ERP系统之下，起到承上启下作用的重要企业信息管理系统。MES的同步数据采集技术应用于企业内部物流的全线追溯、制造工程配置、生产及品质过程控制，它能提升制造环节的透明度，填补了生产现场到计划系统间的“信息鸿沟”，为计划系统的再调整提供可以信赖的决策依据。MES提供了自演化能力的建模特性，采用螺旋式渐进的敏捷实施方法论，主动匹配并优化客户的业务过程，能显著降低系统部署风险，最大限度提升企业执行力，助其实现核心价值，并快速取得成功。</p><p>客观分析：</p><p>1、企业生存环境的客观需要</p><p>因为工厂级、车间级管理面临着新的挑战：需要上下游车间的高效沟通，信息的及时性、准确性，随时面临的计划变动，越来越小的任务单元，越来越高的质量追溯要求，管理人员成本的不断升高等。</p><p>2、企业管理的内在规律</p><p>MES从时间空间角度以及业务链条看，是企业管理中的不可或缺的领域和环节。</p><p>3、是信息技术发展的必然结果</p><p>信息技术在企业中的应用，首先是解决生产过程控制的问题，而且发展迅速。接着就是提升经营管理水平和手段，产生了以ERP为代表的一大批软件。</p><p>但是，在实施ERP的过程中，人们发现：ERP的规模大、周期长，导致ERP项目有46%逾期完成；支出多、投入大，导致41%超过预算；多种原因致使49%没有达到预期的社会效益、经济效益和目标。其中一个重要因素是与生产现场的连接与集成被忽略了，而生产现场的数据，即完美的生产信息是ERP的基础，是集成的关键。</p><p>由于ERP层和DCS层的工作是分别进行的，因此产生了两个问题：</p><ul><li>一是横向系统之间的信息孤岛（Island of Information）；<br/>二是ERP和DCS两层之间形成缺损环或断缺链接（Missing Ring or Link）。</li></ul><p>这也是催生MES的重要原因。</p><p>因此，国际上公认的信息化总体架构是原则上将框架划分为3层：PCS层、MES层、ERP层。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436821" alt="image.png" title="image.png" loading="lazy"/></p><p>PCS层为底层，以硬设备为主，主要面向操作工人，实现生产过程操作运转自动化，减少操作工人编制；</p><p>MES层为中间层，承上启下，以生产运行管理软件为主，主要面向生产管理人员，实现生产管理信息化，以及管理组织的扁平化和紧密化；</p><p>ERP层为最高层，以经营管理软件为主，主要面向经营管理和决策人员，实现经营决策管理信息化以及管理组织的扁平化和集约化。</p><p>近几年从最上层分离出决策层，演变成4层结构。突出实时制造性能监控、实时运营智能等管理理念。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436822" alt="image.png" title="image.png" loading="lazy"/></p><h2>二、MES与ERP的区别</h2><p>MES和ERP各有作用，但在制造企业运营中，两个系统又互有补足，可以互相整合。二者的主要区别有：</p><p>1、管理目标不同</p><p>ERP的重点在于销售、订单、供应链、财务，也就是从业财一体化的角度出发，来对企业的资源进行计划，相关的模块也是以人员、业务、财务为核心展开，最终的管理数据是集中到经营报表上。</p><p>MES的重点在于制造，也就是以产品质量、准时交货、设备利用、流程控制等作为管理的目标。因为不同的企业管理重点不同，在选择信息系统的组成时，重点也不同。集团公司、商业企业、物流企业等更着重于ERP管理，而制造企业更需要MES管理。</p><p>2、管理范围不同</p><p>ERP的管理范围较大，涉及采购、财务、销售、订单管理、发运管理、成品仓储计划控制等计划层面，主要对企业资源进行有效共享与利用，使企业资源在购、存、产、销、人、财、物等各个方面能够得到合理地配置与利用，但是不够详细具体。</p><p>MES管理比ERP细致，主要涉及车间的工单派发、制程防错、产品谱系、SPC质量分析、设备数据分析、制程追溯等执行层面，能更细致到每个制造工序，对每个工序进行任务的下达、执行的控制和数据采集、现场调度。</p><p>3、管理功能不同</p><p>除了财务管理、人力资源管理、客户关系管理等功能，ERP在制造管理方面的功能主要是编制生产计划，收集生产数据。</p><p>而MES除了细化生产计划和收集生产数据外，还有批次级的生产控制和调度的管理功能，例如：批次级的工艺流程变更，对制造设备、人员和物料的验证控制，批次分拆、合并，批次的生产订单变更等现场调度功能。</p><p>4、实现方式不同</p><p>ERP主要采用填写表单和表单抛转的方式实现管理，现场收到的制造任务是通过表单传达，现场制造数据也是通过填写表单完成收集。</p><p>MES是采用事件的方式实现管理，监测生产订单的变化和现场的制造情况，通过MES内置的WIP（在制品）引擎立刻触发相关事件，要求相关人员或设备采取相应的行动。因此，MES可以减少数据的输入工作，通过信息系统实现工作现场的指令下达和数据收集，减少差错，提高及时性。</p><p>5、时间周期不同</p><p>正是因为MES采取了WIP（在制品）引擎来驱动管理，能够做到现场的“实时管理”：上级生产计划和生产调度能立刻反映在制造现场的作业界面，现场的生产数据和异常情况也能实时反映在管理岗位的监督界面，在企业上下层之间提供一个双向的生产信息流，使得及时调度成为可能。</p><p>ERP的表单方式则不可避免会存在录入的周期，管理的数据以周、天为时间周期，无法对现场执行进行实时的有效管控，即所谓的在制造过程中存在“信息黑洞”，这个“信息黑洞”对制造过程的管理和控制的影响愈发不利。</p><p>6、技术要求不同</p><p>ERP主要处理计划数据，数据量小，不需要和底层硬件交互，易于采用集中的方式管理，在实施时，计划的流程相对固定。</p><p>MES的数据粒度小，数据量大，和工厂的工艺、车间管理流程、自动化程度密切相关，不同企业实施时差异很大，需要不断适应车间管理模式的变革，此外，MES系统直接记录生产的过程数据，在系统的可靠性和稳定性方面比ERP要求更高。</p><h2>三、MES的核心功能</h2><p>MES主要管理4种资源，包括生产活动中的人力资源（Personnel Resources）、生产设备（Equipment）、物料和能源（Material and Energy）以及工艺过程链（Process Segments）；在企业经营计划层面与生产过程控制层面之间，实现生产能力信息的交换、产品信息的交换、生产调度信息的交换、生产绩效信息的交换（4P交换功能）。</p><p>AMR组织定义的MES有11个功能：</p><p>（1）生产资源分配与监控；</p><p>（2）作业计划和排产；</p><p>（3）工艺规格标准管理；</p><p>（4）数据采集（装置在线连接采集实时数据和各种参数信息，控制系统接口，生成生产数据记录、质量数据、绩效信息、台账累计）；</p><p>（5）作业员工管理；</p><p>（6）产品质量管理；</p><p>（7）过程管理（过程控制、APC、基于模型的分析与模拟、与外部解析系统接口）；</p><p>（8）设备维护；</p><p>（9）绩效分析；</p><p>（10）生产单元调度；</p><p>（11）产品跟踪。</p><p>AMR组织则又把按着11个功能实现的整体解决方案称为MESⅡ（Manufacturing Execution Solution）。其中生产资源计划、排产与调度是主线。如图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436823" alt="image.png" title="image.png" loading="lazy"/></p><p>但随着近些年的发展与不断演练，业界针对大多数制造企业的实际需求，又重新梳理出一个全新的MES功能，具体如：</p><p>1、生产流程监控模块</p><p>解决问题：漏工序、生产进度难以管控、生产瓶颈问题难以发现。</p><p>通过MES系统对产品的加工流程进行实时监控，可杜绝漏工序问题，并可实时了解生产进度，发现生产瓶颈问题，保证产品质量，确保如期交货。</p><p>2、人员绩效管理模块</p><p>解决问题：人员有效工作时间、人员每日产量、每小时产能、产出良率等</p><p>MES系统人员绩效管理模块较为典型的应用为人员上岗离岗（非上班下班）刷卡记时，统计人员有效工作时间。人员在各工位通过登录系统并记录产出数量（可依据实际情况选择人工录入或自动获取），并由系统自动统计出每日总产量、产出良率等。从而实现为绩效考核和生产计划制定提供数据基础的目的。</p><p>3、智能排产模块</p><p>解决问题：资源约束，均衡生产，快速反应。优化排产，降低库存，减少成本。</p><p>智能排产模块于ERP与MES的应用基础，使得计划与生产一线信息得以实时反馈，从而解决了传统排产软件人工干预比较多的问题，使计划更科学，更合理，更准确，更方便。对接受的订单进行交期承诺，有限产能约束优化生产计划，物料计划及机组作业安排，并将结果下达到MES。同时MES又将生产线发生的实时状态反馈给计划，进行动态调整。</p><p>4、SPC统计过程控制模块</p><p>解决问题：品质预警，过程控制，品质分析。</p><p>SPC应用在现代制造中越来越广泛，随着生产力的进一步发展，大规模生产的形成，如何控制大批量产品质量成为一个突出问题，单纯依靠事后检验的质量控制方法已不能适应当时经济发展的要求，必须改进质量管理方式。而SPC统计过程控制软件则是其中的一个核心工具。SPC管理模块将SPC应用引擎内置于MES系统中，对质量管理提供一个质的飞越。</p><p>5、看板报表管理模块</p><p>解决问题：实时掌控生产及物流信息、作业指导书集中管理实时更新、增强展示效果提升企业形象等。</p><p>电子看板是用于显示当前产线生产进度、良品率等信息，以直观的形式表现在员工面前，便于员工能及时调整或改善生产环境，利于生产水平的提高；一般用TV或LED屏来作显示设备，也可利用PC来打开系统中的电子看板。</p><p>MES系统看板报表管理模块较为典型的应用包括产线看板、仓库备料看板、会客厅展示看板、工位看板及各类型的统计分板报表。其中工位看板支持DOC、XLS、PDF等多种格式文档，服务器对文档进行集中管理和设置，可展示指定文档的某一页，也可以多页轮播等。</p><p>6、智能仓储模块</p><p>解决问题：物料先进先出，收发物料效率低下，新员工收发物料特别容易出错。</p><p>作为一套完整MES系统的起点，物料收发工作所起的作用十分关键，MES系统可以在来料入库阶段通过对接点料机、打印机、扫描枪或PDA等外设，实现快速收料入库和打印条码标签。同时，系统支持与智能货架进行对接，实现快速的存放和寻找物料，提高收发物料的效率和准确度，大幅降低出错风险。</p><p>7、条码管理模块</p><p>解决问题：条码重复、条码使用混乱、标签格式多、打印机品牌型号多等。</p><p>通过在系统中统一生成和领用标签，使标签得到清晰有序的管理，从而杜绝因条码问题导致的返工。MES系统具有强大而灵活的条码规则设置功能，用户可以通过JavaScript自定义条码规则。标签的格式亦可通过所见即所得的方式自定义，支持一维码和二维码。</p><p>8、上料防错与追溯模块</p><p>解决问题：上料错误，物料追溯困难、用错PCB、用错锡膏、用错钢网等。</p><p>上料防错：通过在系统中导入站位表，上料时扫描料盘条码与站位表进行对比实现防错目的。</p><p>物料追溯：通过投板扫描及炉后扫描、分板扫描，实现追溯到产品使用了哪些物料、物料用在了哪些产品上。</p><p>9、销售管理模块</p><p>解决问题：准确快速发货，售后问题，异常处理，如何计划，如何改正。</p><p>直接按订单分拣货物，打印发货地址及配送，防止发货出错。</p><p>10、品质/质量管理模块</p><p>解决问题：如何控制（QC）预防出错，异常处理。如何计划，如何改正。</p><p>MES质量管理模块，实现了制造业务和质量管控过程的自然融合，确保了质量活动与制造过程的完美交互，制造过程中所有静态和动态的数据在系统中，随着制造业务的开展，自然而然的从各个环节被自动采集，形成庞大的制造数据集合，为质量活动的设计、执行、评价和改进提供了丰富的数据基础；系统中的质量管控、质量分析等模块对自动采集得到的海量数据进行筛选、分析与反馈控制，形成数字化为特征的企业车间质量管理体系，能够有效提高质量管理活动的执行效率，并使制造过程的质量反应能力和质量控制能力得到极大的提高。</p><p>11、设备管理、设备通讯模块</p><p>解决问题：设备的使用防错，数据收集，状态监控，保养维护、维修、报废等</p><p>设备管理模块较为典型的应用包括印刷机、SPI、贴片机、AOI、回流炉、烧录机、测试测量治具等设备的定期保养维护，稼动率分析，使用防错，数据采集，使用次数统计，部分工具的定期校验（内校或外校）等等。</p><h2>四、实施MES的关键</h2><p>1、对企业制造执行能力进行分析</p><p>MES是制造执行系统，为生产制造管理服务，旨在提高生产制造执行能力和水平。因此，实施MES前，首要问题是对现有的制造执行能力进行评估。从制造战略、制造质量、供应链协调、信息收集、绩效管理与改进、制造与IT基础设施等角度把企业的制造执行能力成熟程度划分为劣、可、良、优、未来追求目标共5个等级。使用该成熟度模型，根据企业目前状况和环境对企业的要求，对企业进行评价，找出差距，定出目标。</p><p>2、确定好功能模块</p><p>MES系统中最重要的模块——生产运行管理模块的核心，即生产计划、统计与调度。生产运行的基础是设备及其运行，因此设备管理模块和数据采集和存储（实时数据库），也应是MES的基本模块。一般还有产品质量管理模块，成本管理模块，物料平衡与仓储管理模块等。目前，随着管理的集约化和精细化，又有新的需求。产生了新的信息技术和软件，增加了MES的功能。</p><p>3、做好集成</p><p>随着信息技术的进步，企业管理的空间、时间范围在扩大，同时管理的细度或粒度又在缩小。从宏观和微观上都要求可视化和实时化，这就需要集成技术。一是MES各模块的集成，二是MES和PCS层面、ERP层面的集成，三是企业内部与企业外部的集成。目前流行的理念和技术是实时绩效管理、制造绩效服务，如图所示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436824" alt="image.png" title="image.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436825" alt="image.png" title="image.png" loading="lazy"/></p><h2>五、最后的个人观点总结</h2><p>1、当下的信息化，普遍设计为有机相连的3或4层架构。</p><p>2、ERP与MES的界限是模糊的，有些功能有重叠。所以，其模块边界的划分是有学问的。ERP不能代替MES，对流程行业尤其是这样。</p><p>3、以DCS为重点的控制系统，以ERP为代表的经营管理系统，以MES为核心的生产运行指挥调度系统，是企业信息化的3大领域，其中MES的效率和效益最具潜力。</p><p>4、MES重点关注“人财物”的“物”，“产供销”的“产”，以及生产运行的“安、稳、长、满、优”。</p><p>5、实施MES可先从基础的、基本的模块做起，再实施扩充的、增强的、高级的模块。</p><p>6、没有一家的产品能包打天下。MES模块应优中选优，再通过第三方集成平台软件进行综合集成是上策。</p><p>7、除了选取MES应用模块、软件外，选取集成商或主承包商提供整体解决方案和集成最关键的。</p><p>最后建议，企业在引入信息化系统初期，切记要合理有效地运用好工具，这样一来不仅可以让公司业务高效地运行，还能最大程度保证团队目标的达成。同时还能大幅缩短系统开发和部署的时间成本。特别是有特定需求功能需要定制化的企业，可以采用我们公司自研的企业级低代码平台：织信。</p><p>织信平台基于数据模型优先的设计理念，提供大量标准化的组件，内置AI助手、组件设计器、自动化（图形化编程）、脚本、工作流引擎（BPMN2.0）、自定义API、表单设计器、权限、仪表盘等功能，能帮助企业构建高度复杂核心的数字化系统。如ERP、MES、CRM、PLM、SCM、WMS、项目管理、流程管理等多个应用场景，全面助力企业落地国产化/信息化/数字化转型战略目标。</p><p>不管说得再天花乱坠，都不能代替产品本身，好产品，值得大家切身体验。</p>]]></description></item><item>    <title><![CDATA[MatrixOne Intelligen]]></title>    <link>https://segmentfault.com/a/1190000047436839</link>    <guid>https://segmentfault.com/a/1190000047436839</guid>    <pubDate>2025-11-28 18:11:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>MatrixOne Intelligence 4.0 全新升级：让数据智能触手可及！</h2><h3>MatrixOne Intelligence 介绍</h3><p>MatrixOne Intelligence 是一套面向多模态数据的 AI 数据智能平台，旨在帮助企业应对数据碎片化、多模态数据整合复杂、GenAI 应用落地困难等挑战。通过数据接入、智能解析、数据工作流、超融合的湖仓底座，MatrixOne Intelligence 为企业提供了一站式的端到端平台，将企业内部的自有数据变成可以服务于 GenAI 落地应用的 AI-Ready 数据。该平台基于创新的云原生架构和存算分离设计，支持结构化和非结构化数据的统一管理和高效处理，具备高度灵活的部署能力，可适配公有云、私有云及本地数据中心的多种环境。</p><p>MatrixOne Intelligence 致力于帮助企业充分挖掘和释放私域数据的潜能，让企业私域数据在 AI 时代得到充分利用，成为其独特竞争力的关键来源。</p><h3>核心升级亮点，全面助力企业数字化转型</h3><h4>一、工作流智能化升级——解放双手，自动完成复杂任务</h4><p>全新引入的Agent模式，让系统能根据用户意图和数据特征自动选择最优执行路径。无需再手动设计复杂的逻辑流程，平台自动感知业务需求，智能做出决策并执行，打造具备"感知 + 决策 + 行动"能力的智能任务，显著降低操作复杂度和时间成本。</p><p><img width="723" height="525" referrerpolicy="no-referrer" src="/img/bVdncD6" alt="" title=""/></p><h4>二、丰富工作流模板，极速搭建自动化流程</h4><p>针对不同业务场景，新增多款开箱即用的工作流模板，覆盖文本解析、数据提取、内容生成等典型应用。用户无需从零开始配置，即可快速构建自动化流程。</p><p>典型应用示例：</p><ul><li><strong>多模态文档RAG数据准备</strong>：支持图文混合内容的数据生成，助力多模态问答系统。</li><li><strong>人才简历信息提取</strong>：自动抓取简历中的核心字段，实现简历数据结构化管理，助力HR高效筛选。</li><li><strong>法律知识数据生成</strong>：从海量法律文本中提取高质量问答对，助力法律大模型的微调与优化。</li></ul><p><img width="723" height="447" referrerpolicy="no-referrer" src="/img/bVdncEn" alt="" title="" loading="lazy"/></p><h4>三、多模态信息提取更强大</h4><p>全新升级的信息提取节点，内置多款针对财务报告、发票、简历等场景的智能模板。用户无需手动定义复杂schema，只需一键即可获得精准结构化结果。结合最新多模态模型技术，实现图文混合内容的深度理解与抽取，极大提升提取准确率和效率。</p><p><img width="723" height="833" referrerpolicy="no-referrer" src="/img/bVdncEo" alt="" title="" loading="lazy"/></p><h4>四、数据中心结构再进化</h4><p>为更好地管理多模态数据，MOI 推出了三级数据中心结构：目录 → 库 → 卷。这一设计支持更灵活的数据隔离与权限管理，满足不同生命周期和业务场景下的多层级治理需求。</p><ul><li><strong>目录</strong>：数据治理最高层，支持生命周期划分（如生产、开发、敏感数据等）；</li><li><strong>库</strong>：组织结构化与非结构化数据，灵活按业务或阶段分类；</li><li><strong>卷</strong>：非结构化文件的逻辑容器，提供高效存储与访问。</li></ul><p><img width="723" height="204" referrerpolicy="no-referrer" src="/img/bVdncEp" alt="" title="" loading="lazy"/></p><h4>五、复杂文档解析更精准，文件处理更灵活</h4><p>新增支持复杂表格结构的智能识别与解析，丰富数据维度和深度。同时，文件处理粒度更加细化，新增一次处理模式下可按单个文件精细选择与操作，极大提升处理的灵活性和控制力。无论是批量处理还是针对特定文件的精准操作，MatrixOne Intelligence都能满足您的多样化需求，帮助您轻松驾驭复杂多样的数据内容。</p><h4>六、数据集成能力再提升</h4><p>新增支持将处理结果直接导出至 MatrixOne、标准 S3、阿里云 OSS 等平台，帮助企业在多种系统间实现更高效的数据流转与管理。</p><p><img width="723" height="340" referrerpolicy="no-referrer" src="/img/bVdncEq" alt="" title="" loading="lazy"/></p><h4>七、HDFS 连接器支持 Kerberos 认证</h4><p>全新 HDFS 连接器已支持企业级 Kerberos 安全认证，在保障数据安全与合规的同时，兼容简单认证模式，让切换更灵活。</p><p><img width="723" height="781" referrerpolicy="no-referrer" src="/img/bVdncEr" alt="" title="" loading="lazy"/></p><h4>八、用户体验全面焕新</h4><p>为提升用户体验，平台做了如下优化：</p><ul><li><strong>账户体系升级</strong>：支持手机或邮箱注册登录，实现官网与 MOI 账号统一使用，一次登录即可访问多个平台。</li><li><strong>产品工作区分离</strong>：MatrixOne 与 GenAI 工作区独立，产品边界清晰，同时支持无缝切换。</li><li><strong>快速上手模块</strong>：新增快速开始功能，将核心流程和功能整合，帮助用户快速熟悉产品。</li><li><strong>流程辅助优化</strong>：在数据处理及工作流中增加操作辅助与帮助提示，使使用流程更顺畅、直观。</li></ul><p><img width="723" height="424" referrerpolicy="no-referrer" src="/img/bVdncEs" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[告别“数据孤岛”，基金公司如何构建秒级响]]></title>    <link>https://segmentfault.com/a/1190000047436847</link>    <guid>https://segmentfault.com/a/1190000047436847</guid>    <pubDate>2025-11-28 18:11:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在资产管理规模不断攀升与金融科技深度渗透的双重驱动下，基金行业正面临前所未有的数据变革。当客户数量突破亿级、日交易量迈向千万级，数据不再仅仅是后台的记录，而是驱动营销获客、投资决策与风险控制的核心资产。</p><p>然而，面对爆发式增长的数据体量与日益复杂的异构数据源，传统的数仓架构正逐渐显露疲态。如何打破数据孤岛，实现海量数据的极速分析与统一管理，成为基金公司数字化转型的关键命题。</p><p>本文将结合某头部基金公司的实战案例，深入解析基金行业的数据痛点，并分享镜舟科技基于 StarRocks 的现代化数据分析解决方案。</p><h3><strong>一、行业现状：数据架构面临代际挑战</strong></h3><p>当前，基金行业的数据建设正面临从报表时代向智能决策时代的跨越，这一过程中主要面临三大挑战：</p><p><strong>数据体量的爆发：</strong> 随着互联网渠道的爆发，个人投资者数量激增。叠加高频交易、定投扣款、以及埋点日志数据，核心业务表的数据量级已从千万级跃升至百亿级。</p><p><strong>时效性要求严苛：</strong> 无论是基金经理的盘中风控，还是营销部门的盘后复盘，业务方对数据的容忍度已从T+1缩短至T+0甚至秒级。</p><p><strong>异构数据源的融合：</strong> 估值系统、TA系统（登记过户）、CRM系统往往采用不同的数据库技术，数据分散在各类孤岛中，跨源分析极其困难。</p><h3><strong>二、行业痛点：被慢与乱拖累的业务</strong></h3><p><strong>当传统架构跑不动海量数据</strong></p><p>在与多家行业客户的交流中，我们发现，为了解决上述问题，IT 部门往往被迫采用加法，导致架构日益臃肿。</p><p><strong>痛点一：多维分析跑不动，决策滞后</strong></p><p>基金公司的核心数据往往达到数亿甚至数百亿行。在 Hadoop 或传统 MPP 数据库上，涉及多表关联的复杂查询响应时间通常在分钟级。</p><p>例如，业务分析师想要进行“某特定客群在不同市场行情下的资产留存分析”，由于查询涉及海量历史流水与客户标签的关联，系统响应往往超过 5-10 分钟，甚至直接超时失败。</p><p><strong>痛点二：组件繁杂，运维高昂</strong></p><p>为了满足不同场景，企业往往被迫引入多套组件：用 <strong>NoSQL 数据库</strong> 承接高并发点查，用 <strong>Cube 预计算引擎</strong> 加速固定报表，用 <strong>传统 MPP</strong> 处理批量计算。</p><p>这种拼盘式架构运维成本极高，还导致了数据搬运过程中的一致性问题。数据开发工程师不得不花费大量时间在 ETL 链路的维护上，而非业务价值的创造。</p><p><strong>痛点三：灵活性差，响应迟钝</strong></p><p>业务需求瞬息万变，而基于预计算的技术方案灵活性极差。一旦市场部想要增加一个新的分析维度（例如新增“渠道偏好”标签），IT 部门需要重新定义模型并回刷历史数据，开发周期以周为单位，完全跟不上市场热点的变化节奏。</p><h3><strong>三、解决方案：化繁为简，构建极速统一数据湖仓</strong></h3><p>针对基金行业数据量大、关联复杂、时效要求高的特点，镜舟科技提出了极速统一的湖仓分析方案。该方案以 StarRocks 为核心，旨在通过一套引擎解决 90% 以上的分析需求。</p><p><strong>1. 极速查询引擎：秒级响应复杂分析</strong></p><p>利用 StarRocks 独有的全面向量化引擎和 CBO 优化器，在无需宽表打平的情况下，即可实现星型/雪花模型的多表关联极速查询。无论是数千亿行的持仓分析，还是复杂的营销圈人，均可实现秒级响应。</p><p><strong>2. 联邦查询：打破数据孤岛</strong></p><p>无需进行繁重的数据迁移，StarRocks 可通过 External Catalog 直接挂载 Oracle、SQL Server、Hive、MySQL 等外部数据源。业务人员可以通过一个统一的 SQL 入口，查询全域数据。既保护了原有资产，又实现了数据的逻辑统一。</p><p><strong>3. 实时与离线融合：简化数据链路</strong></p><p>支持高并发的实时写入与更新，同时具备强大的离线批处理能力。替代原有多组件的复杂组合，一套系统同时满足实时大屏、即席查询（Ad-hoc）和固定报表需求，大幅降低 TCO。</p><h3><strong>四、最佳实践：某头部基金公司的架构重构</strong></h3><p><strong>【客户背景】</strong></p><p>该客户为创新型基金公司，业务发展迅猛。其数据应用场景涵盖了投研、营销、财务、估值四大核心条线，对数据的准确性与实时性有着极高要求。</p><p><strong>【面临挑战】</strong></p><p>客户原有架构高度依赖某传统老牌数据库。随着业务积累：</p><ul><li><strong>性能瓶颈：</strong> 信用评级系统的查询耗时超过 5 分钟，严重影响风控效率。</li><li><strong>时效滞后：</strong> 核心报表跑批需耗时近 6 小时，大客户定制报表排期严重积压。</li><li><strong>稳定性差：</strong> 在绩效系统计算“累计收益率”等指标时，Java 程序经常因内存消耗过大而崩溃。</li></ul><p><strong>【实施方案】</strong></p><p>客户引入 StarRocks 作为全公司统一的 OLAP 分析引擎，重构了数据流转链路：</p><ul><li><strong>数据同步层：</strong> 业务系统（TP库）数据实时同步至从库。</li><li><p><strong>数据接入层：</strong></p><ul><li><strong>实时链路： </strong>通过 CDC 技术捕获变更数据发送至 Kafka，利用 StarRocks 的 Routine Load 实现秒级写入。</li><li><strong>离线链路：</strong> 利用 DataX 将批量历史数据定期同步至 StarRocks。</li></ul></li><li><strong>数据应用层： </strong>基于 StarRocks 构建 ADS、DWS 和 DWD，统一对外提供查询服务。</li></ul><p><strong>【应用效果：从6小时跑批到5分钟就绪】</strong></p><p><strong>1. 核心查询性能提升 10 倍+</strong></p><p>新架构上线后，关键业务系统的响应速度有了极大提升：</p><p>信评系统重，复杂关联查询从原有的 5 分钟缩短至 秒级，风控人员可实时获取最新评级信息，并且解决了估值线在大跨度时间范围内查询数据的性能卡顿问题。</p><p><strong>2. 报表时效性从小时级至分钟级</strong></p><p>通过 MPP 架构的极速计算能力，替代了原有的慢速跑批，报表延迟骤降，业务报表的整体数据准备时间从 6 小时 压缩至 5 分钟以内，真正实现了T+0的动态经营分析。</p><p><strong>3. 架构统一，成本可控</strong></p><p><strong>技术栈收敛：</strong>一套 StarRocks 引擎同时满足了实时大屏、自助报表、固定跑批等多种需求，避免了引入 Elasticsearch 或 HBase 等额外组件的复杂性。</p><p><strong>计算下推：</strong>将复杂的净值计算逻辑从 Java 层下沉至数据库层，不仅提升了稳定性，更大幅降低了应用服务器的硬件资源消耗。</p><h3><strong>结语：让数据跑在业务前面</strong></h3><p>该基金公司的实践证明，在数据体量庞大、业务逻辑复杂的金融场景下，StarRocks 不仅能解决金融机构查询慢和成本高的问题，更能释放数据的实时价值，赋能投研与营销的每一次决策。</p><p>对于基金行业而言，数字化转型不仅是技术的更迭，更是业务模式的重塑。镜舟科技致力于通过极速统一的数据分析底座，帮助金融机构卸下历史包袱，让数据从成本中心转变为价值中心，在瞬息万变的资本市场中抢占先机。</p>]]></description></item><item>    <title><![CDATA[能级跃迁！数字孪生从可视化迈向智能决策 ]]></title>    <link>https://segmentfault.com/a/1190000047436852</link>    <guid>https://segmentfault.com/a/1190000047436852</guid>    <pubDate>2025-11-28 18:10:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年第三季度，浙江省数字孪生水利平台在防汛防台中发挥关键作用，通过精准推演洪峰演进轨迹，提前72小时预测淹没范围，指导人员转移，使应急响应效率提升50%以上。这一成功实践，体现了数字孪生技术从 “精准映射”到“智能干预” 的能级跃迁。</p><p>数字孪生技术已从简单的三维可视化和状态监测，演进为具备预测预警和自主决策能力的智能系统。其能级跃迁的核心在于突破了静态映射的局限，实现了感知、分析、决策、控制的闭环。<br/><img width="723" height="279" referrerpolicy="no-referrer" src="/img/bVdncEx" alt="" title=""/></p><h2>一、精准映射：从物理实体到数字空间的毫秒级同步</h2><p>精准映射是数字孪生的基础，其目标是在数字空间中构建一个与物理实体高度一致且实时同步的虚拟模型。这不仅包括几何形状的还原，更包含物理属性、行为规则乃至环境交互的全面复现。</p><p>技术实现核心在于空—天—地—水—工一体化感知网络的构建。通过部署数以万计的传感器（如浙江水利系统覆盖数万个点位），实时采集物理实体的状态数据（如水位、温度、振动等）。数据通过时间敏感网络（TSN） 等技术实现毫秒级低延时传输和同步。</p><p>关键的一步是高保真建模与渲染。采用如NVIDIA Omniverse等引擎，进行复杂物理场（如流体动力学、结构力学）的仿真，使得虚拟模型能够精准反映物理实体的动态行为，将虚实空间的位置误差控制在毫米级。<br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdncEw" alt="" title="" loading="lazy"/></p><h2>二、模拟推演：从状态监测到未来预测的跨越</h2><p>在精准映射的基础上，数字孪生能级跃迁的第二步是模拟推演，即利用虚拟模型对物理实体的未来状态进行预测。这依赖于水利、机械、电气等专业模型与数据分析模型的深度融合。</p><p>例如，在水利领域，系统通过求解圣维南方程组等水动力学模型来推演洪峰演进：<br/> ∂Q/∂t + ∂(Q²/A)/∂x + gA(∂h/∂x) + gAS_f = 0<br/> （其中Q为流量，A为过水面积，h为水位，S_f为摩擦坡度）。在智能制造领域，则可能采用深度强化学习等AI算法。系统通过分析海量历史运行数据与实时数据，构建预测模型，其目标函数可表述为最大化长期奖励：V^π(s) = E[∑γ^t R(s_t,a_t)]。这使得系统能够提前预测设备故障或模拟不同生产策略的效果。</p><h2>三、智能干预：从虚拟仿真到实体执行的闭环</h2><p>数字孪生技术的最高能级体现在智能干预，即将虚拟空间中仿真优化后得出的最佳决策，反向作用于物理实体，形成闭环控制。</p><p>决策优化是智能干预的前提。系统通常需要在多重约束下（如成本、能耗、安全边界），寻找最优解。其数学模型可简化为一个多目标优化问题：min[f₁(x), f₂(x), …, f_m(x)]^T。通过蒙特卡洛模拟、粒子群优化等算法，在数字孪生体中评估成千上万种可能方案，并选出最优策略。<br/><img width="594" height="441" referrerpolicy="no-referrer" src="/img/bVdncEz" alt="" title="" loading="lazy"/></p><p>最终，优化的决策指令通过数字线程精准下发至物理世界的执行机构（如调节阀门、启停设备、改变机器人运动轨迹）。这一过程强调指令的精准性与执行的实时性。例如，在智慧水网中，系统可自动生成引水方案并执行调度，有效缓解旱情。</p><p>凡拓数创的实践为我们提供了观察这一跃迁的窗口。通过数字孪生致力于实现从物理实体到虚拟空间的高精度映射与实时交互，在甘泉堡智慧园区中数字孪生技术为精准映射和模拟推演提供了支持。此外，与时俱进布局具身智能，旨在进一步强化数字孪生系统的智能干预能力，通过仿真训练与推演，优化智能体的决策执行水平。<br/><img width="723" height="205" referrerpolicy="no-referrer" src="/img/bVdncED" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[“疫苗临床试验”思维导图创作实践解析 图]]></title>    <link>https://segmentfault.com/a/1190000047436863</link>    <guid>https://segmentfault.com/a/1190000047436863</guid>    <pubDate>2025-11-28 18:09:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="272" referrerpolicy="no-referrer" src="/img/bVdncEO" alt="" title=""/><br/>                “疫苗临床试验”思维导图</p><p><a href="https://link.segmentfault.com/?enc=J3Y%2FpemdjAbQXaOye9QUCg%3D%3D.EZV12otxoRHdv2VuAq78nYjHKL74%2Ba943%2B9ZimO9o97vWvoptZQT1R9jCKpugEaOhk6QRD8Ehl%2FCux9zKeq9aA2BktXYTgz27ohbBXvyc78%3D" rel="nofollow" target="_blank">“疫苗临床试验”思维导图模板获取链接</a></p><h2>一、核心主题确定</h2><p>以“疫苗临床试验”为核心主题，鉴于其涉及多维度科学管理流程，需系统梳理关键环节。以临床试验的逻辑链为主线，整合变量控制、分组策略、数据采集及质量管控，构建闭环知识框架，确保全面覆盖试验管理的各个方面。</p><h2>二、导图结构设计</h2><h3>主分支划分</h3><ul><li>依据试验执行阶段，将导图拆解为四大核心模块，即变量体系、分组逻辑、数据采集、质量管理，确保导图结构能够完整覆盖试验全流程。</li></ul><h3>层级逻辑</h3><ul><li><strong>变量体系</strong>：首先区分自变量与因变量，进而细化设计要素，如疫苗类型、剂量梯度等，随后补充干扰因素的控制措施，包括基线平衡、环境监测等，形成层次分明的变量控制体系。</li><li><strong>分组逻辑</strong>：从对照组类型（如安慰剂对照、阳性对照、交叉对照）出发，逐步深入到随机化方法的选择，层层递进，体现科学严谨的分组原则。</li><li><strong>数据采集与质量管理</strong>：按照时间节点（如筛查期、免疫期）与检测维度（实验室检测、临床评估）进行交叉设计，同时设置独立的质量管理模块，监控数据准确性与伦理合规性。</li></ul><h2>三、导图样式设计</h2><ul><li><strong>布局架构</strong>：采用<strong>鱼骨图</strong>布局的基础结构，串联起各个核心模块，以此增强导图的叙事流动性和视觉吸引力。</li><li><strong>颜色搭配</strong>：运用图形天下思维导图提供的<strong>17套主题配色</strong>，对各分支进行区分，如绿色代表变量体系、红色代表分组逻辑等，以提升信息检索的效率和导图的整体美观度。</li><li><strong>层次结构</strong>：针对导图中信息量较大的分支，如“干扰因素控制”下的基线平衡、治疗限制等子项，可使用<strong>分支折叠</strong>功能，将详细信息隐藏于主分支之下，避免单节点信息过载，提升导图的整体平衡性和可读性。</li></ul><p><a href="https://link.segmentfault.com/?enc=tupL6Rx0kwnVOz6Pd7eqnQ%3D%3D.xA%2BbhXC3q81cP1IGgTfuns4EeJenKfpPB9OKsMoX560QFny1TN%2FoooT9%2BA%2FGqJBBT9gXNf0H0%2Fyu%2BX7mAx7uVUKWRXndCopDSlZRnMt7GWmKptgccIBwVynFAEfbHQbV" rel="nofollow" target="_blank">“疫苗临床试验”思维导图模板在线免费体验链接</a></p><h2>四、导图工具与流程</h2><ul><li><strong>工具选择</strong>：使用图形天下思维导图（Amind）软件进行创作，该软件支持复杂分支的拖拽调整与样式的统一管理，极大提升了创作效率。特别是其提供的<strong>12种结构化布局图形</strong>和<strong>37种计算逻辑组合</strong>，能够满足多样化设计需求。</li><li><p><strong>创作流程</strong></p><ul><li><strong>大纲整理</strong>：提炼疫苗试验的核心要素，归类至预设模块。</li><li><strong>结构设计</strong>：利用<strong>鱼骨图</strong>布局，搭建主分支框架，填充子节点内容，确保逻辑连贯。同时应用<strong>分支折叠</strong>功能优化导图的可读性。</li><li><strong>样式调整</strong>：使用<strong>样式设置</strong>对导图进行整体优化，调整字体、颜色、布局等，提升导图的美观度和可读性。</li><li><strong>校对验证</strong>：检查术语准确性（如“区组随机化”定义），确保与临床试验标准一致。</li></ul></li></ul><p><a href="https://link.segmentfault.com/?enc=bsUWkRg2Wl1kISIyNEScgA%3D%3D.AH0aYh0pBQX8uiaegEvxgfJcX6AjBfxx%2F8JRGfSCYPvhXYE%2Bg%2FZN4SHPwDvHv2tOI5p8LwCRObXm%2FhLJ5msK3w%3D%3D" rel="nofollow" target="_blank">图形天下思维导图（Amind）软件免费下载链接</a></p><h2>五、总结</h2><p>本次疫苗临床试验规划思维导图的创作，充分利用了图形天下思维导图（Amind）软件的<strong>鱼骨图</strong>布局和<strong>分支折叠</strong>等功能，该导图不仅覆盖了试验的全流程，还通过视觉优化和层级管理，提升了信息的可读性和检索效率，为临床试验的科学管理提供了有力支持。</p><p>访问图形天下思维导图（Amind）<strong>模板库</strong>与<strong>教程资源</strong>，获取更多免费导图素材与实操指南，激发你的无限创意。</p><ul><li><a href="https://link.segmentfault.com/?enc=en6XtYmlIhGxHN%2FzLKKRog%3D%3D.YIN01KqRLLJ5HGWvjvcMx2M%2FJqjbX5yx3OXt5SY0B4pyxHHWP57lXHQTuLpUik%2FOcpZsEHTb3w2kQWuHCiK8Tw%3D%3D" rel="nofollow" target="_blank">Amind思维导图模板库</a></li><li><a href="https://link.segmentfault.com/?enc=1xejkqO0ki6tda%2BA6SVeaA%3D%3D.FVXlweZrvgQahmGe3yWTRdzITfoPXUt1LFQa5mqc%2BwQjlvzFhDvVG6iCX2UubUmmmKqUZQNKpiPXmp3LSnVwnA%3D%3D" rel="nofollow" target="_blank">思维导图使用教程资源</a></li></ul>]]></description></item><item>    <title><![CDATA[开放原子大会上最意外的一幕：时序数据库 ]]></title>    <link>https://segmentfault.com/a/1190000047436880</link>    <guid>https://segmentfault.com/a/1190000047436880</guid>    <pubDate>2025-11-28 18:08:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当 AI 与开源成为产业级趋势时，我们不得不思考几个现实问题：真正能把开源工程体系打磨扎实的团队有多少？又有多少项目能在技术浪潮之外，持续保持高质量迭代与社区活跃？</p><p>上周举行的 <strong>2025 开放原子开发者大会</strong>，给出了一个有代表性的答案。大会主题定为「一切为了开发者——AI 共智，开源共享」，更关注项目实践、工程落地和开发者生态，而不仅仅停留在概念层面的讨论。而今年的 <a href="https://link.segmentfault.com/?enc=aWAEGiM1f%2F6RmKIFqSR69A%3D%3D.I7yOUg4J28fPC8zxwwkFtVXYePeLLvKxwEfmO%2Fh2inK815gg%2FxGNLf9KDihw2wjD86pmB%2FR18ROD70ijsL9vM59RqfOSrlRAuqJ0awnnK14NYdc7RKnjO3ttRYgg8Lg%2F" rel="nofollow" target="_blank">TDengine</a>，在这个舞台上出现了两次：一次在主论坛，一次在颁奖仪式。</p><h2>“一切用代码说话”：TDengine 的工程方法论</h2><p>在主论坛演讲中，<a href="https://link.segmentfault.com/?enc=NnPsWZ2DMiLHF%2F7Da1xXuQ%3D%3D.vmFNqFjquo%2FVV9zHM1GsZl%2BSZDi7%2FSFQOUG1afTQ%2FFpV4O94ryoaoPLDfSWmywgg7zUvPAz7QI4M25JMgxdPfDhfEjWPi0Gux9IvSyUIPhi%2FzIrMVc6It%2Bjr40sEvqak" rel="nofollow" target="_blank">涛思数据</a>（TDengine）创始人 &amp; CEO 陶建辉带来了《一切用代码说话》的主题分享——一个来自一线研发团队的实践复盘。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436882" alt="" title=""/></p><p>他在演讲中提到一个颇具冲击力的事实：<strong>2020 年以前，TDengine 的研发效能是如今的至少 5 倍。</strong></p><p>这一对比背后，是他长期在中美研发团队观察到的问题：新人环境上手慢、规范分散、测试覆盖不可追踪、测试资源长期紧张……这些情况并非某个团队的特例，而是大量软件团队的共性挑战。TDengine 给出的解决思路很直接——<strong>流程、规范、测试、配置，全都代码化。</strong></p><p>代码化带来的结果同样直接：工程规范变得可审查、可复现、可协作，测试场景可追踪，研发节奏不再被资源瓶颈阻断。更重要的是，随着团队规模扩大，这套体系避免了“人走知识散”的反复重建，让工程质量可以持续累积。在《研发天天加班，但总是忙不过来，为什么这样，有解吗？》这篇文章中，陶建辉也更系统地阐述了这一点，强调只有把所有关键环节代码化，研发效率才能真正提升。</p><p>在一个以开源为底的项目里，工程效率不是锦上添花，而是保证社区持续活跃、保证版本稳定交付的基础设施。陶建辉的分享，本质是在回答一个问题：<strong>中国开源项目如何构建长期生命力？</strong></p><h2>两位 TDengine 开发者入选“开源之星”：属于工程师的现场答卷</h2><p>如果说主论坛让大家看到了 <a href="https://link.segmentfault.com/?enc=Fg1jReQpPiwu2JysoNh01w%3D%3D.z%2B1ZLkJpckWn5jnZHnkJtgbHBuTP51tibZ2Blv9lgfLR%2F6ytp5E9EG2jFZ83dQVx8SmBK9lj22j1bo1Kkwtw%2BFHKKcffcms3fj2XWZNvD3iM5Pxi16sizBNFA2BPfUhq" rel="nofollow" target="_blank">TDengine</a> 创始人对于研发效率与工程实践的思考，那么在“开源创新力量致谢仪式”上，TDengine 的另一面被更多人看见了。</p><p>在今年覆盖操作系统、数据库、人工智能三大领域的评选中，共有 47 位开发者获得表彰，TDengine 的两位研发同事——<strong>谭雪峰、邝金清</strong>——同时入选开源先锋项目开发者及“开源之星”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436883" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436884" alt="" title="" loading="lazy"/></p><p>这两位同事长期投入在核心代码、质量改进、社区维护等关键工作中。对于开源项目来说，这些投入往往不在台前，却直接影响着项目的稳定性与演进速度。正是这些日常而持续的工程贡献，构成了一个开源项目能够长期发展的基础。</p><p>从主论坛到颁奖现场，今年 TDengine 在大会上的两次亮相，展示的是同一个事实：<strong>开源项目真正的价值，始终由实打实的工程投入和开发者自身的贡献决定。</strong></p><h2>工程为先，生态自来</h2><p>今年开放原子大会也设置了多个主题论坛和开源市集，展示从基础软件到 AI 工具链的多种开源能力。整体呈现出一个趋势：产业不再单纯追求“项目数量”，而是开始关注“项目质量”。</p><p>对于 TDengine 来说，这意味着两件事：</p><ul><li>在工程体系中继续把研发、测试、规范统一到可复现的框架里；</li><li>在社区层面，让更多参与者愿意基于工程质量继续投入贡献。</li></ul><p>开源项目的竞争不在发布会，而在代码库；不在宣传稿，而在工程体系，最终能把生态撑起来的，始终是过硬的工程能力。</p>]]></description></item><item>    <title><![CDATA[FlowyAIPC 发布全新 4.0：开]]></title>    <link>https://segmentfault.com/a/1190000047436904</link>    <guid>https://segmentfault.com/a/1190000047436904</guid>    <pubDate>2025-11-28 18:07:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>【2025年11月28日】 —— 端侧 AI 生产力工具 <strong>FlowyAIPC</strong>正式发布全新 <strong>4.0</strong> 版本。本次更新围绕 <strong>“主动生产力”“端侧智能”“本地化大模型加速”</strong> 三个方向进行了深度演进，进一步推动 AI 从工具型能力向真正的智能操作系统层能力迈进。</p><p><img width="723" height="733" referrerpolicy="no-referrer" src="/img/bVdncuo" alt="" title=""/></p><p>FlowyAIPC 4.0 聚焦于让一台普通电脑成为可主动协助用户处理任务的<strong>个人 AIPC（AI Personal Computer）</strong> ，支持本地大模型推理、桌面级智能交互、会议全流程自动化、个人资料深度理解等功能，且完全可离线运行，适用于企业、开发者和个人用户等多场景。</p><h3>深度融合 WML，NPU 本地推理速度显著提升</h3><p>在 4.0 版本中，FlowyAIPC 与 Windows 全新的 <strong>Windows Machine Learning（WML）</strong>  机制深度融合。本地模型将直接运行在设备 NPU 上，相比 CPU/GPU 方案，拥有更低功耗、更快响应以及更流畅的多任务处理能力。</p><p>这意味着用户在处理文本生成、代码补全、总结归纳、文件搜索等任务时，几乎无需等待，端侧 AI 真正实现“随叫随到”。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdncuv" alt="" title="" loading="lazy"/></p><h3>OCR：智能截图，打破单一截图的局限</h3><p>FlowyAIPC 4.0 将截图能力升级为 <strong>智能 OCR 流程</strong>：可通过AI鼠标的左侧前进键、键盘快捷键（Alt+B）或使用悬浮球截图后，系统可自动完成 <strong>文字提取 → 实时翻译 → AI 问答 → 一键存入笔记</strong> 的闭环操作。用户无需切换工具，就能把屏幕信息立刻转化为可检索、可编辑的内容。</p><p>场景示例：网页、PPT、图片、视频字幕的快速提取与翻译；截取资料后马上“问问 Flowy”获取上下文解读；一键存入个人知识库，方便后续检索与复用。<br/><img width="723" height="526" referrerpolicy="no-referrer" src="/img/bVdncuw" alt="" title="" loading="lazy"/></p><h3>全新的会议智能助手：一键从录音到会议纪要</h3><p>FlowyAIPC 4.0 将会议流程拆解成可自动执行的步骤：</p><ul><li>一键录音、实时转字幕</li><li>实时翻译多语种内容</li><li>自动捕捉关键议题与行动项</li><li>会议结束后自动生成正式<strong>会议纪要文档</strong></li></ul><p>整个流程全自动执行，让每个人都能拥有一个随身的“智能会议秘书”。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdm4dz" alt="" title="" loading="lazy"/></p><p><img width="723" height="402" referrerpolicy="no-referrer" src="/img/bVdm4dR" alt="" title="" loading="lazy"/></p><h3>桌面浮球：升级为主动提醒与工作中心</h3><p>FlowyAIPC 4.0 引入更先进的“主动关怀提醒系统”，可在本地分析用户资料、任务节点、文档内容及日程信息，实现：</p><ul><li>天气变化提醒</li><li>待办事项提醒</li><li>重要事项的跟踪与复盘</li><li>根据使用习惯提出下一步工作建议（如寻找资料、起草文稿、整理内容）</li></ul><p>所有计算均在本地完成，可离线使用，兼顾隐私与效率。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdncuz" alt="" title="" loading="lazy"/></p><h3>Agent Store：智能体商店，人人都能创建专属智能体</h3><p>FlowyAIPC 4.0 引入 Agent Store（智能体商店），内置丰富多样的智能体模版（例如：周报生成、文本续写、合同审核等）。用户可直接选用模型与模版，并通过极简提示词配置其行为。无需编程，即可快速搭建、调整专属智能体，实现高度定制化的桌面自动化与工作流优化。<br/><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdncuB" alt="" title="" loading="lazy"/></p><h3>本地个人知识库：让 AI 更懂你、更有记忆</h3><p>FlowyAIPC 4.0 支持上传本地笔记、会议记录、工作资料，结合端侧模型形成用户专属的“个人模型记忆”。相比云端大模型，它对本地文件更敏锐，也能持续记住用户偏好和工作方式。</p><p><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdncuC" alt="" title="" loading="lazy"/></p><h3>受邀亮相 Intel 英特尔技术创新与产业生态大会</h3><p>FlowyAIPC 受邀参加由 Intel 主办的<strong>英特尔技术创新与产业生态大会（Intel Connection）</strong> ，并将在大会期间面向行业伙伴展示全新的 <strong>FlowyAIPC 4.0</strong> 版本。</p><p>此次亮相将重点展示 FlowyAIPC 在 Windows AI PC 生态下的本地化智能体验，以及其在 NPU 加速、本地推理、端侧知识库等方面的能力，展现面向未来的个人智能环境。</p><p><img width="723" height="676" referrerpolicy="no-referrer" src="/img/bVdncuD" alt="" title="" loading="lazy"/></p><h3>面向未来：让 AI 成为长期陪伴的桌面伙伴</h3><p><strong>FlowyAIPC 4.0</strong> 的目标不是让 AI 只是“回答问题”，而是让它真正融入个人电脑的日常使用场景，成为长期陪伴、深度理解用户的桌面伙伴。</p><p>FlowyAIPC 团队表示，未来版本将继续聚焦端侧智能、跨设备协同、知识库隐私能力等方向，推动 <strong>AIPC</strong> 成为面向个人的核心生产力工具。</p><p><strong>立即访问官网，体验FlowyAIPC 4.0：</strong><br/>👉 <a href="https://link.segmentfault.com/?enc=kO2d4DddO%2B7wmdUtbRemTQ%3D%3D.qzp3XACBbDWtO5NR4HMvzUKY2%2BmrPXHWqI2jyu5dws0%3D" rel="nofollow" target="_blank">www.flowyaipc.cn</a></p>]]></description></item><item>    <title><![CDATA[如何在Android上恢复缩略图（有/无]]></title>    <link>https://segmentfault.com/a/1190000047436906</link>    <guid>https://segmentfault.com/a/1190000047436906</guid>    <pubDate>2025-11-28 18:07:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>缩略图是存储在Android设备上的图片的小版本，通常用于帮助应用快速预览较大的文件，而无需加载整个图片。这些照片或视频的缩略图有时会丢失或损坏，尤其是在您意外删除文件或设备系统损坏的情况下。幸运的是，即使没有备份，您也可以使用多种方法在Android上恢复这些宝贵的缩略图。</p><p>在本文中，我们将探讨在Android上恢复缩略图的最有效方法，包括无需备份的选项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436908" alt="图片" title="图片"/></p><p>第一部分：如何从回收站恢复Android上的缩略图？</p><p>尝试恢复已删除的缩略图时，首先应该检查Android设备上的回收站（也称为“垃圾箱”）。许多照片图库应用或文件管理器应用，例如 Google Photos 或三星的图库应用，都具有“回收站”功能，已删除的图片（包括缩略图）会暂时存储在回收站中，然后再被永久删除。</p><p>从回收站恢复缩略图的步骤：</p><p>步骤 1. 打开您的图库或照片应用。</p><p>步骤 2. 在菜单中查找“垃圾桶”或“回收站”选项（通常可以在应用程序的设置或侧边菜单中找到）。</p><p>步骤 3. 浏览最近删除的文件。</p><p>步骤 4. 选择要恢复的缩略图。</p><p>步骤 5. 点击“恢复”选项，将图像恢复到其原始位置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436909" alt="图片" title="图片" loading="lazy"/></p><p>第二部分：如何使用云备份在Android上恢复缩略图？</p><p>如果您已为Android照片启用云备份（这是防止数据丢失的好习惯），则可以轻松地从云存储恢复丢失的缩略图。由于缩略图与原始图像关联，因此恢复原始图像会自动重新生成其缩略图。以下是一些使用常用云服务恢复缩略图的方法。</p><p>选项 1：从 Google 相册或云端恢复</p><p>如果您已将照片备份到 Google Photos、Dropbox 或 OneDrive 等云服务，恢复缩略图就非常简单。只需从云端恢复原始图像，相应的缩略图就会自动在您的设备上重新生成。</p><p>从 Google 相册或云端恢复缩略图的步骤：</p><p>步骤 1. 打开您的云备份应用程序（Google Photos、Dropbox、OneDrive 等）。</p><p>步骤 2. 进入“照片”或“相册”部分。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436910" alt="图片" title="图片" loading="lazy"/></p><p>步骤 3. 搜索要恢复的缩略图。</p><p>步骤 4. 如果在云端找到它们，只需将它们下载或恢复到您的Android设备即可。</p><p>方案二：从品牌专属云恢复</p><p>对于使用品牌专属云服务的用户，恢复过程与 Google Photos 类似。以下介绍如何从一些常用云服务中恢复缩略图：</p><p>三星云：</p><p>步骤 1. 打开三星图库应用程序。</p><p>步骤 2. 点击“菜单”图标，然后进入“设置”&gt;“云同步”&gt;“恢复数据”。</p><p>步骤 3. 选择要恢复的照片，缩略图将随之显示。</p><p>华为云：</p><p>步骤 1. 打开图库应用。</p><p>步骤 2. 点击“我” &gt; “云端图库” &gt; “最近删除”。</p><p>步骤 3. 选择要恢复的项目，或点击云端的“从云端恢复”来恢复备份的照片。</p><p>第三部分：如何在没有备份的情况下找回缩略图？</p><p>如果您没有回收站备份或云备份，也不用担心。您仍然可以使用专业的Android数据恢复工具来恢复丢失的缩略图。这些工具可以深度扫描您设备的内部存储或SD卡，查找丢失的文件，即使系统不再显示它们。无论您的缩略图是意外删除、损坏还是在系统更新后丢失，这些工具都能帮您找回它们。</p><p>Coolmuster Lab.Fone for Android是专为Android设备设计的最佳数据恢复工具之一。它以高恢复成功率、用户友好的界面以及能够扫描已 root 和未 root 设备而著称。</p><p>Coolmuster Lab.Fone for Android的主要功能：</p><pre><code>对内部存储和 SD 卡进行深度扫描，查找丢失的缩略图、照片、视频等。
支持恢复前预览，因此您可以只选择所需的缩略图。
适用于 6000 多款Android机型，包括三星、华为、小米、谷歌 Pixel 等。
基本恢复无需root权限（root权限可以进行更深入的扫描，以查找永久删除的文件）。
恢复过程中不会覆盖您现有的数据。

</code></pre><p>使用Coolmuster Lab.Fone for Android恢复缩略图的分步指南</p><p>01从官方网站获取Coolmuster Lab.Fone for Android ，并将其安装到您的Windows或Mac电脑上。</p><p>02使用 USB 数据线将手机连接到电脑。在Android设备上启用 USB 调试模式。</p><p>03它会自动检测您的设备。选择“照片”（因为缩略图与图库图像相关联）作为要扫描的数据类型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436911" alt="图片" title="图片" loading="lazy"/></p><p>04选择“快速扫描”或“深度扫描”，然后点击“继续”开始扫描您的设备。扫描时间取决于手机上的数据量。请耐心等待并保持设备连接。</p><p>05扫描完成后，所有可恢复的图库项目（包括缩略图）都会显示出来。使用预览功能检查项目，然后选择要恢复的缩略图。点击“恢复”按钮即可将其恢复。</p><p>Android缩略图常见问题解答</p><ol><li>Android缩略图存储在哪里？</li></ol><p>Android会将缩略图存储在一个名为 .thumbnails 的隐藏文件夹中。该文件夹通常位于设备的 DCIM 或“图片”文件夹内。但是，为了避免不必要的文件混乱和系统负担过重，该文件夹对用户是隐藏的。</p><ol start="2"><li>如何重新生成缩略图？</li></ol><p>如果您的缩略图丢失或损坏，您可以手动重新生成。以下是具体操作方法：</p><p>重新生成缩略图的步骤：</p><p>步骤 1. 导航至“内容”&gt;“图像”。</p><p>步骤 2. 从右侧面板点击“重新生成缩略图”。</p><p>步骤 3. 选择存储缩略图的特定库。</p><p>第四步：点击“重新生成缩略图”以重建丢失或损坏的缩略图。这将刷新缩略图缓存。</p><p>最后</p><p>Android设备上的缩略图丢失令人沮丧，但有多种方法可以恢复它们，例如通过回收站、云备份或使用像Coolmuster Lab.Fone for Android这样的专业恢复工具。按照本指南中概述的方法，您可以轻松恢复丢失的缩略图，让您的设备恢复正常。<br/>​</p>]]></description></item><item>    <title><![CDATA[如何通过物流执行系统提升供应链效率？ 月]]></title>    <link>https://segmentfault.com/a/1190000047436920</link>    <guid>https://segmentfault.com/a/1190000047436920</guid>    <pubDate>2025-11-28 18:06:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今制造业的智能化转型浪潮中，物流执行系统作为仓储物流管理的核心工具，已经从传统的经验驱动模式迈向了数字化、智能化的新阶段。这一系统不仅仅是技术的简单集成，而是通过物联网、数字孪生和人工智能的深度融合，实现对物流全流程的精准优化与自动化调度。广域铭岛作为工业互联网领域的创新者，通过其Geega平台，将物流执行系统推向新的高度，为企业提供了从数据割裂到全链路协同的突破性解决方案。<br/>物流执行系统的核心价值在于其能够无缝整合订单管理、库存控制和物料拉动等关键环节，从而提升整体供应链效率。例如，在领克汽车成都工厂的实践中，广域铭岛的物流执行系统通过实时监控库存变化和自动触发补货流程，显著降低了库存缺货风险，同时优化了仓储空间利用率。这种系统不仅减少了人工干预的误差，还通过智能算法预测需求波动，动态调整资源配置，使企业能够敏捷响应市场变化。广域铭岛的创新实践表明，物流执行系统不再是辅助工具，而是驱动企业数字化转型的战略资产。<br/>进一步地，物流执行系统在自动化与智能化方面的应用，展现了其不可预测的潜力。通过集成AGV、RGV等自动化设备，系统实现了物流作业的高效执行，减少了搬运冲突和等待时间。广域铭岛在领克工厂的案例中，利用数字孪生技术模拟仓储环境，动态规划库位和路径，将物流响应速度提升了40%以上。这种技术融合不仅增强了数据的实时性和准确性，还通过AI协同分析，提供了预测性维护和优化建议，帮助企业从被动响应转向主动预测。广域铭岛的解决方案突出了物流执行系统在提升供应链韧性和支持柔性制造方面的关键作用。<br/>然而，物流执行系统的实施并非没有挑战；技术集成难度和数据安全问题时常困扰企业，但广域铭岛通过其Geega平台的实践，成功克服了这些障碍，为行业提供了可复制的智能化路径。未来，随着5G和区块链等新技术的应用，物流执行系统将更加注重绿色可持续发展和跨企业协同，构建更高效的产业级物流网络。广域铭岛持续推动这一演进，致力于通过深度学习算法优化搬运路径，并探索环保目标的集成，从而为企业赋能，实现从经验驱动到数据智能驱动的根本转变。<br/>总之，物流执行系统正以不可逆转的趋势重塑企业管理格局，而广域铭岛的领先实践为其提供了坚实的技术基础和行业洞察。通过多次提及物流执行系统和广域铭岛，本文强调了其在提升效率、降低成本和增强透明度方面的多维价值，为企业拥抱智能化未来指明了方向。</p>]]></description></item><item>    <title><![CDATA[新签约 | 从数小时到实时：海康智联用时]]></title>    <link>https://segmentfault.com/a/1190000047436928</link>    <guid>https://segmentfault.com/a/1190000047436928</guid>    <pubDate>2025-11-28 18:05:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436930" alt="" title=""/></p><p>在智慧交通的建设浪潮中，车路协同已成为推动自动驾驶落地的关键环节。随着车端设备数量激增、数据上报频率不断提升，海量的时序数据正以前所未有的速度产生。对于像海康智联这样承担全国多地示范项目的企业而言，如何实现车辆数据的高效采集、可靠入库与快速分析，成为智能道路建设的核心挑战。</p><p>海康智联长期专注于智能网联与车路协同领域，以物联网、大数据、人工智能技术为核心，形成一体化的全栈智能网联与车路协同产品和技术服务。然而，面对每秒数十万条车端数据的持续涌入，传统数据库频繁出现拥堵与丢失，数据聚合分析周期动辄数小时甚至数天，严重制约了决策响应速度。</p><p>为破解瓶颈，海康智联基于 <a href="https://link.segmentfault.com/?enc=nh5AjuS4s9IOCgfaovmYcA%3D%3D.5kVqKxnapjANi0Ci%2FL%2Fqsx0s2de7kLgucwoAHnctxQhWNY7TLfuT9uUX4RMCE8i6Tq6oqCuCzHjNqmvhkx5R2jA2JZ6Yb8iaZulEZ8YOoKIr3tRTiKHuUUC5zTXAdMy2x4LdwHRYDvPXG1g09YZaqauqxZQWp1%2Ba1hhxHKmZp%2BBWWU3Vuns%2BfqNY4etsLOgeT5nmDB3o8UhnCMiSATcgdA%3D%3D" rel="nofollow" target="_blank">TDengine</a> 构建了“一车一子表”数据模型。车辆通过 T-Box 以 MQTT 协议实时上报数据，<a href="https://link.segmentfault.com/?enc=7Qw%2FYmPci1Au3gGTRXyjCw%3D%3D.sw6dTPeDd7r8Y43HqVv%2FozrXHXf0RYyAnLHYVrzNyRRXhNt7adXt4Z51vEl5CF3MzD4z0xrVgYvG2UhmQTQYgOuOhTOmRLZKofvk3WzOthsY718xq1crOFXsLikYoRQ1oF3jKESkkkbk9ZvQA%2FMoolN5QIgwihgew1eznPZfwVyzCifLrUyPVSjUFWXZz9gcV1mgiH1oD976U2WaVy2BMQ%3D%3D" rel="nofollow" target="_blank">TDengine</a> 实现毫秒级入库，稳定支撑十万级车辆同时在线运行，确保数据全量、无延迟、无丢失。依托超级表的聚合查询能力，任意时段、任意路段的车流量与平均速度可在数秒内计算完成，决策分析由“延时数小时”变为“实时响应”，为智慧交通体系提供坚实的时序数据底座。</p><h2>关于海康智联</h2><p>浙江海康智联科技有限公司成立于 2019 年 10 月，是中电海康集团旗下控股的高科技企业。公司专注于车路协同和智慧交通领域，致力于成为数智道路产品与解决方案提供商，业务覆盖全国多个省份。</p>]]></description></item><item>    <title><![CDATA[如何将 iPhone 上的视频发送到 i]]></title>    <link>https://segmentfault.com/a/1190000047436935</link>    <guid>https://segmentfault.com/a/1190000047436935</guid>    <pubDate>2025-11-28 18:05:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>如果您正从 iPhone 换到 iQOO 设备，或者只是想与使用 iQOO 手机的朋友分享 iPhone 上的视频，您可能会发现视频传输不像在同一系统（ iOS和Android ）的设备之间传输那样顺畅。别担心，本文将介绍几种可靠的方法，帮助您轻松地将视频从 iPhone 发送到 iQOO。无论您喜欢一键传输还是无线分享，都能找到适合自己的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436937" alt="图片" title="图片"/></p><p>第一部分：如何一键将视频从 iPhone 发送到 iQOO</p><p>如果您想以最快、最稳定、最便捷的方式将视频从 iPhone 传输到 iQOO， Coolmuster Mobile Transfer是最佳选择。这款专业工具支持跨平台数据迁移，并通过 USB 连接传输视频，避免了无线网络的不稳定性。它能保持视频原始画质，不进行压缩，因此非常适合传输 4K 视频、长时间录制的视频以及其他大型文件。</p><p>Coolmuster Mobile Transfer能为您做什么？</p><ul><li>一键轻松将视频从 iPhone 传输到Android (iQOO)。</li><li>轻松将联系人、短信、音乐、视频、电子书（PDF 和 ePub）和照片从iOS传输到Android 。</li><li>提供四种多样的传输模式： iOS到iOS 、 Android到iOS 、 iOS到Android和Android到Android 。</li><li>完全支持最新的iOS 26 和Android 16 系统。</li><li>体验快速、流畅、安全的传输，绝无数据丢失。</li></ul><p>如何一键将iPhone上的视频传输到iQOO？请按照以下步骤操作：</p><p>01在电脑上下载并安装Coolmuster Mobile Transfer 。启动程序，并使用 USB 数据线连接 iPhone 和 iQOO。</p><p>02 iQOO 和 iPhone 成功连接到电脑上的程序后，会出现如下界面。请确保已选择 iPhone 作为“源”设备。如果没有，只需点击“翻转”按钮即可交换它们的位置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436938" alt="图片" title="图片" loading="lazy"/></p><p>03从列出的数据类型中选择“视频”，然后单击“开始复制”开始传输。</p><p>第二部分：如何通过 EasyShare 将视频从 iPhone 传输到 iQOO</p><p>EasyShare 是 vivo/iQOO 官方开发的文件共享应用。它支持在iOS和Android设备之间传输数据，无需使用电脑。该应用通过本地 Wi-Fi 热点连接两部手机，使传输过程更加便捷。但是，传输速度可能会因网络环境而异，尤其是在传输大型视频文件时。</p><p>以下是如何使用 EasyShare 将视频从 iPhone 发送到 iQOO：</p><p>步骤 1. 在 iPhone 和 iQOO 上启动 EasyShare，然后点击“手机克隆”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436939" alt="图片" title="图片" loading="lazy"/></p><p>步骤 2. 在 iPhone 上，选择“旧手机”，在 iQOO 上，选择“新手机”。</p><p>步骤三：iPhone 上会出现一个二维码。使用 iQOO 手机扫描该二维码。</p><p>步骤 4. 等待几秒钟，让设备连接。</p><p>步骤 5. 在 iPhone 上，选择要传输的数据，然后点击“开始手机克隆”。</p><p>步骤 6. 如果您的 iPhone 已锁定，系统会要求您输入密码或 PIN 码才能继续。</p><p>步骤 7. 传输完成后，点击“开始使用新手机”完成设置。</p><p>如果您正在寻找 iQOO EasyShare 的替代品，这里有一份您不容错过的列表。</p><p>7 款最佳 iQOO EasyShare 替代方案，实现快速便捷的数据传输</p><p>第三部分：如何使用 Google 云端硬盘将视频从 iPhone 传输到 iQOO</p><p>如果您偏好云端存储方式或需要在不同设备上访问视频，Google 云端硬盘是一个可靠的选择。它允许您从 iPhone 上传视频，然后使用同一帐户在 iQOO 设备上下载。唯一的限制是您的网速和 Google 云端硬盘的存储空间。</p><p>以下是如何使用 Google 云端硬盘将 iPhone 上的视频传输到 iQOO 的方法：</p><p>步骤 1. 在 iPhone 上打开 Google 云端硬盘应用。</p><p>步骤 2. 点击“+” &gt; “上传” &gt; “照片和视频”。</p><p>步骤 3. 选择要上传的视频，然后等待上传完成。</p><p>步骤 4. 在您的 iQOO 设备上，打开 Google 云端硬盘并使用同一帐户登录。</p><p>第五步：找到已上传的视频并将其下载到您的设备。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436940" alt="图片" title="图片" loading="lazy"/></p><p>第四部分：如何使用 SHAREit 将 iPhone 中的视频分享到 iQOO</p><p>SHAREit是一款跨平台传输工具，支持在 iPhone 和Android设备之间快速共享视频、照片、文件等。它通过 Wi-Fi Direct 连接，因此通常比蓝牙或电子邮件更快。虽然界面包含一些广告，但其传输速度和兼容性依然出色，是发送中小尺寸视频的理想选择。</p><p>以下是如何通过 SHAREit 将 iPhone 上的视频发送到 iQOO 的方法：</p><p>步骤 1. 在您的 iPhone 和 iQOO 设备上安装 SHAREit。</p><p>步骤 2. 在 iPhone 上打开 SHAREit，然后点击“发送”。</p><p>步骤 3. 在您的 iQOO 设备上打开 SHAREit，然后点击“接收”。</p><p>步骤 4. 当 iPhone 检测到 iQOO 设备后，选择它进行配对。</p><p>第五步：选择你要发送的视频。</p><p>步骤 6. 点击“发送”，然后等待处理完成。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436941" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>第五部分：如何通过电子邮件将视频从 iPhone 发送到 iQOO</p><p>如果您只需要发送一段短视频或一个小文件，电子邮件是最简便的方法。它无需安装任何应用程序，只要两台设备都能连接互联网即可完成传输。但是，大多数电子邮件服务将附件大小限制在 20-25MB，因此这种方法不适合发送大型视频文件。</p><p>以下是如何通过电子邮件将视频从 iPhone 传输到 iQOO 的方法：</p><p>步骤 1. 打开 iPhone 上的“照片”应用。</p><p>步骤 2. 选择要发送的视频，然后点击“分享”。</p><p>步骤 3. 选择邮件，输入您将在 iQOO 设备上访问的电子邮件地址，然后点击“发送”。</p><p>步骤 4. 在您的 iQOO 设备上，打开电子邮件应用程序并下载附件视频。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436942" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>结尾</p><p>以上所有方法都能帮助您解决如何将视频从 iPhone 发送到 iQOO 的问题，但最佳方案取决于您的具体需求。如果您正在寻找最快、最稳定、最可靠的方法， Coolmuster Mobile Transfer是最佳选择。它支持大文件传输，无需压缩即可保持原始视频质量，提供稳定的连接，并且可以同时传输其他数据（照片、音乐、联系人等）。对于从 iPhone 迁移到 iQOO 设备的用户来说，这是一个绝佳的选择。<br/>​</p>]]></description></item><item>    <title><![CDATA[保姆级教程：3分钟带你轻松搭建N8N自动]]></title>    <link>https://segmentfault.com/a/1190000047436943</link>    <guid>https://segmentfault.com/a/1190000047436943</guid>    <pubDate>2025-11-28 18:04:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今天带大家干一件大事——<strong>在本地部署自动化神器 n8n</strong>。</p><p>很多同学想用 n8n 做工作流自动化，但又担心数据安全或者不想付订阅费。没关系，咱们直接部署在自己电脑上，数据自己通过 MySQL 掌握，稳得很！</p><p>废话不多说，直接开整，保姆级教程走起。</p><hr/><h2>视频教程</h2><p><a href="https://www.bilibili.com/video/BV11pSjBLEAi/" target="_blank">https://www.bilibili.com/video/BV11pSjBLEAi/</a></p><h2>第一步：搞定 Docker Desktop</h2><p>n8n 的本地运行依赖于 Docker，所以咱们第一步得先把环境搭好。</p><ol><li><strong>下载软件</strong>：去 Docker 官网下载 <code>Docker Desktop</code>。是用 Mac、Linux 还是 Windows，看你自己电脑情况。</li><li><p><strong>安装注意</strong>：</p><ul><li>Windows 用户下载完就是个 <code>.exe</code>，双击一路“下一步”就行。</li><li><strong>重点来了</strong>：Docker 本质是帮你装了一个 Linux 子系统，所以安装过程可能会下载一些组件，过程有点长，<strong>请保持网络通畅，耐心等待</strong>。</li></ul></li><li><strong>验证安装</strong>：装好后打开，如果界面像 QQ 音乐一样正常显示，说明环境搞定了。</li></ol><hr/><h2>第二步：拉取 n8n 镜像</h2><p>环境有了，接下来去“进货”。</p><ol><li>打开 Docker Desktop，点击顶部的搜索栏。</li><li>输入关键词 <strong><code>n8n</code></strong> 回车。</li><li>如果搜不到，可能是因为网络问题（你懂的），这时需要加上一点“魔法”。</li><li><p>找到列表里的第一个结果，点击 <strong><code>Pull</code></strong>（下载）。</p><ul><li><em>注意：镜像大概 1.6GB，比较大，去喝杯茶等它下完。</em></li></ul></li></ol><hr/><h2>第三步：启动配置（⚠️最关键的一步）</h2><p>镜像下好了，别急着点 Run 完事！这里面的<strong>参数配置</strong>才是决定你后期用得爽不爽的关键。</p><p>我们在 Docker 的 Images 列表里找到 n8n，点击 <strong>Run</strong> 按钮，这时候会弹出一个设置页面。这里有两点强烈建议大家配置：</p><h3>1. 挂载数据目录（防止数据丢失）</h3><p>Docker 就像一个独立的沙盒系统。如果你不把数据映射出来，万一容器删了，你的工作流就全没了。</p><ul><li><strong>操作</strong>：在设置里做一个路径映射。</li><li>比如把你本地 D 盘的 <code>D:\Date\n8n\mnt</code>（确保你本地有这个文件夹，路径不要带中文和空格），映射到 Docker 里的 n8n 数据存储目录。</li></ul><h3>2. 连接 MySQL 数据库（强烈推荐 🔥）</h3><p>n8n 默认用的是 SQLite 数据库，它是存成文件的。但磊哥<strong>强烈建议大家换成 MySQL</strong>，原因很简单：</p><ul><li><strong>性能吊打</strong>：SQLite 并发差，MySQL 性能高。</li><li><strong>团队协作</strong>：MySQL 支持多人连接，张三写的工作流，李四也能同步看到。</li><li><strong>扩展性</strong>：以后数据量大了，MySQL 扛得住。</li></ul><p><strong>配置方法：</strong><br/>在 Docker 启动页面的 <strong>Environment Variables（环境变量）</strong> 里，填入你本地 MySQL 的信息：</p><ul><li><strong>Host</strong>：填写你本机的局域网 IP 或专用宿主机地址（不要填 127.0.0.1，因为那是容器内部）。</li><li><strong>Port</strong>：默认 <code>3306</code>。</li><li><strong>Database</strong>：起个名，比如 <code>n8n</code>。</li><li><strong>User</strong>：一般填 <code>root</code>。</li><li><strong>Password</strong>：填你安装 MySQL 时设置的密码。</li><li><strong>DB_TYPE</strong>：记得设置为 <code>mysqldb</code>。</li></ul><p><em>(PS: 如果你本地还没装 MySQL，去翻翻我之前的 MySQL 安装教程，先把数据库装好)</em></p><hr/><h2>第四步：端口设置与启动</h2><ul><li><strong>端口号</strong>：默认是 <code>5678</code>。除非你像我一样本地已经占用了这个端口（视频里我改成了 5688），否则大家<strong>保持默认 5678</strong> 就行，省得后面麻烦。</li></ul><p>一切设置妥当后，点击 <strong>Run</strong>！<br/>当你在 Logs（日志）里看到版本号和访问地址时，恭喜你，启动成功！</p><hr/><h2>第五步：初始化与激活</h2><ol><li>打开浏览器，访问 <code>http://localhost:5678</code>。</li><li><strong>注册账号</strong>：填写邮箱、姓名和密码。这个账号是保存在你本地数据库的，不用担心隐私泄露。</li><li><strong>跳过问卷</strong>：之后的调查问卷可以跳过。</li><li><p><strong>激活高级功能</strong>（可选）：</p><ul><li>在设置里填个邮箱，点击 Send，去邮箱拿个 Key 填回来。</li><li>这样可以解锁“工作流分组”等功能，白嫖的功能不要白不要嘛！</li></ul></li></ol><hr/><h2>搞定收工！</h2><p>到这里，你的本地 n8n 就彻底搭建好了。下一步，不管你是想做自动回复、数据抓取还是办公自动化，都可以通过拖拽节点来实现了。</p><p>关于如何搭建 n8n 就讲到这里，大家赶紧动手试试吧！有问题评论区见！👇</p><blockquote>本文已收录到我的技术小站 <a href="https://link.segmentfault.com/?enc=cmAkeyGUcLoA2OXQ7U%2FvCg%3D%3D.vl%2F9iCG09P2iSngTve%2FCmZI6VClbyacI1yetkahH6vo%3D" rel="nofollow" target="_blank">www.javacn.site</a>，网站包含的内容有：<strong>LangChain/N8N/SpringAI/SpringAIAlibaba/LangChain4j/Dify/Coze/AI实战项目/AI常见面试题</strong>等技术分享，欢迎各位大佬光临指导~</blockquote>]]></description></item><item>    <title><![CDATA[如何实施AGV巡检系统在工业4.0中提升]]></title>    <link>https://segmentfault.com/a/1190000047436949</link>    <guid>https://segmentfault.com/a/1190000047436949</guid>    <pubDate>2025-11-28 18:03:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化浪潮席卷全球制造业之际，AGV巡检（自动导引车智能巡检）作为一种融合机械工程、计算机科学与AI决策的创新技术，正在深刻重构传统工业的质量管理逻辑。其本质是通过动态路径规划与多源异构数据的实时集成，将设备异常监测从被动响应转向主动预防，成为工业4.0时代不可缺失的“守望者”。<br/>AGV巡检系统的核心在于其硬件与软件的协同进化。轻量化高强度合金结合模块化设计，赋予了这台“铁机器人”环境适应性；而边缘计算引擎与微服务架构，则让数据处理能力从云端延伸至现场。例如，矿业中的粉尘环境需要震动传感器与气体检测模块的完美融合，这种自定义化的硬件组合正是AGV巡检能力的关键。<br/>在电子制造领域，焊接缺陷检测曾以人工巡检为主，依赖经验与视觉。然而，AGV巡检结合高精度机器视觉与红外热成像，彻底改变了这种惯性模式。以半导体光刻机为例，AGV搭载的智能感知系统能以亚毫米级精度捕捉设备状态，将异物残留的识别时间从分钟级缩短至秒级，并输出标准化数据记录。这种技术的高度复杂性，体现在其可解析200+类设备参数的能力，同步响应工艺波动与能效需求，堪称制造业的“火眼金睛”。<br/>工业实践中的瓶颈往往来自单点设备的弱点，但广域铭岛的解决方案通过打破数据孤岛，将场景边界转化为空间流动。他们的Geega（际嘉）工业互联网平台不仅整合设备状态、工艺参数，还赋予AGV集群智能决策能力。在电解铝行业220kV高压开关站的改造项目中，原有人工巡检需穿戴厚重绝缘装备且响应时间长达4小时。如今通过Geega平台，AGV巡检在物理轨迹规避风险的同时，利用多模态感知网络实时监测设备异常，仅需15分钟即可将问题闭环。<br/>更值得关注的是，广域铭岛将传统“经验判断”转化为可量化的AI规则。以铝业工厂为例，电解槽火焰颜色的微妙变化曾是人为判断的玄学难题，但通过将色温值、波动频率等特征拆解为算法模型，新员工在短期内就能模拟老师傅的水准。这种知识封装的技术路径，正是其AGV巡检系统引以为傲的基石。<br/>AGV巡检的落地应用，不仅限于传统意义上的质量把关，更是以更短决策链、更强响应力重塑生产生态。以石化行业为例，耐腐蚀机器人在反应器外围实时回传泄漏数据，其路径规划避开高温区域的同时，通过3D空间建模将误检率压降至2%以下。这种动态感知与智能路径重构的耦合，直接创造了一线人员从高风险场景中解放的可能。<br/>在物流仓储领域，广域铭岛开发的Geega平台实现了“从箱式AGV到全地形避障”的跨越。比如仓储机器人通过5G调度系统动态划分作业区域，算法优化后的移动距离减少近30%，人力投入占比下降15%。同时，冷链监控功能结合热成像传感技术，使得库存损耗率下降12%，见证了AGV巡检在生产力释放上的独特价值。<br/>尽管AGV巡检技术日臻成熟，但其集成开放性仍需突破。当前，复杂的厂区网络往往存在旧系统兼容问题，数据传输延迟可达数百毫秒，这对于高危场景的实时响应构成制约。因此，广域铭岛将持续缩短算法部署时长，确保在极短时间内迭代符合场景所需的型号模型。<br/>未来，AGV巡检将通过“超级智能体网络”实现集群智能。这在网络协同层级上催生了新技术的涌现——例如多传感器融合支持多种工业环境下的波动采集，如电解槽需IMU惯性测量单元而变电站需光纤感知模块，AGV巡检将成为制造企业适应“多元场景、高频迭代”新常态的核心装备。<br/>从汽车生产线到核电站迷宫，从本地仓储库到跨国铝业基地，AGV巡检正在以不可阻挡之势渗透至工业的每个毛细血管。而广域铭岛通过其Geega平台，不仅提供可持续的技术底座，更在数据驱动与策略优化层面实现了突破。他们的实践证明，AGV巡检不是替代，而是进化——当人力的边界被逐步推远，机器的“智慧”却正在掀起产业变革的新浪潮。工业智能化的下一阶段，无人工厂不再是科幻模型，而是拥有敏捷感知与自主迭代能力的真实存在。</p>]]></description></item><item>    <title><![CDATA[如何将Android手机上的联系人复制到]]></title>    <link>https://segmentfault.com/a/1190000047436951</link>    <guid>https://segmentfault.com/a/1190000047436951</guid>    <pubDate>2025-11-28 18:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>联系人是我们智能手机上的重要信息，可以保存在Android内存和 SIM 卡中。如今， Android手机的整体性能和通讯技术都在快速升级，导致很多人更换手机的频率比以前更高。如果你将联系人存储在SIM卡中，当你换新手机时，事情就会变得容易得多。但如果您将联系人存储在Android内存中，您仍然可以将其导出到 SIM 卡。</p><p>这篇文章深入探讨了如何将Android手机中的联系人复制到 SIM 卡的问题，并为您提供最新的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436953" alt="图片" title="图片"/><br/>​<br/>第 1 部分：在将手机联系人复制到 SIM 卡之前管理Android联系人<br/>第 2 部分. 如何将联系人从Android手机复制到 SIM 卡</p><p>第 1 部分：在将手机联系人复制到 SIM 卡之前管理Android联系人</p><p>“如何使用Android手机添加和管理 SIM 卡上的联系人，我知道一些应用程序能够执行此操作，但我不知道如何......”</p><ul><li>兰姆多</li></ul><p>三天前我在一个名为 Stack Exchange 的论坛上看到了这篇文章，该文章于 2010 年首次发布，但今年仍然活跃。如果您需要在将联系人导出到 SIM 之前管理Android联系人，您可以按照以下指南管理Android联系人。</p><p>Coolmuster Android Assistant是一款专业的Android管理器，让您一键批量传输、管理、备份和恢复电脑上的Android数据。它支持联系人、短信、通话记录、音乐、视频、照片、文档、应用程序、电子书等。您可以轻松地在电脑和手机之间传输它们。</p><p>此外，您还可以快速管理计算机上的联系人和其他数据，例如，您可以在Android手机上添加新联系人、创建新的联系人组、导入、导出、编辑和删除计算机上的Android联系人。</p><p>接下来，我们看看如何在电脑上创建新的联系人组。</p><ol><li>在电脑上安装并启动Coolmuster Android Assistant ，然后从工具箱中进入Android Assistant模块。</li><li>使用USB线将Android手机连接到电脑，然后根据提示打开USB调试并授予手机权限。该程序识别您的设备后，您将看到该程序的主界面，如下所示。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436954" alt="图片" title="图片" loading="lazy"/></p><p>3.管理Android联系人。单击左侧面板中的“联系人” ，点击顶部菜单上的“新建” ，在弹出窗口中输入新的联系人详细信息并确认您的选择。</p><p>第 2 部分. 如何将联系人从Android手机复制到 SIM 卡</p><p>好吧，只要您选择正确的方式，从Android手机将联系人导出到 SIM 卡是很容易的。通过我的试用，我发现以下两种方法非常简单且经济地将手机联系人复制到SIM卡上。</p><p>2.1 如何通过联系人应用程序将联系人从Android手机复制到 SIM 卡</p><p>首先，您可以使用手机上的通讯录应用程序将手机联系人复制到 SIM 卡，这样您就可以在Android手机上编辑、添加、导入、导出和共享联系人，而无需安装额外的应用程序。</p><p>三星如何将手机上的联系人转移到SIM卡上？</p><ol><li>确保 SIM 卡已插入您的三星手机，然后打开手机上的联系人应用程序。</li><li>点击三个垂直点（或更多选项）&gt;设置&gt;导入/导出联系人。</li><li>在导出&gt; 单击窗口中的导出联系人 &gt; 选择所需联系人下的电话，选择导出到 SIM ，或按全选。转移过程结束后，所有手机联系人都将位于 SIM 卡上。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436955" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>进一步阅读：SIM 卡中的联系人意外丢失？这里有 3 种方法可以帮助您 从Android SIM 卡中检索已删除的联系人。</p><p>2.2 如何通过轻松备份将联系人从手机转移到 SIM 卡</p><p>将联系人从Android手机导出到 SIM 卡的另一种方法是使用Easy Backup 。这个简单的应用程序让您只需单击一下即可将手机的整个联系人列表备份到安全的云中。此外，它还支持将.VCF联系人共享到任何电子邮件地址、WhatsApp、Gmail、Google Drive、SMS、Dropbox、Skype、Telegram等。此外，您还可以使用它导入/导出联系人。</p><p>如何将联系人转移到 SIM 卡？</p><ol><li>在Android手机上安装并启动 Easy Backup。</li><li>将联系人备份到云端。点击应用程序主界面中的“备份”选项，选择“点击上传”并将所选联系人从Android设备传输到云端。</li><li>将联系人从云端恢复到SIM卡。将目标 SIM 卡插入Android手机，在Android手机上打开 Easy Backup，然后单击我的备份。接下来，选择您要传输的云备份，然后点击“点击下载” ，然后您的联系人将被下载到插入的 SIM 卡中。之后，您可以预览从此应用程序下载的联系人。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436956" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>结论</p><p>如果您更换了新的Android手机或 SIM 卡，并且正在寻找如何将联系人从Android手机复制到 SIM 卡，这篇文章可能会很好地解决您的问题。您可以选择任一方式将联系人导出到 SIM 卡。如果您需要在导出之前或之后管理联系人，您可以尝试Coolmuster Android Assistant ，它是在计算机上安全编辑、添加、删除、导入或导出联系人的一键方式。</p><p>好了，如果您在使用过程中遇到任何问题，或者您有更好的将手机联系人复制到SIM卡的方法，请随时在下面留言。<br/>​</p>]]></description></item><item>    <title><![CDATA[朝日集团遭黑客攻击 200万个人信息或被]]></title>    <link>https://segmentfault.com/a/1190000047436970</link>    <guid>https://segmentfault.com/a/1190000047436970</guid>    <pubDate>2025-11-28 18:02:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>据有关报道，日本知名企业朝日集团控股近期公布了今9月网络遭黑客攻击事件，报告显示，日本当地事件9月27日上午，企业系统被迫中断，经技术排查发现部分文件被非法加密。事故发生4小时后集团切断了网络连接，并针对数据采取紧急隔离措施。经过内部调查，员工电脑中的数据已经泄露，而服务器中存储的数据也同样被入侵，约200万份个人信息存在被泄露的风险。JoySSL安全技术专家指出，此次网络黑客是通过朝日集团的本地网络设备获取了数据中心的访问权限，通过部署勒索软件成功入侵内部系统。企业不仅要承担巨大的经济损失，数百万用户隐私信息被泄露也会让企业形象与信誉遭受广泛质疑。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdncGq" alt="" title=""/></p><p>随着数字化浪潮的推进，全球网络安全事故愈发频繁，数据窃取事件频频上演。小到中小型电商平台，大到跨国集团乃至行业巨头，都免不了出现各种数据泄露事件。朝日集团的案例再次给全球企业敲响警钟：在互联网高度发达的当下，无论什么样的机构组织都不可忽视网络安全建设，任何未做加密防护的数据传输或存储，都极有可能成为黑客的攻击目标。</p><p><strong>数据泄露揭示网络安全隐患</strong></p><p>朝日集团此次被网络攻击事件，本质上已然是数据传输通道存在安全漏洞，被黑客捕捉并加以利用。JoySSL技术负责人指出，如果企业所构建的平台并未部署加密或防护等相关措施，在提交个人信息时就如同信件传递一般，可以被任何人随意查看或阅览，通过技术手段篡改或窃取也非常简单。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdncGx" alt="" title="" loading="lazy"/></p><p>除了在用户注册或登录等界面容易泄露信息外，网站或平台上的在线交易、信息提交或数据查询等功能，均有可能产生数据泄露的风险。由于互联网的不断深入和普及，线上办公已发展为常态化，数据所遭受的网络风险更大。</p><p><strong>SSL证书构建可信赖数字生态</strong></p><p>在数字化时代下，数据的重要性毋庸置疑，对于大型企业来说，数据甚至可以作为关乎企业存亡的核心要素。数十年发展的企业规模、积累的信誉，极有可能因为一次信息泄露，而毁于一旦。对此，企业有必要认识到SSL安全证书的重要性，以可信、专业的互联网安全产品，为企业数据防护提供可行性方案。利用数字证书打造坚实的防护体系，创造稳定安全的数字生态体系。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdncGy" alt="" title="" loading="lazy"/></p><p><strong>全方位证书产品线化被动为主动</strong></p><p>不同规模的企业，对于网络安全防护需求也各不相同。因此，提供全方位SSL证书产品线，方能满足市场多样化需求。例如DV证书普遍应用于中小企业，加密措施高效且经济；而OV或EV证书则更适合大型企业，具备更强的加密功能与验证技术，能够面对更为严峻的网络攻击。JoySSL市场负责人表示，纵观近年来多起网络攻击与数据泄露事件，不难看出被动应对的前提条件是损失已经造成，补救为时已晚。相比之下，未雨绸缪率先部署数字证书，主动抵御，才是正确的做法。</p>]]></description></item><item>    <title><![CDATA[如何永久删除Android手机中的照片 ]]></title>    <link>https://segmentfault.com/a/1190000047436972</link>    <guid>https://segmentfault.com/a/1190000047436972</guid>    <pubDate>2025-11-28 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>有时，您可能想要清理Android设备，永久删除某些照片。这可能是因为您想重新开始，或者打算将手机出售给他人，不希望第三方访问您的照片。</p><p>无论出于什么原因，都有多种方法可以删除这些照片。本文将向您展示如何永久删除Android手机上的照片，这有助于保护您的隐私免遭泄露。</p><p>第一部分：如何一键永久删除Android手机中的照片（100% 无法恢复）</p><p>如何永久删除照片，使其无法恢复？清除Android设备上所有数据的第一种方法是使用Coolmuster Android Eraser 。这种方法的优点在于，只需单击一下，所有记忆，包括Android手机上的所有照片，都会被永久删除。这意味着一旦删除过程完成，即使使用恢复工具也无法恢复这些文件。</p><p>它与几乎所有Android手机和平板电脑完全兼容，例如三星（包括三星 Galaxy S25 Ultra）、小米、OPPO、一加、TECNO、Infinix、Itel、vivo、摩托罗拉、荣耀等。</p><p>Coolmuster Android Eraser 的亮点：</p><pre><code>永久删除Android手机中的照片、短信、联系人、视频等内容。
确保100%永久删除，并防止个人数据被恢复。
提供三种递进的数据擦除级别：低、中、高。
清除Android设备上的所有内容，包括已删除的文件、私人数据、系统设置等。
兼容最新的Android 16版本。

</code></pre><p>以下是使用Coolmuster Android橡皮擦时需要遵循的步骤：</p><p>01首先，请确保程序已检测到手机。这只能通过将设备连接到个人电脑来实现。可以使用 USB 数据线建立连接。或者，用户也可以使用 Wi-Fi 连接两者。</p><p>下载Coolmuster Android Eraser 并将其安装到您的计算机上。</p><p>02连接后，您可以点击“擦除”按钮开始从Android中删除照片。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436974" alt="图片" title="图片"/></p><p>03在安全级别旁边有一个菜单。点击该菜单选择低、中或高安全级别。点击“确定”确认您的选择。</p><p>04要删除所有文件（包括照片），请在空格处输入“删除”，然后点击界面上的“擦除”按钮。程序将进行快速扫描，清除所有文件并立即覆盖数据。请务必确保在删除过程完成之前不要中断连接。</p><p>第二部分：如何有选择地从Android手机中永久删除照片</p><p>如果您想有选择地从Android手机中删除照片，或者从Android相册中删除照片，您还可以使用Coolmuster Android Assistant ，它可以帮助您更好地管理（删除/导出/添加）任何Android手机上的照片。它不仅可以管理照片，还可以管理联系人、短信、视频和音乐等各种文件。</p><p>此外，它还支持一键备份和恢复Android手机上的所有数据，并支持Android手机和电脑之间的无缝数据传输。Coolmuster Coolmuster Android Assistant是一款功能全面的解决方案，是您的理想之选。</p><p>试试下面的工具。</p><p>01启动Coolmuster Android Assistant后，您可能会看到下图所示的界面。请确保您的Android手机已通过 USB 数据线或 Wi-Fi 安全连接到您的电脑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436975" alt="图片" title="图片" loading="lazy"/></p><p>02点击“照片”类别，即可显示您Android手机上存储的所有照片。选择要删除的照片，然后点击顶部的“删除”选项。</p><p>注：您可以点击此处查看如何从Android手机批量删除照片。</p><p>第三部分：如何通过恢复出厂设置永久删除Android设备上的照片</p><p>我们的设备通常会存储大量个人信息，包括我们可能想要永久删除的照片。如果您是Android用户，想知道如何安全地删除照片，一个有效的方法是恢复出厂设置。此过程不仅会清除设备上的所有数据，还能确保您的照片被永久删除。</p><p>在本节中，我们将逐步指导您完成此操作。</p><p>第一步：打开Android设备上的“设置”应用。具体位置可能因设备制造商和Android版本而略有不同，但通常以齿轮图标表示。</p><p>步骤 2. 在“设置”菜单中，根据您的设备，找到并选择“系统”或“常规管理”。</p><p>步骤 3. 查找类似“重置”或“重置选项”的选项。具体措辞可能有所不同，但通常都包含“重置”一词。</p><p>第四步：在“重置”菜单中，您应该能找到名为“恢复出厂设置”或类似名称的选项。点击该选项即可开始重置过程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436976" alt="图片" title="图片" loading="lazy"/></p><p>步骤 5. 系统可能会提示您确认恢复出厂设置。请注意，此操作将清除设备上的所有数据，包括照片、应用和设置。</p><p>步骤 6. 如果您的设备受 PIN 码、密码或图案保护，您需要输入该密码才能继续进行恢复出厂设置。</p><p>步骤 7. 输入 PIN 码或密码后，系统会要求您确认是否执行恢复出厂设置。确认后，恢复出厂设置过程将开始。</p><p>注意：恢复出厂设置是永久删除Android设备上照片的有效方法。但是，请务必谨慎操作，因为此过程会清除所有数据。在进行此操作之前，请务必备份所有重要信息。</p><p>第四部分：如何在Android相册中永久删除照片</p><p>智能手机用户还可以选择性地从相册中删除照片。您可以按照以下步骤操作：</p><p>步骤 1. 打开图库应用（或照片应用），您将看到所有文件和文件夹的列表。</p><p>步骤 2. 在图库应用中，点击要删除的照片。</p><p>步骤三：点击“垃圾桶”按钮，从Android相册中删除照片。如果系统提示，请确认您的选择。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436977" alt="图片" title="图片" loading="lazy"/></p><p>第四步：要永久删除照片，您需要访问回收站或垃圾箱，已删除的项目会暂时存储在那里。此选项通常位于“照片”应用或设备的文件管理器中。</p><p>步骤 5. 打开回收站后，检查其中的照片，确保只有您需要删除的照片。然后，找到“清空回收站”或“永久删除”选项。点击此选项即可从设备中永久删除所选照片。</p><p>如果您的设备没有内置清空回收站的功能，请考虑使用文件管理器应用。找到回收站文件夹，然后从中删除照片。此步骤可确保照片不仅从应用中移除，还会从设备存储空间中删除。</p><p>总结</p><p>以上介绍了一些可以用来清除Android设备上照片的方法。请选择最适合您设备的方法。不过，在众多方法中，我们最推荐使用Coolmuster Android Eraser，因为它能够彻底永久地删除手机上的照片。<br/>​</p>]]></description></item><item>    <title><![CDATA[8 个最佳 Google Sheets ]]></title>    <link>https://segmentfault.com/a/1190000047436397</link>    <guid>https://segmentfault.com/a/1190000047436397</guid>    <pubDate>2025-11-28 17:17:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>原文链接：<a href="https://link.segmentfault.com/?enc=3jGAD9ghyBgGKdbnF51ELg%3D%3D.gx5z9jWqvkMZKP%2FewD3Kn3H%2Bdiwkc6iABI3nvnuCleQ1jqTHhTyB9wz5YgL352QhzzpsrVDoRLurfGw%2FP2t%2FucOlbYkVqjoEsLGyjaBNDd4ob%2BlSRTYWvonqKibMmLOYL03kjTUfEA62FNXBX59mzQ%3D%3D" rel="nofollow" target="_blank">https://www.nocobase.com/cn/blog/the-8-best-google-sheets-alternatives-with-full-cost-and-capability-analysis</a></p><p>你们团队现在或者曾经使用过 Google Sheets 吗？</p><p>它轻量、易上手、协作方便，是许多团队在数字化初期最常选择的工具。在数据记录、任务跟踪、内容管理或基础统计等场景中，表格的确能快速解决问题。</p><p>但随着团队规模扩大、参与者增多、业务逻辑变得复杂，表格在结构和协作方面的限制会逐渐显现。跨部门协作、细粒度权限管理、流程化操作等需求越来越多时，Google Sheets 往往难以满足后续的扩展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436400" alt="Hacker News.png" title="Hacker News.png"/></p><p>在 <a href="https://link.segmentfault.com/?enc=D8jvRZlY%2Bt2RWKcRzaVOkA%3D%3D.KppnAfpih2aoz7aJjU7NrDV79S5af8OErDQPQ%2FEpXXDHEPge4xEmgMWIWwjCBeqn" rel="nofollow" target="_blank">Hacker News</a> 的讨论里，也有不少用户在寻找 Google Sheets 的替代方案。一些用户提到，它在复杂场景下显得更像是工具链里的“临时选项”，难以承担严肃的数据工作；面对更大的数据规模或更复杂的模型构建时，也容易力不从心，难以支持完整的数据分析流程。</p><p>本质上，Google Sheets 的定位始终是电子表格，而不是业务系统。</p><p>因此，当团队尝试用它承载 CRM、项目管理、资产登记或审批流程时，往往会遇到数据关系难以维护、权限管理受限、自动化链路难以扩展等问题。</p><p>基于这些真实的使用背景，我整理了几类常见的替代工具，希望能帮助你在不同阶段、不同需求下，找到比 Google Sheets 更适合的选择。</p><p>接下来会从三个关键维度展开介绍：</p><ul><li><strong>团队规模成本</strong>：按 10 / 50 / 100 人团队计算的年度预估投入</li><li><strong>最佳使用场景</strong>：最适合什么团队与什么业务需求</li><li><strong>能力表现</strong>：数据结构、协作权限、扩展性、自动化、使用门槛等核心能力</li></ul><h2>Google Sheets 替代方案</h2><p>在进入正文前，我根据常见业务需求将今天要介绍的 8 个工具做了分类，你可以先快速浏览下面的工具类型\~</p><p><strong>① 构建业务系统型工具</strong></p><p><strong>NocoBase、Retool、Appsmith、Budibase</strong></p><p>适合从“表格混乱”升级到“真正的内部系统”，需要数据模型、权限、自动化流转或模块化应用的团队。</p><p><strong>② 结构化数据型工具</strong></p><p><strong>Airtable、Smartsheet、Baserow、NocoDB</strong></p><p>适合已经不满足传统表格，但还不需要完整业务系统的团队；提供结构化数据模型、多视图、基础权限协作、自托管选项（部分工具），用于更规范地管理数据。</p><h3>价格对比表格</h3><table><thead><tr><th>工具名称</th><th>版本 / 模式</th><th>10 人团队每年成本</th><th>50 人团队每年成本</th><th>100 人团队每年成本</th></tr></thead><tbody><tr><td>Airtable</td><td>Team</td><td>&amp;dollar;2,400</td><td>&amp;dollar;12,000</td><td>&amp;dollar;24,000</td></tr><tr><td>Airtable</td><td>Business</td><td>🔴&amp;dollar;5,400</td><td>🔴&amp;dollar;27,000</td><td>🔴&amp;dollar;54,000</td></tr><tr><td>NocoBase</td><td>Standard（一次性）</td><td>✅&amp;dollar;800</td><td>✅&amp;dollar;800</td><td>✅&amp;dollar;800</td></tr><tr><td>NocoBase</td><td>Professional（一次性）</td><td>&amp;dollar;8,000</td><td>&amp;dollar;8,000</td><td>&amp;dollar;8,000</td></tr><tr><td>NocoDB</td><td>Team</td><td>✅&amp;dollar;228</td><td>✅&amp;dollar;1,140</td><td>✅&amp;dollar;2,280</td></tr><tr><td>NocoDB</td><td>Business</td><td>&amp;dollar;1,188</td><td>&amp;dollar;5,940</td><td>&amp;dollar;11,880</td></tr><tr><td>Baserow</td><td>Premium</td><td>&amp;dollar;1,200</td><td>&amp;dollar;6,000</td><td>&amp;dollar;12,000</td></tr><tr><td>Baserow</td><td>Advanced</td><td>&amp;dollar;2,160</td><td>&amp;dollar;9,000</td><td>&amp;dollar;18,000</td></tr><tr><td>Retool</td><td>Standard</td><td>&amp;dollar;1,200</td><td>&amp;dollar;6,000</td><td>&amp;dollar;12,000</td></tr><tr><td>Budibase</td><td>Premium</td><td>约&amp;dollar;2,700</td><td>约&amp;dollar;5,850</td><td>约&amp;dollar;8,700</td></tr><tr><td>Appsmith</td><td>Business</td><td>&amp;dollar;1,800</td><td>&amp;dollar;9,000</td><td>&amp;dollar;18,000</td></tr><tr><td>Teable</td><td>Professional</td><td>&amp;dollar;1,200</td><td>&amp;dollar;6,000</td><td>&amp;dollar;12,000</td></tr></tbody></table><p>💡 从上表可以看出几个重要趋势：</p><ul><li>随着团队规模扩大，采用按用户计费的工具（如 Baserow、Appsmith、Retool、Teable 等）成本都会快速上升，小团队负担可控，但到了 50 人、100 人规模时，年度投入会成倍增长。</li><li>NocoBase 是其中唯一不随用户增加而变动价格的方案，Standard 年成本保持在 &amp;dollar;800，Professional 固定为 &amp;dollar;8,000，这种模式在需要多人协作或跨部门使用的场景下更具可预测性。</li><li>NocoDB、Baserow 等数据库型工具的成本相对稳定，但仍基于人数计费，适合作为从 Google Sheets 过渡到结构化数据管理的中间路线。</li><li>Budibase、Teable 等工具保持中等成本区间，界面体验对习惯电子表格的用户较友好，适合从 Google Sheets 升级但不需要完整业务系统的团队。</li><li>整体而言，自托管工具在团队人数越多时越具成本优势，适合计划将原本依赖 Google Sheets 的数据逐步迁移为更结构化、长期稳定使用的系统；而 SaaS 类工具更适合小团队或短期项目。</li></ul><p>💡推荐阅读：<a href="https://link.segmentfault.com/?enc=uqSKglo2QuItUR%2BAA5sCGg%3D%3D.Eykd6P%2FHrfghvn%2FHT4T0QaI5CZJ8YAYu7Ke57OXIv3YJRx%2FLgR5xtldxCke7DEBnWSBR92CDfRvga%2FmCEmb6YxszaCnEAuL%2BjQE5x%2F1njFk%3D" rel="nofollow" target="_blank">7 款最佳自托管 AI 工具，快速构建业务应用</a></p><h3>NocoBase</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436401" alt="NocoBase.PNG" title="NocoBase.PNG" loading="lazy"/></p><p>官网：<a href="https://link.segmentfault.com/?enc=G0wJ80S2KAACZki527VmhA%3D%3D.2y9OzuRB73gL3BTxP4JzKEpv06y9BdRkH7u6ApOvyeM%3D" rel="nofollow" target="_blank">https://www.nocobase.com/cn/</a></p><p>NocoBase 是一个开源的无代码／低代码业务应用构建平台，用于搭建内部系统、管理后台以及以数据为中心的业务流程。支持自托管，也提供官方托管方案，适合希望从表格迁移到系统化管理的团队。平台基于可视化配置构建应用，并可通过插件和 API 扩展能力，部分场景还可以结合官方提供的 AI 功能提升数据录入与流程效率。</p><p><strong>使用场景</strong></p><p>适合已经不满足于用表格管理数据，希望构建更结构化、更可控的内部系统的团队。  典型场景包括：</p><ul><li>以数据管理为核心的业务，如客户管理、库存管理、资产管理</li><li>依赖流程流转的场景，如审批、项目协作、任务推进</li><li>需要多人协作、分角色使用的后台系统</li><li>对数据安全、自托管或内部部署有要求的企业环境</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据结构能力</strong> 可以通过界面创建数据表、字段和关系，把业务数据整理为清晰的结构化模型，并在视图中灵活展示。</li><li><strong>协作与权限能力</strong> 提供多层级权限，包括角色、表、字段和页面层面的访问控制，适合团队间协作与权限隔离。</li></ul><p>💡推荐阅读：<a href="https://link.segmentfault.com/?enc=tuXs6G6ksiKO4a2ewSczyw%3D%3D.8aFA4Qa6mhqBeWtmi7nW4zCsFd2yStS%2F2Z0eqosoCUKJXmVO1%2BYwvn1xu2T9IJQjVjMbIpsVFjtglsQJ%2Fl2ZoXsIugerKii8dFhyiS4%2B3v%2F0ZfwkRiQFCMXTaJVYwkLe" rel="nofollow" target="_blank">6 大企业级无代码低代码平台 RBAC 权限体系深度对比</a></p><ul><li><strong>自动化能力</strong> 支持基于事件的流程配置，可以在数据变更时触发通知、审核和跨系统动作，减少手工操作。</li><li><strong>扩展能力</strong> 具备插件体系和开放接口，可以安装或开发插件，也能集成外部系统，使功能随着业务扩展而增长。</li><li><strong>自托管与部署能力</strong> 可以在本地或云服务器部署，团队能够自主掌控数据与运行环境。</li><li><strong>API 与集成能力</strong> 平台提供可由数据模型自动生成的 API，并支持通过插件或配置与外部系统集成，适合在更大的业务体系中衔接前端、移动端或其他服务。</li><li><strong>使用门槛</strong>  非技术人员可以通过界面创建业务应用，而技术团队可以基于插件、接口和自托管做更深入的扩展。</li></ul><p><strong>总结</strong>  如果团队正在从电子表格转向系统化管理，并希望在数据结构、权限、流程和扩展性上有更完整的能力，同时希望系统能随业务规模逐步增长，NocoBase 是一个适合长期投入的选择。它兼顾可视化易用性与深度扩展能力，适合从小团队到企业的不同阶段。</p><h3>Retool</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436402" alt="Retool.png" title="Retool.png" loading="lazy"/></p><p>官网：<a href="https://link.segmentfault.com/?enc=QJwwjH6WYL1NcgwolpTIjQ%3D%3D.hBHvQOZflufSlSaCTfQ%2FzhKlwazPb6QGdPZhYlIAFJE%3D" rel="nofollow" target="_blank">https://retool.com/</a></p><p>Retool 是一个用于快速构建内部工具的闭源 SaaS 平台，也提供自托管版本（企业计划）。它主要面向工程团队，通过拖拽组件和少量代码，将数据库、API 和后台逻辑快速组合成可用的内部应用，例如运营后台、数据面板、审核系统或客服支持工具。</p><p><strong>使用场景</strong></p><p>适合拥有工程师团队、需要快速构建内部业务工具的公司。  典型场景包括：</p><ul><li>后台运营工具，例如订单管理、申诉处理、内容审核</li><li>数据运营与支持团队使用的数据面板或工具化界面</li><li>工程团队搭建内部控制台、运营系统、管理界面</li><li>接入现有数据库或 API 的轻量业务流程工具</li></ul><p><strong>能力表现</strong></p><ul><li><strong>扩展能力</strong>  Retool 在扩展方面非常灵活，可以通过 JavaScript 直接处理数据、调用 API、组合逻辑，也能与数据库、消息队列或第三方服务集成。对于需要将多个系统整合到一个操作界面的团队来说，它能显著减少开发成本，是工程团队常用的内部工具框架。</li><li><strong>数据连接能力</strong>  Retool 支持连接大量数据源，包括 SQL、NoSQL、REST API、GraphQL 等，也允许开发者在界面上直接编写查询语句、处理逻辑、进行数据绑定。它并不负责数据结构本身，而是作为上层应用界面，把不同系统的数据接入进来并统一展示。</li><li><strong>使用门槛</strong>  虽然有拖拽和可视化编辑，但 Retool 的核心仍然依赖工程能力。实际使用中需要编写 JavaScript、SQL 或配置 API，产品、运营团队较难独立完成复杂应用。对于开发者来说，上手快；但对于不懂代码的用户来说，门槛较高。</li><li><strong>局限性</strong>  Retool 能快速产出内部工具，但不适合做外部产品或真正的业务系统。它也不负责数据建模、业务流程或系统级权限架构，只能依赖底层系统实现。同时，Retool 项目与逻辑较多时，管理和维护的复杂度会上升。</li></ul><p><strong>总结</strong>  如果团队拥有工程师，并且核心诉求是快速搭建内部工具或运营后台，而不是构建完整业务系统，Retool 会是效率非常高的选择。但如果团队没有开发人员，或希望从数据结构到流程自动化都可视化完成，它可能不是最合适的工具。</p><h3>Appsmith</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436403" alt="Appsmith.png" title="Appsmith.png" loading="lazy"/></p><p>官网：<a href="[https://appsmith.com/" target="_blank">https://appsmith.com/</a></p><p>Appsmith 是一个开源低代码／无代码平台，专为构建内部工具、后台系统和管理面板设计。它允许用户通过拖拽组件、连接数据库或 API、编写少量脚本，快速搭建业务界面，也支持自托管部署，对数据安全和隐私有更好控制。</p><p><strong>使用场景</strong>  适合以下情况：</p><ul><li>团队需要快速构建后台管理系统、运营工具、审核系统、客服后台等内部工具</li><li>希望连接数据库、API，但不想从零开始写代码的公司或团队</li><li>对数据安全或合规敏感，需要自托管或对数据基础设施有控制权的组织</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据连接与多数据源支持能力</strong>  Appsmith 支持连接 SQL、NoSQL 数据库、REST / GraphQL API、多种数据源。它能够把已有的数据源“拉进来”，然后通过 UI 组件展示、操作、过滤、修改数据。这样即使是复杂或者异构的数据体系，也能被整合到一个统一界面中，方便非工程师也能使用和管理。</li><li><strong>扩展与定制能力</strong>  作为开源工具，Appsmith 支持完全自托管，也允许用户自定义界面组件、脚本逻辑、权限设置与用户角色管理。对于需要高度定制、系统集成、多团队协作的企业，它可以作为内部系统平台基础，提供比普通具更灵活、更可控的长期方案。</li><li><strong>使用门槛</strong>  对于没有技术背景的团队来说，Appsmith 的界面相对友好，但要充分利用它的数据连接、逻辑定制、自托管等能力，仍需要一定技术能力（至少了解数据库或 API、基本脚本/逻辑）。相比纯表格工具，上手门槛更高；但对于有技术储备的团队，它比从零开发更快、更省力。</li><li><strong>局限性</strong>  虽然 Appsmith 提供 UI + 数据连接 + 自托管，但它并不是“无需任何代码”的 100% 无代码解决方案。对于极为复杂的业务需求（高度定制业务逻辑、大量并发、复杂权限系统等），仍可能需要工程资源进行二次开发或扩展。同时，对于完全不懂技术的团队，其学习曲线比纯 SaaS 表格工具更陡峭。</li></ul><p><strong>总结</strong>  如果你的团队有一定技术基础，或者你们重视数据安全和可控性，并希望快速搭建内部工具（后台管理、运营面板、审核系统等），Appsmith 是一个性价比高、灵活、适合长期使用的平台。但如果团队完全没有工程资源，也不愿意做运维和维护，那么就要慎重考虑它的使用门槛和长期投入。 💡推荐阅读：<a href="https://link.segmentfault.com/?enc=WcivViUDB2gWLno0yStbKg%3D%3D.lGcMY59rcHP%2BJpj9WFJxNEToBq51pKEILByszVFKJCtJZhSCLWr6pipPI5uaTCICgQT9NcS2PlQjw97Fii8prg%3D%3D" rel="nofollow" target="_blank">NocoBase 与 Appsmith：哪个开源低代码平台更适合你？</a></p><h3>Budibase</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436404" alt="Budibase.png" title="Budibase.png" loading="lazy"/></p><p>官网：<a href="[https://budibase.com/" target="_blank">https://budibase.com/</a></p><p>Budibase 是一个开源的低代码平台，用于构建内部工具和管理后台。它支持自托管，也支持部署到云端，对于需要掌控数据安全或希望在本地运行系统的团队来说更加灵活。</p><p><strong>使用场景</strong>  适合希望快速搭建后台管理系统、数据录入工具或简单业务系统的团队。典型场景包括：</p><ul><li>客户信息、库存、资产等数据的录入和管理</li><li>小型内部 CRM、审批流或运营后台</li><li>希望具备自托管能力的小型团队或初创公司</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据管理能力</strong>  Budibase 支持连接外部数据库，也可以在平台内创建结构化的数据表，定义字段类型、记录关系、表单和列表视图。对于中小规模的数据管理任务，它比普通表格提供更清晰的结构和更规范的组织方式。</li><li><strong>扩展能力与自托管支持</strong>  提供角色和权限定义、触发器和动作配置，可以在系统中加入简单的自动化逻辑。作为开源工具，它允许团队自托管，从而在数据安全性和系统自主性方面更可控，这一点在许多团队的内部工具场景中具备吸引力。</li><li><strong>使用门槛</strong>  Budibase 对没有前端经验的用户来说比传统开发方式更易用，但要熟练使用数据库连接、自定义视图或部署自托管服务，需要一定的技术基础。对于完全没有技术背景的团队，上手会有一定学习成本。</li><li><strong>局限性</strong>  虽然 Budibase 可以覆盖数据录入、基础权限、简单自动化，但对于复杂的多表关系、细粒度权限体系、大型流程编排或高并发场景而言，它的能力有限。如果团队需要构建更复杂的应用或业务系统，Budibase 往往只能作为基础框架使用。</li></ul><p><strong>总结</strong>  如果你需要一款能够快速搭建内部工具、具备结构化数据管理、自托管能力和基础权限控制的平台，Budibase 是一个灵活且成本相对较低的选择。但对于更复杂的业务场景，它更适合作为轻量级框架，而不是完整的系统解决方案。</p><h3>Baserow</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436405" alt="Baserow.png" title="Baserow.png" loading="lazy"/></p><p>官网：<a href="https://link.segmentfault.com/?enc=flrswhj7FDZ7m4bVmgLClQ%3D%3D.fp3IPlD3xlkqfyaC6e%2BDmGtEtxf8d01IEn572eJph3g%3D" rel="nofollow" target="_blank">https://baserow.io/</a></p><p>Baserow 是一个开源、以数据库为核心的数据管理工具。它提供类似表格的界面，但本质是结构化数据管理平台，支持自托管部署，也提供云端服务。相比传统表格工具，它更强调数据模型清晰度和可扩展性，适合希望掌握数据所有权或需要本地部署的团队。</p><p><strong>使用场景</strong>  适用于希望从电子表格升级到结构化数据库，但不想直接进入工程型数据库工具的团队。典型场景包括：</p><ul><li>团队内部的轻量数据管理、表格结构化、数据录入</li><li>需要自托管的中小企业，用于数据整理、数据看板</li><li>作为前端系统或应用的后端数据源（REST API）</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据结构能力</strong>  Baserow 提供比电子表格更明确的字段类型、表之间的关联关系、记录引用等数据库特性。它保留了表格的直观体验，同时让数据结构具备更好的可维护性，更适用于中等规模的数据建模。</li><li><strong>扩展能力</strong>  作为开源工具，Baserow 拥有良好的扩展性，支持自建插件、定制字段类型、Webhook，以及用于开发场景的 API 接口。对于有一定技术能力的团队，Baserow 可以作为轻量数据库使用，也能成为内部系统的基础数据层。</li><li><strong>使用门槛</strong>  对非技术用户来说比传统数据库更易上手，但需要一定的结构化思维，尤其在表关系、多表管理上的学习成本高于普通电子表格。对技术团队而言，它比 Airtable 等 SaaS 工具更灵活，但需要负责部署和维护。</li><li><strong>局限性</strong>  Baserow 是数据管理工具，不提供应用界面构建、复杂自动化或业务流程能力。如果团队希望构建完整的业务系统，Baserow 往往只能作为数据后端，需要搭配其他工具使用。</li></ul><p><strong>总结</strong>  如果团队目标是从表格升级到更可维护的结构化数据管理，同时希望掌握数据所有权、支持自托管或进行二次开发，Baserow 是一个合适的选择。但如果需要业务流程、自动化或界面构建能力，它更适合作为系统的数据层，而非完整的应用平台。</p><h3>NocoDB</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436406" alt="NocoDB.png" title="NocoDB.png" loading="lazy"/></p><p>官网：<a href="[https://nocodb.com/" target="_blank">https://nocodb.com/</a></p><p>NocoDB 是一个开源的电子表格风格数据库管理工具。它提供类似 Airtable 的界面体验，但底层使用真实的关系型数据库，并支持自托管部署。团队可以将现有的 MySQL、PostgreSQL 或其他数据库直接连接到 NocoDB，让原本需要工程师才能管理的数据库以更可视化的方式呈现。</p><p><strong>使用场景</strong>  适用于希望从表格过渡到数据库，同时又希望保持类表格界面的团队。  典型场景包括：</p><ul><li>数据量较大、需要用关系型数据库承载的业务数据管理</li><li>中小团队希望可视化管理数据库，而无需直接操作 SQL</li><li>将 NocoDB 作为 API 数据源，提供给内部工具或前端系统使用</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据结构能力</strong>  NocoDB 底层基于真实的关系型数据库，支持更多字段类型、外键关系和结构化操作。相比普通表格或类表格工具，它在数据一致性、多表关联和数据规模上都有更强的表现。对已有数据库的可视化管理是它的重要优势。</li><li><strong>扩展能力</strong>  支持 API 自动生成、Webhook、多种数据库连接和自定义视图，适合作为系统的数据后端。开发者可以通过其自动生成的 REST API 快速集成到其他工具或内部项目中，使其在多系统协作环境中具有较高灵活度。</li><li><strong>使用门槛</strong>  对非技术用户来说，界面与表格较为接近，基本功能容易理解。不过如果涉及数据库迁移、连接或复杂结构设计，仍需要一定的技术能力。对工程团队来说，使用成本相对低，但需要负责部署与维护。</li><li><strong>局限性</strong>  NocoDB 的核心是数据管理和数据库可视化，它不包含业务系统构建、流程编排或复杂自动化内容。如果团队希望构建 CRM、审批流、库存系统等完整应用，NocoDB 更适合作为数据后端，而不是满足完整业务需求。</li></ul><p><strong>总结</strong>  对于需要管理结构化数据、希望以可视化方式操作数据库、并且需要自托管或 API 输出能力的团队，NocoDB 是优于表格工具的选择。但如果团队的需求已经延伸到流程编排、多模块业务系统或更完整的应用构建，需要再搭配其他平台联合使用。💡推荐阅读：<a href="https://link.segmentfault.com/?enc=lXiFVTmcNmgDNVaOwDV%2BUg%3D%3D.F2bmBJ1KbJs1%2BIFyJA4uYpKeU6x%2FW64wBV88Y10SZyw8LsoT13%2FBGLfJF4i53LyxyMX32D8rp85AQ1i4gUzHNQ%3D%3D" rel="nofollow" target="_blank">NocoBase 与 NocoDB：开源无代码（零代码）工具深度对比</a></p><h3>Airtable</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436407" alt="Airtable" title="Airtable" loading="lazy"/></p><p>官网：<a href="https://link.segmentfault.com/?enc=l%2FX1iXg%2FUVO4JV4tqv54yg%3D%3D.hvDaGzDR9ifjjIfgIDZyb2IHgXPzPVv8%2B0HZgDlLYQ8%3D" rel="nofollow" target="_blank">https://airtable.com</a></p><p>Airtable 是一个集电子表格、数据库和协作能力于一体的闭源 SaaS 工具。它提供灵活的数据结构、直观的界面设计和多人在线协作，但不支持自托管或本地部署，所有数据均托管在 Airtable 的云端。</p><p><strong>使用场景</strong>  适合需要比普通表格更结构化的数据管理，但又不希望投入后端开发或搭建数据库的团队。典型场景包括：</p><ul><li>内容和运营团队用于内容日历、任务分配、状态追踪</li><li>小型项目或活动管理，用于统筹人员、进度和交付物</li><li>初创企业用于联系人管理、轻量 CRM、会员运营等</li></ul><p><strong>能力表现</strong></p><ul><li><strong>数据结构能力</strong>  Airtable 支持表与表之间建立关联，数据结构比传统电子表格更清晰，更适合管理轻量到中等复杂度的数据模型。💡推荐阅读：<a href="https://link.segmentfault.com/?enc=8rdX%2BKZ5WA%2FgkY%2B%2FYT1R%2Bg%3D%3D.%2FB7HKacCieAQ0iZM2t0Zv%2BR64Qq4ffFTeVSoHIaxR97sjfCULizqfe5XygifJ1TEbXqWb3ncSxN70re3tNYSILEGkmZT8NJ0DetD%2FGvOFAs%3D" rel="nofollow" target="_blank">Airtable 的数据超出上限，3 种常见应对方式</a></li><li><strong>协作与权限</strong>  提供基础的多人协作功能，包括查看、编辑、评论和权限分配。相比本地表格，它在协作场景中的体验更为顺畅，也更容易保持数据一致。</li><li><strong>使用门槛</strong>  对非技术用户非常友好，不需要写代码即可搭建基础的数据结构和视图。界面直观，学习成本较低。</li><li><strong>局限性</strong>  当数据量增大、业务需要更多表结构、更加细致的权限控制或自动化流程时，Airtable 的能力会逐渐受到限制。</li></ul><p><strong>总结</strong>  如果团队需要一套比 Google Sheets 更结构化、更便于协作的工具，而不涉及复杂逻辑或自动化场景，Airtable 是一个实用的选择。但如果业务逐渐扩大，需要更精细的权限、更多表之间的关系或更完整的自动化体系，Airtable 可能无法满足后续的发展需求。</p><p>💡推荐阅读：<a href="https://link.segmentfault.com/?enc=DwlSiIvBCNX2EdEbMqYhhg%3D%3D.8rTQ%2FpVSXoowMtHpYQfXs9loz3ZTOOBcdm0AFr9q64LHfCnQkqficeAmuhav28ZPoDixrWsEQWKvptVSqZxtAMakknb%2FOuz0LZD0aBLkqrE%3D" rel="nofollow" target="_blank">Airtable 太贵了？5 个自托管替代方案成本&amp;功能对比</a></p><h3>Smartsheet</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436408" alt="Smartsheet.png" title="Smartsheet.png" loading="lazy"/></p><p>官网：<a href="https://link.segmentfault.com/?enc=n6Dpp%2BacoTBeGYpiaGTkDA%3D%3D.g62xy9se5R96x3r%2FzmdTGk8byeJq1VsYYrBl2shKGBk%3D" rel="nofollow" target="_blank">https://www.smartsheet.com/</a></p><p>Smartsheet 是一款面向企业级项目管理和协作的闭源 SaaS 工具。它以电子表格为基础界面，但强化了任务管理、甘特图、自动化流程与企业权限能力，适用于更规范化的团队协作场景。Smartsheet 同样不支持自托管，所有数据存储在其云端服务中。</p><p><strong>使用场景</strong>  适合需要在表格基础上进一步规范任务管理、流程协作和跨部门项目推进的团队。  典型场景包括：</p><ul><li>中大型团队的项目计划、进度管理、资源排期</li><li>运营团队的活动管理、审批流、任务分配</li><li>企业 PMO/运营管理部门的多项目协同、进度可视化</li></ul><p><strong>能力表现</strong></p><ul><li><strong>协作与权限</strong> Smartsheet 提供较为完善的权限体系，包含行级共享、查看/编辑控制、审批流、以及基于角色的访问设置，适合多人并行协作的项目管理环境。在权限细粒度和流程管理上，相比一般的表格类工具更成熟。</li><li><strong>自动化与流程管理</strong> 具有基于触发条件的自动化功能，如任务提醒、状态更新、审批流、跨表同步等，降低手工更新的成本。对于需要将任务流、审批、项目节点串联起来的团队，它能提供相对稳定的流程支持。</li><li><strong>使用门槛</strong> 界面依旧是表格风格，基础使用不需要技术背景，但要充分发挥其项目管理和自动化能力，需要一定的学习成本。对于不熟悉结构化项目管理的团队，前期可能会感到偏“重”和复杂。</li><li><strong>局限性</strong>  Smartsheet 的结构灵活度不及数据库型或应用型平台，在数据之间的关系表达上能力有限；自定义程度也受到工具自身框架限制。此外，由于其定位偏企业级，整体体验和价格相比轻量工具更“重”，在小团队场景下可能不够轻便。</li></ul><p><strong>总结</strong>  如果团队的核心需求是以表格为界面，开展更规范化的项目管理、资源排期和协作流程，Smartsheet 会比 Google Sheets 更稳定、功能更全面。但如果你希望在数据结构、灵活建模或自定义应用上有更深的能力，它的框架可能会显得较为固定。</p><h2>结语</h2><p>Google Sheets 是一个优秀的表格工具，但并不是为业务系统而设计的。</p><p>当数据规模变大、协作复杂度提升、流程开始依赖结构化管理时，团队往往会需要明确的数据模型、更细致的权限控制、自动化的业务流转、可控的数据环境，以及能够随业务扩展的系统能力。</p><p>希望这一份工具清单，能够让团队在评估替代方案时有更清晰的视角——从使用场景、核心能力到价格与成本，帮助你在早期就判断哪种工具更贴近自己的实际需求。</p><p>如果这些内容对你有启发，也欢迎你分享给同样在寻找工具的团队，也许能帮助更多人更快地找到合适的方案。</p><p>相关阅读：</p><ul><li><a href="https://link.segmentfault.com/?enc=2D2i%2BuvLd9FxaLB8FT8qJg%3D%3D.fz20jFZ%2BzmoPAh7jRo9LPd%2BqaPHpGQWabwjZZUmgB5TK%2FBXGRoR%2F24Q%2BVvicgavbon7u3mZ%2FuIF5W%2FCBUAaUA40pB6vF9jSqUHbxx2WT5jW6TgOsCf6AmMxBMoOozcNS" rel="nofollow" target="_blank">6个适合做 PoC 的开源无代码/低代码工具推荐 </a></li><li><a href="https://link.segmentfault.com/?enc=uCelFBPcC6EuZCDeAEBp7A%3D%3D.i38e8SkO5NGHuRMEcAyRocUsdo4ql6qNcuvmniioiXnJlEGWKCb%2FNh8Yo8Mkm71TNYWt9YKyr4jO8%2BGGfUKyC6%2FZS2JaZm45IPGguslQ2rUUvMeCI%2FHHWXUv5sN3jcIJ" rel="nofollow" target="_blank">给开发者的无代码/低代码技术决策指南（2026）</a></li><li><a href="https://link.segmentfault.com/?enc=SbkLXYBdijiQAsG5CncbIQ%3D%3D.jf%2FBtJNbisSJ4Eka5pCATyOVKb8N8wYFnGwSugSEtkMuj3%2BNHstn399%2BdoewbRvIn5ZY5ZxVjN%2FXwx47fkKJtuQo9Iy%2BCgMsURHFUXUzRLO%2FZBSwuT0cQMv2Cy2Uyya6" rel="nofollow" target="_blank">6 大企业级无代码低代码平台 RBAC 权限体系深度对比</a></li><li><a href="https://link.segmentfault.com/?enc=lNTi3dTYovFaTc8yRk4YlA%3D%3D.hXxRlpYY%2F0Hd%2B1L0Scuhk56joLlCeQYtyZft9z%2F%2FAUmQcXxIrjBxYkBg%2BMjyumGW84YFXbzr%2B%2FcS1i8I54iktAfif4BSGoA9AZ9ZzbH8MP4%3D" rel="nofollow" target="_blank">GitHub 上最值得关注的 14 个开源 AI 低代码工具 </a></li><li><a href="https://link.segmentfault.com/?enc=Jfuxb%2BU8yALClLH8vu70qA%3D%3D.71VPEcOPk1x41nxPLZnMZU%2BejbrgsEppxVFodFzwHOHBTTM3ywxHdqgvu5QRu0IVltFf8IgkAya7KmgOYbVfUCPrAUrMLptgvrryasglsns%3D" rel="nofollow" target="_blank">11 个在 GitHub 上最受欢迎的开源无代码 AI 工具 </a></li><li><a href="https://link.segmentfault.com/?enc=JaY6aG88TVUjMChfGLqybg%3D%3D.0%2FLkfCcFHLIYqzq5aEy8vagXl2HOYYMC4x1yQPaeMV60b5MGtg8yjF5cMB6cEBYSm2IVoNgNo9w9A%2BcG0Ti12WW6ne2nmxWYcMZRMlDSt%2BA%3D" rel="nofollow" target="_blank">GitHub 上 Star 数量前 18 的开源 AI Agent 项目</a></li><li><a href="https://link.segmentfault.com/?enc=7IbNrBXunVIWER3xXNYr9w%3D%3D.3aZNYUk8g9IMT8AHdB%2BHkZcGjbucZuB%2FrGphKtKSEsQkHsNcGe0xkykRv5tVb4R87Jo%2Frt4dUSEY3%2BZyEoF6LQ%3D%3D" rel="nofollow" target="_blank">GitHub 上 Star 数量前 20 的开源 AI 项目</a></li></ul>]]></description></item><item>    <title><![CDATA[H-ZERO前端开发：编辑组件选型实用指]]></title>    <link>https://segmentfault.com/a/1190000047436439</link>    <guid>https://segmentfault.com/a/1190000047436439</guid>    <pubDate>2025-11-28 17:16:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436442" alt="" title=""/></p><h2>一、引言</h2><p>编辑组件作为 H-ZERO 前端生态中的核心组件之一，其选型直接关系到开发效率和产品质量。面对前端开发中编辑组件的多样选择。本文将为您揭示 H-ZERO 项目中编辑组件选型思路，帮助您轻松应对选型挑战！</p><h2>二、富文本编辑</h2><p>H-ZERO 前端团队提供了两种适用于不同场景的富文本编辑组件：</p><p><strong>NewRichTextEditor</strong>：基于 Quill 2.x 封装，具有轻量级、易于扩展，功能全面且适配移动端的特点，对于有跨端需求的场景更推荐使用。</p><p><strong>RichTextEditor</strong>：基于 CKEditor 4.x 封装的企业级富文本编辑器，支持多种格式和高级功能。</p><p>接下来，给大家分别介绍具体的使用场景：</p><h3>场景一：评论、回复类场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436443" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047436444" alt="" title="" loading="lazy"/><br/><strong>❇ 推荐组件：NewRichTextEditor</strong></p><ul><li><strong>轻量级</strong>：意味着它占用的前端资源更少。</li><li><strong>license 无限制</strong>：允许免费用于个人及商业用途。</li><li><strong>多端适配性好</strong>：能很好地适应不同设备间的屏幕尺寸差异。</li><li><strong>文件上传便捷</strong>：支持与标准的 H-ZERO 文件服务绑定。</li><li><strong>自带预览组件</strong>：可以直接支持富文本预览。</li><li><strong>多语言同屏编辑</strong>：提供了强大的多语言内容创建能力，允许用户在同一文档内切换多种语言进行编辑。</li><li><strong>DataSet 双向绑定</strong>：简化开发流程，还提高了开发效率！</li></ul><h3>场景二：模板编辑类场景<img referrerpolicy="no-referrer" src="/img/remote/1460000047436445" alt="" title="" loading="lazy"/></h3><p><strong>❇ 推荐组件：RichTextEditor</strong></p><p>RichTextEditor 组件在支持 <strong>DataSet 双向数据绑定、多语言编辑</strong> 的同时，对于模板编辑类场景具有以下优势：</p><ul><li><strong>保留特殊标记与内容保护</strong>：可以通过配置使其不自动清理未识别的标签和属性，从而保留 #foreach、#end 等模板引擎所需的特殊标记符。</li><li><strong>源码视图增强控制</strong>：内置的源码视图功能，允许开发者直接查看并编辑生成的 HTML 代码。这对于需要对模板进行精细控制的情况非常有用，比如调整样式、添加额外的数据属性等。</li><li><strong>版本安全性兼容</strong>：已处理 CKEditor 4.x 低版本的安全性问题。</li></ul><blockquote><strong>注意事项</strong><br/>1、移动端的使用效果不佳，更适用于 PC 端的使用场景。<br/>2、功能使用受限，因为 license 限制，无法升级到 CKEditor 5.x。</blockquote><h2>三、代码编辑</h2><p>随着业务需求的扩展，各产品对在线代码编辑功能的需求也在日益增长。接下来，我们将介绍在H-ZERO平台中，在不同代码编辑场景下的使用情况。</p><h3>场景一：代码配置类场景</h3><p>比如：仪表板组件支持 JSON 格式的代码片段配置，如下图所示<img referrerpolicy="no-referrer" src="/img/remote/1460000047436446" alt="" title="" loading="lazy"/></p><p><strong>❇ 推荐组件：CodeMirror</strong></p><ul><li><strong>轻量</strong>：浏览器加载前端资源更少。</li><li><strong>支持语法高亮</strong>：比如：JSON、yaml、html 等。</li><li><strong>自动代码格式化</strong>：自带的格式化功能可以自动调整代码布局，减少了因格式错误而引发的问题。</li></ul><h3>场景二：代码开发、调试类场景</h3><p>比如：飞搭的前端自定义脚本功能<img referrerpolicy="no-referrer" src="/img/remote/1460000047436447" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047436448" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047436449" alt="" title="" loading="lazy"/></p><p><strong>❇ 推荐组件：Monaco Editor</strong></p><blockquote><strong>注意事项</strong><br/> 1、代码智能提示需要开发者根据实际情况去自行定制。 <br/> 2、断点调试能力没有提供标准服务，如有需求可以联系飞搭产品同事进行交流</blockquote><p>综上所述，建议各业务团队根据实际业务需求进行编辑类组件选型。如有疑问，可在<a href="https://link.segmentfault.com/?enc=ixGxHO02MlzKAcwYMnG9qw%3D%3D.rtRMWqCK8M4qbDN88x61yyztI2FEltDayLDvlkg1ttP9GauEoSu8Nh7A1DShU4kP" rel="nofollow" target="_blank">开放平台</a>提交咨询工单进行反馈。</p><h2>欢迎试用</h2><p>我们将持续优化增强编辑组件，优化客户体验。如果你有更好的想法和建议，欢迎您积极反馈给我们。</p><p>● <a href="https://link.segmentfault.com/?enc=9PqnjYW854YCvrPN28OeAQ%3D%3D.JG60Aq6lIvgVC3p5Kee98df%2F7UyuMtVU5dmBAedq6BVEptn28jTJQx%2F8%2Fiwgp0TWVaPcXNMoWB5QLNjPfblMQhQAHi9vdnWYet3%2F9ryYwqB4qW0pYZcTGWUiH%2FrSjCV2dEQqqzeuO9E%2BUWBmHcIGUQ%3D%3D" rel="nofollow" target="_blank">NewRichTextEditor 使用文档</a></p><p>● <a href="https://link.segmentfault.com/?enc=HMA7IVR7yw3gOmHt5Vbtbg%3D%3D.98VhGCtfwhm%2BEM9DSd1msk7tdSwQqNvq96yLCewWzpKty2zd19dolWPX75Vmfts9jRBqHUByrRYdH%2FaMg5mYoBXpPiHolMvgBM3MVcC4WRqswtd3toDnVHD3avMkLttpzA9Da4duwuHdm0w8gJr3rg%3D%3D" rel="nofollow" target="_blank">RichTextEditor 使用文档</a></p><p>● <a href="https://link.segmentfault.com/?enc=se4odqltZtERAzZjJrOTCQ%3D%3D.UFiaIMF0HSB5fep3RZQAXetHhWS0UJtoQYACArewQsgGtpGkNbHpyJ4c%2BUlMwc1r" rel="nofollow" target="_blank">帮助与反馈</a></p>]]></description></item><item>    <title><![CDATA[OceanBase 年度发布会 Hand]]></title>    <link>https://segmentfault.com/a/1190000047436458</link>    <guid>https://segmentfault.com/a/1190000047436458</guid>    <pubDate>2025-11-28 17:15:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3><strong>简介</strong></h3><p>本次 Workshop 是 OceanBase 新品发布会期间的特别活动，主要关注如何基于 OceanBase seekdb 快速构建 AI 原生应用。活动打破了常规的 PPT 宣讲模式，重点在于代码实操。在来自 LangChain Community、Dify 和 OceanBase 的技术专家指导下，现场数百名开发者在两小时内，基于 seekdb 实际动手完成了从环境部署、Agentic RAG 搭建到构建具备“长期记忆”智能体的完整开发流程。</p><p>现场小伙伴直观感受到了具备原生 AI 能力的轻量数据库是如何有效降低应用开发门槛的。本文将详细回顾这场实战 Workshop。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436461" alt="" title=""/></p><h3><strong>一、OceanBase seekdb 介绍</strong></h3><p>本次 Workshop 的所有实验均基于 <strong>OceanBase seekdb</strong> 进行，这是 OceanBase 推出的面向 AI 应用场景的轻量嵌入式数据库。</p><p><strong>解决什么问题</strong>？在目前的 AI 应用开发架构中，开发者通常需要同时维护关系型数据库（存储结构化数据）和向量数据库（存储非结构化向量数据）。这种架构不仅带来了较高的数据一致性维护成本，还可能产生跨系统查询的延迟。seekdb 的设计目标是实现“结构化数据”与“非结构化向量数据”的统一存储与检索，并以 1C2G 轻量的方式简化 AI 应用的数据架构。</p><h3><strong>二、Workshop 环境配置环境指南</strong></h3><p>Workshop 的第一个环节是基础环境配置。现场为了规避 Wi-Fi 网络高并发可能带来的延迟，我们为每位参会者都准备了阿里云 ECS 云服务器作为实验环境。以下是本地电脑(Mac/Windows)详细安装流程(以 CLI 为主）：</p><ol><li>Docker (用于三个实验)<br/>Mac: 下载 Docker Desktop 并安装。<br/>Windows: 同样下载 Docker Desktop，需开启 WSL2，安装后启动 Docker 应用。</li><li>Python 3.10+ (用于三个实验)<br/>下载 Python 官方安装包 进行安装。<br/>安装完成后检查：</li></ol><pre><code class="plain">python --version # 应输出 3.10 或更高</code></pre><ol start="3"><li>uv (Python 包管理器)</li></ol><pre><code class="plain">pip install uv</code></pre><ol start="4"><li>seekdb (用于三个实验)</li></ol><p>我们将使用 Docker 快速启动一个 seekdb 实例。针对国内网络环境和不同的芯片架构，提供了相应的镜像源。</p><pre><code class="plain"># 获取docker 镜像
docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/oceanbase/seekdb:latest
docker tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/oceanbase/seekdb:latest  docker.io/oceanbase/seekdb:latest

# arm机器
docker pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/oceanbase/seekdb:latest-linuxarm64
docker tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/oceanbase/seekdb:latest-linuxarm64  docker.io/oceanbase/seekdb:latest

# 方式一: docker 方式安装
docker run -d \
  --name seekdb \
  -p 2881:2881 \
  -v ./data:/var/lib/oceanbase/store \
  oceanbase/seekdb:latest</code></pre><ol start="5"><li>powermem (用于实验三)</li></ol><pre><code class="plain">pip install powermem</code></pre><ol start="6"><li>Dify(用于实验二)</li></ol><pre><code class="plain">git clone https://github.com/langgenius/dify.git
cd dify/docker
docker-compose up -d</code></pre><ol start="7"><li>Jupyter(用于实验一)</li></ol><pre><code class="plain">pip install jupyter</code></pre><ol start="8"><li>Qoder:  下载: <a href="https://link.segmentfault.com/?enc=URm2FuLa%2Fk5hmH4RqCmfZw%3D%3D.myXE7Zxva087x0BmWav6Wnw21woBN1eMUd%2F0VxtYzgk%3D" rel="nofollow" target="_blank">https://qoder.com/download</a> (选择适合自己的电脑的qoder包下载，请注意 qoder 需要注册方可使用。）</li></ol><h3><strong>三、基于 LangChain V1 和 OceanBase seekdb</strong></h3><h3><strong>快速构建 Agentic RAG</strong></h3><p>讲师：LangChain Ambassador 张海立</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436462" alt="" title="" loading="lazy"/></p><p>本实验基于 <strong>LangChain v1</strong> 的最新 Agent 构建标准和 <strong>OceanBase seekdb</strong> 的向量存储和混合检索能力，让 AI 读懂一份 Nike 2023 财报（PDF），并能回答相关财务问题。 <strong>核心逻辑：</strong> 文档切片 -&gt; 存入 seekdb -&gt; 封装为 Tool -&gt; 绑定 Agent。</p><ul><li>将一份非结构化的 Nike 2023 财报 PDF 文档转化为计算机可理解的向量数据</li><li>通过定义检索工具并调用 LangChain v1 的 <code>create_agent</code> 接口，打造具备<strong>推理能力</strong>的 AI Agent。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436463" alt="" title="" loading="lazy"/></p><h5><strong>核心操作步骤</strong></h5><p><strong>步骤 1：文档处理与向量化</strong> 首先，我们需要将 PDF 文档加载并切割成适合模型处理的小块（Chunks）。 在 LangChain 中，我们使用 <code>PyPDFLoader</code> 加载文档，然后使用 <code>RecursiveCharacterTextSplitter</code> 进行切分。</p><pre><code class="plain">from langchain_community.vectorstores import OceanBase

# 初始化 OceanBase 向量存储
docsearch = OceanBase.from_documents(
    documents, 
    embeddings, 
    connection_string="127.0.0.1:2881..."
)</code></pre><p>这一步在后台，seekdb 会自动创建一张表，并将文本的向量（Embedding Vector）和原始内容存储在同一行记录中。</p><p><strong>步骤 2：构建检索工具</strong> 我们将上一步生成的 <code>docsearch</code> 封装为一个 LangChain Tool。</p><pre><code class="plain">retriever_tool = create_retriever_tool(
    docsearch.as_retriever(),
    "nike_financial_report",
    "搜索并返回关于 Nike 2023 财报的详细信息。"
)</code></pre><p>注意 <code>description</code> 参数非常重要，LLM 会根据这段描述来判断何时调用这个工具。</p><p><strong>步骤 3：初始化 Agent 并执行</strong> 使用 LangChain V1 的 <code>create_tool_calling_agent</code> 接口，将 LLM（如 GPT-4 或通义千问）与我们定义的工具绑定。</p><p>详细步骤请参考：<a href="https://link.segmentfault.com/?enc=cC0ytKnevuGREP62JEPWJw%3D%3D.zbF%2FFBVHW5a1PGVpaJKFRaLV4tlPrcqS3FF%2Bhe0dU6%2FpgZ3twoIDuhLMT5BkGgtg" rel="nofollow" target="_blank">https://ask.oceanbase.com/t/topic/35634850</a></p><h3><strong>四、基于 Dify 和 OceanBase，快速构建 AI 应用</strong></h3><p>讲师：郑立，Sr. Developer Relations, Dify</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436464" alt="" title="" loading="lazy"/></p><p>本实验旨在验证 OceanBase seekdb 对 AI 应用的一体化支撑能力。通过部署 seekdb 并修改 Dify 核心配置，将原本分离的向量库与元数据库统一替换为 seekdb。在简化架构的同时，验证在 RAG 场景下的完整可用性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436465" alt="" title="" loading="lazy"/></p><h5><strong>核心操作步骤</strong></h5><p><strong>步骤 1：修改 Docker Compose 配置</strong> 进入 Dify 的 <code>docker</code> 目录，编辑 <code>.env</code> 文件或 <code>docker-compose.yaml</code>。需要将 Dify 的数据库连接指向我们启动的 seekdb 容器。</p><p><strong>步骤 2：配置向量后端</strong> 在 Dify 的系统设置文件或环境变量中，将 Vector Store 的类型指定为 <code>OceanBase</code>。当用户在 Dify 界面上传知识库文件时，Dify 会将切片后的向量数据写入 seekdb 的向量表中。</p><p><strong>步骤 3：构建知识库应用</strong> 重启 Dify 容器组后，进入 Web 界面：</p><p>详细步骤请参考：<a href="https://link.segmentfault.com/?enc=MGDhxgGumkkk9gkTAHdnjA%3D%3D.KmgUQIu5GtJLlGP8ZDqfY%2B%2BTqtxBzIQUHEPSBAbvPkrJ%2F44SC9baLu%2BYcud8ftLl" rel="nofollow" target="_blank">https://ask.oceanbase.com/t/topic/35634856</a></p><h3><strong>五、让 AI 记住你，基于 OceanBase 构建具备上下文记忆的智能体实践</strong></h3><p>讲师：汤庆 OceanBase 技术专家</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436466" alt="" title="" loading="lazy"/></p><p>本实验核心目标是解决 AI 智能体“遗忘”对话上下文的问题。通过集成 <strong>OceanBase seekdb</strong> 作为向量与结构化数据的混合存储底座，并使用 <strong>PowerMem</strong> 进行记忆管理，展示了如何让 AI 拥有“长期记忆”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436467" alt="" title="" loading="lazy"/></p><p><strong>步骤 1：部署 PowerMem，Dify + PowerMem MCP 环境集成</strong>，我们在 Dify 环境中配置 PowerMem MCP，打通底层记忆通道。这一步也使其具备了访问 OceanBase seekdb 进行长久记忆存储与检索的能力。</p><p><strong>步骤 2：Vibe Coding 挑战</strong>，我们将一段提示词复制到 AI 编程助手 <strong>Qoder</strong> 中，要求它参照 PowerMem 的官方示例，自动生成一套代码审查智能体。</p><p>详细步骤请参考：<a href="https://link.segmentfault.com/?enc=8HWN6vh97V2Zm9Ro58Wd3g%3D%3D.M0imxVjt%2FvFJkTLC66uYY36EmLhqKWQWCg865mWtB0bOAxSOi%2Bgy1hwOOTljVKm8" rel="nofollow" target="_blank">https://ask.oceanbase.com/t/topic/35634483</a></p><h2><strong>最后</strong></h2><p>通过上述三个实验，我们从不同维度验证了 OceanBase seekdb 在 AI 应用开发中的实际能力。本次 Workshop 只是一个起点。seekdb 的轻量化和易用性，旨在让每一位开发者都能在自己的笔记本上，以最低的成本探索 AI 原生应用的开发。</p><p>在此，我们要特别致谢参与本次社区生态共建的开源伙伴，感谢 Dify, LangChain Community, Qoder 的鼎力支持，感谢每一位参与的开发者。</p>]]></description></item><item>    <title><![CDATA[飞搭系列 | 导入导出模板配置检查升级，]]></title>    <link>https://segmentfault.com/a/1190000047436477</link>    <guid>https://segmentfault.com/a/1190000047436477</guid>    <pubDate>2025-11-28 17:14:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047436480" alt="" title=""/></p><h2>前言</h2><p>飞搭低代码平台（FeiDa，以下简称“飞搭”），为企业提供在线化、灵活的业务应用构建工具，支持高低代码融合，助力企业低门槛、高效率和低成本地快速应对市场变化，加速复杂业务场景落地。</p><h2>概要介绍</h2><p>日常工作中，导入导出模板配置出错、依赖项失效等问题易导致数据处理中断、重复返工。</p><p>为解决这一痛点，飞搭低代码平台新增导入导出模板的通用检查功能，保存模板时自动校验配置合理性，精准定位问题并支持快速修改，让模板配置更稳妥、数据处理更顺畅。</p><p>该功能覆盖多种校验需求，核心优势体现在三方面：</p><ul><li><strong>自动校验更全面</strong>：保存模板时系统会自动检查字段依赖、事务处理流状态、编码格式等各类常见问题，包括业务对象字段删除、事务流禁用、编码别名重复等，无需手动排查。</li><li><strong>问题定位更精准</strong>：校验结果会明确标注问题级别、来源、具体位置及详细说明，支持通过搜索快速筛选目标问题。</li><li><strong>修改操作更便捷</strong>：每个问题都支持点击&lt;去修改&gt;直达操作，无需手动查找配置位置，大幅缩短问题修复时间。</li></ul><h2>一、场景说明</h2><ul><li><strong>模板字段依赖失效</strong>：配置模板后，若使用的业务对象字段被删除或字段类型变更导致模板配置不可用，保存时即时触发错误提示，避免后续导入导出失败。</li><li><strong>事务处理流异常</strong>：事务处理流的出入参格式不符合模板的使用条件，或依赖的事务处理流被禁用时，主动校验并提醒，防止因事务处理流不可用导致流程中断。</li><li><strong>配置格式不规范</strong>：模板字段编码别名重复、格式错误、序号冲突等细节问题，无需手动核对，系统自动校验并定位至问题字段。</li></ul><h2>二、实现效果</h2><ol><li>以导出模板为例，进入“导出模板”页面，新建导出模板。</li><li>导出模板字段与事务处理流的配置：按照业务需求新建Sheet页、添加模板字段、配置筛选参数、过滤条件以及数据查询扩展的事务处理流。<img referrerpolicy="no-referrer" src="/img/remote/1460000047436481" alt="" title="" loading="lazy"/></li></ol><ol start="3"><li>保存触发校验：完成配置后点击&lt;保存&gt;按钮，系统将自动执行全量校验：</li><li>无错误时：异常弹窗自动收起，模板直接保存成功；</li><li>有错误/警告时：自动展开问题检查弹窗，展示所有问题详情，问题列表包含问题级别、问题来源、问题位置、问题信息与操作列的展示。<img referrerpolicy="no-referrer" src="/img/remote/1460000047436482" alt="" title="" loading="lazy"/></li><li>问题处理：在异常弹窗中，可通过以下操作高效处理问题：</li></ol><ul><li>筛选与搜索：按“全部/错误/警告”标签分类查看，支持根据问题位置、错误信息进行问题的模糊搜索；<img referrerpolicy="no-referrer" src="/img/remote/1460000047436483" alt="" title="" loading="lazy"/></li><li>定位修改：点击操作列&lt;去修改&gt;按钮，直接打开存在问题的配置页，移入警告图标可查看具体问题信息，并进行快速修改；<img referrerpolicy="no-referrer" src="/img/remote/1460000047436484" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047436485" alt="" title="" loading="lazy"/></li><li>刷新校验：修改完成后点击弹窗&lt;刷新&gt;按钮，重新校验问题是否已经解决。<img referrerpolicy="no-referrer" src="/img/remote/1460000047436486" alt="" title="" loading="lazy"/></li></ul><h2>结语</h2><p>飞搭低代码平台作为 H-ZERO 生态的重要组成部分，致力于充分融合 H-ZERO 的各平台能力，提供企业用户在线化灵活搭建业务应用的能力，支撑企业普惠化（低门槛、高协作）、敏态化（高效率）和低成本化地快速响应市场变化，加速复杂业务场景落地。</p><p>本篇介绍了飞搭导入导出模板的通用检查功能，以“自动校验、精准定位、便捷修改”为核心，让导入导出模板的配置使用更高效、更稳妥。无论是新手用户还是资深开发者，都能通过该功能降低配置门槛、减少出错概率。</p><p>接下来，我们将持续推出飞搭平台专题系列教程，帮助您更好地掌握飞搭平台的使用技巧，敬请期待！</p><h2>联系我们</h2><ol><li>如果您想了解飞搭更详细的功能介绍和产品信息请查阅我们的产品文档：<br/>请在PC端打开 👉汉得焱牛开放平台<br/><a href="https://link.segmentfault.com/?enc=HgZKslyZHo8MNO39p75HyA%3D%3D.gEGwHYb0Cs1IdFctk%2BoYbVJKaAW3nXWLMS4UjsgKo4Jm3aJ8SsjB%2BowZtLAzP7V1UrXFRfud8zHDk4GdOpsM2mTQiBudrZLAY7K8Ho5503cu1qoXyS5GjuNzCuMunp8h" rel="nofollow" target="_blank">https://open.hand-china.com/document-center/doc/product/10001...</a></li></ol><ol start="2"><li>如果您有疑问或者建议，可以通过开放平台进行工单反馈，问题分类请选择【产品/汉得aPaaS平台-飞搭】：请在PC端打开👉汉得焱牛开放平台<br/><a href="https://link.segmentfault.com/?enc=OEHNmFpbVowj6QZbT4EfXA%3D%3D.HadlxsXtI4mUrW%2BBHQ1M8jcSr1iicZxqiHZjkXiRWHc%3D" rel="nofollow" target="_blank">https://open.hand-china.com/</a></li></ol><ol start="3"><li>相关产品咨询或更多信息了解，欢迎联系我们<br/>邮箱：<a href="mailto:openhand@vip.hand" target="_blank">openhand@vip.hand</a>-china.com</li></ol>]]></description></item><item>    <title><![CDATA[体系-AI人工智能算法工程师(完结) 微]]></title>    <link>https://segmentfault.com/a/1190000047436504</link>    <guid>https://segmentfault.com/a/1190000047436504</guid>    <pubDate>2025-11-28 17:13:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在AI技术快速迭代的今天，算法工程师已成为科技企业的核心岗位之一👇🏻ke🍊：xingkeit点top/9440/。无论是深度学习、机器学习还是自然语言处理（NLP）、计算机视觉（CV）等细分领域，掌握核心知识体系与应对面试高频考点是通往理想岗位的必经之路。本文将从基础理论、核心算法、工程实践、面试策略四大维度，系统梳理AI算法工程师的必备知识点与面试应对技巧。</p><p>一、基础理论：构建AI思维的基石<br/>1.1 数学基础<br/>线性代数：矩阵运算（如矩阵乘法、逆矩阵）、特征值分解、奇异值分解（SVD）在降维（PCA）和推荐系统中的应用。<br/>概率论与统计：贝叶斯定理、最大似然估计（MLE）、最大后验估计（MAP）、常见分布（如高斯分布、泊松分布）及假设检验。<br/>优化理论：梯度下降法及其变种（如SGD、Adam）、凸优化与非凸优化的区别、拉格朗日乘数法在约束优化中的应用。<br/>1.2 机器学习基础<br/>模型评估与选择：过拟合与欠拟合的判断方法、交叉验证（如K-fold）、评估指标（如准确率、召回率、F1值、AUC-ROC）。<br/>损失函数：均方误差（MSE）、交叉熵损失（Cross-Entropy）、Hinge Loss等在回归与分类任务中的应用场景。<br/>正则化技术：L1/L2正则化、Dropout、Early Stopping的原理及作用。<br/>1.3 深度学习基础<br/>神经网络结构：全连接层、卷积层（CNN）、循环层（RNN/LSTM/GRU）、注意力机制（Attention）的原理与适用场景。<br/>激活函数：Sigmoid、ReLU、LeakyReLU、Swish等函数的特性及选择依据。<br/>反向传播：链式法则、梯度消失与爆炸问题的解决方案（如梯度裁剪、Batch Normalization）。<br/>二、核心算法：掌握主流技术方向<br/>2.1 计算机视觉（CV）<br/>目标检测：两阶段检测器（如Faster R-CNN）与单阶段检测器（如YOLO、SSD）的对比，Anchor-based与Anchor-free方法的演进。<br/>图像分割：语义分割（如U-Net）、实例分割（如Mask R-CNN）的核心思想，Transformer在CV中的应用（如ViT、Swin Transformer）。<br/>视频理解：光流法、双流网络（Two-Stream Network）、3D卷积（C3D）在动作识别中的技术路径。<br/>2.2 自然语言处理（NLP）<br/>预训练模型：Transformer架构解析，BERT、GPT、T5等模型的设计差异，Prompt Learning与Fine-tuning的适用场景。<br/>序列建模：RNN/LSTM在机器翻译中的应用，Seq2Seq模型与Attention机制的结合，Transformer的并行化优势。<br/>知识图谱：实体识别、关系抽取、图神经网络（GNN）在知识推理中的实践。<br/>2.3 强化学习（RL）<br/>马尔可夫决策过程（MDP）：状态、动作、奖励、转移概率的定义，值函数（Value Function）与策略函数（Policy Function）的更新规则。<br/>Q-Learning与Deep Q Network（DQN）：经验回放（Experience Replay）与目标网络（Target Network）的作用，解决连续动作空间的策略梯度方法（如PPO、SAC）。<br/>多智能体强化学习：合作与竞争场景下的算法设计（如MADDPG、QMIX）。<br/>三、工程实践：从模型到落地的关键能力<br/>3.1 数据处理与特征工程<br/>数据清洗：缺失值处理（如填充、删除）、异常值检测（如3σ原则、IQR方法）、数据平衡（如过采样、欠采样）。<br/>特征提取：文本特征（如TF-IDF、Word2Vec）、图像特征（如SIFT、HOG）、时序特征（如滑动窗口统计、傅里叶变换）。<br/>特征选择：过滤法（如方差阈值）、包裹法（如递归特征消除）、嵌入法（如L1正则化）的适用场景。<br/>3.2 模型训练与调优<br/>超参数优化：网格搜索（Grid Search）、随机搜索（Random Search）、贝叶斯优化（Bayesian Optimization）的效率对比。<br/>分布式训练：数据并行（Data Parallelism）与模型并行（Model Parallelism）的实现原理，框架支持（如PyTorch的DistributedDataParallel）。<br/>模型压缩：知识蒸馏（Knowledge Distillation）、量化（Quantization）、剪枝（Pruning）在移动端部署中的应用。<br/>3.3 部署与监控<br/>模型服务化：RESTful API设计、gRPC协议的优势，模型版本管理（如MLflow、DVC）。<br/>性能监控：延迟（Latency）、吞吐量（Throughput）、资源利用率（CPU/GPU）的监控指标，A/B测试与灰度发布策略。<br/>伦理与安全：模型偏见检测（如Fairness Indicators）、对抗样本防御（如对抗训练、输入净化）。<br/>四、面试策略：高频考点与答题技巧<br/>4.1 高频考点分类<br/>理论题：如“解释梯度消失问题及解决方案”“对比BERT与GPT的差异”。<br/>算法题：如“设计一个推荐系统的架构”“如何优化模型推理速度”。<br/>项目题：如“在项目中遇到的最大挑战是什么？如何解决？”“如何评估模型效果？”。<br/>开放题：如“未来3年AI技术的发展趋势”“如何平衡模型精度与效率”。<br/>4.2 答题技巧<br/>结构化表达：采用“总-分-总”结构，先给出核心结论，再分点阐述细节，最后总结价值。<br/>STAR法则：描述项目经历时，按“情境（Situation）、任务（Task）、行动（Action）、结果（Result）”展开。<br/>举一反三：遇到不熟悉的问题时，可关联已知知识点（如将“Transformer”类比“CNN的局部注意力机制”）。<br/>展示思考过程：即使答案不完美，也要体现逻辑性（如“我首先会尝试A方法，如果效果不佳，再考虑B方案”）。<br/>4.3 避坑指南<br/>避免空谈理论：结合实际场景说明（如“在图像分类中，ResNet通过残差连接解决了深层网络退化问题”）。<br/>不贬低前公司/团队：即使项目失败，也可强调“从中学到了XX经验”。<br/>不夸大个人能力：如实描述贡献（如“主导了数据标注流程设计，但模型调优由团队共同完成”）。<br/>五、总结：持续学习与职业成长<br/>AI算法工程师的成长路径是“理论-实践-创新”的螺旋上升过程。建议：</p><p>紧跟技术趋势：关注顶会论文（如NeurIPS、ICML、CVPR）、开源框架（如PyTorch、TensorFlow）的更新。<br/>参与开源项目：通过GitHub贡献代码，积累工程经验。<br/>构建知识网络：将零散知识点串联成体系（如将“注意力机制”从NLP扩展到CV、语音领域）。<br/>培养软技能：沟通能力（跨团队协作）、商业思维（理解业务需求）、领导力（技术选型与团队管理）。<br/>AI的未来属于那些既懂技术又懂业务的复合型人才。掌握核心知识体系，灵活应对面试挑战，你离理想岗位仅一步之遥！</p>]]></description></item><item>    <title><![CDATA[Unity GameFramework框]]></title>    <link>https://segmentfault.com/a/1190000047436570</link>    <guid>https://segmentfault.com/a/1190000047436570</guid>    <pubDate>2025-11-28 17:13:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本专栏是 Unity 生态中热门的 GameFramework 框架系统性学习合集，专为解决开发者 “框架上手难、源码看不懂、项目不会用” 三大痛点打造。内容涵盖框架核心功能快速应用（模块配置、资源管理、UI 框架、事件系统等）、详细使用说明（场景适配、性能优化、跨平台兼容）、深度源码分析（核心逻辑拆解、设计模式解读），并结合 VR 游戏、虚拟仿真教育真实项目案例，让你从 “会用” 到 “精通”，真正掌握框架的底层逻辑与实战技巧。每篇文章都包含实操步骤、代码示例、问题解决方案，零基础也能快速跟上，资深开发者可直击核心源码，提升技术深度。</p><p><img width="723" height="1503" referrerpolicy="no-referrer" src="/img/bVdncy1" alt="" title=""/></p><p><strong>张尧：</strong> CSDN博客专家、Unity3D领域优质创作者、《Unity 3D从入门到实战》作者</p><p>先后任职于北京界源科技（VR 游戏开发）、郑州某公司（开发负责人），深耕 VR、虚拟仿真教育领域，主导多款商业项目从 0 到 1 落地。同时获得 CSDN 博客专家、Unity3D 领域优质创作者头衔，以及华为云云享专家、阿里云社区专家、腾讯云专家博主，累计输出百万字技术干货，帮助数万开发者少走弯路。</p><hr/><h2>目录</h2><p>1｜框架介绍</p><p>2｜GameFramework框架介绍</p><p>3｜快速启动</p><p>4｜GameFramework框架内置模块</p><p>4-1｜全局配置（Config）</p><p>4-2｜数据节点（Data Node）</p><p>4-3｜数据表（Data Table）</p><p>4-4｜内置模块之调试器（Debugger）</p><p>4-5｜下载（Download）</p><p>4-6｜实体（Entity）</p><p>4-7｜事件（Event）</p><p>4-8｜文件系统（File System）</p><p>4-9｜有限状态机（FSM）</p><p>4-10｜本地化（Localization）</p><p>4-11｜网络（Network）</p><p>4-12｜对象池（Object Pool）</p><p>更新中...</p><hr/><p>本篇转载自<a href="https://link.segmentfault.com/?enc=Yjxr4MtyJZwAOzBi%2BPoNrQ%3D%3D.%2FQGhraJJQLoP4pq9rkrdZ8BwrtDpa4HtpPva%2BUZ2wbGll%2FuzgolQURSyl5AsHrIJ" rel="nofollow" target="_blank">《Unity GameFramework框架理论解析与应用实践》</a>的第1节。</p><h3>1.1 前言</h3><p>这是GameFramework框架教程的第一篇，但是博主不准备直接就讲GameFramework框架。</p><p>博主准备从框架开始说起，讲讲框架的“前世今生”，心急的小伙伴可以直接跳转到下一篇：GameFramework框架介绍。</p><hr/><h3>1.2 框架（Framework）介绍</h3><p><strong>1. 框架（Framework）是什么</strong></p><p>框架（Framework）通常被理解为一种基础要素的集合，它们用于承载系统的必要功能。</p><p>在不同的技术和领域中，如IT和软件开发，框架具有特定的意义和作用：</p><ul><li>约束性：框架定义了解决特定问题的边界，并将相关的软件组件约束在这个范围内，以保持框架的内聚性和解决问题的专注性。</li><li>支撑性：框架本身不直接解决问题，而是提供一个基础的支撑结构，使得在其上构建的解决方案更为灵活和高效。框架通常会包括一系列的约定、配置和工具，这些辅助性工具帮助开发者简化复杂任务，提高开发效率。</li></ul><p>框架可以被视为一种未完成的半成品，需要在使用时赋予它具体的业务含义。</p><p>常见的例子包括模型-视图-控制器（MVC）、WPF等，它们包含了对特定问题的解决方法的设计模式，以及一系列的工具和库，以便于开发者快速搭建和维护应用。</p><p>综上所述，框架是一种设计模式，旨在通过预定义的结构和规则来简化复杂系统的开发过程，同时确保系统的稳定性和可扩展性。</p><p><em>PS：上面的内容摘录自百度。</em></p><p>刚开始接触框架的同学是不是感觉已经有点懵了，这里我再用一个简单的例子给大家解释一下框架：</p><p>小明早上出门要去出差，他会在自己的包里放入：牙膏、牙刷、毛巾、充电器、充电宝、纸巾、湿巾、U盘、钥匙等等。</p><p>小明要取钥匙，然后就在包里翻了半天，最后把东西全部倒出来，才找到了钥匙。</p><p>后来，小明找了一个女朋友小红，小红总是会在小明出门前将小明的包里面的东西整理一下：</p><p>充电器、充电宝、钥匙、纸巾、湿巾等常用的放在最外侧的兜里；</p><p>U盘小玩意放到内侧小兜里；</p><p>牙膏、牙刷、毛巾放到内包里。</p><p>这样，小明要找钥匙的时候，直接去外侧找，一下就找到了钥匙。</p><p>这个例子呢，就很生动了说明了框架是什么。</p><p>框架就是在架构内构建了一套固定流程，开发人员按照这个流程，以及条条框框进行开发，知道自己的函数写在那里，如何架构，会快速找到自己想要的东西，以此来提高开发效率。</p><p>框架指的是在某些应用领域具有通用的完备功能的底层服务，使用框架的编程人员可以在一个通用的实现的基础上开始具体的系统开发。框架提供了抽象的默认行为类集合，具体的实现可以通过重写子类或组装对象来支持应用特有的功能模块。</p><p><strong>2. 为什么用框架（Framework）</strong></p><p>简单说就是提高工作效率。</p><p>框架是一个可复用的设计构件，规定了应用的体系结构，设计了协作构件之间的依赖关系、责任分配和流程控制，表现为一组抽象类以及其实例之间的协作的方法。</p><p>作为一名程序员，在开发过程中会发现，有框架和没有框架开发起来效率差距有多大。</p><p>一个好的框架可以带来更快的开发效率、提高程序的健壮性和鲁棒性、提高程序的性能、提高团队协作、方便后续功能维护和拓展等。</p><p><strong>3. 怎么用框架（Framework）</strong></p><p>使用框架的话可以先找一些框架的文档、教程等东西先学习。</p><p>然后找一些案例进行学习，使用框架的功能，有不清楚的地方再回头仔细看这部分额文档。</p><p>当对于某个框架用的比较熟悉的时候，就可以分析框架的原理和实现过程，以及功能为什么这么设计。再用来优化之前的项目。</p><p>当然，更进一步的话，可以学习源码，为框架做贡献。</p><p><strong>4. 怎么设计框架（Framework）</strong></p><p>框架与具体功能的实现不一样，框架强调的是软件的设计重用性和可拓展性。</p><p>好的框架是相对的，它有自己特定的应用领域，合适的才是最好的。</p><p>比如你做虚仿项目使用一个游戏框架，里面的网络通信、帧同步、战斗模块你可能永远都用不上，这不就有点浪费了嘛。</p><p>言归正传，设计框架，需要在实际开发中的具体情况来看，细想架构需要分三层还是五层，每个层之间如何解耦，要用什么设计模式。</p><p>当然，设计框架的目的还是效率，一些不常用的功能可以在写的时候留下拓展方案或思路，在以后实际用上的时候再添加上，慢慢升级框架。</p><p>还可以在使用过程中提高能力和积累经验，将学习到的新技术新知识融合到框架中，使得框架更加强大，更加健壮。</p><p>框架通过小步快跑，不断迭代升级来慢慢拓展，当项目应用框架后，再根据新需求和碰到的问题去调整，让框架更加强大。</p><p>设计框架的时候有以下几个要求：</p><ol><li>代码模板化</li></ol><p>框架一般都有统一的代码风格，同一分层的不同类代码，都是大同小异的模板化结构，方便使用模板工具统一生成，减少大量重复代码的编写。在学习时通常只要理解某一层有代表性的一个类，就等于了解了同一层的其他大部分类结构和功能，容易上手。团队中不同的人员采用类同的调用风格进行编码，很大程度提高了代码的可读性，方便维护与管理。</p><ol start="2"><li>重用</li></ol><p>开发框架一般层次清晰，不同开发人员开发时都会根据具体功能放到相同的位置，加上配合相应的开发文档，代码重用会非常高，想要调用什么功能直接进对应的位置去查找相关函数，而不是每个开发人员各自编写一套相同的方法。</p><ol start="3"><li>高内聚（封装）</li></ol><p>框架中的功能会实现高内聚，开发人员将各种需要的功能封装在不同的层中，给大家调用，而大家在调用时不需要清楚这些方法里面是如果实现的，只需要关注输出的结果是否是自己想要的就可以了。</p><ol start="4"><li>规范</li></ol><p>框架开发时，必须根据严格执行代码开发规范要求，做好命名、注释、架构分层、编码、文档编写等规范要求。因为你开发出来的框架并不一定只有你自己在用，要让别人更加容易理解与掌握，这些内容是非常重要的。</p><ol start="5"><li>可扩展</li></ol><p>开发框架时必须要考虑可扩展性，当业务逻辑更加复杂、数量记录量爆增、并发量增大时，能否通过一些小的调整就能适应？还是需要将整个框架推倒重新开发？当然对于中小型项目框架，也不必考虑太多这些内容，当个人能力和经验足够时水到渠成，自然就会注意到很多开发细节。</p><ol start="6"><li>可维护</li></ol><p>成熟的框架，对于二次开发或现有功能的维护来说，操作上应该都是非常方便的。比如项目要添加、修改或删除一个字段或相关功能，只需要简单的操作，十来分钟或不用花太多的工夫就可以搞定。新增一个数据表和对应的功能，也可以快速的完成。功能的变动修改，不会对系统产生不利的影响。代码不存在硬编码等等，保证软件开发的生产效率和质量。</p><ol start="7"><li>协作开发</li></ol><p>有了开发框架，我们才能组织大大小小的团队更好地进行协作开发，成熟的框架将大大减轻项目开发的难度，加快开发速度，降低开发费用，减轻维护难度。</p><ol start="8"><li>通用性</li></ol><p>同一行业或领域的框架，功能都是大同小异的，不用做太大的改动就可以应用到类似的项目中。在框架中，我们一般都会实现一些同质化的基础功能，比如权限管理、角色管理、菜单管理、日志管理、异常处理......或该行业中所要使用到的通用功能，使框架能应用到某一行业或领域中，而不是只针对某公司某业务而设定（当然也肯定存在那些特定功能的应用框架，这只是非常少的特殊情况，不在我们的考虑范围）。</p><p><strong>5. 常用的Unity 3D框架（Framework）</strong></p><p>5.1、MVC框架</p><p>基础常用的框架</p><ul><li>表现层（View）：游戏画面，UI</li><li>逻辑层（Controller）：数据接口，操作控制，AI</li><li>数据层（Model）：数据保存，图片、声音等资源</li></ul><p>5.2 MVP框架</p><p>MVP从MVC演变而来，通过表示器将视图与模型巧妙地分开。在该模式中，视图通常由表示器初始化，它呈现用户界面（UI）并接受用户所发出命令，但不对用户的输入作任何逻辑处理，而仅仅是将用户输入转发给表示器。</p><p>MVP的全称为Model-View-Presenter，Model提供数据，View负责显示，Controller/Presenter负责逻辑的处理。MVP与MVC有着一个重大的区别：在MVP中View并不直接使用Model，它们之间的通信是通过Presenter（MVC中的Controller）来进行的，所有的交互都发生在Presenter内部，而在MVC中View会直接从Model中读取数据而不是通过Controller。</p><p>5.3 MVVM框架</p><p>MVVM 模式将MVP中的Presenter改名为ViewModel，基本上与MVP模式完全一致。</p><p>MVVM是Model-View-ViewModel的简写。它本质上就是MVC的改进版。MVVM就是将其中的View的状态和行为抽象化，让我们将视图UI和业务逻辑分开。</p><p>MVVM（Model-View-ViewModel）框架的由来便是MVP（Model-View-Presenter）模式与WPF结合的应用方式时发展演变过来的一种新型架构框架。它立足于原有MVP框架并且把WPF的新特性糅合进去，以应对客户日益复杂的需求变化。</p><p>5.4 PureMVC框架</p><p>最核心的三个层：Model，View，Controller。</p><p>PureMVC在传统MVC基础上做了许多的改进，通过结合多个“设计模式”的应用，让耦合性变得更低，也变得更加地易用，在扩展性、灵活性、重用性方面也做得更好。</p><p>设计模式的存在，其实很重要的一个职责就是解决耦合性。PureMVC用到的这些设计模式，贯穿了整个游戏框架，即便你项目中使用的不是MVC框架，你都离不开这些设计模式的应用，下面是PureMVC中使用到的设计模式：</p><ol><li>代理设计模式</li><li>中介者设计模式</li><li>外观设计模式</li><li>观察者设计模式</li><li>命令设计模式</li><li>单例设计模式</li></ol><p>5.5 MVCS框架</p><p>StrangeIOC中所指MVCS的“S”，为服务（程序外部的服务例如：Web服务）。</p><p>StrangeIoc是依据控制反转和解耦原理设计的，支持依赖注入。</p><p>控制反转即Ioc（Inversion of Control），它把传统上由程序代码直接操控的对象的调用权交给容器，通过容器来实现对象组件的装配和管理。所为的“控制反转”概念就是对组件对象控制权的转移，从程序代码本身转移到了内部的容器。</p><p>依赖注入（Dependency Injection）的基本原则是：应用组件不应该负责查找资源或者其他依赖的写作对象。配置对象的工作应该由Ioc容器负责。</p><p>5.6 ECS框架</p><p>Unity本身的组件开发就是ECS框架，ECS很适合游戏开发，在游戏引擎中比较常见，谷歌曾在Github上发布了一个名叫Entitas的ECS框架，下面我们就来介绍：</p><ul><li>Entity就是只有数据的GameObject对象，不包括方法；</li><li>每一个Entity拥有Component组件，负责Entity数据处理；</li><li>Group是拥有相同Component的Entity集合；</li><li>Context就是创建销毁Entity的工厂；</li><li>Collector收集器提供了简单的方法来处理Group中Entity变化的反应。</li></ul><p>5.7 GameFramework游戏框架</p><p>OK，说到我们这系列文章的主角了，GameFramework游戏框架。</p><p>Game Framework是一个基于Unity引擎的游戏框架，主要对游戏开发过程中常用模块进行了封装，很大程度地规范开发过程、加快开发速度并保证产品质量。</p><p>内置模块有：</p><p>◾ 基础和工具</p><p>◾ 全局配置（Config）</p><p>◾ 数据结点（Data Node）</p><p>◾ 数据表（Data Table）</p><p>◾ 调试器（Debugger）</p><p>◾ 下载（Download）</p><p>◾ 实体（Entity）</p><p>◾ 事件（Event）</p><p>◾ 有限状态机（FSM）</p><p>◾ 本地化（Localization）</p><p>◾ 网络（Network）</p><p>◾ 对象池（Object Pool）</p><p>◾ 流程（Procedure）</p><p>◾ 资源（Resource）</p><p>◾ 场景（Scene）</p><p>◾ 游戏配置（Setting）</p><p>◾ 声音（Sound）</p><p>◾ 界面（UI）</p><p>◾ Web请求（Web Request）</p><p>完整的Game Framework包含三部分：</p><ul><li><strong>GameFramework</strong> – 封装基础游戏逻辑，如数据管理、资源管理、对象池、有限状态机、本地化、事件、实体、网络、界面、声音等，此部分逻辑实现不依赖于Unity引擎，以程序集的形式提供。</li><li><strong>UnityGameFramework.Runtime</strong> – 依赖UnityEngine.dll进行对GameFramework.dll的补充实现。为了方便兼容Unity的各个版本，此部分已经以代码的形式包含在Unity插件中。</li><li><strong>UnityGameFramework.Editor</strong> – 依赖UnityEditor.dll进行对工具、Inspector的实现。为了方便兼容Unity的各个版本，此部分已经以代码的形式包含在Unity插件中。</li></ul><p>5.8 SFramework游戏框架</p><p>Sunset Game制作组自主设计研发的一款Unity通用游戏框架，设计思想类似MVC+ECS。</p><ul><li>不限于3D-ARPG游戏的万能框架Unity-Framework</li><li>独立设计开发的原创游戏框架，持续更新中</li><li>控制游戏生命周期，框架尽量不继承Monobehavior</li><li>基于单例模式，外观模式，桥接模式等设计模式</li><li>可使用PhysX物理引擎，动画帧事件等基于Monobehavior的功能</li><li>代码追求精简高效，核心代码仅5000+行</li></ul><hr/><p>以上就是<a href="https://link.segmentfault.com/?enc=zUh4Qz2sXMNhmzDqp5wokw%3D%3D.QS%2F8bYfs6nAh7AW7dSGoZbdNfsvgcmhByLAlIEp0%2F5DljIZ4v8iADYvMlklvUGM5" rel="nofollow" target="_blank">《Unity GameFramework框架理论解析与应用实践》</a>的第1节，此篇文章比较适合想快速掌握成熟框架，提升项目开发效率的<strong>Unity 初学者 / 进阶者</strong>；需要针对性解决框架在特定场景的应用问题的<strong>VR / 虚拟仿真教育领域开发者</strong>；希望通过源码分析提升技术深度，解决实际项目难点的<strong>在职 Unity 工程师</strong>；想了解框架架构设计，用于团队技术选型与项目落地的<strong>技术负责人 / 项目管理者</strong>；以及对 Unity 框架开发、游戏引擎架构感兴趣，寻求系统性学习路径的<strong>编程学习者</strong>。</p><p>读完全篇后你会获得：</p><p>1、<strong>快速上手能力</strong>：掌握 GameFramework 框架核心模块使用方法，1 小时搭建基础项目架构；</p><p>2、<strong>源码解读思维</strong>：看透框架底层实现逻辑，理解设计模式在实际开发中的应用；</p><p>3、<strong>实战落地经验</strong>：获取 VR 游戏、虚拟仿真教育项目中框架的适配技巧与性能优化方案；</p><p>4、<strong>问题解决工具库</strong>：覆盖框架使用中 90% 的常见问题（资源加载异常、UI 层级冲突、跨平台兼容等）及解决方案；</p><p>5、<strong>技术背书加成</strong>：掌握主流框架技能，提升求职、晋升竞争力（适配 Unity 开发、VR / 虚拟仿真相关岗位需求）；</p><p>6、<strong>持续更新权益</strong>：专栏将同步框架版本迭代，新增功能解析与实战案例，终身免费查看。</p>]]></description></item><item>    <title><![CDATA[漫格拼车系统：一站式同城拼车平台解决方案]]></title>    <link>https://segmentfault.com/a/1190000047436608</link>    <guid>https://segmentfault.com/a/1190000047436608</guid>    <pubDate>2025-11-28 17:12:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>格拼车系统是一款适配微信公众号的专业拼车平台搭建工具，支持微信小程序部署，以微擎系统交付为核心，提供从前端用户交互到后端管理的全流程功能，助力快速打造合规、高效的同城拼车服务平台。</p><p><strong>一、概述总结</strong><br/>漫格拼车系统聚焦同城拼车场景，涵盖人找车、车找人、货找车、车找货四大核心需求，通过简洁易用的操作界面与完善的功能体系，连接乘客、司机与货主，实现出行与货运资源的高效匹配。系统支持 PHP7.1 及以上版本与 MySQL5.7 及以上环境，源码未加密且提供 1 年免费更新服务，新购用户可获赠 1 年服务套餐，无需额外续费即可享受版本升级权益，降低平台搭建与维护成本。</p><p><strong>二、功能介绍</strong><br/>（一）核心发布模块<br/>支持人找车、车找人、货找车、车找货四种发布类型，覆盖出行与货运两大核心场景。</p><p>发布信息需填写出发地、目的地、出发时间、有效时间等关键信息，车辆相关发布还需补充品牌、型号、座位数等细节，保障信息精准度。</p><p>（二）用户与司机管理<br/>司机需通过入驻审核方可发布车找人、车找货信息，确保服务合规性与安全性。</p><p>用户中心支持查看发布记录、浏览历史、订单信息，可控制是否跳转主插件功能，支持独立插件模式运行。</p><p>（三）线路与筛选功能<br/>提供推荐线路展示，支持线路自动生成与地图导航，精准计算行程公里数与预计用时。</p><p>支持按出发地、目的地、出发时间、有效时间进行多维度筛选，快速匹配需求资源。</p><p>（四）后台管理系统<br/>包含拼车信息管理、用户订单管理、司机入驻管理、幻灯片管理等核心模块，支持信息编辑、导出操作。</p><p>可自定义系统名称、分享标题与描述，设置司机入驻、信息发布及查看价格（默认免费），灵活配置平台规则。</p><p>（五）附加功能<br/>支持信息分享、联系方式查看、行程导航等实用功能，提升用户体验。</p><p>支持插件内部跳转与独立授权链接两种方式，适配不同运营需求。</p><p><strong>三、适用场景与行业价值</strong><br/>适用场景<br/>同城出行：城市内及周边短途通勤、跨城出行拼车需求，如上班族通勤、节假日返乡等。</p><p>同城货运：个人及小微企业的小件货物运输需求，实现货车资源与货运需求的快速对接。</p><p>平台运营：创业者、企业或机构搭建专属拼车平台，服务本地用户或特定群体。</p><p>行业价值<br/>对用户：简化拼车与货运对接流程，通过精准筛选快速找到匹配资源，降低出行与运输成本。</p><p>对司机 / 货主：盘活闲置运力资源，增加额外收入渠道，提高资源利用效率。</p><p>对运营方：无需复杂开发即可快速搭建专业拼车平台，源码开放且支持定制化部署，可根据需求扩展功能，适配微信生态流量红利。</p><p><strong>四、常见问答</strong><br/>漫格拼车系统支持哪些部署环境？</p><p>答：支持 PHP7.1、PHP7.2、PHP7.3、PHP7.4、PHP8.0 版本，需搭配 MySQL5.7 及以上环境，且需开启 mysql_pdo 组件。</p><p>系统是否支持小程序部署？</p><p>答：支持微信小程序部署，如需部署可联系客服处理，同时支持在线打包 Android 和 iOS 版本。</p><p>司机发布信息有什么限制？</p><p>答：司机需完成入驻审核后，才能发布车找人、车找货相关信息，审核机制保障平台服务合规性。</p><p>系统能否实现信息导出功能？</p><p>答：支持后端拼车信息导出与订单导出，方便运营方进行数据统计与管理。</p>]]></description></item><item>    <title><![CDATA[精准查背调神器企业版：权威大数据核验解决]]></title>    <link>https://segmentfault.com/a/1190000047436612</link>    <guid>https://segmentfault.com/a/1190000047436612</guid>    <pubDate>2025-11-28 17:11:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、概述总结</strong><br/>精准查背调神器企业版是涉案精查团队出品的权威大数据核验系统，支持 WEB 版部署且无需配置支付功能，即装即用。系统依托合规正规的数据资源与流程，提供司法文书、失信执行人、学历、房产、婚姻状态等多维度数据核验服务，仅面向企业用户开放。产品采用微擎系统在线交付，源码未加密，兼容 PHP7.2 环境，能帮助企业快速获取准确信息，降低决策风险，提升工作效率与决策质量。</p><p><strong>二、功能介绍</strong><br/>核心核验功能<br/>司法相关核验：涵盖企业被起诉记录、失信信息、行政处罚等，支持全国法院裁判文书检索与全国失信被执行人名单核验。</p><p>身份与资质核验：可核实学历证书真伪、当前婚姻状态，辅助确认个人或相关方的核心资质信息。</p><p>资产信息核验：提供房产登记信息、车辆登记信息核验，助力评估财务与资产状况。</p><p>其他关键信息核验：包括名下企业查询、婚恋风险排查、多头贷款信息查询、社保缴纳记录核验、车辆出险记录核验等。</p><p>产品优势<br/>合规便捷：数据正规、流程合规，安装后即可使用，无需繁杂配置。</p><p>灵活适配：支持微信公众号适用场景，可按需选择具体核验数据类型。</p><p>安全保障：官方正品保障，核验结果有效期 7 天，到期自动删除，用户需妥善保管信息不得私自传播或售卖。</p><p><strong>三、适用场景与行业价值</strong><br/>主要适用场景<br/>企业风控与法律行业：司法文书、失信执行人核验用于风险评估；法律尽职调查中补充关键数据支持。</p><p>人力资源与第三方背调：学历、婚姻状态等信息核验，辅助员工背景调查与资质审核。</p><p>金融与保险行业：银行及金融机构贷前审核时，通过房产、车产核验评估申请人资产状况；保险公司核保环节，借助社保、车辆出险记录优化风险评估与保费计算。</p><p>婚恋与生活服务：婚恋交友中核实对方婚姻状态、学历背景，保障交友安全；租房 / 二手房交易时，核实房东身份与房产产权，规避交易风险。</p><p>其他场景：在线教育行业核验教师资质与学历背景；家政服务行业开展保姆、月嫂背景调查（含无犯罪记录核实）；共享经济与职业社交场景中，完成用户身份与资质核验，搭建可信合作关系。</p><p>行业价值<br/>降低风险：通过多维度数据核验，帮助各行业规避商业合作、人员招聘、交易往来中的潜在风险。</p><p>提升效率：无需线下繁琐调查，在线快速获取核验结果，缩短决策周期。</p><p>保障合规：依托正规数据与流程，确保信息获取与使用符合相关规定，避免法律风险。</p><p>优化体验：为企业决策提供精准数据支持，同时为合作方、用户营造可信环境，提升业务可靠性。</p><p><strong>四、常见问题解答</strong><br/>问：精准查背调神器企业版仅面向企业用户吗？</p><p>答：是的，本系统明确限制为企业使用，不向个人开放。</p><p>问：系统的交付方式是什么？是否需要复杂配置？</p><p>答：采用微擎系统在线交付，源码未加密，兼容 PHP7.2 环境，无需配置支付功能，安装后即可使用，无需繁杂配置。</p><p>问：核验结果的有效期是多久？可以作为正式事实依据吗？</p><p>答：核验结果有效期为 7 天，到期会自动删除；结果仅供参考，不能作为正式事实依据。</p><p>问：系统支持哪些数据类型的核验？部分数据是否暂未开放？</p><p>答：支持司法文书、失信执行人、学历、房产、婚姻状态、名下企业、婚恋风险等多种数据类型核验；部分数据正在开发中，后续将逐步完善。</p><p>问：使用系统过程中，需要遵守哪些规定？</p><p>答：需遵守相关法律法规，不得用于入职背调；妥善保管核验信息，不得传播、售卖给第三方。</p>]]></description></item><item>    <title><![CDATA[越客证书查询系统：高度自定义的全场景证书]]></title>    <link>https://segmentfault.com/a/1190000047436616</link>    <guid>https://segmentfault.com/a/1190000047436616</guid>    <pubDate>2025-11-28 17:10:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、概述总结</strong><br/>越客证书查询系统是一款支持高度自定义的证书管理与查询工具，通过微擎系统交付，适配微信公众号与 PC 多端使用。系统以 “智能防伪、灵活自定义、多端适配” 为核心优势，提供证书字段配置、数据导入导出、扫码验证、个性化页面设计等一站式功能，帮助用户快速搭建专属品牌证书查询平台。</p><p><strong>二、功能介绍</strong><br/>灵活自定义配置<br/>支持自由添加证书字段，如专业、等级、机构、导师、培训时长等，可无限拓展字段类型。</p><p>后台可自定义查询页主题色、标题、LOGO、背景图及页脚说明，适配品牌视觉风格。</p><p>高效数据管理<br/>支持 Excel/CSV 格式批量导入导出证书数据，一键生成专属证书库，操作便捷。</p><p>提供多条件组合查询功能，可按姓名、证书编号、证书类型等字段精准检索证书信息。</p><p>多端适配与防伪验证<br/>移动端查询页采用响应式自适应布局，支持扫码验证，方便用户随时查验。</p><p>查询页与详情页自动适配品牌风格，展示效果专业规范，提升品牌可信度。</p><p>多用户独立管理<br/>每个用户拥有专属证书库与页面配置权限，数据互不干扰，保障信息安全。</p><p>可视化设计界面，无需专业技术即可完成页面个性化设置，降低使用门槛。</p><p><strong>三、适用场景与行业价值</strong><br/>适用场景<br/>教育培训机构：用于颁发课程合格证书、结业证书，方便学员查询验证。</p><p>企业内部：适用于内部认证、防伪培训证管理，规范员工资质档案。</p><p>行业协会 / 学会：可发放专业资格、等级评定证书，提升行业认证公信力。</p><p>品牌方：用于经销商授权书、加盟授权证书管理，明晰授权关系。</p><p>行业价值<br/>提升效率：批量处理证书数据，替代人工查询，节省管理成本。</p><p>强化防伪：通过专属查询平台与扫码验证，杜绝假证乱象，保障证书权威性。</p><p>品牌升级：个性化页面设计融入品牌元素，提升品牌专业形象与用户信任度。</p><p>规范管理：集中存储证书信息，实现资质档案的系统化、可追溯管理。</p><p><strong>四、常见问题问答</strong><br/>系统支持哪些运行环境？</p><p>答：支持 PHP5.5、PHP5.6、PHP7.1 版本，适配微信公众号与 PC 端使用。</p><p>证书数据如何导入和导出？</p><p>答：支持 Excel/CSV 格式批量导入导出证书数据，可一键生成或备份证书库。</p><p>能否设置多个独立的证书管理页面？</p><p>答：可以，系统支持多用户独立管理，每个用户可拥有专属证书库与页面配置，数据互不干扰。</p><p>前台查询页可展示哪些信息？</p><p>答：可自由选择展示字段，包括姓名、证书编号、证书类型、发证日期、到期日期、培训时长、教师、专业等。</p>]]></description></item><item>    <title><![CDATA[读书助手微信小程序：高效识字记词的阅读好]]></title>    <link>https://segmentfault.com/a/1190000047436622</link>    <guid>https://segmentfault.com/a/1190000047436622</guid>    <pubDate>2025-11-28 17:10:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、概述总结</strong><br/>读书助手是一款专为读书场景设计的微信小程序，核心聚焦查字、记词记句需求，助力用户在阅读过程中快速解决生字难题、积累优质词句。小程序基于微擎系统交付，支持 PHP7.1、PHP7.2 环境，提供未加密源码保障官方正品权益。首次购买即赠送 1 年服务套餐，期间可免费更新至最新版本。自上线以来，小程序持续迭代优化，先后新增腾讯识别接口、DeepSeek 搜索与识别相关功能，不断提升使用体验与功能实用性。</p><p><strong>二、功能介绍</strong><br/>核心识字功能<br/>手写识字：支持用户手写输入生字，快速获取准确识别结果，轻松认识新汉字。</p><p>精准识别升级：接入腾讯识别接口，大幅提升识字识别的准确性，减少识别误差。</p><p>拓展学习功能<br/>DeepSeek 智能搜索：可输入文字或直接使用识别结果进行拓展学习，获取丰富知识补充。</p><p>搜索与识别灵活控制：小程序端配备 DeepSeek 搜索与识别展示开关，支持区分功能，满足不同使用场景需求。</p><p>生字管理功能<br/>生字本存储：自动保存用户查询过的生字，形成专属生字库，方便后续复习回顾。</p><p>复习数据统计：清晰展示生字总数、今日新增数量及已复习次数，助力用户掌握学习进度。</p><p>基础查询功能<br/>全面信息展示：查询生字时，同步呈现拼音、释义等核心信息，如 “好” 字标注拼音 “hǎo” 及 “美、貌美” 等本义与引申义。</p><p>安全优化保障：针对小程序安全问题进行专项优化，为用户使用提供安全环境。</p><p><strong>三、适用场景与行业价值</strong><br/>适用场景<br/>个人阅读场景：学生、职场人士、中老年读者等在阅读书籍、文章时，遇到生字可即时查询，不中断阅读节奏。</p><p>学习积累场景：学生群体可通过生字本功能积累课本外生字，提升语文素养；文学爱好者可收集阅读中的好词好句，用于写作积累。</p><p>亲子共读场景：家长陪伴孩子阅读时，借助手写识字功能快速解答孩子的生字疑问，培养孩子阅读兴趣。</p><p>行业价值<br/>教育辅助领域：为语文学习、课外阅读提供便捷工具，弥补传统字典查询效率低的不足，助力高效学习。</p><p>小程序开发领域：作为成熟的微擎应用，为开发者提供可直接部署的源码方案，降低读书类小程序的开发成本与周期。</p><p>知识服务领域：通过 “识别 + 搜索 + 存储” 的闭环功能，满足用户碎片化知识积累需求，提升知识获取与管理效率。</p><p><strong>四、问答环节</strong><br/>问：读书助手小程序支持哪些运行环境？</p><p>答：支持 PHP7.1、PHP7.2 运行环境，基于微擎系统交付。</p><p>问：小程序的识字识别功能准确性如何？</p><p>答：小程序已接入腾讯识别接口，显著提升识别准确性，同时搭配 DeepSeek 智能搜索辅助，进一步保障使用效果。</p><p>问：生字本功能可以记录哪些信息？</p><p>答：生字本会自动保存用户查询过的生字，同时展示生字总数、今日新增数量及已复习次数，方便用户管理与复习。</p><p>问：小程序是否支持搜索结果拓展学习？</p><p>答：支持，内置 DeepSeek 智能搜索功能，可输入文字或直接使用识别结果进行拓展学习，获取更多相关知识。</p>]]></description></item><item>    <title><![CDATA[漫格同城信息程序：一站式打造本地微信同城]]></title>    <link>https://segmentfault.com/a/1190000047436626</link>    <guid>https://segmentfault.com/a/1190000047436626</guid>    <pubDate>2025-11-28 17:09:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>漫格同城信息程序是一款专为本地场景打造的微信同城分类信息解决方案，支持微信公众号和 H5 双模式运行，可搭配微信小程序使用，通过红包朋友圈传播等功能，助力快速搭建高效、便捷的本地信息交互平台。</p><p>核心功能亮点<br/>信息发布灵活可控，支持免费发布与付费发布两种模式，不同分类可设置差异化价格及信息有效期。</p><p>个性化配置便捷，支持后台一键设置主题色调，前端实时生效，还能自由创建模型以适配不同行业信息发布需求。</p><p>多端适配与插件支持，兼容微信公众号、H5，可拓展 Android 和 iOS 打包服务；集成腾讯短信验证码、阿里云 OSS 存储等实用插件。</p><p>运营管理高效，具备首页幻灯片管理、导航管理、订单统计、数据导出等功能，同时支持信息审核与查看管理，保障内容合规。</p><p>曝光提升功能，提供信息置顶服务，置顶后可提高 6-8 倍曝光量，还支持分享到微信朋友圈获取刷新靠前权益。</p><p><strong>适用场景与行业价值</strong><br/>适用场景<br/>本地生活服务平台：涵盖家政保洁、水电维修、补课班等服务信息发布与查询。</p><p>房产租赁与求职招聘：满足房屋出租、职位招聘等信息的精准对接需求。</p><p>便民信息交互：支持商品出租、失物认领、同城电话等各类便民信息的发布与传播。</p><p>行业价值<br/>对创业者：低成本搭建本地信息平台，无需复杂技术开发，依托微信生态实现快速获客与传播。</p><p>对本地商家 / 服务提供者：提供高效的信息曝光渠道，通过精准定位本地用户，提升服务对接效率。</p><p>对普通用户：打造便捷的本地信息获取入口，可快速查找所需服务、职位、房源等信息，解决生活需求。</p><p><strong>常见问题问答</strong><br/>程序的安装环境有哪些要求？<br/>答：需满足 PHP8.1 及以上版本、MySQL5.7 及以上版本，且需开启 mysql_pdo 组件。</p><p>程序支持哪些用户信息获取权限？<br/>答：可获取用户微信昵称、头像、性别、地区等用户信息，以及位置信息和相册权限。</p><p>信息发布后如何提高曝光量？<br/>答：可选择信息置顶服务，支持 7 天、30 天、60 天等不同置顶周期，也可将信息分享到微信朋友圈获取刷新靠前权益。</p><p>平台是否有交易安全提示？<br/>答：官方提示请勿线下交易，90% 的欺诈、纠纷、资金盗取均由线下交易导致，保障用户资金安全。</p>]]></description></item><item>    <title><![CDATA[有哪些公共DNS的地址？ 有点小烦扰 ]]></title>    <link>https://segmentfault.com/a/1190000047436646</link>    <guid>https://segmentfault.com/a/1190000047436646</guid>    <pubDate>2025-11-28 17:08:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为影响网络体验的“隐形之手”，公共DNS不仅能解决运营商DNS卡顿、劫持等问题，还能带来更快的解析速度和更强的安全防护。但面对众多公共DNS服务，普通用户往往困惑：公共DNS有哪些？哪些适合追剧、哪些适合游戏？如何选择最适合自己的那一款？</p><h3><strong>一、公共DNS是什么？</strong></h3><p>公共DNS是由第三方机构（免费提供的域名解析服务，区别于网络运营商默认分配的DNS服务器。它的核心优势在于速度更快、安全更强、无广告劫持。据《2025年中国DNS服务质量报告》显示，使用优质公共DNS的用户，网页加载速度平均提升35%，恶意网站拦截率达98.2%。</p><h3><strong>二、主流公共DNS推荐</strong></h3><p>1、114DNS：全能型“秒开王”，家庭用户首选</p><p>作为国内运营时间最长的公共DNS之一，114DNS（主地址：114.114.114.114）以响应速度快著称。2025年实测数据显示，其国内平均解析延迟仅10-15ms，短视频、直播平台加载速度提升明显，被用户称为“刷剧福音”。</p><p>它最大的特色是三版本细分服务：普通版专注速度优化；安全版（114.114.114.119）内置恶意域名库，日均拦截诈骗网站超500万个，适合金融、电商用户；少儿版（114.114.114.110）可过滤色情、暴力内容，家长设置后能有效保护儿童上网安全。不过部分用户反馈，普通版可能会推送少量广告，对广告敏感者建议选择安全版。</p><p>2、阿里DNS：本土优化强者，电商访问更流畅</p><p>阿里DNS（主地址：223.5.5.5）依托阿里云全球节点布局，在国内网站解析上表现突出。实测显示，访问淘宝、天猫、B站等阿里系及国内主流平台时，解析延迟比运营商DNS低20%-30%，高峰期也能保持稳定。</p><p>该服务支持DoH/DoT加密协议，能有效防范DNS劫持和数据泄露，同时提供IPv6地址（2400:3200::1），适配未来网络升级需求。但缺点是海外网站解析能力较弱，若经常观看Netflix、YouTube等海外内容，可能会出现加载卡顿。</p><p>3、腾讯DNS：游戏专项优化，电竞玩家必备</p><p>腾讯DNS（主地址：119.29.29.29）针对游戏场景做了深度优化，是《王者荣耀》《原神》等热门游戏官方推荐的DNS服务。实测数据显示，使用该DNS后，游戏服务器连接延迟平均降低20%，团战时技能释放响应更及时，有效减少“卡技能”“掉线”等问题。</p><p>其节点覆盖以国内为主，北方地区用户反馈稳定性更佳，南方部分地区可能出现偶尔抽风。此外，腾讯DNS还与微信、QQ等社交软件联动，能加速小程序加载速度，社交办公用户使用体验较好。</p><p>4、其他特色公共DNS：满足细分需求</p><p>除了上述三款主流服务，还有一些针对性更强的公共DNS可供选择。例如360安全DNS（101.226.4.6）以安全防护见长，能拦截钓鱼网站和恶意软件下载；GoogleDNS（8.8.8.8）海外解析能力强，但国内访问延迟较高，适合有频繁海外业务需求的用户。</p><h3><strong>三、公共DNS要怎么选择？</strong></h3><p>1、明确核心需求：追剧刷视频选114DNS，电商购物选阿里DNS，玩游戏选腾讯DNS，海外访问选GoogleDNS。<br/>2、测试本地速度：通过“DNS测速工具”（如DNSBenchmark）检测不同公共DNS在本地的响应延迟，选择延迟最低的服务。<br/>3、简单配置生效：手机在“无线局域网”详情页手动输入DNS地址；电脑在“网络适配器属性”中修改DNS服务器；路由器后台配置可实现全屋设备同步使用。</p><p>公共DNS虽小，却直接影响网络访问的速度、安全与稳定性。114DNS的全能、阿里DNS的本土优化、腾讯DNS的游戏专项能力，分别对应不同用户的核心需求。在选择时，无需盲目追求“最好”，而是结合自身使用场景——是追剧、购物还是打游戏，再配合本地速度测试，就能找到最适合的公共DNS。更换后，你会发现网络体验的提升远比想象中明显，让每一次点击都更流畅、更安心。</p><p>参考资料：<a href="https://link.segmentfault.com/?enc=zG16KvyKtdu4S%2BLz935B5g%3D%3D.3juPQOuXNYZMJxZea9t%2BDc%2Bp3aixjejTodUbCYUqcQaC9dwfZ7h4dbv3ZCkG5o%2Be" rel="nofollow" target="_blank">https://www.51dns.com/dns/public</a></p>]]></description></item>  </channel></rss>