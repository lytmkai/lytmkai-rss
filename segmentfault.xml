<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[《Render Graph与光追API融合应用指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597395</link>    <guid>https://segmentfault.com/a/1190000047597395</guid>    <pubDate>2026-02-06 18:18:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>URP以轻量化、跨平台为核心诉求，在资源调度上追求极致精简，适配移动端、主机等多终端的硬件限制；而HDRP则聚焦高清渲染，在光照计算、材质表现上堆砌复杂逻辑，专为高端PC与次世代主机打造。这种定位差异导致两条管线在核心架构、资源管理、效果实现上形成难以逾越的壁垒，开发者往往需要为不同管线单独构建内容、适配逻辑，既增加了开发成本，也让跨平台体验的一致性大打折扣。共享Render Graph与统一光线追踪API的出现，并非简单的功能叠加或参数调优，而是对渲染逻辑的深层重构，其核心在于构建一套脱离管线专属限制的通用渲染语言。这种语言让HDRP的高品质光照计算不再依赖专属的管线架构，而是能通过Render Graph的资源适配与调度优化，以符合URP性能基线的方式落地；同时，URP的跨平台灵活性也不再局限于基础渲染能力，而是能通过统一光线追踪API，承载HDRP级别的复杂场景光照交互。在长期的技术探索中发现，管线间的差距本质上是资源管理逻辑与效果计算范式的割裂—URP为追求效率简化了资源依赖解析，HDRP为实现品质强化了专属计算模块，而共享Render Graph通过对渲染流程的节点化抽象，将资源分配、Pass调度、依赖解析等核心逻辑抽离为独立层，让两条管线能基于同一套底层规则管理资源；统一光线追踪API则打破了光照计算的管线专属限制，让实时光线追踪从HDRP的“高端配置”转变为可根据硬件能力动态适配的“通用功能”。这种转变要求开发者跳出“为URP添加高清功能”或“为HDRP做性能裁剪”的传统思维，转而从渲染本质出发，让两条管线基于同一套核心逻辑，按需组合渲染模块，实现从移动端到高端PC的无缝能力伸缩，最终达成“性能与品质并行不悖”的渲染目标。</p><p>共享Render Graph的核心价值，在于构建了一套标准化的渲染资源语义映射体系，让URP与HDRP能精准理解彼此的资源描述规则，从而实现跨管线的资源复用与流程互通，彻底改变了传统管线中资源壁垒林立的局面。在传统开发模式中，URP为适配移动端硬件，采用紧凑的纹理压缩格式与精简的缓冲布局，而HDRP为追求高清表现，使用高精度纹理与复杂的缓冲结构，这种差异导致同一材质资源在两条管线中需要重复构建适配逻辑，不仅增加了开发工作量，还容易出现资源不一致的问题。共享Render Graph通过抽象资源的核心属性描述与使用场景，将资源的具体实现细节与上层渲染逻辑解耦—无论管线对资源的精度要求、压缩格式有何差异，都能通过统一的资源接口进行调用，Render Graph则在底层自动完成适配转换。例如，在处理复杂场景的多Pass渲染时，HDRP为实现全局光照计算生成的高精度光照贴图，可通过Render Graph的资源适配层，自动转换为URP可高效采样的压缩格式，无需额外添加格式转换Pass，既减少了性能开销，又保证了光照效果的一致性；同样，URP针对移动端优化的纹理流式加载逻辑，也能被HDRP复用，在高清场景中根据硬件内存情况动态加载纹理资源，有效降低内存占用压力。更重要的是，Render Graph的节点化架构让渲染流程具备了模块化重组能力，HDRP中包含的环境光遮蔽、屏幕空间反射、体积雾等复杂后处理链路，可拆分为独立的功能节点，URP可根据自身性能预算，选择性启用核心节点，省略高精度计算步骤，无需重新设计整套后处理管线。这种模块化复用不仅大幅降低了两条管线的功能差距，还提升了开发效率—开发者只需维护一套核心渲染流程节点，即可通过Render Graph的适配逻辑，自动适配两条管线的性能与品质需求，让URP的渲染效果向HDRP靠拢，同时保持自身轻量化的核心优势。在实际技术探索中还发现，Render Graph的资源依赖解析能力，能有效解决跨管线渲染中的资源冲突问题，例如当两条管线同时调用同一材质资源时，Render Graph会根据当前管线的渲染上下文，自动分配对应的资源实例，避免出现资源竞争或格式不兼容的情况，进一步强化了管线间的协同能力。</p><p>统一光线追踪API的关键突破，在于实现了光照计算的范式归一，让URP与HDRP能基于同一套光线行为描述逻辑，达成光照效果的一致性与性能的差异化适配，彻底改变了此前两条管线光照表现泾渭分明的局面。在此之前，HDRP的光线追踪依赖专属的光照计算管线，支持复杂的光线反弹、材质交互与全局光照采样，能呈现出逼真的阴影、反射与折射效果，但计算开销巨大，仅能在高端硬件上运行；而URP受限于性能预算，无法承载完整的光线追踪计算，仅能通过屏幕空间近似算法模拟简单光照效果，导致两条管线的光照表现存在本质差距，同一场景在不同管线中呈现出截然不同的视觉质感。统一光线追踪API通过抽象光线的发射、相交、着色等核心行为，构建了一套与管线无关的光照计算模型，让光线追踪的核心逻辑脱离管线专属实现，成为一套可灵活适配的通用能力。在实际应用中，这套API会根据管线的性能目标与硬件能力，动态调整计算精度与采样策略：在HDRP中，光线可支持多轮反弹与复杂材质采样，充分发挥高端PC与次世代主机的计算潜能，呈现出电影级的光照质感；在URP中，则通过一系列智能化优化—如光线采样策略动态调整，优先采样对视觉影响显著的区域；反弹次数自适应控制，根据场景复杂度与硬件性能动态调整反弹轮次；加速结构简化，采用更紧凑的空间划分算法—在保证光照效果合理性的前提下，将计算开销控制在移动端与中端PC可承受的范围。这种适配并非简单的参数削减，而是基于硬件能力的智能决策，例如在移动设备上，API会自动将全局光线追踪转为局部关键区域的光线查询，结合屏幕空间信息补全光照细节，让URP的光照表现既符合性能要求，又能无限接近HDRP的视觉质感；在中端PC上，则可启用有限次数的光线反弹，平衡效果与性能。此外，统一API还实现了光照数据的跨管线互通，HDRP中烘焙的光线追踪加速结构，可通过API的适配层转换为URP可高效使用的简化版本，减少重复计算开销，让两条管线在光照计算上实现能力同源，进一步缩小了视觉差距。</p><p>场景描述体系的统一，是缩小URP与HDRP差距的重要支撑，共享Render Graph与统一光线追踪API共同构建了一套可跨管线解析的场景语义规范，让复杂场景的描述不再依赖特定管线的专属逻辑，实现了场景资源的一次创建、多管线复用。在传统开发流程中，HDRP的复杂场景通常包含大量高精度几何信息、分层材质属性与全局光照参数，这些信息往往针对HDRP的渲染架构进行优化，无法直接被URP解析，导致同一场景在两条管线中需要重新配置—URP需简化几何模型、削减材质层数、调整光照参数，不仅耗时耗力，还容易导致场景效果失真；而共享Render Graph通过对场景元素的结构化描述，将几何数据、材质属性、光照信息等拆分为独立的语义单元，每条管线可根据自身能力解析对应的语义层级，无需对场景资源进行破坏性修改。例如，HDRP中使用的多层材质，包含基础颜色、粗糙度、金属度、次表面散射等多个属性层，在URP中，Render Graph会通过语义适配，自动提取基础颜色、粗糙度等核心属性，忽略次表面散射等高精度细节，同时保留材质的核心视觉特征，让材质在URP中既符合性能要求，又能保持与HDRP一致的视觉风格；而URP中的简化几何模型，在HDRP中可通过API自动补充细节层次，如添加高模细节贴图、启用几何细分，满足高清渲染需求。统一光线追踪API则进一步强化了场景的光照交互一致性，无论是URP的轻量化场景还是HDRP的高精度场景，光线与物体的相交判定、材质反射计算都遵循同一套语义规则，确保在不同管线中，光照对场景氛围的影响保持一致—例如同一光源照射下，物体的阴影形状、反射强度、颜色衰减在两条管线中呈现出高度统一的效果，避免了跨管线体验的割裂感。这种场景语义的统一，让开发者无需为两条管线单独构建场景资源，只需维护一套核心场景描述，Render Graph与光线追踪API会自动完成适配转换，大幅降低了跨管线开发的复杂度；同时，场景资源的复用也让URP的场景表现力得到显著提升，原本只能在HDRP中呈现的复杂场景细节，如今可通过语义适配在URP中高效呈现，进一步缩小了两条管线的视觉差距。</p><p>着色器生态的协同演进，是弥合URP与HDRP差距的关键环节，共享Render Graph与统一光线追踪API为两条管线提供了可互通的着色器开发框架，让高品质着色逻辑能在两条管线中高效复用，彻底改变了此前着色器开发“管线专属”的局面。在此之前，HDRP的着色器支持复杂的次表面散射、屏幕空间反射、多层材质混合等高级效果，这些效果的实现依赖HDRP专属的光照计算管线与资源调度逻辑；而URP的着色器受限于性能预算，往往只能实现基础的PBR光照计算，导致同一材质在两条管线中视觉差异显著—HDRP中材质表现细腻、光影过渡自然，而URP中材质效果单薄、细节缺失，严重影响了跨管线体验的一致性。共享Render Graph通过模块化着色器设计，将着色逻辑拆分为独立的功能单元，每个单元可根据管线的性能与画质需求，动态调整计算复杂度，实现“一套逻辑、多端适配”。例如，HDRP中使用的PBR着色逻辑，可拆分为基础光照计算、高级材质交互、全局光照融合等模块：在HDRP中，可启用全部模块，实现复杂的材质表现；在URP中，则可选择性启用基础光照计算模块，同时通过统一光线追踪API补充关键光照细节（如高精度反射、软阴影），让URP的PBR表现兼具性能与质感，与HDRP的视觉差异大幅缩小；而HDRP也可复用URP中针对移动端优化的纹理采样模块，通过更高效的采样算法提升高清场景的纹理加载效率，减少性能开销。这种模块化设计不仅实现了着色逻辑的跨管线复用，更让着色器的扩展能力大幅提升—新的着色效果只需开发一次，即可通过Render Graph的适配层自动适配两条管线的渲染架构，无需针对每条管线重复开发。在实际技术实践中发现，这种着色器生态的协同，让URP的材质表现力得到了质的飞跃：原本只能在HDRP中实现的复杂材质交互，如布料的漫反射衰减、金属的镜面反射变化，如今可通过统一API与模块化着色逻辑，在URP中以适配性能的方式呈现；同时，HDRP的着色器也因复用了URP的优化模块，在保持高品质的同时，资源占用与计算开销显著降低，实现了“品质不打折、性能更出色”的目标。</p><p>技术融合的深层价值，在于构建了渲染管线的弹性演进体系，让URP与HDRP不再是相互割裂、各自为战的发展路线，而是基于同一技术底座的差异化表达，实现了两条管线的双向赋能与协同升级。在这套体系下，两条管线的核心能力不再局限于初始定位，而是能随着底层技术的迭代同步升级：URP能持续吸收HDRP的高品质渲染技术，通过Render Graph的资源优化与统一API的性能适配，将其转化为自身的轻量化实现，不断提升跨平台场景的视觉表现力；而HDRP也能借鉴URP的跨平台优化经验，将移动端的高效资源调度、轻量化计算逻辑迁移过来，提升高清场景的资源利用效率与硬件适配范围。</p>]]></description></item><item>    <title><![CDATA[《Android瘦LTO与Swift集成层启动优化实战指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047597399</link>    <guid>https://segmentfault.com/a/1190000047597399</guid>    <pubDate>2026-02-06 18:17:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Android的瘦LTO构建绝非传统编译优化的简单升级，而是通过对符号依赖的精准画像与模块关联的动态重构，在保留代码逻辑完整性的前提下，实现编译产物的结构化精简—它不再对全量代码进行无差别优化，而是聚焦启动阶段的核心执行路径，筛选出必须即时加载的关键符号与依赖单元，剥离非必要的冗余代码与关联引用，让应用启动时的代码加载体积与解析耗时实现双重压缩。而Swift重写Apple集成层的核心价值，在于用原生语言的语义特性替代跨语言适配的中间桥接链路，让集成层与Apple系统底层API形成直接的能力对接，消除启动过程中因语言转换、接口适配带来的延迟损耗。这两项技术的联动优化，并非单端独立的性能修补，而是跨平台架构下编译逻辑与集成层设计的深度协同—Android端通过瘦LTO优化启动时的代码加载效率，减少CPU在初始化阶段的计算压力；Apple端借助Swift的运行时优势压缩集成层的初始化链路，降低内存分配与系统调用的延迟，双端形成互补的优化闭环，从编译产物到运行时执行的全流程破解启动性能与跨平台兼容性的核心矛盾。这种优化思路跳出了“单点调优”的传统框架，聚焦跨端启动的本质痛点，通过编译层与集成层的双向革新，让启动性能的提升具备可复制的方法论与规模化落地的可能，为复杂跨平台应用的性能升级提供了全新的技术路径。</p><p>瘦LTO构建的核心竞争力，在于其对编译优化的精准化与高效化革新，它摒弃了全量LTO模式下资源密集型的全局优化逻辑，转而采用分层处理与关键路径聚焦的优化策略，在保证启动性能提升的同时，规避了全量优化带来的编译周期延长问题。在实际优化实践中，瘦LTO的落地需要先完成启动链路的全景解构—通过对应用启动流程的逐环节分析，明确初始化阶段必须加载的核心模块、服务依赖与调用关系，建立启动关键路径的可视化图谱。在此基础上，针对性配置瘦LTO的优化粒度：对于启动时即时初始化的核心服务，如基础配置加载、权限校验、核心功能初始化等模块，进行深度优化处理，包括合并重复调用逻辑、消除无效依赖引用、优化函数执行链路，让代码执行更紧凑高效；对于启动后才按需加载的功能模块，如非核心业务组件、设置页面、辅助工具等，则保持基础编译状态，仅进行必要的符号精简，避免过度优化带来的资源消耗。这种差异化优化策略，既确保了启动关键路径的加载效率，又控制了整体编译开销。在复杂应用场景中，瘦LTO还能与编译缓存机制形成高效协同—通过缓存优化后的中间产物，在后续迭代构建中仅对变更模块进行增量优化，大幅缩短编译周期，同时确保每次构建的优化效果一致性，让启动性能的提升具备稳定可复现的特性。这种精准化的编译优化思路，打破了“优化效果与编译效率不可兼得”的固有认知，实现了编译产物精简、加载效率提升、编译周期可控的三重增益，成为Android端启动性能优化的核心支撑。</p><p>Swift重写Apple集成层的优化逻辑，本质是通过语言原生特性与系统生态的深度耦合，重构跨平台能力的适配链路，彻底替代传统依赖中间桥接层的实现模式，从根源上消除跨语言适配带来的启动损耗。传统跨平台应用的Apple集成层，往往为了兼容多语言调用逻辑，引入大量的接口转换代码、数据格式适配模块与中间调度层，这些冗余链路在启动阶段会产生显著的性能开销—数据在不同语言类型间的转换消耗内存与CPU资源，中间层的调度延迟拉长了初始化周期，同时增加了系统调用的不确定性。Swift作为Apple生态的原生语言，具备与系统底层API的天然适配优势，能够直接调用核心系统能力，省去中间转换环节，让集成层的初始化逻辑更贴合系统的运行时调度机制，实现更高效的能力衔接。实践过程中，重写工作需聚焦两个核心维度：一是集成层的语义对齐，在保持跨平台核心能力一致性的前提下，用Swift的原生语法重构适配逻辑，最大化利用语言的内存管理特性—例如通过值类型优化减少启动时的内存分配与释放操作，避免引用计数带来的额外开销；利用函数派发优化提升调用效率，让核心接口的响应速度更快捷。二是初始化流程的拆分与延迟加载，将集成层的功能模块按启动优先级进行划分，仅保留核心能力的即时初始化，如基础配置适配、系统权限对接等必须在启动阶段完成的逻辑，而将非核心的适配功能，如统计上报、第三方服务对接等，通过懒加载机制延迟到启动完成后执行，进一步压缩启动耗时。这种原生适配的思路，让集成层从启动流程中的“阻滞点”转变为“助推器”，在保证跨平台兼容性的同时，实现了启动性能的质的飞跃。</p><p>Android瘦LTO构建与Swift重写Apple集成层的协同优化，核心在于构建覆盖跨端启动全流程的性能优化闭环，让双端的优化策略形成互补效应，而非孤立的单端升级。Android端通过瘦LTO构建，削减了启动时的代码加载体积与解析耗时，减少了CPU在初始化阶段的计算压力，让核心服务能够更快完成启动准备；Apple端借助Swift重写的集成层，压缩了跨语言适配的中间链路，优化了内存分配效率与系统调用延迟，让集成层的初始化更高效。这种双端协同并非简单的功能叠加，而是基于跨平台应用启动共性逻辑的深度适配—无论是Android的代码加载流程，还是Apple的集成层初始化链路，本质上都是对启动资源的调度与利用，两项技术分别从编译端与运行端切入，形成覆盖“编译产物优化-代码加载加速-集成层初始化精简-系统能力对接高效”的全流程优化体系。实践中，协同优化的落地需要先统一双端的启动性能优化目标，明确核心指标的基准线，例如启动完成时间、初始化阶段的CPU占用、内存峰值等，再根据双端的技术特性制定差异化的优化策略：Android端侧重通过瘦LTO实现编译产物的精简化，缩短代码加载与解析路径，同时优化启动时的资源调度优先级；Apple端聚焦通过Swift的原生优势压缩集成层的初始化链路，减少中间环节的性能损耗，提升系统API的调用效率。通过这种协同设计，跨平台应用能够在双端同时获得启动性能的跃升，避免单端优化导致的用户体验失衡，让不同设备上的启动流程都能保持流畅高效，真正实现跨端启动体验的一致性与高性能。</p><p>启动性能的深度优化，离不开对技术细节的精准把控与场景化的动态适配，瘦LTO构建与Swift重写的落地过程，并非一成不变的标准化流程，而是需要根据应用的实际场景与架构特点进行灵活调整。对于瘦LTO构建而言，优化粒度的选择是关键—过粗的优化会导致启动关键路径的优化不充分，无法达到预期的性能提升效果；过细的优化则可能引入不必要的编译开销，延长构建周期，甚至影响代码的稳定性。因此，在实际操作中，需要借助启动链路分析工具，精准定位每个模块在启动阶段的加载耗时、依赖关系与资源占用情况，建立模块级别的性能画像，再针对性配置优化范围：对启动时首先加载的核心框架，如基础库、路由管理、核心服务等，进行最大程度的优化，合并重复符号，消除循环依赖，优化函数执行逻辑；对后续按需加载的功能模块，如非核心业务组件、多媒体处理、扩展功能等，则采用轻量级优化策略，仅保留必要的符号与依赖，避免过度优化带来的资源消耗。在Swift重写集成层的过程中，集成层的拆分逻辑同样需要贴合应用的启动流程，将必须在启动阶段完成的适配逻辑，如基础配置同步、系统权限申请、核心能力对接等，与可延迟的功能解耦，通过懒加载机制将非必要的适配逻辑延迟到启动完成后执行。同时，需充分利用Swift的编译优化特性，如模块间的接口精简、无用代码自动剔除、编译期常量折叠等，让集成层的产物体积更小巧，加载更快速。这种场景化的精准优化，避免了“一刀切”的优化模式带来的局限性，让每项技术的优势都能在关键场景中充分发挥，实现启动性能的最大化提升，体现了技术优化从“广谱适配”到“精准赋能”的进阶思维。</p><p>瘦LTO构建与Swift重写Apple集成层的优化实践，其长远价值远不止于启动性能的即时提升，更在于为跨平台应用构建了可扩展、可迭代的性能优化体系与技术底座。瘦LTO带来的编译链路优化思路，不仅适用于启动性能的提升，还能延伸到应用运行时的内存占用控制、CPU效率优化与功耗降低—通过持续优化编译产物的结构，让代码执行更高效，资源利用更合理，为应用全生命周期的性能表现奠定坚实基础。而Swift重写的集成层，凭借语言的原生优势与系统兼容性，大幅降低了后续功能迭代的适配成本与维护难度——Swift与Apple系统的深度耦合，让集成层能够快速响应系统版本的更新与API的迭代，无需频繁进行跨语言适配调整；同时，原生代码的可读性与可维护性更强，减少了后续迭代中的技术债务。</p>]]></description></item><item>    <title><![CDATA[百度一见×赣南师大：多模态视觉扎根赣南，共筑产教融合新标杆 百度一见 ]]></title>    <link>https://segmentfault.com/a/1190000047597441</link>    <guid>https://segmentfault.com/a/1190000047597441</guid>    <pubDate>2026-02-06 18:16:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，赣南师范大学代表团莅临百度，双方正式签署校企合作战略协议。这不仅是一场强强联手的签约，更是前沿AI技术与深厚学术积淀的一次“握手”。当“AI for Science”遇上“产教融合”，百度与赣南师大正联手开启智能时代复合型人才培养的新篇章。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597443" alt="图片" title="图片"/><br/>在座谈环节，双方达成高度共识：在智能时代，AI素养已不再是人才的“加分项”，而是“必选项”，产教融合势在必行。百度一见产品部总经理朱名发详细分享了一见在多模态大模型领域的战略布局，以及在能源、制造、连锁、运输等行业的产业实践。“要把最前沿的技术，转化为课堂上的生产力。” 在热烈的氛围中，赣南师范大学党委常委、副校长罗序中与百度代表双方签署协议。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597444" alt="图片" title="图片" loading="lazy"/><br/>随后，校方代表团走进百度展厅，近距离感受百度文心大模型赋能千行百业的实战场景。从实验室的算法到产业实践里的深度应用，双方对“AI+教育”的未来达成了高度共识。朱名发总经理强调：“大模型时代，具备AI素养、能熟练运用AI工具提升效率的复合型人才，已成为企业的首选。我们希望通过合作，让学生在校期间就掌握‘AI生产力’，赋能在就业市场具备核心竞争优势。”<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597445" alt="图片" title="图片" loading="lazy"/><br/>罗序中副校长对此深表认可。他表示，作为江西省“双一流”建设高校，赣南师大拥有扎实的AI学科基础，百度一见在视觉管理领域的深厚积淀，将为学校科研创新注入强劲动力。双方将通过产教融合，加速成果转化，联合培养懂产业、精技术的实战型人才。拒绝纸上谈兵，直击地方痛点。 依托国家脐橙工程技术研究中心等国家级平台，双方明确将“赣州脐橙智能化种植”与“赣州电子制造”作为首批科研攻关方向。通过百度一见的多模态专业视觉技术赋能，双方将合力打造具有全国影响力的应用标杆，真正将产教融合的实践“写”在赣南大地的田间地头与工厂车间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597446" alt="图片" title="图片" loading="lazy"/><br/>签约只是起点，赋能才是目标。未来，百度一见将持续以技术创新为核心，深度融入赣南师大的教学与科研土壤。从实验室的创新火花，到产业界的落地成果，双方将共同探索AI赋能实体经济的新路径。当“AI新范式”扎根老区沃土，一场关于人才、科研与产业的化学反应，正在发生！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597447" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[精灵潜入C++,莲花咒语显神奇 李兴球 ]]></title>    <link>https://segmentfault.com/a/1190000047597482</link>    <guid>https://segmentfault.com/a/1190000047597482</guid>    <pubDate>2026-02-06 18:15:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>看这一行长长的代码：</p><pre><code class="C++">while(1)r.bgcolor("black").pensize(5).speed(0).color(r.heading()).circle(100,90).left(90).circle(100,90).left(90).right(20);</code></pre><p>主要就是这一行代码，画了一幅美妙的莲花图案。下面是完整的，C++精灵库画莲花的代码：</p><pre><code class="C++">#include "sprites.h"  //包含C++精灵库 
Sprite r;      //建立角色叫r
 
int main(){        //主功能块 
 
  while(1)r.bgcolor("black").pensize(5)
          .speed(0).color(r.heading())
          .circle(100,90).left(90)
          .circle(100,90).left(90).right(20);
 
   return 0;    //返回0
}</code></pre><h3>神仙对话泄天机</h3><p>哪吒（手持乾坤圈）：“俺是哪吒三太子，刚刚听闻有位小魔法师用几行代码画出了一朵美轮美奂的莲花。那莲花的花瓣颜色还会随他的笔转向而不断变换，真是神奇！你可知道他是如何做到的？”</p><p>太上老君（手持拂尘）：“此乃C++精灵库的妙用也。那小魔法师创建了一个名为r的角色，就像我身边的童子一样，然后在main函数里用了一个永不停歇的while循环，让r不停地舞动乾坤。”</p><p>哪吒：“你这葫芦里卖的什么药？快讲讲r是怎么画莲花的？”</p><p>太上老君：“那小魔法师在循环里让r做了好多动作。他先把r的背景色设为黑色，就像天庭的黑夜一样深邃。接着把笔画粗细调粗到5个单位，笔速设为0，意味着笔走如飞，一点都不拖沓。”</p><p>哪吒：“嘿嘿，俺这乾坤圈也重达千斤，画笔画粗些倒也般配。那他还做了什么？”</p><p>太上老君：“他把画笔的颜色设置为r.heading()，也就是根据r当前的方向来取颜色。这就好比r在不停地旋转，每转一个角度，颜色就变一变，仿佛r的心情在变，颜色也跟着变。”</p><p>哪吒：“这颜色还会变？那r是怎么转的呢？”</p><p>太上老君：“r画了两个半径100的圆弧，每次转90度。具体来说，先画了一个90度的圆弧，然后左转90度，再画另一个90度的圆弧，又左转90度，然后右转20度。如此循环往复，就像你在打旋子一样，一圈一圈地转。”</p><p>哪吒：“这不是和我用乾坤圈画圈一样吗？那最后r会不会停下来？”</p><p>太上老君：“那小魔法师在循环里没有停下来的意思，while(1)就是无限循环。”</p><p>哪吒：“原来如此！这C++精灵库真像一位多才多艺的画匠，寥寥数笔就能画出五彩斑斓的莲花。而且它的命令和Python的turtle库差不多，对于喜欢Python的孩子来说，学这个C++库就像换了个平台继续玩耍，真是一举两得！”</p><p>太上老君：“哈哈，哪吒你说得对！C++精灵库让孩子们在学习编程时，既可以延续熟悉的图形命令，又能领略C++的强大功能，确实是非常值得学习的库。”</p><p>哪吒：“俺这就回去告诉师傅，让他也教教我C++精灵库，说不定俺也能画出更漂亮的莲花呢！”</p><p>太上老君：“好啊，希望你早日成为C++小能手，画出属于你自己的绚丽莲花！”<br/><img width="289" height="293" referrerpolicy="no-referrer" src="/img/bVdnSri" alt="" title=""/></p><h3>代码解析学咒语</h3><p>下面的逐行解释了main函数中while循环内的代码，并说明其作用：</p><p>代码行                  作用<br/>r.bgcolor("black")    设置画笔背景色为黑色。<br/>.pensize(5)            设置画笔粗细为5个像素单位。<br/>.speed(0)            设置画笔移动速度为0（最快速度）。<br/>.color(r.heading())    根据画笔当前方向heading()获取颜色值，并设置画笔颜色。方向值会被转换为色相，从而实现颜色随方向变化。<br/>.circle(100, 90)    以当前位置为圆心，半径100逆时针绘制一个90度的圆弧。<br/>.left(90)            画笔向左旋转90度。<br/>.circle(100, 90)    再次向左绘制一个90度的圆弧。<br/>.left(90)            画笔再次向左旋转90度。<br/>.right(20)            画笔向右旋转20度（调整方向，使下次循环继续）。<br/>上述代码通过链式调用的方式组合了一系列绘图命令，在无限循环中不断重复执行。每次循环中，画笔都会以黑色背景、粗线条、动态颜色绘制两个圆弧，然后旋转方向，如此往复，形成了莲花形状的图案。</p><h3>始作俑者详剖析</h3><p>C++精灵库（Sprite库）是一个基于SDL2库的少儿C++编程教学库，提供了类似Python turtle库的简洁命令，通过绘制图形和制作动画或小游戏创意C++作品来让少年儿童学习C++。它具有以下几个特点和优势：</p><p><strong>简单易学</strong>： 库中的命令与Python turtle的命令非常相似，用法绝大多数一模一样。这使得熟悉Python绘图的用户可以快速上手C++编程。对于少年儿童来说，使用熟悉的命令可以降低学习门槛，激发他们对编程的兴趣。<br/><strong>功能强大</strong>： 虽然命令简单，但C++精灵库基于SDL2库，同时具备C++的强大性能和灵活性。用户可以利用C++的高级特性，如对象、函数和循环，实现更复杂的图形和动画效果。<br/><strong>丰富的图形效果</strong>： 库支持设置画笔颜色、粗细、速度，以及绘制各种图形（直线、圆圈、圆点、圆弧、椭圆等）并且增强了对画笔颜色的一些更精细的控制。比如让颜色渐变的coloradd命令。实际是逐步增加颜色的色相。比如设定颜色的饱和度命令(pensat)，还有设定颜色的明度命令(penvalue) 及洪水填充命令fill等。用户通过组合这些命令，用户可以创造出丰富多彩的图形和动画效果。例如，本示例中通过动态改变画笔颜色，实现了颜色随方向变化的绚丽图案。<br/><strong>拓展与互动性强</strong>： C++精灵库的底痤基于SDL2库，可以完美融入SDL2库的命令，从而方便地响应用户输入（如鼠标点击、键盘按键等）。这使得用该库开发的程序具有更强的交互性，也可以用于游戏和教育应用的开发制作。</p><p>综上所述，C++精灵库是一个非常适合少年儿童学习编程的工具。它将Python turtle的易用性与C++的强大功能相结合，使孩子们在享受编程乐趣的同时，也能逐步掌握C++语言的基本概念和编程技巧。对于培养少年儿童的逻辑思维和创造力，C++精灵库无疑是一个“一箭双雕”的选择。</p>]]></description></item><item>    <title><![CDATA[MaxCompute Autoscale自动弹性功能上线：资源随需而动，成本精打细算 阿里云大数据A]]></title>    <link>https://segmentfault.com/a/1190000047597485</link>    <guid>https://segmentfault.com/a/1190000047597485</guid>    <pubDate>2026-02-06 18:15:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在云原生数据仓库的演进过程中，如何在<strong>保障作业SLA</strong>与<strong>优化资源成本</strong>之间取得平衡，始终是用户关注的核心问题。传统静态资源配置模式难以应对现代数据作业中普遍存在的<strong>突发性、非周期性、不可预测性</strong>负载特征。</p><p>MaxCompute 全新推出 <strong>自动弹性（Autoscale）功能</strong>——基于实时负载感知的秒级弹性扩缩容机制，结合按量计费模型，实现计算资源供给与业务需求的动态对齐。</p><h2>一、背景：从静态预留到智能弹性</h2><p>过去，MaxCompute 用户主要依赖 <strong>包年包月预留资源</strong>：稳定可靠，但缺乏灵活性；面对突发需求，只能提前大量采购，造成大量闲置。</p><p>后来，基于推出的<strong>弹性预留</strong> 模式：用户可自定义时间计划和扩缩规则，适用于有明显周期性波动的场景（如每天凌晨跑批）。但这也要求用户具备较强的运维能力，且难以应对突发或不规则的负载变化。</p><p>现在，MaxCompute 全新推出 <strong>自动弹性（Autoscale）功能</strong> —— 通过系统的负载感知与调度策略，实现“无感扩缩”，填补了<strong>非稳态、高动态</strong>场景下的资源管理空白，真正做到“用多少，付多少”。</p><table><thead><tr><th><strong>资源类型</strong></th><th><strong>扩缩机制</strong></th><th><strong>计费模型</strong></th><th><strong>适用场景</strong></th></tr></thead><tbody><tr><td>包年包月预留</td><td>固定CU，长期持有</td><td>为购买量付费</td><td>负载稳定、无波动</td></tr><tr><td>弹性预留</td><td>用户自定义时间/CU规则扩缩</td><td>按用户分时配置的固定CU量计费</td><td>周期性波动、峰谷可预测、用户有精细化配置经验</td></tr><tr><td><strong>自动弹性</strong></td><td><strong>系统实时感知负载后自动扩缩容</strong></td><td><strong>按实际用量和使用时长付费</strong></td><td><strong>波动频繁、不可预测，追求成本效率</strong></td></tr></tbody></table><p><img width="723" height="479" referrerpolicy="no-referrer" src="/img/bVdnSrg" alt="" title=""/></p><p>三者可组合使用：以包年包月为基础保障，弹性预留应对可预测高峰，自动弹性兜底突发流量，构建MaxCompute Serverless 弹性资源体系。</p><h2>二、自动弹性核心优势</h2><h3>1. 开箱即用，低运维负担</h3><ul><li>用户只需设置 <code>AutoscaleLimitCU</code>（自动弹性上限），系统自动完成扩缩决策；</li><li>支持一级/二级 Quota 粒度配置，二级 Quota 共享一级自动弹性CU资源池，自动分配。</li></ul><h3>2. 按需供给，按量计费</h3><ul><li>仅对实际使用的自动弹性CU（<code>AutoscaleUsedCU</code>）用量按秒计量，按小时统计出账；</li><li>单价：<strong>0.36元 / (CU·时)</strong>，无需预付，无最低消费。</li></ul><h3>3. 秒级响应，保障作业SLA</h3><ul><li>相比小时级调度窗口，自动弹性支持<strong>秒级资源调整</strong>，有效应对突发作业排队；</li><li>后端基于历史负载与预测模型优化库存保障和资源调度，提升弹性资源可用性。</li></ul><p>⚠️ 注意：自动弹性依赖实时资源库存，无法100%保证极端突发场景下的资源可达性。对于强SLA要求场景（如大促），建议<strong>同步配置弹性预留</strong>作为资源兜底。</p><h2>三、真实场景案例</h2><h3>场景一：突发业务高峰下的作业SLA保障</h3><p>某电商平台客户，日常使用 <strong>50 CU 包年包月Quota</strong>，足以支撑日常数据加工分析任务。但每逢大促，作业量激增3倍，原有资源严重不足，作业排队超2小时，严重影响数据产出时效。</p><p>客户曾评估扩容包年包月Quota至150 CU，但大促仅占全年不到20%的时间，全年多花<strong>约18万元</strong>，长期持有高配资源性价比极低。</p><p><strong>启用 Autoscale 后</strong>：</p><ul><li>设置 自动弹性上限 AutoscaleLimit 为 <strong>100 CU</strong>（即最多可额外使用100 CU自动弹性CU）</li><li>系统在检测到作业队列积压后，<strong>秒级自动扩容，</strong> 动态将可用CU提升至140CU（50 CU包年包月 + 90CU自动弹性），作业完成时间恢复至30分钟内，满足业务SLA要求；</li></ul><p>“以前不敢做大促实时分析，现在敢了，而且花得更少！” —— 客户反馈</p><h3>场景二：替代分时弹性，实现降本增效</h3><p>某金融客户每日需执行大量 T+1 批处理任务，用于全量交易对账、监管报送数据聚合等，长期采用 <strong>分时弹性预留策略</strong>：每日22:00–6:00 时段将 Quota 从 包月预留 50 CU 扩容至 100 CU。</p><p>但时常因业务活动、节假日调休、系统割接活上游产出延迟等，常出现“资源空转”或“容量不足”并存的问题，运维团队需频繁调整弹性计划，但人工干预滞后性强，且易出错。</p><p><strong>切换至 Autoscale 后</strong>：</p><ul><li>设置自动弹性上限 AutoscaleLimitCU 为 <strong>60 CU</strong> ，允许系统在 50 – 110 CU 范围内动态扩缩；</li><li>系统根据实际作业队列动态调整弹性CU，夜间平均仅使用 <strong>30 CU</strong> 自动弹性资源；</li><li>月度弹性费用从分时弹性CU <strong>3780元</strong> （50CU *0.315元/CU*8小时*30天）降至 <strong>2592元</strong>（30CU *0.36元/CU*小时*8小时*30天），<strong>降本32%</strong>，且作业完成时间更稳定。</li></ul><p>“不用再熬夜调配置了，系统自己会‘看饭下菜’！” —— 运维工程师点赞</p><h2>四、快速启用</h2><h3>概念说明</h3><p>自动弹性上限CU（AutoscalelimitCU）：指用户为Quota设置的弹性CU资源总上限。当该值 &gt; 0 时，则为启用自动弹性功能，系统可在此上限范围内按实际负载自动扩缩容。自动弹性使用CU（AutoscaleUsedCU）：指在启用自动弹性后，Quota中实际消耗的自动弹性CU资源使用量。系统将根据作业负载自动调整CU用量，并按此实际CU使用量计费。</p><h3>使用须知</h3><p><strong>前提条件</strong>：必须已购买<strong>包年包月计算资源Quota</strong>；<strong>计费单位</strong>：CU·时，按秒采样、按小时聚合；<strong>自动弹性CU价格</strong>：0.36元 /(CU*时)；<strong>计费公式：</strong>每小时的费用 = 该小时自动弹性CU用量（单位：CU*时）× 自动弹性CU价格。</p><h3>谁适合用自动弹性？</h3><p>✅ <strong>业务负载波动频繁、难以预测</strong>（如营销活动、临时分析） <br/>✅ <strong>希望保障作业性能，同时避免资源浪费</strong><br/>✅ <strong>已有包年包月Quota，想进一步补充/优化弹性资源</strong></p><p>登录 MaxCompute 控制台 → Quota管理 → 编辑基础配置 → 设置 <strong>AutoscaleLimitCU</strong></p><p>即可开启智能弹性之旅！</p><p>更多说明文档请参考 <a href="https://link.segmentfault.com/?enc=Cw56NZRL1%2FIksOGpPOQnKA%3D%3D.tWXs5gRJUyBbis92lMaxth27Zm9cSIPfsxTjlS%2BcgEfqEKgP4sgB%2BAjiT4y%2BMgRYWN6opqdw%2BF3InOT%2BLwe8d2b9l02RzIltkdrfD6nl4EOknshGdP5CFboJAoRQSsauuQrLYUXU6h1UJm%2BItYd7yfLfMPNrt6hsWDlAAa3UgtY%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/maxcompute/use-cases/auto-elastic-usage-best-practices?spm=a2c4g.11174283.help-menu-search-27797.d\_0</a></p><h2>五、总结</h2><p>自动弹性不是简单的“资源扩容”，而是 MaxCompute 在<strong>智能调度、成本治理、SLA保障</strong>三位一体方向上的重要演进。它让资源管理从“静态规划”走向“动态协同”，真正实现“<strong>用多少，付多少；要多少，给多少</strong>”。</p><p>欢迎您的试用并反馈您的生产实践。我们将持续优化弹性调度算法与资源保障能力，助力企业构建更高效、更经济的云原生数据基础设施。</p>]]></description></item><item>    <title><![CDATA[TDengine TSDB 3.4.0.0 上线：虚拟表、流计算性能显著提升，安全能力全面进阶 TD]]></title>    <link>https://segmentfault.com/a/1190000047597492</link>    <guid>https://segmentfault.com/a/1190000047597492</guid>    <pubDate>2026-02-06 18:14:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在实际生产环境中，时序数据库面临的挑战早已不只是“能不能存数据”，而是如何在复杂查询、高并发计算、多源接入和安全合规要求不断提高的情况下，依然保持稳定、高效和可控。</p><p>近日，TDengine TSDB 发布新版本 3.4.0.0，围绕查询性能、流计算能力、安全体系与数据接入生态进行了系统性增强。本次更新在虚拟表查询、状态窗口计算、流计算性能等核心场景下带来了显著优化，同时补齐了多项安全能力，并进一步扩展了数据订阅、授权服务及主流数据源接入能力。</p><p>本文将为你梳理该版本的主要更新亮点，帮助你快速了解哪些改进能在实际场景中带来更直接的性能提升、更稳定的运行体验，以及更灵活的系统集成方式。</p><h2>重要更新亮点</h2><h3>安全功能全面提升（企业版）</h3><p>通过对身份鉴别、权限控制、审计、传输与存储等关键环节的系统性安全加固，新版本整体安全能力得到显著提升，为安全可靠性测评及等保三级、四级要求提供有力支撑。</p><h4>身份鉴别</h4><p>新版本在身份鉴别与访问控制方面进行了增强，支持强口令策略及密码生命周期管理，引入多因素认证与 TOKEN 认证机制，并完善用户锁定与会话控制能力。同时，系统支持基于 IP 与时间段的访问限制，口令在存储与传输过程中均采用加密保护，进一步提升整体安全性。</p><h4>访问控制</h4><p>新版本完善了基于 RBAC 的权限管理体系，引入系统级与对象级权限划分，内置互斥的 SYSDBA、SYSSEC、SYSAUDIT 系统角色，实现权限制衡与职责分离。同时支持权限与角色的创建、删除、授予与回收，提供标准 GRANT/REVOKE 语法，并支持对象所有者权限转移。在访问控制层面，支持库、表、列等多层级权限控制。</p><h4>安全审计</h4><p>新版本完善了分级审计能力，按粒度分为系统级、集群级、数据库级、子表级、数据级五级审计，支持查询、删除、写入等数据操作审计。审计操作与业务访问相互隔离，强制启用加密存储并设置不少于 5 年的保留策略，相关安全属性不可修改。同时强化审计链路安全与防篡改能力，保障审计数据的完整性与可信性。</p><h4>传输安全</h4><p>新版本完善了传输安全与连接管控机制，采用 TLS 传输加密与 SASL 身份认证的分层架构，保障通信安全。在连接层面，支持按用户配置并发会话数、会话时长及空闲超时等参数，并增强黑白名单访问控制能力。同时引入通信失败监测与异常告警机制，可在异常场景下自动触发告警并临时锁定相关用户/IP。TLS 私钥采用加密存储并支持安全轮换，相关安全操作均可审计，在保障安全性的同时总体性能下降不超过 10%。</p><h4>存储安全</h4><p>新版本完善了存储安全能力，采用分级密钥体系，对配置文件、元数据及时序数据实现透明加密，密钥生成、变更、到期及恢复等过程统一由系统管理，用户方面无感知。核心密钥通过加密通信机制安全传输，并支持国密算法适配，敏感操作需管理员权限并全程留存审计。同时提供加密状态与范围的可观测能力，支持密钥到期告警配置。</p><h4>加密算法</h4><p>新版本增强了加密算法管理能力，新增系统表用于集中查看和管理可用加密算法，覆盖对称加密、非对称加密与散列算法等类型。系统内置国密与国标算法，适配数据加密、密钥交换与完整性校验等多种场景，并支持通过动态链接库方式扩展自定义加密算法，满足不同环境下的算法适配需求。</p><h4>安全函数</h4><p>新版本补充了安全相关内置函数能力，提供数据加密、脱敏、哈希及编码转换等函数，支持国密与国际算法，满足数据存储、传输及查询过程中的安全处理需求。</p><h3>流计算事件窗口新增「子事件窗口」触发机制</h3><p>本次版本在流计算中引入事件窗口的子窗口触发机制，支持为同一事件定义多个开始条件。不同开始条件满足时，可依次触发对应的子事件窗口，系统自动维护父事件窗口的开启与关闭关系；父窗口及各子窗口均可独立触发计算与通知。该能力特别适用于分级告警、状态升级、阈值递进等复杂场景，使事件驱动的流计算逻辑更加贴近真实业务变化过程，而无需通过多条规则或多条流任务进行拆分实现。</p><pre><code class="sql">EVENT_WINDOW(START WITH (start_cond_1, start_cond_2 [,...]) [END WITH end_cond])</code></pre><h3>流计算的资源消耗和计算延迟显著降低</h3><p>新版本在 Nevados 实际业务场景下对流计算引擎进行了针对性优化，显著降低了资源消耗并改善了计算延迟表现。优化后，CPU 平均使用率<strong>由 321.7% 降至 30.3%</strong>，降幅约 <strong>90.6%</strong>；内存平均占用<strong>由 8.65 GB 降至 1.49 GB</strong>，减少约 <strong>82.8%</strong>。同时，流计算的平均处理延迟由原来的<strong>约 1 小时缩短至 5 分钟以内</strong>，整体响应速度提升约 <strong>92%</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597494" alt="" title=""/></p><h3>虚拟表的查询性能优化</h3><h4>投影查询性能优化（虚拟超级表 / 子表）</h4><p>新版本针对虚拟超级表及虚拟子表的投影查询场景，系统对查询路径进行了针对性优化，覆盖包含 <code>tbname</code>、<code>tag</code> 条件、时间过滤以及全量扫描等多种常见查询模式。在包含 <code>tbname</code> 或 <code>tag</code> 条件的查询场景下，查询性能提升最高可达<strong>千倍量级</strong>，显著改善了典型业务查询的响应速度；在全量扫描或单表查询场景中，性能提升相对有限，但仍体现了底层执行与数据访问优化带来的整体收益。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597495" alt="" title="" loading="lazy"/></p><h4>聚合与选择函数查询性能优化（虚拟超级表）</h4><p>新版本针对虚拟超级表在聚合函数与选择函数场景下的查询性能进行了系统性优化，覆盖是否使用 <code>partition by</code>、函数参数是否包含 tag 等多种常见用法。优化后，虚拟超级表在上述典型查询用例中的执行时间由原来的 <strong>68–86 秒</strong> 显著缩短至 <strong>0.088–0.640 秒</strong>，整体性能提升约 <strong>119×–796×</strong>，大幅改善了统计分析与状态类查询的响应效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597496" alt="" title="" loading="lazy"/></p><h4>状态窗口查询性能优化（虚拟超级表 / 子表）</h4><p>针对虚拟超级表及子表在状态窗口计算场景下的性能问题，新版本对窗口判定与计算流程进行了优化，特别用于解决稀疏数据与密集数据混合计算时的效率瓶颈。新机制下，系统可在窗口触发前先提取窗口边界信息，再按策略激活后续计算，并支持按批次或单窗口两种优化策略。在数据分布密集场景下推荐使用批处理策略，在数据分布相对均匀且窗口数量较少的场景下，可选择单窗口策略，其余情况沿用默认策略。</p><h3>查询性能优化及语法增强</h3><h4>新增非相关标量子查询</h4><p>在查询能力方面，新增对非相关标量子查询的支持，子查询可返回单行单列结果并作为常量参与主查询计算与条件判断。</p><h4>状态窗口零状态支持</h4><p>新增状态窗口零状态（zeroth state）能力，可在状态窗口计算完成后，将状态值等于指定零状态的窗口整体排除，不参与后续计算。该机制与通过 <code>WHERE</code> 条件过滤数据不同：零状态是在完整状态窗口判定之后进行过滤，而 <code>WHERE</code> 条件是在数据进入窗口计算之前生效，可用于更精确地区分“无效状态窗口”与“有效状态但需排除的数据”。</p><pre><code class="sql">STATE_WINDOW(col[, extend][, zeroth_state]) [TRUE_FOR(true_for_duration)]</code></pre><h4>多类典型查询与能力边界优化</h4><p>新版本针对多种高频查询场景进行了集中优化，包括 <code>last_row + tags</code> 查询、系统表统计子表数量以及窗口查询能力扩展。优化后，<code>last_row + tags</code> 查询平均耗时由 <strong>25.9 秒</strong> 降至 <strong>0.385 秒</strong>，性能提升约 <strong>68 倍</strong>；基于系统表统计子表数量的查询平均耗时由 <strong>3.824 秒</strong> 降至 <strong>0.003 秒</strong>，同样提升约 <strong>68 倍</strong>。同时，窗口查询不再强制要求包含聚合函数，可仅使用窗口伪列（\_wstart、tbname 等）参与查询；虚拟表支持的最大列数提升至 <strong>32767 列</strong>，且不影响写入与查询性能。</p><h3><strong>XNODE 高可用与负载均衡支持</strong></h3><p>在新版本中，taosX 正式成为 TSDB 的一个内部组件：XNODE，由 MNODE 统一管理并通过 <code>xnoded</code> 调度器进行调度，支持高可用与负载均衡能力。</p><h3><strong>OAuth 2.0 / OIDC 单点登录（SSO）支持</strong></h3><p>新版本新增对 OAuth 2.0 与 OIDC 的单点登录支持，兼容 OAuth 2.0 与 OIDC 1.0 标准 API，并支持基于 OIDC 的端点自动发现。同时提供可配置的自定义 OAuth 2.0 API 接入能力，并支持对 SSO 用户的基础管理功能（部分能力已实现）。</p><h3><strong>KingHistorian  数据源支持</strong></h3><p>新版本新增对 KingHistorian 数据源的支持。KingHistorian 是 Wellintech 于 2006 年推出的工业实时数据库，已在现场运行近 20 年，支持单机最高 200 万标签规模，广泛应用于大规模设备数据采集与实时计算场景。</p><h3><strong>Pulsar 数据源支持</strong></h3><p>新版本新增对 Apache Pulsar 数据源的支持。Apache Pulsar 是一款分布式发布订阅消息平台，支持灵活的消息模型与流式消费方式，可用于消息队列及流处理场景。在使用体验上，Pulsar 数据源的 UI 界面与 Kafka 保持一致，降低多数据源场景下的使用与运维成本。</p><h3><strong>taosgen 发布到 Kafka</strong></h3><p>taosgen 新增对 Apache Kafka 的数据发布能力，支持将生成的数据直接写入 Kafka 主题。Apache Kafka 是一款开源的分布式流处理平台，常用于构建实时数据管道与流式应用，该能力可用于数据生成、测试与流处理场景的联动验证。</p><h3>taosAdapter 功能增强（JSON 写入与查询管控）</h3><p>taosAdapter 新增对 HTTP POST JSON 写入的支持，可接收任意格式的 JSON 数据，并通过 JSONata 进行数据转换，同时支持时间字段解析；同时引入 SQL 查询请求管控能力，支持对 SQL 请求进行拦截，并按用户维度设置并发限制，提升接口访问的可控性与稳定性。</p><h3><strong>C WebSocket（WS）支持 TLS</strong></h3><p>C WebSocket 连接器新增 TLS 支持，实现通信过程的端到端加密，提升数据传输的安全性。</p><h3><strong>OpenTSDB 支持自定义列名、子表名</strong></h3><p>OpenTSDB 接入新增对自定义字段与子表命名的支持，可灵活配置时间戳字段、数值字段以及子表名，提升不同 OpenTSDB 数据模型下的接入适配能力。</p><h2>其他优化</h2><ol><li>TDgpt 的数据补全算法支持任意采样间隔，支持  dtw、dtw\_path、tlcc 等相关性分析函数</li><li>新增 maxSQLLength 设置 SQL 语句的最大长度，最大可为 64M</li><li>虚拟表支持的最大列数提升至 32767 列</li><li>STMT2 对虚拟表查询的支持</li><li>Compact 命令支持 force 选项</li><li>Show connections 命令新增客户端版本号字段</li><li>Show vgroups 命令新增 is\_ready 列</li><li>优化 event\_window 按 tbname 分组查询的效率</li><li>优化子查询做主键过滤条件时的性能</li></ol><p>除此之外，每个版本都会做很多其他的工作，比如 bug 修复、功能优化等等。如果想要了解新版本（时序数据库功能更新）更加详细的发布信息，可以移步至 <a href="https://link.segmentfault.com/?enc=czEQxVZyWx48w2btiaEpEw%3D%3D.xoBwoZUnEAXACQsDhqjGaTE1M%2B6%2FdlWq2eWu0f6V37ZP5Mno4J%2Bd0xyszbXS22zwTpG8l680ID%2BAeu%2FPk81SWA%3D%3D" rel="nofollow" target="_blank">https://github.com/taosdata/TDengine/releases/tag/ver-3.4.0.0</a> 查看发布说明。</p><p>欢迎大家下载使用，也欢迎在评论区提出建议和意见，如有任何问题请及时联系我们获得支持。</p>]]></description></item><item>    <title><![CDATA[工业智能体：从单点自动化到全链路自主决策的进化之路 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047597522</link>    <guid>https://segmentfault.com/a/1190000047597522</guid>    <pubDate>2026-02-06 18:13:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的智能化浪潮中，人们曾一度将AI视为提升效率的“万能工具”——图像识别质检、预测性维护、自动排产……这些单点突破看似亮眼，却始终难以撬动全局。真正的变革，不是让机器学会看图，而是让它学会“思考”整个生产系统的运行逻辑。工业智能体的出现，正是为了弥合这一鸿沟。它不是某个算法模型的简单封装，而是一套能感知、推理、决策并执行的数字生命体，其核心价值在于将工业知识、数据流与业务流程深度融合，形成闭环的自主运行能力。与通用大模型不同，工业智能体必须扎根于车间的振动数据、工艺参数、物料流转节奏之中，它需要理解“为什么这台设备在凌晨三点振动加剧”，而不仅仅是“这组数据异常”。<br/>要实现这种深度嵌入，技术底座必须足够坚实。许多企业试图直接部署智能体应用，却忽略了数据治理、知识沉淀与系统集成的底层工程。工业现场的数据往往碎片化、非结构化，工艺经验散落在老师傅的脑子里，ERP、MES、PLC等系统彼此割裂。真正的工业智能体，必须能打通这些断点，把隐性知识转化为可计算的规则，把分散的系统整合为统一的决策网络。这要求平台不仅提供算法能力，更要具备工业Know-How的封装能力——就像为AI建立一本“懂行的字典”，让它能读懂工程师的意图，也能用生产语言输出建议。这种能力，不是靠堆算力就能获得的，而是长期与产线共处、反复迭代的结果。<br/>在这一领域，广域铭岛的“工业智造超级智能体”提供了一个极具参考价值的范式。其平台通过“数据标准化+知识封装+积木式智能体开发”三位一体架构，让企业能快速构建覆盖研发、生产、物流、服务的智能体矩阵。例如，在某新能源电池企业，当某批次电芯容量波动时，仓储智能体自动关联原材料批次、环境温湿度与设备参数，15分钟内定位到是某台涂布机的张力控制异常，并联动工艺参数自动调整，避免了整批报废。这种“感知—诊断—决策—执行”的闭环，不再依赖人工巡检与会议决策，而是由多个智能体协同完成。类似实践也出现在海外：德国西门子的“数字孪生智能体”在安贝格工厂实现产线自优化，当订单变更时，系统自动重排工艺路径并模拟能耗影响；美国通用电气的Predix平台则通过设备智能体，对燃气轮机进行实时健康评估，提前72小时预测关键部件失效，将非计划停机减少40%。这些案例共同揭示了一个趋势：工业智能体的竞争力，不在于模型多大，而在于它是否真正“懂”这个工厂。</p>]]></description></item><item>    <title><![CDATA[超越跑分：新一代 AI 基准与模型评测的范式转变 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047597524</link>    <guid>https://segmentfault.com/a/1190000047597524</guid>    <pubDate>2026-02-06 18:13:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>人工智能的竞技场上，每一次新模型的发布都伴随着激动人心的基准测试结果。“在 MMLU 上达到 92.5%！”“在 HumanEval 上超越 GPT-4！” 这些头条新闻确实抓人眼球，但敏锐的 AI 开发者们越来越意识到：<strong>数字并不能讲述完整的故事</strong>。当今最前沿的模型评测，正在经历一场从 “单纯跑分” 到 “全面理解” 的深刻转变。</p><p><strong>新基准的崛起：ARC-AGI 与 GPQA 为何与众不同</strong><br/>传统基准数据集如 MMLU、GSM8K 虽然仍有价值，但它们逐渐暴露出局限性 —— 可能被大量纳入训练数据、无法真正衡量推理能力、或与现实世界问题脱节。这正是 ARC-AGI 和 GPQA 等新一代基准引起广泛关注的原因，也成为 <a href="https://link.segmentfault.com/?enc=F8vNixEAmHZVhMOLlD2HNQ%3D%3D.tlaa%2BO%2BDxxhuXdPBGgvp2Wqs8GqhGs109xNDOsfKKSw%3D" rel="nofollow" target="_blank">Smoothcloud 润云</a>构建企业级 AI 评测矩阵的核心参考依据。</p><p>ARC-AGI（Abstract Reasoning Corpus for AGI）由 OpenAI 前研究员 François Chollet 创建，其核心理念直指 AI 系统的要害：泛化能力。ARC-AGI 不测试记忆或模式匹配，而是评估模型在面对全新类型问题时的抽象推理能力。数据集包含一系列基于网格的模式完成任务，每个任务都设计得独一无二，确保模型无法从训练数据中直接回忆答案。这种设计迫使模型必须真正 “理解” 问题背后的抽象规则，而非简单应用已见模式。Smoothcloud 润云正是基于此类核心基准的设计逻辑，为不同行业客户定制化开发了避免 “训练污染” 的专属评测数据集，确保评测结果能真实反映模型在实际业务中的泛化能力。</p><p>GPQA（Graduate-Level Google-Proof Q&amp;A）则走向另一个极端：深度领域专业知识。这个基准由耶鲁大学科学家创建，包含 400 多个涵盖物理、化学、生物学等学科的研究生级别问题。关键之处在于，这些问题被设计为 “谷歌无法直接解答”—— 无法通过简单搜索获得答案，需要深度的学科知识和多步骤推理。GPQA 不仅测试模型的知识广度，更重要的是测试其深度理解和复杂推理链的构建能力。针对这一特性，Smoothcloud 润云已将 GPQA 的评测逻辑融入到金融、生物医药、高端制造等领域的模型评估中，助力企业筛选出真正具备深度行业推理能力的 AI 模型。</p><p><strong>全面评测的艺术：弱点分析比高分更重要</strong><br/>当 Llama 3、GPT-4o 或 Claude 3 等新模型发布时，前沿开发者不再仅仅关注它们在排行榜上的位置，而是深入挖掘模型的能力边界与失败模式 —— 这也是 Smoothcloud 润云为客户提供的核心评测服务方向。</p><ol><li>能力边界的精细测绘高级评测者会进行 “压力测试”：模型在长上下文中的一致性如何？面对对抗性提示的鲁棒性怎样？在不同语言和文化背景下的表现是否均衡？例如，一个模型可能在英语科学问题上表现优异，但在非拉丁语系的诗歌分析中却漏洞百出。Smoothcloud 润云通过自研的多维度压力测试工具，能够为企业精准绘制目标模型的能力边界，甚至细化到不同语种、不同业务场景下的性能表现差异。</li><li/><li>失败模式的系统分类真正的洞察来自分析模型如何失败而非如何成功。失败模式可能揭示：<br/>系统偏差：模型是否过度依赖某些思维模式？<br/>知识断层：在哪些知识领域存在明显盲点？<br/>推理短路：是否倾向于选择表面合理而非真正正确的答案？<br/>Smoothcloud 润云的评测体系中，专门包含 “失败模式归因模块”，不仅能系统分类模型的失败类型，还能结合企业业务场景分析失败背后的潜在风险，为后续优化提供可落地的方向。</li><li>真实世界适用性评估开发者关注模型在特定应用场景中的表现：在代码生成任务中，生成的代码是否考虑了边缘情况？在医学问答中，是否表现出过度自信倾向？这种评估往往通过精心设计的领域特定测试集进行，而非通用基准。Smoothcloud 润云依托海量的行业场景数据，已搭建起覆盖电商、医疗、金融、工业等数十个领域的专属测试集，让模型评测结果直接对接真实业务需求。</li></ol><p><strong>评测方法的创新：从静态测试到动态交互</strong><br/>传统基准如同标准化的多项选择题考试，而新兴评测方法更像是一场对话或合作项目，这与Smoothcloud 润云倡导的 “场景化动态评测” 理念高度契合。</p><p>动态评估框架如 Chatbot Arena 采用众包配对比较，让人类评估者在真实对话中判断模型回答的质量。这种方法的优势在于捕捉模型在开放域交互中的综合表现，包括一致性、有用性和安全性。Smoothcloud 润云已将此类动态评估框架产品化，结合人机协同的评测模式，为企业提供贴近真实用户交互场景的模型评估结果。</p><p>诊断性探针则通过精心设计的提示词，主动探测模型的内部机制和局限性。例如，通过逐渐增加问题复杂性，观察模型性能下降的 “拐点”；或通过语义改写，测试模型是否真正理解概念而非记忆表面模式。Smoothcloud 润云的技术团队还对诊断性探针进行了场景化改造，使其能适配企业的专属业务逻辑，精准探测模型在核心业务环节的表现。</p><p><strong>实践意义：这对 AI 开发者意味着什么？</strong><br/>对于构建和部署 AI 系统的开发者而言，这种评测范式的转变有着直接影响，而 Smoothcloud 润云则成为连接新一代评测理念与企业实际应用的桥梁：</p><p>技术选型更明智：了解模型的特定优势和弱点，有助于为不同应用场景选择最合适的模型。例如，一个在 GPQA 上表现平平但在代码基准上卓越的模型，可能是开发工具的理想选择，但不适合作为科学研究助手。Smoothcloud 润云会基于企业的业务目标，输出模型选型的量化分析报告，避免企业因单纯参考跑分而做出不当决策。</p><p>风险规避更有效：通过弱点分析，开发者可以预先识别模型在特定领域可能产生的错误类型，从而设计防护措施或备用流程。Smoothcloud 润云还会结合行业合规要求，在评测中融入风险预警模块，提前识别模型在数据安全、合规性等方面的潜在问题。</p><p>微调方向更精准：知道模型的失败模式，可以针对性地收集数据、设计微调策略，更高效地提升模型在实际任务中的表现。Smoothcloud 润云能基于评测结果，为企业提供定制化的模型微调方案，包括数据采集方向、微调策略设计等，让模型优化更具针对性。</p><p><strong>未来展望：全面评测的挑战与方向</strong><br/>尽管全面评测的理念日益普及，但仍面临挑战：如何平衡评测的深度与可扩展性？如何设计真正无法被 “训练污染” 的基准？如何量化模型行为的细微差别？Smoothcloud 润云也正围绕这些挑战展开技术探索，力求为企业提供兼具深度与效率的评测服务。</p><p><strong>未来，我们可能会看到更多：</strong></p><p>多模态综合评估：同时测试文本、图像、音频和视频理解能力 ——Smoothcloud 润云已启动多模态评测体系的研发，适配企业日益增长的多模态 AI 应用需求；<br/>长期交互评估：在延长时间尺度上测试模型的记忆和一致性 —— 这也是 Smoothcloud 润云针对客服、智能助手等长期交互场景重点布局的评测方向；<br/>价值观与安全性评估：超越表面无害，深入评估模型的价值对齐程度 ——Smoothcloud 润云已将价值观对齐评测纳入金融、教育等敏感行业的评测标准中。</p><p><strong>结语</strong><br/>在人工智能快速发展的今天，对新模型的评判标准正在从 “有多聪明” 转变为 “在哪些方面聪明，在哪些方面还有局限，以及为什么会这样”。这种转变不仅反映了领域成熟度的提升，也标志着 AI 开发者社区对技术理解的深化。</p><p>Smoothcloud 润云始终认为，真正有价值的 AI 评测，是让企业跳出 “跑分竞赛” 的误区，精准把握模型的独特特征、适用场景和内在局限性—— 这些洞察才是将 AI 技术有效、负责任地应用于现实世界的关键。无论是依托新一代基准构建的定制化评测体系，还是贴合业务场景的动态交互评估，Smoothcloud 润云都在以技术赋能的方式，帮助企业将全面评测的理念落地，让 AI 模型的价值真正在实际业务中释放。在这个意义上，一次由 Smoothcloud 润云助力的深入弱点分析，往往比一个漂亮的跑分数字更能为企业创造长期价值。</p>]]></description></item><item>    <title><![CDATA[Hadoop基础认知——HDFS、YARN、MapReduce在现代体系中的位置与价值 南城 ]]></title>    <link>https://segmentfault.com/a/1190000047597539</link>    <guid>https://segmentfault.com/a/1190000047597539</guid>    <pubDate>2026-02-06 18:12:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>写在前面，本人目前处于求职中，如有合适内推岗位，请加：lpshiyue 感谢。</strong></p><blockquote>HDFS 是海量数据的基座，MapReduce 是批量计算的引擎，而 YARN 是集群资源的调度者——它们共同构成了大数据处理的“古典三位一体”。</blockquote><p>在深入探讨了数据平台的全景与角色分工之后，我们触及了现代数据体系的基石。无论是 OLTP 的实时交易，还是 OLAP 的深度分析，其背后都需要强大的底层基础设施来支撑海量数据的存储与计算。本文将聚焦于大数据领域的奠基者——Hadoop，解析其核心组件 HDFS、YARN 与 MapReduce 的经典架构、协同原理及其在当今技术浪潮中的独特价值。</p><h2>1 Hadoop 的起源与核心命题</h2><p>Hadoop 并非凭空诞生，它源于互联网时代一个根本性的挑战：<strong>当数据规模远超单机极限，我们该如何存储和处理它？</strong></p><p>在 2000 年代初，Google 面临索引整个互联网的难题。其给出的答案是两篇划时代的论文：关于分布式文件系统的 <strong>GFS</strong> 和关于分布式计算的 <strong>MapReduce</strong>。Hadoop 正是这两大思想的开源实现，它要解决的核心问题可以归结为三点：</p><ul><li><strong>数据存储</strong>：如何将 PB 级文件可靠地存储在成千上万台普通服务器上。</li><li><strong>计算能力</strong>：如何将巨大的计算任务拆解，并分发到集群中并行处理。</li><li><strong>资源协调</strong>：如何让多个计算任务共享集群资源，且互不干扰。</li></ul><p>Hadoop 的核心理念是 <strong>“移动计算比移动数据更划算”</strong>。与其将海量数据通过网络传输到计算程序所在的地方，不如将小巧的计算程序发送到数据存储的节点上本地执行。这一理念贯穿于其三大核心组件的设计之中。</p><h2>2 HDFS：分布式存储的基石</h2><p>HDFS 是 Hadoop 的存储基石，它的设计目标非常明确：<strong>一次写入，多次读取</strong>，以流式数据访问模式来存储超大文件。</p><h3>2.1 架构与核心组件</h3><p>HDFS 采用了经典的<strong>主从架构</strong>：</p><ul><li><strong>NameNode</strong>：集群的“大脑”或“总目录”。它负责管理文件系统的<strong>命名空间</strong>（目录树结构）以及所有文件的<strong>元数据</strong>（如文件名、权限、每个文件块对应的 DataNode 列表等）。所有这些元数据都存储在内存中，以实现快速访问。</li><li><strong>DataNode</strong>：集群的“劳动力”。它们负责在本地磁盘上存储实际的数据<strong>块</strong>，并负责块的创建、删除和复制。</li><li><strong>Secondary NameNode</strong>：容易被误解的组件，它<strong>不是</strong> NameNode 的热备。其主要职责是定期合并 NameNode 的镜像文件和编辑日志，协助主节点进行元数据管理，以防日志过大导致重启时间过长。</li></ul><h3>2.2 关键机制与设计哲学</h3><ul><li><strong>分块存储</strong>：HDFS 将大文件切分成固定大小的<strong>块</strong>。在较早的版本中，默认块大小为 64MB，后续版本（如 Hadoop 2.x 及以后）通常默认为 <strong>128MB</strong>。分块的好处在于，一个大型文件可以分布存储在集群的多个节点上，从而为并行处理奠定了基础。同时，它也简化了存储系统的设计，无需管理巨大的文件，而只需管理固定大小的块。</li><li><strong>多副本机制</strong>：为了保证数据的可靠性，HDFS 默认将每个数据块复制<strong>3份</strong>，并遵循一种<strong>机架感知</strong>策略将它们分布在不同节点甚至不同机架上。这极大地增强了数据的容错能力。</li><li><strong>数据写入流程</strong>：客户端写入数据时，HDFS 会建立一个<strong>管道</strong>。数据块会依次从客户端流向管道中的第一个 DataNode，再由第一个 DataNode 传给第二个，以此类推。这种线性传输方式有效利用了每个节点的网络带宽。</li></ul><h3>2.3 现代体系中的价值</h3><p>尽管对象存储（如 AWS S3）如今常被用作 HDFS 的替代品，但 HDFS 在特定场景下仍有其不可替代的价值：</p><ul><li><strong>高性能计算场景</strong>：当计算框架需要极低延迟的数据本地性访问时，HDFS 由于数据直接存储在计算节点本地磁盘上，往往能提供比通过网络访问对象存储更高的吞吐量。</li><li><strong>混合负载环境</strong>：在同时运行多种批处理作业的集群中，HDFS 可以避免所有任务同时访问外部存储可能带来的带宽瓶颈。</li><li><strong>数据湖的底层存储</strong>：许多企业的数据湖架构中，HDFS 依然扮演着存储原始数据和热数据的核心角色。</li></ul><h2>3 MapReduce：分布式计算的灵魂</h2><p>MapReduce 是一种编程模型，其核心思想是 <strong>“分而治之”</strong>。它将复杂的计算任务分解为两个阶段：<strong>Map</strong> 和 <strong>Reduce</strong>，使得开发者无需关心分布式计算的底层细节（如网络通信、容错等），只需专注于实现业务逻辑。</p><h3>3.1 核心工作流程</h3><p>以一个经典的词频统计任务为例，其流程如下：</p><ol><li><p><strong>Map 阶段</strong>：</p><ul><li><strong>输入</strong>：每个 Map 任务读取 HDFS 上的一个数据块。</li><li><strong>处理</strong>：对每一行数据，执行用户自定义的 Map 函数。例如，输入 <code>“Hello World Hello”</code>，Map 函数会输出 <code>[("Hello", 1), ("World", 1), ("Hello", 1)]</code> 这样的键值对。</li><li><strong>输出</strong>：每个 Map 任务输出一系列中间键值对。</li></ul></li><li><strong>Shuffle 与 Sort 阶段</strong>：这是 MapReduce 框架最核心且最“神秘”的一步。框架会自动将所有 Map 任务输出的中间结果，<strong>按照键进行分组和排序</strong>，保证相同键的所有值会被发送到同一个 Reduce 任务进行处理。</li><li><p><strong>Reduce 阶段</strong>：</p><ul><li><strong>输入</strong>：经过 Shuffle 后，一个 Reduce 任务的输入可能是 <code>[("Hello", [1, 1]), ("World", [1])]</code>。</li><li><strong>处理</strong>：执行用户自定义的 Reduce 函数，对值列表进行汇总。例如，对 <code>“Hello”</code> 进行求和计算：<code>1+1=2</code>。</li><li><strong>输出</strong>：最终结果写入 HDFS，如 <code>[("Hello", 2), ("World", 1)]</code>。</li></ul></li></ol><h3>3.2 容错与局限性</h3><p>MapReduce 的强大还在于其<strong>容错性</strong>。如果某个节点上的 Map 或 Reduce 任务失败，YARN 会自动在另一个健康的节点上重新启动该任务，因为输入数据在 HDFS 上是有副本的。</p><p>然而，MapReduce 模型也有其<strong>局限性</strong>。由于每个阶段（尤其是 Shuffle）都涉及磁盘 I/O，因此它更擅长<strong>批处理</strong>，而对迭代式计算（如机器学习）和交互式查询的延迟较高。这也催生了 Spark 等内存计算框架的兴起。</p><h2>4 YARN：集群资源的“大管家”</h2><p>在 Hadoop 1.x 时代，MapReduce 自身负责资源管理，这导致集群只能运行 MapReduce 一种计算框架，资源利用率低且孤立。<strong>YARN 的诞生，解耦了资源管理与计算框架，是 Hadoop 从“一套系统”演变为“一个平台”的关键</strong>。</p><h3>4.1 架构与核心组件</h3><p>YARN 同样采用了主从架构：</p><ul><li><strong>ResourceManager</strong>：集群资源的最终仲裁者。它掌管着整个集群的资源（CPU、内存）情况，并负责接收和调度来自客户端提交的应用程序。</li><li><strong>NodeManager</strong>：每个节点上的代理。它负责启动并监控本节点上的资源容器，并向 ResourceManager 汇报本节点的资源使用情况。</li><li><strong>ApplicationMaster</strong>：这是 YARN 设计的精妙之处。<strong>每个应用程序</strong>（例如一个 MapReduce 作业或一个 Spark 应用）都有一个专属的 ApplicationMaster。它负责向 ResourceManager 申请资源，并与 NodeManager 通信来启动和监控具体的任务。这种设计将资源管理的全局视角和应用程序的具体管理分离开来。</li></ul><h3>4.2 工作流程示例</h3><ol><li>客户端向 ResourceManager 提交一个 MapReduce 作业。</li><li>ResourceManager 在一个空闲的 NodeManager 上分配第一个容器，并在其中启动该作业的 <strong>ApplicationMaster</strong>。</li><li>ApplicationMaster 根据作业需求（如需要运行 100 个 Map 任务），向 ResourceManager 申请资源。</li><li>ResourceManager 根据调度策略，在各个 NodeManager 上分配容器。</li><li>ApplicationMaster 与对应的 NodeManager 通信，在分配到的容器中启动 Map 或 Reduce 任务。</li><li>ApplicationMaster 监控所有任务的运行状态，直到作业完成。</li></ol><h3>4.3 现代体系中的核心价值</h3><p>YARN 的价值在于其<strong>通用性</strong>。它本身不关心运行的是 MapReduce、Spark、Flink 还是 Tez。它作为一个<strong>统一的资源管理和调度平台</strong>，允许多种计算框架在同一个集群上共享资源，提高了集群利用率，并简化了运维。在今天，YARN 依然是许多大规模 Hadoop 集群不可或缺的底层调度系统。</p><h2>5 三位一体：协同工作原理与在现代数据生态中的位置</h2><p>HDFS、MapReduce 和 YARN 共同构成了一个完整的闭环。</p><p><strong>协同工作流程</strong>：用户编写的 MapReduce 程序被打成 JAR 包提交给 YARN。YARN 的 ResourceManager 为作业分配 ApplicationMaster。ApplicationMaster 根据输入数据在 HDFS 上的位置（通过询问 NameNode 获得），向 YARN 申请在存储了相应数据块的 DataNode 上启动 Map 任务，以实现“计算向数据靠拢”。Map 任务处理本地数据，Reduce 任务通过网络拉取数据并进行汇总，最终结果写回 HDFS。</p><p><strong>在现代数据生态中的位置</strong>：尽管如今 Spark、Flink 等更快速、更灵活的计算框架大放异彩，但 Hadoop 三要素并未过时，而是找到了新的定位：</p><ul><li><strong>HDFS</strong>：依然是许多企业数据湖的<strong>可靠存储底层</strong>，尤其是在需要高吞吐、数据本地性强的场景。</li><li><strong>MapReduce</strong>：作为一种<strong>经典的编程模型</strong>，其思想深刻影响了后续几乎所有的大数据计算框架。在处理超大规模、非迭代的冷数据批量计算时，它依然稳定可靠。</li><li><strong>YARN</strong>：作为<strong>成熟的资源调度器</strong>，在管理由数千节点组成的大型混合负载集群时，其稳定性和资源隔离能力备受青睐。</li></ul><p>可以说，Hadoop 生态系统从“一套特定技术”演变成了“一系列技术选择的基石”。新一代的计算框架大多选择与 HDFS 兼容，并可以运行在 YARN 之上，这本身就是对 Hadoop 核心组件设计价值的肯定。</p><h2>6 总结与展望</h2><p>Hadoop 的核心三要素为解决大数据问题提供了一套经过实践检验的、完整的<strong>基础范式</strong>。HDFS 解决了“数据怎么存”，MapReduce 解决了“计算怎么做”，YARN 解决了“资源怎么分”。它们所体现的<strong>分治、容错、可扩展</strong>的设计思想，至今仍是构建分布式系统的黄金法则。</p><p>理解 Hadoop，不仅是掌握一套工具，更是建立一种应对海量数据挑战的<strong>基础性思维框架</strong>。即使在云原生和实时计算成为潮流的今天，这套框架所解决的存储、计算和调度问题，依然是任何数据平台架构师需要深刻理解的根本命题。</p><hr/><p><strong>📚 下篇预告</strong><br/>《Hive与离线数仓方法论——分层建模、分区与桶的取舍与查询代价》—— 我们将深入探讨：</p><ul><li>🗃️ <strong>数仓分层</strong>：ODS、DWD、DWS、ADS 的职责边界与数据流转设计</li><li>⚖️ <strong>分区策略</strong>：按时间、地域分区的优缺点与数据倾斜规避方案</li><li>🪣 <strong>分桶优化</strong>：桶的数量抉择、数据均匀分布与 JOIN 性能的提升逻辑</li><li>💰 <strong>代价评估</strong>：不同分区与桶策略下的存储、计算成本量化分析</li><li>🔄 <strong>演进路径</strong>：从传统数仓到 Hive 批处理的最佳实践迁移路线</li></ul><p><strong>点击关注，掌握离线数据仓库的构建精髓！</strong></p><blockquote><p><strong>今日行动建议</strong>：</p><ol><li>在本地搭建一个 Hadoop 单机伪分布式环境，亲手体验 <code>hdfs dfs</code> 命令和运行 WordCount 示例程序。</li><li>使用 <code>hadoop fs -put</code> 上传一个文本文件到 HDFS，观察其被分成了几个块。</li><li>通过 YARN 的 Web UI（通常为 <code>http://&lt;resourcemanager-host&gt;:8088</code>）提交一个 MapReduce 作业，直观理解其资源申请和执行流程。</li><li>思考当前业务中是否存在适合用 MapReduce “分而治之”思想处理的离线批量计算任务。</li></ol></blockquote>]]></description></item><item>    <title><![CDATA[物联网设备分布分析需要精准地理信息？支持IPv4IPv6双栈批量解析的IP离线库 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047597554</link>    <guid>https://segmentfault.com/a/1190000047597554</guid>    <pubDate>2026-02-06 18:12:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、物联网设备分布分析，真的“必须”精准地理信息吗？</h2><p>在讨论物联网设备分布之前，很多团队第一步就会接触到类似 <strong>IP数据云IP地址查询</strong>——通过设备日志里的 IP，还原设备大致所在的行政区域，但物联网场景，真的需要“越精细越好”的地理信息吗？</p><h3>物联网和互联网业务最大的不同</h3><p><strong>普通 Web/App：</strong></p><ul><li>用户是“人”</li><li>地理信息更多用于画像、推荐或内容分发</li></ul><p><strong>物联网（IoT）：</strong></p><ul><li>对象是“设备”</li><li><p>地理信息直接影响：</p><ul><li>运维</li><li>网络调度</li><li>合规判断</li><li>成本控制</li></ul></li></ul><p>因此，对IoT来说是<strong>基础数据层的一部分</strong>。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSsm" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析.png"/></p><h3>常见物联网场景，对地理精度的真实需求</h3><table><thead><tr><th>场景</th><th>是否需要精准地理</th><th>说明</th></tr></thead><tbody><tr><td>设备区域分布统计</td><td>国家/省级</td><td>宏观态势、市场决策</td></tr><tr><td>网络质量分析</td><td>省/市级</td><td>排查区域性丢包、延迟</td></tr><tr><td>运维调度</td><td>市/区级</td><td>人员派单、仓储规划</td></tr><tr><td>合规/制裁判断</td><td>国家/地区级</td><td>是否落在受限区域</td></tr><tr><td>边缘节点规划</td><td>城市级</td><td>CDN/边缘计算部署</td></tr></tbody></table><p><strong>IoT并不追求“街道级定位”</strong> ，而是<strong>稳定、可批量、可解释的行政区级定位</strong>。<br/>这也是为什么在真实项目中，很多团队会优先选择基于 IP 的地理解析方案，而不是复杂的设备侧定位能力。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSqd" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析、.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析、.png" loading="lazy"/></p><h2>二、为什么物联网更适合用「IP离线库」，而不是在线接口？</h2><p>这是很多IoT团队在早期容易低估的一点。<br/>即便你已经验证过某些在线 IP 地址查询接口（比如在测试环境用过 <strong>IP数据云IP地址查询</strong>）。</p><h3>物联网的三个现实约束</h3><p><strong>① 数据量极大</strong></p><ul><li>设备数：几十万/几百万</li><li>日志规模：每天TB级</li><li>实时接口调用成本极高</li></ul><p><strong>② 网络环境复杂</strong></p><ul><li>专网/内网</li><li>边缘节点</li><li>海外或弱网环境</li></ul><p><strong>③ 稳定性和可控性优先</strong></p><ul><li>运维分析≠实时用户交互</li><li>离线可复现，比“快几毫秒”更重要<br/>这种背景下IP离线库几乎是IoT场景的解法</li></ul><h3>离线库在IoT场景的优势</h3><ul><li>批量解析（百万级IP无压力）</li><li>本地运行（无外部依赖）</li><li>结果可追溯（版本固定）</li><li>成本可控（一次部署，多次使用）<br/>适合：</li><li>日志回放</li><li>周/月度设备分布报告</li><li>异常区域复盘</li></ul><h2>三、IPv4/IPv6双栈是刚需</h2><h3>为什么 IoT 里 IPv6 占比越来越高？</h3><ul><li>设备数量爆炸，IPv4不够用</li><li>运营商网络天然支持IPv6</li><li>NB-IoT、5G、蜂窝网络大量走IPv6</li><li>海外部署（尤其亚太、欧洲）IPv6更常见</li></ul><p>现实很多IoT平台中，IPv6设备占比已经达到30%～50%。</p><h3>双栈支持在离线库中的技术含义</h3><p>一个合格的IP离线库，至少需要做到：</p><ul><li>同时支持IPv4/IPv6</li><li>统一输出结构（国家/省/市/ASN 等）</li><li>支持批量解析</li><li><p>不需要维护两套SDK、两套逻辑<br/>否则，在IoT场景中维护成本会变高。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnSr6" alt="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析1.png" title="物联网设备分布分析需要精准地理信息？IP离线库支持IPv4IPv6双栈批量解析1.png" loading="lazy"/></p><h2>四、在真实物联网系统中，IP地理数据通常怎么用？</h2><h3>典型流程示例</h3></li><li>设备上报日志（包含IP）</li><li>日志落库/对象存储</li><li>离线任务（Spark/Flink/MapReduce）</li><li>调用IP离线库做 <strong>批量解析</strong></li><li><p>生成：</p><ul><li>设备区域分布</li><li>国家/省级设备数量</li><li>区域异常告警</li><li>合规统计报表</li></ul></li></ul><h2>五、唠叨（给技术/产品都能用）</h2><ul><li>物联网设备分布分析需要地理信息，但不是“GPS 级”，而是<strong>稳定、可批量的行政区级精度</strong></li><li>IP离线库天然适合IoT大规模、离线、可复现的数据分析</li><li>IPv4/IPv6双栈批量解析，已经是物联网分析的<strong>基础能力，而不是可选项</strong></li><li><strong>IP数据云IP地址查询</strong>是IoT数据体系中的基础功能</li></ul>]]></description></item><item>    <title><![CDATA[电子签章为教育行业赋能 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047597567</link>    <guid>https://segmentfault.com/a/1190000047597567</guid>    <pubDate>2026-02-06 18:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>教育行业的电子签章需求正随着数字化转型的加速而日益凸显，它不仅是技术工具的应用，更是提升运营效率、保障合规性与优化用户体验的重要环节。我们就教育行业电子签章的核心需求、应用场景及实施要点进行一番浅析：</p><p>1．教育行业电子签章的核心需求</p><p>1) 流程高效化</p><p>Ø 减少纸质文件的打印、邮寄、存储成本，加快合同、证明等文件的签署周期。</p><p>Ø 实现远程签署，打破地域限制，适合在线教育、跨校区合作等场景。</p><p>2) 合规与法律效力</p><p>Ø 需符合《电子签名法》及相关教育法规要求，确保电子签章的法律效力。</p><p>Ø 学历证书、成绩单、录取通知书等重要文件需具备防篡改、可追溯的特性。</p><p>3) 安全与隐私保护</p><p>Ø 学生、教职工的身份信息及敏感数据需加密保护，防止泄露。</p><p>Ø 支持实名认证（如身份证、人脸识别），确保签署主体真实性。</p><p>4) 集成与兼容性</p><p>Ø 与现有教务系统、OA平台、学籍管理系统等无缝对接，避免数据孤岛。</p><p>Ø 支持多终端（PC、移动端）操作，适应多元化的使用场景。</p><p>5) 管理与审计需求</p><p>Ø 全流程留痕，便于追踪签署状态、时间、IP等信息。</p><p>Ø 教育机构需对签署文件进行统一归档与管理，满足审计要求。</p><p>2．典型应用场景</p><p>1) 招生与入学</p><p>Ø 在线报名表、录取通知书、入学协议的电子签署。</p><p>Ø 家长同意书（如课外活动、体检授权）的远程签署。</p><p>2) 教学与管理</p><p>Ø 成绩单、学历学位证书的电子签发与验证。</p><p>Ø 科研项目合同、学术合作协议的签署。</p><p>Ø 教职工劳动合同、保密协议等人事文件在线签署。</p><p>3) 学生事务</p><p>Ø 奖学金/助学金申请、实习协议、交换生项目的文件签署。</p><p>Ø 宿舍协议、校园安全责任书等后勤管理文件。</p><p>4) 合作与对外事务</p><p>Ø 与校企合作单位、供应商的合同签署。</p><p>Ø 学术论文投稿、知识产权协议等科研相关文件。</p><p>3．实施电子签章的关键要点</p><p>1) 选择合规可靠的服务商</p><p>确保服务商具备权威认证（如CA机构资质）、符合国密标准，并提供法律支持。</p><p>2) 定制化流程设计</p><p>针对不同文件类型（如录取通知 vs 劳动合同）设计差异化的签署流程与权限控制。</p><p>3) 用户培训与体验优化</p><p>针对教职工、学生、家长等不同用户群体提供操作指导，简化签署步骤。</p><p>4) 长期存证与司法服务</p><p>选择支持区块链存证、与公证机构对接的服务，增强文件的法律保障。</p><p>5) 安全与灾备方案</p><p>部署数据加密、防篡改技术，并建立文件备份与容灾机制。</p><p>4．挑战与趋势</p><p>1) 挑战：</p><p>Ø 传统教育机构对纸质文件的惯性依赖，需推动观念转变。</p><p>Ø 跨区域、跨国场景下的法律差异（如留学生文件需符合国际认可标准）。</p><p>Ø 老年家长或偏远地区用户的数字使用能力差异。</p><p>2) 趋势：</p><p>Ø AI融合：通过智能校验自动识别文件关键信息，减少人工审核。</p><p>Ø 区块链存证：学历证书等关键文件的防伪与全球验证。</p><p>Ø 生态整合：与智慧校园、数字孪生平台深度融合，形成全链路数字化管理。</p><p>5．实施路径</p><p>1) 需求调研：梳理校内高频签署场景，确定优先级（如从录取通知书开始试点）。</p><p>2) 方案选型：对比服务商的合规性、集成能力、成本及行业案例。如：北京安证通、契约锁、法大大等</p><p>3) 试点运行：选择单一部门或场景进行小范围试点，收集反馈并优化流程。</p><p>4) 全面推广：逐步扩大至全校范围，配套制定电子文件管理制度。</p><p>5) 持续优化：定期评估效率提升效果，关注技术更新与法规变化。</p><p>通过电子签章的规范化应用，教育机构可显著提升行政效率、降低运营成本，同时构建更安全、透明的数字化管理体系。如需进一步探讨具体场景的解决方案，可提供更多细节信息</p>]]></description></item><item>    <title><![CDATA[TDengine 2026 路线图来了：从 TSDB 到 IDMP，存储、分析与 AI 的下一步 T]]></title>    <link>https://segmentfault.com/a/1190000047597578</link>    <guid>https://segmentfault.com/a/1190000047597578</guid>    <pubDate>2026-02-06 18:10:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果说前几年，工业企业谈数据，更多是在解决“能不能采、能不能存”；那这两年，越来越多客户开始问的是另一类问题：</p><ul><li>数据规模上来之后，系统还能不能稳？</li><li>复杂分析越来越多，查询是不是一定会慢？</li><li>业务想用数据，但每次都要找技术同事，能不能更“自动”一点？</li><li>AI 说了这么多年，真正想落到工业场景里的，到底应该怎么做？</li></ul><p>这些问题，其实正是 TDengine 在规划 2026 年产品路线时反复讨论的出发点。</p><p>最近，我们正式对外发布了 <strong>TDengine TSDB &amp; TDengine IDMP 的 2026 年年度路线图</strong>。相比“多加几个功能”，这份路线图更想解决的是一件事：在真实、长期、复杂的工业数据场景里，系统如何继续向前演进。</p><h2>TDengine TSDB｜2026 年路线图</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597580" alt="" title=""/></p><p>从规划可以看到，TDengine TSDB 在 2026 年的重点，并不只是“更快”，而是<strong>让复杂场景变得可控</strong>。</p><p>一方面，查询能力持续向真实工业分析靠拢：关联查询、子查询、自然周期窗口、累计窗口、窗口函数……这些能力背后，都是越来越复杂的分析逻辑需求。</p><p>另一方面，虚拟表与流计算被反复强化，意味着计算正在前移：不再只是“数据进库 → 再算”，而是让系统本身承担更多实时与持续计算的职责。</p><p>而在更底层，引擎、缓存、多副本、资源管控的优化，则是在为<strong>长期稳定运行</strong>打基础。</p><h2>TDengine IDMP｜2026 年路线图</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597581" alt="" title="" loading="lazy"/></p><p>TDengine IDMP 于 2025 年 7 月正式发布。从一开始，它就不是一个“补充型工具”，而是围绕工业数据长期使用所设计的平台级产品。</p><p>在过去半年多的迭代中，IDMP 始终保持着“快迭代、小步快跑”的节奏：依托 TDengine TSDB 的高性能时序数据底座，持续强化工业数据的<strong>标准化管理与情景化分析</strong>，并在此基础上进一步拓展 <strong>AI 原生能力</strong>，让数据从“可管理”走向“可决策”。</p><p>这些更新更多聚焦在<strong>语义一致性、分析可复用性、视图沉淀与 AI 使用门槛</strong>等方面，为后续复杂场景与规模化落地打下稳定基础。</p><p>2026 年，IDMP 的演进重点开始从“能力补齐”转向“体系化建设”：</p><ul><li>在延续既有 AI 能力的基础上，引入更完整的事件体系与根因分析能力；</li><li>强化面板、仪表板与分析之间的组合、继承与钻取关系；</li><li>同时在平台层面补充可观测性、权限与数据治理能力，使分析与 AI 能力能够长期、稳定地运行在真实工业环境中。</li></ul><p>从 2025 到 2026，TDengine IDMP 正在从“能力集合”走向“可长期演进的工业数据平台”。</p><h2>写在最后</h2><p>工业数据的下一阶段不是“有没有数据”，而是系统能不能承载更复杂的分析、更长周期的运行，以及更高层次的智能应用。2026 年，TDengine 正在为这一阶段提前铺路。</p><p>如果你正在使用 TDengine，或正在评估下一代工业数据平台，这份路线图，或许能帮你更早看清接下来一年的演进方向。同时，我们也欢迎你基于真实场景和实际需求反馈建议，一起把这份 Roadmap 打磨得更加“落地”。</p>]]></description></item><item>    <title><![CDATA[全景解析 KaiwuDB 数据库智能体工具 KaiwuDB ]]></title>    <link>https://segmentfault.com/a/1190000047597601</link>    <guid>https://segmentfault.com/a/1190000047597601</guid>    <pubDate>2026-02-06 18:09:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>1. KAT 背景介绍</strong></h2><p>在 AI 技术飞速发展的当下，"让数据库更智能、更易用"成为行业核心探索方向，尤其国产数据库使用过程中，普遍存在学习成本高、运维流程复杂、特色功能上手难度大等痛点，各类手册文本繁杂，不利于用户快速落地使用。</p><p>针对这一现状，KaiwuDB 在 AI 与数据库融合领域持续深耕，形成了<strong>DB for AI</strong> 与 <strong>AI for DB</strong> 两大核心布局，既打造了适配多场景的预测分析引擎，又推出了 <strong>KAT - KaiwuDB 数据库智能体工具</strong>，构建起完整的数据库 AI 赋能体系，其中 KAT 作为 AI for DB 领域的核心成果，重点解决用户操作、运维、研发中的各类痛点。</p><p>本期直播核心围绕数据库智能体工具 KAT 展开，全面拆解其背景价值、架构功能及实操效果，为 DBA、研发工程师、数据科学家等技术从业者，提供 AI 与数据库融合的全新解决方案，助力降低数据库使用门槛、提升全流程工作效率。</p><h2><strong>2. KAT 架构和功能</strong></h2><h3><strong>2.1 KAT 核心架构</strong></h3><p>KAT 采用先进的 <strong>Multi-Agent（多智能体）架构</strong>，规避单 Agent 系统处理复杂任务时的效率低、准确性不足等短板，通过"分工协同、各司其职"的设计，实现复杂任务的高效拆解与落地。</p><p><strong>• Main Agent（主智能体）</strong>：作为核心调度中枢，负责接收用户请求、识别核心需求、拆解复杂任务，并分配给对应 Subagent，同时监控任务执行进度、整合最终结果。</p><p><strong>• Subagent（子智能体）</strong>：具备独立决策与执行能力，聚焦特定任务类型，包括 NL2SQL 转换、性能分析、数据分析、安装部署、知识库管理、故障诊断等，通过多轮迭代完成复杂需求。</p><p><strong>• 核心组件</strong>：包含 Agent UI、Agent Server、Task Manager 三大组件，Agent UI 提供图形化交互与配置能力，Agent Server 以 RESTful API 形式提供 Agent 功能，Task Manager 支持定时任务与 Webhook 通知。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597603" alt="" title=""/><br/>KAT 架构图</p><h3><strong>2.2 KAT 核心功能特性</strong></h3><p>KAT 具备五大核心功能特性，全面覆盖数据库操作、运维、分析全流程，大幅提升工作效率：</p><p><strong>•自然语言交互</strong>：用户可通过对话完成各类数据库相关任务，无需掌握复杂操作指令。</p><p><strong>• 智能问题诊断</strong>：快速定位 KaiwuDB 使用过程中的问题，提供精准解决方案。</p><p><strong>• 性能调优</strong>：依托 KaiwuDB 专家知识，针对性优化数据库性能，提升运行稳定性。</p><p><strong>• 自动化任务</strong>：支持定时巡检、备份、报表生成等运维任务，简化日常工作。</p><p><strong>• 数据管理与分析</strong>：支持自然语言查询、趋势预测及可视化展示，让分析结果更直观。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597604" alt="" title="" loading="lazy"/></p><p>KAT 功能特性</p><h3><strong>2.3 KAT 针对不同角色的赋能</strong></h3><p><strong>• DBA</strong>：提供故障预防、巡检自动化、智能告警、部署自动化等能力，解放重复劳动，聚焦高价值工作。</p><p><strong>• 研发工程师</strong>：支持自然语言生成 SQL、辅助业务设计、快速熟悉业务逻辑，大幅提升研发效率。</p><p><strong>• 数据科学家</strong>：提供智能数据预处理、分析预测、结果可视化等支撑，助力高效挖掘数据价值。</p><h2><strong>3. KAT 相关演示</strong></h2><p>视频演示详见：<a href="https://link.segmentfault.com/?enc=Cu28Em9Q2gko29KATyK6LQ%3D%3D.AMNPUHIrNjM9xseER9bWzVwg%2BMN3xiwJUxRwjMgLTYmWjvbiN8cD9dDr9kVU9shG00E2Y%2FCqf69p8wjAN17Zag%3D%3D" rel="nofollow" title="全景解析 KaiwuDB 数据库智能体工具" target="_blank">全景解析 KaiwuDB 数据库智能体工具</a></p>]]></description></item><item>    <title><![CDATA[网络攻击的“集中靶区” JoySSL解析如何以数字证书为行业构筑安全防御屏障 完美的铁板烧 ]]></title>    <link>https://segmentfault.com/a/1190000047597614</link>    <guid>https://segmentfault.com/a/1190000047597614</guid>    <pubDate>2026-02-06 18:08:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着数字化进程加快，各行各业的运营方式、服务模式以及核心资源正快速向数字领域迁移。这一重大变革改变了风险的布局，网络攻击不再是随机分布，而是如精准制导武器般，集中瞄准那些拥有高数据价值、业务中断影响大的行业领域，或是安全防御较为薄弱的目标。识别这些重点目标的特征及其弱点，不仅是理解风险的关键，更是一种基础性的威胁应对策略。JoySSL 有关专家指出，通过全面的行业分析可以得出结论，网络攻击的针对性让安全防护的基础设施重要性再次凸显。而数字证书的作用，早已超越了为网站提供加密保障的单一功能。在应对针对性极强的网络威胁时，能够凭借先进的加密技术与身份验证系统，有效抵御网络攻击，逐渐成为各行业在数字架构中构建“基础通信安全免疫”的必备工具，同时也随着数字威胁的升级，发挥着日益重要的作用，在现阶段不可或缺。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnStz" alt="" title=""/></p><p><strong>SSL证书应对网络攻击的行业偏好</strong></p><p>金融领域是数字经济的“金库”，极易遭受黑客网络攻击，包括勒索软件、供应链攻击、API数据泄露等。SSL证书保护通信安全，确保数据在传输时不被非法截取。医疗领域则是存储个人健康信息的“保险库”，数据在黑市价格极高。数字证书保障医疗数据的绝对安全，避免因钓鱼攻击导致泄露。</p><p>电子商务是海量交易与消费者数据的关键平台，业务高度数字化，是不法分子的高度关注对象。SSL证书通过加密通信保护用户登录和支付环节的隐私安全，增强支付页面的信誉，平衡安全性与业务增长需求。</p><p><img width="723" height="475" referrerpolicy="no-referrer" src="/img/bVdnStA" alt="" title="" loading="lazy"/></p><p>教育领域则被称为开放网络中的“知识存储库”，拥有海量的学生及教师个人信息，包含创新性研究成果和知识产权数据。数字证书保护网络教学平台及科研数据资源的访问安全，为开放式学术环境建立传输通信的安全基准，维护知识产权及个人隐私。</p><p><strong>数字证书共通价值直击行业痛点</strong></p><p>即使各行各业面临的安全威胁各不相同，SSL证书的技术解决路径却能精准解决共同的基础安全难题。无论是金融、医疗、电商还是教育领域，均以“加密传输”和“身份溯源”为核心基础，满足多项法律法规的要求，为数字化合法经营提供技术保障。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnStB" alt="" title="" loading="lazy"/></p><p>JoySSL技术专家解释道，借助SSL证书的强加密技术及服务器身份验证功能，可在通信阶段设置防护屏障，确保数据安全传输，提升钓鱼攻击难度。凭借安全类标识，建立品牌信任资产，提升企业竞争优势。</p><p><strong>建立可信基础抵御不确定网络威胁</strong></p><p>在数字化发展的过程中，安全问题既存在又分布不均。应对风险的关键，在于为所有数字交互构建基础且广泛的信任基石。虽然无法针对特定威胁，但却是行业稳定运作不可或缺的基本条件。</p>]]></description></item><item>    <title><![CDATA[专访麦斯时代刘剑锋：时序数据库 TDengine 钻石级合作背后，是一次长期路线判断 TDengin]]></title>    <link>https://segmentfault.com/a/1190000047597616</link>    <guid>https://segmentfault.com/a/1190000047597616</guid>    <pubDate>2026-02-06 18:07:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业数字化进入深水区之后，越来越多企业开始意识到一个问题：真正限制系统上限的，往往不是应用功能，而是底层对<strong>时序数据</strong>的处理能力。</p><p>设备、工艺、能耗、安全、环保——这些最核心的数据形态，几乎全部以高频、连续、长期积累的方式存在。它们不是“报表型数据”，而是贯穿生产全过程的运行数据。一旦规模上来，传统数据库方案很容易在性能、成本或稳定性上同时失效。</p><p>正是在这样的背景下，麦斯时代与涛思数据达成了 <strong>TDengine 钻石级分销商</strong>合作。围绕这次选择背后的判断逻辑，我们与麦斯时代总经理刘剑锋（Jeff）进行了一次深入交流。</p><h2>不只是技术选型，而是长期路线判断</h2><p>在刘剑锋看来，当前时序数据市场的变化，并不是简单的“技术风口”，而是工业系统结构性变化的结果。</p><p>“在工业现场，几乎所有关键决策，都越来越依赖对运行数据的持续分析。但很多系统在设计之初，并没有为这种规模和频率的数据做好准备。”</p><p>在他过往参与的大型工业互联网项目中，这种矛盾反复出现：一方面，设备数量和采集频率不断上升；另一方面，数据系统却难以支撑长期、稳定、低成本运行，最终影响的是业务系统本身。</p><p>这也是麦斯时代判断<strong>时序数据库将成为工业数字化底层公共能力</strong>的核心原因。</p><p>在国内外众多时序数据库厂商都在布局渠道合作的背景下，麦斯时代最终选择 TDengine，并直接以<strong>最高级别钻石合作</strong>切入，这也并非偶然。</p><p>刘剑锋将核心原因归结为三个关键词：<strong>性能、成本与生态匹配度</strong>。“我们服务的工业场景，对读写性能和长期存储成本都极其敏感。TDengine 在这两点上的优势非常突出，而且不是通过复杂架构堆出来的，这对实际交付非常重要。”</p><p>但技术也并不是唯一因素。“对我们来说，生态合作的确定性同样重要。”刘剑锋表示，“TDengine 的合作伙伴体系不是停留在口头层面，而是把转售、市场、客户拓展等关键环节的权益与支持方式都提前明确下来。尤其是在钻石级别的合作机制下，无论是市场资源协同、客户拓展支持，还是长期能力共建的空间，都具备清晰预期，这让我们敢于在方案、团队和市场上做长期投入。”</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597618" alt="" title=""/></p><p>这一合作背后，其实是一次对长期路线稳定性的判断。</p><h2>先打穿一个行业，再谈规模复制</h2><p>在落地策略上，麦斯时代并没有选择多行业同时推进。刘剑锋坦言，不同行业对时序数据的关注重点，本身就决定了落地路径不可能“一刀切”。</p><p>在矿山和冶炼行业，设备连续运行时间长、工况复杂，对设备可靠性和运行稳定性的要求极高；化工行业更关注安全与环保指标的持续监测；而汽车制造场景中，工艺一致性和质量追溯能力往往是核心诉求。<strong>关注点不同，也意味着数据模型、采集频率和分析方式存在明显差异。</strong>“如果没有对行业运行逻辑的长期理解，很容易只是在‘换一个数据库’，却解决不了真正的问题。”刘剑锋表示。</p><p>基于这一判断，麦斯时代选择优先在矿山和冶炼行业推动 TDengine 落地。这些行业不仅是麦斯时代积累最深、客户痛点最集中的领域，也更容易在真实生产环境中验证方案的稳定性和可复制性。</p><p>在具体实践中，这类场景往往同时面临<strong>高并发数据写入、海量历史数据长期存储，以及对实时分析响应的刚性需求</strong>。“我们会把 TDengine 作为核心数据底座，嵌入到现有的生产管理、能源管理、设备健康管理解决方案中，替代传统数据库模块。这不仅能极大缓解客户的数据存储压力，还能提升实时分析效率，为客户的精细化分析和 AI 技术落地提供优秀的数据底座。”</p><p>谈及合作首年的规划，麦斯时代并没有给出激进的扩张目标。“第一年最重要的不是铺量，而是把样板工程跑通。”刘剑锋强调。</p><p>按照规划，麦斯时代希望在 <strong>3–5 个重点行业</strong>中落地标杆项目，覆盖 <strong>20 家以上新客户</strong>，并在此基础上形成标准化、可复制的“TDengine + 麦斯时代”解决方案模型。“只要样板工程成立，后续的行业复制是自然发生的。”</p><h2>不止是分销商，而是能力延伸者</h2><p>作为一家深耕工业数字化 16 年的解决方案提供商，麦斯时代并不把自己简单定位为产品代理方。在刘剑锋看来，工业数字化项目的成败，往往不取决于某一项单点技术，而取决于技术是否真正融入业务流程，能否在长期运行中稳定发挥作用。</p><p>“我们不是把 TDengine 简单当作一个‘卖点’，而是要把它真正嵌入到客户的业务闭环里。”</p><p>这意味着，TDengine 在麦斯时代的解决方案中，并不是一个独立存在的数据库组件，而是与生产管理、能源管理、设备运维等业务系统一起，被纳入整体架构设计和交付体系之中。</p><p>在这种模式下，麦斯时代的优势也不再体现在某一项单点技术能力上，而是来自三方面的协同：对行业运行逻辑的长期理解、完整的解决方案交付能力，以及服务头部工业客户过程中积累的工程经验。只有同时具备这三点，数据库能力才能在真实工业场景中被“用起来”，而不是停留在技术选型层面。</p><p>也正因如此，刘剑锋将“钻石分销商”视为一种责任导向的角色，而不仅是权益层面的合作级别。“钻石分销商对我们来说，既是一种权益保障，更是一份责任承诺，<strong>我们要成为 TDengine 在工业领域的技术延伸和价值传递者，帮客户把能力用起来、跑稳定。</strong>”</p><p>从更长周期看，他将这次合作理解为一次<strong>工业应用生态与时序数据库能力的深度融合实践</strong>。“未来，我们希望与 TDengine 在行业数据模型、联合解决方案等层面展开更深入的协同，逐步沉淀出适合特定行业的通用方法和实践路径，甚至参与相关行业标准的探索与共建。”</p><p>他也对 TDengine 的后续演进提出了期待：“如果能在工业场景中提供更多开箱即用的适配能力，以及更贴近端侧的新产品形态，将显著提升整体交付效率。此外，我们也希望未来 TDengine 能开放更多联合方案推广与培训资源，助力生态伙伴提升交付能力。”</p><h2>结语</h2><p>在工业数字化的真实世界里，技术并不缺，缺的是<strong>能长期跑、敢规模用的底层能力</strong>。对麦斯时代而言，与 TDengine 的合作，并不是一次简单的渠道合作，而是一次围绕“工业时序数据底座”的长期选择。</p><p>正如刘剑锋最后总结的那样：“麦斯时代携手 TDengine，希望让工业数据不只是被采集，而是真正成为企业持续进化的基础能力。”</p><h2>关于麦斯时代</h2><p>北京麦斯时代信息技术有限公司深耕工业物联网、低代码平台与数据管理解决方案，构建了“1平台 + N应用 + M模型”的全栈数字化服务体系，服务于有色金属、汽车制造、能源化工等多个行业的头部客户，具备千万级智能工厂项目的实施经验与深厚行业积累。</p><h2>关于采访人</h2><p>刘剑锋（Jeff），麦斯时代 COO，拥有二十余年工业数字化领域经验，曾任职于施耐德电气、西门子和第四范式等头部企业。他主导过多项大型工业互联网项目落地，擅长技术与业务的融合推进，致力于通过生态合作与技术创新，推动工业企业的数字化转型与价值提升。</p>]]></description></item><item>    <title><![CDATA[五款客户、销售管理系统对比，2026CRM选型核心手册 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047597622</link>    <guid>https://segmentfault.com/a/1190000047597622</guid>    <pubDate>2026-02-06 18:07:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在数字化时代，企业的核心竞争力已从“单一环节效率”转向“全链路协同能力”——<strong>采购-库存-产品-物流-回款</strong>的闭环管理，直接决定了企业的成本控制、交付效率与客户满意度。然而，不同品牌的供应链能力差异巨大：有的聚焦销售端，有的侧重进销存，有的覆盖全链路。本文基于<strong>核心能力深度拆解+关键场景适配性</strong>，对5类主流品牌进行横向对比，为企业选择提供专业参考。</p><h2>一、对比框架与核心维度</h2><p>本次对比围绕<strong>供应链全链路的5大关键环节</strong>，聚焦“<strong>原生功能深度</strong>”“<strong>全链路协同能力</strong>”“<strong>集成复杂度</strong>”三大核心维度，覆盖以下品牌：</p><ol><li><strong>超兔一体云</strong>：全链路一体化云平台（覆盖采购-库存-物流-回款）；</li><li><strong>Freshworks</strong>：销售与客户服务核心（依赖第三方集成补全供应链）；</li><li><strong>Highrise</strong>：CRM停止新用户，ERP提供基础库存管理；</li><li><strong>管家婆</strong>：进销存一体化（适配商贸企业）；</li><li><strong>轻量CRM</strong>（如Less Annoying CRM、微盟CRM）：聚焦客户管理（无供应链原生能力）。</li></ol><h2>二、各环节能力深度对比</h2><h3>（一）采购管理：从“需求预测”到“供应商闭环”</h3><p>采购是供应链的起点，核心能力是<strong>精准需求预测</strong>与<strong>供应商全生命周期管理</strong>。</p><table><thead><tr><th>品牌</th><th>核心能力</th><th>优劣势总结</th></tr></thead><tbody><tr><td>超兔一体云</td><td>1. 智能需求预测（销售订单+生产工单+库存缺口联动，支持多订单整合）； 2. 供应商全生命周期管理（评级雷达图、询价比价、三流合一对账）； 3. 采购订单全流程（一键生成、供应商直发、实时跟踪）。</td><td>优势：全链路需求联动，供应商管理闭环； 劣势：暂无（适配生产/商贸企业）。</td></tr><tr><td>Freshworks</td><td>无原生采购功能，需集成第三方ERP（如NetSuite）实现采购流程。</td><td>优势：销售端可查看采购进度（集成后）； 劣势：采购全流程依赖外部系统，数据割裂。</td></tr><tr><td>Highrise ERP</td><td>基础采购订单管理（手动录入需求，简单供应商信息存储）。</td><td>优势：ERP原生采购流程； 劣势：无智能需求预测，供应商管理功能薄弱。</td></tr><tr><td>管家婆</td><td>支持采购订单、询价、验收，但需求预测依赖手动。</td><td>优势：商贸企业采购流程覆盖； 劣势：智能需求能力弱，无法联动生产。</td></tr><tr><td>轻量CRM</td><td>无原生采购功能，需手动记录采购信息。</td><td>优势：暂无； 劣势：完全依赖外部系统，无采购流程管理。</td></tr></tbody></table><h3>（二）库存/备货：从“实时监控”到“智能补货”</h3><p>库存/备货的核心是<strong>平衡库存成本与交付效率</strong>，关键能力是“实时监控”“智能预警”与“备货联动”。</p><h4>1. 超兔的库存/备货流程（Mermaid流程图）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597624" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><h4>2. 各品牌能力对比</h4><table><thead><tr><th>品牌</th><th>核心能力</th><th>优劣势总结</th></tr></thead><tbody><tr><td>超兔一体云</td><td>1. 实时库存监控（多级分类、多成本算法：先进先出/加权平均/手工指定）； 2. 智能预警（库存上下限、备货缺口）； 3. 库存作业（盘点、调拨、BOM联动生产）。</td><td>优势：全场景库存覆盖，备货联动采购需求； 劣势：暂无。</td></tr><tr><td>Freshworks</td><td>销售端显示库存状态（集成第三方库存系统），无原生预警或备货功能。</td><td>优势：销售与库存状态联动； 劣势：备货进度需手动同步，无智能预警。</td></tr><tr><td>Highrise ERP</td><td>基础库存跟踪（库存数量更新）、自动补货（手动设置阈值）。</td><td>优势：ERP原生库存管理； 劣势：无法关联销售订单，补货逻辑简单。</td></tr><tr><td>管家婆</td><td>库存预警（上下限）、出入库管理、对接仓储系统。</td><td>优势：商贸企业库存流程覆盖； 劣势：BOM管理弱，无法支持生产型企业备货。</td></tr><tr><td>轻量CRM</td><td>无原生库存功能，需集成第三方工具同步库存数据。</td><td>优势：暂无； 劣势：无法实时监控库存，无备货能力。</td></tr></tbody></table><h3>（三）产品库存：从“BOM管理”到“销售联动”</h3><p>产品库存的核心是<strong>产品数据精细化</strong>与<strong>销售-库存闭环</strong>，适配生产型企业的BOM管理是关键。</p><h4>1. 超兔的产品库存能力脑图</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597625" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><h4>2. 各品牌能力对比</h4><table><thead><tr><th>品牌</th><th>核心能力</th><th>优劣势总结</th></tr></thead><tbody><tr><td>超兔一体云</td><td>1. BOM/套餐/租赁/非标产品支持； 2. 销售-库存联动（锁库、实时扣减）； 3. 多维度分析（销量、毛利、自定义查询）。</td><td>优势：生产/商贸产品全覆盖，分析能力强； 劣势：暂无。</td></tr><tr><td>Freshworks</td><td>集成第三方库存系统（如Zoho Inventory）同步产品数据，无BOM或非标支持。</td><td>优势：销售端显示产品库存； 劣势：无法管理复杂产品（如BOM、租赁）。</td></tr><tr><td>Highrise ERP</td><td>产品分类管理，无BOM或套餐支持，库存变动手动更新。</td><td>优势：基础产品数据管理； 劣势：无法适配生产型企业，分析能力弱。</td></tr><tr><td>管家婆</td><td>商贸产品库存管理（多价格策略），无BOM支持。</td><td>优势：商贸产品流程覆盖； 劣势：生产型企业产品管理能力不足。</td></tr><tr><td>轻量CRM</td><td>无原生产品库存功能，需手动录入产品信息。</td><td>优势：暂无； 劣势：无法关联销售与库存，产品数据零散。</td></tr></tbody></table><h3>（四）发货物流跟踪：从“流程自动化”到“客户同步”</h3><p>发货物流的核心是<strong>流程可控</strong>与<strong>客户透明化</strong>，关键能力是“物流系统集成”与“状态同步”。</p><table><thead><tr><th>品牌</th><th>核心能力</th><th>优劣势总结</th></tr></thead><tbody><tr><td>超兔一体云</td><td>1. 发货流程自动化（分批发货、审批、关联销售订单）； 2. 物流集成（实时跟踪、客户同步）； 3. 收货确认闭环（异常反馈、库存更新）。</td><td>优势：全流程自动化，客户透明化； 劣势：暂无。</td></tr><tr><td>Freshworks</td><td>集成第三方物流平台（如FedEx）获取跟踪信息，无法在系统内完成发货流程。</td><td>优势：客户可查物流状态； 劣势：发货流程需跳转到外部系统，体验割裂。</td></tr><tr><td>Highrise ERP</td><td>基础发货单管理，物流跟踪需手动录入。</td><td>优势：ERP原生发货流程； 劣势：无物流集成，客户无法同步状态。</td></tr><tr><td>管家婆</td><td>发货单、分批发货，对接快递接口（如菜鸟）跟踪物流，客户可查信息。</td><td>优势：商贸企业物流覆盖； 劣势：无法支持复杂物流（如多仓调拨）。</td></tr><tr><td>轻量CRM</td><td>无原生物流功能，需手动记录发货信息。</td><td>优势：暂无； 劣势：无法跟踪物流，客户体验差。</td></tr></tbody></table><h3>（五）回款管理：从“应收触发”到“财务闭环”</h3><p>回款是供应链的终点，核心能力是<strong>应收智能触发</strong>与<strong>财务协同</strong>。</p><table><thead><tr><th>品牌</th><th>核心能力</th><th>优劣势总结</th></tr></thead><tbody><tr><td>超兔一体云</td><td>1. 应收智能触发（签约/开票/发货多规则，自动拆分多期）； 2. 回款闭环（核销、信用管理、财务对账）； 3. 超发控制（关联客户信用额度）。</td><td>优势：全链路应收联动，财务闭环； 劣势：暂无。</td></tr><tr><td>Freshworks</td><td>销售端关联合同与信用额度，应收提醒，但回款确认需集成财务软件（如Zoho Books）。</td><td>优势：销售与信用联动； 劣势：回款闭环依赖外部系统，无财务核销功能。</td></tr><tr><td>Highrise ERP</td><td>基础应收提醒，回款手动录入，无财务集成。</td><td>优势：ERP原生应收管理； 劣势：无法闭环，财务对账麻烦。</td></tr><tr><td>管家婆</td><td>自动生成回款单，关联订单，支持财务对账，但智能应收触发能力弱。</td><td>优势：商贸企业回款流程覆盖； 劣势：无法支持多期应收或信用控制。</td></tr><tr><td>轻量CRM</td><td>手动关联订单与回款，无应收触发或财务集成。</td><td>优势：暂无； 劣势：回款管理零散，无法管控风险。</td></tr></tbody></table><h2>三、全链路协同能力对比</h2><p>供应链的核心价值是<strong>全链路数据打通</strong>，避免“信息孤岛”。以下用<strong>流程图</strong>展示超兔的全链路协同逻辑：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597626" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><p>对比各品牌的全链路协同能力：</p><ul><li><strong>超兔一体云</strong>：全链路闭环，数据实时同步（销售→采购→库存→物流→回款）；</li><li><strong>Freshworks</strong>：销售端与供应链断开，需集成3+系统（ERP+SCM+财务）；</li><li><strong>Highrise</strong>：ERP与CRM断开，全链路数据无法联动；</li><li><strong>管家婆</strong>：进销存闭环，但无法联动生产或复杂财务；</li><li><strong>轻量CRM</strong>：无全链路能力，完全依赖外部系统。</li></ul><h2>四、综合能力雷达图（10分制）</h2><p>选取<strong>全链路覆盖度</strong>“原生功能完善度”“集成复杂度”“行业适配性”“成本可控性”5大指标，各品牌得分如下：</p><table><thead><tr><th>指标</th><th>超兔一体云</th><th>Freshworks</th><th>Highrise</th><th>管家婆</th><th>轻量CRM</th></tr></thead><tbody><tr><td>全链路覆盖度</td><td>10</td><td>2</td><td>5</td><td>7</td><td>1</td></tr><tr><td>原生功能完善度</td><td>10</td><td>3</td><td>6</td><td>8</td><td>2</td></tr><tr><td>集成复杂度（低→高）</td><td>2</td><td>8</td><td>7</td><td>3</td><td>1</td></tr><tr><td>行业适配性</td><td>9</td><td>6</td><td>5</td><td>8</td><td>3</td></tr><tr><td>成本可控性</td><td>8</td><td>7</td><td>6</td><td>9</td><td>10</td></tr></tbody></table><h2>五、适用场景建议</h2><ol><li><strong>全链路需求企业</strong>（生产/全渠道零售）：选<strong>超兔一体云</strong>（覆盖采购-库存-物流-回款，无需集成）；</li><li><strong>销售驱动型企业</strong>（聚焦客户转化）：选<strong>Freshworks+ERP集成</strong>（销售与客户服务强，供应链依赖集成）；</li><li><strong>商贸企业</strong>（进销存需求）：选<strong>管家婆</strong>（原生进销存，成本低）；</li><li><strong>轻量客户管理</strong>（无供应链需求）：选<strong>Less Annoying CRM</strong>（成本低，操作简单）；</li><li><strong>Highrise现有客户</strong>：继续使用其ERP的基础库存功能，但需补全采购与回款的集成。</li></ol><h2>结论</h2><ul><li><strong>超兔一体云</strong>是<strong>全链路供应链管理的最优解</strong>，覆盖从采购到回款的所有环节，原生功能完善，无需复杂集成；</li><li>Freshworks适合<strong>销售驱动型企业</strong>，但需搭配ERP/SCM系统补全供应链；</li><li>管家婆是<strong>商贸企业的高性价比选择</strong>，但无法支持生产型企业的复杂需求；</li><li>轻量CRM仅适合<strong>无供应链需求的小微企业</strong>，需手动管理供应链流程。</li></ul><p>企业选择时需优先匹配<strong>核心需求</strong>：若需全链路协同，选超兔；若需销售与客户服务，选Freshworks；若需进销存，选管家婆。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[工业AI平台不是工具，而是企业的新生产力——该怎么理解？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047597630</link>    <guid>https://segmentfault.com/a/1190000047597630</guid>    <pubDate>2026-02-06 18:06:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的转型浪潮中，“工业AI”这个词早已不新鲜。但真正让人困惑的，不是它有多先进，而是为什么那么多企业投入重金，却依然看不到实质性的效率提升。问题的根源，往往不在于算法不够复杂，而在于平台本身是否真正理解工厂的呼吸节奏——那些藏在设备振动、工艺参数、排产冲突背后的隐性知识。工业AI平台的真正价值，不在于它能跑通多少个模型，而在于它能否把数据、流程、人和机器编织成一个能自主思考、持续进化的系统。<br/>要理解这一点，首先得跳出“AI=自动化”的简单思维。过去，企业部署的智能系统大多是孤立的“烟囱”，质量检测用一套软件，排产用另一套，能源管理又是独立模块，数据彼此不通，决策各自为政。真正的工业AI平台，必须是一个能打通全链路的“神经系统”。它不仅要能采集数据，更要能清洗、标准化、封装成可被AI理解的“语言”，并把工艺专家几十年的经验，转化为可复用的数字知识库。这不是技术堆砌，而是一场对工业逻辑的深度重构。平台必须成为企业的“数字大脑”，让AI不再只是执行指令的工具，而是能主动感知异常、推演方案、甚至预判风险的“数字员工”。<br/>更进一步，工业AI的终极形态，是智能体的协同网络。单个AI模型再强大，也难以应对制造现场的复杂性。真正的突破，来自于多个“超级智能体”之间的默契配合——一个负责排产，一个监控库存，一个分析焊点质量，另一个动态调整能耗。它们不是各自为战，而是在统一平台上实时对话、协同决策。当一条生产线突发故障，不是等人工排查，而是多个智能体在几分钟内自动重组资源、调整计划、通知供应商，整个过程像一支训练有素的乐队，无需指挥，自然合拍。这种能力，才是从“数字化”迈向“智能化”的分水岭。<br/>在这一领域，广域铭岛的Geega平台提供了一个极具参考价值的中国样本。它没有盲目追求大模型的参数规模，而是深耕汽车制造的每一个痛点：在领克成都工厂，其“质量归因助手Agent”将问题分析时间从数小时压缩到分钟级，焊接虚焊率直降90%；在排产环节，智能体能在15分钟内完成传统需数天的多约束优化，年节省成本超千万元。更难得的是，它把AI能力封装成“开箱即用”的积木模块，让一线员工也能快速搭建专属智能体，真正实现了“AI下沉到岗位”。<br/>放眼全球，西门子的MindSphere平台以工业物联网为基，强调设备互联与数据中台，适合重资产、高标准化的制造场景；而通用电气的Predix则更侧重于航空、能源等复杂系统的预测性维护，其优势在于对高价值设备寿命模型的深度建模。<br/>工业AI平台的未来，不属于那些喊着“颠覆”的喧嚣者，而属于那些愿意蹲在车间里，听老师傅抱怨“这台机器最近总出问题”的人。真正的智能，不是炫技，是让每一个微小的改进，都成为企业活下去的底气。</p>]]></description></item><item>    <title><![CDATA[双 11 大促峰值不翻车：淘天集团 Paimon + StarRocks 大规模 OLAP 查询实战]]></title>    <link>https://segmentfault.com/a/1190000047597646</link>    <guid>https://segmentfault.com/a/1190000047597646</guid>    <pubDate>2026-02-06 18:05:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：朱奥（盖可）/ 淘天集团高级数据工程师</p><h2>导读：</h2><p>双 11 等大促场景会在短时间内集中爆发：运营与业务 BI 在开卖后的窗口期密集访问数据产品，瞬时请求量陡增，对查询引擎的稳定性、成本与治理体系提出极高要求。与此同时，业务对近实时数据产品的诉求持续增强，传统“多存储、多链路、依赖回刷”的模式在研发效率、回刷成本与响应速度上逐步暴露瓶颈。</p><p>本文围绕 Paimon 与 StarRocks 的组合实践，梳理淘天在大规模 OLAP 查询场景下的架构演进与双 11 保障体系：通过实时与离线统一入湖，消除数据同步链路与多份存储成本；基于稳定中间层叠加在线现算与维表实时关联，将高消耗回刷转化为秒级查询，核心场景回刷效率提升约 80%，年化节省成本接近 1000 万；同时结合 StarRocks + RoaringBitmap 低成本解决跨天交叉实时 UV 计算难题，满足大促近实时决策需求。</p><h2>1、淘天集团营销活动 OLAP 查询的探索背景与核心策略</h2><h3>1.1 业务诉求与核心痛点当前数据架构</h3><p><img width="723" height="348" referrerpolicy="no-referrer" src="/img/bVdnStJ" alt="5a7bb2fdd2234116b9ffa58f4a5aa606.png" title="5a7bb2fdd2234116b9ffa58f4a5aa606.png"/></p><p>首先，简要介绍当前的数据架构与数据流转方式。</p><p>从 DWD 层开始，我们的数据分为实时与离线两条主链路：</p><ul><li>实时数据主要存储在 TT中，在业界可类比为 Kafka 一类的消息队列；</li><li>离线数据主要存储在 ODPS 中。</li></ul><p>在数据加工与写入层面，我们会启动 Flink 流批一体任务：</p><ul><li>实时侧持续消费 TT 中的数据；</li><li>离线侧消费 ODPS 中的数据；</li><li>在计算过程中，任务会关联多类 ODPS 维表，例如类目维表、商家分层等维度信息。</li><li>计算完成后，结果统一写入 ADS 层的 Holo 表中，并在数据服务层对外透出。</li></ul><p>在纯离线场景下，我们会通过 ODPS 任务读取 ODPS 数据，同时写入 ADS 层对应的 ODPS 表。这里既包含历史天级数据，也包含历史小时级数据。当存在查询加速需求时，我们还会将 ODPS 数据进一步导入到 Holo 中。</p><p>在数据服务层，我们主要通过 Holo 或 MC 对外提供数据服务。我们会根据查询时延要求选择不同的服务路径：当业务对响应速度要求更高、需要达到毫秒级时，通常通过 Holo 提供查询服务；当时延要求相对宽松，例如百毫秒级或秒级，则更多通过 MC 来承载查询请求。</p><h3>1.2 业务诉求与核心痛点</h3><p>随着业务发展，我们当前面临的诉求主要来自两个方向：一是业务侧希望获得更多实时数据产品；二是业务 BI 的实时分析需求持续增长。这对数据研发提出了新的挑战：进一步提升研发效率。</p><p>回到现有架构，其核心痛点主要体现在两方面：</p><ul><li><strong>流批存储不统一</strong>：实时数据存储在 TT 中，离线数据存储在 ODPS 中；当存在查询加速需求时，部分数据还需要进一步落到 Holo 表中。</li><li><strong>整体开发架构较为复杂</strong>：数据需要在多个存储介质之间流转，导致端到端链路拉长。</li></ul><p>在查询特性上，Holo 在点查场景具备更突出的性能优势，且整体稳定性较强，在淘天历年大促期间的表现也相对稳定。</p><p>但在更常见的 Shuffle 场景下，整体查询性能相对一般。尤其当 OLAP 查询负载更重、需要进行更复杂的计算，或需要关联规模较大的维表时，Shuffle 相关的执行效率会成为瓶颈，导致查询耗时明显拉长。</p><p>数据更新与维护上也存在较高成本。以 ODPS 中的维表为例（如类目维表、商家分层维表等），当维表发生业务变更时，往往需要触发 ADS 层任务的回刷，从而带来额外的回刷开销。</p><p>业务对“近实时”的诉求在部分场景下出现了被动降级。例如在跨天实时 UV 等场景中，由于 state 规模较大、成本较高等原因，方案不得不从实时级别降级到小时级别。从业务视角看，近实时能力仍然是明确存在的需求。</p><h3>1.3 核心策略</h3><p><strong>1）架构简化提效</strong></p><ul><li>架构上实现<strong>存储介质的统一</strong>：将实时与离线数据统一沉淀到 Paimon 的湖存储中。在此基础上，<strong>StarRocks 可以直接面向湖存储进行高性能分析查询</strong>，从而能够消除数据同步链路以及多份存储带来的成本。</li><li><strong>降低使用门槛</strong>，让数据更容易被上层分析与 BI 使用。以实时链路为例，原本实时数据存储在 TT 中，而 TT 的数据形态具有明显特征：每行数据是一个字符串、缺少 schema。在这种形态下，数据虽然可以被消费，但如果要面向 BI 分析使用，往往还需要额外进行反序列化与解析，这会带来不可忽视的工程与使用成本。</li></ul><p>在统一存储之后，Paimon 将实时与离线数据沉淀在同一张表中，并提供明确的 schema。这意味着，上层使用方可以直接面向结构化数据开展分析：即使分析师的数据开发能力不强，也可以基于 Paimon 的近实时中间层，通过 <strong>StarRocks</strong> 自助完成对近实时数据的分析。</p><p>在这种模式下，过去一些相对简单的取数与分析需求，可以由 BI 或分析师通过自助方式直接完成，不再必须提交给数据研发排期处理，从而在一定程度上减少数据开发侧的需求量与交付压力。</p><p><strong>2）业务难点攻坚</strong></p><p>通过稳定的中间层 Paimon，以及“OLAP 实时关联易变维度”的模式，将原本高消耗的 ADS 回刷任务转化为秒级查询来完成。在后续内容中，我会进一步展开这一改造如何将高消耗回刷去掉，并带来显著的成本收益——每年可节省近千万元级别的回刷成本。</p><p>同时，我们通过 StarRocks + RoaringBitmap 的方案，高性能解决了跨天交叉实时 UV 的计算难题，以更低成本的方式满足大促期间对近实时能力的诉求。</p><h3>1.4 新数据架构</h3><p><img width="723" height="325" referrerpolicy="no-referrer" src="/img/bVdnStR" alt="be42fb67ef9846c08fa064277e5a73fc.png" title="be42fb67ef9846c08fa064277e5a73fc.png" loading="lazy"/></p><p>在秒级数据链路上，我们通过实时 Flink 任务消费 DWD 层的 Fluss（秒级实时数据），并将结果写入 ADS 层的 Fluss。</p><p>Fluss 提供“湖流一体”的同步开关。开启后，Fluss 中的数据会按配置周期自动同步到 Paimon 表中，默认周期可以是每 3 分钟，且该时间间隔支持用户自定义配置。同步完成后，Paimon 表中会形成当天分钟级数据（t 当天）以及 t−n 的历史数据。</p><p>在此基础上，我们会启动 Flink 流批一体任务，同时消费 DWD 层 Paimon 的 t 当天数据与 t−n 历史数据，并将加工结果写入 Paimon 表的 ADS 层与 DWD 层，分别沉淀 t 当天与历史数据。</p><p>此外，基于 Paimon 的 partial-update 能力，我们也可以构建离线大宽表，用于承载同一业务对象的多状态聚合。以订单为例，订单存在支付、确收、退款等多种状态，可以构建一张以 order\_id 为主键的 Paimon 大宽表，将这些状态写入同一行记录。这样在使用侧只需读取对应 order\_id 的一条记录，即可获取该订单的多种状态信息，使用成本与分析便利性都会更高。</p><p>在 ADS 层，我们沉淀的计算结果主要面向“叶子粒度”的维度：例如类目侧以叶子类目为主；若涉及商家分层维表，则对应叶子商家分层。在数据服务层，我们通过 StarRocks 对外提供数据服务。具体而言，在 StarRocks 层既可以直接读取 ADS 层数据进行点查，也可以直接读取 DWD 层的中间层数据进行在线计算。后一种方式的查询负载通常更重、数据量更大，但在当前实践中，StarRocks 仍然能够将查询时延控制在秒级范围内，在查询量较大的情况下保持较快响应。在查询过程中，我们也可以进一步关联 Paimon 维表，最终将查询结果在数据产品端进行展示与交付。</p><p>在我们的业务场景中，一般来说，DWD 层的中间层事实数据相对稳定；真正“易变”的往往是维度侧的数据，例如 Paimon 维表（类目维表、商家分层维表等）。当业务规则或口径发生调整时，通常只需要更新维表即可。相较于回刷大规模中间层数据，维表更新的成本更低、执行也更快。</p><p>更关键的是，我们在查询侧采用现算方式：维表更新后，查询会在读取中间层数据的基础上实时关联最新维表，因此中间层数据无需随业务变更反复回刷。由于中间层计算量较重，如果依赖回刷来响应业务调整，整体周期往往较长——快则一到两天，慢则可能需要一周。通过“更新维表 + 查询现算”的方式，业务变更后可以更快在数据产品侧看到最新结果。</p><p>在数据服务层，我们进一步利用 StarRocks 的 Warehouse 机制，对读集群进行隔离与分级保障，避免不同业务互相影响。我们按照业务重要性划分为三类：</p><ul><li>默认 Warehouse：保障级别相对一般；</li><li>重保 Warehouse：承载最核心业务，保障级别最高；</li><li>业务 BI 专用 Warehouse：面向业务 BI 或其他业务的专用资源池，保障级别相对一般。</li></ul><h2>2、Paimon+StarRocks 在双11大规模 OLAP 查询场景下的实践与优化</h2><h3>2.1 业务背景</h3><p>在日常情况下，运营和业务 BI 往往在不同时间访问数据产品，因此 StarRocks的瞬时请求量（RPS）整体较低，压力相对平稳。</p><p>但在大促期间情况会明显不同。以开卖时段为例，运营和业务 BI 通常会在接下来的一小时内集中访问数据产品，导致 StarRocks 的瞬时请求 RPS 急剧升高，对 StarRocks 集群带来显著挑战。</p><p>因此，本部分的实践与优化工作主要围绕“大促场景稳定运行”这一目标展开。</p><h3>2.2 集群侧保障</h3><p><img width="723" height="345" referrerpolicy="no-referrer" src="/img/bVdnStW" alt="5b0f2bb0168249bc98beaa726abd5806.png" title="5b0f2bb0168249bc98beaa726abd5806.png" loading="lazy"/></p><p><strong>1）在应用层面推广数据集缓存策略：</strong></p><p>目前配置 180 秒的查询缓存窗口。也就是说，同一条查询在 180 秒内被多次触发时，实际下发到 StarRocks 执行的仅为首次请求；后续请求直接复用首次查询结果。通过该策略，可以有效降低大促高峰期 StarRocks 集群的瞬时压力。</p><p><strong>2）集群层面的保护机制：</strong></p><p>集群侧设置了 30 秒的全局超时：如果一条 SQL 在 30 秒内仍未执行完成，会被自动终止。该机制属于 StarRocks 的集群保护能力，当查询执行时间超过 30 秒，即可判定该 SQL 需要进一步优化，不适合直接上线，需要回退并完成优化后再进入生产环境。对于少量确有必要、且在 30 秒内无法完成的特殊 SQL，也支持为单条 SQL 配置更长的超时时间。但此类 SQL 数量通常极少，上线评估也会更加严格，以确保不会对整体集群稳定性产生影响。整体目标是避免单条慢 SQL 拖垮集群。</p><p><strong>3）架构层隔离：按业务重要性划分只读实例。</strong></p><p>基于业务重要性对只读查询资源进行分层，将不同业务的读请求隔离到不同的只读实例上，避免相互干扰。</p><p><strong>4）集群初始化配置</strong></p><p>在新的 StarRocks 集群初始化时，比较推荐先设置一套基础参数，如下：</p><pre><code class="plaintext">set global cbo_cte_reuse_rate=0; </code></pre><p>当 CTE 被多处引用时，可能触发同一数据源的重复读取。例如，一个表在 select 中读取三次，那么 StarRocks会对同一张 Paimont 表执行三次读取，读 I/O 开销相当于被放大为 3 倍。将该参数设置为 0 后，可使同一张表在同一条查询中只读取一次。</p><pre><code class="plaintext">set global query_timeout=30; </code></pre><p>设置 30 秒的集群全局查询超时<strong>，</strong>避免单条慢 SQL 拖垮集群。</p><pre><code class="plaintext">set global new_planner_optimize_timeout=10000;</code></pre><p>适当调大执行图优化器的超时时间。如果该参数设置过小，SQL 在调度过程中更容易直接失败；适当增大后，可降低 SQL 失败的频率。</p><pre><code class="plaintext">set global pipeline_dop=8; </code></pre><p>调整 pipeline 的 DOP，用于控制每台机器上拉起的 driver 数量。压测结果显示，在大促场景中 SQL 请求高度集中，若 DOP 设置过大（例如 64），单条 SQL 在每台机器上会拉起大量 driver，带来调度开销飙升，甚至可能打满 driver 阻塞队列，导致 CPU 利用率反而上不去，集群进入不可用状态。</p><p>在我们 StarRocks 集群的双 11 压测中，DOP 调整到 8 时整体查询表现最优，因此给出 DOP=8 作为建议值。需要强调的是，该值是经验建议，最终仍应以各自集群的压测结果为准进行配置。</p><pre><code class="plaintext">set global scan_paimon_partition_num_limit=100;  </code></pre><p>限制 scan paimon外表的最大分区，用于杜绝因条件缺失或下推失败导致的全表/超大范围扫描。</p><h3>2.3 核心指标监控</h3><p>通过观察 StarRocks 核心指标的水位变化，可以持续评估实例健康状况。常用的核心指标如图。<br/><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnStX" alt="image.png" title="image.png" loading="lazy"/></p><h3>2.4 报警规则</h3><p><img width="723" height="371" referrerpolicy="no-referrer" src="/img/bVdnStY" alt="cdb33b569699410c9cb33cc79cee25fb.png" title="cdb33b569699410c9cb33cc79cee25fb.png" loading="lazy"/></p><p>建立 StarRocks 实例的异常报警机制非常关键，它能够帮助及时发现实例异常并快速介入处理。报警项的设置通常围绕“资源水位、节点可用性、调度拥塞、查询失败与时延”几类核心信号展开，其中有一部分阈值来自大促压测与实战探索，具有较强参考价值：</p><ul><li>BE/CN 的 CPU 与内存使用率设置阈值，例如当使用率持续高于 70% 时触发告警；</li><li>FE 的 CPU 与内存使用率同样设置 70% 的告警阈值；</li><li>在可用性方面，可以监控 BE/CN 或 FE 的可用率是否低于 100%，一旦出现低于 100% 的情况，通常意味着有节点不可用或发生故障。</li><li>当 BE 阻塞队列数超过 2000 时，StarRocks 集群的查询时延可能出现陡增；</li><li>在查询侧，可以增加查询失败次数与查询时延分位数的告警，例如“查询失败次数大于 n”“查询延迟 TP99 大于 n”。其中 n 的取值需要结合业务特性与可接受的服务水平目标进行配置。</li></ul><h3>2.5 元数据监控</h3><p><img width="723" height="206" referrerpolicy="no-referrer" src="/img/bVdnStZ" alt="73cea218e8ee4922b80e8b5ecba11fbf.png" title="73cea218e8ee4922b80e8b5ecba11fbf.png" loading="lazy"/></p><p>为更有效地治理 StarRocks的各类查询请求，可以实时获取审计日志，并基于审计日志构建元数据监控大盘，为后续的慢查询 SQL 治理提供数据支撑与定位依据。</p><pre><code class="plaintext">select * from _starrocks_audit_db_.starrocks_audit_tbl;</code></pre><p>审计日志相关数据落在 <strong>StarRocks</strong> 的内表中，对应信息可实时查询。也就是说，某条 SQL 执行完成后，可以立即在该内表中查到这条 SQL 的执行耗时等关键字段。基于这一基础能力，如果需要进一步做更细的源数据与查询行为监控，也可以围绕审计日志中记录的 SQL 信息进行扩展。</p><p>在监控大盘的组织方式上，支持按 Warehouse 维度拆分（例如划分为多个 Warehouse），同时也可以按数据集进行过滤。在筛选完成后，重点关注的数据字段通常包括：数据集名称、总 CPU 消耗、总查询大小、查询次数、查询行数、失败率与失败次数、单次查询的 CU 消耗、查询时间以及查询发起人等。这些指标支持排序与聚合，便于在优化过程中选取特定时间窗口，对总 CPU 消耗、总查询大小、总查询行数等维度进行 Top SQL 排查与治理。通过优先治理这些“高消耗/高影响”的 SQL，往往能够显著改善集群整体健康状况，因为在许多情况下，集群不稳定的根因来自少量高风险的“坏 SQL”。</p><h3>2.6 大促保障</h3><p><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdnSt0" alt="image.png" title="image.png" loading="lazy"/><br/>大促保障的目标，是把不确定性尽量前置消化，确保开卖高峰期间查询链路稳定可控。</p><ul><li><strong>在资源侧</strong>，会结合历史数据与业务预测，在大促开始前对 StarRocks 集群进行主动扩容，并在大促结束后主动缩容。</li><li><strong>在需求侧</strong>，提前与业务负责人对齐本次大促的核心变更点，重点关注改造或新增页面，并将核心页面的 QPS 进行量化，为全链路压测与容量评估做准备。</li><li><strong>针对重保页面</strong>，我们还会建立一套智能应急机制，分为实例级与查询级两层。实例级故障切换方面，当 StarRocks 主实例不可用时，可通过自动化预案工具（FBI）将重保页面的查询请求批量切换到备库 Warehouse，完成实例级容灾；查询级自动容错方面，当重保页面出现单次查询失败或超时，系统会将该查询自动路由到备库 Warehouse 重试，尽量做到用户无感，为关键 SQL 增加一次“二次机会”，提升整体稳定性。</li></ul><h3>2.7 大促压测</h3><p><img width="723" height="294" referrerpolicy="no-referrer" src="/img/bVdnSt1" alt="ec2dd45519bb480d8784695847ee4629.png" title="ec2dd45519bb480d8784695847ee4629.png" loading="lazy"/><br/>大促压测通常分为两层：<strong>核心页面单压与全链路压测。</strong></p><p>在核心页面单压阶段，会先梳理大促期间的核心页面及新增页面，并对这些页面进行单独压测。这样做的目的，是尽可能在活动前置暴露并解决单点问题导致的性能瓶颈，为后续上线留出精细化优化空间。</p><p>在全链路压测阶段，会模拟“所有页面同时达到流量峰值”的极限场景，用以验证 StarRocks 集群在峰值冲击下的整体资源水位与关键性能指标是否符合预期。重点关注的资源水位通常包括 CPU、内存与 I/O，同时结合查询时延等指标，评估集群在极端并发与高负载下的稳定性与承载边界。</p><h3>2.8 压测发现的问题和优化方案</h3><p><strong>1）分区裁剪失效或缺少分区过滤，导致扫全表</strong><br/><img width="723" height="307" referrerpolicy="no-referrer" src="/img/bVdnSt2" alt="1e442b8c79984e2a85440c9c085d46c5.png" title="1e442b8c79984e2a85440c9c085d46c5.png" loading="lazy"/></p><p>压测中发现，部分 SQL 因分区裁剪失效或未配置分区过滤条件，出现扫描范围过大甚至扫全表的风险。针对该类问题，治理原则是必须启用分区过滤并确保分区裁剪生效，不允许存在扫全表 SQL 在线运行。</p><p>分区裁剪生效的常见写法包括：对分区字段进行日期传参，直接基于分区字段触发裁剪；或使用日期函数触发裁剪，例如 date\_format、date\_add 等函数也可以触发分区裁剪。</p><p>分区裁剪失效的典型场景是分区字段与子查询结果进行比较，例如将分区字段与子查询返回的最小活动时间进行对比时，分区裁剪会失效。原因在于分区裁剪发生在 FE 阶段，而子查询需要到 BE 执行，FE 在规划阶段无法获得子查询结果，从而无法生成有效的分区裁剪信息。</p><p><strong>2）读取 Paimon 生表时小文件过多，导致读取数据块数过大</strong><br/><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnSt3" alt="f7565bca02374c3a9dd1ae6e94058b02.png" title="f7565bca02374c3a9dd1ae6e94058b02.png" loading="lazy"/></p><p>压测还发现，读取 Paimon 表时存在小文件过多的问题。</p><p><strong>定位方法</strong>：在 StarRocks 执行 SQL 时可开启 profile（通过 hint：/*+ SET\_VAR(enable\_profile = true) */）生成 profile 文件；在 profile 中搜索 “metadata”，其中 nativeReaderReadNum 表示读取的数据块数，nativeReaderReadBytes 表示读取的字节数。实践中，当单个分区的 nativeReaderReadNum 大于 200 时，通常建议考虑对表进行排序治理。</p><p><strong>优化方案</strong>：在构建流批排序Paimon表时，建议采用分支表模式：离线分支将 bucket 设为 -1，实时分支按需设置 bucket。离线分支表通过 clustering columns 指定排序字段，可支持指定多个字段（如 f1、f2），一般选择 OLAP 查询中最常用的过滤字段，以提升过滤命中与读取效率。该能力仅支持 Flink 批写入，不支持 ODPS 写入；写入表时需要使用 hint：<strong>/*+ OPTIONS('sink.parallelism' = '64') */</strong>。对于 ODPS 写入的 Paimon 表，则需要在任务下挂一个单独的 compact 排序任务。</p><p><strong>为何有效</strong>：在双 11 场景下，活动周期往往持续数十天。当天数据属于实时增量，而从活动开始到昨天的历史数据占比更大；因此对离线数据进行表排序收益显著。压测实测显示，排序后读取的数据块数约为排序前的 1/1000。  离线分支完成排序后，活动开始到昨天（占比最大的历史数据）基本都处于“已排序、数据块读取量很小”的状态；实时分支由于无法排序，读取的数据块会相对多一些，但实时数据通常只存在于当天，整体占比小，因此对整条 SQL 的查询时延影响相对有限。</p><p><strong>3）检查是否命中 MapJoin：小维表建议显式 broadcast</strong></p><p>当 SQL 需要 join 小表（例如小于 10MB 的维表）时，建议在维表前显式加 broadcast，以触发类似离线 MapJoin 的执行策略。实测显示，引入 broadcast 后查询时延可显著下降，典型场景下可从十几秒优化到约 3 秒，整体查询时延约为原先的 1/3。</p><pre><code class="plaintext">SELECT xxx
FROM table_a t0
LEFT JOIN [broadcast] dim_table_b t1 
ON t0.cate_id = t1.slr_main_cate_id
AND t1.ds = 'xxx'</code></pre><p><strong>4）检查跨地域访问：计算与存储尽量同地域部署</strong></p><p>还需要确认 StarRocks 实例与所读取的 Paimon 表是否处于同一地域。若不在同一地域，查询时延会明显增加。建议将 StarRocks 的部署地域与 Paimon 表存储地域保持一致。</p><p><strong>5）主键表建议开启 deletion vectors：减少无效数据读取</strong></p><p>对于 Paimon 主键表，建议开启 'deletion-vectors.enabled' = 'true'参数。该能力会在写入阶段记录哪些主键数据已被删除；读取时可跳过已删除数据，减少无效扫描，从而提升查询性能。非主键表不需要开启该参数。</p><h2>3、阶段成果与未来规划</h2><h3>3.1 阶段成果</h3><p>整体来看，该方案带来了四方面阶段性成果。</p><ul><li><strong>数据链路得到简化</strong>：通过统一存储与统一查询面，消除了数据同步链路，并降低了多份存储带来的成本与复杂度。</li><li><strong>数据使用门槛显著降低：</strong>基于 Paimon 的实时/离线中间层，不仅数据开发人员可以使用，业务分析师也可以通过 StarRocks 自助消费近实时数据，从而减少部分简单需求对数据研发排期的依赖。</li><li><strong>回刷开销得到明显削减</strong>：<strong>核心场景的回刷效率提升约 80%，年化节省成本接近 1000 万。</strong>其关键在于查询可以直接读取 Paimon 公共层并关联 Paimon 维表，业务变更时只需刷新维表，无需回刷与该维表相关的整条数据链路。</li><li><strong>在高性能实时分析方面，低成本解决了跨天交叉维度实时 UV 的计算难题，满足大促期间近实时决策需求。</strong> 具体做法是将可累加指标（如订单数、订单支付金额等）与不可累加指标（如 user_id）分开处理：可累加指标在查询侧直接聚合；不可累加指标则将 user_id 做 RB 化后存入中间层，StarRocks 读取 Paimon 表时通过 RB 相关函数计算 UV。</li></ul><h3>3.2 未来规划</h3><p>面向下一阶段，规划主要集中在四个方向。</p><p>第一，<strong>希望 StarRocks 具备更强的自动物化能力</strong>：针对用户高频查询的 SQL 自动生成物化结果，并在后续查询中自动完成改写，直接命中物化表。由于物化表往往已经完成聚合，其数据量相较直接查询中间层可以小很多个量级，从而显著降低扫描与计算开销，进一步提升查询速度与稳定性。</p><p>第二，计划进一步<strong>丰富 StarRocks 的元数据能力</strong>。</p><p>第三，<strong>优化 StarRocks 的调度策略</strong>，重点是调度层面的 CPU 负载均衡能力。</p><p>第四，希望 <strong>StarRocks 具备直接读取 Fluss 的能力</strong>，从而支持秒级查询场景。目前 Paimon 仍以分钟级链路为主，如果能够在读取侧进一步下探到 Fluss，将更好覆盖对秒级实时性有明确诉求的业务场景。</p>]]></description></item><item>    <title><![CDATA[4月17日，博睿数据受邀出席GOPS全球运维大会2026 · 深圳站！ 博睿数据 ]]></title>    <link>https://segmentfault.com/a/1190000047597648</link>    <guid>https://segmentfault.com/a/1190000047597648</guid>    <pubDate>2026-02-06 18:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>关于博睿数据北京博睿宏远数据科技股份有限公司（简称博睿数据）(股票号688229）是AI驱动的全球智能可观测性领导者，蝉联中国应用性能管理及可观测性APMO市场份额第一，已获得1000+头部客户的选择和信赖。专注于构建以用户为中心的简捷，高效，智能的新型IT运维，有效提升云资源利用效率，驱动业务创新增长，助力企业提升核心竞争力，抢占数字经济先机。17年以来，博睿数据以深厚的技术积累不断打磨产品和服务能力，已在IT系统可观测领域形成了自身的独特优势，并将智能可观测解决方案落地到各种客户生产环境之中，为银行，证券，保险，高端制造等行业的数字化、智能化转型持续赋能，已经获得中国银行、中国工商银行、中国建设银行、国泰海通、国信证券、泰康保险、新华保险、华为、中国南方航空等1000+头部客户的选择和信赖。2026年4月17-18日，智能体驱动的 GOPS 全球运维大会将于深圳举行，博睿数据产品中心总监贺安辉将带来《智能体协同矩阵：重塑下一代故障智能诊断范式》的精彩演讲，敬请期待。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597650" alt="图片" title="图片"/><br/>Bonree关于 GOPS 全球运维大会<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597651" alt="图片" title="图片" loading="lazy"/><br/>当前，智能体（Agent）技术正以前所未有的深度重塑IT领域的生产范式。从运维自动化的闭环决策到开发环节的智能协同，从测试流程的自主验证到基础设施的AI驱动，智能体正成为推动研发运维体系向自治化、智能化演进的核心引擎。在这一关键发展窗口，第28届智能体驱动的GOPS全球运维大会2026 · 深圳站将于2026年4月17日-18日隆重启幕。大会由高效运维社区（GreatOPS）与BizDevOps软件工厂联合主办，DAOPS基金会、开放运维联盟（OOPSA）指导，作为业内IT技术的高端行业盛会，GOPS大会自2015年发起以来已成功举办27届，覆盖国内外城市包括北京、上海、深圳、美国硅谷、新加坡举办，主要面向IT行业的中高端技术人员，累计吸引超9万人次参会，覆盖金融、通信、制造、互联网等各行业一线技术决策者与实践者。本届大会为期2天，侧重运维智能体、开发智能体、测试智能体、AI Infra、AI+DevOps、SRE、AIOps、AI+可观测性等热门技术领域。与行业一线专家共同探讨智能体驱动下的技术变革。</p>]]></description></item><item>    <title><![CDATA[4大主流CRM系统横向对比分析：从客户管理到团队协作的深度测评 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047597656</link>    <guid>https://segmentfault.com/a/1190000047597656</guid>    <pubDate>2026-02-06 18:04:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：CRM系统的价值定位</h2><p>客户关系管理(CRM)系统已成为企业数字化转型的核心工具，其价值不仅体现在销售流程的自动化，更在于通过数据整合实现全链路客户生命周期管理。本文选取<strong>超兔一体云</strong>、<strong>Free CRM</strong>、<strong>Streak</strong>、<strong>悟空CRM</strong>四个典型CRM系统，从<strong>客户信息管理</strong>、<strong>销售漏斗追踪</strong>、<strong>任务与日程管理</strong>、<strong>报表分析</strong>、<strong>团队协作</strong>五个维度展开深度对比，为企业选型提供专业参考。</p><h2>一、核心能力对比总览</h2><h3>1. 客户信息管理能力对比</h3><table><thead><tr><th>能力维度</th><th>超兔一体云</th><th>Free CRM</th><th>Streak</th><th>悟空CRM</th></tr></thead><tbody><tr><td><strong>数据采集渠道</strong></td><td>★★★★★（多渠道整合）</td><td>★★★☆☆（基础信息录入）</td><td>★★★☆☆（邮件集成）</td><td>★★★★☆（多场景导入）</td></tr><tr><td><strong>信息整合能力</strong></td><td>★★★★★（工商/社交数据自动补全）</td><td>★★★☆☆（基础档案存储）</td><td>★★★☆☆（Gmail内沟通记录）</td><td>★★★★☆（客户池与跟进记录关联）</td></tr><tr><td><strong>唯一性保障</strong></td><td>★★★★★（自定义查重规则+自动简称）</td><td>★★★☆☆（基础字段查重）</td><td>★★☆☆☆（无主动查重设计）</td><td>★★★☆☆（手动查重+标签辅助）</td></tr><tr><td><strong>客户生命周期管理</strong></td><td>★★★★★（八阶段客池分类+经纬度标记）</td><td>★★★☆☆（基础阶段划分）</td><td>★★★☆☆（线索→成交线性管理）</td><td>★★★★☆（客户池分级管理）</td></tr><tr><td><strong>数据安全机制</strong></td><td>★★★★★（细粒度权限+工作流控制）</td><td>★★★☆☆（基础权限控制）</td><td>★★★☆☆（团队内部分享）</td><td>★★★☆☆（角色权限+可见范围控制）</td></tr></tbody></table><p><strong>关键差异解析</strong>：</p><ul><li><strong>超兔一体云</strong>通过微信生态、广告渠道、线下地推等多入口采集数据，结合天眼查、百度工商信息自动补全，构建企业级客户画像；</li><li><strong>Streak</strong>作为Gmail原生工具，仅支持邮件渠道信息整合，适合依赖邮件沟通的外贸/咨询企业；</li><li><strong>悟空CRM</strong>的“客户池自动认领”机制有效解决销售团队资源流失问题，支持跨部门数据共享。</li></ul><h3>2. 销售漏斗追踪能力对比</h3><table><thead><tr><th>能力维度</th><th>超兔一体云</th><th>Free CRM</th><th>Streak</th><th>悟空CRM</th></tr></thead><tbody><tr><td><strong>跟单模型多样性</strong></td><td>★★★★★（小单快单/商机/多方项目三维模型）</td><td>★★★☆☆（简化版销售管道）</td><td>★★★☆☆（基础阶段管理）</td><td>★★★★☆（自定义阶段+子流程）</td></tr><tr><td><strong>可视化程度</strong></td><td>★★★★★（360°跟单视图+时间线+通信集成）</td><td>★★★☆☆（基础管道图）</td><td>★★★☆☆（基础阶段进度条）</td><td>★★★★☆（甘特图+漏斗分层视图）</td></tr><tr><td><strong>流失预警机制</strong></td><td>★★★★★（14/30天无跟进自动预警+智能调整）</td><td>★★★☆☆（自定义阶段停留阈值）</td><td>★★★☆☆（无主动预警规则）</td><td>★★★☆☆（人工标记+系统提醒）</td></tr><tr><td><strong>预测能力</strong></td><td>★★★★★（AI驱动销售预测+趋势分析）</td><td>★★★☆☆（历史数据统计推算）</td><td>★★★☆☆（阶段转化率预估）</td><td>★★★★☆（BI驱动多维度预测）</td></tr><tr><td><strong>项目化管理</strong></td><td>★★★★★（分组隔离+多方协同+收支管控）</td><td>★★★☆☆（标准销售流程）</td><td>★★☆☆☆（单客户维度管理）</td><td>★★★★☆（项目看板+资源分配）</td></tr></tbody></table><p><strong>关键差异解析</strong>：</p><ul><li><strong>超兔一体云</strong>的“多方项目模型”针对医院/高校等组织型客户设计，支持跨部门分组隔离；</li><li><strong>Streak</strong>依赖用户手动标记阶段转换，缺乏自动化规则触发能力；</li><li><strong>悟空CRM</strong>的“公海客户池”有效解决资源浪费问题，支持客户在销售间流转。</li></ul><h3>3. 任务与日程管理能力对比</h3><table><thead><tr><th>能力维度</th><th>超兔一体云</th><th>Free CRM</th><th>Streak</th><th>悟空CRM</th></tr></thead><tbody><tr><td><strong>任务颗粒度</strong></td><td>★★★★★（主任务+子任务+检查清单）</td><td>★★★☆☆（单任务基本管理）</td><td>★★★☆☆（任务创建+截止日提醒）</td><td>★★★★☆（任务拆解+依赖关系设置）</td></tr><tr><td><strong>协同提醒系统</strong></td><td>★★★★★（多渠道+AI智能提醒+时间窗口）</td><td>★★★☆☆（基础日历提醒）</td><td>★★★☆☆（邮件/弹窗提醒）</td><td>★★★☆☆（多渠道+日程冲突检测）</td></tr><tr><td><strong>团队协作视图</strong></td><td>★★★★★（甘特图+看板+日历混合视图）</td><td>★★★☆☆（团队日程共享）</td><td>★★★☆☆（个人日程）</td><td>★★★★☆（团队日程+任务进度看板）</td></tr><tr><td><strong>外勤管理</strong></td><td>★★★★★（定位打卡+拜访记录GPS轨迹）</td><td>★★☆☆☆（无外勤功能）</td><td>★★☆☆☆（无外勤管理）</td><td>★★★☆☆（定位打卡+拜访记录）</td></tr><tr><td><strong>移动端适配</strong></td><td>★★★★★（全功能+离线操作）</td><td>★★★☆☆（基础同步）</td><td>★★★☆☆（移动端浏览）</td><td>★★★☆☆（基础移动端功能）</td></tr></tbody></table><p><strong>关键差异解析</strong>：</p><ul><li><strong>超兔一体云</strong>的外勤管理模块支持GPS轨迹自动记录，解决远程团队监管难题；</li><li><strong>Streak</strong>仅支持邮件内任务创建，与Gmail强绑定但灵活性受限；</li><li><strong>悟空CRM</strong>的“子任务拆解”和“依赖关系设置”适合复杂项目管理。</li></ul><h3>4. 报表分析能力对比</h3><table><thead><tr><th>能力维度</th><th>超兔一体云</th><th>Free CRM</th><th>Streak</th><th>悟空CRM</th></tr></thead><tbody><tr><td><strong>分析引擎功能</strong></td><td>★★★★★（多表聚合+同比环比+AI计算）</td><td>★★★☆☆（基础统计+同比分析）</td><td>★★★☆☆（基础阶段转化率分析）</td><td>★★★★★（BI可视化+自定义指标）</td></tr><tr><td><strong>报表自定义程度</strong></td><td>★★★★★（拖拽式+10+维度组合）</td><td>★★★☆☆（基础字段筛选）</td><td>★★★☆☆（固定模板+简单筛选）</td><td>★★★★☆（多表聚合+维度自由组合）</td></tr><tr><td><strong>决策支持工具</strong></td><td>★★★★★（漏斗瓶颈AI识别+异常预警）</td><td>★★★☆☆（数据导出Excel）</td><td>★★★☆☆（销售数据导出）</td><td>★★★★☆（多维度BI仪表盘）</td></tr><tr><td><strong>数据可视化效果</strong></td><td>★★★★★（动态图表+交互钻取）</td><td>★★★☆☆（基础图表类型）</td><td>★★★☆☆（基础图表+静态展示）</td><td>★★★★☆（动态饼图/柱状图）</td></tr><tr><td><strong>数据安全与权限</strong></td><td>★★★★★（报表级细粒度权限）</td><td>★★★☆☆（用户级权限控制）</td><td>★★★☆☆（团队数据共享）</td><td>★★★☆☆（子报表权限隔离）</td></tr></tbody></table><p><strong>关键差异解析</strong>：</p><ul><li><strong>超兔一体云</strong>的“工作台数字卡片+AI异常检测”实现销售过程动态监控；</li><li><strong>悟空CRM</strong>的“BI仪表盘”支持多维度数据联动，适合集团化企业多部门数据分析；</li><li><strong>Free CRM</strong>作为通用型CRM，其“销售数据导出”功能满足中小企业基础分析需求。</li></ul><h3>5. 团队协作能力对比</h3><table><thead><tr><th>能力维度</th><th>超兔一体云</th><th>Free CRM</th><th>Streak</th><th>悟空CRM</th></tr></thead><tbody><tr><td><strong>组织架构支持</strong></td><td>★★★★★（9级结构+矩阵式临时小组）</td><td>★★★☆☆（层级化组织）</td><td>★★★☆☆（单组织架构）</td><td>★★★☆☆（基础层级+部门隔离）</td></tr><tr><td><strong>实时沟通工具</strong></td><td>★★★★★（IM+视频会议+文件协作）</td><td>★★★☆☆（基础消息+文档共享）</td><td>★★★☆☆（Gmail内邮件集成）</td><td>★★★★☆（IM+日志+消息通知）</td></tr><tr><td><strong>流程自动化</strong></td><td>★★★★★（全流程工作流+自然语言生成）</td><td>★★★☆☆（基础审批流程）</td><td>★★★☆☆（无流程引擎）</td><td>★★★☆☆（自定义审批表单+多分支流程）</td></tr><tr><td><strong>权限精细度</strong></td><td>★★★★★（全局权限+步骤级权限控制）</td><td>★★★☆☆（基础角色权限）</td><td>★★★☆☆（团队内部分享）</td><td>★★★☆☆（字段级+功能级权限组合）</td></tr><tr><td><strong>跨系统集成</strong></td><td>★★★★★（100+应用API+Webhook支持）</td><td>★★★☆☆（基础第三方集成）</td><td>★★★☆☆（无额外集成）</td><td>★★★★☆（基础ERP/财务系统对接）</td></tr></tbody></table><p><strong>关键差异解析</strong>：</p><ul><li><strong>超兔一体云</strong>的“自然语言生成工作流”可直接通过AI创建客户跟进流程，减少手动配置；</li><li><strong>Streak</strong>完全依赖Gmail生态，适合无复杂协作需求的小型团队；</li><li><strong>悟空CRM</strong>的“工作日志+审批管理”模块强化了内部协作效率，适合销售驱动型团队。</li></ul><h2>二、核心流程可视化展示</h2><h3>1. 客户信息管理数据采集流程（超兔一体云）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597658" alt="" title=""/></p><p>暂时无法在飞书文档外展示此内容</p><p><strong>核心优势</strong>：通过多触点数据整合，实现客户信息自动补全与实时更新，避免人工录入错误。</p><h3>2. 团队协作组织架构脑图（超兔一体云）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597659" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><p><strong>核心优势</strong>：支持大型集团企业复杂组织管理，兼顾层级控制与灵活协作需求。</p><h3>3. 销售漏斗优化逻辑（通用流程图）</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597660" alt="" title="" loading="lazy"/></p><p>暂时无法在飞书文档外展示此内容</p><p><strong>差异化应用</strong>：</p><ul><li><strong>超兔</strong>通过AI自动识别延迟节点并触发优化规则；</li><li><strong>Streak</strong>需手动分析转化率低的阶段，适合资源有限的初创团队。</li></ul><h2>三、雷达图综合评分（满分10分）</h2><table><thead><tr><th>品牌/维度</th><th>客户信息管理</th><th>销售漏斗追踪</th><th>任务日程管理</th><th>报表分析</th><th>团队协作</th><th>综合得分</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>9.2</td><td>9.5</td><td>9.0</td><td>9.4</td><td>9.6</td><td>9.3</td></tr><tr><td><strong>Free CRM</strong></td><td>7.8</td><td>7.5</td><td>7.6</td><td>7.7</td><td>7.6</td><td>7.6</td></tr><tr><td><strong>Streak</strong></td><td>7.0</td><td>7.2</td><td>6.8</td><td>6.9</td><td>6.5</td><td>6.9</td></tr><tr><td><strong>悟空CRM</strong></td><td>8.9</td><td>8.5</td><td>8.7</td><td>9.0</td><td>8.3</td><td>8.6</td></tr></tbody></table><h2>四、各品牌适用场景与选型建议</h2><h3>1. 超兔一体云：大型复杂业务场景首选</h3><ul><li><strong>适用类型</strong>：多产品线、多渠道销售、大型集团企业</li><li><strong>典型场景</strong>：医疗器械招标项目管理、连锁品牌全国客户资源整合、跨部门数据安全协作</li><li><strong>核心价值</strong>：通过AI驱动的全链路管理，降低复杂业务的沟通成本与数据安全风险</li></ul><h3>2. 悟空CRM：成长型企业性价比之选</h3><ul><li><strong>适用类型</strong>：销售团队扩张期、客户资源分配型、需要BI分析能力的中小企业</li><li><strong>典型场景</strong>：电商平台客户分层运营、教育机构学员管理、制造业项目跟进</li><li><strong>核心价值</strong>：以客户池与BI分析为核心，平衡功能完整性与性价比</li></ul><h3>3. Free CRM：标准化需求通用方案</h3><ul><li><strong>适用类型</strong>：需求稳定、团队规模小、预算有限的创业公司</li><li><strong>典型场景</strong>：咨询公司项目跟进、小型服务企业客户管理、初创期团队基础销售管理</li><li><strong>核心价值</strong>：基础功能全面但无复杂场景设计，适合快速上手</li></ul><h3>4. Streak：外企/外贸团队Gmail依赖型</h3><ul><li><strong>适用类型</strong>：依赖邮件沟通、全球团队协作的外贸/国际业务</li><li><strong>典型场景</strong>：跨境电商客户跟进、海外市场开发、多语言客户管理</li><li><strong>核心价值</strong>：Gmail生态无缝集成，减少工具切换成本</li></ul><h2>五、结论与趋势展望</h2><p>本次对比表明，<strong>超兔一体云</strong>凭借多渠道数据整合、AI工作流引擎、全场景协作管理，在复杂业务场景下展现绝对优势；<strong>悟空CRM</strong>在客户池管理与BI分析上表现突出，适合成长型企业；<strong>Streak</strong>作为Gmail原生工具，在邮件依赖型团队中不可替代。</p><p>未来CRM系统发展趋势：</p><ol><li>AI深度融入：自然语言生成工作流、客户需求预测、销售话术推荐；</li><li>生态化整合：与ERP、财务系统、OA等打通，实现全业务链管理；</li><li>移动端体验：AR客户拜访、语音操作等沉浸式交互。</li></ol><p>企业选型时，建议优先考虑业务复杂度、团队规模及协作需求，通过试用对比选择与长期战略匹配的CRM系统。</p><p>【注：本文数据基于各品牌公开资料整理，具体功能请以官方最新版本为准】 【图表说明：雷达图分数为各维度加权平均，综合得分由9.3/8.6/7.6/6.9四品牌对比结果得出】</p>]]></description></item><item>    <title><![CDATA[迈向全面自主运维！《智能体协同矩阵重塑自主运维新范式》白皮书重磅发布！ 博睿数据 ]]></title>    <link>https://segmentfault.com/a/1190000047597663</link>    <guid>https://segmentfault.com/a/1190000047597663</guid>    <pubDate>2026-02-06 18:03:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597665" alt="图片" title="图片"/><br/>当前，我们正处在一个AI技术飞速发展的时代。企业运维的演进脉络清晰可见：从信息时代的效率提升与自动化，到数字时代的数据驱动与智能运维（AIOps）崛起，如今正大步迈向多智能体协作（Multi-Agent System, MAS）的新阶段。在这一新阶段，多智能体通过任务分解、知识共享与协同决策所形成的“群体智慧”，将推动运维从被动响应走向全面自主，成为企业驾驭复杂系统、保障业务连续性的关键。Bonree《智能体协同矩阵重塑自主运维新范式》白皮书点击下方海报或扫描二维码即刻免费下载👇<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597666" alt="图片" title="图片" loading="lazy"/><br/>博睿数据重磅发布《智能体协同矩阵重塑自主运维新范式》白皮书。该书立足国内运维行业发展趋势，深度融合博睿数据在多智能体协作领域的实践经验与技术洞察，详解多智能体协作在运维体系化演进中的核心原理、架构设计、技术实现、落地路径与未来趋势，兼顾行业共性需求与企业个性化应用场景，为各行业布局智能运维、提升智能体协作下的运维效能提供全方位的理论支撑与技术指引。白皮书剖析了当前运维领域面临的共性难题，指出单点智能的局限性，并提出以多智能体协同为核心的解决方案框架。此外，白皮书重点阐述了博睿数据BonreeONE“三位一体”的智能体协作体系——通过基于Workflow的故障诊断Agent、基于知识驱动的故障诊断Agent、基于自主决策的故障诊断Agent三种不同类型的故障诊断Agent互补共存，以应对不同确定性的运维场景； 针对多智能体协作的特有需求，博睿数据创新性提出构建包含语义、认知、协作、成本、安全五大核心层面的立体化治理架构，全面覆盖多智能体协作全流程，保障协作的高质量、高效率与高可信度。多智能体协作正持续拓宽运维价值维度，推动运维体系向智能协同的新阶段演进，重塑智能运维生态格局，成为企业数字化韧性构建的核心支柱。博睿数据将依托多智能体协作这一“群体智慧”，助力企业构建高效、全域的智能可观测能力，迈向全面自主运维新征程。</p>]]></description></item><item>    <title><![CDATA[基于Flink CDC的企业级日志实时入湖入流解决方案 ApacheFlink ]]></title>    <link>https://segmentfault.com/a/1190000047597673</link>    <guid>https://segmentfault.com/a/1190000047597673</guid>    <pubDate>2026-02-06 18:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：</p><p>徐榜江(雪尽) ，阿里云Flink数据通团队负责人，Flink PMC 成员，Flink CDC 开源项目负责人</p><p>李昊哲(米灵)，阿里云Flink高级产品经理，负责阿里云 Flink 稳定性、可观测性、数据摄入等企业级产品特性</p><p><strong>内容概要</strong></p><p>本文主要介绍阿里云基于开源 Flink CDC 打造的企业级日志实时入湖入流的技术解决方案，涵盖产品功能介绍、日志场景挑战与解决方案、最佳实践案例以及联合解决方案等内容。</p><h2>一、阿里云企业级Flink CDC数据摄入功能介绍</h2><h3>1、Flink CDC开源项目概述</h3><p>开源 Flink CDC 是一款用于处理数据变更捕获（Change Data Capture）、支持增量数据的分布式数据集成工具。该项目早期主要聚焦于数据库入库入仓场景，在数据库增量数据同步领域积累了丰富的实践经验。</p><p>从 3.0 版本开始，Flink CDC 支持通过 YAML 格式描述数据传递过程以及 ETL 转换逻辑，极大简化了用户的数据集成与同步工作。Flink CDC 的核心价值在于结合数据库的变更捕获技术（Data Capture），打造全增量一体化的集成框架，有效降低用户的使用成本，同时满足数据时效性与一致性方面的需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597675" alt="幻灯片4.png" title="幻灯片4.png"/></p><p>Flink CDC 最主要的应用场景是在数仓分层架构中作为数据入湖入仓的第一步。增量快照算法是其核心能力之一，支持读取历史数据、全增量一体化同步以及整库同步等功能。此外，Schema 信息管理功能在后续版本迭代中持续增强，进一步提升了用户对社区的信任度与粘性。YAML ETL 将复杂的高级功能平民化，使更多 BI 领域的用户能够通过 YAML 脚本完成复杂的作业配置。Flink CDC 在社区的主要应用场景集中在数据库的实时入湖入仓领域。</p><p>在传统数据同步方案中，用户通常需要分别处理全量数据与增量数据，使用不同的链路与业务系统，最终通过定时合并完成数据同步。这种 Lambda 架构存在以下问题：链路组件较多，数据合并的时效性较差，且合并过程中存在位点无法强对齐的情况，容易导致数据一致性问题。对于研发人员而言，技术栈过于复杂，普通用户难以驾驭。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597676" alt="幻灯片5.png" title="幻灯片5.png" loading="lazy"/></p><p>Flink CDC 将上述复杂流程整合到一个 YAML 作业中，实现全增量一体化，Flink 作业可支持亚秒级延迟。框架层面从原理上保证数据不丢不重，同时提供端到端的作业管理体验。用户仅需编写一个 YAML 文本即可启动作业，这是 Flink CDC 在社区中最核心的应用场景。</p><h3>2、阿里云企业版Flink CDC对比开源Flink CDC</h3><p>阿里云企业版 FlinkCDC-数据摄入在开源基础上对企业版进行了多项增强，主要包括以下几个方面：</p><p><strong>引擎层面优化</strong>：阿里云企业版引擎内部称为 VVR，在作业自动调优、数据摄入（即 Flink 作业的热更新能力）、State Backend、SQL 算子等方面均进行了企业级优化。资源分配方面支持弹性力度的动态调整。</p><p><strong>管控平台支持</strong>：阿里云提供 VVP 平台负责 Flink 作业的开发与运行。相比开源版本仅支持数据库入湖入仓，VVP平台扩展支持了日志入湖入仓，具备更丰富的企业级上下游生态。</p><p>阿里云产品之间相互打通，整体用户体验更佳。平台支持资源动态扩缩容、全链路监控、告警机制等功能，同时支持 YAML 作业的全生命周期管理，包括作业版本管理、日志查询、资源配置、依赖管理等。</p><p>阿里云企业级 Flink CDC 的定位是在开源内核的基础上，通过插件化开发提供更多增值服务，提升易用性并降低开发运维门槛。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597677" alt="幻灯片6.png" title="幻灯片6.png" loading="lazy"/></p><p><strong>阿里云企业版Flink CDC-数据摄入产品优势</strong></p><p>阿里云 Flink CDC 数据摄入的产品优势可从功能特性与性能成本两个维度进行阐述。</p><p><strong>功能优势</strong>：提供更多企业级功能特性，包括引擎侧有更强大的表结构变更自动同步（无需作业重启）和 DB 入湖场景的数据限流功能，以及日志入湖场景的 Schema Inference 能力、全链路脏数据收集功能等。得益于阿里云 Flink 产品底座的长期建设，CDC YAML 作业也能复用诸多企业级能力，比如弹性扩缩容、Hot-Update 资源调优、监控和告警等能力，同时具备丰富的数据源支持，涵盖大数据存储、关系数据库、湖仓、流存储等上下游生态。</p><p><strong>性能优势</strong>：阿里云 Flink CDC 数据摄入在读取和写入上均做过深度的性能优化，在读取 MySQL 和 MongoDB 场景，支持了多线程解析和高效下推过滤等优化，对比社区有数倍性能优势。在写入 Paimon 和 Fluss 时均支持 Dynamic Shuffle 优化，能够根据每个并发的实时数据量自适应调整写入流量分布，作业运行更加智能和平稳。此外，CDC YAML 作业默认支持整库同步或多表入湖，单 Sink 节点可写多表的拓扑模式，避免拓扑节点过多导致资源消耗过大、部分表数据量少造成资源浪费等问题。</p><p>最佳用户体验体现在端到端 Pipeline 的便捷性上：用户仅需关注 YAML 文本，作业提交与部署均由平台自动完成。阿里云还提供丰富的场景与最佳实践方案文档，用户可根据实时数仓、数据库或结合 Fluss 等不同业务场景参考相应的最佳实践，直接复制粘贴 YAML 文本即可。另外，作为云产品，SLA 保障、运维监控体验更佳。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597678" alt="image.png" title="image.png" loading="lazy"/></p><p>当前最新版本已迭代至 VVR 11.5，该版本功能最全、稳定性最佳，建议用户使用最新的稳定版本以获得更好的用户体验。</p><h2>二、日志场景实时入湖入流的趋势与挑战</h2><p>随着 AI 技术、Agent 以及 AGI 等技术的兴起，AI 应用日益普及，用户对非结构化数据、日志数据乃至多模态数据的需求持续增长，Flink CDC 需要具备更强的数据接入能力。</p><p>日志实时入湖入流可为数据分析与 AI 两大赛道解锁更加新鲜的数据，帮助业务运营人员、决策人员乃至 Agent 完成更快的业务决策。数据新鲜度越高，基于数据的判断就越准确，这在风控反欺诈、广告投放等时间敏感的业务场景中尤为关键。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597679" alt="幻灯片10.png" title="幻灯片10.png" loading="lazy"/></p><h3>1、日志场景的业务痛点</h3><p>日志入湖入流领域存在以下三个主要痛点：</p><p><strong>数据定义多样化：</strong>与数据库数据不同，日志数据定义极为多样化。不同应用甚至同一应用的不同终端（如手机、iPad）采集的日志数据格式可能不同，语义也可能不一致，缺乏统一标准。数据库表字段通常固定且有明确类型约束，而日志数据可能存在 Integer、Bigint、Big Decimal 等不同类型表示同一语义的情况。因此，该场景需要具备数据规范化处理能力。</p><p><strong>日志加工时效性要求高</strong>：日志数据规模通常较大，需要实时采集处理。这不仅是对日志入湖工具系统的要求，更是端到端的要求。海量实时批量数据对数据湖引擎（如 Flink、Starrocks 等）的分析能力提出了更高要求，各子系统均需满足端到端的高性能需求。</p><p><strong>表结构变更频繁</strong>：日志数据定义多样化、终端不确定性及多版本迭代导致表结构变更频繁。数据库表变更通常需 DBA 审核，遵循加字段而非删字段的最佳实践。而日志场景灵活性高，终端采集字段的增删变化是常态。这要求端到端日志处理链路具备 Schema 推断与演进能力，支持从无 Schema 的裸 JSON 数据推断 Schema，并在下游 ODS 表自动新增字段，对技术能力提出更高要求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597680" alt="幻灯片11.png" title="幻灯片11.png" loading="lazy"/></p><h3>2、基于阿里云企业级Flink CDC日志实时入湖入流解决方案</h3><p>阿里云 Flink CDC 提供一键实时入湖功能，用户仅需编写 YAML 文本即可完成日志的实时入湖入流。入湖支持 DLF 的 Paimon Sink 服务格式，入流支持 Fluss 等流存储。</p><p>传统日志入湖入流方案通常将日志数据采集到消息队列（如 Kafka、SLS），然后通过编写 Java 代码（如 Flink DataStream 作业）进行解析处理，每个字段需手动判断处理，拓扑需根据下游表数量配置。这种方案门槛较高，要求用户熟练掌握 Java 与 Flink 核心概念，需手动处理表结构推导，且作业是黑盒不可见，开发、迭代与资源调优均较困难。</p><p>阿里云通过 YAML 方式支持 Kafka、SLS 等数据源，可自动对 Topic 内数据进行 Schema 推断与推导，并通过路由写入下游不同表。用户仅需编写 YAML 文本即可实现零代码开发，Schema 自动推导，业务复制修改即可复用。开发调优体验类似 SQL 开发，修改配置参数或动态加表均可在平台上直接编辑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597681" alt="幻灯片12.png" title="幻灯片12.png" loading="lazy"/></p><h3>3、基于阿里云企业级Flink CDC日志实时入湖入流客户案例</h3><p>某用户业务场景中，数据已采集至 Kafka，包含 DB 字段与 Table 字段，需将一个 Topic 的数据分发至下游八千多张表，要求一个作业完成。用户期望根据 DB 与 Table 字段自动建表并同步数据，新增列时 ODS 表自动加列。</p><p>该场景通过一个 YAML 文本即可解决，支持下游自动建表、分库分表、Schema 自动推导。UserId 自动推断为 String 类型，EventTime 推断为 Timestamp 类型。支持数据清洗（如 Projection 只选特定字段）、Where 过滤、UDF 过滤、表名转换等功能。</p><p>用户数据进入 DLF（Paimon、Iceberg）后，可基于 Flink 加 DLF 方案，结合 Starrocks 构建实时数仓完整解决方案，数据入湖过程高效便捷。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597682" alt="幻灯片13.png" title="幻灯片13.png" loading="lazy"/></p><h2>三、基于阿里云企业级Flink CDC日志入湖入流最佳实践</h2><h3>1、作业配置示例</h3><p>以下是一个线上的真实作业示例，API 与社区 Amazon API 一致。配置包含 Source（数据源，如 Kafka）与Sink（目标端，如 Paimon）。Transform 为可选数据转换配置，可指定所有列或通过Projection选择特定字段。可通过组件配置指定主键字段，如用 ID 作为主键。Route 可进行表名映射，如将 user 表映射为 origin\_user 或 ODS\_user 表。简单的 YAML 文本即可在阿里云 Flink 完成数据摄入作业开发。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597683" alt="幻灯片15.png" title="幻灯片15.png" loading="lazy"/></p><p>YAML 文本提交后将自动生成线上Flink作业，支持部署配置、Metric 监控、告警配置等功能。作业日志查询、Metric 查询、配置告警等体验与全托管Flink作业一致。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597684" alt="幻灯片16.png" title="幻灯片16.png" loading="lazy"/></p><h3>2、核心特性说明</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597685" alt="幻灯片17.png" title="幻灯片17.png" loading="lazy"/></p><p><strong>数据过滤与计算</strong>：支持 MySQL 语法风格的数据过滤与计算，对用户友好。例如可对表内age字段进行过滤（如 age 大于100的数据），或统计字段长度。提供内置函数与内存函数，支持 UDF 调用及 SQL 表达式调用，实现数据过滤与清洗。在数据过滤时，斜杠星（/*）表示匹配原数据所有字段，且支持 Schema Evolution。假设原数据有 ID、name、age 三个字段，新增address字段后，作业会自动在下游添加该字段，计算列与filter规则继续生效。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597686" alt="幻灯片18.png" title="幻灯片18.png" loading="lazy"/></p><p><strong>组件与分区键重定义</strong>：支持重新定义主键与分区键。例如 MySQL 表主键为 ID，但希望 DLF 数据湖表将主键换成其他字段或增加分区键（因多个数据库实例数据写入同一张表）。YAML 中可指定 PK 与分区键。</p><p>Pre Transform 与 Post Transform 执行逻辑不同。Pre Transform 侧重原数据修改，包括修改表主键、分区键、加列等操作。Post Transform 侧重数据处理，包括 Filter 与 Projection。两个算子通常嵌入为一个 Transform，既支持 Schema 裁剪与重定义，也支持数据过滤与处理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597687" alt="幻灯片19.png" title="幻灯片19.png" loading="lazy"/></p><p><strong>3、日志入湖-DLF（Paimon）快速入门</strong></p><p>阿里云 DLF 提供全托管 Paimon，可以参考阿里云的帮助文档，文档采用 Step By Step 方式，从配置白名单、准备测试数据到编写作业，用户可按步骤完成快速入门。文档中提供完整可运行的作业样例，用户只需替换 Kafka 地址与 Topic，可选配置已加上注释说明。此外，文档包含脏数据处理能力配置、Deletion Vector 优化配置等内容，用户参照文档即可将 Kafka 日志数据通过 Flink CDC 一键写入阿里云 DLF（Paimon）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597688" alt="幻灯片20.png" title="幻灯片20.png" loading="lazy"/></p><p><strong>4、日志入流-流存储Fluss版快速入门</strong></p><p>阿里云提供全托管 Fluss，当前已经开启公测。Fluss 作为流存储，相比原生 Kafka，在列裁剪、Schema 化、湖流一体化等方面优势明显。将原始采集数据同步至 Fluss 后，可构建流式数仓，对 Paimon 数据进行加工处理。Fluss 场景支持类似配置，将 Source 换为 Kafka，Sink 换为 Fluss，提交 YAML 文档后作业即可运行。即使 Kafka 内数据为无 Schema 的 JSON，也会自动推导 Schema。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597689" alt="幻灯片21.png" title="幻灯片21.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597690" alt="image.png" title="image.png" loading="lazy"/></p><h3>5、日志入湖入流最佳实践</h3><p><strong>自动推导表结构</strong></p><p>Flink CDC 数据摄入支持丰富的推导表结构策略，默认策略为自动推导表结构，该策略默认配置适用于大多数业务场景。比如通过配置预读取 Kafka 记录数为100，从指定新位点消费累计100条数据，对100条数据的 Schema 进行推导，获取推导的最宽表结构作为初始表结构。例如前面50条推导出10个字段，后面50条推导出12个字段，最终合并为最宽的15-16个字段作为下游 Paimon 表结构，自动建表并写入数据，缺失字段填 Null。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597691" alt="幻灯片22.png" title="幻灯片22.png" loading="lazy"/></p><p><strong>灵活指定表结构</strong></p><p>Flink CDC 数据摄入也支撑用户手动指定初始表结构，如下图所示用户可通过 DDL 语句声明作业初始化表结构，您可以直接粘贴下游已有表的 DDL，比如通过 Flink Catalog 执行 show create table 命令快速获取您期待的初始表结构。语法与 Flink SQL 对齐，指定初始表结构后按该结构继续演进。适用于 Kafka Topic 数据太少或尚未开始采集的场景，可先编写数据摄入作业，数据到达后自动拉起。</p><p>部分字段指定类型，自动推导可能存在误差，用户可指定部分字段为固定类型。如指定 ID 为 bigint 或 string，name 为 varchar 等。对于不符合规则的数据，可通过脏数据收集器处理。灵活指定表结构以满足特定业务需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597692" alt="幻灯片23.png" title="幻灯片23.png" loading="lazy"/></p><p><strong>脏数据处理</strong></p><p>日志场景与数据库不同，弱结构化数据不可避免存在脏数据。阿里云提供脏数据容忍与收集配置：用户可设置脏数据容忍条数，脏数据支持收集。业务运行时不查看脏数据，过后可据此调整下游 Schema 或反馈给上游业务方，确保 Pipeline 稳定运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597693" alt="幻灯片24.png" title="幻灯片24.png" loading="lazy"/></p><p><strong>常见问题排查</strong></p><p>阿里云积累了大量常见问题与排查手段，相关链接已整理。包括 Flink CDC 数据摄入的常见问题与解决方案，涵盖数据库入湖、日志入湖等场景。日志场景最多涉及 Kafka 与 SLS 两类，问题总结包括配置方法、网络联通性、嵌套 JSON 格式解析等，用户可参照文档快速排查。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597694" alt="幻灯片25.png" title="幻灯片25.png" loading="lazy"/></p><h2>四、阿里云企业级Flink CDC联合解决方案</h2><h3>1、湖流一体解决方案：阿里云企业级Flink CDC+Fluss+Flink+Paimon</h3><p>基于 Fluss 加 Flink 加 Paimon 的湖流一体解决方案中，Flink CDC 作为数据接入层，可接入数据库数据、日志数据、OSS 数据（OSS 支持开发中），摄入至 Fluss 与 Paimon。</p><p>对时效性不敏感的业务可直接写入 Paimon，对时效性要求更高的业务先写入 Fluss，通过 Fluss 的湖流一体能力自动将热数据写入 Paimon。Flink CDC 支持直接写入 Fluss 或直接写入 Paimon。用户可基于此方案，结合 OLAP 查询引擎（如 Starrocks、SelectDB 等）完成报表、Dashboard、数据探查、数据分析等应用。</p><p>根据业务场景选择方案：中级时效需求通过数据摄入直接写 Paimon；秒级时效需求先写 Fluss 加速再写 Paimon。端到端实时数仓可达到秒级时效。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597695" alt="幻灯片27.png" title="幻灯片27.png" loading="lazy"/></p><h3>2、金融入湖入仓解决方案：阿里云企业级Flink CDC+ EMR StarRocks +EMR Spark</h3><p>阿里云某金融行业客户案例具有一定代表性。客户原数据架构包含数据采集、数据库、数据应用及离线调度。阿里云基于 Flink CDC 数据摄入对原有方案升级为实时数仓架构，替代自建 Kafka 集群，大幅降低自建 Kafka 集群的管理运维成本。Flink 作业直接采集至 Kafka 后，可通过 Flink SQL 进行实时 ETL、聚合等复杂分析，也可通过 Flink CDC 日志入湖能力将 Kafka 内的 JSON 等日志类型数据直接写入数据湖，再进行后续的计算和分析。</p><p>该方案在客户环境稳定运行一年多。开源方案在企业级场景存在性能瓶颈与运维管理困难，阿里云方案开箱即用，资源弹性几分钟内即可扩展。Flink CDC 采集能力提升 50% 以上，实时计算性能相比开源内核提升 2-3 倍，在大型性能要求极致场景中得到客户认可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597696" alt="幻灯片28.png" title="幻灯片28.png" loading="lazy"/></p><h3>3、智驾实时数据湖解决方案：阿里云企业级Flink CDC+DLF(Paimon)</h3><p>汽车行业尤其是新能源汽车快速发展，阿里云 Flink CDC 与某行业头部客户在自动驾驶场景展开合作。车端数据量巨大，采集后通过 Flink CDC 写入数据库，基于数据库进行模型训练、搜索等自动驾驶业务场景。</p><p>Flink CDC 处于业务链路前端，快速接入端侧数据，后续链路处理能获取更新鲜的数据，业务效果更佳。支持端侧日志数据入湖，数据库数据（关系型 DB、NoSQL DB 如 MongoDB）摄入。开源版本已具备初步能力，企业版进一步优化性能，帮助头部客户快速完成自动驾驶场景数据湖方案建设。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597697" alt="幻灯片29.png" title="幻灯片29.png" loading="lazy"/></p><h2>五、总结</h2><p>阿里云 Flink CDC 数据摄入旨在快速高效智能化地将用户数据写入数据湖与流存储，主要包含两类场景：数据库与日志。</p><p><strong>数据库场景核心能力：</strong>Schema Evolution、表级入湖、整库同步、内置函数与 UDF 处理、数据限流（避免打挂核心业务库）。</p><p><strong>日志场景核心能力：</strong>Schema Inference（从杂乱无章原数据推出表结构和结构化数据）、主键与分区键灵活指定、脏数据处理（日志场景脏数据较为常见）、多表拆分入湖（Kafka Topic 较贵，单 Topic 可能存储数百上千张表数据）、JSON 智能解析（筛选特定字段、字段合并规则、版本号字段映射等）。</p><p>阿里云 Flink CDC 针对数据库与日志场景分别打造企业级核心能力与最佳实践，适用于阿里云 Flink 产品用户或开源用户，均可获得启发与参考。这些最佳实践是云产品孵化过程中踩坑沉淀的结晶，云上用户可获得更多底座能力支持，与兄弟团队云产品 DLF、Fluss、Hologres、Maxcompute、Starrocks 深度融合，打通用户体验，开箱即用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597698" alt="幻灯片30.png" title="幻灯片30.png" loading="lazy"/></p><p>阿里云企业级 Flink CDC 在 Serverless Flink 中可以直接使用，入湖场景支持多种湖格式，已支持 DLF Paimon、DLF Iceberg 和 Fluss 等，对 Paimon 与 Fluss 的支持走在业界前沿。</p><p>实时湖仓场景中，Flink CDC 核心功能为入湖入仓，支持写入 DLF、EMR-Starrocks、Hologres、Maxcompute。湖流一体方案中，Flink CDC 将数据库业务库数据与日志业务日志高效写入 Fluss 流存储，再通过 Fluss 自动同步至 Paimon，形成湖流一体解决方案，在实时湖仓基础上为核心业务提供更高实时性。</p><p>经典实时数仓解决方案中，Flink 与 Hologres 团队合作推出的 Flink CDC 直接写入 Hologres 方案较为经典。Flink CDC 也支持写入 EMR Starrocks，用户可根据偏好选择商业产品或开源产品。无论是实时数仓、湖仓还是湖流一体方案，Flink CDC 数据摄入均能完成方案第一步。</p><p>欢迎大家免费开通 Serverless Flink 来使用企业级 Flink CDC，如需更多交流，可加入阿里云实时计算 Flink 版交流群，开源 Flink CDC 问题可在 Flink CDC 社区群讨论。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597699" alt="幻灯片31.png" title="幻灯片31.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[算力中心散热技术(1) 青山 ]]></title>    <link>https://segmentfault.com/a/1190000047597728</link>    <guid>https://segmentfault.com/a/1190000047597728</guid>    <pubDate>2026-02-06 18:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>一、风冷：算力中心的“传统空调”，可靠但遇瓶颈</strong></p><p>风冷，顾名思义，就是用空气作为散热介质，靠“吹风”带走服务器的热量，原理和我们家用空调、电风扇几乎一致，是目前应用最广泛、最成熟的散热技术，遍布各类中小型算力中心。</p><p><strong>1. 核心原理：</strong></p><p>风冷系统主要由两部分组成：服务器内部的散热风扇，以及机房整体的精密空调或列间空调。服务器运行时，CPU、GPU等核心部件会快速发热，内部风扇会加速转动，将冷空气吸入机箱，冷空气穿过散热片（吸收芯片热量）后，变成热空气被排出机箱；机房的精密空调则负责制造冷风、控制机房温度和湿度，将热空气冷却后循环利用，形成完整的散热闭环。</p><p>简单说，风冷就像给发烧的人吹电风扇，靠空气流动带走体表热量，技术逻辑简单，不需要复杂的管路设计。</p><p><strong>2. 主流类型 </strong></p><p>风冷分为两种常见形式，适配不同场景：</p><ul><li>被动风冷：完全依靠自然对流和散热片散热，没有风扇，静音、无能耗，但散热效率极低，仅适用于发热量极小的低端服务器，现在已基本淘汰。</li><li>主动风冷：就是我们现在最常见的形式，服务器内置风扇、机柜搭配散热风机，机房配备精密空调，可通过调节风扇转速、空调温度，适配不同的发热场景，散热效率比被动风冷高5-10倍，能应对单机柜功率密度<strong>≤10kW的传统算力需求</strong>。</li></ul><ol start="3"><li>优劣势：成熟可靠，但扛不住高密度算力</li></ol><p>风冷能沿用多年，核心优势在于“简单实用”：</p><ul><li>成本低：设备采购、安装和维护都很简单，初期投入少，运维人员上手快；</li><li>可靠性高：没有复杂的管路和液体介质，不存在漏液风险，故障率极低；</li><li>兼容性强：适配所有类型的服务器，改造难度小，存量算力中心升级成本低。</li></ul><p>但随着AI大模型、云计算的爆发，算力密度大幅提升（部分智算中心单机柜功率突破50kW），风冷的短板也越来越明显，逐渐触及物理极限：</p><ul><li>散热效率低：空气的导热系数仅为0.026W/(m·K)，导热能力极差，当机柜功率超过15kW时，多台空调叠加运行也难以降温，甚至会限制服务器性能，被动降频，导致计算效率下降30%；</li><li>能耗极高：制冷系统（精密空调、风扇）的能耗占算力中心总能耗的30%-40%，是名副其实的“能耗黑洞”，导致PUE（能源利用效率）居高不下，南方地区夏季风冷PUE普遍突破1.6，难以满足国家新建数据中心PUE≤1.3的要求；</li><li>噪音大：服务器风扇和机房空调持续运转，噪音可达80分贝，相当于站在马路边，对机房环境要求较高。</li></ul><p><strong>二、液冷</strong></p><p>液体的导热效率是空气的20倍以上，比热容是空气的4倍。现在，液冷已成为高端智算中心、AI训练集群的“首选方案”，能实现PUE低至1.04，较风冷节能40%-50%。</p><p><strong>1. 核心原理</strong></p><p>液冷的核心的是用液体介质（水、矿物油、氟化液等）替代空气，直接或间接接触服务器发热部件，通过液体对流和相变吸热，将热量快速带走，再通过冷却系统将热水（或热液体）降温，循环利用。</p><p>与风冷相比，液冷的核心突破的是取消了高功耗的空调压缩机，改用低功率的闭式冷却塔和冷量分配单元（CDU），制冷系统能耗降低90%以上，从根源上实现节能。</p><ol start="2"><li>主流类型：三种技术，适配不同算力场景</li></ol><p>根据液体与服务器部件的接触方式，液冷主要分为三种，各自有明确的适配场景，目前冷板式和浸没式应用最广泛：</p><p>（1）冷板式液冷：给核心部件“敷冷毛巾”</p><p>这是目前最主流、最易落地的液冷技术，相当于给CPU、GPU等核心发热部件，贴了一块“可循环制冷的冷毛巾”。</p><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnSuo" alt="image.png" title="image.png"/></p><p>原理是将铜或铝制的冷板，紧密贴合在芯片等发热部件表面，冷板内部有密闭流道，乙二醇溶液（防结冰、防腐蚀）在流道内循环，直接吸收芯片热量，再通过管路将热液体输送到冷却模块，降温后循环使用。</p><p><img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdnSue" alt="image.png" title="image.png" loading="lazy"/></p><p>优势是改造无需改变服务器结构，支持“液冷+风冷”混合模式，适配10-30kW/机柜的场景，PUE可降至1.15-1.25，改造周期仅2个月。机柜密度可以提高20kW以上。</p><p>（2）浸没式液冷：</p><p>让服务器泡冷水澡，相当于把整个服务器，放进一个装满特殊冷却液的“浴缸”里，全程浸泡散热。</p><p><img width="723" height="219" referrerpolicy="no-referrer" src="/img/bVdnSuu" alt="image.png" title="image.png" loading="lazy"/></p><p>所用的冷却液（矿物油、氟化液）是绝缘、无毒、不导电的，不会对服务器部件造成损坏。服务器完全浸没在冷却液中，运行时产生的所有热量，都会被冷却液直接吸收，冷却液吸热后会自然对流，将热量传递到容器壁，再通过外部冷却系统降温，部分还能利用冷却液的相变（液体变气体），实现高效吸热。</p><p>这种方式的散热效率是冷板式的2-3倍，适配30-100kW/机柜的高密度智算场景，PUE可低至1.05-1.15，几乎没有风扇噪音（可低至45分贝），还能大幅节省机房空间。中兴通讯怀来项目部署48kW机柜，年节电超110万度，CO₂减排900吨；华为全液冷方案在50kW机柜上，年省50万度电，减排237.5吨。</p><p>（3）两相液冷：</p><p>这是更先进的液冷技术，基于航天级相变原理，利用液体气化时的潜热换热，散热效率是风冷的1000倍以上，能应对100kW以上的极端算力场景。</p><p><img width="723" height="221" referrerpolicy="no-referrer" src="/img/bVdnSvb" alt="image.png" title="image.png" loading="lazy"/></p><p>原理是让冷却液在发热部件表面沸腾，从液体变成气体，这个过程会吸收大量热量，气体上升后遇到冷却管，再凝结成液体，循环往复。塔能科技泵驱两相系统实现PUE≤1.12，某南方电信机房改造后PUE从1.8降至1.196，制冷负载系数（CLF）仅0.036。</p><ol start="3"><li>优劣势：高效节能，但门槛较高</li></ol><p>液冷的核心优势，完美解决了风冷的痛点，适配算力爆发的需求：</p><ul><li>散热效率极高：液体导热能力远超空气，能轻松应对高密度算力的发热需求，避免服务器因过热宕机；</li><li>能耗极低：制冷系统能耗大幅降低，能实现PUE≤1.1，部分项目可达1.04，符合国家“双碳”和节能政策；</li><li>噪音小、稳定性强：减少或取消风扇，机房噪音大幅降低；同时液体温度波动小，能将芯片温度控制在55℃以下，较风冷低15℃，芯片故障率下降30%，服务器寿命延长2-3年；</li><li>可回收余热：液冷系统回收的高温冷却液，可用于机房供暖、热水供应等场景，实现能源二次利用，北方某智算中心通过余热回收，额外实现年节能15%。</li></ul><p>但液冷也有明显的短板，限制了其快速普及：</p><ul><li>初期成本高：设备采购、管路铺设、机房改造的投入，比风冷高10%-30%，冷却液（尤其是氟化液）价格较高；</li><li>运维难度大：需要专业的运维人员，负责监测管路漏液、冷却液更换和补充，漏液若未及时发现，可能损坏服务器；</li><li>兼容性有限：浸没式液冷需要专用服务器，无法直接适配传统风冷服务器，改造存量机房的成本较高。</li></ul><p><strong>三、风冷vs液冷：</strong></p><p>很多人会觉得，液冷崛起后，风冷就会被淘汰，但实际上，两者并不是“非此即彼”的关系，而是根据算力需求，形成互补共生的格局。简单来说：低算力、低成本需求，风冷依然是最优选择，比如中小型企业的算力节点、传统办公用的服务器机房，风冷的可靠性和低成本足以满足需求；高密度、高节能需求，液冷是必然趋势，比如AI大模型训练中心、大型云厂商的算力集群，液冷能破解散热和能耗困局，长期来看能节省大量电费，2年左右即可回收初期额外投入。</p><p><strong>四、未来趋势：液冷普及加速，风冷持续优化</strong></p><p>随着“东数西算”工程推进，以及国家对数据中心PUE的严苛要求（2025年新建数据中心PUE≤1.3），液冷技术的普及速度会越来越快。行业趋势显示，液冷在算力中心的占比，将从2025年的15%升至2030年的50%，标准化也会加速，未来会出台液冷系统设计、测试的统一规范。</p><p>同时，液冷技术也在不断升级：漏液检测技术越来越精准（可实现秒级响应），冷却液成本持续下降（规模化采购可降低40%），国产化替代加速，华为、塔能等企业已实现冷板、工质、控制算法全链条自主可控，打破国外技术垄断。</p><p>而风冷也不会被淘汰，而是会持续优化——比如优化风扇转速调节、改进散热片设计、采用间接蒸发冷却技术，提升散热效率、降低能耗，适配中低端算力需求，与液冷形成“高低搭配”，共同支撑算力时代的发展。</p>]]></description></item><item>    <title><![CDATA[从银行被罚谈起：金融行业数据安全正在走向“精细化治理” 全知科技 ]]></title>    <link>https://segmentfault.com/a/1190000047596698</link>    <guid>https://segmentfault.com/a/1190000047596698</guid>    <pubDate>2026-02-06 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047545249" alt="图片" title="图片"/><br/>近期，金融监管部门公布了多起银行因数据管理、数据安全相关问题被处罚的案例。其中，浙江绍兴某农商银行因违反数据安全与网络安全管理规定等多项要求，被处以较大金额罚款；邮储银行某分行也因“数据管理不审慎”等问题受到行政处罚。这些事件再次表明，在金融业务高度数字化、系统高度互联的背景下，数据已经不仅是资产，更是必须被严格管理与持续审计的核心对象。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596700" alt="图片" title="图片" loading="lazy"/><br/>从监管逻辑看，当前对金融机构的要求，已经从“有没有制度”转向“有没有能力”，从“事后追责”转向“事前防控”。尤其是在接口化、平台化、生态化的金融IT架构中，数据的流动越来越依赖API接口完成，数据接口是否可控、可审计、可溯源，正成为金融数据安全治理的新焦点。在实际业务运行中，金融机构的数据流转大多通过各类API接口完成：核心系统与业务中台、渠道系统、外部平台之间，持续进行高频的数据交互。数据接口越多、调用越频繁，系统运行效率越高，但同时，风险也越隐蔽。很多机构面临的现实情况是：数据在不停地“跑”，但数据接口调用的行为是否合理、是否越权、是否存在异常模式，却难以及时、全面地掌握。传统安全体系更多关注网络边界、主机、数据库本身，对“数据接口层面的行为风险”缺乏持续、智能的监测手段，容易形成监管与实际运行之间的“能力断层”。当数据接口成为数据流动的主要通道时，如果缺乏系统化、可视化的风险感知能力，就很难真正做到数据使用的可控与合规。</p><h3><strong>知影-API风险监测系统——让数据接口安全真正“看得见、管得住”</strong></h3><p>正是基于金融行业对数据接口安全的现实需求，全知科技推出了「知影-API风险监测系统」。该系统面向金融机构API接口场景，构建以“行为感知 + 风险识别 + 可视治理”为核心的新一代API安全监测平台，帮助机构实现从“被动防护”向“主动风控”的转变。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596701" alt="图片" title="图片" loading="lazy"/></p><h4><strong>1、核心功能：基于智能算法的多维风险监测</strong></h4><p>「知影-API风险监测」模块是系统的核心能力，融合了多维检测模型、动态基线分析和自适应防护策略三大技术体系，实现从“识别”到“处置”的全流程防护。<br/><strong>API识别与梳理：摸清资产“家底”</strong><br/>支持RESTful、SOAP等主流格式，API识别准确率超98%；实时洞察敏感信息，覆盖多领域数据标签并动态更新暴露面；通过独创算法完成API四级分类分级，聚焦高风险资产；自动跟踪API全生命周期状态，确保资产清单同步。<br/><strong>弱点检测：提前堵住漏洞</strong><br/>集成OWASP API Top10，内置50+项弱点规则；精准识别逻辑异常、硬编码密钥等风险，结合数据泄露行为分析影响面；提供弱点测试与修复建议，实现闭环整改。<br/><strong>风险防护：动态拦截威胁</strong><br/>基于API画像实时监测异常行为；依托自研引擎构建三大维度风险规则，秒级识别适配多行业；自动建立并调整智能基线降低误报，支持自定义指标，可对恶意IP阻断、限流。<br/><strong>审计溯源：实现责任</strong><br/>可追结构化提取关键信息，平衡存储效率与溯源需求；提供线索、主体双模式溯源，1小时内定位责任方。<br/><strong>多节点管理：适配复杂部署</strong><br/>支持多城市、多机房跨地域部署及流量本地化处理；节点数据汇聚中心节点，实现资产、风险统一管理与配置下发，降低运维成本。2、产品优势：轻量高效、智能联动的防护体系<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596702" alt="图片" title="图片" loading="lazy"/><br/><strong>AI驱动更智能：</strong>融合大模型技术，实现资产识别、风险降噪、策略调整自动化，响应时间从小时级缩至秒级。<br/><strong>覆盖场景更全面：</strong>适配互联网、生产网、办公网等多元环境，支持金融、运营商等多行业需求。<br/><strong>合规适配更精准：</strong>满足《数据安全法》《个人信息保护法》及行业规范，日志留存、审计溯源符合合规要求。<br/><strong>联动能力更强：</strong>可与全知自有产品（数据资产地图、数据脱敏系统）及第三方安全工具联动，形成全域防护网。</p><p>随着金融业务不断向平台化、生态化发展，数据安全治理正在从单点技术建设，走向体系化能力构建。数据接口不只是连接系统的“管道”，而是数据合规与风险控制的核心节点。只有把API风险监测纳入整体数据安全治理框架，与数据分类分级、数据库审计、日志审计、合规管理等能力协同联动，金融机构才能真正构建起“看得见流动、控得住使用、管得住风险”的数据安全底座。而「知影-API风险监测系统」，正是这一治理体系中不可或缺的“感知层”和“前哨站”。</p><p>面向金融行业数据要素流通与风险治理不断深化的新阶段，全知科技将持续围绕核心场景需求，深化数据接口安全技术与AI能力的融合应用，在“看得见风险”的基础上进一步实现“预判风险、降低风险、治理风险”。值得一提的是<strong>，由全知科技牵头，公安部第三研究所、中国电子技术标准化研究院 、国家信息中心 、中国信息通信研究院等制定的 《数据安全技术 数据接口安全风险监测方法》国家标准已正式发布</strong>，将多年技术积累与一线场景经验转化为行业通用的方法论与规范框架。未来，在标准与AI双轮驱动下，全知科技将不断夯实金融行业数据接口安全与数据治理的技术底座，助力金融机构在数字化与合规要求持续升级的环境中，实现真正可控、可视、可持续的数据安全治理。</p>]]></description></item><item>    <title><![CDATA[别被功能表迷惑！终极拷问：红圈跟明建云哪个好，关键看它解决了哪个“人”的问题？ 看点 ]]></title>    <link>https://segmentfault.com/a/1190000047597168</link>    <guid>https://segmentfault.com/a/1190000047597168</guid>    <pubDate>2026-02-06 17:05:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>深夜11点,某中型建筑公司的总经理李总,正对着电脑屏幕上密密麻麻的Excel表格皱眉。这是他为明天季度经营会准备的“项目经营分析”,数据来自成本部、物资部、工程部七八个不同格式的报表。</p><p>“成本超支3.2%,但物资报表显示采购节约了5%,这数据对不上啊……”他揉了揉太阳穴,知道明天的会议,又将陷入长达两小时的数据核对与拉扯中。</p><p>而同一时间,公司的采购主管小张,正在某查查网站上,手动翻看着一家潜在供应商的司法风险、经营异常信息。她需要为这份评估报告附上自己的“主观判断”,但心里直打鼓:“信息不全,万一踩雷了怎么办?”</p><p>这,就是当下无数工程企业数字化转型中,最真实、也最容易被忽略的切面。当我们谈论选择“红圈”还是“明建云”时,绝大多数决策者还停留在功能列表的比对:谁的功能模块多?谁的界面更美观?谁的价格更优惠?</p><p>但一个灵魂拷问往往被淹没在参数里:这款软件,究竟解决了我们公司里,哪个具体“人”的痛点?是让总经理决策更安心了,还是让采购员睡觉更踏实了?</p><p>今天,我们就抛开冰冷的功能清单,潜入工程企业的真实场景,看看当红圈遇上明建云,关键分水岭不在“事”的管理,而在对“人”的赋能。</p><p>在为经营会头疼?你可能缺个能“读心”的AI军师</p><p>工程企业的老板或高管,最缺的是什么?时间。最烦的是什么?开会。</p><p>传统经营会的困境堪称经典:会前,项目人员整理数据容易出现漏、误、错,汇报内容与实际情况偏差大,准备仓促缺少结构化分析。会上,大量时间耗费在矫正数据和集体分析问题上,而非形成有效决议。会后,行动计划执行与否、结果如何,难以跟踪。</p><p>问题的核心是:管理者被困在数据迷雾里,缺少一双能瞬间穿透表象、直抵核心的“眼睛”。</p><p>红圈的思路是,不给管理者增加阅读负担,而是直接给他一个“智能指挥官”。这个就是红圈AI的“项目360°AI解读”,像一位永不疲倦的经营分析师。它通过整合全维经营指标(资金/成本/合同/付款等),一键生成项目全景作战图,并借助大模型深度解读经营风险与应对策略。其目标是让复杂数据转化为清晰决策语言,让管理全局尽在掌握。</p><p>管理者获得的不是简单报表,而是一份带有 “AI智能评级” (对项目经营状况综合评定后智能分级)和 “AI经营分析” (用3句以内描述描述核心问题)的洞察报告。更重要的是,它能围绕项目风险及问题,匹配业务管理实践方案,提供可落地的改进建议,将研讨快速转变为行动。这背后,是AI在调用行业专家经验积累进行诊断。</p><p>那么,以流程稳健著称的明建云呢?</p><p>明建云能出色地完成一项基础且至关重要的工作:把经营数据规范、清晰、实时地呈现出来。它能够帮助管理者实时了解项目整体资金状况是否安全、经营风险是否可控这给了管理者一双“看得清”的眼睛,解决了信息不透明的问题。</p><p>但红圈AI想给的是 “看得懂且会思考” 的眼睛。一个帮你“呈现数据”,一个试图帮你“分析数据并给出策略雏形”。对于日理万机、需要在信息洪流中快速抓住关键的管理者而言,后者解决的不仅仅是“看”的问题,更是 “判断”和“决策启动”效率的问题,能够让经营决策效率提升10倍。当你的软件开始尝试帮你思考业务,这已经是从“工具”到“伙伴”的跨越。</p><p>供应商评估还在“凭感觉”?小心埋下赔光利润的雷</p><p>在工程行业,一次失败的供应商选择,足以拖垮一个项目的利润。传统采购评估,高度依赖个人经验和零散查询,像在雷区里凭感觉走路。</p><p>采购员小王的日常:收到一份新供应商资料,打开数个网页,手动查询工商信息、法律诉讼、失信记录……这个过程,耗时、不全面,且充满人工主观误差。</p><p>红圈AI的“采购助理Agent”,目标就是成为小王的“外挂大脑”。</p><p>它的工作流程极具冲击力:整合多维度供应商企业数据并通过AI算法智能动态评分,3秒完成信用数据抓取,40秒AI完成各风险排查及评估,10秒生成完整报告。报告不仅给出“企业得分:44,风险等级:高风险,合作建议:建议终止合作或高度谨慎合作”的结论,还会用精炼的语言列出致命伤:“企业存在破产案件记录;被法院列为限制高消费企业;存在多起终本案件;因未按规定提交年度报告信息而被列入经营异常名录……”。这等于将风控专家经验与多维数据挖掘能力,压缩成了一个自动化流程。</p><p>明建云在采购环节能做什么?</p><p>它能很好地构建一个 规范化、可追溯的采购流程管理体系。作为专业的工程项目管理系统,其功能模块涵盖招采管理,能够确保采购动作的合规与高效,解决了“怎么买”的流程问题。</p><p>而红圈AI解决的,是更前端的 “向谁买”的本质安全问题。一个守护 “流程的合规”,一个狙击 “源头的风险”。对于采购员而言,明建云让他的工作更规范;而红圈AI则在努力让他避免职业生涯的“惊天大雷”,快速筛选优质供应商、实时监测潜在风险。当软件开始为你规避系统性风险时,它守护的不仅是公司资产,更是每个相关岗位从业者的职业安全。</p><p>项目部的“表哥表姐”,你们被Excel绑架多久了?</p><p>去过工地的人都知道,项目部的桌子上堆满了各种单据:手写的混凝土签收单、机打的送货单、手写确认单甚至外文单据……项目材料员、成本专员的大量时间,就消耗在将这些信息逐字敲进系统,比如手动处理合同文本、结算单信息或出入库单。</p><p>这个工作枯燥、易错、价值感低,却至关重要。传统软件的解决方案,是设计更友好的录入界面,但这依然没有改变 “人服务于系统” 的本质。</p><p>红圈AI的 “录单助手 Agent pro” (或称“AI录单助手”)提出了一个想法:为什么不能是系统主动理解并服务于人? 它通过大模型自动识别各类单据,实现从图像识别到高质量系统录入的秒级闭环,减少90%人工操作。无论是合同、结算单还是出入库单,系统能智能提取关键字段、智能匹配相关数据(如合同明细)并回填业务系统。更关键的是,它能智能分析入库材料匹配的合同明细并挂接,厘清成本发生源头,实现后期实际成本精准统计及溯源。项目人员从“数据搬运工”,变成了“流程确认官”。</p><p>明建云在单据处理上自然不会缺席。</p><p>它通常提供完善的单据管理模块,支持创建、审批、关联、归档全流程。手机端拍照上传附件、在线填写表单已是标配,这已经比传统纸质流转先进了无数倍,实现了单据的 “线上化与协同化”。</p><p>但两者的哲学在此有所区别:明建云在优化“录入的流程”,而红圈AI在挑战“录入”这个动作本身的存在必要性。 一个提供了更先进的“笔和纸”,另一个则试图给你一支“自动书写笔”。对于常年被重复劳动折磨的一线项目人员来说,谁能真正解放他们的双手和精力,让他们去做更有价值的现场协调与管理,谁的吸引力无疑是指数级上升的。</p><p>老师傅退休,经验就带走?你的公司需要一个“永生大脑”</p><p>工程企业的核心资产,除了设备和资质,就是那些藏在老师傅脑子里、散落在历年项目硬盘中的经验:某个特殊工艺工法、一份成功的投标策略、一次诉讼的关键判例、公司的差旅报销制度……</p><p>但这些知识,大多处于“沉默”状态。新员工问不到;投标时想找历史方案,得在共享盘里大海捞针;法务遇到纠纷,手动检索相关判例效率极低。</p><p>红圈AI的 “AI企业知识库” ,想成为企业的 “知识中枢”。它不仅仅是一个云盘,而是一个能用自然语言对话的“智慧专家”。通过大模型与智能检索技术,它将分散知识转化为即问即答的能力。</p><p>员工可以像聊天一样提问:“去哈尔滨住410块的酒店可以么?”“我的年假有多少天?”系统能在3秒内获取精准答案。它甚至能为商务部快速检索并整合历史投标成果,涵盖中标/未中标标书、技术方案等;为法务部构建专属的“诉讼智库”,涵盖判决书、律师函等所有相关资料。这大幅降低了新人培养周期,让企业核心经验传承提升3倍。</p><p>明建云如何管理知识?</p><p>作为项目核心平台,它自然是 项目资料归档和共享的最佳场所。所有与项目相关的文档都能存储在对应项目下,权限清晰,查找方便。它建立了一个有序的、项目维度的 “知识档案馆”。</p><p>红圈AI企业知识库所做的,是在档案馆之上,加装了一个 “超级智能检索员”和“金牌解说员”。前者解决了知识“存而不易找”的问题,后者解决了“找到而不易读懂用活”的问题。当新员工能随时向AI询问公司制度并得到精准解读,当项目经理能瞬间获得历史上类似风险的全部处理方案时,软件解决的就不再是信息存储问题,而是组织智慧的传承与激活问题,是团队整体学习曲线和应变能力的飙升。</p><p>你的选择,暴露了你的管理优先级</p><p>行文至此,我们可以清晰地画出一条分界线:</p><p>明建云,像一位严谨的“流程架构师”。它致力于为工程企业搭建一个标准化、可视化、强协同的数字管理基座。它把混乱的线下流程变得有序在线,让数据得以归集,让协作得以穿透部门墙。如果你的企业正处于数字化转型的初级阶段,迫切需要把管理规范立起来、把数据黑洞照亮,那么明建云代表的是一条稳健、成熟、已被验证的路径。</p><p>红圈(红圈工程项目管理系统 + AI系列智能产品),则像一位“角色赋能大师”。它在坚实的项目业务管理基础上,涵盖项目资金管理、成本管理、物资管理、劳务管理等功能,将一系列AI能力精准注入管理者、采购员、项目人员、新员工等关键角色的高频痛点场景中。它不满足于只管理好“事”,更执着于提升每个核心岗位上的“人”的决策质量、风险免疫、工作效率与知识获取速度。其AI系列智能产品,如BOSS助理Agent、采购助理Agent、录单助手Agent pro、项目360°AI解读、AI报表助手、AI企业知识库以及AI业务助手等,共同构成了一个更懂工程企业经营的赋能体系。</p><p>所以,回到那个终极拷问:红圈和明建云哪个好?</p><p>答案不在于软件本身,而在于你更想优先解决哪个层面的问题。</p><p>如果你认为当前最大的瓶颈是流程的规范化与标准化,那么请关注谁能把“事”理得更顺。</p><p>但如果你觉得,流程已经初步在线,而更深层的焦虑在于:高管如何从繁杂数据中快速洞察?业务如何防范未知风险?一线员工如何从重复劳动中解放?组织经验如何不随人员流失而消散?——那么,红圈所展现出的“以AI深度赋能具体角色”的思路,无疑指向了一个更锐利、更面向未来的答案。</p><p>最终,最好的软件,是那个能让你公司里最重要的那些“人”,工作得更强大、更安心、更有价值的那一个。这场选择,实则是对你管理哲学和公司下一阶段重心的一次隐秘洞察。</p>]]></description></item><item>    <title><![CDATA[OpenClaw对接聊天APP及AI助手工具 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047597182</link>    <guid>https://segmentfault.com/a/1190000047597182</guid>    <pubDate>2026-02-06 17:04:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OpenClaw对接聊天APP及AI助手工具</p><h2><strong>1、对接飞书聊天APP</strong></h2><h3>openclaw配置</h3><p>此处以飞书为例，输入插件下载安装命令：</p><pre><code>openclaw plugins install @m1heng-clawd/feishu  
cd /root/.openclaw/extensions/feishu/ 
npm install --verbose  #此步骤安装时间较强长，请耐心等待。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597184" alt="图片.png" title="图片.png"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597185" alt="图片.png" title="图片.png" loading="lazy"/></p><p>等下载安装完成后。在命令行下运行openclaw图形配置界面。</p><pre><code>open channels add
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597186" alt="图片.png" title="图片.png" loading="lazy"/></p><p>回车</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597187" alt="图片.png" title="图片.png" loading="lazy"/></p><p>回车后，根据提示输入飞书App ID和App Secret。</p><p>在飞书开放平台应用凭证获取，打开飞书（<a href="https://link.segmentfault.com/?enc=R3btK7onxocB%2BeS6nmlOvQ%3D%3D.Mz6fd1%2Bw5TfGKQnbGD7YcKKIejoAELu0CdiSFw56Jkg%3D" rel="nofollow" target="_blank">https://open.feishu.cn/app/</a>），点击左边凭证基础信息，在页面中找到 “App ID” 和 “App Secret” 两个数据，分别点击右侧 “复制” 按钮，将其复制到openclaw配置界面。飞书配置参考下一章节。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597188" alt="图片.png" title="图片.png" loading="lazy"/></p><p>输入应用凭证后，回车完成配置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597189" alt="图片.png" title="图片.png" loading="lazy"/></p><p>openclaw gateway restart ，重启网关服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597190" alt="图片.png" title="图片.png" loading="lazy"/></p><p>运行openclaw status，查看状态是否正常。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597191" alt="图片.png" title="图片.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597192" alt="图片.png" title="图片.png" loading="lazy"/></p><h3><strong>验证飞书接入</strong></h3><ol><li>打开手机或电脑端飞书 APP，登录与飞书开发者平台相同的账号；</li><li>在飞书首页，找到 “工作台” 入口，点击进入，在工作台列表中找到已发布的 openclaw 应用（如 “openclaw 助手”），点击进入；</li><li>系统将自动启动私聊窗口，发送任意消息（如 “你好”“查询今日日程”）；</li><li>验证结果：如果收到 openclaw 的回复，即为飞书接入成功</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597193" alt="图片.png" title="图片.png" loading="lazy"/></p><p>以上配置完成后，即可上手使用。快上京东云开启一键部署，让 openclaw 成为你专属的 24 小时数字员工！</p><h3><strong>飞书机器人配置</strong></h3><p><strong>创建飞书企业自建应用</strong></p><ol><li>访问飞书开发者平台（<a href="https://link.segmentfault.com/?enc=WGAtLg781b4D885UV8Jglg%3D%3D.fRQrjv1L9y3jIKIWYfkd5NALQYCxZhX8wT4kb5um5JbWuobz%2FEo91fZfVQu23IeZ" rel="nofollow" target="_blank">https://open.feishu.cn/app?lang=zh-CN）；</a></li><li>登录后，点击 “创建企业自建应用”；</li><li>填写应用基础信息：应用名称（如 “openclaw 助手”），选择应用图标，点击 “创建” 按钮，进入应用管理页面。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597194" alt="图片.png" title="图片.png" loading="lazy"/></p><ol><li>添加机器人能力：在应用管理页左侧导航栏，找到并点击 “添加应用能力”，在弹出的列表中选择 “机器人”，点击 “添加”。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597195" alt="图片.png" title="图片.png" loading="lazy"/></p><ol><li>点击上方的创建版本并发布。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597196" alt="图片.png" title="图片.png" loading="lazy"/></p><p>飞书应用权限与事件配置</p><p>此步骤分为「事件配置」「权限配置」「应用发布」三部分，全程在飞书开发者平台操作，按顺序执行：</p><h3>（1）事件配置</h3><ol><li>在飞书应用管理页，左侧导航栏找到 “事件与回调” 栏目，点击进入；</li><li>事件配置：在页面中选择 “长连接”，点击 “保存”；</li></ol><p>⚠️如果这一步提示 <strong>“未建立长连接”，</strong> 请检查自己的APP ID和APP Secret是否已正确配置。</p><p>（操作见上一步：添加飞书channel配置-配置APP ID与APP Secret）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597197" alt="图片.png" title="图片.png" loading="lazy"/></p><ol><li>添加事件：点击页面中的 “添加事件”，在弹出的事件列表中，选择 “消息与群组” 分类，勾选 “接收消息”，点击 “确定”，完成事件订阅。</li></ol><p>!<a href="" target="_blank"/></p><ol><li>回调配置：订阅方式选择 “使用长连接”，无需填写其他地址，配置自动生效。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597198" alt="图片.png" title="图片.png" loading="lazy"/></p><h3>（2）权限配置</h3><ol><li>在飞书应用管理页，左侧导航栏找到 “权限管理” 栏目，点击进入；</li><li>点击页面中的 “批量导入权限” 按钮，弹出权限导入窗口；</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597199" alt="图片.png" title="图片.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597200" alt="图片.png" title="图片.png" loading="lazy"/></p><p>3.复制以下 JSON 代码，粘贴到导入窗口的输入框中，点击 “导入” 按钮，等待权限导入完成：</p><p>代码语言：txt</p><p>AI代码解释</p><pre><code>{  
 "scopes": {     "tenant": [       
 "contact:user.base:readonly",    
    "im:chat",       
    "im:chat:read",     
    "im:chat:update",      
    "im:message",       
    "im:message.group_at_msg:readonly",       
    "im:message.p2p_msg:readonly",       
    "im:message:send_as_bot",       
    "im:resource"     ],     
    "user": []   } }
</code></pre><p>4.导入验证：页面显示 “导入成功”，且权限列表中出现上述导入的权限，即为配置完成。</p><h3>（3）发布应用</h3><ol><li>在飞书应用管理页，左侧导航栏找到 “版本管理与发布” 栏目，点击进入；</li><li>点击右上角的新建版本；</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597201" alt="图片.png" title="图片.png" loading="lazy"/></p><p>填写版本号与描述后，保存并发布；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597202" alt="图片.png" title="图片.png" loading="lazy"/></p><h2><strong>2、更换模型</strong></h2><p>openclaw onboard</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597203" alt="图片.png" title="图片.png" loading="lazy"/></p><p>按键盘上左右方向键，选择到“Yes”后，按回车键开始配置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597204" alt="图片.png" title="图片.png" loading="lazy"/></p><p>Onboarding mode 选 QuickStart。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597205" alt="图片.png" title="图片.png" loading="lazy"/></p><p>选择Qwen等模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597206" alt="图片.png" title="图片.png" loading="lazy"/></p><p>回车确认。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597207" alt="图片.png" title="图片.png" loading="lazy"/></p><p>复制地址在浏览器中打开，后登录账号确认通过。（需要先注册账号<a href="https://link.segmentfault.com/?enc=FVfkW5sD9YO87QZkTN2osw%3D%3D.h0ODPqPxin1wN%2FfOlKLkXzF4nYTd%2BSnn%2BbChFvEXvoM%3D" rel="nofollow" target="_blank">https://chat.qwen.ai/</a>）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597208" alt="图片.png" title="图片.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597209" alt="图片.png" title="图片.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597210" alt="图片.png" title="图片.png" loading="lazy"/></p><p>选择默认模型后，回车确认。选择选择重启网关服务后即生效。</p><p>技能包（Skill）配置</p><p>新手暂时无需添加额外技能包，选中 “No” 跳过，按回车键确认；</p><p>Hooks 配置</p><p>选中 “session-memory”（启用记忆功能，支持多轮对话上下文关联，避免每次聊天都需要重复说明需求），其他两项可根据需求选择。按回车键确认；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597211" alt="图片.png" title="图片.png" loading="lazy"/></p><h2>3、常用命令汇总</h2><table><thead><tr><th>查看版本</th><th>openclaw --version</th></tr></thead><tbody><tr><td>查看运行状态</td><td>openclaw status</td></tr><tr><td>启停Bot</td><td>openclaw start/stop/restart</td></tr><tr><td>启动openclaw配置</td><td>openclaw onboard</td></tr><tr><td>启动Openclaw的TUI</td><td>openclaw tui</td></tr><tr><td>通过SSH连接TUI</td><td>ssh root@公网IP</td></tr><tr><td>重启网关</td><td>openclaw gateway restart</td></tr><tr><td>查看已安装插件</td><td>openclaw plugins list</td></tr><tr><td>安装插件</td><td>openclaw plugins install 插件名</td></tr><tr><td>更新插件</td><td>openclaw plugins update 插件名</td></tr><tr><td>卸载插件</td><td>openclaw plugins uninstall 插件名</td></tr><tr><td>查看帮助</td><td>openclaw --help</td></tr></tbody></table>]]></description></item><item>    <title><![CDATA[OpenClaw闪电部署，立即体验AI助手 京东云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047597213</link>    <guid>https://segmentfault.com/a/1190000047597213</guid>    <pubDate>2026-02-06 17:04:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>OpenClaw是少数真正具备系统级操作能力的个人智能助理：</p><ul><li>本地长期记忆（对话数据自主可控）</li><li>跨平台推送（支持对接通讯类软件）</li><li>真实任务执行（文件整理、表单提交、代码生成）</li></ul><p>这样一个强大又敏感的 AI 助手，该运行在哪里？更安全、更稳定的方式，是将其部署在独立、隔离的云端环境中。</p><p>京东云全面上线 openclaw(Clawbot)！京东云轻量云主机现已预置 openclaw 应用镜像，无需手动配置环境，2步即可完成部署，让你的 AI 助手秒级上线、全天候待命。</p><h2><strong>第一步：一键创建实例</strong></h2><p>在京东云控制台创建轻量云主机，选择：</p><p>镜像名称：openclaw（Clawdbot）</p><p>规格建议：2 核 2G 起 <strong>（推荐 2 核 4G 以获得更佳体验）</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597215" alt="图片.png" title="图片.png"/></p><p>创建成功后，进入云主机列表页，点击右边<strong>操作列</strong>的【查看】，打开18789端口。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597216" alt="图片.png" title="图片.png" loading="lazy"/></p><p>点击【防火墙】，打开后点击【添加规则】。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597217" alt="图片.png" title="图片.png" loading="lazy"/></p><p>在目标端口中输入18789，源IP输入0.0.0.0/0，点击【保存】。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597218" alt="图片.png" title="图片.png" loading="lazy"/></p><p>再次转到轻量云主机列表，<strong>记录下公网IP地址</strong>，点击【登录】。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597219" alt="图片.png" title="图片.png" loading="lazy"/></p><p>通过WebTerminal登录云主机。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597220" alt="图片.png" title="图片.png" loading="lazy"/></p><p>进入配置界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597221" alt="图片.png" title="图片.png" loading="lazy"/></p><h2><strong>第二步：配置openclaw</strong></h2><p>通过webTerminal登录系统后，执行下面的命令。</p><p>bash init-openclaw.sh 18789 pk-03bf0b76-8326-4ad7-91bf-293c5dd35c8d 公网IP    </p><p>\#红色部分替换成你的API-KEY，API-KEY获取步骤参考下方【<strong>获取API-KEY</strong>】章节。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597222" alt="图片.png" title="图片.png" loading="lazy"/></p><p>回车后，生成可以直接访问的地址【<a href="https://link.segmentfault.com/?enc=LaABu5MmrcKPP5ig2QfiKg%3D%3D.D4yI%2FBC8pmQJF2BHP8KhccuBJ0eDQRZjNJfKc9NC8E1rc9Zlw5iWhaRBZ3Os%2FJLY" rel="nofollow" target="_blank">http://公网IP:18789?tokan=字符串</a>】。复制在Web浏览器中回车打开。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597223" alt="图片.png" title="图片.png" loading="lazy"/></p><p>出现openclaw的web交互网站，可以直接和AI助手对话了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597224" alt="图片.png" title="图片.png" loading="lazy"/></p><h3><strong>获取API-KEY</strong></h3><p>登录京东云JoyBuilder 模型开发平台 2.0，进入API-KEY 管理页面：</p><p>（<a href="https://link.segmentfault.com/?enc=H8Vch2JFL5%2B7CBjIePrGoA%3D%3D.JJaU5ZA7MiFEIq94%2Fvj3reMQStt8j5gQ6MAP2fEYyT0FjbiBYa9d725N58HwjQiIcRPRRecoJfdRqKGjDNr1kw%3D%3D" rel="nofollow" target="_blank">https://joybuilder-console.jdcloud.com/system/api-key/list</a>）</p><p>1、点击【创建】按钮，生成新的 API-Key；</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597225" alt="图片.png" title="图片.png" loading="lazy"/></p><p>2、选择长期有效，点击【确定】。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597226" alt="图片.png" title="图片.png" loading="lazy"/></p><p>3、生成长期的API-KEY，点击右边复制图标复制API-KEY。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597227" alt="图片.png" title="图片.png" loading="lazy"/></p><p>4、在“修改 API-KEY”页面中，为该 Key 配置可访问的模型服务，必须包含以下模型：DeepSeek-V3-0324、DeepSeek-V3.2、DeepSeek-v3、Qwen3-235B-A22B、Kimi-K2-0905-jcloud</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597228" alt="图片.png" title="图片.png" loading="lazy"/></p><p>5、选择需要的模型，或者全部选上。点击【确定】完成模型绑定。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597229" alt="图片.png" title="图片.png" loading="lazy"/></p><p>6、为了保证模型正常工作，需要至少保证账户中有金额。点击充值完成京东云账号充值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597230" alt="图片.png" title="图片.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[人员定位管理系统怎么选？从实际管理需求看清三种主流模式 果断的小刀 ]]></title>    <link>https://segmentfault.com/a/1190000047597259</link>    <guid>https://segmentfault.com/a/1190000047597259</guid>    <pubDate>2026-02-06 17:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：王博涵 小步外勤产品总监，外勤管理数字化专家</p><p>在外勤人员占比较高的企业中，管理难点往往并不在于制度本身，而在于执行过程是否真实可控。</p><p>员工每天在外奔波，管理层却很难持续判断<strong>行程是否合理，现场是否到位，费用是否按实际发生</strong>。</p><p>过去数年里，定位打卡曾一度成为外勤管理的常用手段。但随着业务规模扩大，这种以点位记录为主的方式，很难支撑过程管理。定位无法连成轨迹，停留无法量化，外勤行为仍然处在不可验证的状态。</p><p>故而近几年越来，越多的企业开始将<strong>人员定位管理系统</strong>引入到外勤管理体系中，用更连续、更真实的数据，支撑人效管理和费用管控。</p><p>从实际应用情况来看，目前人员定位管理系统主要呈现出三种不同的发展方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597261" alt="" title=""/></p><h2>当前人员定位管理系统的三种常见形态</h2><h3><strong>第一类 偏基础的定位考勤系统</strong></h3><p>这一类系统多是从行政考勤延伸而来的，常见于通用办公平台中。其主要功能便是<strong>记录上下班位置，配合简单的地理围栏</strong>，解决是否到岗的问题。</p><p>这类系统在人员规模较小、外勤占比不高的企业中有一定的实用价值。但在实际外勤场景中，它们往往<strong>只能记录零散的定位点，无法形成完整轨迹</strong>，也无法判断员工在客户现场的真实停留情况。</p><p>同时，<strong>防作弊能力相对有限</strong>，对虚拟定位、位置修改等行为缺乏有效识别手段。</p><p>因此，这类产品更适合作为<strong>基础考勤工具</strong>，而不是完整意义上的人员定位管理系统。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597262" alt="" title="" loading="lazy"/></p><h3><strong>第二类 面向业务过程的人员定位管理系统</strong></h3><p>随着外勤管理要求的不断提高，越来越多企业开始关注<strong>过程真实性和执行质量</strong>。而这正是管控型人员定位管理系统的核心价值所在。</p><p>这一类系统不再只是关注员工有没有到达，而是重点解决几个关键问题：<strong>是否真实到场，是否按要求完成拜访或巡检动作，路线是否合理，费用是否真实发生</strong>。</p><p>围绕这些真实的管理需求，人员定位管理系统也在不断演进，从单一记录位置，逐步走向对过程真实性和执行质量的管理：</p><ul><li><strong>在定位层面</strong>：通过低功耗持续定位，还原员工全天的外勤轨迹，形成清晰的时间线。系统会自动分析每个点位的停留时长，帮助管理者识别异常拜访和形式化执行。</li><li><strong>在防作弊层面</strong>：通过对手机运行状态和定位行为的综合判断，识别虚拟定位、模拟轨迹、异常失联等情况，并形成异常记录，避免外勤过程成为管理盲区。</li><li><strong>在取证层面</strong>：现场拍照会自动生成包含时间、地点、人员信息的水印内容，便于后台快速核验，减少人工反复确认的成本。</li><li><strong>在费用管理方面</strong>：基于真实轨迹自动核算里程，为油补和差旅报销提供可靠依据，减少人为填报带来的误差和风险。</li></ul><p>这一类人员定位管理系统，已经成为<strong>快消巡店、医药代表拜访、物业巡检、自备车销售</strong>等场景中的主流选择。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597263" alt="" title="" loading="lazy"/></p><h3><strong>第三类 面向安全场景的增强型定位系统</strong></h3><p>在部分高风险行业，人员定位管理的核心目标并不在于效率，而在于<strong>安全</strong>。</p><p>这类系统通常<strong>结合室内高精度定位、传感设备或卫星通信技术</strong>，实现更高精度的人员定位和状态监测。例如在矿井、化工园区、电力施工等场景中，用于实时掌握人员位置，并在发生异常时及时预警。</p><p>由于其建设成本和部署复杂度较高，这类系统通常<strong>只适用于特定行业</strong>，并不适合大多数日常外勤管理场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597264" alt="" title="" loading="lazy"/></p><h2>如何判断人员定位管理系统是否真正适合企业</h2><p>在选型过程中，企业很容易被各种功能描述吸引，但真正决定系统价值的，往往是其<strong>是否贴合实际管理逻辑：</strong></p><p>首先是<strong>定位数据是否连续可信</strong>，只有能够还原完整轨迹，定位数据才具备管理意义。</p><p>其次看<strong>系统是否围绕真实业务流程设计</strong>，是否支持巡店、巡检、拜访等标准动作的顺序执行，是否能够通过定位约束避免走形式。</p><p>然后看<strong>费用数据是否基于真实行为产生</strong>，如果里程和补贴仍然依赖人工填写，那么人员定位管理系统的价值就会大打折扣。</p><p>最后还要看<strong>系统是否具备长期落地能力</strong>，外勤管理不是一次性全部上线，而是需要持续优化制度和流程的，这对服务能力提出了更高要求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597265" alt="" title="" loading="lazy"/></p><h2>人员定位管理系统选型建议</h2><p>从实际使用情况来看，不同企业的需求差异明显：</p><ul><li>如果企业主要<strong>解决考勤问题，外勤比例较低</strong>，基础定位功能即可满足需求。</li><li>如果企业希望<strong>提升外勤执行力，确保过程真实</strong>，同时优化人效和费用结构，那么以过程管理为核心的人员定位管理系统，如<strong>小步外勤</strong>更为合适。</li><li>如果企业处于<strong>高危或特殊作业环境</strong>，应优先考虑以安全为目标的深度定位方案。</li></ul><p>最后，需要清楚的是人员定位管理系统的价值，从来不只是记录位置本身，而在于<strong>通过真实数据，让外勤管理从经验判断走向客观决策</strong>。</p>]]></description></item><item>    <title><![CDATA[名字改了三次还没凉？这只龙虾竟然想接管我的电脑！ 三_清 ]]></title>    <link>https://segmentfault.com/a/1190000047597275</link>    <guid>https://segmentfault.com/a/1190000047597275</guid>    <pubDate>2026-02-06 17:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><p>最近 AI 圈有个“红色胖龙虾”火得离谱。</p><p>它原来叫 Clawdbot，因为太火被 AI 巨头 Anthropic 盯上，直接一份&lt;span style="color: red;font-size: 16px"&gt;“友好通知”&lt;/span&gt;说你侵权。于是连夜改名 Moltbot，最后成了现在的 openClaw。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597277" alt="官方改名通告" title="官方改名通告"/></p><p>最魔幻的是，因为这玩意的爆火，全球的 Mac Mini 居然被这帮极客买涨价了。</p><p>大家都在传：只要 1 分钟，你就能在手机上远程白嫖一个 24 小时待命的“数字员工”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597278" alt="相关视频：為什麼Mac mini糟全球瘋搶？在mini使用3天OpenClaw之後... " title="相关视频：為什麼Mac mini糟全球瘋搶？在mini使用3天OpenClaw之後... " loading="lazy"/></p><p>作为一名不折腾会死星人，我也在本地部署折腾了几天。体验下来发现：</p><p>&lt;span style="color: red;font-size: 16px"&gt;它确实很不错，是一个称得上“住”在你电脑并且拥有管理员权限的管理员，当然，如果姿势不对，它就是你硬盘的“拆迁办”。&lt;/span&gt;</p><h2>核心拆解：为什么是 openClaw？</h2><p>很多兄弟会问：我用网页版 Claude/Gpt 不香吗？非要折腾这个？</p><p>这就是 openClaw 设计聪明的地方。<strong>它内置了非常丰富的 function calling 功能，并且拥有终端系统权限。</strong></p><p>简单来说，以前的 AI 是关在笼子里的，只能跟你聊聊天。而现在的 openClaw 相当于给 AI 做成一个<strong>智能终端</strong>。它不再只是“建议你删掉多余文件”，而是会有可能直接一句“好的老板”，然后顺手在你的某个目录敲下&lt;span style="color: red;font-size: 16px"&gt;rm -rf。&lt;/span&gt;</p><p>这种<strong>从“问答”到“执行”</strong>的范式转变，才是让极客们高潮的根本原因。</p><h2>技能树：全能战神 or 电子垃圾？</h2><p>打开 openClaw 的 Skill 列表，多达 7 个分类，49 个默认 skill 可供配置。说实话，这地方最能体现“卖家秀”和“买家秀”的区别。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597279" alt="默认Skill列表" title="默认Skill列表" loading="lazy"/></p><p>不过在我看来，绝大部分都是“洋垃圾”，大陆用户用不上。</p><h3>那些会让你觉得“神了”的功能</h3><p>亲身体验下来有几个组合功能比较值得推荐。</p><ul><li><strong>TG+群聊机器人：</strong>将 openClaw 接入 Telegram，由于默认就有“长期记忆”功能，所以它能记住你们绝大部分对话，从而实现体验良好的私人或群聊机器人（也可以配置到飞书/企微）。</li><li><strong>浏览器自动化：</strong>openClaw 原生具备控制浏览器的能力，能帮你在不方便打开电脑时操控电脑网页，并生成截图，对网页进行总结等等，效果不错。（浏览器万岁！）</li><li><strong>定时提醒到手机：</strong>&lt;span style="color: red;font-size: 16px"&gt;极其有用的一个功能&lt;/span&gt;，并且因为可以连通到手机，这让 Agent 有望成为低配版本的“贾维斯”，比方说，你可以让它定时每天推送 GitHub Trending 并加以总结给你；又或者把你经常逛的新闻网站让 openClaw 爬取然后每天推送给你；更有甚者，可以尝试把日程安排全做成定时任务，让 openClaw 变成你的“个人助理”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597280" alt="一句话开启定时任务" title="一句话开启定时任务" loading="lazy"/></li><li><strong>本地文件管理：</strong> 由于拥有终端权限，你可以让 openClaw 化身 C 盘清理专家，帮你清理本地垃圾文件或查询需要的文件等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597281" alt="本地文件管理" title="本地文件管理" loading="lazy"/></li></ul><p>我们再来看下部分知乎用户对其的讨论</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597282" alt="知乎用户讨论总结" title="知乎用户讨论总结" loading="lazy"/></p><h3>劝退指南？谁在给这只“龙虾”交税？</h3><p>当然，作为一个还在“长身体”的开源项目，openClaw 远没到小白上手即用的程度。在折腾了一周后，我搜集了各大平台的反馈，基本可以总结为三个字：&lt;span style="color: red;font-size: 16px"&gt;坑挺大&lt;/span&gt;。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597283" alt="B站部分真实评论反馈AI总结" title="B站部分真实评论反馈AI总结" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597284" alt="小红书部分真实评论反馈AI总结" title="小红书部分真实评论反馈AI总结" loading="lazy"/></p><h4>Windows 用户：“后妈”生的</h4><p>目前来看，这款产品的评价呈现极端两极分化：Mac 用户直呼真香，Windows 用户直呼退钱。<br/>大概率是因为作者和核心贡献者都是基于 Mac 环境开发的，导致 Windows 版的 Bug 密集成灾：路径识别错误、依赖冲突、环境变量玄学报错。如果你不是一个习惯折腾环境的开发者，&lt;span style="color: red;font-size: 16px"&gt;千万别在 Windows 上挑战自己的血压&lt;/span&gt;。<br/>实在想玩，可以去买云厂商搞好的 9.9 元月付套餐，花杯奶茶钱买个清静。</p><h4>手机写代码：“赛博行为艺术”？</h4><p>有人吹嘘 openClaw 能让你在手机上远程撸代码。但听小弟一句劝，在手机上改 Bug 绝对是伪命题。<br/>我做手机上尝试让它在帮我修复一个 JS 逻辑，结果......完全没有反馈，我就已经开始 瑟瑟发抖了，我对“盲写”并没有兴趣。你可以用它来紧急重启个服务，或者跑个自动化脚本，但真要拿它写代码？&lt;span style="color: red;font-size: 16px"&gt;那你很牛。&lt;/span&gt;</p><h4>致命伤：“死循环”偷钱可能性</h4><p>虽然发生概率很少，但这是 Agent 架构目前比较无解的痛点。</p><p>笔者之前有幸在 cursor 上体验过几次，由于提示词或大模型本身的判断逻辑出现异常，AI 有时会陷入一种<strong>“鬼打墙”</strong>的状态。</p><p>比如：你让它找一个文件，它找不着，然后触发“重试”技能；重试又找不着，再触发“搜索”技能……在这个过程中，它每动一下都在疯狂调用 API。</p><p>当你发现它还没干完活时，它可能已经在短短几分钟内烧掉了你一顿饭钱。如果不设 Token 熔断，可能会让你有亿点点肉疼。</p><h2>最后</h2><p>虽然有不少毛病，并且未必有啥提高产能的能力，但我依然对 openClaw 保持高关注，为什么？</p><p>因为这个项目的 Agent 具备了&lt;span style="color: red;font-size: 16px"&gt;“权限”&lt;/span&gt;。以前我们用 AI，是“一问一动”；现在是“一问多动”，甚至你设置好定时任务，它是“不问也动”。</p><p>&lt;span style="color: red;font-size: 16px"&gt;这个项目开始，实现了从“对话”到“接管”。&lt;/span&gt;</p><p>尽管它现在还会“拆家”，还会“乱花钱”，但这种架构值得我们共同关注。</p><p>本文由<a href="https://link.segmentfault.com/?enc=jQGrli7NeRRCiLBXozirgg%3D%3D.gvFWjiY6fMiQNiQCpZ%2Fbe77gyLEFvo1IrDhUaLbkwMw%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[地平线 征程 6 工具链进阶教程 | 多任务 不同帧率 部署方案介绍 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047597302</link>    <guid>https://segmentfault.com/a/1190000047597302</guid>    <pubDate>2026-02-06 17:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.需求场景</h2><p>在智能驾驶等复杂业务场景中，模型往往具备​<strong>多任务分支结构</strong>​，例如在同一个网络中同时包含​<strong>​ BEV 动态任务</strong>​（如目标检测、跟踪、运动预测）与​<strong>​ BEV 静态任务</strong>​（如地图构建、车道线提取、可行驶区域预测），这些任务对推理频率（Frames Per Second， FPS）的要求通常并不相同。也就是有不同任务分支 推理不同帧率的需求，例如 BEV 动态任务 20 帧，静态任务 10 帧这种情况，BEV 模型结构简单示例如下所示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597304" alt="" title=""/></p><h2>2.技术分析</h2><p>以 BEV 动静态任务为例，实现不同任务分支推理不同帧率（动态 20 帧，静态 10 帧），很容易想到两种方案：</p><p><strong>​方案 1-​</strong>拆分为三个子模型：模型 1-公共部分（backbone+neck）、模型 2-动态 head、模型 3-静态 head</p><ol><li>模型 1 推理 20 次，输出分别送给模型 2 推理 20 次，模型 3 推理 10 次。</li><li>优点：应用层可灵活调度 3 个子模型的推理；模型 1-公共部分 只需要推理 20 次；</li><li>缺点：模型 1-公共部分的输出内存需要额外存储，增加 load/store 带宽消耗；拆分次数多，影响编译时的全图优化，可能会增加 latency；</li></ol><p><strong>​方案 2-​</strong>拆分为两个子模型：模型 1-公共动态（backbone+neck+ 动态 head）、模型 2-公共静态（backbone+neck+ 静态 head）：</p><ol><li>模型 1-公共动态推理 20 次，模型 2-公共静态推理 10 次。</li><li>优点：应用层可灵活调度 2 个子模型的推理；只需准备整个模型输入/出内存，无需准备公共部分输出的内存；拆分次数少，编译时可全图优化，减小 latency；</li><li>缺点：公共部分（backbone+neck）需要推理 30 次，造成 latency 增加与 BPU 资源浪费；公共部分需要存储两份；</li></ol><p>为了兼顾方案 1 与方案 2 的优点，同时实现不同任务分支推理不同帧率，工具链提供了 link 打包功能，具体打包方式如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597305" alt="" title="" loading="lazy"/></p><p>工具链提供的 link 功能，能够 复用 不同 模型/任务 的公共部分 constant 常量（包括权重等），即不会存储多份，在模型加载时，公共部分只会占用一份静态内存，需要注意推理时动态内存不会复用（​<strong>作为不同模型处理</strong>​），关于内存占用相关介绍可见文章《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=xarHYkZDYt1SlF%2FpWUdDQg%3D%3D.S88QZl1LvgCvhkt5mzzom5nzK7V%2F3V%2BT5QWtbkxYGQ%2BIUJhb%2Fh4SktdJYbKCR6UA" rel="nofollow" target="_blank">【地平线 J6 工具链入门教程】板端部署 UCP 使用指南-内存占用</a>&lt;/u&gt;》。</p><p>上图中将模型 1 与模型 2 link 打包生成的模型 3，相比于模型 1 体积不会大多少，同时具备推理模型 1 与模型 2 的功能。根据需求，调整模型 1 与模型 2 的推理次数，即可实现不同任务采用不同帧率部署。</p><p>如下图所示：推理一次模型 1，可实现动态任务 head 与静态任务 head 各推理一次，推理模型 2 可实现仅推理一次动态任务 head，当模型 1 推理 10 次、模型 2 推理 10 次时，即可实现动态推理 20 次，静态推理 10 次的效果。（公共部分 backbone+neck 仅推理 20 次）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597306" alt="" title="" loading="lazy"/></p><h2>3.方案实现</h2><h3><strong>3.1 模型 link 打包</strong></h3><p>根据需求场景，先将多任务模型拆分导出为不同子任务的 qat.bc，然后分别将他们编译成 hbo 文件，最后将多个 hbo 文件 link 打包为一个 hbm 模型。</p><p>在工具链用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=MurOfTE1cZr0xSbQyQwkog%3D%3D.qJ1YvW9mUUGiwI0c%2BbhsQtUmyQx92aJS1jLkMBMyZFSTGXTpdAhaNvUHIdAiSIPIyzZAigfkYuBTV8tl8c31o661oJowvATDKY5PAGc03Jo%3D" rel="nofollow" target="_blank">HBDK Tool API Reference</a>&lt;/u&gt;》章节中详细介绍了 compile 与 link API，可以看到：</p><ul><li>compile 输出同时支持 hbm 与 hbo 两种文件格式，可通过配置文件后缀名为"。hbm" or ".hbo"来区分。</li><li>link 支持将多个 hbo 文件打包生成一个 hbm 文件。</li></ul><p>将两个 hbo 文件通过 link 打包生成一个 hbm 模型，示例代码如下：</p><pre><code class="Plain">from horizon_plugin_pytorch.quantization.hbdk4 import export
from hbdk4.compiler import load, convert, compile, link
# export 阶段记得配置 name
qat_bcA = export(qat_model_A, example_input, name="1_backbone_head1_head2")
quantized_modelA = convert(qat_bcA, "nash-m")
# 注意：此时compile生成的模型后缀名为.hbo
hbo_nameA = "nameA_compiled.hbo"
hboA = compile(quantized_modelA, path=hbo_nameA, march="nash-m", opt=2, progress_bar=True, jobs=48)

qat_bcB = export(qat_model_B, example_input, name="2_backbone_head1")
quantized_modelB = convert(qat_bcB, "nash-m")
hbo_nameB = "nameB_compiled.hbo"
hboB = compile(quantized_modelB, path=hbo_nameB, march="nash-m", opt=2, progress_bar=True, jobs=48)

# link生成打包模型，后缀名为.hbm
hbm_name = "compiled.hbm"
hbm = link([hboA, hboB], hbm_name)
# 如果在其他地方已经生成了hbo
# 可以通过 hbo = Hbo(hbo_name) 进行加载 所需头文件： from hbdk4.compiler.hbm import Hbo</code></pre><h3>3.2 打包模型推理</h3><h4>3.2.1 hrt\_model\_exec 工具推理</h4><p>通过 <code>hrt_model_exec model_info --model_file compiled.hbm</code> 可查看打包模型的数量，输入输出等信息，示例如下</p><pre><code class="Plain">This model file has 2 model:
[2_backbone_head1]      [1_backbone_head1_head2]
---------------------------------------------------------------------
[model name]: 2_backbone_head1

input[0]:
name: ...

output[0]:
name: ...

---------------------------------------------------------------------

---------------------------------------------------------------------
[model name]: 1_backbone_head1_head2

input[0]:
name: ...

output[0]:
name: ...

output[1]:
name: ...</code></pre><p>结合--model\_file 与--model\_name 即可实现对打包 compiled.hbm 模型中的某一个模型进行推理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597307" alt="" title="" loading="lazy"/></p><p>以 perf 评测打包 compiled.hbm 模型 中 2\_backbone\_head1 的性能为例，参考命令如下：</p><pre><code class="Plain">hrt_model_exec perf --model_file compiled.hbm --model_name 2_backbone_head1</code></pre><h4>3.2.2 UCP API 推理</h4><p>在工具链用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=tcU9n8D0D4FptP5V1TQsOw%3D%3D.Bp%2FHg65zUllkk7L2cM6%2B9yUcDB26n4D30OAGjJSkWzpSBPM9nIXmYfZ1lvLYvfrTDXCbG91155WlvmFSOsXY13IIMSw4mu2WHiIXO%2BwiZy9Y7Ka0n7Y49QVb1sV3WEOU6fsFP8vF8GvD1aFGFbr6ew%3D%3D" rel="nofollow" target="_blank">统一计算平台 UCP - 模型推理开发 - 模型推理 API 手册 - 功能接口</a>&lt;/u&gt;》中，详细介绍了加载打包模型 hbDNNInitializeFromFiles 与 获取单个模型句柄 hbDNNGetModelHandle 的使用方式，截图如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597308" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597309" alt="" title="" loading="lazy"/></p><p>在工具链开发包路径：OE/samples/ucp\_tutorial/dnn/basic\_samples 下方的示例中有用到这两个接口，可参考使用。</p><h3>3.3 多任务不同帧率推理</h3><p>根据需求，调整打包模型 compiled.hbm 中的 模型 1 backbone\_head1\_head2 与模型 2 backbone\_head1 的推理次数，即可实现不同任务采用不同帧率部署。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597310" alt="" title="" loading="lazy"/></p><h3>3.4 性能数据示例</h3><p>下表中，backbone\_head1 是公共部分，​<strong>注意</strong>​：公共部分权重是一样的</p><table><thead><tr><th><strong>模型名称</strong></th><th><strong>模型大小/KB</strong></th><th><strong>模型 name</strong></th><th><strong>latency/ms</strong></th></tr></thead><tbody><tr><td>1\_backbone_head1\_head2.hbm</td><td>30295</td><td>/</td><td>5.19</td></tr><tr><td>2\_backbone_head1.hbm</td><td>21781</td><td>/</td><td>4.84</td></tr><tr><td>compiled.hbm</td><td>30776</td><td>1\_backbone\_head1\_head2</td><td>5.18</td></tr><tr><td>2\_backbone\_head1</td><td>4.83</td></tr></tbody></table><p>可以看到，compiled.hbm 体积相比于 1\_backbone\_head1\_head2.hbm 并没有增加多少。</p><p>模型加载推理时，ION 内存差异如下：</p><p>加载 1\_backbone\_head1\_head2.hbm，直接推理：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597311" alt="" title="" loading="lazy"/></p><p>加载 compiled.hbm，推理 1\_backbone\_head1\_head2：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597312" alt="" title="" loading="lazy"/></p><p>可以看到，compiled.hbm 占用的内存相比于 1\_backbone\_head1\_head2.hbm 并没有增加多少。</p>]]></description></item><item>    <title><![CDATA[白皮书解读 | 在不确定性成为常态的时代，能源矿产行业如何重构运行与治理方式？ 袋鼠云数栈 ]]></title>    <link>https://segmentfault.com/a/1190000047597329</link>    <guid>https://segmentfault.com/a/1190000047597329</guid>    <pubDate>2026-02-06 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>过去两年，能源矿产行业，正在从一个“以规模和资源驱动”为主的行业，进入一个以复杂运行与系统治理为核心约束的新阶段。一方面，全球能源博弈加剧、关键矿产战略属性抬升，安全、稳定与可控成为底线要求；另一方面，绿色低碳、双碳约束、成本透明化，正在把过去“被吸收”的不确定性，逐步转化为显性经营压力。与此同时，AI 对算力与能源的需求反向放大了能源系统的战略地位，使能源矿产不再只是“上游产业”，而是全球产业体系中的基础能力提供者。</p><p>这三股力量叠加，带来的并不是简单的业务增长或下行，而是一种更根本的变化：企业运行本身，正在变得前所未有地复杂。复杂性不再来自“业务更多”，而来自“变量更多、耦合更强”。</p><p>正是在这一判断之下，这本《能源矿产行业 Data+AI 数智化转型白皮书》试图回到一个更基础的问题：能源矿产企业，究竟需要怎样一套面向未来十年的数智化体系？<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597331" alt="图片" title="图片"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047464189" alt="图片" title="图片" loading="lazy"/><br/>产业链拆解：矿山、冶炼、加工、集团企业，各有各的“卡点”从产业链整体看，能源矿产行业正同时承受来自安全、成本、效率与治理复杂度的多重压力，但不同环节的问题形态并不相同。矿山企业长期处在高风险、高耦合的运行环境中，生产、安全与成本高度依赖现场经验进行平衡判断，虽然系统与数据不断增加，但信息分散在采掘、通风、运输、安全监测等多个专业系统中，难以形成连续、稳定的运行态势认知，一旦出现异常，管理动作往往发生在结果之后，风险演化过程缺乏提前感知能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597332" alt="图片" title="图片" loading="lazy"/><br/>进入冶炼环节，问题逐步从“能不能稳住生产”转向“利润结构是否可被解释”，能耗、原料、工艺路线、排产节奏相互影响，加工费、能源价格与碳成本波动频繁，财务结果可以核算清楚，但利润变化背后的驱动因素难以被拆解，生产侧与经营侧之间长期缺乏贯通分析视角。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597333" alt="图片" title="图片" loading="lazy"/><br/>加工企业表面上资产更轻、流程更灵活，但项目数量多、订单碎片化、区域分散，使成本、进度、交付与质量高度交织，管理层往往只能看到阶段性结果，对项目运行节奏与关键偏差缺乏整体把控能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597334" alt="图片" title="图片" loading="lazy"/><br/>到了集团层面，上述问题进一步叠加放大，数据持续上行但语义口径不统一，指标体系不断扩充却难以横向对比、纵向追溯，分析结果更多停留在展示层面，难以真正嵌入资源配置、考核机制与管理动作之中。这种状态下，企业对外部不确定性的感知能力持续增强，对内部运行复杂性的掌控能力却并未同步提升，成为当前能源矿产行业普遍面临的系统性挑战。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597335" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597336" alt="图片" title="图片" loading="lazy"/></p><p>统一底层逻辑：一体两翼架构如何支撑能源矿产企业数智化转型在具体展开矿山、冶炼、加工与集团四类企业实践之前，有必要先回答一个更基础的问题：这些看似差异巨大的场景，是否存在一套可复用的数智化底层逻辑？</p><p>从大量项目实践来看，答案是肯定的。能源矿产行业面临的问题虽然分布在不同环节，但在数据形态、管理诉求与运行机制上，呈现出高度一致的结构特征——数据来源复杂、业务耦合度高、风险容忍度低、管理链条长。这决定了数智化建设需要一套能够长期运行、持续演进的通用架构作为基础。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597337" alt="图片" title="图片" loading="lazy"/><br/>一体两翼方案架构基于这一认识，白皮书提出了“一体两翼”的整体方案框架。“一体”指向统一的数据底座，核心目标是解决数据在跨系统、跨层级、跨业务域流动过程中的可解释性问题。通过多模态数据智能中台与治理体系建设，将组织、资产、项目、装置、人员、合同、物料等关键对象纳入统一的数据模型与语义体系，明确主数据口径、指标计算逻辑与责任归属，使不同系统产生的数据能够在同一语境下被理解、被对比、被追溯。这一层解决的是“数据能不能支撑管理”的问题，是所有后续分析与应用的前提。</p><p>在统一数据底座之上，“两翼”分别对应数据智能能力与空间智能能力。数据智能侧重于把治理后的数据转化为可用的分析与判断能力，包括主题域分析、指标体系运行、异常识别、趋势判断以及智能问数与辅助分析等能力形态，其价值在于降低分析门槛、提升判断效率，使管理层能够从数据中快速获得“发生了什么、为什么发生、接下来可能会怎样”的连续认知。</p><p>空间智能则承担另一类关键任务——将复杂业务运行状态放入空间语境中表达，通过数字孪生把生产现场、工程项目、管网资产、园区设施等实体对象转化为可感知、可联动、可推演的运行载体，让风险、进度、资源与规则以更直观的方式支撑监管、指挥与协同。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597338" alt="图片" title="图片" loading="lazy"/><br/>图：车辆及驾驶员精准定位及状态监控</p><p>“一体两翼”框架并不追求一次性覆盖所有场景，而是为不同类型企业提供了一套可按需展开的通用底盘。矿山企业更多把能力落在空间智能与实时监管上，解决高风险场景下的运行感知与风险前移；冶炼企业重点强化数据治理与主题域分析，打通生产、能耗与经营之间的分析链路；加工与工程型企业围绕项目主线展开数据中台与经营分析，提升对经营节奏与风险暴露的掌控能力；集团企业则在统一底座之上，通过经营分析、统管机制与空间化承载，实现对子公司的可比、可控与可调度管理。差异体现在应用重点，而非底层逻辑本身。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597339" alt="图片" title="图片" loading="lazy"/></p><p>也正因为有这样一套通用方案作为支撑，后续不同企业类型的实践，才能在保持行业差异的同时，呈现出清晰一致的建设路径与演进逻辑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597340" alt="图片" title="图片" loading="lazy"/></p><p>矿山、冶炼、加工、集团企业的不同落地路径：一体两翼走向行业实践要把矿山、冶炼、加工与集团统管这四类问题真正解决到位，方案需要同时回答三件事：数据怎么汇、口径怎么统一、场景怎么持续运行。</p><p>矿山环节优先建设“矿山实景数字孪生实时监管平台”，它的关键在于把巷道/采场/硐室等空间对象变成可承载业务规则的“运行容器”。在平台底座上，矿山空间模型持续随采掘推进更新，人员定位、车辆轨迹、作业票证、岗位资质、环境越限、重点风险源等数据按空间对象绑定与联动，形成“谁在何处、在做什么、是否具备资质、环境是否越限、风险是否可控”的连续态势。</p><p>监管能力通过两类机制落地：一类是空间化的风险分级与联动处置，例如越界、越限、禁入、临边与关键设备区的规则触发，自动生成预警清单与处置闭环；另一类是面向应急的路径推演与指挥协同，将人员分布、撤离路线、应急物资点位、救援力量调度纳入同一空间语境，支持从“态势掌握”进入“指挥动作”。这类平台真正解决的是深部开采条件下的“运行可见性”和“风险前移”，让安全管理从事后统计转向过程监管。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597341" alt="图片" title="图片" loading="lazy"/></p><p>图：采矿车辆空间运行状态监控概览</p><p>进入冶炼环节，解决方案落点是建设“智慧运行平台”，建设重点围绕数据中台、治理体系与主题域分析同步展开。冶炼业务天然具有强耦合特征：原料结构、工艺参数、能耗水平、产量节奏与质量指标相互牵引，财务结果虽能核算，利润变化的形成过程长期缺少可解释链路。智慧运行平台首先把 DCS/MES/ERP、计量、能源与质量等系统数据纳入统一治理框架，建立主数据（组织、装置、物料、工序、计量点、能源介质等）与指标口径映射关系，确保“同一指标在不同装置、不同基地、不同时间粒度下可对比、可追溯”。在此基础上，以既定的主题域框架沉淀分析体系，将经营、财务、合同、人资、采购、合规等管理视角与生产过程数据打通，形成从“工艺—能耗—成本—利润”的贯通分析路径，使管理动作能够落在可解释的数据链路上：哪里波动、为什么波动、影响到哪个利润环节、该由哪个责任单元承接改进。冶炼方案的价值锚点在于把“算得出结果”升级为“看得见机制”，为排产优化、能效对标、成本拆解与风险预警提供稳定底盘。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597342" alt="图片" title="图片" loading="lazy"/><br/>图：冶炼行业多模态数据智能中台架构</p><p>加工型企业的方案，以全厂级实景建模为基础，构建覆盖厂区、车间与关键装置的数字空间底座，将设备、工艺、人员与环境等要素统一承载，为现场管理与分析提供稳定语境。在此之上，通过人员定位与风险感知能力建设，形成覆盖全厂的实时态势感知，使作业行为、风险区域与异常状态能够被持续识别与管控。围绕运行保障与生产组织，数字孪生进一步用于支撑应急演练、处置推演与跨车间态势协同，帮助管理层在同一视角下理解不同工序与车间的运行关系。随着现场数据持续沉淀，经营分析能力逐步与生产过程耦合，通过将产量、能耗、设备状态与成本、收率等指标关联，管理层能够在空间化视角下理解经营结果的形成路径，实现从结果统计向过程解释的转变，并在指挥大厅与接待场景中形成可操作、可表达的整体能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597343" alt="图片" title="图片" loading="lazy"/></p><p>图：企业作战驾驶舱现场实景</p><p>集团层面的方案要回答“怎么统管子公司、怎么横向对比、怎么把分析转成抓手”。集团数据中台承担统一标准、统一口径、统一组织维度的工作，把成员企业数据上行与集团管理下行打通；经营分析体系则围绕集团既定主题域框架运行，将财务经营、人力资源、工程运营、客户服务等关键场景沉淀为集团级可对比的管理视图，支撑跨区域、跨业态的态势判断与资源配置。数据门户体系负责把集团分析、预警与任务清单组织成统一入口，形成“会前看态势、会中抓异常、会后盯整改”的运行机制。空间化能力在集团中更多承担“资产与运行态势的承载方式”，将管网、场站、工程项目与重点风险点的空间分布与经营分析结果关联呈现，降低跨层级沟通成本，让集团层的统管动作更容易落到区域与项目公司。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597344" alt="图片" title="图片" loading="lazy"/></p><p>主题域规划</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597345" alt="图片" title="图片" loading="lazy"/></p><p>运营平台（原型测试数据）<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597346" alt="图片" title="图片" loading="lazy"/><br/>四类企业的真实落地：一体两翼架构在复杂场景中的实践验证在地下矿山场景中，十五冶的实践验证了数字孪生作为运行级基础能力的价值。通过构建地下矿山实景数字孪生实时监管平台，企业将人员、设备、环境与作业活动统一纳入空间化运行视角，矿山管理从分系统监控转向整体态势感知。安全管理由事后复盘逐步演进为事前预判与过程干预，调度与应急指挥建立在实时状态之上，为深部开采条件下的安全治理与稳定运行提供了可持续支撑。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597347" alt="图片" title="图片" loading="lazy"/><br/>在工程型、项目型企业场景中，相关实践围绕数据中台重构了经营认知方式。通过以项目为核心对象，贯通合同、预算、采购、结算与财务数据，企业逐步摆脱了阶段性报表驱动的管理模式，建立起对项目收支、成本偏差与风险敞口的连续判断能力。经营管理从“看结果”转向“盯过程”，为项目统筹、资源配置与风险控制提供了更具前瞻性的决策基础。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597348" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597349" alt="图片" title="图片" loading="lazy"/><br/>在铜业加工场景中，实践重点落在生产现场与经营分析的协同。通过全厂级实景建模、人员定位与风险感知能力建设，企业构建了覆盖多车间、多工序的实时运行视角；在此基础上，将产量、能耗、设备状态与关键经营指标关联分析，使管理层能够在空间语境中理解经营结果的形成过程。数字孪生演进为支撑生产协同、应急处置与经营分析的综合载体。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597350" alt="图片" title="图片" loading="lazy"/><br/>在集团层面，中国燃气的实践展现了数据中台与经营分析在超大规模组织统管中的价值。通过统一数据治理与分析体系，集团将分散在多业态、多区域、多层级的经营与运行数据重新组织，形成可对比、可追溯、可下钻的集团级分析视角。经营、工程、人力与客户服务等关键领域逐步纳入统一认知框架，集团管理由“看得到数据”转向“推得动管理动作”，为规模化发展背景下的稳健运行与治理升级提供支撑。向左滑动查看更多<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597351" alt="图片" title="图片" loading="lazy"/><br/>图：财务经营分析场景建设框架<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597352" alt="图片" title="图片" loading="lazy"/><br/>图：组织绩效分析场景建设框架<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597353" alt="图片" title="图片" loading="lazy"/><br/>图：干部画像分析场景建设框架<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597354" alt="图片" title="图片" loading="lazy"/><br/>图：抄收＆保修场景分析建设框架<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597355" alt="图片" title="图片" loading="lazy"/><br/>图：工程运营场景建设框架<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597331" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597356" alt="图片" title="图片" loading="lazy"/><br/>实践共性路径及方法论总结</p><p>在矿山、冶炼、加工与集团四类企业的实践中，可以看到一个高度一致的结论：能源矿产行业的数智化挑战，来自业务复杂性、风险密度与治理半径的持续放大。当生产环境高度不确定、经营链条高度耦合、组织层级不断拉长，仅靠零散系统叠加或局部智能化，很难形成稳定、可持续的管理能力。真正起作用的，是一套能够长期运行、持续演进的体系化路径。“一体两翼”的价值，正体现在这种体系能力之上。</p><p>一方面，通过统一的数据底座与指标体系，把分散在现场、系统与组织中的信息，转化为可被理解、可被比较、可被追溯的经营与运行认知；</p><p>另一方面，通过空间化能力与智能分析能力，将这些认知嵌入到真实业务场景中，服务于安全监管、生产协同、经营分析与集团统筹等关键决策动作。</p><p>这使得数智化逐步参与到企业运行方式本身。从地下矿山的实景监管，到冶炼企业对生产—能耗—经营链路的穿透分析；从加工企业在复杂现场中构建整体态势感知，到集团层面对跨区域、跨业态运行与经营的统一统筹，这些实践并未追求“一步到位”的智能化目标，而是围绕各自最核心的业务矛盾，选择合适的切入点持续推进。也正是在这种渐进式、可复用的建设过程中，数智化能力开始真正沉淀为企业的治理能力。这或许是能源矿产行业在不确定性时代的一条现实路径：以能否降低风险暴露、提升经营透明度、增强组织协同为最终检验。Data+AI数智化转型的意义，不在于让企业“看起来更先进”，而在于让企业在复杂环境中运行得更稳、更清楚，也更有韧性。回到这本白皮书所呈现的，不是一套标准答案，而是一组来自真实实践的路径参考。无论是矿山、冶炼、加工企业，还是集团型组织，只有尊重行业特性、正视业务矛盾，在统一底座之上持续积累认知与能力，数智化才可能从“项目建设”走向“长期能力”。在能源博弈加剧、产业周期波动加深的时代背景下，这种能力本身，正逐渐成为企业最重要的竞争壁垒。我们也希望，这些实践与方法，能够为更多行业参与者提供启发——不是为了追逐概念，而是为了在不确定的时代，找到一条更稳健、更可持续的前行路径。</p>]]></description></item><item>    <title><![CDATA[从登顶BIRD-CRITIC榜单到亮相国际顶会，瑶池Data Agent 的性能与体验双突破 数据C]]></title>    <link>https://segmentfault.com/a/1190000047596779</link>    <guid>https://segmentfault.com/a/1190000047596779</guid>    <pubDate>2026-02-06 16:11:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在很多企业里，数据并不稀缺，真正稀缺的是——能被快速理解、被信任、并真正参与决策的数据洞察。我们一直在思考：如果获取数据价值不再需要复杂的工具切换、专业门槛和漫长等待，会是什么样？</p><p>这正是我们打造 Data Agent 的起点。</p><h2>性能验证：Bird-Critic 榜单第一</h2><p>在全球权威的 SQL 评测基准 BIRD-CRITIC中，阿里云瑶池 Data Agent 凭借卓越的表现荣登榜首。BIRD-CRITIC 榜单旨在验证“大模型能否解决现实应用场景中的数据库难题”，该基准 整体难度远高于传统的NL2SQL测试。瑶池 Data Agent 的登顶，标志其在复杂场景下的泛化实力已达世界领先水平！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596782" alt="图片" title="图片"/></p><h3>为什么选择 BIRD-CRITIC？</h3><p>BIRD-CRITIC 是目前全球 SQL 诊断领域最具挑战性的基准测试之一。它打破了传统 NL2SQL 仅聚焦“语句生成”的局限，深度下探至查询修复、DDL 变更安全及性能优化等真实运维核心场景。该基准不仅横跨 MySQL、Oracle 等四大主流方言，更在评估中推行近乎苛刻的列级匹配标准。这种高难度的设定，真实映射了 Data Agent 必须直面的业务挑战——在复杂的场景中，坚守严谨的执行逻辑。</p><h3>核心引擎：DMS 的技术沉淀</h3><p>Data Agent 能够取得优异的成绩，离不开阿里云 DMS 在数据库领域的技术积累。我们将 DMS 在多方言语法规则、性能优化模式及数据治理方面的实践经验，通过工程化手段转化为 Agent 可利用的知识库。这使得 Agent 在处理 Oracle 的特殊分页或 MySQL 的隐式转换等细节问题时，能够更加准确和规范。<br/>真实架构：多 Agent 协同<br/>在架构设计上，我们沿用了 Data Agent 生产环境中的<strong> Multi-Agent 协同机制</strong>：</p><ul><li>意图规划 Agent (Coordinator): 主要负责解析模糊需求，利用元数据能力探测数据分布，协助消解业务歧义。</li><li>执行校验 Agent (Critic): 基于规划生成 SQL，并进行确定性验证（Determinism Check）和安全性评估，保障执行过程的可靠性。<br/>这种“规划-执行-校验”的闭环流程，不仅在测试中验证了有效性，也是我们处理复杂数据任务的基础范式。</li></ul><p>这对用户来说，这背后的意义只有一个：它不是 Demo，而是可以放心交给真实业务的数据智能体！</p><p>它能同时覆盖传统BI分析（描述性、诊断性）和高级分析（预测性、规范性），既可以作为自然语言到SQL查询的转换工具（NL2SQL），也可以生成预定义报表的聊天式BI（包括ChatBI），是一个具备理解分析意图、规划分析路径、执行复杂任务、并生成深度洞察的自主智能系统，能稳定完成复杂、多步骤的数据分析任务。</p><h2>体验认证：斩获国家级创新奖项，并入选 CCF A 类国际会议</h2><p>在性能之外，Data Agent 的产品体验也获得了行业权威认可。</p><p>Data Agent 荣获 中国设计智造大奖（DIA）。DIA是国内最具影响力的国际创新设计奖项之一，评审严格、覆盖多维度价值，被视为衡量产品创新实力与国际竞争力的重要标志。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596783" alt="图片" title="图片" loading="lazy"/><br/>同时，围绕 Data Agent 在分析过程透明度与人机协同体验上的实践成果，还成功入选 CCF A 类国际会议 CSCW 2025，进一步证明了 Data Agent 在复杂交互与体验可信性方面的工程深度和行业关注度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596784" alt="图片" title="图片" loading="lazy"/><br/><a href="https://link.segmentfault.com/?enc=4t4RSqBG8qg4t6xbUKCFvQ%3D%3D.BcdKQ75Ac2OLK8gw%2F5ONjQZGBE2z3YY2Y4FAMrLIH3wig1n27qZ2tUiRqAjEZ9ck" rel="nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3715070.3749256</a><br/>这些权威背书验证了一点：<br/>当 AI 承担更复杂的数据分析任务时，体验本身已经成为产品竞争力的重要组成。</p><h3>数据安全 ：“三位一体”全链路保障</h3><p>DMS Data Agent 构建了“身份-环境-管控”三位一体的安全体系：</p><ul><li>访问控制：通过“安全托管”实现账号密码不落地；支持细粒度权限、自动脱敏与全程审计。</li><li>环境隔离：采用内核级沙箱与VPC闭环隔离，确保数据交互在闭环内完成，阻断外网威胁。</li><li>管控安全：租户级会话隔离，任务结束即销毁环境并清除数据，杜绝数据残留。</li></ul><p>该方案实现了“账号不落地、环境全隔离、操作可审计、数据无残留”的全链路安全保障。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596785" alt="图片" title="图片" loading="lazy"/></p><h2>Data-for-All：人人可用的数据分析，从任务开始</h2><p>我们始终坚持一个判断：不是所有分析任务，都需要同样的过程呈现方式。<br/>因此，Data Agent 采用任务驱动的透明度策略，根据任务复杂度，自动选择最合适的体验方式。</p><h3>简单任务：低透明度，效率优先</h3><p>在日常问数、快速判断场景中，Data Agent 直接给出结果，并生成可交互的图表。<br/>用户无需被执行过程打断，<strong>用最短路径，得到可用答案</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596786" alt="图片" title="图片" loading="lazy"/></p><h3>复杂任务：高透明度，助推认知</h3><p>在业务复盘、策略分析等复杂场景中，Data Agent 会先生成分析计划，再分步骤执行分析，最终产出结构化的网页洞察报告。</p><p>用户不仅能看到结论，也能理解“这个结论是如何得出的”，整个分析过程清晰可见、可控且可信，关键步骤可回溯，确保结果既可靠又透明。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596787" alt="图片" title="图片" loading="lazy"/></p><h2>Human in the loop：人智协同，分析准确</h2><p>在 AI 代理式分析过程中，用户始终处在决策链路中。</p><p>分析过程清晰可追溯；涉及关键或高风险操作时，系统会主动确认；用户可随时调整、补充分析方向。</p><p>AI 不替代判断，而是成为一个<strong>可协作、可校正的分析伙伴</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596788" alt="图片" title="图片" loading="lazy"/></p><h2>结语</h2><p>我们坚信，真正深耕业务的数据智能，绝非单纯的“智力堆砌”，而是平时的极致高效与关键环节的透明可信。从问鼎 BIRD-CRITIC 榜单，到收获 DIA 与 CCF A 类顶会的双重认可，Data Agent 实现了在性能与体验上的双突破，也是我们迈向“人人可用的数据分析”的重要一步。</p><ul><li>了解更多产品详情：<a href="https://link.segmentfault.com/?enc=Fj1jiPtDOiHhAU1O11TGHA%3D%3D.2umbGPU%2BTZEwDQz5oEMsnJgX08PB5TqitUYpvcOa%2Fcz0GmzkWlDlnb7BGxy4ayVY6rV2Eq1%2FZy5H0VQo9yfRHA%3D%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/dms/data-agent-for-analytics</a></li><li>报名参加Data Agent训练营，带你7天获得“职场外挂”：<a href="https://link.segmentfault.com/?enc=sl19nNAioWsfnZ9oSAEMvQ%3D%3D.ZxV3kbxRtoI6M9THLWb73WlduCzVmR%2FLOhaZS4%2Fck5%2Bo8JJjTTgJdh6wdgoEmfM6" rel="nofollow" target="_blank">https://edu.aliyun.com/trainingcamp/3524000</a></li><li>欢迎钉钉搜索群号“105130018526” 或扫码加入钉群<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596789" alt="图片" title="图片" loading="lazy"/></li></ul>]]></description></item><item>    <title><![CDATA[一文速通 OceanBase 物化视图能力 OceanBase技术站 ]]></title>    <link>https://segmentfault.com/a/1190000047596957</link>    <guid>https://segmentfault.com/a/1190000047596957</guid>    <pubDate>2026-02-06 16:10:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><strong><em>OceanBase 针对现代数据架构核心挑战重构物化视图能力，融合分布式架构与多模引擎，提供非实时和实时两类视图、灵活刷新机制及多维度查询加速技术，底层基于 LSM-Tree 引擎和 MLOG 日志实现。该能力在电商大促、SaaS ERP 等场景落地，实现查询加速、链路简化与负载隔离，也存在存储、维护等使用限制，后续将从运维、链路、场景类型三方面持续迭代优化。</em></strong></p><p>本文作者 | 朱涛，OceanBase 高级技术专家，负责 OceanBase 查询优化器的研发工作</p><p>在实时数仓、HTAP（混合事务/分析处理）与库内计算（In-database Computing）成为主流的今天，数据架构的核心矛盾已悄然转变：企业不再仅仅追求“更快的查询”，而是面临着如何降低算力成本、简化数据链路、并保障核心系统稳定性的艰巨挑战。</p><p>传统的、依赖于复杂 ETL 管道的 T+1 数仓架构，日益无法满足企业的实时决策需求。正是在这一背景下，物化视图（Materialized View）这项经典技术，正被重新审视并赋予全新的战略价值。</p><p>物化视图的本质，是将高频、复杂的查询结果预先计算并物理存储在数据库内部。这看似简单的“空间换时间”，在现代架构中解决的远不止单个查询的性能问题。它通过将计算左移至数据源头，极大地简化了外部数据处理链路，减少了数据冗余和不一致的风险。更重要的是，它将消耗巨大的分析负载从核心交易流程中剥离，从而保障了在线业务的稳定性。因此，设计精良的物化视图能力，已不再是分析型数据库的“附加功能”，而是衡量一个现代数据库能否高效支撑 HTAP 和实时分析场景的核心指标。</p><p>基于此，OceanBase 对物化视图进行了深度重构，使其不仅仅是一个性能加速器，更是一个与分布式架构、多模引擎（行存、列存）深度融合的数据处理中枢，其实时物化视图能力能够在保证数据新鲜度的同时，提供高性能的查询服务。</p><p>本文将对 OceanBase 物化视图的核心能力、技术原理及应用场景进行全面介绍。</p><h2>OceanBase 物化视图的核心能力</h2><p>OceanBase 的物化视图并非单一的功能，而是一套包含多种类型、灵活刷新策略和多样化查询加速机制的完整解决方案。其核心能力可归纳为以下几个方面：</p><p><strong>01多样化的物化视图类型</strong></p><p>为了适应不同业务场景对数据新鲜度的需求，OceanBase 提供了两种主要的物化视图类型 ：</p><p>非实时物化视图：此类型视图中的数据并非总是与基表保持实时同步。它根据预设的计划（如定时）或手动触发进行刷新，在刷新间隔期内，查询将访问物化视图中已物理存储的数据。这种方式适用于对数据新鲜度要求不高，但对查询性能和资源消耗更为敏感的场景，如 T+1 的报表生成。</p><p>实时物化视图：该类型视图能够提供实时或准实时的数据查询结果。它通过内部的物化视图日志（MLOG）机制，捕获基表的增量数据变更。在查询时，系统会在线计算物化视图的存量数据和日志中的增量数据，从而返回最新的结果集。这使得用户即便在物化视图尚未完成物理刷新时，也能查询到最新的数据状态，特别适合实时监控、实时大屏等对数据时效性要求高的场景。</p><p><strong>02灵活的刷新机制</strong></p><p>数据的刷新是维持物化视图生命力的关键。OceanBase 提供了全面且灵活的刷新策略与方式，以平衡数据时效性、系统资源开销和管理复杂度。</p><p><img width="440" height="486" referrerpolicy="no-referrer" src="/img/bVdnSiZ" alt="" title=""/></p><p><strong>03全方位的查询加速技术</strong></p><p>创建物化视图的最终目的是加速查询。OceanBase 为此提供了一系列配套功能，最大化其性能优势：</p><p>查询改写 (Query Rewrite)：这是物化视图最核心的价值之一。当启用查询改写功能后，优化器能够自动将用户针对基表的查询请求，智能地重定向到已经预计算好的物化视图上，整个过程对应用透明，极大地降低了业务改造的复杂度。</p><p>与列存深度融合：自 4.3.3 版本起，OceanBase 支持创建基于列存格式的物化视图。当物化视图的查询逻辑涉及复杂的分析和聚合操作时，将其存储为列存格式可以获得比传统行存更优的查询性能，尤其是在“大宽表”分析场景下。</p><p>索引、主键与分区支持：物化视图在 OceanBase 中被视为一种特殊的表对象，因此可以像普通表一样，在其上创建索引、定义主键和设计分区策略。这些手段可以进一步优化对物化视图自身的查询性能，例如通过索引加速特定字段的过滤，或通过分区裁剪减少扫描的数据量。</p><p>OceanBase 物化视图的实现深度依赖其分布式架构和核心组件 ：</p><p>LSM-Tree 存储引擎：作为 OceanBase 的基石，LSM-Tree 引擎的特性使得列存表可以支持事务和流式写入，为实时数仓和物化视图的实现提供了基础。</p><p>物化视图日志 (MLOG)：这是实现增量刷新和实时物化视图的核心。当基表发生 DML 操作时，变更的增量信息会被同步记录到 MLOG 中。刷新时，系统只需读取 MLOG 即可获取变更数据，避免了对整个基表的扫描。</p><h2>OceanBase 物化视图的典型场景及案例</h2><p>典型场景</p><p>01实时数据分析</p><p>对于需要实时洞察业务动态的场景，如实时监控大屏、实时推荐、实时风控等，OceanBase 的实时物化视图能够提供强有力的支持。通过结合 Flink CDC 等实时数据同步工具，可以构建端到端的实时数仓，而实时物化视图则作为查询加速层，确保在数据持续流入的同时，分析查询依然能够获得极低的延迟。</p><p>02复杂查询性能优化</p><p>在许多 OLTP（在线事务处理）和 HTAP（混合事务/分析处理）系统中，存在一些消耗大量资源的“慢查询”，这些查询往往涉及多张大表的连接和复杂的聚合计算。通过为这些特定查询创建物化视图，可以将其计算成本从每次查询时发生，转移到后台的刷新任务中，从而有效降低在线业务高峰期的系统负载，保障核心业务的稳定性。</p><p>相关案例</p><p>电商大促价格计算：多表 Join 加工宽表，支撑近实时分析</p><p>业务问题：多源商品关系数据需要预加工为分析宽表</p><p>在电商大促期间，运营人员需要将商品基础信息、商品销售属性、促销活动商品等多维数据整合为统一宽表，用于运营分析与决策查询。上游数据同步进入 OceanBase 后，如果每次分析查询时再做多表 join，会带来较高计算开销和不稳定延迟。</p><p>因此，优化目标是在库内完成多表数据加工，沉淀可复用的加工明细宽表，供下游 AP 查询分析，如运营分析、数据服务等，直接消费。</p><p>典型模型为四表 join，即：活动商品池表 + 商品基础信息表 + 商品销售属性表 + 商家店铺表，通过 inner join 生成加工结果宽表。</p><p>技术挑战：Join 成本高 + 变更模式不均匀</p><p>该场景的难点不只是表规模，而是变更分布极不均匀，对实时 join 和全量重算都不友好。</p><p>测试数据特征如下：<br/>竞品基础信息表：全量约 40 万（预期可到 180 万）；高峰单次增量 10–15 万（新品/状态变更）；<br/>商品销售属性表：全量约 44 万，高峰单次增量 2–3 千；<br/>活动商品池表：日常全量仅约 1 千，但存在单次 400–500 万级集中变更；<br/>加工结果宽表（物化视图）：约 50–100 万级。</p><p>这种模式下：<br/>查询时 join → 成本高且波动大；<br/>批量变更触发全量重算 → 代价不可控；<br/>下游分析查询与加工计算争抢资源。</p><p>方案：使用 OceanBase 物化视图做增量 Join 加工</p><p>解决方案是在 OceanBase 内定义物化视图（MV），将多表 join 的加工逻辑固化为数据库内持续维护的结果集。</p><p>实现方式：<br/>基于四张源表定义 join 型 MV，生成加工宽表；<br/>下游查询直接访问 MV，不再执行多表 join；<br/>采用增量刷新模式：REFRESH FAST ON DEMAND；<br/>基于基表变更日志，仅对受影响数据做增量重算。</p><p>这样将“查询时 join”转换为“写入后增量维护”。</p><p>效果：增量刷新可控，宽表近实时可用</p><p>基于测试环境数据：<br/>增量刷新周期：每 5 分钟一次；<br/>非高峰期刷新耗时：1 分钟以内；<br/>全量刷新耗时：约 20 分钟（用于追位或重建）。</p><p>MV 宽表规模稳定在 50–100 万行，在大批量集中变更场景下，增量刷新仍可维持可控窗口，避免频繁全量重算。</p><p>该方案的核心收益集中在三个方面：<br/>将多表 join 的高成本计算前移为库内增量维护；<br/>适配“大批量突发变更 + 多表关联”的数据模式；<br/>为大促价格计算分析提供稳定的近实时宽表数据层。</p><p>对分析侧来说，查询路径从“多表实时 join”简化为“单宽表查询”，执行代价与延迟稳定性明显改善。</p><p>SaaS ERP 报表与分析 — 基于物化视图（MV）的数仓 ETL 简化与性能提升</p><p>业务问题：ERP 报表与分析需求</p><p>在企业资源计划（ERP）系统中，报表与分析对数据口径的稳定性和准确性要求较高。传统的 ETL（提取、转换、加载）流程可能涉及到多个系统或步骤，导致数据的加工过程冗长，效率低下。</p><p>在此场景下，需要解决以下问题：<br/>TP 实时入库与基于 MV 的近实时 ETL 加工在同一个 OceanBase 集群中并行运行；<br/>物理隔离：确保这两者的负载不相互干扰，并保证加工结果的稳定性与可持续性；<br/>稳定加工链路：报表与分析查询必须直接消费加工后的数据，避免重复计算和保证查询响应稳定。</p><p>技术挑战与方案：物化视图（MV）简化 ETL 分层</p><p>为了解决上述挑战，采用了 OceanBase 的物化视图（MV）功能，将数仓 ETL 的不同层次（明细层、主题加工层、报表层）直接固化为物化视图，并通过级联刷新机制保证数据一致性。</p><p><img width="610" height="264" referrerpolicy="no-referrer" src="/img/bVdnSi0" alt="" title="" loading="lazy"/></p><p>上述方案中：</p><p>ETL 分层处理：通过 MV 完成从明细层到主题加工层，再到报表层的数据流动。每一层的加工都在数据库内完成，最终结果直接存储在物化视图中。</p><p>物理隔离：将 TP 数据写入与 MV 数据容器表的 leader 放置到不同节点上，实现计算和存储的物理隔离。ETL 加工操作仅在 MV 所在节点进行，而数据的增量更新（MLOG）从 TP 节点读取，从而有效减少了负载冲突和资源争抢。</p><p>嵌套 MV 和级联刷新：通过自底向上的刷新机制，确保每个层次的数据都能保持一致性，特别适合需要稳定数据口径的分层加工模式。</p><p>方案优势：简化架构、加速报表分析</p><p>该方案的实施带来了明显的架构和性能优势：</p><p>架构简化：通过在 OceanBase 内部使用 MV 承接 ETL 分层加工，避免了外部计算和数据拼接链路的复杂性，减少了中间处理环节，降低了故障点和运维成本。</p><p>报表分析加速：高频查询的报表数据从“每次实时重算”变为“读取已经预计算好的 MV 数据”，使得报表查询变得更加高效和稳定，响应时间大幅缩短。</p><p>可控的实时性：通过物化视图的增量刷新，报表查询的实时性变得更加可控，避免了因复杂计算而导致的查询延迟。</p><p>性能与效果：稳定的 ETL 和报表查询性能</p><p>实施该方案后，OceanBase 系统的 ETL 加工和报表查询性能得到了显著提升，特别是在高频报表查询场景下，具体效果如下：</p><p>报表查询响应加速：查询不再依赖实时计算，而是直接读取加工后的 MV 数据，显著提高了查询的稳定性和响应速度。</p><p>ETL 加工稳定性：由于采用了物化视图的嵌套与级联刷新，ETL 加工链路中的每个环节都能保持一致性，减少了数据刷新过程中可能出现的错误或不一致。</p><p>高频操作的负载隔离：物理隔离机制使得 TP 入库负载与 MV 刷新负载互不干扰，保证了系统的整体性能稳定。</p><p>这套方案带来了多方面的技术价值：</p><p>提高 ETL 加工效率：通过物化视图将 ETL 流程内的多层次数据预计算并存储，减少了外部计算链路的依赖，提升了数据处理效率。</p><p>提升报表查询稳定性：报表查询通过直接访问 MV 中的加工数据，而不再依赖实时计算，减少了系统负担，提高了报表分析的稳定性和效率。</p><p>架构简化与运维降低：简化了 ETL 流程和查询路径，减少了复杂度，同时降低了运维成本，避免了多点故障的风险。</p><p>总的来说，OceanBase 的物化视图功能通过提供高效的数据加工和稳定的报表查询，帮助用友 ERP 系统实现了数据处理的优化，提升了业务分析的效率与可持续性。</p><h2>物化视图的使用限制</h2><p>值得注意的是，尽管物化视图功能强大，但在使用时也需要权衡其带来的成本与限制：</p><p>存储开销：物化视图是数据的物理副本，会额外占用存储空间。</p><p>维护成本：刷新物化视图会消耗 CPU 和 I/O 资源，需要合理规划刷新策略，避免对在线业务造成影响。</p><p>数据一致性：对于非实时物化视图，其数据与基表之间存在一定的延迟，应用需要能够容忍这种数据“过时”。</p><p>使用限制：物化视图本身不支持直接的 DML 操作，且基表的 DDL 操作可能会影响物化视图的有效性。</p><h2>物化视图能力演进计划</h2><p>为了让用户使用更顺手、更安心，OceanBase 会持续迭代物化视图能力，接下来的版本主要聚焦在以下核心能力：</p><p>01运维透明化（可观测性）</p><p>拒绝“黑盒”运行。上线刷新任务 <code>Explain</code> 及全链路可视化监控，提供任务级吞吐、延迟指标及明确的异常诊断报告，确保问题看得清、排得准。</p><p>02复杂链路支撑（Nested MV）</p><p>针对多层级数仓场景，持续优化嵌套物化视图的级联刷新能力，支持构建更深度的 ETL 加工链路。</p><p>03场景与类型扩展</p><p>广泛兼容：逐步支持外表（External Table）的物化能力。<br/>丰富类型：原生支持 JSON、LOB、Geometry 等复杂数据类型的增量计算。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=ziOmyAXJ9D%2FjByHiDAtGRw%3D%3D.dwrkW4E1AYmBxwUmu%2FzZogiESF7696lRbBSxYFm6Rn4%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[摩尔线程：云渲染负载能力测评 点量实时云渲染 ]]></title>    <link>https://segmentfault.com/a/1190000047596976</link>    <guid>https://segmentfault.com/a/1190000047596976</guid>    <pubDate>2026-02-06 16:09:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnSi5" alt="" title=""/><br/>点量团队在与用户交流的过程中发现，有不少用户对摩尔线程显卡的实际图形负载能力存在疑问。为解答这一疑问，点量团队将Linux系统下摩尔线程S80显卡，和Windows系统下的RTX 3060显卡做了个对比，测试了WebGL和UE两个场景，以实际数据评估其性能表现。</p><ul><li>测评环境1：Windows系统、RTX3060、点量云流实时云渲染windows版</li><li>测评环境2：Linux系统、摩尔线程S80、点量云流实时云渲染Linux版</li><li>测评3D应用：WebGL和UE两种引擎</li></ul><p>测试之前，在云推流过程中统一了分辨率为1920x1080，帧率为60FPS，并且同时设置开三路。得到以下测试结论：</p><h3>一、WebGL引擎测试</h3><p>以ThreeJS官方的某个示例做测试，结果如下：<br/><strong>1、Windows下3060：显卡利用率，平均在21%。</strong><br/><img width="723" height="287" referrerpolicy="no-referrer" src="/img/bVdnSi6" alt="" title="" loading="lazy"/><br/><img width="723" height="302" referrerpolicy="no-referrer" src="/img/bVdnSi7" alt="" title="" loading="lazy"/><br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdnSi8" alt="" title="" loading="lazy"/></p><p><strong>2、Linux下摩尔线程S80：显卡利用率在20%左右，和3060相差不大。</strong><br/><img width="723" height="607" referrerpolicy="no-referrer" src="/img/bVdnSi9" alt="" title="" loading="lazy"/><br/><strong>由此可见，摩尔线程S80在WebGL模式下几乎等同于3060显卡。</strong></p><h3>二、UE引擎测试</h3><p>以某个游戏UE场景做云推流测试，结果如下。若以Unity场景做测试，测试结果类似，这里不再展开。<br/><strong>1、Windows下3060：可以跑244.55fps，显卡利用率75%。</strong><br/><img width="206" height="232" referrerpolicy="no-referrer" src="/img/bVdnSja" alt="" title="" loading="lazy"/><br/><img width="723" height="431" referrerpolicy="no-referrer" src="/img/bVdnSjb" alt="" title="" loading="lazy"/><br/><img width="723" height="383" referrerpolicy="no-referrer" src="/img/bVdnSjc" alt="" title="" loading="lazy"/></p><p><strong>2、Linux下摩尔线程S80：只能跑20多FPS，显卡利用率97%。</strong><br/><img width="723" height="429" referrerpolicy="no-referrer" src="/img/bVdnSje" alt="" title="" loading="lazy"/><br/><img width="723" height="641" referrerpolicy="no-referrer" src="/img/bVdnSjf" alt="" title="" loading="lazy"/><br/><img width="723" height="466" referrerpolicy="no-referrer" src="/img/bVdnSjh" alt="" title="" loading="lazy"/></p><p>我们判断，由于UE默认使用的Vulkan RHI, 猜测是摩尔线程驱动针对Vulkan优化不足的缘故。随后，我们继续测试了S80在Windows下的效果，用相同的UE程序（默认Windows下是使用DirectX），只能到30多帧，因此可能也不只是对Vulkan优化的问题。对比3060的话，UE是可以跑到240多帧，这方面的差异还是比较明显。</p><p>另外，测试还发现，S80在Windows下的WebGL效果也不如Linux下的表现，Windows下开三个WebGL就掉帧了，GPU利用率100%。</p><p>以上效果说明，S80在Linux下的WebGL效果还是不错的，能跟3060达到类似性能效果。但在UE程序、Windows系统等一些效果上还是差距比较明显。</p><p>特别说明：本次所有测试均为在特定测试环境（包括但不限于特定机型、驱动版本、系统设置）中完成的结果。不同软硬件配置、测试方法或环境变量均可能导致数据差异，本文内容仅作为客观事实记录与经验分享，不作为官方性能指标或决策依据，请读者结合多方信息进行综合判断。</p><p>通过本次测试，明确了摩尔线程在云渲染负载中的性能表现，并验证了摩尔线程在相关场景下的实际承载能力。点量云流系统的兼容适配能力并不局限于单一系统或硬件，且已在多系统、多配置场景中实现全面支持，真正做到了“一次适配，处处运行”，为不同技术架构下的用户提供统一、可靠的高性能云流服务。</p><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnSji" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[代码重构: 用实际的例子去讲解模版方法 代码丰 ]]></title>    <link>https://segmentfault.com/a/1190000047596978</link>    <guid>https://segmentfault.com/a/1190000047596978</guid>    <pubDate>2026-02-06 16:09:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>模板方法模式</h2><p>在很多业务代码中，我们都会遇到这种场景：</p><ul><li><strong>整体流程是固定的</strong></li><li><strong>但流程中的某些步骤因业务类型不同而不同</strong></li><li>又不希望子类随意修改流程顺序</li></ul><p>如果你把这些逻辑全部写进一个类里，往往会出现：</p><ul><li>类越来越大</li><li>if / switch 越来越多</li><li>改一处逻辑容易影响其他场景</li></ul><p>这正是 <strong>模板方法模式</strong> 要解决的问题。</p><hr/><h3>一、问题从哪里来？</h3><p>一开始，我有一个用于“保存代码文件”的工具类，大概长这样：</p><ul><li>校验参数</li><li>创建唯一目录</li><li>把代码写成文件</li><li>返回目录路径</li></ul><p>当前代码存在不同的报错逻辑：</p><ul><li>HTML 单文件是一套保存逻辑</li><li>HTML + CSS + JS 又是一套保存逻辑</li><li>所有逻辑都堆在一个类里，越来越臃肿</li></ul><p>导致：  <br/><strong>这个类在不断变大，而且每加一种类型就要改原有代码。</strong></p><hr/><h3>二、什么是不变的？</h3><p>整理之后会发现：</p><p><strong>不变的部分：</strong></p><ol><li>保存流程是固定的</li><li>都要校验参数</li><li>都要创建目录</li><li>最后都返回一个目录</li></ol><p><strong>变化的部分：</strong></p><ul><li>写哪些文件</li><li>每个文件的内容来自哪里</li></ul><p>这正好符合一句模版方法的逻辑：</p><blockquote>流程固定，具体实现内容待定。</blockquote><hr/><h3>三、模板方法模式的核心想法</h3><p>模板方法模式其实很简单：</p><blockquote><strong>把“流程”放在父类，把“变化”交给子类。</strong></blockquote><p>父类只做一件事：  <br/><strong>规定顺序，不让子类乱来。</strong></p><hr/><h3>四、一个简化后的模板类</h3><pre><code class="bash">public abstract class CodeFileSaverTemplate&lt;T&gt; {

    public final File saveCode(T result) {
        validate(result);
        String dir = createDir();
        saveFiles(result, dir);
        return new File(dir);
    }

    protected void validate(T result) {}

    protected abstract void saveFiles(T result, String dir);

    protected String createDir() {
        
        return dir;
    }
}</code></pre><p>这里有三个关键信号：</p><ul><li><code>saveCode()</code> 是 <code>final</code>：流程不能被改【固定的模版流程】</li><li><code>protected</code>：只给子类用</li><li><code>abstract</code>：子类必须实现</li></ul><hr/><h3>五、子类只关心自己具体实现</h3><h4>HTML 保存</h4><pre><code class="bash">public class HtmlSaver extends CodeFileSaverTemplate&lt;HtmlCodeResult&gt; {

    @Override
    protected void saveFiles(HtmlCodeResult result, String dir) {
        write(dir, "index.html", result.getHtmlCode());
    }
}</code></pre><h4>多文件保存</h4><pre><code class="bash">public class MultiFileSaver extends CodeFileSaverTemplate&lt;MultiFileCodeResult&gt; {

    @Override
    protected void saveFiles(MultiFileCodeResult result, String dir) {
        write(dir, "index.html", result.getHtmlCode());
        write(dir, "style.css", result.getCssCode());
        write(dir, "script.js", result.getJsCode());
    }
}</code></pre><p>子类不需要关心：</p><ul><li>目录怎么建</li><li>校验顺序</li><li>返回值</li></ul><p>只管一件事：  <br/><strong>我要写哪些文件。</strong></p><hr/><h3>六、为什么不用 if / switch？</h3><p>当然可以写成这样：</p><pre><code class="bash">if (type == HTML) { ... }
else if (type == MULTI) { ... }</code></pre><p>但问题是：</p><ul><li>每加一种类型就要改这个类</li><li>老逻辑和新逻辑混在一起</li><li>长期一定失控</li></ul><p>模板方法的好处是：</p><blockquote><strong>新增一种类型 = 新增一个类，不动旧代码。</strong></blockquote><hr/><h3>七、什么时候该用模板方法？</h3><p>适合用在：</p><ul><li>流程天然有顺序</li><li>顺序不允许被破坏</li><li>变化点明确、可控</li></ul><p>不适合用在：</p><ul><li>流程差异非常大</li><li>需要频繁运行时切换逻辑</li><li>不想使用继承的场景</li></ul>]]></description></item><item>    <title><![CDATA[Python量化实战：WebSocket协议在美股行情获取中的应用 EmilyLi ]]></title>    <link>https://segmentfault.com/a/1190000047596994</link>    <guid>https://segmentfault.com/a/1190000047596994</guid>    <pubDate>2026-02-06 16:08:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在金融科技（FinTech）开发中，Real-time Data Fetching 是最基础也是最核心的模块。最近在重构我的交易系统，特地把数据接入层剥离出来做一个技术分享。</p><p><strong>背景与问题</strong> 传统的Web开发中，我们习惯用REST API处理请求。但在金融交易场景下，HTTP协议存在明显的短板：</p><ol><li>Header开销大：高频请求下，流量浪费严重。</li><li>被动获取：无法做到服务器端的主动推送（Server Push）。</li><li>并发限制：容易触流控（Rate Limit）。</li></ol><p>对于美股这种Tick级别的数据量，WebSocket是唯一的正解。</p><p><strong>技术实现路径</strong> 我的需求很简单：订阅AAPL、TSLA等热门标的的实时Tick，并存入Redis做清洗。在对比了多家数据提供商后，为了兼容性和稳定性，我选择了AllTick作为上游数据源，配合Python的<code>websocket-client</code>库进行开发。</p><p><strong>代码架构</strong> 整个模块采用异步回调的方式处理数据，确保主线程不阻塞。以下是最小可行性产品（MVP）的代码实现：</p><pre><code>import websocket
import json

# WebSocket连接地址（替换为实际API接口）
url = "wss://api.alltick.co/realtime/stock"

# 请求体，订阅的股票代码和API密钥
message = {
    "api_key": "your_api_key_here",  # 你的API密钥
    "symbol": "AAPL"  # 订阅Apple的实时行情
}

def on_message(ws, message):
    data = json.loads(message)
    print(f"实时获取的数据：{data}")

def on_error(ws, error):
    print(f"发生错误：{error}")

def on_close(ws, close_status_code, close_msg):
    print("WebSocket连接已关闭")

def on_open(ws):
    ws.send(json.dumps(message))

# 创建WebSocket应用并启动
ws = websocket.WebSocketApp(url,
                            on_message=on_message,
                            on_error=on_error,
                            on_close=on_close)
ws.on_open = on_open

# 保持连接并接收数据
ws.run_forever()</code></pre><p><strong>技术细节注意事项</strong> 在实际部署中，还需要考虑断线重连（Reconnection）和心跳检测（Heartbeat）。上述代码展示了最基础的订阅逻辑。通过<code>on_message</code>回调，我们可以直接解析JSON数据包。</p><p>经测试，这种方式比传统的<code>while True: requests.get()</code>循环，延迟降低了至少两个数量级。对于开发者来说，掌握WebSocket在金融数据处理中的应用，是一项必备技能。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnSar" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[从零开始学Flink：Flink SQL 极简入门 代码匠心 ]]></title>    <link>https://segmentfault.com/a/1190000047597004</link>    <guid>https://segmentfault.com/a/1190000047597004</guid>    <pubDate>2026-02-06 16:07:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Flink SQL 是 Apache Flink 的核心模块之一，它让开发者可以使用标准的 SQL 语法来编写流处理和批处理作业。对于不想深究 Java/Scala 复杂 API 的“小白”来说，Flink SQL 是进入实时计算领域的最佳敲门砖。</p><p>本文将基于 <strong>Flink 1.20.1</strong> 版本，手把手教你在 WSL2 (Ubuntu) 环境下搭建环境，并运行你的第一个 Flink SQL 任务。</p><h2>一、为什么选择 Flink SQL？</h2><ol><li><strong>低门槛</strong>：会写 SQL 就能开发实时任务。</li><li><strong>统一性</strong>：批流一体，同一套 SQL 既可以跑历史数据（批），也可以跑实时数据（流）。</li><li><strong>生态丰富</strong>：内置了大量的 Connector（连接器），轻松连接 Kafka、MySQL、Hive 等主流组件。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597007" alt="Flink SQL 架构图" title="Flink SQL 架构图"/><br/><em>(图：Flink SQL 架构示意图，展示 SQL 解析、优化到执行的过程)</em></p><h2>二、环境准备 (WSL2 Ubuntu)</h2><p>本教程演示环境为 Windows 下的 WSL2 (Ubuntu 20.04/22.04)，这是目前 Windows 用户体验 Linux 开发环境的最佳姿势。<br/>参考以前些的文章<a href="https://link.segmentfault.com/?enc=FUQM4YA0xtCPalvJn4gdRQ%3D%3D.X1DsA%2Bte5p6MYuW1y6M%2BHGYijUZnhzfDmRT%2BrZlKVBoMTg1YCL%2BSQ%2FLxxVZJReMxExgckjka5jDzmeiR6C%2Fzkg%3D%3D" rel="nofollow" target="_blank">从零开始学Flink：揭开实时计算的神秘面纱</a>，搭建好 Flink 环境。</p><h2>三、体验 Flink SQL Client</h2><p>Flink 提供了一个交互式的命令行工具：<strong>SQL Client</strong>。它允许你直接在终端编写和提交 SQL 任务。</p><h3>1. 启动 SQL Client</h3><p>如果没有启动Flink集群,则先启动flink集群:</p><pre><code class="bash">./bin/start-cluster.sh</code></pre><p>,然后在 Flink 目录下执行：</p><pre><code class="bash">./bin/sql-client.sh</code></pre><p>你将看到那只著名的松鼠 LOGO：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597008" alt="SQLClient启动界面" title="SQLClient启动界面" loading="lazy"/><br/><em>(图：SQL Client 启动欢迎界面)</em></p><h3>2. Hello World：数据生成与打印</h3><p>我们不依赖任何外部组件（如 Kafka），直接使用 Flink 内置的 <code>datagen</code> 连接器生成模拟数据，并用 <code>print</code> 连接器打印结果。</p><p><strong>第一步：创建源表 (Source Table)</strong></p><p>复制以下 SQL 到 SQL Client 中执行：</p><pre><code class="sql">CREATE TABLE source_table (
    id INT,
    name STRING,
    ts TIMESTAMP(3),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
) WITH (
    'connector' = 'datagen',       -- 使用数据生成器
    'rows-per-second' = '1',       -- 每秒生成1条数据
    'fields.id.kind' = 'sequence', -- id 字段为序列
    'fields.id.start' = '1',       -- id 从1开始
    'fields.id.end' = '100'        -- id 到100结束
);</code></pre><p>执行后显示 <code>[INFO] Execute statement succeed.</code>。</p><p><strong>第二步：创建结果表 (Sink Table)</strong></p><pre><code class="sql">CREATE TABLE print_table (
    id INT,
    name STRING,
    ts TIMESTAMP(3)
) WITH (
    'connector' = 'print'          -- 使用控制台打印连接器
);</code></pre><p><strong>第三步：提交任务</strong></p><p>将源表的数据插入到结果表：</p><pre><code class="sql">INSERT INTO print_table SELECT * FROM source_table;</code></pre><p>此时，SQL Client 会提交一个异步任务到集群。你会看到类似 Job ID 的输出。</p><h3>3. 查看运行结果</h3><p>由于我们使用的是 <code>print</code> 连接器，在 Standalone 模式下，输出会打印到 TaskManager 的日志文件中。</p><p>打开一个新的 WSL2 终端窗口，进入 Flink 目录查看日志：</p><pre><code class="bash"># 进入 log 目录
cd log

# 查看最新的 .out 文件 (文件名包含 taskexecutor)
tail -f flink-*-taskexecutor-*.out</code></pre><p>你应该能看到屏幕上不断跳动的数据流：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597009" alt="运行结果日志截图位置" title="运行结果日志截图位置" loading="lazy"/><br/><em>(图：终端 tail -f 命令看到的实时数据输出)</em></p><h2>四、常用命令速查</h2><p>在 SQL Client 中，你可以使用以下命令：</p><ul><li><code>HELP</code>: 查看帮助。</li><li><code>SHOW TABLES</code>: 查看当前创建的表。</li><li><code>SHOW JOBS</code>: 查看运行中的作业。</li><li><code>DESCRIBE table_name</code>: 查看表结构。</li><li><code>QUIT</code>: 退出 SQL Client。</li></ul><h2>五、总结</h2><p>恭喜你！你已经成功运行了人生中第一个 Flink SQL 任务。</p><p>通过本文，我们完成了：</p><ol><li>WSL2 下 Java 和 Flink 1.20.1 的安装。</li><li>启动了 Flink 本地集群。</li><li>使用 SQL Client 创建了 Source 和 Sink 表，并跑通了数据流。</li></ol><p>下一篇，我们将深入讲解 Flink SQL 中的<strong>窗口（Window）</strong>操作，看看如何处理“过去5分钟的订单总额”这类经典需求。</p><hr/><p><strong>参考资料</strong>：</p><ul><li><a href="https://link.segmentfault.com/?enc=dlUPzWnp6W%2BJ3Yp%2BNEt1Fg%3D%3D.HtdlzlW3Yb9J0gucvz%2FlFmDYnA9jkyCi%2B9c4Ko3OImISEzT7MgOxu8hXieVeKCYp7HtgtpWZ6kzFzUYa1fjsCw%3D%3D" rel="nofollow" target="_blank">Flink 官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=UCZTHTy1yZ3xQvKoyiKsPQ%3D%3D.WHM3Fo8WBhai9n4cVQEdGd6%2Br%2B3o52y3VhvBAH5zkELk3PKGU9h6riklIZDwaoZO%2B7bS%2B26naeHm4keqIl1YYQ%3D%3D" rel="nofollow" target="_blank">原文来自</a></li></ul>]]></description></item><item>    <title><![CDATA[一款可提高后台系统开发效率的低代码平台 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047597015</link>    <guid>https://segmentfault.com/a/1190000047597015</guid>    <pubDate>2026-02-06 16:06:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>后台系统（如ERP、OA、CRM，）作为企业内部系统，具有需求明确、变化频繁、逻辑复杂度适中的特点，而后台管理系统的开发效率和响应速度直接影响到运营效率，更甚者可能影响到业务竞争力。</p><p>传统开发模式面临成本高、周期长、维护难等挑战。</p><p>本文从背景分析、设计方案、核心模块及核心功能三个维度，系统阐述低代码平台如何从根本上提升运营后台开发效率，为企业数据化转型提供新路径。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597017" alt="image.png" title="image.png"/></p><h2>一、背景分析</h2><p>传统运营开发模式的核心痛点：</p><p>1、重复造轮子，创新成本高昂</p><p>运营后台70%功能是增删改查，简单的列表查询配置业务，占比30%，相同功能在不同项目中反复开发，不仅浪费资源，更导致技术债务累积。</p><p>2、响应速度滞后</p><p>简单的列表查询配置业务，从需求提出到最终上线，传统开发需求经历需求评审、技术设计、编码实现、测试验证、部署上线等多个环节，平均耗时3-5天。</p><h2>二、设计方案</h2><p>以贴合自身业务需求、落地开发提效为核心导向，前期充分调研业内主流低代码平台，深度分析各平台的功能优势，不盲从”全功能“标杆，而是以自身开发需求、业务场景、技术栈特性为标尺，构建低代码平台。</p><p>平衡功能与易用性，拒绝过度开发，针对平台基础能力与通用组件，采用“基础配置+业务轻封装”的设计思路。基础项保留灵活的原生配置能力；同时结合运营后台业务的通用场景，对高频使用的功能、组件进行简单封装，既保留配置灵活性，又能适配业务实际。平台整体架构设计如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597018" alt="image.png" title="image.png" loading="lazy"/></p><p>应用层模块介绍</p><p>业务中心模块：接收页面管理模块同步的已发布页面，展示用户有权限的业务页面。</p><p>权限模块管理：管理系统权限、页面权限、用户。</p><p>数据源管理模块：</p><p>数据源管理：对接数据库、API接口，支持数据源的新增、编辑、停用等；</p><p>元数据配置：可视化配置数据库字段信息，定义字段类型（文本/数值/日期）、显示别名等。</p><p>页面管理模块</p><p>页面创建：提供空白画布、预制组件，支持拖拽式组件布局（文本、表格、筛选器等）。</p><p>组件配置：配置基础属性、数据源、事件交互等。</p><p>预览并保存：生成预览结果；保存即发布。</p><p>四大模块共同构建了平台的基础能力矩阵，但从业务落地的优先级来看，页面管理模块是驱动整个平台运转的核心引擎 —— 运营人员的所有配置操作都围绕它展开，其他模块也都是为它提供数据、权限、业务关联的支撑。</p><p>基于此，我们将从整体框架下沉到核心模块及核心功能。</p><h2>三、核心模块及核心功能</h2><p>01、核心模块——页面管理模块<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597019" alt="image.png" title="image.png" loading="lazy"/></p><p>页面管理模块分三大核心：引擎、物料服务、数据源。</p><p>三个核心的职责</p><p>引擎（蓝色模块）：是页面构建的 “操作中枢”，负责页面的可视化编辑与最终输出。它提供了页面拖拽布局、页面编排、组件树管理、组件属性配置等功能（运营人员通过这些操作完成页面的样式与逻辑配置），最终通过 “渲染、出码” 能力，将配置好的内容转化为可访问的页面。</p><p>物料服务（绿色模块）：是页面的 “组件资源库”，负责组件的生产、管理与规范。它提供标准化的组件（如按钮、表格、图表），并通过 “物料规范” 保证组件的统一性；同时支持组件的版本管理，确保不同页面使用的组件版本一致。</p><p>数据源（黄色模块）：是页面的 “数据支撑”，负责提供页面所需的业务数据。它对接企业的数据库、API 等数据来源，为页面中的组件（如数据表格、报表）提供数据绑定的基础。</p><p>三个核心的协同流程</p><p>物料服务与引擎联动：物料服务将标准化组件（按物料规范）同步到引擎的 “组件树” 中，供引擎的页面编辑功能调用。</p><p>数据源与引擎联动：数据源提供数据元信息，供引擎在 “属性配置” 环节将组件与具体业务数据绑定。</p><p>引擎整合输出：引擎通过 “拖拽布局、页面编排” 整合物料组件与数据源，最终通过 “渲染、出码” 生成可访问的页面（图下方的最终产物）。</p><p>核心功能</p><p>页面管理模块，通过具体能力支撑业务快速开发落地，核心功能主要包含：可视化页面开发、自定义组件、发布、出码。</p><p>可视化页面开发：可视化页面开发是低代码的核心优势。开发者通过拖拽组件绘制页面布局和配置组件与数据源绑定，整合物料组件与数据源，无需编写大量前端代码，快速实现项目落地。下面以实例介绍可视化页面开发过程。</p><p>实例：开发项目阶段信息维护页面</p><p>页面需求：展示页面名称“项目阶段信息维护，展示数据表格，包含id、项目阶段名称、操作三列，表格按照 id 正序排列，列表项支持编辑和删除两种操作，支持弹窗新增数据，并且需要校验阶段名称必填。</p><p>页面开发流程：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597020" alt="image.png" title="image.png" loading="lazy"/></p><p>重点介绍拖拽组件绘制页面布局和配置组件与数据源绑定。</p><p>页面绘制区块介绍：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597021" alt="image.png" title="image.png" loading="lazy"/><br/>图源：织信低代码</p><p>在组件库中，选择合适的组件，拖拽到编辑器画布中；</p><p>对画布中的组件，进行组件配置，例如组件基础属性、数据源、交互事件、样式等。</p><p>针对实例需求：</p><p>标题：使用Title组件</p><p>新增按钮和列表：使用高级表格组件</p><p>配置组件与数据源绑定</p><p>列表：配置数据列，配置列标题、数据类型、数据字段、对齐方式等。</p><p>新增按钮：配置操作栏，配置名称、按钮样式、是否禁用、是否隐藏、点击事件等；其中点击事件绑定用户在左侧源码面板中创建的组件事件。</p><p>新增弹窗：配置弹窗标题、弹窗按钮、弹窗中表单、表单项校验是否必填项等。</p><p>最终页面结果：在上面的实例中，我们使用的是平台标准化组件，拖拽组件搭建页面布局，配置组件属性实现交互逻辑，将开发时间从天压缩到小时，显著提升开发效率。</p><p>功能二</p><p>自定义组件</p><p>组件库是整个平台的核心基石与价值载体，它直接决定了平台的开发效率、功能上限、交付质量和用户体验，是低代码模式能够实现 “少写代码、快速交付” 的核心支撑。</p><p>自定义组件让平台能够满足个性化的业务需求，突破标准化组件的功能局限。下面以实例介绍自定义组件开发过程。</p><p>实例：自定义组件table，实现表格展示支持多场景</p><p>背景介绍：标准化表格组件，不满足运营对列表展示要求，存在展示局限性：</p><p>仅支持基础文本展示，无法满足运营后台多样化内容展示需求，例如图片、超链接、音频、视频等</p><p>原始数据可读性差，例如：操作时间展示时间戳，状态展示0、1等</p><p>解决思路：</p><p>归纳运营后台常见列展示需求，覆盖多类型内容形式，</p><p>增加表格列属性配置，不同类型搭配不同配置项，</p><p>支持拓展，实现跨列数据源展示等。</p><p>开发实现：遵守物料协议和引擎规则标准化开发，确保组件能无缝融入平台的页面编辑与渲染体系；</p><p>类型汇总：</p><p>基础类：文字，直接展示列绑定数据，无需其他配置项</p><p>时间类：时间，将时间戳转换为YYYY-MM-DD HH:mm:ss 格式，无需其他配置项</p><p>媒体类：</p><p>图片，配置图片路径、替代文本</p><p>音频，配置音频资源</p><p>视频，配置视频资源</p><p>链接类：超链接，配置跳转地址、链接文案</p><p>拓展类：HTML自定义片段，配置渲染函数，满足个性化需求</p><p>部署发布：</p><p>组件物料独立打包发布</p><p>修改平台依赖包配置</p><p>平台重新构建发布</p><p>平台共实现自定义组件10+个，包括表格组件、统计图组件、菜单组件、布局组件等，满足运营后台个性化业务需求，丰富组件库生态，实现根据业务需求持续新增、优化组件，保障平台能力与企业业务发展同频，降低技术架构的迭代成本。</p><p>功能三</p><p>发布</p><p>平台采用保存即发布模式，页面配置完成后，直接同步至平台数据库，无需编写代码、单独测试与复杂的环境部署，可通过平台生成的专属链接直接访问使用，部分轻量需求甚至能实现小时级配置、即时上线，大幅简化开发流程，省去传统模式中编码、部署等核心耗时环节，将简单需求的交付周期从天级压缩至小时级，大幅提升业务需求的响应与落地效率。</p><p>功能四</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597022" alt="image.png" title="image.png" loading="lazy"/></p><p>出码</p><p>平台深度基于 Vue Render 渲染机制构建，支持通过可视化拖拽、属性配置完成页面 / 组件的可视化绘制后，一键导出基于 Vue2.js 框架 + ElementUI 组件库的标准 Vue 单文件组件（.vue）源代码。导出的代码完全遵循 Vue2 语法规范与 ElementUI 组件调用逻辑，保留完整的组件结构（template/script/style）、属性配置、事件绑定及逻辑关联，无黑盒封装，可直接在 Vue2 项目中复用、二次开发或部署运行，实现 “可视化搭建” 与 “原生代码” 的无缝衔接。</p><p>出码优势：摆脱平台绑定，掌握开发自主权</p><p>获取完整的源代码，能基于导出的文件进行独立部署、运维和迭代，让系统开发不再依赖低代码平台本身。</p><p>兼顾高效开发与深度定制，平衡效率与灵活：前期依托低代码可视化拖拽快速完成项目主体搭建，大幅缩短开发周期；针对高复杂度、高个性化的业务需求，可导出源文件通过纯代码进行深度定制优化，既用低代码省时间，又用原生代码满足极致化需求，实现“高效搭建 + 深度定制”的双重效果。</p><p>二次开发的灵活性：跨项目、跨技术栈开发时，源文件能被复用、拆解，提升整体开发资源利用率。</p><p>跨项目、跨技术栈开发时，源文件能被复用、拆解，提升整体开发资源利用率。</p><h2>结论</h2><p>总而言之，织信低代码平台以可视化配置、快速部署、低门槛上手的核心优势，契合当下对数字化开发 “快、准、省” 的需求。它不仅是一款开发工具，更是企业数字化转型的专属解决方案，用技术简化开发，用效率赋能业务，这正是低代码平台的核心价值所在。</p>]]></description></item><item>    <title><![CDATA[如何通过AI技术实现汽车冲压工艺的精度与效率双重突破？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047597047</link>    <guid>https://segmentfault.com/a/1190000047597047</guid>    <pubDate>2026-02-06 16:05:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代汽车制造中，冲压工艺作为车身成型的核心环节，其稳定性与精度直接影响整车质量与生产节拍。然而，传统冲压依赖工程师经验进行参数调整，面对材料批次差异、模具磨损、环境温湿度波动等复杂变量，往往陷入“试错—反馈—再试错”的低效循环。模具更换动辄数小时，首件合格率徘徊在80%左右，不仅造成产能浪费，更推高了单位制造成本。这一困境的本质，是多变量强耦合与动态不确定性超出了人类经验的掌控边界。要真正突破瓶颈，必须引入一种能实时感知、自主学习、动态响应的智能系统，而人工智能，正成为破解这一难题的关键钥匙。<br/>AI对冲压工艺的赋能，不在于替代人，而在于扩展人的感知与决策能力。通过在压力机、模具、送料系统等关键节点部署高精度传感器，结合PLC与MES系统，AI平台能够采集每一道工序的温度、压力、位移、振动等数百项参数，并以毫秒级频率进行流式处理。这些数据不再是孤立的数字，而是被赋予了时间序列与工艺语义的“工艺语言”。基于LSTM、Transformer等深度学习模型，系统能从历史数据中挖掘出材料回弹、模具热变形与压力补偿之间的隐性关联，构建出超越传统物理模型的预测能力。更重要的是，AI能将资深工程师的“手感”转化为可复用的决策规则，比如当模具温度上升至某一阈值时，自动触发压力微调补偿，这种隐性知识的显性化，使工艺优化从经验驱动转向数据驱动。<br/>在实践层面，广域铭岛的Geega工业AI平台已在多家头部车企落地。以某自主品牌新能源车企为例，其冲压线曾因模具热变形导致尺寸超差，返修率高达12%。部署AI系统后，平台通过实时监测模具温度场变化，结合历史回弹数据训练出动态补偿模型，自动调节液压机压力曲线，使曲轴冲压件尺寸精度稳定在±0.02mm以内，换模时间从4小时压缩至1.5小时，设备综合效率提升35%，年节约成本超1800万元。无独有偶，德国博世旗下汽车零部件工厂也引入了基于数字孪生的AI冲压优化系统，通过在虚拟环境中模拟不同材料与润滑条件下的成型过程，提前预测模具寿命与缺陷风险，实现预防性维护与参数预调，使模具更换周期减少40%，废品率下降32%。两家企业的共同点在于，都未追求“一步到位”的全面改造，而是从关键工位试点，逐步构建起数据闭环与持续优化机制。<br/>AI驱动的冲压工艺优化，正在重新定义汽车制造的精度边界。它不是一次性的技术升级，而是一场从“人适应机器”到“机器理解工艺”的范式转变。未来，随着多模态感知与自主决策代理的发展，冲压线或将实现真正意义上的“无人干预、自适应运行”，为全球汽车产业迈向零缺陷、零浪费的智能制造新阶段提供坚实支撑。</p>]]></description></item><item>    <title><![CDATA[软件开发交付模式的进化：为何「周期订阅制」正在成为最优选择 vistart ]]></title>    <link>https://segmentfault.com/a/1190000047597051</link>    <guid>https://segmentfault.com/a/1190000047597051</guid>    <pubDate>2026-02-06 16:04:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化浪潮中，软件定制开发已成为企业竞争力的核心引擎。然而，项目的失败率依然居高不下，常见场景是：客户对最终交付物不满意，开发方则抱怨需求反复无常。其根源往往不在于技术能力，而在于选择了不匹配的<strong>交付与协作模式</strong>。</p><p>本文将深入剖析两种主流的交付模式，并论证为何<strong>周期性订阅制（升级迭代模式）</strong> 正逐渐取代传统的一次性买断模式（按图施工模式），成为对客户和开发方都更为明智的选择。</p><hr/><h2>一、两种交付模式的本质剖析</h2><h3>1. 按图施工式（一次性买断模式）</h3><ul><li><strong>核心理念</strong>：交付一个<strong>确定的、完整的软件产品</strong>。</li><li><strong>类比</strong>：如同建筑领域的「总包工程」。客户提供详细的设计图纸（需求文档），开发方作为「施工队」，负责在规定时间和预算内按图完工，验收交付后即完成主要合同义务。</li><li><strong>特点</strong>：需求需高度明确，合同范围固定，采用<strong>固定总价或分阶段里程碑付款</strong>。变更需要繁琐的流程和额外的费用。</li></ul><h3>2. 协作设计式（周期订阅制/迭代模式）</h3><ul><li><strong>核心理念</strong>：提供一项<strong>持续的软件进化服务</strong>。</li><li><strong>类比</strong>：如同客户雇佣了一位长期的「建筑设计师兼工程顾问」。双方从概念草图开始，通过不断沟通、建造样板间（MVP）、调整优化，最终共同建造出理想的房屋，并在入住后持续提供改建和维保服务。</li><li><strong>特点</strong>：需求在过程中动态澄清和调整，采用<strong>敏捷开发、小步快跑</strong>，按时间材料或周期性（如月度/年度）订阅付费。</li></ul><hr/><h2>二、模式对比：一次性买断 vs. 周期订阅</h2><table><thead><tr><th align="left">维度</th><th align="left"><strong>一次性买断模式（按图施工）</strong></th><th align="left"><strong>周期订阅制（协作设计）</strong></th></tr></thead><tbody><tr><td align="left"><strong>需求适应性</strong></td><td align="left">低。前期必须锁定需求，后期变更成本极高。</td><td align="left">高。拥抱变化，需求随市场与认知迭代。</td></tr><tr><td align="left"><strong>客户风险</strong></td><td align="left"><strong>极高</strong>。承担了「图纸是否正确」的全部产品风险。</td><td align="left"><strong>低</strong>。风险被分摊到各个周期，每个迭代都在验证和降低风险。</td></tr><tr><td align="left"><strong>开发方风险</strong></td><td align="left"><strong>交付风险高</strong>。必须完全按约定规格交付，超支风险自担。</td><td align="left"><strong>前期启动风险存在</strong>，但通过MVP可控制。长期关系更稳定。</td></tr><tr><td align="left"><strong>费用与现金流</strong></td><td align="left">客户：一次性大笔支出。<br/>开发方：现金流波动大，项目间隙有收入空窗。</td><td align="left">客户：可预测的运营费用，启动门槛低。<br/>开发方：<strong>持续稳定的现金流</strong>。</td></tr><tr><td align="left"><strong>合作关系</strong></td><td align="left">交易型、契约型，易因范围变更产生对立。</td><td align="left"><strong>伙伴型、协作型</strong>，目标一致（让产品成功）。</td></tr><tr><td align="left"><strong>心理账户</strong></td><td align="left">客户视为「重大资本投资」，期待完美，容错率低。</td><td align="left">客户视为「持续运营成本」，关注价值增长，容错与协作空间大。</td></tr><tr><td align="left"><strong>交付体验</strong></td><td align="left">漫长的开发期后「开盲盒」，<strong>峰终体验取决于最终验收</strong>。</td><td align="left">频繁的小版本交付与演示，持续获得<strong>正向反馈与成就感</strong>。</td></tr><tr><td align="left"><strong>对开发方的长期价值</strong></td><td align="left">项目结束，价值终止。代码复用偶然。</td><td align="left"><strong>持续积累可复用组件与行业解决方案</strong>，形成技术资产与竞争壁垒。</td></tr></tbody></table><hr/><h2>三、为何劝说各方拥抱周期订阅制？</h2><h3><strong>对客户的八大价值主张</strong></h3><ol><li><strong>降低决策门槛与风险</strong>：无需在项目启动时就押注全部预算，可以用最小的成本验证核心思路，让投资始终追逐可见的价值。</li><li><strong>掌控感十足</strong>：您不是花钱买一个「黑盒」，而是雇佣一个团队。您全程参与，每2-4周就能看到进展并调整方向。</li><li><strong>灵活应对变化</strong>：市场在变，您的需求也会变。订阅制让您的软件能够像业务一样敏捷进化，而非被半年前签订的合同所束缚。</li><li><strong>财务更健康</strong>：将不确定的资本性支出（CAPEX）转化为清晰、可预测的运营费用（OPEX），便于财务规划和审批。</li><li><strong>获得持续关怀</strong>：软件上线只是开始，持续的优化、适配和运维支持同样关键。订阅制天然包含了这份长期承诺。</li><li><strong>期望值管理</strong>：多次、小额的支付降低了「巨额花费必须完美」的焦虑感，让协作氛围更积极、理性。</li><li><strong>专注于业务，而非技术细节</strong>：您无需一次性想清所有功能细节，可以优先解决当前最痛的痛点，把专业问题交给专业团队。</li><li><strong>建立长期技术伙伴</strong>：您获得的不是一个供应商，而是一个深度理解您业务、伴随您成长的技术伙伴。</li></ol><h3><strong>对开发方的六大战略优势</strong></h3><ol><li><strong>构建可持续的商业模式</strong>：从「项目驱动」的饥饱不定，转向「服务驱动」的稳定现金流，业务更健康，估值更高。</li><li><strong>沉淀核心资产，实现复利增长</strong>：每个项目积累的通用模块、行业解决方案，都是未来项目的「加速器」。开发成本边际递减，交付速度和质量却边际递增，形成强大竞争壁垒。</li><li><strong>深化客户关系，提升生命周期价值</strong>：长期合作带来深度信任，您从成本部门转变为价值部门，客户流失率低，增购和转介绍机会多。</li><li><strong>优化团队与项目管理</strong>：稳定的工作流和节奏，有利于团队技术成长和知识沉淀，减少因项目频繁启动/结束带来的管理损耗。</li><li><strong>专注于创造价值，而非争论范围</strong>：摆脱了「按合同条款扯皮」的消耗战，将精力真正投入到为客户解决问题、创造价值上，从而获得更高的职业成就感和客户满意度。</li><li><strong>吸引优质客户</strong>：这种模式会自动筛选出那些看重长期价值、理性决策、寻求伙伴的优质客户，远离那些一味比价、期望不切实际的客户。</li></ol><hr/><h2>四、如何成功开启订阅制合作？</h2><ol><li><strong>从最小可行产品（MVP）开始</strong>：用4-8周时间，集中资源打造一个解决最核心痛点的、可运行演示的MVP。这是建立信任的第一块基石。</li><li><strong>设计清晰的服务产品包</strong>：明确订阅费包含的内容（如：每月X人天的开发投入、运维支持范围、沟通机制等），让服务标准化、透明化。</li><li><strong>拥抱极致透明</strong>：使用协作工具共享开发看板，坚持定期演示与复盘。让客户的钱花得明明白白，进度看得清清楚楚。</li><li><strong>签订着眼于长期的合同</strong>：在合同中明确服务模式、知识产权归属（通用组件归属开发方）、续约与终止条款，为长期合作奠定法律基础。</li></ol><h3><strong>给开发方的关键话术</strong></h3><p>“我们建议采用分阶段订阅合作，核心是 <strong>‘为您降低风险，而非抬高预算’</strong>。我们先投入一小笔资金和几周时间，快速打造一个核心功能可用的‘样品’。您能立刻验证方向。若可行，我们就像您的内部技术团队一样，每月规划、每月投入、每月交付可见价值。您的资金始终在驱动确定的进展，而非消耗在漫长的、不确定的等待中。”</p><hr/><h2>结语</h2><p>从「按图施工」到「协作设计」，从「一次性买断」到「周期订阅」，这不仅仅是收费方式的变化，更是软件开发行业从<strong>手工作坊模式向现代专业服务模式</strong>的范式转移。</p><p>它要求开发方从代码工匠进化为价值顾问，也要求客户从被动验收者进化为积极参与的共创者。当双方基于信任与共同目标，以订阅制为纽带结成长期伙伴时，软件才能真正摆脱「项目」的桎梏，进化为持续驱动业务增长的<strong>活的生命体</strong>。</p><p>选择订阅制，就是选择了一条更稳健、更灵活、更具长期价值的数字化征程。</p>]]></description></item><item>    <title><![CDATA[[大模型实战 04] 从玩具到生产：基于 ChromaDB 打造工程级 RAG 系统 阿尔的代码屋 ]]></title>    <link>https://segmentfault.com/a/1190000047597060</link>    <guid>https://segmentfault.com/a/1190000047597060</guid>    <pubDate>2026-02-06 16:04:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>[大模型实战 04] 从玩具到生产：基于 ChromaDB 打造工程级 RAG 系统</h2><blockquote><p><strong>核心摘要 (TL;DR)</strong></p><ul><li><strong>痛点</strong>：大模型不知道最新的新闻，也不知道企业的私有文档（如员工手册）。</li><li><strong>方案</strong>：<strong>RAG (检索增强生成)</strong>。就像“开卷考试”，先去翻书找答案，再回答问题。</li><li><strong>工具链</strong>：<strong>LlamaIndex</strong> (框架) + <strong>BGE</strong> (嵌入模型) + <strong>ChromaDB</strong> (向量数据库) + <strong>Qwen2.5</strong> (推理模型)。</li><li><strong>实战</strong>：在 Kaggle 上从零搭建一个能回答“企业内部机密”的 AI 助手。</li></ul></blockquote><h3>前言</h3><p>各位友人们，大家好，这里是<strong>阿尔</strong>。在上一节中，我们大概知道了大模型的构成，safetensor格式的大模型的文件组成，transformers库的基本使用。我们已经能够使用大模型去做一些简单对话应用了，它可以是上知天文，下知地理，中间还能知道人情冷暖。但是，我们需要加一个限定词，在<strong>训练数据截止日期前</strong>的。因为训练一次需要耗费很多的计算资源，时间和人力，当我们想让它知道一些新知识的时候，比如让它知道现在美国的总统是拜登还是特朗普，我们可以在对话中告诉他，这没问题，但是如果我们想让它知道更多，比如我的<strong>私人日记</strong>?比如我<strong>刚写的那篇博客</strong>？比如公司的<strong>员工手册</strong>, 比如自己产品的<strong>使用说明书</strong>？</p><p>这类<strong>私有数据</strong>，是大模型企业应用的痛点，毕竟大模型是基于在互联网上公开数据训练的。重新把这部分资料加进去，再训练一下模型？也不是不行，但是有点没有性价比，这时候就引出了大模型落地应用的核心技术-&gt; <strong>RAG (Retrieval-Augmented Generation，检索增强生成)</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597063" alt="本地大模型的痛点，以及RAG如何解决的示意图" title="本地大模型的痛点，以及RAG如何解决的示意图"/></p><h3>1. RAG（检索增强生成）</h3><h4>1.1 什么是RAG?</h4><p>考试的时候，如果考到不会的知识，不知道各位友人们会不会头疼，如果这时候，允许我们翻书，现去书里找，我们也很有可能找得到对应的答案，哪怕我们可能完全没学过。这就是<strong>RAG</strong>的大致思路：<strong>不让模型凭空回忆，而是先给它找资料。</strong></p><p>RAG，检索增强生成,字面上讲，就是 拿到考题-&gt;然后去翻书，通过目录之类的索引，快速翻到（<strong>检索</strong>）相关的内容-&gt;再根据这些内容（<strong>增强</strong>了的内容），回答出问题（<strong>生成</strong>回答）。</p><p>对比简单地把东西一股脑全部跟大模型说一遍，我们能清楚得发现，我们只用了检索到的那一部分内容，并没有让整本书大模型的脑子将占用, 这就是RAG的效率体现。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597064" alt="用开卷考试比喻RAG和离线模型的示意图" title="用开卷考试比喻RAG和离线模型的示意图" loading="lazy"/></p><h4>1.2 RAG的步骤</h4><p>RAG技术的思路很简单，但是实现并非只是一个单一的技术能实现的，它有一套<strong>流水线（流水线）</strong>。 把这头"大象"放进冰箱，总共需要两步：<strong>准备好数据</strong>和<strong>让模型拿到数据</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597065" alt="用把“大象”装进冰箱的比喻来描述RAG的两个过程的示意图" title="用把“大象”装进冰箱的比喻来描述RAG的两个过程的示意图" loading="lazy"/></p><h5>第一个阶段：数据准备(Indexing) -&gt; 把书装进书包</h5><p>在大模型能够<strong>翻书</strong>之前，咱们得先把我们想给它看的<strong>书</strong>整理好，放进<strong>书包</strong>里。</p><ol><li><strong>加载 (Load)</strong>：咱们的资料可能是各种各样的格式，一般大模型是不认识这么些格式的，所以我们就需要把 PDF、Word、网页等各种格式的文件读进来，统一提取出纯文本。</li><li><strong>切分 (Chunking)</strong>：大模型一次吃不下整本书,就和我们一眼看不完整本《三国演义》一样，它有<strong>上下文长度限制</strong>。我们需要把长文本切成一个个小的<strong>片段 (Chunks)</strong>，比如每 500 个字切一段。</li><li><p><strong>向量化 (Embedding)</strong>：<strong>这是最关键的一步！</strong></p><ul><li>计算机无法直接比较“苹果”和“iphone”是不是相关的。</li><li>我们需要用一个专门的模型（Embedding Model），把每一段文字变成一串<strong>数字向量</strong>（比如 <code>[0.1, -0.5, 0.8, ...]</code>）,是不是有点耳熟，对这和大模型训练的Embedding是一个思路，但是我们一般会使用<strong>特制</strong>的嵌入模型来做这个<strong>专业</strong>的事情。</li><li>在这个数学空间里，语义相近的词，距离就越近, 这样我们就能知道，这本书中的所有向量，哪些是和我们的问题相关的了。</li></ul></li><li><strong>存储 (Storage)</strong>：把这些向量和对应的文字，存入<strong>向量数据库 (Vector DB)</strong> 中。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597066" alt="RAG的数据准备流程，包括“加载”，“切分”，“向量化”，“存储”等步骤的示意图" title="RAG的数据准备流程，包括“加载”，“切分”，“向量化”，“存储”等步骤的示意图" loading="lazy"/></li></ol><h5>第二个阶段：应用数据给大模型生成（Retrieval &amp; Generation）-&gt; 开卷答题</h5><p>拿到书了之后，我们想要<strong>翻书</strong>，就得找到和问题<strong>有关系</strong>的内容，然后再将这些内容和我们自己的常识<strong>结合</strong>起来，对提出的问题进行答题。</p><ol><li><strong>问题向量化(Embedding)</strong>：要想知道用户的提问（例如“火星基地吃什么？”）和内容的相关性，我们就需要像对准备的数据一样，用同一个 Embedding 模型将问题变成向量。</li><li><p><strong>检索 (Retrieval)</strong>：拿着这个“问题向量”，去向量数据库里搜, 去找到关系性高的内容。</p><ul><li>系统会计算：“哪个文档片段的向量，和问题向量的距离最近？”</li><li>找出最相似的前 3-5 个片段 (Top-k)。</li></ul></li><li><p><strong>增强 (Augmentation)</strong>：把这 3-5 个片段拼在一起，和用户的问题组合成一个超级长的 Prompt。</p><ul><li><p><em>Prompt 模板示例：</em></p><blockquote>你是一个助手。请根据以下参考资料回答问题。<br/>参考资料：[片段1]... [片段2]...<br/>用户问题：火星基地吃什么？</blockquote></li></ul></li><li><strong>生成 (Generation)</strong>：把这个 Prompt 喂给大模型（LLM）。大模型阅读资料，总结并生成最终答案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597067" alt="RAG的检索生成阶段，包括向量化，检索，增强和生成" title="RAG的检索生成阶段，包括向量化，检索，增强和生成" loading="lazy"/></li></ol><h3>2. RAG技术选型</h3><p>好了，理论我们已经懂了，现在我们撸起袖子，准备来实操一下子吧。我们打算<strong>从零开始</strong>快速搭建一个<strong>工程级</strong>的RAG系统: <strong>私有API助手</strong>, 在我直接告诉各位友人们我们要用到的工具前，我觉得也有必要大概让各位友人们知道还有哪些别的选择，我们为什么选择了这几个。</p><h3>2.1 框架: LlamaIndex vs. LangChain</h3><ul><li><strong>LangChain</strong>：万能胶水，适合做复杂的 Agent（智能体），但写 RAG 代码比较啰嗦，抽象层级太碎，我们后面写智能体的时候（如果有精力做智能体的教程的话）再来使用它。</li><li><strong>LlamaIndex</strong>：<strong>数据专家</strong>。专门为 RAG 也就是“索引和检索”而生。接口极度简洁，且对数据清洗（Ingestion）的处理更专业。</li><li><strong>结论</strong>：我们做RAG，直接先上<strong>LlamaIndex</strong>, 快速地实现效果。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597068" alt="LlamaIndex vs. LangChain的示意图" title="LlamaIndex vs. LangChain的示意图" loading="lazy"/></li></ul><h4>2.2 嵌入模型 (Embedding)：BGE vs. OpenAI</h4><ul><li><strong>OpenAI (text-embedding-3)</strong>：效果好，但要钱，且数据要传给 OpenAI（隐私风险）。</li><li><strong>BAAI/bge-small-zh-v1.5</strong>：<strong>国货之光</strong>。中文效果霸榜，体积极小（几百 MB），完全可以在 Kaggle 本地跑。</li><li><strong>结论</strong>：为了免费和隐私，首选 <strong>BGE-Small</strong>。</li><li><strong>PS</strong>: 如果是英文资料的话，建议换成 <code>BAAI/bge-small-en-v1.5</code> 或者 OpenAI 的 <code>text-embedding-3-small</code><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597069" alt="BGE vs. OpenAI示意图" title="BGE vs. OpenAI示意图" loading="lazy"/></li></ul><h4>2.3 向量数据库：Chroma vs. Milvus vs. Pinecone</h4><ul><li><strong>Pinecone</strong>：纯云端 SaaS，不可本地部署，对 Kaggle 不友好。</li><li><strong>Milvus</strong>：性能强悍，适合十亿级数据，需要 Docker 部署，适合数据量大的时候使用，但是对于咱们的这个项目来说，太重了。</li><li><strong>ChromaDB</strong>：<strong>轻量级王者</strong>。可以像 SQLite 一样以“本地文件”形式存在，也可以部署成服务器。</li><li><strong>结论</strong>：中小型项目，首选 <strong>ChromaDB</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597070" alt="向量数据库：Chroma vs. Milvus vs. Pinecone对比示意图" title="向量数据库：Chroma vs. Milvus vs. Pinecone对比示意图" loading="lazy"/></li></ul><h3>3. 上手实操</h3><p><strong>项目背景</strong>：假设我们是一家名叫 "DeepStar" 的初创公司，我们有一套内部绝密的 API 文档，新来的实习生总是问重复的问题。我们要用 RAG 让他自己查。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597071" alt="项目背景的示意图" title="项目背景的示意图" loading="lazy"/></p><h4>3.1 环境配置 (Kaggle)</h4><p>启动 Kaggle Notebook，确保 <strong>Internet: On</strong>，<strong>Accelerator: GPU T4 x2</strong>。</p><pre><code class="bash"># 1. 更新transformers及其相关库
!pip install -U transformers peft accelerate bitsandbytes sentence-transformers

# 2. 安装 LlamaIndex 核心及相关插件
!pip install llama-index-core llama-index-llms-huggingface llama-index-embeddings-huggingface

# 3. 安装 ChromaDB 向量库支持
!pip install llama-index-vector-stores-chroma chromadb</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597072" alt="环境部署步骤的示意图" title="环境部署步骤的示意图" loading="lazy"/><br/><strong>下载依赖库可能会需要一点时间</strong>，之后我看看能不能在kaggle上用uv去做包管理。</p><h4>3.2 造数据：模拟企业内部文档</h4><p>我们创建两份文档：一份是核心接口定义，一份是错误码说明。</p><pre><code class="python">import os

data_path = "/kaggle/working/data"
# 创建数据目录
os.makedirs(data_path, exist_ok=True)

# 文档 1: 核心 API 定义
api_doc = """
[机密] DeepStar 核心交易接口 v2.0
1. 创建订单 API: POST /api/v2/order/create
   - 必填参数: 'user_id' (String), 'amount' (Decimal), 'token' (X-Auth-Token)
   - 特殊逻辑: 如果 amount &gt; 10000, 必须额外传递 'audit_code' (审计码)。
   - 频率限制: 单用户每秒最多 5 次调用。
2. 查询余额 API: GET /api/v2/balance
   - 缓存策略: 默认缓存 5 秒。传递 'no-cache=true' 可强制刷新。
"""

# 文档 2: 错误码字典
error_doc = """
[机密] DeepStar 全局错误码字典
- E1001: 签名验证失败。请检查 X-Auth-Token 是否过期。
- E2009: 余额不足。注意：冻结金额不计入可用余额。
- E5003: 审计风控拦截。大额交易未通过自动审计，请联系人工客服。
"""

with open(f"{/data_path}/api_specs.txt", "w") as f:
    f.write(api_doc)
with open(f"/{data_path}/error_codes.txt", "w") as f:
    f.write(error_doc)

print("[Success] 企业文档库已就绪！")</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597073" alt="3.2模拟企业内部文档示意图" title="3.2模拟企业内部文档示意图" loading="lazy"/></p><h4>3.3 初始化大脑与眼睛 (Settings)</h4><p>提前根据自己的情况来配置待会儿用的<strong>词嵌入模型</strong>和<strong>推理模型</strong>。</p><pre><code class="python">embedding_model ="BAAI/bge-small-zh-v1.5"
llm = "Qwen/Qwen2.5-7B-Instruct"
# 在本地服务器，可以用modelscope下载下来, 把路径配置在这儿</code></pre><p>利用 <code>Settings</code> 全局配置，将默认的 OpenAI 替换为本地模型。</p><pre><code class="python">import torch
from llama_index.core import Settings
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# 1. 设置 Embedding (眼睛)
# 使用 BGE-Small，显存占用极低，检索中文效果极佳
print("正在加载 Embedding 模型...")

Settings.embed_model = HuggingFaceEmbedding(
    model_name=embedding_model
)

# 2. 设置 LLM (大脑)
# 使用 Qwen2.5-7B-Instruct
print("正在加载 LLM 模型...")
Settings.llm = HuggingFaceLLM(
    model_name=llm,
    tokenizer_name=llm,
    context_window=30000,
    max_new_tokens=512,
    generate_kwargs={"temperature": 0.1, "do_sample": True}, # 技术文档要求严谨，温度调低
    device_map="auto",
    model_kwargs={"dtype": torch.float16, "trust_remote_code": True}
)
print("[Success] 模型加载完毕！")</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597074" alt="初始化词嵌入模型和推理模型的示意图" title="初始化词嵌入模型和推理模型的示意图" loading="lazy"/></p><h4>3.4 核心组件：ChromaDB 持久化流水线</h4><p>这是本篇最关键的代码。我们要实现：<strong>如果本地已经有数据库，就直接读；如果没有，才去解析文档。</strong></p><pre><code class="python">import chromadb
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore

# 定义持久化路径
CHROMA_DB_PATH = "/kaggle/working/chroma_db"
COLLECTION_NAME = "deepstar_docs"

# 1. 初始化 Chroma 客户端 (PersistentClient 实现了写硬盘功能)
db_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

# 2. 创建或获取集合 (Collection)
chroma_collection = db_client.get_or_create_collection(COLLECTION_NAME)

# 3. 将 Chroma 对接给 LlamaIndex
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# 4. 智能加载逻辑 (幂等性设计)
if chroma_collection.count() == 0:
    print("[Info] 数据库为空，开始初始化...")
    # 读取 data 目录下的所有文件
    documents = SimpleDirectoryReader("./data").load_data()
    # 建立索引并自动存入 Chroma (Ingestion)
    index = VectorStoreIndex.from_documents(
        documents, storage_context=storage_context
    )
    print("[Success] 数据写入完成！")
else:
    print(f"[Info] 发现 {chroma_collection.count()} 条存量数据，直接加载...")
    # 直接从 Vector Store 加载，无需重新计算 Embedding
    index = VectorStoreIndex.from_vector_store(
        vector_store, storage_context=storage_context
    )
    print("[Success] 索引加载完成！")</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597075" alt="ChromaDB流水线示意图" title="ChromaDB流水线示意图" loading="lazy"/></p><h4>3.5 验收测试：复杂逻辑问答</h4><p>现在，我们模拟实习生提问。注意，这个问题需要结合两个文档（接口定义 + 错误码）以及逻辑推理才能回答。</p><pre><code class="python"># 创建查询引擎
query_engine = index.as_query_engine(similarity_top_k=3)

# 实习生的提问
questions = [
    "创建订单时，如果你只有 100 块钱，能传 amount=20000 吗？为什么？",
    "我收到了 E5003 错误，这是什么意思？该怎么办？"
]

print("======== 开始 RAG 问答测试 ========")

for q in questions:
    print(f"\n[Question] {q}")
    response = query_engine.query(q)
    print(f"[Answer]\n{str(response)}")

    # 打印引用源 (Debug 必备，看看它参考了哪个文件)
    source_file = response.source_nodes[0].metadata.get('file_name')
    print(f"[Source]: {source_file}")</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597076" alt="验收测试部分示意图" title="验收测试部分示意图" loading="lazy"/></p><p><strong>答复如下</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047597077" alt="RAG的回答示意图" title="RAG的回答示意图" loading="lazy"/></p><hr/><h3>4. 进阶技巧：如何管理你的数据库？</h3><p>既然用了 ChromaDB，我们就可以像查 SQL 一样查它。这在 Debug 时非常有用。</p><pre><code class="python"># 偷看数据库里的前 2 条记录
data = chroma_collection.peek(limit=2)

print("\n[Debug] 数据库抽查:")
for i, doc in enumerate(data['documents']):
    print(f"--- 片段 {i} ---")
    print(f"内容: {doc[:50]}...") # 只打印前50个字
    print(f"来源: {data['metadatas'][i]}")</code></pre><h3>5. 完整代码</h3><p>完整代码可以点击<a href="https://link.segmentfault.com/?enc=%2B01G0BpAXHW7nRvecHt70Q%3D%3D.UH9JUoNQYSKTUEd9hpzIJpr%2BFaM7a2ADTmopNPMligeupSyNj2UM4YL4ox2OMq87d7p7qIP8EjQsh7%2Fyk7tBGVyqAR3uePlT2BW%2FjG1wC7U%3D" rel="nofollow" target="_blank">kaggle笔记</a>获取</p><h3>5. 常见问题 (Q&amp;A)</h3><p><strong>Q: 为什么不直接把所有文档都塞进 Prompt 里 (Long Context)？</strong><br/><strong>A:</strong> 虽然现在很多模型支持长文本（比如 128k），但直接塞文档有三个问题：</p><ol><li><strong>太贵</strong>：Token 是要钱的（如果用商业 API）。</li><li><strong>太慢</strong>：上下文越长，推理越慢。</li><li><strong>记不住</strong>：大模型有“长上下文迷失 (Lost in the Middle)”现象，塞太多反而会忽略中间的关键细节。RAG 相当于先做了一次筛选，只给模型看最有用的，效果反而更好。</li></ol><p><strong>Q: LlamaIndex 和 LangChain 我该学哪个？</strong><br/><strong>A:</strong></p><ul><li>做 <strong>RAG/知识库</strong>：首选 <strong>LlamaIndex</strong>，它对数据索引、切分、向量化做了极其深度的优化，接口更简洁。</li><li>做 <strong>Agent/工具调用</strong>：首选 <strong>LangChain</strong>，它的生态和工具链更丰富。</li><li><strong>结论</strong>：咱们这个项目专注于“找资料”，所以 LlamaIndex 是最佳选择。</li></ul><p><strong>Q: ChromaDB 的数据存在哪里了？</strong><br/><strong>A:</strong> 在上面代码中，我们通过 <code>PersistentClient</code> 指定了路径 <code>/kaggle/working/chroma_db</code>。<br/>它就像 SQLite 一样，数据就存在这个文件夹里的 <code>.sqlite3</code> 和 <code>.bin</code> 文件中。咱们可以把这个文件夹拷贝到任何电脑上，无需重新向量化就能直接使用。</p><hr/><p><strong>本文作者：</strong> Algieba<br/><strong>本文链接：</strong> <a href="https://link.segmentfault.com/?enc=KVO%2FRlNZA%2BYwIf0xsp1TiA%3D%3D.5Gj5KTmqsLhcTAHAcvfpsmCXGT19DLE60seivvlg%2FN35A8CLzTKkkredmDi9mgRGd7GyceZ8iKzidaX99aPXSQ%3D%3D" rel="nofollow" target="_blank">https://blog.algieba12.cn/llm04-rag-llamaindex-chromadb/</a><br/><strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</p>]]></description></item><item>    <title><![CDATA[Skills 出世，Prompt 已死？2026 年，如何为 Agent 构建可控思维？ 老纪的技术]]></title>    <link>https://segmentfault.com/a/1190000047597097</link>    <guid>https://segmentfault.com/a/1190000047597097</guid>    <pubDate>2026-02-06 16:03:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2><strong>别卷 Prompt 了！它只是你 AI 员工的“开机键”</strong></h2><p>进入 2026 年，Skills 的爆火和 Clawdbot（OpenClaw）的横空出世，传递了一个清晰的信号：当 Agent 从酷炫的演示走向支撑业务的生产系统时，单纯依靠优化提示词（Prompt）的“艺术”，已<strong>无法满足企业对可靠性、执行力与持续进化能力的刚性需求。</strong></p><p>这并不是说 Prompt 不再重要，而是它的角色发生了根本性转变。它从一个需要被无限雕琢、承载所有逻辑的“总指挥”，演变为一个触发器。它的新任务是：<strong>准确理解人类指令，然后高效地唤醒后方一套庞大且专业的能力系统</strong>。就像手机的开机键，按一下就可以打开各种应用功能的入口。</p><p>这个能力系统，正是现代 AI 工程的核心——一个为 Agent 打造的“可控思维”架构。</p><p>它由三个相互协作的引擎构成：</p><ul><li><strong>记忆引擎（Memory）</strong>：确保 Agent 有“记性”，能够记住用户偏好和交互历史。这意味着它能记住重要的对话历史和你的要求，做事有头有尾，不用你每次都从头交代。</li><li><strong>知识引擎（RAG）</strong>：确保 Agent 有“实时的知识库”，能够从海量、动态的企业数据中精准检索信息，保证它给出的信息永远准确、最新，不会凭空乱造。</li><li><strong>技能引擎（Skills）</strong>：确保 Agent 有“手脚”，能够将复杂的业务操作（如数据查询、报告生成、系统调用）封装为可被随时调用的标准化模块，从“能说”走向“会做”。</li></ul><p>Prompt、Memory、RAG、Skills 共同构成了一个能独立干活、不出错、有记性的 AI 员工，当它要完成的任务越复杂、越关键，后三者的系统化工程价值就越发凸显，Prompt 也因此必须从舞台中央退下。作为使用者，<strong>我们不再只是和模型对话的“提问者”，而是为 Agent 设计和组装能力模块的“架构师”，思考重点也从“怎么问得好”，全面转向“怎么让 AI 干得好”。</strong></p><p>理解这种从孤立提示到系统工程的范式迁移，是我们今天话题的起点。</p><p>下面，就让我们聆听来自 1 月 31 日 OceanBase 社区嘉年华的圆桌讨论，看顶尖的实践者们如何具体拆解这些核心组件的演进与融合。</p><h2><strong>从 Prompt 到 Skills，RAG 还行不行</strong></h2><p><strong>主持人</strong>：张海立，LangChain Ambassador、OceanBase Ambassador，up主“沧海九粟”</p><p><strong>对话嘉宾：</strong></p><ul><li>张颖峰，RAGFlow CEO</li><li>余金隆，FastGPT 负责人</li><li>古思为，Co-founder of Nowledge Labs</li><li>吉剑南，OceanBase AI 平台与应用负责人</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597099" alt="" title=""/></p><h2><strong>议题一：2026 年 RAG 生态何去何从？</strong></h2><p><strong>张海立</strong>：从去年末到今年年初，AI 领域热点频发。除了近期备受关注的 Clawdbot（OpenClaw），Skills 成为另一个重要话题。我在进行 Skills 相关实践时发现，许多 Skills 与本地文件系统紧密相关，但都离不开 RAG 体系对外部数据的召回，这对 Agent 发挥更大作用至关重要。LangChain 在构建 Agent 生态时，RAG 也是核心体验之一。想请教各位老师：在当前大环境下，您认为 2026 年 RAG 生态将如何发展？请结合各自产品进行简要介绍。</p><p><strong>张颖峰</strong>：先说个笑话，2025 年被称为 Agent 元年，当时有朋友问我们要不要（从 RAGFlow）改名为 AgentFlow。而今年是 Agent 落地元年，我们内部也讨论要不要改名为 ContextFlow。实际上我们永远不会改名，因为我们认为“R”是核心点，<strong>单纯的 RAG 确实不足以服务 Agent，但“R”是服务 Agent 数据层的核心点。</strong></p><p><strong>当前 Agent 需要的是上下文（Context），它来自三方面数据：企业内部数据、工具数据以及对话过程中生成的数据。Skills 偏向工具层面，但比工具更高一层，还包含了规划（Plan）能力。Skills 本身也需要搜索——当企业内部有 1000 个 MCP 时，如何调用对应的 Tools 和 Skills 同样需要检索能力。因此 RAG 永远不会消失。</strong></p><p>我们的布局是从 RAG 引擎向上层引擎演进。技术本身未变，但内涵发生变化：数据从简单的企业内部数据，扩展到 Agent 过程中的上下文数据。<strong>我们判断，未来所有 Agent 都是 Coding Agent，包括对工具的调用也将变成代码生成（Code Generation）</strong>，需要 RTC（Run-Time Code）在沙箱中执行，访问各类 Tools 和 Skills，最终通过文件系统返回结果。这也是我们向上下文引擎方向演进的核心计划。</p><p><strong>余金隆</strong>：我赞同颖峰老师关于 Code Generation 解决所有问题的观点，这也是我们团队的认知。无论是做 RAG 引擎还是 Workflow 引擎，都在向代码生成靠拢。</p><p>RAGFlow 不想改名，我们有点想改名字。因为近几年我们发现，<strong>做 Agent 本质是把数据使用起来</strong>，所以我们的平台主要解决数据连接层问题。过去数据分布在数据库、文档等各种结构中，现在通过大量连接器实现不同数据的连接。Skills 出现后，以前需要写代码和 Webhook 连接的数据层，现在可以通过 Skills 实现。这对国内交付场景特别有价值——国内系统数据格式不统一、缺乏标准，交付同学以前需要写大量适配代码，现在通过 Skills 将数据标准化连接到平台。</p><p>今年我们主要做两件事：一是完善连接层，二是优化 RAG 的 Retrieval 层。Retrieval 效果很大程度上取决于召回过程，不同场景的召回流程差异很大。过去需要通过 Workflow 形式搭建积木、进行意图识别分类、编写不同提示词适配不同场景，链路复杂。现在我们探索通过 Skills 这种偏语义化的方式生成代码，类似 Test-to-Code 的思路，但生成的是 SDK 代码来构建整个 Retrieval 流程，这是一个很有意思的探索方向。</p><p><strong>古思为</strong>：关于 2026 年 RAG 相关变化，可以看到<strong>在 Coding Agent 中对代码的检索已从纯 Embedding 转向 AST（抽象语法树）、Agentic FS Graph 或 AST Graph 等方案</strong>。包括 PageIndex 项目，以及我们公司在 Haicon 2024 发布的实验性项目 OpenKL，尝试用类文件系统方法处理 Memory 和 RAG Docs。</p><p><strong>另一个趋势是 RAGFlow 等通用内容引擎同时处理文档和 Memory</strong>。我们已发布的第一个产品是面向 C 端的 Memory 桌面 APP Knowledge MAM，动机是帮助用户在不同工具间无缝切换工作流。例如在 ChatGPT 完成 Deep Research 后，无需重新解释即可继续在 Cursor 中工作；或者当 Agent 帮助发帖子进入热榜后，可以切换到另一个 Agent 继续任务，同时保留所有交互历史和偏好设置。</p><p><strong>吉剑南</strong>：OceanBase 面向 AI 的能力——seekdb、PowerRAG 与 PowerMem 均已开源。我们团队除了做向量数据库和 AI 应用基础设施外，也在探索面向数据库的 AI 应用，比如面向开发者工具的 Text-to-SQL 和数据库智能运维。</p><p>关于 2026 年趋势，我认可颖峰老师说的 <strong>RAG 不会消失，它和Skills、MCP处于不同维度</strong>。即使未来 Skills 和 MCP 越来越多，最终仍需通过 RAG 或某种方式召回，不能将所有 Skills 都喂给模型。</p><p>但我有不同观点：当前 RAG 仍集中在知识库领域，通过搭建 Chatbot 做问答，而问答更像玩具而非生产应用。<strong>真正的生产应用应将 RAG 融入日常工作</strong>，如销售根据集团材料为客户生成定制化 PPT或“一指禅”。<strong>未来 RAG 会结合应用反馈，反向影响数据如何切分、如何做更精细化的 Embedding，而非仅仅前置处理。</strong></p><hr/><h2>议题二：AI系统中的多路检索与数据源管理</h2><p><strong>张海立</strong>：感谢各位的分享，Skills 给我们带来了更多机会，能创建更多 Agent 和 RAG 应用。同时有一个概念非常重要：我们常说的 RAG 里的“R”，到底指什么？它指的是 Retrieval，是一个 “检索过程”。Retrieval 的 source可以是文件系统，可以是数据库，可以是 Web，甚至多种来源并存。</p><p>引申出第二个问题：<strong>随着 Skills 和 RAG 体系的发展，未来多路检索会越来越常见，RAG 不会消失，它将长期存在于 Agent 体系中。这样一来，数据源头的管理就变得更加重要</strong>。最简单的是把数据直接塞进软件系统，但更常见的情况可能是：越来越多的数据会落在数据库中。在这种情况下，<strong>当数据库的多路检索能力得到极大增强之后，做 RAG 应更多依赖数据库，还是在数据入库层面通过一些技巧将复杂的事情交给基础设施？</strong></p><hr/><p><strong>吉剑南</strong>：必然入库是最大影响，这也是 OceanBase 提出混合搜索（Hybrid Search）概念的核心。如果完全以非结构化数据或切片方式进入系统，召回效率顶天就是向量化的近似能力。去年所有 RAG 产品都在强调从非结构化数据中提取结构化数据，存为 JSON 等半结构化形式，用于前置过滤或与结构化数据一起做混合搜索。</p><p>为什么要这样做？本质上是语义理解包含两个层面：一是你问的是模糊问题，但脑子里想的是确定性答案；二是问题模糊，答案也模糊，希望召回所有相关点。大部分实践场景属于第一种。</p><p>在文档预处理时，结构化提取非常重要。例如从医疗文档或简历中提取结构化字段，召回时先对结构化数据做精确匹配，再对字段内的非结构化内容做向量检索。半结构化数据解决范围和准确性问题，向量检索解决语义理解问题。通过混合搜索模式，入库时做文档理解提取结构化数据，召回时统一检索，效率会大幅提升。数据库也应在接下来一年面向这个方向发展，我们看到 Chroma 等国外开源数据库已在往这个方向演进。</p><p><strong>古思为</strong>：我们比较早做 Graph RAG，可能是第一个探索的团队。张老师分享的新架构与我们上一家公司做的 FusionGraph 很像。核心思想是：要让复杂 RAG 系统表现好，索引结构既要贴近知识本质，又要把特定场景的领域知识元信息投射到 Retrieve、Index、Transform 各环节做优化。</p><p>通用方法是知识后加工时做 Entity Graph 或 Semantic Graph，同时在做 IDP（Intelligent Document Processing）和 Parsing 时，对多层 folder 和复杂章节的长文档要识别 layout，涉及多模态时考虑是否转换模态。要做好这些并能演进，<strong>不要过度领域化 pipeline，而是按基本原理拆分，确保各组件能力跟上。</strong></p><p><strong>Database 是重要基础设施</strong>，比如 RAGFlow 的 Graph 和 Tree 结构能否原生保留、高效检索；要做 Dynamic Agents Retrieve，模型能否自然利用复杂多层结构。数据库的高性能、索引召回率和内置 Hybrid RRF 都很重要，<strong>决定系统下限</strong>。</p><hr/><p><strong>余金隆</strong>：在交付过程中，数据源解析是基础且重要，但更重要的是召回（Retrieval）层。即使使用最简单的原始向量，只要检索词和检索语句构建得好，也能得到很好效果，只是效率较差。我们在此基础上扩展了语义化加标量方式。</p><p>但标量遇到较大问题：它不固定，用户自己也不知道需要什么标量。我们今年研究的方向是标量的动态扩展，包括用户自身扩展和模型自生成。例如给模型一些 Skills，或用户编写场景来生成场景下的标量存入数据库。当然这会引发多租户系统中成千上万标量的高效索引问题，以及渐进式生成问题——很难在预处理时生成所有标量，很多需要在检索时评估并渐进补全。在Retrieval阶段，多标量关联查询的生成方式也借鉴了 Text-to-SQL 的思路。我们希望找到通用存储方式覆盖 80% 场景，目前看语义化加标量检索加动态标量可以覆盖很多场景，所以我们没有用图，因为图是以复杂方式解决复杂问题，而 AI 时代可能有更简单的方式处理复杂问题。</p><p><strong>张颖峰</strong>：我们现在是数据库使用者，但曾经也是数据库开发者。从纯技术角度，我非常喜欢“一边推理一边搜索”的技术方向，我称之为 Attention Engine，我认为它也是一种 RAG。DeepSeek 近期已大体实现类似方式，因显存限制不得不用内存，在推理时通过内存索引搜索内容，从外置记忆变为内置记忆。但从商业角度这条路行不通，要求检索与模型延迟极低，必须在同一交换机后，意味着只能卖一体机。因此我们仅作为调研方向。</p><p>从业务视角看，我们最早做 Infra 、做数据库时发现离业务太远，后来做 RAG 流量较大，促使我们重新思考 Data+AI 落地生态。我们的观点是：过去数据库是底座，上面写应用做增删改查；现在应用是 Agent，底座是以 RAG 为基础的组件，数据库在底层支撑 RAG 中间件。Data+AI 建设不能 AI 和 Data 各干各的，接口有时不清晰，因为中间层用 Python 实现，其好处是适应多变需求，召回策略可随时调整，<strong>不过 Python 带来的效率问题也让人头疼</strong>。AI 时代的数据底座让 Infra 人员直接触达业务，通道变短。因此中间层需要一个 Python 层适应业务多样化，一旦发现好的方式就迅速下沉到数据库解决效率问题。</p><p>我们在 2024 年底就鼓吹跨模态，但至今未落地，因为 Infra 到模型都未准备好。跨模态需要多向量搜索（Tensor Search），用多向量表示图片或文本，语义更准确、排序更准，但数据会膨胀两三个数量级，这是灾难。这需要模型、算法、Infra 共同解决挑战。因此我们需要端到端的、以 RAG 为中间层的体系，这其实就是 Agent 的数据库。</p><h2>议题三：Memory 与 RAG 到底有何区别？</h2><p><strong>张海立</strong>：我非常认同颖峰老师提到的“端到端”。作为 LangChain 社区大使，我们主要做应用层框架，今年非常想做的一件事情是：和各个厂商比如 OceanBase seekdb一起提供真正的端到端解决方案，服务企业和个人用户，帮助他们快速构建生产级 Agent。</p><p>简单总结一下几位老师的理解：当我们面向用户提供检索能力时，会在中间层、应用层、数据库层进行多层协同优化，共性问题会逐步下沉到数据库解决。以我的个人体验为例：在最初布道时，我会给大家讲很多 RAG 的流程和算法，但从<strong>去年底开始，我更多会建议“你直接用这个数据库就好了”，因为它已经帮我们解决了很多多路检索的问题。这种 “沉淀” 是应用方和数据库厂商不断联合实践的结果。</strong></p><p>下一个问题也与此有关：我们经常被问到Memory 和 RAG到底有什么区别？从 Memory 召回和从数据库召回有何区别？近期 Clawdbot（OpenClaw）从文件系统读取，到支持 PowerMem 直接接入进行更有效的内存管理。想请教剑南老师，这里做了什么特别工作？<strong>以及各位如何理解 Memory 与 RAG 的关系？</strong></p><p><strong>吉剑南</strong>：Memory 是为让大模型更像人而引入的。如果查询的都是客观事实且不存在人与人之间的理解，RAG 已能解决问题。但问题在于每个人对客观事实的理解和描述不同，加上人有记忆曲线，希望记住昨天强调的内容——这些内容虽非客观事实，但是主观认可。</p><p>例如每个人都有一个叫“老王”的朋友，随着时间推移这个“老王”可能已变化，但在记忆中一直叫“老王”，这时 RAG 搞不定，但 Memory 能搞定，因它会更新对“老王”的认知。“老王”是一个知识吗？并不是，因此，<strong>Memory 的核心是个性化和千人千面。</strong></p><p><strong>无论是 RAG 还是 Memory，整体是搭建一整套解决方案面向 Agent 为业务带来价值，不应区分该用 RAG 还是 Memory，而应思考如何组合好共同为业务赋能</strong>。</p><p><strong>古思为</strong>：我们目前做 Memory，之前做 Graph RAG。Memory 有广义和狭义之分，狭义指 Agent 或 LLM 需要检索的更外部的 Memory，它确实是特殊的 RAG，特殊在几个方面：</p><ul><li>原始数据是持续的 message thread。</li><li>知识需求是时序性的（temporal），包含两个时间维度：信息创建时间、事件/事实时间。</li><li>时序性存在一个问题，遗忘（forget）是 feature 而非 bug，需结合时间、访问频率和正反馈影响 Retrieval。</li><li>条目层面有 category 和不同类型，取决于 Memory 目的，可能需要schema 区分 ephemeral（瞬时）和 permanent（永久）。</li><li>不同结构间需要 transform 关系，可在 Retrieve 或写入过程触发 event，或周期性处理（类似大脑做梦处理记忆）。</li><li>多租户和 sessional scoping。</li></ul><p>如果做细会发现与典型 RAG 差别很大，但二者又有很大 overlap。RAG Engine 可以处理 Memory，Memory Engine Service 项目也会处理文档，界限会变得模糊。</p><p><strong>余金隆</strong>：我理解 Memory 算是广义 RAG 的一种，无非也是数据 I/O、Pipeline 处理、特殊数据结构，比较偏个性化。</p><p>从产品角度看，Memory 目前 C 端个性化场景用得较多。在任务流中，用户提 Memory 的还不多。在技术实践中，Mem0 有工具调用的 Memory 用于长 Agent 任务，但看其架构有点像 Context Engine，与 Memory 又不太一样。所以感觉 Memory 还是 RAG 的一种特殊 Pipeline 形式，没有太大区别，可能实时性比 RAG 更高。</p><p><strong>张颖峰</strong>：单从技术角度而言，Memory 与 RAG 确实没有本质区别，都是 Retrieval。但重要的是 Memory 如何发挥作用，这是在快速变化的。</p><p>我在分享 Context Engine 时提到三类数据：企业内部数据、Tools 数据、Agent 使用过程中生成的数据。但它们存储在两个地方：RAG 专有区域和 Memory 专有区域。可见所有大模型生成的内容都要存到 Memory，包括 Skills 的元数据（Skills 本身数据存文件系统）。</p><p>怎么存、什么时候存、什么时候取，这些设计点很难决策。例如生成 Plan 是否存入 Memory？作为 Plan Cache 有价值，但如果 Human-in-the-loop 干预修改了 Plan，应如何存储？以后如何根据 Memory 数据抽取内部 MCP Tools 的 Skills？这些都是新问题。</p><p><strong>从 Infra 角度，RAG 和 Memory 没区别；但从使用者角度，Memory 是重要的基础设施，解锁了大量场景</strong>。因此 Memory 项目很多（如 Mem0、MemU），但对 Memory 区域的定义（数据库该有哪些表）尚未完全一致，反映 Agent 到底需要什么样的 Memory 还在进化中。不过整个 Agent 体系需要哪些组件，已进入收敛期，就是 Context。</p><h2>议题四：Skills 开发实践与推荐</h2><p><strong>张海立</strong>：各位老师都在做 Workflow、数据库或融合方案，是否开发了自己的 Skills 帮助用户更好地使用产品？如有请推荐，如无请设想会开发什么样的 Skills 服务开发者？</p><p><strong>张颖峰</strong>：抱歉我目前没有特别好的推荐。我比较关注如何针对大量内部 MCP Tools 生成对应 Skills，这需要一个专门的 Agent 平台来实现。我的观点是：<strong>未来 Agent 平台可能没有统一标准</strong>，所有都是 Coding Agent，但特定 Agent（如低代码、无代码、Workflow）可能因良好交互而便于生成 Skills。</p><hr/><p><strong>余金隆</strong>：我们内部 Skills 用得很多，运营和 SEO、GM 等场景一大把。产研团队用得不算多，主要是代码开发和 Review。交付团队用得特别多：面向用户时遇到各种问题，排查系统后沉淀为 Skills，辅助交付和运维。因此，内部有句玩笑话“交付同学比研发同学更懂系统”，他们做了二十多个 Skills，涵盖工作流搭建、问题排查、RAG 优化等。<strong>总体感觉 Skills 更像自然语言工作流，虽更抽象，但目前大部分还是偏自然语言的 Workflow</strong>。对非开发人员在生产流程上比较友好。</p><p><strong>古思为</strong>：我们维护基于 Skills 的插件，在 Skills 发布第二天就推出了 Cloud Code 插件支持。早期没有 Skills 时，我们只能基于 MCP，让插件调用 MCP 的 Custom Command 触发操作，用 Hook 实现功能。</p><p>后来发现 MCP 规范了工具调用，但有两个地方不如 Skills：</p><p>1.MCP 有 Prompt 抽象，实现为斜杠命令可主动调用类似 Workflow 的东西，但并非所有 Client 都实现，我们要做很多额外工作。Skills 天然支持主动说和自动做。</p><p>2.Skills 的打包方式让不同工具间组合更灵活。我们内部将 Skills 从 MCP 换成 CLI 后变化很大。例如让 Agent 做 Memory 复杂更新查询时，MCP 需要多轮次，即使 interleave 也不够好。但 CLI 可以动态组合 Linux Shell Pipeline，在一个 turn 里精确完成复杂操作，且内部 CLI/Script 可以 self-contain，打包给用户后自然享受复杂能力。</p><p>调试经验方面，Skills 比较通用，容易用不同平台测试。我们发现一个有意思的案例：<strong>Skills 对应的工具有很多具体选择</strong>，如何调优模糊的问题？我们的做法是用最聪明的 Agent 做 honest 的复杂 long run 评估，像跟客户聊天一样告诉我们如何改进。有时需要更端到端看细节，不得不自己server model，在 template 解析过程中用小模型发现工具复杂类型定义的问题，虽然其他模型能克服，但会影响 performance。</p><p><strong>吉剑南</strong>：OceanBase 内部沉淀了很多 Skills。Skills 本质是最佳实践，告诉大模型最佳实践是什么，而最佳实践无非两类：一是提升工作效率的工程类（如 Cursor 的 rules），二是业务类 Skills。</p><p><strong>Skills 也可以用在 RAG 上</strong>，RAG 效率和准确性今天跟两个因素相关：相似度和 Top K。但大家有没有想过，召回前 Top K 和相似度有时不能完全指定，需要反复调，知识库又在更新。如果针对不同的业务实现写不同的 Skills，例如当需要某类数据时，希望相似度设到什么位置、Top K 设到什么位置，根据召回结果动态调整，这就变成了一个 Skills。这是 RAG 搞不定的，需要根据具体召回内容判断，是 RAG 的最佳实践。</p><p>之前大家可能想是否把 RAG 数据放 Skills 里就不用召回了，而<strong>我觉得 Skills 是对 RAG 的增强</strong>。关于 OceanBase 的 Skills，我们是有准备的，包括 seekdb 的研发人员今天也在现场，未来应该会有更多相关的 Skills 开放出来。</p><p><strong>张海立</strong>：非常感谢各位老师精彩分享。简单总结：<strong>RAG 还“行”！只要理解 RAG 的 R 是 Retrieval，有 Memory、传统数据库等多种数据来源</strong>，随着各位老师所在厂商的努力，多路检索能力、应用层提升、流程算法优化都在推进。相信 2026 年RAG会有更大发展。</p><h2><strong>Agent 可控思维的工程实现：从分散工具到一体化基座</strong></h2><p>本次圆桌讨论，为我们清晰地勾勒出 2026 年 AI 工程化的演进路径。专家们的共识指向一个明确的结论：构建可靠、可用的 Agent ，其核心不再是追求某个单一组件的极致，而在于如何<strong>系统性地整合记忆（Memory）、检索（RAG）与技能（Skills）</strong>，形成一个协同的“可控思维”体系。</p><p>综合专家观点，这一体系的发展呈现出三大趋势。</p><p>01 <strong>RAG 不会消失，反而会变得更加基础与核心</strong></p><p>它的内涵正在从狭义的文档问答，扩展为 Agent 对所有上下文数据的 Retrieval 能力——无论是企业内部文档、数据库中的业务数据，还是工具（Tools）与技能（Skills）的元数据，都需要被高效检索与调用。</p><p>未来的 RAG 将深度融入工作流（Workflow），根据应用反馈动态优化，并与混合搜索（Hybrid Search）等技术结合，实现更精准的“语义理解+精确过滤”。</p><p><strong>02 Memory 与 RAG 边界模糊，融合为数据层</strong></p><p>从技术基础设施（Infra）视角看，Memory 与 RAG 的本质都是数据的存储与召回。</p><p>二者的区别更多在于数据特性和使用场景：Memory 更侧重于个性化的、时序性的对话与状态记忆；RAG更侧重于客观的、相对静态的知识存储。但在服务 Agent 时，它们共同构成了支撑“上下文（Context）”的数据层。一个优秀的底层平台，应能一体化地管理这两种数据范式。</p><p><strong>03 工程复杂度下沉，呼唤一体化数据基座</strong></p><p>当应用层通过 Skills 和灵活编排满足业务多变需求时，通用的、性能瓶颈性的复杂度会自然下沉到底层基础设施。无论是多路检索、混合搜索，还是海量 Skills 元数据的管理，都对底层数据平台的能力提出了更高要求。</p><p>专家们指出，未来的理想路径是依赖一个强大的数据基座，它能原生支持向量检索、关系查询与结构化记忆，从而让开发者从繁琐的多系统集成工作中解放出来，更专注于 Agent 本身的业务逻辑。</p><p><strong>因此，构建“可控思维”的终极路径，在于选择或打造一个能够统一承载 Agent 记忆、知识与状态的数据基座</strong>。这样的基座，正如专家们在讨论中多次暗示的，能够将 Memory 的个性化记录、RAG 的海量知识检索、以及支撑 Skills 运行的业务数据，融于一个简洁、高效、一致的系统中。它让 Agent 的“思维”过程变得可管理、可观测、可优化。</p><p>最终，Prompt、RAG、Skills、Memory 这些活跃于应用层的概念，都将在这样稳固的基座之上，更好地各司其职、协同工作，共同将 Agent 从“聪明的对话者”转变为“可靠的业务执行者”。这标志着 AI 应用开发正式进入系统工程时代，<strong>而坚实的数据基础设施，是这一切得以实现的基石。</strong></p>]]></description></item><item>    <title><![CDATA[为什么是轮足？——从数据中心巡检机器人的演进看云智慧 Cloudwise X1的工程化选择 云智慧 ]]></title>    <link>https://segmentfault.com/a/1190000047597111</link>    <guid>https://segmentfault.com/a/1190000047597111</guid>    <pubDate>2026-02-06 16:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、数据中心巡检之“困”</h2><p>数据中心与智算中心作为数字基础设施的核心，其稳定运行依赖高频次、高精度的日常巡检。在以人力为主的运维模式下，巡检工作正面临系统性瓶颈。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597113" alt="图片" title="图片"/></p><p>当前的巡检模式，已逐渐无法满足现代数据中心日益提升的巡检需求。AI时代下，<strong>以智能巡检机器人为代表的自动化方案，正逐步成为行业的新选择。</strong></p><h2>二、从轨道到全地形：数据中心巡检机器人的演进之路</h2><p>数据中心包含动力、暖通、机房等多个系统，空间结构天然存在梯坎、门槛、斜坡等复杂地形，对巡检载体的通过性提出持续挑战。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597114" alt="图片" title="图片" loading="lazy"/></p><p>近年来，四足等全地形机器人在其他领域被广泛应用，但在数据中心狭窄通道、防静电地板等环境中面临实用性障碍，尚未有效落地。</p><p>真正适合数据中心的巡检载体，必须在通过性、效率与可靠性之间取得平衡——这为轮足机器人的出现提供了明确方向。</p><h2>三、轮足机器人：数据中心巡检中通过性、效率与可靠性的平衡之选</h2><p>在智能巡检载体的形态探索中，轮足式机器人，逐渐被视为数据中心场景的一种理性选择。它融合轮式底盘的高效、低噪与长续航优势，同时具备跨越台阶、斜坡等非平整地形的能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597115" alt="图片" title="图片" loading="lazy"/></p><p>稳定上楼梯</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597116" alt="图片" title="图片" loading="lazy"/></p><p>轻松过门槛</p><p>相比纯轮式机器人受限于地面条件，履带或四足方案又普遍存在噪音大、速度慢、维护复杂等问题。轮足构型在通过性、作业效率与长期运行可靠性之间实现了有效平衡，可在不改造建筑结构的前提下，适应多楼层、多房间的复杂布局，满足数据中心对稳定和连续作业的要求，真正推动巡检范围从单机房走向全站覆盖。</p><h2>四、云智慧 Cloudwise X1：专为数据中心打造的轮足巡检机器人</h2><p>云智慧Cloudwise X1 并非通用轮足平台的简单移植，而是云智慧针对数据中心多地形环境（楼梯、斜坡、窄道、门槛等）深度定制的轮足巡检机器人。</p><p>其轮足底盘具备20cm越障与30°爬坡能力，可自主上下电梯、穿越台阶与狭窄通道，轻松应对跨楼层复杂场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597117" alt="图片" title="图片" loading="lazy"/></p><p>在运营超过5年的混合架构机房中，云智慧Cloudwise X1 轮足巡检机器人无需改造地面或加装导轨，即可实现全站无死角覆盖，巡检范围从单一机房扩展至动力、暖通、IT机房及消防等多个区域。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597118" alt="图片" title="图片" loading="lazy"/></p><p>在移动能力之外，云智慧Cloudwise X1  轮足巡检机器人的作业体系全面面向数据中心需求构建：</p><p><strong>* 7×24小时自主作业</strong></p><p>依托边缘计算单元与激光雷达SLAM系统，云智慧Cloudwise X1  轮足巡检机器人能在高噪音、弱光环境中实时建图、动态避障，定位精度达毫米级，夜间巡检全自动执行，运维人员无需值夜班。</p><ul><li><strong>多模态AI感知融合</strong></li></ul><p>可见光、红外热成像、声纹与气体传感器数据，智慧Cloudwise X1  轮足巡检机器人 内置17项自研AI算法，支持110+巡检项。例如，在配电柜区域，可通过温差分析提前预警“虚接”隐患，早期故障发现率提升50%。</p><ul><li><strong>端云协同</strong></li></ul><p>所有巡检数据由端侧自主采集，自动附加时间戳与空间坐标，加密上传至一体化运维平台。面对审计时，可一键调取任意设备的历史完整证据链，告别纸质打勾表的主观争议。</p><p>基于全地形覆盖能力与多模态智能感知，云智慧Cloudwise X1 轮足巡检机器人的工程潜力，转化为一套面向数据中心、可落地且可验证的智能巡检方案。</p><h2>五、跨楼层全自动巡检，重塑数据中心运维范式</h2><p>轮足机器人的价值，不在于形态本身，在于它能否真正解决数据中心的巡检难题。云智慧Cloudwise X1 轮足巡检机器人的实践表明：只有深度理解数据中心场景，并将通过性与多模态感知能力有效结合，智能巡检才能逐步从概念走向实际应用。</p><p>云智慧Cloudwise X1 轮足巡检机器人已在大型数据中心完成部署验证，稳定支撑跨楼层、多系统的常态化巡检任务。</p><p>未来，云智慧持续致力于为数据中心提供可靠性保障服务，AI赋能提升产品创新力，为金融、政企及云服务商等行业提供更安全、高效、可落地的智能巡检解决方案。</p><p>同时，作为专注于AI 基础设施智能运维的服务商，云智慧助力客户构建智能电力、AI算力与服务、AI 智能体的全栈安全和可靠性保障体系。</p><p>致力于保障AI基础设施规模化、连续性、稳定运行，通过监测、预警、快速响应、自动化运维与合规治理，帮助客户实现更高可用性、更低风险与更优运营成本。</p><p>详询热线：400-666-1332</p>]]></description></item><item>    <title><![CDATA[唯一完测！涛思数据 TDengine IDMP 全项完成中国信通院基于 AI 大模型的时序数据管理平]]></title>    <link>https://segmentfault.com/a/1190000047597128</link>    <guid>https://segmentfault.com/a/1190000047597128</guid>    <pubDate>2026-02-06 16:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597130" alt="" title=""/></p><p>近日，在中国信通院组织开展的 2026 上半年批次“可信数据库”测试中，<strong>涛思数据 TDengine IDMP 成为截至目前唯一一家</strong>全项完成“基于 AI 大模型的时序数据管理平台”基础能力检验的产品。</p><p>经中国信通院测试验证，TDengine IDMP <strong>符合《基于 AI 大模型的时序数据管理平台技术要求》标准的全部能力要求</strong>，覆盖 AI 时序数据应用、时序数据建模与组织、情景化与标准化、实时分析、事件管理、安全与扩展性等关键能力方向。这也标志着 TDengine IDMP 在 <strong>AI 大模型与时序数据深度融合的工业数据平台领域</strong>，已达到国内技术先进水平。</p><h2>《基于 AI 大模型的时序数据管理平台技术要求》标准简介</h2><p>为规范基于AI大模型的时序数据管理平台技术和能力，指导提升AI大模型在时序数据领域的管理、建设应用，促进相关技术创新发展，完善行业协同生态，中国信通院依托CCSA TC601开展《基于AI大模型的时序数据管理平台技术要求》标准编制工作，围绕AI时序数据应用、时序模型管理、时序数据建模和组织、时序数据情景化、时序数据标准化、时序数据预处理、时序数据可视化、时序数据实时分析、事件管理、时序数据服务、平台管理、兼容性和扩展性、安全性等维度进行规范，为相关产品的应用落地提供了可供参考的技术规范。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047597131" alt="" title="" loading="lazy"/></p><h2>TDengine IDMP：AI 原生工业数据管理平台的四大核心优势</h2><p>作为时序数据库领域的长期实践者，涛思数据的 TDengine TSDB 已在工业、物联网等场景中广泛应用，覆盖智能制造、能源、电网、石油石化、汽车、矿山、新能源、制药、IT 基础设施等众多行业。</p><p>随着 AI 技术与工业互联网、物联网的深度融合，企业对数据平台的要求正在从“能存、能查”，升级为“能理解、能推理、能主动给出决策线索”。</p><p>在这一背景下，涛思数据于 2025 年 7 月正式发布 <strong>TDengine IDMP（AI 原生的工业数据管理平台）</strong>，与 TDengine TSDB 协同演进，从底层架构重构工业数据平台能力，打通数据采集、汇聚、存储、分析、实时计算、可视化、事件管理与智能洞察的全链路，帮助企业以极高的性能、极低的成本和极简的体验，全面释放数据价值。</p><p>TDengine IDMP 具备以下四大核心优势：</p><ul><li><strong>无问智推，数据自己说话：</strong>无需主动提问，基于采集的数据，TDengine IDMP 能够利用 LLM，自动感知应用场景，自动生成场景特有的的指标、可视化面板、报表和实时数据分析。无需业务知识的多年积累，无需主动查询，核心洞察主动推送。</li><li><strong>智能问数，实时分析零等待：</strong>除 AI 主动推送的面板、分析之外，用户还可以用自然语言主动提问与数据相关的任何问题。无需数据分析师、IT 工程师的帮助，AI 基于采集的数据实时给予答案，即可形成行动方案。从提问到决策，分钟级闭环。</li><li><strong>工业数据全栈解决方案：</strong>与 TDengine 时序数据库一起，为工业数据管理提供从数据采集、清洗、情景化、标准化，到存储、查询、实时分析、预测、异常检测，再到可视化、事件管理等全栈的解决方案。架构极简，运维轻量化。</li><li><strong>开放的企业级应用：</strong>支持单点登录、基于角色的权限控制、数据模型版本管理，提供数据备份、异地容灾与实时分发能力，支持虚机与容器化部署，兼容 Windows 与 Linux，可与 MES、ERP、AI 等企业应用系统无缝集成。</li></ul><h2>唯一完测，赋能百业，智驱未来</h2><p>作为<strong>截至目前唯一一家</strong>全项通过中国信通院基于 AI 大模型的时序数据管理平台基础能力检验的产品，TDengine IDMP 在推出不足半年内，已在能源、化工、智能制造、交通、食品等多个行业实现落地应用，客户覆盖海内外市场。</p><p>此次全项完测，不仅是对 TDengine IDMP 技术体系完整性与成熟度的权威验证，也体现了涛思数据在 AI 与时序数据融合方向上的长期投入与工程实践能力，<strong>标志着 TDengine IDMP 在这一领域的成熟度已达到国内领先水平。</strong></p><p>面向未来，涛思数据将持续提升平台的开放性、实时性与智能化水平，推动 AI 真正参与工业数据消费与决策过程，为企业数字化与智能化转型提供更加可靠、可持续的技术底座。</p>]]></description></item><item>    <title><![CDATA[电子制造行业MES系统解决方案 万界星空科技 ]]></title>    <link>https://segmentfault.com/a/1190000047597143</link>    <guid>https://segmentfault.com/a/1190000047597143</guid>    <pubDate>2026-02-06 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>——覆盖芯片制造、封装测试、智能仓储与供应链协同的一体化智造平-台 </strong> </p><p>在半导体国产化加速与高端电子制造升级的双重驱动下，电子制造企业（涵盖晶圆厂Fab与封测厂OSAT）正面临前所未有的挑战：工艺复杂度指数级上升、客户追溯要求严苛至单颗芯片、物料成本占比超60%、设备停机1分钟损失数万元。传统ERP+WMS+通用MES的割裂架构已难以为继。</p><p>万界星空电子制造行业专属MES系统——深度融合芯片制造（前道）、电子封装测试（后道）、智能仓储与供应链执行的全栈式解决方案，真正实现从“硅片进厂”到“芯片出货”的端到端闭环管控。<br/><img width="723" height="405" referrerpolicy="no-referrer" src="/img/bVdnSlT" alt="" title=""/><br/><strong>一、行业痛点：为何通用系统失效？</strong><br/>芯片制造（Fab）   工艺步骤超千道，Lot路径动态分裂；设备Recipe毫秒级同步；缺陷根因分析依赖跨工序数据</p><p>电子封装（OSAT）   多芯片异构集成（SiP/Chiplet）；金线/塑封料温湿度敏感；汽车电子需满足AEC-Q100追溯</p><p>供应链与仓储   关键物料（光刻胶、金线）保质期短；洁净室库位管理复杂；投料错配=整批报废</p><p>共性需求   全链路追溯（Wafer→Die→Package→终端产品）、EHS合规、OEE提升、零缺陷交付</p><p>普通MES仅关注“报工”，而电子制造需要的是以物料流、信息流、控制流三流合一的智能执行中枢。</p><p><strong>二、系统整体架构：前道+后道+仓储一体化</strong></p><p>见图<br/><img width="723" height="583" referrerpolicy="no-referrer" src="/img/bVdnSl0" alt="" title="" loading="lazy"/></p><p><strong>三、电子行业MES核心功能体系</strong></p><p>✅ 1. 芯片制造（Fab）全流程管控</p><ul><li>Lot/Wafer级追踪：支持Split/Merge操作，记录每片晶圆上千道工序历史；</li><li>Recipe与设备闭环：下发工艺配-方至设备，实时监控腔室参数，异常自动Hold Lot；</li><li>缺陷智能分析：集成AOI/E-beam数据，自动关联工艺步骤与设备，生成Yield根因报告；</li><li>洁净室EHS监控：粒子数、压差、特气泄漏实时告警，保障Fab安全运行。</li></ul><p>✅ 2. 电子封装测试（OSAT）高精度执行</p><ul><li>先进封装支持：管理Fan-Out、2.5D/3D、SiP等工艺，绑定RDL、TSV、Microbump数据；</li><li>全流程防错：贴片扫码校验Die与基板匹配，回流焊曲线自动比对，X-ray未检禁止流转；</li><li>测试数据闭环：ATE（CP/FT）结果自动归集，不良品关联失效模式，驱动FA分析；</li><li>汽车电子合规：一键生成PPAP文件包，满足IATF 16949与AEC-Q100要求。</li></ul><p>✅ 3. 采购与供应商协同（MES驱动）</p><ul><li>智能物料需求：基于MPS与BOM，自动计算光刻胶、金线、靶材等关键物料净需求；</li><li>供应商门户：共享交付计划、质量标准、包装规范，支持ASN电子化；</li><li>来料质量预控：COA（分析证书）预加载，IQC结果自动比对，超标物料冻结。</li></ul><p>✅ 4. 智能投料与物料防错</p><ul><li>Fab投料校验：启动Lot前，校验光刻胶有效期、靶材使用次数、特气余量；</li><li>OSAT投料拦截：Die Bin码、基板烘烤状态、湿度卡不合格 → 设备联锁停机；</li><li>FIFO与效期管控：化学品按开封时间强制先进先出，超期自动锁定。</li></ul><p>✅ 5. 精细化出入库与仓储管理</p><ul><li><p>智能库位分配：</p><ul><li>恒温区（光刻胶）、氮气柜（金线）、防静电架（FOUP）自动匹配；</li></ul></li><li>AMHS/AGV协同：MES下发配送任务，自动送物料至机台口；</li><li>库存实时可视：展示在库量、库龄、洁净室水位，预警呆滞与缺料风险；</li><li>退料与危废管理：不良品隔离、废酸废溶剂登记，满足EHS审计。</li></ul><p>✅ 6. 全链路追溯与合规</p><ul><li>正向追踪：某片晶圆 → 切割Die → 封装成品 → 终端手机型号；</li><li>反向溯源：客户投诉 → 精准定位至光刻层、刻蚀机台、贴片时间、测试Bin；</li><li>电子批记录（EBR）：自动生成不可篡改档案，支持FDA 21 CFR Part 11、ISO 9001。</li></ul><p>✅ 7. 设备物联与智能排产</p><ul><li>千台设备接入：通过SECS/GEM、OPC UA对接中微、北方华创、ASM等设备；</li><li>OEE自动分析：精准统计时间开动率、性能率、良品率；</li><li>柔性排程：Fab按腔室可用性调度，OSAT考虑模具准备与交期，支持插单模拟。</li></ul><p><strong>四、实施价值</strong><br/>产品良率（Yield）   ↑ 2–5%（缺陷根因快速定位）</p><p>设备综合效率（OEE）   ↑ 10–15%（减少非计划停机）</p><p>物料错用事故   ↓ 90%（投料防错拦截）</p><p>库存周转率   ↑ 20%（智能FIFO+呆滞预警）</p><p>追溯响应速度   从“天级” → “分钟级”</p><p>客户飞检通过率   100%（电子批记录完整合规）</p><p>**在“中国芯”崛起的时代，  <br/>制造的竞争力不再仅靠设备，而在于数据驱动的协同力、过程受控的稳定性与快速响应的柔性力。**  <br/>电子行业MES——不止于执行，更赋能中国电子制造迈向自主、高效、可靠的新纪元。</p><p>📞 立即预约，获取《电子制造行业数字化转型MES解决方案》+ 行业免费Demo演示！</p>]]></description></item><item>    <title><![CDATA[Python运行本地Web服务并实现远程访问 ZeroNews内网穿透 ]]></title>    <link>https://segmentfault.com/a/1190000047596512</link>    <guid>https://segmentfault.com/a/1190000047596512</guid>    <pubDate>2026-02-06 15:14:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Python是一种功能强大的编程语言，其简洁的语法和丰富的标准库使得它成为快速搭建Web服务的理想工具。</p><p>本文将引导您从零开始，通过Python内置模块搭建本地Web服务，并结合 ZeroNews 实现远程访问。</p><h3>一、 安装Python并运行本地服务</h3><p><strong>环境准备</strong><br/>安装Python服务<br/>实现一个本地 web.py 本地服务</p><p>1. 首先在Python官网下载python服务<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596515" alt="图片" title="图片"/></p><p>2. 下载完成后，根据步骤安装即可3. 安装完成过后，我们可以通过命令检查我们的python是否安装成功。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596516" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596517" alt="图片" title="图片" loading="lazy"/></p><p>4. 看到上述出现对应的版本，就表示安装成功了5. 接下来，我们进入到我们Web本地服务的文件夹，例如 D:\Download\zeronews\python<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596518" alt="图片" title="图片" loading="lazy"/></p><p>5. 小编搭建了一个比较简单的 web服务（仅供参考，可以替换成自己的web服务项目）<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596519" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596520" alt="图片" title="图片" loading="lazy"/></p><ol start="6"><li>然后我们打开cmd窗口，并通过命令进入到web服务文件夹中<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596521" alt="图片" title="图片" loading="lazy"/></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596522" alt="图片" title="图片" loading="lazy"/></p><p>7. 然后通过python运行我们的本地服务<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596523" alt="图片" title="图片" loading="lazy"/><br/>httpserver.py 为我们本地服务运行的文件<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596524" alt="图片" title="图片" loading="lazy"/></p><p>8. 运行成功后，可以看到服务已经启动，可以通过浏览器访问以下地址：Web界面:127.0.0.1:8000<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596525" alt="图片" title="图片" loading="lazy"/></p><p>接下来，我们可以通过 ZeroNews 服务，将我们的web服务映射到公网访问</p><h3>二、 创建 ZeroNews 映射服务</h3><p>打开 ZeroNews 网站，然后选择您的系统（小编用的是用Win10，选择Windows即可），并按照对应的步骤和命令安装运行 Agent 服务。</p><p><strong>注意：</strong><br/>Agent 前台运行不能关闭命令窗口<br/>如果您想要开机自启动，可以执行后台运行命令</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596526" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596527" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596528" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596529" alt="图片" title="图片" loading="lazy"/></p><p>1. 运行完成之后，您可以在 Agent 页面看到已经在线的 Agent 服务。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596530" alt="图片" title="图片" loading="lazy"/></p><p>2. 接着，我们在域名端口页面，创建一个可用的公网域名（自定义前缀），并勾选HTTPS 协议端口。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596531" alt="图片" title="图片" loading="lazy"/></p><p>3. 域名创建完成之后，我们继续打开映射页面，并按下面的步骤添加映射<br/>Agent：选择第一步运行的 Agent<br/>映射协议：选择 HTTPS 协议<br/>域名：选择刚创建好的域名<br/>带宽：根据需要选择带宽大小<br/>内网IP：我们是本地部署，直接使用 127.0.0.1 即可<br/>内网端口：输入本地服务的端口 8000 即可<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596532" alt="图片" title="图片" loading="lazy"/></p><p>4. 照上述步骤创建完成之后，我们就可以得到一条可公网访问的映射域名<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596533" alt="图片" title="图片" loading="lazy"/></p><h3>三、 公网访问您的web本地服务</h3><p>我们在任意有网络访问电脑的浏览器上，复制上面的链接并打开访问我们的本地服务了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596534" alt="图片" title="图片" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[基于 YOLOv8 的包装箱纸板破损缺陷检测系统 [目标检测完整源码] 风筝 ]]></title>    <link>https://segmentfault.com/a/1190000047596678</link>    <guid>https://segmentfault.com/a/1190000047596678</guid>    <pubDate>2026-02-06 15:13:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>基于 YOLOv8 的包装箱纸板破损缺陷检测系统 [目标检测完整源码]</h2><h3>—— 面向工业产线的视觉缺陷检测完整解决方案</h3><hr/><h3>一、行业背景：包装箱质检为何成为“隐形瓶颈”？</h3><p>在制造业与物流行业中，纸板包装箱几乎无处不在。无论是电商仓储、食品包装，还是工业零部件运输，<strong>包装箱的完整性直接影响商品安全、客户体验与品牌信誉</strong>。</p><p>然而在实际生产中，纸板破损检测长期面临几个现实问题：</p><ul><li>👀 <strong>高度依赖人工目检</strong>，效率低、主观性强</li><li>📦 <strong>产线速度快</strong>，人工难以及时响应</li><li>📉 <strong>缺陷形态多样</strong>，如裂纹、孔洞、压痕、破边</li><li>🧠 <strong>经验难以复制</strong>，新员工学习成本高</li></ul><p>在“降本增效”和“智能制造”的双重驱动下，<strong>用视觉算法替代人工质检</strong>已成为趋势，而目标检测技术正是解决此类问题的核心手段。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596680" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>源码下载与效果演示</h3><p>哔哩哔哩视频下方观看：<br/><a href="https://www.bilibili.com/video/BV1k3b9z1E6E/" target="_blank">https://www.bilibili.com/video/BV1k3b9z1E6E/</a><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047561324" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/>包含：</p><p>📦完整项目源码</p><p>📦 预训练模型权重</p><p>🗂️ 数据集地址（含标注脚本</p><h3>二、技术选型：为什么纸板缺陷检测适合用 YOLOv8？</h3><h4>2.1 纸板破损的视觉特性分析</h4><p>从计算机视觉角度看，纸板破损具有以下特点：</p><ul><li>缺陷尺寸不一，小裂纹与大孔洞并存</li><li>缺陷形态不规则，难以用规则算法描述</li><li>背景纹理复杂，存在纸板纹路干扰</li></ul><p>这意味着，传统基于阈值、边缘或模板的方法很难稳定工作。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596681" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><hr/><h4>2.2 YOLOv8 的工程优势</h4><p>YOLOv8 作为新一代目标检测模型，在该场景中具备显著优势：</p><ul><li><strong>Anchor-Free 架构</strong>：对尺度变化与不规则目标更友好</li><li><strong>单阶段检测</strong>：满足产线实时检测需求</li><li><strong>结构轻量</strong>：适合部署在工控机或边缘设备</li><li><strong>生态成熟</strong>：训练、推理、导出流程清晰</li></ul><p>因此，本项目选择 YOLOv8 作为核心检测引擎，用于构建一套<strong>可直接落地的工业质检系统</strong>。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596682" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047596683" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>三、系统整体架构设计</h3><p>本项目并非停留在“模型能跑”，而是从一开始就按照<strong>完整工程系统</strong>来设计，整体结构如下：</p><pre><code>数据采集与标注
        ↓
YOLOv8 缺陷检测模型训练
        ↓
统一推理接口封装
        ↓
PyQt5 可视化质检界面
        ↓
一键运行与结果保存</code></pre><p>目标非常明确：</p><blockquote><strong>让算法真正服务于产线，而不是停留在实验室。</strong></blockquote><hr/><h3>四、缺陷数据集构建与标注经验</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596684" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>4.1 缺陷类型定义</h4><p>在纸板质检场景中，常见缺陷可归纳为：</p><ul><li>撕裂裂纹</li><li>穿孔破损</li><li>明显压痕</li><li>边缘破损</li><li>表面结构异常</li></ul><p>在数据集构建阶段，将不同缺陷统一建模为检测目标，便于模型学习空间位置与外观特征。</p><hr/><h4>4.2 数据集结构设计</h4><p>采用 YOLO 标准格式组织数据：</p><pre><code>dataset/
├── images/
│   ├── train/
│   └── val/
├── labels/
│   ├── train/
│   └── val/</code></pre><p>每张图片对应一个文本标注文件，记录缺陷目标的位置与类别。<br/>这种结构便于快速复训、扩展类别或迁移到其他工业缺陷场景。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596685" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>五、模型训练与调优要点</h3><h4>5.1 训练命令示例</h4><pre><code class="bash">yolo detect train \
  data=defect.yaml \
  model=yolov8n.pt \
  epochs=100 \
  batch=16 \
  imgsz=640</code></pre><p>在训练过程中，需要重点关注：</p><ul><li><strong>小缺陷召回率</strong>（避免漏检）</li><li>过拟合风险（缺陷外观相似）</li><li>数据增强是否破坏缺陷特征</li></ul><hr/><h4>5.2 训练结果评估</h4><p>YOLOv8 会自动输出：</p><ul><li>mAP 曲线（整体检测性能）</li><li>box / cls / dfl 损失变化</li><li>混淆矩阵（类别区分能力）</li></ul><p>在实际工业应用中，当 <strong>mAP@0.5 达到 90% 左右</strong>，即可满足大部分产线质检需求。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596686" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>六、统一推理逻辑：适配多种输入源</h3><p>为了贴近真实使用场景，系统支持多种检测方式：</p><h4>6.1 静态图片检测</h4><ul><li>适用于离线质检</li><li>数据回溯分析</li><li>模型效果验证</li></ul><hr/><h4>6.2 视频检测</h4><ul><li>用于产线录像分析</li><li>支持逐帧检测与结果保存</li><li>可作为质检复盘工具</li></ul><hr/><h4>6.3 实时摄像头检测</h4><p>这是工业落地的核心场景：</p><ul><li>实时显示缺陷位置</li><li>可对接报警系统</li><li>为后续自动剔除提供依据</li></ul><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047596687" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>七、PyQt5 图形界面：让质检人员“用得起来”</h3><p>很多算法项目的痛点在于：<br/><strong>只有算法工程师会用，现场人员用不了。</strong></p><p>本项目通过 PyQt5 构建完整 GUI，有效解决这一问题。</p><h4>7.1 界面功能设计</h4><ul><li>输入方式选择（图片 / 视频 / 摄像头）</li><li>检测结果实时显示</li><li>缺陷类别与置信度可视化</li><li>一键保存检测结果</li></ul><hr/><h4>7.2 工程价值</h4><ul><li>无需命令行操作</li><li>降低部署与培训成本</li><li>可直接作为产线质检终端原型</li></ul><hr/><h3>八、核心推理代码逻辑说明</h3><pre><code class="python">from ultralytics import YOLO

model = YOLO("best.pt")
results = model(frame, conf=0.25)

for box in results[0].boxes:
    cls_id = int(box.cls)
    score = float(box.conf)</code></pre><p>推理结果中即可获取：</p><ul><li>缺陷位置坐标</li><li>缺陷类别</li><li>置信度评分</li></ul><p>为后续 <strong>报警、统计、剔除</strong> 等业务逻辑提供基础数据。</p><hr/><h3>九、项目打包与“即用型”交付</h3><p>项目已完成完整工程封装，包含：</p><ul><li>训练完成的模型权重</li><li>全部 Python 源码</li><li>数据集与标注说明</li><li>PyQt5 主程序</li></ul><h4>运行方式极其简单：</h4><pre><code class="bash">python main.py</code></pre><p>无需重新训练，即可直接体验完整检测流程。</p><hr/><h3>十、可扩展方向与工业升级空间</h3><p>在现有框架基础上，可轻松拓展为：</p><ul><li>多缺陷类别精细化检测</li><li>接入 PLC / MES 系统</li><li>与自动分拣机构联动</li><li>部署至边缘 AI 设备</li></ul><p>从“辅助检测”逐步升级为“全自动智能质检”。</p><hr/><h3>总结：让 AI 真正走进包装产线</h3><p>本文围绕包装箱纸板破损这一典型工业痛点，系统性介绍了一套 <strong>基于 YOLOv8 的智能缺陷检测解决方案</strong>。项目不仅验证了深度学习在工业质检场景中的可行性，更通过 PyQt5 图形界面和完整工程封装，打通了从模型训练到实际使用的最后一公里。</p><p>如果你正在寻找一个<strong>可学习、可复用、可落地的工业视觉项目案例</strong>，那么这套包装箱纸板破损检测系统，具备非常高的实践价值与扩展空间。</p><p>通过引入 YOLOv8 目标检测模型并结合工程化系统设计，本文展示了一套面向真实工业产线的纸板包装箱破损缺陷智能检测方案。该方案从数据集构建、模型训练与调优出发，进一步延伸至统一推理接口与 PyQt5 可视化界面，实现了从算法验证到实际应用落地的完整闭环。实践表明，基于深度学习的视觉检测技术不仅能够显著提升质检效率与一致性，还为后续的自动剔除、质量追溯与产线智能化升级奠定了坚实基础，具有较高的推广与复用价值。</p>]]></description></item><item>    <title><![CDATA[云游戏企业避坑指南：如何选IDC机房？成都极云科技给出标准答案 极云Cloud ]]></title>    <link>https://segmentfault.com/a/1190000047596696</link>    <guid>https://segmentfault.com/a/1190000047596696</guid>    <pubDate>2026-02-06 15:12:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>一、云游戏的 “生死线”：被服务器拖垮的业务痛点</strong></p><p>做云游戏 5 年，我们曾因西南地区玩家延迟超 75ms，3 天流失 18% 核心用户；为承载 20 万并发，单月云带宽支出破 15 万，占营收 30%。这并非个例，中国音数协游戏工委数据显示，72% 用户因 “延迟超 50ms” 放弃体验，云游戏服务器托管有三大痛点：</p><p><strong>延迟敏感</strong>：每增 10ms 延迟，操作失误率升 8%，传统 IDC 单一网络易致跨区域体验崩盘；</p><p>带宽刚需：1080P/60 帧单用户需 8-12Mbps，10 万并发需 1T 带宽，扩容成本高、灵活性差会卡业务脖子；</p><p>存储算力双高：游戏安装包（平均 50GB / 款）需高速存储，高规格 GPU 服务器对供电、散热要求高，普通机房难满足。</p><p>此时，选适配的 IDC 机房成企业生死关键。</p><p><img referrerpolicy="no-referrer" src="https://image-static.segmentfault.com/347/112/347112119-698580557de85" alt="" title=""/></p><p><strong>二、云游戏选 IDC 的 5 个 “黄金标准”，缺一不可</strong></p><p>经 3 个月调研、20 + 机房对比，总结出核心逻辑 —— 围绕 “玩家体验” 与 “成本可控”，这 5 点是硬指标：</p><p><strong>标准 1：网络架构 “低延迟优先”，多线 BGP 是基础</strong></p><p>云游戏延迟由 “物理距离 + 网络节点” 决定，单一线路易致多运营商用户体验差。适配 IDC 需具备三线 （电信 + 联通 + 移动）。</p><p>成都极云科技主机房不仅支持电信、联通、移动单线机房，更有三线 接入，还搭建 “西南 - 华北 - 华东” 骨干网直连通道。我们测试时，西南玩家延迟 28-35ms，华北≤40ms，比云托管降 45%。更可按玩家分布定制带宽配比，避免冗余浪费。</p><p><strong>标准 2：带宽 “足量 + 灵活”，扩容成本可控</strong></p><p>云游戏带宽有 “潮汐特性”，闲时利用率仅 30%，传统 IDC 固定套餐浪费多、临时扩容需 3-5 天，难应对突发需求。</p><p>极云带宽方案破解矛盾：</p><p>基础带宽性价比高：100M 独享电信带宽月费 1800 元（18 元 / M / 月），远低于云厂商 50-80 元 / M / 月；</p><p>弹性扩容秒级响应：运维平台可实时申请临时扩容（最高 1000M），按小时计费，去年双十一加 500M 带宽 3 小时仅 225 元，省 60%；</p><p>流量监控可视化：实时查看分区带宽，可关停低效分区控成本。</p><p><strong>标准 3：存储 “高低速分层”，适配游戏数据特性</strong></p><p>云游戏热数据（安装包、缓存）需毫秒级响应，冷数据（存档、日志）需大容量，单一存储方案难平衡体验与成本。</p><p>极云定制分层存储方案：</p><p>热数据区：NVMe SSD 阵列读写 3500MB/s，游戏加载时间从 25 秒压至 8 秒，投诉降 70%；</p><p>冷数据区：HDD+zstd 压缩，1000 款游戏存档从 50TB 压至 22TB，成本降 56%；</p><p>自动分层调度：按访问频率自动迁移数据，无需人工干预，运维效率升 80%。</p><p><strong>标准 4：算力承载 “适配高规格服务器”，供电散热有保障</strong></p><p>云游戏依赖高配置 GPU 服务器（如 RTX 4090，单台功耗 800W），普通 IDC 机柜供电（10A）、散热不足易死机。</p><p>极云硬件承载优势显著：</p><p>高功率机柜：16A/32A 规格，单柜供电 7.68KW，配独立散热，温度稳定 22-25℃；</p><p>灵活部署：支持 4U/8U 高密度托管，20 台 GPU 服务器仅占 5 机柜，年省 3.6 万；</p><p>硬件兼容：工程师提前对接厂商测兼容性，20 台服务器 2 天完成上架。</p><p><strong>标</strong><strong>准 5：运维 “7×24 小时零中断”，故障响应快</strong></p><p>云游戏需全天候服务，1 小时故障或致玩家流失，IDC 运维需快速解决、提前预防。</p><p>极云运维让我们放心：</p><p>分钟级响应：去年春节机柜电源故障，工程师 12 分钟到场，切换备用电源，中断仅 45 秒（行业平均 30 分钟）；</p><p>主动巡检：每周 2 次硬件巡检，提前更换 2 块故障 SSD，避数据丢失；</p><p>专属对接：1 对 1 运维经理，可按业务节奏（如新版本上线）提前扩容，无需反复沟通。</p><p><strong>三、实战效果：托管半年，玩家留存升 20%，成本降 40%</strong></p><p>迁移极云半年，业务数据显著改善：</p><p>体验端：平均延迟从 62ms 降至 32ms，卡顿率从 15% 降至 3%，核心玩家月留存升 20%，新增次日留存升 12%；</p><p>成本端：月均托管成本从 15 万降至 9 万，省 40%（带宽省 52%、存储省 56%、运维人力省 35%）；</p><p>稳定性端：机房可用性 99.92%（云托管 99.5%），故障从每月 3-4 次降至 0 次。</p><p>成都某同行用极云 “IDC + 云弹性扩容” 方案，峰值并发从 10 万升至 30 万，成本仅增 50%，玩家满意度居行业 Top3。</p><p><strong>四、结语：云游戏选 IDC，找对 “适配者” 比选 “贵的” 更重要</strong></p><p>对云游戏企业，IDC 是业务增长的基础设施，无需盲目追高规格，需找匹配需求（用户分布、带宽波动、服务器配置）的伙伴。成都极云科技懂云游戏，方案围绕核心需求设计，实现 “体验不打折，成本可控制”。</p><p>若你正被延迟、带宽、成本困扰，可了解极云机房产品，更多详情访问官网，或咨询云游戏专属解决方案顾问。</p>]]></description></item><item>    <title><![CDATA[如何使用 MyDumper 重建 MySQL 副本？ 本文系翻译，阅读原文
https://www.]]></title>    <link>https://segmentfault.com/a/1190000047596709</link>    <guid>https://segmentfault.com/a/1190000047596709</guid>    <pubDate>2026-02-06 15:12:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>作者：David Ducos，Percona 团队 DBA。</p><p>原文：<a href="https://link.segmentfault.com/?enc=0DwC0yqlwI97UOrSQmULjQ%3D%3D.B683eqTLSlMZWBbfeoD7jP6Nslu65xyxQ3SnGtUKWjKDnP7kZO0WAloHWvuQWb5uQwYWFe%2Fhd%2BMumTYtGavHIm6C5rmLw8oDcMHnr6CQ8ig%3D" rel="nofollow" target="_blank">https://www.percona.com/blog/rebuilding-a-replica-with-mydumper/</a></p></blockquote><h2>1. 什么是 MyDumper？</h2><p>当副本因损坏或漂移而失效时，如果无法使用 <code>pt-table-sync</code>，标准解决方案是从主数据库的全新副本重建副本。传统上，为了快速重建副本，我们会使用 <strong>物理备份</strong>，但在某些情况下，逻辑备份仍然必不可少。例如，当您迁移到特定供应商（例如：从 MariaDB 迁移到 MySQL）或存储引擎（过去是从 MyISAM 迁移到 InnoDB，现在是从 InnoDB 迁移到 RocksDB）、升级到新的数据库版本或迁移到云端解决方案时。</p><p><strong>逻辑备份</strong> 正是在这种情况下发挥作用，它提供了可移植性和简易性，但前提是能够快速执行。<a href="https://link.segmentfault.com/?enc=ZeOmd6HIoKzXbR%2B6i9TeGA%3D%3D.t9Tcb5KDme4RFV83%2FYfu1DDzgDWTe1iwqZdMHLWnCMv3p2SeylvAi5UJrLFkGz1YzjbRqJc7tH4%2BokWzi0uh1A%3D%3D" rel="nofollow" title="MyDumper 文档页" target="_blank">MyDumper</a> 应运而生，成为一款必不可少的现代化解决方案，它兼具两者的优势：逻辑转储的跨平台、跨版本灵活性，以及以往只有物理方法才能实现的并行、多线程速度，使其成为快速重建一致性副本的理想之选。</p><h2>2. 备份</h2><p>第一步是进行备份。<em>mydumper</em> 有多个参数可供使用，本例中我们将使用以下参数：</p><pre><code class="bash">mydumper -v 4 -o data --clear 
--regex '^(?!(mysql.|sys.))' 
--source-data</code></pre><p>前 3 行与日志记录和备份目录有关，第二行用于忽略 <code>mysql</code> 和 <code>sys</code> 模式，最后 <code>–source-data</code> 将指示 <em>mydumper</em> 将恢复后复制配置所需的所有信息保存到元数据文件中，位于 <code>[source]</code> 部分。</p><p>以下是输出示例：</p><pre><code class="yaml">[source]
# Channel_Name = '' # It can be used to setup replication FOR CHANNEL
# SOURCE_LOG_FILE = "binlog.000020"
# SOURCE_LOG_POS = 6803936
#SOURCE_HOST = "172.17.0.3"
#SOURCE_PORT =
#SOURCE_USER = ""
#SOURCE_PASSWORD = ""
#SOURCE_SSL = {0|1}
executed_gtid_set = "941fdce6-47c4-11f0-87b2-0242ac110006:1-52"
SOURCE_LOG_FILE = "binlog.000020"
SOURCE_LOG_POS = 6803936
#SOURCE_AUTO_POSITION = {0|1}
myloader_exec_reset_replica = 0
myloader_exec_change_source = 0
myloader_exec_start_replica = 0</code></pre><p>如图所示，这些选项已启用：</p><pre><code class="yaml">executed_gtid_set = "941fdce6-47c4-11f0-87b2-0242ac110006:1-52"
SOURCE_LOG_FILE = "binlog.000020"
SOURCE_LOG_POS = 6803936</code></pre><p>但是，这些命令的执行已被禁用：</p><pre><code class="yaml">myloader_exec_reset_replica = 0
myloader_exec_change_source = 0
myloader_exec_start_replica = 0
We can enable them, if we set --source-data=7, then the metadata will change to:
myloader_exec_reset_replica = 1
myloader_exec_change_source = 1
myloader_exec_start_replica = 1</code></pre><p>这是自动配置复制所必需的。</p><h2>3. 配置复制</h2><p>默认情况下将使用 <code>SOURCE_LOG_FILE</code> 和 <code>SOURCE_LOG_POS</code>，但如果您配置 <code>SOURCE_AUTO_POSITION = 1</code>，则可以设置 GTID 位置。</p><p>如您所知，要设置复制，我们需要执行 <code>CHANGE SOURCE</code> 命令。但是，根据您的具体使用情况，您可能需要执行 <code>RESET REPLICA</code> 命令，并且在执行 <code>CHANGE SOURCE</code> 命令后，通常需要执行 <code>START REPLICA</code> 命令。如果您在元数据文件中使用以下方式进行设置，<a href="https://link.segmentfault.com/?enc=u0GHa0NFnAMllq0jbjl9QA%3D%3D.BV7eHVXxsqVr8vdDAvq2mTBz3GMI0ZNnm%2FIiOib8zgckhxFQnGMQdTjWPfaZbuzKeczKc1xcLyzePBQlcv47dKS5%2FMU5I8lOPOMNDJpQu68%3D" rel="nofollow" title="myloader 文档页" target="_blank">myloader</a> 可以自动完成此操作：</p><pre><code class="yaml">myloader_exec_reset_replica = 1
myloader_exec_change_source = 1
myloader_exec_start_replica = 1</code></pre><p>或者，您可以在 <em>myloader</em> 中使用 <code>--source-data=7</code> 作为参数。是的！<em>myloader</em> 也接受 <code>--source-data</code> 参数。</p><p>根据您的使用场景，您可能需要在元数据文件中配置以下其他选项：</p><pre><code class="yaml">#SOURCE_HOST = "172.17.0.3"
#SOURCE_PORT =
#SOURCE_USER = ""
#SOURCE_PASSWORD = ""
#SOURCE_SSL = {0|1}
executed_gtid_set = "941fdce6-47c4-11f0-87b2-0242ac110006:1-52"
SOURCE_LOG_FILE = "binlog.000020"
SOURCE_LOG_POS = 6803936
#SOURCE_AUTO_POSITION = {0|1}</code></pre><p>由于存在多种使用场景，如果您想从头开始重建副本，则需要按如下方式配置：</p><pre><code class="yaml">[source]
SOURCE_HOST = "172.17.0.3"
SOURCE_PORT = 3306
SOURCE_USER = "replica"
SOURCE_PASSWORD = "r3pl1c4"
executed_gtid_set = "941fdce6-47c4-11f0-87b2-0242ac110006:1-52"
SOURCE_LOG_FILE = "binlog.000020"
SOURCE_LOG_POS = 6803936
myloader_exec_reset_replica = 1
myloader_exec_change_source = 1
myloader_exec_start_replica = 1</code></pre><p>如果您已经建立了一个正在运行的复制系统，并且想要在不更改主机或凭据的情况下重建它，那么您可以按以下方式进行配置：</p><pre><code class="yaml">[source]
executed_gtid_set = "941fdce6-47c4-11f0-87b2-0242ac110006:1-52"
SOURCE_LOG_FILE = "binlog.000020"
SOURCE_LOG_POS = 6803936
myloader_exec_reset_replica = 0
myloader_exec_change_source = 1
myloader_exec_start_replica = 1</code></pre><p>SSL 是 <em>myloader</em> 中 <code>--source-data</code> 参数可以设置的另一个选项，无需在元数据文件中使用 <code>SOURCE_SSL</code>。完整的选项列表如下：<code>exec_start_slave (1)</code>、<code>exec_change_master (2)</code>、<code>exec_reset_slave (4)</code>、<code>SSL (8)</code>、<code>auto_position (16)</code> 和 <code>exec_start_replica_until (32)</code>。根据您要设置的配置和要执行的语句，您需要将这些值相加，并将其传递给 <code>--source-data</code> 参数。</p><h2>4. 恢复</h2><p>配置好元数据文件后，即可执行 <em>myloader</em>，其界面如下所示：</p><pre><code class="bash">myloader -d data -v 4 
-o --max-threads-for-schema-creation=1 
-h replica_host</code></pre><p>在日志中，你会发现 <em>myloader</em> 发送了以下命令：</p><pre><code class="bash">2025-12-18 16:57:09 [INFO] - Schema create checksum confirmed for sakila
2025-12-18 16:57:09 [INFO] - Sending reset replica
2025-12-18 16:57:09 [INFO] - Sending change replication source
2025-12-18 16:57:09 [INFO] - Sending start replica
2025-12-18 16:57:09 [INFO] - Restore completed</code></pre><p><em>mydumper</em> 会发送命令，但不会检查输出，这意味着如果复制配置失败或无法启动，您需要手动检查并修复。但是，它会检测到命令是否失败，例如，如果使用了 <code>SOURCE_USER</code> 而不是 <code>SOURCE_USER</code>：</p><pre><code class="bash">2025-12-18 17:02:56 [WARNING] - Sending replication command: CHANGE REPLICATION SOURCE TO SOURCE_HOST = "172.17.0.4", SUORCE_USER = "root", SOURCE_PASSWORD = "", SOURCE_LOG_FILE = "binlog.000020", SOURCE_LOG_POS = 1362220 FOR CHANNEL ''; - ERROR 1064: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'SUORCE_USER = "root", SOURCE_PASSWORD = "", SOURCE_LOG_FILE = "binlog.000020", SO' at line 1</code></pre><h2>5. 对故障副本进行重建</h2><p>有一个有趣的用例，我们可以使用 <code>START REPLICA UNTIL</code> 来修复某些表的偏移，而 <code>pt-table-sync</code> 或重建整个副本是不可能的。</p><p>假设我们有一个源数据库和一个副本数据库，我们发现副本数据库上的数据发生了，并且复制过程停止并出现如下错误：</p><pre><code class="bash">LAST_ERROR_MESSAGE: Worker 1 failed executing transaction 'ANONYMOUS' at source log binlog.000020, end_log_pos 1369103; Could not execute Update_rows event on table test.test_table; Can't find record in 'test_table', Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's source log binlog.000020, end_log_pos 1369103</code></pre><p>我们检查了二进制日志，发现它因对一组行进行更新而失败：</p><pre><code># at 1368995
#251218 19:34:59 server id 1 end_log_pos 1369103 CRC32 0x60a481d6 Update_rows: table id 344 flags: STMT_END_F
### UPDATE `test`.`test_table`
### WHERE
### @1=12 /* INT meta=0 nullable=0 is_null=0 */
### @2=7062 /* INT meta=0 nullable=1 is_null=0 */
### SET
### @1=12 /* INT meta=0 nullable=0 is_null=0 */
### @2=7063 /* INT meta=0 nullable=1 is_null=0 */
### UPDATE `test`.`test_table`
### WHERE
### @1=15 /* INT meta=0 nullable=0 is_null=0 */
### @2=7521 /* INT meta=0 nullable=1 is_null=0 */
### SET
### @1=15 /* INT meta=0 nullable=0 is_null=0 */
### @2=7522 /* INT meta=0 nullable=1 is_null=0 */
### UPDATE `test`.`test_table`
### WHERE
### @1=17 /* INT meta=0 nullable=0 is_null=0 */
### @2=8706 /* INT meta=0 nullable=1 is_null=0 */
### SET
### @1=17 /* INT meta=0 nullable=0 is_null=0 */
### @2=8707 /* INT meta=0 nullable=1 is_null=0 */
### UPDATE `test`.`test_table`
### WHERE
### @1=18 /* INT meta=0 nullable=0 is_null=0 */
### @2=8108 /* INT meta=0 nullable=1 is_null=0 */
### SET
### @1=18 /* INT meta=0 nullable=0 is_null=0 */
### @2=8109 /* INT meta=0 nullable=1 is_null=0 */
# at 1369103</code></pre><p>我们检查了数据库，发现数据确实发生了偏移：</p><p><strong>源端</strong></p><pre><code class="sql">mysql&gt; select count(*) from test.test_table;
+----------+
| count(*) |
+----------+
| 15 |
+----------+
1 row in set (0.00 sec)</code></pre><p><strong>副本</strong></p><pre><code class="sql">mysql&gt; select count(*) from test.test_table;
+----------+
| count(*) |
+----------+
| 14 |
+----------+
1 row in set (0.00 sec)</code></pre><p>使用 <em>MyDumper</em>，我们可以按照以下步骤重建表：</p><p>我们需要忽略该表，以便副本能够赶上进度。</p><pre><code class="sql">mysql-replica&gt; STOP REPLICA;
Query OK, 0 rows affected (0.00 sec)

mysql-replica&gt; CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE= (test.test_table);
Query OK, 0 rows affected (0.00 sec)

mysql-replica&gt; START REPLICA;
Query OK, 0 rows affected (0.00 sec)</code></pre><p>副本更新完成后，我们需要停止副本：</p><pre><code class="sql">mysql-replica&gt; STOP REPLICA;
Query OK, 0 rows affected (0.00 sec)</code></pre><p>并对源服务器进行备份：</p><pre><code class="bash">mydumper -v 4 -o data --clear 
-T test.test_table 
--source-data</code></pre><p>我们使用 <code>-T</code> 来备份有问题的表，而 <code>–source-data</code> 将启用我们需要的元数据文件上的复制变量。</p><p>然后，我们使用正确的值通过 <code>--source-data</code> 参数恢复表。</p><pre><code class="bash">myloader -d data -v 4 
-o --max-threads-for-schema-creation=1 
-h replica_host 
--source-data=32</code></pre><p>第 32 行是执行 <code>START REPLICA UNTIL</code>。</p><p>最后，我们移除忽略表选项并重新启动副本：</p><pre><code class="sql">mysql-replica&gt; CHANGE REPLICATION FILTER REPLICATE_IGNORE_TABLE= ();
Query OK, 0 rows affected (0.00 sec)

mysql-replica&gt; START REPLICA;
Query OK, 0 rows affected (0.00 sec)</code></pre><p><em>myloader</em> 在备份开始时执行的 <code>START REPLICA UNTIL</code> 将强制副本在备份表的位置停止，从而使我们能够在一致的场景中继续复制。</p><h2>6. 结论</h2><p>从传统的数据转储方法转向 <em>MyDumper</em> 不仅仅意味着性能的提升，更代表着数据完整性和迁移性的现代化。通过将备份过程从单线程执行的限制中解耦，数据库管理员现在可以像以往处理小型测试环境一样灵活地处理海量数据集。</p><p>将 <em>MyDumper</em> 集成到您的标准操作手册中，可确保您能够应对各种不可预测的情况 —— 无论是紧急副本重建还是计划内的架构迁移。在数据量持续呈指数级增长的时代，拥有一款兼具逻辑灵活性和并行速度的工具至关重要，而 <em>MyDumper</em> 正是这样一款工具。将其保留在您的工具箱中，下次遇到“仅逻辑恢复”场景时，您将拥有显著的竞争优势。</p>]]></description></item>  </channel></rss>