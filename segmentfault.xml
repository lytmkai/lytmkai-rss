<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[离GPT-5最近的一次！中国1万亿参数开]]></title>    <link>https://segmentfault.com/a/1190000047384739</link>    <guid>https://segmentfault.com/a/1190000047384739</guid>    <pubDate>2025-11-10 11:10:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：倾倾 桃子</p><p>【新智元导读】Kimi K2 Thinking重磅开源，1万亿「思考Agent模型」在推理、智能体基准上干翻GPT-5。关键，还能连调300次工具，直出3D模拟。</p><p>昨天，月之暗面发布全新模型Kimi K2 Thinking，一上线就挤爆了服务器。</p><p>思考，是它的核心卖点，自称是开源的「思考Agent模型」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384741" alt="" title=""/></p><p>它同样采用了MoE架构，总参数约1万亿，每次激活约320亿，上下文256K token。</p><p>在各大基准测试中，Kimi K2 Thinking性能表现亮眼。</p><p>尤其是，在BrowseComp、HLE测试中，实力完全碾压GPT-5、Claude Sonnet 4.5。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384742" alt="" title="" loading="lazy"/></p><p>在Tau2 Bench Telecom基准测试中，K2 Thinking位列第一。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384743" alt="" title="" loading="lazy"/></p><p>最关键的是，在无人干预情况下，K2 Thinking可连续调用200-300次工具。</p><p>国外研究者Nathan Lambert 称它为：「开源模型距闭源前沿最近的一次。」</p><p>这句话在技术圈广为流传，人们也开始重新审视这款模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384744" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384745" alt="" title="" loading="lazy"/></p><p>不只是聊天工具，K2 Thinking更像是一个会自己推理、自己动手的智能体。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384746" alt="" title="" loading="lazy"/></p><p><strong>一款真正会思考的模型</strong></p><p>Kimi K2 Thinking没有强调算力更大，而是强调更会「思考」。</p><p>这些配置让它在处理长文本、复杂任务时能维持更稳定的推理过程。</p><p>苹果大牛Awni Hannun测试后惊叹道：</p><p>1万亿参数，只用2台M3 Ultra芯片的Mac电脑即可流畅运行，而且int4压缩后性能几乎无损。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384747" alt="" title="" loading="lazy"/></p><p>通过mlx-lm并行技术，它生成了大约3500个token，速度每秒15个token。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384748" alt="" title="" loading="lazy"/></p><p>但真正让人关注的，是它的「思考能力」。</p><p>如前所述，K2 Thinking可以在一次任务中<strong>连续执行200到300次工具调用</strong>，全程无需人工干预。</p><p>有网友实测「工具调用」，立即制作出如下的数学和物理讲解动画。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384749" alt="" title="" loading="lazy"/></p><p>不同于其他模型的胡编乱造，它在面对复杂问题时，会自己拆解步骤、搜索信息、调用外部工具、再整合结果。</p><p>团队把这种机制称为「交替思考」——模型在「思考」和「执行」之间循环往复，让推理更连贯。</p><p>K2 Thinking在性能上的表现也很亮眼。</p><p>在Humanity’s Last Exam（HLE）和 BrowseComp（网页搜索综合能力）任务上，成绩已经接近甚至超过GPT-5和Claude Sonnet 4.5。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384750" alt="" title="" loading="lazy"/></p><p>Kimi K2 Thinking与GPT-5、Claude Sonnet 4.5在多项基准测试中的表现</p><p>除了推理表现，它在工程落地上也做了不少优化。</p><p>K2使用<strong>量化感知训练（QAT）</strong> 对MoE模块进行INT4权重量化，在保证性能的同时，将生成速度提升了约两倍。</p><p>除了推理和搜索任务，K2 Thinking在编码、工具使用、数学推理等更细分的测试中表现也很突出。</p><p>在SWE-bench、LiveCodeBench、GPQA-Diamond等任务上，它的成绩已经超过DeepSeek、GPT-4 Turbo等多个主流模型，显示出更强的「执行力」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384751" alt="" title="" loading="lazy"/></p><p>Kimi K2 Thinking在多项编程与数学任务中的表现对比</p><p>这意味着，K2 Thinking的测试成绩就是它在真实环境下的表现，而非理想化打分。</p><p>它目前已经在kimi.com上线，并开源API和模型权重，开发者可以直接试用。</p><p>从实验室到真实场景，这个模型的「思考能力」明显超过了现有的其他模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384752" alt="" title="" loading="lazy"/></p><p><strong>智能体编码一流，300次工具调用</strong></p><p>这一次，月之暗面没再让模型停留在论文里。</p><p>K2 Thinking不是展示品，而是一台真正能被人用起来的智能体。</p><p>发布当天，团队同步上线了 <strong>kimi.com聊天模式</strong>、开放了<strong>API 接口</strong>，还在Hugging Face公布了完整权重。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384753" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=UT6HNlx%2FWtkthgE%2BSDqQfQ%3D%3D.Rh2IVMB0cRI8IjXWb4D4HNU04S84oGioX2FyK%2BRWg61Q%2FyEkM2fiYwFc%2FtoHdrmojbQgX0hbmmHmGnKHDAXyWA%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/moonsh...</a>\_source</p><p>开发者不需要等待内测邀请，也不用注册繁琐流程，任何人都能直接使用。</p><p>K2 Thinking的从训练开始，到优化，再到上线，周期不到半年。</p><p>在这个动辄以年为单位更新的大模型时代，这个速度意味着它已具备完整的工程化能力。</p><p>打开kimi.com，就能直接体验到K2 Thinking的思考过程。</p><p>与一般聊天模型不同，它在生成答案前，会清晰地展示自己的推理链。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384754" alt="" title="" loading="lazy"/></p><p>此外，研究人员特别提到，K2 Thinking在软件和编码任务上进步显著。</p><p>它在 SWE-Multilingual测试中得分61.1% ，在SWE-Bench Verified测试中得分71.3%，在Terminal-Bench测试中得分47.1%。</p><p>这无疑证明了，该模型在HTML、React等方面的任务上有了很明显的进步。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384755" alt="" title="" loading="lazy"/></p><p><strong>写代码前，先写计划</strong></p><p>当用户输入「分析我发给你的CSV文件，并生成图表来支持你的分析」时，K2不会直接输出代码。</p><p>他会先列出自己的行动方案：首先，加载数据集，接下来，筛选数据集，然后，分析内容，调用绘图库，最终生成结果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384756" alt="" title="" loading="lazy"/></p><p>有了行动方案，它才会逐步生成代码，执行、验证、修正。</p><p>如果出错，它会提示「正在重新规划」，然后自动尝试新方案。整个过程，都能在屏幕上看到。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384757" alt="" title="" loading="lazy"/></p><p>最终，我们能得到K2生成的数据分析图表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384758" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384759" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384760" alt="" title="" loading="lazy"/></p><p>仅仅调用14次python，就能生成这样完美的可视化图表、准确的统计数据以及包含详细分析的交互网页。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384761" alt="" title="" loading="lazy"/></p><p><strong>私人定制行程：比管家还靠谱</strong></p><p>你是否想过拥有一个完美管家？那K2可以满足你的需求。</p><p>你只要提出你的需求，比如「我的预算是1000美元，给我规划我的演唱会之旅」。</p><p>输入之后，K2就会像一位尽职尽责的管家，询问你的喜好、目的、工作安排，甚至查阅你的谷歌邮件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384762" alt="" title="" loading="lazy"/></p><p>之后，他开始搜索，查机票、看演唱会场次，甚至会考虑到演唱会附近的餐厅。简直比管家还贴心！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384763" alt="" title="" loading="lazy"/></p><p>最后，结合各方数据，交出最适合你的演唱会计划。</p><p>而做到这些，仅仅调用了17次工具！很难想象如果亲自做计划，要耗时多久。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384764" alt="" title="" loading="lazy"/></p><p><strong>一针见血的数学讲解员</strong></p><p>除了长段的提示词，短短几句话，K2 Thinking也能完美运行。</p><p>比如，对它说「解释二维梯度下降」。</p><p>它就能调用工具，以最直观、形象的方式向你作出解释：</p><p>蓝色的等高线越靠近中心，函数值越小；黄色的路径是优化算法从起点到最优点的下降轨迹；红色小箭头表示梯度（∇f）的方向；黄色点表示当前的模型参数位置，它沿着梯度的反方向移动。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384765" alt="" title="" loading="lazy"/></p><p>配合上动图，一目了然。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384766" alt="" title="" loading="lazy"/></p><p><strong>触手可及的「细胞战」</strong></p><p>不仅仅是数学，K2 Thinking甚至进军生物学领域！</p><p>你只要输入「做一个可以调节免疫参数的病毒模拟程序」，就可以得到一个可交互的病毒仿真系统。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384767" alt="" title="" loading="lazy"/></p><p>红蓝两种粒子在屏幕上相互追逐、碰撞、吞噬。拖动滑块，就能调整病毒复制率、免疫细胞数量。</p><p>对于Kimi K2 Thinking真实表现，你怎么看？</p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=vSqGOpxejLcMtZluh4bsGg%3D%3D.3IaIz4%2FobyA7%2FYDDIxOGCY0vGD6T2rUL9YsGEJrUzlX11jR6rrWJG8%2Bj48MrxPhk2WSsG99aGVxvQeLrAtCVeA%3D%3D" rel="nofollow" target="_blank">https://www.interconnects.ai/...</a></p><p><a href="https://link.segmentfault.com/?enc=d%2FDk78Xd5zEtn49rjvmf1w%3D%3D.Plg9BXS4zLF%2BzM1skzTGbxiczarPUgmit4TdEjORXLw%3D" rel="nofollow" target="_blank">https://twitter.com/Kimi</a>\_Moonshot/status/1986449512538513505</p>]]></description></item><item>    <title><![CDATA[终结Transformer统治！清华姚班]]></title>    <link>https://segmentfault.com/a/1190000047384725</link>    <guid>https://segmentfault.com/a/1190000047384725</guid>    <pubDate>2025-11-10 11:09:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：元宇</p><p>【新智元导读】大模型「灾难性遗忘」问题或将迎来突破。近日，NeurIPS 2025收录了谷歌研究院的一篇论文，其中提出一种全新的「嵌套学习（Nested Learning）」架构。实验中基于该框架的「Hope」模型在语言建模与长上下文记忆任务中超越Transformer模型，这意味着大模型正迈向具备自我改进能力的新阶段。</p><p>「灾难性遗忘」，是神经网络最根深蒂固的毛病之一，比如：</p><p><strong>·</strong> 刚学会减法，就忘记了以前学到的加法；</p><p><strong>·</strong> 切换到一个新游戏，模型在前一游戏的得分就会掉到随机水平；</p><p><strong>·</strong> 微调大模型，常出现「风格漂移」与「旧知识遗忘」现象</p><p>……</p><p>它的存在，使得大模型难以像人类那样持续学习。</p><p>在过去十年中，得益于强大的神经网络结构及其训练算法，机器学习取得了惊人的进步。</p><p>但「灾难性遗忘」的老毛病并没有被根治。</p><p>为破解这一难题，来自谷歌的研究人员提出了一种持续学习的全新范式——嵌套学习（Nested Learning），并且已被NeurIPS 2025接收。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384727" alt="" title=""/></p><p>论文地址：<a href="https://link.segmentfault.com/?enc=E%2FsxSV0h%2FYMAdlsSN0RvDQ%3D%3D.KbpfxOdR4xBE8sKhTtS74r%2BWcbpTQooCQjcTnFR%2BqzTafKyIEtmhsJ69pykLZUef" rel="nofollow" target="_blank">https://abehrouz.github.io/fi...</a></p><p>「嵌套学习」将模型视为一系列更小的、相互嵌套的优化问题，每个问题都有其独立的内部工作流程。</p><p>这样的设计旨在缓解甚至完全避免大模型的「灾难性遗忘」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384728" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384729" alt="" title="" loading="lazy"/></p><p><strong>破解「灾难性遗忘」根源</strong></p><p>在「持续学习」与「自我改进」方面，人类大脑无疑是黄金标准。</p><p>它通过「神经可塑性」不断重构自身结构，以应对新的经验、记忆与学习任务。</p><p>缺乏这种能力的人，会陷入类似「顺行性遗忘」的状态——只能依赖即时情境而无法积累知识。</p><p>当前的大模型同样存在类似局限：</p><p>它们的知识要么局限于输入窗口的即时上下文，要么被固定在预训练阶段学到的静态信息中。</p><p>这正是大模型出现「灾难性遗忘」的根源——在学习新任务时会牺牲对旧任务的掌握能力。</p><p>这也是长期困扰机器学习的核心问题。</p><p>简单地不断用新数据更新模型参数的方法，往往会导致「灾难性遗忘」。</p><p>研究者通常通过修改网络结构（Architecture Tweaks）或优化算法（Optimization Rules）来缓解这种问题。</p><p>然而这样做，长期存在一个误区：我们一直将模型结构（网络架构）与优化算法视作两个独立的部分。</p><p>这阻碍了统一且高效学习系统的构建。</p><p>在论文中，研究人员提出了「嵌套学习」，打破了结构与算法的界限，以弥合二者之间的鸿沟。</p><p>也就是说「嵌套学习」不再将机器学习模型视作一种单一、连续的过程，而是一个由多层相互关联的优化问题组成的系统，这些问题同时进行优化。</p><p>研究人员认为，「模型结构」与「训练规则」本质上是同一概念，只是处于不同的「优化层级」上，每个层级都有独立的信息流动与更新速率。</p><p>通过识别这种内在结构，使得我们能够构建更深层的学习组件，从而解决像「灾难性遗忘」这类长期难题。</p><p>为了验证这一理论假设，研究人员提出了一个概念验证型的自我修正架构，命名为「Hope（希望）」。</p><p>该模型在语言建模任务中表现出色，并在长上下文记忆管理上优于当前最先进的模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384730" alt="" title="" loading="lazy"/></p><p><strong>嵌套学习的新范式</strong></p><p>在嵌套学习的框架下，一个复杂的机器学习模型，是由多个一致且相互连接的优化问题组成的系统。</p><p>这些优化问题可以是层层嵌套的，也可以并行运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384731" alt="" title="" loading="lazy"/></p><p>每个内部优化子问题，都有自己独立的信息，即其学习所依赖的信息集合。</p><p>这一视角意味着：现有的深度学习方法，从本质上是在压缩其内部信息流。</p><p>嵌套学习允许我们设计出具备更深计算深度的学习组件。</p><p>为了说明这一范式，研究人员以「联想记忆」为例，这是一种能够通过一个刺激唤起另一个记忆的能力，就像我们看到一张脸就想起一个名字。</p><p>研究人员推论，在训练过程中，尤其是「反向传播」阶段，可以被建模为一种联想记忆。该模型学习将数据点映射到其对应的局部误差值，该局部误差值衡量了该数据点的「惊奇度」或「意外性」。</p><p>根据前人研究，研究人员发现关键的网络结构组件，比如Transformer模型的「注意力机制」，也可形式化为简单的联想记忆模块，用于学习序列中各个token之间的映射关系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384732" alt="" title="" loading="lazy"/></p><p>如上图所示，大脑中统一且可重用的结构以及多时间尺度的更新是人类持续学习的关键组成部分。</p><p>嵌套学习允许大脑的每个组成部分进行多时间尺度的更新，同时表明诸如Transformer等众所周知的架构，实际上是具有不同更新频率的线性层。</p><p>嵌套学习模型中的统一结构与多频率更新机制，与人脑的神经波动与神经可塑性结构十分相似。</p><p>它使我们能够在人工模型中定义「多时间尺度更新」：每个学习组件可在不同频率下更新参数。</p><p>通过定义「更新频率」，即每个组件参数被调整的频率，我们可以将这些优化问题组织成有序的层级结构。</p><p>这一结构正是嵌套学习范式的核心。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384733" alt="" title="" loading="lazy"/></p><p><strong>嵌套学习的应用</strong></p><p>嵌套学习视角，为我们提供了改进现有算法和架构的原则性方法：</p><p><strong>深度优化器（Deep Optimizers）</strong></p><p>嵌套学习将优化器（Optimizer，例如基于动量的优化器）视为一种联想记忆模块。</p><p>这使我们能从联想记忆的角度重新设计优化算法。</p><p>传统优化器通常依赖「点积相似度」来衡量样本间相似性，但这一机制无法考虑样本间的复杂关系。</p><p>通过将优化目标改为标准损失函数，研究人员提出了新的动量更新公式，使优化过程对不完美数据更具鲁棒性。</p><p><strong>连续记忆系统（Continuum Memory Systems）</strong></p><p>在标准的Transformer模型中，序列建模层承担短期记忆，保存当前上下文信息；而前馈神经网络层则充当长期记忆，存储训练前的知识。</p><p>嵌套学习扩展了这一思想，提出「连续记忆系统」，这是一种由多个模块组成的记忆谱系，每个模块以不同频率进行更新，这为持续学习创建了一个更加丰富、高效的记忆系统。</p><h2>Hope：具备连续记忆的自我修正架构</h2><p>研究人员基于以上嵌套学习原则设计了「Hope」模型，它是「Titans架构」的改进版本。</p><p>Titans架构是一种长期记忆机制，会根据记忆的「惊奇度」来优先保留信息。</p><p>但它仅支持两层参数更新，因此只能实现一阶的「上下文内学习」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384734" alt="" title="" loading="lazy"/></p><p>上图比较了Hope与Transformers的架构主干。</p><p>相比之下，Hope是一种可自我修改的递归架构，能实现无限层级的上下文内学习。</p><p>它还结合了连续记忆系统（CMS），能够扩展到更大的上下文窗口。</p><p>换言之，Hope可以通过自指过程优化自身记忆，形成具有无限嵌套学习层级的架构。</p><p>研究人员进行了多组实验，来评估深度优化器与Hope架构在语言建模、长上下文推理、持续学习及知识整合等任务上的表现。</p><p>实验结果显示：</p><p>在常用的语言建模与常识推理任务上，Hope相较现代递归模型与标准Transformer模型展现出更低的困惑度与更高的准确率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384735" alt="" title="" loading="lazy"/></p><p>在长上下文任务中，Hope与Titans模型均显著优于TTT与Mamba2，证明连续记忆系统能更高效地处理超长序列信息。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384736" alt="" title="" loading="lazy"/></p><p>Hope框架在标准基准上表现优于现有模型，印证了当架构与算法被统一后，学习系统可以变得更具表现力、更高效、更具自我改进能力。</p><p>这意味着，我们对深度学习的理解迈出了新的一步。</p><p>通过将「模型结构」与「优化过程」统一为一个连贯的、层层嵌套的优化系统，Hope框架为模型设计提供了一种新范式。</p><p>这一发现，为弥合当前大模型遗忘特性与人脑持续学习能力之间的差距奠定了坚实基础，或许将有助于破解大模型「灾难性遗忘」的根源性问题。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384737" alt="" title="" loading="lazy"/></p><p><strong>作者介绍</strong></p><p><strong>Peilin Zhong</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384738" alt="" title="" loading="lazy"/></p><p>Peilin Zhong</p><p>Peilin Zhong是谷歌纽约（Google NYC）算法与优化团队的一名研究科学家，该团队由Vahab Mirrokni领导。</p><p>他的博士毕业于哥伦比亚大学，师从Alex Andoni、Cliff Stein及Mihalis Yannakakis教授，本科毕业于清华大学交叉信息研究院（姚班）。</p><p>Peilin Zhong致力于理论计算机科学，尤其侧重于算法的设计与分析。他的具体研究方向有并行与大规模并行算法、Sketching算法、流式算法、图算法、机器学习、高维几何、度量嵌入等。</p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=z5bs27xJx9YVesRlxt%2B50g%3D%3D.1bswuJGW1hXHGQW7WIWVMsHdDNO7%2BnWE3CTMCmhVRaFGjf71g3Nx9p89TQRvFzZCwzIaq01pADaSIa%2Fze16yY9nqkTCzW7cjD9JA2SUueCFpV%2BsvwBPouGi4PV1Xf4x7YV1mEqkh%2FBsZ2I4YL0VnAA%3D%3D" rel="nofollow" target="_blank">https://research.google/blog/...</a></p>]]></description></item><item>    <title><![CDATA[藤校拒了又怎样？18岁天才少年打造爆款A]]></title>    <link>https://segmentfault.com/a/1190000047384706</link>    <guid>https://segmentfault.com/a/1190000047384706</guid>    <pubDate>2025-11-10 11:08:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：元宇</p><p>【新智元导读】Cal AI联合创始人Zach Yadegari自7岁起学习编程，16岁卖出自己首个应用赚得近10万美元，并与另外一名高中生联合创办了一家年营收达3000万美元的AI应用公司。在被常春藤盟校拒绝后，Yadegari选择进入迈阿密大学。Yadegari认为AI时代会出现更多年轻的创业者，他给出的最重要的一条创业建议就是：立刻行动。</p><p>7岁时，参加编程夏令营并开始对编程萌生兴趣，随后通过Google、YouTube等互联网资源开始自学如何制作游戏和应用。</p><p>16岁时，卖掉自己的第一个应用，赚得将近10万美元。</p><p>看到电影《社交网络》中马克·扎克伯格的创业故事，便拉着另一位高中生共同打造了一款追踪卡路里的AI应用，年营收3000万美元。</p><p>这位天才创业少年，便是Cal AI的联合创始人Zach Yadegari。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384708" alt="" title=""/></p><p>Zach Yadegari在高中时创立了Cal AI</p><p>一位创投圈的网友vas曾发推表示：「恕我直言，不存在15岁以下的创业者」。</p><p>刷到这篇推文的Yadegari霸气回怼：</p><p>「13 岁时，我有一个游戏网站，每年能赚6万美元」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384709" alt="" title="" loading="lazy"/></p><p>不同于那些在「氛围编程」陪伴下长大的青少年程序员，Yadegari自称在初中时就已经开始掌握Python和C#了，并且在大约13岁时创建了自己的第一个应用，一个名为「Totally Science」的游戏网站。</p><p>16岁时，Yadegari以将近10万美元的价格将Totally Science卖给了另一家游戏公司FreezeNova，他用这笔钱与几位联合创始人一起成立了Cal AI。</p><p>据Cal AI官网显示，该应用被500万用户打出了接近5星的好评。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384710" alt="" title="" loading="lazy"/></p><p>在Apple App Store上Cal AI的评分为4.8分，在Google Play上评分为4.6，有超过1百万次下载。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384711" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384712" alt="" title="" loading="lazy"/></p><p>不难看出，这是一款相当成功的AI应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384713" alt="" title="" loading="lazy"/></p><p><strong>用AI追踪卡路里</strong></p><p><strong>年入3000万美金</strong></p><p>「只需一张照片，即可追踪卡路里」。</p><p>这是Cal AI在官网上打出的旗号。</p><p>Yadegari表示，他所开发的每一个应用或游戏，都是为了解决自己生活中的问题。</p><p>Cal AI也不例外。</p><p>「我小时候很瘦，为了增重开始去健身房，但很快发现真正的关键在于饮食。于是，我和我的联合创始人决定打造一款AI驱动的卡路里追踪应用。」</p><p>创业动机说起来似乎有点搞笑，是为了给女孩子们留下好印象。</p><p>2023年夏天Yadegari拉上另一位Cal AI的联合创始人Henry，两个高中生开始从零开始一起做应用。</p><p>在这个过程中，他们主动向几位成功的应用创业者寻求指导，其中的一位就是后来成为新联合创始人的Blake Anderson。</p><p>Anderson比Yadegari年长几岁，此前具有打造数百万下载量应用的成功经验。</p><p>产品上线几个月后，Yadegari和Henry等人搬去了旧金山专注研发，开始在陌生的城市从零开始组建团队。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384714" alt="" title="" loading="lazy"/></p><p>上图为Cal AI创始团队成员，右下为Jake Castillo，右上为Blake Anderson，左上为Henry Langmack，左下为Zach Yadegari</p><p>Yadegari在X平台上描述了创始团队人员的分工：</p><p>他一直都是Cal AI的CEO，Henry担任CTO，他和Henry、Blake一起打造了应用；当时的Blake正在开发Umax，所以兼职负责传播策略。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384715" alt="" title="" loading="lazy"/></p><p>他们找到一些网红合作拍视频，在这一宣传策略下新应用取得了超乎想象的增长速度。</p><p>Yadegari迅速意识到其中非同寻常的机会，并为此押上了自己的全部「家底」。Cal AI应用迅速起飞，发展到如今30人团队、年营收约3000万美元的规模。</p><p>在这个过程中，Yadegari和创始团队开始意识到：他们打造的产品，不止是一个短时的爆款产品，还可能成为一个持续扩展的事业。</p><p>现在， Yadegari经常会看到身边有人在手机上展示Cal AI，这会带给他一种无与伦比的骄傲。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384716" alt="" title="" loading="lazy"/></p><p><strong>7岁学编程</strong></p><p><strong>靠YouTube自学成才</strong></p><p>在Yadegari 7岁时，他的父母送他去编程夏令营，这是他最早接触编程的开始。</p><p>虽然那次夏令营经历并没有让他学到太多的知识，但却成功激发起他对编程的兴趣，为他打开了一个全新的世界。</p><p>随后，Yadegari靠YouTube自学成才。</p><p>每天他都会花几小时看视频学别人编写游戏，自己尝试复刻并加入改动，这个过程让他受益良多。</p><p>一个转折点来自于电影《社交网络》。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384717" alt="" title="" loading="lazy"/></p><p>这部以马克·扎克伯格创办Facebook（Meta前身）为故事原型的电影，给了Yadegari最大的创业灵感，让他完成了从「做游戏」向「做产品」的重大飞跃。</p><p>也许是从这一时刻起，他开始从一个技术极客，逐渐成长为一位矢志打造成功产品的创业者。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384718" alt="" title="" loading="lazy"/></p><p><strong>天生的创业者，自甘被上大学「耽误」</strong></p><p>Yadegari似乎是一个天生的创业者。</p><p>他认为自己并不是一个非常特别的孩子，他的成绩不错，和其他人一样也有自己的社交生活。</p><p>唯一不同的是：</p><p>每天放学后，他都会花好几个小时做各种项目；甚至上课时，也常常在构思新的创意。</p><p>他似乎对创业有着与生俱来的兴趣，而不是像更多的孩子一样只是被动地按照家长和社会安排的固定路线成长。</p><p>在创办Cal AI时，Yadegari整天与二十多岁或三十多岁创业者一起摸爬滚打，他的心中始终有个「大学梦」挥之不去。</p><p>如果上了大学，生活会怎样？</p><p>他感觉到自己内心深处希望上大学，而不是成为那种典型的硅谷辍学者。</p><p>虽然拥有丰富的创业背景、GPA满分以及34分（接近满分）的ACT成绩，但Yadegari仍然收到超过十所常青藤盟校等知名学校的拒绝信，甚至连以「创业圣地」著称的斯坦福大学也未录取他。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384719" alt="" title="" loading="lazy"/></p><p>Yadegari引以为傲的创业经历惨遭顶级名校无视，最后佐治亚理工学院、迈阿密大学和德克萨斯大学录取了他。</p><p>Yadegari选择了迈阿密大学，并不是冲着学术声望，而是这里的社交氛围。</p><p>如果进不了学术上最好的大学，就要尽量上社交氛围最好的大学。</p><p>从这一点可以看出Yadegari个人决策的风格。</p><p>也有网友质疑Yadegari，认为他上大学纯粹是浪费时间，而且硅谷也不乏大学生辍学创业的故事。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384720" alt="" title="" loading="lazy"/></p><p>但Yadegari直言不讳，自己上大学主要的目的是为了结识人脉，「是为融入社交圈而暂时做出的权衡」。</p><p>在Yadegari看来，社交是大学中最占时间的，他对此的评价是「虽然有趣，但生产力并不高」。</p><p>尽管大多数人认为他的做法不值得，但Yadegari似乎更看重上大学带给他的人生体验和情绪价值：「我玩得很开心，我认为这对我来说是值得的……为了创造回忆，那将是值得的」。</p><p>但他也表示一旦这些变得不值得，他也会停止。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384721" alt="" title="" loading="lazy"/></p><p><strong>现在是创业的最佳时机</strong></p><p>入读迈阿密大学后的Yadegari仍在利用课余时间负责Cal AI，但已不需要再参与日常事务。</p><p>他表示公司已经建立了完善的流程，目前主要负责制定愿景和方向。</p><p>如今，寻找志同道合的人已成为Yadegari当前最大的挑战，已经历多次创业的他有着远超同龄人的成熟。</p><p>他不太愿意与人聊课程，更想谈现实世界的问题、解决方案、商业模式等，而这些往往又不是他这个年龄段人们所感兴趣的东西。</p><p>Yadegari认为，现在创业比以往任何时候都更容易。</p><p>就在前不久，OpenAI创始人兼CEO奥特曼在OpenAI的DevDay上也表达过相似的观点：</p><p>「我羡慕当今20岁的辍学者，因为他们现在能创造的东西太多了，AI领域的机遇非常广阔。」</p><p>奥特曼在与Yadegari相仿的年龄从斯坦福大学辍学创立了Loopt，后来又领导了Y Combinator和OpenAI。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384722" alt="" title="" loading="lazy"/></p><p>OpenAI创始人兼CEOSam Altman</p><p>「新闻里那些越来越年轻的创业者不是偶然」，Yadegari认为创业成功的故事将趋于年轻化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384723" alt="" title="" loading="lazy"/></p><p>「创业没有完美的时机，只有愿不愿意迈出第一步。」</p><p>早点开始，敢于行动。别理那些噪音，别信那些说你太年轻做不成的人，也不要让别人替你规划人生！</p><p>这是Yadegari为新手建议的第一条创业军规。</p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=W1NGSRzR%2FXpW5OhPxEJtmg%3D%3D.9py7Ee147tZWbBEEcAEGAZVstGjaAJKwib6kPLIkHHMPdMcQGePqtG%2B%2Bk%2B1aSvmM3DBvbN%2Biti%2BWeyU2YmDOFniVEXL98YHv923GmG3MoAVE8wdE4b%2Fg1rRbFtq5vXSE8k6bqttCiGkzjh2kQRrTAqAoaE3Ul6a979%2Ffs1GUuuw%3D" rel="nofollow" target="_blank">https://techcrunch.com/2025/0...</a></p><p><a href="https://link.segmentfault.com/?enc=Jhl%2Fbn6arkCbKLWnnCmGvw%3D%3D.p5gXdkmqFkFmpGHL3mjXKjSDdWeIFyU0A9yBokQYPDsAUbcd4NUW%2B%2BAuHFXrZLPk7ARnDxZthe%2FhRUmyuArH0JPIrr6pqup80ZGxw3usC63rIPVFPUf43xzFXc%2BPAx4RGMgo%2FDjAoKPPhWXHs%2B6MlA%3D%3D" rel="nofollow" target="_blank">https://fortune.com/2025/10/1...</a></p>]]></description></item><item>    <title><![CDATA[谷歌二代Nano Banana爆出！一键]]></title>    <link>https://segmentfault.com/a/1190000047384646</link>    <guid>https://segmentfault.com/a/1190000047384646</guid>    <pubDate>2025-11-10 11:08:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：KingHZ 桃子</p><p>【新智元导读】就在今天，谷歌Nano Banana 2预览版闪现第三方平台，生成速度飙到10秒、画质拉到4K。网友实测炸锅，一句话直出OS+UI复杂界面，还能在黑板上一键推导微积分。真正的「PS终结者」即将上线。</p><p>Nano Banana 2，下一个备受期待的「AI图像王者」，离正式发布不远了。</p><p>今天，全网发现谷歌下一代Nano Banana，已在第三方平台Media IO上现身。</p><p>目前，还只是一个预览版本，爆料人预测NB2将在本月中下旬发布。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384648" alt="" title=""/></p><p>传送门：<a href="https://link.segmentfault.com/?enc=cS5CWjTIbR8A9np2WGHI7A%3D%3D.XNmTw%2FyM0ICiEzWVk3ooyTfkkV4rOXOFtt7vVZivz7IGxyQDmsTQi%2BEtqxAhLUaR" rel="nofollow" target="_blank">https://www.media.io/ai/text-...</a></p><p>两个月前，初代Nano Banana发布后，因出色图像生成+编辑能力，在全世界掀起一场创意狂欢。</p><p>从真人OOTD、3D手办，到铅笔素描、建筑多维视图，网友们大开脑洞，玩转视觉创作的无限可能。</p><p>如今，从网友首测来看，Nano Banana 2更加出色——</p><ul><li>分辨率：原生2K，<strong>可选4K超分</strong></li><li>生成速度更快：复杂场景仅需10秒</li><li>文字渲染更锐利，提示词响应更精准</li></ul><p>一句话就能让它在黑板上，直接推导出「√2是无理数」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384649" alt="" title="" loading="lazy"/></p><p>在人物生成上，其高度一致性再次树立新标杆，精准捕捉、还原人物的特征。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384650" alt="" title="" loading="lazy"/></p><p>同样，基于纯文本生成Windows 11桌面+YouTube博主的主页，Nano Banana 2细节表现堪称疯狂。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384651" alt="" title="" loading="lazy"/></p><p>总的来说，新一代Nano Banana 2在文本渲染、信息图表、世界知识、图表、指令遵循方面表现非常出色。</p><p>虽未正式发布，NB2一大波惊艳实测席卷全网。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384652" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384653" alt="" title="" loading="lazy"/></p><p><strong>攻克微积分，一张图解题</strong></p><p>在Reddit上，网友发现了Nano-banana 2生成的一些参考图像。</p><p>只要输入积分问题的图片，Nano-banana 2就能在白板上解决并提供全部步骤！</p><p>虽然不知道到底如何做到的，但他之前测试过Nano-banana 2，可以肯定的确是真的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384654" alt="" title="" loading="lazy"/></p><p>这让Correct\_Mistake2640顿生出一种「活久见」的历史感，他还记得1999年他读大学时，微积分老师告诉他，计算机永远不会做积分，所以为了未来应该学习积分。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384655" alt="" title="" loading="lazy"/></p><p>事实上，Mathematica至少在2005年就能求解部分积分问题，而1988年Mathematica第一版已经问世了。</p><p>虽然Nano-banana 2有一些轻微的错误，但对于外行而言，图像模型能如此有逻辑地完成任务，就像是魔法。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384656" alt="" title="" loading="lazy"/></p><p>Nano-banana 2就像是AI版的达芬奇：一个「画家」也能解决大学数学问题。</p><p>比如，下面的高阶微分问题，看起来比2.5 Pro的文本墙舒服多了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384657" alt="" title="" loading="lazy"/></p><p>总之，这次大概率Nano-banana 2真来了——</p><p>Reddit网友马上要证明这是不是真的！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384658" alt="" title="" loading="lazy"/></p><p>让Nano Banana 2生成一幅地中海周边地区的老式地图，旧羊皮纸质感很复古。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384659" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384660" alt="" title="" loading="lazy"/></p><p><strong>一句话，直出OS+UI网页</strong></p><p>如开篇案例实操类似，Nano Banana 2基于纯文本，直接生成Windows桌面+网页浏览器Edge。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384661" alt="" title="" loading="lazy"/></p><p>甚至，网页浏览器中的Gemini 3.0主页也是生成的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384662" alt="" title="" loading="lazy"/></p><p>有网友表示，在看到这张图的一瞬间，还上网搜索Gemini 3.0主页是否真的存在。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384663" alt="" title="" loading="lazy"/></p><p>还有人点评道——</p><p>浏览器、桌面、窗口，所有的一切都是一张「生成的图像」。</p><p>换句话说， UI和OS全部整合在一起，进入了一键生成的时代。这已经不是「图像」的领域了。</p><p>YouTube博主WorldofAI实测后，直言NB2就是PS的终结者。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384664" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384665" alt="" title="" loading="lazy"/></p><p><strong>逼真人物生成，肉眼无法识别</strong></p><p>Nano Banana 2在人物角色生成方面，效果更加逼真了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384666" alt="" title="" loading="lazy"/></p><p>如下同一个提示下，两代NB效果的对比，NB2的视角多了一层。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384667" alt="" title="" loading="lazy"/></p><p>左：Nano Banana 2；右：Nano Banana</p><p>在飞机中摆拍的效果，NB2人物表现更加真实。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384668" alt="" title="" loading="lazy"/></p><p>左：Nano Banana 2；右：Nano Banana</p><p>二次元Cosplay，完全看不出是AI生成的效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384669" alt="" title="" loading="lazy"/></p><p>用AI模拟手机照相效果，绝对难分真假、眼见不一定为实：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384670" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384671" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384672" alt="" title="" loading="lazy"/></p><p><strong>二次元封神，怼脸眼神杀暴击</strong></p><p>在二次元生成上，Nano Banana 2又开启了一片新天地。</p><p>假设让它生成一个男主Sung Jin-Woo，双手拿着两把冒蓝光的匕首往前猛冲，又带有蓝色光效的图片。</p><p>指令中，要求的重点怼脸拍，还有专注凶狠的表情，NB2都给足了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384673" alt="" title="" loading="lazy"/></p><p>prompt：A dynamic, low-angle action shot of Sung Jin-Woo leaping forward, dual-wielding his glowing blue daggers (like the ‘Demon King’s Daggers’). He is a blur of motion, with energy trails following his blades. The background is a dark, stylized dungeon interior. Focus on the intense, focused expression on his face. Style: high-contrast, anime, action.</p><p>大雪纷飞，《东京食尸鬼》金木研抱着朋友，一步步往前走，氛围感拉满。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384674" alt="" title="" loading="lazy"/></p><p>prompt：Ken Kaneki carrying his friend in his arms in the snow, Tokyo Ghoul</p><p>更绝的是，NB2还能保持高度的角色一致性。</p><p>上传一张背影图，让它P个转身，结果二次元女主头发和配饰高度还原。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384675" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384676" alt="" title="" loading="lazy"/></p><p>日漫《航海王》Kaido，NB2也是手拿把捏。尤其是，和NB1对比，效果更加明显。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384677" alt="" title="" loading="lazy"/></p><p>左：Nano Banana 2；右：Nano Banana</p><p>治愈系风格的多人场景，书架、手作甜点、黑板菜单、慵懒猫咪等，各种小细节让人着迷。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384678" alt="" title="" loading="lazy"/></p><p>从吉卜力系，到热血少年，Nano Banana 2真的可以做到，纯文本直出动漫。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384679" alt="" title="" loading="lazy"/></p><p>赛博朋克风格也没问题，比如身处未来高科技环境中的赛博朋克机器人黑客，正面对多块显示器进行操作。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384680" alt="" title="" loading="lazy"/></p><p>prompt：Cyberpunk hacker robot working in front of many monitors</p><p>还有网友生成卡通风格的开局国际象棋棋盘：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384681" alt="" title="" loading="lazy"/></p><p>A cartoon image of a chess board, with the starting position on the board.</p><p>目前还并非完美，但如此高质量的生成结果，已让试用过的网友感到非常非常惊讶。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384682" alt="" title="" loading="lazy"/></p><p><strong>手写体堪比真人，伪造监控录像</strong></p><p>没想到，Nano Banana 2可以在一张图中，一次性通过了「时钟」和「满杯红酒」的测试。</p><p>AI终于能看清楚自己在画什么了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384683" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384684" alt="" title="" loading="lazy"/></p><p>黑板写字，手写体风格几乎难以让人辨认是AI生成的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384685" alt="" title="" loading="lazy"/></p><p>还有阿拉伯语的手写体，同样令人印象深刻。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384686" alt="" title="" loading="lazy"/></p><p>更惊艳的是，它还能生成监控录像图，和马斯克同框也是这么简单。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384687" alt="" title="" loading="lazy"/></p><p>有网友脑洞大开，构思了一个玻璃汉堡，反光、质感、透明度，全都绝了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384688" alt="" title="" loading="lazy"/></p><p>a burger made of glass, reflections, textures, transparency, all spot on.</p><p>让整片海洋完美地变成粉色，准确地在每个涟漪上反射光线。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384689" alt="" title="" loading="lazy"/></p><p>精准度简直难以置信。</p><p>还有美国网友用它PS了自己的车，惊呼「几乎完美」，绝对是顶级水平。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384690" alt="" title="" loading="lazy"/></p><p>Paint car with really nice airbrush ofpresidentsof US and text about them</p><p>NB2还能一键直出奔驰汽车宣传图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384691" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384692" alt="" title="" loading="lazy"/></p><p><strong>Nano Banana升级前后对比</strong></p><p>Nano Banana 2真强！</p><p>Kotovskiy找到了一个很有趣的例子，Nano Banana 2的表现远超上一代。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384693" alt="" title="" loading="lazy"/></p><p>Prompt：Danganronpa court with all characters, cinematic official art</p><p>任务要求《弹丸论破系列》全体角色出庭的场景，保持官方风格的电影感插画。</p><p>这是新版本的生成的结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384694" alt="" title="" loading="lazy"/></p><p>这是旧版的：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384695" alt="" title="" loading="lazy"/></p><p>在抽象艺术上，Fix发了多张Nano Banana新旧版的对比图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384696" alt="" title="" loading="lazy"/></p><p>抽象是真抽象，一起看一下吧：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384697" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384698" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384699" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384700" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384701" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384702" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384703" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384704" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384705" alt="" title="" loading="lazy"/></p><p>截止发稿前，Media IO平台上NB2入口关闭。接下来，就是坐等谷歌的发布了。</p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=7Pvn1ex6CYCL6rXD0K0GrA%3D%3D.UkByhKd1TFcsxL8VyLlmqZDhrdQkN%2BP%2BKVCoGZxdzBs9ls15M8IbaL%2FxIFZsD3405VCVWiDg1cablZOz7CjFLw%3D%3D" rel="nofollow" target="_blank">https://twitter.com/AiTesty5/...</a></p><p><a href="https://link.segmentfault.com/?enc=QMFEP8GoKocTVpFCC74CWg%3D%3D.DP1D%2F6sU43Z7%2BmT%2FqbYEU4k3E70bOJQfQt%2Fl%2Bj7YgMQ17mCIBMdQpM0C9EobZd4AqPotyUJ3sbALQzNSN4laks6VcX%2FQxRV1GLTecXIvf7I%3D" rel="nofollow" target="_blank">https://www.reddit.com/r/sing...</a>\_2\_is\_available\_on\_medioio</p><p><a href="https://link.segmentfault.com/?enc=pfiMSpDhAqHEWFRkXOYsiw%3D%3D.EPsf0mFfttj8CfDnfjgpfzLrE%2BCL7a8F64vqmdhcI2ejSNxe7g5IhDxFO1ZToExyuU8C9tkh6zAql9zCBUo5VPKXjNdmyHnSOf0OUX4OA6455OIBgNVlW30e7ZZRuxUNthETa7BitK368Zq9R5NOSs5swcujmmMWhMMnhBUxHgU1ccAtd9kWRJuXxDiWvYA8" rel="nofollow" target="_blank">https://www.tomsguide.com/ai/...</a></p>]]></description></item><item>    <title><![CDATA[[Qt学习笔记]Qt5.15.2版本安装]]></title>    <link>https://segmentfault.com/a/1190000047384477</link>    <guid>https://segmentfault.com/a/1190000047384477</guid>    <pubDate>2025-11-10 11:07:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、5.15.2镜像源<br/>Qt5.15是现在用的比较多的一个版本，现在安装一版使用清华镜像源，也有中科大镜像源<br/>中科大镜像源：<br/>Index of/qtproject/<br/><a href="https://link.segmentfault.com/?enc=zGlM5iW3wA0zGVLG1GXepw%3D%3D.sVohUJm2qJZqeRFagJrw4tVhPzPcdHF8igpDkhP0sRZGmwaoFTohE6vg3nSvzLkt" rel="nofollow" target="_blank">http://mirrors.ustc.edu.cn/qtproject/</a><br/>清华镜像源：<br/><a href="https://link.segmentfault.com/?enc=pd4Uu91wkOtThr0tQ1C1nw%3D%3D.IJeLmR4WJ8CSDSn076r9SgKMA%2BGpnbM84j56PA1BoKwjcAJoQsyhhwunUANEjfX%2F" rel="nofollow" target="_blank">https://mirrors.tuna.tsinghua.edu.cn/qt/</a></p><p>Qt5.15.2镜像</p><pre><code>https://mirrors.tuna.tsinghua.edu.cn/qt/online/qtsdkrepository/windows_x86/desktop/qt5_5152/
 
https://mirrors.tuna.tsinghua.edu.cn/qt/online/qtsdkrepository/windows_x86/desktop/qt5_5152_src_doc_examples/
 
https://mirrors.tuna.tsinghua.edu.cn/qt/online/qtsdkrepository/windows_x86/desktop/qt5_5152_wasm/</code></pre><p>主要在安装流程中设置临时资料库，输入上述镜像源，就可以选择对应的5.15.2安装。</p><p>二、组件更新<br/>在安装后需要新增组件，进入卸载/更改界面后，配置步骤一中的镜像源，如果5.15.2没出现对应组件选择和安装，只需像下图勾选Archive，就可以调出历史版本的所有组件<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384479" alt="调出历史版本.png" title="调出历史版本.png"/></p><p>如果出现“无法下载文档”或“网络故障”问题，可以在设置里先清理本地缓存和重启电脑来解决</p>]]></description></item><item>    <title><![CDATA[【URP】Unity[后处理]帕尼尼投影]]></title>    <link>https://segmentfault.com/a/1190000047384481</link>    <guid>https://segmentfault.com/a/1190000047384481</guid>    <pubDate>2025-11-10 11:06:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=j7v7546KtLG7k8iGI1KJVQ%3D%3D.3bk9H3rjBjibRTy1AdhyaWAgXVcq7KrMq7ZWigZxRQ1Io2fQKJzhk%2FBUCUwDoXCCVV7h8fxhMGVgEjcgAcI1aoHW5QMfchP8VJTBZGhwijee3gkpc2G1vqAZBeyJ%2BmDLg%2F3WbzoFMJelItmsK148Dbw0NqJxCqBraT0yyLRNI78xK10%2BbjcYzbTx9RWkA%2FGzINpQKrd0tlzDQSpWtSUHXgD9zMGEww5H1InaDZdZDFI%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong></blockquote><p>Panini Projection是Unity URP后处理中的一种圆柱形投影效果，主要用于大场景中保持透视视图的直线特性。它通过特殊的几何变换使垂直直线和穿过图像中心的放射线保持笔直，从而解决广角镜头产生的畸变问题。</p><h2><strong>核心特性与用途</strong></h2><ul><li>‌<strong>视觉矫正</strong>‌：在建筑可视化或全景拍摄中修正广角畸变，保持垂直线条笔直</li><li>‌<strong>艺术表现</strong>‌：可创造类似鱼眼镜头的夸张透视效果，但比标准圆柱投影更自然</li><li>‌<strong>场景适配</strong>‌：特别适用于需要展示广阔空间但不想产生桶形畸变的场景</li></ul><h2><strong>发展历史</strong></h2><p>该技术源自18世纪意大利画家Panini的透视画法，后由PanoTools项目实现数字化算法。Unity在2019年URP管线中首次引入该效果作为体积后处理组件。</p><h2>原理</h2><p>Panini Projection 是 Unity URP/HDRP 后处理系统中的一种圆柱形投影变换技术，其核心原理是通过数学变换修正广角镜头产生的透视畸变。以下从技术实现到应用场景的完整解析：</p><h3><strong>数学原理</strong></h3><p>采用非线性的坐标变换公式：</p><p>$r=\sqrt{x+y}$</p><p>$x'=x \cdot \frac {sin(r/D)}{r/D}$</p><p>$y'=y \cdot \frac {sin(r/D)}{r/D}$</p><p>其中：</p><ul><li>(x,y) 为原始屏幕坐标</li><li>D 为 Distance 参数控制畸变强度</li><li>r 表示像素到画面中心的距离</li></ul><h3><strong>实现流程</strong></h3><ul><li>‌<strong>坐标归一化</strong>‌：将屏幕坐标转换为[-1,1]范围</li><li>‌<strong>径向计算</strong>‌：计算当前像素到画面中心的距离r</li><li>‌<strong>正弦变换</strong>‌：应用公式进行非线性坐标偏移</li><li>‌<strong>边缘处理</strong>‌：当Crop to Fit启用时，裁剪超出屏幕范围的畸变区域</li></ul><h3><strong>URP 实现示例</strong></h3><pre><code class="csharp">csharp
// 在URP后处理Shader中的核心代码片段
float2 PaniniProjection(float2 uv, float distance) {
    float2 center = uv * 2.0 - 1.0;// 归一化到[-1,1]
    float r = length(center);
    float distortion = (r &gt; 0.001) ? sin(r * distance) / (r * distance) : 1.0;
    return (center * distortion + 1.0) * 0.5;// 还原到[0,1]
}</code></pre><h2><strong>参数作用机制</strong></h2><table><thead><tr><th>参数</th><th>数学影响</th><th>视觉表现</th></tr></thead><tbody><tr><td>Distance</td><td>控制sin函数的输入幅度</td><td>值越大边缘拉伸越明显</td></tr><tr><td>Crop to Fit</td><td>限制输出坐标在[0,1]范围</td><td>自动切除画面黑边</td></tr></tbody></table><p>典型应用场景：</p><ul><li>‌<strong>建筑可视化</strong>‌：Distance=0.3-0.5 保持垂直线条笔直</li><li>‌<strong>艺术风格化</strong>‌：Distance=1.2-1.5 创造鱼眼镜头效果</li><li>‌<strong>全景图展示</strong>‌：配合Crop to Fit=0.8 实现无黑边展示</li></ul><p>该技术通过体积框架(Volume Framework)集成到URP管线中，在Post-processing阶段完成坐标空间变换。实际开发时建议通过AnimationCurve动态调节Distance参数实现镜头过渡效果。</p><h2><strong>参数说明</strong></h2><table><thead><tr><th>参数</th><th>类型</th><th>作用</th><th>典型值</th></tr></thead><tbody><tr><td>Distance</td><td>float(0-1)</td><td>控制投影强度，0为正常透视</td><td>0.5-0.8</td></tr><tr><td>Crop To Fit</td><td>bool</td><td>自动裁剪空白区域</td><td>true</td></tr></tbody></table><h2><strong>实现流程</strong></h2><ul><li>创建Volume对象：GameObject &gt; Volume</li><li>新建Profile：Inspector中Create New Profile</li><li>添加效果：Add Override &gt; Panini Projection</li><li><p>调整参数：</p><ul><li>Distance: 0.65</li><li>Crop To Fit: 启用</li></ul></li></ul><h2><strong>应用示例</strong></h2><h3>‌<strong>建筑展示场景</strong>‌：</h3><ul><li>创建URP项目并导入Post-Processing包</li><li>主相机添加Volume组件</li><li>配置Panini参数Distance=0.7，保持建筑垂直线条笔直</li><li>配合Vignette效果增强视觉焦点</li></ul><h3>‌<strong>全景图查看器</strong>‌：</h3><ul><li>使用360°全景材质</li><li>启用Panini并设置Distance=0.85</li><li>添加Lens Distortion增强沉浸感</li></ul><p>该效果需配合URP 12.0+版本使用，在移动端需注意性能消耗。实际开发中建议通过脚本动态调整Distance参数实现镜头过渡效果</p><hr/><blockquote><a href="https://link.segmentfault.com/?enc=2IHYpB014EokI55%2Fo6CYhg%3D%3D.RbiRbAKfFjfIvyiHnCIYPBFk1JO8a2Qr7iHKYg94g75seS1Sc0y2OgOvEkkPDbZ47FM5lj589%2F7LqHae9XnsE6kH8RjJXlkED5pHUYD%2F95QeGm0Q0qTVXQzsHo%2FwwzNfSt2gaNUQxI2G6BK7pGNBPL9Xi1yHMlMKs6imC96KCFUU2bfa89rby3hun5OMoP1iCwe4SCVeetNCr9BE23wHfxC5ffRJUCK9OxNoZvQYwM0%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[罗永浩数字人斩获世界互联网大会科技大奖，]]></title>    <link>https://segmentfault.com/a/1190000047384519</link>    <guid>https://segmentfault.com/a/1190000047384519</guid>    <pubDate>2025-11-10 11:05:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>此次获奖的罗永浩数字人，并非简单的形象复刻产物。</p><p>使用数字人分身进行直播，该数字人最核心的突破在于“形神兼备”的超写实表现——不仅在面部神态、肢体动作上高度还原罗永浩本人的直播风格，更通过deepseek大模型的深度赋能，精准复刻了其独特的语言逻辑、产品讲解风格甚至临场互动技巧。</p><p>在2025年6月的直播首秀中，数字人罗永浩与助播搭档连续直播6个半小时，完成超8300个动作，调用知识库1.3万次，生成9.7万字讲解内容，创下1300万人次观看、5500万元GMV的行业纪录，部分品类带货量甚至反超真人直播。</p><p>数字人在实时互动上也十分智能，依托deepseek大模型的AI剧本生成和智能互动问答能力，数字人可根据用户评论实时调整讲解重点，解决了传统数字人“机械念稿”的痛点；在成本层面，数字人直播投入仅为真人直播的1/20，却能实现7×24小时“日不落”直播，显著提升投入产出比。从2023年双十一助力国货品牌GMV提升60%，到2024年服务超1万家商家实现50% GMV增量，这些数据早已印证了技术的商业化价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384521" alt="" title=""/></p><p>青否数字人的AI互动方案全新升级，互动增强+主动互动+智能互动+定时互动，行业最强全自动AI互动方案！</p><p>互动增强：升级后的互动增强功能，颠覆传统关键词回复模式。多条关键词关联 1 条回复，系统自动泛化回复内容。</p><p>面对直播间用户一样的问题，主播能每次给出不同的回复，但意思一致。</p><p>主动互动：主动互动无需在后台设置，就能自动精准捕捉直播关键节点。</p><p>主动欢迎用户进入直播间、引导关注和点赞、催单和促单等激发用户弹幕互动和下单，大幅提高互动率和下单率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384522" alt="" title="" loading="lazy"/></p><p>智能互动：依托自定义知识库、直播话术、主播人设以及自研直播小模型，实时回复用户弹幕问题，全面解答用户疑问，回复率 95% 以上</p><p>定时互动：数字人主播定时播报，还支持定时发送文字弹幕。定时播报内容同样自动泛化，避免机械重复。主播可提前规划直播节奏，精准推送关键信息，刺激用户下单。</p><p>罗永浩数字人的获奖，更折射出AI数字人直播赛道的三大核心发展趋势。</p><p>其一，超头主播复刻成为商业化破局关键。此前数字人直播多集中于中小商家的基础带货场景，而罗永浩数字人的成功验证了超写实、强IP属性数字人在流量聚合和转化上的巨大潜力，未来头部IP与AI技术的结合将成为行业竞争焦点。</p><p>其二，技术迭代推动“降本增效”向“提质增效”升级。早期数字人竞争聚焦于成本优势，使用数字人主播实现了从“能播”到“播得好、卖得多”的跨越，智能交互能力成为核心竞争力。</p><p>其三，生态化解决方案取代单一产品竞争。青否数字人不仅提供数字人主播，更整合了智能脚本、智能场控、数据复盘等全链路服务，形成“AI主播+AI大脑+AI直播间”的完整生态，这种一体化能力成为企业胜出的关键。</p><p>对于整个AI数字人行业而言，此次获奖事件的示范意义远超奖项本身。在消费互联网流量见顶的背景下，直播电商作为重要的转化场景，一直面临真人主播成本高、续航能力有限、风格不稳定等痛点，而罗永浩数字人的成功验证了AI技术对这一行业的改造价值。</p><p>罗永浩数字人的成功也为其他科技企业提供了清晰的技术落地路径：以核心IP为切入点，通过全栈式技术能力构建生态壁垒，最终实现技术价值与商业价值的统一。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384523" alt="image67.jpeg" title="image67.jpeg" loading="lazy"/></p><p>未来，随着AI大模型技术的持续迭代和硬件成本的下降，数字人直播有望从电商领域向教育、金融、文旅等更多场景渗透。此次世界互联网大会的认可，不仅是对罗永浩数字人技术的肯定，更是对整个AI数字人商业化赛道的强心剂，标志着一个“人人可拥有数字人主播”的新时代正在加速到来。</p>]]></description></item><item>    <title><![CDATA[被权重出卖的“脏数据”：GPT-oss ]]></title>    <link>https://segmentfault.com/a/1190000047384568</link>    <guid>https://segmentfault.com/a/1190000047384568</guid>    <pubDate>2025-11-10 11:05:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>前段时间，OpenAI 为展示开源诚意，公开了 <strong>GPT-oss 的全部模型参数</strong>。结果没想到，这件事反倒像一次“体检报告公开”。一些开发者顺着权重数据深挖，反向分析出了模型训练阶段“吃进去”的各种素材，结论只能说—— <strong>OpenAI 中文训练数据，可能比我们想象得还要草台一些</strong>。</p><p><img width="723" height="318" referrerpolicy="no-referrer" src="/img/bVdmY12" alt="" title=""/></p><p>这件事最早来自今年 9 月 fi-le 的一篇研究<a href="https://link.segmentfault.com/?enc=iFXL1AyYmtv4rwCH4lMVdg%3D%3D.ob%2FlL2Nisq2XnJya5E2XEx7rYHFzYDMadLOfllS0zbE%3D" rel="nofollow" target="_blank">《GPT-oss 泄露了哪些 OpenAI 的训练数据》</a>，文章作者用一套开源的分析办法，对 GPT-oss 的权重做了完整扫描：</p><p><strong>🔍 第一招：看哪些词“最重”</strong></p><p>模型里面每个 token 都有自己的向量权重。哪些词越“重”（L2 Norm 越大），说明模型越容易被它们激活，也意味着训练集中它们出现得越频繁。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384570" alt="" title="" loading="lazy"/></p><p>结果：  <br/><strong>中文里出现了大量离谱词汇，权重比正常词还高。</strong></p><p>比如日常词汇 “因此”“描述”“设置”“代码”可以理解，但当分析范围扩展到“非 ASCII 标记”（非英语类 token）后，榜单里开始出现大量“不宜展示”的东西。</p><p><img width="723" height="650" referrerpolicy="no-referrer" src="/img/bVdmY2c" alt="" title="" loading="lazy"/></p><p>—— 这些并不是大家会正常去讨论的词，却在模型内部占据了“高权重位置”。</p><p>这意味着什么？</p><p>即便你输入“你好，帮我写个程序”，模型依旧要把这些乱七八糟的 token 全部加载参与推理。</p><p><strong>是的，它“常驻内存”。</strong></p><p><strong>🔍 第二招：直接问模型“你认识这个词吗？”</strong></p><p>把一些敏感、广告、网络黑话投给模型，让它解释含义。模型一旦表现出非常“懂”，说明这些词可能在训练中多次出现过。</p><p><img width="723" height="556" referrerpolicy="no-referrer" src="/img/bVdmY2d" alt="" title="" loading="lazy"/></p><p>测试中，GPT-5 能明确识别某些中文敏感短语，甚至能拆分出汉字来源，虽然回复时比较克制，但能看出来——  <br/><strong>这些词至少在训练集中出现过。</strong></p><p>这种方法在机器学习里叫 <em>Membership Inference</em>，俗称“顺着反应推语料”。</p><p><strong>🔍 第三招：做排行榜，看类别</strong></p><p>研究者把模型识别特别强的 token 做聚类，结果一分组，大致出现几种类型：</p><p>一些是正常中文词汇</p><p>一些是网络热门词</p><p><strong>更多是：广告词、成人站点名、灰色领域词汇……</strong></p><p>尤其是“非 ASCII 高权重 token”榜单，一看真会让人皱眉。</p><p><strong>🔍 第四招：让模型玩网络梗 &amp; 怪词</strong></p><p>研究者故意丢进去一些无意义网络词、恶搞梗，看模型懂不懂。  <br/>结果表明：  <br/><strong>有些词模型懂得离谱地多，说明训练数据里出现得不算少。</strong></p><p><strong>GPT-4o 曾出现同类迹象</strong></p><p><img width="570" height="517" referrerpolicy="no-referrer" src="/img/bVdmY2e" alt="" title="" loading="lazy"/></p><p>事实上，这已经不是第一次有人质疑 OpenAI 模型的中文语料质量了。早些时候有人分析过 GPT-4o 的数据，也揭示出不少类似情况。</p><p>简单来说：</p><p>训练数据里混杂了大量不规范、不健康、不适宜出现在大规模通用模型里的东西。</p><p><strong>🔬 更进一步：跨模型测试“敏感 token 识别度”</strong></p><p>研究者把 GPT-oss、GPT-5、GPT-4o 和 Claude 拿来做对比测试。</p><p>方法是把权重最高的 50 个敏感中文 token 输入模型，让它们判断词义及语言类型。</p><p>结果非常有趣：</p><p>有些模型能非常准确识别</p><p>有些模型直接拒答</p><p>有些模型干脆说“不认识”</p><p>规律是：<strong>越容易识别的 token，越可能在训练语料出现得多；且在</strong> <a href="https://link.segmentfault.com/?enc=S09lL%2BEIXbD3QtQp7tJAQw%3D%3D.1igYk%2BPw89vGIrGbmI4r8f5NKc259MCgG9QmwtZHGYOGg5PXgG2Sd%2B6HECPKd4rg" rel="nofollow" target="_blank"><strong>GitHub</strong></a> <strong>公共仓库里也更容易搜到对应黑名单记录。</strong></p><p>换句话说：<strong>AI 的“知识盲区”和“知识污染”，都藏在训练数据里。</strong></p><p><strong>为什么会这样？</strong></p><p>理论上，模型训练会经过权重衰减，不常出现的词本该“弱化”。</p><p>但如果某些词在训练集中被反复出现（例如抓取 GitHub / 公网爬虫时混进来的广告、灰产词、垃圾站内容），权重就会被异常放大。</p><p>这类中文互联网垃圾内容的比重不算低，因此模型“吃进去多少”，几乎决定了它内部记住多少。</p><p>而且——<strong>越是开源模型，这些痕迹越容易被暴露出来。</strong></p><p><strong>DeepSeek 做得不一样</strong></p><p>作为对照，DeepSeek 在训练阶段做过明确的“脏数据清洗”策略：</p><p>过滤成人内容</p><p>清理广告文本</p><p>删除灰色信息</p><p>进行人工审核</p><p>多级过滤才进入训练集</p><p>这也解释了为什么很多人觉得 DeepSeek 的中文输出比海外模型更“干净”、更本土化一些。</p><p><strong>🧾 最终结论</strong></p><p>GPT-oss 权重公开后，开发者用反向分析方法挖出了中文训练集中的大量异常 token。</p><p>高权重敏感词说明：训练数据里确实混入了不少广告词、垃圾内容、灰色站点信息。</p><p>这些污染可能来自 GitHub 的公开黑名单、爬虫抓取的中文垃圾内容等。</p><p>多模型对敏感 token 的识别能力差异明显，证明不同模型在数据清洗上采取了不同策略。</p><p>相比之下，DeepSeek 在中文语料清洗上更严格，也因此更“干净”。</p>]]></description></item><item>    <title><![CDATA[一文带你全面解读数据治理 数据集成与治理]]></title>    <link>https://segmentfault.com/a/1190000047384582</link>    <guid>https://segmentfault.com/a/1190000047384582</guid>    <pubDate>2025-11-10 11:04:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>先问大家一个问题：<strong>公司的数据现在处于一种什么状态</strong>？</p><ul><li>两年前我干数据工作时，团队总是会出现：开会的时候不同部门因为“客户数量”根本对不上而争吵；</li><li>新来的同事想查找信息，却发现同一个产品叫法有好几个；</li><li>公司想搞个精准营销，却发现客户手机号一堆是空的，结果根本就执行不了。</li></ul><p>听着是不是很熟？ 其实就是因为<strong>没有进行数据治理</strong>。说白了，数据治理可是一项<strong>必须要做的基础性工作</strong>。</p><p>但是要怎么做？怎样做好？有哪些架构？90%的人都说不出来，那么今天我就从这三个方面来讲讲数据治理，看完你就有了一个深入的了解了。</p><h3>一、 为什么要进行数据治理？</h3><p>要知道，数据治理是<strong>为了解决这些实际业务痛点、支撑企业健康发展</strong>。它有以下作用：<br/><img width="692" height="371" referrerpolicy="no-referrer" src="/img/bVdmY4b" alt="image.png" title="image.png"/></p><h4><strong>1.支撑决策</strong></h4><p>如果决策者依据一份错误或不一致的数据报告做决策，很容易把公司带进坑里。<strong>数据治理的首要目标，就是保障数据的准确性和一致性</strong>。让团队提到“用户活跃度”，指的都是同一个概念，算的都是同一套逻辑。这样，决策才能建立在坚实的事实基础上。</p><h4><strong>2.提升效率</strong></h4><p>以前IT同事大量的时间花在哪儿？不是搞创新，而是找数据、洗数据。</p><p>数据治理会<strong>建立起一套清晰的规则和目录</strong>，告诉你数据在哪、叫什么、是什么意思、质量如何。这能极大地解放生产力，让员工能把精力花在更有价值的分析和工作上。</p><p>但是要怎么做最省时省力？这里我建议用专门的<strong>数据集成工具</strong>，比如<strong>FineDataLink</strong>就能设置清洗规则，在“数据管理”点击“清洗规则”就可以一键实现全局清洗，非常方便简单。<br/><img width="692" height="643" referrerpolicy="no-referrer" src="/img/bVdmY4d" alt="image.png" title="image.png" loading="lazy"/></p><h4>3.控制风险</h4><p>随着<strong>《数据安全法》、《个人信息保护法》</strong>等法规的出台，不合规的数据处理方式会让企业面临巨额罚款和声誉损失。这里我们可以在FineDataLink对不同部门不同人员进行权限设置，一旦安全出了问题就能快速定位相关人员。<br/>!<a href="" target="_blank">上传中...</a><br/>要知道，客户信息、财务数据和核心技术资料等，这些都是企业的命脉，要避免核心数据被随意查看、拷贝、泄露。</p><h4>4.释放价值</h4><p>数据治理能将分散、杂乱的数据转变为结构清晰、质量可靠、可供业务<strong>直接使用的有效资产</strong>。我一直强调，数据治理的根本驱动力来自业务需求，而非技术理想。 它的最终目的，是<strong>让业务运行得更顺畅、决策更智能、发展更稳健</strong>。了解了数据治理的必要性，那么这项工作应该如何启动和推进？ 用过来人的经验告诉你，<strong>成功的数据治理必须遵循清晰的路径</strong>，急于求成往往会适得其反。</p><h3>二、 怎么进行数据治理工作？</h3><p>数据治理绝对不是一个单纯的IT项目，它是一场涉及<strong>组织、流程、技术</strong>的企业级变革。想成功，必须讲究科学的方法。</p><h4>1.获得高层支持</h4><p>这是所有工作的起点，你必须争取到公司高层的真正理解和支持。</p><p>要让他们明白，数据治理是一项长期投资，短期内可能看不到成果，但它关乎企业未来的核心竞争力；最好能成立一个由高层监督的数据治理委员会，明确这项工作的战略地位。</p><h4>2.摸清家底，建立规则体系</h4><ul><li><strong>盘点资产</strong>： 先搞清楚手里的数据。把公司各个系统如ERP、CRM、OA等的核心数据资产梳理一遍，形成一份数据资产清单。<br/><img width="692" height="450" referrerpolicy="no-referrer" src="/img/bVdmY4e" alt="image.png" title="image.png" loading="lazy"/></li></ul><p><img width="692" height="532" referrerpolicy="no-referrer" src="/img/bVdmY4f" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>建立标准</strong>： 这是核心。我们需要统一语言。比如，全公司统一叫“客户ID”，而不是叫“客户编号”，也不叫“客户代码”。定义好每个数据字段的含义、格式、来源和责任部门。</li><li><p><strong>明确权责</strong>： 每个重要的数据，都必须有一个负责人。比如产品数据的负责人应是产品管理部门。这个负责人要对这个数据的定义、质量和安全负责，避免部门之间相互甩锅。</p><h4><strong>3.聚焦核心场景</strong></h4><p>千万不要一上来就搞全面治理，不仅费时，问题还是快速没解决，所以<strong>最好是从业务价值最高、痛点最明显的场景入手</strong>。</p></li></ul><p>举个例子：</p><p>对于“客户主数据”，它关系到所有业务部门，同时它出现的问题很多。那么我们把它的标准定好，把重复、错误的客户信息清理干净，建立一个唯一、准确的“客户中心”。你看，这样一来查找和核对数据是不是就很清楚了？</p><h4>4.善于利用工具</h4><p>当规则制定好后，需要借助技术工具来固化和管理。</p><ul><li><strong>数据建模工具</strong>： 帮助我们用统一的标准去设计数据库。</li><li><strong>数据血缘工具</strong>： 当某个数据源出问题时，能快速定位影响哪些报表和业务。</li><li><strong>数据质量平台</strong>： 可以定期自动地对数据进行检测，比如检查关键字段有没有空值、数据是否符合业务规则，并生成质量报告。<br/><img width="692" height="403" referrerpolicy="no-referrer" src="/img/bVdmY4g" alt="image.png" title="image.png" loading="lazy"/></li><li><strong>数据安全平台</strong>： 在公司的各种数据系统中，严格落地权限管理，确保数据不被越权访问。</li></ul><h4>5.员工培训管理</h4><p>数据治理不是一次性的项目，而是<strong>一项需要长期坚持的管理活动</strong>，它需要通过持续的<strong>沟通、培训和宣贯</strong>，让全员都认识到数据的价值，并理解自身在数据管理中的责任。</p><p>当我们明确了行动步骤后，一个支撑所有这些工作的框架性概念就十分重要了。这个框架就是数据治理架构，它确保了各项工作能够协调、有序地开展。</p><h3>三、 数据治理架构怎么开展？</h3><p>这个架构通常包含三个核心部分：<strong>组织架构、制度架构和技术架构</strong>。这三者缺一不可。</p><h4>1. 组织架构：谁来干</h4><p>数据治理不能只靠IT部门单打独斗，必须建立一个层次分明的组织体系。</p><ul><li><strong>决策层</strong>： 由企业高管和业务负责人组成，负责审批战略、裁决争议、提供资源。</li><li><strong>管理层</strong>： 这是一个专职的团队，负责制定具体的管理办法、协调资源、推动项目执行、监督和考核。</li><li><strong>执行层</strong>： 由业务部门指派人员，负责本领域内数据标准的定义、质量监控等具体工作。</li></ul><p>没有清晰的组织架构，所有规则都会难以落地。</p><h4>2. 制度架构：怎么管</h4><p>这是数据治理的“法律体系”，把所有规则和流程书面化、制度化。</p><ul><li><strong>管理办法</strong>：比如《数据标准管理办法》、《数据安全管理办法》，明确各项管理活动的具体要求和流程。</li><li><p><strong>流程和细则</strong>： 这是操作手册，比如“数据标准申请流程”、“数据质量问题处理流程”。</p><h4>3. 技术架构：用什么干</h4><p>技术是让制度高效落地的赋能者。一个典型的数据治理技术架构，会包含以下几层：</p></li><li><strong>数据源层</strong>： 公司里所有的业务系统数据库、文件、日志等。</li><li><strong>数据集成与存储层</strong>： 通过ETL等工具，把数据从各个源头抽取出来，集中到数据仓库或数据湖里。</li><li><p><strong>数据治理核心层</strong>：</p><ul><li><strong>元数据管理</strong>： 包括它在哪、谁创建的、是什么意思、和谁有关系。</li><li><strong>数据标准管理</strong>： 存储和管理我们制定的所有数据标准。</li><li><strong>数据质量管理</strong>： 配置质量规则，自动检测并报告问题。</li><li><strong>数据安全管理</strong>： 管理数据的分类分级、加密、脱敏和访问权限。</li></ul><p><img width="692" height="241" referrerpolicy="no-referrer" src="/img/bVdmY4h" alt="image.png" title="image.png" loading="lazy"/></p></li><li><strong>数据服务与应用层</strong>： 把治理好的、干净的数据，通过API、报表、分析平台等方式，提供给业务人员和使用。</li></ul><p>这个技术架构，本质上就是为我们前面提到的所有工作，提供了一个统一的、自动化的管理平台。</p><h3>总结</h3><p>你看，数据治理本质上是一场关于“秩序”的建设，是<strong>解决我们日常工作中数据冲突、效率低下和决策失准等实际痛点的系统性方法</strong>。</p><p>说白了，数据治理就是要确保数据干净、安全、可信，能够作为决策的依据，这才是我们所追求的数字化转型和智能化升级，同时还是企业能够长久活下去的秘诀。你说对不？</p>]]></description></item><item>    <title><![CDATA[2025 年 AI、机器学习与数据工程趋]]></title>    <link>https://segmentfault.com/a/1190000047384603</link>    <guid>https://segmentfault.com/a/1190000047384603</guid>    <pubDate>2025-11-10 11:03:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文已收录在<a href="https://link.segmentfault.com/?enc=h9RRkrOdN3cnNnR%2BqUSClw%3D%3D.Yr%2BekrQ0wthlFlMtg59KPfeaZcaZtBBhCqG2oFoGzhLcWO3Db5ywOYDeoE2%2FLfSvwXRUYE%2FEZNPO9nmogCtB%2BA%3D%3D" rel="nofollow" target="_blank">Github</a>，<strong>关注我，紧跟本系列专栏文章，咱们下篇再续！</strong></p><ul><li>🚀 魔都架构师 | 全网30W技术追随者</li><li>🔧 大厂分布式系统/数据中台实战专家</li><li>🏆 主导交易系统百万级流量调优 &amp; 车联网平台架构</li><li>🧠 AIGC应用开发先行者 | 区块链落地实践者</li><li>🌍 以技术驱动创新，我们的征途是改变世界！</li><li>👉 实战干货：<a href="https://link.segmentfault.com/?enc=Mw%2Fny8xVKFQuhB4FDo72Ww%3D%3D.dN9EnBIYRl2jrWKDTZE8TUL2JkVlWiKO6zkXC3XR4o4%3D" rel="nofollow" target="_blank">编程严选网</a></li></ul><h2>0 关键要点</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384605" alt="" title=""/></p><ul><li>AI 技术的下一个前沿将是“物理智能”（Physical AI）</li><li>RAG已逐渐商品化，在企业应用中得到广泛采用</li><li><strong>AI 正从“助手”转变为“共同创造者”</strong>。未来不仅是写代码更快，而是 AI 将参与应用的完整生命周期——开发、测试与发布。</li><li><strong>AI 驱动的 DevOps 流程与实践</strong> 成为今年备受关注的热点。</li><li><strong>人机交互（HCI）</strong> 领域强调技术要服务于真实人类需求，设计要融入人们的生活场景。</li><li>新的协议如MCP与A2A将继续推动 AI 客户端与后台系统间的互操作性。</li></ul><p>AI/ML 趋势报告为读者提供人工智能、机器学习与数据工程领域的新兴趋势与技术概览。本报告总结了与多位嘉宾的播客讨论，重点介绍未来 12 个月值得关注趋势。</p><h2>1 AI与机器学习趋势图</h2><p>报告中的核心部分是 <strong>年度趋势图（Trends Graph）</strong>，展示了不同技术从“创新者阶段”到“早期采用者”、“早期大众”的演进路径。</p><p>这一框架基于 Geoffrey Moore 的著作《跨越鸿沟》（<em>Crossing the Chasm</em>）。主要聚焦尚未“跨越鸿沟”的新兴技术领域。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384606" alt="" title="" loading="lazy"/></p><p>自 2022 年 11 月 ChatGPT 发布以来，<strong>生成式 AI</strong> 与LLMs）彻底主导了 AI 技术版图。各大科技巨头持续推出更强的语言模型版本。延续过去几年的爆发式增长，AI 领域在过去一年中出现了大量重大创新。</p><p>与去年趋势报告的对比更新：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384607" alt="" title="" loading="lazy"/></p><p>本文重点介绍趋势图中新增与更新的技术主题，并说明它们在采用曲线中的最新位置。</p><h2>2 创新者阶段</h2><p>今年的“创新者”类别新增了多个前沿主题。</p><h3>2.1 AI Agents</h3><p>今年 AI 代理领域发展迅猛。Anthropic 推出了 <strong>Claude Subagents</strong>，Amazon 发布 <strong>Bedrock Agents</strong>，AI 代理正从执行单一任务进化为能够编排复杂工作流、做出上下文决策的系统。</p><p>其他重要进展包括：</p><ul><li>OpenAI 公布 <strong>通用 ChatGPT Agent</strong>，可在表格与演示类应用中自动执行任务。</li><li>Amazon 开源 <strong>Strands Agents SDK</strong>，用于构建 AI 代理。</li><li>NVIDIA 推出 <a href="https://link.segmentfault.com/?enc=0mXtsPj27uwjaCSTf3jtYg%3D%3D.uQ2Wemu3bCYUK5ZY3VsxYtc0I%2FOkW93ye7Is7i7y7EwXTntYdu%2BBBrxHqpibitW4gXpBegH2qYIExS5mAiTn4svQU3UIzGJ0CT5evXT%2BHS4%3D" rel="nofollow" target="_blank">Visual AI Agents</a>，用于视频分析。</li></ul><p><strong>Daniel Dominguez</strong> 指出：</p><blockquote>我们正从简单的聊天机器人，迈向能帮我们安排会议、更新数据库、启动云资源的 AI 助手。如 Amazon Bedrock Agents 能在不管理基础设施的前提下，基于任意基础模型创建生产级代理，这些代理可安全地链式调用任务与服务，将“代理范式”引入 AWS 生态中，大大缩短从实验到生产的路径。</blockquote><p><strong>Anthony Alford</strong> 补充：</p><blockquote>代理功能强大但也潜藏风险。如能访问文件系统的代理若执行错误命令（如“rm -rf”）可能造成灾难性后果。这些工具极大地提高了生产力，但也放大了安全隐患——当 AI 能访问你的银行账户或删除硬盘时，安全问题比以往更严峻。</blockquote><hr/><h3>2.2 多模态语言模型（Multi-Modal LLMs）</h3><p>语言模型如今具备多模态能力，可同时处理文本、图像、音频与视频等多种数据类型，实现更深层的语义理解与跨模态推理，从而生成更准确、有价值的输出。</p><hr/><h3>2.3 物理智能（Physical AI）</h3><p><strong>物理智能</strong> 是今年 AI 技术格局中的重大突破，即 AI 在机器人和本地设备中的实体化。</p><ul><li><strong>Google</strong> 推出 <a href="https://link.segmentfault.com/?enc=xnkAkrBQIZ3czjIES9sIEA%3D%3D.AusLvKVizJ1KoSITXfx5k%2BJNTJjJjjYxQn%2B8IEfsJa6Re%2FjQt6yjq9mtDwH14fCl" rel="nofollow" target="_blank">Gemma 3n</a>，专为手机、笔记本和平板设计的生成式模型</li><li><strong>Microsoft</strong> 推出 <a href="https://link.segmentfault.com/?enc=NsgweYGOmykEfWBGO992tQ%3D%3D.KrhKs9NznlGq2qC6Lu1BwcAbEn40z5FJpAtfCmtzfjACuex0ZJlDy7hPJ5ohbrfH6ngu5oSt7Jb016GZtU8ZU8KiPZGvnrNUFI6yJ7IYH3MxYNznKTKW8PPN%2B8TKq8fxALOht8I2Lk5FM4uEsKSCalEz7pzFEoXTnl6tlWLPUnLV%2FWODKdyDURoFot6qaqsn" rel="nofollow" target="_blank">Mu</a>，用于 Windows 系统设置的轻量级小语言模型（SLM），每秒可生成上百 token</li><li><strong>Google DeepMind</strong> 发布 <a href="https://link.segmentfault.com/?enc=GjwdCYqXlBNP65kcgZQvgA%3D%3D.hxMxAvGatYNpn0fgFMg1IO3d0eBcOOahzkFuvjMGEFsh8MU%2F1zfLWHjsBDVuCpUJMA9c7xws0gZ37CtYnXxRsmTSY1kcrRPXVN9bVVEseocdkjuwY9Kw6g6aONY%2B%2BPwE4ER0BgRcyFMvaW3bK%2FqOvA%3D%3D" rel="nofollow" target="_blank">Gemini Robotics On-Device</a>，让 Gemini 2.0 的多模态推理与现实理解延伸至本地机器人设备</li><li><strong>NVIDIA</strong> 推出完整的三层物理 AI 开发体系：DGX 超算（训练）、Omniverse 与 Cosmos（仿真）、Jetson AGX Thor（机器人推理），实现从训练到部署的全流程物理 AI 开发。</li></ul><p><strong>Savannah Kunovsky</strong> 表示：</p><blockquote>当 AI 进入家庭等私密空间时，设计“可信赖的边缘体验”至关重要。用户更希望数据在本地处理，而不是传输到遥远的服务器。未来的物理 AI 应在实用性与隐私保护之间取得平衡，让人们愿意“邀请”它进入生活。</blockquote><p><strong>Anthony Alford</strong> 补充：</p><blockquote>推理型语言模型正成为机器人智能的重要路径。NVIDIA 的机器人主管 Jim Fan 认为：“<strong>若没有实体化，AGI 永远不会到来</strong>。”换言之，真正的通用人工智能必须以物理形式存在。</blockquote><hr/><h3>2.4 MCP</h3><p><strong>Anthropic</strong> 于 2024 年 11 月提出，是一种开放标准，在为LLM提供统一的数据集成接口，减少碎片化和定制化集成的复杂度。</p><p>OpenAI、Microsoft、Google 均宣布支持 MCP。</p><p><strong>Anthony Alford</strong>：</p><blockquote>MCP 已被广泛采纳，它是实现 AI 代理互通的关键技术。尽管存在安全挑战，但它已成为主流，如用于运行测试的 Playwright MCP 服务器、或让代理读取 Figma 原型的应用，都是当前热门场景。</blockquote><p><strong>Daniel Dominguez</strong>：</p><blockquote>MCP 最令人兴奋的地方在于“互操作性”。它让不同公司的模型与数据源共享同一协议，实现多代理系统协作，真正让 AI 跨平台协同工作。</blockquote><hr/><h3>2.5 人机交互（HCI）</h3><p>借助 <strong>Agentic AI</strong> 与 <strong>Physical AI</strong>，人机交互正迎来重大变革。我们与软件交互的方式将更加自然与情境化。</p><p><strong>Savannah Kunovsky</strong>（IDEO 设计专家）：</p><blockquote>如今要用好大语言模型，必须懂得“提示工程”，这阻碍了大众采用。我们应让技术界面更流动、更自然。比如，信息应出现在我们需要的地方：烹饪时显示食谱、行走时轻松回复消息，而非停下来掏出手机。技术的目标是<strong>将信息嵌入日常情境中</strong>。</blockquote><p>她还提到 AI 在设计行业的应用：</p><blockquote>设计师现在能用生成式工具更快表达创意。如 IDEO 的一位设计师利用 AI 为儿童玩具设计制作“预告片”，通过视觉短片展示可持续材料的理念，沟通效率显著提高。这种跨领域协作正在改变设计的创作方式。</blockquote><p>其他新增至“创新者”类别的趋势还包括 <strong>推理模型（Reasoning Models）</strong> 与 <strong>AI DevOps</strong>。</p><hr/><h2>3 早期采用者阶段</h2><p>主要关注：<strong>语言模型创新</strong> 与 <strong>RAG</strong>。</p><h3>3.1 语言模型创新</h3><p>大型语言模型是生成式 AI 的基础。</p><p>今年出现了多种新类型与突破：视觉语言模型（VLM）、小语言模型（SLM）、推理模型与状态空间模型（SSM）等。</p><p>代表性成果包括：</p><ul><li><a href="https://link.segmentfault.com/?enc=Om4XJeiY7zlHxtJbNT2YWA%3D%3D.1iS7%2B47lIKWHXX0PGEiXT0%2B6B0HyhL0uZGHd5oPgc8Ppj%2BUvNemnEDoaaUgt053O" rel="nofollow" target="_blank">OpenAI 的 GPT-5</a></li><li><a href="https://link.segmentfault.com/?enc=b9KPKmBg4n%2BZIqQKuizj0w%3D%3D.azFT7tXY68OqQBdZ73V0RXdqq3jeD7%2BHoSWAoRpLQrE%3D" rel="nofollow" target="_blank">OpenAI Sora</a>：开创性的生成视频模型</li></ul><p><strong>小语言模型（SLM）</strong> 继续在端侧推理、隐私保护与成本优化领域扩展应用。</p><p><strong>Anthony Alford</strong>：</p><blockquote>GPT-5 的发布方式让人意外：用户无需选择，系统自动选用最合适的模型版本。<br/>这解决了命名混乱问题（如 4o 与 o4），简化了体验。</blockquote><p><strong>Savannah Kunovsky</strong>：</p><blockquote>这种做法旨在<strong>简化用户界面</strong>。<br/>随着技术成熟，真正的差异化将来自“交互体验”，<br/>即如何让普通用户更自然地使用这些强大能力。</blockquote><hr/><h3>3.2 RAG</h3><p>过去一年，RAG 应用在企业级开发中实现了爆发式增长，已从创新走向<strong>标准配置</strong>。</p><p><strong>Anthony Alford</strong>：</p><blockquote>几乎所有拥有大量文档与知识库的企业，都在考虑构建自己的 RAG 系统。</blockquote><p><strong>Savannah Kunovsky</strong>：</p><blockquote>RAG 改变了我们的设计调研方式。以往我们需要与多人访谈、收集资料；现在系统能自动整合公司内部文件与报告，让设计团队在动手前就获得充足上下文。这让非技术团队也能轻松构建基于 RAG 的应用。</blockquote><p>此外，<strong>自动化机器学习（AutoML）</strong> 也进入早期采用阶段，已被多家机构纳入生产流程。</p><h2>4 早期大众阶段</h2><p>以下技术已在各类组织的开发团队中普遍采用：</p><ul><li>向量数据库（Vector DBs）</li><li>MLOps</li><li>合成数据（Synthetic Data）</li></ul><h2>5 晚期大众阶段</h2><p>这些技术现已完全成熟，成为企业核心架构的一部分：</p><ul><li>数据湖仓（Lakehouses）</li><li>流式处理（Stream Processing）</li><li>分布式计算（如 Apache Storm）</li></ul><h2>6 总结</h2><p>AI 技术正从“任务执行者”演变为<strong>值得信赖的智能协作者</strong>，能够解决现实世界的复杂问题。AI 将持续深化并拓展至我们尚未设想的领域。未来一年预测：</p><ul><li><strong>AI 代理与 AI 驱动开发工具</strong> 将持续发展</li><li><strong>真正对人有用的 AI 应用</strong> 将奠定下一代互联网的基础</li><li><strong>视频 RAG</strong> 将兴起，区分真人与 AI 生成视频将成为新挑战</li><li>业界可能开始讨论“AI 泡沫”，并非因技术失效，而是行业结构面临调整</li><li><strong>AI 将更加隐形与上下文化</strong>，成为生活中自然、幕后运行的一部分</li></ul>]]></description></item><item>    <title><![CDATA[Palantir Ontology：革新]]></title>    <link>https://segmentfault.com/a/1190000047384801</link>    <guid>https://segmentfault.com/a/1190000047384801</guid>    <pubDate>2025-11-10 11:02:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><em>本文介绍了 Palantir Ontology，基于语义 AI 实现商业智能的企业级 AI 操作系统。原文：<a href="https://link.segmentfault.com/?enc=ApEfkdzZTLOqGBJz9vXL%2FQ%3D%3D.L%2BqRthFfH%2Fd%2F0n2oh1ZLY0tJT6O%2FQfoMlb%2FY6PpWo2A7N%2FvvRQy4BDGvdgKXx%2FbYQOh54Adrjd0BSMjoWpSwdsyMzmDvvvaoJAdufst1Jh4JHfufyqBUrMkzaXTndlOAIq9lpI9Xad9ixXTGRg2Xv5BlSiC0y0YhUt8v85QIC7tOaDUP57cnoVbRnizt2SQ%2F" rel="nofollow" title="Palantir's Ontology: The Enterprise AI Operating System Revolutionizing Business Intelligence" target="_blank">Palantir's Ontology: The Enterprise AI Operating System Revolutionizing Business Intelligence</a></em></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384803" alt="" title=""/></p><h2>引言：10 亿美元概念重塑企业技术</h2><p>Palantir Technologies 悄无声息完成了企业软件历史上最重要的转折点之一。它最初是专门的国防承包商，后来发展为雄心勃勃的为 AI 驱动的企业开发语义操作系统。数据说明一切：自 2022 年以来，Palantir 股价上涨了 656%，仅 2024 年就上涨了 341%，最重要的是，2025 年第二季度，Palantir 季度收入首次突破 10 亿美元里程碑，同比增长率达到 48%。</p><p>但真正的故事与股价无关，而是关于解决困扰企业 IT 几十年的问题：如何让 AI 不仅理解数据，而且理解数据背后的含义，如何弥合支离破碎的系统和统一的商业智能之间的差距。这就是 Palantir Ontology 的切入点 —— 超越传统数据架构概念而成为更基本的东西：人、机器、业务流程之间的通用语言。</p><p>“ontology（本体论）”这个术语听起来很学术，但在 Palantir 手中是非常实用的，是 Palantir AI 平台（AIP，Artificial Intelligence Platform）的基础，该平台推动美国商业收入从 2024 年第一季度的 20% 增长到 2025 年第二季度的 93%。这种加速并不是侥幸，反映了更深层次的东西：世界各地的企业都认识到，通用人工智能需要以特定领域的业务知识为基础，才能创造真正的价值。</p><p>我们来看看这个系统如何工作，为什么很重要，以及对企业软件的未来意味着什么。</p><h2>从数据孤岛到语义统一：根本问题</h2><p>每个大型企业都面临同样的长期问题：数据碎片化。走进任何一家《财富》500 强公司，都会看到同样的困境。Salesforce 保存客户关系数据，SAP 管理财务交易，物联网传感器实时传输设备状态信息，Excel 电子表格大量存在，API 连接外部数据提供商，云数据湖无差别整合一切数据。</p><p>这场悲剧在于：系统之间很少能够相互交流信息。当 CFO 提出“哪些客户具有最高的终身价值（相对于其获取成本而言）？”这样的问题时，组织却无法迅速给出答案。这个问题需要整合客户交易历史（SAP）、客户互动（Salesforce）、财务成本分配（自定义报告）以及预测模型（位于数据科学团队 Jupyter Notebook 的某个地方）等信息。数据工程师往往要花费数周时间来完成这些工作，但等到这些见解得以传达时，分析结果往往已经过时了。</p><p>传统解决方案（数据仓库和数据湖）通过物理方式整合数据来应对这一问题。提取-转换-加载流程将所有数据复制到中央存储库，统一数据模式，然后在此基础上构建分析功能。这种方法是可行的，但就像试图通过建造巨大的中央车站来解决城市公共交通系统的混乱问题一样，仍然存在一些根本性问题：中央存储库会成为瓶颈，更新会滞后于实际情况，新的用例需要新的 ETL 任务、添加新模式以及建模新的数据。</p><p>Palantir 采取了不同策略，并非通过物理方式进行整合，而是构建了一个“转换层” —— 一个语义模型，能够将每一条数据映射其在业务中的具体含义，无论这些数据存在于何处。</p><h2>语义层（The Semantic Layer）：让 AI 像企业一样思考</h2><p>这就是 Palantir Ontology 所带来的革命性变化。Ontology 并非只是移动数据，而是对其进行映射。在 Salesforce 中的“客户”、财务系统中的“付款方”以及产品数据库中的“用户”都是同一个业务概念。Ontology 创建了统一的对象类型 —— “客户”，并建立与每个源系统的双向映射关系。</p><p>这不仅是数据分类工作。在语义模型中，每种对象类型都有：</p><ul><li>对象类型定义以下实体：员工、机器、订单、客户、供应商、产品，可以将它们视为面向对象编程中的类，代表业务所依赖的基本概念。</li><li>属性描述了特征：员工有姓名、职位、部门和薪资区间。机器有运行状态、温度、振动水平和上次维护日期。属性有类型，并且可以从多个数据源获取，或者通过模型计算得出。</li><li>链接类型捕捉关系：员工向经理汇报（另一位员工）。订单由客户下单并在工厂执行。机器位于仓库中。这些不仅是数据库中的外键，更是业务逻辑的连接纽带。</li></ul><p>其精妙之处在于，这种语义模型成为其之上所有内容的“应用程序编程接口”（API）。构建分析应用程序时，不用查询原始表并编写复杂的连接语句，而是提问：“请给我显示过去 30 天内高价值客户的所有订单。”该系统明白“高价值客户”指的是信用评分大于 80 的客户，“过去 30 天”这一条件会根据日期对订单对象进行筛选，而客户与订单之间的关系链在 Ontology 中已经预先设定。</p><p>但这就出现了突破性的部分：这种语义结构正是 LLM 所渴望的。LLM 是专门针对结构化关系、层级结构和上下文信息进行优化的模式匹配引擎，而 Ontology 恰好为它们提供了这样的东西 —— 一种用于理解的业务语法规则。</p><h2>行动层（The Kinetic Layer）：从理解到行动</h2><p>语义层是基础且静态的，定义了“存在什么”。而行动层则补充了“发生了什么” —— 即业务的动态行为、实时的行动流程。</p><ul><li>动作类型反映了业务操作：“批准采购订单”、“触发设备维护”、“将客户分配至区域”。这些并非普通的 CRUD 操作，而是具有语义意义的业务操作，会带来后续影响。</li></ul><p>当某人在 Palantir 中批准一份采购订单时，会发生一系列自动操作：订单对象的状态会更新，财务系统中的付款授权流程会启动，采购团队会收到通知，供应商会通过 API 接收到确认订单的信息。一个在多个系统中协调运作的动作，被作为一个具有语义意义的事件记录下来。</p><ul><li>功能承载着业务逻辑：包括计算、机器学习模型、优化程序以及决策规则。一个功能可能被称为“计算客户信用评分”（依据历史支付数据、行业信号以及外部信用报告进行计算），或者“预测设备故障”（通过训练好的神经网络使用 12 个月的传感器数据进行分析），或者“优化供应链路径”（在数千个节点上运行复杂的算法优化程序）。</li></ul><p>行动层会记录每一个操作及其结果，这点至关重要：当人类决策者批准对客户给予折扣时，系统不仅会执行这一操作，还会记录：背景信息（客户资料、订单详情、当时市场状况）、决策（批准 X% 的折扣）、结果（实际客户行为、收入影响、满意度评分）以及决策者的推理过程（如果有所记录的话）。</p><p>这一决策记录将成为 AI 层的训练数据。</p><h2>动态层（The Dynamic Layer）：能够对业务进行推理的 AI</h2><p>这就是 AI 超越普通聊天机器人并成为真正决策支持系统的所在之处。动态层结合了：</p><ul><li>情境推理：当你向 AI 提出诸如“我们应该整合哪些供应商？”这样的问题时，系统不会仅仅在知识库中进行搜索，它会遍历 ontology：读取供应商对象（其能力、可靠性评分、成本结构），遵循链接类型访问订单和历史表现，检查机器数据以了解生产依赖关系，并在这一关系图中进行推理。</li></ul><p>这种情境意识可以避免错误假设。传统 AI 可能会自信的推荐整合供应商，但并不理解某个“效率较低”的供应商是关键部件的唯一国内供应源 —— 而这些信息存在于业务知识中，但并未包含在语言模型的训练数据中。</p><ul><li>闭环学习：还记得动作层中的决策记录吗？它们会反馈到 AI 模型中。比如说，系统建议与供应商 A 合作，因为他们成本更低，而业务团队批准了这一建议。六个月后，你注意到供应商 A 出现了两周的生产延误，导致 200 万美元的收入损失，系统记录了这一失败。下次询问合作事宜时，AI 已经学习到：“仅成本低是不够的，可靠性对实际业务价值具有乘数效应。”</li></ul><p>这就是企业级 AI 从通用型向领域特定型发展的过程。它不是基于互联网文本进行训练，而是基于公司实际决策历史进行训练。</p><ul><li>多步骤模拟：系统能够通过基于 ontology 的模拟来构建“假设情景”。例如：“如果将安全库存减少 10%，那么缺货频率、持有成本和收入损失会怎样变化？”该模拟会梳理各种关系：库存对象会影响订单履行，这又会影响客户满意度，进而影响收入。所有这些在 ontology 中都是相互关联的。</li></ul><h2>与英伟达整合：大规模应用中的性能表现</h2><p>这里有个实际情况：在实时状态下对庞大的知识体系进行推理需要强大的计算能力。Palantir 和英伟达于 2025 年 10 月宣布了一项深度合作，将这一理论架构转化为实际应用。</p><p>该整合内容包括：</p><ul><li>NVIDIA CUDA-X 加速计算技术用于实现超快速的数据处理和模型推理</li><li>Nemotron 开源语言模型经过企业推理优化处理</li><li>cuOpt 决策优化软件用于解决复杂物流和路径规划问题</li><li>Blackwell 架构 GPU 加速技术使端到端 AI 流水线能够以前所未有的速度运行</li></ul><p>Lowe's 公司提供了有力证明：他们正在使用 Palantir Ontology 结合 NVIDIA 加速技术构建其全球供应链的数字孪生模型。当需求突然发生变化（比如一场飓风影响了某个地区）时，该系统可以立即模拟数千种供应链重新配置方案，并推荐最佳应对措施 —— 一切都在几分钟内完成，而非数小时或数天。</p><p>该合作关系特别注重规模问题。在 2025 年第二季度，Palantir 公司报告称能够管理涵盖数千对象和关系的复杂数据模型，并通过 AI 推理可在数秒内完成处理。而传统方法在处理数量少得多的对象时就难以应对。</p><h2>Palantir 应用生态系统：让能力触手可及</h2><p>Ontology 是基础，但若缺乏可用性，能力便毫无意义。Palantir 构建了完整生态系统，将 Ontology 的能力转化为实际应用：</p><ul><li>Workshop 是一款可视化应用构建工具。非技术性的业务用户可以通过拖放组件来组装数据视图、工作流程和决策界面。由于其底层语义模型是一次性定义的，因此 Workshop 应用会自动继承数据的一致性、安全策略和语义正确性。</li><li>Quiver 专为时间序列和历史分析而设计 —— 追踪对象及其属性随时间的变化情况。这对于预测性维护场景（如分析设备历史状态）或金融分析（如需要交易时间线）来说至关重要。</li><li>Contour 能处理大量数据分析 —— 分析数百万的订单、分析数以十万计的供应商关系中的模式，或者处理数十亿的物联网传感器数据点。</li><li>Vertex 是知识图谱可视化工具。当你排查订单延迟的原因时，Vertex 会展示相关对象：提供关键组件的供应商、生产该组件的工厂、可能处于维护中的设备、参与其中的员工以及排队中的竞争订单。复杂因果关系变得清晰可见。</li></ul><p>更重要的是，该系统支持自然语言交互。“请给我展示未来 30 天内有延迟交付风险的所有订单”会被转换成一个 Ontology 查询。 “为什么客户 X 的订单延迟了？”会通过语义关系追溯以确定根本原因。“如果再雇佣 5 名生产工人会怎样？”会模拟动态层。</p><p>这种大众化举措至关重要。公司中具备编写 SQL 能力的那 20% 员工现在能够使用那些此前需要深厚专业知识和编程技能才能操作的系统。这种转变就是从“企业软件是专业工具”转变为“企业软件是业务合作伙伴”。</p><h2>现实世界影响：理论与实践的交汇之处</h2><p>理论固然有趣，结果也极具说服力，但以下这些才是实际生产中真实发生的情况：</p><ul><li>Wendy 的供应链优化：Palantir 的数字孪生技术解决了过去需要 15 人花费一整天时间才能解决的问题：当其 6450 家餐厅的糖浆供应出现短缺时，该系统在几分钟内就识别出了问题，并提出了最佳的重新分配方案。结果是，问题得以在五分钟内得到解决，而过去则需要 24 小时。</li><li>Walgreens 门店运营：Walgreens 在 10 家门店进行了 Palantir 平台的试点测试，运营任务的效率提高了 30%，随后在八个月内扩展到了 4000 家门店。这就是拥有可复制语义模型所带来的力量 —— 在一个门店有效的方法在 4000 家门店中也能自动奏效，因为语义模型是统一的。</li><li>Lowe 全球供应链：将 Palantir 语义模型与英伟达加速技术相结合，Lowe 创建了其整个全球供应链的实时数字孪生 —— 制造工厂、配送中心、运输公司、供应商和客户需求模式。当地区出现供应中断时，AI 会立即推荐最佳应对措施，而不是通过数周的手动分析来解决。</li><li>房利美欺诈检测：房利美部署了Palantir AIP 来检测抵押贷款欺诈。通过理解贷款申请、借款人历史记录、房产估值以及欺诈模式之间的语义关系，AI 实现了超过 99% 的准确率，远远优于那些未能捕捉到细微欺诈迹象的基于规则的传统系统。</li><li>花旗银行客户审核：花旗银行利用 AIP 来处理客户申请，通过理解语义背景来完成：信用历史、交易模式、行业风险因素、地缘政治风险暴露以及相关实体。其结果是更快的决策（几分钟而非数小时），并且能进行更准确的风险评估。</li></ul><p>这些并非孤立的案例，而是正在处理实际商业决策、具有量化财务影响的生产系统。仅供应链优化的例子就表明，企业通过效率提升可以在几个月内收回 Palantir 的实施成本。</p><h2>财务验证：数据证明一切</h2><p>市场对这一方法表现出了极大热情并予以认可。Palantir 公司 2024 至 2025 年的业务数据清晰展现了其迅速普及的趋势：</p><ul><li>收入加速：2025 年第二季度标志着公司历史上首个营收达 10 亿美元的季度，季度营收达到 10.3 亿美元，同比增长 48%。更令人瞩目的是：这代表着连续第八个季度收入增长加速，且增长曲线近期愈发明显。</li><li>商业部门爆发：2025 年第二季度，美国商业部门的营收 —— 这是 AIP 最具影响力的部分 —— 同比增长 93%，而 2024 年第一季度仅增长 20%。这并非单个季度的异常现象，而是由 AIP 的采用所驱动的持续二次增长。</li><li>客户拓展：2025 年第二季度客户数量达到 849 家，同比增长 43%。更值得注意的是：前 20 位客户的 12 个月累计营收达到 7500 万美元，较上年增长 30%。客户不仅保持不变，而且随着他们在各个部门深化实施，其支出大幅增加。</li><li>总合同价值（TCV）：2025 年第二季度的总合同价值预订额总计达到 230 亿美元——创历史新高。更令人震惊的是：有 66 笔交易超过 500 万美元，42 笔超过 1000 万美元。这表明企业正在开展为期数年的、跨部门的部署工作，而不仅仅是进行试点项目。</li><li>盈利与效率：2025 年第二季度的营业利润率扩大至 26.8%（较两年前的 8% 有了大幅提升），而自由现金流利润率达到了 57%。帕兰提尔的“40 原则”得分达到了惊人的 74.8%（48%的增长率加上 26.8% 的营业利润率），远超软件行业对于健康 SaaS 公司 40% 的基准要求。</li><li>全年业绩指引：管理层将 2025 年全年的营收预期上调至 41.42 亿至 41.50 亿美元（增长约 40%），并要求美国商业营收增长至少达到 85% —— 对于 Palantir 这样规模的公司来说，这是一个惊人的增长率。调整后的营业利润率预计将超过 30%。</li></ul><p>这些数字很重要，因为它们将理论转化为实践。当客户每年支付数千万美元，并以 30% 以上的增长率续约时，他们用资本投票表明 Ontology 确实带来了实际的商业价值。</p><h2>架构比较：为何 Ontology 胜过传统方法</h2><p>要全面理解 Palantir 所构建的体系，将其与传统企业架构进行直接对比会有所帮助：</p><p>传统数据仓库和数据湖侧重于数据存储的整合，擅长对集中式数据进行历史分析，但在实时操作、不同系统之间的语义一致性以及与原生 AI 集成方面存在困难。</p><p>传统商业智能系统具备可视化和报告功能，非常适合回答“发生了什么？”这类问题。但在“我们应该怎么做？”以及“如果发生这种情况会怎样？”这类需要跨系统推理和预测能力的问题上表现欠佳。</p><p>传统 AI/ML 平台在特定任务上能够优化模型准确性，在诸如欺诈检测或推荐这类特定问题上表现强大。但在企业层面的协调方面，则存在局限性，因为在一个领域（如供应链）中的决策会通过多个领域（如财务、人力资源、运营）层层传递。</p><p>Palantir 公司的基于 Ontology的方法则有所不同，具体表现为：</p><ul><li>在不进行物理整合的情况下实现语义统一（同时尊重源系统的所有权和实时要求）</li><li>支持双向同步（在 Palantir 流程中所做的决策会自动回传至源系统）</li><li>将 AI 置于业务环境中（防止出现幻觉，并支持特定领域的推理）</li><li>记录决策历史（便于持续改进模型）</li><li>实现大众化访问（通过自然语言和可视化界面进行）</li><li>经济高效扩展（一个语义模型可服务于所有应用程序）</li></ul><p>根本区别在于：传统系统是“数据平台”，而 Palantir 则是“操作系统”，是其他一切运行的基础层。</p><h2>三层架构：从理解到行动再到学习</h2><p>Palantir 工厂将复杂性整合为三个相互关联的层次：</p><ul><li>语义层（“存在的事物”）定义了业务现状。对象代表业务实体，属性描述特征，链接类型捕捉关系。这就是数字孪生 —— 组织的全面、统一的模型。</li></ul><p>业务用户和分析师会持续与这一层进行交互。当他们提出问题或构建报告时，所查询的是经过语义定义的对象，而非原始数据库表。这种一致性是基础性的，确保每个人都能以一致的定义讨论相同的业务概念。</p><p>行动层（“发生了什么”）负责记录操作并实现执行。动作类型定义了具有业务意义的操作，功能嵌入决策逻辑。行动层会记录每一个操作及其结果，从而形成审计记录和决策数据集。</p><p>这就是 Palantir 从分析层面转向操作层面的地方。它不仅是能展示业务现状的系统，还是能够执行业务决策并追踪结果的系统。当你批准一项复杂的供应链重新配置方案时，系统会协调在多个系统之间实施该方案，监控执行情况，并记录结果。</p><p>动态层（“可能的情况”）运用 AI 和分析技术来模拟未来并从经验中学习。机器学习模型会处理历史数据和决策记录，模拟探索各种情景。决策记录会反馈到模型训练中。</p><p>每一层都相互连接，数据从语义层、动态层依次流向动态层、行动层和感知层。所获得的见解和模式会反向回流，并为后续决策提供参考。随着经验的积累，该系统会变得越来越智能。</p><h2>合作的重要性：为何 Palantir 与英伟达的合作意义重大</h2><p>无需具备深厚的技术知识，也能明白 Palantir 为何在 2025 年 10 月宣布与英伟达建立重要合作关系。在企业规模上进行 Ontology 推理需要强大的计算能力，而传统 CPU 在此方面存在局限性：</p><p>在成千上万个相互关联的语义对象中进行查询，以实现情境感知的智能推理。运行模拟以探索成千上万种不同的情况。实时处理数十亿个物联网数据点，以更新数字孪生模型。基于历史决策数据训练机器学习模型，以改进推荐结果。</p><p>英伟达的架构 —— 针对并行计算优化的图形处理器、加速数据处理的 CUDA 库、针对推理进行微调的 Nemotron 模型、以及优化复杂路由和分配问题的 cuOpt —— 直接解决了这些计算难题。</p><p>这种合作关系使 Palantir 能够做出前所未有的承诺：“以往需要数小时或数天才能完成的复杂企业级人工智能推理，现在可以在几秒钟内完成。”在供应链优化、能源电网管理以及金融风险评估等领域，这种性能上的差异对于业务至关重要。</p><p>此外，英伟达还带来了可靠性和与云服务提供商的整合优势。英伟达 Blackwell GPU 可通过 AWS、Azure 和 GCP 提供，意味着企业可以在其首选云环境中部署 Palantir 与英伟达的架构，而无需受到锁定的限制。</p><p>这一合作关系标志着其已走向成熟。Palantir 公司不再仅仅销售数据分析软件，而是正在成为涵盖整个 AI赋能企业架构的集成商：包括 Ontology（Palantir）、基础设施（英伟达）以及云部署（AWS/Azure/GCP）。</p><h2>挑战与现实</h2><p>如果对有关 Palantir 公司未来发展的合理质疑视而不见，那未免太天真了：</p><p>估值风险：Palantir 公司的市盈率（基于预期未来收益计算）约为 225 倍，这表明其增长预期极为乐观。这意味着几乎没有空间容许执行过程中的失误或市场饱和情况出现。如果年增长率低于 30%-35%，很可能会导致市盈率大幅压缩。</p><p>政府集中风险：尽管商业业务增长势头迅猛，但美国政府合同仍约占总收入的 55%。政治变动或预算限制可能会对这一收入来源的稳定性造成影响。</p><p>竞争愈发激烈：大型云服务提供商（AWS、Azure、GCP）拥有庞大的资源和现成的客户关系，正在推出竞争性产品（AWS QuickSight、Google Vertex AI、Azure Synapse）。问题不在于竞争是否会加剧，而在于 Palantir 的语义优势是否具有护城河。</p><p>实施复杂性：构建真正的企业 Ontology 并非简单的“即插即用”式安装，需要对业务流程有深入了解，需要致力于标准化的数据治理，并且需要实现组织层面的一致性。实施可能会失败，而且客户获取速度可能会因早期采用者（其组织结构较为简单）的饱和而放缓。</p><p>经济敏感性：许多 Palantir 的应用场景（供应链优化、金融风险管控）在稳定时期能带来价值，但在危机时期则变得至关重要。然而，这种现象却可能导致出现周期性的“繁荣-萧条”式应用推广模式。</p><p>风险真实存在，而非抽象概念。股价自 2022 年以来的 656% 涨幅是基于极为理想的情况得出的，而实际情况往往更为复杂。</p><h2>更大的转变：从数据驱动到语义驱动</h2><p>暂且放远目光，Palantir Ontology 不仅是一种更优的数据架构，更代表着企业对待技术方式的根本性转变。</p><p>“数据驱动时代”（2010 年至 2020 年）提出了这样一个问题：“如何收集更多数据，并通过分析从中提取模式？”企业建立了数据仓库，聘请了数据科学家，并投资于商业智能工具。当时的假设是，数据量的增加和分析技术的提升能够释放出价值。</p><p>“语义驱动时代”（2020 年至今）提出了这样一个问题：“如何让机器理解业务含义并对其进行推理？”从“从数据中提取模式”转变为“以清晰的语义形式呈现业务概念，以便 AI 能够对其进行推理”。数据量的重要性降低，语义的清晰度变得更为重要。</p><p>其意义深远。在数据驱动的时代，价值源自数学 —— 更为复杂的算法胜过简单的算法。而在语义驱动的时代，价值则来自知识 —— 更清晰的业务表述使 AI 能够做出更明智的决策。</p><p>Palantir通过 Ontology，实质上正在构建企业的知识基础设施。它并非只是一个获取见解的分析工具，而是运行企业决策的认知层。</p><p>公司如此迅猛的发展在这一背景下是合乎情理的。企业采用 Palantir 系统并非是因为喜欢其精美的用户界面或令人印象深刻的仪表盘，而是因为它从根本上改变了决策方式 —— 从缓慢、人工、官僚式的流程转变为快速、由 AI 辅助、具有情境感知能力的决策过程。</p><h2>实际实施：前进的方向</h2><p>如果这与组织理念相符，那么具体实施起来会是怎样的呢？</p><p>阶段 1：基础：确定核心业务对象（客户、订单、产品、供应商、员工等）。将属性与数据源进行关联。建立代表关键关系的链接类型。这在理论上很简单，但在组织层面上却颇具挑战 —— 需要各部门就如何对各自领域进行建模达成共识。</p><p>第二阶段：整合：利用 Palantir 的整合工具（虚拟表、流水线构建器、联邦）将数据源与 Ontology 进行连接，建立双向同步机制，以使 Ontology 始终与源系统保持同步，并且在 Palantir 流程中做出的决策能够反馈回运营系统。</p><p>第三阶段：自动化：为关键业务流程定义操作类型，嵌入代表决策逻辑和业务规则的功能模块，建立实时监控机制，以发现可采取行动的机会。此阶段将平台从分析工具转变为运营系统。</p><p>第四阶段：优化：引入基于已获取决策历史数据训练的 AI 模型，进行模拟以识别优化机会，建立闭环反馈机制以持续改进模型。</p><p>第五阶段：规模：在各部门之间复制成功的案例，开发满足特定业务需求的应用程序（供应链优化、风险管理、客户分析），与外部合作伙伴（供应商、监管机构）进行整合，这些合作伙伴能够从语义可见性中获益。</p><p>这是一段为期数年的历程（通常为 18 至 36 个月的完整部署周期），需要高层领导的支持、跨部门的协调以及对数据治理的真正承诺。但完成这一历程的企业通常会报告决策速度提高 40% 至 60%，决策质量（通过实际业务成果衡量）提高 30% 至 50%，以及通过运营优化实现 20% 至 40% 的成本降低。</p><h2>“万亿美元论”</h2><p>一些分析师推测，如果 Palantir 公司能保持 30% 以上的增长率，并将运营利润率提高到 40%以上，那么到 2030 至 2035 年可能会达到 1 万亿美元估值。这种情景假设：</p><ul><li>随着越来越多公司认识到语义方法的优势，企业对 AI 的采用速度不断加快。</li><li>Palantir Ontology 已成为企业知识表示的公认标准。</li><li>成功实现国际扩张（目前以美国市场为主）。</li><li>尽管面临竞争威胁，仍能保持其技术壁垒。</li><li>与 AI 基础设施（英伟达、云服务提供商）的整合变得无缝且标准化。</li></ul><p>这些都是合理但并非必然的假设。从 41 亿美元的营收（2025 年预期）增长到 200 - 300 亿美元（支持万亿美元的估值）的营收水平，需要在所有方面都取得成功执行。</p><p>话虽如此，但从理论角度来看，这一观点是极具说服力的。如果 Palantir 公司能够成功将企业 AI 推理技术普及化，就像为政府情报部门实现数据整合的普及化那样，那么所触及的市场规模每年将达到数百亿美元。</p><h2>结论：语义化企业的时代</h2><p>我们正目睹企业对技术架构思考方式的转变。在经历了长达二十年的数据仓库、商业智能工具以及孤立的机器学习模型之后，企业开始意识到一个根本性的局限性：当 AI 系统融入业务环境之中时，其表现最为出色。</p><p>Palantir Ontology 正是这种认知的体现。它并非在任何单一组件（如语义数据模型、双向集成、决策记录、AI 推理、模拟引擎）方面具有革命性意义 —— 这些概念在各种产品中各自独立存在，其真正变革在于它们被整合进统一的操作系统中。</p><p>财务数据令人瞩目：商业收入增长了 93%，季度营收达到数十亿美元，运营利润率高达 26.8%，年度合同价值预订额达 230 亿美元。这些并非是一款小众工具所具有的数据，代表的是一套基础平台的指标，而企业普遍认为这套平台至关重要。</p><p>更重要的是，这些实际应用正在带来可量化的商业价值。当 Wendy 快餐公司将一个由 15 人参与、耗时一天的流程简化为只需 5 分钟的系统推荐时，这并非是渐进式的改进，而是具有变革性的举措。</p><p>企业软件行业正步入全新时代，在这个时代，语义清晰度和决策智能已成为竞争的必备条件。Palantir 公司通过多年来对这一问题的研究，已成为使企业 AI 不仅成为可能，而且变得实用且可扩展的领军者。</p><p>对于那些真心想要在 AI 时代一展身手的企业来说，选择已变得清晰起来：要么自行构建基于 Ontology 的系统（这是一项规模庞大的工程，需要数年时间和数亿资金），要么选择一个已经在大规模应用中验证了这一理念的平台。</p><p>企业技术领域的语义变革正在展开，问题不在于企业是否需要这种变革，而在于会以何种速度采用。Palantir 公司的迅猛发展表明了答案：速度之快超出所有人的预期。</p><hr/><blockquote>你好，我是俞凡，在Motorola做过研发，现在在Mavenir做技术工作，对通信、网络、后端架构、云原生、DevOps、CICD、区块链、AI等技术始终保持着浓厚的兴趣，平时喜欢阅读、思考，相信持续学习、终身成长，欢迎一起交流学习。为了方便大家以后能第一时间看到文章，请朋友们关注公众号"DeepNoMind"，并设个星标吧，如果能一键三连(转发、点赞、在看)，则能给我带来更多的支持和动力，激励我持续写下去，和大家共同成长进步！</blockquote><p>本文由<a href="https://link.segmentfault.com/?enc=GbpvOoeiQSX8wqOgQ7Yr8Q%3D%3D.hf5vax7%2F3b%2F5wcgE2a%2B%2BF4D5feYxY2ec7cAdX%2BGWjag%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[实力认证！Zoho 入围长三角 AI 服]]></title>    <link>https://segmentfault.com/a/1190000047384814</link>    <guid>https://segmentfault.com/a/1190000047384814</guid>    <pubDate>2025-11-10 11:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年11月7日，2025年数字长三角发展大会之长三角工业AI创新发展对接会在浙江乌镇举行。长三角地区三省一市百余位数智领航人才、优秀服务商及制造业企业代表齐聚一堂，共探工业AI场景落地新路径。领跑全球的SaaS软件提供商Zoho受邀出席，助力企业数智化转型。</p><p><img width="723" height="548" referrerpolicy="no-referrer" src="/img/bVdmY4D" alt="" title=""/></p><p><img width="723" height="478" referrerpolicy="no-referrer" src="/img/bVdmY7k" alt="" title="" loading="lazy"/></p><p>会上，沪苏浙皖四地协会联合发起的长三角 AI 服务商资源池正式发布，经多轮严格筛选，Zoho成功入围榜单。</p><p>作为全球企业服务领域的深耕者，Zoho 携 Zia 智能助手生态及工业 AI 解决方案亮相，依托自主研发的全栈技术栈，提供从流程自动化到智能决策的全周期服务。其 AI 智能体矩阵可实现生产协作优化、销售效率提升等核心场景落地，助力制造业打破信息孤岛，降低转型成本。</p><p>此次入围是行业对 Zoho 技术实力与服务能力的权威认可。未来，Zoho 将持续深耕长三角制造业特性，优化区域化服务，以场景化、可落地的 AI 解决方案，为三省一市企业数智化转型提供精准支撑。</p>]]></description></item><item>    <title><![CDATA[实践Web开发 爱看书的领带 ]]></title>    <link>https://segmentfault.com/a/1190000047384850</link>    <guid>https://segmentfault.com/a/1190000047384850</guid>    <pubDate>2025-11-10 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。</p>]]></description></item><item>    <title><![CDATA[产品经理哭晕，我用AI+MCP把他需求秒]]></title>    <link>https://segmentfault.com/a/1190000047383735</link>    <guid>https://segmentfault.com/a/1190000047383735</guid>    <pubDate>2025-11-10 10:15:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>最近，我们领导一拍脑袋，说要弄块可视化大屏挂在公司为业务赋能，别的不谈，起码客户来拜访得能唬的住！</p><p>于是产品经理光速拉通了会议，总结一下重点就是：</p><ul><li>大屏要有科技感，第一眼看起来要酷炫</li><li>要让人感觉数据是活的</li><li>敏捷开发，快速落地</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383738" alt="细品" title="细品"/></p><p>会后产品经理发了一张图给我，让我参考参考，大概长下面这样👇</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383739" alt="3" title="3" loading="lazy"/></p><p>是的，我们公司没有UI，开局全靠产品经理一张图～</p><p>最后加了好几天的班才勉强交付。</p><p>&lt;img src="https://web-3.obs.cn-south-1.myhuaweicloud.com/image/%E5%A4%B4%E5%86%B7.jpg?x-image-process=style/mark" alt="头冷" style="zoom: 50%;" /&gt;</p><p>痛定思痛，公司要我一岗多能，那我只能给自己降本增效了～</p><p>苦练AI编程两年半，带薪摸鱼悄悄上岸！</p><p>话不多说，撸袖开干！</p><h2>AI编程入门指南</h2><p>工欲善其事，必先利其器，咱都是搞IT的，一款好工具的重要性相信不用我多说。</p><p>在经过多方调研后我选择Cursor作为主力AI IDE，在使用体验上与VsCode基本一致，非常友好！</p><p>目前市面上主流的AI编程IDE交互设计方面基本雷同，都是通过一个AI侧边栏窗口进行交互。</p><p>选择<strong>语言模型</strong>、<strong>工作模式</strong>以及<strong>上下文</strong>，然后发送提示词，等待AI输出，这样就完成了一次完整的交互。</p><blockquote><p>工作模式一般分问答（Ask）和智能体（Agent）两种模式，Ask模式下AI不会修改工作区的代码，只会在侧边栏输出，Agent模式则直接修改工作区代码并通过Diff形式让用户选择是否采纳。</p><p>上下文可以是项目内的文件、文件夹、代码片段、Git提交等。<br/>当不选择上下文时IDE会默认将整个项目作为上下文，导致AI输出变慢。</p></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383740" alt="13" title="13" loading="lazy"/></p><p>花了一点时间熟悉了基础的AI编程功能后，很快我就写出了N个小工具，不得不说，真的有被几句提示词就搞定一个项目的感觉爽到～</p><p>刚好最近产品经理又提了几个新需求，拿来试试！</p><p>然而这一次，在用光了好几个Cursor帐号白嫖的免费额度后也没能达到一个预期的结果。</p><p>&lt;img src="https://web-3.obs.cn-south-1.myhuaweicloud.com/image/9.jpg?x-image-process=style/mark" alt="9" style="zoom:50%;" /&gt;</p><p>这几个需求明明并不复杂，问题出在了哪里？</p><h2>为AI安装“应用商店”</h2><p>在企业级web项目开发中，前端UI层需要和原型图、设计稿进行交互，后端数据层则是要和数据库、消息队列进行交互，前后端接口联调需要进行与接口文档工具进行交互。</p><p>涉及UI的前端部分，当我上传原型图图片作为上下文时，图片提取识别准确度不够，使用提示词精准描述UI布局的细节是个难题。</p><p>涉及数据库表相关的后端部分也存在类似的问题，描述数据库模型需要的提示词文本量大，并且涉及多表关联的复杂查询容易在描述提示词时产生歧义。</p><p>而这些归根结底都是同一个问题，<strong>AI缺少了外部服务的上下文</strong>。</p><p>AI大模型本身的上下文只局限于项目内部，而我们上面提到的原型图、设计稿、数据库、消息队列、接口文档都属于外部服务，AI是“看不见”的。</p><p>问题是如何让这些外部服务和AI大模型结合到一起？</p><h3>什么是MCP</h3><p>有请本章节的主角<code>MCP</code>来帮我们解决这个难题！</p><blockquote><p>Model Context Protocol（模型上下文协议），简称MCP，是一种将 AI 应用程序连接到外部系统的开源标准。</p><p>使用 MCP，Claude 或 ChatGPT 等 AI 应用程序可以连接到数据源（例如本地文件、数据库）、工具（例如搜索引擎、计算器）和工作流（例如专门的提示）——使它们能够访问关键信息并执行任务。</p><p>可以将 MCP 想象成 AI 应用的 USB-C 端口。正如 USB-C 提供了连接电子设备的标准化方式一样，MCP 也提供了将 AI 应用连接到外部系统的标准化方式。</p></blockquote><p>&lt;img src="https://web-3.obs.cn-south-1.myhuaweicloud.com/image/2.png?x-image-process=style/mark" alt="2" style="zoom:50%;" /&gt;</p><p>简单来说，MCP是一种开源标准协议，支持这一协议的外部服务都能被AI大模型访问！</p><p>假设AI大模型是IOS（操作系统），那么<strong>MCP</strong>就是Swift（IOS专属的APP编程语言），<strong>MCP Servers</strong>是IOS系统中的App Store（应用商店），<strong>MCP Server</strong>是App Store中具体的某个应用。</p><p>这意味着，开发所需的各种外部服务（如数据库、API、云平台）都能通过MCP Server无缝引入AI的上下文窗口，让AI大模型直接调用！</p><p>光懂理论可不行呐，动手实践一下！</p><h3>MCP资源的获取</h3><p>以接入接口文档MCP Server为例，我们团队平时使用Apifox这个文档平台。</p><p>那么第一步需要先确认是否存在Apifox的MCP Server资源。</p><blockquote>MCP Server资源的常见获取方式有两种，第一种是直接去到MCP市场上找，第二种是直接到平台的官方文档找。</blockquote><p>在Apifox的官方文档中我们成功找到了MCP Server的接入说明。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383741" alt="4" title="4" loading="lazy"/></p><h3>接入MCP Server</h3><p>成功获取到资源后我们在MCP Client（Cursor）中进行MCP Server的接入。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383742" alt="5" title="5" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383743" alt="6" title="6" loading="lazy"/></p><blockquote>大多数MCP Server通过npm获取，不同的Server所需的NodeJS版本不一致，需要特别注意！</blockquote><p>到这里我们就完成了一项MCP Server的接入啦～</p><h3>MCP Server的使用</h3><p>接入了MCP Server，我们该如何使用呢？</p><p>首先需要确保MCP Server处于启动状态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047383744" alt="7" title="7" loading="lazy"/></p><p>剩下的就和平时使用一样，直接在上下文窗口进行交互即可。</p><p>想象一下，后端完成接口开发后，通过Swagger生成接口文档，前端利用MCP将接口文档接入AI的上下文后自动在api层进行代码生成，再也不用吭哧吭哧地一个一个对接口了！</p><h2>结语</h2><p>当我将接口文档、数据库、原型图、设计稿都通过MCP整合起来时，这次产品经理的需求终于超预期提前完成了！</p><p>这一刻，我突然意识到原来AI编程真的已经发展到了令人惊叹的地步，还真是有了一点危机感。</p><p>但回过神来又发现，焦虑大可不必。</p><p>AI是无比强大的画笔，但执笔作画的，依然是我们，它接管了重复性劳动，恰恰将我们推向了更高阶的舞台，从“如何实现”的思维，升级到“为何创造”的思考。</p><p>工具越强大，使用工具的人的判断力就越发关键。</p><p>所以，唯一的危机并非来自AI，而是停止学习和思考的自己。专注于提升认知、理解业务、定义问题，这才是我们永远无法被替代的护城河。</p><p>好了，先摸会儿鱼吧，摸鱼使我快乐～</p><p>&lt;img src="https://web-3.obs.cn-south-1.myhuaweicloud.com/image/10.jpg?x-image-process=style/mark" alt="10" style="zoom:67%;" /&gt;</p>]]></description></item><item>    <title><![CDATA[终于有人把数据库搭建讲清楚了 数据集成与]]></title>    <link>https://segmentfault.com/a/1190000047383819</link>    <guid>https://segmentfault.com/a/1190000047383819</guid>    <pubDate>2025-11-10 10:14:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在信息时代，<strong>数据已成为最宝贵的资产</strong>。</p><p>如何科学地管理这些数据，让它们从杂乱的信息碎片成为有序的知识宝藏？</p><p>我们可以借助数据库来实现，数据库能让数据管理变得高效可靠。</p><p>你看，从网站用户信息到购物记录，从业务报表到日志数据，几乎所有现代应用都离不开数据库的支撑。</p><p>今天我就来给大家聊聊数据库怎么搭建，有哪些困难和挑战，在今后发展中，它有着什么样的发展趋势。</p><h3>一、数据库的定义</h3><p>数据库，就是<strong>一个高度结构化的、由统一管理系统进行操作的电子化仓储系统</strong>。它包括：</p><ul><li><strong>表</strong>：负责管理同一类数据。比如，你可以有一个专门存放所有用户信息的“用户表”，还有一个专门记录所有交易行为的“订单表”。</li><li><strong>行</strong>：它代表了表中的一个具体实体。比如说，在用户表中，关于“张三”的所有信息构成一行，就是一条完整的记录。</li><li><strong>列</strong>：也被称为“字段”，它定义了数据的某个特定属性。例如，用户表中的姓名、手机号、创建时间都是不同的列。</li></ul><p>总而言之，<strong>数据库就是由许多张这样的表构成的</strong>。而且，<strong>这些表之间并非孤立存在，它们可以通过特定的列相互关联，从而形成一张紧密的数据关系网</strong>。这类数据库也因此被称为“<strong>关系型数据库</strong>”，它是目前最主流的形式。</p><p>那么，理解了数据库的基本构成后，这样一个结构清晰的数据仓库，我们该如何从零开始把它搭建起来呢？</p><h3>二、怎么搭建数据库？</h3><p>提到搭建，你可能会觉得这是资深工程师的专属领域，但实际上，它的核心流程有着清晰的逻辑。用过来人的经验告诉你，为一个业务系统搭建数据库，通常离不开下面这五个关键步骤：</p><h4>第一步：需求分析与规划</h4><p>这是决定后续所有工作成败的基石。在敲下任何一行代码之前，你必须反复问自己：</p><ul><li>我的业务究竟需要存储哪些数据？</li><li>这些数据之间存在怎样的内在联系？</li><li>数据的规模预计会有多大？增长速度如何？</li><li>预计会有多少人同时访问数据库？对响应速度的要求有多高？</li></ul><p>这些海量数据要怎么收集？可以用专门的<strong>数据集成工具</strong>，比如<strong>FineDataLink</strong>，它支持接入多种数据源，还能实现多表的数据同步，能够帮你省去大把写代码的时间。</p><p>这个过程，就如同建造大楼前进行的蓝图设计。如果前期规划不周全，后期很可能面临巨大的修改成本，甚至需要推倒重来。</p><h4>第二步：选择适合的数据库类型</h4><p>技术选型没有万能钥匙，关键在于匹配你的业务场景。主要分为两大类：</p><ul><li>关系型数据库：比如 MySQL、PostgreSQL。这是最经典和通用的选择。它们极度强调数据的一致性和关联性，使用标准的SQL语言进行管理。如果你的业务涉及复杂的关联操作和事务处理，比如银行的转账、电商的下单，那么选择它通常不会错。</li><li>非关系型数据库：比如 MongoDB、Redis。它们提供了更灵活的数据模型，在特定场景下能提供极高的性能。MongoDB适合处理结构不固定的文档数据，而Redis则是一款极快的内存数据库，常被用作缓存来提升系统速度。</li></ul><p>简单来说，对于刚入门的朋友，从MySQL这类关系型数据库开始学习，是路径最平滑、学习资源最丰富的选择。</p><h4>第三步：设计数据库结构</h4><p>现在，我们要把第一步分析得出的需求，转化为具体的、可执行的数据库表结构，这个过程被称为“<strong>数据库建模</strong>”。</p><ul><li>你需要确定创建哪些表。</li><li>明确每个表包含哪些列。</li><li>定义每一列的数据类型，是整数、文本、还是日期时间？</li><li>设定约束条件，比如哪些列的值必须唯一、不能为空。</li><li><p>规划表与表之间的关联关系。<br/>这一步极其考验你对业务逻辑的理解深度和思维的严谨性。一个设计优良的数据库结构，是保障整个应用系统稳定、高效运行的坚实基础。</p><h4>第四步：部署数据库软件</h4><p>接下来，你需要为数据库安一个“家”。这个家可以是一台云服务器，也可以是你本地的一台电脑，然后，在上面安装你选定的数据库软件。</p></li></ul><p>现在各类数据库的安装过程都已经非常简化，社区有大量的指导文档可供参考。安装完成后，你需要进行一些基础配置，比如设置访问端口和管理员密码。</p><h4>第五步：创建与持续管理</h4><p>软件环境准备就绪后，你就可以通过命令行或图形化工具登录数据库，执行SQL语句，将第三步设计好的表结构逐一创建出来。至此，你的应用程序便可以通过编程语言连接到这个数据库，实现数据的增、删、改、查等核心操作。</p><p>不过，搭建完成仅仅只是个开始，后续的<strong>日常维护，包括定期备份、性能监控、安全加固</strong>，是一项同样重要且需要长期投入的工作。</p><p>听起来不算难？但在实际搭建和运营过程中，还是会有许多的挑战和困难。</p><h3>三、数据库搭建的困难</h3><p>如果你认为严格按照上述步骤就能高枕无忧，那可能低估了实践的复杂性。以下是几个我们经常会遇到的棘手问题：</p><ol><li><strong>结构设计困难</strong><br/>在项目初期，如果<strong>对业务发展的预见性不足</strong>，很容易导致表结构设计存在缺陷。<br/>比如，最初没有预料到某个文本字段的内容会异常庞大，或者错误地判断了数据实体间的关系复杂度。等到系统上线、数据量积累到一定程度后，再想去修改表结构，成本会非常高，可能涉及长时间的停机、复杂的数据迁移，并伴随极高的风险。</li><li><strong>数据一致性的难题</strong><br/>举个例子：从A账户扣款100元，向B账户增加100元。<br/>这两个操作必须作为一个不可分割的原子单元，要么全部成功，要么全部失败，如果中间发生系统故障，导致只完成了扣款而加款未成功，就会产生数据错乱。</li></ol><p>但在高并发访问的压力下，如何精细地设计事务范围，在确保数据一致性的同时，又不因过多的锁等待而拖垮系统性能，是一个需要深厚经验才能处理好的平衡艺术。</p><ol start="3"><li><strong>性能优化漫长</strong><br/>当数据越来越多时，最初的查询可能会慢到令人无法忍受，这时，数据库优化就成了必修课。</li></ol><ul><li>你需要学会分析慢查询日志，精准定位导致性能瓶颈的SQL语句。</li><li>你需要为高频查询的条件列创建索引。但索引并非越多越好，因为每个索引都会增加数据写入的开销并占用额外存储空间。</li><li>在数据量极大的情况下，你可能还需要采取“分库分表”这种更复杂的架构手段，将一个巨型表拆分成多个较小的、更易管理的部分。</li></ul><p>性能优化是一个没有终点的过程，需要持续地观察、分析和调整。</p><ol start="4"><li><strong>安全与备份难题</strong><br/><strong>数据库通常存储着企业的核心数字资产</strong>。如何防止外部黑客攻击和数据泄露？如何精细化管理内部人员的数据访问权限？如果存储数据的物理硬盘发生损坏，如何保证数据不丢失？所以，建立一套可靠、自动化的数据备份与恢复机制，并定期进行恢复演练，确保在灾难发生时能真正快速复原数据，是要特别关注的事。</li></ol><p>了解了当下的困难，我们不妨把目光放得更远一些，看看数据库未来可能的发展方向。</p><h3>四、数据库的未来发展趋势</h3><p>技术浪潮奔涌向前，数据库领域正经历着深刻而有趣的变革。</p><ol><li><strong>云数据库</strong>。现在直接使用各大云服务商提供的云数据库服务，已成为新项目的首选。它们负责所有底层的运维工作，包括硬件故障、软件补丁、备份和弹性扩容，让你可以专注于业务逻辑和数据分析本身。这极大地<strong>降低了数据库的使用和维护门槛</strong>。</li><li><strong>多模数据库</strong>。现在，单一的数据库产品开始融合多种数据模型。比如说，一个数据库内核可以同时高效地处理结构化的表数据、半结构化的JSON文档，甚至复杂的图关系数据。这为开发者应对多元化的业务需求提供了<strong>更大的灵活性和便利性</strong>。</li><li><strong>自动化与智能化</strong>。我们可以借助机器学习技术，未来的数据库可能能够自动诊断性能瓶颈，主动推荐或创建最优索引，甚至预测潜在的硬件故障。这将把<strong>数据库管理员从大量重复性的运维工作中解放出来，转而专注于更高价值的数据库架构设计和业务支撑工作</strong>。</li><li><strong>与大数据、AI的深度融合</strong>。数据库的边界正在不断扩展。现代的数据仓库和数据湖技术，使得数据库能够直接对海量历史数据进行复杂的分析与挖掘，直接为商业决策提供洞察。数据存储与智能计算正在走向深度融合。</li></ol><h3>总结</h3><p>看了这篇文章，相信你对数据库有了一个整体的认知，本质上是<strong>学习一种在混沌中建立秩序、从信息中提炼价值的思维方式</strong>。用过来人的经验告诉你，掌握数据库的本质，就是要学会在<strong>数字世界中如何有序地安放与运用信息</strong>。不如就从现在开始建立一个简单的数据库，哪怕只有Excel也没关系，重要的是你能学习和掌握这些数据，为后续的工作提供可靠的支撑。</p>]]></description></item><item>    <title><![CDATA[C#.NET 路由机制深入解析：从传统路]]></title>    <link>https://segmentfault.com/a/1190000047383897</link>    <guid>https://segmentfault.com/a/1190000047383897</guid>    <pubDate>2025-11-10 10:13:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>简介</h3><p>路由是 <code>ASP.NET Core</code> 的核心基础设施，负责将 <code>HTTP</code> 请求映射到对应的处理程序（如控制器方法）。它决定了 <code>URL</code> 如何与应用程序代码交互，是现代 <code>Web</code> 开发的关键组件。</p><p>在 <code>ASP.NET Core</code> 中，路由系统解决了以下问题：</p><ul><li><code>URL</code> 映射：将用户友好的 <code>URL</code> 映射到具体的处理程序。</li><li>灵活性：支持多种路由配置（如 <code>RESTful</code> 路径、动态参数）。</li><li>性能优化：高效解析请求，快速定位处理逻辑。</li><li>可扩展性：通过中间件和自定义路由约束扩展功能。</li><li>异步支持：与 <code>async/await</code> 无缝集成，适合现代 <code>Web</code> 应用。</li></ul><h4>主要功能</h4><ul><li><p>基于约定的路由（<code>Convention-based Routing</code>）：</p><ul><li>使用模板（如 "<code>{controller}/{action}/{id?}</code>"）定义路由规则。</li><li>适合传统 <code>MVC</code> 应用。</li></ul></li><li><p>特性路由（<code>Attribute Routing</code>）：</p><ul><li>通过 <code>[Route]</code> 特性直接在控制器或动作上定义路由。</li><li>适合 <code>RESTful API</code> 和复杂 <code>URL</code> 模式。</li></ul></li><li><p>参数绑定：</p><ul><li>支持路由参数（如 <code>{id}</code>）、查询字符串、请求体等。</li><li>支持可选参数、默认值和约束。</li></ul></li><li><p>中间件集成：</p><ul><li>通过 <code>UseRouting</code> 和 <code>UseEndpoints</code> 中间件处理路由。</li><li>支持自定义路由中间件。</li></ul></li><li><p>路由约束：</p><ul><li>限制路由参数（如 <code>int、guid、regex</code>）。</li><li>提高性能和安全性。</li></ul></li><li><p>区域支持（<code>Areas</code>）：</p><ul><li>将控制器分组到不同区域（如 <code>Admin、User</code>）。</li></ul></li><li><p>动态路由：</p><ul><li>支持动态生成路由（如基于数据库配置）。</li></ul></li><li><p>端点路由：</p><ul><li>统一管理 <code>MVC、Razor Pages</code> 和 <code>SignalR</code> 的路由。</li></ul></li></ul><h3>核心概念</h3><table><thead><tr><th>名称</th><th>说明</th></tr></thead><tbody><tr><td><strong>路由模板</strong></td><td>由文字、参数（<code>{id}</code>）、可选参数（<code>{id?}</code>）、默认值、约束组成的字符串</td></tr><tr><td><strong>路由参数</strong></td><td>路径中的占位符，匹配 URL 段并绑定到 Action 方法参数</td></tr><tr><td><strong>路由数据</strong></td><td>匹配结果的键值对集合，可在中间件或 Controller 中通过 <code>RouteData</code> 访问</td></tr><tr><td><strong>路由约束</strong></td><td>通过类型（<code>int</code>、<code>guid</code>）、正则（<code>regex(...)</code>）等限制参数匹配规则</td></tr><tr><td><strong>默认值</strong></td><td>当 URL 中未提供参数时使用的值</td></tr><tr><td><strong>终结点（Endpoint）</strong></td><td>最终与请求匹配并执行的代码单元，如 MVC Action、Razor Page、Minimal API</td></tr></tbody></table><h3>配置方式</h3><h4>启用 Endpoint Routing</h4><pre><code class="csharp">// Program.cs （.NET 6+ Minimal Hosting）
var builder = WebApplication.CreateBuilder(args);
builder.Services.AddControllers();   // MVC / API

var app = builder.Build();
app.UseRouting();                     // 启动路由中间件

app.UseAuthorization();               // 授权、认证等

app.UseEndpoints(endpoints =&gt;
{
    endpoints.MapControllers();       // 特性路由 + 约定路由
    endpoints.MapRazorPages();        // Razor Pages
    endpoints.MapHub&lt;ChatHub&gt;("/hub"); // SignalR
    // endpoints.MapGet("/ping", () =&gt; "pong"); // Minimal API
});
app.Run();</code></pre><h4>约定路由（Conventional Routing）</h4><pre><code class="csharp">builder.Services.AddControllersWithViews();
// ...
app.UseEndpoints(endpoints =&gt;
{
    endpoints.MapControllerRoute(
        name: "default",
        pattern: "{controller=Home}/{action=Index}/{id?}");

    endpoints.MapAreaControllerRoute(
        name: "admin",
        areaName: "Admin",
        pattern: "Admin/{controller=Dashboard}/{action=Index}/{id?}");
});</code></pre><ul><li><code>pattern</code>：路由模板，包含默认值（<code>=Home</code>）和可选参数（?）。</li><li>在 <code>Controller</code> 中可不标注任何特性，直接按约定匹配 <code>URL</code>。</li></ul><h4>特性路由（Attribute Routing）</h4><pre><code class="csharp">[ApiController]
[Route("api/[controller]")]
public class ProductsController : ControllerBase
{
    [HttpGet]                     // GET api/products
    public IActionResult GetAll() { … }

    [HttpGet("{id:int}")]        // GET api/products/5，且 id 必须是整数
    public IActionResult Get(int id) { … }

    [HttpPost("batch/{type?}")]   // POST api/products/batch 或 api/products/batch/special
    public IActionResult CreateBatch(string? type) { … }
}</code></pre><ul><li><code>[Route]、[HttpGet]、[HttpPost]</code> 等特性定义路由模板、方法限制。</li><li>支持在控制器和方法级别混用，覆盖或叠加。</li></ul><h3>路由模板详解</h3><h4>模板语法元素</h4><table><thead><tr><th>语法</th><th>示例</th><th>说明</th></tr></thead><tbody><tr><td>字面值</td><td><code>api/products</code></td><td>固定匹配的路径段</td></tr><tr><td>参数 <code>{param}</code></td><td><code>{controller}</code></td><td>捕获值并绑定到参数</td></tr><tr><td>可选参数 <code>{id?}</code></td><td><code>{id?}</code></td><td>参数可选</td></tr><tr><td>默认值 <code>{id=5}</code></td><td><code>{page=1} </code></td><td>未提供时的默认值</td></tr><tr><td>约束 <code>{id:int}</code></td><td><code>{id:min(1)}</code></td><td>限制参数格式</td></tr><tr><td>通配符 <code>*</code></td><td><code>{*slug}</code></td><td>捕获剩余路径</td></tr><tr><td>命名空间</td><td><code>[Namespace("Admin")]</code></td><td>控制器命名空间约束</td></tr></tbody></table><h4>路由约束类型</h4><table><thead><tr><th>约束</th><th>示例</th><th>说明</th></tr></thead><tbody><tr><td>类型约束</td><td><code>{id:int}</code></td><td>必须是整数</td></tr><tr><td>范围约束</td><td><code>{age:range(18,99)}</code></td><td>值在指定范围内</td></tr><tr><td>正则表达式</td><td><code>{ssn:regex(^\\d{{3}}-\\d{{2}}-\\d{{4}}$)}</code></td><td>匹配正则模式</td></tr><tr><td>必需值</td><td><code>{name:required}</code></td><td>必须提供非空值</td></tr><tr><td>自定义约束</td><td><code>{code:validProductCode}</code></td><td>实现 <code>IRouteConstraint</code> 接口</td></tr></tbody></table><h3>高级特性</h3><h4>动态路由</h4><p>基于数据库动态生成路由：</p><pre><code class="csharp">using Microsoft.AspNetCore.Mvc;
using Microsoft.AspNetCore.Routing;
using System.Collections.Generic;
using System.Threading.Tasks;

public class DynamicRouteController : ControllerBase
{
    private readonly Dictionary&lt;string, string&gt; _routes = new()
    {
        { "page1", "Content for Page 1" },
        { "page2", "Content for Page 2" }
    };

    [HttpGet("{key}")]
    public IActionResult GetDynamic(string key)
    {
        if (_routes.TryGetValue(key, out var content))
            return Ok(content);
        return NotFound();
    }
}

var builder = WebApplication.CreateBuilder(args);
builder.Services.AddControllers();
var app = builder.Build();
app.UseRouting();
app.UseEndpoints(endpoints =&gt; endpoints.MapControllers());
app.Run();</code></pre><h4>多路由规则</h4><pre><code class="csharp">[Route("api/[controller]")]
[Route("v2/[controller]")] // 支持多个路由
public class ProductsController : ControllerBase
{
    [HttpGet("all")]
    [HttpGet("list")] // 多个路径映射同一方法
    public IActionResult ListProducts() { /*...*/ }
}</code></pre><h4>路由参数转换</h4><pre><code class="csharp">[Route("products/{id:guid}")]
public IActionResult GetProductById(Guid id) { /*...*/ }

[Route("products/{slug:slugify}")] // 自定义 SlugConstraint
public IActionResult GetProductBySlug(string slug) { /*...*/ }</code></pre><h4>端点元数据（Endpoint Metadata）</h4><p>通过 <code>WithMetadata(...)</code> 或在特性上添加自定义属性，可为终结点附加元数据，在中间件中读取以执行额外逻辑（例如权限校验、文档生成）。</p><h4>路由优先级（Order）</h4><p>特性路由和约定路由都可设置 <code>Order</code> 属性，高优先级先匹配：</p><pre><code class="csharp">[Route("special", Order = 1)]
public IActionResult Special() { … }

[Route("{**catchAll}", Order = 100)]
public IActionResult CatchAll() { … }</code></pre><h4>端点过滤器（Endpoint Filter，.NET 7+）</h4><p>在 <code>Minimal APIs</code> 或 <code>Controller</code> 上，可注册 <code>Endpoint Filter</code> 以在终结点执行前后插入逻辑，相当于细粒度中间件。</p><h4>最小 API 路由</h4><pre><code class="csharp">app.MapGet("/weather/{day:datetime}", (DateTime day) =&gt;
    $"Weather for {day:yyyy-MM-dd}");</code></pre><ul><li>直接在 <code>WebApplication</code> 上映射，既是特性路由也是约定路由的简化版。</li><li>支持参数绑定、依赖注入、请求处理器委托等。</li></ul><h4>自定义路由约束</h4><p>实现 <code>IRouteConstraint</code> 接口，注册到路由选项中，即可在模板里使用自定义约束</p><h3>路由诊断与调试</h3><h4>路由信息中间件</h4><pre><code class="csharp">app.UseEndpoints(endpoints =&gt; { /*...*/ });

// 添加诊断中间件
app.Use(async (context, next) =&gt;
{
    var endpoint = context.GetEndpoint();
    if (endpoint != null)
    {
        Console.WriteLine($"Endpoint: {endpoint.DisplayName}");
        Console.WriteLine($"RoutePattern: {endpoint.Metadata.GetMetadata&lt;RoutePattern&gt;()}");
    }
    await next();
});</code></pre><h3>优缺点</h3><p>优点</p><ul><li>灵活性：支持约定路由和特性路由，适应多种场景。</li><li>性能高：基于端点路由，匹配速度快。</li><li>异步友好：支持 <code>async/await</code>，适合现代 <code>Web</code>。</li><li>可扩展：支持自定义约束、中间件和动态路由。</li><li><code>DI</code> 集成：与 <code>ASP.NET Core DI</code> 无缝结合。</li></ul><p>缺点</p><ul><li>配置复杂：复杂路由规则需仔细设计，避免冲突。</li><li>学习曲线：特性路由和约束需要熟悉语法。</li><li>调试难度：路由冲突或错误需调试工具（如日志）。</li><li>进程内限制：路由配置不跨实例，需结合外部配置。</li></ul><h3>资源和文档</h3><ul><li><p>官方文档：</p><ul><li><code>Microsoft Learn</code>：<a href="https://link.segmentfault.com/?enc=UGEx7Bb6ZDG4uz6SXfJOTQ%3D%3D.GWtkMKlGM22ju%2FeRlpa5hzqxw8EA%2Bnbx3LA22NMng8JQIe%2BTnuxLipUl3yt6WvNBIJDRg8iWTRhPpGD32vlPITjoKRBFEfbuNfa6RhKhffM%3D" rel="nofollow" target="_blank">https://learn.microsoft.com/en-us/aspnet/core/fundamentals/ro...</a></li><li><code>ASP.NET Core MVC</code>：<a href="https://link.segmentfault.com/?enc=kV9r3EAuOyoX8IjPNw7ErQ%3D%3D.6OXBghHdmeerf8eDaBux6gWaTOcuWgZQHXJQV4XOng8QPTz4qZPNqDyYmlz%2BBI1nQfmHGcnRfYOOqAL7f8R2EWo9vkQYs5jdop30S%2FNzyrE%3D" rel="nofollow" target="_blank">https://learn.microsoft.com/en-us/aspnet/core/mvc/controllers...</a></li></ul></li><li><code>NuGet</code> 包：<a href="https://link.segmentfault.com/?enc=qyVOMV%2BJTPqULzb%2Fr3Jkzg%3D%3D.C5nD5HJY9eoPHSr7Vfcs8IBiu4l7oedlGIhPbZF3lTgrTEGepZDyOdtsz%2BYXzQk%2Bz4xsrZsGb5r2yacj%2FcpVxg%3D%3D" rel="nofollow" target="_blank">https://www.nuget.org/packages/Microsoft.AspNetCore.Mvc.Core</a></li><li><code>GitHub</code>：<a href="https://link.segmentfault.com/?enc=GlnPfK2rV6gc6ze1LsRkFQ%3D%3D.Sry91JF1sz0kxjb73776xe00Tb04lIhZBnP7J4pNrz2M963y0N8v7KKGvwK9XM0G" rel="nofollow" target="_blank">https://github.com/dotnet/aspnetcore</a></li></ul>]]></description></item><item>    <title><![CDATA[英伟达、DeepSeek集体跟进！18个]]></title>    <link>https://segmentfault.com/a/1190000047384192</link>    <guid>https://segmentfault.com/a/1190000047384192</guid>    <pubDate>2025-11-10 10:13:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>面向大众；</p><p>找到跟读者链接的点</p>]]></description></item><item>    <title><![CDATA[马斯克Grok 4深夜大升级：200万逆]]></title>    <link>https://segmentfault.com/a/1190000047384174</link>    <guid>https://segmentfault.com/a/1190000047384174</guid>    <pubDate>2025-11-10 10:12:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：犀牛</p><p>【新智元导读】Grok家族一夜两大升级：Grok 4 Fast拉到2M上下文、Grok Imagine生成逼真到真假难辨。</p><p>太快了！</p><p>一天之内Grok连迎两大更新——Grok 4 Fast与Grok Imagine都进行了大升级。</p><p>Grok 4 Fast把上下文窗口提高到2M，并把完成率拉到94.1%（推理）与97.9%（非推理）。</p><p>这意味着，你不必再把一本书或一整个代码库切碎喂给模型，它可以一次吞下，然后稳定地给出结果。</p><p>与此形成共振的是，x.ai在OpenRouter上的API调用份额也在走高，这说明Grok模型的质量也受到了开发者的青睐。</p><p>Grok Imagine的升级则主要体现在生成质量上——其输出已经到了真假难辨的程度！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384176" alt="" title=""/></p><p><strong>2M上下文</strong></p><p><strong>一次装下两本《战争与和平》</strong></p><p>首先看Grok 4 Fast，它的最大升级在于具备了200万token的上下文能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384177" alt="" title="" loading="lazy"/></p><p>这意味着它可以一次性处理相当于150万个英文单词或6000页的文本量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384178" alt="" title="" loading="lazy"/></p><p>这规模太惊人了！</p><p>相当于Gemini 2.5 Pro的2倍、GPT-5的5倍！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384179" alt="" title="" loading="lazy"/></p><p>毫不夸张地说，200万token的上下文窗口彻底改变了游戏规则——</p><p>整本书、完整的代码库或者是海量的数据集，都能够一次性处理。</p><p>要知道列夫·托尔斯泰的大部头《战争与和平》也仅有大约80万个token——Grok 4 Fast可以轻松一次装下两部还有富余！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384180" alt="" title="" loading="lazy"/></p><p>再加上Grok 4 Fast近乎零内存泄漏的急速性能，使得模型能够进行大规模的思考而不失焦。</p><p>这将会重新定义「实时AI推理」的样貌。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384181" alt="" title="" loading="lazy"/></p><p><strong>推理质量同步提升</strong></p><p>上下文数量增长的同时，Grok 4 Fast的推理质量也有着明显的进步。</p><p>具体来说，Grok 4 Fast在推理模式下，完成率从77.5%跃升至94.1%。</p><p>非推理模式下，完成率从77.9%跃升至97.9%。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384182" alt="" title="" loading="lazy"/></p><p>这意味着模型成功输出符合要求结果的概率大幅提升，失败率（比如中途断开、输出逻辑断裂、无法完成任务等）显著降低。</p><p>可以说，Grok变得聪明多了。</p><p>对此，有网友评论道，Grok要是照这样增长下去，「它可能成为首个真正实现速度与推理能力协同优化而非相互制约的模型」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384183" alt="" title="" loading="lazy"/></p><p>这一点也体现在了OpenRouter API调用量的市场份额变化上。</p><p>根据最新的数据，x.ai的API调用量已经达到了26.4%，远远超过了谷歌以及Anthropic。</p><p>可以说，占据了绝对的统治地位。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384184" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384185" alt="" title="" loading="lazy"/></p><p><strong>Grok Imagine：逼真到真假难辨</strong></p><p>与Grok 4 Fast一同升级的还有Grok Imagine。</p><p>升级后的Grok Imagine输出非常逼真，几乎达到了真假难辨的程度。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384186" alt="" title="" loading="lazy"/></p><p>马斯克自己也是在x上玩的飞起。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384187" alt="" title="" loading="lazy"/></p><p>提示词：add a boyfriend and they transition into muppets</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384188" alt="" title="" loading="lazy"/></p><p>有网友还玩起了西方古典文学与神话中的经典情节——埃涅阿斯逃离特洛伊。</p><p>这个故事源自古罗马诗人维吉尔的史诗《埃涅阿斯记》，也呼应了古希腊神话中特洛伊战争的后续脉络。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384189" alt="" title="" loading="lazy"/></p><p>还有不同角度的——</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384190" alt="" title="" loading="lazy"/></p><p>大家脑洞大开，甚至生成了苏格拉底饮下毒芹的悲情时刻。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384191" alt="" title="" loading="lazy"/></p><p>有想法的同学快去试试吧。</p>]]></description></item><item>    <title><![CDATA[太顶了！文心全新模型LMArena榜文本]]></title>    <link>https://segmentfault.com/a/1190000047384171</link>    <guid>https://segmentfault.com/a/1190000047384171</guid>    <pubDate>2025-11-10 10:11:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月8日消息，据LMArena大模型竞技场最新排名显示，文心全新模型ERNIE-5.0-Preview-1022登上文本排行榜全球并列第二、中国第一，该模型在创意写作、复杂长问题理解、指令遵循等方面表现突出，超过gpt-5-high等多款国内外主流模型。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384173" alt="" title=""/></p><p>文心全新模型ERNIE-5.0-Preview-1022登上LMArena文本排行榜国内第一</p><p>从榜单上来看，创意写作可用于生成文章、营销文案、剧本等内容，大幅提升内容产出的效率，ERNIE-5.0-Preview-1022在创意写作维度得分第一。复杂长问题理解用于处理多层逻辑和长文本任务，如学术问答、报告分析、知识推理等。指令遵循保证模型能准确理解并执行用户意图，适用于智能助理、代码生成、业务流程自动化等场景。在复杂长问题理解和指令遵循两项维度中，ERNIE-5.0-Preview-1022得分突出，为多场景内容生成提供了高效支持。</p><p>据早前消息透露，文心大模型最新基座模型将于下周11月13日的2025百度世界大会上正式对外发布。</p><p>公开资料显示，文心大模型于2019年首次公开亮相，经6年技术研发迭代，2025年先后推出多模态模型文心大模型4.5与4.5 Turbo，以及深度思考模型文心X1、X1 Turbo和X1.1，在多项权威评测中持续稳居中文大模型第一梯队。</p>]]></description></item><item>    <title><![CDATA[硅谷华人女CEO杀入！全球首家AI电影厂]]></title>    <link>https://segmentfault.com/a/1190000047384154</link>    <guid>https://segmentfault.com/a/1190000047384154</guid>    <pubDate>2025-11-10 10:11:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：好困 定慧</p><p>【新智元导读】当好莱坞还在为预算头疼时，硅谷的AI、韩国的IP和中东的资本已经悄然联手，他们的目标是用一个全新的「导演级AI」物种，彻底重塑电影工业。</p><p>AI原生影视工作室Utopai Studios与全球创新投资平台Stock Farm Road（SFR）共同宣布，成立资本规模达数十亿美元的合资企业<strong>Utopai East</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384156" alt="" title=""/></p><p>SFR由LG集团继承人Brian Koo、阿联酋主权基金推动者Amin Badr-El-Din联合创立。</p><p>该合作旨在将AI影视生成技术从实验阶段推向大规模产业化，并押注正处在爆发期的全球韩流内容市场，以「AI原生工作流+韩娱+全球市场通道」的组合拳，为下一代视听内容生产提供全新范式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384157" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384158" alt="" title="" loading="lazy"/></p><p><strong>黄金机遇</strong></p><p><strong>韩娱100倍的全球增长</strong></p><p>在传统好莱坞电影产业深陷「高成本、低回报」的结构性困境之时，韩国流行文化正展现出惊人的全球吸引力与商业潜力。</p><p>根据最新行业数据，韩娱内容全球观看时长占比在短短五年内从22%迅猛提升至35%，Netflix平台上的《僵尸校园》《黑暗荣耀》等剧集连创观看时长新纪录，凸显了其稳固的受众基本盘。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384159" alt="" title="" loading="lazy"/></p><p>更令人瞩目的是，融合了K-POP、奇幻叙事与顶级视效的创新IP《K-POP Demon Hunter》，在全球Z世代观众中引发了现象级热潮。</p><p>该IP首季全球流媒体播放量即突破12亿次，相关音乐榜单霸榜超过20周，衍生周边商品收入高达数亿美元，充分证明了优质韩流内容具备强大的跨文化穿透力和多元变现能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384160" alt="" title="" loading="lazy"/></p><p>伴随着韩流内容全球市场需求激增，Utopai Studios创始人兼CEO Cecilia Shen和LG集团现任会长Brain Koo在采访中都认为，韩娱起码还有100倍的全球增长。</p><p>Utopai与SFR的联手，正是瞄准了这一机会。</p><p>SFR在韩国规划的350亿美元AI数据中心枢纽，将为Utopai的先进AI影视模型提供强大的算力底座，共同将高质量、电影级长片内容的生成从技术验证推向规模化、经济化的产业应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384161" alt="" title="" loading="lazy"/></p><p>新公司Utopai East的治理结构也体现了深度整合，由LG北美创新中心高管Kevin Chong和CJ集团前国际化负责人Richard Lee领导，能够有效融合科技与文创资源。</p><p>同时，Cecilia将出任SFR战略顾问委员会首席创始成员，与Alphabet董事长John Hennessy等领袖共同制定AI影视伦理标准。</p><p>这一布局确保了技术、资本（特别是连接硅谷与中东的资本网络）与全球市场渠道的深度融合，目标是打造一个新型创作基础设施，助推韩国内容以好莱坞级制作标准高效对接全球市场，从而支持更多像《K-POP Demon Hunter》一样的顶级韩流IP，以更高效率和更可控的成本实现全球化制作与发行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384162" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384163" alt="" title="" loading="lazy"/></p><p><strong>技术架构</strong></p><p><strong>「导演级AI」如何统筹全局</strong></p><p>面对韩流内容日益增长的全球需求与叙事复杂度的不断提升，UtopaiAI模型的创新之处在于——</p><p>跳出了当前AI视频领域「Diffusion vs. AR」的模型之争，构建了一个以叙事为中心、具备因果规划能力的「导演级AI」系统架构。</p><p>Utopai的解决方案基于一个独特的洞察：传统AI视频技术的问题在于，它们是「画师」而非「导演」。</p><p>目前主流的Diffusion模型在专业影视制作中存在天然短板。</p><p>它本质上是「概率性生成模型」，擅长从噪音中「雕刻」出单帧高质量画面，但其逐帧或短片段独立生成的模式，缺乏对长叙事逻辑的全局规划能力。</p><p>这导致生成的视频难以保证人物外貌、动作和场景元素在不同镜头中的一致性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384164" alt="" title="" loading="lazy"/></p><p>Utopai的破解之道是构建一个「理解—生成」一体化的统一架构，采用前帧预测后帧的机制，其中「导演级AI」（采用自回归全能模型）扮演「总规划师」的角色。</p><p>该架构的核心是创建一个统一的状态空间，用于承载叙事、几何与运动约束。</p><p><strong>·</strong> 规划器在此空间内对未来时空进行可预测的演化推演，并确保与历史观测高度一致；</p><p><strong>·</strong> 渲染器则依据此规划生成最终画面，从而在长片段上实现可控的叙事一致性与高保真画质。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384165" alt="" title="" loading="lazy"/></p><p><strong>Utopai的架构精髓则在于规划与渲染的解耦。</strong></p><p>上层的序列规划器（世界模型）充当系统的「导演大脑」，其核心任务是叙事规划与一致性约束。</p><p>它以脚本和分镜为输入，生成包含角色ID向量、关键帧布局、相机位置与运动轨迹、场景约束以及情绪走向曲线等细节的shot级时空计划。</p><p>更重要的是，规划器能维护一个可回放的长程状态记忆，确保在超长时程的叙事中，角色身份、场景状态和光影变化能够稳定演进，进而从根本上解决跨镜头元素「漂移」的难题。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384166" alt="" title="" loading="lazy"/></p><p>在确定叙事蓝图后，专业化的生成模块（包含优化的Diffusion技术）下层的条件渲染器（时空扩散）则专注于执行高质量的画面生成。</p><p>它在潜空间进行操作，生成条件包括深度、法线、光流、遮罩、参考帧、相机轨迹等丰富的结构化信号。</p><p>这种分工使得规划器可以专注于长序列的因果结构与约束传播，充当「导演」角色；而渲染器（经过优化的Diffusion技术）则作为顶尖的「执行团队」，发挥其在画面细节与动态质感上的优势。</p><p>二者通过统一的状态接口紧密耦合，形成一个高效的闭环系统。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384167" alt="" title="" loading="lazy"/></p><p><strong>理解世界</strong></p><p><strong>独特的训练方法论</strong></p><p>Utopai模型能力的飞跃源于其独特的训练方法论，核心是让AI从大量高质量、带精确标注的3D合成数据中学习。</p><p>与主要依赖网络二维视频进行训练的通用模型不同，这种方法使AI模型能够内化物理规律，从根本上理解空间、遮挡和碰撞等三维世界规则，有效避免生成内容违背物理规律的「幻觉」问题。</p><p>训练过程分为两个关键阶段：</p><p><strong>1. 几何与语义对齐预训练：</strong>此阶段目标是建立模型对物理世界和视觉元素的底层理解，进行文本-视频-几何对齐以及下一状态/掩码重建等任务。</p><p><strong>2. 多模态指令微调：</strong>此阶段增强模型对复杂、抽象的叙事指令和跨模态约束的遵循能力，使其能精准理解并实现导演的创作意图。</p><p>这种训练方式使模型能够对复杂的情感和叙事意图进行状态级的精确建模与可视化表达。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384168" alt="" title="" loading="lazy"/></p><p>例如，当指令要求表现角色「从怀疑转变为恍然大悟」时，模型能够协调身体姿态、视线方向、镜头语言以及光影变化等一系列要素，呈现一个在表演逻辑上合理、情感层次分明的完整转变过程，而非简单地替换一个表情贴图。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384169" alt="" title="" loading="lazy"/></p><p><strong>共生进化</strong></p><p><strong>AI与影视创作的关系</strong></p><p>Utopai技术路径的精妙之处在于并非简单地用AR模型替代Diffusion模型，而是形成一种规划与执行分离的协同范式。</p><p>通过构建深度理解叙事逻辑与物理规则的智能系统，Utopai的影视模型实现了人与AI的共生进化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384170" alt="" title="" loading="lazy"/></p><p>其「规划—渲染」的协同范式将长程一致性与叙事可控性转化为可计算、可优化的状态规划问题，为专业影视制作者提供了兼具创造性自由度与工业化可控性的全新解决方案。</p><p>随着AI技术有望击穿传统制作的成本与效率壁垒，电影与高端视听内容的未来，正从「预算的暴政」转向「想象力的自由」。</p><p>对于正值黄金时代的韩流内容产业而言，Utopai带来的不仅仅是一项降本增效的工具，更是一台推动其创意潜能无限释放、迈向全球主流市场的强大引擎。</p><p>那些曾因制作成本与周期限制而被搁置的宏大创意，特别是融合了K-POP、webtoon等独特文化基因的创新IP，正迎来被全球观众看见的曙光。</p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=NuZB6YgrYaEQ6MdAhH3u7A%3D%3D.decdKZMyjP0wsccXuV3IMb9kTz9XUu2unx5qjjYLlMY%3D" rel="nofollow" target="_blank">https://www.utopaistudios.com</a></p>]]></description></item><item>    <title><![CDATA[特斯拉造出终结者之手！马斯克要挣1000]]></title>    <link>https://segmentfault.com/a/1190000047384115</link>    <guid>https://segmentfault.com/a/1190000047384115</guid>    <pubDate>2025-11-10 10:10:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：KingHZ 定慧</p><p>【新智元导读】马斯克的万亿梦想即将启航！特斯拉股东大会通过史诗级薪酬方案，未来十年若市值飙升至8.5万亿美元，马斯克将获近万亿股票奖励，引领人类迈向AI与机器人时代。</p><p><strong>马斯克最开心的一天！</strong></p><p>特斯拉股东大会刚刚批准了一项史无前例的高管薪酬方案——</p><p>未来十年，马斯克将有机会获得近<strong>1万亿美元</strong>的股票奖励。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384117" alt="" title=""/></p><p>马斯克兑现承诺，高兴地在股东大会和机器人擎天柱「Optimus」共舞：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384118" alt="" title="" loading="lazy"/></p><p>网友们说，马斯克天真无邪、幼儿一样的欢快舞姿，滑稽到可爱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384119" alt="" title="" loading="lazy"/></p><p>跳得太好了！令人印象深刻！动作很棒……不过说的是机器人「擎天柱」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384120" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384121" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384122" alt="" title="" loading="lazy"/></p><p>看完视频之后，只能说群众的眼睛是雪亮的，机器人跳舞已经比一般人跳得好了。</p><p>要是机器人擎天柱也套上一套「人类皮肤」，谁能分得清是不是机器人在跳舞？</p><p>科幻电影中的场景马上成真了：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384123" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384124" alt="" title="" loading="lazy"/></p><p><strong>马斯克的万亿富翁之路</strong></p><p>超过75%的股东投票通过了价值最高达1万亿美元的薪酬方案。</p><p>全场沸腾！欢呼声响彻全场。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384125" alt="" title="" loading="lazy"/></p><p>但这绝非唾手可得。</p><p>要兑现全部奖励，马斯克必须带领特斯拉实现前所未有的史诗级增长：</p><p>将市值提升至<strong>8.5万亿美元</strong>（较当前估值增长逾500%），并交付<strong>2000万辆电动车</strong>、<strong>100万个人形机器人（Tesla Bot）以及100万辆投入商业运营的Robotaxi</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384126" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384127" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384128" alt="" title="" loading="lazy"/></p><p>整个<strong>对赌周期长达10年</strong>，共有12个阶段，每个阶段都设有市值和运营目标。</p><p>这是企业史上最雄心勃勃（也最毫不谦逊）的任务清单——半是科幻畅想，半是个人崇拜。</p><p><strong>8.5万亿美元</strong>，这相当于现在<strong>「Meta+微软+谷歌」的市值总和。</strong></p><p>而1万亿薪酬，直接让马斯克加冕为全球首位万亿富翁，让贝索斯的财富都显得像「穷人」。</p><p>但每一分钱都与业绩挂钩。未达标？分文不取。</p><p>这是美国企业最纯粹的形态。</p><p>要么功成名就，要么一无所有。股东们已押上全部赌注，坚信马斯克必将再创奇迹。</p><p>马斯克<strong>目前净资产估值约4730亿美元</strong>（周三已达到5000亿美元）。</p><p>若计划顺利实施，马斯克在特斯拉的持股比例将从13%提升至近29%——这是他长期追求的控股权门槛。</p><p>值得注意的是，当马斯克年初将大量精力投入其他事务时，特斯拉股价在1至3月间暴跌43%。而自他回归后，股价已强势反弹，年内累计涨幅达16%。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384129" alt="" title="" loading="lazy"/></p><p>这次投票本质是押注特斯拉能否蜕变为8.5万亿美元市值的AI巨头。</p><p>华尔街知名分析师Dan Ives指出，自动驾驶与机器人技术已成为「特斯拉的关键转折点」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384130" alt="" title="" loading="lazy"/></p><p>他预测，特斯拉市值将在未来12到18个月内冲刺3万亿美元大关。</p><p>资本市场正用真金白银投票——</p><p>周三特斯拉股价应声上涨4.1%至462美元，年内累计涨幅已达77%。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384131" alt="" title="" loading="lazy"/></p><p><strong>人形机器人太像人了！</strong></p><p>在特斯拉股东会议，机器人展示V2 Optimus机器人灵活手：</p><p>马斯克的母亲发文祝贺：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384132" alt="" title="" loading="lazy"/></p><p>马斯克回复到下一个版本V3灵活手则更上一层楼。</p><p>有网友回复到下一代Optimus机器人配有22个自由度的灵活手，以后机器人等完成弹钢琴、杂耍等高难度任务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384133" alt="" title="" loading="lazy"/></p><p>如果看过詹姆斯·卡梅隆导演的电影《终结者》，你可能还记得下面的镜头：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384134" alt="" title="" loading="lazy"/></p><p>是时候，认真考虑机器人崛起、人类文明面临的危险的可能性了。</p><p>在大会上，马斯克称，特斯拉的人形机器人将在明年启动量产，确保人类安全优先。</p><p>在上个月的Q3财报电话会议上，马斯克就对分析师们说，</p><p>如果我们真的造出一支机器人军队，那我至少得对它有足够的影响力吧？如果我无法掌控它，那我就没法安心去造这样一支军队。</p><p>财报电话会议上，马斯克还再度强调人形机器人「擎天柱」（Optimus）的重要性，称其「有望成为人类史上最伟大的产品」。</p><p>他曾表示，双足机器人未来可以胜任工厂工人、甚至保姆的工作。而现在，他的预期更进了一步：</p><p>Optimus未来将是一名了不起的外科医生。有了Optimus和自动驾驶，我们可以真正创造一个没有贫困、人人享有优质医疗的世界。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384135" alt="" title="" loading="lazy"/></p><p>他表示，人形机器人经济效应巨大，可将全球经济规模扩大10-100倍。</p><p><strong>Optimus就像一个「无限金钱生成器」（infinite money glitch）。</strong></p><p>未来，甚至「金钱」本身可能不再以货币衡量，而是以「能量」（瓦特）来衡量：你能调动多少电力，决定了你的「购买力」。</p><p>一旦实现每年100万台的持续产量，生产成本将在20000美元左右。</p><p>特斯拉人形机器人将成为史上最伟大的产品，预计市场规模达数十亿台，其中个人机器人与工业用途机器人的比例大概在1:3到1:5。</p><p>特斯拉将启动「史上所有大型复杂制造产品中最快的产能爬坡」。从弗里蒙特工厂启动年产100万台的机器人生产线，之后在得州建年产能1000万台的产线。</p><p>也就是说，和马斯克共舞的机器人还会升级。</p><p>明年，特斯拉将开始生产<strong>Optimus V3 版本。 V3</strong>将带来巨大提升。</p><p><strong>看到它的人会以为那是穿着机器人服的人类。</strong></p><p>之后，每年人形机器人发布一个大版本，每次都会有重大改进。</p><p>说到，拟人机器人，最近国内的小鹏汽车推出了「最拟人的机器人」：</p><p>为自证清白，何小鹏剪开人形机器人小腿。这甚至超越了「恐怖谷」效应，有网友认为诞生才一天，就被当众「开膛破肚」，对机器「人」太残忍，太无情了……</p><p>小鹏汽车公开展示人形机器人及自动驾驶出租车技术路线，其战略布局与特斯拉长期坚持的技术方向形成呼应。</p><p>美国的市场观察人士指出，同业者的跟进印证了特斯拉技术路线的商业价值。</p><p>特斯拉仍具有独特优势，已突破复杂的手部工程、现实世界人工智能和大规模量产三大难点。</p><p>特斯拉<strong>人形机器人本质上是「有胳膊和腿的智能车」</strong>，与特斯拉车辆现有的电池、电机、人工智能等技术同源打造。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384136" alt="" title="" loading="lazy"/></p><p><strong>特斯拉FSD：太酷啦</strong></p><p>既然股东大会已经通过了马斯克的薪酬计划，马斯克还将继续为特斯拉工作。</p><p>说起特斯拉，除了三电技术、操控和车身安全外，<strong>FSD绝对特斯拉最核心的技术之一。</strong></p><p>甚至可以说，特斯拉就是一家<strong>披着电车外衣的AI公司</strong>。</p><p>特斯拉FSD使全球车队每680万英里仅发生一起事故，安全性为美国平均驾驶员的十倍，较2018年接近翻倍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384137" alt="" title="" loading="lazy"/></p><p>如果继续提升FSD的能力，将来可挽救数百万生命、避免数亿事故。</p><p>FSD V14.1版本已经很流畅，V14.3版本将达到「睡一觉醒来就抵达目的地」的水平。</p><p>大量网友上传了他们用FSD驾驶的惊艳案例。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384138" alt="" title="" loading="lazy"/></p><p>在FSD加持下，无人驾驶的运营车辆也将进入人们的生活。</p><p>特斯拉计划在年底前在奥斯汀取消Robotaxi的随车安全员。</p><p>特斯拉Robotaxi无人驾驶网约车业务目前已经在美国得州和加州运营，接下来将进驻内华达州、佛罗里达州和亚利桑那州。</p><p>特斯拉Robotaxi运营车辆，无方向盘、无踏板、无后视镜，每公里运行只需几毛钱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384139" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384140" alt="" title="" loading="lazy"/></p><p><strong>马斯克：做梦都在画芯片</strong></p><p>另一个值得一提的就是为了特斯拉而专门设计的AI 5芯片。</p><p>为了让机器人真正有智能，必须拥有强大的AI芯片——</p><p>既要性能卓越，又要<strong>成本低、能耗低</strong>。</p><p>AI 5芯片专为特斯拉人工智能优化，功耗仅为英伟达Blackwell的1/3，性能相当，成本却不足其10%。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384141" alt="" title="" loading="lazy"/></p><p><strong>为什么特斯拉能在芯片领域做到这种提升？</strong></p><p>特斯拉不需要兼容所有客户的需求，AI 5只为特斯拉工作。</p><p>这种专注的垂直整合能大幅简化设计，让芯片更高效、更强大。</p><p>另一个关键是，特斯拉的神经网络主要使用整数运算（Integer Arithmetic），而不是浮点数（Floating Point）。</p><p>这听起来有些繁琐，但其实是革命性的改变。</p><p>这也是为什么特斯拉芯片在性能功耗比上能做到领先数倍。</p><p>他们甚至已经规划好下一代芯片——<strong>AI6</strong>。</p><p>计划在AI5投产不到一年后，就完成过渡，实现性能与效率的全面翻倍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384142" alt="" title="" loading="lazy"/></p><p>这款芯片据说马斯克也在深度参与设计，甚至做梦都在画芯片。</p><p>芯片供应仍需扩大，供应商产能不足，特斯拉可能自建「Terafab」（巨型工厂，Tera代表万亿，规模超Giga工厂），目标建设每月100万片晶圆的大型晶圆厂。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384143" alt="" title="" loading="lazy"/></p><p><strong>Never bet against Elon Musk</strong></p><p>还是那句话，Never bet against Elon Musk。</p><p>这句话是彼得·蒂尔的名言：「永远不要押注与埃隆·马斯克为敌。」</p><p>股东大会上，马斯克上台前，股东们将双手举过头顶，高喊Elon、Elon、Elon。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384144" alt="" title="" loading="lazy"/></p><p>75%的投票股东选择继续相信马斯克，相信马斯克能带领特斯拉继续创造奇迹。</p><p>相信马斯克能够带领人类飞出地球，相信给马斯克万亿薪酬，给他实际控制权，他会给人类带来不一样。</p><p>就像Mario所说，特斯拉的股东们刚刚把历史的钥匙交给了马斯克。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384145" alt="" title="" loading="lazy"/></p><p>即使很多大佬，很多主权基金都反对将马斯克送上特斯拉的「神位」。</p><p>美国最大的养老基金之一，加利福尼亚公务员退休系统（Calpers）投了反对票，Calpers持有大约500万股特斯拉股票。</p><p>挪威主权财富基金，也是特斯拉的第七大股东，所持特斯拉股份约 1.16%–1.17%，投出了反对票。</p><p>Glass Lewis和ISS这两家代理咨询机构敦促特斯拉股东投票反对该提案。</p><p>马斯克随后在特斯拉十月的财报电话会议上回击，称他们为「企业恐怖分子」，不愧是天天网上冲浪的老马，造梗能力一流。</p><p>甚至连教宗都表示反对。</p><p>教宗列奥十四世虽然并非特斯拉的投资者，但也最近对马斯克成为万亿富翁所传达的信息以及贫富差距日益扩大的问题表示关切。</p><p>「若埃隆·马斯克成为首位万亿富翁，教皇利奥警告称世界将陷入「严重麻烦」」</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384146" alt="" title="" loading="lazy"/></p><p>尽管马斯克的财富，目前仍落后于约翰·D·洛克菲勒经通胀调整后的6300亿美元，但若他达到新的业绩目标，可能会成为现代历史上最富有的人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384147" alt="" title="" loading="lazy"/></p><p>这些代理投票机构和主权基金追求稳定收益，都觉得给马斯克太大权力不是好事。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384148" alt="" title="" loading="lazy"/></p><p>但科技领域最知名的投资人之一，木头姐Cathie Wood早就一眼看出，马斯克将在十一月取得压倒性胜利。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384149" alt="" title="" loading="lazy"/></p><p>其实早在2018年，就有一次马斯克的薪酬方案投票，那时候特斯拉还没有进入标普500指数。</p><p>Cathie Wood提到，现在<strong>特斯拉占标普500的比重为2.4%</strong>。</p><p>这个比例并不小，但也不足以让<strong>指数基金</strong>一股脑地左右投票结果。因为它们的投资组合是分散的，且通常会依据管理层推荐的方案进行投票，而不会主动反对或改变方案。</p><p>在2018年，特斯拉<strong>不在标普500</strong> 中，这意味着<strong>没有太多指数基金</strong>持有特斯拉的股票，投票结果相对更由<strong>主动投资者</strong>（如大机构和富有远见的投资者）主导。</p><p>那时的薪酬方案获得了压倒性的支持，他们相信马斯克带领下的特斯拉未来有很大增长潜力。</p><p>木头姐对机构或者代理机构的担忧不以为然：</p><p>「难道这不是令人悲哀，甚至可谴责吗，机构股东竟然依赖代理公司来告诉他们应该如何投票？」</p><p>「指数基金不做基础研究，却主导机构投票。基于指数的投资是一种社会主义。我们的投资体系已经崩坏。」</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384150" alt="" title="" loading="lazy"/></p><p>除了这些知名的投资人，当翻看特斯拉股东对马斯克的支持言论时，不得不说，你一定会有点小小的触动甚至感动。</p><p>这种无条件的信任，深信不疑，有种星星之火可以燎原的感觉。</p><p>比如现在X徽标的设计者Alex表示，他7年前投资特斯拉，现在已经增长了20倍！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384151" alt="" title="" loading="lazy"/></p><p>还有人告诉我们，为什么马斯克都这么有钱了，还要追求1万亿的薪酬。</p><p>本质上还是要获得对特斯拉的足够控制权，否则，还不如离开特斯拉，把精力放在xAI上。</p><p>这很容易让人联想到当年京东CEO刘强东所说的：如果对公司失去掌控权，宁可把股票兑现，离开公司。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384152" alt="" title="" loading="lazy"/></p><p>就像木头姐所说，很多投资者认为，通过马斯克的一万亿薪酬计划是只有利好的事情。</p><p>如果马斯克想要拿到一万亿薪酬，他就要把特斯拉做到8.5万亿的市值，投资人赚钱；</p><p>如果马斯克没有把特斯拉做到8.5万亿，大概率仍然会让特斯拉市值增长，只不过低于8.5万亿而已，投资人还是赚钱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384153" alt="" title="" loading="lazy"/></p><p>本质上这次投票，是散户和华尔街的一次对赌。</p><p>木头姐长期以来一直是马斯克在华尔街忠实的朋友，她认为这些薪酬仅仅是对马斯克创新的奖励而非过度报酬。</p><p>如果新的薪酬方案通过，马斯克在特斯拉的持股比例可能从13%上升到接近29%。</p><p>对于特斯拉这样一家公司来说，接近三分之一的持股意味着马斯克将有更多权力，将资源投入到AI、机器人和自动驾驶。</p><p>对于这个结果，木头姐毫不意外，毕竟「信仰马斯克」的散户投资者很可能才是主导者。</p>]]></description></item><item>    <title><![CDATA[PyTorch之父闪电离职，AI半壁江山]]></title>    <link>https://segmentfault.com/a/1190000047384094</link>    <guid>https://segmentfault.com/a/1190000047384094</guid>    <pubDate>2025-11-10 10:09:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：桃子 好困</p><p>【新智元导读】小扎痛失老将！PyTorch创始人之一今早官宣离职，加入Meta十一年，一手打造出响彻AI界的PyTorch。如今，离职原因也很纯粹：不愿余生只与PyTorch绑定，去开启下一个新篇章。</p><p>刚刚，PyTorch创始人Soumith Chintala官宣，将于11月17日离职Meta！</p><p>原因很简单，不想一辈子搞PyTorch。</p><p>另一个原因是，正好休长假回来，Soumith发现项目没有自己也转得挺好，这恰恰是转型最佳时机。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384096" alt="" title=""/></p><p>Soumith加入Meta十一年，八年领导PyTorch项目，一手将它带大，如今AI圈超90%的人都在用。</p><p>可以说，Soumith整个职业生涯全在这儿了。</p><p>他发文表示，「要说离开，真是我人生里最难下的决定之一。不过，现在心怀圆满，无憾离去」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384097" alt="" title="" loading="lazy"/></p><p>一时间，来自学界，以及OpenAI、特斯拉、英伟达、Hugging Face等大厂和初创的AI大牛们，纷纷向Soumith送上告别与祝福。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384098" alt="" title="" loading="lazy"/></p><p>Karpathy称，自己经历过那么多ML框架的折腾，每次都得几乎重写所有代码，但从Torch转向PyTorch至今，是体验最愉快、最久的。</p><p>在那个20维的目标和约束设计空间中，它无疑碰到了时代的「金矿」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384099" alt="" title="" loading="lazy"/></p><p>LeCun也深切地送上了祝福。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384100" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384101" alt="" title="" loading="lazy"/></p><p><strong>一封离别信</strong></p><p>为了好好道别这十一年来的职业生涯，PyTorch创始人Soumith Chintala特意写了一封离别信——</p><p>告别Meta与PyTorch</p><p>终于到了这个时候…</p><p>2025年11月6日</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384102" alt="" title="" loading="lazy"/></p><p>如今，PyTorch已能处理百亿亿次级别的训练任务，为那些正在重新定义智能的基础模型提供着动力。</p><p>几乎每一家顶尖的AI公司都在生产环境中使用它。从麻省理工到印度的乡村，它的身影遍布各地课堂。</p><p>我曾梦想让那些工具触手可及？如今它们做到了。我曾想降低行业的入门门槛？如今这门槛已几乎不复存在。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384103" alt="" title="" loading="lazy"/></p><p>当然，需要说明的是，前方仍有长路要走。只要AI还在以惊人的速度发展，PyTorch就需要不断追赶。</p><p>但我们不应因执着于未来，而忘记了我们已经取得的辉煌成就。</p><p>致所有与我并肩作战的伙伴们——你们相信研究应充满乐趣，工具应优雅美观，开源能改变世界——谢谢你们。这不是我一个人的征程，而是我们共同的旅程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384104" alt="" title="" loading="lazy"/></p><p><strong>下一站去哪？</strong></p><p>我的下一站是哪里？去做一些小而新的事，一些我尚未完全理解的事，一些会让我感到不适的事。</p><p>我本可以调去Meta内部的其他岗位，但我需要去看看外面的世界。我需要重新开始，去做一些小事。</p><p>如果不去尝试Meta以外的世界，我将永远活在「要是当初如何如何」的遗憾中，这是我无法接受的。</p><p>离开，真的很难。</p><p>我或许坐拥AI行业中最有影响力的位置之一：我领导着驱动整个AI产业的软件层。各大AI公司和硬件厂商的巨头，我都能随时联系到。这种影响力确实难以割舍。但最终，我的好奇心占了上风。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384105" alt="" title="" loading="lazy"/></p><p>请继续让AI变得酷炫又好用。我会一直关注着。也许会提提issue。但绝对会继续参与其中。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384106" alt="" title="" loading="lazy"/></p><p><strong>PyTorch会更好吗？</strong></p><p>我不想一辈子都扑在PyTorch上。我不想像Guido或Linus那样，几十年来被一件事绑得死死的。</p><p>去年十一月，恰逢我女儿出生，我便开始与Aparna筹划我的退出。我的目标是，在我离开时，能让PyTorch处于一个良好而稳定的状态。</p><p>到了今年八月，在我休第二段育儿假的时候，我知道时机已到：Edward、Suo、Alban、Greg、John、Joe和Jana都已准备就绪。</p><p>团队面临着棘手的人员、产品、技术和组织问题，但他们（不像过去那样）并没有觉得需要指望我来解决。</p><p>他们为PyTorch大会打造的产品路线清晰一致——真的非常清晰一致。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384107" alt="" title="" loading="lazy"/></p><p>那些我曾亮起红灯的问题，如今也已步入正轨。这个项目不再需要我了。</p><p>与2020-2022年（当时我卸任去做机器人项目，后来因Lin、Dima和Dwarak离职而回归）不同，这一次，我坚信PyTorch真正有了韧性。</p><p>PyTorch文化的核心传承者——Greg、Alban、Ed、Jason和Joe如今都已坐镇决策席，而与他们价值观高度一致的Suo、John和Jana也加入了进来。</p><p>并且，还有一长串价值观同样契合的人才储备，一旦有人离开，他们随时愿意补上。</p><p>有许多细节让我对这个团队充满信心：John在Julia和开源领域深耕多年（实际上，我们在2015年就一起写过一个Torch.jl），Suo是我过去两年里最强大的系统构建者和战略伙伴，而Jana长期致力于弹性核心系统的研发，过去几个月我与她的多次深入技术和组织探讨也让我倍感安心。</p><p>而2025年的产品线和执行力，将足以打消任何剩余的疑虑。</p><p>我相信，这支PyTorch团队将会取得非凡的成就。PyTorch的风格或许会因为我不再从顶层施加个人品味而有所改变，但我确信，它的核心价值观将保持不变，产品也必将继续出色。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384108" alt="" title="" loading="lazy"/></p><p><strong>我在Meta的时光</strong></p><p>FAIR早期的岁月简直美妙绝伦。我曾是一个由顶尖天才组成的小家庭的一员，我们一起开诚布公地构建着最前沿的AI。</p><p>从与Emily Denton、Rob Fergus、Leon Bottou、Martin Arjovsky以及（如今已是传奇的）Alec Radford合作研究 GAN，到与Gabriel Synnaeve一起构建星际争霸机器人，再到与Howard Mansell共同搭建第一个FAIR集群，以及与Adam Lerer和Piotr Dollar携手进行物体检测，直到最后构建PyTorch。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384109" alt="" title="" loading="lazy"/></p><p>那段时光的乐趣难以言表。2015和2016年或许是我一生中效率最高、职业生涯最愉快的两年。我大概会用一生去回味那段浪漫的岁月。</p><p>我刚加入FAIR时，有严重的「冒名顶替综合征」，头三个月过得异常艰难。我必须不遗余力地感谢Andrew Tulloch，他是我遇到过最体贴、最善良、最热情的导师，没有他，我不可能坚持下来。</p><p>单单是他重回Meta这一点，就让我对公司的未来极其看好。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384110" alt="" title="" loading="lazy"/></p><p>我在PyTorch的时光是特别的。</p><p>我热爱构建它的方方面面——设计、管理、担任产品经理、技术负责人、沟通主管、文档工程师、发布工程师、修复bug、推动增长、与数百人协力将其打造成一个连贯的产品，再到将其移交给行业利益相关方——我享受这整个过程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384111" alt="" title="" loading="lazy"/></p><p><strong>致谢所有人</strong></p><p>致Meta的PyTorch核心团队：工程师、研究员、开源维护者、文档作者、CI基础设施同仁、硬件合作伙伴以及社区建设者们。致Meta内外成百上千的同仁们——谢谢你们。是你们将一个程序库变成了一场运动。</p><p>需要感谢的人太多太多，但我不能不提Adam Paszke, Sam Gross, Greg Chanan, Joe Spisak, Alban Desmaison, Edward Yang, Richard Zou, Tongzhou Wang, Francisco Massa, Luca Antiga, Andreas Köpf, Zach DeVito, Zeming Lin, Adam Lerer, Howard Mansell 和 Natalia Gimelshein。还有Schrep。是他们让项目得以成功发布。</p><p>后来，又有更多人成为了中流砥柱：Lu Fang, Xiaodong Wang, Junjie Bai, Nikita Shulga, Horace He, Mark Saroufim, Jason Ansel, Dmytro Dzhulgakov, Yangqing Jia, Geeta Chauhan, Will Constable, Briah Hirsh, Jane Xu, Mario Lezcano, Piotr Balecki, Yinghai Lu, Less Wright, Andrew Tulloch, Bruce Lin, Woo Kim, Helen Suk, Chris Gottbrath, Peng Wu, Joe Isaacson, Eli Uriegas, Tristan Rice, Yanan Cao, Elias Ellison, Animesh Jain, Peter Noordhuis, Tianyu Liu, Yifu Wang, Lin Qiao以及其他数百位同仁。</p><p>如果不能在此将所有应提及的人一一列出，我将深感愧疚。没有你们，PyTorch将一无是处❤️。</p><p>打造PyTorch最快乐的瞬间，莫过于遇见那些热切分享着喜悦、热爱和反馈的用户。我记得在2017年的NeurIPS大会上，一位研究生走到我面前，激动得声音都有些颤抖，他说他为了自己的研究苦苦挣扎了三年，但在使用PyTorch的三个月里就取得了巨大进展，足以顺利毕业。</p><p>那一刻让我真切地感受到，我们所做的一切意义非凡，它对许许多多的人都至关重要，即便我们不常听到他们的声音。</p><p>我确实怀念PyTorch社区曾经的亲密无间，那时的三百人大会就像一场大型家庭聚会。</p><p>但考虑到PyTorch今日所产生的巨大影响，我认为这是微小的代价——是的，如今的大会已有三千人规模，能够影响市场格局的合作也在这里达成，但它正帮助着数量级更多的人们完成他们最出色的AI工作。我怀念那份亲密，但更为这份成长感到自豪。</p><p>致Mark Zuckerberg和Mike Schroepfer，你们坚信开源至关重要，并且是一项稳健的商业战略。这一点在商业进程中极难被多数人理解，但我们却在这个战略上步调完全一致，甚至无需商讨。没有你们二位，就不会有FAIR，也不会有PyTorch。而这两者对我而言，意义非凡。</p><p>致Yann LeCun和Rob Fergus，感谢你们打造了我所深深敬仰的那个美妙的早期FAIR。</p><p>致Aparna Ramani，我在Meta见过的最出色的领导者之一，你为团队设立了极高的标准，才华横溢，能在同一次对话中游刃有余地探讨底层基础设施系统和行业战略，并且是一位绝对的执行力超人！我从你身上学到了太多。</p><p>致Santosh、Kaushik、Delia、Oldham和Ben，感谢你们如此热忱地欢迎我加入基础设施部门。对于我这个来自文化迥异的FAIR的人来说，是你们让我感受到了家的温暖，让我融入了这个大家庭，谢谢你们。</p><p>致所有在PSC这场视频游戏中一路支持我的经理们——Serkan, Howard, Jerome, Abhijit, Yoram, Joelle, Aparna和Damien——我欠你们一辈子的酒。</p><p>就此暂别。</p><p>—Soumith</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384112" alt="" title="" loading="lazy"/></p><p><strong>十一年老将，从零铸PyTorch</strong></p><p>Soumith Chintala在印度海得拉巴长大，现居纽约。</p><p>他毕业于纽约大学（NYU）和韦洛尔理工学院（VIT Vellore）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384113" alt="" title="" loading="lazy"/></p><p>此前，他在Meta和纽约大学工作，专注于AI基础设施、AI研究和机器人学。</p><p>可以看到，从2014年入职后，Soumith在Meta就像升级打怪一样，不断晋级到VP级别。</p><p>最核心的角色，当属联合创立PyTorch。</p><p>他还在PyTorch和Torch论坛上回答了数千个问题，为此投入了生命中相当大一部分时间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384114" alt="" title="" loading="lazy"/></p><p>个人的研究领域曾涉及物体与行人检测、图像、视频的生成式建模、游戏AI，以及机器学习系统研究。</p><p>在2012年前，他还曾与Pierre Sermanet和Yann LeCun一同维护EBLearn，一个基于C++的深度学习框架。</p><p>在图像生成及其他 AI 领域，Soumith发表了多篇高被引AI研究论文，其中包括名声大噪的GAN。</p><p>作为合著者，他发表了三篇高被引论文：LAPGAN、DCGAN和Wasserstein GAN。</p><p>此外，Soumith还参与了NYU家庭机器人项目，希望打造一个处理各种家务的家庭机器人。</p><p>而他的目标，就是要构建一个世界模拟器，让机器人在「脑海」中推演各种场景并选择最优解。</p><p>目前，他和合作者Lerrel Pinto一起，使用名为Hello Robot | Stretch机器人，取得了一些成就：</p><p>其中包括，机器人通用模型、CLIP-Fields、Holo-Dex等。</p><p>在投资领域，Soumith的投资大多在个人的社交圈内——主要是当朋友们创业时，除此之外很少投资。投资过的公司包括Runway、1X、Osmo、Anthropic、Together.ai、Lepton等。</p>]]></description></item><item>    <title><![CDATA[什么是SSL证书？ 冷冷的炒面 ]]></title>    <link>https://segmentfault.com/a/1190000047384015</link>    <guid>https://segmentfault.com/a/1190000047384015</guid>    <pubDate>2025-11-10 10:08:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一个生动的比喻：门锁与身份证</strong><br/>如果把网站服务器比作一个存放重要物品的房间，那么SSL证书就是这个房间的“智能门锁+身份证”组合。当你想安全地访问这个房间时，SSL证书同时完成两件事：第一，验证这个房间确实是你要找的正规房间（身份认证）；第二，为你们的对话加上一把坚固的锁（加密传输），防止被窃听!<br/><img width="700" height="400" referrerpolicy="no-referrer" src="/img/bVdmYUq" alt="" title=""/></p><p><strong>SSL证书的核心作用</strong></p><ol><li>身份认证 就像我们通过身份证确认对方身份一样，SSL证书由受信任的第三方机构（CA）颁发，确保证书持有者的身份是真实可信的。当你访问网站时，浏览器会检查SSL证书的有效性，如果是伪造的，就会发出安全警告。</li><li>数据加密 SSL证书在你的浏览器和网站服务器之间建立加密通道，将所有传输的数据变成“密文” 。即使数据被截获，黑客看到的也是一堆乱码，无法读取真实内容。</li><li>数据完整性 确保传输过程中的数据不被篡改。就像快递包裹的密封条，如果中途被拆开修改，接收方立刻就能发现。 <br/>申请入口 直接访问JoySSL，注册一个账号记得填写注册码230973获取技术支持<br/><a href="https://link.segmentfault.com/?enc=8vwSv9FUnrsdJzE3E7q%2FRw%3D%3D.tQjlzhh3jzR1stEE%2Fc4JjPrkDbq%2BKW%2BGuXRcfM%2Bhzjla5UArMIjiFcBHIBPEZlPXvDQOOiZUCiUg5LMKc1%2Bj1bescpWGp8VNQ%2BA7%2FRjldUg%3D" rel="nofollow" target="_blank">申请入口</a><br/><strong>如何识别SSL证书？</strong><br/>看浏览器提示： 网址以“https:// ”开头（多出的“s”代表安全） 地址栏显示锁形图标 部分高级证书会使地址栏变成绿色或显示公司名称</li></ol><p>警惕无证书网站： 如果浏览器提示“不安全”或出现警告三角，意味着该网站没有SSL证书保护，在此类网站输入密码或银行卡信息风险极高。</p><p><strong>SSL证书的应用场景</strong></p><p>必须使用SSL的场合： 所有登录页面：保护用户名和密码 在线支付：保障银行卡信息安全 个人信息提交：身份证号、电话号码等敏感数据 企业邮箱：防止商业机密泄露</p><p>现代网络新标准： 如今，几乎所有正规网站都部署了SSL证书，不仅是金融类网站，连普通博客、新闻网站也都启用HTTPS加密，这已成为网络安全的基准配置。</p><p><strong>为什么每个人都应该关心SSL证书？</strong></p><p>保护隐私安全： 在使用公共WiFi等不安全网络时，SSL证书有效防止“中间人攻击” ，避免你的聊天记录、浏览历史被窃取。</p><p>提升信任度： 对于网站所有者来说，部署SSL证书显著增强用户信任感，同时还是搜索引擎排名的重要因素。</p><p>总结 SSL证书不是可有可无的技术选项，而是现代互联网的基础安全设施。它如同网络世界的“安全卫士”，默默守护着每一次数据交换的安全。</p>]]></description></item><item>    <title><![CDATA[The Life of a Read/W]]></title>    <link>https://segmentfault.com/a/1190000047384021</link>    <guid>https://segmentfault.com/a/1190000047384021</guid>    <pubDate>2025-11-10 10:08:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Apache Iceberg 作为开源分层表格式，通过解耦数据与元数据管理，为大数据处理提供事务性与可扩展性。本文以读写查询生命周期为核心，解析 Iceberg 表的处理逻辑。从 “Apache Iceberg 101” 基础概念出发，阐述查询引擎对读写请求的解析流程：元数据查询获取表结构与文件布局，扫描执行实现数据过滤与结果返回，结合时间旅行特性访问历史数据。写入部分聚焦插入、删除、upsert / 合并操作的底层实现，揭示 Iceberg 借助事务日志与文件管理保障数据一致性的机制。全文为开发者呈现 Iceberg 高效管理数据读写生命周期的全流程视角，助力理解其在大数据场景中的应用价值。</p><p>原文：<a href="https://link.segmentfault.com/?enc=GJo8TIbuoogQrdhKn9DGjQ%3D%3D.zksXTbLfVTmxOSZRRzmEdvBijIZJT2iTbWo3vGzsfC2Oxw2odpBXT6NIkMYcXdeDsMjsMmYRKHfnhSafEVPvCoNK3NVrg4Og3wzGC%2FFgRPQ%3D" rel="nofollow" target="_blank">https://www.dremio.com/blog/the-life-of-a-read-query-for-apac...</a></p><h4>Apache Iceberg 101</h4><p>存储层的结构如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384024" alt="图片" title="图片"/><br/>Data LayerData files – Parquet or ORC 的存储格式，真实数据放这里Delete files – 被认为是删除的数据Metadata LayerManifest files – 一个快照的子集，这些文件用于追踪这些子集中独立的数据文件，以及进一步的元数据裁剪Manifest lists – 定义一个表的快照，列出构成这个快照的所有 manifest文件，以及进一步裁剪的元数据Metadata files – 定义表，跟踪 manifest lists，当前以及前一个快照，schemas，以及 分区 schemasThe Catalog这个指定当前的元数据文件，可以提供类似数据库的事务保证，以及元数据存储功能官网的描述信息Snapshots 格式细节描述Manifests 格式细节描述Table MetadataJSON serialization</p><h4>How a Query Engine Processes the Query</h4><p>基于order表的查询，表结构<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384025" alt="图片" title="图片" loading="lazy"/><br/>查询语句<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384026" alt="图片" title="图片" loading="lazy"/></p><h4>Metadata Qeruy</h4><p>查询首先被提交到 engine，然后被解析，engine 需要知道这个表的元数据，才能做plan<br/>查询元数据，四个步骤执行过程如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384027" alt="图片" title="图片" loading="lazy"/><br/>第一步，获取 catalog，根据 catalog 获取 orders 表的当前元数据文件第二步，通过 order 表的最新的元数据，可以拿到表的 schema表的分区信息，之后在 plan 中可以做裁剪当前快照的 “manifest list” 可以知道 需要再进一步扫描哪些文件第三步，通过 manifest list 文件，我们可以再进一步partition-spec-id，可以拿到分区的 schema信息，当前的 order 只有一个分区每个分区还包含了一些统计信息，通过 lower 和 upper 边界，可以可以确定 是否要跳过这个分区当后面拿到 manifest file 时候还可以再做进一步的 skip<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384028" alt="图片" title="图片" loading="lazy"/><br/>第四步，根据每个 manifest file 文件，可以继续做裁剪，然后获取每个文件对应的 dataschema-id， partition-id 包含的就是数据文件信息内容的类型，比如可以跳过 delete类型的数据列中包含了 value内容、唯一值、lower、upper信息，可以做裁剪<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384029" alt="图片" title="图片" loading="lazy"/></p><h4>Performing the Scan and Returning the Results</h4><p>经过上面几次 裁剪后，真正要扫描的数据文件就很少了<br/>parquet 文件本身也包含 min/max信息，还可以继续在文件级别继续 裁剪<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384030" alt="图片" title="图片" loading="lazy"/></p><h4>Time Travel</h4><p>查询前一个版本的信息<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384031" alt="图片" title="图片" loading="lazy"/><br/>这里使用了 TIMESTAMP AS OF ，用作 time travel 用的<br/>完整的执行过程如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384032" alt="图片" title="图片" loading="lazy"/><br/>通过最新的 快照，可以：通过比较快照 id，或者 快照的 timestamp，找到 AS OF 之前的快照获取这个表的 schema获取查询分区，用于裁剪，以及目标快照的 manifest list<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384033" alt="图片" title="图片" loading="lazy"/><br/>跟普查查询类似，找到 manifest list 文件通过 partition-spec-id，可以知道分区信息分解分区的统计信息，lower 和 upper做进一步裁剪在这里可以 skip 大量不需要的分区 <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384034" alt="图片" title="图片" loading="lazy"/><br/>对于 manifest filesschema-id、partition-id 对应于数据文件跳过 delete 类型的文件根据 统计信息，如 lower、upper，做裁剪<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384035" alt="图片" title="图片" loading="lazy"/><br/>接下来就是 scan 数据文件，因为前面已经 skip 了大量的文件，此时需要扫描的文件就少很多了<br/>parquet文件本身也可以做谓词下推，进一步减少读取的数量<br/>整个过程，其实跟普通的读取差不多How a Query Engine Processes the Write</p><h4>Insert</h4><p>查询语句<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384036" alt="图片" title="图片" loading="lazy"/><br/>执行过程查询被提交到 engine，解析，之后查询引擎开始解析获取 catalog，拿到 schema，尽管只是append，但要确认两件事确保满足表的 schema，以及哪些字段不能为 null获取表的分区，确定如何组织写入的数据写入数据因为是 insert，所以不会对已有文件造成影响，只要写入新文件即可写入时根据表的分区 schema信息做写入如果设置了排序顺序，并且engine 也支持，则写入也会按照指定要求写入写入元数据写入每个 manifest file 文件，每个文件包含了数据文件的具体路径，每个列的统计信息等将本地快照中新增的 manifest file，已经存在的 manifest file，一起写入到 新的 manifest list 中在这个 manifest list中，包含了所有 manifest file 的路径，以及每个文件的信息，如每个分区的 lower和upper等创建一个 metadata file用来汇总表信息，包含文件路径，以及 manifest list 信息等提交engine 再一次获取 catalog，以确保在写入的同时，没有新的快照出现这是为了防止并发写入冲突当出现冲突时第一个写入成功，后面的写入者会重试 3 或者 4，直到成功或者 重试失败退出读取的时候总是拿到最新的快照，所以并发写入还没有提交时不会有影响</p><h4>Delete</h4><p>语句<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384037" alt="图片" title="图片" loading="lazy"/><br/>执行过程发送请求到 engine，准备解析获取 catalog拿到最新的 metadata file根据表的分区信息，确定如何写入数据获取当前的 序列ID，事务序列ID， 假设在完成之前没有其他的写入冲突，利用了 OCC 机制写文件写策略取决于表的删除策略：“copy-on-write” (COW) or “merge-on-read” (MOR)如果是 copy-on-write通过元数据，识别出哪些文件包含删除的数据完整的读取这些文件，确定删除的内容写一个新文件替换这个快照中的文件，新文件中不包含被删除的内容如果是 merge-on-write 避免了重写数据文件，但包含了墓碑的删除数据文件，读取时需要进一步处理，包含两种删除Position deletesEquality deletesPosition deletes通过扫描元数据，确定哪些文件被删除通过扫描数据文件，确定要删除的记录位置在分区中写一个删除类型的文件，记录了哪些记录是按位置从 哪些文件中删除的<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384038" alt="图片" title="图片" loading="lazy"/><br/>Equality deletes不需要扫描任何文件，只需要写一个删除类型的文件，列出哪些行中的值要被删除在读取时，根据删除类型文件和分区中的数据记录做重建，这个操作代价很高<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384039" alt="图片" title="图片" loading="lazy"/><br/>原始的文件仍然被保留，当后续读取时，会根据 删除类型文件，旧文件做合并处理写元数据文件开始写入 manifest file，每个文件包含 数据文件的路径，以及文件的 列统计信息，lower和upper等将新的 和存在的 manifest files文件一起当做一个快照，写入到 manifest list 中在新的 mainfest list中包含所有 manifest file 的路径，以及统计信息，如增加了多少数据，lower、upper等再写入 metadata file，记录表的汇总信息，包括manifest list的路径和相信信息提交记录再次获取 catalog，同时确保在写入时候，没有出现新的快照同样也是为了预防并发冲突的，失败会继续重试读取总是拿到最新的快照，正在写入的不会影响读取Upsert/Merge<br/>语句<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047384040" alt="图片" title="图片" loading="lazy"/><br/>执行过程解析查询，准备建立查询计划检查 catalog校验表的 schema，确认非空字段，获取分析信息获取当前的 序列ID，事务序列ID，使用 OCC 做并发控制写数据文件同时从源、目标读取数据文件放入内存中，然后做匹配，比如根据 id 做比较，copy-on-write，只要匹配更新任何一行，整个文件都需要读取，重写merge-on-read，生成新的更新/删除/插入 数据文件，读取时会忽略旧的文件，只有更新的数据在内存中，占用内存会减少如果 WHEN MATCHED 匹配，则做更新，则做插入操作写元数据跟之前的一样，写入 manifest files，以及数据文件路径、统计信息当前快照中已经存在的 manifest file，跟新的一起，写入到 manifest list中，以及manifest file 的路径和统计信息写入metadata file汇总表信息，包含 manifest list路径和信息等提交再次获取 catalog信息，确保这段时间没有写冲突，如果有冲突则继续重试利用了OCC 做并发控制，快照隔离级别，串行化隔离级别，来确保 ACID 事务</p><h4>Reference</h4><p>The Life of a Write Query for Apache Iceberg TablesMaintaining Iceberg Tables – Compaction, Expiring Snapshots, and MoreHow Z-Ordering in Apache Iceberg Helps Improve PerformanceWhat Is Apache Iceberg? Features &amp; BenefitsLakehouse Trifecta — Delta Lake, Apache Iceberg &amp; Apache HudiIceberg Table SpecYoutub: CHUG Talks: Introduction to Apache IcebergRow-Level Changes on the Lakehouse: Copy-On-Write vs. Merge-On-Read in Apache IcebergOptimistic concurrency controlHow Netflix uses eBPF flow logs at scale for network insight</p>]]></description></item><item>    <title><![CDATA[如何在 macOS 上安装和配置 Red]]></title>    <link>https://segmentfault.com/a/1190000047384067</link>    <guid>https://segmentfault.com/a/1190000047384067</guid>    <pubDate>2025-11-10 10:07:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047384069" alt="Install and Configure Redis on macOS" title="Install and Configure Redis on macOS"/></p><p>Redis 是一个免费的内存数据存储，用作 message broker、数据库、缓存等。它支持不同的数据类型，比如字符串，哈希，列表、集合等，对许多应用程序都很有用。</p><p>本指南将向您展示如何在 macOS 上安装和设置 Redis 系统，以提高您的应用程序的性能。</p><h3>Step 1: Install Redis</h3><p>输入如下命令更新 Homebrew</p><pre><code>brew update</code></pre><p>输入以下命令安装 Redis</p><pre><code>brew install redis</code></pre><h3>Step 2: Configure Redis</h3><p>可以编辑 Redis 配置文件来更改内存使用、日志记录和数据存储等设置。</p><pre><code>sudo nano /usr/local/etc/redis.conf</code></pre><p>使用默认配置启动 Redis</p><pre><code>redis-server</code></pre><p>使用指定配置启动 Redis</p><pre><code>redis-server /usr/local/etc/redis.conf</code></pre><h3>Step 3: Manage Redis as a Background Service</h3><p>将 Redis 作为后台服务启动，请运行如下命令：</p><pre><code>brew services start redis</code></pre><p>要停止 Redis 服务，请运行如下命令：</p><pre><code>brew services stop redis</code></pre><p>重新启动 Redis 服务，请运行如下命令：</p><pre><code>brew services restart redis</code></pre><h3>Step 4: Test Redis Installation</h3><p>要测试你的 Redis 安装，使用 Redis 命令行界面（CLI）运行：</p><pre><code>redis-cli</code></pre><p>该命令将连接到本地 Redis 服务器，你现在可以发出命令与 Redis 交互，比如：</p><pre><code>set mykey "Hello, Redis!" 
get mykey</code></pre><h3>Step 5: Secure Redis (Optional)</h3><p>默认情况下，Redis 不需要身份验证，您可以修改 Redis 配置文件启用密码认证。</p><pre><code>sudo nano /usr/local/etc/redis.conf</code></pre><p>找到以“# requirepass”开头的行，取消注释，并设置一个安全密码：</p><pre><code class="conf">requirepass your_secure_password</code></pre><p>保存并关闭文件，重启 Redis 应用更改。</p><pre><code>brew services restart redis</code></pre><p>当用 CLI 连接到 Redis 时，你现在需要提供密码：</p><pre><code>redis-cli -a your_secure_password</code></pre><h3>我的开源项目</h3><p><a href="https://link.segmentfault.com/?enc=3vQpSULXcMHQp7n0FIXBsw%3D%3D.oLHlnC5nZiA5nK1zERRZ6ZYDyRi%2FJpFIT%2FsD9Eq8Otg%3D" rel="nofollow" target="_blank"><img referrerpolicy="no-referrer" src="/img/remote/1460000043302459" alt="酷瓜云课堂-在线教育解决方案" title="酷瓜云课堂-在线教育解决方案" loading="lazy"/></a></p><ul><li><a href="https://link.segmentfault.com/?enc=5dCksLarS3%2FXfYhrpYMfhw%3D%3D.QxHow675xzubS2VZPd%2FClmFZAn0KK0cCQGhD5AL6XdmgIbSIj0BmrsDAN7cNjoXk" rel="nofollow" target="_blank">course-tencent-cloud（酷瓜云课堂 - gitee仓库）</a></li><li><a href="https://link.segmentfault.com/?enc=4z69C5fgC98%2FtNQkJrpNlQ%3D%3D.VSjw4lE%2BV0C1j%2BbL6%2BqU4i7vu7KfQsS5gBZT%2Bf%2B8NjcU8qoa8EEgbEhsq0bkXWFGPgkFiyAf97%2Fp4MKpCPUbnQ%3D%3D" rel="nofollow" target="_blank">course-tencent-cloud（酷瓜云课堂 - github仓库）</a></li></ul>]]></description></item><item>    <title><![CDATA[告别手动修改：使用 Spire.PDF ]]></title>    <link>https://segmentfault.com/a/1190000047384079</link>    <guid>https://segmentfault.com/a/1190000047384079</guid>    <pubDate>2025-11-10 10:06:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化办公日益普及的今天，PDF 文档因其跨平台、内容固定的特性，成为企业合同、报告、发票等各类文档的首选格式。然而，随之而来的挑战是，当需要批量修改 PDF 文档中的特定文本内容时（例如，统一更新公司名称、产品型号，或纠正报告中的错误信息），手动逐一修改不仅效率低下，而且极易引入人为错误。这种重复性工作不仅耗时费力，更可能成为业务流程中的瓶颈。</p><p>面对这一痛点，我们急需一种高效、准确的自动化解决方案。本文将聚焦于如何利用 <strong>Java 编程语言</strong>，结合功能强大的第三方库 <strong>Spire.PDF for Java</strong>，实现 PDF 文档中文本的自动化替换，从而将你从繁琐的手动工作中解放出来。</p><h2>为什么选择 Spire.PDF for Java？</h2><p><strong>Spire.PDF for Java</strong> 是一款专业的 Java PDF 组件，它提供了丰富的功能，包括 PDF 的创建、读取、编辑、转换、打印等。在 PDF 文本处理方面，Spire.PDF for Java 展现出其独特的优势：</p><ul><li><strong>功能全面：</strong> 不仅支持简单的文本替换，还能处理复杂的文本查找模式（如正则表达式）。</li><li><strong>易用性：</strong> 提供直观的 API 接口，开发者可以快速上手并集成到现有项目中。</li><li><strong>高性能：</strong> 针对大型 PDF 文档也具有良好的处理速度。</li><li><strong>兼容性强：</strong> 支持广泛的 PDF 标准和特性。</li></ul><p>要开始使用 Spire.PDF for Java，首先需要在你的 Maven 项目中添加相应的依赖。</p><p><strong>Maven 依赖配置：</strong></p><pre><code class="xml">&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;e-iceblue&lt;/id&gt;
        &lt;name&gt;e-iceblue&lt;/name&gt;
        &lt;url&gt;https://repo.e-iceblue.cn/repository/maven-public/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;e-iceblue&lt;/groupId&gt;
        &lt;artifactId&gt;spire.pdf&lt;/artifactId&gt;
        &lt;version&gt;12.11.0&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre><hr/><h2>核心步骤：替换 PDF 文本的实战指南</h2><p>使用 Spire.PDF for Java 替换 PDF 文本，主要可以分解为以下几个清晰的步骤：</p><ol><li><strong>加载 PDF 文档：</strong> 使用  PdfDocument  类加载目标 PDF 文件。</li><li><strong>遍历页面：</strong> 获取文档中的所有页面，因为文本替换通常需要逐页进行。</li><li><strong>创建文本替换器：</strong> 为每个页面创建  PdfTextReplacer  实例。</li><li><strong>设置替换选项（可选）：</strong> 配置替换规则，例如是否区分大小写、是否全词匹配等。</li><li><strong>执行文本替换：</strong> 调用替换方法查找并替换文本。</li><li><strong>保存修改后的 PDF：</strong> 将更改保存到新的 PDF 文件或覆盖原文件。</li></ol><p>以下是一个完整的 Java 代码示例，演示如何将 PDF 中所有出现的“旧公司名称”替换为“新公司名称”。</p><pre><code class="java">import com.spire.pdf.PdfDocument;
import com.spire.pdf.PdfPageBase;
import com.spire.pdf.general.find.PdfTextFind;
import com.spire.pdf.general.find.PdfTextReplacer;
import com.spire.pdf.general.find.PdfTextReplaceOptions;
import com.spire.pdf.general.find.ReplaceActionType;

import java.util.EnumSet;

public class ReplacePdfText {
    public static void main(String[] args) {
        // 1. 加载 PDF 文档
        PdfDocument pdf = new PdfDocument();
        pdf.loadFromFile("input.pdf"); // 替换为你的输入文件路径

        // 2. 遍历文档中的每个页面
        for (int i = 0; i &lt; pdf.getPages().getCount(); i++) {
            PdfPageBase page = pdf.getPages().get(i);

            // 3. 创建 PdfTextReplacer 实例
            PdfTextReplacer replacer = new PdfTextReplacer(page);

            // 4. 设置替换选项（可选）
            PdfTextReplaceOptions options = new PdfTextReplaceOptions();
            // 设置为全词匹配，避免替换部分单词
            options.setReplaceType(EnumSet.of(ReplaceActionType.WholeWord));
            // 设置为不区分大小写
            options.setReplaceType(EnumSet.of(ReplaceActionType.IgnoreCase));
            replacer.setOptions(options);

            // 5. 执行文本替换
            // 将 "旧公司名称" 替换为 "新公司名称"
            replacer.replaceAllText("旧公司名称", "新公司名称");
            System.out.println("页面 " + (i + 1) + " 处理完成。");
        }

        // 6. 保存修改后的 PDF
        pdf.saveToFile("output.pdf"); // 替换为你的输出文件路径
        pdf.close(); // 关闭文档，释放资源
        System.out.println("PDF 文本替换完成，文件已保存为 output.pdf");
    }
}</code></pre><p>为了更直观地理解 Spire.PDF for Java 在文本替换中的关键方法，这里提供一个简要的表格：</p><table><thead><tr><th align="left">方法名称</th><th align="left">描述</th><th align="left">关键参数</th></tr></thead><tbody><tr><td align="left">PdfDocument</td><td align="left">加载和操作 PDF 文档的入口</td><td align="left">String filePath  (文件路径)</td></tr><tr><td align="left">getPages()</td><td align="left">获取 PDF 文档中的所有页面集合</td><td align="left">无</td></tr><tr><td align="left">replaceAllText</td><td align="left">查找并替换页面中所有匹配的文本</td><td align="left">String originalText ,  String newText</td></tr><tr><td align="left">PdfTextReplacer</td><td align="left">负责执行页面内的文本查找和替换操作</td><td align="left">PdfPageBase page  (目标页面)</td></tr><tr><td align="left">setOptions</td><td align="left">设置文本替换的规则，如是否区分大小写等</td><td align="left">PdfTextReplaceOptions options</td></tr><tr><td align="left">saveToFile</td><td align="left">保存修改后的文档到指定路径</td><td align="left">String outputPath  (输出文件路径)</td></tr><tr><td align="left">close()</td><td align="left">关闭 PDF 文档并释放相关资源</td><td align="left">无</td></tr></tbody></table><hr/><h2>进阶技巧与注意事项</h2><ul><li><strong>替换特定页码的文本：</strong> 如果你只想替换特定页码的文本，只需修改循环条件，或者直接通过  pdf.getPages().get(pageIndex)  获取指定页面。</li><li><strong>处理字体、样式等属性：</strong> Spire.PDF for Java 在替换文本时，通常会尽力保留原有文本的字体、大小、颜色等样式。但在某些复杂情况下，如果新文本的长度与旧文本差异过大，可能会导致布局微调。</li><li><strong>使用正则表达式替换：</strong>  PdfTextReplacer  也支持通过正则表达式进行高级文本查找和替换，这对于匹配复杂的文本模式（如日期、邮箱地址等）非常有用。你可以使用  replaceAllText(Pattern pattern, String newText)  方法。</li><li><strong>性能优化：</strong> 对于包含大量页面或复杂内容的 PDF 文件，处理时间可能会较长。建议在批处理时，考虑多线程处理，或在内存允许的情况下，一次性加载文档，避免重复加载。</li><li><strong>错误处理：</strong> 在实际项目中，务必加入  try-catch  块来处理可能发生的  IOException  或其他异常，确保程序的健壮性。</li></ul><hr/><h2>总结</h2><p>通过本文的介绍，我们看到 Spire.PDF for Java 为 Java 开发者提供了一个强大且易于使用的解决方案，能够高效地实现 PDF 文档中的文本替换。它不仅解决了手动修改 PDF 的效率低下和易出错问题，更通过编程的方式赋予了文档处理更高的自动化和灵活性。</p><p>掌握 Spire.PDF for Java 的文本替换功能，将极大地提升你在处理批量文档时的效率，无论是自动化报告生成、合同修订，还是数据清洗，都将如虎添翼。我们鼓励读者立即动手尝试，将这一强大的工具应用到你的实际项目中。未来，Spire.PDF for Java 在 PDF 内容提取、表格处理、文档合并等更多高级功能上，也将展现出其卓越的价值，期待你在 PDF 自动化处理的道路上探索更多可能性！</p>]]></description></item><item>    <title><![CDATA[DV、OV、EV SSL证书如何选择？一]]></title>    <link>https://segmentfault.com/a/1190000047384085</link>    <guid>https://segmentfault.com/a/1190000047384085</guid>    <pubDate>2025-11-10 10:05:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当您决定为网站部署SSL证书，以开启HTTPS加密、提升安全性时，一个现实的问题便摆在眼前：市场上主流的DV、OV、EV三种证书，我究竟该选哪一种？它们之间不只是价格的差异，更是安全等级、信任表现和适用场景的不同。<br/>选择正确的SSL证书，如同为您的数字资产匹配一把最合适的“安全锁”。本文将为您清晰解析三者的区别，并提供一个简单明了的决策框架，助您做出最佳选择。</p><h3>一、 核心差异：一场关于“身份验证”的严格考试</h3><p>要理解如何选择，首先必须明白这三类证书在获取门槛和验证深度上的根本区别。</p><h4><strong>域名验证型（DV SSL）</strong>： “基础入门级”</h4><p><strong>验证方式</strong>：证书颁发机构（CA）仅验证您是否拥有该域名的管理权。验证通常通过邮箱接收验证邮件或添加一条DNS解析记录来完成，全程自动化，几分钟即可签发。<br/><strong>核心特点</strong>：验证快、成本低、只加密不验身份。它向用户证明“您与服务器之间的连接是加密的”，但无法证明网站运营者的真实身份。</p><h4><strong>组织验证型（OV SSL）</strong>： “商业标准级”</h4><p><strong>验证方式</strong>：除了验证域名所有权，CA还会严格审核申请企业的真实性。这包括核查政府备案的营业执照、企业电话、地址等信息（通常需要提供相关文件）。CA甚至会致电公司进行核实。<br/><strong>核心特点</strong>：验证严谨、周期较长（1-3天）、身份可信度更高。它不仅加密数据，还向用户证明了网站背后是一个真实存在的合法实体。</p><h4><strong>扩展验证型（EV SSL）</strong>： “顶级信任级”</h4><p><strong>验证方式</strong>：遵循全球统一的严格验证标准，是审核最为严苛的证书。除了完成OV证书的所有验证步骤，还会对申请组织的法律、物理和运营存在进行更深入的调查。<br/><strong>核心特点</strong>：最高安全等级、验证最严格、周期最长（数天至一周）、视觉信任度最高。其最显著的特征是在高版本浏览器中，地址栏会变为绿色并直接显示公司名称，这是信任的最高视觉象征。</p><h3>二、 信任表现与视觉差异：用户看到了什么？</h3><p>不同类型的证书在浏览器中给用户带来的视觉体验和信任感天差地别。<br/><strong>DV证书</strong>：显示为地址栏的“锁”形标志 + “安全”字样。用户点击小锁，只能看到“证书有效”和连接已加密，看不到任何企业信息。</p><h4><strong>OV证书</strong>：同样显示为“锁” + “安全”。但当你点击小锁查看证书详情时，可以在“颁发给”一栏中看到清晰的公司名称和组织信息。 <a href="https://link.segmentfault.com/?enc=qwDHObbbOkw7v8EyYXNpWQ%3D%3D.Gb3m8ZSp0S1XDt2feoYpm%2FzOTVn91kqQa8jAVCUr866iqGmRlS9NW%2BdzpPIw4CqpZuLBQy784RJL6H7hTLhHJg%3D%3D" rel="nofollow" target="_blank">申请入口</a></h4><p><strong>EV证书</strong>：<strong>最醒目的视觉标识。除了锁的标志，还会在地址栏直接、醒目地展示绿色的企业名称（在某些现代浏览器中可能以其他形式突出显示）。这为访问者提供了最即时、最强大的信任保证。</strong></p><h3>三、 决策指南：如何为您的情况做出最佳选择？</h3><p>了解了差异后，您可以根据自身网站的类型和核心目标，轻松做出决定。<br/>选择 DV SSL证书，如果您的网站是：<br/><strong>个人博客或网站</strong>：内容分享为主，不涉及敏感信息交易。<br/><strong>小型展示类网站</strong>：主要用于发布信息，仅有简单的联系表单。<br/><strong>测试服务器或内网服务</strong>：需要快速实现加密，但无需对外证明身份。<br/>预算有限，且核心需求仅为满足HTTPS基础要求。<br/>一句话总结：DV证书是“经济适用之选”，实现了基础的加密功能。</p><p>选择<strong> OV SSL证书</strong>，如果您的网站是：<br/><strong>所有类型的企业官网</strong>：需要向客户证明您的合法身份，建立商业信任。<br/><strong>电子商务网站</strong>：用户需要登录、提交个人信息，展示企业详情能极大增强购买信心。<br/><strong>会员门户、在线服务平台</strong>：处理用户数据，需要超越基础加密，展示对安全和身份真实性的重视。<br/>需平衡安全、信任与成本：<strong>OV证书在安全、信任和价格之间取得了最佳平衡，是企业级应用的性价比首选。</strong><br/>一句话总结：<strong>OV证书是“商务人士的标配”，在安全之上，增加了身份可信度。</strong></p><p>选择 EV SSL证书，如果您的网站是：<br/><strong>金融与支付平台</strong>：银行、证券、P2P、在线支付网关等，安全与信任是生命线。<br/><strong>大型电商平台</strong>：如天猫、京东等，需要最高级别的视觉信任来保障海量交易。<br/><strong>政府及公共事业机构</strong>：需要向公众展示其官方性和权威性，杜绝仿冒网站。<br/><strong>任何将品牌信任视为核心资产的组织</strong>：绿色的地址栏是对品牌价值最直观的投资。<br/>一句话总结：<strong>EV证书是“尊贵的身份象征”，用于打造顶级的品牌信任与安全形象</strong>。<img width="493" height="310" referrerpolicy="no-referrer" src="/img/bVdmYVx" alt="" title=""/></p><h3>结语</h3><p>选择DV、OV还是EV，答案藏在您网站的业务性质和对用户信任的追求中。请记住这个简单的决策树：<br/>追求快速加密 → 选DV<br/><strong>兼顾加密与身份信任 → 选OV（绝大多数企业的最优解）</strong><br/>追求顶级信任与品牌彰显 → 选EV</p>]]></description></item><item>    <title><![CDATA[效率翻倍2025年敏捷开发必备的10大协]]></title>    <link>https://segmentfault.com/a/1190000047384302</link>    <guid>https://segmentfault.com/a/1190000047384302</guid>    <pubDate>2025-11-10 10:05:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>引言：CIO的选型困境与破局之道</h2><p>在数字化转型深化的2025年，敏捷开发已成为企业应对不确定性的核心能力。但对于IT决策者而言，选型却陷入“嘈杂陷阱”：公有云工具（如Jira、飞书）功能全面却面临数据主权风险，私有化工具（如禅道、鼎捷）安全可控但需评估集成成本，SaaS工具（如Trello）轻量灵活却难以支撑复杂项目。本文基于长期行业观察，建立10大评估维度，客观对比10款主流工具，为政企、金融、制造等对安全与合规有高要求的行业提供决策框架。</p><h2>一、建立评估标准：5大核心维度与10项细分指标</h2><p>为避免“功能堆砌”的无效对比，我们基于企业级项目管理本质需求，提炼以下评估维度（附细分指标）：</p><table><thead><tr><th><strong>一级维度</strong></th><th><strong>关键评估点</strong></th></tr></thead><tbody><tr><td><strong>数据主权与可控性</strong></td><td>数据存储位置（本地/云端）、跨境流动限制、用户权限颗粒度</td></tr><tr><td><strong>国产信创适配度</strong></td><td>兼容国产芯片（鲲鹏/海光）、操作系统（统信/UOS）、数据库（达梦/人大金仓）</td></tr><tr><td><strong>部署灵活性</strong></td><td>支持模式（公有云/私有云/混合云）、部署周期（天/周/月）、硬件资源要求</td></tr><tr><td><strong>敏捷实践支持深度</strong></td><td>Scrum/XP/Kanban原生支持、自定义工作流能力、需求-开发-测试全链路闭环</td></tr><tr><td><strong>安全合规体系</strong></td><td>等保三级/四级认证、漏洞修复响应时效、审计日志完整性</td></tr></tbody></table><h2>二、10大工具客观对比：从全球巨头到本土方案</h2><h3>1. 禅道（ZenTao）——私有化敏捷管理的“中国样本”</h3><p><strong>定位</strong>：专注研发全生命周期管理的私有化协作平台。  <br/><strong>核心优势</strong>：</p><ul><li><strong>数据主权</strong>：支持纯本地化部署，符合《数据安全法》要求，金融/军工客户占比超35%（2024年行业报告）。</li><li><strong>信创适配</strong>：已完成与鲲鹏920、统信UOS、达梦数据库的兼容认证，在制造业国产化替代项目中落地率第一。</li><li><strong>敏捷深度</strong>：原生支持Scrum/XP，可自定义需求-任务-缺陷追踪链路，配套“敏捷成熟度模型”帮助企业从基础敏捷向规模化演进。</li><li><strong>成本可控</strong>：一次性授权+按需增购模式，中小团队初始投入仅为Jira的1/3（据Gartner 2024成本分析）。</li></ul><p><strong>适用场景</strong>：对数据主权有强要求的中大型企业（如军工、金融）、需深度定制研发流程的制造企业。  </p><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl902" alt="" title=""/></p><h3>2. Jira Align（Atlassian）——全球巨头的规模化敏捷方案</h3><p><strong>定位</strong>：面向企业级规模化敏捷（SAFe）的云端协作工具。  <br/><strong>核心优势</strong>：</p><ul><li><strong>全局规划</strong>：支持跨团队/跨部门的史诗级需求管理，与Jira Software深度联动，适合500人以上技术团队。</li><li><strong>生态整合</strong>：无缝对接Confluence、Slack、AWS CodePipeline，开发者体验流畅。  <br/><strong>局限</strong>：</li><li>数据需存储在海外AWS，国内金融/政府客户需额外部署本地镜像，合规成本高。</li><li>私有化部署需定制开发，周期长达3-6个月（对比禅道标准部署仅需2周）。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl909" alt="" title="" loading="lazy"/></p><h3>3. Azure DevOps（微软）——微软生态的集成式平台</h3><p><strong>定位</strong>：覆盖代码托管、CI/CD、项目管理的端到端开发平台。  <br/><strong>核心优势</strong>：</p><ul><li><strong>微软生态协同</strong>：与Office 365、Azure云服务深度集成，适合依赖Windows环境的制造/零售企业。</li><li><strong>轻量敏捷</strong>：内置看板、冲刺计划工具，小团队可快速上手。  <br/><strong>局限</strong>：</li><li>国内节点由世纪互联运营，数据主权仍受部分监管限制。</li><li>敏捷实践模板标准化程度高，但自定义工作流灵活性弱于禅道。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmJEq" alt="" title="" loading="lazy"/></p><h3>4. Trello（Atlassian）——轻量协作的“视觉化工具”</h3><p><strong>定位</strong>：基于看板的轻量化项目管理工具，主打团队协作灵活性。  <br/><strong>核心优势</strong>：</p><ul><li><strong>易上手</strong>：拖拽式操作，无需培训即可管理简单项目，适合互联网初创团队。</li><li><strong>插件生态</strong>：通过Power-Up扩展支持甘特图、时间跟踪等功能。  <br/><strong>局限</strong>：</li><li>缺乏需求-开发-测试全链路追踪，复杂项目易“信息断层”。</li><li>数据存储在海外，国内金融客户需额外购买合规插件。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmc6h" alt="" title="" loading="lazy"/></p><h3>5. 飞书多维表格（字节跳动）——国内SaaS的“灵活协作代表”</h3><p><strong>定位</strong>：融合文档、表格与项目管理的协同工具。  <br/><strong>核心优势</strong>：</p><ul><li><strong>字节生态</strong>：与飞书IM、日历、视频会议无缝衔接，适合互联网/内容行业。</li><li><strong>低代码能力</strong>：通过公式、仪表盘自定义管理逻辑，小团队可快速搭建个性化流程。  <br/><strong>局限</strong>：</li><li>深度敏捷支持弱（如无原生Scrum燃尽图），需依赖第三方应用补充。</li><li>数据存储在国内，但私有化部署能力有限（仅支持部分行业定制）。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmc6j" alt="" title="" loading="lazy"/></p><h3>6. 钉钉宜搭（阿里巴巴）——企业协同的“轻量管理入口”</h3><p><strong>定位</strong>：基于钉钉的项目管理与任务协作工具。  <br/><strong>核心优势</strong>：</p><ul><li><strong>钉钉生态</strong>：与考勤、审批、文档打通，适合中小企业日常运营管理。</li><li><strong>低成本</strong>：基础功能免费，适合预算有限的初创团队。  <br/><strong>局限</strong>：</li><li>敏捷开发专业功能缺失（如迭代计划、缺陷追踪），需外接专业工具。</li><li>安全合规侧重业务数据，研发过程数据保护能力弱于禅道。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmYY1" alt="" title="" loading="lazy"/></p><h3>7. 金蝶云·星空（金蝶）——制造业的“业研一体化方案”</h3><p><strong>定位</strong>：面向制造业的研发-生产-财务一体化平台。  <br/><strong>核心优势</strong>：</p><ul><li><strong>行业适配</strong>：内置BOM管理、生产排期模块，与ERP深度联动，适合离散制造企业。</li><li><strong>信创适配</strong>：支持华为鲲鹏、麒麟操作系统，符合制造业国产化要求。  <br/><strong>局限</strong>：</li><li>敏捷开发模块为附加功能，流程灵活性弱于专业工具（如禅道）。</li><li>学习成本高，非制造业团队易“功能冗余”。</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdmWLt" alt="" title="" loading="lazy"/></p><h3>8. 泛微e-cology（泛微）——OA与项目的“流程融合派”</h3><p><strong>定位</strong>：以流程管理为核心的协同平台。  <br/><strong>核心优势</strong>：</p><ul><li><strong>流程定制</strong>：通过建模工具可灵活设计研发审批、需求变更等流程，适合强流程管控的国企。</li><li><strong>本地化服务</strong>：全国有200+服务中心，实施响应快。  <br/><strong>局限</strong>：</li><li>敏捷方法论支持弱，更侧重流程合规而非研发效率提升。</li><li>与代码托管、CI/CD工具集成需额外开发。</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdmPEG" alt="" title="" loading="lazy"/></p><h3>9. 蓝凌MK-PaaS（蓝凌）——知识管理与项目的“智能协同”</h3><p><strong>定位</strong>：融合知识图谱的项目管理平台。  <br/><strong>核心优势</strong>：</p><ul><li><strong>知识沉淀</strong>：自动归档需求文档、缺陷记录，形成企业知识库，适合技术密集型企业。</li><li><strong>AI辅助</strong>：通过NLP自动提取需求关键信息，减少人工整理成本。  <br/><strong>局限</strong>：</li><li>敏捷实践深度不足，更适合研发后期的知识管理而非全周期协作。</li><li>私有化部署成本较高（需配套服务器集群）。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmYY2" alt="" title="" loading="lazy"/></p><h3>10. 华为云DevCloud（华为）——云原生的“研发效能平台”</h3><p><strong>定位</strong>：覆盖需求、开发、测试、部署的全流程云服务。  <br/><strong>核心优势</strong>：</p><ul><li><strong>云原生支持</strong>：与华为云容器服务、Serverless深度集成，适合云转型中的企业。</li><li><strong>自动化能力</strong>：内置代码检查、测试覆盖率工具，提升研发质量。  <br/><strong>局限</strong>：</li><li>数据存储在华为云，部分金融客户对“单一云厂商依赖”存顾虑。</li><li>自定义工作流需通过低代码平台实现，灵活性弱于禅道。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdmYY3" alt="" title="" loading="lazy"/></p><h2>三、结论：为何“禅道路径”是政企行业的最优解？</h2><p>对于政企、军工、金融、制造等对数据安全、国产化、流程可控有高要求的行业，选择项目管理工具需优先考虑：</p><ul><li><strong>数据主权</strong>：禅道纯本地化部署，避免跨境数据风险；</li><li><strong>信创适配</strong>：已完成主流国产软硬件兼容，降低替代成本；</li><li><strong>敏捷深度</strong>：既支持基础Scrum，也可自定义复杂流程，匹配企业从“小步快跑”到“规模化敏捷”的演进需求；</li><li><strong>成本可控</strong>：一次性授权模式避免长期订阅支出，更符合国企/制造业的预算逻辑。</li></ul><h2>FAQ：10大工具选型常见问题解答</h2><h3>Q1：数据主权和安全性是企业选型的核心考量，禅道与其他工具在这方面的差异具体体现在哪里？</h3><p>A：禅道支持纯本地化部署（物理机/虚拟机/私有云），所有研发数据（需求、代码关联、测试记录）均存储在企业自有服务器，符合《数据安全法》《个人信息保护法》要求。对比来看，Jira Align、Azure DevOps等公有云工具数据默认存储在海外，需额外部署本地镜像（增加合规成本）；飞书多维表格等SaaS工具虽国内存储，但私有化能力有限。禅道的“数据不出域”特性，对军工、金融等敏感行业更具吸引力。</p><h3>Q2：禅道的“轻量级”与“强管理”如何平衡？是否会因功能复杂而增加团队学习成本？</h3><p>A：禅道的“轻量”体现在部署与使用门槛：标准部署仅需2周（对比Jira Align的3-6个月），界面设计遵循“研发全链路”逻辑（需求-任务-缺陷-测试一站式追踪），团队无需切换多个工具。而“强管理”通过自定义工作流、权限颗粒度（如字段级/操作级权限）、审计日志实现，既满足标准化流程（如ISO 9001），也支持企业个性化管理需求。实际客户反馈显示，中小团队1天内可上手，大型企业2周完成全员培训。</p><h3>Q3：在国产信创趋势下，禅道与其他工具的信创适配进展如何？</h3><p>A：禅道已完成与鲲鹏920芯片、统信UOS操作系统、达梦数据库、东方通中间件的兼容认证，覆盖“芯片-OS-数据库-中间件”全栈信创环境，在制造业国产化替代项目中落地超200家。对比来看，Jira、Trello等国际工具暂无官方信创适配计划；金蝶云·星空、蓝凌MK-PaaS虽适配但侧重ERP/OA场景。禅道是少数专注研发管理领域的信创适配产品。</p><h3>Q4：如果企业已有Jira或飞书，是否还需要引入禅道？如何判断是否需要“替换”或“补充”？</h3><p>A：若企业需求停留在“基础任务协作”（如互联网小团队），飞书多维表格或Jira Software已足够；但如果涉及“研发全生命周期管理”（需求追踪到上线运维）、“严格的安全合规”（如军工涉密项目），或需要“国产化替代”，则需引入禅道作为专业工具。实践中，多数企业选择“Jira/飞书+禅道”组合：前者负责日常协作，后者管理研发核心流程。</p><h3>Q5：预算有限的情况下（如中小制造企业），如何在禅道与其他工具中选择？</h3><p>A：中小团队可优先评估“长期成本”而非“初始价格”。禅道采用“一次性授权+按需增购”模式，5年总成本约为Jira订阅制的1/2（Gartner 2024数据）。若企业需私有化部署且重视数据安全，禅道的性价比更高；若仅需轻量协作且接受云端存储，飞书多维表格或Trello可作为过渡。但需注意：随着企业规模扩大（如团队超50人），专业工具的流程管理能力将成为效率瓶颈。  </p><p><strong>结语</strong>：2025年的敏捷开发工具选型，本质是“战略适配”而非“功能比拼”。禅道以“私有化、强安全、深适配”的差异化路径，为政企行业提供了兼顾效率与合规的最优解。决策者需结合自身数据主权要求、信创进度、团队规模，选择真正“长在业务里”的工具。</p>]]></description></item><item>    <title><![CDATA[源码赋能婚恋业务：婚恋小程序搭建全流程，]]></title>    <link>https://segmentfault.com/a/1190000047384353</link>    <guid>https://segmentfault.com/a/1190000047384353</guid>    <pubDate>2025-11-10 10:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>源码搭建是婚恋业务快速落地的核心路径 —— 无需从零开发核心功能，直接复用成熟模块，聚焦 “注册 - 匹配 - 聊天” 三大核心场景，15-25 天即可上线可用、可获客的婚恋小程序。<br/><strong>一、先定源码：匹配 “注册 - 匹配 - 聊天” 核心需求</strong><br/>源码的核心价值是 “省去核心功能开发”，因此选型需围绕 “三大模块是否成熟、是否易配置、是否合规” 展开，避免后期二次开发成本过高。</p><ol><li>源码选型核心标准（聚焦核心模块）<br/><img width="592" height="513" referrerpolicy="no-referrer" src="/img/bVdmYZL" alt="image.png" title="image.png"/></li><li>高性价比源码获取渠道（附成本）<br/>开源社区：GitHub/Gitee 搜索「uni-app 婚恋小程序」「相亲小程序 聊天模块」，选择 stars≥300、近 3 个月有更新的项目（免费，需自行配置聊天接口、修复小 bug，适合有基础技术的团队）；<br/>商用模板平台：微擎、即速应用、有赞云，直接采购 “婚恋核心版” 模板（1500-4000 元 / 套，自带注册 - 匹配 - 聊天全模块，支持可视化配置，无需技术开发）；<br/>小型开发团队定制：要求 “复用成熟核心模块 + 简单个性化调整”（如增加地域精准匹配），费用 5000-15000 元（含部署 + 1 年售后，适合需要轻微差异化的团队）。<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmw5l" alt="" title="" loading="lazy"/><img width="723" height="511" referrerpolicy="no-referrer" src="/img/bVdmw5m" alt="" title="" loading="lazy"/><br/><strong>二、核心模块优化技巧：提升用户留存与互动率</strong><br/>搭建完成后，通过简单优化，让 “注册 - 匹配 - 聊天” 流程更顺畅，提高用户留存：<br/>注册模块：补填资料页面分步骤展示（如第一步基础信息、第二步择偶意向），每完成一步显示进度条（降低心理压力）；<br/>匹配模块：每天推送 3-5 个 “高相似度推荐”，并标注匹配亮点（如 “同城 + 同兴趣”“学历匹配”），提高用户点击意愿；<br/>聊天模块：内置 “破冰话题模板”（如 “你平时休息喜欢做什么？”“你理想的相处模式是怎样的？”），用户可直接选用，降低聊天尴尬；<br/>消息提醒：开启微信服务通知（匹配成功、收到消息时推送），避免用户错过互动。<br/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVde018" alt="" title="" loading="lazy"/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVde019" alt="" title="" loading="lazy"/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVde02b" alt="" title="" loading="lazy"/><img width="723" height="247" referrerpolicy="no-referrer" src="/img/bVdmcMZ" alt="" title="" loading="lazy"/></li></ol>]]></description></item><item>    <title><![CDATA[MIAOYUN | 每周AI新鲜事儿（1]]></title>    <link>https://segmentfault.com/a/1190000047384359</link>    <guid>https://segmentfault.com/a/1190000047384359</guid>    <pubDate>2025-11-10 10:03:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本周AI领域动态密集，美团、360、银河通用、字节、腾讯、Kimi与科大讯飞等分别发布多模态、图文、导航及视频推理模型；工具层面，寒武纪、百度、昆仑万维、腾讯均推出新平台或功能。技术方面，在长序列处理、多智能体协同及代码执行效率上取得突破。市场方面，OpenAI与AWS达成巨额合作，小鹏发布人形机器人「IRON」。整体呈现高效化、多模态与实用化趋势，一起来回顾本周发生的AI新鲜事儿吧！</p><h2><strong>AI 大模型</strong></h2><p><strong>腾讯联合厦门大学开源3D场景生成模型「FlashWorld」</strong></p><p>10月30日消息，腾讯联合厦门大学开源的3D场景生成模型「FlashWorld」，能够在单GPU上以5-10秒的速度，从单张图像或文本提示中生成高质量3D场景，速度提升可达10至100倍。该模型通过跨模式蒸馏技术，结合多视角和三维两种方案的优势，实现了高保真与3D一致性。</p><p><strong>以色列AI公司Lightricks推出视频生成模型「LTX-2 AI」</strong></p><p>10月31日，以色列AI公司Lightricks推出视频生成模型「LTX-2 AI」，成为首个支持原生4K分辨率、50帧每秒输出且具备音画同步能力的开源模型。该模型采用混合扩散-变换器架构，是一个融合“时域（Time）+空间（Frame）+声波（Audio）”的扩散模型，支持多种输入控制方式，包括镜头运动指令、物体轨迹设定等，赋予创作者更高自由度。内置LoRA微调模块，允许用户使用少量样本训练专属风格模型，保持跨场景一致性。此外，「LTX-2 AI」可在消费级GPU上本地运行。</p><p><strong>美团发布并开源全模态实时交互大模型「LongCat-Flash-Omni」</strong></p><p>11月3日，美团正式发布并开源全模态实时交互大模型「LongCat-Flash-Omni」，并同步推出首款AI助手App「LongCat」，开启多模态交互新阶段。该模型总参数量560B，激活参数27B，是业界首个实现全模态覆盖、端到端架构、大参数量高效推理于一体的开源大语言模型，支持128K上下文窗口及超8分钟音视频交互，在文本、图像、音频、视频等各项模态的能力达到开源SOTA。</p><p><strong>360人工智能研究院开源「FG-CLIP2」成最强图文跨模态VLM模型</strong></p><p>11月4日，360人工智能研究院最新开源的「FG-CLIP2」模型，在八大类任务、29项测试中，全面超越Google与Meta，成为目前最强的图文跨模态视觉基础（VLM）模型。该模型通过实现局部细粒度识别与中英双语均衡训练，解决了以往视觉模型的局部理解能力不足的问题，能够准确解析复杂场景和空间关系。其训练体系采用了FineHARD数据集和“两阶段”训练策略，使模型在细节、空间与语义的感知能力显著提升，推动AI视觉理解的行业基准向前发展。</p><p><strong>银河通用联合高校推出首个跨本体全域环视导航基座大模型「NavFoM」</strong></p><p>11月5日，银河通用联合北京大学、阿德莱德大学等多所顶尖高校推出全球首个跨本体全域环视导航基座大模型「NavFoM」（Navigation Foundation Model）。该模型创新应用TVI Tokens与BATS策略两项关键技术，还构建了一个跨任务数据集，包含800万条跨任务、跨本体导航数据和400万条开放问答数据，实现时空理解和实时响应，让机器人“看懂指令、自主走路”。基于该模型，银河通用还发布「TrackVLA++」、「UrbanVLA」和「MM-Nav」三个应用模型，针对不同的落地需求。</p><p><strong>北京字节联合开源首个时空推理视频模型「Open-o3 Video」</strong></p><p>11月5日，北京大学和字节跳动联合推出了首个将显式时空证据嵌入视频推理全过程的开源模型「Open-o3 Video」，让AI不仅能回答有关视频内容问题，还能在思维过程中同步直观标出具体位置，真正实现有迹可循的视频推理。模型采用non-agent架构，避免了复杂的工具调用和多轮推理，关键指标可提升至24.2%，性能表现超越「GPT-4o」和「Gemini-2-Flash」等模型。</p><p><strong>月之暗面发布迄今能力最强的开源思考模型「Kimi K2 Thinking」</strong></p><p>11月6日，月之暗面发布「Kimi K2 Thinking」，是Kimi迄今能力最强的开源思考模型，具有通用Agentic能力和推理能力的思考模型，擅长深度推理，可以通过多轮工具调用，解决各类复杂的难题。在人类最后的考试（Humanity's Last Exam）、自主网络浏览能力（BrowseComp）、复杂信息收集推理（SEAL-0）等多项基准测试中表现达到 SOTA 水平。</p><p><strong>科大讯飞发布「讯飞星火X1.5」及系列AI产品</strong></p><p>11月6日，科大讯飞发布全新星火深度推理大模型「X1.5」，基于全栈国产算力平台训练，采用MoE架构，总参数293B，推理激活仅30B，推理效率相比「讯飞星火X1」提升100%。其语言理解、文本生成、知识问答、逻辑推理、数学能力、代码能力等六大核心能力对标国际主流大模型，其中，数学能力持续保持国际领先。</p><h2><strong>AI 工具</strong></h2><p><strong>寒武纪推出基础软件平台「Cambricon NeuWare」</strong></p><p>11月3日，寒武纪正式发布基础软件平台「Cambricon NeuWare」，让用户与开发者能够跨越不同的寒武纪硬件和应用场景，降低上手难度，提升开发效率，快速迁移与部署AI应用。该平台全面兼容最新PyTorch版本和Triton算子开发语言，支持用户模型和自定义算子快速迁移，在大模型与搜广推训练推理方面完成大规模技术验证，支持DeepSeek V3、Qwen系列等MoE类模型训练，实现发布即适配。此外，平台还提供完整的驱动运行时库、编译器、算子库和集群工具，推动AI能力真正走进千行百业。</p><p><strong>百度文心APP推出「魔法漫画」功能</strong></p><p>11月3日，百度文心APP推出「魔法漫画」功能，用户只需一句话或一张照片，两分钟即可生成多图多页、剧情完整的AI连载漫画。该功能支持自定义角色形象、九种风格选择(吉卜力、二次元、国风水墨等)，每页漫画自动生成文字解说，可一次性生成6-7页。此外，还支持“续写”和“改编”功能，用户可基于原剧情延伸或重写新版本，生成的漫画可下载图片或分享到微信朋友圈。</p><p><strong>昆仑万维全新AI视频创作平台「SkyReels」正式上线</strong></p><p>11月4日，昆仑万维旗下AI视频创作平台「SkyReels」正式焕新上线，Web端与移动端APP已全面登陆。模型侧，强势聚合「Google Veo 3.1」、「Sora 2」等全球顶尖AI多模态模型；功能侧，一站式提供图片生成、视频生成、数字人、音乐生成等多种AI创作方式。此次更新主要推出无限画布、数字人口播、模版功能、专家Agent、视频延长和风格化等核心能力，自研「SkyReels V3」模型是业内首个支持单镜头多人多轮对话的数字人模型，推动AI视频创作迈向“零门槛创意生成时代”。</p><p><strong>腾讯「ima」正式支持导入、导出「腾讯文档」</strong></p><p>11月4日，腾讯「ima」正式支持导入、导出「腾讯文档」 ，助力工作流再提速。在「ima」PC端导入文件（含文档、表格、幻灯片、智能文档和PDF等品类）到知识库时，可以选择「腾讯文档」内容，进行提问和分析；对于「ima」的回答，支持一键导出为「腾讯文档」，进行再次编辑、协作及创作。两款应用打通后一站式完成内容导入、输出全流程，无需在应用间来回切换，效率翻倍提升工作学习体验。</p><p><strong>腾讯云CodeBuddy成为国内首个支持「Skills」标准化接口的AI编程工具</strong></p><p>11月6日，腾讯云CodeBuddy宣布成为国内首个支持「Skills」标准化接口的AI编程工具。通过该接口，开发者可以为AI添加多样化技能（如智能处理PDF、自动生成PPT、全自动发小红书、全栈自动化开发等），AI从单一指令执行者升级为能独立完成复杂任务的“智能代理”。「Skills」将不同领域专业知识，封装成独立可复用的技能模块，每个技能包是对应技能的SOP，让AI读完就能高效、高质量执行；同时结合MCP协议实现外部工具联动，显著提升开发效率并降低上下文成本。</p><h2><strong>AI Agent</strong></h2><p><strong>OpenAI发布了使用「GPT-5」寻找和修复安全漏洞的智能体「Aardvark」</strong></p><p>10月31日，OpenAI发布了使用「GPT-5」寻找和修复安全漏洞的智能体「Aardvark」，其工作原理是监控代码库的提交与变更，在识别漏洞的同时分析其潜在利用方式，并自动提供修复建议。具体来说，它的工作流程从Git仓库出发，依次经历：威胁建模→漏洞发现→沙盒验证→Codex 修复→人工复审→提交Pull Request。目前，「Aardvark」还处于beta测试阶段，但在标准代码库的基准测试中，已识别出了92%的已知与人工注入漏洞，而且能定位仅在复杂条件下出现的问题。</p><p><strong>阿里云通义千问更新「AgentScope1.0」，增加两款开源Agent</strong></p><p>11月5日，阿里云通义千问宣布「AgentScope1.0」更新，增加了两款基于AgentScope构建的开源智能体应用，分别是用于各种实际任务的「Alias-Agent」和用于数据处理的「Dat，a-Juicer Agent」。并扩展其核心能力，低代码适配Trinity-RFT框架进行Agentic RL训练，集成ReMe的长期记忆实现，同时上线「AgentScope-Samples」，构建“开箱即用型”智能体实现和全栈应用的集合。</p><p>技术突破</p><p><strong>月之暗面推出创新性混合线性注意力架构「Kimi Linear」</strong></p><p>10月31日，月之暗面推出创新性混合线性注意力架构「Kimi Linear」，解决当前LLMs在处理长序列任务时面临的计算效率和性能瓶颈。该架构融合三份Kimi Delta Attention（KDA）与一份全局MLA，通过细粒度门控机制压缩记忆状态，在处理百万级token时KV Cache占用减少75%，解码吞吐量最高提升6倍，TPOT指标较传统MLA快6.3倍。</p><p><strong>斯坦福大学及其合作团队提出了「AgentFlow」框架</strong></p><p>11月3日消息，近期斯坦福大学及其合作团队提出了「AgentFlow」框架，采用模块化架构，通过4个专门化智能体协同工作，配合专门设计的Flow-GRPO算法，使系统能够在真实交互环境中持续优化决策策略，使得小规模的7B参数模型在搜索、数学等多个推理任务中超越大模型「GPT-4o」（约200B参数），为AI系统的高效推理和持续学习提供了新思路。</p><p><strong>Anthropic发布「代码执行」新范式，效率提升98.7%</strong></p><p>11月5日，Anthropic发布新的Agent技术博客，详细阐述「代码执行」新范式，建立在模型上下文协议（MCP）之上，让模型编写代码调用工具而非直接调用，将Token消耗从15万降至2000，效率提升98.7%。新范式采用按需加载工具定义、数据本地流转设计，解决了工具定义过载和中间结果消耗两大Agent效率瓶颈。此外还带来“渐进式披露、上下文高效工具、强大控制流、隐私保护和状态持久化”五大核心优势。</p><h2><strong>市场动态</strong></h2><p><strong>OpenAI与AWS官宣达成价值380亿美元为期7年的战略合作</strong></p><p>11月4日，OpenAI与AWS官宣达成价值380亿美元为期7年的战略合作。OpenAI 将立即并持续获得AWS世界级的基础设施支持，以运行其先进的AI工作负载。AWS将向OpenAI提供配备数十万颗芯片的Amazon EC2 UltraServers（计算服务器），并具备将计算规模扩展至数千万个CPU的能力，以支持其先进的生成式AI任务。</p><p><strong>小鹏发布全新一代人形机器人「IRON」</strong></p><p>11月5日，小鹏发布全新一代人形机器人「IRON」，身高1.78米，体重70公斤，具备仿生骨骼、肌肉和柔性皮肤结构。它拥有22个自由度的灵巧手和82个全身自由度，能以“猫步”姿态自然行走，搭载3颗图灵AI芯片（2250TOPS算力）和物理世界大模型，支持对话、交互等智能功能。</p><p><strong>高德与小鹏达成合作，未来将共同提供「Robotaxi」服务</strong></p><p>11月5日，高德宣布与小鹏汽车达成合作，未来将共同面向全球提供「Robotaxi」服务，高德通过「TrafficVLM」模型实现“超视距”能力，可在几公里外感知突发事故并预判拥堵发展，提前推送预警信息。高德地图沉淀了数十万亿级时空样本，还构建了“时空信息建模+视觉感知监测+行业官方信息+用户分享与验证”的多渠道数据融合体系，成为「Robotaxi」行业的“空间智能基础设施”，降低行业创新门槛。</p>]]></description></item><item>    <title><![CDATA[省时又方便：三款常见报表模板，一键套用，]]></title>    <link>https://segmentfault.com/a/1190000047384367</link>    <guid>https://segmentfault.com/a/1190000047384367</guid>    <pubDate>2025-11-10 10:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。<br/>yayijia.cc/thread-187355-1-1.html<br/>yayijia.cc/thread-187352-1-1.html<br/>yayijia.cc/thread-187345-1-1.html<br/>yayijia.cc/thread-187336-1-1.html<br/>yayijia.cc/thread-187329-1-1.html<br/>yayijia.cc/thread-187323-1-1.html<br/>yayijia.cc/thread-187317-1-1.html<br/>yayijia.cc/thread-187303-1-1.html<br/>yayijia.cc/thread-187300-1-1.html</p>]]></description></item><item>    <title><![CDATA[实战指南：利用React与SpringB]]></title>    <link>https://segmentfault.com/a/1190000047384369</link>    <guid>https://segmentfault.com/a/1190000047384369</guid>    <pubDate>2025-11-10 10:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、引言</p><p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境<br/>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序<br/>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p><p>三、Go语言进阶</p><p>理解包和模块<br/>Go语言使用包（package）来组织代码，每个包都可以包含多个文件。了解包的概念和使用方法对于编写模块化、可复用的代码非常重要。此外，从Go 1.11版本开始，Go引入了模块（module）的概念，用于解决依赖管理和版本控制的问题。</p><p>掌握并发编程<br/>Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。你需要熟悉goroutine的创建、运行和管理方法，以及如何使用channel进行协程之间的通信和同步。</p><p>学习标准库和第三方库<br/>Go语言拥有丰富的标准库和第三方库，这些库提供了大量的功能和工具，可以帮助你快速构建各种应用。你需要了解标准库的基本组成和使用方法，同时学会如何使用第三方库来扩展你的应用。</p><p>实践Web开发<br/>Web开发是Go语言的一个重要应用领域。你可以学习如何使用Go语言编写Web服务器和客户端程序，了解HTTP协议和Web开发的基本概念。同时，你还可以学习一些流行的Web框架（如Gin、Echo等）来提高开发效率。</p><p>深入了解底层原理<br/>随着对Go语言深入的了解，你可以进一步学习其底层原理和实现细节。这包括内存管理、垃圾回收、协程调度等方面的知识。了解这些底层原理可以帮助你更好地理解Go语言的性能和优化方法。</p><p>四、总结</p><p>Go语言作为一门简洁、高效、并发的编程语言，具有广泛的应用前景。从入门到进阶的旅程中，你需要不断学习和实践，掌握Go语言的基本语法、并发编程、标准库和第三方库等方面的知识。同时，你还需要关注Go语言的最新动态和社区发展，以便更好地应用这门强大的编程语言。<br/>yayijia.cc/thread-187432-1-1.html<br/>yayijia.cc/thread-187421-1-1.html<br/>yayijia.cc/thread-187412-1-1.html<br/>yayijia.cc/thread-187407-1-1.html<br/>yayijia.cc/thread-187391-1-1.html<br/>yayijia.cc/thread-187386-1-1.html<br/>yayijia.cc/thread-187379-1-1.html<br/>yayijia.cc/thread-187376-1-1.html<br/>yayijia.cc/thread-187362-1-1.html</p>]]></description></item><item>    <title><![CDATA[提升代理 IP 稳定性的五个核心技巧 B]]></title>    <link>https://segmentfault.com/a/1190000047384385</link>    <guid>https://segmentfault.com/a/1190000047384385</guid>    <pubDate>2025-11-10 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在跨境电商、广告验证、数据采集以及 AI 模型训练等技术场景中，代理 IP 的稳定性决定了项目的运行效率与安全性。一个稳定的代理能让系统持续运转、降低封禁风险；而不稳定的代理则可能造成数据丢失、任务中断、甚至账户异常。<br/>那么，如何真正提升代理 IP 的稳定性？<br/> 本文将从五个关键角度入手，带你全面掌握优化代理连接的实用方法。</p><h2>一、优化网络环境，确保本地连接稳定</h2><p>很多人忽视了一个事实：代理稳定性不仅取决于服务商，也取决于你的网络环境。<br/> 如果本地网络延迟高、丢包严重，即便代理服务再好，也无法保持高可用。<br/>实用建议：<br/>●优先使用有线网络，避免公共 Wi-Fi 或热点。<br/>●关闭可能干扰代理通信的 VPN、防火墙或杀毒软件。<br/>●对高并发任务，限制单机连接数，防止带宽过载。<br/>你可以通过以下命令快速检测当前网络质量：</p><pre><code>ping 8.8.8.8
tracert google.com
</code></pre><p>延迟低于 100ms、丢包率低于 1%，是稳定代理连接的理想状态。</p><h2>二、选择高质量节点，优先使用住宅或静态代理</h2><p>代理类型直接影响连接稳定性。<br/> 一般而言，数据中心 IP 虽然速度快，但易被封禁；住宅代理（Residential Proxy） 和 静态住宅代理（Static Residential Proxy） 则更接近真实用户网络，封禁率低、连接更稳定。<br/>如果你的业务需要：<br/>●长期登录和账号维护（如 TikTok、Facebook、广告账户） → 建议使用静态住宅代理；<br/>●大规模采集或验证任务 → 可选择动态住宅代理，通过智能轮换实现持续可用。<br/>住宅代理的核心优势在于：稳定性与匿名性兼备。选择覆盖全球、节点分布丰富的服务商，也能显著减少区域性失效。</p><h2>三、合理配置协议与超时机制</h2><p>很多连接不稳定的问题，其实源于代理协议配置不当或超时设置不合理。<br/>HTTP、HTTPS 与 SOCKS5 各有特点：<br/>●SOCKS5：兼容 TCP 与 UDP，性能稳定，适合高频请求；<br/>●HTTPS：适配 Web 请求与 API 调用，安全性更高；<br/>●HTTP：适用于基础网页访问。<br/>此外，一定要配置 timeout（超时）与 retry（重试） 参数。<br/> 以下示例展示了一个稳定性更强的配置方式：</p><pre><code>import requests

proxies = {
    "http": "http://user:pass@global.rrp.b2proxy.com:8000",
    "https": "http://user:pass@global.rrp.b2proxy.com:8000"
}

for i in range(3):
    try:
        r = requests.get("https://www.google.com", proxies=proxies, timeout=10)
        print("Status:", r.status_code)
        break
    except requests.exceptions.RequestException:
        print("Connection failed, retrying...")
</code></pre><p>通过控制请求超时与自动重试，可以有效降低因网络波动导致的失败率。</p><h2>四、优化会话管理与 IP 轮换策略</h2><p>对于需要同时运行多个请求的项目而言，会话管理 是保持稳定的关键。<br/> 错误的 IP 切换策略容易导致代理封禁或访问中断。<br/>实用技巧：<br/>●短时任务：启用频繁轮换，防止被识别为固定模式；<br/>●长时会话：保持黏性 IP（Session Stickiness），确保账号持续在线；<br/>●合理切换间隔：过于频繁的更换可能被目标网站识别为异常流量。<br/>通过智能轮换与会话隔离，可以在并发任务中同时保持高匿名性与高稳定性。</p><h2>五、建立监控与容错机制</h2><p>再高质量的代理也难免会偶发失效，因此一个完善的监控与故障恢复机制不可或缺。<br/>你可以从以下几个方面着手：<br/>●实时检测代理可用性（心跳检测）；<br/>●记录并分析请求错误日志；<br/>●构建备用代理池，在主节点失效时自动切换；<br/>●定期清理或替换低成功率 IP。<br/>简单实现示例：</p><pre><code>def check_proxy(ip):
    try:
        r = requests.get("https://api.ipify.org", proxies={"http": ip, "https": ip}, timeout=5)
        return r.status_code == 200
    except:
        return False
</code></pre><p>当监控系统发现节点异常时，立即启用备用代理池，可显著提升整体稳定性与容错能力。</p><h2>结语：稳定，是一场系统工程</h2><p>想要实现高稳定性的代理环境，既需要优质的住宅代理资源，也离不开你在技术层面的优化。<br/>总结来看：<br/>1.优化本地网络，减少物理干扰<br/>2.选择高质量住宅或静态 IP<br/>3.合理配置代理协议与超时<br/>4.制定科学的 IP 轮换策略<br/>5.构建实时监控与容错机制<br/>掌握以上五个技巧，你的代理系统将更稳定、更安全、更具可控性。<br/>在一个日益依赖分布式访问和自动化任务的时代，稳定的代理 IP 不仅是基础，更是竞争力。</p>]]></description></item><item>    <title><![CDATA[MyBatis 常见面试题 程序员Sev]]></title>    <link>https://segmentfault.com/a/1190000047381997</link>    <guid>https://segmentfault.com/a/1190000047381997</guid>    <pubDate>2025-11-10 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>Mybatis基础</h3><h4>Mybatis是什么？</h4><ul><li>MyBatis框架是一个开源的数据持久层框架。</li><li>它的内部封装了通过JDBC访问数据库的操作，支持普通的SQL查询、存储过程和高级映射，几乎消除了所有的JDBC代码和参数的手工设置以及结果集的检索。</li><li>MyBatis作为持久层框架，其主要思想是将程序中的大量SQL语句剥离出来，配置在配置文件当中，实现SQL的灵活配置。</li><li>这样做的好处是将SQL与程序代码分离，可以在不修改代码的情况下，直接在配置文件当中修改SQL。</li></ul><h4>为什么使用Mybatis代替JDBC？</h4><p>MyBatis 是一种优秀的 ORM（Object-Relational Mapping）框架，与 JDBC 相比，有以下几点优势：</p><ol><li>简化了 JDBC 的繁琐操作：使用 JDBC 进行数据库操作需要编写大量的样板代码，如获取连接、创建 Statement/PreparedStatement，设置参数，处理结果集等。而使用 MyBatis 可以将这些操作封装起来，通过简单的配置文件和 SQL 语句就能完成数据库操作，从而大大简化了开发过程。</li><li>提高了 SQL 的可维护性：使用 JDBC 进行数据库操作，SQL 语句通常会散布在代码中的各个位置，当 SQL 语句需要修改时，需要找到所有使用该语句的地方进行修改，这非常不方便，也容易出错。而使用 MyBatis，SQL 语句都可以集中在配置文件中，可以更加方便地修改和维护，同时也提高了 SQL 语句的可读性。</li><li>支持动态 SQL：MyBatis 提供了强大的动态 SQL 功能，可以根据不同的条件生成不同的 SQL 语句，这对于复杂的查询操作非常有用。</li><li>易于集成：MyBatis 可以与 Spring 等流行的框架集成使用，可以通过 XML 或注解配置进行灵活的配置，同时 MyBatis 也提供了非常全面的文档和示例代码，学习和使用 MyBatis 非常方便。</li></ol><p>综上所述，使用 MyBatis 可以大大简化数据库操作的代码，提高 SQL 语句的可维护性和可读性，同时还提供了强大的动态 SQL 功能，易于集成使用。因此，相比于直接使用 JDBC，使用 MyBatis 更为便捷、高效和方便。</p><p>然而，也要注意一些缺点：</p><ul><li>虽然 MyBatis 很强大，但编写 SQL 语句可能会相对繁琐，特别是当涉及多个字段或多个关联表时。这就要求开发人员在 SQL 编写方面有一定的功底。</li><li>由于 SQL 语句依赖于特定的数据库，如果想要更换数据库，移植性就会受到影响。这意味着不能轻易地更改数据库，可能需要进行一些适应性的修改。</li></ul><h4>ORM是什么</h4><p>ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单Java对象（POJO）的映射关系的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。</p><h4>Mybatis和Hibernate的区别？</h4><p>主要有以下几点区别：</p><ol><li>Hibernate的<strong>开发难度</strong>大于MyBatis，主要由于Hibernate比较复杂，庞大，学习周期比较长。</li><li>Hibernate属于<strong>全自动</strong>ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。</li><li><strong>数据库扩展性</strong>的区别。Hibernate与数据库具体的关联在XML中，所以HQL对具体是用什么数据库并不是很关心。MyBatis由于所有sql都是依赖数据库书写的，所以扩展性、迁移性比较差。</li><li><strong>缓存机制</strong>的区别。Hibernate的二级缓存配置在SessionFactory生成配置文件中进行详细配置，然后再在具体的表对象映射中配置那种缓存。MyBatis的二级缓存配置都是在每个具体的表对象映射中进行详细配置，这样针对不同的表可以自定义不同的缓冲机制，并且MyBatis可以在命名空间中共享相同的缓存配置和实例，通过Cache-ref来实现。</li><li><strong>日志系统完善性</strong>的区别。Hibernate日志系统非常健全，涉及广泛，而Mybatis则除了基本记录功能外，功能薄弱很多。</li><li><strong>sql的优化上，Mybatis要比Hibernate方便很多</strong>。由于Mybatis的sql都是写在xml里，因此优化sql比Hibernate方便很多。而Hibernate的sql很多都是自动生成的，无法直接维护sql；总之写sql的灵活度上Hibernate不及Mybatis。</li></ol><h4>MyBatis 与 JPA 有哪些不同？</h4><p>JPA是Java Persistence API的简称，中文名Java持久层API，是JDK 5.0注解或XML描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。</p><p>首先，我们来聊聊编程模型。MyBatis和JPA采用了不同的方式来处理数据操作。</p><ul><li>MyBatis使用基于SQL的编程模型，这意味着开发人员需要自己编写SQL语句，并将它们映射到Java方法。这给开发人员提供了更大的灵活性，可以精确地控制SQL的编写和执行过程。</li><li>JPA则采用了基于对象的编程模型，你只需定义实体类并使用注解或XML配置来将实体映射到数据库表。JPA会自动生成SQL语句，开发人员不必过多关心底层SQL的细节。</li></ul><p>其次，我们来看一下SQL控制。</p><ul><li>在MyBatis中，可以编写和优化SQL语句，这在需要特定优化或使用数据库特性时非常有用。</li><li>JPA则将大部分SQL细节隐藏起来，自动生成SQL语句。这使得开发人员无需深入了解底层SQL，但在某些情况下可能会影响性能或限制你的操作。</li></ul><p>接下来是灵活性和控制</p><ul><li>MyBatis提供了更多的灵活性，适用于需要定制化SQL查询或调用存储过程的场景。</li><li>JPA则提供了更高层次的抽象，用于简化常见数据库操作。然而，这也可能会在某些高级或复杂情况下产生一些限制。</li></ul><p>关于查询语言</p><ul><li>MyBatis使用原生SQL作为查询语言，这要求开发人员对SQL有一定了解。</li><li>JPA则引入了JPQL作为查询语言，它更加面向对象，类似于SQL，但操作的是实体对象。</li></ul><p>在缓存方面</p><ul><li>MyBatis的缓存控制更精细，可以更准确地控制缓存行为。</li><li>JPA也支持缓存，但通常对缓存的控制较少，更多地由框架自动管理。</li></ul><p>总的来说，选择使用MyBatis还是JPA取决于项目需求和团队技术背景。如果你需要更多的SQL控制和定制化，MyBatis可能更适合；如果你希望更快速地进行常见数据库操作，JPA可能更适合</p><h4>为什么说 Mybatis 是半ORM 映射工具？</h4><p>首先，Mybatis被称为半ORM框架是因为它在数据库操作方面提供了一些对象关系映射的功能，但相对于全ORM框架，它更加灵活和轻量级。在Mybatis中，我们需要手动编写SQL来执行数据库操作，这跟传统的JDBC方式有点类似。但是，Mybatis通过映射文件来实现Java对象与数据库表之间的映射，这就是它的ORM特性。  </p><p>区别的话，全ORM框架通常更加自动化，它会完全代替你来生成SQL语句，进行数据库操作。这在某些情况下能够提高开发效率，因为你不需要写太多的SQL代码。但是，全ORM框架也可能在性能方面略有影响，因为它们可能会生成复杂的SQL语句，导致查询效率下降。  </p><p>相比之下，Mybatis更加灵活，你可以精确地控制要执行的SQL语句，这对于需要优化查询性能的场景很有帮助。另外，Mybatis在映射文件中可以明确指定每个字段的映射关系，这样你能更好地控制数据库表和Java对象之间的对应关系。</p><h4>MyBatis框架的优缺点及其适用的场合</h4><p><strong>优点</strong></p><ol><li>与JDBC相比，减少了50%以上的代码量。</li><li>MyBatis是易学的持久层框架，小巧并且简单易学。</li><li>MyBatis相当灵活，不会对应用程序或者数据库的现有设计强加任何影响，SQL写在XML文件里，从程序代码中彻底分离，降低耦合度，便于统一的管理和优化，并可重用。</li><li>提供XML标签，支持编写动态的SQL，满足不同的业务需求。</li><li>提供映射标签，支持对象与数据库的ORM字段关系映射。</li></ol><p><strong>缺点</strong></p><ol><li>SQL语句的编写工作量较大，对开发人员编写SQL的能力有一定的要求。</li><li>SQL语句依赖于数据库，导致数据库不具有好的移植性，不可以随便更换数据库。</li></ol><p><strong>适用场景</strong></p><p>MyBatis专注于SQL自身，是一个足够灵活的DAO层解决方案。对性能的要求很高，或者需求变化较多的项目，例如Web项目，那么MyBatis是不二的选择。</p><h3>Mybatis原理</h3><h4>MyBatis的核心组件有哪些？</h4><ul><li>SqlSessionFactoryBuilder：是创建 SqlSessionFactory 的构建器。它使用配置文件或配置类来创建 SqlSessionFactory。SqlSessionFactoryBuilder 本身是一个工具类，通常在应用程序启动时使用一次，之后就可以丢弃。</li><li>SqlSessionFactory，是一个会话工厂，同时也承担了配置数据库连接信息和事务管理的功能。它的任务是创建 SqlSession 对象，这个对象是我们与数据库交互的主要途径。一旦这个工厂被建立起来，它就会加载一些必要的配置和映射文件，为后续的数据库操作提供一个可靠的基础。</li><li>SqlSession，是业务与数据库交互的窗口，能够执行 SQL 语句，提交或回滚事务，还可以获取 Mapper 接口的实例。不过需要注意的是，SqlSession 的生命周期是短暂的，通常在数据库操作完成后就应该关闭它，这样可以释放资源。</li><li>Mapper 接口，MyBatis 通过动态代理的方式，把接口方法和映射文件中的 SQL 语句关联起来，这样我们就可以方便地通过接口来执行数据库操作。</li><li>Mapper 映射文件，可以定义 SQL 语句、参数映射、结果映射等等。里面的 SQL 语句可以包括增删改查等操作，MyBatis 会根据我们调用的方法来选择正确的 SQL 语句来执行。</li><li>Type Handlers：负责将预处理语句中的参数从 Java 类型转换为 JDBC 类型，以及将结果集中的列从 JDBC 类型转换为 Java 类型。MyBatis 内置了许多默认的 Type Handlers，并且允许用户自定义 Type Handler。</li><li>Result Maps：描述了数据库结果集与对象属性之间的映射关系。这是 MyBatis 中最强大的特性之一，允许你处理复杂的映射场景，如嵌套结果和关联查询。</li><li>Caching： MyBatis 支持一级缓存和二级缓存。一级缓存默认开启，作用范围是同一个 SqlSession；二级缓存需要手动配置，可以在不同的 SqlSession 之间共享缓存数据，提高系统性能。</li><li>Transaction Manager：MyBatis 提供了一个简单的事务管理机制，可以与 Spring 框架的事务管理集成。这使得开发者能够以声明式的方式管理事务，而不是编程式地处理事务逻辑。</li></ul><h4>Mybatis的工作原理</h4><ul><li>读取MyBatis配置文件：mybatis-config.xml为MyBatis的全局配置文件，配置了MyBatis的运行环境等信息，例如数据库连接信息。</li><li>加载映射文件。映射文件即SQL映射文件，该文件中配置了操作数据库的SQL语句，需要在MyBatis配置文件mybatis-config.xml中加载。mybatis-config.xml文件可以加载多个映射文件，每个文件对应数据库中的一张表。</li><li>构造会话工厂：通过MyBatis的环境等配置信息构建会话工厂SqlSessionFactory。</li><li>创建会话对象：由会话工厂创建SqlSession对象，该对象中包含了执行SQL语句的所有方法。</li><li>执行SQL语句：MyBatis底层定义了一个Executor 接口来操作数据库，它将根据SqlSession传递的参数动态地生成需要执行的SQL语句，同时负责查询缓存的维护。</li><li>MappedStatement 对象：在Executor接口的执行方法中有一个MappedStatement类型的参数，该参数是对映射信息的封装，用于存储要映射的SQL语句的id、参数等信息。</li><li>输入参数映射：输入参数类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输入参数映射过程类似于 JDBC对preparedStatement对象设置参数的过程。</li><li>输出结果映射：输出结果类型可以是Map、List等集合类型，也可以是基本数据类型和POJO类型。输出结果映射过程类似于 JDBC对结果集的解析过程。</li></ul><h4>能详细说说 MyBatis 的执行流程吗?</h4><p>MyBatis 的执行原理基于其核心设计思想：通过映射文件(XML 或注解)将 SQL 语句与 Java 对象进行绑定,整个执行流程可以分为以下几个步骤：</p><ol><li>SqlSessionFactory 的创建：MyBatis 的执行过程从 SqlSessionFactory 开始。SqlSessionFactory 是一个工厂类，负责创建 SqlSession实例 ，SqlSession 是 MyBatis 与数据库交互的核心对象。SqlSessionFactory 是通过 SqlSessionFactoryBuilder 构建的(通常是通过读取 MyBatis 配置文件 mybatis-config.xml 来初始化)。</li><li>Sqlsession 的获取：SqlSessionFactory 通过 openSsesion()方法获取一个 SqlSession 对象，Sqlsession 是操作数据库的主要入口，用户通过它执行SQL语句、提交事务等。</li><li>执行映射语句：当调用sqlsession的方法(例如selectone、selectlist、insert、update、delete 等)时，MyBatis 会根据传入的 SQL映射语句的 ID，去寻找对应的 SQL语句执行。</li><li>命名空间和映射语句的查询：SQL映射语句通过映射文件中的 namespace  和 id 进行定位。MyBatis 会将映射文件解析成一个 MappedStateanent 对象，MappedStateanent 保存了SQL语句、参数类型、返回类型等信息。</li><li>参数封装和 SQL语句执行：在SQL执行前，MyBatis会根据映射文件中配置的 parameterType 类型，将传入的参数封装成适当的时象(例如，使用 JavaBean、Map、XML格式等。然后，MyBatis 会很据不同的执行环境(如 MySQL、Oracle 等数据库)，将 SQL语句执行到数据库中，并将查询结果通过映射文件中配置的 resultType 类型返回。</li><li>返回结果的映射：MyBatis会将查询结果根据resultType或 resulMap进行转换，将查询结果转换成Java 对象(如List、Map 或指定的 POJO 类)。</li><li>事务管理：Matis的事务管理通过 Sqlsession 来处理，Sqlsession 提供了事务的提交和回滚方法，当调用 Sqlsession.commit()时，SQL执行的结果会被提交到数据库；若发生异常，Sqlsession.roolback()，事务会被回滚。</li><li>最后关闭 SqlSession：在操作完成后，Sqlsession 会通过 close()方法关闭，释放数据库连接和资源。</li></ol><h4>简述 MyBatis 的插件运行原理，Mybatis的插件能够在哪些地方进行拦截？</h4><p>MyBatis 的插件机制是通过 动态代理 实现的，主要是在 SQL 执行的关键点(如执行查询、更新、插入)拦截操作并增强功能。</p><p>MyBatis的插件可以在MyBatis的执行过程中的多个关键点进行拦截和干预。这些关键点包括：</p><ol><li>Executor（执行器）层面的拦截： 这是SQL语句的执行层面，插件可以在SQL语句执行前后进行拦截。这包括了SQL的预处理、参数设置、查询结果的映射等。</li><li>StatementHandler（语句处理器）层面的拦截： 这是对SQL语句的处理层面，插件可以在SQL语句被执行之前进行拦截，你可以在这里修改、替换、生成SQL语句。</li><li>ParameterHandler（参数处理器）层面的拦截： 这是处理参数的层面，插件可以在参数传递给SQL语句之前进行拦截，你可以在这里修改参数值。</li><li>ResultSetHandler（结果集处理器）层面的拦截： 这是处理查询结果的层面，插件可以在查询结果返回给调用方之前进行拦截，你可以在这里对查询结果进行修改、处理。</li></ol><p>Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是<code>InvocationHandler</code>的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。</p><h4>如何编写一个插件？</h4><p>插件机制的核心是Interceptor接口，你可以实现这个接口，编写自己的插件逻辑。一个插件主要包括以下几个步骤：</p><ol><li>实现Interceptor接口： 创建一个类，实现MyBatis提供的Interceptor接口，该接口包含了intercept和plugin两个方法。</li><li>实现intercept方法： intercept方法是插件的核心，它会在方法执行前后进行拦截。你可以在这个方法中编写自己的逻辑。</li><li>实现plugin方法： plugin方法用于创建代理对象，将插件包装在目标对象上，使得插件逻辑能够被执行。</li><li>配置插件： 在MyBatis的配置文件中，通过<code>&lt;plugins&gt;</code>标签配置你的插件。通常需要指定插件类和一些参数。</li></ol><h4>Mybatis 是如何进行分页的？</h4><p>Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页，先把数据都查出来，然后再做分页。</p><p>可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。</p><p>常用的分页插件和技巧：</p><ol><li>PageHelper 插件： PageHelper 是一个流行的 MyBatis 分页插件，它简化了分页查询的操作。只需要在查询方法前调用 PageHelper.startPage(pageNum, pageSize)，然后执行查询语句，PageHelper 就会自动处理分页逻辑。</li><li>使用 RowBounds： 在 MyBatis 中，你还可以使用 RowBounds 对象来实现分页查询。通过在查询方法中传递一个 RowBounds 对象，你可以指定从哪一行开始取数据，以及每页显示多少条数据。</li><li>自定义分页插件： 如果你有特殊的分页需求，你可以编写自己的分页插件。这可能涉及到在 MyBatis 的拦截器链中插入你自己的逻辑，以实现定制化的分页处理。</li></ol><h4>分页插件的基本原理是什么？</h4><p>分页插件是一种扩展机制，它允许MyBatis在查询过程中，自动应用分页逻辑而不需要手动编写分页查询语句。分页插件的一般原理如下：</p><ol><li>拦截器(Interceptor)：分页插件实际上是MyBatis的一个拦截器，它可以在查询被执行之前或之后进行干预。</li><li>处理分页逻辑：在查询执行之前，分页插件会检测是否有分页参数传入。如果有分页参数，插件会根据数据库方言生成适当的分页查询语句。</li><li>修改查询参数：插件会修改查询的SQL语句，添加分页的限制条件。同时，它还会修改参数对象，将分页参数替换为实际的分页偏移量（offset）和每页条数（limit）。</li><li>执行查询：修改后的查询语句被执行，得到查询结果。</li><li>封装分页结果：插件会根据查询结果和分页参数，将查询结果进行切割，得到分页后的结果。</li></ol><p>分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql（SQL 拼接 limit），根据 dialect 方言，添加对应的物理分页语句和物理分页参数，用到了 JDK 动态代理，用到了责任链设计模式。</p><h4>Mybatis 是否支持延迟加载？</h4><p>所谓的延迟加载，其实就是一种优化方法，目标是为了在查数据库的时候，尽量不读取多余的数据，从而提高我们应用的表现和节约资源。在MyBatis里，这个延迟加载的技巧主要是用在处理对象关系映射的时候，也就是ORM。</p><p>来个例子帮你理解：假设有两张表，一张是订单表，另一张是商品表。每个订单下面可能有好几个商品。用延迟加载的话，当我们查一个订单的时候，MyBatis不会马上查出这个订单的所有商品，而是等到我们真的要用商品的数据时才去查。这样做就避免了在查订单的时候额外加载了一堆没用的商品。但要注意，虽然延迟加载能提升性能，可别用得过了，免得碰上懒加载的N+1问题，就是要查很多次才能拿到关联数据，结果性能就拖垮了。所以用延迟加载的时候，得根据实际情况合理配置和使用。</p><p>Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis 配置文件中，可以配置是否启用延迟加载<code>lazyLoadingEnabled=true|false</code>。</p><h4>延迟加载的基本原理是什么？</h4><p>延迟加载的基本原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法。</p><p>比如调用<code>a.getB().getName()</code>，拦截器 invoke()方法发现 a.getB()是 null 值，那么就会单独发送事务，先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用<code>a.setB(b)</code>，于是 a 的对象 b 属性就有值了，接着完成<code>a.getB().getName()</code>方法的调用。</p><p>当然了，不光是 Mybatis，几乎所有的ORM框架、包括 Hibernate，支持延迟加载的原理都是一样的。</p><h4>MyBatis如何处理懒加载和预加载？</h4><p>当谈到MyBatis中的懒加载和预加载时，我们实际上在讨论在获取数据库数据时如何处理关联对象的加载方式。  </p><p>懒加载是一种延迟加载技术，它在需要访问关联对象的时候才会加载相关数据。这意味着，当你从数据库中获取一个主对象时，它的关联对象并不会立即加载到内存中，只有当你实际调用访问关联对象的方法时，MyBatis才会去数据库中加载并填充这些关联对象的数据。懒加载适用于关联对象较多或者关联对象数据较大的情况，这样可以减少不必要的数据库查询，提升性能。  </p><p>预加载则是一种在获取主对象时同时加载其关联对象的技术。这样一来，当获取主对象时，它的所有关联对象也会被一并加载到内存中，避免了多次数据库查询。预加载适用于你确定在后续使用中肯定会访问关联对象，这样可以减少每次访问关联对象时的延迟。  </p><p>选择懒加载还是预加载取决于具体需求和场景。如果希望在尽量少的数据库查询次数下获取数据，懒加载是个不错的选择。如果在获取主对象后会频繁地访问其关联对象，预加载可能更适合，因为它可以减少多次查询带来的性能开销。  </p><p>两者都是优化数据库访问性能的手段，根据具体的使用场景选择合适的加载方式非常重要。</p><h4>{}和${}的区别是什么？</h4><h2>{ } 被解析成预编译语句，预编译之后可以直接执行，不需要重新编译sql。</h2><pre><code class="sql">//sqlMap 中如下的 sql 语句
select * from user where name = #{name};
//解析成为预编译语句；编译好SQL语句再取值
select * from user where name = ?;</code></pre><p>${ } 仅仅为一个字符串替换，每次执行sql之前需要进行编译，存在 sql 注入问题。</p><pre><code class="sql">select * from user where name = '${name}'
//传递的参数为 "seven" 时,解析为如下，然后发送数据库服务器进行编译。取值以后再去编译SQL语句。
select * from user where name = "seven";</code></pre><h4>where 1=1会不会影响性能？</h4><p>where 1=1 和 <code>&lt;where&gt;</code> 标签 两种方案，该如何选择？</p><ul><li>如果 MySQL Server版本小于 5.7，用了 MyBatis的话，建议使用<code>&lt;where&gt;</code> 标签。</li><li><p>如果 MySQL版本大于等于 5.7，两个随便选；因为在MySQL5.7后，有一个所谓的（常量折叠优化）可以在编译期消除重言式表达式。</p><ul><li>什么是重言式表达式，就是任何时候永远都为true的结果， 就会被优化器识别并优化掉，好奇的话你可以通过show warnings 查看，就会发现1=1没有了。</li></ul></li></ul><p>当然现在 MySQL Server版本基本都是 5.7以上了，不是的话那赶紧升级吧还是。</p><h4>Mybatis的预编译</h4><p>数据库接受到sql语句之后，需要词法和语义解析，优化sql语句，制定执行计划。这需要花费一些时间。如果一条sql语句需要反复执行，每次都进行语法检查和优化，会浪费很多时间。预编译语句就是将sql语句中的<code>值用占位符替代</code>，即将<code>sql语句模板化</code>。一次编译、多次运行，省去了解析优化等过程。</p><p>mybatis是通过<code>PreparedStatement</code>和占位符来实现预编译的。</p><p>mybatis底层使用<code>PreparedStatement</code>，默认情况下，将对所有的 sql 进行预编译，将#{}替换为?，然后将带有占位符?的sql模板发送至mysql服务器，由服务器对此无参数的sql进行编译后，将编译结果缓存，然后直接执行带有真实参数的sql。</p><p>预编译的作用：</p><ol><li>预编译阶段可以优化 sql 的执行。预编译之后的 sql 多数情况下可以直接执行，数据库服务器不需要再次编译，可以提升性能。</li><li>预编译语句对象可以重复利用。把一个 sql 预编译后产生的 <code>PreparedStatement</code> 对象缓存下来，下次对于同一个sql，可以直接使用这个缓存的 PreparedState 对象。</li><li>防止SQL注入。使用预编译，而其后注入的参数将不会再进行SQL编译。也就是说其后注入进来的参数系统将不会认为它会是一条SQL语句，而默认其是一个参数。</li></ol><h4>MyBatis 如何实现数据库类型和 Java 类型的转换的?</h4><p>MyBatis 类型转换主要依赖于 MyBatis 的 类型处理器(TypeHandler)机制。</p><p>TypeHandler 的核心作用：</p><ul><li>将 Java 类型转换为 JDBC 类型，用于 SQL参数设置</li><li>将 JDBC 类型转换为 Java 类型，用于查询结果的映射。</li></ul><p>具体操作流程如下：</p><ol><li>MyBatis 在加载映射文件时，根据字段类型(如 jdbcType)和Java类型(如 resultType确定使用的 TypeHandler。</li><li>在执行SQL时，ParameterHandler会使用TypeHandler 将Java 参数转换为JDBC 类型</li><li>在解析结果集时，ResultsetHandler使用TypeHandler 将JDBC类型转换为 Java对象</li></ol><h4>说说 MyBatis 的缓存机制?</h4><p>缓存：合理使用缓存是优化中最常见的方法之一，将从数据库中查询出来的数据放入缓存中，下次使用时不必从数据库查询，而是直接从缓存中读取，避免频繁操作数据库，减轻数据库的压力，同时提高系统性能。</p><p>Mybatis里面设计了二级缓存来提升数据的检索效率，避免每次数据的访问都需要去查询数据库。</p><p><strong>一级缓存是SqlSession级别的缓存</strong>：Mybatis对缓存提供支持，默认情况下只开启一级缓存，一级缓存作用范围为同一个SqlSession。在SQL和参数相同的情况下，我们使用同一个SqlSession对象调用同一个Mapper方法，往往只会执行一次SQL。因为在使用SqlSession第一次查询后，Mybatis会将结果放到缓存中，以后再次查询时，如果没有声明需要刷新，并且缓存没超时的情况下，SqlSession只会取出当前缓存的数据，不会再次发送SQL到数据库。若使用不同的SqlSession，因为不同的SqlSession是相互隔离的，不会使用一级缓存。</p><blockquote>与springboot集成时一级缓存不生效问题：一级缓存是<strong>会话级别</strong>的，要生效的话，必须要在同一个 SqlSession 中。但是与 springboot 集成的 mybatis，默认每次执行sql语句时，都会创建一个新的 SqlSession！所以一级缓存才没有生效。<br/>解决：加上 <code>@Transactional</code> 注解。如果当前线程存在事务，并且存在相关会话，就从 ThreadLocal 中取出。如果没有事务，就重新创建一个 SqlSession 并存储到 ThreadLocal 当中，供下次查询使用。</blockquote><p><strong>二级缓存是mapper级别的缓存</strong>：可以使缓存在各个SqlSession之间共享。当多个用户在查询数据的时候，只要有任何一个SqlSession拿到了数据就会放入到二级缓存里面，其他的SqlSession就可以从二级缓存加载数据。</p><p>主要区别就在于作用范围：一级缓存只在一个会话内部有效，而二级缓存可以在不同会话之间共享数据。</p><p>二级缓存默认不开启，需要在mybatis-config.xml开启二级缓存：</p><pre><code class="xml">&lt;!-- 通知 MyBatis 框架开启二级缓存 --&gt;
&lt;settings&gt;
  &lt;setting name="cacheEnabled" value="true"/&gt;
&lt;/settings&gt;</code></pre><p>并在相应的Mapper.xml文件添加cache标签，表示对哪个mapper 开启缓存：</p><pre><code class="xml">&lt;cache/&gt;</code></pre><p>二级缓存要求返回的POJO必须是可序列化的，即要求实现Serializable接口。</p><p>当开启二级缓存后，数据的查询执行的流程就是 二级缓存 -&gt; 一级缓存 -&gt; 数据库。</p><h4>为什么不推荐使用 MyBatis 二级缓存？</h4><ul><li>有复杂的数据模型或者数据之间的关联关系的会有数据不一致的影响</li></ul><p>二级缓存是以 <code>namespace(mapper)</code> 为单位的，不同 namespace 下的操作互不影响。且 insert/update/delete 操作会清空所在 <code>namespace</code> 下的全部缓存。</p><p>那么问题就出来了，假设现在有 <code>ItemMapper</code> 以及 <code>XxxMapper</code>，在 <code>XxxMapper</code> 中做了表关联查询，且做了二级缓存。此时在 <code>ItemMapper</code> 中将 item 信息给删了，由于不同 namespace 下的操作互不影响，<code>XxxMapper</code> 的二级缓存不会变，那之后再次通过 <code>XxxMapper</code> 查询的数据就不对了，非常危险。</p><p>例如：</p><pre><code class="java">@Mapper  
@Repository  
@CacheNamespace  
publicinterfaceXxxMapper{  
  
    @Select("select i.id itemId,i.name itemName,p.amount,p.unit_price unitPrice " +  
            "from item i JOIN payment p on i.id = p.item_id where i.id = #{id}")  
    List&lt;PaymentVO&gt; getPaymentVO(Long id);  
  
}  
  
@Autowired  
private XxxMapper xxxMapper;  
  
@Test  
voidtest(){  
     System.out.println("==================== 查询PaymentVO ====================");  
     List&lt;PaymentVO&gt; voList = xxxMapper.getPaymentVO(1L);  
     System.out.println(JSON.toJSONString(voList.get(0)));  
     System.out.println("====================  更新item表的name ==================== ");  
     Item item = itemMapper.selectById(1);  
     item.setName("java并发编程");  
     itemMapper.updateById(item);  
     System.out.println("====================  重新查询PaymentVO ==================== ");  
     List&lt;PaymentVO&gt; voList2 = xxxMapper.getPaymentVO(1L);  
     System.out.println(JSON.toJSONString(voList2.get(0)));  
}</code></pre><p>由于 <code>itemMapper</code> 与 <code>xxxMapper</code> 不是同一个命名空间，所以 <code>itemMapper</code> 执行的更新操作不会影响到 <code>xxxMapper</code> 的二级缓存；</p><p>再次调用 <code>xxxMapper.getPaymentVO</code>，发现取出的值是走缓存的，<code>itemName</code> 还是老的。但实际上 <code>itemName</code> 在上面已经被改了</p><h4>Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？</h4><p>最佳实践中，通常一个 xml 映射文件，都会写一个 Dao 接口与之对应。Dao 接口就是人们常说的 <code>Mapper</code> 接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中 <code>MappedStatement</code> 的 id 值，接口方法内的参数，就是传递给 sql 的参数。 <code>Mapper</code> 接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个 <code>MappedStatement</code> ，举例：<code>com.mybatis3.mappers.StudentDao.findStudentById</code> ，可以唯一找到 namespace 为 <code>com.mybatis3.mappers. StudentDao</code> 下面 <code>id = findStudentById</code> 的 <code>MappedStatement</code> 。在 MyBatis 中，每一个 <code>&lt;select&gt;</code>、 <code>&lt;insert&gt;</code>、 <code>&lt;update&gt;</code>、 <code>&lt;delete&gt;</code> 标签，都会被解析为一个 <code>MappedStatement</code> 对象。</p><p>Dao 接口里的方法可以重载，但是 Mybatis 的 xml 里面的 ID 不允许重复。并且需要满足以下条件：</p><ol><li>仅有一个无参方法和一个有参方法</li><li>多个有参方法时，参数数量必须一致。且使用相同的 <code>@Param</code> ，或者使用 <code>param1</code> 这种</li></ol><p>Mybatis 版本 3.3.0：</p><pre><code class="java">/**
 * Mapper接口里面方法重载
 */
public interface StuMapper {

 List&lt;Student&gt; getAllStu();

 List&lt;Student&gt; getAllStu(@Param("id") Integer id);
}</code></pre><p>然后在 <code>StuMapper.xml</code> 中利用 Mybatis 的动态 sql 就可以实现。</p><pre><code class="xml">&lt;select id="getAllStu" resultType="com.pojo.Student"&gt;
  select * from student
  &lt;where&gt;
    &lt;if test="id != null"&gt;
      id = #{id}
    &lt;/if&gt;
  &lt;/where&gt;
&lt;/select&gt;</code></pre><p>能正常运行，并能得到相应的结果，这样就实现了在 Dao 接口中写重载方法。</p><p><strong>Mybatis 的 Dao 接口可以有多个重载方法，但是多个接口对应的映射必须只有一个，否则启动会报错。</strong></p><p>Dao 接口的工作原理是 JDK 动态代理，MyBatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行 <code>MappedStatement</code> 所代表的 sql，然后将 sql 执行结果返回。</p><h4>MyBatis 写个 Xml 映射文件，再写个 DAO 接口就能执行，这个原理是什么?</h4><p>核心原理是 JDBC 的能力 和 动态代理，通过解析 XML映射文件和动态生成 DAO 接口实现类来完成 SQL的执行。</p><p>以下是详细的执行原理:</p><ol><li>加载配置和 Mapper 映射文件:MyBatis 启动时，通过配置文件(mybatis-config.xml)加载数据库连接信息和 Mapper 映射文件(Mapper.xml )XML文件中的 SQL被解析为内部的 MappedStatement 对象，包含 SQL语句、参数映射规则和返回结果映射规则。</li><li>动态代理实现 DAO 接口：MyBatis 为每个 DAO 接口生成一个动态代理类(Mapperproxy )，拦截接口方法调用。动态代理的核心是根据方法名和参数匹配到对应的 MappedStatement ，然后调用JDBC 来执行 SQL。</li><li>通过 JDBC 执行 SQL：MyBatis的 SqlSession 是对JDBC的封装，它的核心是使用PreparedStatement 来完成SQL的执行。根据 XML 中定义的 SQL和 DAO 接口方法传入的参数，生成完整的SQL查询，并将参数通过占位符(?)绑定到 PreparedStatement 。最终通过JDBC执行SQL，并获取Resultset。</li><li>结果映射：JDBC的查询结果( Resultset )会被 MyBatis 的 Resultmap 或 resultType 映射为 DAO 接囗方法的返回值类型(如 POJO、Map或 List)。</li></ol><h4>MyBatis 动态 sql有什么用?执行原理?有哪些动态 sq!?</h4><p>动态SQL是在 MyBatis中根据不同的条件、需求动态生成 so!语句的一种机制。它的主要目的是提高 sql 的灵活性和复用性，在复杂的查询或更新场景中，根据参数动态构建不同的 sqL语句，</p><p>动态 SQL的执行基于XML映射文件中定义的 SOL片段与标签，如 if、choose、when、otherwise、where、foreach 等，这些标签被解析，在运行时根据传入的参数值评估，最终形成完整的 SQL 语句发送到傲数据库</p><p>执行MyBatis 解析动态 SQL 的流程如下：</p><ol><li>解析动态 SQL：在映射文件加载时，MyBatis 会解析 XML文件中的动态 SQL 标签。</li><li>参数绑定：当执行 SQL语句时，MyBatis 会根据传入的参数绑定具体的值。</li><li>生成最终 SQL：根据参数值和动态 SQL 标签生成最终的 SQL语句。</li><li>执行 SQL：MyBatis 执行生成的 SQL语句，并返回结果。</li></ol><p>常见动态sql：</p><ul><li>if标签允许在 SQL 中根据条件包含不同的部分</li><li>where 标签智能地插入 WHERE 关键字，并在必要时去除多余的 AND 或 OR。</li><li>foreach 标签适用于需要遍历列表或数组，生成重复的 SQL 片段，如批量插入或 IN 条件查询。</li><li>choose, when, otherwise 标签加一起相当于Java 中的 switch 语句，根据多个条件选择一个执行。</li></ul><h3>Mybatis使用</h3><h4>使用 MyBatis 的 mapper 接口调用时有哪些要求?</h4><ul><li>接口方法名与 SQL 映射文件中的 id 要一致。</li><li>接口的全限定名要作为 xml 文件的命名空间。</li><li>参数和返回值要与映射文件的配置匹配，同时通过 @Param 注解可以处理多个参数的情况。</li><li>如果在 Spring 环境中需要进行 Mapper 扫描以注册为 Bean。</li></ul><h4>Mybatis都有哪些Executor执行器？它们之间的区别是什么？</h4><p>Mybatis有三种基本的Executor执行器，<code>SimpleExecutor</code>、<code>ReuseExecutor</code>、<code>BatchExecutor</code>。</p><p><code>SimpleExecutor</code>：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。</p><p><code>ReuseExecutor</code>：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用Statement对象。</p><p><code>BatchExecutor</code>：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。</p><p>作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。</p><h4>MyBatis中接口绑定有几种实现方式?</h4><ol><li>通过注解绑定，在接口的方法上面加上 @Select@Update等注解里面包含Sql语句来绑定（SQL语句比较简单的时候，推荐注解绑定）</li><li>通过xml里面写SQL来绑定, 指定xml映射文件里面的namespace必须为接口的全路径名（SQL语句比较复杂的时候，推荐xml绑定）</li></ol><h4>xml 映射文件中，除了常见的 select、insert、update、delete 标签之外，还有哪些标签？</h4><p>除了常见的select、insert、update和delete标签，MyBatis的XML映射文件中还有一些其他标签用于更复杂的操作和配置。以下是一些常见的额外标签：</p><ol><li>resultMap： 用于定义查询结果与Java对象之间的映射关系，可以在多个查询中重复使用。</li><li>association和collection： 用于在resultMap中定义关联关系，用于处理一对一和一对多的关系。</li><li>discriminator： 在resultMap中使用，根据不同的条件选择不同的映射规则，用于处理继承关系的映射。</li><li>sql： 可以定义可重用的SQL片段，然后在其他地方引用。主要用于减少重复编写SQL语句。</li><li>include： 用于在SQL语句中引入外部定义的SQL片段，提高可维护性。</li><li>if、choose、when、otherwise： 用于在SQL语句中进行条件判断和逻辑控制，用于动态SQL的构建。</li><li>trim、where、set： 用于在SQL语句中添加固定的SQL片段，如where和set关键字，用于动态的条件构建。</li><li>foreach： 用于在SQL语句中进行集合迭代，适用于生成IN语句等。</li><li>bind： 用于在SQL语句中声明并绑定一个变量，可以在查询中重复使用。</li><li>cache： 用于配置二级缓存。</li><li>selectKey： 用于在插入操作后获取生成的主键值。</li><li>insert、update、delete的flushCache、useGeneratedKeys、keyProperty属性： 用于配置插入、更新和删除操作的一些属性。</li></ol><h4>MyBatis 的 xml 映射文件中，不同的 xml 映射文件，id 是否可以重复？</h4><p>不同的 xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。</p><p>原因就是 namespace+id 是作为 <code>Map&lt;String, MappedStatement&gt;</code> 的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。</p><h4>MyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？</h4><p>第一种是使用 <code>&lt;resultMap&gt;</code> 标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，MyBatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，MyBatis 一样可以正常工作。</p><p>有了列名与属性名的映射关系后，MyBatis 通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。</p><h4>MyBatis 是否可以映射 Enum 枚举类？</h4><p>MyBatis 可以映射枚举类，不单可以映射枚举类，MyBatis 可以映射任何对象到表的一列上。映射方式为自定义一个 <code>TypeHandler</code> ，实现 <code>TypeHandler</code> 的 <code>setParameter()</code> 和 <code>getResult()</code> 接口方法。 <code>TypeHandler</code> 有两个作用：</p><ul><li>一是完成从 javaType 至 jdbcType 的转换；</li><li>二是完成 jdbcType 至 javaType 的转换，体现为 <code>setParameter()</code> 和 <code>getResult()</code> 两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。</li></ul><h4>MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？</h4><p>虽然 MyBatis 解析 xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，MyBatis 都可以正确识别。</p><p>原理是，MyBatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，MyBatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，MyBatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。</p><h4>模糊查询 like 语句该怎么写?</h4><p>在MyBatis中，要执行模糊查询（使用LIKE语句），可以使用SQL语句的字符串拼接或使用动态SQL来构建查询语句。</p><p>假设你要在一个查询中执行模糊查询，搜索用户的用户名包含特定关键字的情况。  </p><p>字符串拼接方式：</p><pre><code class="sql">&lt;select id="searchUsers" resultMap="userResultMap"&gt;
SELECT * FROM users
    WHERE username LIKE CONCAT('%', #{keyword}, '%')
&lt;/select&gt;</code></pre><p>在这个例子中，#{keyword}是参数占位符，表示要搜索的关键字。CONCAT('%', #{keyword}, '%')用于构建模糊匹配的字符串。  </p><p>动态SQL方式：</p><pre><code class="sql">&lt;select id="searchUsers" resultMap="userResultMap"&gt;
SELECT * FROM users
    &lt;where&gt;
        &lt;if test="keyword != null"&gt;
            AND username LIKE CONCAT('%', #{keyword}, '%')
        &lt;/if&gt;
    &lt;/where&gt;
&lt;/select&gt;</code></pre><p>在这个例子中，使用了<code>&lt;if&gt;</code>标签来创建动态条件。只有在keyword参数不为null时，才会添加AND username LIKE CONCAT('%', #{keyword}, '%')这个条件到查询语句中。</p><h4>MyBatis 自带的连接池有了解过吗?</h4><p>MyBatis 自带的连接池是 PooledDataSource 类实现的，是一个简单的连接池实现，提供了连接复用和基本的资源管理功能</p><p>执行原理：</p><ol><li>初始化连接池：PooledDataSource 会初始化一定数量的连接，放入空闲连接队列。</li><li>获取连接：调用 getconnection()方法时，优先从空闲队列中获取连接，如果空闲队列为空，且活跃连接未达上限，则创建新连接。</li><li>回收连接：使用完毕后，通过 pushConnection()方法将连接放回空闲队列。</li><li>失效检测：通过 poolPingQuery 定期检查空闲连接的可用性；失效的连接会被丢弃。</li></ol><p>关键配置：</p><ul><li>poolMaximumActiveConnections：最大活跃连接数</li><li>poolMaximumIdleConnections：最大空闲连接数。</li><li>poolMaximumCheckouTime：单个连接的最大占用时间，超过时间会被强制回收</li><li>poolPingQuery：检测连接可用性的 SQL。</li></ul><h4>Mybatis 如何实现一对一、一对多的关联查询 ?</h4><p>在 MyBatis 中，实现一对一和一对多的关联査询主要是通过 resultMap 来完成的。MyBatis 提供了两种方式来处理关联关系</p><ul><li><p>嵌套结果映射(Nested Result Mapping)</p><ul><li>适用于一次 SQL 查询中同时返回主表和关联表的数据。</li><li>使用<code>&lt;association&gt;</code>标签表示一对一的关系。</li><li>使用<code>&lt;collection&gt;</code>标签表示一对多的关系。</li></ul></li><li><p>嵌套查询(Nested Select)</p><ul><li>主查询只查主表数据，关联表数据通过单独的 SQL查询获取。</li><li>使用<code>&lt;association&gt;</code>或<code>&lt;collection&gt;</code>的select 属性指定子查询</li></ul></li></ul><h4>MyBatis如何实现动态数据源切换？</h4><p>在实现动态数据源切换方面，MyBatis有几种方法，让你能够在不同的数据库之间轻松切换。比如，你可能会在开发环境和生产环境中使用不同的数据库。下面是一些可以考虑的方法：</p><ol><li>我们可以通过配置文件来实现切换。可以在MyBatis的配置文件里配置多个数据源，然后根据需要在代码中进行切换。这就涉及到定义多个数据源的连接信息和配置，然后在代码里通过指定数据源的标识来选择要使用哪个数据源。这种方法需要在配置文件中进行一些准备工作，但切换过程相对比较容易。</li><li>可以运用AOP切面编程来实现切换。通过使用面向切面编程（AOP），可以在方法调用之前进行拦截，然后根据条件来动态地切换数据源。可以创建一个切面，将切入点设定为需要切换数据源的方法，然后在切面中实现数据源切换的逻辑。这样的做法能够将切换逻辑和业务逻辑分隔开，有助于提高代码的可维护性。</li><li>可以使用MyBatis提供的AbstractRoutingDataSource类。这个类允许你创建一个数据源路由器，根据特定的规则来选择数据源。你可以继承这个类，然后实现其中的determineCurrentLookupKey()方法，以返回当前应该使用的数据源标识。这种方式非常灵活，可以根据不同的条件来切换数据源。</li><li>使用第三方库。例如Druid和HikariCP等。这些库通常提供了更多的功能和配置选项，可以根据实际需求来选择合适的库。</li></ol><h4>MyBatis 和 MyBatis Plus 有哪些区别？</h4><ol><li>MyBatis Plus 是 MyBatis 的增强工具，提供了许多开箱即用的功能和简化的操作接口。对于单数据表的常见操作（CRUD）提供了自动化的方法，减少了 SQL 代码的编写，但对于复杂查询仍需手动编写。</li><li>MyBatis 不提供内置的分页功能，通常需要使用第三方分页插件（如 PageHelper）或手动编写 SQL。MyBatis Plus 内置了分页功能，使用非常简单。</li></ol><p>就可以看成 MyBatis  能实现的 MyBatis Plus都实现了，是增强版工具</p>]]></description></item><item>    <title><![CDATA[Agentic AI基础设施实践经验系列]]></title>    <link>https://segmentfault.com/a/1190000047383832</link>    <guid>https://segmentfault.com/a/1190000047383832</guid>    <pubDate>2025-11-10 02:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="160" referrerpolicy="no-referrer" src="/img/bVdkFLr" alt="image.png" title="image.png"/></p><h2>1. 引言</h2><p>人工智能正经历深刻变革。传统AI多为被动工具，而随着大型语言模型（LLM）和多智能体系统（MAS）的快速发展，AI Agent正向具有高度自主性的主动智能体（Agentic AI）演进。这些AI Agents能够自主思考、规划和执行复杂任务，甚至协同完成更复杂的目标。</p><p>这种演进带来了前所未有的机遇，同时也引发了新的安全挑战，特别是在身份认证与授权管理方面。近年来发生的多起安全事件充分说明了AI Agent身份管理的重要性和紧迫性。2024年11月，LangChain生态中的LangSmith平台Prompt Hub暴露出严重的身份与权限管理漏洞“AgentSmith”。攻击者通过上传带有恶意代理配置的prompt，当用户fork并执行这些prompt时，用户的通信数据包括API密钥和上传内容会被悄然中转至攻击者控制的服务器，导致敏感信息泄露。同时，攻击可能引发代理配置篡改、未经授权的API调用及远程代码执行，严重影响系统安全。</p><p>2025年披露的MCP Inspector远程命令执行漏洞（CVE-2025-49596）则是另一典型案例。该工具缺乏客户端与本地代理之间的认证，攻击者仅需诱导开发者访问恶意网页，便能绕过浏览器安全策略，通过跨站请求伪造（CSRF）攻击向本地服务发送恶意命令，实现对终端的远程控制。</p><p>这些安全事件揭示了Agentic AI系统中两个核心问题：一是AI Agent的身份认证与授权机制如何确保可信且安全；二是在复杂的代理调用链中如何有效传递和验证身份信息。本文将深入分析这些挑战，并结合亚马逊云科技平台的实践经验，提供完整的解决方案。</p><blockquote>📢限时插播：无需管理基础设施，利用亚马逊技术与生态，快速集成与部署生成式AI模型能力。<br/>✨ 精心设计，旨在引导您深入探索Amazon Bedrock的模型选择与调用、模型自动化评估以及安全围栏(Guardrail)等重要功能。<br/>⏩快快点击进入《<a href="https://link.segmentfault.com/?enc=N5LdxqgYmTdV04fSec1%2FdA%3D%3D.065p%2FZQq5zL%2B8Y7UpQbLKvE6dlxZsNUNfpIYuC%2BtLfLD8pOQs4zLQIk%2BQhIjpk0Pcldeyqslh27VdPCX4V61x9X4Mf%2BKyYyHH6IkaOQcXcCwDbNBnBgwOPaSMEun7nGy23P1t9yV4J%2FLqRsUqV6TNxuPTvZaTBryFpdiRxNeVTdCGOrsJEood9%2B0DDYhWQLwxhrijTn9j3KUJqNuYXQf%2FqiwYUmMidvcMyNUHg7ecnI%3D" rel="nofollow" target="_blank">多模一站通 —— Amazon Bedrock 上的基础模型初体验</a>》实验构建无限, 探索启程！</blockquote><h2>2. AI Agent 身份管理的核心概念与技术要求</h2><p>Agentic AI 身份（Agent Identity）相关的概念和术语，用于描述代理和工具等身份管理及凭证处理所涉及的组件、流程与关系。理解<a href="https://link.segmentfault.com/?enc=STnlSZhBHVzpk90Sy9G7DA%3D%3D.2L1T%2FB5cDKFfR51%2BdaWHzZYTWSJFom0zjqmDCZTgsFzkVqw%2BJh%2FZTI8yAbnaxAlZiXBD6B6L6%2FWovoNN0s1IAcof3l18OFSPo%2F4wgKAPI5OT5S8IBvB3mm658Bimj5QZ" rel="nofollow" target="_blank">这些术语</a>有助于深入把握AI Agent 工作流中如何协调多方组件的身份认证与授权机制。</p><p>以下列出的术语是在开发 Agentic AI 系统中常用的，已被身份与访问管理（IAM）领域广泛采纳，非亚马逊云科技的专有用语。其他通用的身份管理术语可以参考相关<a href="https://link.segmentfault.com/?enc=iG3Fgw9gr4rSVDrpE8QDDw%3D%3D.XYsieejVIRgvmdHeaZ%2FlMxj%2Fi4nZq01u4hcIYjiIZNHHZRcmikDPd21p1XIsP1BZKDNxHsYiZUAOR2rRL%2FQQ8bGJ53aR2jThODV5qKRg1vI%3D" rel="nofollow" target="_blank">官方文档</a>。</p><h3>2.1 身份与认证方面的概念与术语</h3><h3><img width="723" height="529" referrerpolicy="no-referrer" src="/img/bVdmXWK" alt="3cfb45e252caf8684179a6e2ed1d616c.jpg" title="3cfb45e252caf8684179a6e2ed1d616c.jpg" loading="lazy"/></h3><h3>2.2 OAuth and Token 管理方面的概念和术语</h3><p><img width="723" height="476" referrerpolicy="no-referrer" src="/img/bVdmXWM" alt="image.png" title="image.png" loading="lazy"/></p><h3>2.3 OAuth 2.0的授权机制是Agentic AI身份授权的核心技术</h3><p>Agentic AI系统中，普遍采用的授权机制是基于OAuth 2.0进行的，包括MCP协议也是基于其进行授权。所以，深入理解Agentic AI各组件间的身份认证与授权机制的前提是充分理解OAuth授权的流程。其中，Anthropic官方对MCP协议中采用的OAuth授权流程进行了详细描述，具体客户参考其官方网站的协议描述部分 <a href="https://link.segmentfault.com/?enc=JzMp%2BCWVuepSNcIViSzz%2FA%3D%3D.qlTVm%2BoVCE%2BAIIKRaoMM%2FzdTGO5%2BDX48YIcWl2ohLFtw3GxgUXb2d6jON93UTSyqIMOWBMQyZyJbegQS0vgW0WRV3LbqVyciRwk7cNw7nl4%3D" rel="nofollow" target="_blank">OAuth 2.0的授权流程</a>。</p><p>在Agentic AI系统的设计中，需要根据不同的业务场景来选择合适的OAuth 工作模型。其中典型的模式有2腿授权（2-Legged Auth，2LO）和3腿授权（3-Legged Auth，3LO），如果需要用户（User）参与其中的授权流程适合用3LO，如果不需要用户参与的适合用2LO。</p><p>下图是2LO和3LO授权流程的典型步骤说明：</p><p><img width="723" height="333" referrerpolicy="no-referrer" src="/img/bVdmXSy" alt="image.png" title="image.png" loading="lazy"/><br/>图1 – 2LO和3LO授权流程</p><h4>2.3.1 2LO 授权流程</h4><p>OAuth 客户端凭据授予用于无需用户交互的机器对机器身份验证，适用于代理使用 2LO 直接向资源服务器进行身份验证。例如代理Agent 访问具有服务角色的同一账户中数据库。</p><p>具体流程包括:</p><p>Client 认证: 应用发送 ‘client_id’ 和 ‘client_secret’ 到认证服务器；</p><p>Token 签发: 服务器验证密钥材料，并返回一个‘access_token‘；</p><p>资源访问: 应用使用Token访问受保护的资源。</p><h4>2.3.2 3LO 授权流程</h4><p>用于应用代表用户进行操作的场景，需要用户的同意。例如代理Agent 代表用户访问电子邮件服务。</p><p>具体流程包括:</p><p>User 重定向: 客户端重定向用户请求到授权服务器；</p><p>User 同意: 用户完成认证并同意；</p><p>Code 交换: 服务器给客户端返回授权code；</p><p>Token 请求: 客户端通过授权code和 <code>client_secret</code> 与授权服务器交换 <code>access_token</code> 和 refresh_token；</p><p>资源访问: 客户端通过使用token访问用户拥有的资源。</p><h2>3. Agentic AI 身份管理面临的核心挑战与防护策略</h2><p><a href="https://link.segmentfault.com/?enc=uquUKzZBG0gHpO7Rm1WLfQ%3D%3D.b4V3DfUKlnvFGK6B9f2TMHutzAK9%2FrWIFY6YCxm9iY8%3D" rel="nofollow" target="_blank">OWASP</a>(开放式全球应用程序安全项目，Open Worldwide Application Security Project)基于Agentic AI的特性及应用系统部署架构、各领域专家的研究及实践经验，总结了15个安全威胁，具体可参考本系列博客之《Agentic AI 安全防护-Agent隐私与安全》的对应内容。在这15个安全威胁中，有2个是与身份相关的，包括T3 权限泄漏和T9 身份欺骗和冒充。</p><ul><li>T3 权限滥用：当攻击者利用权限管理中的弱点执行未经授权的操作时，就会发生权限滥用。这通常涉及动态角色继承或错误配置。</li><li>T9 身份欺骗和冒充：攻击者利用身份验证机制冒充人工智能代理或人类用户，从而以虚假身份执行未经授权的操作。</li></ul><p>对于身份欺骗和冒充威胁，在一个典型的Agentic AI逻辑架构中，需要进行身份认证与授权的交互点非常多、且涉及到非一方自研的部分，导致风险点的控制变得复杂。身份的传递（如下图蓝色箭头和编号）是重要内容之一，最初User的身份管理和认证、Agent Action对User 身份和鉴权会话（Access Token）的传递、tools对User 的授权等，如下图：</p><p><img width="723" height="322" referrerpolicy="no-referrer" src="/img/bVdmXSz" alt="image.png" title="image.png" loading="lazy"/><br/>图2 – 身份欺骗和冒充威胁的分布点</p><h3>3.1 Agentic AI中的身份认证与授权，和传统应用中的身份认证与授权的核心区别</h3><p>传统应用（没有使用Agentic AI之前的应用）对用户的身份管理和授权是非常明确的，即对当前登录应用系统的用户身份进行认证和授权，包括单点登录SSO认证、细粒度授权和OAuth授权等。但应用系统中引入Agentic AI技术后，数据的查询和第三方系统的调用等，会由AI Agent代理来完成，因为当前登录的用户要查询或操作的内容可能不是对其自身的查询或操作，有可能是通过prompt的方式查询或操作其他用户的信息，这一点是与传统应用的最大区别。我们通过两个示例图来进行对比和说明：</p><p><img width="723" height="294" referrerpolicy="no-referrer" src="/img/bVdmXSA" alt="image.png" title="image.png" loading="lazy"/><br/>图3 – 传统应用中的身份认证与授权架构<br/><img width="723" height="323" referrerpolicy="no-referrer" src="/img/bVdmXSC" alt="image.png" title="image.png" loading="lazy"/><br/>图4 – Agentic AI系统中的身份认证与授权架构</p><p>Agentic AI 应用系统中的授权问题反映了传统访问控制模型的根本性局限。当数据通过训练、微调或检索增强生成（RAG）方式被输入到LLM中后，模型本身无法判断请求者是否有权访问这些数据。这种情况下，授权决策必须在AI应用的其他层面实现，而不能依赖模型本身的判断。</p><p>在企业级AI应用中，这个问题变得更加复杂。不同用户可能对同一数据集具有不同的访问权限，但LLM无法自主区分这些权限差异。例如，在一个企业知识管理系统中，销售团队和财务团队可能对客户数据具有不同的访问权限，但如果这些数据都被用于训练或增强同一个LLM，模型就无法自动执行这种权限区分。</p><p>解决授权问题需要在应用架构层面实施确定性的授权机制。这包括在数据输入LLM之前进行权限检查，根据用户身份和权限过滤可访问的数据源。在RAG系统中，可以根据用户权限动态选择可查询的向量数据库或知识库。在模型输出阶段，也需要根据用户权限对响应内容进行过滤和脱敏。</p><p>基于会话属性的权限传递是一种有效的解决方案。通过安全侧通道（如Amazon Bedrock Agents的会话属性）传递用户身份和权限信息，使后端系统能够在处理AI请求时执行适当的授权检查。这种方法将授权决策从不可靠的LLM推理转移到可控的应用逻辑中。</p><h3>3.2 MCP协议中混淆代理人提权问题</h3><p>MCP协议的设计理念导致其在架构层面就系统性地引入了传统安全领域中经典的“ 混淆代理人 ” 问题（ Confused Deputy Problem）。首先，MCP 服务器作为一个独立的进程运行，拥有其自身在主机系统上的权限集合，例如文件系统读写权限或网络访问权限。其次， LLM 客户端通常代表用户行事，向服务器发送请求以执行工具。但是 MCP 规范在其默认状态下，缺乏一个统一且被一致性执行的认证和授权机制，来将终端用户的身份和权限安全地传递给服务器。因此，当一个用户（可能是低权限用户）通过 LLM 提示调用一个工具时，服务器实际上是使用其自身的权限（可能是高权限）来执行该操作，而非用户的权限。这就创造了一个典型的权限提升场景，即一个低权限的请求者（用户）欺骗了一个高权限的代理（服务器）来执行越权操作。此类越权问题，通过给大模型LLM作系统级提示词限制也只能在少部分情况下生效，且有不确定性。</p><p>混淆代理问题是AI应用安全中最具挑战性的威胁之一，并把传统的威胁效果放大。这种攻击利用了AI系统的代理特性，通过具有更高权限的AI应用间接获取原本无权访问的资源。攻击的典型场景是：用户直接访问某个资源会被拒绝，但通过AI应用访问同样的资源却能成功，从而绕过了原有的安全控制。混淆代理安全威胁示例：直接访问 S3 存储桶的用户会被拒绝访问；但访问 LLM 的用户（使用 RAG 并存储来自同一 S3 存储桶的数据）则会获得访问权限。</p><p><img width="723" height="212" referrerpolicy="no-referrer" src="/img/bVdmXSD" alt="image.png" title="image.png" loading="lazy"/><br/>图5 – Agent系统中混淆代理示意图</p><p>这种攻击的根本原因在于AI应用和底层资源之间的权限不匹配。AI应用为了完成复杂任务，往往被授予了较高的系统权限，但这些权限的使用缺乏细粒度的控制。当用户通过AI应用间接访问资源时，实际上是借用了AI应用的权限，而不是基于用户自身的权限。</p><h3>3.3 构建端到端的Agentic AI身份管理的整体防护策略</h3><p>防范混淆代理攻击需要实施严格的权限一致性检查。无论用户通过何种途径访问资源，都应该基于相同的权限模型进行授权决策。这要求在AI应用的架构设计中引入用户身份传递机制，确保底层系统能够识别真实的请求者身份。</p><p>实施细粒度的权限代理是另一种有效的防护策略。AI应用不应该拥有超出其功能需求的权限，而应该基于具体的用户请求动态获取相应的权限。这可以通过权限委托机制实现，AI应用代表用户请求特定的权限，而不是拥有固定的高权限。</p><p>审计和监控机制对于检测混淆代理攻击至关重要。系统应该记录所有的权限使用情况，包括权限的来源、使用者、访问的资源和操作类型。通过分析这些审计日志，可以识别异常的权限使用模式和潜在的安全威胁。</p><p>OWASP对于Agentic AI的15个威胁风险中，虽然只有2个与身份相关，但这两个风险点特别是T9身份欺骗与冒充威胁，会发生在Agentic AI系统中的多个环节，因此构建Agentic AI系统的端到端身份管理解决方案是非常有必要的，具体参考架构图如下：</p><p><img width="723" height="298" referrerpolicy="no-referrer" src="/img/bVdmXSE" alt="image.png" title="image.png" loading="lazy"/><br/>图6 – Agentic AI系统中端到端身份认证与授权架构</p><p>端到端的身份认证与授权系统，包括如下几个核心能力。具体示例可以参考下一章节的基于亚马逊Bedrock AgentCore Identity开发Agentic AI系统的身份模块的相关内容。</p><ol><li>Agent入方面的认证和授权：对请求的用户进行认证和授权，包括通过第三方身份提供商登录进来的用户。</li><li>外部工具或服务对Agent的授权：不论是自己研发的一方工具，还是第三方研发的商业化或开源的工具，都需要对Agent或Agent代表用户的授权，避免委托人攻击。</li><li>Agent访问外部工具或服务的能力（即出方向）：为了配合外部工具或服务对Agent的授权，Agent需要具备 OAuth 客户端的能力，包括2LO和3LO的方式。如果是访问云资源，需要有保存云资源访问短期权限的能力，如IAM role或STS。</li><li>Tool Gateway入方向认证和授权：如果通过Tool Gateway方式集中管理多个MCP服务器，Agent由Tool Gateway来访问tools，那么Tool Gateway需要具备对Agent或Agent代表用户的授权。</li></ol><h2>4. 亚马逊云科技云平台上的AI Agent身份管理解决方案</h2><h3>4.1 方案一： Amazon Bedrock AgentCore Identity – 全托管一站式解决方案</h3><p>Amazon Bedrock AgentCore Identity 是一项全面的身份和凭证管理服务，专为 AI 代理和自动化工作负载而设计。它提供安全的身份验证、授权和凭据管理功能，使用户能够调用代理，而代理能够代表用户访问外部资源和服务的同时保持严格的安全控制和审计跟踪。该服务与 Amazon Bedrock AgentCore 原生集成，为代理应用程序提供全面的身份和凭证管理。</p><p>AgentCore Identity 解决了 AI 代理部署中的一个根本性挑战：让代理能够在多个服务中安全地访问用户特定数据，同时不牺牲安全性和用户体验。传统方法要么使用广泛的访问凭证而缺乏细粒度控制，要么需要为每次服务集成获取明确的用户同意（这会带来糟糕的用户体验）。AgentCore Identity 通过一个全面的工作流实现零信任安全原则和基于委托的身份验证来解决这一问题。</p><h4>Amazon Bedrock AgentCore Identity 涉及的2种身份认证和授权</h4><p>入站授权：Inbound Auth 是指验证用户或客户端应用的认证机制，用于控制谁可以访问和调用您的代理或工具。</p><p>出站授权：Outbound Auth 是指已通过入站认证的代理，安全访问目标服务的认证机制，使代理能够安全地调用各种外部API、Lambda函数等资源。</p><p><img width="723" height="575" referrerpolicy="no-referrer" src="/img/bVdmXSF" alt="image.png" title="image.png" loading="lazy"/><br/>图7 – Bedrock AgentCore Identity认证与授权架构图</p><h4>入站授权</h4><p>用户通过其组织的现有身份提供者（如 Auth0、Cognito 或其他 OIDC 兼容系统）进行身份验证，并获得访问令牌或身份令牌。该令牌包含用户身份信息和授权范围，为整个工作流程建立用户的身份上下文。应用程序接收此令牌，并将使用它来授权对代理的请求。</p><p>下面以<a href="https://link.segmentfault.com/?enc=5dE58h1tfAzUXErlTv5%2BMQ%3D%3D.LNjDqfjDE6zjczxj69gj7%2B17ZfK54Ab6ARddqYz3QDjn2fmhMHlHngAhSsXpB%2BBU" rel="nofollow" target="_blank">Amazon Cognito</a> 为例，讲解如何配置入站授权。</p><p>在开发应用的时候，在代码中可以授权Cognito User Pool里面的用户访问权限。Amazon Cognito 是 Web 和移动的应用程序的身份识别平台。借助 Amazon Cognito，您可以从Cognito用户池、企业目录或者 Google 和 Facebook 等消费者身份提供商提供用户的身份验证和授权。首先创建1个Cognito User Pool，在这个User Pool中创建1个App client，以及1个用户，假设你执行下面的命令通过用户名密码的方式授权用户：</p><pre><code># Authenticate User
aws cognito-idp initiate-auth \
--client-id "$CLIENT_ID" \
--auth-flow USER_PASSWORD_AUTH \
--auth-parameters USERNAME='testuser',PASSWORD='MyPassword123!' \
--region "$REGION" \
auth.json</code></pre><p>可以得到授权的返回结果，结果中包含JSON Web 令牌（JWT）格式的AccessToken，RefreshToken和IdToken，AccessToken 包含权限范围(scope)，比如”cognito:groups”: [”developers”, “team-alpha”]定义允许哪些组访问，”scope”: ”myApi/profile.read myApi/profile.write myApi/account”用于API访问授权，RefreshToken 用于获取新的Access Token和ID Token，IdToken 包含用户身份信息，用于身份验证。返回的信息类似于：</p><pre><code>"eyJraWQiOiJvcHE5UmpBMTFBMWFcLzdoUFdRaUgwRmlDTjlMYm1QbnJQRWM3SVQ1M05XTT0iLCJhbGciOiJSUzI1NiJ9.eyJvcmlnaW5fanRpIjoiZTRkN2M1OWUtM2NjZC00OGFhLWFkN2YtMmY3ZTgzMmEzZDAzIiwic3ViIjoiZDRhODA0ZjgtYTA0MS03MGU0LTY3MmYtNzk2ZWM2M2VlNzQwIiwiYXVkIjoiNWFmNXBvaW8wbG5pa29zbWtnZjQxYjNrODAiLCJldmVudF9pZCI6IjFiNGZiYzA2LWVlMzgtNDBlYi1iNjRhLTA3ZTllNzIzNzVlNiIsInRva2VuX3VzZSI6ImlkIiwiYXV0aF90aW1lIjoxNzUzNzc5MzQ4LCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAudXMtZWFzdC0xLmFtYXpvbmF3cy5jb21cL3VzLWVhc3QtMV9vQTJKTkMxdnUiLCJjb2duaXRvOnVzZXJuYW1lIjoidGVzdHVzZXIiLCJleHAiOjE3NTM3ODI5NDgsImlhdCI6MTc1Mzc3OTM0OCwianRpIjoiNDBiNjEwODMtZmRiZi00YzBmLTk3OTEtOGJmYTJjOGUwNDk1In0.A9wVkmed3aet3Q3mgvSF5-KLcEskBf5JOQnqSuyP4Rv2uz_Q7BhGgDV-AfHiBn9SNI1LI6QxlRp-YqRrh4lyyxsdrQGAH5fIlYvVproslLHlSdq2tPb9klHzPjOpyYTNt3cBCq1WRGiiklfvSM1R-RJ8546IPLP2LN-oEXL-kKNdUpxSLUWSsjuk9kkH1ZkN27NxRtnjzc1KG7MBHJRtVsGUsF8b7m5L1rSYzorbj19j2z5oCHnfZHm8wZkWteEAED2jsjJaRCEwyzqXBgSpnTIy8gYO_tlpwswCpg9dgR1MSwK72OjQvLsf_xubRq2Ykv2RylF4cdF8EuGtLOIb_w"</code></pre><p>如果没有带上Access Token，直接调用AgentCore Runtime的时候将看到一条错误消息：”AccessDeniedException: An error occurred (AccessDeniedException) when calling the InvokeAgentRuntime operation: Agent is configured for a different authorization token type”。这些token短期有效，可以在cognito中配置刷新的时长。并且Amazon Cognito提供托管的登录和注册页面，以及丰富的安全能力，比如要求用户绑定MFA，这样可以避免未授权的人拿到用户名密码伪装成授权用户使用访问权限。</p><h4>出站授权</h4><p>Amazon Bedrock AgentCore Identity验证对亚马逊云科技 资源、第三方服务或 AgentCore Gateway 目标的访问权限。您可以使用 OAuth 2LO/3LO 或 API 密钥。身份系统简化了管理多种凭证类型的复杂性，同时为身份验证和授权操作提供了统一的接口。</p><p>在代码中可以通过调用API：create_api_key_credential_provider，将能够访问第三方工具的API密钥保存到AgentCore Identity的Resource Credential Provider中。当用户发起代理交互时，应用程序会发出请求，代表用户调用 AI 代理。此请求包含用户的身份验证令牌以及代理需要完成的任何必要上下文。应用程序充当代理，将用户的身份验证请求转发到代理基础架构，同时维护用户的身份上下文。</p><p>出站授权支持以下三种身份：</p><p><img width="723" height="373" referrerpolicy="no-referrer" src="/img/bVdmXW4" alt="image.png" title="image.png" loading="lazy"/></p><h4>AgentCore Identity 的安全性</h4><ul><li>安全的凭证存储，代码中没有硬编码的API密钥，不容易泄漏机密</li><li>跨多种资源类型的一致身份验证接口</li><li>全面的审计日志以确保安全性和合规性</li><li>基于身份和上下文的细粒度访问控制</li><li>通过 AgentCore SDK 简化集成</li></ul><h3>4.2 方案二： 基于Amazon Bedrock Agent构建端到端的细粒度访问控制的AI Agent</h3><p>我们为客户提供了一个基于Amazon Bedrock Agents构建AI Agent的细粒度访问控制的<a href="https://link.segmentfault.com/?enc=xUJzwaxgtRUyvVAdYIyduQ%3D%3D.7%2Bdf9n1KmkEXEbVq6M9K0G8ggRo1m0%2FCyzm%2B8dOEgB5eLKQXlFT3YGmocJow%2FQwOX8bA5fsWFzHN1Kh9i1GvCA%3D%3D" rel="nofollow" target="_blank">安全实施方案</a>。该方案通过结合多个亚马逊云科技服务，实现了安全可靠的Agentic AI 应用访问控制体系。整个系统设计注重安全性和访问控制，确保用户只能访问其权限范围内的数据，是一个典型的教育领域生成式AI应用安全实践案例。核心内容包括：</p><p>1）通过实际的应用场景，以学校助手(SchoolAgent)为例，通过聊天界面让不同角色（如学生、教师、监护人）基于各自权限查询和获取信息。</p><p>2）安全控制机制的设计，</p><ul><li>使用Amazon Cognito进行用户身份验证</li><li>通过Amazon Verified Permissions 实施细粒度访问控制</li><li>确保AI代理能识别用户身份并只提供授权范围内的数据</li></ul><p>3）访问权限设计：</p><ul><li>家长只能访问其子女的数据</li><li>教师仅可查看其任教班级的信息</li><li>通过分层安全控制确保数据访问安全</li></ul><p>通过如下参考架构，可以实现完整的身份认证与授权流程。</p><p><img width="723" height="345" referrerpolicy="no-referrer" src="/img/bVdmXSQ" alt="image.png" title="image.png" loading="lazy"/><br/>图8 – 构建端到端身份认证与授权架构</p><h3>4.3 方案三： 基于亚马逊云科技中国区服务构建灵活可控全自建方案</h3><p>鉴于 Amazon Bedrock AgentCore Identity 和 Bedrock Agents 等专有托管方案在中国区尚未落地，企业可基于亚马逊云科技中国区现有服务，采用完全自建方式灵活实现 Agentic AI 应用系统中的身份认证与授权管理。</p><p>本方案充分结合企业自有 SSO/OIDC/AD、JWT Token、自主用户池、API Gateway、IAM、STS、Secrets Manager 及标准 API 调用链，实现全流程零托管、数据可控、最小权限、合规可审计。架构兼容混合云和本地 IT，便于演进和平滑升级。</p><p>优势：无需依赖云上托管服务，政策合规性强，权限控制细腻，组件替换灵活，适配大型企业和高度自定义场景。</p><p>适用场景：数据高敏感、权限差异大、异构 IT 环境、对接自有 ID 体系或多活部署的 Agent 应用。</p><p><img width="723" height="783" referrerpolicy="no-referrer" src="/img/bVdmXSR" alt="image.png" title="image.png" loading="lazy"/><br/>图9 – 基于亚马逊云科技中国区服务构建灵活可控全自建方案</p><h3>4.4 MCP Server 认证&amp;授权管理</h3><p>以下章节介绍使用Amazon  AgentCore和Amazon Cognito组件实现Agent代理MCP server认证鉴权的实施示例。</p><p>Amazon AgentCore 是一种服务，旨在简化和加速 AI 代理的部署和管理。它提供了一系列工具和 API，帮助开发者快速构建、部署和管理 AI 代理。Amazon AgentCore 支持多种编程语言和框架，使得开发者可以灵活选择最适合自己的工具。</p><p>Amazon Cognito 是一种用户身份和访问管理服务，它允许开发者轻松地添加用户注册和登录功能到他们的应用中。Amazon Cognito 提供了两个主要组件：</p><ol><li>用户池：用户池是一个完全托管的用户目录，可以用于管理和验证用户。用户池支持多种身份验证机制，包括用户名和密码、电子邮件和短信验证码、社交身份提供商（如 Google、Facebook）等。</li><li>身份池：身份池允许应用程序获取临时亚马逊云科技凭证，以访问亚马逊云科技资源。身份池可以与用户池集成，为经过身份验证的用户提供访问权限。<br/>部署 AgentCore MCP Server 的详细步骤：</li></ol><p>在部署 AgentCore MCP Server 时，我们可以利用 Amazon Cognito 进行用户池和应用客户端的设置，以实现鉴权功能。以下是详细的实现步骤和代码示例：</p><ol><li>创建 Amazon Cognito 用户池和应用客户端</li></ol><p>首先，我们需要在亚马逊云科技管理控制台中创建一个 Cognito 用户池和应用客户端。以下是创建用户池和应用客户端的命令示例：</p><pre><code>aws cognito-idp create-user-pool --pool-name mcp-server-user-pool
aws cognito-idp create-user-pool-client --user-pool-id &lt;USER_POOL_ID&gt; --client-name mcp-server-app-client</code></pre><ol start="2"><li>创建 IAM 角色</li></ol><p>为了让 MCP 服务能够与 Cognito 进行交互，我们需要创建一个 IAM 角色，并附加相应的策略。以下是创建 IAM 角色的示例代码：</p><pre><code>import boto3

client = boto3.client('iam')

# 创建 IAM 角色
response = client.create_role(
    RoleName='agentcore-mcp-server-role',
    AssumeRolePolicyDocument=json.dumps({
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Principal": {
                    "Service": "ec2.amazonaws.com"
                },
                "Action": "sts:AssumeRole"
            }
        ]
    })
)

# 附加 Cognito 访问策略
client.attach_role_policy(
    RoleName='agentcore-mcp-server-role',
    PolicyArn='arn:aws:iam::aws:policy/AmazonCognitoPowerUser'
)</code></pre><ol start="3"><li>配置 AgentCore Runtime</li></ol><p>在配置 AgentCore Runtime 时，我们需要指定 Cognito 用户池和应用客户端的信息，以及 IAM 角色的 ARN。以下是配置示例：</p><pre><code>response = agentcore_runtime.configure(
    entrypoint='mcp_server.fixed.py',
    execution_role_arn='arn:aws:iam::687912291502:role/agentcore-mcp-server-auto-role',
    auto_create_ecrs=True,
    requirements_txt='requirements.txt',
    region='region',
    authorizer_configuration=auth_config,
    protocol='MCP',
    agent_name='mcp_server_auto'
)</code></pre><ol start="4"><li>启动 AgentCore Runtime</li></ol><p>最后，我们可以启动 AgentCore Runtime，并通过 Cognito 进行用户认证和鉴权。以下是启动示例代码：</p><p>launch_result = agentcore_runtime.launch()</p><p>在 AI Agent应用中，MCP Server 的鉴权机制是确保系统安全性和稳定性的关键。通过 Amazon AgentCore 和 Amazon Cognito，我们可以轻松地实现和管理 MCP 服务器的鉴权功能。这不仅提高了系统的安全性，还简化了开发和部署过程，使得开发者可以更专注于 AI 模型和业务逻辑的开发。</p><h2>5. 结语</h2><p>在Agentic AI应用系统快速发展的今天，身份认证与授权管理已成为构建安全可信AI生态系统的基石。从”AgentSmith”到”MCP Inspector”等安全事件的教训表明，我们必须高度重视AI Agent的身份管理问题。构建安全可信的系统需要遵循最小权限、零信任验证、纵深防御和持续监控等核心设计原则，同时采用阶段化实施策略，从基础身份认证逐步扩展到完整的安全体系。</p><p>面对传统身份管理向动态化、智能化和细粒度方向的演进，企业可以通过采用OAuth 2.0、去中心化身份等技术框架，结合<a href="https://link.segmentfault.com/?enc=CoF1H5pIsvP5S%2BWRxIXASw%3D%3D.0Xz%2FJDWZBJ5SgGxWJCQiHiO%2B%2B03IKNsYA0kuQqw67dujsCO3OuWxG3wN4HoB6NOvWoJIJmT7jEO91i20RbmitA%3D%3D" rel="nofollow" target="_blank">Amazon Bedrock AgentCore Identity </a>等云平台解决方案，有效应对混淆代理人等特有的安全威胁。通过建立确定性的授权机制和安全的身份信息传递通道，确保AI Agent在代表用户执行任务时既保持高效性，又不失安全性。</p><p>随着技术的持续成熟和标准化进程的推进，AI Agent身份管理将变得更加智能和自适应。现在正是企业规划和实施AI Agent身份管理系统的最佳时机，唯有建立起完善的身份认证与授权管理体系，才能确保AI Agent在为人类社会创造价值的同时，始终保持在可控和安全的轨道上运行。这不仅是技术发展的必然要求，更是AI技术走向成熟和普及的重要基石。<br/><strong>本篇作者</strong><br/><img width="723" height="562" referrerpolicy="no-referrer" src="/img/bVdmYRm" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>本期最新实验《<a href="https://link.segmentfault.com/?enc=KWSdn3QRfQpu61T3y3LF4A%3D%3D.olT6ShmBVN9AZJyHHRsFGtVAjl3mqn9B0NnKMJxXmJ8VaNRcGiYUm1QbzQP%2FKd8IHaZ%2FeoDlVxIPxeeKX1PuRqk8qc8wjGJMLWVZVuRADjAzgFe7YzAUawOcrbDfItYVcaT%2FgCMNmOA%2FAEgiqLb%2FS88NiJ6BFyE82hnLwqaHWlLKXlNPdfJ2c0Gb7NM2QJm9yiMbErOOIkFBHCvoqhYJsM6%2BHdJQJUz1Ows4zv3HQJY%3D" rel="nofollow" target="_blank">多模一站通 —— Amazon Bedrock 上的基础模型初体验</a>》<br/>✨ 精心设计，旨在引导您深入探索Amazon Bedrock的模型选择与调用、模型自动化评估以及安全围栏(Guardrail)等重要功能。无需管理基础设施，利用亚马逊技术与生态，快速集成与部署生成式AI模型能力。<br/>⏩️[点击进入实验] 即刻开启  AI 开发之旅<br/>构建无限, 探索启程！</blockquote><p>关于Agentic AI基础设施的更多实践经验参考，欢迎点击：<br/><a href="https://link.segmentfault.com/?enc=xPvLbaw%2BUAfMUgWqJnVA3Q%3D%3D.8qPBK4LVfWpB%2BJAEiVdyFn8upikJs4%2F%2BaYI1nMwHGyqM4L%2B%2Bce%2FmmMU%2BuUEsLPCXBdKKsl9bubz%2FRD4AoGnA%2BXwNHnR%2FV1%2FNb7w9QbeLA%2FjWd%2FDyxjrTS2MyvJR9GbY4" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（一）：Agent应用开发与落地实践思考</a></p><p><a href="https://link.segmentfault.com/?enc=TOviAV144P91hkvaV9FKfA%3D%3D.mxX33gbNwmw2L60hC1yY%2BZZ5x3E%2BMUctBOrSteMf0R66ZO%2ByKmGRV%2BT4uZW4UMRKPhy1haL%2BJsXMByQBD5utj8ZnZXBUYA368lc9Qia0%2Boo%3D" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（二）：专用沙盒环境的必要性与实践方案</a></p><p><a href="https://link.segmentfault.com/?enc=BDa9PywmyXShhBYRbu9n5w%3D%3D.dGDvAQHdvdJsd21Y0CN61tbMgIV9BRfsL%2BWQLhtXDHkMLRhvL20ZM7AaX%2BXXopZmpw1fH3smNOuYUdiW8HiC%2BpxGLt06lTCJYt2tNW5kbSq9MbZzUL%2FPn06NV031MphqydfxW7KvHnr%2FM%2FljZLvZNVUF5LCWwuyZV8m4uZaGfHO%2FtGb3e4ZfXNaLqGCUxfOaEHMcsseO2ekifQWmSOgIDg%3D%3D" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（三）：Agent记忆模块的最佳实践</a></p><p><a href="https://link.segmentfault.com/?enc=Xsr1hwDFoqlaZ1lNVN%2BARw%3D%3D.gajqZweD3ADdhq6Xe6hIQ9HGHyShQ1LKoKujdznU%2Btc2U6Kc2o1SiiBIQ%2FBfyF9wbCtBnhuHrAcqiVJdY6D6uU2su33hK4NEl2T7yb0NvTQns3PRqSirEx0OK8R11209DImDKZjxxSLqqFx7UjGr2pAKZiP8HyvotzQuUpqcWQA%3D" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（四）：MCP服务器从本地到云端的部署演进</a></p><p><a href="https://link.segmentfault.com/?enc=HW1H45%2Bwex0zxRkDLhxjYg%3D%3D.ExeBBOLDrZcFJCJ5EJLdlTvaftHN6AFj2wbs%2FNRkxKYljt2oYzLhmP6iPYXhLn4eELTno5FPOaGuNpFSDaJY6iDPgIXkc%2Fjlu4QZ%2BLWiuOuIeQ35tmeNYpubZGfDHcnX" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（五）：Agent应用系统中的身份认证与授权管理</a></p><p><a href="https://link.segmentfault.com/?enc=fO%2FizRp9rFjWGddd%2FgdWZA%3D%3D.%2BPTcVSK93UymBC9ZnyVs5mMdsNtF4rt1Y5vwc38w5vwdqojL47hUmHSYfCuFC6SMJyiKfk6%2BDMgjnyk01GzCbg%3D%3D" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（六）：Agent质量评估</a></p><p><a href="https://link.segmentfault.com/?enc=pwi%2Fw%2BBixMke4YL5hbt2MQ%3D%3D.3KWQA4W9H3ht8XtVtpB9XzpfHuGCdK9h6m%2BS3MgAUawk%2B1DuBUtuG%2F8gP9CLefx3MqwLfq%2BCLIF4U9aujeR3LJCrd%2BNUke9o0EgT1E%2B5%2FmbveddzfegU9Lnworii1jgk" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（七）：可观测性在Agent应用的挑战与实践</a></p><p><a href="https://link.segmentfault.com/?enc=OOLSoaJ7EtE3OWW0YL8ziw%3D%3D.mIyChxUCth5yOUDzP04tRZ3Yl%2FOnbqbTaN%2BXEcX7TCTZ7%2FnSjksQQ299UhwsSbGVEfKAHrUmJUbz%2FqQdfGMGWvztTPia%2FBnBul147M9cL0rfYuWd8nsxEl79rHbL%2FWqm" rel="nofollow" target="_blank">Agentic AI基础设施实践经验系列（八）：Agent应用的隐私和安全</a></p><p>*前述特定亚马逊云科技生成式人工智能相关的服务目前在亚马逊云科技海外区域可用。亚马逊云科技中国区域相关云服务由西云数据和光环新网运营，具体信息以中国区域官网为准。</p>]]></description></item><item>    <title><![CDATA[想不到吧！68%做AI的Java开发者选]]></title>    <link>https://segmentfault.com/a/1190000047383776</link>    <guid>https://segmentfault.com/a/1190000047383776</guid>    <pubDate>2025-11-10 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>文 / 勇哥<br/>原创文章，转载请联系授权</blockquote><p>在前一篇文章中，我们探讨了《<a href="https://link.segmentfault.com/?enc=PaFcP8PQP4mT14mJVBunCw%3D%3D.LTP%2FzJvwo52zRqzrtGdhUvTxhP%2FQjy40RjObyNrkyfQfzc87cNlgVs8GdHy%2F%2F1Sq4Cwb1y3c0XbhskiN804bdw%3D%3D" rel="nofollow" target="_blank">Spring AI，一个让 Spring 应用轻松拥抱 AI 的统一框架</a>》。今天，让我们深入剖析LangChain4j——这个被Java开发者亲切称为"大模型开发瑞士军刀"的框架，它是在2023年年底由LangChain官方和社区贡献者共同发起，旨在为Java生态提供专业、高效的大模型应用开发解决方案，它的出现填补了Java生态在大模型应用开发领域的关键空白。</p><p>作为一名拥有10年Java开发经验的架构师，我见证了太多Java团队在大模型时代的迷茫与挣扎：一方面是Python生态的LangChain、LlamaIndex等框架如日中天，另一方面是企业中大量Java系统亟需拥抱AI能力。LangChain4j的出现，就像给Java开发者送来了一把钥匙，让他们能够用熟悉的语言和工具打开大模型应用开发的大门。</p><p><strong>核心观点：LangChain4j是Java开发者的"大模型应用开发加速器"，它为企业Java应用与大模型能力之间搭建了一座标准化、专业化的桥梁，让Java开发者能够以最低的学习成本构建企业级大模型应用。</strong></p><h2>一、LangChain4j：为什么它是Java开发者的"大模型开发利器"？</h2><p>想象一下，你是一名Java开发者，想要将大模型能力集成到现有企业系统中。你面临的选择是什么？</p><p>要么使用Python的LangChain框架，然后通过各种复杂的集成方式与Java系统连接；要么直接调用OpenAI等API，自己处理繁琐的请求构造、响应解析、状态管理等工作。这两种方式都像是在崎岖不平的山路上行驶，充满了颠簸和不确定性。</p><p>LangChain4j就像是为Java开发者铺设的一条高速公路，它提供了：</p><ul><li><strong>统一的API抽象</strong>：屏蔽不同大模型服务提供商的差异，提供一致的调用体验；</li><li><strong>丰富的企业级组件</strong>：从聊天记忆到向量检索，从提示词模板到工作流编排，满足企业应用开发的各种需求；</li><li><strong>与Java生态深度融合</strong>：支持Spring Boot、JPA等主流Java技术栈，让集成变得简单自然。</li></ul><p>一句话，LangChain4j让Java开发者能够用"Java的方式"开发大模型应用，无需学习新的编程语言或框架理念。</p><h2>二、LangChain4j的核心框架：6大组件构建大模型应用的"积木系统"</h2><p>LangChain4j提供了一套完整的组件体系，让开发者能够像搭积木一样构建大模型应用：</p><h3>2.1 LLM Client：连接大模型的"桥梁"</h3><p>一句话概括：LLM Client是与各类大语言模型交互的标准化接口，负责处理请求构造、响应解析和错误处理。</p><p><strong>核心功能：</strong></p><ul><li><strong>模型适配</strong>：支持OpenAI、Azure OpenAI、Anthropic、Llama等多种模型服务；</li><li><strong>参数配置</strong>：统一的参数设置接口，包括温度、最大token数、超时时间等；</li><li><strong>异步支持</strong>：提供同步和异步调用方式，满足不同场景需求。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>按需选择模型</strong>：根据任务复杂度和预算选择合适的模型；</li><li><strong>合理设置参数</strong>：温度值控制创造性，较高的温度适合创意生成，较低的温度适合精确回答。</li></ul><p>适用场景：基础问答、内容生成、代码辅助等各类大模型交互场景。</p><h3>2.2 Chat Memory：维护对话上下文的"记忆系统"</h3><p>一句话概括：Chat Memory负责存储和管理对话历史，让模型能够"记住"之前的交互内容，实现连续对话。</p><p><strong>核心功能：</strong></p><ul><li><strong>消息存储</strong>：记录用户问题和模型回复；</li><li><strong>上下文管理</strong>：智能截断和保留重要信息；</li><li><strong>多种实现</strong>：支持基于窗口、令牌计数等多种记忆策略。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>合理设置记忆大小</strong>：避免过长历史导致token超量，又要保留足够的上下文；</li><li><strong>考虑性能影响</strong>：内存型记忆适合小规模应用，大规模应用应考虑持久化方案。</li></ul><p>适用场景：聊天机器人、客户服务、交互式助手等需要上下文理解的场景。</p><h3>2.3 Prompt Template：标准化提示词的"模板引擎"</h3><p>一句话概括：Prompt Template提供了一种结构化创建提示词的方式，通过变量替换生成个性化提示。</p><p><strong>核心功能：</strong></p><ul><li><strong>模板定义</strong>：使用占位符定义可复用的提示词模板；</li><li><strong>变量替换</strong>：在运行时动态填充模板内容；</li><li><strong>格式控制</strong>：支持不同的输出格式要求。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>模板复用</strong>：将常用提示词抽象为模板，提高代码复用性；</li><li><strong>提示词工程</strong>：结合提示词工程最佳实践设计模板内容。</li></ul><p>适用场景：需要根据用户输入动态生成提示词的各类应用。</p><h3>2.4 Chain：编排工作流的"流程引擎"</h3><p>一句话概括：Chain是将多个组件组合成工作流的核心抽象，实现复杂业务逻辑的编排。</p><p><strong>核心功能：</strong></p><ul><li><strong>组件组合</strong>：将模型、记忆、模板等组件链接起来；</li><li><strong>数据流转</strong>：控制数据在组件之间的流动和转换；</li><li><strong>错误处理</strong>：提供统一的异常处理机制。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>模块化设计</strong>：将复杂流程拆分为简单的链式组件；</li><li><strong>职责单一</strong>：每个Chain专注于完成一个具体功能。</li></ul><p>适用场景：复杂的大模型应用，需要多步骤处理的业务流程。</p><h3>2.5 Agent：自主决策的"智能助手"</h3><p>一句话概括：Agent是能够基于目标和环境做出决策并执行动作的高级组件，实现智能化任务处理。</p><p><strong>核心功能：</strong></p><ul><li><strong>目标分解</strong>：将复杂任务分解为子任务；</li><li><strong>工具使用</strong>：调用外部工具完成特定操作；</li><li><strong>决策路径</strong>：根据执行结果动态调整后续步骤。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>合理定义工具</strong>：根据任务需求定义清晰的工具接口；</li><li><strong>设置边界</strong>：明确Agent的能力范围，避免无限递归或越界行为。</li></ul><p>适用场景：需要自主完成复杂任务的应用，如数据分析助手、个人助理等。</p><h3>2.6 Embedding：文本向量化的"翻译官"</h3><p>一句话概括：Embedding负责将文本转换为向量表示，是实现语义搜索、相似性比较的基础。</p><p><strong>核心功能：</strong></p><ul><li><strong>文本编码</strong>：将文本转换为高维向量；</li><li><strong>向量存储</strong>：与各类向量数据库集成；</li><li><strong>相似度计算</strong>：提供向量相似度比较功能。</li></ul><p><strong>实战要点：</strong></p><ul><li><strong>选择合适的嵌入模型</strong>：根据文本类型和精度要求选择合适的模型；</li><li><strong>优化向量存储</strong>：考虑索引策略和检索性能。</li></ul><p>适用场景：知识库问答、文档检索、内容推荐等需要语义理解的场景。</p><h2>三、LangChain4j实战：从入门到精通的4个步骤</h2><h3>3.1 步骤1：环境准备与基础配置</h3><p><strong>核心工作：</strong></p><ul><li><strong>添加依赖</strong>：在Maven或Gradle项目中添加LangChain4j相关依赖；</li><li><strong>配置API密钥</strong>：安全管理大模型服务的API密钥；</li><li><strong>设置开发环境</strong>：确保Java版本兼容（推荐JDK 17+）。</li></ul><p><strong>实战建议：</strong></p><ul><li>使用环境变量或配置管理系统存储API密钥，避免硬编码；</li><li>从官方示例项目开始，快速熟悉基本使用方式。</li></ul><h3>3.2 步骤2：构建基础聊天应用</h3><p><strong>核心工作：</strong></p><ul><li><strong>创建模型实例</strong>：初始化适合任务的大模型客户端；</li><li><strong>配置聊天记忆</strong>：实现多轮对话能力；</li><li><strong>封装交互接口</strong>：为用户提供友好的交互方式。</li></ul><p><strong>实战建议：</strong></p><ul><li>从简单的单轮对话开始，逐步添加记忆功能；</li><li>实现流式响应，提升用户体验；</li><li>考虑添加错误处理和日志记录。</li></ul><h3>3.3 步骤3：实现知识库增强（RAG）</h3><p><strong>核心工作：</strong></p><ul><li><strong>文档处理</strong>：加载、解析和分块文档；</li><li><strong>向量化处理</strong>：使用嵌入模型转换文本为向量；</li><li><strong>向量存储</strong>：选择合适的向量数据库存储向量；</li><li><strong>检索增强</strong>：实现基于相似度的检索和上下文增强。</li></ul><p><strong>实战建议：</strong></p><ul><li>针对不同类型文档选择合适的解析器；</li><li>合理设置分块大小，平衡语义完整性和检索精度；</li><li>实现检索结果排序和过滤，提高相关性。</li></ul><h3>3.4 步骤4：构建复杂应用和部署</h3><p><strong>核心工作：</strong></p><ul><li><strong>组件组合</strong>：使用Chain编排复杂业务流程；</li><li><strong>集成外部系统</strong>：与企业现有系统对接；</li><li><strong>性能优化</strong>：缓存、异步处理等性能调优；</li><li><strong>监控与维护</strong>：实现日志、指标收集和告警。</li></ul><p><strong>实战建议：</strong></p><ul><li>采用模块化设计，便于测试和维护；</li><li>实现熔断和限流机制，防止服务过载；</li><li>建立完善的监控体系，及时发现和解决问题。</li></ul><h2>四、LangChain4j实战经验：避免4个常见陷阱</h2><p>在多年的大模型应用开发实践中，我总结了4个最容易踩的坑和对应的解决方法：</p><p><strong>陷阱1：API密钥安全问题</strong></p><ul><li><strong>表现</strong>：在代码中硬编码API密钥，导致密钥泄露风险；</li><li><strong>解决方法</strong>：使用环境变量、配置文件或密钥管理系统存储API密钥，并限制访问权限。</li></ul><pre><code class="java">// 推荐方式
String apiKey = System.getenv("OPENAI_API_KEY");</code></pre><p><strong>陷阱2：模型选择不当</strong></p><ul><li><strong>表现</strong>：盲目使用最高级模型，导致成本过高；或使用过低级模型，导致结果质量不佳；</li><li><strong>解决方法</strong>：根据任务复杂度和预算选择合适的模型，简单任务使用gpt-3.5-turbo，复杂推理使用gpt-4，企业部署考虑本地模型。</li></ul><p><strong>陷阱3：Token管理失控</strong></p><ul><li><strong>表现</strong>：未设置合理的token限制，导致生成内容过长或超出预算；</li><li><strong>解决方法</strong>：设置最大token限制和超时时间，实现成本控制。</li></ul><pre><code class="java">OpenAiChatModel model = OpenAiChatModel.builder()
            .apiKey(apiKey)
            .maxTokens(1000)  // 限制生成长度
            .timeout(Duration.ofSeconds(30))  // 设置超时
            .build();</code></pre><p><strong>陷阱4：用户体验忽视</strong></p><pre><code class="java">@GetMapping(value = "/chat/streaming", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux&lt;String&gt; streamingAssistant(
            @RequestParam(value = "msg", defaultValue = "What is the current time?") String message) {
        return streamingAssistant.chat(message);
    }</code></pre><ul><li><strong>表现</strong>：对于长响应采用同步等待方式，导致用户等待时间过长；</li><li><strong>解决方法</strong>：使用流式响应，实时输出内容，提升用户体验。</li></ul><h2>五、LangChain4j的行业趋势与未来展望</h2><p>作为Java生态中的大模型应用开发框架，LangChain4j正在引领Java开发者进入AI时代。根据行业观察和技术发展趋势，我们可以预见：</p><h3>5.1 企业级AI应用将进入爆发期</h3><p>根据Gartner预测，到2027年，80%的企业将部署至少一个基于大模型的核心业务应用。考虑到Java在企业应用中的主导地位（据统计，全球企业中约60%的后端系统基于Java开发），LangChain4j等Java大模型框架将成为企业AI转型的关键基础设施。</p><h3>5.2 Java开发者角色正在重塑</h3><p>在大模型时代，Java开发者的角色正在从传统的"代码编写者"转变为"AI编排者"。他们不再需要从零开始实现业务逻辑，而是通过组合和定制大模型能力来创造价值。LangChain4j正是为这种角色转变提供了有力工具。</p><h3>5.3 与Spring生态深度融合</h3><p>随着Spring AI的推出，LangChain4j与Spring生态的融合将更加紧密，虽然现在LangChain4j已经在开发Spring boot相关的Starter来加速Spring生态的融合了，但是使用起来还是需要一定的学习成本和使用难度的。未来，我们可能会看到更多专为企业级应用设计的特性，如声明式API、自动配置、与Spring Security集成等，进一步降低Java开发者使用大模型的门槛。</p><h2>六、总结与行动建议</h2><p>LangChain4j不是一个简单的Java版本的LangChain，而是为Java开发者和企业应用量身定制的大模型应用开发框架。它填补了Java生态在大模型应用开发领域的关键空白，为企业Java系统拥抱AI能力提供了一条平滑路径。</p><p><strong>给Java开发者的3个行动建议：</strong></p><ol><li><strong>立即开始学习</strong>：花时间掌握LangChain4j的核心概念和使用模式，将其视为职业发展的加分项；</li><li><strong>从小项目练手</strong>：选择一个内部工具或非关键业务场景，尝试用LangChain4j构建原型，积累实战经验；</li><li><strong>关注生态发展</strong>：密切关注LangChain4j社区动态和版本更新，参与开源贡献，成为技术浪潮的引领者而非跟随者。</li></ol><p><strong>记住LangChain4j的核心理念</strong>："让Java开发者能够以熟悉的方式构建企业级大模型应用"——这也是我们在AI时代保持竞争力的关键。</p><p>可参考的资源：</p><ul><li><a href="https://link.segmentfault.com/?enc=Qaxihfd24K5YGzJMowkFKg%3D%3D.AW3h4jeMs5d48gRKW6ytQocfVKQrPh2HHVmFVL9KkVA%3D" rel="nofollow" target="_blank">LangChain4j官方文档</a></li><li><a href="https://link.segmentfault.com/?enc=9rqyHmbpXGpN%2FRYu2e9h8A%3D%3D.3GRYt7e1fM6mwC%2BPad3A57PINCzYfF%2FR%2B3SrS5dTsQWzRbNhxNSL9ePVBL%2BKs2Vi" rel="nofollow" target="_blank">LangChain4j官方网站</a></li><li><a href="https://link.segmentfault.com/?enc=xuZMepnrmGkgZPzHHXgakA%3D%3D.%2BQkcd2HtDzh8iL0nxiIPpkpeABWXDySoZ2EevzPik4GfokX9MvZ18X88oars8CAMR1udZu1k8vr8gojCcdxRXA%3D%3D" rel="nofollow" target="_blank">LangChain4j示例项目</a></li></ul><hr/><p><strong>互动话题</strong>：你在使用LangChain4j开发过程中，遇到过哪些有趣的挑战或成功案例？或者你对Java生态与大模型的融合有什么独特的见解？欢迎在评论区与我分享你的故事和想法。</p><p><strong>关于作者</strong>：勇哥，10多年的开发和技术管理经验，从程序员做到企业技术高管。目前专注架构设计和人工智能应用实践，全网帐号统一名称"六边形架构"，有些不太合适发到公号的内容我会单独发到我的朋友圈，欢迎关注我，一起交流学习。</p><p><em>原创不易，如果觉得有帮助，请点赞、收藏、转发三连支持！</em></p>]]></description></item><item>    <title><![CDATA[进入职场第二课—融入 老李说技术 ]]></title>    <link>https://segmentfault.com/a/1190000047383732</link>    <guid>https://segmentfault.com/a/1190000047383732</guid>    <pubDate>2025-11-09 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>经过观察后要做的第二件事一定是融入，大白话来说，就是经过观察，结论是想要留下来，接下来要顺利融入，适应这家公司、这个团队，安全度过新手期。融入和观察一样，都属于入职后的第一阶段，从完全陌生到逐渐熟悉。《易经》乾卦中的“潜龙勿用”，其实说的就是这两件事相加——观察是完全接收输入之后判断，而融入则是在观察的基础上，开始主动输出。</p><p>经过新手期的观察，你已经对业务，团队，成员，以及现状，有了充分的认识，那么融入，就是需要我们根据这些信息，走的更顺，更稳。</p><p>通过以下 4 个关键动作，快速完成从“局外人”到“团队一份子”的蜕变。</p><h3>一、顺着领导的脾气干：先成为他“预期中”的靠谱下属</h3><p>你的直属领导，也就是你的老板，是你想要融入这个团队必须搞定的人。他不认可你，不看好你，可能连试用期都过不了，更别提融入这个团队了。</p><p>观察出来领导的脾气，顺着这个脾气干，就是你获得领导认可的关键。看看他喜欢的下属是怎么工作的，他们之间又是怎么配合的，很快你就会有答案，照着学，千万别犹豫。</p><p>我的前领导曾说：“我不要求你一开始多厉害，但得让我看到你在‘努力契合’——这种态度比能力更重要。” 比如他爱早会同步进度，就别总等周报才汇报；他重视数据支撑，汇报时就多带具体案例和结果；他习惯简洁沟通，就别用长篇大论消耗他的时间。</p><h3>​二、和关键同事建立信任：他们是你的“隐形导师”​</h3><p>无论你是职场老鸟，还是应届新人，入职之后，你的老板都会给你制定试用期计划。这个计划涉及的工作内容，你可能有经验，也可能什么都不懂——除非你的上一家公司是现在这家公司的竞争对手，业务做的够大、够接近，否则你都得有一个学习上手的过程。</p><p>这个过程一般包含两件事：看代码、看资料、学业务，问老人。这个“老人”指的是懂业务的同时，也是对你来说的关键同事——是你不能得罪，必须要建立信任的人。因为他告诉你多一点，你就会少走很多弯路，避开很多雷区；他告诉你少一点，你就惨了，自己费很大的劲不说，还会踩很多值钱的坑。</p><p>请他吃个饭拜拜码头，时不时买杯咖啡——伸手不打笑脸人。不需要刻意讨好，真诚 + 分寸感就够了：一句“前辈，这个问题我实在搞不懂，能耽误您 10 分钟吗？”配上认真记笔记的态度，事后反馈“按您说的做，果然少走了弯路”，就能快速拉近距离。</p><h3>三、完成本职工作：“好”永远比“快”重要</h3><p>所有老板都希望你做的又快又好，但这句话还有后半句：如果让老板在快和好里面只能选一个，所有老板都会选好，而不会选快。快一定是建立在好的基础上，如果做不好，达不到要求，交付之后频频出问题，天天被客户投诉，没有一个老板想要这样的结果。他们宁愿你慢慢做，做好了再交付。</p><p>对你来说，本职工作一定要做好，试用期计划里的内容也一定要实现。切记，是完成本职工作，而不是快速的完成。这个阶段快没有任何好处——职场想发展的好，走得稳，走得远，这一点必须学会。比谁做题做得快，比谁背课文背得快，比谁回答问题反应快，这都是还在上小学、初中的学生在比的东西，职场里没人关注这个。</p><h3>四、适应公司的已有气场：先“入乡随俗”，再谈改变*</h3><p>每个公司都有自己的气场，作为新人，你只能收起自己的锋芒。原因很简单，你还没有做出任何成绩。平时加不加班，考勤严不严格，流程繁不繁琐，氛围活不活跃，搞清楚这些之后，加入他们，照着做，千万别特立独行。</p><p>公司开会多，你就跟着开，听听大家都是什么套路；说事情总是发邮件，不直接说，你也跟着发，措辞照着写的好的学；办公区随处可见口号横幅、价值观标语，其他员工什么态度，你也什么态度；如果公司组织学习价值观的话，你就好好认真听。</p><h2>写在最后：潜龙勿用，是为了更好地“见龙在田”</h2><p>其实融入并不难，把上面这 4 个动作做个七七八八，大概率都不会有啥问题。融入之后，恭喜你，你也就顺利度过了新手期。</p><p>再提醒一下，“潜龙勿用”就是龙还潜在水下，还不到你施展才华的时候。别露头，攒着劲，别着急，找出新公司的规律，按这个规律办事，就是顺势而为。</p><p>记住：观察是“找出规律”，融入是“学会按规律办事”——二者叠加，就是《易经》里“潜龙勿用”的完整含义。</p>]]></description></item><item>    <title><![CDATA[LightRAG 实战： 基于 Olla]]></title>    <link>https://segmentfault.com/a/1190000047383682</link>    <guid>https://segmentfault.com/a/1190000047383682</guid>    <pubDate>2025-11-09 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>LightRAG 是个开源的 RAG 框架，专门用来快速搭建模块化的检索增强生成管道。这个项目在 GitHub 上热度不低，我们今天来看看他到底怎么用<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383684" alt="" title=""/></p><h2>基础安装与环境配置</h2><p>LightRAG 的安装过程很简单，几行命令就能搞定：</p><pre><code> pip install "lightrag-hku[api]"  
 cp env.example .env # ---&gt;这个有很多参数 非常丰富
 lightrag-server</code></pre><p>官方提供的 UI 界面做得还算不错，不过测试时基本没用上，因为更关注的是代码层面的实现。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383685" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383686" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383687" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383688" alt="" title="" loading="lazy"/><br/>环境搭好之后，可以先跑一下官方提供的示例代码(摘自 readme)：</p><pre><code> import os  
import asyncio  
from lightrag import LightRAG, QueryParam  
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed  
from lightrag.kg.shared_storage import initialize_pipeline_status  
from lightrag.utils import setup_logger  

setup_logger("lightrag", level="INFO")  

WORKING_DIR = "./rag_storage"  
if not os.path.exists(WORKING_DIR):  
    os.mkdir(WORKING_DIR)  

async def initialize_rag():  
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        embedding_func=openai_embed,  
        llm_model_func=gpt_4o_mini_complete,  
    )  
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    await initialize_pipeline_status()  # Initialize processing pipeline
    return rag  

async def main():  
    try:  
        # Initialize RAG instance
        rag = await initialize_rag()  
        await rag.ainsert("Your text")  

        # Perform hybrid search
        mode = "hybrid"  
        print(  
          await rag.aquery(  
              "What are the top themes in this story?",  
              param=QueryParam(mode=mode)  
          )  
        )  

    except Exception as e:  
        print(f"An error occurred: {e}")  
    finally:  
        if rag:  
            await rag.finalize_storages()  

if __name__ == "__main__":  
     asyncio.run(main())</code></pre><p>官方示例里还有个基于 Gemini 的版本，看着比较简单就选了这个来测试：</p><pre><code> # pip install -q -U google-genai to use gemini as a client  

import os  
import numpy as np  
from google import genai  
from google.genai import types  
from dotenv import load_dotenv  
from lightrag.utils import EmbeddingFunc  
from lightrag import LightRAG, QueryParam  
from sentence_transformers import SentenceTransformer  
from lightrag.kg.shared_storage import initialize_pipeline_status  

import asyncio  
import nest_asyncio  

# Apply nest_asyncio to solve event loop issues
nest_asyncio.apply()  

load_dotenv()  
gemini_api_key = os.getenv("GEMINI_API_KEY")  

WORKING_DIR = "./dickens"  

if os.path.exists(WORKING_DIR):  
    import shutil  

    shutil.rmtree(WORKING_DIR)  

os.mkdir(WORKING_DIR)  

async def llm_model_func(  
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs  
) -&gt; str:  
    # 1. Initialize the GenAI Client with your Gemini API Key
    client = genai.Client(api_key=gemini_api_key)  

    # 2. Combine prompts: system prompt, history, and user prompt
    if history_messages is None:  
        history_messages = []  

    combined_prompt = ""  
    if system_prompt:  
        combined_prompt += f"{system_prompt}\n"  

    for msg in history_messages:  
        # Each msg is expected to be a dict: {"role": "...", "content": "..."}
        combined_prompt += f"{msg['role']}: {msg['content']}\n"  

    # Finally, add the new user prompt
    combined_prompt += f"user: {prompt}"  

    # 3. Call the Gemini model
    response = client.models.generate_content(  
        model="gemini-1.5-flash",  
        contents=[combined_prompt],  
        config=types.GenerateContentConfig(max_output_tokens=500, temperature=0.1),  
    )  

    # 4. Return the response text
    return response.text  

async def embedding_func(texts: list[str]) -&gt; np.ndarray:  
    model = SentenceTransformer("all-MiniLM-L6-v2")  
    embeddings = model.encode(texts, convert_to_numpy=True)  
    return embeddings  

async def initialize_rag():  
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        llm_model_func=llm_model_func,  
        embedding_func=EmbeddingFunc(  
            embedding_dim=384,  
            max_token_size=8192,  
            func=embedding_func,  
        ),  
    )  

    await rag.initialize_storages()  
    await initialize_pipeline_status()  

    return rag  

def main():  
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())  
    file_path = "story.txt"  
    with open(file_path, "r") as file:  
        text = file.read()  

    rag.insert(text)  

    response = rag.query(  
        query="What is the main theme of the story?",  
        param=QueryParam(mode="hybrid", top_k=5, response_type="single line"),  
    )  

    print(response)  

if __name__ == "__main__":  
     main()</code></pre><pre><code>.env</code></pre><p>配置文件的参数非常丰富，需要根据实际使用的工具链做适配。为了调用本地模型，我这里用Ollama 做了相应调整。</p><pre><code> ### This is sample file of .env

###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='My Graph KB'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
# WORKERS=2
### gunicorn worker timeout(as default LLM request timeout if LLM_TIMEOUT is not set)
# TIMEOUT=150
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

### Directory Configuration (defaults to current working directory)
### Default value is ./inputs and ./rag_storage
# INPUT_DIR=&lt;absolute_path_for_doc_input_dir&gt;
# WORKING_DIR=&lt;absolute_path_for_working_dir&gt;

### Tiktoken cache directory (Store cached files in this folder for offline deployment)
# TIKTOKEN_CACHE_DIR=/app/data/tiktoken

### Ollama Emulating Model and Tag
# OLLAMA_EMULATING_MODEL_NAME=lightrag
OLLAMA_EMULATING_MODEL_TAG=latest

### Max nodes return from graph retrieval in webui
# MAX_GRAPH_NODES=1000

### Logging level
# LOG_LEVEL=INFO
# VERBOSE=False
# LOG_MAX_BYTES=10485760
# LOG_BACKUP_COUNT=5
### Logfile location (defaults to current working directory)
# LOG_DIR=/path/to/log/directory

#####################################
### Login and API-Key Configuration
#####################################
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

######################################################################################
### Query Configuration
###
### How to control the context length sent to LLM:
###    MAX_ENTITY_TOKENS + MAX_RELATION_TOKENS &lt; MAX_TOTAL_TOKENS
###    Chunk_Tokens = MAX_TOTAL_TOKENS - Actual_Entity_Tokens - Actual_Relation_Tokens
######################################################################################
# LLM response cache for query (Not valid for streaming response)
ENABLE_LLM_CACHE=true
# COSINE_THRESHOLD=0.2
### Number of entities or relations retrieved from KG
# TOP_K=40
### Maximum number or chunks for naive vector search
# CHUNK_TOP_K=20
### control the actual entities send to LLM
# MAX_ENTITY_TOKENS=6000
### control the actual relations send to LLM
# MAX_RELATION_TOKENS=8000
### control the maximum tokens send to LLM (include entities, relations and chunks)
# MAX_TOTAL_TOKENS=30000

### chunk selection strategies
###     VECTOR: Pick KG chunks by vector similarity, delivered chunks to the LLM aligning more closely with naive retrieval
###     WEIGHT: Pick KG chunks by entity and chunk weight, delivered more solely KG related chunks to the LLM
###     If reranking is enabled, the impact of chunk selection strategies will be diminished.
# KG_CHUNK_PICK_METHOD=VECTOR

#########################################################
### Reranking configuration
### RERANK_BINDING type:  null, cohere, jina, aliyun
### For rerank model deployed by vLLM use cohere binding
#########################################################
RERANK_BINDING=null
### Enable rerank by default in query params when RERANK_BINDING is not null
# RERANK_BY_DEFAULT=True
### rerank score chunk filter(set to 0.0 to keep all chunks, 0.6 or above if LLM is not strong enough)
# MIN_RERANK_SCORE=0.0

### For local deployment with vLLM
# RERANK_MODEL=BAAI/bge-reranker-v2-m3
# RERANK_BINDING_HOST=http://localhost:8000/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Cohere AI
# RERANK_MODEL=rerank-v3.5
# RERANK_BINDING_HOST=https://api.cohere.com/v2/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Jina AI
# RERANK_MODEL=jina-reranker-v2-base-multilingual
# RERANK_BINDING_HOST=https://api.jina.ai/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

### Default value for Aliyun
# RERANK_MODEL=gte-rerank-v2
# RERANK_BINDING_HOST=https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

########################################
### Document processing configuration
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### Document processing output language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=English

### Entity types that the LLM will attempt to recognize
# ENTITY_TYPES='["Person", "Creature", "Organization", "Location", "Event", "Concept", "Method", "Content", "Data", "Artifact", "NaturalObject"]'

### Chunk size for document splitting, 500~1500 is recommended
# CHUNK_SIZE=1200
# CHUNK_OVERLAP_SIZE=100

### Number of summary segments or tokens to trigger LLM summary on entity/relation merge (at least 3 is recommended)
# FORCE_LLM_SUMMARY_ON_MERGE=8
### Max description token size to trigger LLM summary
# SUMMARY_MAX_TOKENS = 1200
### Recommended LLM summary output length in tokens
# SUMMARY_LENGTH_RECOMMENDED_=600
### Maximum context size sent to LLM for description summary
# SUMMARY_CONTEXT_SIZE=12000

### control the maximum chunk_ids stored in vector and graph db
# MAX_SOURCE_IDS_PER_ENTITY=300
# MAX_SOURCE_IDS_PER_RELATION=300
### control chunk_ids limitation method: FIFO, KEEP
###    FIFO: First in first out
###    KEEP: Keep oldest (less merge action and faster)
# SOURCE_IDS_LIMIT_METHOD=FIFO

# Maximum number of file paths stored in entity/relation file_path field (For displayed only, does not affect query performance)
# MAX_FILE_PATHS=100

### maximum number of related chunks per source entity or relation
###     The chunk picker uses this value to determine the total number of chunks selected from KG(knowledge graph)
###     Higher values increase re-ranking time
# RELATED_CHUNK_NUMBER=5

###############################
### Concurrency Configuration
###############################
### Max concurrency requests of LLM (for both query and document processing)
MAX_ASYNC=4
### Number of parallel processing documents(between 2~10, MAX_ASYNC/3 is recommended)
MAX_PARALLEL_INSERT=2
### Max concurrency requests for Embedding
# EMBEDDING_FUNC_MAX_ASYNC=8
### Num of chunks send to Embedding in single request
# EMBEDDING_BATCH_NUM=10

###########################################################
### LLM Configuration
### LLM_BINDING type: openai, ollama, lollms, azure_openai, aws_bedrock
###########################################################
### LLM request timeout setting for all llm (0 means no timeout for Ollma)
# LLM_TIMEOUT=180

LLM_BINDING=ollama
LLM_MODEL=granite4:latest  
LLM_BINDING_HOST=http://localhost:11434
[#LLM](#LLM)_BINDING_API_KEY=your_api_key

### Optional for Azure
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# AZURE_OPENAI_DEPLOYMENT=gpt-4o

### Openrouter example
# LLM_MODEL=google/gemini-2.5-flash
# LLM_BINDING_HOST=https://openrouter.ai/api/v1
# LLM_BINDING_API_KEY=your_api_key
# LLM_BINDING=openai

### OpenAI Compatible API Specific Parameters
### Increased temperature values may mitigate infinite inference loops in certain LLM, such as Qwen3-30B.
# OPENAI_LLM_TEMPERATURE=0.9
### Set the max_tokens to mitigate endless output of some LLM (less than LLM_TIMEOUT * llm_output_tokens/second, i.e. 9000 = 180s * 50 tokens/s)
### Typically, max_tokens does not include prompt content, though some models, such as Gemini Models, are exceptions
### For vLLM/SGLang deployed models, or most of OpenAI compatible API provider
# OPENAI_LLM_MAX_TOKENS=9000
### For OpenAI o1-mini or newer modles
[#OPENAI](#OPENAI)_LLM_MAX_COMPLETION_TOKENS=9000

#### OpenAI's new API utilizes max_completion_tokens instead of max_tokens
# OPENAI_LLM_MAX_COMPLETION_TOKENS=9000

### use the following command to see all support options for OpenAI, azure_openai or OpenRouter
### lightrag-server --llm-binding openai --help
### OpenAI Specific Parameters
# OPENAI_LLM_REASONING_EFFORT=minimal
### OpenRouter Specific Parameters
# OPENAI_LLM_EXTRA_BODY='{"reasoning": {"enabled": false}}'
### Qwen3 Specific Parameters deploy by vLLM
# OPENAI_LLM_EXTRA_BODY='{"chat_template_kwargs": {"enable_thinking": false}}'

### use the following command to see all support options for Ollama LLM
### If LightRAG deployed in Docker uses host.docker.internal instead of localhost in LLM_BINDING_HOST
### lightrag-server --llm-binding ollama --help
### Ollama Server Specific Parameters
### OLLAMA_LLM_NUM_CTX must be provided, and should at least larger than MAX_TOTAL_TOKENS + 2000
OLLAMA_LLM_NUM_CTX=32768
### Set the max_output_tokens to mitigate endless output of some LLM (less than LLM_TIMEOUT * llm_output_tokens/second, i.e. 9000 = 180s * 50 tokens/s)
# OLLAMA_LLM_NUM_PREDICT=9000
### Stop sequences for Ollama LLM
# OLLAMA_LLM_STOP='["&lt;/s&gt;", "&lt;|EOT|&gt;"]'

### Bedrock Specific Parameters
# BEDROCK_LLM_TEMPERATURE=1.0

####################################################################################
### Embedding Configuration (Should not be changed after the first file processed)
### EMBEDDING_BINDING: ollama, openai, azure_openai, jina, lollms, aws_bedrock
####################################################################################
# EMBEDDING_TIMEOUT=30
EMBEDDING_BINDING=ollama
EMBEDDING_MODEL=granite-embedding:latest
EMBEDDING_DIM=1024
EMBEDDING_BINDING_API_KEY=your_api_key
# If LightRAG deployed in Docker uses host.docker.internal instead of localhost
EMBEDDING_BINDING_HOST=http://localhost:11434

### OpenAI compatible (VoyageAI embedding openai compatible)
# EMBEDDING_BINDING=openai
# EMBEDDING_MODEL=text-embedding-3-large
# EMBEDDING_DIM=3072
# EMBEDDING_BINDING_HOST=https://api.openai.com/v1
# EMBEDDING_BINDING_API_KEY=your_api_key

### Optional for Azure
# AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large
# AZURE_EMBEDDING_API_VERSION=2023-05-15
# AZURE_EMBEDDING_ENDPOINT=your_endpoint
# AZURE_EMBEDDING_API_KEY=your_api_key

### Jina AI Embedding
# EMBEDDING_BINDING=jina
# EMBEDDING_BINDING_HOST=https://api.jina.ai/v1/embeddings
# EMBEDDING_MODEL=jina-embeddings-v4
# EMBEDDING_DIM=2048
# EMBEDDING_BINDING_API_KEY=your_api_key

### Optional for Ollama embedding
OLLAMA_EMBEDDING_NUM_CTX=8192
### use the following command to see all support options for Ollama embedding
### lightrag-server --embedding-binding ollama --help

####################################################################
### WORKSPACE sets workspace name for all storage types
### for the purpose of isolating data from LightRAG instances.
### Valid workspace name constraints: a-z, A-Z, 0-9, and _
####################################################################
# WORKSPACE=space1

############################
### Data storage selection
############################
### Default storage (Recommended for small scale deployment)
# LIGHTRAG_KV_STORAGE=JsonKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
# LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

### Redis Storage (Recommended for production deployment)
# LIGHTRAG_KV_STORAGE=RedisKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=RedisDocStatusStorage

### Vector Storage (Recommended for production deployment)
# LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=FaissVectorDBStorage

### Graph Storage (Recommended for production deployment)
# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
# LIGHTRAG_GRAPH_STORAGE=MemgraphStorage

### PostgreSQL
# LIGHTRAG_KV_STORAGE=PGKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=PGGraphStorage
# LIGHTRAG_VECTOR_STORAGE=PGVectorStorage

### MongoDB (Vector storage only available on Atlas Cloud)
# LIGHTRAG_KV_STORAGE=MongoKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=MongoDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=MongoGraphStorage
# LIGHTRAG_VECTOR_STORAGE=MongoVectorDBStorage

### PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=your_username
POSTGRES_PASSWORD='your_password'
POSTGRES_DATABASE=your_database
POSTGRES_MAX_CONNECTIONS=12
# POSTGRES_WORKSPACE=forced_workspace_name

### PostgreSQL Vector Storage Configuration
### Vector storage type: HNSW, IVFFlat
POSTGRES_VECTOR_INDEX_TYPE=HNSW
POSTGRES_HNSW_M=16
POSTGRES_HNSW_EF=200
POSTGRES_IVFFLAT_LISTS=100

### PostgreSQL Connection Retry Configuration (Network Robustness)
### Number of retry attempts (1-10, default: 3)
### Initial retry backoff in seconds (0.1-5.0, default: 0.5)
### Maximum retry backoff in seconds (backoff-60.0, default: 5.0)
### Connection pool close timeout in seconds (1.0-30.0, default: 5.0)
# POSTGRES_CONNECTION_RETRIES=3
# POSTGRES_CONNECTION_RETRY_BACKOFF=0.5
# POSTGRES_CONNECTION_RETRY_BACKOFF_MAX=5.0
# POSTGRES_POOL_CLOSE_TIMEOUT=5.0

### PostgreSQL SSL Configuration (Optional)
# POSTGRES_SSL_MODE=require
# POSTGRES_SSL_CERT=/path/to/client-cert.pem
# POSTGRES_SSL_KEY=/path/to/client-key.pem
# POSTGRES_SSL_ROOT_CERT=/path/to/ca-cert.pem
# POSTGRES_SSL_CRL=/path/to/crl.pem

### PostgreSQL Server Settings (for Supabase Supavisor)
# Use this to pass extra options to the PostgreSQL connection string.
# For Supabase, you might need to set it like this:
# POSTGRES_SERVER_SETTINGS="options=reference%3D[project-ref]"

# Default is 100 set to 0 to disable
# POSTGRES_STATEMENT_CACHE_SIZE=100

### Neo4j Configuration
NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD='your_password'
NEO4J_DATABASE=neo4j
NEO4J_MAX_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=30
NEO4J_MAX_TRANSACTION_RETRY_TIME=30
NEO4J_MAX_CONNECTION_LIFETIME=300
NEO4J_LIVENESS_CHECK_TIMEOUT=30
NEO4J_KEEP_ALIVE=true
# NEO4J_WORKSPACE=forced_workspace_name

### MongoDB Configuration
MONGO_URI=mongodb://root:root@localhost:27017/
[#MONGO](#MONGO)_URI=mongodb+srv://xxxx
MONGO_DATABASE=LightRAG
# MONGODB_WORKSPACE=forced_workspace_name

### Milvus Configuration
MILVUS_URI=http://localhost:19530
MILVUS_DB_NAME=lightrag
# MILVUS_USER=root
# MILVUS_PASSWORD=your_password
# MILVUS_TOKEN=your_token
# MILVUS_WORKSPACE=forced_workspace_name

### Qdrant
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your-api-key
# QDRANT_WORKSPACE=forced_workspace_name

### Redis
REDIS_URI=redis://localhost:6379
REDIS_SOCKET_TIMEOUT=30
REDIS_CONNECT_TIMEOUT=10
REDIS_MAX_CONNECTIONS=100
REDIS_RETRY_ATTEMPTS=3
# REDIS_WORKSPACE=forced_workspace_name

### Memgraph Configuration
MEMGRAPH_URI=bolt://localhost:7687
MEMGRAPH_USERNAME=
MEMGRAPH_PASSWORD=
MEMGRAPH_DATABASE=memgraph
 # MEMGRAPH_WORKSPACE=forced_workspace_name</code></pre><p>参考前面的 Gemini 示例，写了下面的代码包含了一些硬编码文本的测试代码：</p><pre><code> # 准备环境
 python3 -m venv venv  
 source venv/bin/activate  
   
 pip install --upgrade pip  
 pip install "lightrag-hku[api]"  
 pip install ollama</code></pre><pre><code> import os  
import asyncio  
from functools import partial  
from datetime import datetime  
from lightrag import LightRAG, QueryParam  
try:  
    from ollama import AsyncClient  
except ImportError:  
    print("Warning: The 'ollama' Python package is required. Please run: pip install ollama")  
    class AsyncClient:   
        def __init__(self, host): pass  
        async def chat(self, **kwargs): raise NotImplementedError("ollama package not installed.")  

from lightrag.llm.ollama import ollama_embed   
from lightrag.utils import setup_logger, EmbeddingFunc  
from lightrag.kg.shared_storage import initialize_pipeline_status  

OLLAMA_BASE_URL = "http://localhost:11434"  
LLM_MODEL = "granite4:latest"  
EMBEDDING_MODEL = "granite-embedding:latest"  
WORKING_DIR = "./rag_storage_ollama"  
EMBEDDING_DIMENSION = 384   

OUTPUT_DIR = "./output"  

setup_logger("lightrag", level="INFO")  

if not os.path.exists(WORKING_DIR):  
    os.mkdir(WORKING_DIR)  

if not os.path.exists(OUTPUT_DIR):  
    os.mkdir(OUTPUT_DIR)  

async def custom_ollama_llm_complete(prompt: str, system_prompt: str = None, **kwargs):  
    """
    A custom function that handles the Ollama client initialization and model/base_url 
    parameters that are injected via functools.partial, while robustly filtering out 
    unwanted internal keywords.
    """
      
    model = kwargs.pop('model')  
    base_url = kwargs.pop('base_url')  
      
    client = AsyncClient(host=base_url)   
      
    messages = []  
    if system_prompt:  
        messages.append({"role": "system", "content": system_prompt})  
    messages.append({"role": "user", "content": prompt})  

    keys_to_filter = {  
        'host',   
        'hashing_kv',   
        'llm_model_name',   
        'history_messages',   
        'keyword_extraction',  
        'enable_cot',  
        'is_system_prompt_only',  
        'prompt_config'  
    }  
      
    cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in keys_to_filter}  

    try:  
        response = await client.chat(  
            model=model,   
            messages=messages,   
            **cleaned_kwargs  
        )  
        return response['message']['content']  
    except Exception as e:  
        raise e  

async def initialize_rag():  
    """Initializes the LightRAG instance using standard Ollama configuration."""

    configured_ollama_complete = partial(  
        custom_ollama_llm_complete,  
        model=LLM_MODEL,  
        base_url=OLLAMA_BASE_URL,  
    )  

    configured_ollama_embed = partial(  
        ollama_embed,  
        embed_model=EMBEDDING_MODEL,  
        base_url=OLLAMA_BASE_URL  
    )  

    wrapped_embedding_func = EmbeddingFunc(  
        embedding_dim=EMBEDDING_DIMENSION,   
        func=configured_ollama_embed,  
    )  
      
    rag = LightRAG(  
        working_dir=WORKING_DIR,  
        llm_model_func=configured_ollama_complete,  
        embedding_func=wrapped_embedding_func,  
    )  
      
    await rag.initialize_storages()  
    await initialize_pipeline_status()  
    return rag  

async def main():  
    rag = None   
    query = "How does RAG solve the problem of LLM hallucination and what are its main use cases?"  
      
    try:  
        print("Checking if required Ollama models are pulled...")  
          
        # the knowledge source
        sample_text = """  
        The concept of Retrieval-Augmented Generation (RAG) is a critical development  
        in the field of large language models (LLMs). It addresses the 'hallucination'  
        problem by grounding LLM responses in external, verified knowledge sources.  
        Instead of relying solely on the LLM's static training data, RAG first retrieves  
        relevant documents from a knowledge base (often a vector store) and then feeds  
        these documents, alongside the user's query, to the LLM for generation.  
        This two-step process significantly improves the accuracy, relevance, and  
        transparency of the generated output. Popular applications include enterprise  
        search, customer support, and domain-specific QA systems.  
        """  

        print(f"--- 1. Initializing RAG with Ollama Models ---")  
        rag = await initialize_rag()  
          
        print(f"\n--- 2. Inserting Sample Text ({len(sample_text.split())} words) ---")  
        await rag.ainsert(sample_text)  
        print("Insertion complete. Data is ready for retrieval.")  

        mode = "hybrid"   
          
        print(f"\n--- 3. Querying the RAG System (Mode: {mode}) ---")  
        print(f"Query: '{query}'")  

        rag_result = await rag.aquery(  
            query,  
            param=QueryParam(mode=mode)  
        )  
          
        response_text = None  
        if hasattr(rag_result, 'get_response_text'):  
            response_text = rag_result.get_response_text()  
        elif isinstance(rag_result, str):  
            response_text = rag_result  

        print("\n" + "="*50)  
        print("FINAL RAG RESPONSE")  
        print("="*50)  
          
        output_content = "" # Prepare string for file output
          
        if response_text and not str(response_text).strip().startswith("Error:"):  
            print(response_text)  
              
            output_content += f"# RAG Query Result\n\n"  
            output_content += f"## Query\n\n&gt; {query}\n\n"  
            output_content += f"## LLM/Cache Response\n\n{response_text}\n\n"  
              
            print("\n" + "="*50)  

            print("\n--- Context Retrieved (Sources) ---")  
            output_content += f"## Retrieved Context (Sources)\n\n"  
              
            if not isinstance(rag_result, str) and rag_result.retriever_output and rag_result.retriever_output.docs:  
                for i, doc in enumerate(rag_result.retriever_output.docs):  
                    source_text = doc.text  
                    print(f"Source {i+1}: {source_text[:100]}...")  
                    output_content += f"### Source {i+1}\n\n"  
                    output_content += f"```text\n{source_text}\n```\n"  
            else:  
                 print("No context documents were retrieved (or result was a cache hit string).")  
                 output_content += "No context documents were retrieved (or result was a cache hit string).\n"  
        else:  
             error_message = "LLM failed to generate a response (Check Ollama logs for details)."  
             print(error_message)  
             output_content += f"# RAG Query Result\n\n## Error\n\n{error_message}\n\n"  
               
             if response_text:  
                 print(f"\nError String from LightRAG: {response_text}")  
                 output_content += f"**Error Detail:** {response_text}\n"  

          
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  
        filename = f"rag_query_output_{timestamp}.md"  
        output_filepath = os.path.join(OUTPUT_DIR, filename)  
          
        with open(output_filepath, 'w', encoding='utf-8') as f:  
            f.write(output_content)  
          
        print(f"\n--- Output Written to File ---")  
        print(f"Successfully wrote output to: {output_filepath}")  

          
    except Exception as e:  
        if "'str' object has no attribute 'retriever_output'" in str(e):  
            print("\n--- ERROR BYPASS: Detected Cache Hit String Result ---")  
             print("The response was successfully retrieved from the cache and written to the output file.")  
        else:  
            # For all other (real) exceptions, print the detailed error block
            print("\n" + "="*50)  
            print("AN ERROR OCCURRED DURING RAG PROCESS")  
            print("="*50)  
            print(f"Error: {e}")  
            print(f"Please ensure Ollama is running and accessible at {OLLAMA_BASE_URL}, and the models '{LLM_MODEL}' and '{EMBEDDING_MODEL}' are pulled locally.")  
            print(f"To pull: 'ollama pull {LLM_MODEL}' and 'ollama pull {EMBEDDING_MODEL}'")  
            print("="*50 + "\n")  
          
    finally:  
        if rag:  
            print("\n--- Finalizing storages ---")  
            await rag.finalize_storages()  

if __name__ == "__main__":  
     asyncio.run(main())</code></pre><p>虽然</p><pre><code>.env</code></pre><p>文件里有参数可以配置 input 文件夹路径，但测试时直接在代码里写死了路径。</p><p>运行后的输出包括控制台日志和 markdown 格式的结果文件，结果太长我就不贴了。</p><p>接下来测试了更实际的场景：准备了几份 markdown 格式的文档（其他格式应该也支持，但没测），用这些文档构建了自己的 RAG 系统，继续用 Ollama 和 Granite 模型来验证效果，这次的代码就没那么硬编码了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383689" alt="" title="" loading="lazy"/></p><blockquote><pre><code>.env</code></pre><p>文件提供了 input 文件夹的配置选项，不过这里还是用的硬编码方式</p></blockquote><pre><code> import os
import asyncio
from functools import partial
from datetime import datetime
from lightrag import LightRAG, QueryParam
import glob 

try:
    from ollama import AsyncClient
except ImportError:
    print("Warning: The 'ollama' Python package is required. Please run: pip install ollama")
    class AsyncClient: 
        def __init__(self, host): pass
        async def chat(self, **kwargs): raise NotImplementedError("ollama package not installed.")

from lightrag.llm.ollama import ollama_embed 
from lightrag.utils import setup_logger, EmbeddingFunc
from lightrag.kg.shared_storage import initialize_pipeline_status

OLLAMA_BASE_URL = "http://localhost:11434"
LLM_MODEL = "granite4:latest"
EMBEDDING_MODEL = "granite-embedding:latest"
WORKING_DIR = "./rag_storage_ollama"
EMBEDDING_DIMENSION = 384 

DOCUMENTS_DIR = "./documents" # Directory to read source files from
OUTPUT_DIR = "./output" # Directory to write RAG results to

setup_logger("lightrag", level="INFO")

if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)
    print(f"Created working directory: {WORKING_DIR}")

if not os.path.exists(OUTPUT_DIR):
    os.mkdir(OUTPUT_DIR)
    print(f"Created output directory: {OUTPUT_DIR}")

if not os.path.exists(DOCUMENTS_DIR):
    os.mkdir(DOCUMENTS_DIR)
    print(f"Created documents directory: {DOCUMENTS_DIR}")

async def custom_ollama_llm_complete(prompt: str, system_prompt: str = None, **kwargs):
    """
    A custom function that handles the Ollama client initialization and model/base_url 
    parameters that are injected via functools.partial, while robustly filtering out 
    unwanted internal keywords.
    """
    
    model = kwargs.pop('model')
    base_url = kwargs.pop('base_url')
    
    client = AsyncClient(host=base_url) 
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})

    keys_to_filter = {
        'host', 
        'hashing_kv', 
        'llm_model_name', 
        'history_messages', 
        'keyword_extraction',
        'enable_cot',
        'is_system_prompt_only',
        'prompt_config'
    }
    
    cleaned_kwargs = {k: v for k, v in kwargs.items() if k not in keys_to_filter}

    try:
        response = await client.chat(
            model=model, 
            messages=messages, 
            **cleaned_kwargs
        )
        return response['message']['content']
    except Exception as e:
        raise e

async def initialize_rag():
    """Initializes the LightRAG instance using standard Ollama configuration."""

    configured_ollama_complete = partial(
        custom_ollama_llm_complete,
        model=LLM_MODEL,
        base_url=OLLAMA_BASE_URL,
    )

    configured_ollama_embed = partial(
        ollama_embed,
        embed_model=EMBEDDING_MODEL,
        base_url=OLLAMA_BASE_URL
    )

    wrapped_embedding_func = EmbeddingFunc(
        embedding_dim=EMBEDDING_DIMENSION, 
        func=configured_ollama_embed,
    )
    
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=configured_ollama_complete,
        embedding_func=wrapped_embedding_func,
    )
    
    await rag.initialize_storages()
    await initialize_pipeline_status()
    return rag

async def load_and_insert_documents(rag: LightRAG):
    """
    Reads files from the DOCUMENTS_DIR and inserts their content into the RAG system.
    Fixed to use a more compatible method for document insertion.
    """
    file_paths = glob.glob(os.path.join(DOCUMENTS_DIR, '*.[mM][dD]')) + \
                 glob.glob(os.path.join(DOCUMENTS_DIR, '*.[tT][xX][tT]'))
                 
    if not file_paths:
        print("\n--- WARNING: No documents found in './documents' directory. ---")
        print("Please add some Markdown (.md) or Text (.txt) files to populate the knowledge base.")
        return False
        
    print(f"\n--- 2. Inserting Documents ({len(file_paths)} file(s) found) ---")
    
    insertion_succeeded = 0
    
    for file_path in file_paths:
        filename = os.path.basename(file_path)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            await rag.ainsert(content, doc_meta={'doc_id': filename})
            
            print(f"  &gt; Successfully inserted: {filename} ({len(content.split())} words)")
            insertion_succeeded += 1
            
        except TypeError as te:
            if "'doc_id'" in str(te) or "'doc_meta'" in str(te):
                print(f"  &gt; FAILED (Type Error): {filename}. Attempting insertion without metadata to check compatibility.")
                try:
                    await rag.ainsert(content)
                    print(f"  &gt; Successfully inserted (no metadata): {filename}")
                    insertion_succeeded += 1
                except Exception as e:
                    print(f"  &gt; FAILED (General Error): {filename} - {e}")
            else:
                 print(f"  &gt; FAILED to read or insert {filename} (Type Error): {te}")
                 
        except Exception as e:
            print(f"  &gt; FAILED to read or insert {filename} (General Error): {e}")
            
    if insertion_succeeded == 0:
        print("Insertion complete, but no documents were successfully inserted. Please check LightRAG documentation for the correct argument name for source IDs.")
        return False
        
    print("Insertion complete. Data is ready for retrieval.")
    return True

async def main():
    rag = None 
    query = "Describe Quantum-Safe cryptography?"
    
    try:
        print("Checking if required Ollama models are pulled...")
        
        print(f"--- 1. Initializing RAG with Ollama Models ---")
        rag = await initialize_rag()
        
        documents_inserted = await load_and_insert_documents(rag)
        
        if not documents_inserted:
            return 

        mode = "hybrid" 
        
        print(f"\n--- 3. Querying the RAG System (Mode: {mode}) ---")
        print(f"Query: '{query}'")

        rag_result = await rag.aquery(
            query,
            param=QueryParam(mode=mode)
        )
        
        response_text = None
        if hasattr(rag_result, 'get_response_text'):
            response_text = rag_result.get_response_text()
        elif isinstance(rag_result, str):
            response_text = rag_result

        print("\n" + "="*50)
        print("FINAL RAG RESPONSE")
        print("="*50)
        
        output_content = "" # Prepare string for file output
        
        if response_text and not str(response_text).strip().startswith("Error:"):
            print(response_text)
            
            output_content += f"# RAG Query Result\n\n"
            output_content += f"## Query\n\n&gt; {query}\n\n"
            output_content += f"## LLM/Cache Response\n\n{response_text}\n\n"
            
            print("\n" + "="*50)

            print("\n--- Context Retrieved (Sources) ---")
            output_content += f"## Retrieved Context (Sources)\n\n"
            
            if not isinstance(rag_result, str) and rag_result.retriever_output and rag_result.retriever_output.docs:
                unique_sources = set()
                
                for i, doc in enumerate(rag_result.retriever_output.docs):
                    source_text = doc.text
                    source_id = doc.doc_id if hasattr(doc, 'doc_id') and doc.doc_id else (
                                doc.doc_meta.get('doc_id') if hasattr(doc, 'doc_meta') and isinstance(doc.doc_meta, dict) else 'Unknown Source'
                            )
                    unique_sources.add(source_id)
                    
                    print(f"Source {i+1} (File: {source_id}): {source_text[:100]}...")
                    output_content += f"### Source {i+1} (File: `{source_id}`)\n\n"
                    output_content += f"```text\n{source_text}\n```\n"
                
                print(f"\nAnswer Grounded in: {', '.join(sorted(list(unique_sources)))}")
            else:
                 print("No context documents were retrieved (or result was a cache hit string).")
                 output_content += "No context documents were retrieved (or result was a cache hit string).\n"
        else:
             error_message = "LLM failed to generate a response (Check Ollama logs for details)."
             print(error_message)
             output_content += f"# RAG Query Result\n\n## Error\n\n{error_message}\n\n"
             
             if response_text:
                 print(f"\nError String from LightRAG: {response_text}")
                 output_content += f"**Error Detail:** {response_text}\n"

        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_query_output_{timestamp}.md"
        output_filepath = os.path.join(OUTPUT_DIR, filename)
        
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(output_content)
        
        print(f"\n--- Output Written to File ---")
        print(f"Successfully wrote output to: {output_filepath}")
        

    except Exception as e:
        if "'str' object has no attribute 'retriever_output'" in str(e):
             print("\n--- ERROR BYPASS: Detected Cache Hit String Result ---")
             print("The response was successfully retrieved from the cache and written to the output file.")
        else:
            print("\n" + "="*50)
            print("AN ERROR OCCURRED DURING RAG PROCESS")
            print("="*50)
            print(f"Error: {e}")
            print(f"Please ensure Ollama is running and accessible at {OLLAMA_BASE_URL}, and the models '{LLM_MODEL}' and '{EMBEDDING_MODEL}' are pulled locally.")
            print(f"To pull: 'ollama pull {LLM_MODEL}' and 'ollama pull {EMBEDDING_MODEL}'")
            print("="*50 + "\n")
        
    finally:
        if rag:
            print("\n--- Finalizing storages ---")
            await rag.finalize_storages()

if __name__ == "__main__":
     asyncio.run(main())</code></pre><p>实际的查询输出示例：</p><pre><code>  RAG Query Result

## Query

&gt; Describe Quantum-Safe cryptography?

## LLM/Cache Response

### What is Quantum-Safe Cryptography?

 Quantum-safe cryptography, also known as post‑quantum cryptography (PQC), refers to cryptographic algorithms and protocols designed to remain secure even against attacks by a sufficiently powerful quantum computer. Traditional public-key cryptosystems like RSA, ECC, Diffie‑Hellman, and elliptic curve variants are vulnerable to Shor’s algorithm, which could efficiently factor large integers and compute discrete logarithms—tasks that form the basis of these cryptographic schemes.</code></pre><h2>知识图谱生成</h2><p>LightRAG 自带知识图谱生成功能，这点比较实用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047383690" alt="" title="" loading="lazy"/><br/>为了把生成的图谱文件展示出来，我还用用 Streamlit 写了个简单的可视化应用。代码比较粗糙，后续可以继续优化。</p><pre><code>import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
import io
import itertools
from io import StringIO

def visualize_graph(G: nx.Graph, layout_name: str, layout_params: dict):
    """
    Generates a Matplotlib plot of the NetworkX graph with custom styling.
    """

    node_labels = nx.get_node_attributes(G, 'label')
    if not node_labels:
        node_labels = nx.get_node_attributes(G, 'text')

    edge_labels = nx.get_edge_attributes(G, 'text')
    node_types = nx.get_node_attributes(G, 'type')

    type_color_map = {
        'Entity': '[#1f78b4](#1f78b4)',    # Blue
        'Chunk': '[#b2df8a](#b2df8a)',     # Light Green
        'Relation': '[#33a02c](#33a02c)',  # Dark Green 
        'Unknown': '[#a6cee3](#a6cee3)'    # Light Blue
    }
    node_colors = [type_color_map.get(node_types.get(node, 'Unknown'), type_color_map['Unknown']) for node in G.nodes()]

    if layout_name == 'Spring Layout':
        pos = nx.spring_layout(G, **layout_params)
    elif layout_name == 'Circular Layout':
        pos = nx.circular_layout(G)
    elif layout_name == 'Spectral Layout':
        pos = nx.spectral_layout(G)
    elif layout_name == 'Kamada-Kawai Layout':
        pos = nx.kamada_kawai_layout(G)
    else:
        pos = nx.spring_layout(G, **layout_params)

    fig, ax = plt.subplots(figsize=(16, 10))

    nx.draw_networkx_nodes(
        G, 
        pos, 
        node_size=2500, 
        node_color=node_colors, 
        alpha=0.9
    )

    # Draw edges
    nx.draw_networkx_edges(
        G, 
        pos, 
        ax=ax,
        edge_color='gray', 
        style='dashed', 
        arrowstyle='-&gt;', 
        arrowsize=25
    )

    nx.draw_networkx_labels(
        G, 
        pos, 
        labels=node_labels, 
        font_size=11, 
        font_color='black',
        font_weight='bold',
    )

    nx.draw_networkx_edge_labels(
        G, 
        pos, 
        edge_labels=edge_labels, 
        font_color='red', 
        font_size=9,
        bbox={"boxstyle": "round,pad=0.4", "fc": "white", "alpha": 0.7, "ec": "none"}
    )

    ax.set_title(f"Visualized Graph: {G.number_of_nodes()} Nodes, {G.number_of_edges()} Edges", fontsize=16)
    plt.axis('off')
    plt.tight_layout()

    st.pyplot(fig)

def app():
    st.set_page_config(layout="wide", page_title="GraphML Viewer")
    st.title("GraphML Visualization App")
    st.markdown("A tool to visualize GraphML (e.g., LightRAG) outputs using NetworkX and Streamlit.")

    st.sidebar.header("Data Upload &amp; Layout Controls")

    uploaded_file = st.sidebar.file_uploader(
        "Upload your .graphml file", 
        type=["graphml"]
    )

    graph_data = None

    if uploaded_file is not None:
        try:
            graph_data = uploaded_file.read().decode("utf-8")
            st.sidebar.success("File uploaded successfully! Graph loading...")
        except Exception as e:
            st.sidebar.error(f"Error reading file: {e}")
            graph_data = None
    else:
        st.info("Please upload a GraphML file in the sidebar to visualize your knowledge graph.")

    st.sidebar.subheader("Layout Algorithm")
    layout_name = st.sidebar.selectbox(
        "Choose Graph Layout:",
        ('Spring Layout', 'Kamada-Kawai Layout', 'Circular Layout', 'Spectral Layout')
    )

    layout_params = {}
    if layout_name == 'Spring Layout':
        st.sidebar.caption("Fine-tune the Spring Layout forces:")
        k_val = st.sidebar.slider("k (Node Spacing)", 0.01, 1.0, 0.15, 0.01)
        iters = st.sidebar.slider("Iterations", 10, 100, 50, 10)
        layout_params = {'k': k_val, 'iterations': iters}

    if graph_data:
        try:
            G = nx.read_graphml(StringIO(graph_data))

            st.header("Knowledge Graph Visualization")
            st.write(f"Graph loaded: {G.number_of_nodes()} Nodes, {G.number_of_edges()} Edges")

            visualize_graph(G, layout_name, layout_params)

        except Exception as e:
            st.error(f"An error occurred while processing the graph: {e}")
            st.code(f"Error details: {e}")
            st.warning("Please check if the GraphML file is correctly formatted and contains valid data.")

if __name__ == '__main__':
    app()</code></pre><p>运行以后你就能看到上面的结果</p><h2>总结</h2><p>LightRAG 在构建 RAG 系统方面提供了相对完整的解决方案。相比传统的纯向量检索，它的核心特点是引入了知识图谱，能把非结构化文本组织成实体-关系网络，这种混合检索策略（语义向量+图谱关系）确实能让 LLM 获得更丰富的上下文信息。</p><p>项目文档写得很详细，并且包含了大量工具和服务的集成方案。整个框架的设计比较模块化，扩展性也不错。如果要在生产环境部署，还需要考虑性能优化、错误处理等细节。</p><p>LightRAG 项目地址：<a href="https://link.segmentfault.com/?enc=LVbbyGCyf1vsyynTGvvPKw%3D%3D.QzWBik7pLJ%2FuuDwzwzTYclZUutZiYLWJVwG5lkaNmDOkWZo2%2BsgzMXPf9jnWP5dC" rel="nofollow" target="_blank">https://github.com/HKUDS/LightRAG</a></p><p>作者：Alain Airom</p>]]></description></item><item>    <title><![CDATA[Monad Blitz 黑客松巡回之旅 ]]></title>    <link>https://segmentfault.com/a/1190000047383602</link>    <guid>https://segmentfault.com/a/1190000047383602</guid>    <pubDate>2025-11-09 21:05:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>Monad Blitz 是一场紧张刺激、充满乐趣、为期 1 天的线下黑客马拉松</strong>。Monad Blitz 已经在<strong>深圳、杭州、北京和成都</strong>成功举办，每一站都吸引了大量开发者参与，现场气氛热烈，作品精彩纷呈。活动不仅展示了 Monad 并行 EVM 的技术潜力，也收获了开发者们的热烈响应与积极反馈，逐渐成为 Monad 生态 Builder 们最受瞩目的创意舞台。</p><p>Monad Blitz 的旅程不会停下，精彩还在继续！<strong>上海站</strong>、<strong>大理站和长沙站</strong>也重磅来袭！</p><p>无论你身处哪里，都有机会加入这场并行 EVM 的创新盛宴，和一群热血 Builder 面对面碰撞灵感、快速动手、结识伙伴，一起体验 Monad 的无限可能！</p><p><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdmYNN" alt="image.png" title="image.png"/></p><p>如果你还没有 Web3 或链上开发经验，不用担心！Monad Blitz 非常适合想要尝试新技术、拓展技能边界的 Web2 开发者。</p><p>在这里，你可以：</p><ul><li>通过了解当下最火热、最具潜力的公链之一 —— <strong>Monad</strong>，加深对整个行业的认知，获得前沿视角</li><li>亲手体验智能合约和链上应用的开发流程，快速入门并理解核心概念</li><li>认识一群对技术同样充满热情的 Web3 Builder，获得一线实战指导</li><li>通过「边做边学」的方式，打破对区块链复杂性的认知门槛，现场氛围轻松、互帮互学</li><li>为自己的技术履历增加一个亮眼的链上经历，打开全新职业发展方向</li></ul><p>不管你是做前端、后端还是全栈，只要对 Web3 感兴趣，都可以来尝试！欢迎所有 Web2 开发者加入，一起快速上手，一起玩，一起成长 🚀</p><h3>Monad Blitz 上海站 · 11月29日</h3><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNM" alt="image.png" title="image.png" loading="lazy"/><br/><strong>Monad × X402</strong><br/>我们鼓励开发者积极探索与 X402 协议相结合的创新方向，利用 Monad 的高性能并行处理能力，在支付与金融应用领域开拓新的想象空间。这将是您项目的一个独特亮点！</p><h3><strong>奖金（1500 美元）</strong></h3><p><strong>🥇 第 1 名：</strong> 600 美元</p><p><strong>🥈 第 2 名：</strong> 500 美元</p><p><strong>🥉 第 3 名：</strong> 400 美元</p><p><strong>前5名每个荣获专属定制 Monad 机械键盘一把！</strong></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmCDU" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>投票采用官方和开发者共同投票形式，Monad Foundation 对规则有最终解释权。</strong></p><p>为了确保大家的体验，我们将进行一定的筛选，请认真填写申请信息。支持线上提前或线下现场自由组队，每支队伍最多 3 名成员。</p><h3>立即报名</h3><p>日期：2025年11月29日</p><p>时间：9:00 - 21:00</p><p>地点：上海（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=kU5NQgWB%2B4WtffDpq2vlkA%3D%3D.BwzdXNKTpOUZ8Y0lnTK3sNMab37HArxtRngN57KzA%2BQ%3D" rel="nofollow" target="_blank">https://luma.com/7zug8z13</a></p><h3>下一波活动已在路上……</h3><h3>📍 Monad Blitz 大理站预告 · 12月20日</h3><p>在苍山洱海畔，我们将延续「快速学习+实战冲刺+深度交流」的核心氛围。结合更多本地及全国的开发者资源，打造更丰富、更具影响力的链上聚会。不论你是想深入研究并行EVM，还是想结识志同道合的Builder，一起组队、一起秀操作，这里都是你的绝佳选择！</p><h3>立即报名</h3><p>日期：2025年12月20日</p><p>时间：9:00 - 21:00</p><p>地点：大理（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=5E2z0l%2BcREw9oKAX0sccEQ%3D%3D.rMIb5kkwxJbjW0Go1uDLh2UCPxoD29HST9nGbin0US0%3D" rel="nofollow" target="_blank">https://luma.com/j7862g95</a></p><h3>📍 Monad Blitz 长沙站预告· 2026年1月17日</h3><p>长沙，这座活力新星城市，正呼唤着下一个明星项目的诞生！本站黑客松将深度聚焦于团队碰撞与协作。无论你是智能合约老手还是前端新锐，都能在这里找到并肩作战的伙伴。让我们在湘江畔，将天马行空的创意转化为可运行的代码，携手打造出属于下一个时代的链上应用。</p><h3>立即报名</h3><p>日期：2026年1月17日</p><p>时间：9:00 - 21:00</p><p>地点：长沙（报名以查看详细地址）</p><p>报名：<a href="https://link.segmentfault.com/?enc=3VU79cLUS404vESoG50j1A%3D%3D.8GomB7XHS%2FAy%2BG2kv4Dgc4Y74RueZ2FI8qqvosX8QkA%3D" rel="nofollow" target="_blank">https://luma.com/b7zggun9</a></p><h3>活动内容</h3><p><strong>体验并行的力量</strong> 在上午的分享环节，跟 Monad DevRel 面对面，现场学习最前沿的并行执行技术，动手把你的第一个合约部署到 Monad Testnet，真正感受高性能链的速度与魅力。</p><p><strong>下午直接开搞，快速上手</strong> 下午就是一场高强度「头脑风暴 + 编码实战」突击赛。无论是做出一个小原型，还是验证一个新想法，这里更看重动手、试错和创意，边学边做，超有成就感。</p><p><strong>秀出你的实力</strong> 我们正在寻找敢想敢做、充满活力的开发者！用你的代码实力、快速学习力和热情给大家留下印象，这就是你闪光的机会。</p><p><strong>找到你的「同路人」</strong> 和一群同样热爱创新、热爱链上世界的 Builder 面对面交流，聊技术、聊创意、一起组队搞项目。现场管吃管喝，氛围轻松有趣，绝对会收获一堆新朋友！</p><h3>活动议程</h3><p>09:00 - 09:30｜签到 &amp; 早餐</p><p>09:30 - 10:15｜Monad 101</p><p>10:15 - 11:00｜Workshop</p><p>11:00 - 12:00｜午餐 &amp; 组队</p><p>12:00 - 18:30｜Hack/Vibe-coding</p><p>18:30 - 19:00｜项目提交截止 &amp; 晚餐</p><p>19:00 - 20:00｜项目展示 / Demos</p><p>20:00 - 20:20｜自由交流 &amp; 投票</p><p>20:20 - 20:45｜颁奖 &amp; 总结致辞</p><p>20:45 - 21:00｜自由交流 &amp; 活动结束</p><h3>现场好礼</h3><p>本次活动为大家特别准备了专属定制的 Monad 和 OpenBuild 周边好礼！从实用的 T 恤到可爱的公仔，更有时尚手链和新颖贴纸等你带回家！</p><p><img width="723" height="291" referrerpolicy="no-referrer" src="/img/bVdjGSf" alt="image.png" title="image.png" loading="lazy"/></p><h3>关于 Monad</h3><p>Monad 是一个去中心化、以开发者为中心的 Layer 1 区块链，通过并行执行和超标量管道技术引入了新的可能性。它旨在解决现有区块链的可扩展性问题，能够实现每秒高达 10,000 笔交易，同时保持与 EVM 的兼容性。 Monad 提供了完整的开发工具链，降低了开发者的技术门槛，为其生态系统的蓬勃发展奠定了坚实基础。</p>]]></description></item><item>    <title><![CDATA[Web3 开发者周刊 75 | ESP ]]></title>    <link>https://segmentfault.com/a/1190000047383613</link>    <guid>https://segmentfault.com/a/1190000047383613</guid>    <pubDate>2025-11-09 21:04:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNX" alt="image.png" title="image.png"/></p><p><strong>欢迎回到 Web3 开发者周刊第 75 期！</strong></p><p>本期周刊内所有黑客松活动、新闻和赏金任务，请大家点击查看原文以获取完整信息。如果您喜欢我们的内容，也欢迎大家订阅 <strong>OpenBuild Substack</strong>，获取最新最全开发者资讯！</p><p>本周我们将聚焦重启后的 ESP 新资助计划，新资助计划涵盖了密码学、隐私保护、应用层、安全、社区发展等关键领域。从隐私、价格飙升和未来走向几个方面探讨 Zcash 的复兴，以及关注 Mantle zk 化升级后正式进入 RWA 战略发力期。</p><p>除此之外，我们还为开发者整理了最新的黑客松资讯与赏金任务。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmYMt" alt="image.png" title="image.png" loading="lazy"/><br/><strong>✅ Monad Blitz @Shanghai</strong></p><p>📅 时间：11月29日</p><p>📍 上海</p><p>💸 奖金：1500 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=F9TKRe0kLbvvGoCszGrAsw%3D%3D.2yLmhxYN5jcBZX5AfAAESjHllkh%2FVnUduo6lx9gjRno%3D" rel="nofollow" target="_blank">https://luma.com/7zug8z13</a></p><p><strong>简介：</strong></p><p>Monad Blitz 的旅程即将登陆上海！挑战一日冲刺，探索 Monad x X402 新前沿！</p><p>Monad 鼓励开发者积极探索与 X402 协议相结合的创新方向，利用 Monad 的高性能并行处理能力，在支付与金融应用领域开拓新的想象空间。这将是您项目的一个独特亮点！</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNM" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Calling For All Agents! Hackathon</strong></p><p>📅 时间：10月27日 - 11月29日</p><p>📍 线上</p><p>💸 奖金：50,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=u545SIMd0xfsPhHUp%2BfLbA%3D%3D.4DjdUcEdb1OO4PkCk33RRem51jqicGhpYPVl7VI93GJTnIiV1utw9V0A32hTtOzgV1ag0RtmjMhYYtVoJGoaog%3D%3D" rel="nofollow" target="_blank">https://dorahacks.io/hackathon/calling-for-all-agents-sf/detail</a></p><p><strong>简介：</strong></p><p>Calling For All Agents! Hackathon 是 Verisense 发起的 AI 代理黑客松，旨在构建自主人工智能代理，突破大型语言模型的极限，并利用 MCPs** 和 RAGs 等关键工具，迈向智能的新高度。没有规则，没有界限！只有智能代理和创造力，让大胆的想法付诸实践！</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYNY" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Mantle Global Hackathon 2025</strong></p><p>📅 时间：2025年10月22日 - 2026年2月7日</p><p>📍 线上</p><p>💸 奖金：150,000 美元 </p><p>🔗 链接：  </p><p><a href="https://link.segmentfault.com/?enc=qvhBI08j%2Fv3R9NME0DAGeA%3D%3D.dtXvD%2BzYLv2wZoW%2BV3Jag2gdFC%2FJTT3i5xzRzcgJbYpQ7ytzj5AVOkA%2F7y62hWLoMgd1U5JMnWcQtTn76trqMn7SxYjMeIz0JGTYb8AKBZg%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a>  </p><p><strong>简介：</strong></p><p>Mantle Global Hackathon 2025 是 Mantle 完成 zk 升级后首次尝试。通过此次全球黑客松活动，Mantle 希望激发全球开发者社区的潜力，共同推进 Mantle 生态的全面落地。了解更多：<a href="https://link.segmentfault.com/?enc=B758c2Wh2xZzCR2chKCs4Q%3D%3D.0jiDamudY0BYNczRL8tSKj%2B11ZXteqM3ndTf%2B2QWcaBNIvTzNHtmuHhWb5Pj1E1k8hfJPGmGVMM7UOmTQFcV2ihp5Fr9iYNDEeSDfMFBA6rej9nKiDXfUsFbRZFgvsjvn6OS2hmBONaGJaCmBZFI7%2Biti6aj1XEAKbC17qXjrHgIMtDpgTCI0RFMbV5DALL0" rel="nofollow" target="_blank">全球开发者集结！Mantle 全球黑客松燃动开启！共创 Web3 生态新高度！</a><br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMu" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="162" referrerpolicy="no-referrer" src="/img/bVdmYMx" alt="image.png" title="image.png" loading="lazy"/><br/><strong>📖 ESP 的新资助计划公布 11/3</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=btEq2ervN%2Fn4esQJTsJBeQ%3D%3D.SRb5lZgD3QBTNI3nvIo0%2FFKSTJjXCCrT%2FMBNmmLmuKtAgpmW%2BaRtllVfzjsN7zeS9XW87oYlDpAS57jEdrUIIg%3D%3D" rel="nofollow" target="_blank">https://blog.ethereum.org/2025/11/03/new-esp-grants</a></p><p>ESP 的首批愿望清单和征案邀请现已上线！涵盖了密码学、隐私保护、应用层、安全、社区发展等关键领域，为开发者提供了众多创造影响力的机会。</p><p><strong>📖 Zcash 的复兴：隐私、价格飙升和未来走向 11/4</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=ovWrlX6Yz9L7%2BqOAYbc5wQ%3D%3D.ApcB%2BOHk2fqu7qx%2F7l6jU82s5qNNF3sd7s6UEGB2OZymZPkKMR507tk%2B9Y5SZvZ93wvv0N4am%2FPT9gcWhGJXH4FmLRcS83os7W1hAoE5ONuDuobfWvjcB0lpAfcrSdZCGmPpDMi4VHRfUZvYjR3VU2kvyKDxJC6KZz8XmQQYxbc%3D" rel="nofollow" target="_blank">https://www.galaxy.com/insights/research/zcash-price-zec-near...</a></p><p>过去几周，隐私话题再次兴起。Zcash (ZEC**) 作为历史最悠久、最知名的隐私币之一，自 9 月以来涨幅高达 700%，仿佛加密货币领域人人都是隐私专家。</p><p><strong>📖 Mantle 完成 「ZK 化」革命：让现实世界的信任逻辑，在链上实现重新定义 11/6</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=9xr9r%2F7Zhc%2F1dldTxjh4wA%3D%3D.LlvDnGpmwKQzYcOuRLjZhQkWYGR%2FAbLLca8ZdBiOEoE%2Fe98hWixaQdfCjOkmpe%2Bo" rel="nofollow" target="_blank">https://foresightnews.pro/article/detail/91763</a>  </p><p>「当潮水退去时，才知道谁在裸泳」。</p><p>RWA 正在成为加密世界的新秩序之镜。它不仅折射出资本回流与机构入场的方向，也会揭示哪里才能成为真正承载 “现实世界的金融信任”。 而 Mantle， 正在成为下一阶段 RWA 基础设施竞争的关键变量。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKOQ" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>💸 zkVerify   11/4</strong></p><p>最高赏金 50,000 美元 </p><p>zkVerify允许模块化区块链网络将计算密集且成本高昂的证明验证过程分流出去，使它们能够专注于自身的主要功能，并在竞争中保持领先地位。</p><p>这种模块化方法不仅简化了操作，还显著提高了整体网络效率。</p><p><strong>💸 Xterio 11/6</strong></p><p>最高赏金 80,000 美元 </p><p>Xterio 是一个 Web3 游戏生态系统与基础设施，作为一家拥有顶尖开发技能和卓越发行专长的游戏发行商而脱颖而出。Xterio 怀揣着一个大胆的愿景——借助数字收藏品的力量重塑娱乐和游戏行业。这些非同质化资产拥有重新定义我们体验乐趣方式的能力。</p><p><strong>💸 Origin Protocol 11/4</strong></p><p>最高赏金 1,000,000 美元</p><p>Origin Protocol是一套互补的 DeFi 产品，旨在为所有人增加经济机会。这些无需许可且可组合的智能合约，在具有开创性的多链收益生态系统中，为整个去中心化金融领域提供卓越的用户体验。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKO4" alt="image.png" title="image.png" loading="lazy"/><br/>OpenBuild 是一个面向 Web3 开发人员的开源社区和平台。我们的目标是将更多的 Web2 开发人员带入 Web3 领域，同时帮助现有的 Web3 开发人员更好地构建并通过我们的产品取得商业成功！</p><p>欢迎在更多平台上关注我们：</p><p><a href="https://link.segmentfault.com/?enc=HOZwKkSP6Xae4Spjx1FZAw%3D%3D.YXwTtxg06gbkA%2B7eETaoespU09fZ3Wm9ZLcFtEZkYTQ%3D" rel="nofollow" target="_blank">https://linktr.ee/openbuild</a> 🙌🙌</p>]]></description></item><item>    <title><![CDATA[全球开发者集结！Mantle 全球黑客松]]></title>    <link>https://segmentfault.com/a/1190000047383618</link>    <guid>https://segmentfault.com/a/1190000047383618</guid>    <pubDate>2025-11-09 21:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025 年 9 月，Mantle 完成历史性跨越：主网从 Optimistic Rollup 全面升级为全球 TVL 规模最大的 ZK Rollup 网络，交易终局性缩短至 1 小时，吞吐量突破 3000 TPS。</p><p>在 TOKEN2049 的 Mantle Mixer 活动中，Mantle 宣布将进军现实世界资产（RWA）领域 —— 这是传统金融与代币化交汇的赛道，也是真实资产在链上获取真实收益的领域。</p><p>这些突破标志着 Mantle 新篇章的开启：让 Mantle 具备了承载大规模金融应用与日常场景的技术底座，但生态的增长和繁荣更需全球开发者的共创力量。</p><p>为此，Mantle 联合 HackQuest、OpenBuild 正式启动 Mantle Global Hackathon 2025: “Real Assets, Real Yield” Hackathon，一场面向全球开发者和区块链爱好者的线上黑客松。诚邀全球开发者将技术潜力转化为生态现实，把 Mantle 升级后的 ZK Rollup 打造为连接加密货币与传统金融的桥梁。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmYN3" alt="image.png" title="image.png"/></p><p>本次 Mantle Global Hackathon 2025: “Real Assets, Real Yield” Hackathon 共设立六大赛道，分别是 RWA/RealFi、DeFi &amp; Composability、AI &amp; Oracles、ZK &amp; Privacy、Infra &amp; Tooling、GameFi &amp; Social。</p><p>所有参赛队伍将在未来三个月内对 $150,000 总奖金池展开角逐，用自己的创造力定义去中心化金融的下一个时代。</p><h3>为什么要参加</h3><p>高额奖金激励：总额 150,000 美元的奖池，等待优秀的项目和团队来赢取。<br/>深度生态支持：获胜项目有机会获得 Mantle 生态基金的长期资金和孵化支持，同时接触潜在投资者与生态合作伙伴。<br/>全球社区曝光：参赛项目将获得在 Mantle、HackQuest 及 OpenBuild 等合作社区平台的联合宣传曝光机会。<br/>个人成长路径：参与系列 Workshop、获得专家和导师指导，以及与顶尖开发者、创始人及投资者进行交流的机会。</p><h3>Hackathon 时间</h3><p>注册报名：2025.10.22 - 2025.12.31<br/>项目提交：2025.10.22 - 2026.1.15<br/>初审投票：2026.1.15 - 2026.1.31<br/>Demo day：2026.2.1<br/>结果公示：2026.2.7</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQGa" alt="image.png" title="image.png" loading="lazy"/></p><h3>🎯参与方式</h3><p>报名注册：<a href="https://link.segmentfault.com/?enc=mCEBVJHXdFK4GcpA4vM7yw%3D%3D.asPiL9wCtwtltGhxgduQuZn%2BDv6U2BWmd0V7dgri0zuzphsb7JLFOdQACuNsi5FBF8ic4Uto%2BGG8nYteanjqUjW6tTbju3ctBN2CKxqKzJk%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a><br/>组队交流：加入 OpenBuild 开发者交流群，寻找志同道合的伙伴，获取最新资讯。 </p><p>项目提交：每支队伍在提交项目时需准备以下材料<br/>GitHub 代码仓库及部署说明<br/>3-5 分钟演示视频或测试网链接<br/>团队简介（2–5 人）与联系方式<br/>合规声明（RWA 相关项目需特别提供）<br/>一页纸白皮书（说明问题、解决方案及 Roadmap）</p><h3>奖金和赛道</h3><p>Mantle Build 2025 — “Real Assets, Real Yield” Hackathon 总奖金池高达$150,000。<br/>每个赛道设独立奖池，总金额 9 万美元，其中：<br/>第一名：$8,000<br/>第二名：$5,000<br/>第三名：$2,000</p><h4>特别奖项：</h4><p>总体冠军：$30,000<br/>社区票选奖（Top 3）：$6,000<br/>最佳 mantle 应用/集成奖：$4,000<br/>最佳演示与体验奖：$5,000<br/>快速孵化补助（3队）：$5,000/队</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQF2" alt="image.png" title="image.png" loading="lazy"/></p><h4>赛道：</h4><p>六大赛道等你挑战：RWA/RealFi、DeFi &amp; Composability、AI &amp; Oracles、ZK &amp; Privacy、Infra &amp; Tooling、GameFi &amp; Social。</p><p>RWA / RealFi（优先级最高）：代币化不动产、债券、票据、现金流产品的示范性应用（KYC/合规演示、托管结构、收益分配模型）。<br/>DeFi &amp; Composability：利率协议、抵押策略、组合器、基于 RWA 的合成资产。<br/>AI &amp; Oracles（AI Agents / 数据 + 预言机）：链上/链下数据驱动的智能策略、自动化资产管理（结合或acles 与 LLM/agent）。<br/>ZK &amp; Privacy：合规前提下的隐私方案（选择性披露、ZK-KYC、证明收益来源等）。<br/>Infrastructure &amp; Tooling：钱包插件、开发者 SDK、节点/监控、测试工具、自动化上链流水线。<br/>GameFi &amp; Social：利用 RWA 或收益机制的消费层场景、用户留存与代币经济设计。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQFP" alt="image.png" title="image.png" loading="lazy"/></p><h3>谁可以参加</h3><p>开发者、设计师、产品经理和区块链爱好者均可参加。<br/>无论是初学者还是资深专家，只要对 Mantle 生态和区块链技术充满热情，我们都欢迎你报名。<br/>支持单人参赛，也可以组队参与（2-5 人）。我们看重的是你的好奇心、创造力，以及推动 Mantle 生态落地 “真实价值” 的决心。</p><h3>评选标准和评委导师</h3><h4>评选标准</h4><p>项目将根据技术实现 、 产品与用户体验 、市场与落地可能性 、对 Mantle 的价值 / 集成度和影响力与可扩展性五个核心维度进行评估。具体如下：<br/>技术实现（40%）：系统完整性、代码质量、部署情况、漏洞基本检测。<br/>产品与用户体验（20%）：易用性、Demo 流畅度、上手成本。<br/>市场与落地可能性（20%）：商业模式、合规路径、目标用户与市场容量。<br/>对 Mantle 的价值 / 集成度（10%）：是否优先使用 Mantle 特性（低费、EVM 兼容、跨链桥）。<br/>影响力与可扩展性（10%）：可复制性、网络效应、后续发展空间。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQF9" alt="image.png" title="image.png" loading="lazy"/></p><h4>评委导师</h4><p>你的项目将由 Mantle 及整个 Web3 生态的顶尖领导者团队进行评审与指导，包括：<br/>Mantle 核心团队（CTO、生态负责人、技术主管）<br/>Bybit / 重要平台代表（投资、产品部门）<br/>外部技术专家（ZK、基础设施、共识机制领域专家）<br/>合规/法律专家（专注 RWA 赛道）<br/>投资人 / VC（后续融资与孵化）<br/>社区代表 / KOL（社区投票审核）</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmQGd" alt="image.png" title="image.png" loading="lazy"/></p><h3>技术支持和参赛资源</h3><p>Mantle 将提供构建、部署及扩展项目所需的全部资源，确保你信心十足地推进开发：</p><p>测试网 / RPC 额度：提供 Gas 费、测试币及 RPC 访问权限（基于配额）<br/>入门工具包：预制示例，包括 RWA 合约模板、USDC 收益分配方案、KYC 白名单演示及 Mantle SDK。<br/>Workshop 资料：包含 SPV（简单支付验证）+ 托管合规架构、部署指南、跨链集成示例<br/>审计支持：获胜团队可通过 Mantle 合作安全机构获得快速通道或折扣审计服务，帮助项目后续落地。</p><p>X：<a href="https://link.segmentfault.com/?enc=gz5ICAk1LxK%2F4excjz9J9g%3D%3D.PfuNQLIhdX351RUfRLEwWn39H4hIw6Kw02TiAiSnGu8%3D" rel="nofollow" target="_blank">https://x.com/Mantle_Official</a><br/>Workshop / AMA 信息：即将公布，敬请期待！</p><h3>期待你的加入</h3><p>未来属于那些敢于用代码绘制愿景的人。</p><p>Mantle 正站在链上新时代的前沿，当下正是你用创意与实践定义下一代链上基础设施的机会。无论你是新手还是资深开发者，只要有想法、有勇气，这次黑客松就是你将想象转化为创新、将创新转化为影响力的机会，同时也是进入 Mantle 生态、获得社区支持和项目曝光的跳板。</p><p>加入我们，与 Mantle 一起，将代码变成现实，打造下一代去中心化基础设施，共筑 Mantle 生态的下一篇章！</p><h3>关于主办方</h3><p>Mantle Network 是一个模块化的以太坊二层网络，专为高性能和高性价比而打造。<br/>通过将执行层、共识层和数据可用性层进行解耦，Mantle 为开发者带来了以下优势：<br/>🚀 超低 Gas 费<br/>⚡ 高吞吐性能<br/>🔧 原生兼容 EVM<br/>🧰 内置开发工具：Mantle DA、Mantle SDK、跨链桥以及测试网</p><p>无论是 DeFi、RealFi 还是其他场景，Mantle 都是构建大规模应用的理想选择。</p><p>HackQuest 是一个一站式、自主导向的 Web3 开发者教育平台。HackQuest 提供由专家策划的学习路径，并与包括 Solana、Mantle Network、Arbitrum 和 Linea 等领先的 Web3 生态系统共同发行链上证书。社区建设者还可以通过共同学习营、聚会、黑客马拉松、加速器和启动平台服务获得进一步支持。</p><p>OpenBuild是一个面向 Web3 开发者的开源社区。致力于为开发者提供高质量的系统性内容和活动，同时连接 Web2 和 Web3，帮助开发者过渡到去中心化的网络，并通过提供必要的工具和资源，帮助开发者建立声誉体系，构建信任，创造商业机会。</p>]]></description></item><item>    <title><![CDATA[企业邮箱有免费吗？免费申请全指南 遭老罪]]></title>    <link>https://segmentfault.com/a/1190000047383633</link>    <guid>https://segmentfault.com/a/1190000047383633</guid>    <pubDate>2025-11-09 21:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>“企业邮箱有免费的吗？”——答案是：有，而且1人也能开。Zoho邮箱提供永久免费版（5用户内），自带域名后缀、SSL加密与反垃圾，3分钟完成注册即可收发全球邮件。下文以Zoho邮箱为例，拆解试用政策、功能上限与升级路径，手把手教你零成本拥有专业企业邮箱。<br/><img width="723" height="443" referrerpolicy="no-referrer" src="/img/bVdmYLZ" alt="" title=""/> <br/>一、主流免费试用企业邮箱介绍：以Zoho邮箱为例</p><ol><li>Zoho邮箱的试用政策<br/>Zoho邮箱允许团队自定义域名接入，具备基本邮件收发、反垃圾邮件、日程管理等功能配置。根据公开数据，目前Zoho邮箱已在全球拥有1800万企业级客户，并进入全球企业邮箱排名前三。</li></ol><p>Zoho邮箱为用户提供1人起的邮箱服务。具体表现为，支持个人/企业通过官网进行注册，只要自有域名即可关联邮箱；无需绑卡，无隐形费用。对于有更高协作、管理需求的企业用户，还提供高级版、专业版等付费升级途径。</p><ol start="2"><li>试用功能及局限解析<br/>Zoho邮箱的基础版已能基本满足大多数小微企业与团队的日常需求。包括域名邮箱收发、日历、任务、备注、文件夹管理、反垃圾邮件等。需要注意的是，基础版在单域名可添加用户数（最多5人）、储存空间等方面有限制。</li><li>适合人群及应用场景<br/>Zoho邮箱的域名邮箱方案，尤其适合：</li></ol><p>对成本极为敏感、预算有限的创业公司<br/>小型团队/自媒体/电商卖家，需要提升业务形象<br/>希望拥有专业域名后缀的个人或社团<br/>在跨境贸易、远程协作、企业对外沟通等场景，Zoho邮箱广受认可。作为实用示例，不少国内外初创团队通过Zoho邮箱简洁明了的注册步骤，迅速搭建专属企业邮箱系统，获得安全、品牌统一、可靠的外部沟通渠道。</p><p>二、如何申请企业邮箱免费试用？</p><ol><li>基本申请流程<br/>以Zoho邮箱申请为例，流程高度简化：</li></ol><p>准备好可以验证的自有域名，进入Zoho邮箱官网<br/>选择“邮箱”入口，填写基础注册信息（如管理员姓名、邮箱、密码等）<br/>按照系统引导绑定域名邮箱，完成域名解析验证<br/>完善企业基础信息、添加团队成员<br/>只需3分钟即可初步开启企业邮箱系统。</p><ol start="2"><li>注册及实名认证所需资料<br/>注册Zoho邮箱需准备：企业管理员联系方式、自有域名管理权限、基础认证信息。实名认证流程，个人/企业均可选择，企业用户建议上传营业执照或组织机构代码证，确保后期邮件发送权属和账户安全。</li></ol><p>部分国内邮箱如腾讯企业邮箱、网易企业邮箱等在试用或正式开通前，也会要求实名注册并核验企业资质，部分可能要求管理员手机号、公司邮箱、企业信用代码等基本信息。</p><ol start="3"><li>试用期间注意事项与后续选择<br/>免费试用期间建议充分测试邮箱功能，比如域名邮箱收发效率、反垃圾拦截能力、移动端支持情况<br/>如果需批量导入成员，建议在Zoho后台用批量导入工具<br/>试用即将到期时，留意系统通知，及时决策是否升级为付费版本，以免邮件服务中断<br/>如果企业有更高安全、审批、分发等高级需求，可后期适时采购功能包。<br/>结语：免费企业邮箱如何选？Zoho邮箱真的值得吗？<br/>企业邮箱不必一掷千金。Zoho邮箱用“免费版+按需升级”把门槛降到零：3步注册即刻开通域名邮箱，15天全功能试用随时升级；AES-256加密、全球节点、1800万企业客户背书，让每一封邮件都专业、安全、秒达。</li></ol>]]></description></item><item>    <title><![CDATA[售后客户服务系统搭建指南 遭老罪的程序猿]]></title>    <link>https://segmentfault.com/a/1190000047383638</link>    <guid>https://segmentfault.com/a/1190000047383638</guid>    <pubDate>2025-11-09 21:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今天用Zoho Desk演示6步打法：需求洞察→流程标准化→团队赋能→科技提效→反馈闭环→品牌增值，让响应时间砍半，客户忠诚度翻倍！<br/><img width="431" height="287" referrerpolicy="no-referrer" src="/img/bVdmYOn" alt="" title=""/><br/>一、理解客户需求和期望<br/>系统化管理售后服务的第一步是深入了解客户的需求和期望。企业可以通过以下方式获取这些信息：</p><p>客户反馈与调查<br/>定期进行客户满意度调查，收集客户在使用产品后的反馈。这不仅可以帮助企业了解客户的真实感受，还能为调整服务策略提供数据支持。</p><p>直接沟通<br/>与客户进行直接的交流和互动，如通过售后电话或社交媒体平台，了解他们对产品和服务的意见。</p><p>数据分析<br/>运用大数据分析技术，挖掘客户行为与偏好，以此预测客户的未来需求。</p><p>通过Zoho Desk，企业可以轻松整合来自多渠道的客户反馈，并利用内置的报告和分析工具深入挖掘客户需求。这种集中化的客户信息管理，有助于企业设计更具针对性和个性化的售后服务方案。</p><p>二、建立高效的服务流程<br/>在理解客户需求后，建立高效的服务流程是系统化管理售后服务的核心。具体而言，这需要涵盖以下几个方面：</p><p>标准化服务流程<br/>制定清晰的售后服务流程图，把每一步的操作标准化，确保所有服务人员在面对不同客户时都能提供一致的高质量服务。 Zoho Desk支持自定义服务流程和自动化规则，企业可以通过设置自动化工作流来优化服务效率，例如自动分配工单、设置优先级等。</p><p>快速响应时间<br/>在售后服务中，响应时间的长短直接关系到客户满意度。企业应确保客服团队能够迅速有效地回应客户的请求，提供及时的解决方案。 Zoho Desk的SLA（服务级别协议）功能，可以帮助企业设定响应时间目标，并实时跟踪和提醒，确保客户问题得到及时处理。</p><p>透明的服务制度<br/>让客户了解售后服务流程和政策，如退换货条件、保修期等，增强客户的信任感。通过Zoho Desk的客户门户功能，企业可以提供一个透明的自助服务平台，让客户随时查询服务进度和相关政策。</p><p>三、培训与激励售后服务团队<br/>售后服务的质量与团队的能力和态度密不可分。因此，系统化管理还需要重点关注以下方面：</p><p>技能培训<br/>定期对售后服务团队进行培训，提升他们的专业技能与服务意识，包括沟通技巧、问题解决能力、产品知识等。</p><p>激励机制<br/>建立有效的激励制度来提升服务团队的工作积极性。可以通过绩效考核、奖励制度等方式，促进团队成员提供更优质的服务。</p><p>团队协作<br/>强调团队协作精神，确保各部门在处理客户问题时能够顺畅对接、快速解决。 Zoho Desk通过其团队协作工具（如工单共享、内部评论等），帮助售后团队高效协作，确保复杂问题能够快速得到解决。</p><p>四、引入科技提升服务水平<br/>在现代科技迅速发展的背景下，引入科技手段能显著提升售后服务的效率和效果：</p><p>客户服务管理平台（如Zoho Desk）<br/>Zoho Desk是一款专为售后服务设计的客户支持软件，能够帮助企业系统化地管理客户问题。通过工单管理、自动化工作流和多渠道支持，Zoho Desk让企业更高效地跟踪和解决客户问题。</p><p>人工智能与自动化<br/>Zoho Desk内置的AI助手Zia可以提供全天候支持，帮助解答客户的常见问题、分析服务趋势，并预测客户需求，从而提升工作效率并减少客服人员的负担。</p><p>数据分析与预测<br/>Zoho Desk内置的报告和仪表盘功能，能够实时分析客户反馈和服务数据，发现潜在问题趋势，并为企业提供数据驱动的服务优化建议。</p><p>五、建立完善的反馈机制<br/>持续完善售后服务，需要关注客户反馈，并将其作为提升服务的依据：</p><p>多渠道反馈收集<br/>通过电话、电子邮件、社交媒体等多种渠道收集客户反馈，全面了解客户体验。Zoho Desk支持多渠道整合，企业可以集中管理所有客户反馈。</p><p>分析反馈数据<br/>对收集到的客户反馈进行深入分析，明确服务中的短板，找出问题根源。</p><p>持续改进<br/>根据反馈分析结果，持续优化和改进售后服务流程，确保问题不再重复出现。借助Zoho Desk的自动化功能，企业可以快速实施改进措施，并通过后续跟进确保客户满意度提升。</p><p>六、打造文化与品牌效应<br/>企业的售后服务质量不仅依赖于流程和技术，更应反映出品牌文化和价值观：</p><p>诚信与责任<br/>在处理客户问题时，体现企业的诚信和责任。透明化的处理过程和合理的补偿措施，让客户感受到企业的诚意。</p><p>客户优先理念<br/>将客户满意度作为衡量售后服务的首要标准，努力为客户提供超出预期的服务体验。</p><p>品牌效应<br/>通过持续的优秀服务，积累良好的市场口碑，不断提升品牌形象和影响力。 Zoho Desk通过其客户满意度评分功能，帮助企业实时监测客户对售后服务的满意度，为品牌口碑的建立提供数据支持。</p><p>工具选对，售后变利润中心。Zoho Desk一张工单打通邮件、电话、社媒、自助门户；AI自动分配+SLA倒计时，复杂问题秒级分流；实时仪表盘把满意度变成可视化KPI。立即免费试用Zoho Desk，15天全功能，今天注册，明天就能看到客服指标全绿！</p>]]></description></item><item>    <title><![CDATA[用Excel做甘特图（2025新版） 遭]]></title>    <link>https://segmentfault.com/a/1190000047383656</link>    <guid>https://segmentfault.com/a/1190000047383656</guid>    <pubDate>2025-11-09 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>“计划的目的不是预测未来，而是为了现在做出预防它偏离正轨的决定。”彼得·德鲁克的提醒在今天依旧振聋发聩。可当这份“预防”落在项目经理的桌面时，最先被打开的往往不是专业系统，而是一张人人都会用的 Excel。用色块和条形图拼出甘特图，看似零成本，却常常把 80% 的精力消耗在 20% 的格式调整上——计划还没起跑，人已被表格拖垮。本文就带你拆解 Excel 做甘特图的全部技巧与天花板，并引出一个更轻盈的出路：Zoho Projects。<br/><img width="723" height="460" referrerpolicy="no-referrer" src="/img/bVdkYV6" alt="" title=""/><br/>一、Excel绘制甘特图的步骤详解<br/>尽管Excel主打电子表格功能，但它的灵活性使得我们可以创造性地用它制作甘特图。这需要一些技巧和耐心。</p><ol><li>起步：数据整理与基本格式设置<br/>一旦数据整理完成，记得设置正确的日期格式，并核查持续时间的天数是否和实际计划相吻合。数据的清晰性直接决定了甘特图绘制的准确性。</li><li>利用条件格式或条形图生成甘特图<br/>Excel中最常用的甘特图制作方法有两种：一是通过条件格式设置，对单元格背景填充颜色；二是借用条形图工具。</li></ol><p>条件格式法：利用Excel的公式设置条件格式，比如在时间轴区域内，只对那些“任务开始日到结束日”的单元格进行背景颜色填充。虽然设置稍显繁琐，但它的好处是在出现数据修改时可以动态调整。<br/>条形图法：使用Excel的“堆积条形图”功能。首先将任务的开始日期和持续时间进行分列，插入堆积条形图后，调整颜色和数据的轴向排列，使之看起来像是标准的甘特图。这个方法视觉效果更美观，同时效率也较高。</p><ol start="3"><li>自定义样式与优化图表<br/>如果甘特图用于对外展示，则需要美化和调整Excel表格的样式，例如更改调色板背景、添加标题、标注关键任务里程碑等。这一步帮助您提升图表的易读性，确保甘特图不至于过于“朴素”。</li></ol><p>二、Excel绘制甘特图的局限性<br/>虽然Excel在绘制甘特图方面可以实现一定的功能，但从效率和灵活性上来看，它并不是最佳的工具。此外，它也存在一些不可忽视的局限性。</p><ol><li>数据更新不够便捷<br/>在大型项目中，任务和日期常需要动态调整，而Excel在处理复杂数据更新时可能显得笨拙。您需要频繁修改表格公式或者手动移动条形图，这在多人协作时更容易出现版本冲突或错误。</li><li>缺乏特定的项目管理功能<br/>甘特图只是项目管理的一个视图，实际管理过程还涉及任务依赖关系、优先级设置、资源分配等功能，而Excel并没有这些功能原生支持。这就意味着，想要完整管理项目，可能需要依赖多个额外的工具。</li><li>协同能力较弱<br/>当团队涉及多人视图共享或实时更新时，只靠Excel难以追踪任何变更。即使在云端协作中，数据冲突、版本不一致等问题也难以杜绝。</li></ol><p>三、Zoho Projects如何成为更优选择？<br/>相比于Excel，专业的项目管理工具如Zoho Projects，可以为企业和个人用户提供更高效和专业的解决方案。以下是几个关键方面的优势：</p><ol><li>直观的甘特图视图与便捷操作<br/>Zoho Projects内置专业的甘特图功能，无需用户手动调整条形图或设置公式。通过简单的拖拽操作，您可以快速调整任务时间、设定依赖关系以及标记关键路线。不仅如此，其甘特图会根据任务动态自动生成，并与其他模块（如任务模块、工时模块）紧密集成，确保数据同步更新。这种自动化显著减少了人为错误。</li><li>更全面的项目管理功能<br/>除了甘特图，Zoho Projects还提供了丰富的项目管理功能，比如：</li></ol><p>任务依赖性设置：在任务之间定义“完成-开始”或“开始-开始”的逻辑，确保项目的有序推进。<br/>资源与团队管理：掌握团队成员的任务负荷，避免过载分配。<br/>里程碑与看板视图：多种视图切换方式，满足不同管理偏好的需求。<br/>这样，您无需在多个工具之间切换，所有管理功能都可以一站式实现。</p><ol start="3"><li>高效的团队协作与反馈跟踪<br/>Zoho Projects拥有内置的协同功能，允许团队成员实时查看任务进展并共享反馈。通过评论区、@提及功能以及自定义通知，团队可以随时沟通，确保任何项目中的问题都可以及时解决。此外，Zoho Projects还支持高级权限设置，确保数据安全。</li><li>提供多样化的报表与分析<br/>相比Excel的静态表格，Zoho Projects可以通过动态报表和仪表盘为管理者提供实时的项目洞察。例如，通过工时报告了解资源消耗，通过进度图标快速识别延迟风险。此外，所有报表都支持导出，无缝对接管理会议需求。</li><li>灵活的跨平台支持<br/>无论是在电脑、手机还是平板端，Zoho Projects都支持无缝访问。团队成员可以随时随地跟踪项目动态，而这些灵活的跨平台应用能力是Excel难以做到的。</li></ol><p>总结：甘特图不止于工具，更是效率的助推器<br/>Excel 像一把万能瑞士军刀，在单人、小样本、一次性场景里仍能快速“止血”；可当任务量级、变更频次、协作人数同时放大，继续用刀片拧螺丝只会划伤手指。把甘特图从单元格中解放出来，迁入 Zoho Projects，让拖拽代替公式、让依赖关系自动排期、让进度报表一键生成——计划不再被工具束缚，而真正成为团队同频的节拍器。毕竟，选对武器，现在的每一步才配得上未来的目标。</p>]]></description></item><item>    <title><![CDATA[跟5次黑客松冠军学 Linera 开发，]]></title>    <link>https://segmentfault.com/a/1190000047383392</link>    <guid>https://segmentfault.com/a/1190000047383392</guid>    <pubDate>2025-11-09 20:06:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdmYKp" alt="image.png" title="image.png"/></p><p>OpenBuild 社区联合 ResPeer 团队的 KK，推出 Linera 开发者实战系列免费课程。</p><p>本次课程将帮助您理解 Linera 如何通过客户端驱动的共识机制和微链架构来解决传统区块链的性能瓶颈问题。</p><h3><strong>🧑‍💻 主讲人</strong></h3><p><strong>KK</strong><br/>ResPeer创始人，ResPeer是Linera生态应用开发团队，获取过5次黑客松冠军，在Linera上开发了钱包、Meme发射协议、Swap**等应用。</p><h3><strong>课程内容：</strong></h3><p><strong>Lesson 01：走进 Linera</strong></p><p>了解项目背景、微链机制及其与传统架构的区别</p><p><strong>Lesson 02：开发环境实战</strong></p><p>熟悉CLI工具链，理解微链和应用概念</p><p><strong>Lesson 03：Token 应用开发</strong></p><p>通过实践理解Operation状态管理</p><h3><strong>👥 适合人群：</strong></h3><ul><li>对区块链新技术感兴趣的开发者</li><li>想了解微链架构的技术人员</li><li>希望参与 Linera 生态建设的工程师</li></ul><h3><strong>📖 开始学习</strong></h3><p>课程现已上线，复制链接开始学习：</p><p>📺 B站：<a href="https://www.bilibili.com/video/BV1wfyaBuEDM" target="_blank">https://www.bilibili.com/video/BV1wfyaBuEDM</a></p><h3><strong>💪 一起学习</strong></h3><p>欢迎加入课程学习群，获取课程PPT, 以及讲师答疑， 关于更多课程进度也会在社群发布！如果二维码过期，请加小助手微信（ID: qq99220909），备注”公开课“ 加入学习群，  一起进步！</p><h3><strong>🔗 关于Linera</strong></h3><p>Linera 是由前 Meta Libra/Diem 核心研究员 Mathieu Baudet 创立的新一代区块链基础设施项目，获得 a16z** 和 Borderless Capital 共1200万美元投资。项目通过创新的"微链"架构解决传统区块链的性能瓶颈——每个用户或应用可拥有独立的微链，实现真正的水平扩展和亚秒级交易确认，彻底告别网络拥堵和 Gas 费波动。  </p><p>与传统区块链的单链模型不同，Linera 采用客户端驱动的共识机制，支持无限 TPS 扩展，特别适合实时支付、游戏、预测市场等对性能和确定性要求极高的应用场景。目前项目处于测试网第三阶段，核心协议和开发工具链已就绪，正通过 Buildthon 黑客松孵化生态应用。</p><p>🌐 官网：linera.io | 📖 GitHub：github.com/linera-io</p>]]></description></item><item>    <title><![CDATA[GitHub Octoverse 202]]></title>    <link>https://segmentfault.com/a/1190000047383490</link>    <guid>https://segmentfault.com/a/1190000047383490</guid>    <pubDate>2025-11-09 20:05:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="396" referrerpolicy="no-referrer" src="/img/bVdmYLS" alt="image.png" title="image.png"/></p><p>前几天，Github 发布了 Octoverse 2025 报告。这篇以 “增长” 为核心主题的报告，通过多维度数据呈现全球开发者生态、技术选择与行业变革：全球开发者社区正以史无前例的速度扩张，AI 已从 “可选工具” 变为 “标配能力”，TypeScript** 重塑语言格局，而新兴市场正成为开源增长的核心动力。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLR" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>核心总结</strong></h4><ul><li>GitHub 开发者超 1.8 亿，过去一年新增 3600 万，平均每秒就有 1 人加入，80% 新开发者首周就用 Copilot free 来开发</li><li>TypeScript 第一次超过 Python 和 JavaScript，成了最常用的语言；Python 在 AI 领域还是最火的</li><li>印度去年新增 520 万开发者，是开源贡献者最多的国家，按趋势 2030 年全球每 3 个新开发者就有 1 个来自印度</li><li>AI 相关的开源项目增长最快，比如 vllm 这些，但很多开源仓库都没有贡献者指南，管理跟不上</li><li>GitHub 上有 430 万 AI 相关仓库，Copilot 智能体能帮忙写代码、提请求，还能自动修复一些漏洞</li><li>关键漏洞修复变快了 30%，但权限漏洞变多了，15.1 万仓库受影响，很多是 AI 生成代码没做好权限检查</li></ul><h3><strong>2025年 GitHub 核心现状：创纪录的增长规模</strong></h3><p>2025 年是 GitHub 历史上增长最快的一年，开发者数量与项目活跃度均突破峰值，平台作为全球开发者协作核心的地位进一步巩固。</p><p>从开发者规模看，全球 GitHub 开发者总量超 1.8 亿，过去一年（2024 年 9 月 - 2025 年 8 月）新增 3600 万，同比增长 23%，<strong>平均每秒新增 1 名开发者</strong>。新增开发者呈现显著地域多样性：每分钟约 25 名来自亚太地区、12 名来自欧洲、6.5 名来自非洲与中东、6 名来自拉丁美洲，其中印度单国新增超 500 万开发者，占全球新增总量的 14% 以上。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLQ" alt="image.png" title="image.png" loading="lazy"/></p><p>项目活跃度同样创下新高：<strong>2025 年 GitHub 代码仓库总数达 6.3 亿个，新增 1.21 亿个</strong>；私有仓库新增 5800 万个（同比增长 33%），占比提升，反映企业级开发向 GitHub 集中的趋势；公共仓库达 3.95 亿个（同比增长 19%），仍是开源生态核心载体。开发者日常操作频次大幅提升：每分钟创建超 230 个新仓库，每月合并 4320 万次拉取请求（同比增长 23%），全年代码提交近 10 亿次（同比增长 25.1%），仅 2025 年 8 月单月提交量就达近 1 亿次。</p><h3><strong>GitHub Copilot：驱动增长的关键变量</strong></h3><p>2024 年 12 月 “GitHub Copilot Free” 的推出，打破了 GitHub 长期稳定的增长规律，成为开发者注册与项目活跃的核心推动力。</p><p>Copilot Free 上线后， 新开发者注册量超出去年同期预测值，且 AI 工具渗透率快速提升：80% 的新开发者在加入 GitHub 的第一周就会使用 Copilot，表明 <strong>AI 辅助工具已从 “可选功能” 变为新开发者的 “默认需求”。</strong></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLP" alt="image.png" title="image.png" loading="lazy"/></p><p>Copilot 对开发效率的提升已量化显现：2025 年 3 月  preview of Copilot coding agent、4 月 Copilot code review 功能相继上线后，开发者问题关闭效率明显提升，3 月关闭问题数量较 2 月增加 140 万，7 月单月关闭问题达 550 万，创历史最高；同时代码提交频次加快，5 月起月均代码推送突破 900 万次，拉动全年提交量增长。此外，Copilot 在安全领域作用凸显，Copilot Autofix 功能每月在 6000 + 仓库修复 “访问控制漏洞”，在 3000 + 仓库修复 “注入漏洞”，成为自动化安全防护的重要工具。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLO" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>编程语言格局：TypeScript 登顶，Python 主导 AI 领域</strong></h3><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYLN" alt="image.png" title="image.png" loading="lazy"/></p><p>2025 年 8 月成为 GitHub 编程语言格局的关键转折点 —— 按贡献者数量统计，<strong>TypeScript 首次超越 Python 与 JavaScript，成为最常用语言</strong>，这是近十年最显著的语言地位变动。TypeScript 和 Python 合计贡献者超过 520 万。AI 不仅加快代码速度，还在影响团队选择何种语言来将 AI 生成的代码投入生产环境。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYLq" alt="image.png" title="image.png" loading="lazy"/></p><p>TypeScript 的增长源于生态与技术需求双重支撑：2025 年新增贡献者 105.4 万（同比增长 66.63%），核心驱动因素包括主流前端框架（React、Vue、Svelte 等）默认采用 TypeScript 搭建项目，降低入门门槛；其类型安全特性与 AI 辅助编码高度适配，能减少 AI 生成代码在生产环境的故障风险，成为企业级项目优先选择。</p><p><strong>Python 虽在总使用量上被超越，但在 AI 与数据科学领域主导地位进一步巩固</strong>：2025 年贡献者达 260 万（同比增长 48.78%），58.2 万余个 AI 标签仓库使用 Python（同比增长 50.7%），占所有 AI 项目近 50%；Jupyter Notebook** 作为 AI 实验核心工具，应用于 40.3 万余个仓库（同比增长 17.8%），仍是模型训练、数据分析、算法原型开发的 “标配环境”。</p><p>此外，小众语言呈现差异化增长：Luau（Roblox 脚本语言，同比增长 194%）、Typst（LaTeX 替代方案，同比增长 108%）、Astro**（轻量前端框架，同比增长 78%）、Blade（Laravel 模板引擎，同比增长 67%）等在细分领域快速崛起，反映开发场景日益多元化。</p><p><img width="713" height="838" referrerpolicy="no-referrer" src="/img/bVdmYLj" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>全球开发者地域分布：新兴市场崛起与2030年预测</strong></h3><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYK1" alt="image.png" title="image.png" loading="lazy"/></p><p>从当前数据看，印度已成为全球开发者增长的 “核心引擎”：2025 年印度新增开发者 520 万，；按 GitHub 数据团队预测，<strong>到 2030 年印度开发者数量将达 5750 万</strong>，占全球新开发者总量的 1/3，成为全球第一大开发者市场。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKJ" alt="image.png" title="image.png" loading="lazy"/></p><p>其他新兴市场同样表现亮眼：亚太地区除印度外，日本、印尼增速显著 —— 日本受益于数字转型政策，开发者数量较 2020 年增长 3 倍；印尼开发者达 437 万（2020 年为 90 万），占东南亚数字经济近 50%，成为区域核心。拉丁美洲的巴西、墨西哥、哥伦比亚新增 320 万开发者，主要依赖美国 / 欧盟企业的远程招聘与金融科技创业生态爆发。非洲与中东新增 340 万开发者，移动设备普及、社区编程训练营与本地 LLM 应用，成为推动增长的关键因素。</p><p><img width="714" height="616" referrerpolicy="no-referrer" src="/img/bVdmYKI" alt="image.png" title="image.png" loading="lazy"/></p><p>成熟市场则保持稳定增长：<strong>美国仍是开发者人口总量第一的国家（2800万），但开发者增速放缓</strong>；欧洲（德国、英国、法国）依赖云基础设施投入与 AI 领域投资，新增 630 万开发者。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKG" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>开源生态：AI 项目主导，治理滞后成挑战</strong></h3><p>2025 年开源生态保持高活力，贡献量与项目数量均创历史新高，但同时面临治理滞后问题。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKE" alt="image.png" title="image.png" loading="lazy"/></p><p>从活跃度看，<strong>2025 年全球开源贡献达 11.2 亿次</strong>（同比增长 13%），合并拉取请求 5.187 亿次（同比增长 29%）；3 月成为 “GitHub 开源新贡献者最多的月份”，新增首次贡献者 25.5 万名。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKD" alt="image.png" title="image.png" loading="lazy"/></p><p>项目层面，AI 基础设施项目成为绝对主力 —— 贡献者增长最快的 10 个开源项目中，6 个为 AI 相关，其中 <strong>vllm-project/vllm 成为贡献者数量最多的开源仓库</strong>，超过传统热门项目 microsoft/vscode、home-assistant/core。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKC" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="708" height="668" referrerpolicy="no-referrer" src="/img/bVdmYKB" alt="image.png" title="image.png" loading="lazy"/></p><p>但开源治理滞后问题凸显：仅 5.5% 的开源仓库提供 “贡献者指南”，2% 配备 “行为准则”，远低于 63% 的 README 文件覆盖率；这导致新贡献者入门门槛高，项目协作中易出现冲突。此外，OpenSSF Scorecard（开源安全评分标准）的采用率虽在头部项目中达 94%（前 50 名项目中 47 个使用），但中小项目的安全配置率仍不足 30%，存在潜在风险。</p><p>从贡献者分布看，开源生态呈现 “全球化” 特征：<strong>印度超越美国成为 “开源贡献者最多的国家”</strong> ，巴西、印尼进入前 5 名；<strong>美国虽贡献总量第一</strong>，但人均贡献频次下降，反映开源生态的 “去中心化” 趋势。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmYKA" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>安全实践：自动化提升与新风险并存</strong></h3><p>2025 年开发者安全实践的 “自动化程度” 显著提升，但同时涌现出新的安全风险，整体呈现 “机遇与挑战并存” 的态势。<br/><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKz" alt="image.png" title="image.png" loading="lazy"/></p><p>自动化工具的应用大幅提升安全效率：2025 年配置 Dependabot 的仓库达 84.6 万个（同比增长 137%），该工具能自动检测依赖项漏洞并提交修复 PR，推动严重漏洞修复速度加快 30%—— <strong>平均修复时间从 2024 年的 37 天缩至 26 天。</strong></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKy" alt="image.png" title="image.png" loading="lazy"/></p><p>同时，GitHub Actions 的使用量增长 35%，11.5 亿分钟的免费 CI/CD 时长被用于公共项目的安全测试，进一步降低安全防护门槛。</p><p>但新的安全风险不容忽视：“访问控制漏洞” 取代 “注入漏洞”，成为 CodeQL 检测到的最常见风险（15.1 万余个仓库被标记，同比增长 172%），主要源于两方面：AI 生成的代码中常缺失权限校验逻辑，导致未授权访问；CI/CD 管道的权限配置不当，引发数据泄露风险。此外，尽管 Copilot Autofix 能修复部分常见漏洞，但针对复杂业务逻辑的安全问题，仍需人工介入，安全防护的 “人机协同” 模式尚未完全成熟。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKx" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>生成式 AI 与 Agentic ：从实验到日常工程</strong></h3><p>2025 年生成式 AI 与智能体工具从 “技术实验” 进入 “日常开发工程”，成为重塑开发流程的核心力量。</p><p><img width="723" height="183" referrerpolicy="no-referrer" src="/img/bVdmYKv" alt="image.png" title="image.png" loading="lazy"/></p><p>从项目规模看，<strong>AI 相关仓库总数突破 430 万个，较 2023 年近乎翻倍</strong>；113 万 + 公共仓库引入 LLM SDK（同比增长 178%），其中 69.3 万余个是过去 12 个月新增，涉及 OpenAI、Anthropic、Mistral 等主流模型的集成。从贡献者来看，<strong>每月有 20 万 + 开发者为 AI 项目提交代码，2025 年 5 月贡献者数量达峰值 20.68 万</strong>（同比增长 132%），反映 AI 开发已从 “小众领域” 变为 “大众方向”。<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKw" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYKu" alt="image.png" title="image.png" loading="lazy"/></p><p>智能体工具的落地是关键突破：GitHub Copilot coding agent 自上线后，5-9 月协助创建超 100 万次拉取请求，能自动完成代码生成、测试执行、PR 创建等流程；且其应用集中在 “高星标、大规模” 的成熟项目中，<strong>表明企业级团队已开始将智能体用于核心开发环节</strong>。此外，AI 对工具链的影响逐步深化 —— 开发者选择 IDE、框架时，会优先考虑 “是否适配 AI 工具”，如本地 LLM 运行器 Ollama、RAG 框架 Rag**flow 的快速增长，均源于对 AI 工作流的适配。</p><p>完整报告：<a href="https://link.segmentfault.com/?enc=19ixdLJgGn042rOwfwqPXQ%3D%3D.r29GMjhanuOhDcyQUwR2dYByK5qdxCbwVyBfXP%2BJeAViyPG8EzTtOPMtouaHknNJmCNhagDH%2BGU%2BWLsxbwxu7Ut5t5Tll%2F28RuILwq%2BNxJS3z9R56oj59eqSMA6frN0EQ0ZsKgUN%2BPhn0CuvnvTMPYi7xDfUX3oIVVIvKAkqdg0%3D" rel="nofollow" target="_blank">https://github.blog/news-insights/octoverse/octoverse-a-new-d...</a></p>]]></description></item><item>    <title><![CDATA[HackerHouse 精彩回顾：深圳五]]></title>    <link>https://segmentfault.com/a/1190000047383515</link>    <guid>https://segmentfault.com/a/1190000047383515</guid>    <pubDate>2025-11-09 20:05:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>10月23日-27日，由 Solar 联合 OpenBuild 举办，华为、BGA 特别支持的 Solana Cypherpunk Mini Hacker House 深圳站完美落幕！近 20 位 Hackers 齐聚大鹏海边别墅，用五天时间沉浸式探索 Solana 生态、协作开发项目。在海浪与代码的交织中，完成了一场从创意萌芽到项目落地的探索之旅。</p><h3><strong>Day 1：集结！Build Together</strong></h3><p>23日下午，阳光洒满大鹏海岸。Hacker 们陆续抵达抵达别墅，为即将开启的五天 co-buildind 时光做准备。初见时的拘谨并未持续太久，有人分享自己对 Web3 领域的独特见解，有人提出极具创新性的项目构想。几句关于技术与生态的交流，以及 idea 的碰撞，志同道合的开发者们迅速找到了共鸣，让原本安静的空间响起热烈的讨论声。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMe" alt="image.png" title="image.png"/></p><p>晚上吹着海风，海鲜大餐成为破冰的关键：蒜蓉粉丝蒸扇贝、椒盐皮皮虾等地道美味让氛围迅速升温。大家开始自己介绍，分享各自的从业经历与项目创意。现场既有充满活力的在校大学生，也有深耕 Web2、Web3 行业多年的资深开发者，还有多次参与黑客松活动的 “老手”。</p><p>这一晚，没有紧绷的开发任务，只有纯粹的技术交流、理念分享，为接下来五天的协作奠定了温暖的基础。</p><h3><strong>Day 2：Co-living、Co-building</strong></h3><p>经过首日的破冰与磨合，Hacker 们在 24 日正式进入项目开发阶段，building 氛围从清晨就开始拉满。当天上午，活动特邀多位嘉宾到场带来精彩分享，为开发者们赋能。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMf" alt="image.png" title="image.png" loading="lazy"/></p><p>首先，来自 OpenBuild 社区的 qiuqiu 为大家详细介绍了本次 Solana 全球黑客松和 Hackerhouse 的规则和注意事项，同时分享了 OpenBuild 社区的发展历程、资源支持以及过往黑客松活动的优秀案例，让开发者们对后续开发方向有了更清晰的认知。</p><p>Solayer 的市场负责人 Margie 同样给 Hacker 们深入剖析了 Solayer 在 Solana 生态中的技术优势与应用场景，为开发者们打开了硬件与区块链结合的新思路。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdmYMg" alt="image.png" title="image.png" loading="lazy"/></p><p>紧接着，来自 Jito Labs 的 Atsushi 介绍了 Jito 的核心功能与技术架构，并分享了 JET Asia（Jito DAO）未来的发展规划与战略布局，让开发者们对 Solana 生态中的基础设施项目有了更全面的了解。</p><p>三位嘉宾的分享干货满满，不仅解答了开发者们在技术选型、生态适配等方面的疑惑，更为大家提供了多元的项目开发思路，现场掌声不断。</p><p>夜幕降临，一场热闹的屋顶 BBQ 音乐派对点燃热烈氛围。夕阳下，烧烤的香气混合着啤酒的清爽，Hacker 们在悠扬的音乐中，开始 social，为精彩捧杯，享受着独属于他们的蓝调时刻。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMh" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMi" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMj" alt="image.png" title="image.png" loading="lazy"/></p><p>晚餐过后，OpenBuild 的 buildhero skyh 为大家分享了以往 Solana 黑客松活动中的优秀项目，从项目创意、技术实现到市场推广，进行了全方位解读，为项目开发注入了更多灵感。</p><h3><strong>Day 3：Mock Demo，代码之夜</strong></h3><p>经过 Day 2 的赋能，开发者们的项目已初步成型。10 月 25 日，活动进入 Mock Demo 环节，通过提前演练与点评，帮助各团队完善项目。</p><p>在模拟演示中，各团队用 5 分钟呈现初期成果。</p><ul><li>WorkWork：聚焦数字游民与自由职业者需求，提出 “Work everywhere, Work anytime” 的核心定位，目前已实现远程工作任务匹配、跨时区协作日程管理、多币种薪酬结算三大基础功能，计划后续接入 Solana 链上身份系统，提升用户信息安全性；</li><li>MindSensor：则带来 AI 与 Web3 结合的创新尝试，以 “Awareness is Value” 为理念，呈现了融合脑波检测、心灵觉醒与链上生态的 AI 平台，现场演示了通过简易脑波设备采集专注度数据，转化为链上积分的初步流程，未来拟将积分用于生态内课程兑换、冥想社群权益解锁；</li><li><p>Pelago：深耕 Solana 生态 DeFi 领域，介绍了其基于隔离市场架构的借贷协议，核心亮点在于支持无许可创建自定义借贷市场，团队已完成基础合约开发，可实现不同资产的独立风险定价，适配从稳定币到小众代币的多样化借贷需求。</p><p>...</p></li></ul><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMk" alt="image.png" title="image.png" loading="lazy"/></p><p>各项目团队在演示结束后，结合现场反馈与讨论，收获了不少跨团队的创新思路，开始进一步梳理了优化方向。直到深夜，整个别墅都充满了热血的开发氛围！</p><h3><strong>Day 4: 沙滩、落日、飞盘</strong></h3><p>经过前三天的高强度开发与优化，10 月 26 日，活动为开发者们安排了轻松的海边休闲环节，让大家在忙碌之余放松身心，为次日的 Demo Day 积蓄能量。</p><p>当天上午，开发者们依旧保持着专注的状态，在别墅内继续打磨项目细节。有人针对演示流程进行反复演练，有人对项目界面进行最后优化，每个人都在为即将到来的 Demo Day 做着充分准备，力求呈现最完美的项目效果。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMl" alt="image.png" title="image.png" loading="lazy"/></p><p>午后，阳光正好，开发者们来到海边，开启了一场充满活力的休闲时光。飞盘在手中传递，笑声在沙滩上回荡；有人纵身跃入海中，感受海水的清凉；有人漫步沙滩，留下一串串脚印…… 大家在欢声笑语中释放压力，享受着海边的惬意与美好。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMm" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMn" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMo" alt="image.png" title="image.png" loading="lazy"/></p><p>当晚，Hacker 们再次齐聚，享用了一顿丰盛的海鲜晚餐。餐桌上，大家不再谈论代码与项目，而是分享着这几天的欢乐瞬间，彼此的情谊在温馨的氛围中进一步加深，本次 Hackerhouse 也即将在海风与星光的见证下圆满收官！</p><h3><strong>Day 5：Demo Day 成果亮相</strong></h3><p>10月27日，Solana Cypherpunk Mini Hacker House 深圳站 Demo Day 正式拉开帷幕。这是五天努力的成果展示，更是一场 Web3 创新思想的碰撞盛宴。</p><p>活动伊始，qiuqiu 为本次活动致开场词，鼓励大家尽情展示成果；Solayer 的市场负责人 Margie 带来主题为《硬件加速，无限扩展》的分享，深入解读技术前沿；Orca 的中文贡献者 Yibo 则通过《DeFi 乐高，基于 Orca 搭建的方向》主题分析，介绍了 Orca 项目情况和进展。</p><p><img width="723" height="1098" referrerpolicy="no-referrer" src="/img/bVdmYMa" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMb" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMc" alt="image.png" title="image.png" loading="lazy"/></p><p>随后，15 + 支团队依次登台，展示各自的创新项目。无论是聚焦 DeFi 领域的新型协议，还是探索 Web3 与硬件结合的创新产品，亦或是关注实体资产上链的 RWA 项目，每一个项目都展现了开发者们对 Solana 生态的深度思考与创新实践。在展示过程中，团队成员自信从容，条理清晰地讲解项目理念、技术架构与应用场景，现场评委与观众不时发出阵阵赞叹。</p><p>此次 Demo Day 的成功举办，离不开多位特邀嘉宾的大力支持。非常感谢 Vesper(Community Lead @Solar)、 Mike(DevRel @Solana基金会)、Margie(Marketing Lead @Solayer)、Atsushi(Asia Lead @Jito)、Yibo(Chinese Contributor @Orca)以及 Skyh (core builder @OpenBuild)，他们不仅在现场为开发者们提供了专业点评与指导，还分享了行业前沿动态，为活动增添了更多价值。</p><h3><strong>结语</strong></h3><p>从海边别墅到 Demo Day 舞台，这段旅程不仅碰撞出了 15 + 个具有潜力的项目，更凝聚了一批热爱 Solana、愿意深耕 Web3 领域的优秀开发者。非常高兴能与各位开发者一同在海边度过这段充实而精彩的时光！</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYL9" alt="image.png" title="image.png" loading="lazy"/></p><p>由衷感谢所有让这一切成为可能的人！特别感谢 Vesper、Mike、Margie、Atsushi、Skyh、Yibo 等多位老师的全程支持与指导，为 Hacker 们指明了开发方向。感谢工作人员 Kang、轩睿、142y、Z.c、Anyi 对 HackerHouse 日常运营的辛苦付出，也感谢各位开发者的积极参与，用自己的热情与创新，让本次 Hackerhouse 充满活力！</p><p>在未来，期待更多开发者加入 Solana 的探索之旅！OpenBuild 也将开发者一起，见证更多优秀的 Solana 项目从想法走向现实！</p>]]></description></item><item>    <title><![CDATA[Web3 开发者周刊 74 | x402]]></title>    <link>https://segmentfault.com/a/1190000047383524</link>    <guid>https://segmentfault.com/a/1190000047383524</guid>    <pubDate>2025-11-09 20:04:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMs" alt="image.png" title="image.png"/></p><p><strong>欢迎回到 Web3 开发者周刊第 74 期！</strong></p><p>本期周刊内所有黑客松活动、新闻和赏金任务，请大家点击查看原文以获取完整信息。如果您喜欢我们的内容，也欢迎大家订阅 <strong>OpenBuild Substack</strong>，获取最新最全开发者资讯！</p><p>本周，我们将探讨叙事型和实用型加密货币对开发者的差异化要求，聚焦最近大火的x402 还有哪些需要改进的地方，以及关注比特币白皮书发布17周年有哪些启示。</p><p>此外，我们还为开发者准备了新的黑客松资讯和赏金任务。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmYMt" alt="image.png" title="image.png" loading="lazy"/><br/><strong>✅ Mantle Global Hackathon 2025</strong></p><p>📅 时间：2025年10月22日 - 2026年2月7日</p><p>📍 线上</p><p>💸 奖金：150,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=%2FKTFCfrxyk1qzX9TcH%2B8mA%3D%3D.rzbWUyWCGnLplSmL9cR9T7bvnoHd4j7c6g7NJidvFBSljc46OjGjhGMzBrC8MWkNUBfJDrC3pSDp1EOJ7TP9dmJKqneAlWc7dVWYAa76sZo%3D" rel="nofollow" target="_blank">https://www.hackquest.io/hackathons/Mantle-Global-Hackathon-2...</a></p><p><strong>简介：</strong></p><p>Mantle Global Hackathon 2025 是 Mantle 的全新篇章。这场为期三个月的黑客松，邀请开发者、创业者及创新者在 Mantle 上设计、开发并部署具备可扩展性的 Web3 产品。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMu" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ Solana X402 Hackathon</strong></p><p>📅 时间：10月28日 - 11月11日</p><p>📍 线上</p><p>💸 奖金：50,000 美元 </p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=IijYExAT9%2BDZhPx89Z%2BeUQ%3D%3D.iSCAWia%2Fc16h2ZhyStCR%2BDdLnjqxR8ym4PYXegKepNAvotRLh2zmFq665lqWUNMB" rel="nofollow" target="_blank">https://solana.com/zh/x402/hackathon</a></p><p><strong>简介：</strong></p><p>构建代理经济的机会来了。Solana X402 Hackathon 旨在基于 x402 构建新的创新工具、应用、基础设施和代理。你将构建开源基础设施和应用程序，以推动 Solana 上的代理经济。创建使 AI 代理能够自主交易的工具，开发创新的支付解决方案，或构建实用的代理应用程序。这是你定义自主代理如何与数字经济互动的机会。</p><p><img width="680" height="383" referrerpolicy="no-referrer" src="/img/bVdmYMv" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>✅ NullShot Hackathon</strong></p><p>📅 时间：10月1日 - 11月22日</p><p>📍 线上</p><p>💸 奖金：30,000 美元</p><p>🔗 链接：<a href="https://link.segmentfault.com/?enc=%2FG3tatJiNWZ%2B5nrWYcemEg%3D%3D.2qxqUiN2rnLLfkUAu5Cp5jMXShg%2F0qKmm2Xg5PYpiamPCTTU9zf%2FrJ%2BVlNuz4k7MOIY87Bi269687GEgsvwqzQ%3D%3D" rel="nofollow" target="_blank">https://dorahacks.io/hackathon/nullshothacks/detail</a></p><p><strong>简介：</strong></p><p>NullShot 黑客松汇聚了开发者、研究人员和建设者，共同探索智能去中心化应用的潜力，并围绕 AI-native 开发培育一个不断发展的社区。黑客松奖金池为30,000美元，专门面向区块链的可组合性与人工智能的互操作性相结合的建设者，助力独立代理和人工智能应用网络能够释放新的实用形式和价值创造方式。</p><p><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdmYMw" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="162" referrerpolicy="no-referrer" src="/img/bVdmYMx" alt="image.png" title="image.png" loading="lazy"/><br/><strong>📖 叙事型加密货币 vs 实用型加密货币 10/27</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=3Uv0bqciyE270M4rffVALQ%3D%3D.451%2FYmGdmpI5hJYrhvEnWKL3JDfdT4aKi9NwgU1kwfLd6i2NowxP9pT0VO%2BV3X4UXzu2ZfPJWq5PhTwIISPKhA%3D%3D" rel="nofollow" target="_blank">https://x.com/yashhsm/status/1982636379650838745</a></p><p>在加密货币领域，“实用性” 与 “叙事性” 二者同等重要。作为开发者，始终要选择其中一个作为开始，并在叙事性或实用性方面做到非常出色。先某一方面打磨成熟，再向另一方面拓展 —— 这才是最终的发展路径。  </p><p><strong>📖 x402 很好，但我们还需要更多 10/30</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=hVnIi%2BmUhqZdGqL%2FLeEhCw%3D%3D.R7XyY%2F0GfP1abBMRli2uLqhWRCkOifNJBLuuZ9dPkBxAf%2BupEyB%2FgbD7WUoy7aohpcQ%2FFpoeOnqVq9FmvkGQBPv77dmkQkrPaZVaEYU7ZL0%3D" rel="nofollow" target="_blank">https://x.com/tbtstl/status/1980363102676824574?s=12&amp;t=2q9OLM...</a></p><p>去中心化互联网需要去中心化支付体系的支撑。X402 v1 已经充分展现了技术的潜力与可能性；X402 v2 虽实现了阶段性的改进，但目前来看，我们还需要更具突破性的进展。</p><p><strong>📖 比特币白皮书迎来 17 岁生日 10/31</strong></p><p>👉 链接：<a href="https://link.segmentfault.com/?enc=VTNdYVdntJmp%2FXeJa2od%2FA%3D%3D.31ttU18A%2BdPweavcDWKBilcyqWsIRZbc8z4EzFL7okeFWMcuyfOjmeWUl0ucyWMHKgCYUuUq2yHDrWGRu5phIg%3D%3D" rel="nofollow" target="_blank">https://x.com/intangiblecoins/status/1984275450168770715</a></p><p>2008 年 10 月 31 日，中本聪**发布了一篇 9 页长的《比特币》白皮书。</p><p>17 年间，比特币的蜕变令人震惊。它最初只是一个仅有数百名密码朋克讨论的小众开源项目，如今已成为一种全球交易的资产，持有者涵盖机构、主权国家以及数千万普通个人。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKOQ" alt="image.png" title="image.png" loading="lazy"/><br/><strong>💸 Ostium   10/27</strong></p><p>最高赏金 100,000 美元 </p><p>Ostium 协议是以太坊第二层 Arbitrum 上的一个开源去中心化交易所，它能让用户以透明且非托管的永续合约方式接触现实世界资产。Ostium 经过定制，支持从加密货币钱包对黄金、石油、标准普尔指数**、日元等资产以及其他传统市场资产进行链上小额交易。</p><p><strong>💸 Yearn Finance 10/29</strong></p><p>最高赏金 200,000 美元 </p><p>Yearn Finance 是 DeFi 领域的一套产品，它在以太坊区块链上提供借贷聚合和收益生成服务。该协议由众多独立开发者维护，并由YFI持有者进行治理。</p><p><strong>💸 Parallel 10/30</strong></p><p>最高赏金 250,000 美元</p><p>Parallel 是一个资本效率高、模块化的稳定币协议，它允许创建超额抵押的去中心化稳定币。该协议由多个不同的模块组成，DAO 可随时间添加或移除这些模块，稳定币可通过这些模块发行或铸造。</p><p><img width="723" height="164" referrerpolicy="no-referrer" src="/img/bVdmKO4" alt="image.png" title="image.png" loading="lazy"/><br/>OpenBuild 是一个面向 Web3 开发人员的开源社区和平台。我们的目标是将更多的 Web2 开发人员带入 Web3 领域，同时帮助现有的 Web3 开发人员更好地构建并通过我们的产品取得商业成功！</p><p>欢迎在更多平台上关注我们：</p><p><a href="https://link.segmentfault.com/?enc=LkQq0ac6GTFQKua6HE95ag%3D%3D.FdsNCexkg05qZh29zD64XelQo%2BCp5SGVJSCaSm2arRo%3D" rel="nofollow" target="_blank">https://linktr.ee/openbuild</a> 🙌🙌</p>]]></description></item><item>    <title><![CDATA[Nexus 中国行成都站圆满收官！百位 ]]></title>    <link>https://segmentfault.com/a/1190000047383551</link>    <guid>https://segmentfault.com/a/1190000047383551</guid>    <pubDate>2025-11-09 20:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmYMF" alt="image.png" title="image.png"/></p><p><strong>Nexus 中国行成都站完美落幕！</strong> 11月1日下午，继北京、上海、深圳三站爆满后，Nexus 中国行成都站在热烈的氛围中圆满结束！</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMG" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>📊 活动数据</strong></h3><p>✅ 150位 Builder 踊跃报名</p><p>✅ 涵盖开发者、创业者、投资人、高校研究者</p><p>✅ Panel Discussion 全程高能</p><p>✅ 现场互动热烈</p><h3><strong>🎯 高能回顾</strong></h3><h4><strong>Nexus 生态进展分享</strong></h4><p>Nexus 团队深入分享了 zkVM 最新技术突破、生态发展路线图，以及开发者激励计划的最新进展。现场开发者对可验证计算在 DeFi 领域的应用展现出极大兴趣。</p><p><img width="723" height="1098" referrerpolicy="no-referrer" src="/img/bVdmYMI" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMH" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdmYMJ" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>Panel Discussion 高光时刻</strong></h4><p>Panel 主题 <strong>"zkVM 赋能 DeFi：从可验证计算到链上金融革新"</strong> ，引发热烈讨论：</p><p>✨ AI 与 DeFi 融合的创新路径</p><p>✨ 全链流动性的技术实现</p><p>✨ 隐私金融与合规的平衡之道</p><p>✨ 开发者生态建设的挑战与机遇<br/><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdmYMK" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>🏆 现场精彩瞬间</strong></h3><p><img width="723" height="548" referrerpolicy="no-referrer" src="/img/bVdmYML" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMM" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMN" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="1099" referrerpolicy="no-referrer" src="/img/bVdmYMO" alt="image.png" title="image.png" loading="lazy"/></p><p>早鸟福利抽奖环节气氛热烈，欢乐谷年卡、Switch 等大奖花落幸运儿</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMP" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMQ" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdmYMR" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMS" alt="image.png" title="image.png" loading="lazy"/></p><p>Coffee Break 环节，开发者、用户之间深度交流，现场 networking 质量很高</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMT" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMU" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMV" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMW" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMX" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdmYMY" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>💬 参会者反馈</strong></p><blockquote><p>成都 Web3 氛围真的太好了！这次活动干货满满，认识了很多志同道合的朋友。</p><p>Nexus 团队的分享很有深度，对我很有启发。</p><p>希望这样的技术交流活动能更多一些！</p></blockquote><p><strong>让我们一起，构建可验证计算的未来！</strong></p><h3><strong>💡 更多 Nexus 了解</strong></h3><p>项目官网：<a href="https://link.segmentfault.com/?enc=Nlotrp2YB6QihyV5UEVLgQ%3D%3D.4%2B05o12lJB9HSkwcfsnYi7g8ZNWs3aYV8Y5zYCMoXDY%3D" rel="nofollow" target="_blank">https://nexus.xyz/</a></p><p>Devnet：<a href="https://link.segmentfault.com/?enc=laWkTeDEGnnDK7xj9pCIKQ%3D%3D.Yc%2B7i4SyvDIzvd7wnglZo9o0A4Xn63DWJR1vWVPqG6Y%3D" rel="nofollow" target="_blank">https://app.nexus.xyz/</a></p><p>Github：<a href="https://link.segmentfault.com/?enc=ciYpc4V6ur%2Fpk9pRX6lJsw%3D%3D.5D1tITJmKKw7rhFv3o15GIUM7Xv%2BTwSn3TJDYZDhGHA%3D" rel="nofollow" target="_blank">https://github.com/nexus-xyz</a> </p><p>Discord：<a href="https://link.segmentfault.com/?enc=BbcTApa%2FSMBxTHf6KNdxjA%3D%3D.2ryfK90TjeTgg%2FVu2%2B2qgXzdWQwYAvAKVCl5TjYGrcWA1CNl1dGXkyxGvN6bTtce" rel="nofollow" target="_blank">https://discord.com/invite/nexus-xyz</a></p><p>官方文档：<a href="https://link.segmentfault.com/?enc=Xs3sj%2FOwTqcYmIycz5DWXA%3D%3D.FvMnqfCxsF3cXtr73B3zgGD0QIrouXmWUlUHm7sI3Mw%3D" rel="nofollow" target="_blank">https://docs.nexus.xyz/home</a></p><p>中文操作指南：<a href="https://link.segmentfault.com/?enc=ABl9yxHJtG1R2ouGs0rDsg%3D%3D.FfscLMKWeq11buyzQAxXm1RjR27zV3HKiQbpIJm9cMk%3D" rel="nofollow" target="_blank">https://www.nexushelp.xyz</a> （中文区大使 Ccool 老师贡献）</p>]]></description></item>  </channel></rss>