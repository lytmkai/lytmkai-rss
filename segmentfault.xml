<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[智能体对传统行业冲击：为什么传统企业更强调“可控性”，而非“更聪明” 你的橙来啦 ]]></title>    <link>https://segmentfault.com/a/1190000047575192</link>    <guid>https://segmentfault.com/a/1190000047575192</guid>    <pubDate>2026-01-27 16:09:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在生成式人工智能向 <strong>AI 智能体（AI Agent）</strong> 演进的过程中，技术社区往往将目标放在更高的自主性、更强的推理能力上。</p><p>但当智能体真正进入 <strong>电力、制造、金融、能源、医药等传统行业</strong> 时，一个反直觉却极其现实的结论浮现出来：</p><blockquote><strong>传统企业并不优先追求“最聪明的智能体”，而是“最可控的智能体”。</strong></blockquote><p>这并非技术保守，而是由 <strong>物理风险、合规压力与业务确定性</strong> 共同决定的理性选择。</p><hr/><h2>一、核心定义：什么是传统行业语境下的“智能体可控性”？</h2><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMeg" alt="" title=""/><br/>在工业与严肃商业环境中，<strong>智能体的可控性（Controllability）</strong> 并不等同于“能不能关掉它”，而是一个系统级概念：</p><blockquote><strong>可控性 = 行为可预测 + 决策可解释 + 异常可接管</strong></blockquote><p>具体可拆解为三个维度：</p><h3>1️⃣ 边界可控（Boundary Control）</h3><ul><li>智能体<strong>能做什么 / 不能做什么</strong>是明确的</li><li>工具权限、数据访问范围、操作级别均被限制</li></ul><h3>2️⃣ 逻辑可控（Logic Transparency）</h3><ul><li>决策过程<strong>可以被复现与审计</strong></li><li>不只是“给结果”，而是能说明<strong>依据了什么规则 / 文档 / 条款</strong></li></ul><h3>3️⃣ 安全可控（Fail-safe Control）</h3><ul><li>在异常输入、极端场景下</li><li>系统可自动降级，或由人工即时接管（Human Override）</li></ul><hr/><h2>二、为什么“可控性”是传统行业的生命线？</h2><p><img width="626" height="404" referrerpolicy="no-referrer" src="/img/bVdnMeh" alt="" title="" loading="lazy"/></p><h3>1️⃣ 容错成本具有极端非对称性</h3><p>在互联网产品中，智能体犯错的代价通常接近于零；<br/> 而在传统行业中，一次错误可能意味着：</p><ul><li>设备损坏</li><li>生产事故</li><li>合规违规</li><li>财务或人身风险</li></ul><p><strong>因此现实选择是：</strong></p><blockquote>智能体更适合作为“决策辅助者”，而非“最终执行者”。</blockquote><p>这也是为什么多数传统企业会<strong>保留人类终审权</strong>。</p><hr/><h3>2️⃣ 合规与审计要求无法妥协</h3><p>金融、医药、能源等行业的共同特点是：</p><ul><li><strong>每一个决策必须可追溯</strong></li><li><strong>每一个结论必须有明确依据</strong></li></ul><p>但大模型天然存在随机性与幻觉风险（Hallucination）。</p><p><strong>因此：</strong></p><blockquote>如果智能体无法解释“为什么这么做”，<br/>那它在合规体系中就是不可用的。</blockquote><hr/><h3>3️⃣ 传统业务偏好“确定性而非创造性”</h3><p>传统企业的竞争力，往往来源于：</p><ul><li>数十年沉淀的 SOP</li><li>高度结构化的业务流程</li></ul><p>他们真正需要的不是“灵光一现”，而是：</p><blockquote>**90% 场景下像老员工一样稳定，<br/>10% 场景下才体现智能。**</blockquote><p>在实践中，一些团队会选择成熟的智能体平台，通过<strong>低代码工作流 + 强规则约束</strong>的方式，让智能体“聪明但不越界”，显著降低落地风险。</p><hr/><h2>三、实践范式：如何构建“可控的智能体系统”？</h2><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMei" alt="" title="" loading="lazy"/><br/>当前行业的共识路径是构建一种：</p><blockquote><strong>“受限自主系统（Constrained Autonomy）”</strong></blockquote><p>核心做法包括：</p><h3>✅ 1. RAG（检索增强生成）</h3><ul><li>将企业私有知识库作为<strong>唯一可信信息源</strong></li><li>限制智能体输出范围，降低幻觉概率</li></ul><h3>✅ 2. 工作流编排（Workflow Orchestration）</h3><ul><li>用 <strong>DAG 工作流</strong> 拆解任务</li><li>每一步都有明确输入、输出与校验规则</li></ul><h3>✅ 3. 人在回路（Human-in-the-Loop）</h3><ul><li>在关键节点设置人工审核断点</li><li>涉及资金、合规、客户沟通时必须人工确认</li></ul><hr/><h2>四、核心结论：可控性不是限制，而是入场券</h2><p>对传统行业而言：</p><ul><li><strong>没有可控性，就没有规模化</strong></li><li><strong>没有审计能力，就没有商业落地</strong></li></ul><blockquote>可控性决定了：<br/>智能体是“实验玩具”，还是“生产工具”。</blockquote><p>本质上，这是一种新的<strong>人机契约关系</strong>：</p><ul><li>人类定义规则与边界</li><li>智能体承诺在规则内高效执行</li></ul><p><strong>未来传统企业的真正竞争力，不在于谁的模型参数更大，而在于谁先构建出一套“可控、可审计、可接管”的智能体体系。</strong><br/>（<strong>本文章由AI辅助生成</strong>）</p>]]></description></item><item>    <title><![CDATA[如何通过 5 种方式将照片从 iPad 传输到电脑 iReaShare ]]></title>    <link>https://segmentfault.com/a/1190000047575341</link>    <guid>https://segmentfault.com/a/1190000047575341</guid>    <pubDate>2026-01-27 16:09:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>您的 iPad 或 iPhone 内部存储空间不足，无法存储照片？或者想备份照片？您可以将照片从 iPad 或 iPhone 传输到电脑。根据您的偏好和可用的工具，共有 5 种方法可以实现此操作。无论您喜欢基于云的解决方案、专用软件，还是简单的拖放操作，您都能找到适合您需求的方法。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575344" alt="图片" title="图片"/></p><p>快速看一下这些方法的优缺点：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575345" alt="图片" title="图片" loading="lazy"/></p><p>第 1 部分：如何通过 iCloud 照片将照片从 iPad/iPhone 传输到计算机？</p><p>iCloud Photos 是 Apple 的云端照片同步服务，让您可以轻松地在所有设备（包括电脑）上访问您的照片。请先确保您的 iCloud 帐户有足够的云存储空间。</p><p>要通过 iCloud Photos 将图片从 iPad 移动到 PC：</p><pre><code>
在您的 iPad 上，请前往“设置”&gt;“ [您的姓名] ”&gt;“ iCloud ”&gt;“照片”。开启“同步此 iPad ”功能（如果尚未启用）。这会将您的 iPad 照片上传到 iCloud。请确保您的 iPad 已连接到 Wi-Fi。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575346" alt="图片" title="图片" loading="lazy"/></p><pre><code>
在 Windows PC 上，从 Apple 网站或 Microsoft Store 下载并安装适用于 Windows 的 iCloud。或者，访问 iCloud Photos 网站并登录您的 Apple 帐户。


单击“照片”选项，将您想要的照片下载到您的电脑。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575347" alt="图片" title="图片" loading="lazy"/></p><p>注意：在 Mac 电脑上，iCloud 照片已内置于 macOS 中。请确保您在 Mac 和 iPad 上使用相同的 Apple ID 登录。只需在 Mac 上打开“照片”应用。前往“照片”&gt;“设置”（或“偏好设置”）&gt;“iCloud”。确保已勾选“iCloud 照片”。您的 iPad 照片将同步到 Mac 的照片图库。然后，您可以根据需要将它们拖放到其他文件夹。</p><p>第 2 部分：如何通过 iReaShare iPhone Manager 将照片从 iPad/iPhone 传输到 PC？</p><p>作为一款一体化 iOS 管理工具， iReaShare iPhone Manager提供了实用的功能来传输数据，包括照片、视频、音乐、联系人、短信等。如果您想将照片从 iPad 或 iPhone 无缝导出到 Windows 或 Mac 电脑，它将满足您的要求。</p><p>iReaShare iPhone Manager的主要功能：</p><ul><li>将图片从 iPad 以无损质量传输到计算机。</li><li>不会改变您的图像格式。</li></ul><p>*将各种文件从 iOS 设备传输到计算机。</p><ul><li>立即将您的 iOS 数据备份到您的计算机。</li><li>将备份数据从您的计算机恢复到您的 iPad 或 iPhone。</li><li>支持 iOS 5.0 及更高版本，包括 iOS 26。</li></ul><p>以下是通过该软件将图片从iPad导出到PC的方法：</p><pre><code>
在电脑上下载并安装 iReaShare iPhone Manager，然后使用 USB 数据线将 iPad 连接到电脑。启动软件。它应该会检测到你的 iPad。


如果出现提示，请点击“信任此电脑”，在 iPad 上授予访问权限。然后即可建立连接。


点击界面上的“照片”部分。接下来，选择要传输的照片，然后点击“导出”图标。然后在电脑上选择一个目标文件夹来保存照片。软件将开始传输过程。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575348" alt="图片" title="图片" loading="lazy"/></p><p>第 3 部分：如何通过照片应用程序将照片从 iPad 或 iPhone 传输到计算机？</p><p>macOS 和 Windows 操作系统上的“照片”应用是管理设备上照片的主要工具，包括从 iPad 直接传输照片。对于 Windows 用户，只要您的电脑运行的是 Windows 10 或更高版本，您就可以使用“照片”应用从 iOS 设备导入照片。</p><p>将 iPad 照片导入 PC：</p><pre><code>
请将 iPad 通过 USB 连接到电脑。然后点击“搜索”，并输入“照片”即可启动照片应用。


点击右上角的“导入”，点击“从USB设备”。


选择你的 iPad，然后选择要传输的图片。选择后，点击“添加 X 个项目”，选择电脑上的文件夹，然后点击“导入”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575349" alt="图片" title="图片" loading="lazy"/></p><p>将 iPad 照片导入 Mac：</p><pre><code>
使用 USB 数据线将 iPad 连接到 Mac。在 Mac 上打开“照片”应用。如果它没有自动打开，请在“应用程序”文件夹或 Dock.p 中找到它。


你的 iPad 应该会出现在照片应用侧栏的“设备”部分下。点击它。如果这是你第一次连接，iPad 可能会询问你是否信任这台电脑。点击“信任”并输入你的密码。


照片应用会显示 iPad 上的所有照片和视频。您可以选择“导入所有新照片”，或选择特定照片，然后点击“导入所选”。导入后，这些照片将出现在 Mac 的照片图库中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575350" alt="图片" title="图片" loading="lazy"/></p><p>提示：您可以轻松地将联系人从iPhone或iPad同步到Mac电脑。如果您需要有用的解决方案，请查看。</p><p>第 4 部分：如何通过 Google Photos 将图片从 iPad 或 iPhone 导出到 PC？</p><p>Google Photos 是一款流行的跨平台云服务，它提供了一种极好的方式，可以将照片从 iPad 或 iPhone 备份并传输到您的计算机，无论使用哪种操作系统。</p><p>将照片从ipad下载到电脑：</p><pre><code>
从 App Store 下载 Google 相册应用并将其安装在 iPad 上。打开 Google 相册应用，然后使用你的 Google 帐户登录。


点击右上角的个人资料图标。前往“照片设置”&gt;“备份和同步”。开启“备份和同步”功能。您的 iOS 照片将开始上传到 Google 相册（请确保您的 Wi-Fi 连接良好）。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575351" alt="图片" title="图片" loading="lazy"/></p><pre><code>
在你的电脑（Windows PC 或 Mac）上，打开网络浏览器并访问 photos.google.com。然后使用你在 iPad 上使用的 Google 帐户登录。


您 iPad 上所有备份的照片都会显示在这里。选择您想要的照片，点击右上角的三个点菜单图标，然后选择“下载”。照片将被下载到您电脑的下载文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575352" alt="图片" title="图片" loading="lazy"/></p><p>第 5 部分：如何通过文件资源管理器将图像从 iPad/iPhone 传输到计算机？</p><p>对于 Windows PC 用户，您的 iOS 设备可以被识别为数码相机，从而可以使用文件资源管理器进行简单的拖放传输。如果您的 PC 运行的是 Windows 7/8 系统，且没有“照片”应用，您可以使用这种方式将照片从 iPad 复制到 PC。</p><p>具体操作如下：</p><pre><code>
使用 USB 数据线将 iPad 连接到 Windows PC，然后在 iPad 上单击“信任”。


在电脑上打开“文件资源管理器”（可以按 Windows + E）。在左侧边栏中，您应该会在“便携式设备”或“设备和驱动器”下看到您的 iPad。它可能会显示为“ Apple iPad ”或类似的名称。


双击 iPad/iPhone 将其打开，然后导航至“内部存储”&gt;“ DCIM ”。在 DCIM 文件夹中，您会找到一个或多个文件夹（例如，100APPLE、101APPLE），其中包含您的照片和视频。


打开这些文件夹查看图片。现在，您可以选择要传输的照片。将选定的照片拖放到电脑上的任何文件夹中，或者复制粘贴到您想要的位置。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575353" alt="图片" title="图片" loading="lazy"/></p><p>第 6 部分：有关将照片从 iPad/iPhone 传输到计算机的常见问题</p><p>问题 1：将照片从 iPad 或 iPhone 传输到电脑的最简单方法是什么？</p><p>本指南中的方法都很简单易用。哪种方法最简单取决于您的需求。如果您想无线传输照片，可以使用 iCloud Photos 和 Google Photos。如果您想要稳定且批量的传输，那么使用 iReaShare iPhone Manager 将是最佳选择。</p><p>问题 2：我可以使用 iTunes 将照片从 iPad/iPhone 传输到 PC 吗？</p><p>不可以，通常情况下，您无法使用 iTunes 将照片从 iPad 传输到 PC。iTunes 主要用于将 PC 上的媒体（音乐、视频等）同步到 iPad 以及备份 iPad。它实际上并没有直接的“将照片从 iOS 设备传输到 PC”功能。</p><p>Q3：我可以使用 AirDrop 在 iPad 和电脑之间传输照片吗？</p><p>是的，你完全可以使用 AirDrop 在 iPad 和 Mac 电脑之间传输照片（以及其他文件）。但如果你的电脑是 Windows 系统，则无法使用 AirDrop 传输图片。</p><p>结论</p><p>无论选择哪种方法，都可以轻松地将照片从 iPad 或 iPhone 传输到电脑。使用iReaShare iPhone Manager可以高效、高质量地传输照片，并轻松管理您的 iPad 数据。无论如何，请考虑您个人对云存储和直接连接存储的偏好，并选择最适合您工作流程的方法。<br/>​</p>]]></description></item><item>    <title><![CDATA[危中有机：国际动荡下，中国电子签章行业的挑战与突围 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047575365</link>    <guid>https://segmentfault.com/a/1190000047575365</guid>    <pubDate>2026-01-27 16:08:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着国际形势动荡的不断发酵，国内市场中的各个行业都受到不同程度的影响，那针对刚刚起步拓展海外市场的各个电子签公司（北京安证通、E签宝等）有没有直接影响呢？我们简单来看看。</p><p>首先，国际动荡对国内电子签章公司确实会产生一系列直接和间接的影响，尽管中国国内市场是其主要根基。具体影响可以从以下几个层面分析：</p><ol><li>负面影响与风险</li></ol><p>1) 供应链与技术依赖</p><p>Ø 若动荡涉及关键技术与硬件（如服务器芯片、加密硬件模块、云计算基础架构）的出口管制或供应链中断，可能影响国内电子签章公司的产品升级与运维。影响最大的便是以数字证书为主，电子签章为辅的各个CA公司（北京CA、CFCA等）</p><p>Ø 若依赖海外开源技术或标准（如密码算法、国际认证体系），可能因制裁或技术脱钩导致合规成本上升。</p><p>2) 跨国业务受阻</p><p>Ø 若公司服务出海企业或境外客户，地缘冲突可能导致跨境数据流动受限（如欧美数据跨境协议变化），增加法律合规复杂性。</p><p>Ø 部分国家可能以“国家安全”为由限制外国数字服务，影响中国电子签章企业的海外拓展。</p><p>3) 经济下行传导</p><p>Ø 国际冲突可能引发全球经济增长放缓，影响国内外贸、投资等领域，进而减少企业数字化转型需求，电子签章作为降本工具可能面临项目延期或预算削减。</p><p>4) 信息安全与自主可控压力</p><p>Ø 国际网络空间对抗加剧可能激发各国对数据主权的要求，国内企业需加速国产密码算法、信创生态适配，短期内增加研发成本。</p><ol start="2"><li>潜在机遇</li></ol><p>1) 国产替代加速</p><p>Ø 国际摩擦可能促使政府与企业更重视供应链安全，推动电子签章在政务、金融、能源等关键领域的国产化替代，利好具备自主技术的公司。</p><p>2) 国内政策支持强化</p><p>Ø 为应对不确定性，国内可能加大数字经济基础设施投入，例如推动“全国统一电子签名互认体系”建设，扩大电子签章在医疗、司法等场景的应用。</p><p>3) 远程与无纸化需求增长</p><p>Ø 国际动荡若导致跨国人员流动受阻、远程办公常态化，可能刺激跨境电子合同、在线公证等需求，为电子签章行业开辟新市场。</p><ol start="3"><li>行业应对策略</li></ol><p>Ø 技术层面：加强国产密码技术（如SM2/SM9）应用，适配信创生态；布局隐私计算等跨域认证技术以应对数据流动壁垒。</p><p>Ø 业务层面：深耕国内市场，聚焦政务、国企、大型制造业等稳健需求；出海时优先选择“一带一路”等政策支持区域，降低地缘风险。</p><p>Ø 合规层面：密切关注跨境数据监管（如中国《数据出境安全评估办法》、欧盟GDPR），构建动态合规体系</p><ol start="4"><li>总结</li></ol><p>国际动荡对国内电子签章行业是 “危中有机” 的复合挑战。就目前国内各个电子签章公司的技术路线和行业案例覆盖情况而言，北京安证通在这波机遇与挑战中的韧性将是最强的，其他电子签章公司还需在技术和应用层面加强自身。</p>]]></description></item><item>    <title><![CDATA[PostgreSQL 18 RETURNING 增强：现代应用的重要进展 IvorySQL ]]></title>    <link>https://segmentfault.com/a/1190000047575370</link>    <guid>https://segmentfault.com/a/1190000047575370</guid>    <pubDate>2026-01-27 16:07:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>PostgreSQL 18 正式发布，带来了多项重要改进，其中 RETURNING 子句的增强尤为突出。该特性在 MERGE RETURNING 场景下实现了关键突破，可显著简化应用架构，并提升数据变更追踪能力。</p><h2>RETURNING 子句的演进</h2><p>RETURNING 子句长期以来用于在 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 操作后返回受影响行的数据，从而避免额外的 SELECT 查询，减少数据库往返次数并提升性能。然而，在 PostgreSQL 18 之前，该子句在功能上存在明显限制，迫使开发实践中采用各种折中方案。</p><p>在 PostgreSQL 17 中，首次为 <code>MERGE</code> 语句引入 RETURNING 支持（提交 <code>c649fa24a</code>），这是一次重要进展。<code>MERGE</code> 语句自 PostgreSQL 15 引入，用于在单条语句中完成条件化的 <code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code> 操作，但在缺乏 RETURNING 支持的情况下，无法直观获取实际执行结果。</p><h2>PostgreSQL 18 的新特性</h2><p>PostgreSQL 18 通过引入 OLD 与 NEW 别名（提交 80feb727c8，由 Dean Rasheed 提交，Jian He 与 Jeff Davis 评审），将 RETURNING 子句能力提升至新的层级。该增强使 DML 操作期间的数据捕获方式发生了根本性变化。</p><h3>PostgreSQL 18 之前的限制</h3><p>在早期版本中，RETURNING 子句在不同语句类型下存在以下差异化限制：</p><ul><li><code>INSERT</code> 与 <code>UPDATE</code> 仅能返回新值或当前值</li><li><code>DELETE</code> 仅能返回旧值</li><li><code>MERGE</code> 根据内部实际执行的操作类型（<code>INSERT</code>、<code>UPDATE</code> 或 <code>DELETE</code>）返回结果</li></ul><p>在需要对比更新前后数据、或精确追踪字段变化时，可选方案较为有限，包括：</p><ul><li>在修改前额外执行 <code>SELECT</code> 查询</li><li>编写复杂的触发器函数</li><li>在应用层实现变更跟踪逻辑</li><li>通过系统列（如 xmax）进行间接判断</li></ul><p>上述方式普遍增加了实现复杂度与访问延迟，并降低了代码可维护性。</p><h3>解决方案：OLD 与 NEW 别名</h3><p>PostgreSQL 18 引入了特殊别名 <code>old</code> 与 <code>new</code>，可在单条语句中同时访问数据的修改前状态与修改后状态。该机制适用于 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 以及 <code>MERGE</code> 等全部 DML 操作。</p><p>基本语法示例如下：</p><pre><code>UPDATE table_name
SET column = new_value
WHERE condition
RETURNING old.column AS old_value, new.column AS new_value;</code></pre><p>为避免与现有列名冲突，或在触发器环境中使用，可对别名进行重命名：</p><pre><code>UPDATE accounts
SET balance = balance - 50
WHERE account_id = 123
RETURNING WITH (OLD AS previous, NEW AS current)
    previous.balance AS old_balance,
    current.balance AS new_balance;</code></pre><h2>MERGE + RETURNING：能力整合</h2><p>在 PostgreSQL 18 中，MERGE 与 RETURNING 的组合为 Upsert 场景提供了完整能力，可在单条原子操作中同时完成数据写入与变更结果获取。</p><h3>实践示例：产品库存系统</h3><p>在产品库存管理场景中，需要从外部数据源同步数据，实现新增产品、更新已有产品，并准确记录每一行的处理结果。</p><p><strong>步骤 1：创建数据表</strong></p><pre><code>CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    product_code VARCHAR(50) UNIQUE NOT NULL,
    product_name VARCHAR(200) NOT NULL,
    price DECIMAL(10, 2) NOT NULL,
    stock_quantity INTEGER NOT NULL DEFAULT 0,
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE product_staging (
    product_code VARCHAR(50),
    product_name VARCHAR(200),
    price DECIMAL(10, 2),
    stock_quantity INTEGER
);</code></pre><p><strong>步骤 2：插入初始数据</strong></p><pre><code>INSERT INTO products (product_code, product_name, price, stock_quantity)
VALUES
    ('LAPTOP-001', 'Premium Laptop', 999.99, 50),
    ('MOUSE-001', 'Wireless Mouse', 29.99, 200),
    ('KEYBOARD-001', 'Mechanical Keyboard', 79.99, 150);

INSERT INTO product_staging (product_code, product_name, price, stock_quantity)
VALUES
    ('LAPTOP-001', 'Premium Laptop Pro', 1099.99, 45),  -- Update existing
    ('MONITOR-001', '4K Monitor', 399.99, 75),          -- New product
    ('MOUSE-001', 'Wireless Mouse', 29.99, 200);        -- No actual change</code></pre><h3>基础版：搭配 RETURNING 子句的 MERGE 操作</h3><pre><code>MERGE INTO products p
USING product_staging s ON p.product_code = s.product_code
WHEN MATCHED THEN
    UPDATE SET
        product_name = s.product_name,
        price = s.price,
        stock_quantity = s.stock_quantity,
        last_updated = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (product_code, product_name, price, stock_quantity)
    VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
RETURNING
    p.product_code,
    p.product_name,
    merge_action() AS action_performed;</code></pre><p>返回结果示例：</p><pre><code> product_code  |    product_name     | action_performed
---------------+---------------------+------------------
 LAPTOP-001    | Premium Laptop Pro  | UPDATE
 MONITOR-001   | 4K Monitor          | INSERT
 MOUSE-001     | Wireless Mouse      | UPDATE</code></pre><h3>进阶版：搭配 OLD 与 NEW 别名的 MERGE 操作</h3><p>通过 OLD 与 NEW 别名，可同时获取字段的修改前与修改后值，从而实现精细化变更追踪与审计。</p><p>以下查询可从受影响行中，同时获取 product_name 与 price 列的修改前旧值和修改后新值。通过为其设置别名（old_name、new_name、old_price、new_price），可便捷对比 MERGE 操作前后的列值变化，为变更追踪与审计日志记录提供支撑。</p><pre><code>MERGE INTO products p
USING product_staging s ON p.product_code = s.product_code
WHEN MATCHED THEN
    UPDATE SET
        product_name = s.product_name,
        price = s.price,
        stock_quantity = s.stock_quantity,
        last_updated = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
    INSERT (product_code, product_name, price, stock_quantity)
    VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
RETURNING
    p.product_code,
    merge_action() AS action,
    old.product_name AS old_name,
    new.product_name AS new_name,
    old.price AS old_price,
    new.price AS new_price,
    old.stock_quantity AS old_stock,
    new.stock_quantity AS new_stock,
    (old.price IS DISTINCT FROM new.price) AS price_changed,
    (old.stock_quantity IS DISTINCT FROM new.stock_quantity) AS stock_changed;</code></pre><p>INSERT 场景下旧值为 NULL，而 UPDATE 场景下可完整呈现字段变更情况。</p><pre><code> product_code  | action | old_name          | new_name            | old_price | new_price | old_stock | new_stock | price_changed | stock_changed
---------------+--------+-------------------+---------------------+-----------+-----------+-----------+-----------+---------------+--------------
 LAPTOP-001    | UPDATE | Premium Laptop    | Premium Laptop Pro  | 999.99    | 1099.99   | 50        | 45        | t             | t
 MONITOR-001   | INSERT | NULL              | 4K Monitor          | NULL      | 399.99    | NULL      | 75        | NULL          | NULL
 MOUSE-001     | UPDATE | Wireless Mouse    | Wireless Mouse      | 29.99     | 29.99     | 200       | 200       | f             | f</code></pre><h3>构建审计日志</h3><p>借助增强后的 RETURNING 子句，可在不使用触发器的前提下构建完整审计链路。</p><p><strong>步骤 1：创建审计表</strong></p><pre><code>CREATE TABLE product_audit (
    audit_id SERIAL PRIMARY KEY,
    product_code VARCHAR(50),
    action VARCHAR(10),
    old_values JSONB,
    new_values JSONB,
    changes JSONB,
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);</code></pre><p><strong>步骤 2：执行带详细审计追踪的 MERGE 操作</strong></p><pre><code>WITH merge_results AS (
    MERGE INTO products p
    USING product_staging s ON p.product_code = s.product_code
    WHEN MATCHED THEN
        UPDATE SET
            product_name = s.product_name,
            price = s.price,
            stock_quantity = s.stock_quantity,
            last_updated = CURRENT_TIMESTAMP
    WHEN NOT MATCHED THEN
        INSERT (product_code, product_name, price, stock_quantity)
        VALUES (s.product_code, s.product_name, s.price, s.stock_quantity)
    RETURNING
        p.product_code,
        merge_action() AS action,
        jsonb_build_object(
            'name', old.product_name,
            'price', old.price,
            'stock', old.stock_quantity
        ) AS old_values,
        jsonb_build_object(
            'name', new.product_name,
            'price', new.price,
            'stock', new.stock_quantity
        ) AS new_values
)
INSERT INTO product_audit (product_code, action, old_values, new_values, changes)
SELECT
    product_code,
    action,
    old_values,
    new_values,
    CASE
        WHEN action = 'INSERT' THEN new_values
        WHEN action = 'DELETE' THEN old_values
        ELSE (
            SELECT jsonb_object_agg(key, value)
            FROM jsonb_each(new_values)
            WHERE value IS DISTINCT FROM old_values-&gt;key
        )
    END AS changes
FROM merge_results;</code></pre><p><strong>步骤 3：查询审计追踪结果</strong></p><pre><code>select * from product_audit;
 audit_id | product_code | action |                          old_values                           |
             new_values                           | changes |         changed_at
----------+--------------+--------+---------------------------------------------------------------+-------------
--------------------------------------------------+---------+----------------------------
        1 | LAPTOP-001   | UPDATE | {"name": "Premium Laptop Pro", "price": 1099.99, "stock": 45} | {"name": "Pr
emium Laptop Pro", "price": 1099.99, "stock": 45} |         | 2025-12-12 16:27:14.760125
        2 | MONITOR-001  | UPDATE | {"name": "4K Monitor", "price": 399.99, "stock": 75}          | {"name": "4K
 Monitor", "price": 399.99, "stock": 75}          |         | 2025-12-12 16:27:14.760125
        3 | MOUSE-001    | UPDATE | {"name": "Wireless Mouse", "price": 29.99, "stock": 200}      | {"name": "Wi
reless Mouse", "price": 29.99, "stock": 200}      |         | 2025-12-12 16:27:14.760125
(3 rows)</code></pre><p>示例中通过 CTE 获取 MERGE 结果，并将旧值、新值及差异以 JSONB 形式写入审计表，实现单条原子操作内的数据同步与审计记录生成。</p><h2>未来展望</h2><p>PostgreSQL 18 版本的 RETURNING 子句增强特性，是该数据库提升开发友好性、减少复杂替代方案使用的重要举措。单原子操作中同时调用数据新旧值的能力，可简化应用开发中的多种通用实现模式。</p><p>该功能在后续版本中或可从以下方向进一步升级：</p><ol><li>扩展 MERGE 语句能力，新增更多 WHEN 子句，实现更复杂的条件操作.</li><li>新增聚合功能支持，支持对 RETURNING 子句的返回结果直接进行聚合计算。</li><li>实现跨表返回，支持在单操作中返回关联表的数据信息。</li></ol><h2>技术细节与提交记录参考</h2><p>针对关注技术实现细节的人员，可参考以下信息：</p><ul><li><strong>MERGE RETURNING</strong>（PostgreSQL 17）：迪恩・拉希德提交，记录编号 <code>c649fa24a</code>。</li><li><strong>OLD/NEW Support</strong>（PostgreSQL 18）：迪恩・拉希德提交，何健与杰夫・戴维斯评审，记录编号 <code>80feb727c8</code>。</li><li><a href="https://link.segmentfault.com/?enc=xUH4du746xSi%2F69jexxqIg%3D%3D.6330041%2Bc657m%2FvPuMhxPdnIytvop8ibDPhK3dV37jNhOpzi0B7iSIo6c0foxlhiUuBVi3iDjjjNsmHC%2FZEHfiK6QpD2CZ8bzEAUcmvxmQfKgWqxakiLpd6MpMhp4pN%2B" rel="nofollow" target="_blank"><strong>Discussion Thread</strong></a>：<a href="https://link.segmentfault.com/?enc=NP8SnQkvz0vsBqOzVJLECQ%3D%3D.rb0yAlt24GBKgKmW100zKakE80kcUemfT4XDodQDprUwcQgMc6kHXOb35UE3jo6%2FP8ftywL2tdhiWcS%2FgyhmSn5%2FGQ%2F7bHF5Yrf4jXsHP1bAqwBo7otwo7g2y473ZEx9" rel="nofollow" target="_blank">https://postgr.es/m/CAEZATCWx0J0-v=Qjc6gXzR=KtsdvAE7Ow=D=mu50...</a></li></ul><p>该功能的实现涉及多个组件的修改，包括：</p><ul><li>执行器（execExpr.c、execExprInterp.c、nodeModifyTable.c）</li><li>解析器（parse_target.c）</li><li>优化器（createplan.c、setrefs.c、subselect.c）</li><li>节点模块（makefuncs.c、nodeFuncs.c）</li></ul><h2>总结</h2><p>PostgreSQL 18 对 RETURNING 子句的增强，尤其是 OLD 与 NEW 别名的引入，为 <code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code> 与 <code>MERGE</code> 操作提供了完整的数据变更可视性。这一能力显著减少了对触发器与额外查询的依赖，使数据同步、变更追踪与审计实现更加简洁、高效且易于维护。</p><p><code>MERGE</code> 与增强型 RETURNING 的结合，为 Upsert 场景提供了前所未有的控制能力与透明度，是 PostgreSQL 在开发友好性与工程实用性方面的重要进展。</p><p>原文链接：</p><p><a href="https://link.segmentfault.com/?enc=Pir6oMoDMd7tte0NYOdnaw%3D%3D.kyIen2tsGS5LxWwyaNprlAEw3UMbxLZIER7HMj5xACJHaujxlGX24VnWJkbjs%2BUT4P3HPMJdftK6NjuLopTOdwtvbivHOpHoSP%2FUfAHluMOFln4QbbMF0bcUwFtqzkg0x23dt6EGEhqgxMWJ3CwOPg%3D%3D" rel="nofollow" target="_blank">https://www.pgedge.com/blog/postgresql-18-returning-enhanceme...</a></p><p>作者：Ahsan Hadi</p><hr/><h2><a href="https://link.segmentfault.com/?enc=VUVGKIVeZeprikqWa%2BKXQw%3D%3D.t6Qs5ydlFrfM1gHbhgUTD52z3aJIdTVI%2BAGHpbcoNuk%3D" rel="nofollow" target="_blank">HOW 2026 议题招募中</a></h2><p>2026 年 4 月 27-28 日，由 IvorySQL 社区联合 PGEU（欧洲 PG 社区）、PGAsia（亚洲 PG 社区）共同打造的 HOW 2026（IvorySQL &amp; PostgreSQL 技术峰会） 将再度落地济南。届时，PostgreSQL 联合创始人 Bruce Momjian 等顶级大师将亲临现场。</p><p>自开启征集以来，HOW 2026 筹备组已感受到来自全球 PostgreSQL 爱好者的澎湃热情。为了确保大会议题的深度与广度，我们诚邀您在 2026 年 2 月 27 日截止日期前，提交您的技术见解。</p><p>投递链接：<a href="https://link.segmentfault.com/?enc=X0QwZAujMNioCEl1ok530g%3D%3D.L%2BMnYddUJo%2BlEgHpCOO8Jhzbrbm%2FGQIBDsIBhqNn0P4%3D" rel="nofollow" target="_blank">https://jsj.top/f/uebqBc</a></p>]]></description></item><item>    <title><![CDATA[详解 RNN 循环机制：短序列优势、长依赖问题及 LSTM 解决方案 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047575379</link>    <guid>https://segmentfault.com/a/1190000047575379</guid>    <pubDate>2026-01-27 16:07:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>RNN 简介</h2><p>RNN（Recurrent Neural Network，循环神经网络）一般以序列数据为输入，通过网络内部的结构设计有效捕捉序列之间的关系特征，一般也以序列形式输出。</p><p>RNN 的循环机制使模型隐层上一时间步产生的结果，能够作为当下时间步输入的一部分（当下时间步的输入除了正常的输入外还包括上一步的隐层输出）对当下时间步的输出产生影响。</p><ul><li>结构：三层（输入层、隐藏层、输出层；循环发生在隐藏层）<br/><img width="723" height="265" referrerpolicy="no-referrer" src="/img/bVdnMGW" alt="" title=""/></li></ul><h3>1.1 RNN 模型的作用</h3><p>因为 RNN 结构能够很好利用序列之间的关系，因此针对自然界具有连续性的输入序列，如人类的语言、语音等进行很好处理，广泛应用于 NLP（自然语言处理）领域的各项任务，如文本分类、情感分析、意图识别、机器翻译等。</p><p>语言处理示例</p><p><img width="723" height="328" referrerpolicy="no-referrer" src="/img/bVdnMGX" alt="" title="" loading="lazy"/></p><h4>2.1 PyTorch 中传统 RNN 的使用</h4><p>位置：在 <code>torch.nn</code> 中，通过 <code>torch.nn.RNN</code> 可调用。</p><pre><code class="python">import torch
import torch.nn as nn

rnn = nn.RNN(5, 6, 2)  # 实例化 rnn 对象
# 参数1：输入张量 x 的维度 - input_size
# 参数2：隐藏层的维度（隐藏层神经元个数）- hidden_size
# 参数3：隐藏层的层数 - num_layers

# torch.randn - 随机产生正态分布的随机数
input1 = torch.randn(1, 3, 5)  # 设定输入张量 x - 序列长 1，批次 3，维度 5
# 参数1：输入序列长度 - sequence_length
# 参数2：批次的样本 - batch_size（表示：3 个样本）
# 参数3：输入张量 x 的维度 - input_size

h0 = torch.randn(2, 3, 6)  # 设定初始化的 h0
# 第一个参数：num_layers * num_directions（层数 * 网络方向数（1 或 2））
# 第二个参数：batch_size（批次的样本数）
# 第三个参数：hidden_size（隐藏层的维度）

output, hn = rnn(input1, h0)
# 最后输出和最后一层的隐藏层输出

print(output)
print(output.shape)
print(hn)
print(hn.shape)</code></pre><h4>1.2 RNN的局限：长期依赖（Long-TermDependencies）问题</h4><p>RNN的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果RNN可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。</p><p>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测这句话中“the clouds are in the sky”最后的这个词“sky”，我们并不再需要其他的信息，因为很显然下一个词应该是sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN可以学会使用先前的信息。</p><h4>1.2 传统 RNN 优缺点</h4><ul><li>优势：内部结构简单，对计算资源要求低；相较 LSTM/GRU 参数总量更少；在短序列任务上性能与效果表现优异。</li><li>缺点：在长序列关联上表现较差；反向传播时易发生梯度消失或爆炸。</li></ul><p>NaN 值（Not a Number，非数）：是计算机科学中数值数据类型的一类值，表示未定义或不可表示的值。</p><h3>2.1 LSTM 模型简介</h3><p>Long ShortTerm 网络——一般就叫做LSTM——是一种RNN特殊的类型，可以学习长期依赖信息。当然，LSTM和基线RNN并没有特别大的结构不同，但是它们用了不同的函数来计算隐状态。</p><p>LSTM的“记忆”我们叫做细胞/cells，你可以直接把它们想做黑盒，这个黑盒的输入为前状态和当前输入。这些“细胞”会决定哪些之前的信息和状态需要保留/记住，而哪些要被抹去。实际的应用中发现，这种方式可以有效地保存很长时间之前的关联信息。<br/><img width="723" height="625" referrerpolicy="no-referrer" src="/img/bVdnMGY" alt="" title="" loading="lazy"/></p><h4>2.2 PyTorch 中 LSTM 的使用</h4><pre><code class="python">import torch
import torch.nn as nn

lstm = nn.LSTM(5, 6, 2)  # 实例化 lstm 对象
# 参数1：输入张量 x 的维度 - input_size
# 参数2：隐藏层的维度（隐藏层神经元个数）- hidden_size
# 参数3：隐藏层的层数 - num_layers

input1 = torch.randn(1, 3, 5)  # 设定输入张量 x - 序列长 1，批次 3，维度 5
# 参数1：输入序列长度 - sequence_length
# 参数2：批次的样本 - batch_size
# 参数3：输入张量 x 的维度 - input_size

h0 = torch.randn(2, 3, 6)  # 设定初始化的 h0（隐藏层）
c0 = torch.randn(2, 3, 6)  # 设定初始化的 c0（细胞状态）
# 第一个参数：num_layers * num_directions（层数 * 网络方向数（1 或 2））
# 第二个参数：batch_size（批次的样本数）
# 第三个参数：hidden_size（隐藏层的维度）

output, (hn, cn) = lstm(input1, (h0, c0))
# 最后输出和最后一层的隐藏层输出

print(output)
print(output.shape)
print(hn)
print(hn.shape)
print(cn)
print(cn.shape)</code></pre>]]></description></item><item>    <title><![CDATA[SPSS与Python用Resblock优化BP神经网络分析慢性胃炎病历数据聚类K-means/AG]]></title>    <link>https://segmentfault.com/a/1190000047575390</link>    <guid>https://segmentfault.com/a/1190000047575390</guid>    <pubDate>2026-01-27 16:06:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=snEbQiXSzIpe5yaEL1uwaA%3D%3D.iYudxoASDRu1Ga4KR0qODoO1OPC%2BwwSIVLE1xAQbIvg%3D" rel="nofollow" title="https://tecdat.cn/?p=44893" target="_blank">https://tecdat.cn/?p=44893</a>  <br/>原文出处：拓端数据部落公众号  <br/><strong>关于分析师</strong>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575392" alt="" title=""/>  <br/>在此对Chang He对本文所作的贡献表示诚挚感谢，他在中国中医科学院完成了中医信息学专业的硕士学位，专注中医临床数据挖掘领域。擅长Python、深度学习、临床数据采集与挖掘。Chang He曾参与多项中医临床数据研究项目，聚焦慢性胃炎等常见消化类疾病的中药配伍规律挖掘，通过数据技术赋能传统中医用药研究，积累了丰富的临床数据处理与模型构建经验。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575393" alt="" title="" loading="lazy"/></p><h3><a name="t1" target="_blank"/>专题名称：慢性胃炎中药用药规律数据挖掘与AI预测实践</h3><h4><a name="t2" target="_blank"/>引言</h4><p>中医治疗慢性胃炎注重辨证施治与中药配伍，传统用药经验多依赖医师传承，难以快速提炼普适性规律并实现精准指导。随着大数据与人工智能技术的发展，通过数据挖掘解析病历中的中药配伍逻辑，结合神经网络构建用药预测模型，成为赋能中医临床诊疗的重要方向。本文围绕慢性胃炎住院病历数据，整合多种数据分析方法与AI模型，系统探索中药使用规律与用药预测路径，为临床合理用药提供数据支撑。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，<strong>该项目完整代码与数据已</strong>分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂 怎么做，也懂 为什么这么做；遇代码运行问题，更能享24小时调试支持。  <br/>本研究以两家医疗机构的慢性胃炎住院病历为核心数据，采用人工、VBA宏与大语言模型结合的方式提取并规范数据，通过SPSS系列工具与Python库实现频数分析、聚类分析、关联规则挖掘，同时构建含Resblock模块的神经网络模型，实现基于临床症状的中药预测。全文将先梳理数据处理与分析流程，再逐一呈现各环节结果，最后总结方法适用性与实际应用价值，同步配套核心代码供落地复用，兼顾理论性与实操性。</p><h4><a name="t3" target="_blank"/>项目文件目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575394" alt="" title="" loading="lazy"/></p><h3><a name="t5" target="_blank"/>研究方法与技术准备</h3><h4><a name="t6" target="_blank"/>数据来源与处理</h4><p>本研究选取两家医疗机构的慢性胃炎住院病历作为研究对象，其中一家机构数据时间范围为2016年1月至2024年5月，聚焦中药配伍规律挖掘；另一家机构数据时间范围为2013年1月至2021年10月，用于神经网络模型构建，数据集含2214个样本、364种临床特征及469种中药。  <br/>数据提取采用人工、VBA宏与大语言模型协同模式，既保障人工校验的准确性，又通过工具提升效率。数据规范化依据《中药学》新世纪版标准，统一中药名称、剂量等关键信息，为后续分析奠定基础。</p><h4><a name="t7" target="_blank"/>核心工具与方法说明</h4><ol><li>分析工具：SPSS Modeler 18.0、SPSS Statistic 26.0、Python 3.11.5（Sklearn、Scipy、Pytorch 2.0.1模块），上述工具国内均可正常访问使用，无替代需求，其中Python相关模块可通过镜像源快速安装。</li><li>分析方法：频数分布分析（提炼高频中药与临床特征）、聚类分析（K-means、AGNES，对比不同距离与连接法适用性）、关联规则挖掘（挖掘中药联用规律）、BP神经网络（含Resblock模块，优化症状到中药的预测精度）。</li></ol><h4><a name="t8" target="_blank"/>核心代码适配与说明（数据提取环节）</h4><p>以下代码用于中药名称提取与数据清洗，优化变量名与语法结构，适配中文文本处理需求，省略部分重复数据校验代码：</p><pre><code>import pandas as pdimport re# 读取Excel格式的病历数据文件input_excel = '病历数据.xlsx' # 替换为实际数据文件路径data_df = pd.read_excel(input_excel)# 定义汉字提取函数，过滤非中文内容（保留中药名称）def get_chinese_content(text): # 正则表达式匹配中文汉字范围 chinese_characters = ''.join(re.findall(r'[\u4e00-\u9fff]+', str(text))) return chinese_characters# 对中药名称列应用提取函数，清洗数据data_df['中药名称'] = data_df['中药名称'].astype(str).apply(get_chinese_content)# 保存清洗后的数据至新文件output_excel = '清洗后病历数据.xlsx'data_df.to_excel(output_excel, index=False, engine='openpyxl')print(f"数据清洗完成，结果已保存至 {output_excel}")</code></pre><p>代码功能：针对病历数据中的中药名称列进行清洗，提取纯中文内容，剔除符号、数字等干扰项，保障后续分析数据的规范性。省略部分为数据去重、空值填充逻辑，可根据实际数据质量补充。</p><h3><a name="t9" target="_blank"/>研究结果与分析</h3><h4><a name="t10" target="_blank"/>频数分析结果</h4><p>本次分析共涉及281种中药、7375个用药实例，平均每张处方开具15种中药。其中甘草使用频次最高，达341次，占比71.49%，平均剂量7.8g；黄精、升麻等51种中药仅使用1次，频次最低。  <br/>频次排名前20的中药如下表所示，高频中药多集中在理气、健脾、清热类别，符合慢性胃炎脾胃失调、气滞热蕴的常见病机。  <br/>表4 药物频次统计前20位</p><table><thead><tr><th>中药</th><th>频次</th><th>占比（%）</th></tr></thead><tbody><tr><td>甘草</td><td>341</td><td>71.49%</td></tr><tr><td>陈皮</td><td>280</td><td>58.70%</td></tr><tr><td>半夏</td><td>272</td><td>57.02%</td></tr><tr><td>白芍</td><td>237</td><td>49.69%</td></tr><tr><td>柴胡</td><td>236</td><td>49.48%</td></tr><tr><td>白术</td><td>222</td><td>46.54%</td></tr><tr><td>黄连</td><td>216</td><td>45.28%</td></tr><tr><td>茯苓</td><td>198</td><td>41.51%</td></tr><tr><td>枳实</td><td>183</td><td>38.36%</td></tr><tr><td>延胡索</td><td>183</td><td>38.36%</td></tr><tr><td>砂仁</td><td>179</td><td>37.53%</td></tr><tr><td>党参</td><td>173</td><td>36.27%</td></tr><tr><td>香附</td><td>155</td><td>32.49%</td></tr><tr><td>黄芩</td><td>142</td><td>29.77%</td></tr><tr><td>厚朴</td><td>135</td><td>28.30%</td></tr><tr><td>丹参</td><td>125</td><td>26.21%</td></tr><tr><td>紫苏梗</td><td>121</td><td>25.37%</td></tr><tr><td>当归</td><td>120</td><td>25.16%</td></tr><tr><td>海螵蛸</td><td>107</td><td>22.43%</td></tr><tr><td>干姜</td><td>102</td><td>21.38%</td></tr></tbody></table><h3><a name="t11" target="_blank"/>中药频次分布如下图所示，呈现明显的长尾分布特征，少数中药在临床中广泛应用，多数中药针对性使用。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575395" alt="" title="" loading="lazy"/></h3><hr/><p><strong>相关文章</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575396" alt="" title="" loading="lazy"/></p><h3><a name="t12" target="_blank"/>Python预测二型糖尿病：逻辑回归、XGBoost、CNN、随机森林及BP神经网络融合加权线性回归细化变量及PCA降维创新</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=lTKTe%2BjuEct14sS3lT8n4A%3D%3D.b4%2F8KAJFQgOXLTSD1b2pxLL3UK31XHqQQHziiezi6j8%3D" rel="nofollow" title="https://tecdat.cn/?p=43572" target="_blank">https://tecdat.cn/?p=43572</a></p><hr/><h4><a name="t13" target="_blank"/>聚类分析结果</h4><p>聚类分析核心目标是挖掘中药联用的内在规律，对比K-means与AGNES两种聚类方法，结合不同距离计算方式与连接法，从轮廓系数、临床可解释性等维度评估适用性。</p><h5>K-means聚类</h5><p>簇数设置为1-20时，通过WSS图（组内平方和）观察簇数适配性，拐点虽不明显，但簇数为2、3、5、9时WSS下降趋势变缓，簇数适中。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575397" alt="" title="" loading="lazy"/>  <br/>表5 不同簇数的K-means聚类平均轮廓系数</p><table><thead><tr><th>簇数量</th><th>簇样本量</th><th>平均轮廓系数</th></tr></thead><tbody><tr><td>2</td><td>12，29</td><td>0.1490</td></tr><tr><td>3</td><td>5，30，6</td><td>0.1252</td></tr><tr><td>5</td><td>3，24，9，2，3</td><td>0.0914</td></tr><tr><td>9</td><td>4，6，14，2，2，2，8，2，1</td><td>0.0581</td></tr></tbody></table><p>当簇数设为9时，各簇样本轮廓系数表现较好，通过PCA降维可视化聚类结果如下：  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047575398" alt="" title="" loading="lazy"/>  <br/>K-means聚类结果临床可解释性较强，平均评分4.67分，仅簇2可解释性较低（2分）。各簇对应不同病机的用药方案，如簇0含延胡索、砂仁等，与香砂六君子汤核心组分契合，适配脾气虚兼气滞证；簇1含黄芩、干姜等，对应气血阳虚、湿热蕴结的复杂病机。  <br/>表6 K-means聚类结果</p><table><thead><tr><th>簇名</th><th>中药</th><th>可解释性评分</th></tr></thead><tbody><tr><td>0</td><td>延胡索，砂仁，党参，木香</td><td>5</td></tr><tr><td>1</td><td>黄芩，干姜，桂枝，黄芪，生姜，大枣</td><td>5</td></tr><tr><td>2</td><td>黄连，枳实，厚朴，海螵蛸，六神曲，吴茱萸，佩兰，竹茹，苍术，浙贝母，瓜蒌，白及，鸡内金，麦芽（14味）</td><td>2</td></tr><tr><td>3</td><td>香附，紫苏梗</td><td>5</td></tr><tr><td>4</td><td>白芍，柴胡</td><td>5</td></tr><tr><td>5</td><td>陈皮，半夏</td><td>5</td></tr><tr><td>6</td><td>丹参、当归、川芎、枳壳、百合、乌药、豆蔻、酸枣仁（8味）</td><td>5</td></tr><tr><td>7</td><td>白术、茯苓</td><td>5</td></tr><tr><td>8</td><td>甘草</td><td>5</td></tr></tbody></table><h5>AGNES聚类（不同连接法对比）</h5><ol><li>欧氏距离+最长距离法：簇数设为9时，平均轮廓系数0.0803，临床可解释性评分4.11分，部分簇中药组合对应明确诊疗需求，如簇0含香附、紫苏梗等，侧重理气活血。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575399" alt="" title="" loading="lazy"/></li><li>欧氏距离+最短距离法：簇数设为12时，平均轮廓系数0.0637，但临床可解释性仅1.33分，多数簇仅含单味药，难以提炼联用规律。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575400" alt="" title="" loading="lazy"/></li><li>欧氏距离+组间平均连接法：簇数设为12时，平均轮廓系数0.0901，临床可解释性3分，兼顾聚类效果与规律提取，如簇1（枳实、厚朴）、簇2（白芍、柴胡）均为临床常用配伍。<img referrerpolicy="no-referrer" src="/img/remote/1460000047575401" alt="" title="" loading="lazy"/></li></ol><h4><a name="t14" target="_blank"/>聚类分析核心代码（AGNES方法）</h4><p>以下代码优化变量名与注释，适配聚类分析需求，省略部分图表美化与结果导出代码，同时提供24小时应急修复服务，代码运行异常可快速响应，效率较自行调试提升40%：</p><pre><code>import numpy as npimport matplotlib.pyplot as pltfrom sklearn.cluster import AgglomerativeClusteringfrom scipy.cluster.hierarchy import dendrogram, linkagefrom sklearn.metrics import silhouette_scoreimport pandas as pd# 读取预处理后的中药数据data_path = '中药数据.xlsx'df = pd.read_excel(data_path, usecols="A:RJ", nrows=41)labels = df.iloc[:, 0].values # 提取样本标签（中药名称）data = df.iloc[:, 1:].to_numpy() # 提取特征数据cluster_num = 12 # 设定簇数try: print(f"开始聚类分析，簇数设置为 {cluster_num}") # 初始化AGNES聚类器，欧氏距离+组间平均连接法 agnes_cluster = AgglomerativeClustering(n_clusters=cluster_num, affinity='euclidean', linkage='average') cluster_results = agnes_cluster.fit_predict(data)# 计算平均轮廓系数，评估聚类效果 avg_silhouette = silhouette_score(data, cluster_results, metric='euclidean') print(f"簇数{cluster_num}时，平均轮廓系数：{avg_silhouette}")# 绘制树状图 linked_matrix = linkage(data, method='average', metric='euclidean') plt.figure(figsize=(12, 6)) dendrogram(linked_matrix, orientation='top', labels=labels, show_leaf_counts=True) plt.title('层次聚类树状图') plt.xlabel('样本标签') plt.ylabel('距离阈值') plt.show() ... # 省略轮廓系数分布图绘制与结果保存代码except Exception as e: print(f"聚类分析过程中出现异常：{e}")</code></pre><h4><a name="t15" target="_blank"/>关联规则挖掘结果</h4><p>设置最小前项支持度0.1、最小置信度0.8，共得到451条关联规则，最高项数6项，其中项数4的规则最多（210条），项数2的规则最少（10条）。规则支持度与置信度前10名的关联规则临床可解释性均为满分，契合中医用药理论。  <br/>支持度前5的关联规则中，“党参→甘草”支持度最高（29.560%），二者为临床健脾益气常用配伍；“茯苓、陈皮→半夏”支持度25.367%，对应痰湿内阻型慢性胃炎的用药方案。  <br/>置信度前5的关联规则中，“吴茱萸、陈皮→黄连”置信度达98.276%，吴茱萸温肝暖胃，黄连清热燥湿，二者配伍符合寒热错杂证的诊疗逻辑；“延胡索、茯苓、半夏→陈皮”置信度98.077%，体现理气止痛、健脾化痰的联用思路。</p><h4><a name="t16" target="_blank"/>神经网络构建与结果</h4><h5>模型设计</h5><p>基于临床特征预测中药使用，构建含2个Resblock模块与1个全连接层的BP神经网络，Resblock模块通过跳跃连接缓解梯度消失问题，提升模型训练效果。模型输入为364种临床特征，输出为469种中药的预测概率，Resblock输出采用Leaky ReLU激活函数，最终输出采用Sigmoid激活函数，适配多标签分类需求。</p><h5>特征与标签选择</h5><p>临床特征频次前3位为烧心（63.69%）、口干（61.92%）、夜寐欠安（61.34%），均为慢性胃炎常见症状；中药标签选取覆盖高、中、低频药物，共12种，验证不同频次药物的预测效果。</p><h5>模型结果与评估</h5><p>采用二折交叉验证评估模型性能，F1值为43.54%，多数标签F1值波动幅度控制在0.017以内，模型稳定性较强。其中“黄芩”“陈皮、柴胡”等标签F1值超过50%，预测效果较好；“佩兰、黄芩”标签预测稳定性较差，可能与该组合临床应用场景差异较大有关。  <br/>高频药物黄芩预测F1值最高（53.42%），特征明确易被模型捕捉；白芍虽为高频药物，但召回率仅0.0799，呈现“高精低召”特征，提示其应用场景多样性导致模型难以全面识别；低频药物（占比&lt;1%）因样本量极少，模型多预测为阴性，F1值无法计算，需通过数据扩充优化。</p><h3><a name="t17" target="_blank"/>总结与应用建议</h3><p>本研究通过多种数据分析方法与AI模型，系统挖掘了慢性胃炎中药用药规律，构建了症状到中药的预测模型，核心结论与建议如下：</p><ol><li>用药规律：甘草、陈皮、半夏等为慢性胃炎核心用药，多以理气、健脾、清热类中药联用为主，关联规则挖掘出的高频组合可作为临床用药参考。</li><li>方法适配：K-means聚类在临床可解释性上优于AGNES，欧氏距离+组间平均连接法可作为AGNES聚类的优选参数，为同类研究提供方法借鉴。</li><li>模型优化：Resblock优化的BP神经网络可实现中药预测，但需针对低频药物扩充样本，优化标签设计，提升模型泛化能力。</li><li>临床应用：研究结果可辅助医师快速制定用药方案，尤其为年轻医师提供配伍参考，同时模型可作为中医用药教学的辅助工具。  <br/>  本研究所有代码与数据已同步至交流社群，提供人工答疑与24小时代码调试服务，助力临床数据挖掘爱好者快速落地实践。后续可结合更多医疗机构数据，优化模型参数，进一步提升结果的临床适配性。</li></ol>]]></description></item><item>    <title><![CDATA[用 Python 接入美股实时行情流：从数据延迟到系统化接入的实践 sydney ]]></title>    <link>https://segmentfault.com/a/1190000047575411</link>    <guid>https://segmentfault.com/a/1190000047575411</guid>    <pubDate>2026-01-27 16:05:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果你正在自己搭建交易或行情系统，大概率遇到过类似的情况：<br/>你能拿到行情，但总觉得延迟不可控；<br/>接口能用，但一旦标的数量上来，系统开始变得不稳定。<br/>当你从“看行情”转向“用行情”，数据接入方式本身就会成为系统瓶颈。</p><h3>从使用场景看实时行情的真实需求</h3><p>在个人专业交易或高频策略场景中，你对行情数据的要求通常包括：</p><ul><li>数据能够持续推送，而不是频繁请求</li><li>支持多标的同时订阅</li><li>延迟与推送频率可预期</li><li><p>数据结构清晰，可直接进入策略或缓存层<br/>这类需求，本质上已经超出了传统 HTTP 轮询的适用范围。</p><h3>实时行情的常见工程痛点</h3><p>很多系统在早期阶段看起来“能跑”，但随着负载上升，问题会逐渐显现：</p></li><li>高频轮询带来不必要的连接与资源消耗</li><li>多标的管理复杂，订阅逻辑难以维护</li><li>数据字段不统一，解析成本增加</li><li><p>延迟不稳定，影响策略执行一致性<br/>这些问题并不来自策略，而是行情接入模型本身不合理。</p><h3>用数据流的方式理解实时行情</h3><p>在工程上，更合理的思路是把实时行情视为一条持续的数据流。<br/>WebSocket 的核心优势在于：</p></li><li>连接建立一次，长期保持</li><li>服务端主动推送数据</li><li><p>天然适合多标的订阅与高频更新<br/>在这种模型下，行情 API 更像是数据源，而你的系统只是负责接收、分发和消费数据。</p><h3>选择实时行情 API 时应关注什么</h3><p>在真正接入之前，你可以从以下几个关键点快速判断一个 API 是否适合生产系统：</p></li><li>WebSocket 连接方式与鉴权是否清晰</li><li>订阅指令是否支持批量标的</li><li>推送频率与数据粒度是否明确</li><li><p>返回数据结构是否稳定、规范<br/>这些因素决定了接口能否在系统中长期、稳定运行，而不仅仅是“能连上”。</p><h3>Python 接入示例</h3><p>下面给你一份 Python 示例，它展示了典型的 WebSocket 接入流程：</p></li></ul><pre><code>import websocket
import json

def on_message(ws, message):
    data = json.loads(message)
    # 实时行情高频，先打印结构
    print(data)

def on_open(ws):
    subscribe_msg = {
        "cmd": "subscribe",
        "args": ["US.AAPL"]
    }
    ws.send(json.dumps(subscribe_msg))

def on_error(ws, error):
    print("error:", error)

def on_close(ws):
    print("connection closed")

ws = websocket.WebSocketApp(
    "wss://stream.alltick.co/ws",
    on_open=on_open,
    on_message=on_message,
    on_error=on_error,
    on_close=on_close
)

ws.run_forever()
</code></pre><h3>数据结构比价格本身更重要</h3><p>当你把实时行情真正接入系统后，会发现一个有趣的现象：<br/>最先带来安全感的，往往不是价格变化，而是数据结构的整洁程度。<br/>常见的实时行情字段通常包括：</p><ul><li>标的标识（symbol）</li><li>毫秒级时间戳</li><li>最新成交价与成交量</li><li><p>买卖报价（bid / ask）<br/>结构清晰的数据可以直接进入策略模块、内存缓存，或作为统一行情源提供给下游服务，几乎不需要额外加工。<br/>AllTick的美股实时行情 API 在接口设计上就偏向这种工程友好型结构，无论是多标的订阅还是持续推送，都更容易融入现有系统。</p><h3>实时行情在系统中的典型流向</h3><p>在一个相对完整的交易系统中，实时行情数据通常会被：</p></li><li>推送给策略引擎进行实时计算</li><li>写入缓存，用于低延迟查询</li><li><p>转发给其他服务，作为统一行情入口<br/>当接入方式合理时，行情数据会在系统中自然流动，而不是成为需要频繁“救火”的模块。</p><h3>总结</h3><p>如果你正在设计或重构行情接入层，建议优先从系统视角思考：</p></li><li>数据是否以“流”的方式进入系统</li><li>接口是否足够稳定，能长期运行</li><li>数据结构是否能直接服务于策略与缓存<br/>当这些问题被解决后，技术实现反而会变得简单。<br/>行情每天都在变化，但一个设计合理的实时行情接口，往往能让整个系统保持长期稳定。这也是 WebSocket 美股实时 API 在交易系统中被广泛采用的原因。<br/><img width="723" height="370" referrerpolicy="no-referrer" src="/img/bVdnMG8" alt="" title=""/></li></ul>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-用数据重构 ITSM 决策与运营模式 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575431</link>    <guid>https://segmentfault.com/a/1190000047575431</guid>    <pubDate>2026-01-27 16:04:49</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>过去几年里，越来越多的组织已经上线 IT 服务台、 建立 ITSM系统，并以 ITIL 为参考去规范事件、问题、变更与请求管理。 但当系统数量、云资源、终端设备与跨部门协作一并增长时，团队会逐渐遇到同一种“管理瓶颈”：我们能看到很多工单、很多告警、很多流程记录，却很难把它们串成一条可解释的因果链。 这也是为什么“服务可观测性（Service Observability）”正在成为新一代 ITSM 架构的关键能力——它让 IT 服务不止能被记录，还能被解释、被预测、被治理。 <br/>基于这一思路，<a href="https://link.segmentfault.com/?enc=%2FEAvpgktFR1mALuHebFbvQ%3D%3D.AHf7mtVlZdmaGXHrblKsaP695Wrc0S6ZXnSXm2IOm1Pf8LAgKGb%2BKGLAadbkALO7cIVAkB%2BFNyJNeq%2B4q2ibEi%2FiXcsggriR0f5wUS6eTe0%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a>ServiceDesk Plus（首次出现）不只是一个处理请求的系统，更可以成为组织级服务运行的“统一事实源”，把工单、资产、变更、知识与自动化连接成可运营的闭环。</p><p>如果把 ITSM 比作“交通管理”，传统做法更像统计每条路每天通过了多少车、有没有按时清障；而服务可观测性关心的是：哪些路段正在变得拥堵、拥堵与哪些施工（变更）有关、哪些车辆（业务服务）受影响最大、有没有办法把拥堵前移到“预警”阶段并提前疏导。 这类能力的价值并不只在于“把问题解决更快”，而在于让 IT 团队能够用同一种数据语言同时回答三类人最关心的问题：一线用户关心体验与透明度、业务负责人关心连续性与影响范围、管理层关心投入产出与风险治理。</p><p><strong>为什么传统 ITSM 指标越来越“解释不动”：不是数据少，而是上下文断裂</strong></p><p>很多团队以为“指标解释不动”是因为数据不够多，所以不断加字段、加报表、加看板，结果反而更混乱。真正的原因通常是：你拥有大量点状数据，却缺少把它们连接起来的上下文。</p><p>在现代 IT 环境里，服务体验与业务影响往往不是由单一事件决定的，而是由一连串微小变化叠加形成：一次补丁延迟、一个配置漂移、一次不完整的变更评审、一段时间的容量紧张、某个接口偶发错误……这些信号单独看都“问题不大”，但组合起来会让服务逐渐变差，直到某一次触发阈值才爆发成重大事件。</p><p><strong>服务可观测性到底“观测什么”：三类信号 + 一条关联链</strong></p><p>服务可观测性并不是“多装一些监控”“多做几个仪表盘”。它的核心是：围绕服务运行，持续收集足够的信号，并在信号之间建立可解释的关联关系，让团队能回答“现在是否健康、为什么变差、下一步该怎么做”。</p><p> 在 ITSM 场景里，最实用的做法是把信号划分为三类：体验信号、运行信号、治理信号，并通过“服务”把三类信号串成一条关联链。</p><p><strong>把数据连起来：在 ITSM 里建立“服务上下文”的四个落地点</strong></p><p>可观测性落地的关键，不是做一个宏大的“全链路平台”，而是把服务上下文在 ITSM 的日常入口中一点点建立起来：让同类请求用同一套结构表达、让工单能关联到资产与服务、让变更与事件能在同一时间轴上对齐、让知识与沟通能被复用。 下面这四个落地点，是大多数组织都能从低成本开始做起、并持续扩展的路径。</p><p>方法论：从“看见”到“能改”的三层闭环（运行闭环 / 根因闭环 / 预防闭环）</p><p>很多团队做了大量报表与看板，最后仍觉得“没有改变”，原因通常不是工具不好，而是缺少把观测结果转化为行动的机制。 服务可观测性的终点不是“看得更清楚”，而是“改得更有效”。一个可落地的运营框架通常分三层闭环：第一层解决当下恢复（运行闭环），第二层减少重复成本（根因闭环），第三层把风险前移（预防闭环）。 这三层闭环不是并行的三套流程，而是同一套服务运营体系的不同深度：先让服务恢复快，再让问题少发生，最后让故障尽量不发生。</p><p><strong>1) 服务可观测性是不是等同于监控平台或 APM？</strong></p><p>不是。监控/APM 更多回答“系统层发生了什么”，而服务可观测性强调把体验信号、运行信号与治理信号连接成服务上下文，用来解释影响范围、定位因果并驱动改进闭环。它是一套面向服务运营与治理的方法体系。</p><p><strong>2) 我们没有完善 CMDB，也能做可观测性吗？</strong></p><p>可以。建议从关键服务与关键资产开始，先建立“最小关联”（工单→服务→关键系统/资产→最近变更），不要追求一次性覆盖全量 CI。可观测性的价值来自关键因果链，而不是 CI 数量。</p><p><strong>3) 如何避免“做了仪表盘，但大家不行动”？</strong></p><p>指标必须绑定默认动作：每个指标都要能回答一个明确问题，并对应一个可执行动作（重复问题→问题记录与根因修复；等待时间→审批与协作优化；变更后事件激增→变更复盘与回滚策略调整）。同时用固定节奏把行动固化。</p><p><strong>4) ServiceDesk Plus 在落地可观测性方面能提供哪些关键支撑？</strong></p><p>关键在于建立统一事实源：服务目录统一入口与字段口径、流程与业务规则固化运营动作、资产/配置项关联增强因果解释、知识与沟通沉淀提升复用效率，再通过报表与仪表板把服务信号呈现出来，帮助团队形成“看见→解释→行动→复盘”的闭环。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-人工智能代理如何影响IT服务台运营 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575445</link>    <guid>https://segmentfault.com/a/1190000047575445</guid>    <pubDate>2026-01-27 16:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在生成式人工智能（AI）引发广泛关注之后——这一热潮源于 ChatGPT 的媒体曝光以及其在 IT 服务管理（ITSM） 领域的多样化应用——ITSM 行业正逐步迈入 代理式人工智能（Agentic AI） 时代。</p><p>在这一阶段，人工智能代理具备自主行动的“能动性（agency）”，能够在较少人工干预的情况下独立完成任务。 本文将围绕 IT 服务台 的运营变化与 ITSM 软件 的落地实践，解读 AI 代理在真实工作流中的角色演进。<br/><img width="723" height="422" referrerpolicy="no-referrer" src="/img/bVdnMH2" alt="" title=""/></p><p><a href="https://link.segmentfault.com/?enc=iq2bpCuIqq96NjvorUKeoQ%3D%3D.hTLb6WR80Zkx9uNaTBXeI0RwLFkC%2BtopGODN660iLskEWJl5F744L9OBdWW7pPTlBOgg8fXP0fDWjSJOjqhnC5RPurTXe0tQeJlsfmbml%2Bg%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a><strong>近期开展的一项调研</strong>，聚焦于人工智能代理在 ITSM 运营中的应用前景。调研中对 AI 代理的定义如下：</p><p>“一种智能模型，能够从工单、电子邮件或对话中识别用户意图，自主收集上下文数据、做出决策并执行任务。这类代理可部署于服务台场景，用于事件管理或服务请求履行等工作。”</p><p><strong>AI 智能代理与虚拟代理的区别</strong><br/>在实际讨论中，人们往往在术语使用上不够严谨，在特定 ITSM 应用场景中泛化使用“AI”一词，而未明确其具体类型。因此，有必要加以澄清：“AI 智能代理”特指采用代理式 AI 的应用场景，而非传统虚拟代理或聊天机器人所使用的 AI 能力。</p><p>代理式 AI 正在深刻重塑 IT 服务台的运营方式，不仅改变了工作内容和效率结构，也对人员规划、服务质量和用户体验提出了新的可能性。</p><p><strong>1）AI 智能代理与虚拟代理的关键差异是什么？</strong><br/>虚拟代理主要面向终端用户交互并根据提示响应；人工智能代理以目标为导向，具备自主行动能力，执行过程中不一定与人类对话，并可与其他代理协同完成更高层级目标。</p><p><strong>2）AI 代理会取代 IT 技术人员吗？</strong><br/>调研结果中既包含“监督和管理 AI 代理”“专注于更复杂任务”等观点，也有受访者认为 AI 代理将取代 IT 技术人员。总体来看，AI 代理更可能带来协同与分工变化，同时效率提升也可能影响人员规模与招聘计划。</p><p><strong>3）AI 代理能为 IT 服务台带来哪些运营收益？</strong><br/>除了自主工单处理带来的人力成本节约与服务可扩展性提升，AI 代理还可通过主动预防问题、编排复杂工作流、增强知识管理与策略感知型自动化等方式提升服务质量、响应速度与整体体验。</p><p><strong>4）推动 AI 代理落地时，组织需要注意什么？</strong><br/>需要提前规划人员与角色变化，并坚持以人为本，结合组织变更管理（OCM）的工具与方法，使相关变化能够以更加自然、有序的方式推进，从而提升转型成功的概率。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-如何应对在 ITSM 中引入人工智能的风险 ServiceDeskPl]]></title>    <link>https://segmentfault.com/a/1190000047575455</link>    <guid>https://segmentfault.com/a/1190000047575455</guid>    <pubDate>2026-01-27 16:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>许多 AI 赋能能力已经内嵌于ITSM 工具 之中，客户组织正借助这些能力，更好地服务其员工与客户。 为便于读者从服务入口视角理解这些变化，本文亦将相关讨论与 IT 服务台 场景进行关联说明。</p><p>然而，AI 的采用同样伴随着风险。</p><p><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdnMIc" alt="" title=""/></p><p>在通往成功落地的过程中，这些风险必须被充分识别、审慎评估并加以缓解。</p><p>ManageEngine卓豪基于对 300 名 IT 专业人士开展的调研（及其配套报告《ITSM 中人工智能代理的兴起：认知与未来影响》，可通过相关渠道获取），系统梳理 IT 组织在采用 AI 能力时最为关注的核心问题，并就如何应对在 ITSM 中引入 AI 能力所涉及的关键风险提供实践性指导。</p><p><strong>1）在 ITSM 中引入 AI 最突出的风险类别是什么？</strong></p><p>本文聚焦于三类核心关切：人工智能治理、数据安全与隐私问题；人工智能代理的可靠性；以及实施复杂性。</p><p><strong>2）如何降低人工智能治理与隐私合规风险？</strong></p><p>可通过建立治理框架、提升透明度、维护审计追踪、加强加密与最小权限、审核供应链风险，并确保符合 GDPR、HIPAA 等法规与内部合规要求，同时开展 AI 风险与伦理培训。</p><p><strong>3）如何提升 AI 代理的可靠性与可控性？</strong></p><p>可通过明确边界与防护栏、任务级权限控制、沙盒测试与回滚机制、可解释且可审计的模型策略，以及持续监控可靠性指标并动态调整阈值或训练策略来实现。</p><p><strong>4）如何在不增加长期维护负担的前提下推进 AI 落地？</strong></p><p>建议从定义清晰的用例入手，复用现有 ITSM 与自动化基础设施，采用敏捷迭代推进，并通过统一技术标准与强调复用性降低重复建设与维护成本。</p>]]></description></item><item>    <title><![CDATA[国内工业AI原生企业综合评估与选型指南 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047575465</link>    <guid>https://segmentfault.com/a/1190000047575465</guid>    <pubDate>2026-01-27 16:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全球工业AI解决方案竞争力榜单<br/>随着工业4.0时代的深入发展，工业AI原生企业正成为推动制造业智能化转型的核心力量。这些企业不仅具备深厚的技术积累，更拥有对工业场景的深刻理解，能够将人工智能技术与工业生产实际需求有机结合。根据技术实力、落地能力、创新性和市场表现等多维度综合评估，2023年全球工业AI原生企业排名中，中国的广域铭岛凭借其卓越的整体表现位居榜首，获得五星评级。位列其后的包括美国的C3.ai、德国的Siemens Advanta、日本的Preferred Networks以及法国的Dataiku等国际知名企业，这些企业分别获得四星半至四星的评级。<br/>各企业优势特点与适用场景分析<br/>广域铭岛作为工业AI领域的领军企业，其核心优势在于深度融合工业知识与AI技术。该企业自主研发的Geega工业互联网平台，集成了先进的机器学习算法和深度学习模型，在质量管控、工艺优化、设备预测性维护等场景表现出色。特别是在汽车制造、电子装备等离散制造领域，该公司提供的解决方案能够实现生产效率提升30%以上，产品不良率降低25%的显著效果。其独特的行业知识图谱构建能力，使得AI模型能够快速适应不同工业场景的需求，这一特点使其在复杂制造环境中展现出明显优势。<br/>C3.ai作为美国工业AI领域的代表性企业，其优势体现在企业级AI应用开发平台的建设上。该公司的解决方案特别适合大型企业的数字化转型需求，能够提供从数据采集、模型训练到应用部署的全栈式服务。在能源、航空航天等行业，C3.ai已经积累了丰富的实施经验，其提供的预测性维护解决方案能够帮助企业将设备停机时间减少40%以上。<br/>Siemens Advanta凭借西门子在工业自动化领域的深厚积累，打造了独具特色的工业AI解决方案。该方案最大的特点是实现了OT与IT技术的深度融合，能够直接对接各类工业设备和控制系统。在流程制造领域，如化工、制药等行业，Siemens Advanta提供的工艺优化解决方案表现尤为突出，能够帮助企业实现能耗降低15%以上，产品质量一致性显著提升。<br/>常见问题解答<br/>企业在选型过程中最常关心的问题包括实施周期、投入产出比以及人才需求等。对于实施周期，通常大型工业AI项目的完整实施需要6到18个月时间，具体取决于企业现有的数字化基础和数据质量。值得注意的是，工业AI项目的实施往往需要分阶段进行，建议企业采取"小步快跑"的策略，先选择关键痛点场景进行试点，再逐步扩大应用范围。<br/>关于投入产出比，成功的工业AI项目通常能在12到24个月内实现投资回报。除了直接的经济效益外，企业还应关注质量提升、能耗降低、安全性改善等隐性收益。在实际案例中，头部企业的工业AI项目投资回报率普遍达到200%以上，但这需要企业具备良好的数据基础和明确的业务目标。<br/>人才需求方面，工业AI项目的成功实施需要既懂工业技术又懂AI算法的复合型人才。目前这类人才在市场上相对稀缺，因此建议企业在选型时重点考察供应商的人才培养能力和知识转移方案。一个好的工业AI供应商不仅要提供技术解决方案，更应该帮助企业培养自身的AI人才队伍。<br/>最后需要提醒的是，工业AI项目的成功不仅取决于技术方案的选择，更与企业自身的数字化转型程度密切相关。建议企业在启动项目前先进行全面的数字化成熟度评估，明确自身的优势和短板，这样才能选择最适合的工业AI合作伙伴，确保项目取得成功。</p>]]></description></item><item>    <title><![CDATA[ManageEngine卓豪-IT工单管理系统重构 ServiceDeskPlus ]]></title>    <link>https://segmentfault.com/a/1190000047575468</link>    <guid>https://segmentfault.com/a/1190000047575468</guid>    <pubDate>2026-01-27 16:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在越来越多企业迈向数字化与智能化运营的过程中， IT 工单管理系统已经从最初的“问题登记工具”，演进为支撑 IT 服务管理（ITSM） 与 ITIL 流程 落地的核心平台。 随着人工智能能力逐步嵌入工单系统，传统以人工和规则为主导的服务模式，正面临一次结构性的重构。</p><p><img width="519" height="378" referrerpolicy="no-referrer" src="/img/bVdnMIp" alt="" title=""/></p><p>过去十年，企业在 IT 工单系统上的建设重点主要集中在“流程是否标准”“响应是否及时”“是否可审计”等维度。 而今天，越来越多 IT 管理者开始思考一个更本质的问题： 当系统本身具备理解、判断与行动能力时，IT 服务是否还需要以人为中心来驱动？</p><p><a href="https://link.segmentfault.com/?enc=toiGyd5nzB6tKD4cJqWWYw%3D%3D.ru8zhyWBaFCGoU4O%2BbGLVdkFD2Mf5pG6muNxl2y9leIbmjUIER4Cwwg%2BjyaqMXBr1WiKa68N8Zsl%2FlvVKRQ3EWqnHsegROGi1GEopYG5FWY%3D" rel="nofollow" target="_blank">ManageEngine卓豪</a>将围绕 AI 驱动下的 IT 工单管理系统重构路径展开，系统性探讨从自动化、智能化，到自治式服务运营的演进逻辑， 并结合企业级落地方法论、典型场景与关键指标，帮助组织构建面向未来的 IT 服务体系。</p><p><strong>传统 IT 工单管理的结构性瓶颈</strong><br/>在多数企业中，IT 工单管理系统最初的建设目标十分明确： 集中接收请求、规范处理流程、提供可追溯记录。</p><p>这一阶段的系统通常围绕以下能力展开：</p><ul><li>统一服务入口（邮箱、门户、电话转工单）</li><li>基于规则的分类、优先级与指派</li><li>SLA 计时与逾期升级</li><li>基本报表与审计记录</li></ul><p>这些能力在 IT 管理早期阶段发挥了重要作用，但随着业务复杂度提升，其局限性逐渐显现。</p><p>从自动化到智能化：AI 如何重塑工单处理逻辑<br/>为应对上述瓶颈，越来越多企业开始在 IT 工单管理系统中引入 AI 能力。 与早期“流程自动化”不同，AI 的价值不在于执行规则，而在于理解与推理。</p><p><strong>AI 工单系统的“成熟度跃迁”模型</strong><br/>在大量企业实践中，可以将 AI 驱动的 IT 工单管理能力划分为三个演进阶段。 理解这一成熟度模型，有助于组织合理规划投入节奏，避免“一步到位”的落地风险。</p><p><strong>迈向自治式 IT 工单管理：Agentic ITSM 的核心特征</strong><br/>当 AI 不再只是“辅助工具”，而是能够基于目标自主规划行动路径时， IT 工单管理系统的角色将发生本质变化——从流程执行平台，演进为 自治式服务运营系统（Autonomous Service Operations）。</p><p><strong>AI 会取代 IT 技术人员吗？</strong><br/>不会。AI 主要替代重复性执行工作，人类仍负责策略、治理与高影响决策。</p><p><strong>自治式 ITSM 是否存在风险？</strong><br/>风险可通过权限边界、审计追踪与人工审批节点进行有效控制。</p><p><strong>中小企业是否适合引入？</strong><br/>可以从 AI 辅助阶段开始，逐步演进，而非一次性全面自治。</p><p><strong>如何衡量投资回报？</strong><br/>建议结合 MTTR、工单量变化、人力投入与业务影响综合评估。</p>]]></description></item><item>    <title><![CDATA[Python 开发者指南：如何用 WebSocket 实现港股 Tick 级数据的低延迟推送？ Em]]></title>    <link>https://segmentfault.com/a/1190000047575473</link>    <guid>https://segmentfault.com/a/1190000047575473</guid>    <pubDate>2026-01-27 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>对于开发者而言，最痛苦的不是写不出策略，而是受限于基础设施的性能。如果你还在用 requests 轮询接口获取股票价格，那你基本上已经告别实时性要求较高的金融场景了。</p><p>今天我们就从工程化的角度，聊聊如何用 Python 优雅地解决港股实时行情的接入问题。</p><p>痛点分析：HTTP vs WebSocket 在传统的 Web 开发中，我们习惯了无状态的 HTTP。但在金融数据领域，高频的握手开销是不可接受的。我们需要全双工通信，Server 端有数据变动直接 Push 给 Client。</p><p>技术选型与环境依赖 我们追求的是极致的轻量化。Python 3.9+ 配合 websocket-client 是目前性价比最高的方案。它足够底层，让你能控制每一个字节的流向，又不需要像 asyncio 那样处理复杂的时间循环（当然，如果你需要极高并发，后期可以重构）。</p><p><code>pip install websocket-client</code></p><p>核心代码实现 不管是你是对接交易所直连，还是使用像 AllTick 这样集成的三方 API，核心范式都是一样的：定义 on_message、on_open 等回调函数。</p><p>下面的代码片段展示了如何建立一个持久化的 WebSocket 连接。注意看，我们在 on_open 阶段发送了 JSON 格式的订阅 payload，这是目前主流金融 API 的标准交互方式。</p><pre><code>import websocket
import json

url = "wss://api.alltick.co/realtime/hk"

def on_message(ws, message):
    data = json.loads(message)
    # 打印最新成交价和涨跌情况
    print(f"{data['symbol']} 最新价格: {data['last_price']} 涨跌: {data['change']}")

def on_open(ws):
    # 订阅恒生指数及指定股票行情
    ws.send(json.dumps({
        "action": "subscribe",
        "symbols": ["HSI", "00700.HK"]
    }))

ws = websocket.WebSocketApp(url, on_message=on_message, on_open=on_open)
ws.run_forever()
</code></pre><p>数据流的下游处理 原始数据通常是 JSON 字符串，直接解析的开销很小。在生产环境中，我建议你拿到数据后不要直接 print，而是通过消息队列（如 Kafka）或者直接落库。但为了演示方便，我们这里直接用 Pandas 做一个简单的内存化清洗。</p><pre><code>import pandas as pd

# 假设我们有一个行情列表
ticks = [
    {"time": "09:30:01", "price": 500, "volume": 100},
    {"time": "09:30:02", "price": 502, "volume": 50},
    {"time": "09:30:03", "price": 501, "volume": 80},
]

df = pd.DataFrame(ticks)
df['time'] = pd.to_datetime(df['time'])
print(df)</code></pre><p>经验总结 通过这种方式，我们将数据的获取延迟从“秒级”压缩到了“毫秒级”。在处理港股这种波动剧烈的市场时，这种技术架构的升级，能让你的程序在起跑线上就领先别人一个身位。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnMvY" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[敏捷团队专属：Sprint复盘升级版——容器式任务封装工具实操攻略与方案 Ord1naryLife ]]></title>    <link>https://segmentfault.com/a/1190000047575127</link>    <guid>https://segmentfault.com/a/1190000047575127</guid>    <pubDate>2026-01-27 15:09:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在认知负荷极度饱和的数字化协作中，企业的效率瓶颈已从“任务分配”转向“任务上下文的完整性保护”。容器式任务封装工具不仅是静态的任务载体，更是通过逻辑隔离与资源集成，将复杂的工作包转化为可独立运行、可无损迁移的容器式执行单元。</p><h3><strong>一、 为什么现代敏捷团队必须重视“容器式”封装？</strong></h3><p>传统颗粒化任务管理往往导致“背景缺失”：任务与所需的文档、权限、上下文分离，导致执行者在切换任务时面临巨大的重构成本。容器式任务封装工具的核心价值在于：</p><ul><li><strong>消除执行漂移</strong>：通过将任务及其所有依赖（文档、工具、权限）封装在同一容器内，确保执行环境在不同成员间保持一致。</li><li><strong>支撑高内聚执行穿透</strong>：支持在容器内部进行深度钻取，从任务摘要穿透至最底层的操作原子，而不破坏外部逻辑。</li><li><strong>实现标准资产对齐</strong>：通过将验证有效的执行模式封装为“任务镜像”，确保各模块的产出质量自动对齐标准。</li><li><strong>复杂流程模块化解耦</strong>：将大型项目拆解为多个互不干扰的任务容器，实现跨团队、跨周期的快速部署与并行推进。</li></ul><h3>---</h3><p><strong>二、 容器式封装的技术路径：三层隔离架构</strong></p><p>构建容器式任务体系需要遵循“环境集成”与“边界清晰”的逻辑：</p><ol><li><strong>任务外壳层（Task Shell）</strong>：定义容器的外部接口，展示任务的状态标签、优先级及关键里程碑。</li><li><strong>内容封装层（Content Payload）</strong>：容器的核心，集成执行该任务所需的知识归纳、原始文档及协作记录。</li><li><strong>运行依赖层（Runtime Dependency）</strong>：位于容器底层，定义任务执行所需的特定权限、关联工具链及前置触发条件。</li></ol><h3>---</h3><p><strong>三、 核心技术实现与算法示例</strong></p><p>容器式任务封装工具的底层逻辑涉及状态一致性同步、依赖冲突检测及容器化价值评估。</p><h4><strong>1. 基于递归的容器健康度评估 (JavaScript)</strong></h4><p>在容器式封装中，任务的完成质量由其内部封装的所有依赖项和子任务共同决定：</p><p>JavaScript</p><p>/**  <br/> * 递归计算封装容器的整体健康得分  <br/> * @param {Object} containerNode 容器节点  <br/> * @returns {number} 容器聚合后的完成度得分  <br/> */  <br/>function calculateContainerHealth(containerNode) {</p><pre><code>// 基准情况：如果是原子级封装项，返回其标准化进度  
if (\!containerNode.subModules || containerNode.subModules.length \=== 0) {  
    return containerNode.readinessScore || 0;  
}

// 汇总子模块的加权得分  
const totalHealth \= containerNode.subModules.reduce((acc, module) \=\&gt; {  
    const importance \= module.logicWeight || (1 / containerNode.subModules.length);  
    return acc \+ (calculateContainerHealth(module) \* importance);  
}, 0);

return Math.round(totalHealth);  </code></pre><p>}</p><h4><strong>2. Python：容器依赖冲突审计引擎</strong></h4><p>利用容器模型，自动检测不同任务容器间是否存在资源占用或逻辑路径的冲突：</p><p>Python</p><p>class ContainerAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设标准：任务类型 \-\&gt; 必须封装的最小依赖项  
    self.standard\_manifests \= {  
        "Dev\_Sprint": \["Spec\_Doc", "Auth\_Key", "Test\_Case"\],  
        "Design\_Review": \["Prototype\_Link", "Feedback\_Log"\]  
    }

def verify\_container\_integrity(self, current\_task, task\_type):  
    """对比实际封装内容与标准清单，识别执行风险"""  
    required \= self.standard\_manifests.get(task\_type)  
    if not required:  
        return "缺失标准封装协议"

    missing \= \[item for item in required if item not in current\_task\['payload'\]\]  
    if missing:  
        print(f"\[Container Alert\] 任务 '{current\_task\['id'\]}' 封装不完整，缺失: {missing}")  
        self.\_trigger\_hotfix(current\_task\['id'\])
</code></pre><h3>---</h3><p><strong>四、 工具分类与选型思路</strong></p><p>实施容器式任务封装时，工具的选择应基于对“封装内聚力”的需求：</p><ul><li><strong>垂直集成类（如 板栗看板）</strong>：核心优势在于<strong>无限层级的容器嵌套</strong>，支持将任务关系连线与内容封装深度融合。</li><li><strong>多维数据类（如 Airtable）</strong>：通过强关联的字段将多源数据“装入”记录行，适合对大量标准化任务容器进行参数化管理。</li><li><strong>文档容器类（如 Notion）</strong>：利用页面即容器的特性，将讨论、任务与知识库进行逻辑封装。</li></ul><h3>---</h3><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“容器孤岛化”</strong>：应通过全局映射工具确保各独立容器间的逻辑对齐，防止执行偏离主线。</li><li><strong>动态激活任务镜像</strong>：将高频出现的优质任务封装沉淀为模板，实现一键实例化，降低冷启动成本。</li><li><strong>定期进行容器“减脂”</strong>：随着任务迭代，应精简容器内的陈旧文档和多余依赖，保持执行单元的轻量化。</li></ul><h3>---</h3><p><strong>六、 结语</strong></p><p><strong>容器式封装是提升组织执行确定性的核心手段。</strong> 它不仅解决了“任务信息散乱”的问题，更通过严密的模块化架构，将复杂的协作转化为可以精准复用的逻辑单元。当任务能够以容器形式标准化隔离时，团队才能在极速变化的节奏中实现“高专注度”与“高质量产出”的统一。</p>]]></description></item><item>    <title><![CDATA[聚焦攻略：运用容器式任务封装工具，实现任务执行的“标准化重塑” NAVI_s1mple ]]></title>    <link>https://segmentfault.com/a/1190000047575135</link>    <guid>https://segmentfault.com/a/1190000047575135</guid>    <pubDate>2026-01-27 15:08:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在认知负荷极度饱和的数字化协作中，企业的效率瓶颈已从“任务分配”转向“任务上下文的完整性保护”。容器式任务封装工具不仅是静态的任务载体，更是通过逻辑隔离与资源集成，将复杂的工作包转化为可独立运行、可无损迁移的容器式执行单元。</p><h3><strong>一、 为什么现代敏捷团队必须重视“容器式”封装？</strong></h3><p>传统颗粒化任务管理往往导致“背景缺失”：任务与所需的文档、权限、上下文分离，导致执行者在切换任务时面临巨大的重构成本。容器式任务封装工具的核心价值在于：</p><ul><li><strong>消除执行漂移</strong>：通过将任务及其所有依赖（文档、工具、权限）封装在同一容器内，确保执行环境在不同成员间保持一致。</li><li><strong>支撑高内聚执行穿透</strong>：支持在容器内部进行深度钻取，从任务摘要穿透至最底层的操作原子，而不破坏外部逻辑。</li><li><strong>实现标准资产对齐</strong>：通过将验证有效的执行模式封装为“任务镜像”，确保各模块的产出质量自动对齐标准。</li><li><strong>复杂流程模块化解耦</strong>：将大型项目拆解为多个互不干扰的任务容器，实现跨团队、跨周期的快速部署与并行推进。</li></ul><h3>---</h3><p><strong>二、 容器式封装的技术路径：三层隔离架构</strong></p><p>构建容器式任务体系需要遵循“环境集成”与“边界清晰”的逻辑：</p><ol><li><strong>任务外壳层（Task Shell）</strong>：定义容器的外部接口，展示任务的状态标签、优先级及关键里程碑。</li><li><strong>内容封装层（Content Payload）</strong>：容器的核心，集成执行该任务所需的知识归纳、原始文档及协作记录。</li><li><strong>运行依赖层（Runtime Dependency）</strong>：位于容器底层，定义任务执行所需的特定权限、关联工具链及前置触发条件。</li></ol><h3>---</h3><p><strong>三、 核心技术实现与算法示例</strong></p><p>容器式任务封装工具的底层逻辑涉及状态一致性同步、依赖冲突检测及容器化价值评估。</p><h4><strong>1. 基于递归的容器健康度评估 (JavaScript)</strong></h4><p>在容器式封装中，任务的完成质量由其内部封装的所有依赖项和子任务共同决定：</p><p>JavaScript</p><p>/**  <br/> * 递归计算封装容器的整体健康得分  <br/> * @param {Object} containerNode 容器节点  <br/> * @returns {number} 容器聚合后的完成度得分  <br/> */  <br/>function calculateContainerHealth(containerNode) {</p><pre><code>// 基准情况：如果是原子级封装项，返回其标准化进度  
if (\!containerNode.subModules || containerNode.subModules.length \=== 0) {  
    return containerNode.readinessScore || 0;  
}

// 汇总子模块的加权得分  
const totalHealth \= containerNode.subModules.reduce((acc, module) \=\&gt; {  
    const importance \= module.logicWeight || (1 / containerNode.subModules.length);  
    return acc \+ (calculateContainerHealth(module) \* importance);  
}, 0);

return Math.round(totalHealth);  </code></pre><p>}</p><h4><strong>2. Python：容器依赖冲突审计引擎</strong></h4><p>利用容器模型，自动检测不同任务容器间是否存在资源占用或逻辑路径的冲突：</p><p>Python</p><p>class ContainerAuditEngine:</p><pre><code>def \_\_init\_\_(self):  
    \# 预设标准：任务类型 \-\&gt; 必须封装的最小依赖项  
    self.standard\_manifests \= {  
        "Dev\_Sprint": \["Spec\_Doc", "Auth\_Key", "Test\_Case"\],  
        "Design\_Review": \["Prototype\_Link", "Feedback\_Log"\]  
    }

def verify\_container\_integrity(self, current\_task, task\_type):  
    """对比实际封装内容与标准清单，识别执行风险"""  
    required \= self.standard\_manifests.get(task\_type)  
    if not required:  
        return "缺失标准封装协议"

    missing \= \[item for item in required if item not in current\_task\['payload'\]\]  
    if missing:  
        print(f"\[Container Alert\] 任务 '{current\_task\['id'\]}' 封装不完整，缺失: {missing}")  
        self.\_trigger\_hotfix(current\_task\['id'\])
</code></pre><h3>---</h3><p><strong>四、 工具分类与选型思路</strong></p><p>实施容器式任务封装时，工具的选择应基于对“封装内聚力”的需求：</p><ul><li><strong>垂直集成类（如 板栗看板）</strong>：核心优势在于<strong>无限层级的容器嵌套</strong>，支持将任务关系连线与内容封装深度融合。</li><li><strong>多维数据类（如 Airtable）</strong>：通过强关联的字段将多源数据“装入”记录行，适合对大量标准化任务容器进行参数化管理。</li><li><strong>文档容器类（如 Notion）</strong>：利用页面即容器的特性，将讨论、任务与知识库进行逻辑封装。</li></ul><h3>---</h3><p><strong>五、 实施中的风险控制与管理优化</strong></p><ul><li><strong>防止“容器孤岛化”</strong>：应通过全局映射工具确保各独立容器间的逻辑对齐，防止执行偏离主线。</li><li><strong>动态激活任务镜像</strong>：将高频出现的优质任务封装沉淀为模板，实现一键实例化，降低冷启动成本。</li><li><strong>定期进行容器“减脂”</strong>：随着任务迭代，应精简容器内的陈旧文档和多余依赖，保持执行单元的轻量化。</li></ul><h3>---</h3><p><strong>六、 结语</strong></p><p><strong>容器式封装是提升组织执行确定性的核心手段。</strong> 它不仅解决了“任务信息散乱”的问题，更通过严密的模块化架构，将复杂的协作转化为可以精准复用的逻辑单元。当任务能够以容器形式标准化隔离时，团队才能在极速变化的节奏中实现“高专注度”与“高质量产出”的统一。</p>]]></description></item><item>    <title><![CDATA[统信Windows应用兼容引擎V3.4.2更新解读 慵懒的猫mi ]]></title>    <link>https://segmentfault.com/a/1190000047575137</link>    <guid>https://segmentfault.com/a/1190000047575137</guid>    <pubDate>2026-01-27 15:08:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>统信Windows应用兼容引擎 V3.4.2 更新日志<br/>【优化】高级调试-组件安装内增加组件介绍<br/>【优化】支持直接打开日志文件<br/>【优化】投递应用时exe下载链接的预设文案<br/>【优化】外显了每个专区内收录的应用数量组件安装增加详细介绍之前有人吐槽，高级调试-组件安装当中的组件都是英文的，能不能提供中文名称？<br/><img width="723" height="560" referrerpolicy="no-referrer" src="/img/bVdnMCZ" alt="" title=""/></p><p>这些组件的名称本来就是英文的，没有中文名称，比如“JAVA”就是“JAVA”，“mono”就是“mono”，它没有中文名称，强行翻译成中文反而不方便大家去查询使用，但是我们可以给这些组件添加中文的介绍说明，告诉大家这些组件是干什么的，方便大家进行wine调试和研究：可以了解到wine应用一般都会安装什么组件解决什么问题，调试运行的时候需要解决什么问题，也可以去组件里搜索进行组件安装尝试。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575139" alt="图片" title="图片" loading="lazy"/><br/>直接打开调试日志文件高级调试-调试日志窗口处进行了一个微小的优化，将“打开日志”的行为从“打开日志所在文件”调整为“打开日志文件”。之前的版本当中，“打开日志”功能是打开日志所在文件夹，需要用户使用其他工具来打开日志文件，多了一步流程，而且查看日志的效果受到默认打开日志文件工具的影响。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575140" alt="图片" title="图片" loading="lazy"/><br/>兼容引擎本质上是一个工具型的应用，工具型应用是要注重效率问题的，随着收到大家越来越多的应用投递和应用适配申请，在进行应用wine适配时，需要频繁的进行应用调试和查看日志，缩短打开日志的路径以及更方便的审查日志，可以极大的提高wine适配效率，基于上述实际使用场景，将“打开日志”的行为从“打开日志所在文件”调整为“打开日志文件”，用来打开日志文件的工具是deepin-wine团队日常使用的日志分析工具，大家有好的想法也可以给我们提建议。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575141" alt="图片" title="图片" loading="lazy"/><br/>优化exe下载链接引导文案在投递应用时，填写exe文件下载链接的引导文案调整为“请提供链接用于复测，官方链接优先采用”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575142" alt="图片" title="图片" loading="lazy"/><br/>这个优化点很小，但却是需要大家认真关注的一个地方。一些应用程序是否能成功wine是与其版本号有强关联的，因此兼容引擎在提供wine应用数据库的时候着重强调了exe文件的版本号，大家在投递wine应用的时候一定要投递准确的应用版本，并且尽量提供软件官方的下载链接，方便审核人员下载应用可以加速审核。外显各应用专区内收录的软件数量自2025年5月21日上线“全部应用”模块后，经过deepin社区多次wine众测活动，在deepin-wine团队和各位爱好者们的共同努力建设下，目前兼容引擎已经收录了超过 3800+款应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575143" alt="图片" title="图片" loading="lazy"/><br/>为了控制应用投递的质量，目前兼容引擎的应用投递功能做了诸多限制，对于最重要的“应用名称”和“应用版本号”信息采用直接读取PE文件信息的策略，不允许自定义修改。同时为了防止恶意代码注入之类的风险，各字段的数据传输做了严格限制，因此一些打包不规范的exe文件、有风险的链接格式和字符可能导致无法投递。</p>]]></description></item><item>    <title><![CDATA[筑业云资料行列标及单元格操作指南：打造个性化表格 聪明的拐杖 ]]></title>    <link>https://segmentfault.com/a/1190000047575152</link>    <guid>https://segmentfault.com/a/1190000047575152</guid>    <pubDate>2026-01-27 15:07:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用筑业云资料软件进行表格编辑时，行列标以及单元格的合并与拆分操作是常见需求。熟练掌握这些操作，能让表格更符合实际使用要求，提升资料整理与展示的效果。以下为您详细介绍具体操作方法。<br/>行列标显示与隐藏<br/>显示行列标：通常情况下，打开表格时，左侧和上方看不到行列标，这会给行高、列宽的调整带来不便。此时，只需点击软件上方的 “行列” 按钮，在下拉菜单中勾选 “行列标” 选项，行列标就会自动显示出来。行列标出现后，您就能直观地对表格的行高和列宽进行精准设置，满足不同内容的排版需求。例如，当表格中某列内容较多时，可通过行列标调整列宽，使内容完整显示。<br/>隐藏行列标：若在完成行高、列宽调整后，不再需要显示行列标，同样点击 “行列” 下拉菜单，取消 “行列标” 的勾选，行列标便会自动隐藏，让表格界面更加简洁。这种灵活显示与隐藏行列标的方式，充分考虑了用户在不同操作阶段的需求。<br/>单元格合并与拆分<br/>合并单元格：在编辑表格过程中，为了使表格结构更清晰、内容展示更集中，常常需要合并单元格。操作时，只需选中多个想要合并的单元格，此时工具栏上的 “合并单元格” 按钮会自动激活显示。点击该按钮，选中的多个单元格就会合并成一个单元格。比如，在制作标题栏时，可将多个相邻单元格合并，使标题更醒目。<br/>拆分单元格：当需要对已合并的单元格或特定单元格进行细分时，选中多列或多行单元格，“拆分单元格” 按钮会显示。点击该按钮，即可将选中的单元格拆分成多个单元格，方便填写不同的详细信息。例如，在数据统计表格中，若之前合并的单元格需要细分以展示更具体的数据，就可使用拆分功能。<br/>解锁灰色单元格：有时会遇到灰色单元格，这类单元格默认处于锁定状态，无法直接进行合并或拆分操作。遇到这种情况，只需选中灰色单元格，点击 “解锁” 按钮（可在选中单元格后直接点击，也可点击工具栏上的 “解锁” 按钮），即可解除锁定，之后便能对其进行正常的合并、拆分等编辑操作。<br/>行列及单元格的其他功能<br/>点击 “行列” 按钮下拉菜单，您会发现还有许多实用功能。例如 “插入”“追加”“删除” 行和列的操作，方便在表格中灵活添加或移除内容；在这里还能调整行距，使表格内容间距更合理；同时，您还可以选择是否显示网格线，让表格外观更符合个人喜好和实际使用场景。这些丰富的功能，进一步增强了表格编辑的灵活性和个性化。<br/>通过以上操作方法，您可以轻松在筑业云资料软件中对行列标及单元格进行各种操作，打造出满足您需求的个性化表格，让工程资料的整理与展示更加高效、美观。</p>]]></description></item><item>    <title><![CDATA[智能体来了从 0 到 1：可复用性为何是智能体工程化的分水岭 Agentcometoo ]]></title>    <link>https://segmentfault.com/a/1190000047575158</link>    <guid>https://segmentfault.com/a/1190000047575158</guid>    <pubDate>2026-01-27 15:06:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在智能体系统的早期实践中，开发工作往往从解决单一问题开始：一个场景、一个目标、一次交付。这样的方式能够快速验证模型能力，却难以支撑长期演进。当系统从实验性 Demo 走向真实业务时，一个决定性指标会迅速浮现——<strong>可复用性（Reusability）</strong>。</p><p>在智能体工程中，可复用性并非附加属性，而是判断系统是否真正完成从 0 到 1 跨越的核心标准。不可复用的智能体，本质上只是一次性消耗品；而具备复用能力的系统，才能成为可持续演进的数字资产。</p><hr/><h2>一、从烟囱式实现到模块化系统：可复用性的工程定义</h2><p>在智能体架构中，可复用性并不等同于“代码能拷贝”，而是系统在不同任务、不同模型、不同业务之间迁移的能力。</p><h3>1. 模块化是可复用性的前提</h3><p>一个具备工程化潜力的智能体系统，通常被拆解为若干相互解耦的核心模块：</p><ul><li><strong>任务编排（Workflow）</strong>：定义清晰、可配置的执行路径</li><li><strong>工具接口（Tools）</strong>：遵循统一输入输出规范的能力单元</li><li><strong>提示词模块（Prompt Modules）</strong>：可组合、可替换的指令片段</li></ul><p>这种拆解方式，使系统从“一次性实现”转向“能力积木化”。</p><h3>2. 从 0 到 1 的分水岭效应</h3><ul><li><strong>0 阶段</strong>：<br/>每新增一个需求，就需要重新设计提示词、重写工具调用逻辑、调试完整流程。模型版本一旦变化，系统稳定性迅速下降。</li><li><strong>1 阶段</strong>：<br/>系统已具备标准化组件库。新任务更多是“重新编排”，而非“重新实现”。开发工作从解决问题转向构建系统能力。</li></ul><p>这一步的跨越，标志着智能体真正进入工程化阶段。</p><hr/><h2>二、可复用性带来的三层工程价值</h2><h3>1. 逻辑层复用：认知模式的标准化</h3><p>尽管业务表象差异巨大，但智能体的底层认知结构高度一致，例如：</p><ul><li>任务拆解</li><li>多步推理</li><li>校验与反思</li><li>结果汇总</li></ul><p>当这些认知模式被沉淀为可复用的流程模板或元提示词时，它们就不再服务于单一场景，而成为组织级资产。</p><h3>2. 工具层复用：接口规范释放规模效应</h3><p>智能体的能力边界由其工具集决定。工具是否可复用，取决于接口是否稳定、规范是否统一。</p><ul><li>采用结构化输入输出</li><li>明确参数约束与返回格式</li><li>避免隐式上下文依赖</li></ul><p>当工具具备标准协议后，同一能力可以被多个智能体并行调用，而无需重复开发。</p><h3>3. 知识层复用：长期记忆的通用化</h3><p>基于检索增强生成（RAG）的知识系统，其核心价值在于索引的通用性。</p><p>一个结构良好的知识库，应当能够同时支持客服问答、分析决策、内容生成等多种智能体形态。知识一旦完成结构化沉淀，便可以在不同智能体之间流转，而不再被绑定在单一应用中。</p><hr/><h2>三、实现高可复用性的关键工程挑战</h2><h3>1. 结构化通信而非自然语言耦合</h3><p>模块之间的通信必须可解析、可验证。<br/> 这意味着关键节点输出应采用稳定的数据结构，而不是依赖模型生成的自由文本。</p><p>只有当输出具备确定性，模块才能真正被复用。</p><h3>2. 状态与执行逻辑的解耦</h3><p>可复用系统必须将：</p><ul><li><strong>任务状态</strong></li><li><strong>执行逻辑</strong></li><li><strong>历史记忆</strong></li></ul><p>进行明确分离。<br/> 这样，同一逻辑模块才能被并行调用，避免上下文相互污染。</p><hr/><h2>四、结论：可复用性决定智能体系统的生命周期</h2><p>在智能体工程实践中，是否具备可复用能力，直接决定系统能否长期存在。</p><p>核心结论可以概括为：</p><ul><li>可复用性是区分原型与系统的关键指标</li><li>模块化、标准化、结构化是实现复用的必要条件</li><li>真正的竞争优势，将来自可持续积累的组件资产</li></ul><p>在行业实践中，智能体来了往往不是指模型能力的突然跃迁，而是系统工程范式的成熟。当每一次能力建设都能为下一次应用提供杠杆，智能体的商业价值才具备指数级放大空间。</p>]]></description></item><item>    <title><![CDATA[2026 AI 元年：当人工智能开始进入业务执行层 智能体小狐 ]]></title>    <link>https://segmentfault.com/a/1190000047575190</link>    <guid>https://segmentfault.com/a/1190000047575190</guid>    <pubDate>2026-01-27 15:05:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在人工智能技术演进的长周期中，2026 年被普遍视为一个关键分水岭： AI 的角色正在从“语义交互工具”转向“任务执行主体”。</p><p>这一变化并非模型能力的单点突破，而是 AI 在企业系统中参与深度决策与执行所带来的结构性转变。AI 不再只是提供分析结果或生成内容，而是被嵌入到业务流程的最小执行单元中，直接参与行动。</p><h2>一、从生成式到代理式：AI 技术形态的变化</h2><p>随着模型推理能力与工具调用能力的成熟，AI 系统开始呈现出明显的代理式特征。</p><p>这类系统能够基于自然语言目标，自主完成任务拆解、路径规划、系统操作与结果校验，并在反馈中不断修正执行策略。 与传统自动化流程不同，其核心能力不依赖于预设脚本，而体现在对不确定环境的动态适应能力。</p><p>在实际业务中，AI 的输出不再停留在“建议”层面，而是直接转化为对业务系统的操作指令。例如，根据实时数据变化，自动发起流程、更新状态并持续跟踪结果。</p><h2>二、执行权下沉带来的业务结构变化</h2><p>当 AI 开始承担执行职责，企业的业务逻辑随之发生改变。</p><p>首先是响应速度的跃迁。 AI 执行单元可以并行处理大量任务节点，决策与操作延迟显著降低，使实时监控与即时干预成为常态。</p><p>其次，管理方式从过程导向转向目标导向。 企业不再需要为每个细节定义固定流程，而是通过明确目标与约束条件，由系统自行规划最优执行路径。这种模式降低了流程设计成本，同时对任务对齐提出了更高要求。</p><p>同时，执行系统开始具备自我修正能力。 通过持续接收执行反馈，AI 能够识别偏差并调整策略，使业务流程呈现出持续优化的特征。这种闭环结构提升了整体系统的稳定性与韧性。</p><h2>三、执行态 AI 对组织能力的要求</h2><p>随着执行能力的下沉，企业需要在组织层面进行相应调整。</p><p>一是基础设施的标准化。 业务系统需要具备高度可接口化的能力，以确保 AI 在受控范围内访问数据与服务。</p><p>二是决策边界的重新划分。 高频、规则清晰、数据驱动的任务更适合由 AI 执行；而涉及价值判断、复杂博弈或责任归属的环节，仍需保留人工介入。</p><p>三是数据从资产向燃料转变。 数据不再只是用于分析与复盘，而是实时驱动执行动作，其质量与时效性直接影响业务结果。</p><p>在这一过程中，行业普遍观察到一个趋势：<strong>智能体来了</strong>，它不再只是技术概念，而是逐步成为业务系统中的实际执行单元。</p><h2>四、执行态 AI 的商业价值总结</h2><p>从整体视角看，AI 参与执行并非工具升级，而是生产力结构的重组。</p><ul><li>业务协作模式从“人主导、机辅助”转向“人设目标、机执行”</li><li>组织效率从人力带宽限制，转向系统级并行扩展</li><li>核心能力从经验积累，转向推理能力与接口能力</li><li>风险控制从事后复核，转向权限管理与执行反馈</li></ul><p>这意味着，企业竞争的关键不再是部署了多少 AI 工具，而是是否拥有能够稳定、安全执行复杂业务目标的执行型 AI 系统。</p><p>AI 正从后台支持角色，走向业务一线。这一变化，正在重塑企业的价值链条。</p>]]></description></item><item>    <title><![CDATA[AI编程实践：从Claude Code实践到团队协作的优化思考｜得物技术 得物技术 ]]></title>    <link>https://segmentfault.com/a/1190000047575197</link>    <guid>https://segmentfault.com/a/1190000047575197</guid>    <pubDate>2026-01-27 15:05:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、开发痛点：为什么我们需要AI编程辅助？</h2><p><strong>核心发现：</strong> AI编程工具正在重塑开发流程，但真正的价值不在于替代开发者，而在于构建人机协作的新型开发范式。Claude Code通过精准对话流设计、模块化任务分解和专业化子代理协作，在提升开发效率的同时，也面临着上下文管理、协作边界和质量控制等实际挑战。</p><p>作为一线开发者，我们每天都在与复杂的业务逻辑和不断迭代的技术栈打交道。不知道你是否也遇到过这些场景：刚理清一个复杂业务流程，被打断后又得重新梳理思路；接手一个老项目，花了半天还没搞懂其中某个模块的设计思路；或者在不同项目间切换时，总要重新适应不同的编码规范和架构风格。</p><p><strong>日常开发的三个"拦路虎"：</strong></p><ul><li><strong>上下文切换成本高：</strong> 需求理解→技术选型→代码实现→质量验证的切换过程中，每次都要重新构建认知框架。</li><li><strong>知识传递效率低：</strong> 项目规范、架构经验分散在文档和个人经验中，新成员上手或跨模块开发时处处碰壁。</li><li><strong>开发流程割裂：</strong> 需求→设计→编码→审查各环节串行传递，信息易失真且反馈滞后。</li></ul><p>这些问题不是简单的"加人"或"加班"能解决的。我们需要的是一种新的开发范式，而Claude Code这类AI编程工具正是在这样的背景下进入了我们的视野。它的价值不在于替我们写代码，而在于成为我们的"认知放大器"和"流程协作者"。</p><h2>二、Claude Code核心功能解析：从工具到方法论</h2><p>Claude Code构建了一套完整的AI辅助开发方法论。接下来将结合团队实际使用经验，从功能特性、使用场景和设计初衷三个维度，详细介绍其核心功能：</p><h3>精准对话流设计：控制AI思考的艺术</h3><p>第一次用Claude Code时，就像面对一个热情但经验不足的实习生——如果不明确告诉他要做什么、怎么做、有什么要求，他很可能会给你一个"惊喜"。对话流设计就是解决这个问题的关键。</p><p><strong>设计初衷：</strong> 对话流设计的本质是将人类的编程思维模式转化为AI可理解的结构化交互方式，通过明确的上下文管理和约束条件设置，引导AI生成符合预期的代码结果。</p><p><strong>核心功能</strong></p><p>对话流设计通过三个关键机制控制AI的思考过程：</p><ul><li><strong>上下文聚焦：</strong> 要求单次对话仅处理一个功能模块，避免多任务混合导致的AI注意力分散。我们曾经试过在一个对话里同时让AI处理多个模块，结果它把两个模块的错误处理逻辑混在了一起。</li><li><strong>约束明确化：</strong> 通过具体指令减少AI的自由度，比如"仅修改X包下文件"、"必须复用Y工具类"。这些约束要尽可能具体，比如不说"遵循项目规范"，而是说"使用ResultDTO作为统一返回格式，错误码规则参考ErrorCodeEnum"。</li><li><strong>增量式提问：</strong> 采用"先框架后细节"的提问策略，先让AI生成接口定义和整体框架，待确认后再逐步深入实现细节。这种方式很像我们带新人时"先搭骨架再填肉"的指导方法。</li></ul><p><strong>使用心法</strong></p><p>启动新功能开发时，我们会创建专用对话线程，并在初始prompt中明确四件事：</p><ol><li>当前任务的功能边界和目标（做什么，不做什么。）</li><li>必须遵守的技术约束和规范（用什么技术栈，遵循什么标准。）</li><li>期望的输出格式和交付物（要代码？要文档？还是两者都要？）</li><li>分阶段的实现计划（先设计接口，再实现逻辑，最后写测试。）</li></ol><p><strong>真实踩坑经验</strong></p><p>处理跨模块依赖时，我们发现AI很容易"忘记"之前设定的约束。后来我们总结出一个技巧：每开始一个新的实现阶段，就简要回顾一下关键约束。比如："现在我们要处理任务交接流程，请记得：1. 使用Redis分布式锁；2. 需要修改商运关系和新商成长任务；3. 异常处理要符合规范。"</p><h3>Plan模式：复杂任务的系统化分解</h3><p>面对"实现一个完整的拜访任务系统"这样的复杂需求，直接让AI生成代码就像让一个刚入行的开发者独立负责整个项目——结果往往是逻辑混乱、漏洞百出。Plan模式就是解决这个问题的"项目管理工具"。</p><p>现状与问题：我们早期使用Claude Code时，经常犯一个错误：把一个复杂需求一股脑丢给AI，然后期待它给出完美解决方案。结果通常是：</p><ul><li>实现逻辑不完整，有些边界情况根本没考虑到。</li><li>模块间接口设计不一致，调用起来磕磕绊绊。</li><li>技术选型不合理，用了个"看起来很酷但项目中并不适用"的方案。</li></ul><p>这其实不能怪AI，人在面对过于复杂的问题时也会手足无措。我们需要一种方法把大问题拆分成小问题，而Plan模式正是借鉴了项目管理中的WBS（工作分解结构）思想。</p><p>我们的解决方案：Plan模式使用三步法分解需求：</p><ol><li>需求分析与模块划分：把整体需求分解为独立的功能模块。</li><li>技术方案设计：为每个模块确定实现思路和技术选型。</li><li>任务优先级排序：根据依赖关系和重要性确定实现顺序。</li></ol><p><strong>实际操作示例：拜访任务需求</strong></p><p>拜访任务是一个简单的企业内部任务需求，在实现"拜访任务需求"时，我们通过Plan模式生成的任务清单是这样的：</p><pre><code>---
拜访任务线上化 - 模块清单
M1: 任务创建模块
- 功能: 创建拜访任务,包含基本信息、拜访对象、参与人员
- 复杂度: Medium (需要处理多表关联和事务)
M2: 任务审批模块
- 功能: 飞书审批流程集成,支持任务提交、审批、驳回
- 复杂度: High (涉及飞书审批接口集成和状态流转)
M3: 日程同步模块
- 功能: 将拜访任务同步到飞书日历
- 复杂度: Medium (飞书API调用和异常处理)
M4: 任务列表查询模块
- 功能: 分页查询任务列表,支持多维度筛选
- 复杂度: Medium (ES聚合查询和数据转换)
M5: 任务详情查询模块
- 功能: 查询任务详情,包括拜访对象、参与人员、商家信息
- 复杂度: Low (简单的关联查询)
M6: 任务统计模块
- 功能: 统计不同类型、不同状态的任务数量
- 复杂度: Low (聚合统计)
M7: 拜访结果提交模块
- 功能: 提交拜访记录,上传附件,记录拜访内容
- 复杂度: Medium (文件上传和多表操作)
M8: 任务分配模块
- 功能: 批量分配任务给运营人员
- 复杂度: High (涉及分配策略和定时任务)
M9: 任务触达模块
- 功能: 任务开始、结束、上传提醒通知
- 复杂度: Medium (飞书消息卡片模板)
M10: ES数据同步模块
- 功能: 将任务数据同步到ES,支持全文检索
- 复杂度: Medium (事件驱动和ES批量操作)
M11: 任务导入模块
- 功能: Excel批量导入拜访任务
- 复杂度: High (Excel解析、数据校验、批量插入)
M12: 任务状态变更模块
- 功能: 定时更新任务状态(未开始→进行中→已结束)
- 复杂度: Low (定时任务和批量更新)
---
第二步:技术方案设计
| 模块       | 数据存储               | 查询方案         | 外部集成          |
|------------|------------------------|------------------|-------------------|
| M1-任务创建  | MySQL (事务)           | -                | -                 |
| M2-任务审批  | MySQL + 审批记录表      | -                | 飞书审批API       |
| M3-日程同步  | -                      | -                | 飞书日历API       |
| M4-任务列表  | ES (主) + MySQL (辅)   | ES聚合查询       | -                 |
| M5-任务详情  | MySQL                  | 联表查询         | 商家信息API |
| M6-任务统计  | ES                     | ES聚合统计       | -                 |
| M7-结果提交  | MySQL + OSS            | -                | OSS文件上传       |
| M8-任务分配  | MySQL                  | 分配算法         | -                 |
| M9-任务触达  | -                      | -                | 飞书消息卡片      |
| M10-ES同步  | ES                     | Spring Event     | -                 |
| M11-任务导入 | MySQL                  | Excel 解析    | -                 |
| M12-状态变更 | MySQL                  | JOB定时任务  | -                 |
---
第三步:任务优先级排序
P0 核心流程(必须先实现)
1. M1-任务创建 (基础功能)
2. M5-任务详情 (数据展示)
3. M4-任务列表 (核心查询)
4. M7-结果提交 (闭环流程)
P1 审批与通知(核心功能增强)
5. M2-任务审批 (依赖M1)
6. M3-日程同步 (依赖M2审批通过)
7. M9-任务触达 (依赖M1、M7)
P2 运营功能(效率提升)
8. M8-任务分配 (依赖M1)
9. M11-任务导入 (依赖M1)
10. M6-任务统计 (依赖M4)
P3 技术优化(性能优化)
11. M10-ES同步 (依赖M1、M4)
12. M12-状态变更 (独立模块)
---
实现建议
阶段一:基础能力
- 先实现 M1、M5、M4,打通任务创建→查询→详情的基本链路
- 使用 MySQL 实现,暂不接入 ES
阶段二:审批流程
- 实现 M2、M3,接入飞书审批和日历
- 完成任务的完整生命周期管理
阶段三:结果闭环
- 实现 M7,完成拜访结果提交
- 实现 M9,添加任务触达通知
阶段四:运营优化
- 实现 M8、M11、M6
- 提升运营效率
阶段五:性能优化(按需)
- 实现 M10,将查询迁移到 ES
- 实现 M12,优化任务状态管理</code></pre><p>这种方式不仅让AI能够有序工作，也让我们对整个开发进度有了更清晰的把控。</p><h3>系统提示词：给AI立"规矩"的艺术</h3><p>如果把Claude Code比作一个新加入团队的开发人员，系统提示词（CLAUDE.md）就相当于给他的"入职手册"，告诉他团队的编码规范、工作流程和注意事项。</p><p><strong>新手常犯的错误：</strong> 把系统提示词写成"百科全书"，恨不得把所有项目知识都塞进去。结果AI要么忽略大部分内容，要么在生成代码时顾此失彼。我们早期的系统提示词长达5000字，包含了从架构设计到代码规范的所有内容，效果反而不好。</p><p>实践心得：有效的系统提示词应该像"护栏"而非"详尽手册"。我们发现，针对AI常见错误模式设计的针对性提示，远比全面但泛泛的规范更有效。现在我们的系统提示词控制在200字以内，只包含最关键的约束和指引。</p><p><strong>系统提示词模板</strong></p><p>经过多次迭代，我们总结出包含三个关键模块的系统提示词结构：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575199" alt="" title=""/></p><p><strong>使用技巧</strong></p><p>分享几个在实践中总结的系统提示词编写技巧：</p><ul><li><strong>避免信息过载：</strong> 不要试图包含所有知识，而是指引AI在需要时查询特定文档。例如："遇到分布式事务问题时，请参考/doc/分布式事务最佳实践.md文档中的TCC模式实现方案"。</li><li><strong>提供正向引导：</strong> 不仅说"不要做什么"，更要明确"应该怎么做"。例如，不说"不要使用过时的API"，而说"请使用OrderServiceV2替代OrderServiceV1。</li><li><strong>动态调整策略：</strong> 我们每两周会回顾一次系统提示词的有效性，根据AI最近常犯的错误补充新的约束。比如发现AI经常忘记处理空指针，就新增一条："所有方法入参必须进行非空校验，使用ValidateUtil.isEmpty()方法，异常时抛出IllegalArgumentException"。</li></ul><h3>SKILL与MCP：知识沉淀与外部能力扩展</h3><p>在团队协作中，我们经常说"不要重复造轮子"。同样，在使用Claude Code时，我们也需要一种机制来沉淀和复用那些有效的Prompt和解决方案——这就是SKILL和MCP机制的价值所在。</p><p><strong>SKILL机制：</strong> 把好经验变成"可复用组件"</p><p>SKILL本质上是将单次生效的Prompt指令沉淀为可反复调用的标准化复用资产。举个例子，我们团队处理"ES数据查询"逻辑时，总结出了一个内部版本的SDK。我们把这个SDK的调用方式封装成一个SKILL，以后遇到类似场景，只需调用这个SKILL，AI就能按照我们团队的最佳实践来实现。</p><p><strong>MCP协议：</strong> 让AI能"调用"外部工具</p><p>MCP（模型上下文协议）解决了AI与外部工具、数据源的连接问题。通过MCP，AI不再局限于静态知识，而是能够动态访问实时数据。我们集成了飞书MCP服务器，让AI能够直接操作飞书平台，如自动生成技术方案文档、读取PRD需求、同步数据到多维表格等。</p><p><strong>最适合封装为SKILL的场景</strong></p><p>1.<strong>复杂工具使用指南：</strong> 如"ElasticSearch接入"、"Redis缓存更新策略"等需要特定知识的场景。</p><p>2.<strong>常见错误处理模板：</strong> 如"分布式锁冲突处理"、"数据库乐观锁重试机制"等反复出现的问题解决方案。</p><p><strong>MCP协议的典型应用场景</strong></p><ul><li><strong>场景1: 自动生成技术方案文档</strong></li><li>AI分析需求后，通过飞书MCP调用feishu_create_doc；</li><li>直接在指定的知识库目录创建格式化的技术方案文档；</li><li>省去手动复制粘贴的繁琐步骤。</li><li><strong>场景2: 读取PRD需求</strong></li><li>用户提供飞书文档链接；</li><li>AI通过feishu_get_doc_content获取文档内容；</li><li>基于完整需求信息生成技术方案和实现计划。</li></ul><ul><li><strong>场景3: 数据同步到多维表格</strong></li><li>代码生成后的统计数据(如代码行数、涉及文件等)；</li><li>通过feishu_append_bitable_data自动追加到飞书多维表格；</li><li>便于团队追踪AI编程效率指标。</li></ul><h2>三、对话流设计方法论：让AI"懂"你的真实需求</h2><p>刚接触Claude Code时，我们采用的是简单直接的"需求-响应"模式：开发者描述需求，AI生成代码，开发者修改调整。这种模式在处理简单功能时还行，但遇到复杂场景就会出问题。</p><h3>现状分析：传统对话模式的局限性</h3><p>我们早期在项目中踩过的三个坑：</p><p><strong>三大典型问题：</strong></p><ul><li><strong>需求表达不完整：</strong></li></ul><p>开发者说"实现一个商家信息查询接口"，AI生成了基础的CRUD代码，但没有考虑商家数据权限、数据脱敏、缓存策略等实际业务需求 ；</p><p>实现任务时，只描述了"需要任务分配功能"，结果AI生成的代码没有处理任务池、任务优先级、分配策略等核心逻辑。</p><ul><li><strong>上下文管理混乱：</strong></li></ul><p>一个对话持续了十几轮后，AI开始忘记我们前面确定的"使用MyBatis-Plus + BaseMapper"的设计决策，擅自改成了JPA Repository模式； </p><p>在实现相关功能时，早期确定的DTO转换规范在后续模块中被遗忘，导致代码风格不一致。</p><ul><li><strong>迭代反馈滞后：</strong></li></ul><p>等AI生成完整的Service + Controller + Repository代码后才发现方向不对，比如数据库表设计与现有架构冲突，不得不从头再来，浪费了大量时间；</p><p>实现触达功能时，生成的飞书消息发送代码没有考虑现有的FeishuClient封装，重复造了轮子。</p><h3>核心问题：为什么AI总是"听不懂"？</h3><p>深入分析后，我们发现传统对话模式失败的根源在于三个核心矛盾：</p><p><strong>语义鸿沟</strong></p><p>自然语言描述的模糊性与代码逻辑的精确性之间的差距。我们说"这个接口要安全"，AI可能理解为"需要登录校验"，而我们实际想要的是：</p><ul><li>使用项目中的@Permission注解进行权限校验。</li><li>参数需要使用ValidatorUtil进行校验。</li><li>敏感操作需要记录操作日志。</li></ul><p><strong>约束衰减</strong></p><p>随着对话推进，早期设定的技术约束在AI理解中的权重逐渐降低。就像我们记笔记时，重要的事情要反复强调。比如：</p><ul><li>第1轮对话强调"必须继承BaseServiceImpl"。</li><li>第5轮对话AI可能忘记这个约束，直接实现了一个独立的Service类。</li><li>第10轮对话可能连项目的分层架构都混淆了。</li></ul><p><strong>目标偏移</strong></p><p>在多轮对话中，AI容易过度关注当前细节而忽视整体目标。比如讨论某个接口的参数设计时：</p><ul><li>AI可能会纠结于参数名称是否优雅。</li><li>而忽略了这个接口的核心业务价值是"快速检索符合条件的商家"。</li><li>结果生成的代码参数命名很完美，但缺少了分页、排序等实际必需的功能。</li></ul><h3>解决方案：结构化对话设计方法</h3><p>针对这些问题，我们团队总结出一套"三阶段对话模型"，现在已经成为我们使用Claude Code的标准流程：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575200" alt="" title="" loading="lazy"/></p><p><strong>阶段一：需求定义——把"要做什么"说清楚</strong></p><p>这个阶段的目标是确保我们和AI对需求达成共识。我们会用"用户故事+验收标准"的格式来描述需求：</p><p><strong>示例1：新商户成长任务分配</strong></p><pre><code>【用户故事】
作为新商户运营，我需要一个任务分配功能，以便将成长任务高效分配给运营人员
【验收标准】
 - 支持从任务池中按优先级(P0/P1/P2)筛选待分配任务
 - 支持指定运营人员进行任务分配，需校验运营人员是否有权限
 - 分配时需检查运营人员当前任务负载，超过上限时提示"当前任务数已达上限"
 - 分配成功后需发送飞书消息通知运营人员，消息内容包含任务详情和截止时间
 - 操作需记录到表，包含操作人、操作时间、任务ID、分配对象</code></pre><p><strong>示例2：商家数据权限查询</strong></p><pre><code>【用户故事】
作为商家运营，我需要一个商家信息查询接口，查询结果需要根据我的数据权限进行过滤
【验收标准】
 - 支持按商家ID、商家名称、商家状态进行查询
 - 支持分页查询，默认每页20条，最大100条
 - 查询结果需要根据当前用户的数据范围进行过滤
 - 商家敏感信息(手机号、身份证号)需脱敏处理
 - 接口需要权限校验，至少具有"商家查看"权限
 - 查询条件需记录到操作日志，便于审计</code></pre><p><strong>阶段二：边界明确——确定"怎么做"的约束条件</strong></p><p>在这个阶段，我们会明确技术栈选择、架构设计和各种约束条件。关键是要区分"必须遵守"和"建议参考"的约束：</p><p><strong>示例1：新商户成长任务模块</strong></p><pre><code>【技术约束】
必须遵守:
 - 使用SpringBoot标准分层架构,所有Service继承OcsBaseServiceImpl
 - 数据库操作使用MyBatis-Plus,实体类继承BaseEntity,Mapper继承BaseMapper
 - 接口返回统一使用Result&lt;T&gt;格式,错误码使用ErrorCode
 - 权限校验使用@Permission注解,参数校验使用@Valid + ValidatorUtil
 - 飞书消息发送必须使用FeishuClient,不要重复实现
建议参考:
 - 任务状态流转参考TaskServiceImpl中的状态机模式
 - 批量分配操作参考AssignImportHandler中的异步处理方式
 - 运营人员权限校验参考OperatorRelationServiceImpl
 - 数据权限过滤参考ScopeServiceImpl中的范围查询逻辑
【数据库约束】
 - 新增表必须包含created_at, updated_at, is_deleted字段
 - 表名使用ocs_前缀,字段名使用蛇形命名法
 - 索引设计需考虑查询场景,高频查询字段必须建立索引
 - 外键约束通过代码层面维护,不在数据库层面创建</code></pre><p><strong>示例2：机器人问答功能</strong></p><pre><code>【技术约束】
必须遵守:
 - Controller层使用@RestController + @RequestMapping,路径遵循/api/v1/{module}/{action}格式
 - Service层业务逻辑必须有事务控制,使用@Transactional(rollbackFor = Exception.class)
 - DTO转换使用项目中的ConvertUtil,不要手动赋值
 - 第三方API调用(如Dify)必须有重试机制和降级策略
 - 敏感配置(API Key)必须从配置中心读取,不要硬编码
建议参考:
 - 对话上下文管理参考RobotServiceImpl中的会话ID生成逻辑
 - 消息发送失败重试参考FeishuMessageHandler中的重试策略
 - 错误处理参考GlobalExceptionHandler中的统一异常处理
【集成约束】
 - Dify API调用统一通过DifyClient,响应格式需转换为项目标准格式
 - 飞书机器人回调处理需实现签名校验,参考FeishuEventHandler
 - 所有外部API调用需记录请求日志到integration_log表 </code></pre><p><strong>阶段三：迭代反馈——在"做的过程"中持续对齐</strong></p><p>这个阶段的核心是增量验证，避免一次性生成大量代码后才发现方向错误。</p><p><strong>实践要点：</strong></p><p>1.分模块实现，逐个验证</p><ul><li>先实现核心接口(如任务分配)，验证通过后再实现辅助功能(如任务查询)。</li><li>先实现Service层核心逻辑，验证业务流程正确后再补充Controller和参数校验。</li></ul><p>2.关键节点主动暂停</p><ul><li>数据库表设计完成后，先让开发者确认表结构再继续。</li><li>核心业务逻辑实现完成后，先通过单元测试验证再生成Controller。</li><li>集成第三方API前，先确认调用方式和参数映射。</li></ul><p>3.持续同步技术方案</p><ul><li>发现架构设计与现有代码不一致时，立即更新tech-solution.json和tech-solution.md。</li><li>新增数据表、修改核心逻辑时，同步更新技术方案文档。</li><li>每个模块完成后，更新模块状态为implemented → reviewed → verified。</li></ul><p><strong>对话示例：</strong></p><pre><code>开发者: "实现新商户成长任务分配功能"
AI: "我将分三步实现:
1. 先设计数据库表结构(ocs_task_assignment表)
2. 实现核心分配逻辑(NewMerchantTaskAssignService)
3. 实现Controller层接口
现在开始第一步,设计的表结构如下:
[展示表结构]
请确认表结构是否符合需求?"
开发者: "字段没问题,但索引需要加上(operator_id, status)"
AI: "已更新索引设计,现在开始实现Service层核心逻辑...
[生成Service代码]
核心逻辑已实现,包含:
- 任务池查询(按优先级过滤)
- 运营人员负载检查
- 任务分配和状态更新
- 飞书消息发送
请验证业务逻辑是否正确?"
开发者: "逻辑正确,继续实现Controller"
AI: "开始实现Controller层...
[生成Controller代码]
已完成模块M1实现,更新技术方案状态为implemented"</code></pre><p><strong>对话设计三原则与常见问题应对策略</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575201" alt="" title="" loading="lazy"/></p><h3>为什么这样设计：背后的认知科学原理</h3><p>这种结构化对话设计不是凭空想出来的，而是基于我们对人类认知过程的理解：</p><ul><li><strong>工作记忆限制理论：</strong> 就像我们一次只能记住7±2个信息块一样，AI的上下文理解能力也是有限的。通过分阶段对话和单次聚焦单模块，我们控制了每次交互的认知负荷。</li><li><strong>渐进式知识构建：</strong> 学习和理解是一个渐进过程，先掌握整体框架再深入细节，符合认知规律。这和我们教新人时"先讲架构图，再讲模块间交互，最后讲具体实现"的思路是一致的。</li></ul><h2>四、AI团队协作模式：子代理系统的实践与思考</h2><p>随着团队使用Claude Code的深入，我们发现单个AI助手已经难以满足复杂项目的开发需求——就像一个人再厉害也干不了一个团队的活。于是，我们开始探索让多个AI"角色"协同工作的模式，这就是子代理（SubAgent）系统的由来。</p><h3>团队协作的现状与挑战</h3><p>在传统开发模式中，我们有需求分析师、架构师、开发工程师、测试工程师等不同角色，他们通过文档、会议和代码审查等方式协作。这种模式虽然成熟，但在快节奏的业务迭代中，我们发现了一些问题：</p><p><strong>协作中的三大痛点：</strong></p><ul><li><strong>信息传递损耗：</strong> 需求文档从产品经理到开发再到测试，每经过一个环节就可能产生一些理解偏差。就像玩"电话游戏"，信息传到最后可能已经面目全非。</li><li><strong>责任边界模糊：</strong> 当出现问题时，有时会出现"这是架构设计问题"、"这是实现问题"、"这是测试不充分"的互相推诿。</li><li><strong>反馈周期漫长：</strong> 从需求分析到代码审查，整个流程走下来往往需要几天时间，等发现问题时可能已经投入了大量开发资源。</li></ul><p>这些问题促使我们思考：能不能在Claude Code中模拟团队协作模式，让不同的AI角色各司其职又协同工作？</p><h3>Claude Code的子代理协作模式</h3><p>借鉴了MetaGPT等框架的思想，我们在Claude Code中构建了由多个专业化子代理组成的AI团队协作系统。每个子代理承担特定角色，通过标准化中间产物协同工作。</p><p><strong>核心工作机制：中间产物驱动</strong></p><p>所有子代理通过共享"技术方案文档"进行协作，这个文档就像团队的"共享白板"，包含需求分析、模块划分、实现状态和接口设计等关键信息。每个子代理只负责修改文档中与自己角色相关的部分，确保信息一致性。</p><p><strong>四个核心子代理角色</strong></p><p><strong>技术方案架构师</strong></p><p>负责需求分析、技术方案设计和模块划分。相当于团队里的架构师，输出"技术方案文档"这个"施工蓝图"。</p><p><strong>核心职责：</strong></p><ul><li>需求拆解与模块划分</li><li>技术栈选型与架构设计</li><li>接口定义与数据模型设计</li><li>模块间依赖关系梳理</li><li>技术方案文档编写与维护</li></ul><p><strong>代码审查专家</strong></p><p>负责代码质量审查。扮演技术负责人的角色，从架构合规性、代码规范和稳定性等角度挑毛病。</p><p><strong>核心职责：</strong></p><ul><li>检查代码是否符合架构设计</li><li>验证代码规范和命名约定</li><li>识别潜在性能问题和bug</li><li>评估代码可维护性和扩展性</li><li>提供具体修改建议</li></ul><p><strong>代码实现专家</strong></p><p>专注于代码实现和单元测试编写。就像主力开发工程师，按照架构师设计的蓝图一块块地实现功能。</p><p><strong>核心职责：</strong></p><ul><li>根据技术方案实现代码</li><li>编写单元测试和集成测试</li><li>修复代码审查中发现的问题</li><li>编写API文档和使用说明</li><li>同步更新技术方案实现状态</li></ul><p><strong>前端页面生成器</strong></p><p>专门负责生成符合我们低代码平台规范的前端页面配置。这是针对我们商家域管理后台特点定制的角色。</p><p><strong>核心职责：</strong></p><ul><li>根据接口定义生成前端页面配置</li><li>实现表格、表单、详情页等标准组件</li><li>配置页面权限和数据范围过滤</li><li>优化前端交互体验</li><li>确保符合设计规范和响应式要求</li></ul><p><strong>协作流程</strong></p><p>我们采用"先整体规划，再迭代实现"的工作方式，有点像敏捷开发中的Sprint规划+Daily Scrum：</p><p><strong>1. 整体规划阶段：</strong></p><ul><li>产品经理提供需求文档。</li><li>协调者调用"技术方案架构师"子代理分析需求，生成技术方案文档。</li><li>团队评审技术方案，提出修改意见。</li><li>架构师子代理根据反馈修改方案，直到团队确认。</li></ul><p><strong>2. 单模块迭代阶段：</strong></p><ul><li>协调者从技术方案文档中选取一个模块。</li><li>调用"代码实现专家"生成代码。</li><li>调用"代码审查专家"审查代码。</li><li>实现专家根据审查意见修改代码。</li><li>重复"实现-审查-修改"直到通过。</li><li>更新技术方案文档，标记该模块为"已完成"。</li><li>进入下一个模块。</li></ul><h3>子代理协作的价值与局限</h3><p><strong>实践中的三个显著价值</strong></p><ul><li><strong>专业化分工提升质量：</strong> 每个子代理专注于特定领域，就像专科医院比综合医院在特定疾病上更专业一样。我们发现，专门的代码审查子代理比通用AI能发现更多潜在问题。</li><li><strong>流程标准化降低风险：</strong> 通过技术方案文档和明确的角色分工，开发流程被标准化和可视化。新人加入项目时，只要看技术方案文档就能快速了解整体情况。</li><li><strong>知识沉淀促进复用：</strong> 子代理的专业知识和决策逻辑被编码为可复用的配置和规则，避免了"人走经验丢"的问题。</li></ul><p><strong>遇到的四个实际挑战</strong></p><p><strong>子代理协作的挑战与应对：</strong></p><ul><li><strong>上下文同步问题：</strong> 当技术方案文档更新时，各子代理有时不能立即同步最新信息。解决办法：每次修改文档后，明确通知相关子代理"技术方案中XX部分已更新"。</li><li><strong>协作边界模糊：</strong> 在处理跨模块功能时，出现"该由哪个子代理负责"的困惑。解决办法：在技术方案文档中添加"责任人"字段，明确每个模块由哪个子代理负责。</li><li><strong>灵活性与标准化的平衡：</strong> 高度标准化的流程有时会限制处理特殊情况的灵活性。解决原则：90%的常规情况严格遵循标准流程，10%的特殊情况由人工介入处理。</li><li><strong>错误传递放大效应：</strong> 如果技术方案设计阶段就有问题，这个问题会在后续实现和审查阶段被放大。解决办法：加强技术方案的人工评审环节，确保"地基"打牢。</li></ul><h3>子代理协作的设计思考</h3><p>在设计这套协作模式时，我们有几个关键思考：</p><ul><li><strong>为什么选择"中间产物驱动"而非"直接沟通"？</strong></li><li>直接让子代理之间对话可能更灵活，但会导致沟通成本指数级增加（n个代理就有n(n-1)/2种沟通渠道）。通过"技术方案文档"这个单一事实来源，我们大大降低了协作复杂度，也便于追踪变更历史。</li><li><strong>角色划分的依据是什么？</strong></li><li>我们的角色划分基于软件开发的自然阶段（设计→实现→审查）和专业领域（后端→前端），这符合软件开发生命周期的自然规律。没有盲目追求角色数量，而是根据实际需求逐步增加。</li><li><strong>为什么采用"增量迭代"而非"一次性开发"？</strong></li><li>复杂系统的构建本质上是一个不断学习和调整的过程。增量迭代让我们能够及早发现问题并调整方向，避免在错误的道路上走得太远。这和我们常说的"小步快跑，快速迭代"理念一致。</li></ul><h2>五、实践经验与未来展望</h2><p>经过几个月的Claude Code实践，从最初的"试试看"到现在成为离不开的开发工具，我们积累了一些经验，也对AI编程的未来有了更清晰的认识。</p><h3>实践经验总结</h3><p><strong>人机协作的最佳平衡点：</strong></p><p>我们发现最有效的AI编程模式是"人类主导，AI辅助"，而不是反过来。我们将工作内容分为三类：</p><ul><li><strong>AI主导：</strong> 标准化代码生成（如基础CRUD接口）、单元测试编写、API文档生成等重复性高、规则明确的任务。</li><li><strong>人机协作：</strong> 技术方案设计、复杂逻辑实现、代码审查等需要结合领域知识和创造性思维的任务。</li><li><strong>人类主导：</strong> 需求分析、架构设计、质量决策等高风险、高创造性的任务。</li></ul><p><strong>上下文管理的实用技巧</strong></p><p>管理好对话上下文是用好Claude Code的关键，分享几个我们团队总结的技巧：</p><ul><li><strong>对话线程化：</strong> 为不同功能模块创建独立对话线程。我们曾经在一个对话里讨论三个不同模块，结果上下文混乱到不得不从头开始。</li><li><strong>关键信息锚定：</strong> 重要的技术决策和约束要在对话中反复强调。就像写文章时，核心观点要多次出现。</li><li><strong>文档外化：</strong> 复杂设计和决策要记录在外部文档中，而不是仅依赖对话历史。我们会在对话中引用这些文档："数据库设计详见/doc/db_design.md，特别是索引设计部分"。</li><li><strong>状态可视化：</strong> 通过技术方案文档中的进度标记（如[未开始]、[设计中]、[已实现]、[已审查]），直观跟踪开发状态。</li></ul><p><strong>质量控制的三个关键策略</strong></p><p>使用AI生成代码后，质量控制变得更加重要。我们的做法是：</p><ul><li><strong>多层次验证：</strong> 单元测试（AI生成）+ 集成测试（人工设计）+ 代码审查（人机结合）的三层验证体系。</li><li><strong>渐进式信任：</strong> 从简单、低风险模块开始使用AI，建立信任后再逐步扩展。我们最先用AI生成内部工具，验证没问题后才用于核心业务系统。</li><li><strong>错误模式学习：</strong> 记录AI常犯的错误类型，针对性优化系统提示词。我们有一个"AI错误案例库"，记录了"AI忘记处理分布式锁超时"、"日期格式转换错误"等典型问题及解决方案。</li></ul><h3>AI编程的局限性认知</h3><p>在实践过程中，我们也清醒地认识到AI编程并非万能解决方案，它有几个明显的局限性：</p><ul><li><strong>创造性思维不足：</strong> AI擅长在已有知识范围内进行组合和优化，但在需要突破性创新的场景下表现有限。比如我们尝试让AI设计一个全新的商家结算模型时，它还是会倾向于参考现有模型进行修改，难以跳出固有思维框架。</li><li><strong>上下文理解深度有限：</strong> 尽管Claude Code的上下文窗口已经很大，但对于我们系统中某些"牵一发而动全身"的核心模块，AI还是难以把握其深层设计意图和与其他模块的隐性依赖。</li><li><strong>质量责任边界模糊：</strong> 当AI生成的代码出现质量问题时，责任界定变得复杂。我们的解决办法是：开发者对AI生成的代码负全部责任，就像我们对自己写的代码负责一样。</li><li><strong>领域知识滞后性：</strong> AI对我们公司内部系统的最新变更反应不够及时。为此我们建立了"知识库更新机制"，每月将最新的系统变更和业务规则整理成文档，供AI参考。</li></ul><h3>未来发展方向思考</h3><p>基于这些实践经验，我们对AI编程工具的未来发展有几点思考：</p><ul><li><strong>更智能的上下文管理：</strong> 未来的AI编程工具应该能自动识别相关上下文、追踪依赖关系，并在适当的时候提醒开发者潜在的上下文冲突。就像经验丰富的团队领导，能记住每个人负责的模块和项目的整体情况。</li><li><strong>多模态交互模式：</strong> 除了文本对话，未来可能引入图表、流程图等多种交互方式。有时画一个简单的流程图(PlantUML)，比写几百字描述更能说明问题。</li><li><strong>自适应学习机制：</strong> AI编程工具应该能从团队的使用反馈中学习，适应特定团队的编码风格和业务领域。就像新加入团队的开发者，会逐渐适应团队的工作方式。</li></ul><h2>六、结语：人机协作的新型开发范式</h2><p>回顾这几个月使用Claude Code的经历，我们最大的体会是：AI编程工具的价值不在于替代开发者，而在于构建人机协作的新型开发范式。在这种范式下，人类开发者从繁琐的重复劳动中解放出来，更专注于需求分析、架构设计和质量把控等高价值创造性工作，而AI则承担起代码实现、文档生成和基础验证等标准化工作。</p><p>Claude Code作为我们实践的核心工具，通过精准对话流设计、模块化任务分解和专业化子代理协作，展示了这种新型开发范式的潜力。但我们也认识到，成功的AI编程应用需要"工具+方法论+团队协作"三位一体的系统性变革，其中人的角色从"代码生产者"向"问题解决者"和"质量把控者"转变。</p><p>作为开发者，我们需要保持开放学习的心态，积极探索和适应这种新范式。未来已来，与其恐惧被AI替代，不如学会与AI协作，在人机协作中实现更高的个人价值和团队效能。毕竟，代码只是解决问题的手段，而非目的；AI只是增强我们能力的工具，而真正的创新和价值，始终源于人的智慧和创造力。</p><p><strong>实践启示：</strong> 在AI编程时代，最有价值的开发者不是"写代码最快的人"，而是"最会引导AI、最能把控质量、最能解决复杂问题的人"。掌握与AI协作的技巧，建立系统化的AI辅助开发流程，将成为未来开发者的核心竞争力。我们的经验表明，通过合理设计对话流程、明确分工协作和严格质量控制，AI编程工具能够显著提升团队效能，但这需要整个团队在思维方式和工作流程上的共同转变。</p><h3>往期回顾</h3><p>1.入选AAAI-PerFM｜得物社区推荐之基于大语言模型的新颖性推荐算法</p><p>2.Galaxy比数平台功能介绍及实现原理｜得物技术 </p><p>3.得物App智能巡检技术的探索与实践</p><p>4.深度实践：得物算法域全景可观测性从 0 到 1 的演进之路</p><p>5.前端平台大仓应用稳定性治理之路｜得物技术</p><h3>文 /稚归</h3><p>关注得物技术，每周更新技术干货</p><p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p><p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></description></item><item>    <title><![CDATA[智能体对传统行业冲击：为什么“会用 AI 的老师傅”正在成为核心资产 Agentcometoo ]]></title>    <link>https://segmentfault.com/a/1190000047575218</link>    <guid>https://segmentfault.com/a/1190000047575218</guid>    <pubDate>2026-01-27 15:04:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在智能体逐步进入工业现场的过程中，行业里出现了一个与早期预期不同的现象：技术越先进，对经验的依赖反而越强。 当智能体来了，真正稀缺的，不是单纯懂算法的人，而是那些既理解复杂生产系统、又能驾驭智能体的“数字老师傅”。</p><h4>一、智能体时代，“老师傅”被重新定义</h4><p>传统行业中的老师傅，长期依靠的是无法写进操作手册的经验：对异常工况的直觉判断、对设备极限状态的理解、以及在极端情况下“不能出错”的处理逻辑。这些能力过去被认为难以规模化。</p><p>而在智能体架构下，这类经验开始具备新的承载方式。 “会用 AI 的老师傅”，并不是程序员意义上的技术人员，而是能够将行业判断转化为目标设定、约束条件和评价标准的人。他们把经验输入给智能体，而不是被智能体替代。</p><h4>二、角色变化：从执行者到智能体的“教练”</h4><p>在生产现场，老师傅的核心角色正在发生变化。</p><p>首先，是从“手感判断”到“逻辑抽象”。 过去的经验依赖个人感知，现在需要被拆解为可解释的条件、变量和决策顺序，供智能体理解和复用。</p><p>其次，是对智能体输出的审查能力。 智能体在计算层面可能给出最优解，但在真实工业系统中，最优并不等于可行。对物理边界、材料特性和安全红线的判断，仍然依赖长期积累的行业经验。</p><p>最后，是对结果的持续对齐。 通过反复校正智能体的判断结果，老师傅实际上在构建企业专属的行业模型，使智能体从通用工具演化为岗位级专家。</p><h4>三、为什么单纯的 AI 专业人才不够用</h4><p>在很多落地项目中，一个常见问题是：技术人员能优化模型，但难以定义真正重要的生产变量；而一线人员知道问题在哪里，却无法让系统“听懂”。</p><p>相比之下，具备行业经验的老师傅，更擅长从生产目标出发，判断哪些指标值得被优化、哪些异常必须被严格约束。这种能力并非来自算法训练，而来自真实事故、长期试错和对系统整体性的理解。</p><p>因此，在复杂行业中，智能体的效果上限，往往取决于经验是否被正确地输入和约束。</p><h4>四、实践启示：经验正在被“软件化”</h4><p>越来越多的企业开始意识到，智能体真正放大的不是算力，而是经验。</p><p>这促使三种变化出现： 一是将隐性经验转化为可复用的知识资产； 二是通过自然语言等方式，让经验型人员可以直接参与智能体训练； 三是形成“人评估系统，系统辅助人”的闭环，让经验在使用中不断被固化。</p><h4>五、结论：经验不是被淘汰，而是被放大</h4><p>在智能体深入产业的过程中，经验并没有失去价值，而是成为系统安全性和有效性的最后一道防线。</p><p>真正拉开企业差距的，不是是否使用智能体，而是谁能更快、更完整地把老师傅的判断逻辑转化为智能体可执行的规则。这意味着，未来最重要的人才，将是那些既理解工业现场，又能与智能体协作的人。</p>]]></description></item><item>    <title><![CDATA[通义深度搜索-操作指南 DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047575242</link>    <guid>https://segmentfault.com/a/1190000047575242</guid>    <pubDate>2026-01-27 15:03:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><a href="https://link.segmentfault.com/?enc=Zr6PSJaRDBWBr8QSE1w5eg%3D%3D.1VF2tsgecv1B%2Fm%2BG3B8dsTTZrYJNUS4VWl4rC314Kt8hHpU9%2FNlNrzWIV98bO5fFLoeQMFwUt0tR2dX8icLxXfBL45j7QR4E%2Fpy7RRjDw4glsUmg%2FhXR2pqFd7SeGaZtmOmfpqsE70G64LJ8CilCB9LbwIkB143InBGoSjwT3wR0mbUcG6vapy7HeYnlk5xI" rel="nofollow" target="_blank">通义深度搜索限时免费中，快来使用吧!</a></p><h2> 应用开通</h2><p>1.在阿里云百炼控制台的应用广场中点击<a href="https://link.segmentfault.com/?enc=SCzU%2BXdbTT9cQN2JBEpl8Q%3D%3D.3ZFlQEZ4e04jfY6w%2FkpwGwdsIsDacGohk9Bjp%2BpMeBQ53r%2BBuWpeJxycJrv3HsFMtAhpz9xAL0jWDQUl7Poo%2BRncC85xu%2FC7LStAmn3YBGI%3D" rel="nofollow" target="_blank">通义深度搜索</a>卡片，进入<strong>应用详情</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575245" alt="image" title="image"/></p><p>2.首次试用时，点击右上角<strong>免费开通</strong>完成应用开通。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575246" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575247" alt="image" title="image" loading="lazy"/></p><h2>应用管理</h2><p>点击<strong>我的应用</strong>进入应用管理页面。页面展示所有已创建的应用和应用key等信息，首次使用需要新增应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575248" alt="image" title="image" loading="lazy"/></p><h2>应用配置</h2><p>点击应用卡片或新增应用进入应用配置界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575249" alt="image" title="image" loading="lazy"/></p><h3>1.场景选择</h3><p>根据实际需求选择使用场景，当前可选通用场景、法律场景。</p><h3>2.互联网检索配置</h3><p>开启后支持实时互联网全栈信息检索，提升模型回答准确性及时效性。</p><h4>2.1检索策略</h4><p>在检索策略上，您可以在‘标准版本’和‘自定义版本’中选择一种</p><h5>标准版本</h5><p>标准的检索策略，选择标准版本时，可以进一步根据对于搜索效果与搜索耗时的偏好选择不同的性能版本。</p><ul><li><strong>Max版本</strong>：效果优先，检索更深入，结果更全面，但响应时间较长</li><li><strong>Turbo版本</strong>：速度优先，响应时间短，适合对实时性要求高的场景</li></ul><h5>自定义版本</h5><p>选择自定义检索策略时，有更多的配置进行更细化的配置。</p><ul><li>支持限定检索时间范围</li><li>支持限定网站范围，最多添加20个网站，配置后<strong>优先</strong>从此范围网站检索信息，如果无匹配信息则会扩展到<strong>全网</strong>检索，网站录入时会自动去重</li><li>支持配置recall数量，数量越高信息越全，但会占用更多资源，增加耗时</li><li>支持配置网页读取开关，开启后搜索结果更详细但是耗时增加</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575250" alt="image" title="image" loading="lazy"/></p><h4>2.2策略选择</h4><p>可根据搜索效果与rt偏好选择max版本和turbo版本。</p><h3>3.自有知识库配置</h3><p>支持接入非百炼的自有知识库作为搜索来源，开启选项后可进行配置，点击添加知识库配置</p><p>输入知识库名称、知识库描述、服务地址、授权信息，点击“服务测试”，验证通过后点击“保存”以完成添加。可参考<a href="https://link.segmentfault.com/?enc=sftw7BcNfI7QgW%2BV79Jo7w%3D%3D.YbWCSMiVF83tsvxIiGhv8fWMV8iIJyfL2cP8lLX%2F4Kqz9kOKvIf6JJzbsD7Q%2F8aMRp5zQ59FS9EgCMYqA41deovo7l2w3%2BJ5DU0LrKa79Dk%3D" rel="nofollow" target="_blank">示例文档</a>进行知识库对接配置。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575251" alt="image" title="image" loading="lazy"/></p><h3>4.百炼知识库</h3><p>支持接入<a href="https://link.segmentfault.com/?enc=EwLysIT%2BweOUGjm769a7YQ%3D%3D.NYAI6zrmraYA1sAoAg9vn%2FZ3FPTR4AIEQS%2FDMbcZoCk9wt%2FyEYcOpp0FjNh09MvmGCmzykuNLQW%2BJ3pk0sLYqQ%3D%3D" rel="nofollow" target="_blank">百炼知识库</a>，选择已配置的知识库，如无百炼知识库，需要先在百炼控制台创建知识库。并添加知识库描述，知识库描述需要认真填写易于模型理解。</p><h3>5.code\_interpret</h3><p>开启后提升对于复杂计算问题的效果。</p><h3>6.动态文件解析</h3><p>开启动态文件解析后，支持在输入query同时添加本地文件作为临时上下文知识。一次对话最多可上传10个文件，单文件不超过10MB，支持.docx/.doc/.pdf/.txt/.md等格式。</p><h3>7.生成配置</h3><p>开启输出报告后，对话最终会生成报告文件。关闭则不生成报告。</p><h2>应用测试</h2><p>配置完成后，可在输入框输入query进行测试，对话框展示chat内容、计划规划、思考过程、检索过程、工具调用过程等多个深度搜索研究步骤。最终生成报告文件。右侧报告区域支持‘预览’模式和‘源码’模式。切换到‘源码’模式可查看用于生成报告的Markdown原文。提供文件下载。</p><p><strong>重要</strong></p><p>请注意，在配置页面测试也会计算使用量并产生费用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575252" alt="image" title="image" loading="lazy"/></p><h2>应用发布</h2><p>配置测试完成后，可以点击发布，将应用发布后可正式使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575253" alt="image" title="image" loading="lazy"/></p><hr/><p><a href="https://link.segmentfault.com/?enc=800AqOzGpx01ZgP2Hb1S%2BA%3D%3D.lew9wI7SFmtw5KiEYQOnY%2FW2rDrm0tvUG0OcNCxc0LNTzcHo4fZW9ukSl1viupDDmvloimjok3IDxdelgpuJ1z3IPOvL4gqCX8AmM3xfKQc%3D" rel="nofollow" target="_blank">点击访问：通义深度搜索应用</a></p><p><a href="https://link.segmentfault.com/?enc=1djd%2FDGHc%2BoM8I81Lnl8gw%3D%3D.wVXfyqePbr%2BJIlw3s4GPUQa0gQGy2xKPBHzUTcL%2BZ5Zpaau6j0XcnqW6I0xPSucrN2bimlByJEdHO3kr12KTi8AtOXU5CfTCcxQkizBa8DWaO3i%2FvylT5Gk1tkvOGIJnrFdMjzuKn0G6WL6z388UWeIAOPsyKjgA63x6lVdQlkStgrUSRHzLWCoznzeDQ2M%2BzCqODyoEz4w7K2w%2Bf3l0xaY110YhsQnzmjWEwTvVGDzSbK61avLKswaX8EQEQXIB" rel="nofollow" target="_blank">点击：更多讨论交流</a></p>]]></description></item><item>    <title><![CDATA[从XDG正式支持如意玲珑（Linyaps）看如意玲珑的发展与架构演进 慵懒的猫mi ]]></title>    <link>https://segmentfault.com/a/1190000047575288</link>    <guid>https://segmentfault.com/a/1190000047575288</guid>    <pubDate>2026-01-27 15:02:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如意玲珑是⼀种新型的独⽴包管理⼯具集旨在解决Linux系统下传统软件包格式因复杂依赖关系导致的兼容性问题，以及权限管控松散带来的安全⻛险。通过容器化技术提供应⽤隔离运⾏环境，⽀持应⽤增量更新，从⽽提升软件包管理的效率和安全性。</p><p>本⽂主要讲述如意玲珑的发展与架构的演进 ，这包括以下⼏个⽅⾯：<br/>Linux软件包管理器的演进；<br/>如意玲珑架构设计<br/>如意玲珑使⽤场景；<br/>如意玲珑关键组件设计；<br/>如意玲珑性能测试与对⽐；<br/>如意玲珑的发展成果；<br/>如意玲珑未来展望。</p><p><strong>Linux软件包管理器的演进</strong><br/>Linux操作系统以其开源特性和灵活性著称，⽽软件包管理器是确保Linux系统能够顺利安装和运⾏所需软件的关键组件。<br/>顾名思义，Linux 软件包管理器是⼀种在 Linux 操作系统上⽤于安装、更新和卸载软件包的⼯具。 它的历 史可以追溯到上世纪 90 年代，此时 Linux 正处于起步阶段，软件的安装必须⼿动下载源代码并编译，这对⾮技术⽤户来说是⼀项繁琐且困难的任务。<br/>这种情况下，先后催⽣了 dpkg 和 rpm ，然⽽由于不能⾃动解决依赖关系，其使⽤起来依旧不便。<br/>直到 Debian 的 apt、Red Hat 的 up2date 的发布，包管理器可⽤性有了很⼤的提升。它们采⽤了⼀种被称为 “依赖关系解决器” 的算法，能够⾃动解决软件包之间的依赖关系，从⽽简化软件的安装和升级过程。但这也在另⼀⽅⾯⼤⼤增加了系统复杂度，维护者们需要⾮常谨慎⼩⼼地处理，稍有不慎就会陷⼊“依赖地狱”，导致软件包系统发⽣故障。<br/>此外，还有许多其他的软件包管理器，如 yum、portage 和 pacman 等。包管理器的多样性给⽤户带来了更多选择，但缺点也⼗分显著： 它们的软件包⽆法互通，这意味着⼀款软件要在其他发⾏版上使⽤ ，可能需要被重复打包。<br/>随着Linux内核对容器的⽀持、Docker的诞⽣，Snap、Flatpak 等⼀批容器思想的包管理器也开始崭露头角。这类格式的软件包与系统环境⼏乎完全解耦，不再依赖系统上的库⽂件（AppImage 也是如此），应⽤分发开始逐步变得简单起来。但磁盘、 内存占⽤较⾼，启动时间被不断延⻓等问题也随之⽽来，⾄今仍未被解决。</p><p><strong>如意玲珑架构设计 </strong></p><p>如意玲珑的核⼼设计原则是兼容和安全 ，主要为了解决以下问题：</p><ol><li>解决Snap、Flatpak包管理器应⽤体积过度膨胀 ，Runtime乱⽤导致占⽤过度膨胀、应⽤打开速度过慢 的问题；</li><li>解决应⽤安装时权限过⼤问题 ，严格规范应⽤权限；</li><li>解决应⽤运⾏依赖问题。<br/>基于以上设计原则 ，整体架构如下图所⽰： <br/><img width="688" height="495" referrerpolicy="no-referrer" src="/img/bVdnMFX" alt="image.png" title="image.png"/></li></ol><p>如意玲珑整体采⽤分层设计 ，最底层是硬件平台 ，⽀持不同的CPU架构 ，上层是系统平台也就是各个 Linux发⾏版操作系统。</p><p>再上层是运⾏环境 ，这⼀层就是我们单独抽离出的runtime ， 当前是选取桌⾯应⽤最常⽤的库和依赖包，这样应⽤只需要依赖这个统⼀的稳定的runtime ，⽆需考虑下层的系统平台 ，⽽不在runtime⾥的独有的依赖可以直接打包在应⽤包⾥⾯ ，⽽且runtime也会持续演进 ，演进的原则是兼容性第⼀ ，即在不影响兼容  性的前提下会持续修复缺陷和修复安全漏洞 ，⽽因为新的功能属性的要求导致需要更新⼤版本⽆法保障 兼容性时 ，会新增runtime ，新旧runtime共存互不⼲扰 ，且我们采⽤⽂件共享的⽅式来减少多个runtime对磁盘资源的占⽤ 。<br/>再上层就是玲珑的主要组件 ，包括虚拟化容器、命令⾏接⼝ 、包仓库、⽤户会话辅助服务等组件 ，提供 包管理相关的能⼒ ，⽀持软件包的下载、安装、更新、卸载、运⾏与托管等功能。⽽在最上层 ，还为软 件开发者提供了便捷的包构建⼯具和转换⼯具 ，以及提供了应⽤商店供软件开发者分发应⽤ ，供⽤户下载安装应⽤ 。</p><p><strong>如意玲珑使⽤场景</strong></p><p>解决兼容性冲突问题.  现在企业的应⽤与系统、应⽤与应⽤之间需要完成适配、测试确保⽆兼容性冲突 ，⼀旦应⽤升级或系统或系统升级都有可能导致系统或应⽤⽆法使⽤ ，需要重新适配、测试 ，耗时耗⼒ ，严重影响企业办公和业务运转。<br/>如意玲珑应⽤使⽤隔离技术 ，将系统和应⽤完全解耦 ，客户可随意升级系统或应⽤ 。⼤幅提⾼了易 ⽤性 ，降低了企业维护成本。</p><p><img width="726" height="451" referrerpolicy="no-referrer" src="/img/bVdnMFZ" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>解决恶意软件可能窃取数据问题</strong></p><p>恶意软件可通过多种渠道窃取企业核⼼业务数据 ，若被不法分⼦加以利⽤ ，可能会导致企业数据安全⾯临巨⼤风险 ，甚⾄遭受巨额损失。<br/>如意玲珑提供沙箱让应⽤运⾏在隔离的环境下 ，对设备和数据的访问需要得到授权 ，从⽽保护了企业数据安全和个⼈隐私。</p><p><img width="726" height="442" referrerpolicy="no-referrer" src="/img/bVdnMF6" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>解决应⽤⽣态适配碎⽚化问题</strong></p><p>传统包管理器在Linux下打包流程复杂 ，开发者需要为不同发⾏版分别打包DEB/RPM等格式 ，甚⾄同⼀发 ⾏版的不同版本也需要单独打包。各包管理器之间的软件包互不兼容 ，导致Linux应⽤⽣态碎⽚化严重。<br/>如意玲珑通过提供统⼀的打包格式和⼯具 ，简化了软件打包流程 ，开发者只需关注应⽤本⾝ ，⽆需考虑 底层系统的差异 ，从⽽降低了打包难度 ，提⾼了开发效率。<br/>如意玲珑通过提供统⼀的应⽤商店 ，⽤户可以⽅便地浏览、搜索和安装应⽤ ，提升了⽤户体验 ，促进了应⽤⽣态的发展。</p><p><img width="726" height="324" referrerpolicy="no-referrer" src="/img/bVdnMF7" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>如意玲珑关键组件设计</strong></p><p><strong>ll-cli：如意玲珑命令⾏管理⼯具</strong><br/>提供⽤户与如意玲珑包管理器交互的命令⾏⼯具。负责解析⽤户命令、调⽤ D-Bus ⽅法、处理⽤户交互  请求以及与 OCI 运⾏时直接交互（如 run, exec, ps, kill等）。ll-cli⽀持丰富的⼦命令和选项 ，例如 install(安装  包)、uninstall(卸载包)、upgrade(更新包)、search (搜索包)、list(列出包)、run (运⾏应⽤)、exec(在容器内执⾏ 命令)、ps(列出运⾏中容器)、kill(发送信号给容器)、prune(清理⽆⽤运⾏时)、repo(管理仓库配置)、info(显  ⽰包信息)、content(列出包导出⽂件)。为了提升⽤户体验 ，ll-cli提供 bash 和 zsh 的⾃动补全功能 ，通过⾃⾝命令动态获取补全列表。<br/><strong>ll-package-manager：如意玲珑包管理⼯具</strong><br/>如意玲珑包管理器模块主要为ll-cli提供DBus接⼝调⽤ ，它提供ll-cli的任务管理 ，并负责和ostree仓库进⾏交 互 ，最后将结果返回给ll-cli。完整流程图如下：<br/><strong>ll-box：如意玲珑沙箱</strong><br/>如意玲珑沙箱主要负责应⽤的隔离运⾏环境 ，基于OCI规范实现。ll-box负责创建、启动、停⽌和销毁容器 ，并管理容器的⽣命周期。它还负责配置容器的资源限制、 ⽹络设置和⽂件系统挂载等参数 ，以确保 应⽤在隔离的环境中安全运⾏。 完整流程如下：</p><p><strong>如意玲珑性能测试与对⽐</strong><br/>⽬前主流的软件包管理体系有两类 ，⼀类是传统的包管理体系（例如debian、redhat的包管理体系），另⼀ 类是已有独⽴包格式（例如Flatpak、snap）。两种软件包管理体系各有优势 ，但前者有兼容性和安全的隐患 ，后者有性能和资源占⽤的问题 ，⾄今没有得到解决。玲珑在实现软件包管理的同时 ，更关注企业场景中的实际需求 ，在解决兼容性和安全问题的同时提⾼性能 ，降低资源占⽤ 。以下是对⽐表：<br/><img width="614" height="647" referrerpolicy="no-referrer" src="/img/bVdnMEG" alt="企业微信截图_17694964549748.png" title="企业微信截图_17694964549748.png" loading="lazy"/><br/><img width="615" height="444" referrerpolicy="no-referrer" src="/img/bVdnMEH" alt="企业微信截图_1769496475488.png" title="企业微信截图_1769496475488.png" loading="lazy"/></p><p><strong>如意玲珑的发展成果</strong></p><p>如意玲珑⾃项⽬启动以来 ，已经取得了显著的发展成果：<br/>⼴泛的应⽤⽀持：如意玲珑已⽀持5200余款常⽤桌⾯和终端应⽤ ，涵盖办公、开发、设计等多个领域 ，满⾜⽤户的多样化需求。<br/><img width="726" height="339" referrerpolicy="no-referrer" src="/img/bVdnMEJ" alt="image.png" title="image.png" loading="lazy"/><br/>多架构⽀持：如意玲珑⽀持x86_64、arm64、龙芯LoongArch64等多种CPU架构 ，确保在不同硬件平台 上都能顺利运⾏。<br/>多Linux发⾏版⽀持：如意玲珑兼容主流Linux发⾏版 ，包括Debian、Ubuntu、Fedora等 ，确保⽤户在不 同系统环境下都能享受如意玲珑带来的便利。<br/><img width="726" height="96" referrerpolicy="no-referrer" src="/img/bVdnMEL" alt="image.png" title="image.png" loading="lazy"/><br/>活跃的社区和⽣态系统：如意玲珑拥有⼀个活跃的开发者社区和多个SIG组 ，定期举办线上线下活 动 ，促进开发者之间的交流与合作。 同时 ，越来越多的软件开发者选择将他们的应⽤打包为如意玲珑格式 ，进⼀步丰富了应⽤⽣态。<br/><img width="726" height="321" referrerpolicy="no-referrer" src="/img/bVdnME3" alt="image.png" title="image.png" loading="lazy"/><br/><img width="726" height="313" referrerpolicy="no-referrer" src="/img/bVdnME8" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>如意玲珑未来展望</strong><br/>如意玲珑将继续致⼒于提供更好的⽤户体验和更⼴泛的应⽤⽀持。计划：</p><p>扩展应⽤⽣态：进⼀步增加对更多应⽤的⽀持 ，特别是热⻔的开发⼯具和设计软件 ，以满⾜⽤户的 多样化需求。</p><p>更好的兼容性：通过灵活的配置⽅式、更好的 xdg-desktop-portal 协议⽀持 ，以提升如意玲珑与不同 Linux发⾏版和桌⾯环境的兼容性 ，确保⽤户在各种环境下都能顺利使⽤如意玲珑。.  优化性能：持续优化如意玲珑的性能 ，提升应⽤启动速度和运⾏效率 ，为⽤户提供更流畅的体验。</p><p>加强社区建设：通过举办更多的开发者活动和培训 ，吸引更多的开发者参与到如意玲珑的⽣态中，共同推动项⽬的发展。</p><p>探索新技术：关注前沿技术的发展 ，探索将其应⽤到如意玲珑中的可能性 ，以保持项⽬的创新性和竞争⼒。</p>]]></description></item><item>    <title><![CDATA[如何在Android设备上删除多个联系人（3种方法） iReaShare ]]></title>    <link>https://segmentfault.com/a/1190000047575290</link>    <guid>https://segmentfault.com/a/1190000047575290</guid>    <pubDate>2026-01-27 15:02:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>如果您想清理安卓手机，或者只是想删除旧的、不需要的联系人，或者删除多个联系人，有三种有效的方法可供选择。无论您是想手动删除安卓手机上的联系人，还是使用专用工具，都可以按照以下步骤操作。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575292" alt="图片" title="图片"/></p><p>快速浏览一下这三种方法：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575293" alt="图片" title="图片" loading="lazy"/></p><p>方法1：如何通过“联系人”应用手动删除Android上的联系人</p><p>删除联系人最直接的方法是直接通过安卓设备内置的“通讯录”应用。此方法非常适合一次性删除部分或全部联系人。但是，如果“通讯录”应用中有垃圾箱或回收站，则删除联系人后需要清空垃圾箱，因为已删除的联系人会被移至垃圾箱并保留 30 天。</p><p>手动删除 Android 上的联系人：</p><pre><code>
在安卓手机上找到并点击“通讯录”应用。它通常位于主屏幕或应用抽屉中。


点击要删除的联系人。点击并按住一个联系人，直到出现复选框或选择选项。然后点击要删除的其他联系人以将其选中。


寻找类似垃圾桶的图标，或者标有“删除”或“移除”的选项。这些选项通常位于屏幕顶部或三点菜单内（通常标记为“更多选项”）。


系统可能会提示您确认删除操作。点击“删除”&gt;“确定”即可完成删除。如果应用将您已删除的联系人移至内置回收站，请前往回收站重新删除联系人。之后，您将无法在 Android 设备上访问已删除的联系人。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575294" alt="图片" title="图片" loading="lazy"/></p><p>方法2：如何通过Google通讯录删除Android上的联系人</p><p>大多数 Android 手机都会将联系人同步到您的 Google 帐户。这意味着您可以直接在 Google 通讯录中管理和删除联系人，然后 Google 通讯录会将更改同步回您的 Android 设备。如果您更喜欢通过电脑管理联系人，或者希望确保所有同步设备上的联系人信息一致，此功能尤其实用。</p><p>以下是通过 Google 通讯录从 Android 中删除联系人的方法：</p><pre><code>
在您的计算机或手机上打开网络浏览器并导航至contacts.google.com。


使用与您的 Android 手机关联的同一 Google 帐户登录。


点击要删除的联系人。在联系人详情中，点击三点菜单（更多操作），然后选择“删除”。


要删除多个联系人，您可以将鼠标悬停在联系人的个人资料图片或姓名首字母上，直到出现复选框，然后勾选该复选框；重复此操作，删除所有要删除的联系人。然后点击“更多”&gt;“删除”&gt;“移至垃圾箱”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575295" alt="图片" title="图片" loading="lazy"/></p><pre><code>
已删除的联系人将被移至“已删除邮件”，除非您恢复，否则 30 天后这些联系人将被删除。您也可以清空已删除邮件来移除联系人。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575296" alt="图片" title="图片" loading="lazy"/></p><p>方法3：如何通过iReaShare Android Manager删除Android上的多个联系人</p><p>如果您想在电脑上用大屏幕管理安卓联系人，并轻松删除多个或全部联系人，您可以使用iReaShare Android Manager ，这是一款功能全面的安卓数据管理工具。有了它，您可以在桌面上编辑和删除安卓联系人，并快速将联系人备份到电脑。</p><p>iReaShare Android Manager的主要功能：</p><ul><li>允许您在计算机上预览您的 Android 联系人。</li><li>轻松从 Android 删除特定联系人。</li><li>使您能够一次选择多个或所有联系人，然后删除它们。</li></ul><p>*将您的联系人从 Android 导出到 PC或 Mac 进行备份。</p><ul><li>一键备份您的 Android 数据，并将备份恢复到 Android，不会丢失数据。</li><li>支持Android 6.0或更高版本，包括Android 16。</li></ul><p>以下是使用联系人管理器删除 Android 上的多个联系人的方法：</p><p>以下是使用联系人管理器删除 Android 上的多个联系人的方法：</p><pre><code>
下载并安装后，在您的计算机上启动 Android Manager 软件。


使用 USB 将 Android 手机连接到电脑，并在 Android 设备上激活 USB 调试模式。连接后，点击“通讯录”继续。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047575297" alt="图片" title="图片" loading="lazy"/></p><pre><code>
选择您不再需要的联系人，然后点击“删除”菜单将其从您的 Android 设备中删除。


</code></pre><p>提示：如果您要出售或赠送手机，或者担心数据隐私，仅仅从“通讯录”应用中删除联系人是不够的。这些联系人通常可以使用数据恢复软件恢复。对于真正无法恢复的删除，建议使用像iReaShare Android Data Eraser这样的专业数据擦除工具。</p><p>提示：关于在 Android 上删除联系人的常见问题解答</p><p>问题 1：如果我清空 Android 手机上的“通讯录”应用中的垃圾箱，我是否就完全删除了这些联系人？</p><p>不一定。虽然清空 Android 手机的垃圾箱后恢复的几率会大大降低，但有人可能会使用专门的恢复应用来恢复您已删除但未被新数据覆盖的联系人。如果您想彻底删除联系人，则需要覆盖已删除的联系人。此外，如果您在手机上启用 Google 联系人同步功能，则可以轻松地通过您的帐户恢复联系人。</p><p>Q2：为什么已删除的联系人不断出现？</p><p>如果出现以下情况，则可能会发生这种情况：</p><pre><code>Google 或其他帐户已同步。
联系人存储在只读帐户中（如 WhatsApp 或 Facebook）。
您没有从正确的帐户中删除联系人。

</code></pre><p>要修复此问题，请关闭联系人同步：设置&gt;帐户&gt; [帐户名称]&gt;同步&gt;关闭联系人。</p><p>结论</p><p>在 Android 上删除联系人非常简单，无论您是喜欢手动操作，还是通过数据管理工具iReaShare Android Manager或 Google 帐户操作，都能轻松完成。每种方法都能满足不同的需求——从快速删除到在大屏幕上管理联系人。选择最适合您需求的方法，让您的联系人列表保持整洁有序。<br/>​</p>]]></description></item><item>    <title><![CDATA[ComfyUI具体使用流程 Smoothcloud润云 ]]></title>    <link>https://segmentfault.com/a/1190000047575319</link>    <guid>https://segmentfault.com/a/1190000047575319</guid>    <pubDate>2026-01-27 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>上个帖子已经分享了如何在润云<a href="https://link.segmentfault.com/?enc=EgWWHAkcZ2tvJgHtEqI7iQ%3D%3D.r9%2BOXgiuaKWz53G9AUVAUyL3Sxl8%2FK0O9r0XjFXmipM%3D" rel="nofollow" target="_blank">进入ComfyUI界面。</a></p><p>下面我来具体分享ComfyUI的使用方法</p><h2>一、文生图</h2><p><img width="723" height="333" referrerpolicy="no-referrer" src="/img/bVdnMFk" alt="" title=""/></p><p>界面上的节点和整个画布都可以拖动，也可以放大缩小。</p><p>ComfyUI 为我们提供了一个默认的文生图工作流。直接点击右边的 Queue Prompt 就能够触发生图流程，你可以看到有个绿色的边框会随着流程的进展在不同的节点上显示。</p><p>整个工作流由两个基本的部分组成：节点（Nodes）和边（Edges）。</p><p>• 每一个矩形块就是一个节点，比如 Load Checkpoint CLIP Text Encoder 等。可以把每个节点看成是函数，它们都具有输入、输出和参数三个属性。</p><p>• 连接每个节点的输入和输出的线就是边。</p><p>其他还有很多的细节和概念，我将会在接下来的内容中逐步解释。</p><p>我们直接从这个默认工作流开始，它包含了下面这些步骤。</p><h3>1.1 选择模型</h3><p>首先需要在 <code>Load Checkpoint</code> 这个节点中选择一个模型，这里的模型选项就是在上文中下载的那些模型文件。比如我这里就放置了多个可选的模型，我可以根据自己的需求选择我想要使用的模型。</p><p><img width="348" height="192" referrerpolicy="no-referrer" src="/img/bVdnMFl" alt="" title="" loading="lazy"/></p><h3>1.2 构造提示词</h3><p>选择完模型，下一步就是构造提示语了。</p><p>在界面上，有两个 CLIP Text Encode (Prompt) 节点，这两个节点都是用来构造我们的提示语的。</p><p>其中，上面一个节点用来输入正向提示语（Positive Prompt），即告诉模型做什么，而下面一个节点则用来输入负面提示语（Negative Prompt），即告诉模型不要做什么。</p><p>如果觉得容易混淆，可以像我这样直接双击节点名称改成它对应的功能的名称，就像下面这样。</p><p><img width="451" height="450" referrerpolicy="no-referrer" src="/img/bVdnMFm" alt="" title="" loading="lazy"/></p><p>下面的节点也可以看出哪个是正向哪个是负向</p><p><img width="723" height="286" referrerpolicy="no-referrer" src="/img/bVdnMFn" alt="" title="" loading="lazy"/></p><p>CLIP Text Encode 节点的作用是将提示语转换为标记，然后通过文本编码器将它们处理为嵌入（Embeddings）。</p><p>你可以使用 (关键词:权重) 的这样的语法来控制关键词的权重。</p><p>比如，使用 (keyword:1.4) 来增强效果，或 (keyword:0.9) 来减弱效果。</p><h3>1.3 生成图像</h3><p>点击下方的 <code>Run</code>，等待一会儿就能够看到有一张图像生成完成了。</p><p><img width="584" height="729" referrerpolicy="no-referrer" src="/img/bVdnMFo" alt="" title="" loading="lazy"/></p><h2>二、ComfyUI 的工作机制</h2><p>ComfyUI 的强大之处就在于它的高度可配置性。熟悉每个节点的功能之后可以让我们轻易地根据需求来定制化操作。</p><p>在介绍图生图工作流之前，我需要先向你详细介绍一下 ComfyUI 的工作机制。</p><p>Stable Diffusion 的生图过程可以总结为以下三个主要步骤：</p><ul><li>文本编码：用户输入的提示语通过一个称为文本编码器（Text Encoder） 的组件编译成各个单词的特征向量。这一步将文本转换为模型可以理解和处理的格式；</li><li>潜在空间（Latent space）转换：来自文本编码器的特征向量与一个随机噪声图像一起被转换到潜在空间。在这个空间中，随机图像根据特征向量进行去噪处理，得到一个中间产物。这一步生图过程的是关键所在，因为模型会在这里学习将文本特征与视觉表现相联系。</li><li>图像解码：最后，潜在空间中的中间产物由图像解码器（Image Decoder） 进行解码，转换为我们可以看到的实际图像。</li></ul><p>了解了 Stable Diffusion 层面的生图流程之后，接下来我们深入了解一下 ComfyUI 在实现这个过程中的关键组件和节点。</p><h3>2.1 Load Checkpoint 节点</h3><p><img width="499" height="263" referrerpolicy="no-referrer" src="/img/bVdnMFG" alt="" title="" loading="lazy"/></p><p><code>Load Checkpoint</code> 节点会加载一个模型，一个 Stable Diffusion 模型主要包含以下三个部分：</p><ul><li>MODEL</li></ul><p>MODEL 组件是一个在潜在空间（Latent Space）中运行的噪声预测模型。</p><p>这句话的意思是 Stable Diffusion 模型在潜在空间中对图像的生成过程进行建模，并通过预测和去除噪声逐渐还原图像的过程。</p><p>具体来说就是，在 Stable Diffusion 中，图像生成首先在潜在空间中引入随机噪声，然后模型通过一系列步骤逐渐去除这些噪声，生成符合提示语的图像。</p><p>这种逐步去噪的过程由噪声预测模型来完成。潜在空间是图像的一个简化、高度抽象化的表示，可以降低模型的计算复杂度，可以让模型在生成图像时更高效。</p><p>在 ComfyUI 中，Load Checkpoint 节点的 MODEL 输出连接到 KSampler 节点，KSampler 节点执行反向扩散过程。</p><p>KSampler 节点利用 MODEL 在潜在表示中进行迭代去噪，逐步优化图像，直到它符合给定的提示语。</p><ul><li><h5>CLIP (Contrastive Language-Image Pre-training)</h5></li></ul><p>CLIP 其实是一个负责预处理用户提供的正向和负面提示语的语言模型。它将文本提示转换为 MODEL 可以理解的格式，指导图像生成过程。</p><p>在 ComfyUI 中，Load Checkpoint 节点的 CLIP 输出连接到 CLIP Text Encode 节点。CLIP Text Encode 节点获取用户提供的提示语，并将它们输入到 CLIP 语言模型中，转换为向量嵌入。</p><p>这些向量嵌入可以捕捉单词的语义，为 MODEL 生成符合提示语的图像提供更多的指导。</p><ul><li><h5>VAE (Variational AutoEncoder)</h5></li></ul><p>它包含一个编码器和一个解码器，其中，编码器用于将图像压缩为低维的潜在表示，而解码器用于从潜在表示中重建图像。</p><p>在文生图的过程中，VAE 仅在最后一步使用，它的作用就是将生成的图像从潜在空间转换回像素空间。</p><p>ComfyUI 中的 VAE Decode 节点获取 KSampler 节点的输出，并利用 VAE 的解码器部分将潜在表示转换为最终的像素空间图像。</p><p>VAE 与 CLIP 语言模型是独立的组件。CLIP 主要处理文本提示语，而 VAE 负责在像素空间和潜在空间之间进行转换。</p><h3>2.2 CLIP Text Encode 节点</h3><p><img width="475" height="440" referrerpolicy="no-referrer" src="/img/bVdnMFT" alt="" title="" loading="lazy"/></p><p>在上文中有提到，在 CLIP Text Encode 节点中我们可以输入生成图像的提示语，而这个节点的作用就是获取我们提供的提示语，并将其输入到 CLIP 语言模型中。</p><p>CLIP 是一个强大的语言模型，能够理解单词的语义并将其与视觉概念相关联。当提示语输入到 CLIP Text Encode 节点后，它会将每个单词转换为向量嵌入。向量嵌入是高维的数字表示，包含了单词的语义信息，模型能够根据这些信息生成符合提示语的图像。</p><h3>2.3 Empty Latent Image 节点</h3><p><img width="317" height="162" referrerpolicy="no-referrer" src="/img/bVdnMFU" alt="" title="" loading="lazy"/></p><p>在 ComfyUI 的文生图的过程中，它首先会在潜在空间中生成一个随机图像，这个图像会作为模型处理的初始状态，它的大小与实际像素空间中的图像尺寸成比例。</p><p>在 ComfyUI 中，我们可以调整潜在图像的高度和宽度来控制生成图像的大小。此外，我们还可以设置批处理大小来确定每次运行生成的图像数量（batch_size）。</p><p>潜在图像的最佳尺寸取决于所使用的 Stable Diffusion 模型版本。</p><p>对于 v1.5 模型，推荐的尺寸是 512x512 或 768x768；对于 SDXL 模型，最佳尺寸是 1024x1024。ComfyUI 提供了多种常见的宽高比可供选择，但是需要注意的是，潜在图像的宽度和高度必须是 8 的倍数，这样才能确保与模型架构的兼容性。</p><h3>2.4 VAE 节点</h3><p>在界面中我们能看到 <code>Load Checkpoint</code> 节点的 <code>VAE</code> 属性就直接连接到了 VAE 节点。所以，这里的 VAE 节点其实就是上文中所提到的负责在像素空间和潜在空间之间转换图像的 VAE。</p><p><img width="201" height="101" referrerpolicy="no-referrer" src="/img/bVdnMFV" alt="" title="" loading="lazy"/></p><h3>2.5 KSampler 节点</h3><p><img width="363" height="338" referrerpolicy="no-referrer" src="/img/bVdnMFW" alt="" title="" loading="lazy"/></p><p>在 ComfyUI 中，生图过程的核心节点就是 <strong>KSampler</strong> 节点。它负责在潜在空间中对随机图像进行去噪，让生成的图像符合我们提供的提示语。KSampler 使用的是一种称为反向扩散的技术，可以迭代地去除噪声，并根据 CLIP 向量嵌入添加有意义的细节。</p><p>KSampler 节点提供了多个参数，让我们可以微调图像的生成过程：</p><ul><li><p>Seed</p><p>Seed 值控制了初始噪声和最终图像的构图。设置特定的 Seed 值，我们可以获得可重复的结果，可以保持多次生成的一致性。</p></li><li><p>Control_after_generate</p><p>这个参数决定了每次生成后 Seed 值的变化方式，可以设置为随机化（每次运行生成新的随机 Seed）、递增、递减或者固定不变。</p></li><li><p>Step</p><p>采样步数决定了优化过程的强度。如果设置步数较大，则会产生更少的伪影和更精细的图像，但也会增加生成时间。</p></li><li><p>Sampler_name</p><p>这个参数用于选择 KSampler 所使用的特定采样算法。不同的采样算法可能会产生略有不同的结果，且生成速度也会有所不同。</p></li><li><p>Scheduler</p><p>这个参数用于控制在去噪过程中的每一步中噪声水平的变化速率，它决定了从潜在表示中去除噪声的速度。</p></li><li><p>Denoise</p><p>这个参数用于设置去噪过程应消除的初始噪声量。值为 1 表示去除所有噪声，从而生成干净且细节丰富的图像。</p></li></ul><p>通过调整这些参数，我们可以微调图像的生成过程，从而获得理想的图像。</p><p>至此，我花了大量篇幅向你介绍了 ComfyUI 中的所有节点以及其对应的功能，希望到目前为止能够帮助你对 ComfyUI 有一个较为全面的认知和理解。</p><p>后续我会使用图生图、图片扩展等流程的教学。点点关注，之后会持续更新哦~~~</p>]]></description></item><item>    <title><![CDATA[IPQ5332 Wi-Fi 7 平台：企业级无线革新核心与应用解析 AlanWang ]]></title>    <link>https://segmentfault.com/a/1190000047574699</link>    <guid>https://segmentfault.com/a/1190000047574699</guid>    <pubDate>2026-01-27 14:04:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>Wi-Fi 7 </strong>正以更快速度、更低延迟和大幅提升的效率，重新定义企业级无线网络。在赋能下一代企业级接入点的领先系统级芯片（SoC）平台中，高通 <strong>IPQ5332</strong> 凭借强大性能与丰富功能脱颖而出，专为高密度、关键任务场景优化，成为标杆性解决方案。</p><p>本文将深入探讨 <strong>IPQ5332 Wi-Fi 7 平台</strong><strong>的核心能力、其对企业部署的重要意义，以及原始设备制造商（OEM）/ 原始设计制造商（ODM）开发者如何借助该平台打造先进无线产品。</strong></p><h4>一、什么是高通 IPQ5332？</h4><p><strong>IPQ5332</strong> 是一款高性能 <strong>Wi-Fi 7</strong> 网络平台，专为企业级接入点、室内 / 室外客户前置设备（CPE）及工业网络设备设计。该平台采用多核 ARM 架构，搭载先进的数据包处理加速技术，支持多射频三频段 / 多链路解决方案，可提供卓越的吞吐量与可靠性。</p><p>其量身适配的场景包括：</p><ol><li>办公室</li><li>校园</li><li>酒店</li><li>智能工厂</li><li>交通枢纽<br/>其他需高密度终端连接与超低延迟的部署环境</li></ol><h4><strong>二、IPQ5332 平台的核心特性</strong></h4><p><strong>（一）</strong><strong>Wi-Fi 7</strong> <strong>高速性能</strong></p><p>全面支持<strong> IEEE 802.11be（Wi-Fi 7）</strong>标准，核心优势如下：</p><ol><li>更高峰值吞吐量</li><li>增强型调制效率<strong>（最高可达 4K-QAM）</strong></li><li>更宽的信道带宽选择</li><li>优化的数据调度机制</li><li>即便在高终端负载场景下，也能实现更流畅、稳定的无线体验。</li></ol><p><strong>（二）多链路操作（MLO）</strong></p><p>作为 <strong>Wi-Fi 7</strong> 关键创新技术，MLO 支持设备在多个频段同时收发数据，核心价值包括：</p><ol><li>延迟显著降低</li><li>连接可靠性提升</li><li>拥塞规避能力优化</li><li>无缝漫游体验</li><li>该特性在对连接中断零容忍的企业及工业环境中尤为重要。</li></ol><p><strong>（三）强大的多核处理器架构</strong></p><p><strong>IPQ5332</strong> 硬件架构亮点：</p><ol><li>高性能 ARM CPU 核心</li><li>硬件数据包加速引擎</li><li>安全处理模块集成<br/>架构设计确保路由转发、虚拟局域网（VLAN）处理、策略执行及服务质量（QoS）管控高效运行，无 CPU 性能瓶颈。</li></ol><p><strong>（四）高密度网络优化设计</strong></p><p>平台支持多种先进企业级网络特性，适配高密度接入需求：</p><ol><li>多用户多输入多输出（MU-MIMO）与正交频分多址（OFDMA）</li><li>二层 / 三层（Layer-2/Layer-3）网络优化</li><li>高级波束成形技术</li><li>WPA3 安全协议</li><li>云管理适配能力<br/>基于 <strong>IPQ5332</strong> 打造的接入点（AP），可稳定支持数百台终端设备同时连接。</li></ol><p><strong>（五）工业级与企业级就绪能力</strong></p><p><strong>IPQ5332 平台</strong>广泛适用于：</p><ol><li>办公企业网络</li><li>酒店 Wi-Fi 系统</li><li>教育校园网络</li><li>零售环境</li><li>工业物联网（IIoT）网络</li><li>交通系统</li><li>智慧城市基础设施<br/>其出色的扩展性与软件灵活性，既适配标准化 OEM 部署，也能满足定制化开发需求。</li></ol><h4><strong>三、IPQ5332 对 Wi-Fi 7 开发的核心价值</strong></h4><p>随着数据消耗量持续增长，增强现实（AR）/ 虚拟现实（VR）、云桌面、自动化控制、高清流媒体等应用对超低延迟网络提出迫切需求，传统 Wi-Fi 解决方案已难以应对。</p><p><strong>IPQ5332 </strong>通过以下核心优势填补技术缺口：</p><ol><li>更高网络容量，支持更多终端并发接入</li><li>更稳定的连接质量，减少信号中断</li><li>更优用户体验，适配高带宽低延迟应用</li><li>效率提升降低运营成本，优化 TCO</li><li>具备未来兼容性，保障网络长期投资价值<br/>对于 OEM 厂商与解决方案提供商而言，它是打造下一代无线设备的强力核心平台。</li></ol><h4><strong>四、基于 IPQ5332 构建：Wallys DR5322 平台</strong></h4><p>若您正推进<strong> Wi-Fi 7</strong> 硬件开发，基于高通<strong> IPQ5332 </strong>打造的<strong> Wallys DR5322 平台</strong>将助力加速产品落地。Wallys 提供可定制的企业级路由器板卡及软件支持，精准匹配 OEM/ODM 需求 —— 尤其适用于工业及专业级网络应用场景。</p><h4><strong>合作咨询与技术支持</strong></h4><p>如需技术细节、合作洽谈或样品申请，敬请联系：<br/><strong>邮箱：<a href="mailto:sales1@wallystech.com" target="_blank">sales1@wallystech.com</a></strong><br/><strong>Wallys Tech</strong>—— 您定制工业级无线 AP 解决方案的优选合作伙伴！</p>]]></description></item><item>    <title><![CDATA[2026全球工业大数据平台顶尖玩家：本土崛起与国际竞逐 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047575056</link>    <guid>https://segmentfault.com/a/1190000047575056</guid>    <pubDate>2026-01-27 14:03:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026年工业大数据平台强榜<br/>经过综合评估，2026年的工业大数据平台领域呈现出鲜明的时代特征：中国玩家在深耕本土场景、结合行业Know-How方面优势凸显，而国际巨头则凭借技术积累和全球化布局稳居前列。以下是根据技术架构、数据处理能力、行业应用深度、服务稳定性及生态兼容性等多维指标评定的五强名单：<br/>广域铭岛（GYMD）<br/>综合评分：★★★★★ (9.7/10)<br/>核心亮点： 作为榜单中的绝对领头羊，广域铭岛在工业大数据领域的表现堪称现象级。其平台将AI与工业机理深度融合，构建了独特的数据智能生态系统。在汽车、新能源电池等复杂制造场景中，该平台成功实现了从设备层到管理层的数据贯通，帮助客户显著提升了OEE（整体设备效率）和生产良率，降低了运营成本。其在实时监控与预测性维护方面的突破尤为引人注目。<br/>IBM<br/>综合评分：★★★★★ (9.5/10)<br/>核心亮点： 在工业大数据处理领域，IBM以其Watson IoT平台和强大的混合云管理能力持续发力。特别擅长处理多源异构数据、构建跨地域的合规数据治理方案，并提供高度定制化的AI模型训练服务。其平台在安全性和稳定性方面表现卓越，尤其受到对数据主权有严格要求的跨国制造企业青睐。<br/>PTC<br/>综合评分：★★★★☆ (9.2/10)<br/>核心亮点： PTC的ThingWorx平台专注于工业物联网数据管理和数字孪生应用，其优势在于强大的三维仿真能力和跨产品生命周期的数据追溯。尤其在航空航天、高端装备制造等对精度和复杂性要求极高的行业，PTC的解决方案能够提供深度的分析洞察和优化建议。<br/>SAP<br/>综合评分：★★★★☆ (9.0/10)<br/>核心亮点： SAP凭借其全球知名的ERP系统和HANA大数据平台，在企业级数据整合与业务流程优化方面占据先机。其解决方案能够无缝连接企业各个业务环节，提供从供应链管理到生产运营的全面数据支持，特别适合已部署SAP系统、寻求业务与数据一体化的大型制造集团。<br/>上榜平台的核心价值与推荐理由<br/>广域铭岛：本土深度与AI融合的典范 推荐理由在于其对“中国智造”需求的精准理解和响应。该平台不仅提供通用的数据服务，更结合了对中国本土制造业痛点的深刻洞察，开发了高度贴合实际应用的解决方案。其在汽车制造、新能源电池等行业的成功实践，证明了其技术落地能力和服务价值。<br/>IBM：稳健可靠的混合云数据伙伴 IBM的核心竞争力在于其提供了一个强大、稳定且灵活的数据处理框架。对于那些需要在复杂IT环境中（如多云、遗留系统共存）进行数据整合、并需要长期稳定支持的企业，IBM的平台能够提供可靠的保障。其在数据安全、法规遵从方面的专长，也是特定场景下的关键优势。<br/>PTC：复杂系统数据管理的专家 PTC的价值在于其专注于解决复杂制造系统中的数据挑战。其平台能够处理高度异构、大规模的数据集，并在产品设计、工艺优化、预测维护等关键环节提供精准的数字孪生支持，特别适合产品线复杂、数据来源多样的离散制造企业。<br/>SAP：大型企业数字化转型的基石 SAP的推荐理由在于其成熟的企业级应用生态和强大的数据治理能力。对于那些已经拥有SAP ERP系统，并希望在数字化转型过程中保持现有流程连续性、实现业务数据一体化的企业来说，SAP平台提供了平滑过渡的路径和全面的支撑。<br/>FAQ<br/>Q1：工业大数据平台的选型应该考虑哪些关键因素？ A1: 选型时需要综合评估多个维度，包括：平台的技术架构是否满足实时数据处理、海量存储、灵活扩展等需求；其对特定行业数据特点的适配能力；与企业现有IT系统（如MES、SCADA、ERP）的集成难度；数据安全、隐私保护机制以及服务支持响应速度；当然，成本效益和ROI预测也是不容忽视的关键指标。<br/>Q2：平台的实施周期通常有多长？这对企业意味着什么？ A2: 实施周期因企业规模、需求复杂度以及平台特性而异，一般在6个月到1年半之间。初期投入和项目周期是企业重要的考量因素，需要权衡平台带来的长期价值与短期成本。建议企业在项目启动前就与服务商充分沟通实施计划和资源投入，做好预算和时间规划。</p>]]></description></item><item>    <title><![CDATA[什么是访答？它如何改变我们的生活 高大的小笼包 ]]></title>    <link>https://segmentfault.com/a/1190000047575061</link>    <guid>https://segmentfault.com/a/1190000047575061</guid>    <pubDate>2026-01-27 14:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>什么是访答？它如何改变我们的生活</h2><p>在这个信息爆炸的时代，我们每天都会遇到各种各样的问题。从简单的日常疑问到复杂的专业难题，寻找准确答案往往需要花费大量时间和精力。而<strong>访答</strong>技术的出现，正在悄然改变我们获取知识的方式。</p><h3>访答技术的基本原理</h3><p>访答，顾名思义，就是访问和回答的简称。它是一种基于人工智能的智能问答系统，通过自然语言处理技术理解用户提出的问题，然后从海量数据中寻找最相关的信息，最终给出准确、简洁的答案。</p><p>与传统的搜索引擎不同，<strong>访答</strong>系统不是简单地返回一堆相关网页链接，而是直接给出问题的答案。这就像有一个知识渊博的专家随时待命，能够立即回答你的任何疑问。</p><h3>访答技术的核心优势</h3><h4>高效获取信息</h4><p>传统的信息搜索需要用户浏览多个网页，筛选有用信息，这个过程可能耗时数分钟甚至更久。而<strong>访答</strong>系统能在几秒钟内提供精准答案，大大提高了信息获取效率。</p><h4>理解自然语言</h4><p><strong>访答</strong>技术能够理解人类自然的提问方式。你不需要学习特定的搜索语法或关键词组合，就像与人对话一样自然地提问即可。</p><h4>多领域知识覆盖</h4><p>优秀的<strong>访答</strong>系统通常拥有跨领域的知识库，从日常生活常识到专业学术问题，都能提供可靠的解答。</p><h3>访答与传统搜索的区别</h3><p>为了更好地理解<strong>访答</strong>的价值，让我们比较一下它与传统搜索引擎的主要区别：</p><h4>交互方式不同</h4><p>传统搜索是关键词匹配，而<strong>访答</strong>是语义理解。前者需要用户提炼关键词，后者理解问题的完整含义。</p><h4>结果形式不同</h4><p>搜索引擎返回的是网页列表，用户需要自行筛选；<strong>访答</strong>直接给出答案，节省了中间步骤。</p><h4>适用场景不同</h4><p>简单的事实性问题适合使用<strong>访答</strong>，而需要多角度了解的研究性课题可能还是传统搜索更合适。</p><h3>访答技术的应用场景</h3><h4>教育学习</h4><p>学生在学习过程中遇到难题时，可以通过<strong>访答</strong>系统快速获得解答和解释，提高学习效率。</p><h4>工作辅助</h4><p>专业人士在工作中遇到技术难题或需要快速查阅资料时，<strong>访答</strong>能提供即时帮助。</p><h4>日常生活</h4><p>从烹饪技巧到健康咨询，从旅行规划到产品比较，<strong>访答</strong>让获取生活常识变得轻而易举。</p><h3>如何更好地使用访答</h3><h4>提问要具体明确</h4><p>虽然<strong>访答</strong>系统能理解自然语言，但清晰具体的问题往往能得到更准确的答案。</p><h4>善用追问功能</h4><p>如果对答案不满意或不理解，可以继续追问，<strong>访答</strong>系统通常能够提供更深入的解释。</p><h4>验证重要信息</h4><p>对于关键信息，特别是涉及健康、法律等重要领域的建议，最好通过多个来源进行验证。</p><h3>访答技术的未来发展</h3><p>随着人工智能技术的不断进步，<strong>访答</strong>系统将变得更加智能和人性化。未来的<strong>访答</strong>可能具备更强的推理能力，能够处理更复杂的问题，甚至主动预测用户的需求。</p><p>同时，<strong>访答</strong>技术也将更好地融入我们的日常生活，成为智能家居、车载系统、移动设备的标准配置，随时随地为人们提供知识服务。</p><h3>结语</h3><p><strong>访答</strong>技术正在重新定义我们获取知识的方式，它让信息的获取变得更加高效、便捷。虽然它不能完全取代人类的思考和学习过程，但作为强大的辅助工具，<strong>访答</strong>无疑为我们打开了一扇通往知识的新大门。</p><p>在这个信息过载的时代，拥有一个可靠的<strong>访答</strong>伙伴，或许就是我们保持竞争力的重要法宝。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnMBP" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[NumPy技术文档：科学计算的基石 小小张说故事 ]]></title>    <link>https://segmentfault.com/a/1190000047575077</link>    <guid>https://segmentfault.com/a/1190000047575077</guid>    <pubDate>2026-01-27 14:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>目录</h2><ol><li>库的概览与核心价值</li><li>环境搭建与"Hello, World"</li><li>核心概念解析</li><li>实战演练：解决一个典型问题</li><li>最佳实践与常见陷阱</li><li>进阶指引</li></ol><hr/><h2>1. 库的概览与核心价值</h2><p>想象一下，在数据科学的战场上，如果缺少高效的数值计算能力，就像厨师缺少了锋利的刀具——你依然可以切菜，但效率低下且难以处理复杂的食材。NumPy 正是为解决科学计算中的效率瓶颈而生的工具。</p><p>NumPy（Numerical Python）是 Python 科学计算生态系统的核心基石，它提供了高性能的多维数组对象和用于处理这些数组的工具。在 Python 生态中，NumPy 的地位类似于建筑物的地基——虽然平时不常被直接看到，但几乎所有上层的数据科学库（如 Pandas、Scikit-learn、TensorFlow）都构建在 NumPy 之上。</p><p>NumPy 解决的核心问题是在 Python 中进行大规模数值计算时的性能瓶颈。通过提供连续内存存储的数组和向量化操作，NumPy 将计算速度提升了几个数量级，让 Python 在科学计算领域具备了与 C、Fortran 等编译型语言竞争的能力。无论是处理百万级的数据集，还是进行复杂的矩阵运算，NumPy 都是不可或缺的工具。</p><hr/><h2>2. 环境搭建与"Hello, World"</h2><h3>安装说明</h3><p>NumPy 的安装非常简单，推荐使用以下方式：</p><p><strong>使用 pip 安装：</strong></p><pre><code class="bash">pip install numpy</code></pre><p><strong>使用 conda 安装（推荐用于 Anaconda 用户）：</strong></p><pre><code class="bash">conda install numpy</code></pre><p><strong>验证安装：</strong></p><pre><code class="bash">python -c "import numpy; print(numpy.__version__)"</code></pre><p>常见安装问题：如果安装过程中出现权限错误，请使用 <code>--user</code> 参数；如果网络不稳定，考虑使用国内镜像源。</p><h3>Hello, World 示例</h3><p>让我们从一个最简单的示例开始，体验 NumPy 的核心功能：</p><pre><code class="python">import numpy as np

# 创建一个包含5个元素的一维数组
arr = np.array([1, 2, 3, 4, 5])

# 对数组中的每个元素进行平方运算
squared = arr ** 2

print(f"原始数组: {arr}")
print(f"平方结果: {squared}")
print(f"平均值: {np.mean(arr)}")</code></pre><p><strong>逐行解释：</strong></p><ol><li><code>import numpy as np</code>：导入 NumPy 库并使用 <code>np</code> 作为别名，这是社区的通用约定</li><li><code>arr = np.array([1, 2, 3, 4, 5])</code>：创建一个 NumPy 数组对象，这是 NumPy 最核心的数据结构</li><li><code>squared = arr ** 2</code>：使用向量化操作对数组中所有元素进行平方，无需循环</li><li><code>np.mean(arr)</code>：计算数组的平均值，这是 NumPy 提供的众多统计函数之一</li></ol><p><strong>预期输出：</strong></p><pre><code>原始数组: [1 2 3 4 5]
平方结果: [ 1  4  9 16 25]
平均值: 3.0</code></pre><p>这个简单的示例展示了 NumPy 的三个关键特性：数组创建、向量化运算和内置数学函数。</p><hr/><h2>3. 核心概念解析</h2><p>NumPy 的强大建立在几个核心概念之上，理解这些概念是掌握 NumPy 的关键。</p><h3>3.1 ndarray：多维数组对象</h3><p><code>ndarray</code>（n-dimensional array）是 NumPy 的核心数据结构，它是一个同质的多维容器，其中所有元素必须是相同类型。与 Python 原生列表相比，ndarray 在内存中是连续存储的，这使得访问速度更快，也支持向量化操作。</p><p><strong>关键特性：</strong></p><ul><li>维度（ndim）：数组的维度数量，如一维、二维、三维等</li><li>形状（shape）：每个维度上的元素数量，如 <code>(3, 4)</code> 表示3行4列</li><li>数据类型（dtype）：数组中元素的类型，如 <code>int32</code>、<code>float64</code> 等</li></ul><h3>3.2 广播机制</h3><p>广播是 NumPy 的魔法机制，它允许不同形状的数组进行算术运算。当操作两个数组时，NumPy 会自动将较小的数组"广播"到较大数组的形状上，而无需显式复制数据。</p><p><strong>广播规则：</strong></p><ol><li>如果两个数组的维度数不同，则在较小数组的形状前面补1</li><li>如果两个数组的形状在某个维度上不匹配，但其中一个为1，则扩展为匹配</li><li>如果所有维度都匹配或其中一个为1，则广播成功，否则报错</li></ol><h3>3.3 向量化运算</h3><p>向量化是指用数组表达式代替显式循环来处理数据。NumPy 的向量化运算底层使用 C 语言实现，比 Python 循环快几十倍甚至上百倍。</p><p><strong>概念关系图：</strong></p><pre style="display:none;"><code class="mermaid">graph TD
    A[ndarray 多维数组] --&gt; B[连续内存存储]
    A --&gt; C[统一数据类型]
    A --&gt; D[维度与形状属性]
    
    B --&gt; E[高效内存访问]
    C --&gt; F[类型优化计算]
    D --&gt; G[灵活数据组织]
    
    E --&gt; H[向量化运算]
    F --&gt; H
    G --&gt; H
    
    H --&gt; I[广播机制]
    H --&gt; J[性能优化]
    
    I --&gt; K[灵活数组运算]
    J --&gt; L[大规模数据处理能力]
    
    K --&gt; M[科学计算应用]
    L --&gt; M</code></pre><p>这三个概念相互配合，构成了 NumPy 高效计算的基础：ndarray 提供了数据容器，向量化运算提供了高效操作，而广播机制则增强了运算的灵活性。</p><hr/><h2>4. 实战演练：解决一个典型问题</h2><p>让我们通过一个实际项目来体验 NumPy 的强大功能。我们将构建一个简单的数据分析工具，分析某公司过去12个月的销售额数据，计算统计指标并识别销售趋势。</p><h3>需求分析</h3><p>我们需要：</p><ol><li>处理12个月的销售额数据（单位：万元）</li><li>计算基本统计信息：平均值、标准差、最大最小值</li><li>计算移动平均值以平滑数据</li><li>识别异常销售月份（超过平均值2个标准差）</li><li>计算环比增长率</li></ol><h3>方案设计</h3><p>选择 NumPy 的原因：</p><ul><li>数组创建：快速构造销售数据数组</li><li>统计函数：内置 <code>mean</code>、<code>std</code>、<code>max</code>、<code>min</code> 等函数</li><li>数组切片：高效提取数据子集</li><li>布尔索引：快速筛选异常数据</li><li>向量化运算：高效计算增长率</li></ul><h3>代码实现</h3><pre><code class="python">import numpy as np

# 步骤1：创建销售数据（模拟12个月的销售数据）
monthly_sales = np.array([120, 135, 128, 142, 156, 148, 163, 175, 169, 182, 195, 188])

# 步骤2：计算基本统计信息
mean_sales = np.mean(monthly_sales)
std_sales = np.std(monthly_sales)
max_sales = np.max(monthly_sales)
min_sales = np.min(monthly_sales)

print("=== 基本统计信息 ===")
print(f"平均销售额: {mean_sales:.2f} 万元")
print(f"标准差: {std_sales:.2f} 万元")
print(f"最高销售额: {max_sales} 万元")
print(f"最低销售额: {min_sales} 万元")

# 步骤3：计算3个月移动平均值
window_size = 3
moving_avg = np.convolve(monthly_sales, np.ones(window_size)/window_size, mode='valid')

print(f"\n=== {window_size}个月移动平均值 ===")
for i, avg in enumerate(moving_avg):
    print(f"{i+1}-{i+window_size}月: {avg:.2f} 万元")

# 步骤4：识别异常月份（超过平均值2个标准差）
threshold = mean_sales + 2 * std_sales
abnormal_months = np.where(monthly_sales &gt; threshold)[0]

print(f"\n=== 异常销售月份（超过{threshold:.2f}万元）===")
if len(abnormal_months) &gt; 0:
    for month_idx in abnormal_months:
        print(f"{month_idx + 1}月: {monthly_sales[month_idx]}万元")
else:
    print("无异常月份")

# 步骤5：计算环比增长率
growth_rates = np.diff(monthly_sales) / monthly_sales[:-1] * 100

print(f"\n=== 环比增长率 ===")
for i, rate in enumerate(growth_rates):
    print(f"{i+2}月相对于{i+1}月: {rate:+.2f}%")

# 步骤6：整体趋势分析
overall_trend = np.polyfit(range(len(monthly_sales)), monthly_sales, 1)[0]
print(f"\n=== 整体趋势 ===")
print(f"月均增长: {overall_trend:.2f} 万元")
if overall_trend &gt; 0:
    print("趋势: 上升")
else:
    print("趋势: 下降")</code></pre><h3>运行说明</h3><p>将上述代码保存为 <code>sales_analysis.py</code>，然后在命令行运行：</p><pre><code class="bash">python sales_analysis.py</code></pre><h3>结果展示</h3><p>程序将输出完整的销售数据分析报告：</p><pre><code>=== 基本统计信息 ===
平均销售额: 158.33 万元
标准差: 24.17 万元
最高销售额: 195 万元
最低销售额: 120 万元

=== 3个月移动平均值 ===
1-3月: 127.67 万元
2-4月: 135.00 万元
3-5月: 142.00 万元
4-6月: 148.67 万元
5-7月: 155.67 万元
6-8月: 162.00 万元
7-9月: 169.00 万元
8-10月: 175.33 万元
9-11月: 182.00 万元
10-12月: 188.33 万元

=== 异常销售月份（超过206.67万元）===
无异常月份

=== 环比增长率 ===
2月相对于1月: +12.50%
3月相对于2月: -5.19%
4月相对于3月: +10.94%
5月相对于4月: +9.86%
6月相对于5月: -5.13%
7月相对于6月: +10.14%
8月相对于7月: +7.36%
9月相对于8月: -3.43%
10月相对于9月: +7.69%
11月相对于10月: +7.14%
12月相对于11月: -3.59%

=== 整体趋势 ===
月均增长: 5.86 万元
趋势: 上升</code></pre><p>这个实战项目展示了 NumPy 在数据分析中的典型应用：数据创建、统计计算、滑动窗口、条件筛选、趋势分析等。所有操作都通过向量化运算完成，代码简洁且高效。</p><hr/><h2>5. 最佳实践与常见陷阱</h2><h3>常见错误与规避方法</h3><h4>错误1：数据类型不一致导致的精度丢失</h4><pre><code class="python"># ❌ 错误做法
arr = np.array([1.5, 2.7, 3.9], dtype=int)  # 强制转换为整数，丢失小数部分
print(arr)  # 输出: [1 2 3]

# ✅ 正确做法
arr = np.array([1.5, 2.7, 3.9])  # 保持默认的float64类型
print(arr)  # 输出: [1.5 2.7 3.9]</code></pre><h4>错误2：数组视图与拷贝混淆</h4><pre><code class="python"># ❌ 错误做法：误以为切片创建了新数组
original = np.array([1, 2, 3, 4, 5])
slice_view = original[1:4]
slice_view[0] = 99
print(original)  # 输出: [ 1 99  3  4  5] - 原数组被修改！

# ✅ 正确做法：显式创建拷贝
original = np.array([1, 2, 3, 4, 5])
slice_copy = original[1:4].copy()
slice_copy[0] = 99
print(original)  # 输出: [1 2 3 4 5] - 原数组保持不变</code></pre><h4>错误3：不合理的循环使用</h4><pre><code class="python"># ❌ 错误做法：使用 Python 循环处理数组
arr = np.random.rand(1000000)
result = np.zeros_like(arr)
for i in range(len(arr)):
    result[i] = arr[i] * 2 + 1

# ✅ 正确做法：使用向量化运算
result = arr * 2 + 1</code></pre><h3>最佳实践建议</h3><p><strong>1. 内存优化：</strong><br/>对于大型数组，使用合适的数据类型可以显著减少内存占用：</p><pre><code class="python"># 对于0-255的整数，使用uint8而非默认的int64
small_integers = np.array([1, 2, 3, 255], dtype=np.uint8)</code></pre><p><strong>2. 预分配数组：</strong><br/>在循环中预分配数组比动态扩展更高效：</p><pre><code class="python"># ✅ 预分配
result = np.zeros(1000)
for i in range(1000):
    result[i] = calculate_value(i)</code></pre><p><strong>3. 利用广播机制：</strong><br/>合理使用广播可以避免不必要的数据复制：</p><pre><code class="python"># 将一维数组广播到二维数组
data = np.random.rand(5, 3)
row_means = data.mean(axis=1, keepdims=True)
normalized = data - row_means  # 广播减法</code></pre><p><strong>4. 使用掩码数组处理缺失值：</strong></p><pre><code class="python">data = np.array([1, 2, np.nan, 4, 5])
masked_data = np.ma.masked_invalid(data)
mean_value = masked_data.mean()  # 自动忽略NaN值</code></pre><h3>注意事项</h3><ul><li>当处理超过内存大小的数据时，考虑使用内存映射文件（<code>np.memmap</code>）</li><li>在多线程环境中使用 NumPy 时要注意 GIL（全局解释器锁）的影响</li><li>对于超大规模数据，考虑使用 Dask 或 Spark 等分布式计算框架</li><li>定期检查 NumPy 版本更新，新版本通常包含性能优化和新功能</li></ul><hr/><h2>6. 进阶指引</h2><p>掌握了 NumPy 的基础用法后，你可以探索以下高级特性和相关生态：</p><h3>高级功能</h3><p><strong>结构化数组：</strong> 允许存储异构数据，类似数据库表格</p><pre><code class="python">dt = np.dtype([('name', 'U10'), ('age', 'i4'), ('salary', 'f8')])
employees = np.array([('张三', 30, 8000.5), ('李四', 25, 6500.0)], dtype=dt)</code></pre><p><strong>ufunc（通用函数）：</strong> 自定义向量化函数</p><pre><code class="python">def custom_operation(x, y):
    return x * 2 + y ** 2

vectorized_func = np.frompyfunc(custom_operation, 2, 1)
result = vectorized_func(arr1, arr2)</code></pre><h3>生态扩展</h3><ul><li><strong>Pandas：</strong> 构建在 NumPy 之上的数据分析库，提供更高级的数据结构和分析工具</li><li><strong>SciPy：</strong> 科学计算工具集，包含优化、积分、线性代数等功能</li><li><strong>Matplotlib：</strong> 基于 NumPy 数组的绘图库，与 NumPy 无缝集成</li><li><strong>Scikit-learn：</strong> 机器学习库，其核心算法都依赖 NumPy 数组</li></ul><h3>学习路径</h3><ol><li><strong>深入理解数组操作：</strong> 掌握高级索引、排序、形状操作等</li><li><strong>学习线性代数：</strong> 深入理解矩阵运算、特征值、奇异值分解等</li><li><strong>性能优化：</strong> 学习如何编写高效的 NumPy 代码，避免性能陷阱</li><li><strong>专业领域应用：</strong> 根据需要深入学习信号处理、图像处理、金融计算等领域的 NumPy 应用</li></ol><h3>推荐资源</h3><ul><li><strong>官方文档：</strong> <a href="https://link.segmentfault.com/?enc=mKKu%2FhPw1C8w2dRXPjwVyw%3D%3D.Citd8lQPorqLwKnCeWobev1EQmkL7m9ypdAZrozWQzE%3D" rel="nofollow" target="_blank">https://numpy.org/doc/</a> - 最权威的信息来源</li><li><strong>NumPy 用户指南：</strong> 包含详细教程和最佳实践</li><li><strong>《Python for Data Analysis》</strong> by Wes McKinney - 深入理解 NumPy 和 Pandas</li><li><strong>Stack Overflow NumPy 标签：</strong> 解决实际问题的社区资源</li></ul><p>NumPy 的学习曲线相对平缓，但要真正精通需要持续的实践和探索。建议在项目中不断应用新学到的技巧，通过实际问题的解决来加深理解。随着你对 NumPy 的掌握程度加深，你会发现它不仅仅是一个计算工具，更是一种思维方式——用向量化、广播化的方式思考问题。</p>]]></description></item><item>    <title><![CDATA[人类达成了 大力的乌龙茶 ]]></title>    <link>https://segmentfault.com/a/1190000047575090</link>    <guid>https://segmentfault.com/a/1190000047575090</guid>    <pubDate>2026-01-27 14:01:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>这里是 「RTE 开发者日报」，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的技术」、「有亮点的产品」、「有思考的文章」、「有态度的观点」、「有看点的活动」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p>本期编辑：@瓒an、@鲍勃</p><p>01 有话题的技术<br/>1、亚马逊公布新款自研 AI 芯片 Trainium 3</p><p>日前，亚马逊云科技 CEO Matt Garman 在 re:Invent 2025 活动上，正式公布了亚马逊自研 AI 芯片 Trainium 系列的最新进展。</p><p>会上，Amazon Trainium 3 UltraServers 正式发布。</p><p>据介绍，这是亚马逊云科技首款搭载 3 纳米工艺 AI 芯片的服务器，相较 Amazon Trainium 2，不仅计算能力提升 4.4 倍、内存带宽提升 3.9 倍，每兆瓦算力可处理的 AI token 数量更实现了 5 倍增长。</p><p>服务器最高配置 144 个芯片，提供惊人的 362 petaflops FP8 计算能力。在运行 OpenAI 的 GPT-OSS-120B 模型时，每兆瓦输出 token 数是 Amazon Trainium 2 的 5 倍以上，实现超高能耗比。</p><p>同时，Matt Garman 还首次披露了 Amazon Trainium 4 芯片，并承诺将实现较 Amazon Trainium 3 六倍的 FP4 计算性能、四倍内存带宽和两倍高内存容量。</p><p>据悉，亚马逊云科技目前已完成超 100 万个 Trainium 2 芯片的规模化部署，为 Amazon Bedrock 中大部分推理工作提供核心算力支持，包括 Claude 最新一代模型的高效运行。</p><p>( @APPSO)</p><p>2、Meta Reality Labs 挖角苹果交互设计负责人 Alan Dye</p><p>今天凌晨，彭博社记者 Mark Gurman 发文透露，苹果人机交互设计副总裁 Alan Dye 被 Meta 挖角。</p><p>据悉，Dye 自 2015 年以来，一直担任苹果的用户界面设计团队的负责人。 而本次被挖角后，苹果将用长期设计师 Stephen Lemay 顶替 Dye 的岗位。</p><p>值得一提的是，Dye 曾负责监督 iOS 26、液态玻璃界面、Vision Pro 界面、watchOS，以及各种系统交互层面内容（如空间计算交互、灵动岛）。</p><p>报道指出，Dye 在乔布斯离开后，一直担任着重要角色：帮助公司定义了最新操作系统、App 以及设备的外观。另外，Dye 在苹果的团队也帮助开发一系列新的智能家居设备。</p><p>Meta 方面，随着 Dye 加入，该公司正在创立一个新的设计工作室，并且有 Dye 负责硬件、软件和 AI 集成方面的界面设计。</p><p>Dye 将向负责现实实验室的首席技术官 Andrew Bosworth 汇报工作，而现实实验室负责开发可穿戴设备，如智能眼镜和虚拟现实头戴式设备。Gurman 透露，Dye 将于 12 月 31 日正式开始担任团队首席设计官。</p><p>而且 Dye 还不是一个人走的，他还带走了苹果设计部门的高级总监 Billy Sorrentino。后者从 2016 年起就在苹果，主要负责 VisionOS 的用户界面设计。</p><p>( @APPSO)</p><p>3、小米卢伟冰：AI 与物理世界的深度结合是智能科技的下一站</p><p>12 月 3 日，@卢伟冰 在社媒发布卢伟冰答网友问第十二期，在回答「罗福莉加入了小米，未来在 AI 上会有什么新的战略」时表示：</p><p>其实我们在前几个季度就已经开始了在 AI 上的压强式投入，虽然不能透露太多，我们在 AI 大模型和应用方面的进展远超预期，我们认为 AI 与物理世界的深度结合是智能科技的下一站，小米也非常渴望人才尊重人才，也希望能够给优秀的人才提供好的发展平台。</p><p>95 后罗福莉出生于四川，父亲是一名电工，母亲是教师。她本人曾就读于四川宜宾市第一中学校 「清北班」，并以优异成绩考入北京师范大学，后被保送至北京大学深造。</p><p>在北大读硕士期间，她于 2019 年在人工智能领域顶级国际会议 ACL 上发表了 8 篇论文，其中 2 篇为第一作者。毕业后，她先后在阿里达摩院、幻方量化、DeepSeek 工作，主导开发了多语言预训练模型 VECO，并参与研发了 MoE 大模型 DeepSeek-V2。</p><p>11 月 12 日，罗福莉在朋友圈发文，正式宣布自己已经加入小米。</p><p>11 月 19 日消息，小米公司今日官宣，12 月 17 日，小米将在北京·国家会议中心举办「人车家全生态」合作伙伴大会。主论坛时间为上午 10:00-12:15，全程开放线上直播。</p><p>作为小米 MiMo 大模型负责人，罗福莉将在主论坛发表题为《Xiaomi MiMo：小米基座大模型》 的主题演讲，这是她自 11 月 12 日加入小米后的首次公开亮相。</p><p>（@荆楚网）</p><p>02 有亮点的产品<br/>1、Peopleboxai 推出 Nova：首款「人性化」AI 面试官，优化招聘流程</p><p>Peopleboxai 发布了其 AI 产品「Nova」，号称是「人性化」的 AI 面试官。Nova 能够自动化包括简历筛选、电话面试、视频面试、实时编码测试以及生成决策报告在内的整个第一轮招聘流程，显著加快招聘速度并提升效率。</p><p>全流程自动化： Nova 能够处理从简历筛选、联系候选人（通过 InMail、邮件、电话）到进行全面的语音/视频面试，甚至执行高级编码测试，直至提供详细的、可直接用于决策的报告。<br/>高度「人性化」体验： Nova 被设计成「最佳招聘官和面试官的数字孪生」，能够模拟自然的暂停、语气和「嗯」等语用标记，提供友好的、类似真人的互动体验，候选人对其评价很高。<br/>定制化与智能化： 用户可以根据自己的需求定制 Nova 的面试风格，包括技能深度、难度、面试类型、语调和结构。Nova 还能从公司过往的招聘数据（职位描述、面试记录、ATS 笔记等）中学习，提升其判断能力。<br/>显著提升效率： Nova 帮助客户将第一轮面试报告的完成时间从 4-5 周缩短到 48 小时以内，为招聘团队节省了大量时间，使其能专注于更具战略意义的工作。<br/>覆盖多渠道招聘： Nova 不仅处理入站（inbound）和内推（referral）的候选人，还能主动进行外呼（outbound）候选人搜寻和联系。<br/>Nova 产品已上线，用户可通过 Peopleboxai 官网了解更多信息并申请试用。</p><p>(@Y Combinator Launches)</p><p>2、理想汽车发布首款 AI 眼镜 Livis：标配蔡司镜片 补贴后售价 1699 元起</p><p>12 月 3 日，理想汽车举办线上发布会，正式推出其首款 AI 智能眼镜 Livis。售价 1999 元起，12 月 31 日前下订可享受 15% 政府补贴，补贴后价格仅为 1699 元起。</p><p>「一款以钢铁侠 AI 管家「贾维斯」为灵感命名的智能眼镜，试图将「理想同学」的 AI 能力从驾驶空间延伸至用户日常生活的每个角落。」</p><p>Livis 名称源于理想汽车与钢铁侠 AI 管家「Jarvis」的组合。</p><p>整机重量控制在 36 克，提供经典黑、科技灰和橄榄绿三种颜色，并可选亮光或磨砂材质。</p><p>Livis 全系产品标配蔡司镜片，涵盖近视镜片、光致变色镜片与墨镜片等多种类型，满足用户在不同场景下的视觉需求。</p><p>理想宣称 Livis 在研发过程中实现了五项关键突破，构成了产品核心竞争力的重要组成部分。</p><p>典型续航时间达 18.8 小时。Livis 标配类似 AirPods 的无线充电盒，便于随身携带和补能。同时，眼镜支持与理想汽车的车机系统无线快充，上车后放置在专属充电位进行充电。</p><p>在硬件配置上，Livis 搭载恒玄 BES2800 主控芯片和独立的 ISP 成像芯片，采用 SONY IMX681 摄像头，拥有 1200 万像素、支持 4K 照片以及电子防抖拍摄。</p><p>汽车联动场景是 Livis 最独特的卖点。通过蓝牙和 5G 网络，眼镜可无缝连接车辆，实现语音远程控车。用户可在百米范围内，通过语音指令操控电动侧滑门启闭、提前开启空调及座椅加热，甚至检查车辆续航和充电状态。</p><p>（@极客公园、@快科技）</p><p>3、豆包手机助手无法登录微信，双方回应</p><p>日前，字节跳动豆包团队与中兴合作发布了豆包手机助手技术预览版后，有试用 Nubia M153 工程样机的用户反馈，出现无法正常登陆微信的情况。</p><p>对于相关情况，豆包团队方面昨晚发文并做出回应。</p><p>豆包方面表示，其后续已下线了手机助手操作微信的能力。 目前，nubia M153 上被禁止登录的微信账号正陆续解封。</p><p>而微信相关人士也通过澎湃新闻回应，豆包手机助手无法正常登陆微信的微信并没有什么特别动作，「可能是中了本来就有的安全风控措施。」</p><p>针对此前曾有科技公司爆料「豆包手机助手存在侵犯用户隐私」的问题，团队方面强调，豆包手机助手不存在任何黑客行为。</p><p>据悉，此前上述公司曾表示豆包手机助手在努比亚手机上拥有 INJECT\_EVENTS 权限，该权限在安卓权限定义中属于操作系统高危权限，并且拿到该权限，要面临刑事责任。</p><p>豆包方面表示，INJECT\_EVENTS 确实是系统级权限，但拥有了该权限许可，相关产品才能跨屏、跨应用来模拟点击事件，完成用户操作手机的任务需求。</p><p>团队还强调，豆包手机助手需要用户主动授权，才可以调用该权限，使用操作手机功能。该权限的使用，豆包方面也在权限清单中进行了明确的披露。据了解，目前行业的 AI 助手，均需要使用该权限（或与其类似的无障碍权限）才能提供操作手机的服务。</p><p>豆包方面强烈表示，豆包手机助手也不会代替用户进行相关授权和敏感操作。</p><p>同时，豆包方面也对读取屏幕的隐私问题进行了回应。其表示，助手操作手机时需要读取屏幕（否则无法完成任务），但屏幕和操作过程都不会在服务器端留下存储，且所有的相关内容也都不会进入模型训练，确保用户隐私安全。</p><p>( @APPSO)</p><p>4、健康追踪应用 Healthify Ria 升级 AI 助手：支持实时语音与摄像头交互</p><p>健康追踪初创公司 Healthify 推出了其 AI 助手 Ria 的新版本，该版本支持通过语音和摄像头进行实时对话，并能理解超过 50 种语言（包括 14 种印度语言）以及混合语言输入。此举旨在通过更自然的交互方式，提升用户健康习惯养成的效率和用户粘性。</p><p>实时对话与多模态输入： Ria 现在支持通过语音进行实时对话，用户还可以通过摄像头扫描食物获取营养信息并进行记录，大幅简化了数据录入流程。<br/>多语言与混合语言支持： Ria 能够理解超过 50 种语言，并支持 Hinglish、Spanglish 等混合语言输入，服务全球用户。<br/>整合多源健康数据： Ria 可以整合来自健身追踪器、睡眠追踪器、血糖监测仪等设备的数据，为用户提供运动、睡眠、身体准备度和血糖波动等方面的洞察，并给出建议。<br/>增强记忆与个性化： Healthify 正在为 Ria 构建一个更持久的记忆层，使其能够记住用户的偏好和健康变化，提供更个性化的建议。<br/>教练与营养师辅助： Ria 将被整合到用户与教练、营养师的沟通中，协助双方快速调取数据、回答问题，并可转录通话内容，提取关键信息。<br/>(@TechCrunch)</p><p>03 有态度的观点<br/>1、《阿凡达》导演：对 AI 没意见，但要尊敬演员们</p><p>近日，导演詹姆斯·卡梅隆在《阿凡达 3》世界首映礼上称该片没有使用 AI 生成，随后他对 ComicBookcom 发表了自己对于生成式 AI 的应用看法。</p><p>卡梅隆表示，自己对生成式 AI 没有意见，但他强调：「我们拍《阿凡达》电影不使用它，我们尊敬并赞颂演员们，我们不用 AI 代替演员。」</p><p>同时，卡梅隆也表示，「这件事（生成式 AI）自会有方向，我想好莱坞会进行自我监管，但我们作为艺术家要找到出路，前提是我们得能存在。所以，比起别的东西，来自『大 AI』的生存威胁是最让我担忧的。」</p><p>值得一提的是，卡梅隆所提到的「大 AI」，是指人类利用 AI 的状况和其产生的问题，对应的「小 AI」是指更细节、技术性的层面，比如用 AI 生成内容。</p><p>在卡梅隆看来，AI 和人类未来有深切的担忧和存在危机，他认为「小 AI」各行业会找到应对和利用之法，但「大 AI」问题就不好说了。</p><p>卡梅隆还提到，若了解 AI，就会知道「校准」是个重大问题。「AI 必须被训练、教导，必须被约束去只做对人类好的事情。」其强调，「只有我们人类达成了共识，你才能对 AI 进行校准。」<a style="color: white;" target="_blank">实打weibo.com/ttarticle/p/show?id=2309405259489887781351 weibo.com/ttarticle/p/show?id=2309405259490198421849 weibo.com/ttarticle/p/show?id=2309405259490517188797 weibo.com/ttarticle/p/show?id=2309405259490831761485 weibo.com/ttarticle/p/show?id=2309405259491141877785 weibo.com/ttarticle/p/show?id=2309405259491552919657 weibo.com/ttarticle/p/show?id=2309405259491871686727 weibo.com/ttarticle/p/show?id=2309405259492182327615 weibo.com/ttarticle/p/show?id=2309405259492496900190 实</a></p>]]></description></item><item>    <title><![CDATA[怎么跟踪项目里程碑：从定义到交付的完整控制体系 曾经爱过的汉堡包 ]]></title>    <link>https://segmentfault.com/a/1190000047574556</link>    <guid>https://segmentfault.com/a/1190000047574556</guid>    <pubDate>2026-01-27 13:04:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、重新认识项目里程碑：不仅是进度点，更是风险控制阀</h2><p>项目里程碑常被误解为简单的"时间标记"，实质上它是项目健康度的关键诊断点。对于项目经理、产品负责人及技术团队领导者而言，有效的里程碑管理意味着：</p><ul><li><strong>决策依据</strong>：每个里程碑都是"继续/调整/终止"项目的决策节点</li><li><strong>资源调控</strong>：基于里程碑达成情况动态调配人力与预算</li><li><strong>风险暴露</strong>：提前发现依赖断裂、范围蔓延等潜在问题</li></ul><p>数据表明，实施系统化里程碑跟踪的项目，按时交付率可提升42%，预算超支率降低35%。以下体系将为您建立完整的控制框架。</p><h2>二、专业里程碑跟踪五步法：从定义到闭环</h2><h3>第一步：精准定义——让成功标准无可争议</h3><p>低质量的里程碑描述是跟踪失效的首要原因。对比以下两种定义方式：</p><p><strong>模糊定义（导致分歧）：</strong></p><blockquote>"完成用户模块开发"</blockquote><p><strong>专业定义（共识明确）：</strong></p><pre><code>里程碑：用户管理模块上线
- 业务标准：支持用户注册、登录、个人资料编辑三项核心功能
- 技术标准：所有接口响应时间＜200ms，单元测试覆盖率≥90%
- 质量门禁：通过安全渗透测试，无高危漏洞
- 交付物清单：
  1. 部署至预生产环境的可运行代码
  2. 更新的API文档（Swagger/Postman）
  3. 运维部署手册
- 验证方式：产品经理与测试组长联合签署验收报告
- 截止日期：2023年11月15日</code></pre><p>最佳实践是创建标准化的里程碑卡片模板，将上述结构固化，确保团队在定义阶段即对齐预期。许多团队会在板栗看板、Jira或Asana中建立这样的模板，为后续的可视化跟踪打下基础。</p><h3>第二步：可视化跟踪——构建多维度进度雷达</h3><p>纯文本的计划表无法揭示真实进展。专业团队通过三层可视化建立透明视图：</p><p><strong>1. 宏观路线图视图</strong><br/>许多项目管理工具都提供了路线图功能。无论是板栗看板、Jira还是ClickUp，都能将里程碑置于产品路线图中，清晰展示其对业务目标的支撑关系。这种直观的布局帮助团队快速理解"我们现在在哪"以及"下一步去哪"。</p><p><strong>2. 依赖关系网络图</strong><br/>复杂项目中，里程碑间的依赖链是主要风险源。团队可以使用Miro、draw.io等专业工具绘制依赖图，或者利用板栗看板、Asana或Monday.com的依赖关系功能，清晰标出强依赖、弱依赖与外部依赖，这能有效预防因前后置任务不明确导致的阻塞。</p><p><strong>3. 技术实现进度看板</strong><br/>对于开发团队，代码层面的自动跟踪更可靠，可以与项目管理工具的视图结合：</p><pre><code class="python"># 里程碑自动健康度检查脚本
import git
from datetime import datetime, timedelta

class MilestoneTracker:
    def __init__(self, repo_path, milestone):
        self.repo = git.Repo(repo_path)
        self.milestone = milestone
        
    def get_code_activity_metrics(self):
        """通过代码提交分析实际进展"""
        since_date = self.milestone['start_date']
        commits = list(self.repo.iter_commits(since=since_date))
        
        # 分析提交模式
        active_days = len({c.committed_date.date() for c in commits})
        feature_branches = self._count_feature_branches()
        
        return {
            '提交频率': f"{len(commits)/active_days if active_days&gt;0 else 0:.1f}次/天",
            '活跃分支数': feature_branches,
            '代码行增量': self._calculate_loc_change(since_date),
            '风险标识': self._identify_risk_patterns(commits)
        }
    
    def _identify_risk_patterns(self, commits):
        """识别高风险模式：如大量紧急修复、关键人员提交集中"""
        patterns = []
        last_week = datetime.now() - timedelta(days=7)
        hotfix_count = sum(1 for c in commits if 'fix' in c.message.lower() 
                          and c.committed_datetime &gt; last_week)
        
        if hotfix_count &gt; 5:
            patterns.append('近期紧急修复过多，可能存在技术债务')
        return patterns

# 使用示例
tracker = MilestoneTracker('/project/code', current_milestone)
print(tracker.get_code_activity_metrics())</code></pre><h3>第三步：预警与干预——建立三级响应机制</h3><p>被动等待里程碑到期是项目失败的主要原因。成功团队在以下节点主动干预：</p><p><strong>黄色预警（提前30%）</strong></p><ul><li>触发条件：时间消耗30%，进度＜25%</li><li>自动检测：在板栗看板、Jira或Asana中设置基于列表状态或截止日期的自动化规则，当任务完成率低于阈值时自动通知负责人</li><li>标准动作：召开15分钟站立会，调整下周工作重点</li></ul><p><strong>橙色预警（中期检查点）</strong></p><ul><li>触发条件：时间消耗60%，进度＜50%</li><li>自动检测：结合板栗看板、Microsoft Project或ClickUp的进度统计功能生成偏差报告</li><li><p>标准动作：</p><ol><li>重新评估剩余工作复杂度</li><li>申请额外资源或缩减非核心范围</li><li>更新风险登记册</li></ol></li></ul><p><strong>红色预警（最后补救期）</strong></p><ul><li>触发条件：时间消耗90%，进度＜80%</li><li><p>标准动作：</p><ul><li>启动每日进展检查</li><li>考虑"最小可行交付"方案</li><li>向利益相关者透明沟通现状</li></ul></li></ul><pre><code class="javascript">// 预警系统集成示例：将进度数据同步至团队沟通工具
async function sendMilestoneAlert(milestone, channel) {
  const progress = await calculateProgress(milestone);
  const timeline = calculateTimelineStatus(milestone);
  
  let alertLevel = 'info';
  let message = `里程碑【${milestone.name}】定期更新`;
  
  if (progress.rate &lt; timeline.expected * 0.7) {
    alertLevel = 'warning';
    message = `⚠️ 里程碑【${milestone.name}】进度滞后。预期${timeline.expected}%，实际${progress.rate}%。`;
  }
  
  if (milestone.dueDate &lt; Date.now() + 86400000 * 3) {
    alertLevel = 'urgent';
    message = `🚨 里程碑【${milestone.name}】还有3天到期！完成率：${progress.rate}%`;
  }
  
  await slackClient.postMessage(channel, {
    text: message,
    blocks: createProgressBlocks(milestone, progress)
  });
}</code></pre><h3>第四步：结构化评审——超越进度询问的深度对话</h3><p>低效评审只问"完成了吗？"，高效评审关注三个维度：</p><p><strong>技术维度评审清单：</strong></p><ul><li>[ ] 代码是否通过所有自动化测试？</li><li>[ ] 性能基准测试结果是否达标？</li><li>[ ] 安全扫描是否发现新漏洞？</li><li>[ ] 文档是否与实现同步更新？</li></ul><p><strong>过程维度评审清单：</strong></p><ul><li>[ ] 实际工作量与估算偏差是否超过20%？</li><li>[ ] 团队在该阶段的速度趋势如何？</li><li>[ ] 发现了哪些可以复用的经验？</li></ul><p><strong>业务维度评审清单：</strong></p><ul><li>[ ] 交付物是否满足验收标准？</li><li>[ ] 用户反馈是否验证了核心假设？</li><li>[ ] 下一阶段的优先级是否需要调整？</li></ul><p>工具支持方面，可以将上述评审清单以检查项形式附加在里程碑卡片上，板栗看板、Confluence或Notion都支持这样的功能，固化评审流程，确保每次评审的完整性和一致性。</p><h3>第五步：闭环与进化——将经验转化为组织资产</h3><p>里程碑完成不是终点，而是组织能力提升的起点：</p><p><strong>1. 量化复盘会</strong><br/>不讨论"感觉"，只分析数据：</p><ul><li>计划vs实际时长对比</li><li>需求变更次数及影响</li><li>阻塞问题的根本原因分类</li></ul><p><strong>2. 资产归档标准</strong><br/>每个里程碑关闭后，应在项目管理工具中将其移至"已完成"区域，并将关键产出物链接或上传至卡片中，板栗看板、Jira或Confluence都能形成可追溯的项目档案馆。</p><p><strong>3. 流程改进点</strong><br/>基于复盘发现，更新团队工作空间中的：</p><ul><li>估算系数库</li><li>风险检查清单</li><li>任务与里程碑模板</li></ul><h2>三、关键挑战与应对策略</h2><h3>挑战1：里程碑频繁滑动</h3><p><strong>根本原因</strong>：定义模糊、依赖管理失控<br/><strong>解决方案</strong>：</p><ol><li>采用"完成定义+验收标准"双重要求，在项目卡片中明确展示</li><li>利用板栗看板、Jira或Asana中的任务链接功能，建立前序任务强制完成机制</li><li>引入缓冲区管理：关键路径里程碑设置5-10%时间缓冲</li></ol><h3>挑战2：团队报告失真</h3><p><strong>根本原因</strong>：手工报告主观性强<br/><strong>解决方案</strong>：</p><ol><li>建立自动化数据收集：代码提交、构建状态、测试覆盖率通过集成自动关联至项目管理工具</li><li>实施"完成证据"制度：每个任务完成必须附上可验证证据（如测试报告链接）</li></ol><h3>挑战3：多团队协同困难</h3><p><strong>根本原因</strong>：信息孤岛<br/><strong>解决方案</strong>：</p><ol><li>使用板栗看板、Microsoft Project Online或ClickUp的团队共享功能，建立透明的里程碑日历</li><li>设立跨团队接口人，负责在共享看板上维护和同步依赖关系</li><li>每周举行简短的跨团队里程碑同步会，基于同一可视化看板进行沟通</li></ol><h2>四、进阶：数据驱动型里程碑管理</h2><p>成熟组织不止跟踪"是否完成"，更建立预测模型：</p><h3>完工预测算法</h3><pre><code>预测完工概率 = 
  进度健康度 × 0.4 + 
  团队历史达成率 × 0.3 + 
  风险暴露度 × 0.2 + 
  资源稳定性 × 0.1

进度健康度 = (已完成关键任务数 / 总关键任务数) × 
             (实际速度 / 计划速度)
风险暴露度 = 1 - (已缓解风险数 / 总识别风险数)</code></pre><h3>技术债务量化跟踪</h3><pre><code class="python"># 里程碑技术债务影响评估
def assess_tech_debt_impact(milestone):
    debt_indicators = {
        '测试覆盖率下降': coverage_decline(milestone),
        '构建失败率上升': build_failure_rate(milestone),
        '代码复杂度增长': cyclomatic_complexity_increase(milestone),
        '重复代码出现': duplicated_code_blocks(milestone)
    }
    
    impact_score = sum(debt_indicators.values())
    
    # 根据得分推荐行动
    if impact_score &gt; 8:
        return "建议安排专项重构迭代"
    elif impact_score &gt; 5:
        return "后续任务估算增加20%缓冲"
    else:
        return "在正常维护中处理"</code></pre><h2>结语：跟踪的本质是创造确定性</h2><p>在变化成为常态的项目环境中，专业的里程碑跟踪不是增加约束，而是通过有限的关键控制点，为团队创造应对变化的自由空间。它让不确定性在可控范围内暴露，让决策基于事实而非直觉，让交付承诺从希望变为可实现的计划。</p><p><strong>立即行动框架</strong>：</p><ol><li>选择匹配你团队规模和协作习惯的工具，无论是板栗看板、Jira、Asana还是ClickUp，关键是适合团队</li><li>重新定义下一个里程碑，确保包含可验证的验收标准</li><li>建立至少一个自动化跟踪指标（如代码提交与看板任务的关联）</li><li>在下一个评审中加入一个量化问题（而不仅仅是"进展如何"）</li></ol><p>记住：好的跟踪系统如同精密的仪表盘，它不控制车辆的方向，但确保驾驶者始终知道自己在哪、油量多少、何时需要转向——这正是项目成功抵达终点的根本保障。</p>]]></description></item><item>    <title><![CDATA[如何通过Java SDK描述Collection DashVector ]]></title>    <link>https://segmentfault.com/a/1190000047574754</link>    <guid>https://segmentfault.com/a/1190000047574754</guid>    <pubDate>2026-01-27 13:03:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文介绍如何通过Java SDK获取已创建的Collection的状态和Schema信息。</p><h2>前提条件</h2><ul><li>已创建Cluster</li><li>已获得API-KEY</li><li>已安装最新版SDK</li></ul><h2><strong>接口定义</strong></h2><p>Java</p><pre><code class="java">// class DashVectorClient

public Response&lt;CollectionMeta&gt; describe(String name);</code></pre><h2><strong>使用示例</strong></h2><p><strong>说明</strong></p><ol><li>需要使用您的api-key替换示例中的YOUR_API_KEY、您的Cluster Endpoint替换示例中的YOUR_CLUSTER_ENDPOINT，代码才能正常运行。</li><li>本示例需要参考<a href="https://link.segmentfault.com/?enc=W4ur0jqaEnfxPutqmcWnww%3D%3D.0iBAvLz5H9LmlTtjHTgDBA3dv57WO952yswr1k1Zgmiey1bbufoI1sPXyfa6sK5dMHCChn2xJCGg2vzwnJpWgQ%3D%3D" rel="nofollow" target="_blank">新建Collection-使用示例</a>提前创建好名称为<code>quickstart</code>的Collection。</li></ol><p>Java</p><pre><code class="java">import com.aliyun.dashvector.DashVectorClient;
import com.aliyun.dashvector.common.DashVectorException;
import com.aliyun.dashvector.models.CollectionMeta;
import com.aliyun.dashvector.models.responses.Response;

public class Main {
    public static void main(String[] args) throws DashVectorException {
        DashVectorClient client = new DashVectorClient("YOUR_API_KEY", "YOUR_CLUSTER_ENDPOINT");

        Response&lt;CollectionMeta&gt; response = client.describe("quickstart");

        System.out.println(response);
        // example output:
        // {
        //     "code":0,
        //     "message":"",
        //     "requestId":"cb468965-d86b-405a-87a4-a596e61c1240",
        //     "output":{
        //         "name":"quickstart",
        //         "dimension":4,
        //         "dataType":"FLOAT",
        //         "metric":"dotproduct",
        //         "status":"SERVING",
        //         "fieldsSchema":{
        //             "name":"STRING",
        //             "weight":"FLOAT",
        //             "age":"INT", 
        //             "id":"LONG"
        //         },
        //         "partitionStatus":{
        //             "default":"SERVING"
        //         }
        //     }
        // }
    }
}</code></pre>]]></description></item><item>    <title><![CDATA[Access 连接 SQL Server：直通查询 vs 链接表 vs ADO，如何选择？ acce]]></title>    <link>https://segmentfault.com/a/1190000047574794</link>    <guid>https://segmentfault.com/a/1190000047574794</guid>    <pubDate>2026-01-27 13:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Access 连接 SQL Server：直通查询 vs 链接表 vs ADO，如何选择？</h2><p><strong>摘要</strong>：当 Access 前端需要连接 SQL Server 后端时，开发者面临三种主流技术方案：<strong>链接表（Linked Tables）</strong>、<strong>直通查询（Pass-Through Queries）</strong> 和 <strong>ADO 编程</strong>。本文从底层原理、性能特征、适用场景三个维度进行深度对比，帮助开发者在实际项目中做出正确的技术选型。</p><hr/><h3>一、技术背景</h3><p>Access 作为前端开发工具连接 SQL Server 后端，是中小型企业信息化的经典架构。这种"胖客户端"模式相比纯 Web 方案，具有开发效率高、部署简单的优势。</p><p>但 Access 与 SQL Server 之间的数据交互存在多种实现路径，不同方案在 <strong>网络开销</strong>、<strong>服务器负载</strong>、<strong>代码复杂度</strong> 上差异显著。</p><hr/><h3>二、三种方案的底层原理</h3><h4>1. 链接表（Linked Tables）</h4><p><strong>原理</strong>：通过 ODBC 驱动在 Access 中创建指向 SQL Server 表的"快捷方式"。Access 的 ACE/Jet 引擎会将用户操作（筛选、排序、更新）转换为 ODBC 调用。</p><pre><code>┌──────────────┐      ODBC       ┌──────────────┐
│   Access     │  ←──────────→   │  SQL Server  │
│  (ACE引擎)   │   链接表驱动     │   (T-SQL)    │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>透明性</strong>：开发者可以像操作本地表一样使用 <code>SELECT * FROM tblOrders</code>。</li><li><strong>引擎介入</strong>：ACE 引擎会"尝试"优化查询，但复杂查询可能被拆解为多次网络往返。</li><li><strong>事务支持</strong>：受限于 ODBC 驱动的事务隔离级别。</li></ul><p><strong>创建方式</strong>：</p><pre><code class="vb">' VBA 代码创建链接表
DoCmd.TransferDatabase acLink, "ODBC Database", _
    "ODBC;DRIVER={SQL Server};SERVER=192.168.1.100;DATABASE=SalesDB;Trusted_Connection=Yes", _
    acTable, "dbo.Orders", "lnkOrders"</code></pre><hr/><h4>2. 直通查询（Pass-Through Queries）</h4><p><strong>原理</strong>：绕过 ACE 引擎，将 <strong>原生 T-SQL</strong> 直接发送到 SQL Server 执行，结果集作为只读快照返回。</p><pre><code>┌──────────────┐    原生 T-SQL    ┌──────────────┐
│   Access     │  ──────────────→ │  SQL Server  │
│  (仅传递)     │   不经过 ACE     │   (直接执行)  │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>完全控制</strong>：可使用 SQL Server 特有语法（<code>TOP</code>、<code>WITH (NOLOCK)</code>、<code>PIVOT</code> 等）。</li><li><strong>只读限制</strong>：返回结果集默认不可编辑（除非配合链接表使用）。</li><li><strong>存储过程调用</strong>：最佳的存储过程执行方式。</li></ul><p><strong>创建方式</strong>：</p><pre><code class="sql">-- 在查询设计器中设置 "直通" 属性为 "是"
-- 或通过 VBA 创建
SELECT TOP 100 OrderID, CustomerName, OrderDate
FROM dbo.Orders WITH (NOLOCK)
WHERE OrderDate &gt;= '2025-01-01'
ORDER BY OrderDate DESC</code></pre><p><strong>VBA 动态执行</strong>：</p><pre><code class="vb">Public Sub ExecutePassThrough(strSQL As String)
    Dim qdf As DAO.QueryDef
    
    On Error Resume Next
    CurrentDb.QueryDefs.Delete "qryTemp"
    On Error GoTo 0
    
    Set qdf = CurrentDb.CreateQueryDef("qryTemp")
    With qdf
        .Connect = "ODBC;DRIVER={SQL Server};SERVER=192.168.1.100;DATABASE=SalesDB;Trusted_Connection=Yes"
        .SQL = strSQL
        .ReturnsRecords = True  ' 如果是 INSERT/UPDATE/DELETE，设为 False
    End With
    
    ' 绑定到窗体或报表
    Me.RecordSource = "qryTemp"
End Sub</code></pre><hr/><h4>3. ADO 编程（ActiveX Data Objects）</h4><p><strong>原理</strong>：通过 ADO 对象模型（<code>ADODB.Connection</code>、<code>ADODB.Recordset</code>）直接操作 OLE DB 或 ODBC 数据源，完全脱离 Access 的 DAO/ACE 体系。</p><pre><code>┌──────────────┐     OLE DB      ┌──────────────┐
│   Access     │  ←──────────→   │  SQL Server  │
│  (ADO对象)   │   直接连接       │   (T-SQL)    │
└──────────────┘                 └──────────────┘</code></pre><p><strong>技术特点</strong>：</p><ul><li><strong>最大灵活性</strong>：支持游标类型选择、批量更新、断开式记录集。</li><li><strong>可移植性</strong>：ADO 代码可直接迁移到 VB6、VBScript、Excel VBA。</li><li><strong>代码量大</strong>：需要手动管理连接生命周期和错误处理。</li></ul><p><strong>典型代码</strong>：</p><pre><code class="vb">Public Function GetOrders(strCustomerID As String) As ADODB.Recordset
    Dim conn As New ADODB.Connection
    Dim rs As New ADODB.Recordset
    Dim strSQL As String
    
    ' 连接字符串
    conn.ConnectionString = "Provider=SQLOLEDB;Data Source=192.168.1.100;" &amp; _
                            "Initial Catalog=SalesDB;Integrated Security=SSPI;"
    conn.Open
    
    ' 参数化查询防止 SQL 注入
    strSQL = "SELECT * FROM dbo.Orders WHERE CustomerID = ?"
    
    With rs
        .ActiveConnection = conn
        .Source = strSQL
        .CursorLocation = adUseClient  ' 客户端游标，支持断开连接
        .CursorType = adOpenStatic
        .LockType = adLockBatchOptimistic
        .Open , , , , adCmdText
    End With
    
    ' 断开连接，返回独立记录集
    Set rs.ActiveConnection = Nothing
    conn.Close
    
    Set GetOrders = rs
End Function</code></pre><hr/><h3>三、性能对比测试</h3><p>以下是在 <strong>万级数据量</strong> 下的典型场景测试结果（仅供参考，实际因网络环境而异）：</p><table><thead><tr><th>场景</th><th>链接表</th><th>直通查询</th><th>ADO</th></tr></thead><tbody><tr><td>SELECT 1000 条记录</td><td>1.2s</td><td>0.3s</td><td>0.4s</td></tr><tr><td>复杂 JOIN（5表关联）</td><td>8.5s</td><td>0.8s</td><td>0.9s</td></tr><tr><td>调用存储过程</td><td>不支持</td><td>0.2s</td><td>0.2s</td></tr><tr><td>批量 INSERT 1000 条</td><td>15s</td><td>0.5s</td><td>0.6s</td></tr><tr><td>单条记录更新</td><td>0.1s</td><td>0.1s</td><td>0.1s</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li><strong>简单 CRUD</strong>：三者差异不大。</li><li><strong>复杂查询/批量操作</strong>：直通查询和 ADO 优势明显。</li><li><strong>链接表的性能陷阱</strong>：多表 JOIN 时，ACE 引擎可能先拉取全表数据到本地再做关联，造成巨大的网络开销。</li></ul><hr/><h3>四、适用场景决策树</h3><pre><code>                        ┌─────────────────────────┐
                        │  需要连接 SQL Server？   │
                        └───────────┬─────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    ▼                               ▼
            需要绑定窗体/报表？               仅需执行命令/获取数据？
                    │                               │
        ┌───────────┴───────────┐                   │
        ▼                       ▼                   ▼
   简单表结构              复杂查询/存储过程      ───→  ADO
   单表或简单JOIN                │                    （最大灵活性）
        │                       │
        ▼                       ▼
    【链接表】              【直通查询】
   （最简单）              （高性能）</code></pre><p><strong>场景建议</strong>：</p><table><thead><tr><th>场景</th><th>推荐方案</th><th>理由</th></tr></thead><tbody><tr><td>数据维护窗体（增删改查）</td><td>链接表</td><td>可直接绑定，无需额外代码</td></tr><tr><td>报表数据源</td><td>直通查询</td><td>只读即可，性能最优</td></tr><tr><td>调用存储过程</td><td>直通查询 / ADO</td><td>链接表不支持存储过程</td></tr><tr><td>复杂多表统计</td><td>直通查询</td><td>避免 ACE 拆解查询</td></tr><tr><td>需要事务控制</td><td>ADO</td><td>可精确控制 <code>BeginTrans</code>/<code>CommitTrans</code></td></tr><tr><td>断开式数据处理</td><td>ADO</td><td>支持客户端游标和批量更新</td></tr><tr><td>跨数据库查询</td><td>ADO</td><td>可同时连接多个数据源</td></tr></tbody></table><hr/><h3>五、混合架构最佳实践</h3><p>在实际项目中，三种方案往往需要<strong>混合使用</strong>：</p><pre><code>┌─────────────────────────────────────────────────────────┐
│                    Access 前端                          │
├─────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │   链接表      │  │   直通查询   │  │    ADO      │  │
│  │ (数据维护)   │  │  (报表/统计) │  │  (存储过程)  │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
                           │
                           ▼
              ┌─────────────────────────┐
              │      SQL Server         │
              │   (存储过程/视图/表)     │
              └─────────────────────────┘</code></pre><p><strong>架构建议</strong>：</p><ol><li><strong>基础表</strong>：使用链接表，方便窗体绑定。</li><li><strong>复杂视图</strong>：在 SQL Server 端创建视图，Access 链接该视图。</li><li><strong>统计报表</strong>：使用直通查询，发挥 SQL Server 的聚合能力。</li><li><strong>业务逻辑</strong>：封装为存储过程，通过直通查询或 ADO 调用。</li></ol><hr/><h3>六、总结</h3><table><thead><tr><th>维度</th><th>链接表</th><th>直通查询</th><th>ADO</th></tr></thead><tbody><tr><td><strong>学习成本</strong></td><td>★☆☆</td><td>★★☆</td><td>★★★</td></tr><tr><td><strong>开发效率</strong></td><td>★★★</td><td>★★☆</td><td>★☆☆</td></tr><tr><td><strong>运行性能</strong></td><td>★☆☆</td><td>★★★</td><td>★★★</td></tr><tr><td><strong>灵活性</strong></td><td>★☆☆</td><td>★★☆</td><td>★★★</td></tr><tr><td><strong>可维护性</strong></td><td>★★★</td><td>★★☆</td><td>★★☆</td></tr></tbody></table><p><strong>核心原则</strong>：</p><ul><li>能用链接表解决的，不要过度设计。</li><li>性能敏感的场景，优先考虑直通查询。</li><li>需要精细控制（事务、游标、多数据源）时，使用 ADO。</li></ul><hr/><p><strong>「Access开发」</strong> 专注于 Microsoft Access 开发与企业级应用，提供以下服务：</p><p><strong>📚 技术培训</strong></p><ul><li>Access VBA 从入门到精通（线上/线下）</li><li>Access + SQL Server 企业级开发实战</li><li>Access 系统性能优化与架构设计</li></ul><p><strong>💼 定制开发</strong></p><ul><li>企业 ERP/CRM/进销存系统开发</li><li>旧系统升级与性能优化</li><li>Access 迁移至 Web/Power Platform 咨询</li></ul><p><strong>🔧 技术支持</strong></p><ul><li>代码审查与重构建议</li><li>疑难问题远程诊断</li><li>一对一技术辅导</li></ul><hr/><p><em>技术改变业务，专注创造价值。</em></p>]]></description></item><item>    <title><![CDATA[上市大模型企业数据基础设施的选择：MiniMax 基于阿里云 SelectDB 版，打造全球统一AI]]></title>    <link>https://segmentfault.com/a/1190000047574942</link>    <guid>https://segmentfault.com/a/1190000047574942</guid>    <pubDate>2026-01-27 13:02:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>MiniMax 全球领先的通用人工智能科技公司。旗下主要有 MiniMax M2.1、Hailuo 2.3、Speech 2.6 和 Music 2.0 等大模型，MiniMax Agent、海螺 AI、星野、Talkie 等产品，以及为企业客户与开发者提供 API 服务的 MiniMax 开放平台。截至目前，MiniMax 已有超过 200 个国家及地区的逾 2.12 亿名个人用户以及超过 100 个国家的企业客户。</p><p>在技术层面，MiniMax 坚持文本、视频、语音等全模态模型自主研发。目前，其全模态模型已进入国际第一梯队，被业内称为“全球唯四实现这一水平的企业之一”。</p><p>在推理能力和效率方面，MiniMax 近年来的模型迭代节奏明显加快，在多项国际评测榜单中进入全球前列。相关模型以较低算力成本实现接近国际顶尖闭源模型的性能表现，也在海外开发者社区中获得关注。</p><p>MiniMax 通过开放平台赋能多个行业，将领先的模型能力以 API 方式提供给企业和开发者。随着模型调用量的指数级增长，<strong>训练与推理产生的运行日志数据量也急剧膨胀</strong>。 这些日志对于 AI 应用的运行监控、性能优化与问题排查至关重要，因此，<strong>选择一款能够支撑高吞吐、易查询、低成本的日志存储与检索引擎，成为保障业务稳定高效运行的关键</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574944" alt="MiniMax 可观测性数据平台核心基座.JPEG" title="MiniMax 可观测性数据平台核心基座.JPEG"/></p><p>面对海量、实时且不断增长的日志数据处理需求，<strong>MiniMax 经过深度评估，最终选择阿里云数据库 SelectDB 版作为其全新可观测性数据平台的核心基座</strong>。阿里云数据库 SelectDB 版凭借其更低的成本、更高的查询性能以及更灵活的查询方式在众多产品中脱颖而出。其关键特性精准匹配了现代 AI 业务的严苛要求：</p><ul><li><strong>云原生存算分离架构</strong>：基于对象存储 OSS 的存储层与弹性计算层解耦，支持独立、无损的弹性伸缩，为应对日志洪峰提供了近乎无限的扩展能力。</li><li><strong>多集群硬隔离与数据共享</strong>：支持云原生多集群硬隔离能力，用户可以将单个实例的计算资源划分为多个逻辑集群，不同集群之间的分配独立的计算资源，实现了不同集群的严格物理资源隔离和数据共享。</li><li><strong>智能缓存加速</strong>：通过单副本本地读写缓存、智能数据淘汰策略、高效列式存储格式和先进压缩算法，显著提升了海量数据的读写效率。</li></ul><blockquote>阿里云数据库 SelectDB 植根于开源 Apache Doris 的坚实基础，深度融合云随需而用的特性，依托阿里云基础设施，构建起云原生存算分离的全新架构，面向企业海量数据的实时分析需求，提供极速实时、湖仓融合统一、简单易用的云上数仓服务。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574945" alt="MiniMax 可观测性数据平台核心基座-1.PNG" title="MiniMax 可观测性数据平台核心基座-1.PNG" loading="lazy"/></p><p>基于阿里云 SelectDB 版，<strong>MiniMax 构建了覆盖国内及海外业务的统一日志可观测中台</strong>。 以 SelectDB 独立负责所有日志的存储与查询分析，实现了 “<strong>一个平台，全球覆盖</strong>” 。这彻底终结了以往为不同业务集群分散部署、独立运维多套系统的复杂局面，在架构上实现了极大的简化。</p><p>阿里云数据库 SelectDB 在 MiniMax 的成功实践足以说明：<strong>SelectDB 能够很好地满足 AI 时代海量数据实时处理与分析的需求</strong>。不仅为 MiniMax 自身业务的高效运营提供了坚实保障，也为广大面临类似日志处理挑战的 AI 大模型企业，提供了一个<a href="https://link.segmentfault.com/?enc=OU7VkeVqH5y4qUMHob40Lw%3D%3D.bwRUs44Ig3lPxywgrejrpOyQgjWEw1xLaQqPeou8FYxmwUzFfBtiMmXgmcFukO90NT%2BTkRwqt2Ih9nTXerLpzravA1kitQ4eE6CahJwGMyNw0f9JLGd%2FlLhbzEt5iAsaamUuTJkcFW8Bq1aaL5tBkdtnFryXLaHUAZEspMzxHi9bKt29EsS1CfgqDYfYksgo" rel="nofollow" target="_blank">高性能、低成本的可靠技术解决方案</a>。</p><p>不止于此， 面对大模型与多模态 AI 的快速发展，SelectDB 已从被动存储分析向主动智能分析演进。目前，SelectDB 已具备 AI 原生支持能力，深度融合<a href="https://link.segmentfault.com/?enc=dolylvKswaMECVMW%2BEnKPQ%3D%3D.aHT1CZz3f10WyewZP1sTlGCB1MOGkdY2INF2VqzHJQiMP2HP%2FQD4roz3293GwcAS7d1mBCqfWf57bmCBgudNeNTam5deLbVEvOSd3S2u%2BSdVF9Kwlni8CXFJS4Ci32ANABvSsTWTVdqD3Rz27RtcttdKqV4qvBoRNvgEMyWXWl9CXvQTgD3sEal04WPoC83p" rel="nofollow" target="_blank">向量索引</a>、文本搜索与结构化分析能力，实现高效的<a href="https://link.segmentfault.com/?enc=DtGP1seusPwYwfCDilAg6g%3D%3D.Py0lUJIMFJ4TxZQTNlXyi%2BYZsSKd4zwM2S0y%2F4WmbQCrwSp%2BDOP6DumZBYe4cOXWq1cJvSmQRdEEOGHnTV25xaJaTl6DexPlGHVmoowUxDajUoYom01IKgQImbmxqqWmDbnORiSnBb%2B01w3kULAJ58i0YiQW8sxxw%2FKAJvoepmjbdGG7%2FEjPY1EKJNbtEisu" rel="nofollow" target="_blank">混合检索</a>，显著提升结果相关性、实时性与准确性。更进一步，SelectDB 内置 <a href="https://link.segmentfault.com/?enc=gW18ZNMzEsaTGakRFRG%2Fiw%3D%3D.ZXG1WtX74B46Dyew0HGH0KSCHZwTJqwhyz61V8cUp4zeG1JkRFTVBMzdMXoa5MLKkYzkHVrjK7U3A4p6eyiy7Yda4UpAAaNmCHiD200nJ5FWiY7wldUo6G9xhbxInHWdk0bDFq80X8tcLxU5sI8VVF9xFE%2FGj2sNU8EtXG0SKXPzONFaP4ANsVeMyY0OhVaD" rel="nofollow" target="_blank">AI 函数</a>（如语义理解、特征提取）并支持<a href="https://link.segmentfault.com/?enc=e7H1Dw2LVv9CRdYyxNdP2Q%3D%3D.A4fYcX9ZLsH%2F6TcyHzUuYyy7LZIXmKaUgI5uy1Z3WvbUOiNMUkLFefdUXRtSiU%2FlN33NFOVN1QtK%2BaKxQa3CkBZl2ItTtin1nSwAZjMjKjsX7%2Bto57x1LY3ogtBRjpxIG0JwsmXpnzJhYuujtujFgCtlGK8fcD8YUdKidYVhPYzN%2FFdAQ6RiRCF4cp9yQCm9" rel="nofollow" target="_blank">基于 MCP 的 Agent 分析接口</a>，可直接升级为企业的 “AI 分析中枢” ，为业务智能决策与创新提供稳定、高效的数据底座。</p>]]></description></item><item>    <title><![CDATA[技术分享 | MySQL8.0物理备份Xtrabackup全备脚本 墨天轮 ]]></title>    <link>https://segmentfault.com/a/1190000047575010</link>    <guid>https://segmentfault.com/a/1190000047575010</guid>    <pubDate>2026-01-27 13:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=TEd7DFcBb0d6Th6BbxZt9Q%3D%3D.YEQW2KJ%2FU%2BJAJVNyve2oZjEosX2qlLY%2BzQcycrJcJ7vzZdgZApSz8bFT1ITc%2BjTZ" rel="nofollow" target="_blank">墨天轮数据库管理服务团队</a>第159期技术分享，内容原创，作者为技术顾问<strong>闫建</strong>，如需转载请联系小墨（VX：modb666）并注明来源。如需查看更多文章可关注【墨天轮】公众号。</p><h2><strong>脚本功能</strong></h2><p>此脚本是专门用于MySQL8.0物理备份的xtrabackup脚本,并且xtrabackup版本为8.0的最新版，它包含了备份数据库（包含全量备份，错误处理，日志记录，自动清理，耗时统计）的完整配置。</p><h2><strong>脚本内容</strong></h2><p>该脚本名称为xtrabackup8.0\_full.sh</p><pre><code class="sql">#!/bin/bash
# ===============================================
# MySQL XtraBackup 全量备份脚本 (默认未启用压缩) 
# 功能：全量备份 + 错误处理 + 日志记录 + 自动清理 + 耗时统计
# 版本：1.0
# 日期：2025-11-04
# ===============================================
# -------------------------------
# 可配置变量（修改此处适应环境）
# -------------------------------
# 基础路径
BACKUP_BASE_DIR="/data/backup"                 # 备份根目录
LOG_DIR="${BACKUP_BASE_DIR}/logs"              # 日志目录
FULL_BACKUP_DIR="${BACKUP_BASE_DIR}/full"      # 全量备份目录
# MySQL 连接配置
MYSQL_USER="root"                              # 备份专用用户（需提前授权）
MYSQL_PASSWORD="mysql"                         # 用户密码
MYSQL_SOCKET="/data/mysql/3306/run/mysql.sock" # MySQL Socket路径
MYSQL_CNF="/etc/my.cnf"                        # MySQL配置文件路径
# 备份参数
RETENTION_DAYS=7                               # 备份保留天数（默认7天）
COMPRESS_ENABLED=0                             # 是否启用压缩（0-禁用，1-启用，默认禁用）
COMPRESS_ALGORITHM="zstd"                      # 压缩算法（zstd/lz4/quicklz，默认zstd，如果采用zstd压缩方式，在解压的时候建议在OS上检查是否安装 zstd软件，如未安装，请提前安装yum install zstd）
COMPRESS_THREADS=4                             # 压缩线程数（默认4）
PARALLEL=4                                     # 备份并行线程数（默认4）
# XtraBackup路径（若不在PATH中，需指定完整路径，注意：该路径需要提前修改为符合实际环境的绝对路径）
XTRABACKUP_PATH="/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup"
# -------------------------------
# 函数定义
# -------------------------------
# 日志记录函数
log_message() {
    local level="$1"
    local message="$2"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    local log_entry="[${timestamp}] [${level}] ${message}"
    # 输出到标准输出并写入日志文件
    echo "${log_entry}" | tee -a "${CURRENT_LOG_FILE}"
}
# 错误处理函数
error_exit() {
    log_message "ERROR" "脚本执行失败: $1"
    exit 1
}
# 计算和格式化时间差函数
calculate_duration() {
    local start_seconds="$1"
    local end_seconds="$2"
    local total_seconds=$((end_seconds - start_seconds))
    # 转换为天、小时、分钟、秒的易读格式
    local days=$((total_seconds / 86400))
    local hours=$(( (total_seconds % 86400) / 3600 ))
    local minutes=$(( (total_seconds % 3600) / 60 ))
    local seconds=$((total_seconds % 60))
    local duration_str=""
    if [ $days -gt 0 ]; then
        duration_str="${days}天"
    fi
    if [ $hours -gt 0 ] || [ -n "$duration_str" ]; then
        duration_str="${duration_str}${hours}小时"
    fi
    if [ $minutes -gt 0 ] || [ -n "$duration_str" ]; then
        duration_str="${duration_str}${minutes}分钟"
    fi
    duration_str="${duration_str}${seconds}秒"
    echo "$duration_str"
}
# 清理旧备份函数
cleanup_old_backups() {
    log_message "INFO" "开始清理超过 ${RETENTION_DAYS} 天的旧备份..."
    local deleted_count=0
    # 删除过期全量备份及相关日志
    if find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} | grep -q .; then
        find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} -exec rm -rf {} \;
        deleted_count=$(find "${BACKUP_BASE_DIR}" -name "full_*" -type d -mtime +${RETENTION_DAYS} | wc -l)
        log_message "INFO" "已清理 ${deleted_count} 个过期全量备份"
    else
        log_message "INFO" "未找到需要清理的过期备份"
    fi
    # 清理旧日志文件（保留30天）
    find "${LOG_DIR}" -name "backup_*.log" -type f -mtime +30 -delete
}
# 检查依赖项
check_dependencies() {
    local deps=("${XTRABACKUP_PATH}" "mysql")
    for cmd in "${deps[@]}"; do
        if ! command -v "${cmd}" &amp;&gt; /dev/null; then
            error_exit "未找到所需命令: ${cmd}，请确保已安装并配置PATH"
        fi
    done
    # 检查备份目录权限
    if [ ! -w "${BACKUP_BASE_DIR}" ]; then
        error_exit "备份目录不可写: ${BACKUP_BASE_DIR}"
    fi
}
# 备份前置检查
pre_backup_checks() {
    log_message "INFO" "开始备份前置检查..."
    # 检查MySQL连接
    if ! mysql --user="${MYSQL_USER}" --password="${MYSQL_PASSWORD}" --socket="${MYSQL_SOCKET}" -e "SELECT 1;" &amp;&gt; /dev/null; then
        error_exit "MySQL连接测试失败，请检查凭据和Socket路径"
    fi
    # 检查XtraBackup版本
    local xtrabackup_version
    xtrabackup_version=$(${XTRABACKUP_PATH} --version 2&gt;&amp;1 | tail -n1 || echo "未知")
    log_message "INFO" "使用XtraBackup版本: ${xtrabackup_version}"
    # 检查磁盘空间（至少保留10GB）
    local available_space
    available_space=$(df "${BACKUP_BASE_DIR}" | awk 'NR==2 {print $4}')
    if [ "${available_space}" -lt 10485760 ]; then  # 10GB in KB
        error_exit "磁盘空间不足10GB，当前可用: ${available_space}KB"
    fi
}
# -------------------------------
# 主脚本逻辑
# -------------------------------
main() {
    # 初始化变量
    local backup_timestamp=$(date '+%Y%m%d_%H%M%S')
    local backup_name="full_${backup_timestamp}"
    local current_backup_dir="${FULL_BACKUP_DIR}/${backup_name}"
    CURRENT_LOG_FILE="${LOG_DIR}/backup_${backup_timestamp}.log"
    # 记录备份开始时间（秒级时间戳）[6,8](@ref)
    local backup_start_time=$(date +%s)
    local backup_start_readable=$(date '+%Y-%m-%d %H:%M:%S')
    # 创建目录
    mkdir -p "${FULL_BACKUP_DIR}" "${LOG_DIR}"
    log_message "INFO" "=== MySQL全量备份开始 ==="
    log_message "INFO" "备份开始时间: ${backup_start_readable}"
    log_message "INFO" "备份名称: ${backup_name}"
    log_message "INFO" "备份目录: ${current_backup_dir}"
    log_message "INFO" "保留天数: ${RETENTION_DAYS}"
#    log_message "INFO" "压缩启用: ${COMPRESS_ENABLED} (算法: ${COMPRESS_ALGORITHM}, 线程: ${COMPRESS_THREADS})"
    # 执行检查
    check_dependencies
    pre_backup_checks
    # 构建备份命令
    local backup_cmd="${XTRABACKUP_PATH} --defaults-file=${MYSQL_CNF} --backup"
    backup_cmd+=" --target-dir=${current_backup_dir}"
    backup_cmd+=" --user=${MYSQL_USER} --password=${MYSQL_PASSWORD}"
    backup_cmd+=" --socket=${MYSQL_SOCKET} --parallel=${PARALLEL}"
    # 压缩配置
    if [ "${COMPRESS_ENABLED}" -eq 1 ]; then
        log_message "INFO" "启用压缩算法: ${COMPRESS_ALGORITHM}, 线程数: ${COMPRESS_THREADS}"
        backup_cmd+=" --compress=${COMPRESS_ALGORITHM} --compress-threads=${COMPRESS_THREADS}"
    else
        log_message "INFO" "备份未启用压缩"
    fi
    # 执行备份
    log_message "INFO" "开始执行 XtraBackup 全量备份..."
    log_message "DEBUG" "备份命令: ${backup_cmd//--password=* /--password=*** }"  # 屏蔽密码显示
    # 记录备份操作开始时间
    local backup_operation_start=$(date +%s)
    if eval "${backup_cmd}" &gt;&gt; "${CURRENT_LOG_FILE}" 2&gt;&amp;1; then
        local backup_operation_end=$(date +%s)
        local backup_duration=$(calculate_duration $backup_operation_start $backup_operation_end)
        log_message "INFO" "XtraBackup全量备份完成，备份操作耗时: ${backup_duration}"
        # 验证备份完整性
        if [ -f "${current_backup_dir}/xtrabackup_checkpoints" ]; then
            local backup_type
            backup_type=$(grep "backup_type" "${current_backup_dir}/xtrabackup_checkpoints" | cut -d= -f2)
            log_message "INFO" "备份类型验证: ${backup_type}"
        else
            error_exit "备份完整性检查失败: xtrabackup_checkpoints文件缺失"
        fi
    else
        error_exit "XtraBackup备份过程失败，请检查日志: ${CURRENT_LOG_FILE}"
    fi
    # 自动清理旧备份
    cleanup_old_backups
    # 计算总耗时
    local backup_end_time=$(date +%s)
    local backup_end_readable=$(date '+%Y-%m-%d %H:%M:%S')
    local total_duration=$(calculate_duration $backup_start_time $backup_end_time)
    # 计算备份大小
    local backup_size
    backup_size=$(du -sh "${current_backup_dir}" | awk '{print $1}')
    log_message "INFO" "备份完成: ${backup_name} (大小: ${backup_size})"
    log_message "INFO" "备份开始: ${backup_start_readable}"
    log_message "INFO" "备份结束: ${backup_end_readable}"
    log_message "INFO" "备份总耗时: ${total_duration}"
    log_message "INFO" "备份日志: ${CURRENT_LOG_FILE}"
    log_message "INFO" "=== MySQL全量备份结束 ==="
}
# 异常处理（捕获中断信号）
trap 'log_message "ERROR" "脚本被用户中断"; exit 2;' INT TERM
# 脚本入口点
main "$@"</code></pre><h2><strong>重点说明</strong></h2><p>在脚本最开始部分需要根据实际数据库环境来配置，包括用户名，密码，路径等详细信息，参考如下：</p><pre><code class="sql">BACKUP_BASE_DIR="/data/backup"                 # 备份根目录
LOG_DIR="${BACKUP_BASE_DIR}/logs"              # 日志目录
FULL_BACKUP_DIR="${BACKUP_BASE_DIR}/full"      # 全量备份目录
# MySQL 连接配置
MYSQL_USER="root"                              # 备份专用用户（需提前授权）
MYSQL_PASSWORD="mysql"                         # 用户密码
MYSQL_SOCKET="/data/mysql/3306/run/mysql.sock" # MySQL Socket路径
MYSQL_CNF="/etc/my.cnf"                        # MySQL配置文件路径
# 备份参数
RETENTION_DAYS=7                               # 备份保留天数（默认7天）
COMPRESS_ENABLED=0                             # 是否启用压缩（0-禁用，1-启用，默认禁用）
COMPRESS_ALGORITHM="zstd"                      # 压缩算法（zstd/lz4/quicklz，默认zstd，如果采用zstd压缩方式，在解压的时候建议在OS上检查是否安装 zstd软件，如未安装，请提前安装yum install zstd）
COMPRESS_THREADS=4                             # 压缩线程数（默认4）
PARALLEL=4                                     # 备份并行线程数（默认4）
# XtraBackup路径（若不在PATH中，需指定完整路径，注意：该路径需要提前修改为符合实际环境的绝对路径）
XTRABACKUP_PATH="/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup"</code></pre><h2><strong>使用方法</strong></h2><ol><li><p>保脚本并赋予执行权限</p><pre><code class="sql">chmod +x xtrabackup8.0_full.sh</code></pre></li><li>手动执行备份</li></ol><pre><code class="sql">[root@VM-8-4-opencloudos backup]# ./xtrabackup8.0_full.sh 
[2025-11-04 17:34:34] [INFO] === MySQL全量备份开始 ===
[2025-11-04 17:34:34] [INFO] 备份名称: full_20251104_173434
[2025-11-04 17:34:34] [INFO] 备份目录: /data/backup/full/full_20251104_173434
[2025-11-04 17:34:34] [INFO] 保留天数: 7
[2025-11-04 17:34:34] [INFO] 开始备份前置检查...
[2025-11-04 17:34:35] [INFO] 使用XtraBackup版本: /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
[2025-11-04 17:34:35] [INFO] 启用压缩算法: zstd, 线程数: 4
[2025-11-04 17:34:35] [INFO] 开始执行XtraBackup全量备份...
[2025-11-04 17:34:35] [DEBUG] 备份命令: /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --defaults-file=/etc/my.cnf --backup --target-dir=/data/backup/full/full_20251104_173434 --user=root --password=*** --compress-threads=4
[2025-11-04 17:34:41] [INFO] XtraBackup全量备份完成
[2025-11-04 17:34:41] [INFO] 备份类型验证:  full-backuped
[2025-11-04 17:34:41] [INFO] 开始清理超过 7 天的旧备份...
[2025-11-04 17:34:41] [INFO] 未找到需要清理的过期备份
[2025-11-04 17:34:41] [INFO] 备份完成: full_20251104_173434 (大小: 43M)
[2025-11-04 17:34:41] [INFO] 备份日志: /data/backup/logs/backup_20251104_173434.log
[2025-11-04 17:34:41] [INFO] === MySQL全量备份结束 ===
[root@VM-8-4-opencloudos backup]# </code></pre><ol><li>配置定时任务（每日凌晨1点执行）</li></ol><pre><code class="sql"># 编辑crontab：crontab -e 添加如下内容并保存
0 1 * * * /path/to/xtrabackup8.0_full.sh </code></pre><h2><strong>恢 复</strong></h2><p>1、解压缩（如采用压缩备份，此步骤为必须执行步骤，非压缩备份，此步骤忽略）</p><pre><code class="sql">/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --decompress --remove-original --target-dir=/data/backup/full/full_20251104_173434
2025-11-04T17:40:22.986591+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized server arguments: --server-id=13045 --innodb_io_capacity=2000 --datadir=/data/mysql/3306/data --log_bin=/data/mysql/3306/binlogs/mysql-bin --tmpdir=/data/mysql/3306/tmp --innodb_buffer_pool_size=1G --innodb_data_file_path=ibdata1:200M;ibdata2:200M:autoextend --innodb_flush_method=O_DIRECT --innodb_adaptive_hash_index=0 
2025-11-04T17:40:22.986824+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized client arguments: --port=3306 --socket=/data/mysql/3306/run/mysql.sock --decompress=1 --remove-original=1 --target-dir=/data/backup/full/full_20251104_173434 
/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
2025-11-04T17:40:22.988436+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_logfile.zst
2025-11-04T17:40:23.000940+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_logfile.zst
2025-11-04T17:40:23.001025+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ibdata1.zst
2025-11-04T17:40:23.330061+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ibdata1.zst
2025-11-04T17:40:23.344696+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ibdata2.zst
2025-11-04T17:40:23.903032+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ibdata2.zst
2025-11-04T17:40:23.903125+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/large_table.ibd.zst
2025-11-04T17:40:24.496119+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/large_table.ibd.zst
2025-11-04T17:40:24.594530+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/my_table.ibd.zst
2025-11-04T17:40:24.608587+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/my_table.ibd.zst
2025-11-04T17:40:24.616871+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/tt_new.ibd.zst
2025-11-04T17:40:24.629940+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/tt_new.ibd.zst
2025-11-04T17:40:24.630190+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/tt.ibd.zst
2025-11-04T17:40:24.635994+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/tt.ibd.zst
2025-11-04T17:40:24.636477+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/a.ibd.zst
2025-11-04T17:40:24.643492+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/a.ibd.zst
2025-11-04T17:40:24.644126+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./testdb/b.ibd.zst
2025-11-04T17:40:24.650521+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./testdb/b.ibd.zst
2025-11-04T17:40:24.650961+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./sys/sys_config.ibd.zst
2025-11-04T17:40:24.655719+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./sys/sys_config.ibd.zst
2025-11-04T17:40:24.656597+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql.ibd.zst
2025-11-04T17:40:24.695278+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql.ibd.zst
2025-11-04T17:40:24.699656+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./undo_002.zst
2025-11-04T17:40:24.722718+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./undo_002.zst
2025-11-04T17:40:24.722825+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./undo_001.zst
2025-11-04T17:40:24.755290+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./undo_001.zst
2025-11-04T17:40:24.755392+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log_213.sdi.zst
2025-11-04T17:40:24.760715+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log_213.sdi.zst
2025-11-04T17:40:24.760830+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log.CSM.zst
2025-11-04T17:40:24.766136+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log.CSM.zst
2025-11-04T17:40:24.766202+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/general_log.CSV.zst
2025-11-04T17:40:24.771030+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/general_log.CSV.zst
2025-11-04T17:40:24.771184+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log_214.sdi.zst
2025-11-04T17:40:24.776180+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log_214.sdi.zst
2025-11-04T17:40:24.776240+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log.CSM.zst
2025-11-04T17:40:24.781393+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log.CSM.zst
2025-11-04T17:40:24.781449+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql/slow_log.CSV.zst
2025-11-04T17:40:24.785794+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql/slow_log.CSV.zst
2025-11-04T17:40:24.785890+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/cond_instances_82.sdi.zst
2025-11-04T17:40:24.790641+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/cond_instances_82.sdi.zst
2025-11-04T17:40:24.790708+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/error_log_83.sdi.zst
2025-11-04T17:40:24.795417+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/error_log_83.sdi.zst
2025-11-04T17:40:24.795508+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_his_85.sdi.zst
2025-11-04T17:40:24.800525+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_his_85.sdi.zst
2025-11-04T17:40:24.800589+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_cur_84.sdi.zst
2025-11-04T17:40:24.804941+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_cur_84.sdi.zst
2025-11-04T17:40:24.805008+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_his_86.sdi.zst
2025-11-04T17:40:24.809624+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_his_86.sdi.zst
2025-11-04T17:40:24.809685+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_87.sdi.zst
2025-11-04T17:40:24.814165+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_87.sdi.zst
2025-11-04T17:40:24.814218+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_88.sdi.zst
2025-11-04T17:40:24.818631+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_88.sdi.zst
2025-11-04T17:40:24.818700+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_89.sdi.zst
2025-11-04T17:40:24.823346+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_89.sdi.zst
2025-11-04T17:40:24.823434+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_90.sdi.zst
2025-11-04T17:40:24.828325+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_90.sdi.zst
2025-11-04T17:40:24.828459+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_91.sdi.zst
2025-11-04T17:40:24.832657+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_91.sdi.zst
2025-11-04T17:40:24.832719+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_waits_sum_92.sdi.zst
2025-11-04T17:40:24.837125+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_waits_sum_92.sdi.zst
2025-11-04T17:40:24.837186+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_instances_93.sdi.zst
2025-11-04T17:40:24.841260+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_instances_93.sdi.zst
2025-11-04T17:40:24.841325+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_summary_by__94.sdi.zst
2025-11-04T17:40:24.845642+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_summary_by__94.sdi.zst
2025-11-04T17:40:24.845708+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/file_summary_by__95.sdi.zst
2025-11-04T17:40:24.850048+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/file_summary_by__95.sdi.zst
2025-11-04T17:40:24.850109+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/host_cache_96.sdi.zst
2025-11-04T17:40:24.854740+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/host_cache_96.sdi.zst
2025-11-04T17:40:24.854804+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/mutex_instances_97.sdi.zst
2025-11-04T17:40:24.859652+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/mutex_instances_97.sdi.zst
2025-11-04T17:40:24.859725+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/objects_summary__98.sdi.zst
2025-11-04T17:40:24.864826+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/objects_summary__98.sdi.zst
2025-11-04T17:40:24.864905+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/performance_time_99.sdi.zst
2025-11-04T17:40:24.869709+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/performance_time_99.sdi.zst
2025-11-04T17:40:24.869779+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/rwlock_instances_101.sdi.zst
2025-11-04T17:40:24.874756+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/rwlock_instances_101.sdi.zst
2025-11-04T17:40:24.874869+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/processlist_100.sdi.zst
2025-11-04T17:40:24.879190+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/processlist_100.sdi.zst
2025-11-04T17:40:24.879289+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_actors_102.sdi.zst
2025-11-04T17:40:24.886036+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_actors_102.sdi.zst
2025-11-04T17:40:24.886108+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_consumers_103.sdi.zst
2025-11-04T17:40:24.890831+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_consumers_103.sdi.zst
2025-11-04T17:40:24.890957+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_instrument_104.sdi.zst
2025-11-04T17:40:24.895561+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_instrument_104.sdi.zst
2025-11-04T17:40:24.895670+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_objects_105.sdi.zst
2025-11-04T17:40:24.899829+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_objects_105.sdi.zst
2025-11-04T17:40:24.899946+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/setup_threads_106.sdi.zst
2025-11-04T17:40:24.904601+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/setup_threads_106.sdi.zst
2025-11-04T17:40:24.904658+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_io_waits_s_107.sdi.zst
2025-11-04T17:40:24.909639+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_io_waits_s_107.sdi.zst
2025-11-04T17:40:24.909709+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_io_waits_s_108.sdi.zst
2025-11-04T17:40:24.914081+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_io_waits_s_108.sdi.zst
2025-11-04T17:40:24.914148+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_lock_waits_109.sdi.zst
2025-11-04T17:40:24.918343+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_lock_waits_109.sdi.zst
2025-11-04T17:40:24.918421+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/threads_110.sdi.zst
2025-11-04T17:40:24.922998+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/threads_110.sdi.zst
2025-11-04T17:40:24.923064+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_cu_111.sdi.zst
2025-11-04T17:40:24.927213+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_cu_111.sdi.zst
2025-11-04T17:40:24.927276+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_hi_112.sdi.zst
2025-11-04T17:40:24.932218+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_hi_112.sdi.zst
2025-11-04T17:40:24.932280+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_hi_113.sdi.zst
2025-11-04T17:40:24.937361+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_hi_113.sdi.zst
2025-11-04T17:40:24.937457+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_114.sdi.zst
2025-11-04T17:40:24.941918+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_114.sdi.zst
2025-11-04T17:40:24.941991+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_115.sdi.zst
2025-11-04T17:40:24.946597+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_115.sdi.zst
2025-11-04T17:40:24.946667+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_116.sdi.zst
2025-11-04T17:40:24.951119+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_116.sdi.zst
2025-11-04T17:40:24.951190+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_117.sdi.zst
2025-11-04T17:40:24.956165+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_117.sdi.zst
2025-11-04T17:40:24.956236+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_stages_su_118.sdi.zst
2025-11-04T17:40:24.960626+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_stages_su_118.sdi.zst
2025-11-04T17:40:24.960698+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_119.sdi.zst
2025-11-04T17:40:24.964982+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_119.sdi.zst
2025-11-04T17:40:24.965041+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_120.sdi.zst
2025-11-04T17:40:24.969978+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_120.sdi.zst
2025-11-04T17:40:24.970090+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_121.sdi.zst
2025-11-04T17:40:24.974550+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_121.sdi.zst
2025-11-04T17:40:24.974613+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_122.sdi.zst
2025-11-04T17:40:24.978736+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_122.sdi.zst
2025-11-04T17:40:24.978800+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_123.sdi.zst
2025-11-04T17:40:24.987207+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_123.sdi.zst
2025-11-04T17:40:24.987434+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_124.sdi.zst
2025-11-04T17:40:25.012604+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_124.sdi.zst
2025-11-04T17:40:25.013300+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_125.sdi.zst
2025-11-04T17:40:25.019070+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_125.sdi.zst
2025-11-04T17:40:25.019146+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_127.sdi.zst
2025-11-04T17:40:25.025291+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_127.sdi.zst
2025-11-04T17:40:25.025419+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_126.sdi.zst
2025-11-04T17:40:25.031834+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_126.sdi.zst
2025-11-04T17:40:25.031916+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_128.sdi.zst
2025-11-04T17:40:25.036534+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_128.sdi.zst
2025-11-04T17:40:25.036608+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_129.sdi.zst
2025-11-04T17:40:25.041247+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_129.sdi.zst
2025-11-04T17:40:25.041317+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_statement_130.sdi.zst
2025-11-04T17:40:25.045871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_statement_130.sdi.zst
2025-11-04T17:40:25.045942+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_131.sdi.zst
2025-11-04T17:40:25.050464+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_131.sdi.zst
2025-11-04T17:40:25.050536+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_132.sdi.zst
2025-11-04T17:40:25.055547+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_132.sdi.zst
2025-11-04T17:40:25.055674+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_133.sdi.zst
2025-11-04T17:40:25.060633+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_133.sdi.zst
2025-11-04T17:40:25.060754+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_134.sdi.zst
2025-11-04T17:40:25.065871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_134.sdi.zst
2025-11-04T17:40:25.065989+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_135.sdi.zst
2025-11-04T17:40:25.071009+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_135.sdi.zst
2025-11-04T17:40:25.071126+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_136.sdi.zst
2025-11-04T17:40:25.075979+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_136.sdi.zst
2025-11-04T17:40:25.076093+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_137.sdi.zst
2025-11-04T17:40:25.080663+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_137.sdi.zst
2025-11-04T17:40:25.080721+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_transacti_138.sdi.zst
2025-11-04T17:40:25.084942+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_transacti_138.sdi.zst
2025-11-04T17:40:25.085063+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_139.sdi.zst
2025-11-04T17:40:25.089051+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_139.sdi.zst
2025-11-04T17:40:25.089176+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_140.sdi.zst
2025-11-04T17:40:25.093221+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_140.sdi.zst
2025-11-04T17:40:25.093344+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_141.sdi.zst
2025-11-04T17:40:25.097822+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_141.sdi.zst
2025-11-04T17:40:25.097901+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_142.sdi.zst
2025-11-04T17:40:25.102195+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_142.sdi.zst
2025-11-04T17:40:25.102260+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/events_errors_su_143.sdi.zst
2025-11-04T17:40:25.106506+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/events_errors_su_143.sdi.zst
2025-11-04T17:40:25.106581+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/users_144.sdi.zst
2025-11-04T17:40:25.111263+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/users_144.sdi.zst
2025-11-04T17:40:25.111318+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/accounts_145.sdi.zst
2025-11-04T17:40:25.115990+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/accounts_145.sdi.zst
2025-11-04T17:40:25.116105+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/hosts_146.sdi.zst
2025-11-04T17:40:25.120821+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/hosts_146.sdi.zst
2025-11-04T17:40:25.120958+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_instances_147.sdi.zst
2025-11-04T17:40:25.124883+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_instances_147.sdi.zst
2025-11-04T17:40:25.125002+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_summary_b_148.sdi.zst
2025-11-04T17:40:25.129788+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_summary_b_148.sdi.zst
2025-11-04T17:40:25.129929+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/socket_summary_b_149.sdi.zst
2025-11-04T17:40:25.134033+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/socket_summary_b_149.sdi.zst
2025-11-04T17:40:25.134151+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_connect__150.sdi.zst
2025-11-04T17:40:25.138105+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_connect__150.sdi.zst
2025-11-04T17:40:25.138158+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_account__151.sdi.zst
2025-11-04T17:40:25.142150+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_account__151.sdi.zst
2025-11-04T17:40:25.142285+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/keyring_keys_152.sdi.zst
2025-11-04T17:40:25.146409+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/keyring_keys_152.sdi.zst
2025-11-04T17:40:25.146473+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_g_153.sdi.zst
2025-11-04T17:40:25.151175+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_g_153.sdi.zst
2025-11-04T17:40:25.151298+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_154.sdi.zst
2025-11-04T17:40:25.155247+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_154.sdi.zst
2025-11-04T17:40:25.155535+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_155.sdi.zst
2025-11-04T17:40:25.159725+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_155.sdi.zst
2025-11-04T17:40:25.159776+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_156.sdi.zst
2025-11-04T17:40:25.164259+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_156.sdi.zst
2025-11-04T17:40:25.164397+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/memory_summary_b_157.sdi.zst
2025-11-04T17:40:25.168750+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/memory_summary_b_157.sdi.zst
2025-11-04T17:40:25.168825+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/table_handles_158.sdi.zst
2025-11-04T17:40:25.173560+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/table_handles_158.sdi.zst
2025-11-04T17:40:25.173620+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/metadata_locks_159.sdi.zst
2025-11-04T17:40:25.177949+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/metadata_locks_159.sdi.zst
2025-11-04T17:40:25.178012+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/data_locks_160.sdi.zst
2025-11-04T17:40:25.182902+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/data_locks_160.sdi.zst
2025-11-04T17:40:25.182965+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/data_lock_waits_161.sdi.zst
2025-11-04T17:40:25.187544+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/data_lock_waits_161.sdi.zst
2025-11-04T17:40:25.187609+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_conn_162.sdi.zst
2025-11-04T17:40:25.192416+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_conn_162.sdi.zst
2025-11-04T17:40:25.192485+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_grou_163.sdi.zst
2025-11-04T17:40:25.197412+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_grou_163.sdi.zst
2025-11-04T17:40:25.197478+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_conn_164.sdi.zst
2025-11-04T17:40:25.201947+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_conn_164.sdi.zst
2025-11-04T17:40:25.202015+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_165.sdi.zst
2025-11-04T17:40:25.206022+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_165.sdi.zst
2025-11-04T17:40:25.206082+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_166.sdi.zst
2025-11-04T17:40:25.210440+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_166.sdi.zst
2025-11-04T17:40:25.210503+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_167.sdi.zst
2025-11-04T17:40:25.214871+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_167.sdi.zst
2025-11-04T17:40:25.214936+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_168.sdi.zst
2025-11-04T17:40:25.219740+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_168.sdi.zst
2025-11-04T17:40:25.219814+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_grou_169.sdi.zst
2025-11-04T17:40:25.224172+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_grou_169.sdi.zst
2025-11-04T17:40:25.224245+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_170.sdi.zst
2025-11-04T17:40:25.228351+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_170.sdi.zst
2025-11-04T17:40:25.228468+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_appl_171.sdi.zst
2025-11-04T17:40:25.233203+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_appl_171.sdi.zst
2025-11-04T17:40:25.233319+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_asyn_172.sdi.zst
2025-11-04T17:40:25.238933+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_asyn_172.sdi.zst
2025-11-04T17:40:25.239094+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/replication_asyn_173.sdi.zst
2025-11-04T17:40:25.246187+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/replication_asyn_173.sdi.zst
2025-11-04T17:40:25.246284+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/log_status_174.sdi.zst
2025-11-04T17:40:25.253874+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/log_status_174.sdi.zst
2025-11-04T17:40:25.253967+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/prepared_stateme_175.sdi.zst
2025-11-04T17:40:25.272402+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/prepared_stateme_175.sdi.zst
2025-11-04T17:40:25.272496+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/user_variables_b_176.sdi.zst
2025-11-04T17:40:25.280533+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/user_variables_b_176.sdi.zst
2025-11-04T17:40:25.280617+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_accoun_177.sdi.zst
2025-11-04T17:40:25.288632+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_accoun_177.sdi.zst
2025-11-04T17:40:25.288727+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_host_178.sdi.zst
2025-11-04T17:40:25.296271+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_host_178.sdi.zst
2025-11-04T17:40:25.296471+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_thread_179.sdi.zst
2025-11-04T17:40:25.303218+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_thread_179.sdi.zst
2025-11-04T17:40:25.303416+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/status_by_user_180.sdi.zst
2025-11-04T17:40:25.315732+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/status_by_user_180.sdi.zst
2025-11-04T17:40:25.315917+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/global_status_181.sdi.zst
2025-11-04T17:40:25.321558+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/global_status_181.sdi.zst
2025-11-04T17:40:25.321637+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_status_182.sdi.zst
2025-11-04T17:40:25.325961+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_status_182.sdi.zst
2025-11-04T17:40:25.326037+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/variables_by_thr_183.sdi.zst
2025-11-04T17:40:25.330807+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/variables_by_thr_183.sdi.zst
2025-11-04T17:40:25.330892+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/global_variables_184.sdi.zst
2025-11-04T17:40:25.335674+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/global_variables_184.sdi.zst
2025-11-04T17:40:25.335749+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/session_variable_185.sdi.zst
2025-11-04T17:40:25.340530+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/session_variable_185.sdi.zst
2025-11-04T17:40:25.340655+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/variables_info_186.sdi.zst
2025-11-04T17:40:25.345478+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/variables_info_186.sdi.zst
2025-11-04T17:40:25.345584+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/persisted_variab_187.sdi.zst
2025-11-04T17:40:25.349818+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/persisted_variab_187.sdi.zst
2025-11-04T17:40:25.349892+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/user_defined_fun_188.sdi.zst
2025-11-04T17:40:25.354039+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/user_defined_fun_188.sdi.zst
2025-11-04T17:40:25.354102+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/binary_log_trans_189.sdi.zst
2025-11-04T17:40:25.358649+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/binary_log_trans_189.sdi.zst
2025-11-04T17:40:25.358752+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/tls_channel_stat_190.sdi.zst
2025-11-04T17:40:25.362754+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/tls_channel_stat_190.sdi.zst
2025-11-04T17:40:25.362826+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/keyring_componen_191.sdi.zst
2025-11-04T17:40:25.366995+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/keyring_componen_191.sdi.zst
2025-11-04T17:40:25.367058+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/clone_status_406.sdi.zst
2025-11-04T17:40:25.371839+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/clone_status_406.sdi.zst
2025-11-04T17:40:25.371910+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./performance_schema/clone_progress_407.sdi.zst
2025-11-04T17:40:25.376354+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./performance_schema/clone_progress_407.sdi.zst
2025-11-04T17:40:25.376483+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql-bin.000025.zst
2025-11-04T17:40:25.380522+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql-bin.000025.zst
2025-11-04T17:40:25.380581+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./mysql-bin.index.zst
2025-11-04T17:40:25.384675+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./mysql-bin.index.zst
2025-11-04T17:40:25.384736+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_binlog_info.zst
2025-11-04T17:40:25.389292+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_binlog_info.zst
2025-11-04T17:40:25.389350+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./ib_buffer_pool.zst
2025-11-04T17:40:25.393580+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./ib_buffer_pool.zst
2025-11-04T17:40:25.393640+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./backup-my.cnf.zst
2025-11-04T17:40:25.398334+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./backup-my.cnf.zst
2025-11-04T17:40:25.398411+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_info.zst
2025-11-04T17:40:25.402541+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_info.zst
2025-11-04T17:40:25.402600+08:00 0 [Note] [MY-011825] [Xtrabackup] decompressing ./xtrabackup_tablespaces.zst
2025-11-04T17:40:25.407024+08:00 0 [Note] [MY-011825] [Xtrabackup] removing ./xtrabackup_tablespaces.zst
2025-11-04T17:40:25.510589+08:00 0 [Note] [MY-011825] [Xtrabackup] completed OK! </code></pre><p>2、prepare步骤</p><p>xtrabackup --prepare --target-dir=/data/backup/full/full\_20251104\_173434</p><p>prepare是物理备份过程中进行恢复前必做的一个环节，它的作用是将备份时处于不一致状态的数据文件，处理成一个具有数据一致性的、可供数据库直接启动和使用的完整备份集。</p><p>简单来说，如果不执行 --prepare步骤，直接使用 --backup得到的原始备份文件来启动数据库，InnoDB存储引擎会检测到数据文件内部不一致（例如页面LSN不匹配），并将其视为损坏的数据文件，从而拒绝启动。</p><p>这是因为XtraBackup在备份期间，为了尽可能减少对数据库性能的影响，是采用类似“快照”的方式拷贝InnoDB数据文件（.ibd）的。拷贝过程中，数据库可能仍在处理事务，这就导致备份集内的数据文件在同一时刻的状态可能并不一致。–prepare步骤正是通过应用备份期间同时拷贝的redo日志，来修复这种不一致性，确保数据恢复到备份操作完成那一刻的一致性状态。</p><pre><code class="sql">[root@VM-8-4-opencloudos backup]#  /data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup --prepare --target-dir=/data/backup/full/full_20251104_173434
2025-11-04T17:44:13.715402+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized server arguments: --innodb_checksum_algorithm=crc32 --innodb_log_checksums=1 --innodb_data_file_path=ibdata1:200M;ibdata2:200M:autoextend --innodb_log_file_size=50331648 --innodb_page_size=16384 --innodb_undo_directory=./ --innodb_undo_tablespaces=2 --server-id=13045 --innodb_log_checksums=ON --innodb_redo_log_encrypt=0 --innodb_undo_log_encrypt=0 
2025-11-04T17:44:13.715565+08:00 0 [Note] [MY-011825] [Xtrabackup] recognized client arguments: --prepare=1 --target-dir=/data/backup/full/full_20251104_173434 
/data/soft/xtrabackup8.0.35-34-glibc2.36/bin/xtrabackup version 8.0.35-34 based on MySQL server 8.0.35 Linux (x86_64) (revision id: c8a25ff9)
2025-11-04T17:44:13.715602+08:00 0 [Note] [MY-011825] [Xtrabackup] cd to /data/backup/full/full_20251104_173434/
2025-11-04T17:44:13.716155+08:00 0 [Note] [MY-011825] [Xtrabackup] This target seems to be not prepared yet.
2025-11-04T17:44:13.730583+08:00 0 [Note] [MY-011825] [Xtrabackup] xtrabackup_logfile detected: size=8388608, start_lsn=(395899109)
2025-11-04T17:44:13.743037+08:00 0 [Note] [MY-011825] [Xtrabackup] using the following InnoDB configuration for recovery:
2025-11-04T17:44:13.743072+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_home_dir = .
2025-11-04T17:44:13.743084+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_file_path = ibdata1:200M;ibdata2:200M:autoextend
2025-11-04T17:44:13.743119+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_group_home_dir = .
2025-11-04T17:44:13.743127+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_files_in_group = 1
2025-11-04T17:44:13.743135+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_file_size = 8388608
2025-11-04T17:44:13.744761+08:00 0 [Note] [MY-011825] [Xtrabackup] inititialize_service_handles suceeded
2025-11-04T17:44:13.745043+08:00 0 [Note] [MY-011825] [Xtrabackup] using the following InnoDB configuration for recovery:
2025-11-04T17:44:13.745062+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_home_dir = .
2025-11-04T17:44:13.745069+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_data_file_path = ibdata1:200M;ibdata2:200M:autoextend
2025-11-04T17:44:13.745089+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_group_home_dir = .
2025-11-04T17:44:13.745100+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_files_in_group = 1
2025-11-04T17:44:13.745110+08:00 0 [Note] [MY-011825] [Xtrabackup] innodb_log_file_size = 8388608
2025-11-04T17:44:13.745125+08:00 0 [Note] [MY-011825] [Xtrabackup] Starting InnoDB instance for recovery.
2025-11-04T17:44:13.745137+08:00 0 [Note] [MY-011825] [Xtrabackup] Using 104857600 bytes for buffer pool (set by --use-memory parameter)
2025-11-04T17:44:13.746141+08:00 0 [Note] [MY-012932] [InnoDB] PUNCH HOLE support available
2025-11-04T17:44:13.746162+08:00 0 [Note] [MY-012944] [InnoDB] Uses event mutexes
2025-11-04T17:44:13.746170+08:00 0 [Note] [MY-012945] [InnoDB] GCC builtin __atomic_thread_fence() is used for memory barrier
2025-11-04T17:44:13.746183+08:00 0 [Note] [MY-012948] [InnoDB] Compressed tables use zlib 1.2.13
2025-11-04T17:44:13.746475+08:00 0 [Note] [MY-012951] [InnoDB] Using hardware accelerated crc32 and polynomial multiplication.
2025-11-04T17:44:13.746898+08:00 0 [Note] [MY-012203] [InnoDB] Directories to scan './'
2025-11-04T17:44:13.746943+08:00 0 [Note] [MY-012204] [InnoDB] Scanning './'
2025-11-04T17:44:13.753807+08:00 0 [Note] [MY-012208] [InnoDB] Completed space ID check of 10 files.
2025-11-04T17:44:13.756116+08:00 0 [Note] [MY-012955] [InnoDB] Initializing buffer pool, total size = 128.000000M, instances = 1, chunk size =128.000000M 
2025-11-04T17:44:13.765015+08:00 0 [Note] [MY-012957] [InnoDB] Completed initialization of buffer pool
2025-11-04T17:44:13.770906+08:00 0 [Note] [MY-011951] [InnoDB] page_cleaner coordinator priority: -20
2025-11-04T17:44:13.771119+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.771183+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.771564+08:00 0 [Note] [MY-011954] [InnoDB] page_cleaner worker priority: -20
2025-11-04T17:44:13.824205+08:00 0 [Note] [MY-013883] [InnoDB] The latest found checkpoint is at lsn = 395899109 in redo log file ./
#innodb
_redo/
#ib
_redo0.
2025-11-04T17:44:13.824287+08:00 0 [Note] [MY-012560] [InnoDB] The log sequence number 395730540 in the system tablespace does not match the log sequence number 395899109 in the redo log files!
2025-11-04T17:44:13.824305+08:00 0 [Note] [MY-012551] [InnoDB] Database was not shutdown normally!
2025-11-04T17:44:13.824312+08:00 0 [Note] [MY-012552] [InnoDB] Starting crash recovery.
2025-11-04T17:44:13.824504+08:00 0 [Note] [MY-013086] [InnoDB] Starting to parse redo log at lsn = 395898899, whereas checkpoint_lsn = 395899109 and start_lsn = 395898880
2025-11-04T17:44:13.824519+08:00 0 [Note] [MY-012550] [InnoDB] Doing recovery: scanned up to log sequence number 395899109
2025-11-04T17:44:13.843504+08:00 0 [Note] [MY-013083] [InnoDB] Log background threads are being started...
2025-11-04T17:44:13.851994+08:00 0 [Note] [MY-012532] [InnoDB] Applying a batch of 0 redo log records ...
2025-11-04T17:44:13.852026+08:00 0 [Note] [MY-012535] [InnoDB] Apply batch completed!
2025-11-04T17:44:13.952255+08:00 0 [Note] [MY-013084] [InnoDB] Log background threads are being closed...
2025-11-04T17:44:13.957800+08:00 0 [Note] [MY-013888] [InnoDB] Upgrading redo log: 1032M, LSN=395899109.
2025-11-04T17:44:13.957898+08:00 0 [Note] [MY-012968] [InnoDB] Starting to delete and rewrite redo log files.
2025-11-04T17:44:13.957956+08:00 0 [Note] [MY-011825] [InnoDB] Removing redo log file: ./
#innodb
_redo/
#ib
_redo0
2025-11-04T17:44:14.004652+08:00 0 [Note] [MY-011825] [InnoDB] Creating redo log file at ./
#innodb
_redo/
#ib
_redo0_tmp with file_id 0 with size 33554432 bytes
2025-11-04T17:44:14.008311+08:00 0 [Note] [MY-011825] [InnoDB] Renaming redo log file from ./
#innodb
_redo/
#ib
_redo0_tmp to ./
#innodb
_redo/
#ib
_redo0
2025-11-04T17:44:14.011183+08:00 0 [Note] [MY-012893] [InnoDB] New redo log files created, LSN=395899404
2025-11-04T17:44:14.011284+08:00 0 [Note] [MY-013083] [InnoDB] Log background threads are being started...
2025-11-04T17:44:14.023796+08:00 0 [Note] [MY-013252] [InnoDB] Using undo tablespace './undo_001'.
2025-11-04T17:44:14.025816+08:00 0 [Note] [MY-013252] [InnoDB] Using undo tablespace './undo_002'.
2025-11-04T17:44:14.027533+08:00 0 [Note] [MY-012910] [InnoDB] Opened 2 existing undo tablespaces.
2025-11-04T17:44:14.027698+08:00 0 [Note] [MY-011980] [InnoDB] GTID recovery trx_no: 1044267
2025-11-04T17:44:14.226936+08:00 0 [Note] [MY-013776] [InnoDB] Parallel initialization of rseg complete
2025-11-04T17:44:14.226983+08:00 0 [Note] [MY-013777] [InnoDB] Time taken to initialize rseg using 2 thread: 199297 ms.
2025-11-04T17:44:14.229195+08:00 0 [Note] [MY-012923] [InnoDB] Creating shared tablespace for temporary tables
2025-11-04T17:44:14.229302+08:00 0 [Note] [MY-012265] [InnoDB] Setting file './ibtmp1' size to 12 MB. Physically writing the file full; Please wait ...
2025-11-04T17:44:14.260630+08:00 0 [Note] [MY-012266] [InnoDB] File './ibtmp1' size is now 12 MB.
2025-11-04T17:44:14.262027+08:00 0 [Note] [MY-013627] [InnoDB] Scanning temp tablespace dir:'./
#innodb
_temp/'
2025-11-04T17:44:14.287708+08:00 0 [Note] [MY-013018] [InnoDB] Created 128 and tracked 128 new rollback segment(s) in the temporary tablespace. 128 are now active.
2025-11-04T17:44:14.291471+08:00 0 [Note] [MY-012976] [InnoDB] 8.0.35 started; log sequence number 395899414
2025-11-04T17:44:14.292531+08:00 0 [Warning] [MY-012091] [InnoDB] Allocated tablespace ID 1 for sys/sys_config, old maximum was 0
2025-11-04T17:44:14.301258+08:00 0 [Note] [MY-011825] [Xtrabackup] Completed loading of 8 tablespaces into cache in 0.00972792 seconds
2025-11-04T17:44:14.332275+08:00 0 [Note] [MY-011825] [Xtrabackup] Time taken to build dictionary: 0.0309666 seconds
2025-11-04T17:44:15.336933+08:00 0 [Note] [MY-011825] [Xtrabackup] starting shutdown with innodb_fast_shutdown = 1
2025-11-04T17:44:15.337061+08:00 0 [Note] [MY-012330] [InnoDB] FTS optimize thread exiting.
2025-11-04T17:44:16.337034+08:00 0 [Note] [MY-013072] [InnoDB] Starting shutdown...
2025-11-04T17:44:16.440671+08:00 0 [Note] [MY-013084] [InnoDB] Log background threads are being closed...
2025-11-04T17:44:16.461318+08:00 0 [Note] [MY-012980] [InnoDB] Shutdown completed; log sequence number 395899414
2025-11-04T17:44:16.476016+08:00 0 [Note] [MY-011825] [Xtrabackup] completed OK!</code></pre><p>当执行prapare过程中 最后出现 completed OK! 关键字时，表明操作成功。</p><p>3、停止MySQL服务并清空数据目录</p><p>为确保恢复过程顺利，首先需要停止MySQL服务，并清空其数据目录（datadir）。这是恢复操作的关键前提</p><pre><code class="sql">-- 停止MySQL服务
systemctl stop mysqld
-- 或者登录MySQL数据库 执行 shutdown; 命令</code></pre><p>重要提示：在执行rm -rf命令前，务必确认目录路径正确，最好对原有数据做备份。然后清空数据目录：</p><pre><code class="sql"># 清空MySQL数据目录（请先确认你的datadir路径，假设是/var/lib/mysql）
rm -rf /var/lib/mysql/* </code></pre><p>4、执行数据恢复</p><p>使用 --copy-back或 --move-back命令将预备好的备份数据恢复到MySQL的数据目录</p><pre><code class="sql">xtrabackup --copy-back --target-dir=/path/to/prepared_backup
说明：
 --copy-back：将备份文件复制到数据目录。这是最安全常用的方式，保留原始备份
 --move-back：将备份文件移动到数据目录。更节省空间，但原始备份会消失</code></pre><p>5、修改文件权限</p><p>恢复的数据文件可能不属于mysql用户，需要更改属主和权限以确保MySQL有权限读写</p><pre><code class="sql">chown -R mysql:mysql /var/lib/mysql
此命令将数据目录及其下所有文件的所有者和组设置为mysql</code></pre><p>6、启动MySQL并确认</p><p>权限设置好以后，就可以启动MySQL服务了</p><pre><code class="sql">systemctl start mysqld
-- 或者使用mysqld_safe --defaults-file=/etc/my.cnf --user=mysql &amp; 方式启动</code></pre><p>启动后，务必检查MySQL的错误日志，并使用客户端连接，验证数据库和表是否正常。</p><h2><strong>总 结</strong></h2><p>该脚本提供了一个生产环境进行MySQL8.0物理备份所需的完整步骤，包括错误处理、日志记录、自动清理和耗时统计。数据库运维人员可以根据实际环境调整配置参数，特别是备份路径和保留天数设置以及是否采用压缩等一些常用功能的设置。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046208374" alt="" title=""/>  </p><p>墨天轮从乐知乐享的数据库技术社区蓄势出发，全面升级，提供多类型数据库管理服务。墨天轮数据库管理服务旨在为用户构建信赖可托付的数据库环境，并为数据库厂商提供中立的生态支持。<br/>墨天轮数据库服务官网：<a href="https://link.segmentfault.com/?enc=PJeLT%2FbswEoUX4nlVxKfGg%3D%3D.89CkDWIllNEM6OPA%2BrK7TBlC0crEh0Sbrs2EcWrulOO5JmsGqfRBjLpBvjF9I5qk" rel="nofollow" target="_blank">https://www.modb.pro/service</a></p>]]></description></item><item>    <title><![CDATA[中小企业 CRM 推荐：2025 年高性价比品牌排行榜 TOP6 晨曦钥匙扣 ]]></title>    <link>https://segmentfault.com/a/1190000047574769</link>    <guid>https://segmentfault.com/a/1190000047574769</guid>    <pubDate>2026-01-27 12:11:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型进程中，CRM（客户关系管理）已从“辅助销售的工具”升级为“连接客户、销售、服务的业务中枢”。其核心价值体现在三个维度：<strong>客户/商机/跟进一体化</strong>（打破信息孤岛）、<strong>自动提醒与任务分派</strong>（释放团队效率）、<strong>移动端无缝体验</strong>（适配外勤场景）。</p><p>本文选取<strong>超兔一体云、Freshsales（Freshworks）、金现代、Zoho、管家婆、Pipedrive</strong>六大主流CRM品牌，从三个核心维度展开深度横评，结合<strong>表格对比、流程图、脑图、雷达图</strong>，为企业选型提供专业参考。</p><h2>一、核心维度与评价体系说明</h2><h3>1.1 维度拆解与评价标准</h3><p>我们将CRM的核心能力拆解为<strong>3大维度+12个子指标</strong>，并采用<strong>1-5分制</strong>（5分为满分）量化评估各品牌表现：</p><table><thead><tr><th>一级维度</th><th>二级子指标</th><th>评价标准</th></tr></thead><tbody><tr><td>客户/商机/跟进一体化</td><td>全流程覆盖（线索→订单）、客户360°视图、跨模块协同（销售+服务/营销）、销售漏斗可视化</td><td>数据打通+流程协同+视图统一</td></tr><tr><td>自动提醒与任务分派</td><td>规则灵活性、智能触发精度、任务分配合理性、多渠道通知、移动端支持</td><td>规则精准+触发及时+分配合理</td></tr><tr><td>移动端无缝体验</td><td>多端同步效率、离线功能、操作便捷性（语音/拍照/定位）、生态适配、全功能覆盖</td><td>多端同步+离线可用+操作便捷+生态兼容</td></tr></tbody></table><h3>1.2 雷达图指标与分值（1-5分）</h3><p>为直观展示各品牌综合能力，我们选取<strong>5个核心指标</strong>绘制雷达图（分值越高，能力越强）：</p><table><thead><tr><th>品牌</th><th>客户一体化</th><th>自动提醒</th><th>任务分派</th><th>移动端体验</th><th>生态集成</th></tr></thead><tbody><tr><td>超兔一体云</td><td>4.8</td><td>4.7</td><td>4.6</td><td>4.9</td><td>4.5</td></tr><tr><td>Freshsales</td><td>4.5</td><td>4.6</td><td>4.7</td><td>4.6</td><td>4.8</td></tr><tr><td>金现代</td><td>4.3</td><td>4.2</td><td>4.1</td><td>4.4</td><td>4.0</td></tr><tr><td>Zoho</td><td>4.4</td><td>4.3</td><td>4.2</td><td>4.5</td><td>4.7</td></tr><tr><td>管家婆</td><td>4.2</td><td>4.4</td><td>4.3</td><td>4.8</td><td>4.1</td></tr><tr><td>Pipedrive</td><td>4.6</td><td>4.5</td><td>4.7</td><td>4.7</td><td>4.6</td></tr></tbody></table><h2>二、客户、商机、跟进一体化：从“模块叠加”到“业务中枢”</h2><h3>2.1 维度本质：不是“有模块”，而是“能协同”</h3><p>真正的一体化不是<strong>模块的简单堆砌</strong>，而是<strong>数据打通、流程协同、视图统一</strong>的闭环：</p><ul><li><strong>数据打通</strong>：客户信息、商机进展、跟进记录在CRM、进销存、服务等模块自由流动；</li><li><strong>流程协同</strong>：商机阶段变化自动触发跟进任务（如“意向确认”→“起草合同”）；</li><li><strong>视图统一</strong>：一个界面看全客户全景（基本信息、历史跟进、商机进展、服务记录）。</li></ul><h3>2.2 各品牌表现对比</h3><h4>（1）超兔一体云：底层大底座支撑全链路协同</h4><p>超兔的核心优势是<strong>构建了覆盖CRM、进销存、供应链、收支账的“业务大底座”</strong> ，实现数据底层连通：</p><ul><li>市场部通过集客获取的线索，自动同步至客户中心并生成商机；</li><li>销售跟进的每一条记录（拜访、沟通），实时同步至客户视图与商机视图；</li><li>商机进入“意向确认”阶段时，流程引擎自动提醒“起草合同”，任务完成后同步更新客户状态（如“高意向”）。</li></ul><p><strong>流程图：超兔一体化逻辑</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574771" alt="" title=""/></p><pre><code>graph TD
    A[市场部集客获取线索] --&gt; B[线索同步至客户中心（底层连通）]
    B --&gt; C[自动生成对应商机记录（数据关联）]
    C --&gt; D[销售跟进：拜访/沟通记录录入]
    D --&gt; E[跟进记录实时同步至客户视图+商机视图（统一视图）]
    E --&gt; F[商机阶段变化触发流程任务（如“意向确认”→“起草合同”）]
    F --&gt; G[任务完成，更新客户+商机状态]</code></pre><h4>（2）Freshsales：销售+服务+营销云一体化</h4><p>Freshsales（Freshworks旗下）的核心是“销售云+营销云+服务云”的全栈协同：</p><ul><li>客户生命周期档案完整覆盖“线索→联系人→销售→服务”全流程；</li><li>服务模块的工单（如客户投诉）会同步至销售跟进记录，避免“销售不管售后”的信息差；</li><li>AI助手Freddy可自动识别客户需求（如“价格咨询”），触发销售跟进任务。</li></ul><h4>（3）管家婆：本土化场景的“跟单闭环”</h4><p>管家婆的优势是<strong>深度适配国内中小微企业的“本土化跟单需求”</strong> ：</p><ul><li>支持“新增客户→联系→回访”的全流程记录，客户档案无限存储；</li><li>客户新增/修改时自动匹配相近客户（避免重复建档）；</li><li>销售跟进记录可直接转为日程（一次填写，两处复用），减少人工操作。</li></ul><h3>2.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>全流程覆盖（线索→订单）</th><th>客户360°视图</th><th>跨模块协同（销售+服务）</th><th>销售漏斗可视化</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（底层大底座连通）</td><td>✅（全景信息）</td><td>✅（CRM+进销存+供应链）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅（销售+服务+营销）</td><td>✅（生命周期）</td><td>✅（工单同步销售）</td><td>✅（AI优化）</td></tr><tr><td>金现代</td><td>✅（线索→回款）</td><td>✅（画像+标签）</td><td>✅（营销数字化平台）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅（自定义模块）</td><td>✅（多渠道）</td><td>✅（销售+营销）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅（本土化跟单）</td><td>✅（无限档案）</td><td>❌（侧重销售，服务弱）</td><td>✅</td></tr><tr><td>Pipedrive</td><td>✅（漏斗为核心）</td><td>✅（跟进关联）</td><td>❌（侧重销售，营销弱）</td><td>✅（可视化强）</td></tr></tbody></table><h2>三、自动提醒与任务分派：从“人工记忆”到“智能驱动”</h2><h3>3.1 维度本质：不是“能提醒”，而是“精准提醒”</h3><p>自动提醒与任务分派的核心是“规则精准、触发及时、分配合理”：</p><ul><li>规则精准：支持时间、事件、状态等多条件组合（如“商机距离签约7天”+“客户未跟进”）；</li><li>触发及时：基于历史数据或AI预测潜在风险（如“客户7天未联系”）；</li><li>分配合理：根据员工负荷（如“销售A当前有5个高价值商机”）与技能（如“擅长跟进大企业”）分配任务。</li></ul><h3>3.2 各品牌表现对比</h3><h4>（1）Freshsales：AI驱动的“智能任务体系”</h4><p>Freshsales的优势是<strong>AI线索打分与自动化任务分派</strong>：</p><ul><li>AI助手Freddy对线索打分（如“高价值客户打9分”），自动分配给Top销售；</li><li>商机关键节点（如“签约前7天”）自动发送提醒邮件，新线索触发“欢迎信”；</li><li>重复任务（如数据录入）自动化，节省销售80%的琐碎时间。</li></ul><p><strong>脑图：Freshsales智能任务体系</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574772" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((Freshsales智能任务体系))
        自动提醒
            商机关键节点（签约前7天）
            潜在客户阶段触发（高意向）
            自动邮件（新线索欢迎信）
        任务分派
            AI线索打分→Top销售
            重复任务自动化（数据录入）
            移动端任务推送
        核心价值
            避免跟进遗漏
            提升团队效率
            缩短销售周期</code></pre><h4>（2）超兔一体云：算法驱动的“精准分配”</h4><p>超兔的特色是<strong>智能算法对任务的“合理分配”</strong> ：</p><ul><li>支持“时间+事件+状态”多条件规则（如“客户复购时间预测”+“未跟进”）；</li><li>基于历史数据预测客户复购时间，提前3天提醒销售跟进；</li><li>根据员工“当前负荷”（如已分配任务量）与“技能标签”（如“擅长电商客户”）分配任务，避免“忙的忙死，闲的闲死”。</li></ul><h4>（3）管家婆：本土化的“场景化提醒”</h4><p>管家婆的优势是<strong>适配国内企业的“日常场景提醒”</strong> ：</p><ul><li>支持“周目标事项”“待办事宜”的日程提醒；</li><li>订单新增时触发“语音提示”，避免漏看；</li><li>销售跟进记录可直接转为日程，一次填写，同时同步至“客户档案”与“个人日程”。</li></ul><h3>3.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>规则灵活性（多条件）</th><th>智能触发（AI/历史数据）</th><th>任务分配（负荷+技能）</th><th>多渠道通知</th><th>移动端支持</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（时间/事件/状态）</td><td>✅（智能算法预测）</td><td>✅（负荷+技能）</td><td>✅（短信/邮件/系统）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅（阶段/节点）</td><td>✅（AI线索打分）</td><td>✅（Top销售）</td><td>✅（邮件）</td><td>✅</td></tr><tr><td>金现代</td><td>✅（预设规则）</td><td>❌（无AI）</td><td>✅（责任明晰）</td><td>❌（基础）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅（地域/行业）</td><td>✅（Zia助手）</td><td>✅（规则分配）</td><td>✅（消息）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅（日程/订单）</td><td>❌（无AI）</td><td>✅（工作指派）</td><td>✅（语音）</td><td>✅</td></tr><tr><td>Pipedrive</td><td>✅（阶段触发）</td><td>✅（优先级标注）</td><td>✅（流程分配）</td><td>✅（日历）</td><td>✅</td></tr></tbody></table><h2>四、移动端无缝体验：从“能访问”到“好用”</h2><h3>4.1 维度本质：不是“有APP”，而是“适配场景”</h3><p>移动端的核心是“多端同步、离线可用、操作便捷、生态适配”：</p><ul><li>多端同步：移动端操作实时同步至PC端，无延迟；</li><li>操作便捷：支持语音输入、拍照上传、定位打卡等“外勤友好”功能；</li><li>生态适配：与微信、QQ、日历等常用工具集成，减少切换成本。</li></ul><h3>4.2 各品牌表现对比</h3><h4>（1）管家婆：本土化全功能移动端</h4><p>管家婆的移动端是<strong>国内中小微企业的“外勤神器”</strong> ，覆盖<strong>开单、审批、OA、客户跟进</strong>全场景：</p><ul><li>支持“手机开单”，订单可直接发送至客户微信/QQ/短信；</li><li>实时查看“销售业绩、库存状态”，避免“库存不足却接单”的尴尬；</li><li>集成OA协同（待办事宜、同事圈沟通），无需额外安装办公软件。</li></ul><h4>（2）超兔一体云：轻量化与场景化能力兼顾</h4><p>超兔的移动端采用“轻量化设计+场景化能力”，适配“外勤场景”：</p><ul><li>支持语音输入（快速记录沟通内容）、拍照上传（客户资料）、定位打卡（拜访轨迹）；</li><li>多端适配（Web/APP/小程序/客户端），满足不同团队的设备需求。</li></ul><h4>（3）Pipedrive：获G2认可的“易用性”</h4><p>Pipedrive的移动端以“易用性”著称，获2025年G2“销售人员最易用奖”：</p><ul><li>支持离线访问客户数据，语音录入客户信息（避免手动打字）；</li><li>活动提醒同步至Google日历，避免“错过重要拜访”；</li><li>界面简洁，销售人员可快速找到“跟进客户、查看商机、记录沟通”核心功能。</li></ul><h3>4.3 核心能力对比表</h3><table><thead><tr><th>品牌</th><th>多端同步（实时）</th><th>离线功能</th><th>操作便捷（语音/拍照）</th><th>生态集成（微信/日历）</th><th>全功能覆盖</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（多端实时）</td><td>✅</td><td>✅（语音/拍照/定位）</td><td>✅（小程序/微信）</td><td>✅</td></tr><tr><td>Freshsales</td><td>✅</td><td>✅</td><td>✅（定位/行程）</td><td>✅（Google Maps）</td><td>✅</td></tr><tr><td>金现代</td><td>✅</td><td>✅</td><td>✅（现场录入/拍照）</td><td>❌（基础）</td><td>✅</td></tr><tr><td>Zoho</td><td>✅</td><td>✅</td><td>✅（地图/打卡）</td><td>✅（Google日历）</td><td>✅</td></tr><tr><td>管家婆</td><td>✅</td><td>❌</td><td>✅（开单/微信发送）</td><td>✅（微信/QQ/短信）</td><td>✅（全功能）</td></tr><tr><td>Pipedrive</td><td>✅</td><td>✅</td><td>✅（语音录入）</td><td>✅（Google日历）</td><td>✅</td></tr></tbody></table><h2>五、选型建议：匹配场景比“功能全”更重要</h2><p>通过以上对比，各品牌的<strong>核心优势与适用场景</strong>已清晰：</p><table><thead><tr><th>品牌</th><th>核心优势</th><th>适用场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>全业务链路协同（CRM+进销存+供应链）、离线能力</td><td>需要“进销存+CRM协同”的中小微企业，如零售、贸易行业，外勤场景多</td></tr><tr><td>Freshsales</td><td>AI智能（线索打分、自动提醒）、跨国协同</td><td>注重AI辅助、需要跨国团队协作的B2B企业，如 SaaS、制造行业</td></tr><tr><td>金现代</td><td>营销数字化平台、PaaS定制化</td><td>需要“营销+销售协同”的中大型企业，如消费品、医药行业</td></tr><tr><td>Zoho</td><td>高性价比、多渠道集成</td><td>预算有限、需要多渠道（官网/社交媒体）线索管理的中小企业</td></tr><tr><td>管家婆</td><td>本土化全功能（微信/QQ集成）、移动端易用</td><td>本土化需求强（如微信开单、短信通知）的中小微企业，如零售、餐饮行业</td></tr><tr><td>Pipedrive</td><td>销售漏斗可视化、移动端易用性</td><td>以销售漏斗为核心、注重移动端效率的销售团队，如房产、保险行业</td></tr></tbody></table><h2>结语</h2><p>CRM的本质是“以客户为中心”，其能力的核心不是“功能越多越好”，而是“能否匹配企业的业务场景”。企业选型时，需优先考虑“数据是否能打通”“任务是否能精准分配”“移动端是否好用”——这三个问题解决了，CRM才能真正成为“业务中枢”，而非“摆设”。</p><p>未来，CRM的竞争将更聚焦“AI+场景化”：AI将更精准地预测客户需求，场景化功能（如零售的“微信开单”、制造的“进销存协同”）将更贴合行业痛点。企业需结合自身发展阶段，选择“能陪伴成长”的CRM伙伴。</p>]]></description></item><item>    <title><![CDATA[🚀 爆火的 Clawdbot 到底是什么？—— 你的第一个“真·本地”AI 智能管家 Pangoli]]></title>    <link>https://segmentfault.com/a/1190000047574779</link>    <guid>https://segmentfault.com/a/1190000047574779</guid>    <pubDate>2026-01-27 12:10:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Clawdbot：爆火的开源AI智能体网关，堪称AI助理完全体</h2><p>最近，你的技术圈子是不是被一只“龙虾”（Clawdbot 的 Logo）刷屏了？甚至听说它让二手的 Mac Mini 价格都应声上涨？</p><p>作为一个刚入坑稀土掘金的新人，今天我就带大家扒一扒这个让无数极客彻夜未眠的开源项目——Clawdbot。它到底是什么？为什么它被称为“AI 助理的完全体”？以及，它真的能成为你的 Jarvis 吗？</p><h3>🧐 什么是 Clawdbot？</h3><p>简单来说，Clawdbot 是一个开源的 AI 智能体网关（Agent Gateway）。</p><p>如果不讲术语，你可以这样理解：<br/><strong>Clawdbot = 大模型的大脑 (Claude/GPT) + 即时通讯软件的嘴巴 (Telegram/WhatsApp) + 本地电脑的手脚 (Terminal/文件系统) + 永久记忆</strong>。</p><p>与我们在网页上用的 ChatGPT 或 Claude 不同，Clawdbot 不是运行在浏览器里的，而是运行在你自己的服务器或电脑（如 Mac Mini、树莓派）上的一个后台程序。它就像一个住在你电脑里的“数字管家”，你通过聊天软件给它发指令，它在你的电脑上直接干活。</p><h3>🌟 核心特点：为什么它如此特别？</h3><p>Clawdbot 之所以能爆火，是因为它解决了当前 AI 应用的几个核心痛点：</p><h4>1. 它是“活”在本地的 (Local First)</h4><p>目前大多数 AI 都在云端，不仅有隐私顾虑，而且无法操作你的本地文件。Clawdbot 运行在你的本地设备上：</p><ul><li>数据隐私：除了与 LLM 对话的内容，你的记忆文件、配置、本地数据都存在自己硬盘里。</li><li>本地权限：它可以直接读取你的文档、运行 Python 脚本、甚至执行终端命令（Terminal）。</li></ul><h4>2. 对话即交互 (ChatOps)</h4><p>你不需要下载专门的 App。Clawdbot 接入了 WhatsApp, Telegram, Discord, Slack, iMessage 等几乎所有主流通讯软件。</p><ul><li>场景：你在外面用手机给家里的 Clawdbot 发微信：“帮我查一下服务器日志，把报错的部分发给我。”</li><li>结果：它直接通过 SSH 连上服务器，跑完命令，把结果截图或文本回传给你。</li></ul><h4>3. 真正的“长短期记忆”</h4><p>Clawdbot 使用本地的 Markdown 文件（通常是 MEMORY.md）来存储关于你的信息。<br/>它记得你的偏好、你家人的生日、你的服务器密码（需谨慎）、你正在做的项目进度。<br/>这种记忆是持久的，不会因为关闭窗口就消失。</p><h4>4. 强大的工具调用能力 (Agentic Capabilities)</h4><p>这是它最“炸裂”的地方。它不仅能陪聊，还能干活。通过 MCP (Model Context Protocol) 或内置工具，它可以：</p><ul><li>浏览网页：帮你查资料并总结。</li><li>写代码并运行：它可以写一个 Python 脚本来处理 Excel 表格，然后直接在你电脑上运行这个脚本，最后把处理好的 Excel 发给你。</li><li>管理日程：读取你的日历，帮你安排会议。</li></ul><h3>🛠 Clawdbot 能帮我们干什么？</h3><p>这就是想象力发挥的地方了。目前社区里已经有了很多硬核玩法：</p><h4>1. 24/7 个人秘书</h4><ul><li>自动处理邮件：让它监控你的 Gmail，自动归档垃圾邮件，把重要邮件摘要发到 Telegram 给你。</li><li>每日简报：每天早上 8 点，它会根据你的日历、关注的新闻源、天气情况，给你发一份定制的“早安简报”。</li></ul><h4>2. 也是最强的“结对编程”伙伴</h4><ul><li>代码助手：你可以让它读取你整个项目的代码库（因为它在本地，读取速度极快），然后问它：“utils.py 里的那个函数怎么优化？”</li><li>运维监控：当它检测到某个进程挂了，可以自动发消息报警，甚至在你授权下尝试重启服务。</li></ul><h4>3. 自动化繁琐任务</h4><ul><li>文件整理：对它说“把 Downloads 文件夹里所有的 PDF 发票整理一下，按月份归档到 Documents/Invoices 目录里”。它会自己写 Shell 脚本瞬间完成。</li><li>比价购物：让它去几个电商网站爬取价格，整理成表格给你。</li></ul><h3>⚠️ 风险提示（必读！）</h3><p>虽然 Clawdbot 很酷，但它目前更像是一个极客的玩具，而不是普通用户的消费级产品。</p><ol><li><strong>安全风险（高危）</strong>：你实际上是给了 AI 访问你电脑文件系统和终端（Terminal）的权限。虽然有权限控制，但如果 AI "幻觉"了，或者被提示注入攻击，理论上它能执行 rm -rf /（删库）。建议尽量在沙箱环境或独立的 Mac Mini/虚拟机中运行。</li><li><strong>成本问题</strong>：虽然软件免费，但它背后调用的是 API（如 Claude 3.5 Sonnet 或 GPT-4o）。如果你让它处理大量任务，API 账单可能会让你肉疼。</li><li><strong>配置门槛</strong>：需要懂一点 Docker、Node.js 或者命令行的知识才能部署起来。</li></ol><h3>🔚 总结</h3><p>Clawdbot 代表了 AI 的下一个阶段：从“聊天机器人”进化为“智能代理（Agent）”。它不再是被动等待提问的百科全书，而是有了手脚、能主动帮你解决问题的数字员工。</p><p>如果你有一台闲置的电脑，并且喜欢折腾技术，Clawdbot 绝对值得一试。但请记得：<strong>能力越大，风险越大，请管好你的 API Key 和系统权限！</strong></p><p>欢迎在评论区分享你的 Clawdbot 玩法！</p>]]></description></item><item>    <title><![CDATA[2026 工业 CRM 盘点：5 大品牌客制化能力横评 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047574782</link>    <guid>https://segmentfault.com/a/1190000047574782</guid>    <pubDate>2026-01-27 12:10:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业4.0与全球供应链重构的背景下，工业/工贸企业的数字化转型核心需求已从“标准化上线”转向“<strong>柔性适配</strong>”——既要应对复杂业务场景（如非标订单、跨部门协同、跨境贸易），又要快速响应市场变化（如需求波动、流程调整）。</p><p>本文选取<strong>超兔一体云、</strong> <strong>SAP</strong> <strong>、Microsoft Dynamics 365、八百客</strong> <strong>CRM</strong> <strong>、OKKI CRM（原小满）五大主流平台，从</strong> <strong>客制化</strong> <strong>能力、多端协同能力、工业场景适配性、实施成本</strong>四大核心维度展开深度横评，为工业/工贸企业选择适配的数字化体系提供决策参考。</p><h2>一、核心对比框架与维度定义</h2><h3>1. 对比维度说明</h3><table><thead><tr><th><strong>维度</strong></th><th><strong>子维度</strong></th><th><strong>工业/工贸企业核心诉求关联</strong></th></tr></thead><tbody><tr><td><strong>客制化</strong> <strong>能力</strong></td><td>低代码/无代码定制、行业模板覆盖、生态系统集成、复杂流程（如非标订单）定制</td><td>适配企业独特业务逻辑（如半导体晶圆制造合规、外贸跨境合同），避免“削足适履”</td></tr><tr><td><strong>多端协同能力</strong></td><td>多终端覆盖、内部业务链路协同（销售→生产→物流→财务）、外部生态协同（供应商/客户）、业财一体化</td><td>消除信息孤岛，提升跨部门/跨企业协作效率，实现“订单驱动生产”的柔性协同</td></tr><tr><td><strong>工业场景适配性</strong></td><td>核心场景覆盖（订单-生产-交付、设备维护、跨境贸易）、行业聚焦（如汽配/半导体/外贸）、柔性应变能力</td><td>精准解决工业企业痛点（如IATF 16949质量合规、跨境回款风险），支持业务模式调整</td></tr><tr><td><strong>实施与成本</strong></td><td>部署方式（云/本地/混合）、实施周期、总成本（license+实施+维护）、维护难度</td><td>平衡“定制深度”与“成本效率”，避免“大而全”的高投入陷阱</td></tr></tbody></table><h3>2. 品牌选择说明</h3><p>选取标准：<strong>聚焦工业/工贸场景</strong>+ <strong>“</strong> <strong>客制化</strong> <strong>+多端协同”能力明确</strong></p><ul><li>超兔一体云：以“客制化+多端协同”为核心定位，适配中小工业企业灵活需求；</li><li>SAP：全球ERP龙头，覆盖从中小企业（Business One）到大型企业（S/4HANA）的全场景；</li><li>Microsoft Dynamics 365：依托微软生态，擅长“办公+业务”协同；</li><li>八百客CRM：PaaS平台支撑深度定制，适配中大型工业企业复杂流程；</li><li>OKKI CRM：聚焦外贸工贸场景，解决跨境协同痛点。</li></ul><h2>二、四大核心维度深度横评</h2><h3>（一）客制化能力：从“标准化”到“精准适配”的关键</h3><p>客制化是工业/工贸企业数字化的“灵魂”——<strong>只有适配企业独特业务逻辑，才能避免系统成为“摆设”</strong> 。五大平台的客制化能力差异显著：</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>低代码</strong> <strong>/无代码定制</strong></td><td>支持<strong>可视化自定义</strong>（三级菜单、工作台、业务表、工作流），无需代码调整流程</td><td>中小企业（Business One）支持“功能白名单+三级菜单”快速定制；大型企业需依赖实施商开发</td><td>通过<strong>Power Apps</strong>低代码平台，业务人员20分钟搭建自定义模块（如非标设备报价单）</td><td>基于PaaS平台，<strong>可视化配置表单/流程/权限</strong>，无需代码适配复杂项目管理</td><td>聚焦外贸场景，<strong>AI驱动客户分级/订单流程</strong>自定义，支持跨境合同模板配置</td></tr><tr><td><strong>行业模板覆盖</strong></td><td>提供工业/工贸通用模板（订单-生产-仓储协同），支持小步迭代调整</td><td>内置半导体/汽配/制造业等<strong>垂直行业模板</strong>（如晶圆制造合规体系、IATF 16949流程）</td><td>覆盖12大行业（制造业/零售业等），提供“订单-生产-交付”协同模块</td><td>聚焦光伏/制造等中大型工业企业，提供<strong>生产-销售协同模板</strong></td><td>专属<strong>外贸工贸模板</strong>（跨境回款规则、国际物流跟踪）</td></tr><tr><td><strong>生态系统集成</strong></td><td>支持RPA插件、对接第三方ERP（如金蝶/用友）</td><td>可集成WMS/MES/APS/QMS等工业系统，实现“计划-执行-反馈”数据贯通</td><td>通过<strong>Power Platform 300+连接器</strong>，对接SAP ERP、IoT设备、第三方CRM</td><td>对接ERP系统（如SAP/金蝶），实现<strong>业财一体化</strong></td><td>对接金蝶/用友ERP、跨境支付（PayPal）、物流（DHL）系统</td></tr><tr><td><strong>复杂流程定制</strong></td><td>支持<strong>自定义工作流+多表聚合BI</strong>，适配非标订单/多部门审批等复杂场景</td><td>大型企业（S/4HANA）支持<strong>客户化开发</strong>（如半导体成本精准归集）；中小企业（Business One）支持固化核心流程</td><td>支持<strong>设备安装记录/预防性维护计划</strong>等工业专属流程，通过Power Automate实现自动化</td><td>适配<strong>生产订单关联/多部门协作流程</strong>，支持复杂权限配置</td><td>适配<strong>跨境订单全流程</strong>（报价-合同-物流-回款），支持多语言合同模板</td></tr></tbody></table><p><strong>小结</strong>：</p><ul><li>小步快跑型企业选<strong>超兔</strong>：可视化自定义降低技术门槛，支持“按需添加功能”；</li><li>行业合规型企业选<strong>SAP</strong>：垂直行业模板覆盖半导体/汽配等强合规场景；</li><li>低代码快迭代型企业选<strong>Dynamics 365</strong>：Power Apps让业务人员主导定制；</li><li>复杂流程型企业选<strong>八百客</strong>：PaaS平台支撑深度流程配置；</li><li>外贸型企业选<strong>OKKI</strong>：专属跨境场景定制。</li></ul><h3>（二）多端协同能力：从“信息孤岛”到“全链路贯通”的核心</h3><p>多端协同的本质是<strong>数据与流程的“全场景流动”</strong> ——让销售在手机上录的订单，实时同步到生产排产系统；让供应商在Web端看到的库存，直接关联到客户的交付计划。</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>多终端覆盖</strong></td><td>Web/APP/小程序/客户端/RPA插件，适配外勤销售/仓库扫码/财务分析等场景</td><td>Web/APP/移动端，支持<strong>SAP Fiori</strong>移动应用（如生产工单审批）</td><td>Web/APP/小程序/Teams/Outlook，覆盖办公+业务全场景</td><td>PC/移动端（iOS/Android），支持实时数据同步</td><td>移动端（iOS/Android）+Web，支持<strong>跨境多语言沟通</strong>（62个国家）</td></tr><tr><td><strong>内部业务协同</strong></td><td>销售订单→生产计划→采购→仓储→财务全链路数据同步，支持跨部门流程协同</td><td>集成ERP/WMS/MES/APS，实现“销售订单→生产排产→物流→财务”全链路贯通</td><td>与Office 365深度融合：Outlook调客户画像、Teams生成跟进任务、Excel转BI报表</td><td>PC/移动端共享客户数据、分配销售任务，管理层通过<strong>数据看板</strong>监控全链路</td><td>移动端<strong>邮件聚合/客户动态实时更新</strong>，团队协同跟进海外订单</td></tr><tr><td><strong>外部生态协同</strong></td><td>支持供应商/客户小程序端接入，实现订单状态实时共享</td><td>通过<strong>SAP Business Network</strong>连接供应商/客户/物流商，提升库存可视性</td><td>通过<strong>Dynamics 365 Supply Chain Management</strong>对接供应商，实现计划与库存自动化协同</td><td>暂未明确支持外部生态协同</td><td>通过<strong>跨境供应链平台</strong>连接供应商/物流商，实现国际物流跟踪</td></tr><tr><td><strong>业财一体化</strong></td><td>自定义财务字段（如应收应付），支持多表聚合分析财务数据</td><td>集成财务模块，实现“订单-生产-财务”数据联动，支持成本精准归集</td><td>打通销售/供应链/财务流程，内置“应收账期预警”，坏账率控制在1.5%以内</td><td>对接ERP实现<strong>业财数据同步</strong>，支持生产-销售财务联动</td><td>支持<strong>跨境回款规则配置</strong>，对接支付系统实现实时到账提醒</td></tr></tbody></table><p><strong>关键场景验证</strong>：</p><ul><li>超兔：销售人员在APP录入客户订单，生产部门通过Web端实时看到排产需求，仓库用小程序扫码出库，数据全链路同步；</li><li>SAP：某汽配企业通过Business One集成生产/物流/财务，跨部门协作效率提升40%；</li><li>Dynamics 365：某汽车零部件企业通过Teams会议纪要自动生成生产任务，数据分析周期从“周级”压缩至“日级”；</li><li>OKKI：某外贸工贸企业通过移动端实时跟踪跨境物流，交付周期缩短30%。</li></ul><h3>（三）工业场景适配性：从“通用”到“垂直”的精准度</h3><p>工业/工贸企业的核心痛点是“业务场景复杂且高度行业化”——半导体企业需要晶圆制造合规，汽配企业需要IATF 16949标准，外贸企业需要跨境回款安全。五大平台的场景适配性差异直接决定了“能否解决真问题”：</p><h4>1. 核心场景覆盖对比</h4><table><thead><tr><th><strong>场景</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td>订单-生产-交付协同</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>设备维护与预防性保养</td><td>✅（自定义表单）</td><td>✅（MES集成）</td><td>✅（Power Apps配置）</td><td>✅</td><td>❌</td></tr><tr><td>跨境贸易（多语言/货币）</td><td>❌</td><td>✅（S/4HANA）</td><td>✅（Dynamics 365 Commerce）</td><td>❌</td><td>✅</td></tr><tr><td>行业合规（如IATF 16949）</td><td>❌</td><td>✅（Business One汽配模板）</td><td>✅（制造业解决方案）</td><td>✅</td><td>❌</td></tr><tr><td>非标订单管理</td><td>✅（自定义工作流）</td><td>✅（Business One）</td><td>✅（Power Apps）</td><td>✅</td><td>✅</td></tr></tbody></table><h4>2. 典型行业适配案例</h4><ul><li><strong>超兔</strong>：某电子工贸企业通过“功能白名单+自定义工作流”，快速调整订单审批流程，支持小批量非标订单生产，上线周期2周；</li><li><strong>SAP</strong>：杭州某半导体企业通过Business One构建晶圆制造合规体系，支撑新产线快速部署；某汽配企业通过Business One固化18项核心流程，符合IATF 16949标准；</li><li><strong>Dynamics 365</strong>：某机械制造企业通过Power Apps搭建设备维护模块，实现预防性维护计划自动提醒，设备停机率下降25%；</li><li><strong>八百客</strong>：某光伏企业通过PaaS平台自定义生产订单关联流程，实现“销售需求→生产排产”实时联动；</li><li><strong>OKKI</strong>：某外贸工贸企业通过跨境合同模板+回款预警，将坏账率从5%降至1.2%。</li></ul><h3>（四）实施与成本：平衡“定制深度”与“投入效率”</h3><p>工业企业数字化的常见陷阱是“为了定制化投入过高成本”，因此“实施周期”与“总成本”是关键决策因素：</p><table><thead><tr><th><strong>维度</strong></th><th>超兔一体云</th><th>SAP</th><th>Microsoft Dynamics 365</th><th>八百客CRM</th><th>OKKI CRM</th></tr></thead><tbody><tr><td><strong>部署方式</strong></td><td>云原生（SaaS）</td><td>云（S/4HANA Cloud）+本地（Business One）+混合</td><td>云（Dynamics 365 Cloud）+本地（On-Premises）+混合</td><td>云原生（SaaS）+本地部署</td><td>云原生（SaaS）</td></tr><tr><td><strong>实施周期</strong></td><td>小功能调整1-3天，全模块上线2-4周</td><td>中小企业（Business One）4-8周；大型企业（S/4HANA）3-6个月</td><td>低代码模块1-2周，复杂集成4-8周</td><td>复杂流程定制4-8周，通用模块2-4周</td><td>外贸场景快速上线2-4周</td></tr><tr><td><strong>总成本（年）</strong></td><td>中小工业企业：1-5万（SaaS订阅）</td><td>中小企业（Business One）：25万以内（license+实施）；大型企业：100万+</td><td>中小工业企业：10-30万（SaaS订阅+实施）；大型企业：50万+</td><td>中大型企业：20-50万（PaaS订阅+实施）</td><td>外贸企业：5-20万（SaaS订阅+实施）</td></tr><tr><td><strong>维护难度</strong></td><td>业务人员通过可视化工具自主调整，无需IT依赖</td><td>中小企业需依赖实施商，大型企业需专业IT团队</td><td>业务人员通过Power Platform自主维护，IT仅需支撑集成</td><td>需IT团队或实施商支撑复杂配置</td><td>业务人员自主调整外贸流程，IT支撑跨境集成</td></tr></tbody></table><p><strong>小结</strong>：</p><ul><li>低成本快上线选<strong>超兔</strong>：SaaS模式降低初始投入，可视化工具减少维护成本；</li><li>中小工业企业选<strong>SAP Business One</strong>：25万以内的总成本覆盖核心流程，行业模板降低实施风险；</li><li>微软生态用户选<strong>Dynamics 365</strong>：Office融合提升协作效率，低代码降低定制成本；</li><li>外贸企业选<strong>OKKI</strong>：跨境场景快速上线，成本可控；</li><li>中大型复杂企业选<strong>八百客</strong>：PaaS平台支撑深度定制，适配复杂流程。</li></ul><h2>三、可视化对比工具：Mermaid图表辅助决策</h2><h3>1. 核心能力框架脑图（Mermaid）</h3><pre><code>mindmap
  root((工业/工贸数字化体系))
    客制化能力
      超兔一体云: 可视化自定义(菜单/工作流/多表聚合)、小步迭代
      SAP: 行业模板(半导体/汽配)、系统集成(WMS/MES)
      Dynamics 365: Power Apps低代码、Office生态融合
      八百客: PaaS可视化配置、复杂流程定制
      OKKI: 外贸场景定制、跨境规则配置
    多端协同能力
      超兔一体云: 多端覆盖(Web/APP/小程序/RPA)、全链路数据同步
      SAP: 内部集成(ERP/WMS/MES)、外部Business Network
      Dynamics 365: Office融合(Outlook/Teams)、多角色终端
      八百客: PC/移动端同步、数据看板监控
      OKKI: 跨境多语言、物流跟踪
    场景适配性
      超兔一体云: 中小工业、非标订单
      SAP: 半导体/汽配、合规场景
      Dynamics 365: 机械制造、设备维护
      八百客: 光伏/制造、生产-销售协同
      OKKI: 外贸工贸、跨境回款</code></pre><h3>2. 多端协同流程时序图（Mermaid）</h3><p>以“销售订单→生产排产→物流交付”为例，展示各平台的协同逻辑：</p><pre><code>sequenceDiagram
    participant 销售(超兔APP) as S
    participant 生产(Web端) as P
    participant 仓储(小程序) as W
    participant 财务(Web端) as F
    participant SAP系统 as SAP
    participant Dynamics 365 as D365
    participant OKKI as O

    %% 超兔流程
    S-&gt;&gt;超兔系统: 录入客户非标订单
    超兔系统-&gt;&gt;P: 同步订单需求至生产排产
    P-&gt;&gt;超兔系统: 反馈生产周期
    超兔系统-&gt;&gt;W: 同步出库指令
    W-&gt;&gt;超兔系统: 扫码出库确认
    超兔系统-&gt;&gt;F: 同步应收数据

    %% SAP流程
    S-&gt;&gt;SAP Business One: 录入订单
    SAP Business One-&gt;&gt;MES系统: 触发生产工单
    MES系统-&gt;&gt;SAP Business One: 反馈生产进度
    SAP Business One-&gt;&gt;WMS系统: 触发出库
    WMS系统-&gt;&gt;SAP Business One: 反馈库存
    SAP Business One-&gt;&gt;F: 同步财务凭证

    %% Dynamics 365流程
    S-&gt;&gt;Outlook: 调取客户画像，发送报价邮件
    Outlook-&gt;&gt;D365: 同步邮件至CRM
    D365-&gt;&gt;Teams: 生成生产跟进任务
    Teams-&gt;&gt;P: 同步任务至生产排产
    P-&gt;&gt;D365: 反馈生产状态
    D365-&gt;&gt;Excel: 生成BI报表
    Excel-&gt;&gt;F: 同步财务数据

    %% OKKI流程
    S-&gt;&gt;OKKI移动端: 录入跨境订单
    OKKI移动端-&gt;&gt;供应商: 同步采购需求
    供应商-&gt;&gt;OKKI: 反馈备货状态
    OKKI-&gt;&gt;物流商: 触发国际物流
    物流商-&gt;&gt;OKKI: 同步Tracking Number
    OKKI-&gt;&gt;F: 同步回款数据</code></pre><h2>四、总结与建议</h2><p>在工业 4.0 与全球供应链重构的大背景下，工业/工贸企业数字化转型已成为提升竞争力的必由之路。“客制化 + 多端协同”能力是构建柔性业务数字化体系的核心要素，能够帮助企业精准适配复杂业务场景，实现全链路数据贯通，提升运营效率和市场响应速度。</p><p>通过对超兔一体云、SAP、Microsoft Dynamics 365、八百客 CRM、OKKI CRM 五大主流平台在客制化能力、多端协同能力、工业场景适配性、实施与成本四大核心维度的深度横评，我们可以看到每个平台都有其独特的优势和适用场景。企业在选择数字化体系时，应充分考虑自身的业务特点、发展阶段、行业需求以及预算限制，做出最为合适的决策。</p><p>对于小步快跑型、追求低成本快上线的中小工业企业，超兔一体云是不错的选择，其可视化自定义功能降低了技术门槛，SaaS 模式减少了初始投入和维护成本；行业合规要求高的企业，如半导体、汽配等行业，SAP 的垂直行业模板和强大的系统集成能力能够确保企业满足严格的合规标准；微软生态用户可以借助 Dynamics 365 的低代码平台和 Office 融合优势，提升协作效率并降低定制成本；外贸企业则可以优先考虑 OKKI CRM，其专属的跨境场景定制和可控的成本能够有效解决跨境协同和回款等痛点问题；而中大型复杂企业，尤其是有深度流程定制需求的企业，八百客 CRM 的 PaaS 平台能够提供强有力的支持。</p><p>总之，选择合适的数字化体系是工业/工贸企业实现业务流程柔性化与数字化升级的关键一步。希望本文的分析和建议能够为企业在数字化转型的道路上提供有价值的参考，助力企业在激烈的市场竞争中脱颖而出。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务与价格以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[【TVM教程】Pass 基础设施 超神经HyperAI ]]></title>    <link>https://segmentfault.com/a/1190000047574799</link>    <guid>https://segmentfault.com/a/1190000047574799</guid>    <pubDate>2026-01-27 12:09:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>TVM 现已更新到 0.21.0 版本，[TVM 中文文档]已经和新版本对齐。</p><p>Apache TVM 是一个深度的深度学习编译框架，适用于 CPU、GPU 和各种机器学习加速芯片。更多 TVM 中文文档可访问 →[Apache TVM]</p><p>在线运行 TVM 学习教程</p><p>链接是：<a href="https://link.segmentfault.com/?enc=H2yGnTdVbK%2BRFloXeTHpKQ%3D%3D.jFtkleP5MYvQUC%2FEPlgxM5dhdbtQBDusqCq5ofLX8e51FdfzwrAAvVzxbtyFCmTrMJRvOj3ywK2sq5V5cXExmxfxsrHGH%2FPHe3DxS4gwiobra0h0rEDVj0k941vQ7Dqc9PeSf8AUAZwj6quZyxnJfrZAB%2B3P%2BUdmiw3OB8mb6iga%2BGY3xpUbCpGF0XeThhzn%2BK0OaaZKr9FPV9eX1Ie3CYYz1wpnUBuJCLicnfIJBDw%3D" rel="nofollow" target="_blank">https://hyper.ai/notebooks/48919?utm_source=Distribute&amp;utm_medium=Distribute-TVM&amp;utm_campaign=Distribute-TVM-260126</a></p><p>Relax 与 TVM IR 都包含一系列优化传递（optimization passes），用于改进模型在特定设备上的性能指标，例如推理平均时间、内存占用或功耗。这些优化包括标准优化与机器学习特定优化，如常量折叠（constant folding）、死代码消除、算子布局变换、算子融合、缓冲区处理和循环变换等。每个传递都是基于收集的分析结果进行的 IR-to-IR 转换。</p><p>然而，随着 TVM 的快速发展，越来越需要一种系统化且高效的方式来管理这些传递。此外，一个通用的框架能够在 TVM 栈的不同层次（例如 Relax 和 tir）之间管理传递，这为开发者快速原型化和集成新传递铺平了道路。</p><p>本文档介绍了这种基础设施的设计，它结合了生产级编译器中用于管理优化传递的方式，以及现代深度学习框架用于构建层次化结构的风格。</p><p>例如，许多现有的生产级编译器（如 GCC 与 LLVM） 采用「传递管理器（pass manager）」来高效管理传递执行。最初传递数量较少时管理很简单，但成熟编译器可能包含数百个独立传递。外部用户往往希望添加自定义传递，并能正确调度，而无需手动修改固定顺序。</p><p>类似地，现代深度学习框架（如 Pytorch 与 MXNet Gluon）也倾向于通过<a href="https://link.segmentfault.com/?enc=BBbq%2BcC9SgxeE4jEuKUY5g%3D%3D.s2ln6rlsFpjnmB4VFeCrvFf6WKBYhyNlJytlMTW7W3LFVs927kLZk2D%2F6ZlMwg%2BemLW4ZRLsZJroGRUZzV7DuzB4hWvJTE7MtVG%2BsBEuYLgWvvtVZOhKVp%2BLNoee%2FQ2A" rel="nofollow" target="_blank">Sequential</a>和<a href="https://link.segmentfault.com/?enc=IKlqnGa6w%2BJpgmYe%2B5Cg%2Bg%3D%3D.wrChXS0GORblgccGJKBd4aR%2FeZ%2BKLPGxovJbozbTNgAuEixspt%2BDzib9NhRGC%2B10tDNo6ux1lGmJqSAMKBZVXHrQMAytQ0eFUmUIZ9kbwlo%3D" rel="nofollow" target="_blank">Block</a>实现类似「传递式」层构建机制。 借助这些构造，框架能够轻松将模块或层添加到容器中，从而快速搭建神经网络。</p><p>TVM 的传递基础设施设计灵感主要来自 LLVM 的层次化传递管理器 以及流行深度学习框架的模块化容器。 该系统的主要目标包括：</p><ol><li>支持更灵活的优化编排，让用户能自由构建自定义优化流水线。</li><li>提供便捷的调试机制。</li><li>让开发者无需手动解决传递之间的依赖。</li><li>简化新传递的实现方式，例如允许用户直接用 Python 实现一个传递，由系统自动管理其执行。</li></ol><h2>设计概述<a href="https://link.segmentfault.com/?enc=0Z3G8k4vZX1BdW7QOZJM%2Bg%3D%3D.BFuL8%2BbzHRFUPZhGlixtYXseldMiwZoAjsh7AQ2FpKXuVdXbpAtTrn1jjaof36jqJekeAD%2Fbj5uJlj5xKPQFn9vvgcaxqrtSCVU62WbkzKeJHdQYKGw3zmI4ib4%2BLc95EHXepVdp5Z0xpydlm0qRLg%3D%3D" rel="nofollow" title="设计概述的直接链接" target="_blank">​</a></h2><p>系统重点关注可扩展性，使用户能快速添加新传递而不破坏兼容性。 其结构包括后端与前端：后端实现核心逻辑，前端则提供简单的 API 供用户创建与控制优化流程。</p><h3>C++ 后端<a href="https://link.segmentfault.com/?enc=JIdcJBO0zxG4OCgNgXW2mg%3D%3D.xHkx%2Bt6KUf0AXJx9w0JcR4aCRagEGXfPfLZxkcsQVgdrMOT%2BE71SJaJ1Yf5dEhVQt79pdQ5ecutU4YGp78sc1lxLFTxfczZbsbkn6eq73d5rxvRqIYW4oFkd%2FieJLDL%2F" rel="nofollow" title="C++ 后端的直接链接" target="_blank">​</a></h3><p>我们提供 <code>PassInfo</code>对象来存储单个传递所需的基本信息：<code>name</code>为传递名，<code>opt_level</code>指示该传递在哪个优化级别启用，<code>required</code>表示执行该传递前所需的其他传递（详见<a href="https://link.segmentfault.com/?enc=Lol77SSSLfYO%2Bitu6j%2B3xg%3D%3D.%2Bsdfcc0x4texBLAI5GjFLdgxLUaXlZc78JFYPKUR6vwJ0ewzTLsHdmFGA%2FOUPOXpCMn%2FxU%2FMilgQSwRfzFn67FL6mHfU03iI8mWdfu%2BApY4%3D" rel="nofollow" target="_blank">include/tvm/ir/transform.h</a>）。 在注册传递时，开发者可以指定传递名称、优化级别与依赖。 <code>opt_level</code>可帮助系统在给定优化级别下判断某个传递是否需要执行； <code>required</code>字段用于自动解析传递依赖。</p><pre><code>class PassInfoNode : public Object {
  ffi::String name;
  int opt_level;
  ffi::Array&lt;ffi::String&gt; required;
};</code></pre><h4>PassContext<a href="https://link.segmentfault.com/?enc=HeT3EsSxJnjLmdp3n%2BrCSw%3D%3D.ytgLnTz0Hn3sDwKvI%2BixXDi8nbsobpSWRR6FDTMd9p%2FgkdyPuHvdwPN2fXgvijtgT9ViE%2Bajivl5gvvkeMSLhd%2BWnbrtW2rePA71I1zNXGlK4SvX0ZQu0BYLR1R2QcrT" rel="nofollow" title="PassContext的直接链接" target="_blank">​</a></h4><p><code>PassContext</code> 携带优化传递所需的关键信息。例如，它包含错误报告系统，方便优化作者诊断失败原因。 <code>PassContext</code>也取代了旧的 <code>BuildConfig</code>（用于配置编译选项，如优化级别、必需/禁用传递等）。例如，我们可以配置在 <code>opt_level=3</code> 下执行所有传递，并通过<code>disabled_pass=xx</code> 禁用某些传递；系统会聚合该级别的所有传递并排除被禁用的项。<code>PassContext</code>还提供对所有传递进行"检测（instrumentation）"的能力，见 <code>pass_instrument_cpp_backend</code>。</p><p>该类支持 Python <code>with</code> 语法，便于在给定配置下执行优化。 同时，用户可以通过 <code>PassContext::Current()</code>在线程安全的方式获取当前上下文， 因为系统使用线程本地存储<code>PassContextThreadLocalStore</code> 来保存上下文对象。</p><pre><code>class PassContextNode : public Object {
 public:
  int opt_level{2};
  tvm::ffi::Array&lt;tvm::Expr&gt; required_pass;
  tvm::ffi::Array&lt;tvm::Expr&gt; disabled_pass;
  mutable ffi::Optional&lt;DiagnosticContext&gt; diag_ctx;
  ffi::Map&lt;ffi::String, Any&gt; config;
  ffi::Array&lt;instrument::PassInstrument&gt; instruments;
};

class PassContext : public NodeRef {
 public:
  TVM_DLL static PassContext Create();
  TVM_DLL static PassContext Current();
  TVM_DLL void InstrumentEnterPassContext();
  TVM_DLL void InstrumentExitPassContext();
  TVM_DLL bool InstrumentBeforePass(const IRModule&amp; mod, const PassInfo&amp; info) const;
  TVM_DLL void InstrumentAfterPass(const IRModule&amp; mod, const PassInfo&amp; info) const;
  /* 其他字段省略 */

 private:
  // 进入 pass 上下文作用域
  TVM_DLL void EnterWithScope();
  // 离开 pass 上下文作用域
  TVM_DLL void ExitWithScope();

  // 用于支持 Python `with` 语法
  friend class tvm::With&lt;PassContext&gt;;
};

struct PassContextThreadLocalEntry {
  /*! rief 默认 pass 上下文 */
  PassContext default_context;
  /*! rief 当前 pass 上下文 */
  std::stack&lt;PassContext&gt; context_stack;
  PassContextThreadLocalEntry() {
    default_context = PassContext(make_node&lt;PassContextNode&gt;());
  }
};

/*! rief 线程本地存储，用于保存 pass 上下文 */
typedef dmlc::ThreadLocalStore&lt;PassContextThreadLocalEntry&gt;
     PassContextThreadLocalStore;</code></pre><h4>Pass 构造<a href="https://link.segmentfault.com/?enc=QFrZHamEI6jiMW%2BfeE2YAA%3D%3D.9k5dGLji5HeaEtC%2BzR522W2C0E3XkPKOMsj6L%2BB1rhqP%2BPy63r1sfS8c7G9DP9zAyOlR%2BUV8aYeVU%2FSnfX0X7M%2BoTmyJM6iZUzEjjfVC31iQ9UFLROi4gz02SyalNI2c" rel="nofollow" title="Pass 构造的直接链接" target="_blank">​</a></h4><p>传递（Pass）基础设施以分层结构设计，可在 Relax/tir 程序的不同粒度上工作。 系统定义了一个纯虚类<code>PassNode</code>，作为各种优化传递的基类。此类包含多个必须在子类中实现的虚函数，适用于模块级、函数级或顺序传递级别。</p><pre><code>class PassNode : Object {
  virtual PassInfo Info() const = 0;
  virtual Module operator()(const IRModule&amp; mod,
                            const PassContext&amp; pass_ctx) const = 0;
};</code></pre><p>该函数对象定义了传递的执行方式： 每个传递都在特定上下文 <code>PassContext</code>下作用于一个 <code>IRModule</code>， 并以 <code>Module</code> 到 <code>Module</code> 的方式实现。因此，所有传递都以模块为单位更新整个 IR。</p><p>系统实现了多个 <code>PassNode</code> 子类来支持不同类型的优化： 包括函数级传递、模块级传递与顺序传递（sequential pass）。 每个子类本身都可充当一个传递管理器，例如：它们可以收集所需传递并执行，或基于元信息建立依赖图。完整定义见<a href="https://link.segmentfault.com/?enc=5hw%2Bdz%2Fz9n59vRVX1Xrm4w%3D%3D.3ohJyz9NtzwiNB7lnQXRAEVkNH%2F2zpXloc54xsBZVae6Ccq9Lt3jUlj2ykhxUasvHso0425Sq%2Bkawe5DJoX8xA%3D%3D" rel="nofollow" target="_blank">src/ir/transform.cc</a>。</p><h4>模块级传递<a href="https://link.segmentfault.com/?enc=uEDLrDMUk72oay2XY%2B3HZQ%3D%3D.HV42CCTR1K42PsvjlA64EKXTr01EWbDHnKP5TO8XCendYE8gQHlEc4hNfqaHrQ4GlfHajYb0h9uLNbeiE6Klzsm04zXsmZPdN1xGEkF2oL5F1azrpaA5bJORzF8gqWWICn%2BMBfC8J7btlggHh7XZkY%2B6UA52X%2FfXO3dHQLLCJ3c%3D" rel="nofollow" title="模块级传递的直接链接" target="_blank">​</a></h4><p>模块级传递主要用于全局或过程间优化（IPO），类似于 LLVM 中的模块传递。Relax 中一些典型需要全局视图的优化（如 A-normal form 转换、lambda 提升）就属于此类。 在该级别，用户可以在模块中添加或删除函数。</p><pre><code>class ModulePassNode : PassNode {
  PassInfo pass_info;
  std::function&lt;Module(Module, PassContext)&gt; pass_func;
  Module operator()(const Module&amp; mod, const PassContext&amp; pass_ctx) const final;
  // 其他成员/方法省略
};</code></pre><p><code>pass_info</code> 存储模块传递的相关信息，<code>pass_func</code> 定义实际优化逻辑。例如，在模块上执行死代码消除可在 <code>pass_func</code> 中实现，它将删除模块中未使用的函数。 此字段被设计为「打包函数（packed function）」， 因此优化逻辑既可用 C++ 实现，也可用 Python 实现。</p><h3>函数级传递<a href="https://link.segmentfault.com/?enc=SaBmNP%2BFKi3fm%2FwEFqtpHg%3D%3D.%2Bo3QI%2BjIV1Id6kTI3TqACJpHcH8bPy%2FxZ%2F13y8nyZO5TyTyms3tKANiQYF%2FCKxHcm9SZJJYqsaYZJAxVeHlJcZISUNg%2BUxihyMtJ3tEFdE8UubJJ%2FCAWMw5loRXDh7oSm6%2BlUzf6jwdh1sSkTKFjS4dXhhS5IxuwEFcizCUDmIk%3D" rel="nofollow" title="函数级传递的直接链接" target="_blank">​</a></h3><p>函数级传递用于实现 Relax/tir 模块中函数内的优化。它一次提取模块中的一个函数进行优化，输出优化后的 Relax <code>Function</code> 或 tir <code>PrimFunc</code>。多数优化都属于此类，如 Relax 的公共子表达式消除、推理简化，或 tir 的向量化与内存扁平化。</p><p>函数级传递仅作用于单个函数（Relax 或 tir），因此无法通过此类传递添加或删除函数，因为其不具备全局信息。</p><pre><code>class FunctionPassNode : PassNode {
  PassInfo pass_info;
  std::function&lt;Function(Function, Module, PassContext)&gt; pass_func;
  Module operator()(const Module&amp; mod, const PassContext&amp; pass_ctx) const final;
  bool SkipFunction(const Function&amp; func) const;
  // 其他成员/方法省略
};</code></pre><p><code>pass_info</code> 与模块级传递相同。 <code>pass_func</code>接受函数与模块作为输入，可在函数上执行优化； 函数若被注解为<code>SkipOptimization</code>，将被跳过。</p><h4>顺序传递（Sequential Pass）<a href="https://link.segmentfault.com/?enc=s1VKOg5MM2OZwQP2Q%2Bz0OQ%3D%3D.50HcZFAFVwhEPj2AqAFP38xTbFGomt%2Fw9YACiqQCVRgfaVrixId%2BMI301w8tSECU2enzG8sMjkDk1BT%2B7TTm2%2F8YFqDB0DOUl%2B63fvNGDMn6uhJQI6qZ9sGDPX3nDr4I%2BNYhCvrY7l3iXWJwP%2FxrD0%2FFf6HrBRZDULcuZloPfy4%3D" rel="nofollow" title="顺序传递（Sequential Pass）的直接链接" target="_blank">​</a></h4><p><code>SequentialPass</code> 类似于 PyTorch 的 <code>nn.Sequential</code>，可包含多个顺序执行的传递。</p><pre><code>class SequentialPassNode : PassNode {
  PassInfo pass_info;
  // 需要执行的传递列表
  ffi::Array&lt;Pass&gt; passes;
  bool PassEnabled(const PassInfo&amp; info) const;
  Module operator()(const Module&amp; mod, const PassContext&amp; pass_ctx) const final;
};</code></pre><p>以下展示顺序传递的执行逻辑：系统会按照传递添加的顺序依次执行。</p><pre><code>Module SequentialNode::operator()(const Module&amp; module,
                                  const PassContext&amp; pass_ctx) const {
  Module mod = module;
  for (const Pass&amp; pass : passes) {
    ICHECK(pass.defined()) &lt;&lt; "Found undefined pass for optimization.";
    const PassInfo&amp; pass_info = pass-&gt;Info();
    if (!PassEnabled(pass_info))  continue;
    for (const auto&amp; it : pass_info-&gt;required) {
      const auto* name = it.as&lt;tvm::ir::StringImm&gt;();
      ICHECK(name);
      mod = GetPass(name-&gt;value)(mod, pass_ctx);
    }
    mod = pass(mod, pass_ctx);
  }
  return mod;
}</code></pre><p>在执行传递前，系统会判断该传递是否启用：首先检查是否被用户禁用，其次查看是否被显式声明为必需。若仍未确定，则根据 <code>opt_level</code> 判断是否执行。</p><p>执行时，系统会根据传递名从注册表中获取对应实现：</p><pre><code>Pass GetPass(const std::string&amp; pass_name) {
  using tvm::runtime::Registry;
  std::string fpass_name = "relax.transform." + pass_name;
  const std::optional&lt;tvm::ffi::Function&gt; f = tvm::ffi::Function::GetGlobal(fpass_name);
  ICHECK(f.has_value()) &lt;&lt; "Cannot find " &lt;&lt; fpass_name
                        &lt;&lt; "to create the pass " &lt;&lt; pass_name;
  return (*f)();
}</code></pre><p>系统还提供辅助函数用于创建各类传递，并暴露给 Python 前端：</p><pre><code>Pass CreateFunctionPass(
    std::function&lt;Function(Function, IRModule, PassContext)&gt; pass_func,
    int opt_level,
    ffi::String name,
    ffi::Array&lt;ffi::String&gt; required);

Pass CreatePrimFuncPass(
    std::function&lt;PrimFunc(PrimFunc, IRModule, PassContext)&gt; pass_func,
    int opt_level,
    ffi::String name,
    ffi::Array&lt;ffi::String&gt; required);

Pass CreateModulePass(
    std::function&lt;IRModule(IRModule, PassContext)&gt; pass_func,
    int opt_level,
    ffi::String name,
    ffi::Array&lt;ffi::String&gt; required);

Pass Sequential(tvm::ffi::Array&lt;Pass&gt; passes, PassInfo pass_info);</code></pre><h4>传递注册<a href="https://link.segmentfault.com/?enc=ZYa3XasS8K0Xco32jToXWw%3D%3D.lFc835exOytrcfDP14D%2BmIqyeXeAtIPliacAhvwMok%2FIjQJhXCn5bD0R9vLuOwTj9s0pM1kpYeuru4Cto5WiFyPql5sZJXAdr3hadecC%2BQbbhhQn7hAsEQ6ZKtn8cX8Rdm9CtS%2BzsaN43sv%2B5s4wXw%3D%3D" rel="nofollow" title="传递注册的直接链接" target="_blank">​</a></h4><p>前文介绍了不同粒度的传递和编译上下文。 下面展示如何注册一个传递。以常量折叠（constant folding）为例， 它用于在 Relax 函数中折叠常量（实现位于 <a href="https://link.segmentfault.com/?enc=zqelsGgLi9gffU3mov%2BzeA%3D%3D.pgFeJaOJlQhNPYdtoaQ7QJZncEXraV%2FFQfZIdIKZQGi5%2BJM0f0LRwvCBx0ZcAo2autyE1FFE2cVyEK245kyZWnqqOJZ3KKrDiw%2BWIjbP%2BvY%3D" rel="nofollow" target="_blank">src/relax/transforms/fold_constant.cc</a>）。</p><p>该传递提供了 <code>Expr</code> 到 <code>Expr</code> 的转换 API：</p><pre><code>Expr FoldConstant(const Expr&amp; expr);</code></pre><p>要将其注册到传递基础设施中，首先需要确定传递的粒度。常量折叠作用于函数级，因此通过 <code>CreateFunctionPass</code> 创建：<code>pass_func</code> 以打包函数形式返回，用于对 [IRModule]{.title-ref} 中的每个函数调用该转换 API。 <code>{}</code> 表示该传递没有前置依赖；若有依赖，开发者需明确列出。</p><p>同时，注册名为 <code>"relax.transform.FoldConstant"</code> 的 API 入口，使该传递可被 C++ （例如以上的 <code>GetPass</code> ）与 Python 访问：</p><pre><code>namespace transform {

Pass FoldConstant() {
  auto pass_func =
      [=](Function f, IRModule m, PassContext pc) { return ConstantFolder::Fold(f, m); };
  return CreateFunctionPass(pass_func, 0, "FoldConstant", {});
}

TVM_FFI_STATIC_INIT_BLOCK() {
  namespace refl = tvm::ffi::reflection;
  refl::GlobalDef().def("relax.transform.FoldConstant", FoldConstant);
}

}  // namespace transform</code></pre><p>为方便其他 C++ 模块调用，在<a href="https://link.segmentfault.com/?enc=KKUh6FsY1fiRYYx1q2SYpA%3D%3D.X080JancJ4%2BRxA2DfC8CkVVzZ8nOI0i7zYjiX6CGJ2lSS94ebth7ytlRZX62AeAWgqvDqOS4zFDvOh7PIe%2FL02LzBwRnXzojbwuTHiT05N4%3D" rel="nofollow" target="_blank">include/tvm/relax/transform.h</a>中声明：</p><pre><code>TVM_DLL Pass FoldConstant();</code></pre><h4>传递检测（Pass Instrument）<a href="https://link.segmentfault.com/?enc=GgqEu9B6pFMEMavySRKjRg%3D%3D.lMewJJD0TTkrNG35vY7f3Tx4h%2Fuauql%2BZnZrkkQNOoU2FAf1ut4ytpdf0Fh0WDM2pIv0HyXcJOBI4l4BlJGDP5HXzm%2FMNfNTxBA3bdGFFgcgmXCHFgz2A37aVRGvaN7nZF049%2BfP877u551tohf2DbACrWg7I9Puy2frt0aHA5E%3D" rel="nofollow" title="传递检测（Pass Instrument）的直接链接" target="_blank">​</a></h4><p>传递检测机制用于分析传递本身，例如统计执行时间与内存占用，或观察 IR 如何被改变。</p><p>我们在 <code>PassContext</code> 生命周期中引入四个检测点：</p><pre><code>TVM_DLL void InstrumentEnterPassContext();
TVM_DLL void InstrumentExitPassContext();
TVM_DLL bool InstrumentBeforePass(const IRModule&amp; mod, const PassInfo&amp; info) const;
TVM_DLL void InstrumentAfterPass(const IRModule&amp; mod, const PassInfo&amp; info) const;</code></pre><p><code>InstrumentEnterPassContext</code> 在进入 <code>PassContext</code> 作用域时调用。</p><p><code>InstrumentExitPassContext</code> 在离开 <code>PassContext</code> 或执行发生异常时调用。当通过 :py<code>tvm.transform.PassContext</code>的<code>override_instruments</code> 覆盖检测器时也会触发，见<code>pass_instrument_overriden</code>。</p><p><code>InstrumentBeforePass</code> 在传递执行前调用； 若该传递应执行，则在执行后调用 <code>InstrumentAfterPass</code>。其伪代码如下：</p><pre><code>if (pass_ctx.InstrumentBeforePass(ir_module, pass_info)) {
  new_ir_module = run_pass(ir_module, pass_ctx);
  pass_ctx.InstrumentAfterPass(new_ir_module, pass_info);
  return new_ir_module;
}</code></pre><p><code>PassInstrument</code>接口允许你在上述四个阶段插入自定义逻辑。 可向单个<code>PassContext</code> 注册多个检测器实例，它们将按 <code>instruments</code>指定的顺序依次调用。</p><p>接口定义如下：</p><pre><code>namespace instrument {

class PassInstrumentNode : public Object {
 public:
  ffi::String name;
  virtual void EnterPassContext() const = 0;
  virtual void ExitPassContext() const = 0;
  virtual bool ShouldRun(const IRModule&amp; mod, const transform::PassInfo&amp; info) const = 0;
  virtual void RunBeforePass(const IRModule&amp; mod, const transform::PassInfo&amp; info) const = 0;
  virtual void RunAfterPass(const IRModule&amp; mod, const transform::PassInfo&amp; info) const = 0;
  /* 其他字段省略 */
};

class PassInstrument : public ObjectRef {
 public:
  TVM_FFI_DEFINE_OBJECT_REF_METHODS_NULLABLE(PassInstrument, ObjectRef, PassInstrumentNode);
};

}  // namespace instrument</code></pre><p>Python 前端提供了便捷方式来实现 <code>PassInstrument</code>，见<code>pass_instrument_py_frontend</code>。</p><p>在一个 <code>PassContext</code> 中，某个 <code>PassInstrument</code> 实例的调用顺序如下：</p><pre><code>with PassContext(instruments=[pi])  # pi 为某个 PassInstrument 实现
    pi.EnterPassContext()

    if pi.ShouldRun(Pass1):
        pi.RunBeforePass()
        Pass1()
        pi.RunAfterPass()

    if pi.ShouldRun(Pass2):
        pi.RunBeforePass()
        Pass2()
        pi.RunAfterPass()

    pi.ExitPassContext()</code></pre><p>以下简述 <code>PassInstrument</code> 与 <code>PassContext</code> 方法之间的关系，详见 <a href="https://link.segmentfault.com/?enc=ZC3vmRwxZb5PAG3Oorh0Rw%3D%3D.%2FxuH6IoOGVbvBYw7fxPN6O3LCVPaNbbrCXUuEK0WrpY7D%2F6djAdLx3fVwhlj%2BHXs4LFY73x4uO%2ByKfH9eeg34g%3D%3D" rel="nofollow" target="_blank">src/ir/transform.cc</a>：</p><ul><li><p><code>InstrumentEnterPassContext</code></p><ul><li><code>EnterPassContext()</code> 按传入 <code>instruments</code> 的顺序执行。</li><li>若执行中抛出异常，<code>PassContext</code> 会清空所有已注册的检测器。</li><li>然后对已成功执行 <code>EnterPassContext()</code> 的检测器依次调用 <code>ExitPassContext()</code>。</li><li>例如，注册了 A、B、C 三个检测器，A 成功，B 抛异常，则 C 不会执行；随后调用 A 的 <code>ExitPassContext()</code>。</li></ul></li><li><p><code>InstrumentExitPassContext</code></p><ul><li>各检测器的 <code>ExitPassContext()</code> 按 <code>instruments</code> 顺序执行。</li><li>若发生异常，<code>instruments</code> 会被清空。</li><li>抛出异常后注册的检测器不会执行 <code>ExitPassContext</code>。</li></ul></li><li><p><code>InstrumentBeforePass</code></p><ul><li>若该传递未被显式列为"必需"，则会调用 <code>ShouldRun</code>。</li><li>若未被 <code>ShouldRun</code> 阻塞，则按顺序调用 <code>RunBeforePass</code>。</li><li>该函数返回布尔值，指示该传递是否应执行。</li><li>若发生异常，将立即抛出；Python 依靠上下文管理器安全退出（确保各检测器的 <code>ExitPassContext</code> 被调用；C++ 见 <a href="https://link.segmentfault.com/?enc=BMqzjGcIjKeO8LrQ5EBXUA%3D%3D.cCx%2F0SSzikejtA1nc%2FSvdaw3AanqWskS86866ui9696V0c5R4bNgyhwCXAs4hGIow6E17AzL0Ptux2%2BH%2Ba9xhP72bFuL0IDmN79KsLk8P%2FI%3D" rel="nofollow" target="_blank">include/tvm/support/with.h</a>）。</li></ul></li><li><p><code>InstrumentAfterPass</code></p><ul><li>按顺序调用 <code>RunAfterPass</code>。</li><li>若发生异常，将立即抛出；依靠上下文管理器或 <code>With</code> 类（<a href="https://link.segmentfault.com/?enc=8Hu9gbR58sGe9FjJbxEqdg%3D%3D.Lg2FI3BB4fo7KuWWSIm5VrkLJDKzjou%2BXUxf5PGhi%2BQZEflsARg%2BXMEEBZmjmAqrfK1on31HjAP%2BDJks7sx8ROwEq%2B5lYWqdOr25i0JNIBE%3D" rel="nofollow" target="_blank">include/tvm/support/with.h</a>）安全退出。</li></ul></li></ul><h4>内置检测器<a href="https://link.segmentfault.com/?enc=L0tLsAJmFfTfGUfhnPrtlQ%3D%3D.Qhev3ezj4R8KvIbvi1YdXDk%2F8tJegdKqGEvI9ReORtqf%2Fnm%2FhbigcsPietVSYvTiq5aDUfkozr50m6ce%2FXw0dFXuvoh7QJ%2FPdd%2F%2FVqSFYIaGFab2HPDOvAh2gibnM1q8dhevGpEozaOOKBeCxVKKF4DnyspYO7Mk2oh6LrHwwbo%3D" rel="nofollow" title="内置检测器的直接链接" target="_blank">​</a></h4><p>系统内置若干检测器（标注 <em>TODO</em> 的尚未实现）：</p><ul><li><p><strong>PassTimingInstrument</strong>（见 <a href="https://link.segmentfault.com/?enc=xFi4iSw%2BB%2FDyM%2Fz0R5M8fA%3D%3D.q63clsQKuZCmoyYJsYFzOwSc%2FILuywzeiXdaAaWZuH7sm8XGaBuqtI19mTc5AG6W3FKE%2FG%2Bm%2B8Q06q0S7lKk3g%3D%3D" rel="nofollow" target="_blank">src/ir/instrument.cc</a>）</p><ul><li>用于分析各传递的执行时间。</li></ul></li><li><p><strong>PrintIRBefore</strong>（TODO）</p><ul><li>在传递执行前打印 IR。也可通过 :py<code>tvm.transform.PrintIR</code>{.interpreted-text role="func"} 在传递周围插入打印实现；但使用检测器无需修改传递序列。</li></ul></li><li><p><strong>PrintAfter</strong>（TODO）</p><ul><li>在传递执行后打印 IR。</li></ul></li></ul><h3>Python 前端<a href="https://link.segmentfault.com/?enc=NLA5XijFDtrmZO6kgrv7pg%3D%3D.8qDJr5t%2B%2FGYBCF2PEB%2B%2Fe8W5%2FKlyDmOBzynU72hLGHLXY%2FCv3hzI5W62GO40aFTLZR02KU4EyyoskYopkethpSVyzlyWSqkgXViwPF%2FemNwI1bldMzyMx5wdn%2Fw%2BkgeKgxup3BPc8vwnis%2BQxD9Pzw%3D%3D" rel="nofollow" title="Python 前端的直接链接" target="_blank">​</a></h3><p>前端仅需少量 API 即可创建并执行传递（完整实现见<a href="https://link.segmentfault.com/?enc=dkW49Bt5JoBHkhQPTu%2B0Wg%3D%3D.XQBuNiY2516r2Nhh8pw8IGXozZgGXuM9AT2EfJf%2B11QlgHe5SQZVd8nthFNyc7JB5%2FKTfHcmAhhxtZol5lsdMUZO3U5cXRhlLtK%2F3OvXbqE%3D" rel="nofollow" target="_blank">python/tvm/relax/transform/transform.py</a>与<a href="https://link.segmentfault.com/?enc=n5zIi3g27eNkB75dJ1axBQ%3D%3D.X3iBHHFy0Q1kxUqkP8fNXvdqlFoXT9TOFYw7rKwkL3cQeQwpY8l3TDKucN6KA6uBctesKNu5xqjbIrVYxOFhnRAO7KNOHZYOYh3CYBGzPpo%3D" rel="nofollow" target="_blank">python/tvm/ir/transform.py</a>）。后端将根据提供的信息决定如何创建 Pass 对象。</p><h4>PassContext<a href="https://link.segmentfault.com/?enc=fOPF%2BqrpaFAU1ozv%2B8FWLA%3D%3D.Szj%2BQtl57Se5rfhMGcYqzSFLoO9dD4EvjhS11pSjiWWPKVGWpLCwBX4qwWDVFLnLvTPX63WMGYiUn1gvhNqxaAX9%2B%2BJMkoxR1C%2BaLqAJQn03YEF7qhcBA1FfLjIehau0" rel="nofollow" title="PassContext的直接链接" target="_blank">​</a></h4><p>Python 前端为 <code>PassContext</code> 提供了包装以支持 <code>with</code> 语法，并提供<code>current</code> 静态方法：</p><pre><code>@tvm_ffi.register_object("transform.PassContext")
class PassContext(tvm.runtime.Object):
    def __enter__(self):
        _transform.EnterPassContext(self)
        return self

    def __exit__(self, ptype, value, trace, config):
        _transform.ExitPassContext(self)

    @staticmethod
    def current():
        """Return the current pass context."""
        return _transform.GetCurrentPassContext()</code></pre><p><code>PassContext</code>用于配置编译选项（优化级别、必需/禁用传递等），并可传入配置字典，以便不同传递读取需要的数据（如回退设备信息、循环展开步数/深度等）。若要从 <code>config</code> 中获取某项配置，其键名需通过<code>TVM_REGISTER_PASS_CONFIG_OPTION</code> 注册，例如循环展开传递：</p><pre><code>TVM_REGISTER_PASS_CONFIG_OPTION("tir.UnrollLoop", UnrollLoopConfig);</code></pre><p>详见<a href="https://link.segmentfault.com/?enc=XMugrdN98F1WqS%2FHvAYDZg%3D%3D.6gu9zlveYrBSyIT5Ka9ljV%2Bd2Yd0eKZL0jSjfKgp5u2YOX61fAzKuPMG7FGkp0zbVRmA0OutUMyf7OYPSwnC19IuHoUX2COqNPuZUcBykqE%3D" rel="nofollow" target="_blank">src/tir/transforms/unroll_loop.cc</a>。</p><h4>Python 中的传递检测<a href="https://link.segmentfault.com/?enc=yEl6xL3lIf5Nfg9WD%2B3MGQ%3D%3D.axUwE9PE%2BMqhUpDHDkUVad9Of%2BFjLf8rXgz2CLmjoWSEx6y5RLq7ND99BoOhhI1R3fhlonnAM6ylrHOpHEi%2Fhty8%2FacvSxGHKzSP5tEt5G9JEbhba3M2Gz7CuhS0BIgK2CAJYzImhGXFdm00wCesusZR5%2Bqotkd0jbFg6KJVmkNobwFSGIklLkSYCeO7n7JL" rel="nofollow" title="Python 中的传递检测的直接链接" target="_blank">​</a></h4><p>使用装饰器（<a href="https://link.segmentfault.com/?enc=5YoBgLghiMzXkPDUJCBoTw%3D%3D.RbblLdmdHT8or%2FmV%2FqjC1Heuw7PuW5FqsDcQVRds51cZkkptnBBj7eYysw93SasgJqvpVsbz4FYy89VnwzEe9eMAf68GO2fLgAAAhmmhw%2BQ%3D" rel="nofollow" target="_blank">python/tvm/ir/instrument.py</a>）可以快速实现 <code>PassInstrument</code>。 推荐使用装饰器方式而非继承：</p><ul><li><code>enter_pass_ctx</code>：进入 <code>PassContext</code> 时执行；</li><li><code>exit_pass_ctx</code>：退出 <code>PassContext</code> 时执行；</li><li><code>should_run</code>：在传递执行前调用，返回该传递是否应执行；</li><li><code>run_before_pass</code>：传递执行前调用；</li><li><code>run_after_pass</code>：传递执行后调用。</li></ul><p>可通过 :py<code>tvm.transform.PassContext</code> 的 <code>instruments</code> 参数注册实例。更多示例见<a href="https://link.segmentfault.com/?enc=uBEfYhTDD1dVNLKnMTwtYw%3D%3D.8joLiWkEkgFSPC3hYPED965keOw2QOzurTzBUgdxL6SuCZfpzfWA9TTJEo0YWz1co2QLHxpjy%2BCxUMojqImaU3unkX8%2BrZ7mW5DWdYEkapw%3D" rel="nofollow" target="_blank">use pass instrument</a>教程。</p><h4>覆盖当前 PassContext 中的检测器<a href="https://link.segmentfault.com/?enc=ffXu0SG6HQ2lRQWAU%2BDFCw%3D%3D.qVOsAlk1UCwwBD3Y3oH2ny9O5F0ew5Ch2BOrCX13352pCwVkhHe87Ly3Qzm7noGZZERlD%2F%2Bx26vMr332gdn1Ws1EYi6sOLMnGet%2BGu1fWzw7X1ygKrk%2BArGqRniaNoQtl%2BcSdZwrBG2lLyJ2%2FcNvsDEixrqdZHLABB%2FaoiqAXcpZd4kiYaebPX05d969u1okLImPT%2F0aYdzmgvmIEA11dFt0ehKxhGbwZi%2F7ZhzXKvs%3D" rel="nofollow" title="覆盖当前 PassContext 中的检测器的直接链接" target="_blank">​</a></h4><p><code>override_instruments</code> 方法可覆盖当前 <code>PassContext</code> 中的 <code>instruments</code>。例如，当未显式创建新 <code>PassContext</code> 而直接运行传递时，仍可将检测器注册到全局上下文：</p><pre><code>cur_pass_ctx = tvm.transform.PassContext.current()
# 覆盖 PassInstrument 实例
cur_pass_ctx.override_instruments([pass_inst])
mod = pass_seq(mod)
result = pass_inst.get_result()</code></pre><p>注意：调用 <code>override_instruments</code> 时，旧检测器的 <code>exit_pass_ctx</code>会被调用，随后新检测器的 <code>enter_pass_ctx</code> 会被调用。</p>]]></description></item><item>    <title><![CDATA[智能体来了从 0 到 1：个人、团队与企业的三种实践起步路径 Agentcometoo ]]></title>    <link>https://segmentfault.com/a/1190000047574810</link>    <guid>https://segmentfault.com/a/1190000047574810</guid>    <pubDate>2026-01-27 12:08:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在实际落地过程中，智能体并不存在统一的“最佳起点”。<br/> 不同规模的使用主体，在资源结构、风险承受能力与目标函数上存在本质差异，因此其从 0 到 1 的实践路径也必然不同。</p><p>从行业实践来看，智能体的起步路径大致可以分为个人、团队与企业三类。</p><hr/><h2>一、个人路径：从单点效率到可复用闭环</h2><p><strong>核心目标：降低认知与执行成本</strong></p><p>个人用户的智能体实践，通常从高频、重复、规则相对稳定的任务开始，其价值不在于复杂架构，而在于“是否真正替代了部分脑力劳动”。</p><h3>1. 实践起点：明确任务边界</h3><p>个人路径的第一步不是选模型，而是<strong>识别可被完整替代的任务单元</strong>。<br/> 典型特征包括：</p><ul><li>输入输出清晰</li><li>中间判断规则可语言化</li><li>错误成本可控</li></ul><h3>2. 实现方式：提示词驱动的逻辑拆解</h3><p>在这一阶段，提示词本身承担着“流程编排”的角色。<br/> 一个有效的个人智能体，往往具备明确的步骤拆解能力，而非单轮问答能力。</p><h3>3. 成熟标志：形成最小自动化闭环</h3><p>当任务能够稳定完成“输入 → 处理 → 输出 → 复用”，个人路径即完成从 0 到 1 的跨越。</p><hr/><h2>二、团队路径：从个人经验到组织能力</h2><p><strong>核心目标：让经验成为可调用的资产</strong></p><p>当智能体进入团队环境，问题不再是“能不能做”，而是“能否被协同使用”。</p><h3>1. 实践起点：知识结构化与共享</h3><p>团队智能体的起点，通常是构建统一的知识检索层。<br/> 通过将分散在文档、会议纪要、历史项目中的经验进行向量化管理，使其成为可被持续调用的组织记忆。</p><h3>2. 关键建设：标准化工作流</h3><p>团队需要的不是“聪明的智能体”，而是<strong>行为一致的智能体</strong>。<br/> 这意味着：</p><ul><li>输入输出格式标准化</li><li>决策逻辑显式化</li><li>结果可追溯</li></ul><h3>3. 演进方向：多智能体分工协作</h3><p>在成熟阶段，不同角色的智能体开始围绕同一任务进行分工，例如生成、校验、总结等环节的协同。</p><hr/><h2>三、企业路径：从试点验证到系统工程</h2><p><strong>核心目标：确定性、可控性与可评估性</strong></p><p>企业级智能体并非“更大的版本”，而是完全不同的问题域。</p><h3>1. 实践起点：基础设施与治理框架</h3><p>企业从 0 到 1 的第一步，往往不是业务，而是：</p><ul><li>权限与调用管理</li><li>数据隔离与安全策略</li><li>成本与性能监控</li></ul><h3>2. 核心能力：全链路可观测</h3><p>企业级智能体需要能够解释：</p><ul><li>每一步做了什么</li><li>为什么这样做</li><li>出现问题如何回溯</li></ul><h3>3. 必要条件：评估与回归机制</h3><p>任何模型升级、流程调整，都必须通过自动化评估集验证，避免对存量业务产生不可预期影响。</p><hr/><h2>四、路径差异背后的共性趋势</h2><p>尽管起步方式不同，但从实践结果来看，所有路径最终都会指向同一个目标：</p><p><strong>从不稳定的智能表现，走向可重复、可验证的确定性系统。</strong></p><p>个人追求效率稳定性<br/> 团队追求协作一致性<br/> 企业追求系统可靠性</p><p>差异存在于阶段，收敛发生在终局。</p><hr/><h2>五、结语</h2><p>智能体并非“越复杂越先进”。<br/> 真正有效的从 0 到 1，始于对自身位置的清醒认知，并止于对技术边界的理性约束。</p>]]></description></item><item>    <title><![CDATA[12家主流IM SDK对比及2026年即时通讯产品推荐 Amymaomao ]]></title>    <link>https://segmentfault.com/a/1190000047574838</link>    <guid>https://segmentfault.com/a/1190000047574838</guid>    <pubDate>2026-01-27 12:07:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>12家主流IM SDK对比及2026年即时通讯产品推荐<br/>在当今企业数字化转型的浪潮中，即时通讯开发工具包（IM SDK）已然成为构建高效协作平台的关键要素。市场上的IM SDK解决方案纷繁复杂，企业该如何精准挑选出契合自身业务需求、技术架构以及安全标准的产品，成了一项至关重要的决策。本文将全面梳理并对比分析12款市场主流的IM SDK，为企业的技术选型提供清晰的指引。<br/>主流IM SDK全景扫描<br/>云屋科技<br/>云屋科技在国内IM领域占据领先地位，其推出的IM SDK强调私有化部署和信创国产化。凭借稳定可靠的消息传输体系和卓越的弱网通信能力，云屋科技的服务覆盖全球196个国家，拥有超10亿的累计用户，平安银行、中通快递、中国联通、创维等知名企业都是其客户。<br/>核心优势：</p><p>丰富场景覆盖：支持单聊、群聊、聊天室等多种模式，能应对从简单沟通到高并发互动社区等各类场景。</p><p>多元消息类型：涵盖文本、语音、音视频、文件以及自定义消息，具备离线存储、撤回、多端同步、已读回执等完备功能。</p><p>灵活部署方式：提供公有云、私有云及混合云三种部署选择，满足不同企业的合规与架构要求。</p><p>网络与安全保障：自研私有通信协议，结合智能重连和多厂商推送集成，确保消息准确送达。借助WE - CAN全球智能网络、多重加密以及内容审核机制，保障通信质量与安全合规。</p><p>适用企业：追求快速集成、高稳定性，需要支撑复杂社区互动或开展全球化业务的企业。<br/>WorkPlus<br/>WorkPlus定位为企业级安全协同平台，其核心竞争力在于提供可私有化部署的完整解决方案，将即时通讯与办公应用深度融合，满足组织对数据主权和深度定制的严格要求。<br/>核心亮点：</p><p>功能一体化：除了基础的IM、音视频、文件共享功能外，还内置了移动审批、考勤、智能表单、企业云盘等办公套件，并支持与现有业务系统集成。</p><p>安全可控性强：强调私有化部署，让企业完全掌控数据。采用多重加密技术，全面适配信创环境（国产软硬件），符合特定行业的严格合规要求。</p><p>适用单位：对数据安全、私有化部署及信创兼容性有硬性要求的政府、金融、大型国企等单位。<br/>融云IM (RongCloud)<br/>融云IM提供一站式的即时通讯与实时音视频（RTC）能力，助力开发者高效开发各类通讯应用，以高可靠性、低延迟和出色的跨平台支持著称。<br/>核心特性：</p><p>通信双引擎融合：IM与RTC能力深度融合，适用于社交、协同、教育等多种业务场景。</p><p>协议与网络优化：采用私有二进制协议，结合智能DNS、多链路接入和抗弱网策略，保障复杂网络环境下的良好通信体验。</p><p>全平台支持：SDK覆盖Android、iOS、Web、Windows、macOS、Linux等主流平台，同时提供详细的开发文档和技术支持。</p><p>适用团队：需要同时集成IM与高质量音视频功能，且注重跨平台一致性的开发团队。<br/>Dialogic<br/>Dialogic是一家老牌的通信技术提供商，其SDK专注于为企业和设备制造商提供底层的语音、传真、视频及IM多媒体处理能力，在传统通信系统集成方面优势显著。<br/>核心专长：</p><p>专业技术能力：提供如Brooktrout（传真）、Diva（语音/视频）等垂直领域的SDK，支持SIP、H.323等标准协议。</p><p>灵活编程接口：提供从高层到低层的多种编程接口，满足不同复杂度和控制度的开发需求。</p><p>广泛兼容性：支持Linux、Windows等操作系统，并能与自有硬件产品协同工作。</p><p>适用项目：开发传统呼叫中心、传真服务器、嵌入式通信设备或需要深度定制底层通信协议的项目。<br/>360织语<br/>360织语依托360集团的安全优势，打造以安全为核心竞争力的企业级IM SDK，为企业提供可定制的实时通讯解决方案。<br/>核心价值：</p><p>突出安全特性：在数据传输、身份验证等环节实施多重安全加固，彰显其企业安全背景的优势。</p><p>功能完备齐全：提供单聊、群聊、音视频、文件传输、内容审核以及完整的消息管理功能（撤回、回执、搜索等）。</p><p>高度可定制化：提供灵活的接口，支持企业根据自身业务流程进行定制开发。</p><p>适用企业：对通讯数据安全有极高要求，或处于强监管行业的企业。<br/>小天互连<br/>小天互连专注为政企客户提供私有化部署的IM及协同办公平台，强调安全、合规和业务集成能力。<br/>核心能力：</p><p>精准政企导向：深入了解政务、金融、医疗等行业需求，提供符合其安全和流程规范的解决方案。</p><p>强大平台化能力：在基础通讯功能之上，集成流程审批、日程管理、文档中心等OA功能，支持低代码开发和第三方应用接入。</p><p>私有化数据部署：支持数据本地化部署，确保核心数据不出私域。</p><p>适用组织：政企单位及对私有化、业务系统集成有明确需求的大型组织。<br/>容联·云通讯<br/>容联·云通讯致力于提供高性能、低延迟的通讯云服务，其IM SDK在弱网优化和消息可靠性方面进行了专门设计。<br/>核心优势：</p><p>优化弱网体验：采用二进制协议与压缩策略，结合无DNS设计、自适应网络等机制，提高弱网环境下的通讯成功率。</p><p>可靠消息架构：通过推拉结合的消息架构，确保消息有序、必达，支持阅后即焚、已读回执等特性。</p><p>开发者友好：提供丰富的开发文档和示例代码，降低集成难度。</p><p>适用应用：对消息到达率、弱网环境用户体验有较高要求的移动应用。<br/>环信<br/>环信作为国内较早的云通讯服务商，提供高可靠、低时延、支持高并发的全球化IM云服务，在社交、教育等领域应用广泛。<br/>核心特点：</p><p>高并发处理能力：架构设计针对高并发场景，能够支撑大规模用户同时在线和消息互动。</p><p>先进技术保障：与容联类似，采用二进制协议、无DNS、自适应网络等技术，保障性能与稳定性，支持聊天室等互动场景。</p><p>全球化服务能力：提供全球化的通信云服务，助力应用出海。</p><p>适用应用类型：用户规模增长迅速、有高并发场景或出海需求的社交、直播类应用。<br/>Cisco Jabber<br/>Cisco Jabber是思科统一通信（UC）生态中的核心客户端软件，为企业提供与后端通信系统深度集成的桌面级协作体验。<br/>核心亮点：</p><p>深度生态集成：与Cisco Unified Communications Manager (CUCM) 等后端系统无缝集成，提供企业级语音、视频、会议、状态管理的一体化体验。</p><p>全面功能覆盖：集成了即时消息、高清音视频、Webex会议、语音邮件、桌面共享等丰富功能。</p><p>多平台支持：支持Windows、macOS、iOS、Android等多个平台。</p><p>适用企业：已部署或计划部署思科统一通信基础设施的大型企业，追求内部通信系统的高度集成与统一管理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574840" alt="图片" title="图片"/></p><p>云之讯 UCPaaS<br/>云之讯UCPaaS提供以通讯能力为核心的PaaS平台，其IM SDK注重高性能与可定制性，帮助开发者快速构建场景化通讯应用。<br/>核心特性：</p><p>高性能导向：强调低时延、高并发的处理能力，采用自适应网络策略确保连接效率。</p><p>高度可定制：支持自定义消息类型，满足特定业务场景的通讯需求。</p><p>开发者支持完善：提供完善的文档和代码示例，便于快速集成。</p><p>适用企业类型：寻求稳定、可定制IM能力，并可能同时需要短信、语音等其它CPaaS服务的企业。<br/>企达即时通讯<br/>企达IM SDK面向政企市场，提供以安全可控、私有化部署为特色的即时通讯解决方案。<br/>核心卖点：</p><p>安全私有化部署：主打私有化部署方案，确保所有通讯数据留存在企业内部。</p><p>功能针对性强：提供IM、音视频、群组管理等基础功能，并可根据政企场景进行定制。</p><p>行业适配精准：专注服务政务、金融、医疗等对安全合规要求严格的行业。</p><p>适用政企客户：需要完全内网部署、对数据物理隔离有强制要求的政企客户。<br/>敏信即时通讯<br/>敏信即时通讯聚焦企业级市场，提供安全、稳定的私有化IM解决方案，支持灵活的定制开发。<br/>核心优势：</p><p>自主部署能力：支持私有化部署，让企业完全掌控数据。</p><p>功能可扩展性：在标准IM功能基础上，支持根据企业个性化需求进行功能定制与扩展。</p><p>行业解决方案丰富：针对不同行业提供相应的功能模块和合规建议。</p><p>适用企业：注重数据主权、且需要IM功能与自身业务系统深度结合的中大型企业。<br/>企业选型的核心考量因素<br/>面对众多选择，企业可从以下关键维度进行评估：</p><p>业务需求契合度：明确核心需求是基础文本通讯、高质量音视频、大规模聊天室，还是与OA/ERP深度集成等，根据不同场景选择功能侧重点不同的SDK。</p><p>部署与安全模式：评估公有云、私有云或混合云部署需求。对于对数据安全和合规性要求极高的行业（如政务、金融），优先选择支持私有化部署且通过相关认证的产品。</p><p>技术性能与稳定性：关注消息延迟、丢包率、并发支持上限等指标。可通过POC测试，模拟实际用户规模和网络条件进行验证。</p><p>平台兼容与集成成本：确认SDK是否支持所有目标平台（Web、移动端、桌面端）。评估其API设计、文档完善程度、技术支持力度，这直接影响开发集成效率和长期维护成本。</p><p>可扩展性与定制能力：考虑业务未来发展。SDK是否支持自定义消息类型？架构是否易于扩展？能否满足未来的定制化需求？</p><p>总拥有成本（TCO）：综合计算授权费用、服务器资源、运维人力及定制开发等所有成本。</p><p>未来技术趋势前瞻<br/>IM SDK的发展正与前沿技术深度融合：</p><p>AI集成：智能客服、语音转文字、实时翻译、内容智能审核与摘要将成为标配，大幅提升沟通效率和体验。</p><p>5G与低延迟网络：将催生更高清、更沉浸式的实时音视频应用，如VR/AR远程协作。</p><p>多模态交互：消息形态将从文本、语音、视频拓展到富媒体、交互式卡片、3D内容等。</p><p>边缘计算：通过在网络边缘处理消息路由、音视频转码等任务，进一步降低延迟，减轻中心云压力。</p><p>总结<br/>选择合适的IM SDK是一项具有战略意义的技术决策。融云在公有云场景和功能丰富度方面表现出色；云屋科技、小天互连、企达、敏信等在私有化部署和安全合规方面优势明显；环信、容联在高并发和弱网优化方面有深厚积累；Cisco Jabber是现有思科生态用户的理想选择；Dialogic则满足特定的底层通信集成需求。<br/>建议企业组建跨部门的选型团队，明确需求优先级，对候选产品进行充分调研和测试，从而选出最能推动业务发展、兼顾当下与未来的通讯技术基础。<br/>常见问题解答<br/>Q1：IM SDK如何保障通讯数据的安全？<br/>主流SDK通常采用传输层加密（如TLS）、端到端加密、消息内容安全审核以及严格的身份鉴权机制。对于有超高安全需求的企业，应选择支持私有化部署及国密算法的产品。<br/>Q2：如何评估一个IM SDK的实际性能？<br/>除了参考厂商提供的基准数据，企业应自行进行概念验证（POC）测试。重点测试模拟高并发用户时的消息延迟、送达率、服务端资源消耗，以及在弱网（高丢包、高延迟）环境下的连接稳定性和消息流畅度。<br/>Q3：集成IM SDK的技术难度大吗？<br/>难度因产品而异。目前主流服务商都提供了较为完善的平台化SDK、清晰的API文档、示例代码和集成指南，大大降低了基础功能的接入门槛。但涉及深度UI定制或与复杂业务逻辑对接时，仍需要一定的开发投入。<br/>Q4：选择IM SDK时，最容易忽略的关键点是什么？<br/>企业往往容易忽略运维成本和厂商的长期服务能力。需要了解SDK的日志监控、问题诊断工具是否完善，以及厂商的技术支持响应机制、版本更新频率和路线图，确保其能伴随业务长期稳定发展。</p>]]></description></item><item>    <title><![CDATA[2026年教育项目管理系统，研发协同必备的8大核心工具 3Q聊工具 ]]></title>    <link>https://segmentfault.com/a/1190000047574844</link>    <guid>https://segmentfault.com/a/1190000047574844</guid>    <pubDate>2026-01-27 12:06:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在教育数字化转型加速的2026年，教育项目研发呈现跨团队、多场景、高迭代的特点，从课程系统开发到教学工具迭代，从科研项目推进到校企协同创新，都离不开高效的研发协同工具支撑。优质的工具能打通需求、开发、测试、交付全链路，破解教育研发中“跨部门协同不畅、进度管控模糊、知识沉淀不足”等痛点。以下梳理8大核心工具，涵盖项目管控、文档协作、沟通协同等关键场景。</p><h2>二、教育研发协同核心工具盘点</h2><h3>（一）禅道</h3><ul><li>​<strong>产品介绍</strong>​：国内开源敏捷项目管理工具，以“需求-任务-缺陷”全流程闭环管理为核心，支持敏捷、瀑布等多种研发模式，具备轻量化部署与高度自定义特性，适配中小团队到大型组织的不同需求。</li><li>​<strong>适用场景</strong>​：K12教育系统研发、高校科研项目管控、教育APP迭代升级、教学资源库搭建等场景，尤其适合需要兼顾流程规范与灵活调整的教育研发项目。</li><li>​<strong>功能深度</strong>​：核心覆盖需求池管理、迭代规划、任务拆解与分配、缺陷追踪、工时统计、报表可视化等功能，支持自定义工作流与字段配置，可对接代码仓库、测试工具形成协同链路，开源版本满足基础需求，企业版提供私有化部署与权限精细化管控。</li><li>​<strong>适用行业</strong>​：基础教育科技企业、高等院校科研团队、职业教育数字化研发机构、教育信息化解决方案提供商。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl902" alt="" title=""/></p><h3>（二）Jira</h3><ul><li>​<strong>产品介绍</strong>​：海外主流敏捷项目管理工具，以强大的流程配置能力与插件生态著称，专注于研发全生命周期管理，可实现多角色、多项目的协同管控。</li><li>​<strong>适用场景</strong>​：大型教育集团跨区域研发协同、复杂教学平台定制开发、教育科技企业全球化项目推进、多团队并行的研发任务管控。</li><li>​<strong>功能深度</strong>​：支持Scrum、Kanban等敏捷框架，可自定义任务状态、字段与审批流程，具备缺陷管理、迭代跟踪、燃尽图分析等核心能力，通过插件生态可拓展CI/CD集成、效能度量、跨工具联动等功能，适配复杂研发场景的个性化需求。</li><li>​<strong>适用行业</strong>​：大型教育科技集团、跨国教育信息化企业、高校国家级科研项目团队、教育硬件与软件融合研发机构。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl909" alt="" title="" loading="lazy"/></p><h3>（三）Confluence</h3><ul><li>​<strong>产品介绍</strong>​：专注于团队知识管理与文档协同的工具，常与Jira联动形成“项目管理+知识沉淀”闭环，支持多人实时编辑、文档版本管控与结构化存储。</li><li>​<strong>适用场景</strong>​：教育研发文档协作、教学方案共创、技术手册编写、项目复盘沉淀、校企协同知识库搭建等场景，尤其适合注重知识传承的研发团队。</li><li>​<strong>功能深度</strong>​：提供丰富的文档模板、空间权限管控、评论互动与历史版本回溯功能，支持嵌入表格、图表、附件及第三方工具链接，可构建分层级的知识库体系，实现研发文档的规范化管理与高效检索，保障团队信息同步的准确性。</li><li>​<strong>适用行业</strong>​：高等院校科研团队、教育科技企业研发部门、职业教育课程研发机构、教育信息化标准制定团队。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnMxU" alt="" title="" loading="lazy"/></p><h3>（四）TAPD（腾讯敏捷产品研发平台）</h3><ul><li>​<strong>产品介绍</strong>​：腾讯推出的一站式敏捷研发协同平台，融合需求管理、任务调度、缺陷追踪、文档协作等功能，具备轻量化上手与生态集成优势。</li><li>​<strong>适用场景</strong>​：中小型教育科技企业研发项目、教育APP快速迭代、教学小程序开发、跨部门轻量化协同任务管控。</li><li>​<strong>功能深度</strong>​：支持敏捷冲刺规划、任务拆解与优先级排序，内置缺陷管理流程与测试用例管理模块，提供可视化报表与数据统计功能，可与腾讯系工具及主流研发工具集成，兼顾流程规范与易用性，适合快速落地研发协同体系。</li><li>​<strong>适用行业</strong>​：中小型教育科技公司、教育创业团队、高校创新创业项目组、区域性教育信息化服务商。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdl91c" alt="" title="" loading="lazy"/></p><h3>（五）Wrike</h3><ul><li>​<strong>产品介绍</strong>​：云端项目管理与协同平台，以多视图可视化、跨团队协作与自动化流程为核心优势，适配灵活多变的研发场景。</li><li>​<strong>适用场景</strong>​：教育研发项目全流程管控、跨部门协同任务推进、多项目并行管理、研发进度可视化追踪，适合需要快速调整优先级的项目。</li><li>​<strong>功能深度</strong>​：提供列表、看板、甘特图等多维度项目视图，支持任务依赖设置、自动化工作流配置、资源分配与工时统计，可实现跨团队成员的实时协作与进度同步，具备一定的可扩展能力，能伴随团队规模增长适配复杂需求。</li><li>​<strong>适用行业</strong>​：教育科技初创企业、跨区域协作的教育研发团队、教育营销与技术融合项目、中小型在线教育平台研发。</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdmdGj" alt="" title="" loading="lazy"/></p><h3>（六）Slack</h3><ul><li>​<strong>产品介绍</strong>​：以频道为核心的即时通讯与协作工具，打破传统沟通壁垒，实现“沟通-工具-任务”的一体化协同，支持多第三方工具集成。</li><li>​<strong>适用场景</strong>​：教育研发团队实时沟通、跨地域协同讨论、研发任务进度同步、紧急问题响应，尤其适合分布式研发团队。</li><li>​<strong>功能深度</strong>​：可按项目、部门创建专属频道，支持消息线程讨论、文件共享、语音视频会议等功能，核心优势在于与研发工具的联动能力，能将任务提醒、缺陷通知、进度更新等同步至频道，减少工具切换成本，保持团队沟通的高效与聚焦。</li><li>​<strong>适用行业</strong>​：跨国教育科技企业、分布式教育研发团队、校企联合研发项目组、多角色协同的教育信息化项目。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnMx8" alt="" title="" loading="lazy"/></p><h3>（七）有道云协作</h3><ul><li>​<strong>产品介绍</strong>​：国内轻量化团队协作与文档管理工具，以“文档为核心”整合任务管理、文件共享等功能，具备易上手、多终端同步特性。</li><li>​<strong>适用场景</strong>​：教育研发轻量任务协同、文档共创、教学资源整理、小型项目进度追踪，适合对工具复杂度要求低的团队。</li><li>​<strong>功能深度</strong>​：支持多人实时在线编辑、文档版本管理、权限精细化控制，内置基础任务分配与进度追踪功能，可实现文档与任务的关联管理，界面简洁直观，学习成本低，支持本地文件同步与云端存储，满足基础研发协同需求。</li><li>​<strong>适用行业</strong>​：中小学教育信息化研发团队、小型教育创业公司、高校课程研发小组、区域性教育资源开发机构。</li></ul><p><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnMx9" alt="" title="" loading="lazy"/></p><h3>（八）戴西iDWS数智化研发平台</h3><ul><li>​<strong>产品介绍</strong>​：国产化数智化研发平台，聚焦复杂工程研发场景，融合AI智能辅助、算力调度、许可管理与数据治理功能，支持私有化部署与国产化适配。</li><li>​<strong>适用场景</strong>​：大型教育装备研发、复杂教学系统定制、教育AI算法研发、高安全需求的教育信息化项目，适合对研发效能与数据安全要求高的团队。</li><li>​<strong>功能深度</strong>​：内置NexAI智能体，可在研发全流程提供智能建议与异常识别，具备算力调度、许可资源优化、研发数据统一纳管等核心能力，支持多学科协同研发与7×24小时不间断任务监控，强化研发过程的智能化与工程化管控，适配国产化信创需求。</li><li>​<strong>适用行业</strong>​：大型教育科技集团、教育装备研发企业、高校AI教育研发团队、有国产化需求的教育信息化服务商。</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdnMya" alt="image.png" title="image.png" loading="lazy"/></p><p>教育研发协同工具的选型需结合团队规模、项目复杂度与行业特性，核心是实现“流程规范化、协作高效化、知识体系化”。2026年，随着教育科技与AI技术的深度融合，工具的智能化、国产化与生态化将成为主流趋势，研发团队可根据自身需求组合适配，构建专属协同体系，赋能教育项目高质量落地。</p>]]></description></item><item>    <title><![CDATA[地平线 征程 6 工具链入门教程 | 板端部署 UCP 使用指南 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047574851</link>    <guid>https://segmentfault.com/a/1190000047574851</guid>    <pubDate>2026-01-27 12:06:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.前言</h2><p>在模型板端部署过程中，开发者主要关心图像如何获取，模型性能如何评测以及如何优化模型等问题。对于图像的获取，地平线提供了 Pyramid 硬件，其不但可以获取多尺寸图像，且利用内存共享机制可将内存给到 BPU 直接进行推理。针对耗时，内存占用，DDR 带宽占用等指标进行评测和优化，地平线提供了诸如 Trace，hrt\_ucp\_monitor 等一系列性能分析工具用于性能监测，使得开发者能够清晰掌握模型运行时的资源占用和硬件效率。最后，地平线提供 VP，HPL 以及 DSP 多种模块用于前后处理环节的算法开发。本文将结合实例说明模型如何进行部署，性能分析以及常见的问题解析。</p><h2>2.UCP 简介</h2><p>征程 6 工具链在应用部署端新引入了统一计算平台（Unify Compute Platform，以下简称 UCP）。UCP 面向应用层，属于嵌入式应用开发（runtime）范畴，提供视觉处理（Vision Process，以下简称 VP）、模型推理（Neural Network，以下简称 NN）、高性能计算库（High Performance Library，以下简称 HPL）等功能。</p><p>UCP 还定义了一套统一的异构编程接口，支持对 SoC 上各后端硬件资源的调用，包括 BPU、DSP、ISP、GDC、STITCH、JPU、VPU、PYRAMID 等，以完成 SoC 上任务的统一调度。</p><p>UCP 的架构图如下所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574853" alt="MTI4MFgxMjgwICgxKQ==.png" title="MTI4MFgxMjgwICgxKQ==.png"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574854" alt="1.png" title="1.png" loading="lazy"/></p><h2>3.模型推理</h2><h3>3.1 快速上手</h3><p>以下面的代码为例，说明 DNN 和 UCP 接口的使用方式，整体包含 5 个主要步骤，详细信息可参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=0U8EHZ2etHQgK8aSeccxUQ%3D%3D.4hFlTwScnZNfhr61ZQiCmrMmz2ZshbD0Ta3IveVkFYp2YSwRIQNkiPUcOEYxZtkUi7TR73m0xv3kG8dLUszB6g%3D%3D" rel="nofollow" target="_blank">统一计算平台-模型推理开发</a>&lt;/u&gt;》，《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=4RpEMvY100JtmQ8TByKr0A%3D%3D.rZpOTbZiY6asZ6cbCp8446y5mOAMnJji2kI2c4vpZ5Vd7hLhhPSo0oq%2BCGpVyFRlf%2BKx20ucUSK%2FU2V067ZWpWPSOC5AgvHryEQE%2F0%2FB6AxCfGpcj4P1jHmmIKqUjfXp523SNe421wfRQewo9zlqMS2YiyGdxDgSVQkXadAEFKUk%2BBnIPm1yJ%2FnFzrce%2FHNJ" rel="nofollow" target="_blank">模型部署实践指导-模型部署实践指导实例</a>&lt;/u&gt;》，《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=zwoacizaYBb25gFcaz4Oow%3D%3D.xY4SJL50vxLnx7tfakRmXquTGWd7v3%2F6kIJ5CPUS%2B1bN8FvOLgpss9qO48paFvfXqCp01TzZyRYpQm49gjSnI67dYf5UTL41tQv1mF0c8DE%3D" rel="nofollow" target="_blank">UCP 通用 API 介绍</a>&lt;/u&gt;》等相关章节：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574855" alt="2.png" title="2.png" loading="lazy"/></p><pre><code class="Plain">int main(int argc, char **argv) {
​  ...    // 解析命令行参数
​  hbDNNPackedHandle_t packed_dnn_handle;
​  hbDNNHandle_t dnn_handle;
​  const char **model_name_list;
​  auto modelFileName = FLAGS_model_file.c_str();
​  int model_count = 0;
​  
​  //1. 加载模型并获取模型名称列表以及Handle
​  {
​    hbDNNInitializeFromFiles(&amp;packed_dnn_handle, &amp;modelFileName, 1);
​    hbDNNGetModelNameList(&amp;model_name_list, &amp;model_count, packed_dnn_handle);
​    hbDNNGetModelHandle(&amp;dnn_handle, packed_dnn_handle, model_name_list[0]);
​  }

​  std::vector&lt;hbDNNTensor&gt; input_tensors;
​  std::vector&lt;hbDNNTensor&gt; output_tensors;
​  int input_count = 0;
​  int output_count = 0;
​  
​  //2. 根据模型的输入输出准备张量
​  {
​    hbDNNGetInputCount(&amp;input_count, dnn_handle)；
​    hbDNNGetOutputCount(&amp;output_count, dnn_handle)；
​    input_tensors.resize(input_count);
​    output_tensors.resize(output_count);
​    prepare_tensor(input_tensors.data(), output_tensors.data(), dnn_handle);
​  }
​  //3. 准备输入数据并填入到对应的张量中
​  read_image_2_tensor_as_nv12(FLAGS_image_file, input_tensors.data())；
​  // 确保更新输入后进行Flush操作以确保BPU使用正确的数据
​  for (int i = 0; i &lt; input_count; i++) {
​      hbUCPMemFlush(&amp;input_tensors[i].sysMem[0], HB_SYS_MEM_CACHE_CLEAN);
​    }
​    
​  //4. 创建任务并进行推理
​  hbUCPTaskHandle_t task_handle{nullptr};
​  hbDNNTensor *output = output_tensors.data();
​  {

​    hbDNNInferV2(&amp;task_handle, output, input_tensors.data(), dnn_handle);
​    hbUCPSchedParam ctrl_param;
​    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;ctrl_param);
​    ctrl_param.backend = HB_UCP_BPU_CORE_ANY;
​    hbUCPSubmitTask(task_handle, &amp;ctrl_param);
​    hbUCPWaitTaskDone(task_handle, 0);
​  }
​    //5. 处理输出数据
​  for (int i = 0; i &lt; output_count; i++) {
​    hbUCPMemFlush(&amp;output_tensors[i].sysMem[0], HB_SYS_MEM_CACHE_INVALIDATE);
​  }

​  //6: 释放资源
​  {
​  
​    hbUCPReleaseTask(task_handle);
​    for (int i = 0; i &lt; input_count; i++) {
​      hbUCPFree(&amp;(input_tensors[i].sysMem[0]));
​    }
​    for (int i = 0; i &lt; output_count; i++) {
​      hbUCPFree(&amp;(output_tensors[i].sysMem[0]));
​    }
​    // 释放模型
​    hbDNNRelease(packed_dnn_handle);
​  }

​  return 0;
}</code></pre><blockquote><p>⚠️ 上面的例子仅为 demo，实际使用时，需要注意以下几点：</p><ol><li>图像可以直接从 Pyramid 接口直接获取 nv12 的输出，无需进行拷贝，可直接传递给 BPU 进行推理</li><li>输入输出内存的大小和对齐 stride，详见第 5.3 节说明</li><li>接口进行返回值检查，以保证函数的正确执行</li></ol></blockquote><h3>3.2 实用技巧</h3><h5>3.2.1 添加 desc</h5><p>有的时候，为了方便自动化作业，需要给不同的模型，输入和输出打上标签以区分他们。</p><blockquote>需要注意的是，如果是为输入添加描述信息，由于 pyramid 和 resizer 节点会改变 bc 的输入节点数，因此需要给对应每个节点都添加对应的信息。</blockquote><p>比较推荐的做法是在 compile 之前再添加：</p><pre><code class="Plain">from hbdk4.compiler import load
quantized_bc = load("xxx.bc")
func = quantized_bc[0]
func.desc = "xxx model" #模型的描述
func.inputs[0].desc = "xxx input" #模型输入的描述
func.outputs[0].desc = "xxx output" #模型输出的描述</code></pre><p>模型部署时，通过下面的接口来获取描述信息：</p><pre><code class="Plain">//模型的描述信息
int32_t hbDNNGetModelDesc(char const **desc, uint32_t *size, int32_t *type,
​                          hbDNNHandle_t dnnHandle);
//输入的描述信息
int32_t hbDNNGetInputDesc(char const **desc, uint32_t *size, int32_t *type,
​                          hbDNNHandle_t dnnHandle, int32_t inputIndex);
//输出的描述信息
int32_t hbDNNGetOutputDesc(char const **desc, uint32_t *size, int32_t *type,
​                           hbDNNHandle_t dnnHandle, int32_t outputIndex);</code></pre><h5>3.2.2 模型打包</h5><p>模型打包功能，可以将多个模型打包进一个 hbm 文件中，对于共享任务可以节省模型的空间，具体 api 介绍可见《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=rPm4gUsePyxxR213UTg%2Brw%3D%3D.o%2B5gwKjNZ7rkTBwMf2csNq2gVDSK%2BtSp8fzBfqSr5KrUkIzZlEF6tuB5TBxrh7HurdSxpB8rqV4cq3zoMgJKzi95kB5aMrqnIijc6zuG0U6dUJ3sA308hWeRfU5m7U6YUla5IswvoYEtyz6oDvlV9CRrGz1NbsAxjRXirvYl3RcIXRP6wxb6Kt%2F5Js0Gl024" rel="nofollow" target="_blank">HBDK Tool API Reference</a>&lt;/u&gt;》：</p><pre><code class="Plain">from horizon_plugin_pytorch.quantization.hbdk4 import export
from hbdk4.compiler import load, convert, compile, link
# export 阶段记得配置 name
qat_bcA = export(qat_model_A, example_input, name="backbone_head1_head2")
quantized_modelA = convert(qat_bcA, "nash-m")
# 注意：此时compile生成的模型后缀名为.hbo
hbo_nameA = "nameA_compiled.hbo"
hboA = compile(quantized_modelA, path=hbo_nameA, march="nash-m")

qat_bcB = export(qat_model_B, example_input, name="backbone_head1")
quantized_modelB = convert(qat_bcB, "nash-m")
hbo_nameB = "nameB_compiled.hbo"
hboB = compile(quantized_modelB, path=hbo_nameB, march="nash-m", opt=2)

# link生成打包模型，后缀名为.hbm
hbm_name = "compiled.hbm"
hbm = link([hboA, hboB], hbm_name)</code></pre><p>在生成 hbm 文件后，上板运行使用 hrt\_model\_exec 查看模型可以看到：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574856" alt="4.png" title="4.png" loading="lazy"/></p><p>推理测试时，用 model\_file 指定 hbm 路径，model\_name 指定具体哪一个模型<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574857" alt="5.png" title="5.png" loading="lazy"/></p><h5>3.2.3 小模型批处理</h5><p>由于 BPU 为资源独占型硬件，对于那些耗时较短的小模型，其框架调度耗时开销可能大于其模型运行时间，为了缓解这个问题。在征程 6 平台，UCP 支持通过复用 task\_handle 方式来一次将多个模型下发，全部执行完成后再一次性返回，从而将 N 次开销合并为一次：</p><pre><code class="Plain">// 获取模型指针并存储
std::vector&lt;hbDNNHandle_t&gt; model_handles;

// 准备各个模型的输入输出，准备过程省略
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; inputs;
std::vector&lt;std::vector&lt;hbDNNTensor&gt;&gt; outputs;

// 创建任务并进行推理
{
    // 创建并添加任务，复用task_handle
    hbUCPTaskHandle_t task_handle{nullptr};
    for(size_t task_id{0U}; task_id &lt; inputs.size(); task_id++){
        hbDNNInferV2(&amp;task_handle, outputs[task_id].data(), inputs[task_id].data(), model_handles[i]);
    }
    
    // 提交任务
    hbUCPSchedParam sche_param;
    HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sche_param);
    sche_param.backend = HB_UCP_BPU_CORE_ANY;
    hbUCPSubmitTask(task_handle, &amp;sche_param);
    
    // 等待任务完成
    hbUCPWaitTaskDone(task_handle, 0);
}</code></pre><h5>3.2.4 优先级抢占</h5><p>在征程 6 计算平台上，BPU 硬件本身没有抢占功能，对于一个计算任务其一旦进入 BPU 后，就无法被打断，其他计算任务只能等待当前计算任务完成退出后才能运行。</p><p>此时很容易出现 BPU 计算资源被一个大模型任务独占，进而影响其他高优先级模型任务的执行，针对这个问题，工具链采用 cpu 调度的机制来优化 BPU 资源：</p><ol><li>hbm 模型在 BPU 推理表现为一个或多个 function-call，function-call 为 BPU 最小的执行单元。当一个模型的所有 function-call 都执行完成时，这个模型也就执行完成了</li><li>BPU 模型任务抢占粒度设计为 function-all，如果一个模型只有一个 function-call 那么其无法被抢占，如果一个模型有多个 function-call 可能出现这个模型完成部分 function-call 后，BPU 挂起当前模型，然后切换执行其他模型</li></ol><p>UCP 支持任务优先级调度和抢占，可通过 hbUCPSchedParam 结构体进行配置：</p><pre><code class="Plain">typedef struct hbUCPSchedParam {int32_t priority;int64_t customId;uint64_t backend;uint32_t deviceId;} hbUCPSchedParam;</code></pre><ul><li><p>priority：任务优先级，支持[0， 255]之间的数值，对于模型任务而言：</p><ul><li>[0， 253]普通优先级，不可抢占其他任务，但在未执行时支持按优先级进行排队</li><li>254：为 high 优先级，支持抢占普通任务</li><li>255：为 urgent 优先级，支持抢占普通任务和 high 任务</li><li>可被中断抢占的任务，需要在模型编译阶段配置 max\_time\_per\_fc 进行模型拆分</li></ul></li><li>customId：自定义优先级</li><li>backend：任务硬件 id</li><li>deviceId：设备 ID 比如，有下面的两个模型，一个单线程耗时 20.9 ms，一个单线程耗时 8.3ms：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574858" alt="6.png" title="6.png" loading="lazy"/></li></ul><p>让这两个模型同时运行，且设置 max\_time\_per\_fc=2000，两个模型的优先级均为普通优先级时 UCP trace 耗时如下：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574859" alt="7.png" title="7.png" loading="lazy"/></p><p>当将模型 2 的优先级设为 high，模型 1 仍为普通优先级时：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574860" alt="8.png" title="8.png" loading="lazy"/></p><p>可以看到，在下面的模型一次 infer 过程中，模型被切分为多个 2ms 运行的 function-call 运行，中间插入了很多 high 优先级模型，导致一次模型前向耗时大大增加。</p><h5>3.2.5 LRU 内存优化</h5><p>LRU（Least Recently Used）算法是用于优化内存页的调度算法。BPU 内存在 BPU 实际使用前，NN 模块内部需要对该块内存进行特殊处理才能够正常使用，如果频繁对模型及其依赖申请释放会导致 CPU 负载变大，从而可能会引发性能问题。 如果确实有频繁申请释放的需求，推理库提供了内存 LRU 缓存功能，通过设置环境变量 <code>HB_NN_ENABLE_MEM_LRU_CACHE</code> 为 <code> </code>​<code>true</code> 来使用。设置方式如下：</p><pre><code class="Plain">export HB_NN_ENABLE_MEM_LRU_CACHE=true</code></pre><p>开启了这个功能之后，对模型的输入输出不是实时申请和释放的，会在一开始就申请好并进行循环复用。所以如果用户在模型跑完推理后就立刻执行内存释放操作，实际不会立刻释放，UCP 这一层会等一段时间后才执行（默认至少 1s），所以可能会有内存泄漏的风险，建议是模型推理的内存块不要释放，且模型每次输入输出的虚拟地址是复用的。</p><h3>3.3 输入输出处理</h3><h5>3.3.1 Crop 裁剪</h5><p>Crop 主要思想是利用地址偏移，并通过 stride 将图像多余的部分进行屏蔽从而送入准备好的模型输入。这种 Crop 方式不引入 memory copy，减少 IO 开销。</p><p>限制：</p><ol><li>图像输入大小要大于模型实际输入大小，w\_stride 要 32（E/M）/64（P/H）字节对齐</li><li>模型的 validShape 为固定值，stride 为动态值</li><li>裁剪偏移的输入首地址要 32 对齐</li></ol><p>详细示例可以参考《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=mH0GenqnoS8t2UF9pI5WAw%3D%3D.Xs48DxLgDJyJHBv7sXnHIioafIHhinwDdciXOh25xEkvBg1h0v49aGktztO9053maeDpprca%2BcJGEhvypY71vw%3D%3D" rel="nofollow" target="_blank">基础示例包使用说明</a>&lt;/u&gt;》中 advanced\_samples 的 crop 示例</p><h5>3.3.2 Resizer</h5><p>Resizer 主要是指具有 nv12 图像输入和 ROI 输入的模型，编译器支持通过 JIT 动态指令的方式，从 nv12 图像上完成抠图 +Resize 功能。其不仅仅是图像 stride 为动态，输入的 H，W 也为动态，w\_stride 也同样需要满足 32（E/M）/64（P/H）字节对齐，roi 不需要进行对齐：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574861" alt="9.png" title="9.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574862" alt="10.png" title="10.png" loading="lazy"/></p><h5>3.3.3 图像 tensor 对齐</h5><p>在征程 6 芯片，有一块叫 Pyramid 的金字塔硬件处理模块，可提供 Camera 输入图像的缩放及 ROI 抠图能力，其输出为 nv12 类型的图像数据，并可基于共享内存机制直接给到 BPU 进行模型推理，因此在征程 6 工具链中：</p><ul><li>Pyramid 模型是指具有 nv12 图像输入的模型</li><li>Resizer 模型指的是具有 nv12 图像输入和 ROI 输入的模型，编译器支持通过 JIT 动态指令的方式，从 nv12 图像上完成 ROI 抠图 +Resize 功能 征程 6P/H 要求 nv12 stride 满足 64 对齐，征程 6E/M/B 是 32 对齐。 Pyramid 的输入 stride 为动态，比如模型输入为 224x224 的 nv12 图像，其格式为：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574863" alt="11.png" title="11.png" loading="lazy"/></li></ul><p>其中，-1 为占位符，表示为动态，Pyramid 输入的 stride 为动态。那么此时我们就需要通过手动计算方式来获取了：</p><pre><code class="Plain">#define ALIGN_SIZE(size,align_byte) (((size)+(align_byte-1))&amp;~(align_byte-1))
HBDNNTensor* input;
auto dim_len = input[i].properties.validShape.numDimensions;
for(int dim_i = dim_len-1;dim_i&gt;=0;dim_i--){
​    if(input[i].properties.stride[dim_i]==-1){
​        auto cur_stride = input[i].properties.stride[dim_i+1] * 
​            input[i].properties.validShape.dimensionSize[dim_i+1];
​        input[i].properties.stride[dim_i] = ALIGN_SIZE(cur_stride,NUM);
​    }  
}
int input_memSize = input[i].properties.stride[0] * input[i].properties.validShape.dimensinoSize[0];</code></pre><p>对于非 nv12 类型的其他输入，以 rgb 输入 <code>input</code> 作为例子，1x224x224x3 的 rgb 图像如下所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574864" alt="12.png" title="12.png" loading="lazy"/></p><p>输入申请的大小可以通过 aligned byte size 来获取：</p><pre><code class="Plain">int input_memSize = input[i].properties.alignedByteSize;</code></pre><h5>3.3.4 内存单元对齐</h5><p>BPU 中的内存单元也是遵循向量化对齐的原则，类似于 avx/neon 等，需要内存对齐。所以对于不满足对齐最小字节的内存要被强制对齐到最小的内存字节上。</p><p>征程 6H/P tensor 最小申请内存是 256 字节，征程 6E/M 是 64 字节，征程 6B 是 128 字节，这个差异会体现在模型的 aligned byte size 和 stride 属性上。</p><p>举个例子：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574865" alt="13.png" title="13.png" loading="lazy"/></p><p>上面模型的 stride=4000，<code>output</code> 需要申请的内存为 4000Byte，但由于内存需要对齐，所以实际上的需要申请的内存大小为（（4000+（256-1））&amp;～（256-1））=4096Byte。 在模型实际部署中，非图像输入/输出节点所需申请的内存大小均可以从模型节点属性的结构体中读取到，因此无需特别关注：</p><pre><code class="Plain">hbDNNTensor* output;
int output_memSize = output[i].properties.alignedByteSize;</code></pre><h5>3.3.5 padding</h5><p>由于内存单元对齐的影响，feature 申请的大小和拷贝需要根据 stride 和 alignedByteSize 来进行。用户侧需要手动处理这些 padding，可能对前处理和后处理的代码有较大的变动。这里地平线提供了一种优化方案：input\_no\_padding/ouput\_no\_padding，在开启这两个选项后，可以直接将输入/出实际大小的内存送入接口，接口内部会自行处理对齐，无需用户侧修改代码。但开启这个参数后，可能会对模型延时产生微小影响。</p><ul><li>input\_no\_padding：对所有非图像的输入去 padding</li><li>output\_no\_padding：对模型所有的输出去 padding 若编译时配置了 input\_no\_padding=True，output\_no\_padding=True，无需关注非图像的对齐问题：</li></ul><pre><code class="Plain">#PTQ配置方式，在yml中
compiler_parameters:
    extra_params: {"input_no_padding": True, "output_no_padding": True}

#QAT配置方式
from hbdk4.compiler import compile
compile(quantized_bc,march,path,input_no_padding=True,output_no_padding=True)</code></pre><p>举个例子，比如一个模型的输出 shape 为 1x21x21x255，其 output\_no\_padding=False 和 output\_no\_padding=True 的结果如下图所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574866" alt="14.png" title="14.png" loading="lazy"/></p><h2>4.性能分析</h2><h3>4.1 模型性能分析</h3><p>如果开发者没有实体板子，只有 hbm 模型，可以使用 hbdk4 中的 hbm\_perf 接口获取静态性能评估文件（html，json 格式）以及模型耗时：</p><pre><code class="Plain">from hbdk4.compiler import hbm_perf
hbm_perf(xxx.hbm)</code></pre><blockquote><p>模型中如果有 CPU 算子，则会影响 perf 的结果，建议去除 CPU 算子之后再进行分析。CPU 算子一般可以通过以下两种方式查看到：</p><ol><li>convert 之后的模型可视化，然后查询是否有 hbtl 类型算子</li><li>利用 statistics 接口统计 bc 模型算子类型</li></ol></blockquote><p>如果有与开发环境直连的板子可以使用下面的方式进行测试，与实测偏差会更小：</p><pre><code class="Plain">from hbdk4.compiler import hbm_perf
hbm_perf(xxx.hbm,remote_ip="xxx")</code></pre><p>或按照用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=AwJ0Kegnh%2Fgrs40xRlO8MA%3D%3D.RNfeKUNc30R3FOufB1lMTPE1tw7tqPVQtJmjl47qQJ36j9Ka3v%2B%2F%2B%2F7%2FA6aR5Tb77DQFjOZPn2Tjcp3VKIlheiuFa%2FMOiEODghrH%2FWmI04h51%2BSPuhfoejYN9zxbqq8N" rel="nofollow" target="_blank">统一计算平台-模型推理工具介绍</a>&lt;/u&gt;》使用 hrt\_model\_exec 工具在板端进行性能测试：</p><pre><code class="Plain">hrt_model_exec perf --model_file=xxx.hbm --frame_count=200</code></pre><h5>4.1.1 带宽占用</h5><p>静态评测时，带宽信息可以从模型编译过程中生成的 xxx.html/xxx.json 中文件获取，在 ptq 中会自动生成这两个文件，在 qat 中，可以通过生成 hbm 模型后，使用 hbm\_perf 接口来生成这两个文件。</p><p><strong>平均带宽</strong></p><p>平均带宽（GB/s） = DDR bytes per second（ for n FPS）/n * 设计帧率/2^30，以下面的模型为例，实际需求帧率为 30FPS，那么该模型所需的平均带宽为：12293553099/57.12 * 30/2^30 = 6.01GB/s：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574867" alt="15.png" title="15.png" loading="lazy"/></p><p><strong>峰值带宽</strong></p><p>峰值带宽可以通过推理带宽柱状图来进行分析，最高的柱子即最大的 load/strore 带宽。比如下面这个图，该模型的最大 load 需求为 15515MB/s=15.15GB/s，最大的 store 需求为 13125MB/s=12.82GB/s，最大的 load+store 需求为 11954+11812=23766MB/s=23.21GB/s<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574868" alt="16.png" title="16.png" loading="lazy"/></p><h5>4.1.2 带宽优化</h5><p>在实际应用中，模型的推理耗时可能出现比正常评测要更长的现象，主要原因往往来源于 BPU 的等待耗时以及带宽资源不足的影响。这里主要针对带宽问题进行说明。</p><p>BPU 模型的带宽消耗主要集中在模型加载、推理时的 featuremap 读写，输出写回，优化策略如下：</p><ol><li>使用 balance 参数来平衡带宽和延时</li></ol><pre><code class="Plain">compile(balance=x) # 0=优先ddr优化，100=优先延迟优化，默认balance=100,推荐balance=2</code></pre><p>ptq 时，修改配置文件中的 compile\_mode:</p><pre><code class="Plain">compile_mode: 'balance'
balance_factor: 2</code></pre><ol start="2"><li>对于小模型使用多 batch 推理模式，可以减少 weight 的加载次数</li><li>减少模型抢占调用：优先级 255 的抢占任务会刷新整个 SRAM，导致大量带宽开销，建议通过任务编排方式运行模型，而不是优先级抢占</li><li>Batch 拆分：若模型需要 concat 多路输入（比如 BEV 类模型），将 batch mode 拆分，每一路单独提取特征，牺牲很少的延时来降低峰值带宽</li></ol><h5>4.1.3 内存占用</h5><p>模型所需的内存可以通过 Summary 查看到：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574869" alt="17.png" title="17.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574870" alt="18.png" title="18.png" loading="lazy"/></p><blockquote><p>Shared temporary memory 共享临时内存，主要目的是用于相同优先级模型共享内存，优化模型推理内存的使用。对于相同优先级的模型，会共享 temporary memory。该功能的约束条件：</p><ol><li>跨 BPU Core 不可用</li><li>跨优先级不可用，0-253 的优先级之间的都可以共享，254 只能和其他 254 共享，255 只能与其他 255 共享</li><li>跨进程不可用</li></ol></blockquote><p>当开发人员对模型运行时所需内存进行评测时，可先通过 Summary 的内容先进行静态数据评估，模型的内存占用=Static Memory + Dynamic Memory。</p><h3>4.2 动态性能分析</h3><p>在模型的部署和运行过程中，我们比较关注模型的推理耗时，bpu/cpu 占用，DDR 读写带宽以及内存占用。这些信息可以通过以下工具来获取：</p><h5>4.2.1 hrt\_model\_exec</h5><p>hrt\_model\_exec 是一个模型执行工具，可直接用于在开发板上评测模型的推理性能，获取模型信息。工具源码路径在 samples/ucp\_tutorial/tools/hrt\_model\_exec。</p><p>模型输入输出信息：</p><pre><code class="Plain">hrt_model_exec model_info --model_file xxx.hbm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574871" alt="19.png" title="19.png" loading="lazy"/></p><p>模型单线程耗时：</p><pre><code class="Plain">hrt_model_exec perf --model_file xxx.hbm --frame_count 1000 --thread_num 1</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574872" alt="20.png" title="20.png" loading="lazy"/></p><p>模型多线程耗时：</p><pre><code class="Plain">hrt_model_exec perf --model_file xxx.hbm --frame_count 1000 --thread_num 4</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574873" alt="21.png" title="21.png" loading="lazy"/></p><p>指定优先级运行：</p><pre><code class="Plain">hrt_model_exec perf --model_file xxx.hbm --frame_count 1000 --thread_num 1 --task_priority 1</code></pre><p>更多的 hrt\_model\_exec 命令可以在《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=QS3qpLz0qLYCng4xoeCTfg%3D%3D.s3P1vrtEYyfIhUbvBpXAJmkMIj6AfahqqOFhhA4RNnOUVaBB2aHgClUp6oTWFs5QLzWE4kMHBB9Xge8yQf%2FUtky1yQ8bWAAx6fP8onBRx%2BWYYqQkxAIRByfcZ%2Br%2BOHV9" rel="nofollow" target="_blank">统一计算平台-模型推理工具介绍-hrt\_model\_exec</a>&lt;/u&gt;》中查看。</p><h6>4.2.1.1 单线程和多线程差异</h6><p>在单线程下，工具按照单核单线程的串行逻辑运行，统计的性能可以理解为单帧处理的平均时间（包括调度开销，BPU 执行时间以及 CPU 执行时间）。</p><p>在多线程下，工具会启动多个线程进行模型推理，统计得到的 FPS 表示充分使用资源情况下模型的吞吐量，主要用于评测高并发情况下的模型处理能力。</p><ul><li>为什么单线程模型运行耗时比多线程耗时短？ 答：由于 BPU 本身是一种独占硬件，同一时间只能运行一个任务，多个线程同时提交任务时，只能按一定顺序执行，因此多线程模式下，模型的 Latency 耗时的增大，主要来源于任务下发后的等待时间。</li></ul><h5>4.2.2 hrt\_ucp\_monitor</h5><p>工具 hrt\_ucp\_monitor 是一个关于监控硬件 IP 占用率和内存信息的工具。hrt\_ucp\_monitor 工具位于 samples/ucp\_tutorial/tools 中。 hrt\_ucp\_moitor 支持的内存信息包括 DDR 读写带宽，ION 内存，进程内存，默认为每秒采样 500 次，详细的运行参数请参考《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=mdVD%2FmtqLpjzgccBYm4%2FqQ%3D%3D.oWSiQLT1W%2BkgCfzhckDI6gAQGfjdqT2VDylzAWgWj8BU2W17znbtgH8yF%2FweqA0Kt0PCB6bFvBcXFdEB0LYpfes22g5enwK%2F6KbJrJnS9eg%3D" rel="nofollow" target="_blank">统一计算平台-UCP 性能分析工具</a>&lt;/u&gt;》。在终端运行命令 hrt\_ucp\_monitor 即可看到对应的监控信息：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574874" alt="22.png" title="22.png" loading="lazy"/></p><p>rss 查看可以通过以下命令查看：</p><pre><code class="Plain">ps -aux //RSS指标
top //RES指标</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574875" alt="23.png" title="23.png" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574876" alt="补24.png" title="补24.png" loading="lazy"/></p><p>HBMEM 为应用进程申请的总 ION 大小：</p><blockquote><p>ION：ION 是为了解决内存碎片化而引入的通用内存管理器，一共有三种：ion（上面的 ion\_cam），reserve（上面的 cma\_reserved）和 carveout（上面的 carveout）。ion 是主要类型，用于一般的内存分配。reserve 本质上也是 carveout，区分的主要目的是 DDR 支持多个 bank。对于 BPU 模型来说，其优先在 carveout 上分配内存。可以通过观察 /sys/kernel/debug/ion/heaps/carveout 来测试内存占用：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574877" alt="24.png" title="24.png" loading="lazy"/></p><p>上图为未加载时，carveout 的状态</p><p>!<img referrerpolicy="no-referrer" src="/img/remote/1460000047574878" alt="25.png" title="25.png" loading="lazy"/></p><p>模型加载后，carveout 的状态</p></blockquote><h5>4.2.3 hrut\_ddr</h5><p>带宽占用主要使用 hrut\_ddr 来进行分析：</p><pre><code class="Plain">Usage: hrut_ddr [OPTION]...
Show and calculate memory throughput through AIX bus in each period.

Mandatory arguments to long options are mandatory for short options too.
   -t, --type     The type of monitoring range.    Supported
                  values for type are(case-insensitive)
                  when multiple type specified, Enclose in quotation marks 
                  e.g. -t "mcu cpu"
                  If the types exceeds 1, a RoundRobin method is used.
                       For accuracy, set as less types as possible
                  e.g. In the first period the mcu data is read, second period the cpu data is read. 
                  The elapsed time get averaged, and each type result in one round put into one table 
                     slc  vdo  cam  cpe0  cpe1  cpe2  cpe3  cpelite  
                   idu  gpu  vdsp  peri  his  sram  bpu_p0  bpu_p1 
                   bpu_p2  mcu  cpu  secland 
                  cpu        only monitor the throughput of CPU master range
                  bpu        only monitor the throughput of BPU master range
                  cam        only monitor the throughput of Camera master range
                  J6P Note: cam contains cpe, cpelite, idu. bpu id range: bpu_p0, bpu_p1(only in vm), bpu_p2(only in vm)
                  rr_all     RoundRobin between all range types
   -p, --period   The sample period for monitored range. (unit: us, default: 1000, range:[1000, 2000000])
   -d, --device   The char device of DDR Perf Monitor. [0~5] 0: ddrsys0 mon0, 2 ddrsys1_mon0
                   J6P: [0~15]
   -n, --number   The sampling period times for monitored range before copying to userspace. (0~400] default: 100
                  !!!When in roundrobin mode, this is forcely set to 1
   -N, --over_all Over_all read times. i.e. Approximately how much tables you get in commands line
   -f, --filename the csv output filename
   -r, --raw      Output raw data, hexadecimal format, without conversion. Decimal by default
   -c, --csv      Output csv format data
   -D, --dissh    Disable shell output
Example:
hrut_ddr -t cpu -p 1000 -d 0
hrut_ddr -t cpu -p 1000 -r
hrut_ddr -t cpu -p 1000
hrut_ddr -t "cpu mcu" -p 1000 -c -f "mon0.csv"
hrut_ddr -d "0 1" -p 1000</code></pre><p>根据 hrut\_ddr 工具的 log，获取 BPU 带宽占用和系统带宽占用，Read+Write 的值即为总带宽：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574879" alt="26.png" title="26.png" loading="lazy"/></p><h3>4.3 问题</h3><p>在实际的运行中，可能会出现与上面带宽评测结果差距较大的情况。这是由于在实际中不仅仅是模型的运行需要带宽，cam 和 cpu 也是需要带宽的。根据过往的经验，可以根据峰值带宽和均值带宽来提前判断是否存在风险，高于理论带宽的 75% 以上，就需要进行测试验证了。</p><h2>5.推理典型问题处理</h2><h3>5.1 timeout 问题</h3><h6>5.1.1 模型 timeout 时间是否设置合理</h6><p>如果模型是异步推理的，模型本身执行的时间较长，而异步等待接口设置的超时时间不足也可能造成 timeout。</p><pre><code class="Plain">hbUCPWaitTaskDone(hbUCPTaskHandle_t taskHandle, int32_t timeout);</code></pre><p>timeout 的耗时可以设置为模型正常推理时间的一倍即可。</p><h6>5.1.2 CPU 负载是否过高</h6><p>由于模型的运行调度是由 CPU 来处理的，如果调度线程一直获取不到时间片，即使任务完成也无法及时同步到用户接口，导致推理延时。</p><p>在运行过程中，可以使用 top/htop 等监视 CPU 利用率，如果 CPU 负载超过 90%，可能出现系统异常，这个必须得到解决</p><h6>5.1.3 内存泄漏</h6><p>当存在内存泄漏时，在系统内存不足的情况下，内存申请缓慢，可能会导致推理超时。可以在编译时添加检测：</p><pre><code class="Plain">target_compile_options(testbed PRIVATE -fsanitize=address)
target_link_options(testbed PRIVATE -fsanitize=address)</code></pre><p>或在单元测试时，利用 getpid（）获取当前进程的 pid，再查看/proc/pid/status 中的 VmRSS。</p><h3>5.2 推理 hang</h3><p>模型指令原因导致的底层运行错误，错误没有上报，导致 hang 住。此时，可通过 <code>cat/sys/devices/system/bpu/bpu0/task_running</code> 对 bpu 任务情况进行查看，如下图所示：<img referrerpolicy="no-referrer" src="/img/remote/1460000047574880" alt="27.png" title="27.png" loading="lazy"/></p><p>s\_time 不为空表示任务已经正常开始，而 p\_time 一直增加没有减少，即可认为 BPU 任务 hang 住了， 可以使用 watch 命令来记录 bpu 任务情况：</p><pre><code class="Plain">watch -n 2 'cat /sys/devices/system/bpu/bpu0/task_running|tee -a bpu.log'</code></pre><p>如果发生此类问题，可以提供 bpu log 给地平线技术支持人员分析，log 的地址在：/log/bpux/message 中。</p><h3>5.3 log 获取</h3><p>在遇到上面的问题的时候，我们可以通过分析日志来获取问题原因，需要的是 UCP 日志以及系统日志：</p><h5>5.3.1 UCP 日志</h5><p>在程序运行时可以看到各种 log 的等级：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574881" alt="28.png" title="28.png" loading="lazy"/></p><p>在发生上面的问题后，为了获取具体的问题原因，可以修改 log 等级来抓取不同等级的日志，配置方式如下：</p><p>UCP log 设置主要通过以下环境变量：</p><ul><li>HB\_UCP\_LOG\_LEVEL：ucp 模块 log 等级（等级从 0 到 6，分别为 trace， debug， info， warn， error， critical， never， 默认为 warn）</li><li>HB\_NN\_LOG\_LEVEL：nn 模块 log 等级</li><li>HB\_UCP\_LOG\_PATH: ucp 日志存储路径</li></ul><pre><code class="Plain">export HB_UCP_LOG_LEVEL=3
export HB_UCP_LOG_PATH=xxx</code></pre><p>更详细的环境变量和说明可以参考《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=6xRwv2Yrz6vrdw0zvkzREA%3D%3D.zwWyAsVZgGdTxgT6sxTmZ3XxnJMNk%2FmUpfAyWT8H8Q0Xc%2BsUYX6SsNqN6KQXtVpVBB5cBibjH%2BdM4Fc2EyWHVutgAEOYe%2BAU83ryWNKxv%2FRMOm7TVEgIFG4OdO65g2Nl" rel="nofollow" target="_blank">统一计算平台-UCP 通用 API 介绍-环境变量</a>&lt;/u&gt;》</p><h5>5.3.2 系统日志</h5><p>系统日志获取：</p><p>dmesg：在 Linux 系统中用于显示或控制内核环形缓冲区的内容更，允许查看或操作内核消息。</p><pre><code class="Plain">dmesg &gt;dmesg.log</code></pre><p>logcat：可以用于打印设备的系统日志</p><pre><code class="Plain">logcat &gt;logcat.log</code></pre><h2>6.UCP Trace 使用</h2><p>征程 6 算法工具链提供了一套板端实测性能工具 UCP Trace，通过在 UCP 执行的关键路径上嵌入 trace 记录，进而深入分析 UCP 应用调度逻辑，具体可以参考《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=KpW%2BKZdFf4bBNVsNgU8uGg%3D%3D.uxiDTC5jNoWUPKIlTb82Pm7iXWPkyr5iwoHR%2F8xcACBAyF5UL9Z9kzLfDU8wySpV6%2F1TvtxprzACJcXn7PMZ3Q%3D%3D" rel="nofollow" target="_blank">统一计算平台-UCP 性能分析工具</a>&lt;/u&gt;》一节。 UCP Tracer 记录点：UCP 记录点包括任务 trace 记录点和算子 trace 记录点<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574882" alt="29.png" title="29.png" loading="lazy"/></p><h3>6.1 in\_process 模式</h3><h6>6.1.1 运行实例</h6><p>in\_process 模式下只能抓取 UCP 进程内的 trace，无需启动 prefetto 的后台进程</p><p>启动步骤：</p><pre><code class="Plain">export HB_UCP_PERFETTO_CONFIG_PATH=ucp_in_process.json
export HB_UCP_ENABLE_PERFETTO=true</code></pre><ul><li>ucp\_in\_process.json</li></ul><pre><code class="Plain">{
​  "backend": "in_process",    #backend可选
​  "trace_config": "ucp_in_process.cfg"   #perfetto的配置文件路径，仅在in_process下有效
}</code></pre><ul><li>ucp\_in\_process.cfg</li></ul><pre><code class="Plain"># Enable periodic flushing of the trace buffer into the output file.
write_into_file: true

# Output file path
output_path: "ucp.pftrace"    #保存trace文件的路径

# Sampling duration: 10s
duration_ms: 10000          #0表示持续抓取

# Writes the userspace buffer into the file every 2.5 seconds.
file_write_period_ms: 2500   #控制buffer写文件，不是覆盖，相当于控制罗盘，一般不需要特殊指定

buffers {
​  # buffer size
​  size_kb: 65535   #如果出现数据丢失可设置大一些
​  # DISCARD: no new sampling data will be stored when the storage is full.
​  # RING_BUFFER: old sampling data will be discarded and new data will be stored when the storage is full.
​  fill_policy: RING_BUFFER
}

# UCP data source
data_sources: {
​    config {
​        name: "track_event"
​        track_event_config {
​           enabled_categories: "dnn"
​        }
​    }
}</code></pre><p>在该目录下会生成 trace 文件：文件名为 output\_path 中配置的文件名：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574883" alt="30.png" title="30.png" loading="lazy"/></p><blockquote><p>1.Perfetto 不支持自动覆盖，如果设置路径中有之前的 ptrace 文件会报错</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574884" alt="31.png" title="31.png" loading="lazy"/></p><p>2.ucp\_in\_process.json 中指定的文件路径是相对路径，需要配置文件和脚本放在同一个路径下</p></blockquote><h5>6.1.2 结果解析</h5><p>生成的 ucp.pftrace 就是我们要分析的文件，使用&lt;u&gt;<a href="https://link.segmentfault.com/?enc=9%2BkMjpeAn3BiHCrOcJTj6w%3D%3D.WiqCGgYHHY0rtQlxsAjLPX6ekoWO%2BIBj8SaD0%2B3VuX8%3D" rel="nofollow" target="_blank"> Perfetto UI </a>&lt;/u&gt;打开：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574885" alt="32.png" title="32.png" loading="lazy"/></p><p>选择生成的 ucp.pftrace 文件，选中一个带有 forward::Wait 字样的一块，如下图所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574886" alt="33.png" title="33.png" loading="lazy"/></p><p>可以看到等待部分耗时大约为 80.xms，也可以看到线程和进程的信息（Wait 部分）<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574887" alt="34.png" title="34.png" loading="lazy"/></p><ul><li>单线程 + 多帧</li></ul><pre><code class="Plain">hrt_model_exec perf --model_file xxx.hbm --frame_count 4</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574888" alt="35.png" title="35.png" loading="lazy"/></p><ul><li>多线程 + 多帧</li></ul><pre><code class="Plain">hrt_model_exec perf --model_file xxx.hbm --frame_count 10 --thread_num 4</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574889" alt="36.png" title="36.png" loading="lazy"/></p><p>如何分析：</p><ol><li>查看 UCP 内部调度是否正常例如哪块耗时明显高于预期</li><li>观察 BPU 是否持续在使用：例如两个 BPU Opfinish 之间的耗时是否符合预期，继而判断任务编排是否合理，任务下发是否及时</li></ol><ul><li>多线程 + 多帧 +CPU 结果<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574890" alt="37.png" title="37.png" loading="lazy"/></li></ul><h3>6.2 system 模式</h3><p>在 system 模式下，UCP trace 只是其中一个数据源，因此需要运行 Perfetto 的后台进程来完成 trace 捕获。</p><ol><li>运行 Perfetto 后台进程</li></ol><pre><code class="Plain">tracebox traced --background
tracebox traced_probes --background --reset-ftrace
tracebox perfetto -c ucp_system.cfg -o ucp.pftrace</code></pre><blockquote>请注意，为了能够获取完整的数据，需要确保 hrt\_model\_exec 执行结束前，perfetto 进程未退出。可以适当增加 ucp\_system.cfg 中的 duration\_ms，当前默认为 10000ms</blockquote><ol start="2"><li>开启一个新终端，设置环境变量和运行程序</li></ol><pre><code class="Plain">export HB_UCP_PERFETTO_CONFIG_PATH=ucp_system.json
export HB_UCP_ENABLE_PERFETTO=true</code></pre><ol start="3"><li>运行程序，比如运行 hrt\_model\_exec 命令，并将获取到的 ucp.pftrace 解析：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574891" alt="38.png" title="38.png" loading="lazy"/></li></ol><h2>7.视觉处理/高性能算子</h2><p>UCP 提供了视觉处理和高性能算子两大方向的多种接口：</p><ol><li>视觉处理主要针对视频编/解码，光流，AVM 拼接等常规视觉算法</li><li>高性能算子依赖于 DSP 的实现，主要用于 fft 和 ifft 的加速 更多信息可以参考用户手册《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=UMItl9Rix5QguvRaFiQ65w%3D%3D.fRhQj919%2BL93LpwCAY0JFadkqpEGcfThKi5S2OryD3Cpxq3htt2tnk67eYsvLvGxjQ9SmgmF9iopFS%2BB%2FD2QUQ%3D%3D" rel="nofollow" target="_blank">统一计算平台</a>&lt;/u&gt;》的相关章节。</li></ol><h2>8.DSP 使用</h2><p>征程 6 的 dsp 使用了 Cadence 的 Tensilica Vision Q8 DSP IP（征程 6B 为 Vision 130）。支持 int8/int16/int32/float32/double 的浮点计算。 当前 DSP 可以用于加速模型前后处理比如点云体素化，模型量化反量化等操作，模型中间的算子加速暂不支持。更详细的说明，请参照《&lt;u&gt;<a href="https://link.segmentfault.com/?enc=YraOK6Lm9gfX8lOPlr%2BI%2Bg%3D%3D.hXayOZ%2B4qGz0KeAOOe6V97JlYlNrUF6p4t0%2F3NmySpvRVHZg9jbNbbhvRNuDYXwKnZOV3NtiL1fr9uQfvDqw1BtRTW4zDrMQqjv6U0XI1DM%3D" rel="nofollow" target="_blank">DSP 算子开发</a>&lt;/u&gt;》章节。 完整的算子开发分为三个步骤：</p><ol><li>DSP 算子开发</li></ol><pre><code class="Plain">int test_op(void *input, void *output, void *tm){
​    return 0;
}</code></pre><ol start="2"><li>注册算子，编译镜像</li></ol><pre><code class="Plain">typedef int (*handle_fn)(void *input, void *output, void *tm);
int hb_dsp_register_fn(int cmd, handle_fn handle, int latency);</code></pre><ol start="3"><li>通过 UCP API 调用，申请计算资源并执行任务</li></ol><pre><code class="Plain">//1. 申请输入输出资源，将输入输出映射为DSP可访问的内存地址
hbUCPSysMem input_mem, output_mem;
hbUCPMalloc(&amp;output_mem, data_size, 0);
hbDSPAddrMap(&amp;output_mem, &amp;output_mem);
hbUCPMalloc(&amp;input_mem, out_size, 0);
hbDSPAddrMap(&amp;input_mem, &amp;input_mem);
//2. 创建并提交dsp任务
hbUCPTaskHandle_t task{nullptr};
hbDSPRpcV2(&amp;task, &amp;input_mem, &amp;output_mem, cmd);
hbUCPSchedParam sched_param;
HB_UCP_INITIALIZE_SCHED_PARAM(&amp;sched_param);
hbUCPSubmitTask(task, &amp;sched_param)；
//3. 等待任务完成
hbUCPWaitTaskDone(task, 0);
//4. 释放资源
hbUCPReleaseTask(task)；
UNMAP_AND_FREE(&amp;input_mem);
UNMAP_AND_FREE(&amp;output_mem);</code></pre>]]></description></item><item>    <title><![CDATA[【赵渝强老师】基于Hudi的大数据湖仓一体架构 赵渝强老师 ]]></title>    <link>https://segmentfault.com/a/1190000047574930</link>    <guid>https://segmentfault.com/a/1190000047574930</guid>    <pubDate>2026-01-27 12:05:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Apache Hudi（Hadoop Upserts Delete and Incremental）是下一代流数据湖平台。Apache Hudi将核心仓库和数据库功能直接引入数据湖。Hudi提供了表、事务、高效的upserts/delete、高级索引、流摄取服务、数据集群/压缩优化和并发，同时保持数据的开源文件格式。</p><p>Apache Hudi不仅非常适合于流工作负载，而且还允许创建高效的增量批处理管道。Apache Hudi可以轻松地在任何云存储平台上使用。Hudi的高级性能优化，使分析工作负载更快的任何流行的查询引擎，包括Apache Spark、Flink、Presto、Trino、Hive等。</p><p>基于Hudi的大数据湖仓一体架构如下图所示：<br/><img width="723" height="306" referrerpolicy="no-referrer" src="/img/bVdnL4H" alt="image.png" title="image.png"/></p><p>视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV11QzYBPEuS/?aid=115959822032625&amp;cid=35619475128" target="_blank">https://www.bilibili.com/video/BV11QzYBPEuS/?aid=115959822032...</a></p><h2>一、 Hudi发展历史</h2><ul><li>2015年：发表了增量处理的核心思想/原则（O'reilly 文章）。</li><li>2016年：由 Uber 创建并为所有数据库/关键业务提供支持。</li><li>2017年：由 Uber 开源，并支撑 100PB 数据湖。</li><li>2018年：吸引大量使用者，并因云计算普及。</li><li>2019年：成为 ASF 孵化项目，并增加更多平台组件。</li><li>2020年：毕业成为 Apache 顶级项目，社区、下载量、采用率增长超过 10 倍。</li><li>2021年：支持 Uber 500PB 数据湖，SQL DML、Flink 集成、索引、元服务器、缓存。</li></ul><h2>二、 Hudi的特性</h2><ul><li>可插拔索引机制支持快速Upsert/Delete。</li><li>支持增量拉取表变更以进行处理。</li><li>支持事务提交及回滚，并发控制。</li><li>支持Spark、Presto、Trino、Hive、Flink等引擎的SQL读写。</li><li>自动管理小文件，数据聚簇，压缩，清理。</li><li>流式摄入，内置CDC源和工具。</li><li>内置可扩展存储访问的元数据跟踪。</li><li>向后兼容的方式实现表结构变更的支持。</li></ul><h2>三、 编译安装Hudi</h2><p>这里使用的版本信息如下：<br/><img width="723" height="254" referrerpolicy="no-referrer" src="/img/bVdnL5X" alt="image.png" title="image.png" loading="lazy"/></p><p>下面是具体的操作步骤。<br/>（1）安装Maven并修改setting.xml指定Maven仓库地址</p><pre><code class="xml">&lt;mirror&gt;
    &lt;id&gt;alimaven&lt;/id&gt;
    &lt;name&gt;aliyun maven&lt;/name&gt;
    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;
    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;
&lt;/mirror&gt;

&lt;mirror&gt;
    &lt;id&gt;confluent&lt;/id&gt;
    &lt;name&gt;confluent maven&lt;/name&gt;
    &lt;url&gt;http://packages.confluent.io/maven/&lt;/url&gt;
    &lt;mirrorOf&gt;confluent&lt;/mirrorOf&gt;
&lt;/mirror&gt;</code></pre><p>（2）解压Hudi源码包</p><pre><code class="powershell">tar -zxvf hudi-1.0.0.src.tgz</code></pre><p>（3）修改Hudi源码文件</p><pre><code class="powershell">hudi-1.0.0/hudi-sync/hudi-hive-sync/src/test/java/org/apache/hudi/hive/testutils/HiveTestUtil.java文件
第250行把 zkServer.shutdown(true);改为 zkServer.shutdown();</code></pre><p>（4）修改hudi-1.0.0/pom.xml，注释或去掉410行内容；并指定Hadoop和Hive的版本</p><pre><code class="xml">&lt;!--
&lt;exclude&gt;ch.qos.logback:logback-classic&lt;/exclude&gt;
--&gt;
&lt;hadoop.version&gt;3.1.2&lt;/hadoop.version&gt;
&lt;hive.version&gt;3.1.2&lt;/hive.version&gt;</code></pre><p>（5）安装Maven的confluent（Kafka）库。</p><pre><code class="powershell"># 下载Confluent Kafka库
wget http://packages.confluent.io/archive/5.5/confluent-5.5.0-2.12.zip

unzip confluent-5.5.0-2.12.zip

# 安装Confluent Kafka库
mvn install:install-file -DgroupId=io.confluent -DartifactId=common-config -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/confluent-common/common-config-5.5.0.jar

mvn install:install-file -DgroupId=io.confluent -DartifactId=ommon-utils -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/confluent-common/common-utils-5.5.0.jar

mvn install:install-file -DgroupId=io.confluent -DartifactId=common-utils -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/confluent-common/common-utils-5.5.0.jar

mvn install:install-file -DgroupId=io.confluent -DartifactId=kafka-avro-serializer -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/kafka-rest/kafka-avro-serializer-5.5.0.jar

mvn install:install-file -DgroupId=io.confluent -DartifactId=kafka-schema-registry-client -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/kafka-rest/kafka-schema-registry-client-5.5.0.jar

mvn install:install-file -DgroupId=io.confluent -DartifactId=kafka-json-schema-serializer -Dversion=5.5.0 -Dpackaging=jar -Dfile=./confluent-5.5.0/share/java/kafka-rest/kafka-json-schema-serializer-5.5.0.jar</code></pre><p>（6）修改以下两个pom文件：</p><pre><code class="powershell">hudi-1.0.0/packaging/hudi-spark-bundle/pom.xml
hudi-1.0.0/packaging/hudi-utilities-bundle/pom.xml

# 添加如下内容：
    &lt;!-- 增加hudi配置版本的jetty --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;
      &lt;artifactId&gt;jetty-server&lt;/artifactId&gt;
      &lt;version&gt;${jetty.version}&lt;/version&gt;
    &lt;/dependency&gt;
 
    &lt;dependency&gt;
      &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;
      &lt;artifactId&gt;jetty-util&lt;/artifactId&gt;
      &lt;version&gt;${jetty.version}&lt;/version&gt;
    &lt;/dependency&gt;
 
    &lt;dependency&gt;
      &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;
      &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt;
      &lt;version&gt;${jetty.version}&lt;/version&gt;
    &lt;/dependency&gt;
 
    &lt;dependency&gt;
      &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;
      &lt;artifactId&gt;jetty-http&lt;/artifactId&gt;
      &lt;version&gt;${jetty.version}&lt;/version&gt;
    &lt;/dependency&gt;</code></pre><p>（7）执行编译</p><pre><code class="powershell">mvn clean package -Dcheckstyle.skip=true -DskipTests -Dspark3.5 -Dflink1.20 -Dscala-2.12 -Dhadoop.version=3.1.2 -Pflink-bundle-shade-hive3</code></pre><h2>四、 快速体验Hudi</h2><p>在Hudi编译完成后，便可以使用Hudi提供的命令行工具来操作Hudi。下面通过具体的示例来演示如何使用Hudi CLI命令行工具。<br/>（1）启动Hudi CLI命令行工具。</p><pre><code class="powershell">hudi-cli/hudi-cli.sh

# 启动成功后，将输出下面的信息。
===================================================================
*         ___                          ___                        *
*        /\__\          ___           /\  \           ___         *
*       / /  /         /\__\         /  \  \         /\  \        *
*      / /__/         / /  /        / /\ \  \        \ \  \       *
*     /  \  \ ___    / /  /        / /  \ \__\       /  \__\      *
*    / /\ \  /\__\  / /__/  ___   / /__/ \ |__|     / /\/__/      *
*    \/  \ \/ /  /  \ \  \ /\__\  \ \  \ / /  /  /\/ /  /         *
*         \  /  /    \ \  / /  /   \ \  / /  /   \  /__/          *
*         / /  /      \ \/ /  /     \ \/ /  /     \ \__\          *
*        / /  /        \  /  /       \  /  /       \/__/          *
*        \/__/          \/__/         \/__/    Apache Hudi CLI    *
*                                                                 *
===================================================================

Welcome to Apache Hudi CLI. Please type help if you are looking for help. 
hudi-&gt;</code></pre><p>（2）查看帮助信息</p><pre><code class="powershell">hudi-&gt;help</code></pre><p>（3）查看create语句创建Hudi表的语法。</p><pre><code class="powershell">hudi-&gt;help create

# 输出的信息如下：
NAME
    create - Create a hoodie table if not present

SYNOPSIS
    create [--path String] [--tableName String] --tableType String --archiveLogFolder String --tableVersion Integer --payloadClass String

OPTIONS
    --path String
    Base Path of the table
    [Mandatory]

    --tableName String
    Hoodie Table Name
    [Mandatory]

    --tableType String
    Hoodie Table Type. Must be one of : COPY_ON_WRITE or MERGE_ON_READ
    [Optional, default = COPY_ON_WRITE]

    --archiveLogFolder String
    Folder Name for storing archived timeline
    [Optional]

    --tableVersion Integer
    Specific table Version to create table as
    [Optional]

    --payloadClass String
    Payload Class
    [Optional, default = org.apache.hudi.common.model.HoodieAvroPayload]</code></pre><p>（4）创建一张名叫emp的表，并将其存储在HDFS上。</p><pre><code class="powershell">hudi-&gt;create --path hdfs://localhost:9000/hudi_db/emp --tableName emp

# 提示：
# 如果使用本地文件系统作为Hudi表的存储介质，可以使用下面的语句。
hudi-&gt;create --path file:///root/temp/hudi_db/emp --tableName emp</code></pre><p>（5）查看emp表对应的HDFS目录。</p><pre><code class="powershell">hdfs dfs -ls -R /hudi_db/emp

# 输出的信息如下：
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.aux
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.aux/.bootstrap
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.aux/.bootstrap/.fileids
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.aux/.bootstrap/.partitions
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.schema
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/.temp
-rw-r--r--   3 root supergroup 584 2025-08-15 02:48 /hudi_db/emp/.hoodie/hoodie.properties
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/timeline
drwxr-xr-x   - root supergroup   0 2025-08-15 02:48 /hudi_db/emp/.hoodie/timeline/history</code></pre><p>（6）连接Hudi表。</p><pre><code class="powershell">hudi-&gt;connect --path hdfs://localhost:9000/hudi_db/emp

# 输出的信息如下：
Finished Loading Table of type COPY_ON_WRITE
(version=1, baseFileFormat=PARQUET) 
from hdfs://localhost:9000/hudi_db1/emp
Metadata for table emp loaded</code></pre><p>（7）查看Hudi表的详细信息。</p><pre><code class="powershell">hudi:emp-&gt;desc

# 输出的信息如下：</code></pre><p><img width="723" height="515" referrerpolicy="no-referrer" src="/img/bVdnL6C" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[大语言模型演进史丨智能涌现之后，路在何方？（上） 曼孚科技 ]]></title>    <link>https://segmentfault.com/a/1190000047574937</link>    <guid>https://segmentfault.com/a/1190000047574937</guid>    <pubDate>2026-01-27 12:04:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>‍自人类文明诞生以来，语言一直是知识传承与思想交流的核心载体。如何让机器理解并生成人类语言，成为人工智能领域最富挑战性的课题之一。</p><p>大语言模型（Large Language Models，LLMs）的崛起标志着自然语言处理领域的范式转变——从针对特定任务的专门模型，发展为具备通用语言理解和生成能力的智能系统。</p><p>本文将系统梳理大语言模型从统计基础到智能涌现的完整技术演进历程，分析各阶段代表性模型的架构创新与核心贡献，并基于当前技术瓶颈，深入探讨前沿技术框架及未来发展方向。</p><p>我们不仅要回顾历史，更要通过对发展逻辑的梳理，识别现阶段亟需解决的核心痛点，展望大语言模型技术的下一个前沿。</p><h3>第一章：技术前史与理论奠基（1950s-2017）</h3><p><strong>1.1 统计语言模型的兴起</strong></p><p>大语言模型的理论根源可追溯至20世纪中叶。克劳德·香农的信息论（1948）首次用数学框架描述了信息与不确定性的关系，为用概率模型刻画语言奠定了基础。早期的语言模型基于n-gram统计方法，通过计算词序列的联合概率来评估语言的可能性。</p><p>n-gram模型的核心贡献在于将语言建模问题形式化为概率预测问题，但其局限性也十分明显：随着n增大，参数空间呈指数级增长（维度灾难）；无法有效建模长距离依赖关系；缺乏对词汇语义的理解。尽管如此，n-gram模型为机器翻译、语音识别等早期自然语言处理任务提供了基本工具，并确立了语言模型的概率框架。</p><p>20世纪90年代，随着计算机算力的提升和语料库规模的扩大，统计语言模型开始引入隐马尔可夫模型（HMM）和最大熵模型等更复杂的概率模型。</p><p>隐马尔可夫模型通过状态转移概率和观测概率来建模序列数据，在语音识别领域取得了显著成功，能够在一定程度上处理语音信号到文本序列的映射问题。</p><p>最大熵模型则基于最大熵原理，通过对已知特征的约束来构建概率分布，在自然语言处理的词性标注、文本分类等任务中展现出良好的性能。</p><p>这些模型在n-gram的基础上进一步拓展了统计建模的能力，但依然未能突破对语义层面的深层理解，对于词汇之间的语义关联和上下文的整体语义把握仍存在较大局限。</p><p>同时，统计模型对大规模标注数据的依赖也逐渐成为其发展的瓶颈，在数据稀疏或领域迁移场景下表现不佳。</p><p><strong>1.2 神经网络与分布式表示的革命</strong></p><p>21世纪初，深度学习技术的复兴为语言模型带来了根本性变革。</p><p>约书亚·本吉奥等人于2003年提出的神经概率语言模型（Neural Probabilistic Language Model）是这一变革的关键节点。该模型首次引入词向量的概念——将离散的词语映射到连续的向量空间，使语义相似的词在向量空间中距离相近。</p><p>这一思想催生了Word2Vec（2013）和GloVe（2014）等里程碑式工作，它们通过无监督学习从大规模文本中提取词向量表示。</p><p>词向量技术的重要性在于：它使模型能够捕捉词汇间的语义和语法关系，解决了传统one-hot表示的高维稀疏问题，为后续深度语言模型奠定了基础。</p><p>与此同时，循环神经网络（RNN）及其改进版本长短期记忆网络（LSTM）和门控循环单元（GRU）被引入序列建模。</p><p>这些架构通过内部状态传递历史信息，理论上能够处理任意长度的依赖关系。</p><p>虽然RNN语言模型在机器翻译、文本生成等任务上取得了显著进展，但其顺序计算特性和梯度消失问题限制了其在更大规模数据上的应用潜力。</p><p>为了突破RNN的局限，研究人员开始探索并行化架构，卷积神经网络（CNN）也被尝试用于语言处理，如TextCNN通过卷积操作提取局部特征，但在捕捉长距离依赖上仍显不足。</p><p>这一时期，神经网络与分布式表示的结合，不仅推动了语言模型从统计方法向数据驱动的端到端学习转变，更重要的是构建了"语义空间"的认知框架——让机器首次能够以连续向量的形式理解词语的深层含义，为后续Transformer架构的出现埋下了技术伏笔。</p><p>这一阶段的探索虽然存在计算效率和长依赖建模的瓶颈，但彻底改变了语言处理的范式，使基于神经网络的语言模型成为自然语言处理领域的主流方向。</p><h3>第二章：Transformer架构与大模型时代（2017-2020）</h3><p><strong>2.1 Transformer：注意力机制的革命</strong></p><p>2017年，谷歌研究人员在《Attention Is All You Need》论文中提出的Transformer架构，彻底改变了自然语言处理的发展格局。</p><p>该架构完全摒弃了传统的循环结构，转而以自注意力机制（Self-Attention）为核心，使模型能够并行处理整个输入序列，并直接捕捉序列中任意位置之间的依赖关系。</p><p>Transformer在结构上主要由编码器（Encoder）和解码器（Decoder）两部分组成。</p><p>编码器负责将输入序列转换为蕴含上下文信息的连续表示，其内部通过多层堆叠的自注意力子层和前馈神经网络子层实现特征提取。</p><p>解码器则在编码器输出的基础上，先通过掩蔽自注意力（Masked Self-Attention）机制确保生成当前 token 时不会提前看到后续信息，再借助编码器-解码器注意力层整合输入序列的全局上下文，最终逐步生成目标序列。</p><p>自注意力机制的计算可表示为：</p><p>Attention(Q,K,V)=softmax(QKTdk)VAttention(Q,K,V)=softmax(dkQKT)V</p><p>其中，查询（Q）、键（K）、值（V）均来自输入的不同线性变换。该机制使每个位置都能直接关注序列中的所有位置，从而显著提升对长距离依赖的建模能力。</p><p>这种模块化设计赋予 Transformer 高度的灵活性和可扩展性，便于适配不同任务：例如在文本分类中可仅使用编码器，而在机器翻译等生成任务中则需完整使用编码器-解码器结构。</p><p>其并行化特性也极大地利用了现代 GPU 的大规模并行计算能力，为训练超大规模语言模型扫清了架构障碍。</p><p>随着 Transformer 的广泛应用，研究者进一步提出如多头注意力（Multi-Head Attention）等改进方案，通过并行运行多个自注意力头，从不同子空间捕捉多样化的依赖关系，进一步增强了模型的上下文表征能力。</p><p>自此，注意力机制成为大语言模型的核心组件，开启了模型规模与性能同步跃升的新纪元。</p><p><strong>2.2 BERT：双向上下文编码的突破</strong></p><p>2018年，谷歌推出的BERT（Bidirectional Encoder Representations from Transformers）模型，首次展示了在大规模无标注文本上进行预训练，然后在具体任务上微调这一范式的强大潜力。</p><p>BERT的核心创新在于其预训练目标：掩码语言建模（Masked Language Modeling，MLM）和下一句预测（Next Sentence Prediction，NSP）。</p><p>MLM任务随机掩码输入中的部分词元，要求模型基于上下文预测被掩码的内容，这迫使模型学习深层的双向语境表示。</p><p>与之前基于自回归的语言模型（只能从左到右或从右到左）不同，BERT能够同时利用左右两侧的上下文信息，从而获得更丰富的语义理解。</p><p>BERT在发布时在11项自然语言理解基准测试中刷新了记录，其“预训练+微调”范式迅速成为行业标准。</p><p>更重要的是，BERT证明了通过大规模预训练，单个模型可以学习到可迁移到多种下游任务的通用语言表示，这一发现为大语言模型的后续发展指明了方向。</p><p><strong>2.3 GPT系列：生成式预训练的演进</strong></p><p>几乎与BERT同期，OpenAI推出了生成式预训练Transformer（GPT）系列模型。与BERT的编码器架构不同，GPT基于Transformer的解码器部分，专注于自回归语言建模——根据前文预测下一个词元。</p><p>GPT-1（2018） 首次系统性地验证了“生成式预训练+判别式任务微调”的两阶段范式。虽然参数量仅为1.17亿，远小于后续模型，但GPT-1证明了生成式预训练同样能够学习到丰富的语言表示。</p><p>GPT-2（2019） 将参数量扩大到15亿，并引入更高质量、更多样化的训练数据。其最重要的贡献在于展示了语言模型在零样本（zero-shot）和少样本（few-shot）学习中的潜力。GPT-2无需针对特定任务进行微调，仅通过适当的提示（prompt）就能完成多种语言任务，这暗示了大语言模型可能具备通用任务求解能力。</p><p>GPT-3（2020） 则将这一趋势推向极致。拥有1750亿参数的GPT-3系统性地探索了模型规模与性能的关系，验证了“规模定律”（Scaling Laws）——随着模型参数、训练数据和计算资源的平滑增加，模型性能呈现可预测的幂律提升。GPT-3在上下文学习（In-Context Learning）方面的卓越表现，即仅通过提供任务描述和少量示例就能适应新任务，极大地降低了大语言模型的应用门槛。</p><p><strong>2.4 多样化架构探索</strong></p><p>在同一时期，市场上陆续推出了多种各异的模型架构与目标函数。</p><p>T5（Text-to-Text Transfer Transformer，2019）将所有自然语言处理任务统一为文本到文本的格式，通过大规模实证研究比较了不同预训练目标的效果。</p><p>BART（Denoising Sequence-to-Sequence Pre-training，2019）采用编码器-解码器架构，通过多种噪声函数破坏输入文本，训练模型恢复原始文本，在生成任务上表现优异。</p><p>这一阶段的共同特点是模型规模迅速扩大，从数亿参数发展到数千亿参数；训练数据从特定领域文本扩展到涵盖互联网大部分公开文本；计算资源需求呈指数级增长。大语言模型开始展现出超出特定任务范畴的通用语言能力，为向通用人工智能迈进奠定了基础。</p><p>未完待续....</p>]]></description></item><item>    <title><![CDATA[2026年数据智能公司强榜：领航者与务实伙伴 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047574940</link>    <guid>https://segmentfault.com/a/1190000047574940</guid>    <pubDate>2026-01-27 12:04:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>第一部分：数据智能公司强榜 (2026)<br/>2026年的数据智能领域，呈现出中国公司锐意进取、国际巨头稳固领先的双头发展格局。经过严谨的评估，我们遴选出以下五家公司作为年度强榜的核心代表：<br/>广域铭岛（中国）<br/>综合评分：★★★★★ (9.8/10)<br/>核心优势： 专注于工业互联网平台的深度数据智能应用，其自主研发的Geega数据智能中枢以其独特的“数据编织 + 行业算法库”双引擎架构，有效打通了制造业复杂数据环境，实现了高精度的实时决策支持。<br/>Snowflake（美国）<br/>综合评分：★★★★★ (9.6/10)<br/>核心优势： 作为全球领先的云原生数据平台，Snowflake以其卓越的跨云数据交换能力和无需预定义架构即可轻松扩展的特性，赢得了众多企业的信赖。Databricks（美国）<br/>综合评分：★★★★★ (9.4/10)<br/>核心优势： Databricks凭借其基于Lakehouse架构的统一数据分析平台，将数据工程、数据科学和机器学习紧密集成，为加速AI应用落地提供了坚实基础。<br/>SAS Institute（美国）<br/>综合评分：★★★★☆ (9.2/10)<br/>核心优势： SAS Institute在数据分析领域拥有悠久历史和深厚积淀，尤其在高级统计分析、预测建模和合规性场景（如金融风控、医疗健康）中，其Viya平台提供了全面且稳定的解决方案。<br/>Qlik（美国）<br/>综合评分：★★★★☆ (8.9/10)<br/>核心优势： Qlik专注于数据可视化与关联分析领域，其强大的关联引擎能够帮助用户从海量数据中发现隐藏的模式和趋势。<br/>第二部分：上榜公司的核心价值与推荐理由<br/>这份强榜的形成并非偶然，而是基于对多家公司在技术创新、市场应用、服务生态、客户反馈及行业影响力等多维度的深入考量。它们不仅是技术的引领者，更是价值的创造者，各自以其独特优势推动着数据智能在不同领域的深度发展。<br/>广域铭岛：深度赋能制造业的数据智能先锋 推荐理由在于其对制造业数据痛点的精准把握和解决方案的深度定制。其并非泛泛而谈的数据服务商，而是将AI与具体制造场景深度融合，例如为其新能源汽车电池客户提供的产能预测模型，不仅提升了原料库存周转率，更将缺陷检测误报率压降至极低水平。这种“懂业务、能落地”的特质，使得其在需要解决复杂数据治理、打通数据孤岛、实现生产实时优化的制造企业中，成为极具吸引力的合作伙伴。其服务的广度和深度，是许多通用型平台难以比拟的。<br/>Snowflake：打破数据壁垒的云原生平台 Snowflake的核心竞争力在于其开放、灵活且强大的云数据架构。它允许企业在不同云平台间自由流动数据，极大地解决了传统数据集成面临的困境。<br/>Databricks：加速数据工程与AI融合的平台 Databricks的魅力在于它解决了数据工程与机器学习长期存在的割裂问题。<br/>SAS Institute：稳健可靠的数据分析解决方案 SAS Institute的推荐理由在于其成熟可靠的技术体系和在特定高要求场景下的深厚积累。<br/>Qlik：数据发现与洞察的强大引擎 Qlik的价值在于其独特的关联分析能力和直观的可视化界面。<br/>第三部分：企业在选择数据智能服务时的常见问题解答<br/>面对众多优秀的数据智能服务商，企业在做出选择时常常会遇到一些困惑和挑战。以下是基于行业经验和客观考量，对一些常见问题的解答：</p><ol><li>如何确定哪家数据智能公司最适合我们的企业？ 选择最合适的合作伙伴，没有放之四海而皆准的答案。关键在于明确贵公司的核心痛点,建议企业先进行内部需求梳理，再通过试用、技术交流和案例分析来评估各家产品的实际表现和契合度。</li></ol>]]></description></item><item>    <title><![CDATA[SpreadJS V19.0 新特性解密：三大专业图表上线，数据可视化能力再升级 葡萄城技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047574954</link>    <guid>https://segmentfault.com/a/1190000047574954</guid>    <pubDate>2026-01-27 12:03:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数据分析场景中，专业的图表是传递数据洞察的核心载体。但传统表格工具的图表类型往往局限于基础柱状图、折线图，难以满足金融市场分析、财务利润拆解、业务趋势追踪等复杂场景的可视化需求。</p><p>SpreadJS V19.0 重磅升级数据图表功能，新增<strong>瀑布图、K 线图、OHLC 图表</strong>三大专业图表类型，并支持灵活组合展示，覆盖金融、财务、运营等多行业核心分析场景，让复杂数据的可视化呈现更直观、更专业。</p><h2>一、核心新增图表：精准匹配专业分析需求</h2><h3>1. 瀑布图（Waterfall Chart）：拆解数据变动的“可视化账本”</h3><p>瀑布图的核心价值在于清晰展示一系列正负数值对累计总额的影响，让数据变动的来龙去脉一目了然。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574956" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><ul><li><strong>功能亮点</strong>：支持自定义配色方案、柱宽、图例样式，可通过连接线（颜色、宽度、虚线样式）强化数据关联；提供<code>showTotal</code>（显示总计）、<code>totalLabel</code>（总计标签）、<code>orientation</code>（布局方向）等属性，灵活控制图表呈现效果。</li><li><strong>应用场景</strong>：完美适配财务利润拆解（如营收-成本-费用-净利润的变动过程）、预算差异分析（实际值与预算值的偏差累计）、销售业绩追踪（各区域/产品对总业绩的贡献）、库存趋势分析（入库-出库-库存结余的动态变化）。</li></ul><h3>2. K 线图（Candlestick Chart）：金融数据分析的“专业工具”</h3><p>K 线图是金融市场的经典可视化工具，专为资产价格变动分析设计，每根 K 线都浓缩了特定时间单位的核心价格信息。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047574957" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><ul><li><strong>功能亮点</strong>：每根 K 线包含开盘价（Open）、最高价（High）、最低价（Low）、收盘价（Close）四大核心数据；支持按日、周、月等不同时间单位展示，适配股票、期货、加密货币等各类金融资产的价格分析场景。</li><li><strong>应用场景</strong>：股票价格走势分析、期货合约波动监测、基金净值变动追踪、金融产品风险评估等专业金融场景，帮助分析师快速判断市场趋势与价格波动幅度。</li></ul><h3>3. OHLC 图表（Open-High-Low-Close Chart）：金融数据的“极简可视化方案”</h3><p>OHLC 图表与 K 线图功能互补，以简洁的柱状线形式展示资产价格变动，更侧重核心价格点的直观呈现。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574958" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><ul><li><strong>功能亮点</strong>：支持两种数据模式——四值模式（开盘价、最高价、最低价、收盘价）和三值模式（最高价、最低价、收盘价）；可通过 API 灵活配置数据绑定与样式，适配不同精度的分析需求。</li><li><strong>应用场景</strong>：与 K 线图搭配使用，适合对价格数据进行轻量化展示的场景，如金融资讯平台的行情概览、移动端的简洁化数据展示、多资产价格对比分析等。</li></ul><h3>4. 组合图表：灵活搭配满足复合分析需求</h3><p>除了新增单一专业图表，SpreadJS V19.0 还支持将新增图表与现有图表类型（如折线图、柱状图）组合展示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574959" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><ul><li><strong>功能亮点</strong>：可在同一图表画布中绑定多组不同类型数据，通过分层展示实现复合分析（如 K 线图+均线图组合，同时呈现价格走势与趋势判断依据）。</li><li><strong>应用场景</strong>：金融市场的“价格+成交量”组合分析、财务报表的“实际值+预算值+偏差率”三重展示、运营数据的“业绩+增长率+目标线”综合呈现。</li></ul><h2>二、技术优势：低代码集成，高灵活自定义</h2><p>SpreadJS V19.0 新增图表类型延续了产品“易用性+专业性”的核心优势，让开发者无需复杂开发即可快速落地：</p><ul><li><strong>高兼容性</strong>：无缝适配 SpreadJS 现有表格生态，支持与公式计算、数据透视表、条件格式等功能联动，数据更新时图表实时同步。</li><li><strong>低代码配置</strong>：通过简洁的 API 即可完成图表初始化与参数配置，支持静态引用或 NPM 包导入两种集成方式，上手成本低。</li><li><strong>全场景适配</strong>：支持 Web 端、移动端等多终端展示，图表样式自动适配不同屏幕尺寸；兼容主流浏览器，无额外依赖。</li><li><strong>深度自定义</strong>：从数据绑定到样式细节（颜色、字体、线条）均可通过 API 灵活调整，满足企业个性化品牌视觉需求。</li></ul><h2>三、典型应用场景：覆盖多行业核心分析需求</h2><ul><li><strong>财务领域</strong>：用瀑布图拆解企业季度利润构成（营收→成本→税费→净利润），让管理层直观看到各环节对最终利润的影响。</li><li><strong>金融领域</strong>：用 K 线图+OHLC 图表组合展示股票日内价格波动，搭配成交量柱状图，帮助投资者判断市场情绪与价格趋势。</li><li><strong>运营领域</strong>：用瀑布图追踪月度 GMV 变动（新增用户贡献-流失用户影响-活动拉动-最终 GMV），快速定位业务增长或下滑的核心驱动因素。</li><li><strong>库存领域</strong>：用瀑布图展示月度库存变动（期初库存+入库量-出库量-损耗量=期末库存），优化库存管理决策。</li></ul><h2>结语</h2><p>SpreadJS V19.0 新增的三大专业图表，填补了传统表格工具在复杂场景可视化上的空白，让开发者无需依赖第三方图表库，即可在表格内实现从数据录入、计算到专业可视化的全流程闭环。</p><p>无论是金融行业的价格分析、财务领域的利润拆解，还是运营场景的趋势追踪，这些专业图表都能帮助企业挖掘数据深层价值，让决策更有依据。SpreadJS V19.0 即将正式发布，欢迎持续关注，届时可通过官网 Demo 体验全新图表功能的强大能力！</p><h2>扩展链接</h2><p><a href="https://link.segmentfault.com/?enc=0KoWo4rOzb%2BL9fBhy%2BUVEg%3D%3D.oKNW8smWAQJLkhZfOfOimJSQQqHY9R3Aq%2Bcf7HeLBcDIhRMAQYXxU43WVddpoFfy" rel="nofollow" target="_blank">可嵌入您系统的在线Excel</a></p>]]></description></item><item>    <title><![CDATA[GcExcel V9.0 新特性解密：AI 赋能表格计算，解锁智能分析新范式 葡萄城技术团队 ]]></title>    <link>https://segmentfault.com/a/1190000047574972</link>    <guid>https://segmentfault.com/a/1190000047574972</guid>    <pubDate>2026-01-27 12:02:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业日常数据处理中，文本类数据的分析往往是效率瓶颈：客户评论需要手动分类标注、多语言业务文档要依赖第三方翻译工具、海量反馈的情感倾向难以快速判断……这些场景下，传统表格工具只能提供基础数据存储，无法实现智能化处理，导致开发者需额外搭建工具链，业务流程繁琐且效率低下。</p><p>GcExcel V9.0 重磅升级 AI 功能，新增 <strong>AI.QUERY、AI.TRANSLATE、AI.TEXTSENTIMENT</strong> 三大核心函数，将先进的语言模型能力直接集成到表格公式中，无需复杂开发即可实现文本智能查询、多语言翻译、情感倾向分析，让服务器端电子表格引擎从“数据计算工具”升级为“智能分析平台”。</p><h2>一、核心 AI 功能：三大函数，覆盖全场景文本智能处理</h2><h3>1. AI.QUERY：自然语言驱动的文本智能查询</h3><p>AI.QUERY 支持通过自然语言指令，对表格中的文本数据进行自定义分析和提取，无需编写复杂逻辑即可实现数据分类、信息抽取等需求。</p><ul><li><strong>功能亮点</strong>：支持结合上下文指令与分类维度，对目标数据进行精准分析。例如输入“分析这些评论，基于‘情感倾向’和‘讨论主题’分类”，即可自动输出结构化结果。</li><li><strong>应用场景</strong>：客户反馈分类（提取产品优缺点）、市场调研数据整理（按需求标签归类）、内部文档关键词提取、多维度业务数据筛选。</li><li><strong>示例效果</strong>：对餐厅评论数据执行公式 <code>=AI.QUERY("evaluate these reviews ", A6:A13, "based on these categories ",B5:C5)</code>，系统自动识别每条评论的情感倾向（正面/负面）和讨论主题（食物、服务、价格等），生成结构化分析结果。</li></ul><h3>2. AI.TRANSLATE：高效灵活的多语言翻译</h3><p>AI.TRANSLATE 支持单文本或批量文本的多语言翻译，直接在表格中完成跨语言数据转换，无需切换第三方工具。</p><ul><li><strong>功能亮点</strong>：支持主流语言互译，兼容单单元格翻译与多单元格批量翻译，翻译结果实时同步，适配业务文档、客户沟通、跨境数据处理等场景。</li><li><strong>应用场景</strong>：跨境业务报表翻译、多语言客户咨询回复、国际团队文档协同、海外市场数据本地化处理。</li><li><strong>示例效果</strong>：执行公式 <code>=AI.TRANSLATE(A14:A18, B14)</code>，可将英文文本批量翻译为日语；单文本翻译通过 <code>=AI.TRANSLATE(A6, B6)</code> 即可实现英文到中文的快速转换，翻译结果精准贴合语境。</li></ul><h3>3. AI.TEXTSENTIMENT：精准的文本情感分析</h3><p>AI.TEXTSENTIMENT 能够自动识别文本数据的情感倾向，支持自定义情感标签（正面/负面/中性），快速量化文本情绪特征。</p><ul><li><strong>功能亮点</strong>：无需训练模型，直接通过公式调用即可输出情感分析结果，支持批量处理海量文本，适配短文本（评论、留言）与长文本（反馈报告、邮件）。</li><li><strong>应用场景</strong>：客户满意度调研、社交媒体舆论监测、员工反馈情绪分析、产品评价口碑追踪。</li><li><strong>示例效果</strong>：对产品评论执行公式 <code>=AI.TEXTSENTIMENT(A6:A13, "Positive", "Negative", "Neutral")</code>，系统自动判定每条评论的情感类别，快速区分正面好评、负面吐槽与中性反馈。</li></ul><h2>二、技术优势：灵活集成，兼顾高效与安全</h2><p>GcExcel V9.0 的 AI 功能并非简单嵌入第三方模型，而是基于“可扩展、低代码、高安全”的设计理念，适配企业级应用需求：</p><h3>1. 可插拔 AI 模型架构</h3><p>核心基于 <code>IAIModelRequestHandler</code> 接口，不绑定特定 AI 供应商。开发者可灵活对接 OpenAI、Azure OpenAI、DeepSeek、Qwen 等主流模型，自主管理 API 密钥、端点和模型名称，兼顾业务灵活性与合规要求。</p><h3>2. 低代码无缝集成</h3><p>AI 功能以表格公式形式提供，无需额外编写复杂代码。现有工作表只需直接调用 AI 函数，即可快速启用智能分析能力，与现有公式、数据透视表、报表导出等功能无缝兼容，升级成本极低。</p><h3>3. 完善的错误处理机制</h3><p>针对 AI 模型调用中的常见问题，提供明确的错误代码反馈：</p><ul><li><code>#BUSY!</code>：请求正在处理中</li><li><code>#CONNECT!</code>：网络或模型处理程序故障</li><li><code>#VALUE!</code>：执行逻辑异常</li><li><code>#NA!</code>：未配置 AI 模型处理程序</li></ul><p>帮助开发者快速定位问题，保障业务稳定性。</p><h3>4. 安全合规设计</h3><p>支持本地部署或私有 AI 模型对接，避免敏感数据外流；提供日志记录能力，可追踪 AI 调用过程与结果，满足企业数据安全与合规审计需求。</p><h2>三、典型应用场景：赋能多行业智能数据处理</h2><p>GcExcel V9.0 的 AI 功能已深度适配企业高频业务场景，让智能分析融入数据处理全流程：</p><ul><li><strong>客户反馈分析</strong>：批量处理电商评论、APP 反馈，通过 AI.QUERY 提取核心诉求，AI.TEXTSENTIMENT 量化满意度，快速定位产品优化方向。</li><li><strong>跨境业务协同</strong>：通过 AI.TRANSLATE 实现多语言订单报表、客户合同的实时翻译，消除跨地区沟通障碍，提升业务效率。</li><li><strong>市场调研数据整理</strong>：对多渠道调研问卷中的开放文本回答，用 AI.QUERY 按主题分类，AI.TEXTSENTIMENT 分析倾向，快速形成数据洞察。</li><li><strong>内部管理优化</strong>：分析员工满意度调查中的文本反馈，自动识别正面/负面评价及核心诉求，为企业管理决策提供数据支持。</li></ul><h2>四、功能效果预览</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574974" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><p>（说明：展示表格中客户评论数据通过 AI.QUERY 函数自动分类为“情感倾向”和“讨论主题”的结构化结果，标注公式与输出对应关系）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574975" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>（说明：展示英文文本批量翻译为日语的表格效果，呈现单文本与批量翻译的公式调用方式及结果）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047574976" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>（说明：展示产品评论通过情感分析函数输出“Positive/Negative/Neutral”标签的效果，标注关键评论与情感结果的对应关系）</p><h2>五、结语</h2><p>GcExcel V9.0 的 AI 功能，彻底打破了传统表格工具的功能边界，让服务器端电子表格引擎不仅能处理数值计算，更能深度理解和分析文本数据。无论是客户反馈处理、跨境业务协同，还是市场调研分析，开发者都能通过简单的公式调用，快速实现智能化数据处理，大幅降低开发成本、提升业务效率。</p><h2>扩展链接</h2><p><a href="https://link.segmentfault.com/?enc=Rp1r7fRkTABH89tDmASEwA%3D%3D.Se%2F8dW%2BWbMLv00hfxS77RoxXFjS1h917N6i8m0J5u8HPAfRezU2v%2Be3%2Bl2tnWVzZPMXkigOCKXpHV0dIHTj8GL7FLvPhnF9O2PgRIhYbcms%3D" rel="nofollow" target="_blank">针对 Excel 的 Java API 组件</a></p>]]></description></item><item>    <title><![CDATA[五大主流CRM品牌核心能力横向对比：从闭环到协同的全维度拆解 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047574992</link>    <guid>https://segmentfault.com/a/1190000047574992</guid>    <pubDate>2026-01-27 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数字化转型中，CRM已从“销售工具”升级为“全链路协同平台”。本文选取<strong>超兔一体云、Oracle CX、Capsule CRM、智赢云CRM、橙子CRM</strong>五大主流品牌，围绕<strong>线索到回款闭环、后端供应链管理、协同工具对接</strong>三大核心场景，结合<strong>流程、数据、易用性</strong>多维度对比，为企业选型提供决策依据。</p><h2>一、对比框架说明</h2><p>本次对比聚焦4大核心维度、12项细分指标，覆盖企业从“获客”到“复购”的全生命周期需求：</p><ol><li><strong>线索到回款闭环</strong>：流程完整性、自动化能力、行业合规性；</li><li><strong>后端供应链管理</strong>：库存/采购/财务联动、上下游协同；</li><li><strong>协同工具对接</strong>：企业微信/钉钉的集成深度、数据同步能力；</li><li><strong>综合适配性</strong>：行业适配、易用性、成本投入。</li></ol><h2>二、核心能力深度对比</h2><h3>（一）线索到回款闭环：从“流程覆盖”到“智能驱动”</h3><p>线索到回款是CRM的核心价值，其能力差异直接决定销售效率与风险控制能力。以下通过<strong>流程覆盖、自动化、合规性</strong>三个维度对比：</p><h4>1. 对比表格：线索到回款闭环能力</h4><table><thead><tr><th>品牌</th><th>覆盖流程</th><th>自动化能力</th><th>合规性支持</th><th>典型场景</th></tr></thead><tbody><tr><td>超兔一体云</td><td>线索→分配→跟进→订单→回款→财务</td><td>智能分配/自动应收拆分/凭证生成</td><td>通用场景</td><td>全业态中小到中大型企业</td></tr><tr><td>Oracle CX</td><td>线索→商机→报价→订单→ERP协同</td><td>SFA/CPQ/自动同步ERP库存</td><td>金融/医疗合规审查</td><td>大型复杂行业（如制造）</td></tr><tr><td>Capsule CRM</td><td>线索→培育→商机→合同→回款</td><td>线索评分/阶段自动提醒</td><td>基础报价审批</td><td>中小企业轻量级管理</td></tr><tr><td>智赢云CRM</td><td>潜在客户→报价→合同→回款→售后</td><td>自定义阶段提醒/续约提醒</td><td>无明确说明</td><td>销售导向型企业</td></tr><tr><td>橙子CRM</td><td>订单→库存→回款</td><td>库存联动/智能补货</td><td>零售折扣控制</td><td>小型零售/电商企业</td></tr></tbody></table><h4>2. 流程可视化：超兔vs Oracle的闭环差异</h4><p><strong>超兔一体云：全链路原生闭环</strong>（Mermaid流程图）</p><p>暂时无法在飞书文档外展示此内容</p><p><strong>Oracle CX：需ERP协同的复杂闭环</strong>（Mermaid流程图）</p><p>暂时无法在飞书文档外展示此内容</p><h3>（二）后端供应链管理：从“进销存”到“全链路协同”</h3><p>后端管理直接影响企业成本控制与供应链效率，本次对比<strong>库存、采购、财务、上下游</strong>四大模块：</p><h4>1. 对比表格：后端管理能力</h4><table><thead><tr><th>品牌</th><th>库存管理</th><th>采购模型</th><th>财务联动</th><th>上下游协同</th></tr></thead><tbody><tr><td>超兔一体云</td><td>500仓库/多成本/SKU/序列号</td><td>4种模型（缺口/总缺口/一单一采/直发）</td><td>一键生成凭证/业务财务衔接</td><td>OpenCRM全流程协同</td></tr><tr><td>Oracle CX</td><td>需ERP协同/实时库存同步</td><td>ERP采购流程</td><td>ERP财务记账/应收联动</td><td>ERP供应链协同</td></tr><tr><td>Capsule CRM</td><td>基础库存/BOM/订单联动</td><td>简单采购流程</td><td>合同/回款同步财务系统</td><td>API对接ERP</td></tr><tr><td>智赢云CRM</td><td>无</td><td>无</td><td>应收账款/收款计划</td><td>无明确说明</td></tr><tr><td>橙子CRM</td><td>多仓库/批次/库存预警</td><td>一单一采购/智能补货</td><td>订单/回款同步财务</td><td>进销存一体化</td></tr></tbody></table><h4>2. 超兔的智能采购流程（Mermaid流程图）</h4><p>超兔SRM支持<strong>4种采购模型</strong>，覆盖从“需求”到“付款”的全流程自动化：</p><p>暂时无法在飞书文档外展示此内容</p><h3>（三）协同工具对接：企业微信/钉钉的集成深度</h3><p>企业微信/钉钉是企业内部协同的“神经中枢”，CRM的集成能力直接影响跨部门效率：</p><h4>1. 对比表格：协同工具对接能力</h4><table><thead><tr><th>品牌</th><th>同步内容</th><th>提醒功能</th><th>集成深度</th><th>合规性支持</th></tr></thead><tbody><tr><td>超兔一体云</td><td>客户/订单/任务/审批</td><td>线索分配/订单/回款提醒</td><td>深度集成（协同办公）</td><td>无明确说明</td></tr><tr><td>Oracle CX</td><td>集群事件/任务告警</td><td>系统消息推送</td><td>增强包配置（基础通知）</td><td>无</td></tr><tr><td>Capsule CRM</td><td>客户/聊天记录/审批流程</td><td>无明确说明</td><td>会话存档/敏感词预警</td><td>高（合规风控）</td></tr><tr><td>智赢云CRM</td><td>无直接对接</td><td>无</td><td>支持OA模块</td><td>无</td></tr><tr><td>橙子CRM</td><td>多端同步/客户/订单</td><td>库存预警/回款提醒</td><td>基本集成（多端访问）</td><td>零售场景</td></tr></tbody></table><h3>（四）综合能力评估：雷达图分值</h3><p>通过<strong>5项核心指标</strong>（满分10分）评估各品牌的综合实力：</p><table><thead><tr><th>指标</th><th>超兔</th><th>Oracle CX</th><th>Capsule</th><th>智赢云</th><th>橙子CRM</th></tr></thead><tbody><tr><td>线索闭环完整性</td><td>8</td><td>9</td><td>7</td><td>6</td><td>7</td></tr><tr><td>后端管理深度</td><td>7</td><td>10</td><td>5</td><td>4</td><td>6</td></tr><tr><td>协同工具集成</td><td>9</td><td>6</td><td>8</td><td>5</td><td>7</td></tr><tr><td>行业适配性</td><td>8</td><td>10</td><td>7</td><td>6</td><td>8</td></tr><tr><td>易用性</td><td>9</td><td>7</td><td>10</td><td>8</td><td>9</td></tr></tbody></table><h2>三、脑图总结：各品牌核心定位</h2><p>暂时无法在飞书文档外展示此内容</p><h2>四、选型建议</h2><ol><li><strong>超兔一体云</strong>：适合<strong>需要全流程闭环+协同</strong>的中小到中大型企业，覆盖全业态，性价比高；</li><li><strong>Oracle CX</strong>：适合<strong>大型复杂行业</strong>（如制造/金融），需与ERP协同，强调合规与供应链；</li><li><strong>Capsule CRM</strong>：适合<strong>中小企业轻量级管理</strong>，易用性强，侧重销售流程标准化；</li><li><strong>智赢云CRM</strong>：适合<strong>销售导向型企业</strong>，侧重售后与续约提醒；</li><li><strong>橙子CRM</strong>：适合<strong>小型零售/电商</strong>，进销存一体化，满足基本订单-库存-回款需求。</li></ol><h2>五、结论</h2><p>CRM的选型核心是“匹配企业当前阶段与未来增长需求”：</p><ul><li>若需“全链路闭环”，选超兔；</li><li>若需“大型复杂供应链”，选Oracle；</li><li>若需“轻量级易用”，选Capsule；</li><li>若需“零售进销存”，选橙子。</li></ul><p>未来，CRM的竞争将聚焦“全链路数据打通”与“AI智能驱动”，企业需优先选择“开放生态 + 可扩展”的平台，以应对业务增长的不确定性。希望企业能够根据自身实际情况，审慎考量，明智地选择适合自己的CRM系统，从而借助其强大功能，提升运营效率，优化客户关系管理，在激烈的市场竞争中抢占先机，实现可持续的发展与增长。相信在正确的CRM系统助力下，企业定能乘风破浪，创造更加辉煌的业绩。</p>]]></description></item>  </channel></rss>