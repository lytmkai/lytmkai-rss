<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[外汇 API 接入与使用指南：实时数据获取经验分享 阶段性debugger ]]></title>    <link>https://segmentfault.com/a/1190000047525184</link>    <guid>https://segmentfault.com/a/1190000047525184</guid>    <pubDate>2026-01-07 11:08:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作为在金融科技领域混了五六年的开发者。最近几年，我越来越依赖外汇 API 来处理全球外汇数据，尤其是那些提供外汇实时报价的接口。记得刚入行时接了个需求，要在我们电商系统里加实时汇率换算。听起来挺简单是吧？结果光是选 API 就看了七八家，文档看得头昏眼花，折腾了好几天才搞定接入。现在回想起来，那时候要是早点知道一些靠谱的外汇实时数据接口，就能省不少事儿。今天我就来分享一下我的经验，重点聊聊外汇 API、汇率 API、外汇实时行情的接入和使用，特别是如何获取全球外汇数据和外汇实时报价。今天我把我使用的外汇 API 的经验都捋出来，如果你也正想要接实时汇率 API，看完这篇文章能省不少时间。</p><h2>为什么选择外汇 API？我的亲身经历</h2><p>外汇市场变化飞快，尤其是实时汇率数据，能帮你做出及时决策。我最早接触外汇 API 是因为一个跨境电商的项目，需要实时显示美元兑欧元、人民币的汇率。手动查数据太 low 了，用 API 一接入，就能自动更新。好处显而易见：数据准确、更新及时，还支持全球外汇数据覆盖，比如主要货币对如 USD/EUR、GBP/JPY 等。</p><p>从我的经验看，选择 API 时要优先考虑免费或低成本的汇率 API，尤其是那些提供外汇实时数据接口的。别一上来就选贵的商用版（如果你预算相当充足当我没说了哈哈），先用免费的练手。注意，有些 API 有调用限额，比如每天 1000 次，够个人用，但商用项目要升级。</p><h2>如何选择外汇 API</h2><p>很多人一上来就问“有没有好用的外汇 API”，这问题太笼统了。你得先想清楚：</p><ul><li><strong>实时性</strong>：实时到底要多“实时”？ 跨境电商页面展示，可能 5 分钟更新一次都够了；但你要做外汇交易工具，那得是秒级甚至毫秒级数据。</li><li><strong>数据覆盖</strong>：API 的覆盖范围是否包含你业务需要的货币对。</li><li><strong>历史数据</strong>: 历史数据要吗？ 如果要做汇率走势图或者是外汇交易工具等需求，就得找提供历史数据的。</li><li><strong>成本控制</strong>：预算多少？ 免费的有，一个月几千美金的也有，差距大了去了。</li></ul><p>捋清楚了上面这些条件，你就能选出一个最合适的 API。</p><h2>试过的几个 API，真实感受</h2><h3>1. ExchangeRate API（新手友好型）</h3><p><strong>优点</strong>：免费额度给得大方，一个月 1500 次请求，对中小项目完全够用。文档清晰，5 分钟就能跑通第一个请求。<br/><strong>坑点</strong>：免费版的实时数据其实有延迟，说是实时，实际可能慢几分钟。<br/><strong>适合</strong>：个人项目、初创公司试水、展示型需求。</p><h3>2. iTick API （稳定实惠型）</h3><p><strong>优点</strong>：数据源靠谱（欧洲央行）货币对覆盖全面，有免费档，付费也不算贵。接口设计很规范，支持 RESTful API 和 WebSocket。<br/><strong>坑点</strong>：免费版有调用频次限制，webSocket 连接数量和订阅产品都有限制。<br/><strong>适合</strong>：正经商业项目，需要稳定服务的。</p><h3>3. OANDA（专业玩家型）</h3><p><strong>优点</strong>：数据质量高，延迟极低，覆盖货币对最全。<br/><strong>坑点</strong>：贵！而且需要申请，不是随便注册就能用。<br/><strong>适合</strong>：金融交易类应用，不差钱的企业。</p><h2>我是怎么接入的</h2><p>综合考量我最后选了 iTick 的专业付费版，这里是我的接入代码，加了不少实际踩坑后的优化：</p><h3>获取外汇实时汇率</h3><p>先安装 requests（如果你本地环境没装，用<code>pip install requests</code>）。</p><pre><code class="python">import requests
import json

# 定义API端点和参数
url = "https://api.itick.org/forex/tick"
params = {
    "region": "GB",
    "code": "EURUSD"  # 使用 EURUSD 获取欧元兑美元，然后计算美元兑欧元
}

headers = {
    "accept": "application/json",
    "token": "your_token"  # 从官网获取你的 token
}

# 发送GET请求
response = requests.get(url, params=params, headers=headers)

# 检查响应
if response.status_code == 200:
    data = response.json()
    if data['code'] == 0:
        ld = data['data']['ld']  # EURUSD 的最新价（1 EUR = ld USD）
        usd_to_eur = 1 / ld if ld != 0 else 0  # 计算 1 USD = ? EUR
        print("外汇实时报价（USD to EUR）:")
        print(json.dumps({
            'amount': 1.0,
            'base': 'USD',
            'date': '2026-01-06',
            'rates': {'EUR': usd_to_eur}
        }, indent=4))  # 美化输出
        # 示例输出: {'amount': 1.0, 'base': 'USD', 'date': '2026-01-06', 'rates': {'EUR': 0.85}}
    else:
        print(f"API 错误: {data['msg']}")
else:
    print(f"错误: {response.status_code}")</code></pre><p>这个代码超级简单，运行后就能看到最新的汇率。我的经验：加个 try-except 块处理网络异常，避免程序崩掉。</p><h3>获取外汇历史数据</h3><pre><code class="python">import requests
import json

# 你的API token
token = "your_token"  # 从iTick官网获取

# 定义API端点
url = "https://api.itick.org/forex/kline"
params = {
    "region": "GB",
    "code": "EURUSD",  # 示例使用 EURUSD
    "kType": "8",  # 日K线 (8 为日K)
    "limit": "10",  # 获取最近 10 条
    "et": "1751328000000"  # 示例截止时间戳
}

headers = {
    "accept": "application/json",
    "token": token
}

# 发送GET请求
response = requests.get(url, params=params, headers=headers)

# 处理响应
if response.status_code == 200:
    data = response.json()
    if data['code'] == 0:
        print("全球外汇历史数据（EURUSD 日K线示例）:")
        print(json.dumps(data['data'], indent=4))  # 打印 K线数据
        # 示例: [{'t': 1741239180000, 'o': 1.0803, 'h': 1.08053, 'l': 1.0803, 'c': 1.08051, 'v': 293, 'tu': 316.57132}, ...]
    else:
        print(f"API 错误: {data['msg']}")
else:
    print(f"错误: {response.status_code}")</code></pre><h2>给几个实在建议</h2><ol><li><strong>起步阶段用免费的</strong>：别一上来就买付费服务，先用免费版跑通流程。</li><li><strong>一定要加缓存</strong>：汇率不会每秒变很多次，缓存能大大减少 API 调用。</li><li><strong>监控！监控！监控！</strong>：记录 API 调用成功率、延迟，设个告警。</li><li><strong>准备降级方案</strong>：API 不可能 100%可靠，要有后备计划。</li></ol><h2>最后说几句</h2><p>接外汇 API 本身技术难度不大，关键是选对 API、处理好异常、控制好成本。我最后选了专业付费套餐，每天缓存+实时更新结合，稳定跑了几个月了。</p><p>其实真正麻烦的其实是业务逻辑：怎么展示汇率（保留几位小数）、什么时候更新、不同国家用户看到什么货币等等。这些业务问题比技术问题更费时间。</p><p>希望我的经验能帮你少走点弯路。有啥问题评论区聊，我尽量回答。</p><blockquote>温馨提示：本文仅供代码参考，不构成任何投资建议。市场有风险，投资需谨慎</blockquote><p>GitHub：<a href="https://link.segmentfault.com/?enc=3wNPi%2F9X5H6H7xquELOS7Q%3D%3D.oxtrHqStfqF6PhikKWcjsdUMQskc2qtf9g4dNty1Qqo%3D" rel="nofollow" target="_blank">https://github.com/itick-org/</a></p>]]></description></item><item>    <title><![CDATA[鸿蒙 HarmonyOS 6 | ArkUI (04)：数据展示 List 列表容器 LazyFor]]></title>    <link>https://segmentfault.com/a/1190000047525611</link>    <guid>https://segmentfault.com/a/1190000047525611</guid>    <pubDate>2026-01-07 11:07:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>前言</h3><p>回想一下我们每天使用手机的场景，无论是清晨浏览新闻资讯，午休时刷短视频，还是睡前查看电商平台的购物订单，这些海量信息的呈现方式无一例外都是列表。对于用户而言，手指在屏幕上滑动的流畅度直接决定了对一款应用的第一印象，哪怕出现几毫秒的掉帧或者瞬间的白屏，都可能让用户心生退意。而对于我们开发者来说，构建一个能跑通的列表界面似乎是入门必修课，甚至在很多初级教程中，只需要几行简单的代码就能把数组里的数据渲染到屏幕上。</p><p>但是，当我们把数据量从几十条增加到一千条、一万条时，那个曾经丝般顺滑的界面可能会突然变得卡顿、手机发烫，甚至因为内存溢出而直接闪退。这就是初级工程师与资深开发者的分水岭所在。</p><p>在鸿蒙 HarmonyOS 6 的开发里，掌握 <strong>List</strong> 列表容器仅仅是起点，而真正能让我们驾驭海量数据、实现极致性能体验的核心钥匙，在于理解并精通 <strong>LazyForEach</strong> 懒加载机制。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047522478" alt="" title=""/></p><h3>一、 走出全量渲染的舒适区与性能陷阱</h3><p>在 ArkUI 的组件体系中，创建一个列表是极其符合直觉的。我们通常会使用 <strong>List</strong> 容器组件，它就像是一个能够滚动的长条盒子，而在盒子内部，我们通过 <strong>ListItem</strong> 来承载具体的每一行内容。对于刚接触鸿蒙开发的同学来说，最顺手的工具肯定是 <strong>ForEach</strong> 循环渲染。它的逻辑非常简单直接，我们给它一个数组，它就老老实实地遍历数组中的每一个元素，然后为每一个元素创建一个对应的组件。这种全量渲染的模式在数据量较少时，比如只有二三十条设置项，是完全没有问题的，代码写起来也清晰易懂。</p><pre><code>// 1. 数据源
@State dataList: string[] = ['核心概念', '组件通信', '路由管理', '状态管理'];

build() {
  // 2. List 容器：类似滚动的长条盒子
  List({ space: 12 }) { 
    // 3. ForEach：循环渲染
    // 参数1：数据源
    // 参数2：组件生成函数
    // 参数3：键值生成函数 (性能关键，用于唯一标识)
    ForEach(this.dataList, (item: string) =&gt; {
      
      // 4. ListItem：承载具体的每一行
      ListItem() {
        Text(item)
          .fontSize(16)
          .width('100%')
          .padding(15)
          .backgroundColor(Color.White)
          .borderRadius(10)
      }
      
    }, (item: string) =&gt; item) // 唯一 Key，避免不必要的重新渲染
  }
  .width('100%')
  .height('100%')
  .padding(16)
}</code></pre><p>我们必须警惕这种舒适区往往也是性能的陷阱。<strong>ForEach</strong> 的工作机制决定了它会一次性加载所有的数据。</p><p>如果服务器给我们返回了一万条历史订单数据，如果我们直接使用 <strong>ForEach</strong> 进行渲染，ArkUI 就会尝试在瞬间创建一万个 <strong>ListItem</strong> 组件以及它们内部的所有子组件。这不仅会瞬间占满应用的内存，大量的布局计算和节点创建任务还会死死地堵塞主线程，导致用户看到页面长时间的白屏或者严重的掉帧。这就是为什么很多新手的应用在测试阶段数据少时跑得飞快，一上线遇到真实数据就崩溃的原因。</p><p>我们必须意识到，屏幕的显示区域是有限的，用户同一时间能看到的可能只有五六条数据，为那些还未出现在屏幕上的九千多条数据提前创建组件，是一种极大的资源浪费。</p><h3>二、 LazyForEach 的按需渲染哲学与数据契约</h3><p>为了解决全量渲染带来的性能灾难，HarmonyOS 引入了 <strong>LazyForEach</strong> 组件。</p><p>它的名字非常直观，<strong>Lazy</strong> 代表懒惰，但在计算机科学中，这里的懒惰意味着极致的高效。<strong>LazyForEach</strong> 的核心哲学是 <strong>按需渲染</strong>。它只会为当前屏幕可见区域以及可视区域附近少量的预加载区域创建组件。当用户向上滑动屏幕时，下方的列表项即将进入屏幕，<strong>LazyForEach</strong> 才会向数据源请求数据并创建新的组件；而当上方的列表项滑出屏幕并远离可视区域时，它们所占用的组件资源会被销毁或者回收进入复用池。这种机制就像是一个滑动的窗口，无论我们的底层数据有多少万条，内存中实际存在的组件数量始终维持在一个很小的、稳定的范围内。</p><p>这种高性能是有门槛的。与 <strong>ForEach</strong> 直接接收一个简单的数组不同，<strong>LazyForEach</strong> 要求我们提供一个实现了 <strong>IDataSource</strong> 接口的数据源对象。这对于很多习惯了直接操作数组的前辈来说，可能是一个思维上的转变。在懒加载的模式下，ArkUI 框架不再直接持有数据的所有权，它变成了一个单纯的索取者。它会不断地问我们：总共有多少条数据？第 5 条数据是什么？作为开发者，我们需要构建一个能够回答这些问题的数据管理代理。</p><p>在实际的工程实践中，我们绝不会在每一个页面里都去手写一遍 <strong>IDataSource</strong> 的实现逻辑。那样不仅代码冗余，而且极易出错。成熟的做法是封装一个 <strong>BasicDataSource</strong> 基类。这样做的好处是，我们可以把那些枯燥的监听器管理代码、数据的增删改查通知逻辑全部封装起来，在具体的业务代码中，我们只需要关注数据的获取本身。这不仅让代码更加整洁，也符合面向对象编程的复用原则。</p><p>我们可以看看下面这个通用的基类封装，它是我们构建高性能列表的基石。</p><pre><code>// BasicDataSource.ets - 通用数据源基类
class BasicDataSource&lt;T&gt; implements IDataSource {
  private listeners: DataChangeListener[] = [];
  private originDataArray: T[] = [];

  // 告诉框架总共有多少条数据
  totalCount(): number {
    return this.originDataArray.length;
  }

  // 告诉框架指定索引的数据是什么
  getData(index: number): T {
    return this.originDataArray[index];
  }

  // 注册监听器，框架通过它来感知数据变化
  registerDataChangeListener(listener: DataChangeListener): void {
    if (this.listeners.indexOf(listener) &lt; 0) {
      this.listeners.push(listener);
    }
  }

  // 注销监听器
  unregisterDataChangeListener(listener: DataChangeListener): void {
    const pos = this.listeners.indexOf(listener);
    if (pos &gt;= 0) {
      this.listeners.splice(pos, 1);
    }
  }

  // 初始化或重置数据
  public setData(data: T[]) {
    this.originDataArray = data;
    this.notifyDataReload();
  }

  // 通知所有监听器：数据重载了
  notifyDataReload(): void {
    this.listeners.forEach(listener =&gt; {
      listener.onDataReloaded();
    });
  }
}</code></pre><h3>三、 键值生成与缓存策略的博弈</h3><p>当我们封装好了数据源基类后，使用 <strong>LazyForEach</strong> 时还有两个技术细节决定了最终的成败：一个是键值生成规则，一个是缓存数量。<strong>LazyForEach</strong> 的第三个参数是 <strong>keyGenerator</strong>，它的作用是为每一个数据项生成一个唯一的身份证。很多开发者容易忽视这一点，甚至为了省事直接使用数组的 <strong>index</strong> 索引作为 Key。这在列表内容静态不变时或许能侥幸过关，可一旦涉及到数据的插入或删除，就会出问题。</p><p>因为当我们删除列表头部的元素时，后面所有元素的索引都会发生变化，这会导致框架误判所有组件都需要更新，从而触发全量的销毁和重建，让懒加载的复用机制彻底失效。正确的做法是永远使用数据对象中本身具备的唯一标识，比如用户 ID 或者订单号。这样无论数据如何在数组中移动，框架都能通过这个唯一的 Key 识别出它，从而复用已经存在的 UI 组件。</p><p>除了 Key，<strong>cachedCount</strong> 属性则是调节性能与体验的杠杆。它控制着列表的预加载数量。默认情况下，<strong>LazyForEach</strong> 只加载屏幕内的项目。但这会带来一个问题，如果用户滑动得非常快，新的列表项还没来得及渲染，屏幕边缘就会出现短暂的白块。我们可以设置 <strong>cachedCount</strong>，比如将其设置为 5，意味着框架会在屏幕可视区域的上下方额外预先渲染 5 个列表项。这样当用户滑动时，内容已经准备好了，体验就会非常丝滑。但这个数值也不是越大越好，过大的缓存数量又会重新带来内存压力，我们需要在流畅度和内存占用之间找到一个平衡点。</p><h3>四、 实战</h3><p>为了让大家更直观地理解这些概念如何协同工作，我们来构建一个完整的新闻列表场景。这个示例代码不仅包含了一个继承自泛型基类的具体业务数据源，还演示了如何在 <strong>List</strong> 组件中正确配置 <strong>LazyForEach</strong> 和 <strong>cachedCount</strong>。你可以直接将这段代码复制到你的项目中，它能够毫无压力地处理上千条数据的渲染。</p><pre><code>import { promptAction } from '@kit.ArkUI';

// 1. 定义数据模型
// 在实际项目中，这里通常对应后端 API 返回的 JSON 结构
class NewsData {
  id: string;
  title: string;
  summary: string;
  timestamp: string;

  constructor(id: string, title: string, summary: string) {
    this.id = id;
    this.title = title;
    this.summary = summary;
    this.timestamp = new Date().toLocaleTimeString();
  }
}

// 2. 引入我们之前定义的通用数据源基类
// (为了代码的完整性，这里再次展示简化版，实际开发中请抽离为单独文件)
class BasicDataSource&lt;T&gt; implements IDataSource {
  private listeners: DataChangeListener[] = [];
  private originDataArray: T[] = [];

  totalCount(): number {
    return this.originDataArray.length;
  }
  getData(index: number): T {
    return this.originDataArray[index];
  }
  registerDataChangeListener(listener: DataChangeListener): void {
    if (this.listeners.indexOf(listener) &lt; 0) {
      this.listeners.push(listener);
    }
  }
  unregisterDataChangeListener(listener: DataChangeListener): void {
    const pos = this.listeners.indexOf(listener);
    if (pos &gt;= 0) {
      this.listeners.splice(pos, 1);
    }
  }
  public setData(data: T[]) {
    this.originDataArray = data;
    this.notifyDataReload();
  }
  notifyDataReload(): void {
    this.listeners.forEach(listener =&gt; {
      listener.onDataReloaded();
    });
  }
}

// 3. 具体的业务数据源
class NewsDataSource extends BasicDataSource&lt;NewsData&gt; {
}

@Entry
@Component
struct LazyListPerformancePage {
  // 实例化我们的数据源对象
  private newsDataSource: NewsDataSource = new NewsDataSource();
  
  // 模拟生成数据的辅助函数
  private generateMockData(count: number): NewsData[] {
    let dataList: NewsData[] = [];
    for (let i = 0; i &lt; count; i++) {
      const id = i.toString();
      dataList.push(new NewsData(
        id, 
        `鸿蒙 HarmonyOS 6 高性能新闻标题 #${id}`, 
        `这是第 ${i} 条新闻的详细摘要。我们正在使用 LazyForEach 技术来确保列表滑动的极致流畅。`
      ));
    }
    return dataList;
  }

  // 页面即将显示时加载数据
  aboutToAppear(): void {
    // 模拟加载 1000 条数据
    const mockData = this.generateMockData(1000);
    this.newsDataSource.setData(mockData);
  }

  build() {
    Column() {
      // 顶部标题栏
      Text('高性能资讯流')
        .fontSize(24)
        .fontWeight(FontWeight.Bold)
        .width('100%')
        .padding(20)
        .backgroundColor('#F1F3F5')

      // List 容器开始
      List({ space: 12 }) {
        // 核心：使用 LazyForEach 替代 ForEach
        LazyForEach(this.newsDataSource, (item: NewsData) =&gt; {
          ListItem() {
            // 列表项的具体布局
            Column({ space: 8 }) {
              Row() {
                Text(item.title)
                  .fontSize(16)
                  .fontWeight(FontWeight.Medium)
                  .maxLines(1)
                  .layoutWeight(1)
                  .textOverflow({ overflow: TextOverflow.Ellipsis })
                
                Text(item.timestamp)
                  .fontSize(12)
                  .fontColor('#999999')
              }
              .width('100%')
              .justifyContent(FlexAlign.SpaceBetween)

              Text(item.summary)
                .fontSize(14)
                .fontColor('#666666')
                .maxLines(2)
                .textOverflow({ overflow: TextOverflow.Ellipsis })
                .lineHeight(20)
            }
            .width('100%')
            .padding(16)
            .backgroundColor(Color.White)
            .borderRadius(12)
            .shadow({ radius: 4, color: '#1A000000', offsetY: 2 })
          }
          .onClick(() =&gt; {
            promptAction.showToast({ message: `点击了新闻 ID: ${item.id}` });
          })
        }, (item: NewsData) =&gt; item.id) // 关键点：使用唯一的 id 作为 Key
      }
      .width('100%')
      .layoutWeight(1) // 让列表占据剩余的所有高度
      .cachedCount(4)  // 关键点：预加载屏幕外的 4 项，防止快速滑动白块
      .padding({ left: 16, right: 16, bottom: 16 })
      .divider({ strokeWidth: 0 }) // 隐藏默认分割线
      .scrollBar(BarState.Off)     // 隐藏滚动条让视觉更清爽
    }
    .width('100%')
    .height('100%')
    .backgroundColor('#F1F3F5')
  }
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047522479" alt="" title="" loading="lazy"/></p><h3>五、 总结</h3><p>回顾我们探讨的内容，从简单的 <strong>ForEach</strong> 到高性能的 <strong>LazyForEach</strong>，这不仅仅是 API 的更换，更是一种开发思维的进阶。</p><p>我们学会了如何通过 <strong>IDataSource</strong> 建立数据与视图的契约，如何通过 <strong>cachedCount</strong> 平衡内存与流畅度，以及如何利用稳定的 <strong>Key</strong> 来榨干框架的复用能力。</p><p>在鸿蒙 HarmonyOS 6 的全栈开发中，列表性能优化是衡量一个应用质量的基石。一个能够流畅加载万级数据的列表，往往比花哨的动画更能赢得用户的信任。</p>]]></description></item><item>    <title><![CDATA[2026-01-07 GitHub 热点项目精选 程序员锋仔 ]]></title>    <link>https://segmentfault.com/a/1190000047525695</link>    <guid>https://segmentfault.com/a/1190000047525695</guid>    <pubDate>2026-01-07 11:06:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>🌟 2026-01-07 GitHub Python 热点项目精选(14个)</h2><blockquote>每日同步 GitHub Trending 趋势，筛选优质 Python 项目，助力开发者快速把握技术风向标～</blockquote><hr/><h3>📋 项目列表（按 Star 数排序）</h3><h4>1. <a href="https://link.segmentfault.com/?enc=7yXPupLnfEPuN9Qu5QHEBg%3D%3D.CTvJqwK7jNKYIOm1n%2Fcjm4HEZlXXMHyY5G%2Fnh6FpoEkQbz2fITtFsiYvN3VjvD8m" rel="nofollow" target="_blank">microsoft/BitNet</a></h4><blockquote>由微软推出的项目，可能涉及网络技术、安全或数据处理等领域，从名称来看，或许与位操作或网络架构相关，旨在通过创新技术提升相关领域的性能或安全性等</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 25351（今日+766）</td></tr><tr><td>Fork 数</td><td>🔄 2017</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=9B7k6sslrrCJnfTkBjaeyg%3D%3D.%2BPqWrFUSFgrIh8kEaohFXqzV8DRMwHfl0%2FtsUNwho9I5MCrltrVhpsgsSv5GeMnj" rel="nofollow" target="_blank">https://github.com/microsoft/BitNet</a></td></tr></tbody></table><hr/><h4>2. <a href="https://link.segmentfault.com/?enc=jcQhgG%2FbL%2BuQj9Ub2kiZvA%3D%3D.FNxl779dccXKpBmyXi9ePhBjAwvdALJnTvqkmPQSJ6K4O1GOEn126dztOPDcdyoWONVkfqNVh5UDkfPvz6PGoQ%3D%3D" rel="nofollow" target="_blank">LuckyOne7777/ChatGPT-Micro-Cap-Experiment</a></h4><blockquote>这是一个关于 ChatGPT 在微型市值领域实验的项目，可能探索如何利用 ChatGPT 的能力来分析、预测或优化微型市值企业的相关业务，比如市场策略、财务分析等</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 7073（今日+68）</td></tr><tr><td>Fork 数</td><td>🔄 1531</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=B4Mzzl438SnTKTk4UokDQg%3D%3D.L1IAwecH7NQId6DyyRdUhvyq9iqoXvv05t8ET2LgOG%2F2hT4T%2BMA9Xk7CmAm4YPjSgS9tntQ8RhrxT3UX6lhgng%3D%3D" rel="nofollow" target="_blank">https://github.com/LuckyOne7777/ChatGPT-Micro-Cap-Experiment</a></td></tr></tbody></table><hr/><h4>3. <a href="https://link.segmentfault.com/?enc=COUEwc4yrqrn21M2NFbGLQ%3D%3D.buVSwyAKk9bv8m5hBrXnSKasIXDsNNvWea8m%2FFAx1jHhdABNqn38mXWAyLK%2BzJbl" rel="nofollow" target="_blank">virattt/ai-hedge-fund</a></h4><blockquote>专注于人工智能对冲基金的项目，可能研究如何运用人工智能算法进行投资决策、风险控制以及市场趋势预测等，通过技术手段提升对冲基金的收益和稳定性</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 44924（今日+262）</td></tr><tr><td>Fork 数</td><td>🔄 7886</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=4QmlGafHoKg%2FAHamHLAQGA%3D%3D.G0d5q5OWWaJQ%2FDEPth3Gx%2FmC87DjapHCMzWYA%2Bp0NKM4NkvdGl9zYRBCOdn9bpi%2F" rel="nofollow" target="_blank">https://github.com/virattt/ai-hedge-fund</a></td></tr></tbody></table><hr/><h4>4. <a href="https://link.segmentfault.com/?enc=BGHtp6LRjsYypx5yiHKVww%3D%3D.2iq4hjBdj8GagRy%2FNnojHpA0GjIDADl%2FtMSmehle0h91tyPeTUP7Re%2BQwOip1sea" rel="nofollow" target="_blank">docling-project/docling</a></h4><blockquote>Docling 项目，从名称推测可能与文档语言（docling）相关，或许致力于构建一种用于描述、处理或生成文档的特定语言或框架，以提高文档开发和管理的效率</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 49193（今日+168）</td></tr><tr><td>Fork 数</td><td>🔄 3420</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=xrX7HlZsQ92jbHRx%2BJdMHg%3D%3D.5g0rV8o9W1shBlfwqPMbofnMThRZtv%2Fm2%2B9RkBAwlnY8dJGNAmCzuWthJsIarKdk" rel="nofollow" target="_blank">https://github.com/docling-project/docling</a></td></tr></tbody></table><hr/><h4>5. <a href="https://link.segmentfault.com/?enc=3GJmw5nGu36GLksuGAxp3w%3D%3D.KtdQKchkTqV3GCvVIjGDvf5Xrx8KrxdGyEsCJ%2BeqAVOpLsQkgYZe%2Fcb7fc6OePfC" rel="nofollow" target="_blank">ahujasid/blender-mcp</a></h4><blockquote>与 Blender（一款流行的三维建模软件）相关的项目，可能是为 Blender 开发的插件或工具，用于增强 Blender 的功能，如模型创建、动画制作等，提升用户在三维创作中的体验</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 14838（今日+32）</td></tr><tr><td>Fork 数</td><td>🔄 1429</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=%2B%2F0QioY%2BHnkPxzVn4za9HQ%3D%3D.hbieVf%2BcVhg33qvsx%2BVSvaiGKvhFb0fnoS6sg5TIZyG%2F7fI9OQvbiEAkrvkd1pj5" rel="nofollow" target="_blank">https://github.com/ahujasid/blender-mcp</a></td></tr></tbody></table><hr/><h4>6. <a href="https://link.segmentfault.com/?enc=6b6UQkULNxz24qNk0Bta9A%3D%3D.37DqQoSz8C4gkesQgiatgBYEc8l91ovG%2BPm2M1UFnCMWz5x5pDU1KO6GVtBv2dXE" rel="nofollow" target="_blank">strands-agents/sdk-python</a></h4><blockquote>这是一个 Python SDK（软件开发工具包）项目，可能用于开发与智能体（agents）相关的应用，比如机器人控制、自动化任务等，为开发者提供便捷的接口和工具来构建相关功能</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 4814（今日+12）</td></tr><tr><td>Fork 数</td><td>🔄 586</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=CyK5ENUvwBdlxoETi7fp%2Bw%3D%3D.vH85D9nCGpld78%2B4iWMTWbWlR%2Byi7pto2a6yDluGN1kzMBX04clH5CUA0ANj79ro" rel="nofollow" target="_blank">https://github.com/strands-agents/sdk-python</a></td></tr></tbody></table><hr/><h4>7. <a href="https://link.segmentfault.com/?enc=F1YrQpmzMnAcwudl81WEtA%3D%3D.1YayaihLmGoi6IRnLYNNXPcwf%2FiSfiO1a8TPeuYtnAXhMP%2BfWHR9q0AnyeK6u7scF%2FiLhiurs2JtJDTar0SIlw%3D%3D" rel="nofollow" target="_blank">DrewThomasson/ebook2audiobook</a></h4><blockquote>项目名称表明其功能是将电子书转换为有声书，通过技术手段实现文本到语音的转换，方便用户在不同场景下获取阅读内容，可能还支持多种格式和定制化功能</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 16501（今日+203）</td></tr><tr><td>Fork 数</td><td>🔄 1328</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=RN1Hnph%2FlFEpJo65rjaCmA%3D%3D.uVc1mByqiaMkZp5cWYqPW8E9rLzUvc4ab2qgLtPE4UfAAWeb8lQ1MvXCjgF6aVTaKG4pbJGC6DO6%2Fp8vOKaIZw%3D%3D" rel="nofollow" target="_blank">https://github.com/DrewThomasson/ebook2audiobook</a></td></tr></tbody></table><hr/><h4>8. <a href="https://link.segmentfault.com/?enc=yUD1mU3VU0Q5fa5JEGramw%3D%3D.V7KUAL7Ihpg%2Flkc2aZVydKejRgoMDS4h6quNIlnsfMxnaj0tX8KJN6bo6bkY7Aup" rel="nofollow" target="_blank">isaac-sim/IsaacLab</a></h4><blockquote>IsaacLab 项目，可能与 Isaac 模拟器或相关技术有关，或许用于构建虚拟实验室环境，进行机器人仿真、物理实验模拟等，帮助研究人员和开发者在虚拟场景中进行测试和开发</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 5963（今日+15）</td></tr><tr><td>Fork 数</td><td>🔄 2879</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=IRuDm6pp3mOJGeWEGhp3yw%3D%3D.mJjdAPo9nshBbLL90vPbYiJS5iXEvAF8h9tKfRn6f9lfYF5abas2HozQ%2BRHN%2FfVB" rel="nofollow" target="_blank">https://github.com/isaac-sim/IsaacLab</a></td></tr></tbody></table><hr/><h4>9. <a href="https://link.segmentfault.com/?enc=WdBKSdCJMZNgvYSYC6NQBg%3D%3D.zuezKiR8xVa7mK953c3sojLlZcfNHLN%2F4r6ltBr0DYr1KrpB6hHTur4K%2FZFltaiV" rel="nofollow" target="_blank">OpenBB-finance/OpenBB</a></h4><blockquote>OpenBB 是一个金融领域的开源项目，可能提供金融数据处理、分析工具，以及交易策略开发等功能，帮助用户更好地理解和参与金融市场</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 57395（今日+358）</td></tr><tr><td>Fork 数</td><td>🔄 5561</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=4UlWkoxKObqBG%2BrTd42Apw%3D%3D.Wkg0FHiofwlYaHzArxKcmNEiVvvxaHHIX2hHGkmCghk7xGQIbeObBMwlTWmyOyEO" rel="nofollow" target="_blank">https://github.com/OpenBB-finance/OpenBB</a></td></tr></tbody></table><hr/><h4>10. <a href="https://link.segmentfault.com/?enc=rdIj69ARwjtAL0tecC4oxw%3D%3D.%2FL5NermGoNpc2fB88B3E0dvsbQtxdbrtFrHTlERcPoUM%2Bo67A%2Fazrx6FIS25zG14" rel="nofollow" target="_blank">Asabeneh/30-Days-Of-Python</a></h4><blockquote>这是一个以 30 天为周期的 Python 学习项目，通过每天的学习任务和实践，帮助初学者快速掌握 Python 编程语言的基本知识和技能，适合编程入门者</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 55050（今日+90）</td></tr><tr><td>Fork 数</td><td>🔄 10606</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=jZQfWFDpeWdUlUIobecSZQ%3D%3D.jY%2FL7UCzDXVuh6yCSHXmRauazv8ou5ZnAQ96FesL4K%2F86HtNenThxf8RrFlGC0%2Fw" rel="nofollow" target="_blank">https://github.com/Asabeneh/30-Days-Of-Python</a></td></tr></tbody></table><hr/><h4>11. <a href="https://link.segmentfault.com/?enc=hGzVNIRPmVzOoYWQpW%2BzSQ%3D%3D.fY63%2B%2Fsqtrpv%2BfK9nlwLpGmY5KuSYoWkR%2FtnxuzEMznhl9NZmDkfoGhevDj4YTF9" rel="nofollow" target="_blank">HKUDS/LightRAG</a></h4><blockquote>由香港大学数据科学团队开发的 LightRAG 项目，可能是一种轻量级的检索增强生成（RAG）模型，用于自然语言处理任务，如文本生成、问答系统等，旨在提高效率和性能</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 27064（今日+68）</td></tr><tr><td>Fork 数</td><td>🔄 3847</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=YDZFYC3%2BC9p0Sc7%2BpA6FEw%3D%3D.Wj3U%2F89d%2B%2BZqVhOfh9NiX3UmG%2BHFrgwqVnqJhk8grlWoj8KyE4Ks20lFD1jFJ4UZ" rel="nofollow" target="_blank">https://github.com/HKUDS/LightRAG</a></td></tr></tbody></table><hr/><h4>12. <a href="https://link.segmentfault.com/?enc=1Fj0Vg4nwwiKC6Xd6FyFJg%3D%3D.MiZR2wiWqvdAZ%2FuDBp5wZgg4lqRVYoZDJnRWbCwYWBL4R%2BBmnbBoU1R6diti%2FvkV" rel="nofollow" target="_blank">microsoft/VibeVoice</a></h4><blockquote>微软的 VibeVoice 项目，可能与语音识别、语音合成或语音交互技术相关，探索如何通过语音技术提升用户体验，应用于智能助手、语音控制等领域</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 20003（今日+366）</td></tr><tr><td>Fork 数</td><td>🔄 2207</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=hO9ZIMJgN85%2BtPt7YR7Wzw%3D%3D.pbk7ynI6rtk0velfaMhFMqmyxAL0iEeXcwO%2FCR70ftQ4FvsZDdMgKCX1TDqejKYt" rel="nofollow" target="_blank">https://github.com/microsoft/VibeVoice</a></td></tr></tbody></table><hr/><h4>13. <a href="https://link.segmentfault.com/?enc=2FgEAAG%2BFd5NEDX%2Fp%2FM46w%3D%3D.mGKer%2FAE9rB397%2BR8PWdSM6DUPB6IYtMLEajeqOqlNYX9%2BqJj0FAzoLWXmLpaatwh0J%2BqFwmp7VZhRBrA50b8YkShHe5826zmoSm3Alxb2o%3D" rel="nofollow" target="_blank">labmlai/annotated_deep_learning_paper_implementations</a></h4><blockquote>该项目提供了深度学习论文的注释实现，帮助研究人员和开发者更好地理解经典和前沿的深度学习模型，通过代码实现和详细注释，促进学术交流和技术应用</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 65175（今日+20）</td></tr><tr><td>Fork 数</td><td>🔄 6571</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=Oe%2BOtrVQauieVRK07vpxrA%3D%3D.Uhk4c1dbvVsgADFwffQmRRhVQAx6apUesYmlAjJISbNwoQmKCv%2BRzJ1imOD9Ajd6y%2B%2BYKgBn7kuid6X6CPn4g2Uerm0rgXtagxiEnAJJtsM%3D" rel="nofollow" target="_blank">https://github.com/labmlai/annotated_deep_learning_paper_implementations</a></td></tr></tbody></table><hr/><h4>14. <a href="https://link.segmentfault.com/?enc=fvemLyKklrgX6%2B7%2FAsS6Gw%3D%3D.MO6wK%2F3xk6CvA8w%2Fs%2BpjLLbZenXEkrA%2FffuLTX8Wxv2TMQCjbm%2BLtL1%2BykHtC9oN0I2Ny781ods3l5GrYolFzg%3D%3D" rel="nofollow" target="_blank">zhaochenyang20/Awesome-ML-SYS-Tutorial</a></h4><blockquote>这是一个关于机器学习系统（ML SYS）的教程集合项目，可能包含丰富的学习资源、实践案例和指导，帮助用户快速掌握机器学习系统的设计、开发和优化</blockquote><table><thead><tr><th>指标</th><th>详情</th></tr></thead><tbody><tr><td>Star 数</td><td>🌟 4932（今日+27）</td></tr><tr><td>Fork 数</td><td>🔄 320</td></tr><tr><td>开发语言</td><td>🐍 Python</td></tr><tr><td>项目地址</td><td><a href="https://link.segmentfault.com/?enc=LF05bh4mft2DpwL%2FFvq1FA%3D%3D.2hxSK1Py9fswHAvH3uQakGnCDW4KuF1cpIFHiGM%2FjTpYILVqlVOqSqdmpz0hS4ve5J%2FuE8eqbR0yX21cveYeKQ%3D%3D" rel="nofollow" target="_blank">https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</a></td></tr></tbody></table><hr/><h3>📝 说明</h3><ul><li>数据来源：GitHub Trending（2026-01-07 每日榜单）</li><li>筛选条件：Python 语言 + 当日热门项目</li><li>自动更新：每日同步最新趋势，建议收藏本文持续关注～</li></ul><h3>⭐ 推荐理由</h3><ol><li>热门项目代表当前技术趋势，学习价值高</li><li>优质项目代码规范，可作为学习参考</li><li>部分项目可直接用于实际开发，提高效率</li></ol>]]></description></item><item>    <title><![CDATA[深入理解 Volatile：C#.NET 内存可见性与有序性 唐青枫 ]]></title>    <link>https://segmentfault.com/a/1190000047525732</link>    <guid>https://segmentfault.com/a/1190000047525732</guid>    <pubDate>2026-01-07 11:05:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>简介</h3><p><code>Volatile</code> 是 <code>C#</code> 中处理内存可见性和指令重排序的关键机制，它提供了对内存访问的精细控制。在并发编程中，<code>volatile</code> 关键字和 <code>Volatile</code> 类都是解决共享变量可见性问题的重要工具。</p><h3>为什么需要volatile？</h3><h4>CPU 缓存导致的 “内存可见性” 问题</h4><p>现代 <code>CPU</code> 为提升性能，会将频繁访问的变量缓存到核心专属的缓存（<code>L1/L2/L3</code>）中，而非每次都读写主内存。这会导致：</p><ul><li>线程 A 修改了共享字段的值（仅写入自己的 <code>CPU</code> 缓存，未同步到主内存）；</li><li>线程 B 读取该字段时，从自己的 <code>CPU</code> 缓存读取（仍是旧值），无法看到线程 A 的修改。</li></ul><h4>编译器 / CPU 的 “指令重排序” 优化</h4><p>编译器（<code>C#</code> 编译器）和 <code>CPU</code> 为提升执行效率，会在不改变单线程逻辑的前提下，调整指令的执行顺序</p><pre><code class="csharp">// 原始代码
bool _isReady = false;
int _data = 100;

// 编译器/CPU可能重排序为：先赋值_data，再赋值_isReady（单线程无影响）
// 但多线程下，线程B可能看到_isReady=true，但_data还是旧值</code></pre><p><code>volatile</code> 的核心作用就是：禁止缓存 + 禁止指令重排序，保证多线程对字段的访问 “所见即所得”。</p><ul><li><p>插入内存屏障（<code>memory barrier</code>）：</p><ul><li><code>Acquire Fence</code>：读取 <code>volatile</code> 字段前，禁止将后续读取提前。</li><li><code>Release Fence</code>：写入 <code>volatile</code> 字段后，禁止将之前写入推迟。</li></ul></li><li>强制每次读写都直接访问主内存，绕过缓存优化。</li></ul><h3>核心定义与语法</h3><h4>语法规则</h4><p><code>volatile</code> 只能修饰字段，且有严格的类型限制，语法如下：</p><pre><code class="csharp">// 正确：修饰实例字段
private volatile bool _isRunning;

// 正确：修饰静态字段
private static volatile int _counter;

// 错误：不能修饰方法/参数/局部变量/属性/常量
public volatile void DoWork() { } // 编译错误
private int VolatileProperty { get; set; } // 编译错误（属性不能加volatile）</code></pre><h4>支持的类型</h4><p><code>volatile</code> 仅支持以下类型（避免 <code>CPU</code> 操作的原子性问题）：</p><ul><li>引用类型（如 <code>object</code>、<code>string</code>、自定义类）；</li><li>值类型：<code>byte、sbyte、short、ushort、int、uint、long、ulong、char、float、bool</code>；</li><li>上述类型的指针（如 <code>int*</code> ）。</li></ul><blockquote>注意：不支持double、decimal、struct（自定义值类型）、DateTime等，这些类型的读写不是原子的，volatile无法保证正确性。</blockquote><h4>等效方法：Volatile.Read/Volatile.Write</h4><p>除了关键字，<code>.NET</code> 还提供 <code>Volatile</code> 静态类的 <code>Read/Write</code> 方法，功能与 <code>volatile</code> 关键字一致，但更灵活（可动态控制读写）：</p><pre><code class="csharp">// 等价于 volatile 修饰的 _isRunning = true
Volatile.Write(ref _isRunning, true);

// 等价于读取 volatile 修饰的 _isRunning
bool current = Volatile.Read(ref _isRunning);</code></pre><h3>核心原理：内存屏障（Memory Barrier）</h3><p><code>volatile</code> 的底层是通过插入内存屏障（<code>Memory Barrier</code>） 实现的：</p><ul><li>读屏障（<code>Load Barrier</code>）：读取 <code>volatile</code> 字段时，插入读屏障，强制 <code>CPU</code> 从主内存读取值，而非缓存；同时禁止将读指令重排序到屏障之前。</li><li>写屏障（<code>Store Barrier</code>）：写入 <code>volatile</code> 字段时，插入写屏障，强制 <code>CPU</code> 将值写入主内存，而非缓存；同时禁止将写指令重排序到屏障之后。</li></ul><h3>基础使用示例</h3><h4>关键字用法</h4><pre><code class="csharp">public class ThreadSafeFlag
{
    private volatile bool _isRunning = true;

    public void Run()
    {
        // 线程1：循环直到标志关闭
        while (_isRunning)
        {
            // 执行工作
            Thread.SpinWait(1000);
        }
        Console.WriteLine("线程停止");
    }

    public void Stop()
    {
        // 线程2：设置标志
        _isRunning = false;
        Console.WriteLine("停止信号已发送");
    }
}</code></pre><p>使用示例：</p><pre><code class="csharp">var flag = new ThreadSafeFlag();
var worker = new Thread(flag.Run);
worker.Start();

Thread.Sleep(100);
flag.Stop();  // 另一个线程能立即看到变化
worker.Join();</code></pre><p>不加 <code>volatile</code>：可能导致 <code>_isRunning</code> 被缓存，线程永远不退出。</p><h4>Volatile 类静态方法（.NET 4.5+ 推荐）</h4><pre><code class="csharp">using System.Threading;

private int _value;

public int ReadValue() =&gt; Volatile.Read(ref _value);
public void WriteValue(int newValue) =&gt; Volatile.Write(ref _value, newValue);</code></pre><ul><li><code>Volatile.Read</code>：带 <code>Acquire</code> 屏障的读取。</li><li><code>Volatile.Write</code>：带 <code>Release</code> 屏障的写入。</li><li>优势：更精确控制屏障方向，比关键字更灵活。</li></ul><h4>双检查锁单例</h4><pre><code class="csharp">public sealed class Singleton
{
    private static volatile Singleton? _instance;

    public static Singleton Instance
    {
        get
        {
            if (_instance == null)
            {
                lock (typeof(Singleton))
                {
                    if (_instance == null)
                        _instance = new Singleton();
                }
            }
            return _instance!;
        }
    }

    private Singleton() { }
}</code></pre><h3>优点与缺点</h3><table><thead><tr><th>方面</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>性能</strong></td><td>极低开销（仅内存屏障），远高于锁</td><td>仍比普通变量慢（禁用部分优化）</td></tr><tr><td><strong>易用性</strong></td><td>简单关键字或方法调用</td><td>语义复杂，易误用</td></tr><tr><td><strong>适用性</strong></td><td>完美用于简单标志位、状态切换、双检查锁</td><td><strong>不能</strong>用于计数器、复合操作</td></tr><tr><td><strong>安全性</strong></td><td>提供必要内存模型保证</td><td>不足以实现复杂同步</td></tr></tbody></table><h3>推荐场景</h3><h4>推荐使用 <code>volatile</code> 的场景：</h4><ul><li>布尔标志（如停止信号 <code>_isRunning</code>）。</li><li>状态枚举（如 <code>Ready/Running/Stopped</code>）。</li><li>引用类型字段的双检查锁单例。</li><li>一写多读（<code>one writer, multiple readers</code>）模式。</li></ul><h4>不推荐使用 <code>volatile</code> 的场景：</h4><ul><li>计数器、累加操作 → 用 <code>Interlocked</code>。</li><li>复杂状态 → 用 <code>lock</code> 或无锁结构。</li><li>64位值（long/double）在32位进程 → 用 <code>Interlocked</code>。</li></ul><h3>Volatile vs Interlocked</h3><table><thead><tr><th>对比项</th><th>Volatile</th><th>Interlocked</th></tr></thead><tbody><tr><td>原子性</td><td>❌</td><td>✅</td></tr><tr><td>内存屏障</td><td>Acquire / Release</td><td>Full Fence</td></tr><tr><td>返回旧值</td><td>❌</td><td>✅</td></tr><tr><td>适用场景</td><td>状态观察</td><td>状态修改</td></tr><tr><td>性能</td><td>更快</td><td>稍慢</td></tr></tbody></table><h3>总结</h3><p><code>volatile</code> 是 <code>.NET</code> 多线程编程中一个低级但关键的工具，适合简单的一写多读标志场景。但绝不能滥用，大多数线程安全需求应优先选择 <code>Interlocked、lock、Lazy&lt;T&gt;</code> 或并发集合。</p><pre><code class="csharp">// 读：Volatile
var state = Volatile.Read(ref _state);

// 写：CAS / Exchange
if (state == A)
    Interlocked.CompareExchange(ref _state, B, A);</code></pre><blockquote>Volatile 是并发程序的“观察者协议”，<br/>Interlocked 才是“修改者协议”。</blockquote>]]></description></item><item>    <title><![CDATA[陪玩搭子系统用户端：功能架构与后端实现指南 DK阿龙 ]]></title>    <link>https://segmentfault.com/a/1190000047525974</link>    <guid>https://segmentfault.com/a/1190000047525974</guid>    <pubDate>2026-01-07 11:04:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3><strong>本文从技术实现角度，深度解析陪玩平台用户端的功能模块设计与实现思路</strong></h3><p><strong>一、技术栈选型建议</strong><br/>后端技术栈<br/>框架：Spring Boot 2.7+ / Go Gin<br/>数据库：Mysql5.6<br/>消息队列：RabbitMQ/Kafka<br/>实时通信：WebSocket + Socket.IO<br/>文件存储：阿里云OSS/腾讯云COS<br/>微服务治理：Spring Cloud Alibaba/Nacos</p><p>前端技术栈<br/>移动端：Uni-vue2.0<br/>Web管理端：Vue 3 + Element Plus<br/>状态管理：Pinia/Redux Toolkit<br/>网络请求：Axios + 拦截器封装</p><p><strong>二、安全与风控设计</strong></p><ol><li>接口安全<br/>JWT Token认证 + 动态刷新机制<br/>敏感操作二次验证（短信/邮箱验证码）<br/>API调用频率限制（Redis + Lua脚本）<br/>SQL注入/XSS攻击防护</li><li>交易安全<br/>金豆变动事务一致性保证<br/>防刷单机制（同一用户限制订单频率）<br/>提现审核流程（人工+自动审核）<br/>资金流水对账系统</li><li>内容安全<br/>UGC内容实时过滤（敏感词库+AI识别）<br/>图片/视频违规检测（对接第三方服务）<br/>聊天内容监控与审核<br/>举报处理流程</li></ol><p><strong>三、部署架构</strong><br/>微服务拆分建议</p><pre><code>┌─────────────────────────────────────────┐
│                API Gateway               │
│           (Spring Cloud Gateway)         │
└───┬────────────┬────────────┬───────────┘
    │            │            │
    ▼            ▼            ▼
┌─────────┐ ┌─────────┐ ┌─────────┐
│ 用户服务  │ │ 订单服务  │ │ 支付服务  │
├─────────┤ ├─────────┤ ├─────────┤
│ - 注册登录│ - 订单创建 │ - 金豆充值 │
│ - 个人中心│ - 订单匹配 │ - 提现处理 │
│ - 关注关系│ - 状态流转 │ - 对账系统 │
└─────────┘ └─────────┘ └─────────┘
    │            │            │
    ▼            ▼            ▼
┌─────────┐ ┌─────────┐ ┌─────────┐
│ 聊天服务  │ │ 推荐服务  │ │ 内容服务  │
├─────────┤ ├─────────┤ ├─────────┤
│ - 实时消息│ - 陪玩推荐│ - 动态发布│
│ - 语音房 │ - 内容推荐│ - 内容审核│
│ - 礼物系统│ - 搜索优化│ - 评论点赞│
└─────────┘ └─────────┘ └─────────┘</code></pre><p><strong>四、开发规范建议</strong></p><ol><li>代码规范<br/>遵循阿里Java开发手册<br/>统一异常处理机制<br/>API版本管理（/api/v1/, /api/v2/）<br/>统一响应格式</li><li>测试策略<br/>单元测试覆盖核心业务逻辑<br/>集成测试验证服务间调用<br/>压力测试模拟高并发场景<br/>自动化回归测试</li><li>CI/CD流程</li></ol><pre><code>代码提交 → 代码审查 → 自动化测试 → 
构建镜像 → 预发环境部署 → 灰度发布 → 
生产环境部署 → 监控告警</code></pre><p><strong>技术要点总结：</strong><br/>实时通信是陪玩平台的核心，需重点优化<br/>推荐算法直接影响用户体验和平台收益<br/>安全风控是平台长期发展的保障<br/>微服务架构便于团队协作和系统扩展</p><p><strong>结语</strong><br/>陪玩平台开发涉及技术点广泛，需要平衡业务快速迭代与系统稳定性。建议采用敏捷开发模式，分阶段实施：<br/>第一阶段：核心功能（用户、订单、支付）<br/>第二阶段：社交功能（动态、聊天、推荐）<br/>第三阶段：高级功能（语音房、算法优化）</p><p>在开发过程中要特别关注数据安全、系统性能和用户体验，建立完善的监控和应急预案，确保平台稳定运行。如需具体模块的详细实现方案或遇到特定技术问题，可进一步探讨。<br/><img width="723" height="382" referrerpolicy="no-referrer" src="/img/bVdnl7V" alt="" title=""/><br/><img width="723" height="413" referrerpolicy="no-referrer" src="/img/bVdmWOo" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[奉劝大家，能去大公司，千万别去小公司 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047525976</link>    <guid>https://segmentfault.com/a/1190000047525976</guid>    <pubDate>2026-01-07 11:04:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>前两天有个粉丝私信我，说自己在一家十几个人的小公司待了三年，技术栈倒是学了不少，但跳槽的时候发现，简历投出去基本石沉大海。</p><p>HR看到他的履历，第一反应就是："你们公司我没听说过啊。"</p><p>这就是小公司最致命的问题——你的平台决定了你的天花板。</p><p>大公司的光环效应是真实存在的。</p><p>同样是做需求开发，在BAT做过的人和在不知名小厂做过的人，市场给的估值完全不一样。</p><p>不是说小公司的人能力差，而是大公司的背书本身就是一种信用背书。</p><p>这不是偏见，这是市场规律。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525978" alt="" title=""/></p><h2>野蛮生长的代价</h2><p>小公司喜欢画饼，说什么"扁平化管理""快速成长""一人顶十人用"。</p><p>听着挺美好，实际上就是没有培训体系、没有规范流程、没有成长路径。</p><p>你以为的快速成长，其实是在用青春试错。</p><p>大公司有成熟的培训体系，新人入职有导师带、有完整的onboarding流程、有系统的技术分享。</p><p>小公司呢？给你扔一堆屎山代码，说"你先看看，有问题随时问"。</p><p>问题是你连问题在哪都不知道，怎么问？</p><p>更要命的是，小公司的技术债往往堆积如山。没有代码规范、没有review机制、没有测试流程。</p><p>你学到的可能不是最佳实践，而是各种野路子。</p><p>等你跳槽到大厂，发现自己养成的习惯全是坏毛病，还得花时间重新学。</p><h2>制度是纸老虎</h2><p>小公司最喜欢跟你讲"我们是一家人"。</p><p>但真到利益面前，你会发现这个"家"的规矩都是老板说了算。</p><p>加班没有加班费，因为"大家都是为了公司好"。</p><p>项目延期要背锅，因为"你是负责人啊"。</p><p>想请个年假，老板一句"现在正是关键时期"就把你噎回去。</p><p>劳动合同、五险一金、年终奖，这些在大公司是标配的东西，在小公司可能都要打折扣。</p><p>大公司不一样。制度再不完美，至少是公开透明的。你的绩效考核有标准、晋升有通道、福利有保障。</p><p>HR部门不是摆设，劳动法也不是一纸空文。</p><p>你可以吐槽大公司的官僚主义，但至少规则是明确的，不会朝令夕改。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525979" alt="" title="" loading="lazy"/></p><h2>资源差距是降维打击</h2><p>小公司永远在为生存焦虑。</p><p>今天拿到一笔融资，明天可能就传出裁员的消息。</p><p>你做的项目可能下个月就被砍掉，你负责的产品可能永远看不到上线那天。</p><p>大公司的容错率高太多了。一个项目失败了，还有其他项目可以做。一个部门不行了，还可以内部转岗。</p><p>你有机会接触到真正的大流量、大数据、大并发场景，这些经验是小公司给不了的。</p><p>更别说那些看得见摸得着的资源了。</p><p>大公司的技术栈是业界领先的，基础设施是完善的，你想用什么工具基本都有。</p><p>小公司呢？服务器能省则省，工具能白嫖就白嫖，出了问题还得你自己想办法。</p><h2>写在最后</h2><p>不是说小公司一无是处，也不是说大公司就是天堂。</p><p>但对于大部分打工人来说，尤其是刚入行的年轻人，大公司提供的平台、资源、培训、规范，是你职业生涯最好的起点。</p><p>你可以在大公司学到规范的做事方法，积累有含金量的项目经验，建立高质量的人脉网络。</p><p>等你真正有了能力、有了资源、有了人脉，那时候再去小公司折腾、去创业，才是真正的降维打击。</p><p>反过来呢？在小公司摸爬滚打几年，想往上走的时候发现，大厂的门槛已经够不着了。</p><p>这个选择题，其实没那么难。</p>]]></description></item><item>    <title><![CDATA[Vue开发必考：defineComponent与defineAsyncComponent，你真的掌握]]></title>    <link>https://segmentfault.com/a/1190000047526052</link>    <guid>https://segmentfault.com/a/1190000047526052</guid>    <pubDate>2026-01-07 11:03:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是前端大鱼。今天聊点实在的——Vue 3里两个名字很像但用途完全不同的函数：<code>defineComponent</code>和<code>defineAsyncComponent</code>。</p><p>你是不是也曾经在代码里见过它们，然后心里嘀咕：“这俩货有什么区别？我该用哪个？”</p><p>别急，今天我用最直白的话给你讲明白。</p><hr/><h2>先看名字，其实已经剧透了</h2><p><strong><code>defineComponent</code></strong> - <strong>定义</strong>组件  <br/><strong><code>defineAsyncComponent</code></strong> - 定义<strong>异步</strong>组件</p><p>名字已经很明显了对不对？一个是普通组件的“身份证”，一个是需要“等会儿再加载”组件的特殊通行证。</p><hr/><h2>defineComponent：每个组件的“出生证明”</h2><p>咱们先说说你天天在用的<code>defineComponent</code>。</p><p><strong>它就是个登记处</strong>。你写的每个Vue组件，都需要在它这里“登记注册”，告诉Vue：“嘿，这是个正经组件，给个合法身份。”</p><h3>两种常见写法</h3><p><strong>传统写法（现在用得少了）：</strong></p><pre><code class="javascript">import { defineComponent } from 'vue'

export default defineComponent({
  name: 'MyButton',
  props: { type: String },
  setup(props) {
    // 你的逻辑
    return { /* 返回给模板用的东西 */ }
  }
})</code></pre><p><strong>现代写法（95%项目都在用）：</strong></p><pre><code class="html">&lt;script setup&gt;
// 注意！这里没有显式调用 defineComponent
// 但Vue编译器在背后帮你调用了

defineProps({ type: String })
// 直接写逻辑，清爽！
&lt;/script&gt;</code></pre><blockquote><strong>重要知识点</strong>：当你用<code>&lt;script setup&gt;</code>时，虽然没写<code>defineComponent</code>，但Vue编译器在打包时<strong>自动给你加上了</strong>。</blockquote><h3>它主要干什么用？</h3><ol><li><strong>给TypeScript提供类型提示</strong>（最重要的功能）</li><li>统一组件定义规范</li></ol><p>说白了，<code>defineComponent</code>就是组件的<strong>基础建设</strong>，没有它，你的组件在Vue世界里就是“黑户”。</p><hr/><h2>defineAsyncComponent：性能优化的“秘密武器”</h2><p>现在来聊聊今天的主角——<code>defineAsyncComponent</code>。</p><p><strong>这是能让你的应用加载速度翻倍的家伙。</strong></p><h3>它解决了什么问题？</h3><p>想象一下这个场景：你的电商网站有个“用户订单分析”页面，里面用了一个超级复杂的图表库，代码有500KB。</p><p>如果用户只是来首页看商品，<strong>为什么要把图表库的代码也一起下载下来？</strong></p><p>这就是<code>defineAsyncComponent</code>要解决的问题：<strong>“你需要的时候，我再给你加载。”</strong></p><h3>基本用法：简单到不可思议</h3><pre><code class="javascript">import { defineAsyncComponent } from 'vue'

// 就这么简单！
const HeavyChart = defineAsyncComponent(() =&gt;
  import('./components/HeavyChart.vue')  // 这是一个独立的代码块
)</code></pre><p>用了这个，<code>HeavyChart</code>组件会被打包成<strong>独立的文件</strong>，只有当你真正要用它的时候，浏览器才会去下载这个文件。</p><h3>高级用法：给用户更好的体验</h3><p>更专业的用法可以配置加载状态：</p><pre><code class="javascript">const AsyncPopup = defineAsyncComponent({
  loader: () =&gt; import('./ExpensivePopup.vue'),
  loadingComponent: LoadingSkeleton,    // 加载时显示骨架屏
  errorComponent: ErrorDisplay,         // 加载失败显示错误提示
  delay: 200,                           // 延迟200ms再显示loading
  timeout: 3000                         // 加载超时时间（3秒）
})</code></pre><h3>实际项目中最常见的用法</h3><p>其实你<strong>可能已经在用</strong>异步组件了，只是没意识到：</p><pre><code class="javascript">// 在Vue Router路由配置里
const routes = [
  {
    path: '/dashboard',
    // 看！这就是异步组件加载
    component: () =&gt; import('@/views/Dashboard.vue')
  }
]</code></pre><p>Vue Router的<code>import()</code>语法，底层就是用的<code>defineAsyncComponent</code>。</p><hr/><h2>一图看懂区别</h2><table><thead><tr><th>方面</th><th>defineComponent</th><th>defineAsyncComponent</th></tr></thead><tbody><tr><td><strong>做什么</strong></td><td>给组件上户口</td><td>给组件发“按需加载”许可证</td></tr><tr><td><strong>加载方式</strong></td><td>跟主包一起加载</td><td>独立分包，用时才加载</td></tr><tr><td><strong>性能影响</strong></td><td>增加主包体积</td><td><strong>减少首屏体积，加快加载</strong></td></tr><tr><td><strong>你用得多吗</strong></td><td>天天用（或间接用）</td><td>路由懒加载时就在用</td></tr></tbody></table><hr/><h2>什么时候该用哪个？</h2><p>记住这个简单的原则：</p><h3>用 defineComponent（或<code>&lt;script setup&gt;</code>）</h3><p><strong>所有常规组件都用这个</strong>，这是默认选择。</p><h3>用 defineAsyncComponent</h3><p>在以下三种情况用它：</p><ol><li><strong>路由页面组件</strong>（必须用！这是性能优化底线）</li><li><strong>体积大的非首屏组件</strong>（如图表、编辑器、PDF预览）</li><li><strong>用户操作才显示的组件</strong>（如复杂弹窗、侧边栏）</li></ol><hr/><h2>我项目里的真实案例</h2><p>之前接手一个复杂的后台管理系统，首屏加载要<strong>5秒多</strong>。我做了三件事：</p><ol><li>把所有路由组件改为异步加载</li><li>把报表页的复杂图表组件异步加载</li><li>把“帮助文档”弹窗异步加载</li></ol><p><strong>改造后，首屏加载降到2秒</strong>。用户打开系统就能操作，图表和文档等需要时才加载。</p><p><strong>这就是异步组件的威力</strong>——不是让代码跑更快，而是让浏览器<strong>少干活</strong>。</p><hr/><h2>一个容易踩的坑</h2><p>注意！异步组件<strong>默认没有包裹<code>&lt;Suspense&gt;</code></strong>。如果你需要在加载时显示fallback内容，要手动处理：</p><pre><code class="html">&lt;template&gt;
  &lt;Suspense&gt;
    &lt;template #default&gt;
      &lt;AsyncUserProfile /&gt;
    &lt;/template&gt;
    &lt;template #fallback&gt;
      &lt;div&gt;加载中...&lt;/div&gt;
    &lt;/template&gt;
  &lt;/Suspense&gt;
&lt;/template&gt;</code></pre><p>或者直接在<code>defineAsyncComponent</code>里配<code>loadingComponent</code>。</p><hr/><h2>总结</h2><ul><li><strong><code>defineComponent</code></strong> 是<strong>定义</strong>组件，给组件合法身份</li><li><strong><code>defineAsyncComponent</code></strong> 是<strong>优化加载</strong>组件，提升用户体验</li></ul><p>它们俩的关系就像：</p><ul><li><code>defineComponent</code> = 造一辆车</li><li><code>defineAsyncComponent</code> = 决定这辆车是随时能开，还是需要时才从车库取出来</li></ul><p>在现在的前端开发中，<strong>路由级别的异步加载已经是标配</strong>。如果你的项目还没做这个优化，今天下班前就能加上，立竿见影。</p><hr/><p><strong>今日思考：</strong>  <br/>你的项目里有哪些“重型”组件可以做成异步加载？在评论区分享你的优化思路吧。</p><p>如果觉得有用，转发给你的团队小伙伴，一起提升用户体验。</p><hr/><p>关注我的公众号" <strong>大前端历险记</strong>"，掌握更多前端开发干货姿势！</p><p>本文由<a href="https://link.segmentfault.com/?enc=x3%2F%2B1UzVfHRQ1XC6zzg%2BAA%3D%3D.0GqaH2CgYNZCKhUbTgW%2Bbcx7jTqfMP1kbNF8l8CzMkQ%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[【节点】[NormalStrength节点]原理解析与实际应用 SmalBox ]]></title>    <link>https://segmentfault.com/a/1190000047526103</link>    <guid>https://segmentfault.com/a/1190000047526103</guid>    <pubDate>2026-01-07 11:03:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=8TRYxproC9qT1sjYLpYG1w%3D%3D.wYd%2BuEE%2FKV0u7ucYLeWMFO%2B1pj3gOoKbr9%2F7TJabJQ0ckKR2joYctnfMfiOASFLzHo%2FC2l2KjbtUL1uZ3b4AHHTJh7NZXiJp8swJaF%2B3qnPOeAv5IEKGqm9bCy%2FIulSqRiBS4QD42eb1VH6BDF%2BRS%2BDBHkeRPnlb5ykFo9fg0M72c%2BvKUqoLZ5S%2BO%2FRmZzd5oL9dTAj8HXQ87jyrnvaBmZiWxXptbuWlZqisFkUE%2FpI%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong></blockquote><h2>节点功能概述</h2><p>NormalStrength节点是Unity URP Shader Graph中用于精确控制法线贴图强度的核心工具。该节点通过<strong>Strength</strong>参数线性调节法线贴图的凹凸表现，在保持法线向量物理正确性的前提下实现细节的动态调整。其核心价值体现在：</p><ul><li><strong>强度无损控制</strong>：当Strength值为1时完全保留原始法线信息，为0时返回空白法线贴图</li><li><strong>光照兼容性</strong>：通过数学变换保持法线向量归一化，确保光照计算准确性</li><li><strong>性能优化</strong>：避免重新计算法线贴图，仅通过强度参数实现动态效果</li><li><strong>实时调节能力</strong>：支持在运行时动态调整强度参数，为游戏中的动态材质效果提供可能</li></ul><h2>端口与参数详解</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526105" alt="" title=""/></p><h3>端口配置</h3><table><thead><tr><th>端口名称</th><th>方向</th><th>数据类型</th><th>绑定</th><th>描述</th></tr></thead><tbody><tr><td>In</td><td>输入</td><td>Vector 3</td><td>无</td><td>输入法线向量</td></tr><tr><td>Strength</td><td>输入</td><td>Float</td><td>无</td><td>强度调节值（0-1范围）</td></tr><tr><td>Out</td><td>输出</td><td>Vector 3</td><td>无</td><td>调整后的法线向量</td></tr></tbody></table><h3>参数特性</h3><ul><li><strong>强度范围</strong>：0-1的线性区间，0表示完全平坦表面，1表示原始法线强度</li><li><strong>数学原理</strong>：通过<code>In.rg * Strength</code>修改XY分量（切线空间中的水平偏移），同时用<code>lerp(1, In.b, saturate(Strength))</code>保持Z分量的稳定性</li><li><strong>物理正确性</strong>：确保输出向量始终为单位长度，避免光照计算异常</li><li><strong>插值精度</strong>：使用lerp函数确保在强度变化时Z分量的平滑过渡，避免视觉突变</li></ul><h2>技术原理解析</h2><h3>数学公式推导</h3><p>该节点的核心运算可表示为：</p><p><code>Out = (In.rg * Strength, lerp(1, In.b, saturate(Strength)))</code></p><p>其中：</p><ul><li><strong>In.rg</strong>：法线向量的XY分量（切线空间中的凹凸方向）</li><li><strong>Strength</strong>：强度系数，控制凹凸程度</li><li><strong>lerp函数</strong>：在原始Z分量和1之间插值，保持向量长度</li><li><strong>saturate函数</strong>：确保强度值始终在有效范围内，防止数值溢出</li></ul><h3>生成代码解析</h3><p>Unity提供的HLSL实现展示了其底层逻辑：</p><pre><code class="c">void Unity_NormalStrength_float(float3 In, float Strength, out float3 Out)
{
    Out = {precision}3(In.rg * Strength, lerp(1, In.b, saturate(Strength)));
}</code></pre><p>该代码通过<code>precision</code>修饰符确保浮点精度，<code>saturate(Strength)</code>将强度限制在0-1范围，防止数值溢出。在实际编译中，<code>precision</code>会根据目标平台自动选择half或float精度，确保性能与质量的平衡。</p><h2>应用场景与实战案例</h2><h3>材质细节动态调节</h3><p><strong>场景</strong>：武器磨损程度可视化</p><ul><li>通过动画控制Strength参数，实现从崭新到磨损的渐变效果</li><li>配合法线混合节点，组合不同磨损程度的法线贴图</li><li>结合时间轴工具，创建基于使用时间的自动磨损效果</li></ul><h3>性能优化策略</h3><p><strong>场景</strong>：移动端LOD切换</p><ul><li>在低细节级别时降低Strength值，减少法线计算量</li><li>结合URP的LOD系统，实现动态性能优化</li><li>针对不同设备性能自动调整强度参数，确保流畅体验</li></ul><h3>动态变形效果</h3><p><strong>场景</strong>：角色肌肉膨胀动画</p><ul><li>使用时间节点驱动Strength参数，创建肌肉膨胀的视觉效果</li><li>配合顶点动画，实现更真实的物理变形</li><li>结合角色状态（如受伤、发力）动态调整不同部位的法线强度</li></ul><h3>环境交互效果</h3><p><strong>场景</strong>：湿润表面效果</p><ul><li>根据环境湿度动态增强法线强度，模拟水珠积聚效果</li><li>配合反射率调整，实现完整的湿润材质表现</li><li>使用距离场控制强度变化，实现局部湿润效果</li></ul><h2>使用技巧与注意事项</h2><h3>强度与光照关系</h3><ul><li>当Strength值超过1时，法线向量可能失去归一化特性，导致光照异常</li><li>建议使用<code>saturate(Strength)</code>确保强度在0-1范围内</li><li>在HDRP管线中，需要考虑强度参数与物理光照模型的兼容性</li></ul><h3>与其他节点配合</h3><ul><li><strong>法线混合节点</strong>：组合多个法线贴图时，NormalStrength可控制各贴图的贡献比例</li><li><strong>高度转法线节点</strong>：用于程序化生成法线时，调整高度图的凹凸强度</li><li><strong>时间节点</strong>：创建动态变化的法线效果，如水面波纹、布料飘动</li></ul><h3>常见问题解决</h3><ul><li><strong>光照闪烁</strong>：检查Strength值是否超出有效范围，或法线贴图是否损坏</li><li><strong>性能下降</strong>：避免在片段着色器中频繁修改Strength参数</li><li><strong>视觉异常</strong>：确保输入法线向量格式正确，避免切线空间转换错误</li></ul><h3>高级应用技巧</h3><ul><li><strong>多层材质融合</strong>：使用多个NormalStrength节点分别控制不同材质层的法线强度</li><li><strong>程序化内容生成</strong>：结合噪声纹理和强度参数，动态生成复杂的表面细节</li><li><strong>艺术风格控制</strong>：通过非标准的强度值范围，创造独特的视觉风格</li></ul><h2>总结与拓展应用</h2><p>NormalStrength节点是URP Shader Graph中实现法线细节控制的利器，其核心优势在于：</p><ul><li>精确调节法线强度而不破坏物理特性</li><li>与URP渲染管线无缝兼容</li><li>支持动态效果和性能优化</li><li>提供艺术家友好的参数控制界面</li></ul><p><strong>拓展应用方向</strong>：</p><ul><li>环境交互：根据角色运动强度动态调整法线细节</li><li>艺术风格化：通过非传统强度值创建独特视觉效果</li><li>跨平台优化：针对不同硬件平台调整强度参数</li><li>实时渲染创新：结合最新的实时渲染技术，探索法线强度在虚拟现实、增强现实等新兴领域的应用潜力</li></ul><p>通过合理利用该节点，开发者可以在保持物理正确性的前提下，创建更具表现力和性能优化的材质效果。随着实时渲染技术的不断发展，NormalStrength节点在材质表现和性能优化方面的重要性将进一步提升。</p><p><strong>未来发展趋势</strong>：</p><ul><li>与机器学习技术结合，实现智能法线强度调节</li><li>支持更复杂的多维度强度控制</li><li>与物理模拟系统深度集成，提供更真实的材质交互效果</li></ul><hr/><blockquote><a href="https://link.segmentfault.com/?enc=a0bQP7i9iYMM13k8sptn6w%3D%3D.XKWfaSMHMOtL7CXViDfKspK4rd1g4Ecy181VqRTm8fKQ8KurDqguf%2Ffcmh2yxJljdGpNMqc1ykz4CjjDUdUnnCJIsH%2FVziTBgjduZbMfqqD%2BKWw8v6iycAKGDHH0xFe22f2ixXQpfpgaZr9b79vD4QpYy0MoGekMlxfhIGkCJVdFhalzF2dyAzrERm1JtIHgg8JhNcxACKhFkz%2FpmLvQJJtzm%2BO78vKzRJUy1OT%2BgwM%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[2025年度大赏 | UWA问答精选 侑虎科技 ]]></title>    <link>https://segmentfault.com/a/1190000047526109</link>    <guid>https://segmentfault.com/a/1190000047526109</guid>    <pubDate>2026-01-07 11:02:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>时光流转，深耕不辍。UWA知识型栏目《厚积薄发 | 技术分享》已悄然陪伴大家走过459个工作周。在这一年的技术征途里，我们依旧扎根真实的游戏开发场景，以问答社区的经验沉淀为基石，又迎来了UWA AI技术的全新助力。</p><p>2025年的十大精选内容，既收录了社区开发者们互助探讨的智慧结晶，也融入了UWA AI基于海量知识沉淀的精准解答，每一篇分享都瞄准开发中的痛点卡点，为大家提供切实可行的破局思路。</p><p>回望这一年，是无数开发者的踊跃分享与积极互动，让技术的价值持续传递；展望2026年，UWA问答社区与UWA AI问答将继续并肩前行，以更专业的内容、更高效的解决方案，陪伴每一位开发者在技术深耕之路上稳步迈进！</p><p>UWA 社区主页：<a href="https://link.segmentfault.com/?enc=bv5kL%2FDKPpO1sKpFOQzX9g%3D%3D.SZrCaL3xW0BaXM3fXYpV4L2u5o%2BKt8Y1iSQlKpnR8Lw%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA QQ群：793972859</p><p><strong>From 问答社区</strong></p><h3><strong>TOP1：如何让GameObject销毁时无论是否Active过，都调用OnDestroy？</strong>&lt;</h3><p><strong>请教一个问题，MonoBehaviour的OnDestroy方法，如果这个GameObject的实例化时就是隐藏的，销毁时不会调用OnDestroy方法。我们项目有些核心逻辑是基于这个OnDestroy做的（时间太久远，耦合太多逻辑不好改出去了）。有没有什么办法，可以让这个GameObject销毁时无论是否Active过，都调用OnDestroy。</strong></p><blockquote><p>A1：要触发OnDestroy，必须要触发过Awake（Awake可以不明着写出来），通常是被实例化或者第一次激活的时候触发Awake，如果一直处于不激活的状态，那么销毁的时候不会自动触发OnDestroy。所以可以在激活的父节点上主动SetActive一下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526111" alt="" title=""/></p><p>A2：1. 一般一个Prefab本身是点亮的，大部分是里面的某些节点才会隐藏。如果是我说的这种情况，应该是在根Prefab摧毁的时候主动调用一次需要控制调用的子组件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526112" alt="" title="" loading="lazy"/></p><ol start="2"><li>如果你真的是根Prefab就隐藏的那更简单了：<br/>（1）做一个工具扫描所有Prefab检查并且修改点亮情况；<br/>（2）编辑器模式下加载完一个Prefab之后检查是否点亮，如果没有则打印Error。</li></ol><p>A3：我们也遇到了这个，主要导致的问题自定义组件清理不掉Lua注册过来的回调，导致返回登录报错。</p><p>我这边解决思路是，在脚本反序列化之后，将自己注册到UI根节点的管理脚本上，由这个管理脚本在Destroy时清理下面的子节点。Base脚本如下图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526113" alt="" title="" loading="lazy"/></p><p>有几个要点说明下：</p><ol><li>我们只处理了UI（其他地方基本不会出这问题），项目的用法上每个UI根节点有必挂的组件，这个组件一定会执行Awake；</li><li>如果组件自身Awake走到了，就不需要额外的销毁管理，需要从管理器中将自己移除；</li><li>反序列化接口中有一堆限制，要小心处理，比如常用的接口只能在主线程执行，如果在子线程要跳过。还有Prefab打开的编辑状态也有一些状态取不到，我这边直接忽略了（因为不常用）。</li></ol></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=ihGq%2Fuw79%2B%2BsjCK7jRqtyA%3D%3D.4n8giSFssw2gHZHNGYV7FXO9iE7K0lAgtR0Rk336ItzSF3q9LBPNXjT7Eiy9REw8QSB7zAR1hQ2Teas49ou1EA%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=454pCyjMdBi2zLR%2BsB2nzA%3D%3D.2P6fVbG4kkE0b3vd3w06xZiqWoBfNX0aMATzQUFob2WJP9K25RWN0f5goYXYkIYUnTGwlw9WDk9Hq6Gz7tq8Kg%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/67d390183d306f3e9d594e7a</a></p><hr/><p><strong>From UWA AI 问答</strong></p><h3><strong>TOP2：Unity中可以禁用GC吗？</strong></h3><p><strong>Unity中可以禁用GC吗？</strong></p><blockquote><p>A：在Unity中，可以禁用垃圾回收（GC），但需谨慎使用，并结合具体场景进行管理。</p><p><strong>禁用GC的方法</strong><br/>Unity提供了通过脚本控制垃圾回收器行为的API：GarbageCollector.GCMode。</p><p>该API支持以下三种模式：</p><ul><li>GarbageCollector.Mode.Enabled：默认模式，自动触发GC。</li><li>GarbageCollector.Mode.Manual：禁用自动GC，但可手动调用System.GC.Collect()或GarbageCollector.CollectIncremental()。</li><li>GarbageCollector.Mode.Disabled：完全禁用GC，包括手动调用也无效。</li></ul><p>将GarbageCollector.GCMode设置为Disabled即可彻底关闭垃圾回收机制。</p><p><strong>使用场景与最佳实践</strong><br/>禁用GC主要用于<strong>性能敏感的关键阶段</strong>，如战斗、加载、过场动画等，以避免GC导致的CPU卡顿（GC Spike）。</p><p>推荐做法：</p><ol><li><strong>预分配内存</strong>：在进入关键阶段前，预先分配所需的所有对象（如对象池、缓存等），确保运行时无新内存分配。</li><li><strong>临时禁用GC</strong>：进入关键阶段时设置GCMode = Disabled，结束后立即恢复为Enabled或Manual。</li><li><strong>手动清理</strong>：恢复GC后，调用System.GC.Collect()回收阶段性内存，防止堆持续增长。</li></ol><p>示例流程：</p><pre><code>// 进入关卡前
AllocateLevelResources(); // 预分配资源
GarbageCollector.GCMode = GarbageCollector.Mode.Disabled;

// 关卡运行中（避免任何new操作）

// 关卡结束
GarbageCollector.GCMode = GarbageCollector.Mode.Enabled;
System.GC.Collect(); // 回收内存</code></pre><p><strong>风险与注意事项</strong></p><ul><li><strong>内存持续增长</strong>：禁用GC后，未引用的对象不会被释放，若持续分配将导致内存溢出。</li><li><strong>必须严格控制内存分配</strong>：禁止使用字符串拼接、装箱、Linq、匿名函数等易产生GC的操作。</li><li><strong>推荐结合0GC编程</strong>：使用StringBuilder、对象池、结构体等技术实现运行时零托管内存分配。</li></ul><p><strong>辅助优化手段</strong></p><ul><li><strong>启用增量GC</strong>（Incremental GC）：在Player Settings中开启Use Incremental GC，将GC工作分摊到多帧，减少单次停顿时间。</li><li><strong>性能监控</strong>：使用Profiler或UWA GOT工具监控GC Alloc值，定位内存分配热点。</li></ul><p>综上，Unity支持运行时禁用GC，但应作为高阶优化手段，在可控内存分配的前提下短期使用，以换取更稳定的帧率表现。</p></blockquote><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP3：为什么不应将游戏逻辑写在FixedUpdate中？</strong></h3><p><strong>请问在向别人解释不要把游戏逻辑放在FixedUpdate，而应正常地放在Update里，除了Input事件外，我还能使用哪些理由呢？</strong></p><blockquote><p>A1：FixedUpdate只处理物理相关的逻辑，Input事件应该放Update。</p><p>A2：如果卡，FixedUpdate会在一帧里执行多次来追上更新次数，可能导致严重的掉帧和发热。若想慢点更新，建议用携程或者用UniTask。</p></blockquote><p><strong>我需要一些硬性的不能做的理由，目前想到的就只有Input事件会重复或被跳过。</strong></p><blockquote><p>A3：那Update里面干什么?而且位置更新不放到Update和渲染匹配不上，看着应该会有顿的感觉。</p><p>A4：FixedUpdate不能动态锁帧，全放里面手机发热发烫耗电也快。</p></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=YEHESLkH5dZim0gHuxRk0g%3D%3D.T3gCzcAcWwusXpkTx5E3xP4DmAtgumLIhSnL%2FELA1%2FFu9wnAK59cjPZ43seUeaR45AxPKAC%2FxJeAEqxvkt48Fg%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=piu8ivAwHWms8%2FCdwXBBBw%3D%3D.wT3ef3UDjM0ocjwFhGdE1d5tfzfzJ5CgEAODAVTzM7jQXio4UvbkcfTkCSgbcERFOnwb31gcpKaOkUr%2FxbLCjg%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/677b429f682c7e5cd61bf9e4</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP4：InstantiateAsync有什么需要特殊处理的吗？</strong></h3><p><strong>请教个问题，InstantiateAsync有什么需要特殊处理的吗？我发现异步实例化出来的角色，好像丢了蒙皮一样，Mesh完全不跟着动画、还有根节点动，始终以T-pose的形态呆在固定位置，而且同一个角色，同步加载是正常的。</strong></p><p><strong>这是我这边的环境：</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047526114" alt="" title="" loading="lazy"/></p><blockquote><p>A1：我们在接近你的版本和更高版本试了下，分别复现了问题和正常加载。查了下可能是类似下面的Unity Bug（异步实例化相关的bug有很多），应该要升一下版本。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526115" alt="" title="" loading="lazy"/></p><p>A2：的确升级Unity版本后ok了。</p><p>A3：没啥问题，效果也比较好、明显，总得来说异步实例化好处多余坏处，我是调研过了并且项目在使用的。</p><p>AsyncInstantiateOperation.GetIntegrationTimeMS() AsyncInstantiateOperation.SetIntegrationTimeMS()可以设置异步实例化的间隔时间，默认是2ms，是Unity 2022.3.20新增加的功能。</p><p>写代码的时候可以考虑Async/Await，当然国际版Unity最新.3.60，国内版.3.54。</p><p>如果你项目没啥特殊可以考虑升级到最新，正常来说如果你没有上线最好到最新或者你得平时关注Unity版本发布日志。</p></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=LmtscRPY63pFh0t8KQExgQ%3D%3D.1C281uuy0uD5CcIeKS0Uy9xtBIch4vqd7lMJMMPus41o0hmkpsA%2FjWXJPPh5ioC7gNz0K4j9TOv7MBdKZHDNLg%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=n5jnrVDiMuMWVcl5NLqKpA%3D%3D.3ZuFwPyOnUhFAqfcbsb7QmkSub5v9AIO6JPCQksOQzLkfQuWBy5wd9aNSpU0VzvRTSXTkyu858dNr6IQcThiSA%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/67ce97913d306f3e9d594e6c</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP5：在Unity转微信小游戏下，如何用Worker实现多线程？</strong></h3><p><strong>想用WebWorker计算Unity里的小球的运动轨迹，请问Unity转微信小游戏的情况下，可以用Worker实现多线程吗？</strong></p><blockquote><p>A：可以参考官方文档：<a href="https://link.segmentfault.com/?enc=nUfXHGoH8FCcpnnNgXhtww%3D%3D.%2BF1%2BKLsBY4K3%2BOvYAcqrQktEoBAZ%2FPbt9Lb3i6ch0lzRTLSxffR24kmNew9rNt3GZuFOshQOSA6Lhn1eHMXS803%2FwgDqkeDuhCCcVkOeGuY%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=UIUu0gpomVrB2%2FbIzByJFg%3D%3D.lyPYU0i0S1c7m1tH0NU%2FTm3fBM%2FLdWkoLsUvJEH%2BZfAr9T661CDJNakTnrvGvj3FUSPWnIcyE%2Bb8tu6Hp%2BZYE68RbYZk6thSz1OpF4%2BdpxE%3D" rel="nofollow" target="_blank">https://developers.weixin.qq.com/minigame/dev/api/worker/wx.c...</a></p><p><strong>功能描述：</strong><br/>创建一个Worker线程。</p><p><strong>参数：</strong><br/>string scriptPath<br/>Worker入口文件的绝对路径</p><p><strong>object options：</strong><br/>可选参数：<br/>属性：useExperimentalWorker<br/>类型：boolean<br/>默认值：false<br/>必填：否<br/>说明：是否使用实验worker。在iOS下，实验worker的JS运行效率比非实验Worker提升数倍，如需在Worker内进行重度计算的建议开启此选项。同时，实验Worker存在极小概率会在系统资源紧张时被系统回收，因此建议配合worker.onProcessKilled事件使用，在Worker被回收后可重新创建一个。<br/>最低版本：2.13.0</p><p><strong>返回值</strong><br/>Worker<br/>Worker对象</p><p><strong>注意事项</strong></p><ul><li>接口使用前需要在game.json（插件为plugin.json）中配置workers字段，表示Worker代码根目录。</li><li>scriptPath为入口文件的绝对路径，且不以/开头。</li><li>目前限制最多只能创建一个Worker，创建下一个Worker前请先调用Worker.terminate。</li><li>多线程Worker指南（小游戏）。</li></ul><p><strong>示例代码</strong></p><pre><code>// 创建普通worker
wx.createWorker('workers/index.js')</code></pre><pre><code>function createNewWorker() {
  const worker = wx.createWorker('workers/index.js', {
    useExperimentalWorker: true
  })
  // 监听worker被系统回收事件
  worker.onProcessKilled(() =&gt; {
    // 重新创建一个worker
    createNewWorker()
  })
}
// 创建实验worker
createNewWorker()</code></pre></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=BSy5N%2B5R6MmGaXqQLbEywA%3D%3D.34mNLmHn47x6Y18XxuFfBCywIWqQhHcuj%2FzjgliGcEZBCdic3pnZh82IkD%2FbQSAt8E67eK1S%2Fg%2FKbXD7XG4PQg%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=ibB4F5ZIHKec%2BvnwYBTybQ%3D%3D.R%2FRyJbV0GLXI6UhFXPqBYY6BItUZ648F4Y%2Bt2CWDSrwSWYOdP%2B9I8bd4NDRDDtNvVAxY1qtcTZdedwK8SDhvhQ%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/67c559603d306f3e9d594e69</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP6：增量GC机制，会导致碎片化更严重吗？</strong></h3><p><strong>请问增量GC这种频繁分配与释放的GC机制，会导致碎片化更严重吗？好像介绍增量GC的资料很少，所以有这种疑问。</strong></p><blockquote>A：对于碎片的产生，理论上增量GC和普通GC应该是没有什么区别的，都是标记-清除。增量GC只是把GC的工作分帧处理了，控制碎片的占用比例更关键的是要减少大块临时分配。</blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=PAfA7TG9mDB478l59o%2B4ag%3D%3D.kgbKnOlfJfdgGqpEdIhq5pxlClK0aG4yN%2F4BMJURlSINXpqQc715suxoz%2FauqSMW7Xnbo%2BR8nKy90VhDG9CDbQ%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=ACS3pCpMzY9Opu%2FHWvUCCw%3D%3D.T%2B3WqIPaP76QTPcHzE4ePKq2ONK6BWZ2sDFiL20JeiYTFGPg3XjD3MTzhqzeuciASNg%2FTeJBzzxTw2tdFLIvCw%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/684fe482682c7e5cd61bfaae</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP7：为什么场景中没有刚体更无碰撞事件，但仍有较高物理耗时？</strong></h3><p><strong>我有个疑问：为什么GOT性能报告和Profiler里看物理模块耗时都很高？但其实这时候场景里已经没有刚体了，按理说不会发生碰撞，只有在代码里用了Raycast判断子弹命中，后来我把设置里的物理模拟关掉，又打了包，物理耗时确实降了下来，看子弹还是能正常命中（因为是代码调的），看粒子里的碰撞也还是能生效。</strong></p><p><strong>我不明白的是：之前那么高的物理耗时是在做什么？可能是什么导致的？是有一些我没留意到的地方用了物理导致的，还是说Unity机制如此？（Unity版本2022.3.59）</strong></p><blockquote>A：的确，当前Unity版本的机制就是这样，很多时候必须要关闭掉物理模拟（指把Simulation Mode改成Script）才可以节省耗时。具体机制是场景中只要Static Collider数量大于0且没有关闭物理模拟就会有耗时，和场景中有没有刚体、有没有发生碰撞事件无直接关联。</blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=J8gqv1xMQoJaShifY1a7Aw%3D%3D.o1r7V3ZgiKN%2BaC1HZCYVyQCr3CKEgJze4BItPzN0oG6LOn6d9wZcfJFpIxQbbZwAybbEjo3ilBng2yW%2Bsa%2FCtw%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=qFTaY%2Fys009uwMl0h%2F8bFw%3D%3D.Y3R7Suzk9sCJAsedNHbt7VsUooUWmAqXvnfaA951%2BFO4WbI06Y%2FO%2BODhJ7f2w6lAOkygvzYPU1EeySAjOwjfCg%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/684688ea682c7e5cd61bfaa0</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP8：如何用GPU Instancing来优化树木草石重复模型？</strong></h3><p><strong>项目场景里的树木草石重复模型非常多，在尝试用GPU Instancing来优化。使用的Shader应该是SRP Batcher和GPU Instancing都支持的，但是材质勾上GPU Instancing后看FrameDebugger里还是走的SRP Batcher，要怎么修改比较好呢？</strong></p><blockquote>A：如果Shader两个都兼容，就会优先走SRP Batcher的。常规做法就是把规划要用GPU Instancing的物体用别的不走SRP Batcher的Shader；也可以考虑仍然用两个都兼容的Shader，但是用MaterialPropertyBlock打断SRP Batcher，这样就会顺延走GPU Instancing了。</blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=7JpRO7NOmC85Ept18NMBQw%3D%3D.Pw25L458qgnSsw1pKaR4IExzfryRk%2F8RavIo3KW71SSVg5lyxu1s1J3ot7sggSbR%2Bu5faSdhr81Ga%2FT1oyGGgA%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=rwi7b%2B1ASi5tmmyFqhUHvQ%3D%3D.Dh9jXawig6tPKj%2Bw49zOQkSbf9l7i4Z2ss8ijOdcQxqe5hFPAmrICDI1beOvPnk%2B2bGMerkOYoqRc9d%2Bmg3%2F3g%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/680dd4d53d306f3e9d594ecc</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP9：如何在运行时获取硬件信息？</strong></h3><p><strong>请问怎样在运行时获取硬件信息？一般参考其中哪些信息进行分级比较合理？</strong></p><blockquote><p>A：一般是用SystemInfo里的一些接口，如图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526116" alt="" title="" loading="lazy"/></p><p>真机上输出效果如图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526117" alt="" title="" loading="lazy"/></p><p>一般用其中deviceType、deviceName去找对应的硬件档次划分；或者直接graphicsDeviceName给到的GPU型号进行画质分级。</p></blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=mYu7VvUfG2EEB%2FirBFoQtQ%3D%3D.LJblwq0zhp2p7BR1KajBpF8wXXALyFQMxfL5sBbPOLmQ7K9FZcOTs3M2lsnKUPJp%2B7KjF0%2Bi8AGZurAIMvQHdw%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=YxsKwU2sQjiMR4F%2BRhPfnw%3D%3D.UvJK0I7uWhQtty%2B9a2kM%2F0G5nvSoAwTmJfcHa3oDvzqswysEg1WrgvWIGuhKwlYmIDBDbdzgeeADzos8JkIZFA%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/67f374e1682c7e5cd61bfa35</a></p><hr/><p><strong>From 问答社区</strong></p><h3><strong>TOP10：如何排查优化URP内置Shader冗余？</strong></h3><p><strong>请教一下Shader冗余应该怎么查，似乎好几个Shader运行时都有两份？</strong></p><blockquote>A：资源冗余最常见的原因是AssetBundle没有依赖打包导致的，可以使用UWA的在线AssetBundle检测进行冗余检测先试试。</blockquote><p><strong>测了AssetBundle，确实有冗余，但AssetBundle冗余的Shader和运行时冗余的Shader好像又不一致。实际运行时的冗余都是Hidden/Universal Render Pipeline/xxx。这又是为什么呢？</strong></p><blockquote><p>A：这些是URP的Shader，通常是URP Asset的引用导致的，因为URP Asset会引用这些Shader。而内存中出现两份Shader，说明内存中出现了两个来源路径不一样的URP Asset，通常一份是在PlayerSetting中引用的URP Asset，另外一份可能来自AssetBundle中动态加载的URP Asset。RendererData里面会引用到PostProcessData，PostProcessData就会引用这些Shader，如果代码里面动态加载的AssetBundle里面也有这种资源，也会引用一份Shader进内存，就会造成冗余。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526118" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526119" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047526120" alt="" title="" loading="lazy"/></p></blockquote><p><strong>请问这个怎么处理，直接删去吗？</strong></p><blockquote>A：一般只处理内存占用比较大的即可，其他的内存占用比较小，冗余开销也不大。比如Hidden/Universal Render Pipeline/Uberpost，需要删除其中用不到的Keyword来降低占用；又比如Hidden/Universal Render Pipeline/HBAO，看是否确实要用到，用不到就解除引用。</blockquote><p><strong>欢迎大家转至社区交流：</strong><br/><a href="https://link.segmentfault.com/?enc=An%2BkAxw%2BxNGjVtDQsLPiIg%3D%3D.A5JEel1mUGD9PIUVlvB%2FWLJ%2BafAc%2BDUmKDopbYjmq5%2FxhMsThtoOOuaFqrOFLdVG35BH7eiVrjEUa%2FL1NWz05A%3D%3D" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=wXRLbgCiZS%2B9ZOKBvUmGLA%3D%3D.HBgA%2BQYf6xLVATSOoyGJTwc%2B%2FFeAssMPTZOJah9sro%2FxgL2HYFThf4bJ2F%2BsipG58ea8OohLSTf2IVJaLp07rQ%3D%3D" rel="nofollow" target="_blank">https://answer.uwa4d.com/question/69245652244ce21ce9ec095c</a></p><p><strong>无论是社区里开发者们的互助讨论，还是AI基于知识沉淀的快速反馈，核心都是为了让每一个技术难题都有解、每一次踩坑都有回响。本期分享分别来自UWA AI问答和UWA问答社区，希望这些从真实开发场景中提炼的经验，能直接帮你解决当下的技术卡点，也让你在遇到同类问题时，能更高效地找到破局方向。</strong></p><p>封面图来源于网络</p><hr/><p>今天的分享就到这里。生有涯而知无涯，在漫漫的开发周期中，我们遇到的问题只是冰山一角，UWA社区愿伴你同行，一起探索分享。欢迎更多的开发者加入UWA社区。</p><p>UWA官网：<a href="https://link.segmentfault.com/?enc=EP12DyvQ251ifDfThCb%2BwQ%3D%3D.BHDJni5Em%2BulnFpeitCMMzU%2FGLULlG9yBd5m58qgMDs%3D" rel="nofollow" target="_blank">www.uwa4d.com</a><br/>UWA社区：<a href="https://link.segmentfault.com/?enc=FQOU9AJC7RqqnrHF3fbt1g%3D%3D.d9VTmzbIgxsgrOJDAz1OGCRQsS%2FBOUn%2FEVISP8%2Bvi9k%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA学堂：<a href="https://link.segmentfault.com/?enc=ud0WFv1yX2iaj580c8tOfg%3D%3D.Vac8wmWWrSf9B3sPRucop%2BKdi01smx1yA1MTGoxxmCE%3D" rel="nofollow" target="_blank">edu.uwa4d.com</a><br/>官方技术QQ群：793972859</p>]]></description></item><item>    <title><![CDATA[外汇量化实盘与回测背离？核心问题在API而非策略｜附Python实测代码 Jackyy ]]></title>    <link>https://segmentfault.com/a/1190000047526130</link>    <guid>https://segmentfault.com/a/1190000047526130</guid>    <pubDate>2026-01-07 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作为长期深耕外汇量化领域的开发者，笔者发现一个共性痛点：很多量化策略在回测环节能跑出近乎完美的收益曲线，逻辑校验也无懈可击，但一旦切入实盘，表现就瞬间“拉胯”——收益波动率骤增、交易信号延迟、短线交易频繁出现入场点位偏移。多数开发者初期都会陷入“优化策略逻辑”的误区，反复调试参数却收效甚微，直到多次排查后才发现，行情数据接口的稳定性与传输延迟，才是导致实盘与回测背离的核心症结。<br/>在外汇实盘交易场景中，“毫秒级”差异直接决定交易成败。哪怕是3-5毫秒的接口延迟，都可能导致交易指令执行偏差；而不同数据源的价格一致性差异，更会直接扭曲买卖信号的有效性。笔者曾接触过不少采用免费外汇数据源的团队，普遍反馈存在价格跳空、关键K线数据缺失等问题——这类问题在低频交易中或许影响有限，但在高频交易或日内短线场景下，往往会直接引发不可逆的亏损。</p><p>这一现象也让行业达成共识：相较于单纯优化策略逻辑，选择低延迟、高稳定性的外汇API，对保障实盘表现的性价比更高。笔者团队曾横向测试过多款主流数据接口，发现专注于高精度实时行情的专业API，能显著缩小回测与实盘的表现差距；其中延迟控制优秀、价格一致性强的接口，能让量化策略的执行效果无限贴近回测预期。</p><p>以AllTick API为例，我们通过实盘实测验证：其数据更新速度几乎与市场实时同步，延迟低至可直观感知的级别。对于依赖精准点位触发的短线策略和日内交易而言，这种低延迟优势尤为关键——能有效避免因信号滞后导致的“追高杀低”，大幅提升交易指令的执行精度。</p><p>外汇量化API选型的3个核心技术维度</p><p>基于多次实测经验，笔者总结出量化开发者挑选外汇API的核心评估维度，避开这些坑就能少走80%的弯路：</p><ol><li>数据延迟与刷新频率：实时性是量化策略的核心基础，直接决定指令执行精度，尤其高频交易场景，建议优先选择刷新频率≥10次/秒、延迟≤10ms的接口；</li><li>价格一致性与历史数据完整性：回测结果的可信度、实盘表现的可预判性，均依赖这两个指标——需重点核查不同周期K线数据的连贯性、分笔数据的完整性，避免因数据缺失导致回测失真；</li><li>接口易用性与稳定性：需关注接口的报错率、断线重连机制、文档完善度。频繁报错或掉线会直接中断交易流程，而清晰的开发文档、完善的异常处理机制，能大幅降低集成成本。</li></ol><p>API选型的实操验证方案（避坑关键）</p><p>从开发实操角度，笔者建议采用“小额实盘验证+梯度放大”的方案：先用小额资金接入目标API跑实盘，重点监控3个核心指标——接口延迟波动范围、数据传输稳定性、极端行情下的响应速度；连续运行1-2个交易周期无异常后，再逐步放大策略交易规模。这种方式既能控制试错成本，又能充分验证API在真实交易场景中的可靠性。</p><p>笔者团队采用该方案后，量化策略的回测与实盘表现差距缩小了60%以上，即便在行情剧烈波动的时段，策略也能按照预设逻辑稳健执行，无需频繁人工干预。</p><p>Python调用<a href="https://link.segmentfault.com/?enc=%2BTkcqWDQ%2B8%2BGwzKaQYcZqg%3D%3D.N5ry9kxNWHaxB8kOJLvLZaIWcYUiR%2BSx6IKucKjIg9k%3D" rel="nofollow" target="_blank">AllTick API</a>实测延迟（代码可直接复用）</p><p>以下是笔者团队常用的API延迟实测代码，基于Python实现，可直接复用用于验证接口延迟表现。核心逻辑为调用AllTick API获取EURUSD实时行情，通过时间戳计算请求延迟，代码完全保留原生逻辑，开发者可替换api_key直接测试：</p><pre><code class="python">import requests
import time

# AllTick API 示例接口
url = "https://api.alltick.com/v1/forex/tick"
params = {
    "symbol": "EURUSD",
    "api_key": "你的API_KEY"
}

# 测量请求延迟
start_time = time.time()
response = requests.get(url, params=params)
end_time = time.time()

if response.status_code == 200:
    data = response.json()
    print(f"EUR/USD 实时价格: {data['price']}")
    print(f"请求延迟: {round((end_time - start_time) * 1000, 2)} ms")
else:
    print("请求失败，状态码:", response.status_code)
</code></pre><p>该代码的核心价值的是“轻量化验证”——无需搭建复杂的测试环境，就能快速判断接口的延迟水平。在日常开发中，我们会将该测试逻辑集成到策略监控系统，实时跟踪接口延迟变化，一旦超出预设阈值（如延迟＞20ms）就触发告警，避免因接口问题导致策略失效。</p><p>对量化开发者而言，外汇API绝非“辅助工具”，而是策略落地的核心基础设施。与其在策略逻辑上“死磕”，不如先花时间筛选一款稳定可靠的API——这笔投入带来的，是实盘表现的确定性提升，以及开发效率的优化。如果你的量化策略也存在实盘与回测背离的问题，不妨先从API排查入手，或许能找到突破口。</p>]]></description></item><item>    <title><![CDATA[OV证书申请流程 才高八斗的杯子_dS2Fpp ]]></title>    <link>https://segmentfault.com/a/1190000047525872</link>    <guid>https://segmentfault.com/a/1190000047525872</guid>    <pubDate>2026-01-07 10:01:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>一、什么是OV SSL证书？</h4><p>OV SSL，英语全称是Organization Validation SSL（组织验证型SSL证书），是一种需要验证网站各单位真实身份的数字证书。在申请证书的过程中，OV SSL证书不仅要严格检查网站的域名所有权，还要检查网站的公司身份，包括企业名称、地址、电话等信息的真实性</p><p>OV SSL证书不仅可以加密网站的隐私信息，还可以识别企业组织机构详情，确认企业网站的真实身份，提高网站的可信度。OV SSL证书性价比高，是中小企业网站的最佳选择。</p><h4>二、OV SSL证书的作用与功能</h4><ol><li><strong>数据加密</strong>：使用先进的加密算法，确保用户与网站之间的数据传输安全。</li><li><strong>身份验证</strong>：通过验证网站的组织身份和合法性，向用户展示网站的真实性和可信度。</li><li><strong>提高信任度</strong>：浏览器在访问使用OV SSL证书的网站时，通常会显示安全锁标志，增加用户对网站的信任感。</li></ol><p><img width="489" height="358" referrerpolicy="no-referrer" src="/img/bVdc91G" alt="" title=""/></p><h4><a href="https://link.segmentfault.com/?enc=5WDvN5v0duLJAYk%2FtQkCCQ%3D%3D.ec7vwzeugC797%2B2uYbpZE0Z%2Fcoa5LpTtNUrywLyuQZkGPTtRJCA%2BX8TmgV1AcvxXXwyjs6JcG3Re9YiQmEKcYw%3D%3D" rel="nofollow" target="_blank">三、OV SSL证书的申请方法</a></h4><ol><li><p><strong>选择证书颁发机构（CA）</strong>  ：</p><p>打开<strong>JoySSL</strong>官网，注册一个账号记得填写注册码<strong>230970</strong>获取技术支持。</p></li><li><p><strong>生成CSR文件</strong>：</p><ul><li>CSR（证书签名请求）是申请SSL证书时必需的文件。它包含了公钥和一些关键信息，如域名和组织信息等。</li><li>根据所使用的Web服务器类型（如Apache、Nginx、IIS等），使用相应的工具生成CSR文件。</li></ul></li><li><p><strong>提交申请</strong>：</p><ul><li>将生成的CSR文件以及相关的申请材料（如组织信息、域名信息等）提交给选定的CA机构。</li><li>CA机构会进行域名所有权验证和组织身份验证。这通常包括通过DNS记录验证、文件上传到服务器等方式确认域名所有权，以及通过电话、邮件或直接联系提供的联系方式来确认组织的合法性和存在性。</li></ul></li><li><p><strong>等待审核与签发</strong>：</p><ul><li>CA机构会对提交的信息进行审核。审核通过后，会签发OV SSL证书。</li><li>签发过程可能需要几个工作日的时间，具体时间取决于CA机构的审核流程和工作量。</li></ul></li><li><p><strong>下载与安装证书</strong>：</p><ul><li>审核通过后，登录到CA机构提供的账户系统或邮件中下载已经签发的OV SSL证书文件。</li><li>将证书文件上传到服务器，并在服务器上安装证书文件和私钥文件。安装过程可能涉及编辑服务器的配置文件，以指定证书和私钥的位置。</li></ul></li><li><p><strong>配置服务器与测试</strong>：</p><ul><li>根据服务器软件，编辑配置文件以启用HTTPS，并确保HTTPS配置正确地指向证书文件。</li><li>完成安装后，使用浏览器访问网站，确保HTTPS正常工作，并且浏览器没有任何安全警告。</li></ul></li></ol>]]></description></item><item>    <title><![CDATA[AI 算力是一种需要被定价、对冲和交易的风险资产？ Baihai_IDP ]]></title>    <link>https://segmentfault.com/a/1190000047525904</link>    <guid>https://segmentfault.com/a/1190000047525904</guid>    <pubDate>2026-01-07 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p><strong>编者按：</strong> 当所有人都将 AI 算力视为下一个云计算风口时，我们是否忽略了它本质上是一种需要被定价、对冲和交易的风险资产？</p><p>我们今天为大家带来的这篇文章，作者的核心观点是：前沿AI算力已超出传统云服务范畴，其不确定性、时效性与稀缺性更接近大宗商品与金融衍生品，未来竞争的关键不在优化服务，而在设计承载算力风险的市场机制。</p><p>文章首先指出 AI 算力具有突发性、稀缺性、时效性与路径依赖等特征，使其从“可消费的服务”转变为“需管理的产能风险”；接着以未对冲的训练任务为例，说明金融工具如何为算力需求方提供风险解决方案；进而对比硅谷的“服务抽象”模式与芝加哥/纽约的“风险市场”模式，揭示二者底层逻辑的本质差异；最后提出，真正的产业制高点将是构建算力风险交易的基础设施，而这不仅将重塑 AI 研发与供应的格局，也可能催生新一代市场型机构。</p></blockquote><p><strong>作者 | Dave Friedman</strong></p><p><strong>编译 | 岳扬</strong></p><p>大多数人谈起 AI 算力市场，脑海里浮现的是云计算那套：把稀缺的硬件封装成一个 API，按用量计费，开出账单，再加点调度算法的“魔法”，然后去融一轮资。</p><p>这是硅谷的思维定式，却是一个错误的心智模型。</p><p>前沿 AI 算力不是一种可以随意采购的商品，而是一个企业必须严肃应对的战略风险源。<strong>它是一种十分稀缺、价格波动巨大、且价值随时间迅速流逝的关键资源，需要为其建立基于未来预期的动态风险定价模型，采取措施对冲其价格和可获性风险，并推动形成可灵活交易转让的市场流动性。</strong> 恰当的类比对象不是亚马逊云服务（AWS）或 Snowflake，而是芝加哥商品交易所（CME）、电力交易市场，以及芝加哥和纽约的衍生品交易部门。</p><h2><strong>01 核心问题：算力的产能具有随机性（Compute as Stochastic Capacity）</strong></h2><p>云计算基础设施建立在几个假设之上：</p><ul><li>供给具有弹性。</li><li>需求平稳。</li><li>成本曲线可预测。</li><li>正确的抽象方式是，把一切变成 “服务的消费”。【译者注：云服务商（如 AWS）将其底层复杂的硬件基础设施（服务器、网络、存储）抽象化，向用户呈现为一个简单的、按需取用的 “服务”。】</li></ul><p>处于技术最前沿、最尖端的 AI 研发打破了这些假设。AI 算力具有以下特征：</p><ul><li><strong>突发性</strong>：由非连续的训练任务驱动，而非稳定的网络流量。</li><li><strong>稀缺性</strong>：受限于晶圆产能周期、出口管制以及需耗时数年的电力基础设施建设。</li><li><strong>时效性</strong>：错过一个前沿 AI 模型的训练窗口期，可能导致整个产品周期的落后。</li><li><strong>路径依赖</strong>：成本受能源价格、硬件代际和算法演进的影响。</li></ul><p>这已经不是简单的“服务使用量”问题了，而是一种实实在在会带来损失的产能风险。当你面对的是一种在供给、时机和价格上都充满不确定性的实体资产时，你就不再处于产品设计的范畴了，而是进入了市场机制设计的领域。</p><h2><strong>02 一个简单例子：未进行对冲的训练任务</strong></h2><p>假设某实验室计划在未来 12 到 18 个月内开展一次大规模训练任务。他们尚不确定该训练任务的具体启动日期（取决于研究进展），但对所需算力规模心里已大致有数 —— 按当前价格算，大约需要 2000 万美元的算力。</p><p>目前，他们只有两个糟糕的选择：</p><p><strong>1）通过长期合约超量预定算力，承担高昂的持有成本；</strong></p><p><strong>2）或者赌一把现货市场，寄希望于等他们准备就绪时，价格和市场供给都能如人所愿。</strong></p><p>这恰恰正是期货、期权和互换合约这类金融工具本应解决的问题。如果将算力视为一种金融基础资产，该实验室就可以：</p><ul><li>买入算力期货，先锁一层已知价的底仓；</li><li>再叠加看涨期权（call options），以防项目规模超预期、需要额外算力；</li><li>并通过互换合约（swaps），将浮动的现货价格置换为固定价格。</li></ul><p>这套逻辑本身并不复杂 —— 不过是把大宗商品风险管理的基础方法从小麦或电力，换成了 GPU 而已。之所以尚未实现，唯一的原因是我们仍把算力当作一种服务型产品，而非一种具有随机性的生产投入要素。</p><h2><strong>03 硅谷 vs 芝加哥/纽约：两种玩法</strong></h2><p>看看两边玩的根本不是同一局牌。</p><p><strong>硅谷局：</strong></p><ul><li>把底层的复杂性封装起来，只通过一个简洁、清晰的应用程序接口（API）对外提供功能。</li><li>以开发者体验为核心优化目标。</li><li>将市场波动平滑处理为分层定价方案。</li><li>通过用量计费，并通过生态或合同把客户“锁定”。</li></ul><p>这套玩法在底层系统“容错性强”时才有效 —— 比如供给能很快跟上、需求足够分散、没有人会因单次价格飙升而彻底出局 —— 此时，你可以把风险当作噪声忽略。</p><p><strong>芝加哥/纽约局：</strong></p><ul><li>不隐藏风险，而是直面风险。</li><li>为高风险标的定义标准化合约。</li><li>建立能让这些合约进行交易的交易场所。</li><li>引入清算机制、保证金制度和风险模型，让机构能够安全地持有风险敞口。</li></ul><p>正是这种思维方式，让天气、波动率、电力储备和货运都变成了可交易的资产。这并不浪漫，只是清醒地承认一个事实：任何反复出现、且会对真实世界的人造成伤害的不确定性，都值得为之建立一个市场。</p><h2><strong>04 “但算力不是石油！”</strong></h2><p>说得对，而这恰恰是其有趣之处。算力是缺乏统一标准、高度异构、难以抽象成单一商品的 —— 它会因硬件、网络、延迟、地理位置和 SLA（服务等级协议）的不同而变化，而且验证起来并不简单。并不存在一个放之四海而皆准的标量，能完美定义“一单位算力”在所有场景下的含义。</p><p>但这并不会让它失去资格。电力市场需要处理地理位置、时段和传输约束；货运市场需要应对航线、船型和港口风险；而波动率产品交易的，甚至只是价格的一种抽象统计特性。</p><p>要将算力金融化，你不需要“一口吃成个胖子”，而是需要一系列标准化的“切片”【译者注：将算力按特定维度（如硬件类型、任务基准、时长等）拆解为可定义、可度量、可合约化的单位。】：</p><ul><li>清晰定义的交易单位（例如：“在 Z 小时内完成 Y 基准测试下的 X 个 token 处理，最大延迟为 L，故障条件事先约定”）；  </li><li>双方都信任的计量与验证机制；  </li><li>交付失败时的违约惩罚条款。</li></ul><p>你不会得到一个覆盖所有场景的“全球统一 GPU 期货”。你最终会得到的，是一系列相互关联、但各有所指的合约 —— 就像电力和大宗商品市场那样。这没什么问题，真实的市场本来就是这样运作的。</p><h2><strong>05 服务派 vs 市场派：谁才是真正的赢家？</strong></h2><p>一旦你把算力看作一种风险，战略格局就变了。服务派的本能是： <strong>“我们替用户把复杂性抽象掉，自己承担这些风险，然后通过加价来赚取利润。”</strong></p><p>于是你得到的是 GPU 版 Airbnb、更花哨的调度系统、更好看的仪表盘 —— 这些固然有用，但本质上仍是线性增长模式。你不过是在一个失灵的市场中，做了一个更高效的中间商。</p><p>而市场派的本能则相反： <strong>“我们要把风险暴露出来，将其标准化，并让它可交易。我们的护城河是市场结构本身，而不是 UI 界面。”</strong></p><p>这会堆出一整套完全不同的东西：</p><ul><li>算力单位的合约标准；  </li><li>支持这些合约交易的交易所与撮合引擎；  </li><li>清算与保证金机制，让机构资本得以参与；  </li><li>做市商，主动承担并管理算力风险；  </li><li>算力价格与波动率的数据、指数；  </li><li>面向算力供应商的信用与抵押框架。</li></ul><p>这更接近 <strong>CME（芝加哥商品交易所） + 电力 ISO 市场</strong>，而不是“GPU 版 Stripe”。  </p><p>而最有能力构建并运营这类系统的机构，并不在沙丘路（Sand Hill Road），而是在芝加哥和纽约。</p><h2><strong>06 对未来的预测（The Prediction）</strong></h2><p>谁掌控了 AI 算力的风险层（即那些用于定价和交易各方风险敞口的金融工具、交易平台与规则体系）谁便掌握了支配以下各方的关键杠杆：</p><ul><li>需要对冲训练风险的 AI 实验室；  </li><li>希望在不引发财务风险的前提下变现算力容量的云厂商和裸金属提供商；</li><li>寻求新型的、多元化的实物资产敞口的基金与金融机构；</li><li>甚至各国政府 —— 一旦它们开始像对待石油和天然气那样，思考“战略性算力储备”问题。</li></ul><p>这不再是“更好的 SaaS 产品”，而是一种市场基础设施（market institution）。</p><p><strong>AI 算力终将走向金融化，因为其底层的不确定性太大、太持久，靠临时合同和 Slack 私聊根本兜不住。</strong></p><p>如果你还在用传统商品销售的思路来看待 GPU —— 比如把它当成电商网站上一个明码标价、规格固定、随时可买的标准化商品（SKU），那你解决的只是昨天的问题。</p><p>谁能够设计出一种更高效、更具韧性的<strong>市场机制</strong>来<strong>分散、转移和管理</strong>伴随算力而来的巨大风险，谁就能在未来的竞争中掌握主动权。</p><p><strong>END</strong></p><p><strong>本期互动内容 🍻</strong></p><p><strong>❓如果你是一家 AI 实验室的负责人，面对算力价格的剧烈波动，你更愿意：</strong></p><p>A）提前锁定长期合约，哪怕成本高些；  <br/>B）赌现货市场，灵活但风险自担；  <br/>C）如果有算力期货/期权，立刻用金融工具对冲。  <br/>为什么？</p><p><strong>本文经原作者授权，由</strong> <strong>Baihai IDP</strong> <strong>编译。如需转载译文，请联系获取授权。</strong></p><p><strong>原文链接：</strong>  </p><p><a href="https://link.segmentfault.com/?enc=rx4YtyvrUkx9mKBQWVxvVQ%3D%3D.ffHqgpAk5qwjG0AL70wxIxwyU9G3WjV2UCJ0rNIutlePTa0sL%2B1FQ20A%2BlvY1xZz8OPUc1Tl9ENNBccE%2BGos%2BtQYTuOk1%2BlN0DmAI%2F2k0gk%3D" rel="nofollow" target="_blank">https://davefriedman.substack.com/p/the-hidden-risk-of-ai-com...</a></p>]]></description></item><item>    <title><![CDATA[Linux再添一员猛将，操作完全不输Windows！ CodeSheep ]]></title>    <link>https://segmentfault.com/a/1190000047525754</link>    <guid>https://segmentfault.com/a/1190000047525754</guid>    <pubDate>2026-01-07 09:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>提到 <strong>Zorin OS</strong> 这个操作系统，可能不少喜欢折腾 Linux 系统的小伙伴之前有尝试过。</p><p>作为一款以 UI 交互和颜值著称的 Linux 发行版系统，Zorin OS 也曾一度被广大爱好者们称为 <strong>Windows 系统的开源替代方案</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525756" alt="" title=""/></p><p>Zorin OS 旨在简单易用，用户无需学习任何新知识即可上手，同时 Zorin OS 作为一款 Linux 发行版系统，<strong>专为从 Windows 迁移的用户设计</strong>，提供类似 Windows 的图形界面与操作逻辑，并且<strong>支持一键切换为 Windows 系统风格</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525757" alt="" title="" loading="lazy"/></p><p>前段时间，Zorin OS 团队在其官博正式宣布，最新的 Zorin OS 18 已经正式突破了 100 万次下载。</p><p>并且据官博数据显示，这些下载中<strong>有超过 78% 是来自于 Windows 系统的用户</strong>，这也再次印证了其可以满足从 Windows 桌面系统迁移到 Linux 发行版的用户需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525758" alt="" title="" loading="lazy"/></p><p>作为一个长期关注 Linux 桌面系统的博主，其实这次 Zorin OS 18 大版本更新刚出来那会我就关注了，不过一直没有抽出时间来写文章、来梳理，所以今天这篇文章正好把这件事情给安排了！</p><p>总体来讲，这次的 Zorin OS 18 是以 Ubuntu 24.04 LTS 为基础并由 Linux 6.14 内核提供支持。</p><p>并且这次的 Zorin OS 18 是继之前 17 版本以来的一次大版本迭代，带来了诸多新特性和改进。</p><p>所以接下来我们也来梳理一下这次 Zorin OS 18 所带来的一些重点更新和变化。</p><h2>视觉与交互进化</h2><p>众所周知，Zorin OS 一直以来都以其独特的个性和简约的美学设计风格而著称。</p><p>那这次更新后的新外观给人最直观的感受就是圆润和通透。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525759" alt="" title="" loading="lazy"/></p><p>任务栏这一次采用了全新的悬浮圆角面板设计，不再是死板地贴在屏幕边缘，而是像 macOS 的控制中心一样有一种轻盈的漂浮感。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525760" alt="" title="" loading="lazy"/></p><p>另外这一次大版本还推出了新主题颜色，新增了黄色和棕色两种主题色，视觉层次更加丰富。</p><p>选中元素的色调更加淡雅，背景和侧边栏颜色更深，长时间盯着屏幕写代码或处理文档，眼睛会舒服很多。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525761" alt="" title="" loading="lazy"/></p><p>另外 Pro 版里还提供了更多可切换的桌面布局。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525762" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525763" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525764" alt="" title="" loading="lazy"/></p><p>除此之外，很多经常使用的日常应用也进行了诸多设计调整和改进。</p><p>比如<strong>文件管理器</strong>的侧边栏重新设计了，操作控件更直观，搜索功能支持了全文搜索，找文件效率大增。</p><p><strong>日历</strong>应用增加了侧边栏，月份和事件视图也一目了然。</p><p><strong>相机</strong>应用也做了更新，新相机应用界面简洁，支持多摄像头切换，这对于现在动不动就开视频会议的环境非常友好。</p><h2>Web 应用深度集成</h2><p>对于用户来说，最大的痛点往往不是系统本身，而是数据迁移和应用生态，那 Zorin OS 18 在这方面下了不少功夫。</p><p>首先就是与 Web 应用程序无缝集成。</p><p>众所周知，现在很多应用都构建在云端，这些渐进式 Web 应用与原生应用之间的用户体验正逐渐融合。</p><p>这次 Zorin OS 18 全新内置的「Web Apps」工具非常强大，它可以将 Web 应用转换为桌面应用，用户的 Web 应用将可以显示在开始菜单中，使用起来与原生应用无异。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525765" alt="" title="" loading="lazy"/></p><p>「Web Apps」工具可以作为后端与各种热门 Web 浏览器集成，同时也允许用户自定义对应 Web 应用内的体验。</p><h2>多任务处理：原生窗口平铺</h2><p>这次 Zorin OS 18 的多任务处理变得好用多了。</p><p>Zorin OS 18 引入了一款功能强大的窗口平铺管理器，它能帮助用户更高效地工作，同时上手起来也十分简单。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525766" alt="" title="" loading="lazy"/></p><p>用户只需要把窗口拖到屏幕顶部，系统就会自动弹出布局选择器。</p><p>预设布局支持左右分屏、三栏布局、角落停靠等，同时在智能建议这块，系统也可以根据用户当前所打开的窗口，智能推荐最佳的排列组合。</p><p>除此之外它还支持高度自定义，创建用户自己的平铺布局。</p><p>这个新特性无论对新手还是资深玩家都非常直观易用，从而定制和提升每个用户的生产力。</p><h2>迁移神器：Windows 应用支持</h2><p>用户可以从内置的软件商店发现适用于 Zorin OS 系统的各类应用，这是在 Zorin OS 中安装应用的推荐方式。</p><p>其软件商店可让用户开箱即用地从 Zorin OS 与 Ubuntu APT 仓库、Flathub 以及 Snap Store 安装应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525767" alt="" title="" loading="lazy"/></p><p>而如果用户是刚从 Windows 转过来，看到满硬盘的 .exe 安装包肯定会头疼。</p><p>Zorin OS 18 的处理方式非常聪明。</p><p>系统内置了一个庞大的软件数据库（覆盖超过 170 款软件），当用户双击一个 Windows 安装包（如 setup.exe）时，系统不会直接报错，而是弹出一个友好的对话框。</p><p>如果有 Linux 原生版本，它就会引导你安装原生版本应用；如果没有原生版的话，它就会推荐你使用 Web 版，或者利用兼容层运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525768" alt="" title="" loading="lazy"/></p><p>在兼容层优化这一块，Zorin OS 18 深度集成了 Wine，对于一些必须在 Windows 下运行的行业软件或游戏，它提供了一个“Windows 应用支持”层。虽然不能保证 100% 兼容，但对于很多老旧的 .exe 工具，它能让你在不装虚拟机的情况下应急使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525769" alt="" title="" loading="lazy"/></p><h2>性能与硬件支持</h2><p>Zorin OS 18 基于 Ubuntu LTS 版本打造，同时它将获得直到 2029 年的稳定安全更新。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525770" alt="" title="" loading="lazy"/></p><p>同时官方宣称它甚至可以在十几年前的古董机上流畅运行。最低配置仅需 1GHz 双核 CPU、2GB 内存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525771" alt="" title="" loading="lazy"/></p><p>同时从用户安装的实际表现来看，在现代硬件上，它的动画流畅度非常高，即便在老机器上，它运行起来也比 Windows 系统更加轻快。</p><h2>写在最后</h2><p>那以上就是关于此次 Zorin OS 18 大版本更新的一些梳理和总结，感兴趣的小伙伴也可以去体验一波。</p><p>总的来看，这次的 Zorin OS 18 不仅仅是一个 Linux 发行版，也像极了一个操作系统迁移解决方案。</p><p>另外这次 Zorin OS 18 的发布，也使得 Linux 桌面系统的易用性又向前迈进了一步。</p><p>文章的最后也期待 Linux 桌面系统在未来能百花齐放，发展得越来越好。</p><p>好了，那以上就是今天的内容分享了，希望能对大家有所帮助，我们下篇见。</p><blockquote>注：本文在GitHub开源仓库「编程之路」 <a href="https://link.segmentfault.com/?enc=PCgLJyqi5O%2BTtLTgm1n%2FyA%3D%3D.IQaXZ73LMOvOMvGkY4035I4lHaZXZXNIysGlvskbc6soFJJPzCruaeoWl3MIhJCD" rel="nofollow" target="_blank">https://github.com/rd2coding/Road2Coding</a> 中已经收录，里面有我整理的6大编程方向(岗位)的自学路线+知识点大梳理、面试考点、我的简历、几本硬核pdf笔记，以及程序员生活和感悟，欢迎star。</blockquote>]]></description></item><item>    <title><![CDATA[剑指offer-60、将⼆叉树打印成多⾏ SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047516587</link>    <guid>https://segmentfault.com/a/1190000047516587</guid>    <pubDate>2026-01-07 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>题⽬描述</h2><p>从上到下按层打印⼆叉树，同⼀层结点从左⾄右输出。每⼀层输出⼀⾏。</p><p>给定的⼆叉树是 {1,2,3,#,#,4,5} :</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047516589" alt="" title=""/></p><p>该⼆叉树多⾏打印层序遍历的结果是：</p><pre><code class="text">[
[1],
[2,3],
[4,5]
]</code></pre><p>示例1<br/>输⼊：{8,6,10,5,7,9,11}<br/>返回值：[[8],[6,10],[5,7,9,11]]</p><h2>思路及解答</h2><p>59题的缩减版</p><h3>迭代法BFS（广度优先搜索）</h3><pre><code class="java">public class Solution {
    ArrayList&lt;ArrayList&lt;Integer&gt; &gt; Print(TreeNode pRoot) {
        //层次打印遍历树
        ArrayList&lt;ArrayList&lt;Integer&gt; &gt; lists = new ArrayList&lt;&gt;();
        if(pRoot == null) return lists;
        Queue&lt;TreeNode&gt; q = new LinkedList&lt;&gt;();
        q.offer(pRoot);
        while(!q.isEmpty()){
            int size = q.size();
            ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();
            for(int i = 0; i &lt; size; i++){
                TreeNode temp = q.poll();
                list.add(temp.val);
                if(temp.left != null) q.offer(temp.left);
                if(temp.right != null) q.offer(temp.right);
            }
            lists.add(list);
        }
        return lists;
    }
    
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，每个节点恰好入队和出队各一次</li><li><strong>空间复杂度</strong>：O(n)，队列中最多存储n个节</li></ul><h3>递归DFS（深度优先搜索）</h3><p>虽然层序遍历通常用BFS，但也可以用DFS通过递归隐式维护层级信息来实现</p><pre><code class="java">import java.util.*;

public class Solution {

    public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; levelOrder(TreeNode root) {
        ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result = new ArrayList&lt;&gt;();
        if (root == null) return result;
        
        dfs(root, 0, result);
        return result;
    }
    
    private void dfs(TreeNode node, int depth, ArrayList&lt;ArrayList&lt;Integer&gt;&gt; result) {
        if (node == null) return;
        
        // 如果当前深度对应的列表不存在，创建新列表
        if (depth &gt;= result.size()) {
            result.add(new ArrayList&lt;&gt;());
        }
        
        // 将当前节点值加入对应深度的列表
        result.get(depth).add(node.val);
        
        // 递归处理左右子树，深度+1
        dfs(node.left, depth + 1, result);
        dfs(node.right, depth + 1, result);
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，每个节点访问一次</li><li><strong>空间复杂度</strong>：O(h)，递归栈深度等于树高，最坏情况O(n)</li></ul>]]></description></item><item>    <title><![CDATA[1956-2026：人类与机器智能的七十年对话 RTE开发者社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525614</link>    <guid>https://segmentfault.com/a/1190000047525614</guid>    <pubDate>2026-01-07 01:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>1956年夏天，当约翰·麦卡锡（John McCarthy）、马文·明斯基（Marvin Lee Minsky）等先驱在达特茅斯学院首次提出“人工智能”这个概念时，他们乐观地预言：十年内机器将具备人类级别的推理能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525616" alt="" title=""/></p><p>七十年过去了，这个预言虽未完全实现，但AI的演进轨迹却远比当初设想的更加波澜壮阔——从符号推理的黄金时代到“AI寒冬”的沉寂，从机器学习的复兴到深度学习的爆发，再到2026年AI全面融入产业基础设施的当下。</p><p>这七十年的历史揭示了一个关键规律：AI的每一次突破，都源于思想碰撞、跨界融合与全球协作。1997年深蓝战胜卡斯帕罗夫，标志着暴力计算与精妙算法的混合突破；2010年后深度学习革命的爆发，则得益于大数据、GPU算力与神经网络架构创新的三重汇聚。而站在2026年这个新起点，当生成式AI、AI4S、具身智能等前沿趋势加速涌现，<strong>当国际竞合格局重塑全球创新生态，我们比以往任何时候都更需要一个能够汇聚顶级思想、链接全球资源、激发跨界创新的高浓度平台。</strong></p><p>“那么，东方为这场持续七十年的对话，按下了哪些关键按钮？”答案写在黄浦江畔。</p><h2>01 七十年的回响：浦江答卷</h2><p>上海的AI实践为这场全球对话提供了丰富的东方注脚。</p><p>这座城市不仅培育了像MiniMax、阶跃星辰这样的基础大模型先锋，在垂直领域，联影医疗将AI融入医疗影像诊断，松鼠AI打造个性化教育系统，小i机器人深耕政务智能化。</p><p>消费级市场则涌现出珞博智能与华为联合打造的“智能憨憨”情感陪伴硬件，探索人与AI之间超越工具性的“养成”关系；XREAL与Google合作深耕轻量级AR生态，推动AR设备从显示工具向日常化的空间计算终端演进。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525617" alt="" title="" loading="lazy"/></p><p>更令人瞩目的是，沪产机器人“智元”打破了“人形机器人行走最远距离”的吉尼斯世界纪录，标志着中国在具身智能领域的硬核突破。这些看似分散的创新成果，共同勾勒出上海作为全球AI重要节点的完整生态图景。</p><p>然而，在AI这场高度依赖算力、数据与资本的长期竞赛中，任何单一创新节点都面临着资源整合与全球链接的挑战。特别是对于寻求国际化发展的AI企业而言，如何高效对接全球市场、资本与人才，成为必须跨越的门槛。</p><h2>02 七十年的回响：双城新篇章</h2><p>当上海的AI产业积累需要更广阔的国际化舞台时，中国香港以其独特的“超级联系人”角色进入了视野。</p><p>这座城市正在迅速崛起为亚洲AI枢纽，目前已汇聚约500个AI相关组织、290家AI企业及180家投资机构，形成了高密度的创新生态。</p><p>香港的优势远不止于数字。其资本市场在2025年以2860亿港元的IPO募资额位居全球第一，成为科技与AI企业上市的首选地之一。与此同时，香港政府宣布投入30亿港元设立人工智能专项资助计划，建设AI研发院与超算中心。</p><p>一边是扎实的产业“底座”，一边是强大的国际“接口”，两者的历史性握手，只差一个契机。这个契机，随着维多利亚港的海风如期而至。2026年AI领域首场高浓度思想盛宴<strong>“WAIC UP!全球年终盛会”</strong>来到香港。这不仅是世界人工智能大会（WAIC）首次在港举办年度会议，更是上海AI产业实践与香港国际枢纽功能的一次历史性握手。</p><p>会议汇聚了WAIC旗下五大生态品牌——<strong>创新孵化引擎WAIC Future Tech、产业对接枢纽WAIC CONNECT、思想启迪窗口WAIC UP!、青年科教阵地WAIC Young以及全球合作舞台AI GRAVITY</strong>。这种多维度、立体化的平台设计，确保了不同背景的参会者都能找到自己的价值定位。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525618" alt="" title="" loading="lazy"/></p><p>对于上海及内地的AI创业者而言，<strong>这场盛会提供的不仅是展示舞台，更铺设了一张更广阔的全球AI协作网络。</strong> 从港交所的上市通道，到香港投资推广署的落地支持，从科学园的研发设施，到数码港的孵化生态，这些以往需要数月才能打通的环节，如今在一天之内就能建立初步联系。然而，链接资源只是第一步。在范式转换的临界点上，比资源更稀缺的是洞察未来的“思想地图”。</p><h2>03 WAIC UP!连接下一个七十年</h2><p><strong>国际级讲者阵容带来的历史纵深与前沿视野</strong></p><p>如今，我们正站在另一个范式转换的临界点：从单模态到多模态，从云端到边缘，从工具到伙伴……这些AI趋势的把握，不能仅靠闭门研发，更需要站在巨人肩膀上的思想启迪。上午场 <strong>“WAKE思想觉醒”</strong> 正是为此而设。</p><ul><li><strong>皮埃罗·斯加鲁菲（Piero Scaruffi）</strong>，作为硅谷人工智能研究院院长与硅谷精神布道师，见证了从专家系统兴衰到深度学习爆发的完整周期。他对AI演进周期性规律的洞察，将帮助参会者避免重蹈历史覆辙，在泡沫与实质之间保持清醒判断。</li><li><strong>史蒂夫·霍夫曼（Steve Hoffman）</strong>，作为Founders Space创始人与硅谷创投教父，孵化过数百家AI创业公司。他将分享从实验室到市场的转化密码，揭示哪些技术趋势真正具备商业化潜力——这正是避免1980年代专家系统式“虚假繁荣”的关键。</li><li><strong>朱晓波</strong>教授，作为“祖冲之号”量子计算总师，代表着AI算力革命的下一个前沿。量子计算与AI的结合，可能重现2010年GPU为深度学习带来的颠覆性加速，这是理解未来十年AI发展的战略制高点。</li><li>……</li></ul><p>如AI历史所示，1997年深蓝的胜利不仅是技术突破，更重塑了人们对“智能”的理解；2023年ChatGPT的爆发不仅是产品成功，更引发了对AGI路径的全球讨论。这些思想领袖的价值不仅在于传递信息，更在于提供思维框架；参会者将获得的不是碎片化的技术细节，而是构建AI时代世界观的思想基石。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525619" alt="" title="" loading="lazy"/></p><p><strong>产业全链条的立体化资源网络</strong></p><p>低代码/无代码AI平台的普及，使得非技术专家也能构建智能应用。但真正的挑战在于：如何将技术能力转化为产业价值？如何在垂直领域找到AI的最佳应用场景？<strong>下午场“UP拓维跃迁”</strong>精准回应这一需求，通过垂直应用案例、商业实战与出海战略的三维透视，构建从技术到商业的完整闭环。</p><ul><li><strong>技术供给侧：</strong> 商汤科技、科大讯飞等头部AI企业展示最新解决方案，从计算机视觉到语音交互，覆盖AI技术的全栈能力。这些企业经历了从研发到规模化部署的完整历程，其经验教训价值千金。</li><li><strong>产业需求侧：</strong> 中国移动国际、神州数码、易鑫集团等传统行业巨头分享数字化转型实践。他们的痛点与需求，正是AI创业者与技术提供商的机遇所在。</li><li><strong>基础设施层：</strong> 算丰等算力提供商、RTE开发者社区等技术生态，构成AI应用的底层支撑。正如深度学习革命依赖GPU算力突破，下一代AI应用同样需要新型基础设施的支撑。</li><li><strong>资本催化剂：</strong> 孚腾资本、Atma Capital等投资机构带来资本视角。他们对赛道的判断、对商业模式的洞察，能够帮助创业者避免方向性错误，加速从0到1的突破。</li></ul><p>这种产、投、创、研的四维聚合，令参会者可以在一天内完成通常需要数月的生态链接：上午吸收前沿思想，下午对接产业资源，实现从“知道”到“做到”的跨越。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525620" alt="" title="" loading="lazy"/></p><p><strong>AI时代的“达特茅斯时刻”</strong></p><p>达特茅斯会议的真正价值，不仅在于定义了“人工智能”这个术语，更在于它创造了一个跨学科思想自由碰撞的场域。数学家、工程师、心理学家、语言学家齐聚一堂，在非正式讨论中激发出影响后世七十年的核心概念。<strong>夜晚场“MORE灵感迸发”</strong> 正是要重现这种魔力。当正式议程结束，当西装革履卸下，当不同代际、不同领域、不同文化背景的参与者在轻松氛围中交流，往往会产生最意想不到的化学反应。</p><ul><li><strong>跨代际对话的独特价值：</strong> 青年科学家带来未被传统范式束缚的新鲜视角；资深专家提供历史纵深与战略判断；初创团队展现颠覆式创新的勇气；企业决策者贡献产业落地的实战智慧。</li><li><strong>跨领域融合的创新源泉：</strong> 夜晚场汇聚的多元群体——从机器人工程师到AGENT开发者，从科技媒体到社区运营者，从BIM国际青年联盟到WAYtoAGI社区——构成了一个思想熔炉。</li></ul><p>历史反复证明，AI的突破往往发生在学科交叉点。一个机器人工程师与一个内容创作者的对话，可能催生下一个爆款AI应用；一个出海企业家与一个国际组织代表的邂逅，可能开启跨境合作的新篇章。这种“计划外的收获”往往比正式议程更有价值。</p><h2>04 人类与未来的永恒对话</h2><p>以1956年达特茅斯会议正式定名“人工智能”为起点，AI的发展虽然历程尚短，却以远超预期的速度穿越了一个又一个技术关口，从逻辑推理、统计学习，到今天的大模型与多模态系统，智能以持续涌现的方式重塑着现实世界的运行节奏。</p><p><strong>而愈是演进加速的时刻，愈需要重新思考人的位置。</strong></p><p>算法可以更快，模型可以更大，但人的判断、情感与责任，从未可被替代。在碳基生命与硅基智能在文明进化的路口相遇时，我们亟需重新建立认知框架，回望我们想成为什么样的人类。</p><p>站在2026年的起点，我们更能体会这场演进的复杂与惊奇，每一次技术跃迁，像是文明的回响。正是在这样的时代拐点上，WAIC作为全球AI的重要思想交流平台，正持续拓展其“科技×人文”的边界，推动议题，凝聚共识。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525621" alt="" title="" loading="lazy"/></p><p>无论是思想刊物《WAIC UP!》，还是连接创业者、产业方、政策制定者、青年一代的多维平台，WAIC一直以来都承载着一个宏大的命题：我们愿意与AI共同走向怎样的未来。而即将到来的“WAIC UP!全球年终盛会”，也将为新一年的探索翻开新的篇章。</p><p>回顾这七十年，一个有趣的对比是：1956年的达特茅斯会议只有几十位参与者，而今天的WAIC将连接成千上万的全球头脑。规模的变化背后，是AI从学术课题到文明议题的演进。</p><p>我们的使命无比清晰，只要答案未至，步履永远不停。</p><p><strong>即刻锁票，与下一个70年对话</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525622" alt="" title="" loading="lazy"/></p><p>参考资料：</p><p>[1] AI养成系潮玩受资本追捧，“不看好早期具身智能”的朱啸虎也出手了，每日经济新闻，202506.</p><p>[2] Hong Kong Cements AI Hub Status with 500 Organizations, 23% IPO Surge, and AI-for-Finance Ecosystemic Leadership，香港金融发展局，202511.</p><p>[3] 打造香港成全球AI重要枢纽，文汇网，202511.</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525623" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525624" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=7drcaJ7LVpL4%2Bz%2F5L%2Bi3ag%3D%3D.2dpIsiRN94j0XvDyuvuhLEoXY4lvbKql6Nrw4iA3QP8%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525625" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[时代的眼泪，nameko 和 eventlet 停止维护后的项目自救，升级和替代之路 rabbitc]]></title>    <link>https://segmentfault.com/a/1190000047525550</link>    <guid>https://segmentfault.com/a/1190000047525550</guid>    <pubDate>2026-01-07 00:02:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img width="723" height="268" referrerpolicy="no-referrer" src="/img/bVdnzJh" alt="图片.png" title="图片.png"/></p><p>nameko 已经凉了, 最后一次 commit 停留在了 2023.11.3 。<a href="https://segmentfault.com/a/1190000045427395" target="_blank">如何将您的 Eventlet 项目迁移到 Asyncio</a></p><p>eventlet 也是几乎停止了维护，已经无法支持 cpython3.13+ 的版本了。<a href="https://segmentfault.com/a/1190000047525496" target="_blank">nameko 无法适配新版的python3.14，eventlet 停止维护导致的失效</a></p><p><a href="https://link.segmentfault.com/?enc=DDl0O7wwJZJ4fXO%2B1L5Cyg%3D%3D.z%2BJ%2Fykz%2BtA0nHJYu2rPZJk%2BbTTt88YTejeooFKKxRU3DBQWavBX15Nux%2FV9LgueRurTS7jTshsL09X8BlwdszQ%3D%3D" rel="nofollow" target="_blank">https://github.com/eventlet/eventlet/issues/1075</a></p>]]></description></item><item>    <title><![CDATA[Wispr 曝光内部项目：不仅转录文本还执行任务；苹果将推送 LLM 架构 Siri：支持屏幕感知与]]></title>    <link>https://segmentfault.com/a/1190000047525555</link>    <guid>https://segmentfault.com/a/1190000047525555</guid>    <pubDate>2026-01-07 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525557" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong>，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、NVIDIA 发布 Nemotron Speech ASR：缓存感知架构实现 24ms 极低延迟与 3 倍并发提升</strong></p><p>NVIDIA 发布开源模型 Nemotron Speech ASR，引入缓存感知流式技术替代传统的重叠缓冲推理。该架构通过仅处理音频增量并复用历史计算状态，解决了高并发环境下的延迟漂移问题，将单卡并发能力提升了 3 倍，为实时语音智能体提供了高性能的基础设施。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525558" alt="" title="" loading="lazy"/></p><ul><li><strong>缓存感知流式架构</strong>：弃用滑动窗口的重叠计算模式。通过在编码器层维护内部缓存状态，确保每帧音频仅被处理一次，实现了内存消耗的线性扩展，彻底消除计算冗余。</li><li><strong>8x 下采样 FastConformer 架构</strong>：模型规模 600M 参数，采用深度可分离卷积实现 8 倍下采样。相比行业主流的 4 倍下采样方案，该架构大幅减少了每秒处理的 Token 数量，显著降低 VRAM 占用。</li><li><strong>24ms 中值最终转录延迟</strong>：在实测中，该模型的 Time-To-Final（最终转录延迟）中值仅为 24ms，且性能不随语音长度增加而衰减。对比之下，同类本地模型延迟约为 90ms，主流 API 方案则通常超过 200ms。</li><li><strong>运行时动态延迟配置</strong>：支持在推理阶段实时切换 80ms、160ms、560ms 及 1.12s 等不同延迟模式。开发者无需重新训练模型，即可根据业务场景在响应速度与识别准确率之间取得平衡。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525559" alt="" title="" loading="lazy"/></p><ul><li><strong>高并发吞吐表现</strong>：单张 H100 GPU 可同时支持 560 个并发流（320ms 块大小），吞吐量较前代方案提升 300%。在 RTX A5000 等工作站级 GPU 上，并发能力提升可达 5 倍。</li></ul><p>模型已在 Hugging Face 开源，支持通过 NVIDIA NeMo 部署。</p><p>Hugging Face: </p><p><a href="https://link.segmentfault.com/?enc=lmx1xobecFhW8QFivgQqTQ%3D%3D.1rEiHmYnuXU%2FO7giLVLrK4CZFjnXMd3ygFfnGxyeLmEYku4lwkzxgomnbe%2BDIaLNEIoafqlmt%2Fu9NZsDeZ4cXA%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b</a></p><p>( @Huggingface)</p><p><strong>2、Boston Dynamics 联合 Google DeepMind：将 Gemini 基础模型集成至新一代 Atlas，开发 VLA 视觉-语言-动作模型</strong></p><p>Boston Dynamics 与 Google DeepMind 宣布达成战略合作，将 Gemini Robotics 基础模型引入新一代全电动「Atlas」机器人。该计划旨在利用大规模多模态模型提升人形机器人的感知推理与灵巧操作能力，首批应用目标锁定为汽车制造业的工业任务。</p><ul><li><strong>集成 Gemini Robotics 基础模型</strong>：基于 Google 的多模态「Gemini」大模型，为机器人提供视觉感知、逻辑推理及工具使用能力，使其能理解并执行复杂的跨模态指令。</li><li><strong>构建视觉-语言-动作（VLA）模型</strong>：双方将共同开发针对人形机器人的 VLA 模型，致力于将非结构化的环境信息直接映射为高维度的执行动作，提升机器人在复杂工业场景下的泛化能力。</li><li><strong>全电动「Atlas」机队部署</strong>：此次合作将完全基于 Boston Dynamics 最新的全电动版 Atlas 平台，利用其超越人类极限的关节活动范围（ROM）验证基础模型在端到端控制上的表现。</li><li><strong>工业级任务对齐</strong>：研发重心处于从「运动智能」向「通用智能」的跨越，重点解决汽车生产线等高动态环境下的灵巧操作与人机协作安全性。</li></ul><p>联合研究计划于 2026 年内正式启动，初期成果将率先在现代汽车工厂进行测试，暂未披露 API 开放计划或具体商用定价。</p><p>( @Boston Dynamics Blog)</p><h2>02 有亮点的产品</h2><p><strong>1、Symbolic Software 发布 Magicall：端到端加密视频通话，支持 SAS 验证与 EU 节点托管</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525560" alt="" title="" loading="lazy"/></p><p>加密咨询公司 Symbolic Software 推出「Magicall」Alpha 版，这是一款强调隐私的浏览器原生视频会议工具。该产品通过端到端加密技术提供无需客户端的即时通讯，旨在通过欧盟本地化托管和无 AI 训练政策解决企业协作中的数据主权与隐私安全问题。</p><ul><li><strong>端到端加密（E2EE）与 SAS 身份验证</strong>：音视频及聊天数据在浏览器端完成加密后再传输；引入「短验证字符串」（Short Authentication Strings， SAS）机制，允许用户通过比对校验码验证参与者身份，防范中间人攻击。</li><li><strong>Zero-Download 架构与固定 URL</strong>：采用 Web 浏览器原生运行，支持 Chrome、Firefox、Safari 和 Edge；用户可申领永久固定的房间链接，访客端无需注册账号或下载任何插件。</li><li><strong>欧盟本土化托管与数据主权</strong>：服务器节点全部位于欧盟境内，由总部位于巴黎的厂商开发，完全符合 GDPR 规范；官方明确承诺不使用通话数据进行 AI 模型训练，且不包含任何广告追踪插件。</li><li><strong>高标准安全背书</strong>：由曾为 Coinbase、1Password、Bitwarden、Zoom 等提供过 250 余项安全审计的 Symbolic Software 团队研发，底层协议基于开放标准构建，强调低延迟与高音频清晰度。</li></ul><p>当前处于 Alpha 测试阶段，提供 Free 永久免费版（单次会议限 5 人、30 分钟，支持无限次重启），用户可通过邮箱注册申领房间名。</p><p>相关链接：</p><p><a href="https://link.segmentfault.com/?enc=xltNwLWkMClnoMN7gq3YcQ%3D%3D.Ce%2BVpU5RQkt%2BaAtwshPd03ixeO3KjNJOQAqXj6uJXU8%3D" rel="nofollow" target="_blank">https://magicall.online/</a></p><p>( @Magicall)</p><p><strong>2、Apple Vision Pro 联合 Spectrum 推出湖人队沉浸式赛事直播：150 Mbps 码率、7 处视角及 3D 悬浮 UI</strong></p><p>Apple 与「Spectrum」宣布将于 2026 年 1 月 9 日起在「Apple Vision Pro」上推出「Spectrum Front Row」直播服务。该服务通过 Apple Immersive 视频技术直播洛杉矶湖人队赛事，旨在通过高带宽流媒体和空间交互技术提供原生虚拟现实观赛体验。</p><ul><li><strong>高吞吐量视频流与 180° 沉浸感</strong>：直播源提供最高 150 Mbps 码率的 Apple Immersive 视频，覆盖 7 个特制拍摄机位，包括记录台、篮架下方、球员通道及解说席。</li><li><strong>3D 空间实时图形渲染</strong>：计分板、球员名单及 24 秒计时器等动态数据以 3D 元素呈现，利用 visionOS 的空间计算能力悬浮于现实环境中。</li><li><strong>Ambisonic 空间音频技术</strong>：利用球场部署的多维麦克风捕捉环境音，通过「Spatial Audio」算法还原球鞋摩擦声、篮网入网声及现场观众的方位感。</li><li><strong>硬件与系统协同</strong>：该功能仅支持搭载 M2 或 M5 芯片的 「Apple Vision Pro」，且系统版本需更新至「visionOS 26」或更高版本。</li><li><strong>分阶段播控策略</strong>：直播期间，暂停、半场休息及球员入场环节将保持实时传输，不切换为传统商业广告广告位，维持全流程场内临场感。</li></ul><p>2026 年 1 月 9 日首播；直播覆盖美国南加州等湖人队转播区，全球其他地区（含日、新、韩等）支持通过「NBA」App 观看部分直播或赛后 24 小时回放。</p><p>( @Apple Newsroom)</p><p><strong>3、Apple 拟于 iOS 26.4 推送 LLM 架构 Siri：支持屏幕感知与 App Intents</strong></p><p>Apple 计划在 iOS 26.4 更新中正式上线基于 LLM 架构的新版 Siri。通过彻底替换运行多年的底层架构，新版 Siri 将具备类 ChatGPT/Gemini 的逻辑理解能力，并实现对系统全局任务的深度接管。</p><ul><li><strong>底层架构重构</strong>：弃用传统的规则/模板匹配系统，转向以 LLM 为核心的推理引擎，旨在提升复杂指令的解析精度与对话连贯性。</li><li><strong>App Intents 实现系统级操作</strong>：通过强化的智能体能力，Siri 可直接调用应用程序内的特定功能，实现全自动的「免提计算」。</li><li><strong>屏幕感知</strong>：Siri 将具备理解当前显示内容的能力，能够基于屏幕上的文本、图像或上下文信息直接执行后续指令。</li><li><strong>个人语境感知</strong>：整合跨 App 的用户数据，使 Siri 能够理解涉及个人日程、偏好及历史交互的私有化指令。</li><li><strong>发布周期预测</strong>：参考 iOS 18.4 与 16.4 的发布节奏（均为 3 月下旬），iOS 26.4 预计于 2026 年 1 月底进入 Beta 测试，3 月正式推送。</li></ul><p>( @9to5Mac\@X)</p><p><strong>4、Amazon 发布 Alexa.com：Alexa+ 全面转向「智能体」架构，支持 Web 端交互与个人数据集成</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525561" alt="" title="" loading="lazy"/></p><p>Amazon 在 CES 2026 上宣布推出 Alexa.com，将基于 LLM 的「Alexa+」服务正式从硬件端延伸至 Web 浏览器。此举通过「智能体化」的 UI 重构与跨平台集成，试图将 Alexa 从单一的语音工具转型为覆盖全平台的个人/家庭自动化中心。</p><ul><li><strong>「智能体」优先的架构重塑</strong>：Alexa 移动端及 Web 端 UI 全面转向聊天机器人界面，将原本的功能图标入口降权，优先通过自然语言交互触发底层服务。</li><li><strong>非原生数据集成能力</strong>：针对缺乏自有办公套件的劣势，Alexa+ 新增邮件、日历及个人文档（如 PDF、照片）的转发与上传接口，允许用户通过文件投喂建立家庭私有知识库，支持检索疫苗记录、学校行程等非结构化信息。</li><li><strong>第三方服务深度接入</strong>：新增 Angi、Expedia、Square 及 Yelp 等 API 集成，配合已有的 Uber、OpenTable 和 Ticketmaster 接口，支持通过智能体直接完成餐厅预订、行程规划及家政预约。</li><li><strong>硬件生态无缝兼容</strong>：Alexa+ 已适配 97% 的现有设备（约 6 亿台 Echo 系列），支持旧款硬件调用新版模型能力，通过后端云端更新实现向后兼容。</li><li><strong>高频交互数据验证</strong>：Early Access 数据显示，转向 Alexa+ 后，用户对话频率提升 2-3 倍，购物行为增长 3 倍，食谱与智能家居控制等高阶功能的使用率分别提升 500% 和 50%。</li></ul><p>已向 Alexa+ Early Access 计划的活跃用户开放，需通过 Amazon 账号登录使用。</p><p>( @TechCrunch)</p><p><strong>5、能帮你做家务的机器人 LG CLOiD 首次亮相 CES</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525562" alt="" title="" loading="lazy"/></p><p>据 The Verge 报道，LG 在 CES 主题演讲中正式展示了其家务机器人 CLOiD 的实际运行效果，LG 将其定位为打造「零劳动家庭」的重要组成部分。</p><p>CLOiD 在舞台上以双手挥动的方式亮相，随后在 LG 家电事业部销售副总裁 Brandt Varner 的指令下，示范了将一条湿毛巾放入洗衣机的完整流程。</p><p>洗衣机门自动打开后，机器人伸出左臂，将毛巾放入滚筒。整个过程耗时约 30 秒，展示了其具备基础家务执行能力，但效率仍有提升空间。</p><p>在演讲后半段，CLOiD 再次登场，为 LG HVAC 事业部高级副总裁 Steve Scarbrough 递上水瓶，并根据其语气判断需求后主动提供帮助，甚至完成了拳碰动作，强调其具备一定的情感交互能力。</p><p>LG 此前已预告该机器人具备多项家务能力，包括从冰箱取牛奶、在烤箱中烤可颂、叠放衣物等。此次演示进一步展示了其在家庭场景中的潜在应用。不过，LG 仍未公布 CLOiD 的上市时间或是否会真正面向消费者销售。</p><p>( @APPSO)</p><h2>03 有态度的观点</h2><p><strong>1、Wispr 创始人：内部项目「Wispr Actions」不仅生成文本，还能直接执行任务</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525563" alt="" title="" loading="lazy"/></p><p>Wispr 首席执行官 Tanay Kothari 近日表示，尽管让办公族放弃键盘是一场「艰苦的战斗」，但 AI 的普及正成为变革的催化剂。目前 Wispr 估值约 7 亿美元，月收入及用户数环比增长达 50%。</p><p>Kothari 认为：<strong>「AI 工具是人们开始使用 Flow 的『gateway drug』。他们下载它，在 ChatGPT 或 Cursor 中使用，到了第二或第三周，他们就会意识到，『为什么我不随处都使用它呢？』然后他们就开始在所有的 Slack 消息和电子邮件中使用它。」</strong></p><p>数据显示，该工具已让深度用户的每日打字时间从 5 小时减至 3 小时，且使用五个月后，72% 的电脑活动均通过语音完成。</p><p>与传统逐字转录工具不同，Flow 侧重于理解语境与意图。Kothari 强调，用户需要的是符合逻辑的书面表达：<strong>「其他模型会逐字转录你所说的一切，但那不是人们想要的——你说的话与你写的字非常不同，所以输出应该反映你实际会写出的样子。」</strong></p><p>通过结合 Llama 3.1 等模型，Flow 实现了高精度输出并降低了在办公室发声的「社交门槛」。在安全性上，Wispr 凭借「零数据留存」模式成功打入严监管领域。</p><p>Kothari 透露，仅约 25% 至 30% 的用户选择共享数据用于训练，这帮助公司：<strong>「获得了一些规模最大、最严格的金融机构的青睐……我们即将在欧洲最大的银行之一进行部署。身处欧洲又是银行——我还没遇到过比这要求更高的地方。」</strong></p><p>展望未来，Kothari 致力于打造现实版 J.A.R.V.I.S。，将人类从屏幕束缚中解放。他感性地表示：「我不希望我的孩子在成长过程中整天盯着手机看。对我来说，那太……令人沮丧了。我希望他们昂首挺胸地走路，而不是被屏幕所束缚。实现这一目标的唯一方法是开发一个人们真正信任的语音界面。」</p><p>其内部称为「Wispr Actions」的项目被列为今年的重点关注内容，语音交互有望从单纯的文本生成，跨越至代为执行复杂任务的新阶段。</p><p>( @Computerworld)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525564" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525565" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=%2F6biZmq5vKYkqowk95cCRQ%3D%3D.79KZq2I%2Fcb2Ymualr83%2Bm53wjysgg5UcJZgZfziwSe8%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525566" alt="" title="" loading="lazy"/></p><p>作者提示：个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[Wget安装教程：Windows命令行下载工具部署步骤（附环境变量配置方法） 读书笔记 ]]></title>    <link>https://segmentfault.com/a/1190000047525224</link>    <guid>https://segmentfault.com/a/1190000047525224</guid>    <pubDate>2026-01-06 23:07:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><h3><strong>1. 先搞清楚：Wget 是什么？</strong> ​</h3><p>Wget 是个命令行下载工具，能从网上下载文件（支持 HTTP/HTTPS/FTP），Linux 自带，但 Windows 没有，得自己装。装完后，在 cmd 里输 <code>wget + 网址</code>就能直接下载，比浏览器右键另存为方便多了。</p><h4><strong>2. 下载 Wget.exe 安装包</strong>​</h4><p>不用找复杂安装程序，直接下<strong>单文件版</strong>最省事：</p><ul><li>选对应系统版本：64位系统下 <code>wget.exe</code>（通常文件名带 <code>x64</code>或直接叫 <code>wget.exe</code>），32位下 <code>wget-i686.exe</code>；</li></ul><h4><strong>3. 安装？其实就是“放对位置”</strong> ​</h4><p>Windows 装 Wget 不用双击安装，两步搞定：</p><ol><li><strong>找个固定文件夹</strong>：比如在 D 盘建个 <code>Tools\Wget</code>文件夹（路径别带中文，比如 <code>D:\Tools\Wget</code>）；</li><li><strong>复制 wget.exe 进去</strong>：把下载好的 <code>wget.exe</code>粘贴到这个文件夹里，完事儿！</li></ol><h4><strong>4. 配置环境变量（关键！否则 cmd 找不到命令）</strong> ​</h4><p>想直接在 cmd 里输 <code>wget</code>就用，得把 Wget 的路径加到系统环境变量里：</p><ol><li>右键“此电脑”→“属性”→“高级系统设置”→“环境变量”；</li><li>在“系统变量”里找到 <code>Path</code>，双击它；</li><li>点“新建”，输入你放 <code>wget.exe</code>的文件夹路径（比如 <code>D:\Tools\Wget</code>），点“确定”保存。</li></ol><h4><strong>5. 验证是否装好</strong>​</h4><p>按 <code>Win+R</code>输 <code>cmd</code>打开命令提示符，输：</p><pre><code>wget --version</code></pre><p>如果显示版本信息（比如 <code>GNU Wget 1.21.3</code>），就成功了！</p><h4><strong>6. 试试用 Wget 下载文件</strong>​</h4><p>比如下载个图片试试：</p><pre><code>wget https://example.com/test.jpg</code></pre><p>当前 cmd 所在文件夹就会出现 <code>test.jpg</code>，说明能正常用。</p><p>​</p>]]></description></item><item>    <title><![CDATA[Web 平台开发日记 - 可观测性实践 天天向尚 ]]></title>    <link>https://segmentfault.com/a/1190000047525329</link>    <guid>https://segmentfault.com/a/1190000047525329</guid>    <pubDate>2026-01-06 23:06:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Web 平台开发日记 - 可观测性实践</h2><blockquote><strong>核心内容</strong>: Prometheus 监控集成、健康检查、请求追踪、结构化日志、可观测性体系<br/><strong>技术栈</strong>: Go + Gin + Prometheus + Correlation ID + Structured Logging</blockquote><hr/><h3>📋 目录</h3><ol><li><a href="#目标" target="_blank">目标</a></li><li><a href="#可观测性架构" target="_blank">可观测性架构</a></li><li><a href="#prometheus-指标集成" target="_blank">Prometheus 指标集成</a></li><li><a href="#健康检查实现" target="_blank">健康检查实现</a></li><li><a href="#correlation-id-请求追踪" target="_blank">Correlation ID 请求追踪</a></li><li><a href="#结构化日志系统" target="_blank">结构化日志系统</a></li></ol><hr/><h3>🎯 目标</h3><ul><li>[x] Prometheus 指标收集与暴露</li><li>[x] Health/Readiness 探针实现</li><li>[x] Correlation ID 请求追踪</li><li>[x] 结构化日志（JSON 格式）</li><li>[x] 完整的验收测试体系</li><li>[x] 监控栈配置（Prometheus + Grafana）</li></ul><p><strong>核心价值</strong>：</p><ol><li><strong>可观测性</strong> - 实时掌握系统运行状态</li><li><strong>故障诊断</strong> - 快速定位和排查问题</li><li><strong>请求追踪</strong> - 跨服务的端到端追踪</li><li><strong>生产就绪</strong> - 符合企业级运维标准</li></ol><p>项目 GitHub 地址：<a href="https://link.segmentfault.com/?enc=2CL%2FDUV1qMk1RS4%2FOo16sw%3D%3D.lN2MvXDdQaZijei4omusMFXKA9OKRHCpNsbMj4Z1HVqPzp4s8KUKzMp%2Ff7bhMMRD" rel="nofollow" target="_blank">https://github.com/Mythetic/web_platform</a></p><hr/><h3>🏗️ 可观测性架构</h3><h4>三大支柱</h4><p>现代应用的可观测性（Observability）由三大支柱构成：</p><pre><code>┌─────────────────────────────────────────────────────────────┐
│                     可观测性三大支柱                          │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  📊 Metrics (指标)          📝 Logs (日志)         🔍 Traces (追踪)  │
│  ────────────────          ───────────────         ────────────────  │
│  • 系统性能指标            • 应用运行日志           • 请求调用链路    │
│  • HTTP 请求计数           • 错误详细信息           • 跨服务追踪      │
│  • 响应时间分布            • 业务操作记录           • 性能瓶颈定位    │
│  • 资源使用率              • 结构化输出             • 依赖关系分析    │
│                                                               │
│  工具: Prometheus          工具: ELK/Loki           工具: Jaeger     │
│                                                               │
└─────────────────────────────────────────────────────────────┘</code></pre><h4>本章实现架构</h4><pre><code>┌────────────────────────────────────────────────────────────┐
│                        用户请求                             │
└──────────────────────┬─────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                   Gin 中间件层                               │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐      │
│  │ CorrelationID│→ │StructuredLog │→ │PrometheusMetrics│   │
│  │  生成请求ID   │  │  JSON日志    │  │   收集指标    │      │
│  └──────────────┘  └──────────────┘  └──────────────┘      │
└─────────────────────────────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│                    业务处理层                                │
│  • API Handlers                                             │
│  • Business Logic                                           │
│  • Database Access                                          │
└─────────────────────────────────────────────────────────────┘
                       │
        ┌──────────────┼──────────────┐
        ▼              ▼              ▼
┌──────────────┐ ┌──────────┐ ┌─────────────┐
│ /metrics     │ │ /health  │ │ server.log  │
│ (Prometheus) │ │ (K8s)    │ │ (JSON)      │
└──────┬───────┘ └────┬─────┘ └──────┬──────┘
       │              │               │
       ▼              ▼               ▼
┌──────────────┐ ┌──────────┐ ┌─────────────┐
│ Prometheus   │ │ LoadBalancer│ │ LogAggregator│
│   Server     │ │ HealthCheck │ │  (ELK/Loki) │
└──────┬───────┘ └──────────┘ └─────────────┘
       │
       ▼
┌──────────────┐
│   Grafana    │
│  Dashboard   │
└──────────────┘</code></pre><h4>数据流向</h4><pre><code>用户请求 → CorrelationID中间件（生成UUID）
         ↓
         StructuredLogger中间件（记录请求信息）
         ↓
         PrometheusMetrics中间件（开始计时、增加并发计数）
         ↓
         业务Handler处理
         ↓
         PrometheusMetrics中间件（记录延迟、状态码、递减并发）
         ↓
         StructuredLogger中间件（记录响应信息）
         ↓
         返回响应（携带 X-Request-ID header）</code></pre><hr/><h3>📊 Prometheus 指标集成</h3><h4>为什么需要 Prometheus？</h4><p><strong>问题场景</strong>：</p><ul><li>❓ 系统现在有多少并发请求？</li><li>❓ API 响应时间是否正常？</li><li>❓ 哪些接口最慢？</li><li>❓ 错误率是否在增加？</li></ul><p><strong>Prometheus 的答案</strong>：</p><ul><li>✅ 实时采集应用指标</li><li>✅ 时间序列数据存储</li><li>✅ 强大的查询语言（PromQL）</li><li>✅ 图形化展示（Grafana）</li></ul><h4>指标类型设计</h4><p>在 <code>server/middleware/metrics.go</code> 中定义了三类核心指标：</p><h5>1. HTTP 请求计数（Counter）</h5><pre><code class="go">var httpRequestsTotal = promauto.NewCounterVec(
    prometheus.CounterOpts{
        Name: "http_requests_total",
        Help: "Total number of HTTP requests",
    },
    []string{"method", "path", "status"},
)</code></pre><p><strong>用途</strong>：统计每个接口的总请求次数，按 HTTP 方法、路径、状态码分类。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看所有接口的请求总数
sum(http_requests_total)

# 查看错误请求（5xx）
sum(http_requests_total{status=~"5.."})

# 查看登录接口的成功率
rate(http_requests_total{path="/base/login",status="200"}[5m])</code></pre><h5>2. HTTP 请求延迟（Histogram）</h5><pre><code class="go">var httpRequestDuration = promauto.NewHistogramVec(
    prometheus.HistogramOpts{
        Name:    "http_request_duration_seconds",
        Help:    "HTTP request latency in seconds",
        Buckets: prometheus.DefBuckets, // [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
    },
    []string{"method", "path"},
)</code></pre><p><strong>用途</strong>：记录接口响应时间的分布情况，支持百分位数计算（P50、P95、P99）。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看 API 的 P95 延迟（95% 的请求在这个时间内完成）
histogram_quantile(0.95, http_request_duration_seconds_bucket)

# 查看平均响应时间
rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])

# 查看慢接口（&gt;1秒）
histogram_quantile(0.99, http_request_duration_seconds_bucket{path="/api/some-slow-endpoint"})</code></pre><h5>3. HTTP 并发请求数（Gauge）</h5><pre><code class="go">var httpRequestsInFlight = promauto.NewGauge(
    prometheus.GaugeOpts{
        Name: "http_requests_in_flight",
        Help: "Current number of HTTP requests being served",
    },
)</code></pre><p><strong>用途</strong>：实时显示当前正在处理的请求数量。</p><p><strong>查询示例</strong>：</p><pre><code class="promql"># 查看当前并发数
http_requests_in_flight

# 查看最近 5 分钟的最大并发数
max_over_time(http_requests_in_flight[5m])</code></pre><h4>中间件实现</h4><pre><code class="go">func PrometheusMetrics() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 跳过 metrics 端点本身（避免递归）
        if c.Request.URL.Path == "/metrics" {
            c.Next()
            return
        }
        
        // 1. 增加并发计数
        httpRequestsInFlight.Inc()
        defer httpRequestsInFlight.Dec()
        
        // 2. 记录开始时间
        start := time.Now()
        
        // 3. 执行业务逻辑
        c.Next()
        
        // 4. 计算请求耗时
        duration := time.Since(start).Seconds()
        
        // 5. 收集指标
        status := strconv.Itoa(c.Writer.Status())
        method := c.Request.Method
        path := c.FullPath() // 使用路由路径而不是原始URL（避免高基数）
        
        httpRequestsTotal.WithLabelValues(method, path, status).Inc()
        httpRequestDuration.WithLabelValues(method, path).Observe(duration)
    }
}</code></pre><p><strong>关键设计考虑</strong>：</p><ol><li><p><strong>避免高基数问题</strong>：</p><ul><li>✅ 使用 <code>c.FullPath()</code> 而不是 <code>c.Request.URL.Path</code></li><li>原因：路由路径固定（如 <code>/api/user/:id</code>），而实际 URL 可能有无数个（<code>/api/user/1</code>, <code>/api/user/2</code>, ...）</li><li>高基数会导致 Prometheus 内存暴涨</li></ul></li><li><p><strong>跳过 /metrics 端点</strong>：</p><ul><li>避免 Prometheus 抓取自身指标时产生递归记录</li><li>减少无意义的指标数据</li></ul></li><li><p><strong>使用 <code>defer</code> 确保计数正确</strong>：</p><ul><li>即使请求 panic，并发计数也会正确递减</li></ul></li></ol><h4>Metrics 端点暴露</h4><pre><code class="go">// server/api/v1/system/metrics.go
type MetricsApi struct{}

func (m *MetricsApi) GetMetrics(c *gin.Context) {
    handler := promhttp.Handler()
    handler.ServeHTTP(c.Writer, c.Request)
}

// server/initialize/router.go
metricsApi := &amp;system.MetricsApi{}
router.GET("/metrics", metricsApi.GetMetrics)</code></pre><p>访问 <a href="https://link.segmentfault.com/?enc=vFhZs7vrLZ%2FJK4nNGa8P%2Fg%3D%3D.wT%2By6KJL20UCy9ppos%2Bp4B2mFSKbu5QZj%2FOi%2Frcobs8%3D" rel="nofollow" target="_blank">http://localhost:8888/metrics</a> 可以看到：</p><pre><code># HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",path="/api/health",status="200"} 145
http_requests_total{method="POST",path="/base/login",status="200"} 23
http_requests_total{method="GET",path="/api/user/getList",status="200"} 67

# HELP http_request_duration_seconds HTTP request latency in seconds
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="0.005"} 142
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="0.01"} 145
http_request_duration_seconds_bucket{method="GET",path="/api/health",le="+Inf"} 145
http_request_duration_seconds_sum{method="GET",path="/api/health"} 0.523
http_request_duration_seconds_count{method="GET",path="/api/health"} 145

# HELP http_requests_in_flight Current number of HTTP requests being served
# TYPE http_requests_in_flight gauge
http_requests_in_flight 2</code></pre><h4>Prometheus 配置</h4><p>在 <code>deploy/monitoring/prometheus.yml</code> 中配置抓取任务：</p><pre><code class="yaml">scrape_configs:
  - job_name: 'ewp-backend'
    static_configs:
      - targets: ['host.containers.internal:8888']
    metrics_path: '/metrics'
    scrape_interval: 15s  # 每 15 秒抓取一次</code></pre><ul><li><code>host.containers.internal</code> 是 Podman 访问宿主机的特殊域名</li><li>容器内的 Prometheus 通过这个域名连接到宿主机的 8888 端口</li><li>在生产环境中，应该使用服务发现（Kubernetes Service、Consul 等）</li></ul><hr/><h3>🏥 健康检查实现</h3><h4>为什么需要健康检查？</h4><p><strong>场景</strong>：</p><ul><li>Kubernetes 需要知道 Pod 是否存活（Liveness）</li><li>负载均衡器需要知道实例是否就绪（Readiness）</li><li>运维人员需要快速判断服务状态</li></ul><h4>Liveness Probe - 存活探针</h4><p><strong>用途</strong>：判断应用进程是否存活，如果失败，Kubernetes 会重启 Pod。</p><pre><code class="go">// server/api/v1/system/health.go
func (h *HealthApi) GetHealth(c *gin.Context) {
    response.OkWithData(gin.H{
        "status":    "ok",
        "timestamp": time.Now().Format(time.RFC3339),
    }, c)
}</code></pre><p><strong>API 返回</strong>：</p><pre><code class="bash">GET /api/health

{
  "code": 0,
  "data": {
    "status": "ok",
    "timestamp": "2026-01-05T10:15:30Z"
  },
  "msg": "success"
}</code></pre><p><strong>Kubernetes 配置示例</strong>：</p><pre><code class="yaml">livenessProbe:
  httpGet:
    path: /api/health
    port: 8888
  initialDelaySeconds: 30  # 启动后 30 秒开始检查
  periodSeconds: 10        # 每 10 秒检查一次
  timeoutSeconds: 5        # 超时时间 5 秒
  failureThreshold: 3      # 连续失败 3 次才重启</code></pre><h4>Readiness Probe - 就绪探针</h4><p><strong>用途</strong>：判断应用是否准备好接收流量，如果失败，负载均衡器会摘除这个实例。</p><pre><code class="go">func (h *HealthApi) GetReadiness(c *gin.Context) {
    checks := make(map[string]string)
    allHealthy := true

    // 1. 检查 MySQL 连接
    if err := checkMySQLConnection(); err != nil {
        checks["mysql"] = "error: " + err.Error()
        allHealthy = false
    } else {
        checks["mysql"] = "ok"
    }

    // 2. 检查 Redis 连接
    if err := checkRedisConnection(); err != nil {
        checks["redis"] = "error: " + err.Error()
        allHealthy = false
    } else {
        checks["redis"] = "ok"
    }

    // 3. 返回结果
    if allHealthy {
        response.OkWithData(gin.H{
            "status": "ready",
            "checks": checks,
        }, c)
    } else {
        c.JSON(503, gin.H{
            "code":   503,
            "status": "not ready",
            "checks": checks,
        })
    }
}</code></pre><p><strong>API 返回示例</strong>：</p><p>成功时（HTTP 200）：</p><pre><code class="json">{
  "code": 0,
  "data": {
    "status": "ready",
    "checks": {
      "mysql": "ok",
      "redis": "ok"
    }
  }
}</code></pre><p>失败时（HTTP 503）：</p><pre><code class="json">{
  "code": 503,
  "status": "not ready",
  "checks": {
    "mysql": "error: connection refused",
    "redis": "ok"
  }
}</code></pre><p><strong>Kubernetes 配置示例</strong>：</p><pre><code class="yaml">readinessProbe:
  httpGet:
    path: /api/ready
    port: 8888
  initialDelaySeconds: 10   # 启动后 10 秒开始检查
  periodSeconds: 5          # 每 5 秒检查一次
  timeoutSeconds: 3         # 超时时间 3 秒
  successThreshold: 1       # 成功 1 次即认为就绪
  failureThreshold: 3       # 连续失败 3 次才摘除</code></pre><h4>健康检查实现细节</h4><pre><code class="go">// 检查 MySQL 连接
func checkMySQLConnection() error {
    if global.EWP_DB == nil {
        return fmt.Errorf("Database connection not initialized")
    }
    
    sqlDB, err := global.EWP_DB.DB()
    if err != nil {
        return err
    }
    
    // 执行一个简单的查询来验证连接
    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
    defer cancel()
    
    return sqlDB.PingContext(ctx)
}

// 检查 Redis 连接
func checkRedisConnection() error {
    if global.EWP_REDIS == nil {
        return fmt.Errorf("Redis connection not initialized")
    }
    
    ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
    defer cancel()
    
    return global.EWP_REDIS.Ping(ctx).Err()
}</code></pre><p><strong>关键设计</strong>：</p><ol><li><strong>超时控制</strong>：每个检查都设置 2 秒超时，避免阻塞</li><li><strong>依赖检查</strong>：只有所有依赖都健康，才返回就绪状态</li><li><strong>详细反馈</strong>：返回每个依赖的具体状态，方便排查</li></ol><hr/><h3>🔍 Correlation ID 请求追踪</h3><h4>为什么需要 Correlation ID？</h4><p><strong>问题场景</strong>：</p><ul><li>用户报告"登录失败"，但日志里有成千上万条记录，如何找到这个用户的请求？</li><li>一个请求经过了多个微服务，如何追踪完整的调用链路？</li><li>如何将前端错误、后端日志、数据库慢查询关联起来？</li></ul><p><strong>Correlation ID 的答案</strong>：</p><ul><li>为每个请求分配唯一的 UUID</li><li>贯穿请求的整个生命周期</li><li>记录在日志、响应头、调用链中</li><li>支持分布式追踪</li></ul><h4>实现方式</h4><pre><code class="go">// server/middleware/correlation.go
const CorrelationIDKey = "X-Request-ID"

func CorrelationID() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 1. 尝试从请求头获取 Correlation ID
        correlationID := c.GetHeader(CorrelationIDKey)
        
        // 2. 如果没有，生成新的 UUID
        if correlationID == "" {
            correlationID = uuid.New().String()
        }
        
        // 3. 存储到 Gin Context（供其他中间件使用）
        c.Set(CorrelationIDKey, correlationID)
        
        // 4. 设置响应头（返回给客户端）
        c.Writer.Header().Set(CorrelationIDKey, correlationID)
        
        c.Next()
    }
}</code></pre><h4>使用场景</h4><h5>场景 1：单次请求追踪</h5><pre><code class="bash"># 客户端发起请求（不带 Request ID）
curl -i http://localhost:8888/api/health

# 响应头包含自动生成的 Request ID
HTTP/1.1 200 OK
X-Request-ID: 3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c
Content-Type: application/json
...</code></pre><p>后端日志中可以看到：</p><pre><code class="json">{
  "correlation_id": "3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c",
  "method": "GET",
  "path": "/api/health",
  "status": 200,
  "duration": "2.5ms"
}</code></pre><h5>场景 2：请求链传播</h5><pre><code class="bash"># 客户端主动带上 Request ID（用于追踪）
curl -H "X-Request-ID: my-custom-request-id" http://localhost:8888/api/user/getList

# 响应会保持相同的 Request ID
HTTP/1.1 200 OK
X-Request-ID: my-custom-request-id
...</code></pre><p><strong>分布式场景</strong>：</p><pre><code>前端 (Request ID: ABC123)
  ↓
API Gateway (透传 ABC123)
  ↓
User Service (使用 ABC123 记录日志)
  ↓ 调用数据库时在 SQL 注释中包含 ABC123
  ↓
MySQL Slow Query Log (/* RequestID: ABC123 */ SELECT ...)</code></pre><h5>场景 3：日志聚合与搜索</h5><p>在 ELK/Loki 中搜索：</p><pre><code># 搜索某个请求的所有日志
correlation_id:"3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c"

# 结果：
# [Service A] 接收请求
# [Service A] 调用 Service B
# [Service B] 查询数据库
# [Service B] 返回结果
# [Service A] 返回响应</code></pre><ol><li><strong>客户端支持</strong>：前端应该在重试、长轮询时保持相同的 Request ID</li><li><strong>下游传播</strong>：调用其他服务时，必须传递 Correlation ID</li><li><strong>数据库注释</strong>：在 SQL 查询中添加注释 <code>/* RequestID: xxx */</code></li><li><strong>错误报告</strong>：错误信息中包含 Correlation ID，方便用户反馈时快速定位</li></ol><hr/><h3>📝 结构化日志系统</h3><h4>为什么需要结构化日志？</h4><p><strong>传统文本日志的问题</strong>：</p><pre><code>2026-01-05 10:15:30 [INFO] User login from IP 192.168.1.100
2026-01-05 10:15:31 [INFO] API /api/user/getList took 45ms, status=200</code></pre><ul><li>❌ 难以解析和搜索</li><li>❌ 没有统一格式</li><li>❌ 缺少关键信息（如 Request ID）</li><li>❌ 无法高效聚合分析</li></ul><p><strong>结构化日志（JSON）的优势</strong>：</p><pre><code class="json">{
  "timestamp": "2026-01-05T10:15:30Z",
  "level": "info",
  "correlation_id": "3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c",
  "method": "GET",
  "path": "/api/user/getList",
  "status": 200,
  "duration": "45ms",
  "duration_ms": 45,
  "ip": "192.168.1.100",
  "user_agent": "Mozilla/5.0...",
  "user_id": "123"
}</code></pre><ul><li>✅ 机器可读，易于解析</li><li>✅ 字段统一，便于搜索</li><li>✅ 包含完整上下文</li><li>✅ 支持高效聚合查询</li></ul><h4>实现方式</h4><pre><code class="go">// server/middleware/logger.go
func StructuredLogger() gin.HandlerFunc {
    return func(c *gin.Context) {
        // 1. 记录开始时间
        start := time.Now()
        
        // 2. 执行业务逻辑
        c.Next()
        
        // 3. 计算请求耗时
        duration := time.Since(start)
        
        // 4. 获取 Correlation ID
        correlationID, _ := c.Get(CorrelationIDKey)
        
        // 5. 获取用户信息（如果已认证）
        userID := ""
        if claims, exists := c.Get("claims"); exists {
            if jwtClaims, ok := claims.(*systemReq.CustomClaims); ok {
                userID = strconv.Itoa(int(jwtClaims.BaseClaims.ID))
            }
        }
        
        // 6. 构造结构化日志
        logData := map[string]interface{}{
            "timestamp":      time.Now().Format(time.RFC3339),
            "correlation_id": correlationID,
            "method":         c.Request.Method,
            "path":           c.Request.URL.Path,
            "status":         c.Writer.Status(),
            "duration":       duration.String(),
            "duration_ms":    duration.Milliseconds(),
            "ip":             c.ClientIP(),
            "user_agent":     c.Request.UserAgent(),
        }
        
        if userID != "" {
            logData["user_id"] = userID
        }
        
        // 7. 输出 JSON 日志
        logJSON, _ := json.Marshal(logData)
        global.EWP_LOG.Info(string(logJSON))
    }
}</code></pre><h4>日志字段说明</h4><table><thead><tr><th>字段</th><th>类型</th><th>说明</th><th>示例</th></tr></thead><tbody><tr><td><code>timestamp</code></td><td>string</td><td>日志时间（ISO 8601）</td><td><code>2026-01-05T10:15:30Z</code></td></tr><tr><td><code>correlation_id</code></td><td>string</td><td>请求追踪 ID</td><td><code>3c5f6a8b-1e2d-4f9a...</code></td></tr><tr><td><code>method</code></td><td>string</td><td>HTTP 方法</td><td><code>GET</code>, <code>POST</code></td></tr><tr><td><code>path</code></td><td>string</td><td>请求路径</td><td><code>/api/user/getList</code></td></tr><tr><td><code>status</code></td><td>int</td><td>HTTP 状态码</td><td><code>200</code>, <code>404</code>, <code>500</code></td></tr><tr><td><code>duration</code></td><td>string</td><td>人类可读的耗时</td><td><code>45ms</code>, <code>1.2s</code></td></tr><tr><td><code>duration_ms</code></td><td>int</td><td>毫秒数（便于聚合）</td><td><code>45</code>, <code>1200</code></td></tr><tr><td><code>ip</code></td><td>string</td><td>客户端 IP</td><td><code>192.168.1.100</code></td></tr><tr><td><code>user_agent</code></td><td>string</td><td>浏览器标识</td><td><code>Mozilla/5.0...</code></td></tr><tr><td><code>user_id</code></td><td>string</td><td>用户 ID（如已登录）</td><td><code>123</code></td></tr></tbody></table><h4>日志查询示例</h4><p><strong>在 ELK 中查询</strong>：</p><pre><code class="javascript">// 查询某个用户的所有请求
user_id:"123"

// 查询慢请求（&gt;1秒）
duration_ms:&gt;1000

// 查询错误请求
status:&gt;=500

// 查询某个时间段的请求
timestamp:[2026-01-05T10:00:00Z TO 2026-01-05T11:00:00Z]

// 聚合分析：统计各状态码的数量
{
  "aggs": {
    "status_codes": {
      "terms": { "field": "status" }
    }
  }
}</code></pre><p><strong>在 Loki 中查询</strong>：</p><pre><code class="logql"># 查询某个 Request ID 的所有日志
{job="ewp-backend"} | json | correlation_id="3c5f6a8b-1e2d-4f9a-b3c7-8d6e5f4a9b2c"

# 统计每分钟的请求数
sum(rate({job="ewp-backend"}[1m]))

# 查询 P99 响应时间
histogram_quantile(0.99, sum(rate({job="ewp-backend"} | json | __error__="" | unwrap duration_ms [5m])) by (le))</code></pre><h4>日志级别规范</h4><pre><code class="go">// 不同场景使用不同日志级别
global.EWP_LOG.Debug(logJSON)   // 调试信息（生产环境不输出）
global.EWP_LOG.Info(logJSON)    // 正常请求（我们的选择）
global.EWP_LOG.Warn(logJSON)    // 警告信息（如慢查询）
global.EWP_LOG.Error(logJSON)   // 错误信息（如 5xx）
global.EWP_LOG.Fatal(logJSON)   // 致命错误（进程退出）</code></pre><p><strong>日志级别选择</strong>：</p><ul><li><code>Info</code>：正常的 HTTP 请求（200, 201, 204）</li><li><code>Warn</code>：可能有问题的请求（401, 403, 404, 请求超时）</li><li><code>Error</code>：服务器错误（500, 502, 503, panic）</li></ul><hr/><h4>后续优化方向</h4><h5>1. 监控告警</h5><pre><code class="yaml"># Prometheus 告警规则示例
groups:
  - name: ewp_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) &gt; 0.05
        for: 5m
        annotations:
          summary: "High error rate detected"
          
      - alert: HighLatency
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) &gt; 1
        for: 5m
        annotations:
          summary: "API latency P95 &gt; 1s"</code></pre><h5>2. 分布式追踪</h5><p>集成 Jaeger 实现完整的分布式追踪：</p><pre><code class="go">// 使用 OpenTelemetry 标准
import "go.opentelemetry.io/otel"

func TracingMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        ctx, span := tracer.Start(c.Request.Context(), c.FullPath())
        defer span.End()
        
        // 传播 Trace Context
        c.Request = c.Request.WithContext(ctx)
        c.Next()
    }
}</code></pre><h5>3. 日志聚合</h5><p>将日志发送到 ELK 或 Loki：</p><pre><code class="yaml"># Promtail 配置（Loki）
clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: ewp-backend
    static_configs:
      - targets:
          - localhost
        labels:
          job: ewp-backend
          __path__: /path/to/server.log</code></pre><hr/><h3>📚 相关文档</h3><h4>技术文档</h4><ul><li><a href="https://link.segmentfault.com/?enc=Yz4uS7F8lG6EwiushlaSsw%3D%3D.qsiiKDIHnE9rx0I4lZVwr5S6Ps24yjSG1J27ZAwCSz8texgm%2BDs4%2Fg%2FQ7vAO9rl8Mh9rNP6VS2te7Iaso7ehvQ%3D%3D" rel="nofollow" target="_blank">Prometheus 官方文档</a> - 指标收集与监控</li><li><a href="https://link.segmentfault.com/?enc=Z9ywPQsuBT1gpD5oveGegA%3D%3D.2rom8yt40ga4zD%2BwsEF8VWDhCFCSew0arhIW4A4R4kj0XuoxHXaqYRqDXUaNxV%2FK" rel="nofollow" target="_blank">Prometheus 最佳实践</a> - 指标命名规范</li><li><a href="https://link.segmentfault.com/?enc=%2FMuR5nY8aRaXIhB7Vqctsg%3D%3D.1FyRQSJ53OtO9Y4%2Fs89YXG7XPCYKa9fH6FH%2FEPc6zcM23H1Txy2r1XWgrTVW8KB0hIWlptDV4uwj7SM8%2FGougg%3D%3D" rel="nofollow" target="_blank">OpenTelemetry Go SDK</a> - 分布式追踪标准</li><li><a href="https://link.segmentfault.com/?enc=MAmhXiTtHyZ1OLmT%2FBfDzQ%3D%3D.VNvN%2BYS7re7kbNXj1hGWAqA5AyGpaN6n7qKN5wp9I7c%3D" rel="nofollow" target="_blank">Structured Logging in Go</a> - Zap 日志库</li></ul><h4>Kubernetes 健康检查</h4><ul><li><a href="https://link.segmentfault.com/?enc=6j%2BCvyS98BOHXA59C3CHqg%3D%3D.bUVj%2ByoR6Jp2bEm0t7NhvYZR1rmuI%2B8iDIGoQKZOAy7XM%2FnH%2F9V3pCXA4eaci1AnGOD8DBwf29VPGodu1VktKe7UzQGuYaOF2ErfDTUC9ZZ4oois8mEU7zBCT4%2BE4TNMF2jHDvVg8a2YU8xY6InJRg%3D%3D" rel="nofollow" target="_blank">Configure Liveness, Readiness Probes</a> - K8s 探针配置</li><li><a href="https://link.segmentfault.com/?enc=cAuH4tOPf9rx4kr%2Bf%2FAPqA%3D%3D.hqc3OtqjlKxovDfDHDxmSy%2F0FFiawv8chgL9AsdTpNcugrLDuU5soFobkAZasi8Ii3y6z0R613394Ogu6Yf5rqnkf3LTo3dnQoudFRvexIkyHS%2Bi8j0MkrJRqEwga%2FM21Y5%2BfqhPizhBv8fhNSZL4tqBgqYBL%2Bkh%2BNqrXWs9dpVqEFF4m3fBLToC6i6yZaZx3a5F4Ws6cd1XlPL%2FknTGcg%3D%3D" rel="nofollow" target="_blank">Health Check Best Practices</a> - Google 最佳实践</li></ul><h4>可观测性理论</h4><ul><li><a href="https://link.segmentfault.com/?enc=qMy%2FXzpVcHckJgsvXVg%2BOQ%3D%3D.zIfqqLnzOKn03VXUUVa9Qa%2FIU5WykPQW0XQBZJdLL6hDNH4wT45khUmBZzOCCot1qP3PYAhUjkT0N6c3HNiUn2rhpVfel96n5iuZHKPRWJsE582pbTWPKya4UKNd26Ld" rel="nofollow" target="_blank">The Three Pillars of Observability</a> - O'Reilly 可观测性理论</li><li><a href="https://link.segmentfault.com/?enc=l8qSGClQefDWb3JoMi%2Fv2w%3D%3D.TFEngsQGHJxraCzNO3NGEzGfjDnq%2BtmLg1PH%2FktrUVIjyo%2BislMRV6A%2Fq5cQFCXNWtUW5kRsLByTEDOx1xxNHBCoYC0kCWDtr%2FmTo1gARKg%3D" rel="nofollow" target="_blank">Logs vs Metrics vs Traces</a> - 三者的区别与联系</li></ul><h3>🔗 项目地址</h3><ul><li><strong>GitHub</strong>: <a href="https://link.segmentfault.com/?enc=MgCJsioo07iqjAODRxOjmg%3D%3D.FFdnSqFPmML%2Brsg0IrMA19upaggki00XCVwNmQ59iPDWhto2oqa13CLKE4G6D8WR" rel="nofollow" target="_blank">https://github.com/Mythetic/web_platform</a></li></ul>]]></description></item><item>    <title><![CDATA[【赵渝强老师】OceanBase的配置文件与配置项 赵渝强老师 ]]></title>    <link>https://segmentfault.com/a/1190000047525398</link>    <guid>https://segmentfault.com/a/1190000047525398</guid>    <pubDate>2026-01-06 23:05:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在OceanBase集群中，OBServer节点工作目录下通常有audit、bin、etc、etc2、etc3、log、run、store等目录，但这些目录并非都是安装必须的。在启动OBServer节点前需要保证etc、log、run、store这4的目录存在，同时store下应该有clog、slog、sstable这3个目录。etc2、etc3是备份配置文件用的，由OBServer节点创建。audit下存放的是审计日志，也由OBServer节点创建。bin目录用于存放observer二进制文件。如下所示：</p><pre><code class="powershell">[root@node11 ~]# tree observer/ -d -L 1
observer/
├── admin
├── audit
├── bin
├── etc                        
├── etc2                    
├── etc3                    
├── lib
├── log                        
├── log_obshell
├── run
└── store -&gt; /root/obdata

11 directories</code></pre><p>etc、etc2、etc3都是配置文件目录。这三个目录里的内容是完全一致的，区别是后两个目录是OBServer节点创建的，第一个目录是启动前需要准备的。etc2和etc3是配置文件额外保存的目录，由配置项config_additional_dir控制。当配置修改以后，除了会写标准的etc/observer.config.bin以外，还会额外在这些目录创建配置项文件。server启动不会读取额外目录的配置项文件，只是作为额外备份。额外目录如果有权限会自动创建，没有权限则日志中报ERROR。</p><p>视频讲解如下：<br/><a href="https://www.bilibili.com/video/BV1oyipB7Ezh/?aid=115840217258517&amp;cid=35196570069" target="_blank">https://www.bilibili.com/video/BV1oyipB7Ezh/?aid=115840217258...</a></p><p>OceanBase数据库的配置项分为集群级配置项和租户级配置项。OBServer节点会将所有的配置项序列化后保存到工作目录下的配置文件etc/observer.config.bin中，之后在这个工作目录下启动OBServer节点都会读取这个配置文件。</p><ul><li>普通租户使用SHOW PARAMETERS语句查看本租户级配置项信息的SQL语句如下：</li></ul><pre><code class="powershell">SHOW PARAMETERS [SHOW_PARAM_OPTS]</code></pre><ul><li>系统租户可以使用SHOW PARAMETERS语句查看集群级配置项和租户级配置项信息。并可通过增加TENANT关键字信息查看指定租户的配置项。</li></ul><pre><code class="powershell">SHOW PARAMETERS [SHOW_PARAM_OPTS] TENANT = tenant_name</code></pre><p>例如：</p><pre><code class="powershell"># 查询所有的配置信息
ob&gt; show parameters;
...+-------------------------+-----------+-------+....
...| name                    | data_type | value |....
...+-------------------------+-----------+-------+....
...| utl_file_open_max       | INT       | 50    |....
...| _use_odps_jni_connector | BOOL      | True  |....
...| ob_java_connector_path  | STRING    |       |....
...| ob_java_opts            | STRING    |       |....
...| ob_java_home            | STRING    |       |....
......

# 执行模糊查询
ob&gt; show parameters like 'datafile%';
...+--------------------------+-----------+-------+...
...| name                     | data_type | value |...
...+--------------------------+-----------+-------+...
...| datafile_disk_percentage | INT       | 0     |...
...| datafile_maxsize         | CAPACITY  | 12G   |...
...| datafile_next            | CAPACITY  | 1G    |...
...| datafile_size            | CAPACITY  | 1G    |...
...+--------------------------+-----------+-------+...
4 rows in set (0.015 sec)</code></pre>]]></description></item><item>    <title><![CDATA[某知名it培训班前端三阶段vue相关面试题 Nedpill ]]></title>    <link>https://segmentfault.com/a/1190000047525408</link>    <guid>https://segmentfault.com/a/1190000047525408</guid>    <pubDate>2026-01-06 23:04:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>1. Vue 的核心是什么？</h4><p>Vue 的核心主要包含两点：</p><ul><li>​<strong>数据驱动（Data-Driven）</strong>​：视图由数据状态决定，数据变更自动更新 DOM，无需手动操作 DOM；</li><li>​<strong>组件化（Component-Based）</strong>​：将页面拆分为独立、可复用的组件，降低耦合度，提升开发效率；</li><li>补充：核心还包括响应式系统、虚拟 DOM 等底层支撑能力。</li></ul><h4>2. 请简述你对 Vue 的理解</h4><p>Vue 是一套​<strong>渐进式 JavaScript 框架</strong>​，核心定位是“渐进式”——可以按需使用核心功能（如响应式、组件），也可结合路由（Vue Router）、状态管理（Vuex/Pinia）等生态扩展复杂应用；</p><ul><li>设计理念：轻量、易用、高效，兼顾开发体验和运行性能；</li><li>核心特性：响应式数据绑定、组件化、指令系统、虚拟 DOM、生命周期等；</li><li>应用场景：从简单的表单页面到复杂的单页应用（SPA）均可覆盖，是前端主流框架之一。</li></ul><h4>3. 请简述 Vue 的单向数据流</h4><p>Vue 的单向数据流核心规则：​<strong>数据只能从父组件流向子组件，子组件不能直接修改父组件传递的 props</strong>​；</p><ul><li><p>具体表现：</p><ul><li>父组件通过 props 向子组件传值，子组件只读 props，不能直接修改；</li><li>若子组件需修改数据，需通过触发父组件的自定义事件，由父组件修改源数据，再反向更新子组件 props；</li></ul></li><li>目的：保证数据流向可追溯，避免多个组件随意修改数据导致状态混乱，符合“单向绑定”的设计思想。</li></ul><h4>4. Vue 常用的修饰符有哪些</h4><p>Vue 的修饰符按用途可分为三类，核心常用如下：</p><table><thead><tr><th>类型</th><th>常用修饰符</th><th>作用举例</th></tr></thead><tbody><tr><td>事件修饰符</td><td>.stop、.prevent、.once</td><td>.stop 阻止事件冒泡，.prevent 阻止默认行为，.once 只触发一次</td></tr><tr><td>按键修饰符</td><td>.enter、.esc、.tab</td><td>监听特定按键触发事件（如 @keyup.enter）</td></tr><tr><td>表单修饰符</td><td>.trim、.number、.lazy</td><td>.trim 去除输入首尾空格，.number 转为数字，.lazy 失去焦点后更新数据</td></tr><tr><td>鼠标修饰符</td><td>.left、.right、.middle</td><td>监听鼠标特定按键（左键/右键/中键）</td></tr></tbody></table><h4>5. v-text 与 {{}}的区别</h4><p>两者均用于渲染文本，核心区别：</p><ul><li><p>​<strong>{{}}（插值表达式）</strong>​：</p><ul><li>可嵌入 HTML 标签内（如 <code>&lt;div&gt;姓名：{{name}}&lt;/div&gt;</code>）；</li><li>存在“闪烁问题”（页面加载时可能先显示 <code>{{name}}</code> 再渲染值，可通过 <code>v-cloak</code> 解决）；</li><li>支持简单表达式（如 <code>{{age + 1}}</code>）；</li></ul></li><li><p>​<strong>v-text</strong>​：</p><ul><li>是指令，需直接绑定在标签上（如 <code>&lt;div v-text="name"&gt;&lt;/div&gt;</code>）；</li><li>无闪烁问题，覆盖标签内所有内容（包括子节点）；</li><li>不支持复杂表达式，仅接收变量/简单值；</li></ul></li><li>补充：两者均会转义 HTML（若需渲染 HTML 用 <code>v-html</code>）。</li></ul><h4>6. v-on 可以绑定多个方法吗？</h4><p>可以，有两种实现方式：</p><ul><li><p>方式 1：绑定一个方法数组（Vue 2.4+ 支持）</p><pre><code class="Plain">&lt;button @click="[handleClick1, handleClick2]()"&gt;点击触发多个方法&lt;/button&gt;</code></pre></li><li><p>方式 2：绑定一个统一方法，内部调用多个子方法</p><pre><code class="Plain">&lt;button @click="handleAll"&gt;点击触发多个方法&lt;/button&gt;
&lt;script&gt;
export default {
  methods: {
    handleAll() {
      this.handleClick1();
      this.handleClick2();
    },
    handleClick1() { /* 逻辑1 */ },
    handleClick2() { /* 逻辑2 */ }
  }
}
&lt;/script&gt;</code></pre></li><li>注意：数组方式中方法需加 <code>()</code> 执行，否则仅定义不触发。</li></ul><h4>7. Vue 循环的 key 作用</h4><p><code>key</code> 是 Vue 列表渲染的核心属性，作用：</p><ul><li>​<strong>唯一标识节点</strong>​：Vue 根据 key 判断节点是否为同一节点，避免复用错误（如输入框值错乱）；</li><li>​<strong>提升更新效率</strong>​：当列表数据变化时，Vue 通过 key 精准定位需要更新的节点，而非重新渲染整个列表；</li><li><p>注意：</p><ul><li>key 需用唯一值（如 id），避免用 index（index 会随数据顺序变化，失去标识意义）；</li><li>无 key 时 Vue 会采用“就地更新”策略，可能导致 DOM 复用异常。</li></ul></li></ul><h4>8. 什么是计算属性？</h4><p>计算属性（computed）是 Vue 用于处理<strong>派生数据</strong>的特性，基于依赖数据动态计算值：</p><ul><li><p>核心特性：</p><ul><li>​<strong>缓存性</strong>​：依赖数据不变时，多次访问计算属性只会执行一次计算，提升性能；</li><li>​<strong>响应式</strong>​：依赖数据变化时，计算属性自动重新计算并更新视图；</li><li>支持 get/set（默认 get，set 可手动修改依赖数据）；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  data() {
    return { a: 1, b: 2 };
  },
  computed: {
    sum() { // 只读计算属性
      return this.a + this.b;
    },
    fullName: { // 可读写计算属性
      get() { return this.firstName + ' ' + this.lastName; },
      set(val) { const [first, last] = val.split(' '); this.firstName = first; this.lastName = last; }
    }
  }
}
&lt;/script&gt;</code></pre></li></ul><h4>9. Vue 单页面（SPA）的优缺点</h4><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>1. 无页面刷新，体验接近原生 App</td><td>1. 首屏加载慢（需加载整包 JS/CSS）</td></tr><tr><td>2. 组件化复用性高</td><td>2. SEO 不友好（页面内容动态渲染）</td></tr><tr><td>3. 前后端分离，开发效率高</td><td>3. 路由切换需手动处理缓存/滚动</td></tr><tr><td>4. 数据管理更集中</td><td>4. 打包体积大，需按需加载优化</td></tr></tbody></table><h4>10. Vuex 是什么？怎么使用？在哪些场景下使用？</h4><ul><li>​<strong>定义</strong>​：Vuex 是 Vue 官方的​<strong>集中式状态管理库</strong>​，用于管理多组件共享的状态（如用户信息、全局配置）；</li><li><p>​<strong>使用步骤</strong>​：</p><ul><li>安装：<code>npm install vuex --save</code>；</li><li><p>创建 store：</p><pre><code class="JavaScript">import Vue from 'vue';
import Vuex from 'vuex';
Vue.use(Vuex);
export default new Vuex.Store({
  state: { count: 0 }, // 状态
  mutations: { increment(state) { state.count++ } }, // 同步修改状态
  actions: { asyncIncrement({ commit }) { setTimeout(() =&gt; commit('increment'), 1000) } }, // 异步操作
  getters: { doubleCount(state) { return state.count * 2 } }, // 派生状态
  modules: { /* 模块拆分 */ }
});</code></pre></li><li>挂载到 Vue 实例：<code>new Vue({ store, ... })</code>；</li><li><p>组件中使用：</p><ol><li>读取 state：<code>this.$store.state.count</code> 或 <code>mapState</code> 辅助函数；</li><li>修改 state：<code>this.$store.commit('increment')</code>（同步）/ <code>this.$store.dispatch('asyncIncrement')</code>（异步）；</li></ol></li></ul></li><li><p>​<strong>使用场景</strong>​：</p><ul><li>多组件共享同一状态（如购物车、用户登录状态）；</li><li>组件层级深，props/emit 传值繁琐；</li><li>需要追踪状态变更（Vuex 可记录状态修改日志）。</li></ul></li></ul><h4>11. Vuex 与 Pinia 的区别</h4><p>Pinia 是 Vue 3 推荐的状态管理库，替代 Vuex 4，核心区别：</p><table><thead><tr><th>维度</th><th>Vuex</th><th>Pinia</th></tr></thead><tbody><tr><td>核心结构</td><td>分 state/mutations/actions/getters/modules</td><td>仅 state/actions/getters（无 mutations/modules）</td></tr><tr><td>模块化</td><td>需通过 modules 嵌套，命名空间复杂</td><td>每个 store 独立，天然模块化，无需命名空间</td></tr><tr><td>TypeScript</td><td>支持差，需手动类型声明</td><td>原生支持 TS，类型推断更友好</td></tr><tr><td>代码简洁度</td><td>冗余（如 mutations 必须同步）</td><td>简洁（actions 可同步/异步，无需 commit）</td></tr><tr><td>Vue 版本支持</td><td>Vue 2/3（Vuex 3 对应 Vue 2，Vuex 4 对应 Vue 3）</td><td>主要支持 Vue 3（也可兼容 Vue 2）</td></tr><tr><td>调试</td><td>依赖 Vue Devtools，需配置</td><td>原生集成 Vue Devtools，调试更友好</td></tr></tbody></table><h4>12. Vue 路由的跳转方式</h4><p>Vue Router 的跳转分两类：声明式（模板）和编程式（JS）：</p><ul><li><p>​<strong>声明式（&lt;router-link&gt;）</strong>​：</p><pre><code class="Plain">&lt;!-- 基础跳转 --&gt;
&lt;router-link to="/home"&gt;首页&lt;/router-link&gt;
&lt;!-- 带参数 --&gt;
&lt;router-link :to="{ path: '/user', query: { id: 1 } }"&gt;用户页&lt;/router-link&gt;
&lt;router-link :to="{ name: 'User', params: { id: 1 } }"&gt;用户页&lt;/router-link&gt;</code></pre></li><li><p>​<strong>编程式（&amp;dollar;router.push/replace/go）</strong>​：</p><pre><code class="JavaScript">// 基础跳转
this.$router.push('/home');
// 带query参数（路径拼接，如/user?id=1）
this.$router.push({ path: '/user', query: { id: 1 } });
// 带params参数（需路由配置name，如/user/1）
this.$router.push({ name: 'User', params: { id: 1 } });
// 替换当前历史记录（不新增历史）
this.$router.replace('/home');
// 前进/后退
this.$router.go(-1); // 后退一页</code></pre></li></ul><h4>13. 跨域的解决方式</h4><p>跨域是浏览器同源策略限制（协议、域名、端口任一不同即跨域），前端常用解决方案：</p><ol><li><p>​<strong>Vue CLI 代理（开发环境）</strong>​：</p><pre><code class="JavaScript">// vue.config.js
module.exports = {
  devServer: {
    proxy: {
      '/api': {
        target: 'http://localhost:3000', // 后端接口地址
        changeOrigin: true, // 开启跨域
        pathRewrite: { '^/api': '' } // 重写路径
      }
    }
  }
}</code></pre></li><li>​<strong>后端 CORS（跨域资源共享）</strong>​：后端设置响应头 <code>Access-Control-Allow-Origin: *</code>（或指定域名）；</li><li>​<strong>JSONP</strong>​：仅支持 GET 请求，通过动态创建 <code>&lt;script&gt;</code> 标签请求；</li><li>​<strong>Nginx 反向代理</strong>​：生产环境通过 Nginx 转发请求，统一域名；</li><li>​<strong>WebSocket</strong>​：基于 TCP 协议，无跨域限制。</li></ol><h4>14. Vue 生命周期请简述</h4><p>Vue 生命周期是组件从<strong>创建到销毁</strong>的全过程，分 8 个核心阶段（Vue 2）：</p><ol><li>​<strong>创建阶段</strong>​：beforeCreate（实例初始化，数据/方法未挂载）→ created（数据/方法挂载完成，DOM 未生成）；</li><li>​<strong>挂载阶段</strong>​：beforeMount（编译模板，即将挂载 DOM）→ mounted（DOM 挂载完成，可操作 DOM）；</li><li>​<strong>更新阶段</strong>​：beforeUpdate（数据更新，DOM 未重新渲染）→ updated（DOM 重新渲染完成）；</li><li>​<strong>销毁阶段</strong>​：beforeDestroy（实例即将销毁，数据/方法仍可用）→ destroyed（实例销毁，所有监听/绑定解除）；</li></ol><ul><li>Vue 3 补充：组合式 API 中用 <code>onMounted</code>/<code>onUpdated</code> 等钩子替代选项式，新增 <code>setup</code>（替代 beforeCreate/created）。</li></ul><h4>15. Vue 生命周期的作用</h4><p>生命周期钩子允许开发者在组件不同阶段插入自定义逻辑，核心作用：</p><ol><li>​<strong>初始化逻辑</strong>​：created 中请求数据、初始化变量；</li><li>​<strong>DOM 操作</strong>​：mounted 中操作 DOM（如初始化第三方插件）；</li><li>​<strong>数据更新处理</strong>​：updated 中处理 DOM 更新后的逻辑；</li><li>​<strong>资源清理</strong>​：beforeDestroy 中清除定时器、取消事件监听，避免内存泄漏；</li><li>​<strong>性能优化</strong>​：按需执行逻辑，避免无效代码（如仅在挂载后请求数据）。</li></ol><h4>16. DOM 渲染在哪个生命周期阶段内完成</h4><ul><li>核心结论：<strong>mounted 阶段</strong>完成 DOM 渲染；</li><li><p>细节：</p><ul><li>beforeMount：模板已编译，但未挂载到 DOM（&amp;dollar;el 为虚拟 DOM）；</li><li>mounted：真实 DOM 挂载完成，&amp;dollar;el 指向真实 DOM 节点，可安全操作 DOM；</li><li>若组件包含子组件，mounted 仅表示当前组件 DOM 挂载完成，子组件可能仍在挂载中。</li></ul></li></ul><h4>17. Vue 路由的实现</h4><p>Vue Router 的核心实现依赖​<strong>前端路由原理</strong>​，分两步：</p><ol><li><p>​<strong>路由匹配</strong>​：</p><ol><li>定义路由规则（routes 数组），每个规则包含 path、component 等；</li><li>Vue Router 监听 URL 变化，匹配对应路由规则；</li></ol></li><li><p>​<strong>视图渲染</strong>​：</p><ol><li>通过 <code>&lt;router-view&gt;</code> 组件作为路由出口，匹配到的组件渲染到该位置；</li><li>底层依赖 Vue 的组件系统，通过动态组件切换实现视图更新；</li></ol></li></ol><ul><li>补充：路由模式（hash/history）决定 URL 的表现形式，底层分别基于 hashchange 事件和 History API。</li></ul><h4>18. 简述 Vue 路由模式 hash 和 history</h4><table><thead><tr><th>维度</th><th>hash 模式（默认）</th><th>history 模式</th></tr></thead><tbody><tr><td>URL 表现</td><td>带#（如 <a href="https://link.segmentfault.com/?enc=r8a5yCwecMsdUbkO%2FzY3Aw%3D%3D.WYI5QhkGMWgBMaubelBKEqvIZxWNZhezbDstpVHrtmw%3D" rel="nofollow" target="_blank">http://xxx/#/home</a>）</td><td>无#（如 <a href="https://link.segmentfault.com/?enc=7yiSjxLa2IEmjjMcSB7gqg%3D%3D.O1%2BrtyfMKPz3PompIG4ybQ%3D%3D" rel="nofollow" target="_blank">http://xxx/home</a>）</td></tr><tr><td>底层原理</td><td>基于 hashchange 事件，#后的内容不会发送到服务器</td><td>基于 HTML5 History API（pushState/replaceState）</td></tr><tr><td>服务器配置</td><td>无需配置，刷新页面不会 404</td><td>需配置后端，刷新页面需重定向到 index.html（否则 404）</td></tr><tr><td>兼容性</td><td>兼容所有浏览器（包括 IE）</td><td>仅支持 HTML5 浏览器</td></tr><tr><td>SEO</td><td>部分搜索引擎不识别#后内容</td><td>更友好，SEO 效果更好</td></tr></tbody></table><h4>19. Vue 路由传参方式，params 与 query 方式和区别</h4><ul><li>​<strong>传参方式</strong>​：分 query 和 params 两种核心方式，均支持声明式/编程式；</li><li>​<strong>核心区别</strong>​：</li></ul><table><thead><tr><th>维度</th><th>query 参数</th><th>params 参数</th></tr></thead><tbody><tr><td>URL 表现</td><td>拼接在路径后（?key=value）</td><td>嵌入路径中（/user/1）</td></tr><tr><td>路由配置</td><td>无需特殊配置</td><td>需在路由 path 中定义（如/user/:id）</td></tr><tr><td>刷新页面</td><td>参数不会丢失</td><td>若路由未定义参数，刷新后丢失</td></tr><tr><td>取值方式</td><td>this.&amp;dollar;route.query.key</td><td>this.&amp;dollar;route.params.key</td></tr><tr><td>可选性</td><td>可传可不传</td><td>路由定义的参数必须传（否则跳转失败）</td></tr></tbody></table><ul><li><p>示例：</p><pre><code class="JavaScript">// query传参
this.$router.push({ path: '/user', query: { id: 1 } }); // URL: /user?id=1
// params传参（需路由name）
this.$router.push({ name: 'User', params: { id: 1 } }); // URL: /user/1</code></pre></li></ul><h4>20. Vue 数据绑定的几种方式</h4><p>Vue 数据绑定分三类，核心是响应式绑定：</p><ol><li><p>​<strong>单向绑定</strong>​：</p><ol><li>插值表达式 <code>{{}}</code>：渲染文本；</li><li><code>v-bind</code>（简写 <code>:</code>）：绑定属性（如 <code>:src="imgUrl"</code>、<code>:class="className"</code>）；</li></ol></li><li><p>​<strong>双向绑定</strong>​：</p><ol><li><code>v-model</code>：主要用于表单元素（如 <code>&lt;input v-model="value"&gt;</code>），本质是 <code>v-bind</code>+<code>v-on</code> 的语法糖；</li></ol></li><li><p>​<strong>一次性绑定</strong>​：</p><ol><li><code>v-once</code>：绑定后数据变化不再更新视图（如 <code>&lt;div v-once&gt;{{name}}&lt;/div&gt;</code>）。</li></ol></li></ol><h4>21. Vue 注册一个全局组件</h4><p>全局组件注册后，所有 Vue 实例/组件均可直接使用，步骤：</p><pre><code class="JavaScript">// 1. 定义组件
const MyComponent = {
  template: `&lt;div&gt;{{msg}}&lt;/div&gt;`,
  data() {
    return { msg: '全局组件' };
  }
};
// 2. 注册全局组件（Vue 2）
import Vue from 'vue';
Vue.component('MyComponent', MyComponent);

// Vue 3（createApp方式）
import { createApp } from 'vue';
const app = createApp({});
app.component('MyComponent', MyComponent);
app.mount('#app');</code></pre><ul><li>注意：全局组件需在 Vue 实例创建前注册，否则无法使用。</li></ul><h4>22. Vue 的路由钩子/路由守卫有哪些</h4><p>Vue Router 的路由守卫分三类，用于控制路由跳转权限：</p><ol><li><p>​<strong>全局守卫</strong>​（所有路由生效）：</p><ol><li><code>router.beforeEach</code>：路由跳转前触发（常用作登录验证）；</li><li><code>router.afterEach</code>：路由跳转后触发（无权限控制）；</li><li><code>router.beforeResolve</code>：所有组件内守卫和异步路由解析完成后触发；</li></ol></li><li><p>​<strong>路由独享守卫</strong>​（单个路由生效）：</p><pre><code class="JavaScript">const routes = [
  {
    path: '/user',
    component: User,
    beforeEnter: (to, from, next) =&gt; { // 仅/user路由生效
      if (/* 验证 */) next();
      else next('/login');
    }
  }
];</code></pre></li><li><p>​<strong>组件内守卫</strong>​（组件内生效）：</p><ol><li><code>beforeRouteEnter</code>：进入组件前触发（无法访问 this，需通过 next 回调）；</li><li><code>beforeRouteUpdate</code>：组件复用（如动态路由）时触发；</li><li><code>beforeRouteLeave</code>：离开组件前触发（如提示未保存）。</li></ol></li></ol><h4>23. Vue 中如何进行动态路由设置？有哪些方式？怎么获取传递过来的参数？</h4><ul><li>​<strong>动态路由定义</strong>​：在路由 path 中用 <code>:参数名</code> 定义，匹配任意值；</li><li><p>​<strong>定义方式</strong>​：</p><pre><code class="JavaScript">// 1. 基础动态路由
const routes = [
  { path: '/user/:id', name: 'User', component: User }
];
// 2. 可选参数（加?）
{ path: '/user/:id?', component: User }
// 3. 通配符（匹配所有）
{ path: '*', component: NotFound }</code></pre></li><li><p>​<strong>获取参数</strong>​：</p><ul><li>组件内通过 <code>this.$route.params</code> 获取（如 <code>this.$route.params.id</code>）；</li><li><p>组合式 API 中用 <code>useRoute</code>：</p><pre><code class="JavaScript">import { useRoute } from 'vue-router';
const route = useRoute();
console.log(route.params.id);</code></pre></li></ul></li></ul><h4>24. Element UI 中常用组件有哪些？请简述并说下他们的属性有哪些</h4><p>Element UI 是 Vue 2 主流 UI 库，核心常用组件及属性：</p><table><thead><tr><th>组件</th><th>用途</th><th>核心属性</th></tr></thead><tbody><tr><td>Button</td><td>按钮</td><td>type（primary/success）、size（small/medium）、disabled、icon</td></tr><tr><td>Input</td><td>输入框</td><td>v-model、placeholder、disabled、clearable、type（text/password）</td></tr><tr><td>Table</td><td>表格</td><td>data（数据源）、columns（列配置）、pagination（分页）、border、height</td></tr><tr><td>Form</td><td>表单</td><td>model（表单数据）、rules（校验规则）、label-width、inline</td></tr><tr><td>Dialog</td><td>弹窗</td><td>visible（显示/隐藏）、title、width、modal（遮罩）、close-on-click-modal</td></tr><tr><td>Select</td><td>下拉选择</td><td>v-model、options（选项）、multiple（多选）、disabled</td></tr><tr><td>Pagination</td><td>分页</td><td>total（总条数）、page-size（每页条数）、current-page（当前页）、layout（布局）</td></tr></tbody></table><h4>25. Vue CLI 中如何自定义指令</h4><p>自定义指令用于扩展 DOM 操作，分全局/局部指令：</p><ul><li><p>​<strong>全局自定义指令</strong>​（main.js）：</p><pre><code class="JavaScript">// Vue 2
import Vue from 'vue';
// 注册v-focus指令（自动聚焦输入框）
Vue.directive('focus', {
  inserted(el) { // 指令绑定到元素并插入DOM时触发
    el.focus();
  }
});

// Vue 3
import { createApp } from 'vue';
const app = createApp({});
app.directive('focus', {
  mounted(el) { el.focus(); }
});</code></pre></li><li><p>​<strong>局部自定义指令</strong>​（组件内）：</p><pre><code class="Plain">&lt;script&gt;
export default {
  directives: {
    focus: {
      inserted(el) { el.focus(); }
    }
  }
}
&lt;/script&gt;</code></pre></li><li>指令钩子：bind（绑定）、inserted（插入 DOM）、update（更新）等（Vue 3 调整为 created/mounted/updated）。</li></ul><h4>26. Vue 中指令有哪些</h4><p>Vue 指令分<strong>内置指令</strong>和​<strong>自定义指令</strong>​，核心内置指令：</p><table><thead><tr><th>类别</th><th>指令</th><th>用途</th></tr></thead><tbody><tr><td>数据绑定</td><td>v-text、v-html、v-bind</td><td>渲染文本/HTML、绑定属性</td></tr><tr><td>事件绑定</td><td>v-on</td><td>绑定事件（简写 @）</td></tr><tr><td>双向绑定</td><td>v-model</td><td>表单数据双向绑定</td></tr><tr><td>条件渲染</td><td>v-if、v-else、v-show</td><td>条件显示/隐藏 DOM</td></tr><tr><td>列表渲染</td><td>v-for</td><td>循环渲染列表</td></tr><tr><td>其他</td><td>v-once、v-cloak、v-pre</td><td>一次性绑定、解决闪烁、跳过编译</td></tr></tbody></table><h4>27. Vue 如何定义一个过滤器</h4><p>过滤器用于格式化数据（Vue 3 已移除，推荐用计算属性/方法替代），Vue 2 定义方式：</p><ul><li><p>​<strong>全局过滤器</strong>​（main.js）：</p><pre><code class="JavaScript">import Vue from 'vue';
// 注册全局过滤器（格式化时间）
Vue.filter('formatTime', (value) =&gt; {
  return new Date(value).toLocaleString();
});</code></pre></li><li><p>​<strong>局部过滤器</strong>​（组件内）：</p><pre><code class="Plain">&lt;script&gt;
export default {
  filters: {
    formatTime(value) {
      return new Date(value).toLocaleString();
    }
  }
}
&lt;/script&gt;</code></pre></li><li>使用：<code>{{ time | formatTime }}</code> 或 <code>v-bind:title="time | formatTime"</code>。</li></ul><h4>28. 对 Vue 中 keep-alive 的理解</h4><p><code>keep-alive</code> 是 Vue 的内置组件，用于​<strong>缓存组件实例</strong>​，避免重复创建/销毁：</p><ul><li><p>核心特性：</p><ul><li>包裹动态组件时，缓存不活动的组件，而非销毁；</li><li>触发组件的 <code>activated</code>（激活）/<code>deactivated</code>（失活）钩子；</li></ul></li><li><p>常用属性：</p><ul><li><code>include</code>：仅缓存指定组件（如 <code>include="User,Home"</code>）；</li><li><code>exclude</code>：排除指定组件；</li><li><code>max</code>：最大缓存数量（超出则销毁最久未使用的组件）；</li></ul></li><li>应用场景：路由切换时缓存表单数据、列表滚动位置等（如 <code>&lt;keep-alive&gt;&lt;router-view&gt;&lt;/router-view&gt;&lt;/keep-alive&gt;</code>）。</li></ul><h4>29. 如何让组件中的 CSS 在当前组件生效</h4><p>通过<strong>样式隔离</strong>实现，核心方式：</p><ol><li><p>​<strong>scoped 属性</strong>​（推荐）：</p><pre><code class="Plain">&lt;style scoped&gt;
.box { color: red; } // 仅当前组件生效
&lt;/style&gt;</code></pre><ol><li>原理：Vue 为组件 DOM 添加唯一属性（如 <code>data-v-xxx</code>），CSS 自动添加属性选择器，实现隔离；</li></ol></li><li><p>​<strong>CSS Modules</strong>​：</p><pre><code class="Plain">&lt;style module&gt;
.box { color: red; }
&lt;/style&gt;
&lt;template&gt;
  &lt;div :class="$style.box"&gt;内容&lt;/div&gt;
&lt;/template&gt;</code></pre><ol><li>原理：类名被编译为唯一哈希值，避免冲突；</li></ol></li><li><p>​<strong>深度选择器</strong>​（如需修改子组件样式）：</p><pre><code class="Plain">&lt;style scoped&gt;
::v-deep .child-box { color: blue; } // Vue 2
:deep(.child-box) { color: blue; } // Vue 3
&lt;/style&gt;</code></pre></li></ol><h4>30. Vue 生命周期一共有几个阶段</h4><ul><li><p>​<strong>Vue 2</strong>​：分 4 个大阶段，8 个核心钩子：</p><ul><li>创建阶段（2 个）：beforeCreate、created；</li><li>挂载阶段（2 个）：beforeMount、mounted；</li><li>更新阶段（2 个）：beforeUpdate、updated；</li><li>销毁阶段（2 个）：beforeDestroy、destroyed；</li><li>补充：还有 activated/deactivated（keep-alive 组件）、errorCaptured（错误捕获）等钩子；</li></ul></li><li>​<strong>Vue 3</strong>​：组合式 API 中钩子更细分，核心阶段一致，钩子名调整为 onXxx（如 onMounted），新增 setup（替代 beforeCreate/created）。</li></ul><h4>31. MVVM 和 MVC 的区别</h4><p>两者均为软件架构模式，核心区别：</p><table><thead><tr><th>维度</th><th>MVC（Model-View-Controller）</th><th>MVVM（Model-View-ViewModel）</th></tr></thead><tbody><tr><td>核心角色</td><td>Model（数据）、View（视图）、Controller（控制器，连接 M/V）</td><td>Model（数据）、View（视图）、ViewModel（桥梁，双向绑定）</td></tr><tr><td>数据流向</td><td>单向（Model→Controller→View）</td><td>双向（View←→ViewModel←→Model）</td></tr><tr><td>耦合度</td><td>View 和 Model 需通过 Controller 通信，耦合较高</td><td>View 和 Model 完全解耦，由 ViewModel 中转</td></tr><tr><td>核心特性</td><td>手动更新视图（需 Controller 操作 DOM）</td><td>自动更新视图（数据驱动，ViewModel 实现响应式）</td></tr><tr><td>应用框架</td><td>jQuery、Backbone.js</td><td>Vue、React（类 MVVM）、Angular</td></tr></tbody></table><h4>32. Vue 组件中的 data 为什么是函数</h4><p>核心原因：​<strong>保证组件实例的独立性</strong>​，避免多个组件实例共享同一数据对象；</p><ul><li><p>原理：</p><ul><li>若 data 是对象，所有组件实例会引用同一个对象，修改一个实例的 data 会影响其他实例；</li><li>若 data 是函数，每次创建组件实例时，函数返回一个新的对象，各实例数据独立；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  // 正确：函数返回新对象
  data() {
    return { count: 0 };
  },
  // 错误：所有实例共享count
  // data: { count: 0 }
}
&lt;/script&gt;</code></pre></li><li>补充：根实例（new Vue({})）的 data 可以是对象（仅一个实例，无共享问题）。</li></ul><h4>33. Vue 双向绑定原理</h4><p>Vue 2 基于​<strong>Object.defineProperty</strong>​，Vue 3 基于​<strong>Proxy</strong>​，核心流程：</p><ul><li><p>​<strong>Vue 2</strong>​：</p><ul><li>数据劫持：通过 <code>Object.defineProperty</code> 监听 data 中所有属性的 get/set；</li><li>依赖收集：模板编译时，访问属性触发 get，收集依赖（Watcher）；</li><li>派发更新：修改属性触发 set，通知 Watcher 更新视图；</li><li>缺陷：无法监听数组下标/长度变化、对象新增属性；</li></ul></li><li><p>​<strong>Vue 3</strong>​：</p><ul><li>数据代理：通过 <code>Proxy</code> 代理整个 data 对象，支持监听数组/对象所有变化；</li><li>依赖收集/派发更新逻辑与 Vue 2 类似，但效率更高；</li></ul></li><li>核心公式：<code>MVVM = 数据劫持 + 发布-订阅模式</code>。</li></ul><h4>34. Vue 组件中的传值方式</h4><p>组件传值分 7 种核心场景，覆盖父子/兄弟/跨级：</p><table><thead><tr><th>场景</th><th>传值方式</th><th>示例</th></tr></thead><tbody><tr><td>父 → 子</td><td>props</td><td>子组件定义 props，父组件 <code>:prop="value"</code></td></tr><tr><td>子 → 父</td><td>自定义事件（&amp;dollar;emit）</td><td>子组件 <code>this.$emit('event', data)</code>，父组件 <code>@event="handle"</code></td></tr><tr><td>兄弟组件</td><td>事件总线/Vuex/Pinia</td><td>事件总线：<code>Vue.prototype.$bus = new Vue()</code>，<code>$bus.$emit/$on</code></td></tr><tr><td>跨级组件</td><td>provide/inject</td><td>父组件 provide 提供数据，子组件 inject 注入</td></tr><tr><td>任意组件</td><td>Vuex/Pinia</td><td>全局状态管理，直接读取/修改共享数据</td></tr><tr><td>路由传参</td><td>query/params</td><td>跳转路由时携带参数</td></tr><tr><td>本地存储</td><td>localStorage/sessionStorage</td><td>持久化传值（非响应式）</td></tr></tbody></table><h4>35. Bootstrap 的原理</h4><p>Bootstrap 是前端 UI 框架，核心原理：</p><ol><li>​<strong>栅格系统</strong>​：基于 Flex/Grid 布局，将页面分为 12 列，通过 <code>col-xs-*</code>/<code>col-md-*</code> 等类实现响应式布局；</li><li>​<strong>响应式设计</strong>​：通过媒体查询（@media）适配不同屏幕尺寸（移动端/平板/PC）；</li><li>​<strong>预定义样式</strong>​：提供按钮、表单、导航等组件的 CSS 样式，直接复用；</li><li>​<strong>jQuery 插件</strong>​：内置轮播、弹窗、下拉菜单等交互插件（Bootstrap 5 移除 jQuery，改用原生 JS）；</li><li>​<strong>变量/混合器</strong>​（Sass 版本）：支持自定义主题，通过变量覆盖默认样式。</li></ol><h4>36. Vue 兄弟组件传值</h4><p>兄弟组件无直接传值通道，常用 3 种方式：</p><ol><li><p>​<strong>事件总线（Vue 2）</strong>​：</p><pre><code class="JavaScript">// 1. 全局注册总线
import Vue from 'vue';
Vue.prototype.$bus = new Vue();
// 2. 组件A发送事件
this.$bus.$emit('sendData', data);
// 3. 组件B接收事件（mounted中）
this.$bus.$on('sendData', (data) =&gt; { /* 处理数据 */ });
// 4. 销毁时解绑（避免内存泄漏）
beforeDestroy() {
  this.$bus.$off('sendData');
}</code></pre></li><li>​<strong>Vuex/Pinia</strong>​：将共享数据存入全局状态，兄弟组件直接读取/修改；</li><li>​<strong>父组件中转</strong>​：组件 A→ 父组件（&amp;dollar;emit）→ 组件 B（props）。</li></ol><h4>37. 如果一个组件需要在多个项目中使用怎么办</h4><p>核心方案：​<strong>组件封装并发布为 npm 包</strong>​，步骤：</p><ol><li><p>​<strong>组件封装</strong>​：</p><ol><li>抽离组件的通用逻辑，参数通过 props 暴露，事件通过&amp;dollar;emit 触发；</li><li>避免硬编码，支持自定义样式/配置；</li></ol></li><li><p>​<strong>打包发布</strong>​：</p><ol><li>用 Vue CLI/lib 模式打包：<code>vue-cli-service build --target lib --name my-component src/index.js</code>；</li><li>配置 package.json（main 指向打包后的文件，指定版本、依赖等）；</li><li>发布到 npm（<code>npm publish</code>）；</li></ol></li><li><p>​<strong>其他方案</strong>​：</p><ol><li>搭建私有 npm 仓库（如 Verdaccio），存放内部组件；</li><li>通过 Git submodule 引入组件源码（适合频繁修改的场景）；</li><li>使用 Monorepo 管理多项目共享组件（如 pnpm workspace）。</li></ol></li></ol><h4>38. 简述槽口（Slot）</h4><p>Slot（插槽）是 Vue 组件的内容分发机制，允许父组件向子组件插入自定义内容：</p><ul><li><p>​<strong>核心类型</strong>​：</p><ul><li>​<strong>默认插槽</strong>​：子组件 <code>&lt;slot&gt;&lt;/slot&gt;</code>，父组件直接写内容；</li><li>​<strong>具名插槽</strong>​：子组件 <code>&lt;slot name="header"&gt;&lt;/slot&gt;</code>，父组件 <code>&lt;template v-slot:header&gt;内容&lt;/template&gt;</code>（简写 <code>#header</code>）；</li><li><p>​<strong>作用域插槽</strong>​：子组件向父组件传递数据，父组件自定义渲染逻辑：</p><pre><code class="Plain">&lt;!-- 子组件 --&gt;
&lt;slot :user="user"&gt;&lt;/slot&gt;
&lt;!-- 父组件 --&gt;
&lt;template v-slot:default="slotProps"&gt;
  {{ slotProps.user.name }}
&lt;/template&gt;</code></pre></li></ul></li><li>作用：提升组件灵活性，实现组件内容的自定义渲染。</li></ul><h4>39. 简述 watch</h4><p>watch 是 Vue 的​<strong>侦听器</strong>​，用于监听数据变化并执行自定义逻辑：</p><ul><li><p>核心特性：</p><ul><li>监听单个/多个数据（如 data、props、计算属性）；</li><li>支持深度监听（deep: true）、立即执行（immediate: true）；</li><li>可监听对象属性（如 <code>'user.name'</code>）；</li></ul></li><li><p>示例：</p><pre><code class="Plain">&lt;script&gt;
export default {
  data() {
    return { user: { name: '张三' }, count: 0 };
  },
  watch: {
    // 监听基本类型
    count(newVal, oldVal) {
      console.log('count变化：', newVal, oldVal);
    },
    // 监听对象（深度监听）
    user: {
      handler(newVal) {
        console.log('user变化：', newVal);
      },
      deep: true,
      immediate: true // 初始化时执行一次
    },
    // 监听对象单个属性
    'user.name'(newVal) {
      console.log('姓名变化：', newVal);
    }
  }
}
&lt;/script&gt;</code></pre></li></ul><h4>40. 简述 Vant UI</h4><p>Vant UI 是有赞开源的​<strong>移动端 Vue UI 组件库</strong>​，核心特点：</p><ol><li>​<strong>适配场景</strong>​：专注移动端（H5/小程序），适配各种屏幕尺寸；</li><li>​<strong>版本支持</strong>​：Vant 2 支持 Vue 2，Vant 3/4 支持 Vue 3；</li><li>​<strong>核心组件</strong>​：Button、Cell、List、PullRefresh、Swipe、Dialog、Toast 等；</li><li><p>​<strong>特性</strong>​：</p><ol><li>轻量：按需引入，减少打包体积；</li><li>易用：API 简洁，文档完善；</li><li>兼容：支持小程序（微信/支付宝）、H5、App（通过 uni-app）；</li></ol></li><li><p>​<strong>使用方式</strong>​：</p><pre><code class="Bash">npm i vant
# 按需引入（需配置babel-plugin-import）
import { Button } from 'vant';
Vue.use(Button);</code></pre></li></ol><h4>41. 计算属性与 watch 的区别</h4><table><thead><tr><th>维度</th><th>计算属性（computed）</th><th>侦听器（watch）</th></tr></thead><tbody><tr><td>核心用途</td><td>派生数据（如 a+b）</td><td>监听数据变化执行副作用（如请求、修改 DOM）</td></tr><tr><td>缓存性</td><td>有缓存，依赖不变则不重新计算</td><td>无缓存，数据变化即触发</td></tr><tr><td>返回值</td><td>必须有返回值</td><td>无需返回值</td></tr><tr><td>语法</td><td>声明式（类似变量）</td><td>命令式（函数）</td></tr><tr><td>适用场景</td><td>简单的同步数据计算</td><td>异步操作、复杂的逻辑处理</td></tr><tr><td>深度监听</td><td>自动深度监听对象属性</td><td>需手动设置 deep: true</td></tr></tbody></table><ul><li><p>示例对比：</p><pre><code class="JavaScript">// 计算属性：适合简单计算
computed: { fullName() { return this.first + ' ' + this.last; } }
// watch：适合异步逻辑
watch: { firstName(newVal) { this.$axios.get('/api', { params: { name: newVal } }); } }</code></pre></li></ul><h4>42. MVVM 框架是什么？它和其他框架的区别是什么？哪些场景适合？</h4><ul><li>​<strong>MVVM 定义</strong>​：MVVM（Model-View-ViewModel）是前端架构模式，核心是 ViewModel 作为 View 和 Model 的桥梁，实现数据与视图的双向绑定；</li><li><p>​<strong>与其他框架的区别</strong>​：</p><ul><li>与 jQuery（无架构）：MVVM 数据驱动，无需手动操作 DOM；jQuery 需手动选择 DOM、修改内容；</li><li>与 React（类 MVVM）：React 核心是单向数据流（State→View），需手动 setState 更新；Vue（MVVM）原生支持双向绑定；</li><li>与 Angular（全量 MVVM）：Vue 更轻量、易用，Angular 功能全但学习成本高；</li></ul></li><li><p>​<strong>适用场景</strong>​：</p><ul><li>中大型单页应用（SPA）：数据交互频繁，需高效管理状态；</li><li>表单类应用：双向绑定简化表单处理；</li><li>移动端/H5 应用：轻量、高性能，适配移动端；</li><li>不适用场景：简单静态页面（如纯展示页），用 jQuery/原生 JS 更高效。</li></ul></li></ul><h4>43. Vue 首屏加载慢的原因，怎么解决的，白屏时间怎么检测，怎么解决白屏问题</h4><h5>（1）首屏加载慢的原因</h5><ol><li>打包体积大：未按需引入组件/库、未压缩代码、包含无用依赖；</li><li>网络问题：请求资源过大、网络延迟高；</li><li>渲染阻塞：JS 执行时间长，阻塞 DOM 渲染；</li><li>服务器响应慢：接口请求耗时久。</li></ol><h5>（2）解决方法</h5><ol><li><p>​<strong>优化打包体积</strong>​：</p><ol><li>按需引入（如 Element UI/Vant）；</li><li>路由懒加载：<code>const Home = () =&gt; import('./Home.vue')</code>；</li><li>压缩代码（Vue CLI 默认开启）、移除 console；</li><li>CDN 引入第三方库（如 Vue、Vue Router），减少打包体积；</li></ol></li><li><p>​<strong>网络优化</strong>​：</p><ol><li>开启 Gzip 压缩（Nginx 配置）；</li><li>使用 HTTP/2、静态资源 CDN；</li><li>预加载/预取（<code>&lt;link rel="preload"&gt;</code>）；</li></ol></li><li><p>​<strong>渲染优化</strong>​：</p><ol><li>首屏骨架屏（Skeleton）；</li><li>异步组件、懒加载图片；</li><li>服务端渲染（SSR）/静态站点生成（SSG）。</li></ol></li></ol><h5>（3）白屏时间检测</h5><ol><li><p>​<strong>浏览器 Performance 面板</strong>​：</p><ol><li>记录首屏时间（First Contentful Paint, FCP）、最大内容绘制（LCP）；</li><li>查看 JS 执行、资源加载耗时；</li></ol></li><li><p>​<strong>代码埋点</strong>​：</p><pre><code class="JavaScript">// 监听DOM加载完成
document.addEventListener('DOMContentLoaded', () =&gt; {
  console.log('DOM加载完成时间：', Date.now() - performance.timing.navigationStart);
});
// 监听首屏绘制
new PerformanceObserver((entryList) =&gt; {
  const entry = entryList.getEntries()[0];
  console.log('首屏时间：', entry.startTime);
}).observe({ type: 'paint', buffered: true });</code></pre></li></ol><h5>（4）解决白屏问题</h5><ol><li>骨架屏：首屏加载时显示占位骨架，替代空白；</li><li>预加载关键资源：优先加载首屏所需 CSS/JS；</li><li>服务端渲染（SSR）：服务端生成首屏 HTML，直接返回；</li><li>减小首屏 JS 体积：路由懒加载、按需引入，只加载首屏必要代码。</li></ol><h4>44. Vue 双向数据绑定中，怎么实现一侧数据改变之后通知另一侧</h4><p>核心是​<strong>发布-订阅模式</strong>​，分两步：</p><ol><li><p>​<strong>数据劫持/代理</strong>​：</p><ol><li>Vue 2 用 <code>Object.defineProperty</code> 监听数据的 setter，Vue 3 用 <code>Proxy</code> 监听对象变化；</li></ol></li><li><p>​<strong>依赖收集与派发更新</strong>​：</p><ol><li>当视图渲染访问数据时（getter），收集依赖（Watcher，关联视图）；</li><li>当数据修改时（setter），触发派发更新，通知所有相关 Watcher 执行更新逻辑，重新渲染视图；</li><li>反向（视图 → 数据）：<code>v-model</code> 监听输入事件（input/change），修改对应数据，完成双向绑定。</li></ol></li></ol><h4>45. Vuex 流程</h4><p>Vuex 的核心数据流向是​<strong>单向循环</strong>​：</p><ol><li>组件通过 <code>dispatch</code> 触发​<strong>Action</strong>​（可执行异步操作）；</li><li>Action 通过 <code>commit</code> 提交​<strong>Mutation</strong>​；</li><li>Mutation 修改​<strong>State</strong>​（唯一能修改 State 的方式）；</li><li>State 变化触发​<strong>Getter</strong>​（可选，派生数据）；</li><li>组件监听 State/Getter 变化，更新视图；</li></ol><ul><li>简化流程：<code>组件 → Action → Mutation → State → 组件</code>（同步操作可直接 <code>commit</code> Mutation）。</li></ul><h4>46. Vuex 怎么请求异步数据</h4><p>Vuex 中异步数据请求需在 <strong>Action</strong> 中执行，步骤：</p><pre><code class="JavaScript">// 1. 定义Action
const store = new Vuex.Store({
  state: { userList: [] },
  mutations: {
    SET_USER_LIST(state, data) {
      state.userList = data;
    }
  },
  actions: {
    // 异步请求数据
    async fetchUserList({ commit }) {
      try {
        const res = await this.$axios.get('/api/user/list');
        commit('SET_USER_LIST', res.data); // 提交Mutation修改State
      } catch (err) {
        console.error('请求失败：', err);
      }
    }
  }
});
// 2. 组件中触发Action
this.$store.dispatch('fetchUserList');</code></pre><ul><li>注意：Mutation 只能执行同步操作，异步操作必须放在 Action 中。</li></ul><h4>47. Vuex 中 Action 如何提交给 Mutation</h4><p>Action 通过 <code>commit</code> 方法提交 Mutation，有两种方式：</p><ol><li><p>​<strong>解构 context 对象</strong>​（推荐）：</p><pre><code class="JavaScript">actions: {
  increment({ commit }) { // 解构commit
    commit('INCREMENT'); // 提交Mutation
  }
}</code></pre></li><li><p>​<strong>完整 context 对象</strong>​：</p><pre><code class="JavaScript">actions: {
  increment(context) {
    context.commit('INCREMENT'); // context包含commit/dispatch/state等
  }
}</code></pre></li></ol><ul><li>带参数提交：<code>commit('INCREMENT', payload)</code>（payload 为任意类型数据）。</li></ul><h4>48. route 与 router 的区别</h4><table><thead><tr><th>维度</th><th>&amp;dollar;route</th><th>&amp;dollar;router</th></tr></thead><tbody><tr><td>核心含义</td><td>当前路由信息对象</td><td>路由实例（导航控制器）</td></tr><tr><td>包含内容</td><td>path、params、query、name 等</td><td>push、replace、go 等导航方法</td></tr><tr><td>用途</td><td>读取当前路由参数/信息</td><td>触发路由跳转</td></tr><tr><td>示例</td><td><code>this.$route.params.id</code></td><td><code>this.$router.push('/home')</code></td></tr></tbody></table><h4>49. Vuex 有哪几种状态和属性</h4><p>Vuex 的核心属性（5 个）：</p><ol><li>​<strong>state</strong>​：存储全局状态（唯一数据源）；</li><li>​<strong>mutations</strong>​：同步修改 state 的方法（唯一入口）；</li><li>​<strong>actions</strong>​：异步操作，提交 mutation 修改 state；</li><li>​<strong>getters</strong>​：派生状态（类似计算属性，基于 state 计算）；</li><li>​<strong>modules</strong>​：模块化拆分 state，解决单一状态树体积过大问题。</li></ol><h4>50. Vuex 的 state 特性是？</h4><ol><li>​<strong>唯一性</strong>​：整个应用只有一个 state（单一状态树）；</li><li>​<strong>响应式</strong>​：state 中的数据是响应式的，修改后视图自动更新；</li><li>​<strong>只读性</strong>​：不能直接修改 state，必须通过 mutation；</li><li>​<strong>可模块化</strong>​：通过 modules 拆分 state，每个 module 有独立的 state/mutations 等；</li><li>​<strong>组件访问</strong>​：通过 <code>this.$store.state</code> 或 <code>mapState</code> 辅助函数访问。</li></ol><h4>51. Vuex 的 getter 特性是？</h4><ol><li>​<strong>缓存性</strong>​：依赖的 state 不变时，多次访问 getter 不会重新计算；</li><li>​<strong>派生状态</strong>​：基于 state 计算新值（如过滤列表、计算总数）；</li><li>​<strong>只读性</strong>​：不能直接修改 getter，需修改依赖的 state；</li><li>​<strong>可传参</strong>​：通过返回函数实现传参（如 <code>getters.getUserById(state) =&gt; (id) =&gt; state.users.find(u =&gt; u.id === id)</code>）；</li><li>​<strong>组件访问</strong>​：通过 <code>this.$store.getters</code> 或 <code>mapGetters</code> 辅助函数访问。</li></ol><h4>52. Vuex 的 mutation 特性是？</h4><ol><li>​<strong>同步性</strong>​：必须是同步函数（异步操作会导致状态变更无法追踪）；</li><li>​<strong>唯一修改入口</strong>​：只能通过 mutation 修改 state；</li><li>​<strong>参数</strong>​：第一个参数是 state，第二个是 payload（可选，传递数据）；</li><li>​<strong>可追踪</strong>​：Vue Devtools 可记录 mutation 的调用记录，便于调试；</li><li>​<strong>调用方式</strong>​：通过 <code>store.commit('mutationName', payload)</code>，不能直接调用。</li></ol><h4>53. Vuex 的 action 特性是？</h4><ol><li>​<strong>异步性</strong>​：支持异步操作（如请求数据、定时器）；</li><li>​<strong>不直接修改 state</strong>​：需提交 mutation 修改 state；</li><li>​<strong>参数</strong>​：第一个参数是 context 对象（包含 commit/dispatch/state/getters）；</li><li>​<strong>支持 Promise</strong>​：action 可返回 Promise，便于链式调用；</li><li>​<strong>调用方式</strong>​：通过 <code>store.dispatch('actionName', payload)</code>，组件中可通过 <code>async/await</code> 等待执行完成。</li></ol><h4>54. Vuex 的优势</h4><ol><li>​<strong>集中式管理</strong>​：多组件共享状态统一存储，避免状态分散；</li><li>​<strong>可追踪性</strong>​：所有状态修改通过 mutation，便于调试和日志记录；</li><li>​<strong>单向数据流</strong>​：状态变更流程清晰，降低维护成本；</li><li>​<strong>模块化</strong>​：支持 modules 拆分状态，适配大型应用；</li><li>​<strong>生态集成</strong>​：与 Vue Devtools 深度集成，可视化调试；</li><li>​<strong>复用性</strong>​：公共逻辑（如数据请求）可封装在 action 中，多组件复用。</li></ol><h4>55. 简述 Vue 路由懒加载</h4><p>路由懒加载（按需加载）是<strong>代码分割</strong>的一种方式，核心是将路由组件拆分为独立的 JS 包，只有访问该路由时才加载对应的包：</p><ul><li><p>​<strong>实现方式</strong>​：</p><pre><code class="JavaScript">// 基础懒加载
const Home = () =&gt; import('./views/Home.vue');
// 带分包命名（webpackChunkName），便于打包后识别
const User = () =&gt; import(/* webpackChunkName: "user" */ './views/User.vue');
const routes = [
  { path: '/home', component: Home },
  { path: '/user', component: User }
];</code></pre></li><li><p>​<strong>优势</strong>​：</p><ul><li>减小首屏 JS 包体积，提升首屏加载速度；</li><li>按需加载，节省带宽和资源；</li></ul></li><li>​<strong>原理</strong>​：基于 ES6 的动态 import 语法，webpack 打包时自动拆分代码块。</li></ul><h4>56. v-for 和 v-if 的区别</h4><table><thead><tr><th>维度</th><th>v-for</th><th>v-if</th></tr></thead><tbody><tr><td>核心用途</td><td>循环渲染列表</td><td>条件渲染 DOM</td></tr><tr><td>优先级</td><td>更高（Vue 2）</td><td>更低（Vue 2）</td></tr><tr><td>执行时机</td><td>每次渲染都循环所有数据</td><td>条件为 true 时才渲染 DOM</td></tr><tr><td>性能</td><td>循环所有数据，性能开销大</td><td>仅渲染满足条件的 DOM</td></tr><tr><td>结合使用</td><td>不推荐直接结合（Vue 2 中 v-for 优先级高，会先循环再判断，性能差）</td><td>推荐用 computed 过滤数据后再循环</td></tr></tbody></table><ul><li><p>优化建议：</p><pre><code class="JavaScript">// 错误：v-for和v-if同节点
&lt;div v-for="item in list" v-if="item.visible"&gt;{{item.name}}&lt;/div&gt;
// 正确：先过滤数据
computed: {
  filteredList() {
    return this.list.filter(item =&gt; item.visible);
  }
}
&lt;div v-for="item in filteredList" :key="item.id"&gt;{{item.name}}&lt;/div&gt;</code></pre></li><li>Vue 3 调整：v-if 优先级高于 v-for，同节点使用会报错，强制开发者先过滤数据。</li></ul>]]></description></item><item>    <title><![CDATA[大模型常见量化方法简介 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047525433</link>    <guid>https://segmentfault.com/a/1190000047525433</guid>    <pubDate>2026-01-06 23:03:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、引言</h2><p>随着大型语言模型（LLM）在具身智能等领域的广泛应用，接下来就该思考如何在有限硬件资源下部署这些模型，量化是其中必不可少的步骤。</p><p>模型量化（Model Quantization）作为一种有效的模型压缩技术，通过将模型中的浮点数参数转换为低比特宽度的整数表示，显著减少了模型的存储和计算需求，同时尽量保持模型的性能。量化的基础知识相信大家都不会陌生，例如必然要介绍两种量化方式：PTQ/QAT。</p><p>QAT 是一种深度融合量化需求与模型训练流程的技术，核心是在模型训练阶段主动嵌入 “伪量化算子”—— 不实际将参数转换为低比特，而是模拟量化过程中的数值截断误差与舍入误差，让模型在学习任务知识的同时，同步适应量化带来的精度损耗。训练中，伪量化算子会实时统计各层输入输出的数据分布（如激活值的极值、权重的方差），动态优化量化参数（如缩放因子、零点），确保量化逻辑与模型参数更新形成配合。这种 “边训练边适配量化” 的特性，能让大语言模型（LLM）在低精度表示（如 4bit、2bit）下，依然保留接近原始浮点模型的性能。</p><p>PTQ 是在 LLM 完全训练完成后执行的量化方案，无需修改模型训练流程，仅需使用少量校准数据（通常 100-1000 条代表性样本）统计模型权重与激活值的分布特征，即可确定量化参数并完成低比特转换。其核心优势在于 “轻量高效”：无需重新训练模型，量化过程仅需数分钟至数小时，无需大规模计算资源；同时无需改动 LLM 架构，可直接适配各类推理框架，兼顾易用性与部署效率。不过，PTQ 的精度天花板相对较低，目前在 4bit 及以下量化时易出现明显精度损失，更适合对精度要求不高、追求快速部署的场景。</p><p>本文重点不是上述两种量化方式，而是将重点介绍四种主流的大模型量化方法：GPTQ、SmoothQuant、AWQ 和旋转量化，下面来分别看一下。</p><h2>二、GPTQ</h2><p>GPTQ（Gradient-based Post-training Quantization）是一种后训练量化方法，其核心思想是利用梯度信息来指导量化过程，从而最小化量化带来的性能损失。论文中仅进行权重量化，权重被量化为 int4 类型，激活值为 float16。</p><p>GPTQ（GPT Quantization）是一种基于最优量化误差最小化的单轮权重量化方法，其核心创新点在于：</p><ul><li>顺序压缩算法：按重要性对模型层进行排序，优先量化对输出影响较小的层</li><li>误差补偿机制：通过梯度下降优化量化参数，减少精度损失</li><li>分组量化策略：将权重矩阵分为小组（Group Size）独立量化，平衡压缩率与精度</li></ul><p>工作流程图如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525435" alt="" title=""/></p><p>GPTQ 通过优化量化误差的目标函数，使得量化后的模型输出尽可能接近原始模型的输出。其优化目标可以表示为：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525436" alt="" title="" loading="lazy"/></p><p>其中，f（⋅） 为模型的前向传播函数，W 为原始浮点权重，Wq 为量化后的权重。</p><p>GPTQ 已被广泛应用于 HuggingFace 等平台，具有完善的工具链和生态支持。但 GPTQ 主要针对模型的权重进行量化，对激活值的处理相对有限，且在量化过程中需要计算梯度信息，会增加额外的计算开销。</p><h2>三、SmoothQuant</h2><p>SmoothQuant 是一种训练后量化方法，旨在解决激活值量化困难的问题。核心理念在于平衡激活值和权重的量化难度。在大模型量化中，激活值通常包含大量离群点，这些离群点会显著拉伸量化范围，增加量化误差。SmoothQuant 提出了一种基于平滑因子的逐通道缩放变换方法，对每个通道的激活值进行缩放以平滑其分布，同时对权重施加反向缩放，确保模型计算的等价性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525437" alt="" title="" loading="lazy"/></p><p>其中，s 为平滑因子，x 为激活值，W 为权重，x′ 和 W′ 分别为经过平滑处理后的激活值和权重。通过这种方式，SmoothQuant 将激活值的量化难度转移到权重上，从而实现更高效的量化。</p><p>SmoothQuant 可以同时对权重和激活值进行 8 位量化，减少模型的存储需求，通过将激活值的异常值减少，SmoothQuant 可以提高推理的效率，但平滑因子 s 的选择对量化效果有较大影响，且不同的模型可能需要不同的平滑因子，需要通过实验进行调优。</p><h2>四、AWQ</h2><p>AWQ（Activation-aware Weight Quantization）是一种自适应权重量化方法，旨在根据激活值的重要性来指导权重的量化过程。其核心思想是识别出对模型输出影响较大的激活值，并根据这些激活值的重要性来调整权重的量化精度。具体而言，AWQ 通过计算激活值的方差来评估其重要性，然后根据重要性为权重分配不同的量化精度：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525438" alt="" title="" loading="lazy"/></p><p>其中，</p><p>$$
\alpha_i
$$</p><p>为第 i 个权重的激活感知度，&amp;dollar;&amp;dollar;b\_i&amp;dollar;&amp;dollar; 为其对应的量化比特数。</p><p>AWQ 方法源于"权重对于 LLM 的性能并不同等重要"的观察，存在约（0.1%-1%）显著权重对大模型性能影响太大，通过跳过这 1% 的重要权重不进行量化，可以大大减少量化误差。根据激活值的重要性动态调整权重的量化精度，这需要计算激活值的方差，实现相对复杂。</p><h2>五、SpinQuant</h2><p>旋转量化（SpinQuant）是一种通过旋转矩阵变换数据空间来实现量化的方法。其核心思想是通过引入旋转矩阵 RRR 将数据映射到新的空间，然后在新的空间中进行量化，从而使得量化误差在数据空间中更加均匀地分布，减少量化误差对模型性能的影响。具体而言，旋转量化的过程可以表示为：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525439" alt="" title="" loading="lazy"/></p><p>其中，w 为原始权重，w′ 为经过旋转变换后的权重，&amp;dollar;&amp;dollar;w'\_q&amp;dollar;&amp;dollar; 为量化后的权重，&amp;dollar;&amp;dollar;w\_q&amp;dollar;&amp;dollar; 为最终的量化权重。</p><p>通过旋转变换数据空间，可以使得量化误差在数据空间中更加均匀地分布，适应不同的模型结构和数据分布，减少量化误差。这个过程需要计算旋转矩阵，并进行矩阵变换，会引入一些计算开销。</p>]]></description></item><item>    <title><![CDATA[linux 常见稳定性问题分析方法 地平线智驾开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047525446</link>    <guid>https://segmentfault.com/a/1190000047525446</guid>    <pubDate>2026-01-06 23:03:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.概述</h2><p>稳定性对项目交付、用户体验有着非常重要的影响，一般定义的稳定性问题是遇到了系统异常重启或者系统卡死等，即无法按照预期为客户继续提供功能和服务。地平线 SoC 平台提供了多种调试手段，去分析系统遇到的稳定性问题。</p><p>首先我们需要了解征程系列的软硬件方案及异常 reset 路径，通过了解异常路径定位发生异常的节点和步骤，定位到问题方向。</p><p>其次，我们需要对发生问题节点提取的调试信息，包括抓取 log、抓取 ramdump 等，对于复杂问题，可能需要不断的迭代 patch 以获取更多调试信息，以缩小问题的范围。</p><p>对于复杂问题，可能需要使用特定的工具去分析问题，如 crash-utility，T32，ftrace 等。</p><p>所以我们将稳定性问题的概述指导分为下面几个章节进行介绍。</p><h2>2. 常见问题</h2><h4>2.1.kernel panic</h4><ul><li>kernel panic 是最常见的系统异常，在 reset\_reason.txt 中显示为 kpanic；</li><li>一般通过 pstore log 就能看到 panic 时的栈和寄存器信息，通过分析上下文配合符号表及 gdb 等工具经常能够直接定位问题；</li><li>对于复杂问题，需要开启 ramdump，抓取 dump 后进行分析。</li></ul><p>一个典型的 kpanic 如下：</p><pre><code class="markdown">&lt;4&gt;[86758.651597] NMI backtrace for cpu 2
&lt;4&gt;[86758.651598] CPU: 2 PID: 105 Comm: khungtaskd Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.651601] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.651602] Call trace:
&lt;4&gt;[86758.651817] NMI backtrace for cpu 4
&lt;4&gt;[86758.651821] CPU: 4 PID: 5110 Comm: glmark2-es2-drm Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.651825] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.651826] pstate: 20400009 (nzCv daif +PAN -UAO -TCO -DIT -SSBS BTYPE=--)
&lt;4&gt;[86758.651829] pc : mas_empty_area_rev+0x2f4/0x574
&lt;4&gt;[86758.651835] lr : mas_empty_area_rev+0x24c/0x574
&lt;4&gt;[86758.651838] sp : ffff800045eafab0
&lt;4&gt;[86758.651839] pmr_save: 000000e0
&lt;4&gt;[86758.651840] x29: ffff800045eafab0 x28: 00000000001fffff x27: 00000000002a0000
&lt;4&gt;[86758.651843] x26: 0000ffdf00000000 x25: 0000000000000018 x24: ffff00040c9d3000
&lt;4&gt;[86758.651846] x23: 0000000000200000 x22: ffff800008d30a28 x21: ffff00041a0cde0c
&lt;4&gt;[86758.651848] x20: 00000000002a0000 x19: ffff800045eafb48 x18: ffffffffffffffc2
&lt;4&gt;[86758.651850] x17: 0000ffdeff1fffff x16: 0000ffdf00000000 x15: 0000ffdeffffffff
&lt;4&gt;[86758.651852] x14: 0000000000200000 x13: 0000ffdeffffffff x12: ffff00041a0cde00
&lt;4&gt;[86758.651854] x11: ffff00041a0cde80 x10: 000000000029ffff x9 : ffffffffffffc005
&lt;4&gt;[86758.651856] x8 : 1fffe00083419bc1 x7 : 0000000000000000 x6 : 0000000000000000
&lt;4&gt;[86758.651858] x5 : 00000000003d0000 x4 : 0000ffdefee30000 x3 : ffff00041a0cde08
&lt;4&gt;[86758.651861] x2 : 0000000000000000 x1 : 0000000000000000 x0 : ffff00041a0cde0c
&lt;4&gt;[86758.651863] Call trace:
&lt;4&gt;[86758.651864]  mas_empty_area_rev+0x2f4/0x574
&lt;4&gt;[86758.651867]  kbase_unmapped_area_topdown.constprop.0+0x150/0x27c [mali_kbase]
&lt;4&gt;[86758.651889]  kbase_context_get_unmapped_area+0x2b0/0x3b0 [mali_kbase]
&lt;4&gt;[86758.651904]  kbase_get_unmapped_area+0x4c/0x7c [mali_kbase]
&lt;4&gt;[86758.651920]  get_unmapped_area+0x60/0xf0
&lt;4&gt;[86758.651925]  do_mmap+0xe4/0x4fc
&lt;4&gt;[86758.651926]  vm_mmap_pgoff+0xf8/0x190
&lt;4&gt;[86758.651929]  ksys_mmap_pgoff+0xb8/0x10c
&lt;4&gt;[86758.651933]  __arm64_sys_mmap+0x38/0x50
&lt;4&gt;[86758.651934]  invoke_syscall+0x50/0x120
&lt;4&gt;[86758.651938]  el0_svc_common.constprop.0+0x58/0x190
&lt;4&gt;[86758.651941]  do_el0_svc+0x34/0xd0
&lt;4&gt;[86758.651944]  el0_svc+0x28/0xb0
&lt;4&gt;[86758.651946]  el0t_64_sync_handler+0xf4/0x120
&lt;4&gt;[86758.651949]  el0t_64_sync+0x19c/0x1a0
&lt;0&gt;[86758.652637] Kernel panic - not syncing: hung_task: blocked tasks
&lt;4&gt;[86758.652639] CPU: 2 PID: 105 Comm: khungtaskd Tainted: P           O       6.1.134-rt51-04836-g0b3e5cbe5431 #13
&lt;4&gt;[86758.652641] Hardware name: Horizon AI Technologies, Inc. HOBOT Sigi-P Matrix (DT)
&lt;4&gt;[86758.652641] Call trace:
&lt;4&gt;[86758.652642]  dump_backtrace+0xe4/0x140
&lt;4&gt;[86758.652644]  show_stack+0x20/0x30
&lt;4&gt;[86758.652645]  dump_stack_lvl+0x64/0x80
&lt;4&gt;[86758.652647]  dump_stack+0x18/0x34
&lt;4&gt;[86758.652649]  panic+0x198/0x394
&lt;4&gt;[86758.652653]  watchdog+0x2d0/0x510
&lt;4&gt;[86758.652655]  kthread+0x138/0x140
&lt;4&gt;[86758.652657]  ret_from_fork+0x10/0x20
&lt;2&gt;[86758.760353] SMP: stopping secondary CPUs
&lt;0&gt;[86758.760362] Kernel Offset: disabled
&lt;0&gt;[86758.760362] CPU features: 0x00000,000700a4,675072ab
&lt;0&gt;[86758.760364] Memory Limit: none</code></pre><p>另外，一些其他的原因也会导致 kernel panic，详细会在 <a href="https://link.segmentfault.com/?enc=WMAY7%2FjM5L2V7RlxgBrvxg%3D%3D.Bd6RJE1d1eygXoDvjAzuqCVrylvuwWEL6EMQFZvYnsVD81zYQnJpJgQf6%2BxyDERxnKmiZxF%2FHcWu4u5ZPhGU%2BKin3kMRMEsqwsa9X5wzdzDrrFd15VH%2F0PkdvyDIAl7QJSoRkJHixZh1g8S9PLr2V8LhfFlyzyNIlOfvMQ0KZC8%3D" rel="nofollow" target="_blank">Kernel panic</a> 进行介绍。</p><h4>2.2. Memory corruption</h4><p>Memory corruption 类问题一般表现也是 kpanic，但是最明显的标志是问题的随机性和不可解释性，出现这类情况，一般要考虑是内存使用上出现了 UAF（Use-After-Free），OOB（Out-of-Bounds）。</p><p>这类问题的难点在于，系统出现异常 crash 时，已经是前面时间发生踩踏的结果，所以需要定位到踩踏发生的位置，才能正向解决这类问题，一般在系统中存在下述两类踩踏问题：</p><p>a&gt; Linux 内核发生踩踏：</p><ul><li>对于 Acore 中运行的 Linux 系统，KASAN 是目前检查内存访问越界（Out-Of-Bound）和释放后访问（Use-After-Free）问题最有效的工具，KASAN 依赖编译器支持，当前 GCC-12.2 可以支持全功能的 KASAN，但是 KASAN 对性能和内存损耗非常严重，对于 slub 内存的使用问题，轻量级的 LUB\_DEBUG 往往也能提供帮助，但功能比较受限且提供的信息也比较有限。</li></ul><p>b&gt; SoC 子系统间的内存踩踏：</p><ul><li>在 征程 6X SoC 拥有 Acore、BPU、VDSP、Secure World（EL3）的多子系统 SoC， DDR 内存空间根据需求划分给不同子系统，如果子系统间内存出现踩踏，整机系统可能会出现各种随机异常；</li><li>征程 6X SoC 中使用 firewall 对 SoC 各子系统间的内存越界踩踏进行检测，当发生踩踏时由 EL3 触发 crash。</li></ul><h4>2.3. Watchdog</h4><ul><li>征程 6X SoC 中有 2 路 watchdog，目前使用 wdt0（监控 linux irq），wdt1（监控 linux 优先级为 50 的 rt kthread）。从/log/reset\_reason.txt 中可以看到 wdt 的 reason：</li></ul><blockquote>2025-06-13-13-35-25: mwdt               xxx@e9e8fadb9907 debug 20250610-193933   1100</blockquote><ul><li>wdt 导致重启后，系统会保存 pstore log，通过检查 pstore 获取异常信息；</li><li>wdt 狗咬中断/事件由 MCU 域处理，MCU 域进行重启。</li><li>wdt0：一般是 kernel 中某个 CPU 处于长时间无法响应中断的状态，征程 6X 平台上在 wdt1 狗咬时间（IRQ\_WDT\_TIMEOUT）到后会触发一个 gic 中断，在这个中断中会使用 NMI 将所有 cpu 的调用栈打印；</li><li>wdt1：一般是 kernel 中某个 CPU 处于长时间无法调度优先级为 50 的 rt kthread 的状态，征程 6X 平台上在 wdt2 狗咬时间（IRQ\_WDT\_TIMEOUT）到后会触发一个 gic 中断，在这个中断中会使用 NMI 将所有 cpu 的调用栈打印。</li></ul><h4><strong>2.4 firewall</strong></h4><p><strong>在 征程 6X SoC 拥有 Acore、BPU、GPU、VDSP0、Secure World（EL3）等多个子系统，DDR 内存空间根据需求划分给不同子系统。</strong></p><p>如果子系统间内存/寄存器空间出现踩踏，整机系统可能会出现各种随机异常。firewall 是 征程 6X SoC 上的硬件单元，功能就是根据配置捕获子系统间的内存越界踩踏，当发生踩踏时由 EL3 触发 crash。</p><p>在 MCU 域的 log 中会输出发生越界访问的地址信息和 master 信息，输出示例如下：ID 为 0xd0 的 master 尝试读地址为 0x80000000 的内存空间。</p><p>MCU 域主动触发 firewall 违例：</p><pre><code class="markdown">horizon:/$ regread 0x80000000</code></pre><p>MCU 域的违例信息 log：</p><pre><code class="markdown">[016.346991 0]firewall module 136 read violation master ID:d0
[016.347654 0]violation port 0[016.348004 0]violation addr high:0, low:80000000
[016.348656 0]Reg 0x80000000 value is 0x12345678

horizon:/$ [016.369156 0][M][time_1: 000016 s, 346 ms] Fchm Info occur (34, 30, 2552, 136)[016.370239 0][M][time_2: 000016 s, 348 ms] 136-6-CF Occur (34, 30, 2552), Payload(00-00-136-00 00-00-00-128 00-00-00-00 208-00-00-00)[016.371715 0]Customer handle(63, 1, 2)[016.421368 0][M][time_1: 000016 s, 348 ms] Fchm Info occur (36, 3, 2553, 349)[016.422437 0][M][time_2: 000016 s, 348 ms] 349-6-CF Occur (36, 3, 2553), Payload(00-00-00-00 00-00-00-00 00-00-00-00 00-00-00-00)[016.423894 0]Customer handle(51, 1, 2)</code></pre><p>征程 6X 部分内存的 firewall 权限设置：</p><p>Master ID 定义，详见：hbbin\_j6p/boot/j6p/bl31-dts/include/hobot\_firewall.h。</p>]]></description></item><item>    <title><![CDATA[B站即时通讯IM消息系统的新架构升级实践 JackJiang ]]></title>    <link>https://segmentfault.com/a/1190000047525462</link>    <guid>https://segmentfault.com/a/1190000047525462</guid>    <pubDate>2026-01-06 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文由B站技术团队比奇堡、Xd、三木森分享，有修订和重新排版。</p><h2>1、引言</h2><p>本文要分享的是B站IM消息系统的新架构升级实践总结，内容包括原架构的问题分析，新架构的整体设计以及具体的升级实现等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525464" alt="图片" title="图片"/><br/>B站技术团队的其它技术文章：B站千万级长连接实时消息系统的架构设计与实践B站实时视频直播技术实践和音视频知识入门B站基于微服务的API网关从0到1的演进之路</p><h2>2、消息系统业务解读</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525465" alt="图片" title="图片" loading="lazy"/><br/>按业务全域现状，在服务端角度分成客服系统、系统通知、互动通知和私信4个业务线，每个业务线内按现状标识了服务分层。私信内分为用户单聊、bToC的批量私信、群聊和应援团小助手四类，这四类细分私信没有技术解耦，单聊和批量私信比较接近系统天花板。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525466" alt="图片" title="图片" loading="lazy"/><br/>私信单聊发送到触达的pv转化和uv转化不足10%，有明显通过业务优化提升触达率的潜力。</p><h2>3、消息系统中的私信业务</h2><p>私信域内的几个概念解释：1）会话列表：按聊天人排序的列表。即B站首页右上角信封一跳后看到的历史聊天人列表，以及点击未关注人等折叠会话看到的同属一类的聊天人列表。传达对方账号、最新私信和未读数的信息。点击一个会话后看到的是对聊历史，也称会话历史。2）会话详情：描述和一个聊天人会话状态的原子概念，包括接收人uid、发送人uid、未读数、会话状态、会话排序位置等。3）会话历史：按时间线对发送内容排序的列表。一份单聊会话历史既属于自己，也属于另一个和自己的聊天的人。群聊的会话历史属于该群，不属于某个成员。会话历史是收件箱和消息内容合并后的结果。4）收件箱：将一次发送的时序位置映射到发送内容唯一id的kv存储，可以让服务端按时间序读取一批发送内容唯一id。5）私信内容：一个包括发送内容唯一id、原始输入内容、消息状态的原子概念。批量私信把同一个发送内容唯一id写入每个收信人的收件箱里。6）timeline模型：时间轴的抽象模型，模型包括消息体、已读位点、最大位点、生产者、消费者等基本模块，可以用于基于时间轴的数据同步、存储和索引。私信涉及timeline模型的包括会话列表和会话历史。7）读扩散：pull模式。群聊每条私信只往群收件箱写一次，让成百上千的群成员在自己的设备都看到，是典型的读扩散。8）写扩散：push模式。单聊每条私信既更新接收人会话也更新发送人会话，是轻微的写扩散，无系统压力。群聊有另一个不一样的特点，就是当群成员发送消息后，需要通过长链接通知其他群成员的在线设备，以及发送人其他的在线设备，这是一个写扩散的技术模型，但是这个写扩散是通知后即时销毁的，并且具有过期时间，所以仅临时占用资源，并不对存储造成压力，且能有较好的并发量。私信核心概念关系表达：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525467" alt="图片" title="图片" loading="lazy"/></p><h2>4、消息系统问题1：会话慢</h2><p>查询当会话缓存过期时，Mysql是唯一回源，Mysql能承载的瞬时QPS受当时应用总连接数和sql平均响应速度的影响，连接数打满时会给前端返回空会话列表。虽然可以增加POD数量、增大akso proxy连接数、优化sql和索引来作为短线方案，来提升瞬时请求Mysql容量，但是这种短线方案无法加快单次响应速度，mysql响应越来越慢的的问题依然在。另外增加POD数量也会降低发版速度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525468" alt="图片" title="图片" loading="lazy"/><br/>会话Mysql使用用户uid%1000/100分库，用户uid%100分表，table总量是1000。单表会话量在1kw-3.2kw。单个大up的会话积累了10W条以上，会话量最大的用户有0.2亿条会话。单个Up的会话会落到一张表中，每张表都有比较严重的数据倾斜。如果考虑增加分库分表的方案，sql查找条件依然需要用户uid，所以相当于倾斜数据要转移到新的单表，问题没有解决。另外，重新分库分表过程中新旧table增量同步和迁移业务读写流量的复杂度也很大，有比较大的业务风险。Mysql的规格是48C 128G和32C 64G。由于会话数据量大，Mysql buffer_pool有限，数据比较容易从内存淘汰，然后mysql需要进行磁盘扫描并将需要的数据加载到内存进行运算，加之比较多的磁盘扫描数据，这时的响应一般在秒级别，接口会给前端返回超时错误，会话列表页空白。为了适配业务发展，Mysql 会话表 已经添加了9个非聚集索引，如果通过增加索引使用业务需要，需要更大的Mysql资源，且解决不了冷数据慢查询的问题。增加更多索引也会让Mysql写入更慢。</p><h2>5、消息系统问题2：私信内容单表空间和写性能接近天花板</h2><p>每条私信内容都绑定私信自己的发号器生成的msgkey，即私信内容唯一id，该msgkey包含私信发送时的时间戳（消息ID生成可参阅读《微信的海量IM聊天消息序列号生成实践》）。读写私信内容Mysql之前先从msgkey解析出时间，用这个时间路由分库分表。私信内容库按季度分库，分库内按月度分表，单表数据量数亿，数据量最大的用户日增私信351.9W条。按照曲率预测，25年全年数据量有近百亿，如果继续按照月度分表，分表规则不适应增长。当前该Mysql最大写qps 790，特别活动时写qps峰值预计是20k，但是为了保障Mysql服务整体的可靠，单库写流量我们需要控制在3000qps以下，无法满足写入量峰值时的需要。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525469" alt="图片" title="图片" loading="lazy"/><br/>此外，消息内容表结构包含了群聊、单聊和应援团小助手全部的属性，增加业务使用难度。绝大部分私信内容是单聊的。</p><h2>6、消息系统问题3：服务端代码耦合</h2><p>B站的四类私信包括：1）单聊；2）群聊；3）B端批量私信；4）应援团小助手。这些私信都需要实现发送和触达两条核心链路，四种私信核心链路的代码逻辑和存储耦合在一起，代码复杂度随着业务功能上线而不断增加，熵增需要得到控制。从微服务这方面来说，实例和存储耦合会带来资源随机竞争，当一方流量上涨，可能给对方的业务性能带来不必要的影响，也会带来不必要的变更传导。</p><h2>7、消息系统新架构的升级路径</h2><p>基于对私信现状的论述，可以确定我们要优化的是一个数据密集型 &gt;&gt; 计算密集型，读多写少（首页未读数）、读少写多（会话）场景兼具的系统。同时需要拥有热门C端产品的稳定性、扩展性和好的业务域解耦。针对读多写少和读少写多制定了针对的技术方案。具体的实施情况请继续往下阅读。</p><h2>8、新架构的整体设计</h2><p>结合B站业务现状，我觉得比较合理的架构：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525470" alt="图片" title="图片" loading="lazy"/><br/>一个兼顾复杂列表查询架构和IM架构的消息域框架，整体分四层：1）接入层：即toC的BFF和服务端网关；2）业务层：按复杂查询设计系统，用于各种业务形态的支撑；3）平台层：按IM架构设计系统，目标是实时、有序的触达用户，平台层可扩展；4）触达层：对接长链和push。</p><h2>9、新架构具体升级1：端上本地缓存降级</h2><p>端上应该支持部分数据缓存，以确保极端情况下用户端可展示，可以是仅核心场景，比如支付小助手、官号通知，用户在任何情况下打开消息页都不应该白屏。</p><h2>10、新架构具体升级2：BFF架构升级</h2><p>BFF网关吸收上浮的业务逻辑，控制需求向核心领域传导。服务端基于业务领域的能力边界，抽象出单聊、群聊、系统通知、互动通知和消息设置共五个新服务，提升微服务健康度。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525471" alt="图片" title="图片" loading="lazy"/><br/>新服务剥离了历史包袱，也解决一些在老服务难解的功能case，优化了用户体验，比如消息页不同类型消息的功能一致性；重新设计会话缓存结构和更新机制，优化Mysql索引，优化Mysql查询语句，减少了一个量级的慢查询。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525472" alt="图片" title="图片" loading="lazy"/></p><h2>11、新架构具体升级3：服务端可用性升级</h2><p>11.1 概述服务端按四层拆分后，集中精力优化业务层和平台层。业务层：按复杂查询设计系统，用于各种业务形态的支撑1）冷热分离：多级缓存 redis(核心数据有过期)+taishan(有限明细数据)+mysql(全部数据)；2）读写分离：95%以上复杂查询可以迁移到从库读。平台层：按IM架构设计系统，目标是实时、有序的触达用户，平台层可扩展1）Timeline模型：依赖雪花发号器，成熟方案；2）读写扩散：单聊-写扩散，群聊-读扩散。11.2 单聊会话1）缓存主动预热：用户在首页获取未读数是一个业务域内可以捕捉的事件，通过异步消费这个事件通知服务端创建会话缓存，提高用户查看会话的缓存命中率。鉴于大部分人打开B站并不会进私信，此处可以仅大UP预热。大UP的uid集合可以在数平离线分析会话数据后写入泰山表，这个泰山表更新时效是T+1。监控UP会话数量实时热点，触发突增阈值时，通过异步链路自动为热点用户主动预热会话列表缓存。对预热成功率添加监控，并在数平离线任务失败或者预热失败时做出业务告警，及时排查原因，避免功能失效。2）泰山和Mysql双持久化：增加泰山存储用户有限会话明细，作为redis未命中后的第一回源选择，Mysql作为泰山之后的次选。基于用户翻页长度分析后确定泰山存储的有限会话的量级。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525473" alt="图片" title="图片" loading="lazy"/><br/>redis 存储24小时数据，taishan 存储 600条/用户（20页），预设到的极端情况才会回源mysql从库。对于ZSET和KV两种数据结构，评估了各自读写性能的可靠性，符合业务预期。业务如果新增会话类型，可以跟本次新增泰山有限明细一样，基于会话类型的具体规则新增泰山Key。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525474" alt="图片" title="图片" loading="lazy"/><br/>3）泰山长尾优化：查询redis未命中时会优先回源泰山，考虑到泰山99分位线在50ms以下，而且Mysql多从实例都能承受来自C端的读请求，所以采用比泰山报错后降级Mysql稍微激进的对冲回源策略。在泰山出现“长尾”请求时，取得比较好的耗时优化效果。可以使用大仓提供的error group结合quit channel实现该回源策略，同时能避免协程泄漏。整个处理过程在业务响应和资源开销中维持中间的平衡，等待泰山的时间可以灵活调整。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525475" alt="图片" title="图片" loading="lazy"/><br/>泰山最初没有数据，可以在泰山未命中时进行被动加载，保证用户回访时能命中。4）一致性保证：虽然我们重构了新服务，但是老服务也需要保留，用来处理未接入BFF的移动端老版本和web端请求，这些前端在更新会话时（比如ACK）请求到了老服务，新服务需要通过订阅会话Mysql binlog异步更新本服务的redis和泰山。为了避免分区倾斜，订阅binlog的dts任务使用id分区，这样方便的是一条会话在topic的分区是固定的。为了避免两次请求分别命中泰山和Mysql时给用户返回的数据不一样，需要解决三大问题：a. 当出现分区rebalance需要避免重复消费；b. 当Mysql一条会话记录在短时间内（秒级）多次更新，要保证binlog处理器不会逆时间序消费同一个会话的binlog，即跳过较早版本的binlog；c. 保证泰山写入正确并且从Mysql低延迟同步。这三个问题都要保证最终一致性，具体解决方案是用redis lua脚本实现compare and swap，lua脚本具有原生的原子性优势。dts每同步一条binlog都会携带毫秒级mtime，当binlog被采用时，mtime被记入redis10分钟，如果下一条binlog的mtime大于redis记录的mtime，这条binlog被采用，否则被丢弃。这个过程可以考虑使用gtid代替mtime，但这个存在的问题是每个从实例单独维护自己的gtid，当特殊情况发生mysql主从切换，或者dts订阅的从节点发生变更，gtid在CAS计算中变得不再可靠，所以我们选择了使用mtime作为Mysql会话记录的版本。通过消费路线高性能设计保证泰山异步更新的延迟在1秒以内，并在特殊情况延迟突破1s时有效告警。高性能消费路线中，每个库的binlog分片到50个partition，业务提供不低于50个消费pod，单pod配置100并发数，按照写泰山999分位线20ms计算，每秒可以消费 50<em>100</em>(1000/20)=250000 条，大约线上峰值8.3倍，考虑dts本身的max延迟在600~700毫秒，同步泰山和redis的延迟会在700毫秒至1秒以内，符合业务预期。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525476" alt="图片" title="图片" loading="lazy"/><br/>11.3 收件箱BFF已经从业务层和平台层将单聊读收件箱独立出来，本次升级主要是从存储做增量解耦 ，存量单聊收件箱的读流量可以访问旧表。 单聊新收件箱存储采用redis+泰山的模式，redis提供热数据，泰山提供全部数据并采用RANDOM读模式，让主副本都能分担读流量。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525477" alt="图片" title="图片" loading="lazy"/><br/> 11.4 私信内容本次升级主要如下：1）单聊增量数据独立存储，按照单聊业务设计表结构，和群聊、应援团小助手彻底解耦。2）写Mysql升级为异步化操作，提高写性能天花板，这种异步写Mysql改造不会影响读消息内容的可用性和设计。3）单聊分库规则升级为月度分库，单库内分表为100张。 群聊、应援团小助手和历史单聊依然使用旧的分库分表规则读写Mysql。业务需要对增量单聊私信路由分库分表时，先从msgkey先解析出时间戳，找到用时间戳对应的月份分库，然后用msgkey对100取余找到分表。这种方案能达到按时间纬度的冷热数据的分离，同时由于msgkey取余的结果具有随机性，平衡了每张表的读写流量。这样预计2025年单表数据量能从9亿下降到900万。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525478" alt="图片" title="图片" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525479" alt="图片" title="图片" loading="lazy"/><br/>11.5 批量私信日常通道：日常批量私信任务共用通道，共用配额。高优通道：主要通过将链路上topic partition扩容、消费POD扩容、POD内消费通道数扩容、缓存扩容、akso proxy连接数扩容，把平均发送速度从3500 人/秒提高到30000人/秒。这个通道可以特殊时期开给特殊业务使用。</p><h2>12、本文小结</h2><p>我们逐步发现技术升级不是一蹴而就的，它是一个逐步优化的过程。设计技术方案前设立合适和有一些挑战的目标，但这个目标要控制成本，做好可行性。设计技术方案的时候，需要清楚现有架构与理想架构的差距和具体差异点，做多个方案选型，并确定一个，这个更多从技术团队考虑。其次要保证功能在新老架构平稳过渡，保证业务的稳定性。后面持续关注新老架构的技术数据，持续优化，老架构要持续关注它的收敛替换。IM系统是一个老生常谈的话题，也是融合众多有趣技术难点的地方，欢迎感兴趣的同行交流研讨。</p><h2>13、参考资料</h2><p>[1] 浅谈IM系统的架构设计[2] 简述移动端IM开发的那些坑：架构设计、通信协议和客户端[3] 一套海量在线用户的移动端IM架构设计实践分享(含详细图文)[4] 一套原创分布式即时通讯(IM)系统理论架构方案[5] 从零到卓越：京东客服即时通讯系统的技术架构演进历程[6] 蘑菇街即时通讯/IM服务器开发之架构选择[7] 微信技术总监谈架构：微信之道——大道至简(演讲全文)[8] 现代IM系统中聊天消息的同步和存储方案探讨[9] 子弹短信光鲜的背后：网易云信首席架构师分享亿级IM平台的技术实践[10] 一套高可用、易伸缩、高并发的IM群聊、单聊架构方案设计实践[11] 从游击队到正规军(一)：马蜂窝旅游网的IM系统架构演进之路[12] 瓜子IM智能客服系统的数据架构设计（整理自现场演讲，有配套PPT）[13] 阿里钉钉技术分享：企业级IM王者——钉钉在后端架构上的过人之处[14] 阿里技术分享：电商IM消息平台，在群聊、直播场景下的技术实践[15] 一套亿级用户的IM架构技术干货(上篇)：整体架构、服务拆分等[16] 从新手到专家：如何设计一套亿级消息量的分布式IM系统[17] 企业微信的IM架构设计揭秘：消息模型、万人群、已读回执、消息撤回等[18] 融云技术分享：全面揭秘亿级IM消息的可靠投递机制[19] 阿里IM技术分享(三)：闲鱼亿级IM消息系统的架构演进之路[20] 基于实践：一套百万消息量小规模IM系统技术要点总结[21] 跟着源码学IM(十)：基于Netty，搭建高性能IM集群（含技术思路+源码）[22] 一套十万级TPS的IM综合消息系统的架构实践与思考[23] 得物从0到1自研客服IM系统的技术实践之路[24] 一套分布式IM即时通讯系统的技术选型和架构设计[25] 微信团队分享：来看看微信十年前的IM消息收发架构，你做到了吗[26] 转转平台IM系统架构设计与实践(一)：整体架构设计[27] 支持百万人超大群聊的Web端IM架构设计与实践[28] 转转客服IM聊天系统背后的技术挑战和实践分享</p><p><strong>即时通讯技术学习：</strong></p><ul><li>移动端IM开发入门文章：《新手入门一篇就够：从零开发移动端IM》</li><li>开源IM框架源码：<a href="https://link.segmentfault.com/?enc=ScdcVz39O%2FPkp0EjAwxI6w%3D%3D.qT%2BIM5jJk2%2FZ4iLlwlwuA2Aet42UAZ4cdIJphZMC%2BYt9d6gEn6dHIu%2BO9YCi%2BF03" rel="nofollow" target="_blank">https://github.com/JackJiang2011/MobileIMSDK</a>（备用地址点此）</li></ul><p>（本文已同步发布于：<a href="https://link.segmentfault.com/?enc=k82NF9Q2%2FcGv0tQ9tv7aCQ%3D%3D.aSoAsnbk036t1FVuzYZT7dOW%2FFkw5l5IxiZipI6O9py6fTw1MkyFM9LkWIpTiDC5" rel="nofollow" target="_blank">http://www.52im.net/thread-4886-1-1.html</a>）</p>]]></description></item><item>    <title><![CDATA[nameko 无法适配新版的python3.14，eventlet 停止维护导致的失效 rabbit]]></title>    <link>https://segmentfault.com/a/1190000047525496</link>    <guid>https://segmentfault.com/a/1190000047525496</guid>    <pubDate>2026-01-06 23:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>nameko 无法适配新版的python3.14，eventlet 停止维护导致的 1 RLock(s) were not greened 报错</p><hr/><p>直接把 nameko2.14.1 用到 python3.14 上会报错 <code>ModuleNotFoundError: No module named 'pkg_resources'</code></p><p>修复之后还是会有警告</p><pre><code class="shell">╰─➤  nameko run --config config.yaml run_services
1 RLock(s) were not greened, to fix this error make sure you run eventlet.monkey_patch() before importing any other modules.
starting services: greeting_service
Connected to amqp://ponponon:**@192.168.31.245:5672//</code></pre><p>这个 <code>1 RLock(s) were not greened</code> 不是因为 nameko 的问题，而是来自 eventlet 本身的问题：<a href="https://link.segmentfault.com/?enc=YWttCbT%2BXH6ikZQpzs%2BQwg%3D%3D.kTpPXaqHCZIflTXH8pJkss0ZJ2a6EYW0NoPnkBq3xD9dDJPsnSn7hg1nkILwRqE5" rel="nofollow" target="_blank">https://github.com/eventlet/eventlet</a></p><p>我通过降低 cpython 版本做测试，发现 cpython3.13 也有这个问题；直到降级到 cpython3.12 则没有这个问题了</p>]]></description></item><item>    <title><![CDATA[Agentic RAG：用LangGraph打造会自动修正检索错误的 RAG 系统 本文系转载，阅读]]></title>    <link>https://segmentfault.com/a/1190000047525372</link>    <guid>https://segmentfault.com/a/1190000047525372</guid>    <pubDate>2026-01-06 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>标准 RAG 流水线有个根本性的毛病：检索到的文档一旦与用户意图对不上号，模型照样能面不改色地输出一堆看似合理的胡话，既没有反馈机制也谈不上什么纠错能力。</p><p>而Agentic RAG 的思路截然不同，它不急着从检索结果里硬挤答案，而是先判断一下拿回来的东西到底有没有用，如果没用则会重写查询再来一轮。这套机制实际上构建了一条具备自我修复能力的检索链路，面对边界情况也不至于直接崩掉。</p><p>本文要做的就是用 LangGraph 做流程编排、Redis 做向量存储，搭一个生产可用的 Agentic RAG 系统。涉及整体架构设计、决策逻辑实现，以及状态机的具体接线方式。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525374" alt="" title=""/></p><h2>传统 RAG 的"一锤子买卖"</h2><p>假设知识库里有一篇《大语言模型的参数高效训练方法》，用户问的是"怎么微调 LLM 效果最好"。</p><p>语义相似度确实存在但不够强。检索器拉回来的可能是模型架构相关的内容虽然沾边但答非所问，LLM 本身没法意识到上下文是错的，照样能生成一段貌似专业实则离题万里的回答。</p><p>传统 RAG 对这种失败模式完全没有办法。查询文档、生成答案，整个过程是单向的没有任何质量把关环节。</p><p>Agentic RAG 的解法是在流程中插入检查点：智能体先判断要不要检索；检索完了有评分环节确认相关性；不相关就重写查询再试；如此循环直到拿到合格的上下文，或者把重试次数耗尽为止。</p><h2>系统架构拆解</h2><p>整个系统拆成六个模块：</p><p>配置层负责环境变量和 API 客户端的初始化工作。Redis 连接串、OpenAI 密钥、模型名称全部归拢到这里统一管理。</p><p>检索器模块承担文档摄取的全套流程，文档经过</p><pre><code>WebBaseLoader</code></pre><p>加载后用</p><pre><code>RecursiveCharacterTextSplitter</code></pre><p>切块，再通过 OpenAI Embedding 向量化，最后存进</p><pre><code>RedisVectorStore</code></pre><p>。检索器本身会被包装成 LangChain 工具供智能体调用。</p><p>智能体节点是决策入口。拿到用户问题后先做判断：这个问题需要查资料还是直接能答？需要查就调检索器，不需要就直出答案。</p><p>评分（Grade Edge）决定检索结果的去向。相关性够就往生成环节走；不够就触发重写。这是整个系统里最关键的质量关卡。</p><p>重写节点把原始问题改写成更适合检索的形式，用户表述太口语化、缺少关键词，这些问题都在这里修正。</p><p>生成节点只有在评分环节确认上下文合格后才会执行，基于检索到的文档产出最终答案。</p><h2>流程图和代码</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525375" alt="" title="" loading="lazy"/></p><p>关键在于从"重写"回到"智能体"这条反馈路径。系统不会因为一次检索失败就直接给出一个牵强附会的答案，它会调整策略重新尝试。</p><pre><code> src/  
├── config/  
│   ├── settings.py      # 环境变量  
│   └── openai.py        # 模型名称和 API 客户端  
├── retriever.py         # 文档摄取和 Redis 向量存储  
├── agents/  
│   ├── nodes.py         # 智能体、重写和生成函数  
│   ├── edges.py         # 文档评分逻辑  
│   └── graph.py         # LangGraph 状态机  
 └── main.py              # 入口点</code></pre><p>职责划分很清晰：配置归</p><pre><code>config/</code></pre><p>，智能体相关的都在</p><pre><code>agents/</code></pre><p>，向量存储操作全在</p><pre><code>retriever.py</code></pre><p>。这种结构调试起来方便，单测也好写。</p><h2>配置模块设计</h2><p>配置层解决两个问题：环境变量加载和 API 客户端复用。</p><pre><code>settings.py</code></pre><p>集中读取 Redis 连接信息、OpenAI API Key、索引名称，不用满项目找配置。</p><pre><code>openai.py</code></pre><p>负责实例化 Embedding 模型和 LLM 客户端。切换到别的模型、调整 Embedding 维度等等配置也只要一处</p><p>这个设计在生产环境里很实用，因为模型会迭代、Key 会轮换、服务商可能换掉，集中管理意味着改动成本可控。</p><h2>检索器实现</h2><p>检索器负责整条数据摄取链路：抓文档、切块、向量化、入库。</p><p>语料选的是 Lilian Weng 关于 Agent 和 Prompt Engineering 的博客文章。</p><pre><code>WebBaseLoader</code></pre><p>负责抓取，</p><pre><code>RecursiveCharacterTextSplitter</code></pre><p>切分成适当大小的块，OpenAI Embedding 完成向量化。</p><p>向量存储用</p><pre><code>RedisVectorStore</code></pre><p>。检索器通过</p><pre><code>create_retriever_tool</code></pre><p>封装成 LangChain 工具形态。这一步的意义在于让智能体能够"调用"检索而不是被动触发，意味着它有权决定什么时候需要查资料、什么时候直接回答。</p><p>为什么用Redis？因为够快，够简单。向量相似度搜索本身 Redis 就能做，不用额外引入专门的向量数据库。对于已经跑着 Redis 的技术栈来说，加 RAG 能力几乎零额外运维负担。</p><h2>智能体节点</h2><pre><code>nodes.py</code></pre><p>里有三个核心函数。</p><p>智能体函数接收当前状态（用户问题、历史对话等），判断下一步怎么走。它能调用包括检索器在内的工具集。问题需要外部知识就调检索，不需要就直接生成回答。</p><p>重写函数处理那些被评分环节打回来的查询。它会让 LLM 把原始问题改写成检索友好的形式，用词更精准、关键信息更突出。改写后的查询再交回智能体重新发起检索。</p><p>生成函数产出最终答案。输入是原始问题加上已确认相关的文档，输出是基于这些上下文的回答。</p><p>三个函数都是无状态的。状态走图，不走函数内部变量。这对测试和排查问题都有好处。</p><h2>文档评分逻辑</h2><pre><code>edges.py</code></pre><p>里的</p><pre><code>grade_documents</code></pre><p>是整个 Agentic 机制的核心。</p><p>检索完成后它会逐个审视返回的文档：这东西跟用户问的相关不相关？能不能帮上忙？</p><p>评分本身是通过一次 LLM 调用完成的，Prompt 设计成要求模型返回二元判断——相关或者不相关。</p><p>判定相关就返回</p><pre><code>"generate"</code></pre><p>，流程走向答案生成；判定不相关则返回</p><pre><code>"rewrite"</code></pre><p>，触发查询改写。</p><p>这个环节的价值在于拦截那些本会导致标准 RAG 胡说八道的情况，与其硬着头皮从不靠谱的上下文里编答案，不如给系统一次修正查询的机会。</p><h2>状态机接线</h2><pre><code>graph.py</code></pre><p>用 LangGraph 的状态机原语把所有节点串起来。</p><p>图结构定义了节点（智能体、检索、生成、重写）和边（节点间的连接关系，包括基于评分结果的条件路由）。</p><p>接线逻辑如下：查询先到智能体节点，智能体决定调检索器的话流程就到检索节点，检索完进评分，评分过了走生成，没过走重写，重写完的查询再回智能体重新来过。生成节点执行完流程结束。</p><p>LangGraph 接管状态流转的细节。每个节点只管接收当前状态、返回状态更新，具体消息怎么路由由图引擎根据边的条件逻辑处理。</p><h2>运行时流程</h2><pre><code>main.py</code></pre><p>是入口，做三件事：构建图、接收问题、流式输出结果。</p><pre><code>build_graph()</code></pre><p>在启动时执行一次，完成 LangGraph 状态机的构建和检索器工具的初始化.</p><p>问题进来之后的流转过程：智能体接收问题决定调检索 → Redis 返回文档 → 评分环节判断相关性 → 相关就生成答案，不相关就重写查询继续循环。</p><p>脚本会把各节点的输出实时打到控制台，方便观察决策过程——什么时候触发了检索、评分结果如何、有没有走到重写环节，一目了然。</p><h2>架构的优势</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525376" alt="" title="" loading="lazy"/></p><p><strong>自校正能力</strong>：检索质量差能发现并修复，不会闷头输出一个基于垃圾上下文的错误答案然后假装没事发生。</p><p><strong>决策透明</strong>：状态机让每个分支点都是显式的。路由决策可以全量记录，想排查为什么系统选择了重写而不是直接生成，日志里全有。</p><p><strong>模块解耦</strong>：每个组件职责单一。想把 Redis 换成 Pinecone？改检索模块。想把 OpenAI 换成 Anthropic？改配置层。其他部分不受影响。</p><h2>总结</h2><p>标准 RAG 把检索当黑盒，查询丢进去、文档出来，至于相不相关全凭运气。Agentic RAG 打开这个黑盒在关键位置加了质量控制。</p><p>LangGraph 加 Redis 的组合提供了一个可以直接上生产的骨架。流程编排的复杂度 LangGraph 消化掉了，向量检索的性能 Redis 兜住了，剩下的评分和重写逻辑负责兜底那些简单系统搞不定的边角案例。</p><p>代码：</p><p><a href="https://link.segmentfault.com/?enc=rtFWx%2B6oN7e8eoOn49TJVg%3D%3D.SC4CAqwoFwohttxer1hArTT0oDfUeQKrtmIThSU3hI2jXjeVOcrAwVmOHczwWJYBHlm7jZ%2BM9mmJCY%2BK0hdV8g%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/a45e19af576a4826a605807d8fcfe298</a></p><p>作者：Kushal Banda</p>]]></description></item><item>    <title><![CDATA[千峰嵌入式2023-完整版 技术站999it点top ]]></title>    <link>https://segmentfault.com/a/1190000047525246</link>    <guid>https://segmentfault.com/a/1190000047525246</guid>    <pubDate>2026-01-06 21:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>《2023 千峰嵌入式开发实战指南：MCU 编程・外设驱动・工业级项目开发精讲》<br/>——从教育公平、科技自立、人文关怀与产业升级多维视角看嵌入式人才的培养价值</p><p>在“万物智联”成为现实的今天，嵌入式系统早已悄然渗透进我们生活的每个角落：从智能家电、车载电子，到工业机器人、医疗设备，再到国家电网、航空航天等关键基础设施，其背后都离不开微控制器（MCU）与底层驱动的精密协同。《2023 千峰嵌入式开发实战指南》以“MCU 编程—外设驱动—工业级项目”为脉络，不仅传授技术细节，更折射出一场关乎国家科技根基、教育转型与产业未来的深层变革。</p><p>教育维度：打破“重应用、轻底层”的失衡格局，重塑工程教育根基<br/>长期以来，高校计算机教育过度聚焦于 Web 开发、移动应用等上层软件，导致大量毕业生对硬件交互、内存管理、中断处理等底层机制缺乏基本认知。这种“空中楼阁”式的培养模式，难以支撑高端制造、芯片设计、工业自动化等国家战略领域的人才需求。</p><p>《千峰嵌入式开发实战指南》以系统化、阶梯式的内容设计，引导学习者从寄存器操作、时钟配置、GPIO 控制等基础入手，逐步掌握 UART、I2C、SPI、ADC 等外设驱动开发，并最终完成如智能温控系统、工业数据采集终端等贴近真实场景的项目。这种“从硅片到系统”的全链路训练，重建了软硬结合的工程思维，为高校教育提供了可借鉴的实践范本，也为自学群体打开了通往硬科技领域的大门。</p><p>科技维度：夯实国产芯片生态，助力关键技术自主可控<br/>当前，全球半导体产业链竞争白热化，MCU 作为“芯片中的芯片”，广泛应用于消费电子与工业控制领域。然而，国内大量嵌入式开发仍依赖国外芯片平台（如 STM32）及配套工具链，存在供应链风险与技术黑盒问题。</p><p>本指南虽以通用原理为主，但其强调的“理解芯片手册、掌握驱动抽象、适配不同硬件平台”的能力，正是构建国产芯片生态适配力的关键。当开发者具备扎实的底层开发功底，便能快速迁移至国产 MCU（如兆易创新、华大半导体、乐鑫等）平台，参与国产芯片的验证、优化与生态建设。从这个角度看，一本嵌入式教材，实则是培育国产半导体“土壤”的重要一环。</p><p>人文发展维度：技术应服务于人的安全、尊严与可持续生活<br/>嵌入式系统不同于普通软件，其失效可能直接导致物理世界的安全事故——如医疗设备误判、工业机械失控、汽车刹车失灵。因此，嵌入式开发天然带有高度的责任伦理。</p><p>《实战指南》在工业级项目讲解中，反复强调实时性保障、异常处理机制、电源管理策略与电磁兼容设计等工程规范，传递出一种严谨、敬畏、以人为本的技术价值观。它提醒开发者：你写的每一行初始化代码，都可能关系到一个工人的安全、一位患者的健康，或一个家庭的用电稳定。这种将技术精度与人文关怀相融合的教育理念，正是培养“负责任工程师”的核心所在。</p><p>经济维度：赋能制造业升级，催生“新蓝领”技术岗位<br/>中国正从“制造大国”迈向“智造强国”，而智能制造的核心在于“感知—决策—执行”的闭环，这正是嵌入式系统的主战场。无论是工厂的 PLC 控制器、物流 AGV 小车，还是农业物联网传感器、新能源充电桩，都急需大量懂硬件、会编程、能调试的复合型嵌入式人才。</p><p>本指南通过工业级项目实战，帮助学习者掌握企业真正需要的技能：如何读芯片 datasheet？如何用示波器调试通信协议？如何在资源受限环境下优化代码？这些能力使学习者能快速胜任嵌入式软件工程师、FAE（现场应用工程师）、测试验证工程师等高价值岗位。更重要的是，它为传统制造业工人、职校学生提供了向“数字新蓝领”转型的可行路径，推动劳动力结构向高技能、高附加值方向演进。</p>]]></description></item><item>    <title><![CDATA[AI 的“性格旋钮”——什么是大模型的温度？ blossom ]]></title>    <link>https://segmentfault.com/a/1190000047525315</link>    <guid>https://segmentfault.com/a/1190000047525315</guid>    <pubDate>2026-01-06 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>你有没有发现：有时候 AI 像个严谨的老教授，回答滴水不漏；有时候它又像个天马行空的艺术家，能编出一堆意想不到的情节？</p><p>这背后往往藏着一个关键参数：<strong>温度（Temperature）</strong>。</p><p>别担心，调高温度并不会让电脑“发烫”，也不是让 AI 发烧。这里的温度，更像一个控制 AI <strong>“有多敢冒险”</strong>的性格旋钮：</p><ul><li>温度低 → 更稳、更像标准答案</li><li>温度高 → 更发散、更有创意，但也更容易跑偏</li></ul><hr/><h2>一、为什么需要温度？（AI 的“填空游戏”）</h2><p>要理解温度，先看大模型是怎么说话的。</p><p>大模型生成文本的过程，近似于一种“逐字填空”的游戏：每输出一个词（token），它都会对“下一步可能出现的候选词”打分。</p><p>比如当 AI 写到：</p><blockquote>“今天天气真——”</blockquote><p>它脑内可能有这样一张“候选词打分表”（通常称为 <strong>logits</strong>）：</p><ul><li><strong>好</strong>：90 分（最稳妥）</li><li><strong>热</strong>：50 分（也合理）</li><li><strong>怪</strong>：5 分（少见但勉强能通）</li><li><strong>紫色</strong>：0 分（基本不通顺）</li></ul><p>如果 AI 每次都只选分数最高的那个词（比如永远选“好”），输出会非常稳定，但也容易变得<strong>模板化</strong>：句子没错，却缺少惊喜，像“复读机”。</p><p>于是，我们需要一种机制：在“稳妥”之外，给 AI 一点点“跳出常规”的空间——这就是温度登场的原因。</p><hr/><h2>二、温度到底做了什么？（神奇的蛋糕分法）</h2><p>在真正选词之前，模型会先把“分数”转换成“概率”，常见做法叫 <strong>Softmax</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525317" alt="" title=""/><br/>你可以把它想象成：</p><blockquote><strong>把一块蛋糕分给候选词：分数越高，分到的蛋糕越多，被选中的概率越大。</strong></blockquote><p>而<strong>温度</strong>，就像影响“怎么切蛋糕”的那把刀——它决定蛋糕分配得<strong>更偏心</strong>还是<strong>更平均</strong>。</p><h3>1）低温（T &lt; 1）：偏心切法（更保守）</h3><p>温度调低后，分配会变得更“极端”：<br/>第一名会拿走绝大多数蛋糕，其他词只剩零头。</p><ul><li><strong>结果</strong>：AI 更倾向选“最常见、最稳”的词</li><li><strong>体验</strong>：更严谨、更稳定，但也更容易“千篇一律”</li></ul><h3>2）高温（T &gt; 1）：均匀切法（更发散）</h3><p>温度调高后，蛋糕切得更平均：<br/>第一名仍然是大头，但第二、第三名也能分到明显份额。</p><ul><li><strong>结果</strong>：AI 更可能选到不那么“标准”的词</li><li><strong>体验</strong>：更有创意、更有变化，但也更容易跑题或胡编</li></ul><hr/><h2>三、温度怎么设置？（三个常见场景）</h2><p>可以把不同温度下的 AI，想象成三种不同“人格”。</p><h3>1）冰块模式（低温：0 ~ 0.3）</h3><ul><li><strong>像谁</strong>：严肃的科学家 / 数学老师</li><li><strong>适合</strong>：做数学题、写代码、严谨问答、总结归纳</li><li><strong>原因</strong>：这类任务追求确定性，“1+1=2”不需要创意</li></ul><h3>2）常温模式（中温：0.5 ~ 0.9）</h3><ul><li><strong>像谁</strong>：正常可靠的聊天伙伴</li><li><strong>适合</strong>：日常对话、写邮件、写周报、写解释说明</li><li><strong>原因</strong>：稳定之余也有一点自然变化，是最常用的平衡区间</li></ul><h3>3）火焰模式（高温：0.9 ~ 1.5 或更高）</h3><ul><li><strong>像谁</strong>：灵感爆棚的艺术家 / 头脑风暴搭子</li><li><strong>适合</strong>：写故事、写诗、创意发想、广告文案、脑洞类任务</li><li><strong>提醒</strong>：温度太高（例如 &gt;1.5）时，输出可能开始发散到不受控，甚至出现“看起来很像话但其实不太对”的内容</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525318" alt="" title="" loading="lazy"/></p><hr/><h2>四、补充：温度 vs Top-k / Top-p（它们到底有什么区别？）</h2><p>温度之外，你可能还见过两个常用的“采样参数”：<strong>Top-k</strong> 和 <strong>Top-p</strong>。它们和温度一样，都是在控制 AI 输出的随机性，但“动手的方式”不同。</p><p>你可以把它们理解成：<strong>温度在“调形状”，Top-k/Top-p 在“划范围”。</strong></p><h3>1）温度（Temperature）：调“整体概率分布”的陡峭程度</h3><ul><li><strong>温度低</strong>：概率分布更“尖”，第一名更容易被选中（更稳）</li><li><strong>温度高</strong>：概率分布更“平”，冷门词也更容易被抽到（更发散）</li></ul><p>👉 它不会删掉任何候选词，只是让“大家的概率差距”变大或变小。</p><h3>2）Top-k：只在“前 k 名”里抽</h3><p>Top-k 的规则很直白：</p><blockquote>只保留概率最高的 <strong>k 个候选词</strong>，其余一律不考虑，然后再在这 k 个里按概率抽。</blockquote><ul><li><strong>优点</strong>：简单、能防止特别离谱的词混进来</li><li><strong>缺点</strong>：k 是固定的——有时候候选词很集中，有时候很分散，固定 k 可能不够灵活</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525319" alt="" title="" loading="lazy"/></p><h3>3）Top-p（Nucleus Sampling）：只在“累计概率达到 p 的那一撮”里抽</h3><p>Top-p 更像“动态的 Top-k”：</p><blockquote>从最高概率开始往下加，直到累计概率达到 <strong>p</strong>（比如 0.9），只在这一小撮里抽。</blockquote><ul><li><strong>优点</strong>：更自适应：模型很确定时范围会自动变小；模型不确定时范围会自动变大</li><li><strong>缺点</strong>：需要理解“累计概率”的概念，但用起来通常更顺手</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525320" alt="" title="" loading="lazy"/></p><h3>怎么搭配最实用？</h3><p>很多实际系统里最常见的是：<strong>温度 + Top-p</strong></p><ul><li><strong>温度</strong>负责“敢不敢跳出最优解”</li><li><strong>Top-p</strong>负责“别跳得太离谱”</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525321" alt="" title="" loading="lazy"/></p><p>一句话记忆：</p><blockquote><strong>温度让你更有变化，Top-p/Top-k 帮你把变化圈在合理范围内。</strong></blockquote><hr/><h2>总结：掌握那个旋钮</h2><p><strong>温度不会让 AI 更聪明</strong>，它改变的是：AI 在“下一步选哪个词”时的<strong>胆量</strong>和<strong>随机性</strong>。</p><ul><li>想要更像“标准答案”？→ <strong>把温度调低</strong></li><li>想要更多惊喜和创意？→ <strong>把温度调高</strong></li></ul><p>下次你可以试试对 AI 说：</p><blockquote>“请把温度设为 1.2，给我讲一个更疯狂、更有画面感的故事。”</blockquote><p>看看它会不会带你去一趟意想不到的冒险。</p><p>本文由<a href="https://link.segmentfault.com/?enc=4BE2hBJL9ERnFowPc2xSug%3D%3D.b7tYCBoRRMPWsc1xdkZF%2FoIe8Pv8N2M65EVUglwbZVY%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Android 车机与 BLE 设备交互 philadelphia ]]></title>    <link>https://segmentfault.com/a/1190000047525180</link>    <guid>https://segmentfault.com/a/1190000047525180</guid>    <pubDate>2026-01-06 20:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Android 车机与 BLE 设备交互全链路实践指南</h2><p>从广播地址、配对绑定到隐私机制的完整理解</p><p>最近在开发车机系统与无屏 BLE 设备（比如智能冰箱）的连接功能，过程中遇到了一连串看似独立、实则紧密关联的问题：为什么扫描到的地址和配对时一样？RPA 地址到底能不能看到？解绑为什么没有公开 API？回连时直接用连接成功时保存的 MAC 行不行？ADB 怎么清空配对列表？</p><p>这些问题背后其实是一套完整的 BLE 安全与隐私机制。这篇文章把整个探索过程串起来，记录下验证过的方法和踩过的坑，希望能帮到正在做类似工作的你。</p><hr/><h3>一、设备广播地址：你以为的“MAC”可能不是真 MAC</h3><p>一开始我在连接成功后将设备的MAC保存到SP中，后续APP启动或者蓝牙开启后直接根据整个mac直接去连接设备，</p><p>大致流程如图：</p><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDj" alt="connect_process" title="connect_process"/></p><p>但是有的设备是OK的，有的设备不行，后来发现不行的设备是开启了BLE Privacy机制，无法直接根据连接时返回的mac直接连接</p><p>后来发现这涉及到 <strong>BLE（Bluetooth Low Energy）隐私保护机制</strong> 的核心设计 —— <strong>RPA（Resolvable Private Address，可解析私有地址）</strong></p><p>首先看下BLE设备的广播类型</p><h4><strong>BLE 广播地址的类型</strong></h4><p>根据 BLE 规范（Core Spec Vol 6, Part B, Section 1.3），广播包中的 <strong>AdvA（Advertiser Address）</strong> 可以是以下之一：</p><table><thead><tr><th>地址类型</th><th>是否可变</th><th>是否可被解析</th><th>说明</th></tr></thead><tbody><tr><td><strong>Public Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>厂商烧录的 MAC</td></tr><tr><td><strong>Static Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>设备自定义的静态随机地址</td></tr><tr><td><strong>Resolvable Private Address (RPA)</strong></td><td>✅ 动态（如每15分钟换）</td><td>✅ 仅对有 IRK 的设备</td><td>隐私保护，可被配对设备解析</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>✅ 动态</td><td>❌ 否</td><td>完全匿名，无法追踪也无法连接（通常用于 beacon）</td></tr></tbody></table><blockquote>📌 大多数支持配对的智能设备（如你的冰箱）会使用 <strong>RPA</strong> 来广播，以保护用户隐私。</blockquote><h3>传统 MAC 地址的问题</h3><p>早期 BLE 设备使用 <strong>静态公开地址（Public Static Address）</strong>，比如：</p><pre><code>AA:BB:CC:11:22:33</code></pre><p>这个地址是固定的、全球唯一（理论上），但也带来严重隐私问题：</p><blockquote>🕵️‍♂️ 攻击者可以在商场、地铁等公共场所通过扫描 BLE 广播包，追踪某个设备（比如你的手机或冰箱）的行踪。</blockquote><p>为了解决这个问题，BLE 4.0 引入了 <strong>Privacy Feature（隐私特性）</strong>，允许设备使用 RPA <strong>随机变化的地址</strong>，而不是固定 MAC。</p><hr/><h3>🛡️ RPA（Resolvable Private Address）是什么？</h3><p>RPA 是一种 <strong>动态变化但可被“信任设备”识别</strong> 的地址机制。</p><h4>工作原理简述：</h4><ol><li><strong>配对（Bonding）时</strong>，双方交换一个密钥：<strong>IRK（Identity Resolving Key）</strong>。</li><li>此后，设备会定期（如每 15 分钟）生成一个新的 <strong>随机地址（RPA）</strong>，该地址由 IRK + 随机数加密生成。</li><li>只有拥有相同 IRK 的设备（即已配对设备）才能 <strong>“解析”这个 RPA，确认它来自同一个物理设备</strong>。</li><li>对未配对的第三方来说，看到的只是一个不断变化的随机地址，无法追踪。</li></ol><p>✅ <strong>优点</strong>：既保护隐私，又不影响已配对设备之间的通信</p><hr/><h3>什么是Identify Address</h3><ul><li><p>Identity Address</p><p>是设备在配对（Pairing + Bonding）过程中交换的“真实身份”，格式为：</p><ul><li>Public Address（如厂商烧录的 MAC）</li><li>或 Static Random Address（由设备制造商设定，固定不变）</li></ul></li><li>这个地址 <strong>在设备生命周期内是固定的</strong>，也是 Android 系统在 <code>BluetoothDevice.getAddress()</code> 中返回的值（对于 bonded 设备）。</li><li><strong>IRK（Identity Resolving Key）就是用来将 RPA 映射回这个 Identity Address 的。</strong></li></ul><hr/><h3>📱 Android 如何处理 RPA？</h3><ul><li><p>当你和一个支持隐私特性的 BLE 设备（如现代智能冰箱）完成 <strong>配对（bonding）</strong> 后：</p><ul><li>Android 系统会 <strong>自动保存该设备的 IRK</strong>；</li><li>即使冰箱下次广播的是一个全新的 RPA（比如 <code>D4:E5:F6:77:88:99</code>），Android 也能通过 IRK 识别出：“这是之前配对过的那台冰箱”。</li></ul></li><li><p>此时，你在代码中调用：</p><pre><code>BluetoothAdapter.getDefaultAdapter().getBondedDevices()</code></pre><p>返回的 <code>BluetoothDevice</code> 对象的 <code>.getAddress()</code> <strong>始终是配对时的“身份地址”（Identity Address）</strong>，通常是 Public 或 Static 地址（如 <code>AA:BB:CC:11:22:33</code>），<strong>而不是当前广播的 RPA</strong>。</p></li></ul><blockquote>✅ 所以：<strong>系统内部已经帮你完成了 RPA → Identity Address 的映射</strong>。</blockquote><hr/><p>这就回到了为什么直接根据扫描到的mac地址直接回连设备会失败的问题了</p><h3>❌ 为什么不要直接用存储的 MAC 字符串调用 <code>getRemoteDevice(mac)</code>？</h3><p>假设你把扫描到的 MAC（如 <code>"AA:BB:CC:11:22:33"</code>）存到 SharedPreferences，下次直接：</p><pre><code>String savedMac = prefs.getString("fridge_mac", null);
BluetoothDevice dev = adapter.getRemoteDevice(savedMac); 
dev.connectGatt(...);</code></pre><ol><li><code>getRemoteDevice(mac)</code> 仅根据地址字符串返回一个设备引用，它不保证能访问到该设备的绑定上下文（如 IRK）。如果传入的地址不是已配对设备的 Iden<code>t</code>ity Address（例如是一个 RPA），即使物理设备已绑定，系统也无法自动解析隐私地址</li><li>如果此时冰箱正在使用 RPA（比如广播地址是 <code>D4:E5:F6:77:88:99</code>），而你传入的是旧的 Identity Address（<code>AA:BB:CC:...</code>）；</li><li>Android <strong>不会自动用 IRK 去解析或关联这个 RPA</strong>，因为 <code>getRemoteDevice()</code> 不知道这个设备是否已配对；</li><li>结果：<strong>连接失败（GATT ERROR 133 或 timeout）</strong>，即使物理设备就在旁边！</li></ol><blockquote>💡 换句话说：<code>getRemoteDevice()</code> 绕过了系统的 bonding 数据库和 IRK 解析机制。</blockquote><hr/><h3>✅ 正确做法：从 <code>getBondedDevices()</code> 中查找</h3><pre><code>String savedIdentityAddress = "AA:BB:CC:11:22:33"; // 这是你配对时记录的身份地址

BluetoothAdapter adapter = BluetoothAdapter.getDefaultAdapter();
for (BluetoothDevice device : adapter.getBondedDevices()) {
    if (device.getAddress().equals(savedIdentityAddress)) {
        // ✅ 这个 device 对象是系统管理的 bonded 设备
        // 即使冰箱当前用 RPA 广播，系统也会自动解析并建立连接
        device.connectGatt(context, false, gattCallback);
        break;
    }
}</code></pre><p>这样做的好处：</p><ul><li>利用了 Android 内置的 <strong>IRK 解析能力</strong>；</li><li>无论冰箱当前使用什么 RPA，系统都能正确路由到物理设备；</li><li>连接成功率高，符合 BLE 规范。</li></ul><hr/><h3>🔧 补充建议</h3><ul><li>要记录配对时的mac地址而不是扫描到的mac地址，因为扫描的到mac可能是PRA，但是绑定时的mac一定是Identify Address</li><li>在配对完成后，<strong>记录的是 <code>device.getAddress()</code>（即 Identity Address）</strong>，这个地址在 bonding 生命周期内是稳定的；</li><li>不要尝试自己解析 RPA（除非你实现完整的 BLE Host 层，不推荐）；</li><li>如果目标设备 <strong>不支持 Privacy（即始终用 Public Address）</strong>，那么 <code>getRemoteDevice()</code> 也能工作，但为了兼容性和未来升级，仍建议走 bonded devices 路径。</li></ul><hr/><h4>✅ 总结</h4><table><thead><tr><th>方式</th><th>是否推荐</th><th>原因</th></tr></thead><tbody><tr><td><code>getRemoteDevice(savedMac)</code></td><td>❌ 不推荐</td><td>无法利用 IRK 解析 RPA，连接可能失败</td></tr><tr><td>从 <code>getBondedDevices()</code> 查找匹配地址</td><td>✅ 强烈推荐</td><td>系统自动处理 RPA，连接可靠</td></tr></tbody></table><p>所以，<strong>永远优先使用系统提供的 bonded device 对象来发起连接</strong>，而不是自己构造设备对象。这不仅是最佳实践，也是应对现代 BLE 隐私机制的必要手段。</p><p><strong>Android 在设备绑定后，所有 API 返回的都是 Identity Address（身份地址），而不是设备当前广播的地址</strong>。</p><p>BLE 设备可以广播四种地址：</p><ul><li><strong>Public Address</strong>：芯片的固定 MAC；</li><li><strong>Static Random Address</strong>：开机生成、运行期间不变的随机地址；</li><li><strong>Non-Resolvable Private Address (NRPA)</strong>：频繁变化、无法追踪的临时地址；</li><li><strong>Resolvable Private Address (RPA)</strong>：频繁变化，但持有 IRK 的设备能解析回 Identity Address。</li></ul><p>真正的 Privacy 机制 = <strong>使用 RPA 广播 + IRK 解析</strong>。</p><p>当你在车机上调用 <code>device.getAddress()</code>，如果设备已绑定，Android 会自动用 IRK 把 RPA “翻译” 成 Identity Address 再返回给你。所以你永远看不到那个变化的 RPA —— 这不是 bug，而是 Privacy 正常工作的表现。</p><blockquote>✅ 验证方法：用手机装 nRF Connect，在<strong>未绑定状态</strong>下扫描设备，隔 15 分钟看地址是否变化。如果变了，说明 Privacy 生效了。</blockquote><hr/><h3>二、RPA 地址能看到吗？怎么获取原始广播地址？</h3><p>既然系统把 RPA 隐藏了，那我们还能不能拿到真实的广播地址？</p><p>答案是：<strong>只有在未绑定状态下才可能看到</strong>。</p><p>当你调用 <code>BluetoothLeScanner.startScan()</code>，如果设备还没配对，且它广播的是 RPA，那么 <code>ScanResult.device.getAddress()</code> 返回的就是这个原始 RPA（比如 <code>D3:A1:F5:09:88:77</code>）。但一旦你完成绑定，下次再扫，系统就会直接给你 Identity Address。</p><p>所以，<strong>不要试图在绑定后获取 RPA</strong>——你不需要它。Identity Address 才是稳定的设备标识，RPA 只是空中传输的“马甲”。</p><p>如果你是在调试固件，建议用 nRF Connect 或蓝牙嗅探器抓包；如果是开发车机 App，请完全忽略 RPA 的存在，只认 <code>getBondedDevices()</code> 里的地址。</p><hr/><h3>三、回连时直接用保存的 MAC 地址行不行？</h3><p>早期我们图省事，配对成功后把设备地址存下来，下次启动直接用：</p><pre><code class="kotlin">val device = adapter.getRemoteDevice(savedMac)
device.connectGatt(...)</code></pre><p>结果某天测试新固件（启用了 RPA）时，连接直接超时失败。</p><p>原因很简单：<code>getRemoteDevice()</code> 创建的是一个“裸设备对象”，它没有 IRK，也不知道这个地址对应的是谁。而冰箱此刻广播的是 RPA，根本不在 <code>savedMac</code> 这个地址上。</p><p>正确做法是：</p><pre><code class="kotlin">val device = adapter.bondedDevices.find { it.address == savedMac }
device?.connectGatt(...)</code></pre><p>因为 <code>bondedDevices</code> 里的设备对象带着完整的绑定上下文（包括 IRK），系统能自动把 RPA 解析出来并建立连接。</p><blockquote>📌 记住：<strong>地址字符串相同 ≠ 设备对象等价</strong>。安全上下文才是关键。</blockquote><hr/><h4>2. <strong>Android 如何处理这个地址？</strong></h4><ul><li>在 <code>onLeScan()</code> 中，Android <strong>直接把广播包里的 AdvA 字段原样封装成 <code>BluetoothDevice</code> 对象的地址</strong>；</li><li>此时系统 <strong>还不知道这个设备是否已配对</strong>，也没有尝试用 IRK 去解析它（因为还没建立 bonding 上下文）；</li><li>所以：<strong><code>device.getAddress()</code> 就是原始广播地址（raw advertising address）</strong>。</li></ul><p>✅ 举例：</p><ul><li>冰箱的 Identity Address 是 <code>AA:BB:CC:11:22:33</code>（Public）；</li><li>当前广播使用 RPA：<code>D4:E5:F6:77:88:99</code>；</li><li>你在 <code>onLeScan()</code> 中拿到的 <code>device.getAddress()</code> 就是 <code>"D4:E5:F6:77:88:99"</code>；</li><li>即使你之前已经和这台冰箱配对过，<strong>扫描回调仍然返回 RPA</strong>，因为这是物理层看到的内容。</li></ul><blockquote>⚠️ 这就是为什么不能在扫描阶段存储这个地址作为设备唯一标识！</blockquote><hr/><h4>3. <strong>那系统怎么知道这是“老朋友”？</strong></h4><ul><li><p>当你调用</p><pre><code>device.createBond() 
或
device.connectGatt(...)</code></pre><p>时，Android 会：</p><ol><li>检查本地是否有该 <strong>Identity Address 对应的 IRK</strong>（即是否已配对）；</li><li>如果有，就尝试用 IRK 解析当前 RPA；</li><li>如果解析成功（RPA 能还原出已知 Identity Address），就走快速重连流程（无需重新配对）；</li><li>连接成功后，<code>BluetoothDevice.getAddress()</code> 在后续 API 调用中（如 GATT 回调、bonded devices 列表）会返回 <strong>Identity Address</strong>。</li></ol></li><li><h3>public Address和identify Address 是一回事吗?</h3></li><li><blockquote><strong>Public Address 是 Identity Address 的一种，但 Identity Address 不一定是 Public Address。</strong></blockquote><p>换句话说：</p><ul><li><strong>Identity Address（身份地址）是一个逻辑概念</strong>，用于唯一标识一个 BLE 设备；</li><li><p>它可以是以下两种之一：</p><ol><li><strong>Public Device Address</strong>（公开地址，即传统 MAC 地址）</li><li><strong>Static Random Address</strong>（静态随机地址）</li></ol></li></ul><p>所以：<br/> ✅ 所有 Public Address 都是 Identity Address，<br/> ❌ 但不是所有 Identity Address 都是 Public Address。</p><hr/><h4>1. <strong>什么是 Identity Address（身份地址）？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 核心规范）</strong>：</p><blockquote><p>The <strong>Identity Address</strong> is the address used to identify a device during the pairing and bonding process. It is either:</p><ul><li>A <strong>Public Device Address</strong>, or</li><li>A <strong>Static Random Address</strong></li></ul></blockquote><p>这个地址在设备的整个生命周期中是 <strong>固定不变的</strong>，并且会和 <strong>IRK（Identity Resolving Key）</strong> 一起在配对时交换，用于后续解析 RPA（Resolvable Private Address）。</p><hr/><h4>2. <strong>Public Device Address（公开地址）</strong></h4><ul><li>就是我们熟悉的 <strong>48-bit IEEE MAC 地址</strong>，如 <code>AA:BB:CC:11:22:33</code>；</li><li>由厂商烧录，全球唯一（理论上）；</li><li>地址的 <strong>最高有效位（MSB）为 0</strong>（即“公共地址”标志）；</li><li><strong>需要向 IEEE（通过 SIG 或直接）购买</strong></li><li>示例：<code>D0:CF:5E:xx:xx:xx</code>（很多手机/模块使用）。</li></ul><p>✅ 特点：固定、可识别、无隐私保护。</p><hr/><h4>3. <strong>Static Random Address（静态随机地址）</strong></h4><ul><li>由设备制造商或开发者设定的一个 <strong>随机生成但永不改变</strong> 的地址；</li><li><p>必须满足：</p><ul><li>最高两位为 <code>11</code>（表示是静态随机地址）；</li><li>不能是全 0 或全 1；</li></ul></li><li>示例：<code>DE:AD:BE:EF:CA:FE</code>（只要符合格式且固定即可）。</li></ul><p>✅ 特点：固定、不依赖 IEEE 分配、有一定匿名性，但仍可作为身份标识。</p><blockquote>📌 很多 IoT 设备（如低成本 BLE 模块）没有 Public Address，就用 Static Random Address 作为 Identity Address。</blockquote><hr/><h4>4. <strong>为什么需要区分？</strong></h4><p>因为 BLE 隐私机制（RPA）依赖于 <strong>Identity Address + IRK</strong> 的组合：</p><ul><li>当设备启用隐私功能时，它会用 IRK 生成 RPA 来广播；</li><li>配对设备收到 RPA 后，用本地存储的 IRK 尝试还原出 <strong>Identity Address</strong>；</li><li>如果匹配成功，就知道“这是之前配对过的那台设备”。</li></ul><p>所以，无论 Identity Address 是 Public 还是 Static Random，只要它是固定的，就能作为“身份锚点”。</p><hr/></li></ul><blockquote>💡 实际开发中，你不需要关心它是 Public 还是 Static Random —— 只需知道：<strong>这是该设备的唯一身份标识，配对后稳定不变，应该存储它。</strong></blockquote><p>------</p><p>## ✅ 总结表</p><table><thead><tr><th>概念</th><th>是否固定</th><th>是否用于配对</th><th>是否可用于长期标识</th><th>备注</th></tr></thead><tbody><tr><td><strong>Public Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>传统 MAC，IEEE 分配</td></tr><tr><td><strong>Static Random Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>设备自定义，高位为 <code>11</code></td></tr><tr><td><strong>Identity Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>= Public 或 Static Random</td></tr><tr><td><strong>RPA（Resolvable Private Address）</strong></td><td>❌ 否（动态）</td><td>❌ 否</td><td>❌ 否</td><td>用于广播，保护隐私</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>❌ 否</td><td>❌ 否</td><td>❌ 否</td><td>完全匿名，通常不可连接</td></tr></tbody></table><p>------</p><p>### 🎯 开发建议</p><ul><li><strong>不要尝试解析地址类型</strong>，只需在 <code>BOND_BONDED</code> 时存储 <code>device.getAddress()</code>；</li><li>这个地址就是系统认可的 <strong>Identity Address</strong>，无论底层是 Public 还是 Static Random；</li><li>后续通过 <code>getBondedDevices()</code> 匹配该地址即可可靠连接。</li></ul><h3>四、配对弹窗是怎么来的？</h3><p>我们从来没调 <code>createBond()</code>，为什么也会弹出系统配对窗口？</p><p>后来发现，<strong>触发配对的不是你的代码，而是 GATT 特征的安全属性</strong>。</p><p>如果你的冰箱声明某个特征需要“认证后才能读写”（比如设置了 <code>AUTHEN</code> 权限），而当前连接还没加密，那 Android 在收到 <code>Insufficient Authentication</code> 错误后，会自动启动配对流程。</p><p>所以，配对弹窗其实是系统在帮你补安全课。你只需要在配对成功后重试 GATT 操作即可。</p><hr/><h3>五、配对流程</h3><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDV" alt="tongyi-mermaid-2026-01-06-194500.png" title="tongyi-mermaid-2026-01-06-194500.png" loading="lazy"/></p><p><strong>在标准 BLE 通信模型中，配对（Pairing）流程是由 Central（车机）发起的，但实际触发时机往往由 Peripheral（BLE设备，如冰箱）的安全需求间接驱动</strong></p><h4><strong>一、协议层面：谁“发起”配对？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 协议规范）</strong>：</p><ul><li><strong>Central（主设备，如车机）</strong> 负责发送 <code>Pairing Request</code>；</li><li><strong>Peripheral（从设备，如冰箱）</strong> 回复 <code>Pairing Response</code>；</li><li>后续密钥交换、确认值计算等均由 Central 主导。</li></ul><p>✅ 所以从<strong>协议动作</strong>看，<strong>配对是由 Central（车机）主动发起的</strong>。</p><hr/><h4><strong>二、应用层面：谁“触发”配对？</strong></h4><p>虽然 Central 发起配对请求，但它通常<strong>不是凭空发起</strong>，而是因为：</p><blockquote><strong>Peripheral 在 GATT 层拒绝了未加密的访问请求，从而迫使 Central 启动配对。</strong></blockquote><p>典型流程如下：</p><ol><li>车机（Central）连接冰箱（Peripheral）；</li><li>车机尝试读取一个被标记为 <strong>“需要认证”</strong> 的特征（例如 <code>read authen</code>）；</li><li>冰箱返回错误：<code>Insufficient Authentication (0x05)</code>；</li><li><strong>Android 系统检测到该错误，自动调用 <code>createBond()</code> 并发送 <code>Pairing Request</code></strong>；</li><li>配对流程启动，弹出系统弹窗或走 Just Works 模式。</li></ol><p>✅ 所以从<strong>触发原因</strong>看，<strong>是 Peripheral 的安全策略“迫使” Central 发起配对</strong>。</p><p>既然是车机发起的配对请求，那车机-Android系统怎么还能收到配对请求广播呢？</p><p>车机（App）收到的 <code>ACTION_PAIRING_REQUEST</code> 广播，<strong>不是来自远端设备的“请求”</strong>，而是 <strong>Android 系统在自己发起配对前，向 App 发出的一个“协商/干预”通知</strong>。</p><p>这个广播的目的是：让 App 有机会干预配对行为</p><p>例如：</p><ul><li>自动确认 Just Works 配对（免弹窗）；</li><li>填入预共享的 PIN 码；</li><li>拒绝某些设备的配对。</li></ul><h4>Pairing 和 Bonding 区别？</h4><p>这两个概念经常混用，但职责完全不同：</p><ul><li><strong>Pairing（配对）</strong>：协商密钥的过程（生成 LTK、IRK 等），确保本次连接加密；</li><li><strong>Bonding（绑定）</strong>：把配对成果（密钥 + 身份）存到本地，供以后复用。</li></ul><p>你可以只配对不绑定（每次连都重新确认），但不能只绑定不配对。在 Android 里，配对成功默认就会绑定，设备进入 <code>getBondedDevices()</code> 列表。</p><hr/><h3>六、怎么用代码解绑设备？</h3><p>Android 没有公开 <code>removeBond()</code> 方法，但它确实存在，只是被标为 <code>@hide</code>。通过反射调用是行业通用做法：</p><pre><code class="kotlin">fun removeBond(device: BluetoothDevice): Boolean {
    return try {
        val method = device::class.java.getMethod("removeBond")
        method.invoke(device) as Boolean
    } catch (e: Exception) {
        false
    }
}</code></pre><p>注意：</p><ul><li>需要 <code>BLUETOOTH_ADMIN</code> 权限；</li><li>解绑是异步的，必须监听 <code>ACTION_BOND_STATE_CHANGED</code> 广播，等状态变成 <code>BOND_NONE</code>；</li><li>最好加兜底：如果反射失败，引导用户去系统设置手动解绑。</li></ul><p>至于为什么不公开？主要是怕 App 滥用——比如偷偷删掉用户的耳机配对。但又不能完全禁掉，毕竟车机、IoT 设备确实需要程序化解绑。所以成了“能用，但不鼓励”的状态。</p><hr/><h3>总结：几个关键原则</h3><ol><li><strong>地址不变是正常的</strong>：那是 Identity Address，不是广播地址；</li><li><strong>RPA 不需要你关心</strong>：系统会自动处理，你只认 bonded 列表；</li><li><strong>回连必须从 <code>getBondedDevices()</code> 取设备</strong>，否则 Privacy 一开就断连；</li><li><strong>配对由 GATT 安全需求驱动</strong>，不是靠你调不调 <code>createBond()</code>；</li><li><strong>解绑用反射没问题</strong>，但要做好兼容和用户引导；</li><li><strong>Privacy 是好东西</strong>，但前提是两端都正确实现——设备要真启用 RPA，车机要会解析。</li></ol><p>BLE 看似简单，但安全和隐私细节很多。理解这套机制，才能做出既好用又安全的产品。希望这篇总结能帮你少走点弯路。</p>]]></description></item><item>    <title><![CDATA[AI智能体元年：IT服务管理行业的拐点已至 ITIL先锋论坛 ]]></title>    <link>https://segmentfault.com/a/1190000047525207</link>    <guid>https://segmentfault.com/a/1190000047525207</guid>    <pubDate>2026-01-06 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>宏观视角下的行业变革信号</strong></p><p>2025年12月13日，广州天河区的一场百人规模Meetup，或许将成为IT服务管理行业转折的标志性事件。这不是一场普通的技术分享会,而是一次行业集体焦虑的集中释放,更是一场关于未来方向的深度探讨。</p><p>当100余位来自大湾区的IT精英齐聚一堂,当四位深耕行业多年的专家倾囊相授,当"AI智能体"这个概念从PPT走向实战演练,我们看到的不仅是技术的演进,更是一个行业在时代巨变前夜的集体转身。</p><p>从更宏观的视角观察,这场活动折射出IT服务管理行业正在经历的三大深刻变革:技术范式的迁移、人才结构的重组、商业模式的重构。而这三大变革的交汇点,正是AI智能体技术的大规模应用。</p><p><img width="723" height="419" referrerpolicy="no-referrer" src="/img/bVdnvsC" alt="" title=""/></p><p><strong>行业痛点:67%的从业者还未真正触碰AI</strong></p><p>长河在现场的调研数据揭示了一个令人警醒的现实:使用AI超过100小时的IT从业者仅占33%,这意味着超过三分之二的行业从业者仍处于AI技术的观望期。</p><p><strong>这个数据背后隐藏着更深层的行业问题:</strong><br/><strong>第一,认知滞后与技术加速的矛盾。</strong><br/>当大语言模型以月为单位迭代升级,当AI智能体技术从概念走向落地,大部分IT从业者的认知仍停留在"AI是高级搜索引擎"的层面。这种认知与现实的错位,正在成为行业人才发展的最大障碍。<br/><strong>第二,技能结构与市场需求的错配。</strong><br/>传统IT服务管理强调的是系统运维、故障处理、流程优化等执行层面的技能。但AI时代需要的是提示词工程、智能体开发、人机协同设计等新型能力。这种技能代际的断层,导致大量经验丰富的从业者面临转型困境。<br/><strong>第三,投入产出与风险收益的博弈。</strong><br/>企业层面对AI技术持谨慎态度:投入巨大但效果未知,试点成功但推广困难,短期收益不明显但长期不投入又可能落后。这种矛盾心态导致行业整体的技术应用进程缓慢。</p><p>从行业发展周期理论看,IT服务管理正处于从"成熟期"向"变革期"过渡的关键节点。这个节点的特征是:传统业务模式增长乏力,新兴技术尚未形成主流,行业参与者普遍焦虑且方向不明。广州这场Meetup的火爆,恰恰说明了行业对方向指引的强烈渴求。</p><p><strong>技术演进:从自动化到智能化的质变</strong></p><p>丁振兴展示的五层智能体架构,标志着IT运维从"自动化"向"智能化"的质的飞跃。这不仅是技术层面的进步,更代表着行业对运维本质认知的深化。<br/>传统自动化解决的是"怎么做"的问题——给定明确的规则和流程,系统按部就班执行。但这种模式的局限在于:面对复杂多变的IT环境,不可能为每一种场景都预设规则。<br/>智能化解决的是"如何判断"和"如何学习"的问题——系统通过感知环境、回忆经验、规划策略、执行行动、持续优化,形成闭环。这种能力的本质是:将运维专家的认知过程数字化。</p><p><strong>从行业发展趋势看,智能运维市场正在经历三个阶段:</strong><br/>1.0阶段:工具集成时代(2010-2018)。市场以监控工具、自动化脚本、ITIL流程管理为主,各厂商提供独立产品,集成度低。<br/>2.0阶段:平台化时代(2018-2023)。市场出现一体化运维平台,打通监控、告警、处置全流程,但仍以规则驱动为主。<br/>3.0阶段:智能化时代(2024-)。市场进入AI驱动阶段,平台具备自主学习、智能决策能力,从"被动响应"转向"主动预防"。</p><p>丁振兴提到的"80%陷阱"是行业当前阶段的真实写照。这不是技术的失败,而是技术成熟度的客观反映。从Gartner技术成熟度曲线看,AI运维正处于"期望膨胀期"向"泡沫破裂期"过渡的阶段。行业需要的是理性认知,而非盲目追捧或全盘否定。</p><p>值得注意的是,乐维软件支持500+厂商、8000+设备型号、100000+指标体系的技术积累,揭示了智能运维的行业门槛:这不是一个可以靠短期投入快速突破的领域,而是需要长期技术沉淀和数据积累的系统工程。这也解释了为什么该领域至今仍是少数头部厂商主导,新进入者难以撼动的市场格局。</p><p><strong>商业模式:从人力密集到智能体驱动</strong></p><p>罗小军展示的企业业务智能体矩阵,预示着IT服务行业商业模式的根本性变革。<br/>传统IT服务的商业模式是人力密集型的:企业需要大量人员提供技术支持、系统运维、项目实施等服务,收入与人力规模直接相关。这种模式的天花板很明显:利润率受制于人力成本,规模扩张受制于人才供给。<br/>智能体驱动的商业模式是技术密集型的:企业投入研发构建智能体平台,通过智能体提供标准化服务,收入与技术能力相关而非人力规模。这种模式的想象空间更大:边际成本递减,规模效应明显,可以实现指数级增长。</p><p><strong>从行业竞争格局看,这种模式转变将带来三个层面的影响:</strong><br/>企业层面:马太效应加剧。拥有技术能力、数据积累、资金实力的头部企业将加速智能体布局,中小企业面临技术门槛高、投入产出不确定的困境,行业集中度可能进一步提升。<br/>项目层面:交付模式重构。传统的"人月成本"定价模式将被"按效果付费"模式取代。智能体处理的任务越多,单位成本越低,但前期研发投入巨大。这要求企业具备长期投入能力和风险承受能力。<br/>人才层面:需求结构变化。对执行型人才的需求下降,对架构型、创新型、复合型人才的需求上升。初级工程师岗位减少,高级架构师岗位增加,行业人才结构呈现"哑铃型"。<br/>罗小军提到的"方案撰写效率提升60倍"案例,在行业引发了广泛讨论。支持者认为这代表了生产力的革命性提升,质疑者认为这种极端案例不具普遍性。<br/>从行业实践看,效率提升的真实情况可能是:对于高度结构化、模板化的工作,效率提升可达10-50倍;对于需要创造性、判断力的工作,效率提升可能只有1.5-3倍。关键是要识别哪些工作适合用AI,哪些工作仍需人工主导。<br/>更深层的问题是:当效率大幅提升后,市场需求能否同步增长?如果需求相对固定,效率提升的结果就是人力需求下降。这是行业必须直面的结构性挑战。</p><p><strong>数据集成:老问题遇上新技术</strong></p><p>王晨光提出的集成中台方案,触及了企业数字化转型的核心痛点。系统孤岛、数据沉睡、重复劳动——这些问题存在了十几年,为何至今未能解决?<br/>从技术演进史看,每隔几年就会出现号称能解决集成问题的新技术:</p><ul><li>2000年代:企业服务总线(ESB)承诺统一集成</li><li>2010年代:微服务架构承诺松耦合集成</li><li>2020年代:集成中台+AI承诺智能化集成<br/>技术在进步,但问题仍在。根本原因在于:集成问题的本质不是技术问题,而是组织问题。<br/>不同系统背后是不同部门,不同部门有不同利益诉求。数据打通意味着权力边界模糊,流程优化意味着责任重新划分。这些组织层面的阻力,远大于技术层面的难度。</li></ul><p>AI在集成方案中的真正价值,不在于技术实现的突破,而在于降低了使用门槛。当业务人员可以用自然语言查询数据,不再依赖IT部门编写SQL,数据的流动就更加顺畅。当数据异常可以被AI自动识别和修复,数据治理的成本就大幅下降。</p><p><strong>从行业发展趋势看,集成中台市场正在从"项目制"向"产品制"转变:</strong><br/>项目制时代:每个企业的集成需求都不同,需要大量定制开发,交付周期长、成本高、可复用性低。<br/>产品制时代:通过零代码配置、智能适配、自学习优化,大部分集成场景可以通过标准产品实现,只有少数个性化需求才需要定制。<br/>这种转变的商业意义在于:集成服务从"一次性项目收入"变为"持续性订阅收入",从"劳动密集"变为"技术密集",商业模式更加健康。<br/>但挑战在于:标准产品能否真正满足企业的个性化需求?零代码配置的灵活性是否足够?AI的智能化水平能否支撑复杂场景?这些问题的答案,将决定集成中台市场的未来格局。</p><p><strong>人才市场:30%-50%岗位影响的深层解读</strong><br/>圆桌讨论中,专家们给出的"未来3-5年AI将影响30%-50%岗位"判断,在行业引发了强烈反响。这个数字是危言耸听还是客观预测?<br/>从劳动经济学角度分析,技术对就业的影响包含三个效应:<br/>替代效应:AI直接替代人工完成某些任务,导致岗位需求下降。这在重复性高、规则明确的岗位上表现明显,如初级运维工程师、基础开发人员、文档撰写人员。<br/>互补效应:AI提升人工效率,使得同样人力可以完成更多工作,进而刺激需求增长。这在咨询、架构设计、创新研发等岗位上表现明显。<br/>创造效应:AI催生新岗位、新业务、新行业,创造就业机会。如AI训练师、提示词工程师、智能体架构师等新兴岗位。</p><p>IT服务管理行业的现实情况是:替代效应在短期内更显著,创造效应在长期才能体现。这就导致了一个过渡期的阵痛:旧岗位快速消失,新岗位缓慢出现,人才供需出现结构性错配。</p><p><strong>从行业数据看,这种影响已经开始显现:</strong><br/>招聘需求变化:2024年初级运维工程师岗位需求同比下降15%,AI相关岗位需求同比上升60%。但绝对数量上,减少的岗位远多于新增的岗位。<br/>薪资结构变化:掌握AI技能的工程师薪资溢价20%-40%,传统技能工程师薪资增长停滞甚至下降。行业内部的薪资分化加剧。<br/>年龄结构变化:35岁以上的从业者转型难度更大,面临的就业压力更明显。年轻从业者因学习能力强、心态开放而适应更快。<br/>长河提出的"六个月转型路线图",在行业引发了两极分化的评价。乐观者认为这是可行的快速转型方案,悲观者认为这过于理想化。<br/>从行业人才培养实践看,六个月确实可以完成从"不懂AI"到"会用AI工具"的跨越,但要成为真正的"AI架构师",可能需要1-2年的持续实践。关键在于:</p><ol><li>明确"转型"的定义。是掌握基本工具使用,还是具备架构设计能力,还是能独立交付项目?不同层级的要求,时间投入差异巨大。</li><li>识别个人的起点。有编程基础的工程师转型更快,纯运维背景的从业者需要补充更多基础知识。</li><li>找到合适的路径。自学、培训、项目实践各有优劣,需要根据个人情况选择。</li></ol><p><strong>市场格局:巨头布局与创业机会并存</strong><br/>从更宏观的市场竞争格局看,AI智能体在IT服务管理领域的应用,正在重塑行业的竞争版图。<br/>巨头企业的布局策略:<br/>国际厂商如IBM、微软、ServiceNow,国内厂商如华为、阿里云、腾讯云,都在加速AI与IT服务管理的融合。它们的优势在于:技术积累深厚、数据资源丰富、客户基础广泛、资金实力雄厚。<br/>但巨头的劣势也很明显:组织庞大决策慢、产品标准化难以满足个性需求、对细分场景的理解不够深入。<br/>创业公司的机会空间:<br/>像乐维软件这样的专业厂商,像猛犸世纪这样的创新企业,在垂直领域、细分场景、特定行业仍有很大机会。它们的优势在于:对客户需求理解深刻、产品迭代速度快、服务响应及时、性价比高。<br/>从行业发展规律看,技术变革期往往是市场格局重塑的窗口期。那些能抓住新技术、切中真需求、建立壁垒的企业,有可能实现弯道超车。</p><p><strong>值得关注的几个趋势:</strong></p><ol><li>垂直化深耕:不追求大而全,而是在某个细分领域(如金融、医疗、制造)做深做透,建立行业壁垒。</li><li>平台生态化:不只是提供工具,而是构建开放平台,让合作伙伴、客户都能参与智能体开发,形成生态效应。</li><li>服务订阅化:从一次性项目收入转向持续订阅收入,提高客户粘性和企业估值。</li><li>开源社区化:通过开源部分核心技术,吸引开发者社区,形成技术影响力和人才聚集效应。</li></ol><p><strong>政策环境:监管与发展的平衡</strong><br/>AI技术的快速发展,也引发了监管层面的关注。虽然本次Meetup未直接涉及政策话题,但这是行业发展不可回避的外部环境。<br/>国家层面的政策导向:<br/>2023年《生成式人工智能服务管理暂行办法》出台,对AI应用提出了明确要求。2024年各部委密集发布AI相关政策,鼓励创新应用的同时,也强化了安全监管。<br/>对IT服务管理行业而言,政策影响主要体现在:<br/>数据安全:智能体训练和运行需要大量数据,如何确保数据不泄露、不滥用,是合规的首要问题。<br/>算法透明:AI决策过程的可解释性要求,对智能运维、智能诊断等应用提出了挑战。<br/>责任界定:当AI做出错误决策导致系统故障,责任如何划分?这涉及法律和保险层面的安排。<br/>行业自律:各行业协会正在制定AI应用的行业标准和最佳实践,参与标准制定将成为企业的竞争优势。<br/>从国际经验看,监管政策对行业发展是把"双刃剑":过严会抑制创新,过松会带来风险。找到平衡点需要监管部门、行业企业、技术专家的共同努力。</p><p><strong>未来展望:三年内的行业图景</strong><br/>基于当前趋势,我们可以对未来3年IT服务管理行业的发展做出以下预判:<br/>2025年:试点探索期</p><ul><li>大型企业启动AI智能体试点项目,聚焦高价值场景</li><li>专业厂商推出成熟度更高的智能体产品</li><li>行业培训和认证体系逐步建立</li><li>初级岗位需求开始明显下降<br/>2026年:规模应用期</li><li>AI智能体从试点走向规模部署</li><li>人机协同的工作模式成为主流</li><li>行业人才结构调整加速</li><li>新商业模式开始产生规模化收入<br/>2027年:深度融合期</li><li>AI成为IT服务管理的基础设施</li><li>行业竞争格局基本稳定</li><li>新一代技术人才成为市场主力</li><li>技术标准和监管框架基本完善<br/>这个演进过程不会一帆风顺,必然伴随着:技术迭代的不确定性、商业模式的试错成本、人才转型的阵痛期、组织变革的阻力。<br/>但历史的车轮不会停止。就像云计算取代传统IDC、移动互联网颠覆PC互联网一样,AI对IT服务管理的重塑已是不可逆转的趋势。</li></ul><p><strong>拐点已至,选择在你</strong><br/>广州这场Meetup的意义,不在于提供了多少技术细节,而在于它标志着行业集体意识的觉醒。<br/>当100多位IT精英主动牺牲周末时间来学习AI,当四位专家不遗余力地分享经验和洞察,当参会者全神贯注地进行实战演练,我们看到的是一个行业在变革前夜的集体行动。<br/>这种行动本身就是信号:IT服务管理行业的拐点已经到来。<br/>站在这个拐点上,每个从业者、每家企业、每个投资机构都面临选择:<br/>是主动拥抱变化,还是被动等待淘汰?<br/>是投入资源转型升级,还是固守传统模式?<br/>是培养新型人才,还是继续依赖旧有能力?<br/>历史告诉我们,在技术变革的拐点上,选择比努力更重要,方向比速度更关键。<br/>2025年,AI智能体元年。IT服务管理行业的新篇章,正在开启。<br/>而你,准备好了吗?</p>]]></description></item><item>    <title><![CDATA[智驾大模型的「隐形战场」：当GPU堆不动了，行业拼什么？ 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524975</link>    <guid>https://segmentfault.com/a/1190000047524975</guid>    <pubDate>2026-01-06 19:10:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会（OpenAnolis Conference，简称 2025 龙蜥大会）于北京圆满结束。同时，由阿里云智能集团编译器技术总监李三红，龙蜥社区运营委员会副主席、龙腾计划生态负责人金美琴联合出品的“数据×模型×软件”分论坛也圆满举办。来自阿里云、安谋科技、HiEV大蒜粒车研所、中兴通讯以及清华大学、澳门大学等企业和高校的 12 位大咖，从操作系统与上下游生态协同的视角出发，与参会嘉宾一起探讨了如何通过技术协作加速智能驾驶的进步，分享了各自在自动驾驶技术栈中的前沿实践与生态思考。以下文章转自 HiEV 大蒜粒车研所公众号：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524977" alt="图片" title="图片"/></p><p>过去两年，随着大模型的发展，智驾行业行业似乎进入一场“军备竞赛”。从大规模装车量产，采集数据喂养模型迭代，“算力”成为一段时间内主机厂们关注的焦点，行业甚至有「千卡是门槛，万卡是入场券」的说法。</p><p>从 BEV+Transformer 到端到端，再到如今大热的 VLA（视觉-语言-动作）模型，参数量指数级膨胀，让整个行业陷入了一种“囤卡狂热”。</p><p>仿佛只要堆砌了足够的 H100 或 H800，L3 甚至 L4 级别的自动驾驶能力就会在 Scaling Law 的魔法下，自动涌现。</p><p>在前不久的 2025 龙蜥操作系统大会“数据×模型×软件”分论坛上，我们听到了一些冷静得近乎“泼冷水”的声音。 </p><p>主持人在圆桌讨论的时候提到一个很有意思的事情： </p><p>之前微软 CEO 萨提亚·纳德拉在接受采访的时候就感慨过，即便拥有大量的 GPU，也面临着缺乏足够的物理基础设施（如机柜与电力环境）来安置它们的尴尬境地。</p><p>这也折射出了智驾行业一个被长期掩盖的痛点：单纯依靠堆砌 GPU，想“大力出奇迹”的模式，正在撞上一堵「物理现实与经济成本」的墙。</p><p>当行业的焦点都集中在英伟达、华为昇腾这些台前的“算力卡”上时，一场关于操作系统、基础软件与异构计算的“隐形战争”早已在水面下打响。</p><p>阿里云副总裁李俊平在开场致辞中提出了一个公式：AI 的效能 = 数据（燃料）× 模型（引擎）× 软件（油门和方向盘）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524978" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云副总裁李俊平）</p><p>今天的智驾竞争，正在从单一的模型之争，演变为这三者乘积效应的系统工程对抗。</p><h3>这届智驾，被“数据搬运”卡脖子</h3><p>“谈卡伤感情，没卡没感情。”这是前两年智驾圈的真实写照。但到了 2025 年，很多车企发现，即便斥巨资买来了卡，训练效率却并没有线性增长。</p><p>问题出在哪？GPU 在“偷懒”。</p><p>这其实不是什么硬件故障，而是数据“喂”得不够快。</p><p>智驾研发并非只有模型训练这一个环节，它是一个包含数据采集、清洗、标注、挖掘、训练、仿真到端侧部署的一条长长的数据闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524979" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团高级架构师张先国分享了一组数据：智驾研发团队，云端存储的数据总量通常已达到 400PB 到 800PB，日增量在 1PB 以上。一个智驾企业同时进行多个模型训练，消耗的算力经常需要万卡以上。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524980" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团高级架构师张先国）</p><p>想象一下，GPU 就像是一台拥有 F1 引擎的赛车，但如果给它输油的管子（I/O带宽）只有吸管那么细，引擎空转就在所难免。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524981" alt="图片" title="图片" loading="lazy"/></p><p>在 2025 龙蜥大会的现场，多位专家指出了“数据闭环”中存在的隐形关卡： 一个是数据加载的问题。训练开始前，海量的小文件（图片、标注信息）需要从存储层搬运到计算层。另一个是预处理可能遭受的瓶颈：视频需要抽帧、解码、清洗，训练集群就在那里，但数据卡在缓存层过不来，GPU 只能闲置等待。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524982" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团产品专家钱君在演讲中提到，为了解决这个问题，行业正在把目光投向存储与操作系统的底层优化。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524983" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团产品专家钱君）</p><p>例如，龙蜥操作系统（OpenAnolis）给出的方案是全链路的“疏通”：针对 CPFS（并行文件系统），龙蜥在 OS 层面进行了深度适配。缓存写场景下的性能可以直接提升 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短：以前需要几小时，现在几十分钟就能搞定。 这种“看不见”的基础设施优化，虽然没有新开发一个大模型那么性感，但它决定了生产智能的效率和成本，是让万卡集群真正跑满的关键。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524984" alt="图片" title="图片" loading="lazy"/></p><h3>CPU：被忽视的“异构协同”</h3><p>在智驾的模型训练中，公众通常认为关键的算力在于 GPU；但在本届大会上，“CPU 的挖掘”成为当下的新共识。</p><p>“不能只关注 GPU，CPU 在数据预处理、存储 I/O 及逻辑控制中扮演着关键角色。” 中兴通讯操作系统产品副总经理胡冲在圆桌讨论中直言。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524985" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统产品副总经理胡冲）</p><p>事实上，在视频转图片（抽帧）、数据清洗、以及 Spark 大数据分析环节，CPU 才是主力军。而且，随着架构的演进，Arm 架构的服务器 CPU（例如如阿里云倚天 710 ）正在展现出独特的优势。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524986" alt="图片" title="图片" loading="lazy"/><br/>（图/安谋科技（Arm China）云人工智能事业部总监侯科鑫）<br/>安谋科技（Arm China）云人工智能事业部总监侯科鑫女士，在演讲中向现场观众展示了数据中心架构的演进逻辑：随着 NVIDIA  Grace Hopper 异构加速平台的推出，CPU 与 GPU 的“紧密协同处理”已成为行业明确的发展方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524987" alt="图片" title="图片" loading="lazy"/></p><p>为什么要协同？是为了打破“内存墙”。</p><p>“视频处理并不是简单的计算，它对高负载下算力要求极高。”张先国指出。</p><p>智驾训练需要把每秒视频抽帧为 8-32 张图片，在视频解码计算（如 H.264/H.265 格式）的高并发场景下，传统的 x86 架构，由于睿频（超线程）机制和功耗墙的存在，在高负载下往往会降频。</p><p>而张先国分享的实测数据显示，Arm 架构处理器凭借更多的物理核和大缓存（L1/L2 Cache），在智驾数据处理场景下表现惊人： </p><p>首先是视频抽帧，性能比传统 x86 提升约 20%，成本却降低了 20%-30%； </p><p>大数据清洗方面，由于拥有更大的 Cache（缓存），数据 Miss 率极低，这意味着 CPU 不需要频繁地去内存“搬砖”，从而使端到端性能提升了 30%，在部分场景下甚至实现了翻倍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524988" alt="图片" title="图片" loading="lazy"/></p><p>一个高效的智驾云端底座，必须是 CPU 与 GPU “各司其职、紧密抱团”的异构系统。</p><p>侯科鑫还从更宏观的维度讲述了硬件底座的变迁。她指出，为了打破“内存墙”和功耗瓶颈，数据中心正在从通用服务器向“定制化 SoC + Chiplet”演进。</p><p>NVIDIA 的 Grace Hopper 平台就是典型案例——通过将 Arm 架构 CPU 与 Hopper  GPU 紧密互联，实现内存共享，极大降低了数据搬运的延迟。这种 CPU 与 GPU 紧密协作的架构，正是为了解决单一算力无法应对复杂数据流的困境。Arm 推出的 Total Design 生态和 Neoverse CSS，正是以推动异构计算规模化落地为核心目标，让芯片设计公司能节省大量工程投入，快速构建这种异构计算的「高速公路」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524989" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><h3>基础软件的魔法：不堆卡也能让训练变得更快</h3><p>摩尔定律在放缓，硬件的红利正在吃紧。这时候，软件工程的价值就被进一步放大了。阿里云智能集团编译器技术总监李三红在圆桌环节提到了一个非常典型的矛盾：模型开发者的“爽”和底层工程师的“痛”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524990" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团编译器技术总监李三红）</p><p>算法工程师喜欢用 PyTorch 的 Eager 模式，因为这样写代码像写 Python 一样灵活，所见即所得；但这种模式对底层硬件极其不友好，运行效率低。而底层工程师希望用 Compile 模式，把代码编译成极致优化的机器码，但这又要求上层改代码，门槛极高。</p><p>“上层的模型开发者追求开发效率（Eager Mode），底层的 Infra 追求成本和性能，这中间的 Gap（鸿沟），就是基础软件的机会。” 阿里云智能集团编译器技术总监李三红在圆桌讨论中一针见血地指出。</p><p>针对如何填补这一鸿沟的问题，阿里云智能集团产品专家钱君与高级架构师张先国在随后的演讲中展示了龙蜥操作系统（OpenAnolis）如何通过全链路优化，在不改变硬件的情况下“白捡”性能：</p><ul><li>存储加速（IO 吞吐）： 针对 CPFS（并行文件系统），系统在 OS 层面进行了深度适配。钱君披露的数据显示，在缓存写场景下，性能提升了惊人的 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短，断点续训不再是噩梦。</li><li>网络加速（打破 TCP 限制）： 张先国指出，通过部署自研的 eRDMA 协议，相比传统 TCP，延迟降低 3 倍，带宽提升 4 倍（实测可达 18GB/s）。这让数据在节点间的跳跃如同在本地总线般顺滑。</li><li>编译器优化（榨干每一滴算力）： 针对 PyTorch 等框架的运行效率痛点，利用 AI Compiler 进行算子融合。据钱君介绍，这套方案在部分通用模型上带来了接近 100% 的性能提升，有效地解决了开发灵活性与运行效率不可兼得的难题。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524991" alt="图片" title="图片" loading="lazy"/><br/>效果有多明显？ </p><p>地平线和小鹏汽车的案例显示，通过这一套“操作系统+编译器+调度”的组合拳，部分场景下的性能提升可达 30% 甚至 100%，而成本却能下降 20%-60%。</p><p>在「降本增效」成为汽车产业主旋律的 2025 年，这种来自基础软件的“软实力”，比盲目堆更多的卡，更有性价比。</p><h3>眺望未来：世界模型与“合成数据”</h3><p>如果说当下智驾行业发展的痛点是“效率”，那么未来的挑战可能会是“认知”。</p><p>清华大学人工智能研究院视觉智能研究中心主任邓志东教授在圆桌论坛上抛出了一个前瞻性观点：智驾模型正在从单纯的感知，向世界模型（World Model）演进。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524992" alt="图片" title="图片" loading="lazy"/><br/>图片来源：CVPR 2024 Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</p><p>目前的端到端大模型，虽然能处理很多场景，但面对极端的 Corner Case（长尾场景），靠实车采集的数据永远是不够的。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524993" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><p>“路是跑不完的，但世界是可以被模拟的。”</p><p>但这种演进这种演进对基础设施提出了更苛刻的要求：</p><ul><li>算力需求的指数级爆炸： 世界模型极重，不仅需要理解物理世界，还要生成虚拟物理世界。这可能需要数百亿甚至更高的算力支撑，甚至触及到供电能力的边界。</li><li>合成数据的崛起： 真实路采数据的效率太低且稀缺。未来，大量的训练数据将来自“虚拟物理世界”的高效生成。这对 GPU 的渲染能力和 CPU 的逻辑模拟能力提出了双重挑战。</li><li>软件定义的灵活性：正如中兴操作系统产品线副总经理胡冲在圆桌中所感慨的，算法迭代极快——“去年可能还是 BEV，今年就是 VLA 了”。而阿里云李三红也证实，一线技术团队确实清晰感知到了模型向 VLA 及世界模型演进的趋势。这种软件层面的极速狂奔，与硬件芯片较长的迭代周期形成了鲜明对比。这就要求编译器和操作系统必须具备极强的适应性，通过软件定义来抹平硬件迭代的时间差。AI 不仅要“看懂”视频，还要能“生成”视频，甚至要理解牛顿定律。</li></ul><p>邓教授指出，这需要底层算力支持极其复杂的“虚实迁移”。这意味着，未来的操作系统不仅要调度计算，还要调度“物理世界的规则”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524994" alt="图片" title="图片" loading="lazy"/><br/>（图/清华大学教授、清华大学人工智能研究院视觉智能研究中心主任邓志东）</p><p>这也解释了为什么像龙蜥这样的开源社区，开始在这个阶段强调“ AI 原生操作系统”的概念——因为旧的底座，真的撑不住新的世界了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524995" alt="图片" title="图片" loading="lazy"/></p><h3>开源底座的长期主义</h3><p>从 2025 龙蜥大会的这场分论坛中，我们看到了汽车科技行业的一个明显转折：</p><p>大家不再盲目迷信硬件的堆砌，开始回归计算机科学的常识——系统协同。</p><p>面对 Arm、x86、RISC-V 等复杂的芯片架构，面对日新月异的模型算法，车企和智驾公司不可能每一家都去从零手搓一套底层软件。</p><p>而龙蜥社区的存在，就是为了提供一个标准化的技术底座，屏蔽底层异构硬件（不同架构的 CPU、GPU、NPU）的差异，让车企和智驾公司能够专注于上层模型和算法的创新。正如 Arm 通过 Arm Total Design 联合产业链一样，软件层面也需要这样一个“连接器”来降低全行业的试错成本。</p><p>正如胡冲所言：“通过社区共建、共享，降低车企的研发门槛与成本，是解决算力荒的另一种路径。”</p><p>在算力资源有限、成本高企、模型日趋复杂的背景下，谁能更高效地榨干每一 Tops 算力的价值，谁能以更低的成本完成数据的闭环流转，谁就能在 L3+ 的量产前夜活下来。</p><p>数据是资产，模型是能力，而软件与操作系统，是这一切的根基。</p><p>自动驾驶的下半场，不再是单点技术的突破，而是“数据-模型-软件”全链路的生态战争。在这个战场上，那个由 CPU、操作系统、编译器、文件系统构成的庞大“新基座”，正在成为决定胜负的隐形力量。</p><p>对中国的自动驾驶产业而言，建立一个自主、可控、高效的基础软件生态，其战略意义或许丝毫不亚于拥有几万张显卡。</p><p>因为只有根扎得够深，智能的树才能长得够高够稳。</p><p>本次分论坛回顾已上线，欢迎点击下方链接查看回放：<a href="https://link.segmentfault.com/?enc=Ad0vvxxozxD8A3cfJDxYVg%3D%3D.KaKY6pQ2Dh7iI5rJZ8a12KlhpFFqCT4%2BFD6fTofl9ZOwG4ruzgEki2alw41JoyB5" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[优秀学子获颁证书，开放原子校源行Meetup活动（中南大学站）圆满举办 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525016</link>    <guid>https://segmentfault.com/a/1190000047525016</guid>    <pubDate>2026-01-06 19:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，由浪潮信息联合龙蜥社区、中南大学信息与网络中心、电子信息学院共同举办的开放原子校源行 Meetup 活动（中南大学站）顺利举行。本次活动吸引了 70 余名中南大学本科生和研究生的积极参与，现场气氛热烈，同学们和与会嘉宾深入交流开源文化与技术应用，收获颇丰。</p><p>活动伊始，中南大学电子信息学院特聘副教授、博士生导师施鹤远主持开场环节。施教授对参与活动的师生、技术专家表示欢迎，并简要介绍了活动背景和目的。他强调，本次活动旨在为同学们搭建一个学习和实践开源技术的平台，帮助大家更好地了解开源文化、技术应用以及未来发展方向。他鼓励同学们积极参与今天的活动，主动与嘉宾交流，积极探索开源技术在实际应用中的价值，为未来的职业发展和个人成长积累宝贵经验。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525018" alt="图片" title="图片"/><br/>（图/中南大学电子信息学院特聘副教授、博士生导师 施鹤远）</p><p>中南大学电子信息学院教授、博士生导师、副院长石金晶为本次活动致辞。石院长在致辞中表示，开源技术不仅是当今科技发展的前沿趋势，更是同学们提升自身竞争力的重要途径。开源社区汇聚了全球最优秀的技术人才和创新项目，同学们在这里可以接触到最前沿的技术理念和实践经验。石院长强调，学院将全力支持同学们的开源实践，为大家提供更多的资源和平台，希望同学们能够珍惜这次机会，积极参与开源活动，为自己的未来职业发展打下坚实的基础，同时也为电子信息学院的学科发展贡献自己的力量。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525019" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、副院长 石金晶）</p><p>中南大学电子信息学院教授、博士生导师、教学实验中心主任胡超为本次活动致辞。胡主任在致辞中表示，开源技术为同学们提供了一个绝佳的实践平台，能够让大家在实践中快速成长。他指出，开源项目不仅能够提升同学们的专业技能，还能培养大家的创新思维和团队合作精神。胡主任鼓励同学们积极参与开源社区，主动探索和学习，勇于挑战自己。他强调，学院将为同学们提供全方位的支持，包括技术指导、实验资源等，帮助大家更好地参与到开源项目中。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525020" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、教学实验中心主任 胡超）</p><p>活动中还举行了“浪潮信息 - 龙蜥技术认证证书颁发仪式”。石院长与胡主任为在技术认证中表现优异的同学颁发了工程师证书，表彰他们在开源技术领域的突出成绩，激励更多同学投身开源实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525021" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息 - 龙蜥技术认证证书颁发仪式）</p><p>中南大学计算中心实验师徐海坤在活动中作了题为《AI 赋能科学计算》的分享。他介绍了中南大学计算平台的发展历程，包括其在 2020 年建成的千万亿次级计算平台，以及该平台在全球和中国相关领域排行榜中的优异表现。报告重点阐述了 AI 技术在科学计算中的应用，包括 AI 能力平台的建设、基于 AI 的智能运维和作业调度优化等内容，强调了开源技术在提升平台性能和运维效率中的重要作用，展示了中南大学在 AI 赋能科学计算领域的创新实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525022" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学计算中心实验师 徐海坤）</p><p>阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦以《让校园代码长出 AI 翅膀：与龙蜥共探下一代开源智能》为题进行分享。他介绍了龙蜥社区的 Anolis OS 23 操作系统，重点阐述其在 AI 场景下的创新应用，包括 AI 辅助开发、系统构建优化和运维智能化。他还展示了 OS Copilot 智能助手的功能，强调其在降低 Linux 使用门槛和提升运维效率方面的优势，并鼓励同学们积极参与开源社区活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525023" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦）</p><p>浪潮信息高级产品经理 Viki Wang在活动中分享了《AI 时代开源操作系统应用及生态创新实践》。他表示，浪潮信息作为龙蜥社区副理事长单位，在开源领域持续创新，其开源贡献排名位居前列，并在多个领域取得显著成果。他还分享了浪潮信息在 AI 时代的系统优化成果，如 GPU 和 CPU 异构算力协同、大模型推理性能提升以及兼容性测试基准的建立等，展示了浪潮信息在开源操作系统领域的技术实力和生态建设成果。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525024" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息高级产品经理 Viki Wang）</p><p>龙蜥社区 CXL SIG Maintainer 李伟在活动中分享了《携手龙蜥 共创芯生态》。他介绍了其团队在开源领域的全面布局和深度合作，特别是在龙蜥社区的积极参与。通过贡献内核优化、虚拟化支持和安全技术，推动了开源生态的发展。李伟强调，其团队致力于通过开源合作，共同打造开放、共赢的芯片与操作系统生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525025" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区CXL SIG Maintainer 李伟）</p><p>本次开放原子校源行 Meetup 活动（中南大学站）在热烈的氛围中圆满落幕。通过本次活动，浪潮信息携手开放原子、龙蜥社区及中南大学，成功搭建了一个前沿技术交流与实践的平台，为同学们开启了通往开源世界的大门。未来，浪潮信息将继续践行“龙蜥+”合作模式，深化与高校的合作，助力更多学子在开源领域成长成才，为开源生态的繁荣发展持续贡献力量。</p><p>龙蜥技术认证学习中心：<a href="https://link.segmentfault.com/?enc=Y2JFwazv9c2RpeTAkxdl%2BA%3D%3D.D%2B8TjMa%2FZo3B02zmnK3x3lKmicqpZuvd0Lny6tggDws%3D" rel="nofollow" target="_blank">https://openanolis.cn/course</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[专访 | 软硬协同、开源共建：英特尔与龙蜥携手打造 AI 时代的可信计算底座 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525040</link>    <guid>https://segmentfault.com/a/1190000047525040</guid>    <pubDate>2026-01-06 19:09:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>编者按：近日，2025 龙蜥操作系统大会（OpenAnolis Conference）在北京圆满召开。当智能驾舱厂商训练自动驾驶 AI 模型、金融机构运行 AI 风控系统时，普遍面临相同困境：数据敏感不敢上云，本地算力又难以支撑大模型需求。AI大爆发后，“算力效率”与“数据安全”的矛盾愈发突出。2025 龙蜥操作系统大会前夕，InfoQ 对话英特尔技术专家与阿里云技术专家，揭秘双方如何通过第六代至强处理器与龙蜥操作系统的深度协同，破解这一行业难题。以下为采访全文：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525042" alt="图片" title="图片"/></p><h3>合作底层逻辑：为什么是“英特尔硬件+龙蜥开源”？</h3><p>AI 时代的算力释放，早已不是“硬件单枪匹马”能实现的。“过去硬件做芯片、软件写系统的分工模式已失效，AI 模型扩大与数据敏感化，要求硬件与软件必须深度协同。”英特尔技术专家强调，“2020 年我们成为龙蜥首批理事单位，正是看中龙蜥操作系统在云原生和 AI 领域的开源属性——它能快速承接硬件新特性，而英特尔的芯片技术，就是要为这个开源生态打下坚实的算力底座。”</p><p>过去一年，双方形成了固定的合作节奏：硬件首发时与龙蜥操作系统同步适配，避免企业“空有硬件用不了”；针对硬件暴露的软件问题联合优化，成果反哺龙蜥及 Linux 上游社区；英特尔提供测试资源，龙蜥联动客户推动技术落地。总结来说，英特尔负责造算力、锁安全，龙蜥负责用算力、传价值，这是双方合作的核心逻辑。</p><p>阿里云技术专家补充道：“阿里云作为云平台方，刚好承接这种协同成果——把英特尔的硬件能力和龙蜥操作系统的系统优势，打包成企业能直接用的云服务，这也是我们作为社区理事长单位的价值。”</p><h3>技术共建：从芯片到工具链，让 AI 算力“跑满效能”</h3><p>AI 算力的高效释放，需要“芯片发动机”“系统公路”与“工具链交通规则”的协同。过去一年，英特尔与龙蜥的技术共建集中在这三大方向：</p><h4>第六代至强适配：不止能用，更要“榨干”性能</h4><p>2025 年发布的第六代至强处理器，针对不同 AI 场景做了细分设计：Granite Rapids 主打高密度计算，适配金融风控、科学计算等强性能需求；Sierra Forest 聚焦云原生大规模部署，优化能效比以降低云厂商运营成本。“我们的芯片设计贴合场景化需求，而龙蜥操作系统能精准匹配这种特性，让硬件能力不浪费。”英特尔技术专家说。</p><h4>为让硬件性能充分释放，双方完成了两项关键优化：</h4><ul><li>全链路适配：覆盖龙蜥操作系统 5.10 长期支持版与 6.6 最新特性版，同时完成主流虚拟化平台定制，为 Granite Rapids 开发“大内存调度补丁”，支持 2TB 以上内存以满足 AI 训练需求；</li><li>突破多核瓶颈：针对新一代处理器近 128 核的硬件特性，重构龙蜥操作系统多核调度算法，通过“专属缓存分配”减少核心资源争抢，优化内存页表管理实现有序读写。</li></ul><p>这些优化最终让 Granite Rapids 在多线程任务中性能较上代提升显著。“性能提升是企业能切实感受到的变化，这是软硬件协同的价值。”英特尔技术专家表示。</p><h4>下一代硬件预研：提前3个月适配，消除“空窗期”</h4><p>为解决企业“硬件到位、系统未就绪”的痛点，双方采用“提前布局”策略。英特尔下一代至强 6 Plus 服务器（代号 Clearwater Forest）尚未上市，2025 年 Q2 已联合龙蜥启动适配。“企业采购硬件投入大，我们把适配周期提前，就是要让客户拿到硬件就能开机测试，这符合龙蜥社区‘开箱即用’的理念。”英特尔技术专家表示。</p><h4>异构工具链：oneAPI+OpenVINO，降低开发门槛</h4><p>AI开发者常受困于“硬件异构”——为 CPU 写的代码无法直接在 GPU、NPU 上运行，重复适配耗费大量精力。英特尔与龙蜥的解法是构建“统一工具链”。“开发者的核心价值是优化 AI 模型精度，不是做硬件适配的‘翻译官’。”英特尔技术专家直言，“oneAPI 和 OpenVINO 的融合，就是要把硬件差异藏在工具链里，让一套代码跑通所有设备。”</p><ul><li>oneAPI 统一开发框架：基于 LLVM 扩展异构编译能力，搭建设备抽象层，一套代码可调用不同硬件能力；该平台支持多种编程语言，包括 C++、Python、Fortran 等，使得 AI 模型的训练和推理能够在不同计算架构上高效执行。</li><li>OpenVINO 工具链即插即用：与龙蜥操作系统深度集成，简单命令即可部署，为云端和边缘计算环境中的 AI 推理任务提供优化方案，进一步降低 AI 部署的计算成本，提高 AI 模型的执行效率。</li></ul><p>“我们的目标是让开发者聚焦模型优化，而非硬件适配。”英特尔技术专家表示，这正是“软件定义、硬件赋能”的核心体现。</p><h3>生态共建：让算力生态“活起来”</h3><p>技术落地离不开生态支撑。作为龙蜥社区副理事长单位，英特尔从社区治理、资源支持、国际化联动三方面推动生态发展：</p><p>首先是深度参与社区治理。英特尔并非单纯的“硬件供应商”，而是深度参与龙蜥社区底层建设：如主导 X86 架构优化的 Arch SIG 项目，制定至强处理器在龙蜥操作系统上的性能基准测试体系；参与《国产服务器操作系统发展报告（2025）》中核心章节的撰写；推动龙蜥社区加速国际化等。“开源社区要靠核心厂商带头做实事，这是我们作为副理事长单位的责任，也是为了让龙蜥生态更有技术厚度。”英特尔技术专家说。</p><p>其次是开放资源、降低参与门槛。为解决中小企业“缺硬件、缺技术”的问题，英特尔向龙蜥社区开放测试硬件，开展联合测试并输出技术文档、联合报告。阿里云技术专家补充：“我们会把测试成果转化为云平台最佳实践，帮客户少走弯路。”</p><p>最后是国际化经验反哺。依托在 Linux Foundation、CNCF 的经验，英特尔帮助龙蜥优化内核补丁流程，深度参与 X86 架构补丁审核；并推动龙蜥开发者参加全球开源峰会，加强国际化交流等。</p><h3>机密计算：用“硬件锁+开源钥匙”守护数据安全</h3><p>AI 时代的核心矛盾是“数据需流动产生价值，却怕流动中泄露”。英特尔与龙蜥的解法，是从硬件隔离到开源方案的全链路防护。</p><p>英特尔从硬件层面构建安全底座，核心依赖两大技术，其原理均通过硬件隔离实现数据防护。“机密计算的核心是‘硬件可信’，软件再安全，硬件被突破就没用。”英特尔技术专家解释，“TDX 和 SGX 就是从芯片层面给数据加‘锁’，让安全成为硬件原生能力。”</p><ul><li>TDX（可信域扩展）：在至强芯片中创建“隔离执行域（TD）”，即使系统内核被攻击，TD内的内容也无法被访问，内存数据通过内存控制器实时加密，仅硬件才能解密；</li><li>SGX（软件防护扩展）：针对轻量级场景在内存中划分“加密区（Enclave）”，仅授权代码可访问，其他进程即使获取内存地址，看到的也只是乱码。</li></ul><p>为让龙蜥操作系统适配这些能力，英特尔在系统内核中集成 TDX 和 SGX 驱动并由硬件实现 AES-GCM 加密协议，确保安全防护不影响性能。</p><p>此外，基于硬件底座，阿里云在龙蜥社区推出 Confidential AI 开源方案，整合 TDX 安全能力、远程证明服务与密态存储/网络能力，降低企业使用 TDX 机密计算的门槛。“英特尔的硬件是‘安全地基’，我们的工作是在地基上搭好‘房子’，让企业不用自己打地基就能用。”阿里云技术专家说，“目前龙蜥社区 Confidential AI 开源方案已落地阿里云异构机密计算实例，并正与消费电子、智能驾舱客户合作。”</p><p>为推动行业规范，双方在标准化工作上已取得明确进展。阿里云技术专家具体介绍：“以 Confidential AI 为技术基础，阿里云已联合 30 多家合作伙伴牵头编写 CCSA AI 数据安全的标准化架构与技术实现方案，重点覆盖 AI 推理和训练两大核心场景，目前这项工作已进入实质推进阶段。”</p><p>国际标准的布局也在同步展开，他补充道：“我们计划以 CCSA 的标准化成果为基础，在 12 月初日内瓦举行的国际电信联盟 ITU-T SG17 分论坛上，推动该标准的国际版本立项。这将形成一个良性闭环——开源解决方案为标准化提供了可落地的技术参考，而标准化规范又能为开源方案的合理性和通用性提供背书。”</p><p>在他看来，这种联动恰好体现了核心价值：“商业需求驱动开源方案迭代，开源方案支撑标准化落地，标准化又反过来赋能商业推广，三者不是孤立的，而是循环共生的关系。”</p><h3>未来方向：AI 原生时代的多元协同</h3><p>在 2025 龙蜥大会上，双方集中展示了第六代至强与龙蜥操作系统的性能优化成果、Confidential AI 落地进展及 Clearwater Forest 适配情况，同时释放 AI 原生时代的算力发展方向。</p><p>“AI 原生算力靠‘芯-OS-云-AI’协同。”英特尔技术专家透露规划：硬件上，下一代至强将进一步强化 AI 与安全能力；系统上，将联合龙蜥开发 AI 任务调度器，优化资源分配效率；场景上，将针对隐私敏感场景推出通用部署方案。</p><p>对企业而言，这意味着未来使用 AI 将更简单：依托“英特尔硬件+龙蜥操作系统”的组合，无需自行解决适配、优化与安全问题，即可直接获得高效且安全的算力支撑。随着龙蜥大会的召开，这套“硬件底座+开源生态”的方案，将成为企业 AI 落地的核心选择。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[不止于用，更在于创！龙蜥社区点燃高校开源火种 | 龙蜥五周年征文精选 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525045</link>    <guid>https://segmentfault.com/a/1190000047525045</guid>    <pubDate>2026-01-06 19:08:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>各位小伙伴，龙蜥社区已启动主题为「5 周年，你与龙蜥的故事」征文活动！征文内容包括但不限于以下：</p><p>故事文章：文字记录你的经历、感悟或技术心得，包括：龙蜥操作系统 Anolis OS 的使用体验、相关技术特性解读、经验分享等。</p><p>故事视频：用镜头讲述你与龙蜥的点滴瞬间，参与某次大会/ MeetUp 的拍摄视频等（3 分钟内即可）。</p><p>优质文章将获得龙蜥社区官网及公众号推荐展出，还可获得神秘礼品。欢迎各位龙蜥社区朋友来稿~</p></blockquote><p>本期征文故事主角：施刚，龙蜥社区 2025 年度优秀贡献者奖获得者、成都东软学院计算机与软件学院教授、中国自动化学会边缘计算专业委员会委员，从事计算机系统研究与操作系统相关课程教学工作。2022 年，通过教育部产学合作协同育人项目开始与龙蜥社区开展深度合作。</p><h3>初识龙蜥社区</h3><p>在高校计算机相关专业中，操作系统相关的专业课程占据着重要的地位。多年来，基于 UNIX 和 Linux 系统来进行相关课程的讲授与学习，是高校计算机相关专业师生的普遍选择。早期，CentOS 系统作为 RHEL 系统的完全功能兼容版且开源免费的特性，是高校学习操作系统及中小 IT 企业用于部署各类应用服务器的首先系统。但随着 2021 年末，CentOS 系统停止更新退出市场后，不仅商业市场，在高校教学领域也必须加紧填补其留下的技术空白。</p><p>近年来，在国产系统走进课堂的大背景下，作为面向智算时代的国产开源操作系统，龙蜥操作系统 Anolis OS 已成为 CentOS 的优秀继承者，它不仅完全兼容 RHEL，更针对云原生、高性能计算进行了优化，是高校在操作系统相关课程尤其是 Linux 相关课程中的最优选择。2021 年底，我开始关注 CentOS 停服后的国产操作系统替代方案，并从此开始了解龙蜥社区。2022 年，我通过申请教育部产学合作协同育人项目（龙蜥社区理事长单位-阿里云发布 Anolis OS 项目），与龙蜥社区正式确立了合作关系，并把龙蜥社区正式引入了成都东软学院计算机相关专业的教学实践工作中来。同年注册龙蜥社区，下载安装或通过龙蜥实验室使用龙蜥操作系统的师生就超 1500 人次。至今，成都东软学院已有累计超 5000 人次师生学习和使用龙蜥操作系统。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525047" alt="图片" title="图片"/></p><p>基于 Anolis OS 8，我们也产出了一套完整的《Linux 系统管理与服务器配置》教材，用于日常教学。社区成员也可在龙蜥文档中心获取：<a href="https://link.segmentfault.com/?enc=ptZx1WglRHCOfgYTNCnpKw%3D%3D.1Lg7dpPFr3sqejFcRRI%2FUgtzwcj67OrF2GiUfZapyCQHGy96Mf94hC1IG9hDCdqVMd0VtwuEuiF2unAPfowhTQ%3D%3D" rel="nofollow" target="_blank">https://docs.openanolis.cn/document/detail/rxli6fw9</a></p><h3>龙蜥实验室：优秀的在线实践与学习平台</h3><p>对高校的计算机类实践教学活动来说，拥有一个优秀的在线实践平台是非常重要的。依靠高校自身打造基于操作系统的服务性平台所需的软硬件资源非常大，国内高校鲜有独立建立的相关实践教学平台。而龙蜥社区提供的龙蜥实验室帮助我完美解决了此问题。作为学生，只需要一台联网的计算机，就可在“龙蜥实验室”申请一台机器并在其进行各类实践和实验活动。我在使用龙蜥实验室的过程中，还不断通过向龙蜥社区后台技术人员反馈使用情况并提出改进意见，使得龙蜥实验室的申请和使用流程越来越优化和便捷。以我讲授的 Linux 基础课程为例，除日常的各类实践类作业，该课程所涉及的 8 个实验项目中有 6 个都可通过龙蜥实验室在线完成。 </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525048" alt="图片" title="图片" loading="lazy"/></p><h3>龙蜥技术认证：为高校学生提供企业认可的职业技能认证</h3><p>对高校学生来说，毕业求职时能提供更多的 IT 职业认证技能证书更能得到对口求职单位的认可。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525049" alt="图片" title="图片" loading="lazy"/></p><p>在 Linux 的认证领域，传统的 RHEL 认证已发展了近二十年，在 RHEL 占据国内高端服务器操作系统的时代其认证证书的含金量是很高的。但随着 CentOS 的退出及以龙蜥操作系统为代表的国产服务器操作系统的强势崛起，国内 IT 企业对龙蜥操作系统的认可度也越来越高。根据2025 龙蜥操作系统大会数据显示，当前龙蜥操作系统的装机量已突破 1000 万套，市场占有率接近 50%，国内 IT 市场对相关运维人才的需要也越来越多。2023 年开始，龙蜥社区联合相关企业开始推出龙蜥技术认证的活动，我在成都东软学院计算机学院配合了该认证活动的推广和实施。</p><p>首次认证是基于龙蜥-统信联合开展的，有 600 多名同学报名，这些同学都经过了一学期的 Linux 课程学习，大部分同学都顺利通过了考试获得了龙蜥社区和统信的双认证证书。在首次活动顺利开展的基础上，2024 年度和 2025 年度，我在龙蜥社区的协助下，又继续开展了两次集中式的认证考试组织，报名同学所覆盖的专业由成都东软学院的计算机科学与技术拓展到了网络空间安全、网络工程、大数据等多个计算机相关专业，报名人数也逐年提升。为提高同学们的认证考试通过率，龙蜥社区还联合浪潮信息和统信软件的技术专家在后续两次考试前为报名同学进行了统一在线培训。</p><p>截止 2025 年下半年，成都东软学院已和龙蜥社区配合共同开展了 3 次龙蜥技术认证考试，累计参与学生 2000 余人次，通过率接近 80%。在推进龙蜥认证活动的过程中，我还通过申请将认证证书与 Linux 相关课程考核成绩相关联的方式，更好地激发和推动了学生参加龙蜥认证活动的热情。随着龙蜥中级认证活动在 2025 年的推出，后续龙蜥认证在我校的推进将更加深入开展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525050" alt="图片" title="图片" loading="lazy"/><br/>（图/技术认证现场）</p><h3>深度合作：龙蜥社区与高校计算机专业教学的发展方向</h3><p>操作系统类课程是计算机专业的核心课程也是我国计算机技术发展的重要基石，高校是为我国计算机行业提供后备人才的最重要基地。在当前国产软件国产系统进校园进课堂的大趋势和大背景下，龙蜥社区在与高校合作领域面临着非常好的发展前景。以我本人为例，当前我已将龙蜥操作系统作为操作系统类课程学习和实践的主要平台，并将龙蜥实验室作为相关课程实践和实验活动开展的主要场景，通过将龙蜥认证活动与相关课程的考核深度绑定。同时，龙蜥社区还提供了丰富的在线学习视频，可作为 Linux 相关课程内容学习的强力辅助。我作为一名高校教师，深感龙蜥社区在专业度上与计算机相关课程教学大纲和教学内容是高度契合的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525051" alt="图片" title="图片" loading="lazy"/><br/>最后，高校与社区间是可以持续开展深度合作互惠互利的关系，龙蜥操作系统作为国产操作系统的优秀代表，高校的土壤可以为龙蜥操作系统培养源源不断的学习和使用者，同时也为 IT 企业输送合格的龙蜥操作系统开发和运维人员。随着我国在操作系统技术领域的不断发展，国产操作系统必将逐步取代 RHEL 和 Windows 为代表的各类非国产操作系统在国内各领域的地位和市场。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[关于 AI 编程的思考 edagarli ]]></title>    <link>https://segmentfault.com/a/1190000047525058</link>    <guid>https://segmentfault.com/a/1190000047525058</guid>    <pubDate>2026-01-06 19:07:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在低代码开发与AI商家提效领域深耕多年后，我对AI编程的核心认知逐渐聚焦于“人机协同的价值放大”——它并非替代开发者，而是通过技术工具将开发者从重复编码中解放，转向更核心的业务拆解、架构设计与质量把控。</p><p>AI编程的核心优势在于对标准化场景的效率提升，以我之前的“AI商家工作台看板生成”项目为例，通过结构化Prompt将商家的业务规范（如销量/库存/毛利等核心指标定义）、技术约束（UI组件库规范、接口对接标准）、权限规则等嵌入交互逻辑，AI能快速生成可直接运行的前端代码，将原本1-2天的开发周期压缩至小时级。但这背后离不开两个关键前提：一是精准的Prompt工程，需要将模糊的业务需求转化为AI可理解的技术语言，这要求开发者既懂业务又懂AI的“认知逻辑”；二是对生成结果的校验能力，AI可能存在边界case遗漏、性能优化不足等问题，开发者需凭借技术经验进行兜底，尤其企业级应用中，安全性、规范性与业务贴合度的校验不可或缺。</p><p>从行业发展来看，AI编程正朝着“领域定制化”方向演进。通用型AI编程工具已无法满足企业级场景需求，针对电商采销、供应链管理等垂直领域的定制化AI模型，通过训练行业专属知识库，能大幅提升代码生成的精准度。同时，开发者的角色也在迭代：从单纯的编码者转变为“需求拆解师+Prompt工程师+架构设计师”，需要更强的业务抽象能力与跨领域整合能力——只有将业务逻辑、技术架构与AI工具特性深度融合，才能让AI真正成为业务提效的催化剂。</p><p>归根结底，AI编程的本质是“技术工具对生产力的重构”，但其价值上限始终由开发者的业务理解深度与技术把控能力决定。未来，人机协同的核心将是“人定义价值、AI落地执行”，开发者需聚焦于更具创造性的工作，让AI成为串联需求与实现的高效桥梁，最终实现技术服务于业务增长的核心目标。</p>]]></description></item><item>    <title><![CDATA[2025全球量子计算产业发展展望报告：技术路线、市场规模与应用落地|附200+份报告PDF、数据、可]]></title>    <link>https://segmentfault.com/a/1190000047525072</link>    <guid>https://segmentfault.com/a/1190000047525072</guid>    <pubDate>2026-01-06 19:07:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>原文链接：<a href="https://link.segmentfault.com/?enc=%2BgYqhnlaAiIvokMsNg5M%2Bw%3D%3D.3MfwCGmj%2Fq0s63W5OmSPga4%2B50b64sCkQ6G1JnFX4l8%3D" rel="nofollow" title="https://tecdat.cn/?p=44713" target="_blank">https://tecdat.cn/?p=44713</a>  <br/>原文出处：拓端抖音号@拓端tecdat</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面"/></p><h3><a name="t0" target="_blank"/>引言</h3><p>当谷歌Willow芯片实现量子纠错关键突破，中国“祖冲之三号”刷新超导量子计算性能基准，全球量子计算产业已从“实验室小众探索”迈入中美双极竞速的战略博弈新阶段。20余年技术演进，让量子计算从理论构想成为重塑全球科技版图的核心变量——一个拥有100个量子比特的系统，理论上可并行处理2^100种可能状态，这一特性让它在密码破解、药物研发、材料设计等经典计算“束手无策”的领域具备颠覆性潜力，也让中美欧等主要经济体展开了围绕技术路线、产业生态、标准制定的全方位竞争。  <br/>本报告洞察基于《发布机构：光子盒研究院：2025全球量子计算产业发展展望》和文末<strong>200+份</strong>量子计算与量子技术行业研究报告及数据，系统梳理全球技术路线竞争格局、市场规模增长逻辑、应用场景落地潜力与风险挑战，为创业者、技术决策者、投资者提供可落地的行业洞察。</p><p>本文完整报告数据图表和文末最新参考报告合集已分享在交流群，阅读原文查看、进群咨询，定制数据、报告和800+行业人士共同交流和成长。</p><p>值得警惕的是，中国在量子计算领域虽实现单点突破，但在技术路线收敛性、产业生态完整性、专利布局广度上仍与美国存在显著差距。技术路线分野、成本高企、应用场景模糊等问题，再叠加美国的技术封锁与生态壁垒，正考验着所有中国参与者的战略定力——稍有迟疑，便可能在这场关乎未来算力主权的竞赛中被系统性甩开。</p><h3><a name="t1" target="_blank"/>一、技术路线竞速：中美主导的路线之争，谁能笑到最后？</h3><p>量子计算的技术路线之争从未停歇，超导、离子阱、光量子、中性原子等路线各有优劣，但竞争的核心已从“多路线并存”转向“中美主导的生态卡位”。以下核心指标直观呈现不同路线的性能差异，更揭示了中美企业的实力差距：</p><h4><a name="t2" target="_blank"/>1. 量子比特数量对比</h4><p><strong>规模扩张的核心竞赛，中美已形成第一梯队</strong>  <br/>量子比特数是衡量量子计算机算力的基础指标，直接决定并行处理能力的上限，也是中美企业的核心竞争点。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525075" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院《2025全球量子计算产业发展展望》，包含超导、离子阱、光量子、中性原子四大主流技术路线核心设备量子比特数统计，量子比特数均指物理的、可用于计算的，逻辑的、耦合的不考虑在内。  <br/>量子比特数量对比图1数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：超导路线在比特数上暂时领先，中国“祖冲之三号”与谷歌Willow均突破105比特，但美国QuEra的中性原子路线已实现256比特规模，中国中科酷原“汉原一号”仅达到100+比特，规模化差距明显。  <br/>对应人群行动建议：创业者可优先关注超导路线的商业化机会，依托中国在该路线的单点优势快速落地；技术团队需警惕“比特数陷阱”——单纯追求数量而忽视保真度无实际意义，同时需紧盯美国中性原子路线的规模化进展，避免技术代差扩大。</p><h4><a name="t3" target="_blank"/>2. 量子门保真度对比</h4><p><strong>计算准确性的生命线，美国企业仍占绝对优势</strong>  <br/>保真度直接影响计算结果的可靠性，是量子算法落地的核心前提，美国在高精度操控技术上的积累已形成壁垒。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525076" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K技术监测数据，统计时间为2024年全年，涵盖全球15家主流量子计算硬件厂商核心产品的单比特门与双比特门保真度测试结果。  <br/>量子门保真度对比图2数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线保真度最高，美国Quantinuum的单比特门保真度超99.99%，中国华翊量子HYQ-A37约为99.9%；超导路线中，谷歌Willow双比特门保真度达99.5%，中国“祖冲之三号”约为99.0%，差距虽小但在高精度场景影响显著。  <br/>对应人群行动建议：金融、医药等对计算准确性要求极高的行业，短期可优先布局美国离子阱路线应用；中国企业需加大量子测控技术研发，缩小保真度差距，避免在核心场景被替代。</p><h4><a name="t4" target="_blank"/>3. 量子相干时间对比</h4><p><strong>量子态稳定的关键，中美路线各有优劣但差距明显</strong>  <br/>相干时间决定量子比特能保持量子态的时长，直接影响计算深度，中国在极低温环境维持技术上仍依赖进口设备。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525077" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算硬件性能基准测试平台（2024年度报告），相干时间测试环境为各技术路线标准运行环境（超导10mK、离子阱超高真空等）。  <br/>量子相干时间对比图3数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线相干时间最长（毫秒级），美国IonQ达到10毫秒，中国华翊量子约为2毫秒；超导路线中，谷歌Willow在自主研发的稀释制冷机支持下，相干时间达500微秒，中国“祖冲之三号”约为300微秒，设备依赖导致差距难以快速缩小。  <br/>对应人群行动建议：需要长计算周期的量子模拟场景（如材料研发）可选择美国离子阱路线，短期快速计算任务（如组合优化）中国超导路线更具优势；国内企业需加速稀释制冷机等核心设备的国产替代，从底层突破相干时间瓶颈。</p><h4><a name="t5" target="_blank"/>4. 量子门操作时间对比</h4><p><strong>计算效率的核心保障，中国超导路线具备局部优势</strong>  <br/>操作时间决定量子门执行速度，影响整体计算效率，中国在超导路线的门操作速度上实现局部反超，但应用场景有限。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525078" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院与清华大学量子信息研究中心联合测试数据，操作时间为单量子门平均执行时间，气泡大小对应技术路线成熟度评分。  <br/>量子门操作时间对比图4数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国超导路线操作时间最短（30纳秒），略优于谷歌Willow的40纳秒；但离子阱路线中，美国Quantinuum虽操作时间较长（100微秒），但可通过高保真度弥补效率差距，应用场景更广泛。  <br/>对应人群行动建议：高频计算场景（如实时风控）优先选择中国超导路线，对效率要求不高的科研场景可接受美国离子阱路线的速度trade-off；中国企业需扩大超导路线的应用场景覆盖，将局部优势转化为生态优势。</p><h4><a name="t6" target="_blank"/>技术路线对比表</h4><table><thead><tr><th>核心结论</th><th>数据差异</th><th>原因分析</th></tr></thead><tbody><tr><td>超导路线成当前主流，中美双雄争霸</td><td>美国比特数105-176、保真度99.5%-99.9%；中国比特数105-176、保真度99.0%-99.5%</td><td>美国与半导体工艺兼容性高，生态成熟；中国技术单点突破快，但设备依赖进口</td></tr><tr><td>离子阱路线美国稳扎稳打，中国追赶中</td><td>美国比特数32-100、保真度99.8%-99.99%；中国比特数37-100、保真度99.0%-99.9%</td><td>美国量子比特天然全同技术积累深；中国依托高校研发快速突破，但激光系统仍依赖进口</td></tr><tr><td>中性原子路线美国潜力巨大，中国起步晚</td><td>美国比特数200-256、保真度99.0%-99.5%；中国比特数100+、保真度98.5%-99.0%</td><td>美国规模化扩展成本低，可构建多维阵列；中国测控难度高，技术成熟度不足</td></tr><tr><td>光量子路线中美进展均缓慢，中国略占优</td><td>美国比特数216、保真度98.0%-99.0%；中国比特数255、保真度98.5%-99.0%</td><td>中国“九章三号”实现光子数突破，但光子纠缠操控难度大，退相干快的问题未解决</td></tr></tbody></table><p>不同路线的竞争本质是“中美生态卡位战”——没有完美的技术，只有适配的场景。当前行业共识是：短期内超导路线将主导商业落地，中国可依托该路线实现局部突破；中长期中性原子路线可能成为美国拉开差距的关键，中国需加速技术攻关；离子阱路线则成为美国巩固高精度场景优势的核心，中国需在细分领域建立差异化壁垒。</p><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047525079" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>2025人工智能AI研究报告：算力、应用、风险与就业|附1000+份报告PDF、数据、可视化模板汇总下载</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=Wu18l8G51gy0HdhvU9zVfA%3D%3D.BtNMBYglGHPlbuEbx8X9M9B5LTrJ0oEzbtfOLi0S3e8%3D" rel="nofollow" title="https://tecdat.cn/?p=44642" target="_blank">https://tecdat.cn/?p=44642</a></p><h3><a name="t8" target="_blank"/>二、市场规模与投融资：资本押注的未来，中美差距正在拉大</h3><p>量子计算市场正呈现“指数级增长”态势，但资本布局的结构性差异已凸显中美产业发展的深层差距——美国聚焦生态构建，中国仍停留在单点技术突破。</p><h4><a name="t9" target="_blank"/>1. 全球量子计算市场规模预测</h4><p><strong>从十亿到万亿的跨越，中国占比仍处弱势</strong>  <br/>量子计算产业规模将在2030年后迎来爆发式增长，但中国市场占比提升缓慢，难以撼动美国主导地位。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525080" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院产业预测模型，基于2024年全球50.37亿美元市场规模，结合技术迭代速度、政策支持力度、应用落地进度综合测算。  <br/>全球量子计算市场规模预测图5数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球市场规模50.37亿美元，美国占比62.7%，中国仅25.3%；2030年全球将达2199.78亿美元，中国占比预计提升至27.96%，仍落后美国30个百分点以上；2035年全球突破8000亿美元，中国占比29.49%，差距仍未缩小。  <br/>对应人群行动建议：投资者可重点布局美国上游核心器件（稀释制冷机、量子芯片）和中游整机厂商，同时关注中国国产替代机会；中国企业需加强产业链协同，避免单点作战，依托政策支持构建自主生态。</p><h4><a name="t10" target="_blank"/>2. 全球量子计算产业规模预测</h4><p><strong>中国市场自主化驱动的增长，难掩生态短板</strong>  <br/>中国量子计算市场在自主化突破下快速增长，但产业生态不完整导致增长质量不高。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525081" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K全球区域市场分析报告（2024），中国市场规模统计包含硬件整机、软件算法、云平台及下游应用四大板块。  <br/>全球量子计算产业规模预测图6数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国市场规模占全球25.3%，其中硬件整机占比超60%，软件算法仅占15%；而美国软件算法占比达40%，生态完整性远超中国，产业抗风险能力更强。  <br/>对应人群行动建议：国内创业者可依托政策支持，聚焦上游国产替代机会（如稀释制冷机、测控系统），同时加大软件算法研发投入，补全生态短板；海外企业可寻求与国内科研机构的合作切入点，共享中国硬件增长红利。</p><h4><a name="t11" target="_blank"/>3. 中国量子计算融资规模</h4><p><strong>本土资本的谨慎布局，单笔体量远逊美国</strong>  <br/>中国量子计算融资活跃度位列全球第二，但单笔体量偏小，反映本土资本对生态构建的信心不足。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525082" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为PitchBook 2024量子技术投资报告及光子盒研究院投融资监测数据，融资规模统计包含种子轮、天使轮、A/B/C轮及政府资助。  <br/>中国量子计算融资规模图7数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国融资规模0.47亿美元，交易笔数6笔，单笔平均0.078亿美元；美国融资规模12.60亿美元，交易笔数17笔，单笔平均0.741亿美元，单笔体量是中国的9.5倍。  <br/>对应人群行动建议：国内初创企业需突出技术差异化与国产替代价值争取融资，避免单纯追求比特数突破；政府引导基金可加大对中游整机厂商的长期投入，同时设立专项基金支持软件生态建设，改变“重硬件、轻软件”的融资格局。</p><h4><a name="t12" target="_blank"/>4. 量子计算企业融资额</h4><p><strong>全球融资头部集中效应显著，美国企业垄断核心资源</strong>  <br/>全球融资向技术成熟、具备生态构建能力的企业集中，美国企业占据绝对主导地位，中国企业难获大额融资。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525083" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算投融资数据库（2024），统计范围为全球量子计算硬件、软件、云平台相关企业公开融资事件。  <br/>量子计算企业融资额图8数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球融资20.15亿美元，美国企业占比62.7%，PsiQuantum以6.246亿美元获最大轮融资，Quantinuum、Q-CTRL紧随其后；中国最大单笔融资仅0.1亿美元，且集中在硬件领域，软件企业融资困难。  <br/>对应人群行动建议：中国初创企业需聚焦细分技术痛点（如低温测控、量子纠错）建立壁垒，避免与美国巨头正面竞争；投资者可关注“硬件+软件”一体化布局的中国企业，降低单一环节风险，同时警惕纯硬件企业的技术迭代风险。</p><h4><a name="t13" target="_blank"/>5. 量子技术专利数量</h4><p><strong>知识产权的全球博弈，中国基础专利差距明显</strong>  <br/>专利数量反映国家技术积累，美国仍占据绝对优势，中国在基础专利上的短板可能制约长期发展。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525084" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为Patsnap量子技术专利分析报告（截至2024年12月），统计范围为全球量子计算核心技术相关授权专利。  <br/>量子技术专利数量图9数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：截至2024年，美国量子技术专利18649件，基础专利占比超40%；中国7601件，基础专利仅占15%，多为应用层专利；日本、德国分别以9400件、8500件位列第二、三位，基础专利布局均优于中国。  <br/>对应人群行动建议：国内企业需加强核心技术专利布局，避免陷入“低端专利陷阱”，重点突破量子芯片、量子纠错等基础领域专利；科研机构可聚焦基础理论与核心器件专利突破，提升行业话语权，减少对美国基础专利的依赖。</p><h3><a name="t14" target="_blank"/>三、应用场景落地：从实验室到产业的跨越，中美应用深度差距显著</h3><p>量子计算的终极价值在于产业赋能，当前已在多个领域展现出落地潜力，但美国在应用深度与广度上已形成优势，中国仍处于试点阶段。</p><h4><a name="t15" target="_blank"/>1. 计算加速倍数</h4><p><strong>效率革命的开始，美国应用场景更广泛</strong>  <br/>量子计算在特定场景实现指数级加速，但美国已在多领域形成规模化应用，中国仍以科研试点为主。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525085" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为麦肯锡《The state of AI in 2025: Agents, innovation, and transformation》，加速倍数为量子计算与经典超级计算机在相同任务下的效率对比。  <br/>计算加速倍数图10数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：基因组组装加速1000倍，美国已应用于生物医药企业的新药研发；AI模型训练加速15.2倍，美国谷歌、微软已用于大模型优化；电力故障定位加速1.34倍，中国仅在个别电力企业试点，应用范围有限。  <br/>对应人群行动建议：中国生物医药企业可优先布局基因组组装、药物研发场景，依托“九章三号”光量子计算机的局部优势快速验证价值；能源企业可聚焦电力优化等轻量级应用，逐步探索深度融合，避免盲目跟风美国的大规模应用。</p><h4><a name="t16" target="_blank"/>2. 预测准确率提升</h4><p><strong>决策质量的提升，中国在核心场景应用滞后</strong>  <br/>量子算法显著提升预测准确率，但中国在金融、医药等核心场景的应用滞后美国3-5年。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525086" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为量子前哨《量子技术赋能金融风控与定价管理白皮书》，测试场景包含金融风控、生物制药分子对接等核心应用领域。  <br/>预测准确率提升图11数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：金融风控准确率提升18.5%，美国摩根大通、花旗已用于信贷风控和衍生品定价；生物制药分子对接提升14.3%，美国辉瑞、阿斯利康已纳入药物研发流程；中国仅个别头部企业开展POC测试，尚未规模化应用。  <br/>对应人群行动建议：中国金融机构可先在信贷风控、衍生品定价场景试点，依托量子云平台降低投入成本；医药企业可与量子计算公司合作开展药物分子模拟，快速验证价值，避免在核心场景被美国企业拉开代差。</p><h4><a name="t17" target="_blank"/>3. 后量子密码市场规模</h4><p><strong>应对量子安全威胁，中国PQC迁移进展缓慢</strong>  <br/>量子计算的发展带来密码安全风险，后量子密码（PQC）成为刚需，但中国PQC迁移进展滞后于美国和欧盟。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525087" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为朗空量子《全球抗量子迁移战略白皮书（2025）》，市场规模预测基于全球关键基础设施PQC迁移需求测算。  <br/>后量子密码市场规模图12数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：后量子密码市场规模将随量子计算成熟度同步增长，2030年将突破百亿规模；美国已完成关键基础设施PQC试点，欧盟2030年将全面完成迁移，中国仍处于标准制定阶段，迁移进度滞后2-3年。  <br/>对应人群行动建议：中国政府、金融、电信等关键基础设施行业需加快PQC迁移规划，避免“量子威胁”冲击；企业可先开展密码系统风险评估，依托国内PQC技术企业开展试点，降低对国外算法的依赖。</p><h4><a name="t18" target="_blank"/>4. NIST算法公钥长度比较</h4><p><strong>技术适配的关键，中国算法适配能力不足</strong>  <br/>不同PQC算法公钥长度差异显著，影响设备适配性与传输效率，中国在算法适配与设备兼容上仍落后。  </p><p>注释：数据来源为NIST后量子密码标准化项目（Round 4）测试数据，公钥长度为各算法标准实现的平均长度。  <br/>NIST算法公钥长度比较图13数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：NIST标准化PQC算法中，美国企业已实现全场景适配，支持从物联网设备到金融核心系统的全覆盖；中国仅能适配部分场景，短公钥长度算法在物联网设备的兼容性不足，长公钥算法在金融系统的传输效率问题未解决。  <br/>对应人群行动建议：中国物联网企业可优先选择短公钥长度算法（如SLH-DSA），与国内PQC企业联合优化兼容性；金融机构等核心场景可选用高安全性算法（如CRYSTALS-Kyber），同时加大传输效率优化投入，平衡安全与效率。</p><h4><a name="t19" target="_blank"/>5. 迁移政策时间线</h4><p><strong>全球PQC迁移协同推进，中国需加快节奏</strong>  <br/>各国明确PQC迁移时间表，形成全球协同防控量子安全风险的格局，中国迁移目标虽与美国一致，但执行力度需加强。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525088" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为信通院《量子信息领域的国家战略布局与研发趋势分析》，迁移目标完成年份为各国官方发布的量子安全相关政策明确时间。  <br/>迁移政策时间线图14数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：美国、中国计划2035年完成PQC迁移，欧盟提前至2030年；美国已出台分阶段迁移细则，明确各行业责任主体；中国虽明确目标，但缺乏具体执行方案，关键基础设施行业的迁移动力不足。  <br/>对应人群行动建议：中国跨国企业需按区域政策要求制定分阶段迁移计划，避免因合规问题影响海外业务；国内企业可依托国家级专项推进自主PQC技术落地，积极参与国际标准制定，提升话语权。</p><h3><a name="t20" target="_blank"/>四、风险提示与应对方案</h3><p>量子计算产业看似前景光明，但暗藏多重“陷阱”，中国企业需比美国更谨慎应对，避免在技术迭代与生态竞争中被淘汰：</p><h4><a name="t21" target="_blank"/>1. 技术路线收敛风险</h4><p><strong>风险描述</strong>：当前技术路线未收敛，美国已形成“超导+中性原子”双主线布局，中国企业多押注单一路线，可能面临“满盘皆输”的风险，如早期光量子路线企业因技术瓶颈陷入发展困境。  <br/><strong>具体应对方案</strong>：采用“主线+支线”布局策略，核心业务聚焦主流路线（如超导），同时小比例投入潜力路线（如中性原子）；加入行业联盟，实时跟踪美国技术路线演进动态，每半年评估一次路线优先级，避免与美国主流路线偏离。  <br/><strong>社群支持</strong>：交流群定期分享美国技术路线进展报告，组织技术选型闭门会，邀请光子盒研究院专家提供定制化建议，对接上下游企业资源，帮助中国企业快速调整技术方向。</p><h4><a name="t22" target="_blank"/>2. 成本高企风险</h4><p><strong>风险描述</strong>：量子计算机硬件成本动辄数亿元，运维成本（如极低温环境）高昂，美国企业可通过生态协同分摊成本，中国中小企业难以承受，单台超导量子计算机年运维成本超千万元，远超美国企业的600万元。  <br/><strong>具体应对方案</strong>：优先采用量子云平台（如中电信“天衍”平台）按需付费，避免重资产投入；聚焦细分场景的轻量化应用，降低算力需求，控制初期投入规模；联合高校、科研机构共享设备资源，分摊运维成本。  <br/><strong>社群支持</strong>：整理全球量子云平台对比手册，重点标注中美平台成本差异，提供中国平台优惠资源对接，组织中小企业量子计算应用试点对接会，帮助企业降低试点成本。</p><h4><a name="t23" target="_blank"/>3. 政策合规风险</h4><p><strong>风险描述</strong>：量子技术涉及国家安全，各国政策限制（如禁运、出口管制）日益严格，欧美已将稀释制冷机、量子芯片等纳入禁运清单，中国企业核心器件进口难度加大，技术迭代受阻。  <br/><strong>具体应对方案</strong>：国内企业加强自主化研发，重点突破稀释制冷机、测控系统等“卡脖子”环节，减少核心器件进口依赖；建立供应链风险预警机制，提前储备替代资源；避免违规合作，优先选择政策友好区域布局海外业务。  <br/><strong>社群支持</strong>：及时更新全球量子技术政策数据库，重点标注美国对华技术封锁清单，提供合规咨询对接服务，组织政策解读直播，帮助企业把握政策导向与机遇，规避合规风险。</p><h3><a name="t24" target="_blank"/>五、可落地的3件事</h3><ol><li>开展“量子就绪”评估：梳理企业核心业务中的计算瓶颈，对比中美应用场景差异，判断是否适合量子计算赋能，优先选择美国已验证、中国有技术基础的场景（如金融风控、药物研发），形成《量子应用潜力评估报告》，避免盲目跟风。</li><li>小步试点验证价值：与国内量子云平台合作开展POC（概念验证），投入少量资源测试量子算法效果，比如金融企业可试点量子组合优化算法优化投资组合，医药企业可测试量子模拟加速药物分子筛选，重点验证国产技术的可行性，避免过度依赖美国平台。</li><li>储备量子人才与专利：招聘具备量子计算基础的技术人员，或对现有团队开展量子技术培训（如参加量旋科技“量子计算实训营”），建立人才护城河；同时对接高校量子信息专业，搭建校企人才输送通道，加强核心技术专利布局，尤其是基础专利，减少对美国专利的依赖。</li></ol><h3><a name="t25" target="_blank"/>六、核心数据表格</h3><h4><a name="t26" target="_blank"/>1. 主要技术路线核心性能指标表</h4><table><thead><tr><th>技术路线</th><th>量子比特数</th><th>单比特门保真度</th><th>相干时间</th><th>单量子门操作时间</th><th>代表企业/设备（美国）</th><th>代表企业/设备（中国）</th></tr></thead><tbody><tr><td>超导</td><td>105-176</td><td>99.5%-99.9%</td><td>400-500微秒</td><td>40纳秒</td><td>谷歌Willow</td><td>中国“祖冲之三号”</td></tr><tr><td>离子阱</td><td>32-100</td><td>99.8%-99.99%</td><td>5-10毫秒</td><td>100微秒</td><td>Quantinuum H2-1、IonQ Forte</td><td>华翊量子HYQ-A37、幺正量子UQM1</td></tr><tr><td>光量子</td><td>216</td><td>98.0%-99.0%</td><td>50-100微秒</td><td>10微秒</td><td>Xanadu Borealis</td><td>中国“九章三号”</td></tr><tr><td>中性原子</td><td>200-256</td><td>99.0%-99.5%</td><td>800微秒-1秒</td><td>1微秒</td><td>QuEra Aquila</td><td>中科酷原“汉原一号”</td></tr></tbody></table><h4><a name="t27" target="_blank"/>2. 全球量子计算市场规模预测表（单位：亿美元）</h4><table><thead><tr><th>年份</th><th>全球市场规模</th><th>美国市场规模</th><th>中国市场规模</th><th>美国占比</th><th>中国占比</th><th>中国年复合增长率</th></tr></thead><tbody><tr><td>2024</td><td>50.37</td><td>31.58</td><td>12.74</td><td>62.70%</td><td>25.30%</td><td>-</td></tr><tr><td>2027</td><td>111.75</td><td>69.90</td><td>30.00</td><td>62.55%</td><td>26.84%</td><td>29.5%</td></tr><tr><td>2030</td><td>2199.78</td><td>1389.26</td><td>615.00</td><td>63.16%</td><td>27.96%</td><td>173.2%</td></tr><tr><td>2035</td><td>8077.50</td><td>5153.00</td><td>2382.00</td><td>63.80%</td><td>29.49%</td><td>29.8%</td></tr></tbody></table><h4><a name="t28" target="_blank"/>3. 量子计算应用场景价值表</h4><table><thead><tr><th>应用场景</th><th>加速倍数</th><th>准确率提升</th><th>落地周期</th><th>美国落地状态</th><th>中国落地状态</th><th>产业估值（2035年，亿美元）</th></tr></thead><tbody><tr><td>基因组组装</td><td>1000倍</td><td>-</td><td>3-5年</td><td>规模化应用</td><td>科研试点</td><td>-</td></tr><tr><td>AI模型训练</td><td>15.2倍</td><td>-</td><td>5-8年</td><td>企业试点</td><td>实验室阶段</td><td>-</td></tr><tr><td>电力故障定位</td><td>1.34倍</td><td>-</td><td>2-3年</td><td>行业应用</td><td>个别试点</td><td>-</td></tr><tr><td>金融风控</td><td>-</td><td>18.5%</td><td>3-5年</td><td>规模化应用</td><td>POC测试</td><td>7000（乐观估值）</td></tr><tr><td>生物制药分子对接</td><td>-</td><td>14.3%</td><td>5-8年</td><td>企业应用</td><td>科研合作</td><td>1830（乐观估值）</td></tr></tbody></table><h4><a name="t29" target="_blank"/>4. 主要经济体PQC迁移政策表</h4><table><thead><tr><th>经济体</th><th>迁移目标完成年份</th><th>核心要求</th><th>重点领域</th><th>执行进度</th></tr></thead><tbody><tr><td>美国</td><td>2035年</td><td>禁用传统密码算法，强制采用NIST标准化PQC算法</td><td>国防、金融、电信</td><td>分阶段执行中</td></tr><tr><td>中国</td><td>2035年</td><td>自主PQC技术落地，关键基础设施率先完成迁移</td><td>金融、能源、政务</td><td>标准制定阶段</td></tr><tr><td>欧盟</td><td>2030年</td><td>关键基础设施完成PQC升级，建立跨境互认机制</td><td>能源、交通、医疗</td><td>全面推进中</td></tr></tbody></table><h3><a name="t30" target="_blank"/>七、数据图表列表</h3><ol><li>量子比特数量对比图1.pdf</li><li>量子门保真度对比图2.pdf</li><li>量子相干时间对比图3.pdf</li><li>量子门操作时间对比图4.pdf</li><li>全球量子计算市场规模预测图5.pdf</li><li>中国量子计算融资规模图6.pdf</li><li>全球量子计算产业规模预测图7.pdf</li><li>量子计算企业融资额图8.pdf</li><li>量子技术专利数量图9.pdf</li><li>计算加速倍数图10.pdf</li><li>预测准确率提升图11.pdf</li><li>后量子密码市场规模图12.pdf</li><li>NIST算法公钥长度比较图13.pdf</li><li>迁移政策时间线图14.pdf</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t31" target="_blank"/>本专题内的参考报告（PDF）目录</h3><p>⦁    PQC-X实验室：全球金融银行业后量子安全迁移白皮书（2025）.pdf  <br/>⦁    2026-01-05 20:34  <br/>⦁    2025年量子技术：健康与医疗保健领导者的战略要务报告.pdf  <br/>⦁    2026-01-03 10:50  <br/>⦁    朗空量子：全球抗量子迁移战略白皮书（2025）.pdf  <br/>⦁    2025-12-31 15:52  <br/>⦁    2025年全球量子生态全景洞察报告：基于创新、企业、投资、技能、贸易及政策数据的综合研究（英文版）.pdf  <br/>⦁    2025-12-29 15:59  <br/>⦁    量子计算行业深度：行业概况、发展趋势、产业链及相关公司深度梳理.pdf  <br/>⦁    2025-12-29 15:53  <br/>⦁    软件与服务行业量子信息技术专题研究报告（二）：科技巨头加速布局，量子产业前景可期.pdf  <br/>⦁    2025-12-24 15:30  <br/>⦁    量子位；2025年度AI十大趋势报告.pdf  <br/>⦁    2025-12-17 16:16  <br/>⦁    量子信息技术发展与应用研究报告（2025年）-中国信通院.pdf  <br/>⦁    2025-12-16 16:29  <br/>⦁    长江证券：量子计算：从“量子优越性”到产业优越性.pdf  <br/>⦁    2025-12-04 16:45  <br/>⦁    ATARC：2025年揭秘当今与未来量子技术能力白皮书汇编（英文版）.pdf  <br/>⦁    2025-11-30 09:17  <br/>⦁    量子科技行业深度报告：量子科技驱动产业变革，激活经济增长新引擎.pdf  <br/>⦁    2025-11-30 09:11  <br/>⦁    2025版量子计算 生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-11-24 15:06  <br/>⦁    2025量子计算+生物制药产业与技术发展研究报告.pdf  <br/>⦁    2025-11-19 15:28  <br/>⦁    2025量子技术：先进制造和供应链的关键机遇白皮书（英文版）.pdf  <br/>⦁    2025-11-08 17:47  <br/>⦁    量子科技行业深度报告：量子革命，量子科技的现状与未来.pdf  <br/>⦁    2025-10-30 15:17  <br/>⦁    2025年量子计算驱动的电力系统弹性提升-探索与展望报告.pdf  <br/>⦁    2025-10-29 16:27  <br/>⦁    海通国际：量子科技行业深度报告：量子革命：量子科技的现状与未来.pdf  <br/>⦁    2025-10-28 16:17  <br/>⦁    电子行业深度报告：量子深潜-计算篇：从比特到Qubit的范式转移.pdf  <br/>⦁    2025-10-26 08:49  <br/>⦁    量子计算硬件深度报告：行业奇点将至，硬件破局当时.pdf  <br/>⦁    2025-10-16 15:09  <br/>⦁    中航证券：量子信息：引领未来全球科技变革之关键力量.pdf  <br/>⦁    2025-10-15 15:17  <br/>⦁    中移智库：移动网络中量子计算应用能力评估模型（2025年）.pdf  <br/>⦁    2025-09-30 16:39  <br/>⦁    中国信通院：量子计算发展态势研究报告（2025年）.pdf  <br/>⦁    2025-09-26 14:27  <br/>⦁    2025量子信息行业研究报告.pdf  <br/>⦁    2025-09-21 17:17  <br/>⦁    计算机行业深度研究：后量子密码技术：应对量子计算威胁的关键防线.pdf  <br/>⦁    2025-09-17 16:27  <br/>⦁    后量子密码学（PQC）测试研究白皮书.pdf  <br/>⦁    2025-09-12 16:36  <br/>⦁    麻省理工学院：2025年量子指数报告（英文版）.pdf  <br/>⦁    2025-09-03 16:55  <br/>⦁    量子计算专题：下一代计算革命，关注核心设备环节.pdf  <br/>⦁    2025-09-01 16:24  <br/>⦁    AI Coding玩家图谱【量子位智库】.pdf  <br/>⦁    2025-08-31 17:47  <br/>⦁    未来网络发展大会：2025量子互联网与算网协同体系架构白皮书.pdf  <br/>⦁    2025-08-23 17:17  <br/>⦁    2025中国量子计算产业市场现状及发展前景研究报告.pdf  <br/>⦁    2025-08-16 16:49  <br/>⦁    2025年全球科技行业：量子计算将如何影响AI发展？（英文版）.pdf  <br/>⦁    2025-08-05 15:31  <br/>⦁    量子位智库：2025上半年AI核心成果及趋势报告.pdf  <br/>⦁    2025-08-02 16:21  <br/>⦁    2025年全球量子计算新进展深度分析报告.pdf  <br/>⦁    2025-08-02 16:16  <br/>⦁    2025年中国联通后量子密码白皮书-中国联通.pdf  <br/>⦁    2025-07-30 16:14  <br/>⦁    量子位智库：2025年AI+游戏产业变革研究报告.pdf  <br/>⦁    2025-07-17 15:53  <br/>⦁    麦肯锡：量子之年：从2025年从概念到现实报告（英文版）.pdf  <br/>⦁    2025-07-13 08:36  <br/>⦁    后量子密码技术白皮书（2025）-东进技术.pdf  <br/>⦁    2025-07-07 16:52  <br/>⦁    2024年量子技术在金融通信安全领域的应用研究报告.pdf  <br/>⦁    2025-07-02 16:38  <br/>⦁    浙商证券-量子科技行业深度报告：超越经典，面向未来.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    政策与战略专题报告：量子科技：产业革命核心赛道，投资风口将至.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    2024年量子传感在位置、导航和定时应用中的案例（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    2024年量子技术在金融消息传递中的应用报告（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    应对量子威胁：SIM体系抗量子密码迁移白皮书（2025年）.pdf  <br/>⦁    2025-06-19 16:02  <br/>⦁    量子算法在金融风控与定价管理领域的应用研究.pdf  <br/>⦁    2025-06-18 15:27  <br/>⦁    2024年量子计算在交通运输与物流领域的应用研究报告（英文版）.pdf  <br/>⦁    2025-06-12 15:35  <br/>⦁    通信行业动态报告：量子计算光量子技术路线进展加速，未来大有可为.pdf  <br/>⦁    2025-06-12 15:34  <br/>⦁    量子位智库：2025大模型架构创新研究报告.pdf  <br/>⦁    2025-06-06 15:38  <br/>⦁    量子位智库：2025年AI眼镜「预选赛」格局报告.pdf  <br/>⦁    2025-06-05 16:09  <br/>⦁    鼎帷咨询：2025年美国量子技术发展研究报告.pdf  <br/>⦁    2025-06-02 08:57  <br/>⦁    2025年全球量子计算用同轴电缆市场分析报告-光子盒研究院.pdf  <br/>⦁    2025-05-26 16:58  <br/>⦁    量子位智库：2025年AI智能助手的SEO策略变革研究报告.pdf  <br/>⦁    2025-05-24 16:38  <br/>⦁    2025美韩科技合作报告：电池、生物技术与量子技术（英文）.pdf  <br/>⦁    2025-05-20 17:05  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年营养健康食品专题报告.pdf  <br/>⦁    2025-05-17 16:13  <br/>⦁    2025年量子技术与未来学习研究报告（英文版）.pdf  <br/>⦁    2025-05-14 16:34  <br/>⦁    Globant：2024年科技趋势报告-人工智能、量子技术、机器人等将如何塑造未来一年（英文版）.pdf  <br/>⦁    2025-05-01 17:54  <br/>⦁    量子计算：打破维度瓶颈，开启化学的“算力革命”.pdf  <br/>⦁    2025-04-29 15:55  <br/>⦁    量子位智库：2025年空间智能研究报告.pdf  <br/>⦁    2025-04-28 17:23  <br/>⦁    量子信息网络产业联盟：2025年光量子计算技术产业研究报告.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    量子信息网络产业联盟：量子计算云平台接口研究报告（2024）.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    2025年量子密钥无线分发技术研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年量子计算应用能力指标与测评研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年经典计算与多制式量子计算异构融合研究报告.pdf  <br/>⦁    2025-04-26 14:27  <br/>⦁    量子信息技术应用案例集（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    量子信息技术产业发展研究报告（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    2025版量子计算+生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-04-21 10:06  <br/>⦁    量子位智库：2025年中国AIGC应用全景图谱报告..pdf  <br/>⦁    2025-04-19 14:49  <br/>⦁    量子计算行业深度：市场现状、发展趋势、产业链及相关企业深度梳理.pdf  <br/>⦁    2025-03-26 15:33  <br/>⦁    2025年全球量子技术专利态势分析白皮书（英文版）.pdf  <br/>⦁    2025-03-13 17:11  <br/>⦁    光子盒：2025年全球量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-12 15:49  <br/>⦁    光子盒：2025年量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-05 15:24  <br/>⦁    中国在量子领域有多大创新性？.pdf  <br/>⦁    2025-03-04 16:09  <br/>⦁    光子盒：2025年全球量子传感产业发展展望报告.pdf  <br/>⦁    2025-03-01 16:55  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:38  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:37  <br/>⦁    光子盒：2025年全球量子计算产业发展展望报告.pdf  <br/>⦁    2025-02-27 14:57  <br/>⦁    2024年量子安全威胁及其对国内金融行业的影响研究报告.pdf  <br/>⦁    2025-02-18 15:53  <br/>⦁    2025年拥抱量子经济：企业领袖的前进之路洞察报告（英文版）.pdf  <br/>⦁    2025-01-22 16:12  <br/>⦁    量子位智库：智能驾驶2024年度报告.pdf  <br/>⦁    2025-01-17 13:14  <br/>⦁    量子信息技术国内外标准化进展报告（2024）.pdf  <br/>⦁    2025-01-15 15:49  <br/>⦁    ITIF：2023年美国的量子政策方针研究报告（英文版）.pdf  <br/>⦁    2025-01-13 10:17  <br/>⦁    2024年量子计算性能评估基准研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    2024年基于量子安全的分布式容错云存储应用场景研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    量子信息技术发展与应用研究报告（2024年）.pdf  <br/>⦁    2024-12-28 16:56  <br/>⦁    移动网络中量子计算应用能力评测白皮书1.0（2024 年）.pdf  <br/>⦁    2024-12-26 15:46  <br/>⦁    2024年量子技术研究报告：投资于拐点（英文版）.pdf  <br/>⦁    2024-12-24 17:14  <br/>⦁    2024年度AI十大趋势报告-量子位.pdf  <br/>⦁    2024-12-14 15:10  <br/>⦁    量子安全技术蓝皮书2024.pdf  <br/>⦁    2024-12-09 16:56  <br/>⦁    量子位智库：2024年大模型落地与前沿趋势研究报告.pdf  <br/>⦁    2024-12-08 16:33  <br/>⦁    2024年量子计算与人工智能：无声的革命报告.pdf  <br/>⦁    2024-12-01 20:55  <br/>⦁    量子位智库：Robotaxi2024年度格局报告.pdf  <br/>⦁    2024-11-30 20:19  <br/>⦁    2023全球量子政策研究报告-光子盒.pdf  <br/>⦁    2024-11-12 16:46  <br/>⦁    量子技术助力社会_实现可持续发展目标.pdf  <br/>⦁    2024-10-19 16:36  <br/>⦁    世界经济论坛：2024年量子技术助力社会：实现可持续发展目标报告（英文版）.pdf  <br/>⦁    2024-10-19 16:30  <br/>⦁    2024中国量子计算应用潜力洞察报告.pdf  <br/>⦁    2024-10-10 15:21  <br/>⦁    2024年AI大模型创业格局报告-量子位智库.pdf  <br/>⦁    2024-10-06 15:17  <br/>⦁    AI教育硬件全景报告【量子位智库】.pdf  <br/>⦁    2024-09-30 15:14  <br/>⦁    量子计算发展态势研究报告（2024年）-中国信通院.pdf  <br/>⦁    2024-09-27 15:55  <br/>⦁    三未信安：抗量子密码技术与应用白皮书（2024）.pdf  <br/>⦁    2024-09-15 15:20  <br/>⦁    光子盒：2024上半年全球量子计算产业发展展望报告.pdf  <br/>⦁    2024-09-14 16:42  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年市场白皮书.pdf  <br/>⦁    2024-09-14 16:39  <br/>⦁    欧洲专利局：2023年量子计算洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:21  <br/>⦁    欧洲专利局：2023年量子模拟洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:20  <br/>⦁    2024量子计算技术全景报告-星河智源.pdf  <br/>⦁    2024-09-05 16:36  <br/>⦁    利亚德&amp;赛富乐斯半导体：2024年T003-量子点（QD-mLED）直显解决方案白皮书.pdf  <br/>⦁    2024-08-31 17:26  <br/>⦁    甲子大脑全球首发：以量子人工智能重新定义智库.pdf  <br/>⦁    2024-08-30 17:46  <br/>⦁    麦肯锡数字量子技术监测.pdf  <br/>⦁    2024-08-27 16:28  <br/>⦁    iCV TA&amp;K：2024年全球量子独角兽企业发展概览报告（英文版）.pdf  <br/>⦁    2024-08-27 16:18  <br/>⦁    光子盒：2024全球量子产业发展现状及展望报告.pdf  <br/>⦁    2024-08-18 17:30  <br/>⦁    尺度定律科普报告【量子位智库】 .pdf  <br/>⦁    2024-08-04 20:05  <br/>⦁    AI视频生成研究报告（2024年）-量子位.pdf  <br/>⦁    2024-07-30 16:30  <br/>⦁    2024中国具身智能创投报告-量子位智库.pdf  <br/>⦁    2024-07-27 17:08  <br/>⦁    AI音乐应用产业报告【量子位智库】.pdf  <br/>⦁    2024-07-22 16:40  <br/>⦁    计算机行业量子科技：见微知著、革故鼎新-国投证券.pdf  <br/>⦁    2024-07-17 10:39  <br/>⦁    光子盒：2024争夺量子优势的芬兰-国家量子战略的政策建议报告.pdf  <br/>⦁    2024-07-10 11:25  <br/>⦁    数据创新中心：2023美国的量子政策报告（英文版）.pdf  <br/>⦁    2024-07-04 11:00  <br/>⦁    赛迪报告：电子信息研究2024年第1期（总第95期）《量子产业发展白皮书》.pdf  <br/>⦁    2024-07-01 09:31  <br/>⦁    ...】2023年中国中老年市场白皮书-中老年服务及产品 “人-货-场”三维解析-CIC灼识咨询&amp;量子之歌.pdf  <br/>⦁    2024-06-28 10:40  <br/>⦁    头豹研究院-企业竞争图谱：2024年量子计算 头豹词条报告系列.pdf  <br/>⦁    2024-06-28 10:39  <br/>⦁    后量子密码迁移白皮书（2024）-西电广研院&amp;LRINF-.pdf  <br/>⦁    2024-06-27 11:20  <br/>⦁    国信证券-海外铜企专题3-第一量子-FM.TO-：高成长性的铜矿公司.pdf  <br/>⦁    2024-06-18 12:51  <br/>⦁    光子盒-量子准备：向后量子密码迁移.pdf  <br/>⦁    2024-06-15 11:02  <br/>⦁    2024上海量子科技产业发展白皮书.pdf  <br/>⦁    2024-06-08 13:03  <br/>⦁    后量子密码应用研究报告（2023年) .pdf  <br/>⦁    2024-06-07 10:11  <br/>⦁    “十五五”时期我国量子产业发展形势研判及思路建议.pdf  <br/>⦁    2024-06-05 10:15  <br/>⦁    西南证券-量子科技专题：量子应用逐步落地，关注政策支持.pdf  <br/>⦁    2024-05-31 14:53  <br/>⦁    量子科技专题系列一：逐梦量子，星辰大海.pdf  <br/>⦁    2024-05-13 13:31  <br/>⦁    通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-05-05 17:55  <br/>⦁    解读新质生产力：量子计算：打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-05-05 17:54  <br/>⦁    计算机：量子加密，一片新蓝海.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    计算机行业深度研究：抢先布局量子信息技术革命.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    2024全球6G技术大会-面向6G时代前沿技术初探：量子信息技术-英文.pdf  <br/>⦁    2024-05-01 11:47  <br/>⦁    面向6G时代前沿技术初探：量子信息技术2024白皮书-29页.pdf  <br/>⦁    2024-05-01 11:44  <br/>⦁    2024年面向6G时代前沿技术初探量子信息技白皮书-全球6G技术大会.pdf  <br/>⦁    2024-04-30 14:38  <br/>⦁    2024量子加密，一片新蓝海.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    2024解读新质生产力：量子计算，打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    中国银河-通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-04-29 12:42  <br/>⦁    抢先布局量子信息技术革命.PDF  <br/>⦁    2024-04-27 10:25  <br/>⦁    量子位：2024中国AIGC应用全景报告.pdf  <br/>⦁    2024-04-26 11:21  <br/>⦁    华鑫证券-量子信息技术行业专题报告：优化运算法则，重塑安全格局.pdf  <br/>⦁    2024-04-20 12:06  <br/>⦁    量子化学方法的开发及其在能源环境材料研究中的应用-赵焱.pdf  <br/>⦁    2024-04-17 10:11  <br/>⦁    量子通信金融应用研究报告.pdf  <br/>⦁    2024-04-11 10:53  <br/>⦁    计算机行业深度报告：量子信息：下一场信息革命.pdf  <br/>⦁    2024-04-07 10:10  <br/>⦁    计算机行业深度研究-量子计算-人工智能与新质生产力的“未来引擎”-民生证券.pdf  <br/>⦁    2024-03-25 14:44  <br/>⦁    计算机行业深度研究：量子计算：人工智能与新质生产力的“未来引擎”.pdf  <br/>⦁    2024-03-24 10:43  <br/>⦁    量子精密测量行业赋能白皮书.pdf  <br/>⦁    2024-03-18 11:08  <br/>⦁    2024量子精密测量产业发展展望.pdf  <br/>⦁    2024-03-06 14:33  <br/>⦁    2024全球量子通信与安全产业发展展望报告-光子盒.pdf  <br/>⦁    2024-02-29 15:40  <br/>⦁    量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-28 11:37  <br/>⦁    北京金融科技产业联盟：2024量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-26 16:13  <br/>⦁    2024全球量子计算产业发展展望.pdf  <br/>⦁    2024-02-22 11:00  <br/>⦁    量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子信息技术产业发展报告（2023年）.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    AI制药深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    存算一体芯片深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    隐私计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    计算生物深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    脑机接口深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    虚拟人深度产业报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    类脑计算神经拟态计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    卫星互联网深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    2021十大前沿科技趋势报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    量子信息网络产业联盟：2024量子人工智能技术白皮书.pdf  <br/>⦁    2024-02-07 15:02  <br/>⦁    量子信息网络产业联盟：2024量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-06 15:05  <br/>⦁    量子信息网络产业联盟：量子信息技术应用案例集（2023年）.pdf  <br/>⦁    2024-02-06 15:04  <br/>⦁    量子信息网络产业联盟：2024量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-05 16:04  <br/>⦁    2023年ARMR技术深度产业报告-量子位智库.pdf  <br/>⦁    2024-01-26 15:18  <br/>⦁    量子位：2024中国AIGC广告营销产业全景报告.pdf  <br/>⦁    2024-01-25 15:09  <br/>⦁    量子十年-2024量子计算未来趋势展望报告第四版-英文版-IBM商业价值研究院.pdf  <br/>⦁    2024-01-24 14:34  <br/>⦁    中国信通院：量子计算发展态势研究报告（2023年）.pdf  <br/>⦁    2024-01-02 14:36  <br/>⦁    量子测量技术发展蓝皮书.pdf  <br/>⦁    2023-12-30 10:11  <br/>⦁    中国信通院：量子信息技术发展与应用研究报告（2023年）.pdf  <br/>⦁    2023-12-29 14:43  <br/>⦁    欧洲量子技术关键绩效指标（2023年9月）（英文版）.pdf  <br/>⦁    2023-12-23 09:52  <br/>⦁    量子位：2023中国AIGC数据标注产业全景报告.pdf  <br/>⦁    2023-12-20 15:12  <br/>⦁    欧洲量子技术关键绩效指标（2023 年 9 月）-英.pdf  <br/>⦁    2023-12-16 15:12  <br/>⦁    赛迪前瞻：应对量子计算挑战需积极推进后量子密码研发和迁移.pdf  <br/>⦁    2023-12-04 15:21  <br/>⦁    应对量子计算挑战需积极推进后量子密码研发和迁移2023-赛迪前瞻.pdf  <br/>⦁    2023-11-24 07:24  <br/>⦁    QIIA：量子计算金融应用白皮书.pdf  <br/>⦁    2023-11-22 18:56  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-21 14:33  <br/>⦁    2023年中国AIGC产业全景报告-量子位.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    2023西班牙量子产业报告英文-Ametic.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    量子信息技术标准化图景(2022)- QIIA.pdf  <br/>⦁    2023-11-10 16:02  <br/>⦁    量子位：2023年中国AIGC产业全景报告.pdf  <br/>⦁    2023-11-10 09:55  <br/>⦁    Y2Q2023量子安全加密之旅报告英文-凯捷.pdf  <br/>⦁    2023-11-09 10:16  <br/>⦁    量子计算金融应用白皮书-QIIA.pdf  <br/>⦁    2023-11-09 10:13  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 16:01  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 11:26  <br/>⦁    Capgemini-Y2Q：量子安全密码之旅【英文版】-2023.pdf  <br/>⦁    2023-11-02 10:25  <br/>⦁    量子计算概念、现状和国会考虑（英）.pdf  <br/>⦁    2023-09-16 09:58  <br/>⦁    中国仿生机器人产业全景报告-量子位智库.pdf  <br/>⦁    2023-08-16 21:47  <br/>⦁    十大AI商业落地趋势-量子位智库.pdf  <br/>⦁    2023-08-16 07:15  <br/>⦁    量子位智库：十大AI商业落地趋势.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    量子位智库：中国仿生机器人产业全景报告.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    2023 AIGC算力全景与趋势报告-量子位.pdf  <br/>⦁    2023-07-26 06:37  <br/>⦁    ChatGPT 实用指南（精编版）（2023）-量子论.pdf  <br/>⦁    2023-04-17 14:37  <br/>⦁    2023全球量子精密测量产业发展展望-量子盒.pdf  <br/>⦁    2023-04-03 10:27  <br/>⦁    2023全球量子通信与安全产业发展展望-光子盒.pdf  <br/>⦁    2023-03-13 17:26  <br/>⦁    2023全球量子精密测量产业发展展望（中）-103页.pdf  <br/>⦁    2023-03-10 09:16  <br/>⦁    AIGC深度产业报告 量子位智库-34页.pdf  <br/>⦁    2023-03-09 10:46  <br/>⦁    量子位2022十大前沿科技报告.pdf  <br/>⦁    2023-03-08 09:52</p>]]></description></item><item>    <title><![CDATA[活动回顾：Arm 龙蜥齐携手，共筑 AI 时代开源 OS 新生态 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525104</link>    <guid>https://segmentfault.com/a/1190000047525104</guid>    <pubDate>2026-01-06 19:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>12 月 11 日下午，在一个温暖和煦的冬日，“龙蜥社区走进系列”之走进 Arm Meetup 在上海成功举办。本次活动吸引了来自云计算、互联网、半导体等领域的众多专家学者齐聚一堂，围绕 Arm 生态、开源社区、AI 基础设施及大模型推理等领域的技术突破，共同探讨了开源操作系统与 Arm Neoverse 平台在人工智能（AI）时代的深度融合与创新实践。</p><p>现场通过一系列主题分享，集中展示了 Arm Neoverse 平台、AI 性能分析工具、异构推理框架及优化等技术成果。这些成果不仅体现了产业链上下游协同创新与开源共建的精神，也加速了 AI 与云计算在 Arm 架构上的落地，为开源操作系统行业提供了更高效、可靠的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525106" alt="图片" title="图片"/><br/>（图/活动现场嘉宾合影）</p><p>活动伊始，阿里云智能集团编译器技术总监、Java 标准委员会委员(JCP-EC)，Java Champion、龙蜥社区 Java 语言与虚拟机 SIG Maintainer 李三红做开场致辞。发言聚焦阿里云在倚天 Arm 架构上的深耕，首先以全栈自研为核心，从芯片、操作系统到编译器全面优化，提升云原生场景的性能与性价比。其次重申对开源的长期投入，积极参与龙蜥社区建设，持续贡献稳定高效的操作系统能力。最后强调通过社区、理事单位和生态伙伴的合作，携手 Arm 共建完整生态，共同推动企业更好地使用 Arm 技术。与此同时，他也鼓励大家在活动中积极交流，共促行业发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525107" alt="图片" title="图片" loading="lazy"/><br/>（图/李三红）</p><p>接下来，来自阿里云、龙蜥社区、趋境科技、鸿钧微电子以及安谋科技的技术和市场专家针对基于 Arm Neoverse 平台，围绕开源龙蜥操作系统 Anolis OS 的各个层面在 AI 浪潮中的变革和演进，进行了精彩的分享与思想碰撞。</p><p>首先由安谋科技云人工智能事业部总监侯科鑫和阿里云智能集团弹性计算高级架构师张先国带来了题为《Arm 基础设施加速云计算智能驾驶》的联合演讲。</p><p>侯科鑫女士回顾了 AI 浪潮中若干行业趋势，强调 AI 的快速发展正在推动基础设施技术的重大转型，Arm 参与其中并重新定义计算。Arm 帮助合作伙伴在通用计算以及智能计算定制化平台上取得了不菲的成绩。Arm Neoverse 已成为这一转型中多个关键领域的首选平台。演讲深入介绍了 Arm Neoverse CSS，以及 Arm 的技术创新是如何帮助合作伙伴在 AI 时代加速产品上市。此外 Arm 在软件生态系统上也持续投入，通过与阿里云等合作伙伴的深度协作，Arm 平台不断推动 AI 基础设施创新，满足智能驾驶行业对高可靠性、弹性扩展和绿色算力的迫切需求，助力企业快速应对 AI 时代的挑战与机遇。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525108" alt="图片" title="图片" loading="lazy"/><br/>（图/侯科鑫）</p><p>张先国则针对智能驾驶场景，重点展示了 Arm 基础设施在智能驾驶场景中的云计算加速能力，围绕智能驾驶全流程——从数据采集、存储、标注，到模型训练、仿真验证及端侧集成，深入剖析行业痛点，包括数据规模庞大、算力消耗高、模型迭代快、训练成本高。针对这些挑战，提出了阿里云弹性计算解决方案，包括高性能、弹性伸缩的 AI 基础设施，支持十万核级资源快速调度，结合容器化算力、Serverless 调度、GPU 切分等创新技术，显著降低运维成本并提升性能（Spark场景优化 10% 以上，MRACC 算子优化 35%以上）。此外，方案还涵盖视频抽帧、点云处理、分布式训练及大规模仿真，为智能驾驶业务提供端到端的高效云端加速能力，助力行业实现 高可靠、低成本、快速迭代的目标。<br/><img width="469" height="313" referrerpolicy="no-referrer" src="/img/bVdnzB5" alt="image.png" title="image.png" loading="lazy"/><br/>（图/张先国）</p><p>安谋科技主任软件工程师方方明做了《RTP-LLM：Arm平台全面支持》的主题演讲。Arm 同龙蜥社区、阿里巴巴在很多技术领域都有深度合作，包括推理引擎。在 AI 时代，基于 Arm CPU，团队实现了对阿里巴巴大模型推理引擎 RTP-LLM 的全面支持，通过与生态伙伴的深度合作，RTP-LLM 不仅实现了对倚天等云端 CPU 的高效适配，还支持主流大模型（如 Qwen、Llama、DeepSeek、Bert 等）及多种量化格式（FP32、FP16、INT8、INT4、GPTQ 等），并集成了 Arm KleidiAI 等高性能 AI 内核库，极大提升了推理效率和灵活性。详细的技术亮点包括：1. 利用 Arm 的加速指令对算子的极致性能优化；2. 全面支持 MoE 架构（如 DeepSeek V3、Qwen3），使用专家融合提高并行计算能力，MoE 推理性能最高提升 4 倍；3. 多种量化与矩阵乘优化方案，显著降低内存占用并提升算力利用率；4. 端到端测试与高效部署，支持云到边多场景落地；5. 性能对比显示，RTP-LLM 在 Arm 平台上推理速度和资源效率均优于同类方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525109" alt="图片" title="图片" loading="lazy"/><br/>（图/方方明）</p><p>随着 AI 时代的到来，Java 在 AI 相关负载中扮演越来越重要的角色。阿里云智能集团高级 JVM 工程师邢其正做了《阿里巴巴 Dragonwell JDK：为 AI 时代而生》的主题演讲。阿里云推出的 Dragonwell 21 AI 增强版，包含 Native 加速、热代码重排和 JTune 三大核心技术。具体来说：</p><ul><li>Native 加速：通过高度优化的原生实现，显著提升 AI 相关计算性能，远超传统 JNI 方案，助力 Elasticsearch 向量搜索、Spark 等场景性能提升 18%-60%。</li><li>热代码重排：智能管理 JVM Code Cache，提升 JIT 编译代码的命中率和运行效率。</li><li>智能调优：AI驱动的自动调优框架，降低JVM参数复杂度，实现更高效的资源利用和运维自动化。</li></ul><p>Dragonwell JDK 不仅让 Java 在 RAG、大数据、智能驾驶等 AI 场景下实现性能飞跃，还兼顾企业级系统集成与运维需求，推动 Java 成为 AI 时代的主流生产力工具。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525110" alt="图片" title="图片" loading="lazy"/><br/>（图/邢其正）</p><p>ModelSight 是龙蜥社区自研的 AI 性能分析工具，基于 eBPF 实现 GPU、CPU、框架事件一体化观测，实现 AI 场景下端到端的性能诊断。阿里云智能集团技术专家、龙蜥社区智算基础设施联盟委员王鹏和常怀鑫联合带来了题为《ModelSight：端到端 AI 性能分析框架》的演讲。两位嘉宾分享了如何利用 ModelSight 对 235B 参数的 Qwen3 推理链路进行压测、热点定位与瓶颈可视化，并结合 TP/PP/EP 并行策略在 SGLang 框架中的落地，最终实现 2.12 倍性能提升。通过优化 SGLang overlap schedule，TTFT（首 Token 响应时间）平均提升 20%+。ModelSight 让 AI 性能分析更智能、更高效，助力企业迈向 AI 时代算力极致优化！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525111" alt="图片" title="图片" loading="lazy"/></p><p>鸿钧微电子产品营销经理吴喆就《鸿钧微电子开源社区实践》话题进行了深度分享。他介绍了鸿钧微电子基于 Arm Neoverse 平台的服务器 CPU 产品的规格特点与主要适配的应用场景，并详细阐述其开源社区策略：积极拥抱开源、服务社区；取之于社区，也回馈社区。基于龙蜥操作系统进行芯片验证、驱动适配与性能优化，并向 Linux kernel、Qemu 等社区贡献多项驱动与功能补丁。在应用层面，鸿旻处理器在内存数据库 (Redis，Memcached)、视频编解码 (X265)、大数据 (Spark、Flink) 等场景展现出卓越的性能与能效优势，助力 Arm 架构服务器在云计算、AI、大数据等领域实现突破。基于高效能 Arm Neoverse 平台的服务器 CPU，鸿钧微电子将持续推动开源协作与生态繁荣。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525112" alt="图片" title="图片" loading="lazy"/><br/>（图/吴喆）</p><p>KTransformers 专注于大语言模型的高效推理和微调，通过 CPU-GPU 异构计算实现资源受限环境下的大模型部署，探索 Arm CPU+GPU 平台下的本地极致推理和个性化微调方案。趋境科技技术专家、KTransformers 核心开发人员袁子为带来了题为《KTransformers：在 Arm CPU 上实现大模型异构推理》的主题演讲，就 KTransformers 的以下创新亮点进行了细致探讨：支持 DeepSeek、Qwen、GLM、LLaMa 等主流大模型，灵活适配多种硬件平台；创新“Expert Deferral”机制，推理吞吐提升 45%，精度无损；针对 Arm 架构深度优化，NUMA 亲和、SVE/SME 指令集加速，矩阵运算性能提升 1.5倍；支持 LoRA 微调，已集成进 Llama-Factory 和 SGLang，便于本地微调与多 GPU 加速。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525113" alt="图片" title="图片" loading="lazy"/><br/>（图/袁子为）</p><p>最后，安谋科技主任软件工程师刘亮亮就话题《llama.cpp 跨 NUMA 节点部署优化实践》展开详细探讨。刘亮亮介绍了 llama.cpp 在 Arm 架构服务器部署中跨 NUMA 节点的性能问题及优化方案。主要通过以下两种优化手段：通过“分治”优化 GGML barrier 大幅度减少跨 NUMA 节点原子操作；性能瓶颈 MUL_MAT 算子通过 dst_tensor=src0_tensor * wdata_tensor 进行 Tensor 的乘积操作，为了实现 MUL_MAT 算子的 NUMA 感知内存访问，避免跨 NUMA 内存访问。对其中的 src0 Tensor 以及 dst Tensor 进行内存分割，实现处在一个 NUMA 节点中的线程只访问本地 NUMA 内存。而 wdata Tensor 是通过在量化的时候同时在不同的 NUMA 节点计算成基于 NUMA 节点的多副本。该方案已在 Arm Neoverse N2 平台实测，实现了 S_TG t/s提升 55%，S t/s 提升 53.2%，内存带宽分布也更为均衡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525114" alt="图片" title="图片" loading="lazy"/><br/>（图/刘亮亮）</p><p>最后，感谢本次活动各位嘉宾的精彩演讲，也感谢龙蜥社区伙伴及 Arm 工作人员：刘捷、蔡佳丽、吴永霞、倪俊雄（以上排名不分先后）等人的组织与配合，使得本次走进 Arm MeetUp 活动圆满结束。未来，期待龙蜥社区与 Arm 持续深化合作，在 AI 浪潮中共筑开放、创新、可持续的开源操作系统新生态！</p><p>本次 MeetUp 回顾视频及 PPT 后续会陆续上传至龙蜥官网，欢迎大家持续关注。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[迈向云+数据中心的国产 CPU 新引擎，龙蜥大会 RISC-V 分论坛回顾一览 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525125</link>    <guid>https://segmentfault.com/a/1190000047525125</guid>    <pubDate>2026-01-06 19:05:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会在京顺利落幕，由中兴通讯操作系统产品规划总工徐立锋，中国科学院软件研究所工程师、如意 RISC-V 社区运营丁欣，阿里巴巴达摩院高级合作伙伴运营专家朱祯贞，龙蜥社区运营委员会副主席、龙蜥智算基础设施联盟秘书处负责人金美琴联合出品的 RISC-V 分论坛也圆满举办。本论坛汇聚 RISC-V 芯片厂商、云服务提供商及顶尖科研机构代表，以 “软硬协同” 为核心主线，围绕系统层适配优化、芯片与软件协同创新、开源生态标准化三大维度展开深度研讨。通过前沿技术案例分享、行业大咖圆桌对话及 Anolis OS RISC-V 版本特性展示，本论坛成为高效的技术交流与合作平台，充分展示了 RISC-V 架构在服务器、边缘计算等高性能场景的商业化落地，助力构建开放共赢的算力新生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525127" alt="图片" title="图片"/></p><p>会议伊始，阿里云智能集团研发副总裁、龙蜥社区理事长马涛，中兴通讯副总裁赵志勇，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕依次开场致辞。马涛认为 RISC-V 是国产 CPU 自主创新与协同发展的关键战略方向，尽管 RISC-V 已在嵌入式、边缘计算领域取得进展，但在云计算、数据中心方面尚处于初步探索阶段。未来，社区将联合合作伙伴，持续完善 RISC-V 生态和技术标准化，加速其在数据中心和云计算场景的产业化落地，推动 RISC-V 成为高效节能云计算基础设施的关键力量。赵志勇指出 RISC-V 作为开放指令集架构的代表，以及模块化可扩展的天然优势，在服务器、AI、工业控制等核心领域展现出了巨大的潜力。龙蜥社区作为开源生态的重要载体，汇聚了众多的产学研用各种各方的力量，为 RISC-V 技术落地搭建了宝贵的协作平台。这既是开源生态众人拾柴火焰高的生动体现，更是响应国家战略，凝聚产业合力的具体实践。于佳耕表示，中国科学院软件研究所自 2019 年以来，持续深耕 RISC-V 架构的基础软件与操作系统生态建设，通过成立“如意 RISC-V 社区”，聚焦操作系统内核、编译器等关键基础软件的架构适配与优化，有效推动了 RISC-V 基础软件生态的快速成长与可持续发展，也期待与龙蜥社区等合作伙伴携手，共同推进 RISC-V 基础软件生态的繁荣与成熟。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525128" alt="图片" title="图片" loading="lazy"/><br/>从左至右：马涛、赵志勇、于佳耕</p><p>阿里巴巴达摩院高级技术专家王云龙分享了《共建 RISC-V 通用高性能平台标准》，他重点介绍了 RISC-V 架构进军高性能服务器领域所面临的标准化挑战与共建之路。当前，RISC-V 在技术指标上已快速接近主流水平，产业落地加速，但整体生态仍处起步阶段。关键挑战在于：软硬件标准接口标准不够完善、测试标准与测试套件缺失、系统软件存在碎片化风险。为破局，业界正联合推动软硬件兼容性标准的制定，并构建配套测试体系，旨在以玄铁 C930 等重点产品为突破，协同研发与标准，目标在 2026 年推出标杆服务器产品，实现从技术到生态的闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525129" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家王云龙）</p><p>中兴通讯操作系统架构师谈虎分享了《操作系统 RISC-V 生态实践》，本次分享聚焦 RISC-V在通用计算与服务器领域的软件生态建设与实践进展。核心在于依托 RVA23 关键规范，系统推进从操作系统、编译器、基础库到上层应用的全栈支持与优化。目前，龙蜥等社区已推出 RVA23 预览镜像，内核及虚拟化支持持续增强，并在编译器与基础库层面实现了显著的性能提升。同时，基于 OpenStack、Kubernetes 的云平台及 PyTorch 等关键 AI 组件已完成验证，贯通了主流业务场景。展望未来，行业将协同完善 Server Platform 标准与内核特性，并推动 Ubuntu 等主流系统的长期支持，加速 RISC-V 的产业应用落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525130" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统架构师谈虎）</p><p>中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕分享了《RISC-V 高性能基础软件共建生态》。他重点探讨了如何构建开放共赢的 RISC-V 高性能基础软件生态。RISC-V 凭借其开放与模块化特性，正通过 RVA23 等标准增强对 AI、云计算等高性能场景的支持。然而，硬件多样性给软件兼容与优化带来挑战。为应对此，国内正通过中电标协工委会与“如意 RISC-V 社区”等平台，推动标准、测试与应用生态建设，并构建集成关键扩展的操作系统参考版。现场介绍了 Sapling 评估框架，当前生态成熟度得分为 55.72。展望未来，业界计划借鉴成功模式，通过共建技术规范、CI/CD 体系与示范软件栈，并利用硬件试验场加速软件合规与交付，最终推动 RISC-V 生态实现规模化发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525131" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕）</p><p>龙蜥社区系统安全 SIG Owner 徐峥分享了《RISC-V 可信计算技术实践》。他介绍了可信计算的发展及其在 RISC-V 生态中的关键进展。可信计算历经从概念萌芽到与机密计算、AI 安全融合的数个发展阶段，其核心技术包括信任根、可信度量链、计算平台、软件栈与远程证明。当前，RISC-V 正积极融入这一体系，不仅制定了相关安全模型与服务器规范，还支持 dTPM、fTPM 等多种信任根实现方案。现场以 KOS 的 KTrusted 组件为例，展示了其在 RISC-V 平台上成功实现的动态度量、国密支持与远程认证，验证了可信计算技术从理论到实践的闭环。未来，RISC-V 生态将继续深化可信计算标准的落地与应用协同。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525132" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区系统安全 SIG Owner 徐峥）</p><p>中兴通讯 RISC-V 生态技术专家申林分享了《多架构源代码向 RISC-V 的迁移工具研发与迁移优化实践》。本次分享聚焦于 x86/Arm 软件向 RISC-V 架构迁移所面临的挑战与智能化解决方案。随着 RISC-V 生态扩展，多语言代码迁移因底层架构差异、依赖复杂及人才稀缺而成本高昂。为应对此，中兴通讯开发了智能迁移工具，其核心是“扫描-建议-生成-评测”的四层架构，并创新性地结合结构化扫描、Agent 增强与知识库匹配，以精准识别和转换架构相关代码。目前已成功应用于 OpenSSL 等开源项目，推动其原生支持 RISC-V 向量指令。展望未来，我们将持续扩展工具能力，深化 AI 迁移支持并优化性能，以加速整个软件生态向 RISC-V 的高效过渡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525133" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯 RISC-V 生态技术专家申林）</p><p>阿里巴巴达摩院高级技术专家童琪杰分享了《玄铁 RISC-V 在 OpenAnolis 上的进展》。童琪杰介绍了玄铁在高性能 RISC-V 软件生态建设上的全面布局与实践。面向 RISC-V 在高性能计算领域的广阔前景，玄铁正系统性地构建从编译器、虚拟化、固件到操作系统与应用的全栈软件能力。其在虚拟化、IOMMU 支持及高性能诊断技术等方面取得关键突破，并通过对 ISA-L、X264/X265 等关键组件的深度优化，实现了数倍的性能提升。在实际云服务场景中，Redis、MySQL 等应用也获得了显著的性能增益。玄铁已向 Linux 及龙蜥等主流社区贡献了数百个补丁，持续推动内核与生态成熟，致力于实现高性能 RISC-V 的软硬件协同与规模化落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525134" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家童琪杰）</p><p>中国科学院软件研究所工程师戴希铨分享了《RISC-V 架构上 AI 应用实践与探索》。戴希铨介绍了软件所/如意社区在 AI 应用方面的研究进展。他表示，RISC-V 作为开源指令集架构，正以年均 40% 的增速迅猛发展，全球出货量已超 130 亿颗，成为国产芯片突围的关键路径。其开源、模块化与可扩展特性，为 AI 算力创新提供坚实基础，尤其在端侧AI与异构计算中表现突出。通过玄铁 C920 CPU+NPU/TPU 协同，如意 OS 平台已成功移植 PyTorch、vLLM 等框架，支撑 DeepSeek、Qwen 等大模型在 AI PC、AI 教育等场景本地化运行，实现数据不出端、隐私有保障。典型应用如 AI 公文写作、多模态助手、智能教学等，验证了 RISC-V 在高性能 AI 推理上的可行性，正加速构建 RISC-V 的 AI 软硬件生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525135" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所工程师戴希铨）</p><p>中国电信研究院先进计算中心研究员崔恩放分享了《RISC-V 云计算：AI 智能体算力基础设施新路径》。他系统介绍了中国电信在 RISC-V 云计算领域的完整实践路径。自 2022 年启动研究以来，团队先后实现了云原生虚拟机、操作系统适配等关键突破，并于 2024 年建成了拥有数千核心的“北海”RISC-V 云计算平台。目前，该平台已支撑视频转码、AI 大模型等多元场景的生态建设。在技术层面，通过定制 TeleVM 轻量虚拟机，实现了大幅度的内存与启动开销优化；提出的 AgentDNS 服务发现系统更获得了国际认可。这些成果标志着 RISC-V 正从技术验证走向规模化的云服务商用部署。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525136" alt="图片" title="图片" loading="lazy"/><br/>（图/中国电信研究院先进计算中心研究员崔恩放）</p><p>会上，由阿里云智能集团高级开发工程师、龙蜥社区 RISC-V SIG Maintainer 田瑞冬主持，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕，阿里巴巴达摩院高级技术专家王云龙，中兴通讯操作系统架构师谈虎，中国电信研究院先进计算中心研究员崔恩放共同参与了主题为“高性能 RISC-V 算力生态展望”的圆桌讨论，围绕 RISC-V 架构在高性能计算和 AI 领域的未来发展方向、面临的挑战与机遇展开深度探讨。中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕表示，尽管面临生态不成熟、软件兼容性、定制化成本等挑战，但开源社区、学术界和产业界的合作被视为加速RISC-V生态发展的关键。阿里巴巴达摩院高级技术专家王云龙聚焦在通用高性能（服务器场景）和 AI 两大领域展开分析了 RISC-V 的性能优势、生态支持以及面临的挑战，并表示，如何有效利用 RISC-V 的优势，特别是在 AI 计算方面，仍需探索和创新技术方案。中兴通讯操作系统架构师谈虎则从操作系统角度来看， RISC-V 在高性能方面已取得显著进展，但在实际应用中，尤其是在虚拟化扩展性能测试中的细节问题，与主流架构相比仍存在差距。此外，谈虎呼吁商业软件、硬件公司，尤其是 AI GPU 厂商，共同参与到 RISC-V 生态建设中来。中国电信研究院先进计算中心研究员崔恩放表示， RISC-V 面临的主要挑战在于如何选择合适的场景引入服务器厂商在适配软件方面的问题，并强调了成本优势对于推动新技术采用的重要性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525137" alt="图片" title="图片" loading="lazy"/><br/>（图/圆桌讨论现场）</p><p>感谢本论坛的出品团队：胡捷、徐立锋、陈盛德、丁欣、朱祯贞、林洛卉、李康雅等。</p><p>附本论坛的精彩集锦：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525138" alt="图片" title="图片" loading="lazy"/><br/>视频回放链接：<a href="https://link.segmentfault.com/?enc=NpjaWlihyhAJeIAns2ha1A%3D%3D.A8eCC42G1bUbGWENa3CgYL58VGT1oqw1b5PrdGRfunq7DE2h6nS0vQE82Wgrj%2Fsd" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a> —— 完 ——</p>]]></description></item><item>    <title><![CDATA[龙蜥社区荣膺 InfoQ “2025 中国技术力量榜单”两大奖项 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525152</link>    <guid>https://segmentfault.com/a/1190000047525152</guid>    <pubDate>2026-01-06 19:04:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，InfoQ 极客传媒携手模力工场发起的 “2025 中国技术力量榜单” 评选结果正式揭晓。龙蜥社区与合作伙伴联合提报的 “AI Serving Stack：面向大模型时代的云原生推理服务全栈解决方案”，凭借创新的 RBG 重新定义推理编排、智能调度实现差异化负载优化、以 KVCache 为中心的 PD 分离和以存换算架构等方面的突出创新，斩获了 “2025 年度 AI 工程与部署卓越奖”。同时，“SysOM AI 基础设施运维解决方案”凭借在大规模 AI 训推集群的“分钟级发现、小时级定界”能力、及持续性能剖析与资源效能提升上的领先实践，荣获“‘人工智能+’行业最佳解决/落地方案”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525154" alt="图片" title="图片"/></p><p>在“人工智能 +”行动规划的推动下，人工智能已从前沿技术逐渐演变为驱动产业升级与经济转型的关键力量。2025 年，中国 AI 技术落地与产业应用进入加速期。龙蜥社区积极布局 AI 基础软件栈，在操作系统内核、工具链与运维体系等关键环节持续创新，构建起支撑大模型高效训练与推理的坚实底座。</p><p>大模型推理正演变为"最昂贵的微服务"——既需 HPC 集群的极致性能，又要求云原生的敏捷弹性。AI Serving Stack 是由龙蜥社区与 SGLang 社区、Mooncake 社区、清华大学、南京大学、小红书、算秩未来、科大讯飞和阿里云联合打造，坚持全栈开源理念，采用开放治理模式，100% 开源架构让企业可零成本快速落地。作为大模型生产级”最后一公里”难题的解决方案，AI Serving Stack 填补开源社区在“生产级 LLM 推理编排”领域的空白，提供了从部署管理、智能路由、弹性伸缩、深度可观测的一体化能力，助力企业无论处于 AI 应用初期还是已运行大规模推理业务，都能轻松驾驭复杂的云原生 AI 推理场景。依托领先的 RBG 部署编排机制、智能负载调度策略，以及以 KVCache 为中心的 PD 分离架构和以存换算架构，AI Serving Stack 实现数倍性能提升，显著降低推理成本。未来，AI Serving Stack 将以更加开放的生态聚合产学研智慧，为产业提供从“能跑通”到“高可用、高吞吐、高弹性”的质变路径。</p><p>SysOM（System Operation&amp;Maintenance）是龙蜥社区系统运维 SIG 打造的一站式操作系统运维平台，通过监控、诊断、持续性能剖析等一体化解决方案，具备常态化、无侵入、低开销、可视化分析等特点，广泛应用于教育、医疗、电商、智驾等场景的性能诊断和分析优化。SysOM AI 基础设施运维解决方案是围绕训练及推理场景“MTTR （平均修复时间）长、无效训练时间长”等痛点，以“1 分钟极速发现、5 分钟快速定界定位问题”为目标，显著提升整体训推效率。目前，龙蜥社区系统运维联盟成员单位阿里云已基于 SysOM 项目发布了操作系统控制台，操作系统控制台为用户提供全面的系统资源监控、问题分析和故障解决能力，旨在优化系统性能，显著提升运维效率和业务稳定性。未来，SysOM 将会继续帮助提升训推业务场景万卡集群规模的 GPU 利用率等问题而努力。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=6yPhoU9r6pOT%2Fle9vWWtnw%3D%3D.u%2BcE3ZhUTnDYyLDVsl5%2F%2FGo48Itp1HE5prBmDl5GjPiCGbcwJPHTRTv4eRjGTz%2FO" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p>此前，龙蜥社区及龙蜥操作系统也获得了业界的广泛认可，荣获 OS2ATC 2025 “最具影响力开源创新贡献奖”、龙蜥操作系统通过工信部电子标准院首批开源项目成熟度评估，唯一获得“卓越级”（最高等级）的开源项目等 40+ 行业奖项。未来，龙蜥社区将持续深耕 AI 与操作系统融合创新，携手更多合作伙伴，共建开放、高效的下一代智能基础设施底座。</p><p>完整榜单见链接：<a href="https://link.segmentfault.com/?enc=c3D3E5PLKj5CstdZ2hmgWg%3D%3D.prz5JcPya7deRXMLFocsnLvU0BqgER%2BNoDN09c01dsdIUu0tvZQBGjs4Hs4Yo0FZ" rel="nofollow" target="_blank">https://www.infoq.cn/zones/chinatechawards2025/</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[聚力同行！这 13 家企业荣获“2025 龙蜥社区最佳合作伙伴奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525157</link>    <guid>https://segmentfault.com/a/1190000047525157</guid>    <pubDate>2026-01-06 19:03:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，龙蜥社区正式揭晓 2025 年度“最佳合作伙伴”获奖名单，并于 11 月落幕的龙蜥操作系统大会上举行颁奖仪式。现场，龙蜥社区理事代表、凝思软件副总经理彭志航为阿里云、浪潮信息、intel、海光信息、AMD、Arm、Tenable 等 13 家获奖企业代表颁奖。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525159" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区年度最佳合作伙伴颁奖现场）</p><p>本次获奖企业是与龙蜥社区有实质性合作落地，在产品共建、技术创新、商业实践及社区运营等多个维度综合评估的基础上，结合龙蜥贡献平台的实际数据，经过社区运营委员会与技术委员会评审，并由理事会公示后最终确认。恭喜这些单位！</p><p>在龙蜥社区生态发展计划--“龙腾计划 2.0”的战略牵引下，2025 年龙蜥社区携手整机、云服务、芯片、安全等全产业链伙伴，依托安全联盟、系统运维联盟与智算联盟，深入开展全栈协同，推动装机量达 1000 万套。在龙蜥社区运营目标牵引下，年度案例、活动与贡献度同比增长最高达 200%。本次获奖企业正是深度融入这一开放生态，积极践行“自愿、平等、开放、协作”理念的优秀代表。</p><p>其中，理事长单位与副理事长单位在社区生态共建上做了良好示范。阿里云作为社区理事长单位，积极引领社区整体规划和技术发展方向。通过推出三大合作计划，有效促进芯片、云服务、基础软件等多方成员的深度协同；通过社区治理和生态协同，有效推动关键项目落地，凝聚生态力量促进龙蜥蓬勃发展。浪潮信息全面参与安全联盟、智算联盟及委员会的工作，并积极参与社区规划和组织工作，有力支撑了龙蜥操作系统 Anolis OS 在 AI、云计算及关键基础设施等领域的生态拓展与规模化落地 。海光信息将新一代处理器的关键计算与安全特性深度融入龙蜥操作系统，实现高性能芯片与开源底座的高效协同。通过生态活动与社区共治机制，积极推动国产芯片与龙蜥在行业场景中的融合落地。intel 在技术适配、AI 生态融合等方面与社区深度共建，推动 Anolis OS 在 intel 平台及异构计算场景的优化落地，积极参与 SIG 协作与国际技术资源整合，为社区注入了产业动能与全球视野。中兴通讯为龙蜥社区提供 SW 与 RISC-V 架构的完整工具链及关键组件适配，降低新架构操作系统构建门槛，并持续维护软件供应链安全稳定；同时，其贡献的 NDE 桌面环境在兼容主流生态基础上，丰富了龙蜥桌面生态多样性。</p><p>在技术研发和社区运营方面，诸多伙伴单位也在积极并深度参与龙蜥社区。AMD 在积极推进 EPYC 处理器在龙蜥操作系统上的适配与优化的同时，还长期支持并参与社区组织的技术交流与生态活动，以实际行动助力开源基础软件生态的发展。Arm 深度投入社区 SIG 建设、参与内核及基础软件栈的技术共建，同时通过联合举办技术活动、推动工具链完善等方式，助力龙蜥生态的多元发展和产业落地。如意社区与社区携手加速 RISC-V 在数据中心、云计算等高性能场景的落地，助力开源芯片生态与操作系统深度融合。达摩院依托在操作系统、AI 与系统软件领域的深厚积累，为龙蜥社区在内核优化、智能调度及云原生工具链等方向提供关键技术协助。</p><p>安全联盟、系统运维联盟和智算联盟的成员单位，也通过技术合作、产品适配等参与龙蜥社区共建。安恒信息基于 Anolis OS 23 完成了其安全防护平台的兼容性验证与性能调优，为关键行业用户提供高可靠的安全运行环境；沐曦高效完成其曦云系列加速产品对 Anolis OS 23 的全面适配，充分释放国产 GPU 在龙蜥操作系统上的计算潜能；Tenable 作为国际安全厂商加入安全联盟， 已启动对 Anolis OS 的适配工作，助力用户构建与国际接轨的系统安全防护体系；信通院则发挥其在 ICT 领域标准与评测方面的权威优势，联合龙蜥社区推进操作系统相关技术标准、兼容性认证及开源治理规范的建设。</p><p>最佳合作伙伴生动诠释了各成员单位开源协作与生态共赢的初心与实践。龙蜥社区理事长马涛表示：“开源不是独行，而是众行。龙蜥的蓬勃活力源于每一位合作伙伴的长期投入与多元共创。未来，我们愿与更多伙伴一道，以开放聚合力，以协同筑根基，共赴基础软件的长远未来。”</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=0TiWPsEZ8VHSzT5657EtYw%3D%3D.vvX3WH%2FGOib7YKzdE0GYC0rarmfa5GrKKwQ64G6%2FNE0%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p>]]></description></item><item>    <title><![CDATA[五年同行，共铸基石！11 位杰出贡献者荣获“龙蜥社区五周年特别贡献奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525162</link>    <guid>https://segmentfault.com/a/1190000047525162</guid>    <pubDate>2026-01-06 19:02:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在第三届龙蜥操作系统大会上，龙蜥社区“五周年特别贡献奖”正式公布，由社区高级顾问、凝思软件董事长宫敏与特约顾问和中国开源软件推进联盟副主席兼秘书长刘澎，为本次 11 位获奖的贡献者颁奖。五周年奖项是为激励那些在龙蜥社区成立至今，不仅坚持个人长期参与并持续牵引所在企业投入关键资源，在社区中产生较大影响力的理事或委员。这 11 位获奖者为：马涛、张磊、高翔、龚文、顾剑、陈绪、杨勇、金美琴、张金利、王洪虎、陈鲍孜。</p><p>这些获奖者不仅是龙蜥社区五年发展的亲历者，更是其关键推动者。在他们的持续投入与引领下，龙蜥社区从零起步，逐步构建起覆盖操作系统、芯片、整机及配件、云服务等产业链上下游核心环节的开源生态；龙蜥操作系统也由此建立起完整的技术体系，Anolis OS 累计发布多个稳定版本，广泛支撑千行百业落地应用——装机量突破 1000 万，社区合作伙伴超 1000 家，生态影响力持续扩大。他们的工作，实实在在地塑造了龙蜥今天坚实的技术底座与蓬勃发展的产业格局。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525164" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区“五周年特别贡献奖”现场合照）</p><p>在开源社区的演进过程中，清晰的技术方向与生态愿景离不开顶层的战略引领与坚定投入。龙蜥社区的发展之所以能够稳步前行、持续突破，离不开以理事长、副理事长及理事为代表的高层治理者们的远见与担当。他们不仅锚定社区长期发展方向，更通过资源协同、组织推动和关键决策，将战略转化为行动，为龙蜥构建坚实的技术底座、繁荣的产业生态和可持续的开源机制提供了根本保障。</p><p>阿里云智能集团研发副总裁马涛作为龙蜥社区理事长和掌舵人，五年来始终身先士卒、躬身入局，引领社区从初创走向成熟。他持续坚定投入阿里云的技术与工程和运营资源，加速操作系统关键技术攻坚，推动龙蜥社区夯实基础、扩大影响，成长为国内领先的开源操作系统阵地。他的远见与坚持，不仅为龙蜥社区的蓬勃发展提供了核心动力，也为中国操作系统产业的进步和开源生态建设作出了重要贡献。</p><p>统信软件高级副总经理、龙蜥社区副理事长张磊深耕社区建设，积极推动统信软件深度参与龙蜥建设，成功促成 DDE 桌面环境的移植，并首创分层分类的开源操作系统架构方法，为系统模块化与可维护性奠定基础。同时，他带领团队积极参与开源安全、Rust 改造、机密计算等关键工作组，持续筑牢社区的安全技术底座，为完善龙蜥产品版图与提升用户体验提供了核心支撑。</p><p>龙芯中科副总裁、龙蜥社区理事高翔作为首批加入社区的理事代表，始终带领龙芯团队深度参与并积极推动社区各项建设。特别是在 Anolis OS 8.4 LoongArch 版本发布和 LoongArch SIG 组建过程中，依托龙芯深厚的技术积累与大量工程投入，他有力推动了龙芯开源发行版的构建、软硬件技术的高效协同，以及应用生态的繁荣发展，为龙蜥社区自主架构领域的拓展作出了重要贡献。</p><p>中科方德高级副总裁、龙蜥社区理事龚文自社区成立之初就亲自带领团队深度参与龙蜥操作系统研发工作，成功发布基于 Anolis OS 的中科方德服务器操作系统，并持续推进技术适配与性能优化，在加强系统的安全性与兼容性方面取得成果，为龙蜥技术演进和生态完善作出了切实的贡献。</p><p>飞腾软件技术方案部高级总监、龙蜥社区理事顾剑积极推动飞腾产品与龙蜥在技术及生态层面的深度协同，不仅推动适配飞腾系列 CPU 平台的龙蜥社区发行版共计 17 项、覆盖腾云 S25000、S5000C 等核心产品，同时牵头在飞腾官网设立龙蜥专区，联合共建生态影响力，在加速龙蜥操作系统与国产化软硬件融合落地过程中发挥了关键作用。</p><p>在整体发展方向锚定之后，要将愿景转化为现实，离不开系统化的组织规划与强有力的治理机制。龙蜥社区以技术委员会和运营委员会承担了这个职责，其关键成员作为社区治理的核心支柱构建起兼具技术前瞻性与执行落地力的治理体系。</p><p>龙蜥社区技术委员会主席杨勇主导制定了龙蜥操作系统整体的技术战略与发展路线，构建了清晰的架构蓝图，以社区“三大技术合作计划”为载体，有效牵引并协同推进九大关键技术方向的持续演进。同时，通过建立开放、规范的技术治理机制，为龙蜥打造高可靠、高性能且可持续演进的操作系统底座提供了坚实引领，是龙蜥不可或缺的技术领航人。</p><p>龙蜥社区运营委员会主席陈绪始终聚焦社区发展顶层架构与运营体系的建设，全面指导并参与运营委员会各项工作。凭借 30 年的开源社区实践经验，参与创立了龙蜥社区，为社区的组织设计与机制完善提供了前瞻性引导，有效连接技术、商业与生态多方力量，持续强化社区联盟与组织韧性，为龙蜥社区的高端心智打造、实现规模化成长和可持续健康发展注入了关键的推动力。</p><p>龙蜥社区运营委员会副主席金美琴参与龙蜥社区的早期建设和筹备工作，自 2020 年 6 月开始，领导并推动了包括社区官网的创建、组织制度的建设、生态合作机制的运行以及重要联盟组织和关键项目的建立等，为社区的运营体系建设和生态发展发挥了至关重要的作用，是龙蜥五周年特别致谢的功勋个人。</p><p>版本研发是龙蜥社区工作的重中之重，在有了清晰的技术路线指引、规范的运营和组织建设后，技术的演进则需要更多核心贡献者的参与，他们就是推动 Anolis OS 逐步走向成熟、稳定与广泛应用的关键支撑力量。</p><p>龙蜥社区技术委员张金利作为龙蜥操作系统版本研发负责人，主导了 Anolis OS 所有重大版本的规划与发布，成功构建了一套稳定、安全且高性能的发行版体系。在此基础上，他持续推进内核优化、提升硬件兼容性，并加强自动化测试能力建设，有效保障了版本的高质量迭代。同时，他还积极协同社区力量，不断完善研发流程与发布机制，为龙蜥操作系统在金融、通信等关键行业的广泛应用提供了坚实可靠的版本研发支撑。</p><p>龙芯中科操作系统研发总监、龙蜥社区技术委员王洪虎带领龙芯研发团队与社区紧密协作，牵头成立 LoongArch SIG 工作组，统筹各方力量，向社区累计提交补丁覆盖 800 余个软件包，贡献代码超 30 万行。他主导完成了 Anolis OS 9 个大版本的移植、同源异构适配、验证与发布工作，使龙蜥操作系统成为首个支持龙芯 3C5000 与 3C6000 系列服务器芯片的社区发行版。</p><p>飞腾公司开源操作系统技术负责人、龙蜥社区技术委员陈鲍孜主导推动飞腾处理器系统软件开源生态的逐步成熟与持续演进。他积极推动飞腾平台在内核、驱动及基础软件等关键领域的深度投入，并持续向龙蜥社区贡献技术成果。同时，他协同社区伙伴共同制定内核 kAPI 与基础配置的统一规范，有效助力构建更加开放、兼容、安全且高效的操作系统生态。</p><p>一人行快，众人行远。过去五年，龙蜥社区之所以能从零起步、稳步成长，离不开这些坚定的贡献者和广大开发者的持续投入——他们的努力不仅体现在一行行代码和一个个版本中，更实实在在地落在千行百业的真实场景里。站在新起点，龙蜥将继续坚持共建、共治、共享的理念，诚邀更多企业和开发者加入，一起夯实基础软件底座，共筑中国开源的未来。</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=7%2BluN7rKbZr2dL8NJp278w%3D%3D.hl4b%2FUEytrxFU6fsCDCUUkONpU0x7J%2F0dXWXVsv1flo%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[直播预告｜智算时代，龙蜥携手玄铁共探开源驱动的高性能 RISC-V 新范式 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525169</link>    <guid>https://segmentfault.com/a/1190000047525169</guid>    <pubDate>2026-01-06 19:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>龙蜥社区早在成立之初就已积极布局并持续投入 RISC-V 生态建设。过去一年中，龙蜥社区在 RISC-V 领域取得双突破：软件生态方面，联合达摩院、中兴通讯、浪潮信息、中科院软件所等机构制定 RISC-V SIG 2.0 规划，并携手中兴通讯、达摩院、浪潮信息、如意社区等伙伴首次发布支持 RVA23 高性能扩展的 Anolis 23 RISC-V 预览版。国际标准化方面，社区专家在 RISC-V 国际基金会担任主席/副主席要职，主导 Data Center SIG 运作，推进 RAS/PMU 云方案增强、AIOE 扩展及虚拟化标准制定，参与全球标准建设。</p><p>1 月 8 日（周四）晚 7 点玄铁【智算系列】专题技术沙龙第二期沙龙聚焦玄铁与龙蜥社区合作最新进展，特邀 RISC-V 国际基金会 Datacenter SIG chair、龙蜥社区 RISC-V SIG co-maintainer 宋卓，阿里云技术专家薛帅，阿里巴巴达摩院玄铁 RISC-V 计算库技术专家周云飞等多位专家，系统分享玄铁 RISC-V 在数据中心、高性能计算及云原生场景下的最新技术突破与实践成果。欢迎大家扫码观看直播。</p><p><img width="723" height="1680" referrerpolicy="no-referrer" src="/img/bVdnzC6" alt="image.png" title="image.png"/></p>]]></description></item><item>    <title><![CDATA[2026招聘分水岭：AI重构决策型招聘新逻辑 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047525175</link>    <guid>https://segmentfault.com/a/1190000047525175</guid>    <pubDate>2026-01-06 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026招聘分水岭：AI重构决策型招聘新逻辑<br/>2026年之前，招聘或许还能依赖经验支撑；但2026年之后，仅靠经验驱动招聘，终将陷入被动。2025年，不少企业已开启AI招聘的初步尝试，从生成招聘文案、自动回复候选人到简化流程，确实为HR减轻了部分负担。但随着实践深入，企业逐渐意识到：浅层的AI工具仅能提升表层效率，已无明显边际价值。真正的变革正在发生——AI不再是单纯的效率辅助工具，而是深度渗透到组织能力核心层，开始重构招聘的底层逻辑。<br/>进入2026年，AI的价值早已超越“提速”，转而介入三个过去高度依赖经验与直觉的关键命题：人才该如何被精准定义、能力该如何被科学评估、招聘决策是否还能仅凭主观感觉。在人力资源领域，这一转变指向一个核心结论：唯有实现评估的精准度，才能真正掌握招聘决策的主动权。</p><p>招聘质变：从“工具辅助”到“决策支撑”<br/>过去一年，AI面试已在企业中广泛落地，但多数应用仍停留在基础层面：能与候选人对话、能提出常规问题、能生成评估报告，却无法直接为招聘决策提供有效支撑。核心症结并非AI的智能化程度不足，而是评估打分缺乏精准度、标准缺乏稳定性、结果缺乏清晰解释，难以支撑决策落地。<br/>招聘的本质是对候选人的综合判断，而判断的核心终究要落到精准评估上。真正能融入招聘全流程的AI工具，必然以“精准度”为核心能力，其评估结果不再是仅供参考的建议，而是可直接纳入决策链路的有效依据。这需要满足三重条件：通过人机“背靠背”对比实验验证效果，经受效标效度与重测稳定信度两大心理学指标检验，形成可复现、可验证的评估体系。这一突破，标志着招聘正式从“经验型判断”转向“数据驱动的规模化决策”。<br/>精准评估：贯穿招聘全流程的核心能力<br/>AI招聘工具的精准度，并非单一环节的能力体现，而是渗透到面试全流程的系统性优势，具体表现为四大维度：<br/>•一问多能：单道问题可同步评估多项胜任力，实现HR初筛与技术复试的无缝衔接，整体评估效率提升50%以上；<br/>•智能追问：借鉴资深面试官的提问逻辑，根据候选人回答即时生成针对性问题，精准捕捉核心信息，避免遗漏关键能力点；<br/>•简历深度挖掘：自动解析简历中的关键信息与模糊表述，生成递进式提问，既能有效降低候选人造假风险，也能避免因人工疏忽错失优质人才；<br/>•全维度覆盖：既可以评估沟通、协作等通用胜任力，也能针对编程、算法、工程、财务等专业领域精准设计考题，同时解放HR与专业面试官的精力。<br/>体验赋能：候选人体验成雇主品牌新抓手<br/>AI招聘的价值，既要兼顾企业端的评估效率，也要保障候选人的体验感——在竞争激烈的人才市场中，面试体验本身就是雇主品牌的重要组成部分。优质的AI面试工具，必然以“拟人化交互”为核心设计逻辑，打造有温度、有尊重感的面试场景：<br/>•情绪感知交互：精准捕捉候选人的语速、情绪与潜台词，通过引导式沟通帮助候选人缓解紧张，充分发挥真实水平；<br/>•无断点流畅体验：无需手动操作启停，系统自动识别回答状态并衔接问题，模拟真人对话节奏，避免机械感；<br/>•沉浸式视觉呈现：实现语音与口型的精准同步，打破传统AI面试的“纸片人”疏离感，提升交互真实度；<br/>•实时答疑解惑：支持候选人随时提问，针对岗位详情、企业福利等问题即时回应，强化候选人对企业的认知与入职意愿。<br/>全流程自动化：开启招聘“无人驾驶”新阶段<br/>如果说AI面试解决了“选得准”的问题，那么AI人才寻访工具则攻克了“找得快”的痛点，将招聘初筛阶段的机械劳动彻底自动化，推动招聘进入“无人驾驶”模式。这类工具并非简单的批量操作工具，而是具备独立判断与执行能力的全流程解决方案：<br/>•快速启动值守：30-60秒即可完成配置启用，全程无需人工值守，大幅降低操作门槛；<br/>•智能筛选匹配：自动按学历、年龄、薪资预期等预设条件筛选简历，精准锁定目标候选人；<br/>•拟人化动态沟通：模拟人类沟通逻辑发起对话，根据交互结果判断适配性，不合适则智能终止沟通，提升转化效率；<br/>•全链路信息闭环：自动遍历未读消息并个性化回复，主动索取候选人简历，同步模拟人类打字节奏提升真实感，最终将简历自动同步至ATS系统并生成候选人档案，实现信息流转无缝衔接。<br/>借助AI大模型能力，招聘彻底从“经验驱动”升级为“数据驱动”，不仅实现效率的几何级提升，更将HR从海量重复劳动中解放，聚焦于人才洞察、战略规划等更具价值的工作，让招聘成为支撑企业核心竞争力的重要环节。2026年的招聘分水岭已然显现，唯有主动拥抱AI重构的决策型招聘逻辑，才能在人才竞争中占据主动。</p>]]></description></item><item>    <title><![CDATA[汽车制造质量大数据分析如何助力企业创新发展？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047524715</link>    <guid>https://segmentfault.com/a/1190000047524715</guid>    <pubDate>2026-01-06 18:13:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在汽车制造业转型升级的关键阶段，质量大数据分析正成为企业提升核心竞争力的重要抓手。随着市场竞争日趋激烈，消费者对汽车产品的要求不断提高，传统的质量管理方法已经难以满足现代生产的需求。质量大数据分析通过整合多源异构数据，构建智能化分析模型，为企业提供了全新的质量管控视角和决策支持手段。<br/>一、质量大数据分析的技术路径与价值实现<br/>汽车制造过程中的质量数据具有多维度、多阶段、多系统的特征，涵盖了从原材料进厂到整车出厂的全生命周期。这些数据如果能够得到有效挖掘和分析，将为企业带来显著的竞争优势。当前，主流的质量大数据分析技术包括数据采集与预处理、特征工程、机器学习建模和深度学习算法等多个环节。<br/>在数据采集方面，现代汽车制造企业普遍采用多种技术手段实现数据的全面采集。例如，通过部署在生产线上的各类传感器，实时采集关键工艺参数；通过连接MES系统，获取生产过程的详细记录；通过对接QMS系统，收集质量检验数据和反馈信息。这些数据经过清洗、转换和标准化处理后，才能为后续分析提供可靠基础。<br/>特征工程是质量大数据分析的核心环节。通过对原始数据进行降维、变换和特征提取，可以发现数据中隐藏的质量规律。常用的特征工程方法包括主成分分析、小波变换、时间序列分析等，这些方法能够将复杂的数据转化为可解释性强的特征指标。<br/>在模型构建层面，企业可以根据实际需求选择不同的算法。传统统计方法如回归分析、方差分析仍然有其价值，但随着数据规模的扩大和复杂度的提高，机器学习算法如随机森林、支持向量机等逐渐成为主流。近年来，深度学习技术在质量预测领域的应用也取得了显著进展。<br/>质量大数据分析的价值不仅体现在质量问题的解决上，更重要的是它能够为企业的创新活动提供数据支撑。通过对质量数据的深度挖掘，企业可以发现产品设计、工艺改进和质量控制的创新点，从而开发出更具竞争力的产品。<br/>二、实施质量大数据分析的关键要素与挑战<br/>实施质量大数据分析项目需要综合考虑技术、管理、人才等多个因素。首先，企业需要建立完善的数据采集体系，确保数据的全面性和准确性。这包括部署各类传感器、建立统一的数据标准、完善数据传输机制等。<br/>数据治理是实施过程中的重要环节。企业需要制定数据质量标准，建立数据清洗流程，完善数据安全机制。特别是在汽车行业，质量数据往往涉及商业机密，如何在保护数据安全的同时实现数据价值，是一个需要认真考虑的问题。<br/>技术平台的建设同样至关重要。企业需要根据自身需求选择合适的大数据分析工具和算法。目前市场上有多种解决方案可供选择，如阿里云Quick BI、华为FusionInsight、浪潮云洲工业互联网平台等。这些平台的功能特点、适用场景各不相同，需要结合企业实际情况进行评估和选择。<br/>在组织保障方面，企业需要调整现有的管理架构和工作流程。质量大数据分析往往需要跨部门协作，这就要求企业打破传统的部门壁垒，建立以数据为中心的协同工作机制。<br/>三、行业应用案例：标杆企业的实践<br/>广域铭岛的质量管理系统（QMS）在汽车制造领域实现了多项突破。该系统通过整合工业互联网技术，实现了质量数据的实时采集、传输和分析。系统架构包括数据采集层、传输层、存储层和应用层四个主要部分，各层之间通过标准化接口实现无缝对接。<br/>在应用效果方面，广域铭岛的QMS系统为某知名汽车零部件企业提供了全面的质量管控解决方案。该项目实现了：<br/>200多个关键质量指标的实时监控<br/>检测效率提升40%，检测时间从原来的数小时缩短到半小时以内<br/>质量预警准确率达到95%以上<br/>年均质量成本降低25%<br/>客户投诉率下降30%<br/>另一个值得关注的案例是某德系豪华品牌汽车制造商。该企业实施了基于大数据的智能质量控制系统，实现了从质量检测到工艺优化的全流程数字化管理。系统采用了实时数据可视化技术，将质量数据以直观的方式呈现给相关人员。通过机器学习算法的持续优化，系统能够自动识别质量异常，并给出预警和建议。<br/>国内领先的新能源汽车制造商也在质量大数据分析方面进行了创新探索。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造需要引入工业智能体进行全链路管理？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524723</link>    <guid>https://segmentfault.com/a/1190000047524723</guid>    <pubDate>2026-01-06 18:12:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在新一轮制造业智能化浪潮中，工业智能体正成为推动产业变革的核心力量，尤其在汽车制造领域，其价值已从概念验证迈向规模化落地。作为融合大语言模型、工业机理与多源数据的新型认知智能系统，工业智能体不再局限于单一任务的自动化执行，而是像一位具备经验与判断力的“数字工匠”，能够自主感知产线状态、动态优化工艺参数、协同调度资源，并持续学习进化，真正实现从“人控设备”到“系统自驱”的跃迁。<br/>在汽车制造这一高度复杂、流程密集的行业中，工业智能体的应用已深入全价值链。以广域铭岛为代表的领先企业，通过其Geega工业超级智能体平台，成功将多年沉淀的工艺经验转化为可复用、可迭代的AI能力。在焊装环节，智能体可直接理解工程师的自然语言指令，自动调整焊缝参数并完成编程，使工艺优化周期缩短60%，缺陷率下降45%；在冲压与涂装工序中，它实时融合设备振动、温度、压力与视觉数据，动态调节运行参数，使零部件精度提升15%，废品率降低18%。更令人瞩目的是，其“设计智能体”能根据需求描述自动生成轻量化零部件方案，将新车研发周期压缩60%，彻底打破传统依赖人工经验的设计瓶颈。<br/>在生产运营层面，工业智能体构建了覆盖排产、质量、物流、运维的全链路协同网络。在汽车工厂中，调度智能体可在1小时内完成原本需6小时的人工排产，每周节省超15小时；质量检测智能体借助多模态感知，实现微米级缺陷识别，效率较人工提升200倍；而当供应链突发中断时，广域铭岛平台上的12类专业智能体可在5分钟内协同生成替代方案并验证可行性，将损失降低80%，极大增强了制造系统的韧性与响应速度。其“黑灯仓库”系统更联动AGV与AMR机器人，实现无人化拣选与缺件预警，打通了从订单到交付的智能闭环。<br/>广域铭岛的突破不仅在于技术应用，更在于其构建了“数据—机理—反馈”三位一体的能力基座。通过边缘计算实时捕捉毫秒级设备信号，结合工业Know-How形成“懂行AI”，并依托低代码平台让非技术工程师也能“搭积木”式开发专属智能应用，大幅降低了部署门槛。这种模式有效破解了工业领域长期存在的“数据孤岛”与“经验失传”难题，将老师傅的隐性知识转化为可传承、可扩展的数字资产。<br/>当前，工业智能体正从“辅助工具”加速演变为汽车制造的“决策中枢”。随着数字孪生、联邦学习等技术的融合，未来智能体将不仅优化单点效率，更将重构整车研发、生产、服务的全生命周期体系。广域铭岛的实践表明，唯有将智能体深度扎根于真实汽车制造场景，构建开放、协同、自进化的智能生态，才能真正释放其作为新型生产力的潜能，推动中国汽车产业从“规模驱动”迈向“认知驱动”的高质量发展新阶段。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造企业需要引入模具智能管理系统？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524727</link>    <guid>https://segmentfault.com/a/1190000047524727</guid>    <pubDate>2026-01-06 18:12:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业4.0深度演进的背景下，模具智能管理正成为制造业提质增效的核心引擎，尤其在汽车这一高度精密、高节奏的产业中，其价值尤为凸显。传统依赖人工经验、固定周期保养的模具管理模式，早已难以应对现代汽车生产对稳定性、一致性与响应速度的严苛要求——停机频发、备件冗余、故障难溯，成为制约产能释放的隐形瓶颈。<br/>广域铭岛数字科技有限公司以“数据驱动、AI赋能”为理念，率先构建起一套面向汽车制造的模具智能管理闭环体系，彻底重塑了模具在生产链中的角色：从被动消耗的“工具”，转变为可感知、可预测、可优化的智能资产。其核心依托于Geega工业AI应用平台与工业智造超级智能体，通过在模具与压机上部署高精度传感器网络，实时采集冲压次数、温度、振动、压力等多维数据，结合材料特性、历史维修知识图谱与生产排程信息，动态生成每副模具的“设备健康指数”（EHI）。这一指数不再是抽象报表，而是模具的“生命体征”——当某副用于高光件生产的模具因表面易划伤导致EHI升高，系统自动缩短保养周期；当高强度钢模具因应力累积预警导柱磨损，系统即推送“更换导柱+优化润滑”的精准干预方案，实现从“定时体检”到“精准诊疗”的跃迁。<br/>在领克汽车成都工厂的实践中，这套系统展现出惊人的实战效能：模具相关停机率下降65%，故障响应时间从2小时压缩至15分钟，润滑剂消耗减少18%，备件库存周转率提升40%，设备故障预测准确率突破95%。更重要的是，系统打通了MES、ERP与库存调度系统，形成全链路协同——当某模具即将达维护阈值，系统可提前48小时自动切换订单至健康模具，避免突发停线；一旦异常发生，15分钟内自动生成包含设备切换与参数调整的应急方案，真正实现“未病先防、小病早治”。<br/>这一模式的价值远不止于降本增效。广域铭岛的解决方案将每副模具的全生命周期数据完整沉淀，质量问题可精准溯源至某次保养中的润滑不足或导柱更换延迟，企业经验不再随技师离职而流失，而是转化为可复用、可迭代的数字资产。在芯片短缺等供应链危机中，系统甚至能基于3000组模具实时状态数据，智能分配稀缺资源至故障风险最低的产线，保障核心车型交付，展现出强大的韧性与战略价值。<br/>如今，模具智能管理已从汽车制造的“关键支撑”升级为“核心竞争力”。广域铭岛所推动的，不仅是技术工具的升级，更是一场工业文明的范式革命——它让沉默的钢铁学会“说话”，让混沌的生产重获秩序。未来，随着5G、边缘计算与数字孪生技术的深度融合，模具的“数字分身”将在虚拟空间中模拟百万次冲压，AI智能体通过“自我对弈”持续优化策略，实现“一处学习，全网受益”的群体智能。在汽车工业迈向高端化、柔性化、智能化的征途中，广域铭岛正以模具智能管理为支点，撬动整个制造体系的进化。</p>]]></description></item><item>    <title><![CDATA[团队协作冲突怎么处理？项目管理常见冲突原因与解决流程 项目管理小胡 ]]></title>    <link>https://segmentfault.com/a/1190000047524731</link>    <guid>https://segmentfault.com/a/1190000047524731</guid>    <pubDate>2026-01-06 18:11:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我从市场转岗做项目经理后，最先卡住的不是排计划，而是团队协作里的冲突：一句“又改需求？”就能把讨论从对事变成对人。我也试过圆场、私聊硬扛，结果问题没少，关系更累。后来我才明白：冲突不可怕，可怕的是没流程、没规则。下面我用自己的踩坑经历，给你一套新人也能照着用的团队协作冲突解决流程。</p><p>读完本文你会得到：</p><ul><li>5类常见冲突根因（目标/范围/资源/边界/信息）怎么识别</li><li>一套“冲突处理5步流程”（含话术 + 纪要模板）</li><li>需求变更、跨部门拉扯、资源抢占时怎么拍板与升级</li></ul><h2>明明想把事做好，为什么越团队协作越拧巴</h2><p>我印象最深的一次，是需求评审会的后半段。业务说按钮颜色得改，不然转化会掉。研发同事直接就是一个问号：“又改“？测试补了一句：“那我这轮回归是不是白做了”？</p><p>我当时脑子里只有一个念头：别吵起来。于是我快速圆场：“先按原计划走，改动我会私下再对齐”。会开完我分别私聊三方，像个客服一样安抚情绪。结果一周后需求照改、返工照来、抱怨更大。</p><p>后来我反思了一下，我总怕场面难堪，觉得“和气”比“结论”重要。后来才发现，回避一次，问题会在延期、返工或下一次更大的争执里补回来。</p><p>我开始把争论改写成一句“可决策的话”：要不要进本期？代价是什么？谁拍板？验收标准是什么？当问题变成“怎么做选择”，团队协作才有机会回到同一张桌子上。</p><p>真正让局面变稳的，是固定住几件事：事实对齐、影响量化、拍板机制、书面结论、行动项追踪。下面我会先讲“冲突根因怎么识别”，再给你完整的“5步解决流程”。</p><h2>团队协作冲突最常见的5个原因（以及你怎么识别它）</h2><p>下面五类，我会配上“项目现场常见句式”，你会更容易判断自己遇到的是哪一种。</p><p><strong>1）目标不一致：大家都在为“正确的事”努力，但方向不在一条线上</strong></p><p>典型句式：业务：“先上再说，错过窗口期就没机会了”；研发：“不把隐患收掉，上线就是埋雷”。</p><p>识别要点：一方在谈“速度/机会”，一方在谈“风险/稳定”。</p><p>我的补救：把目标拆成“本期必须守住的三件事”（比如合规、核心质量、关键路径），其余作为后续迭代。目标一旦写清，争论会自动收敛。</p><p><strong>2）范围不清/需求频变：冲突表面是情绪，背后是变更控制缺失</strong></p><p>典型句式：“这不是小改吗？” vs “你们根本不知道影响面。”<br/>识别要点：一方在谈“功能点”，另一方在谈“系统影响/回归成本”。<br/>我的补救：建立最小变更闭环：变更内容 → 影响面 → 成本/风险 → 决策人 → 验收口径。只要闭环跑起来，需求变更就不再等于“临时加塞”。</p><p><strong>3）角色与边界模糊：很多冲突不是讨论不出来，是没人有权拍板</strong></p><p>典型句式：“这个你问产品。”<br/>“这个要业务确认。”<br/>“那你们决定吧。”<br/>识别要点：决策权漂移，责任也漂移。<br/>我的补救：至少明确三件事：谁拍板、谁负责落地、谁负责验收。不用一上来就做完整RACI，但要让“决策出口”存在。</p><p><strong>4）资源稀缺：你以为在争优先级，其实在争“风险与代价谁来背”</strong></p><p>典型句式：“我们也很忙，你这个插进来我那边就要延期。”<br/>识别要点：争论持续围绕“谁先做”，却没人把“代价”写清。<br/>我的补救：把资源冲突显性化：关键人同时承担哪些任务？每个任务的代价是什么？让优先级回到“业务价值 × 风险”，而不是“谁声音大”。</p><p><strong>5）信息不对称：一方看见客户压力，一方只看见工期压缩</strong></p><p>典型句式：“客户已经在催了。” vs “那也不能不讲基本规律。”<br/>识别要点：双方讲的不是同一套语言。<br/>我的补救：练习“翻译”：业务把客户压力翻译成风险等级；研发把技术影响翻译成成本、失败概率与兜底方案。<br/>识别了原因，你就能更从容地进入“处理流程”。接下来这套 5 步，我自己反复用，尤其适合新人 PM 不被情绪带跑。</p><h2>方法与实践：我现在用的“冲突处理5步流程”</h2><p>我把它当成一个“小型可复制流程”。每次冲突来了，我就按步骤走，自己也不容易被情绪裹挟。</p><p><strong>Step 0：先“降温”，再“推进”（30秒把会议从对人拉回对事）</strong></p><p>我常用两句开场：</p><ul><li>“我先确认一下：我们目标是一致的——按期交付、风险可控，对吗？”</li><li>“我们先对齐事实，再讨论方案，最后明确谁拍板。”</li></ul><p>这不是装冷静，而是在给团队协作一个“共同底盘”：我们不是来赢对方，我们是来解决问题。</p><p><strong>Step 1：用“事实—影响—待决策”清单，结束空转争论</strong></p><p>我会把白板（或在线文档）写成三段：</p><ul><li>事实（可验证）：发生了什么？版本/时间/承诺/数据是什么？</li><li>影响（可量化）：对范围/进度/质量/风险的影响分别是什么？</li><li>待决策（一句话）：我们现在要决定什么？（进不进本期？延期？降级？拆分？）</li></ul><p>可复制模板（你直接粘贴到纪要里）：</p><ul><li>事实：</li><li>影响（范围/进度/质量/风险）：</li><li>待决策：</li><li>拍板人：</li><li>结论：</li><li>行动项（Owner/DDL/验收口径）：</li></ul><p>示例（拿“颜色变更”举例）：</p><ul><li>事实：提出变更发生在联调后；影响页面 6 个；涉及埋点 2 个。</li><li>影响：回归增加 1.5 人日；本期上线风险 +1。若不改，业务预计转化下降 X%（先标记为“假设”，待数据验证）。</li><li>待决策：是否进入本期？若进入，是否接受回归工期延长？验收口径是什么？</li></ul><p>小技巧：把“观点”先标为“假设”，再决定要不要验证。这样团队协作会更少陷入“我觉得”。</p><p><strong>Step 2：从“立场”转到“利益”（问两句就够用）</strong></p><p>冲突里最耗人的，是大家都在捍卫立场：</p><ul><li>业务立场：“必须改。”</li><li>研发立场：“不能改。”</li></ul><p>我会刻意追问“利益/担忧”：</p><ul><li>“你最担心的是什么风险？”</li><li>“如果只能保一个，你想保住什么？”</li></ul><p>这就是我借鉴的“原则式谈判”思路：先把深层诉求拉出来，方案空间才会出现。</p><p><strong>Step 3：选对冲突处理方式：不是永远“协作”，而是看情境</strong></p><p>这里我会用 TKI 的两条轴提醒自己，从而形成五种方式：竞争、回避、迁就、妥协、协作。</p><ul><li>坚持度（assertiveness）：我多想满足自己的诉求</li><li>合作度（cooperativeness）：我多想满足对方的诉求</li></ul><p>我给新人一个更“好用”的判断：</p><ul><li>合规/安全/关键质量：可以更“强制/直接”，但必须讲清依据与责任边界；</li><li>信息不足：先“延后”，补数据，不要在不确定里硬拍；</li><li>双方诉求都合理：优先“协作”，找拆分、找阶段性方案；</li><li>必须快速推进：选择“妥协”，但把代价写进记录里（别让代价消失在空气中）。</li></ul><p><strong>Step 4：开一场15分钟“问题解决会”，把结论落地到行动项</strong></p><p>我现在处理冲突，会尽量快速拉一个短会（人越少越好，但必须包含拍板人或授权人）：</p><ul><li>5分钟：事实&amp;影响对齐</li><li>5分钟：列方案（至少2个）+评估标准（进度/风险/收益/成本）</li><li>5分钟：决策 + 行动项（负责人/截止时间/验收口径/风险兜底）</li></ul><p>我常用的“拍板句”：</p><ul><li>“我们现在不是在找完美解，是在找‘可交付的最优解’。”</li><li>“我把代价写进纪要：我们接受 A，意味着 B 风险增加；谁确认/谁承担/怎么兜底？”</li></ul><p><strong>Step 5：把冲突变成机制：写进“工作约定”，减少下次重复爆炸</strong></p><p>如果同一类冲突反复出现，我会在复盘后补一条“工作约定”，比如：</p><ul><li>需求变更三问：改什么？影响什么？谁拍板？</li><li>优先级原则：以业务价值×风险为准（并把评估标准写出来）</li><li>升级路径：卡住超过 24 小时 → @相关Owner；超过 48 小时 → 升级到负责人；所有升级必须带“事实—影响—建议方案”。</li></ul><p>流程能止血，机制能治本。把冲突写进规则里，你的团队协作会越来越省心。</p><h2>启发与建议：我从实践里提炼的5条心得</h2><ol><li>先对齐目标，再讨论方案：目标是方向盘，方案只是路线图。</li><li>冲突不是人不好，多半是规则不清：范围、边界、信息、决策出口，缺哪补哪。</li><li>把结论写下来：没有记录的共识，下一次冲突会把你带回原点。</li><li>让“说真话”变安全：心理安全感常被定义为团队成员相信“表达观点、提出问题、承认错误不会被惩罚或羞辱”。这也是健康团队协作的重要底层条件。</li><li>把冲突当成体检报告：每次冲突都在告诉你——哪条规则缺失、哪条流程漏风、哪个角色没被授权。</li></ol><h2>常见问题 FAQ：</h2><p><strong>Q1：团队协作冲突出现时，PM 是不是应该立刻站队？</strong><br/>A：通常不要。先把冲突从“立场对抗”拉回“事实与影响”。当事实清晰、代价明确时，站队会变成“基于原则的决策”，而不是“情绪偏好”。</p><p><strong>Q2：需求变更引发冲突，最关键的一句话是什么？</strong><br/>A：把争论写成“可决策的问题”：“要不要进本期？代价是什么？谁拍板？验收是什么？”——这句话能把团队协作从互怼拉回协作。</p><p><strong>Q3：跨部门协作总卡住，怎么推动？</strong><br/>A：用“升级路径 + 书面化影响”推动。升级不是告状，而是把“卡住的代价”摆在台面上，让组织机制来做取舍。</p><p><strong>Q4：冲突解决后还要做什么？</strong><br/>A：一定做一次“轻量复盘”：本次冲突的根因属于目标/范围/资源/边界/信息哪一类？把对应规则补上，否则还会复发。</p><p>我到现在也不敢说自己“很会处理冲突”。但我越来越确定：项目经理的成长，不是把所有人都哄开心，而是把复杂的团队协作问题，变成大家都能理解、都能执行的流程与规则。</p><p>如果你也是跨岗位转型、刚做 PM，遇到冲突别急着怀疑自己——很多时候不是你不行，而是你还在学习“从沟通者走向协调者”。愿我们都能在一次次复盘里，把拉扯变成对齐，把争论变成决策，把冲突变成更稳的协作机制。</p>]]></description></item><item>    <title><![CDATA[这篇一定要看，观测云 2026 产品路线图全公开 观测云 ]]></title>    <link>https://segmentfault.com/a/1190000047524757</link>    <guid>https://segmentfault.com/a/1190000047524757</guid>    <pubDate>2026-01-06 18:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>序言：奇点临近，可观测性的代际跨越</h2><p>站在 2026 年的时间节点回望，我们正处于 IT 基础设施历史上最深刻的变革之中。这不仅是云计算的延续，更是一场由人工智能（AI）主导的“认知革命”。如果说云原生（Cloud Native）时代解决了资源的弹性问题，那么 AI 原生（AI Native）时代则致力于解决决策的自主性问题。</p><p>Gartner 的战略预测早已指出，到 2026 年底，由于缺乏足够的 AI 风险护栏，甚至可能出现数千起因 AI 决策失误导致的法律索赔案件。这一预测不仅揭示了 AI 技术的双刃剑效应，更深刻地指出了当前技术栈中最大的空白——对于自主智能体（Autonomous Agents）的深度可观测性与治理能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524759" alt="图片" title="图片"/></p><p>在 2026 年的企业环境中，由于 Agentic AI 的普及，软件不再仅仅是执行预定义代码的静态指令集，而是变成了具有推理、规划和执行能力的“数字员工”。这些智能体像 F1 赛车的维修团队一样协作，以模块化的方式处理复杂的业务逻辑。然而，这种自主性带来了前所未有的不确定性：一个简单的用户请求可能触发成百上千次非确定性的模型推理、工具调用和数据库交互。传统的应用性能监控（APM）工具，基于确定性的堆栈跟踪和静态的拓扑图，已无法完全解释这种动态生成的行为链路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524760" alt="图片" title="图片" loading="lazy"/></p><p>与此同时，数据重力的法则依然生效且愈发严苛。随着生成式 AI 和多模态交互的爆发，企业产生的数据量呈指数级增长，但 IT 预算的增长却远远滞后。如何在数据爆炸的背景下，既保持对所有信号的敏锐捕捉，又严格控制存储成本，成为了 SRE 和 CIO 面临的头号难题。传统的“索引一切”（Index Everything）的日志管理模式在经济上已然破产，市场迫切呼唤一种全新的、基于存算分离架构的数据底座。<br/>本文将作为观测云（Guance）2026 年的产品技术展望，深入剖析在这一大变革背景下，我们如何通过产品演进解决测试、业务、数分、SRE 等多角色的核心痛点。我们将沿着“从上层业务应用到底层基础设施”的逻辑脉络，抽丝剥茧，呈现一个全栈可观测的 2026 图景。</p><h2>1. 市场趋势：驱动变革的四股力量</h2><p>在展开产品细节之前，我们需要厘清推动 2026 年可观测性技术变革的宏观力量。</p><h3>1.1 AI Agent 的崛起与黑盒治理危机</h3><p>2026 年，AI 不再是辅助工具，而是核心生产力。Gartner 指出，AI 原生开发平台正在让自主 Agent 协作完成复杂任务。然而，Agent 的引入带来了全新的不可预测性：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524761" alt="图片" title="图片" loading="lazy"/></p><ul><li>非确定性路径：Agent 的决策逻辑是动态生成的，传统的基于固定代码路径的 APM 难以追踪其思维链。</li><li>Token 经济学：每一次 API 调用都对应着真金白银。监控系统的核心指标从 CPU 使用率转向了“Token 消耗率”与“任务完成成本”。</li><li>黑盒风险：当 Agent 陷入死循环或产生幻觉时，传统的监控告警往往滞后，导致巨额的 API 费用浪费。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524762" alt="图片" title="图片" loading="lazy"/></p><h3>1.2 数据引力与存算分离的必然</h3><p>随着数字化转型的深入，企业数据量正以每年 180 EB 的速度增长。传统的基于本地磁盘（SSD/HDD）的存储架构（存算耦合）面临巨大的成本压力：</p><ul><li>扩容困境：为了增加存储空间，不得不增加计算节点，导致计算资源闲置浪费。</li><li>冷热数据鸿沟：90% 的查询集中在最近 24 小时的数据，但为了合规，企业必须存储数年的历史数据。将所有数据都放在昂贵的块存储上在经济上已不可行。</li><li>解决方案：市场正全面转向基于对象存储（S3/OSS）的存算分离架构，这也是 GuanceDB 演进的必然方向。</li></ul><h3>1.3 平台工程（Platform Engineering）与左移</h3><p>DevOps 正在进化为平台工程。开发者不再满足于被动接收告警，他们需要自服务的、可编程的观测能力。可观测性正在“左移”进入 CI/CD 流水线，开发者要求能够通过代码（Monitoring as Code）定义监控规则，并通过 API 触发自动化修复流程。</p><h3>1.4 FinOps 与数据主权的博弈</h3><p>随着全球数据法规（GDPR 等）的收紧，大型企业越来越倾向于“控制面与数据面分离”的架构。他们希望利用 SaaS 厂商提供的先进 AI 分析能力（控制面），但要求原始遥测数据保留在自己的云账号下的对象存储桶中（数据面），即 BYOS（Bring Your Own Storage）模式。</p><h2>2. 观测云新的产品功能：蓝图 (Blueprint)</h2><p>—— 可观测性编排与自动化引擎</p><p>在观测云 2026 的规划中，“蓝图”（Blueprint）不是一张静态的架构图或一套预设的 Dashboard 模板。基于最新的用户需求与 UI 设计，蓝图被重新定义为“官方组件支持计划”的核心载体，是一个低代码/无代码的可观测性编排与自动化引擎。</p><p>它通过可视化工作流（DAG - 有向无环图）将分散的观测能力串联起来，形成从 数据查询 -&gt; 逻辑转换 -&gt; AI 分析 -&gt; 行动 的完整闭环。</p><h3>2.1 蓝图的核心架构：可视化 DAG 工作流</h3><p>传统的监控告警是离散的：一个阈值触发一封邮件。而 2026 年的蓝图引擎引入了状态机与流式处理的概念。蓝图工作流由以下四类核心节点构成，支持用户通过拖拽方式构建复杂的运维逻辑：</p><h4>2.1.1 数据查询节点（Input / Sensor）</h4><ul><li><p>DQL (Data Query Language) 驱动：支持复杂的查询逻辑，包含了简单的指标阈值（如 CPU &gt; 80%），更加支持跨数据源的关联查询。</p><ul><li>示例：“查询最近 5 分钟支付接口的 P99 延迟，且仅当该延迟不仅超过阈值，同时伴随错误日志激增时触发。”</li></ul></li><li>多源异构：支持 Metrics、Logs、Traces、RUM（用户体验数据）的混合查询。</li></ul><h4>2.1.2 转换与逻辑节点（Processor / Logic）</h4><ul><li>低代码处理：支持 JavaScript/TypeScript 片段或表达式语言（Expression Language）。</li><li>上下文丰富：原始告警往往缺乏上下文。转换节点可以调用外部 CMDB 或 K8s API，为告警数据打上“业务线”、“负责人”、“部署版本”等标签。</li><li>价值：解决“告警疲劳”的核心手段。通过逻辑判断（如去重、抑制、时间窗聚合），将 100 条原始告警压缩为 1 条高价值根因分析。</li></ul><h4>2.1.3 AI 分析节点（Intelligence / Obsy AI）</h4><ul><li>ObsyAI 智能体介入：这是蓝图的智能核心。当逻辑节点检测到异常后，自动唤起 ObsyAI 进行根因分析。</li><li>能力：自动关联该时间段内的变更事件（Change Events）、错误日志聚类（Log Patterns）和异常链路等等。</li><li>输出：一段自然语言描述的诊断建议：“检测到支付服务延迟升高，关联到 3 分钟前 payment-service 的 v2.1 发布，且 DB 连接池报错激增。”</li></ul><h4>2.1.4 行动节点（Action / Actuator）</h4><ul><li>OpenAPI 闭环：这是蓝图与传统监控的最大区别。它通过 OpenAPI 与外部系统对接，执行实质性操作。</li><li><p>场景覆盖：</p><ul><li>通知：发送富文本消息到 Slack/钉钉/企业微信（包含 AI 诊断结果）等任意communication channel。</li><li>监控器管理：自动静默非核心服务的告警，或在流量高峰期动态调整阈值。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524763" alt="图片" title="图片" loading="lazy"/></p><h2>3. 更加全面的变更观测 (Change Observability)</h2><p>—— 根因分析的时间维度</p><h3>3.1 变更：系统熵增的核心问题</h3><p>根据 SRE 的经验法则，80% 的生产事故是由变更（Change）引起的。无论是代码发布、配置文件的修改、Feature Flag 的切换，还是基础设施的扩缩容操作，都是打破系统稳态的潜在因素。然而，传统的监控工具往往只记录了“结果”（Metrics 的突变、Logs 的报错），却丢失了“原因”（谁、在什么时候、做了什么变更）。</p><p>观测云 2026 将“变更”提升为与 Logs、Metrics、Traces 同等的一级数据公民（First-Class Citizen），构建了全维度的 变更观测（Change Observability） 体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524764" alt="图片" title="图片" loading="lazy"/></p><h3>3.2 变更数据的全栈采集与关联</h3><h4>3.2.1 统一变更数据模型</h4><p>为了捕捉系统中的每一次变化，观测云 2026 建立了一套标准化的变更数据模型：</p><ul><li>应用层：深度集成 Jenkins、GitLab、GitHub Actions 等 CI/CD 工具，自动捕获部署事件（Deployment）、Commit 信息、Artifact 版本。</li><li>基础设施层：监听 Kubernetes Events（如 Pod Killing, Scaling）、云厂商审计日志（如 AWS CloudTrail、阿里云 ActionTrail），捕获资源的创建、销毁和规格变更。</li><li>配置层：对接 Nacos、Apollo、Consul 等配置中心，实时记录配置项的 Diff。记录配置变了，还记录从什么变成了什么。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524765" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.2 变更叠加分析（Change Overlay）</h4><p>变更观测的核心价值在于上下文的融合。在观测云的所有时序图表（Metric Charts）上，系统会自动叠加变更事件的标记（Annotations）。</p><ul><li><p>场景示例：</p><ul><li>传统视图：看到 API 错误率曲线在 14:00 突然飙升，SRE 开始排查日志。</li><li>变更观测视图：看到错误率飙升的同时，时间轴上显示 13:59 分有一个“支付服务 v3.2 Canary 发布”的标记。鼠标悬停即可看到该发布的 Commit Message 和变更人。</li></ul></li></ul><p>这种直观的视觉关联，能够将 MTTR（平均修复时间）从小时级缩短至分钟级。运维人员不再需要去各个聊天群里询问“刚才谁动了线上环境？”，变更观测直接给出了答案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524766" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.3 变更风险评分与智能门禁</h4><p>结合 Arbiter 引擎的历史分析能力，系统能对每一次变更进行风险评分。如果某次代码提交修改了核心链路的关键文件，且缺乏足够的测试覆盖率，或者历史数据显示该开发者的变更回滚率较高，系统将在变更发生前发出预警，甚至联动 CI/CD 流水线进行阻断。</p><h2>4. Obsy AI SRE Agent 推出：可交互的根因分析侦探</h2><p>观测云 2026 颠覆了传统人找数据的排查模式，推出了一套基于 动态假设树（Dynamic Hypothesis Tree） 的交互式排查界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524767" alt="图片" title="图片" loading="lazy"/></p><h3>4.1 触发与情境感知</h3><p>当监控器发现异常（例如 flight-query-api 接口响应时间 P99 &gt; 2s），系统将直接启动 Obsy AI SRE Agent。在观测云的 Console 中，用户会看到一个关联了错误上下文（Error Trace、Latency Chart）的交互式卡片。</p><h3>4.2 动态假设引擎（Dynamic Hypothesis Engine）</h3><p>AI Agent 不会盲目列出所有指标，而是像一位经验丰富的 SRE 工程师一样进行逻辑推演。它会基于当前的异常特征，生成多条排查路径（Investigative Plans），并依据历史数据和专家知识库计算出每一条路径的置信度概率：</p><ul><li>Plan A (概率：高)：假设为数据库超时（DB Connection Block / Slow SQL）。</li><li>Plan B (概率：低)：假设为上游依赖服务响应变慢。</li><li>Plan C (概率：低)：假设为网络网关故障。</li></ul><h3>4.3 交互式思维导图与递归诊断</h3><p>用户点击高概率的 Plan A，界面将展开一个可视化的排查思维导图。这不仅仅是静态图表，而是 AI 正在执行的逻辑动作流：</p><ul><li>节点展开：Agent 自动检查 "RDS 资源水位" -&gt; "数据库连接池状态" -&gt; "慢查询日志分析"。</li><li>执行验证：每个节点会显示执行状态（Check Passed / Failed）。例如，AI 发现连接池正常，但捕获到了一条全表扫描的慢 SQL。</li><li>根因锁定：当 AI 找到确凿证据（如：flight_no 字段缺失索引导致全表扫描），它会标记为“Root Cause Identified”，并生成自然语言的结论报告。</li></ul><h3>4.4 闭环与反馈</h3><ul><li>对话式追问：在锁定根因后，用户可以直接与 Agent 对话：“如何修复这个问题？”Agent 会根据知识库提供 Runbook 建议（如：添加索引的 SQL 语句）。</li><li>多路径回溯：如果 Plan A 的排查结果显示一切正常（Negative Result），Agent 会智能建议用户切换至 Plan B 或 Plan C。系统会自动保留已排查过的路径记录，避免重复工作，直到递归找到真正的问题源头。</li><li>人工接管：整个 UI 包含清晰的 "Abort/Take Over" 按钮，允许工程师随时打断 AI 的自动化逻辑，手动介入排查。</li></ul><p>这套设计融合了现代工程美学与 AI 智能，将原本黑盒的 AI 思考过程透明化（White-box），让 SRE 既能享受 AI 的效率，又能保持对排查逻辑的掌控。</p><h2>5. GuanceDB 演进策略：云原生内核的重构</h2><p>GuanceDB 3.0 是观测云强健的心脏。现有的数据库架构大多基于本地磁盘（Shared-Nothing 架构），在面对 PB 级数据时，扩展成本高昂且缺乏弹性。GuanceDB 3.0 的核心目标是演进为基于对象存储（S3-Native）的存算分离架构。在这一演进过程中，我们必须正视目前与行业标杆的技术差距，并提出针对性的优化策略。</p><h3>5.1 关键演进挑战与探索方向</h3><p>GuanceDB 的演进要解决对象存储带来的物理限制：高延迟与元数据管理。</p><h4>5.1.1 挑战一：海量小文件元数据瓶颈 (Metadata Bottleneck)</h4><ul><li>痛点：在实时写入场景下（如 IoT），会产生数以亿计的小文件（Objects）。如果 GuanceDB 3.0 的元数据层不够强大，查询时的“列出文件”操作就会成为瓶颈，导致查询超时。</li><li><p>演进方向：分布式元数据架构</p><ul><li>探索：不再依赖单体 SQL 数据库存储元数据。探索分布式 Key-Value 存储来构建元数据层。</li><li>目标：支持每秒数十万次的元数据读写，确保即使底层有百亿个 S3 对象，查询规划器也能在毫秒级定位到需要扫描的文件。</li></ul></li></ul><h4>5.1.2 挑战二：存算分离后的查询延迟 (Cold Start Latency)</h4><ul><li>痛点：S3 的首字节延迟（TTFB）通常在几十到几百毫秒。对于“老板看数”的实时 Dashboard 场景，这种延迟是不可接受的。</li><li><p>演进方向：智能分层与分布式缓存 (Smart Tiering &amp; Caching)</p><ul><li>热数据 (Hot)：近期的数据查询直接走本地内存/磁盘，速度极快。</li><li>温数据 (Warm)：引入分布式缓存层。对于经常访问的“昨天”或“上周”的数据，在计算节点的 SSD 上进行 LRU 缓存。</li><li>冷数据 (Cold)：完全沉淀在 S3。查询时按需拉取，接受秒级延迟，换取极致成本。</li><li>价值：实现“像 SSD 一样快，像 S3 一样便宜”。</li></ul></li></ul><h4>5.1.3 挑战三：Compaction (压缩) 策略与写放大</h4><ul><li>痛点：为了优化查询，必须将 S3 上的小文件合并为大文件（Compaction）。但 S3 的 PUT 操作是收费的，且消耗网络带宽。</li><li><p>演进方向：成本感知的智能 Compaction</p><ul><li>策略：不盲目压缩。引入基于“查询热度”和“S3 计费模型”的代价函数。</li><li>探索：利用 Spot Instances（竞价实例）在云厂商的闲时运行 Compaction 任务，将小文件合并为列式存储（Parquet/ORC 变体），同时构建布隆过滤器（Bloom Filters）和 Min/Max 索引，以减少未来的扫描量。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524768" alt="图片" title="图片" loading="lazy"/></p><h2>6. 落地 Targeting Needs：场景化痛点的精准打击</h2><p>技术必须服务于业务。不同的大客户场景对数据平台的需求是截然不同的，甚至是互斥的。我们不能用一套参数满足所有人，而是提供灵活，可以满足特种需求的的数据引擎。</p><h3>6.1  场景一：实时查询（Real-Time Query）—— 老板看数</h3><ul><li>用户：CIO、CTO、NOC 监控大屏。</li><li>痛点：Dashboard 需要秒级刷新。读多写少，并发高。传统的 OLAP 引擎在处理聚合查询时延迟较高，且并发能力受限。</li><li><p>观测云 2026 解决方案：流式聚合。</p><ul><li>原理：GuanceDB 不再每次刷新都扫描原始日志。在数据摄取（Ingest）阶段，通过流式预聚合引擎（Pre-aggregation Engine）自动维护常用指标（如 Global_Error_Rate）。</li><li>效果：Dashboard 查询实际上是在读取一张极小的预计算表，无论原始数据量是 1TB 还是 1PB，大屏刷新始终保持在亚秒级。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524769" alt="图片" title="图片" loading="lazy"/></p><h3>6.2 场景二：批量报表与数据挖掘 —— 分析师的深潜</h3><ul><li>用户：SRE 专家、安全分析师、运营人员。</li><li>痛点：读少，但 IO 极重。需要扫描过去 30 天的海量日志进行根因分析或生成月度运营报告。容易导致数据库 OOM (Out of Memory) 或查询超时。</li><li><p>观测云 2026 解决方案：向量化执行引擎 + Serverless 扫描。</p><ul><li>原理：利用存算分离架构，当检测到此类大查询时，GuanceDB 动态弹出一组 Serverless 计算节点（Worker），并行扫描 S3 上的数据块。利用 SIMD 指令集和向量化执行（Vectorized Execution）加速过滤。</li><li>开放性：支持通过 DQL 导出数据到 Notebook 或外部数仓，满足深度挖掘需求。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524770" alt="图片" title="图片" loading="lazy"/></p><h3>6.3 场景三：高并发写入 —— IoT 与车联网数据海啸</h3><ul><li>用户：车企（V2X）、智能制造、IoT 架构师。</li><li>痛点：写多读少。Tag（标签）基数极高（High Cardinality）。例如，百万辆车，每辆车有唯一的 VehicleID，传统时序数据库的倒排索引会因此膨胀爆炸，导致内存溢出。</li><li><p>观测云 2026 解决方案：稀疏索引与列式存储优化。</p><ul><li>原理：放弃对高基数 Tag 建立全量倒排索引。GuanceDB 借鉴先进的架构设计，采用 稀疏索引（Sparse Indexing）和数据分区（Micro-partitions） 技术。</li><li>效果：将 VehicleID 作为排序列，通过 Min/Max 索引快速跳过无关数据块。在不牺牲写入性能的前提下，支持对高基数标签的高效过滤，彻底解决“索引爆炸”问题。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524771" alt="图片" title="图片" loading="lazy"/></p><h3>6.4 场景四：AI/LLM 可观测 —— Agent 行为治理</h3><ul><li>用户：AI 平台工程师、大模型应用开发者。</li><li>痛点：Agent 行为具有不确定性（幻觉、死循环），且 Token 成本昂贵。传统的 CPU/内存监控无法反映 AI 业务的健康度。</li><li><p>观测云 2026 解决方案：Model Telemetry 与成本归因。</p><ul><li>数据模型：引入专用的数据类型追踪 Prompt 和 Completion 的 Token 消耗、延迟、模型版本。</li><li>蓝图集成：通过蓝图实时监控 Token 消耗速率。一旦发现某个 Agent 陷入死循环（Token 消耗斜率异常），立即触发熔断机制（Action 节点），并通知开发者。</li><li>价值：进阶到 AI 业务治理，为企业节省真金白银的算力成本。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524772" alt="图片" title="图片" loading="lazy"/></p><h3>6.5 场景五：日志成本黑洞 —— 拒绝存不起，查不到</h3><ul><li>用户： 运维总监、合规审计部门、FinOps 负责人。</li><li>痛点： 日志数据量呈指数级增长（每天几十 TB），但 99% 的日志通常都用不上，只有故障时才需要回溯。传统方案要么全量索引导致存储成本天价，要么为了省钱只存 3 天导致关键数据丢失。</li><li><p>观测云 2026 解决方案： 冷热分层（Tiered Storage）+ Schema-on-Read（读时建模）。</p><ul><li>原理： GuanceDB 引入智能分层策略。热数据（最近 3 天）存高性能 SSD 并建立全索引；温/冷数据（3 天 - 3 年）自动下沉至对象存储（S3/OSS），不建立繁重倒排索引。当需要查询冷数据时，利用算子下推（Pushdown）临时扫描目标块。</li><li>效果： 将日志的长期存储成本降低 80% 以上。让企业存得起海量日志，还能在需要审计时，无需数据迁移即可直接查询历史归档。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524773" alt="图片" title="图片" loading="lazy"/></p><h3>6.6 场景六：微服务风暴 —— 抓住百万分之一的异常</h3><ul><li>用户： 架构师、中台研发负责人。</li><li>痛点： 在成百上千个微服务的调用链中，每天产生数亿条 Trace 数据。传统 APM 采用头部采样（Head-based Sampling）（如只采 1%），容易导致“关键的报错请求正好被丢弃了”，无法还原故障现场。</li><li><p>观测云 2026 解决方案： 100% 全量摄取 + 尾部采样（Tail-based Sampling）。</p><ul><li>原理： 数据进入系统时不做丢弃，先在内存缓冲区暂存。通过流式引擎实时分析整条链路的尾部状态（是否报错、是否高延迟）。只有有问题或高价值的链路才会被持久化存储，正常的无用链路自动丢弃。</li><li>价值： 在不增加存储预算的前提下，实现100% 的异常捕获率。不再靠运气抓 Bug，而是靠精准的算法。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524774" alt="图片" title="图片" loading="lazy"/></p><p>当然以上仅是冰山一角。观测云的统一数据底座已打破场景壁垒，无论是日志降本还是链路追踪，皆能以一套架构，从容应对万千需求。</p><h2>结语：观测云的2026</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524775" alt="图片" title="图片" loading="lazy"/></p><p>观测云 2026 的产品预告是对未来观测形态的一次预判与押注。</p><ul><li>市场在变：AI Agent 带来了复杂性，FinOps 带来了成本压力，数据主权带来了架构约束。</li><li>产品在变：蓝图将会成为企业的自动化中枢；GuanceDB 拥抱 S3，打破存储的物理边界，用云原生的架构解决云时代的规模问题。</li><li>价值在变：我们针对不同角色（CIO、SRE、IoT 架构师、AI 工程师等等）提供不同场景都可用的灵活解决方案。</li></ul><p>对于 CTO 和 CIO 而言，选择观测云 2026，不仅是选择了一个监控平台，更是选择了一套能够驾驭 AI 时代不确定性、从容应对数据洪流的系统。请查收这份产品路线图。</p>]]></description></item><item>    <title><![CDATA[从“救火”到“预见”：汽车行业操作系统智能运维解决方案 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524793</link>    <guid>https://segmentfault.com/a/1190000047524793</guid>    <pubDate>2026-01-06 18:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>汽车行业趋势与核心挑战</h3><p>近年来，新能源汽车加速普及，智能座舱、车联网和智能辅助驾驶等技术已成为整车厂商竞争的关键。这些功能基于端云协同架构，云端基础设施至关重要——无论是用户在车上点播音乐、远程控制车辆，还是智能车联网系统上传传感器数据，背后都离不开稳定、高效的基础设施云平台支持。</p><p>随着车辆联网率的提升以及 AI 模型能力的增强，汽车行业IT系统的数据吞吐量与计算负载呈指数级增长。一辆具备智能辅助驾驶能力的测试车，单日即可产生数 TB 的原始数据；一次面向百万用户的 OTA 升级，也可能在短时间内引发流量洪峰。在此业务特点下，云端基础设施的稳定性已成为直接影响用户体验甚至行车安全的核心环节。</p><h4>汽车行业的基础设施面临的四大核心运维挑战</h4><p>在上述业务压力下，支撑汽车场景的基础设施频繁遭遇以下四类典型问题，传统的运维手段往往难以有效应对：</p><p>1、周期性高峰业务-资源超载与系统夯机<br/>在 OTA 推送或早晚高峰、节假日远程控制集中触发时，服务器内存和 CPU 瞬时过载，系统进入“假死”状态——进程无法调度、命令无响应，即使未完全宕机，业务也已不可用。</p><p>2、出行服务下的资源超卖-内存失控与服务中断<br/>内存泄漏、缓存膨胀或显存异常增长等问题隐蔽性强，初期不易察觉，但会逐步耗尽系统资源，最终触发OOM（Out-Of-Memory）导致关键进程被强制终止，服务中断。</p><p>3、车联网服务响应迟滞-性能抖动与偶发卡顿<br/>系统在多数时间运行正常，却偶尔出现毫秒级延迟突增，且无法稳定复现。这类问题通常源于锁竞争、高频系统调用或 I/O 瓶颈，传统监控指标难以捕捉根因。</p><p>4、智能驾驶业务-智算可观测能力缺失<br/>在 GPU 集群中，显存使用异常、NCCL 通信失败、任务卡死等问题频发，但缺乏从应用层到硬件层的全栈观测能力，导致排查周期长、依赖人工经验，严重影响模型训练与推理效率。</p><p>这些问题共同指向一个核心诉求：汽车行业需要一套能够贯通“应用—操作系统—硬件”的智能运维体系，实现故障的提前预警、精准定位与自动恢复，而非被动响应。</p><h3>通过操作系统管理平台一站式解决 OS 运维卡点</h3><h4>操作系统管理平台介绍</h4><p>操作系统控制台是阿里云自研的操作系统管理平台，覆盖主流 Linux 操作系统，旨在为客户提供便捷易用、高效、专业的操作系统生命周期管理能力，包括运维管理、操作系统智能助手 OS Copilot、订阅等功能，支持通过界面、OpenAPI、MCP、CLI 等多种方式提供服务。致力于降低操作系统的技术门槛，通过系统解决客户应用与云平台运维信息不对称等问题，提升用户的云上体验。操作系统控制台智能运维可以让用户摆脱冗长的运维垂直栈和分析链，让平台更懂用户业务的异常根因，懂资源的消耗。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=vmTNRifaTQOsj1de9lMeow%3D%3D.uYj4k8BtGkFAkkSovLqFxFlnyZL96ZPP%2FZ%2FvAgkmMfOIAvB7mEJWlvfr%2FDOs6kjW" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524795" alt="图片" title="图片"/></p><h4>解决方案概述</h4><p>面对智能座舱与自动驾驶业务对云端基础设施提出的高并发、低延迟、强稳定等严苛要求，传统运维手段已难以应对资源超载、内存失控、性能抖动和 AI 任务异常等复杂问题。操作系统控制台作为面向汽车行业的综合运维平台，致力于打通“应用—操作系统—硬件”全栈链路运维能力。</p><h4>场景化解决方案与核心能力</h4><p>针对智能座舱、自动驾驶等业务以上提到的汽车行业四大典型运维痛点，操作系统控制台推出对应的诊断及观测能力，在常见的夯机、OOM、抖动及 AI 观测都给出了对应的解决方案，弥补汽车行业的企业在基础设施可观测性的能力短板。</p><p>应对资源超载与系统夯机 —— 主动内存保护</p><p>核心收益：解决用车周期性高峰业务场景，资源的夯机问题，减少业务卡顿及异常。</p><p>适用场景：OTA 大规模推送、远程控制指令洪峰、AI 模型高并发推理等瞬时高负载场景。</p><p>在高峰期，系统常因内存迅速耗尽而进入“near-OOM”状态，传统 Linux OOM 机制响应滞后，往往在系统已卡死或无响应后才触发进程终止，且易误杀缓存型或 I/O 密集型进程，进一步加剧磁盘压力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524796" alt="图片" title="图片" loading="lazy"/></p><p>通过以下机制实现主动防护：</p><ul><li>堆内存精准评分：不再依赖 RSS（常驻内存），而是聚焦可回收的堆内存使用量，更准确识别真正造成内存压力的进程。</li><li>批量终止策略：单次释放不足以缓解压力时，可同时终止多个高内存占用进程，快速释放大量内存。</li><li>多级压力响应：支持低、中、高三档灵敏度配置，适配不同业务对延迟的容忍度。</li><li>关键进程白名单：通过进程名或命令行参数显式保护车控、推理等关键服务，避免误杀。</li></ul><p>在内存压力上升初期即介入干预，有效防止系统夯机，保障远程控制、OTA 下发等关键指令的可达性和执行时效。</p><p>破解内存黑盒 —— 内存全景分析</p><p>核心收益：解决出行出行服务下的资源超卖所引起的服务中断问题，提升业务连续性。</p><p>适用场景：内存使用率持续飙升、频繁触发 OOM、缓存占用异常、GPU 显存增长不明等复杂内存问题。</p><p>传统运维难以回答“内存到底被谁用了”——是应用泄漏？文件缓存堆积？还是驱动或 GPU 隐性占用？内存全景分析提供统一、细粒度的内存视图。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524797" alt="图片" title="图片" loading="lazy"/></p><ul><li>一键生成全链路报告：无需登录机器，控制台点击即可输出包含进程、容器、缓存、驱动、GPU 显存的完整内存分布。</li><li>穿透应用堆内存：支持对 Java、Python、C++ 等语言进程的堆内对象进行二次拆解，定位具体泄漏点。</li><li>关联缓存与原始文件：如识别出“/ota/firmware_v2.1.bin”占用了 8GB page cache，便于优化预加载或清理策略。</li><li>纳入 GPU 与网卡内存：将 RDMA 缓冲区、GPU 显存映射等“不可见”内存纳入监控范围，消除盲区。</li></ul><p>内存全景从“猜测谁吃内存”转变为“秒级定位泄漏源”，显著缩短故障排查时间，支撑容量规划与资源优化。</p><p>消除性能抖动 —— 进程热点分析</p><p>核心收益：解决车联网服务响应迟滞问题，提升用户体验。</p><p>适用场景：偶发性卡顿、CPU 或 I/O 突发飙升、毫秒级延迟突增等难以复现的性能问题。</p><p>这类问题往往无固定复现路径，传统监控无法捕获瞬时调用栈，导致根因长期悬而未决。</p><p>进程热点分析基于 eBPF 实现轻量、持续追踪，该功能具有以下特点：</p><ul><li>小于 3% 性能开销：无侵入采集函数调用栈、上下文切换、系统调用等数据，适用于生产环境长期运行。</li><li>火焰图 + Diff 对比：直观展示 CPU 热点路径，并支持抖动前后或版本升级前后的性能差异比对，自动高亮退化点。</li><li>智能识别：结合大模型语义理解，识别高频/proc 访问、锁竞争、阻塞 I/O 等常见性能陷阱，并给出优化建议。</li><li>秒级回溯抖动时刻：系统持续缓存轻量调用栈，问题发生时可立即锁定瞬时高负载进程及其热点函数。</li></ul><p>进程热点分析解决了“无法复现”的性能难题，让偶发卡顿变得可追踪、可解释、可修复。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524798" alt="图片" title="图片" loading="lazy"/></p><p>保障智算基础设施的稳定 —— GPU 持续追踪时序图</p><p>核心收益：解决智能驾驶业务在 GPU 场景运维难的问题，提升训推效率，节省成本。</p><p>适用场景：自动驾驶模型训练、vLLM 等大模型推理、多 GPU 通信任务等 AI 密集型负载。</p><p>AI 任务对 GPU 资源稳定性高度敏感，但显存泄漏、XID 错误、通信瓶颈等问题往往隐蔽且难定位。</p><p>操作系统控制台构建基于内核的持续追踪体系，它具有以下特点：</p><ul><li>分钟级异常告警：实时监控显存、SM 利用率、温度、XID 错误码等，及时发现GPU掉卡、硬件报错或任务卡死。</li><li>小时级问题定界：支持慢节点识别、NCCL 通信延迟分析、单卡/整机资源瓶颈判断，快速缩小排查范围。</li><li>函数级根因剖析：通过 GPU 火焰图和 Timeline Profiling，将 Python 层调用、框架算子与 CUDA Kernel 关联，可视化算子执行序列与等待时间。</li><li>让 AI 任务“看得见、说得清、改得准”，避免因底层资源异常导致训练中断或推理延迟，提升 AI 基础设施可靠性。</li></ul><h3>行业成功案例分享</h3><h4>案例一：车机服务高峰期无响应</h4><p>案例背景</p><p>某头部物流行业用户节假日出现业务无响应、登录实例也十分卡顿。通过监控发现客户实例使用的内存在某个时间点开始徒增，接近系统的总内存（即 available 非常低），但没有超过系统总内存。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524799" alt="图片" title="图片" loading="lazy"/></p><p>通过 top 命令可以看到系统的 CPU sys 利用率和 iowait 利用率和系统负载都持续飙高，kswapd0 线程占用非常高的 CPU 进行内存回收。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524800" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>通过配置开启节点级别的 FastOOM 功能，由于业务是实验较为敏感的业务，内存压力选择中，且设置业务程序（以 python 启动，进程名包含 python 子串）为避免被 OOM 进程且设置无关的日志程序优先杀死。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524801" alt="图片" title="图片" loading="lazy"/></p><p>开启后，当节点内存水位处于 near-OOM 状态时，用户态提前介入，根据配置杀死了如下进程，从而释放了部分内存避免系统进入了夯机状态。通过操作系统控制台的系统概览可以看到 FastOOM 介入的相关记录。</p><p>如下图所示，由于 kube-rbac-proxy 和 node_exporter 等进程 oom_score_adj 被设置为接近 999，FastOOM 会匹配内核策略优先杀死这些进程，但是由于杀死这些进程后释放内存较小，仍处于 near-OOM；因此 FastOOM 杀死了配置优先杀死的 logcollect 进程。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524802" alt="图片" title="图片" loading="lazy"/><br/>由于用户态及时介入杀死进程释放出内存，使系统避免进入了near-OOM的抖动状态。</p><h4>案例二：AI 推理场景显存异常增长</h4><p>案例背景<br/>某头部自动驾驶方案公司部署的 vLLM 线上推理服务：KV-Cache 利用率并未打满，但通过 GPU 监控（DCGM）观察到有显存明显增长。vLLM 启动时使用显存预分配机制，在 KV-Cache 利用率未满情况下理论显存值不应上涨。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524803" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>对在线业务应用进行 continuous Profiling，在 TimeLine 上找到显存申请的 cudaMalloc 调用，打上标记线，即可找到具体的 Python 调用，进一步定位到导致显存额外申请的调用栈如下所示，结合 decorate_context() 实现可以判断出显存增长的原因 Torch 的缓存管理机制，可以通过调整 vLLM 显存预占或 Torch 缓存的显存占用环境变量来进行相应的问题规避。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524804" alt="图片" title="图片" loading="lazy"/></p><h4>案例三：智能汽车全球发布会——高并发实时交互下的零卡顿保障</h4><p>案例背景<br/>2025 年春季，某智能电动汽车品牌在全球同步发布其旗舰车型，并启动大规模整合营销活动。由于发布会覆盖全球多个时区，且关键的价格公布环节引发高度关注，价格揭晓后 5 分钟内，App 总访问量突破 800 万，商城相关接口请求峰值高达 12 万 QPS，整体流量达日常水平的 200 倍以上。在此极端并发场景下，系统面临严峻挑战：核心交互接口的端到端响应必须控制在 30ms 以内，任何毫秒级的延迟都可能导致 APP 白屏、操作无响应或直播卡顿，严重影响用户体验并威胁品牌形象。与此同时，瞬时流量洪峰、极致的体验敏感性以及秒级故障定位与恢复的严苛要求，使得传统依赖日志回溯的运维排查方式完全失效，系统稳定性与实时可观测性面临前所未有的考验。</p><p>解决方案<br/>依托操作系统控制台构建“三位一体”保障体系：</p><p>1.高并发资源防护 —— 主动内存保护 + 关键进程隔离 提前识别车控指令服务、视频流网关、身份认证微服务为 关键路径组件，加入 FastOOM 白名单； 配置中等灵敏度内存压力策略，在系统进入 near-OOM 前主动释放低优先级进程内存，避免 kswapd 抢占 CPU 导致 API 延迟飙升，实现发布会全程实现 “零白屏、零卡顿、零交互失败”。</p><p>2.实时性能监控 —— 进程热点分析持续追踪 全链路启用 eBPF 驱动的进程热点分析，持续采集函数调用栈； 当某区域用户集中反馈“点击无反应”时，系统秒级回溯到问题时刻； 结合大模型辅助诊断，自动建议“缓存网络指标”而非实时读取，热修复后延迟 P99 从 50ms 降至 30ms。</p><h3>展望</h3><p>随着智能电动汽车的持续发展，车载系统与云端基础设施的耦合将更加紧密。未来，汽车不仅是交通工具，更是移动的计算终端和数据节点。这要求云平台不仅具备更强的弹性、更低的延迟和更高的可靠性，还需在资源调度、故障自愈和性能优化等方面实现更深层次的智能化。</p><p>操作系统控制台将持续围绕汽车行业核心场景打磨能力。一方面，我们将进一步强化对高并发、高实时性业务的支持，优化 FastOOM、内存全景分析、进程热点追踪等能力在 OTA 洪峰、自动驾驶训练推理等典型负载下的表现；另一方面，我们将探索 AI 驱动的智能运维（AIOps）路径，结合大模型与实时可观测数据，构建具备预测、诊断、决策和执行能力的 AI Agent 运维体系。</p><p>联系我们 </p><p>您在使用操作系统控制台的过程中，有任何疑问和建议，可以搜索群号：94405014449 加入钉钉群反馈，欢迎大家扫码加入交流。</p>]]></description></item>  </channel></rss>