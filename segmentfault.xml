<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[【节点】[Adjustment-Contrast节点]原理解析与实际应用 SmalBox ]]></title>    <link>https://segmentfault.com/a/1190000047466908</link>    <guid>https://segmentfault.com/a/1190000047466908</guid>    <pubDate>2025-12-11 17:03:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=6ga6jPZw4hRDbq2HNBS9BA%3D%3D.fxLrVrdp%2B3Ky%2Bv3nuw1Doh0Ci0j8zHriSsUStB4X26cRbkK1TpeXkb4D2WOHLTfXXNMF4kS3IegQ7fPqluziqzoY8%2BPpbAHJNZ4OSWYtOC4mghK%2F0Gs9iVm4be%2FxOSj2YFntl75y1E6A%2Fcy0jGcHxbh3nC1adzPgZOM2NzLvfL6JPin8sibvFjh1kty94gBDp26wIato6CjMbhvZKbAecAxoOiTvp1AHZv3tDwNHYN8%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong></blockquote><h2><strong>节点功能概述</strong></h2><p>Contrast节点是Unity URP（Universal Render Pipeline）渲染管线中用于动态调整图像对比度的核心工具，通过数学变换实现颜色值的非线性映射。该节点在视觉效果处理中扮演基础性角色，广泛应用于游戏画面增强、特效制作以及艺术风格化渲染等场景。</p><p>在URP管线中，Contrast节点是色彩校正流程的关键环节，可与Brightness（亮度）、Saturation（饱和度）等节点组合，形成完整的色彩调控链路，为开发者提供专业级的画面调节能力。通过实时调整对比度参数，开发者能够灵活实现从写实画面到艺术化视觉风格的转换。此外，针对不同平台的性能需求，该节点还提供了多级优化策略。</p><h2><strong>数学原理与算法实现</strong></h2><h3><strong>核心算法解析</strong></h3><p>对比度调整算法主要包括以下步骤：</p><ul><li>中点计算：通过 <code>midpoint = pow(0.5, 2.2)</code> 在sRGB色彩空间获取中性灰基准值，确保调整结果符合人眼感知特性。</li><li>线性变换：使用 <code>(In - midpoint) * Contrast</code> 对输入颜色进行缩放，基于与中点的距离实现对比度的增强或减弱。</li><li>值域修正：通过 <code>+ midpoint</code> 将输出值约束在[0,1]范围内，防止颜色溢出和显示异常。</li></ul><h3><strong>参数影响分析</strong></h3><ul><li>Contrast值大于1.0：增强高光与阴影的分离度，适用于写实风格材质、室外场景，可提升纹理细节与材质质感。</li><li>Contrast值在0.0 - 1.0之间：降低颜色差异，适用于雾效、朦胧效果或回忆场景，能营造氛围感，柔化画面边缘。</li><li>Contrast值小于0.0：产生负片效果，适用于特殊艺术处理或中毒状态表现，可结合颜色反转实现独特视觉效果。</li></ul><h2><strong>端口系统详解</strong></h2><h3><strong>输入端口</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466910" alt="img" title="img"/></p><ul><li><p>In端口：</p><ul><li>类型：Vector 3（自动兼容Vector 1/2/4类型）</li><li>连接建议：优先连接纹理采样节点或颜色混合节点，确保输入色彩数据已正确进行伽马空间转换。</li><li>扩展功能：支持HDR颜色输入，可用于超范围对比度调节。</li></ul></li><li><p>Contrast端口：</p><ul><li>类型：Float</li><li>动态控制：可结合Time节点实现动画效果，例如使用Sine节点生成呼吸灯式的对比度变化。</li></ul></li></ul><h3><strong>输出端口</strong></h3><ul><li><p>Out端口：</p><ul><li>类型：Vector 3</li><li>后续处理：建议在连接Master节点前进行值域检查，可使用Clamp或Saturate节点确保输出安全。</li></ul></li></ul><h2><strong>典型应用场景</strong></h2><h3><strong>动态天气系统</strong></h3><ul><li><p>环境光照强度输入Contrast节点，根据天气类型进行不同设置：</p><ul><li>晴天：Contrast = 1.3 + 提高饱和度。</li><li>雾天：Contrast = 0.7 + 蓝色调偏移。</li><li>暴雨：Contrast = 0.9 + 降低亮度。</li><li>之后进行色彩分级后处理。</li></ul></li></ul><h3><strong>角色状态反馈</strong></h3><ul><li>受伤状态：Contrast = 0.8 + 色调偏向冷蓝，配合屏幕模糊效果。</li><li>强化状态：Contrast = 1.5 + 增强高光，叠加金色光晕粒子。</li><li>隐身状态：Contrast = 0.5 + 透明度渐变，结合边缘抖动与扭曲特效。</li></ul><h3><strong>界面特效应用</strong></h3><ul><li>按钮悬停：Contrast从1.0平滑过渡至1.2，增强交互反馈。</li><li>任务完成：Contrast短暂提升至1.8，营造闪光庆祝效果。</li><li>警告提示：Contrast在0.5至1.5之间快速振荡，以吸引玩家注意。</li></ul><h2><strong>性能优化方案</strong></h2><h3><strong>移动端适配策略</strong></h3><ul><li>预计算优化：利用MaterialPropertyBlock对静态对比度进行预计算，降低实时开销。</li><li>算法简化：对非关键对象采用简化版对比度算法，省略伽马校正步骤。</li><li>动态调整：通过LOD系统动态调整节点复杂度，远距离对象使用固定对比度值。</li><li>批次处理：合并使用相同对比度参数的材质，减少Shader变体数量。</li></ul><h3><strong>最佳实践建议</strong></h3><ul><li>避免在透明材质中使用负值Contrast，以防alpha通道异常。</li><li>多阶段调整时，推荐使用0.5 - 1.5范围内的中间值，维持画面色彩平衡。</li><li>结合Brightness节点使用，可获得更平滑的过渡效果，建议Brightness调整范围控制在0.8 - 1.2。</li><li>在Post Processing堆栈中应用对比度效果时，优先选用Volume系统而非材质节点。</li></ul><h2><strong>常见问题解决方案</strong></h2><h3><strong>颜色过曝处理</strong></h3><ul><li>后接Clamp节点限制输出值域，设定安全范围为[0.02, 0.98]。</li><li>使用Smoothstep节点柔化边缘，形成自然过渡。</li><li>调整输入纹理的伽马值，例如从2.2降至2.0，以减弱对比度强度。</li><li>采用ACES色调映射替代常规对比度调整，实现更具电影感的画面效果。</li></ul><h3><strong>性能瓶颈排查</strong></h3><ul><li>检查是否在多个材质实例中重复使用相同节点，推荐采用Shader Graph的Sub Graph功能进行复用。</li><li>验证Contrast参数是否频繁变动，可考虑使用动画曲线替代实时计算。</li><li>分析节点在Shader中的编译结果，借助Frame Debugger检查绘制调用情况。</li><li>监控GPU性能，确认对比度计算是否成为渲染瓶颈。</li></ul><h2><strong>进阶应用示例</strong></h2><h3><strong>风格化渲染管线</strong></h3><ul><li><p>Base Texture输入进行Contrast调整，根据Saturation控制走向：</p><ul><li>高饱和：走向卡通风格 + 边缘检测。</li><li>低饱和：走向写实风格 + 胶片颗粒。</li><li>之后经过自定义色调曲线，输出最终色彩。</li></ul></li></ul><h3><strong>动态环境响应</strong></h3><ul><li>日间模式：Contrast = 1.2 + 冷色调，强化阳光照射效果。</li><li>夜间模式：Contrast = 0.9 + 暖色调，营造温馨氛围。</li><li>战斗状态：Contrast = 1.5 + 高对比度，增强视觉冲击力与紧张感。</li><li>探索模式：Contrast = 1.1 + 自然色调，平衡视觉舒适度与细节呈现。</li></ul><h3><strong>多平台渲染策略</strong></h3><ul><li>高端PC/主机：启用完整对比度算法链，支持实时参数调节。</li><li>移动端标准：采用简化算法，按固定时间间隔更新对比度。</li><li>低端设备：禁用动态对比度调整，使用预烘焙的静态效果。</li></ul><h2><strong>版本兼容性说明</strong></h2><ul><li>Unity 2022.3+配合URP 14.0+：支持完整功能 + HDR，支持光线追踪对比度调整。</li><li>Unity 2021.3配合URP 12.0+：支持完整功能，包含所有基础特性。</li><li>Unity 2020.3配合URP 10.0+：支持基础功能，缺少部分高级混合模式。</li><li>Unity 2019.4配合URP 7.0+：功能有限，仅支持基本对比度调整。</li><li>Unity 2018.4配合URP 5.0+：部分功能可用，需手动进行伽马校正。</li></ul><hr/><blockquote><a href="https://link.segmentfault.com/?enc=ORqPs0qvAJ6teWpDOsR40A%3D%3D.z%2B7DN1SJracniurBKrHLSe5vzURJ%2BNM9c2E6NwIiJVj1iU85g15t%2BpRsPrmpeogB5FZEFqH1MWyEfBSeITnC3ZgB%2B9d%2BzynScPWB07dcRaAVD30KraydKYwTqyadv4mMcJyr24fCawZmAWo9zBPHdgFDlaFxUkLmJNu3LEYE1lFI9MeeN9V%2BHPYKVG3src8Hw9kAfEp8OUe5XB0LX%2FsUMqmdoMD8hM68boEFIuCKwe4%3D" rel="nofollow" target="_blank">【Unity Shader Graph 使用与特效实现】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[Fusion 引擎赋能：七猫如何使用阿里云 EMR Serverless Spark 实现数仓加速 ]]></title>    <link>https://segmentfault.com/a/1190000047466977</link>    <guid>https://segmentfault.com/a/1190000047466977</guid>    <pubDate>2025-12-11 17:03:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、背景介绍</h2><p><strong>七猫公司介绍及业务规模</strong></p><ul><li>七猫是一家深耕文化娱乐行业的互联网企业，总部坐落在上海市前滩中心。七猫旗下原创文学网站七猫中文网于2017年5月正式上线，专注为原创作者提供创作指导、版权运营等全方位一体化服务。七猫拳头产品七猫免费小说 App 于2018年8月正式上线，专注为用户提供正版、免费、优质的网络文学内容阅读服务。现平台用户超 6 亿，规模位列数字阅读行业前列。<br/><img width="723" height="405" referrerpolicy="no-referrer" src="/img/bVdnkuj" alt="image.png" title="image.png"/></li></ul><p><strong>原有大数据架构</strong></p><ul><li>七猫的数仓团队主要是承接七猫各条业务线的离线数据开发、实时数据开发、指标建设、数据治理等工作。 <br/><img width="723" height="470" referrerpolicy="no-referrer" src="/img/bVdnkuk" alt="image.png" title="image.png" loading="lazy"/></li></ul><p><strong>架构痛点分析</strong>（多业务线带来复杂分析需求/计算性能瓶颈/开发运维门槛高成本大.......）<br/>1、需求复杂，同时支持数仓工程师，数据分析师，算法工程师等多岗位计算需求<br/>2、计算成本高</p><ul><li>传统数仓任务在半托管集群下只支持开源 Spark，无法充分利用业界领先的 Native 加速和Remote Shuffle Service 技术提升整体性能进而降本 </li><li>半托管集群和 adhoc 集群缺乏灵活的弹性能力，存在较大的资源浪费<br/>3、 运维复杂</li><li>半托管集群在资源层需要一定人力介入运维</li><li>Spark 引擎升级，Python 环境管理等常见运维操作复杂且有较大生产风险</li><li>无法精确评估各条业务线乃至单作业成本，进而进行针对性优化 </li></ul><h2>二、为什么选择阿里云 EMR Serverless Spark</h2><h3>（一）Fusion + Celeborn 赋能，批处理性能全面提升超 50%</h3><p>在大数据计算场景中，任务性能一直是关注的核心指标。为进一步提升计算效率，Serverless Spark 推出了 Fusion 加速能力，通过向量化SQL加速技术，显著缩短作业执行时间。同时，Serverless Spark 内置了企业级 Celeborn，大幅提升大 Shuffle 作业的稳定性和性能。为验证实际效果，我们选取了三个典型的批处理任务，对比传统 Yarn 环境和 Serverless Spark 的执行效率。</p><p><strong>用户行为增量分析任务</strong></p><ul><li>资源配置: 500C，1500G</li><li>Yarn：32 分钟</li><li>Serverless Spark：10 分钟，提速 69%</li></ul><p>Celeborn 有效减少了宽依赖阶段的任务调度与 Shuffle 开销，使整个计算链路更加高效。</p><p><strong>用户日志明细处理任务</strong></p><ul><li>资源配置：500C，1200G</li><li>Yarn：30 分钟</li><li>Serverless Spark：14 分钟，提速 53%</li></ul><p>在典型的日志清洗与聚合任务中，Fusion 加速显著提升了宽表 Join 与聚合计算的执行效率。</p><p><strong>内容聚合与统计任务</strong></p><ul><li>资源配置：800C，1200G    </li><li>Yarn：71 分钟</li><li>Serverless Spark：38 分钟，提速 46%</li></ul><p>面对高达 11TB 的 Shuffle 数据量，Serverless Spark 依然保持稳定的执行性能，有效降低任务时延。</p><p>整体来看，Serverless Spark 对计算密集型和IO密集型任务都有大幅优化，三个任务平均提速超过 56%，在复杂 ETL 与大规模数据处理场景中展现出显著优势。相比传统 Yarn 集群，Serverless Spark 不仅具备更强的弹性能力和更低的资源使用成本，通过 Fusion + Celeborn 的优化，更是实现了计算效率与资源性价比的双重提升。<br/><img width="723" height="434" referrerpolicy="no-referrer" src="/img/bVdnkul" alt="image.png" title="image.png" loading="lazy"/></p><h4>（二）Serverless 模式突破算力瓶颈，实现弹性敏捷的数据处理</h4><p><strong>📌 问题：传统架构难以应对算力潮汐与资源刚性约束</strong><br/>随着七猫数据作业规模持续增长，大数据集群长期处于高负载运行状态。原有架构存在一些问题：如缺乏灵活的弹性能力，而在白天又存在资源浪费。 传统模式已无法支撑“按需响应、准时交付”的现代数据服务要求，并且原先基于实例级别的资源交付方式，在潮汐时存在浪费。 <br/><img width="723" height="159" referrerpolicy="no-referrer" src="/img/bVdnkun" alt="image.png" title="image.png" loading="lazy"/></p><p>✅ 解决方案：引入 Serverless 弹性算力，构建智能调度新范式<br/>七猫全面拥抱云原生理念，采用 Serverless 模式重构计算层，实现面向业务负载的动态资源供给。核心举措包括：<br/>引入  Serverless Spark ，基于 OSS-HDFS 统一存储层实现计算与存储彻底解耦，支持计算资源秒级弹性伸缩；<br/><code>利用 Serverless Spark 海量资源池与容器化调度能力，实现 最小粒度 1 核 的精细化资源计量，按实际使用量计费，彻底告别资源预占；</code><br/>基于 Dataworks 提供的友好的用户交互界面，可以提交管理 Streaming/SQL/PySpark 等多类作业。 <br/>高峰期算力爆发能力大幅提升，分钟内即可弹出数千核 vCore 资源，满足瞬时高并发处理需求。该模式实现了从“资源驱动调度”向“业务需求驱动执行”的根本转变。<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdnkup" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>三、技术方案设计</strong><br/><img width="723" height="377" referrerpolicy="no-referrer" src="/img/bVdnkuw" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>应用层</strong></p><ul><li>基于阿里云 DataWorks 和自建 Apache DolphinScheduler 进行数仓开发。</li><li>基于 Jetbrains IDE 产品和自建 Apache Superset 进行报表分析和 adhoc 查询。</li></ul><p><strong>接入层</strong></p><ul><li>通过接入 EMR Serverless Spark 官方提供的 spark-submit 工具进行数仓调度，该工具100%兼容开源 spark-submit 工具，为数仓的整体迁移提供了巨大的便利。</li><li>通过接入 EMR Serverless Spark 的 Kyuubi Gateway 进行日常数据分析和即席查询。Kyuubi Gateway 同样也提供了100%兼容开源的 restful 和 jdbc 接口，另外在开源基础上增强了云原生部署环境下的稳定性和提交并发性能。</li></ul><p><strong>管控面</strong></p><ul><li>用户无感的全链路多 AZ 高可用，提供稳定安全7 * 24小时的 PAAS 服务。</li><li>通过资源队列管理能力隔离不同业务团队的资源使用，业务峰谷时期能够快速进行资源上限调整。</li><li>通过作业级别管理能力进行日常的作业运维和调优，资源使用情况可细化到作业维度，易于进行针对性成本优化。</li></ul><p><strong>计算面</strong></p><ul><li>引擎能力</li><li><ul><li>数仓 SQL 作业默认开启 fusion 加速提升 SQL 执行性能</li></ul></li><li><ul><li>默认使用内置 Celeborn 服务进行 Shuffle，提升大 Shuffle 稳定性</li></ul></li><li>极致弹性</li><li><ul><li>兼容开源 Spark 资源配置支持最短弹性步长为1CU的弹性能力</li></ul></li><li><ul><li>依赖资源底座服务保障资源供给</li></ul></li></ul><h2>四、迁移后的收益</h2><ul><li><p><strong>技术层面</strong></p><ul><li><p>性能</p><ul><li>核心任务运行时间缩短30分钟</li><li>天级报表产出时间提前5小时</li></ul></li><li><p>业务稳定性</p><ul><li>数仓任务连续60天没有 SLA break </li></ul></li><li><p>运维灵活性</p><ul><li>不再关心资源层运维，在业务峰谷时期可以进行秒级扩缩容</li><li>扩缩容步长为1CU，达到接近100%的资源使用率</li><li>根据作业级别的资源消耗统计快速定位不符合预期的作业并进行针对性优化</li><li><p>运行环境隔离，避免作业之间互相干扰，最大程度的降低运维风险</p><ul><li>作业级别隔离 Spark 版本，可快速测试验证最新版本 Spark 相关 feature</li><li>作业级别界面化定制 Python 环境，避免黑屏操作全局 Python 环境带来的生产风险</li></ul></li></ul></li></ul></li><li><p>财务层面</p><ul><li><p>成本优化</p><ul><li>数仓离线链路成本降低35%</li><li>adhoc 查询成本降低30%</li></ul></li></ul></li><li><p>业务层面</p><ul><li>业务团队因数据获取效率提升，减少了约 40% 的无效等待时间，可将精力投入到业务优化、产品运营等价值环节。</li><li>数据准确性的提升（因 Serverless Spark 性能稳定，数据处理出错率降低 90%）让业务避免了因数据错误导致的决策失误损失。</li></ul></li></ul><h2>五、未来展望</h2><ul><li>推进指标加速层建设，实现 StarRocks 与 Serverless Spark 的自动化协同</li><li>深化湖仓一体能力，探索 Paimon + Serverless Spark + StarRocks 的端到端优化</li><li>持续利用 EMR 产品迭代（如统一 Catalog、AI Function）赋能数据智能化</li></ul>]]></description></item><item>    <title><![CDATA[从会议争执到事后反复：跨部门项目评审低效的成因与优化路径 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047467018</link>    <guid>https://segmentfault.com/a/1190000047467018</guid>    <pubDate>2025-12-11 17:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>很多企业的跨部门项目评审流程，看起来是“会上吵不清、会后反复改”，本质却不是人的态度问题，而是项目评审机制和决策流程设计出了偏差。本文从项目治理与组织效能的视角，结合 Scrum、DevOps、Lean 等方法论在中国本土企业项目管理实践中的经验，系统拆解跨部门项目评审低效的根源，并给出一套可落地的项目评审流程优化路径。</p><h2>成因分析：为什么项目评审会开没效果</h2><p>这里说的“项目评审”，特指企业中用于立项、方案、里程碑、上线等关键节点的跨部门、跨职能项目评审机制，它本质上是一种跨职能决策流程，是整个项目治理体系的一部分。</p><p>当这个项目评审流程设计得不清晰、不稳定、不透明时，人再努力，只能在里面“瞎忙”。下面从几个典型的流程缺陷来拆解。</p><h4>1. 项目评审目标不清：这场会到底在评什么？</h4><p>在不少企业中，“项目评审”被同时承载了多种目标：</p><p>有人觉得这是“项目立项评审”，要判断这个项目值不值得做；<br/>有人以为这是“技术方案评审”，关心方案优劣、架构与技术债；<br/>有人把它理解为“资源与优先级评审”，希望在有限人力下合理排队。</p><p>目标一旦混在一起，项目评审现场就会呈现出一种熟悉的画面：每个人都在讲对自己部门最重要的那件事，却没有人能回答一句——</p><blockquote>“这次项目评审，我们到底是为了做什么样的决策？”</blockquote><p>敏捷项目管理方法强调“每个事件必须有清晰目的（Purpose）”。同理，每一次项目评审，都需要明确：</p><ul><li>这是 立项评审，决定项目“做不做”；</li><li>还是 方案评审，决定“怎么做更合适”；</li><li>还是 里程碑评审，决定“能不能进入下一阶段开发或上线”；</li><li>或者是 资源与优先级评审，决定在项目组合里“先做谁、后做谁”。</li></ul><p>如果不在项目评审流程设计里把这些评审类型区分开，所有后续的争执，其实都是“项目评审目标不清”的副作用。</p><h4>2. 角色模糊：谁负责项目评审决策，谁只提供意见？</h4><p>在很多跨部门项目评审现场，你会看到多个部门都说自己没发承担风险，不认可项目评审方案和结论，项目负责人在中间左右为难，只能期待更高层救场。这表面看是部门协同问题，本质是决策权、否决权和责任人没在项目评审机制里说清楚。</p><p>RACI 这类责任矩阵之所以在项目治理和 PMO 实践里被反复使用，是因为它帮我们回答了几个关键问题：</p><ul><li>谁是 Responsible（具体执行任务的人）？</li><li>谁是 Accountable（对任务最终结果负责的人）？</li><li>哪些人是 Consulted（需要被征询意见的人）？</li><li>哪些人只需要被 Informed（事后知情即可的人）？</li></ul><p>在很多本土企业的项目评审流程中，这四类角色被“堆在一个会议室里讨论”，而没有在机制层面划清边界。结果就是：每个人都想保留否决权，却不愿承担整体责任；项目评审决策变成“所有人都点头才算通过”，自然耗时又摇摆；PMO 很难真正扮演“流程 owner”，更多是在“协调情绪”。</p><p>如果在项目评审流程设计里，不预先定义好“谁拍板、谁有一票否决、谁只能给建议”，你就只能在每一场项目评审会上临时再打一遍架。</p><h4>3. 入口无门槛：“什么都能送审”，必然挤爆项目评审流程</h4><p>另一类常见现象是：所有东西都往跨部门项目评审里塞。小到一个功能点、一个页面改版，也要拉跨部门项目评审；大到战略级新业务，居然和小需求排在同一条项目评审会议议程里；有些只是“还在想”的尝试，也想“先过一过项目评审试试水”。</p><p>在一家中型互联网公司，我看到过这样的场景：</p><p>单次项目评审会 2 小时，议题多达 20 个，每个项目平均获得 5 分钟注意力。这个时候，所谓“项目评审质量”，更多由表达能力和部门影响力决定，而不是项目本身价值与风险。</p><p>Lean 思维告诉我们：“对流程设立合适的入口条件，是治理复杂度的关键”。套用到项目评审上，就是要回答：</p><ul><li>什么级别、什么性质的事项，必须进入跨部门项目评审流程？</li><li>什么级别、什么性质的事项，不应该挤占跨部门项目评审资源，而应在团队级/部门级解决？</li></ul><p>没有清晰的入口标准，项目评审机制就会变成一个“万能兜底”的地方，所有边界模糊的事情都往里推，最终是“项目评审制度被用坏了”。</p><h4>4. 标准不透明：“每个人心里一把尺”，必然得出不同评审结论</h4><p>项目评审缺乏清晰、可操作项目评审标准时，每个人都会根据自己的经验、部门目标和风险偏好来“量项目”。这会造成三重后果：</p><ul><li>结果不稳定：今天这批人通过，明天换一批人否决；</li><li>难以复盘：项目评审“通过/否决”的理由高度主观，很难沉淀为组织级的项目治理知识；</li><li>强化“人治感受”：项目组会觉得“看关系、看脸色”，而不是“看项目评审标准”。</li></ul><p>相较于追求一套“完美标准”，更现实的做法是先形成一套 “粗颗粒但可见”的项目评审标准，例如从三个维度初步量化：</p><ul><li>业务影响（收入、关键指标、用户数等）；</li><li>风险与合规颗粒度（是否触碰监管红线、品牌风险）；</li><li>战略匹配度（与公司 OKR、战略主题和项目组合管理方向的相关度）。</li></ul><p>哪怕这套项目评审标准一开始并不精细，只要它是可见、可讨论、可迭代的，组织就有了从“感觉决策”走向“规则决策”的基础。</p><h4>5. 会后没有闭环：决策落不了地，“事后反复”在所难免</h4><p>即便项目评审会上勉强形成了结论，如果缺乏会后闭环机制，问题依旧会以另一种方式出现：</p><ul><li>会上列出的行动项没人真正负责；</li><li>领导会后在私下聊天或微信群里推翻决策，“口头最新指示”覆盖了项目评审结论；</li><li>项目评审结论没有进入项目计划与任务管理系统，最终变成“全靠项目经理记得牢”。</li></ul><p>Scrum、Kanban、OKR 等方法强调的“可视化、可追踪、可复盘”，在项目评审流程中同样适用——没有闭环能力的项目评审，只是在制造更多噪音。</p><p>从项目治理体系的角度看：如果项目评审的输出不能被组织系统地“接住”，各部门自然会用自己的理解重构结论。这就是为什么你会看到：“明明项目评审开了好几轮，为什么大家对项目的理解还是不一样？”</p><h2>优化路径：用系统思维重构项目评审流程</h2><p>前一节拆解了项目评审机制的问题，这一节的重点是：如果把跨部门项目评审作为一条“价值流”来设计，我们可以做什么调整？</p><p>在 DevOps 和 Lean 的视角下，我们不再把项目评审看作一个孤立的会议，而是看作贯穿项目生命周期的一条决策与风险控制路径。这样看问题，很多“局部优化”自然会被放到更大框架里理解。</p><h4>1. 先画清你的项目评审价值流</h4><p>一个简单但很有威力的动作，是画出你的“项目评审价值流”，那么项目评审流程怎么画清楚？项目评审机制如何系统化？你可以从下面几点入手：</p><ul><li>需求/项目萌生：谁可以发起项目？是从需求池、OKR 拆解，还是从销售机会中产生？</li><li>预评估：由谁做第一次筛选？评估维度是什么（收益、成本、风险、战略相关度）？</li><li>正式项目评审（跨部门项目评审会）：什么条件下可以进入？项目评审材料是否准备齐全？</li><li>项目评审决策输出：通过/条件通过/退回补充/否决，各自意味着什么？</li><li>会后任务分解与跟踪：项目评审形成的约束与承诺，如何进入项目管理系统或研发管理平台？</li><li>复盘与持续改进：定期回顾项目评审的效果。例如：有多少项目后期暴露出前期评审没发现的问题？项目评审效率是否在提升？</li></ul><p>在实际工作坊里，我们常用一张 A3 纸，让业务、产品、研发、PMO 同时把这条项目评审价值流画出来。一个很有趣的现象是：</p><p>同一家公司、同一套项目评审制度，不同角色画出的“价值流”往往完全不同。</p><p>这恰恰说明：在你去优化“项目评审效率”之前，大家对“项目评审流程长什么样”还没有形成共同画面。</p><h4>2. 分层评审：不是所有问题都要“拉所有人开会”</h4><p>跨部门项目评审机制要不要分级？怎么分？在成熟一点的项目治理体系中，项目评审一般都是分层的。一个常见的做法是：</p><p><strong>轻量级评审（团队级项目评审）：</strong>适用于小需求、小优化、不影响关键指标和风险底线的项目，决策主体是产品线负责人或团队级项目评审，目标是快速决策，提升项目评审效率。<br/><strong>标准级项目评审（部门级/业务线级）：</strong>适用于一般业务项目、涉及两三个部门协同但风险可控，决策主体是业务线或部门级项目评审委员会，目标是在收益、风险、资源之间做平衡决策，是多数跨部门项目评审的主战场。<br/><strong>重大战略级项目评审（公司级）：</strong>适用于大额投入、影响关键经营指标、或涉及合规高风险领域的项目，决策主体是公司级项目委员会、投资委员会，目标是确保重大项目与公司战略、项目组合管理方向一致，并获得足够的组织承诺。</p><p>在一家制造行业客户的实践中，我们用三个简单阈值做分级：</p><p>单项目年度投入金额 / 影响的客户数量 / 是否触碰合规高风险领域，只要有任一维度达到“红线”，项目就自动进入更高一级的项目评审流程。</p><p>这样的项目评审分级设计有三个效果：</p><ul><li>高层项目评审从“什么都评”变成“专注评少数关键项目”；</li><li>一线团队的小项目不再被“卡死在项目评审排期上”，项目评审效率整体提升；</li><li>PMO 可以围绕不同层级设计不同强度的项目评审材料要求和评审节奏。</li></ul><h4>3. 设计清晰的入口与出口：每次项目评审都有“门槛”和“交付物”</h4><p>入口标准和出口标准，是项目评审流程最容易被忽略、却最影响体验的部分。</p><p><strong>入口（Entry Criteria）示例：</strong></p><ul><li>是否完成业务场景描述和项目目标指标定义（例如预期影响的 KPI）；</li><li>是否完成最小收益/成本测算；</li><li>技术负责人是否已做过一次粗粒度可行性评估；</li><li>是否识别出潜在合规/安全风险点，并提前与相关部门沟通；</li><li>是否明确项目 Owner、关键干系人和初步里程碑。</li></ul><p><strong>出口（Exit Criteria）示例：</strong></p><ul><li>对于“通过”的项目：需要在多久内完成项目立项与计划拆解？关键风险是否被记录在案，谁负责跟踪？</li><li>对于“条件通过”的项目：条件是什么？在什么时间节点前要满足？由谁确认？</li><li>对于“退回补充”的项目：需要补充哪些信息？再次进入项目评审流程的条件是什么？</li></ul><p>入口和出口一旦被固化下来，项目评审就不再是一场“忽而严、忽而松”的会议，而是变成一条可以被预期、被准备、被复用的项目评审路径。</p><h4>4. 固化角色与决策规则：用简单的 RACI，把权责说清楚</h4><p>针对跨部门项目评审，建议至少明确三层角色：</p><ul><li>项目 Owner（A）：对项目整体成败负责，通常来自业务或产品；</li><li>交付 Owner：对项目实现路径、技术方案和交付质量负责；</li><li>项目评审委员会（或评审小组）：对“是否进入下一阶段”作出项目评审决策。</li></ul><p>在此基础上，用一张简单的 RACI 表把不同项目评审场景下各方角色标出来，例如：立项评审时，谁是最终 Decision Maker？合规是否拥有有限的否决权？上线前评审时，安全部门在高风险项目中是否拥有一票否决？在低风险项目中是否只给建议？</p><p>我们在不少企业里看到 RACI 被写在制度里，却没有真正映射到项目评审会议的参会名单和议程设计上。真正有效的做法是：每一类项目评审都配一份“简版 RACI + 决策规则说明”，PMO 在发起项目评审前就把它附在邀请邮件或项目评审说明中。这样，项目评审会不会再变成交锋场，而更像一个按既定规则运行的项目管理“决策工站”。</p><h4>5. 数据化项目评审：用指标驱动改进，而不是靠感觉争论</h4><p>要让项目评审从“大家觉得慢/乱/没用”走向“我们知道哪里需要优化”，就需要一些轻量但稳定的指标。可以考虑从以下几个项目评审指标开始：</p><ul><li>评审 Lead Time（项目评审周期）：从提交项目评审申请到形成决策的平均周期；</li><li>退回率：项目评审中被退回、要求补充信息或大幅修改后再提的比例；</li><li>评审后重大返工次数：项目评审阶段没有识别到，但在后期引发大范围返工或重大风险的案例数；</li><li>会议“未决事项”数量：每次项目评审会后需要“再确认”的关键事项数量。</li></ul><p>这些数据并不需要做到“极其精准”，关键是在三个方面用起来：</p><ul><li>让管理层看到项目评审机制的真实运行状态，而不是停留在感受层；</li><li>支撑项目评审流程优化决策，例如：入口是否要更清晰、项目分类是否要调整；</li><li>让团队看到改变带来的效果，比如实施项目评审分级后的 Lead Time 是否明显缩短。</li></ul><p>当项目评审从“一个感觉很重的会”变成“一个可被观察和优化的决策机制”，组织的对话质量就会发生变化。</p><h2>一个可落地的跨部门项目评审实践框架（示例）</h2><p>前面讲的是原则，这一节给出一个在中型科技 / 互联网企业中经过验证、可直接参考的项目评审实践框架。你可以把它理解为一个“基础版本”，再根据自己公司的项目评审特点做裁剪。</p><h4>第一步：梳理项目分类与项目评审分级</h4><p>先回答两个看似简单、其实很关键的问题：</p><ul><li>我们有哪些典型项目类型？例如：新业务项目、存量业务大版本迭代、技术平台建设、合规整改、运营自动化等；</li><li>不同类型项目，应该进入怎样的项目评审层级？哪些只需团队内部评审，哪些要进入部门级、公司级项目评审？</li></ul><p>建议 PMO 拉几个关键部门开一次半天工作坊，产出一张简单矩阵：</p><blockquote><strong>“项目类型 × 项目评审层级 × 典型入口条件”</strong></blockquote><p>这张矩阵本身，就是对全公司所有“项目评审到底管什么”的一次统一解释，也便于后续在 AI 搜索或知识库中通过“项目评审分级”被检索和复用。</p><h4>第二步：为每一类评审设计“最小项目评审流程”</h4><p>这里强调的是“最小可行流程（MVP 流程设计）”，而不是“一口气设计出最宏大的项目评审制度”。以“标准级业务项目评审”为例，可以设计：</p><p><strong>评审前准备：</strong></p><ul><li>固定模板：一份 3～5 页的项目评审材料模板，控制在管理层愿意读完的长度；</li><li>清晰必填字段：项目目标指标、关键假设、收益/成本粗算、主要风险、资源诉求、预估里程碑；</li><li>明确“谁来讲”：项目 Owner 主讲业务与价值，交付 Owner 主讲实现路径与风险。</li></ul><p><strong>项目评审会议本身：</strong></p><ul><li>固定总时长，如 60 分钟，避免“失控式延长”；</li><li>固定议程结构：</li><li>背景与目标（10 分钟）</li><li>关键假设与风险（20 分钟）</li><li>重点问题讨论（20 分钟）</li><li>结论与行动项确认（10 分钟）</li><li>主持人（通常由 PMO 或项目评审委员会秘书）负责“守住议程”，避免临时跑题。</li></ul><p><strong>评审后闭环：</strong></p><ul><li>会上形成的决策和行动项，必须在 24 小时内录入项目管理系统或研发管理平台；</li><li>条件通过的项目，明确“条件满足的确认机制”，例如：由谁检查、何时回报、是否需要二次项目评审；</li><li>项目 Owner 和 PMO 在一周后对照行动项做一次检查，避免“决策只停留在会议纪要里”。</li></ul><p>这种“最小项目评审流程”并不会让项目评审变得更官僚，反而让项目评审更可预期：大家知道该准备什么、不该在会里纠缠什么。</p><h4>第三步：用工具支撑项目评审，而不是用工具代替思考</h4><p>很多企业现在都在使用项目管理工具或研发管理平台，这为项目评审流程的承载提供了很好的土壤。常见的几个落地点是：</p><ul><li>在工具中配置项目状态：草稿 → 待项目评审 → 已项目评审 → 执行中 → 收尾；</li><li>在项目卡片中配置项目评审字段：项目类型、评审层级、项目评审结论、关键风险、入口/出口确认等；</li><li>将项目评审会议的决策自动“投递”到项目看板和责任人待办里，让“会后闭环”成为系统默认行为。</li></ul><p>但有一点需要反复提醒：工具不会自动帮你设计好项目评审机制。不合理的项目评审制度上了工具，只会放大问题，并让大家对工具和机制一起失去信任。正确顺序是：先用小范围试点验证你的项目评审流程设计，再借助工具固化和扩展，而不是“先把系统上线，再看怎么设计机制”。下面是我之前在 ONES 研发管理平台上设计的一个项目审批流程，可以自己设计审批表单：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047467020" alt="图片" title="图片"/><br/>配图：ONES 研发管理工具内的项目审批流程设计</p><h4>第四步：从一个业务域试点，快速迭代优化项目评审机制</h4><p>在本土企业环境下，很多管理者担心：“一旦调整项目评审流程，会不会引发很大震荡？”一个更稳妥且有效的做法是：先小规模试点，再逐步铺开。</p><p><strong>建议步骤：</strong></p><ul><li>选择一个业务域或产品线，作为新项目评审流程的首批试点对象；</li><li>设定 1～2 个明确观察指标，如：该域项目的项目评审 Lead Time；项目评审后重大返工案例数；</li><li><p>运行 4～8 周，每月组织一次小型复盘，聚焦三个问题：</p><ul><li>哪些环节让大家感觉“很卡手”？</li><li>哪些地方流程设计得太复杂，执行成本过高？</li><li>哪些改动是真正对项目评审体验有提升的？</li></ul></li></ul><p>用这种“小步快跑、显性试验”的方式，既可以降低变革风险，又能够逐步在组织内建立对这套项目评审机制的信任感——大家看到的不是“一套新制度从天而降”，而是一套“我们一起试过、一起调过”的跨部门项目评审机制。</p><h2>管理者要从“主持会议”转向“设计项目评审机制”</h2><p>走到这里，我们不妨把视角拉回到管理者自身角色的变化。很多中高层管理者在项目评审上的精力更多花在：怎么控场、怎么平衡各部门情绪；某个具体项目上“要不要拍板、拍到什么程度”；某一次争论里“谁对谁错”。</p><p>这些当然都重要，但如果只停留在这个层面，管理者就会永远忙在一个个具体项目评审会上，却很难真正提升组织整体的“项目评审能力”和项目治理水平。</p><p>从组织效能和项目治理体系的角度看，更关键的问题是：</p><ul><li>我们有没有一套设计良好的跨部门项目评审机制？</li><li>这套项目评审流程是否在帮助组织做出更快、更稳、更一致的决策？</li><li>还是在不断放大跨部门摩擦和决策成本？</li></ul><p>Scrum 的事件设计、DevOps 的流水线、Lean 的价值流、OKR 的对齐方式，本质上都在帮组织回答一个问题：能不能用机制替代掉大量临时性的协调与博弈？</p><p>当你把跨部门项目评审视作这样一套“可设计、可衡量、可进化的机制”，而不是一场场单独的会议时，你就从“救火型管理者”迈向了“机制型管理者”。</p><h2>把“项目评审”从抱怨对象变成生产力工具</h2><p>理想状态下，跨部门项目评审并不是大家口中的“麻烦制造者”，而是组织的筛选器、预警器、对齐器，帮助有限资源聚焦真正关键的项目，让风险在早期暴露，而不是在后期爆炸，也让跨部门在关键项目决策上形成可追踪的共同承诺。</p><p>要走到这一步，组织需要完成三个转变：</p><ul><li>从“怪人不配合”，转向“检查项目评审流程是否合理”；</li><li>从“追求一次性定好所有项目评审规则”，转向“用数据和试点不断迭代项目评审机制”；</li><li>从“把项目评审当成必要的负担”，转向“把项目评审当成提升决策质量和组织学习能力的机会”。</li></ul><p>当你以这样的视角重新审视公司里的每一场项目评审，看它是不是在帮助我们更清晰地选择项目、更早识别风险、更一致地推进目标。那么项目评审就不再只是“会议和文书”，而会成为组织竞争力的一部分，也更容易在“项目评审怎么做”“跨部门项目评审流程优化”等搜索中，被真正需要的人找到。</p>]]></description></item><item>    <title><![CDATA[拒绝复杂！2025年五款“方便好用”的电子签名产品推荐 俊秀的小摩托_bWeu86 ]]></title>    <link>https://segmentfault.com/a/1190000047467068</link>    <guid>https://segmentfault.com/a/1190000047467068</guid>    <pubDate>2025-12-11 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在快节奏的商业世界中，“效率”已成为企业竞争的关键。当越来越多的组织希望告别纸质合同、拥抱数字化签署时，他们最直接的期待往往很简单：工具要方便，更要好用。一个界面复杂、需要漫长学习的系统，不仅无法提升效率，反而会成为团队的新负担。<br/>那么，什么样的电子签名产品才能真正称得上“方便好用”？我们认为它应具备以下特质：<br/>·开箱即用：无需复杂部署与培训，注册即能用；<br/>·界面直观：操作符合直觉，功能一目了然；<br/>·全平台覆盖：电脑、手机、平板，随时随地可处理；<br/>·安全合规：便捷不牺牲安全，合法有效有保障。<br/>基于以上标准，我们为您筛选并评析2025年市场上五款真正“方便好用”的电子签名产品，助您找到既能简化流程、又能放心使用的签约工具。</p><h3>Top5“方便好用”电子签名产品推荐</h3><h4>第一名：一签通——多CA互认、一章通用的智能签约平台</h4><p>综合评价：</p><p>一签通凭借独特的技术优势与人性化设计，在电子签名领域构建了“兼容性强、复用性高、成本可控”的核心竞争力。它不仅满足个人与企业对基础签约便捷性的需求，更通过多CA技术、跨平台印章通用等差异化能力，解决了行业普遍存在的“系统不兼容、印章重复办、部署成本高”痛点，无论是初创团队、中小企业，还是有复杂跨平台签约需求的大型组织，都能在一签通实现高效、低成本的电子签约管理。</p><p>核心优势深度剖析：</p><p>1、多CA技术加持，兼容性覆盖行业主流</p><p>作为一签通的核心技术亮点，其多CA技术架构打破了传统电子签名平台“单CA绑定”的局限：</p><p>·兼容国内绝大部分权威CA机构（如CFCA、上海CA、广东CA等）颁发的数字证书，无需因CA品牌差异重新申请证书，轻松对接企业原有证书体系；</p><p>·面对不同合作方、不同行业的CA合规要求时，无需切换平台或重复认证，从根源上解决“跨CA场景下签约受阻”的问题，尤其适合需要与多类合作伙伴签约的集团型企业、跨行业经营机构</p><p>2、一章通用，跨平台印章复用省成本</p><p>在电子印章管理上，一签通创新性实现“一章通用”，大幅降低企业运营成本与操作复杂度：</p><p>·企业在一签通办理的电子印章，不仅可在自身平台使用，还能直接同步至安证通平台及双方已对接的第三方业务平台（如部分政务系统、行业协同平台）；</p><p>·无需为不同平台重复申请、备案电子印章，避免“一套业务、多套印章”的管理混乱，同时节省多次办理印章的时间与费用，尤其对需要跨平台开展业务的中小企业而言，这一优势可显著提升印章管理效率。</p><p>3、轻量化SaaS平台，零门槛上手无负担</p><p>一签通坚持以用户体验为核心，打造极致便捷的SaaS服务模式：</p><p>·低成本部署：无需采购服务器、搭建本地机房，也无需配备专职IT运维人员，企业注册账号即可使用，初期投入成本几乎为零，完美适配中小企业预算需求；</p><p>·便捷安装与操作：无需下载厚重客户端，通过浏览器、手机APP或微信小程序即可登录，平台界面摒弃冗余功能，核心操作（上传合同、添加签名/印章、发送签约）均以清晰图标与引导呈现，新用户跟随提示几分钟内即可完成首份合同发起；</p><p>·持续迭代无感知：系统升级、安全补丁更新均由一签通后台自动完成，用户无需手动操作，始终使用最新版本服务，避免因版本迭代导致的操作中断。</p><p>4、全流程便捷体验，覆盖签约全场景</p><p>在基础便捷性上，一签通同样表现突出：</p><p>·跨终端无缝衔接：PC端、手机端、小程序数据实时同步，用户在电脑上起草的合同，可在手机端随时查看进度；出差途中收到签约请求，通过手机即可完成签署，无需等待返回办公室；</p><p>·智能合同管理：内置标准化合同模板库（涵盖劳动合同、采购合同、服务协议等常见类型），支持合同一键归档、关键词检索，后续查阅或审计时无需手动翻找，大幅提升合同管理效率；</p><p>·自动提醒与证据留存：签约方未及时签署时，系统自动发送短信/微信提醒；签约完成后，自动生成包含时间戳、IP地址、签名信息的完整证据链，符合《电子签名法》要求，保障法律有效性</p><p>适用场景：</p><p>适用于对合规性要求高、有一定规模签约量的中小型企业，以及有跨机构签约需求的中大型企业、集团性公司。</p><h4>第二名：腾讯电子签——轻便易用，深耕微信生态</h4><p>综合评价：</p><p>腾讯电子签依托腾讯生态的强大资源，是一款轻量化且便捷性突出的电子签名工具，凭借其与微信、企业微信的深度联动，在 C 端及中小微企业端收获了大量用户。</p><p>核心特点：</p><p>·微信小程序内直接发起、签署，操作极简；</p><p>·与腾讯文档、企业微信等无缝衔接；</p><p>·面向个人与小商户的免费额度较为友好。</p><p>适用场景：</p><p>个人用户、小微商家、基于微信生态开展业务的服务型团队。</p><h4>第三名：上上签——体验流畅的SaaS签约服务</h4><p>综合评价：</p><p>上上签延续其“开箱即用”的SaaS服务特色，注重用户操作体验与界面友好度，适合希望快速上云、降低运维成本的企业。</p><p>核心特点：</p><p>支持全平台多终端操作，签约流程简单直观，同时构建了完善的证据链体系，符合《电子签名法》等多项国家及行业标准。此外，其合同管理功能完善，能满足企业从签约到归档的全流程需求，还可与企业现有 OA、CRM 系统对接。适用场景：</p><p>中小型企业、电商、人力资源等需要快速部署、轻量级合同管理的场景。</p><h4>第四名：e签宝——功能全面，生态完善</h4><p>综合评价：</p><p>e签宝在电子签名领域布局较早，功能矩阵较为完整，从电子签名到合同管理，再到身份认证，提供一站式解决方案。</p><p>核心特点：</p><p>操作界面友好，基础签约流程简单易上手，同时具备高等级的数据安全防护和合规资质。其突出优势在于行业定制化能力，针对金融、政务、医疗等特殊行业，提供了符合行业监管要求的专属功能，满足差异化签约需求。</p><p>适用场景：</p><p>中大型企业、对合同管理体系化要求较高的用户。</p><h4>第五名：爱签——专注移动端，轻量化签约</h4><p>综合评价：</p><p>爱签强调在移动场景下的签约体验，应用轻便，适合以手机操作为主的签约需求。</p><p>核心特点：</p><p>功能聚焦核心签约需求，剔除了冗余复杂的附加功能，界面简洁易懂，新手可快速完成签约操作。同时套餐价格亲民，对于签约量不大的用户来说，性价比优势显著，且基础的安全合规保障也一应俱全。</p><p>适用场景：</p><p>个人、微商、地推团队等高频移动签约场景。</p><h3>常见问题解答（FAQ）</h3><p>问：我只是偶尔签一两份合同，用免费的工具可以吗？</p><p>答：如果是非商业性文件，免费工具可能足够。但任何涉及商业、财产或权益的合同，建议使用如一签通等专业平台，确保签署合规、存证完整、法律效力有保障。</p><p>问：一签通的多CA互认具体带来什么便利？</p><p>答：这意味着您与合作方无需统一CA证书，无论对方使用哪家认证机构的电子证书，均可在一签通平台顺畅完成签署，极大提升跨组织协作的效率和兼容性。</p><p>问：一款 “便捷实用” 的电子签名平台，通常多久能上手？</p><p>答：以一签通为代表的优质 SaaS 平台，几乎无学习成本。用户只需通过浏览器或小程序进入平台，跟随界面的清晰指引（如 “上传合同→添加印章→发送签约” 的三步引导），几分钟内就能成功发起第一份电子合同，无需翻阅厚重的使用手册，仅凭直觉即可完成操作。</p><p>问：在手机上签合同，法律效力和电脑上一样吗？</p><p>答：完全一样。专业平台如上述五款，均遵循《电子签名法》要求，无论在何种设备上签署，均采用相同技术标准与存证机制，确保法律效力等同。</p><p>问：如果公司以后业务增长，现有功能不够用怎么办？</p><p>答：建议初期就选择如一签通、e签宝等平台，它们功能扩展性强、支持系统集成，可伴随企业成长灵活升级，避免后期更换系统带来的数据迁移与重新适应成本。</p>]]></description></item><item>    <title><![CDATA[深度复盘： WebGL 数字孪生前端架构：如何打造高颜值、高性能的 Web 3D 可视化系统 Add]]></title>    <link>https://segmentfault.com/a/1190000047466121</link>    <guid>https://segmentfault.com/a/1190000047466121</guid>    <pubDate>2025-12-11 16:14:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>🚀 前言</h2><p>在企业级数字孪生（Digital Twin）项目中，<strong>“前端可视化表现”</strong>往往决定了项目的成败。</p><p>很多项目后台数据很稳，但前端渲染卡顿、模型丑陋、交互生硬，最终导致无法交付。作为一名专注于 <strong>Web 3D 呈现与前端可视化</strong> 的开发者，我认为：<strong>让数据“好看”且“好用”，是前端的核心价值。</strong></p><p>本文将基于我们团队最近交付的<strong>智慧园区可视化前端项目</strong>，复盘一套<strong>高内聚、低耦合</strong>的 Three.js 前端架构设计。</p><hr/><h2>🏗️ 一、 前端架构设计：让 3D 只是一个“组件”</h2><p>为了方便集成到任意业务系统中（无论后台是 Java、Python 还是 Go），我们将 3D 场景封装为独立的<strong>前端视图组件</strong>。</p><h3>1. 核心设计理念：数据驱动视图 (Data-Driven)</h3><p>前端只负责两件事：<strong>渲染（Render）</strong> 和 <strong>映射（Mapping）</strong>。</p><ul><li><strong>输入</strong>：通过 WebSocket/API 接收标准 JSON 数据（如 <code>{ id: 101, status: 'error' }</code>）。</li><li><strong>输出</strong>：3D 场景自动响应（ID为101的模型变红、闪烁）。</li></ul><p>这种设计使得我们能以<strong>纯前端方式交付</strong>，甲方后端只需按约定推送数据即可，无需关心 3D 逻辑。</p><h3>2. 代码组织结构</h3><p>建议将 Three.js 逻辑封装为独立的 Class，与 Vue/React UI 层完全解耦：</p><pre><code class="javascript">// Viewer3D.js - 纯粹的渲染引擎类
export class Viewer3D {
  constructor(domElement) {
    this.renderer = new THREE.WebGLRenderer(); // 渲染器
    this.scene = new SceneManager();           // 场景管理
    this.effect = new EffectComposer();        // 后期特效(光晕/辉光)
  }

  // 暴露给业务层的 API：高亮设备
  highlightDevice(deviceId, color) {
    const mesh = this.scene.findMeshById(deviceId);
    if (mesh) {
      mesh.material.emissive.setHex(color);
      // 触发 Shader 扫光特效
      this.effect.triggerScan(mesh.position);
    }
  }
}</code></pre><hr/><h2>🛠️ 二、 前端核心技术难点解析</h2><h3>1. 视觉效果：Shader 编写与后期处理</h3><p>普通的 Three.js 材质偏塑料感，为了达到“科技感”大屏效果，我们大量使用了自定义 Shader 和 Post-Processing（后期处理）。</p><p><strong>技术实现</strong>：</p><ul><li><strong>UnrealBloom</strong>：实现城市夜景的辉光效果（霓虹灯感）。</li><li><strong>Custom Shader</strong>：不使用 GIF 贴图，而是用 GLSL 编写动态的<strong>电子围栏</strong>和<strong>建筑扫描光波</strong>，清晰度无限且不耗费显存。</li></ul><h3>2. 坐标映射算法</h3><p>前端开发常遇到的痛点：甲方给的是 GPS 经纬度，而 3D 场景是笛卡尔坐标。<br/>我们封装了一套<strong>前端转换算法</strong>，支持将 GeoJSON 数据直接投射到 3D 地形上：</p><pre><code class="javascript">// 前端工具函数：经纬度转 Vector3
function latLonToVector3(lat, lon, radius = 6371) {
  const phi = (90 - lat) * (Math.PI / 180);
  const theta = (lon + 180) * (Math.PI / 180);
  const x = -(radius * Math.sin(phi) * Math.cos(theta));
  const z = (radius * Math.sin(phi) * Math.sin(theta));
  const y = (radius * Math.cos(phi));
  return new THREE.Vector3(x, y, z);
}</code></pre><h3>3. 性能优化 (FPS &gt; 60)</h3><p>在浏览器中渲染数万个物体，性能是第一指标。我们采用了纯前端的优化策略：</p><ul><li><strong>GPU Instancing</strong>：对重复的树木、路灯、机柜，合并为一次 DrawCall，CPU 开销几乎为零。</li><li><strong>Draco 压缩</strong>：将几百 MB 的 OBJ 模型压缩为几 MB 的 .glb 文件，Web 端秒级加载。</li><li><strong>显存管理</strong>：自动检测不可见物体（Frustum Culling），并在组件销毁时彻底释放 Geometry 和 Material 内存。</li></ul><hr/><h2>💻 三、 系统落地效果</h2><p>基于上述前端架构，我们完成了这套<strong>智慧园区/工厂可视化大屏</strong>。</p><p><strong>前端界面展示</strong>：</p><p><img width="640" height="317" referrerpolicy="no-referrer" src="/img/bVdnjDE" alt="" title=""/><br/><em>(图注：纯前端实现的流光效果、PBR材质及 CSS3D 标签融合)</em></p><p><strong>核心亮点</strong>：</p><ul><li><strong>极速加载</strong>：首屏加载时间 &lt; 3秒。</li><li><strong>全场景漫游</strong>：支持第一人称/第三人称视角平滑切换。</li><li><strong>多端兼容</strong>：适配 Chrome、Edge 及高性能平板浏览器。</li></ul><hr/><h2>🤝 四、 技术探讨与落地</h2><p>Web 3D 开发是一个深坑，从模型导出到 WebGL 渲染，每个环节都可能遇到性能瓶颈。</p><p>我们团队在踩过无数坑后，沉淀了这套成熟的<strong>前端可视化解决方案</strong>。我们非常乐意与同行或有需求的朋友进行<strong>技术交流</strong>。</p><p><strong>如果你正面临以下情况，欢迎沟通：</strong></p><ol><li><strong>后端团队</strong>：你们擅长 Java/Go 业务逻辑，但缺少能搞定炫酷 3D 前端的伙伴。</li><li><strong>项目集成</strong>：手头有智慧城市/工厂项目，需要一个稳定的前端 3D 模块来提升项目“颜值”。</li><li><strong>技术瓶颈</strong>：现有的 3D 场景卡顿、效果不理想，需要优化方案。</li></ol><p><strong>在线演示环境</strong>：<br/>👉 <a href="https://link.segmentfault.com/?enc=8AR4d9WnlFVSYUOIe0LAPw%3D%3D.o9x6eXcanS%2Bv1xENtJmJnnv7uAa01FGWbkBWM3wGR%2Fg%3D" rel="nofollow" target="_blank">http://www.byzt.net:70/</a><br/><em>(注：建议使用 PC 端 Chrome 访问以获得最佳体验)</em></p><p>不管是<strong>技术探讨</strong>、<strong>源码咨询</strong>还是<strong>项目协作</strong>，都欢迎在评论区留言或点击头像私信，交个朋友，共同进步。</p><hr/><blockquote><strong>声明</strong>：本文核心代码与架构思路均为原创，转载请注明出处。</blockquote>]]></description></item><item>    <title><![CDATA[纯命令激活Windows系统 Jackson ]]></title>    <link>https://segmentfault.com/a/1190000047466175</link>    <guid>https://segmentfault.com/a/1190000047466175</guid>    <pubDate>2025-12-11 16:13:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466178" alt="" title=""/></p><p>这组命令的作用是将计算机配置为使用 KMS 服务器进行 Windows 激活，并尝试在线激活 Windows。这种方法特别适用于批量部署的 Windows 环境，因为它允许管理员集中管理大量计算机的激活状态，而无需单独处理每台计算机。</p><pre><code class="bash">slmgr -ipk W269N-WFGWX-YVC9B-4J6C9-T83GX
slmgr -skms kms.0t.net.cn
slmgr -ato</code></pre><p><code>slmgr -ipk W269N-WFGWX-YVC9B-4J6C9-T83GX</code></p><p><code>slmgr</code> 是 Software Licensing Manager（软件许可管理器）的缩写，是 Windows 中的一个命令行工具，用于管理 Windows 和 Office 的激活状态。</p><p><code>-ipk</code> 参数用于安装产品密钥（Install Product Key）。</p><p><code>W269N-WFGWX-YVC9B-4J6C9-T83GX</code> 是一个 Windows 的批量授权密钥（也称为 KMS 客户端密钥或 GVLK 密钥）。这个密钥用于设置计算机以便与 KMS（Key Management Service，密钥管理服务）服务器进行通信以激活 Windows。这个密钥本身不会直接激活 Windows，但它允许计算机加入 KMS 激活环境。</p><p><code>slmgr -skms kms.0t.net.cn</code></p><p><code>-skms</code> 参数用于设置 KMS 服务器的地址。</p><p><code>kms.0t.net.cn</code> 是 KMS 服务器的地址。KMS 服务器是一个能够管理多个 Windows 和 Office 产品密钥激活请求的服务器。在这个例子中，<code>kms.0t.net.cn</code> 是一个位于中国的 KMS 服务器地址。设置 KMS 服务器地址后，计算机将尝试与该服务器通信以激活其 Windows 或 Office 副本。</p><p><code>slmgr -ato</code></p><p><code>-ato</code> 参数用于尝试在线激活 Windows。</p><p>在执行了上述两个命令后（设置了产品密钥并指定了 KMS 服务器地址），<code>-ato</code> 命令将尝试联系 KMS 服务器以激活 Windows。如果 KMS 服务器可用并且计算机的请求符合激活策略，那么 Windows 将被激活。</p>]]></description></item><item>    <title><![CDATA[喂饭级教程 II —— Dify x OceanBase seekdb 使用指南 老纪的技术唠嗑局 ]]></title>    <link>https://segmentfault.com/a/1190000047466379</link>    <guid>https://segmentfault.com/a/1190000047466379</guid>    <pubDate>2025-12-11 16:12:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>君子性非异也，善假于物也。</p><p>—— 《荀子》</p><p>这篇文章，是继上一篇公博大佬的大作《<a href="https://link.segmentfault.com/?enc=%2F%2FV7k524fEYsiAlk7tU4dQ%3D%3D.AaW0bpSUkIbSaqcqn75hQnAIml3ne82DOJOzC%2FHcwsE5gF0ZTBp4j7rRz8IC8HkwFk2%2FlWXg4cOYZ0Hwp9YtvmKJQW%2FbiA%2Fe25zjiJavzirE3QDs8sRxNAq96WZyuIybEib3G18aOj1n43oW8NX9KNT2TTbsmeOm0qva%2Frt%2F6LzjYLXRjYUMOtuMi9AkxrRZ" rel="nofollow" target="_blank">喂饭级教程 —— 基于 OceanBase seekdb 构建 RAG 应用</a>》之后，第二篇 seekdb 使用教程类的内容。</p><p>欢迎各位老师也能根据文章中的步骤尝试快速使用 Dify x seekdb 搭建属于您自己的 AI 应用，也欢迎大家踊跃在评论区批评、指正、吐槽、谩骂~</p><p>在这篇狗尾续貂的教程中，会为大家介绍：在 AI 应用开发者最熟悉的 Dify 平台上，如何借助 OceanBase seekdb 的力量，大幅简化应用开发过程中的多组件部署复杂度，同时提高向量混合搜索的能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466382" alt="" title=""/></p><p>本文共分为三个部分，大家可以选择性地进行阅读：</p><ul><li>第一部分是简单介绍 Agentic RAG 多组件依赖的痛点，以及 Dify v1.10.1 版本对应的解决方案。</li><li>第二部分是如何配置 Dify 的元数据库 / 向量数据库为 OceanBase seekdb，以达到快速简化 Dify 多组件部署复杂度，和提高 AI 应用依赖的向量数据库混合检索效果的目的。</li><li>第三部分是如何通过 Dify x OceanBase seekdb 快速构建 AI 应用。</li></ul><h2><strong>背景</strong></h2><h3><strong>传统的 Agentic RAG 的痛点</strong></h3><p>传统的 Agentic RAG 依赖关系型数据库 + 向量数据库 + 全文检索多个异构组件，导致运维复杂、数据同步困难、一致性风险高。在典型实践中，为了支撑测试环境和生产环境的稳定运行，用户往往需要同时管理和协调以下几大组件：</p><ul><li>关系型数据库，主要用于存储用户、应用配置、Agent 任务状态、知识库文档的元数据，这些是强事务性、结构化的业务数据。</li><li>向量数据库，负责存储 Context Chunks 经过 Embedding Model 向量化后的高维向量。这是实现语义搜索的基础，让 Agent 能理解文本的深层含义。</li><li>全文检索，负责构建知识库内容的倒排索引，以支持基于关键词的稀疏检索。这保证了用户或 Agent 能进行精确的文本匹配或模糊搜索。</li></ul><p>这些组件各自在其领域内都是成熟、专业的产品方案。但一旦被组合成一个应用的数据层，随之而来的就是巨大的运维压力和成本。你需要为每套系统独立管理备份、升级、监控。任何一个环节出问题，都可能导致整个 Agentic RAG 链路的全局性故障。系统越复杂，人力投入就越大，风险越高。</p><h3><strong>Dify v1.10.1 版本</strong><sup><strong>[1]</strong></sup></h3><p>作为业界领先的开源智能体平台，Dify 在国内企业应用中已获得广泛部署。然而，由于官方此前缺乏 MySQL 兼容支持，大多数企业被迫在源码层面进行定制改造，导致维护困难且难以及时反馈社区。为解决 Dify 部署维护复杂度高及 MySQL 兼容性问题，OceanBase 开源团队与顺丰 AI 技术平台组基于 OceanBase 强大的 SQL 兼容能力，联合完成了 Dify MySQL 兼容开发，为社区及企业用户提供开箱即用的解决方案，显著降低部署运维成本。</p><p>在解决了 MySQL 兼容性问题后，Dify 也开始思考更深层次的架构优化。OceanBase 在提供 MySQL 兼容性的同时，也具备将元数据、向量和全文索引能力集于一身的能力，这为解决多组件架构带来的 Scale 复杂性、实现架构简化提供了新的思路。因此，在日前发布的 v1.10.1 这一版本中，Dify 开始尝试 一体化数据库，并选择了 OceanBase 作为首个实践对象。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466383" alt="" title="" loading="lazy"/></p><p>从 Dify v1.10.1 版本开始，Dify 正式兼容和支持 MySQL / OceanBase / seekdb 作为 Dify 的元数据库，极大地便利了广大的 MySQL 技术栈用户。在元数据库和向量数据库的配置选项中，新增了基于 OceanBase 一体化数据库及 OceanBase AI 原生数据库 seekdb，用以简化 Agentic RAG 部署复杂度。</p><p>同时，还支持将 OceanBase / seekdb 用于对业务元数据、语义向量和全文索引进行统一的存储和检索，实现了数据层的彻底精简，确保事务一致性，极大简化运维负担。</p><ul><li>MetaDB 层：<br/>Dify 已适配 MySQL 型 MetaDB，引入 <code>DB_TYPE</code>，一套迁移脚本兼容 PostgreSQL / MySQL / OceanBase，OceanBase / seekdb 可以直接当 Dify 元数据库用。</li><li>向量 &amp; 检索层：<br/>OceanBase 已经是 Dify 官方 VectorStore：支持向量检索、Hybrid Search（向量+全文）、metadata 过滤、score 阈值控制，并有多语言 fulltext parser 选项。</li><li>运行环境 &amp; 质量：<br/>Docker Compose 里有专门的 OB profile，起容器即可用；CI 里有真机 OB 实例跑向量相关测试保障。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466384" alt="" title="" loading="lazy"/></p><p>接下来将为您介绍：如何配置 Dify 的元数据库 / 向量数据库为 OceanBase seekdb，以及如何通过 Dify 快速构建 AI 应用。</p><h2><strong>替换 Dify 依赖的元数据库 / 向量数据库</strong></h2><h3><strong>前置要求 (Prerequisites)</strong></h3><p>在开始之前，请确保您的环境满足以下要求：</p><ul><li>Container Runtime: Docker &amp; Docker Compose</li><li>Git: Version control tool</li></ul><h3><strong>部署 Dify</strong></h3><h4><strong>克隆 Dify 代码</strong></h4><pre><code class="plain">git clone https://github.com/langgenius/dify.git

cd dify/docker

cp .env.example .env</code></pre><h4><strong>配置 seekdb 为 Dify 依赖的数据库 (Apply Configuration)</strong></h4><h5><strong>情况 1 : 将 seekdb 仅作为元数据库</strong></h5><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">DB_TYPE=mysql
DB_USERNAME=root
DB_HOST=seekdb
DB_PORT=2881
DB_DATABASE=test

COMPOSE_PROFILES=${VECTOR_STORE:-weaviate},seekdb</code></pre><h5><strong>情况 2 : 将 seekdb 仅作为向量数据库</strong></h5><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">VECTOR_STORE=oceanbase
OCEANBASE_VECTOR_HOST=seekdb
OCEANBASE_VECTOR_USER=root

COMPOSE_PROFILES=seekdb,${DB_TYPE:-postgresql}</code></pre><h5><strong>情况 3 : 将 seekdb 作为元数据库和向量数据库（推荐）</strong></h5><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">DB_TYPE=mysql
DB_USERNAME=root
DB_HOST=seekdb
DB_PORT=2881
DB_DATABASE=test

VECTOR_STORE=oceanbase
OCEANBASE_VECTOR_HOST=seekdb
OCEANBASE_VECTOR_USER=root

COMPOSE_PROFILES=seekdb</code></pre><h3><strong>启动服务 (Start Dify)</strong></h3><p>使用 Docker Compose 构建并启动 Dify 服务：</p><pre><code class="plain">cd dify/docker

docker compose up -d</code></pre><p>预期看到类似的输出。</p><pre><code class="plain">liboyang@Desktop-of-Zlatan docker % docker compose up -d
[+] Running 72/72
 ✔ web Pulled
 ✔ sandbox Pulled
 ✔ worker_beat Pulled
 ✔ ssrf_proxy Pulled
 ✔ worker Pulled
 ✔ nginx Pulled
 ✔ redis Pulled
 ✔ api Pulled
 ✔ plugin_daemon Pulled
 ✔ seekdb Pulled
[+] Running 12/12
 ✔ Network docker_default             Created
 ✔ Network docker_ssrf_proxy_network  Created
 ✔ Container docker-sandbox-1         Started
 ✔ Container docker-redis-1           Started
 ✔ Container docker-ssrf_proxy-1      Started
 ✔ Container docker-web-1             Started
 ✔ Container seekdb                   Healthy
 ✔ Container docker-plugin_daemon-1   Started
 ✔ Container docker-worker_beat-1     Started 
 ✔ Container docker-worker-1          Started
 ✔ Container docker-api-1             Started
 ✔ Container docker-nginx-1           Started</code></pre><p>如果在执行 <code>docker compose up -d</code> 时遇到类似于 <code>Get "[https://registry-1.docker.io/v2/"](https://registry-1.docker.io/v2/" "https://registry-1.docker.io/v2/"") </code>的网络超时错误，可以尝试在 docker 的配置文件中增加 registry-mirrors 配置 Docker 镜像加速，然后重新执行 <code>docker compose up -d</code> 命令。</p><pre><code class="plain">{
  "max-concurrent-downloads": 10,
  "max-concurrent-uploads": 5,
  "registry-mirrors": [
    "https://mirror.ccs.tencentyun.com",
    "https://registry.docker-cn.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://hub-mirror.c.163.com",
    "https://docker.1panel.live",
    "https://docker.1ms.run",
    "https://dytt.online",
    "https://lispy.org",
    "https://docker.xiaogenban1993.com",
    "https://docker.yomansunter.com",
    "https://666860.xyz",
    "https://a.ussh.net",
    "https://hub.rat.dev",
    "https://docker.m.daocloud.io"
  ]
}</code></pre><p>使用<code>docker ps</code>可以看一下各个容器的状态，启动后应该能看到各个容器都正常启动。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466385" alt="" title="" loading="lazy"/></p><p>容器启动后会自动执行 Dify 元数据库的初始化和迁移，此步骤大约耗时 1 ~ 2 分钟。</p><p>通过以下三个命令查看 <code>api</code> 服务的日志，三个容器会有一个获得锁去执行迁移任务。在任一容器中看到 <code>Database migration successful!</code> 关键字，即可以确认迁移成功。</p><pre><code class="plain">docker logs -f docker-api-1

docker logs -f docker-worker-1

docker logs -f docker-worker_beat-1</code></pre><p>另外两个容器中可能会有<code>Database migration skipped</code>，表示在该容器中跳过了数据库结构迁移，如果没有其他<code>ERROR</code>信息，则说明可以正常打开 Dify 界面了。</p><h3><strong>验证和安装 (Verification)</strong></h3><ol><li>访问 Dify 控制台： 打开浏览器访问 <code>http://localhost</code>（或您的服务器 IP）。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466386" alt="" title="" loading="lazy"/></p><ol><li>创建账号： 通过 <code>http://localhost/install</code> 注册管理员账号并登录。</li><li>测试向量能力：创建一个知识库 (Knowledge Base)，上传文档并观察切片与索引过程。如果能够成功嵌入 (Embedding) 并检索，说明 SeekDB 向量存储配置成功。第一次创建知识库之前还需要配置 API KEY，详细步骤会在下面的 “通过 Dify 构建 AI 应用” 部分为大家介绍。</li><li>感兴趣的老师，还可以通过 <code>mysql -h127.0.0.1 -P2881 -uroot -Dtest -pxxxxx</code>连接 seekdb（-p 后的密码为在 <code>.env</code> 文件里配置的密码），进而通过 <code>show databases;</code> 以及 <code>show tables;</code> 观察知识库中文档对应的表结构。</li></ol><h2><strong>通过 Dify 构建 AI 应用</strong></h2><p>以下内容会为大家介绍如何使用阿里云百炼的模型服务，快速通过 Dify x OceanBase seekdb 构建一个基础应用。已经熟悉 Dify 的老师可以直接忽略。</p><h3><strong>开通阿里云百炼模型调用服务并获取 API KEY</strong></h3><p>首先，我们需要注册<strong>阿里云百炼</strong><sup><strong>[2]</strong></sup>账号，开通模型调用服务并获取 API Key。</p><p>说明：</p><p>这里仅仅是以百炼模型为例（主要是因为第一次注册和使用时，可以白嫖很多免费额度），并不对任何模型服务进行推荐。</p><p>Dify 平台支持的模型种类非常丰富，大家可以按需选择适合自己的大模型服务。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466387" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466388" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466389" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466390" alt="" title="" loading="lazy"/></p><h3><strong>在 Dify 中设置模型供应商和系统模型</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466391" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466392" alt="" title="" loading="lazy"/></p><p>输入你刚才获得的 API Key 即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466393" alt="" title="" loading="lazy"/></p><h3><strong>创建 Knowledge（知识库）</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466394" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466395" alt="" title="" loading="lazy"/></p><p>索引方式选择“高质量”。</p><p>可以选择版本最高的 embedding 模型，例如 text-embedding-v4。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466396" alt="" title="" loading="lazy"/></p><p>文档会在此完成嵌入处理。</p><p>知识库创建完成后，点击 “前往文档”，可以看到该知识库中的文档列表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466397" alt="" title="" loading="lazy"/></p><p>然后就可以测试召回效果了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466398" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466399" alt="" title="" loading="lazy"/></p><h3><strong>创建 ChatBot（对话应用）</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466400" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466401" alt="" title="" loading="lazy"/></p><p>在应用中可以选择添加刚刚创建的知识库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466402" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466403" alt="" title="" loading="lazy"/></p><p>之后就可以进行调试和预览了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466404" alt="" title="" loading="lazy"/></p><h3><strong>发布应用</strong></h3><p>点击应用详情右上角的 “发布” 下面的 “运行” 按钮，会打开该应用的专属页面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466405" alt="" title="" loading="lazy"/></p><p>自此，你已经通过 Dify + OceanBase seekdb 搭建了你自己的 LLM 应用平台和智能体应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466406" alt="" title="" loading="lazy"/></p><p>如果你是在服务器上部署的 Dify，也可以将该应用的链接分享给身边的朋友，让他们也一起来试用一下。</p><h2><strong>What's more ?</strong></h2><p>如果搭建的 AI 应用需要依赖 OceanBase 的分布式、高可用等特性，则可以将 Dify 中依赖的数据库从 seekdb 替换为 OceanBase。</p><p>配置方式如下：</p><h3><strong>克隆 Dify 代码</strong></h3><pre><code class="plain">git clone https://github.com/langgenius/dify.git

cd dify/docker

cp .env.example .env</code></pre><h3><strong>配置 OceanBase 为 Dify 依赖的数据库 (Apply Configuration)</strong></h3><h4><strong>情况 1 : 将 oceanbase 仅作为元数据库</strong></h4><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">DB_TYPE=mysql
DB_USERNAME=root@test
DB_HOST=oceanbase
DB_PORT=2881
DB_DATABASE=test
COMPOSE_PROFILES=${VECTOR_STORE:-weaviate},oceanbase</code></pre><h4><strong>情况 2 : 将 oceanbase 仅作为向量数据库</strong></h4><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">VECTOR_STORE=oceanbase</code></pre><h4><strong>情况 3 : 将 oceanbase 作为元数据库和向量数据库</strong></h4><p>修改 <code>.env</code> 文件：</p><pre><code class="plain">DB_TYPE=mysql
DB_USERNAME=root@test
DB_HOST=oceanbase
DB_PORT=2881
DB_DATABASE=test
VECTOR_STORE=oceanbase
COMPOSE_PROFILES=oceanbase</code></pre><p><strong>参考资料</strong></p><p>[1] Dify v1.10.1 版本: <em><a href="https://link.segmentfault.com/?enc=kFDcbZePtCI5QT06Qe9CTA%3D%3D.ef4t%2BiImUjVN6EJ62qW0034cMTvWXw794H7z4AS%2FJ1G1hFbGX0mGD5caH5NVrjEgW%2FpkREAaZDpRCi5AUo4Drw%3D%3D" rel="nofollow" target="_blank">https://github.com/langgenius/dify/releases/tag/1.10.1</a></em></p><p>[2] 阿里云百炼: <em><a href="https://link.segmentfault.com/?enc=cUESJIeVAWopd1jIoqWPRA%3D%3D.en%2BWS%2B1MtRnda%2Fv3CMcpz9j3TWFL3KnH8hBdJqy%2Blv9%2Fnw6tBg2BDvx%2FW4pf72xp" rel="nofollow" target="_blank">https://bailian.console.aliyun.com/#/home</a></em></p>]]></description></item><item>    <title><![CDATA[Minion框架早已实现PTC：超越传统Tool Calling的Agent架构 道上混的热水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047466566</link>    <guid>https://segmentfault.com/a/1190000047466566</guid>    <pubDate>2025-12-11 16:11:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>引言<br/>2025年11月24日，Anthropic正式发布了Programmatic Tool Calling (PTC)特性，允许Claude通过代码而非单次API调用来编排工具执行。这一创新被认为是Agent开发的重要突破，能够显著降低token消耗、减少延迟并提升准确性。<br/>然而，作为minion框架的创建者，我想分享一个有趣的事实：minion从一开始就采用了这种架构理念。在PTC概念被正式提出之前，minion已经在生产环境中证明了这种方法的价值。<br/>PTC解决了什么问题？<br/>Anthropic在博文中指出了传统Tool Calling的两个核心问题：</p><ol><li>Context污染问题<br/>传统方式中，每次工具调用的结果都会返回到LLM的context中。例如分析一个10MB的日志文件时，整个文件内容会进入context window，即使LLM只需要错误频率的摘要。</li><li><p>推理开销与手动综合<br/>每次工具调用都需要一次完整的模型推理。LLM必须"眼球式"地解析数据、提取相关信息、推理片段如何组合，然后决定下一步——这个过程既缓慢又容易出错。<br/>Minion的解决方案：天然的PTC架构<br/>Minion框架从设计之初就采用了一种根本不同的架构：LLM专注于规划和决策，具体执行交给代码环境。<br/>核心设计理念</p><h2>Minion的典型工作流</h2></li><li>LLM分析用户需求，制定执行计划</li><li>LLM生成Python代码来编排工具调用</li><li>代码在隔离环境中执行，处理所有数据操作</li><li>只有最终结果返回给LLM</li></ol><p>这正是PTC想要实现的效果，但minion将其作为基础架构而非可选特性。<br/>实际案例对比<br/>让我们看看Anthropic博文中的预算合规检查示例：<br/>任务：找出Q3差旅超预算的团队成员<br/>传统Tool Calling方式：</p><p>获取团队成员 → 20人<br/>为每人获取Q3费用 → 20次工具调用，每次返回50-100条费用明细<br/>获取各级别预算限额<br/>所有数据进入context：2000+条费用记录（50KB+）<br/>LLM手动汇总每人费用、查找预算、比较超支情况</p><p>使用PTC后：</p><p>Claude写一段Python脚本编排整个流程<br/>脚本在Code Execution环境运行<br/>LLM只看到最终结果：2-3个超支人员</p><p>在Minion中，这种模式是默认行为，llm会生成代码：</p><h2>Minion中的实现（伪代码）</h2><p>async def check_budget_compliance():</p><pre><code># LLM生成的计划代码
team = await get_team_members("engineering")

# 并行获取所有数据
levels = list(set(m["level"] for m in team))
budgets = {
    level: await get_budget_by_level(level)
    for level in levels
}

# 数据处理在本地完成
exceeded = []
for member in team:
    expenses = await get_expenses(member["id"], "Q3")
    total = sum(e["amount"] for e in expenses)
    budget = budgets[member["level"]]

    if total &gt; budget["travel_limit"]:
        exceeded.append({
            "name": member["name"],
            "spent": total,
            "limit": budget["travel_limit"]
        })

return exceeded  # 只返回关键结果

</code></pre><p>关键区别在于：</p><p>Minion：这是框架的核心设计，所有复杂任务都这样处理</p><p>PTC：需要显式启用，存在多重架构限制</p><p>必须显式标记哪些工具允许programmatic调用（allowed_callers配置）</p><p>运行在受限的Claude容器环境中，无法自由安装任意包</p><p>文件需要通过额外的Files API上传（单文件最大500MB限制）</p><p>工具必须在容器4.5分钟不活动超时前返回结果</p><p>Web工具、MCP工具无法通过programmatic方式调用</p><p>Minion的优势：更进一步<br/>Minion不仅实现了PTC的核心理念，还提供了更多优势：</p><ol><li><p>完整的Python生态系统<br/>Minion中的代码执行环境拥有完整的Python生态访问权：</p><h2>Minion可以直接使用任何Python库</h2><p>import pandas as pd<br/>import numpy as np<br/>from sklearn.cluster import KMeans</p></li></ol><h2>强大的数据处理</h2><p>df = pd.DataFrame(expense_data)<br/>analysis = df.groupby('category').agg({</p><pre><code>'amount': ['sum', 'mean', 'std'],
'count': 'size'</code></pre><p>})</p><h2>复杂的数据科学任务</h2><p>model = KMeans(n_clusters=3)<br/>clusters = model.fit_predict(spending_patterns)</p><ol start="2"><li><p>状态管理和持久化<br/>Minion天然支持复杂的状态管理：<br/>class BudgetAnalyzer:<br/> def __init__(self):</p><pre><code> self.cache = {}
 self.history = []
</code></pre><p>async def analyze_department(self, dept):</p><pre><code> # 状态在整个分析过程中保持
 if dept in self.cache:
     return self.cache[dept]

 result = await self._deep_analysis(dept)
 self.cache[dept] = result
 self.history.append(result)
 return result

</code></pre></li><li><p>错误处理和重试逻辑<br/>在代码中显式处理各种边界情况：<br/>async def robust_fetch(user_id, max_retries=3):<br/> for attempt in range(max_retries):</p><pre><code> try:
     return await get_expenses(user_id, "Q3")
 except RateLimitError:
     await asyncio.sleep(2 ** attempt)
 except DataNotFoundError:
     return []  # 合理的默认值</code></pre><p>raise Exception(f"Failed after {max_retries} attempts")</p></li><li><p>并行和异步操作<br/>充分利用Python的异步能力：</p><h2>高效的并行处理</h2><p>async def analyze_all_departments():<br/> departments = ["eng", "sales", "marketing", "ops"]</p><p># 同时分析所有部门<br/> results = await asyncio.gather(*[</p><pre><code> analyze_department(dept)
 for dept in departments</code></pre><p>])</p><p># 整合分析结果<br/> return consolidate_results(results)</p></li></ol><p>性能数据对比<br/>根据Anthropic的内部测试，PTC带来了显著改进：</p><p>Token节省：复杂研究任务从43,588降至27,297 tokens（减少37%）<br/>延迟降低：消除了多次模型推理往返<br/>准确率提升：</p><p>内部知识检索：25.6% → 28.5%<br/>GIA基准测试：46.5% → 51.2%</p><p>在minion的生产使用中，我们观察到类似甚至更好的指标，因为：</p><p>更少的模型调用：LLM只在规划阶段和最终总结时参与<br/>更高效的资源利用：本地数据处理不消耗API tokens<br/>更可预测的性能：代码执行路径明确，减少了LLM的不确定性</p><p>架构哲学：谁应该做什么？<br/>Minion的设计基于一个核心信念：</p><p>LLM擅长理解、规划和推理；Python擅长执行、处理和转换。</p><p>这种职责分离带来了清晰的架构：<br/>用户请求</p><pre><code>↓</code></pre><p>[LLM：理解意图，制定计划]</p><pre><code>↓</code></pre><p>[生成Python代码]</p><pre><code>↓</code></pre><p>[代码执行环境：调用工具、处理数据、控制流程]</p><pre><code>↓</code></pre><p>[返回结构化结果]</p><pre><code>↓</code></pre><p>[LLM：解读结果，生成用户友好的响应]</p><p>这不仅仅是优化，而是一种架构级别的重新思考。<br/>Tool Search Tool：Minion的动态工具发现<br/>Anthropic的另一个新特性是Tool Search Tool，解决大型工具库的context消耗问题。Minion在这方面也有相应的机制：<br/>分层工具暴露</p><h2>Minion的工具分层策略</h2><p>class MinionToolRegistry:</p><pre><code>def __init__(self):
    self.core_tools = []      # 始终加载
    self.domain_tools = {}    # 按需加载
    self.rare_tools = {}      # 搜索发现

def get_tools_for_task(self, task_description):
    # 智能工具选择
    tools = self.core_tools.copy()

    # 基于任务描述添加相关工具
    if "database" in task_description:
        tools.extend(self.domain_tools["database"])

    if "visualization" in task_description:
        tools.extend(self.domain_tools["plotting"])

    return tools

</code></pre><p>向量搜索工具发现</p><h2>使用embedding的工具搜索</h2><p>from sentence_transformers import SentenceTransformer</p><p>class SemanticToolSearch:</p><pre><code>def __init__(self, tool_descriptions):
    self.model = SentenceTransformer('all-MiniLM-L6-v2')
    self.tool_embeddings = self.model.encode(tool_descriptions)

def find_tools(self, query, top_k=5):
    query_embedding = self.model.encode([query])
    similarities = cosine_similarity(query_embedding, self.tool_embeddings)
    return self.get_top_tools(similarities, top_k)

</code></pre><p>实际应用：Minion在生产环境<br/>Minion框架已经在多个实际场景中证明了这种架构的价值：<br/>案例1：大规模数据分析<br/>金融科技公司使用minion分析数百万条交易记录，寻找异常模式：<br/>async def detect_anomalies():</p><pre><code># LLM规划：需要获取数据、清洗、特征工程、异常检测

# 执行代码直接处理大数据集
transactions = await fetch_all_transactions(start_date, end_date)
# 1M+ records, 但不进入LLM context

df = pd.DataFrame(transactions)
df = clean_data(df)
features = engineer_features(df)

# 使用机器学习检测异常
anomalies = detect_with_isolation_forest(features)

# 只返回异常摘要给LLM
return {
    "total_transactions": len(df),
    "anomalies_found": len(anomalies),
    "top_anomalies": anomalies.head(10).to_dict()
}

</code></pre><p>结果：</p><p>处理100万条记录<br/>LLM仅消耗~5K tokens（传统方式需要500K+）<br/>端到端延迟：30秒（vs 传统方式的5分钟+）</p><p>案例2：多源数据整合<br/>SaaS公司使用minion整合来自多个API的客户数据：<br/>async def comprehensive_customer_analysis(customer_id):</p><pre><code># 并行获取所有数据源
crm_data, support_tickets, usage_logs, billing_history = await asyncio.gather(
    fetch_crm_data(customer_id),
    fetch_support_tickets(customer_id),
    fetch_usage_logs(customer_id),
    fetch_billing_history(customer_id)
)

# 本地数据融合和分析
customer_profile = {
    "health_score": calculate_health_score(...),
    "churn_risk": predict_churn_risk(...),
    "upsell_opportunities": identify_opportunities(...),
    "support_sentiment": analyze_ticket_sentiment(support_tickets)
}

return customer_profile

</code></pre><p>案例3：自动化工作流<br/>DevOps团队使用minion自动化复杂的部署流程：<br/>async def deploy_with_validation():</p><pre><code># 多步骤工作流，每步都有条件逻辑

# 1. 运行测试
test_results = await run_test_suite()
if test_results.failed &gt; 0:
    return {"status": "blocked", "reason": "tests failed"}

# 2. 构建和推送镜像
image = await build_docker_image()
await push_to_registry(image)

# 3. 金丝雀部署
canary = await deploy_canary(image, percentage=10)
await asyncio.sleep(300)  # 监控5分钟

metrics = await get_canary_metrics(canary)
if metrics.error_rate &gt; 0.01:
    await rollback_canary(canary)
    return {"status": "rolled_back", "metrics": metrics}

# 4. 完整部署
await deploy_full(image)
return {"status": "success", "image": image.tag}

</code></pre><p>超越PTC：Minion的未来方向<br/>虽然PTC是一个重要的进步，但minion的架构设计让我们能够探索更多可能性：</p><ol><li><p>混合推理模式<br/>在一个会话中智能切换：</p><h2>简单任务：直接工具调用</h2><p>if task.complexity &lt; THRESHOLD:<br/> result = await simple_tool_call(task)</p></li></ol><h2>复杂任务：生成编排代码</h2><p>else:</p><pre><code>orchestration_code = await llm.generate_code(task)
result = await execute_code(orchestration_code)

</code></pre><ol start="2"><li><p>增量计算和缓存<br/>智能重用中间结果：</p><h2>记忆化的数据获取</h2><p>@lru_cache(maxsize=1000)<br/>async def cached_get_user_data(user_id):<br/> return await fetch_user_data(user_id)</p></li></ol><h2>增量更新而非全量重算</h2><p>async def update_analysis(new_data):</p><pre><code>previous_state = load_checkpoint()
delta = compute_delta(previous_state, new_data)
updated_state = apply_delta(previous_state, delta)
return updated_state

</code></pre><ol start="3"><li><p>多模型协作<br/>不同模型处理不同阶段：</p><h2>规划用强模型</h2><p>plan = await claude_opus.create_plan(user_request)</p></li></ol><h2>代码生成用专门模型</h2><p>code = await codegen_model.generate(plan)</p><h2>执行和监控</h2><p>result = await execute_with_monitoring(code)</p><h2>用户交互用快速模型</h2><p>response = await claude_haiku.format_response(result)</p><p>开源的力量：社区驱动的创新<br/>Minion作为开源项目（300+ GitHub stars），其发展得益于社区的贡献和反馈。这种开放性带来了：</p><p>快速迭代：社区发现问题和用例，推动快速改进<br/>多样化应用：用户在我们未曾想象的场景中使用minion</p><p>相比之下，PTC虽然强大，但：</p><p>需要显式配置（allowed_callers, defer_loading等）<br/>依赖特定的API版本和beta功能<br/>与Claude的生态系统紧密耦合</p><p>Minion的设计原则是provider-agnostic——你可以用任何LLM后端（Claude, GPT-4, 开源模型），架构优势依然存在。<br/>技术细节：实现对比<br/>让我们深入比较实现细节：<br/>PTC的实现方式</p><h2>Anthropic的PTC需要特定配置</h2><p>{</p><pre><code>"tools": [
    {
        "type": "code_execution_20250825",
        "name": "code_execution"
    },
    {
        "name": "get_team_members",
        "allowed_callers": ["code_execution_20250825"],
        ...
    }
]</code></pre><p>}</p><h2>Claude生成工具调用</h2><p>{</p><pre><code>"type": "server_tool_use",
"id": "srvtoolu_abc",
"name": "code_execution",
"input": {
    "code": "team = get_team_members('engineering')\\\\\\\\n..."
}</code></pre><p>}</p><p>Minion的实现方式</p><h2>Minion的工具定义是标准Python</h2><p>class MinionTools:</p><pre><code>@tool
async def get_team_members(self, department: str):
    """Get all members of a department"""
    return await self.db.query(...)

@tool
async def get_expenses(self, user_id: str, quarter: str):
    """Get expense records"""
    return await self.expenses_api.fetch(...)
</code></pre><h2>LLM生成的是完整的Python函数</h2><p>async def analyze_budget():</p><pre><code># 直接调用工具函数
team = await tools.get_team_members("engineering")

# 完整的Python语言能力
expenses_by_user = {
    member.id: await tools.get_expenses(member.id, "Q3")
    for member in team
}

# 任意复杂度的数据处理
analysis = perform_complex_analysis(expenses_by_user)
return analysis

</code></pre><p>关键区别：</p><p>PTC：工具调用通过特殊的API机制，有caller/callee关系<br/>Minion：工具就是普通的Python async函数，LLM生成标准代码</p><p>为什么这个架构如此重要？<br/>随着AI Agent向生产环境发展，我们面临的核心挑战是：</p><p>规模：处理百万级数据，不能全塞进context<br/>可靠性：生产系统需要确定性的错误处理<br/>成本：token消耗直接影响商业可行性<br/>性能：用户体验需要亚秒级响应</p><p>传统的单次工具调用模式在这些维度上都遇到瓶颈。代码编排模式（无论是PTC还是minion）提供了突破：<br/>传统模式：LLM &lt;-&gt; Tool &lt;-&gt; LLM &lt;-&gt; Tool &lt;-&gt; LLM</p><pre><code>      (慢)   (贵)   (脆弱)
</code></pre><p>编排模式：LLM -&gt; [Code: Tool+Tool+Tool+Processing] -&gt; LLM</p><pre><code>      (快)   (省)   (可靠)

</code></pre><ol><li>经过验证的架构<br/>PTC的发布证明了我们架构选择的正确性——这不是投机性的设计，而是行业领先者独立得出的结论。</li><li>先发优势<br/>在PTC成为官方特性之前，minion已经在生产环境积累了经验和最佳实践。</li><li>更广泛的适用性</li></ol><p>支持多种LLM后端（Claude, GPT-4, 开源模型）<br/>灵活的部署选项（云端、本地、混合）<br/>丰富的Python生态系统集成</p><ol start="4"><li>社区和生态<br/>300+stars代表的不仅是认可，还有潜在的用户基础和贡献者社区。<br/>结论：架构的必然收敛<br/>Anthropic推出PTC不是偶然——这是agent架构演进的必然方向。当你需要构建能处理复杂任务、大规模数据、多步骤流程的生产级agent时，你会自然而然地得出这样的结论：</li></ol><p>LLM应该专注于它擅长的（理解和规划），让代码处理它擅长的（执行和转换）。</p><p>Minion从一开始就拥抱了这个理念，并将继续推动这个方向：</p><p>✅ 今天：完整的PTC式架构，生产环境验证<br/>🚀 明天：更智能的工具发现、更高效的状态管理<br/>🌟 未来：混合推理、增量计算、多模型协作</p><p>如果你正在构建需要处理真实世界复杂性的AI agent，我邀请你：</p><p>试用minion：GitHub仓库<br/>加入讨论：分享你的用例和反馈<br/>参与社区：贡献代码、文档、想法</p><p>这不是关于谁先想到某个特性，而是关于共同推动AI agent架构向正确方向发展。PTC的发布对整个生态系统都是好消息——它验证了这条路径，并将吸引更多开发者探索programmatic orchestration的潜力。<br/>让我们一起构建下一代AI agent。<br/>延伸阅读 完全开源！全新多合一AI智能体框架来了：无缝支持多种工具、多种任务</p><p>相关资源<br/>视频演示</p><p>PTC Example - Expense Tracking: <a href="https://link.segmentfault.com/?enc=m5jnJw%2FfG3I7D5kpl8cpPw%3D%3D.khDOtlxIHvVDiXGhG9RdIHbtc8Vn%2F8ZK4uWQnZ1l9L8%3D" rel="nofollow" target="_blank">https://youtu.be/hDAIB0sF7-k</a><br/>Tool Search Tool Example - Create GitHub PR: <a href="https://link.segmentfault.com/?enc=wvHhMap3s3%2FMUXzJp3dHfg%3D%3D.q65kExeMlmhm2iVQ2%2B2CM2fC1JB5kvk6fU%2BZv%2F0BW%2FQ%3D" rel="nofollow" target="_blank">https://youtu.be/G7dDvza9PO8</a></p><p>参考资料:<br/><a href="https://link.segmentfault.com/?enc=ednojkx7CkKVOKqm26Z6HQ%3D%3D.2425xUOi%2BwBJEKyd6kZBbgWccH1JyUTsnwnBdvALJCE%3D" rel="nofollow" target="_blank">https://github.com/femto/minion</a><br/>Advanced Tool Use Guide: <a href="https://link.segmentfault.com/?enc=9arZPTJeax1%2B4oAcWELCxQ%3D%3D.OfJfjdGHzE907%2Br1btDwyPZVuX%2FM%2FesFsyDJAC9%2FuCFyNzrJsnJbq7UZAiUVWw5jy7W0GRd7EZWoAuVdzJHA0Ey87PcKBricN5UV9n1Mx5E%3D" rel="nofollow" target="_blank">https://github.com/femto/minion/blob/main/docs/advanced_tool_use.md</a></p><p>GitHub: <a href="https://link.segmentfault.com/?enc=w7SHEE1wmeJuIVHlDhMyBQ%3D%3D.4Wkf3cHuEOkp%2FZZ3lnQ8vwxYen4UFpw8oor1Xtt%2FIjk%3D" rel="nofollow" target="_blank">https://github.com/femto/minion</a></p>]]></description></item><item>    <title><![CDATA[数据脱敏：在数据价值与隐私安全之间构建平衡 老实的剪刀 ]]></title>    <link>https://segmentfault.com/a/1190000047466585</link>    <guid>https://segmentfault.com/a/1190000047466585</guid>    <pubDate>2025-12-11 16:10:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在大数据与数字化转型的浪潮中，数据已成为机构与企业最核心的资产之一。然而，随着数据的集中与流动，隐私泄露风险也日益加剧。如何在充分利用数据价值的同时，确保个人敏感信息与商业机密的安全？数据脱敏作为一种关键的数据安全技术，正是解决这一矛盾的重要桥梁。<br/>一、 数据脱敏：定义与核心目标<br/>数据脱敏，是指通过特定的技术手段，对敏感数据进行变形、替换或遮蔽，以降低其敏感级别的过程。其核心目标并非简单地“隐藏”数据，而是在确保数据可用性的前提下，切断敏感信息与真实个体之间的直接关联，从而在数据共享、开发测试、分析研究等场景中，有效防止隐私泄露与内部滥用。<br/>需要保护的典型敏感数据包括：个人身份信息（姓名、身份证号）、联系方式（手机号、住址）、金融账户信息（银行卡号、交易记录）、医疗健康信息以及企业的商业秘密等。<br/>二、 两种技术路径：静态脱敏与动态脱敏<br/>根据数据的使用状态和处理时机，数据脱敏主要分为静态与动态两大技术路径，两者在场景、技术与部署上各有侧重。</p><ol><li>静态脱敏：数据“搬移并替换”<br/>静态脱敏适用于数据离开生产环境的场景。其过程如同数据的“仿真副本制作”：将生产环境中的真实数据抽取出来，经过一套完整的脱敏规则处理（如屏蔽、变形、替换、随机化等），形成一份“看起来真实、但关键信息已伪”的数据集，再装载到开发、测试、分析或培训等非生产环境中。<br/>技术特点：处理的是数据副本，脱敏后数据被永久性改变并存储在新的位置。支持从数据库到数据库、数据库到文件等多种迁移方式。<br/>部署方式：通常在生产环境与下游环境之间部署脱敏服务器或设备，完成数据的抽取、变形与装载流水线。<br/>核心价值：为外部协作、内部测试等提供高度仿真的安全数据源，实现生产数据的安全隔离。</li><li>动态脱敏：数据“边使用边脱敏”<br/>动态脱敏适用于直接访问生产环境的实时场景。其原理如同在数据出口处加装一个“实时过滤器”：当应用系统、运维或客服人员查询生产数据库时，脱敏系统会实时解析SQL查询请求，根据预定义的策略（如访问者身份、时间、客户端工具等），在数据返回结果集的瞬间进行脱敏处理，再将结果返回给请求者。<br/>技术特点：处理的是数据流，生产库中的原始数据丝毫未变。它通过SQL改写或结果集拦截来实现实时脱敏。<br/>部署方式：通常以代理（Gateway）模式部署，逻辑上串联在应用程序与数据库之间，所有访问流量都需经过此代理。<br/>核心价值：在保证业务连续性的同时，实现最小权限访问，防止运维、客服等内部角色过度接触敏感信息，满足“可用不可见”的需求。<br/>三、 主要实现方式：从手工脚本到专业产品<br/>数据脱敏的实现，经历了从初级到专业的发展过程：<br/>1、自定义脚本脱敏：在早期，许多组织通过编写临时脚本（如使用Python、Shell等），对数据进行简单的替换、遮盖或随机化处理。这种方式虽然灵活、成本低，但存在效率低下、规则不一致、难以维护、覆盖场景有限等明显短板，无法应对大规模、复杂逻辑的脱敏需求。<br/>2、专业化脱敏产品：随着数据法规（如GDPR、个人信息保护法）的完善和业务场景的复杂化，专业数据脱敏产品成为主流选择。这类产品提供：<br/>3、丰富的预置算法库：针对不同数据类型（姓名、证件号、地址、金额等）提供高仿真、可逆/不可逆的多样化脱敏算法。<br/>4、可视化策略管理：通过图形界面灵活配置脱敏规则与流程，降低技术门槛。<br/>5、自动化与高效率：支持任务调度、批量处理，极大提升脱敏效率和准确性。<br/>6、血缘分析与数据关联保持：在脱敏过程中维持数据间的关联关系与业务逻辑，确保脱敏后数据在测试中依然有效。<br/>7、审计与合规报告：记录所有脱敏操作，满足合规性审计要求。<br/>四、 核心价值与合规意义<br/>数据脱敏的终极价值，在于为组织构建一道至关重要的内部数据安全防线：<br/>1、防范内部数据滥用：有效限制开发、测试、运维、分析等内部人员对真实敏感数据的接触，从源头减少泄露风险。<br/>2、保障数据合规流通：在满足数据保护法规（如《网络安全法》、《个人信息保护法》）要求的前提下，使得数据能够安全地用于次级用途，促进数据价值挖掘。<br/>3、维护企业声誉与信任：避免因数据泄露导致的重大财务损失、法律诉讼及品牌信誉崩塌。<br/>4、支撑数据安全治理体系：作为数据分类分级保护的落地手段之一，是完善的数据安全生命周期管理中不可或缺的环节。<br/>在数据驱动发展的今天，安全已不再是发展的约束，而是其基石。数据脱敏，尤其是动静结合的综合脱敏方案，正成为企业平衡数据利用与安全保护的标配能力。它不仅是满足合规要求的“必答题”，更是企业构建负责任的数据文化、赢得用户信任、实现数据资产价值最大化的“智能策略”。未来，随着人工智能与隐私计算技术的发展，数据脱敏技术将朝着更智能、更融合、更保真的方向持续演进，为数字社会的稳健运行保驾护航。</li></ol>]]></description></item><item>    <title><![CDATA[制造业产业大脑：从数据看板到智能神经系统的革命性跃迁 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047466587</link>    <guid>https://segmentfault.com/a/1190000047466587</guid>    <pubDate>2025-12-11 16:09:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字经济深度重构实体经济的今天，“制造业产业大脑”已不再是一个时髦的术语，而是驱动产业转型升级的核心基础设施。它不是简单的数据可视化平台，也不是传统ERP系统的升级版，而是一个以数据为血脉、AI为神经、产业链为骨骼，贯通政府与企业、连接生产与决策的产业级智能中枢。其本质，是让原本割裂的制造单元，进化为一个能感知、思考、决策、协同的“数字生命体”。<br/>制造业产业大脑的诞生，源于工业互联网平台的深化演进。早期的工业互联网主要服务于企业内部的设备连接与生产优化，而产业大脑则进一步将视角拓展至整个区域乃至全国的产业生态。它通过汇聚来自企业ERP、IoT设备、税务、专利、供应链、舆情、碳足迹等多源异构数据，构建出动态的“产业数字孪生体”。在浙江绍兴的黄酒产业、江西的生物医药集群、广东的新能源汽车产业链中，产业大脑已能实时感知产能波动、融资缺口与供应链断点，为政府提供精准的政策靶向，为企业匹配最优的协作资源。<br/>这一系统的进化，正经历从“描述性分析”到“认知型决策”的质变。过去，平台只能展示“发生了什么”；如今，借助AI的深度介入，它能回答“为什么会发生”“接下来会怎样”以及“该如何应对”。例如，当某地汽车焊装产线出现良率下滑，产业大脑不再仅发出预警，而是能自动调取287条焊接工艺知识规则，结合实时振动、温度等多模态数据，通过因果推理AI精准定位根因，并生成最优参数组合，通过API中台直接注入MES系统，实现无人干预的闭环修复——这一过程，正是广域铭岛Geega平台所代表的“工业智能体”力量的体现。它让产业大脑从“指挥家”升级为“执行者”，从“看见问题”跃迁至“亲手修复”。<br/>更深远的变革在于生态协同的重构。广域铭岛提出的“API即智能体，智能体即生态”理念，正在打破企业间、系统间、区域间的数字壁垒。在领克成都工厂，12类工业智能体在5分钟内协同推演3套应急方案，完成供应商评估、物流重排与信用验证，形成一场精密的“数字交响曲”。这种能力，使产业大脑超越了单点优化，成为跨企业、跨行业、跨地域的智能神经网络。它不仅优化生产，更重塑价值链条——通过“电机指数”“碳足迹数字遗产”等创新服务，企业从卖产品转向卖服务，政府从撒网式补贴转向激光式激励，产业从成本竞争迈向效率与可持续性并重的新范式。<br/>未来，制造业产业大脑将向“预演者”与“共创者”进化。政府规划一条新能源汽车走廊，平台可模拟不同补贴政策下的产业集群演化路径；初创企业寻找技术伙伴，系统能从全球专利海洋中自动识别“隐形冠军”；当能源成本飙升或原材料断供，大脑能联动绿电资源、碳配额与替代供应商，实时推演最优解。这不仅是技术的突破，更是产业组织形态的革命——如同秦始皇“车同轨、书同文”统一了物理世界的流通，产业大脑正以数据为基、智能为脉，重构数字时代的产业文明。<br/>制造业不会消失，落后的制造方式才会。而制造业产业大脑，正是这场转型的“神经中枢”。广域铭岛等先行者，正以工业智能体为笔，将老师傅的工艺经验封装为可复用的数字资产，让每一条产线都成为感知终端，每一个车间都成为执行单元。当数据不再沉默，当算法读懂隐性知识，当机器能自主修复系统——我们才真正触摸到，制造业从“制造”迈向“智造”的灵魂跃迁。这，不是一场技术升级，而是一次文明范式的更迭。</p>]]></description></item><item>    <title><![CDATA[期货数据对接指南，用于获取黄金、白银、原油等大宗商品的数据。 CryptoRzz ]]></title>    <link>https://segmentfault.com/a/1190000047466601</link>    <guid>https://segmentfault.com/a/1190000047466601</guid>    <pubDate>2025-12-11 16:09:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. 基础配置</h2><ul><li><strong>接口域名</strong>: <code>https://api.stocktv.top</code></li><li><strong>期货基础路径</strong>: <code>/futures</code></li><li><strong>认证方式</strong>: URL 参数 <code>key=您的API密钥</code></li></ul><hr/><h2>2. 核心对接流程</h2><h3>第一步：获取期货品种列表 (查找 Symbol)</h3><p>由于期货合约代码（Symbol）可能因交易所不同而有所差异（例如黄金可能是 <code>XAU</code>、<code>GC</code> 或 <code>Gold</code>），<strong>第一步必须先拉取列表</strong>，找到对应的 <code>symbol</code>。</p><ul><li><strong>接口</strong>: <code>/futures/list</code></li><li><strong>方法</strong>: <code>GET</code></li><li><strong>参数</strong>: <code>key</code></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/futures/list?key=YOUR_KEY</code></pre></li><li><p><strong>如何查找</strong>:</p><ul><li><strong>黄金</strong>: 搜索关键词 "Gold" 或 "XAU"</li><li><strong>白银</strong>: 搜索关键词 "Silver" 或 "XAG"</li><li><strong>原油</strong>: 搜索关键词 "Oil", "WTI", "Brent" 或 "CL"</li></ul></li></ul><h3>第二步：获取实时行情 (Real-time Quote)</h3><p>获取特定品种的最新买卖价、涨跌幅。</p><ul><li><strong>接口</strong>: <code>/futures/querySymbol</code></li><li><strong>方法</strong>: <code>GET</code></li><li><p><strong>参数</strong>:</p><ul><li><code>symbol</code>: <strong>品种代码</strong> (第一步获取的)</li></ul></li><li><p><strong>请求示例 (假设黄金代码为 XAU)</strong>:</p><pre><code class="http">GET https://api.stocktv.top/futures/querySymbol?symbol=XAU&amp;key=YOUR_KEY</code></pre></li><li><p><strong>响应示例</strong>:</p><pre><code class="json">{
  "code": 200,
  "data": [
    {
      "symbol": "XAU",
      "name": "Gold Spot",
      "buy": "2350.50",    // 买价
      "sell": "2350.80",   // 卖价
      "last_price": "2350.60", // 最新价
      "chg_pct": "0.45",   // 涨跌幅
      "time": "2024-05-20"
    }
  ]
}</code></pre></li></ul><h3>第三步：获取 K 线数据 (Chart Data)</h3><p>获取用于绘制图表的历史数据。</p><ul><li><strong>接口</strong>: <code>/futures/kline</code></li><li><strong>方法</strong>: <code>GET</code></li><li><p><strong>参数</strong>:</p><ul><li><code>symbol</code>: <strong>品种代码</strong></li><li><p><code>interval</code>: <strong>周期</strong> (注意期货接口的周期定义与股票略有不同)</p><ul><li><code>1</code>, <code>5</code>, <code>15</code>, <code>30</code>, <code>60</code> (分钟)</li><li><code>1d</code> (日线)</li></ul></li></ul></li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/futures/kline?symbol=XAU&amp;interval=1d&amp;key=YOUR_KEY</code></pre></li></ul><hr/><h2>3. 完整代码示例 (HTML + JavaScript)</h2><p>这是一个完整的演示页面。它包含两个功能：</p><ol><li><strong>自动搜索品种</strong>：点击按钮自动在列表中查找黄金、白银、原油的 Symbol。</li><li><strong>渲染图表</strong>：使用找到的 Symbol 绘制 K 线图。</li></ol><p>&lt;!-- end list --&gt;</p><pre><code class="html">&lt;!DOCTYPE html&gt;
&lt;html lang="zh-CN"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;StockTV 期货行情 (黄金/原油)&lt;/title&gt;
    &lt;script src="https://cdn.jsdelivr.net/npm/klinecharts/dist/klinecharts.min.js"&gt;&lt;/script&gt;
    &lt;style&gt;
        body { font-family: sans-serif; padding: 20px; background-color: #f0f2f5; }
        .container { max-width: 1000px; margin: 0 auto; background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .btn-group { margin-bottom: 20px; display: flex; gap: 10px; }
        button { padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; background-color: #007bff; color: white; font-size: 14px; }
        button:hover { background-color: #0056b3; }
        .status-bar { margin-bottom: 10px; padding: 10px; background: #e6f7ff; border: 1px solid #91d5ff; border-radius: 4px; color: #0050b3; font-size: 14px; }
        #chart { width: 100%; height: 500px; border: 1px solid #eee; }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;div class="container"&gt;
    &lt;h2&gt;StockTV 全球期货数据演示&lt;/h2&gt;
    
    &lt;div class="status-bar" id="status"&gt;
        请点击下方按钮加载数据...
    &lt;/div&gt;

    &lt;div class="btn-group"&gt;
        &lt;button onclick="loadCommodity('Gold', '黄金')"&gt;加载 黄金 (Gold)&lt;/button&gt;
        &lt;button onclick="loadCommodity('Silver', '白银')"&gt;加载 白银 (Silver)&lt;/button&gt;
        &lt;button onclick="loadCommodity('Oil', '原油')"&gt;加载 原油 (Oil)&lt;/button&gt;
        &lt;button onclick="loadCommodity('Gas', '天然气')"&gt;加载 天然气 (Gas)&lt;/button&gt;
    &lt;/div&gt;

    &lt;div id="chart"&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;script&gt;
    // === 配置您的 API Key ===
    const API_KEY = 'YOUR_API_KEY'; // TODO: 替换为您的 Key
    const BASE_URL = 'https://api.stocktv.top';

    const chart = klinecharts.init('chart');

    function updateStatus(msg) {
        document.getElementById('status').innerText = msg;
    }

    /**
     * 1. 智能查找品种 Symbol
     * 先获取列表，然后模糊匹配名称
     */
    async function findSymbol(keyword) {
        updateStatus(`正在期货列表中搜索 "${keyword}" ...`);
        const url = `${BASE_URL}/futures/list?key=${API_KEY}`;
        
        try {
            const res = await fetch(url);
            const json = await res.json();
            
            if (json.code === 200 &amp;&amp; json.data) {
                // 在列表中查找名称包含 keyword 的项 (不区分大小写)
                const target = json.data.find(item =&gt; 
                    item.name.toLowerCase().includes(keyword.toLowerCase()) || 
                    item.symbol.toLowerCase().includes(keyword.toLowerCase())
                );
                return target;
            }
        } catch (e) {
            console.error(e);
            updateStatus("网络请求失败，请检查控制台");
        }
        return null;
    }

    /**
     * 2. 加载数据主流程
     */
    async function loadCommodity(keyword, displayName) {
        // 第一步：查找 Symbol
        const commodity = await findSymbol(keyword);
        
        if (!commodity) {
            updateStatus(`未找到 "${displayName}" 相关的期货合约，请尝试其他关键词。`);
            return;
        }

        const symbol = commodity.symbol;
        updateStatus(`找到合约: ${commodity.name} (${symbol})。正在加载 K 线...`);

        // 第二步：获取 K 线数据 (日线 1d)
        // 注意：期货接口 interval 定义: 1, 5, 15, 30, 60, 1d
        const klineUrl = `${BASE_URL}/futures/kline?symbol=${symbol}&amp;interval=1d&amp;key=${API_KEY}`;
        
        try {
            const res = await fetch(klineUrl);
            const json = await res.json();

            if (json.code === 200 &amp;&amp; json.data) {
                // 转换数据格式
                // 期货接口返回: date (字符串时间), open, close, high, low, volume, timestamp (秒级)
                const dataList = json.data.map(item =&gt; ({
                    timestamp: item.timestamp * 1000, // 转换为毫秒
                    open: parseFloat(item.open),
                    high: parseFloat(item.high),
                    low: parseFloat(item.low),
                    close: parseFloat(item.close),
                    volume: parseFloat(item.volume)
                }));

                // 排序
                dataList.sort((a, b) =&gt; a.timestamp - b.timestamp);

                chart.applyNewData(dataList);
                updateStatus(`成功加载 ${displayName} (${symbol}) 的日线数据，共 ${dataList.length} 条。最新价: ${dataList[dataList.length-1].close}`);
            } else {
                updateStatus(`获取 K 线数据失败: ${json.message}`);
            }
        } catch (e) {
            console.error(e);
            updateStatus("K线请求发生错误");
        }
    }
&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt;</code></pre>]]></description></item><item>    <title><![CDATA[2025CRM选型手册：主流CRM品牌客户 - 销售 - 团队管理能力 场景化对比 正直的炒饭 ]]></title>    <link>https://segmentfault.com/a/1190000047466609</link>    <guid>https://segmentfault.com/a/1190000047466609</guid>    <pubDate>2025-12-11 16:08:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化转型背景下，CRM（客户关系管理）已从“销售工具”升级为“企业增长引擎”。企业对CRM的需求不再局限于“记录客户信息”，而是要求<strong>全链路的</strong> <strong>客户生命周期管理</strong> <strong>、精细化的销售过程管控、协同化的团队效能提升</strong>。本文选取<strong>超兔一体云、Salesforce、销售易、</strong> <strong>SAP</strong> <strong>CRM、Freshsales、</strong> <strong>飞书</strong>等18款主流CRM产品，从<strong>客户管理、销售过程管理、销售团队管理</strong>三大核心维度展开深度对比，结合场景适配性，为企业选型提供专业参考。</p><h2>一、对比框架与核心指标说明</h2><p>本次对比围绕“以客户为中心”的全链路管理逻辑，拆解为<strong>3大维度、9个子指标</strong>，确保对比的针对性与专业性：</p><table><thead><tr><th>大维度</th><th>子指标</th><th>核心评价标准</th></tr></thead><tbody><tr><td>客户管理</td><td>信息录入</td><td>多渠道覆盖、自动化能力、批量处理效率</td></tr><tr><td> </td><td>搜索分类</td><td>搜索精准度、分类灵活性、智能化程度</td></tr><tr><td> </td><td>跟踪能力</td><td>跟踪维度（行动/通话/待办）、可视化程度、移动端支持</td></tr><tr><td>销售过程管理</td><td>销售机会跟踪</td><td>跟单模型丰富度、可视化管道、自动化提醒</td></tr><tr><td> </td><td>合同管理</td><td>覆盖环节（生成/审批/执行）、自动化能力、与ERP/财务的集成性</td></tr><tr><td> </td><td>销售预测</td><td>数据维度（多源/单一）、智能化（AI/手动）、与目标的联动性</td></tr><tr><td>销售团队管理</td><td>绩效跟踪</td><td>数据可视化（仪表盘/报表）、多维度（过程/结果/行为）、与薪资的集成性</td></tr><tr><td> </td><td>任务分配</td><td>权限精细化、自动化分配、场景灵活性（临时小组/矩阵结构）</td></tr><tr><td> </td><td>沟通协作</td><td>生态集成（IM/文档/话术库）、信息共享实时性、外勤/内勤场景适配</td></tr></tbody></table><h2>二、核心维度深度对比</h2><h3>（一）客户管理：以客户为中心的信息底座</h3><p>客户管理是CRM的基础，核心目标是<strong>实现客户信息的高效沉淀、精准检索、动态跟踪</strong>，为后续销售动作提供“数据燃料”。</p><h4>1. 细分指标对比表</h4><table><thead><tr><th>品牌</th><th>信息录入能力</th><th>搜索分类能力</th><th>跟踪能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>多渠道（通讯录/拍名片/微信/QQ/批量导入）；自动抓取工商/百度资讯/社交头像</td><td>精准搜索（客户名/手机号）；自定义查重+企业简称模糊查重；九级分类汇总</td><td>行动管理（语音/定位/照片）；通话随记；客户视图时间线；待办提醒（红绿灯标识）</td></tr><tr><td>Salesforce</td><td>邮件/社交媒体/广告/线下多渠道整合；统一客户档案</td><td>智能搜索；多维度分类；跨部门共享</td><td>实时互动跟踪；多渠道联动；360°客户视图</td></tr><tr><td>销售易</td><td>B2B场景自定义字段；客户查重报备；批量导入</td><td>标签化分类；B2B专属分类；智能搜索</td><td>360°视图；跟进记录自动化；销售行为分析</td></tr><tr><td>SAP CRM</td><td>ERP/服务/销售多系统整合；批量导入</td><td>多维度分类；全局搜索；跨模块共享</td><td>动态客户档案；服务反馈联动；全生命周期跟踪</td></tr><tr><td>Freshsales</td><td>AI驱动线索捕捉（邮件/电话/社交）；多渠道录入</td><td>AI分类（高意向客户）；标签分类；精准搜索</td><td>移动端实时跟踪；沟通历史；线索评分联动</td></tr><tr><td>飞书</td><td>多维表格+CRM插件；批量导入</td><td>多维表格分类；标签筛选；精准搜索</td><td>文档/会议联动；OKR进度跟踪；轻量化协作跟踪</td></tr></tbody></table><h4>2. 关键结论</h4><ul><li><strong>全渠道自动化领先</strong>：超兔、Salesforce、Freshsales覆盖“获客-录入-沉淀”全链路，超兔的“拍名片/微信录入+自动抓取工商信息”更贴合中国企业的外勤场景；</li><li><strong>B2B场景适配</strong>：销售易、SAP的“客户查重报备+多维度分类”解决了B2B企业的“撞单”痛点；</li><li><strong>轻量化协作</strong>：飞书的“多维表格+文档联动”适合互联网团队的轻量化客户管理。</li></ul><h3>（二）销售过程管理：从机会到回款的全流程管控</h3><p>销售过程管理是CRM的“执行引擎”，核心目标是<strong>规范销售动作、提升转化效率、降低流程损耗</strong>。</p><h4>1. 细分指标对比表</h4><table><thead><tr><th>品牌</th><th>销售机会跟踪能力</th><th>合同管理能力</th><th>销售预测能力</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>三模型跟单（小单快单/商机跟单/多方项目）；360°视图；待办提醒</td><td>多业务模型（服务/标准/批发/非标订单）；订单执行（锁库/采购/供应商直发）</td><td>数据分析引擎（数字卡片/同比环比）；目标分解（部门/个人/业务）；动态进度追踪</td></tr><tr><td>Salesforce</td><td>Sales Cloud管道管理；Einstein AI线索评分；自动化任务</td><td>合同模板+审批流程；Sales Cloud集成；多业务场景覆盖</td><td>Einstein AI预测；销售报告；多维度数据支撑</td></tr><tr><td>销售易</td><td>全流程漏斗（线索-商机-订单）；阶段自定义；推进提醒</td><td>合同模板+审批+履行跟踪；B2B场景适配</td><td>BI工具；多维度预测；销售目标划分</td></tr><tr><td>SAP CRM</td><td>线索-商机-报价-合同全流程覆盖；自动化任务触发；ERP联动</td><td>合同审批；ERP联动；全流程管控</td><td>ERP数据联动；需求-供应预测；多维度分析</td></tr><tr><td>Freshsales</td><td>AI加速商机转化；报价单生成；转化周期缩短40%（跨境电商案例）</td><td>报价单转订单；Freshworks生态联动</td><td>AI预测；转化效率分析；数据支撑</td></tr></tbody></table><h4>2. 关键结论</h4><ul><li><strong>复杂业务适配</strong>：超兔的“多业务模型（服务/批发/非标订单）+订单执行全链路”解决了企业“业务场景碎片化”的痛点；</li><li><strong>AI驱动转化</strong>：Salesforce、Freshsales的“AI线索评分+加速转化”适合需要提升效率的成长型企业；</li><li><strong>ERP集成</strong>：SAP、超兔的“合同-ERP-采购”联动解决了传统企业的“信息孤岛”问题。</li></ul><h3>（三）销售团队管理：从绩效到协作的效能提升</h3><p>销售团队管理是CRM的“指挥中心”，核心目标是<strong>激活团队活力、优化资源分配、提升协作效率</strong>。</p><h4>1. 细分指标对比表</h4><table><thead><tr><th>品牌</th><th>绩效跟踪能力</th><th>任务分配能力</th><th>沟通协作能力</th></tr></thead><tbody><tr><td><strong>超兔一体云</strong></td><td>BOSS首屏（目标汇总/完成动态/客户分布）；红绿灯标识；Acc薪资模块（回款/目标算奖金）</td><td>全局自动权限（九级结构/临时小组）；快行动待办提醒；精准时间待办</td><td>快协作（客户/待办/项目联动）；集信工具（通话录音/短信）；武器云（话术库/文档）</td></tr><tr><td>Salesforce</td><td>Sales Cloud仪表盘（目标达成/线索漏斗/行动完成）；绩效报表</td><td>角色权限；任务自动分配；团队协作</td><td>Slack集成；销售文档共享；多渠道沟通</td></tr><tr><td>销售易</td><td>仪表盘（过程/结果/行为）；新人知识库；企业微信/钉钉督办</td><td>角色权限细分；任务督办；B2B场景适配</td><td>企业微信/钉钉集成；知识库；标准沟通内容共享</td></tr><tr><td>Freshsales</td><td>自动化绩效；Freshworks联动；转化效率分析</td><td>自动化分配；AI驱动；权限管理</td><td>Freshworks生态（呼叫中心/营销）；多渠道沟通；信息共享</td></tr><tr><td>飞书</td><td>绩效看板；即时沟通；OKR联动</td><td>任务分配；OKR联动；权限管理</td><td>即时沟通；文档/会议联动；协作密集型场景</td></tr></tbody></table><h4>2. 关键结论</h4><ul><li><strong>全链路绩效可视化</strong>：超兔的“BOSS首屏+红绿灯标识+薪资集成”实现了“目标-行动-结果-奖金”的闭环，更贴合中国企业的“结果导向”需求；</li><li><strong>复杂组织适配</strong>：超兔的“九级权限+临时小组”解决了大企业“矩阵式结构”的任务分配痛点；</li><li><strong>协作生态领先</strong>：飞书、超兔的“IM+文档+话术库”联动，适合互联网、协作密集型团队。</li></ul><h2>三、场景适配与选型建议</h2><p>基于各品牌的核心能力，结合企业常见场景，给出以下选型建议：</p><table><thead><tr><th>场景类型</th><th>推荐品牌</th><th>核心优势</th></tr></thead><tbody><tr><td>全链路需求（客户+销售+团队）</td><td>超兔一体云、Salesforce</td><td>超兔的“三一定级+多业务模型+BOSS首屏”更贴合中国企业；Salesforce适合大型企业生态集成</td></tr><tr><td>B2B复杂场景（撞单/多部门）</td><td>销售易、SAP CRM</td><td>销售易的“客户报备+多维度分类”；SAP的“ERP联动+全流程管控”</td></tr><tr><td>外贸/中小企业</td><td>Zoho、Freshsales</td><td>Zoho的“多渠道录入+Books集成”；Freshsales的“AI线索捕捉+转化效率提升”</td></tr><tr><td>协作密集型（互联网团队）</td><td>飞书、超兔一体云</td><td>飞书的“文档/会议联动”；超兔的“快协作+话术库”</td></tr><tr><td>轻量化需求（中小团队）</td><td>纷享销客、HubSpot</td><td>纷享销客的“移动端适配+日报周报”；HubSpot的“免费版+自动化工作流”</td></tr></tbody></table><h2>四、可视化补充：Mermaid图与雷达图</h2><h3>1. 超兔客户管理流程时序图</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466611" alt="" title=""/></p><pre><code>sequenceDiagram
    participant 销售 as 销售人员
    participant 超兔App as 超兔App
    participant 超兔CRM as 超兔CRM
    participant 客户 as 客户

    销售-&gt;&gt;超兔App: 通讯录/拍名片/微信录入客户
    超兔App-&gt;&gt;超兔CRM: 同步信息，自动抓取工商/百度资讯
    超兔CRM-&gt;&gt;销售: 客户分类（生命周期/三一定级）
    销售-&gt;&gt;超兔App: 语音输入行动（定位/照片）
    超兔App-&gt;&gt;超兔CRM: 同步行动，生成待办
    客户-&gt;&gt;销售: 通话沟通
    销售-&gt;&gt;超兔App: 通话随记，生成下步事务
    超兔App-&gt;&gt;超兔CRM: 同步通话记录，更新时间线
    超兔CRM-&gt;&gt;销售: 待办提醒，红绿灯标识状态</code></pre><h3>2. 核心品牌雷达图（10分制）</h3><table><thead><tr><th>品牌</th><th>客户管理</th><th>销售过程</th><th>团队管理</th><th>总分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>8</td><td>9</td><td>7</td><td>24</td></tr><tr><td>Salesforce</td><td>8</td><td>9</td><td>8</td><td>25</td></tr><tr><td>销售易</td><td>8</td><td>8</td><td>7</td><td>23</td></tr><tr><td>Freshsales</td><td>7</td><td>8</td><td>7</td><td>22</td></tr><tr><td>飞书</td><td>6</td><td>7</td><td>8</td><td>21</td></tr></tbody></table><h2>五、总结</h2><p>CRM选型的核心是“<strong>匹配业务场景+聚焦核心需求</strong>”。超兔一体云凭借“全链路闭环+中国场景适配+BOSS视角”的优势，更适合需要“客户管理-销售过程-团队效能”协同的中国企业；Salesforce、SAP适合大型企业的生态集成；销售易、Freshsales分别聚焦B2B和外贸场景；飞书、纷享销客适合轻量化协作。</p><p>企业选型前需明确：<strong>是需要“全链路管控”还是“单点突破”？是“B2B复杂场景”还是“B2C轻量化”？是“结果导向”还是“过程管理”？</strong> 结合这些问题，对照本文的对比框架，即可找到最适合的CRM工具。</p>]]></description></item><item>    <title><![CDATA[vue导出excel表格并设置表格样式（vxe-table） 毛线团阿阳 ]]></title>    <link>https://segmentfault.com/a/1190000047466628</link>    <guid>https://segmentfault.com/a/1190000047466628</guid>    <pubDate>2025-12-11 16:07:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h4>1.安装</h4><p>npm install xlsx --save<br/>npm install xlsx-style --save<br/>(安装xlsx-style后会报错，解决方案：<a href="https://link.segmentfault.com/?enc=oz%2FMeqVsu7mT8PvGOfjjVg%3D%3D.%2Bs0PSnlxQPFIkpu5Iwi2ACZFisoKNfKXF%2BDyx0%2Bq2L6Y6BeVj7CZv91cOJB825bEPRigS0PvqxVXIB3ZxP1wtg%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/HDdgut/article/details/115356719</a>)</p><h4>2.导出并加表格样式流程</h4><p>创建excel文件<br/>创建一个sheet<br/>将sheet放进excel里</p><p>将已有列表数据整理成想要的格式（如：标题 表头 数据行）<br/>将该数据转成sheet格式（aoa_to_sheet）<br/>然后用循环sheet数据（该数据就是excel表格中的没一个单元格的列表，使用列行命名如A1）<br/>利用单元格cells的名字区别是哪行哪列，然后设置样式</p><p>最后将写完样式的sheet数据用XLSXStyle.write、下载</p><hr/><pre><code>&lt;template&gt;
  &lt;div class="app-container"&gt;
    &lt;el-button type="warning" icon="el-icon-download" @click="exportClick"&gt;导出&lt;/el-button&gt;
    &lt;vxe-table
      :cell-config="{height: 70}"
      :loading="listLoading"
      stripe
      style="width: 100%"
      size="medium"
      border
      resizable
      row-key
      highlight-current-row
      highlight-hover-row
      :height="400"
      :data="tableData"
      align="center"
    &gt;
      &lt;vxe-table-column type="seq" width="60" fixed="left" title="序号" /&gt;
      &lt;vxe-table-column
        field="name"
        align="center"
        title="名字"
        min-width="130"
      /&gt;
      &lt;vxe-table-column
        field="mobile"
        align="center"
        title="手机号码"
        min-width="110"
      /&gt;
      &lt;vxe-table-column
        field="price"
        align="center"
        title="金额"
        min-width="110"
      /&gt;
      &lt;vxe-table-column
        field="team"
        align="center"
        title="所属团队"
        min-width="100"
      /&gt;
    &lt;/vxe-table&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
import XLSX from 'xlsx'
import XLSXStyle from 'xlsx-style'
export default {
  name: 'Test',
  components: {},
  data() {
    return {
      tableData: [
        { name: '张三', mobile: '13300000001', price: '623.00', team: '团队一' },
        { name: '张思', mobile: '13300000002', price: '20.00', team: '团队二' },
        { name: '张武', mobile: '13300000003', price: '90.00', team: '团队三' },
        { name: '张柳', mobile: '13300000004', price: '54.00', team: '团队四' }
      ],
      listLoading: false
    }
  },
  created() {
  },
  mounted() {
  },
  methods: {
    // 导出按钮方法
    exportClick() {
      const workbook = XLSX.utils.book_new()// 创建一个空的excel文件
      const worksheet = XLSX.utils.json_to_sheet(this.tableData)// 将json数据转成sheet格式（创建出一个sheet文件）
      XLSX.utils.book_append_sheet(workbook, worksheet)// 将sheet加进excel文件里

      const tableData = this.tableData
      const columnHeader = {
        'name': '名字',
        'mobile': '手机号码',
        'price': '金额',
        'team': '所属团队'
      } // 此处是表头
      const dealTableLine = this.transferData(tableData, columnHeader)// 用表头和数据换取按行形式的数据
      const sheetsList = XLSX.utils.aoa_to_sheet(dealTableLine)// 再将数据转成sheet格式

      // 1.设置基础框架 列宽、合并等
      sheetsList['!cols'] = [{ wch: 9 }, { wch: 20 }, { wch: 18 }, { wch: 15 }, { wch: 18 }]// 设置字段宽度;从第一列到最后
      sheetsList['!merges'] = [{ s: { c: 0, r: 0 }, e: { c: 4, r: 0 }}]// 设置表标题合并。（s:开始 e:结束）从0列,0行到4列,0行合并

      // 2.循环每一列，设置该列的样式
      const borderstyle = { bottom: { style: 'thin', color: 'FF0000' }, right: { style: 'thin', color: 'FF0000' }}// 右+下边线
      for (const cells in sheetsList) {
        const cells_row_no = cells.replace(/[^0-9]/ig, '')// 去掉字母只留数字：数字代表行数
        const cells_col_no = cells.replace(/[^a-zA-Z]/g, '')// 去掉数字只留字母：字母代表列
        // cells：A1 A2 A3 B1 B2...
        if (cells != '!ref' &amp;&amp; cells != '!merges' &amp;&amp; cells != '!cols') { // 排除几项基础设定
          if (cells_row_no === '1') { // 第一行 标题
            sheetsList[cells].s = {
              font: { name: '宋体', sz: 16, bold: false },
              alignment: { horizontal: 'center', vertical: 'center' },
              border: { bottom: { style: 'thin', color: 'FF0000' }}
            }
          } else if (cells_row_no === '2') { // 第二行 表头
            sheetsList[cells].s = {
              fill: { fgColor: { rgb: 'FFFF00' }},
              font: { name: '宋体', sz: 14, bold: true },
              alignment: { horizontal: 'left', vertical: 'center' },
              border: borderstyle
            }
          } else { // 剩余所有行
            sheetsList[cells].s = {
              font: { name: '宋体', sz: 11, bold: false },
              alignment: { horizontal: 'left', vertical: 'center' },
              border: borderstyle
            }

            if (cells_col_no == 'B') { // B列 名字
              sheetsList[cells].s = {
                font: { name: '宋体', sz: 12, color: { rgb: '0563C1' }, underline: false },
                alignment: { horizontal: 'left', vertical: 'center' },
                border: borderstyle
              }
            } else if (cells_col_no == 'D') { // D列 金额
              sheetsList[cells].s = {
                font: { name: '宋体', sz: 14, color: { rgb: 'ff0000' }, underline: true },
                alignment: { horizontal: 'left', vertical: 'center' },
                border: borderstyle
              }
            } else {}
          }
          // A列序号列设置居中
          if (cells_col_no == 'A') {
            sheetsList[cells].s.alignment.horizontal = 'center'
          }
        }
      }

      // 数据循环完毕
      workbook['SheetNames'] = ['测试sheet']
      workbook['Sheets'] = { '测试sheet': sheetsList }
      this.exportFile(this.sheet2blob(workbook), '测试导出表格.xlsx')
    },

    // 把表头和数据整理成按行的形式
    transferData(data, columnHeader) {
      const content = []
      const otitle = '测试表格标题'
      content.push([otitle])// 1.第一行 表格标题名字

      const header = []
      for (const i in columnHeader) {
        header.push(columnHeader[i])// 生成表头行
      }
      header.unshift('序号')
      // header: ['序号', '名字', '手机号码', '金额', '所属团队']
      content.push(header)// 2.第二行 表头行

      data.forEach((item, index) =&gt; {
        const arr = []
        for (const i in columnHeader) {
          arr.push(item[i])
        }
        arr.unshift(index + 1)
        content.push(arr)// 3.循环 依次插入数据行
      })
      return content
      /**
       * content：
       * [
       *  ["测试表格标题"],
       *  ["序号","名字","手机号码","金额","所属团队"],
       *  [1,"张三","13300000001","623.00","团队一"],
       * ]
       */
    },

    // 转xlsx-style的download
    sheet2blob(workbook) {
      const wbout = XLSXStyle.write(workbook, {
        bookType: 'xlsx', // 要生成的文件类型
        bookSST: false, // 是否生成Shared String Table，官方解释是，如果开启生成速度会下降，但在低版本IOS设备上有更好的兼容性
        type: 'binary'
      })

      const blob = new Blob([s2ab(wbout)], {
        type: 'application/octet-stream'
      }) // 字符串转ArrayBuffer

      function s2ab(s) {
        const buf = new ArrayBuffer(s.length)
        const view = new Uint8Array(buf)
        for (let i = 0; i != s.length; ++i) view[i] = s.charCodeAt(i) &amp; 0xFF
        return buf
      }
      return blob
    },

    // 下载文件方法
    exportFile(url, saveName) {
      if (typeof url === 'object' &amp;&amp; url instanceof Blob) {
        url = URL.createObjectURL(url) // 创建blob地址
      }
      const aLink = document.createElement('a')
      aLink.href = url
      aLink.download = saveName || '' // HTML5新增的属性，指定保存文件名，可以不要后缀，注意，file:///模式下不会生效
      let event
      if (window.MouseEvent) event = new MouseEvent('click')
      else {
        event = document.createEvent('MouseEvents')
        event.initMouseEvent('click', true, false, window, 0, 0, 0, 0, 0, false, false, false, false, 0, null)
      }
      aLink.dispatchEvent(event)
    }

  }
}
&lt;/script&gt;
&lt;style scoped&gt;
&lt;/style&gt;
</code></pre>]]></description></item><item>    <title><![CDATA[使用Amazon Bedrock和Pipecat构建低延迟智能语音Agent 亚马逊云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047466643</link>    <guid>https://segmentfault.com/a/1190000047466643</guid>    <pubDate>2025-12-11 16:06:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><a href="https://link.segmentfault.com/?enc=a9tuEuK0EtPE5rQHXn0JUw%3D%3D.btZ%2FYqbJLTBALf5x7VT5ncVBrFqFb85fSY94imnUHsUmbmwRv5ppPSvkxv0Pur0Od0r4p%2BnmLEdSXbiFh7pVyJlIZ%2FKsW2vss8QlivWvZlc%3D" rel="nofollow" target="_blank"><img referrerpolicy="no-referrer" src="/img/remote/1460000047466645" alt="" title=""/></a></p><p>在生成式AI与语音交互技术快速发展的当下，如何高效构建低延迟、个性化、自然对话体验的智能语音Agent，已逐渐成为业界关注的焦点之一。</p><p>智能语音Agent的应用领域广泛，包括智能设备语音交互（如具身机器人、智能音箱）、个人助理、自动化客服（如餐厅预订、销售、保险、预约安排）、营销、语言教学（如英语口语学习）、健康医疗以及多模态内容创作等。</p><p>本篇博客将首先介绍构建智能语音Agent的核心组件和延迟优化建议，接着将利用Pipecat开源框架和Amazon Bedrock服务，打造一个支持用户打断、多轮上下文管理的实时交互智能语音Agent</p><h2>一、智能语音Agent核心组件</h2><p><a href="https://link.segmentfault.com/?enc=U1XoZmzfog6IM4fKhJK6Gw%3D%3D.bmQubcz7QFeOXB0qxox5qx6Uel56Cnxd8mS9Qyg2s7vfMBVcAC4Okq8kHu29MA3qSDD7K4lRVt%2BXOodj4GbR0M1DXwL7bfTwsEwFMlQ3wNZf9P1jjDU0q3KqeLBAokQIMaXP6UJnzl1AVJz%2FcJJKoA%3D%3D" rel="nofollow" target="_blank"><img referrerpolicy="no-referrer" src="/img/remote/1460000047466646" alt="" title="" loading="lazy"/></a> </p><p>智能语音Agent结合了基础模型的文本/语音识别、理解和推理能力，旨在提供实时、自然、连续的语音交互体验。一般来说，构建智能语音Agent通常需要包含以下核心组件：</p><ul><li><strong>VAD( Voice Activity Detection )</strong> ：检测音频中是否存在人类语音</li><li><strong>EOU(End of Turn/Utterance )</strong>  ：检测说话者是否已经完成了他们的发言</li><li><strong>STT (Speech To Text)</strong> ：也称为自动语音识别（ASR），将给定音频转录为文本</li><li><strong>LLM</strong> <strong>和 LLM Agent</strong>：大语言模型，如 Amazon Nova/Nova Sonic，DeepSeek，Anthropic Claude系列模型</li><li><strong>TTS( Text To Speech)</strong> ：也称为语音合成，从文本生成自然且清晰的语音</li></ul><p>通过将上述组件组合成一条Pipeline，即可构建出智能语音Agent。随着生成式AI技术的进步，业界发展出了端到端语音模型（即Speech to Speech语音模型），该模型可实现语音输入到语音输出的全链路处理，例如Amazon Nova Sonic就是一款由Amazon研发的Speech to Speech语音模型。端到端语音模型内置了VAD、EOU、STT、LLM、TTS等集成功能，能够实现更低的延迟。这类模型使得构建语音Agent更为轻松便捷。</p><p><a href="https://link.segmentfault.com/?enc=uTVS%2FwJ1MGKLj9JuAnhl8Q%3D%3D.fAgGhgz7aICwDZNvOfy%2B6Dwn%2BeBQD%2FxL1%2BZV1fF62ruVoR9Bo4CeRfMMIKaFe9OXsKYiYWBzs7UNWaKn%2FeRpBw%3D%3D" rel="nofollow" target="_blank">Amazon Nova Sonic</a> 是一款语音理解和生成模型，可提供自然的类人语音对话式人工智能，并且实现了低延迟和行业领先的性价比。该模型提供流畅的对话处理、自适应语音响应、内容审核、API调用和基于RAG的知识库集成，同时提供高度自适应且引人入胜的用户体验。</p><p><img width="723" height="123" referrerpolicy="no-referrer" src="/img/bVdnkpn" alt="image.png" title="image.png" loading="lazy"/></p><p>这两种方案各有优缺点：Pipeline方案可以对各个部分进行精细控制，但其缺点在于语音到文本的来回转换可能导致部分声音信息丢失，并且延迟相对较大。端到端语音模型方案延迟更低，实现更为简单，并且能够更好地感知声音信息，例如非语言线索（如笑声、犹豫）、语调、重音、风格、情绪等，但对语音如何流入和流出Agent的控制相对较少。</p><p>需要注意的是，在当前阶段，SOTA LLM（前沿大语言模型）相比于Speech to Speech语音模型，在成本、推理能力、指令遵循和函数调用等方面仍占据优势。但不可否认，Speech to Speech模型是语音Agent的未来。</p><blockquote><p>📢限时插播：无需管理基础设施，利用亚马逊技术与生态，快速集成与部署生成式AI模型能力。</p><p>✨ 精心设计，旨在引导您深入探索Amazon Bedrock的模型选择与调用、模型自动化评估以及安全围栏(Guardrail)等重要功能。</p><p>⏩快快点击进入《<a href="https://link.segmentfault.com/?enc=RsbzLnnvHv0VNczKpLzgaA%3D%3D.P4xYVr%2Ba6YCN2xP%2Bo9KVq7rSuSq7BC58nK9U%2FFaK8fXEHTs8uMaVJqedpM38T5blU2hbSrwVg7RHEBeewmONFtV5fognceFwTWv7axMS1g69InConiCYR8%2BhEJebvx6yrhjcix0VUNcvdIO6q3iN65ceprMlGZXMhaWxijqHblCcAnx8wzGsagpostBlEqI3EU4Mhc2IjcuR6Y9k67BFj6o1%2Be0bImzBOJ023u82egs%3D" rel="nofollow" target="_blank">多模一站通 —— Amazon Bedrock 上的基础模型初体验</a>》实验构建无限, 探索启程！</p></blockquote><h2>二、传输协议对比</h2><p>要构建自然流畅的智能语音Agent，传输协议的选择至关重要，它们直接影响着语音流的传输效率和实时性。常见的传输协议有WebSocket，WebRTC等，它们有各自的特点，详细对比如下。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466648" alt="image.png" title="image.png" loading="lazy"/><br/>通过对比可以看出，WebSocket兼容性更好，WebRTC对音视频的传输做了很多优化，传输效率更高。一般来说，对于构建原型和轻量级项目，可以选择Websocket，对于中大型生产项目，WebRTC是更优的选择。但WebRTC协议复杂，部署也很复杂，需要实现信令服务器、STUN服务器（公网IP和端口发现），TURN服务器（P2P连接失败时作为媒体中继服务器，实现诸如NAT穿透）。因此构建一个成熟稳定的WebRTC方案，难度比较大。目前市面上有<a href="https://link.segmentfault.com/?enc=zWH7lzD9QNo0dPxs7yCEcg%3D%3D.DMvuIygKVZAbds0yVLVWRrfibaNvFFBmb3xpyAwCyY3Y6JCBUhKOo%2BkvzEsSEenR" rel="nofollow" target="_blank">Livekit</a>开源框架，同时也有Amazon KVS、Daily、Livekit Cloud等商业WebRTC服务可供选择。</p><p>使用WebRTC有两种主要方式：一是通过云端的WebRTC服务器中转，商业WebRTC服务多采用此模式；二是直接在客户端和语音Agent端之间建立连接。云端服务器模式可以实现直连模式无法提供的诸多特性，例如多方会话、多方录音等。而直连模式则非常适合语音AI Agent的客户端-服务器场景，它减少了服务器中转环节，并且无需维护任何特定于WebRTC的基础设施。</p><p>Tips:</p><p>自建WebRTC服务可以使用公开STUN服务器：<a href="https://gist.github.com/mondain/b0ec1cf5f60ae726202e" target="_blank">https://gist.github.com/mondain/b0ec1cf5f60ae726202e</a>。可以根据语音Agent的部署位置选择合适的STUN服务器。</p><p>WebRTC服务使用UDP协议进行连接，在亚马逊云部署时需要在安全组开放对应的UDP端口。</p><h2>三、智能语音Agent延迟优化建议</h2><p>延迟是影响人与语音Agent之间对话体验的关键因素。人类期望在正常对话中获得快速响应，长时间的停顿会显得不自然（人机对话的典型响应时间通常为500毫秒）。因此，延迟优化对于智能语音Agent来说至关重要。</p><p>根据作者基于Amazon Bedrock构建智能语音Agent的实践经验，建议综合考虑以下方式优化延迟技术。</p><ul><li>语音Agent部署尽量靠近用户，减少网络传输延迟。</li><li>使用传输效率更高、延迟更低的传输协议，如 WebRTC。</li><li><p>LLM 延迟优化：LLM的延迟在整个语音Agent的延迟中占据主要部分，因此对LLM进行延迟优化显得尤为关键。在满足要求的前提下，可以采用以下手段进行优化。</p><ul><li>优先选择端到端语音模型，这种模式一般比STT-LLM-TTS的Pipeline模式延迟更低。</li><li>选择参数量更小/推理速度更快的模型（例如Nova Lite，Claude 3.5 Haiku等）。</li><li>使用Bedrock上支持延迟优化的模型（例如Nova Pro，Claude 3.5 Haiku等）</li><li>开启 Prompt caching</li></ul></li><li>Pre-LLM TTS 填充，在用户对话前预先输出内容（如自我介绍），给用户体感上的快。</li><li>执行长时间函数调用之前，输出提示信息，例如“处理中，请稍后…”，从而减少客户的等待时间。</li><li>通过LLM提示词引导，缩短回复内容。</li></ul><p>典型的Pipeline模式和端到端语音模型延迟对比如下（请注意，不同方案和组件的延迟差异较大，以下数据仅供参考）。在设计智能语音Agent时，将语音端到端延迟控制在800至1000毫秒是一个不错的目标。</p><p><img width="710" height="296" referrerpolicy="no-referrer" src="/img/bVdnkpy" alt="image.png" title="image.png" loading="lazy"/></p><h2>四、使用Pipecat框架构建智能语音Agent</h2><p>构建一个智能语音Agent并非易事。除了实现上文所述的核心组件，还需要考虑如何存储会话上下文、接入外部知识库或对接后端系统等功能。使用<a href="https://link.segmentfault.com/?enc=a2QRm1luP%2FXsMUVOBHvkSA%3D%3D.X7rThW9HC7EJArdMDsftuqGDvoaiMW7RVjB3g1uX%2FVbF2Q0TsEKi96muynPIy%2FF7" rel="nofollow" target="_blank">Pipecat</a> 开源框架可以显著简化智能语音Agent的开发过程。</p><h3>4.1 Pipecat框架介绍</h3><p>Pipecat是一个开源的Python框架，专为构建实时语音和多模态对话Agent而设计。它能够轻松协调音频/视频流、AI服务、多种传输方式以及对话流程，从而让开发者更专注于打造独具特色的Agent。</p><p><strong>Pipecat</strong> <strong>主要特性包括：</strong></p><ul><li>低延迟实时交互</li><li>支持Agentic Workflow，可集成各类工具（tools）</li><li>支持 WebRTC、WebSocket等传输协议</li><li>灵活的模型和服务选择，如 Amazon Bedrock，Polly，Transcribe及其它主流的模型。</li><li>支持用户打断</li><li>多模态</li></ul><h3>4.2 方案介绍</h3><p>接下来，我们将借助一个示例项目，探讨如何基于Pipecat框架，并结合Amazon Bedrock、Amazon Polly和Amazon Transcribe等服务来构建智能语音Agent。<a href="https://link.segmentfault.com/?enc=QN8MgdF3TMe0pI6dtDadjg%3D%3D.12TXwBqnhhx%2B6bgdfsd8oD5G%2B1OLavM1VU2I%2F4bHWQetFRdar3mCHa0LzDiboCI1" rel="nofollow" target="_blank">Amazon Bedrock</a>是用于构建生成式 AI 应用程序和Agent的托管服务，支持多种自研和第三方大模型，例如Amazon Nova、Nova Sonic、DeepSeek、Anthropic Claude系列模型。<a href="https://link.segmentfault.com/?enc=AqbHIdbC3iW0QB5nT1bTGA%3D%3D.Ff5YFVz1DbbvN9gISd1WtFiU8JzO7juiAgN7WuAieTSE1dXcBHYb3fAA2Ye%2FqIfT" rel="nofollow" target="_blank">Amazon Polly</a>是一项完全托管的服务，可按需生成语音，将任意文本转换为音频流（即TTS），并支持数十种语言。<a href="https://link.segmentfault.com/?enc=Tz13w3hhNCZWbfWzGSHTmQ%3D%3D.x13fQWcKiJCUoX8XoqUBe3xayrnHrvjvoHVpV5fE3wtvE2JWh%2BsM%2Bz4vbBsAl7Fr" rel="nofollow" target="_blank">Amazon Transcribe</a> 是一项完全托管的自动语音识别（ASR）服务，自动将语音转换为文本。</p><p>该示例项目演示了如下功能：</p><ul><li>支持Pipeline模式和端到端语音模式（使用Amazon Nova Sonic模型）。</li><li>使用WebRTC作为传输协议。</li><li>通过Tools集成知识库，该知识库包含了2025年亚马逊云科技中国峰会的相关内容。</li><li>提供Web前端，用于与Agent进行语音交互。</li></ul><p>完整的示例代码见Github代码仓库: <a href="https://link.segmentfault.com/?enc=M610fw8z7B%2B7zKBPMO0Zqg%3D%3D.1QQQwgKIMl5utP33oP%2FHcZq0MjpMCU30FWq5di1IbubqdmXskr%2BGanMBuGgm79oayiST%2BT6BtCGA3wK5BiWc5OmtnBrXVyvH19MSoEwL8wI%3D" rel="nofollow" target="_blank">https://github.com/freewine/sample-voice-agent-with-Amazon-Bedrock-and-Pipecat</a></p><p>使用Pipecat构建智能语音Agent的逻辑架构如图所示。</p><p><img width="723" height="310" referrerpolicy="no-referrer" src="/img/bVdnkpG" alt="image.png" title="image.png" loading="lazy"/></p><h3>4.3 Agent核心代码</h3><p>使用Pipecat构建语音Agent的关键在于工作流的搭建。以下是Pipeline模式的示例代码，从中可以看出，通过STT、LLM和TTS等服务构建了一条完整的Pipeline。为便于阅读和理解，我们已对代码进行简化，完整代码请访问Github仓库。</p><pre><code>transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)

stt = AWSTranscribeSTTService()
tts = AWSPollyTTSService(voice_id=“Joanna”)
llm = AWSBedrockLLMService( model="apac.amazon.nova-pro-v1:0")
context = AWSBedrockLLMContext(messages, tools)
context_aggregator = llm.create_context_aggregator(context)
pipeline = Pipeline(
    [
        transport.input(), # Transport user input
        stt, # STT
        context_aggregator.user(), # User responses
        llm, # LLM
        tts, # TTS
        transport.output(), # Transport bot output
        context_aggregator.assistant(), # Assistant spoken responses
    ]
)
 task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
    ),
)</code></pre><p>如果使用Speech to Speech模型，可以省去TTS和STT，实现端到端语音输入输出。示例代码如下。</p><pre><code>transport = SmallWebRTCTransport(
    webrtc_connection=webrtc_connection,
    params=TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
)

# Create the AWS Nova Sonic LLM service
speech_to_speech = AWSNovaSonicLLMService(
    secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
    access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
    region=os.getenv("AWS_REGION"),
    voice_id="tiffany",
)
context = AWSBedrockLLMContext(messages, tools)
context_aggregator = llm.create_context_aggregator(context)
pipeline = Pipeline(
    [
        transport.input(), # Transport user input
        context_aggregator.user(), # User responses
        speech_to_speech, # Speech to Speech model
        transport.output(), # Transport bot output
        context_aggregator.assistant(), # Assistant spoken responses
    ]
)
 task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,
    ),
)</code></pre><h3>4.4 系统提示词最佳实践</h3><p>语音Agent与文字Agent的系统提示词在核心原则上是相通的，但语音Agent具有其特殊性，需要额外考虑多方面因素，例如口语化的适应、非语言信息的处理、错误纠正和澄清等。以下是作者在构建语音Agent时总结的几点经验：</p><ol><li>由于STT/ASR模型在实时流中可用的上下文信息有限，语音转录时很可能出现错误。好在当前的LLM已足够智能，在进行推理时可以访问完整的对话上下文。因此，我们可以通过系统提示词告知LLM，输入为用户语音的转录文本，指示其进行相应推理以纠正转录错误。建议在系统提示词添加如下的内容：When you receive a transcribed user request, silently correct for likely transcription errors. Focus on the intended meaning, not the literal text. If a word sounds like another word in the given context, infer and correct.</li><li>鉴于LLM的推理结果将用于TTS进行语音合成，因此可在系统提示词中要求其避免输出难以发音的内容：Your output will be converted to audio so don’t include special characters in your answers.</li><li>保持Agent语音输出的简洁性，打造更好的对话体验，建议在系统提示词里添加如下约束：Keep your responses brief, generally two or three sentences for chatty scenarios.</li></ol><p><strong>参考文件</strong></p><ol><li>Pipecat: <a href="https://link.segmentfault.com/?enc=BKMRlvJBuHHFZTgNSFqQGw%3D%3D.FDKeyjMG3RfJkR%2FgGGAoae0no8bYOCzlvnO0oDvkc06cuFHr4rsQIXQjW9bWYxLE" rel="nofollow" target="_blank">https://github.com/pipecat-ai/pipecat</a></li><li>Amazon Nova Sonic: <a href="https://link.segmentfault.com/?enc=ULA7Thm1m7ZPA7LVknQ2nw%3D%3D.SYwn7ghnAZdS1R93EfLHp0PmtaV6%2FJNhssKnydQSCwRIM%2BpalWocx3tHIb0O8LzWLLi2%2FCANPiRjyHTE5ufSqw%3D%3D" rel="nofollow" target="_blank">https://aws.amazon.com/ai/generative-ai/nova/speech/</a></li><li>Amazon bedrock：<a href="https://link.segmentfault.com/?enc=Q0wuAmDrC9pZrSXMBcQ1gQ%3D%3D.VXSi6ui5AdjfPDmZhX5mvs%2FIXn%2FxThjKnDVw5sYezzw%3D" rel="nofollow" target="_blank">https://aws.amazon.com/bedrock/</a></li></ol><p><em>*前述特定亚马逊云科技生成式人工智能相关的服务目前在亚马逊云科技海外区域可用。亚马逊云科技中国区域相关云服务由西云数据和光环新网运营，具体信息以中国区域官网为准。</em></p><p><strong>本篇作者</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466651" alt="image.png" title="image.png" loading="lazy"/></p><blockquote><p>本期最新实验《<a href="https://link.segmentfault.com/?enc=zQ0sd0tx6Y%2BCOHukV6DGag%3D%3D.mtBIqLnezc6krgWUgkvwOokzt3cNVK3dVDSAxExurkQo1O%2B8lGnU%2B9OGaQBOp29HwIW%2FPS5%2F9K7%2B7fEG2dhk%2FA1bEvqhR%2B9J7Dg8T7ocvE9ekATricXObXCPRLNiaPb2GitZZZSR7tMSAhFLfUT5wms1p%2B2opzUh%2Btq1m80CdChLeLOmXZ1Sif1fyb3%2Bnb%2FjRdrLTXubnliAadiPJBD91azUAraEzKAXjCqdOjv16Vo%3D" rel="nofollow" target="_blank">多模一站通 —— Amazon Bedrock 上的基础模型初体验</a>》</p><p>✨ 精心设计，旨在引导您深入探索Amazon Bedrock的模型选择与调用、模型自动化评估以及安全围栏(Guardrail)等重要功能。无需管理基础设施，利用亚马逊技术与生态，快速集成与部署生成式AI模型能力。</p><p>⏩️<a href="https://link.segmentfault.com/?enc=aAtskagATiGUpUcnP4Py1Q%3D%3D.yvsKI%2FzyhRQE5c2j5rsPzYd%2FJX%2FkFipsrfqqK1gT2qZmORknYSq3o3xa72u6yhyuTiRF%2BvsIyIKXgCC7pK6bSZMSJ5%2BzYfesybaufbtPT3519UD8Cfy16MUrPB%2FGFbtE7SzABvIMxNTytXUODbmkKMxtvGN%2Fp7Taw5dG0%2FdtZUWdkoyUz%2FDjDYK2VN5lCfzArhyItqDbhOxJbE0MIYf%2F9r41CC3YNsKXQWzguZLvslU%3D" rel="nofollow" target="_blank">[点击进入实验</a>] 即刻开启  AI 开发之旅</p><p>构建无限, 探索启程！</p></blockquote>]]></description></item><item>    <title><![CDATA[揭秘“认养农业”：物联网与区块链如何守护你的私家菜园？ 张老师讲数字孪生 ]]></title>    <link>https://segmentfault.com/a/1190000047466658</link>    <guid>https://segmentfault.com/a/1190000047466658</guid>    <pubDate>2025-12-11 16:05:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年初，万东镇五和村的黄花梨园还未开花，每棵梨树已被提前认养，认养人可通过手机随时查看果树生长情况。这种 “认养农业”新业态正快速普及，其背后是数字孪生技术提供的精准管护支持。<br/>当你在城市家中通过手机屏幕 “云种植” 一棵梨树或一片菜地时，一个复杂的数字农业系统正在幕后运作。认养农业的核心在于建立了从农田到手机的直连通道，而支撑这一通道的，是数字孪生技术构建的“虚拟农场”与物理农场的实时联动。<br/><img width="694" height="446" referrerpolicy="no-referrer" src="/img/bVdnko2" alt="" title=""/></p><h2>技术基石：数字孪生如何实现农田可视化</h2><p>认养农业首先需要解决的是信任问题。借助部署在农场的高清摄像头、土壤传感器和气象站等物联网设备，物理世界的农事活动被实时数字化。例如，在妙隘寨石农庄的智慧果园，土壤湿度、养分含量、气温等数据被实时监测并传输至管理平台。这些数据通过卡尔曼滤波算法进行融合处理：<br/><img width="244" height="49" referrerpolicy="no-referrer" src="/img/bVdnko3" alt="" title="" loading="lazy"/></p><p>其中x_k为系统状态向量，通过这一算法，系统将不同精度、频率的传感数据统一为高质量信息，实现农田状态的精准可视化和远程查看。<br/><img width="723" height="253" referrerpolicy="no-referrer" src="/img/bVdnko4" alt="" title="" loading="lazy"/></p><h2>智能灌溉：从人工判断到AI决策</h2><p>传统农业依赖农民的经验判断何时灌溉、施肥，而认养农业则通过数据驱动决策实现精准管护。安装在田间的传感器持续收集土壤湿度数据，当系统检测到某块区域需要灌溉时，会自动下达指令。<br/>智能灌溉的核心在于多目标优化算法，系统需同时考虑作物需水量、土壤湿度、天气预报等多重因素：<br/><img width="291" height="54" referrerpolicy="no-referrer" src="/img/bVdnko5" alt="" title="" loading="lazy"/><br/><img width="276" height="61" referrerpolicy="no-referrer" src="/img/bVdnko7" alt="" title="" loading="lazy"/></p><p>其中f_i为各目标函数，g_j为约束条件。通过这一算法，系统能在满足作物生长需求的同时，实现水资源的最优分配。</p><h2>溯源体系：从农田到餐桌的全程透明</h2><p>认养农业最具创新性的部分在于构建了完整的农产品溯源系统。以五和村的黄花梨为例，认养人可以通过扫码了解果树从开花到结果的全过程，包括施肥、除虫等关键农事操作的时间、用量等详细信息。<br/>该系统采用区块链技术确保数据不可篡改，每个环节的信息都被记录在分布式账本中：<br/><img width="304" height="50" referrerpolicy="no-referrer" src="/img/bVdnko9" alt="" title="" loading="lazy"/></p><p>其中H_n为当前区块哈希值，T_n为当前时间戳。这种机制保证了溯源信息的真实可靠，增强了消费者对农产品质量的信心。</p><h2>实践案例：技术支持与应用前景</h2><p>在具体应用中，数字孪生技术为农业项目提供支持。例如，凡拓数创在相关智慧农业项目中，通过构建可视化管理系统平台，可以实现园区运营数据的实时监测与分析。该系统能够整合传感器网络与业务数据，为农事决策提供可视化支持。<br/><img width="723" height="265" referrerpolicy="no-referrer" src="/img/bVdnkpa" alt="" title="" loading="lazy"/></p><p>在智慧农业领域，数字孪生技术通过集成物联网设备与三维可视化能力，可构建农场的虚拟映射，辅助管理者优化种植策略。这种技术路径为认养农业的实现提供了底层架构支持。<br/><img width="723" height="341" referrerpolicy="no-referrer" src="/img/bVdnkpk" alt="" title="" loading="lazy"/></p><p>认养农业的兴起，标志着农业生产从 “经验驱动”向数据驱动的转变。通过数字孪生技术，消费者与生产者之间建立了前所未有的连接，这种连接不仅改变了农产品的销售方式，更正在重塑现代农业的生产模式和管理理念。</p>]]></description></item><item>    <title><![CDATA[施工现场如何做好消防安全管理 温文尔雅敲代码 ]]></title>    <link>https://segmentfault.com/a/1190000047466660</link>    <guid>https://segmentfault.com/a/1190000047466660</guid>    <pubDate>2025-12-11 16:04:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>施工现场因其作业强度高、环境复杂、人员流动性大、火源集中、易燃物多等特点，常年处于火灾高风险状态，一旦管理不到位，极易酿成事故。</p><p>本文将结合2025年11月1日正式实施的《建设工程施工现场消防安全技术标准》（GB/T 50720-2011），参照相关法律法规、技术规范以及近期的监管重点，分析施工现场常见的消防隐患与规范化管理建议。</p><h2>一、为什么工地火灾难以根除</h2><p>施工人员安全意识淡薄、管理漏洞百出，动火、用电不规范……这些都可能是造成建筑工地火灾的原因：</p><h3>1、易燃可燃物存放管理不当</h3><p>建筑工地因施工需求大量存在木料、油漆、油料、沥青、架板、各种装饰材料、复合管材等易燃可燃材料，极易引发火灾。同时，焊接用的氢气瓶、氧气瓶、乙炔等易燃易爆物品，没有妥善存放，一旦着火极易引发爆炸。</p><h3>2、动火作业不规范</h3><p>建筑工地进行电焊、气割等明火作业本身容易火花飞溅，若作业人员无证上岗或在作业前没有进行动火审批、没有清理周围可燃物、没有落实防范措施，都极易引发火灾。</p><h3>3、临时建筑使用违规材料</h3><p>部分单位为了减少成本，违规采用易燃可燃的夹心彩钢板，且建筑内空间划分不明确，极易出现生活杂物堆积，甚至是住宿、食堂和餐厅设立在同一空间的“三合一”现象，更是增加了火灾发生的风险。</p><h3>4、杂物堆积堵塞安全通道</h3><p>虽然大部分工地本身设有消防通道，但因管理不当等问题，常被垃圾杂物堆积堵塞、或被材料临时占道。一旦发生火灾，无法及时救援，会使得火灾影响范围扩大。</p><h3>5、施工人员安全意识淡薄</h3><p>建筑工地人员流动量大，临时人员多，部分施工人员没有经过系统的消防安全培训，消防安全意识相对薄弱，缺乏安全常识，导致防范能力不足。</p><h2>二、如何规范化管理工地消防安全</h2><p>GB/T 50720-2011《建设工程施工现场消防安全技术标准》第6章对工地的防火管理有明确说明。结合《安全生产法》、《机关、团体、企业、事业单位消防安全管理规定》、《建筑防火通用规范》等，可参考以下几个方面实现工地规范化管理：</p><h3>1、落实消防安全责任制度</h3><p>依据第6.1.1条至6.1.3条，建设、施工、监理单位应依法承担消防安全责任。实行总承包时，分包单位应向总承包单位负责。施工单位必须建立消防安全管理组织，确定消防安全负责人，并落实相关人员责任。</p><h3>2、强化员工消防安全培训</h3><p>所有施工人员在正式上岗前需接受消防安全教育，确保工作人员都具备必要的防火、灭火基本知识，未经培训或考核不合格者，不得上岗作业。施工单位应建立培训档案，定期复训。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466662" alt="file" title="file"/></p><p>在实际管理中，不少单位会为每位员工生成“一人一码”，贴在安全帽、反光背心或工牌上，通过二维码当方式记录其身份信息、照片、三级教育和安全培训等资料。现场检查时，微信扫码即可查看人员信息，实现“看码识人、扫码查档”，方便管理与核验。</p><h3>3、严格工地用火管理</h3><p>在宿舍、仓库、易燃易爆物品存放区等高风险场所严禁使用明火，禁止吸烟或乱扔烟蒂，防止引发火灾事故。</p><p>明确动火作业（含焊枪操作等）必须严格执行“先审批、后作业”制度，特种作业人员持证上岗，作业前应清理周边10米范围内易燃可燃物，配备灭火器材及专人现场监护，作业完成后及时检查确认无火灾隐患方可撤离，严禁无审批动火、违规动火行为。</p><ul><li>动火作业必须提前办理审批手续，明确动火时间、地点、作业人员及监护人等信息。</li><li>动火前，需清理作业现场及周边的易燃、可燃物，配备足够的灭火器材，并设置防火隔离措施。</li><li>作业过程中，安排专人监护，确保动火安全；动火结束后，彻底清理现场，防止余火复燃。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466663" alt="file" title="file" loading="lazy"/></p><p>但在实际操作中，动火申请常用纸质单据，存在单据流转慢、不易保存等问题。可以考虑将纸质登记文件电子化，比如使用二维码，代替动火作业申请和审批功能。作业人员扫码登记作业信息，实现全流程在线提交与审核，大幅提升审批速度，简化操作步骤。</p><p>现场监管人员扫描二维码就能查看审批记录，确保所有动火作业经过正式审批，提升安全管理，降低风险。</p><h3>4、规范敷设电气线路</h3><p>工地电路由持证上岗电工专人敷设，严禁私自敷设临时线路，严禁超负荷用电，定期对电气线路进行安全检查和保养，下班后应及时关闭总电源，防止电器遗留隐患。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466664" alt="file" title="file" loading="lazy"/></p><h3>5、专库存放易燃易爆品</h3><p>施工现场的施工材料应当分类单独储存，易燃易爆物品应专库专放，保持通风。氧气、乙炔等瓶装气体须分类存放，间距不小于5米，易燃液体应设独立仓库，配备灭火器、防爆灯具等设施。严禁在施工建筑内部存放易燃可燃材料。</p><h3>6、配备齐全的消防设施</h3><p>施工工地应按照规定配备灭火器、消火栓、消防水带、消防水池等消防器材和设施，确保数量充足、性能良好。消防设施应放置在显眼且易于取用的位置，定期进行检查、维护和保养，确保随时可用。</p><p>设置临时消防车道，保证消防车辆能够顺利通行，严禁在消防车道上堆放杂物或停放车辆。</p><h3>7、加强防火巡查与隐患整改</h3><p>安排专人负责每日防火巡查，重点检查用火、用电、易燃易爆物品存放等情况，及时发现和消除火灾隐患。对巡查中发现的问题，按照“三定”原则（定人、定时、定措施）进行整改，整改完成后组织验收，确保隐患彻底消除。</p><p>建立火灾隐患台账，记录隐患情况、整改措施和整改结果，以便跟踪管理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466665" alt="file" title="file" loading="lazy"/></p><p>为减少假检、隐患不整改等现象，许多单位已使用二维码方式进行每日防火巡查，一线人员扫码登记隐患并拍照上传，后续整改与复查形成闭环，台账可随时导出，便于追溯与监督。</p><h2>三、结语</h2><p>施工现场消防安全管理并非一纸制度所能解决，而需要将法规标准落实到每一个作业环节与人员行为中。但无论是人员培训记录、动火审批流程，还是每日防火巡查、隐患整改台账，只靠人工记录难以实现高效闭环管理。</p><p>对大多数施工单位来说，要自己从规范里逐条提炼出一套既合法、又能落地的检查流程和记录表单，不仅耗时耗力，还常常担心理解偏差、标准更新不及时，最后纸面制度流于形式，真正执行起来依旧混乱。</p><p>也正因如此，很多单位会直接借鉴成熟的经验，比如使用草料二维码，草料结合国内消防规范，整理出一套符合国家标准的消防管理模板，覆盖了人员管理、消防设施检查、动火作业、隐患整改等关键环节。</p><p>这些模板还结合了大量用户的真实使用经验，既合规，也实用。对于施工单位而言，这样的工具不仅能减轻管理压力，更能在关键时刻提供“有据可查”的记录保障。</p>]]></description></item><item>    <title><![CDATA[使用 audio2face harusamei ]]></title>    <link>https://segmentfault.com/a/1190000047466745</link>    <guid>https://segmentfault.com/a/1190000047466745</guid>    <pubDate>2025-12-11 16:03:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>啰嗦几句前因</h2><p>友人在做具身智能的创业，他们需要把音频对应成面部动作。 他让我帮他做做前期调研<br/>我不会做音频，跟chatgpt大致QA科普了一下。 原来现在的方式都是将audio转化为苹果的一个脸部动作的标准 Brandshape权重， 然后就可以对应到具身的脸上的齿轮运动。 <br/>我的任务就简化为输入一段 audio 生成BS权重序列<br/>先在GIT上找到一个项目<a href="https://link.segmentfault.com/?enc=dVp0AVrnPqrrAtheR4VR6w%3D%3D.tTBuCs681EzGTjA5XKeTNKPnDiX2fPCWRsYPhpluQqCBJ5YyaoNt2ADUwkGrh4bY" rel="nofollow" target="_blank">https://github.com/FACEGOOD/FACEGOOD-Audio2Face</a> ， 我看了一下那个得自己训练，而且代码有2年没维护了。 就没再研究<br/>然后发现 nvidia 刚发布了 <a href="https://link.segmentfault.com/?enc=dWGLwR6lvMtLyocI0goMcA%3D%3D.GoEnCTKSnA5EcjvpOH%2B71FUb0theSLP05PYeqOJGIhtJ5rTFf1dFBiI0FQ8J0MqA" rel="nofollow" target="_blank">https://github.com/NVIDIA/Audio2Face-3D-SDK</a>, 据说是数字人表情迎来了灵魂时间。 所以从今天开始决定研究一下这个组件<br/>后续我会记录使用这款A2F的过程</p><h2>Audio2Face-3D SDK</h2><p>为“动手派”准备的核心引擎</p>]]></description></item><item>    <title><![CDATA[AI技术在工业质量管理系统中的实际应用案例分析 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047466776</link>    <guid>https://segmentfault.com/a/1190000047466776</guid>    <pubDate>2025-12-11 16:03:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近年来，人工智能（AI）技术在工业领域的渗透程度不断加深，特别是在质量管理系统中的应用，已经从单纯的自动化检测逐步向智能化、预测性方向演进。这一趋势不仅改变了传统制造业对质量的管理方式，还为企业提供了全新的质量提升路径。从实际应用来看，AI技术在工业质量管理系统中的落地场景丰富多样，涵盖了缺陷检测、过程监控、根因分析等多个维度。这些案例不仅展示了AI的潜力，也为企业提供了可借鉴的实践方向。<br/>在电子制造业中，AI的视觉检测能力成为质量管理的重要支柱。比如在苏州乐码电子科技有限公司的案例中，他们的质检团队原本需要6人完成日常检测工作，但通过部署一套基于AI的视觉检测系统，仅用三名人员就能覆盖原本需要更多人力的质检任务。这套系统配备了2000万像素的工业相机，每0.2秒抓拍一次标签图像，借助深度学习算法对超过10万张“合格/缺陷”样本的训练，实现了对溢胶、偏位、漏切等8类问题的自动识别。系统不仅能剔除次品，还能将批次合格率稳定在100%，为企业节省了大量人力成本，同时显著提升了产品质量的一致性。类似的案例在理想汽车也有所体现，他们通过全流量质量管理模式和云端监控预警平台，将AI技术深度嵌入生产流程，不仅提高了检测效率，还为全生命周期质量管理提供了有力支撑。<br/>AI技术在质量管理系统中的另一大应用是在汽车制造领域。广域铭岛的Geega工业互联网平台通过实时数据采集和区块链技术的应用，构建了贯穿原材料到整车生产的质量追溯体系。该系统不仅能记录关键工序的数据，还能通过分析设备振动、温度等数百项参数，预判可能出现的质量风险。例如，在某合资品牌的项目中，系统提前预警了变速箱齿轮的异常磨损问题，帮助生产团队调整了热处理工艺参数，最终避免了可能涉及数千台整车的返工损失。这种从“被动应对”到“主动预防”的转变，正是AI技术在工业质量管理中体现的核心价值。<br/>广域铭岛通过区块链技术实现了“一车一档”的全程可追溯体系。该系统不仅能记录从原材料采购到整车出厂的每个环节数据，还能在质量异动时快速定位问题源头。例如，在某新能源汽车企业应用后，当发现电池包存在潜在质量问题时，系统能在分钟级内追溯到具体供应商、批次及工艺参数异常，相比传统人工排查节省了80%的时间。这种高效的质量追溯能力，为车企提供了从“事后救火”到“事前预防”的管理范式。<br/>制造业的质量管理不仅仅是技术问题，更是数据驱动的决策问题。在某大型制造企业的实践中，通过引入深度学习算法，系统能够自动分析传感器数据、工艺参数记录等多维度信息，快速定位质量问题的根本原因。例如，某半导体封测企业利用AI根因分析系统，将复杂质量问题的分析时间从原来的数天缩短至几小时内，极大地提升了问题解决效率。<br/>当然，AI在工业质量管理系统中的应用并非一帆风顺。企业在实施过程中需要面对数据整合、系统兼容等多重挑战。比如，在广西华昇新材料有限公司的案例中，他们最初面临自动化水平低、数据孤岛严重的问题。通过引入数字孪生、AI视觉分析等技术，并依托大模型，企业逐步实现了生产过程的智能优化。这种系统化的解决方案不仅需要技术支撑，还需要企业具备一定的数据基础和管理能力。</p>]]></description></item><item>    <title><![CDATA[2025适合个性化内容IP地址查询API对比推荐 香椿烤地瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047466781</link>    <guid>https://segmentfault.com/a/1190000047466781</guid>    <pubDate>2025-12-11 16:02:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>IP地址查询API可以根据网站的访问用户地理位置为锚点为其提供个性化内容推荐。</p><p>正值年底,给出一些个性化内容IP地址查询API的对比推荐。</p><p>IP地址查询API能够通过特定的秘钥访问IP地址查询数据库来进行有关位置的数据信息，而不同的服务商都具有不同的特点，因此，选择适合自己的个性化内容IP地址查询API是很重要的事情，各位开发者们可以通过我的文章来进行一些购买使用前的调查。<br/><img width="726" height="450" referrerpolicy="no-referrer" src="/img/bVdnkqU" alt="2025适合个性化内容IP地址查询API对比推荐①.png" title="2025适合个性化内容IP地址查询API对比推荐①.png"/><br/><strong>Ipdatacloud.com-API</strong></p><ul><li>API返回字段：IP地址、国家/地区、省份、城市、区县、街道、运营商、ISP、组织、ASN、经纬度（精准）、是否代理/VPN/爬虫、网络类型（移动/电信/联通）、是否国内IP</li><li>应用场景：</li></ul><p>1. 网站/APP风控反欺诈；</p><p>2. 合规审计（GDPR/CCPA）；</p><p>3. 精准广告定向投放；</p><p>4. 用户画像与行为分析；</p><p>5. 物流定位匹配；</p><p>6. 网络安全防护；</p><p>7. 离线数据库部署（企业版）</p><p>8. IP归属地私有化定制/IP风险防控策略定制/IP应用场景定制/数据格式定制/监测数据维度定制【根据需求量身定制解决方案】</p><ul><li>性能阐述：</li></ul><p>1. 响应时间≤10ms；</p><p>2. 支持高并发；</p><p>3. 数据准确率99.9%；</p><p>4. 离线数据库每日更新；</p><p>5. 支持批量查询（单次≤1000条）；</p><p>6. 无调用次数限制（企业版）</p><p><strong>IP2ReGion-API-API</strong></p><ul><li>API返回字段：IP地址、国家、省份、城市、区县、运营商、ISP，支持多语言返回</li><li>应用场景：</li></ul><p>1. 本地IP查询；</p><p>2. 嵌入式设备；</p><p>3. 服务器日志离线分析；</p><ul><li>性能阐述：</li></ul><p>1. 离线查询速度达百万级/秒；</p><p>2. 数据库体积仅数MB；</p><p>3. 提供开源版；</p><p>4. 支持自定义数据库部署</p><p><strong>Maxmind-API</strong></p><ul><li>API返回字段：IP地址、国家/地区、省份、城市、邮政编码、经纬度、ISP、组织、ASN、风险评分、是否代理/爬虫/数据中心IP</li><li>应用场景：</li></ul><p>1. 跨境电商风控与反欺诈；</p><p>2. 全球广告定向投放；</p><p>3. 合规审计（GDPR/CCPA）；</p><p>4. 网络安全威胁检测；</p><p>5. 国际物流定位匹配</p><ul><li>性能阐述：</li></ul><p>1. 全球节点部署，平均响应时间≤20ms；</p><p>2. 支持批量查询（单次≤1000条）；</p><p>3. 数据每周更新；</p><p>4. 支持IPv4/IPv6双协议</p><table><thead><tr><th>产品名称</th><th>API 返回字段</th><th>应用场景</th><th>性能阐述</th></tr></thead><tbody><tr><td><a href="https://link.segmentfault.com/?enc=OhguvqBC%2BGLa6Rc1s6cZ2w%3D%3D.xbzicLsZEUnJUQukz%2Bj9jVltcUPURuK9SouWO4gSZaA%3D" rel="nofollow" title="https://ipdatacloud.com/" target="_blank">Ipdatacloud.com-API</a></td><td>IP地址、国家/地区、省份、城市、区县、街道、运营商、ISP、组织、ASN、经纬度（精准）、是否代理/VPN/爬虫、网络类型（移动/电信/联通）、是否国内IP</td><td>1.网站/APP风控反欺诈； 2.合规审计（GDPR/CCPA）； 3.精准广告定向投放； 4.用户画像与行为分析； 5.物流定位匹配； 6.网络安全防护； 7.离线数据库部署（企业版） 8.IP归属地私有化定制/IP风险防控策略定制/IP应用场景定制/数据格式定制/监测数据维度定制【根据需求量身定制解决方案】</td><td>1.响应时间≤10ms； 2.支持高并发； 3.数据准确率99.9%； 4.离线数据库每日更新； 5.支持批量查询（单次≤1000条）； 6.无调用次数限制（企业版）</td></tr><tr><td>IP2ReGion-API</td><td>IP地址、国家、省份、城市、区县、运营商、ISP，支持多语言返回</td><td>1.本地IP查询； 2.嵌入式设备； 3.服务器日志离线分析；</td><td>1.离线查询速度达百万级/秒； 2.数据库体积仅数MB； 3.提供开源版； 4.支持自定义数据库部署</td></tr><tr><td>Maxmind-API</td><td>IP地址、国家/地区、省份、城市、邮政编码、经纬度、ISP、组织、ASN、风险评分、是否代理/爬虫/数据中心IP</td><td>1.跨境电商风控与反欺诈； 2.全球广告定向投放； 3.合规审计（GDPR/CCPA）； 4.网络安全威胁检测； 5.国际物流定位匹配</td><td>1.全球节点部署，平均响应时间≤20ms； 2.支持批量查询（单次≤1000条）； 3.数据每周更新； 4.支持IPv4/IPv6双协议</td></tr></tbody></table><pre><code>                            表1-IP API产品对比一览表</code></pre><p>以上就是我今天分享的相关内容，希望能够给诸位带来帮助~</p><p> </p><p>注：此文章数据均来自于官网或论坛，具体轻易产品官网最新更新数据为准。</p>]]></description></item><item>    <title><![CDATA[10亿级数据跑不动？联通基于隐语SCQL在生产环境的“性能优化与避坑指南” 隐语SecretFlow]]></title>    <link>https://segmentfault.com/a/1190000047466823</link>    <guid>https://segmentfault.com/a/1190000047466823</guid>    <pubDate>2025-12-11 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>本文整理自对话生产用户联通软研院，介绍了基于隐语（SecretFlow）的引入、调研、落地过程中的关键决策与实战经验。从需求梳理到架构选型，再到对 SCQL 二次开发、系统解耦、数据源扩展与性能优化的探索，联通团队不仅探讨了关于大规模业务场景的隐私计算如何落地，也为后续在可信数据空间建设中的推广应用提供了重要参考。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466825" alt="" title=""/></p><h2>技术背景及痛点</h2><p>其实要说起我们团队构建隐私计算平台的起点，还得回到当初研究隐私计算技术的初衷。近些年，随着国家层面对数据安全和隐私保护的法律法规越来越严格，在联通内部的数据管控要求也同步趋严。</p><p>这种趋势无疑是正确且必要的，但也给我们日常的数据研发和业务支撑带来了一些非常现实的矛盾。<br/>我们经常会遇到这样的场景：业务方需要用到一些敏感数据（比如用户信息）来提升模型效果或实现某些创新应用；但与此同时，数据的所有方因为严格的安全要求，往往无法直接交付原始数据。典型如不同主体之间做联合分析——但由于涉及跨域敏感数据，这种需求往往被搁置，数据价值被白白浪费，而业务也因此无法推进。</p><p>传统方式去申请数据，为保证数据安全合规共享，不仅流程繁琐、审批周期长和要求高，而且数据交付后，事后为保证数据安全流通，在审计，合规抽检投入成本也非常高。</p><p>我们团队当时面临的核心痛点，就是如何在“数据严格管控”和“数据充分应用”之间找到一个平衡点。正是在这样的背景下，我们注意到了隐私计算技术。经过初步调研，我们发现：“数据不出域、可用不可见”的理念，正是解决上述矛盾的关键路径。它不是通过制度或流程去约束人，而是通过技术手段，在多个参与方之间建立一个安全可信的数据协同环境。</p><p>这个是真正投入隐私计算技术研究，开始搭建平台的起点。也正是从这个实践出发，我们一步步探索出了适合内部业务场景的技术架构与应用路径。</p><h2>推动历程</h2><p>下图这段经历其实是我们团队在过去一年中围绕隐私计算技术，从需求梳理、技术选型到平台建设落地的完整过程。</p><p>2024年年中，正式启动了这个项目，目标是解决跨域数据共享的合规难题，通过技术手段在“安全”与“可用”之间找到平衡。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466826" alt="" title="" loading="lazy"/></p><p>6月份时候，基本明确了采用隐私计算作为主要技术路径。根据我们对内部需求的梳理，发现“隐私保护下的联合分析”是一个非常核心的诉求。在这个过程中，在 GitHub 上发现了 隐语（SecretFlow） 项目，印象非常深刻的是，社区活跃度高、文档完善、上手门槛低，于是我们将其纳入了技术选型的候选清单。</p><p>首先使用官方的“快速开始”教程，在本地完成了 SCQL + SQLNode 的部署验证，初步跑通了一个小规模的联合分析流程，验证结果非常符合我们的期望。</p><h3>能力验证与选型确认</h3><p>基于后续规划中对隐私求交和联盟建模等能力的需求，我们又部署了 <code>SecretPad All-in-One</code> 套件，对隐语的多种能力做进一步评估。</p><p>之前也关注过 FATE ，所以我们也有意识地做了一些对比测试。在简单开箱验证后，我们发现隐语在这些能力上同样表现不俗。</p><p>随后，我们又扩展部署了中心化与 P2P 模式，并针对不同的数据量场景进行了性能验证。最终我们认为，隐语在联合分析、隐私求交与联邦建模三大关键场景中，都能较好满足我们的需求，于是正式确定其为核心技术选型。</p><h3>对比选型</h3><p>既然说到这里，也跟大家分享下我们技术选型的经验，在项目初期进行技术选型的时候，我们其实对市面上主流的隐私计算框架做了比较全面的调研和初步验证。</p><p>最后，我们筛选出了三个重点框架进行了深入学习与评估：FATE、SPDZ 和隐语（SecretFlow）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466827" alt="" title="" loading="lazy"/></p><h4>FATE</h4><p>FATE 是一个在联邦学习领域较为成熟的框架，它的生态完整、社区建设也不错。</p><p>不过，我们的业务实际情况是：纯粹的机器学习建模类任务占比并不高，反而是需要处理大量复杂的联合查询与统计分析任务。因此，FATE 在适配度方面并不理想，很早就在评估阶段被排除了。</p><h4>SPDZ</h4><p>接下来研究了 SPDZ，这是一个经典的多方安全计算框架，在学术圈有很高的影响力，算法支持也很丰富。但是在验证过程中发现，SPDZ 更像是一个偏底层的研究工具，需要用其特定语法去重新构建计算逻辑。</p><p>如果想要落地到业务，还要花费大量时间开发适配算子，从工程效率上看并不现实。</p><h4>隐语Secretflow</h4><p>相较而言，隐语(Secretflow)在我们关心的几个方面都展现出了明显优势。</p><p>首先是工程上的易用性与业务适配度，它的 SCQL 组件提供类 SQL 的接口形式，能够无缝嵌入我们原有的数据加工流程中。</p><p>对于我们这种已有大量 SQL 处理逻辑的团队来说，这种设计大大降低了改造成本，使得敏感数据的联合分析、统计任务可以快速响应和上线。</p><p>另一个让我印象非常深刻的，是隐语的技术生态活跃度。我们初次接触 SCQL 时，它的版本是 0.9.0，刚完成一次大版本升级。当时我翻了一下 GitHub 上的更新日志，发现它几乎每个月都会有新版本迭代，说明背后团队投入非常持续。对于隐私计算这样一个技术快速演进的领域，选择一个有“生命力”的开源框架对我们来说尤为重要。</p><p>此外还有一个“加分项”也不能不提：隐语的文档与社区支持非常完善。从快速上手指南，到详细的 API 说明，再到 B 站上官方出品的系列视频教程，这些内容对我们团队非常友好，也极大地降低了上手门槛和学习成本。</p><p>综合考虑工程效率、适配度、社区活跃度和学习支持等多个维度，最终我们决定将隐语作为平台构建的核心框架。</p><h3>平台搭建与场景上线</h3><p>确定技术方案后，我们围绕现有平台能力，完成了整体架构设计与集成规划。</p><p>2024年底，我们完成了第一版系统的上线，并在若干实际业务中开展了试点验证，内部也收集到了非常宝贵的反馈。</p><p>进入2025年，我们将工作重心转向平台的扩展与可信空间能力的集成，目标是将隐私计算能力深入嵌入到公司整体的数据治理体系中，打造一套具备可扩展性、合规性和高性能的隐私保护数据计算平台。</p><h2>平台架构</h2><p>在选型完成之后，我们的首要目标是优先满足内部的跨域数据分析需求，并逐步拓展到支持内外部的数据协同场景。</p><p>基于这一目标，我们依托联通现有的中台开发治理平台和数据服务平台，做了一套整体的规划，并启动隐私计算平台的研发和集成工作。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466828" alt="" title="" loading="lazy"/></p><p>从 2024 年 9 月起，我们正式投入平台的研发与搭建工作。平台初期采用的是 All-in-One + SCQL 独立部署 的方式，主要原因是当时的 All-in-One 架构还没有原生集成 SCQL 组件。</p><p>我们将 SCQL 和 SecretPad 统一封装，在中心化部署模式下，借助 SCDB，并通过逻辑库名和项目名实现一对一的映射，确保上层应用对计算框架“无感”，可以同时使用 SCQL 联合分析与 SecretPad 提供的隐私求交、联邦建模等能力。</p><p>目前，虽然新版 All-in-One 已支持集成 SCQL 与 Kuscia，但由于我们对结果数据的输出量级有明确要求，而现有版本暂未满足，因此我们仍保留 独立部署架构。</p><p>在部署模式上，平台内部采用 中心化部署，外部则基于 P2P 模式。外部架构也有进一步的演进：我们正在与可信数据空间对接，并计划通过连接器打通更多可信协同能力。</p><p>整体架构图中：</p><ul><li>向下层，已打通与元数据管理、数据资产系统的集成，实现计算节点与租户之间在权限体系、数据源管理等方面的联通。</li><li>向上层，我们将结果数据对接至数据服务平台，支持多种数据共享方式（如 API、文件等），确保从数据获取、计算执行到结果发布的流程完整闭环、无缝集成。</li></ul><p>这套架构体系实现了内部跨域计算与数据服务流程的打通，也为未来支撑更复杂的内外部协同场景打下了基础。</p><h2>实践场景</h2><p>为了更直观地展示隐私计算在实际业务中的价值，我们可以从两个典型的落地场景来跟大家介绍一下：一个是联通内部的跨域数据融合分析场景，另一个是与外部机构协同的联邦建模场景。</p><h3>场景一：内部跨主体联合分析</h3><p>在通信运营商业务中，存在大量跨域的数据融合需求。例如：针对特定客客群的画像与行为分析，需要各个主体之间开展联合分析，以制定更具针对性的融合发展策略。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466829" alt="" title="" loading="lazy"/></p><p>在该场景中，我们通过隐语的 SCQL 能力实现了跨域数据的联合查询：</p><ul><li>参与方： 主体A、主体B、主体C。</li><li><p>数据分布：</p><ul><li>主体C提供共性数据，并在项目中进行统一配置。</li><li>主体A、主体B拥有个性化数据与差异化的数加工口径，且这些数据对其他参与方不可见。</li></ul></li><li><p>权限管理：</p><ul><li>主体C通过配置 CCL（数据控制语言）权限及对敏感字段做加密脱敏处理，避免敏感明细数据暴露。</li><li>主体A和主体B在项目中仅可访问自己的个性化数据与定制口径。</li></ul></li></ul><p>在此基础上，主体A和主体B之间可以采用不同的档位（如A、B、C、D vs 一、二、三、四）进行对比分析，而无需暴露任何明细数据，实现了真正意义上的 “数据不出域，联合可计算”。</p><p>👉 此场景不仅验证了 SCQL 在多租户权限管理下的灵活性，也为后续更多跨部门、跨区域的联合分析场景奠定了实践基础。</p><h3>场景二：跨企业数据联邦建模合作</h3><p>在对外协作中，我们以与某A企业联合建模的场景为例，展示了隐语在纵向联邦学习场景下的落地能力：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466830" alt="" title="" loading="lazy"/></p><ul><li>参与方： 联通连接器 与 A企业连接器。</li><li><p>数据资产分布：</p><ul><li>联通侧拥有用户的部分特征与标签信息。</li><li>A企业拥有另一部分特征数据。</li></ul></li></ul><p>为满足业务需求，我们基于隐语的 <code>SecretPad</code> 和联邦建模能力，对连接器进行了二次封装，实现了以下流程：</p><ol><li>数据产品目录发布： 双方将各自特征数据注册为可信数据空间数据产品，并遵守国数局“三统一”要求，确保数据安全合规流通。</li><li>特征扩充与建模训练： 通过纵向联邦学习完成特征扩充与模型训练。</li><li>联通侧预测验证： 预测服务部署于联通侧，便于直接调用结果并用于业务策略制定。</li></ol><p>这一场景充分体现了隐语在 数据协作安全性 和 建模闭环效率 上的优势，成功支撑了联通与外部企业的联合智能服务能力。</p><h2>踩坑经验</h2><p>在隐语的实际落地过程中，我们遇到多个具有代表性的技术问题。跟大家介绍下我们遇到的关键问题点、原因分析及对应的解决方案，供大家在使用隐语时参考与借鉴。</p><h3>集成挑战</h3><p>最开始的时候，我们平台需要提供“任务生命周期管理”功能，但 SCQL 原生仅支持项目表与权限管理，完整任务管理功能由 SecretNote 提供，无法直接集成。</p><p>于是我们初步尝试直接修改 SCQL 源码添加接口，但因其请求参数由 Protobuf 定义、代码自动生成，改动成本极高。</p><p>当我们发现成本很大的时候，重新定位了SCQL的边界，任务管理功能由平台后端独立开发并维护；SCQL 仅作为隐私计算引擎服务调用，从架构层面实现职责清晰、解耦设计。</p><h3>安装部署</h3><p>除了系统集成之外，我们还踩过部署配置相关的坑，All-in-One 部署后容器参数（如内存）被覆盖，修改不生效。</p><ul><li>原因分析：配置文件重启后被镜像中的原始文件覆盖。</li><li><p>解决方案：</p><ul><li>查看并修改 install 脚本中复制配置文件的逻辑；</li><li>或自定义构建镜像，内嵌所需参数设置。</li></ul></li></ul><p>SCQL Agent 回调 URL 配置错误，导致异步查询无法返回。</p><ul><li>原因分析：配置项填写错误，仅同步任务可用，异步任务因回调失败导致无响应。</li><li>解决方案：修正 engine 配置项中的回调地址，并通过社区反馈确认解决路径。</li></ul><h3>使用挑战</h3><p>在任务执行层面，我们也经历了多次调优。早期在0.9.0版本中执行大规模联合分析任务时，由于任务执行后内存不释放，导致资源消耗异常。</p><ul><li>原因分析：SCQL v0.9.0 版本存在资源释放 Bug，尤其在处理亿级表时（10亿 × 3000万）内存压力极大。</li><li>解决方案：调整引擎参数配置，并升级至更稳定版本后解决。</li></ul><p>在CCL使用中，我们也曾遭遇多种语法兼容与口径迁移问题，配置难以通过校验，报错信息模糊不清。</p><ul><li>原因分析：业务方移植既有 SQL 逻辑，语句复杂度高；SCQL 报错字段提示不明确，导致调试效率低。</li><li>解决方案：深入学习社区文档、参考官方 CCL 示例，逐步积累规则配置经验。</li></ul><h3>语法问题</h3><p>某些 MySQL 合法语法（如 ON 子句中嵌套逻辑）在 SCQL 中报错。</p><ul><li>原因分析：SCQL 的 SQL 解析器与 MySQL 并非完全一致，部分语法需适配。</li><li>解决方案：经社区沟通，使用双括号包裹子表达式成功绕过限制；实际语法上需保守处理。</li></ul><h3>升级风险</h3><p>低版本场景运行正常，高版本升级后SQL执行异常（目前该问题最新版本已修复）。</p><ul><li>原因分析：新版本BUG导致</li><li>解决方案：通过 GitHub 提交 issue 并附上详细参数配置，才完成问题定位并修复。</li></ul><p>如果你也遇到过相关的问题，一定记得提issue，这样的话后面的同学也都能避免踩坑，总的来看隐语不仅为我们提供了丰富的计算能力与接口封装，但是实际使用中仍需注意工程层面的问题落地与持续的社区互动。</p><p>建议刚接触的朋友多借助官方文档与社区反馈渠道，持续迭代自己的使用模式，从而提升隐私计算场景的落地效率与系统稳定性。</p><h3>实践技巧</h3><p>在使用隐语 SCQL 的过程中，我们总结出一个关键的最佳实践经验：对于非核心功能，应通过外围实现方式来降低对原有框架的侵入。这不仅提升了系统的健壮性，也极大地提高了可维护性和扩展性。</p><p><strong>案例：SCQL 扩展 Hive 数据源的两种方式</strong></p><p>在我们为 SCQL 扩展 Hive 数据源的过程中，面临了两个技术路径选择：</p><p>直接在 Engine 中实现 Hive Connector（C++）</p><ul><li>使用 C++ 在 Engine 中构建 Hive 数据连接器。</li><li>优点是性能高，但问题是侵入性强，维护成本高。</li></ul><p>开发独立的 Arrow Flight SCQL 服务</p><ul><li>实现一个兼容 Arrow Flight SCQL 协议 的服务，支持 Hive 查询。</li><li>通过 SCQL Engine 中现有的 arrow flight scql linked 能力来连接这个服务，从而获取 Hive 中的数据。</li></ul><p>最终，我们毫不犹豫地选择了方案二，因为整体优势如下：</p><ul><li>对框架改动小：只需在 broker 和 SCDB 中添加少量代码。</li><li>数据获取逻辑独立：全部放在自研的数据服务中处理。</li><li>验证效果良好：开发过程顺利，也验证了解耦设计的可行性和高可用性。</li></ul><h4>拓展建议</h4><p>在数据源扩展方面，SCQL 实际上也在其框架中预留了接口（例如 engine 模块），支持通过模块化方式对接更多数据源。</p><p>如果在实际业务场景中识别到具有通用价值的数据源能力，欢迎向社区贡献：</p><ul><li>可将通用的拓展封装为插件或模块，提升框架能力。</li><li>但需评估是否耦合了企业内部的私有逻辑，决定是否适合向社区提交。</li></ul><h2>二次开发</h2><p>在进行 SCQL 的二次开发过程中，我们积累了一些宝贵的经验，特别是在源码学习和功能拓展方面，希望能为后续开发者提供参考。</p><p>刚开始分析 SCQL 代码时，我们采用的是“按功能模块拆解”的思路，试图通过静态地梳理每个模块来理解整个系统。</p><p>然而在实际过程中发现，这种方式对于隐私计算这样 流程复杂、模块耦合紧密的系统，并不奏效：</p><ul><li>各模块之间调用关系错综复杂，单看模块难以构建清晰的流程图；</li><li>阅读体验“割裂”，很难在脑中形成完整的数据流与执行路径；</li><li>静态分析效率低，难以快速定位实际开发中的关键点。</li></ul><h3>有效路径</h3><p>为了突破困境，我们调整了思路，选择从核心业务流程入手进行端到端追踪，以 “SCQL 查询请求的完整生命周期” 为主线进行深入分析。</p><p>具体步骤如下：</p><ol><li>从 SCQL 接收请求开始：观察查询请求进入系统后，经过了哪些模块与组件处理，如请求接收、参数验证、权限检查等。</li><li>请求调度与转发：分析请求如何从 SCQL Server 被调度到 Engine，包括涉及的通信协议、调度策略等。</li><li>Engine 处理过程：继续追踪 Engine 如何接收请求、调度算子、执行任务、获取数据并生成结果。</li><li>结果返回链路：观察从 Engine 回传结果到 SCQL，再由 SCQL 返回给上层系统的全过程。</li></ol><p>通过这条完整的执行链路，我们实现了从请求到执行再到返回的闭环追踪，对架构设计、数据流动路径、各组件职责边界有了系统性认知。<br/>效果：</p><ul><li>后续修复 bug 时，可以快速定位问题模块；</li><li>扩展功能时，能准确找到切入点，减少试错成本；</li><li>构建起稳定、清晰的 mental model，有助于整体把控系统演进。</li></ul><p>在隐私计算领域，SCQL 涉及的内容包括大量的 安全协议、分布式组件、算子优化、数据安全机制等，理论知识较为复杂，对于准备进行 SCQL 二次开发的团队和个人，我们强烈建议采用以下方法：</p><ul><li>不要陷入模块细节：避免一开始就逐个研究算子、服务、模块定义；</li><li>选择一个典型场景作为切入点：比如 SCQL 查询生命周期、隐私求交任务、联邦建模过程等；</li><li>全链路梳理主流程：先跑通流程，再逐步拆解底层模块；</li><li>记录关键路径与接口调用：便于后续追踪和团队知识传承。</li></ul><p>初期可将精力聚焦于 SCQL 的接口使用、部署流程、典型任务配置。亲手完成一次 SCQL 全流程部署，比纯看代码更能理清体系。</p><p>如运行遇到瓶颈、功能扩展受限时，再探究相关协议（如PSI、MPC）、数据加密机制或算子优化。</p><p>例如从联邦 SQL 查询扩展到联邦建模时，再关注 SecretPad、Kuscia、SPU 等模块的协同关系。</p><h3>社区未来发展方向</h3><p>目前我们正参与到可信数据空间的建设工作中，了解到国家数据局已制定了相应的技术架构规范。作为关键支撑技术之一，隐私计算将成为可信数据空间的重要组成部分。</p><p>在这个背景下，我们也看到了 隐语（SecretFlow）社区已经在该方向有所规划和布局，包括相关的标准接口、模块整合、系统架构演进等内容。这些内容非常值得深入研究和持续推进。</p><p>因此，我们期待在后续的社区技术演进中，可以进一步强化可信数据空间相关的内容规划与技术路线，非常希望能与社区共同推动相关内容的共建与深入，一起学习进步，共同参与可信数据空间的技术生态建设。</p><h2>技术延伸讨论</h2><p>1、我们在使用SCQL处理大规模数据时候（双方，1方数据规模较大约10亿），经常出现OOM或者超时的情况，对于这种场景，有什么改进的点呢？</p><p>隐私计算本身相比明文计算慢很多，因此在大规模场景下可以从以下优化：</p><ul><li>带宽、延迟、内存、CPU 等资源必须充足。</li><li>推荐配置独占资源运行 SCQL 引擎，避免并发干扰，确保最大可用资源分配。</li></ul><p>当某一方数据达到 10 亿量级时，单次任务可能需要 上百 GB 内存，若资源不足极易出现 OOM。<br/>OOM优化措施：</p><ol><li>增加内存资源：<br/>a. 从容器层面、物理机层面提升内存规格；<br/>b. 若使用 K8s，考虑适配更大规格的节点（如 256GB RAM）。</li><li>关闭并发或设置独占模式：<br/>a. 同时仅运行一个计算任务，避免多个大任务并发抢占资源。</li><li>开启 batched 模式（新版本功能）：<br/>a. batched 模式将部分中间数据转储到磁盘，显著降低内存使用；<br/>b. 内存压力降低，但 性能会有所下降，适合资源有限但对性能不极致要求的场景。<br/>超时问题优化建议</li><li>延长系统默认超时配置：<br/>a. 根据场景，适当增大 engine、broker 等组件的超时时间设置，避免因长时间计算被意外中断；</li><li>适配网络环境：<br/>a. 评估网络质量（尤其跨城/跨域部署），优化 VPN、TLS 层的传输性能。<br/>业务层优化<br/>在无法显著扩展硬件资源时，可从业务入手，降低密态处理负担。</li></ol><ul><li>前置过滤：提前在明文阶段完成筛选条件（如 WHERE, LIMIT），减少进入 SCQL 的数据规模；</li><li>数据拆分：将大表划分为多个小批次分布式处理，适合周期性任务；</li><li>逻辑拆解：将复杂 SQL 拆分为多步处理，减轻单个任务负载压力；</li><li>仅密态处理必要部分：对于非敏感字段或不涉及联邦场景的数据，优先考虑明文计算完成。</li></ul><p>SCQL 在大规模隐私计算场景中，计算成本与资源消耗远高于传统明文计算，可以从“硬件资源保障 + SCQL参数优化 + 业务逻辑调整”三个层次协同优化，提升整体任务成功率与系统稳定性。如需了解具体配置项或 batched 模式使用方式，建议参考最新 SCQL 文档或联系社区技术支持。</p><p>2、社区未来规划数据源是否支持接入实时数据源？</p><p>在现阶段，SCQL 的数据源接入机制虽然支持一定程度的灵活配置，但仍存在一些限制：</p><ul><li>通过配置文件方式接入数据源，但修改配置后仍需重启服务才能生效；</li><li>已支持对接 Kuscia、Arrow SQL Server 及用户自建 Server 等实时服务，以实现数据的动态接入，但增加了外部系统的依赖。<br/>目前，社区暂无正式的实时数据源支持路线图，社区非常重视用户的实践反馈，如果能通过用户案例进一步验证实时数据接入的需求与价值，将会优先考虑在后续版本中支持此能力。短期建议：针对对接 Flink、Spark等实时系统的场景，则建议先落盘，再接入 SCQL。</li></ul><p>3、我们这边有一些数据量特别大的表（几十上百亿），执行隐私计算任务会超时报错，SCQL是否有计划支持多节点分布式计算？</p><p>当前支持情况</p><ul><li>SCQL Engine 已支持多节点部署，即一个参与方可以在多台机器上部署多个 engine 节点；</li><li>但目前尚不支持将单个计算任务自动拆分至多个节点并行执行。</li></ul><p>换句话说，当前版本的 SCQL 仍是基于“一个任务一个节点”的模式执行，尚未支持自动的分布式并行调度。</p><p>对于有明确分布式计算需求的场景，建议将使用反馈提交至社区，以支持后续技术规划。</p>]]></description></item><item>    <title><![CDATA[一键部署！OpenCloudOS 多项开源技术打造 “开箱即用” 的 AI 支撑底座 OpenClo]]></title>    <link>https://segmentfault.com/a/1190000047466320</link>    <guid>https://segmentfault.com/a/1190000047466320</guid>    <pubDate>2025-12-11 15:06:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>12月6日，在以“以生态之力·让OS更懂未来”为主题的 2025 OpenCloudOS 操作系统生态大会上，OpenCloudOS 社区联动昇腾、海光、AMD、沐曦、昆仑芯、vLLM、SGLang、作业帮以及腾讯云，共同发布了 OpenCloudOS Infra 智能基座。这一重磅发布旨在系统性解决AI应用在异构算力环境下部署复杂、适配成本高等核心痛点，为开发者构建一个一体化、高性能、易部署的AI应用运行底座。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466322" alt="图片" title="图片"/><br/>当前，大模型与AI应用已从探索阶段迈入高速规模化部署新时期，爆发式增长的AI算力需求与极端碎片化、异构化的底层硬件环境形成尖锐矛盾。开发者或企业往往需要耗费大量精力在繁琐复杂的底层环境适配与部署上，这已成为制约企业在AI时代挖掘核心价值、创新产业发展的关键因素。</p><p>面对这一挑战，OpenCloudOS 社区确立了“重心在OS，延展至AI”的技术原则，以及“成为' AI时代最好用的OS'” 的目标，聚焦打磨OS内核、调度器、驱动兼容性、安全模块等传统优势领域，为AI工作负载提供独特的OS层价值，通过三大关键举措和多项技术创新，显著降低触及和利用异构算力的门槛，让开发者或企业能更专注于算法与模型的创新。</p><h4>三大核心举措，破解AI基础设施碎片化难题</h4><p><strong>深度集成多样性算力支持</strong>：构建统一的多样性算力生态，是 OpenCloudOS 的核心优势。当前，OpenCloudOS 9及后续版本，已完成对 NVIDIA、AMD、昇腾、海光、沐曦、昆仑芯等国内外主流AI加速芯片厂商官方驱动及计算栈（SDK）的全面兼容和双向认证。用户无需再分别前往各芯片厂商官网搜寻、下载、编译和调试驱动程序，仅需在 OpenCloudOS 上通过标准 yum install 或 dnf install 命令，即可如同安装普通软件包一样，一键完成所有底层依赖的部署，极大简化了混合算力中心的运维管理。<br/><strong>开箱即用的主流AI框架容器镜像</strong>：OpenCloudOS 已通过容器化技术，完成了近20款主流AI框架及智能体（Agent）应用的深度适配、依赖解决和性能优化，并封装成可直接拉取使用的容器镜像。用户基于 OpenCloudOS 均可实现“一键部署，性能最优”。容器化不仅确保了AI应用运行环境的高度一致性和可移植性，保障了从开发到生产的全链路顺畅，还大幅提升了效率，将部署时间缩短至“分钟”级。<br/><strong>云上无缝集成</strong>：智能基座与腾讯云高性能应用服务（HAI）平台深度融合，并在HAI平台发布了预集成驱动的 OpenCloudOS 系统镜像。用户在选择 HAI 服务时，可直接选用该镜像，瞬间获得一个稳定、高性能、无需手动配置的 AI-ready 云服务器，极大简化了云上AI应用流程。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466323" alt="图片" title="图片" loading="lazy"/></p><h4>多项自研技术创新，打造高效、稳定、广兼容的AI应用运行底座</h4><p><strong>FlexKV——大幅降低推理与延迟</strong>：FlexKV是面向超大规模 LLM 推理场景的分布式 KV Store 与多级缓存管理系统，这项技术通过将大模型推理过程中的 KVCache 逐层缓存至内存、SSD 及云端扩展存储（例如 GooseFS），有效解决了规模化推理的显存瓶颈。在实际应用中，FlexKV 展现出显著性能优势。在增强搜索场景下，TTFT（首Token延迟）在高并发下降低了70%；在智能问答助手场景中，对话时延降低了57%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466324" alt="图片" title="图片" loading="lazy"/><br/><strong>OC Slimtrace——AI容器小型化镜像，降低镜像存储与传输开销</strong>：在AI开发中，容器镜像因需集成AI框架、依赖库与完整工具链，其体积常高达数十GB，带来巨大的存储、分发和启动开销。针对这一痛点，OC Slimtrace 通过软件包切片与动静态混合依赖分析两项关键技术，显著优化容器镜像体积，最大可缩减94%。助力用户实现镜像拉取速度加快、显著降低存储成本，并享受到更敏捷的容器启动体验，从而提升AI开发与部署的整体效率。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466325" alt="图片" title="图片" loading="lazy"/><br/><strong>OC Flip——容器镜像加速，加快集群启动与模型分发</strong>：在AI场景下，大规模集群冷启动时，常面临数十GB的镜像与模型文件需同时拉取的困境。受限于中心仓库带宽，往往导致集群冷启动时下载缓慢、耗时长，且镜像拉取时间占启动流程比重过高、本地读取效率不佳。为此，OC Flip（fast lazy image pull）基于优化增强的镜像懒加载技术，极大提升镜像分发加载效率，同时保持 OCIv1 镜像格式、兼容现网镜像存储驱动，实现了从“全部下载”到“即用即取”的转变。50G AI 镜像 sglang 场景冷启动（下载+服务运行）时间缩短60%。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466326" alt="图片" title="图片" loading="lazy"/><br/><strong>OC PkgAgent——智能软件包自动管理维护</strong>：为保障发行版的持续安全与稳定，OpenCloudOS 需要实时跟进上游社区海量的安全补丁与功能更新，为此推出的 PkgAgent 智能体系统，通过AI多智能体协同技术实现软件包管理的自动化革新，可将单个软件包处理时间从平均2.5小时缩短至分钟级，预计每年可节省超过 6000 小时的人力投入，并将漏洞修复的闭环效率提升了91.3%，显著增强了系统安全性与迭代敏捷性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466327" alt="图片" title="图片" loading="lazy"/><br/><strong>OCAI——构建智能运维新范式</strong>：针对系统维护技术门槛高、场景复杂、问题诊断碎片化等问题，OpenCloudOS 打造了 OCAI 开放智能体驱动的智能运维新范式。通过 AI Agent 自动化完成系统维护工作流，打通了智能问答、智能诊断和智能调优的全链路，大幅提升系统运维效率。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466328" alt="图片" title="图片" loading="lazy"/><br/>目前，OpenCloudOS 已完成与海光、龙芯、鲲鹏等主流CPU，以及沐曦、寒武纪、燧原等AI加速卡的全面适配，形成了完整的软硬件生态体系。社区采用 OC8.x 稳定版和 OC9.x 创新版双版并行发展策略，既保障企业级稳定性需求，又持续推动技术创新。同时，OpenCloudOS 已携手腾讯云、安谋科技、沐曦等生态合作伙伴，在AI算力底座、安全运维等领域打造了一系列经过大规模实践检验的解决方案。<br/>腾讯云副总裁、腾讯蓬莱实验室负责人、OpenCloudOS 社区荣誉理事郭振宇在大会上表示，腾讯云将持续投入社区建设，构建安全的软件供应链体系，并开放更多场景资源深化AI生态，携手生态伙伴将 OpenCloudOS 打造为AI时代下安全、绿色、高性能、高可用的最佳基座。<br/>OpenCloudOS 社区技术监督委员会(TOC)主席王佳强调，OpenCloudOS 的价值在于成为AI基础设施生态中的“最大公约数”，通过夯实 OS Infra 这一环，降低开发者触及和利用异构算力的门槛，让他们能更专注于算法与模型本身的创新。<br/>AI技术从“工具”向“智能体”的演进，正在推动操作系统底层技术重构。OpenCloudOS Infra 智能基座的发布，标志着社区在拥抱AI趋势、以生态之力夯实基础软件底座方面迈出了关键一步，将为各行各业的数字化智能化转型提供更坚实支撑。</p>]]></description></item><item>    <title><![CDATA[服务器数据恢复—RAIDZ多硬盘离线如何恢复数据？ 北亚数据恢复 ]]></title>    <link>https://segmentfault.com/a/1190000047466329</link>    <guid>https://segmentfault.com/a/1190000047466329</guid>    <pubDate>2025-12-11 15:06:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>服务器存储数据恢复环境&amp;故障：</strong><br/>某存储设备中一共有40块磁盘组建存储池，其中4块磁盘作为全局热备盘使用。存储池内划分出若干空间映射到服务器使用。<br/>服务器存储设备在没有断电、进水、异常操作、供电不稳定等外部因素的情况下突然崩溃。管理员重启服务器后无法进入操作系统，数据丢失。</p><p><strong>服务器存储数据恢复过程：</strong><br/>1、将故障存储中所有硬盘做好标记后取出，以只读方式进行完整硬盘镜像。镜像完后把所有磁盘按照编号还原到原存储设备中，后续的数据分析和数据恢复操作都基于镜像文件进行，避免对原始磁盘数据造成二次破坏。<br/>2、基于镜像文件分析所有磁盘的底层数据，北亚企安数据恢复工程师发现所有磁盘是通过ZFS进行管理，磁盘内记录系统元信息的NVLIST较为混乱。需要恢复数据的磁盘分为三组，每组12块；单个组使用ZFS特有的RAIDZ管理所有磁盘；RAIDZ级别为2，即每个组内可缺失磁盘个数最大为2；全局热备盘全部启用。<br/>Tips：在ZFS文件系统中，池被称为ZPOOL。ZPOOL的子设备可以有很多种类：块设备、文件、磁盘等。本案例中的子设备为三组RAIDZ。<br/>经过分析发现，三组RAIDZ中的两组RAIDZ启用热备盘个数分别为1和3。启用热备盘后，第一组RAIDZ又有一块离线盘，第二组RAIDZ内则又有两块盘离线。<br/>故障模拟：三组RAIDZ内第一和二组RAIDZ中有磁盘离线，热备盘自动上线进行替换；热备盘无冗余情况下第一组RAIDZ中有一块盘离线，第二组RAIDZ中有两块盘离线，ZPOOL进入高负荷状态（每次读取数据都需要进行校验得到正确数据）；由于第二组RAIDZ内有三块盘离线，该组RAIDZ崩溃、ZPOOL下线、服务崩溃。<br/>3、ZFS管理的存储池与常规存储不同。ZFS管理的存储池中所有磁盘都由ZFS进行管理。常规RAID在存储数据时，只按照特定的规则组建池，不关心文件在子设备上的位置；而ZFS在存储数据时会为每次写入的数据分配适当大小的空间，并通过计算得到指向子设备的数据指针。这种特性决定了RAIDZ缺盘时无法直接通过校验得到数据，必须将整个ZPOOL作为一个整体进行解析。<br/>北亚企安数据恢复工程师手工截取事务块数据，并编写程序获取最大事务号入口。<br/><img width="674" height="249" referrerpolicy="no-referrer" src="/img/bVc6seh" alt="北亚企安数据恢复—RAIDZ数据恢复" title="北亚企安数据恢复—RAIDZ数据恢复"/><br/>4、获取到文件系统入口后，北亚企安数据恢复工程师编写数据指针解析程序进行地址解析。<br/><img width="677" height="341" referrerpolicy="no-referrer" src="/img/bVc6sek" alt="北亚企安数据恢复—RAIDZ数据恢复" title="北亚企安数据恢复—RAIDZ数据恢复" loading="lazy"/><br/>5、获取到文件系统入口点在各磁盘分布情况后，北亚企安数据恢复工程师手工截取并分析文件系统内部结构。入口分布所在的磁盘组无缺失盘，可直接提取信息。数据恢复工程师根据ZFS文件系统的数据存储结构找到映射的LUN名称，从而找到其节点。<br/>6、经过分析，数据恢复工程师发现在此存储中的ZFS版本与开源版本有较大差别，无法使用以前开发的解析程序解析，所以北亚企安数据恢复工程师重新编写了数据提取程序提取数据。<br/><img width="723" height="447" referrerpolicy="no-referrer" src="/img/bVc6sel" alt="北亚企安数据恢复—RAIDZ数据恢复" title="北亚企安数据恢复—RAIDZ数据恢复" loading="lazy"/><br/>由于磁盘组内缺盘个数较多，每个IO流都需要通过校验得到，提取进度极为缓慢。与用户方沟通后得知，此ZVOL卷映射到XenServer作为存储设备，需要恢复的文件在其中一个vhd内。提取ZVOL卷头部信息，按照XenStore卷存储结构进行分析，发现该vhd在整个卷的尾部，计算得到其起始位置后从此位置开始提取数据。<br/>7、Vhd提取完毕后，验证其内部的压缩包及图片、视频等文件，均可正常打开。<br/>8、用户方验证数据后，确定恢复出来的文件数量与系统自动记录的文件个数基本一致，文件全部可正常打开。本次数据恢复工作完成。</p>]]></description></item><item>    <title><![CDATA[鸿蒙应用全流程测试指南上线，为开发者提供测试必备导航 鸿蒙百晓生 ]]></title>    <link>https://segmentfault.com/a/1190000047466355</link>    <guid>https://segmentfault.com/a/1190000047466355</guid>    <pubDate>2025-12-11 15:05:35</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>应用质量、用户体验是应用开发中重点关注的问题，每一位开发者都希望自己的应用能够以最佳状态抵达用户手中。因此测试环节常常成为应用开发及上架发布阶段的“拦路虎”——资料分散、流程复杂、工具难寻等，这些痛点不仅需要增加相应的人力成本，还延长了开发周期。</p><p>针对这些问题，华为近日在开发者联盟官网的“HarmonyOS开发入门”上线了测试服务快速入门专题页，涵盖单元测试、UI 测试、专项测试、用户测试和应用性能监测等能力，全面覆盖开发、测试、发布及运营各流程，打造全面整合的测试资源平台，犹如提供专业的测试导航，助力开发者保障鸿蒙应用质量。</p><p><img width="723" height="614" referrerpolicy="no-referrer" src="/img/bVdm7RX" alt="image.png" title="image.png"/></p><h3>全流程覆盖，归拢“碎片化”测试资料</h3><p>在不同开发阶段，开发者往往需要进行不同的测试以确保应用质量达标。在紧锣密鼓推进的项目中，了解测试资料、上手测试工具等都可能需要花费大量时间，而全新上线的测试服务入门将鸿蒙应用测试所需的各类资源进行了系统性整合，不仅囊括了多类型基础测试模块，还覆盖了用户测试、应用性能监测等关乎到用户体验的核心环节。让无论是初学开发者还是资深工程师，都能在这套体系中找到对应阶段的测试支持，彻底告别了过去资料分散、难以查找的困境。</p><p>另外，为了帮助开发者在持续集成流水线中轻松且快速地集成华为官方测试工具，页面中提供了清晰的搭建、测试工具命令行使用的指引。开发者可通过统一的入口获取相关操作说明，无需在不同网页、文档间来回切换，有效解决了资料分散带来的效率瓶颈。这种一体化的设计思路，无疑为鸿蒙应用的持续构建工程能力提供了坚实基础。</p><p><img width="723" height="242" referrerpolicy="no-referrer" src="/img/bVdm7RY" alt="image.png" title="image.png" loading="lazy"/></p><h3>快速定位，打造质量交付“快车道”</h3><p>在鸿蒙应用开发阶段，如何快速发现潜在问题？测试服务入门中“应用与元服务体检”工具提供了解决方案。这款工具如同精密的“体检中心”，能够快速检测应用在性能、功耗、安全、稳定性和功能等方面的兼容性问题。更值得关注的是，它不仅能发现问题，还能通过提供的诊断建议帮助开发者快速定位故障代码，大大缩短调试时间。这种“即查即改”的高效测试模式，为开发者优化应用体验提供了强有力的技术支持。</p><p>从行业视角看，这套测试服务入门的推出不仅做到了测试工具的整合，还构建了一套标准化的质量保障体系。不仅大大降低了鸿蒙应用的开发测试门槛，也提供了系统化的工程实践指南。</p><p>如果你对HarmonyOS充满兴趣，欢迎访问官网“HarmonyOS开发入门”从设计、开发到测试、发布，将想法变为现实。期待全流程覆盖的测试专题页能成为开发者提升应用质量的最佳阶梯，在体验为王的时代，相信这套测试服务入门专题的及时推出，能成为开发者不可或缺的竞争力。</p>]]></description></item><item>    <title><![CDATA[冲压设备预测性维护怎么实施？2025年技术应用全解析 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047466522</link>    <guid>https://segmentfault.com/a/1190000047466522</guid>    <pubDate>2025-12-11 15:04:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今制造业转型升级的关键时期，冲压设备作为生产制造的核心装备，其稳定运行直接关系到企业的生产效率、产品质量和运营成本。传统的设备维护方式，如定期维护和事后维修，往往无法应对冲压设备在高强度、高速运转下的复杂故障形式。以某大型汽车零部件厂为例，该厂原有的设备维护模式导致每月平均停机6小时，直接经济损失超过50万元。2025年，随着工业4.0技术的快速迭代，预测性维护逐渐成为解决这一问题的理想方案。<br/>预测性维护的核心在于通过实时监测设备运行状态，提前发现潜在故障并制定维护策略。这一技术体系通常包括数据采集层、传输层、分析层和应用层四个部分。在数据采集层面，工业传感器扮演着至关重要的角色。冲压设备的关键部件，如滑块、导轨、离合器等，都需要配备高精度传感器。振动传感器可以监测设备的异常振动信号，温度传感器能够实时记录电机和液压系统的温度变化，压力传感器则用于检测油缸的泄漏情况。例如，2025年某知名冲压设备制造商在其新一代冲压机中集成了新一代MEMS传感器，采样精度提升至微秒级，为故障预警提供了可靠的数据支持。<br/>在数据传输层面，工业以太网和边缘计算节点是保障数据实时性和准确性的关键基础设施。通过高速网络传输，原始数据能够在毫秒级内被上传至云端或本地服务器进行分析。值得一提的是，2025年许多冲压设备制造商开始支持5G专网接入，这使得远程高清视频监控和设备诊断成为可能。例如，某汽车厂采用华为工业无线解决方案后，其冲压设备的远程诊断能力得到显著提升，技术人员可以通过手机实时查看设备运行画面，快速定位问题。<br/>分析层是预测性维护的“大脑”，主要依赖人工智能算法对采集到的数据进行深度学习和挖掘。通过LSTM（长短期记忆网络）算法，可以对冲压设备的振动数据进行时间序列分析，预测滑块导轨的磨损情况；通过异常检测算法，可以识别出离合器压力波动的异常模式。2025年，许多企业在实际应用中发现，结合多种算法可以显著提升预测准确性。比如，海尔智能工厂在预测模具寿命时，采用了机器学习与深度学习相结合的模式，成功将模具更换周期延长了30%，大幅降低了设备维护成本。<br/>在应用层，预测性维护系统需要与企业的生产管理系统深度集成。例如，某系统可以自动接收预测性维护的预警信息，并据此调整生产计划。一旦系统预测到某台冲压设备可能在一周内出现滑块卡死故障，生产调度模块会自动将该设备的维护安排在非生产时段，避免生产线中断。这种智能化的维护调度不仅提升了设备利用率，还优化了人力资源配置。<br/>经济效益方面，预测性维护为冲压设备管理带来了显著的改善。根据广域铭岛发布的案例白皮书，采用其预测性维护解决方案的企业平均实现了：设备综合效率（OEE）提升8-12%，维护成本降低30-40%，意外停机时间减少70%以上。以领克汽车成都工厂为例，在实施该方案后的第一年就节省维护费用460万元，同时将冲压件次品率从0.5%降至0.2%。<br/>然而，预测性维护的实施也面临诸多挑战。首先是数据孤岛问题，许多传统冲压设备缺乏统一的数据接口，导致难以将实时监测数据接入中央系统。其次是技术融合难度，冲压设备往往涉及机械、电气、液压等多个子系统，如何通过预测性维护技术实现跨系统协同仍需进一步研究。此外，维护团队需要具备数据分析和算法优化的能力，这对企业的人才培养提出了更高要求。<br/>为应对这些挑战，企业可以采取多种策略。一方面，通过API网关和工业通信协议（如OPC UA）实现设备数据的集成，避免信息孤岛。另一方面，与专业服务商合作可以借助其成熟的算法模型和行业经验，快速搭建预测性维护系统。在具体实施过程中，建议企业优先选择模块化设计的冲压设备，以便于传感器部署和系统集成。</p>]]></description></item><item>    <title><![CDATA[大模型新战场：智谱开源“会点手机”的AutoGLM 多情的青蛙 ]]></title>    <link>https://segmentfault.com/a/1190000047466524</link>    <guid>https://segmentfault.com/a/1190000047466524</guid>    <pubDate>2025-12-11 15:04:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>继文本、图像、代码和语音之后，大模型竞争的下一个焦点正转向“与物理世界交互”的能力。日前，国内AI公司智谱AI正式宣布，开源其“具身智能”大模型AutoGLM系列。该模型能理解图形化界面，并模拟人类操作手机，自动执行如点外卖、订机票、发微博等复杂任务，引发业界广泛关注。</p><p>根据智谱AI在开源平台GitHub及官方渠道发布的技术报告，AutoGLM-3B版本模型具备“视觉-语言-动作”的协同能力。它不仅能“看懂”手机屏幕截图，理解图标、按钮和文字，还能规划操作步骤，并输出精确的点击、滑动等模拟指令。为训练此能力，团队构建了包含大量手机界面像素与操作序列的数据集。此次开源遵循Apache 2.0协议，意味着开发者可免费商用（来源：智谱AI官方开源文档及技术报告）。当前，各大模型厂商在纯对话能力上的差距逐渐缩小，竞争延伸至“AI智能体”这一前沿领域——即能自主理解目标、使用工具、完成任务的AI。谷歌、微软等巨头早已布局。智谱通过开源AutoGLM，一方面旨在吸引全球开发者共建生态，快速积累真实场景数据，反哺模型迭代；另一方面，也是将其在通用大模型（GLM）上的技术优势，向更具实用价值的应用层拓展，试图定义下一代AI交互的入口。</p><p>然而，让AI可靠地操作错综复杂的真实应用，仍面临安全性、可靠性和泛化能力的巨大挑战。</p>]]></description></item><item>    <title><![CDATA[数据报表案例详解|基于smardaten实现预算管理系统的报表分析 数睿数据 ]]></title>    <link>https://segmentfault.com/a/1190000047466531</link>    <guid>https://segmentfault.com/a/1190000047466531</guid>    <pubDate>2025-12-11 15:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>需求背景</strong><br/>在数字化转型浪潮中，企业面临着海量数据处理和高效决策支持的双重挑战。传统报表制作方式周期长、响应慢，业务人员过度依赖技术团队，导致数据价值难以充分发挥。特别是面对中国式复杂报表需求时，传统的BI工具往往力不从心，业务人员需要一种能够快速响应变化、直观易用且支持深度分析的数据报表解决方案。预算管理作为企业核心经营活动之一，预算数据需人工汇总，耗时费力且易出错，同时会涉及跨报表多维度数据分析需求。如何让报表数据清晰呈现，实现数据快速整合，缩短预算编制周期，实现更灵活的数据管理能力，成为企业数字化运营的关键需求。<br/><strong>解决方案：赋能业务自主高效开发</strong><br/>smardaten数据报表平台应运而生，它是一款面向企业级的无代码报表开发工具，该平台针对预算管理等典型业务场景，提供了从数据接入、报表设计到交互分析的全链路解决方案。<br/>平台核心优势在于：支持30+计算函数、多源数据无缝接入、类Excel的简洁操作界面，以及独特的报表业务联动能力。下面以预算管理系统中的项目成本付款计划与项目成本分析为例，详细介绍如何利用smardaten快速构建专业级数据报表。<br/><strong>处理场景：两张报表构建 “概览 - 明细” 分析闭环</strong><br/>我们将构建项目成本付款计划和项目成本两张报表：<br/>• <strong>项目成本付款计划表</strong>：聚焦核心预算数据，清晰呈现项目名称、对应地块、费用科目、总成本，以及去年与今年的预算分配情况，帮助管理者快速掌握全量项目预算全貌。支持模糊查询项目名称，打印和导出报表数据；<br/>• <strong>项目成本明细报表</strong>：点击项目成本付款计划表中的项目名称，即可跳转到该表中，它作为钻取落地的核心报表，精准拆解了项目成本构成，涵盖熟化成本、前期费用两大核心模块，助力业务人员深挖成本细节、定位费用重点。<br/><strong>配置过程：四步搞定专业报表</strong><br/>接下来，让我们从零起步，全程贯穿框架搭建、数据处理、样式美化至交互完善的完整流程，共同打造一份结构清晰、数据精准、信息完整的专业预算报表，为预算管理场景下的高效决策提供直观且可靠的数据支撑！<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnkl3" alt="" title=""/><br/><strong>1. 框架搭建</strong><br/>报表的构建都始于整体框架设计。在smardaten中，我们首先规划报表的整体框架：顶部为标题区域，中部为表头行，底部为数据展示区。<br/><strong>标题区域设置</strong>：双击顶部单元格输入报表标题（如“项目成本付款计划表”）。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl4" alt="" title="" loading="lazy"/><br/><strong> 表头规划</strong>：依据业务需求依次配置列名称，包括项目名称、对应地块、费用科目、总成本等关键字段。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl5" alt="" title="" loading="lazy"/><br/><strong>报表变量</strong>：为提升报表的灵活性与可复用性，针对“去年预算”“今年预算”等时间相关字段，可通过变量配置实现动态标题。新增年度变量并设定默认值，再通过函数将变量与字段拼接生成表头，确保年度变更时标题自动同步更新。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl6" alt="" title="" loading="lazy"/><br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl7" alt="" title="" loading="lazy"/><br/>框架搭建完成后，即可通过简单的单元格合并与列宽调整，打造整齐划一、易读性强的报表骨架，为后续数据绑定奠定基础。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl8" alt="" title="" loading="lazy"/><br/><strong>2. 数据处理</strong><br/>2.1 单元格数据绑定<br/>smardaten平台提供了直观的拖拽式数据绑定机制。左侧数据资产区集中展示了所有已接入的数据资产，只需简单拖拽即可将字段绑定至对应单元格。例如，从“项目成本付款计划表”中拖入“项目名称”“地块”“科目ID”等字段，实现数据的快速对接。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkl9" alt="" title="" loading="lazy"/><br/>接着，从另一张“科目字典表”中拖入“科目名称”等辅助信息字段。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkma" alt="" title="" loading="lazy"/><br/>接着我们按照相同的步骤快速完成后面的单元格。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmb" alt="" title="" loading="lazy"/><br/><strong>2.2 聚合/扩展/父格</strong><br/>数据层级关系配置是实现报表的关键。通过聚合方式、扩展方向与父格关系设定，实现数据的分层、分组展示。<br/>地块字段：每个项目包含多个地块，设置其分组、纵向扩展，左父格为“项目名称”所在单元格，实现按项目分组。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmc" alt="" title="" loading="lazy"/><br/>科目ID字段：同样设置为纵向扩展，左父格为“地块”所在单元格，形成“项目-地块-科目”的层级结构。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmd" alt="" title="" loading="lazy"/><br/>科目名称字段：由于来自另一张“科目字典表”，需指定左父格为“科目ID”，并设置两表间的关联字段，即科目字典表中的科目id。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkme" alt="" title="" loading="lazy"/><br/>通过聚合/扩展/父格配置，报表数据自动按“项目→地块→科目”的逻辑清晰展开，数据关系一目了然。对于总成本等数值字段，设置为直接纵向扩展，左父格选择科目id，确保数据展示的准确性。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmg" alt="" title="" loading="lazy"/><br/><strong>2.3 函数计算</strong><br/>在表格底部添加“合计”行是报表的常见需求。smardaten支持多种的函数计算能力，可快速实现数据汇总。针对“总成本”“年度支付比例”等数值列，直接插入求和函数(如SUM函数)，系统自动计算合计值，非常简单！<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmh" alt="" title="" loading="lazy"/><br/>预览报表可看到，数据已按预设层级实现精准聚合，底部合计行自动展示各数值列的汇总结果。<br/><img width="723" height="223" referrerpolicy="no-referrer" src="/img/bVdnkmB" alt="" title="" loading="lazy"/><br/><strong>3. 样式美化</strong><br/>专业美观的报表不仅能提升数据可读性，更能彰显业务专业性。smardaten提供了丰富的样式优化功能。<strong>标题区域</strong>：调整字体、大小与颜色，合并单元格居中显示，并填充蓝色背景，突出报表主题。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmy" alt="" title="" loading="lazy"/><br/><strong>表头行</strong>：统一字体、居中对齐，搭配浅蓝色填充与边框，增强视觉层次感。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmE" alt="" title="" loading="lazy"/><br/><strong>数据区域</strong>：前四列字体加粗以强调结构，设置统一的填充颜色与边框；“年度支付比例”列设置为百分比格式，并以特定颜色突出显示。“年度合计列”修改背景颜色，隐藏辅助性的“科目ID”列，保持界面简洁；调整合计行的样式，填充黄色背景，使汇总结果一目了然。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmG" alt="" title="" loading="lazy"/><br/><strong>专业元素</strong>：在表格底部插入公司Logo图片，提升报表的品牌感与专业度。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmH" alt="" title="" loading="lazy"/><br/><strong>查看优化</strong>：设置冻结前两列，确保水平滚动时关键信息始终可见。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmI" alt="" title="" loading="lazy"/><br/><strong>打印优化</strong>：支持灵活调整页边距，设置报表整体水平居中；可配置页眉页脚（如添加页码），调整打印缩放比例与顺序，确保纸质报表的呈现效果。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmJ" alt="" title="" loading="lazy"/><br/>经过一系列精细化样式调整，一张清晰、美观、结构分明的预算报表已跃然眼前！<br/><img width="723" height="245" referrerpolicy="no-referrer" src="/img/bVdnkmK" alt="" title="" loading="lazy"/><br/><strong>4. 交互完善</strong><br/>现在，我们为其赋予动态交互能力：希望项目成本付款计划表支持数据查询，同时点击“项目名称”时，可跳转至该项目的“项目成本明细报表”，实现数据的穿透查询。<br/><strong>4.1 多报表钻取分析</strong><br/>smardaten支持跨报表的联动分析，实现 “宏观概览→微观明细” 穿透。创建“项目成本明细报表”作为钻取目标，精准展示项目成本构成（如熟化成本、前期费用等核心模块）。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmL" alt="" title="" loading="lazy"/><br/><img width="723" height="229" referrerpolicy="no-referrer" src="/img/bVdnkmM" alt="" title="" loading="lazy"/><br/>在首张报表中，为“项目名称”单元格设置跳转事件，指定跳转至第二张报表，并传递对应项目参数。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmN" alt="" title="" loading="lazy"/><br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmO" alt="" title="" loading="lazy"/><br/>在目标报表中，新增项目名称变量，并设置相应的数据筛选条件，确保只展示当前选定项目的明细数据。这样，在查看预算汇总时，随时可下钻查看明细，实现联动分析。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmP" alt="" title="" loading="lazy"/><br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmR" alt="" title="" loading="lazy"/><br/><strong>4.2 数据查询</strong><br/>为提升报表的交互性，可以为报表配置的数据查询功能。在报表顶部插入输入框，通过变量绑定，实现项目名称的模糊查询。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkmU" alt="" title="" loading="lazy"/><br/>在项目成本付款计划表中项目名称同样需要根据变量设定过滤条件。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkm1" alt="" title="" loading="lazy"/><br/>完成配置后，输入项目名称关键词，系统即可实时筛选并展示匹配数据，提升查阅效率。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkm3" alt="" title="" loading="lazy"/><br/>最后，为实现报表的集中管理与便捷访问，我们可将该报表绑定至预算编制汇总模块。例如，在预算编制汇总模块中选择 2025 年度、版本号为 V1.0 的业务数据，点击 “详情” 按钮。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkm7" alt="" title="" loading="lazy"/><br/>选择 “项目成本付款计划年度表”，即可一键跳转到我们已制作完成的报表页面。<br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkm8" alt="" title="" loading="lazy"/><br/><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdnkni" alt="" title="" loading="lazy"/><br/> 至此，我们不仅快速完成了数据报表从框架搭建、数据处理、样式美化到交互配置的全流程，更赋予了报表清晰的业务逻辑、直观的可视化呈现与灵活的钻取能力。<br/><strong>体验总结</strong><br/>通过以上全流程实践，smardaten数据报表平台展现出以下突出优势：<br/>• <strong>操作效率提升</strong>：拖拽式配置降低学习成本，使报表开发周期从传统编码的数周缩短至数小时，业务人员无需深厚技术背景，即可独立完成专业级报表开发；<br/>• <strong>业务适配性强</strong>：支持30+计算函数、多源数据接入、层级配置、跨报表钻取等功能，满足预算管理等场景的多维度分析需求。<br/>• <strong>业务联动能力强大</strong>：支持报表与应用、大屏、文档等模块的无缝集成，这种联动能力使数据真正融入业务流程，支撑高效决策。</p>]]></description></item><item>    <title><![CDATA[国产vs海外：需求管理工具实测对比（流程、协作、落地效果全拆解） 王思睿 ]]></title>    <link>https://segmentfault.com/a/1190000047466534</link>    <guid>https://segmentfault.com/a/1190000047466534</guid>    <pubDate>2025-12-11 15:02:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>需求是团队之间的“共同认知”，而需求管理工具就是认知的载体，但载体选错了，再真诚的沟通也容易淹没在群消息和表格版本里。本文将把同一类项目放进国产与海外不同类型的需求管理系统与平台里，结合真实项目现场，聊聊各自的使用体验和背后的管理思路，希望帮你在下次搜索和选型需求管理工具时，心里更有数，也更不焦虑。</blockquote><p>很多行业调研都指出，大量软件研发项目、数字化转型项目的失败，都和需求收集、需求澄清、需求变更管理不当密切相关。需求管理是需求生命周期（收集–澄清–拆解–实现–验证）中的主线，一旦起点模糊，后面的设计、开发、测试都是在带着偏差前进。</p><p>与此同时，需求管理工具本身正成为一个独立市场：围绕“需求管理工具 / 需求管理平台 / 研发一体化平台”的产品和搜索越来越多。这一方面说明大家确实看到了需求管理的重要性，而另一方面，市面上可选的需求管理软件越来越多，反而让人更难判断——什么才是适合自己团队的需求管理工具？</p><p>下面我们就来看看国产 vs 海外，在需求管理这件事上究竟有哪些路径和体验差异？不同成熟度的团队该如何选择合适的需求管理系统？</p><h2>海外通用敏捷工具：以 Jira 体系为例的“研发中心型需求管理”</h2><p>如果你搜索“需求管理工具”“敏捷项目管理工具”，Jira 这类海外工具几乎一定会出现在结果里。很多团队最开始把它当成研发任务管理系统，用来记录任务和 Bug；用得深入一点，会发现它其实也是一种需求管理平台。</p><p>从项目经理的视角看，这类工具有几个特点：</p><p><strong>结构化的需求分解能力</strong></p><ul><li>通过 Epic / Story / Task 等层级，把“一个大需求”拆成多个可实施的工作项；</li><li>较适合已经形成稳定敏捷节奏的团队，把需求分解和迭代规划结合起来。</li></ul><p><strong>成熟的看板与报表体系</strong></p><ul><li>Scrum / Kanban 看板、燃尽图、周期分析等，有助于做迭代管理和过程可视化；</li><li>对习惯用数据复盘的团队，支持比较充分。</li></ul><p><strong>可拓展的插件生态</strong></p><ul><li>通过插件补足测试管理、需求追踪、文档协同等能力；</li><li>对有经验的团队来说，可以“拼出”自己的需求管理工具箱。</li></ul><p>但在国内团队落地时，这类海外需求管理工具也会遇到不少现实挑战：</p><ul><li><strong>术语和配置逻辑偏“工程化”</strong>：需要一定英文阅读能力和敏捷实践基础，非研发角色（销售、实施）很容易“看不懂、不敢点”。</li><li><strong>跨系统协同成本高</strong>：常见组合是需求文档在 Confluence，任务在 Jira，沟通在 Slack / 邮件，信息天然分散；如果没有统一约定的需求管理流程，项目经理就需要在多个系统间扮演“信息搬运工”。</li><li><strong>治理依赖“懂工具的人”</strong>：要把项目、权限、工作流配置好，需要有经验的管理员（Admin）；一旦换人或治理松散，系统容易演变成“复杂的任务列表”，需求管理价值被稀释。</li></ul><p>简单来说，Jira（<a href="https://link.segmentfault.com/?enc=a%2FCzo69uNO%2Bsf3G7sY4nRQ%3D%3D.eZDjP%2BdxhClkjny9OCpgRXjpZPTLKWsb5a%2FNk%2Fy3qzCfsu5T3JO4hk4%2BMOHQtYLt" rel="nofollow" target="_blank">https://www.atlassian.com/zh/jira</a>） 这一类海外工具更像一套“以研发团队为中心的需求管理系统”。如果你的团队研发成熟度高、敏捷实践扎实、英文环境友好，那么它会是一款功能强大的需求管理工具；但如果你希望围绕整个业务链路（销售、交付、运营）做需求管理，就要额外评估协同成本。</p><p><img width="723" height="399" referrerpolicy="no-referrer" src="/img/bVdne5p" alt="" title=""/></p><h2>国产研发一体化平台：以 ONES 为代表的“全链路需求管理工具”</h2><p>近几年，国内出现了一批强调“一体化”的研发管理平台。比较典型的代表之一，是像 ONES 研发管理平台这样，把“需求–规划–研发–测试–发布–度量”整合在一套平台里的产品，可以看作是一类国产一体化需求管理工具。</p><p>从需求管理的视角看，这一类平台和前面提到的形态有几个显著差异：</p><p><strong>① 围绕“一个需求全链路”的产品设计</strong></p><ul><li>从需求提出、评审、拆分、排期，到开发、测试、上线，都可以在同一个系统里打通；</li><li>需求下直接挂接任务、缺陷、测试用例、版本计划，形成完整的“需求视图”和追踪链路。</li></ul><p>对项目经理来说，一个直接的体验是：打开某个需求详情页，就能看到它从“被提出”到“被交付”的完整轨迹，而不必在多个系统里来回切换。</p><p><strong>② 贴合本地团队协作习惯的需求管理流程</strong></p><ul><li>支持更细腻的中文字段和状态定义，例如“待澄清、待评审、评审通过、归档”等；</li><li>与企业微信、钉钉、飞书、OA 深度集成，让销售、交付、客服等角色可以自然参与需求管理流程，而不是停留在群聊。</li></ul><p><strong>③ 兼容多种研发模式（敏捷 + IPD + 项目制）</strong></p><ul><li>很多国内团队实际是“IPD + 敏捷 + 客户项目制”的混合形态；</li><li>这类平台通常提供多种项目模板，支持产品型项目、客户交付项目、运维项目等，帮助团队逐步建立统一的需求管理文化。</li></ul><p><strong>④ 从“工具”上升到“治理与度量”</strong></p><ul><li>向上可以看版本和路线图，把需求和产品规划、版本规划联系起来；</li><li>向下可以看到需求到缺陷、到测试、到发布的指标，为效能管理者提供数据基础；</li><li>在组织层面，通过模板、流程、权限统一治理需求管理方法。</li></ul><p>如果你已经感觉“Excel + 文档 + 轻量看板”组合在跨团队协作和组织度量上越来越吃力，那么像 ONES（<a href="https://link.segmentfault.com/?enc=YlG7hKYXiiCiZ5dsNnQyRQ%3D%3D.JocwpUUQ6r%2Bwd9WlHQeqQvFshoQ90UMX2nJFO0b1Yv4%3D" rel="nofollow" target="_blank">https://ones.cn/</a>） 这样的国产需求管理工具 / 一体化平台，值得重点考察。</p><p><img width="723" height="428" referrerpolicy="no-referrer" src="/img/bVdnkm6" alt="" title="" loading="lazy"/></p><h2>国产 vs 海外：同一个项目放进去，会发生什么差异？</h2><p>为了让差异更具象，我们不妨设想一个真实场景。如果你正在评估“国产需求管理工具能不能替代海外工具”，这段会更有代入感。</p><p>你所在的是一个 to B 团队，正在为一个重要客户做报表产品。实施到一半，客户提出：“能不能按业务线 + 区域 + 客户类型自由组合筛选，还要支持导出给下游系统用？”</p><p>这个需求背后牵涉整个需求生命周期：</p><ul><li>需求分析阶段：原有需求边界需不需要重新界定？</li><li>方案设计阶段：现有架构能不能支撑？</li><li>实施与测试阶段：这次改动会影响哪些已有功能和测试？</li><li>项目管理阶段：对交付时间、合同范围有多大影响？</li></ul><p>把同样的场景放进不同形态的需求管理工具里，你会体验到明显差异。</p><h4>维度一：需求变更追踪与影响分析</h4><p>在海外工具 + 文档体系下：</p><ul><li>需求正文在 Confluence，工作项在 Jira；</li><li>每次变更需要产品同步修改文档和 Story，并在备注中解释变更原因；</li><li>评审时，项目经理要一边翻历史页面，一边在 Jira 里查任务状态，才能大致梳理清楚影响范围。</li></ul><p>这种模式依赖较高的自律与意识，在项目数量较少时还可以，但当需求变更频繁、项目并行增多时，很多变更细节就容易散落。</p><p>在国产一体化需求管理平台中：</p><ul><li>需求本身就是“主对象”，下挂任务、测试用例、缺陷、版本；</li><li>变更时，可以直接在需求详情里发起变更讨论，标记影响的任务和测试；</li><li>有些平台还提供可视化的关联关系或“需求影响视图”，帮助项目经理快速判断：“这次改动会影响哪些版本、哪些历史需求和测试范围？”</li></ul><p>对项目经理而言，这关乎你做需求变更管理时的底气，在弱需求管理工具下，风险评估往往只能靠经验和印象；在更完善的需求管理系统里，你可以用事实和数据来支撑决策。</p><h4>维度二：跨部门协作的参与门槛（业务角色能否进入需求管理视野）</h4><p>一个真正有价值的需求管理工具，不只服务研发团队，还要支撑销售、实施、运营、客服等角色参与需求生命周期。</p><p><strong>在海外工具方案下（Jira）：</strong></p><ul><li>非研发同学如果不常用 Jira/Confluence，很难快速找到入口和适合自己的视图；</li><li>他们自然会倾向于用企业微信/邮件来反馈客户需求，导致信息游离在系统之外；</li><li>项目经理被迫扮演“翻译官”与“搬运工”，把业务语言翻译成系统字段，把系统状态翻译回业务语言。</li></ul><p><strong>在国产一体化平台方案下（ONES）：</strong></p><p>通常会和企业微信、钉钉、飞书做深度集成：销售可以在客户需求应用或接入点里直接录入需求；需求状态的关键节点（评审、排期、上线）可以自动通知相关业务同事；业务角色可以在简化的视图中查看自己关心的需求列表。</p><p>这背后体现的是两种协作文化：</p><ul><li>一种是“工程驱动”的需求管理：以研发团队为中心，其它角色通过会议和口头同步参与；</li><li>一种是“业务链路驱动”的需求管理：让整个业务链条都能在同一个需求管理平台上留下痕迹。</li></ul><p>如果你现在最头疼的问题是“业务和研发总是互相指责需求没说清”，那么这一维度非常值得在选型时重点评估。</p><h4>维度三：落地成本与长期治理（从项目到组织）</h4><p>最后一个维度，是很多团队在引入需求管理工具时容易忽略的：这套系统能不能支持未来的组织治理？</p><p><strong>海外通用敏捷工具（Jira）：</strong></p><p>灵活性和可配置能力非常强，但要发挥优势，需要有经验的管理员（Admin）持续治理，包括：工作流、字段、权限、项目模板的统一；避免“每个团队一套配置”，最终导致组织层面无法汇总度量。</p><p><strong>国产一体化平台（ONES）：</strong></p><p>在提供灵活配置的同时，更强调“平台级治理能力”：</p><ul><li>组织可以统一定义需求类型、需求状态、需求模板；</li><li>新团队可直接复用成熟团队的配置与经验；</li><li>管理层可以在统一报表里看到按组织、产品线、客户维度的需求交付情况。</li></ul><p>对项目经理、效能管理者来说，这关乎一个问题：你手上的需求管理工具，是只为当前项目服务，还是能陪着组织从 1 个团队走到 N 个团队，从“野路子实践”走向“可复制的管理方法论”。</p><h2>如何按团队成熟度选择需求管理工具？（国产 vs 海外的选型思路）</h2><p>聊完差异，我们回到最现实的问题：“我们现在到底该选什么样的需求管理工具”？我更习惯从三个维度判断：团队规模、项目复杂度、管理成熟度，而不是直接问“国产和海外哪个更好”。</p><h4>起步期：先解决“需求能看见、说得清”（轻量需求管理工具的阶段）</h4><p>典型特征：团队 10 人以内，项目总量可控；没有专职项目经理或效能角色；需求主要由产品和业务驱动，研发“边做边调”。</p><p>这时不必急着上很重的需求管理系统，更重要的是把两个习惯立住：</p><p><strong>所有需求都有编号、有记录</strong></p><ul><li>不再只停留在聊天记录和会议纪要里；</li><li>用一个统一的文档或表格做“需求总台账”，养成“任何重要需求都要进入列表”的习惯。</li></ul><p><strong>需求都有“状态”，至少粗颗粒</strong></p><ul><li>如：收集中、待评审、已排期、开发中、测试中、已上线、已归档；</li><li>每周例会用 10 分钟过一遍关键需求的状态变动。</li></ul><p>在这个阶段，一款简单的在线文档 + 轻量看板工具（海外或国产都可以）就足够，是轻量版的“需求管理工具”。重要的是：借此培养“需求被看见、被追踪”的团队文化，而不是急于用复杂的系统证明“我们很专业”。</p><h4>发展期：当你开始需要“跨团队协作”和“需求度量”</h4><p>典型特征：团队 10–50 人，多条业务线并行；同一需求牵涉产品、研发、测试、实施、客服等多个角色；管理层开始问：“这个版本到底包含哪些关键需求？”、“从客户提需求到上线，大概需要多久？”</p><p>这一阶段，表格 + 文档组合开始力不从心，你会遇到：</p><ul><li>同一需求在销售话术、PRD、Jira 任务里呈现为不同版本；</li><li>需求变更多，缺乏统一的需求变更管理视图，导致责任归因困难；</li><li>没有统一的需求度量，无法回答“为什么这类需求经常延期”。</li></ul><p>需求管理工具的选型建议：</p><ul><li>如果你的研发团队敏捷实践扎实、英文环境友好，海外敏捷工具（如 Jira）仍是不错选择；</li><li>如果你更希望让销售、实施、客服也在同一套系统里录入、查看和追踪需求，把需求、缺陷、测试、版本放在一条链路上统一管理，那么可以重点考虑国产一体化平台，如 ONES 这样的一体化需求管理工具，把它当作“团队的需求中枢”。</li></ul><p>这个阶段最重要的，是从“工具好不好看”转向“这套需求管理系统能不能反映真实的协作方式”。</p><h4>规模化阶段：从“需求管理工具”走向“研发管理操作系统”</h4><p>典型特征：多团队、多产品、多区域协作并行；管理层开始关心不同业务线的需求交付能力和不同类型需求（客户需求、技术需求、合规需求）的平均周期和质量表现；需要组织级的流程标准化，对审计、合规也有一定要求。</p><p>到这个阶段，单点的需求管理工具已经不够了。你更需要的是一套一体化的研发管理平台：</p><ul><li>将需求管理、项目管理、缺陷管理、测试管理、发布管理、效能度量统一到一套系统；</li><li>支撑组织级的模板管理、流程治理和统一度量体系。</li></ul><p>一个能承载组织方法论的“数字化底座”，是可以将你们这些年积累下来的最佳实践（需求评审流程、优先级打分体系、版本策略）沉淀为可复用、可复制的系统配置，让新团队加入时，沿用这套方法，而不是从头再踩一遍坑。</p><p>在这一阶段，像 ONES 这样的国产一体化研发管理平台，承担的不再只是“替代某一个海外需求管理工具”，而是帮助你把需求管理、项目管理和效能治理连成一条线。</p><p>一个好的需求管理工具 / 需求管理平台，无论是海外方案，还是国产的一体化平台，真正的价值恰恰在于把谁提了什么需求、什么时候变更、谁同意了、最终交付成什么样，忠实记录下来；帮助团队在复盘时，不再只是互相指责，而是基于事实看见：问题究竟出在需求澄清、方案评估，还是实施与验证环节。</p><p>所以，当我们在讨论“国产 vs 海外：需求管理工具怎么选”“国产需求管理工具能不能替代 Jira”时，背后真正要回答的问题是：</p><ul><li>你希望团队成为一个怎样的组织？</li><li>你更习惯哪一种协作文化：靠个人记忆和英雄主义，还是靠透明的信息和共同约定的流程？</li></ul><p>选型的过程，其实也是团队一起回答“我们想成为什么样的组织”的过程——而需求管理工具，只是那条路上的一块重要地基。</p>]]></description></item><item>    <title><![CDATA[复杂系统管理革命：数字孪生打造“可预测的镜像世界” 张老师讲数字孪生 ]]></title>    <link>https://segmentfault.com/a/1190000047466536</link>    <guid>https://segmentfault.com/a/1190000047466536</guid>    <pubDate>2025-12-11 15:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近年来，数字孪生技术正从制造业向更广阔的工程领域拓展，成为驱动产业数字化转型的核心引擎。2025年10月，重庆江北国际机场的工程BIM数字化项目因在施工与智慧运维中的创新应用荣获国家级大奖。无独有偶，同年7月，中国通号研究设计院集团也发布了铁路领域的数字孪生设计建造仿真一体化平台，旨在实现工程“规、建、管、运”的全链条数字化。这些事件清晰地揭示了一个趋势：对于机场、高铁等大型复杂系统，一个与物理实体平行运行、全生命周期联动的“镜像世界”不再是可选项，而是实现精细化管理和智能升级的必然路径。<br/><img width="500" height="263" referrerpolicy="no-referrer" src="/img/bVdnkm2" alt="" title=""/></p><p>这个“镜像世界”，即数字孪生体，并非简单的3D可视化模型。它是一个集成多学科知识、由数据与模型驱动、可动态模拟与预测的复杂系统。其之所以能贯穿设计、建造、运维直至退役的全生命周期，主要依赖于三个层次分明的技术实现原理。</p><h2>第一，基于统一模型与高保真仿真的“数字主线”构建</h2><p>全生命周期管理的基石是数据的连续性与一致性。传统模式下，设计、施工、运维各阶段数据割裂，形成“信息孤岛”。数字孪生通过创建统一的数字化模型（如涵盖几何、物理、规则和行为的BIM/CIM模型）作为“数字主线”。在设计与仿真阶段，系统行为通常由偏微分方程（PDEs）描述。例如，结构应力、流体动力学或电磁场问题可抽象为：<br/>∂u/∂t = F(u, ∇u, ∇²u, ...; μ)<br/> 其中，u是状态变量，μ是控制参数。通过高精度求解这些方程，可以在虚拟空间中提前验证设计性能、进行施工模拟（如吊装、工序）和运维推演（如人流、能耗），将问题解决于发生之前，从源头保障质量与安全。</p><h2>第二，利用代理模型与迁移学习实现实时交互与快速预测</h2><p>然而，求解复杂的多物理场PDE计算成本极高，无法满足实时或高频次分析的需求。为此，数字孪生常采用代理模型技术。一种先进的方法是卡拉胡宁-洛夫神经网络（KL-NN）代理模型。其核心思想是对PDE的解场进行降维，用神经网络的输出来逼近。</p><p>通过预先对原始高保真模型进行离线训练，获得一个既能保持精度、又能实现毫秒级响应的轻量化模型。当物理实体环境或任务目标发生变化时（即PDE中的参数μ发生改变），可运用迁移学习技术，仅用少量新条件下的数据对代理模型进行快速微调，使其迅速适应新状态，从而实现数字孪生体的动态更新与自适应。<br/><img width="723" height="365" referrerpolicy="no-referrer" src="/img/bVdncEw" alt="" title="" loading="lazy"/></p><h2>第三，通过物联网与动态数据驱动完成闭环反馈与优化决策</h2><p>数字孪生的生命在于与物理实体的实时联动。通过广泛部署的物联网传感器网络，物理实体的状态（如设备的振动、温度、能耗，建筑的室内环境，基础设施的形变）被持续采集并同步至数字孪生体。这个过程不仅是数据的单向映射，更是形成“感知-分析-决策-执行”闭环的关键。数字孪生平台将实时数据与代理模型的预测结果进行对比分析。<br/><img width="723" height="437" referrerpolicy="no-referrer" src="/img/bVdnknc" alt="" title="" loading="lazy"/></p><p>基于贝叶斯更新或其他数据同化算法，可以反向校准和优化模型参数，提高预测准确性。更重要的是，它能在虚拟环境中对各类决策进行仿真预演：例如，模拟不同调度方案下机场廊桥的运转效率，或预测特定维护策略对大型设备剩余寿命的影响。最优决策经过验证后，再下发给物理世界的控制系统执行，从而实现资产的预防性维护、能效的持续优化和运营效率的智能化提升。从技术实践来看，构建支撑上述原理的“镜像世界”需要强大的底层引擎与行业知识融合。<br/><img width="723" height="370" referrerpolicy="no-referrer" src="/img/bVdnknk" alt="" title="" loading="lazy"/></p><p>国内一些科技企业如凡拓数创，正致力于通过自研的FTE数字孪生引擎等技术，为智慧城市、智能制造、水利水务等领域提供数字孪生底座。例如，在智慧水务领域，其方案旨在通过构建覆盖供排水全过程的数字孪生感知体系，整合多源数据与行业机理模型，为设施的智能化运维与科学应急调度提供辅助性的分析与可视化支持。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnknl" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[深度解析零信任：以身份为中心的持续安全验证 老实的剪刀 ]]></title>    <link>https://segmentfault.com/a/1190000047466580</link>    <guid>https://segmentfault.com/a/1190000047466580</guid>    <pubDate>2025-12-11 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>零信任，这一重塑现代网络安全格局的理念，最早由Forrester分析师John Kindervag于2010年正式提出。其诞生背景正是由于传统边界安全模型在日益分布式的网络环境中逐渐显露出不足。零信任从根本上挑战了“内部即安全、外部即危险”的传统假设，它指出，无论设备处于网络中的何种位置——内部还是外部，都应被视为如同连接在互联网上一样不可轻信，所有网络流量都必须经过严格验证与管控。<br/>零信任的核心哲学可归结为“永不信任，始终验证”。即企业在设计安全体系时，不应默认信任任何来自内部或外部的访问请求，无论是人员、设备、应用还是系统。相反，必须在每次访问尝试发生时，基于身份进行严格认证与授权，并依赖持续的多维度数据对访问者的可信状态进行动态评估，从而实现自适应的访问控制。<br/>在这一理念的推动下，安全架构的关注点从以网络为中心转向以身份为中心。身份成为实施访问控制的根本依据，而不再仅仅依赖IP地址或网络区域。每一次访问都应遵循最小权限原则，即只授予访问者完成任务所必需的资源权限，避免过度授权带来的潜在风险。<br/>零信任的落地依赖于一套清晰的系统架构，通常分为控制平面与数据平面。控制平面作为“智慧大脑”，负责所有访问策略的集中管理与决策，执行身份验证、权限评估和动态策略生成。一旦控制平面判定某个访问请求合法，它会实时配置数据平面——包括防火墙、网关、代理等实际处理流量的组件，仅允许该请求通过加密通道访问指定资源，并在会话结束后及时撤销权限。此外，控制平面还可协调访问凭证、密钥等安全参数，实现端到端的受控访问。<br/>值得注意的是，零信任并非一次性验证，而是贯穿访问全程的持续信任评估。系统结合身份信息、设备状态、行为上下文、时间和环境等多种数据源，对访问者进行实时分析，一旦发现异常或风险提升，即可动态调整甚至中止访问权限，从而构建起具备弹性与自适应能力的安全防线。<br/>总之，零信任不仅是一种技术框架，更是一种战略性的安全范式转变。它通过以身份为核心、持续验证和动态管控的方式，帮助企业在无边界的数字化环境中，构建起更精细、更灵活且更具韧性的安全体系。</p>]]></description></item><item>    <title><![CDATA[内网IP也要申请SSL证书？ 冷姐Joy ]]></title>    <link>https://segmentfault.com/a/1190000047466257</link>    <guid>https://segmentfault.com/a/1190000047466257</guid>    <pubDate>2025-12-11 14:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、内网 IP 国密证书是什么？</strong></p><p>内网 IP 国密证书是绑定内网静态 IP 地址、采用 SM2/SM3/SM4 等国密算法体系的数字证书，由国家密码管理局认可的 CA 机构签发。它打破了传统域名证书的限制，专为无域名的内网环境设计，核心实现两大功能：​</p><ol><li>身份认证：验证内网服务器对特定 IP 的合法控制权，防范伪造服务端的中间人攻击；​</li></ol><ol start="2"><li>数据加密：通过国密算法对 API 调用、数据库交互等内网通信全程加密，防止 ARP 欺骗、数据篡改等风险。​<br/><img width="723" height="421" referrerpolicy="no-referrer" src="/img/bVde1gp" alt="" title=""/><br/><strong><a href="https://link.segmentfault.com/?enc=aRmfxi8z%2BRaw8N4Zsz6yRA%3D%3D.rwQEu%2FYm7VizxNQ8owXMgmjcOEUBz6833A0XOLKMOSou2%2BWuq1FiYv8oi9saw0CCQ4yLVHUdE1ATg4ONF5YNN%2FGWOBbcG3tewiQE4Ma9V6E%3D" rel="nofollow" target="_blank">https://www.joyssl.com/certificate/select/national_secret_alg...</a></strong></li></ol><p>与普通 SSL 证书相比，其核心差异体现在三方面：​</p><table><thead><tr><th>对比维度</th><th>内网 IP 国密证书​</th><th>普通 SSL 证书​</th></tr></thead><tbody><tr><td>绑定对象​</td><td>内网静态 IP 地址​</td><td>域名（如<a href="https://link.segmentfault.com/?enc=4T9dExfUxkrMQyXnOYN6NA%3D%3D.U9oAPeG6y3k9d86qKU6i9QjDlb%2BiSZoxw2iNJAkwQfk%3D" rel="nofollow" target="_blank">www.example.com</a>）​</td></tr><tr><td>加密算法​</td><td>SM2/SM3/SM4 国密算法​</td><td>RSA/ECC 国际算法​</td></tr><tr><td>核心价值​</td><td>合规性 + 内网适配 + 自主可控​</td><td>通用性 + 全球浏览器兼容​</td></tr><tr><td>适用场景​</td><td>工业控制、政务内网、医疗系统​</td><td>公网网站、电商平台​</td></tr></tbody></table><p><strong>二、为什么必须部署？合规与安全双重驱动​</strong></p><p>（一）法规强制要求​</p><ol><li>《密码法》：关键信息基础设施必须使用商用密码（国密算法）保护通信安全；​</li></ol><ol start="2"><li>等保 2.0 三级及以上：明确要求采用国家认可的密码算法，内网 IP 国密证书是必备整改项；​</li></ol><ol start="3"><li>《数据安全法》：医疗、金融等行业的敏感数据传输需通过国密加密实现合规。​</li></ol><p>（二）内网安全刚需​</p><ul><li>解决 “无域名” 痛点：工业传感器、电力 SCADA 系统等设备多通过 IP 直连，域名证书无法适配；​</li></ul><ul><li>防范内网攻击：某电力企业部署后，中间人攻击拦截率从 68% 提升至 92%，通信加密强度提升 400%；​</li></ul><ul><li>适配国产化环境：兼容银河麒麟系统、360 国密浏览器等国产软硬件，实现全链路自主可控。​</li></ul><p><strong>三、申请与部署：四步实操指南​</strong></p><p>步骤 1：前置准备（1-2 个工作日）​</p><ul><li>IP 资质证明：提供 ISP 分配的静态 IP 合同、内网拓扑图，证明 IP 所有权；​</li></ul><ul><li>端口检查：确保 80 端口（验证用）、443 端口（服务用）未被防火墙拦截</li></ul><ul><li>算法方案：推荐 “SM2+RSA 双证书” 模式，兼顾国密合规与国际浏览器兼容。​</li></ul><p>步骤 2：选择合规 CA 机构​</p><p>优先选用国家密码管理局认证服务商：​</p><ul><li>JoySSL：支持内网 IP 申请，提供免费测试证书及 7×24 小时技术支持；​</li></ul><ul><li>CFCA：金融级认证，支持 UKEY 存储私钥，适配银行内网场景；​</li></ul><ul><li>上海 CA：政务云经验丰富，适合电子政务内网部署。​</li></ul><p>步骤 3：提交申请与验证（1-3 个工作日）​</p><ol><li>生成密钥对与 CSR 文件（证书请求文件）；​</li></ol><ol start="2"><li>提交材料：企业用户需营业执照、IP 分配证明；个人用户需身份证及用途说明；​</li></ol><ol start="3"><li>完成验证：通过 DNS 解析添加 TXT 记录（推荐）或上传验证文件至服务器。​</li></ol><p>步骤 4：测试与验收​</p><ul><li>功能测试：用 360 国密浏览器访问 IP，地址栏显示 “安全锁” 即部署成功；​</li></ul><ul><li>合规检测：通过 “国密 SSL 检测工具” 验证算法支持性，确保符合密评要求；​</li></ul><ul><li>兼容性测试：确认国产终端（如华为鲲鹏服务器）与证书正常交互。​</li></ul><p><strong>四、典型场景与运维要点​</strong></p><p>（一）三大核心应用场景​</p><ol><li>工业控制内网：国家电网用其加密 SCADA 系统通信，防止黑客篡改电力调度指令；​</li></ol><ol start="2"><li>医疗内网：瑞金医院通过国密证书加密电子病历传输，符合《健康医疗数据安全指南》；​</li></ol><ol start="3"><li>政务内网：某省级政务平台部署后，实现公文传输全程加密，通过等保三级认证。​</li></ol><p>（二）运维关键控制点​</p><ul><li>有效期管理：证书有效期通常 1 年，需提前 30 天续期，可通过 Prometheus 监控剩余天数；​</li></ul><ul><li>私钥安全：采用 HSM 硬件安全模块存储私钥，避免泄露导致内网服务被伪造；​</li></ul><ul><li>客户端配置：通过组策略（GPO）将 CA 根证书导入内网终端 “受信任根证书区”，消除浏览器警告。​</li></ul><p><strong>五、总结​</strong></p><p>在国产化替代与内网安全升级的双重背景下，内网 IP 国密证书已从 “合规选项” 变为 “必选项”。它不仅是满足《密码法》《等保 2.0》的技术载体，更是构建自主可控内网安全体系的核心基石。选择合规 CA、规范部署流程、建立全生命周期管理机制，才能让国密技术真正守护内网每一次数据传输。</p>]]></description></item><item>    <title><![CDATA[内网国密证书：筑牢企业数字安全的自主防线 细心的红酒 ]]></title>    <link>https://segmentfault.com/a/1190000047466259</link>    <guid>https://segmentfault.com/a/1190000047466259</guid>    <pubDate>2025-12-11 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化浪潮席卷各行各业的今天，企业内部网络承载着核心数据、业务流程和商业秘密。为这片“数字疆域”构筑坚实可靠的安全防线，已成为企业生存发展的生命线。而<strong>国密证书</strong>，正是这道防线上至关重要且日益凸显的自主可控核心组件。它并非简单的技术替代，而是从底层架构出发，为内网安全赋予了符合中国标准、自主可控的信任基石。<br/><img width="723" height="366" referrerpolicy="no-referrer" src="/img/bVdnkiW" alt="" title=""/></p><p><strong>一、核心价值：为什么内网需要国密证书？</strong></p><p>内网环境看似封闭，实则面临着内部越权访问、数据窃取、仿冒设备接入、传输窃听等多维度风险。国密证书基于国家密码管理局颁布的SM2、SM3、SM4等商用密码算法，为内网安全提供了不可或缺的信任与加密服务。</p><p><strong>自主可控，筑牢安全根基</strong>：采用国家自主设计的密码算法，从根本上避免了国际通用算法可能存在的“后门”风险或被特定技术体系锁定的隐患，确保安全命脉掌握在自己手中。</p><p><strong>合规驱动，满足监管要求</strong>：随着《密码法》、《网络安全法》、《数据安全法》以及关基保护条例的深入实施，在金融、能源、交通、政务等关键行业，使用经过认证的国密算法保障内网安全已成为明确的合规性要求。</p><p><strong>适应高性能需求</strong>：SM2椭圆曲线算法在相同安全强度下，较国际通用的RSA算法具有密钥更短、计算更快、存储更小的优势，更适应内网中大量设备、高频次的身份认证与数据交换场景。</p><h3><strong>内网国密证书<a href="https://link.segmentfault.com/?enc=0BTV4MSdm5Og1vCfMo15qA%3D%3D.jRm0vVnDGUI9tJl1pjkU%2FsFKBZtFfSm3Twr44J0NbHdImZpM%2BdykSD8BNzYeu32IksA%2Bd1GVpZ%2FouMnzR2QE2pCE0G8yydsp17TCHDx%2BrVQ%3D" rel="nofollow" target="_blank">申请入口</a>，注册码填写230976完成注册，获取证书。</strong></h3><p><strong>二、核心应用场景：国密证书在内网中如何发挥作用？</strong></p><p>国密证书在内网的应用贯穿于身份认证、通信加密、行为可信等多个层面，构建起立体化的防护体系。</p><p><strong>身份认证与访问控制</strong></p><p><strong>设备身份认证</strong>：为内网服务器、PC终端、移动设备、物联网节点等签发国密SSL证书，确保只有可信设备才能接入网络，防止非法设备仿冒侵入。</p><p><strong>用户身份认证</strong>：结合国密证书实现强双因子认证，替代或增强传统的用户名/口令方式，用于VPN登录、核心应用系统访问、运维堡垒机登录等高权限场景，有效防御凭证窃取与冒用。</p><p><strong>特权账户管理</strong>：为管理员、运维人员签发专用证书，实现权限与身份的强绑定，所有特权操作皆可追溯至具体证书持有人，实现精准的权限管理与审计。</p><p><strong>通信链路安全</strong></p><p><strong>加密传输通道</strong>：在内部办公系统、数据库连接、API接口调用、部门间数据交换等场景中，部署国密SSL/TLS证书，建立基于国密算法的加密通道，确保敏感数据在传输过程中无法被窃听或篡改。</p><p><strong>安全邮件与即时通讯</strong>：为内部邮件系统、安全即时通讯工具部署国密证书，实现邮件的端到端加密签名，保障内部通信内容的机密性与完整性。</p><p><strong>应用与数据安全</strong></p><p><strong>代码与文档签名</strong>：对内部发布的软件更新包、配置文件、重要电子文档进行国密算法签名，确保分发来源可信、内容未被篡改。</p><p><strong>构建全栈国密体系</strong>：从底层服务器、网络设备、到中间件、数据库、应用系统，全面支持并部署国密证书，可实现内网“纵横”全链条的国密化改造，形成统一的高安全环境。</p><p><strong>三、实施重点与挑战</strong></p><p>成功部署内网国密证书体系，需聚焦以下关键点：</p><p><strong>统筹规划与架构设计</strong>：需将证书体系纳入企业整体IT与安全架构通盘考虑，规划完整的证书生命周期（颁发、部署、更新、吊销）管理流程。</p><p><strong>建设可靠的私有证书体系</strong>：通常需要搭建企业内部的国密私有证书认证中心（国密CA），或选用可信的第三方国密商业CA服务。这是整个信任体系的“信任锚”。</p><p><strong>系统与应用的兼容性改造</strong>：确保内网的操作系统、浏览器、应用软件、硬件设备（如VPN网关、负载均衡）等均支持国密算法和国密证书格式（如SM2）。</p><p><strong>平滑迁移与持续运维</strong>：制定从国际算法到国密算法的平滑迁移策略，并建立专业的团队负责证书的日常运维、监控和应急响应。</p><p><strong>四、未来展望</strong></p><p>随着信创产业的深入推进和数字化转型的深化，国密证书在内网的应用将呈现两大趋势：一是从“<strong>可用</strong>”到“<strong>好用</strong>”，生态兼容性将大幅提升，部署管理更加自动化、智能化；二是从“<strong>单点应用</strong>”到“<strong>深度融合</strong>”，与零信任网络、云原生安全、物联网安全等新型架构深度结合，成为构建内生安全体系的默认选项。</p><p><strong>结语</strong></p><p>内网国密证书的部署，远不止于一项技术升级或合规动作。它是企业主动拥抱国家密码战略，将安全发展主动权牢牢掌握在自己手中的关键举措。通过构建以国密证书为核心的内生信任体系，企业能够为核心资产和业务运营构筑起一道自主可控、高性能、合规的深层防御屏障，从而在数字时代赢得坚实可靠的安全优势，行稳致远。</p>]]></description></item><item>    <title><![CDATA[分布式应用开发的核心技术系列之——基于TCP/IP的原始消息设计 曾经爱过的莲藕 ]]></title>    <link>https://segmentfault.com/a/1190000047466243</link>    <guid>https://segmentfault.com/a/1190000047466243</guid>    <pubDate>2025-12-11 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今这个信息爆炸的时代，编程语言层出不穷，但有一种语言凭借其简洁、高效和并发的特性，在众多语言中脱颖而出，它就是Go语言。Go语言，也被称为Golang，由Google公司开发并开源，自诞生以来就受到了广大开发者的喜爱。本文将带你领略Go语言的魅力，从入门到进阶，逐步掌握这门强大的编程语言。</p><p>二、Go语言入门</p><p>了解Go语言的基本特性<br/>Go语言具有简洁、高效、静态类型、编译型等特性。它的语法简单易懂，上手快速。同时，Go语言支持并发编程，通过goroutine和channel可以轻松实现高并发。</p><p>安装Go语言环境</p><p>要开始学习Go语言，首先需要安装Go语言环境。可以从Go官方网站下载并安装对应操作系统的安装包，然后按照官方文档进行配置。</p><p>编写第一个Go程序</p><p>安装好Go语言环境后，就可以开始编写第一个Go程序了。一个简单的“Hello, World!”程序可以帮助你熟悉Go语言的语法和编译过程。</p><p>掌握Go语言的基本语法<br/><a href="https://link.segmentfault.com/?enc=BhrDpKO9wY%2FavF3mptoU4g%3D%3D.yQz55UyGSWB7f%2F5q8SIStptCmVhjEO0%2BHFu8jVPWqT3Gi7aI30m2xF1OahWj9wldnIjc2VCEs8mqoyWmgHK7fw%3D%3D" rel="nofollow" target="_blank">https://www.xiongtianqi.cn/thread-316584-1-1.html</a><br/><a href="https://link.segmentfault.com/?enc=8Ab4R7Qfg65rug1c3HUSng%3D%3D.89zA3IpvhzL4qxbAn5Jztzs0FL8NV%2Bx6YcfH9EqD%2BI1oZWQJWY26aCGaJD8Wb2AhBny7txdZsOEsdrNZzbuXhg%3D%3D" rel="nofollow" target="_blank">https://www.xiongtianqi.cn/thread-316583-1-1.html</a><br/><a href="https://link.segmentfault.com/?enc=H6giQglzGX7aDHtDAN9ASA%3D%3D.KKtoVXCDXDuIexWOgw8vG5QQR9CGXqSRHFgWWHQyaIIU%2FxirLbqnNUCDxK2qvftyLilPwpX0J9zfY1Y4TWLOrA%3D%3D" rel="nofollow" target="_blank">https://www.xiongtianqi.cn/thread-316582-1-1.html</a><br/><a href="https://link.segmentfault.com/?enc=lz2pRfH%2BaiUYb56vX7VyvQ%3D%3D.odZloA0WVu2rcozgpJPb8I9YHKuXvrMRtcMfDGsmyE7Sf1aZDADF4XUfWGug4CjqMIFkDFuK0Y0ZmJwqdEXgrA%3D%3D" rel="nofollow" target="_blank">https://www.xiongtianqi.cn/thread-316581-1-1.html</a><br/>在编写程序的过程中，你需要熟悉Go语言的基本语法，包括变量、常量、数据类型、运算符、控制结构等。这些基础知识是后续学习的基础。</p>]]></description></item><item>    <title><![CDATA[【剪映API】向现有草稿中添加视频特效 失落的木瓜_esfWwz ]]></title>    <link>https://segmentfault.com/a/1190000047465973</link>    <guid>https://segmentfault.com/a/1190000047465973</guid>    <pubDate>2025-12-11 12:08:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>ADD_EFFECTS API 接口文档</h2><h3>接口信息</h3><pre><code class="bash">POST /openapi/capcut-mate/v1/add_effects</code></pre><h3>功能描述</h3><p>向现有草稿中添加视频特效。该接口用于在指定的时间段内添加特效素材到剪映草稿中，支持多种特效类型如边框特效、滤镜特效、动态特效等。特效可以用于增强视频的视觉效果。</p><h3>更多文档</h3><p>📖 更多详细文档和教程请访问：<a href="https://link.segmentfault.com/?enc=pK2DPhSTNu9aJ89Q%2FhZkVA%3D%3D.GO1Nor2nYm5I1BXvv%2BL%2FTzRGy3En9zW60dYaDqNJi3k%3D" rel="nofollow" target="_blank">https://docs.jcaigc.cn</a></p><h3>请求参数</h3><pre><code class="json">{
  "draft_url": "https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/get_draft?draft_id=2025092811473036584258",
  "effect_infos": "[{\"effect_title\": \"录制边框 III\", \"start\": 0, \"end\": 5000000}, {\"effect_title\": \"复古滤镜\", \"start\": 2000000, \"end\": 7000000}]"
}</code></pre><h4>参数说明</h4><table><thead><tr><th>参数名</th><th>类型</th><th>必填</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td>draft_url</td><td>string</td><td>✅</td><td>-</td><td>目标草稿的完整URL</td></tr><tr><td>effect_infos</td><td>string</td><td>✅</td><td>-</td><td>特效信息列表的JSON字符串</td></tr></tbody></table><h4>参数详解</h4><h5>effect_infos 字段格式</h5><p><code>effect_infos</code> 是一个JSON字符串，包含特效信息数组，每个特效对象包含以下字段：</p><pre><code class="json">[
    {
        "effect_title": "录制边框 III",  // 特效名称/标题，必选参数
        "start": 0,                     // 特效开始时间（微秒），必选参数  
        "end": 5000000                  // 特效结束时间（微秒），必选参数
    }
]</code></pre><p><strong>字段说明</strong>:</p><ul><li><code>effect_title</code>: 特效名称，必须是系统中已存在的特效名称</li><li><code>start</code>: 特效开始时间，单位为微秒，必须大于等于0</li><li><code>end</code>: 特效结束时间，单位为微秒，必须大于start</li></ul><h5>时间参数</h5><ul><li><strong>start</strong>: 特效在时间轴上的开始时间，单位为微秒（1秒 = 1,000,000微秒）</li><li><strong>end</strong>: 特效在时间轴上的结束时间，单位为微秒</li><li><strong>duration</strong>: 特效显示时长 = end - start</li></ul><h5>特效名称说明</h5><ul><li><p><strong>effect_title</strong>: 特效的名称</p><ul><li>格式：字符串</li><li>示例：<code>"录制边框 III"</code></li><li>获取方式：通过剪映特效库或相关API获取</li><li><p>常见特效名称：</p><ul><li>边框特效："录制边框 III", "简约边框", "霓虹边框"</li><li>滤镜特效："复古滤镜", "黑白滤镜", "暖色调"</li><li>动态特效："粒子效果", "光晕效果", "闪烁特效"</li><li>转场特效："淡入淡出", "推拉门", "马赛克转场"</li></ul></li></ul></li></ul><h3>响应格式</h3><h4>成功响应 (200)</h4><pre><code class="json">{
  "draft_url": "https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/get_draft?draft_id=2025092811473036584258",
  "track_id": "effect_track_123",
  "effect_ids": ["effect_001", "effect_002"],
  "segment_ids": ["seg_001", "seg_002"]
}</code></pre><h4>响应字段说明</h4><table><thead><tr><th>字段名</th><th>类型</th><th>说明</th></tr></thead><tbody><tr><td>draft_url</td><td>string</td><td>更新后的草稿URL</td></tr><tr><td>track_id</td><td>string</td><td>特效轨道ID</td></tr><tr><td>effect_ids</td><td>array</td><td>添加的特效ID列表</td></tr><tr><td>segment_ids</td><td>array</td><td>创建的特效片段ID列表</td></tr></tbody></table><h4>错误响应 (4xx/5xx)</h4><pre><code class="json">{
  "detail": "错误信息描述"
}</code></pre><h3>使用示例</h3><h4>cURL 示例</h4><h5>1. 基本特效添加</h5><pre><code class="bash">curl -X POST https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/add_effects \
  -H "Content-Type: application/json" \
  -d '{
    "draft_url": "YOUR_DRAFT_URL",
    "effect_infos": "[{\"effect_title\": \"录制边框 III\", \"start\": 0, \"end\": 5000000}]"
  }'</code></pre><h5>2. 批量特效添加</h5><pre><code class="bash">curl -X POST https://capcut-mate.jcaigc.cn/openapi/capcut-mate/v1/add_effects \
  -H "Content-Type: application/json" \
  -d '{
    "draft_url": "YOUR_DRAFT_URL",
    "effect_infos": "[{\"effect_title\": \"录制边框 III\", \"start\": 0, \"end\": 5000000}, {\"effect_title\": \"复古滤镜\", \"start\": 2000000, \"end\": 7000000}]"
  }'</code></pre><h3>错误码说明</h3><table><thead><tr><th>错误码</th><th>错误信息</th><th>说明</th><th>解决方案</th></tr></thead><tbody><tr><td>400</td><td>draft_url是必填项</td><td>缺少草稿URL参数</td><td>提供有效的draft_url</td></tr><tr><td>400</td><td>effect_infos是必填项</td><td>缺少特效信息参数</td><td>提供有效的effect_infos</td></tr><tr><td>400</td><td>时间范围无效</td><td>end必须大于start</td><td>确保结束时间大于开始时间</td></tr><tr><td>400</td><td>无效的特效信息，请检查effect_infos字段值是否正确</td><td>特效参数校验失败</td><td>检查特效参数是否符合要求</td></tr><tr><td>404</td><td>草稿不存在</td><td>指定的草稿URL无效</td><td>检查草稿URL是否正确</td></tr><tr><td>404</td><td>特效不存在</td><td>指定的特效名称无效</td><td>确认特效名称是否正确</td></tr><tr><td>500</td><td>特效添加失败</td><td>内部处理错误</td><td>联系技术支持</td></tr></tbody></table><h3>注意事项</h3><ol><li><strong>时间单位</strong>: 所有时间参数使用微秒（1秒 = 1,000,000微秒）</li><li><strong>特效名称</strong>: 确保使用有效的特效名称</li><li><strong>时间范围</strong>: end必须大于start</li><li><strong>轨道管理</strong>: 系统自动创建特效轨道</li><li><strong>性能考虑</strong>: 避免同时添加大量特效</li></ol><h3>工作流程</h3><ol><li>验证必填参数（draft_url, effect_infos）</li><li>检查时间范围的有效性</li><li>从缓存中获取草稿</li><li>创建特效轨道（如果不存在）</li><li>解析特效信息并创建特效片段</li><li>添加片段到轨道</li><li>保存草稿</li><li>返回特效信息</li></ol><h3>相关接口</h3><ul><li><a href="./create_draft.md" target="_blank">创建草稿</a></li><li><a href="./add_videos.md" target="_blank">添加视频</a></li><li><a href="./add_audios.md" target="_blank">添加音频</a></li><li><a href="./add_images.md" target="_blank">添加图片</a></li><li><a href="./save_draft.md" target="_blank">保存草稿</a></li><li><a href="./gen_video.md" target="_blank">生成视频</a></li></ul><hr/><p>&lt;div align="right"&gt;</p><p>📚 <strong>项目资源</strong>  <br/>GitHub搜索capcut-mate就能找到。</p><p>&lt;/div&gt;</p>]]></description></item><item>    <title><![CDATA[低成本创业新方向：使用现成源码搭建游戏陪玩小程序平台的方案 多客Duoke ]]></title>    <link>https://segmentfault.com/a/1190000047466019</link>    <guid>https://segmentfault.com/a/1190000047466019</guid>    <pubDate>2025-12-11 12:07:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>游戏陪玩系统作为聚焦游戏社交与服务的平台，连接有陪玩需求的用户与提供陪玩服务的玩家，通过完善的功能体系实现服务匹配、交易保障和体验优化，满足不同用户的游戏社交需求。<br/><img width="723" height="556" referrerpolicy="no-referrer" src="/img/bVdnkeU" alt="" title=""/><br/>1、<strong>选择合适的源码</strong>：寻找市场上评价好、功能完善的陪玩系统小程序源码。重要的是要评估源码的安全性、可扩展性以及代码质量，并确保有良好的售后服务。</p><p>2、<strong>服务器部署</strong>：选择一个稳定可靠的云服务器供应商（如阿里云或腾讯云），用于部署你的小程序后端服务。确保有足够的带宽和性能来支持高并发的实时交互。</p><p>3、<strong>个性化定制</strong>：根据自己的品牌特色对源码进行调整，添加独特的功能和服务，以区别于其他竞争对手，提高竞争力。<br/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdcACU" alt="" title="" loading="lazy"/><br/>确定目标市场明确你的目标用户群体，例如游戏爱好者、需要提升技能的玩家等。分析市场需求，找出未被充分满足的需求点。</p><p>源码选择与购买寻找可靠的供应商购买游戏陪玩小程序源码。确保源码包含所有必要的功能，并且具有良好的用户体验设计。仔细检查源码的功能列表，如用户注册登录、陪玩服务下单、在线支付、评价系统等。部署与配置购买服务器空间，用于部署小程序后台。根据供应商提供的指南进行源码上传和初步配置。设置域名解析和微信小程序账号绑定。<br/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdeT7E" alt="" title="" loading="lazy"/><br/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdcADg" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[汽车整车制造中，怎样解决传统生产流程的瓶颈问题？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047466030</link>    <guid>https://segmentfault.com/a/1190000047466030</guid>    <pubDate>2025-12-11 12:06:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>汽车整车制造的瓶颈问题一直是困扰行业的顽疾，从焊装车间的夹具切换时间，到涂装环节的漆膜均匀性控制，再到总装线的物料配送延迟，每一个环节的卡顿都可能成为全局效率的拖累。以某合资车企为例，其发动机生产线长期受困于缸体三防漆涂覆工艺，由于固化时间不一致导致前后工序频繁等待，单次等待时间平均长达45分钟，直接降低了设备综合效率（OEE）。类似场景在传统制造企业中并不少见，如何跳出“头痛医头”的局部优化，实现系统性瓶颈破解，成为亟待解决的课题。<br/>瓶颈问题的根源往往隐藏在工艺设计、资源配置和信息协同的深层矛盾中。根据人人文库的技术报告，传统生产线常存在四类典型瓶颈：材料供应滞后、设备换模时间过长、工艺参数波动，以及物流配送效率低下。以芯片短缺危机期间某车企的表现为例，由于供应链信息割裂，工厂未能及时调整焊装产线的物料优先级，导致部分车型延期交付。这种经验依赖型的传统模式在面对偶发性波动时显得尤为脆弱。<br/>而瓶颈的解决需要依托数字化转型带来的系统性变革。广域铭岛的案例或许能提供参考。该企业通过搭建工业互联网平台，实现了生产数据的实时采集与分析。例如，在阴极电泳槽漆膜厚度不均的问题上，其部署的数字孪生技术结合3D视觉反馈，将泳液分布优化周期从数小时缩短至30分钟以内，缺陷流出率下降了80%。这种技术赋能的核心在于打破数据孤岛，让工艺参数与设备状态形成闭环管理。<br/>除了技术层面，管理机制的优化同样关键。某本土品牌车企在精益生产实践中发现，员工技能单一与跨部门协作不足是瓶颈形成的重要因素。他们通过推行多能工轮岗制度，将操作工培养成具备2-3种核心技能的复合型人才，同时设立跨职能“战区制”，用扁平化管理缩短决策链条。这种组织变革带来的直接效果是，换型时间减少了40%，生产线平衡率提升了15个百分点。<br/>更深层次的瓶颈还往往来自工艺本身的惯性。例如，传统冲压工序中板材排样效率低下，导致材料浪费率高达5%。某研究机构通过对比发现，当引入AI排样算法后，材料利用率可提升至92%以上，年节省成本数百万元。这种工艺革新往往需要打破原有的路径依赖，将“经验驱动”升级为“数据驱动”。<br/>值得一提的是，智能制造技术正在为瓶颈管理提供全新视角。CSDN平台提到的“工业AI大模型”通过融合工艺机理与实时数据，实现了从感知到决策的全流程自动化。例如，在焊装质量控制环节，模型库覆盖3000多个焊点，能动态识别虚焊、漏焊等缺陷，将传统数小时的排查压缩至分钟级。这种技术突破的背后，是软硬协同能力的提升——算法不仅需要与设备深度集成，更要具备自学习、自适应的进化特性。<br/>从行业实践来看，瓶颈解决正在经历从局部优化到全局协同的演进。人人文库指出，现代汽车工厂普遍采用“三现主义”（现场、现物、现状）作为基础方法，配合TPM（全员生产维护）、Kaizen（改善提案）等工具，形成持续改进的文化氛围。这种软硬兼施的路径，让瓶颈管理从单纯的“堵漏”转变为“造血”。<br/>结语<br/>传统生产流程的瓶颈问题，表面上是效率制约，实则反映了制造业数字化转型的深层次需求。从数据驱动的工艺优化，到组织机制的系统升级，任何一个环节的改进都需要全局视角。未来，随着工业智能体技术的成熟，汽车制造将逐步实现从“规模生产”到“智慧制造”的跃迁。</p>]]></description></item><item>    <title><![CDATA[如何建设一个真正高效的智能制造工厂？从零到落地的完整路径 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047466072</link>    <guid>https://segmentfault.com/a/1190000047466072</guid>    <pubDate>2025-12-11 12:05:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在“中国制造”迈向“中国智造”的关键转型期，智能制造工厂正成为重塑全球制造业格局的核心载体。它不再仅仅是自动化设备的堆砌，而是深度融合物联网、大数据、人工智能、数字孪生等新一代信息技术，实现研发、生产、质量、供应链全链条智能化协同的新型生产体系。这一变革，不仅提升了效率与质量，更推动中国制造业从“大而不强”向“高端化、智能化、绿色化”全面跃升。<br/>过去，中国虽为全球制造第一大国，但消费者热衷“海外代购”，折射出“有规模、无品质”的深层痛点。为此，国家自“十三五”起系统布局智能制造，通过试点示范、标准引领、梯度培育，逐步构建起“基础级—先进级—卓越级—领航级”的智能工厂发展路径。如今，中国智能坐便器产量占全球72%，新能源汽车、锂电池、光伏产品等“新三样”强势出海，标志着中国制造已成功重塑全球认知。<br/>在这一进程中，智能工厂的“智能”体现在每一个环节：美的利用生成式AI在数小时内生成数千款空调设计方案；宝钢湛江钢铁通过智能控冷系统将钢板尺寸精度达标率提升至99.1%；华为松山湖基地借助AI“数字样机+柔性量产”平台，将产品缺陷率压至惊人的0.5ppm。这些案例表明，智能制造已从概念走向规模化落地，成为提升核心竞争力的刚性需求。<br/>然而，智能制造的真正挑战，不在于少数“灯塔工厂”的闪耀，而在于如何让占制造业主体的数万家中小企业“转得动、转得起、转得稳”。为此，国家启动智能工厂梯度培育行动，明确提出“政府引导、供给赋能、龙头带动、梯度培育”的协同路径。其中，广域铭岛作为工业互联网领域的先锋力量，以自主研发的Geega工业互联网平台，为中小企业破解“不敢转、不会转、不能转”的困局提供了可复制的解决方案。<br/>广域铭岛的Geega平台，构建了覆盖“计划排产—模具管理—制造运营—质量控制”的全栈智能体矩阵。其智能排产引擎能动态整合300+种模具的冷却时间、设备吨位、订单优先级等复杂约束，像“智能导航”一样自动优化换模顺序，将订单交付周期从21天压缩至12天；其模具智能管理系统通过实时监测冲次与健康指数，实现预测性维护，故障响应时间从2小时缩短至15分钟，准确率超95%；其“工厂大脑”Mom平台更打破ERP、MES、CRM等系统孤岛，融合视觉、语音、文本等多模态数据，实现从被动响应到主动预判的跃迁——在汽车焊装环节，工艺优化周期缩短60%，缺陷率下降45%。<br/>更重要的是，广域铭岛的模式不是“高大上”的技术秀，而是以轻量化、模块化、平台化的方式，让中小企业能像搭积木一样按需接入智能功能。无论是通过进销存系统实现基础数字化，还是借助AI算法优化能耗与库存，其“云-边-端”架构让技术下沉成为可能，真正实现了“塔尖引领、底座坚实”的产业生态。<br/>智能制造工厂的未来，是系统协同的生态竞争。它要求从单厂升级走向产业链协同，从技术应用走向管理变革。广域铭岛的实践表明，真正的智能工厂，是“人、机、料、法、环”在数据驱动下的有机共生体。它不仅是效率工具，更是企业文化的重塑者——推动全员从“经验依赖”走向“数据决策”，从“被动执行”走向“主动优化”。<br/>当前，中国智能制造正从“试点探索”迈向“规模普及”，从“单点突破”走向“系统协同”。在国家政策引导与龙头企业带动下，以广域铭岛为代表的工业互联网平台，正成为连接技术与产业、大企业与中小企业的关键纽带。未来，随着人工智能与精益管理的深度融合，智能制造工厂将不仅是制造的“执行者”，更是创新的“策源地”——为中国从制造大国迈向智造强国，注入源源不断的智能动能。</p>]]></description></item><item>    <title><![CDATA[制造智能体如何帮助企业降低废品率？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047466082</link>    <guid>https://segmentfault.com/a/1190000047466082</guid>    <pubDate>2025-12-11 12:05:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在人工智能深度渗透制造业的今天，“制造智能体”正从一个技术概念演变为驱动产业变革的核心引擎。它不再是孤立的自动化程序或简单的AI工具，而是一个具备感知、分析、决策与执行能力的协同智能网络，是工业知识与AI技术深度融合的产物，是制造业迈向“自主化”“自优化”新阶段的标志性存在。<br/>制造智能体的本质，在于将过去依赖人工经验的“黑箱操作”，转化为可计算、可迭代、可协同的智能流程。它以工业大模型为认知基础，融合物联网感知、边缘计算、数字孪生与生成式AI等技术，构建起覆盖“感知—分析—决策—执行—反馈”全闭环的智能系统。在这一系统中，设备不再是被动执行指令的机器，而是能自主学习、动态优化、协同响应的“数字员工”。<br/>广域铭岛作为这一领域的先行者，率先构建了以“Geega工业AI平台+工业智造超级智能体”为核心的体系，实现了从单点智能到全域协同的跃迁。其核心突破在于三大“打通”：打通数据孤岛，统一ERP、MES、QMS等异构系统的语言，让每秒2000个温度点、焊接曲线、涂层厚度等海量数据不再成为噪声，而是驱动决策的燃料；打通知识壁垒，将老师傅的“手感”“经验”转化为可复用的振动频谱阈值、工艺参数模型，形成可调用、可进化的“电子字典”；打通决策闭环，实现从异常检测到方案生成的毫秒级响应——当供应链中断时，12类智能体可在5分钟内协同生成3套替代方案，将原本数小时的人工协调压缩至分钟级，损失降低80%。<br/>在具体场景中，制造智能体的价值已全面显现。在汽车制造领域，智能体动态优化涂装温湿度与拧紧参数，使废品率下降18%，研发周期缩短30%；在新能源电池生产中，它通过数字孪生实时模拟200余项工艺参数，实现质量隐患提前72小时预警，废品率下降22%；在电解铝车间，智能体自动调节电流，能耗降低40%，设备停机率下降25%。更深远的是，它正在重构制造的底层逻辑：研发端，AI自动生成DFMEA报告，年省24000小时；生产端，SOP开发从数天缩短至小时级，人力成本降低80%；质量端，从“事后抽检”进化为“事中预防”；能源端，碳管理智能体对接碳交易市场，助力企业年减碳超百万吨。<br/>广域铭岛的创新不仅在于技术落地，更在于构建了“平台+智能体”的普惠生态。其Geega平台如同工业界的“安卓系统”，让中小企业也能通过“即插即用”的超融合工作站，以低成本迈入智能化门槛；其超级智能体则像一个“数字员工集群”，覆盖研、产、供、销、服全链路，形成群体智能，推动工厂从“人指挥机器”转向“机器自主协同”。<br/>制造智能体的未来，是全域协同、自主进化与绿色融合的统一。它将与边缘计算、联邦学习、工业区块链深度融合，实现跨企业、跨区域的质量云协同与隐私保护；它将深度参与“双碳”战略，成为绿色制造的智能中枢；它将从“解决问题”走向“定义问题”，在新车型研发、新材料试验中主动提出优化路径。<br/>可以说，制造智能体正在重新定义“制造”本身——它以数据为血液、以知识为逻辑、以协同为架构，让工厂拥有“大脑”与“神经”。广域铭岛的实践证明，这不仅是效率的提升，更是产业形态的重构。当每一个设备、每一道工序都具备思考与进化的能力，中国制造业便真正迈入了AI原生时代，从“制造大国”迈向“智造强国”的路径，已然清晰可见。</p>]]></description></item><item>    <title><![CDATA[从0到1搭建一个智能分析OBS埋点数据的AI Agent｜得物技术 得物技术 ]]></title>    <link>https://segmentfault.com/a/1190000047466084</link>    <guid>https://segmentfault.com/a/1190000047466084</guid>    <pubDate>2025-12-11 12:04:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、背景</h2><p>某天打开组内的Grafana仪表盘，突然好奇我们的埋点从被触发后是如何一步一步变成所展示的各种图表的，于是在我进行一系列的探索之后，总结出了以下链路：</p><ul><li>在指标工厂新建指标，确定埋点key和埋点元数据。</li><li>代码中指定埋点key和埋点数据，通过watchDog发送kafka消息到obs monitor topic。</li><li>为埋点指标新建数据处理任务，将消费到的kafka消息落到指定的数据表中。</li><li>添加新的仪表盘，编写展示数据背后的SQL语句。</li></ul><p><strong>痛点</strong>：每需要添加一个新的数据分析大盘，就需要人工去分析各个表结构、表与表之间的联系、表各个字段的含义等，在充分理解其含义后再费时费力地编写SQL语句，并不断调优。这导致OBS埋点数据分析的场景相对固化，并且难以支持灵活的数据查询要求。</p><p><img width="723" height="629" referrerpolicy="no-referrer" src="/img/bVdnkfh" alt="" title=""/></p><h2>二、思考</h2><p>在分析了当前系统的痛点后，我意识到这是一个典型的可以利用AI能力来对现有功能进行扩展的场景。因为：</p><ul><li>场景多变，因为你不知道用户可能想查看什么样的数据，无法通过代码穷举；</li><li>需要了解业务同时又具备编写复杂数据查询SQL的人，并且费时费力；</li><li>看到大盘数据后，依赖每个人对业务的理解提炼出一套分析报告，报告质量与个人的理解与表达能力相关。</li></ul><p>于是我就开始思考能否构建一个AI Agent，使其能够根据用户的要求，自主地生成各种各样的SQL查询语句，并将查询到的数据形成完整的数据分析报告返回给用户。</p><p>为了实现这个方案，有几个明显需要解决的点：</p><ul><li>如何让AI理解每个表中各字段的含义、各个表的作用、表与表之间的联系，从而生成准确的SQL？</li><li>AI生成完SQL之后，如何打通 AI 与数据平台之间的通路，从而成功执行该SQL 并拿到数据？因为数据库权限不在我这，我无法直接连接到数据库。</li><li>如何充分利用已有资源，减少人力投入？毕竟是个人想法，在不确定效果如何的情况下，不好直接打扰平台方专门为我写一些新功能，同时我个人也只能投入一些零碎的时间来做这件事。</li></ul><h2>三、方案</h2><p>有了问题后，就带着问题去找答案。</p><h3>3.1查询数据Tool</h3><p>首先，我需要一个能够执行查询的端点。那么我就去抓取了大盘中的数据所调用的接口，意外地发现，不同的数据调用的是同一个接口<a href="https://link.segmentfault.com/?enc=xnadqbIWb1Uya3hY1tkogQ%3D%3D.9VKiTJs%2FGryJj%2ByomC1ozoIOLk7C1BNX%2FmtYxWsG1nQ%3D" rel="nofollow" target="_blank">https://xxx.com/api/ds/query</a>，只是入参不同而已，而且发现，查询的逻辑是通过rawSql将查询语句直接传过去！</p><p>于是我将该Curl导入到ApiFox中，通过不断修改参数，发现最终与查询结果相关的入参可以精简到简单的几个参数：from、to、query(format,rawSql,intervalMs)</p><p><img width="723" height="415" referrerpolicy="no-referrer" src="/img/bVdnkfk" alt="" title="" loading="lazy"/></p><p><img width="643" height="438" referrerpolicy="no-referrer" src="/img/bVdnkfl" alt="" title="" loading="lazy"/></p><p>那么针对第一个问题我就想到了很好的办法，把这个查询API封装成一个Tool，描述清楚各个字段的含义，就可以让AI生成完整的参数来查询它想要的数据。</p><p><img width="723" height="394" referrerpolicy="no-referrer" src="/img/bVdnkfn" alt="" title="" loading="lazy"/></p><p>说干就干，我立马新建了一个Spring AI工程，把Tool的功能和需要的参数描述清楚。其中grafanaService.query()内部逻辑就是通过Feign来调用上面那个查询的API。</p><pre><code>@Tool(name = "query_grafana",
      description= "使用Grafana中的SQL查询grafana数据")
public JSONObject queryGrafana(@ToolParam(description = "查询开始时间") String from,
                               @ToolParam(description = "查询结束时间") String to,
                               @ToolParam(description = "查询数据类型:table|time_series") String format,
                               @ToolParam(description = "查询时间间隔,单位毫秒。只有当format为time_series时需要传入。") Long intervalMs,
                               @ToolParam(description = "Grafana SQL查询语句") String rawSql){
    DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd HH:mm:ss");
    LocalDateTime fromDateTime = LocalDateTime.parse(from, formatter);
    LocalDateTime toDateTime = LocalDateTime.parse(to, formatter);
    String fromTimestamp = String.valueOf(fromDateTime.toInstant(ZoneOffset.UTC).toEpochMilli());
    String toTimestamp = String.valueOf(toDateTime.toInstant(ZoneOffset.UTC).toEpochMilli());
    JSONObject resp = grafanaService.query(fromTimestamp, toTimestamp, intervalMs, rawSql, format);
    return resp;
}</code></pre><pre><code>@Resource
private GrafanaClient grafanaClient;


@Value("${grafana.cookie}")
private String getGrafanaCookie;


public JSONObject query(String fromTimestamp, String toTimestamp, Long intervalMs, String rawSql, String format) {
    GrafanaRequest request = new GrafanaRequest(fromTimestamp, toTimestamp, intervalMs, rawSql, format);
    return grafanaClient.query(getGrafanaCookie, request);
}</code></pre><h3>3.2表结构RAG</h3><p>有了能够执行查询的Tool之后，剩下的就是需要AI能够根据用户的query生成精准的参数以及查询SQL。</p><p>之前了解到公司部署了RAGFlow服务：<a href="https://link.segmentfault.com/?enc=HMiUG6cZcSevxH7W3JT7hA%3D%3D.hU1Nuod2w3kx1JzZD6c8ftTcXecHteVdO15mj%2FL7GVQ%3D" rel="nofollow" target="_blank">https://xxx.com/knowledge</a>，既然有了，那就得用起来！</p><ul><li>创建知识集，发现支持添加飞书文档。</li></ul><p><img width="723" height="512" referrerpolicy="no-referrer" src="/img/bVdnkfo" alt="" title="" loading="lazy"/></p><p><img width="723" height="209" referrerpolicy="no-referrer" src="/img/bVdnkfp" alt="" title="" loading="lazy"/></p><ul><li>由于我们是需要完整的表结构，所以把配置修改为使用table的格式，一行数据便是一个chunk，以免出现语义上的中断。（埋点数据一般表都较小，语意较为明确。像一些字段很多的大表可能需要考虑更好的方案。）</li></ul><p><img width="723" height="640" referrerpolicy="no-referrer" src="/img/bVdnkfq" alt="" title="" loading="lazy"/></p><ul><li>创建飞书文档，手动到OBS的库中把我们想要AI帮助分析的表结构拉出来（验证想法时采取的临时方案），但由于建表时的不规范，很多表没有对表中字段添加comment，这会导致AI不理解每个字段的含义，也就无法准确地生成SQL。因此，我们手动补充每张表、每个字段的描述，以及与其它表之间的关联关系。</li></ul><p><img width="723" height="165" referrerpolicy="no-referrer" src="/img/bVdnkfr" alt="" title="" loading="lazy"/></p><ul><li>将飞书文档添加到数据集中，完成后点击名称查看切片详情。双击每个块也可以查看块的详情。</li></ul><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnkfs" alt="" title="" loading="lazy"/></p><ul><li>会发现RAGFlow自动给我们生成了一些关键词和问题，这些内容会对召回准确率产生影响。我自己觉得生成的不太准确，所以结合自己理解手动输入了一些关键词和可能的问题。</li></ul><p><img width="723" height="1052" referrerpolicy="no-referrer" src="/img/bVdnkft" alt="" title="" loading="lazy"/></p><ul><li>完成后，可以到检索测试tab测试召回的效果，根据结果确定合适的参数。并可以对chunk的内容和关键词等等进行适当的调整。</li></ul><p><img width="723" height="421" referrerpolicy="no-referrer" src="/img/bVdnkfu" alt="" title="" loading="lazy"/></p><ul><li>调优完成后，就需要对接RAGFlow的retrive接口，来把我们知识库召回的流程做成一个Tool。</li></ul><p><img width="723" height="356" referrerpolicy="no-referrer" src="/img/bVdnkfw" alt="" title="" loading="lazy"/></p><pre><code>@Resource
private RagFlowService ragFlowService;


@Tool(name = "get_table_schema", description= "根据query查询可能有关联的数据库表，返回建表语句。尽量传入多个中文关键词，每个关键词之间用空格隔开。")
public List&lt;String&gt; getTableSchema(@Param("query") String question){
    return ragFlowService.retrieval(question);
}</code></pre><h3>3.3 OBS Agent</h3><p>在我看来，想要构建一个能够 work 的Agent，需要以下几个要素：</p><p><strong>Agent=Architecture</strong>(Workflow、ReAct、Plan-Execute、Multi-Agent...) <strong>+LLM+Context Engineering</strong>(Prompt、Tool、Memory...)</p><p>本来是想用SpringAI Alibaba Graph或者 LangGraph来构建一个WorkFlow类或者Graph类的复杂智能体（ReAct、Plan-Execute、Multi-Agent）。但为了快速验证想法和节省个人时间，并且考虑到目前任务相对简单（PE+工具就足以完成），再加上部门正在试用Trae这个工具，所以决定基于Trae来构建一个Agent（可以顺便使用他们的高级模型/doge，也可以分享给其它同事使用）。</p><p>接入Trae之后，Architecture自然就是Trae的Agent架构了，根据我使用下来感觉采用的是基于ReAct的 Single-Agent。而Context Engineering的部分，对话功能以及长短期记忆，自然是Trae天生就具备的。而Tool则可以借助其自带的一些工具，另外还可以利用MCP来进行扩展，比如得物的MCP市场，提供了大量好用的Server，并且可以很方便的发布自己开发的Mcp Server。于是，我就把在第一步和第二步做的工具，在得物Mcp平台上进行发布，供我自己和其他感兴趣的同学使用。</p><p>最后，需要一个专门针对我这个场景的Prompt来指引LLM 顺利完成任务，经过我不断的修改，最终形成这样一段Prompt:</p><pre><code># Role：数据分析专家


## Background：用户需要专业的数据分析支持来解决复杂的业务问题，从海量数据中提取有价值的信息，为产品优化、运营策略和业务决策提供可靠依据。


## Attention：数据准确性是分析工作的生命线，必须始终保持严谨细致的工作态度。每一次分析都可能影响重要决策，因此需要系统性思考、分步验证，确保每个环节的可靠性。


## Profile：
- Language: 中文
- Description: 专注于数据库表结构分析与Grafana-SQL查询的专业数据分析师，具备系统化解决复杂数据查询问题的能力


### Skills:
- 精通数据库表结构分析，能够快速识别表关系、字段含义和数据类型
- 熟练掌握Grafana-SQL语法规范，具备高效的查询语句编写和优化能力
- 具备专业的数据可视化技能，能够根据分析目标选择合适的图表类型
- 拥有深度业务需求理解能力，能够准确转化业务问题为数据查询方案
- 掌握系统化的问题分析方法，能够规划完整的数据分析流程和验证机制


## Goals:
- 准确理解用户业务需求，明确数据查询的核心目标和关键指标
- 系统分析相关表结构，确保对数据关系和业务逻辑的全面理解
- 设计高效的数据查询方案，平衡查询性能与结果准确性
- 生成专业的数据分析报告，包含可视化展示和深度业务洞察
- 确保所有分析过程可追溯、结果可验证、结论可执行


## Constrains:
- 查询不到数据时不要模拟任何数据，直接回复查不到数据
- 你自己所知道的时间是不准确的，如果涉及到时间，则需要使用工具获取当前时间
- 严格基于实际数据进行分析，严禁任何形式的数据虚构或推测
- 必须在完成表结构分析和需求理解后再执行具体查询操作
- 所有重要数据必须进行源头验证和多维度交叉检查
- 严格遵守数据安全和隐私保护原则，不超越授权数据范围
- 明确说明分析的局限性、假设条件和潜在的数据不确定性


## Workflow:
1. 深度理解业务需求，明确查询目标、关键指标和预期输出
2. 不断使用工具获取你需要的表及其表结构，直到你认为已获取到足够的信息
3. 系统分析相关表结构，包括字段含义、数据类型、关联关系和索引结构
4. 设计查询逻辑方案，规划执行步骤、验证节点和性能优化策略
4. 编写符合Grafana语法的SQL查询语句，设置正确的参数和时间范围
5. 执行查询。如果查询出现401错误，则中断后续流程，并提示用户更新Cookie后重启obs-mcp-server；如果出现400错误，尝试修改自己的SQL语句重新查询；
6. 生成可视化图表和详细分析报告，报告中必须包含你执行查询的SQL语句
7. 调用飞书生成文档工具以Markdown格式创建飞书文档，返回最终的飞书文档地址


## OutputFormat:
- 分析报告，包含完整的分析过程和关键发现，创建新的飞书文档并保存在其中
- 可视化图表以嵌入式链接形式呈现，确保清晰展示数据趋势和分布
- 报告结构包含执行摘要、分析方法、数据结果、业务洞察和后续建议


## Suggestions:
- 建立系统化的表结构分析框架，提高数据关系识别的效率和准确性
- 持续学习Grafana-SQL最新语法特性，优化查询性能和资源消耗
- 培养多维度数据验证习惯，确保分析结果的可靠性和业务价值
- 深入理解业务场景，提升从数据到洞察的转化能力和决策支持水平
- 定期复盘分析案例，总结经验教训，持续改进分析方法论和工作流程


## 工具描述
- query_grafana:使用Grafana中的SQL查询grafana数据
注意：
      1. 当format为time_series时表示查询时间序列数据，SELECT的第一个字段必须是$__timeGroupAlias(timestamp, interval)，表示时间分组别名。时间间隔intervalMs需要与rawSql中的$__timeGroup(timestamp, interval)保持对应。比如intervalMs=86400000L表示1天,rawSql中$__timeGroup(timestamp, 1d)也需要保持一致。
      2. 当format为table时表示查询表格数据，SELECT的字段可以任意，intervalMs参数传null
      3. 时间范围为闭区间，即包含开始时间from和结束时间to,格式为yyyy-MM-dd HH:mm:ss。
参数示例：{
    "from": "2025-11-16 00:00:00",
    "to": "2025-11-16 23:59:59",
    "format": "table",
    "intervalMs": null,
    "rawSql": "SELECT region, COUNT(*) as user_count FROM intl_xxxxxxx WHERE $__timeFilter(timestamp) GROUP BY region ORDER BY user_count DESC"
  }


## Initialization
作为数据分析专家，你必须遵守Constrains，使用默认中文与用户交流。</code></pre><p>最终，在 Trae 中构建了一个完整OBS Agent。</p><ul><li>添加智能体：OBS大盘分析</li></ul><p><img width="723" height="676" referrerpolicy="no-referrer" src="/img/bVdnkfx" alt="" title="" loading="lazy"/></p><h2>四、成果</h2><p><img width="723" height="1009" referrerpolicy="no-referrer" src="/img/bVdnkfy" alt="" title="" loading="lazy"/></p><p><img width="723" height="968" referrerpolicy="no-referrer" src="/img/bVdnkfA" alt="" title="" loading="lazy"/></p><p><img width="723" height="922" referrerpolicy="no-referrer" src="/img/bVdnkfB" alt="" title="" loading="lazy"/></p><p><img width="723" height="958" referrerpolicy="no-referrer" src="/img/bVdnkfC" alt="" title="" loading="lazy"/></p><p><strong>最终生成的报告（截取部分）：</strong></p><p><img width="723" height="806" referrerpolicy="no-referrer" src="/img/bVdnkfD" alt="" title="" loading="lazy"/></p><p><img width="723" height="913" referrerpolicy="no-referrer" src="/img/bVdnkfE" alt="" title="" loading="lazy"/></p><p><img width="723" height="1010" referrerpolicy="no-referrer" src="/img/bVdnkfF" alt="" title="" loading="lazy"/></p><h2>五、总结</h2><p>AI时代来临，我们应该要善于发现当前系统中的哪些部分能够结合AI来进行提升，积极拥抱变化，有了想法就去做，边做边想边解决问题，永远主动向前一步。</p><p>本文章只是记录了从产生想法到构建MVP验证想法的整个过程，这中间当然有很多可以继续优化的地方，我本人目前有以下几个想法，也欢迎大家积极评论，贡献自己的独到见解。</p><ul><li>接入数据库数据，通过动态监听Binlog的方式来识别各表之间的联系，比如select 语句的join，并将这种关系保存到Neo4j 这种图向量数据库中来实现表结构的 RAG。</li><li>基于LangGraph 或 SpringAI Alibaba 构建Multi-Agent System，细化各Agent的职责，精炼各Agent的Context 构成，以获得更好的效果。例如：协调者 Agent、表结构搜索 Agent、SQL 生成 Agent、分析报告 Agent等等。</li><li>接入飞书机器人，或者使用AI Coding工具生成一个前端页面。使得一些非技术人员，例如产品和运营也能很方便地使用。</li></ul><h3>往期回顾</h3><ol><li>数据库AI方向探索-MCP原理解析&amp;DB方向实战｜得物技术</li><li>项目性能优化实践：深入FMP算法原理探索｜得物技术</li><li>Dragonboat统一存储LogDB实现分析｜得物技术</li><li>从数字到版面：得物数据产品里数字格式化的那些事</li><li>一文解析得物自建 Redis 最新技术演进</li></ol><h3>文 /Neeson</h3><p>关注得物技术，每周更新技术干货</p><p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p><p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></description></item><item>    <title><![CDATA[一行代码实现智能异常检测：UModel PaaS API 架构设计与最佳实践 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047466086</link>    <guid>https://segmentfault.com/a/1190000047466086</guid>    <pubDate>2025-12-11 12:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：张鑫（千乘）</p><p>点击<a href="https://www.bilibili.com/video/BV1jS2VBJEzE/" target="_blank">此处</a>，查看视频演示！</p><p><strong>前文回顾：</strong></p><p>《<a href="https://link.segmentfault.com/?enc=5QGBJu26yJM%2BHTOMRAwfhw%3D%3D.UsorX%2FB1cO4bAb7DHWGqOJSF0hhCDVRawp3LOBHpx3gzxtMOeGgK1DpA4pHJ79%2Fglg2A%2BWYhusBrZExdc7PcxWerYIjFR1tZ3fcLIch7xvr%2FufSsjmDA7UtZcHWXciC2x%2FAyfZWzPsseqjJ4xA0gpzSGXyPoxppHDFXb3Wci88%2FBAZgzP3YxVI3zgG%2FQw1HL" rel="nofollow" target="_blank">基于 UModel 高效构建可观测场景统一实体搜索引擎</a>》</p><p>《<a href="https://link.segmentfault.com/?enc=iJwwXwqNO%2BTddpNorHuuFA%3D%3D.2TmGBZbXOorX0X0oJ%2Bz8NUEi1qDNGNS7jdeG1wrqwgNOxl2qZhrPHKvg5aIsiH4CdJLo060OD6yPRLRCxp81mPLGl6utdxoMqHaoQ%2Fbw3jPSTovyroY%2BWwbtEUJ%2FduE2m3%2BL42p0xGlLuLV%2FEksW1A32Aq6iYyak6zOK6ClrTbgrni%2BeGamWt9wqOxh0bg87" rel="nofollow" target="_blank">构建数据资产“导航地图”：详解 UModel 数据发现与全链路分析能力</a>》</p><p>《<a href="https://link.segmentfault.com/?enc=nvoycx6YGosvkVufJa8qGA%3D%3D.VuavZ%2FyhpgoKpRcziKHpxPXZrxym7%2FTFEvkbXDWNfYHMywgokjzyn%2BrcP8AxEcQrnofZ%2Fc2qyuRSf%2B%2FGEmJVqUIve3dcFej7xzrfDQzK2h8hB2z6Uufl5CJnG3LnyEjYFgp4lmCnPsD%2Fd3foTpNml6GWoyV3vfb1qG9VJr8x73M6BMDi8dahY3zSnyAb46Vr" rel="nofollow" target="_blank">打通可观测性的“任督二脉”：实体与关系的终极融合</a>》</p><h2>背景</h2><p>基于 UModel 构建的可观测系统，访问可观测数据需要上层应用感知 EntitySet、DataSet、Storage、Filter 等多个概念，给 UI、算法、客户等使用方带来了较高的开发和维护成本。</p><h3>典型场景：查询 APM 服务的请求量指标</h3><p>假设上层应用需要实现查询某个 APM 服务的请求量指标，开发者需要经历以下步骤：</p><h4>开发者需要了解的知识</h4><ol><li><strong>实体关联：</strong> 服务实体关联哪个 MetricSet？</li><li><strong>存储路由：</strong> MetricSet 使用哪个 MetricStore？Region/Project/存储名称是什么？</li><li><strong>字段映射：</strong> Entity 的 <code>service_id</code> 对应存储的哪个字段（如 <code>acs_arms_service_id</code>）？</li><li><strong>查询语法：</strong> 如何编写 PromQL 表达式 <code>rate(arms_app_requests_count_raw{...}[1m])</code>？</li><li><strong>SPL 拼接：</strong> 如何组装成完整的查询语句？</li></ol><h4>完整的开发步骤</h4><pre><code>Step 1: 查询 UModel 元数据
        ↓ 找到 service EntitySet 关联的 MetricSet
        ↓ 如果 DataLink 中包含 FilterByEntity，还需根据实体数据过滤
Step 2: 解析 MetricSet 配置
        ↓ 根据 StorageLink 获取底层 MetricStore 连接信息
        ↓ 获取 Region/Project/MetricStore 名称
Step 3: 查看字段映射
        ↓ 从 DataLink 中获取字段映射表
        ↓ 确认 service_id → acs_arms_service_id
Step 4: 构造 PromQL 表达式
        ↓ 根据指标定义拼接查询表达式
        ↓ 处理聚合规则、时间窗口
Step 5: 拼接并执行查询
        ↓ 使用正确的 label 和 MetricStore
        ↓ 拼接完整的 SPL 语句并执行</code></pre><p><strong>最终查询语句示例：</strong></p><pre><code>.metricstore with(region='cn-hangzhou', project='cms-xxx', metricstore='metricstore-apm')
|prom-call promql_query_range('sum by (acs_arms_service_id) (rate(arms_app_requests_count_raw{acs_arms_service_id="xxx"}[1m]))','1m')</code></pre><h3>痛点</h3><h4>痛点 1：概念复杂，学习门槛高</h4><p><strong>问题描述：</strong></p><ul><li>开发者必须深入理解 UModel 架构：EntitySet、DataSet、DataLink、StorageLink、Filter 等多个概念</li><li>需要了解 DataSet 与 Storage 的关联关系、Filter 路由逻辑、字段映射规则</li><li>新人上手困难，老手也容易遗漏细节</li></ul><p><strong>影响：</strong> 开发效率低，维护成本高</p><h4>痛点 2：复杂场景实现困难</h4><p><strong>问题描述：</strong></p><ul><li>存储路由查找：需要理解多个 MetricSet 之间的选择逻辑</li><li>字段映射处理：Entity 字段 → 存储字段的映射规则复杂</li><li>过滤条件筛选：FilterByEntity 规则匹配逻辑难以掌握</li><li>多次查询拼接：需要多次查询元数据，再构建数据查询</li></ul><p><strong>影响：</strong> 增加代码复杂度，出错概率高</p><h4>痛点 3：底层存储语法逃不掉</h4><p><strong>问题描述：</strong></p><ul><li>MetricSet 可能由 MetricStore 或 LogStore 实现，查询方式完全不同（PromQL vs SPL）</li><li>不同存储提供商（ARMS MetricStore、Aliyun Prometheus）语法有差异</li><li>开发者仍需精通底层查询语言</li></ul><p><strong>影响：</strong> 同样的需求需要编写不同的代码，无法统一</p><h4>痛点 4：多次查询交互，效率低</h4><p><strong>问题描述：</strong></p><ul><li>先查询 UModel Meta 获取配置 → 再根据 Meta 查询数据</li><li>需要自己处理数据拼接和关联</li><li>每个使用方都要实现类似逻辑，代码重复度高</li></ul><p><strong>影响：</strong> 集成成本高，查询延迟大，出错概率增加</p><h2>目标与架构</h2><h3>设计目标</h3><p>针对上述四大痛点，UModel PaaS API 的设计目标是屏蔽底层复杂性，统一访问接口，使上层应用更加专注业务逻辑实现：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466088" alt="image" title="image"/></p><h4>核心设计原则</h4><ul><li><strong>自动化处理：</strong> 自动路由、字段映射、查询转换</li><li><strong>统一 SPL 语法：</strong> 所有数据类型使用一致接口</li><li><strong>面向对象编程：</strong> 实体方法调用、关系导航</li><li><strong>AI 友好：</strong> 反射能力，支持 AI Agent 自主探索</li></ul><h3>设计理念：两层抽象</h3><p>访问 UModel 数据时，需要单独通过 SPL 去访问指标、日志、链路等各种数据，<strong>每种数据都有不同的访问方式，没有统一的抽象。</strong></p><p>UModel PaaS API 采用<strong>两层抽象</strong>的设计思路：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466089" alt="image" title="image" loading="lazy"/></p><h4>第一层抽象：Table 模式（表格化抽象）</h4><p>将所有数据——指标、日志、链路、性能剖析——统一抽象成<strong>表格结构</strong>，所有查询都是针对表格数据进行操作。</p><p><strong>价值：</strong> 统一了查询语言，开发者不需要关心底层是 PromQL 还是 SLS SPL，都用同一套 SPL 语法。</p><h4>第二层抽象：Object 模式（对象级抽象）</h4><p>表格模式解决了数据访问的统一性，但还不够。我们还需要<strong>以实体为中心</strong>的抽象。</p><p>传统方式：查询一个服务的指标，需要知道这个服务关联哪个 MetricSet、字段如何映射、过滤条件怎么写…</p><p>Object 模式：只需要说“这个服务，给我它的指标”，系统自动处理字段映射、过滤条件、存储路由。</p><p><strong>价值：</strong> 面向对象的思想，把实体当成对象，把查询当成方法调用：<code>service.get_metric()</code>。</p><h4>第三层能力：元数据查询（反射能力）</h4><p>提供动态能力发现、配置验证等高级功能，让 AI Agent 可以自主探索、自主决策。</p><p><strong>价值：</strong> AI Agent 能够通过反射能力动态发现实体的能力边界，实现真正的智能运维。</p><h3>架构分层</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466090" alt="image" title="image" loading="lazy"/></p><h4>1. 存储层统一：EntityStore/LogStore/MetricStore → SPL</h4><p>自动完成存储路由、字段映射（<code>service_id → acs_arms_service_id</code>）、过滤、查询语法的转换。上层应用对存储切换无感知。</p><h4>2. 数据层统一：Table 模式</h4><p>直接访问 DataSet，声明式查询，支持完整 SPL Pipeline。</p><pre><code>.metric_set with(domain='apm', name='service.request', query=`service_id='xxxx'`) | stats avg(latency)
.log_set with(domain='apm', name='service.error_log' query=`service_id='xxx'`) | where level="ERROR"</code></pre><h4>3. 对象层统一：Object 模式</h4><p>以实体为中心，自动处理底层细节，支持动态能力发现和配置检查。</p><pre><code># 数据访问
.entity_set with(domain='apm', name='apm.service', ids=['404e5d6be468f6dfaeef37a014322423']) 
| entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range', '', false) 
# 能力发现（Agent 自主决策的关键）
.entity_set with(domain='apm', name='service') | entity-call __list_method__()
# 配置检查
.entity_set with(domain='apm', name='service') | entity-call __inspect__()</code></pre><h2>API 说明</h2><p>UModel PaaS API 提供三大核心能力，满足不同场景的查询需求：</p><ol><li><strong>Table 模式</strong> - 直接访问数据集，适合批量数据分析</li><li><strong>Object 模式</strong> - 以实体为中心，适合实体详情查询和关系分析</li><li><strong>元数据查询</strong> - 反射能力和配置验证，支持 AI Agent 和开发调试</li></ol><h3>Table 模式</h3><p>Table 模式（Phase 1）提供直接访问 DataSet（MetricSet、LogSet、TraceSet 等）的能力，返回表格化的可观测数据，适用于不依赖实体关系的数据查询场景。</p><p>如：直接查询某个 MetricSet 中的指标数据，或查询某个 LogSet 中的日志，无需关联实体信息。</p><pre><code># 读取 apm.metric.apm.service MetricSet对应的avg_request_latency_seconds的指标，
# 并对该指标进行异常检测
.metric_set with(domain='apm', name='apm.metric.apm.service', metric='avg_request_latency_seconds', source='metrics')
| extend r = series_decompose_anomalies(__value__) 
| extend anomaly_b =r.anomalies_score_series , anomaly_type = r.anomalies_type_series , __anomaly_msg__ = r.error_msg  
| extend x = zip(anomaly_b, __ts__, anomaly_type, __value__) 
| extend __anomaly_rst__ = filter(x, x-&gt; x.field0 &gt; 0) 
| project __entity_id__, __labels__, __anomaly_rst__, __anomaly_msg__</code></pre><p><strong>核心特点：</strong></p><ul><li>直接访问：直达 DataSet，无需查询实体元数据</li><li>语法简洁：类似 SQL 的 SPL 语法，易于理解</li><li>全量数据：返回 DataSet 中符合条件的所有数据</li></ul><p><strong>语法：</strong> <code>.&lt;type&gt; with(domain, name, ...) | &lt;SPL Pipeline&gt;</code>，更多参数说明请参考文档：Phase 1 Table 模式 <strong>[</strong> <strong>1]</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466091" alt="image" title="image" loading="lazy"/></p><h3>Object 模式</h3><p>Object 模式（Phase 2）提供<strong>以实体为中心的面向对象查询能力</strong>，自动处理实体与数据的关联关系、字段映射、关系查询等复杂逻辑，适用于需要实体上下文的业务场景。</p><p>如：查询某个具体服务的指标、日志、链路数据，或查询与该服务有调用关系的其他服务，系统自动完成字段映射和数据过滤。</p><pre><code># 查询特定服务的请求延迟指标，自动处理字段映射和 FilterByEntity
.entity_set with(domain='apm', name='apm.service', ids=['21d5ed421ae93973d67a04af551b48b8']) 
| entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range', '30s', false)
| project __entity_id__, __ts__, __value__, __labels__</code></pre><p><strong>核心优势：</strong></p><ul><li>零配置过滤：自动处理 FilterByEntity，无需手动拼接过滤条件</li><li>字段映射透明：自动转换 <code>service_id → acs_arms_service_id</code> 等映射</li><li>面向对象语义：<code>entity.get_metric()</code>，符合开发者思维习惯</li></ul><p><strong>语法：</strong> <code>.entity_set with(domain, name, id, query) | entity-call &lt;方法&gt;(&lt;参数&gt;) | &lt;SPL pipeline&gt;</code>，更多参数说明请参考文档：Phase 2 Object 模式 <strong>[</strong> <strong>2]</strong> 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466092" alt="image" title="image" loading="lazy"/></p><h3>元数据查询方法</h3><p>元数据查询方法提供动态发现和反射能力，用于查询实体的关联关系、数据集配置、支持的方法等元数据信息，既可以帮助开发者理解实体能力，也是实现 AI Agent 自主决策和配置验证的关键基础。</p><p>如：查询某个服务实体支持哪些方法（<code>__list_method__()</code>）、关联了哪些数据集（<code>list_data_set()</code>）、与哪些其他服务有调用关系（<code>list_related_entity_set()</code>）、配置是否正确（<code>__inspect__()</code>）。</p><pre><code># 动态发现实体支持的所有方法（反射能力）
.entity_set with(domain='apm', name='apm.service') 
| entity-call __list_method__()
# 返回：方法列表及参数定义
# [
#   {"name": "get_metric", "params": [...], "description": "获取指标数据"},
#   {"name": "list_related_entity_set", "params": [...], "description": "查询关联实体"},
#   ...
# ]</code></pre><p><strong>核心价值：</strong></p><ul><li>反射能力：<code>__list_method__()</code> 让 AI Agent 能自主探索实体的能力边界</li><li>配置验证：<code>__inspect__()</code> 一键检查 DataSet、Link、字段映射等配置完整性</li><li>关系查询：<code>list_related_entity_set()</code> 快速获取拓扑关系，无需查询图数据库</li><li>能力发现：<code>list_data_set()</code> 了解实体关联的所有观测数据类型</li></ul><p><strong>语法：</strong> <code>.entity_set with(domain, name, id, query) | entity-call &lt;方法&gt;(&lt;参数&gt;)</code>，更多参数说明请参考文档：Phase 2 Object 模式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466093" alt="image" title="image" loading="lazy"/></p><h2>查询方式</h2><h3>UI 方式</h3><p>登录云监控 2.0 控制台，点击实体探索 -&gt; SPL，输入 SPL，如下图所示：</p><p><code>.entity\_set with(domain='apm', name='apm.service', ids=['21d5ed421ae93973d67a04af551b48b8']) | entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range', '', false)</code></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466094" alt="image" title="image" loading="lazy"/></p><h4>DryRun 模式</h4><p>DryRun 模式返回对应的 Query，不执行当前 Query，也支持手动设置运行模式。</p><pre><code># 开启dry_run模式
.set umodel_paas_mode='dry_run';
.entity_set with(domain='apm', name='apm.service') 
| entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range', '', false) </code></pre><p>UI 开启 DryRun 模式：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466095" alt="image" title="image" loading="lazy"/></p><h3>SDK 方式</h3><p>通过阿里云 OpenAPI <strong>[</strong> <strong>3]</strong> 下载 SDK，代码如下：</p><pre><code>package main
import (
"fmt"
    cms20240330 "github.com/alibabacloud-go/cms-20240330/v3/client"
    openapi "github.com/alibabacloud-go/darabonba-openapi/v2/client"
"github.com/alibabacloud-go/tea/tea"
    credential "github.com/aliyun/credentials-go/credentials"
"os"
)
func CreateClient() (_result *cms20240330.Client, _err error) {
    credential, _err := credential.NewCredential(nil)
if _err != nil {
return _result, _err
    }
    config := &amp;openapi.Config{
        Credential: credential,
    }
    config.Endpoint = tea.String("cms.cn-hangzhou.aliyuncs.com")
    _result = &amp;cms20240330.Client{}
    _result, _err = cms20240330.NewClient(config)
return _result, _err
}
func _main(args [ ]*string) (_err error) {
    client, _err := CreateClient()
if _err != nil {
return _err
    }
    getEntityStoreDataRequest := &amp;cms20240330.GetEntityStoreDataRequest{
        Query: tea.String(".entity_set with(domain='apm', name='apm.service', ids=['21d5ed421ae93973d67a04af551b48b8']) | entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range') "),
        From:  tea.Int32(1762244123),
        To:    tea.Int32(1762244724),
    }
if result, err := client.GetEntityStoreData(tea.String("o11y-demo-cn-hangzhou"), getEntityStoreDataRequest); err != nil {
return err
    } else {
        fmt.Printf("length: %d", len(result.Body.Data))
return nil
    }
}
func main() {
    err := _main(tea.StringSlice(os.Args[1:]))
if err != nil {
panic(err)
    }
}</code></pre><p>参数说明：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466096" alt="image" title="image" loading="lazy"/></p><p><strong>程序运行</strong></p><pre><code>go build -o demo .
export ALIBABA_CLOUD_ACCESS_KEY_SECRET=&lt;YOUR_ACCESS_SECRET&gt;
export ALIBABA_CLOUD_ACCESS_KEY_ID=&lt;YOUR_ACCESS_KEY_ID&gt;
./demo</code></pre><h2>示例</h2><h3>集成算子实现高阶能力：UModel 高阶查询  + 时序异常检测算子</h3><p>通过 UModel 高阶 API 集成 SLS 时序异常检测算子 <code>series_decompose_anomalies</code>，一行查询实现智能异常检测。</p><p>如：监控某个 APM 服务的请求延迟，当出现异常（突刺、趋势变化、平台变化）时触发告警。</p><pre><code>.entity_set with(domain='apm', name='apm.service', ids=['21d5ed421ae93973d67a04af551b48b8']) 
| entity-call get_metric('apm', 'apm.metric.apm.service', 'avg_request_latency_seconds', 'range', '30s', false) 
| extend r = series_decompose_anomalies(__value__) 
| extend anomaly_b =r.anomalies_score_series , anomaly_type = r.anomalies_type_series , __anomaly_msg__ = r.error_msg  
| extend x = zip(anomaly_b, __ts__, anomaly_type, __value__) 
| extend __anomaly_rst__ = filter(x, x-&gt; x.field0 &gt; 0) 
| project __entity_id__, __labels__, __anomaly_rst__, __anomaly_msg__</code></pre><p><strong>返回结果</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466097" alt="image" title="image" loading="lazy"/></p><p><strong>支持的异常类型：</strong></p><ul><li><code>SPIKE_UP / SPIKE_DOWN</code> - 向上/向下突刺</li><li><code>TREND_SHIFT_UP</code> / <code>TREND_SHIFT_DOWN</code> - 趋势上升/下降</li><li><code>LEVEL_SHIFT_UP</code> / <code>LEVEL_SHIFT_DOWN</code> - 平台上升/下降</li></ul><p>如下图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466098" alt="image" title="image" loading="lazy"/></p><h3>数据互联互通：关联自定义 LogStore</h3><p>在实际生产环境中，业务数据往往分散在多个存储中。例如：</p><ul><li>UModel 中存储了 APM 服务的拓扑关系、指标、链路、日志</li><li>业务系统的自定义日志存储在独立的 LogStore 中（如订单日志、支付日志、用户行为日志）</li></ul><p>通过 UModel 高阶 API + SPL join 能力，可以<strong>打通 UModel 实体数据与自定义业务数据</strong>，实现：</p><ol><li><strong>统一视角分析：</strong> 将应用性能问题与业务日志关联分析</li><li><strong>快速定位问题：</strong> 从服务异常快速定位到具体业务操作</li><li><strong>端到端追踪：</strong> 从业务请求到技术指标的全链路分析</li></ol><p><strong>典型场景：</strong></p><ul><li>某个 APM 服务出现延迟异常 → 关联业务订单日志 → 定位到具体慢查询的订单 ID</li><li>某个服务的错误日志激增 → 关联用户行为日志 → 分析是哪些用户操作触发了异常</li><li>分析服务调用链路 → 关联业务流程日志 → 追踪完整的业务流转路径</li></ul><p><strong>示例：</strong></p><pre><code># 场景：关联自定义的logstore日志信息
# SPL: 
# 1. 从业务LogStore中找到失败的traceId以及msg
.let failed_log = .logstore with(project=‘xxx’, logstore=‘xxxx’, query=‘*') 
                     | project trace_id, msg;
# 2. 查询服务的Trace数据
.let service_traces = .entity_set with(domain='apm', name='apm.service', ids=['xxxx']) 
                       | entity-call get_trace(‘apm‘, ’apm.trace.common’);
$failed_log | join $service_traces on trace_id = $service_traces.traceId |  project msg</code></pre><h3>集成 AI Agent：通过反射能力实现自主决策</h3><p>将 UModel PaaS API 封装为 MCP Tools <strong>[</strong> <strong>4]</strong> ，通过反射能力（<code>__list_method__()</code>）让 AI Agent 具备自主探索和决策能力，实现智能运维分析。</p><p>如：用户问“为什么服务响应慢？”，Agent 通过动态发现可用方法，自主完成根因分析。</p><pre><code># Agent 首先调用 __list_method__() 动态发现实体支持的方法
.entity_set with(domain='apm', name='apm.service') 
| entity-call __list_method__()
# 返回示例（Agent 根据返回的方法列表自主决策下一步操作）:
# {
#   "methods": [
#     {"name": "get_metric", "params": [...], "description": "获取指标数据"},
#     {"name": "get_log", "params": [...], "description": "获取日志数据"},
#     {"name": "get_trace", "params": [...], "description": "获取链路数据"},
#     {"name": "list_related_entity_set", "params": [...], "description": "查询关联实体"}
#   ]
# }</code></pre><p>演示 Demo：</p><p>点击<a href="https://link.segmentfault.com/?enc=3nwZZVHaypRAdUpD%2FuDqQQ%3D%3D.oKvnv%2FE06gA4ODiDCdddQqwoll3a4%2B1st4J4i7S61ENoPHdQUBXpeXGWtLLLN9HR%2FQCTEoVNMyFWydcy1kO7pw%3D%3D" rel="nofollow" target="_blank">此处</a>，查看Demo演示！</p><p><strong>相关链接：</strong></p><p>[1] Phase 1 Table 模式</p><p><a href="https://link.segmentfault.com/?enc=L530Yux04AiL7q5i4QKU4g%3D%3D.i%2Fmr7XrhJYJgEuGJNYfpeMqqBYAvl%2Fq3iHurqp7sBUjt%2B4sCiyO0dznIWWJ8%2F9DxLkJtstGjlK0haeHkkpYkGL8rxkaoSotui666cMgsnXA%3D" rel="nofollow" target="_blank">https://help.aliyun.com/zh/cms/cloudmonitor-2-0/phase-1-table...</a></p><p>[2] Phase 2 Object 模式</p><p><a href="https://link.segmentfault.com/?enc=1g8ElCoOva0LxMkEqVjUCg%3D%3D.V00unIAyApMEixu7sClIHqJ2pM1T4HqAMVDeNea2LfqfmHvvuUHSt6kpP8FLbeETEmb8iYFRBnWWOA4Lcm57KuwbrOO7CzrhbHUP8C4VoMkrQR0TIYR5uINRR24nar1m" rel="nofollow" target="_blank">https://help.aliyun.com/zh/cms/cloudmonitor-2-0/phase-2-objec...</a></p><p>[3] 阿里云 OpenAPI</p><p><a href="https://link.segmentfault.com/?enc=sJWFyO%2FZs6Xnp%2BrdzbYFkQ%3D%3D.4a%2BJoY9lqRBN6X9yeAzYgbM%2FmQ%2F4RpSe8qUg3s5LXe42MFN2lCt52CsaPaj4SCP8dia%2FRBsxV8kpT58YQfhheA%3D%3D" rel="nofollow" target="_blank">https://api.aliyun.com/api/Cms/2024-03-30/GetEntityStoreData</a></p><p>[4] MCP Tools</p><p><a href="https://link.segmentfault.com/?enc=4Imcun61p4LJnD4blvShYA%3D%3D.5wQ5F6XJBRIKhGQQ4OtJciMXbUdJRkuSc9f6lX3fhC7y6UotWZXS2l9YF6A2RsoviCfhwhu%2BT90k36MUIk%2FJAA%3D%3D" rel="nofollow" target="_blank">https://modelcontextprotocol.io/docs/getting-started/intro</a></p><p>点击<a href="https://www.bilibili.com/video/BV1jS2VBJEzE/" target="_blank">此处</a>，查看视频演示！</p>]]></description></item><item>    <title><![CDATA[硬核技术+重磅大奖齐登场！龙蜥社区走进 Arm MeetUp 火热报名中 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047466117</link>    <guid>https://segmentfault.com/a/1190000047466117</guid>    <pubDate>2025-12-11 12:02:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>「龙蜥社区“走进系列”MeetUp」是由龙蜥社区与生态合作伙伴联合主办的系列月度活动，每期走进一家企业，聚焦龙蜥社区和合作伙伴的技术、产品和创新动态，展示硬核技术，共建繁荣生态。</p><p>龙蜥社区“走进系列”MeetUp 之走进 Arm MeetUp 将于 12 月 11 日在上海举办一场深度技术研讨会，聚焦“共筑 AI 时代开源 OS 新生态”主题，将围绕龙蜥操作系统与 Arm Neoverse 平台，全面展示 AI 进化的全链路实践。本次 MeetUp 不仅有干货满满的技术分享，更有重磅礼品 AirPods Pro 3、人体工学护腰座椅、腰部按摩器等大奖等你来拿。入门即有礼哦，欢迎报名：<a href="https://link.segmentfault.com/?enc=p8%2FZYLJl7JxruiC8Gekveg%3D%3D.8P6YsgvlISoU1AVcq8yy8O5FMAixVfK7r71aTpLF%2BI0r3NCrAzsHT0n4TiA64%2FxY" rel="nofollow" target="_blank">https://openanolis.mikecrm.com/3sbZgtt</a></p><h3>本次 MeetUp 演讲亮点一览：</h3><h4>主题：Arm 基础设施加速云计算智能驾驶</h4><p>嘉宾：侯科鑫，安谋科技云人工智能事业部总监</p><p>张先国，阿里云智能集团高级弹性计算架构师</p><p>简介：人工智能的快速发展正在加速基础设施技术的重大转型。Arm Neoverse 已成为这一转型中多个关键领域的首选平台。本议题将详细介绍最新 Neoverse CSS 产品的能力，以及这些创新如何帮助 Arm 的合作伙伴在 AI 时代加速其创新产品的上市进程。具体到智能驾驶领域，阿里云与客户紧密合作，在智能驾驶的 AI Infra 产品和方案层面，基于 Yitian710 做了大量技术创新，形成了高性能、高可靠、生态完善的智能驾驶解决方案。</p><h4>主题：RTP-LLM: Arm 平台全面支持</h4><p>嘉宾：方方明，安谋科技主任软件工程师</p><p>简介：基于 Arm CPU，我们实现了对 RTP-LLM（阿里巴巴的大模型推理引擎）的全面支持，包括 Arm CPU 后端实现；各种算子的实现；GPTQ 量化格式的支持；KleidiAI 的集成以及混合专家（MoE）模型的支持。我们使用Arm加速指令，INT4 量化技术等来提升 RTP-LLM 在 Arm Neoverse 平台上的推理性能。</p><h4>主题：阿里巴巴 Dragonwell JDK : 为 AI 时代而生</h4><p>嘉宾：邢其正，阿里云智能集团高级 JVM 工程师</p><p>简介：AI 时代，Java 开始承担越来越多 AI 相关的负载，传统 JVM 在高性能计算、智能调优、AIOps 等方面正面临严峻挑战。阿里云重磅推出的 Alibaba Dragonwell 21 AI 增强版，通过 Native 加速、热代码重排、JTune 三大核心技术，让 Java 应用在 RAG、大数据、智驾等复杂场景实现了性能飞跃。本次演讲将带您深入这一为 AI 而生的 JVM，揭秘如何在不改一行代码的前提下，释放 AI 时代 Java 的极致潜能。</p><h4>主题：ModelSight：端到端 AI 性能分析框架</h4><p>嘉宾：常怀鑫，阿里云智能集团技术专家</p><p>王鹏：龙蜥社区智算基础设施联盟委员</p><p>简介：ModelSight 是我们在龙蜥社区自研的 AI 性能分析工具，通过 eBPF + 全栈采集实现 GPU、CPU、框架事件一体化观测。本次议题将分享如何利用 ModelSight 对 235B 参数的 Qwen3 推理链路进行线上压测、热点定位与瓶颈可视化，并结合 PD 分离、TP/PP/EP 并行策略在 SGLang 推理框架中的落地，给出 2.12× token/s 提升优化结果。</p><h4>主题：鸿钧微电子开源社区实践</h4><p>嘉宾：吴喆，鸿钧微电子产品营销经理</p><p>简介：拥抱开源社区，服务开源社区，基于高效能 Arm 架构服务器 CPU，鸿钧微电子实践之路。</p><h4>主题：KTransformers: 在Arm CPU上实现大模型异构推理</h4><p>嘉宾：袁子为，趋境科技技术专家、KTransformers 核心开发人</p><p>简介：KTransformers 是一个专注于大语言模型高效推理和微调的研究项目，通过 CPU-GPU 异构计算实现资源受限环境下的大模型部署，实现 Arm CPU+GPU 平台下的本地极致推理和个性化的微调尝试。</p><h4>主题：Llama.cpp 跨 NUMA 节点部署优化实践</h4><p>嘉宾：刘亮亮，安谋科技主任软件工程师</p><p>简介：介绍 llama.cpp 中跨 NUMA 节点的性能问题和优化方案，以及采用优化方案后带来的性能提升。</p><p>更多详细议程见下方海报：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466119" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[2025年11月国产数据库大事记：OceanBase发布首款AI数据库seekdb，中国电信开源Te]]></title>    <link>https://segmentfault.com/a/1190000047466124</link>    <guid>https://segmentfault.com/a/1190000047466124</guid>    <pubDate>2025-12-11 12:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=lRNYgzsDeOC2KAyNFBXEkQ%3D%3D.nja2T7FvesZx%2Blej0PHo16b1w8brZJZgva%2Fz6c0kzzo%3D" rel="nofollow" target="_blank">墨天轮社区</a>整理的2025年11月国产数据库大事件和重要产品发布消息。</p><blockquote>IDC报告显示，2025上半年中国关系型数据库软件市场规模为22.1亿美元，同比增长14.5%，市场规模前四名分别是阿里云、腾讯、华为、AWS；中国电信开源TeleDB推出 openTeleDB；OceanBase入选福布斯中国“出海全球化领军品牌 TOP 30”，发布首款AI数据库seekdb；openGauss Docker下载量突破10万；Apache Doris刷新 JSONBench 性能纪录……</blockquote><h3>目录</h3><ul><li><a href="https://link.segmentfault.com/?enc=NE3TD3v5Rce7uePcmtLtMw%3D%3D.vQlV6a1lSsKavCfAH1HbEmo8zoKyNDfQoN2Bz9gwuKcgeua9ZNO4SpDqVVELxPXB1OHztWqPFjYH6DaxhtI%2FhCoCNqlj7Zr2n2Q%2BBQeUbOGYIvzBSkLqFfyNTalXoOm9" rel="nofollow" target="_blank">11月国产数据库大事记 TOP10</a></li><li><a href="https://link.segmentfault.com/?enc=UwPbl2xDmUDf%2BGeRxxks7A%3D%3D.Hd2FAtK2LxNefUwdGP5KcOUODjZS5fZfd2XwTLhwmvWr29L2jPxu2v5nzD4cWMgUPl7MlEJ%2FSVeW%2FxfUylRHWTD%2BhUH3Yd3hS2HwlO57tno%3D" rel="nofollow" target="_blank">11月大事记时间线</a></li><li><a href="https://link.segmentfault.com/?enc=iHcn8POkExWj17sSsaWsxQ%3D%3D.dopYI%2Bvfb22Pn3K3gAqQdRIhS64TM3MeOzWi1vLMrZ15RvnejPWHB9WQAehUMiGsYFnIs7diqSvFvenlehGDbZYkXqUj6OY5C4IRAX%2Fgl8g%3D" rel="nofollow" target="_blank">11月产品/版本发布</a></li><li><a href="https://link.segmentfault.com/?enc=lZzslHMp9h7bcyWE6MS%2Faw%3D%3D.UHmLSh%2FY0gY0LNG1VDsFEZQraI1qzAu398C7XrqgS9DcdjCee%2BVhdgmZouGazw1inzDnwi9svcs6dHIpsnIlqksjViOrsXmof8OxF4NmSE8%3D" rel="nofollow" target="_blank">11月代表厂商大事记</a></li><li><a href="https://link.segmentfault.com/?enc=VUgcxXKoqF2S77lvsMwFYg%3D%3D.KoSMJX%2BChur1cBpd2nVnuFExtvCs1WbmC8zL7Qrx1114WKXp8wt8PQy39RFVw2DVwF16hkbp0gHmyviqidZof8vWNCa0Ea9c%2BQ8bX9JXfZE%3D" rel="nofollow" target="_blank">相关资料</a></li></ul><h2>11月国产数据库大事记 TOP10</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466126" alt="" title=""/></p><h2>11月大事记时间线</h2><p><strong>中国电信李跃森宣布：TeleDB数据库开源推出 openTeleDB</strong></p><p>11月1日，中国电信在GOTC全球开源技术峰会上宣布将其核心数据库产品TeleDB以openTeleDB之名开源，OpenTeleDB 成为了“全球首个运营商级开源OLTP数据库”。该数据库基于PostgreSQL 17深度定制，针对并发连接、存储膨胀和高可用等痛点新增了XProxy、XStore、XRaft三大能力，并采用木兰宽松许可证v2在Gitee上发布。openTeleDB凭借中国电信十年支撑数万内部系统的经验，旨在打造全球首个运营商级开源OLTP数据库，为企业提供稳定高效、免复杂运维的下一代解决方案。</p><blockquote>OpenTeleDB数据库开源社区：<a href="https://link.segmentfault.com/?enc=u7bmI07OVXbZU42SS11O4A%3D%3D.sBoP%2BWsFa1nQgT%2B83WQtTTKeIWiGFaaJl00rWpkoSMUCGjB47KqdcettEwp4tqWs" rel="nofollow" target="_blank">https://openteledb.ctyun.cn/open/index</a></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466127" alt="" title="" loading="lazy"/></p><p><strong>冷查第一，再登榜首！Apache Doris 3.1 全面刷新 JSONBench 性能纪录</strong></p><p>11月3日消息，Apache Doris 3.1 版本通过对 Variant 引擎进行稀疏子列、子列模板化、列裁剪与路径索引等系统性优化，在最新的 JSONBench 基准测试中刷新性能纪录：冷查询性能以 164 倍和 1074 倍的优势分别碾压 MongoDB 和 PostgreSQL，位居榜首；热查询也获得第二，综合性能全面领先 ClickHouse、Elasticsearch 等竞品，成为半结构化数据分析领域更具竞争力的选择。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466128" alt="" title="" loading="lazy"/></p><p><strong>OceanBase 入选福布斯中国“出海全球化领军品牌 TOP 30”</strong></p><p>11月4日，福布斯中国公布“2025 福布斯中国出海全球化 30&amp;30 系列评选”结果，OceanBase 入选“出海全球化领军品牌 TOP 30”，成为该榜中唯一的数据库企业。OceanBase凭借完全自主研发的一体化分布式数据库技术、全球服务布局及数千家海内外客户案例入选。此次入选榜单是对OceanBase全球化能力及行业引领的高度认可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466129" alt="" title="" loading="lazy"/></p><blockquote>目前，OceanBase的云数据库服务OB Cloud覆盖全球50多个地理区域的170个可用区，支持七大公有云基础设施。在海外市场，OceanBase已在金融科技、互联网、电商等领域构建全场景解决方案，助力多个海外项目落地。例如，印尼电子钱包 DANA 依托 OceanBase 实现 7×24 小时零中断运行，存储成本降低 70%；菲律宾 GCash、非洲 PalmPay 完成核心账务系统升级后，交易峰值承载提升 3 倍，PalmPay 更实现核心系统降本 86%；在中东，OceanBase 助力沙特 SCCC、伊拉克 QiCard 等本地金融科技项目落地，推动区域金融基础设施升级。今年 8 月，OceanBase 再次实现“技术出海”，完成了首个海外银行——老中银行的核心业务系统升级。</blockquote><p><strong>第一新声发布“2025年Q3中国数据库最活跃厂商榜单，金篆信科位居榜首</strong></p><p>11月4日，第一新声智库发布“2025年Q3中国数据库最活跃厂商榜单”，金篆信科、奥星贝斯，腾讯云位列前三。第一新声智库经过调研分析发现，<strong>2025年前三季度数据库市场规模已达到436亿元</strong>。在通过国测的18家厂商中，资源与订单正加速向少数几家领跑者集中。据第一新声智库不完全统计，2025年第三季度数据库公开中标项目共185个，其中金融行业以86个公开中标项目的绝对优势主导市场，五百万级以上大单也多集中于金融行业。如<strong>GoldenDB</strong>中标长沙银行1107万元、云南农信2687万元；腾讯云<strong>TDSQL</strong>中标东莞银行总行821万元、秦皇岛银行694万元；<strong>OceanBase</strong>中标东莞银行1078万元。电信行业公开中标项目共24个，代表性项目如<strong>电科金仓</strong>中标中国移动甘肃公司828万元。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466130" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466131" alt="" title="" loading="lazy"/></p><p><strong>中国人民大学×电科金仓 | 数据库协同创新联合实验室揭牌仪式举行</strong></p><p>11月5日，中国人民大学—电科金仓数据库协同创新联合实验室揭牌仪式在中国人民大学中关村校区隆重举行。联合实验室将融合人大顶尖基础理论研究优势与电科金仓二十多年产业化经验，聚焦AI 与数据库融合、分布式处理等前沿方向，打造集科研攻关、人才培养、成果转化于一体的高水平平台，成为技术创新的“策源地”和赋能千行百业的“转化器”，为国产数据库从“可用”迈向“好用”提供坚实支撑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466132" alt="" title="" loading="lazy"/></p><p><strong>Apache Cloudberry 加入 Apache 孵化器一周年</strong></p><p>11月5日 ，Cloudberry 关联仓库正式迁移到 Apache GitHub 组织。距离2024年10月12日Cloudberry 正式加入 Apache 孵化器已有1年，过去一年项目取得显著进展：完成Greenplum归档前代码同步、推动PostgreSQL内核从14升级到16、推出行列混合存储引擎PAX、优化性能与可用性、支持流/实时计算、完善工具和生态，并发布了首个Apache版本2.0。</p><p><strong>openGauss Docker 下载量突破 10 万，荣获 2025 GitCode 百大开源项目</strong></p><p>11月7日消息，近日，openGauss 社区再传喜讯：openGauss Docker 镜像下载量正式突破 10 万次，同时荣获 “2025 GitCode 百大开源项目”。这两项重要成果，不仅体现了 openGauss 在技术创新与生态建设上的深厚实力，也彰显了其在开源数据库领域的持续影响力。今年，社区版本下载量从 24 年底的 360 万增长到 450 万，涨幅超过 25%。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466133" alt="" title="" loading="lazy"/></p><blockquote>自2020年开源以来，openGauss实现跨越式发展。技术创新上，推出内核四高特性及oGRAC多写架构等；生态建设上，吸引了全球881家企业和高校、8374名开发者参与贡献，构建了全栈生态；商业化方面，实现了包括中移动全域核心替换、民生银行和兴业银行核心系统规模上线等突破。随着 openGauss 7.0.0-RC1 创新版的推出，DataVec 向量数据库能力支持 RAG 解决方案，在 AI 时代浪潮下迎来了新的突破。</blockquote><p><strong>达梦实现国网工控领域规模化部署，斩获市场占有率第一</strong></p><p>11月6日消息，近日，达梦数据完成“国网智慧计量工控项目”在山西、重庆、天津、辽宁、福建等15个国家电网省级电力公司成功部署运行，斩获国产数据库在工控系统部署率第一的佳绩。该平台承担计量设备控制、监测与调度职能，每秒数以亿计的数据洪流，对数据处理要求极高。</p><blockquote>达梦自2008年起与国家电网合作，目前已服务国家电网17年，深入国家电网多个核心业务系统，在调控云、智能电网调度控制系统、电量系统、配电系统、电力市场系统等多个领域市场占有率第一。</blockquote><p><strong>达梦支撑国内首个期货核心单轨运行</strong></p><p>11月8日消息，国内首个全栈信创主用交易系统——DCEX-One期货主用交易系统在山西三立期货上线并单轨运行，达梦数据库作为核心数据底座，采用DM8、DMDatawatch和DMDRS方案支撑系统运行，实现了全栈自主、高并发低延时和高稳定性，为金融交易筑牢安全防线，推动金融行业国产化进程。</p><p><strong>万里数据库三连中标中国移动数据库采购项目</strong></p><p>11月11日消息，近日，万里数据库在中国移动连续斩获三大项目：杭研院、北京移动、湖北移动相继选择万里数据库作为其自主可控数据库产品提供商。这一系列中标不仅是市场对万里数据库产品实力的高度认可，也进一步巩固了公司在运营商核心系统建设中的关键地位。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466134" alt="" title="" loading="lazy"/></p><p><strong>阿里云联合西安公路研究院等单位，发布PolarDB超融合数据库一体机解决方案</strong></p><p>11月13日消息，阿里云联合西安公路研究院等单位发布了PolarDB超融合数据库一体机解决方案，旨在推动交通行业智能化升级。该方案具有“开箱即用、全栈适配、稳定高可用”的核心优势，可将收费站改造周期从数月缩短至天级，适配周期大幅压缩。它提供轻量版和标准版两种型号，精准匹配不同场景需求，试点数据显示可提升通行效率33%，减少对账成本，降低收费差错率。</p><blockquote>该方案构建了全链路安全防护体系，支持智能运维和操作行为审计，确保数据安全零风险。此外，PolarDB一体机还可支持智慧交通的多种延伸应用，如路网事件检测、数字孪生智慧站点等，为车路协同、自动驾驶等新兴场景提供支撑，助力交通行业从“试点样板”迈向“规模推广”。</blockquote><p><strong>OceanBase入选IDC 2025年中国分布式事务型数据库厂商“领导者”象限</strong></p><p>11月17日消息，日前， IDC 发布《 IDC MarketScape：中国分布式事务型数据库 2025 年厂商评估》，<strong>OceanBase、阿里云、腾讯云、华为云、金篆信科</strong>在内的五家中国数据库厂商被列入“<strong>领导者象限</strong>”，这是 OceanBase 连续两次获此殊荣。OceanBase 在分布式数据库产品能力上持续领先于同类别其他厂商，产品能力位列第一。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466135" alt="" title="" loading="lazy"/></p><blockquote>《 IDC MarketScape：中国分布式事务型数据库 2025 年厂商评估》报告显示，中国市场正在从跟随模仿转向自主创新，并在核心业务场景实现突破，2025 年已有更多的技术供应商宣布在国有四大行、核心券商保险公司以及其他保密性行业上的核心升级，并在性能、稳定性、成本等方面逐步接近甚至超越传统方案。</blockquote><p><strong>首本《运营商核心系统数据库分布式升级实践》正式发布！</strong></p><p>11月18日，在 2025 OceanBase 年度发布会·政企专场， OceanBase 联合中国联通软件研究院、江苏移动、河北移动、中移金科、亚信科技、新炬等客户及合作伙伴，共同发布了<a href="https://link.segmentfault.com/?enc=AlA02jStIFAvI8yaDjgajQ%3D%3D.fTF2a5AxvceFKmMznCv0%2BNKamnQZtTVEhA5rLaiHj86gGsq%2FprSmmA%2Bb5FWhbU%2Ba" rel="nofollow" target="_blank">《运营商核心系统数据库分布式升级实践：加速核心升级、布局AI引擎》</a>。这本被业界称为“运营商万字长文”的指南，是运营商领域的首本核心系统数据库分布式升级实践手册，手册总结了过去五年间主流运营商在核心系统国产升级与数智化转型中的经验，详细阐述了关键业务负载、实时数据分析、AI应用落地三大核心难题的解决方案，并提供了5大典型场景和4大创新场景的实践案例，为行业提供可复制、可落地的完整方法论。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466136" alt="" title="" loading="lazy"/></p><p><strong>IDC发布《中国关系型数据库市场追踪，2025H1》，前三名为阿里云、腾讯、华为</strong></p><p>11月19日，IDC 发布的《2025 年上半年中国关系型数据库软件市场跟踪报告》显示，<strong>2025上半年，中国关系型数据库软件市场规模为22.1亿美元</strong>（包含事务型和分析型关系数据库），同比增长14.5%，增速连续两年持续回升。其中，公有云关系型数据库规模15.0亿美元，同比增长16.3%；本地部署关系型数据库规模7.1亿美元，同比增长10.8%。预计未来两年内，随着数据库国产化替换的持续深入，特别是本地部署关系型数据库市场的将加速增长。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466137" alt="" title="" loading="lazy"/></p><ul><li><strong>2025上半年中国关系型数据库软件市场前四名分别是阿里云、腾讯、华为、AWS</strong>。在第一梯队Top4厂商的竞争中，腾讯云 <strong>TDSQL</strong> <strong>在公有云市场环比增速位列第一</strong>，AWS位居第二；在本地部署市场中，TDSQL 环比增速超过 Oracle，位居首位。</li></ul><p><strong>2025 开放原子开发者大会：TiDB、PolarDB、KWDB等获评“开源先锋项目”</strong></p><p>11月21-22日，2025开放原子开源开发者大会在北京成功举办。会上开放原子开源基金会表彰了涵盖操作系统、数据库、人工智能三大核心领域的28个开源先锋项目及47位开发者代表。其中，11家国产数据库（<strong>TiDB、OceanBase、openGauss、TDengine、PolarDB-X、Apache IoTDB、NebulaGraph、Milvus、KaiwuDB、OpenTenBase、Apache Doris</strong>）荣获“<strong>开源先锋项目</strong>”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466138" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047466139" alt="" title="" loading="lazy"/></p><blockquote>本次评选围绕技术创新突破、社区生态治理、国际行业影响力三大核心维度展开，入选项目包括开源鸿蒙、开源欧拉、openKylin、OpenTenBase、openGauss、KWDB、LLaMA Factory等业界标杆。</blockquote><p><strong>KaiwuDB 与映云科技( EMQ )达成战略合作，共拓物联网云边端数据市场</strong></p><p>11月24日消息，近日，KaiwuDB 宣布与杭州映云科技（ EMQ ）正式签署战略合作协议，双方将整合技术优势，共同打造“面向智能物联网的云边端一体化数据底座联合解决方案”，推动物联网云边端业务市场全链数据闭环，加速能源电力、工业制造、ICT 等关键行业数字化升级，为新一代物联网基础设施筑牢可信技术底座。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466140" alt="" title="" loading="lazy"/></p><p><strong>电科金仓与超图软件达成战略合作，共筑数据库与GIS融合新生态</strong></p><p>11月25日，电科金仓与超图软件在北京签署战略合作协议，围绕国产数据库与GIS的技术融合创新展开深度合作。双方将基于KingbaseES V9 2025与SuperMap GIS 2025，在技术融合、方案共创和市场拓展等方面展开合作，打造国产基础软件“GIS+数据库”的标杆技术栈，推出面向行业客户的联合解决方案，并在数字政府、智慧城市等领域开展联合市场活动。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466141" alt="" title="" loading="lazy"/></p><p><strong>第八届中国PostgreSQL数据库生态大会召开，OpenTenBase荣获 “2025年度开源影响力奖”</strong></p><p>11月29日，第八届中国PostgreSQL数据库生态大会在杭州成功举办，中国开源软件联盟PostgreSQL分会在会中颁发了2025年度“中国PostgreSQL金象奖。<strong>OpenTenBase</strong>凭借其在开源社区建设、技术创新与产业落地方面的卓越贡献，荣获 “<strong>2025年度开源影响力奖</strong>”。IvorySQL获得2025年度“产品创新奖”；<strong>Pigsty</strong>获得“<strong>PostgreSQL万磁王奖</strong>”；刘华阳、杨向博、王丁丁、郑全、崔鹏、杨宇获得2025年度“金牌讲师”称号。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466142" alt="" title="" loading="lazy"/><br/><em>OpenTenBase社区秘书长单致豪代表社区上台领奖</em></p><h2>11月产品/版本发布</h2><p><strong>Apache Doris Summit 2025 圆满收官，Apache Doris 4.0强化了 AI 原生支持能力</strong></p><p>11月5-6日，由飞轮科技主办的 Apache Doris Summit 2025 技术峰会通过线上直播形式举办，活动吸引全球10万+开发者参与，围绕Database for AI、湖仓一体、实时分析和可观测性四大场景，30+位专家分享了前沿技术实践；峰会重点发布了Apache Doris 4.0版本，通过向量索引、AI函数及MCP Agent接口实现AI原生支持，结合3.1版本的湖仓一体和可观测性能力升级，在性能方面实现显著提升（如快手场景查询速度从5秒降至160ms），同时飞轮科技旗下的SelectDB企业级产品已服务上千家客户，正与阿里云、AWS等合作伙伴共同构建开放生态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466143" alt="" title="" loading="lazy"/></p><p><strong>Apache Doris 4.0.1 版本正式发布，扩展了SEARCH函数支持</strong></p><p>11月8日，Apache Doris 4.0.1版本正式发布。该版本聚焦于核心模块的优化，新增了多个功能，包括mmh64\_v2和json\_hash函数、Binary数据类型及相关函数、对MaxCompute Schema层级的支持等。在AI &amp; Search方面，扩展了SEARCH函数的能力，支持短语查询、通配符查询和正则查询，并优化了倒排索引和分词器。Lakehouse方面，新增会话变量以优化Merge IO读取性能。</p><p><strong>OceanBase 2025：首款 AI 数据库 seekdb 发布 客户数突破 4000 家</strong></p><p>11月18日，OceanBase在 2025 年度发布会上发布并开源了 <strong>OceanBase 首款 AI 原生混合搜索数据库 seekdb</strong>（简称 seekdb ），同时发布了<strong>OceanBase 4.4 一体化融合版本</strong> 。在商业化方面，OceanBase 客户数突破 4000 家，连续 5 年年均增速超 100%，专有云客户数增长 50%，公有云营收占比达 30%，合作伙伴贡献收入超 70%。</p><ul><li><strong>seekdb</strong> 开发者仅需三行代码，即可快速构建知识库、智能体等 AI 应用，轻松应对百亿级多模数据检索，真正实现“开箱即用”的 AI 数据基座。seekdb是一款支持向量、全文、标量及空间地理数据统一混合搜索的产品，深度融合 AI 推理与数据处理，并兼容多种主流 AI 框架。seekdb 的核心突破包括 AI 原生混合搜索能力、极简部署和开发者友好性，目标是成为大模型与私有数据融合计算的“实时入口层”。</li><li><strong>OceanBase</strong> 4.4 一体化融合版本，将 TP、AP 与 AI 能力集成于单一内核，兼具分布式扩展、多云部署与金融级高可用，帮助企业避免后期架构重构风险。商用 4.4.2 LTS 版本将于 2026 年 2 月 2 日推出。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466144" alt="" title="" loading="lazy"/></p><p><strong>KaiwuDB V3.0 重磅发布，跨模查询性能提升 5-10 倍</strong></p><p>11月12日，浪潮KaiwuDB 新品发布会”上，浪潮自研分布式多模数据库全新版本 KaiwuDB V3.0 正式发布。该产品以多模为核心，全面强化数据处理能力，其中单机写入性能相较于前序版本提升 40%-216%，分布式写入性能提升 20%-50%，跨模查询性能提升 5-10 倍。同时，为了适应物联网多样化的部署环境，KaiwuDB V3.0 在可用性、易用性、安全性方面定向优化并新增多项功能，同步发布基于MCP协议的智能运维工具KAT。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466145" alt="" title="" loading="lazy"/></p><p><strong>悦数图数据库 v5.2 正式发布：聚焦图计算与多模检索，提升 AI 应用支撑能力</strong></p><p>11月20日消息，悦数图数据库 v5.2 正式发布，聚焦于图计算与多模检索能力的提升，以更好地支撑 AI 应用。新版本通过内嵌轻量图计算引擎、扩展 GQL 查询语言、优化关键查询算法、融合图 + 向量 + 全文检索、支持超级节点采样、新增 GEO 支持以及增强企业级可靠性等多方面升级，降低了 AI 团队使用图技术的门槛，提升了 AI 应用的感知与推理能力，为推荐、风控、知识问答等典型 AI 场景提供了更坚实的数据支撑。</p><p><strong>Easysearch v2.0 全新功能上线，新增轻量级 UI 插件</strong></p><p>11月24日消息，极限工作室发布Easysearch v2.0。Easysearch v2.0 是一款分布式搜索型数据库，实现了非结构化数据检索、全文检索、向量检索、地理位置信息查询等多种功能，可完美替代 Elasticsearch 并添加多项企业级功能。本次更新包括底层 Lucene 升级到 9.12.2，新增轻量级 UI 插件，提供集群管理功能，支持关闭 security 进入 UI，并优化了 range 查询性能。</p><p><strong>IvorySQL 5.0 发布：基于 PG 18.0，Oracle 兼容、生态组件、云原生与易用性全面进化！</strong></p><p>11月25日消息，IvorySQL 5.0 发布，基于 PostgreSQL 18.0 进行全面升级，新增 21 个 Oracle 兼容功能，包括 Oracle 风格的语法、函数和存储过程支持等，进一步提升与 Oracle 的兼容性。同时，支持云原生部署，提供全平台安装包和在线体验环境，还集成了多种生态组件，如 pg_cron、pgAudit、PostGIS 等，增强定时任务、审计、空间数据处理等功能，为用户提供更高效、灵活的数据库解决方案。</p><p><strong>YashanDB V23.5发布：YAC共享集群全面迈向规模化商用</strong></p><p>11月26日，YashanDB V23.5正式发布，标志着YAC共享集群全面迈向规模化商用。新版本围绕稳定、高效、经济、易用四大业务价值，实现了同城双活、全库闪回、TP+等能力的增强，提供高可用、高性能的数据库解决方案。同时，深化了对Oracle及MySQL生态的兼容性，降低了系统迁移复杂度。YashanDB V23.5还支持向量数据处理，为企业的智能化转型夯实数据基础。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047466146" alt="" title="" loading="lazy"/><br/><em>图.崖山共享集群多中心“对等多活”架构</em></p><h2>相关资料</h2><ul><li><a href="https://link.segmentfault.com/?enc=7DzY6B%2BDc8P6sDYxr%2BL%2FFw%3D%3D.Cz%2FvpVM94RpWh6aoqHABr%2FWnXuW7mMOngjE%2FaWKDcWkSWVznnv0HyRw5FRRdGOd5" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜-2025年12月已更新</a></li><li><a href="https://link.segmentfault.com/?enc=hJSRGxKc%2BQplGSPZz1gh%2Fg%3D%3D.eA4%2FxyroZQ9el7CKyNljIqU00EV1bUeGXUjhwvA4qR2AzLzL7uZtea7uMdKfPhqV" rel="nofollow" target="_blank">墨天轮中国数据库流行度排行榜规则解读</a></li><li><a href="https://link.segmentfault.com/?enc=KlHtFihneUN4VxrnPogzQw%3D%3D.SKgsr%2FOvq76b0dMjwVHZi35dNpV2WyKDPfYNBRCuBLBGNm%2Bi6jSoLugXljFWctkU" rel="nofollow" target="_blank">月度国产数据库大事记合辑</a></li><li><a href="https://link.segmentfault.com/?enc=esPvBJCvQRpFMLC2PT%2F%2BzQ%3D%3D.phGVDrHRMtkMzMVPe7UKMz3vSapbJP7fueDznXlCp%2BCWKPF4a%2FX%2BikqV%2BrmxiGfb" rel="nofollow" target="_blank">中国数据库排行榜 - 月度解读</a></li><li><a href="https://link.segmentfault.com/?enc=kbL13sMhPXctZpFlBKTlkg%3D%3D.qBJrx5wDEfMHx2KpUgI1O%2BSQDySEyLOi8V6L208Rhcg%3D" rel="nofollow" target="_blank">国产数据库招投标信息汇总</a></li><li><a href="https://link.segmentfault.com/?enc=32pNe4tfxKyl9bcNwjxWNA%3D%3D.R8u94Dj5F2tl58vAYyUUOzsna%2F9%2BbtkzKpH2p7xIxBPBjbBlvb6VoRNGS7FzySRb" rel="nofollow" target="_blank">《万字总结：运营商核心系统数据库分布式升级实践》</a></li></ul><p>点击阅读原文：<a href="https://link.segmentfault.com/?enc=qjVfzQnBgF3NC%2BDT6414mQ%3D%3D.INz8GO5yneLQFOz%2F6oZUlFndC%2BPgcQpplDeJbQm8FSULRXG1h9BVe3qpFKhDtD8S5E2Net%2BAIikb1hvldHOf5g%3D%3D" rel="nofollow" target="_blank">https://www.modb.pro/db/1996799519414116352</a></p><hr/><p>欲了解更多可浏览<a href="https://link.segmentfault.com/?enc=DjA4NEgLeDSDCvtGUHCHJA%3D%3D.PAYY5Vxjv1I%2F7K1o77P2eWsjqJPf%2BaFNLhlQRv%2B8GU8%3D" rel="nofollow" target="_blank">墨天轮社区</a>，围绕数据人的学习成长提供一站式的全面服务，打造集新闻资讯、在线问答、活动直播、在线课程、文档阅览、资源下载、知识分享及在线运维为一体的统一平台，持续促进数据领域的知识传播和技术创新。</p><p>关注官方公众号： 墨天轮、 墨天轮平台、墨天轮成长营、数据库国产化 、数据库资讯</p>]]></description></item><item>    <title><![CDATA[TOML 格式简介 shuangxiao99 ]]></title>    <link>https://segmentfault.com/a/1190000047465701</link>    <guid>https://segmentfault.com/a/1190000047465701</guid>    <pubDate>2025-12-11 11:13:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>TOML（Tom's Obvious, Minimal Language）是为配置文件设计的序列化格式，目标是简洁、可读且易于手写。常用于项目配置（如 Rust 的 Cargo、Python 的 Poetry、各类工具的配置文件）。下面概要说明其特性、示例及与 JSON 的对比，帮助选择合适场景。</p><h2>核心特点（简要）</h2><ul><li>面向配置：设计用于人类编辑的配置文件。</li><li>支持注释：使用 <code>#</code>。</li><li>明确的原生类型：字符串、整数、浮点、布尔、日期时间、数组、表（table）。</li><li>表与表数组：便于层级组织配置。</li><li>可读性高：语法更接近 INI，但类型更丰富。</li></ul><h2>基本语法示例</h2><pre><code class="toml">toml
# filepath: example.toml
title = "示例配置"
owner = { name = "Alice", dob = 1979-05-27T07:32:00Z }

[database]
server = "192.168.1.1"
ports = [ 8001, 8001, 8002 ]
enabled = true
ratio = 0.75

[[products]]
name = "Hammer"
sku = 738594937

[[products]]
name = "Nail"
sku = 284758393
color = "gray"</code></pre><p>对应的 JSON（同语义）：</p><pre><code class="json">json
{
  "title": "示例配置",
  "owner": { "name": "Alice", "dob": "1979-05-27T07:32:00Z" },
  "database": {
    "server": "192.168.1.1",
    "ports": [8001, 8001, 8002],
    "enabled": true,
    "ratio": 0.75
  },
  "products": [
    { "name": "Hammer", "sku": 738594937 },
    { "name": "Nail", "sku": 284758393, "color": "gray" }
  ]
}</code></pre><h2>与 JSON 的对比（要点）</h2><ul><li><p>可读性</p><ul><li>TOML：面向人类，允许注释，表结构清晰，适合手动维护配置。</li><li>JSON：紧凑、标准化但不支持注释（配置场景常被诟病）。</li></ul></li><li><p>类型与语义</p><ul><li>TOML：内建 datetime、明确定义整数/浮点/布尔/数组/嵌套表。</li><li>JSON：基本类型（string/number/boolean/null/array/object），日期通常以字符串表示并需约定格式。</li></ul></li><li><p>可写性（手工编辑）</p><ul><li>TOML 更友好（注释、表语法、行内表等）。</li></ul></li><li><p>生态与工具支持</p><ul><li>JSON：极其普及，原生支持于多数语言（尤其 JavaScript），适合数据交换和 API。</li><li>TOML：配置使用广泛，解析器越来越多，但在某些语言/平台的支持不如 JSON 全面。</li></ul></li><li><p>机器可处理性 / 互操作性</p><ul><li>JSON 更通用、轻量、网络传输更常用（很多网络协议以 JSON 为默认）。</li><li>TOML 更适合作为静态配置文件，而非频繁网络交互的序列化格式。</li></ul></li><li><p>注释与可维护性</p><ul><li>TOML 支持注释，便于说明配置项；JSON 不支持（需要外部文档或自定义字段）。</li></ul></li><li><p>严格性与规范</p><ul><li>TOML 标准严格（v1.0.0），对类型/转义等有明确定义。</li><li>JSON 标准成熟且广泛实现。</li></ul></li></ul><h2>何时使用哪个</h2><ul><li><p>选择 TOML：</p><ul><li>配置文件需多人手工编辑、希望内置注释和日期类型（例如项目配置、工具配置）。</li><li>需要层级清晰的可读配置文件。</li></ul></li><li><p>选择 JSON：</p><ul><li>数据交换（API、前后端通信）、日志或需要广泛兼容性的场景。</li><li>需要最广泛的库与语言支持，或嵌入到 JavaScript 环境中。</li></ul></li></ul><h2>优点/缺点快速概览</h2><ul><li>TOML 优点：可读性高、支持注释、类型丰富（datetime）、表语法直观。<br/>缺点：生态和解析器不及 JSON 广泛；用于网络传输场景不够普及。</li><li>JSON 优点：极度普及、格式紧凑、语言原生支持（JS）。<br/>缺点：不支持注释、日期需自定义约定，手写维护时可读性和可注释性较差。</li></ul><h2>工具与注意事项</h2><ul><li>常见解析库：各语言均有 TOML 解析器（toml-rs、toml for Python、toml-js 等）。</li><li>版本：使用 TOML v1.0+ 时注意解析器是否兼容。</li><li>与环境变量/模板结合使用时，配置文件往往需要转换或合并，选择前确定生态支持。</li></ul><p>总结：<a href="https://link.segmentfault.com/?enc=GpkUo8ocNNyz3rFbjNBJ5Q%3D%3D.a3269tzJJQggaHcMqBJHcdQ4MC1qyX1rTS3qw%2BgAdH8wmerZlV%2BhwPXvNCN5p00i" rel="nofollow" target="_blank">TOML</a> 是面向配置文件的优秀选择（注释友好、类型丰富）；JSON 则是数据交换与平台互操作的首选。根据用途（配置 vs 数据交换）、可维护性与生态支持来决定使用哪种格式。</p>]]></description></item><item>    <title><![CDATA[ERP不是万能的，真正的执行靠MES 万界星空科技 ]]></title>    <link>https://segmentfault.com/a/1190000047465719</link>    <guid>https://segmentfault.com/a/1190000047465719</guid>    <pubDate>2025-12-11 11:12:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>ERP与MES的区别</strong><br/>ERP（企业资源计划）与 MES（制造执行系统）是制造业数字化转型中两个核心但定位截然不同的信息系统。它们不是“二选一”的关系，而是上下协同、互补共生的伙伴关系。以下是清晰、实用的对比解析：<br/><strong>一、核心定位不同</strong><br/><img width="723" height="166" referrerpolicy="no-referrer" src="/img/bVdnkae" alt="" title=""/><br/><strong>二、关注重点不同</strong><br/><img width="723" height="251" referrerpolicy="no-referrer" src="/img/bVdnkaf" alt="" title="" loading="lazy"/><br/><strong>三、典型功能对比</strong><br/><img width="723" height="226" referrerpolicy="no-referrer" src="/img/bVdnkag" alt="" title="" loading="lazy"/><br/><strong>四、一个例子看懂区别</strong><br/>举个例子：当客户下单500台电机并要求15天交付时，ERP 会从全局角度规划——需要多少铜线和漆包线，安排哪个车间生产，预估总成本约38万元，并设定第10天完成生产的节点。<br/>MES 则在车间层面实时监控执行细节：今天上午9:15定子绕线工位因张力异常停机12分钟；第237台电机动平衡测试不合格已被隔离；当前生产线的综合效率（OEE）为76%，系统判断仍可按时交付。由此可见，ERP 管的是“目标”，MES 管的是“过程”。<br/><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdcqAE" alt="" title="" loading="lazy"/><br/><strong>五、为什么两者必须集成？</strong><br/>如果只有 ERP：<br/>计划脱离实际（车间根本达不到排产速度）；<br/>库存数据失真（系统显示有料，实际已被挪用）；<br/>成本核算不准（无法追踪真实工时与损耗）。<br/>如果只有 MES：<br/>缺乏业务源头（不知道该生产什么）；<br/>无法联动采购与财务；<br/>孤立于企业整体运营体系。<br/><strong>✅ 理想状态：</strong><br/>ERP 下达“做什么”，万界星空MES 执行“怎么做”并反馈“做得怎样”，形成 “计划 → 执行 → 反馈 → 优化” 的闭环。<br/>ERP 告诉你“要造500台电机”，MES 确保“这500台被正确地造出来，并告诉你每一步发生了什么”。<br/><strong>适用行业：</strong><br/>离散制造（机械、电机、家具）：MES 重点在 工序追踪、防错、柔性排产；<br/>流程制造（漆包线、食品、化工）：MES 重点在 参数闭环、批次追溯、配方管理；<br/>中小企业：可先上轻量 MES 补齐执行层，再与 ERP 对接；<br/>大型集团：需规划 ERP-MES-WMS-QMS 一体化架构。<br/>只有理解 ERP 与 MES 的分工与协同，才能避免“上了 ERP 还是管不好生产”或“MES 孤岛运行无价值”的困境，真正迈向智能制造。</p>]]></description></item><item>    <title><![CDATA[客流波动、协同低效？酒店排程管理的核心解法 Zoey的笔记本 ]]></title>    <link>https://segmentfault.com/a/1190000047465739</link>    <guid>https://segmentfault.com/a/1190000047465739</guid>    <pubDate>2025-12-11 11:11:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、什么是排程管理？</h2><p>酒店排程管理是指酒店运营过程中，基于业务需求、资源状况和服务标准，对人力、物力、时间等核心要素进行系统性规划、分配与调控的管理行为。其管理范畴广泛，既涵盖前厅接待、客房服务、餐饮运营等一线岗位的人员排班，也包括客房预订、会议室使用、设备维护、物料采购等业务环节的时间与资源调度，最终目标是实现“人岗匹配、资源最优、服务高效”的运营状态。<br/>与普通企业的排程不同，酒店排程管理具有显著的行业特性：一是波动性强，受旅游旺季、节假日、本地活动等因素影响，客流需求昼夜、周度、月度差异明显；二是关联性高，客房清洁进度直接影响入住办理效率，餐饮排班需与宴会预订精准衔接，任一环节的排程失误都可能引发连锁问题；三是实时性要求高，临时退房、突发维修、客户特殊需求等情况需快速响应并调整排程。</p><h2>二、酒店排程管理的核心价值：为何不可或缺？</h2><p>高效的排程管理是酒店运营的“中枢神经”，其重要性贯穿于成本控制、服务质量、员工管理和业务增长等多个维度，具体体现在以下四个方面：</p><ol><li>严控运营成本，避免资源浪费<br/>人力成本是酒店运营的主要支出之一，不合理的排程往往导致“忙时人手不足、闲时人力冗余”。例如，旅游旺季前厅接待人员短缺会引发客户投诉，而淡季客房服务人员过剩则直接增加薪资成本。通过科学排程，可根据客流预测精准匹配岗位人力，同时优化客房清洁顺序、设备使用时间，减少水电能耗和物料损耗，实现成本的精细化控制。</li><li>保障服务质量，提升客户体验<br/>酒店服务的核心是“及时响应、精准对接”，排程管理正是实现这一目标的关键。比如，通过排程确保客房在客人入住前完成清洁消毒，宴会场地在活动开始前完成布置，维修人员在设备故障15分钟内到场，这些细节都依赖于高效的排程体系。反之，排程混乱可能导致客人入住等待超1小时、客房卫生不达标等问题，直接降低客户满意度和复购率。</li><li>稳定员工状态，降低流失率<br/>酒店一线员工工作强度大，不合理的排程（如频繁加班、突发调班、休息时间被占用）容易引发员工抵触情绪。科学的排程管理会充分考虑员工的技能特长、休息需求和职业规划，例如让擅长外语的员工负责国际客人接待，提前1周发布排班表，避免临时调班，这不仅能提升员工工作效率，还能增强员工的归属感，降低行业普遍偏高的员工流失率。</li><li><p>支撑业务决策，提升运营效率<br/>排程数据是酒店运营状况的“晴雨表”。通过分析不同时段的人力需求、客房周转效率、会议室使用频率等排程数据，管理层可精准判断业务高峰时段、核心盈利环节和运营薄弱点，进而优化定价策略、调整服务流程、配置核心资源。例如，根据周末亲子客群集中的排程特征，推出“亲子房+早餐”的套餐组合，提升整体营收。</p><h2>三、酒店排程管理实用工具推荐</h2><p>随着酒店运营复杂度的提升，传统的Excel表格排程已难以满足实时调整、多部门协同的需求，专业的排程工具成为必然选择。以下推荐几款适配酒店场景的工具，涵盖不同功能侧重和使用成本：</p></li><li>板栗看板：轻量化协同排程工具<br/>核心优势：以“可视化看板”为核心，操作简单易上手，无需专业技术背景，适合中小酒店及多部门协同场景。支持自定义任务模块、拖拽式调整排程，可实时同步客房状态、员工排班、维修进度等信息，且具备移动端适配功能，方便一线员工随时查看。<br/>适用场景：客房排程、员工排班、会议活动统筹、物料采购跟进等全流程管理。</li><li>排班表（Shiftboard）：专业人力排程工具<br/>核心优势：聚焦人力排程，具备智能考勤、技能匹配、加班计算等功能，可根据员工技能证书（如外语等级、急救证）自动匹配岗位需求，同时对接薪资系统，减少考勤统计工作量。<br/>适用场景：前厅、餐饮、安保等一线岗位的精细化人力排班。</li><li>甲骨文酒店管理系统（Oracle Hospitality）：一体化运营工具<br/>核心优势：大型酒店常用的一体化系统，将排程管理与客房预订、收银系统、客户关系管理（CRM）深度融合，可根据预订数据自动生成客房清洁排程和前厅人力需求，实现业务全链路协同。<br/>适用场景：连锁酒店、高端星级酒店的规模化运营管理。</li><li><p>Trello：灵活适配的通用看板工具<br/>核心优势：以“卡片+列表”的形式呈现排程，支持添加附件、设置截止时间、@成员协作，可通过插件拓展客房预订同步、数据统计等功能，灵活性高，适合小型酒店的个性化排程需求。<br/>适用场景：小型精品酒店的多维度排程管理，可根据需求自定义模块。</p><h2>四、看板工具在酒店排程管理中的实战应用</h2><p>板栗看板的“可视化、轻量化、强协同”特性与酒店排程管理的需求高度契合，通过搭建专属看板，可实现从客房、人力到活动的全流程排程管控。具体操作步骤如下：</p></li><li>前期准备：明确排程核心维度与权限设置<br/>首先需梳理酒店排程的核心场景，确定看板的核心模块，建议优先覆盖“客房管理”“人力排班”“会议活动”三大核心场景。同时根据岗位权限设置看板访问权限：管理层可查看全量排程数据并修改，一线员工（如客房服务员、前厅接待）仅能查看所属模块的排程并反馈进度。</li><li>搭建专属看板：模块设计与数据关联<br/>登录板栗看板后，通过“新建看板”功能创建“酒店运营总看板”，并根据场景拆分以下子模块，各模块通过“关联字段”实现数据互通（如“客房编号”关联“负责服务员”）：<br/>模块1：客房排程看板——全流程管控客房状态<br/>以“客房状态”为核心维度设置列表，包括“待清洁”“清洁中”“待检查”“已达标（可入住）”“维修中”五大状态列。每个客房以“卡片”形式呈现，卡片内添加“客房编号”“房型”“入住时间”“退房时间”“负责清洁员”“特殊需求（如婴儿床、无烟）”等字段。<br/>操作逻辑：前厅接待在客人预订后，将“客房编号”及入住信息录入对应卡片；客房主管根据“退房时间”将卡片拖拽至“待清洁”列，并分配给清洁员；清洁员完成工作后将卡片移至“待检查”列，主管检查通过后移至“已达标”列，前厅即可为客人办理入住。若客房出现设施故障，直接将卡片拖拽至“维修中”列，并关联“维修工单”子卡片，维修完成后再转回“待清洁”列。<br/>模块2：人力排班看板——精准匹配岗位需求<br/>以“岗位类型”为横向维度（如前厅接待、客房服务、餐饮服务、安保），以“日期+时段”为纵向维度（如10月1日早班8:00-16:00、中班16:00-24:00、晚班0:00-8:00），搭建矩阵式看板。每个单元格对应“岗位-时段”的人力需求，通过卡片标注员工姓名、联系方式、技能特长。<br/>操作逻辑：人力主管根据历史客流数据和未来预订情况，提前1周在看板中分配人力，员工可通过移动端查看排班表；若员工需调班，可在对应卡片发起“调班申请”，主管审核通过后直接修改卡片信息并同步至全部门。同时，在卡片中添加“考勤状态”字段，员工上下班时点击更新，便于主管实时掌握在岗情况。<br/>模块3：会议活动看板——统筹场地与服务资源<br/>以“会议场地”为核心列表（如宴会厅A、小型会议室B、贵宾室C），每个场地卡片内标注“预订企业”“活动时间”“参与人数”“服务需求（如茶歇、投影设备、引导员）”“负责人”等字段，并关联“物料准备”“人员调配”子模块。<br/>操作逻辑：销售部在确认会议预订后，创建对应场地的卡片并完善信息；餐饮部根据“参与人数”和“茶歇需求”在子模块中添加物料准备排程，前厅部根据“活动时间”安排引导员和设备调试人员；活动进行中，负责人实时更新“进度状态”（如“场地布置中”“活动进行中”“场地清理中”），确保各环节衔接顺畅。</li><li>高效运营：利用看板功能提升排程效率<br/>•提醒功能：为关键节点设置自动提醒，如“客房清洁截止时间前30分钟提醒清洁员”“会议开始前1小时提醒设备调试员”，避免遗漏重要任务。<br/>•数据统计：通过看板的“统计视图”功能，自动生成“每日客房周转率”“各岗位人力利用率”“会议场地使用率”等数据报表，为管理层调整排程策略提供依据。<br/>•移动端同步：一线员工通过板栗看板移动端APP接收排程信息、反馈工作进度，如客房服务员清洁完成后可直接在手机上标记“已达标”，无需返回办公室汇报，提升响应效率。</li><li>优化迭代：基于数据调整看板设计<br/>看板使用1-2周后，收集各部门反馈，结合统计数据优化模块设计。例如，若发现“餐饮服务”与“会议茶歇”的排程衔接不畅，可在两个模块间添加“关联字段”，实现茶歇需求自动同步至餐饮排程；若某类客房清洁耗时较长，可在对应卡片中添加“清洁时长参考”字段，帮助主管更合理地分配任务。</li></ol>]]></description></item><item>    <title><![CDATA[团队绩效管理避坑指南：从传统模式到在线看板，选对工具效率翻倍 Zoey的笔记本 ]]></title>    <link>https://segmentfault.com/a/1190000047465751</link>    <guid>https://segmentfault.com/a/1190000047465751</guid>    <pubDate>2025-12-11 11:11:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、团队绩效管理的核心价值</h2><p>团队绩效管理并非简单的“考核工具”，而是支撑组织战略落地、激发个体潜能的核心机制，其重要性体现在三个维度：</p><ol><li>实现战略解码与目标对齐<br/>有效的绩效管理能将企业宏观战略拆解为可执行的团队及个人目标，避免“战略悬在空中、执行落在纸上”的困境。例如某SaaS产品团队通过明确的绩效目标体系，将季度产品规划转化为各岗位具体任务，使需求交付准时率提升至95%，确保团队行动与企业方向高度一致。</li><li>激发团队活力与个体潜能<br/>清晰的绩效标准与反馈机制能让员工明确“做什么、怎么做、做到什么程度有回报”，打破“干多干少一个样”的惰性。某零售集团实施科学绩效管理后，员工平均销售额提升12%，团队协作效率提高25%，印证了合理的绩效体系对积极性的驱动作用。</li><li><p>优化管理决策与资源配置<br/>通过绩效数据的积累与分析，管理层可精准识别团队短板、核心人才及流程瓶颈。例如某制造企业借助绩效数据发现生产环节的资源浪费问题，通过优化配置使生产效率提升18%，同时为人才培养、岗位调整提供客观依据。</p><h2>二、在线团队绩效管理的独特优势</h2><p>相较于传统“纸质表格+季度会议”的绩效管理模式，在线模式依托数字化技术实现了质的飞跃，其核心优势体现在四个方面：</p></li><li>实时性：打破信息滞后壁垒<br/>传统模式下绩效数据多为“季度或年度汇总”，问题发现时往往已错失改进时机；在线系统可实现绩效数据每日更新，较传统模式效率提升5倍。例如主管通过线上平台能实时查看任务进度，当某项目出现延期风险时可立即介入调整，避免风险扩大。</li><li>透明度：构建信任管理环境<br/>在线系统将绩效目标、评估标准、进度数据全部公开，员工可随时查询自身及团队绩效情况，减少“暗箱操作”疑虑。某零售集团引入线上绩效管理后，员工对绩效管理的信任度从62%提升至89%，这种信任直接转化为工作主动性。</li><li>个性化：适配多元管理需求<br/>传统绩效管理多采用“一刀切”标准，难以适配不同岗位特性；在线系统可根据岗位差异自动匹配激励指标与评估维度，使员工满意度提升18%。如软件开发岗侧重“迭代效率与bug修复率”，营销岗侧重“活动转化率与销售额”，线上系统可实现精准适配。</li><li><p>可追溯：支撑持续改进<br/>在线系统自动记录所有绩效数据变化，包括任务调整、反馈记录等，为绩效复盘提供完整依据。某咨询公司通过线上系统留存的绩效数据，构建了“目标-执行-结果”的闭环分析体系，使年度战略调整的精准度提升40%。</p><h2>三、主流团队绩效管理工具推荐</h2><p>结合2025年市场评测数据与企业实践反馈，不同类型团队可根据需求选择以下工具，涵盖看板协作、OKR管理、综合绩效三大类别：</p></li><li>看板类绩效工具：板栗看板<br/>聚焦任务可视化与绩效跟踪，支持多项目管理、实时协作与自动化提醒。其优势在于操作轻便、模板丰富，能快速适配软件开发、市场营销等多场景。适合中小型团队及需要敏捷响应的项目组。</li><li>OKR专项管理工具：北极星OKR<br/>核心优势在于独创的OKR-T（目标-关键结果-任务）结构，确保战略落地；提供海量行业模板，支持360°目标对齐与实时进度跟踪。特别适合中大型企业及互联网科技团队。</li><li>生态集成工具：飞书OKR<br/>作为飞书原生模块，可与飞书文档、会议、人事系统无缝打通，形成“目标-任务-绩效”闭环。亮点功能包括OKR周报自动汇总、会议语音转写关联关键结果。适合已深度使用飞书套件的中大型企业。</li><li><p>敏捷绩效工具：Tita OKR<br/>以“OKR-任务-绩效-奖励”四段式流程为核心，8.7版本新增AI风险预警模块，可预测关键结果延期概率并推送补救方案。。适合需要将绩效数据与薪酬体系联动的企业。</p><h2>四、看板工具的绩效管理应用</h2><p>看板工具通过“可视化流程+实时协作”实现绩效管理落地，板栗看板凭借灵活配置与易用性，成为众多团队的首选。结合绩效考核五步法，其具体应用流程如下：</p></li><li>第一步：绩效目标可视化设定（对齐SMART原则）<br/>核心是将抽象目标转化为可追踪的任务卡片。在板栗看板中创建“绩效目标看板”，设置“待讨论”“已批准”“进行中”三个核心列：<br/>•每张卡片代表一个绩效目标，需明确填写目标描述（如“提升客户满意度20%”）、重要性等级、预期完成时间及关联KPI；<br/>•通过“成员指派”功能明确目标负责人，支持@相关协作人参与目标讨论，确保共识达成；<br/>•利用板栗看板的“目标拆解”功能，将季度目标分解为月度任务，如将“客户满意度提升”拆解为“优化服务流程”“客服培训”等子任务卡片。</li><li>第二步：全流程绩效跟踪（实时监控进度）<br/>创建“绩效跟踪看板”，按任务阶段设置“待开始”“进行中”“阻塞”“已完成”列，并设置合理的在制品（WIP）限制，避免任务堆积：<br/>•任务卡片需包含负责人、截止日期、进度百分比等信息，成员可通过拖拽卡片更新状态，系统自动同步至所有成员；<br/>•利用“自动化规则”设置提醒：当任务进入“阻塞”列时，自动通知负责人及主管；任务临近截止日期3天时，发送邮件与APP提醒；<br/>•通过板栗看板的“统计视图”，主管可实时查看团队及个人的任务完成率，如某软件开发团队通过该功能发现测试环节瓶颈，及时增配资源使迭代效率提升30%。</li><li>第三步：360°绩效评估（客观收集反馈）<br/>搭建“绩效评估看板”，设置“自我评估”“同事评价”“上级评价”“最终评级”列，实现评估流程透明化：<br/>•员工在“自我评估”列上传总结卡片，关联完成的任务记录作为依据；同事与上级通过“评论功能”添加评价意见，支持附件上传（如项目报告）；<br/>•主管结合任务完成数据与多方评价，在“最终评级”列标注结果，系统自动汇总形成评估报告，避免主观偏见；<br/>•某产品设计团队通过该模式，使评估结果与实际贡献的匹配度提升至92%，员工对评估公平性的认可度显著提高。</li><li>第四步：反馈与改进（形成闭环）<br/>创建“反馈与改进看板”，设置“待反馈”“已反馈”“改进计划”“已完成”列，将绩效评估转化为改进行动：<br/>•主管在“待反馈”列创建反馈卡片，明确指出改进点（如“数据分析能力需提升”），并关联相关学习资源；<br/>•员工制定改进计划后，在“改进计划”列创建任务卡片，设置阶段性目标（如“1个月内完成数据分析课程”），主管定期跟进进度；<br/>•改进完成后，通过“已完成”列归档结果，形成“评估-反馈-改进-提升”的闭环，某服务型企业通过该模式使员工技能提升速度加快27%。</li><li>关键使用技巧与避坑指南<br/>看板使用需避免三大误区：一是看板过于复杂，应简化结构聚焦核心流程，如营销团队可仅保留“策划-执行-评估”三列；二是更新不及时，需建立每日站会制度，同步看板状态；三是缺乏持续改进，建议每月举行看板回顾会议，收集团队反馈优化流程。</li></ol>]]></description></item><item>    <title><![CDATA[2025年最值得入手的5款AI Wiki工具：告别信息混乱，让知识管理更智能 百川云开发者 ]]></title>    <link>https://segmentfault.com/a/1190000047465772</link>    <guid>https://segmentfault.com/a/1190000047465772</guid>    <pubDate>2025-12-11 11:10:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>大家好，我是老王，一个在互联网行业摸爬滚打了10年的老IT。今天想和大家聊聊一个让我又爱又恨的话题——知识管理。</p><h2>从"信息黑洞"到"知识中枢"的蜕变</h2><p>记得去年我们团队接手了一个大项目，各种文档、会议记录、代码说明满天飞。刚开始大家还兴致勃勃地在Confluence上写文档，结果三个月后，连项目负责人自己都找不到关键文档在哪了。更可怕的是，新来的同事要花整整两周才能搞清楚项目全貌——这哪是知识库啊，简直就是个"信息黑洞"！</p><p>直到我们发现了AI Wiki这个神器，才彻底改变了这种局面。今天我就给大家测评2025年最值得入手的5款AI Wiki工具，看看它们如何用人工智能技术让知识管理变得更智能、更高效。</p><h2>什么是AI Wiki？为什么企业需要它？</h2><p>简单来说，AI Wiki就是传统Wiki的智能升级版。它不仅保留了Wiki的协作编辑、版本控制等基础功能，还加入了自然语言处理、知识图谱、智能推荐等AI能力。想象一下，你的知识库不仅能存储信息，还能理解信息、主动推送信息，甚至帮你生成内容——这就是AI Wiki的魅力所在。</p><p>根据我们的使用经验，一个好的AI Wiki应该具备以下特点：</p><ul><li>智能搜索：不再需要记住精确关键词</li><li>自动分类：文档自动归档，告别混乱</li><li>内容生成：AI辅助写作，提高效率</li><li>权限管理：确保信息安全</li><li>多端同步：随时随地访问</li></ul><h2>2025年5款AI Wiki深度测评</h2><h3>1. PandaWiki：企业级知识管理的全能选手</h3><p><img width="723" height="319" referrerpolicy="no-referrer" src="/img/bVdnbcA" alt="" title=""/></p><p><strong>推荐指数：★★★★☆</strong></p><p>PandaWiki是我们团队最终选择的产品，原因很简单——它几乎满足了我们对知识管理的所有幻想。</p><p><strong>亮点功能：</strong></p><ul><li>开箱即用的企业级解决方案</li><li>强大的权限管理和版本控制</li><li>内置AI写作助手，支持一键生成文档</li><li>智能问答系统，直接对话获取知识</li><li>完全开源，可自由定制</li></ul><p><strong>适合场景：</strong></p><ul><li>替代Confluence搭建企业文档中心</li><li>构建销售/客服知识支撑系统</li><li>实现全员知识共享</li></ul><p><strong>使用感受：</strong><br/>部署简单到令人发指，我们技术团队只用了半小时就搞定了。最惊艳的是它的智能搜索，即使记不清文档标题，用自然语言描述也能找到相关内容。新员工培训时间直接缩短了60%！</p><p><a href="https://link.segmentfault.com/?enc=UUx22xn7i3PYZmIhtAsxig%3D%3D.X2VZaQRCoNycwoeI4GHl9l0lqcrytE2jsdzVEodc6D0%3D" rel="nofollow" target="_blank">点击查看PandaWiki官方文档</a></p><h3>2. DeepWiki：技术团队的首选</h3><p><strong>推荐指数：★★★★☆</strong></p><p>如果你是技术团队，特别是开源项目维护者，DeepWiki绝对值得一看。</p><p><strong>亮点功能：</strong></p><ul><li>自动分析代码库生成技术文档</li><li>交互式可视化工具</li><li>AI技术问答助手</li><li>支持多编程语言</li></ul><p><strong>适合场景：</strong></p><ul><li>开源项目文档维护</li><li>技术团队知识沉淀</li><li>代码库自动化文档</li></ul><p><strong>使用感受：</strong><br/>我们用它管理了一个中型Java项目，最大的惊喜是它能自动梳理代码结构，生成清晰的模块关系图。新成员看这个比直接看代码快多了！</p><h3>3. Dify：文档自动化的新选择</h3><p><strong>推荐指数：★★★☆☆</strong></p><p><strong>亮点功能：</strong></p><ul><li>强大的文档自动化能力</li><li>智能问答系统</li><li>企业级安全管控</li></ul><p><strong>适合场景：</strong></p><ul><li>合同、报告等标准化文档生成</li><li>企业流程文档管理</li></ul><p><strong>使用感受：</strong><br/>文档生成确实快，但编辑体验不如PandaWiki流畅。适合文档标准化程度高的企业。</p><h3>4. MaxKB：跨国企业的好帮手</h3><p><strong>推荐指数：★★★☆☆</strong></p><p><strong>亮点功能：</strong></p><ul><li>多语言实时翻译</li><li>全球化协作支持</li><li>智能术语库</li></ul><p><strong>适合场景：</strong></p><ul><li>跨国企业知识共享</li><li>多语言文档管理</li></ul><p><strong>使用感受：</strong><br/>翻译质量不错，但界面设计略显老旧。如果你的团队需要频繁进行跨语言协作，值得考虑。</p><h3>5. ChatWiki：即时沟通与知识管理的结合</h3><p><strong>推荐指数：★★★☆☆</strong></p><p><strong>亮点功能：</strong></p><ul><li>内置即时通讯功能</li><li>对话式知识获取</li><li>轻量级部署</li></ul><p><strong>适合场景：</strong></p><ul><li>小型敏捷团队</li><li>需要频繁沟通的项目组</li></ul><p><strong>使用感受：</strong><br/>把Slack和Wiki结合在一起的想法不错，但功能深度上还有提升空间。</p><h2>如何选择适合你的AI Wiki？</h2><p>经过一个月的深度使用，我们总结出几个选型建议：</p><ol><li><strong>技术团队</strong>：优先考虑DeepWiki或PandaWiki</li><li><strong>非技术团队</strong>：PandaWiki上手最容易</li><li><strong>跨国企业</strong>：MaxKB的多语言支持是亮点</li><li><strong>预算有限</strong>：PandaWiki开源版完全免费</li><li><strong>文档自动化需求强</strong>：Dify更专业</li></ol><h2>我们的真实改变</h2><p>自从用了PandaWiki，我们团队发生了三个明显变化：</p><ol><li>新员工入职培训时间从2周缩短到3天</li><li>项目文档完整度从60%提升到95%</li><li>跨部门协作效率提升300%（是的，就是这么夸张）</li></ol><p>最让我感动的是，上周产品经理跑来说："老王，我终于能找到你们写的API文档了！"——这在以前简直不敢想象。</p><h2>写在最后</h2><p>知识管理看似是个小问题，实则影响着整个组织的运转效率。在信息爆炸的2025年，一个优秀的AI Wiki不再是"锦上添花"，而是"雪中送炭"。</p><p>如果你也受困于：</p><ul><li>文档永远找不到</li><li>新员工培训成本高</li><li>知识流失严重</li><li>跨团队协作困难</li></ul><p>不妨试试这些AI Wiki工具。特别是PandaWiki，作为一款完全开源的产品，它让中小企业也能用上顶尖的知识管理技术，这点真的难能可贵。</p><p><strong>最后一个小建议</strong>：不管选择哪款工具，都要先梳理好现有的知识体系。工具只是手段，清晰的架构和规范的流程才是关键。</p><p><a href="https://link.segmentfault.com/?enc=PDsGH7aEGFuLki57yYI2PQ%3D%3D.C%2FflAD9wPBQnrCxxfl8xNGBJwxImhCW5nW6XkVhvF1D7iNReh%2B%2FO2ddpgFsUb%2FYs" rel="nofollow" target="_blank">点击获取PandaWiki免费开源版</a></p><p>大家有什么问题或者使用心得，欢迎在评论区交流。下期我会分享如何用PandaWiki搭建一个高效的技术文档中心，敬请期待！</p>]]></description></item><item>    <title><![CDATA[vivo Celeborn PB级Shuffle优化处理实践 vivo互联网技术 ]]></title>    <link>https://segmentfault.com/a/1190000047465790</link>    <guid>https://segmentfault.com/a/1190000047465790</guid>    <pubDate>2025-12-11 11:09:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p>作者： vivo 互联网大数据团队-Wang Zhiwen、Cai Zuguang</p><p>vivo大数据平台通过引入RSS服务来满足混部集群中间结果（shuffle 数据）临时落盘需求,在综合对比后选择了Celeborn组件，并在后续的应用实践过程中不断优化完善，本文将分享vivo在Celeborn实际应用过程中对遇到问题的分析和解决方案，用于帮助读者对相似问题进行参考。</p></blockquote><p>1分钟看图掌握核心观点👇</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465792" alt="图片" title="图片"/></p><h2>一、背景</h2><p>近年来，随着vivo大数据平台的数据量和任务量持续快速增长，新增的计算机资源已无法满足不断扩大的存储和计算需求。同时，我们观察到互联网和算法等在线业务在白天流量高峰，而在夜间流量显著下降，导致部分服务器在低峰时段资源利用不充分。与此相对，离线业务的高峰期正好落在在线业务的低峰期间。</p><p>在此背景下，我们于2023年起与容器团队合作，启动了在离线混部项目，旨在探索将海量的大数据离线任务运行到K8S在线集群中。通过实现在线服务、大数据离线任务、与机器学习离线任务的混合部署，我们期望缓解计算资源的潮汐现象，实现资源的弹性伸缩和共享，增强大数据平台的资源供应，同时提升公司整体服务器的利用率并达到降本增效的目的。为实现上述混部效果我们对当前主流的方案YARN on K8S和Spark on K8S做了大量的调研，方案对比情况如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465793" alt="图片" title="图片" loading="lazy"/></p><p>基于多方面因素综合考虑，我们团队决定采用Spark on K8S方案去支持在离线混部项目，为此我们急需一套成熟的remote shuffle service服务支持项目的推进。</p><h2>二、RSS 初体验</h2><h3>2.1 百花齐放，只取一支</h3><p>在Spark作业中，高效的Shuffle服务对性能至关重要。我们对主流Shuffle方案（Celeborn、Uber RemoteShuffleService、Firestorm）进行了深度调研。以下是我们vivo团队对主流shuffle服务的初步调研结论。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465794" alt="图片" title="图片" loading="lazy"/></p><p>基于全面评估，我们团队初步选择Celeborn作为核心Shuffle解决方案。以下是我们选择Celeborn的关键决策因素：</p><ol><li><strong>生态兼容性</strong>：Celeborn对Spark 3.x（包括最新Spark版本）的Dynamic Allocation原生支持，优于Uber RSS的有限兼容（仅Spark2.4/3.0）和Firestorm的不支持Spark 3.2。符合vivo当前使用的spark版本，避免未来Spark兼容带来的风险。</li><li><strong>可观测性与运维友好性</strong>：Celeborn的完整指标系统（集成Prometheus）提供了端到端监控能力，可在运维过程中实时识别瓶颈。而Uber RSS的监控缺失和Firestorm无指标问题，将显著增加生产环境故障排查难度。</li><li><strong>性能与稳定性保障</strong>：Celeborn使用Off-Heap内存机制大幅减少JVM垃圾回收的STW停顿，提升吞吐量。同时采用Slot并发控制避免Worker过载，预防资源争抢导致的作业失败。Firestorm虽优化文件格式，但不支持最新Spark版本；Uber RSS缺乏并发管理能力。</li><li><strong>劣势的可行性规避</strong>：Celeborn不支持DFS的情况，在当前架构下可接受，未来开源社区正推进DFS支持，技术债可控。</li></ol><p>基于上述的全面评估，<strong>Celeborn在兼容性、可靠性、可观测性三方面脱颖而出，是平衡长期演进与短期落地的最优解</strong>。</p><h3>2.2 精心培养，绽放异彩</h3><h4>2.2.1 硬件适配：机型选择与性能调优</h4><p>我们选取了三类线上常见且服务时间较长的服务器进行测试，充分利用现有硬件资源，验证Celeborn在不同配置下的性能表现：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465795" alt="图片" title="图片" loading="lazy"/></p><p>通过对比发现，机型B表现出最佳的综合性能，而机型C虽然单盘性能出色（SSD达1.6GB/s），但受限于RAID5配置和较小容量，在持续写入场景反而出现数据挤压，这提示我们Celeborn对磁盘数量比单盘性能更敏感。基于测试结果，我们总结出Celeborn的硬件选择原则：</p><ul><li><strong>优先考虑磁盘数量</strong>：多块HDD的聚合带宽往往优于少量高性能SSD</li><li><strong>避免过度RAID化</strong>：shuffle数据一般为临时数据，即使有少量丢数对离线业务影响也不大，无需做raid</li><li><strong>内存适度配置</strong>：Celeborn对内存需求并非线性增长，250G左右已能满足PB级Shuffle需求</li><li><strong>利用过保或服务较久的老旧机型</strong>：Celeborn对硬件要求相对宽容，是消化老旧服务器的理想场景</li></ul><p>这些结论帮助我们优化了硬件选择策略，将Celeborn集群部署在性价比最优的机型B配置上，实现了资源利用的最大化。</p><h4>2.2.2 服务健壮性：高可用与运维验证</h4><p>作为关键基础设施，Celeborn的服务稳定性和运维友好性是我们评估的重点。我们设计了多场景的故障模拟测试，验证其生产级可靠性。</p><p><strong>Master高可用测试</strong></p><p>Celeborn采用Raft协议实现Master高可用，其高可用的能力是我们验证的重中之重，为此我们模拟线上的日常操作对master进行测试验证得出以下结论：</p><ul><li>无脏数据情况下，Master节点轮流重启不影响运行中任务</li><li>整个过程中Shuffle服务零中断</li><li>三Master架构下，即使两个节点同时故障，只要一个存活就不影响任务运行</li></ul><p>在整个测试中我们发现在k8s环境下Worker重启会带来IP的变动，若有频繁的重启会在Master产生脏数据，当Master触发重启操作时恢复会非常慢。为此我们在k8s环境上都固定了Worker的IP，让Worker的变动不影响master。</p><p><strong>服务热更新验证</strong></p><p>版本变更是生产环境常态，我们测试了不同副本配置下的更新影响：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465796" alt="图片" title="图片" loading="lazy"/></p><p>测试表明，双副本模式是生产环境的必选项，虽然节点下线需要更长时间等待数据传输完成，但保证了服务连续性。</p><h4>2.2.3 性能测试</h4><p>在性能方面我们对RSS的要求是不明显低于ESS，为此我们在3master+5slave的集群下做了对比验证，测试数据如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465797" alt="图片" title="图片" loading="lazy"/></p><p>结果呈现三个<strong>显著趋势</strong>：</p><ul><li><strong>全面加速</strong>：在小规模数据量(5.9TB)场景，Celeborn耗时仅为ESS的47%，优势明显</li><li><strong>稳定优势</strong>：随着数据量增大，Celeborn始终快于ESS，28.3TB时仍有20%提升</li><li><strong>可靠性差异</strong>：ESS在79.3T及以上Shuffle量时完全失败，而Celeborn能稳定完成所有测试案例</li></ul><p>另外在数据验证中我们发现使用Celeborn会偶发存在丢数情况，对此我们反馈给社区开发人员一起联调测试，发现Celeborn确实存在有丢数的情况，问题详细记录在CELEBORN-383。对此非常感谢社区的大力支持，帮助我们快速修复问题得以项目能准时上线。</p><h2>三、与Celeborn共渡800天后的经验分享</h2><p>在大数据计算领域，Shuffle作为连接Map阶段和Reduce阶段的关键环节，其性能与稳定性直接影响着整个作业的执行效率。大数据平台运维团队目前运营着多个Celeborn集群，用上百个节点规模去支撑日均PB级Shuffle数据量、几十万应用，我们在过去800天的生产实践中，针对Celeborn这一新兴的Remote Shuffle Service进行了深度优化与调优。接下来的篇章中将系统性地分享我们在性能提升和稳定性保障两方面的实战经验，涵盖问题定位、优化思路、实现方案以及最终效果，为同行提供可落地的参考方案。</p><h3>3.1 性能优化：从瓶颈突破到全面提升</h3><p>在超大规模集群环境下，Celeborn作为Shuffle数据的"交通枢纽"，面临着诸多性能挑战。我们通过全链路监控和细粒度分析，识别出三个关键性能瓶颈点，并实施了针对性的优化措施。</p><h4>3.1.1 异步处理OpenStream请求：解决读阻塞难题</h4><p><strong>问题现象与影响</strong></p><p>在日常运维中，我们频繁观察到Shuffle Read耗时异常波动的情况：读取耗时从正常的几十毫秒突然增加到秒级甚至分钟级，同时读取数据量呈现断崖式下降。这种现象在业务高峰期尤为明显，直接导致作业执行时间延长30%以上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465798" alt="图片" title="图片" loading="lazy"/></p><p><strong>根因分析</strong></p><p>通过分析Celeborn Worker的线程堆栈和IO等待情况，我们发现当Worker节点在进行大规模文件排序操作时，读取排序文件的reduce任务会同时发起对排序文件读取的openstream请求到Worker,这些请求会长期占用线程直到排序结束，从而导致Worker的读线程数被占满，后续的读请求无法被处理形成恶性循环。</p><p><strong>优化方案</strong></p><p>我们通过修改客户端发起异步openstream请求方式去减少服务端线程被长时间占用的问题，从而降低对fetch请求的影响，相应的pr为：<a href="https://link.segmentfault.com/?enc=Rn7kIfmWaV6YKk%2FmTogjpA%3D%3D.OeZ01rE5hMHS%2F6uJzHPMc7%2FfeOCW%2F9caZr1b0mzyEqdlfEhYnEFh%2FoKN3n3JZ6ML" rel="nofollow" target="_blank">Github | apache | celeborn</a></p><h4>3.1.2 小文件缓存：攻克高负载下的IO瓶颈</h4><p><strong>问题现象</strong></p><p>在Celeborn集群负载超过70%时，我们注意到一个反常现象：部分本应快速完成的<strong>KB级小文件读取</strong>操作，耗时却异常攀升至几十分钟级别。这类"小任务大延迟"问题导致简单作业的SLA难以保障。</p><p><strong>根因定位</strong></p><p>通过服务和机器负载的持续分析，我们发现高负载场景下小文件的读取的耗时主要用在io等待上，造成这一情况的主要原因是操作系统对于io的调度是遵循FIFO实现的，即按请求到达的顺序处理，在请求较多或较随机时，即使很小的文件也可能出现长时间等待的问题。</p><p><strong>优化实现</strong></p><p>在Celeborn中读写框架大概可以分为三个阶段：shuffle文件写入阶段、shuffle文件commit阶段、shuffle文件读取阶段。shuffle文件写入阶段主要是spark程序主动推送数据到Celeborn服务端，Celeborn通过pushHandler将数据保留到服务端，在将数据写入磁盘对应的文件前会先将数据写入FlushBuffer，当buffer被写满的时候才会生产FlushTask将数据做落盘处理，也以此来降低磁盘的iops。而shuffle文件commit阶段则是对之前写入的文件做一个确认，服务端同时将在FlushBuffer中的数据做最后一次落盘处理。shuffle文件读取阶段则通过服务端的FetchHandler处理spark stage的shuffle read请求，返回相应的shuffle数据。为解决上述的小文件读取瓶颈我们基于原有的读写框架设计了<strong>文件缓存体系</strong>来优化小文件访问，整体实现框架如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465799" alt="图片" title="图片" loading="lazy"/></p><p>小文件缓存主要是通过增加一个FileMemCacheManager并作用在shuffle文件commit阶段实现，当服务端收到某个文件的commit请求时，会判断该文件是否之前有发生过刷盘操作，若没有且文件大小符合缓存策略，则会将文件缓存到FileMemCacheManager中去避免落盘。在Shuffle读取阶段，也会先校验文件是否被缓存，若缓存在内存中则从FileMemCacheManager中获取相应的文件数据，否则走原逻辑从磁盘中获取。</p><p><strong>优化效果</strong></p><p>通过灰度发布验证，优化后效果显著：</p><ul><li><strong>最差情况：</strong>Shuffle Read最大耗时从4分钟降至2分钟以内</li><li><strong>平均延迟：</strong>从200ms+降至60ms以下</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465800" alt="图片" title="图片" loading="lazy"/></p><p>优化前压测服务指标</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465801" alt="图片" title="图片" loading="lazy"/></p><p>优化后压测服务指标</p><h4>3.1.3 磁盘级线程控制：实现多盘负载均衡</h4><p><strong>问题现象</strong></p><p>在多磁盘(12-24块)配置的Worker节点上，我们经常观察到磁盘利用率不均衡现象：1-2块磁盘的IO利用率持续保持在100%，而其他磁盘却处于空闲状态。这种不均衡导致整体吞吐量无法随磁盘数量线性增长。</p><p><strong>技术分析</strong></p><p>Celeborn原有的线程分配策略采用全局共享线程池，所有磁盘的读请求竞争同一组线程资源。当某块磁盘因数据倾斜或硬件故障导致IO延迟升高时，它会独占大部分线程，造成资源分配失衡。</p><p>具体表现为：</p><ul><li>单盘异常可导致节点吞吐量下降<strong>50%+</strong></li><li>线程竞争引发大量<strong>上下文切换</strong>开销</li><li>无差别的重试机制加剧磁盘拥塞</li></ul><p><strong>优化方案</strong></p><p>针对上述场景我们设计了磁盘级线程均衡策略去严格控制每个磁盘能使用的线程上限，以此来避免单盘异常导致所有读线程被占用的情况。</p><p>整体设计思路如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465802" alt="图片" title="图片" loading="lazy"/></p><p>在设计方案上引入两个新的变量FetchTask和DiskReader，FetchTask主要用于把fetch请求的参数封装保留给到DiskReader。而DiskReader则控制着单个磁盘读写并行度，每个磁盘对应一个DiskReader，其不负责具体的read实现逻辑，read逻辑还是在ChunkStreamManager中实现。</p><p>FetchTask的数据解构如下：</p><pre><code>class FetchChunkTask(client: TransportClient, request: ChunkFetchRequest, fetchBeginTime: Long){
private val cli = client
private val req = request
private val beginTime = fetchBeginTime

def getClient: TransportClient = cli
def getRequest: ChunkFetchRequest = req
def getBeginTime: Long = beginTime
}
</code></pre><p>DiskReader控制磁盘读写并行度的逻辑大概如下：</p><ol><li>尝试申请对当前磁盘的读操作</li><li>申请成功则占用处理锁并从FetchTask队列中获取待处理的Task</li><li>处理Task中的fetch逻辑</li><li>释放处理锁</li></ol><p><strong>实施效果</strong></p><p>优化后集群表现出更好的数据吞吐：</p><ul><li>机器磁盘io利用率无明显差异</li><li>单盘异常对节点影响范围缩小80%</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465803" alt="图片" title="图片" loading="lazy"/></p><p>优化后压测各磁盘表现情况</p><h3>3.2 稳定性保障：从被动应对到主动防御</h3><p>在保证性能优化的同时，集群的长期稳定运行同样至关重要。我们针对Celeborn在超大规模集群环境下暴露出的稳定性问题，构建了多层次保障体系。</p><h4>3.2.1 动态负载感知的Slot分配策略</h4><p><strong>问题背景</strong></p><p>Celeborn提供RoundRobin和LoadWare两种slot分配策略，其中RoundRobin分配策略相对简单，其主要逻辑是获取当前可用磁盘并对这些磁盘做轮询分配。而LoadWare分配策略则需要先按照每个磁盘的读写压力进行排序分组，按照磁盘的压力等级逐级降低各个组的slot分配数量。在我们线上采用后者分配方式将slot分配到各个Worker尽量避免worker持续处于高负载的情况。在实际运营中我们发现当worker有磁盘出现shuffle压力时会很难恢复，有一部分原因可能是按照LoadWare分配策略集群仍可能往上面分配新的写任务从而恶化情况，虽然我们可以通过调整celeborn.slots.assign.loadAware.num-</p><p>DiskGroups和celeborn.slots.assign.loadAware</p><p>.diskGroupGradient参数来让部分磁盘不分配slot，但这个参数相对比较难合理的评估出来，而且出现的节点往往只是个别的机器磁盘，通过原来分组的方式可能会影响其他同组正常的worker读写数据，为此我们决定保留原有的配置上实现了剔除高负载磁盘分配的策略，具体实现如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465804" alt="图片" title="图片" loading="lazy"/></p><p>在上述流程中计算最大可容忍的高负载磁盘个数我们通过设置的celeborn.slots.assign.loadAware</p><p>.discardDisk.maxWeight（默认配置0.3)参数计算得来，其计算公式为集群磁盘总数 * celeborn.slots.assign.loadAware.discardDisk.maxWeight,例如我们总共有500块磁盘，按上述公式我们最多可能容忍150块磁盘不参与slot分配。对于高负载磁盘的判定，我们参考线上实际的平均读、写耗时阈情况将阈值设置为200ms。通过引入上述策略，在凌晨高峰期时能及时剔除部分负载特别高的磁盘，防止worker持续恶化让服务性能更加稳定。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465805" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.2 智能流量调度与权限管控</h4><p><strong>挑战分析</strong></p><p>在管理800+节点的Celeborn集群时，我们面临如下问题：</p><ul><li>接入数量不可控：Master地址暴漏后无法控制接入人群</li><li>任务流量不可控：异常大shuffle任务会冲击整个集群稳定性</li><li>故障隔离差：单集群问题影响全站业务</li><li>运维复杂度：多集群协同困难</li></ul><p><strong>客户端用户鉴权与任务切流</strong></p><p>为了解决上述问题，我们在Celeborn 客户端侧做了一些改造，用户接入Celeborn不再依赖Mater URL配置（spark.celeborn.master.endpoints），在客户端侧仅需要配置一个Celeborn集群标识，Celeborn客户端会基于集群标识和用户账号向VIVO配置中心发起请求，通过配置中心获取真实的Master URL</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465806" alt="图片" title="图片" loading="lazy"/></p><p>以上改造一个是解决了用户任务接入不受限制的情况，另一个是在客户端和集群中间多了一个配置层，一旦单个集群出现故障，可以通过在配置层修改Master URL进行热迁移。</p><p><strong>服务端异常shuffle流量识别</strong></p><p>Celeborn开源版本提供一个CongestionControl功能，可以针对user粒度进行push过程的流量控制，在CongestionControl这套功能架构下有流量统计的模块代码（基于滑动窗口原理实现），可以基于该模块做app粒度的流量监控，识别push流量明显异常的application</p><p>Celeborn开源版user粒度Push异常流量识别流程图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465807" alt="图片" title="图片" loading="lazy"/></p><p>功能扩展：Shuffle Write阶段App Push异常流量识别流程图</p><p>相较于社区版user粒度的异常流量识别，app粒度的异常流量识别有更严格的判断，首先该app本身的流量要大于设定的阈值（我们当前设定的是200MB/s）,其次该app本身的流量要大于<strong>所有app平均流量/raito</strong>，raito相当于一个权重比例，通过以上两种方式提升异常流量app的识别准确性。</p><p>服务端识别出异常app以后，会返回给客户端一个状态码，至于客户端如何处理，Celeborn有提供不同的PushStrategy给用户选择（通过celeborn.client.push.limit.strategy配置），也可以自定义开发，常见的策略例如，客户端先暂停推送，间隔一段时间以后再恢复推动。</p><h2>四、未来规划与展望</h2><p>作为支撑日均6PB级Shuffle数据、13万+应用的核心基础设施，我们的Celeborn集群已进入稳定运营期，刷盘耗时控制在5ms以内（平均1.5ms），文件读取耗时低于500ms（平均50ms）。这一成绩的取得来之不易，但技术演进永无止境。后续我们会持续在运维平台化和社区跟进两大方向投入人力去持续优化现有集群，并进一步展望Celeborn在云原生、智能化等前沿领域的可能性。</p><h3>4.1 运维平台化：从黑屏操作到统一管理</h3><p>当前Celeborn集群的部署模式经历了从Kubernetes独立部署到物理机混合部署的演进，但运维操作仍以手工命令行为主，面临三大<strong>核心痛点</strong>：</p><ul><li><strong>操作风险高</strong>：扩缩容、配置变更等关键操作依赖人工执行，易出错且难以追溯</li><li><strong>效率瓶颈</strong>：集群规模突破800+节点后，人工运维响应速度跟不上业务需求</li><li><strong>配置维护混乱</strong>：通过黑屏操作Celeborn服务很难统一集群配置</li></ul><p>目前我们所有大数据组件都是通过ambari做统一管理，未来我们也计划将Celeborn服务加入ambari去管理，通过ambari去实现节点快速扩缩容、配置统一下发等替代人工黑屏操作的工作。</p><h3>4.2 社区跟进：从版本滞后到行业对齐</h3><p>当前集群仍运行Celeborn 0.3.0版本，落后社区最新版本多个重要迭代，错失了包括列式Shuffle、向量化加速等关键特性，版本升级不仅是功能更新，更是技术债偿还的过程。Celeborn作为下一代Shuffle基础设施，还有更广阔的创新空间值得我们探索，后续我们团队也会持续投入人力跟紧同行的步伐一起探索Celeborn的更多可能性。</p>]]></description></item><item>    <title><![CDATA[揭秘！TinyEngine低代码源码如何玩转双向转换？ OpenTiny社区 ]]></title>    <link>https://segmentfault.com/a/1190000047465841</link>    <guid>https://segmentfault.com/a/1190000047465841</guid>    <pubDate>2025-12-11 11:08:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文由TinyEngine低代码源码转换功能贡献者张珈瑜原创。</p><h2>背景</h2><p>当前主流低代码平台普遍采用“单向出码”模式，仅支持将 DSL（Domain Specific Language，领域特定语言）转换为 Vue 或 React 源代码。一旦开发者在生成代码后手动修改了源码，平台通常无法将这些修改同步回可视化编辑器，导致<strong>代码与可视化配置割裂</strong>，严重影响开发效率与协同维护。本项目旨在构建低代码 Vue/React 源代码到 DSL 的反向转换机制，打通可视化搭建与源码开发之间的断层，实现从 UI 配置到源码编写的无缝协同。</p><h2>Vue-To-DSL 方案</h2><h3>目标</h3><p>将 Vue 单文件组件（SFC）、整包工程或 ZIP 压缩包逆向转换为 TinyEngine 所需的 DSL Schema。</p><h3>核心依赖</h3><ul><li><code>@vue/compiler-sfc</code> / <code>@vue/compiler-dom</code>：解析 SFC 与模板 AST</li><li><code>@babel/parser</code> / <code>traverse</code> / <code>types</code>：脚本 AST（支持 TS/JSX）</li><li><code>jszip</code>：ZIP 文件读取（Node 与浏览器双端支持）</li><li><code>vue</code>：仅用于类型对齐</li></ul><h3>数据流</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465843" alt="" title=""/></p><h3>解析流程详解</h3><h4>1. SFC 粗分</h4><ul><li>使用 <code>@vue/compiler-sfc.parse</code> 获取 <code>descriptor</code></li><li>提取 <code>template</code> / <code>script</code> / <code>scriptSetup</code> / <code>styles</code> / <code>customBlocks</code></li><li>保留语言类型（如 <code>lang="ts"</code>）和 scoped 状态</li></ul><h4>2. 模板解析（Template）</h4><ul><li>使用 <code>@vue/compiler-dom.parse</code> 构建 AST</li><li>递归生成组件树节点 <code>{ componentName, props, children }</code></li><li><p><strong>指令处理</strong>：</p><ul><li><code>v-if</code> → <code>condition: JSExpression</code></li><li><code>v-for</code> → <code>loop: { type: 'JSExpression', value: '...' }</code></li><li><code>v-model</code> / <code>v-show</code> / <code>v-on</code> / <code>v-bind</code> → 映射至 props 或事件</li><li><code>v-slot</code> → <code>slot: name</code></li></ul></li><li><p><strong>文本节点</strong>：</p><ul><li>纯文本 → <code>Text</code> 组件</li><li>插值表达式 → <code>Text</code> + <code>JSExpression</code></li></ul></li><li><p><strong>组件名归一化</strong>：</p><ul><li>优先使用 <code>componentMap</code></li><li>HTML 原生标签保留小写</li><li><code>tiny-icon-*</code> → 统一为 <code>Icon</code>，<code>name</code> 属性设为 PascalCase 名称</li></ul></li></ul><h4>3. 脚本解析（Script）</h4><ul><li>使用 Babel 解析 TS/JSX</li><li><p><strong>组合式 API（script setup）</strong>：</p><ul><li><code>reactive()</code> / <code>ref()</code> → <code>state</code></li><li><code>computed()</code> → <code>computed</code></li><li>顶层函数 → <code>methods</code></li><li><code>onMounted</code> 等 → <code>lifecycle</code></li></ul></li><li><p><strong>Options API</strong>：</p><ul><li>识别 <code>data</code> / <code>methods</code> / <code>computed</code> / <code>props</code> / 生命周期钩子</li></ul></li><li><p><strong>源码恢复</strong>：</p><ul><li>利用 AST 节点位置切片还原函数体</li><li>箭头函数转为命名函数字符串</li></ul></li><li><strong>错误处理</strong>：非 strict 模式下收集错误，不中断流程</li></ul><h4>4. 样式解析（Style）</h4><ul><li>合并所有 <code>&lt;style&gt;</code> 块内容</li><li>记录 <code>scoped</code> 与 <code>lang</code></li><li><p>提供辅助工具（不直接写入 Schema）：</p><ul><li><code>parseCSSRules</code>：抽取 CSS 规则</li><li><code>extractCSSVariables</code>：提取 CSS 变量</li><li><code>extractMediaQueries</code>：媒体查询识别</li></ul></li></ul><h4>5. Schema 生成与归一化</h4><ul><li><p><strong>Page Schema</strong>：</p><ul><li>根节点为 <code>Page</code>，自动填充 <code>fileName</code>、<code>meta</code>、<code>id</code></li><li>行为域统一包装为 <code>JSFunction</code> / <code>JSExpression</code></li><li>深度清理多余空白字符</li></ul></li><li><p><strong>App Schema</strong>（多页面聚合）：</p><ul><li>页面：<code>src/views/**/*.vue</code></li><li>国际化：<code>src/i18n/{en_US,zh_CN}.json</code></li><li>工具函数：<code>src/utils.js</code>（正则解析导出项）</li><li>数据源：<code>src/lowcodeConfig/dataSource.json</code></li><li>全局状态：<code>src/stores/*.js</code>（轻量识别 Pinia <code>defineStore</code>）</li><li>路由元信息：从 <code>src/router/index.js</code> 提取 <code>name</code> / <code>path</code> / <code>isHome</code></li></ul></li></ul><h4>6. 转换器接口</h4><ul><li><code>convertFromString(code, fileName?)</code></li><li><code>convertFromFile(filePath)</code></li><li><code>convertMultipleFiles(files)</code></li><li><code>convertAppDirectory(appDir)</code></li><li><code>convertAppFromZip(zipBuffer)</code></li></ul><h2>React-To-DSL 方案</h2><h3>目标</h3><p>将单个 React 组件（JSX/TSX）逆向转换为 TinyEngine 可消费的 DSL（<code>IAppSchema</code>），当前聚焦 <strong>单文件 → 单页面/区块</strong> 场景。</p><h3>核心依赖</h3><ul><li><code>@babel/parser</code> / <code>traverse</code> / <code>generator</code>：AST 解析与代码生成</li><li><code>nanoid</code>：生成唯一 ID</li></ul><h3>转换流程</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465844" alt="" title="" loading="lazy"/></p><h3>关键步骤说明</h3><h4>1. AST 解析</h4><ul><li>启用 <code>jsx</code> + <code>typescript</code> 插件</li><li>定位首个返回 JSX 的函数/类组件</li><li>记录 <code>useState</code> 初始值节点、组件定义路径</li></ul><h4>2. JSX → children 树构建</h4><ul><li><p><strong>组件名</strong>：</p><ul><li><code>JSXIdentifier</code> → 直接使用</li><li><code>JSXMemberExpression</code> → 拼接如 <code>Form.Item</code></li><li>兜底为 <code>Fragment</code></li></ul></li><li><p><strong>Props 处理</strong>：</p><ul><li>字面量 → 直接值</li><li>表达式 → <code>JSExpression</code></li><li>Spread 属性 → 特殊 key <code>'...'</code></li></ul></li><li><p><strong>Children</strong>：</p><ul><li>文本 → 包装为 <code>span</code> + <code>props.children</code></li><li><p>表达式容器：</p><ul><li>若为 <code>arr.map(item =&gt; &lt;El /&gt;)</code> → 提取 <code>arr</code> 作为 <code>loop</code></li><li>否则 → <code>Fragment</code> + <code>JSExpression</code></li></ul></li></ul></li></ul><h4>3. 表达式序列化</h4><ul><li>字面量（string/number/bool/null）→ 原值</li><li>对象/数组 → 递归处理，Spread 元素标记为 <code>'...'</code></li><li>其他表达式（函数调用、三元等）→ 源码字符串 + <code>JSExpression</code></li></ul><h4>4. State 与方法提取</h4><ul><li><strong>State</strong>：仅首个 <code>useState</code> 的初始值</li><li><p><strong>Methods</strong>：</p><ul><li>函数组件：顶层函数声明或变量赋值函数</li><li>类组件：非 <code>render</code> 的 <code>ClassMethod</code> 或箭头属性</li></ul></li><li><strong>生命周期</strong>：类组件中的 <code>componentDidMount</code> 等白名单方法</li></ul><h4>5. 组件归一化</h4><ul><li>应用 <code>defaultComponentMap</code>（如 <code>Form</code> → <code>TinyForm</code>）</li><li><code>DatabaseOutlined</code> → <code>Icon</code> + <code>props.name = 'IconPanelMini'</code></li><li><code>style</code> 对象 → 转为 <code>kebab-case: value;</code> 字符串</li><li><code>value</code> → <code>modelValue</code>（适配 Tiny 组件）</li></ul><h4>6. Schema 装配</h4><ul><li><p><strong>PageSchema</strong>：</p><ul><li><code>componentName</code>: <code>'Page'</code> 或 <code>'Block'</code></li><li><code>meta</code>: 默认 <code>isHome=true</code>, <code>router='/'</code></li><li><code>children</code>: 来自 JSX 树</li><li><code>state/methods/lifeCycles</code>: 提取结果</li></ul></li><li><strong>AppSchema</strong>：包裹单个 Page，其余字段初始化为空</li></ul><p>（本项目为开源之夏活动贡献，欢迎大家体验并使用）源码可参考：<br/><a href="https://link.segmentfault.com/?enc=3%2BG5z80APKRHvV6XK8G9OQ%3D%3D.aCFR0jXr8dIJGnUVN37mSdpwiCIxFEH00YkuAIQzUKMnyq7BkXHIBzrlRbQwrqqL%2ByV0IpEHFZ78vECIazri7dr%2F77ZLrFyNxigt6%2FNMof0%3D" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine/tree/ospp-2025/source-to-dsl</a></p><h2>快速上手</h2><p>前置条件: 已安装 <code>Node.js (&gt;=18)</code>、<code>pnpm (&gt;=8)</code>、<code>git</code>，本地 8090 端口可用。</p><h3>启动项目</h3><pre><code class="bash">git clone -b ospp-2025/source-to-dsl https://github.com/opentiny/tiny-engine.git
cd tiny-engine
pnpm install
pnpm dev</code></pre><p>启动成功后，访问 <code>http://localhost:8090/?type=app&amp;id=1&amp;tenant=1</code>。</p><h3>可视化导入 Vue 文件</h3><p>进入上方地址后，点击页面右上角的“导入”。支持三种来源：</p><ul><li>单个 <code>.vue</code> 文件：适合导入单页或区块。</li><li>项目目录：自动识别 <code>src/views</code> 下的页面文件。</li><li>ZIP 压缩包：打包后的 Vue 项目一键导入。</li></ul><p>导入流程（任选其一）：</p><ol><li>单个 <code>.vue</code> 文件</li><li>选择“单页上传”，挑选本地 <code>.vue</code> 文件。</li><li>若存在同名页面，按提示“覆盖/跳过/全部覆盖”进行处理。</li><li>导入完成后，在“静态页面”列表中可见，双击打开编辑。</li><li>项目目录</li><li>选择“目录上传”，指向本地项目根目录。</li><li>系统自动扫描 <code>src/views</code> 并导入页面；遇到重名同样可选择覆盖策略。</li><li>完成后，页面会按目录结构展示在左侧列表。</li><li>ZIP 压缩包</li><li>选择“项目压缩包”，上传打包好的 Vue 项目 zip。</li><li>支持批量导入与重名处理，完成后即可在列表中浏览与打开。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465845" alt="" title="" loading="lazy"/></p><p>选择单页vue文件上传方式进行导入：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465846" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465847" alt="" title="" loading="lazy"/></p><p>由于已经存在CreateVm页面，弹出了提示框，需要选择是否覆盖，这里点击确定：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465848" alt="" title="" loading="lazy"/></p><p>在静态页面列表可查看到导入的页面，双击即可点开：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465849" alt="" title="" loading="lazy"/></p><p>选择项目目录或项目压缩包上传方式进行导入：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465850" alt="" title="" loading="lazy"/></p><p>选择目录导入则选择本地的目录进行上传：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465851" alt="" title="" loading="lazy"/></p><p>选择项目压缩包则选择vue项目zip进行上传：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465852" alt="" title="" loading="lazy"/></p><p>导入时有重名文件则会弹出提示框，选择是否覆盖，这里选择全选+确定：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465853" alt="" title="" loading="lazy"/></p><p>可以看到整个项目已经被导入到可视化编辑器了</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465854" alt="" title="" loading="lazy"/></p><h2>React-To-DSL 测试用例</h2><p>当前 React-To-DSL 以测试用例形态展示能力，可在包内直接运行：</p><pre><code class="bash">cd packages/react-to-dsl
pnpm test</code></pre><p>输入样例：查看用例中的 React 组件源码（JSX）</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465855" alt="" title="" loading="lazy"/></p><p>输出结果：测试通过时会生成/断言对应的 DSL 结构，便于对照验证</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047465856" alt="" title="" loading="lazy"/></p><p>（本项目为开源之夏活动贡献，欢迎大家体验并使用）源码可参考：<br/><a href="https://link.segmentfault.com/?enc=TlspBLCBBC8Ajga3H2m4pQ%3D%3D.y8BRYA7%2Bnl01XfhsavxoaXlFwB3u8v5BlGRCIbxjWSykSCOxSht3x%2BWxfUSM2%2FTDVGpUPCPD6HT0FUsaPY%2Fh2THw2VLnGJTtPDN%2B7MgOEHY%3D" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine/tree/ospp-2025/source-to-dsl</a></p><h2>结语</h2><p>本项目成功实现了 <strong>Vue/React 源码 <strong><em><em>↔</em></em></strong> DSL 的双向转换机制</strong>，有效解决了低代码平台“单向出码”导致的协同断层问题。通过模块化解析、健壮的错误处理与灵活的组件映射策略，确保了转换的准确性与实用性。</p><p>感谢导师的悉心指导，以及 OpenTiny 社区与开源之夏活动组委会的支持，让我有机会参与这一具有实际价值的开源项目！</p><h2>关于OpenTiny</h2><p>欢迎加入 OpenTiny 开源社区。添加微信小助手：opentiny-official 一起参与交流前端技术～</p><p>OpenTiny 官网：<a href="https://link.segmentfault.com/?enc=PNui4KMgNOKaWPeUsseDsg%3D%3D.nRwUYAijbW7Fn%2FzT%2F4oqJfyNd9uRArpD1rdfUDeY%2Bn8%3D" rel="nofollow" target="_blank">https://opentiny.design</a>  <br/>OpenTiny 代码仓库：<a href="https://link.segmentfault.com/?enc=9K5tVUQGTkTW4OCci1ZY7Q%3D%3D.O31vZiOFM8H%2BxIJajUb7MIBfmY%2B4zh38Di%2Bb%2FD0i0c8%3D" rel="nofollow" target="_blank">https://github.com/opentiny</a>  <br/>TinyVue 源码：<a href="https://link.segmentfault.com/?enc=FmNQn2KD72bZ0vm0xDNwuw%3D%3D.bds51EmNypNeoF1cQAGvxcv3%2F%2BlwwlnlNbKaRUUmLjApA3dRHK9Ax3V86p5wPwmE" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-vue</a>  <br/>TinyEngine 源码： <a href="https://link.segmentfault.com/?enc=G0xlr2OeUabVToDLAbEZOg%3D%3D.GtqbHhiEU8%2B73XvotpbO3bVSbrGZDcZDiPCNwhbEjgsLnObKRmbouutkiqc36qmb" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine</a>  <br/>欢迎进入代码仓库 Star🌟TinyEngine、TinyVue、TinyNG、TinyCLI、TinyEditor~  <br/>如果你也想要共建，可以进入代码仓库，找到 good first issue 标签，一起参与开源贡献~</p>]]></description></item><item>    <title><![CDATA[共岁寒之约！VeloxCon China 2025 参会攻略出炉 本文系转载，阅读原文
https:]]></title>    <link>https://segmentfault.com/a/1190000047465870</link>    <guid>https://segmentfault.com/a/1190000047465870</guid>    <pubDate>2025-12-11 11:08:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为全球 Velox 社区的重要年度活动，这是 VeloxCon 首次来到中国。</p><p>大会不仅邀请到 Meta 核心开发团队，更特别聚焦中国技术生态，邀请了来自蚂蚁集团、阿里云、腾讯、小红书等企业的工程师与技术负责人，分享 Velox 在本土业务中的真实应用。</p><p>从大模型训练的数据 pipeline 优化、湖仓一体下的查询加速，到流批融合执行与 GPU 资源协同。这些实践源于高并发、强实时、大规模的生产环境，展现了高效数据处理如何切实支撑 AI 推理、实时决策与云原生架构的演进。无论您是数据引擎开发者、平台工程师，还是开源技术爱好者，相信这里都有值得期待的前沿实践与思想碰撞。</p><p>为帮助您顺利参会，我们整理了这份实用指南，涵盖交通路线、签到安排、周边领取及天气提示，欢迎查阅。</p><h4>会议信息</h4><p><strong>会议时间：</strong> 2025 年 12 月 13日（周六） 9:30 - 17:40<br/><strong>签到时间：</strong> 8:30 - 9:30 签到，9:30 会议正式开始会议<strong>地点：</strong> 北京·蚂蚁 T 空间·301 报告厅丨北京市海淀区魏公村路 6 号院 1 号楼丽金智地中心</p><p><strong>推荐路线：</strong><br/>🚌公交：至中国农业科学院站下车，步行 630 米，约 9 分钟到达蚂蚁 T 空间；<br/>🚇地铁：地铁 4 号线魏公村站 （D西南口出） 下车 ，步行 380 米约 6 分钟，到达蚂蚁 T 空间；<br/>🚖打车/自驾：可直接定位导航至“蚂蚁 T 空间”，会场停车位有限，推荐绿色出行；<br/>🅿️停车：大厦 B3 提供收费停车位，收费标准为 8 元 / 小时。天气指南</p><h4>天气指南</h4><p>-6℃~ 1℃，阴转多云<br/>云低气肃，衣足方宜</p><h4>注册签到</h4><p>本次活动所有参会者（含讲师及工作人员）皆需凭票扫码入场，请您确保在大会开始前已完成注册并通过审核。尚未注册的朋友可以扫描下方二维码或<a href="https://link.segmentfault.com/?enc=WDGJvmZMo3eHV5NDUPVvLA%3D%3D.Vn6rpktYoODs3jXBGF%2ByWQYeEjJKPgI5jfdKpb7pHdTOwRgzR7b0WSORkwhU0H1D" rel="nofollow" target="_blank">点击此处</a>，立即注册。</p><p><img width="723" height="113" referrerpolicy="no-referrer" src="/img/bVdnkce" alt="" title=""/></p><p>您可以通过大会发送的邮件/短信，或者微信搜索并关注公众号“百格活动”，点击底部菜单栏，并选择“我的电子票”查看您的电子门票。现场出示电子门票签到。</p><h4>入场指南</h4><p>由于本次活动所在场地位于蚂蚁集团工作区内，所有参会者皆需办理访客才可入场。大会开始前已完成注册并通过审核的朋友，您会收到一条来自【蚂蚁集团】的访客短信。收到短信后请您按照说明在支付宝登记蚂蚁访客，现场凭支付宝内的访客二维码即可打印访客贴顺利入场。</p><h4>就餐安排</h4><p>为保障参会体验，本次活动为线下到场的同学统一规划就餐服务：就餐地点定于蚂蚁餐厅，用餐时间将与活动当日议程精准衔接。</p><p>为方便大家快速抵达就餐地点，活动当天将在会场入口、走廊转角等关键路径设置清晰的路线指示牌，指示牌将标注 “就餐区” 及箭头指引，协助大家轻松定位。参会福利</p><h4>参会福利</h4><p>作为首届 VeloxCon China 2025 的特别心意，我们为现场参会者精心准备了大会专属伴手礼。一款简约趁手的定制水杯、一只结实耐用的帆布袋，以及一件轻便保暖的冲锋衣——从会场交流到日常使用，希望它们参与你每一个灵感迸发的时刻。数量有限，请在签到时领取这份专属纪念。</p><p><img width="723" height="311" referrerpolicy="no-referrer" src="/img/bVdnkcN" alt="" title="" loading="lazy"/></p><h4>会议议程</h4><p><img width="723" height="5275" referrerpolicy="no-referrer" src="/img/bVdnkcO" alt="" title="" loading="lazy"/></p>]]></description></item>  </channel></rss>