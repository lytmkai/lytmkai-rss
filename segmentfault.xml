<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[AI杀死了AI！Cloudflare全球]]></title>    <link>https://segmentfault.com/a/1190000047413338</link>    <guid>https://segmentfault.com/a/1190000047413338</guid>    <pubDate>2025-11-20 10:08:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>编辑：定慧</p><p>【新智元导读】一次「常规更新」搞崩半个地球，Cloudflare CTO紧急谢罪：我们搞砸了！Cloudflare自杀式Bug引发连锁反应，波及全球20%网站。当数百万爬虫撑爆了防御名单，Cloudflare的崩溃揭示了AI时代最深的基建隐忧，人类还能跟得上AI进化的脚本吗？</p><p>果然这个世界建立在脆弱性之上。</p><p>昨天，2025年11月18日，全球AI数字生态系统经历了一次近乎心跳骤停般的休克。</p><p>Cloudflare崩溃了！</p><p>然后全球五分之一的互联网服务几乎全部宕机，尤其是，<strong>AI巨头集体断网！ChatGPT、X全线崩盘！</strong></p><p>当你还在拿着这个图调侃的时候。。。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413340" alt="" title=""/></p><p>殊不知，真正的情况是，Cloudflare现在互联网真正的底座。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413341" alt="" title="" loading="lazy"/></p><p>马斯克在之前亚马逊宕机时还调侃，这次终于是风水轮流转！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413342" alt="" title="" loading="lazy"/></p><p>只不过，老马没想到的是，这次CF的影响直接让自己家服务也宕机了～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413343" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413344" alt="" title="" loading="lazy"/></p><p>Cloudflare的CTO赶紧发了个申明：是我们的错误，立正挨打。</p><p>下面会详细介绍这次引发故障的原因，简直就是草台班子级别，只能说人类社会用规则来和计算机打交道还是太脆弱了！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413345" alt="" title="" loading="lazy"/></p><p>先说说这次事故的影响。</p><p>这起事故影响范围之广前所未见，被称为<strong>「半个互联网的停摆」毫不夸张</strong>——<strong>约20%的网站依赖Cloudflare提供服务</strong>。</p><p><strong>Cloudflare还有个称号叫做</strong>赛博活佛，特别是在极客群体中，很多服务，如果个人用都是免费的。</p><p>这里作为对比，必须夸一下咱们国内的基建服务了，不管是微信、B站、视频网站，你很少碰到如此级别大规模的故障。（ToB的服务不细讲，这个离普通消费者也很远）</p><p>故障高峰时，网站故障追踪平台Downdetector收到了<strong>累计逾210万条</strong>报错反馈，成为近年来最严重的基础设施级中断之一。</p><p>包括亚马逊、Spotify、Zoom、Uber等知名服务也受到波及（部分功能异常或加载缓慢）。</p><p>Cloudflare作为支撑全球海量流量的「隐形基建」，一次失误便牵一发而动全身，令股价盘中一度重挫约7%。</p><p>更令人深思的是，一些本用于监测网络故障的工具（如Downdetector）因本身也使用Cloudflare，甚至在事故中一同瘫痪—整个互联网生态对单一底层的依赖程度，由此可见一斑。</p><p>更「细思极恐」的是，当Cloudflare的工程师想要打开ChatGPT来修复故障时，AI也宕机了～</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413346" alt="" title="" loading="lazy"/></p><p>很多网友都形容那宕机的三个小时，如此「黑暗」，就像回到了远古时期。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413347" alt="" title="" loading="lazy"/></p><p>Cloudflare这次导致全球断网的技术故障，其实是一次典型的「好心办坏事」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413348" alt="" title="" loading="lazy"/></p><p>网友们制作的恶搞图</p><p>简单来说，就是工程师试图升级安保系统的权限，却意外让负责安检的软件「吓晕」了。</p><p><strong>（如果是AI来操作，以硅基同步的能力和运算的能力，大概率不会出这种岔子，这也就是碳基人类写下的固定规则才能导致了，还是人类规则太脆弱了。这里让我联想到马斯克为啥一直坚持FSD使用纯视觉，就是人类你不可能遍历所有驾驶过程，就像这次CF的故障，没有工程师能预先为这种场景写下规则）</strong></p><p>根据Cloudflare官方博客的复盘，事情是这样发生的：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413349" alt="" title="" loading="lazy"/></p><p>起因是系统「大扫除」。</p><p>工程师原本在进行一项常规的维护工作，目的是为了提高安全性。他们调整了数据库的权限，想把原本公用的「系统账号」改成责任更明确的「个人账号」。</p><p>然后这个看似无足轻重的操作，触动了隐藏在系统中「旧伤」。</p><p>系统里潜伏着一段很老的代码，它负责生成一份用来识别网络机器人的「特征名单」（Feature File）。</p><p>这段代码以前只在一个默认的数据库里找名单，所以没问题。</p><p>但这次权限升级后，它突然能看到另一个备份数据库了。</p><p>由于代码里没写清楚「只看哪一个」，它傻乎乎地把两边的名单都抓取了过来，名单被「膨胀」了。</p><p>这导致那份原本精简的「特征名单」瞬间膨胀，内容重复了一倍。</p><p>结果就是，保安「罢工」。</p><p>Cloudflare负责在全球各地转发流量的核心软件有一个硬性规定：为了保证速度，名单长度不能超过200条。</p><p>当这份意外「发福」的名单被推送到全球服务器时，软件发现名单太长读不完，直接触发了内存溢出保护机制（Panic），也就是彻底崩溃。</p><p><strong>为了安全起见，它切断了所有连接。</strong></p><p>简单说就是，本来机器数据库权限不够，调整后，它突然权限高了点，然后也没有为这个情况提前写下判断代码。</p><p><strong>打个再通俗的比方（可能不是那么准确）。</strong></p><p>这就好比大楼物业给保安发了一副新眼镜（升级权限），本意是让他看得更清楚。</p><p>结果因为新眼镜度数没调好，保安看手里的「访客黑名单」时出现了重影，原本100人的名单在他眼里变成了200人。</p><p>保安的脑容量（系统限制）记不住这么多人，瞬间由于信息过载而「死机」晕倒，<strong>导致大楼门禁系统自动锁死，把所有访客（包括X和ChatGPT的用户）都关在了门外。</strong></p><p>不过目前问题已经修复了（其实不是啥大问题，就是逻辑改改就行）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413350" alt="" title="" loading="lazy"/></p><p>如果不只是把这次事故看作一个单纯的技术故障，而是放在2025年「AI疯狂吞噬数据」的背景下去看，你会发现这充满了黑色的讽刺意味。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413351" alt="" title="" loading="lazy"/></p><p><strong>AI杀死AI</strong></p><p>导致这次崩溃的核心组件是「机器人管理系统（Bot Management）」。</p><p>在2025年，这个系统的主要假想敌是谁？<strong>正是AI爬虫。</strong></p><p>随着大模型训练对数据的极度渴求，互联网上充斥着无数自动化的AI抓取程序。</p><p>Cloudflare作为「守门人」，必须不断升级其算法来区分「真人」和「AI机器人」。</p><p><strong>特征文件（Feature File），对就是</strong>报告中提到的那个导致崩溃的「特征文件」，实际上就是机器学习模型用来判断流量性质的「参数集」。</p><p>每一个「特征（Feature）」都是一个判断维度（比如鼠标移动轨迹、点击频率、IP行为模式等）。</p><p>为了应对越来越狡猾的AI机器人，Cloudflare的防御系统变得越来越复杂，需要调用的「特征」越来越多。</p><p>这次故障的直接原因就是数据库错误地吐出了过多的特征数据，导致<strong>防御系统的「大脑」过载</strong>。</p><p><strong>这不是一次普通的软件崩溃，这是「数字免疫系统」在试图升级以对抗AI病毒时，因自身的排异反应而休克。</strong></p><p>这次事件最荒诞的地方在于<strong>受害者名单</strong>。</p><p><strong>OpenAI、xAI、Perplexity：</strong>这些是全球最大的AI公司，它们同时扮演了两个角色：</p><p>它们的爬虫在全网搜刮数据，迫使Cloudflare建立更复杂的防御系统（即这次崩溃的源头）。</p><p>它们自己也极其依赖Cloudflare来防止被别人攻击或滥用。</p><p><strong>结果呢？</strong></p><p>Cloudflare为了防御AI抓取行为而维护的系统，因为一次配置错误，反过来「杀死了」最顶级的AI服务商。</p><p>这就像是为了防止野兽入侵而把城墙修得太高太重，结果城墙倒塌，把住在城里的国王（AI巨头）给压垮了。</p><p>这揭示了AI时代基础设施的<strong>内卷化困境</strong>——为了对抗技术的滥用，我们不得不把基础设施造得越来越复杂、越来越脆弱。</p><p>你问这和AI有什么关系，或许这就是AI时代的「技术债」。</p><p>这里有一个更深层的隐喻：<strong>「特征膨胀」</strong>。</p><p>在传统的软件工程中，逻辑通常是线性的。</p><p>但在涉及AI和机器学习的防御体系中，系统依赖于成百上千个「特征」来进行概率判断。</p><p>这次故障是因为特征数量突破了<strong>200个</strong>的硬编码限制而引发的。</p><p>这暴露了一个问题：我们正在构建一种人类难以完全掌控的「黑箱基建」。</p><p>为了拦截智能程度极高的AI机器人，防御规则不能再是简单的黑白名单，而必须是动态的、基于行为分析的复杂模型。</p><p>这种<strong>复杂度的指数级上升</strong>，意味着未来类似的「不可预测的崩溃」会越来越多。</p><p>我们正在用复杂的AI（防御）去对抗复杂的AI（进攻），而夹在中间的，是脆弱的物理互联网。</p><p>这次宕机不仅是一个配置错误，它是人类互联网为了适应AI寄生而进行的一次痛苦痉挛。</p><p>它是「矛」（AI抓取）与「盾」（AI防御）在无限升级的军备竞赛中，把战场（互联网基础设施）给撑爆了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413352" alt="" title="" loading="lazy"/></p><p>但是，这波也有用AI来打败AI的正面例子。</p><p>比如，吴恩达团队就在Cloudflare宕机的过程中，用AI快速实现了Cloudflare功能的克隆版本，成为最早一批恢复运行的网站。</p><p>属实是用魔法打败魔法了！</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413353" alt="" title="" loading="lazy"/></p><p>最后再放一个彩蛋。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413354" alt="" title="" loading="lazy"/></p><p><strong>彩蛋：元凶被原地解雇</strong></p><p>X上这位名为Rob Hallam的哥们发了个帖子。</p><p>说他正是那位搞崩全球互联网的工程师（可能是之一）。</p><p>自称是，能用单个正则表达式让20%互联网瘫痪，哈哈哈</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413355" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413356" alt="" title="" loading="lazy"/></p><p>参考资料：</p><p><a href="https://link.segmentfault.com/?enc=L0AB6wvWR%2B7FF2pJwbwSIw%3D%3D.q6B1xJndI2nPMD83mkWcRFPuYSt2h1j0xELfYoVytEqmUbFbwO0Ykp2N77Ot0vEEQ%2FghLsFa2x0mngSb9rHzIA%3D%3D" rel="nofollow" target="_blank">https://blog.cloudflare.com/1...</a></p>]]></description></item><item>    <title><![CDATA[不止于“锁”：免费SSL证书如何为你的小]]></title>    <link>https://segmentfault.com/a/1190000047413276</link>    <guid>https://segmentfault.com/a/1190000047413276</guid>    <pubDate>2025-11-20 10:08:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>一、核心保障：数据安全与用户信任的基石</strong></p><p>这可以说是SSL证书最直接也是最重要的作用。<br/><img width="723" height="692" referrerpolicy="no-referrer" src="/img/bVdmPz6" alt="" title=""/></p><p><a href="https://link.segmentfault.com/?enc=9SS779LsyUHsZm8fTqSdiw%3D%3D.%2F8%2FNOR0aUzOYVlTMSRQOZhKmv6WJXH%2FXaFU4a9l%2BKO6BRYv4jC9ladLtujHtkWfHKtZXdeVsyiE78GjDGsPUSQ%3D%3D" rel="nofollow" target="_blank">https://www.joyssl.com/certificate/select/free.html?nid=59</a></p><p><strong>注册码230959⬆️</strong></p><ul><li><strong>加密数据传输，守护用户隐私</strong>：当用户访问你的网站并输入任何信息（即使是简单的留言或邮箱订阅）时，SSL证书会在用户的浏览器和你的服务器之间建立一条加密通道。没有这条通道，这些数据就可能在传输过程中被不法分子轻易截获。这对于保护用户隐私至关重要。</li><li><strong>验证网站身份，抵御钓鱼风险</strong>：SSL证书由权威的证书颁发机构（CA）签发，意味着这个“锁”不仅是加密工具，也是一个身份认证。它能向用户证明他们正在访问的是真正的你的网站，而不是冒充的钓鱼网站。这对于建立用户信任感非常有帮助。</li></ul><p><strong>二、隐形推手：提升搜索引擎排名与可见度</strong></p><p>这一点对于希望获得更多流量的小型网站来说尤其关键。</p><ul><li><strong>SEO排名优势</strong>：像谷歌、百度这样的主流搜索引擎早已明确表示，HTTPS是其搜索排名算法的一个重要考量因素。这意味着，在其他条件相同的情况下，部署了SSL证书的网站更有可能获得靠前的排名，从而获得更多的自然流量。</li><li><strong>避免“不安全”警告</strong>：没有SSL证书的网站，现代浏览器会明确标记为“不安全”，并在地址栏显示红色警告或感叹号。这种视觉提示会极大地打击用户的访问意愿，导致潜在访客流失。有了SSL证书，就能有效避免这种情况。</li></ul><p><strong>三、成本效益：零门槛实现专业级安全防护</strong></p><p>过去，高昂的成本是阻碍小型网站部署SSL证书的主要原因，但现在情况已经完全不同。</p><ul><li><strong>成熟可靠的免费方案</strong>：以 <strong>Let's Encrypt</strong> 为代表，它们提供了完全免费且高度自动化的证书申请和续期流程，极大地降低了技术门槛和使用成本。虽然其有效期较短（通常为90天），但配合自动化工具可以实现近乎“一次配置，长期有效”。此外，如 <strong>JoySSL证书助手</strong> 等国内平台还提供了免费通配符证书申请，进一步方便了有多个子域名的用户。</li><li><strong>满足绝大多数场景需求</strong>：对于个人博客、作品集、小型企业官网等非电商、非金融类网站，免费的域名验证（DV）SSL证书提供的加密强度和身份验证级别已经完全足够。它与付费证书在安全性上没有差异。</li></ul><p><strong>四、实践指南：选择、部署与维护的最佳实践</strong></p><p>要让免费SSL证书真正发挥价值，正确的操作同样重要。</p><ul><li><strong>如何选择</strong>：根据你的网站类型选择。个人站点首选 <strong>Let's Encrypt</strong>；如果拥有多个子域名，可以寻找支持免费通配符证书的平台；小型企业官网若想展示更多组织信息，甚至可以关注一些平台提供的免费基础型OV证书。</li><li><strong>如何部署</strong>：现在获取和部署SSL证书非常方便。很多云服务商、域名注册商都提供了一键申请和部署的功能。你也可以使用服务器管理面板（如宝塔）、CMS系统插件等方式快速完成安装。</li><li><p><strong>注意事项</strong>：</p><ul><li><strong>定期检查与续期</strong>：务必设置好自动续期或手动记录到期时间，防止因证书过期导致网站无法访问。</li><li><strong>修复混合内容错误</strong>：安装后，要检查网站内所有的图片、脚本、样式表是否都通过HTTPS加载。如果存在HTTP资源，浏览器仍会提示不安全。可以通过浏览器开发者工具查找并修复这些问题。</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[HTTPS开头的小绿锁，到底在守护什么？]]></title>    <link>https://segmentfault.com/a/1190000047413278</link>    <guid>https://segmentfault.com/a/1190000047413278</guid>    <pubDate>2025-11-20 10:07:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><strong>HTTPS开头的小绿锁，到底在守护什么？</strong></p><p>每天浏览网页时，我们早已习惯了地址栏里那个小小的锁形图标。它静默无声，以至于我们常常忽略它的存在。但你是否曾想过，这把<strong>小绿锁</strong>，究竟在为我们守护着什么？</p><p>它不仅仅是一个装饰性的图标，而是网络安全的一道重要防线。今天，我们就来揭开这把“守护之锁”的神秘面纱。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413280" alt="21.jpeg" title="21.jpeg"/></p><p><strong>一、小绿锁是谁？—— 安全连接的“守门人”</strong></p><p>简单来说，当你在浏览器地址栏看到小绿锁和“HTTPS”前缀时，意味着你与当前网站之间的连接是安全的、加密的。</p><p><strong>HTTP</strong>：是互联网数据传输的基础协议，但信息以明文传输，如同邮寄一张明信片，途中的任何人都可以窥视其内容。</p><p><strong>HTTPS</strong>：= HTTP + S（Secure Socket Layer / TLS）。它相当于为“明信片”加上了一个坚固的保险箱。小绿锁，就是这个保险箱的“锁”，象征着安全连接已建立。</p><p><strong>二、小绿锁的四大守护使命</strong></p><p>这把锁守护的，远比你想象的更多。它是一位尽职尽责的卫士，主要在四个关键领域为我们提供保护。</p><p><strong>🛡️ 守护一：你的隐私与敏感数据</strong></p><p>这是小绿锁最核心的使命——加密。它确保了你与他人之间“说悄悄话”时，不会被第三者偷听。</p><p>它保护的具体信息包括：</p><p><strong>登录凭证</strong>：你的用户名和密码。</p><p><strong>财务信息</strong>：信用卡号、网银账户、支付密码。</p><p><strong>个人身份信息</strong>：身份证号码、手机号、住址。</p><p><strong>私人通信</strong>：在聊天应用、邮件、表单中输入的任何敏感内容。</p><p><strong>没有小绿锁的后果</strong>：如果你在一个“不安全”（HTTP）的网站上输入密码，黑客在同一个公共Wi-Fi下，可以像从空气中“嗅探”一样，轻松截获你的明文密码。</p><p><strong>🛡️ 守护二：信息的完整性与真实性</strong></p><p>小绿锁确保你收到的信息，在传输过程中没有被篡改或调包。</p><p><strong>它如何工作：</strong></p><p>在加密的基础上，安全协议会验证数据的“指纹”。如果数据在传输中被第三方恶意修改（例如，在捐款网站上，将你输入的“100元”篡改为“1000元”），接收方立刻就能发现，并丢弃这份被污染的数据。</p><p><strong>没有小绿锁的后果</strong>：你下载的软件可能被植入病毒，你看到的新闻内容可能被中间人随意篡改，你支付的金额可能被恶意修改。</p><p><strong>🛡️ 守护三：网站的身份真实性</strong></p><p>小绿锁是一个信任的印章，它向用户证明：“<strong>你正在访问的网站，就是它声称的那个网站，而不是一个高仿的钓鱼网站。</strong>”</p><p><strong>它如何工作：</strong></p><p>SSL证书在颁发前，证书颁发机构（CA）会对申请者的身份进行不同严格程度的验证。</p><p><strong>域名验证（DV）</strong>：确认申请人控制着该域名。这是最常见的小绿锁基础。</p><p><strong>组织验证（OV）与扩展验证（EV）</strong>：会严格核实企业的真实性和合法性。点击小绿锁，可以查看证书详情，看到网站所有者的公司名称。</p><p>没有小绿锁的后果：<strong>你很容易掉入钓鱼网站的陷阱</strong>。比如，你访问的 www.abc-bank.com 可能是一个精心伪装的假网站，专门用来盗取你的银行账户信息。</p><p><strong>🛡️ 守护四：用户与网站的双向信任</strong></p><p>这把锁不仅保护用户，也为网站所有者建立了可信赖的品牌形象。</p><p><strong>对用户而言</strong>：小绿锁提供了心理安全感，让你可以放心地进行登录、浏览和交易。</p><p><strong>对网站主而言</strong>：拥有小绿锁意味着尊重并保护用户隐私，这是一种专业和负责任的体现，能有效提升用户停留时间、转化率和忠诚度。</p><p><strong>没有小绿锁的后果</strong>：浏览器明确的“不安全”警告会吓跑大部分用户，导致流量和业务直接流失。</p><p><strong>三、如何判断连接是否真正安全？</strong></p><p>请养成查看地址栏的习惯：</p><p><strong>安全连接</strong>：🔒 HTTPS + 小锁图标 + 可能显示的“安全”字样。</p><p>不安全或部分安全连接：</p><p><strong>⚠ 三角感叹号</strong>：通常表示网站是HTTPS，但加载了不安全的资源（混合内容），安全性打折扣。</p><p><strong>“不安全”文字</strong>：明确表示该网站使用HTTP协议，连接完全未加密。</p><p><strong>最佳实践</strong>：在进行任何登录或支付操作前，请务必确认小绿锁的存在。</p><p><strong>总结</strong><br/>HTTPS开头的小绿锁，绝不是一个可有可无的图标。它是一位沉默的守护者，同时肩负着四大使命：</p><p><strong>加密者</strong>：守护你的隐私数据，防止窃听。</p><p><strong>校验者</strong>：守护信息的完整性，防止篡改。</p><p><strong>认证者</strong>：守护网站的真实身份，防止钓鱼。</p><p><strong>信任基石</strong>：守护用户与网站之间的双向信任。</p><p>它守护的，是数字世界里最基本也最珍贵的资产——安全与信任。下次当你看到这把小绿锁时，可以放心地知道，你正受到现代密码学技术的坚实保护。</p>]]></description></item><item>    <title><![CDATA[为何安装了SSL证书，网站仍被标记“不安]]></title>    <link>https://segmentfault.com/a/1190000047413281</link>    <guid>https://segmentfault.com/a/1190000047413281</guid>    <pubDate>2025-11-20 10:06:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数字化时代，网络安全已成为网站运营的基石。许多网站管理员已经意识到为网站安装SSL证书，将HTTP升级为HTTPS的重要性，期待在浏览器地址栏看到那把象征安全的“小锁”。然而，不少人却困惑地发现，即便成功安装了证书，浏览器依然会弹出“不安全”的红色警告。这并非SSL证书本身无效，而是背后隐藏着更深层次的原因。要理解并解决这一问题，我们需要从几个关键层面进行剖析。<br/><img width="723" height="400" referrerpolicy="no-referrer" src="/img/bVddlwr" alt="" title=""/></p><h4><strong>一、证书自身的问题：身份与时效的验证</strong></h4><h4>申请办法：打开JoySSL官网，填写注册码230970获取技术支持<a href="https://link.segmentfault.com/?enc=LTdnM8nREcK%2Fqw9Xfzz9kg%3D%3D.vRI3Lc%2BC2f6vZxbj4ATsg16BVY8IP0Mq%2BMrUx5r8ekH3ZGqNlCljpHBY2cWSn%2FbbLmKKRLKDWKqHtp6Y6pArew%3D%3D" rel="nofollow" target="_blank">申请入口</a></h4><p>首先，最直接的原因可能出在SSL证书本身。</p><ol><li><strong>证书已过期或未生效</strong>：SSL证书并非永久有效，它们都有明确的有效期，通常为一年或更短。就像食品有保质期一样，过期的证书会立即被浏览器判定为无效，从而发出严重警告。同样，如果服务器时间设置错误，导致当前时间不在证书的生效期内，也会触发警报。</li><li><strong>证书与域名不匹配</strong>：每个SSL证书都是针对特定域名签发的。如果您为 <code>www.example.com</code> 购买了证书，但用户访问的是 <code>example.com</code>（或无<code>www</code>版本），浏览器会认为域名不匹配，提示不安全。解决方法是确保证书覆盖所有需要保护的域名变体，或使用支持多域名的通配符证书。</li><li><strong>证书链不完整或不受信任</strong>：SSL证书的信任建立在一条“信任链”之上，从根证书、中间证书到您的站点证书。如果网站在配置时，只部署了站点证书，而遗漏了中间证书，浏览器将无法追溯到受信任的根证书，从而判定其为“不受信任的发行机构”并发出警告。</li></ol><h4><strong>二、网站内容的“混合”风险</strong></h4><p>这是最常见且最容易被忽略的原因。即使您的网站主文档是通过安全的HTTPS加载的，但如果页面中包含了通过不安全的HTTP协议加载的子资源，就会构成“混合内容”。</p><p>浏览器会将整个页面视为“不安全”，因为攻击者可能篡改那些通过HTTP加载的资源，从而窃取信息或进行恶意操作。这些资源通常包括：</p><ul><li><strong>图片</strong>：通过<code>&lt;img src="http://..."&gt;</code>引用的图片。</li><li><strong>脚本</strong>：通过<code>&lt;script src="http://..."&gt;</code>引用的JavaScript文件。</li><li><strong>样式表</strong>：通过<code>&lt;link href="http://..."&gt;</code>引用的CSS文件。</li><li><strong>iframe</strong>：通过HTTP嵌入的第三方内容。</li></ul><p><strong>解决方案</strong>：使用浏览器的开发者工具（按F12键），查看控制台或网络选项卡，将所有被标记为不安全的资源链接，全部修改为HTTPS协议，或者使用相对路径（<code>//example.com/resource.js</code>）。</p><h4><strong>三、服务器配置与缓存作祟</strong></h4><ol><li><strong>服务器配置错误</strong>：即使证书文件正确，Web服务器的配置也至关重要。例如，在Nginx或Apache中，需要正确指定证书文件和私钥的路径。配置不当会导致服务器无法正常提供HTTPS服务。</li><li><strong>浏览器或中间设备缓存</strong>：浏览器为了加速访问，会缓存大量数据，其中就包括过去访问该网站时的“不安全”状态。即使您已修复所有问题，旧的缓存仍可能让警告持续一段时间。尝试清除浏览器缓存，或使用“无痕模式”访问，可以验证问题是否已解决。</li></ol><h4><strong>四、更深层次的安全策略：HSTS的缺失</strong></h4><p>HSTS是一种重要的Web安全策略机制。它告诉浏览器，在接下来的一段时间内，只能使用HTTPS与该网站通信，即使用户手动输入HTTP地址，浏览器也会自动转为HTTPS。</p><p>如果您的网站没有启用HSTS，用户在首次访问时，仍有可能通过HTTP入口进入，从而被劫持或降级攻击。而启用HSTS后，不仅能提升安全性，还能让浏览器更“坚定”地认可您的HTTPS状态。您可以通过在服务器响应头中添加 <code>Strict-Transport-Security</code> 来启用它。</p><h4><strong>总结与行动指南</strong></h4><p>当您的网站出现“不安全”警告时，请不要急于责怪证书提供商，而应遵循一个系统的排查流程：</p><ol><li><strong>检查证书状态</strong>：点击地址栏的“不安全”标识，查看证书详情，确认其有效期、颁发给的对象以及颁发者是否可信。</li><li><strong>审查混合内容</strong>：打开开发者工具，仔细检查控制台是否有关于混合内容的错误报告。</li><li><strong>清除缓存测试</strong>：使用无痕窗口访问网站，看问题是否依旧存在。</li><li><strong>验证服务器配置</strong>：使用在线SSL检测工具，它们能提供一份详细的报告，指出证书、证书链和服务器配置中存在的所有问题。</li><li><strong>考虑启用HSTS</strong>：在确保网站所有资源都已HTTPS化后，启用HSTS以提供更深层的保护。</li></ol><p>总之，安装SSL证书只是迈向网站安全的第一步，而非终点。它像是一把精密的锁，但如果您没有把门上好（服务器配置），或者窗户还开着（混合内容），安全便无从谈起。只有全面排查、精细配置，才能让那把象征着信任与安全的“小锁”稳固地出现在用户面前，真正赢得他们的信赖。</p>]]></description></item><item>    <title><![CDATA[免费的SSL证书可以用吗? 傻傻的开心果]]></title>    <link>https://segmentfault.com/a/1190000047413285</link>    <guid>https://segmentfault.com/a/1190000047413285</guid>    <pubDate>2025-11-20 10:05:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、先搞懂：<strong>SSL 证书到底是啥？为啥非装不可？</strong></p><p>你可能没意识到，当浏览器显示 <strong>“不安全” 警告</strong>时，用户会毫不犹豫关掉你的网站 —— 而<strong>SSL 证书 </strong>，就是解决这个问题的 “网站身份证”。它的核心作用有三个：</p><ul><li><strong>数据加密</strong>：像给用户和网站的沟通加了把锁，防止密码、支付信息被黑客截取；</li><li><strong>身份认证</strong>：证明你的网站是 “正经平台”，不是钓鱼网站冒充的；</li><li><strong>SEO 加分</strong>：谷歌、百度等搜索引擎明确表示，带<strong>HTTPS</strong>（装了 SSL）的网站排名更靠前。</li></ul><p>简单说，没有 SSL 证书的网站，就像开了家没挂营业执照、大门敞开的店铺，用户不敢进，搜索引擎也不待见。</p><p>二、免费 SSL 证书：<strong>香还是坑？</strong></p><p>市面上最火的免费 SSL 证书当属<strong>Let’s Encrypt</strong>，还有阿里云、腾讯云提供的免费版。它们真的能放心用吗？答案是：<strong>分情况！</strong></p><p>✅ 免费 SSL 的 3 个 “真香场景”</p><ol><li><strong>个人博客 / 静态网站</strong>：如果你的网站只是分享文章、展示作品，没有用户登录、支付功能，免费 SSL 完全够用。Let’s Encrypt 支持<strong>自动续期</strong>，配置简单，零成本就能搞定 “HTTPS” 标志；</li><li><strong>初创小微企业试水</strong>：刚起步的小网站，预算有限又想提升可信度，免费 SSL 是过渡阶段的绝佳选择，先解决 “有无” 问题，再考虑 “好坏”；</li><li><strong>测试环境 / 临时项目</strong>：开发中的网站、短期活动页面，用免费 SSL 避免 “不安全” 警告，项目结束后无需额外处理，性价比拉满。</li></ol><p>❌ 免费 SSL 的 4 个 “隐形坑”</p><ol><li><strong>有效期极短，续期麻烦</strong>：免费证书有效期通常只有<strong>90 天</strong>，虽然支持自动续期，但如果服务器配置出错、域名解析变更，很容易导致续期失败，网站突然变回 “不安全”；</li><li><strong>不支持多域名 / 通配符</strong>：如果你的网站有多个子域名（比如blog.xxx.com、shop.xxx.com），免费 SSL 需要逐个申请，管理繁琐；而付费证书的<strong>通配符证书（*.xxx.com）</strong> 能一次性覆盖所有子域名；</li><li><strong>无技术支持，出问题自救</strong>：免费 SSL 没有官方技术团队，遇到配置失败、浏览器不识别等问题，只能靠搜索教程、论坛求助，对于不懂技术的网站主来说，可能要折腾好几天；</li><li><strong>信任等级较低，影响转化</strong>：部分免费 SSL 的根证书不被少数老旧浏览器支持，虽然比例不高，但可能导致部分用户无法正常访问；更重要的是，电商、金融等需要用户付费、提交敏感信息的网站，付费 SSL 的<strong>EV 证书（显示绿色地址栏 + 公司名称）</strong> 能显著提升用户信任度，而免费 SSL 只有基础的<strong>DV 证书（仅验证域名所有权）</strong> 。</li></ol><p><img width="530" height="343" referrerpolicy="no-referrer" src="/img/bVdbwbw" alt="" title=""/></p><p>三、终极选择指南：<strong>到底该用免费还是付费？</strong></p><table><thead><tr><th>场景</th><th>推荐选择</th><th>核心原因</th></tr></thead><tbody><tr><td>个人博客、静态展示网站</td><td>免费 SSL</td><td>零成本，满足基础安全需求</td></tr><tr><td>初创企业官网（无交易功能）</td><td>免费 SSL 过渡，后期升级付费</td><td>控制成本，同时提升网站可信度</td></tr><tr><td>电商、金融、付费会员网站</td><td>付费 SSL（EV/OV 证书）</td><td>绿色地址栏增强信任，支持多域名，有技术保障</td></tr><tr><td>多子域名网站（3 个以上）</td><td>付费通配符 SSL</td><td>统一管理，避免重复申请</td></tr><tr><td>政府、教育、医疗等权威网站</td><td>付费 OV/EV 证书</td><td>符合行业规范，提升官方公信力</td></tr></tbody></table><p>四、重要提醒：<strong>免费 SSL 这么用才安全！</strong></p><ol><li>开启<strong>自动续期</strong>：在服务器配置中设置自动续期脚本，避免证书过期导致网站 “不安全”；</li><li>强制<strong>HTTPS 跳转</strong>：通过配置服务器，将所有 HTTP 请求自动跳转至 HTTPS，防止用户访问未加密的页面；</li><li>定期检查<strong>证书状态</strong>：用 SSL 检测工具（比如<strong>SSL Labs</strong>）定期扫描，确认证书有效、配置正确，避免出现漏洞；</li></ol><ol start="4"><li>备份<strong>证书文件</strong>：将申请到的证书文件备份，防止服务器迁移、重装系统时丢失，导致需要重新申请。</li></ol><p>总结</p><p>免费 SSL 并非 “洪水猛兽”，它是<strong>低成本实现网站 HTTPS</strong>的好工具，适合非盈利、非交易类的基础网站；但如果你的网站涉及交易、敏感信息收集，或者需要提升用户信任度、简化管理，<strong>付费 SSL 的价值绝对值得投资</strong>—— 毕竟，网站的安全和用户信任，从来都不是 “免费” 能完全覆盖的。</p><p>与其纠结 “免费能不能用”，不如根据自己的网站需求选择：<strong>基础需求选免费，核心业务选付费</strong>，才是最理性的选择！</p>]]></description></item><item>    <title><![CDATA[小米技术教父，离职创业了！ CodeSh]]></title>    <link>https://segmentfault.com/a/1190000047413294</link>    <guid>https://segmentfault.com/a/1190000047413294</guid>    <pubDate>2025-11-20 10:04:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近，科技圈的又一位技术大佬被曝入局创业了。</p><p>没错，他就是小米的技术教父、小米前副总裁，同时也是雷军在武大念书时的下铺兄弟，并且在拍照时可以和雷军并列而坐的技术大佬崔神：</p><p><strong>崔宝秋</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413296" alt="崔宝秋（第一排右二）" title="崔宝秋（第一排右二）"/></p><p>那这一次创业，据说大佬瞄准的也是当下炙手可热的「<strong>具身智能</strong>」赛道，创业方向为「<strong>家庭服务机器人</strong>」方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413297" alt="" title="" loading="lazy"/></p><p>并且在这次创业消息被曝之前，据传大佬已与多家顶级 VC 深入接洽，融资等进展顺利。</p><p><strong>说回到小米的发展历程，崔宝秋是其绕不开的一个人</strong>。</p><p>大家都知道，小米这个公司以产品见长。</p><p>提到小米产品，大家基本上都耳熟能详。从智能手机到影音数码，从家电生活到智能家居，另外各种软件、平台、云服务、OS等都在持续发力，包括现在又在全速力推新能源智能汽车，软硬件产品线可以说覆盖得非常广泛。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413298" alt="" title="" loading="lazy"/></p><p>但是说到「<strong>小米技术</strong>」，大家反而可能没有什么特别深的印象。</p><p>不过提到小米技术，就不得不提：<strong>小米集团技术委员会</strong>，其诞生于 2019 年初。</p><p>当时的 2018 年 Q4 季度，国内智能手机市场前五曾分别为：</p><p>华为、OPPO、VIVO、苹果、小米。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413299" alt="" title="" loading="lazy"/></p><p>其中前三家国产手机厂商出货量增幅均为正增长，而当时排名第 5 的小米则出现了较大跌幅。</p><p>彼时的雷军开始在内部会议中一直强调的一件事情就是：</p><p><strong>技术事关小米生死存亡</strong>。</p><p>在随后不久后的 2019 年 2 月，小米就专门成立了<strong>集团技术委员会</strong>，主要负责把握集团的技术方向，预研前沿技术，以及推动技术创新和成果转化。</p><p>小米技术委员会在小米的科技创新和产品研发中扮演着重要的角色。</p><p>而首任挂帅的正是小米当时的技术大牛：<strong>崔宝秋</strong>。</p><p>而聊起小米的技术研发，他更是绕不开的一个人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413300" alt="" title="" loading="lazy"/></p><p>崔宝秋和雷军都是武汉大学计算机系的同学，而且同时还是同寝室的上下铺室友。</p><p>在加入小米之前，崔宝秋在国外生活工作过很多年。</p><p>他曾在 IBM、雅虎和 LinkedIn 等知名公司负责研发，积累了丰富的技术和管理经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413301" alt="" title="" loading="lazy"/></p><p>众所周知，2012 年时候小米的手机业务初露头角。</p><p>同样也是这一年，崔宝秋应雷军之邀回国加入小米工作，并成为小米的首席架构师。</p><p>提到崔宝秋，有网友称其为“小米的技术教父”。</p><p>确实，崔宝秋在小米期间担任过多个重要职务，包括首席架构师、人工智能与云平台副总裁、集团技术委员会主席和集团学习发展部总经理等。</p><p>除此之外，他在小米期间的一个重大贡献就是主导并推动了小米「<strong>云计算-大数据-人工智能</strong>」的技术变革主线，为小米后来的技术发展奠定了基础。</p><p>然而，根据小米发布的内部全员信，2022 年底崔宝秋因个人原因从小米离职。</p><p>至此，崔神在小米的这段长达十年的职业生涯，也宣告结束了。</p><p>而这十年，也是小米这家公司从蓄力到发展，从厚积到薄发的十年。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413302" alt="" title="" loading="lazy"/></p><p>自 2022 年底，崔宝秋从小米集团离职后在业界似乎一直就“隐身”了，随后几年也一直没有看到他比较明显的下一步职场动向。</p><p>那这一次机器人创业也是崔宝秋离职小米两年多后的一个比较大的动作。</p><p>怎么说呢，祝福崔神，也期待后续能看到大佬更多的发展和动向。</p><blockquote>注：本文在GitHub开源仓库「编程之路」 <a href="https://link.segmentfault.com/?enc=%2Fd33B%2FKZ9tE%2FfQppjQ3cIA%3D%3D.sTVL03HoiON3gQ2GzdLmsVgoQ6WIqJRpDSnEsKukJqdwiBWoKlIUJY6Uav%2BwxN1S" rel="nofollow" target="_blank">https://github.com/rd2coding/Road2Coding</a> 中已经收录，里面有我整理的6大编程方向(岗位)的自学路线+知识点大梳理、面试考点、我的简历、几本硬核pdf笔记，以及程序员生活和感悟，欢迎star。</blockquote>]]></description></item><item>    <title><![CDATA[【youtube爬虫】油管评论采集软件v]]></title>    <link>https://segmentfault.com/a/1190000047413314</link>    <guid>https://segmentfault.com/a/1190000047413314</guid>    <pubDate>2025-11-20 10:03:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><em>本软件工具仅限于学术交流使用，严格遵循相关法律法规，符合平台内容合法合规性，禁止用于任何商业用途！</em></blockquote><h2>一、背景介绍</h2><h3>1.1 爬取目标</h3><p>您好！我是<strong>@马哥python说</strong>，一枚10年+程序猿，现全职独立开发。</p><p>我用Python独立开发了一款爬虫工具：<strong>爬油管评论软件</strong>。作用是：爬取油管指定视频下的评论数据，支持批量视频的采集。</p><p>包含10个关键字段：</p><pre><code>1. cid(评论id)
2. text(评论内容)
3. time(评论时间_相对)
4. author(评论者昵称)
5. channel(评论者频道)
6. votes(评论点赞数)
7. replies(评论回复数)
8. time_parsed(评论时间转换)
9. time2(评论时间_绝对)
10. video_id(视频id)</code></pre><p>软件是通过调用YouTube的网页接口，不是模拟操作浏览器，所以稳定性较高！</p><p>开发成界面软件的目的：方便不懂编程代码的小白用户使用，无需安装python，无需改代码，双击打开即用！<br/>软件运行界面：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413316" alt="软件运行截图" title="软件运行截图"/></p><p>《目标视频.xlsx》模板中的填写：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413317" alt="目标视频" title="目标视频" loading="lazy"/></p><p>也就是说，在目标视频文件中填入了5个待爬视频，然后在软件界面上选择爬这5个视频的前30条热门评论。所以，采集结果csv文件中会自动导出150条热门评论，如下：</p><p>爬取结果截图：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413318" alt="采集到的评论数据" title="采集到的评论数据" loading="lazy"/></p><p>以上。</p><h3>1.2 演示视频</h3><p>软件使用过程演示：<a href="https://link.segmentfault.com/?enc=RI2XwzpHjsPvIclvGXrMGQ%3D%3D.0KyKb8LULPUwOdTdXkaaf%2B4ZTSB0DYkmgQDtCs%2F24SGoqEd63LDgybL1KBMQTNbrg33Xhjnxgq4za47OIvsb2w%3D%3D" rel="nofollow" target="_blank">【爬虫演示】爬油管评论软件v3.0版</a></p><h3>1.3 软件说明</h3><p>几点重要说明：</p><ol><li>专为文科生研发，Win系统、Mac系统均可直接运行，无需配置python环境</li><li>软件通过接口爬取，并非通过模拟浏览器等RPA类工具，稳定性较高！</li><li>软件运行完成后，会在当前文件夹（即，软件所在文件夹）生成csv结果文件</li><li>爬取过程中，每爬一页，存一次结果。并非爬完最后一次性保存！防止因异常中断导致丢失前面的数据（每页请求间隔1~2s）</li><li>爬取过程中，有log文件详细记录运行过程，方便回溯</li><li>采集结果有10个字段，含：cid(评论id),text(评论内容),time(评论时间_相对),author(评论者昵称),channel(评论者频道),votes(评论点赞数),replies(评论回复数),time_parsed(评论时间转换),time2(评论时间_绝对),video_id(视频id)</li></ol><p>以上。</p><h2>二、代码概要</h2><h3>2.1 调用接口</h3><p>为保护软件原创版权，不开放核心爬虫逻辑代码。</p><p>最后，把json数据转出到csv文件：</p><pre><code class="python">self.tk_show('[第{}/{}个][{}][{}]comment:{}'.format(video_idx, video_id_total, video_id, cnt, comment['text']))
with open('./jsons/{}.json'.format(video_id), 'a+', encoding='utf-8') as f:
    f.write(json.dumps(comment, ensure_ascii=False))
    f.write('\n')</code></pre><p>我采用csv库保存结果，实现每爬一条存一次，防止中途异常停止丢失前面的数据。</p><p>完整代码中，还含有：读取配置判断、循环结束条件判断、拼接频道URL、try异常保护、日志记录等关键实现逻辑。</p><p>另外，魔法是一切的前提，此处不便多说！</p><h3>2.2 软件界面模块</h3><p>主窗口部分：</p><pre><code class="python"># 创建主窗口
root = tk.Tk()
root.title('爬油管评论软件v3.0 | 马哥python说 |')
# 设置窗口大小
root.minsize(width=850, height=650)
# 左上角图标
root.iconbitmap('mage.ico')</code></pre><p>输入控件部分：</p><pre><code class="python"># 爬取数量
tk.Label(root, text='爬取数量:').place(x=30, y=125)
comment_num = tk.Spinbox(root, from_=-1, to=9999999, increment=1, width=10, font=('微软', 15))
comment_num.place(x=100, y=125, anchor='nw')
tk.Label(root, fg='red', text='每个视频爬前几条评论，-1代表爬取全部').place(x=240, y=125)</code></pre><p>运行日志部分：</p><pre><code class="python"># 运行日志
tk.Label(root, justify='left', text='运行日志:').place(x=30, y=280)
show_list_Frame = tk.Frame(width=780, height=260)  # 创建&lt;消息列表分区&gt;
show_list_Frame.pack_propagate(0)
show_list_Frame.place(x=30, y=310, anchor='nw')  # 摆放位置</code></pre><p>底部版权部分：</p><pre><code class="python"># 版权信息
copyright = tk.Label(root, text='@马哥python说 All rights reserved.', font=('仿宋', 10), fg='grey')
copyright.place(x=290, y=625)</code></pre><p>以上。</p><h3>2.3 日志模块</h3><p>好的日志功能，方便软件运行出问题后快速定位原因，修复bug。 <br/>日志核心代码：</p><pre><code class="python">def get_logger(self):
    self.logger = logging.getLogger(__name__)
    # 日志格式
    formatter = '[%(asctime)s-%(filename)s][%(funcName)s-%(lineno)d]--%(message)s'
    # 日志级别
    self.logger.setLevel(logging.DEBUG)
    # 控制台日志
    sh = logging.StreamHandler()
    log_formatter = logging.Formatter(formatter, datefmt='%Y-%m-%d %H:%M:%S')
    # info日志文件名
    info_file_name = time.strftime("%Y-%m-%d") + '.log'
    # 将其保存到特定目录
    case_dir = r'./logs/'
    info_handler = TimedRotatingFileHandler(filename=case_dir + info_file_name,
                        when='MIDNIGHT',
                        interval=1,
                        backupCount=7,
                        encoding='utf-8')</code></pre><p>日志文件：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413319" alt="log文件" title="log文件" loading="lazy"/></p><p>以上。</p><h2>END、原创声明</h2><p>工具【爬油管评论软件】首发于众公号"老男孩的平凡之路"，欢迎交流！！</p>]]></description></item><item>    <title><![CDATA[前端AI流式输出深度思考<think>标]]></title>    <link>https://segmentfault.com/a/1190000047413332</link>    <guid>https://segmentfault.com/a/1190000047413332</guid>    <pubDate>2025-11-20 10:03:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>假如我们有这么一段ai回复的内容</p><pre><code>let text = "&lt;think&gt;\n好的，用户想了解如何判断手机摄像头是否是凑数的，希望我用直白易懂的语言来解释。这个问题需要从实用角度出发，帮助用户识别那些功能冗余或性能低下的摄像头。\n我看到的搜索结果提供了很多有价值的信息。根据，凑数摄像头通常是指那些功能可以被其他摄像头或算法替代的镜头，比如低像素的景深或微距镜头。进一步解释了如何通过反证法来判断：如果去掉某个摄像头后，其他摄像头通过算法也能实现相同功能，那这个摄像头就很可能是凑数的。\n\n&lt;/think&gt;\n\n1.先看像素​：遇到200万像素的镜头，心里先打个问号。\n2.再看名称​：如果镜头名称是“景深”、“人像风格”等，而非明确表示焦段（如超广角、3x长焦），就要多留意。\n3.思考能否被替代​：这个镜头做的事，​主摄拍照后裁剪或者装个APP能不能搞定？如果能，它大概率是凑数的。\n4.关注核心镜头​：​主摄的素质是根本。一颗优秀的主摄远比一堆凑数镜头重要。其次看超广角和长焦的规格是否扎实。记住一个简单的道理​：手机厂商增加一颗真正的、高质量的摄像头，成本会显著上升。如果一款手机摄像头数量很多但价格不高，你就需要多警惕那些低规格的镜头了。"</code></pre><p>有些AI-Markdown组件是不支持自动识别<code>&lt;think&gt;&lt;/think&gt;</code>标签的深度思考的，所以我们需要手动分割<code>&lt;think&gt;</code>标签，用来区分深度思考和正文内容。我在这里封装了一个行数，可以在流式输出中调用，自动分割</p><pre><code>/**
 * ai回复的思考和正文分块
 * @param {string} chunk 流式文字
 * @param {object} state ai的content
 * @return 返回分块后的内容 { buffer: '',thinkParts: [] }
 */
export const processStreamedOutput = (chunk, state) =&gt; {
    // 将新的块添加到缓冲区
    state.buffer += chunk;

    // 处理所有完整的 &lt;think&gt; 标签（完整保留标签）
    const thinkRegex = /&lt;think&gt;[\s\S]*?&lt;\/think&gt;/g; // 修改后的正则表达式

    while (true) {
        const match = thinkRegex.exec(state.buffer);
        if (!match) break;

        // 保留完整的 &lt;think&gt;...&lt;/think&gt; 标签
        state.thinkParts.push(match[0]);

        // 更新缓冲区，移除已处理的部分
        thinkRegex.lastIndex = 0; // 重置正则状态
        state.buffer = state.buffer.replace(match[0], '');
    }

    // 检查是否存在未闭合的 &lt;think&gt; 标签
    const openTagIndex = state.buffer.indexOf('&lt;think&gt;');
    if (openTagIndex !== -1) {
        // 保留 &lt;think&gt; 标签及其后的所有内容到缓冲区
        state.buffer = state.buffer.substring(openTagIndex);
    }

    return state;
};</code></pre><p>使用示例：</p><pre><code>// 当前ai的对话内容，比如 chatList.at(-1).content，其中content的结构是 { buffer: '',thinkParts: [] }
let chat = { buffer: '',thinkParts: [] }
// 分割
let data = processStreamedOutput(text, chat)
// 结果
console.log(data)</code></pre><p><img width="723" height="52" referrerpolicy="no-referrer" src="/img/bVdm6xd" alt="" title=""/><br/>支持在流式对话中的连续调用，每次输出都可以调用该函数进行一次分割<br/>默认将<code>text</code>添加到<code>buffer</code>缓冲字段，如果深度思考的  <code>&lt;think&gt;&lt;/think&gt;</code> 标签出现则说明当前深度思考结束，将包含<code>&lt;think&gt;&lt;/think&gt;</code>的深度思考内容添加到 <code>thinkParts</code> 数组内，后续正文的内容依旧添加到<code>buffer</code>缓冲字段<br/>在页面使用的时候需要两个个辅助函数实现</p><pre><code>    /**
     * 深度思考取值
     * @param {number} type 1取深度思考 2取正文
     * @param {object} content AI的content内容
     * @returns 深度思考或正文的正确内容
     */
    const getMarkdown = (type, content) =&gt; {
        let {
            thinkParts,
            buffer
        } = content;
        if (type == 1) {
            if (buffer.includes('&lt;think&gt;')) return buffer;
            let state = thinkParts.some((item) =&gt; item.includes('&lt;think&gt;'));
            if (state) return thinkParts.at(-1);
            return '';
        } else {
            if (!buffer.includes('&lt;think&gt;')) {
                if (thinkParts.length) {
                    return buffer || '';
                } else {
                    return buffer || '已暂停生成';
                }
            }
        }
    };</code></pre><pre><code>    /**
     * 是否有深度思考
     * @param {object} content AI的content内容
     * @returns 是否存在深度思考标签
     */
    const isThink = (content) =&gt; {
        let {
            thinkParts,
            buffer
        } = content;
        let state = thinkParts.some((item) =&gt; item.includes('&lt;think&gt;'));
        if (buffer.includes('&lt;think&gt;') || state) {
            return true;
        } else {
            return false;
        }
    };</code></pre><p>页面使用</p><pre><code>&lt;!-- 深度思考 --&gt;
&lt;view class="ai-think-chunk" v-show="isThink(msg.content)"&gt;
        &lt;markdown-view :theme-color="'#252B3A'" :markdown="getMarkdown(1, msg.content)" /&gt;
&lt;/view&gt;
&lt;!-- 正文内容 --&gt;
&lt;markdown-view :theme-color="'#252B3A'" :markdown="getMarkdown(2, msg.content)" /&gt;</code></pre><p>如果深度思考标签存在，则直接显示深度思考，并且取深度思考的内容<br/>正文内容直接取值就好了，因为在分割深度思考的那一步已经做了区分了，另外加上<code>getMarkdown</code>辅助函数做内容判断，如果正文能够取到，说明正文一定是有内容的</p>]]></description></item><item>    <title><![CDATA[【营销数据洞察系列1】渠道效果实时监控：]]></title>    <link>https://segmentfault.com/a/1190000047413378</link>    <guid>https://segmentfault.com/a/1190000047413378</guid>    <pubDate>2025-11-20 10:02:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从营销理论来看，渠道价值的判断需基于 “全链路数据闭环”，而非单一曝光或点击指标，核心是通过投入产出比（ROI）、转化效率等多维度综合评估。借助助睿 BI 搭建渠道日报，导入 Excel 或数据库数据，曝光、点击、转化、花费等核心指标自动整合呈现，无需手动汇总，投放现状可直观掌握，为科学评估奠定基础。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047413380" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[【新手必看】火语言 RPA 中 Whil]]></title>    <link>https://segmentfault.com/a/1190000047413382</link>    <guid>https://segmentfault.com/a/1190000047413382</guid>    <pubDate>2025-11-20 10:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、核心差异</h2><p><strong>循环体执行的 “先后顺序”—— 是否先判断条件，再执行内容，用表格对比更清晰：</strong></p><table><thead><tr><th>特性</th><th>While 循环（先判断，后执行）</th><th>DoWhile 循环（先执行，后判断）</th></tr></thead><tbody><tr><td>执行逻辑</td><td>1. 先判断条件是否成立 → 2. 条件成立才执行循环体</td><td>1. 先执行 1 次循环体 → 2. 再判断条件是否成立 → 3. 成立则继续循环</td></tr><tr><td>适用场景</td><td>不确定循环体是否需要执行（可能 1 次都不执行）</td><td>循环体必须执行至少 1 次（无论条件是否成立）</td></tr><tr><td>条件不成立时表现</td><td>循环体 1 次都不执行</td><td>循环体已执行 1 次，之后停止</td></tr></tbody></table><p><strong>通俗理解：</strong></p><pre><code>While 循环：“先审题，再做题”—— 符合要求才动手，不符合就直接跳过；
DoWhile 循环：“先做题，再审题”—— 不管符不符合，先做 1 遍，做完再看要不要继续。
</code></pre><h2>二、案例</h2><p>对变量 数字 1 数字 2 执行 “变量赋值” 操作，初始值设为5,</p><h3>1、While 循环执行逻辑（先判断，后执行）</h3><p>循环条件：#数字1 ＜ 5（判断变量 “数字 1” 是否小于 5）。<br/>循环体操作：对 数字 1 执行 ++赋值操作（即数字 1 自增 1）。<br/>执行结果：因初始值 数字 1=5，循环条件5小于5不成立，While :False， 循环体一次都不会执行。</p><h3>2、DoWhile 循环执行逻辑（先执行，后判断）</h3><p>循环体操作：先对 数字 2 执行 ++ 赋值操作（数字 2 自增 1，变为6）。<br/>循环条件：#数字2 ＜ 5（判断变量 “数字 2” 是否＜ 5）。<br/>执行结果：因 数字 2=6 循环条件5小于5不成立，While :False，，循环内执行一次循环体操作。<br/><img width="723" height="258" referrerpolicy="no-referrer" src="/img/bVdm6x2" alt="57b64cbe-ce8f-4992-8c47-fac82c4cd2c8.png" title="57b64cbe-ce8f-4992-8c47-fac82c4cd2c8.png"/></p><p>以上案例分享: <a href="https://link.segmentfault.com/?enc=hRxQohDTVCPHH0rl43aJWQ%3D%3D.FhNL%2B%2BpFmNldWuaLWYHIqbcgTEPpSaJE%2BEYR%2Bal0RiLWvMB6OKuPjun%2F4H7A%2B3Fku0x4SmpbeQB%2FlhZZRGR7mh81Hzm1vXl0GDJmc4NzP2htT9uqUZY4JygWRD%2B5p1hol31ftxE2qGPkYw19w6OUrvyl7xPuYsdox%2BDcDhPmv9k%3D" rel="nofollow" target="_blank">https://www.huoyuyan.com/share.html?key=eyJhdXRvQ29kZSI6IkZhb...</a> 提取码: 7t2i</p><p><strong>通过这个流程，可清晰对比两种循环的本质区别：</strong><br/><strong>While 循环：</strong>因初始条件不满足，循环体完全不执行；<br/><strong>DoWhile 循环：</strong>不管条件是否满足，先执行一次循环体，再判断条件是否成立。</p>]]></description></item><item>    <title><![CDATA[.NET STS 版本支持 24 个月 ]]></title>    <link>https://segmentfault.com/a/1190000047413386</link>    <guid>https://segmentfault.com/a/1190000047413386</guid>    <pubDate>2025-11-20 10:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>.NET团队在博客上发布 《.NET STS 版本支持 24 个月》，调整 .NET 的 标准支持（STS） 版本生命周期，从原先的 18个月延长至24个月。一、STS 支持周期调整支持时长变更旧政策：STS 版本支持 18 个月（如网页 1 所述）。新政策：STS 版本现支持 24个月（如网页 2 和网页 5 明确提到）。<br/>示例：.NET 9（STS 版本）于 2024 年 11 月发布，支持至 2026 年 11 月。与 LTS 版本的对比LTS（长期支持）：至少 3 年或至下一个 LTS 发布后 1 年（以较晚者为准）。<br/>示例：.NET 8（LTS）支持至 2026 年 11 月。<br/>STS（标准支持）：24 个月固定期限，适合需要频繁更新功能的场景。二、调整背景与影响现代生命周期模型 Microsoft 采用更灵活的更新策略，缩短支持周期但提供更频繁的服务更新（如每月安全补丁），以平衡稳定性和新特性迭代。更新频率：每月发布累积更新（含安全与非安全修复）。默认行为：应用会自动使用最新安装的运行时补丁（“向前滚动”）。<br/>用户选择建议STS 适用场景：需快速获取新特性的服务型应用，团队能持续跟进版本升级。LTS 适用场景：面向消费者的客户端应用或需长期稳定的企业环境。混合策略：即使使用 STS 版本，仍推荐升级至最新 SDK 以兼容多运行时。</p><p>四、注意事项维护支持期：所有版本（STS/LTS）在支持期的最后 6 个月仅接收安全更新。终止支持风险：未及时升级的应用可能面临安全漏洞或兼容性问题。</p>]]></description></item><item>    <title><![CDATA[你的游戏没有这个怎么能够顺利出海？ 亿元]]></title>    <link>https://segmentfault.com/a/1190000047409652</link>    <guid>https://segmentfault.com/a/1190000047409652</guid>    <pubDate>2025-11-20 09:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>点击上方&lt;font color=blue&gt;亿元程序员&lt;/font&gt;+关注和&lt;font color=orange&gt;★&lt;/font&gt;星标</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409654" alt="" title=""/></p><h2>引言</h2><p><strong>哈喽大家好</strong>，不知道小伙伴们最近有没有发现，如今的游戏出海，已经不是从前的粗放买量靠堆砌素材喂算法了，现在都在拼长线运营或者<code>AI</code>了。</p><p><strong>其中</strong><code>Supercell</code>的《荒野乱斗》就是最好的例子，上线五年了，如今成功逆袭。</p><p><strong>笔者想起</strong>，八年前就已经参与过游戏的多语言版本了，那时候的主流是港台（繁体）、东南亚（英文）、韩国（韩文）。</p><p><strong>除去</strong>接入对应的<code>SDK</code>外，最为深刻的就是翻译和本地化，那时候不需要很高端的操作，就是把游戏内所有的中文整理出来，给到专门负责翻译的人，完成后导回到游戏即可。</p><p><strong>其中</strong>最头疼的就是英文，两个字能变<code>10</code>多个字母，很容易导致超框，以上的处理有个“国际化”的简称，叫<code>i18n</code>。</p><p><strong>言归正传</strong>，今天我们就来聊聊<code>i18n</code>。</p><h2>什么是i18n？</h2><p><strong>先简单科普一下</strong></p><blockquote><p><code>i18n</code>是国际化的简称，来源是英文单词<code>internationalization</code>的首末字符<code>i</code>和<code>n</code>，<code>18</code>为单词中间的字符数。</p><p><strong>在资讯领域</strong>，国际化（<code>i18n</code>）可以让产品无需做大的改变，就能够满足不同语言和地区的需要。</p></blockquote><h2>i18n的优势是什么？</h2><p><strong>对程序来说</strong>，在不修改内部代码的情况下，就能根据不同语言及地区切换相应的语言界面。</p><p><strong>正如笔者</strong>引言所说，把文本交给翻译人员，回来后原路返回即可。</p><p><strong>从</strong>最开始就制定严格的规范，所有的文本都需要通过<code>i18n</code>，这样能够减少后期提取文本和导回文本的麻烦操作。</p><h2>一起来看个例子</h2><p><strong>既然</strong><code>i18n</code>如此重要，那么我们一起来看看它在<code>Cocos</code>游戏开发中如何使用。</p><h3>1.安装插件</h3><p><strong>首先</strong>在<code>Store</code>找到<code>i18n多语言化插件</code>，选择添加到项目即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409655" alt="" title="" loading="lazy"/></p><p><strong>添加</strong>完成之后，就可以在资源管理器看到插件对应的脚本，分别对应数据、<code>Label</code>管理和<code>Sprite</code>管理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409656" alt="" title="" loading="lazy"/></p><h3>2.添加语言</h3><p><strong>首先</strong>通过菜单<code>扩展-&gt;I18n Setting</code>打开本地化控制面板。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409657" alt="" title="" loading="lazy"/></p><p><strong>简单</strong>添加<code>8</code>个语言，从下到上分别为中文、俄语、韩语、日语、法语、西班牙语、英语和德语（首字母排序）。</p><p><strong>为什么</strong>是<code>8</code>个？因为可以和别人说你“精通”八国语言（你好，世界）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409658" alt="" title="" loading="lazy"/></p><p><strong>编辑完成</strong>后，插件会自动在<code>resources\i18n</code>目录生成对应的<code>Ts</code>配置文件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409659" alt="" title="" loading="lazy"/></p><h3>3.编辑中文</h3><p><strong>在</strong><code>zh.ts</code>中添加<code>你好，世界</code>，作为示例。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409660" alt="" title="" loading="lazy"/></p><h3>4.翻译其他7种语言</h3><p><strong>把</strong>完成的所有中文内容，交给专业的翻译人员进行翻译，获得其余<code>7</code>种语言，我们这里简单示例就找搭子就行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409661" alt="" title="" loading="lazy"/></p><p><strong>还是</strong>非常靠谱的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409662" alt="" title="" loading="lazy"/></p><h3>5.LocalizedLabel组件</h3><p><strong>插件</strong>生成的<code>LocalizedLabel</code>组件，就是对<code>Label</code>的简单封装，根据<code>key</code>和语言配置获取对应的文本。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409663" alt="" title="" loading="lazy"/></p><p><strong>直接</strong>挂到<code>Label</code>组件上，配置对应的文本的<code>key</code>即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409664" alt="" title="" loading="lazy"/></p><h3>6.效果演示</h3><p><strong>我们</strong>通过点击本地化编辑面板中不同语言的“小眼睛”，即可完成语言的切换，并且看到切换语言后的效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409665" alt="" title="" loading="lazy"/></p><p><strong>上面</strong>只是编辑器演示，实际需要修改默认语言，可以在脚本中修改。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409666" alt="" title="" loading="lazy"/></p><h3>7.进阶</h3><p><strong>如果</strong>一个包体内有多种语言，想要支持动态切换语言，可以通过<code>import * as i18n from 'db://i18n/LanguageData';</code>导入<code>i18n</code>，并通过<code>i18n.init('en');</code>进行语言切换，最后通过<code>i18n.updateSceneRenderers();</code>刷新即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409667" alt="" title="" loading="lazy"/></p><p><strong>此外</strong>，<code>LocalizedSprite</code>组件也是同理，对<code>Sprite</code>的简单封装。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047409668" alt="" title="" loading="lazy"/></p><h2>结语</h2><p><strong>当然了</strong>，并非所有的项目都需要使用这套插件，国际化的逻辑还是简单的，一般公司项目都有自己的技术积累，有自己的实现。</p><p><strong>其实</strong>最主要是一个规范的问题，通过语言包的限制，避免文本到处都是。但是也使得配置表比较难以配置，这个需要小伙伴们自己权衡了。</p><p><strong>你们使用的是什么方案？</strong></p><p><strong>我是"亿元程序员"，一位有着8年游戏行业经验的主程。在游戏开发中，希望能给到您帮助, 也希望通过您能帮助到大家。</strong></p><p>AD:笔者线上的小游戏《打螺丝闯关》《贪吃蛇掌机经典》《重力迷宫球》《填色之旅》《方块掌机经典》大家可以自行点击搜索体验。</p><p>实不相瞒，想要个<strong>赞</strong>和<strong>爱心</strong>！请把该文章<strong>分享</strong>给你觉得有需要的其他小伙伴。谢谢！</p><p>推荐专栏：</p><p><a href="https://link.segmentfault.com/?enc=pBsguJ5Sb3eU8MejokzIZA%3D%3D.5eaM43WwPTaCHUYgJGRVlpFJgEw8suJytTe%2FuzcUspEAt3k1CQCzSRECmaci1bwPGjEDA9SaB7G0%2B0pZI8F25SfpEoto6wX0dR2XBvXHqEGQpUHDnaclFowZcaQhON6G6mfaPYsjkJ3mi120IxUcrHrmqWt5Ka2%2FseRmpwKDkrc%3D" rel="nofollow" target="_blank">知识付费专栏</a></p><p><a href="https://link.segmentfault.com/?enc=WQg%2F%2FR0yoKUorBNiaYYhSw%3D%3D.58zTyxj00ZOvZyZydaLt%2FPWBqsexk%2FhgwiQXQghUuCPW4XiX1pZP25efIQXN%2FwlQQFY%2F37kSwCduHWrUGp5OQAYsXVt%2FvZoz%2Bax%2BHh0ED2n1P6rM3biMvOMZpgpR2Vu5H1DKMj3fckTNRMCsDZ0S20TdszdVMPp89JXrmqen8vcncSmMNRZ%2FGpuHnDIZy%2FWGHmB9yeYgimqnx1koliL50%2BcIuZu2Qo8aL7sWIUhR1DAulAR9etPGezwBo3SpQEL2u%2FrJ2OrJsFlBUdYyz4UpD2CQuL4Fdq6SwA9l9IPN%2Bzg%3D" rel="nofollow" target="_blank">你知道和不知道的微信小游戏常用API整理，赶紧收藏用起来~</a></p><p><a href="https://link.segmentfault.com/?enc=9IgR%2BZzWQjEHgETJkRIx1g%3D%3D.9CimkFwA3ckzm%2BuZ2vWnqJ4pgtXCUi0GN1ta616efS9Cm0iQTOSsL86pkv%2FFY8ryrLPCQA5ZZ8sly2e7CwwVhf9CWiEuVzTCJmWjKegj6keqicQdHToGanzZZWQOfiILgPhgocgtmgqQlgBmmeoVGRnL6dd2fosow5TZLudz1xU%3D" rel="nofollow" target="_blank">100个Cocos实例</a></p><p><a href="https://link.segmentfault.com/?enc=CSp%2BiTGx%2F7fMy%2F%2B5Jsz%2FmQ%3D%3D.JBrA0PTkDq%2FARAAqRKX26cVRCbw%2F4VUogmP3jy0zp2R5zFR%2Bb7HDh7XV2XSmBKqw2LxM29O7IX3pl%2FL5i1uC8TLMPIpj5t%2FculOI49ECAYRkPEmHPUkAKY3GfoMMtPHXHCWIJrILn4JS1tl9YjYbef21y45qt64amkJcu7%2BQ16o%3D" rel="nofollow" target="_blank">8年主程手把手打造Cocos独立游戏开发框架</a></p><p><a href="https://link.segmentfault.com/?enc=A89RDinBIgQH8UanoeY5GA%3D%3D.uXi6yd%2Fk3h9NGe%2Bfs%2B%2BzSxa%2BjVGde04Qxh0KnSudpZFMiZbzNnb4pu2CVgWGq2lCP06miGBkvlmiYaqvOeirHuoZYc8cnHipF9zcMCftEmguygTuu9pfUWlfoTmlgBmlBdzxDBzjB66w3KD0J%2BVxGT7LzvlTCKx5H9MOv0QTstg%3D" rel="nofollow" target="_blank">和8年游戏主程一起学习设计模式</a></p><p><a href="https://link.segmentfault.com/?enc=Z5hSoNDJw5z%2FErUEnTF0MA%3D%3D.7YD7KN7wJHQz7shOGtfMaUkZGy0YBjavNH3l%2FiWCQBthU2UPjZUOghGUo%2ByFICzCR1UZjyV%2FPLR0RbmL574D6KG9Ql49kmwOxZX5s7fMsxjGXBvmKNQDUIgka1x88SnfeBNy3fxxQ9SbgLqtzgmQ36niF0v4nQX4O3DzDdxvusDGAvVDmXiyrs6ruJm08fdr" rel="nofollow" target="_blank">从零开始开发贪吃蛇小游戏到上线系列</a></p><p>点击下方&lt;font color=gray&gt;灰色按钮&lt;/font&gt;+关注。</p>]]></description></item><item>    <title><![CDATA[RubyMine 2025.2.4 11]]></title>    <link>https://segmentfault.com/a/1190000047413218</link>    <guid>https://segmentfault.com/a/1190000047413218</guid>    <pubDate>2025-11-20 09:02:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <ul><li>2025-11-20 亲测</li><li>支持最新版本2025.2.4</li><li>支持Windows、MAC、Linux<br/><img width="684" height="442" referrerpolicy="no-referrer" src="/img/bVdm6vp" alt="ruby.png" title="ruby.png"/></li></ul><h2>一 安装</h2><p>官网下载 ：<a href="https://link.segmentfault.com/?enc=ojN1D0WMDoRBuVOw%2FMalcg%3D%3D.FlOqYJ2PiW5raX5QZ0VCzVu3j8%2BcmrsgXg0%2FRGzic8hLSVvRcLOWvQeUPop5nxRt" rel="nofollow" target="_blank">https://www.jetbrains.com/zh-cn/ruby/</a><br/>根据提示安装</p><h2>二 授权说明</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413220" alt="图片" title="图片" loading="lazy"/><br/>回复 《ruby》获取<br/>新版本安装后不提示授权，需要手动处理</p><h2>三 使用</h2><p>打开自己的项目，配置环境，开始开发<br/><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdm6vt" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[IP SSL证书助力公网内网IP地址实现]]></title>    <link>https://segmentfault.com/a/1190000047413223</link>    <guid>https://segmentfault.com/a/1190000047413223</guid>    <pubDate>2025-11-20 09:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化转型加速推进的当下，网络安全的重要性日益凸显。我国的《网络安全法》《数据安全法》从不同层面强调了网络安全的重要性。SSL证书作为实现HTTPS加密与可信身份认证的有力工具，已成为构建安全网络环境的基石。</p><p>通常，我们会为域名申请SSL证书。但在许多实际场景中，存在大量只能通过IP地址直接访问的服务，此时就需要为IP地址申请SSL证书。这类证书通常被称为<strong>IP SSL证书可以</strong> <strong>助力公网内网IP地址实现HTTPS</strong>  <strong>，</strong>  为那些不依赖域名、直接通过IP提供服务的场景，提供完整的数据传输安全与身份验证解决方案。</p><h2><strong>一、什么是IP SSL证书？</strong></h2><p>IP SSL证书，是一种专门用于为IP地址实现HTTPS加密的数字证书，也可以称之为IP HTTPS证书。IP SSL证书通过在服务器和客户端之间建立加密通信通道，保障数据传输过程的机密性与完整性，解决了IP地址与服务器端的数据传输安全问题，并可帮助用户识别企业网站身份真伪。</p><p>IP SSL证书适用于多种场景，包括但不限于：物联网（IoT）设备、API服务接口、测试或临时云服务等通过IP直接提供公网访问的应用；同时也广泛用于内部系统（如OA、ERP、远程办公平台）、开发测试环境及局域网服务等内网环境。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413225" alt="企业微信截图_17635146629936.png" title="企业微信截图_17635146629936.png"/></p><h2><strong>二、IP SSL证书的作用</strong></h2><p><strong>1、数据传输安全保护</strong></p><p>IP SSL证书可助力IP地址实现HTTPS加密，在服务器和浏览器之间建立一个安全通道，以确保服务器和浏览器之间传输的所有数据保持机密性和完整性。</p><p><strong>2、网站身份可信认证</strong></p><p>IP SSL证书由证书颁发机构（CA）对IP所有权及相关身份进行验证后签发，能提高IP身份的可辨识度，防范IP仿冒与欺诈风险。</p><p><strong>3、提升用户信任度</strong></p><p>部署IP SSL证书后，用户访问IP地址时浏览器将显示“https:// ”协议及安全锁标志。若使用企业型（OV）IP SSL证书，还会展示企业名称，有利于提升用户信任度。</p><p><strong>4、满足合规性要求</strong></p><p>实现HTTPS加密可协助企业符合网络安全法、等保2.0、PCI/DSS等法规中对数据加密的要求，规避因不合规导致的法律与业务风险。</p><h2><strong>三、IP SSL证书的品牌与类型</strong></h2><p>IP SSL证书在品牌上覆盖国内外主流CA机构，类型根据验证方式、保护IP数量以及密码算法可以分为6种类型。</p><p><strong>1、主要品牌</strong></p><p>国产品牌：sslTrus、CFCA、JoySSL等可信的国产证书品牌。</p><p>国际品牌：Sectigo、GlobalSign、Digicert是具备国际声誉的国际证书品牌。</p><p><strong>2、证书类型</strong></p><p><strong>按验证方式：</strong></p><p>DV型：仅验证IP所有权，签发速度快，通常几分钟即可完成。</p><p>OV企业型：需验证IP所有权及企业真实信息，安全性更高，审核时间约为1-3个工作日。</p><p><strong>按保护IP数量：</strong></p><p>单个IP证书：保护1个IP地址，支持一个IP地址实现HTTPS。</p><p>多个IP证书：保护多个IP地址，支持多个IP地址实现HTTPS。</p><p><strong>按密码算法：</strong></p><p>RSA算法证书：兼容谷歌、火狐、Edge、Safari等国际主流浏览器。</p><p>SM2国密算法证书：适用于360、奇安信、赢达信速龙等国密浏览器。</p><p><strong>四、</strong>  <strong>IP SSL证书</strong> <strong>申请</strong></p><p>IP SSL证书申请步骤很简单，基本需要经过以下流程：</p><ul><li>确认IP地址类型（公网或内网）；</li><li>选择合适的证书品牌和类型；</li><li>提交申请证书所需要的资料；</li><li>CA会对提交的信息进行验证；</li><li>验证通过后签发证书，部署即可。</li></ul><p>总结而言，IP SSL证书能够有效帮助公网与内网IP地址实现HTTPS加密，不仅增强数据传输的安全性，也提高了IP身份的可信识别度，减少冒充风险。在企业全面推进数字化转型的背景下，部署IP SSL证书有助于构建全覆盖的安全访问体系，满足日趋严格的合规要求，为企业能够安全、稳定、持续运营提供坚实保障。</p>]]></description></item><item>    <title><![CDATA[剑指offer-40、数组中只出现⼀次的]]></title>    <link>https://segmentfault.com/a/1190000047402185</link>    <guid>https://segmentfault.com/a/1190000047402185</guid>    <pubDate>2025-11-20 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>题⽬描述</h2><p>⼀个整型数组⾥除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现⼀次的数字。</p><p>示例<br/>输入：[92,3,43,54,92,43,2,2,54,1]<br/>输出：3，1</p><h2>思路解答</h2><h3>哈希表统计</h3><p>使⽤ hashmap 存储数字出现的次数， key 为出现的数字， value 为该数字出现的次数。遍历⾥⾯所有的数字，如果 hashmap 中存在，那么 value （次数）+1，如果 hashmap 中不存在,那么 value 置为1。</p><p>遍历完成之后，需要将次数为 1 的数字捞出来，同样是遍历 hashmap ，由于只有两个满⾜条件，我们设置⼀个标识变量，初始化为1，如果找到第⼀个满⾜条件的数字，除了写⼊放回数组中，还需要将该标识置为 2 ，表示接下来找的是第 2 个。</p><p>如果找到第 2 个，那么写⼊之后，直接 return 。</p><pre><code class="java">public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) {
     Map&lt;Integer, Integer&gt; maps = new HashMap&lt;&gt;();
     if (array != null) {
         for (int n : array) {
             Integer num = maps.get(n);
             if (num == null) {
                 // map中不存在
                 maps.put(n, 1);
             } else {
                 // map中已经存在
                 maps.put(n, num + 1);
             }
         }
      }
      int index = 1;
      for (Map.Entry&lt;Integer, Integer&gt; entry : maps.entrySet()) {
          if (entry.getValue() == 1) {
             if (index == 1) {
                 num1[0] = entry.getKey();
                 index++;
             } else {
                 num2[0] = entry.getKey();
                 return;
             }
         }
     }
 }</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，需要遍历数组两次</li><li><strong>空间复杂度</strong>：O(n)，需要HashMap存储频率信息</li></ul><h3>排序遍历</h3><p>先对数组排序，然后遍历查找不连续的数字。排序后相同的数字会相邻，遍历找到不连续的数字</p><pre><code class="java">public class Solution {

    public int[] FindNumsAppearOnce(int[] nums) {
        if (nums == null || nums.length &lt; 2) {
            return new int[0];
        }
        
        // 对数组进行排序
        int[] sorted = nums.clone();
        Arrays.sort(sorted);
        
        int[] result = new int[2];
        int index = 0;
        
        // 遍历查找不连续的数字
        for (int i = 0; i &lt; sorted.length; i++) {
            // 检查当前数字是否与前后都不同
            boolean isSingle = true;
            
            // 检查前一个元素（如果不是第一个元素）
            if (i &gt; 0 &amp;&amp; sorted[i] == sorted[i - 1]) {
                isSingle = false;
            }
            
            // 检查后一个元素（如果不是最后一个元素）
            if (i &lt; sorted.length - 1 &amp;&amp; sorted[i] == sorted[i + 1]) {
                isSingle = false;
            }
            
            if (isSingle) {
                result[index++] = sorted[i];
                if (index == 2) break; // 找到两个数字后退出
            }
        }
        
        // 确保结果按升序排列
        if (result[0] &gt; result[1]) {
            int temp = result[0];
            result[0] = result[1];
            result[1] = temp;
        }
        
        return result;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(nlogn)，主要来自排序操作</li><li><strong>空间复杂度</strong>：O(1) 或 O(n)，取决于是否克隆数组</li></ul><h3>位运算（最优解）</h3><p>⾸先需要了解⼀定位运算知识，异或是指⼆进制中，⼀个位上的数如果相同结果就是0，不同则结果是0.</p><p>也就是如果⼀个数的最低位是0，另⼀个数的最低位是0，那么异或结果的最低位是0；如果⼀个数的最低位是0，另⼀个数的最低位是1，那么异或结果的最低位是1。</p><p>异或操作可以交换，不影响结果：A^B^C = A^B^C</p><p>A^A=0,任何⼀个数异或⾃身，等于0，因为所有位都相同</p><p>A^0 = A，任何⼀个数异或0，等于⾃身，因为所有位如果和0不同，就是1，也就是保留了⾃身的数值</p><p>假设⾥⾯出现⼀次的两个元素为 A 和 B ，初始化异或结果 res 为0，遍历数组⾥⾯所有的数，都进⾏异或操作，则最后结果 res = A^B 。</p><p>但是我们拿到这个 A 和 B 异或之后的结果，怎么区分呢？</p><p>有⼀种巧妙的思路，因为 A 和 B 的某⼀位不同才会在结果中出现 1 ，说明在那⼀位上， A 和 B 中肯定有⼀个是 0 ，有⼀个是 1 。</p><p>那我们取出异或结果 res 最低位的1，假设这个数值是 temp （temp只有⼀个位是1，也就是A和B最后不同的位）</p><p>遍历数组中的元素，和 temp 进⾏与操作，如果和 temp 相与，不等于0。说明这个元素的该位上也是1。那就说明它很有可能就是 A 和 B 中的⼀个。</p><p>只是有可能，其他的数也有可能该位上为 1 。但是符合这种情况的，其他数肯定出现两次，⽽ A 和 B只可能有⼀个符合，并且只有可能出现⼀次 A 或者 B 。</p><p>凡是符合和 temp 相与,结果不为0的，我们进⾏异或操作。</p><p>也就是可能出现， res1 = B^C^D^C^D...^E^E^B 或者 res1 = A^C^D^C^D...^E^E 。</p><p>上⾯的式⼦可以视为 res1 = B 或者 res1 = A</p><p>这样操作下来，我们就知道了 res1 肯定是其中⼀个只出现⼀次的数（ A 或者 B ）,⽽同时上⾯计算出了 res = A^B ,也就是可以通过 res1^res 求出另⼀个数。</p><pre><code class="java">public void FindNumsAppearOnce(int[] array, int num1[], int num2[]) {
     // A和B异或的结果
     int res = 0;
     for (int val : array) {
         res ^= val;
     }
    
     // temp保存了两个数最后⼀个不同的位
     int temp = res &amp; (-res);
     // 保存和最后⼀个不同的位异或的结果
     int res1 = 0;
     for (int val : array) {
         // 不等于0说明可能是其中⼀个数，⾄少排除了另外⼀个数
         if ((temp &amp; val) != 0) {
             res1 ^= val;
         }
     }
     // 由于其他满⾜条件的数字都出现两次，所以结果肯定就是只出现⼀次的数
     num1[0] = res1;
     // 求出另外⼀个数
     num2[0] = res ^ res1;
 }</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，只需要遍历数组两次</li><li><strong>空间复杂度</strong>：O(1)，只使用固定数量的变量</li></ul><h4>位运算原理解析</h4><p>通过示例详细解释算法过程：</p><p><strong>输入</strong>：<code>[92,3,43,54,92,43,2,2,54,1]</code><br/><strong>单次数</strong>：3 和 1</p><p><strong>步骤1：计算所有数字的异或结果</strong></p><pre><code class="java">92 ^ 3 ^ 43 ^ 54 ^ 92 ^ 43 ^ 2 ^ 2 ^ 54 ^ 1
= (92 ^ 92) ^ (43 ^ 43) ^ (2 ^ 2) ^ (54 ^ 54) ^ 3 ^ 1
= 0 ^ 0 ^ 0 ^ 0 ^ (3 ^ 1)
= 3 ^ 1 = 2</code></pre><p><strong>步骤2：找到异或结果的最低位的1</strong></p><pre><code class="java">3的二进制: 0011
1的二进制: 0001
3^1=2的二进制: 0010
最低位的1在从右往左第2位（值为2）</code></pre><p><strong>步骤3：根据最低位1分组</strong></p><ul><li><strong>第1组（第2位为0）</strong>：3(0011), 43(101011), 54(110110), 1(0001), 92(1011100)</li><li><strong>第2组（第2位为1）</strong>：2(0010)</li></ul><p><strong>步骤4：分别异或各组</strong></p><pre><code class="java">第1组: 3 ^ 43 ^ 54 ^ 1 ^ 92 ^ 43 ^ 54 ^ 92
     = (3 ^ 1) ^ (43 ^ 43) ^ (54 ^ 54) ^ (92 ^ 92) 
     = 3 ^ 1 = 2 ❌ 这里应该是 3 ^ 1 = 2，但我们需要重新计算正确的分组

让我们重新正确分组计算：
实际分组应该是：
第1组（第2位为0）：3, 1  // 只有这两个数在第2位为0且是单次数
第2组（第2位为1）：所有其他数

正确的计算：
第1组: 3 ^ 1 = 2
第2组: 92 ^ 43 ^ 54 ^ 92 ^ 43 ^ 2 ^ 2 ^ 54 = 0</code></pre><p><strong>位运算特性利用：</strong></p><ul><li>相同数字异或为0：<code>a ^ a = 0</code></li><li>任何数与0异或为本身：<code>a ^ 0 = a</code></li><li>异或满足交换律和结合律</li></ul>]]></description></item><item>    <title><![CDATA[基于Strands Agents SDK]]></title>    <link>https://segmentfault.com/a/1190000047412944</link>    <guid>https://segmentfault.com/a/1190000047412944</guid>    <pubDate>2025-11-20 08:01:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000046555790" alt="图片" title="图片"/></p><h2>背景</h2><p>AI Agent 作为当前发展最快的技术趋势之一,  不同于专注特定领域任务的传统 AI 应用, 能在更少人工干预的情况下,  管理和执行端到端的流程,  从单纯的工具进化为团队成员,  使企业生产力、效率和增长方面进入一个新的时代. 有研究报告指出: “AI Agent 竞争势头已经明显增强, 93%的企业管理者认为在接下来的一年内在企业内部规模扩展AI Agent 的使用会让企业在同类竞争者中保持领先地位”. 尽管如此, 现实情况是: 大部分企业还不知道以何种方式快速构建、运行和管理AI Agents, 将AI Agents 快速在企业内部落地应用. 基于此, 本文从AI Agent 的结构和组成入手, 结合Amazon Strands Agents SDK,  构建一个集 Agent 构建和运行的参考实现平台, 且在参考实现平台集成了AgentCore Browser user tool 和 AgentCore Code Interpreter tool, 方便用户使用相应的工具来构建Agents.</p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=th3gVnHoqLxWDyATKgjcHA%3D%3D.Fpf%2FENVBSKftVfcy0cMfaxaudotpH4gvpzYlMcnA0kiXzN%2B99slrsUcHYEeChvLxZMKUmn1KyZFn2A%2FkHHrTrqes8ubRTXt9wdeZTvYRsn8QMoNaqA5htSJbMm3vzbv%2BKACxMDIlE13f3cKP4%2BWIdmUX%2F5N%2BjFJAHQCjlacZz9xEp6mD2rKjE%2FsTUySTUElqeCpL4n9cIPHSqUSGIEt7tckJYIotnT1AEnOZy288cxA%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=3KPdbbt8VXvOUW2DaLU6WA%3D%3D.GD8fYCrMMHN6RkbT1md87M9TRt%2F2uonf1DSjcV%2FBRgYiX%2FouXs9mRR5%2FJe3uefZm8nRLPQ2fWlLlS3uAsKCFek7biZEE6bOeyFUF0ZHJvOpQTEF3l9Qt3SNIGG2LqCpf5daRIoN7upcDuvcIbDUZz%2FDpwrd3TixEQZLfLg8CluxSogBbIDWrNIbiRprWzYyUzrRfHpPbVKeVwji0gXAJfHHuFqxvolEp2UMbKdeBn2I%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote><h2>AI Agent 组成、架构及开发框架</h2><p>在Google 的Agents 白皮书中, 针对AI Agent的核心功能, 给出了其组成构件和架构, 如下图所示.</p><p><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdm5If" alt="image.png" title="image.png" loading="lazy"/></p><p>AI Agent 包含三个必不可少的组件:</p><ol><li>The model(模型)<br/>在AI Agent 范畴, 模型是指大语言模型(LLM), 作为Agent的中心化决策者, 进行任务分解、执行和检测任务是否完成. 一般来讲, Agent 中使用的模型, 基于推理和逻辑框架(如, ReAct, Chain-of-Thought, 或者 Tree-of-Thoughts), 需要有很强的指令遵循能力.</li><li>Tools(工具集)<br/> 尽管模型在文本、图片等方面具有很强的生成能力, 但由于模型不能与外部世界交互, 模型的能力受到极大约束. 而工具可以抹平这种裂痕, 使得Agents 可以与外部数据和服务进行交互, 更大范围拓展Agent 除模型本身之外的能力.</li><li>The orchestration layer(编排层)<br/>编排层描述Agent 如何 接收外部信息, 执行内部推理(Reasoning), 使用推理结果来确定下一步动作和决策的这一循环过程.这个过程会一直循环, 直到Agent完成任务目标或者达到停止点.</li></ol><p>Agent 的工作流程如下图所示:</p><p><img width="723" height="335" referrerpolicy="no-referrer" src="/img/bVdm5Ih" alt="image.png" title="image.png" loading="lazy"/></p><ol><li>AI Agents 与外部环境交互, 并从用户接收输入、触发或者目标.</li><li>通过模型的推理, 将目标分解成特定的动作和步骤, 并确定任务的顺序</li><li>任务执行过程中, AI Agents 可以访问内部的数据和工具(如知识库、企业系统)以及通过MCP 协议访问外部的工具(如第三方数据库, Web 搜索等)</li><li>AI Agent Orchestration 组件循环执行 Reasoning -&gt; Action Plan -&gt; Execution 这个Loop. 循环过程中可能会有安全护栏(Guardrails) 来确保道德、运营和安全标准</li><li>由于Agent 执行过程可能会执行多轮循环, 受限于模型的上下文窗口长度以及对模型精度的要求, AI Agent 使用 Memory 来维护执行过程的上下文, 从前面的迭代或者过去的执行中学习, 提升Agent的性能表现</li><li>在Multi-Agent 场景, Agent -to -Agent(A2A) 协议被用来实现Agent 之间的协作.</li></ol><p>Strands Agents SDK 是一个开源的AI Agent 开发框架, 使用Strands Agents SDK, 可以利用几行代码快速开发生产可用的multi-agents AI系统. 具有丰富的特性:</p><ol><li>Agent Loop: Strands Agents 实现了处理用户输入、决策、执行工具、响应生成的循环过程, 这个过程支持复杂的、多步骤的推理和动作执行, 并且可以无缝的集成Model 和</li><li>在Model 方面, 提供多种Model Provider 实现, 如Amazon Bedrock, Anthropic, OpenAI, Ollama, LiteLLM 等, 开发者可以灵活 的选择各种商业或者开源的模型来驱动AI Agent.</li><li><p>在Tools 方面, Strands Agents 支持</p><ul><li>Python Tools: 支持 以@tool 函数装饰器方式和基于Python 模块方式来定义Agent 可用的Tools</li><li>Model Context Protocol(MCP) Tools: 集成MCP Server 的Tools 作为 Agent 可用的Tools, 并且支持STDIO 和 Streamable HTTP 两种Transport 类型的MCP Server</li><li>Agent Tools: 在multi-agent 场景, Strands Agents 框架支持将其他Agent作为 当前Agent的一个Tool 来完成特定领域的任务.</li></ul></li></ol><p>Strands Agents 自带了丰富的、开箱即用的基础工具, 如文件读写、memory(mem0_memory 和agent_core_memory)、browser use(local_chromium_browser和agent_core_browser(remote))、Code Interpretation(python_repl 和 agent_core_interpreter)、多模态(image_reader, generate_image, nova_reels, diagram) 等等, 极大地方便了Agent 应用的开发, 使得开发者可以像搭积木一样, 快速开发出自己的Agent.</p><ol start="4"><li><p>在Multi-Agent 方面, Strands Agents 支持以下模式:</p><ul><li>Agent-to-Agent(A2A) protocol, 支持不同平台和实现的AI Agents 间无缝沟通</li><li>Agents as Tools, 通过编排(orchestration) Agent 来接收用户输入并决定使用哪个特定的Agent 来完成相应的任务, 也就是将特定的Agent 当具体的工具来用</li><li>Swarm, 多个Agent 以团队的方式共同来完成复杂任务, 这种方式允许Agent 间自主协作, 共享上下文以及记忆来完成共同的任务</li><li>Graph, 以有向无环图(DAG)的方式编排各个Agent, Agent作为节点, 以DAG结构图作为执行顺序, 一个节点(Agent) 的输出将传播到与其连接的节点, 并作为其输入. 这种模式可以实现非常的复杂的Agent系统</li><li>Workflow, 这种模式适合复杂的多步骤流程, 并且任务间有依赖关系, 必须等上一步执行完成后, 才能执行下一步, 每一步都由一个专家Agent 来完成对应的任务.</li></ul></li></ol><p>除上述核心特性外, Strands Agents SDK 在Agent 的安全性、应用的可观测性以及Agent 部署方面提供了原生支持, 可以说, Strands Agents SDK 提供了开发AI Agent所需的方方面面, 使开发者可以便捷的、快速的开发自己的Agent 应用.</p><h2>AI Agent 构建和运行时</h2><p>Strands Agents SDK显著提升了AI Agent的开发效率，但仍存在两个关键挑战：首先，开发完成后的Agent部署与运行管理问题；其次，尽管SDK已经极大地简化了开发流程，但依然需要编写代码，对非技术背景的业务人员形成了使用障碍. 进一步思考 :</p><ol><li>是否可以不用写代码就可以构建自己的Agent, 这样业务人员都可以快速构建想要的Agent</li><li>Agent 构建好后, 是否可以不用考虑Agent 的部署, 立即可运行, 对外提供可访问的端点</li><li>很多业务场景需要Agent 周期性运行, 是否能让配置或者构建好的Agent 按时调度运行</li></ol><p>从工程实现的视角对AI Agent进行解构，我们可以识别出其由以下四个核心组件构成:</p><ol><li>Model(LLM): Agent 的大脑</li><li>Tools: Agent 的 触手</li><li>System Prompt: Agent 的行为规范</li><li>Envrioments: Agent 需要感知的环境信息, 如系统环境变量, 当前对话的文档等</li></ol><p>基于上述考虑, 我们可以通过配置的方式 声明一个AI Agent 这四部分分别是什么, 然后通过代码根据这四部分的配置, 动态构建出一个 AI Agent 实例, 并提供Agent 的运行环境,  实现 1) 通过配置即可构建Agent, 2) Agent 构建即可运行, 使AI Agent 可以快速在企业内部进行落地实施.</p><h2>基于Strands Agents SDK 的 AI Agent 构建和运行时架构</h2><p>基于上述思考, 笔者基于Strands Agents SDK 开发了一个AI Agent 的构建和运行平台(参考实现)-<a href="https://link.segmentfault.com/?enc=q3mw%2BPHU5mkj48c25gFNnA%3D%3D.jfxIdXSNLkVvgV0yv4j%2BUtXcLV0PgnK1OjHisqohSjoD%2FyPjgmH2fx%2BLmivjRoSWi6BKIZym810mqkr4LSjqpA%3D%3D" rel="nofollow" target="_blank">AgentX</a>. 可以实现:</p><ol><li>通过配置的方式来构建AI Agent. 支持Strands Agents 自带的Tools, MCP Server Tools 以及Agent as Tool, 并且支持各种模型接入.</li><li>配置好的Agent 可以通过API Endpoint 来进行调用, 无需构建专门的Agent 运行时, 真正做到Agent 配置即可运行</li><li>支持 Agent 定时调度执行配置</li><li>支持Agent 运行历史记录管理</li></ol><p>AgentX 架构如下图所示:</p><ol><li>使用ECS 来部署整个应用, 使用ECS Fargate 作为Capacity Provider</li><li>整体应用分为前端、后端、以及各种MCP Server</li><li>使用Amazon DynamoDB 来存储 Agent 的配置以及Agent的运行历史记录</li><li>使用Amazon Lambda 和 EventBridge 实现Agent 的定时调度执行</li></ol><p><img width="723" height="724" referrerpolicy="no-referrer" src="/img/bVdm5ME" alt="image.png" title="image.png" loading="lazy"/></p><p>整个项目可以通过CDK 一键部署, 部署前需要将前、后端应用以及需要部署的MCP Server 构建成Docker 镜像并推送到的Amazon ECR 中.</p><h2>配置Agent</h2><p>部署完成后, 在Agent 管理页面, 可以配置Agent. 配置Agent的过程 实际就是对Agent 使用的模型, System Prompt, 需要使用的Tools以及环境变量参数进行设置.</p><p><img width="586" height="1000" referrerpolicy="no-referrer" src="/img/bVdm5MF" alt="image.png" title="image.png" loading="lazy"/></p><h2>运行和调用Agent</h2><p>Agent 配置完成后, 可以通过在UI 界面上选择配置好的Agent 来完成相应的任务, 如下图所示. 也可以在应用层面, 通过API Endpoint 来调用Agent. 可以看到, 我们配置好Agent 之后, Agent 立即可运行, 不需要额外的部署.</p><p><img width="723" height="160" referrerpolicy="no-referrer" src="/img/bVdm5MH" alt="image.png" title="image.png" loading="lazy"/></p><p>Agent 执行 以及 MCP Server Tools 调用如下图所示.</p><p><img width="723" height="647" referrerpolicy="no-referrer" src="/img/bVdm5MI" alt="image.png" title="image.png" loading="lazy"/></p><h2>MCP Server 管理</h2><p>可以对企业内部或者一些公开的MCP Server 进行维护和管理, 通过配置的方式,  将这些MCP Server 的 Tools 作为Agent的Tools. 目前该项目只支持Streamable HTTP Transport 类型的MCP Server.</p><p><img width="723" height="339" referrerpolicy="no-referrer" src="/img/bVdm5MJ" alt="image.png" title="image.png" loading="lazy"/></p><h2>Agent 调度</h2><p>通过配置Cron 表达式的方式, 来定时调度Agent 执行任务. 对于需要周期性让Agent 来完成某项任务的场景提供了开箱即用的支持.</p><p><img width="723" height="343" referrerpolicy="no-referrer" src="/img/bVdm5MK" alt="image.png" title="image.png" loading="lazy"/></p><h2>应用场景</h2><p>使用上述Agent 构建和运行时平台, 用户可以快速在以下场景(但不限于)落地Agent应用.</p><h2>智能数据分析</h2><p>结合数仓 Redshift MCP Server(或者其他的数据库MCP Server), 可以配置Agent 来实现基于Agent 的数据分析, 如对数据进行预测性分析或者诊断性分析. 对于简单的查数以及指标计算场景, Agent 结合数据库MCP Server就能很好的支持, 如果数据表比较多, 表间的关系比较复杂, 在结合MCP Server 的同时, 可以将Schema 信息放到RAG 中, 将RAG 也作为Agent 的工具, 来实现 Chat BI 或者Text2SQL 的场景.</p><p>通过配置Redshift MCP Server 作为Agent 的工具, 来实现游戏埋点事件数据(模拟数据)的预测性分析, 如下图所示. Agent 会充分利用MCP 工具以及的模型本身的知识, 构建数据预测模型, 对基础数据进行预测性分析, 并将分析结果以HTML页面的方式进行可视化展示.</p><p><img width="651" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MO" alt="image.png" title="image.png" loading="lazy"/></p><h2>基于Agent 的云端资源巡检</h2><p>在一些关键业务场景中, 企业IT运维人员需要定时对资源的各项运行指标进行检查, 来确保基础架构以及上层应用的稳定运行. 由于需要检查的方面比较多, 如数据库运行指标检查, EC2 运行指标检查, EKS 集群运行检查, 可以每个方面的检查配置一个专门的Agent来执行, 最后通过一个编排(Orchstrator) Agent 来统筹执行整个巡检任务, 将不同方面的巡检指派给不同的Agent, 也是Multi-Agent 的场景.</p><p>下图是一个编排 Agent 结合两个其他的Agent(MySQL 巡检Agent 和 EC2巡检Agent) 的Multi-Agent 配置.</p><p><img width="643" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MQ" alt="image.png" title="image.png" loading="lazy"/></p><p>保存配置后, 此编排Agent 就可以对Amazon EC2 和 RDS 资源进行运行指标巡检, 编排Agent 对巡检任务进行分解, 将EC2的巡检任务指派给 EC2巡检Agent, 将RDS 资源巡检的任务交给RDS巡检Agent. 其中EC2 巡检结果如下图所示, 可以看到, EC2 巡检Agent 识别到有EC2 CPU峰值利用率 超过 93.89%, 超过设定的阈值(75%)18.89%.</p><p><img width="723" height="490" referrerpolicy="no-referrer" src="/img/bVdm5MR" alt="image.png" title="image.png" loading="lazy"/></p><p>RDS 巡检结果如下图所示, RDS 巡检Agent 查找到有RDS 实例可用内存(55.28MB)小于阈值(500MB).</p><p><img width="723" height="478" referrerpolicy="no-referrer" src="/img/bVdm5MS" alt="image.png" title="image.png" loading="lazy"/></p><p>最后编排Agent对上述检查结果进行统一整理和分析, 并给出行动建议, 非常详尽, 对于查找的问题, 可以添加其他的Tools, 如slack tool, 将巡检报告以及查找的问题发送到对应的Channel, 实现告警通知.</p><p><img width="589" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MT" alt="image.png" title="image.png" loading="lazy"/></p><h2>远程Browser Use 和 Code Interpreter 沙箱</h2><p>在内容营销的场景, 可能需要通过浏览器自动化网络内容操作, 需要Agent 有Browser Use 的能力; 在一些数据分析的场景, 需要执行代码对数据进行分析, 需要有安全的代码执行环境. 对于上述两种场景, Strands Agents 已经集成了Amazon AgentCore 中的 Browser Use 和 Code Interpreter 工具, 在笔者构建的平台上也可以通过配置的方式将这两个工具集成到对应的Agent中. 下面分别是AgentCore Browser Use 和 Code Interpreter 工具在Agent中的使用示例.</p><p><img width="576" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MU" alt="image.png" title="image.png" loading="lazy"/></p><h2>亚马逊云科技知识专家</h2><p>将Amazon Knowledge MCP 作为Agent 的工具, 可以使用户立刻化身亚马逊云科技知识专家, 对于亚马逊云科技服务的使用, 最佳实践都可以通过Agent 来回答, 如下图所示.</p><p><img width="619" height="500" referrerpolicy="no-referrer" src="/img/bVdm6o3" alt="image.png" title="image.png" loading="lazy"/></p><p>由于本平台很好的提供了Agent的构建和运行时环境, 用户可以根据实际的业务场景需求来快速构建出Agent, 并将Agent应用到实际的业务场景中去. 如果业务场景需要用到更多的工具, 可以开发相应的MCP Server 或者开发对应的Strands Agents Tool 来满足Agent 的需求.</p><h2>总结</h2><p>本文对AI Agent 的组成和架构进行了阐述, 介绍了AI Agent 开发框架 Strands Agent SDK 及其相关特性, 并基于Strands Agent SDK 开发了一站式Agent 构建和运行时平台AgentX, 支持以配置的方式构建Agent, 配置完Agent 即可运行, 用户可快速将Agent 应用到的Agentic Data Analytics、智能IT巡检等业务场景中.</p><p><strong>参考资料</strong></p><ol><li><a href="https://link.segmentfault.com/?enc=XBOgilGYTBNmX1fV6QmAJg%3D%3D.hELbG3qlKNu8hJxFqtBNDs8boFdgSIueLvC%2FnY2BqeSnpRGXmZ1PjKXqgDZm%2BhlTl4eVlORJo%2FpjG0h6IU3UsoeQ%2FKR72HcBMLBwzFRhLD8%3D" rel="nofollow" target="_blank">Google Whitepaper on AI Agents</a></li><li><a href="https://link.segmentfault.com/?enc=pzNzPiao9yvUP3%2FeBmHbSg%3D%3D.ATc62mBjq%2Be%2FtnL0zpKvhjVJYTixP0cXUi8HiSCsSHvvRkG20x2pbQmsATTbboj7t6rj6EWTleM%2FVtPnWG1IiB%2F7KnJEn1N6wiZCc3bICPnMo7uxXeu1q%2B3y%2B92hgo3D" rel="nofollow" target="_blank">Rising of Agentic AI</a></li><li><a href="https://link.segmentfault.com/?enc=3OW0TlKNNJeZkIwFZji8OQ%3D%3D.f3Onwo06ePbrpmgnZAuog4C0Zo%2BEmUJIDQFmJcgW71I0oHOdohHg8Un9yYR1k%2FDqqzOSsWxn%2Fqq2%2Bokxj%2BcnCw%3D%3D" rel="nofollow" target="_blank">AgentX Repo</a></li><li><a href="https://link.segmentfault.com/?enc=Yb4CYPBnb0mSfttSAhuW%2Fw%3D%3D.vNGx%2B7MFtPrXUZxl1e%2FL5PxzRdzIfdf%2BV%2FTvdDhg8Qsxq8M6rUUwsnzdVd9pgL89qo0%2Fokbz6Sm6NpI09Hjqk8INNlxN%2F05CMWMQlkYogLhQ3Bt2bFA3%2BzfGUuIns1Q%2B" rel="nofollow" target="_blank">Amazon Bedrock AgentCore</a></li></ol><p><em>*前述特定亚马逊云科技生成式人工智能相关的服务目前在亚马逊云科技海外区域可用。亚马逊云科技中国区域相关云服务由西云数据和光环新网运营，具体信息以中国区域官网为准。</em></p><p><strong>本篇作者</strong><br/><img width="723" height="156" referrerpolicy="no-referrer" src="/img/bVdm5M2" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=UG83utKYCtbEAW%2B3xQkdaw%3D%3D.NPEfyXlGBJYiO7b1BtQ99CIok9mm9J%2B8hP8EfUGcrN8Sq4hOoJc72%2FCbVmqEtU1XTMVWpSVEirimzR%2FLoKE9OXvWkE6Mxq5X0yp2X2ISMvzTa21FeOrPTY9wYG%2FrV9YZ3VEjjUvMTNKOnevvXw5tXjiYyWp5SBEaNfoUM6dZCWK9fYqPt%2F3VbE55jysqlkk%2BO0x4jqHGHg3OB3E1oeoK2lp23OK2yaJ1BM7gLSRILLw%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=r9Zumd6Wu5JK3xTvnZJoeA%3D%3D.7MwnX2opz1UoXfHVZsTu25OYKr%2FFRteHqqn8vXJ%2BRBl4Tyk83fyI099fVcFqgb9d03ShmxRE9iCZTwavX5MUPvu0DhlkZHATXfq2erNQoP64S6N%2B%2FaGbVviIQQQ0N4wOWOXbngCY5p1QiwKQN%2BmaLYR50KPe0t9a%2F18dfZHzMtt0w0Cxxd2Anw%2BrHcqAg1Uq9YOgpnLVoJ%2BQ8K7zi1Y5kFXXY040m7EFXS7yNlAaQLM%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote>]]></description></item><item>    <title><![CDATA[蜜蜂目标检测数据集的构建与标注方法（70]]></title>    <link>https://segmentfault.com/a/1190000047412956</link>    <guid>https://segmentfault.com/a/1190000047412956</guid>    <pubDate>2025-11-20 00:03:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>蜜蜂目标检测数据集的构建与标注方法（7000张图片已标注划分）</h2><h3>背景</h3><p>随着人工智能和计算机视觉技术的不断发展，目标检测任务在多个领域中都得到了广泛的应用。尤其是在农业和生态研究领域，自动化目标检测技术逐渐成为提高生产效率、保障生态环境的重要工具。蜜蜂作为生态系统中的关键物种，其活动的监控和分析对农业、生态环境保护及科学研究都具有重要意义。</p><p>为了更好地实现蜜蜂目标检测，本数据集专门设计并收集了高质量的蜜蜂图像，旨在帮助研究人员和开发者构建和训练蜜蜂目标检测模型。通过这些数据，相关的机器学习模型可以高效识别蜜蜂，并应用于各类场景，如农业监控、生态保护和无人机监控等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412958" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>数据集获取</h3><blockquote>链接:<a href="https://link.segmentfault.com/?enc=rr73oEcuMQPIjA%2F9amxGOw%3D%3D.WFVR5x8DUFc4WHg5pEYXXwcOQQm1s%2B9ECjb6uPbR1ymKf2%2FuQrwTtg1TYycKE6wBjHPb1n9ll%2BdfUPFZpC9UkA%3D%3D" rel="nofollow" target="_blank">https://pan.baidu.com/s/19wEg4vB6d-SjhPlBJzx7aw?pwd=gy1b</a> 提取码:gy1b 复制这段内容后打开百度网盘手机App，操作更方便哦</blockquote><p>本数据集专为蜜蜂目标检测任务设计，包含了八千张高质量的图像，适用于训练、验证和测试蜜蜂检测模型。数据集中的每张图片均经过精心标注，旨在为目标检测模型提供足够的数据，帮助其高效地检测蜜蜂。</p><p>数据集结构</p><p>数据集包含三个主要部分：</p><p>训练集 (train)：用于训练目标检测模型的图像数据。</p><p>验证集 (valid)：用于在训练过程中进行验证，帮助调节模型的超参数。</p><p>测试集 (test)：用于评估最终模型的性能，确保模型的泛化能力。<br/>标签与类别</p><p>本数据集当前支持检测的唯一目标是蜜蜂。数据集中使用了以下标签：</p><p>类别数 (nc): 1</p><p>类别名称 (names): ['bees']</p><p>数据集特点</p><p>八千张图像：数据集包含了丰富的蜜蜂图像，适用于训练深度学习模型。</p><p>高质量标注：每张图片中的蜜蜂均经过精确标注，标注格式为常见的目标检测格式（如YOLO格式）。</p><p>多样化场景：数据集中的蜜蜂出现在不同的环境和场景中，包括花丛、树枝、空旷地等，增加了模型的泛化能力。</p><p>应用场景</p><p>此数据集可广泛应用于以下领域：</p><p>农业监控：自动化检测蜜蜂活动，为农业研究提供支持。</p><p>生态研究：为蜜蜂行为、种群动态等生态研究提供数据支持。</p><p>无人机监控：结合无人机图像采集，对蜜蜂进行监控和分析。</p><p>使用说明</p><p>数据格式：本数据集采用常见的目标检测数据格式，适配YOLO、Detectron2、TensorFlow等深度学习框架。</p><p>训练建议：对于YOLO等模型，可以直接利用此数据集进行训练与测试。建议将数据集按照80%-10%-10%划分为训练集、验证集和测试集。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412959" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412960" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>数据集概述</h3><p>本数据集包含了约八千张高质量的蜜蜂图像，图像内容多样且经过精确的标注，支持多种目标检测框架。数据集被划分为训练集、验证集和测试集，确保在不同阶段对模型进行全面的评估与优化。</p><ul><li><strong>训练集 (train)</strong>：用于训练目标检测模型的图像数据，帮助模型学习蜜蜂的特征。</li><li><strong>验证集 (valid)</strong>：用于在训练过程中进行验证，调整模型超参数，防止过拟合。</li><li><strong>测试集 (test)</strong>：用于评估训练后的模型性能，确保模型在实际应用中的泛化能力。</li></ul><h3>数据集详情</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412961" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>图像数量</h4><p>数据集总共包含八千张蜜蜂图像，每张图像都经过精心选择，确保图像的多样性和质量。这些图像涵盖了不同环境中的蜜蜂，包括花丛、树枝、空旷地等。每一张图片都包含至少一个蜜蜂目标，且图像中蜜蜂的位置和类别已精确标注。</p><h4>标注与格式</h4><p>每张图片中的蜜蜂都被标注为一个目标，采用常见的目标检测格式（如YOLO格式），便于与深度学习框架兼容使用。数据集支持的标签如下：</p><ul><li><strong>类别数 (nc)</strong>：1</li><li><strong>类别名称 (names)</strong>：['bees']</li></ul><p>数据格式采用了标注框（bounding box）方式，确保了数据在不同深度学习框架（如YOLO、Detectron2、TensorFlow）中的高效使用。</p><h4>场景多样性</h4><p>数据集中的蜜蜂图像呈现了多种环境背景，包括但不限于：</p><ul><li><strong>花丛</strong>：蜜蜂在花朵上采蜜或飞行。</li><li><strong>树枝</strong>：蜜蜂在树枝附近活动，适应不同的自然环境。</li><li><strong>空旷地</strong>：蜜蜂在没有遮蔽的空旷环境中飞行或停驻。</li></ul><p>这些场景的多样性大大增加了数据集的泛化能力，能帮助模型识别不同环境中的蜜蜂，增强其在实际应用中的效果。</p><h3>适用场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412962" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>本数据集广泛适用于以下领域：</p><h4>1. 农业监控</h4><p>在农业中，蜜蜂作为重要的授粉昆虫，直接影响农作物的产量和质量。利用目标检测技术对蜜蜂进行实时监控，可以帮助农业研究者分析蜜蜂活动的规律，及时发现蜜蜂数量的变化，从而为农作物的授粉提供有效支持。</p><h4>2. 生态研究</h4><p>蜜蜂的行为与种群动态在生态学研究中具有重要价值。通过本数据集，生态研究人员可以分析蜜蜂的分布、迁徙模式及其生态环境的变化。这些数据为生态系统的保护和蜜蜂种群的可持续性提供了宝贵的数据支持。</p><h4>3. 无人机监控</h4><p>结合无人机的图像采集技术，可以实现蜜蜂的大范围监控与数据采集。利用目标检测模型对无人机拍摄的图像进行分析，可以实时获取蜜蜂活动数据，辅助研究人员在广阔区域内对蜜蜂进行精确的监测。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412963" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>目标检测模型训练建议</h3><p>本数据集适用于主流的目标检测框架，如YOLO、Detectron2、TensorFlow等。为确保高效训练，以下是一些使用建议：</p><h4>1. 数据划分</h4><p>建议将数据集按照以下比例进行划分：</p><ul><li>训练集：80%</li><li>验证集：10%</li><li>测试集：10%</li></ul><p>这样的划分可以确保训练过程中模型能够在验证集上进行及时调优，同时使用测试集评估最终模型的泛化能力。</p><h4>2. 模型选择</h4><ul><li><strong>YOLO</strong>：YOLO系列模型非常适合目标检测任务，训练速度快，检测精度高，且支持实时推理。使用YOLO框架进行训练时，可以直接加载YOLO格式的标注数据集。</li><li><strong>Detectron2</strong>：Detectron2是Facebook AI研究院开发的目标检测框架，具有强大的功能和灵活性，适合进行高精度的目标检测任务。其支持多种标注格式，易于扩展与调试。</li><li><strong>TensorFlow</strong>：TensorFlow框架也是进行目标检测任务的重要选择，支持训练多种目标检测模型，如Faster R-CNN、SSD等。</li></ul><h4>3. 训练技巧</h4><p>在训练过程中，可以采用以下技巧来提高模型的性能：</p><ul><li><strong>数据增强</strong>：可以通过旋转、翻转、调整亮度等方式对图像进行数据增强，从而提高模型的鲁棒性。</li><li><strong>超参数调整</strong>：在验证集上进行模型超参数调优，特别是学习率、批量大小等关键参数。</li><li><strong>早停机制</strong>：设置早停机制，防止过拟合，并减少训练时间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412964" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3>结语</h3><p>蜜蜂目标检测数据集是一个高质量、丰富多样的数据集，适用于各类目标检测任务，特别是蜜蜂行为分析、农业监控和生态保护等领域。通过充分利用该数据集，研究人员和开发者能够训练出高效、准确的蜜蜂检测模型，并将其应用于多种实际场景中，为农业生态保护与研究提供数据支持。</p><p>随着目标检测技术的不断进步，未来我们期望能进一步扩展数据集，增加更多的图像类型和检测目标，为深度学习和人工智能技术的广泛应用提供更强大的数据支持。</p>]]></description></item><item>    <title><![CDATA[为什么实时更新场景下 Doris 查询性]]></title>    <link>https://segmentfault.com/a/1190000047412991</link>    <guid>https://segmentfault.com/a/1190000047412991</guid>    <pubDate>2025-11-20 00:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数据驱动的商业环境中，企业越来越依赖数据分析来驱动决策。无论是用户行为分析、业务报表还是运营监控，企业都需要具备快速、高效的数据处理能力。企业在数据分析能力上的演进，往往始于 TP（事务处理）系统，随着业务发展不断探索 TP 系统的扩展方案，最终走向构建独立的 AP（分析处理）系统。</p><h2>企业实时分析典型演进过程</h2><h3>第一阶段：使用 TP 系统支撑事务处理和数据分析</h3><p>在企业信息系统建设的早期，主要存储在 OLTP（在线事务处理）系统中，比如 PostgreSQL、MySQL、SQL Server 等。因为数据“就在那儿”，最自然的方式就是直接从 TP 系统中执行 SQL 查询来获取所需分析数据：</p><ul><li>查询订单、用户、库存、交易等业务数据；</li><li>生成运营报表，支撑内部管理；</li><li>快速开发查询接口，满足临时的 BI 需求。</li></ul><p>在业务初期，数据规模有限，分析需求也相对简单，系统架构轻量，能够高效支撑当下业务，避免了多系统部署和复杂数据流带来的额外成本与运维压力。然而，随着业务快速发展，数据量迅速增长，分析场景愈发复杂，查询延迟上升，事务与分析负载相互影响，原有系统逐渐难以支撑持续扩展的业务需求。</p><h3>第二阶段：探索 TP 系统的扩展方案</h3><p>为了在不破坏 TP 系统稳定性的前提下支撑持续扩展的业务需求，部分企业开始尝试基于 TP 的扩展方案。以下是行业中常见的做法与代表系统：</p><p><strong>1. 分片与分布式扩展：Citus、TiDB、Vitesse</strong> </p><p>这些系统具备了基于关系数据库的水平扩展能力，使 TP 系统可以存储更大规模的数据并处理更高并发。</p><ul><li><strong>Citus</strong>：是 PostgreSQL 的分布式扩展插件，可将表自动切分到多个节点，实现并行处理和查询加速。</li><li><strong>TiDB</strong>：兼容 MySQL 协议的分布式 HTAP 数据库，事务与分析融合，适用于在线业务+报表的场景。</li><li><strong>Vitesse</strong>：基于 MySQL 的分布式中间件，解决数据库扩展、容错和自动化问题，常用于大规模 TP 系统。</li></ul><p><strong>2. 读写分离与多副本架构：Amazon Aurora</strong></p><ul><li><strong>Amazon Aurora</strong> 提供了高性能、弹性扩展的云数据库，并支持最多 15 个只读副本，用于分担查询压力。这类方案适用于中等复杂度的分析任务，但在海量数据和复杂查询面前，Aurora 等 TP 架构依然面临查询瓶颈。</li></ul><p>这些方案虽然在一定程度上缓解了单机性能瓶颈，但本质仍属于 TP 系统架构，难以根本解决事务与分析并存所带来的矛盾。面对大规模、多维度的聚合分析，这类系统能力有限，查询操作常常干扰写入，导致系统性能波动，影响整体稳定性。同时，架构复杂、运维门槛高，随着写入量和查询压力的持续上升，资源消耗不断加剧，系统成本快速攀升，性能瓶颈也日益显现。尤其在高吞吐数据导入和实时更新方面能力不足，限制了对业务变化的快速响应。而 TP 系统以行存为主的特性，使其在处理 TB 级以上数据的分析任务时，性能与专为分析设计的 AP 系统存在显著差距，难以胜任更复杂、更大规模的分析需求。</p><h3>第三阶段：复杂分析使用 AP 系统</h3><p>随着数据量不断增长、分析需求日益复杂，很多企业逐渐意识到，无法再依赖原有的 TP 系统同时满足事务处理与分析需求。因此，企业通常会将一些复杂的分析查询迁移到专门的 AP 系统中，例如 Redshift、Snowflake、BigQuery 等，用于支撑大规模的数据分析任务。而对于对实时性要求高、并发量大的查询，仍会保留在 TP 系统中运行，以确保系统的快速响应和稳定性。在一些高并发场景中，数据甚至会在 AP 系统中完成加工处理后，回流到 TP 系统中，进一步支撑实时查询和业务服务。</p><h3>第四阶段：拥抱 AP 系统，实现分析计算与事务的解耦</h3><p>随着数据规模持续扩大，事务处理系统（TP）的数据导入速度难以跟上行为数据生成的节奏，导致数据延迟持续增加。与此同时，部分复杂查询被转移到分析处理系统（AP）执行，其他分析任务仍在 TP 系统中完成，这使得系统运维难度和资源成本不断攀升，远超专门的 AP 系统。</p><p>下表总结了 OLTP 与 OLAP 在关键维度上的主要区别，便于理解两者在架构定位上的差异。</p><h3>OLTP vs OLAP 对比</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412993" alt="OLTP vs OLAP 对比.jpeg" title="OLTP vs OLAP 对比.jpeg"/></p><p>面对这种挑战，越来越多企业开始认识到：对于实时分析场景，采用具备高并发与高性能的一体化 AP 系统来统一承载，能够大幅提升效率；而批量处理和离线分析等需求，则可以选择更适合的 AP 系统来完成。这样不仅优化了整体架构，也有效控制了成本，同时提升了数据分析能力，助力企业实现更加高效的数据驱动运营。</p><p>实时数据需求直接导入实时 OLAP 系统，常见做法是将事务处理系统（TP）与分析处理系统（AP）解耦，通过变更数据捕获（CDC）技术，实现 TP 系统数据的实时同步，同时行为数据也直接写入实时 OLAP。实时 OLAP 系统需具备快速更新能力和高效查询性能。该架构不仅避免了对核心业务系统的影响，还使企业能够第一时间获取并分析最新数据，广泛应用于用户行为分析、实时报表、风险监控等关键场景，显著提升了数据决策的及时性和价值。</p><h2>OLAP 系统的选择：为什么是 Apache Doris？</h2><p>Apache Doris 适合大数据量下需要高并发查询、AdHoc 和实时数据更新的场景：</p><h3>低延迟高吞吐写入</h3><p>支持多种数据导入方式，通过 <code>Stream Load</code> 可实现数据<strong>秒级可见</strong>、单节点写入吞吐可达<strong>百万行/秒</strong>，轻松满足海量数据的实时入库需求。</p><h4>用户案例：网易云音乐</h4><p>网易云音乐作为知名音乐流媒体平台，每天产生大量用户行为数据、业务数据及日志数据，这些数据在异常行为跟踪、客诉问题定位、运行状态监控、性能优化等方面扮演守护者的角色。面对每日万亿级别数据的增量，<strong>网易云音乐选择使用 Apache Doris 替换 ClickHouse 构建新的日志平台，目前已稳定运行 3 个季度，规模达到 50 台服务器，2PB 数据，每天新增日志量超过万亿条，峰值写入吞吐达 6GB/s。</strong></p><p><strong>在低延迟高吞吐写入场景中</strong>，网易云音乐采用 Flink + Doris Connector 实现流式数据无缝对接，通过多项关键优化措施显著提升了写入性能：</p><ul><li><strong>写入流程优化</strong>：在 append 数据操作时，直接写入压缩流，无需经过 ArrayList 中转，大幅降低内存使用，TM 内存占用从 8G 降至 4G，有效避免了因 batch size 设置过高导致的 OOM 问题。</li><li><strong>单 tablet 导入功能</strong>：开启单 tablet 导入功能（要求表使用 random bucket 策略），极大提升写入性能，解决了写入 tablet 过多时元数据产生过多影响写入性能的问题。</li><li><strong>负载均衡优化</strong>：每个 batch flush 完成后随机选择 BE 节点写入数据，解决 BE 写入不均衡问题，相较之前导入性能提升 70%。</li><li><strong>容错能力增强</strong>：调整 failover 策略，优化重试逻辑并增加重试时间间隔，当 FE/BE 发生单点故障时能自动感知和重试恢复，保证服务高可用。</li></ul><p><strong>在元数据性能优化方面</strong>，针对 HDD 硬盘环境下 Stream Load 耗时突增问题，通过调整 3 台 Follower FE 为异步刷盘模式，实现了 4 倍性能提升，有效解决了同步元数据阶段的严重耗时问题。</p><p><strong>整体收益显著</strong>：查询响应整体 P99 延迟降低 30%，并发查询能力从 ClickHouse 的 200 提升至 500+，写入稳定性大幅改善，运维成本显著降低，在坏盘和宕机场景下实现自恢复能力。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=gucMtvDM7k6n1LBPsCpIgw%3D%3D.v%2Bf67bzW9WNwQCZJRQMDmBt6gHCa7kv8LwAgVj7q16v%2FXJG66SudSQSnZzGHjmsq" rel="nofollow" target="_blank">网易云音乐基于 Apache Doris 的万亿级日志平台建设实践</a></p><h3>实时更新能力强</h3><p>配合主流数据同步工具（如 Kafka、Canal、DataX、SeaTunnel 等）可实现从 TP 系统到 Doris 的准实时数据同步。其“Merge-on-Write”的更新机制兼顾写入性能与更新效率，适配主流 CDC 场景。</p><h4>用户案例 ：中通快递</h4><p>随着中通快递业务的持续增长，昔日双 11 的业务高峰现已成为每日常态，原有数据架构在数据时效性、查询效率、与维护成本方面，均面临着较大的挑战。为此，中通快递引入 SelectDB，借助其高效的数据更新、低延时的实时写入与优异的查询性能，在快递业务实时分析场景、BI 报表与离线分析场景、高并发分析场景中均进行了应用实践。</p><p><strong>在实时分析场景中，基于 SelectDB 灵活丰富的 SQL 函数公式、高吞吐量的计算能力，实现了结果表的查询加速，能够达到每秒上 2K+ 数量级的 QPS 并发查询，数据报表更新及时度大大提高。</strong></p><p>SelectDB 的引入满足了复杂与简单的实时分析需求。目前，SelectDB 日处理数据超过 6 亿条，数据总量超过 45 亿条，字段总量超过 200 列，并实现服务器资源节省 2/3、查询时长从 10 分钟降至秒级的数十倍提升。</p><p>案例回顾：<a href="https://www.bilibili.com/video/BV1eF6HY9Ecr/" target="_blank">中通快递基于 SelectDB 实时数仓的应用实践</a></p><h3>极致的查询性能</h3><p>Doris 天生为分析优化，具备列式存储、向量化执行引擎、位图索引、多级缓存、物化视图等优化手段，能够支撑亚秒级的分析响应时间，并支持复杂的多维分析、聚合与 JOIN 查询。</p><h4>用户案例 ：拉卡拉</h4><p>拉卡拉（股票代码 300773）是国内首家数字支付领域上市企业，从支付、货源、物流、金融、品牌和营销等各维度，助力商户、企业及金融机构数字化经营。随着实时交易数据规模日益增长，拉卡拉早期基于 Lambda 架构构建的数据平台面临存储成本高、实时写入性能差、复杂查询耗时久、组件维护复杂等问题。<strong>拉卡拉选择使用 Apache Doris 替换 Elasticsearch、Hive、HBase、TiDB、Oracle/MySQL 等组件，完成 OLAP 引擎的统一，实现了查询性能提升 15 倍、资源减少 52% 的显著成效。</strong></p><p><strong>在极致查询性能场景中</strong>，拉卡拉充分发挥了 Apache Doris 天生为分析优化的多项能力，在金融核心业务中实现了显著的性能提升：</p><ul><li><strong>风控场景查询优化</strong>：替换 Elasticsearch 后，查询响应时间从 15 秒缩短至 1 秒以内，查询性能提升 15 倍。Doris 的标准 SQL 查询接口和强大的多维分析能力，支持复杂的多表 JOIN、子查询和窗口函数等场景。</li><li><strong>对账单系统高并发处理</strong>：借助 Doris 优秀的并发处理和极速查询能力，支持每日亿级数据规模和百万级查询请求，查询 99 分位数响应时长控制在 2 秒以内，数据延迟可控制在 5 秒。</li><li><strong>倒排索引加速大表关联</strong>：通过添加倒排索引、调整分桶策略以及表结构优化，大表关联查询耗时从 200 秒缩短至 10 秒，查询效率提升超过 20 倍。</li><li><strong>Light Schema Change 灵活变更</strong>：相比 Elasticsearch 需要通过 Reindex 进行 Schema 变更，Doris 的 Light Schema Change 机制更加高效灵活，支持字段和索引的快速增删修改，极大提升了数据管理的便捷性和业务适应性。</li></ul><p><strong>整体收益显著</strong>：服务器数量下降 52%，开发运维效率大幅提升，通过统一的 OLAP 引擎简化了技术架构，降低了学习成本和运维复杂性。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9RWTXpccH0RKojqONkJoZQ%3D%3D.JQykpenpgUuz%2FycWb%2BoIaJAcmw%2FZd3Q3lz7JWwlGa7hpDgEDyEHclmn9x0HFVI9x" rel="nofollow" target="_blank">拉卡拉 x Apache Doris：统一金融场景 OLAP 引擎，查询提速 15 倍，资源直降 52%</a></p><h3>高并发处理能力</h3><p>得益于 MPP 架构，Doris 可轻松扩展计算资源，支持海量用户并发访问分析报表，是支撑数据门户、运营后台、用户行为分析等实时应用场景的理想方案。</p><h4>用户案例：快手</h4><p>快手作为知名短视频平台，OLAP 系统为内外多个场景提供数据服务，每天承载近 10 亿的查询请求。原有湖仓分离架构面临存储冗余、资源抢占、治理复杂等问题。<strong>快手通过引入 Apache Doris 湖仓一体能力替换 ClickHouse，升级为湖仓一体架构，涉及数十万张表、数百 PB 的数据增量处理。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 凭借强大的 MPP 架构和分布式查询引擎，为快手提供了卓越的并发查询支撑能力：</p><ul><li><strong>海量并发查询支撑</strong>：系统每天承载近 10 亿查询请求，覆盖 ToB 系统（商业化报表引擎、DMP、磁力金牛、电商选品）和内部系统（KwaiBI、春节/活动大屏、APP 分析、用户理解中心等）的高并发访问需求。</li><li><strong>智能查询路由</strong>：通过查询路由服务分析和预估查询的数据扫描量，将超大查询自动路由到 Spark 引擎，避免大查询占用过多 Doris 资源，确保高并发场景下的系统稳定性。</li><li><strong>物化视图透明改写</strong>：结合 Doris 的物化视图改写能力和自动物化服务，实现查询性能提升至少 6 倍，百亿级别以下数据可实现毫秒级响应，有效支撑高并发查询场景。</li><li><strong>缓存优化加速</strong>：通过元数据缓存和数据缓存双重优化，元数据访问平均耗时从 800 毫秒降至 50 毫秒，显著提升高并发场景下的查询响应效率。</li></ul><p><strong>整体收益</strong>：实现了统一存储和链路简化，无需数据导入即可直接访问湖仓数据，在支撑海量并发查询的同时大幅降低了运维复杂度和存储成本。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=BPOhVPoCwZe659CHxQTEJQ%3D%3D.wVLvr0LG%2FDtEN%2BiRNHtzmkrnlam20s5eLzJtfS6zGYQRGAVXjcC30urhITwapzr3" rel="nofollow" target="_blank">快手：从 Clickhouse 到 Apache Doris，实现湖仓分离向湖仓一体架构升级</a></p><p><strong>统一数据体验</strong> Doris 提供类 SQL 的接口，兼容 MySQL 协议，易于 BI 工具对接（如 Tableau、Power BI、Superset 等），同时可通过视图、物化视图等能力，提供类似数据仓库的建模支持。</p><p>可以满足的典型场景包括：</p><ul><li><strong>面向用户的实时分析（Customer-Facing Analytics）</strong> 将订单、交易、行为等业务数据从数据库中捕获并同步至实时数仓（如 Apache Doris），支持用户在前端系统中秒级查看订单状态、活动参与情况、积分变化等。提升用户体验的同时，也为推荐、搜索等系统提供最新的数据支持。</li><li><strong>运营监控与分析</strong> 运维、客服、市场等部门可以实时查看关键指标（如系统交易量、失败率、退货率），并快速响应业务波动。CDC 保证了数据与业务系统的实时一致性，使监控结果具有可信度。</li><li><strong>模型训练与特征回填</strong> 将用户行为日志和业务数据同步到分析库后，ML 工程师可以基于最新数据生成训练样本、回填特征值，显著加快模型迭代速度，提升预测准确率。</li><li><strong>多维分析与自助 BI</strong> 将结构化业务数据实时汇聚到 OLAP 系统，结合维度模型，支持业务人员进行灵活多维分析，满足从明细到聚合的多层级洞察需求，减少对开发人员的依赖。</li></ul><h2>ClickHouse 实时更新原理</h2><p>在 ClickHouse 中，虽然底层存储以追加为主，但通过 <code>ReplacingMergeTree</code> 引擎，用户可以实现类似“实时更新”的效果。其核心思想是：在写入时保留所有版本的数据，在后台合并时自动保留最新版本，从而实现数据的“更新”。</p><blockquote>详情<a href="https://link.segmentfault.com/?enc=9WhmDjn7tkDsjNwUmlqbHg%3D%3D.PdmIPkBVO%2BU9Co7%2B0OSNi%2BM9Tg7qx7rC%2B39tHAspchSCAmCWAu6pJeldDFCJ2zV72kgpe8hVH7j%2FaihHEW7g4g%3D%3D" rel="nofollow" target="_blank">参考文档</a></blockquote><p>具体工作原理如下：</p><ol><li><strong>写入阶段</strong> 使用 <code>ReplacingMergeTree</code> 创建表时，通常会指定一个唯一标识列（如主键）和一个版本列（如 <code>update_time</code>）。每次对同一主键的数据更新时，系统不会直接覆盖旧数据，而是插入一行新版本的记录。</li><li><strong>合并阶段（Merge）</strong> ClickHouse 的后台合并线程会在空闲时自动执行数据文件的合并操作。对于 <code>ReplacingMergeTree</code> 表，合并过程中会根据主键值和版本列，自动保留每组主键下版本最新的记录，删除旧版本，实现最终的“更新”语义。</li><li><strong>查询阶段</strong> 查询过程中可能会读到尚未合并的多个版本记录，因此建议设置 <code>FINAL</code> 查询语法，如：</li></ol><pre><code class="SQL">SELECT * FROM my_table FINAL;</code></pre><h2>Apache Doris 实时更新原理</h2><p>在需要频繁更新数据的场景中，可以使用 Doris 提供的 Unique Key 模型来建表，实现对同一主键的数据进行高效覆盖更新。Doris 通过一种名为标记删除（Delete Bitmap）的机制，有效提升查询性能。与 ClickHouse 查询时进行更新清理的方式不同，Doris 的标记删除机制无需在查询时实时计算删除逻辑，因此可以显著减少查询延迟，确保查询响应时间稳定在百毫秒以内，并支持高并发访问。</p><p>具体来说，Doris 的处理分为两个阶段：</p><ol><li><strong>写入阶段</strong> 在使用 Unique Key 创建表时，您通常会指定一个唯一标识（如主键 ID）和一个版本列（如更新时间 <code>update_time</code>）。每当新数据写入时，如果主键相同且版本更新，Doris 会自动为旧数据打上“删除标记”，这些信息随着数据一同写入底层存储。</li><li><strong>查询阶段</strong> 在查询过程中，Doris 会自动识别并跳过那些已被标记删除的旧数据行，无需实时对比或扫描多个版本，从而实现低延迟、高效率的数据读取。</li></ol><p>借助这套机制，Doris 能够同时满足 <strong>实时更新</strong> 和 <strong>高速查询</strong> 的双重需求，非常适合用于用户画像、订单中心、指标快照等典型更新型分析场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412994" alt="Apache Doris 实时更新原理.png" title="Apache Doris 实时更新原理.png" loading="lazy"/></p><h2>性能对比</h2><p>当分析负载从 TP 或者 HTAP 演进到 AP 时，一个常见场景是将 TP 系统中的变更数据（通过 CDC）同步到 AP 系统，用于后续的报表分析和业务监控。这类场景通常涉及大量的<strong>数据更新</strong>，而不仅仅是新增数据，因此对分析系统的更新处理能力和查询性能提出了更高要求。</p><p>为了评估 Doris 和 ClickHouse 在这一类实时更新分析场景下的表现，我们基于典型的行业测试模型 <strong>ClickBench</strong> 和 <strong>SSB（Star Schema Benchmark）</strong> 进行了测试，分别对数据集中的 <strong>25% 和 100% 的记录进行了更新操作</strong>。</p><blockquote><a href="https://link.segmentfault.com/?enc=Rkz5uIE8kpcxxfeFJSPZVg%3D%3D.hohOd08le8PguJkPeOUw95wmr8gEbXWCfKF0X29BqoIq112yHdu%2FPnNEvcb10VAlwf%2BdtA3v6WvdOUJC8EBzkdoCfNlmeuUXAtCfRdEK%2Fps%3D" rel="nofollow" target="_blank">更新 SQL </a> 详情参考</blockquote><p>为确保性能对比的合理性，结合 ClickHouse Cloud 与 SelectDB Cloud 套餐配置的差异，制定了如下测试方案：ClickHouse Cloud 采用双副本，单副本分别配置为 8 核 32GB 和 16 核 64GB；SelectDB Cloud 则采用单副本 16 核 128GB 配置。通过该设计，可在整体资源层面分别实现核数对等（16 核）与内存对等（128GB）的横向对比，从而更全面地评估两者在不同资源维度下的性能表现。</p><p>原始数据：<a href="https://link.segmentfault.com/?enc=n3tBH%2FAGA7dRV4DL7JnIdw%3D%3D.jgTOPi98ezE6vdePS6GxXRUsTlhWYKtVVUWt7zUJqp%2Bfz%2Bq0SxuKgdNF8WSUPYB5PbewiwOwHI2PHibs6T6S1w%3D%3D" rel="nofollow" target="_blank">ssb</a></p><h3>SSB-sf100</h3><h4>ClickHouse MergeTree  vs  SelectDB Duplicate Key</h4><ul><li>SelectDB （16c 128GB）的性能是 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）的 5 倍。</li><li>SelectDB （16c 128GB）的性能是 ClickHouse 16c 64GB（2 replica 每个 replica 16c 64GB）的 9.8 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412995" alt="性能对比-1.png" title="性能对比-1.png" loading="lazy"/></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>25% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 1.6 倍。</li><li>100% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 2.5 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412996" alt="性能对比 02.png" title="性能对比 02.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 14 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 18 倍。</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 25 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 34 倍。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412997" alt="性能对比-2.png" title="性能对比-2.png" loading="lazy"/></p><h3>ClickBench</h3><p>原始数据：<a href="https://link.segmentfault.com/?enc=8%2FUxz8kvcI2zK9nU8NPdEA%3D%3D.jJ%2Fah4d%2Ffwb3VImorzaQ8W3mlwVw3jgjVCfafRQjbrgiW9tOzg9SUhQjZwxoqarnQDuS4dg2gYXcFXZQfureXg%3D%3D" rel="nofollow" target="_blank">clickbench</a></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>ClickBench 下 25% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 170%。</li><li>ClickBench 下 100% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 290%。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412998" alt="性能对比-3.png" title="性能对比-3.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412999" alt="性能对比-4.png" title="性能对比-4.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，查询耗时低 43%</li><li>100% 更新比例条件下，查询耗时低 60%</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，查询耗时低 68%。</li><li>100% 更新比例条件下，查询耗时低 78%。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413000" alt="性能对比-5.png" title="性能对比-5.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413001" alt="性能对比-6.png" title="性能对比-6.png" loading="lazy"/></p><h2>用户案例</h2><h3>案例一：森马服饰（MySQL）</h3><p>森马服饰作为中国休闲服饰和童装领域的领先企业，覆盖线上线下全渠道零售，门店总数达到 8000+ 家。为支撑全域货通中台项目，<strong>森马引入阿里云 SelectDB 替换原 Elasticsearch + 分布式 MySQL 混合架构，统一分析 16+ 核心业务，实现复杂查询 QPS 提升 400%，响应时间缩短至秒级。</strong></p><p><strong>在高并发处理场景中</strong>，阿里云 SelectDB 凭借 MPP 架构为森马提供了强大的并发查询支撑：</p><ul><li><strong>多场景并发支撑</strong>：同时支撑 2B 业务、2C 业务、直营店、加盟商等多场景下的高并发数据分析需求，复杂查询 QPS 达到 200+ 水平。</li><li><strong>资源隔离能力</strong>：基于存算分离架构，在线订单查询服务和离线聚合分析 BI 场景分别使用独立计算组，避免相互干扰，确保高并发场景下系统稳定性。</li><li><strong>弹性扩缩容</strong>：在直播大促等高压力时段，可快速在线扩容应对流量激增，无需停服和数据搬迁，显著提升应对突发高并发的灵活性。</li><li><strong>统一架构简化</strong>：替换双系统架构，统一支持简单过滤查询、海量数据聚合分析、复杂多表关联查询，无需维护复杂业务逻辑来处理高并发多表关联分析。</li></ul><p><strong>显著收益</strong>：亿级库存流水聚合查询缩短至 8 秒内，运维成本大幅降低，业务高峰期系统运行平稳，为全渠道运营提供可靠的高并发数据分析支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=UN61ZOZgmF83qRcbsXftQw%3D%3D.YZGUTNWiBDhN%2B4cyoL7xkBH68wsgKemvxTWKdFKOUZDE%2BFYikwil8tmPB236tG%2FI" rel="nofollow" target="_blank">森马服饰从 Elasticsearch 到阿里云 SelectDB 的架构演进之路</a></p><h3>案例二：天眼查（PostgreSQL）</h3><p>天眼查是一家数据服务公司，为用户提供超过 3 亿家企业的商业、财务和法务信息查询服务，涵盖 300+ 维度数据。随着业务增长，其尽调平台需要支持内部营销和运营团队的即席查询及用户分群等新需求。<strong>该平台使用 Apache Doris 替换了原有的 Apache Hive、MySQL、Elasticsearch 和 PostgreSQL 混合架构，实现数据写入效率提升 75%，用户分群延迟降低 70%。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 的 MPP 架构为平台提供了强大的并发查询支撑能力：</p><ul><li><strong>即席查询能力</strong>：原架构每次新需求都需要在 Hive 中开发测试数据模型，写入 MySQL 调度任务。现在 Apache Doris 拥有全量明细数据，面对新请求只需配置查询条件即可执行即席查询，仅需低代码配置即可响应新需求。</li><li><strong>高效用户分群</strong>：在结果集小于 500 万的用户分群场景中，Apache Doris 能够实现毫秒级响应。通过连续密集的用户 ID 映射优化，用户分群延迟降低 70%，显著提升高并发分群任务处理效率。</li><li><strong>统一架构简化</strong>：消除了多组件间的复杂读写操作，无需预定义用户标签，标签可基于任务条件自动生成，大幅简化用户分群流程，提高 A/B 测试的灵活性。</li><li><strong>稳定数据写入</strong>：支持每天近 10 亿条新数据流入，使用不同数据模型适配不同场景（MySQL 数据采用 Unique 模型，日志数据采用 Duplicate 模型，DWS 层数据采用 Aggregate 模型）。</li></ul><p><strong>显著收益</strong>：数据仓库架构更加简单，对开发者和运维人员更加友好，2 个 Apache Doris 集群承载数十 TB 数据，为客户提供实时、准确的企业信息查询服务。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=nwBEl0YGmwWgqY4Db6LnAA%3D%3D.ljdueJtfWkhKHmOTA5aiOFa%2FY5%2BkoVE0ETjGd8gMVwO4FFclwvbvjgzeqBu1dDvQ" rel="nofollow" target="_blank">秒级数据写入，毫秒查询响应，天眼查基于 Apache Doris 构建统一实时数仓</a></p><h3>案例三：宝舵 BOCDOP（TiDB）</h3><p>宝舵是宝尊集团旗下商业化独立品牌，拥有 1000 余名技术工程师，为集团 8000+ 员工和全球 450+ 品牌提供电商全渠道数据分析服务。<strong>宝舵早期基于 TiDB 构建实时数仓，随着数据量增长面临处理效率、OLAP 扩展、成本等挑战。通过引入 SelectDB 替换 TiDB，实现写入速度提升 10 倍，成本直降 30% 的显著成效。</strong></p><p><strong>在实时更新场景中</strong>，SelectDB 为宝舵提供了强大的实时数据处理能力，特别体现在多源数据同步方面：</p><ul><li><strong>多源异构数据实时接入</strong>：支持 100+ 业务模块的多渠道数据实时接入，通过 Canal、Mongo-Connector、OGG 等工具获取 MySQL、MongoDB、Oracle 等不同类型业务数据库的 binlog，实现秒级延迟数据同步。</li><li><strong>高吞吐实时写入</strong>：利用分区分桶策略与单副本写入机制，在双 11 峰值时段实现每秒百万级数据写入，最高写入速度从 20 万/分提升至 230 万/分，较传统方案提升 10 倍。</li><li><strong>流式数据处理</strong>：通过 Kafka + Flink + SelectDB 流式写入能力，将分散在订单、支付、物流等业务模块的数据实时汇聚，数据同步提速 30%。</li><li><strong>资源隔离保障</strong>：为"作战室看板"单独分配计算资源组，避免高并发查询与实时写入的资源争用，确保关键业务查询响应时间稳定在 500ms 内。</li></ul><p><strong>显著收益</strong>：在双 11 等大促期间数据量达平日 30-60 倍的情况下，实现数据供应 0 事故、报表服务可用性 99.9%，查询性能提升 66%，为多渠道电商运营提供稳定的实时数据支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9wdZQTdAQ9RnY4TC0CbTMw%3D%3D.ATu7H%2Bp3yL5tAvJOK7R9UBSyD7PImHJj%2FYZI8BN%2F%2F%2FAa5UOz%2BB7nFVobDXTqwOlw" rel="nofollow" target="_blank">SelectDB 实时分析性能突出，宝舵成本锐减与性能显著提升的双赢之旅</a></p>]]></description></item><item>    <title><![CDATA[恶意滥用行为 留胡子的饼干_dlibGI]]></title>    <link>https://segmentfault.com/a/1190000047413006</link>    <guid>https://segmentfault.com/a/1190000047413006</guid>    <pubDate>2025-11-20 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从生成式 AI 的惊艳亮相引起全球科技巨头军备竞赛般的投入开始，整个 AI 行业仿佛被注入了无限的想象力。似乎在宣告着即将出现一个生产力即将被彻底解放、商业模式即将被完全颠覆的光明未来。<br/>微软、谷歌、亚马逊等云巨头纷纷将资本支出的绝大部分押注于 AI 基础设施建设，而无数逐利而来的 AI 初创公司，更是如雨后春笋般涌现试图分一杯羹，全球 AI 领域的投资额也达到了史无前例的高度。<br/>然而，正如任何过热的淘金热最终都会迎来冷静期当技术以超乎预期的速度普及时，潜在的负面效应也以同样的速度被放大，正在悄然侵蚀着行业参与者。<br/>从 " 可选项 " 到 " 必选项 " 的巨额支出<br/>根据奇安信集团对外发布《2024 人工智能安全报告》来看，在 2023 年基于 AI 的深度伪造欺诈便已暴增了 3000%，基于 AI 的钓鱼邮件也增长了 1000%；而内容生成环节更是实现规模化生产。<br/>基于 Stable Diffusion 和 GPT-4 的定制模型，可每小时生成 2000 条伪原创研报、800 段逼真视频。暗网平台 "DarkGPT" 更是提供包月服务，1 万美元即可获得每日 5000 条金融虚假内容的产能。<br/>而且 "AI 滥用 " 的后遗症并不仅仅在社会新闻版块，可以说它已经穿透了科技公司的防火墙直接作用于其财务报表。而金融行业正是这场风暴的中心，当 AI 以假乱真的能力被精准地应用于金融诈骗时，其破坏力可以说是指数级的增长。<br/>据行业估算，2024 年由深度伪造技术引发的各类欺诈造成的全球经济损失已高达 120 亿美元。尤其在监管相对滞后、交易更为匿名的加密货币领域，AI 滥用更是如鱼得水。根据相关的报告也显示 2024 年仅 AI 深度伪造技术全年造成的损失便高达 46 亿美元。<br/>随着 AI 滥用事件的频发，过去模糊的 " 伦理指南 " 正在迅速转变为具有强制约束力的法律条文，而且这种转变直接导致了企业合规成本的急剧攀升。<br/>而且一旦出现违规需要付出的代价更是惨痛的，罚款最高可达全球年营业额的数个百分点或数千万欧元，而且合规也不再是法务部门的单一工作，而是渗透到研发、产品、市场的每一个环节。<br/>这些 " 反噬 " 也并非凭空产生，在 AI 商业化过程中对速度和规模的追求，长期以来压倒了对安全和伦理的考量所以形成了这种 " 原罪 "。因此未来合规成本的升高是不可避免的，而欧盟的《AI 法案》可以说是这一趋势的先行者。<br/>该法案于 2024 年 8 月 1 日正式生效并分阶段实施，着重对高风险的 AI 系统施加了严格的合规要求。而且这不仅仅是一项区域性法规，更可能产生 " 布鲁塞尔效应 " 从而影响全球的 AI 监管格局。<br/>监管的落地也将会直接转化为企业的合规成本。据公开信息推算，仅欧盟 AI 法案便可能导致欧洲企业的 AI 采纳成本增加约 310 亿欧元，并使 AI 投资减少近 20%。而美国联邦贸易委员会也已对 OpenAI 展开调查，谷歌等公司也不得不调整其营销话术，避免被处以巨额罚款。<br/>可以说 " 监管的铁幕 " 正在迫使整个行业从过去 " 快速行动，打破陈规 " 的互联网思维转向一种更为审慎、合规驱动的开发模式。可以说这种转变无疑会减缓创新速度并增加运营成本，对于那些资源有限的中小企业和初创公司构成尤为严峻的挑战。<br/>对 " 信任 " 的侵蚀或许是 AI 滥用最难修复的一种<br/>这源于在激烈的竞争压力下，企业急于抢占市场将产品快速推向用户，所以将风险控制和安全测试置于次要位置。但是这种 " 快速行动并打破规则 " 的心态在 AI 时代尤为危险，因为 AI 技术的潜在破坏力远超以往的软件应用。<br/>并且市场对于 AI 技术的可靠性极度敏感，甚至一次小小的失误都可能引发巨大的信任危机和财务损失。谷歌的 Bard 模型之前便在一次演示中出现事实性错误，竟然导致其母公司 Alphabet 的股价在单日内暴跌 7%，市值蒸发超过 1000 亿美元。<br/>并且随着 AI 投资的巨额支出持续攀升，投资者开始担忧其回报前景，这种悲观情绪导致 Meta、Microsoft、Alphabet 和 Nvidia 等 AI 领域的领军企业股价普遍承压下跌，市场也开始讨论 "AI 泡沫 " 的风险，并开始质疑哪些不计成本的 " 军备竞赛 " 式投资。<br/>更何况大量公司缺乏对 AI 伦理的明确责任归属，高管层面也并未对其有所调整。所以 AI 系统的决策过程像一个 " 黑箱 "，在责任主体模糊的情况下滥用和误用的风险便难以控制。企业内部也未建立有效的问责机制。<br/>但是更深层次的原因在于当前主流生成式 AI 商业模式本身所内含的风险。这些模型依赖于海量数据的投喂，其训练过程难以完全避免偏见和有害信息的吸收。而其强大的生成能力却为恶意利用提供了温床。<br/>因此当商业模式的核心是追求更强大的模型、更广泛的应用时，如果缺乏与之匹配的强大 " 安全刹车 " 系统，滥用就成了可预见的副产品。这种商业逻辑与伦理要求之间的结构性失衡才是导致 " 反噬 " 的根本内因。<br/>所以当企业享受了技术红利带来的增长，如今便也不得不为其模式所伴生的风险 " 买单 "。哪怕科技公司以 " 让世界更美好 " 的叙事推广 AI，公众在实际体验中，也会频繁受到隐私泄露、算法偏见、就业替代、虚假信息等负面影响。<br/>这种落差也导致了广泛的 "AI 焦虑 " 和不信任感。公众普遍认为现有的监管法规不足以应对 AI 带来的社会风险期望政府采取更加果断的行动。这种强大的民意压力也是推动监管机构加速行动的根本动力。<br/>面对公众的呼声和潜在的社会风险，监管机构的介入是必然的。但由于技术发展的速度远超立法速度监管往往表现出一定的滞后性，欧盟 AI 法案便被部分人士认为可能增加企业负担、抑制创新。<br/>全球主要经济体在 AI 领域的竞争，也使得监管变得更加复杂。各国都希望在鼓励创新和防范风险之间找到平衡点但这种平衡点的位置各不相同，因此形成了复杂的国际监管格局给跨国企业的合规带来了巨大挑战。<br/>而且这种外部滥用对整个 AI 行业的声誉造成了 " 连坐 " 效应。即使一家公司本身恪守伦理，也无法完全独善其身，因为公众对 AI 的信任是整体性的。恶意滥用行为如同向池塘中投下的毒药，在污染了整个水域后迫使所有 " 池中生物 " 共同承担后果。<br/>这场危机成为 AI 自我革新的契机<br/>这场 " 反噬 " 带来的阵痛，是 AI 产业从野蛮生长走向规范发展的必经阶段。它正在淘汰那些只想赚快钱、缺乏责任感的 " 玩家 "，筛选出真正有实力、有远见的长期主义者。从长远来看，这也是为 AI 产业的健康、可持续发展所必须付出的代价。<br/>其中最大的机遇在于将 " 信任 " 从一种道德呼吁，转变为一种可量化、可变现的商业资产和竞争壁垒。数据显示近 85% 的客户也更愿意与重视 AI 伦理实践的公司合作，而那些优先考虑伦理和透明度的公司收入增长也更快。<br/>可以说在 AI 产品同质化日益严重的未来，谁能赢得用户的信任谁就能赢得市场。" 负责任的 AI" 将不再仅仅是公关部门的口号，而是必须贯穿于产品设计、开发、部署全流程的核心战略。<br/>谷歌和微软等公司已经开始调整其策略，谷歌选择利用 AI 技术提升广告安全审核的效率，打击欺诈内容；微软则发布了负责任 AI 透明度报告，并推出了 AzureAIContentSafety 等服务，帮助客户构建更安全的 AI 应用。这些举措既是应对风险的防御，也是在构建新的竞争优势。<br/>正是 " 反噬 " 催生了全新的 " 安全即服务 " 市场。随着 AI 滥用风险的加剧企业对 AI 安全审计、风险评估、内容过滤、合规咨询等服务的需求将急剧增长。这为专门从事 AI 安全和伦理治理的科技公司、咨询机构创造了巨大的市场空间。<br/>而科技巨头自身也可以将其内部成熟的安全工具和能力平台化、服务化，开拓新的收入来源。例如，谷歌和微软在内容审核、风险识别方面的技术积累，完全可以转化为对外输出的商业服务。<br/>虽然监管的收紧虽然带来了成本，但也为行业设定了 " 准入标准 "，能够率先满足高标准合规要求的企业将获得更强的市场公信力和竞争优势，从而在未来的市场整合中占据有利地位。这实际上是一种由监管驱动的市场出清和格局优化。weibo.com/ttarticle/p/show?id=2309405234825153348099<br/>weibo.com/ttarticle/p/show?id=2309405234825304342582<br/>weibo.com/ttarticle/p/show?id=2309405234829599310183<br/>weibo.com/ttarticle/p/show?id=2309405234829741654183<br/>weibo.com/ttarticle/p/show?id=2309405234829880066874<br/>weibo.com/ttarticle/p/show?id=2309405234830022673245<br/>从滥用事件的激增，到资本市场的审慎，再到全球监管的收紧，这股 " 反噬 " 之力正在重塑 AI 产业的发展轨迹。它迫使整个行业从过去对技术力量的无限崇拜，转向对技术责任和社会价值的深刻反思。<br/>麦肯锡预测，到 2030 年 AI 将为全球经济创造 13 万亿美元价值。但价值分配取决于我们如何驾驭这头猛兽。未来的竞争，将不仅仅是模型参数和算力大小的竞争，更是治理能力、责任担当和用户信任的竞争。</p>]]></description></item><item>    <title><![CDATA[《从被动修复到免疫：游戏Bug闭环体系的]]></title>    <link>https://segmentfault.com/a/1190000047412846</link>    <guid>https://segmentfault.com/a/1190000047412846</guid>    <pubDate>2025-11-19 23:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>每一个Bug的出现，都绝非孤立的代码失误，可能是模块间数据流转的隐性断点、场景触发条件的边缘冲突，或是玩家非常规操作与设计预期的偏差，甚至可能是架构层面的适应性缺陷。这些异常表现如同系统的“隐性病灶”，轻则影响局部体验，重则引发连锁反应，导致核心玩法崩塌、玩家流失。多数开发团队对Bug的处理仍停留在“发现-修复-验证”的线性流程，将Bug视为需要消灭的“敌人”，却忽视了其背后承载的系统优化价值。真正成熟的Bug处理逻辑，并非以“零Bug”为终极目标—这在复杂游戏生态中几乎不具备可行性—而是构建一套让系统能够自我感知、自我调节、自我进化的“自愈体系”。这套体系的核心价值，在于通过对Bug全生命周期的深度拆解与闭环管理，将每一次异常处理转化为系统的“免疫记忆”，让同类问题的复发概率持续降低，同时推动架构、设计与测试环节的协同优化。在长期的开发实践中深刻体会到，那些能够在多版本迭代与海量玩家检验中保持体验稳定性的游戏，其背后必然存在一套超越表面流程的Bug自愈闭环。它不是一份僵化的操作手册，而是与游戏架构深度绑定、与开发节奏动态适配、与团队认知共同成长的思维模式，从Bug的源头预判到根因追溯，再到经验沉淀与体系迭代，每一个环节都在为系统韧性注入养分，最终实现体验品质的长期守恒。</p><p>构建Bug自愈体系的首要前提，是打破“被动等待Bug暴露”的传统模式，建立一套具备“预判性”与“协同性”的多维度感知网络。游戏开发中，Bug的发现渠道往往分散在内部测试、玩家反馈与线上监控三个维度，但多数团队未能让这三者形成有效协同，导致大量隐性Bug在上线后才集中爆发，增加了修复成本与体验损失。内部测试环节的核心痛点在于“场景覆盖的局限性”，传统测试用例多基于设计文档的预期流程，侧重验证功能的正常运行，却容易忽略不同模块交互时的边缘场景、极端数值组合、跨场景切换的时序冲突，或是玩家突破设计预期的非常规操作路径—比如在战斗中同时触发多个道具效果、在剧情触发节点强制退出游戏、在网络波动时进行关键操作等。解决这一问题的关键，并非无限制扩充测试用例数量，而是构建“模块交叉场景库”：以游戏核心玩法为轴心，梳理每个模块与其他系统的关键交互节点，比如战斗系统与道具系统的数值联动逻辑、剧情触发与场景切换的时序校验机制、网络同步与本地计算的一致性保障流程、角色状态与环境交互的边界条件等，将这些交叉点转化为可复现、可量化的测试场景，同时引入“反向测试思维”，主动模拟玩家可能的异常操作，提前暴露潜在风险。玩家反馈则常常呈现“碎片化”与“模糊化”特征，玩家往往只能描述异常现象（如“奖励未到账”“角色卡住”），却无法提供精准的触发条件、操作路径与设备信息。此时需要建立一套“反馈信息提炼机制”，通过对反馈内容中的关键词聚类、场景描述还原、设备型号与系统版本统计，从大量零散信息中识别出共性问题，区分“个体设备兼容问题”“网络环境导致的偶发异常”与“系统性Bug”。例如，当多名玩家反馈“某副本结算时奖励缺失”，通过提取他们的操作路径（是否中途退出、是否组队参与、是否触发特殊剧情分支）、设备类型（移动端/PC端）、网络状态（Wi-Fi/流量）等信息，可快速锁定结算逻辑中与“状态判定”“数据同步”相关的漏洞。而线上监控的核心，不应局限于报错日志的统计与告警，更要关注“异常行为序列”的捕捉与分析—比如玩家在某一功能模块的操作频率突然异常（远超正常玩家的点击次数）、某一场景的加载时长出现离散型峰值（多数玩家加载正常，少数玩家加载超时）、特定操作后玩家的退出率显著上升（如使用某道具后立即退出游戏）等，这些隐性信号往往是未被发现的Bug的前兆。通过将内部测试的“模块交叉场景库”、玩家反馈的“信息提炼机制”与线上监控的“异常行为捕捉”三者深度联动，让感知网络具备“主动识别”与“精准定位”能力，在Bug影响范围扩大前就完成初步判定，为后续的快速修复争取时间，同时减少无效排查带来的开发资源消耗。</p><p>Bug发现后的分级与优先级判定，是决定自愈体系效率的核心环节，其本质是对“影响权重”的精准权衡与动态调整。多数团队采用“严重程度+影响范围”的二元分级法，将Bug简单划分为致命、严重、一般、轻微四个等级，但这种方式容易陷入“高优先级Bug拥堵”“重要Bug被遗漏”或“资源分配失衡”的困境—比如将所有影响核心玩法的Bug都标记为高优先级，导致开发人员陷入多线作战，反而降低了整体修复效率；或是忽视了某些看似轻微但高频出现的体验类Bug，长期积累后影响玩家口碑。真正合理的分级逻辑，需要构建一个多维度的“影响权重模型”，除了直观的影响范围（覆盖玩家数量）与严重程度（是否阻碍核心流程），还需纳入“潜在扩散风险”“修复成本”“版本节奏适配性”“玩家感知敏感度”四个关键指标。潜在扩散风险指Bug是否可能随着玩家行为的传播、版本迭代中的模块联动，从局部场景蔓延到更多功能模块，比如某类数值计算错误若未及时修复，可能会在后续的道具更新、活动上线、跨服玩法开启后引发连锁反应，导致数值平衡崩坏；修复成本则需综合评估所需的开发工时、跨模块协作成本（是否需要多个团队配合）、代码改动范围（局部调整还是架构层面的修改），以及修复后可能引入新问题的概率，避免为了修复一个低影响Bug而占用核心功能开发、版本上线筹备等关键任务的资源；版本节奏适配性则要求分级与当前开发阶段的核心目标匹配，比如临近上线时，对影响核心玩法运行、付费流程、账号安全的Bug需优先处理，而在迭代中期，可适当将资源倾斜给那些虽不紧急但影响长期体验的隐性Bug（如极端场景下的轻微卡顿、界面显示瑕疵）；玩家感知敏感度则需结合游戏的目标用户群体特征，比如面向核心玩家的竞技类游戏，对操作响应延迟、数值平衡性相关的Bug敏感度极高，而面向休闲玩家的养成类游戏，可能更关注剧情连贯性、道具获取体验相关的问题。在实践中，我们将Bug划分为“阻断级”“核心体验级”“一般体验级”“隐性优化级”四类：阻断级指直接导致游戏无法运行、玩家进度丢失、核心玩法失效或账号安全风险的Bug，需启动紧急响应流程，暂停非核心开发任务，集中核心开发人员进行修复，必要时可采取临时屏蔽功能、回滚版本等应急措施；核心体验级指不影响游戏基本运行，但会严重破坏玩家沉浸感、影响核心玩法体验的Bug，如战斗系统的技能释放异常、剧情触发断裂、关键道具无法使用等，需在当前版本周期内优先处理，确保不影响版本核心目标的达成；一般体验级指对核心玩法无影响，但存在显示异常、音效缺失、操作逻辑不流畅等问题的Bug，可根据资源情况安排修复，若当前版本资源紧张，可纳入下一个迭代周期；隐性优化级则指在特定极端条件下才会触发、影响范围极小且不影响核心体验的Bug，如特定设备下的界面布局轻微偏移、极端数值组合下的非关键数据显示错误等，可纳入长期优化队列，结合后续版本的模块优化一并处理。分级的核心不是给出固定标签，而是建立“动态调整机制”—某一隐性优化级Bug若在后续迭代中因模块变动、玩法扩展而扩大影响范围，需及时提升优先级；而部分一般体验级Bug若玩家反馈集中、舆情关注度高，即使修复成本较高，也需重新评估资源分配，避免因忽视玩家感受导致留存下滑。通过这套多维度的分级模型与动态调整机制，让团队能够在复杂的开发节奏中，精准分配修复资源，既保证核心体验的稳定性，又避免资源浪费。</p><p>Bug修复环节的关键，在于避免“头痛医头、脚痛医脚”的表面修复，建立“全链路管控”机制，确保修复的有效性、安全性与彻底性。很多开发团队在修复Bug时，往往只关注报错的直接原因，比如看到“数据为空”的报错就直接添加空值判断，看到“界面显示异常”就调整布局参数，却忽略了Bug产生的上下文逻辑、数据流转链路与潜在关联影响，导致修复后不久同类问题再次出现，或引入新的兼容性漏洞、逻辑冲突。修复前的“根因定位”需要突破“代码层面”的局限，深入到“架构逻辑”“设计初衷”与“模块交互”的层面，还原Bug的完整生命周期。例如，某游戏曾出现“跨场景传送后角色技能CD异常重置”的Bug，初期开发人员仅针对传送逻辑中的CD数值赋值进行修正，但问题反复出现，甚至在后续版本中衍生出“技能效果无法正常触发”的新问题。直到团队重新梳理了技能系统、场景系统、网络同步三者的交互流程，才发现根源在于传送时的状态同步时序错误—角色技能CD的本地计算与服务器同步存在时间差，传送触发的状态重置指令覆盖了正确的CD数值，而初期的修复仅解决了表面的数值赋值问题，并未修复时序同步的核心矛盾。这一案例说明，有效的根因定位需要“层层拆解、溯本求源”：首先还原Bug的触发条件（如操作路径、场景环境、数值组合），然后梳理相关模块的交互链路（如数据从客户端到服务器的流转过程、不同系统的调用顺序），再分析每个环节的逻辑是否存在漏洞（如状态判定条件是否完善、数据同步是否一致、边界值处理是否全面），最终找到导致系统失效的核心断点。修复过程中，需坚守“最小改动原则”，即仅针对根因涉及的逻辑进行必要调整，避免为了简化修复而修改无关代码，或进行大面积的架构重构—除非根因明确指向架构层面的缺陷。同时，需建立“修复上下文档案”，详细记录修复思路、涉及的模块与代码范围、可能影响的功能点、测试验证的重点方向，便于后续的追溯与复盘。修复后的验证环节，不能依赖单一测试人员的确认，而要构建“多层验证体系”：开发自验需复现原始Bug场景及相关交叉场景（如与修复逻辑相关的模块交互场景、极端数值场景），确保修复有效且未影响其他功能；测试专项验证需结合“模块交叉场景库”，覆盖与修复逻辑相关的所有交互节点，同时进行兼容性测试（不同设备、系统版本）与压力测试（高并发场景）；线上灰度验证则针对影响范围较大的Bug（如核心玩法相关、覆盖大量玩家的问题），选择小部分服务器或特定玩家群体（如内测玩家、付费玩家）进行测试，观察修复后的系统稳定性、性能表现与玩家反馈，避免全量上线后引发新的问题。通过这套“根因定位-最小修复-多层验证”的全链路管控机制，确保每一次Bug修复都能彻底解决问题，同时规避修复带来的次生风险。</p><p>Bug修复完成并非自愈体系的终点，真正的价值沉淀来自于“根因追溯与经验复用”，让每一次Bug处理都成为系统优化的养分。多数团队在Bug验证通过后便结束流程，将其从任务列表中移除，却错失了通过单个Bug优化整个系统、提升团队能力的机会。每一个Bug的产生，都暴露了系统在设计、开发、测试或协作环节的薄弱点—可能是设计文档中的逻辑模糊地带、模块交互的边界定义不清晰，可能是编码规范的缺失、跨模块协作时的沟通偏差，也可能是测试用例的覆盖不足、自动化测试的场景遗漏。根因追溯的核心就是将这些薄弱点从“个案问题”转化为“通用规则”，避免同类问题重复出现。根因追溯需避开“归因于偶然”“归因于个人失误”的误区，从多个维度进行深度拆解：若Bug源于设计逻辑的漏洞，需反思设计文档是否存在歧义、模块交互的流程是否经过充分评审、是否考虑了极端场景与异常分支；若源于开发实现的偏差，需审视编码规范是否完善、代码审查是否到位、跨模块协作时的接口定义是否清晰、是否存在技术认知上的盲区；若源于测试覆盖的缺失，需分析测试用例的设计是否全面、测试方法是否合理、是否缺乏针对边缘场景的专项测试；若源于协作流程的问题，需思考沟通机制是否高效、信息同步是否及时、责任划分是否明确。在实践中，我们建立了“Bug复盘双周会”制度，选取典型Bug案例（如高频出现的同类问题、修复成本高的复杂问题、影响范围广的核心问题）进行集体拆解，而非局限于修复者个人的总结。复盘会并非简单的“问题汇报”，而是让设计、开发、测试、运营等相关人员共同参与，从各自的专业视角分析问题产生的原因：设计人员反思逻辑漏洞，开发人员分享编码过程中的困惑，测试人员总结覆盖不足的教训，运营人员反馈玩家的实际感受。例如，针对某类反复出现的“数值同步异常”Bug，复盘后发现，核心问题在于不同模块对同一数值的修改权限未做明确界定，导致多线程操作时出现数据冲突，同时测试用例中缺乏对多线程场景的覆盖。随后团队优化了数值管理架构，明确了核心数值的修改流程、同步机制与权限划分，同时补充了多线程场景的测试用例，引入自动化测试工具覆盖相关交互，后续同类Bug的出现频率下降了70%以上。复盘的关键不仅在于记录根因，更在于将复盘结论转化为可落地的优化措施：针对设计层面的问题，更新设计规范文档，增加模块交互评审环节，要求核心功能必须出具详细的异常分支处理方案；针对开发层面的问题，完善编码检查清单，强化代码审查中的重点关注项（如接口兼容性、数据校验、多线程安全），组织技术分享会弥补团队的认知盲区；针对测试层面的问题，扩充“模块交叉场景库”，引入自动化测试覆盖高频交叉场景与边缘场景，建立测试用例评审机制；针对协作层面的问题，优化信息同步工具，明确Bug处理的责任划分与沟通流程。通过这种方式，每一次Bug复盘都在为系统构建“防御工事”，为团队积累“避坑经验”，让自愈体系具备持续优化的能力，实现“处理一个Bug，解决一类问题”的目标。</p>]]></description></item><item>    <title><![CDATA[《游戏Bug快修手册：根因锁定与最小改动]]></title>    <link>https://segmentfault.com/a/1190000047412849</link>    <guid>https://segmentfault.com/a/1190000047412849</guid>    <pubDate>2025-11-19 23:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数研发团队面对突发Bug时，往往陷入“海量日志狂刷+无目标调试+仓促修改”的低效循环：有人埋头排查代码细节却忽视场景关联性，有人急于提交修复版本却未验证边缘情况，最终不仅浪费了黄金修复时间，还可能因盲目改动引入新的逻辑冲突，导致问题扩大化。快速解决Bug的核心，从来不是单纯追求“修复速度”，而是建立一套“精准定位→优先级动态判定→最小风险修复→分层高效验证→经验沉淀复用”的体系化能力。这种能力的本质，是对游戏系统逻辑的深度认知、排查思路的结构化拆解，以及风险控制的前置意识。Bug的解决效率，往往取决于能否在复杂的系统链路中快速锁定核心矛盾，而非被无关细节裹挟。长期的研发实践中深刻体会到，那些能在紧急场景下从容破局的团队，都具备一套独特的排查思维：他们不会急于动手修改，而是先通过场景拆解、逻辑溯源缩小问题范围，再以“最小改动”原则精准打击根因，最后用分层验证确保修复安全。这种思路彻底打破了“越快修复越容易出错”的固有误区，让快速修复与风险控制形成良性平衡，真正实现“高效且稳妥”的Bug解决闭环，既守住了用户体验的底线，也最大化降低了研发资源的浪费。</p><p>精准定位是快速解决Bug的前提与核心，其关键在于构建“场景拆解+逻辑溯源”的结构化排查路径，用科学方法替代盲目摸索，从源头缩短排查时间。很多研发同行容易陷入的典型误区是：拿到Bug反馈后立即投入日志排查，面对动辄数万条的日志数据无从下手，反复筛选却始终找不到关键信息，最终在无效操作中浪费大量时间。高效定位的第一步，是提炼“场景复现四要素”，通过用户反馈、后台数据、测试复现等多渠道收集完整信息：完整的操作路径（用户从进入场景到触发Bug的每一步操作，包括点击顺序、技能释放、道具使用等）、具体的环境条件（设备型号、系统版本、网络状态、服务器分区、游戏版本号等）、明确的数据状态（用户等级、角色属性、组队人数、副本进度、道具持有情况等）、清晰的触发频率（是100%触发、高频触发还是偶发触发，是否与特定时间、特定场景强关联）。例如，某动作游戏中“连续释放技能A后使用道具B，角色会出现1-2秒卡顿”的Bug，通过用户反馈与后台数据收集到四要素：操作路径（技能A×3+道具B点击）、环境条件（集中在安卓13及以上版本、Wi-Fi网络）、数据状态（角色等级30级以上、组队场景、副本为高难度模式）、触发频率（组队场景中触发率90%，单机场景零触发）。基于这些信息，首先通过反向推导排除无关因素：若为本地资源加载问题，单机场景应同样触发，因此排除；若为设备兼容性问题，不应仅局限于组队场景，因此缩小范围至“组队状态下的网络与数据同步”。接下来，沿着“组队状态激活→技能释放指令发送→服务器接收与处理→数据同步至组队成员客户端→客户端渲染反馈”的核心链路拆解，重点排查服务器与客户端的同步机制：服务器处理组队指令的并发能力、同步频率是否与客户端指令发送频率匹配、数据传输过程中是否存在丢包或延迟。最终发现，高难度副本中组队成员的技能指令发送频率较高，而服务器的同步频率设置过低，导致数据堆积在传输链路中，客户端接收数据不完整引发卡顿。定位阶段的另一关键是“排查优先级排序”：先排查高频触发场景（优先复现100%或高频触发的Bug，避免在偶发问题上浪费时间），再处理偶发场景；先验证核心逻辑链路（如数据流转、指令传输等核心流程），再排查边缘分支（如特殊数值、极端条件下的逻辑）；先排除简单易验证的原因（如配置错误、参数异常等可快速确认的问题），再深入复杂模块（如架构层面的时序冲突、多线程交互等）。这种结构化排查思路，能有效避免“大海捞针”式的无效操作，将定位时间缩短60%以上，为后续的修复工作争取宝贵的黄金时间。</p><p>Bug定位完成后，优先级的动态判定直接决定修复效率与资源分配的合理性，其核心是建立“三维评估模型”，在多任务并行的压力下精准锁定核心矛盾，避免资源浪费或核心问题延误。很多研发团队在优先级判定上存在明显误区：要么采用“一刀切”的简单分类（如仅按严重程度分为致命、严重、一般），导致所有“严重”Bug堆积，研发人员陷入多线作战，反而降低整体修复效率；要么过度依赖主观判断，忽视用户感知与修复成本的平衡，导致高敏感度Bug被遗漏，引发用户不满。三维评估模型的核心指标包括“影响范围”“用户敏感度”“修复成本”，三者需综合权衡而非孤立判定，每个指标都需结合游戏类型、研发阶段、运营场景进行细化拆解。影响范围不仅指覆盖的用户数量，还包括影响的功能重要性：如全服用户均可触发的Bug，或核心付费功能、核心玩法相关的Bug，影响范围权重更高；而仅影响少数特定设备、特定场景（如冷门单机副本）的Bug，影响范围权重较低。用户敏感度与游戏的核心定位强相关：竞技类游戏对操作响应延迟、数值平衡、对战公平性的敏感度极高，哪怕是轻微的数值计算错误，也可能引发核心用户的强烈不满；休闲养成类游戏则更关注剧情连贯性、道具获取体验、界面交互流畅度，对单机场景的轻微卡顿或显示异常敏感度较低。修复成本需从多维度综合评估：研发工时（解决问题所需的时间）、跨模块协作需求（是否需要多个团队或模块负责人配合）、代码改动范围（是局部参数调整、单一逻辑修改，还是涉及架构层面的重构）、风险系数（修复后是否可能引入新的逻辑冲突、兼容性问题）。例如，某跨服对战游戏中“结算时玩家积分计算错误”的Bug，影响范围是全服参与跨服玩法的用户（核心用户群体），用户敏感度（竞技公平性）拉满，而修复仅需调整服务器端的积分计算逻辑，改动范围小、工时短、风险低，应直接定为最高优先级，启动紧急修复流程，甚至暂停非核心功能的研发进度集中资源解决；而某单机解谜游戏中“某冷门关卡的背景音效缺失”，仅影响少数通关该关卡的用户，用户敏感度低，修复需调整多个音频预制件，还可能影响其他关卡的音效播放，修复成本高、风险高，可纳入后续迭代周期，待资源充裕时处理。优先级判定并非一成不变的静态标签，需建立动态调整机制：若某低优先级Bug的用户反馈量在短时间内激增（如1小时内反馈量突破千条），或被行业媒体、核心KOL提及引发舆情风险，需实时上调优先级，启动紧急处理；若高优先级Bug在修复过程中发现根因涉及架构层面（如数据存储方式不合理），修复成本远超预期（需耗时3天以上），则需评估是否采取临时规避方案（如屏蔽相关功能入口、限制触发条件），先缓解用户影响，再在后续版本中彻底重构，而非硬磕导致更大损失。精准的优先级判定，能让团队在复杂的研发节奏中始终聚焦核心矛盾，确保有限的研发资源投入到“影响大、用户敏感、成本低”的关键问题上，最大化提升修复效率与用户满意度。</p><p>高效修复的核心是“根因聚焦+最小改动”，既要保证问题彻底解决，又要将修复风险降至最低，避免因仓促改动或过度优化引发新的问题。很多研发人员在紧急场景下容易陷入两个极端：要么追求“一劳永逸”，试图通过全面重构解决问题，结果导致修复时间翻倍，还可能破坏原有稳定逻辑；要么仅针对表面现象进行“补丁式修复”，忽视根因，导致Bug反复出现。最小改动的本质是“精准打击”—仅针对Bug的根因涉及的逻辑进行必要调整，不触碰无关模块，不优化非相关问题，不引入新的功能或逻辑。例如，某RPG游戏中“组队副本结算时，部分玩家未收到奖励”的Bug，通过定位发现根因是“服务器端未正确读取组队成员的通关时长数据，导致奖励计算条件不满足”，此时只需修正服务器端读取通关时长的逻辑（如补充缺失的字段读取、修复条件判断错误），确保数据获取准确，即可彻底解决问题；若此时同时优化奖励发放的动画效果、调整其他道具的数值平衡、修复副本内的无关文本错误，不仅会大幅增加修复时间，还可能因改动过多引发新的兼容性问题（如动画效果与部分设备不兼容）或逻辑冲突（如数值调整导致其他奖励计算错误）。修复前的风险前置控制同样关键，是避免修复失败的重要保障：首先需备份核心代码分支，建立修复专用分支，确保修复过程中不影响主分支的稳定，若修复出现意外可快速回滚至稳定版本；其次需预留临时规避开关，在代码中加入功能屏蔽或触发限制开关，若修复后验证出现问题，可通过后台配置快速屏蔽相关功能，避免影响线上用户体验；最后需明确改动范围，与相关模块负责人同步修复方案，确认改动不会影响其他模块的逻辑（如是否涉及公共接口、共享数据结构），必要时进行交叉评审。修复过程中，需坚守“根因不明确不盲目动手”的原则—很多紧急场景下，研发人员为了赶时间，在根因尚未完全确认时就仓促修改，结果导致Bug反复出现或衍生新问题。例如，某手游中“网络波动时使用道具，道具消耗但效果未生效”的Bug，初期误判为本地数据存储问题，修改了客户端的道具消耗记录逻辑，结果问题仍持续出现，还导致部分用户的道具数据异常；直到重新定位发现，根因是服务器同步确认机制缺失—用户提交道具使用请求后，客户端未收到服务器的成功确认指令就默认消耗道具，而服务器因网络波动未处理该请求，导致数据不一致。这种情况下，盲目修复不仅浪费了宝贵的时间，还引发了新的用户投诉。因此，即使在最紧急的场景下，也需预留10-15分钟彻底确认根因：通过复现场景、梳理逻辑链路、验证假设，确保每一个修改都有明确的针对性，避免“试错式修复”。</p><p>修复完成后的高效验证，是避免Bug二次爆发、保障线上稳定的最后一道防线，核心在于构建“分层验证+场景覆盖”的闭环体系，在有限时间内兼顾验证速度与全面性，杜绝“修复一个Bug，引入另一个Bug”的情况。很多研发团队在紧急修复后，仅进行简单的功能验证（如复现原始场景确认Bug消失）就匆忙上线，结果因验证不充分导致Bug复现、边缘场景异常或衍生新问题，反而增加了后续处理的成本。分层验证体系分为三个核心环节：开发自验、测试专项验证、灰度验证，每个环节都有明确的验证重点与操作标准。开发自验需覆盖三个核心维度：原始Bug的触发场景（多次复现确保问题彻底解决，避免偶发修复）、相关联的边缘场景（如组队/单机、不同等级、不同道具组合、网络波动环境等，验证修复不影响其他功能）、反向测试（模拟异常输入、极端数值、错误操作，验证系统的容错能力）。例如，修复副本结算异常后，不仅要验证正常通关的结算结果，还要测试中途退出、组队解散、网络中断后重连、极端通关时长（如超短时间通关、超时通关）等边缘场景，确保结算逻辑在各种情况下都稳定；同时模拟服务器负载过高、数据传输延迟等异常情况，验证系统是否能正确处理错误数据，避免崩溃。测试专项验证需聚焦高风险点，结合游戏类型与Bug特征制定针对性验证方案：兼容性验证需覆盖主流设备（移动端需包含高中低端机型、不同品牌）、系统版本（安卓各版本、iOS各版本）、网络环境（Wi-Fi、4G、5G、弱网）；高并发验证需通过压力测试工具模拟峰值在线人数，测试服务器的承载能力与数据同步稳定性；数据一致性验证需重点核对客户端与服务器的核心数据（如角色属性、道具数量、积分排名），确保同步无误。考虑到紧急场景下的时间压力，测试专项验证可采用“核心场景优先”策略：优先覆盖用户高频使用的场景（如核心玩法、付费流程），再逐步扩展到边缘场景，避免因追求全面性导致上线延误。灰度验证是线上安全的关键保障，其核心是“小范围测试、快速反馈、动态调整”：选择10%-20%的核心用户群体（如付费用户、高活跃用户）或特定服务器（如新区、测试服）进行小范围上线，实时监控关键指标—Bug报错率、用户反馈量、功能使用率、服务器负载、数据同步成功率等；若15-30分钟内无异常反馈，且核心指标稳定，再逐步扩大灰度范围，最终全量上线；若出现异常，立即启动回滚机制，将影响范围控制在最小。验证过程中，需建立快速反馈通道：通过用户社群、客服系统、后台反馈入口收集用户的实时反馈，安排专人监控反馈信息，对用户提及的新异常快速响应，确保未覆盖的场景问题能及时发现。高效验证的核心不是“全面测试”，而是“精准覆盖风险点”—基于Bug的根因、改动范围、影响场景，聚焦最可能出现问题的环节，用最少的时间实现最大程度的风险排查，确保修复版本的稳定性。</p><p>快速解决Bug的长期效率提升，离不开系统性的经验沉淀与思路复用，核心是将单次修复的实践智慧转化为可复制、可传承的通用规则，推动团队整体处理能力的持续迭代，而非停留在“单次高效”的层面。很多研发团队在Bug修复后便结束流程，没有进行总结沉淀，导致同类问题反复出现，团队陷入“排查-修复-再排查”的恶性循环，长期来看反而浪费了大量研发资源。</p>]]></description></item><item>    <title><![CDATA[从 Flink 到 Doris 的实时数]]></title>    <link>https://segmentfault.com/a/1190000047412851</link>    <guid>https://segmentfault.com/a/1190000047412851</guid>    <pubDate>2025-11-19 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Flink-Doris-Connector 作为 Apache Flink 与 Doris 之间的桥梁，打通了实时数据同步、维表关联与高效写入的关键链路。本文将深入解析 Flink-Doris-Connector 三大典型场景中的设计与实现，并结合 Flink CDC 详细介绍了整库同步的解决方案，助力构建更加高效、稳定的实时数据处理体系。</p><h2>一、Apache Doris 简介</h2><p>Apache Doris 是一款基于 MPP 架构的高性能、实时的分析型数据库，整体架构精简，只有 FE 、BE 两个系统模块。其中 FE 主要负责接入请求、查询解析、元数据管理和任务调度，BE 主要负责查询执行和数据存储。Apache Doris 支持标准 SQL 并且完全兼容 MySQL 协议，可以通过各类支持 MySQL 协议的客户端工具和 BI 软件访问存储在 Apache  Doris 中的数据库。</p><p>在典型的数据集成和处理链路中，往往会对 TP 数据库、用户行为日志、时序性数据以及本地文件等数据源进行采集，经由数据集成工具或者 ETL 工具处理后写入至实时数仓 Apache Doris 中，并由 Doris 对下游数据应用提供查询和分析，例如典型的 BI 报表分析、OLAP 多维分析、Ad-hoc 即席查询以及日志检索分析等多种数据应用场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412853" alt="Apache Doris 简介 .PNG" title="Apache Doris 简介 .PNG"/></p><p>Flink-Doris-Connector 是 Apache Doris 与 Apache Flink 在实时数据处理 ETL 的结合，依托 Flink 提供的实时计算能力，构建高效的数据处理和分析链路。Flink-Doris-Connector 的使用场景主要分为三种：</p><ol><li><strong>Scan</strong>：通常用来做数据同步或是跟其他数据源的联合分析；</li><li><strong>Lookup Join</strong>：将实时流中的数据和 Doris 中的维度表进行 Join；</li><li><strong>Real-time ETL</strong>：使用 Flink 清洗数据再实时写入 Doris 中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412854" alt="Apache Doris 简介 -2.PNG" title="Apache Doris 简介 -2.PNG" loading="lazy"/></p><h2>二、Flink-Doris-Connector 典型场景的设计与实现</h2><p>本章节结合 Scan、Lookup Join、Write 这三种场景，介绍 Flink-Doris-Connector 的设计与实现。</p><h3>01 Scan 场景</h3><p>Scan 场景指将 Doris 中的存量数据快速提取出来，当从 Doris 中读取大量数据时，使用传统的 JDBC 方法可能会面临性能瓶颈。因此 Flink-Doris-Connector 中可以借助 Doris Source ，充分利用 Doris 的分布式架构和 Flink 的并行处理能力，从而实现了更高效的数据同步。</p><h4>Doris Source 读取流程</h4><ul><li>Job Manager 向 FE 端发起请求查询计划，FE 会返回要查询的数据对应的 BE 以及 Tablet；</li><li>根据不同的 BE，将请求分发给不同的 TaskManager；</li><li>通过 Task Manager 直接读取每个 BE 上对应 Tablet 的数据。</li></ul><p>通过这种方式，我们可以利用 Flink 分布式处理的能力从而提高整个数据同步的效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412855" alt="Doris Source 读取流程.PNG" title="Doris Source 读取流程.PNG" loading="lazy"/></p><h3>02 Lookup Join 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412856" alt="Lookup Join 场景.PNG" title="Lookup Join 场景.PNG" loading="lazy"/></p><p>对于维度表存储在 Doris 中的场景，可通过 Lookup Join 实现对实时流数据与 Doris 维度表的关联查询。</p><h4>JDBC Connector</h4><p>Doris 支持 MySQL 协议，所以可以直接使用 JDBC Connector 进行 Lookup Join，但是这一方式存在一定的局限：</p><ul><li>Jdbc Connector 中的 Lookup Join 是同步查询的操作，会导致实时流中每条数据都要等待 Doris 查询的结果，增加了延迟。</li><li>仅支持单条数据查询，在上游数据量吞吐较高时，容易造成性能瓶颈和反压。</li></ul><h4>Flink-Doris-Connector 的优化</h4><p>因此针对 Lookup Join 场景 ，Flink-Doris-Connector 实现了异步 Lookup Join 和攒批查询的优化：</p><ul><li><strong>支持异步 Lookup Join：</strong> 异步 Lookup Join 意味着实时流中的数据不需要显式等待每条记录的查询结果，可以大大的降低延迟性。</li><li><strong>支持攒批查询：</strong> 将实时流的数据追加到队列 Queue 中，后台通过监听线程 Watcher，将队列里面的数据取出来再推送到查询执行的 Worker 线程池中，Worker 线程会将收到的这一批数据拼接成一个 Union All 的查询，同时向 Doris 发起 Query 查询。</li></ul><p>通过异步 Lookup join 以及攒批查询，可以在上游数据量比较大的时候大幅度提高维表关联吞吐量，保障了数据读取与处理的高效性。</p><h3>03 实时 ETL 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412857" alt="实时 ETL 场景.png" title="实时 ETL 场景.png" loading="lazy"/></p><p>对于实时写入来说，Doris Sink 的写入是基于 Stream Load 的导入方式去实现的。Stream Load 是 Apache Doris 中最为常见的数据导入方式之一，支持通过 HTTP 协议将本地文件或数据流导入到 Doris 中。主要流程如下：</p><ul><li>Sink 端在接收到数据后会开启一个 Stream Load 的长链接请求。在 Checkpoint 期间，它会将接收到的数据以 Chunk 的形式持续发送到 Doris 中。</li><li>Checkpoint 时，会对刚才发起的 Stream Load 的请求进行提交，提交完成后，数据才会可见。</li></ul><h4>如何保证数据写入的 Exactly-Once 语义</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412858" alt="如何保证数据写入的 Exactly-Once 语义 .png" title="如何保证数据写入的 Exactly-Once 语义 .png" loading="lazy"/></p><p>那么，如何保证数据写入期间，端到端数据的精确一次性？</p><p>以 Kafka 同步到 Drois 的 Checkpoint 过程为例：</p><ol><li>Checkpoint 时，Source 端会接收到 Checkpoint Barrier；</li><li>Source 端接收到 Barrier 后，首先会对自身做一个快照，同时会将 Checkpoint Barrier 下发到 Sink 端；</li><li>Sink 端接收到 Barrier 后，执行 Pre-commit 提交，成功后数据就会完整写入到 Doris，由于此处执行的是预提交，所以在 Doris 上，此时对用户来说数据是不可见的；</li><li>将 Pre-Commit 成功的事务 ID 保存到状态中；</li><li>所有的算子 Checkpoint 都做完后，Job Manager 会下发本次 Checkpoint 完成的通知；</li><li>Sink 端会对刚才 Pre-commit 成功的事务进行一次提交。</li></ol><p>通过这种两阶段提交，就可以实现端到端的精确一次性。</p><h4>实时性与 Exactly-Once</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412859" alt="实时性与 Exactly-Once.png" title="实时性与 Exactly-Once.png" loading="lazy"/></p><p>上面提到，Doris Sink 端的写入与 Checkpoint 绑定，数据写入 Doris 的延迟性取决于 Checkpoint 的间隔。但在一些用户的场景下，希望数据可以实时写入，但是 Checkpoint 不能做的太频繁，同时对于一些作业来说，如果 Checkpoint 太频繁会消耗大量资源，针对该情况，Flink-Doris-Connector 引入了攒批机制，以平衡实时性与资源消耗之间的矛盾。</p><p>攒批的实现原理是 Sink 端接收上游数据之后，不会立即将每条数据单独写入 Doris，而是先在内存中进行缓存，然后通过对应参数设置，将缓存数据提交到 Doris 中。结合攒批写入和 Doris 中的主键模型，可以确保数据写入的幂等性。</p><p>通过引入攒批机制，既满足了用户对数据实时写入的需求，又避免了频繁 Checkpoint 带来的资源消耗问题，从而实现性能与效率的优化。</p><h2>三、基于 Flink CDC 的整库同步方案</h2><p>以上是对 Flink-Doris-Connector 的典型场景和实现原理介绍，接下来我们来看它在实际业务中的一个重要应用——整库同步。相比底层实现，整库同步更偏向具体使用场景。下面我们基于前面介绍的能力，进一步探讨如何通过 Flink CDC 实现 TP 数据库到 Doris 的高效、自动化同步。</p><h3>01 整库同步痛点</h3><p>在数据迁移过程中，用户通常希望可以尽快将数据迁移到 Doris 中，然而在同步 TP 数据库时，整库同步往往面临以下几点挑战：</p><ul><li><p><strong>建表：</strong></p><ul><li><strong>存量表的快速批量创建</strong>：TP 数据库中往往存在成千上万的表，这些表的结构各异，对于存量表而言需要逐一在 Doris 中创建对应的表结构；</li><li><strong>同步任务开启后，新增表的自动创建与同步：</strong> 为了保证数据的完整性和实时性，同步工具需要实时监控 TP 数据库的变化，并自动在 Doris 中创建和同步新表。</li></ul></li><li><strong>元数据映射：</strong> 上下游之间字段元数据的便捷映射，包括字段类型的转换、字段名称的对应修改等。</li><li><strong>DDL 自动同步：</strong> 增加、删除列等操作会导致数据库结构发生变化，进而影响到数据同步。因此，同步工具需要能够实时捕获 DDL 并动态地更新 Doris 表结构，以确保数据的准确性和一致性。</li><li><strong>开箱即用：</strong> 零代码，低门槛，理想的同步工具只需进行简单配置，即可实现数据的迁移和同步。</li></ul><h3>02 基于 Flink CDC 实现整库同步</h3><p>在数据抽取方面，Flink-Doris-Connector 借用了 Flink CDC 的特性能力：</p><ul><li><p>增量快照读取</p><ul><li>无锁读取与并发读取：不论存量数据量多大，都可以通过横向提高 Flink 的并发提升数据读取速度。</li><li>断点续传：当存量数据比较大时，可能面临同步中断的情况，CDC 支持中断任务的衔接同步。</li></ul></li><li>丰富数据源支持，Flink CDC 支持多种数据库，如 MySQL、Oracle、SQLServer 等。</li><li>无缝对接 Flink 现有生态，方便与 Flink 已有Source 和 Sink 结合使用。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412860" alt="基于 Flink CDC 实现整库同步.png" title="基于 Flink CDC 实现整库同步.png" loading="lazy"/></p><h4>一键建表与元数据自动映射</h4><p>Flink-Doris-Connector 中集成了 Flink CDC 等能力，可以让用户只提交一个操作，就能进行整库同步的操作。其主要原理是 Flink CDC Source 在接收到上游的数据源之后，会进行分流处理，不同的表用不同的 Sink。同时在最新的 Connector 版本中，也支持单个 Sink 同步多张表，支持新增表的创建和同步。</p><p>集成 Flink CDC 的功能后，<strong>用户仅需通过 Flink-Doris-Connector 提交任务，就可以在 Doris 自动创建所需的表，而无需配置上下游表之间的显式关联，实现数据快速同步</strong>。</p><p>当 Flink 任务启动后，Doris-Flink-Connector 将自动识别对应的 Doris 表是否存在。如果表不存在，Doris Flink Connector 会自动创建表，并根据 Table 名称进行分流，从而实现下游多个表的 Sink 接入；如果表存在，则直接启动同步任务。</p><p>这一改进，不仅简化了配置流程，还使得新增表的创建和同步更加便捷，从而提升数据处理的整体效率。</p><h4>Light Schema Change 与 DDL 自动同步</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412861" alt="Light Schema Change 与 DDL 自动同步.png" title="Light Schema Change 与 DDL 自动同步.png" loading="lazy"/></p><p>在 Apache Doris 1.2 版本之前，Schema Change 操作比较繁琐，需要手动增改数据列。在上游 TP 数据库发生表结构变更时，需要暂停数据同步任务、待 Doris 中的 Schema Change 完成后再重启任务。</p><p>自 Apache Doris 1.2 版本起，我们引入了轻量级的 Light Schema Change 机制，极大地简化了操作流程，常见的增减列场景其处理速度可达毫秒级。Light Schema Change 机制原理如下：</p><ul><li><p><strong>Schema Change：</strong></p><ul><li>客户端向 FE 发起增减列的请求；</li><li>FE 在接收到请求后，修改当前元数据，并将最新的 Schema 持久化；</li><li>FE 向客户端同步 Schema Change 的结果；</li></ul></li><li><p><strong>Data Load：</strong></p><ul><li>当后续导入任务发起时，FE 将导入任务与最新的 Schema 信息发送给 BE；</li><li>在数据写入过程中，BE 的每个 Rowset 都会存储当前导入的 Schema 信息；</li></ul></li><li><p><strong>Query：</strong></p><ul><li>FE 将查询计划与最新的 Schema 一起发送给 BE；</li><li>BE 使用最新 Schema 执行查询计划；</li></ul></li><li><p><strong>Compaction：</strong></p><ul><li>在 BE 中，对参与合并的 Rowset 版本进行比较；</li><li>根据最新的  Schema Change 信息进行数据合并。</li></ul></li></ul><p>经测试，与早期的 Schema Change 相比，Light Schema Change 的数据同步性能有了数百倍的提升，</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412862" alt="Light Schema Change 与 DDL 自动同步-2.png" title="Light Schema Change 与 DDL 自动同步-2.png" loading="lazy"/></p><p>Light Schema Change 与 Flink-Doris-Connector 的结合，通过 Flink CDC 可以实现 DDL 的自动同步，具体步骤如下：</p><ol><li>Source 端捕获上游 Schema Change 信息，开启 DDL 变更同步；</li><li>Doris Sink 端识别并解析 DDL 操作（加减列）；</li><li>Table 校验，判断是否可以进行 Light Schema Change；</li><li>发起 Schema Change 操作；</li></ol><p>基于这一实现，Doris 能自动获取到 DDL 语句并在毫秒级即可完成 Schema Change 操作，在上游 TP 数据库发生表结构变更时，数据同步任务无需暂停。</p><h4>开箱即用：MySQL 整库同步示例</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412863" alt="开箱即用：MySQL 整库同步示例.png" title="开箱即用：MySQL 整库同步示例.png" loading="lazy"/></p><p>对于用户来讲，只要有 Flink 客户端，通过上图的操作就可以提交整库同步作业。支持传入 Flink 的配置，比如并发设置、Checkpoint 间隔等，也支持正则表达式去配置需要同步的表， 同时可以将 Flink CDC Source 和 Doris Sink 的配置直接透传给具体的 Connector。通过这种方式，用户可以很便捷地提交整库同步作业。</p><h3>03 Flink-Doris-Connector 核心优势</h3><p>基于以上优化，可以完美解决用户的痛点：</p><ol><li>自动建表，即存量表与增量表的自动创建，无需用户提前在 Doris 中预先创建对应的表结构；</li><li>自动映射上下游字段，无需手动写入上下游字段间的匹配规则，节省大量人力成本；</li><li>增减列无感同步，及时获取上游 DDL 语句并自动在 Doris 中实现毫秒级 Schema Change，无需停服、数据同步任务平稳运行；</li><li>开箱即用，降低学习成本，更专注业务本身。</li></ol><h3>04 最佳实践</h3><p>在生产环境中，若作业数量较多，直接采用上述提交方式的作业管理复杂度较高。通常建议借助任务托管平台（如 StreamPark），实现对作业的统一创建、监控与运维，从而提升任务管理效率与系统稳定性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412864" alt="最佳实践.png" title="最佳实践.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412865" alt="最佳实践-2.png" title="最佳实践-2.png" loading="lazy"/></p><h2>四、未来规划</h2><p>未来，基于 Flink-Doris-Connector 的能力规划如下：</p><ol><li>支持实时读取。目前 Doris Source 只是把数据 Scan 出来，是一个有界流的读取，后续会支持 CDC 的场景，可以使用 Flink 来对 Doris 中的数据进行流式的读取。</li><li>Sink 一流多表。目前Flink-Doris-Connector支持单个 Sink 同步多张表，但是 Stream Load 的导入方式还是只支持单个表的导入。所以在表特别多的时候，需要在 Sink 端维护大量 StreamLoad 的连接，在后续会做到单个 Stream Load 的连接支持多张表的写入。</li><li>整库同步方面，支持更多的上游数据源，满足更多数据同步场景。</li></ol>]]></description></item><item>    <title><![CDATA[编程项目怎么学习 cpp辅导的阿甘 ]]></title>    <link>https://segmentfault.com/a/1190000047412883</link>    <guid>https://segmentfault.com/a/1190000047412883</guid>    <pubDate>2025-11-19 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>星球很多同学，在做星球项目，或者做自己项目的时候，都会遇到各种坎坷，说看不懂，不理解。</p><p>那项目，一个从未接触过的项目应该怎么学习呢。</p><h2>观点分享</h2><p>说方法之前，我们可以先对要学习的项目进行一个分类，分一下学习的两个境界。我认为可以整体分两类：</p><p>（1）一类是，自己学习，用于提升自己，用于跳槽，找工作给简历加分的 （个人项目）</p><p>（2）一类是，工作公司的项目，自己实际工作中的</p><h3>个人项目</h3><p>对于个人项目，拿来面试。面试主要考察什么呢，你这个人设计能力的完善性，即你项目的某个功能，对于极端场景是否有考虑到。</p><p>那这对于一个项目，熟悉到什么程度算可以了呢。主要就是项目的架构，项目功能的实现思路。对代码细节，写法没必要细究。</p><p>原因：</p><p>（1）相同的功能实现，不同的人可能就会有不同的写法，以及相同的人不同时期也会有不同的写法；</p><p>（2）面试重点是思维逻辑的交流，让人家可以听懂，可以认可，能够产生共鸣；毕竟人家也没看过你的代码，语法、写法人家也不知道，你说的这么细，反而让人家听不懂，效果还很差； </p><p>（3）这也是一直强调的，在学项目的时候也要注重文档的梳理编写。能够让一个搞python的，搞java的可以看懂，快速写出来。别说一堆自己项目自己命名，这确实详细，但是谁也看不懂，听不懂，那效果很差</p><h3>公司的项目</h3><p>公司的项目，我们进公司，主要是要解决项目bug，优化项目代码的，开发新功能的。解决项目的代码bug，肯定要能够精确定位，要对代码细节，调用过程了解，需要熟悉项目代码。</p><h2>建议</h2><p>知道了对于不同场景下，项目的学习程度。那么再聊聊项目应该怎么学习。</p><p>相信很多同学，都再网上听过很多前辈分享的各种源码阅读方法。比如main函数开始追、分功能模块看、按住一个功能调用过程追等等。</p><p>在这里，主要想给大家强调的方法是什么呢？</p><p><strong>借助AI，优先借助AI。</strong></p><p>现在AI能力，确实足够强大了，比如gpt5、claude 4.5等等。并且像个人项目一般最多也就几万行，或者就算公司项目上亿行代码，但是到你部门负责的可能也就几万行，数十万行，代码量都不大。可以先让AI对你的项目代码分析分析，架构、功能，实现逻辑等等。先通过它帮助你了解百分之七八十，再自己慢慢解决剩下的百分之二十，效率会高很多，很给力。</p><p>可能有的同学，在知名公司工作，说公司内部模型，没有这最先进的，其实用你们公司目前内供的，我认为目前也是可以帮助你进行分析的。</p><p>（为什么会给大家强调这个呢，主要还是通过大家问我的一些技术问题项目问题发现，这些问题直接喂给AI基本就可以快速出方案进行解决，远远没必要在那里抓脑瞎。给大家写这个，就是让大家有用AI的意识，优先考虑，现在模型能力是够的了）</p><h2>知识星球介绍（公认的cpp c++学习地）</h2><p>星球名字：奔跑中的cpp / c++</p><p>里面服务也不会变，四个坚守目前:</p><p>1.每天都会看大家打卡内容，给出合理性建议。</p><p>2.大家如果需要简历指导，心里迷茫需要疏导都可以进行预约周六一对一辅导。</p><p>3.每周五晚上九点答疑聊天不会变。</p><p>4.进去星球了，后续如果有什么其他活动，服务，不收费不收费(可以合理赚钱就收取下星球费用，但是不割韭菜，保持初心)</p><p>（还有经历时间考验的独家私密资料）</p><p>加入星球的同学都可以提问预约，一对一帮做简历，一对一  职业规划辅导    ，解惑。同时有高质量的项目以及学习资料</p><p>学cpp基础，可以把最近开发的这个编程练习平台利用起来<br/>cppagancoding.top</p><p>本文由<a href="https://link.segmentfault.com/?enc=5uTJ45M5FtItUCs18qiyVQ%3D%3D.KYsCbBaqEPDeQykZBSbhxyjUFONb6lGj8%2F33PniNwS0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[LEANN：一个极简的本地向量数据库 本]]></title>    <link>https://segmentfault.com/a/1190000047412778</link>    <guid>https://segmentfault.com/a/1190000047412778</guid>    <pubDate>2025-11-19 22:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在软件开发领域，提到轻量级、嵌入式的本地数据库，我们首先会想到 <strong>SQLite</strong>，它快速且无需独立服务进程。现在检索增强生成（RAG）和向量数据库的世界里，一个定位相似的新工具出现了。你可以把LEANN看作是<strong>嵌入式、轻量级的向量数据库</strong>。它完全不需要依赖庞大的数据中心或者 GPU 集群。一个<strong>个人专属的 RAG 引擎</strong>，它能完全放在你的笔记本电脑里，可以索引和搜索数百万份文档，而且最不可思议的是，它比常规向量数据库<strong>少用了 97% 的存储空间</strong>，而且**准确性还没有任何损失。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412780" alt="" title=""/></p><h2>LEANN 的核心技术差异</h2><p>传统的向量数据库简直就是存储“巨兽”。它们为每一个文档都预先计算好嵌入（Embeddings）并全部存储下来，磁盘空间很快就会被塞满。LEANN则不一样：</p><p>它压根儿不存储所有嵌入，而是采用一种<strong>基于图的选择性重计算</strong>方法，并辅以<strong>高保留度的修剪（Pruning）</strong>。这几个术语听起来有点花哨，但核心思想很简单：<strong>只在真正需要时，才去计算必需的数据</strong>。</p><p>LEANN 不是一个嵌入数据的“囤积者”，它会按需重新计算嵌入，并通过一个非常<strong>轻量级的图结构</strong>将它们智能地连接起来。这让存储用量大幅下降，而图结构则确保了语义相似性和检索准确性得以完整保留。</p><h2>为什么这项技术值得关注</h2><p>有了 LEANN，你的笔记本电脑瞬间就能变身成一个个人 AI 搜索引擎。所有这些功能都<strong>在本地跑起来</strong>，<strong>没有一分钱的云服务开销</strong>，<strong>隐私也得到了绝对保障</strong>。</p><p>如果你使用 Claude Code，应该知道它目前的搜索能力只停留在基本的关键词匹配。LEANN 可以无缝接入，作为 <strong>MCP（Model Context Protocol）服务</strong>，为你的模型增加<strong>真正的语义检索能力</strong>，让你搜索的是“意义”，而不是单纯的“词语”。而且整个过程中你都不需要改变原有的工作流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412781" alt="" title="" loading="lazy"/></p><h2>机制解析</h2><p>LEANN 之所以强大，关键在于它<strong>修剪掉了大量的“赘肉”</strong>：</p><ul><li><strong>基于图的重计算</strong>从根本上消除了对大型嵌入仓库的需求。</li><li><strong>使用 CSR（Compressed Sparse Row，压缩稀疏行）格式修剪的图</strong>，显著降低了存储的额外开销。</li><li><strong>智能缓存与重计算逻辑</strong>在检索速度和磁盘使用之间找到了一个绝佳的平衡点。</li></ul><p>这些优化结合起来创造了一个既轻量又具备可扩展性的系统。这是一个能装进你笔记本电脑、却能处理数百万记录的数据库。</p><h2>快速上手</h2><p>上手 LEANN 非常简单，代码如下：</p><pre><code>#git clone https://github.com/yichuan-w/LEANN.git leann  
#cd leann  
#uv pip install leann  
      
from leann import LeannBuilder, LeannSearcher, LeannChat  
from pathlib import Path  
INDEX_PATH = str(Path("./").resolve() / "demo.leann")  
      
# Build an index  
builder = LeannBuilder(backend_name="hnsw")  
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")  
builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")  
builder.build_index(INDEX_PATH)  
      
# Search  
searcher = LeannSearcher(INDEX_PATH)  
results = searcher.search("fantastical AI-generated creatures", top_k=1)  
      
# Chat with your data  
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})  
response = chat.ask("How much storage does LEANN save?", top_k=1)</code></pre><h2>总结</h2><p>LEANN 的出现，标志着向量数据库领域正在迎来自己的 <strong>SQLite 时刻</strong>。SQLite 的成功在于它提供了<strong>轻量级、零配置、无服务器</strong>的本地数据管理能力，它让应用开发者可以轻松地在边缘设备、桌面应用中嵌入强大的 SQL 能力。</p><p>LEANN 也在做同样的事情，但它针对的是 <strong>RAG 和语义搜索能力</strong>。它能够将数百万条嵌入向量存放在本地文件中还能快速检索，对于那些需要构建<strong>离线应用、移动端 AI 功能</strong>或纯粹关注<strong>个人数据隐私</strong>的开发者而言，LEANN 提供了一种开箱即用的、极度高效的解决方案。所以有兴趣的赶紧试试吧。</p><p><a href="https://link.segmentfault.com/?enc=bItDfZrtWs6fv91GcM7omA%3D%3D.4CmGpUDMGynEbqlluru9ym2f%2FNfdFM8Ia1z5qITRtDEBr5RB1zKHndK2dIw7Cerbou2V%2FaDGPxqkSGesPV%2FGfA%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/1b13106d6d7a46f2adebc33bc49ff8d8</a></p>]]></description></item><item>    <title><![CDATA[Doris MCP Server 0.5]]></title>    <link>https://segmentfault.com/a/1190000047412793</link>    <guid>https://segmentfault.com/a/1190000047412793</guid>    <pubDate>2025-11-19 22:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近日，Doris MCP Server 0.5.1 版本带来了多项企业级数据治理与分析能力的功能升级，进一步提升系统稳定性与易用性，欢迎下载体验。</p><p>项目地址：<a href="https://link.segmentfault.com/?enc=p6kvNSK2rMvz3AXspWJkPA%3D%3D.rMSJluJ6HSjnXRy4XIAt0W6XHGR8r%2Fx0UMf3DfYCFTHfy1ocMcHyazkD0KyvaFqB" rel="nofollow" target="_blank">https://github.com/apache/doris-mcp-server</a></p><h2>新增能力概览</h2><ul><li>全局 SQL 超时配置增强：支持通过配置项统一控制所有 SQL 查询超时时间，所有入口（MCP 工具、API、批量查询等）均严格受控</li><li>解决连接池 at_eof 异常连接问题：全新自愈式连接池架构，99.9% 消除连接异常，生产环境稳定性大幅提升。</li><li>新增支持 8 项数据治理与分析工具：数据质量分析、血缘追踪、数据新鲜度监控、访问模式分析、依赖分析、慢查询分析、资源增长分析等一站式能力；</li><li>新增支持 ADBC 数据传输协议：基于 Arrow Flight SQL 协议带来 3-10 倍的查询加速。</li><li>全面升级日志系统：企业级分级日志、自动清理、审计追踪，运维无忧。</li><li>调参文档改版优化：当前所有参数支持环境变量，文档更清晰，配置更灵活。</li></ul><p>本次升级完全兼容 Doris MCP Server 0.4.x 版本，可参考文档步骤平滑迁移。详情请见项目文档。</p>]]></description></item><item>    <title><![CDATA[用了半年Cursor，我为什么退了会员？]]></title>    <link>https://segmentfault.com/a/1190000047412823</link>    <guid>https://segmentfault.com/a/1190000047412823</guid>    <pubDate>2025-11-19 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>开头</h2><p>我做了件让社群炸锅的事——</p><p>把用了半年的Cursor会员，退了。</p><p>20美元一个月那个。</p><p>学员们私信我：「老师你不搭产品了？」</p><p>我说：「搭啊，而且速度更快了。」</p><p>「那你咋实现的？你又不会写代码。」</p><p>「一个3块钱的工具。」</p><p>「？？？」</p><p>他们以为我在开玩笑。直到我在社群里演示了一遍——</p><p>用3块钱的工具，20分钟搭了个落地页。</p><p>这才发现，不会写代码的产品经理，也能“轻松”玩转AI编程工具（毕竟 cursor 虽然能玩，但绝不算轻松😭）。</p><p>关键是，比Cursor还好用。</p><h2>Cursor哪里惹到我了？</h2><p>先说清楚，Cursor本身没啥大毛病。</p><p>半年多用下来，体验还行。代码补全准，理解上下文也到位，确实能提效。</p><p>但三个问题，越用越难受。</p><p><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdm6oL" alt="CleanShot 2025-11-19 at 17.00.44.png" title="CleanShot 2025-11-19 at 17.00.44.png"/></p><p><strong>第一个：贵</strong></p><p>20美元。</p><p>一个月。</p><p>我不是全职写代码的。一个月可能就写两三个项目。算下来，每个项目要分摊7-10美元。</p><p>我算了笔账。一年下来，240美元。够我喝多少杯咖啡了？</p><p>对我来说，不划算。</p><p><strong>第二个：限速破防了</strong></p><p>Pro会员，每月500次快速响应。</p><p>听起来挺多？不够用。</p><p>写个中等规模的功能，调个几十次很正常。项目一多，很快见底。</p><p>超了咋办？等。或者加钱。</p><p>这感觉就像，你办了健身卡，结果还得排队。</p><p><strong>第三个：国内体验真的差</strong></p><p>这个最要命。</p><p>很多人说，国内用Cursor经常抽风。有时候慢得要死，有时候直接连不上。</p><p>我也遇到过好几次。正写着代码呢，突然卡住。等半天，没反应。</p><p>工作的时候最怕这种。你不知道今天能不能顺利干活。这种不确定性，真的很烦。</p><p>有次我赶项目，连续三天晚上Cursor都抽风。我直接破防了。</p><p><strong>你用Cursor遇到过啥坑？评论区聊聊</strong> 👇</p><p><img width="605" height="472" referrerpolicy="no-referrer" src="/img/bVdm6oM" alt="图片" title="图片" loading="lazy"/></p><h2>TRAE SOLO到底是啥？</h2><p>字节搞的AI编程工具。SOLO模式去年7月上线。</p><p>定位和传统工具不太一样。</p><p><strong>核心区别在哪？</strong></p><p>Cursor是助手。你写代码，它辅助你。</p><p>TRAE SOLO是工程师。它能理解整个项目，独立干活。</p><p>差别大吗？</p><p>很大。</p><p><strong>用Cursor</strong>：</p><ul><li>你写代码，AI补全</li><li>遇到问题，你告诉它咋改</li><li>需要你盯着每一步</li></ul><p><strong>用TRAE SOLO</strong>：</p><ul><li>你提需求，它独立完成</li><li>自己理解项目结构</li><li>自己生成和集成代码</li><li>自己测试和修bug</li></ul><p>区别看出来了吧？</p><p>Cursor需要你参与每一步。SOLO更像个能独立干活的工程师。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdm6oN" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>两个智能体</strong></p><p>TRAE SOLO提供了两个：</p><ol><li><strong>SOLO Coder</strong>：懂技术的人用</li></ol><ul><li>处理复杂项目</li><li>理解完整代码库</li><li>多文件同步改</li></ul><ol start="3"><li><strong>SOLO Builder</strong>：小白也能用</li></ol><ul><li>快速搭应用</li><li>可视化界面</li><li>门槛低</li></ul><p>我主要用SOLO Coder。它的理解能力确实行。</p><p>不是理解当前这个文件。是理解整个项目。用了啥框架，模块咋关联的，代码风格啥样的。</p><p>这种理解能力，让它写出来的代码能无缝融入你的项目。不是那种一看就很AI的代码。</p><p><strong>关键优势：对国内用户友好</strong></p><p>这点很重要。</p><p>TRAE是国内团队开发的。网络环境对国内用户更友好。不用担心突然连不上，或者模型抽风。</p><p>实际体验，响应速度和稳定性确实比Cursor好。</p><p>基本上每次提问，1-2秒就回了。不像Cursor，有时候要等5-10秒，甚至更久。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6oO" alt="图片" title="图片" loading="lazy"/></p><h2>价格对比（重点）</h2><p>最直观的部分来了。</p><p>项目</p><p>Cursor</p><p>TRAE SOLO</p><p><strong>首月价格</strong></p><p>$20</p><p>$3</p><p><strong>后续月费</strong></p><p>$20/月</p><p>$10/月</p><p><strong>年度成本</strong></p><p>$240</p><p>$113</p><p><strong>快速响应限制</strong></p><p>500次/月</p><p>600 次/月</p><p><strong>网络稳定性</strong></p><p>国内用户可能不稳定</p><p>相对稳定</p><p><strong>中文支持</strong></p><p>一般</p><p>原生支持</p><p>我算了笔账。</p><p><strong>第一年总成本</strong>：</p><ul><li>Cursor：$240</li><li>TRAE SOLO：$3(首月)+$10×11=$113</li></ul><p>省了127美元。约900人民币。</p><p>够干啥的？够我喝一整年的奶茶了。</p><p>而且TRAE的10美元/月，Pro会员600 次（现在还有另赠送的次数）。</p><p>你算算看，值不值？评论区扣个数 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oP" alt="战术层-隐性陷阱卡片" title="战术层-隐性陷阱卡片" loading="lazy"/></p><h2>限时活动：手快有手慢无</h2><p>重点来了。</p><p>TRAE现在有个活动——<strong>Pro会员邀请计划</strong>。</p><p>机制是这样：</p><ol><li>你成为Pro会员（首月3美元）</li><li>你邀请朋友用你的链接升级Pro</li><li>朋友升级成功后，你俩在<strong>活动期内</strong>都能无限用SOLO</li><li>前提是，你得保持Pro会员身份</li></ol><p><strong>注意两点</strong></p><p><strong>第一：限时的</strong></p><p>官方页面写着"This program runs until --"。有结束时间，具体日期没公开。</p><p>应该是TRAE为了推广SOLO搞的早期活动。</p><p>现在可能是好时机。错过了，可能就没了。</p><p><strong>第二：不是白嫖</strong></p><p>你需要保持Pro会员（每月10美元）才能持续用。</p><p>但用邀请机制，能在活动期内无限用SOLO。比正常订阅划算。</p><p><strong>真实价值在哪？</strong></p><p>正常来说：</p><ul><li>TRAE Pro会员：首月$3，后续$10/月</li><li>用邀请活动：活动期内无限用SOLO</li></ul><p>对比Cursor：</p><ul><li>费用更低</li><li>比500次/月多</li></ul><p><strong>稀缺性：需要邀请码</strong></p><p>这个活动的门槛是——你需要有邀请链接。</p><p>不是所有人都能直接参与。需要有Pro会员的邀请。</p><p>我手上有邀请链接。想试试的话，评论区扣1。</p><p>但我也不知道活动会持续多久。官方随时可能调整。</p><h2>实际体验：真的好用吗？</h2><p>光说不练假把式。</p><p>我用TRAE SOLO做了几个项目。分享下真实感受。</p><p><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdm6oQ" alt="CleanShot 2025-11-19 at 16.59.38.png" title="CleanShot 2025-11-19 at 16.59.38.png" loading="lazy"/></p><p><strong>别人咋说的</strong></p><p>有用户分享了用TRAE SOLO 3小时搭女装电商网站的经历。</p><p>爱范儿的测评也提到：“一张草图变网页，实测字节 TRAE SOLO，这些功能甚至比 Cursor 还好用”。</p><p>这些都是真实案例和第三方测评。</p><p><strong>我的真实感受</strong></p><p>用了一段时间，我的感受：</p><p>✅ <strong>响应速度</strong>：比Cursor快，基本秒回✅ <strong>理解能力</strong>：能理解项目结构，改得比较准✅ <strong>稳定性</strong>：网络连接稳定，没遇到过用不了的情况✅ <strong>性价比</strong>：10美元/月，比Cursor便宜一半</p><p>⚠️ <strong>还不够完美的地方</strong>：</p><ul><li>生态还在建设，插件不如Cursor丰富</li><li>有时候对需求理解会有偏差，需要多解释</li><li>文档还在完善，有些功能不太好找</li></ul><p>但考虑到价格和稳定性，这些小瑕疵完全能接受。</p><h2>AI工具这事儿，其实是生态之争</h2><p>很多人问我：模型哪个强？</p><p>我的回答可能让你意外。</p><p><strong>模型不是最重要的。</strong></p><p>生态才是。</p><p>为啥？</p><p><strong>第一：模型会变</strong></p><p>今天GPT-4领先，明天Claude可能反超。大模型迭代太快了，谁都不敢说自己永远第一。</p><p><strong>第二：生态是长期的</strong></p><p>你在一个生态里积累的数据、习惯的工作流，迁移成本很高。</p><p>就像你用惯了iPhone，换安卓会很不适应。不是安卓不好，而是你的数据、习惯都在苹果生态里。</p><p><strong>第三：体验差距来自生态</strong></p><p>同样的模型，接入了你的工作场景，体验完全不一样。</p><p>这就是为啥有人觉得某个AI特别好用，而你用起来感觉一般。因为它没和你的工作流打通。</p><p><strong>对我们意味着啥？</strong></p><p>选AI工具，本质上是选生态和阵营。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oR" alt="图片" title="图片" loading="lazy"/></p><p>你得想清楚：</p><ul><li>你日常主要用啥平台？微信？飞书？还是Slack、Notion？</li><li>你的数据积累在哪？</li><li>你希望AI能连接哪些场景？</li></ul><p><strong>举个实际例子</strong></p><p>我有个朋友，做新媒体的，主要在微信生态工作。那腾讯系的AI工具可能更适合他。</p><p>我是程序员，代码库都在GitHub。那Cursor、TRAE这种能深度理解代码仓库的工具更合适。</p><p>如果你是跨境从业者，主要用国外工具，那ChatGPT生态可能是首选。</p><p>没有绝对的最好。只有适不适合。</p><p>而且，我建议大家<strong>不要把鸡蛋放一个篮子里</strong>。</p><p>今天Cursor封了TRAE的API，明天可能还会有其他变化。</p><p>但如果你选的工具，本身就在你常用的生态里，至少不会说停就停。</p><p><strong>TRAE的生态优势</strong></p><p>我选它，除了价格和体验，还有个重要原因：它背后是字节的生态。</p><p>字节在国内的工具矩阵——飞书、Coze、剪映、抖音……覆盖了办公、创作、传播的全链路。</p><p>TRAE未来可能会和这些工具打通。</p><p>比如：</p><ul><li>用TRAE写的代码，直接部署到飞书小程序</li><li>用TRAE生成的内容，直接导入剪映做视频</li><li>用TRAE分析的数据，直接同步到飞书文档</li></ul><p>这些不是幻想。是有生态基础的。</p><p>这就是生态的力量。</p><p><img width="720" height="306" referrerpolicy="no-referrer" src="/img/bVdm6oS" alt="图片" title="图片" loading="lazy"/></p><p><strong>你在啥生态里工作？评论区聊聊</strong> 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oT" alt="未来层-生态想象卡片" title="未来层-生态想象卡片" loading="lazy"/></p><h2>啥人适合？啥人别碰？</h2><p>TRAE SOLO不是万能的。</p><p><strong>我有个朋友系列</strong></p><p>我有个朋友，做独立开发的。每个月预算就500块。20刀的Cursor对他来说确实有点贵。</p><p>他试了TRAE之后，直接把Cursor退了。首月3美元，后续10美元/月。对他来说，刚刚好。</p><p>还有个做外包的哥们，全职写代码。一天要调用AI几百次。Cursor的500次限制对他来说根本不够用。</p><p>换了TRAE之后，无限制调用。再也不用担心次数用完了。</p><p>但我另一个朋友，在大厂做架构师。他们项目很复杂，需要很多企业级功能。TRAE的生态还在建设，暂时不太够。</p><p>他还是在用Cursor。</p><p>还有个朋友，重度Cursor用户。装了一堆插件，工作流已经深度绑定了。迁移成本太高。</p><p>他也没换。</p><p><strong>你是哪种情况？评论区说说</strong> 👇</p><h2>我的选择</h2><p>三个月过去了。</p><p>Cursor的会员自动续费，我没开。</p><p>TRAE用得挺顺手。</p><p>为啥？</p><ol><li><strong>价格</strong>：省了一半的钱</li><li><strong>稳定性</strong>：网络更稳定</li><li><strong>体验</strong>：响应快，理解准</li><li><strong>活动</strong>：限时福利，性价比高</li></ol><p>我不确定3美元的首月价格会持续多久。限时活动也不知道啥时候会结束。</p><p>但至少现在，TRAE SOLO对我来说是性价比很高的选择。</p><p>省下的钱，够我喝一个月奶茶了。</p><p><strong>想试试的话</strong>：</p><ol><li>评论区扣1，我发邀请链接</li><li>首月3美元，可以体验一个月再决定</li><li>觉得不合适可以随时取消</li></ol><p>用我的邀请链接注册，咱俩在活动期内都能无限用SOLO。</p><p>先到先得。</p><p>有问题评论区见！🚀</p><hr/><p><strong>补充说明</strong>：</p><p>这篇文章基于我的真实使用体验，没收广告费（新号大厂也看不上😭）。推荐TRAE是因为它确实解决了我的痛点，价格也合理。</p><p>如果你有不同的使用场景或需求，欢迎在评论区讨论。选工具这事，没有绝对的好坏，只有适不适合。</p>]]></description></item><item>    <title><![CDATA[如何安装 MousePlus_v5_4_]]></title>    <link>https://segmentfault.com/a/1190000047412595</link>    <guid>https://segmentfault.com/a/1190000047412595</guid>    <pubDate>2025-11-19 21:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p>一、准备工作</p><ol><li><strong>下载好安装包</strong>：<strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=fFHJkUie2jlN%2B4CkhBpphg%3D%3D.RpT7wna%2FSVtDLSqj6ELgy9MHrCwDrYFW74WmKmOMcq2B0eKGMbHsfKgEx9APJjo6" rel="nofollow" title="https://pan.quark.cn/s/cdd7206bc5a8" target="_blank">https://pan.quark.cn/s/cdd7206bc5a8</a>，下载了 <strong>MousePlus_v5_4_13.exe</strong>这个文件，一般是个exe格式的程序，双击就能运行。</li></ol><h3>二、开始安装</h3><ol><li><strong>找到安装文件</strong>：打开你下载文件的那个文件夹（比如“下载”文件夹），找到 <strong>MousePlus_v5_4_13.exe</strong>，双击它。</li><li><strong>可能会弹出安全提示</strong>：Windows 可能会问你“是否要运行这个程序”，点  <strong>“是”</strong> 或者  <strong>“允许”</strong> 就行。</li><li><strong>安装向导启动</strong>：稍等一下，会跳出一个安装界面，一般有“下一步”“安装”“完成”这些按钮。</li><li><p><strong>选择安装位置（可选）</strong> ：</p><ul><li>有的版本会让你选安装到哪个文件夹，默认一般是在 C 盘某个地方（比如 Program Files）。</li><li>如果你想改地方，可以点“浏览”自己选个盘或文件夹；不想改就直接点  <strong>“下一步”</strong> 。</li></ul></li><li><strong>开始安装</strong>：看到“安装”按钮后，点它，程序就会自动复制文件到你的电脑里，这个过程可能几秒钟到十几秒，耐心等等。</li><li><strong>安装完成</strong>：等它搞完后，会跳出提示说“安装成功”或者“安装完成”，点  <strong>“完成”</strong> 或者  <strong>“关闭”</strong> 就行。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[JSONBench 榜单排名第一！ 10]]></title>    <link>https://segmentfault.com/a/1190000047412610</link>    <guid>https://segmentfault.com/a/1190000047412610</guid>    <pubDate>2025-11-19 21:05:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>坦白讲，每次看性能测试排行榜，我都会下意识地先找找 Apache Doris 在哪个位置。</p><p>这次打开 JSONBench 的榜单，心情一如既往的期待加紧张。</p><p>好在结果让我松了一口气：默认配置下就能排到第三，仅次于维护方 ClickHouse 的两个版本。</p><p>不过，Doris 只能止步于此了吗？经过一系列优化后，查询时长能不能再缩短点？和 ClickHouse 的差距在哪里？</p><p>调优前后对比图镇楼，至于调优的具体思路，请一起往下看吧。</p><ul><li><p>Apache Doris 排名 (Default)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412612" alt="Apache Doris 排名 (Default)" title="Apache Doris 排名 (Default)"/></p></li><li><p>Apache Doris 排名 (Unofficial Tuned)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412613" alt="Apache Doris 排名 (Unofficial Tuned)" title="Apache Doris 排名 (Unofficial Tuned)" loading="lazy"/></p></li></ul><h2>JSONBench 简介</h2><p>JSONBench 是一个为 JSON 数据而生的数据分析 Benchmark，简单来说，它由 10 亿条来自真实生产环境的 JSON  数据、5 个针对 JSON  构造的特定 SQL 查询组成，旨在对比各个数据库系统对半结构化数据的处理能力。目前榜单包括 ClickHouse、MongoDB、Elasticsearch、DuckDB、PostgreSQL 等知名数据库系统，截至目前，<strong>Doris 的性能表现是 Elasticsearch 的 2 倍，是 PostgreSQL 的 80 倍</strong>。</p><blockquote><em>JSONBench 官网地址：jsonbench.com</em></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412614" alt="JSONBench 简介.png" title="JSONBench 简介.png" loading="lazy"/></p><p>不仅在性能上 Apache Doris 领先其他同类产品，<strong>在数据集相同的情况下，Apache Doris 的存储占用是 Elasticsearch 的 1/2、PostgreSQL 的 1/3</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412615" alt="JSONBench 简介-2.png" title="JSONBench 简介-2.png" loading="lazy"/></p><p>JSONBench 测试具体流程：首先在数据库中创建一张名为 Bluesky 的表，并导入十亿条真实的用户行为日志数据。测试过程中，每个查询重复执行三次，并且在每次查询前清空操作系统的 Page Cache，以模拟冷热查询的不同场景。最终，通过综合计算各查询的执行耗时得出数据库的性能排名。</p><p>在这个测试中，Apache Doris 使用了 Variant 数据类型来存储 JSON 数据，默认的建表 Schema 如下：</p><pre><code class="SQL">CREATE TABLE bluesky (
    `id` BIGINT NOT NULL AUTO_INCREMENT,
    `data` variant NOT NULL
)
DISTRIBUTED BY HASH(id) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>Variant 是 Apache Doris 2.1 中引入一种新的数据类型 ，它可以存储半结构化 JSON 数据，并且允许存储包含不同数据类型（如整数、字符串、布尔值等）的复杂数据结构，而无需在表结构中提前定义具体的列。Variant 类型特别适用于处理复杂的嵌套结构，而这些结构可能随时会发生变化。在写入过程中，该类型可以自动根据列的结构、类型推断列信息，动态合并写入的 schema，并通过将 JSON 键及其对应的值存储为列和动态子列。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=MduoGda%2BRx53pszxTiDWcA%3D%3D.JO%2BNfiZCht94rcI3SW7rZpojGi0IIBzjzLmLH6qP0ndoSt5HegZfrW%2BnRrL4EQpiC5l7TPs0txGFtl6bUtkxeoFSMiG674ypePxMTBDLfpsIBD3hfNtd9cE8XGg0bPRuHvz3asTH3p50iRQIn7RzEg%3D%3D" rel="nofollow" target="_blank">Apache Doris Variant 类型详情</a></li></ul></blockquote><h2>调优思路与原理</h2><p>JSONBench 榜单排名依据各个数据库系统在默认配置下的性能数据，那么能否通过调优，让 Apache Doris 进一步释放性能潜力，实现更好的性能效果呢？</p><h3>01 环境说明</h3><ul><li>测试机器：AWS M6i.8xlarge(32C128G);</li><li>操作系统：Ubuntu24.04;</li><li>Apache Doris: 3.0.5;</li></ul><h3>02 Schema 结构化处理</h3><p>由于 JSONBench 特定查询中涉及到的 JSON 数据都是固定的提取路径，换言之，半结构化数据的 Schema 是固定的，因此，我们可以借助生成列，将常用的字段提取出来，实现半结构化数据和结构化数据结合的效果。类似的高频访问的 JSON 路径或者需要计算的表达式，都可以使用该优化思路，添加对应的生成列来实现查询加速。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=fNz6ImJBc0u%2BEK70VNGU%2FA%3D%3D.T2QZhn2HY%2FTZcnJ95aj%2F4uQKtR1R4V7vXlwLgtfDjK2hL2DEPFmdiWUjg1VsEg%2F0o8eb3CghQW%2BTRN6vhDIShoixmYyZdW1V%2FXLccXpnlEk%3D" rel="nofollow" target="_blank">查看 JSONBench 查询</a></li><li><a href="https://link.segmentfault.com/?enc=hsmdTUvp0tMkCRbher4hgg%3D%3D.1D7tjNihUSmJhAs70ug0lJzOXgwdKbGk5uwLdq2IRsutXXq3vgevO4Hxto9YJK7czjWR78dR3Yk7ldy8er66UZ0%2FiQjKYnc6PsZ0rxohlVnBWWkeSQ5C8sao1vZIBNTfZiI3NuSj464zAawaY%2BAh4w%3D%3D" rel="nofollow" target="_blank">Apache Doris 生成列详情</a></li></ul></blockquote><pre><code class="SQL">CREATE TABLE bluesky (
    kind VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.kind')) NOT NULL,
    operation VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.operation')) NULL,
    collection VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.collection')) NULL,
    did VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data,'$.did')) NOT NULL,
    time DATETIME GENERATED ALWAYS AS (from_microsecond(get_json_bigint(data, '$.time_us'))) NOT NULL,
    `data` variant NOT NULL
)
DUPLICATE KEY (kind, operation, collection)
DISTRIBUTED BY HASH(collection, did) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>除了可以减少查询时提取数据的开销，还可以用展平出来的列作为分区列，使得数据分布更均衡。</p><p>需要注意的是，查询的 SQL 语句也要改为使用展平列的版本：</p><pre><code class="SQL">// JSONBench 原始查询：
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count, COUNT(DISTINCT cast(data['did'] AS TEXT )) AS users FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, HOUR(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT cast(data['did'] AS TEXT ) AS user_id, MIN(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS first_post_ts FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT cast(data['did'] AS TEXT ) AS user_id, MILLISECONDS_DIFF(MAX(from_microsecond(CAST(data['time_us'] AS BIGINT))),MIN(from_microsecond(CAST(data['time_us'] AS BIGINT)))) AS activity_span FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;

// 使用展平列改写的查询：
SELECT collection AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT collection AS event, COUNT(*) AS count, COUNT(DISTINCT did) AS users FROM bluesky WHERE kind = 'commit' AND operation = 'create' GROUP BY event ORDER BY count DESC;
SELECT collection AS event, HOUR(time) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT did AS user_id, MIN(time) AS first_post_ts FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT did AS user_id, MILLISECONDS_DIFF(MAX(time),MIN(time)) AS activity_span FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;</code></pre><h3>03 Page Cache 调整</h3><p>调整查询语句后，开启 profile，执行完整的查询测试：</p><pre><code class="Plain">set enable_profile=true;</code></pre><p>进入 FE 8030 端口的 Web 页面，找到相关 profile 进行分析，此时发现 SCAN Operator 中的 Page Cache 命中率较低，导致热读测试过程中存在一部分冷读操作。</p><pre><code class="SQL">-  CachedPagesNum:  1.258K  (1258)
-  TotalPagesNum:  7.422K  (7422)</code></pre><p>这种情况通常是由于 Page Cache 容量不足，无法完整缓存 Bluesky 表中的数据。建议在 <code>be.conf</code> 中添加配置项 <code>storage_page_cache_limit=60%</code>，将 Page Cache 的大小从默认的内存总量的 20% 提升至 60%。重新运行测试后，可以观察到冷读问题已得到解决。</p><pre><code class="SQL">-  CachedPagesNum:  7.316K  (7316)
-  TotalPagesNum:  7.316K  (7316)</code></pre><h3>04 最大化并行度</h3><p>为了进一步挖掘 Doris 的性能潜力，可以将 Session 变量中的<code>parallel_pipeline_task_num</code>设为 32，因为本次 Benchmark 测试机器<code>m6i.8xlarge</code>为 32 核，所以我们将并行度设置为 32 以最大程度发挥 CPU 的计算能力。</p><pre><code class="SQL">// 单个 Fragment 的并行度
set global parallel_pipeline_task_num=32;</code></pre><h2>调优结果</h2><p>经过上述对 Schema、Query、内存限制、CPU 等参数的调整，我们对比了调优前后 Doris 的性能表现以及一些其他数据库系统的成绩，有如下结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412616" alt="调优结果.png" title="调优结果.png" loading="lazy"/></p><p>可以看到，<strong>对比调优前的 Doris，调优后 Doris 查询整体耗时降低了 74%，对比原榜单第一的 ClickHouse 产品实现了 39% 的领先优势</strong>。</p><h2>总结与展望</h2><p>通过对 Schema 的结构化处理、查询语句的优化、缓存配置的调整以及并行参数的设置，Apache Doris 整体查询耗时显著下降，并超越 ClickHouse。</p><p>在默认设置下，Doris 在 10 亿条 JSON 的查询耗时与 ClickHouse 仍有数秒的差异。然而，依托于 Doris 在 JSON 处理、Variant 类型支持及生成列等能力的加持，经调优后，其半结构化数据处理性能获得了进一步显著提升，并在同类数据库中表现出明显的领先优势。</p><p>未来，Apache Doris 将继续打磨在半结构化领域的数据处理能力，为用户带来更加优质、高效的分析体验，包括：</p><ul><li>优化 Variant 类型稀疏列的存储空间，支持万列以上的子列；</li><li>优化万列大宽表的内存占用；</li><li>支持 Variant 子列根据列名的 Pattern 自定义类型、索引等。</li></ul><h2>推荐阅读</h2><ul><li><a href="https://link.segmentfault.com/?enc=EiltnwORoL2PXtlZeXWj%2Fw%3D%3D.JIBRSS9NUFAUjOPZwmbpG2CLXEqDG2v1sFzlSjwHRtYoMMea6HUBiFAEkCd80QN%2B" rel="nofollow" target="_blank">Apache Doris 针对半结构化数据分析的解决方案及典型场景</a></li><li><a href="https://link.segmentfault.com/?enc=CXJHuLEc9QuwQo3UAeJBKA%3D%3D.J79R97kkQGGm6PHEzs1C7XL1dXg706GnrFcnZew04kh92PlJb%2BINZUdOnuj1pI3E" rel="nofollow" target="_blank">揭秘 Variant 数据类型：灵活应对半结构化数据，JSON 查询提速超 8 倍，存储空间节省 65%</a></li></ul>]]></description></item><item>    <title><![CDATA[Apache Doris 实时更新技术揭]]></title>    <link>https://segmentfault.com/a/1190000047412623</link>    <guid>https://segmentfault.com/a/1190000047412623</guid>    <pubDate>2025-11-19 21:04:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>数据，是有保质期的。正如冰箱里的牛奶，今天新鲜，明天可能就有点酸，后天直接倒掉。数据的价值，也会随着时间的推移而递减。</p><p>过去那些“老派”的 OLAP 系统，只能批量处理账目，对实时性要求高的“流水账”就力不从心了。它们在面对高并发实时写入和复杂的分析查询时，常常会露出疲态，数据延迟、查询性能、并发处理和数据更新等问题层出不穷。</p><p>当所有人都焦虑于如何让数据“快”起来的时候，Apache Doris 在底层逻辑上进行了一系列颠覆性的技术迭代，能快速接入各种数据源，并且拥有强大的实时更新能力，让你的数据从产生的那一刻起，就具备了“快”的生命力。</p><p>到底 Doris 是怎么做到，让数据流动得如此低延迟？这正是我们接下来要深挖的“冰山之下”。</p><h2>实时更新的挑战</h2><p>在实现实时更新的过程中，系统需要应对多个方面的挑战，这些挑战直接关系到系统在实际业务场景中的稳定性与性能表现：</p><ul><li><strong>数据延迟</strong>：实时更新的核心在于“快”。数据从产生到可查询的过程必须尽可能短，实际生产中要求在 5-10 秒可见，理想情况下甚至要求低于 1 秒可见。同时，还需具备足够的写入吞吐能力，保证在并发写入场景下也能稳定运行。</li><li><strong>查询性能</strong>：一边持续高频地接收数据更新，一边还能保持百毫秒级别的查询响应，对底层系统架构提出了极高要求。如何在更新密集的情况下，仍然提供快速、稳定的查询体验，是实时 OLAP 系统必须解决的问题。</li><li><strong>并发处理</strong>：实时分析场景多面向终端用户，不仅查询操作需要支持高并发，同时写入也常常是并发的。传统的表或者分区级别的写冲突处理机制影响范围较大，会影响数据写入效率与业务体验。理想状态下，系统应允许用户制定冲突处理策略，从而提高数据接入的灵活性与可控性。</li><li><strong>数据流维护与易用性保障</strong>：实时数据流的维护受多项复杂因素的影响，例如 TP 系统通过 CDC 捕获的删除操作以及 Schema 变更带来的下游兼容性问题。同时，在链路重启或容灾恢复时，如何确保数据既不重复和不丢失，这对数据一致性的要求非常高。</li></ul><p>这些挑战正是评估一个 OLAP 系统是否真正具备实时更新能力的关键指标。</p><h2>常见方案对比</h2><p>在面临数据更新的上述挑战时，市面上的常用方案通常涉及三个关键点，分别是表达方式、更新实现和冲突解决，这些方案各有其适用场景。</p><ol><li><strong>在表达方式上，</strong> Snowflake、Redshift、Iceberg、Databricks 和 Hudi 通常使用 MERGE INTO 来处理数据更新，这要求变更数据必须先落盘成为 MERGE 的数据源，因此可能带来一定的数据延迟和额外的 I/O 开销。相比之下，Doris 和 ClickHouse 采用更加轻量的方式，通过特定列值表示删除操作，使得写入和删除可以统一在同一数据流中处理，更加契合实时处理需求，尤其适用于 OLTP 类事务变更、订单或账单状态更新等场景。</li><li><p><strong>在更新实现方面，</strong> 业界常见的方案有四种：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412625" alt="常见方案对比" title="常见方案对比"/></p></li></ol><pre><code>- **Copy on Write**：在写入时找出需要更新的文件，读取并结合新的更新，生成新文件再写入。这种方式优化了读取性能，但在写入时会显著增加 I/O 开销，特别是在随机更新情况下，会引发大量读写 I/O，限制了实时更新能力。此方案的典型产品包括 Redshift、ClickHouse、Snowflake、Iceberg 和 Hudi。
- **Merge on Read**：在写入时仅需添加新数据，读取时再合并新旧数据，类似于 LSM Tree。这种方式优化了写入性能，但查询效率较低，难以满足某些实时场景的查询延迟要求，典型产品包括 Iceberg、Hudi 和 Doris Merge on Read Unique 表。
- **Delete bitmap / deletion vector：** 标记删除的实现方案。在写入时标记文件中被删除的数据，并写入删除标记及新数据，查询时跳过删除标记的数据行。此方式既能避免 Copy on Write 的 I/O 放大效应，也获得了 Copy on Write 的查询性能。但是对于没有主键索引的实现，生成删除标记（Delete Bitmap / Delete Vector）时 I/O 和 CPU 消耗较大，效率低下，难以满足高频实时写入场景。
- **Delete bitmap / deletion vector + primary index：** 标记删除与主键索引结合的方式。主键索引能够降低标记删除时的查询 I/O 和 CPU 消耗，使高频实时更新成为可能。Doris 的 Merge on Write Unique 表采用了这种实现方式。
</code></pre><ol start="3"><li><strong>在冲突解决方面，</strong> 经典的写写冲突会导致写入无法并行，从而显著降低写入吞吐量。Doris 提供了基于业务语义的冲突机制，可很好避免该问题（<a href="https://link.segmentfault.com/?enc=u6X8arbQ94eZ2%2F05cPodGA%3D%3D.xiDTOWgzxOf9kWyBCFNNKYw9RMjwg9PmQbjySXslw13YY0XFa3i4G6J3k1vn2pGSmg2kjE4mG0sZvcFV1CXcf9ri2fRP7aNIy%2BlBUjBtS0oqedV22gA7xp4IcrKuKiA6" rel="nofollow" target="_blank">参考文档</a>）。而 Redshift、Snowflake、Iceberg 和 Hudi 等则采用了文件级别的冲突处理，因而不具备实时更新的能力。</li></ol><p>Apache Doris 作为一款为实时分析场景打造的高性能 MPP 分析型数据库，具备强大的数据写入能力、亚秒级查询性能以及出色的并发处理能力，因此成为构建面向用户的实时数据服务的优选方案。基于上述常见方案，本文将详细拆解 Apache Doris 实时更新技术的核心设计，揭示其如何实现“极低延迟”的数据流动性。</p><h2>为什么 Apache Doris 实时更新更具优势？</h2><p>传统 OLAP 数据库主要用于批量分析，数据更新周期通常以小时甚至天为单位，适合以报表为主的内部系统。然而，<strong>随着业务及数据服务的多样化发展，越来越多的分析应用开始面向终端用户，要求亚秒级的查询延迟和秒级的数据更新。</strong> 在此背景下，要求实时分析数据库能够应对高速写入数据（每秒百万级别的数据导入）、并在大规模场景下提供实时查询。Apache Doris 凭借其主键模型、数据延迟、查询性能、并发处理、易用性等多方面特性的表现，在分析领域展现了独特的实时更新能力。</p><h3>01 主键模型</h3><p>Doris 提供了主键表，确保数据主键的唯一性，支持基于主键的 upsert 语义。以下是一个以 <code>user_id</code> 主键的表的创建示例：</p><pre><code class="Plain">CREATE TABLE IF NOT EXISTS example_tbl_unique
(
    user_id         LARGEINT        NOT NULL,
    user_name       VARCHAR(50)     NOT NULL,
    city            VARCHAR(20),
    age             SMALLINT,
    sex             TINYINT
)
UNIQUE KEY(user_id)
DISTRIBUTED BY HASH(user_id) BUCKETS 10;</code></pre><p>在这个表中，初始数据包括四行： 101、102、103、104 。当新写入 101、102 之后，表中的数据仍然保持是四行，但原先 101 和 102 的数据会被更新。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412626" alt="主键模型.jpeg" title="主键模型.jpeg" loading="lazy"/></p><p>参考文档：<a href="https://link.segmentfault.com/?enc=%2FvDcyV%2FhJDqPBCICIlwK8A%3D%3D.aX6pYvy8MSS7hgrtFs9cMo5jSBJeZAedkHH8vt8dbWXSReCDMIFIPvIpNEndVsFEq%2BFZSjzN9l%2BmB3EDX%2F21e%2BIlayyOmw3gO7s9sqpmYsk%3D" rel="nofollow" target="_blank">主键模型的导入更新</a></p><h3>02 数据延迟</h3><p>Doris 提供强一致性语义，确保数据写入后立即可见，从而满足低延迟的实时数据更新需求。数据组织使用 LSM tree 的方式组织，写入操作采用删除标记（Delete Bitmap）方式，相较于传统的 Copy-on-Write 机制，能够显著减少 I/O 操作，提高写入效率。这一设计不仅降低了存储空间的浪费，还减轻了系统的负担，从而提供了更高效的数据处理能力。</p><p>此外，Doris 利用主键索引优化了在更新数据时定位和检索相历史数据的性能，进一步提升了写入速度并降低了资源消耗。<strong>通过这些设计，Doris 实现了综合性的低延迟数据更新，能够提供 1s 以下的数据延迟，满足高效实时分析和快速数据响应的需求。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412627" alt="数据延迟.png" title="数据延迟.png" loading="lazy"/></p><h3>03 查询性能</h3><p>在更新场景下，Doris 采用标记删除（Delete Bitmap）方式加速查询性能。与 Merge-on-Read 的实现相比，标记删除能够避免在查询时进行大量的删除逻辑计算，从而减少查询延迟并提升整体性能，确保查询响应时间低于百毫秒，并支持高并发访问。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412628" alt="查询性能.png" title="查询性能.png" loading="lazy"/></p><p>此外，Doris 基于以下几项技术，进一步提升了查询性能：</p><ul><li><strong>分区和分桶裁剪技术：</strong> 智能跳过无关数据，进一步优化数据扫描过程，减少不必要的数据读取，显著提高查询效率。</li><li><strong>向量化技术：</strong> 在处理大规模数据时，通过批量化处理多个数据操作，减少 CPU 的上下文切换，显著提升数据处理速度，尤其适用于大数据量的查询场景。</li><li><strong>优化器：</strong> 通过智能的查询计划选择和执行，自动根据查询条件调整最佳执行路径，避免不必要的计算开销，进一步提高查询响应速度。</li><li><strong>丰富的索引</strong>：包括点查索引和跳数索引。点查索引常用于加速点查，包括前缀索引和倒排索引，原理是通过索引定位到满足 WHERE 条件的有哪些行，直接读取那些行。跳数索引常用于加速分析，包括 ZoneMap 索引、BloomFilter 索引、NGram BloomFilter 索引，原理是通过索引确定不满足 WHERE 条件的数据块，跳过这些不满足条件的数据块，只读取可能满足条件的数据块并再进行一次逐行过滤，最终得到满足条件的行。</li></ul><p>这些技术的结合使得 Doris 在高并发环境下能够保持稳定的低延迟，确保其在秒级和毫秒级查询性能上表现出色，满足实时数据处理的严格要求。</p><h3>04 并发处理</h3><p>Doris 主键表支持应用语义处理冲突，在高并发乱序写入时能够保证数据的最终一致性。建表时，可以通过指定 SEQUENCE COLUMN 来自定义 MVCC 的冲突处理逻辑，Doris 的写入负载均衡机制优先选择 SEQUENCE 列较大的行。这一机制不仅适用于写入冲突，还同样适用于存量数据。</p><pre><code class="sql">CREATE TABLE test.test_table
(
    user_id bigint,
    date date,
    group_id bigint,
    modify_date date,
    keyword VARCHAR(128)
)
UNIQUE KEY(user_id, date, group_id)
DISTRIBUTED BY HASH (user_id) BUCKETS 32
PROPERTIES(
    "function_column.sequence_col" = 'modify_date',
    "replication_num" = "1",
    "in_memory" = "false"
);</code></pre><p>例如，在 OLTP 表中，<code>modify_date</code> 字段每次更新时都会设置为当前时间。在将 OLTP 数据库的 CDC 同步到 Doris 时，可以将 <code>modify_date</code> 指定为 SEQUENCE 列。这样，具有较大 <code>modify_date</code> 的数据行将生效，而如果后写入的数据 <code>modify_date</code> 较小，则存量数据不会被更新。这一机制使得实时数据同步的冲突处理变得非常简单，同时不影响写入效率。</p><p>参考文档：[主键模型的更新并发控制<br/>](<a href="https://link.segmentfault.com/?enc=P8qQEOdR4UAd8osjUQcvwA%3D%3D.Jnn0wnAH%2FMSyq7xBLd%2FGKXTmQAqlaouHp6%2FiGzUBIsrSWngLYaoL9gL6q5REh%2FaJ3iFbirCv3EOrTfWJtZK%2BWoDwKW5vL%2BJXdt6KT8HMQ0jv9YcPs3ZbdfKHw%2FQt0fWv" rel="nofollow" target="_blank">https://doris.apache.org/zh-CN/docs/data-operate/update/uniqu...</a>)</p><h3>05 易用性</h3><ul><li>首先，Doris 确保每次数据写入的一致性和完整性，保证在高并发和实时更新环境中，数据始终保持一致并立即可见。结合标记删除机制，Doris 使数据更新更加高效，减少了存储开销，并提升了查询性能。</li><li>其次，Doris 还支持在线 Schema 变更，允许动态调整表结构，从而简化数据流的维护，避免复杂的数据迁移过程。同时，灵活的列更新功能使数据更新更为高效，特别是在频繁更新部分数据时，避免了全表更新带来的性能开销。</li><li>最后，Doris 支持隐藏列标记删除方式，即为每个 Unique 表生成隐藏的 <code>DORIS_DELETE_SIGN</code> 列，利用该标志直接进行删除操作，避免了传统的复杂删除步骤，提升了系统性能。同时，Doris 还支持将 SEQUENCE 列与删除标志结合使用，确保过期数据的删除不会影响新数据，简化了实时数据流中的更新与删除操作。</li></ul><p>受益于写入原子性、强一致性语义，以及灵活的在线 Schema 变更和列更新机制等机制，Doris 能够在高并发和实时更新场景中高效处理数据，简化开发工作，并提升系统的响应速度和可靠性。</p><p>参考文档：<a href="https://link.segmentfault.com/?enc=JnQveyiiQbgUNnDHGTvR0w%3D%3D.r%2BsauALD7YjsvLy4afylyFJxjxmtuV70ZctLuLoVvvEOkwBZaLwUGGtiIKaNrGaqjjYKuCmTJa4tjqDQnzvdFxkr3%2Fu9tXQJR6BlIMmoRpQ%3D" rel="nofollow" target="_blank">基于导入的批量删除</a></p><h2>生态融合</h2><p>Doris 提供丰富的 API 和连接器，方便与现有的数据处理工具和框架（如 Spark、Flink、Kafka）进行集成，增强了生态灵活性，使得 Doris 能够为用户提供更加强大的数据处理能力，适应多样化的业务需求和技术环境。</p><h3>01 Kafka</h3><p><a href="https://link.segmentfault.com/?enc=bdaXgm0RhzqkkN6TpJOEMw%3D%3D.SjTh3vuy4mqqOtVCvL0BqREpLkQaoIiNCufkBzmgLs10QWi5mI9fgPcDZr%2BmOXtvcSa7ZMlzZBZqDCy3nhDt4g%3D%3D" rel="nofollow" target="_blank">Kafka Connect</a> 是一款可扩展、可靠的在 Apache Kafka 和其他系统之间进行数据传输的工具，可以定义 Connectors 将大量数据迁入迁出 Kafka，并通过<a href="https://link.segmentfault.com/?enc=lheSLR40F%2Bv9XYv6upWv4w%3D%3D.HJzxFhWlc9J8Xk%2BfCHPPo6s2%2FJaxjppPIbHzUKABOgzngT6NslMfZpnQisSYxCLp%2BfQpCrxOcfqqs0Ni3tB5tc%2BP%2BwN8uAy%2BsFUWwex0Y8Frnra%2BuPO2LlucYoRgC6gn" rel="nofollow" target="_blank"> Doris Kafka Connector </a>将上游 topic 中的数据读取后写入到 Doris 中。</p><p>在 Kafka Connect 集群上新增一个 Doris Sink 的 Connector，示例如下：</p><blockquote>详细步骤参考<a href="https://link.segmentfault.com/?enc=Z2b9zTbcnMaAqOqhXCRmiQ%3D%3D.v50xOwQYBV1eRyoXZ9%2BBm%2BJgCl6z2E11496e9xZ5NqWxKAr3Y2agkO8ldNr52mft2eqyhySJmPizVxkSivDJYdRwtNzCted52nOiuF5hgWc%3D" rel="nofollow" target="_blank">文档</a></blockquote><pre><code class="SQL">curl -i http://127.0.0.1:8083/connectors -H "Content-Type: application/json" -X POST -d '{
  "name":"test-doris-sink-cluster",
  "config":{
    "connector.class":"org.apache.doris.kafka.connector.DorisSinkConnector",
    "topics":"topic_test",
    "doris.topic2table.map": "topic_test:test_kafka_tbl",
    "buffer.count.records":"50000",
    "buffer.flush.time":"120",
    "buffer.size.bytes":"5000000",
    "doris.urls":"10.10.10.1",
    "doris.user":"root",
    "doris.password":"",
    "doris.http.port":"8030",
    "doris.query.port":"9030",
    "doris.database":"test_db",
    "key.converter":"org.apache.kafka.connect.storage.StringConverter",
    "value.converter":"org.apache.kafka.connect.json.JsonConverter"
  }
}'</code></pre><h3>02 Flink</h3><p>Apache Flink 是一个框架和分布式处理引擎，用于在无界和有界数据流上进行有状态的计算。可以使用 <a href="https://link.segmentfault.com/?enc=1iyjgtFsfYtXAhwdKjbbgg%3D%3D.13mDyRM9uc2ZhYZ6y%2FjKhxqR9TL6eRDeELGjQmcf4xOOboGU8vD9I43jx4ihTSzKC2T1J301CxU9KUqL6hiwXzHaH8mzIbN9%2BlowbcxJzIg%3D" rel="nofollow" target="_blank">Flink Doris Connector</a> 将上游的数据，比如 Kafka、MySQL 等产生的数据，实时写入至 Doris。</p><p>使用 Flink 自带的 DataGen 模拟数据写入 Doris 中，示例如下：</p><blockquote>具体步骤参考<a href="https://link.segmentfault.com/?enc=RpLbbvhnjQ7eanH3RrZrsQ%3D%3D.qS%2FFdEOGvgyNw9%2BvKw3sMrkQueCRLAISimx8e5KZWV%2BUbPyigobD%2FlSuuYJYF7J5Nfi5Negezga22nrRYCZVIKpC5Shve2DHdvGgvB1nUDt7XJzoRbTxLqL8lYwtVYalsdMDRsSce21dryzzHau4jII9ObZ9SHTD8g0fnm4CLKYUOXzxoZouTc7fH%2BVSiHnMSq9Zh98XjHi4aPTtF77Rnw%3D%3D" rel="nofollow" target="_blank">文档</a></blockquote><pre><code class="sql">SET 'execution.checkpointing.interval' = '30s';

CREATE TABLE student_source (
    id INT,
    name STRING,
    age INT
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '1',
  'fields.name.length' = '20',
  'fields.id.min' = '1',
  'fields.id.max' = '100000',
  'fields.age.min' = '3',
  'fields.age.max' = '30'
);

-- doris sink
CREATE TABLE student_sink (
    id INT,
    name STRING,
    age INT
    ) 
    WITH (
      'connector' = 'doris',
      'fenodes' = '127.0.0.1:8030',
      'table.identifier' = 'test.student',
      'username' = 'root',
      'password' = 'password',
      'sink.label-prefix' = 'doris_label'
);

INSERT INTO student_sink SELECT * FROM student_source;</code></pre><h3>03 Spark Structured Streaming</h3><p>Structured Streaming 是一个构建在 Spark SQL 引擎之上的可扩展、容错的流处理引擎。借助 Structured Streaming，可以高效地读取上游数据源，并通过 <a href="https://link.segmentfault.com/?enc=28sIv3JG%2FVVCVXtjgH7kvw%3D%3D.l2YwtfGpHM7XlU%2B42QOcUiLzMQxEV3Gwnmrp%2BnexeJ8LtxKZItXD%2FU0Ow7wCo0bZ6tTAScGol4DiyEzt9DT0tdPY8S8DMIyvIXP3%2For8lIU%3D" rel="nofollow" target="_blank">Spark Doris Connector </a>，以 Stream Load 的方式将数据实时写入 Doris，实现端到端的流式数据处理流程。</p><p>使用 Spark 自带的 rate 数据源模拟数据写入 Doris 中，示例如下：</p><blockquote>完整代码参考： <a href="https://link.segmentfault.com/?enc=0ziBlGlAJgOPnpkfISOz4A%3D%3D.tSfNHuC%2FAaDv9kMSWLBIj2hETeYkMEdt5BmJ8O40KJJw3gYihKa1AfveW6YIQrrIPoBh4BYoCFcNOd4jf8XDGiWLUv08wOJXEXPAtD%2BTAb2ot7lQZer5wTphW17%2BmVKKoozF%2FEfuBsyKvJ5VsZU6pHNZ%2Fxab4q1nOXS%2BZLJn3r%2FFywAnuAMvZPUplvjCTgbvRWpr%2BPXnhLnyrY6%2Fg5GAjjnXdhAjr0u26oYqrrPQ4Tqp4WZcn%2FuJYPCUFj46HXEK" rel="nofollow" target="_blank">https://github.com/apache/doris-spark-connector/blob/master/spark-doris-connector/spark-doris-connector-it/src/test/java/org/apache/doris/spark/example/DorisWriteStreamExample.scala</a></blockquote><pre><code class="protobuf">val spark = SparkSession.builder()
  .appName("RateSourceExample")
  .master("local[1]")
  .getOrCreate()

val rateStream = spark.readStream
  .format("rate")
  .option("rowsPerSecond", 10)
  .load()

rateStream.writeStream
  .format("doris")
  .option("checkpointLocation", "/tmp/checkpoint")
  .option("doris.table.identifier", "db.table")
  .option("doris.fenodes", "127.0.0.1:8030")
  .option("user", "root")
  .option("password", "")
  .start()
  .awaitTermination()

spark.stop();</code></pre><h2>实时数据分析最佳实践</h2><h3>用户案例 1：中通快递</h3><p>随着中通快递业务的持续增长，昔日双 11 的业务高峰现已成为每日常态，原有数据架构在数据时效性、查询效率、与维护成本方面，均面临着较大的挑战。为此，中通快递引入 SelectDB，借助其高效的数据更新、低延时的实时写入与优异的查询性能，在快递业务实时分析场景、BI 报表与离线分析场景、高并发分析场景中均进行了应用实践。</p><p><strong>在实时分析场景中，基于 SelectDB 灵活丰富的 SQL 函数公式、高吞吐量的计算能力，实现了结果表的查询加速， 能够达到每秒上 2K+ 数量级的 QPS 并发查询，数据报表更新及时度大大提高。</strong></p><p>SelectDB 的引入满足了复杂与简单的实时分析需求。目前，SelectDB 日处理数据超过 6 亿条，数据总量超过 45 亿条，字段总量超过 200 列，并实现服务器资源节省 2/3、查询时长从 10 分钟降至秒级的数十倍提升。</p><h3>用户案例 2：招联金融</h3><p>招联金融（全称“招联消费金融股份有限公司”）旗下拥有“好期贷”“信用付”两大消费金融产品体系，为用户提供全线上、免担保、低利率的普惠消费信贷服务。早期采用 Lambda 架构，包含 ClickHouse、Spark、Impala、Hive、Kudu、Vertica 等，受限于运维依赖性高、资源利用率低、数据时效性低、并发能力弱等诸多问题。引入 Apache Doris 进行架构升级后，实现了高效实时分析、架构简化、混合部署与弹性伸缩等多项目标。</p><p><strong>在客群筛选分析场景中，之前使用 Vertica 计算引擎处理 2.4 亿条数据耗时 30-60 分钟，替换为 Doris 后用时降至 5 分钟，性能提升 6 倍以上</strong>，并且 Doris 作为开源数据库，相比商业化产品 Vertica 有显著的成本优势。</p><h2>结束语</h2><p>以上就是 Apache Doris 在分析领域的实时更新能力详细介绍。在主键表方面，Doris 支持易用的 UPSERT 语义，结合主键索引和标记删除机制，确保了优异的写入性能和低延迟的查询性能。此外，用户自定义的冲突解决机制进一步提升了实时写入的并发能力，快速的 Schema 变更功能则避免了实时数据流的中断。列更新及灵活的列更新选项为更广泛的实时场景提供了便捷支持。</p><p>展望未来，我们将在以下几方面重点投入：</p><ul><li>降低数据可见性延迟，以实现更加实时的数据访问体验；</li><li>提升生态工具在自动调整 Schema 方面的能力，并扩展 Light Schema 的适用范围；</li><li>更加灵活的列更新，为用户提供更加高效、灵活的数据管理能力。</li></ul>]]></description></item><item>    <title><![CDATA[MacX Video Converter]]></title>    <link>https://segmentfault.com/a/1190000047412640</link>    <guid>https://segmentfault.com/a/1190000047412640</guid>    <pubDate>2025-11-19 21:03:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p><strong>MacX Video Converter Pro</strong>是一款专为 <strong>Mac 电脑用户</strong>设计的 <strong>专业视频格式转换软件</strong>，支持 <strong>快速转换各种视频格式</strong>，比如 MP4、AVI、MOV、MKV、WMV 等，并且还能 <strong>高清转换、提取音频、压缩视频大小、下载在线视频</strong>等。</p><h3><strong>1. 下载文件</strong></h3><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=5DGot4fzeQnImL4m3kc57A%3D%3D.XAk9%2BRTDP4TRar6mCRDjO4kAwVLapfKGEo7gkz4NVm9zCnHnJKTQfm383Flb1VMO" rel="nofollow" title="https://pan.quark.cn/s/5cf9b8ed4190" target="_blank">https://pan.quark.cn/s/5cf9b8ed4190</a>，把那个叫 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>的文件下载下来了，一般是在“下载”文件夹里。</li></ul><h3><strong>2. 找到 .dmg 文件</strong></h3><ul><li>打开电脑上的  <strong>“访达”（就是屏幕左上角那个像笑脸的图标）</strong> ，然后点左边栏里的  <strong>“下载”</strong> ，找到那个 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>文件，双击它。</li></ul><blockquote>🖱️ 双击后，它会自动打开，通常会看到一个窗口弹出来，里面有一个 <strong>应用程序图标（可能是软件的Logo）</strong> ，还有一个  <strong>“Applications”（应用程序）文件夹的快捷方式</strong>。</blockquote><h3><strong>3. 把软件拖进“应用程序”文件夹</strong></h3><ul><li>看到那个软件图标（比如写着 <strong>MacX Video Converter Pro</strong>的那个），直接用鼠标 <strong>按住它别松手，然后拖到右边那个 “Applications”（应用程序）文件夹图标上</strong>，松开鼠标。</li></ul><blockquote>✅ 这一步相当于把软件“安装”到了你的 Mac 应用程序列表里，跟其他软件放在一起。</blockquote><h3><strong>4. 等复制完成</strong></h3><ul><li>拖过去之后，系统会自动把软件文件复制到“应用程序”里，等进度条走完或者那个小窗口提示完成就行。</li></ul><h3><strong>5. （可选）弹出安装盘</strong></h3><ul><li>复制完成后，你可能会看到那个原来的 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>文件窗口还在，左上角有个小小的  <strong>“弹出”按钮（📀 图标旁边）</strong> ，点一下把它弹出，省得以后误点。</li></ul><h3><strong>6. 打开软件</strong></h3><ul><li>现在去  <strong>“访达” &gt; “应用程序”</strong> 里找到 <strong>MacX Video Converter Pro</strong>，双击打开。</li></ul><blockquote><p>⚠️ 第一次打开时，Mac 可能会弹窗说“这个软件来自不明开发者，不能打开”，别慌：</p><ul><li>去屏幕左上角  <strong>“苹果图标” &gt; “系统设置”（或“系统偏好设置”）&gt; “隐私与安全性”</strong> ，往下拉会看到一条提示说“MacX Video Converter Pro 被阻止”，点旁边的  <strong>“仍要打开”</strong> 就行。</li><li>或者右键点击软件图标，选  <strong>“打开”</strong> ，然后再点确认。</li></ul></blockquote><h3><strong>7. 开始使用</strong></h3><ul><li>打开之后你就可以导入视频，开始转换格式啦！</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[公开免费！Apache Doris & ]]></title>    <link>https://segmentfault.com/a/1190000047412658</link>    <guid>https://segmentfault.com/a/1190000047412658</guid>    <pubDate>2025-11-19 21:03:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>Apache Doris &amp; SelectDB 培训与认证课程上线信息分析</h2><h3>一、课程上线背景</h3><p>随着 Apache Doris 用户群体不断壮大，用户在学习和掌握该数据库过程中面临“缺乏系统性指导”的问题。无论是初次接触的新手，还是希望在特定场景深度应用的用户，均迫切需要一套从0到1、覆盖全面的学习路径。基于此需求，飞轮科技正式推出 Apache Doris &amp; SelectDB 培训与认证体系，旨在帮助用户从理论到实践，逐步成长为实时分析型数据库专家。</p><h3>二、基础课程详情</h3><h4>（一）课程开放方式</h4><p>首批上线的初级课程面向所有用户，且<strong>全部免费开放在 SelectDB 官网</strong>。</p><h4>（二）课程核心内容</h4><table><thead><tr><th>课程模块</th><th>具体学习要点</th></tr></thead><tbody><tr><td>基础概念</td><td>全面理解 Apache Doris 的核心理念与架构</td></tr><tr><td>安装部署</td><td>掌握 Doris 的安装与部署方法，能够快速搭建首个 Doris 环境</td></tr><tr><td>表设计</td><td>学习表结构设计逻辑，掌握在 Doris 中新建表的操作</td></tr><tr><td>数据操作</td><td>掌握 Doris 中数据的导入、更新与删除操作</td></tr><tr><td>数据查询</td><td>了解 Doris 支持的查询类型，具备基础查询能力</td></tr><tr><td>基础运维</td><td>了解日常运维操作流程，保障 Doris 集群稳定运行</td></tr><tr><td>数据湖查询</td><td>了解 Doris 联邦分析能力，为后续进阶学习湖仓一体奠定基础</td></tr></tbody></table><h3>三、初级认证考试信息</h3><h4>（一）认证名称</h4><p>Apache Doris 与 SelectDB 初级认证考试（SCDA）</p><h4>（二）考试核心规则</h4><ol><li><strong>考试形式</strong>：远程考试，需在1小时内完成</li><li><strong>考题设置</strong>：共50道单选题，每题2分，正确率达到60%（即得分≥60分）视为通过认证</li><li><strong>考试频率</strong>：认证考试平均每月开放一场，当前已开放报名与考试</li><li><strong>报名入口</strong>：<a href="https://link.segmentfault.com/?enc=lMTW4jR5qyErcJbfYFkrmw%3D%3D.D3vGarNU1vp5OdC%2BdqqXqjjhU%2FFRnEUizvk9wM0JBz9N3w5y0d8C17LymjffYeWEEephF3NXCftpQHJ6KvlKPQ%3D%3D" rel="nofollow" target="_blank">https://www.selectdb.com/resources/training/scda?examId=1</a></li><li><strong>报考建议</strong>：参加认证无前提条件，但官方强烈建议先完成基础入门培训课程后再参与考试</li></ol><h4>（三）认证证书信息</h4><p><img width="723" height="515" referrerpolicy="no-referrer" src="/img/bVdm6mr" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[AI 招聘系统功能落地指南 爱跑步的香蕉]]></title>    <link>https://segmentfault.com/a/1190000047412675</link>    <guid>https://segmentfault.com/a/1190000047412675</guid>    <pubDate>2025-11-19 21:02:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI 招聘系统功能落地指南<br/>AI全面重塑招聘行业：效率与体验的双重革新<br/>如果你仍在为“候选人爽约”“简历筛选繁重”“面试效率低下”而困扰，可能已经忽视了一个激进的行业趋势：招聘正被AI全面接管。这并非停留在讨论或小范围试点阶段，而是大规模落地、快速替代传统面试流程的现实。<br/>过去一年，多项行业数据印证了这一变革：近50%的企业已将初筛环节完全交给AI；金融、互联网、消费、制造等行业中，约2000家公司将AI面试纳入主流流程；超百万人通过AI面试完成测评、初筛与技术问答；部分银行甚至创下“2小时完成上万人初筛”的行业纪录。<br/>这些数字背后，是AI技术进步速度远超部分企业组织升级速度的残酷现实。当一些团队仍在逐份筛简历、重复提问基础问题时，竞争对手已用AI将初筛流程压缩至“分钟级”；当部分人质疑AI专业性时，不少企业已直接用AI面试官的评分做招聘决策；当担心AI影响候选人体验时，有些企业已将“AI面试”打造成雇主品牌亮点。这道行业分水岭清晰呈现：慢一步不仅是效率落后，更是错失优秀人才的风险。</p><p>01 精准评估：打破传统招聘的主观局限<br/>传统面试的核心痛点在于主观性强，不同面试官、同一面试官在不同时间的判断都可能存在差异。而AI面试系统通过技术优化，实现了评估精准度的大幅提升。<br/>其评分经过多维度验证：在人机对比实验中，与人工评分高度一致；通过效标效度与重测信度双重心理学指标检验，评分稳定且可信；迭代版本的模型升级，进一步提升了评估精度。这使得AI评估结果不仅具备高参考价值，更可直接作为招聘决策依据，让企业摆脱“凭经验”“凭感觉”的招聘困境。<br/>02 核心技术升级：让评估更高效全面<br/>AI面试系统的精准性，体现在招聘各环节的技术突破中：<br/>•一问多能：一道问题可同步评估沟通、逻辑、岗位胜任力等多项维度，既能满足HR初筛需求，也能无缝衔接专业复试，评估效率提升50%以上。<br/>•自由追问：针对候选人模糊的回答，即时生成针对性问题，如同资深面试官般抓住关键信息，避免遗漏核心能力点。<br/>•简历深度挖掘：自动识别简历中的关键经历与疑点，生成递进式提问，既防范候选人过度包装、信息造假，也避免HR因忙碌漏看亮点，导致优秀人才在粗筛环节被淘汰。<br/>•全维度专业适配：既能评估沟通、协作等通用胜任力，也能针对编程、算法、工程、财务等专业岗位精准出题，同时减轻HR与专业面试官的工作负担。<br/>03 拟人化交互：优化候选人面试体验<br/>传统AI面试常因“机械、生硬”引发质疑，而新一代AI面试系统通过拟人化设计，让面试体验成为雇主品牌加分项：<br/>•情绪感知：能够识别候选人的语速、情绪与潜台词，引导紧张的候选人稳定发挥，充分展现自身实力。<br/>•流程顺滑：自动识别回答状态，无需候选人手动点击按钮续答或结束，交互过程如同面对面沟通般自然。<br/>•视觉升级：大幅提升语音与口型同步精度，摆脱传统AI面试的“纸片人感”，增强沉浸式体验。<br/>•即时答疑：支持候选人随时提问职位信息、企业福利等内容，AI精准解答，帮助候选人深入了解企业，提升入职意愿。<br/>04 全流程自动化：重构招聘效率天花板<br/>AI招聘工具已不止于面试环节，更发展为覆盖全流程的自动化系统，实现从简历筛选到入库的一体化执行：<br/>•快速启动：30-60秒即可完成初始化，无需复杂设置，立即自动启动服务。<br/>•智能筛选：根据企业预设的年龄、学历、薪资等条件，自动筛选符合要求的候选人。<br/>•动态沟通：模拟真人语气与候选人进行问答互动，自动回复所有未读消息，避免错过潜在人才。<br/>•资料补全：当候选人信息缺失时，主动索取相关资料，交流方式贴近人工沟通习惯。<br/>•数据同步：自动下载简历并同步至企业ATS系统，生成候选人档案，保障数据流转高效安全。<br/>这类自动化系统将招聘效率提升10-100倍，推动招聘流程从“人工驱动”向“技术驱动”转型，成为企业应对人才竞争的重要支撑。</p>]]></description></item><item>    <title><![CDATA[浩瀚深度：从 ClickHouse 到 ]]></title>    <link>https://segmentfault.com/a/1190000047412679</link>    <guid>https://segmentfault.com/a/1190000047412679</guid>    <pubDate>2025-11-19 21:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>浩瀚深度（[SHA: 688292]）旗下企业级大数据平台选择 Apache Doris 作为核心数据库解决方案，目前已在全国范围内十余个生产环境中稳步运行，<strong>其中最大规模集群部署于 117 个高性能服务器节点，单表原始数据量超 13PB，行数突破 534 万亿，日均导入数据约 145TB，节假日峰值达 158TB，是目前已知国内最大单表</strong>。凭借 Apache Doris 的高可靠、高性能与高可扩展能力，该集群已<strong>持续稳定运行半年以上</strong>，充分验证了其在超大规模数据场景下的卓越表现。</blockquote><p>浩瀚深度作为国内互联网流量解析与数据智能化领域的领军企业，深耕行业三十余载，持续为国内互联网提供高性能、高精度、高可靠的整体解决方案。公司业务覆盖网络可视化、AI 智能、数据治理、数据价值挖掘及安全防护，是一家集软硬件产品研发、生产、销售和服务于一体的大型高科技企业。</p><p>顺水云大数据平台（StreamCloud）作为浩瀚深度自主研发的企业级的大数据平台产品，涵盖了从数据采集、数据存储、数据处理、数据挖掘、数据治理到数据共享的完整数据开发流程。帮助企业客户快速构建 PB 级数据中台，目前已经在通信、金融、交通等领域落地部署 100+ 项目，管理超过 130PB 数据，集群节点规模近万个。</p><p>为满足客户每日写入及查询万亿级增量数据的严苛需求，顺水云对 MPP 数据库产品进行了多轮选型测试，并在实际生产环境中尝试过 Greenplum、ClickHouse 等多个方案。经过综合比对，最终选定 Apache Doris 作为核心数据库解决方案。<strong>目前，该方案已在全国十余个生产环境上线运行，其中规模最大的集群部署于 117 个高性能服务器节点之上，单表原始数据量超 13PB，行数突破 534 万亿，日均导入数据量约 145TB ，节假日峰值数据量约 158TB，且已持续稳定运行半年以上。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412681" alt="平台架构.png" title="平台架构.png"/></p><h2>早期架构以及痛点</h2><p>早期架构如图所示，数据主要来源为用户上网日志，数据经过采集设备解析还原后发送到接口机上，再由接口机上程序接入 HDFS 集群，基于 Apache  Spark 将不同类型话单经过加工、回填、合成等流程处理后生成结果数据，最终写入至 ClickHouse 中，用于日志存储与快速查询、流量质量分析、面向政企市场的用户画像 &amp; 精准营销等场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412682" alt="早期架构以及痛点.png" title="早期架构以及痛点.png" loading="lazy"/></p><p>随着业务数据体量逐渐庞大，对于高吞吐的数据写入、亿级数据的秒级响应、海量数据关联查询的需求也愈加迫切，以 ClickHouse 为核心的 OLAP 查询分析引擎体系在使用过程中对业务人员开发、运维人员维护存在如下痛点：</p><ol><li><strong>写入稳定性差且存储成本较高</strong>：为降低存储成本，我们尝试使用 ZSTD 压缩格式，但因其高压缩比带来的性能开销，频繁出现“too many parts”及当日数据入库积压问题。为保障业务稳定运行，只能增加存储成本使用默认的 LZ4 压缩；</li><li><strong>运维成本高</strong>：使用自研的数据入库工具，由于不同接口机上数据量和导入性能差异，导致 ClickHouse 集群上各节点数据量不均衡，由于 ClickHouse 架构特性，坏盘时数据无法自动迁移，需要人工持续干预；</li><li><strong>并发查询能力不足</strong>：在并发查询较多场景下，查询性能下降明显，无法支持业务需求；</li><li><strong>JOIN 能力不足</strong>：由于 ClickHouse 自身组件设计无法支持多表或大表 Join 查询场景，难以满足大表关联查询需求。</li></ol><h2>Apache Doris vs.  ClickHouse 对比测试</h2><h3>测试准备</h3><p>为了进一步对比验证 Doris 的写入和查询性能，我们使用了三台物理机模拟生产环境数据和业务对 Apache Doris 和 ClickHouse 做了一系列对比测试。主要分为以下 3 部分：</p><ul><li>前缀索引测试对比</li></ul><blockquote>查看<a href="https://link.segmentfault.com/?enc=f%2BA98PHZyhw7nt%2BLfQo7%2FQ%3D%3D.RDadRKs%2BI%2FyRRGlp%2Bg5mtTv0jloYvN9rCYly6TsJwFoLjVL2N1%2FFegnfIyMe33uoYKp45%2FMhKsB95uGi3UuFDdi3J8N5RgElbgNHctLBLMM%3D" rel="nofollow" target="_blank">前缀索引文档</a>详情</blockquote><ul><li>二级索引测试对比</li></ul><blockquote><p>查看 <a href="https://link.segmentfault.com/?enc=6a%2FYPgNo3kxXFQ6Pg3gQdg%3D%3D.tL5IBf8enZVaDf1f46gwPpWHdTIy3ibeayZmUE1Of4hYqIWqh5DRYSTwAHsRgniMqJ6xLMlXThQzh14YpVlrbKUbEou4P3q8Awjj6Sm1ZaA%3D" rel="nofollow" target="_blank">BloomFilter Index 文档</a>详情</p><p>查看 <a href="https://link.segmentfault.com/?enc=OZw%2B1QKNuWrnFU3kNOwFpQ%3D%3D.Xw3YuZCo9DWyCxWOM7RMXghOe208LMPzVnrMYF1n7PUz76Wr3PtD2EtqwFgBT2brz%2F0yaMe9mYMZOZFCyOI7s5xRAEZVaMhERb5kgZu9w8o%3D" rel="nofollow" target="_blank">Inverted Index 文档</a>详情</p></blockquote><ul><li><p>全表扫描测试对比</p><p><strong>测试参数：</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412683" alt="全表扫描测试对比-1.png" title="全表扫描测试对比-1.png" loading="lazy"/></p><p><strong>建表：</strong></p><p>Doris 自 2.0.0 版本支持倒排索引能力，因此我们在表的数据量和字段一致的背景下，针对不同的排序键与索引类型进行查询速度测试：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412684" alt="全表扫描测试对比-2.png" title="全表扫描测试对比-2.png" loading="lazy"/></p></li></ul><h3>测试一：前缀索引</h3><p>前缀索引可以加速等值查询和范围查询。在 Doris 中，建表时自动取表的 Key 的前 36 个字节作为前缀索引。前缀索引测试分为 3 次冷查询，测试结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412685" alt="测试一：前缀索引.png" title="测试一：前缀索引.png" loading="lazy"/></p><h3>测试二：二级索引</h3><ul><li>BloomFilter Index 是基于 BloomFilter 的一种跳数索引，其原理是利用 BloomFilter 跳过等值查询指定条件不满足的数据块，达到减少 I/O 查询加速的目的。</li><li>Inverted Index 可以用来进行文本类型的全文检索、普通数值日期类型的等值范围查询，快速从海量数据中过滤出满足条件的行。</li></ul><p>二级索引分别对比测试了 Doris 的<a href="https://link.segmentfault.com/?enc=W46CJyaBSbXjOs7CRn3M4w%3D%3D.7M4xDfaUIcdHyvUAlBIpYz3Q4AlLtQzUuFUFJ8EjOLY1A4cGirlvWSiDXMnXV%2BM8jER3wa8QY%2F5J7md8x8ACC%2BHG1RjW2vjmrUyCGKaAXEeQfpHZ%2FB0uRfUE%2FLIBPZLxAp76%2FnU2g1vNdbpHNu7QNw%3D%3D" rel="nofollow" target="_blank"> BloomFilter Index</a> 和 <a href="https://link.segmentfault.com/?enc=ZxnHW1kuqeSKfUw%2BYahgFg%3D%3D.h7neGmgDGmQZlyrtXFwCe7xf7fLeceNktnHOWjUJ3bRmChHLSoaqeElAffQ6Aj8SfZerakeUx9uthsYv0lEUkrwOKbXxQah%2FssKZNkHc%2FNlRgTDvWF%2FGhRV%2FugWaiQIubgK4sksbPpf9cLpXCYoSaQ%3D%3D" rel="nofollow" target="_blank">Inverted Index</a> ，测试分为 3 次冷查询，测试结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412686" alt="测试二：二级索引.png" title="测试二：二级索引.png" loading="lazy"/></p><h3>测试三：全表扫描</h3><p>全表扫描测试主要是测试了常用业务查询场景：like 模糊查询和 IP 函数，根据测试结果， <code>IS_IP_ADDRESS_IN_RANGE</code> 函数查询方面 ClickHouse 略胜。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412687" alt="测试三：全表扫描.png" title="测试三：全表扫描.png" loading="lazy"/></p><h3>结果分析</h3><p>在合理配置索引的前提下，Doris 在关键查询场景下展现出显著性能优势：</p><ul><li><strong>前缀索引：</strong> Doris  查询速度是 ClickHouse 的 2 倍以上。</li><li><p><strong>二级索引：</strong></p><ul><li>使用 BloomFilter 索引时，Doris 查询速度领先 ClickHouse 达 2 倍。</li><li>相同场景下 Doris 的倒排索引功能，使得查询性能更是大幅跃升，速度远超 ClickHouse，是其性能的 5 倍以上。</li></ul></li><li><strong>全表扫描：</strong> 两者性能接近，ClickHouse 在特定函数调用上略占优势。</li></ul><p>综合来看，两者各有所长，但测试表明：在常用业务查询场景中，Doris 的前缀索引、BloomFilter 和倒排索引性能全面优于 ClickHouse。据此评估，迁移至 Doris 后，查询响应速度预计提升超 2 倍。</p><h2>Doris 替换实践</h2><p>由于 ClickHouse 和 Doris 均为 MPP 架构数据库，且 Doris 支持 MySQL 语法，因此架构变化小、迁移便捷。仅需将日志存储与分析引擎由 ClickHouse 替换为 Doris，具体步骤如下：</p><ul><li>调整上游 Importer 写入组件的配置，使其将日志数据直接写入 Doris 表；</li><li>更新下游查询服务的 SQL 语句以适配 Doris 语法即可完成无缝迁移。</li></ul><p>虽然我们对 Doris 做了几 TB 的数据测试做使用参考，但考虑生产环境日增数百 TB 的数据量级，再加上引入新组件的不确定性，实施初期我们采用了 ClickHouse 和 Doris 并行运行的方式。</p><p>同时在压测期间也遇到一些问题，目前均已解决，我们将这些问题的解决思路整理并分享至社区，以供参考。</p><h3>实践一：解决大批量写入报错问题</h3><p>在进行导入压力测试阶段，小数据量下，集群运行状况和导入性能均表现良好。但是在逐渐上升至 30 台接口机同时并发导入时，系统逐渐出现一些异常情况。具体如下：</p><pre><code class="Plain">org.apache.doris.common.UserException: errCode = 2, detailMessage = get tableList write lock timeout, tableList=(Table [id=885211, name=td_home_dist, type=OLAP])
 
[INTERNAL_ERROR]cancelled: tablet error: tablet writer write failed, tablet_id=24484509, txn_id=3078341, err=[E-216]try migration lock failed, host: ***</code></pre><p>在出现异常情况后，我们及时和社区同学联系沟通，参考<a href="https://link.segmentfault.com/?enc=Wuh%2B%2BwDf78zzr12a08nrvg%3D%3D.%2BC0MfmMBsrAoxmZDOMx4qJZYqX4eg0FnD7v7uDDJxzv5y1U8wAtjA36USNDIruqgqyd%2BdWWw9kiz2r7R67FoTg%3D%3D" rel="nofollow" target="_blank">官方文档</a>中的《日志存储和分析》模块的参数进行调优后，导入任务恢复正常。</p><h3>实践二：Compaction 压力过载优化</h3><p>进一步压测，我们发现在导入持续一段时间后，BE 节点在监控中出现异常。结合监控以及 <code>top -H</code> 的输出，发现 Compaction 占用 CPU 资源比较多，导致 BE 出现假死。经过 Compaction 问题排查，发现与 Bucket 数量有关。</p><p>由于当时按照 ClickHouse 自动合并后单个分区下文件数量的最大值来设置， Bucket 数量我们设为了 480，导致 Tablet 过多而引发问题。</p><p>后来结合实际业务场景，以及业务高峰期数据量等因素进行了计算后，将 Bucket 缩减为 280 ，调整后 Compaction 资源占用恢复正常 ，BE 节点恢复平稳运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412688" alt="实践二：Compaction 压力过载优化.png" title="实践二：Compaction 压力过载优化.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412689" alt="实践二：Compaction 压力过载优化-2.png" title="实践二：Compaction 压力过载优化-2.png" loading="lazy"/></p><h3>实践三：导入异常问题排查</h3><p>在压测期间导入出现一个事物卡住的情况，异常如下：</p><pre><code class="Plain">errCode = 2, detailMessage = current running txns on db 10194 is 10000, larger than limit 10000</code></pre><p>根据报错内容引导处理，我们调整了单个 DB 下的事务数量，发现不是根本原因。随后，我们将问题反馈给社区同学，在他们的协助下，很快定位到了问题根源。具体定位步骤如下：</p><ol><li>找到一个具体的事务 ID</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412690" alt="实践三：导入异常问题排查.png" title="实践三：导入异常问题排查.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412691" alt="实践三：导入异常问题排查-2.png" title="实践三：导入异常问题排查-2.png" loading="lazy"/></p><ol start="2"><li><p>登录 FE 的主节点，搜索事务对应的具体 BE</p><pre><code class="Plain">grep "16788479" /opt/install/doris-2.15/fe/log/fe.log</code></pre></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412692" alt="实践三：导入异常问题排查-3.png" title="实践三：导入异常问题排查-3.png" loading="lazy"/></p><ol start="3"><li><p>登录到具体 BE，在日志中搜索该事务 ID</p><pre><code class="Plain">grep "16788479" /opt/install/doris-2.15/be/be.INFO</code></pre></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412693" alt="实践三：导入异常问题排查-4.png" title="实践三：导入异常问题排查-4.png" loading="lazy"/></p><p>排查发现，事故起因是某台机器磁盘写满，导致集群开始将该节点的写入和副本调度至其他节点，而当前写入压力较大，进而引发了事务积压。此外，由于磁盘上仍残留部分 ClickHouse 数据，进一步加剧了磁盘空间分布不均的问题。</p><h3>实践四：使用 Broker Load 解放接口机</h3><p>由于 Doris 中的导入功能非常丰富，几乎在每个场景都有对应的导入，刚开始我们使用的是 Stream Load 导入本地数据文件，在进一步学习了解使用 Doris 后，发现 Broker Load 的导入方式更契合业务场景，因为系统加工合成的数据本身就存储在 HDFS 上，Broker Load 可以直接从 HDFS 上拉取数据，相比较之前从 HDFS 下载数据到本地再进行导入方式，这样不仅能减少一次数据传输，还解放了数据导入使用的接口机，进一步提高了效率。我们需要做的就是申请 Doris 集群和 HDFS 集群的网络打通和 Broker 的部署，并编写一套检测数据并提交  Broker Load 的脚本。</p><p>经测试，目前已经上线上百台的 Broker 节点去进行并行拉取 HDFS 的数据并进行导入，写入性能优异。但使用 Broker Load 时要注意按照业务场景进行调整，如果默认子任务失败则会导致整个目录的文件导入全部失败。HDFS 中数据以 15 分钟粒度为目录存储的，需要做好相应的失败检测与重试机制。</p><p>在机器成本方面，相较于使用 ClickHouse 导入一天数据需要 32 台接口机，改用 Doris 后，省去了从 HDFS 拉取数据的机器，仅需 23 台即可完成同等数据量的导入，<strong>机器资源节省超过 28%</strong>，显著降低了成本并提效。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412694" alt="实践四：使用 Borker Load 解放接口机-1.png" title="实践四：使用 Borker Load 解放接口机-1.png" loading="lazy"/></p><h2>架构升级成果</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412695" alt="架构升级成果.png" title="架构升级成果.png" loading="lazy"/></p><p>目前，浩瀚深度已在某运营商客户环境使用 Doris 替换 ClickHouse 构建了新的查询分析平台，服务器规模超百台，并实现日增数据量峰值近 158TB 的数据导入。采用双副本 + 倒排索引 + ZSTD 压缩后，存储量约 6.5PB，和原始数据相比，Doris 中单个副本的压缩率在 4 倍左右，且目前已稳定运行半年多，这次升级带来查询响应、并发能力、稳定性和运维效率等多方面的收益，成果显著。</p><ul><li><strong>显著降低硬件资源成本</strong>：利用 Doris Broker Load 高效导入机制，释放原 ClickHouse 所需的 32 台专用接口机，这些资源可灵活用于计算或存储，整体硬件成本节超 28%。采用 ZSTD 高压缩比格式，在未出现写入瓶颈的前提下，存储资源消耗相较 ClickHouse（LZ4 压缩）降低了 6%。</li><li><strong>大幅提升查询效率</strong>：Doris 卓越的索引优化（前缀索引、Bloom Filter、倒排索引）及多表 JOIN 性能全面超越 ClickHouse。单 SQL 查询响应速度提升近 2 倍。批量查询任务执行效率提升近 30%。</li><li><strong>有效降低运维复杂度与成本</strong>：服务器宕机或坏盘时，Doris 自动完成副本切换与写入重定向，保障服务连续性。集群扩缩容时，Doris 自动实现 Tablet 均衡分布，快速恢复集群负载均衡。通过 Doris 原生 Web UI 与 Grafana 监控，异常节点与磁盘故障可被快速定位。</li></ul><h2>未来规划</h2><p>未来，浩瀚深度将从以下方面重点发展：</p><ul><li><strong>持续深化Doris 的湖仓一体化应用</strong>：通过 Doris 的 Hive Catalog 功能整合数据仓库资源，统一数据访问接口，实现对全量数据的统一查询与分析；</li><li><strong>复杂查询加速</strong>：在多维度分析、聚合计算等复杂查询场景下，依托 Doris 强大的整合能力提升查询效率，加速报表生成；</li><li><strong>成本优化</strong>：利用 Doris 的冷热数据分层存储等特性，在持续优化查询性能的同时，进一步降低总体存储成本。</li></ul><p>最后，衷心感谢飞轮科技技术团队与 Doris 社区对浩瀚深度的持续、专业的技术支持，有力推动了我们的国产化架构转型进程。 我们热忱期待更多同仁加入 Apache Doris 的应用实践与社区贡献行列，共同丰富其功能生态、扩展函数支持，助力 Apache Doris 在全球 MPP 数据库领域绽放璀璨光芒！</p>]]></description></item><item>    <title><![CDATA[AI 语音转贴纸，儿童打印机 Stick]]></title>    <link>https://segmentfault.com/a/1190000047412518</link>    <guid>https://segmentfault.com/a/1190000047412518</guid>    <pubDate>2025-11-19 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412520" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、Spatial AI 发布第一人称真实世界数据集，教会机器人在现实世界完成任务</strong></p><p>Spatial AI 发布的首个开源数据集 SEA（Spatial Everyday Activities，空间日常活动）是迄今为止规模最大的、经过精心策划的以第一人称视角记录的、人们执行真实任务的数据集，数据量高达 10,000 小时。</p><p>Huggingface: <br/><a href="https://link.segmentfault.com/?enc=pxU71wtN3XeQen0Pf6egVQ%3D%3D.kOdMSZiqikGmVpMvjzJtBnsl%2FGvsew6IhURtfgc9bU0YskAr0pqJ8wL8GnReCq%2BtajDLx3k4Oks4xkGeK5L%2F4g%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/datasets/spatial-ai/sea-small</a></p><p>官网：<br/><a href="https://link.segmentfault.com/?enc=MTEVuYM93RnffLZYv6dJAw%3D%3D.WJqtznKltJ95rXpIC21nt2qnxjQDa8h%2BdNjMEDxVpwM%3D" rel="nofollow" target="_blank">https://spatial-ai.com/</a></p><p>（@ycombinator\@X）</p><p><strong>2、LiveKit 重磅推出三大新功能，赋能开发者构建更卓越的语音智能体</strong></p><p>LiveKit 在 Dev Day 活动宣布上线三项革命性新功能——Agent Builder、Phone Numbers 和 Agent Observability，旨在极大地简化语音智能体的开发、部署与调试流程，助力开发者以前所未有的速度构建和优化更智能、更可靠的语音应用。</p><p><strong>1. Agent Builder：3 分钟内构建生产级语音智能体</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412521" alt="" title="" loading="lazy"/></p><p>全新的 Agent Builder 是一款基于浏览器的无代码/低代码平台，让开发者能够快速创建、测试并部署语音智能体。用户只需提供系统提示（system prompt），即可在几分钟内启动语音智能体的构建。该平台集成了背景降噪、多语言识别等内置语音处理能力，并支持通过 HTTP 工具轻松扩展功能，连接 CRM、API 等外部服务。Agent Builder 提供内置的实时测试与一键部署功能，并集成了 Deepgram、AssemblyAI、GPT-4.1、Gemini 2.5 Flash 等多家知名供应商的模型，让开发者可以在内测阶段自由对比模型性能。最重要的是，Agent Builder 被设计为从原型到代码的桥梁，支持导出代码，实现从低代码到专业 SDK 开发的平滑过渡，大幅缩短语音 AI 应用的上市时间。</p><p><strong>2. Phone Numbers：60 秒内接入电话网络</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412522" alt="" title="" loading="lazy"/></p><p>LiveKit Phone Numbers 是一项全新的首方电话服务，开发者无需第三方 SIP Trunk 即可在数分钟内将语音智能体直接连接到电话网络。用户可在 LiveKit Cloud 仪表盘或 CLI 中直接购买美国本地或免费电话号码，并快速关联到语音智能体，实现「零配置」上线。此举消除了中间环节，减少了网络延迟和故障点，显著提升了通话质量、音频清晰度和语音转文本（STT）准确率，进而优化了整体语音交互体验。这项服务尤其适用于需要电话交互的行业，可快速搭建 24/7 咨询、订单查询等语音智能体。</p><p><strong>3. Agent Observability：统一视图助力语音智能体故障排查</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412523" alt="" title="" loading="lazy"/></p><p>Agent Observability Beta 版本现已集成于 LiveKit Cloud Dashboard，旨在解决语音智能体开发与部署中的棘手故障排查问题。用户现在可以在单一视图下，同步查看会话的音频、转录、逐轮智能体调用（LLM、TTS、转折点检测、工具调用等）以及日志。这种端到端的可见性，使得开发者能够像「听」和「看」一样，快速定位导致响应延迟、中断或任务失败的根本原因，极大地简化了以往需要在不同服务间手动关联日志和时间戳的复杂过程。会话录制功能支持可选启用，并提供灵活的数据控制选项。</p><p>( @LiveKit 官网)</p><p><strong>3、Google 正式发布 Gemini 3</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412524" alt="" title="" loading="lazy"/></p><p>今天凌晨，<strong>Google DeepMind 正式发布新一代旗舰模型 Gemini 3</strong>，号称拥有最先进的推理能力、世界领先的多模态理解能力，并支持新的智能编码体验。<strong>本次首发版本为 Gemini 3 Pro</strong>，并且即日起开始全球范围内推出，亮点如下：</p><ul><li>Gemini 3 Pro 预览版原生多模态支持（文字、图像、视频、音频）</li><li>在 LMArena 排行榜登顶，在推理、多模态、编程等主流测试中全面领先</li><li>推理能力创纪录（GPQA Diamond 91.9%、MathArena Apex 23.4%）</li><li>提供 Deep Think 深度思考模式（未来几周开放）</li><li>100 万 token 上下文窗口 + 64K 输出</li><li>推出全新 AI IDE：Google Antigravity，新模型已集成 Cursor、GitHub、JetBrains 等工具</li></ul><p>值得一提的是， 推理能力方面， <strong>Gemini 3 Pro 在「人类最后的考试（Humanity’s Last Exam）」中拿到了 37.5% 的博士级推理成绩。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412525" alt="" title="" loading="lazy"/></p><p><strong>有趣的是，OpenAI CEO Sam Altman 刚刚也在 X 平台发文称：「恭喜 Google 成功推出 Gemini 3！看起来是个很棒的模型。」而马斯克则是表示「做得好」。</strong></p><p>( @APPSO)</p><hr/><h2>02 有亮点的产品</h2><p><strong>1、Hapiko 获 700 万美元融资，推出儿童 AI 语音转贴纸打印机 Stickerbox</strong></p><p>总部位于布鲁克林的创意科技公司 Hapiko 宣布获得 700 万美元融资，并正式发布了其首款产品「Stickerbox」。这款 AI 驱动的语音转贴纸打印机专为儿童设计，允许他们通过语音描述图像，然后打印出可供涂鸦和定制的黑白贴纸，旨在鼓励安全、具象的创意玩乐，而非被动屏幕时间。</p><ul><li><strong>AI 语音转贴纸：</strong> 「Stickerbox」是首款能让儿童通过语音（如「骑着滑板的恐龙」）生成图像并打印成黑白贴纸的设备。</li><li><strong>鼓励具象化创意：</strong> 产品旨在引导儿童从被动接受信息转向主动、开放式的想象和创造，将口头想法转化为实体贴纸。</li><li><strong>安全与隐私优先：</strong> 采用墨水无关的热敏打印技术，使用不含 BPA 和 BPS 的纸张，无摄像头，无持续数据收集，AI 系统从零开始设计，内置年龄适宜性过滤。</li><li><strong>独立玩乐体验：</strong> 无需智能手机或笔记本电脑即可独立操作，支持儿童动手实践的玩乐方式。</li><li><strong>700 万美元融资：</strong> 本轮融资由 Maveron 和 Serena Ventures（由网球明星 Serena Williams 创立）领投，将用于扩大生产和拓展儿童创意玩具及 AI 市场。</li></ul><p>「Stickerbox」现已在美国上市，可通过 stickerbox.com 购买，零售价为 99.99 美元。Hapiko 计划利用此次融资扩大生产规模并拓展市场。</p><p>(@Pulse 2.0)</p><p><strong>2、连锁餐饮正在用炒菜机器人取代预制菜</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412526" alt="" title="" loading="lazy"/></p><p>据 36 氪报道，连锁餐饮品牌正加速引入炒菜机器人，以应对预制菜风波带来的行业挑战。</p><p>报道指出，老乡鸡、小菜园等餐企已在数百家门店部署智能烹饪设备，歌尔、富士康等大厂也开始在食堂场景中使用机器人，以提升出餐效率并降低人力成本。</p><p>炒菜机器人通过精准控温、自动搅拌与智能投料系统，实现标准化作业，既保留中餐「锅气」风味，又显著提高出餐速度。</p><p>烹饪机器人企业智谷天厨 CEO 耿凯平透露，过去一个月咨询需求增长近 10 倍，显示行业对智能化厨房的接受度快速提升。</p><p><strong>市场数据显示，一台售价约 6 万元的中型炒菜机器人，使用寿命可达 8 至 10 年，月均成本约 600 元。</strong>相比传统厨师每月 8000 至 15000 元的薪资，企业可在生命周期内节省数十万元至百万元不等的运营开支。</p><p>在企业团餐场景中，机器人还配备智能调度系统，可实时监测菜品余量与人流分布，动态补餐，避免食材浪费。</p><p>据实测，其浪费率可降低 70% 以上。业内人士指出，随着消费者对预制菜的抵触情绪加剧，机器人现炒成为餐企兼顾效率与品质的更优解。</p><p>( @APPSO)</p><p><strong>3、曝 Rabbit 公司停薪数月，官方坚称 Rabbit R1 下一代版本将面世</strong></p><p>11 月 18 日消息，据外媒报道，最近 Rabbit 因另一件事又被推上了风口浪尖：多名 Rabbit 员工称公司已连续数月拖欠工资，部分员工甚至从 10 月起直接罢工——可就在这个节骨眼上，Rabbit CEO 吕骋（Jesse Lyu）仍对外声称「计划在 2026 年推出下一代 AI 硬件」。</p><p>据报道，今年 1 月起，Rabbit 公司就开始出现发薪延迟的问题。原本每月仅是晚几天，可后来越来越严重：最严重的一次延迟了整整 37 天才发薪。到了 7 月，Rabbit 部分员工与外包人员彻底「停薪」，再没收到任何工资。10 月初，三名员工因长期未领到薪水而发起罢工，并一直持续到现在。更戏剧性的是，Rabbit 并未否认这一点，其发言人确认罢工的确存在，并强调：「公司共有 26 名员工，目前只有 3 人在罢工，我们完全尊重他们的决定。」</p><p>此前报道，Rabbit 公司在 2024 年推出了「口袋 AI 设备」Rabbit R1，当时官方宣称要让这款设备成为用户和智能手机的交互中介，并在开售 5 天后卖出 50000 台。不过首发用户上手后却发现，这款设备的实际表现远逊于发布会上宣称的效果，例如在实景拍摄过程中至少要等 20 秒才会回应，「比 Siri 还慢」，甚至有用户指出这款设备就是一个低端安卓手机装了一个 APK 文件。</p><p>（@IT 之家、CSDN、雷锋网）</p><h2>03 有态度的观点</h2><p><strong>1、Take-Two Interactive CEO 认为游戏行业正转向 PC</strong></p><p>Take-Two Interactive CEO Strauss Zelnick 在接受采访时表示，游戏行业正转向 PC 从封闭转向开放，但游戏机作为一种体验并不会消失。Zelnick 表示，游戏机和手游的市场份额产不多，但手游增长速度比游戏机更快。索尼 PS 和任天堂 Switch 的游戏机业务取得了成功，而竞争对手拥有 Xbox 的微软则暗示下一代硬件将更面向 PC 游戏。Valve 最近宣布的 Steam Machin 就是游戏机和 PC 的混合设备。</p><p>(@Solidot)</p><h2>04 社区黑板报</h2><p>招聘、项目分享、求助……任何你想和社区分享的信息，请联系我们投稿。（加微信 creators2022，备注「社区黑板报」）</p><p><strong>1、招聘实习生丨加入我们，共建 RTE 开发者社区</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412527" alt="" title="" loading="lazy"/></p><p><strong>RTE 开发者社区·运营实习生（实时互动 / Voice AI 方向，本招聘长期有效）</strong></p><p><strong>地点：北京·朝阳区望京南/上海·杨浦区五角场</strong></p><p><strong>这份实习将给你带来：</strong></p><p><strong>产品与技术成长：</strong> 深入学习垂类 AI 产品从技术到落地的全生命周期，构建全面的产品视角。</p><p><strong>社区运营实战：</strong> 与高潜力的开发者和创业者深度交流，共同探索行业前沿；并亲身体验顶级 AI 大会，拓展行业视野。</p><p><strong>【你的职责】</strong></p><ol><li><strong>Voice AI / RTE 情报官：</strong> 每日关注 Voice AI /实时互动领域的最新动态，提炼整理并分享行业洞察，定期撰写学习笔记，帮助团队和社区保持信息前沿。</li><li><strong>社区连接者：</strong> 负责 RTE 领域开发者、初创企业等核心群体的社群运营，主动建立并深化联系，鼓励并协助他们融入社区，共同维护社区的活力与生态。</li><li><strong>活动协作者：</strong> 深度参与 RTE Open Day、Meetup、Dev Talk 等线上线下活动的全流程运营，包括前期策划、中期执行、后期复盘，从实践中提升组织和协调能力。</li><li><strong>行业洞察者：</strong> 协助开展 RTE 相关行业及应用场景调研、产品竞争力分析，整理相关资料，形成对业务的深入理解和独到见解。</li></ol><p><strong>【希望你】</strong></p><ol><li>本科及以上学历，商业、技术、产品、媒体专业或经验背景优先，具备良好英文能力；</li><li>对 RTE / Voice AI 有浓厚兴趣和求知欲；具备优秀的信息收集与整合能力，乐于快速学习新事物，并具备严谨的逻辑思维。</li><li>能保证每周至少 4 天的工作时间，持续 3 个月以上。</li></ol><p><strong>【薪资】</strong></p><p>180-220 元/天</p><p><strong>【投递方式】</strong></p><p>实习地点北京或上海，请将简历发送至 rtedevcommunity\@gmail.com ；邮件标题请注明：【社区运营实习-姓名-学校-毕业年份-到岗日期-城市】</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412528" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412529" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=l3LGwWW8YkDNy1EuLIdY%2BQ%3D%3D.Q8klS8VDVqPLUfEX2OU9hHiUL8%2BzQVIwwcoul1qoKyc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412530" alt="" title="" loading="lazy"/><br/>素材来源官方媒体/网络新闻</p>]]></description></item><item>    <title><![CDATA[Python梯度提升树GBT、随机森林、]]></title>    <link>https://segmentfault.com/a/1190000047412108</link>    <guid>https://segmentfault.com/a/1190000047412108</guid>    <pubDate>2025-11-19 19:05:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>**全文链接：<a href="https://link.segmentfault.com/?enc=bEqhS7nHT81L2B%2FYrCSjNw%3D%3D.0hhfJiiOXgZwPbggvN8RGopizPhddqFxCIokZgF0KiM%3D" rel="nofollow" title="https://tecdat.cn/?p=44342" target="_blank">https://tecdat.cn/?p=44342</a>  <br/>原文出处：拓端数据部落公众号  <br/>分析师：Liping Xiao**</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412110" alt="封面" title="封面"/></p><h3><a name="t0" target="_blank"/>引言</h3><p>随着国内房地产市场进入精细化发展阶段，二手房交易已成为楼市流通的核心组成部分，购房者、投资者及行业从业者对市场动态与价格趋势的精准把握需求日益迫切。作为数据科学家，我们始终相信“数据驱动决策”的核心价值——从海量房产信息中挖掘规律，既能为普通购房者提供理性参考，也能为行业调控提供数据支撑。  <br/>本文内容改编自我们为某房地产咨询机构完成的实际项目，团队通过链家平台爬取多城市二手房数据，构建了一套从数据采集到模型落地的完整解决方案，已在实际业务中验证有效性。文章将详细拆解“数据爬取-清洗-分析-建模-优化”的全流程：先通过Python爬取三个城市各区二手房核心信息，经数据预处理后开展多维度可视化分析，再利用决策树（DT）、梯度提升树（GBT）、随机森林（RF）三种模型实现价格预测，最后通过异常值处理与网格搜索优化模型性能。  <br/>本文内容源自过往项目技术沉淀与已通过实际业务校验，该项目完整代码与数据已分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂怎么做，也懂为什么这么做；遇代码运行问题，更能享24小时调试支持。</p><p>数据爬取</p><p>数据清洗</p><p>探索式数据分析</p><p>特征工程</p><p>模型训练 DT/GBT/RF</p><p>模型优化 异常值处理+网格搜索</p><p>结果输出 价格预测+区域洞察</p><h3><a name="t1" target="_blank"/>项目背景与目标</h3><h4><a name="t2" target="_blank"/>项目背景</h4><p>在居住品质升级与楼市结构调整的双重驱动下，二手房市场的区域差异、价格影响因素愈发复杂。不同城市、同一城市不同区域的房价受地理位置、房屋属性、建筑年代等多重因素影响，传统经验判断已难以适应市场变化。基于此，我们聚焦三个代表性城市，通过数据挖掘技术构建分析与预测体系，填补市场洞察的精准度缺口。</p><h4><a name="t3" target="_blank"/>项目目标</h4><ol><li>数据爬取：通过Python requests库获取链家平台二手房核心字段，包括房源位置、面积、户型、总价、单价等关键信息。</li><li>数据清洗：完成去重、缺失值填充、异常值处理、数据类型转换等预处理，保障数据质量。</li><li>数据可视化：通过直方图、词云图、地理分布图等工具，直观呈现市场分布与价格特征。</li><li>特征工程：筛选并编码对房价有显著影响的特征，构建高效建模数据集。</li><li>模型训练：基于DT、GBT、RF三种算法构建价格预测模型，评估不同因素对房价的影响。</li><li>模型优化：通过异常值剔除与网格搜索调整参数，提升模型预测精度。</li><li>技术实现：依托Python生态，结合pandas、numpy、matplotlib等库完成全流程开发。</li></ol><h3><a name="t4" target="_blank"/>数据采集与预处理</h3><h4><a name="t5" target="_blank"/>数据采集</h4><p>本次数据来源于链家网二手房板块，通过分析网页URL规律，设计了分城市、分区的爬虫方案。先爬取目标城市各区域的URL，再按分页规则遍历每一页房源，最后下载单套房源的HTML文件并提取信息。  <br/>核心爬虫代码（修改后关键片段）：</p><pre><code>import requestsimport osfrom lxml import etreeimport pandas as pd# 定义爬取函数：参数为城市URL和城市名称def crawl_city_houses(city_url, city_name): # 创建城市对应的存储文件夹 city_dir = f'htmls/{city_name}' if not os.path.exists(city_dir): os.makedirs(city_dir) # 请求城市首页，获取各区链接 response = requests.get(city_url, headers=headers) html = etree.HTML(response.text) area_urls = html.xpath('//div[@data-role="ershoufang"]//a/@href') area_names = html.xpath('//div[@data-role="ershoufang"]//a/text()') # 遍历各区，爬取分页数据 for area_url, area_name in zip(area_urls, area_names): area_dir = f'{city_dir}/{area_name}' if not os.path.exists(area_dir): os.makedirs(area_dir)</code></pre><p>爬取流程与代码示意：  </p><p>信息提取代码示意：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412111" alt="" title="" loading="lazy"/></p><h4><a name="t6" target="_blank"/>数据清洗</h4><p>三个城市的原始数据采用相同提取逻辑，因此清洗流程保持一致，以下以赣州数据为例说明核心步骤：</p><ol><li>读取数据并查看基础信息：  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412112" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412113" alt="" title="" loading="lazy"/></li><li>缺失值处理：房屋特色等描述型字段用“无”填充，数值型字段根据分布特征填充均值或中位数。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412114" alt="" title="" loading="lazy"/></li><li>数据格式优化：去除字符型数据中的空格与换行符，删除冗余字段（如“所在区域”因已有“所在市”“所在县区”可剔除）。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412115" alt="" title="" loading="lazy"/></li><li>特征拆分与提取：</li></ol><ul><li>从“面积”字段中提取数字，转换为浮点型用于建模。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412116" alt="" title="" loading="lazy"/></li><li>将“楼层”字段拆分为“楼层高度”（低/中/高）和“总层数”两个独立特征。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412117" alt="" title="" loading="lazy"/></li><li>从“户型”字段中提取“室、厅、厨、卫”的数量，转换为整型特征。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412118" alt="" title="" loading="lazy"/></li><li>简化“抵押信息”字段，归类为“有抵押、无抵押、暂无数据”三类。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412119" alt="" title="" loading="lazy"/></li></ul><h3><a name="t7" target="_blank"/>探索式数据分析</h3><p>通过可视化工具从区域分布、价格特征、房源属性等维度解析数据，以下为三个城市的核心分析结果：</p><h4><a name="t8" target="_blank"/>赣州数据可视化</h4><ul><li>各县区房源数量分布：直观呈现不同区域的房源流通活跃度。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412120" alt="" title="" loading="lazy"/></li><li>房源相关特征分布：展示房屋属性的整体特征，为后续特征筛选提供依据。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412121" alt="" title="" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412122" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412123" alt="" title="" loading="lazy"/></li><li>房价分布直方图：因高价房源占比极低，分高低价（以1000万为界）分别展示，更清晰呈现价格集中区间。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412124" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412125" alt="" title="" loading="lazy"/></li><li>房源标题词云图：提炼市场热门宣传关键词，反映购房者关注焦点，词云图生成代码如下。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412126" alt="" title="" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412127" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412128" alt="" title="" loading="lazy"/></li></ul><hr/><p><strong>相关文章</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412129" alt="" title="" loading="lazy"/></p><h3><a name="t9" target="_blank"/>Python电影票房预测模型研究——贝叶斯岭回归Ridge、决策树、Adaboost、KNN分析猫眼豆瓣数据</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=DwbQnU43ICiZhGbu4WcX7A%3D%3D.HrEHfY21Ha0MDeWYx2z5BHbmcQw6k7mBxYdWYzSRmJ0%3D" rel="nofollow" title="https://tecdat.cn/?p=43754" target="_blank">https://tecdat.cn/?p=43754</a></p><hr/><ul><li>各县区平均房价地理图：可视化区域价格差异，直观展示核心城区与郊区的房价梯度。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412130" alt="" title="" loading="lazy"/></li><li>数据可视化大屏：整合核心指标（房源数量、均价、户型分布等），全方位展示市场概况。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412131" alt="" title="" loading="lazy"/></li></ul><h4><a name="t10" target="_blank"/>南昌数据可视化</h4><p>南昌市可视化分析逻辑与赣州一致，仅因城市数据差异呈现不同特征，核心可视化结果如下：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412132" alt="" title="" loading="lazy"/></p><h4><a name="t11" target="_blank"/>深圳数据可视化</h4><p>深圳作为一线城市，房价水平与区域差异显著，核心可视化结果如下：  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412133" alt="" title="" loading="lazy"/>  <br/>通过跨城市对比发现：深圳二手房价格显著高于赣州、南昌，且异常值较多；赣州、南昌的房价分布更集中，区域差异主要受核心城区与郊区地理位置影响；三地房源均以刚需户型为主，装修情况以简装、中装为主流。</p><h3><a name="t12" target="_blank"/>模型设计与优化</h3><h4><a name="t13" target="_blank"/>数据合并与特征处理</h4><ol><li>数据集合并：将三个城市的清洗后数据纵向拼接，最终得到42640条记录、31个特征的建模数据集，合并过程代码与结果如下。  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412134" alt="" title="" loading="lazy"/>  <br/>  核心合并代码（修改后关键片段）：</li></ol><pre><code>import pandas as pd# 读取三个城市的清洗后数据ganzhou_data = pd.read_csv('清洗后数据/赣州_清洗后.csv', index_col=False)nanchang_data = pd.read_csv('清洗后数据/南昌_清洗后.csv')shenzhen_data = pd.read_csv('清洗后数据/深圳_清洗后.csv')# 纵向合并数据combined_data = pd.concat([ganzhou_data, nanchang_data, shenzhen_data], axis=0)# 去重并重置索引combined_data.drop_duplicates(inplace=True)combined_data = combined_data.reset_index(drop=True)# 保存合并后数据combined_data.to_csv('./合并后数据集.csv', index=False)</code></pre><ol><li>异常值检测：通过箱线图发现深圳存在较多高价异常值，这些异常值会干扰模型训练，导致预测偏差。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412135" alt="" title="" loading="lazy"/></li><li>合并后数据分布：查看合并数据的直方图，因价格跨度极大，分高低价展示分布特征，为异常值处理提供依据。  <br/>   </li><li>特征筛选与编码：</li></ol><ul><li>剔除标题、挂牌时间等难以量化或相关性低的特征，保留核心特征用于建模。</li><li>对分类特征（如户型结构、装修情况等）采用LabelEncoder编码，编码代码如下。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412136" alt="" title="" loading="lazy"/>  <br/>核心编码代码（修改后关键片段）：</li></ul><pre><code>from sklearn.preprocessing import LabelEncoderimport joblib# 筛选建模特征model_data = combined_data.drop(['标题', '标题详情', '套内面积', ...], axis=1)# 定义需要编码的分类特征cat_features = ['小区名称', '户型结构', '建筑类型', ..., '所在县区']label_encoders = {}# 对分类特征进行编码for feat in cat_features: le = LabelEncoder() model_data[feat] = le.fit_transform(model_data[feat]) label_encoders[feat] = le# 保存编码器joblib.dump(label_encoders, '模型/label_encoders.pkl')</code></pre><ol><li>特征相关性分析：通过热力图筛选与“总价”相关性较高的特征，剔除小区名称、关注人数等相关性较低的特征，优化建模特征集。  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412137" alt="" title="" loading="lazy"/></li></ol><h4><a name="t14" target="_blank"/>模型训练（未处理异常值）</h4><p>采用8:2比例划分训练集与测试集，分别训练决策树（DT）、梯度提升树（GBT）、随机森林（RF）三种模型，使用R²、MSE、RMSE作为评估指标。</p><ol><li><p>决策树模型：  </p><p>决策树预测值与真实值对比：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412138" alt="" title="" loading="lazy"/></p></li><li><p>梯度提升树模型：  </p><p>梯度提升树预测值与真实值对比：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412139" alt="" title="" loading="lazy"/></p></li><li><p>随机森林模型：  </p><p>随机森林预测值与真实值对比：  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412140" alt="" title="" loading="lazy"/>  <br/>  未处理异常值时的模型性能：</p></li><li>分析可见：未处理异常值时，模型R²值偏低，高价房源的预测偏差尤为明显，需通过异常值处理与参数优化提升性能。</li></ol><h4><a name="t15" target="_blank"/>模型优化</h4><ol><li><p>异常值处理：采用IQR法则，剔除总价高于Q3+3IQR的异常数据（主要为深圳高价房源），处理代码如下。  </p><p>核心处理代码（修改后关键片段）：</p></li></ol><pre><code># 计算四分位数Q1 = model_data['总价'].quantile(0.25)Q3 = model_data['总价'].quantile(0.75)IQR = Q3 - Q1# 定义异常值边界upper_bound = Q3 + 3 * IQR# 剔除异常值optimized_data = model_data[model_data['总价'] &lt;= upper_bound]optimized_data.to_csv('./训练数据.csv', index=False)</code></pre><ol><li>参数优化：使用网格搜索（GridSearchCV）为各模型寻找最优参数，提升模型泛化能力。</li></ol><ul><li>决策树网格搜索：</li></ul><p>决策树预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412141" alt="" title="" loading="lazy"/></p><ul><li><p>梯度提升树优化：  </p><p>梯度提升树预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412142" alt="" title="" loading="lazy"/></p></li><li>随机森林网格搜索：</li></ul><p>随机森林预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412143" alt="" title="" loading="lazy"/></p><ol><li><p>优化后模型性能：</p><table><thead><tr><th>模型</th><th>R²</th><th>MSE</th><th>RMSE</th></tr></thead><tbody><tr><td>决策树</td><td>0.871</td><td>4220.42</td><td>64.96</td></tr><tr><td>梯度提升树</td><td>0.895</td><td>3422.84</td><td>58.50</td></tr><tr><td>随机森林</td><td>0.911</td><td>2928.50</td><td>54.16</td></tr></tbody></table><p>结果表明：剔除异常值并优化参数后，三种模型的预测精度显著提升，其中随机森林模型表现最佳，R²达到0.911，预测值与真实值拟合度良好，能有效捕捉二手房价格的核心影响因素。</p></li></ol><h3><a name="t16" target="_blank"/>结论与服务支持</h3><p>本次研究通过完整的数据挖掘流程，实现了多城市二手房价格预测与区域差异分析，验证了决策树、梯度提升树、随机森林在房价预测场景的有效性。核心结论包括：地理位置（所在城市、县区）、建筑面积、户型结构是影响二手房价格的关键因素；深圳房价整体偏高且波动较大，赣州、南昌房价分布更集中；随机森林模型经异常值处理与参数优化后，预测精度最优，可为市场参与者提供可靠参考。</p><h4><a name="t17" target="_blank"/>核心服务保障</h4><ol><li>应急修复服务：24小时响应“代码运行异常”求助，比学生自行调试效率提升40%，避免因代码问题耽误项目进度。</li><li>人工创作保障：所有代码与论文内容均经人工优化，直击“代码能运行但怕查重、怕漏洞”的痛点，保障原创性与可靠性。</li><li>全流程支持：提供从数据爬取到模型落地的全流程答疑，不仅教会“怎么做”，更解释“为什么这么做”，帮助真正掌握数据分析思维。  <br/>  本文项目完整代码、数据及可视化素材已同步至交流社群，进群即可获取。如需个性化修改、代码调试或润色服务，可联系团队获取一对一支持，让数据分析学习更高效、更省心。</li></ol><h2><a name="t18" target="_blank"/>关于分析师</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412144" alt="" title="" loading="lazy"/></p><p>在此对 Liping Xiao 对本文所作的贡献表示诚挚感谢，其专业方向为数据科学与大数据技术，曾担任北京中电中采数据服务有限公司的数据处理。擅长 Python 编程，在深度学习、数据分析、数据采集等领域具备专业的实践能力与技术储备。</p>]]></description></item><item>    <title><![CDATA[在 RTE2025 大会，我看到了 AI]]></title>    <link>https://segmentfault.com/a/1190000047412192</link>    <guid>https://segmentfault.com/a/1190000047412192</guid>    <pubDate>2025-11-19 19:04:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>10 月 31 日，我们受 RTE 开发者社区邀请参加了 RTE2025 大会。</p><p>这不是寻常的科技展会，没有冰冷的技术参数展示，取而代之的是AI与人类自然交流的场景。</p><p>今年大会以「AI 有声」为主题，这巧妙地道出了行业的变化——<strong>AI正在从无声的工具变为有声的伙伴。</strong> 从能贴心对话的毛绒玩具，到随时陪伴的AI语伴，从智能调研助手到趣味互动桌游，对话式AI已不再是科幻构想，而是触手可及、富有情感的日常存在。</p><p>这股“AI有声”的浪潮背后，是怎样的技术基石在支撑？一个优秀的语音智能体，从灵光一现的Demo到稳定可靠的产品，需要跨越哪些鸿沟？</p><p>为了探寻这些答案，我们不仅深入了专为语音智能体开发者打造的 “<strong>RTE101技术专场</strong>” ，系统梳理从语音前端处理到交互逻辑的核心技术栈；也穿梭于各大展台，亲身体验了如<strong> Chikka.ai </strong>这样的智能调研工具、<strong>ValidFlow.AI</strong> 的用户洞察平台以及 <strong>LookeeAI</strong> 教育硬件等前沿应用。</p><p>今天，我们来分享一下我们的所见所得、所思所想。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412194" alt="" title=""/></p><h2>101技术专场：语音智能体开发者的第一课</h2><p>这场为语音智能体开发者设计的“第一课”从最底层的信号处理一直讲到上层的应用实践，系统性地剖析了一个语音智能体从Demo到产品化需要跨越的鸿沟。</p><p><strong>音频3A处理（AEC、ANS、AGC）是对话式AI的基石。</strong> 声网音频算法工程师林子毅现场演示了回声消除的重要性：当关闭AEC功能时，AI设备会陷入“自问自答”的循环——说出回答后又被自己的声音触发，不断重复响应，根本无法正常交流。而降噪技术则让AI在嘈杂的机场环境中也能准确识别主要说话人的指令。</p><p><strong>语音活动检测（VAD）决定交互体验。</strong> 传统的VAD基于声带振动检测，但对于发轻音或辅音时无效。声网开源的TEN VAD基于深度学习，能更准确地检测语音起止点，将端到端响应延迟控制在毫秒级，避免了“说话被打断”或“说完等几秒没反应”的糟糕体验。</p><p>让我印象比较深刻的是 <strong>关于对话式AI架构的讨论。</strong> 目前主流的三段式架构将ASR、大语言模型、TTS串联，优点是各模块可独立优化，技术成熟度高。但端到端语音大模型直接将语音输入映射为语音输出，减少了信息损失，理论上更接近人类对话方式。</p><p>三段式架构灵活可控，但累积延迟较高；端到端模型响应更快，但训练数据和成本要求极高，且可控性较差。</p><p>在探讨<strong>对话式AI中传输音视频和数据的最佳协议选择</strong> 时，声网生成式AI产品负责人毛玉杰从产品角度指出，技术决策的出发点应该是用户体验而非技术本身。“我们总是在讨论协议的好坏，但核心是这个协议能给用户带来什么体验”。他建议开发者采用混合架构：WebSocket用于信令控制，WebRTC用于媒体传输，各取所长。</p><blockquote>此外，还分享了工具调用与MCP实践、对话式AI体验评估等内容。如果大家感兴趣的话，我们可以单独出一期“101技术专场”的内容完整回顾，欢迎在评论区留言</blockquote><h2>展会亮点：三款产品背后的AI语音革新</h2><p><strong>（1）Chikka.ai：让用户调研会“说话”</strong></p><p>在活动现场展台，我们遇到了这款专注于客户访谈的AI平台。Chikka.ai基于AI语音代理Ava，能与受访者进行自然对话，快速收集分析语音反馈。</p><p>最吸引我的功能是它支持多语言访谈，能同时进行数百次对话。想象一下，企业可以在短时间内完成大规模的用户调研，而且不是通过冰冷的问卷，而是模拟专业访谈者的对话交流。平台还提供个性化访谈计划、自动转录与见解提取，大大缩短了从收集到洞察的周期。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412195" alt="" title="" loading="lazy"/></p><p><strong>（2）ValidFlow.AI：从数据到洞察的智能升级</strong></p><p>相类似的，AI用户洞察应用ValidFlow.AI的展台吸引了不同的人群。</p><p>与Chikka.ai侧重于对话式数据收集不同，ValidFlow.AI更像是一个完整的用户洞察平台，它通过AI研究员和全球用户池的组合，全面高效地完成用户洞察的全流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412196" alt="" title="" loading="lazy"/></p><p>两者的区别在于：Chikka.ai像是擅长对话的采访者，而ValidFlow.AI则更像是整个研究团队——从设计调研到回收分析，提供更全面的解决方案。现场工作人员表示，他们的目标是让用户洞察不再是大公司的专利，让初创企业也能轻松获得高质量的市场洞察。</p><blockquote>进一步阅读：<a href="https://link.segmentfault.com/?enc=7VqBvtwPvshy5nicElA6Qw%3D%3D.CHtJ3Wk2enmv3aYbapNlUNZJ6akIIMfyH894D65XgpqdIHtFTXoT2jC7BSngzlE2JjWz%2BlAPajEZehX3TYeXqv2fbuWHtuOUyCmrxJLyTxDGm5DB5lPEFON60M2yKHUl%2FOQnQMlV0%2FhT608jt6XVgFsk%2BKWM5NMTSJZJYItyhB3FpF4zECSMl38Yy61F7nRH5COgTQZKDHLoiZz3bN4Kqh7bxEYxcPwsQF7DJtjX2cd81mv6pjwFAxlSl2zXSQnRTzcOx8KOuCxGI%2BJVf5%2BO5remqsRlL6OJkmQBHLFAXo752XQBk9dmpAvElSImsSDTO8GRoX%2F2tKXZHsXk38qUUDzV8nOmYtxLTU6pBTb7O2msQ55N1EwXM1xfowL3qUq9KgczSk%2Bxl09DHoZXCAGD%2FtB0wpTaqLU5ta6ckHG5onpJdSfRkQPDkpeegBjy9ArXifwfI%2FQ7cKuA%2Fr8PZL7WmtQ6TKf4BsDP1zoFwyPYma9oBk8C4kR825bbEVi7Xk4i" rel="nofollow" target="_blank">RTE现场看到的超绝AI调研产品，调研人福音！</a></blockquote><p><strong>（3）Lookee：会互动的英语学习伙伴</strong></p><p>教育硬件展区总是人山人海，盒智科技推出的LOOKEE口语侠尤其受欢迎。这款被称为“全球最小AI英语学习语伴”的产品，大小不过一个耳机盒，重量不到50克，专为5-12岁儿童设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412197" alt="" title="" loading="lazy"/></p><p>他们自研的AURA系统能实时分析孩子的情感状态、内容理解度和对话积极性，并动态调整对话策略。通过情感化交互设计，为孩子打造零压力的专属口语环境，解决孩子们“没处说、不敢说”的痛点。</p><p>起初我觉得这类 AI 教育硬件应该是产品同质化的重灾区，但在现场我发现，一个孩子在与LOOKEE进行英语对话时，摇一摇LOOKEE，LOOKEE不仅会换一个口语话题，甚至还会换一个非常有动感的表情，非常可爱。</p><p>后面LOOKEE团队告诉我，他们做了大量用户调研，挖掘孩子们真正痛点和兴趣点。我觉得这种用心在初创团队上是十分难得的，也是真正打动我的。</p><h2>AI桌游《Talk With》：聊天能解决的事儿</h2><p>大会的互动环节中，最让我惊喜的是全球首款AI主题语言类桌游《Talk With》（中文名：聊天能解决的事儿）试玩会。</p><p>游戏规则颇具创意：3-5名玩家抽取随机场景卡，每个场景都潜含着困难和危机。玩家需要竞拍合适的对话式AI和语音技术，构建自己的技术壁垒，最终通过路演展示产品愿景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412198" alt="" title="" loading="lazy"/></p><p>我参与的一局游戏中，抽到的场景是“解决开车犯困问题”。我们小组竞拍到了情感语音合成技术，设计了一款能通过聊天保持驾驶员清醒的AI助手。游戏中的“市场行情卡”带来了意想不到的转折——突然的“技术突破”让我们的方案更具竞争力。</p><p>这款桌游巧妙地将AI语音技术融入轻松愉快的游戏中，让参与者在40分钟的游玩中理解了不同语音技术的应用场景和组合价值。正如游戏名称所暗示的，有时候聊天真的能解决很多问题。</p><h2>总结：AI语音的未来是更自然的“相处”</h2><p>回顾一天的逛展体验和技术专场的深度学习，我最大的感受是：<strong>AI语音技术正从“工具性”走向“关系性”。</strong></p><p>从会陪伴的芙崽Fuzozo，到能教学的LOOKEE，再到帮助企业洞察用户的ValidFlow.AI，这些产品不再强调技术的强大，而是聚焦于<strong>如何让AI更懂人心</strong>。101技术专场中各位讲师反复强调的一个观点让我深有共鸣：<strong>技术最终要为体验服务。</strong></p><p>当AI学会了倾听与对话，它不再只是执行命令的工具，而成为我们生活中<strong>有温度的伙伴</strong>。这或许就是对话式AI最美好的前景——不是取代人类，而是以更自然的方式与人“相处”，<strong>让技术真正服务于人的情感需求。</strong></p><p>我相信，我们迎来的将是一个更有温度的数字未来。</p><p>最后，再次感谢 RTE 开发者社区邀请我们来参加这次活动，真的非常用心，dev party也让我们交到了不少有意思的新朋友。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412199" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412200" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=NBX%2F7D2aOQwu8RZF8Ah9vg%3D%3D.JOkuAyv%2Fw1RGoiYwaaohjv%2FBbMPXmHqtOikzLcMyOoc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412201" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[EMR Serverless Stell]]></title>    <link>https://segmentfault.com/a/1190000047412219</link>    <guid>https://segmentfault.com/a/1190000047412219</guid>    <pubDate>2025-11-19 19:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在今年云栖大会上，EMR Serverless Stella 1.0正式发布，这是一款面向企业级场景深度优化的高性能数据分析引擎。阿里云开源大数据平台OLAP引擎负责人周康系统性地分享了 Stella 在存算分离架构、Lakehouse 场景以及全文检索等三大核心场景下的深度优化经验，为业界提供了大规模 OLAP 系统工程化实践的宝贵参考。Stella引擎的发布将为企业级用户提供更加专业、高效的OLAP解决方案。</p><h2>站在巨人肩膀上：与 StarRocks 开源社区的深度合作</h2><p>阿里云与StarRocks开源社区的合作可以追溯到2021年，从开源第一天起就建立了深度合作关系。在过去四年中，双方在源码共创、产品发布和技术优化方面积累了丰富的经验。</p><p><strong>合作历程回顾：</strong></p><ul><li><strong>2021年</strong>：开启源码共创，重点推动数据湖分析相关框架和性能优化</li><li><strong>2022年3月</strong>：推出EMR半托管StarRocks形态</li><li><strong>2023年</strong>：响应市场需求，推出全托管产品形态</li><li><strong>2024年</strong>：正式商业化存算分离版本</li></ul><p>随着产品的成熟，阿里云EMR已积累数百家B端企业客户。“我们始终站在巨人的肩膀上，”阿里云开源大数据平台OLAP引擎负责人周康表示，“Stella 所有功能和优化都会逐步回馈给社区，同时确保API层面与开源版本完全兼容。”<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdm6eJ" alt="image.png" title="image.png"/></p><h2>Lakehouse 成为业界共识：Stella 应运而生</h2><p>2024年，阿里云正式发布 OpenLake 方案，标志着 Lakehouse 架构在数据基础设施领域的全面落地：<br/><img width="723" height="273" referrerpolicy="no-referrer" src="/img/bVdm6eL" alt="image.png" title="image.png" loading="lazy"/><br/>2024云栖大会重磅发布OpenLake解決方案，StarRocks 为 OLAP场景核心组件</p><p>伴随这一趋势，Lakehouse（数据湖仓一体）已成为国内外头部公司的业界共识：<br/><img width="723" height="298" referrerpolicy="no-referrer" src="/img/bVdm6eM" alt="image.png" title="image.png" loading="lazy"/><br/>海外Lakehouse发展趋势  Snowflake/Databricks/BigQuery + Iceberg/Delta/Hudi</p><p>阿里云推出了 OpenLake 一体化湖仓解决方案，StarRocks 在其中担任核心 OLAP 引擎角色。然而，在大规模生产环境中，StarRocks 在存算分离架构和湖表查询方面仍有优化空间。Stella 项目正是为了应对这些挑战而生。通过在调度、查询优化、执行引擎和存储引擎四个层面的全面改进，Stella 1.0 针对几十 TB 甚至 PB 级数据场景，解决了事务机制、Compaction 效率、查询性能、元数据管理等一系列生产环境痛点。</p><h2>Stella 1.0 三大核心场景突破</h2><p>EMR Serverless Stella 1.0版本于今年5月正式发布，主要聚焦三大核心技术能力的重大突破：</p><h3>一、存算分离：性能和稳定性大幅提升</h3><p>Stella 1.0 在存算分离架构下实现了三大突破：</p><p><strong>1. 冷查性能大幅提升</strong></p><ul><li>实现 IO 合并，减少对象存储访问次数</li><li>优化 Compaction 调度器，大幅减少小文件数量 </li><li>针对轻量级 ETL 场景优化负载调度</li></ul><p><strong>2. 写入性能保障</strong></p><ul><li>开发 Batch Publish 能力，解决串行化导入瓶颈 </li><li>推出 Collocated PK Index，避免缓存盘和索引盘互相影响 </li><li>优化 FE 侧 Tablet 创建删除效率</li></ul><p><strong>3. 缓存利用率优化</strong></p><ul><li>引入 Index Cache 和 Meta Data Cache，提升元数据访问速度 </li><li>实现自适应 IO Stream，智能选择本地缓存或远端访问 </li><li>针对 ETL 场景优化空间利用</li></ul><p>在TPC-H 10T基准测试中，存算分离版本的Stella相比上一版本<strong>性能提升超过120%</strong>，充分展现了云原生架构的技术优势。<br/><img width="723" height="430" referrerpolicy="no-referrer" src="/img/bVdmZu8" alt="image.png" title="image.png" loading="lazy"/></p><h3>二、Paimon 湖表查询：Co-design 驱动性能飞跃</h3><p>Stella 1.0在Paimon表分析方面，重点聚焦在三个方向的提升：</p><p><strong>1. 数据读写效率提升</strong> </p><ul><li>实现自适应 Batch Size 优化 </li><li>支持Native Paimon Writer，性能大幅提升</li></ul><p><strong>2. 元数据访问优化</strong> </p><ul><li>针对 Manifest 数量众多场景，实现分布式解析能力 </li><li>适配异步 Splits 调度框架 </li><li>优化 Manifest Cache 策略</li></ul><p><strong>3. 深度集成阿里云 DLF 2.x</strong> </p><ul><li>与 Data Lake Formation 产品深度整合 </li><li>借助 DLF 能力提升 Paimon 查询和写入的性能与稳定性 </li><li>针对DV表实现Native读取优化</li></ul><p>Stella在Lakehouse场景下查询Paimon下性能的提升非常明显：<br/><img width="723" height="513" referrerpolicy="no-referrer" src="/img/bVdm6eU" alt="image.png" title="image.png" loading="lazy"/></p><p>虽然 Flink + Paimon 已成为成熟的实时入湖方案，但计算引擎与 Paimon 存储的查询优化结合仍有巨大提升空间。Stella 与 Paimon 将在多个方便持续进行Co-Design，更多优化成果将在后续版本中发布。</p><h3>三、全文检索：打造高性能、高可用的文本分析能力</h3><p>Stella 1.0 正式推出全文检索能力，支持高效、精准的文本查询。</p><ul><li><strong>架构重构</strong>：对 Inverted Index（倒排索引） 整体解决方案进行架构优化</li><li><strong>存算分离主键表支持</strong>：新增主键表全文检索能力，实现高效精准的查询能力</li><li><strong>小文件合并</strong>：解决存算分离架构下的“性能杀手”问题（单个 Segment 产生十几个小文件） <br/><img width="723" height="529" referrerpolicy="no-referrer" src="/img/bVdm6eV" alt="image.png" title="image.png" loading="lazy"/></li></ul><p>文本过滤性能benchmark: Stella vs EMR StarRocks 3.3</p><p>目前，全文检索功能已在阿里集团内部和云上客户中投入使用，所有优化代码已通过 PR 提交至 StarRocks 开源社区。</p><h2>技术创新路线图持续演进</h2><p>面向未来，Stella引擎制定了清晰的技术发展路线图，在四个关键领域持续深耕：</p><ol><li><strong>迈向Stella 2.0时代：轻量 ETL Production Ready</strong>  <br/>全面强化轻量级 ETL 能力，打通从数据接入、转换到分析的端到端链路，使用户无需依赖外部调度系统即可高效完成日常数据加工任务，真正实现“开箱即用、生产就绪”。</li><li><strong>Lake Optimizer：湖表性能全面对齐甚至超越内表</strong>  <br/>推出专为开放数据湖设计的 Lake Optimizer，显著提升 Apache Paimon 等湖表格式的查询性能，让湖表在复杂分析场景中媲美甚至超越传统内表体验。</li><li><strong>智能化 Background Job Service：彻底释放用户运维负担</strong>  <br/>针对企业用户长期面临的内表运维复杂、资源争抢等问题，Stella 将推出智能化后台作业服务，自动处理 compaction、索引构建、统计信息收集等任务，实现高智能化的自治运维，大幅提升系统稳定性与资源效率。</li><li><strong>全文检索与向量检索能力持续提升</strong>  <br/>在已有的高性能 OLAP 基础上，进一步融合全文检索与向量检索能力，支持非结构化与多模态数据的统一分析，为 AI 原生应用、智能搜索等新兴场景提供底层引擎支撑。</li></ol><p>这四大方向不仅体现了 Stella 对 Lakehouse 架构的深度适配，更彰显了其从“高性能分析引擎”向“智能数据平台核心引擎”演进的战略决心。随着这些能力的逐步落地，Stella 将为企业用户提供更开放、更智能、更易用的下一代实时分析体验。</p><h2>技术探索与社区协作深度融合</h2><p>Stella引擎在技术架构探索方面持续深化与开源社区的合作：</p><p>Lakehouse架构能力的持续拓展体现了Stella引擎的前瞻性设计理念。在现有Lakehouse架构基础上，系统将支持更多检索功能，为企业的多元化分析需求提供全面支持。向量搜索技术是与Apache Paimon深度集成的创新探索，在AI和大数据时代，向量搜索能力将成为差异化的技术优势。</p><h3>开源社区贡献亮点</h3><ul><li><strong>JSON等半结构化数据处理能力持续增强</strong>，推动整个生态发展</li><li><strong>大规模场景技术实践经验分享</strong>，为社区贡献宝贵技术智慧</li><li><strong>与Apache Paimon团队深度技术合作</strong>，确保生态整合持续优化</li><li><strong>所有优化方案回馈开源社区</strong>，推动开源生态系统发展进步</li></ul><p>开源社区的深度贡献体现了Stella团队的技术责任感和开放合作精神。JSON等半结构化数据处理能力的持续增强将推动整个生态的发展，为企业在数字化转型过程中处理多样化数据提供更强支持。大规模场景下的技术实践经验分享不仅展示技术实力，更为社区贡献了宝贵的技术智慧。</p><p>“我们不仅要在云上提供增值服务，更要推动整个开源生态的发展，”周康强调，“通过深度参与开源社区，确保所有用户都能从技术进步中受益。”</p><h2>技术意义与未来规划</h2><p>EMR Serverless Stella 1.0的发布标志着阿里云在湖仓一体技术领域达到新的里程碑，为用户提供从数仓加速、湖仓查询到全文检索的全方位OLAP能力支持。该版本不仅解决了企业在实际生产环境中遇到的关键技术挑战，更通过持续的技术创新和社区贡献，推动了整个StarRocks生态系统的发展。</p><p>未来，Stella将继续围绕Lakehouse架构演进，在缓存调度、查询优化、存储引擎和写入能力等核心领域持续创新，为企业数字化转型提供更加强劲的技术引擎。</p>]]></description></item><item>    <title><![CDATA[普通人如何免费使用Gemini3-Pro]]></title>    <link>https://segmentfault.com/a/1190000047412226</link>    <guid>https://segmentfault.com/a/1190000047412226</guid>    <pubDate>2025-11-19 19:02:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>大模型的用法主要分为以下两类：</p><ol><li><strong>个人用途</strong>：作为日常知识补充、内容创作及辅助工具。</li><li><strong>商业用途</strong>：集成至 Dify、Coze 等工作流与应用程序中，或在编程工具及第三方客户端（如 Cherry Studio）中使用。</li></ol><p>目前，<strong>普通用户几乎可以免费使用各类主流大模型</strong>，Google 最新推出的 Gemini 3 Pro 也不例外。</p><h2>使用方法</h2><ol><li><strong>访问 AI Studio 官网</strong>：<a href="https://link.segmentfault.com/?enc=S2Plhp%2B4YOdkQOOArN%2Bm%2Bw%3D%3D.b2OqHiM6sFkooR4UZlwi5lnyC0TdolmhvZFOP4uUBYY%3D" rel="nofollow" target="_blank">https://aistudio.google.com/</a></li><li><strong>注册并登录 Google 账号</strong></li></ol><p>登录后，即可免费通过 Gemini 3 Pro 进行对话，或完成代码生成任务。</p><p>此外，Google 还提供了其他免费模型，支持图片生成与编辑、音视频制作，甚至播客生成功能。</p><p>例如，生成图片可以使用 Google 的 Imagen，视频生成则可以使用 Veo 等模型。</p><h2>视频演示</h2><p><a href="https://www.bilibili.com/video/BV14ByjBqEAS/" target="_blank">https://www.bilibili.com/video/BV14ByjBqEAS/</a></p><h2>对话演示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412228" alt="" title=""/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412229" alt="" title="" loading="lazy"/></p><h2>图片生成</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412230" alt="" title="" loading="lazy"/></p><p>视频与音频的生成方式与此类似，此处不再赘述。</p><h2>代码生成（App / 网站 / 小游戏开发）</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412231" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412232" alt="" title="" loading="lazy"/></p><h2>小结</h2><p>本文简要介绍了 Google Gemini 3 Pro 的免费使用渠道，及其在文本对话、代码编写及多模态生成方面的核心功能。</p><blockquote>本文已收录到我的技术小站 <a href="https://link.segmentfault.com/?enc=twc7Uo1MLlsctnbVQIrUvg%3D%3D.kJLSzWOQtvCpaXhkutX4uR%2BJCEv%2F3UQ2vlDo4n0w1HY%3D" rel="nofollow" target="_blank">www.javacn.site</a>，其中包含的内容有：Spring AI、Spring AI Alibaba、LangChain4j、Dify、Coze、N8N、智能体（AI Agent）、MCP、Function Call、RAG、向量数据库、Prompt、多模态、向量数据库、嵌入模型、AI 常见面试问题等内容。</blockquote>]]></description></item><item>    <title><![CDATA[Blender 5.0 (Linux, ]]></title>    <link>https://segmentfault.com/a/1190000047412272</link>    <guid>https://segmentfault.com/a/1190000047412272</guid>    <pubDate>2025-11-19 19:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Blender 5.0 (Linux, macOS, Windows) - 开源 3D 创意软件 (渲染 建模 雕刻)</p><p>Open-Source 3D Graphics App Introduces Experimental Vulkan Backend</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=%2FkLbUuDgvadeGAqxkqsA9g%3D%3D.YlG8qYs%2BN%2Fk3BVwmgPXvw35IqgOvV2TMVInhe5SCwfk%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=EwEU1XUK1d2o%2BKPuV%2FCuKA%3D%3D.Zk2XBXTOzuYYifoQW0eIUH205l0tuRdlN%2BmabjiZERI%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412274" alt="Blender" title="Blender"/></p><h2>创造的自由</h2><p>Blender 获得 GNU GPL 许可，由其贡献者拥有。</p><p>因此，Blender 永远是免费和开源软件。</p><p>✅ <strong>使命</strong></p><p>以免费/开源软件的形式将世界上最好的 3D CG 技术交到艺术家手中。</p><p>✅ <strong>愿景</strong></p><p>每个人都应该自由地创作 3D CG 内容，拥有自由的技术和创意制作手段以及自由的市场准入。</p><p>✅ <strong>组织机构</strong></p><p>Blender 基金会 (2002) 是一个独立的公益组织。其衍生公司 Blender Institute (2007) 是基金会的办公室所在地，目前拥有 24 名员工，负责 Blender 软件和创意项目，以在生产环境中验证和测试 Blender。</p><p>2020 年，该研究所拆分为两家公司；Blender Institute 现在仅作为 Blender 基金会的工作公司，新的 Blender Studio 将为 Blender 制作内容和测试生产流程的使命做出贡献。</p><p>这些组织支持 <a href="https://link.segmentfault.com/?enc=Knd2Ia9jm6KfmoJuOoNkig%3D%3D.bkWThFZ7Nydsbk8bdYS5LhNB2ByMs9viz795LktpaEo%3D" rel="nofollow" target="_blank">Blender.org</a> 上的贡献者社区。这就是 Blender 的诞生地。</p><p>✅ <strong>软件</strong></p><p>Blender 是免费的开源 3D 创作套件。它支持整个 3D  管道——建模、绑定、动画、模拟、渲染、合成和运动跟踪，甚至视频编辑和游戏创建。高级用户使用 Blender 的 Python 脚本 API  来定制应用程序并编写专门的工具； 通常这些都包含在 Blender 的未来版本中。Blender 非常适合个人和小型工作室  (sysin)，他们可以从其统一的流程和响应式开发流程中受益。<a href="https://link.segmentfault.com/?enc=En5rTIfoW%2FQ7BzI8SmRWjQ%3D%3D.eGyO738F9xnhF%2BrcgFB4kjXsbxZvkQfZ4%2BLKHYtlGQbWIPUz1VTjfklt4rWQBu2w" rel="nofollow" target="_blank">特性展示</a> 中提供了许多基于 Blender 的项目的示例 。</p><p>Blender 是跨平台的，在 Linux、Windows 和 Macintosh 计算机上运行得同样好。其界面使用 OpenGL 来提供一致的体验。为了确认特定的兼容性，<a href="https://link.segmentfault.com/?enc=D3ERtV09ccgr0OjYeMIiAQ%3D%3D.DhB%2B1EPnPPF2VgN8kwCqu8171C587zgMRxJ8CkqT3UdPemvjIUWfK1jRnhIBwzrn" rel="nofollow" target="_blank">支持的平台列表</a> 显示了开发团队定期测试的平台。</p><p>作为 GNU 通用公共许可证 (GPL) 下的社区驱动项目，公众有权对代码库进行大大小小的更改，从而带来新功能、响应性错误修复和更好的可用性。Blender 没有价格标签，但您可以投资、参与并帮助推进强大的协作工具：Blender 是您自己的 3D 软件。</p><p>随时欢迎更多帮助！从开发和改进 Blender 到编写文档等，您可以做很多不同的事情来参与其中。</p><p>✅ <strong>隶属关系</strong></p><p>Blender 基金会是 Open Invention Network、Khronos、Linux Foundation 和 Academy Software Foundation 的成员。</p><p>✅ <strong>许可证</strong></p><p>Blender 是免费软件。您可以 <strong>目的自由</strong> 使用 Blender <em>出于任何</em>，包括商业或教育目的。了解有关 <a href="https://link.segmentfault.com/?enc=PXTyNoc%2FjC6QB6e7Irq7bw%3D%3D.jzKKy5UYFgtnq7AyFAU1vo8jrAtT7wYgKHChhUj8PlHSesSI2pp%2F7anw0hRoL2qT" rel="nofollow" target="_blank">许可证的</a> 更多信息。</p><h2>下载地址</h2><p>Blender 4.5 LTS for macOS, Linux, Windows (2025-11-18)</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=vOUGTHAKxz7x9F3AznLlBw%3D%3D.EpaD66jhsleZahvOeaxqOANn5tNb0uiGIfQr9awSQ9o%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a></li><li>Blender 4.5.5 LTS for macOS x64 (Intel 处理器) (blender-4.5.5-macos-x64.dmg)</li><li>Blender 4.5.5 LTS for macOS ARM64 (Apple 芯片) (blender-4.5.5-macos-arm64.dmg)</li><li>Blender 4.5.5 LTS for Linux x64 (blender-4.5.5-linux-x64.tar.xz)</li><li>Blender 4.5.5 LTS for Windows x64 (blender-4.5.5-windows-x64.msi)</li><li>Blender 4.5.5 LTS for Windows ARM64 (blender-4.5.5-windows-arm64.msi)</li></ul><p>Blender 5.0 for macOS, Linux, Windows (2025-11-18)</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=F2CJO3DcCTk6qSbLWXccIg%3D%3D.SbGe5LIrB5SUintJchVXKdlzXjw6legpDxW2dH7T6FQ%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a></li><li>Blender 5.0 for macOS x64 (Intel 处理器) (blender-5.0.0-macos-x64.dmg)</li><li>Blender 5.0 for macOS ARM64 (Apple 芯片) (blender-5.0.0-macos-arm64.dmg)</li><li>Blender 5.0 for Linux x64 (blender-5.0.0-linux-x64.tar.xz)</li><li>Blender 5.0 for Windows x64 (blender-5.0.0-windows-x64.msi)</li><li>Blender 5.0 for Windows x64 (blender-5.0.0-windows-arm64.msi)</li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=8Ze8RN5jmSij6Vo32P9kww%3D%3D.itFc45Dq4wHQizC04z3XHJ317O0wmTJOimK%2FPAaamic%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[AI起飞到这份上了，普通人还能赶上这波风]]></title>    <link>https://segmentfault.com/a/1190000047412285</link>    <guid>https://segmentfault.com/a/1190000047412285</guid>    <pubDate>2025-11-19 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今年只要一聊职业规划，大家最爱问的就是三句话：“AI这么火，我现在转行还来得及吗？”<br/>“我是理工生，计算机0基础也能进AI行业吗？”<br/>“学完Python就能当算法工程师吗？”别急，我们先把“热度”说清楚，再告诉你：谁能吃到红利、要学什么、怎么拿到offer。<br/>01｜AI行业到底有多火？一组新数据扎心了<br/>猎聘大数据研究院最新发布的《2025上半年人才供需洞察报告》给出了极具冲击力的答案：AI技术岗整体需求增长36.82%。<br/>其中——家电行业的AI技术职位同比暴增277.43%！这意味着什么？<br/>AI不再是互联网大厂的“专属”，已经渗透进家电、制造、软件、零售几乎所有行业。<br/>这也是为什么你身边的洗衣机、扫地机、空调都越来越“会思考”——智能家居正在成为新一轮AI人才需求爆发的真正推手。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412287" alt="图片" title="图片"/><br/>更扎心的来了：<br/>AI技术岗50万以上年薪的职位占比高达31.03%。<br/>几乎是全行业（6.40%）的5倍。深度学习岗里，这个比例更是高达39.54%。<br/>市场的态度很直接：<br/>会AI的人，不够用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412288" alt="图片" title="图片" loading="lazy"/></p><p>02｜但别误会：AI不是“博士专属领域”<br/>很多人以为AI行业等同于：“数学大神+清北硕博+顶尖竞赛”。<br/>其实完全不是。目前企业真正缺的是三类人：<br/>①有项目能力的技术岗（算法/深度学习/机器学习）增长最快的是算法工程师岗（同比+50%），企业不是看你学历，而是看你能不能把模型训好、效果调优。<br/>②懂AI逻辑的产品、运营、数据岗专业不限制，关键是能和工程师对话，知道模型能做什么、不能做什么。<br/>③“交叉型人才”（最稀缺）比如懂医疗的AI顾问、懂制造的AI应用经理、懂语言能力的AI训练师。<br/>这些岗位目前缺口都在10万量级。所以文科、跨专业、转行的人，反而更有机会切入AI的落地场景。</p><p>03｜热门岗位怎么选？5类路径，一条总有适合你<br/>1）算法工程师（高薪核心岗）做模型训练、调优、优化。<br/>适合数学/计算机基础好、希望冲高薪的人。<br/>2）AI产品经理（需求越来越大）负责把“AI能力”变成“能使用的产品”。<br/>适合逻辑好、懂业务、有产品思维的人。<br/>3）数据分析/数据标注（入门友好）做数据清洗、标签标注，是所有模型的基础工作。<br/>适合刚入门、希望低门槛进入行业的人。<br/>4）AI训练师（新兴热门）训练大模型的风格、纠错能力、推理能力。<br/>适合语言类、教育类、沟通能力好的同学。<br/>5）行业垂直AI顾问（未来最吃香）把AI落地到医疗、金融、制造等场景。<br/>适合行业背景+AI基础的复合型人才。</p><p>04｜应届生和转行党最关心的三个问题，直接说结论<br/>❶“现在转行AI还来得及吗？”不仅来得及，现在是正当时。<br/>AI技术岗需求还在增长区间（增幅超36%），人才缺口大、工资高、流动快。<br/>❷“非科班能进AI吗？”能。<br/>企业看的是“你能不能解决问题”。<br/>只要补齐基础技能+项目经验，有大量岗位愿意给机会。<br/>❸“学Python就能当算法工程师吗？”当然不行。<br/>Python只是最基础的工具，真正的算法岗需要的是一整套系统能力换句话说：Python是敲门砖，但不是通行证。<br/>不过——<br/>如果你的目标只是进入AI行业，并不一定非要走最难的“算法那条线”。<br/>产品、运营、训练师、行业顾问等岗位同样需要AI思维，也有成熟的学习路径，更适合转行和非科班的同学。</p><p>05｜最后：AI不是神话，是一场“技能革命”<br/>当智能家电、工业AI、AI办公、AI客服全面落地，企业已经从“要不要AI”，变成——“有没有懂AI又懂业务的人？”<br/>接下来3–5年，AI技术岗会继续贵，继续涨，继续难招。最有机会的人是：会AI+懂行业+能落地的复合型人才。<br/>如果你正在找方向、准备转行、想提升竞争力，AI行业依然是这几年少有的“确定性增长赛道”。<br/>现在入场的人，将站在行业的黄金红利期。<br/>现在还在观望的人，大概率会错过下一轮职业跃迁窗口。</p>]]></description></item><item>    <title><![CDATA[普林斯顿大学数据库遭黑客攻击 信息疑似泄]]></title>    <link>https://segmentfault.com/a/1190000047411911</link>    <guid>https://segmentfault.com/a/1190000047411911</guid>    <pubDate>2025-11-19 18:12:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>据彭博社消息，美国时间11月16日，普林斯顿大学进修办公室一系统在11月10日遭到网络攻击。该系统中存有姓名、联系方式以及捐款记录等诸多敏感信息，数据已全部泄露。校方表示黑客利用钓鱼攻击手段，获取了一名员工的账户权限，从而顺利侵入学校办公系统，目前已经成功阻断非法入侵。</p><p>无独有偶，近期已经有多所常春藤高校遭遇网络攻击：宾夕法尼亚大学、哈佛大学和哥伦比亚大学的校园信息系统均遭到入侵，学生信息以及校友数据都被泄露。其中，哥伦比亚大学遭遇的网络攻击最为严重，约87万人信息遭到泄露。JoySSL安全总监指出，普林斯顿大学作为全球顶尖学府，尚且发生规模和影响如此严重的数据泄露事故，可见组织机构的数据防护工作依然任重道远。在数据库层面部署数字证书，提升安全防护等级，已成为防范网络攻击的重要手段。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6ae" alt="" title=""/></p><p><strong>数据泄露源于加密工作缺失</strong></p><p>根据安全专家对此次泄露事件的分析，网络黑客很可能利用了普林斯顿大学在数据存储和传输方面的加密漏洞，从而顺利侵入系统获取重要信息。事实上，很多知名高校或者平台，都在网络安全防护建设上存在认知误区，认为普通的边界防护足以守护信息安全。但实际上，数据库与应用程序之间的数据传输通道极容易遭受网络攻击，普通的防护技术难以有效抵御各种攻击手段。</p><p>自2023年以来，全球高等教育机构遭遇网络攻击和信息泄露的事件已经超过30起，绝大多数泄露事件均因数据库加密不足所致。JoySSL市场部专家表示，数据库系统信息遭窃主要出现在教育和医疗等领域，然而，由于认知缺陷，超过三成的医疗机构和教育平台依旧未部署SSL加密，导致数据泄露事件频频上演。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6af" alt="" title="" loading="lazy"/></p><p><strong>SSL证书提供专业解决方案</strong></p><p>网络安全形势日益严峻，采用有效的加密防护方案迫在眉睫。JoySSL作为专业的数字安全服务商，旨在利用SSL证书在安全领域的专业性能，为数据库提供全链路传输加密，确保从应用服务器到数据库的每一次请求或数据交换都经过高强度加密。基于SHA384算法签发的数字证书可有效保障数据安全，即使被截获也能够让攻击者无法获取有效信息。</p><p><strong>专业防护助推企业线上发展</strong></p><p>越是复杂的网络环境，传输安全风险越大，尤其涉及到云计算，数据库与应用服务器通常分布在不同的网络区域，很容易受到攻击。SSL证书的专业防护性能让攻击者在加密层无功而返，确保数据传输的安全性。某电商平台在部署数字证书后，受到的网络攻击大幅缩减，数据传输安全性提升超过70%。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6ai" alt="" title="" loading="lazy"/></p><p><strong>构建安全防护体系防患于未然</strong></p><p>有关专家指出，随着全球范围内网络安全威胁现象频频出现，组织机构应该率先对自己进行全方位的安全评估，检查所有可能存在的安全漏洞，积极部署SSL证书以保障数据安全传输与存储，配合实时监测机制，建立起有效的安全防护体系，真正做到防患于未然。</p>]]></description></item><item>    <title><![CDATA[国防航天领域的智慧指挥新引擎 图观 ]]></title>    <link>https://segmentfault.com/a/1190000047411917</link>    <guid>https://segmentfault.com/a/1190000047411917</guid>    <pubDate>2025-11-19 18:11:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在国防航天领域，面对日益复杂的任务环境和瞬息万变的战场态势，如何实现高效、精准的指挥决策与运维管理，一直是行业关注的焦点。传统的信息系统往往存在数据孤岛、响应滞后等问题，难以满足现代国防航天任务对实时性、协同性和智能化的高要求。而今，随着数字孪生技术的成熟，一种全新的智能运营解决方案正悄然改变这一局面。孪易数字孪生IOC ProMAX版，作为一款集全场景监控、智能分析、应急协同于一体的平台，正为国防航天领域注入新的智慧动力。</p><h2>全场景一体化监控：从虚拟到现实的无缝映射</h2><p>在国防航天任务中，无论是卫星发射、飞行器测试，还是基地运维，都需要对多地点、多维度数据进行实时监控。孪易通过其基础控制模块，支持多地点切换、场景剖分和环境仿真，帮助用户构建高精度的数字孪生模型。例如，在卫星发射任务中，用户可以通过平台实时模拟发射场环境，结合历史回放功能，回溯关键时间点的数据变化，快速定位问题根源。这种全场景一体化监控能力，不仅提升了运维效率，更让指挥人员能够“透视”复杂系统，实现从虚拟到现实的无缝映射。<br/><img width="640" height="314" referrerpolicy="no-referrer" src="/img/bVdmQxT" alt="" title=""/></p><h2>智能应急与协同处置：应对突发事件的“智慧大脑”</h2><p>国防航天任务中，突发事件如设备故障、环境异常等，往往需要快速响应和跨部门协同。孪易IOC ProMAX版的应急处突模块，通过数字化预案管理和任务全过程监控，实现了资源的智能调度与多方联动。例如，在航天器发射过程中，若监测到异常数据，系统可自动触发应急预案，分配任务给相关团队，并通过视频会商功能一键发起跨部门会议，确保问题在最短时间内得到解决。这种智能协同机制，不仅提升了应急响应的效率，还降低了人为失误的风险，成为国防航天领域的“智慧大脑”。</p><h2>AI驱动的智能运维：从被动响应到主动预警</h2><p>人工智能技术的融入，让孪易IOC ProMAX版在国防航天应用中更具前瞻性。其智能助理模块支持自然语言交互，用户只需通过语音或文本指令，即可操作孪生体对象或查询关键数据，大幅降低了系统操作门槛。同时，智能分析功能基于历史数据进行趋势预测，并结合视觉识别技术，对视频流中的目标进行检测与行为标记。例如，在航天基地安防监控中，系统可自动识别异常入侵行为并发出预警，实现从“被动响应”到“主动防控”的升级。这种AI驱动的运维模式，不仅提升了系统的智能化水平，还为国防航天任务提供了更可靠的安全保障。</p><h2>高度可定制与扩展性：灵活适配复杂业务场景</h2><p>国防航天领域的业务需求多样且复杂，孪易通过零代码与低代码开发模式，为用户提供了高度可定制的扩展能力。零代码平台允许用户通过拖拉拽方式快速配置页面与交互逻辑，而低代码模式则支持基于JavaScript的深度开发，满足个性化业务需求。此外，平台兼容多种数据源与建模工具，如3DMax、Revit和GIS数据，确保系统能够无缝集成现有业务系统。这种灵活性，让国防航天用户能够根据任务演进，持续优化系统功能，避免因技术迭代带来的重复投入。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmmM0" alt="" title="" loading="lazy"/></p><h2>全平台适配与成熟落地：经得起实战检验的解决方案</h2><p>孪易具备多端自适应能力，针对指挥中心大屏、桌面中屏与移动端分别优化交互体验，确保用户在任何场景下都能高效操作。更重要的是，该平台经过近20年、上千个项目的行业验证，在华为、京东等企业的实际应用中积累了成熟经验。在国防航天领域，这种经过实战检验的稳定性与可靠性，尤为关键。无论是日常运维还是重大任务保障，孪易都能快速适配复杂环境，为用户提供持续、稳定的技术支持。</p><p>数字孪生技术正逐步成为国防航天领域智能化转型的核心驱动力。孪易数字孪生IOC ProMAX版通过全场景监控、智能分析、应急协同与灵活扩展等核心功能，为国防航天用户提供了一站式的智慧运营解决方案。它不仅提升了指挥决策的精准性与效率，更通过AI与大数据技术，实现了从被动响应到主动预警的跨越。未来，随着技术的不断演进，数字孪生平台必将在国防航天领域发挥更大价值。<br/>数字孪生智能运营中心已不再是遥远的概念，而是切实助力国防航天领域提升效能与安全性的实用工具。无论是日常运维还是应急响应，孪易IOC ProMAX版都能以其实时性、智能化和灵活性，成为国防航天任务的“智慧伙伴”。</p>]]></description></item><item>    <title><![CDATA[从零到一，解锁城市治理数字孪生高效开发秘]]></title>    <link>https://segmentfault.com/a/1190000047411920</link>    <guid>https://segmentfault.com/a/1190000047411920</guid>    <pubDate>2025-11-19 18:11:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名数字孪生应用开发者，我深知在城市治理项目中，我们常常面临这样的挑战：如何快速构建一个既宏观又精细的三维城市场景？如何让业务数据与三维场景无缝联动？如何在有限的开发资源下，实现跨平台、多终端的应用适配？今天，我想和大家分享一些我在实际项目中总结出的高效开发技巧，希望能为你的城市治理数字孪生项目带来启发。</p><h2>一、快速搭建城市级场景底图，让项目赢在起跑线</h2><p>城市治理项目往往需要宏观的城市背景作为支撑。传统方式下，我们需要手动收集地理信息数据，再通过专业建模软件一点点构建，耗时耗力。现在，通过图观数字孪生平台的“端渲染城市生成插件”，我们可以一键导入全国近300个城市的基础数据，快速生成三维城市场景。<br/><strong>使用技巧</strong>：在项目初期，建议先使用平台的预设风格模板快速搭建场景原型，这样可以在最短时间内向客户展示效果。待方案确认后，再通过深度自定义功能，精细调整建筑、道路、水系等元素的视觉效果。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmUOz" alt="" title=""/></p><h2>二、精细化场景编辑，让每个细节都“活”起来</h2><p>有了宏观的城市底图，接下来就是让场景更加生动逼真。图观的“端渲染场景编辑器”支持导入各类主流格式的精细模型，更重要的是，它提供了强大的材质编辑和动态效果配置能力。<br/><strong>材质渲染技巧</strong>：利用平台的14层PBR物理材质渲染能力，我们可以为不同的建筑类型设置不同的材质属性。比如，政府建筑可以使用更庄重的材质，商业区建筑可以设置更鲜艳的色彩，通过调节金属度和粗糙度参数，让建筑群在阳光下呈现出真实的质感。<br/><strong>动态效果配置</strong>：城市治理中经常需要展示突发事件的处理过程。通过“关节编辑”和“动画编辑”功能，我们可以将外部数据与模型状态进行绑定。比如，当某个区域发生交通拥堵时，对应的模型可以自动变红警示；通过“粒子系统”，我们可以模拟火灾现场的烟雾效果，为应急指挥提供更直观的展示。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmUPX" alt="" title="" loading="lazy"/></p><h2>三、零代码快速搭建，让业务专家也能参与应用开发</h2><p>在城市治理项目中，业务逻辑往往比较复杂。图观的“零代码应用编辑器”让我们可以通过拖拽式操作，快速集成三维场景、二维图表和业务数据。<br/><strong>交互联动技巧</strong>：利用平台的“参数”机制，我们可以轻松实现多维数据的联动分析。比如，当用户在二维图表中点击某个街道的统计数据时，三维场景会自动聚焦到对应的区域，并高亮显示相关的建筑群。这种交互效果过去需要大量编码实现，现在通过简单配置就能完成。<br/><strong>多终端适配</strong>：城市治理应用需要在指挥中心大屏、领导平板、巡查人员手机等多个终端上使用。通过零代码编辑器的多设备自适应功能，我们可以为不同终端定制专属的页面布局，确保每个使用者都能获得最佳体验。</p><h2>四、低代码深度定制，满足复杂业务需求</h2><p>对于需要高度定制化的项目，图观的“低代码统一开发API”提供了最大的灵活性。这套API最大的优势在于“统一API，双核渲染”的设计理念。<br/><strong>开发技巧</strong>：在项目开始前，建议先评估客户端的硬件条件。如果客户端显卡性能较好，可以选择“端渲染”模式以获得更好的视觉效果；如果需要支持大量用户并发访问，可以选择“流渲染”模式。重要的是，无论选择哪种模式，我们只需要维护一套代码，这大大提升了开发效率。<br/><strong>API调试技巧</strong>：善用平台提供的“API调试器”，可以在真实场景中实时调试代码。我习惯先在调试器中验证核心功能逻辑，确认无误后再集成到项目中。调试器还支持将标绘数据直接生成API代码，这个功能在开发地图标注相关功能时特别实用。</p><h2>五、资产库的妙用，加速项目进度</h2><p>图观平台预置了上万的模型、材质和特效资源，这些都是我们可以直接利用的宝贵财富。<br/><strong>使用建议</strong>：在项目启动阶段，先浏览平台的案例库和资产库，往往能找到可以直接复用或稍作修改就能使用的资源。特别是在开发智慧交通、智慧安防等常见城市治理场景时，平台提供的行业模板能帮我们节省大量前期准备工作时间。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmXgO" alt="" title="" loading="lazy"/></p><h2>实战心得：让技术真正服务于业务</h2><p>经过多个城市治理项目的实践，我最大的体会是：技术最终要服务于业务需求。图观平台的价值不仅在于提供强大的技术能力，更在于它降低了数字孪生应用的门槛，让开发团队能够更专注于业务逻辑的实现，而不是陷在技术实现的细节中。<br/>记得在一个智慧城管项目中，我们利用平台的零代码能力，让业务专家直接参与到了应用配置过程中。他们根据自己的工作经验，配置出了最符合实际工作流程的交互逻辑，这在传统开发模式下是难以实现的。<br/>另一个智慧交通项目中，我们利用统一API的双渲染能力，为指挥中心提供了高清大屏版本，同时为路面巡查人员提供了手机端轻量版本，一套代码满足多种使用场景，大大降低了项目的开发和维护成本。</p><p>数字孪生技术正在深刻改变城市治理的模式，而一个好的开发平台能让这个过程事半功倍。图观数字孪生平台通过其全流程、低门槛的特性，为我们开发者提供了强大的技术支撑。无论是快速原型搭建，还是深度定制开发，都能找到合适的解决方案。<br/>数字孪生的世界很大，让我们一起探索更多可能。</p>]]></description></item><item>    <title><![CDATA[Studio 3T 2025.21 发布]]></title>    <link>https://segmentfault.com/a/1190000047411944</link>    <guid>https://segmentfault.com/a/1190000047411944</guid>    <pubDate>2025-11-19 18:10:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Studio 3T 2025.21 (macOS, Linux, Windows) - MongoDB 的终极 GUI、IDE 和 客户端</p><p>The Ultimate GUI, IDE and client for MongoDB</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=fBT04%2BVHF9pesSn1FIK8sA%3D%3D.Gp5P8gHAbvCq3psKBF1Tuc1lozADSNjMuSUq3J%2FmSs0vIZx2Y0L14TXWj2hRJqT1" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=%2F7QwSo9E%2B7lcqr5BJ0op5g%3D%3D.yNAqDPPT7Ra%2FnpE%2FXjOqvaZDIE0hHEIP%2BnavsxIwGM8%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>Studio 3T，MongoDB 的终极 (卓越、非凡) GUI、IDE 和 客户端</p><p>适用于 MongoDB 的所有 IDE、客户端和 GUI 工具 —— 在 Atlas 上或任何地方。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432404" alt="sysin" title="sysin"/></p><p>MongoDB 的强大工具。</p><p>超过 100,000 名开发人员和数据库管理员使用 Studio 3T 作为他们首选的 MongoDB GUI</p><h2>MongoDB 客户端、GUI 与 IDE</h2><p>那么 Studio 3T 到底是什么？ 在这里，我们解释了它戴的许多帽子中的三个。</p><ul><li><p><strong>Studio 3T 作为 MongoDB 客户端</strong></p><p>客户端是允许您连接到服务器的软件程序或应用程序。尽情使用 Studio 3T 的连接管理器，根据需要连接到尽可能多的 MongoDB 服务器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432411" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB GUI</strong></p><p>图形用户界面 (GUI) 完全按照它说的去做。它提供了一个带有图形菜单、图标、对话框、向导和其他可视元素的用户界面。使用 MongoDB  GUI 的替代方法是使用 mongo shell，尽管 Studio 3T 仍然有  IntelliShell——一个易于导航的内置版本——当你需要的时候。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432412" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB IDE</strong></p><p>集成开发环境 (IDE) 将应用程序和数据库开发的许多方面整合到一个功能齐全的 “工作室” 环境中 (sysin)。Studio 3T  正是通过提供一个 GUI 来做到这一点，该 GUI 的编辑器具有自动完成和语法突出显示、内置 JSON  验证、七种语言的自动查询代码生成以及许多其他功能，可帮助您更快地工作并节省时间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432413" alt="sysin" title="sysin" loading="lazy"/></p></li></ul><h2>新增功能</h2><p><strong>2025.21.</strong>（2025 年 11 月 18 日）</p><p><strong>新增功能</strong>：</p><ul><li>IntelliShell - 为破坏性 IntelliShell 命令添加了确认提示，以防止意外数据丢失。</li></ul><p><strong>改进</strong>：</p><ul><li>OIDC - 添加了访问令牌获取的超时和中断处理，防止在忽略身份提供者（IDP）登录时线程无限期阻塞。</li></ul><p><strong>修复</strong>：</p><ul><li>偏好设置 - 修复了从许可故障排除选项打开网络代理设置时导致应用程序崩溃的问题。</li><li>连接管理器 - 修复了 X.509 认证设置中不活动的 SSL 标签链接，现在可以正常打开 SSL 标签。</li></ul><h2>下载地址</h2><p>Studio 3T 2025.21, released 2025-11-18</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=2TuScuCDc9Z9lPb%2F2DCNsA%3D%3D.9SV%2BkxGju9hUeI71Mi7i4KndLTC168LKNMFUsBLs%2BCp2jO2wjjYACN98ITW92GAr" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a></li><li>Studio 3T for macOS x64：<code>Studio-3T-&lt;Version&gt;-x64.dmg</code></li><li>Studio 3T for macOS ARM64 (Apple silicon)：<code>Studio-3T-&lt;Version&gt;-arm64.dmg</code></li><li>Studio 3T for Linux x64：<code>studio-3T-&lt;Version&gt;-linux-64.tar.gz</code></li><li>Studio 3T for Windows x64：<code>Studio-3T-&lt;Version&gt;-x64.exe</code></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=1546uQdAkmKZAJv9%2B2jh0w%3D%3D.ak8UtTlA9R4%2BcphdQnDyIDBAMXjOaLp0BQcfKKURyQY%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[使用基于用户的工作流规则为提高您的项目管]]></title>    <link>https://segmentfault.com/a/1190000047411953</link>    <guid>https://segmentfault.com/a/1190000047411953</guid>    <pubDate>2025-11-19 18:09:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="208" referrerpolicy="no-referrer" src="/img/bVdm6a4" alt="" title=""/></p><p>项目管理工具中的用户管理是指管理员控制个人如何访问和使用系统的流程和功能。它包括创建用户帐户、分配角色和权限、将团队成员组织成组或项目，以及管理他们对任务、数据和工作流程的访问权限。通过用户管理，项目经理可以确保每个人都拥有履行职责所需的适当访问权限，同时维护安全性和责任性。此功能有助于简化协作、保护项目信息，并在整个项目生命周期中支持高效的团队协调。</p><p>用户管理是项目管理中不可或缺的一部分，需要进行充分的监控。随着团队规模的扩大和项目数量的增加，手动协调更新用户和权限变得越来越困难，也更容易出错。在项目管理的过程中，项目所有者需要收集门户里面添加新用户的信息，他还需要有用户在门户中暂停或者删除的信息。 这些信息自己手动跟踪这些信息不太舒服，但是如果自动收到这些信息，项目管理的过程转为很方便。<br/>Zoho Projects 的用户自动化功能有助于避免错误，同时确保流程的统一性和一致性。使用 Zoho Projects 用户自动化功能，即可自动执行与用户相关的工作流程和 Webhook。<br/>Zoho Projects 有两中用户：门户用户和客户用户。 门户用户可以说是您的公司的成员比如说您的同事，客户用户可以说是您的客户公司的成员或者您的客户。您可以在门户里面为两中用户创建工作流规则为自动进行任何操作。 </p><p>用户自动化的优势：</p><ul><li>为新用户分配默认权限并完成入职流程。</li><li>自动发送用户更新提醒。</li><li>简化访问权限移除或帐户停用流程。</li></ul><p>用户自动化帮助管理员和经理简化用户管理，并改善项目中最终用户的体验。<br/>例如，您可以创建一个用户工作流规则，以便在团队成员的个人资料更新时收到通知。将规则设置为在用户个人资料更新时执行。添加条件“向您汇报”，并为该条件关联一个电子邮件提醒操作，以便通知您或相关用户。这样，每当向您汇报的用户的个人资料更新时，系统都会向您或选定的用户发送电子邮件。</p>]]></description></item><item>    <title><![CDATA[数智化破局：光伏储能逆变器行业CRM全场]]></title>    <link>https://segmentfault.com/a/1190000047411957</link>    <guid>https://segmentfault.com/a/1190000047411957</guid>    <pubDate>2025-11-19 18:09:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>光伏储能逆变器行业正迎来全球化扩张与技术迭代的双重机遇，作为新能源产业链的核心环节，企业面临着客户类型多元、销售周期长、跨区域协同难、售后需求专业等行业痛点。CRM（客户关系管理）系统作为数字化转型的核心工具，已不再是单纯的客户数据存储平台，而是贯穿“<strong>营销-销售-服务-决策</strong>”全链路的增长引擎，为光伏储能逆变器企业破解增长难题提供关键支撑。</p><h2>行业核心痛点与CRM适配逻辑</h2><p>光伏储能逆变器行业的业务特性决定了其客户管理的复杂性：B2B为主的业务模式中，客户涵盖经销商、安装商、项目方、终端企业等多层级；产品需适配不同国家的电网标准、认证体系，海外市场布局需求迫切；从线索获取到项目落地周期可达数月甚至数年，需全程精准跟进；售后涉及技术调试、故障排查、备件供应等专业服务，直接影响客户复购与口碑。</p><p>珍客AI CRM系统通过数字化手段，将分散的客户资源、销售数据、服务记录整合为统一资产，精准匹配行业需求：解决客户信息碎片化问题，实现全生命周期可视化管理；打通跨部门、跨区域协作壁垒，提升业务响应效率；通过数据沉淀与分析，让营销、销售决策更具针对性，助力企业在激烈的市场竞争中抢占先机。</p><h3>1. 客户分层管理：AI赋能精准画像与价值洞察</h3><p>光伏储能逆变器企业的客户群体差异显著，需求痛点各不相同。珍客AI CRM系统按客户类型（经销商 / 项目方 / 终端用户）、合作阶段（潜在 / 意向 / 成交 / 复购）、采购规模、区域市场、产品偏好等维度进行精细化分层，建立完整的客户画像。<br/>针对海外经销商，记录其所在区域的认证要求、渠道覆盖范围、回款能力等关键信息，适配不同国家的合规标准与合作政策；<br/>对大型光伏项目方，关联项目备案信息、技术参数需求、招标进度等，实现项目全流程追踪；<br/>为终端企业客户标注产品使用场景（工商业储能 / 户用储能 / 电站配套）、维护周期、历史故障记录，便于精准推送服务与升级方案。</p><p>通过AI算法整合客户采购历史、技术反馈、区域合规需求、互动记录等多维度数据，自动生成动态更新的360°全景视图。利用NLP技术分析客户邮件、会议纪要等非结构化数据，识别组织决策链关键人（如技术负责人、采购决策者）及核心诉求（如海外客户的认证标准偏好、项目方的交付周期要求）。基于深度学习模型自动计算客户价值评分与合作风险等级，精准区分战略客户与潜力客户，为资源倾斜提供数据支撑。</p><p><img width="723" height="303" referrerpolicy="no-referrer" src="/img/bVdmUkT" alt="珍客AI CRM 客户管理" title="珍客AI CRM 客户管理"/></p><h3>2. 营销自动化：AI驱动线索精准捕获与内容个性化</h3><p>光伏储能逆变器行业的营销需兼顾品牌曝光与线索转化，尤其依赖行业展会、技术研讨会、线上内容营销等渠道。珍客AI CRM系统实现营销活动的全流程数字化管理，提升获客效率与精准度。<br/>整合线上线下营销渠道数据，自动捕获展会登记、白皮书下载、官网咨询等线索，通过标签化分类筛选高意向客户；<br/>针对不同区域、不同客户层级，自动推送个性化营销内容，如海外市场的本地化技术案例、政策解读、产品升级通知等，强化客户粘性；<br/>追踪营销活动效果，通过数据反馈优化渠道投入，比如分析不同展会的线索转化率、线上推广的获客成本，让营销预算向高价值渠道倾斜。</p><p>在AI技术的驱动下，珍客AI CRM系统通过AI线索评分模型，自动分析展会登记、白皮书下载、官网咨询等行为数据，结合关键词识别（如“户用储能逆变器”“欧盟CE认证”）筛选高意向线索，优先分配给对应领域销售人员。借助AIGC技术，根据客户所在区域、行业场景自动生成本地化营销内容，如海外市场的多语言技术案例、区域政策解读、产品升级方案等，替代60%以上的人工文案工作。通过AI算法分析不同营销渠道的ROI，动态优化预算分配，聚焦高转化率的展会或线上推广渠道。</p><p><img width="723" height="345" referrerpolicy="no-referrer" src="/img/bVdmUkS" alt="珍客AI CRM 线索评分" title="珍客AI CRM 线索评分" loading="lazy"/></p><h3>3. 销售全流程管控：AI缩短周期与提升赢单率</h3><p>光伏储能逆变器产品的销售流程复杂，涉及方案设计、报价、招投标、合同签订、交付跟进等多个环节，珍客AI CRM系统实现全流程可视化管控，降低沟通成本与流失风险。<br/>线索分配自动化，根据销售人员的区域分工、专业领域（如户用储能 / 大型电站）自动分配客户资源，避免线索闲置；<br/>销售漏斗实时更新，清晰呈现各阶段客户数量与推进进度，管理人员可及时发现卡壳环节，提供针对性支持（如技术方案优化、商务谈判协助）；<br/>合同与订单关联管理，自动记录产品型号、交付周期、付款条款等关键信息，同步至库存与生产部门，实现销售与供应链的协同联动。</p><p>在有了AI的加持，珍客AI CRM系统搭建AI赢单概率预测模型，整合12项核心指标（需求明确度、预算匹配度、竞品接触情况等），实时计算商机成交概率，自动触发跨部门协同任务（如研发输出定制化技术方案、生产同步交付周期），使项目周期缩短20%-30%。智能报价系统结合原材料价格、汇率波动、区域税费等实时数据，自动生成最优报价方案，适配全球100+币种与合规要求。招投标阶段，AI通过NLP技术解析标书关键词，快速匹配产品优势与技术参数，自动生成标书框架，提升标书制作效率70%。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmJbE" alt="珍客AI CRM 销售全流程管理" title="珍客AI CRM 销售全流程管理" loading="lazy"/></p><h3>4. 售后与服务升级：AI实现预测性维护与智能响应</h3><p>光伏储能逆变器的使用周期长，售后技术支持与维护服务是客户留存的关键。CRM 系统可打造标准化、高效化的售后服务体系，提升客户满意度与复购率。<br/>建立统一的服务工单系统，客户通过邮件、电话、线上平台提交的技术咨询、故障报修等需求，自动生成工单并分配给对应技术人员，全程追踪处理进度；<br/>记录客户的产品安装时间、维护记录、备件更换情况，基于数据预判维护需求，主动推送巡检提醒与保养建议，实现从 “被动响应” 到 “主动服务” 的转变；<br/>整合技术知识库，将常见故障解决方案、产品操作指南、区域认证标准等上传至系统，方便销售人员与技术人员快速查询，提升服务专业度。</p><p>珍客AI CRM系统与设备数据打通，通过IoT设备采集逆变器运行数据（电压、温度、功率转换效率等），结合机器学习算法构建故障预测模型，提前30-48小时预警潜在故障（如IGBT模块老化、散热系统异常），自动触发预防性维护工单。AI客服机器人通过NLP技术理解客户故障描述，快速匹配技术知识库中的解决方案，24小时响应常见咨询，将人工介入率降低50%以上。智能工单分配系统结合技术人员的专业领域（如大型电站逆变器、户用储能系统）、地理位置与负载情况，自动匹配最优服务资源，使故障响应时间从24小时缩短至4小时。</p><p><img width="723" height="328" referrerpolicy="no-referrer" src="/img/bVdmXcZ" alt="珍客AI CRM 智能服务管理" title="珍客AI CRM 智能服务管理" loading="lazy"/></p><h3>5. 数据驱动决策：AI赋能趋势预判与战略优化</h3><p>随着新能源政策的持续加码与全球化竞争的加剧，光伏储能逆变器企业需基于数据快速调整战略。珍客AI CRM系统的数据分析功能，可挖掘客户行为规律与市场趋势，为企业决策提供科学支撑。<br/>分析客户采购周期、复购频率、产品偏好，优化产品研发与库存布局，比如针对高需求区域加大某类逆变器的产能倾斜；<br/>监控不同区域、不同渠道的销售业绩，识别优势市场与潜力市场，辅助制定全球化扩张策略；<br/>通过客户流失预警模型，及时发现客户合作风险（如长期无互动、竞品接触），并触发挽留机制，降低客户流失率。</p><p>珍客AI CRM系统通过实时监测全球政策动态、区域装机量数据、客户采购周期，预测未来3-6个月的市场需求，为产能规划与库存优化提供精准支撑（如向高需求区域倾斜某类逆变器产能）。AI客户流失预警模型自动识别风险信号（如长期无互动、竞品接触、付款逾期），生成针对性挽留方案（如推送升级产品信息、优化服务条款），降低客户流失率。借助数字孪生技术，模拟不同区域市场的业务拓展效果，为全球化布局与渠道优化提供数据支撑，使决策效率提升40%。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmUkU" alt="珍客AI CRM 数据分析" title="珍客AI CRM 数据分析" loading="lazy"/></p><h2>结语</h2><p>在光伏储能逆变器行业向数字化、全球化深度转型的背景下，珍客AI CRM系统已成为企业整合资源、提升效率、构建核心竞争力的关键工具。它不仅解决了行业客户管理分散、流程繁琐、响应滞后等痛点，更通过全链路的数字化赋能，让企业实现从“粗放式增长”到“精细化运营”的转变。</p><p>对于光伏储能逆变器企业而言，选择适配行业特性的CRM解决方案，意味着抢占了数字化时代的增长先机，能够在激烈的市场竞争中精准对接客户需求、高效推进业务落地、持续沉淀客户价值，为企业的长期发展注入持久动力。</p>]]></description></item><item>    <title><![CDATA[Gemini 3.0 发布，Antigr]]></title>    <link>https://segmentfault.com/a/1190000047411964</link>    <guid>https://segmentfault.com/a/1190000047411964</guid>    <pubDate>2025-11-19 18:08:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>昨天，谷歌不开任何发布会，直接甩出了一枚重磅炸弹——Gemini 3.0。</p><p>这一波更新来得猝不及防。<a href="https://link.segmentfault.com/?enc=OmzfeedUwHGYE%2BlAkSdJJA%3D%3D.0m4kDYHhNjTL3fcp7vWSrWGPqNhTjIrRYqT11iGdw7RsHgD%2FUEeKS9Xd%2B8lfk53C" rel="nofollow" target="_blank">Gemini 3.0</a> 不仅第一时间登陆了 AI Studio 和 Gemini CLI，还直接渗透到了开发者最常用的工具链里：Cursor、GitHub Copilot、JetBrains 全家桶，以及 Cline。甚至连谷歌自家的一系列产品，今天起也都集成了 Gemini 3 Pro 预览版。</p><p><img width="723" height="410" referrerpolicy="no-referrer" src="/img/bVdm6a8" alt="image.png" title="image.png"/></p><p>伴随模型发布，谷歌还掏出了一个全新的开发平台，<strong>Google Antigravity</strong>。谷歌说这是 VS Code 的分支，但它的野心显然不在于做一个编辑器，而是试图将开发模式从编写代码转向任务导向的一次尝试。</p><h3>Gemini 3.0 多项基准测试碾压一众模型</h3><p>在深入了解 Antigravity 之前，先来了解一下 Gemini 3.0 的三个主要技术特点。</p><h4><strong>逻辑推理能力的提升</strong></h4><p>Gemini 3 Pro 在 LMArena 等基准测试中取得了 1501 的高分，显示出接近博士水平的逻辑推理能力。</p><p>此外，谷歌还引入了 <strong>Gemini 3 Deep Think</strong> 模式。与普通版的快速响应不同，Deep Think 模式类似于人类的慢思考，在回答前会进行深度的思维链推导，专门用于解决数学、科学及复杂逻辑问题。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6bb" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>从生成内容到生成界面</strong></h4><p>Gemini 3.0 引入了 Generative UI（生成式界面）。传统的 AI 问答通常返回文本或代码片段，而 Gemini 3.0 支持生成完整的交互式界面。例如查询贷款计算方式时，模型可以直接构建一个包含滑块和输入框的计算器应用界面，而非仅仅列出计算公式。</p><h4><strong>对抽象风格的理解</strong></h4><p>新版本强调了对Vibe Coding（氛围感编程）的支持。模型能够理解较为抽象、模糊的需求描述。开发者无需提供法律条文般严谨的指令，只需描述想要的设计风格（如赛博朋克风、故障艺术感），模型即可将其转化为具体的代码实现。</p><h3>重点解析：Antigravity 与任务导向型开发</h3><p>Antigravity 是此次发布的重头戏。据说这是谷歌基于 VS Code 开发的分支版本，谷歌的亲儿子，但其核心逻辑发生了根本性转变。</p><h4><strong>从文件导向到智能体导向</strong></h4><p>传统 IDE 的工作流围绕文件展开：打开文件、编写代码、手动运行。Antigravity 的工作流则围绕智能体（Agent）展开。其核心理念是让开发者从繁琐的实现细节中抽离，转向更高层次的任务管理。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6bb" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>Antigravity 的工作机制：</strong></p><ol><li><strong>任务下发</strong>：开发者以自然语言描述完整需求，例如“构建一个航班追踪器，界面简洁并支持实时更新”。</li><li><strong>任务拆解</strong>：平台内置的智能体自动将需求拆解为具体的技术步骤。</li><li><strong>全链路执行</strong>：Antigravity 深度集成了 <strong>Gemini 2.5 Computer Use</strong> 模型。这使得智能体不仅具备编写代码的能力，还拥有浏览器自动化操作的能力。智能体可以编写代码，随后自动打开浏览器进行测试，模拟点击、输入，并在发现错误时自动返回编辑器修正代码。</li><li><strong>本地与云端协同</strong>：配合最新的 Nano Banana 技术，整个开发过程在本地环境与云端资源之间进行调度。</li></ol><p>这种模式将开发者的角色从代码录入者转变为任务指挥官。</p><h3>对初级开发岗位的冲击与转型</h3><p>Antigravity 展示了一种可能性，那就是基础的代码编写、测试和调试工作，正逐渐被 AI 接管。对于技能仅限于将需求直译为基础代码，或依赖网络搜索复制粘贴的初级程序员而言，职业空间确实面临压缩。</p><p>然而，这并不意味着程序员这一职业的消亡，而是职能的向上迁移。未来的开发趋势指向懂代码的架构师。</p><p><strong>开发者的新核心竞争力：</strong></p><ul><li><strong>代码审查能力</strong>：AI 能够生成代码，但也可能产生幻觉或逻辑漏洞。开发者必须具备阅读和审查 AI 产出的能力，以确保系统的安全性与稳定性。</li><li><strong>任务拆解与指令工程</strong>：Antigravity 的执行效率取决于指令的清晰度。如何将模糊的业务需求转化为 AI 可精准执行的技术任务，将成为关键技能。</li><li><strong>系统架构思维</strong>：AI 擅长执行具体的战术任务（如编写函数），但在宏观的战略布局（如高可用架构设计）上仍需人类把控。</li></ul><p>初级程序员应当减少对语法细节的死记硬背，转而投入到系统设计原理与调试逻辑的学习中。</p><h3>快速构建本地开发环境</h3><p>对于希望第一时间体验 Gemini 3.0 新特性（特别是 Gemini CLI）的开发者来说，配置基础环境往往是第一道门槛。<a href="https://link.segmentfault.com/?enc=1FQWxqAFD1Tzrbd9PouUwQ%3D%3D.nQujUW8Pfx4t5I4clAB%2B2MIfnZSIItd5%2FzjOQVLP65c%3D" rel="nofollow" target="_blank">Node.js 环境</a>的配置、版本管理常常耗费大量精力。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdm6bd" alt="image.png" title="image.png" loading="lazy"/></p><p>此时，ServBay 是一个高效的解决方案。ServBay 专为开发者设计，旨在简化本地开发环境的部署流程.</p><ul><li><strong>环境配置</strong>：支持<a href="https://link.segmentfault.com/?enc=%2BuGVih3Cr9mVc5PNvCdLpg%3D%3D.ph0ckyRknOaBp%2FaqisnlfzP9C0HfHnYn1Vt2N4dT%2F8y6DK%2FWgcKEh8YVHlECizAA" rel="nofollow" target="_blank">一键部署 Node.js 环境</a>，无需处理复杂的环境变量和版本冲突，方便开发者快速运行 Gemini CLI。</li><li><strong>本地 AI 部署</strong>：ServBay 同样支持一键部署本地 AI 模型。开发者可以在本地运行 Gemma、Qwen 3 等开源模型，方便与 Gemini 3.0 进行对比测试，既满足了隐私需求，也便于低延迟调试。</li></ul><h3><strong>结语</strong></h3><p>Gemini 3.0 与 Antigravity 的出现，降低了写代码的门槛，却提高了构建软件”=的标准。工具的进化旨在释放生产力，开发者只需善用工具，从繁杂的重复劳动中脱身，专注于更有价值的创造与设计。</p>]]></description></item><item>    <title><![CDATA[分享一名海外独立开发者的 AI 编程工作]]></title>    <link>https://segmentfault.com/a/1190000047411970</link>    <guid>https://segmentfault.com/a/1190000047411970</guid>    <pubDate>2025-11-19 18:07:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p><strong>编者按：</strong> 当 AI 编程智能体宣称能自动化一切时，我们是否在工具与概念的丛林中迷失了方向，反而忘记了如何最简单、直接地解决问题？</p><p>本文的核心主张尖锐而明确：与其追逐繁杂的“智能体套件”、子智能体（Subagents）、RAG 等概念，不如回归本质 —— 选择一个强大且高效的模型，像与一位靠谱的工程师同事那样，通过简洁的对话和直觉性的协作来直接解决问题。作者直言不讳地批评了当前生态中许多“华而不实”的工具，认为它们不过是绕开模型本身低效的临时补丁，并分享了他如何用多个终端窗口和经典工具（如 tmux）实现比许多专用工具更灵活、更可控的工作流。</p></blockquote><p><strong>本文系原作者观点，Baihai IDP 仅进行编译分享</strong></p><p><strong>作者 | Peter Steinberger</strong></p><p><strong>编译 | 岳扬</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411972" alt="" title=""/></p><p>最近我没怎么在社交平台上活跃，因为我正全身心投入到最新的项目中。如今，智能体工程（Agentic engineering）已经变得非常强大，几乎能编写出我需要的 100% 的代码。然而，我却看到很多人还在费力解决本不该存在的问题，搞出一堆繁复的表演，而不是专注把事搞定。</p><p><strong>这篇文章的部分灵感来自最近在伦敦参加的“Claude Code Anonymous”活动[1]上的对话，另一部分则是因为距离我上次更新工作流已经整整一年（还是 AI 年[2]😏）。是时候做个回顾了。</strong></p><p>所有基础理念依然适用，像上下文管理这类简单内容本文不再赘述。想了解基础内容，请阅读我之前写的《Optimal AI Workflow》[3]一文。</p><h2><strong>01 我的工作背景与技术栈</strong></h2><p>我是一名独立开发者，当前开发的项目是一个约 30 万行代码的 TypeScript React 应用，外加一个 Chrome 扩展、一个 CLI 工具、一个基于 Tauri 的客户端应用，以及一个使用 Expo 的移动应用。网站托管在 Vercel 上，每次 PR 后大约两分钟就能测试新版本，其他应用尚未实现自动化部署。</p><h2><strong>02 我所使用的技术工具和处理开发任务的总体思路</strong></h2><p>我已完全改用 codex cli 作为主力工具。通常我会在一个 3x3 的终端网格中同时运行 3 到 8 个实例，它们大多位于同一目录[4]，部分实验性任务则会放在独立文件夹中。我尝试过 worktrees、PR 等方式，但总会回到当前这套配置，因为它能最快地把事情做完。</p><p>我的智能体（agents）会自行执行原子化的 Git commits[5]。为了保持相对干净的 commit 历史，我在 agent 配置文件[6]上反复迭代优化。这样一来，Git 操作更精准，每个智能体只提交它实际修改过的文件。</p><p>是的，用 Claude 你可以设置 hooks（译者注：可能是 git commit hook），而 codex 目前还不支持 hooks，但大模型极其聪明 —— 一旦它们下定决心，没有任何 hook 能拦得住[7]。</p><p>过去我曾因此被嘲讽为垃圾代码制造机[8]，如今看到并行运行智能体的做法逐渐成为主流[9]，深感欣慰。</p><h2><strong>03 模型选择</strong></h2><p>我几乎所有的开发工作都交由 gpt-5-codex 在“medium 配置”下完成。它在智能程度与速度之间取得了极佳的平衡，还能自动调节思考深度。我发觉过度纠结这些设置并无明显的回报，而且不用操心“超深度思考”（ultrathink）的感觉真的很轻松。</p><h3><strong>3.1 爆炸半径 💥</strong></h3><p>每次工作时，我都会考量“爆炸半径” —— 这个词不是我发明的，但我非常喜欢。当构思某个改动时，我基本能预判其耗时及波及的文件范围。我可以向代码库投掷多枚“小手雷”，或是一发“胖子”配几颗小炸弹。但如果你同时扔下多个大炸弹，就几乎不可能做出隔离良好的提交，一旦出错也更难回滚。</p><p>这同时也是我观察智能体运行时的一个重要指标。如果某项任务耗时超出预期，我会直接按 Esc，然后问一句“当前状态如何？”来获取任务进度，再决定是帮模型调整方向、中止任务，还是继续执行。<strong>别害怕在中途打断模型 —— 文件修改是原子性的，它们非常擅长接续未完成的工作。</strong></p><p>当我对改动的影响不确定时，会先让模型“在修改前给我几个选项”，以此评估影响范围。</p><h3><strong>3.2 为何不用 Worktree？</strong></h3><p>我始终只运行一个开发服务器。在迭代项目时，我会通过实时操作界面，一次性测试多处改动。如果为每个功能变更都创建独立的工作树（worktree）或分支（branch），会严重拖慢我的测试流程。而同时启动多个开发服务器又会带来不必要的操作负担。此外，我的项目受 Twitter OAuth 规则限制，只能注册有限数量的回调域名，这从客观上也不支持多环境并行的开发方式。</p><h3><strong>3.3 那 Claude Code 呢？</strong></h3><p>我曾经很喜欢 Claude Code，但如今实在受不了了（即便 codex 对其赞誉有加[10]）。那种语言风格、那种斩钉截铁的“绝对正确”[11]、那种测试明明失败却宣称“100%满足生产要求”的语气——实在令人无法继续。相比之下，codex 更像是那个内向但靠谱的工程师：默默推进，把事情做完。它在开始工作前会读取更多文件，因此即使是简短的提示词，通常也能精准实现我想要的效果。</p><p>在我关注的信息流中，大家已普遍认为 codex 才是当前的首选[12-13]。</p><h3><strong>3.4 codex 的其他优势</strong></h3><ul><li><strong>约 23 万的可用上下文（context），而 Claude 只有 15.6 万。</strong> 是的，如果你运气好或愿意按 API 定价付费，Sonnet 确实有 100 万上下文，但现实中 Claude 在耗尽上下文之前就已经开始胡言乱语了，所以这个超长上下文实际上并不可用。</li><li><strong>更高的 token 利用效率。</strong> 我不知道 OpenAI 做了什么不同处理，但我的上下文空间在 codex 中消耗得明显更慢。用 Claude 时我经常看到 “Compacting…” 提示，而在 Codex 中我极少触及上下文上限。</li><li><strong>消息队列（Message Queuing）。</strong> Codex 支持消息排队[14]。Claude 以前也有这功能，但几个月前改成了“消息会实时引导模型”的机制。如果我想引导 codex，只需按 Esc 再回车就能发送新消息。能同时选择“排队”或“即时干预”显然更好。我经常一次性将多个相关功能任务放入队列，它总能可靠地逐个完成。</li><li>速度。<strong>OpenAI 用 Rust 重写了 codex，效果立竿见影 —— 响应速度快得惊人。</strong> 而用 Claude Code 时，我经常遇到数秒的卡顿，内存占用动辄飙到几个 GB。还有终端显示的闪烁问题，尤其是在用 Ghostty 时。Codex 完全没有这些问题，感觉极其轻量、流畅。</li><li><strong>语言风格。</strong> 这点对我的心理健康真的很重要[15]。我曾无数次对 Claude 大吼大叫，但很少对 codex 发火。哪怕 codex 模型能力稍弱，光凭这一点我也愿意用它。只要你两个都用上几周，就懂我在说什么。</li><li><strong>不会到处乱生成 markdown 文件</strong>[16]。懂的都懂（IYKYK）[17]。</li></ul><h3><strong>3.5 为何不选用其他开发工具</strong></h3><p>在我看来，终端用户和大模型公司之间其实没有太多中间空间。我目前通过订阅获得的性价比远高于其他方式。我现在有 4 个 OpenAI 订阅和 1 个 Anthropic 订阅，每月总花费大约 1000 美元，基本可以享受“无限 token”的使用体验。如果改用 API 调用，成本大概会高出 10 倍。别太较真这个数字——我用过像 ccusage 这样的 token 统计工具，数据多少有些不精确，但即便只是五倍，也已是相当划算的交易了。</p><p>我很欣赏像 amp 或 Factory 这样的工具，但我不认为它们能长期存活。无论是 codex 还是 Claude Code，每个版本都在变得更强，而且功能理念正在快速趋同。某些工具可能在待办列表、引导控制或细微的开发者体验（DX）上暂时领先，但我不觉得它们能真正超越大型 AI 公司。</p><p>amp 已经不再以 GPT-5 为核心驱动，转而称其为“Oracle”（神谕）[18]。而我直接使用 codex，本质上就是一直在和那个更聪明的模型——也就是“Oracle”——打交道。是的，有各种基准测试[19]，但考虑到使用场景的巨大不同，我不太信任那些结果。实际体验中，codex 给我的输出远优于 amp。不过我得承认，他们在会话共享方面确实做了些有趣的创新。</p><p>Factory？我还没被说服。他们的演示视频有点尴尬，虽然我在信息流里确实听到一些正面评价 —— 尽管目前还不支持图像（至少现在还不行），而且也有标志性的闪烁问题[20]。</p><p>Cursor……如果你还在亲手写代码，那它的 Tab 补全模型确实是业界领先。我主要用 VS Code，但确实欣赏他们在浏览器自动化和计划模式（plan mode）等方面的探索。我试过 GPT-5-Pro，但 Cursor 依然存在那些从五月起就让我烦躁的 bug[21]。听说他们正在修复，所以它还留在我的程序坞里。</p><p>像 Auggie 这样的工具，只在我的信息流上昙花一现，之后就再没人提过。归根结底，它们底层无非是封装了 GPT-5 和/或 Sonnet，完全可以被替代。RAG 对 Sonnet 或许有点用，但 GPT-5 本身在代码检索上已经强到根本不需要额外的向量索引。</p><p>目前最有希望的是 opencode 和 crush，尤其是搭配开源模型使用时。你当然也能通过它们使用 OpenAI 或 Anthropic 的订阅（得益于一些巧妙的技术手段[22]），但这是否合规仍存疑，况且为何要为一个专为 Codex 或 Claude Code 优化的模型，配上一个能力较弱的“外壳”呢。</p><h3><strong>3.6 关于开源模型</strong></h3><p>基准测试只能说明一半的问题。在我看来，智能体工程（agentic engineering）大约在 Sonnet 4.0 发布的五月，才真正从“这玩意儿真烂”迈入“这还不错”的阶段；而随着 gpt-5-codex 的出现，我们又迎来了一次更大的进步 —— 从“不错”直接进入“这简直太棒了”的境界。</p><h3><strong>3.7 计划模式（Plan Mode）与方法</strong></h3><p>基准测试所忽略的，是模型与工具在接到指令后所采取的策略。codex 要谨慎得多 —— 它会在决定行动前读取你代码库中更多的文件。当你提出一个荒谬请求时，它也更倾向于明确反对[23]。相比之下，Claude 或其他智能体会更急切地直接动手尝试。虽然可以通过“计划模式”（plan mode）和严谨的结构化文档来缓解这个问题，但对我而言，这感觉像是在给一个有缺陷的系统打补丁。</p><p>如今我几乎不再为 codex 使用大型的计划文件。其实 codex 甚至没有专门的计划模式（plan mode） —— 但它对提示词的理解和遵循能力实在太强，我只要写一句“我们先讨论一下”或“给我几个选项”，它就会耐心等待我确认后再行动。完全不需要那些花里胡哨的东西，直接跟它对话就行。</p><h3><strong>3.8 但 Claude Code 现在有插件了</strong></h3><p>你听见远处那声叹息了吗？那是我在叹气。这真是彻头彻尾的胡扯。Anthropic 的这一举动让我对他们的产品方向感到非常失望。他们试图用插件[24]来掩盖模型本身的低效。当然，为特定任务维护优质文档是个好主意 —— 我自己就在一个 docs 文件夹里存了大量有用的 Markdown 文档。</p><h3><strong>3.9 但是！子智能体呢</strong></h3><p>但关于这场“子智能体”（subagents）的盛宴，我有些话不吐不快。今年五月时，这还叫“子任务”（subtasks），主要是当模型不需要完整上下文时，把任务拆出去单独处理——比如并行执行，或避免把冗长的构建脚本塞进主上下文造成浪费。后来他们重新包装并升级为“子智能体”，让你可以带着指令“优雅地”打包并分派任务。</p><p>但使用场景本质上没变。<strong>别人用子智能体干的事，我通常用多个终端窗口就搞定了。</strong> 如果我想调研某个问题，可能会在一个终端窗格里操作，再把结果粘贴到另一个窗格。这种方式让我对上下文工程拥有完全的控制权和可见性，而子智能体反而让上下文变得难以查看、引导或控制。</p><p>还有 Anthropic 博客里推荐的那个子智能体 —— 你去看看他们那个所谓的“AI Engineer”智能体[25]。那简直就是一锅大杂烩：一边吹集成了 GPT-4o 和 o1，一边堆砌一堆自动生成的空洞词汇，试图显得有逻辑。里面根本没有能让智能体真正变成更好“AI 工程师”的实质内容。</p><p>这到底有什么用？如果你希望获得更好的输出，光告诉模型“你是一位专精于生产级 LLM 应用的 AI 工程师”是没用的。<strong>真正有用的是提供文档、示例，以及明确的“该做什么/不该做什么”。</strong> 我敢打赌，你让智能体去“搜索 AI 智能体构建的最佳实践”并加载几个网页，效果都比那堆废话强得多。你甚至可以说，这种胡扯本身就是一种上下文污染（context poison）[26]。</p><h2><strong>04 我的提示词撰写之道</strong></h2><p>以前用 Claude 时，我（当然不是手打，而是靠语音）会写非常详尽的提示词，因为那个模型“给越多上下文，越懂我”。虽然所有模型多少都这样，但我发现换用 codex 后，提示词明显变短了 —— 常常就一两句话，外加一张图。这个模型读代码库的能力极强，就是能精准理解我的意图。有时候我甚至又愿意打字了，因为 codex 根本不需要太多上下文就能明白。</p><p><strong>添加图片是个绝妙的技巧，能快速补充上下文。</strong> 模型非常擅长精准定位你截图中的内容 —— 无论是字符串还是界面元素，它都能迅速匹配并跳转到你提到的位置。我至少有一半的提示词都包含截图，虽然添加标注效果更佳但效率更低，而直接拖拽截图到终端仅需两秒。</p><p>带语义纠错的 Wispr Flow[27] 仍是当前最优方案。</p><h2><strong>05 Web 端智能体新体验</strong></h2><p>最近我又重新尝试了一些 Web 端智能体：Devin、Cursor 和 Codex。Google 的 Jules 界面美观，但配置流程繁琐，且 Gemini 2.5 现在已经算不上好模型了。不过一旦 Gemini 3 Pro 上线[28]，情况或许会有所转变。目前唯一留下来的只有 codex web。虽然它也存在配置复杂的问题，而且现在还有 Bug（终端目前就无法正确加载），但我靠一个旧版环境让它跑起来了，代价是启动速度更慢。</p><p>我把 codex web 当作临时的问题追踪器。在外突发灵感时，就用 iOS App 发一条一行字的提词词，回头在 Mac 上再仔细处理。当然，我完全可以在手机上做更多事，比如审查、合并代码，但我刻意保持克制。我的工作已经够让人上瘾了，所以当我出门或和朋友聚会时，不想被进一步拉回工作状态。说这话的人，可是曾花将近两个月专门开发了一款便于使用手机编程的工具啊。</p><p>codex web 上的任务原本不计入使用额度，可惜这样的好日子恐怕快到头了。</p><h2><strong>06 The Agentic Journey</strong></h2><p>聊聊那些工具吧：Conductor[29]、Terragon[30]、Sculptor[31] 等数以千计的同类产品。有些是个人爱好项目，有些则被 VC 投来的钱淹得喘不过气。我试过太多太多，没一个能让我长期用下去。在我看来，它们都是在绕开当前模型的低效，推行一种并不真正高效的工作流。而且大多数还藏起终端，不让你看到模型的全部输出。</p><p>绝大多数不过是 Anthropic SDK 的浅层封装 + 工作树管理，毫无技术护城河可言。我甚至怀疑：我们真的需要在手机上更方便地调用编程智能体吗？这些工具的有限应用场景，现在 codex web 已经完全覆盖了。</p><p>不过我确实观察到一个普遍现象：几乎每个工程师都会经历一个“自己造工具”的阶段 —— 主要是因为好玩，也因为现在做这件事确实太容易了。既然如此，还有什么比造一个“（我们以为）能让造工具变得更简单的工具”更自然呢？</p><h2><strong>07 但 Claude Code 能处理后台任务！</strong></h2><p>确实如此。<strong>codex 目前缺少一些 Claude 有的小功能，其中最让人头疼的就是后台任务管理。</strong> 虽然理论上应该有超时机制，但我确实多次遇到它卡在不会自动结束的 CLI 任务上，比如启动开发服务器，或者死锁的测试。</p><p>这曾是我一度切回 Claude 的原因之一。但鉴于那个模型在其他方面实在太不靠谱，我现在改用 tmux。tmux 是一个老牌工具，能在后台持久化运行 CLI 会话，而且模型里早就内置了大量相关知识 —— 你只需要说一句“用 tmux 运行”，就能搞定，无需任何复杂的智能体配置流程。</p><h2><strong>08 那 MCPs 呢？</strong></h2><p>关于 MCP（Model Context Protocol），其他人已经写了很多。在我看来，大多数 MCP 本质都只是市场部门用来打勾炫耀的工具。几乎所有 MCP 其实都应该做成 CLI。这话出自一个自己写过 5 个 MCP[32] 的人之口。</p><p>我可以直接按工具名字调用一个 CLI，根本不需要在 agent 配置文件里写任何说明。模型第一次调用时可能会试一些乱七八糟的命令（$randomcrap），CLI 会自动返回帮助菜单，上下文立刻就拥有了完整的使用信息 —— 从此一切顺利。我不用为任何工具付出额外代价，而 MCP 却是持续的成本，还会污染我的上下文。试试 GitHub 的 MCP，瞬间吃掉 23k tokens。好吧，他们后来优化了 —— 刚上线时可是接近 5 万 tokens！换成 gh CLI 呢？功能基本一样，模型本来就认识它，还完全不用交“上下文税”。</p><p>我自己开源了一些 CLI 工具，比如 bslog[33] 和 inngest[34]。</p><p>我现在确实在用 chrome-devtools-mcp[35] 这个工具来做最终验证[36]，它已经取代了 Playwright，成为我进行网页调试时的首选 MCP 工具。虽然我不常用它，但一旦需要，它就能帮我完成从“代码修改”到“验证结果”这个关键闭环，非常有用。我还专门设计了我的网站，让模型能通过 curl 查询任意接口（通过我生成的 API key）——这在几乎所有场景下都比 MCP 更快、更省 token。所以就连这个 MCP，我也不是每天都需要。</p><h2><strong>09 但生成的代码太糟糕了！</strong></h2><p>我约 20% 的时间[37]投入在重构上。当然，这些全由智能体完成，我绝不会手动浪费时间干这种事。当我不太需要高度专注或感到疲惫时，“重构日”就特别有用 —— 即使状态一般，也能取得显著进展。</p><p>典型的重构工作包括：用 jscpd 找重复代码，用 knip[38] 清理死代码，运行 eslint 的 react-compiler 和弃用插件（译者注：一类 ESLint 插件，用于检查代码中是否使用了已过时的 API、方法或特性，并提示你改用现代、推荐的替代方案。），检查是否有可合并的 API 路由，更新文档，拆分过大的文件，为复杂逻辑补充测试和注释，更新依赖项，升级工具链，调整目录结构，找出并重写慢测试，引入现代 React 模式（比如你可能根本不需要 useEffect）等等。总有做不完的事。</p><p>有人可能会说这些应该在每次提交时就做完。但我发现，先快速迭代、再集中维护和优化代码库——即阶段性偿还技术债务——这种方式不仅效率更高，而且整体上有趣得多。</p><h2><strong>10 你采用规范驱动开发（spec-driven development）吗？</strong></h2><p>我去年六月还在用这种方式：先写一份详尽的规格文档，然后让模型去实现，理想情况下能连续跑上好几个小时。但现在我觉得，这种“先设计后构建”的思路已经是过时的软件开发范式了。</p><p>我现在的做法通常是：<strong>先直接和 codex 展开讨论，贴一些网站链接、初步构想，让它解读现有代码，然后我们一起把新功能逐步梳理出来。如果问题比较棘手，我会让它把思路整理成一份规范文档，然后交给 GPT-5-Pro（通过 chatgpt.com）做评审，看看是否有更好的建议 —— 出乎意料的是，这经常能大幅优化我的方案！接着，我会把其中我觉得有用的部分粘回主上下文，用于更新实际文件。</strong></p><p>现在我对不同任务消耗多少上下文已经有不错的直觉，而 codex 的上下文容量也相当充足，所以很多时候我干脆直接开干。有些人很“虔诚”，总喜欢为每个新计划新开一个上下文窗口 —— 我觉得这在 Sonnet 时代还有点用，但 GPT-5 处理长上下文的能力强得多，如果还这么做，每次都会白白多花 10 分钟，因为模型得重新慢慢加载所有构建功能所需的文件。</p><p>更有趣的方式是做基于 UI 的开发。我经常从一个非常简单的东西开始，故意把需求写得极其模糊，然后一边看模型编码，一边在浏览器里实时看到效果。接着我再排队加入更多调整，逐步迭代这个功能。很多时候我自己也不确定最终该长什么样，这种方式让我能边玩边试，看着想法慢慢成形。有时 codex 甚至会做出一些我根本没想到但很妙的设计。我从不重置进度，只是一步步迭代，把混沌慢慢塑造成我觉得对的形状。</p><p>开发过程中，我也常会冒出一些关联功能的新点子，顺势对其他部分也做些调整 —— 这部分工作我会放到另一个智能体里处理。通常我主攻一个核心功能，同时并行处理一些次要但相关的任务。</p><p>就在我写这段文字时，我正在给 Chrome 扩展开发一个新的 Twitter 数据导入器，为此我正在重构 graphql 导入模块。因为还不确定这个方案是否合理，我把这部分代码放在一个单独的文件夹里，这样可以通过 PR 预览来判断思路是否成立。主仓库则在做重构，让我能专心写这篇文章。</p><h2><strong>11 请分享您的斜杠命令！</strong></h2><p>我只有少数几个斜杠命令，而且很少用：</p><ul><li>/commit（自定义说明文本，用于协调多智能体在同一目录协作时仅提交自身修改。这样能保持提交信息干净，也能防止 GPT 因看到其他改动而 panic，比如 linter 报错时乱 revert（译者注：Git 版本控制中的常用术语，撤销某次或某几次提交（commit）所引入的更改。））</li><li>/automerge（一次处理一个 PR：响应机器人评论、回复、等 CI 通过后自动 squash 合并（译者注：Git 版本控制中的常用术语，将多个连续的提交记录合并成一个单一的、干净的提交。））</li><li>/massageprs（和 automerge 类似，但不用 squash，方便在有大量 PR 时并行处理）</li><li>/review（内置命令，偶尔用 —— 因为 GitHub 上已有 review bot，但有时还是有用）</li></ul><p>即便如此，大多数时候我其实就直接打 “commit” 两个字。除非我知道当前有太多脏文件，担心智能体在没有引导的情况下出错。如果我确信简单指令就够了，就绝不会搞那些花哨的表演或浪费上下文。这种直觉是慢慢练出来的。到目前为止，我还没见过其他真正有用的斜杠命令。</p><h2><strong>12 其他实用技巧</strong></h2><p><strong>与其费尽心思写出完美的提示词去“激励”智能体完成一个长期任务，不如用点偷懒的变通方法。</strong> 比如进行大型重构时，Codex 常会在中途暂停响应。这时候，只要提前排好几条 “continue” 消息，你就可以走开，等回来时活儿就干完了。如果 codex 已经完成了任务，再收到更多消息，它也会愉快地忽略掉。</p><p><strong>每次完成一个功能或 Bug 修复后，请让模型在同一上下文中顺手写点测试用例。</strong> 这样做不仅能产出质量高得多的测试用例，还常常能暴露代码实现中的 bug。如果是纯 UI 调整，可能测试意义不大。但对于其他情况，我强烈建议这么做。AI 写测试用例总体上还是不太行，但已经比没有强多了 —— 而且说实话，你自己每次改代码都会写测试用例吗？</p><p><strong>让模型“保留你的原始意图”，并“在复杂逻辑处添加代码注释”，这对您和后续模型理解代码都大有裨益。</strong></p><p><strong>当遇到棘手难题时，在提示词中加入一些触发词</strong>，比如 “take your time”（慢慢来）、“comprehensive”（全面一点）、“read all code that could be related”（读所有可能相关的代码）、“create possible hypothesis”（提出可能的假设） —— 这些都能让 codex 解决最棘手的问题。</p><h2><strong>13 你的 Agents/Claude 配置文件是什么样的？</strong></h2><p>我创建了一个名为 Agents.md 的主配置文件，然后为它创建了一个符号链接（译者注：Linux 操作系统中一个特殊的文件，内容存储指向目标文件或目录的路径字符串），这个链接的名字叫 claude.md。我这么做是因为开发 Claude 的 Anthropic 公司没有采用和其他工具（比如 Codex）统一的配置文件命名标准。我承认这很麻烦也不理想 —— 毕竟 GPT-5 和 Claude 偏好的提示词风格差异很大[39]。如果你还没看过它们各自的提示词指南，建议现在就去读一读。</p><p>Claude 对那种 🚨 全大写咆哮式命令 🚨[40]（比如“如果你执行 X 命令，后果将极其严重，100 只小猫会死掉！”）反应良好，但这会让 GPT-5 直接崩溃（也确实该崩溃）。所以，请彻底放弃这种写法，像正常人一样用平实的语言就行。这也意味着这些配置文件很难被最优地共享。不过对我来说问题不大，因为我主要用 codex，即使偶尔让 Claude 上场，我也接受这些指令对它来说可能强度不足。</p><p>我的 Agent 配置文件目前大约 800 行，感觉就像一堆“组织创伤”留下的疤痕组织。这不是我手写的，而是 codex 自己生成的。每次出了状况，我都会让它在文件里加一条简洁备注。我应该找个时间清理一下配置文件，但尽管文件很长，它却运行得极其可靠 —— GPT-5 也确实几乎总是遵守里面的规则。至少比 Claude 以前强太多了。（当然也得承认，Sonnet 4.5 在这方面确实有进步）</p><p>除了 Git 操作说明，文件里还包含产品说明书、我偏好的命名规范和 API 模式、关于 React Compiler 的注意事项等等 —— 很多内容甚至比模型的“世界知识”还新，因为我的技术栈相当激进。我预计随着模型更新，这部分内容还能进一步精简。例如，Sonnet 4.0 当年需要大量指导才能理解 Tailwind 4，而 Sonnet 4.5 和 GPT-5 已经内置了相关知识，所以我直接删掉了所有冗余的相关说明。</p><p>文件里很大一块内容专门描述我偏好的 React 模式、数据库迁移管理策略、测试规范，以及如何使用和编写 ast-grep 规则。（如果你还不知道 ast-grep，或者没把它用作代码库的 linter，请立刻停下来，让模型帮你把它设为 Git hook，用来拦截不符合规范的提交。）</p><p>我还尝试过一种基于文本的“设计系统”，用来规定 UI 应该长什么样 —— 不过这个实验目前还没下定论。</p><h2><strong>14 那么 GPT-5-Codex 是完美的吗？</strong></h2><p>当然不是。有时候它会花半个小时重构代码，然后突然 panic，把所有改动全 revert 掉 —— 这时候你得重新运行，并像哄小孩一样安抚它：“你有足够的时间，慢慢来。” 有时它会忘记自己其实能执行 bash 命令，需要你鼓励一下。偶尔它还会用俄语或韩语回复。更离谱的是，有时候这个“怪物”一滑手，直接把内部思考过程原样扔进了 bash 终端。但总体而言，这些情况相当罕见，而它在其他几乎所有方面都强到离谱，让我完全可以忽略这些小毛病。毕竟，人类也不是完美的。</p><p>我对 codex 最大的不满是它会“丢失文本行” —— 快速向上滚动时，部分文本会莫名其妙消失。真心希望 OpenAI 把这个 Bug 放在修复清单的最顶端，因为这是目前唯一迫使我放慢操作速度的原因，就怕消息突然不见了。</p><h2><strong>15 结论</strong></h2><p>别在 RAG、子智能体（subagents）、Agents 2.0 或其他华而不实的花架子上浪费时间了。直接跟它对话，动手试，慢慢培养直觉。你和智能体合作得越多，结果就会越好。</p><p>Simon Willison 的文章[41]说得特别到位：<strong>管理智能体所需的许多技能，其实和管理工程师非常相似 —— 而这些能力，几乎全都是资深软件工程师的特质。</strong></p><p>而且没错，写出好软件依然很难。我不再亲手写代码，并不意味着我不再深入思考架构、系统设计、依赖关系、功能实现，或者如何让用户感到惊喜。使用 AI 只意味着：大家对你交付成果的期望值变高了。</p><p>PS: 本文 100% 原创手写。我热爱 AI，但也清楚有些事用老办法反而更好。保留这些笔误，保留我的声音。🚄✌️</p><p>PPS: 文章头图由 Thorsten Ball 提供[42]，特此致谢。</p><p><strong>END</strong></p><p><strong>本期互动内容 🍻</strong></p><p><strong>❓文中哪个观点你极度认同？或者，哪个地方你持保留意见？</strong>  </p><p><strong>文中链接</strong></p><p>[1]<a href="https://link.segmentfault.com/?enc=BQp08sgpYokQR1HsPU2uxA%3D%3D.sbiNKsEjBxtvAQgTqFwi4kP2rVpAV8zm3Xq3pZIzL8Xv6QPR5LtDuwbxn2lo34Wjd0RFiZGuu%2FLzezePDMjAPg%3D%3D" rel="nofollow" target="_blank">https://x.com/christianklotz/status/1977866496001867925</a></p><p>[2]<a href="https://link.segmentfault.com/?enc=JPVbY6FxxJeWPXWKjpmySQ%3D%3D.8wGAAxE%2BE2l5IkppWFcmy4AlIschuCmZ10UydkoiogqaOcKlUvlyHSZ%2FQcbcxcnkQkEYME41ENaOgo%2F8ZOXUug%3D%3D" rel="nofollow" target="_blank">https://x.com/pmddomingos/status/1976399060052607469</a></p><p>[3]<a href="https://link.segmentfault.com/?enc=MCkaZ12BJ6nwRtNGSejnVA%3D%3D.2LZGJxo4AbrwTx0sNe2M4DUgkgShtU4yxjMOclA6l6W9y6GuGnc8cWlPBgS9JQKhmY354DM2eojTpWpe6HCvnw%3D%3D" rel="nofollow" target="_blank">https://steipete.me/posts/2025/optimal-ai-development-workflow</a></p><p>[4]<a href="https://link.segmentfault.com/?enc=d1mp7EX4fS%2BcHaJ4be9B1A%3D%3D.PvpZpajasYRZDjc1yDGp7QSVWr%2BkSPAsfQzvFP3N89gwS1juUt9UoP2ZZ%2BPwZv86cZFB74Gxbj9%2FzjNiKqb5gg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977771686176174352</a></p><p>[5]<a href="https://link.segmentfault.com/?enc=O8T34knkV48vYcyzIeBqsQ%3D%3D.W11UMfK9M1GpedAS3bwk8UYxoSyKbAQLP%2BaFrDrYiPZGaxLD2fmvk5tL1Znpq%2FyYJUPTWn1c55OHB858z5vfsw%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977498385172050258</a></p><p>[6]<a href="https://gist.github.com/steipete/d3b9db3fa8eb1d1a692b7656217d8655" target="_blank">https://gist.github.com/steipete/d3b9db3fa8eb1d1a692b7656217d8655</a></p><p>[7]<a href="https://link.segmentfault.com/?enc=fratZijdKrIGvJMB386EaA%3D%3D.qirrTyrTIKb5p1shS1TFU3n3ZlpnMd43oWDf%2BqrFb5t8e353SfmFXvM3cDoOgJvVxh%2F4NSg%2BOGVzEG0Y5%2FJ0VQ%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977119589860601950</a></p><p>[8]<a href="https://link.segmentfault.com/?enc=B07qGY2mqE4L6rsyywYpRw%3D%3D.Z3mYvlYJDMnrYFF8nox8dF78ngPg%2FwM8fcVZv3GQuWbfXfGD6j%2FEehAOEcPbw9ioBAemdautYD%2FDGoVn%2F9eEnw%3D%3D" rel="nofollow" target="_blank">https://x.com/weberwongwong/status/1975749583079694398</a></p><p>[9]<a href="https://link.segmentfault.com/?enc=dUr%2FF49h9RVb7Le6ukv6%2Fw%3D%3D.UC0IhytV84jujqXpU5WFqbY4641JQ%2FLIkbqVtahkXQ%2BYG%2F4aVhEUCOhGIBGTXKTU4i0tYiIqL9tUnfSc%2FiaD%2FA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976353767705457005</a></p><p>[10]<a href="https://link.segmentfault.com/?enc=799S8u2neWiCjQBuFjnrHA%3D%3D.e5HeJ3R6MMZW5TYZBtVpntU1jjML9QIe2CWj91%2BqQ3nbO0tOvIRkmdf6bsbSiAFnEPnv8WToH%2B1IhIO47Twx4g%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977072732136521836</a></p><p>[11]<a href="https://link.segmentfault.com/?enc=0LjD0G1M3yBdR5r3XcU1Bg%3D%3D.zAdyTNXLoHF7M5ofoQ%2FnBZ7lN%2B5gisCjEX2DJp1QeDWlGIwF%2B9JW0rq4DzpB6ktReR2mNePk7CttXv4cUkbQ%2Bw%3D%3D" rel="nofollow" target="_blank">https://x.com/vtahowe/status/1976709116425871772</a></p><p>[12]<a href="https://link.segmentfault.com/?enc=8hXnE1ySSgRj70CweZCprA%3D%3D.DF7PzexZLjOgBhjoDgKGGke5VV5s%2FWayMO7YD5sNZQBnOygfHHrD%2FgLexLMsoXopULYWtNa%2BtSYhnI7hSNCRsA%3D%3D" rel="nofollow" target="_blank">https://x.com/s_streichsbier/status/1974334735829905648</a></p><p>[13]<a href="https://link.segmentfault.com/?enc=gyynes%2BC7An5rty397Thyw%3D%3D.Q5MTkbcC2RLMARiZaqZduDMMslr2IzvD6zD4DDWLhqIImUSir3aAXnzrprhsP%2FM6xWzwP9%2BA9i2RCBfYsonLlw%3D%3D" rel="nofollow" target="_blank">https://x.com/kimmonismus/status/1976404152541680038</a></p><p>[14]<a href="https://link.segmentfault.com/?enc=MB9FWpwenhdOulaQDlDrww%3D%3D.TPuUtfogMWm6CK3mDz0XtLynozlQisggbSXC4CQXfoPtpZQZeZGhD0eHJWAGUja36IlCfJ1tZQyKFkzQ88u77A%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1978099041884897517</a></p><p>[15]<a href="https://link.segmentfault.com/?enc=s3qHtvA8oTYEfRQBrqyj9Q%3D%3D.KR0iuxP5rJP718YaN%2BjRxMCnWC5eleftoMwD92bVg5C3ZImOkLvy47sPUNDXk0TP384lXK8tR3RwFJn6t1P7lA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1975297275242160395</a></p><p>[16]<a href="https://link.segmentfault.com/?enc=cnhxXPHCRxhfic43p1EIPA%3D%3D.dPxIJPPHCXOE6VsgVAQxUPS9TFCHWuVawxja%2FJ9G7FTnJcXVRYl%2FXVupxJAmODqc%2ByilcEwoB6mVgT34Iq2hRw%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977466373363437914</a></p><p>[17]<a href="https://link.segmentfault.com/?enc=oxFlIHFj%2FLiHWf%2BVvjHy7A%3D%3D.E7UsOvWnvjreSN7ExK4vIYfd6fYkP0AuagUNxLSd%2FdUfNpH9MiTqmqaH7gsoFpVIcvRRoAMWfDiMeTY3NeOiLw%3D%3D" rel="nofollow" target="_blank">https://x.com/deepfates/status/1975604489634914326</a></p><p>[18]<a href="https://link.segmentfault.com/?enc=HwRci1YpIhntNiN87tVtfg%3D%3D.dPEgCkuQJ3kfsGkkqhm7urrDF2LuY%2FQCEelFTNr2YCUm6BsO%2BMdlXI%2FVTwBve3EU" rel="nofollow" target="_blank">https://ampcode.com/news/gpt-5-oracle</a></p><p>[19]<a href="https://link.segmentfault.com/?enc=MR4%2FrNkaG4FOlwN%2F2vXeRA%3D%3D.TYWB%2BXh8YvngNOnqimD94y4Hs5pEIk7OyaUOuyUtLHBun9z0v0xZSfegwGflS6NwQqWJvzAwDL0LLq70G%2BzKHw%3D%3D" rel="nofollow" target="_blank">https://x.com/btibor91/status/1976299256383250780</a></p><p>[20]<a href="https://link.segmentfault.com/?enc=wGf2Ja%2Fuk12NkuX6R4I0Nw%3D%3D.mxXXlXm%2BOOWcS6QKrQaNvhQOEB%2BbOn9%2F48He14qmIC6u44cJcfbXIfEpLSZqBD5IrTZSSEjfGuelGw2U0o%2BnQw%3D%3D" rel="nofollow" target="_blank">https://x.com/badlogicgames/status/1977103325192667323</a></p><p>[21]<a href="https://link.segmentfault.com/?enc=0TQgdwAm%2Fu1P3YZPFy%2F6kA%3D%3D.rw5AP48aLLdjqCw%2Fe0qy2XpSvYDnfMMsit4klZ2yFxLwNUAyd1hkBetHoFaZbf6fFrITPE4O%2BpYgjBkzm729Lg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976226900516209035</a></p><p>[22]<a href="https://link.segmentfault.com/?enc=%2BFpSZU%2FTALaduRF992%2FwRg%3D%3D.QwDnB4V%2FRvpTh9FDliKl4rVfCT99UjGm2oj9Udu10r8pttuhqoNlFDsmIaInDSEZnDEoFiqnXkzgw5Y2itXW8A%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977286197375647870</a></p><p>[23]<a href="https://link.segmentfault.com/?enc=RyhUHTGktTmy%2BilDo7%2Ft0A%3D%3D.p933ZlbJKqdu8A444qbNKyMWM5S5eV5ZVn%2B2AHLp6CLq%2Bwf3pxKyqlpEgJ60wsYfNSAX1fvcozaQ%2BMhimSNUUQ%3D%3D" rel="nofollow" target="_blank">https://x.com/thsottiaux/status/1975565380388299112</a></p><p>[24]<a href="https://link.segmentfault.com/?enc=j2yIlHp%2FqGHxN7kAHY0PxQ%3D%3D.GDRnkKVYD1dxHTc1NOiaXL2x1d7X%2BN4xaUQP5JWod0TMvlHyhuA4QkbePAbcMEvGNjiMxSxZD4Xon3ePlR4cJA%3D%3D" rel="nofollow" target="_blank">https://www.anthropic.com/news/claude-code-plugins</a></p><p>[25]<a href="https://link.segmentfault.com/?enc=CEuAjTle8si29cP%2Bt8FKgg%3D%3D.fe1HBlLpSR5b2%2BTo%2FS0d9C6O3IZsN2jFasReSRrY5UXHKtVZpOlSWpRK3GfDiapweAzNxS5twOnD7Wifo22mceE7mIzd5%2BZd4HXkUtbRP2mEgc1iw04m7qhjrps5EG7l" rel="nofollow" target="_blank">https://github.com/wshobson/agents/blob/main/plugins/llm-application-dev/agents/ai-engineer.md</a></p><p>[26]<a href="https://link.segmentfault.com/?enc=hqr%2FUVNhNr%2Bey77xLyJQ1Q%3D%3D.DuvC%2FZsXJ7YFM3QBVHHZ45r1ATJivbPOiAGU4tPjSFhNryYwgK2QCPS5KW1eQBfZZZo%2FCYy1CDPwH0T1qJ0M2A%3D%3D" rel="nofollow" target="_blank">https://x.com/IanIsSoAwesome/status/1976662563699245358</a></p><p>[27]<a href="https://link.segmentfault.com/?enc=kidY0Qsm4u9q98B38Ry82Q%3D%3D.VybMEsEvYaoFzUF0hl61SZs3jZ7kJNUbrdqXwM2IUNk%3D" rel="nofollow" target="_blank">https://wisprflow.ai/</a></p><p>[28]<a href="https://link.segmentfault.com/?enc=aiVvRVa%2FOZEnyRc79a0VCA%3D%3D.bvv%2BhXhcRw48gHjy%2BLNNUMzplAmNOiATmaBmiyZDEzQ2%2FGjOuHD6Uonyk6J%2BhdgemUMQQguFp%2BdxX9YbM9CbcA%3D%3D" rel="nofollow" target="_blank">https://x.com/cannn064/status/1973415142302830878</a></p><p>[29]<a href="https://link.segmentfault.com/?enc=7ze6HS6A%2BPkgOVjimR0KEg%3D%3D.8UH605J%2F1TL7kQDqowrMqmccsQYDlFIBA1WkospXN1M%3D" rel="nofollow" target="_blank">https://conductor.build/</a></p><p>[30]<a href="https://link.segmentfault.com/?enc=IxyaB6HT22T25ZPNTPJMgQ%3D%3D.Pra7COioL0WmEOi4hBM%2F2CmyIDbN2WuBM%2FFY%2BLICxKk%3D" rel="nofollow" target="_blank">https://www.terragonlabs.com/</a></p><p>[31]<a href="https://link.segmentfault.com/?enc=LNOniWkMpSR8KJKulj5XUg%3D%3D.YaAH2aiVJuO5xJPagg6Gu0Z6w%2FCqCV%2FLZBVtM7sWOTtqjVq5HsbQSfKqE75GFDFuFk9E2tA6yarWn6lAMsWZtQ%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1973132707707113691</a></p><p>[32]<a href="https://link.segmentfault.com/?enc=9NC%2Bu0OObmO%2FF6YKY91Zfg%3D%3D.HXYqH3tIEtNWzaosZZZgM2a63zcddPbp2cbInBtUJniULmz3sRoURg8Dl8dunvv3" rel="nofollow" target="_blank">https://github.com/steipete/claude-code-mcp</a></p><p>[33]<a href="https://link.segmentfault.com/?enc=s%2FTWmC3eELINUOL6HJZQ2A%3D%3D.gvqyu%2Bk5ItpyANGpL7CjOj2K8xUKgo50x4rl3SmpRitzuou7s7abNKZIWa85tcal" rel="nofollow" target="_blank">https://github.com/steipete/bslog</a></p><p>[34]<a href="https://link.segmentfault.com/?enc=OmnTLoP6NzlUGrvwYwsVWw%3D%3D.Ue6DMSK6x8U3KiK3zDsa%2FL1iBgEyK7DkNo81VndS6VcVQIzc%2B5%2BR%2FDB17DPzf9yg" rel="nofollow" target="_blank">https://github.com/steipete/inngest</a></p><p>[35]<a href="https://link.segmentfault.com/?enc=ZbYUN5PdyWtRAG%2Fpxw5A0w%3D%3D.PB0VX4m4UlKl8cXsk8PwVI%2FRkky0Cwg7R3g42wukbWYjds4VQWVmmhvBKmekpLSl41XYxGokSVhEXAIwPaXjRQ%3D%3D" rel="nofollow" target="_blank">https://developer.chrome.com/blog/chrome-devtools-mcp</a></p><p>[36]<a href="https://link.segmentfault.com/?enc=PX84R1eRgflgBaSzD186qg%3D%3D.Na7MLCfPGyFKFrgWMv4Y1XjJ9hPyFssAhBAXEnQ%2FNTbuiwKc1Q6rMO1Q%2B485q9J8Rk6RcZaAUzHT7wAIqTz7KA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977762275302789197</a></p><p>[37]<a href="https://link.segmentfault.com/?enc=c4A07jaDnRjLZtVMLyejVQ%3D%3D.ZAqVAoRauPdDvtS3yhFcjtWL7e2oJxlpYNOKBo0BXyNpSa6YD9MLcvweEpd9PidFrhMJ17yFWq3Lns4%2BhDzjKg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976985959242907656</a></p><p>[38]<a href="https://link.segmentfault.com/?enc=6tnLCueEnrfjOY%2B7aEgSQw%3D%3D.OoZi2ycu2eFl%2FSQBMTczDaupzoZuN8nuIBdAsF49%2F10%3D" rel="nofollow" target="_blank">https://knip.dev/</a></p><p>[39]<a href="https://link.segmentfault.com/?enc=15WEv5%2BJopiX9t9fenErnA%3D%3D.%2Brqw6VEWy1aMDQSSZz7ba6Y84zZ9sSU3eOPYPr1MsvQQafcHS5s2GJPaIjG7%2FLOxrWO0yzazhKJiY7QD5PKcCm9v0f%2BN%2BfL3BnAyodxXqEg%3D" rel="nofollow" target="_blank">https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide</a></p><p>[40]<a href="https://link.segmentfault.com/?enc=sGda5%2B0KeQLJWuRuUCsNkg%3D%3D.8A5s6BqfMy5z19MRgen73h53QqKp1fFjYZqnIu3tngldrqZScPUffaARvAiko7IFTEkWr0wz2%2BE82jbWv9Z2oA%3D%3D" rel="nofollow" target="_blank">https://x.com/Altimor/status/1975752110164578576</a></p><p>[41]<a href="https://link.segmentfault.com/?enc=Ot9jmmFdaxc7bxE4K0YHMA%3D%3D.4huIH7BkofGOfRC44Fnfb2%2BQhZFf1SkuU5Pj8Nj18KfVNQa1Ws3VVdah9gRn0EVr62FZCc%2FXpYPO%2F%2Fihjd0srA%3D%3D" rel="nofollow" target="_blank">https://simonwillison.net/2025/Oct/7/vibe-engineering/</a></p><p>[42]<a href="https://link.segmentfault.com/?enc=%2F07wYwAqCwZYe7nHXaTSMQ%3D%3D.eb9p2TNdhoaqrvSQnP7sAsjiU89VT%2BYd8pk9HwqhDrOLNlKZ29DLD6OCVPGAlK1A%2BGfxc3MSErvNOLJz0876bQ%3D%3D" rel="nofollow" target="_blank">https://x.com/thorstenball/status/1976224756669309195</a></p><p><strong><em>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</em></strong></p><p><strong>原文链接：</strong>  </p><p><a href="https://link.segmentfault.com/?enc=KKTyDRZmuAm7FuB3sZ5WZw%3D%3D.ipSan3yTkpOftxGqYkj%2FrS4vrBiOXiDJgDuSVPyk8qUOg8lVoVl8BguFigm7Gy9p" rel="nofollow" target="_blank">https://steipete.me/posts/just-talk-to-it</a></p>]]></description></item><item>    <title><![CDATA[多模一库——架构简化，能力翻倍 Kaiw]]></title>    <link>https://segmentfault.com/a/1190000047411980</link>    <guid>https://segmentfault.com/a/1190000047411980</guid>    <pubDate>2025-11-19 18:06:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>何为多模？</strong></h2><p>"多模数据库"这个词在如今的数据库领域里，已经不再是个新鲜词。所谓多模数据库：目标是通过 "<strong>整合与效率</strong> "解决传统 "单一模型数据库" 的局限性，<strong>用一个系统统一管理多种结构的数据，支持统一查询接口（如一套 SQL 扩展），减少数据同步和跨库操作的成本</strong>。它把多种数据库（如 MySQL、MongoDB、Influxdb）的能力整合到一个系统里，让用户不用为不同结构的数据搭建多个数据库。</p><h2><strong>什么场景需要多模？</strong></h2><p>举个例子，企业要存工厂设备类型信息（关系型）以及工厂各类设备（如拧紧机等）采集的数据（时序型），如果用 2 个独立数据库，会面临数据孤岛、跨库查询复杂、维护成本高的问题。而 KaiwuDB 作为国内面向物联网 AIoT 场景的分布式多模数据库代表，能够支持在同一实例同时创建时序库和关系库，融合处理包括结构化数据、非结构化数据在内的多模数据。KaiwuDB 通过<strong>单一数据库系统</strong>统一管理时序数据和关系数据，一库代替多库，从而实现技术架构的简化，并帮助客户降低开发和运维复杂度及成本。</p><p><img width="723" height="246" referrerpolicy="no-referrer" src="/img/bVdm6bj" alt="" title=""/></p><p>KaiwuDB一库代替关系库+时序库</p><h3><strong>KaiwuDB 3.0 多模融合的内核</strong></h3><p><img width="723" height="431" referrerpolicy="no-referrer" src="/img/bVdm6bl" alt="" title="" loading="lazy"/></p><p>KaiwuDB 多模架构图</p><h4><strong>1. 接入层：多源数据统一收敛</strong></h4><p>• <strong>协议兼容</strong>：JDBC/ODBC（关系数据）、RESTful API（时序 / 流式数据）、MQTT（IoT 设备直连）。</p><p>• <strong>多模透明性</strong>：应用无需感知底层数据模型，统一接口适配时序 / 关系 / 非结构化 数据写入。</p><h4><strong>2. 解析优化层：跨模型查询语句解析、编译与优化</strong></h4><p>• <strong>多模 SQL 处理</strong>：</p><p>① <strong>解析器</strong>：兼容标准 SQL + 时序扩展语法（如 time_bucket 函数、窗口函数）。</p><p>② <strong>优化器</strong>：自动识别数据模型（时序 / 关系），匹配最优执行路径。动态生成跨模执行计划，支持两种核心优化策略，跨模查询性能提升 5 倍以上。</p><ul><li>Outside-In：当关联查询中的关系数据量较少时，关系数据过滤后下推至时序引擎。</li><li>Inside-Out：当关联查询中的时序数据量较少时，时序数据过滤或预聚合后返回到关系引擎进行关联。</li></ul><p>• <strong>元数据管理</strong>：统一管理时序库（TS DATABASE）与关系库元数据，支持跨模关联索引。</p><h4><strong>3. 计算引擎层：模型自适应执行</strong></h4><p>• 专属跨模计算算子，支持亿级时序数据与千万级关系数据秒级关联（如设备状态时序表 JOIN 设备属性表）。</p><p>• 专属单设备数据扫描算子，增加单设备跨模查询性能。</p><p>• 算子内并行，提升扫描和聚合性能。</p><p>• 聚合计算下推到时序引擎，从源头端压降数据量。</p><h4><strong>4. 存储管理层：混合架构适配</strong></h4><p>• <strong>分层存储策略</strong>：</p><p>① <strong>成本优化</strong>：通过冷热介质差异化存储，降低整体存储成本 30%\~60%（时序场景下，冷数据占比通常 \&gt; 80%）。</p><p>② <strong>性能保障</strong>：热数据留存于高速介质，确保实时业务（如监控告警、实时报表）的低延迟需求。</p><p>③ <strong>灵活适配</strong>：支持自定义冷热规则、迁移策略与归档周期，适配 IoT 监控、工业日志等不同场景。</p><p>• <strong>多模适配机制</strong>：</p><table><thead><tr><th><strong>数据类型</strong></th><th><strong>存储结构</strong></th><th><strong>核心技术</strong></th></tr></thead><tbody><tr><td>时序数据</td><td>列式存储</td><td>自研压缩算法、就地计算技术、自研"主键标签"机制</td></tr><tr><td>关系数据</td><td>行式存储 + 主键索引</td><td>分布式事务、并行计算技术</td></tr></tbody></table><h4><strong>5. AI 扩展层（多模增强）</strong></h4><p>支持 TensorFlow/XGBoost 模型全生命周期管理，可直接通过 SQL 调用模型推理，实现 "数据存储 - 训练 - 预测" 一体化。</p><h3><strong>核心功能特性\&amp;价值</strong></h3><p>• <strong>一体化数据管理</strong></p><p>通过<strong>单一数据库系统</strong>统一管理时序数据和关系数据，可以简化技术架构，降低开发和运维复杂度及成本。</p><p>• <strong>高效时序数据处理能力</strong></p><p>针对时序场景，<strong>自研"主键标签"机制</strong>，内置时序特色函数，提升数据库的读写性能；百万级 / 秒写入、毫秒级查询响应，支持 400 万 + 测点实时接入。</p><p>• <strong>跨模查询</strong></p><p><strong>原生跨模关联查询，一套标准 SQL 操作两种数据</strong>，无需数据迁移。3.0 通过增加高效跨模连接算子，时序算子并行处理优化，跨模查询性能提升 5-10 倍。</p><p>• <strong>资源与成本优化</strong></p><p><strong>避免异构数据库间的数据同步</strong> ，减少数据冗余；<strong>通过计算下推</strong>，减少网络传输，降低数据处理时延。</p><p>• <strong>AI 深度赋能</strong></p><p>① <strong>DB For AI</strong>：内置预测分析引擎，支持 SQL 级模型全生命周期管理，通过内置函数实现数据价值主动挖掘。</p><p>② <strong>AI For DB</strong>：提供 Agent 智能体工具，借助 MCP 协议，结合 LLM 技术，将自然语言处理与 KaiwuDB 深度结合，赋能数据库自动化运维与智能管理。</p>]]></description></item>  </channel></rss>