<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[快速上手：Chrome/Firefox/Edge 浏览器 Canvas 指纹防护实战 ToDetec]]></title>    <link>https://segmentfault.com/a/1190000047605695</link>    <guid>https://segmentfault.com/a/1190000047605695</guid>    <pubDate>2026-02-11 17:13:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在日常上网过程中，你可能听说过“浏览器指纹检测”这个概念，但具体它是怎么工作的，尤其是“浏览器Canvas 指纹”，很多人还是一头雾水。</p><p>今天，就给大家聊聊什么是 Canvas 指纹，它对隐私的威胁，以及如何在 Chrome、Edge、Firefox 上有效防护，顺便分享一些实用工具和操作技巧。</p><h3>什么是浏览器Canvas 指纹？</h3><p>简单来说，浏览器Canvas 指纹是浏览器指纹检测的一种方式。它通过浏览器渲染一段隐藏的图形（Canvas），然后读取渲染结果的像素信息。每台电脑、每个浏览器的硬件和软件环境不同，比如显卡型号、操作系统字体、驱动版本等等，这些细微差异会让 Canvas 渲染出的图像“唯一化”，从而生成一个“指纹”，可以用来追踪你的上网行为。</p><p>也就是说，即便你关闭了 Cookie，网站仍然可以通过 Canvas 指纹识别你是不是同一位访问者。听起来有点吓人对吧？这也是很多隐私保护爱好者特别关注的点。</p><h3>浏览器指纹检测的危害</h3><p>浏览器指纹检测不仅用于广告追踪，还可能被用来：</p><p>精准识别用户身份，即便切换 IP 或隐身模式也能追踪</p><p>个性化广告投放，让你的隐私被过度利用</p><p>防止访问某些网站或限制功能，比如一些服务会根据指纹限制访问次数</p><p>所以，了解 Canvas 指纹的工作原理，并学会防护，确实很必要。<br/><img width="723" height="485" referrerpolicy="no-referrer" src="/img/bVdnUzU" alt="" title=""/></p><h3>如何在 Chrome/Edge/Firefox 上防护 Canvas 指纹</h3><ol><li>使用隐私浏览器或增强隐私插件</li></ol><p>Firefox：Firefox 对隐私保护比较友好，可以在 about:config 中开启 privacy.resistFingerprinting，它会主动对 Canvas 指纹进行干扰，降低被唯一识别的风险。</p><p>Chrome / Edge：可以安装类似 uBlock Origin、Privacy Badger 的插件，有些插件提供 Canvas 指纹保护功能，会在 Canvas 渲染请求时提示你是否允许。</p><p>温馨提示：完全屏蔽 Canvas 指纹可能会导致某些网站功能异常，比如图形验证码或绘图功能。建议按需开启。</p><ol start="2"><li>修改浏览器 Canvas 行为</li></ol><p>部分浏览器插件可以随机化 Canvas 指纹，或者在读取 Canvas 数据时注入“噪声”，从而干扰指纹生成。例如：</p><p>CanvasBlocker（Firefox / Chrome）：这是一个专门防护 Canvas 指纹的插件，可以随机化你的 Canvas 输出，阻止网站准确识别你的浏览器。</p><p>Trace（Chrome / Edge）：提供多种防护选项，包括 Canvas、WebGL 和字体指纹保护。</p><p>通过这些插件，你可以在保证上网体验的前提下，有效降低 Canvas 指纹被利用的风险。</p><ol start="3"><li>使用隐身或隔离浏览模式</li></ol><p>虽然隐身模式不能完全防止 Canvas 指纹，但结合插件使用，可以大幅降低追踪成功率。此外，多账户浏览器或容器插件（Firefox 的 Multi-Account Containers）也可以隔离网站数据，避免跨站点追踪。</p><ol start="4"><li>检测你的浏览器指纹安全性</li></ol><p>防护前，最好先知道自己的浏览器有多“容易被识别”。ToDetect指纹查询：</p><p>查看自己浏览器的 Canvas 指纹信息</p><p>检测是否存在其他指纹威胁（WebGL、字体、插件等）</p><p>评估当前防护措施的效果</p><p>操作也很简单，只需访问网站，点击检测即可生成报告。这样你可以直观了解防护是否成功。</p><h3>总结</h3><p>Canvas 指纹是现代浏览器指纹检测中一个比较精准的手段，它能在无 Cookie 情况下追踪用户。想要在 Chrome、Edge、Firefox 上防护 Canvas 指纹，主要方法就是：</p><p>使用隐私浏览器或增强隐私插件</p><p>随机化或干扰 Canvas 输出</p><p>使用隔离浏览或容器模式</p><p>借助 ToDetect指纹查询工具 评估防护效果</p><p>如果你平时比较注重隐私，上述方法结合起来使用，会显著降低浏览器被跟踪的概率。毕竟，保护自己的数字足迹，是每一个现代网民都该掌握的技能。</p>]]></description></item><item>    <title><![CDATA[别再折腾配置了！OpenCloudOS推出OpenClaw“极速版”脚本 OpenCloudOS ]]></title>    <link>https://segmentfault.com/a/1190000047605697</link>    <guid>https://segmentfault.com/a/1190000047605697</guid>    <pubDate>2026-02-11 17:12:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在上篇文章（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）中，我们介绍了如何基于OpenCloudOS 9 安装配置OpenClaw，并接入企业微信等IM，让你最终拥有一位 7x24 的“AI全能助理”。一些用户看完文章后跃跃欲试，但一上手实操，却被繁琐的配置劝退了：</p><p>● “Node.js 版本不对，报错了……”</p><p>● “GitHub 连不上，插件装不下来……”</p><p>● “我想接企业微信，怎么还得手动改配置文件？”</p><p>大家的痛点，OpenCloudOS社区听到了！</p><p>为了让大家把精力从“装环境”转移到“用 AI”上，带来了 OpenClaw x OpenCloudOS 2.0 部署方案 。这一次，我们推出了一键安装脚本，用户只要一条命令就能极速体验OpenClaw。</p><h2><strong>一、 新版“极速部署脚本”做了什么？</strong></h2><p>1. 环境全自动适配 ：自动检测系统环境，帮你搞定 Node.js 24 等所有底层依赖，不再担心版本冲突。</p><p>2. 国内 IM 原生支持 ：不再需要满世界找插件。 企业微信、QQ、飞书、钉钉 ，你想用哪个，脚本直接帮你装好。</p><p>3. 网络与源优化 ：针对国内网络环境做了深度适配，下载更稳、速度更快。</p><p>简单来说：以前需要 30 分钟的手工操作，现在只需要 1 分钟等待。</p><h2><strong>二、极速版上手指南</strong></h2><h4><strong>2.1 安装 Openclaw 及IM相关插件</strong></h4><h5><strong>场景A：我全都要（推荐）</strong></h5><p>如果你希望 OpenClaw 能连接企业微信、QQ、飞书、钉钉等所有国内主流 IM，可直接运行如下脚本。</p><pre><code class="auto"># 默认完整安装所有国内 IM 插件
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash</code></pre><p>备注：因一键安装脚本会执行较多依赖并启动OpenClaw安装，所以整个安装过程大概耗时15-20min左右，中途请不要终止或推出。</p><p>一键安装脚本代码请见：<a href="https://link.segmentfault.com/?enc=onCDwfn1UjcIq9qOH7y77A%3D%3D.xblJS0304C9yxtL%2BL40pBCjtwJbentI9eUkRk%2BE4ACam%2BYv7MeEFtuWqqG%2FUQiy1mdjnL%2FdiUMWT0NPsdyUxadAsi5g2%2FO898BjgFonMhZY%3D" rel="nofollow" target="_blank">scripts/deploy\_opneclaw.sh · OpenCloudOS/web-extra - Gitee.com</a></p><h5><strong>场景B：我只需要特定渠道</strong></h5><p>如果你只想安装某个指定IM（如企业微信和QQ），不想安装多余插件，可直接运行如下脚本。安装耗时：5-10min</p><pre><code class="auto"># 安装指定IM（下方以同时安装企业微信和QQ为例）：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --plugins wecom,qq</code></pre><h5><strong>场景C：纯净版安装</strong></h5><p>如果你只需要 OpenClaw 的核心功能（比如只在终端使用，或者通过 Web 界面交互），不需要连接任何国内IM，可直接运行如下脚本。安装耗时：2-3min</p><pre><code class="auto"># 跳过所有 IM 插件安装：
curl -fsSL https://opencloudos.org/extra/deploy_openclaw.sh | bash -s -- --skip-plugins</code></pre><pre><code class="auto"># 手动打开交互命令
openclaw onboard</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605701" alt="image.png" title="image.png"/></p><h4><strong>2.2 配置 OpenClaw</strong></h4><p>OpenClaw配置流程较多，OpenCloudOS已在上篇内容（<a href="https://segmentfault.com/a/1190000047589937" target="_blank">你的 7x24 “AI 运维同事”，OC 9 + OpenClaw 部署及实战指南</a>）进行了详细展示，这篇不再赘述。唯一不同的是，如您在2.1章节中执行了指定IM安装（如企业微信、飞书等），则会在IM配置环节的列表中，看见该可选项。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605702" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>2.3 OpenClaw 运行状态确认</strong></h4><pre><code class="auto"># 查看openclaw是否在后台运行
openclaw health
# 查看模型状态，是否连上了大模型
openclaw models list
# 查看聊天通道,比如qq，企业微信等
openclaw channels list</code></pre><h2><strong>三、接入IM及实际应用演示</strong></h2><p>接下来，我们将详解如何配置企业微信、QQ、飞书、钉钉.</p><h4><strong>3.1 接入企业微信</strong></h4><p>OpenClaw 原生基本只支持国外社交软件，可以通过插件的方式来支持国内的社交软件。这里我们以企业微信为例，演示接入教程。</p><p>注意要接入企业微信有两个条件，首先你的clawdbot安装在有公网ip的机器上，2.你是企业管理员能创建APP或者机器人</p><pre><code class="auto"># 查看企业微信插件运行是否加载
openclaw plugins list | grep -i wecom</code></pre><p>企业微信插件使用目录</p><p><a href="https://link.segmentfault.com/?enc=M4unXfzxKC1wlWK0ec%2BXtA%3D%3D.bkn4FrxKLgQfkD83StSJsO9DYhe6Qa8Nv5sAwEOTSj%2FqAJYNKGq8XjjAieQedzYRJqDoQES%2FUTAQiyWh1imRfg%3D%3D" rel="nofollow" target="_blank">@marshulll/openclaw-wecom - npm</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605703" alt="image.png" title="image.png" loading="lazy"/><br/>接下来需要在企业微信里创建一个一个应用，这一步需要<a href="https://link.segmentfault.com/?enc=w24hTWU05IaZHoxFDYy7jA%3D%3D.1BTne%2FRQWyv5XVwdV2Tfkp8T4vhsgSPJgrDJNELX5u1SnYu7xQEoFMAKep0bUhkJ" rel="nofollow" target="_blank">首页 - 企业微信开发者中心</a>先在这里创建一个应用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605704" alt="image.png" title="image.png" loading="lazy"/><br/>选择个人<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605705" alt="image.png" title="image.png" loading="lazy"/><br/>配置企业微信应用相关信息</p><p>首先获取如下信息</p><p>1.  登录 <a href="https://link.segmentfault.com/?enc=Ka0Sfy1zvkOThQOkLrExXA%3D%3D.hYiLdHJNfU3Oo%2FNXMunamF56dyNJnhnjvp6PkmpefJI2%2F8tDMMrJfBxObQ1NgRp7" rel="nofollow" target="_blank">企业微信管理员后台</a></p><p>2.  在"我的企业"中查看 企业ID (CorpID)<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605706" alt="image.png" title="image.png" loading="lazy"/></p><p>3.  进入"应用管理" → 选择或创建应用</p><p>4.  在应用详情页获取：AgentId：应用ID</p><p>Secret(corpsecret)：点击"查看Secret"获取<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605707" alt="image.png" title="image.png" loading="lazy"/></p><p>5.  在"接收消息"设置中获取：Token：点击"随机获取"</p><p>EncodingAESKey：点击"随机获取"<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605708" alt="image.png" title="image.png" loading="lazy"/><br/><strong>然后在部署了openclaw的服务器上输入如下命令：</strong></p><pre><code class="auto"># 企业微信应用配置（必需）
# 这里配置的是 app 模式，可以参考插件使用指南换成bot或者both模式
openclaw config set channels.wecom.mode "app"
openclaw config set channels.wecom.defaultAccount "app"
openclaw config set channels.wecom.accounts.app.mode "app"
openclaw config set channels.wecom.accounts.app.webhookPath "/wecom/app"
openclaw config set channels.wecom.accounts.app.corpId "你企业ID"
openclaw config set channels.wecom.accounts.app.corpSecret "应用secret"
openclaw config set channels.wecom.accounts.app.agentId "你的应用ID"
openclaw config set channels.wecom.accounts.app.callbackToken "你设置的应用的token"
openclaw config set channels.wecom.accounts.app.callbackAesKey "你设置的应用的aes-key"
openclaw config set channels.wecom.enabled true
 
# 设置openclaw链接公网
openclaw config set gateway.bind lan
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p>配置应该是这样的，我只配置了app，如果你配置了bot会更丰富一点<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605709" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605710" alt="image.png" title="image.png" loading="lazy"/><br/>如上执行后，回到企业微信app管理界面，点击保存，企业微信会回发送token和AESKey去和openclaw服务器进行匹配<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605711" alt="image.png" title="image.png" loading="lazy"/></p><p>如果匹配成功界面如下<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605712" alt="image.png" title="image.png" loading="lazy"/><br/>在企业微信里找到相关应用，直接和他聊天<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605713" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到 Clawdbot 确实识别到了相关的用户和请求<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605714" alt="image.png" title="image.png" loading="lazy"/><br/>让 ClawdBot 创建一个定时任务：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605715" alt="image.png" title="image.png" loading="lazy"/><br/>可以看到确实创建完成了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605716" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.2 接入QQ</strong></h4><p>QQ更方便个人用户使用，OpenCloudOS也提供一个接入QQ的场景。</p><p>QQ插件地址<a href="https://link.segmentfault.com/?enc=SJ6GDjY%2FbiFNnizbhJjIow%3D%3D.wpMB9sbXKjNGQvEyZZyoBwtWRX8g3VJKFWb0Rruk34K5fyxqK25EpOVq%2FPnxNuYR" rel="nofollow" target="_blank">https://github.com/sliverp/qqbot#</a></p><pre><code class="auto"># 一件安装脚本已经安装了qq相关插件
# 查看当前的脚本
openclaw plugins list | grep qq
 
# 如果你开始没安装qq插件可以执行如下命令安装
openclaw plugins install https://github.com/sliverp/qqbot.git</code></pre><p>创建QQ机器人：</p><p>访问 <a href="https://link.segmentfault.com/?enc=jjyNDI%2Fqn%2F5s3fR8naIHYw%3D%3D.RiV9oCLjjuUluVMSUBPRbV9dFZX5J4z2OrqiewlgD30%3D" rel="nofollow" target="_blank">QQ 开放平台</a></p><p>创建机器人应用</p><p>获取 AppID 和 AppSecret（ClientSecret）</p><p>Token 格式为 AppID:AppSecret，例如 102146862:Xjv7JVhu7KXkxANbp3HVjxCRgvAPeuAQ<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605717" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605718" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">#方式一：交互式配置,选择 qqbot，按提示输入 Token
openclaw channels add
#方式二：命令行配置
openclaw channels add --channel qqbot --token "AppID:AppSecret"
# 示例
openclaw channels add --channel qqbot --token "102146862:xxxxxxxx"</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605719" alt="image.png" title="image.png" loading="lazy"/><br/>配置好后在qq开发平台里的，沙箱配置里先点击添加成员再扫描二维码就能和ClawdBot沟通，并安排他工作了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605720" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605721" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.3 接入飞书</strong></h4><p>飞书插件地址</p><p><a href="https://link.segmentfault.com/?enc=34Ee5DR7U2AFtU9R3bTG1Q%3D%3D.k0YXbPbj4ccvLi8igNVRuZa1vm2keV7XD93FagP5O%2BFU2SeOcuV2JBuKPfLsZ%2B2v" rel="nofollow" target="_blank">GitHub - m1heng/clawdbot-feishu</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable feishu
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install @m1heng-clawd/feishu</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605722" alt="image.png" title="image.png" loading="lazy"/><br/>飞书应用（机器人）配置</p><p>进入飞书应用中心：<a href="https://link.segmentfault.com/?enc=2s4OZ%2BC23MertWnPl7gzHg%3D%3D.JmoZSJtUuLQQSDFj6Te%2BfphdTp%2FsyvEEjTuyp64PhF4%3D" rel="nofollow" target="_blank">开发者后台 - 飞书开放平台</a></p><p>创建企业自建应用</p><p>路径： 创建应用 → 企业自建应用</p><p>  基础信息按提示填写即可（名称、描述等），完成后进入应用详情页。</p><p>配置应用权限</p><p>进入 权限管理，添加以下权限（按插件文档要求）：</p><p>必要权限</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im:message</td><td>消息</td><td>发送和接收消息</td></tr><tr><td>im:message.p2p_msg:readonly</td><td>私聊</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im:message.group_at_msg:readonly</td><td>群聊</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im:message:send_as_bot</td><td>发送</td><td>以机器人身份发送消息</td></tr><tr><td>im:resource</td><td>媒体</td><td>上传和下载图片/文件</td></tr></tbody></table><p>可直接复制如下配置直接导入</p><pre><code class="auto">{
  "scopes": {
    "tenant": [
      "bitable:app:readonly",
      "contact:user.base:readonly",
      "docx:document",
      "docx:document.block:convert",
      "docx:document:create",
      "docx:document:readonly",
      "drive:drive",
      "drive:drive:readonly",
      "im:chat:readonly",
      "im:message",
      "im:message.group_at_msg:readonly",
      "im:message.group_msg",
      "im:message.p2p_msg:readonly",
      "im:message:send_as_bot",
      "im:resource",
      "wiki:wiki:readonly"
    ],
    "user": [
      "contact:contact.base:readonly"
    ]
  }
}</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605723" alt="image.png" title="image.png" loading="lazy"/><br/>进入凭证与基础信息 页面，记录 App ID / App Secret 同步更新到 openclaw 配置中<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605724" alt="image.png" title="image.png" loading="lazy"/></p><pre><code class="auto">openclaw config set channels.feishu.appId "你的appid"
openclaw config set channels.feishu.appSecret "你的app_secret"
openclaw config set channels.feishu.enabled true
 
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605725" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605726" alt="image.png" title="image.png" loading="lazy"/><br/>然后进入事件与回调界面，订阅方式选择长链接<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605727" alt="image.png" title="image.png" loading="lazy"/><br/>之后点击右下角的添加事件</p><p>添加如下事件</p><table><thead><tr><th>权限</th><th>范围</th><th>说明</th></tr></thead><tbody><tr><td>im.message.receive_v1</td><td>接收消息（必需）</td><td>发送和接收消息</td></tr><tr><td>im.message.message_read_v1</td><td>消息已读回执</td><td>读取发给机器人的私聊消息</td></tr><tr><td>im.chat.member.bot.added_v1</td><td>机器人进群</td><td>接收群内 @机器人 的消息</td></tr><tr><td>im.chat.member.bot.deleted_v1</td><td>机器人被移出群</td><td>以机器人身份发送消息</td></tr></tbody></table><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605728" alt="image.png" title="image.png" loading="lazy"/></p><p>之后点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605729" alt="image.png" title="image.png" loading="lazy"/><br/>在飞书里直接搜索 你机器人的名字就能和他聊天了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605730" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>3.4 接入钉钉</strong></h4><p>钉钉插件地址</p><p><a href="https://link.segmentfault.com/?enc=mgfhDzZJ7kmqByBVMBhYUA%3D%3D.jJIjBFGmfKFbLPY%2BG8bZIswSWbfroMwQyn5CZfwtR61nGTFNxTnNnVGSmmePWMJSDmSucGhxYZCp2KzRdUkh6Q%3D%3D" rel="nofollow" target="_blank">GitHub - soimy/openclaw-channel-dingtalk: A dingtalk bot channel plugin for clawdbot</a></p><pre><code class="auto"># 一件安装脚本已经安装了飞书，查看飞书插件运行是否加载
openclaw channels list
# 如果没看到飞书执行如下命令
openclaw plugins enable dingtalk
openclaw gateway restart
# 如果没有安装飞书，那么执行如下命令
openclaw plugins install https://github.com/soimy/clawdbot-channel-dingtalk.git</code></pre><p>前往<a href="https://link.segmentfault.com/?enc=5Tq1vpHrgRs8uYSokC0G5w%3D%3D.5EcpqxB%2FvjaCw26FCO90oUK0%2BNw9gI3fjYDoRmRy0a8%3D" rel="nofollow" target="_blank">钉钉开发者后台</a>，使用具有管理员权限的账号进行登录。选择</p><p>创建应用-&gt;填写应用名称 和 描述 -&gt; 再点击左侧“添加应用能力” -&gt; 选择 “机器人"-&gt;完善机器人配置 -&gt; 消息接受模式选择 stream模式 -&gt; 点击发布<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605731" alt="image.png" title="image.png" loading="lazy"/><br/>然后点击坐上角的 "凭证与基础信息" 找到Client ID与Client Secret两个参数<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605732" alt="image.png" title="image.png" loading="lazy"/><br/>然后再进入发布界面，点击保存<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605733" alt="image.png" title="image.png" loading="lazy"/><br/>然后在服务器上输入</p><pre><code class="auto">openclaw config set channels.dingtalk.clientId "你的ClientID"
openclaw config set channels.dingtalk.clientSecret "你的Client Secret"
openclaw config set channels.dingtalk.enabled true
openclaw gateway restart
# 查看相关配置
openclaw channels list
openclaw config get channels.dingtalk</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605734" alt="image.png" title="image.png" loading="lazy"/><br/>接着你就可以在钉钉里搜索到你的应用并让他给你干活了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605735" alt="image.png" title="image.png" loading="lazy"/></p><p>参考链接</p><p><a href="https://link.segmentfault.com/?enc=3J%2BLGc0g4i3s9rOIm7gvAA%3D%3D.fpPlpSbELX1EFKt8LdhwEjpo6g7pf7Kx3UYnPGAszlZNEM%2FcOZofpa9Hw9aq9eXL" rel="nofollow" target="_blank">Node.js — 下载 Node.js®</a></p><p><a href="https://link.segmentfault.com/?enc=ClwCQ9mfBGQ%2BiwlZrGLRrQ%3D%3D.dFjgb0mKz6hxzaoEkHv60qC1XRaG7WNC7OrNUIIrl%2B4%3D" rel="nofollow" target="_blank">Moltbot — Personal AI Assistant</a></p><p><a href="https://link.segmentfault.com/?enc=QTKRY%2FAt0me0xOKdVE0%2F%2Fw%3D%3D.DRh5uGmND4NkEP59iOmxvO6CX8NB6hOYJdNtOn5nHWSro37GR9dS7jMUzu5vgFVM2rPjx8szL%2Fs1VMFmXLq%2BWQ%3D%3D" rel="nofollow" target="_blank">openclaw企业微信插件</a></p><p><a href="https://link.segmentfault.com/?enc=GK1N58jdxtA7rus14uH8VQ%3D%3D.LVPjTp%2B535z5rWMPTQdSx2wu4yNB7s64%2BxkLJiLZ1XA%3D" rel="nofollow" target="_blank">MoltHub</a></p><p><a href="https://link.segmentfault.com/?enc=%2FPy2DIg98xZcZDy9cT50qg%3D%3D.MlelPgfy3upjMHOwA%2FAdW%2B2xtIczDDhqlWQCarmNDDzbIXe7egJ1W1ZI1GPAebq%2F" rel="nofollow" target="_blank">https://linux.do/t/topic/1518570</a></p><p><a href="https://link.segmentfault.com/?enc=mketfBKmN7BOaXbI7k1FLw%3D%3D.RLQL0IOI2wHdPR1%2FGjFfP1aZZvXYUIz5y%2F%2FeE7t8ziRe%2BSbdC1pO8oSMDFBbRC9FCEy0u9DcpXGWKv422ESKfw%3D%3D" rel="nofollow" target="_blank">🚀 云上Moltbot（原Clawdbot）最全实践指南合辑-腾讯云开发者社区-腾讯云</a></p><p><a href="https://link.segmentfault.com/?enc=2Cf9uABs%2BjPGPIYdupiHqg%3D%3D.P7bbZOBVuc7Jn1Ek3LffY6TJoJX0gouBub%2FxR8S8adnBtBw5chqBtRHlQ2hsJnTw" rel="nofollow" target="_blank">openclaw的QQ机器人插件</a></p><p><a href="https://link.segmentfault.com/?enc=U5v7oM84FdPCDcgQK2UjyA%3D%3D.ekOuV%2BmnJn30a8VV1hF8r5XgHYnscHBoAu%2FNt0jUufkFhRjPEpsiWsrgJOh2Sr5gx7LSeYUwCTpRgKPy17m8PA%3D%3D" rel="nofollow" target="_blank">Clawdbot 全面指南 - 汇智网</a></p>]]></description></item><item>    <title><![CDATA[推理速度 10 倍提升，蚂蚁集团开源业内首个高性能扩散语言模型推理框架 dInfer 蚂蚁开源 ]]></title>    <link>https://segmentfault.com/a/1190000047605777</link>    <guid>https://segmentfault.com/a/1190000047605777</guid>    <pubDate>2026-02-11 17:11:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>蚂蚁集团开源业界首个高性能扩散语言模型（Diffusion Large Language Model，dLLM）推理框架 dInfer。<br/>在基准测试中，dInfer 将 dLLM 的推理速度相比于 Fast-dLLM 提升了 10 倍以上，并在关键的单批次（batch size=1）推理场景下，作为首个开源框架实现了大幅超越经过高度优化的自回归（AR）模型的性能里程碑，在 HumanEval 上达到 1011 tokens / 秒的吞吐量。dInfer 通过一系列算法与系统协同创新，攻克了 dLLM 的推理瓶颈，兑现了其内生并行生成带来的推理效率潜力。<br/>这不仅为开发者提供了即刻可用的高效推理框架，更标志着扩散语言模型这一全新的范式迈出了走向成熟的坚实一步。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605779" alt="图片" title="图片"/></p><ul><li>论文链接: <a href="https://link.segmentfault.com/?enc=Bd9ZlJwd0CVaI3YPoYKltA%3D%3D.ylZYHI2SwHdO%2BC9%2Ft7yOEo9Q%2BUzzS9bIT36fiMq4aMxytTi%2F2eUzW7YazI7s3ym2" rel="nofollow" target="_blank">https://arxiv.org/abs/2510.08666</a></li><li><p>项目地址: <a href="https://link.segmentfault.com/?enc=oPjzL0zsP5o5XRvZmWPwIw%3D%3D.CRR099b3Pbx6BNoxHARCwd%2FQWvLOS%2FfpPoHJAjZSPv4VutrEaNtPo7OB7oe5P%2FDH" rel="nofollow" target="_blank">https://github.com/inclusionAI/dInfer</a> </p><h2>理论的「翅膀」，现实的「枷锁」：扩散语言模型的推理困境</h2><p>近年来，以自回归（Autoregressive，AR）范式为核心的大语言模型（Large Language Models）已经取得了巨大的成功，推动了智能问答、代码生成、智能体助手等领域的重大进步。然而，AR 生成范式也存在其固有瓶颈：生成过程完全依赖前序结果，必须逐词串行生成，这导致推理延时难以降低，即使 GPU 的并行计算能力强大也无用武之地。<br/>作为一种全新的范式，扩散语言模型（dLLM）应运而生。它将文本生成视为一个 「从随机噪声中逐步恢复完整序列」的去噪过程。这种模式天然具备三大优势：</p></li><li>高度并行：理论上可以在单次迭代中，并行地预测和更新序列中的多个 token</li><li>全局视野：模型的每一步决策都基于对整个序列的全局上下文理解，而非仅依赖于已生成的部分</li><li>结构灵活：更易于适应多模态、代码生成等需要复杂结构和长程依赖的任务<br/>凭借这些优势，以 LLaDA-MoE 为代表的 dLLM 已在多个基准测试中，展现出与顶尖 AR 模型相媲美的准确性 。然而在推理效率方面，dLLM 理论上的强大潜能，却长期被残酷的现实「枷锁」所束缚。dLLM 的高效推理面临三大核心挑战：<br/>1.高昂的计算成本：多步迭代去噪的特性，意味着模型需要反复对整个序列进行计算，这带来了巨大的算力开销<br/>2.KV 缓存的失效：dLLM 中的双向注意力机制，使得 token 对应的 KV 值在每次迭代中都会改变。这导致 AR 模型中「一次计算、永久复用」的 KV 缓存技术直接失效，使得推理过程异常昂贵<br/>3.并行解码的双刃剑：尽管理论上可以并行生成序列中的所有 token，但在难以精准刻画其联合概率分布的情况下一次性解码太多 token，极易引发彼此间的语义错配，导致「并行越多，质量越差」的窘境</li></ul><p>这些瓶颈使得 dLLM 的推理速度一直不尽人意，其并行生成带来的效率沦为「纸上谈兵」。如何打破枷锁，释放 dLLM 在推理效率的潜能，成为整个领域亟待解决的难题。</p><h2>dInfer：人人可上手的扩散语言模型高效推理框架</h2><p>为彻底突破上述瓶颈，蚂蚁集团推出了 dInfer—— 一个专为 dLLM 设计的、算法与系统深度协同的高性能推理框架，可支持多种扩散语言模型，包括 LLaDA、 LLaDA-MoE、LLaDA-MoE-TD 等。</p><p>dInfer 的设计哲学是模块化与可扩展性，以系统性集成算法与系统优化。如下图所示，dInfer 包含四大核心模块：模型接入（Model）、KV 缓存管理器（KV-Cache Manager），扩散迭代管理器（Iteration Manager），和解码策略（Decoder）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605780" alt="图片" title="图片" loading="lazy"/><br/>图 1. dInfer 架构</p><p>这种可插拔的架构，允许开发者像搭乐高一样，进一步组合和探索不同模块的优化策略，并在统一的平台上进行标准化评测。更重要的是，dInfer 针对上述三大挑战，在每个模块中都集成了针对性的解决方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605781" alt="图片" title="图片" loading="lazy"/><br/>表 1. dInfer 组件</p><h2>dInfer 如何「快」起来？</h2><p>1.削减计算成本，控制生成质量：邻近 KV 缓存刷新 (Vicinity KV-Cache Refresh)<br/>dLLM 使用双向注意力机制让模型获得更全局的视野，代价是每次解码会影响所有的 token 的 KV 值，导致 AR 模型依赖的 KV 缓存技术不能直接应用到 dLLM 上。如果不使用任何 KV 缓存，在一个 sequence 上的一次 diffusion 迭代会导致大量的计算。</p><p>为了削减计算成本，Fast-dLLM 提出的将 sequence 划分为 block，然后再逐个对 block 进行解码，并在当前解码 block 之外进行 KV 缓存的方法，可以有效降低 diffusion 迭代的计算成本。然而虽然利用上了 KV 缓存，但在大部分情况下，缓存中的 KV 实际上是过时的，因此会导致生成质量的下降。</p><p>为了缓解这一问题，dInfer 采取了一种邻近刷新的策略：KV 缓存过时的原因是 dLLM 中一个新 token 的确定，会影响全局所有 token 的 KV 表示。而 dInfer 基于「语义局部性」原理（ 一个词的更新，对其近邻词的影响最大），在每次迭代解码一个 block 时，dInfer 只选择性地重新计算该区块及其邻近一小片区域的 KV，而让远处的缓存保持不变。这好比修改文档中的一句话，你只需检查上下文是否通顺，而无需重读整篇文章。<br/>这种策略结合 dInfer 的其它优化，在计算开销和生成质量之间取得了平衡，首次让 KV 缓存机制在 dLLM 上高效、可靠地运作起来。</p><p>2.系统优化：让 dLLM 的前向运算速度追上 AR<br/>在利用上 KV 缓存之后，dInfer 选择了合适的 block 大小和 Vicinity KV-Cache Refresh 的范围，并做了一系列的系统优化，以使 dLLM 一次迭代的速度能追上运行在 SOTA 的推理服务框架如 vLLM 上的 AR 模型，包括：</p><ul><li>多卡并行：结合了张量并行 (TP) 与专家并行 (EP)，即使在 batch size=1 的条件下，也能充分利用 GPU 的算力，效率提升超 100%。</li><li>编译优化：通过 torch.compile 进行内核融合并编译为 CUDA Graph 执行，消除了 PyTorch 框架的执行开销，结合上述的多卡并行，可让效率提升 200%。</li><li>消除迭代之间的气泡：采用循环展开 (Loop Unrolling) 技术，让 Python 可以连续不断地启动 CUDA 内核，消除了迭代间的 GPU 空闲气泡，带来 5-10% 的性能提升。</li><li>早停：在生成 EOS token 后，跳过后续 block 的推理过程，可以减少 5-40% 不必要的开销。</li></ul><p>3.并行解码：层级解码 (Hierarchical) 与信用解码 (Credit)<br/>为了在保证生成质量的前提下，最大化并行解码的 token 数量，dInfer 提出了两种无需额外训练的解码算法 ：</p><ul><li>层级解码 (Hierarchical Decoding)：该算法借鉴了「分治」思想，将待解码的区域不断递归地一分为二，并优先在每个子区域的中心位置解码 token 。这种方式自然地拉开了新生 token 间的距离，减少了它们之间的语义干扰 。在理想情况下，它能以近似对数级的复杂度完成多点并行生成，既快又稳 </li><li>信用解码 (Credit Decoding)：在多轮迭代中，有些正确的 token 可能很早就被模型稳定地预测出来，但因其单次置信度未能「达标」而被反复重算 。dInfer 为此引入了「累积信用」机制，持续追踪并累积每个 token 在历史迭代中的置信表现 。一个长期被稳定预测的 token，即使当前置信度稍低，也能凭借高累积信用被「破格」解码，从而有效避免了大量冗余计算</li></ul><p>4.压榨每步迭代价值：迭代平滑 (Iteration Smoothing)<br/>传统 dLLM 在每轮迭代中，只利用了置信度最高的 token 信息，而将其他位置的概率分布整个丢弃。dInfer 的迭代平滑算法，旨在回收这些被浪费的信息。</p><p>它基于未解码位置的 logits 分布得到该位置的加权 Embedding，并将其作为宝贵先验知识，平滑地融入下一轮迭代的 Embedding 中。这极大地丰富了上下文信息，使得单次迭代解码的 token 数量平均提升了 30-40%。</p><p>此外，由于 dInfer 可以无障碍地接入多种扩散语言模型，此次率先支持了基于轨迹蒸馏（Trajectory Distillation）加速 diffusion 去噪过程的 LLaDA-MoE-TD 模型，推理性能更强。</p><h2>实测数据：里程碑式的性能飞跃</h2><p>在配备 8 块 NVIDIA H800 GPU 的节点上，dInfer 的性能表现令人瞩目。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605782" alt="图片" title="图片" loading="lazy"/><br/>图 2. 评测数据</p><ul><li>10 倍性能提升：在与先前的 dLLM 推理方案 Fast-dLLM 的对比中，dInfer 在模型效果持平的情况下，平均推理速度（avg TPS）实现了 10.7 倍的巨大提升（681 vs 63.6）</li><li>超越自回归：与在业界顶尖的推理服务框架 vLLM 上运行的、参数量和性能相当的 AR 模型 Qwen2.5-3B 相比，dInfer 的平均推理速度是其 2.5 倍（681 vs 277） </li><li>突破推理极速：在代码生成任务 HumanEval 上，dInfer 在单批次推理中创造了 1011 tokens / 秒的纪录 。这是开源社区首次见证，扩散语言模型在延迟敏感的单批次推理场景下，速度显著超越经过高度优化的自回归模型</li></ul><p>更进一步，当结合轨迹蒸馏（Trajectory Distillation）技术（一种让模型学会 「跳跃式」去噪的后训练优化方法）后，dInfer 的平均推理速度飙升至 847 TPS，实现了超过 3 倍于 AR 模型的性能。</p><h2>开源开放：共建下一代 AI 推理新生态</h2><p>dInfer 的诞生，不仅是一个工具的发布，更是一次 LLM 范式的试炼：它证明了扩散语言模型的效率潜力并非空中楼阁，而是可以通过系统性的创新工程兑现，使其成为 AGI 道路上极具竞争力的选项。<br/>目前，dInfer v0.1 的全部代码、技术报告与实验配置已开源。<br/>蚂蚁希望 dInfer 能成为：</p><ul><li>研究者的标准平台：为 dLLM 领域的算法创新提供一个公平、高效的试验场 。</li><li>开发者的加速引擎：助力社区将强大的 dLLM 轻松部署到实际应用中，享受极致性能 。<br/>dInfer 连接了前沿研究与产业落地，标志着扩散语言模型从「理论可行」迈向「实践高效」的关键一步。我们诚邀全球的开发者与研究者一同加入，共同探索扩散语言模型的广阔未来，构建更加高效、开放的 AI 新生态。</li></ul>]]></description></item><item>    <title><![CDATA[新年小惊喜！龙蜥之旅五周年特辑上线啦，解锁你的社区足迹 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605785</link>    <guid>https://segmentfault.com/a/1190000047605785</guid>    <pubDate>2026-02-11 17:11:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>开源的世界里，没有微不足道的参与，只有共同成就的未来。</p><p>回望 2025，龙蜥社区始终向上突破——从技术演进到解决方案规模化落地，从高校开源教育到生态拓展，龙蜥社区交出了一份扎实而令人自豪的答卷。</p><p>而你，或许曾提交过代码、参与过活动，又或许只是默默关注、静静使用——无论以何种方式同行，你都是这段旅程中不可或缺的一份力量。正是因为你的每一次关注与信任，都为龙蜥注入了前行的动力。</p><p>值此龙蜥社区成立五周年&amp; 2026 新年来临之际，为感谢大家的一路相伴，我们特别准备了上千份精美礼品，包括龙蜥定制保温杯、限定款龙蜥卫衣、萌趣小龙抱枕、猫超卡、B 站/腾讯视频月卡等。诚挚邀请每一位关注、使用或贡献过龙蜥的朋友，打开龙蜥社区官网（openanolis.cn），一起回望 2025 年那些你在社区留下的珍贵“足迹”，重温属于你与龙蜥的共同记忆，还有精美周边领取哦。</p><p>活动时间：2026 年 2 月 10 日-3 月 31 日</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605787" alt="图片" title="图片"/><br/>（图/龙蜥开源足迹活动礼品图集）</p><h3>开启你的“龙蜥开源足迹”</h3><p>活动期间，首次登录龙蜥官网，会自动弹出“龙蜥开源足迹”，一键点击开启。若手滑退出或再次进入龙蜥官网，不用担心，可在官网顶部点击“龙蜥开源足迹”图片或官网右侧图标，都可开启活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605788" alt="图片" title="图片" loading="lazy"/></p><h3>兑换礼品</h3><p>活动截止时间：2026 年 3 月 31 日。</p><p>📌 兑换流程：开启龙蜥之旅，回顾年度开源足迹，点击抽盲盒。<br/>🎁 礼品邮寄：根据中奖提示，填写收件信息。工作人员会在 3 月起陆续安排邮寄。</p><h3>彩蛋</h3><p>除领取盲盒外，还可以下载“龙蜥开源足迹”图片，并在龙蜥公众号（搜 OpenAnolis 龙蜥）发布的这篇文章评论区留言，注意必须带“龙蜥开源足迹”图 + 评论（图片示意图如下所示；评论格式为“2026，我祝龙蜥社区...”内容不限，祝福或需求都可）。我们将按照规则送出龙蜥定制双肩包。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605789" alt="图片" title="图片" loading="lazy"/><br/>（图/“龙蜥开源足迹”示意图）</p><p>✨ <strong>送礼规则</strong></p><p>高赞有礼！我们将分别在 2 月 27 日（统计 2 月 10-26 日 23:59 的点赞数）、4月 1 日（统计 2 月 27 日-3 月 31 日 月 1- 30 日 23:59 的点赞数）各公布评论点赞排名前 10 的用户，送出龙蜥定制双肩包一个。</p><p>届时请大家及时关注龙蜥公众号（OpenAnolis龙蜥），获奖名单将在以上两个开奖日通过公众号文章的形式公布，请及时填写邮寄地址。</p><p>注意：评论须为原创，禁止发广告、拉踩等无意义内容，同一用户多条评论仅取点赞最高的一条参与当期评选。2 月已获奖的用户将不再参与 3 月的点赞排名评选。</p><p>快来一键开启你的龙蜥开源足迹吧～</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[2025 年度回顾｜龙蜥这一年：AI 领航，生态共荣 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605796</link>    <guid>https://segmentfault.com/a/1190000047605796</guid>    <pubDate>2026-02-11 17:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605798" alt="图片" title="图片"/></p>]]></description></item><item>    <title><![CDATA[如何通过设计研发协同平台实现制造业的高效创新？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047605803</link>    <guid>https://segmentfault.com/a/1190000047605803</guid>    <pubDate>2026-02-11 17:09:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业的竞争格局中，产品研发的效率与质量已成为企业核心竞争力的关键。然而，传统的研发管理模式往往面临信息割裂、流程冗长、协作低效等痛点。设计图纸版本混乱、跨部门沟通成本高、质量风险难以提前识别等问题，不仅拖慢了产品上市速度，也增加了研发过程中的隐性成本。面对这些挑战，越来越多的企业开始将目光投向设计研发协同平台，希望通过数字化手段重构研发流程，实现数据驱动的协同创新。<br/>设计研发协同平台的核心价值<br/>设计研发协同平台并非单一的工具叠加，而是一种系统性的研发管理理念的落地。它通过整合产品生命周期中的需求、设计、仿真、试制、质量等环节，构建起一个统一的数据底座和协作环境。在这个平台上，三维模型、技术文档、物料清单（BOM）等核心数据得以结构化存储和动态关联，任何变更都能实时传递到所有相关环节，从而避免因信息滞后导致的错误和返工。更重要的是，平台打破了部门壁垒，使得设计、工艺、生产、质量等团队能够在同一语境下协作。例如，设计人员发起的模型修改，工艺人员可以即时反馈可制造性意见，质量人员则能同步更新FMEA（失效模式与影响分析）中的风险控制措施。这种“设计-工艺-质量”的一体化协同，不仅加速了决策流程，也从根本上提升了产品的可制造性和可靠性。<br/>技术实现与流程重构<br/>从技术层面看，现代设计研发协同平台通常基于云原生架构，支持分布式协同和轻量化应用。通过模型轻量化技术，非设计人员（如采购或质量工程师）无需安装专业CAD软件，即可通过浏览器查看、批注甚至参与评审复杂的三维模型。同时，平台内置的流程引擎将传统的纸质审批、邮件沟通转变为自动化的工作流，任务推送、节点提醒、权限控制等功能大幅减少了人为延误。在质量管控方面，平台通过集成FMEA管理模块，将历史故障库、行业标准与实时项目数据打通。系统能够基于相似产品或设计特征，自动推荐潜在的失效模式及改进措施，从而帮助工程师在研发早期识别风险，而非事后补救。这种预防性的质量保障机制，使得平台不再是简单的文档管理系统，而是贯穿产品创新全过程的智能决策支持系统。<br/>实践案例<br/>在国内，广域铭岛旗下的Geega（际嘉）工业互联网平台已成为研发协同领域的代表性解决方案。在某汽车零部件企业的实践中，Geega平台通过统一数据源和流程集成，实现了BOM准确率提升至98%，设计变更审批周期缩短50%，零部件复用率提高30%。其特点在于深度契合中国制造业的需求，注重轻量化部署和低成本适配，尤其擅长与现有ERP、MES系统的集成，帮助企业以较低门槛实现研发数字化。相比之下，国际厂商如PTC的Windchill和西门子的Teamcenter则代表了另一种路径。PTC通过强化AR/VR与数字孪生技术的融合，使研发协同不再局限于桌面屏幕，而是延伸至车间现场和远程运维场景。例如，工程师可通过AR设备在物理原型上叠加虚拟设计模型，直接进行偏差比对和装配验证。西门子Teamcenter则依托其完整的PLM（产品生命周期管理）生态，实现了从设计、仿真到制造执行的全链条数据闭环，特别适用于大型跨国企业的多地点、多学科协同需求。尽管路径不同，这些平台都在试图解决同一问题：如何让研发更高效、更可靠、更贴近市场需求。<br/>设计研发协同平台的崛起，标志着制造业研发模式从“粗放式管理”向“精细化运营”的转变。它不仅仅是一种技术工具，更是企业重塑研发体系、构建数字驱动文化的重要契机。当数据成为新的生产要素，协同成为新的创新范式，那些率先拥抱这一变革的企业，无疑将在未来的市场竞争中占据先机。</p>]]></description></item><item>    <title><![CDATA[让 AI Agent 安全“跑”在云端：基于函数计算打造 Agent 代码沙箱 Serverless]]></title>    <link>https://segmentfault.com/a/1190000047605809</link>    <guid>https://segmentfault.com/a/1190000047605809</guid>    <pubDate>2026-02-11 17:09:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：安全沙箱与 Serverless 的技术交汇</h2><p>随着大语言模型（LLM）从“对话框”走向“行动体（Agent）”，其能力边界正在迅速扩张。现代 AI Agent 不再是文字的搬运工，而是能够自主思考、调用工具、甚至编写并运行代码以解决复杂问题的智能助手。然而开发者始终面临一个根本性挑战：如何在保证执行效率的同时，实现资源强隔离与资源可控性？<br/>阿里云函数计算 FC 为这一难题提供了全新的解题思路。其底层基于轻量级安全沙箱，天然具备进程级隔离、资源极致伸缩、按需付费等特性。这种架构与 Agent 对代码执行环境的需求高度契合，使得构建高密度、低成本、安全可靠的 Agent 运行时成为可能。</p><h2>为什么需要 Agent 代码沙箱？</h2><p>Agent 的核心价值在于其“自主执行”能力，而代码执行是实现这一能力的关键路径。在工具调用、动态数据分析、自动化任务处理等典型场景中，Agent 生成的代码往往来自不可信的推理过程，若缺乏有效的沙箱保护，开发者将面临多重风险，为此 AI 开发者对运行时有着如下多个核心诉求：</p><ul><li>安全与隔离特性：必须确保不同用户的 Agent 代码在文件系统、网络访问上完全隔离，严防恶意指令注入导致的越权操作。</li><li>资源管理控制：代码缺陷或恶意行为可能导致 CPU/内存耗尽。系统需要能够对单个执行任务进行精细化的资源配额限制。</li><li>生命周期管理：Agent 任务存在短时型突发、长周期会话等多种任务模型，需提供灵活生命周期管理能力。</li><li>按资源消耗计费：若简单按实例运行时长计费，在长周期交互场景下，用户将为大量的“等待时间”支付不必要的费用。需在用户成本控制与平台资源利用率之间寻找平衡点。</li></ul><p>由此可见，构建一个强隔离、可管控、即开即用且按需回收的代码执行环境——Agent 代码沙箱，已成为 AI 应用架构中的刚需。</p><h2>为什么是 Serverless？函数计算的核心优势</h2><p>在众多技术路线中，Serverless 函数计算凭借其天然的“沙箱基因”，成为了构建 Agent 运行时的理想底座：</p><ol><li>底层安全隔离：主流云厂商的函数计算服务普遍采用 MicroVM 或强化容器技术作为执行单元。每个函数实例运行在一个轻量级、启动迅速的 MicroVM 中，具备完整的内核隔离。这种架构从进程、内存、文件系统等多维度实现安全保障。</li><li>极致的弹性伸缩：Agent 的请求模式具有高度不确定性。函数计算的毫秒级扩缩容能力，让开发者无需担心容量规划，轻松应对从零到万级并发的波动。</li><li>按量付费的经济性：传统常驻服务无论是否处理请求，均持续产生费用。而函数计算采用“用多少付多少”的计费模式，极大降低用户成本。（下文也将介绍 AI 场景下如何实现经济计费）</li><li>简化的运维体验：函数计算将基础设施管理完全托管给云平台，开发者只需关注代码逻辑，这种“代码即服务”的模式，极大加速了 AI 业务的迭代与上线周期。</li><li>异构算力支持：针对图像处理、音视频编解码等高性能场景，函数计算成熟的 GPU 实例支持，为 Agent 提供了更广阔的技能空间。</li></ol><h2>产品化实践：基于函数计算构建沙箱能力</h2><p>为了将通用的函数计算转化为专业的 Agent 运行时，我们不仅需要底层的隔离，更需要在协议层、会话层和调度层进行深度重构。</p><h3>协议扩展：定义多元化业务的接入标准</h3><p>Agent 的交互模式远比传统 Web 应用复杂。为了让 Agent 沙箱能够无缝嵌入现有的 AI 生态，我们针对不同场景实现了协议适配：</p><ol><li>针对工具生态：支持 MCP SSE 与 Streamable 协议<br/>随着 Model Context Protocol (MCP) 成为 Agent 工具调用的事实标准，函数计算在网关层实现了兼容标准的 MCP 协议，这意味着可以在函数计算平台实现一键托管 MCP 服务。</li><li>针对 Web/Browser Agent：支持标准 Cookie 协议<br/>Browser Agent 需要模拟登录状态或维持持久化的 Web 会话。函数计算的接入层通过实现兼容标准 Cookie 协议，使得沙箱环境能够保持与目标网站的交互状态，支持复杂的自动化操作。在用户首请求时，服务端将生成全局唯一的 CookieID 并通过 Response 中的 Set-Cookie 字段返回，后续请求用户仅需携带相同 CookieID 便实现定向路由。</li><li>针对灵活接入：定义统一 Header Field 协议<br/>在基于 Header Field 的会话亲和机制中，仅需客户端通过在 HTTP Header 中注入特定的元数据。函数计算系统网关会解析请求头中的会话 ID，并将其作为路由键，确保携带相同会话 ID 的后续请求被精准路由到同一函数实例。这种方式不依赖客户端状态（如 Cookie），可以应用在任何客户端以 HTTP 协议交互的业务场景中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605811" alt="" title=""/></p><h3>底座能力：构建有状态的会话管理</h3><p>在解决了协议层“如何接入”后，接下来的挑战是如何在无状态的 FaaS 架构上，构建“有状态”的会话体验。</p><h4>会话生命周期管理</h4><p>Agent 的执行往往不是一次性的，而是多轮对话，为此需要赋予会话生命周期管理能力，如下图所示，系统提供用户主动、系统自动两种能力实现灵活、完整的管理机制：</p><ol><li>用户主动管理</li></ol><ul><li>续期：面对 Agent 执行逻辑的不确定性，在生命周期配置上通常很难做到“一次性设对”。期间为延续状态的连续性，避免任务中断，可通过 Update API 实现对 Session TTL/IdleTimeout 的续期，主动延长沙箱寿命，续期后会话仍处于活跃状态且继续可用。</li><li>销毁：显式通过 Delete API 删除会话，实现提前销毁释放资源。</li></ul><ol start="2"><li>系统自动管理</li></ol><ul><li>Session TTL：会话达到 TTL（最大存活时长上限）后，无论是否仍在使用，平台都会自动回收资源。</li><li>Session IdleTimeout：会话在 IdleTimeout 规定时间内没有活动，平台判定为空闲并自动回收。</li></ul><p>两类方式最终都会走到生命周期结束 → 会话销毁 → 关联资源释放。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605812" alt="" title="" loading="lazy"/></p><h4>会话亲和能力</h4><p>这是将 FaaS 转化为“AI 运行时”的关键。通过会话亲和，我们保证了 Agent 上一轮生成的中间变量、本地文件在下一轮交互中依然可用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605813" alt="" title="" loading="lazy"/></p><p>整个流程分为用户首请求和非首请求，以 HeaderField 为例：</p><p><strong>会话初始化流程（首请求）</strong></p><ol><li>发起请求：Client（客户端）向 Gateway（网关）发送请求，并在 Header 中携带特定的 x-fc-session-id，用于标识该请求属于哪个 Agent 会话。</li><li>生成内部 ID：Gateway 接收请求后，对 session_key 进行哈希处理，生成一个系统内部使用的 internal_session_id。</li><li>查询会话状态：Gateway 向 MetaDB（元数据库）发起查询，核实该 session_id 是否已经存在（即是否已经有对应的运行实例）。</li><li>未命中处理：MetaDB 未搜到到相关信息，表明这是一个新会话，或者之前的会话已失效，需要重新分配资源。</li><li>触发调度：由于是新会话，Gateway 随机选择一个 Scheduler（调度器）节点，请求为该会话分配计算资源。</li><li>分配实例：Scheduler 根据当前资源情况，从资源池中分配一个可用的 VM实例（即沙箱环境）。</li><li>持久化映射关系：Scheduler 将 session_id 与分配到的 instance（实例）的对应关系写入 MetaDB。这样后续携带相同 ID 的请求就能实现“会话亲和性”，直接路由到该实例。</li><li>路由响应：Scheduler 将实例的路由信息返回给 Gateway。</li><li>返回首包：Gateway 完成链路建立，将处理后的首包数据返回给 Client。至此，该 Agent 会话正式建立，后续交互将直接复用此路径。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605814" alt="" title="" loading="lazy"/></p><p><strong>热请求数据流程</strong></p><ol><li>发起请求：Client（客户端）发起请求，并在 Header 中携带已有的 x-fc-session-id。</li><li>查询会话记录：Gateway（网关）接收请求后，前往 MetaDB（元数据库）查询该 Session ID 对应的记录。</li><li>返回映射信息：MetaDB 返回该会话之前绑定的 Instance（实例）信息以及负责管理该实例的 Target Scheduler（目标调度节点）。</li><li>直连调度节点：Gateway 根据返回的信息，直接联系对应的 Target Scheduler。</li><li>确认路由实例：Target Scheduler 告知 Gateway 该实例有效，可以进行数据转发。</li><li>转发请求：Gateway 将客户端的业务请求转发给对应的 Instance。</li><li>处理并响应：Instance（Agent 沙箱）执行代码逻辑处理请求，并将结果返回给 Gateway。</li><li>返回业务数据：Gateway 将最终的执行结果回传给 Client，完成一次有状态的会话交互。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605815" alt="" title="" loading="lazy"/></p><h4>会话隔离能力</h4><p>为了极致的安全，我们引入了“一会话一实例”的隔离模型。每个 Agent Session 独占一个底层的运行实例。一旦会话结束，实例立即销毁并擦除数据。通过会话配额控制，可以有效防止单个用户创建过多沙箱导致资源过载。</p><h3>扩展配套能力，强化 Agent 底座</h3><p>除了核心的调度与协议，针对生产环境中的性能与成本挑战，我们进一步扩展了配套能力：</p><ol><li>预热能力<br/>冷启动是 Serverless 的天敌。针对 Agent 实时交互的要求，我们支持 <code>CreateSession</code> 主动预热。在用户刚进入对话页面时，系统提前准备好预留实例。将沙箱的就绪时间压缩至极低延时。</li><li>会话级存储隔离<br/>Agent 经常需要读写文件。我们实现了会话维度的动态存储挂载。每个沙箱可以根据 Session ID 动态挂载独立的 NAS 或 OSS 路径。这样既保证了数据在会话内的持久化，又确保了不同会话间的文件系统是物理隔离的。同时满足沙箱异常 Crash 后数据的可恢复。</li><li><p>计费升级模型进化：从 FAAS 的“按请求”到“按资源消耗”<br/>FaaS按请求计费模式，在AI场景下会产生巨大的“保活成本”。会话计费模型必须与资源的实际使用强挂钩，因此系统针对会话函数的计费模式升级到Serverless AI 计费模式。</p><ul><li>活跃期：当会话实例正在处理用户请求时，按照活跃单价计费。</li><li>空闲期：当会话处于空闲、仅维持连接和上下文状态时，系统切换到一个极低的“保活”费率。仅收取内存、磁盘的费用，不再收取相对较高的CPU费用。</li></ul></li></ol><p>这个模式对客户而言，相对传统常驻实例完整生命周期计费模式成本大幅降低。</p><h2>总结与展望</h2><p>Serverless 函数计算凭借其安全隔离、弹性伸缩、按需付费等基因，正成为构建 Agent 运行时的理想选择。通过协议生态扩展、会话管理能力增强、配套能力完善，我们已实现从“单一函数执行”到“复杂 Agent 托管平台”的跨越。未来，我们也将持续聚焦启动优化、更长会话支持等等核心能力，做好AI原生时代坚实的护航者。</p>]]></description></item><item>    <title><![CDATA[『n8n』不用写SQL，了解一下内置的Datatable 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605823</link>    <guid>https://segmentfault.com/a/1190000047605823</guid>    <pubDate>2026-02-11 17:08:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个n8n小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=1%2Bs8biC3qr0KLLd3oT0OKg%3D%3D.K9PJ%2BnLxPIDKwvN%2BXNTNsmnuLoIf4qRX1Ory2Gu6QfEUzcN6ZQdy9dfRy466MR%2FtjrkSDS5uQdmXsK8%2FRT666WmP2tRDzyUwi5C7eOiG1hTyhi5BU%2F3A%2FaviR6hzHzPNBAp%2BOEKOURZN%2FBntxrmP1ee87sjqO7VJ%2FuZ3M5pNhl4%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a></blockquote><p>非技术出身的工友在使用 n8n 时是否会遇到这样的困惑：爬取了一堆数据不知道存哪里，总不能每次都导出到Excel再来回导入？想让多个工作流共用一套数据，却找不到简单的方法？不想折腾MySQL、PostgreSQL这些复杂的外部数据库，也看不懂晦涩的SQL语句？</p><p>如果有以上困惑，而且你的数据结构不是那么复杂的话，可以试试 n8n 内置的 Data tables。</p><p>我用一个简单的例子介绍一下 Data tables 的用法，顺便讲讲不同格式的字段该如何转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605825" alt="" title=""/></p><p>我们可以在 Data tables 面板管理各个数据表，点击右上角的“Create data table”创建一个数据表。</p><p>我以“员工信息表”作为演示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605826" alt="" title="" loading="lazy"/></p><p>点击加号新增 <code>name</code> 和 <code>married</code>。</p><p><code>name</code> 是“员工姓名”，Type 选择 string。</p><p><code>married</code> 是“是否结婚”，Type 选择 boolean（这个类型只有“是”和“否”两个选项）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605827" alt="" title="" loading="lazy"/></p><p>创建一个工作流，添加要给表单节点。</p><p>只有2个字段，表单的“姓名”对应数据表里的 <code>name</code>；表单的“是否结婚”对应数据表里的 <code>married</code>。</p><p>但是，数据表的 <code>married</code> 的类型是布尔型，也就是 <code>true</code> 表示已结婚，<code>false</code> 表示未结婚。但表单的“是否结婚”的选项却是字符串的“已结”和“未结”。这里要做一下转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605828" alt="" title="" loading="lazy"/></p><p>在表单节点后面添加一个「Insert row」节点，搜 table 就能找到它。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605829" alt="" title="" loading="lazy"/></p><p>给「Insert row」节点做以下配置。</p><p>「From list」选择刚刚创建的“员工信息”表。</p><p>「Values to insert」填入 <code>name</code> 和 <code>married</code> 这两个字段。</p><p><code>name</code> 这项填入 <code>{{ $json['姓名'] }}</code> 比较好理解，我不讲解了。</p><p><code>married</code> 这项填入 <code>{{ $json['是否结婚'] === '已结' ? true : false }}</code>，这是 JS 的三元运算符，判断上一个节点传入的“是否结婚”这项的值是否为“已结”，如果是的话就存入 <code>true</code> ，否则存入 <code>false</code>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605830" alt="" title="" loading="lazy"/></p><p>测试一下整个工作流。</p><p>我提交了2次表单：</p><ul><li>雷猴，已结</li><li>鲨鱼辣椒，未结</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605831" alt="" title="" loading="lazy"/></p><p>来到数据表就能看到这两项了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605832" alt="" title="" loading="lazy"/></p><p>鲨鱼辣椒突然说他今天要结婚了，作为 HR 也应该更新一下数据。</p><p>此时再提交一次表单：鲨鱼辣椒，已结。你会发现工作流又多了一条数据，这并不是我们想要的结果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605833" alt="" title="" loading="lazy"/></p><p>在 Data tables 里先手动删除第3项。</p><p>回到工作流这边，新增了一些节点。</p><ul><li><p>上面那条：</p><ul><li>「If row exists」：如果传入的“姓名”<strong>已在</strong>数据表里，就走这条。</li><li>「Update row(s)」：更新表中的数据。</li></ul></li><li><p>下面那条：</p><ul><li>「If row does not exist」：如果传入的“姓名”<strong>不在</strong>数据表里，就走这条。</li><li>「Insert row」：新增一条数据。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605834" alt="" title="" loading="lazy"/></p><p>「If row exists」和「If row does not exist」的判断条件都是一样的，如下图所示。只不过这两个节点的功能不同而已。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605835" alt="" title="" loading="lazy"/></p><p>在来看「Update row(s)」这个节点，通过 <code>name</code> 这个字段找到要修改的那行数据，找到后就修改 <code>married</code> 这列的值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605836" alt="" title="" loading="lazy"/></p><p>「Insert row」节点的配置不需要改变。</p><p>试试～</p><p>提交一项：鲨鱼辣椒，已婚</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605837" alt="" title="" loading="lazy"/></p><p>回到数据表这边就能看到鲨鱼辣椒的婚姻状态变成“true”了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605838" alt="" title="" loading="lazy"/></p><p>再提交一项：蝎子莱莱，未婚。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605839" alt="" title="" loading="lazy"/></p><p>由于“蝎子莱莱”不在数据表里，所以走的是“新增”路线。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605840" alt="" title="" loading="lazy"/></p><hr/><p>以上就是本文的全部内容啦，想了解更多n8n玩法欢迎关注<a href="https://link.segmentfault.com/?enc=2EhXDv9wyvkBho6%2FOzptww%3D%3D.XC0vk1%2BC0zi%2F7TeobnIwX3BUmsXNSSsEeV47RXU7wvrwNn%2B7n8hZczeTSVEEpnjXAdWypa81v24m5FjigFqo7qX0eoZOiSkXe585dtAOjQXMP2jtmI3a10ncAmnVjMt0nmefilHkTZDBFTr1h528wdCNEJzFV2jMSRVw2OAkEk8%3D" rel="nofollow" target="_blank">《n8n修炼手册》</a>👏</p><p>如果你有 NAS，我非常建议你在 NAS 上部署一套 n8n，搞搞副业也好，帮你完成工作任务也好  <a href="https://link.segmentfault.com/?enc=QVefuQl2ghqxx1XEmEAPpA%3D%3D.%2FKSidRKzhzuCFS8Ds4OXoVWqLkiYB6SnS2zSaDtwdCoPXlCK0Nc5aJzTmMxxQlvxJ26EOegoV9Ni7OqGSHB5KA%3D%3D" rel="nofollow" target="_blank">《『NAS』不止娱乐，NAS也是生产力，在绿联部署AI工作流工具-n8n》</a></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[『NAS』在绿联部署隐私优先的媒体文件格式工具-VERT.sh 德育处主任 ]]></title>    <link>https://segmentfault.com/a/1190000047605897</link>    <guid>https://segmentfault.com/a/1190000047605897</guid>    <pubDate>2026-02-11 17:07:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p><blockquote>整理了一个NAS小专栏，有兴趣的工友可以关注一下 👉 <a href="https://link.segmentfault.com/?enc=VIgaLS5CWZ2ZY%2F3%2BJZLtYQ%3D%3D.h5NIi8HPRwSyOjatedO%2BCrl45vRNof7NaSYHOUWfWBMrcWDs7kPK2aCN7AXOuPqwy3UVhH%2Bofgx8qvP%2FNopPOYMzQwyUvwaZN00QCQV5JrHuaxnoqrzqZmlSDasbMuC26GHq2aa4EmgQTUC3SAwpW3HgllkUw%2FlZ28ZYVXCddNQ%3D" rel="nofollow" target="_blank">《NAS邪修》</a></blockquote><p>VERT.sh 是一款开源文件转换工具，它所有转换都在本地浏览器完成，文件（除视频外）永不上传云端，支持 n 种格式（文档、图片、音频、视频），无广告、无文件大小限制，开箱即用！</p><p>这次我使用绿联的 NAS 部署 VERT.sh，其他品牌的 NAS 只要支持 Docker 的，部署步骤都是大同小异。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605899" alt="" title=""/></p><p>首先在“文件管理”应用里找到“docker”文件夹，在里面创建一个“vert”文件夹。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605900" alt="" title="" loading="lazy"/></p><p>打开“Docker”应用，点击左侧菜单的“项目”，创建一个新项目。</p><ul><li>项目名称：vert。</li><li>存放路径：选择上一步创建的 <code>xxx/docker/vert</code> 文件夹。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605901" alt="" title="" loading="lazy"/></p><p>Compose配置 输入以下代码：</p><pre><code>services:
  vert:
    image: ghcr.io/vert-sh/vert:latest
    container_name: vert
    ports:
      - 2340:80 
    restart: unless-stopped</code></pre><p>这里我配置了 <code>2340</code> 这个端口，你根据你实际情况配置吧，只要不跟其他项目的端口冲突就行。</p><p>然后等 Docker 下载镜像和构建项目，等就行，速度取决于你的网速。</p><p>项目构建完成后，点击左侧菜单”容器“这项，找到 vert 这项，点击它右侧的小箭头会弹出端口号，点击端口这个按钮。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605902" alt="" title="" loading="lazy"/></p><p>点击端口按钮后，浏览器会新开一个窗口运行 VERT.sh。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605903" alt="" title="" loading="lazy"/></p><p>它默认界面是英文的，点击顶部菜单的“Settings”项，找到“Language”，切换成中文就行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605904" alt="" title="" loading="lazy"/></p><p>回到首页可以看到 VERT.sh 支持图片、音频、文档和视频等文件的格式转换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605905" alt="" title="" loading="lazy"/></p><p>测试了一下，图片是可以转格式的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605906" alt="" title="" loading="lazy"/></p><p>但视频要传到人家的服务器转，而且还不一定成功😮‍💨</p><p>就当没转视频格式这个功能吧😤</p><hr/><p>以上就是本文的全部内容啦，<strong>有疑问可以在评论区讨论～</strong></p><p><strong>想了解更多NAS玩法可以关注<a href="https://link.segmentfault.com/?enc=fFgXTmv6QthMvvdoNFf8BA%3D%3D.ipHZudRMwvb5anEVIdEdTS7DK3PmG48paxrNVICj443e8uvMbXBjYc98bGUlSS86GaK0Oe1SH86WUuma6ks0k3e%2BJ5hPa%2FzZaqdN2bJcRhX8qCVbhCm%2BrGV65pEDtPnDdyeD3gxnW5gTYqBsjM26bwQBWozsa3zXlvDkvGAFkzM%3D" rel="nofollow" target="_blank">《NAS邪修》👏</a></strong></p><p><strong>点赞 + 关注 + 收藏 = 学会了</strong></p>]]></description></item><item>    <title><![CDATA[R语言优化沪深股票投资组合：粒子群优化算法PSO、重要性采样、均值-方差模型、梯度下降法|附代码数据]]></title>    <link>https://segmentfault.com/a/1190000047605916</link>    <guid>https://segmentfault.com/a/1190000047605916</guid>    <pubDate>2026-02-11 17:06:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>全文链接：<a href="https://link.segmentfault.com/?enc=G%2BafYjCBdcOD3o7U3NjxBw%3D%3D.GKQqcOekisfPcApG8oLZUnecuKhHq9DGp%2BmkO2bkon0%3D" rel="nofollow" title="https://tecdat.cn/?p=44965" target="_blank">https://tecdat.cn/?p=44965</a>  <br/>原文出处：拓端数据部落公众号  <br/><strong>关于分析师</strong>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605918" alt="" title=""/></p><p>在此对 Hongxuan Liu 对本文所作的贡献表示诚挚感谢，他完成了应用统计专业的硕士学位，专注机器学习、风险管控领域。擅长 R 语言、Python，在机器学习、风险管控领域具备扎实的技术功底，可熟练运用相关软件开展数据分析、建模及风险管控相关工作。Hongxuan Liu 曾在中国农业银行从事数据分析工作，深耕金融领域数据分析与风险管控相关业务，负责银行各类数据的整理、分析与建模，为银行的风险防控、投资决策等核心业务提供数据支撑与实操建议，积累了丰富的金融行业数据分析实战经验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t1" target="_blank"/>专题：2025年智能优化算法在A股投资组合配置中的实践与创新</h3><h4><a name="t2" target="_blank"/>引言</h4><p>在国内A股市场的投资实践中，普通投资者和中小机构始终面临一个核心难题：如何在多只股票间分配资金，既能控制波动风险，又能实现资产稳健增值。早年间，多数投资者依赖经验或“等权重均分”的方式配置资产，这种缺乏量化支撑的策略，在2020年后市场波动加剧的背景下，资产波动幅度比科学配置方案高出30%以上。  <br/>马科维茨的均值-方差模型为量化配置提供了理论基础，但该模型对应的优化问题存在非凸性，传统梯度下降算法极易陷入局部最优，无法找到真正的全局最优配置。粒子群优化算法（PSO）凭借全局搜索能力强的优势成为解决这类问题的有效工具，但传统PSO初始粒子随机分布，导致收敛速度慢、无效计算多。基于此，我们结合为金融机构提供投资组合优化咨询项目的实战经验，提出将重要性采样（IS）与PSO融合的IS-PSO算法，通过定向生成高质量初始粒子群，解决传统PSO“盲目搜索”的痛点。本文将拆解该算法的设计逻辑、落地步骤及在沪深A股样本上的应用效果，让读者既能掌握实操方法，也能理解背后的核心原理。  <br/>本文内容改编自过往客户咨询项目的技术沉淀并且已通过实际业务校验，该<strong>项目完整代码与数据已</strong>分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂 怎么做，也懂 为什么这么做；遇代码运行问题，更能享24小时调试支持。</p><h4><a name="t3" target="_blank"/> 项目文件目录</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605920" alt="" title="" loading="lazy"/></p><h4><a name="t4" target="_blank"/>整体流程脉络</h4><p>&lt;pre data-index="0" name="code" style="color: rgb(0, 0, 0); font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"&gt;&lt;img alt="" src="https://i-blog.csdnimg.cn/direct/fa519e9285cd4756b5c826972a17bbba.png" style="border: 0px; max-width: 650px;"&gt;<br/>&lt;/pre&gt;</p><h4><a name="t5" target="_blank"/>1 投资组合优化的行业痛点与技术基础</h4><p>对于A股投资者而言，“分散投资却没分散风险”是普遍痛点——不少投资者将资金分散到多只股票后，要么收益跑不赢大盘，要么遇到市场调整时亏损远超预期。这背后的核心原因，是没有科学量化不同股票的收益和风险特征，仅依靠经验或简单的行业均分策略，无法适配复杂的市场环境。  <br/>马科维茨均值-方差模型为解决这一问题提供了核心框架：将投资组合的收益定义为各资产预期收益率的加权平均值，风险用收益率的标准差衡量，核心目标是找到“有效前沿”——即在给定风险下收益最高，或给定收益下风险最低的资产组合。这一目标转化为数学问题后，核心是最大化效用函数：max (ω^Tμ - λ/2*sqrt(ω^TΣω))。其中ω是资产权重向量（各股票配置比例），μ是资产预期收益率向量，λ是风险厌恶系数（数值越大代表投资者越不愿承担风险），Σ是资产收益率的协方差矩阵。  <br/>但这个效用函数具有非凸性，传统梯度下降算法依赖目标函数的梯度信息迭代，很容易停在局部最优解，无法找到真正的全局最优权重配置，这也是传统算法在实际投资中效果不佳的关键原因。</p><h4><a name="t6" target="_blank"/>2 IS-PSO算法的创新设计与实现</h4><p>粒子群优化算法（PSO）是模拟鸟群觅食行为的智能优化算法，每个“粒子”对应一组资产权重，通过迭代更新粒子的位置（权重）和速度，跟踪个体最优位置（pbest）和全局最优位置（gbest），最终找到最优权重配置。但传统PSO的初始粒子是随机生成的，大量粒子落在无效解区域，导致算法收敛慢、计算效率低。  <br/>我们的核心创新点在于，用重要性采样（IS）优化初始粒子群的生成逻辑：先随机生成大量权重样本，筛选出目标函数值前20%的“优质样本”，再基于这些样本的分布生成80%的初始粒子，剩余20%粒子随机生成（保证群体多样性）。这种方式让初始粒子聚焦在最优解附近，大幅减少无效迭代，提升收敛效率。</p><h5>2.1 IS-PSO算法的R语言核心实现</h5><p>以下是修改后的核心代码（变量名、代码结构均做调整，英文注释已翻译为中文，省略部分通用迭代逻辑）：</p><pre><code># ===== 融合重要性采样的改进粒子群优化算法（IS-PSO） =====enhanced_pso_with_is &lt;- function(converge_threshold = 1e-6, min_diversity = 1e-4, rand_seed = NULL) { # 设置随机种子，保证结果可重复 if (!is.null(rand_seed)) { set.seed(rand_seed) }# 步骤1：重要性采样预生成大量权重样本 pre_sample_total &lt;- 500 # 预生成500个随机权重样本 pre_weight_matrix &lt;- matrix(runif(pre_sample_total * asset_num), pre_sample_total, asset_num) # 权重归一化处理（确保所有资产权重和为1） pre_weight_matrix &lt;- t(apply(pre_weight_matrix, 1, function(x) x / sum(x))) # 计算每个预生成样本的目标函数值 pre_obj_scores &lt;- apply(pre_weight_matrix, 1, calc_objective_func)# 步骤2：筛选前20%的优质权重样本 top_sample_indexes &lt;- order(pre_obj_scores, decreasing = F)[1:(pre_sample_total * 0.2)] top_weight_samples &lt;- pre_weight_matrix[top_sample_indexes, ] sample_central &lt;- colMeans(top_weight_samples) # 优质样本的中心位置 sample_cov_matrix &lt;- cov(top_weight_samples) # 优质样本的协方差矩阵# 步骤3：生成初始粒子群（80%来自优质区域，20%随机生成） particle_positions &lt;- matrix(0, particle_total, asset_num) for (i in 1:particle_total) { if (i &lt;= particle_total * 0.8) { # 80%粒子从优质样本区域生成，考虑资产间相关性 asset_dim &lt;- asset_num # 尝试对协方差矩阵做Cholesky分解（添加小值保证矩阵正定） chol_matrix &lt;- try(chol(sample_cov_matrix + 1e-6 * diag(asset_dim)), silent = TRUE) if (inherits(chol_matrix, "try-error")) { # 分解失败时，使用对角协方差生成扰动值 disturbance_val &lt;- sqrt(diag(sample_cov_matrix)) * rnorm(asset_dim) } else { # 分解成功则生成符合协方差分布的扰动值 disturbance_val &lt;- chol_matrix %*% rnorm(asset_dim) } # 调整扰动幅度（0.2为实战验证的经验系数） temp_weight &lt;- sample_central + 0.2 * disturbance_val temp_weight &lt;- pmax(temp_weight, 0) # 保证权重非负（不允许卖空） } else { # 20%粒子随机生成，维持粒子群的多样性 temp_weight &lt;- runif(asset_num) } # 对生成的权重做归一化处理 particle_positions[i, ] &lt;- temp_weight / sum(temp_weight) }# 省略：粒子速度初始化、个体最优/全局最优参数初始化代码 ......# 省略：粒子位置/速度迭代更新、收敛条件判断的核心循环代码 ......# 返回算法最优结果 return(list( best_weight_config = global_best_pos, best_objective_val = -(global_best_score), portfolio_annual_return = sum(global_best_pos * return_vector), portfolio_annual_risk = sqrt(t(global_best_pos) %*% cov_mat %*% global_best_pos), converge_iterations = iter_count ))}</code></pre><p><strong>代码说明</strong>：</p><ul><li>核心逻辑是通过预采样筛选优质权重样本，让80%的初始粒子聚焦在最优解附近，减少无效搜索；</li><li>变量名如<code>n_pre_samples</code>改为<code>pre_sample_total</code>、<code>n_assets</code>改为<code>asset_num</code>，更贴合中文使用习惯；</li><li>省略部分为PSO算法通用的迭代更新逻辑（如粒子速度调整、收敛判断循环），可参考常规PSO实现补充。</li></ul><hr/><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047605921" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>专题：2025年游戏科技的AI革新研究报告</h3><h3><a name="t8" target="_blank"/>原文链接：<a href="https://link.segmentfault.com/?enc=Myp5h5WC0B%2BASmWAPSACAw%3D%3D.imfeGLiOhCySBr7wnb1yRHJh7AFbDE9pZEoqJn2NKbA%3D" rel="nofollow" title="https://tecdat.cn/?p=44082" target="_blank">https://tecdat.cn/?p=44082</a></h3><hr/><h4><a name="t9" target="_blank"/>3 IS-PSO算法在A股中的实际应用效果</h4><p>我们选取沪深A股10只不同行业的股票（覆盖装备制造、信息技术、环保、医药等领域），以2020年7月至2025年6月共1198个交易日的收盘价为基础数据，先计算日对数收益率（Rt = ln(Pt/Pt-1)），再年化处理得到预期收益率和协方差矩阵，对比等权重配置、梯度下降算法、传统PSO、IS-PSO四种方式的应用效果。</p><h5>3.1 收敛效率大幅提升</h5><p>传统PSO算法平均需要120次迭代才能收敛，而IS-PSO仅需31次迭代，收敛速度提升74.4%。在计算效率上，IS-PSO平均运行时间为66.789毫秒，远低于传统PSO的232.732毫秒，这意味着在实际应用中，IS-PSO能更快给出最优配置方案，降低计算资源消耗。</p><h5>3.2 收益风险比更优</h5><p>在高风险厌恶场景（λ=0.9）下，IS-PSO配置的投资组合平均收益达到0.165，高于传统PSO的0.157；且IS-PSO生成的权重呈现“稀疏性”——仅聚焦2只核心股票，却实现了更高的收益风险比。这对中小投资者而言，大幅降低了选股和资金配置的门槛，无需分散到多只股票就能实现风险与收益的平衡。</p><h5>3.3 权重配置结果可视化</h5><p>（空行）  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605922" alt="" title="" loading="lazy"/>  <br/>（空行）  <br/>上图清晰展示了不同算法的权重配置差异：梯度下降算法配置的资产数量多，但收益表现不如PSO类算法；IS-PSO与传统PSO均聚焦少数核心资产，但IS-PSO在收敛速度和高风险场景下的收益表现更优，更适合普通投资者使用。</p><h4><a name="t10" target="_blank"/>4 应急修复服务与落地建议</h4><p>针对算法落地过程中可能出现的代码运行异常、结果不符合预期等问题，我们提供<strong>24小时响应的应急修复服务</strong>，相比投资者自行调试，问题解决效率提升40%，能快速定位并解决代码报错、参数设置不当、数据适配异常等问题。  <br/>在实际落地层面，IS-PSO算法可直接适配中小投资者的需求：只需导入股票收盘价数据，设置风险厌恶系数，算法就能自动输出最优权重配置。后续可进一步优化方向包括：纳入债券、基金等多元资产，考虑交易成本、流动性等实际交易约束，通过贝叶斯优化实现算法参数的自动调整。</p><h4><a name="t12" target="_blank"/>总结</h4><ol><li>针对A股投资组合优化的非凸性问题，融合重要性采样的PSO算法（IS-PSO）通过优化初始粒子群分布，将收敛速度提升74.4%，大幅提升计算效率；</li><li>IS-PSO在高风险厌恶场景下收益表现更优，且生成的稀疏权重配置降低了中小投资者的实操门槛；</li><li>该算法已通过实际咨询项目验证，配套的24小时应急修复服务可保障算法稳定落地应用。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605919" alt="封面" title="封面" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[手把手教你用云效 MCP 实现项目自动化管理 阿里云云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047605929</link>    <guid>https://segmentfault.com/a/1190000047605929</guid>    <pubDate>2026-02-11 17:06:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：延枚</p><p>云效 MCP Server 已正式发布。它是一个为研发全生命周期提供统一可编程能力的元控制平面，旨在打通工具间的壁垒，实现研发流程的高度自动化。</p><p>目前，MCP Server 已提供超过 <strong>150</strong> 个原子能力（API），全面覆盖云效的核心模块，包括：</p><ul><li><strong>组织管理：</strong> 组织列表、部门信息、角色、成员信息等</li><li><strong>代码管理：</strong> 代码仓库、分支、合并请求、文件树操作等</li><li><strong>项目管理：</strong> 项目、工作项、字段配置、评论、工时管理等</li><li><strong>流水线管理：</strong> 流水线、资源、标签、部署等</li><li><strong>制品仓库管理：</strong> 制品仓库、制品列表等</li><li><strong>应用交付：</strong> 部署单、应用、应用标签、变量组等</li><li><strong>测试管理：</strong> 测试用例、用例目录、测试计划、测试结果等</li></ul><p>通过将云效 MCP Server 与自定义脚本，或与通义灵码、Cursor、iFlow-Cli 等本地/云端大模型工具结合，研发团队可以赋予程序直接“操作云效”的能力，从而高效完成各类重复性工作。</p><p>例如，在项目协作场景中，你可以实现：</p><ul><li><strong>智能查询：</strong> 快速获取指定项目、迭代或成员名下的需求列表。</li><li><strong>自动编排：</strong> 将复杂需求自动拆解为子任务，并生成验收标准。</li><li><strong>批量处理：</strong> 一次性更新多个工作项的状态、负责人或标签。</li><li><strong>数据同步：</strong> 读取 Excel 或其他系统的数据，批量在云效中创建工作项。</li></ul><p>为了帮助大家快速上手，我们推出了 <strong>【玩转云效 MCP】</strong> 系列专题文章。</p><p>本篇作为系列的第一篇，将聚焦于<strong>项目管理与协作</strong>场景，提供一系列“即刻可用”的 Prompt 示例与实战演练，手把手教你如何利用 MCP 实现项目管理的自动化。</p><h2>前期环境准备</h2><ol><li>在本地准备好你的 AI 工具：</li></ol><ul><li><p>通义灵码</p><p><a href="https://link.segmentfault.com/?enc=FfQH%2FPeIQUwtaLsDobV80A%3D%3D.fRfHmSvIS7enfsOzaRajMCRFldwonzE%2BBOkgYYojF%2BY%3D" rel="nofollow" target="_blank">https://lingma.aliyun.com/</a></p></li><li><p>Qoder</p><p><a href="https://link.segmentfault.com/?enc=j1w8%2B%2BTn32KeEFZbhuhSfA%3D%3D.bedgErDWf%2BIUZsd0FD4tZhes%2F%2BR6WfBNky14yfuUwbQ%3D" rel="nofollow" target="_blank">https://qoder.com/</a></p></li><li><p>Cursor</p><p><a href="https://link.segmentfault.com/?enc=0mi%2FJYMnn%2FUwHtL80RwgEg%3D%3D.gBaF21T201Rx9Ooel2ilIlhk6rHNGzmw%2BBQXBR8Cgjc%3D" rel="nofollow" target="_blank">https://cursor.com/cn</a></p></li><li><p>Iflow-Cli</p><p><a href="https://link.segmentfault.com/?enc=FObA%2BzLiChRy9GcRRa9tNQ%3D%3D.KatZI3foAKy8ipRrlqfFA8JdABSPW0C81c4xVc1qtNk%3D" rel="nofollow" target="_blank">https://iflow.cn/</a></p></li><li>……</li></ul><ol start="2"><li>按照云效 MCP Server 的说明完成配置（包含 Token、组织信息等）。</li></ol><ul><li><p>配置说明参考：GitHub 文档</p><p><a href="https://link.segmentfault.com/?enc=vJvv6d4lQ8ysNrJ83NwO%2Bg%3D%3D.QQtZLq9o1nt0bfAKvTvPvnsPVdN3BTCQyRk8l5AUpV0QWMQNMiRZ1780LGw3xOvuK%2FyGDp%2BEjSDoBTx%2BhBcmIuRedkG%2B2MLM0cWsABuY2a%2FeDFt1PKRDSFiHQCLYyrZj" rel="nofollow" target="_blank">https://github.com/aliyun/alibabacloud-devops-mcp-server/blob/master/README.zh-cn.md</a></p></li></ul><p>配置完成后，你的 AI 工具就可以通过 MCP 协议直接调用云效的项目 / 工作项等接口。</p><h2>检查 MCP Server 配置是否生效</h2><p>完成配置后，可以先用两条最简单的 Prompt 做“自检”。本文中演示示例的 AI 工具为 Qoder。</p><h3>1. 查看当前组织信息</h3><pre><code>查看云效当前的组织信息</code></pre><p>预期：AI 会调用 <code>YUNXIAO/GET_CURRENT_ORGANIZATION_INFO</code>，并返回：</p><ul><li>组织 ID（lastOrganization）</li><li>用户 ID</li><li>用户名</li></ul><p>这些信息后续会被用来继续检索项目、工作项等。</p><h3>2. 查看当前用户信息</h3><pre><code>查看云效当前的用户信息</code></pre><p>预期：AI 会调用 <code>YUNXIAO/GET_CURRENT_USER</code>，并返回用户名、邮箱、组织 ID 等相关信息。</p><p>只要上述两个调用能正常返回结果，基本可以认为 MCP Server 配置是正确的。</p><h2>实用场景 1：检索+统计/批量处理</h2><p>云效 MCP 提供了非常丰富的检索能力：</p><ul><li><strong>项目级：</strong> 按项目名称、状态、创建时间等检索项目</li><li><strong>工作项级：</strong> 按标题、描述、状态、优先级、负责人、标签等检索工作项</li></ul><p>你可以先让 AI 告诉你“有哪些可用条件”，再组合场景：</p><pre><code>云效中检索项目都有哪些条件可使用？
云效中对于检索工作项都提供了哪些条件？</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605931" alt="image" title="image"/></p><p>拿到条件后，就可以开始“场景编排”了。</p><h3>1. 按创建人检索工作项</h3><pre><code>查看 云效正式自动化 组织中 bowentestmcp 项目 我自己创建的工作项</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605932" alt="image" title="image" loading="lazy"/></p><p>AI 一般会自动完成：</p><ol><li>查询你所属组织列表</li><li>切换到“云效正式自动化”组织</li><li>查询该组织下名为 bowentestmcp 的项目</li><li>在该项目下检索“我创建的工作项”</li></ol><h3>2. 基于结果做统计 / 批量修改</h3><p>拿到工作项列表后，可以继续下发指令：</p><pre><code>把这两个工作项的状态改为已完成</code></pre><p>AI 会：</p><ol><li>查询该项目的工作流信息，确认“已完成”状态的 ID（比如：100014）</li><li>使用 <code>YUNXIAO/UPDATE_WORK_ITEM</code> 批量更新状态</li><li>再次检索校验修改是否成功</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605933" alt="image" title="image" loading="lazy"/></p><h3>3. 更多检索 + 批量处理示例 Prompt</h3><p>以下 Prompt 都是可以直接用的“套路”：</p><ul><li><strong>按标签批量打标</strong></li></ul><pre><code>某某项目下近一周我创建的需求，请统一加上「一期」标签</code></pre><ul><li><strong>按迭代统计</strong></li></ul><pre><code>统计一下某某项目下，迭代名为「xx」的需求数以及完成情况分析</code></pre><ul><li><strong>按状态批量流转</strong></li></ul><pre><code>请帮我找出 bowentestmcp 项目中所有状态为「已完成」的需求，然后统一将它们的状态改为「已关闭」</code></pre><ul><li><strong>按标题关键词 + 批量调优先级</strong></li></ul><pre><code>查询 xx 项目中所有标题包含「登录」或「注册」的需求，将它们的优先级统一调整为「高」</code></pre><ul><li><strong>按创建人 + 批量改负责人</strong></li></ul><pre><code>找出我创建的所有待处理状态的任务，把它们全部指派给张三（工号：xxx）</code></pre><h2>实用场景 2：拆分需求</h2><p>大模型 + MCP 非常适合做“把一个大需求拆成很多小需求并直接录入云效”这类工作。</p><h3>1. 按功能点拆分父需求</h3><p>示例：已有父需求 <code>QAAB-3</code>，描述里包含一段“功能列表”：</p><ul><li>支持加法运算</li><li>支持减法运算</li><li>支持乘法运算</li><li>支持除法运算</li></ul><p>你只需要一句：</p><pre><code>QAAB-3 这个工作项，请按照里面的功能点描述建立相应的子需求</code></pre><p>AI 会：</p><ol><li>查询 QAAB-3 的详细描述</li><li>自动解析描述中的有序列表（1/2/3/4）</li><li>通过 <code>YUNXIAO/CREATE_WORK_ITEM</code> 依次创建 4 个子需求</li><li>每个子需求继承父需求的类型、负责人等</li></ol><p>最终在云效需求列表页就能看到新创建的需求：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605934" alt="image" title="image" loading="lazy"/></p><h3>2. 更多拆分需求的示例 Prompt</h3><ul><li><strong>按实现层拆分</strong></li></ul><pre><code>找到「实现用户登录功能」这个需求，将它拆分成：前端 UI、后端接口、数据库设计三个子任务</code></pre><ul><li><strong>按列表自动拆分</strong></li></ul><pre><code>查看 QAAB-8 的需求描述，自动识别其中的功能点列表（如 1. 2. 3.），为每个功能点创建一个独立的子需求</code></pre><ul><li><strong>按开发阶段拆分</strong></li></ul><pre><code>将「实现用户注册机制」这个需求拆分为：
- 需求分析
- 技术方案设计
- 前端开发
- 后端开发
- 联调测试
- 上线部署每个阶段创建一个子任务</code></pre><ul><li><strong>按 INVEST 原则拆分大需求</strong></li></ul><pre><code>QAAB-5 这个需求过大，请按照 INVEST 原则将它拆分成：
- 独立的（Independent）
- 可协商的（Negotiable）
- 有价值的（Valuable）
- 可估算的（Estimable）
- 小的（Small）
- 可测试的（Testable）
多个小需求</code></pre><h2>实用场景 3：优化需求内容</h2><p>很多需求最初往往只是“半句话”，比如：</p><p>支持乘法运算：实现计算器的乘法运算功能，输入两个数，输出两数之积。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605935" alt="image" title="image" loading="lazy"/></p><p>这类需求很难直接用于评审 / 开发 / 测试。借助 MCP，你可以让 AI：</p><ul><li>补全为<strong>用户故事</strong>形式</li><li>写出<strong>业务流程与影响分析</strong></li><li>给出<strong>可测试的验收条件</strong></li><li>关键是：<strong>写回云效原工作项中</strong></li></ul><h3>1. 完整优化一个需求示例（QAAB-5）</h3><pre><code>QAAB-5 这个需求，请进行业务分析优化：
要求：
1. 改为用户故事的结构
2. 分析业务流程和影响
3. 提供验收条件</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605936" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605937" alt="image" title="image" loading="lazy"/></p><p>AI 典型的处理步骤：</p><ol><li>读取 QAAB-5 的原始描述（“实现计算器的乘法运算功能...”）</li><li><p>生成：</p><ul><li>用户故事（作为谁 / 我希望 / 以便）</li><li>业务流程（打开计算器 → 输入数字 → 选择乘号 → 输入第二个数 → 点击等号 → 展示结果）</li><li>业务影响（对前端 / 后端 / 错误处理 / 测试等的影响）</li><li>验收条件（覆盖正数、负数、小数、0、边界值、非法输入、性能等场景）</li></ul></li><li>使用 YUNXIAO/UPDATE_WORK_ITEM 将上述内容整体写回 QAAB-5 的描述字段</li></ol><p>优化完成后，在云效中查看 QAAB-5，就会看到一个完整、可评审、可测试的需求说明。</p><h3>2. 更多需求优化的示例 Prompt</h3><ul><li>改写为用户故事格式</li></ul><pre><code>查看「实现用户登录功能」这个需求，将它改写为用户故事格式：
- 作为【谁】
- 我希望【做什么】
- 以便【达成什么价值】
并直接更新回工作项</code></pre><ul><li>批量用户故事化技术需求</li></ul><pre><code>找出所有技术描述类的需求（标题以「实现」开头），将它们统一改写为用户故事格式，突出用户角色和业务价值</code></pre><ul><li>补充验收条件</li></ul><pre><code>QAAB-8 缺少验收标准，请根据需求描述补充至少 5 条可量化的验收条件，
包括：
- 功能性验收
- 性能要求
- 边界条件
- 异常处理
并更新回原需求</code></pre><ul><li>补充测试场景</li></ul><pre><code>查看「支持乘法运算」需求，补充完整的测试场景：
- 正常场景（正数、负数、小数）
- 边界场景（0、极大值、极小值）
- 异常场景（非法输入、溢出）
更新回原需求的验收条件部分</code></pre><ul><li>批量为无验收条件的需求补充标准</li></ul><pre><code>找出所有没有验收条件的需求（描述中不包含「验收」关键词），
为每个需求根据其标题和描述自动生成 3-5 条验收标准，并写回工作项</code></pre><h2>实用场景 4：批量导入需求</h2><p>当你已经有一份 Excel / CSV 需求列表时，可以直接让 AI + MCP 帮你“批量录入到云效”。</p><h3>1. 从 Excel 表格导入示例</h3><p>假设有一个 Excel：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605938" alt="image" title="image" loading="lazy"/></p><p>只需要一句 Prompt：</p><pre><code>请将「需求列表.xlsx」中的内容录入到云效 bowentestmcp 这个项目中</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605939" alt="image" title="image" loading="lazy"/></p><p>AI 的典型处理流程：</p><ol><li>解析 Excel 结构（首行是表头：标题 / 内容 / 优先级）</li><li><p>将每一行映射为：</p><ul><li>subject → 标题</li><li>description → 内容</li><li>priority → 优先级（高 / 中 / 低）</li></ul></li><li>调用 <code>YUNXIAO/CREATE_WORK_ITEM</code> 创建 4 条新需求</li></ol><p>回到云效 bowentestmcp 项目的需求列表页，就能看到刚刚导入的 4 条需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605940" alt="image" title="image" loading="lazy"/></p><h3>2. 更多批量导入的示例 Prompt</h3><ul><li>从“产品需求.xlsx”导入</li></ul><pre><code>从「产品需求.xlsx」导入需求，全部创建为「产品类需求」类型，指派人统一设置为我本人</code></pre><ul><li>指定字段映射导入</li></ul><pre><code>导入「backlog.xlsx」到 bowentestmcp 项目：
- 第 1 列（需求名称） → 标题
- 第 2 列（详细说明） → 描述
- 第 3 列（重要程度） → 优先级（高/中/低）
- 第 4 列（负责人姓名） → 指派人
- 第 5 列（所属模块） → 标签</code></pre><ul><li>带层级关系导入</li></ul><pre><code>导入「需求层级.xlsx」，根据「父需求ID」列建立父子关系：
- 第一级：Epic / 主题需求
- 第二级：Feature / 功能需求
- 第三级：Story / 用户故事
自动建立层级关联</code></pre><ul><li>自动识别表头并导入</li></ul><pre><code>分析「需求文档.xlsx」的表头结构，自动识别对应的字段映射关系，将数据导入 bowentestmcp 项目</code></pre><ul><li>导入前先做数据校验</li></ul><pre><code>导入「需求池.csv」前先验证：
- 必填字段不能为空（标题、描述）
- 优先级只能是「高/中/低」
- 负责人必须是项目成员
- 标题长度不超过 50 字
验证通过后再批量创建</code></pre><h2>更多项目管理场景示例</h2><p>在前面几个场景基础上，还可以进一步组合出更复杂的“项目级”能力。</p><h3>1. 项目健康度检查</h3><pre><code>分析 bowentestmcp 项目健康状况：
- 统计各状态需求分布
- 检查逾期未完成的需求
- 识别长期无人认领的需求
- 分析需求平均完成周期
- 检测可能的瓶颈（某状态停留过久）
生成健康度报告</code></pre><p>AI 可以基于 SEARCH_WORKITEMS 等接口，做出一份结构化的项目健康度分析报告。</p><h3>2. 迭代规划</h3><pre><code>为即将开始的 Sprint 5 规划任务：
1. 从需求池中筛选高优先级需求
2. 智能推荐适合本迭代的需求组合
3. 自动分配给合适的成员
4. 创建迭代并关联需求</code></pre><h3>3. 迭代回顾</h3><pre><code>为刚结束的 Sprint 3 生成回顾报告：
- 完成需求数 vs 计划需求数
- 各成员完成情况统计
- 延期需求分析
- 紧急插入需求统计
- 提取改进建议</code></pre><h3>4. 工作负载分析</h3><pre><code>分析 bowentestmcp 项目团队成员工作负载：
- 统计每人当前进行中的任务数
- 计算每人的工作时长总和
- 识别负载过重或过轻的成员
- 建议任务重新分配方案</code></pre><h3>5. 需求质量评估</h3><pre><code>批量检查 xx 项目中所有「待开发」状态的需求质量：
- 描述完整性（是否包含背景、目标、验收标准）
- 验收条件清晰度（是否可测试）
- 依赖关系完整性
- 工作量评估准确性
不合格的标记为「待补充」并通知负责人</code></pre><h3>6. Bug 关联需求分析</h3><pre><code>分析 xx 项目中 Bug 与需求的关联：
- 统计每个需求关联的 Bug 数量
- 识别高缺陷率的需求
- 分析 Bug 产生的阶段（开发/测试/生产）
- 提供质量改进建议</code></pre><h2>小结</h2><p>通过本文的实战演练，我们看到：当云效 MCP Server 与自动化脚本或智能体结合，它不再仅仅是一组 API，而是一种全新的研发协作范式。它将“会用云效”的人，从大量机械操作中解放出来。总结来说，MCP 为项目管理者带来了三大核心价值的转变：</p><ul><li><strong>执行指令化：</strong> 将原本需要数十分钟的手动点击，压缩为几行指令或一句自然语言描述。</li><li><strong>经验模板化：</strong> 将个人的最佳实践与团队规范，沉淀为可复用、可共享的自动化模板。</li><li><strong>精力聚焦化：</strong> 将管理者从繁琐的工具操作中解放，回归到思考产品、业务和团队等更高价值的工作上。</li></ul><p>云效 MCP Server 就像一套高效的“项目协作外骨骼”——它增强你的能力，放大你的效能，让你跑得更快、更远。而这，仅仅是开始。<strong>项目管理是 MCP 能力版图的第一块拼图。</strong></p><p>在下一篇文章中，我们将深入 <strong>【代码管理】</strong> 场景，探索如何通过 MCP 实现分支自动创建、权限精细化管理、代码合规性检查等高阶玩法。</p><p>敬请期待！也欢迎你在评论区分享使用心得，或告诉我们你最希望 MCP 在哪个领域帮你实现自动化。</p>]]></description></item><item>    <title><![CDATA[我是如何把 API 响应时间从 200ms 压到了 10ms 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047605959</link>    <guid>https://segmentfault.com/a/1190000047605959</guid>    <pubDate>2026-02-11 17:05:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>少年呀，当你遇到这样的情况：API 慢得像蜗牛，P95 延迟超高，服务器在凌晨 3 点因为流量突发而崩溃，你是选择花三个月用 Rust 重写所有东西，还是选择看着用户流失呢。</p><p>或者，你可以像我一样，用一种作弊的方式，把 Bun 的极致速度嫁接到 Node.js 的庞大生态上。</p><p>别笑，我是认真的。我就能在不重写 5 年陈旧业务逻辑的前提下，把一个臃肿的后端接口压进 10ms 以内的。</p><p><img width="723" height="384" referrerpolicy="no-referrer" src="/img/bVdnUD9" alt="image.png" title="image.png"/></p><h3>边缘侧使用 Bun，通过 IPC 唤醒 Node 进程池</h3><p>众所周知，Node 处理 HTTP 请求的开销太大了，但我的业务逻辑里全是依赖 <code>crypto</code> 和老旧 SDK 的代码，根本没法移植到 Bun。</p><p>所以我的解决办法是前店后厂。</p><p>我用 Bun 搭建了一个极薄的 HTTP 层，专门负责路由、参数校验和挡掉无效请求。只有真正需要那个老旧业务逻辑时，我才通过 IPC（进程间通信）把任务扔给后台常驻的 Node 进程。</p><p>千万别在请求来的时候才 <code>spawn</code> 一个 Node 进程，那样比单用 Node 还慢。你要做的是预先启动一组 Node Worker，要先预热才行。。</p><p><strong>Bun 端（前台）：</strong></p><pre><code class="typescript">// bun-gateway.ts
const textDecoder = new TextDecoder();
const textEncoder = new TextEncoder();

// 启动一个常驻的 Node 进程，而不是每次请求都启动
const nodeWorker = Bun.spawn(["node", "heavy-lifter.js"], {
  stdin: "pipe",
  stdout: "pipe",
});

// 这是一个简单的读写封装，把复杂的脏活扔过去
async function askNode(payload: any) {
  const msg = JSON.stringify(payload) + "\n";
  nodeWorker.stdin.write(textEncoder.encode(msg));
  
  // 这里简化了读取逻辑，生产环境记得处理粘包
  const reader = nodeWorker.stdout.getReader();
  const { value } = await reader.read(); 
  return JSON.parse(textDecoder.decode(value));
}

Bun.serve({
  port: 3000,
  async fetch(req) {
    if (req.url.endsWith("/fast")) return new Response("Bun is fast!");
    
    // 只有这种重活才找 Node
    if (req.url.endsWith("/heavy")) {
      const data = await req.json();
      const result = await askNode(data);
      return Response.json(result);
    }
    return new Response("404", { status: 404 });
  },
});</code></pre><p><strong>Node 端（后台）：</strong></p><pre><code class="javascript">// heavy-lifter.js
const readline = require('readline');

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
  terminal: false
});

rl.on('line', (line) =&gt; {
  const data = JSON.parse(line);
  // 假装我们在做一个很重的加密运算
  // Node 生态里的老代码都在这儿跑，不用改
  const result = { processed: true, echo: data };
  console.log(JSON.stringify(result));
});</code></pre><p>这样搞，路由和 I/O 也是亚毫秒级的，而 Node 只需要处理纯计算，效率直接翻倍。</p><h3>别让 CPU 搬砖，学会利用 Bun 的零拷贝特性</h3><p>我发现服务器 CPU 居高不下，居然是因为我们在读取本地的配置文件和静态 JSON，然后序列化发给用户。</p><p>在 Node 里，通常会 <code>fs.readFile</code> 然后 <code>res.send</code>。这中间发生了好几次数据拷贝：从磁盘到内核，到用户空间 buffer，再到 socket。</p><p>在 Bun 里，我改用 <code>Bun.file()</code>。这不仅是写法上的区别，这是直接告诉操作系统：“把这个文件扔到网卡上去，别经过我的手。”</p><pre><code class="typescript">// 别再 readFile 了，直接流式传输
Bun.serve({
  fetch(req) {
    if (req.url.endsWith("/config")) {
      return new Response(Bun.file("./big-config.json"));
    }
    return new Response("404");
  }
});</code></pre><p>这一行代码改动，让我的静态资源吞吐量提升了 3 倍。</p><h3>排好队，微批处理（Micro-batching）</h3><p>高并发最可怕的是什么？是 1000 个请求同时涌进来，每个都要单独去调一次数据库或者调一次 Node 进程。就像刚下课，一堆学生全涌到食堂打饭。</p><p>而我加了一个极小的缓冲窗口。</p><p>如果在 3 毫秒内来了 50 个请求，我把它们打包成一个数组，一次性发给 Node 或者数据库。</p><pre><code class="typescript">let buffer: any[] = [];
let timer: Timer | null = null;

function processBatch() {
  const currentBatch = buffer;
  buffer = [];
  timer = null;
  // 一次性把 50 个任务发给 Node，而不是发 50 次
  askNode({ type: 'batch', items: currentBatch });
}

function enqueue(item: any) {
  buffer.push(item);
  // 只有在第一次推进来时启动计时器
  if (!timer) {
    timer = setTimeout(processBatch, 3); // 3ms 的延迟用户无感，但吞吐量巨大提升
  }
}</code></pre><p>这 3 毫秒的等待，换来的是 CPU 负载降低 60%。</p><h3>别在循环里 <code>new</code> 对象，求你了</h3><p>我审查代码时发现，很多人喜欢在 <code>fetch</code> 或者 <code>handleRequest</code> 里写 <code>const db = new DatabaseClient()</code> 或者 <code>const regex = new RegExp(...)</code>。</p><p>每次请求都重新分配内存、建立连接、编译正则，GC（垃圾回收）不炸才怪。</p><p>把所有能复用的东西——数据库连接池、<code>TextEncoder</code>、正则表达式、加密 Key，全部提到全局作用域。在 Bun 和 Node 混合架构里，这一点非常重要，因为我们追求的是极致的低延迟。</p><h3>双层缓存：内存不够，磁盘来凑</h3><p>以前我只用 Redis，但网络请求还是有开销。后来我发现，Bun 读取文件的速度超级快。</p><p>于是我搞了个双层缓存：</p><ol><li><strong>L1 内存缓存</strong>：用 LRU 存最热的 1000 个 Key，微秒级响应。</li><li><strong>L2 文件缓存</strong>：把稍微冷一点的数据直接写成 JSON 文件放在 <code>/tmp/cache/</code> 下。</li></ol><p>检查文件是否存在，比发起一个 TCP 请求去连 Redis 要快得多。</p><h3>丢掉那些臃肿的 npm 包</h3><p>以前在 Node 里，为了生成个 UUID 或者解析个参数，我们习惯性 <code>npm install uuid</code> 或者 <code>qs</code>。</p><p>在 Bun 里（其实现代 Node 也是），<code>crypto.randomUUID()</code>、<code>URLSearchParams</code> 都是内置的，而且是 C++ 层面优化的。</p><p>我把代码里所有非必要的 npm 依赖全部剔除，改用原生 API。这不仅让冷启动快了，更重要的是减少了 <code>node_modules</code> 的 I/O 噩梦。</p><h3>解决精神分裂的开发环境</h3><p>这一套架构就是Bun 做网关，Node 做计算。但在本地开发时，我差点崩溃。</p><p>我的电脑上本来跑着 Node 22，为了维护老项目，又要装 Node 14，还要装 Bun，甚至偶尔还要用 Deno 跑个脚本。</p><p><code>nvm</code> 切换来切换去让我心力交瘁，端口冲突、路径报错、环境变量乱成一锅粥。我经常是修好了 <a href="https://link.segmentfault.com/?enc=8dZIdGctv4OSXoinJ0Ukkg%3D%3D.VUG66AULKqZNQEenhVio1R4kQKwxFz7sW8iJ4AFjqjo61r9885EV261Ln6AUkIay" rel="nofollow" target="_blank">Bun 环境</a>，Node 的老项目又跑不起来了。</p><p>直到我发现了 ServBay，开发者的救命稻草，它不是那种简陋的版本切换器，它是一个完整的、隔离的运行环境平台。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnUEb" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>多版本并存</strong>：我可以同时开启 Node 14、Node 22 和 Bun 1.1 的环境，它们之间完全隔离，互不打架。</li><li><strong>一键全家桶</strong>：我需要的 Redis（做缓存）、PostgreSQL（存数据）、Caddy（做反代），它全都能一键安装并运行。</li></ul><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnUEc" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>零配置</strong>：我不由得感叹，以前为了配 Docker 和 Homebrew 浪费了不少时间啊。</li></ul><p>有了 ServBay，我在本地完美复刻了线上的混合架构：Bun 监听 3000 端口，Node 监听内部管道，Redis 跑在后台。我再也不用担心这是环境问题还是代码问题了。</p><h3>总结</h3><p>只要能把响应时间压进 10ms，我不在乎混用多少种运行时。</p><p>Bun 给了我速度，Node 给了我稳定性，ServBay 给了我一个不发疯的<a href="https://link.segmentfault.com/?enc=VzgGzy7uG3k8MzIV4WFdsg%3D%3D.QcTMNePLfDJRfxC4qYSStR%2B6TJqTdKrpSQkJrX3qyE0%3D" rel="nofollow" target="_blank">开发环境</a>。</p><p>别再纠结用 Bun 还是用 Node.js了，都成年人了，为什么不能两个都要。把它们结合起来，现在就去把你的 API 延迟砍掉 90%。</p>]]></description></item><item>    <title><![CDATA[内存占用最高降低75%，美国能源部科学家提出跨通道分层聚合方法D-CHAG，实现极大规模模型多通道数]]></title>    <link>https://segmentfault.com/a/1190000047605973</link>    <guid>https://segmentfault.com/a/1190000047605973</guid>    <pubDate>2026-02-11 17:04:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>基于视觉的科学基础模型在推动科学发现与创新方面具有巨大潜力，主要源于其能够聚合多样化来源的图像数据（例如不同的物理观测场景），并利用 Transformer 架构学习时空相关性。然而，图像的 token 化与聚合过程计算开销巨大，而现有的分布式方法如张量并行（TP）、序列并行（SP）或数据并行（DP），尚未充分解决这一挑战。</p><p>在此背景下，来自美国能源部橡树岭国家实验室的研究人员提出了一种面向基础模型的分布式跨通道分层聚合方法（Distributed Cross-Channel Hierarchical Aggregation, D-CHAG）。该方法对 token 化过程进行分布式处理，并采用分层策略进行通道聚合，从而使极大规模模型能够在多通道数据集上运行。研究人员在高光谱成像与天气预测任务上对 D-CHAG 进行了评估，将该方法与张量并行和模型分片相结合后，在 Frontier 超级计算机上最多可将内存占用降低 75%，并在最多 1,024 块 AMD GPU 上实现持续吞吐量提升超过 2 倍。</p><p>相关研究成果以「Distributed Cross-Channel Hierarchical Aggregation for Foundation Models」为题，已发表于 SC25。</p><p>研究亮点：</p><ul><li>D-CHAG 解决了多通道基础模型训练中的内存瓶颈和计算效率问题</li><li>与仅使用 TP 相比，D-CHAG 可实现最高 70% 的内存占用降低，从而支持更高效的大规模模型训练</li><li>在天气预测与高光谱植物图像掩码预测两种科学工作负载上验证了 D-CHAG 的性能</li></ul><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnUD7" alt="" title=""/><br/><em>论文地址：</em>\<br/><em><a href="https://link.segmentfault.com/?enc=HC5do4DTSaAHR2eLuBbNfw%3D%3D.NIGG8KkSGJCCjtp0%2Fbo2A9NeAJjN%2F9opoHLGJD6bnU0%2BJGLaAhIpr3mWIJG6%2Biym" rel="nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3712285.3759870</a></em>\<br/>关注公众号，后台回复「跨通道」获取完整 PDF</p><h2>使用两类典型的多通道数据集</h2><p>本研究使用了两类典型的多通道数据集来验证 D-CHAG 方法的有效性：植物高光谱图像（Hyperspectral Images）和气象 ERA5 数据集。</p><p>其中，用于自监督掩码预测的植物高光谱图像数据由 Oak Ridge National Laboratory（ORNL）高级植物表型实验室（APPL） 收集。数据集包含 494 张杨树（Poplar）高光谱图像，每张图像包含 500 个光谱通道，覆盖波长从 400nm 到 900nm。</p><p>此数据集主要用于生物质研究，是植物表型分析和生物能源研究的重要资源。这些图像用于掩码自监督训练，即将图像切片作为 token 进行 mask，模型的任务是预测缺失的内容，从而学习图像的潜在数据分布。值得注意的是，该数据集未使用任何预训练权重，完全基于自监督学习进行训练，这也凸显了 D-CHAG 在高通道自监督任务中的适用性。</p><p>此外，在气象预测实验中，研究团队使用了 ERA5 高分辨率再分析数据集。研究选择了 5 个大气层变量（位势高度、温度、风速 u 分量、风速 v 分量、比湿度）和 3 个地表层变量（2 米温度、10 米 u 分量风速、10 米 v 分量风速），覆盖超过 10 个压力层，总共生成 80 个输入通道。为了适配模型训练，原始分辨率为 0.25° 的数据（770 × 1440）被重网格化为 5.625°（32 × 64），采用 xESMF 工具包 和双线性插值算法完成。</p><p>模型任务是进行未来时间步的气象变量预测，例如 500 hPa 位势高度（Z500）、850 hPa 温度（T850）、10 米 u 分量风速（U10），从而验证 D-CHAG 方法在时间序列预测任务上的性能。</p><h2>D-CHAG ：将层级聚合与分布式 Token 化结合</h2><p>简单而言，D-CHAG 方法来自两种独立方法的融合，分别是：</p><p>分布式 token 化方法</p><p>在前向传播过程中，每个 TP rank 仅对输入通道的子集进行 token 化。在进行通道聚合步骤之前，需要执行一次 AllGather 操作，以便在所有通道之间实现跨通道注意力（cross-attention）。理论上，该方法能够降低每块 GPU 的 token 化计算开销。</p><p>层级跨通道聚合</p><p>这种方法的主要优势在于每个跨通道注意力层的内存占用减少，因为每层处理的通道数量更少。然而，增加层数会导致整体模型规模增大、内存使用增加。对于通道数量庞大的数据集而言，这种权衡更为有利，因为标准跨通道注意力的二次内存开销更高。</p><p>这两种方法虽然各有优势，但也存在一些不足，比如分布式 token 化方法在 TP rank 之间存在较高的通信开销，并未解决通道维度大内存占用的问题；而层级跨通道聚合方法会增加每块 GPU 上的模型参数数量。D-CHAG 方法通过分布式方式将两种方法结合起来，整体架构如下图所示：</p><p><img width="723" height="444" referrerpolicy="no-referrer" src="/img/bVdnUD8" alt="" title="" loading="lazy"/><br/>D-CHAG 方法在基础架构上的示意图</p><p>具体而言，每个 TP rank 对总通道子集中的二维图像进行 token 化。由于每块 GPU 仅持有全部通道的一部分，在这些通道上本地执行通道聚合——该模块称为部分通道聚合模块（partial-channel aggregation module）。在每个 TP rank 内完成通道聚合后，收集输出并使用跨通道注意力进行最终聚合。前向传播过程中仅需执行一次 AllGather 操作；在反向传播时，只收集每块 GPU 的相关梯度，从而避免额外通信。</p><p>D-CHAG 方法能够充分利用分布式 token 化和层级通道聚合的优势，同时缓解它们的不足。通过将层级通道聚合分布到 TP rank 上，研究人员将 AllGather 通信减少为每个 TP rank 仅需处理单个通道，在反向传播过程中无需任何通信。此外，通过增加模型深度保留了每层聚合处理通道数量减少的优势，同时通过部分通道聚合模块将额外模型参数分布到各 TP rank 上。</p><p>研究对比了两种实现策略：</p><ul><li>D-CHAG-L（Linear Layer）：层级聚合模块使用线性层，内存占用低，适合通道数较多的情况。</li><li>D-CHAG-C（Cross-Attention Layer）：使用交叉注意力层，计算成本较高，但在超大模型或极高通道数时性能提升显著。</li></ul><h2>成果：D-CHAG支持高通道数数据集上更大模型的训练</h2><p>在构建 D-CHAG 后，研究人员对模型性能进行了验证，然后进一步评估了其在高光谱成像与天气预测任务上的表现：</p><h3>模型性能分析</h3><p>下图展示了 D-CHAG 在不同部分通道聚合模块配置下的性能表现：</p><p><img width="723" height="420" referrerpolicy="no-referrer" src="/img/bVdnUEf" alt="" title="" loading="lazy"/><br/>图中展示了针对 1.7B 参数模型，在不同部分通道聚合模块配置下，每块 GPU 相对于仅使用 TP 基线的性能提升</p><ul><li>Tree0 表示部分聚合模块中仅有一层聚合，Tree2 表示两层，依此类推；</li><li>后缀 -C 和 -L 表示所用层的类型：-C 中所有层为 cross-attention，-L 中所有层为 linear</li></ul><p>结果显示：</p><p>对于 512 通道数据，使用单层 cross-attention 层的性能略低于基线，但对 1024 通道数据可提升约 60%。</p><p>随着层次结构加深，即便是 512 通道数据，也能获得明显性能提升，而 1024 通道数据的性能保持相对稳定。</p><p>使用 linear 层时，即使层次结构较浅，也能在 512 和 1024 通道图像上获得性能提升。实际上，最佳性能出现在 D-CHAG-L-Tree0，即仅包含一层通道聚合层。增加聚合层会增加模型参数，引入额外内存开销。虽然对于 512 通道情况，增加层数似乎有益，但对于两种通道规模，仅使用一层 linear 层的性能优于更深的配置。</p><p>D-CHAG-C-Tree0 在两块 GPU 时对性能略有负面影响，但扩展至 8 块 GPU 时可获得 60% 提升。</p><h3>植物高光谱图像的自监督掩码预测</h3><p>下图比较了基线方法与 D-CHAG 方法在高光谱植物图像掩码自编码器应用中的训练损失，结果显示：在训练过程中，单 GPU 实现与 D-CHAG 方法（在两块 GPU 上运行）的训练损失表现高度一致。</p><p><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnUEj" alt="" title="" loading="lazy"/><br/>基线方法与 D-CHAG 方法在高光谱植物图像掩码自编码器应用中的训练损失</p><p>橡树岭国家实验室分子与细胞成像组的高级研究员拉里·约克表示，D-CHAG 可以帮助植物科学家快速完成诸如直接从图像中测量植物光合作用活性等任务，从而取代费时费力的手动测量。</p><h3>天气预测</h3><p>研究人员在 ERA5 数据集上进行 30 天气象预测实验，下图比较了基线方法与 D-CHAG 方法在天气预测应用中的训练损失及三个测试变量的 RMSE：</p><p><img width="723" height="690" referrerpolicy="no-referrer" src="/img/bVdnUEo" alt="" title="" loading="lazy"/><br/>基线方法与 D-CHAG 方法在天气预测应用中的训练损失及三个测试变量的 RMSE</p><p>下表则展示了模型在 7、14 和 30 天预测任务上的最终对比，包括 RMSE、MSE 以及 Pearson 相关系数（即 wACC）</p><p><img width="723" height="430" referrerpolicy="no-referrer" src="/img/bVdnUEp" alt="" title="" loading="lazy"/><br/>D-CHAG 方法相较于单 GPU 训练在 7、14 和 30 天预测任务中的 MSE、RMSE 及 wACC 的百分比变化（% Δ）</p><p>结合图和表总体来看，训练损失与基线模型高度一致，各项指标的偏差极小。</p><h3>随模型规模扩展的性能</h3><p>下图显示了 3 种模型规模在需要使用 TP 的通道配置下，D-CHAG 方法相较于仅使用 TP 的性能提升：</p><p><img width="723" height="657" referrerpolicy="no-referrer" src="/img/bVdnUEq" alt="" title="" loading="lazy"/><br/>D-CHAG 方法结合 TP 的情况下，相较于仅使用 TP 时，7B、15B 和 26B 参数模型每个 GPU 的性能提升情况</p><p>结果显示，对于 7B 参数模型，使用部分通道聚合模块中的线性层（linear layers）可获得 30% 至 70% 的性能提升，而使用交叉注意力层（cross-attention layers）可获得 10% 至 60% 的提升；对于 15B 参数模型，性能提升超过 20% 至 50%；而 26B 参数模型的性能提升在 10% 至 30% 之间。</p><p>此外，在固定模型规模下，随着通道数增加，性能提升更明显，这是因为在给定架构下，增加通道数不会增加 transformer block 的计算量，但会增加 tokenization 和 channel-aggregation 模块的工作量。</p><p>另一方面，仅使用 TP 无法训练 26B 参数、256 通道图像，但使用 D-CHAG 方法时，可以训练 26B 参数、512 通道的模型，仅使用不到 80% 的可用内存——这表明该方法能够支持高通道数数据集上更大模型的训练。</p><h2>ViT：视觉 AI 从感知模型走向通用视觉基础模型</h2><p>过去十年，计算机视觉模型主要围绕「单任务优化」展开——分类、检测、分割、重建各自独立发展。然而，随着 Transformer 架构在自然语言领域催生出 GPT、BERT 等基础模型（Foundation Models），视觉领域也正在经历类似的范式转移：从任务特化模型走向通用视觉基础模型。在这一趋势下，Vision Transformer（ViT）被视为视觉基础模型的关键技术基石。</p><p>Vision Transformer（ViT）首次将 Transformer 架构完整引入计算机视觉任务，其核心思想是：将图像视为一系列 patch token 序列，用自注意力机制替代卷积神经网络的局部感受野建模。具体而言，ViT 将输入图像划分为固定大小的 patch，并将每个 patch 映射为 embedding token，然后通过 Transformer Encoder 建模 patch 之间的全局关系。</p><p>与传统 CNN 相比，ViT 对科学数据尤其具有优势：适合高维多通道数据（如遥感、医学影像、光谱数据），可处理非欧几里得空间结构（如气候格点、物理场），适用于跨通道建模（不同物理变量之间的耦合关系），这也正是 D-CHAG 论文所关注的核心问题。</p><p>除了上文研究中提及的场景，ViT 正在更多场景发挥核心价值。2025 年 3 月，北京大学国际医院皮肤科主任医师韩钢文携其团队开发出一种名为 AcneDGNet 的深度学习算法，这是一种融合视觉 Transformer 与卷积神经网络，能获取更高效的分层特征表，让分级更精准。经前瞻性评估表明，AcneDGNet 的深度学习算法不仅比初级皮肤科医生更准确，而且与高级皮肤科医生的准确性相当，能够在不同的医疗保健场景中同时准确地完成痤疮病变检测并判断严重程度，有效帮助皮肤科医生和患者在在线问诊和线下就医场景中诊断和管理痤疮。</p><p>论文标题：</p><p>Evaluation of an acne lesion detection and severity grading model for Chinese population in online and offline healthcare scenarios\<br/>论文地址：</p><p><a href="https://link.segmentfault.com/?enc=M%2Bkq4tTgvS%2F%2B83n0SSqUMw%3D%3D.UfGeLtaBrBFGzrzQ9gF2cnHJ5v4QxmqY%2BkpgHO7wOeNehi1yCC6IIYcN%2BufKzJ0bZhqJDLjjH6msXCBnBwCHfg%3D%3D" rel="nofollow" target="_blank">https://www.nature.com/articles/s41598-024-84670-z</a></p><p>从产业视角看，Vision Transformer 标志着视觉 AI 从感知模型走向通用视觉基础模型的关键拐点。其统一的 Transformer 架构为跨模态融合、规模化扩展与系统级优化提供了通用底座，使视觉模型成为 AI for Science 的核心基础设施。未来，围绕 ViT 的并行化、内存优化与多通道建模能力，将成为决定视觉基础模型产业落地速度与规模的关键竞争点。</p><p>参考文献：<br/>\<br/>1.<a href="https://link.segmentfault.com/?enc=DMehiJrP2nZ%2B2EQ%2FLeSDJA%3D%3D.Odz0qg1iBjflQ1SWVpqH%2FPXRp8bw2kfemmcd0RSkYb%2B9CoHtot1%2Be6PXPNRDdJfqDRabtPx%2FzZ203Enl%2BFHyMw%3D%3D" rel="nofollow" target="_blank">https://phys.org/news/2026-01-empowering-ai-foundation.html</a><br/>\<br/>2.<a href="https://link.segmentfault.com/?enc=UwXcIUpz98k2csPOUIrexA%3D%3D.1rUdce6OrGPiXCtQ3yy9sM13cdSMJ6k%2FAN5jSUPGqIkfrPKuiAQj2kiv8Z0L0Tyc" rel="nofollow" target="_blank">https://dl.acm.org/doi/10.1145/3712285.3759870</a><br/>\<br/>3.<a href="https://link.segmentfault.com/?enc=hZvQYnQWnigT5oNlxKU13A%3D%3D.CEqBJKFn0UCP%2BTOBMWvY%2FDNue7WEyc3GsbN1QyJbhT2F9QRXzpcb8GUOyqUzIqKjKGVrqoIWe9z7QHrEg2NBNQ%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/JvKQPbBQFhofqlVX4jLgSA</a><br/></p>]]></description></item><item>    <title><![CDATA[智能制造企业如何选研发管理平台？看看行业标杆客户怎么说 PM老周 ]]></title>    <link>https://segmentfault.com/a/1190000047605997</link>    <guid>https://segmentfault.com/a/1190000047605997</guid>    <pubDate>2026-02-11 17:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>智能制造行业的研发协作，重点在于能不能把复杂链路管住：需求如何收敛、研发如何协同、测试如何闭环、交付如何可追溯、跨团队如何对齐、IPD/ASPICE等体系如何落地。当产品线越来越多、软硬件协同越来越频繁、跨地域研发成为常态时，一套能承载复杂研发节奏的研发管理平台，就会从“效率工具”变成“研发体系的数字底座”。</p><h2>一、智能制造行业的研发痛点</h2><p><strong>1）项目越来越多，信息越来越散，管理靠“人肉对齐”</strong></p><p>多项目并行、跨部门协同、跨地域交付，最典型的问题是：</p><ul><li>进度、风险、资源分布无法实时汇总</li><li>信息分散在群聊/邮件/表格/个人文档里</li><li>关键决策缺乏数据支撑，复盘也缺少事实链路</li></ul><p><strong>2）需求—研发—测试—交付链路断裂，质量与交付不可控</strong></p><p>智能制造企业通常兼顾硬件、软件、嵌入式、算法、平台等多团队协作，如果链路不统一，常见现象是：</p><ul><li>需求变更难追踪，交付范围经常漂移</li><li>缺陷无法闭环回溯，测试用例与版本发布脱节</li><li>交付质量依赖“项目经理盯人”，难规模化复制</li></ul><p><strong>3）流程体系想落地（IPD/ASPICE/功能安全），但缺少“可执行的数字化载体”</strong></p><p>很多企业在推 IPD、ASPICE、功能安全、矩阵式协作时，会遇到：</p><ul><li>流程写在制度里，执行落在个人习惯里</li><li>指标口径不统一，过程数据难沉淀</li><li>跨团队协作缺少统一的协作语言与工作项标准</li></ul><p><strong>4）权限与数据治理要求更高，既要共享协同，又要边界清晰</strong></p><p>尤其在供应链协作、跨部门共享与研发资料沉淀场景下：</p><ul><li>哪些信息该共享？哪些必须隔离？</li><li>权限边界是否能按组织/角色/项目分层治理？</li><li>项目过程资产如何沉淀为可复用知识？</li></ul><h2>二、针对这些痛点，ONES 的解题思路是什么？</h2><p>下面的能力点不是简单的功能罗列，而是把智能制造企业最常见的管理诉求，归纳为几类平台能力。</p><p><strong>1）把研发全过程放到一套统一协作体系里，让信息可视、可追溯</strong></p><p>在智能制造企业里，平台要能把工单、需求、任务、迭代、缺陷、测试等协作对象串起来，形成统一链路：</p><ul><li>项目全生命周期可视化跟踪</li><li>端到端追溯（从需求到上线/交付）</li><li>用数据支撑管理决策</li></ul><p><strong>2）把“流程标准化”变成可执行的线上机制，支撑多产品线、多团队协作</strong></p><p>平台不仅要能支持标准化，还要能在不同团队/不同项目下灵活配置：</p><ul><li>支持按团队/项目配置更贴合自身的协作模式</li><li>兼顾流程规范与灵活性</li><li>多项目并行时，依然能保持信息透明与对齐效率</li></ul><p><strong>3）支撑 IPD/ASPICE 等体系落地：把方法论固化成流程与数据</strong></p><ul><li>对推体系的企业来说，平台要能承载：</li><li>IPD 研发管理体系的数字化落地</li><li>ASPICE 认证相关过程要求的执行与追溯</li><li>研发流程的数字化与知识化沉淀</li></ul><p><strong>4）支撑跨部门、跨地域协同与权限治理：共享协作与边界管理并重</strong></p><p>平台需要既能促进共享，又能避免数据泄露和越权访问：</p><ul><li>多层权限保障信息安全</li><li>知识库与项目协作打通，同时保持权限边界可控</li><li>跨研发中心协作效率提升，降低交接成本</li></ul><h2>三、来自 ONES 智能制造行业标杆客户证言</h2><p>以下内容来自 ONES 智能制造相关行业客户证言。若你正在评估研发管理平台，可将这些原话作为参考样本，对照自身的研发链路复杂度（需求—研发—测试—交付）、跨团队协作与过程追溯要求进行判断。</p><p><strong>诺瓦星云：全球最大的 LED 显示解决方案服务商</strong></p><p>研发负责人：ONES 上线后，通过数据治理、流程梳理与系统配置培训，高效助力我司落地软件研发项目标准化建设，各产品线之间信息共享互通，提升了 IT 管理和研发效率。同时推动 IPD 项目在 ONES 落地，破解数据分散难题，降低管理难度。ONES Wiki 打通项目管理与文档管理的数据壁垒，解决了数据安全和权限管控问题。</p><p><strong>光格科技：产品服务于国家电网、南方电网、华能集团、国家电投等</strong></p><p>研发管理部副总监李老师：公司使用 ONES 系统已三年多了，ONES 对我们团队的研发管理效率起到很大提升，尤其是 ONES Wiki 知识库功能丰富，有效解决了部门间的沟通协作难题，显著提升了协作效率。</p><p><strong>埃斯顿：连续七年稳居中国工业机器人国产品牌出货量首位</strong></p><p>我司通过 ONES 实现项目在线化统一管理，串联起对待导入需求的收集及响应，有效管控研发参与的不同类型的项目，精细化到具体不同工作项的规范管理，整体提升项目的在线规范化管理、降低沟通成本、提升团队效能。</p><p><strong>良信股份：27年智慧低压电气解决方案专家</strong></p><p>软件开发部：感谢 ONES 实施团队在过去两年中给予我们的专业支持。你们不仅协助我们高效配置了需求、任务与缺陷管理流程，还将测试场景和用例系统纳入统一管理，使项目全链路清晰可控。每当遇到问题，团队总能快速响应、专业解决，帮助我们切实提升了研发协同效率与交付质量。诚挚感谢你们的用心付出!</p><p><strong>卡莱特：领先的视频图像领域综合化解决方案服务商</strong></p><p>项目管理部负责人：在引入 ONES 项目管理系统之后，我们实现项目全流程统一管理，并且可以根据不同团队和项目的实际情况进行灵活配置。通过属性、工作流和视图的搭配，项目组能打造更贴合自身的协作模式，提升信息透明度，沟通对齐的成本也随之降低。多项目并行时，ONES 能兼顾流程规范性和灵活性，高效提升项目管理效率。</p><p><strong>凌云光：机器视觉与光纤器件行业领导者</strong></p><p>通过 ONES 项目管理平台，我们实现了对复杂项目全生命周期的可视化跟踪与精准管控，全局进度一目了然，跨部门协同效率显著提升，它已成为我们保障项目如期上线不可或缺的“项目管理引擎”！</p><p><strong>瑞纳智能：智慧供热行业第一家上市公司</strong></p><p>瑞纳智能正处于技术升级与业务拓展的关键阶段，现阶段以研发创新为核心，依托 ONES 研发管理系统，实现了跨业务线的研发项目全生命周期高效管控，显著提升技术成果转化效率，加速智能化产业布局。</p><p><strong>科思科技：国内领先的电子信息装备供应商</strong></p><p>深圳科思科技依托 ONES 构建符合 IPD 核心理念的研发项目管理平台，实现研发流程数字化与知识化升级，系统性提升产品开发效率、质量与资源利用率，打造国际化端到端集成产品开发能力。</p><p><strong>中盾安民：国内最早研发生产安检产品的单位</strong></p><p>通过 ONES，我司实现了项目信息的可视化、流程化，提高了管理透明度和规范性，便于及时发现项目风险和问题，提升了团队协作效率，降低了沟通成本，为进一步精细化管理奠定了良好基础。</p><p><strong>鸿湖万联：软通动力旗下控股公司</strong></p><p>研发负责人：ONES 帮助我们将项目不同团队的工作流全面打通，实现了从需求到上线的端到端可视化追踪，通过多种图表形态，实时呈现项目表现，保障了软件研发过程的完整性、一致性和可追溯性，提高了研发效率。</p>]]></description></item><item>    <title><![CDATA[模型微调：AI+场景下的落地实践 Fabarta ]]></title>    <link>https://segmentfault.com/a/1190000047605999</link>    <guid>https://segmentfault.com/a/1190000047605999</guid>    <pubDate>2026-02-11 17:03:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者：枫清实验室团队</p><h2>1.DeepSeek 带来的一些思考</h2><p>DeepSeek 的技术突破，并不仅体现在模型指标或参数规模上，而在于其清晰地展示了一种趋势：模型能力的跃迁，正在从“更大的预训练”转向中期+后训练阶段对“知识、行为与推理模式”的系统性对齐。这些变化，对工业界尤为重要。相比通用模型评测，企业场景往往具有以下特征：</p><p>任务分布高度集中，但规则与边界极其复杂，SOP与隐性经验并存；错误代价远高于“泛化能力不足”，模型“答错了还看起来很确定”往往比“不回答”更危险；模型的主要风险并非知识缺失，而是在规则边界与异常组合条件下，沿着看似合理但不可控的推理路径持续放大错误。</p><p>这些特征决定了企业对微调范式的需求，必然超越技术选型本身。因此，对大多数企业而言，真正的关注点并不在于"是否要用蒸馏或 SFT"，而在于如何用一条工程上可控、可复用、可演进的路径，对齐模型的真实业务能力？<br/>在这一视角下，模型训练不再是一次性的参数优化问题，而是一个由数据→ 结构 → 行为逐层约束、逐步收敛的系统工程。</p><h2>2.工业场景下的逐层约束</h2><p>在化工、单证、电磁频谱、跨境报关等客户的高专业度场景中，这些约束具体表现为：</p><ul><li>数据约束<br/>领域知识天然以结构化形式存在，但模型是参数化的。通用大模型并不天然具备对这些结构的稳定建模能力，而领域知识通常以如下形式存在：</li></ul><p>Plain Text<br/>(实体 A, 关系 R, 实体 B) + 条件/规则 C → 结论 D</p><p>例如：</p><ol><li>高分子聚合物A 在温度 T、压力 P 下 → 反应路径 X</li><li>报关品类HSCODE → 对应监管规则集合</li><li>频谱频段f → 允许/禁止的使用方式</li></ol><p>知识与行为约束在化工与电磁频谱等高专业度场景中，规则复杂、条件多维，模型“只学语言模式”远不足以支撑安全可靠的推理。需要采用结构化知识注入策略，将关键知识显式内化到模型参数中，实现多条件、多约束推理的稳定性电磁频谱。</p><p>Plain Text<br/>Ltotal = 语言建模损失 LLM(x,y) + 知识一致性损失 λLKG(y,K)</p><ul><li>推理与策略约束<br/>工业场景中，模型推理阶段所面对的输入往往更加嘈杂、不完整且高度偏向边界情况，当面临一些多约束推理时，推理轨迹容易产生漂移。例如，海关出口货物报关单中商品编码“4819100000” 字段提取0数量的遗漏，导致整单提取的失败。商品规格型号"切割冲孔过的"中核心语义的一致性问题导致进出口优惠的错误等等。同样边缘规则触发异常操作判断，例如单证业务中印章和信息重叠部分的提取错误也容易形成上游业务的级联崩塌。</li></ul><p>Plain Text<br/>Lon-policy = 分布对齐 KL(Pθ||Pteacher) + 约束惩罚 β[违反约束 c]</p><p>在此约束下，我们开始探索，在数据受限条件下的高质量数据集构建，并采用高效微调的方式注入领域知识，将复杂业务规则内化为模型行为，同时利用策略蒸馏建立可控的错误边界。</p><h2>3.高质量数据集构建</h2><h3>数据结构化</h3><p>在分子科学、化学与材料等高专业领域中，模型能力的瓶颈并不主要来自通用语义理解，而来自：专业概念密集，论证链条长，方法与条件强依赖上下文，表达高度依赖论文结构（章节、段落、实验设置）。将分散在学术PDF 中的隐性知识结构，转化为可用于模型训练与评估的高质量问答样本，并显式保留证据上下文与质量度量信息。</p><p>整个增强流程遵循两个基本原则：第一，先结构化，再生成。只有将原始论文从排版文档转化为段落级、章节级的可计算结构，LLM 生成的问题与答案才具有稳定语义边界。第二，生成必须伴随质量评估与自动筛选。在高风险专业领域，不能接受“看起来合理但事实上不严谨”的样本，数据构建过程本身必须引入自动评估闭环。从教材、多源学术文献出发，经过文档解析、结构重建、问题生成、质量评估、相似度校验与排序筛选，最终构建带有上下文与质量指标的高质量问答数据集。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606001" alt="图片1.png" title="图片1.png"/></p><p><strong>应用场景</strong><br/>本数据集结构化的构建流程应用于客户的材料研发人员内部检索与设计辅助系统，结构化后的问答数据语料作为领域模型SFT和 RAG 的检索知识单元，例如典型场景：</p><ul><li>逆合成信息查询</li><li>产物预测</li><li>反应条件查询</li><li>物质信息和化工材料专业查询等</li></ul><h3>数据即“隐性规则”的显式化载体</h3><p>大量企业知识并不存在于文档中，而是：</p><ul><li>SOP 的例外条款</li><li>老员工的经验判断</li><li>审批中的“潜规则”</li></ul><p>示例:海关申报中的"潜规则"</p><blockquote>老员工经验:"如果货值低于申报阈值但重量异常大,需要人工复核"</blockquote><p>形式化为: <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606002" alt="图片2.png" title="图片2.png" loading="lazy"/></p><p>数据构建策略:</p><ul><li>正样本生成:在约束边界内随机采样；</li><li>负样本生成:故意违反某个约束,并标注错误类型；</li><li>对比对生成:(X,Y正确,Y错误,差异说明)。</li></ul><h3>数据图谱化</h3><p>把知识最小单元，从句子级，降到关系级。这是做结构化控制的前提。把训练样本变成：「问题+ 结构约束上下文 + 多跳结构视角」。</p><p>首先，对原始问答数据进行联合语义解析，将隐含在自然语言中的领域知识抽取为实体—关系—实体 的可计算结构。</p><p>通过关系归一化（relation canonicalization），将高噪声、长尾的细粒度关系映射至有限的核心关系集合，并显式构建正反向关系，以提升结构连通性与推理可达性。<br/>在关系抽取完成后，将三元组组织为有向多关系图结构，允许实体间的多语义连接，以保留领域知识的结构复杂性。</p><p>针对具体问答样本，以文本中出现的实体为条件，进行受控的多跳子图扩展，构建与当前任务最相关的局部知识视角。通过显式限制扩展深度与节点规模，在覆盖潜在推理路径的同时抑制结构噪声。</p><p>通过上述流程，我们将原始问答数据中的隐性领域知识，逐步转化为可计算、可裁剪、可内化的结构化表示，并将其系统性地引入模型训练过程，从而在不进行全参数更新的前提下，显著增强模型对领域逻辑与推理结构的稳定建模能力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606003" alt="图片3.png" title="图片3.png" loading="lazy"/></p><h2> 4.知识注入</h2><h3>图谱结构化</h3><p>注入使用Adapter 结构，避免破坏基础模型参数，将实体与关系嵌入注入注意力或 FFN 层，其本质不是“让模型记住更多事实”，而是：在参数层面引导模型形成稳定的结构化推理模式。</p><p>在大语言模型的多层Transformer 结构中插入轻量级图谱适配模块；图谱适配采用“降维—非线性变换—升维”的典型结构，参数规模远小于模型主体；将知识结构嵌入作为适配的输入或调制信号，与语言模型的隐状态进行融合；通过残差连接方式，确保原模型语言能力不被破坏。在训练阶段：冻结大语言模型的原始参数；仅更新 适配模块参数及知识结构编码模块参数；通过端到端训练实现知识表示与语言表示的协同对齐。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606004" alt="图片4.png" title="图片4.png" loading="lazy"/><br/><strong>应用场景</strong><br/>本方案应用于电磁频管业务，面向无线电管理与电磁空间治理场景，服务对象包括无线电管理机构、频谱规划部门及设备备案与用频审批单位，主要用于提升频谱法规理解、用频合规判断与复杂业务推理的自动化水平。</p><h3>Lora 微调领域知识特征表达</h3><p>与通用对话模型中的LoRA（W′=W+ΔW,ΔW=BA） 主要用于指令风格对齐不同，在材料化学领域，LoRA 承担的是：结构与规则敏感型能力的参数载体角色。具体体现在： </p><ul><li>学习SMILES 语法中的长距离依赖模式（环闭合、支链嵌套、立体标注等）；</li><li>学习反应物与产物之间的原子重排与结构映射规律；</li><li>学习反应条件、催化剂与产物分布之间的隐含关联；</li><li>学习逆合成中常见结构断裂模式与合成子映射方式。</li></ul><p>这些特征体现在损失函数的设计，损失函数计算采用了双损失机制：计算全局损失（所有token）以及针对仅化学关键token（如SMILES、分子式）的核心损失。通过差异化的损失计算策略,实现了"通用能力保持"与"领域知识强化"的有机平衡:<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606005" alt="图片5.png" title="图片5.png" loading="lazy"/><br/>全局损失在此不再赘述，核心损失如图所示：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606006" alt="图片6.png" title="图片6.png" loading="lazy"/><br/>核心token 包括:</p><ul><li>SMILES 字符串: 如 C1CCCCC1 (环己烷)</li><li>分子式: 如 C6H12 </li><li>IUPAC 命名: 如 "cyclohexane"</li><li>反应条件: 如 "催化剂: Pd/C", "温度: 120°C"</li><li>数值约束: 如 "[120-180°C]", "[0.5-2.0 MPa]"</li></ul><p>核心loss可以强制模型重点学习化学结构的精确表达,提升在分子生成、反应预测等核心任务上的性能，Core Loss 会放大 SMILES 生成错误的惩罚，即使整体语言流畅，SMILES 错误会导致 Core Loss 激增，引导模型优先学习化学结构的精确表达。</p><p><strong>应用场景</strong><br/>通过在科研智能体平台中集成领域模型，为上游业务提供统一的专业能力底座，从而增强智能化学检索、合成路径设计、反应条件优化等核心应用的底层推理引擎，同时支撑AI4S 智能体平台中的任务规划与工具调用，实现多工具协同下的自动化科研流程编排，提升整体科研效率与决策可靠性。</p><h3>策略蒸馏</h3><p>在单证识别场景中，主要错误并不集中在“是否识别到文本”，而集中在： </p><ul><li>数字位数错误（多一位/ 少一位）；</li><li>连续数字的重复与幻觉；</li><li>字段边界混乱导致的拼接错误；</li><li>对齐与排版诱导导致的空格、换行、列对齐偏差。</li></ul><p>这些错误具有一个非常明显的特征：错误高度依赖学生模型自身当前生成分布。<br/>我们采用on-policy 蒸馏范式：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047606007" alt="图片7.png" title="图片7.png" loading="lazy"/></p><ul><li>由学生模型在当前参数状态下生成推理轨迹；</li><li>对其生成过程进行规则检测与结构校验；</li><li>由教师模型或规则系统对其行为进行反馈。</li></ul><p>单证识别的本质不是“条件文本生成”，而是视觉条件下的序列对齐与字符选择问题。<br/>在此场景下，on-policy 蒸馏在该任务中具有两个直接优势：<br/>第一，能直接纠正学生常犯路径上的分布偏差。</p><ul><li>在数字序列处，学生更容易进入“重复概率高”的状态；</li><li>在相邻字段边界，学生更容易错误拼接。</li></ul><p>教师在这些“学生最容易出错的位置”上提供分布监督，远比在 GT 轨迹上模仿有效。<br/>第二，天然抑制曝光偏差（exposure bias）。<br/>学生不再只在教师路径上被训练，而是在自己的生成分布上被纠正。</p><p><strong>应用场景</strong><br/>本方案面向高准确率票据与证照场景，在小数据规模下，通过大模型蒸馏与轻量微调，实现了收据类复杂文本的高精度识别，数字与关键字段的稳定输出，轻量模型可部署能力，同时兼顾跨文档类型的泛化能力。</p><h2> 5.总结</h2><p>本文章围绕工业高专业度场景下大模型的落地优化展开，以DeepSeek 带来的行业趋势思考为切入点，针对企业场景任务集中、错误代价高、推理易漂移的核心痛点，提出了数据→结构→行为逐层约束的大模型调优体系，并从高质量数据集构建、领域知识注入、策略蒸馏三大核心环节，阐述了可工程化、可复用的落地技术路径，核心是让大模型在数据受限的工业场景中，实现知识的精准内化、推理的稳定可控。上述数据约束、结构建模、知识注入与策略蒸馏方法，在客户的一部分场景中形成实践，例如报关审单、频谱活动知识问答、材料研发辅助、单证结构化识别等多个系统中持续迭代。</p>]]></description></item><item>    <title><![CDATA[中国工业AI平台出海成功案例有哪些？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047606011</link>    <guid>https://segmentfault.com/a/1190000047606011</guid>    <pubDate>2026-02-11 17:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在全球制造业加速向智能化、柔性化和绿色化转型的背景下，工业AI平台正成为驱动这一变革的核心引擎。它不再仅仅是自动化设备的升级，而是通过数据驱动、算法决策与系统协同，重构从设计、生产到服务的全链条能力。与传统工业软件不同，真正的全球工业AI平台必须具备跨文化适配性、多语言支持能力、本地化服务生态以及可复制的行业解决方案，才能在不同国家的制造体系中生根发芽。这种平台的竞争力，不再取决于单一技术的先进性，而在于能否将中国经验转化为全球通用的语言，让不同发展水平的制造企业都能找到适合自己的数字化路径。<br/>要理解这一趋势，必须认识到工业AI平台的本质是“系统性赋能”。它不是孤立的工具，而是一个融合了边缘计算、物联网、深度学习与流程优化的有机体。在德国、日本等制造业强国，企业长期依赖高精度设备与严谨的流程管理，对AI的接受度建立在可验证的稳定性之上；而在东南亚等新兴市场，企业更关注成本控制与快速落地，平台必须足够轻量化、易部署。因此，一个真正全球化的工业AI平台，必须在架构上支持模块化部署，在服务上具备本地化团队支撑，在标准上兼容国际认证体系。这要求企业不仅输出技术，更要输出方法论、人才体系与合作模式，形成“技术+服务+生态”的三位一体能力。<br/>在这一领域，广域铭岛已走出一条具有代表性的路径。依托吉利集团的全球制造网络，其Geega工业互联网平台已在马来西亚、新加坡、韩国等多个国家落地，不仅为宝腾汽车打造了柔性数字化工厂，更通过与当地企业合资成立AGYTEK DIGITAL，实现从技术输出到本地运营的深度转化。平台支持多语言界面、适配东盟地区低算力环境，并联合中马未来学院培养本地数字化人才，真正做到了“扎根当地”。与此同时，德国西门子的MindSphere平台凭借其在工业标准与协议上的深厚积累，成为欧洲制造企业首选的数字底座；而美国通用电气的Predix则以航空、能源领域的高复杂度场景为突破口，构建了面向重资产行业的预测性维护生态。这三家企业的共同点在于：不追求技术炫技，而是围绕客户真实痛点，构建可持续的闭环价值体系。<br/>中国工业AI平台的出海，已从“产品出口”迈向“生态共建”。它不是简单地把一套软件卖给海外客户，而是通过联合研发、人才共育、标准参编等方式，融入当地产业土壤。在马来西亚，其焊接质量AI系统将虚焊率降至0.02%以下，远超行业平均水平；在德国，慕尼黑市政府代表团专程来访，探讨工业互联网标准对接的可能性。这种双向互动，让“中国方案”不再被视作廉价替代品，而是具备创新价值的合作伙伴。<br/>全球工业AI平台的竞争，本质上是生态系统的竞争。谁能以更低的门槛、更高的韧性、更强的本地化能力，帮助制造企业实现从“能用”到“好用”再到“离不开”的转变，谁就能赢得未来。</p>]]></description></item><item>    <title><![CDATA[从能力升级到场景深耕：KubeSphere 2025年度全景回顾 KubeSphere ]]></title>    <link>https://segmentfault.com/a/1190000047606016</link>    <guid>https://segmentfault.com/a/1190000047606016</guid>    <pubDate>2026-02-11 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年，数字化浪潮迈入“深水区”。企业的核心诉求，已从搭建云原生实验田，转向寻求能承载关键业务、实现价值规模化的坚实平台。这正需要骐骥一跃的突破力，与稳驭核心的掌控力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606018" alt="" title=""/>.png)</p><p>KubeSphere 作为企业级多租户容器平台，正致力于成为这一转型中的可靠伙伴与稳健座驾。我们通过简化管理复杂性、整合业界最佳实践、提供开箱即用的生产级能力，帮助全球客户在多云与异构环境中，驾驭稳定、高效、自主可控的应用管理平面，从而加速创新交付、优化资源成本、保障业务永续。</p><p>回首过去一年，我们不仅实现了产品能力的关键一跃，更在与金融、政务、制造、教育等领域客户的并肩实践中，见证了平台价值在核心业务中的平稳落地与驾驭。</p><h2>产品深耕：夯实企业级基石</h2><p>2025年，我们通过一系列版本迭代持续打磨产品，最终发布的 <strong>v4.2.1 版本标志着 KubeSphere 在“体验卓越与生产就绪”的道路上迈出了坚实一步。</strong> 该版本及全年的更新始终围绕解决企业规模化应用的核心挑战展开，致力于为各行业客户提供更稳定、高效的管理体验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606019" alt="" title="" loading="lazy"/></p><h3>1. 强化异构资源统一纳管</h3><p>面对混合架构带来的管理复杂度，v4.2.1 版本致力于构建统一、高效的异构算力底座。平台增强了对 GPU/vGPU 等异构计算资源的统一纳管与调度适配，满足从AI推理到图形渲染的多元化算力需求。同时，通过集成智能数据缓存加速能力，显著降低了I/O密集型应用的存储访问延迟。这些能力共同为企业提供了一个可跨云、跨芯的标准化资源池，实现了算力资源的全局统筹与弹性供给。</p><h3>2. 升级弹性调度体系</h3><p>为实现极致的资源效率与成本优化，v4.2.1 引入了多维联动的智能弹性调度体系。平台创新性地一站式集成垂直伸缩（VPA）、水平伸缩（HPA）与事件驱动伸缩（KEDA）。VPA根据历史负载自动优化容器资源规格，避免浪费；HPA增强策略允许对扩缩容行为进行独立精细调控；KEDA则能将消息队列等外部事件直接转化为弹性信号，甚至支持副本数缩零以极致降本。</p><h3>3. 筑牢集群稳定防线</h3><p>为保障大规模、多集群生产环境的全局稳定，v4.2.1 在治理与可观测层面进行了重磅增强。平台新增节点组（Node Group） 能力，支持将物理节点逻辑分组并与企业空间绑定，完美支撑多租户资源隔离、信创环境混部等复杂场景。在运维层面，提供成员集群可视化在线升级与状态精准同步，极大简化了多集群生命周期管理。</p><h2>全球实践：深入关键场景</h2><p>产品的最终价值，在用户应对核心业务挑战时得到最真实的体现。2025年，KubeSphere 的可靠性、灵活性与开放性，在全球多个行业的关键业务场景中赢得了深度信任。</p><h3>国内实践</h3><ul><li><strong>某头部汽车金融公司</strong>：采用 KubeSphere 企业版构建混合云统一平台，在满足金融级强合规与隔离要求的前提下，实现了资源全局弹性调度与一站式运维，使整体运维效率提升超 40%，为创新金融业务的快速上线与稳定运行提供了强大支撑。</li><li><strong>某直辖市规划和自然资源局</strong>：打造“一云多芯”政务容器云平台，无缝纳管异构芯片架构资源，实现跨架构统一调度与标准化流程，有力支撑了智慧城市、空间规划等核心政务系统的数字化升级与高效协同。</li><li><strong>澳门气象局</strong>：选用KubeSphere可信版为其核心气象预报与监测系统构建云原生底座，在满足高安全与合规要求的同时，实现了资源的统一调度与服务的自动化运维，有力保障了气象业务7x24小时不间断稳定运行。</li></ul><h3>海外拓展</h3><ul><li><strong>欧洲 IT 解决方案商 AMPLUS S.A.</strong>：在为希腊某公立大学构建科研平台时，基于 KubeSphere 打造了标准化云原生管理方案。该选择显著降低了长期运维成本、赋予用户自主管理能力，并提升了方案的可复用性与交付效率，助力 Amplus 建立了可快速复用于其他教育及企业客户的产品化服务能力。</li></ul><h2>生态共建：汇聚行业力量</h2><p>2025年，KubeSphere 的技术影响力与生态认可度持续提升，<strong>GitHub 全球星标数突破 16,000，</strong> 这标志着产品价值获得了全球开发者的广泛关注。我们始终秉持协作共赢的理念，致力于与业界伙伴及用户共同构建一个充满活力的技术生态。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047606020" alt="" title="" loading="lazy"/></p><ul><li><strong>培育云原生新生力量</strong>：我们通过技术合作、联合创新及人才培养等多种形式，与众多行业伙伴及高校建立了深度链接，例如通过“开源之夏”等活动，成功孵化了智能运维助手等创新工具原型，推动了前沿技术的实用化探索。</li><li><strong>汇聚业界最佳实践</strong>：通过灵活的扩展组件架构，我们实现了与 Fluid（数据编排）、OceanBase（分布式数据库）等业界顶尖技术的深度集成。这使用户能够根据自身需求，灵活选用并组合经过生产验证的最佳技术组件，构建坚实可靠的底层架构。</li><li><strong>推动技术融合与标准实践</strong>：我们持续将大规模企业级部署中沉淀的稳定性保障与性能优化经验，贡献至行业生态，并与业界社区紧密协作，共同推动云原生应用管理标准的成熟与落地。</li></ul><h2>未来图景：平台化敏捷进化</h2><p>为更敏捷地响应技术演进与多元的业务场景，KubeSphere 将在 2026 年采用全新的“核心底座年度更新，扩展组件季度发布”演进模式。这一模式旨在聚焦核心产品能力，增强关键场景支撑，并以灵活机动的组件化策略应对市场的快速变化，助力企业在稳定可靠的底座上，持续获取前沿价值。</p><p>我们的前行路径将围绕以下四大方向深化，致力于让平台更智能、更坚韧、更开放：</p><h3>1. 全局治理与高可用保障</h3><p>为支撑企业全球业务的连续性，我们将强化多集群联邦治理能力。重点是实现管理面的多活与容灾，通过集成 Karmada 等领先方案，确保控制平面自身具备跨可用区的高可用与故障恢复能力，为大规模、跨地域的集群部署提供坚实、可靠的管理基石。</p><h3>2. 智能运维与成本洞察</h3><p>我们将推动运维从“可视化”向“智能化”与“精细化”演进。全新设计的全局总览大屏与集中式告警中心，将为管理员与租户提供一目了然的运行状态与事件管理。在此基础上，计划引入的 AI 运维助手，将能通过对告警、日志及性能数据的分析，提供根因定位与修复建议。同时，我们将深化成本洞察能力，通过分析资源消耗与计费数据，提供可视化的成本分摊报表与优化建议，让云原生资源的使用成本清晰可见、可控可优，真正实现降本增效。</p><h3>3. 生态集成与标准实践</h3><p>我们将通过深化与云原生顶尖技术的集成，提供开箱即用的生产级方案，并推动实践标准化。具体而言：</p><ul><li><strong>集成 Cilium</strong>：将提供基于 eBPF 的 Cilium 容器网络方案，它不仅带来更高的网络性能与可观测性，其强大的安全策略能力（如网络策略、服务网格）能更好地满足企业安全合规需求。</li><li><strong>拥抱 Gateway API</strong>：随着社区广泛采用的 Ingress NGINX 逐步进入维护尾声，我们将提供基于下一代标准 Gateway API 的流量管理方案，帮助用户从现有 Ingress 平滑演进至功能更强大、标准更统一的治理体系，有效规避技术断代风险，从容面向未来架构。</li></ul><p>这些集成旨在将社区最佳实践产品化，推动企业采用行业公认的先进标准，降低技术选型与落地复杂度。</p><h3>4. 核心PaaS能力延伸</h3><p>为简化企业关键中间件的云原生部署与管理，我们将通过扩展组件，完善对基于容器的数据库、消息队列等中间件的全生命周期管理体验，提供部署、监控、备份等一站式能力，助力企业关键应用平滑、稳定地运行在统一的容器平台之上。</p><h2>结语</h2><p>2025年的每一次“一跃”，都源于客户、伙伴与社区成员的信任共创，铸就了台阶。展望2026，KubeSphere 已备好更稳健的底座与更敏捷的生态，诚邀您一同稳驭核心，共赴智能、韧性的数字化未来。</p><p>骐骥已备，征程万里。KubeSphere 邀您共驭数字化未来。</p>]]></description></item><item>    <title><![CDATA[MindSpore 自动并行实战：如何零代码修改实现单机到分布式训练的升级 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605576</link>    <guid>https://segmentfault.com/a/1190000047605576</guid>    <pubDate>2026-02-11 16:10:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​当模型参数或数据量过大时，分布式训练​ 成为必然选择。传统方法需要手动切分模型、管理通信，过程复杂且易错。MindSpore 的 自动并行​ 特性能够自动寻找最优的并行策略，极大降低了分布式训练的门槛。</p><h2>1. 问题场景：为何需要自动并行？</h2><p>假设我们有一个简单的全连接网络，当参数量增长到单卡无法容纳时，通常需要：</p><ul><li>数据并行：切分数据，每卡持有完整模型。</li><li>模型并行：切分模型参数，数据在不同卡间流转。</li><li>手动实现这两种并行方式的混合，策略设计非常复杂。MindSpore 的自动并行可以 通过分析计算图、设备拓扑与资源约束，自动为算子分配最佳的并行执行方式。</li></ul><h2>2. 关键步骤：从单机代码到分布式训练</h2><p>以下是一个经典的多层感知机（MLP）示例。你只需要专注于模型定义，并行策略可交由框架自动生成。</p><pre><code class="python">import mindspore as ms
from mindspore import nn, ops

# 定义网络（与单机代码完全一致）
class MLP(nn.Cell):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.dense1 = nn.Dense(784, 2048)  # 大参数层
        self.dense2 = nn.Dense(2048, 512)
        self.dense3 = nn.Dense(512, 10)
    
    def construct(self, x):
        x = self.flatten(x)
        x = self.dense1(x)
        x = ops.relu(x)
        x = self.dense2(x)
        x = ops.relu(x)
        return self.dense3(x)

net = MLP()</code></pre><h2>3. 启用自动并行</h2><p>只需在训练脚本中增加几行配置，即可切换到自动并行模式：# 配置并行环境</p><pre><code class="python">ms.set_auto_parallel_context(parallel_mode="auto_parallel",  # 启用自动并行
                             device_num=4,                    # 使用4张设备（如GPU）
                             dataset_strategy="data_parallel") # 数据集自动切分策略

# 后续的损失函数、优化器定义及训练循环与单机代码基本无异
loss_fn = nn.CrossEntropyLoss()
optimizer = nn.SGD(net.trainable_params(), learning_rate=0.01)
# 使用 Model API 封装并训练...</code></pre><h2>4. 核心优势与理解</h2><ul><li>策略自动化：框架会分析计算图中每个算子的计算量、参数大小及依赖关系，自动选择“数据并行”、“模型并行”或“混合并行”策略。</li><li>通信优化：自动插入必要的通信算子（如AllReduce、AllGather），并优化通信与计算的重叠，以提升整体效率。</li><li>对开发者透明：最大程度地保持了单机训练代码的样貌，仅需增加并行上下文配置，真正实现了“零代码修改”的分布式训练升级。</li></ul><p>借助自动并行，开发者可以将精力聚焦于模型结构本身，而将复杂的分布式调度交给 MindSpore。下一步，您可以尝试在更大规模的模型（如Transformer）上体验这一特性，并观察其性能提升。</p>]]></description></item><item>    <title><![CDATA[教程上新｜微信AI团队提出扩散语言模型WeDLM，相较vLLM部署AR模型实现3倍推理加速 Open]]></title>    <link>https://segmentfault.com/a/1190000047605598</link>    <guid>https://segmentfault.com/a/1190000047605598</guid>    <pubDate>2026-02-11 16:08:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在规模化部署和商业落地场景中，推理速度的权重日益提升，甚至在许多情况下超过了单纯的模型参数量，成为决定其工程价值的关键因素。尽管自回归（Autoregressive，AR）生成范式凭借稳定性和成熟生态，仍是当前主流解码方式，<strong>但其逐 token 生成的内在机制，使模型在推理阶段几乎无法充分利用并行计算资源。</strong> 这一限制在长文本生成、复杂推理和高并发服务场景中尤为突出，也直接推高了推理延迟与算力成本。</p><p>为突破这一瓶颈，研究界近年来不断探索并行解码路径，<strong>其中扩散语言模型（Diffusion Language Models，DLMs）因其「每步生成多个 token」的特性，被视为最具潜力的替代方案之一。</strong> 然而，理想与现实之间仍存在明显鸿沟：在真实部署环境中，许多 DLLMs 并未展现出预期中的速度优势，甚至在性能上难以超越高度优化的 AR 推理引擎（如 vLLM）。问题并非源于并行本身，而是隐藏在模型结构与系统层面的深层冲突之中——<strong>大量现有扩散方法依赖双向注意力机制，破坏了前缀 KV 缓存这一现代推理系统的效率基石，迫使模型反复重算上下文，抵消了并行带来的潜在收益。</strong></p><p>在此背景下，<strong>腾讯微信 AI 团队提出了 WeDLM（WeChat Diffusion Language Model），</strong> 这是首个在工业级推理引擎（vLLM）优化条件下，推理速度超越同等 AR 模型的扩散语言模型。其核心思想是在保持严格因果掩码的前提下，让每个被掩码位置都能够条件化于当前所有已观测的 token。为此，研究人员引入了一种拓扑重排（Topological Reordering）方法，在不改变 token 逻辑位置的情况下，将已观测 token 移动到物理上的前缀区域。</p><p>实验结果表明，WeDLM 在保持强自回归 backbones 生成质量的同时，实现了显著的推理加速，具体而言，其在数学推理等任务上相较 vLLM 部署的 AR 模型实现了 3 倍以上加速，低熵场景的推理效率提速更是达到 10 倍以上。</p><p>目前，「WeDLM 高效大语言模型解码框架」已上线 OpenBayes 官网的教程版块，点击下方链接即可体验一键部署教程 ⬇️</p><p><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=P%2Bra1be%2B5FONp%2FRqD8zxtg%3D%3D.kq75QDaNni0ZnNd5Hi8%2BTXUQmfZUoeivl2hG0oAIsCs%3D" rel="nofollow" target="_blank">https://go.openbayes.com/9ooQJ</a></em></strong></p><p><strong>Demo 运行</strong></p><p><strong>01</strong></p><p><strong>Demo 运行阶段</strong></p><p>1.登录 OpenBayes.com，在「公共教程」页面，选择「WeDLM 高效大语言模型解码框架」教程。</p><p><img width="723" height="535" referrerpolicy="no-referrer" src="/img/bVdnUya" alt="" title=""/><br/>2.页面跳转后，点击右上角「克隆」，将该教程克隆至自己的容器中。</p><p><img width="723" height="537" referrerpolicy="no-referrer" src="/img/bVdnUyb" alt="" title="" loading="lazy"/><br/>3.选择「NVIDIA GeForce RTX 5090」以及「PyTorch」镜像，按照需求选择「按量付费」或「包日/周/月」，点击「继续执行」。新用户使用下方邀请链接注册，即可获得满 ¥10 赠 ¥10 优惠券，更有机会获得 ¥15 赠金！</p><p>小贝总专属邀请链接（直接复制到浏览器打开）：</p><p><strong><em><em><a href="https://link.segmentfault.com/?enc=FVR4J75Bt8JIcP%2FlU%2FhhNQ%3D%3D.pkxIvr%2Fzcel31dZ%2Bpo%2BG1qY%2Fc9EjyS3b9jVMHp7U%2FOjEzMrzVCj5%2FO27oho1KYEX" rel="nofollow" target="_blank">https://go.openbayes.com/9S6D******r</a></em></em></strong></p><p><img width="723" height="569" referrerpolicy="no-referrer" src="/img/bVdnUye" alt="" title="" loading="lazy"/><br/><img width="723" height="566" referrerpolicy="no-referrer" src="/img/bVdnUyf" alt="" title="" loading="lazy"/></p><p>4.等待分配资源，当状态变为「运行中」后，点击「打开工作空间」进入 Jupyter Workspace。</p><p><img width="723" height="569" referrerpolicy="no-referrer" src="/img/bVdnUyg" alt="" title="" loading="lazy"/><br/><strong>02</strong></p><p><strong>效果演示</strong></p><p>页面跳转后，点击左侧 README 页面，进入后点击上方「运行」。</p><p><img width="723" height="566" referrerpolicy="no-referrer" src="/img/bVdnUyh" alt="" title="" loading="lazy"/><br/><img width="723" height="523" referrerpolicy="no-referrer" src="/img/bVdnUyi" alt="" title="" loading="lazy"/></p><p>待运行完成，即可点击右侧 API 地址跳转至 demo 页面。</p><p><img width="723" height="441" referrerpolicy="no-referrer" src="/img/bVdnUyj" alt="" title="" loading="lazy"/><img width="723" height="375" referrerpolicy="no-referrer" src="/img/bVdnUyk" alt="" title="" loading="lazy"/></p><p><strong>教程链接：</strong></p><p><strong><em><a href="https://link.segmentfault.com/?enc=gLpdfiBMa%2BSpv6xayLtzOw%3D%3D.tcePUDFGIPPE7aQmbnheMjUSigLIfpd3iPUs4Xqc3u0%3D" rel="nofollow" target="_blank">https://go.openbayes.com/9ooQJ</a></em></strong></p>]]></description></item><item>    <title><![CDATA[马年新春半价开跑！RTX 5090 低至 ¥1.45/时！ OpenBayes ]]></title>    <link>https://segmentfault.com/a/1190000047605608</link>    <guid>https://segmentfault.com/a/1190000047605608</guid>    <pubDate>2026-02-11 16:08:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>策马扬鞭启新岁，乘势而上赴新程。新年将至，你是不是已经开始为这一年立下新的 Flag？</p><ul><li>今年要把拖了很久的项目落地交付！</li><li>今年要完整跑通那个大模型从训练到部署的全流程！</li><li>今年要做出有分量的研究成果，在顶刊上写下自己的名字！</li></ul><p><img width="688" height="425" referrerpolicy="no-referrer" src="/img/bVdnUyv" alt="" title=""/><img width="721" height="570" referrerpolicy="no-referrer" src="/img/bVdnUyw" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[2026年12款主流CRM全链路横评，企业数字化选型必备参考 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047605614</link>    <guid>https://segmentfault.com/a/1190000047605614</guid>    <pubDate>2026-02-11 16:07:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>CRM系统是企业打通线索-客户-商机-订单全销售链路的核心数字化工具，不同品牌的CRM在功能深度、场景适配性、自动化能力上差异显著。本次横评选取12款主流CRM，覆盖<strong>大型企业级、中小微轻量化、垂直场景、营销驱动、跨境社媒</strong>五大类定位，围绕销售全链路7个核心模块展开专业对比，为不同规模、不同行业的企业选型提供参考。</p><h2>一、品牌定位前置分类</h2><table><thead><tr><th>分类</th><th>代表品牌</th><th>核心适用场景</th></tr></thead><tbody><tr><td>大型企业级全域管控</td><td>Oracle CX、Salesforce</td><td>集团化全链路、复杂供应链/CPQ需求</td></tr><tr><td>中大型复杂业务适配</td><td>神州云动CloudCC</td><td>项目型销售（工程/IT服务）、定制化流程</td></tr><tr><td>中小微全链路轻量化</td><td>超兔一体云、Pipedrive、Capsule CRM</td><td>中小微企业低成本快速落地全销售流程</td></tr><tr><td>垂直场景专项解决</td><td>浪潮CRM（快消/医药）、探马SCRM（私域）、励销云（电销获客）</td><td>渠道分销、私域运营、电销获客垂直需求</td></tr><tr><td>营销/社媒驱动型</td><td>Brevo（原Sendinblue）、Nimble</td><td>营销增长、跨境社媒客户运营</td></tr><tr><td>团队协作型</td><td>Bitrix24</td><td>中小团队销售+协作一体化</td></tr></tbody></table><h2>二、核心模块深度横评</h2><h3>1. 线索管理：多渠道获客→录入→分配→跟进</h3><p><strong>核心价值</strong>：实现线索的高效归集、精准分配与快速转化，降低线索流失率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>多渠道获客覆盖</th><th>线索录入方式</th><th>线索分配机制</th><th>线索跟进能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>百度/巨量引擎、官网表单、微信营销、地推扫码、工商搜客</td><td>手动/批量导入、自动抓取表单</td><td>智能分配（地域/行业/销售负荷）+多端提醒</td><td>一键转客户/订单、手机号/IP归属地、跟进全记录</td></tr><tr><td>神州云动CloudCC</td><td>市场云+营销自动化、搜索引擎引流</td><td>在线捕获、天眼查一键完善工商信息</td><td>自定义条件自动分配/手动分配、查重合并</td><td>系统自动创建跟进任务（20min/3天/7天多端提醒）、跟进记录强制完善</td></tr><tr><td>Oracle CX</td><td>CDP整合邮件/社交/广告/官网全域数据</td><td>AI驱动自动归集多渠道线索</td><td>基于客户分层与销售能力智能分配</td><td>AI精准触达建议、跟进轨迹全链路追踪</td></tr><tr><td>探马SCRM</td><td>私域活码/裂变工具、多渠道线索自动归集</td><td>批量加好友自动同步标签</td><td>员工二维码自动分流、自定义分配规则</td><td>基于SOP的阶段跟进提醒、客户行为轨迹追踪</td></tr><tr><td>励销云</td><td>搜客宝/微名片/电销机器人、广告助手</td><td>表格/名片导入、自动抓取</td><td>自定义规则分配、公海机制</td><td>电销一键拨号/录音、跟进场景还原记录</td></tr></tbody></table><h4>典型流程时序图（超兔一体云）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605616" alt="" title=""/></p><pre><code>sequenceDiagram
    participant 多渠道 as 多渠道获客平台&lt;br/&gt;(百度/巨量/微信/地推)
    participant 超兔 as 超兔一体云
    participant 销售 as 销售人员
    participant 公海 as 线索公海池

    多渠道-&gt;&gt;超兔: 自动抓取线索表单/扫码数据
    超兔-&gt;&gt;超兔: 线索查重、补全归属地/工商信息
    超兔-&gt;&gt;公海: 未分配线索存入公海
    超兔-&gt;&gt;超兔: 按预设规则(地域/负荷)自动分配
    超兔-&gt;&gt;销售: 推送线索提醒(APP/短信)
    销售-&gt;&gt;超兔: 一键处理线索(转客户/待办/订单)
    销售-&gt;&gt;超兔: 录入跟进记录/下一步计划
    超兔-&gt;&gt;超兔: 更新线索状态、同步至客户档案</code></pre><p><strong>场景点评</strong>：</p><ul><li>大型企业全域获客选Oracle CX的CDP整合能力；</li><li>电销为主的中小微选励销云的搜客宝+电销机器人组合；</li><li>私域运营选探马SCRM的活码裂变与标签化跟进；</li><li>中小微全渠道覆盖选超兔一体云的工商搜客+多平台自动抓取。</li></ul><h3>2. 客户与联系人管理：360°档案→关系链路</h3><p><strong>核心价值</strong>：构建完整客户画像，清晰掌握客户决策链，提升沟通精准度。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>详细客户档案能力</th><th>联系人关系管理</th></tr></thead><tbody><tr><td>Salesforce</td><td>360°视图整合销售/服务/营销数据、自动补全企业信息、自定义字段</td><td>关联联系人与客户账户、记录决策链角色、互动轨迹全跟踪</td></tr><tr><td>超兔一体云</td><td>自动补全工商/天眼查信息、微信/支付宝头像同步、自定义布局</td><td>多联系人管理、清晰记录联系人与客户的职务/关系</td></tr><tr><td>神州云动CloudCC</td><td>知识图谱结构化信息、360°视图关联工单/商机/合同</td><td>精确分配联系人、AI秒级响应客户需求、多联系人权限管控</td></tr><tr><td>探马SCRM</td><td>微信生态客户画像（聊天/行为/内容标签）、360°视图关联私域互动数据</td><td>客户细分运营、记录联系人私域互动轨迹</td></tr><tr><td>Oracle CX</td><td>整合销售+服务云数据、AI分析客户行为轨迹识别高意向客户</td><td>全局联系人关系映射、跨部门数据共享</td></tr></tbody></table><h4>360°客户档案脑图（Salesforce）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605617" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((Salesforce 360°客户档案))
        基础信息层
            企业工商数据(自动补全)
            多渠道联系方式(邮件/电话/微信)
            企业资质与规模(员工数/年营收)
        互动轨迹层
            销售跟进记录(邮件/电话/会议)
            营销触达数据(打开/点击/转发)
            服务工单与售后反馈
        业务关联层
            商机阶段与成交概率
            报价单/订单/回款记录
            项目与合同信息
        自定义标签层
            RFM客户分层
            行业专属标签
            决策链角色标记</code></pre><p><strong>场景点评</strong>：</p><ul><li>大型企业全链路客户运营选Salesforce/Oracle CX的360°视图；</li><li>中小微快速搭建客户档案选超兔一体云的自动信息补全；</li><li>私域运营选探马SCRM的微信生态客户画像；</li><li>项目型销售选神州云动的知识图谱结构化信息。</li></ul><h3>3. 商机管理：销售阶段→金额/概率→AI预测</h3><p><strong>核心价值</strong>：可视化商机进度，精准预测业绩，把控销售转化节点。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>销售阶段精细化</th><th>预计金额与成交概率管理</th><th>AI与扩展能力</th></tr></thead><tbody><tr><td>Oracle CX</td><td>自定义多阶段销售漏斗、打通计划-执行全流程</td><td>自动关联供应链能力、CPQ模块同步金额</td><td>AI实时销售预测、商机优先级智能排序</td></tr><tr><td>神州云动CloudCC</td><td>精细化商机阶段跟踪、集成项目成本与工时管理</td><td>按商机类型关联金额、成交概率动态调整</td><td>项目型商机多人跟踪汇总、合同一键关联</td></tr><tr><td>超兔一体云</td><td>多跟单模型（小单快单/商机跟单/多方项目）</td><td>支持预计金额录入、成交概率动态更新</td><td>线索手机号/IP辅助商机判断</td></tr><tr><td>Salesforce</td><td>可视化销售漏斗、阶段自定义配置</td><td>预计金额与成交概率AI动态调整</td><td>Einstein AI预测销售趋势、商机健康度分析</td></tr><tr><td>探马SCRM</td><td>基于RFM模型的客户分层转化阶段</td><td>关联私域互动数据评估商机价值</td><td>私域转化路径跟踪、营销素材互动分析</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>大型集团业绩预测选Oracle CX/Salesforce的AI预测能力；</li><li>项目型销售（工程/IT服务）选神州云动的项目成本集成；</li><li>中小微多场景销售选超兔一体云的多跟单模型；</li><li>私域转化选探马SCRM的客户分层转化阶段管理。</li></ul><h3>4. 活动与任务管理：日程→待办→提醒</h3><p><strong>核心价值</strong>：规范销售动作，确保任务按时落地，提升团队协作效率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>日程与待办管理</th><th>提醒与自动化</th><th>特色能力</th></tr></thead><tbody><tr><td>神州云动CloudCC</td><td>集成日历、任务优先级设置</td><td>系统自动创建跟进任务（APP/邮件/短信提醒）</td><td>外勤拜访签到、销售实时轨迹追踪</td></tr><tr><td>探马SCRM</td><td>群日历、客户SOP任务管理</td><td>基于销售阶段自动触发跟进提醒</td><td>群SOP统一执行、私域互动任务提醒</td></tr><tr><td>超兔一体云</td><td>日程规划、待办任务分配</td><td>多端提醒（APP/PC）、任务状态同步</td><td>一键关联线索/客户/商机创建任务</td></tr><tr><td>Oracle CX</td><td>AI驱动的智能日程安排</td><td>自动同步销售任务与客户互动节点</td><td>跨部门任务协同、资源冲突预警</td></tr><tr><td>励销云</td><td>销售任务目标分解、业绩进度可视化</td><td>公海线索跟进提醒、电销任务提醒</td><td>外勤打卡、附近客户查找</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>线下销售团队管理选神州云动的外勤轨迹追踪；</li><li>私域社群运营选探马SCRM的群SOP与日历；</li><li>大型跨部门协作选Oracle CX的智能日程与冲突预警；</li><li>中小微销售任务管理选超兔一体云的一键关联任务创建。</li></ul><h3>5. 报价与订单：商机→报价→订单→执行</h3><p><strong>核心价值</strong>：减少重复操作，实现商机到订单的无缝衔接，同步供应链与财务数据。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>商机转报价/订单能力</th><th>订单执行与联动</th><th>特色能力</th></tr></thead><tbody><tr><td>Oracle CX</td><td>从商机一键生成报价、CPQ模块简化配置报价</td><td>订单/物流/资金三流合一、联动ERP/供应链</td><td>复杂产品配置报价、全球多币种支持</td></tr><tr><td>浪潮CRM</td><td>商机转订单自动关联渠道信息</td><td>订单联动ERP、原生库存/采购模块同步</td><td>快消医药渠道分销库存同步、促销费用量化</td></tr><tr><td>超兔一体云</td><td>从商机一键生成报价/订单、模板化管理</td><td>订单工作流、锁库/采购计划生成、供应商直发</td><td>多业务模型适配（服务型/实物型/批发型）</td></tr><tr><td>Salesforce</td><td>商机转报价/订单自动关联客户信息</td><td>订单与合同/回款联动、Einstein AI订单预测</td><td>CPQ集成、自定义订单字段</td></tr><tr><td>励销云</td><td>商机转订单减少重复操作</td><td>订单关联开票/回款/退货闭环管理</td><td>电销订单快速创建、交易场景还原</td></tr></tbody></table><p><strong>场景点评</strong>：</p><ul><li>大型集团全供应链管控选Oracle CX的三流合一；</li><li>快消/医药渠道分销选浪潮CRM的库存与ERP联动；</li><li>中小微多业务场景选超兔一体云的多订单模型适配；</li><li>电销交易闭环选励销云的订单与回款联动。</li></ul><h3>6. SOP流程管理：定制→执行→优化</h3><p><strong>核心价值</strong>：固化最佳销售实践，规范销售动作，提升团队执行效率。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>SOP定制能力</th><th>场景适配</th><th>执行与优化</th></tr></thead><tbody><tr><td>超兔一体云</td><td>AI定制行业SOP（CJM/销售分析/话术）、自定义工作流</td><td>中小微全销售场景、多业务模型适配</td><td>执行数据统计、SOP迭代优化</td></tr><tr><td>神州云动CloudCC</td><td>PaaS平台自定义流程、SOP按需配置</td><td>复杂业务场景、项目型销售</td><td>流程节点监控、合规性管控</td></tr><tr><td>探马SCRM</td><td>客户SOP/群SOP/群日历定制</td><td>私域社群运营场景、客户分层运营</td><td>按阶段自动触发SOP任务、执行效果分析</td></tr><tr><td>Oracle CX</td><td>合同全生命周期SOP管控、多级审批流程</td><td>大型企业合规性管控、复杂合同流程</td><td>AI流程优化建议、全链路流程监控</td></tr><tr><td>浪潮CRM</td><td>渠道分销SOP、促销活动流程配置</td><td>快消医药渠道管理场景</td><td>促销费用流程管控、渠道数据同步</td></tr></tbody></table><h4>AI定制SOP流程图（超兔一体云）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605618" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
    A[企业行业与业务需求输入] --&gt; B[超兔AI生成行业专属SOP&lt;br/&gt;(CJM/销售话术/流程节点)]
    B --&gt; C[SOP流程可视化配置&lt;br/&gt;(自定义触发条件/执行节点)]
    C --&gt; D[系统自动触发SOP任务&lt;br/&gt;(跟进提醒/动作要求)]
    D --&gt; E[销售执行任务并录入跟进记录]
    E --&gt; F[SOP执行数据统计&lt;br/&gt;(转化率/周期/节点完成率)]
    F --&gt; G[SOP流程迭代优化]</code></pre><p><strong>场景点评</strong>：</p><ul><li>中小微快速搭建SOP选超兔一体云的AI定制能力；</li><li>复杂业务/合规需求选神州云动/Oracle CX的PaaS自定义与全生命周期管控；</li><li>私域运营选探马SCRM的场景化SOP；</li><li>渠道分销选浪潮CRM的促销与渠道流程配置。</li></ul><h3>7. 报表与分析：销售报表→业绩统计→漏斗分析</h3><p><strong>核心价值</strong>：用数据驱动销售决策，优化销售流程，提升业绩。</p><h4>横向对比表格</h4><table><thead><tr><th>品牌</th><th>销售报表覆盖</th><th>业绩统计与漏斗分析</th><th>AI与特色分析</th></tr></thead><tbody><tr><td>Oracle CX</td><td>全渠道ROI分析、销售绩效报表</td><td>销售漏斗全阶段转化分析、AI实时销售预测</td><td>全局数据决策支撑、多维度钻取分析</td></tr><tr><td>Salesforce</td><td>各类销售报表自定义生成</td><td>业绩目标完成统计、销售漏斗健康度分析</td><td>Einstein AI预测销售趋势、客户流失预警</td></tr><tr><td>探马SCRM</td><td>私域转化报表、群运营效果报表</td><td>客户复购分析、私域销售漏斗分析</td><td>客户互动轨迹分析、营销素材效果评估</td></tr><tr><td>浪潮CRM</td><td>促销费用全流程分析、渠道分销报表</td><td>业绩统计与渠道转化率分析</td><td>终端数据与促销费用量化分析</td></tr><tr><td>超兔一体云</td><td>销售业绩/线索转化率/客户分析报表</td><td>销售漏斗转化分析、业绩目标完成统计</td><td>财务数据自动汇总、自定义报表配置</td></tr></tbody></table><h2>三、综合能力雷达图分值（1-10分）</h2><table><thead><tr><th>品牌</th><th>线索管理</th><th>客户与联系人</th><th>商机管理</th><th>活动与任务</th><th>报价与订单</th><th>SOP流程</th><th>报表与分析</th></tr></thead><tbody><tr><td>Oracle CX</td><td>9.5</td><td>10</td><td>9.5</td><td>9</td><td>10</td><td>9.5</td><td>10</td></tr><tr><td>Salesforce</td><td>9</td><td>9.5</td><td>9.5</td><td>9</td><td>9.5</td><td>9</td><td>9.5</td></tr><tr><td>神州云动CloudCC</td><td>8.5</td><td>9</td><td>9</td><td>8.5</td><td>8</td><td>9</td><td>8.5</td></tr><tr><td>超兔一体云</td><td>8</td><td>8.5</td><td>8</td><td>8</td><td>8.5</td><td>8.5</td><td>8</td></tr><tr><td>浪潮CRM</td><td>7.5</td><td>8</td><td>8</td><td>7</td><td>9</td><td>8</td><td>8.5</td></tr><tr><td>励销云</td><td>9</td><td>7.5</td><td>7</td><td>8</td><td>7.5</td><td>7.5</td><td>7</td></tr><tr><td>探马SCRM</td><td>8</td><td>8.5</td><td>7.5</td><td>9</td><td>6</td><td>9</td><td>8</td></tr><tr><td>Brevo</td><td>8.5</td><td>7</td><td>7.5</td><td>7</td><td>7</td><td>7</td><td>8.5</td></tr><tr><td>Pipedrive</td><td>7</td><td>7.5</td><td>8</td><td>7.5</td><td>7</td><td>6.5</td><td>7.5</td></tr><tr><td>Capsule CRM</td><td>6.5</td><td>7</td><td>7</td><td>7</td><td>6.5</td><td>6</td><td>6.5</td></tr><tr><td>Bitrix24</td><td>7</td><td>7</td><td>7.5</td><td>8.5</td><td>7</td><td>7</td><td>7.5</td></tr><tr><td>Nimble</td><td>8</td><td>7.5</td><td>7</td><td>7</td><td>6</td><td>6</td><td>7.5</td></tr></tbody></table><p>结合品牌定位、核心模块能力与雷达图评分，为不同企业提供精准选型建议：</p><ol><li><strong>大型集团/全球化企业</strong>：优先选<strong>Oracle CX</strong>或<strong>Salesforce</strong>，二者具备全链路数字化能力、AI决策支持与复杂供应链适配，满足集团化多场景、合规管控与全球业务需求。</li><li><strong>中大型项目型企业（工程/IT服务）</strong> ：推荐<strong>神州云动CloudCC</strong>，精细化商机跟踪、项目成本集成与PaaS级流程定制，完美适配项目型销售的复杂业务逻辑。</li><li><strong>快消/医药渠道分销企业</strong>：首选<strong>浪潮CRM</strong>，原生库存/采购联动、促销费用量化分析与渠道SOP配置，精准匹配垂直行业渠道管理需求。</li><li><strong>电销获客为主的中小微企业</strong>：选择<strong>励销云</strong>，搜客宝+电销机器人组合提升线索效率，公海机制与电销场景化跟进适配电销团队需求。</li><li><strong>私域运营驱动型企业</strong>：必选<strong>探马SCRM</strong>，微信生态全链路客户画像、群SOP运营与私域转化跟踪，为私域变现提供全流程支撑。</li><li><strong>中小微全流程轻量化需求</strong>：推荐<strong>超兔一体云</strong>（适配国内全场景+AI定制SOP）或<strong>Pipedrive</strong>（可视化漏斗适配海外/轻量化团队）；追求极简操作选<strong>Capsule CRM</strong>。</li><li><strong>跨境社媒营销企业</strong>：选择<strong>Nimble</strong>，LinkedIn/Twitter社媒数据整合与客户兴趣图谱能力，适配跨境电商与互联网品牌的社媒获客需求。</li><li><strong>中小团队协作优先</strong>：选择<strong>Bitrix24</strong>，集成日历、文档共享与团队任务协同，实现销售+协作一体化管理。</li></ol>]]></description></item><item>    <title><![CDATA[MindSpore 动态图模式深度体验：像写NumPy一样调试神经网络 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605619</link>    <guid>https://segmentfault.com/a/1190000047605619</guid>    <pubDate>2026-02-11 16:06:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在深度学习开发中，高效的调试​ 与 灵活的模型验证​ 至关重要。MindSpore 提供了 动态图模式（PYNATIVE_MODE），允许开发者以类似 NumPy/PyTorch 的命令式执行方式，逐行运行和调试代码，极大降低了复杂模型的前期开发门槛。</p><h2>1. 动静结合的独特优势</h2><p>MindSpore 默认以高性能的 静态图模式（GRAPH_MODE）执行，但在模型开发阶段，我们常需：</p><ul><li>即时打印张量值，检查数据流。</li><li>使用 Python 原生调试工具（如 pdb）。</li><li>动态修改网络结构进行快速实验。</li><li>此时，仅需一行代码即可切换到动态图模式：</li></ul><pre><code class="python">import mindspore as ms
ms.set_context(mode=ms.PYNATIVE_MODE)  # 切换至动态图模式
# ms.set_context(mode=ms.GRAPH_MODE)   # 切换回静态图模式</code></pre><h2>2. 动态图下的直观调试实践</h2><p>以下是一个简单的卷积网络示例，展示如何在动态图模式下插入调试语句：</p><pre><code class="python">from mindspore import nn, ops
import numpy as np
class SimpleCNN(nn.Cell):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1)
        self.bn = nn.BatchNorm2d(64)
        self.relu = ops.ReLU()
    
    def construct(self, x):
        x = self.conv(x)
        print(f"[调试] 卷积输出形状: {x.shape}, 均值: {x.asnumpy().mean():.4f}")  # 动态图下可即时打印
        x = self.bn(x)
        x = self.relu(x)
        return x
# 运行网络
net = SimpleCNN()
fake_input = ms.Tensor(np.random.randn(8, 3, 32, 32).astype(np.float32))
output = net(fake_input)  # 执行时，print语句将直接输出</code></pre><ul><li>输出提示：[调试] 卷积输出形状: (8, 64, 30, 30), 均值: 0.0123</li></ul><h2>3. 进阶技巧：结合 Python 原生调试工具</h2><p>动态图模式下，你可以直接使用 pdb进行断点调试，深入跟踪前向与反向过程：import pdb</p><pre><code class="python">class DebuggableNet(nn.Cell):
    def construct(self, x):
        x = ops.matmul(x, x.transpose())
        pdb.set_trace()  # 在此处进入调试器，可检查x的值
        return x.sum()</code></pre><h2>4. 核心理解与应用建议</h2><ul><li>开发-部署闭环：建议在 模型开发与调试阶段使用 PYNATIVE_MODE，在 性能敏感的训练与推理阶段切换回 GRAPH_MODE，实现灵活性与性能的统一。</li><li>调试范围：动态图模式下，不仅可以调试前向计算，还可以在自定义的梯度函数（bprop）或损失函数中插入调试逻辑，全方位验证计算正确性。</li><li>性能提醒：动态执行会带来一定的开销，因此在大规模数据训练前，完成调试后应及时切换回静态图模式。</li></ul><p>掌握动态图调试，意味着你拥有了更快的 模型验证循环。在构建复杂模型或尝试新颖结构时，不妨先用动态图快速迭代想法，再用静态图进行强化训练，这是 MindSpore 助力高效研发的秘诀之一。</p>]]></description></item><item>    <title><![CDATA[重塑研发逻辑：工业设计协同平台如何成为制造企业的隐形引擎 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047605626</link>    <guid>https://segmentfault.com/a/1190000047605626</guid>    <pubDate>2026-02-11 16:06:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业加速向智能化、个性化转型的今天，研发环节的效率与协同能力，正悄然决定着一家企业的生死。过去，设计图纸散落在不同工程师的电脑里，BOM表版本混乱，审批靠打印签字、邮件来回，FMEA分析成了“填表任务”而非风险预警工具——这些看似琐碎的流程断点，实则累积成巨大的时间黑洞和成本损耗。真正的挑战，不是技术不够先进，而是信息被割裂在孤岛之中。工业设计研发协同平台的出现，不是为了炫技，而是为了把人从低效的重复劳动中解放出来，让研发回归创新的本质。<br/>这类平台的核心价值，在于构建一个以数据为中心、以流程为脉络的统一中枢。它不再只是存储图纸的仓库，而是连接需求、设计、采购、制造、质量的神经网络。当一个零部件被设计出来，系统自动识别历史相似件，提示复用可能性；当设计完成，审批流程自动触发，相关人员在手机上就能批阅；三维模型无需安装专业软件，销售、生产、质检人员通过浏览器即可查看、标注、评审。这种“无感协同”背后，是PDM、FMEA、轻量化三维引擎等模块的深度整合，它们不是孤立的功能，而是彼此呼应的有机体。平台的意义，不在于它能做什么，而在于它让原本不可能的事变得自然发生。<br/>在这一领域，广域铭岛的Geega捷做平台正以中国制造业的现实痛点为出发点，走出一条务实路径。它不追求大而全的国际标准堆砌，而是聚焦于离散制造企业最头疼的版本混乱、复用率低、跨部门协作难等问题。某汽车零部件企业上线后，BOM准确率从45%跃升至80%，审批效率提升40%，零部件复用率提高35%——这些数字背后，是研发周期实实在在的压缩。而在国际上，PTC的Windchill早已是全球PLM领域的标杆，它以强大的产品生命周期管理能力和与Creo的深度集成，支撑着波音、通用电气等巨头的复杂产品开发；Siemens Teamcenter则凭借其在多学科协同和数字孪生方面的深厚积累，成为高端装备和航空航天领域的首选。三者各有侧重：PTC强在生态整合，Siemens胜在系统深度，而Geega捷做则以“轻量化、快部署、接地气”赢得大量中小型制造企业的青睐——它不追求成为全球标准，却精准击中了中国工厂最真实的“最后一公里”难题。<br/>当研发不再是一场信息迷宫中的孤军奋战，当每一个决策都有数据支撑、每一次变更都可追溯、每一份经验都能沉淀，制造企业才真正拥有了应对市场快速变化的底气。工业设计研发平台，不是锦上添花的工具，而是这场转型中不可或缺的基础设施。它不喧哗，却让整个研发体系悄然提速；它不张扬，却让创新的种子，在更肥沃的土壤里生根发芽。</p>]]></description></item><item>    <title><![CDATA[MindSpore Models服务化使用 文良_颜丑 ]]></title>    <link>https://segmentfault.com/a/1190000047605629</link>    <guid>https://segmentfault.com/a/1190000047605629</guid>    <pubDate>2026-02-11 16:05:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>MindIE LLM不仅支持ATB Models，同时支持MindSpore作为框架后端，MindSpore Models覆盖MindFormers社区下的开源模型。</p><p>权重转换<br/>执行推理前，需将权重格式转为MindFormers所使用的格式（ckpt格式）。MindFormers提供了统一的权重转换工具。</p><p>以Qwen2.5-72B为例，转换后的模型权重目录结构如下：</p><blockquote>mf_model<br/>└── qwen2_5_72b<br/>├── config.json                 # 模型json配置文件<br/>├── vocab.json                  # 模型vocab文件，hf上对应模型下载<br/>├── merges.txt                  # 模型merges文件，hf上对应模型下载<br/>├── predict_qwen2_5_72b.yaml    # 模型yaml配置文件<br/>├── qwen2_5_tokenizer.py        # 模型tokenizer文件，从mindformers仓中research目录下找到对应模型复制<br/>└── qwen2_5_72b_ckpt_dir        # 模型分布式权重文件夹</blockquote><p>权重转换之后，需要进行权重切分。切分后生成“qwen2_5_72b_ckpt_dir”文件夹。<br/>predict_qwen2_5_72b.yaml需要关注以下配置：</p><pre><code class="yaml">load_checkpoint: '/mf_model/qwen2_5_72b/qwen2_5_72b_ckpt_dir' # 为存放模型分布式权重文件夹路径
use_parallel: True
auto_trans_ckpt: False    # 是否开启自动权重转换，离线切分设置为False
parallel_config:
  data_parallel: 1
  model_parallel: 4       # 多卡推理配置模型切分，一般与使用卡数一致
  pipeline_parallel: 1
processor:
  tokenizer:
    vocab_file: "/mf_model/qwen2_5_72b/vocab.json"  # vocab文件路径
    merges_file: "/mf_model/qwen2_5_72b/merges.txt"  # merges文件路径</code></pre><p>模型的config.json文件可以使用save_pretrained接口生成，示例如下：</p><pre><code class="python">from mindformers import AutoConfig

model_config = AutoConfig.from_pretrained("/mf_model/qwen2_5_72b/predict_qwen2_5_72b.yaml")
model_config.save_pretrained(save_directory="./json/qwen2_5_72b/", save_json=True)</code></pre>]]></description></item><item>    <title><![CDATA[开发者欢呼，普通人迷茫：OpenClaw之后，“可用AI”的路该怎么走？ 老纪的技术唠嗑局 ]]></title>    <link>https://segmentfault.com/a/1190000047605640</link>    <guid>https://segmentfault.com/a/1190000047605640</guid>    <pubDate>2026-02-11 16:04:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年的“Agent元年”为我们留下了遍地开花的演示和无限可能的想象。然而，当烟花散去，一个更现实的问题摆在所有从业者面前：<strong>在令人眼花缭乱的 Agent 之后，真正能在日常工作和生活中扎根、被高频使用的“可用AI”究竟长什么样？</strong></p><p>答案或许正从最务实的角落浮现。在1月31日的 OceanBase 社区嘉年华活动中，主题为“Agent 元年之后，真正能用的 AI 长什么样”的圆桌讨论揭示了一个清晰的共识：<strong>当下最接近“真正能用”状态的AI，并非无所不能的科幻管家，而是那些在特定领域解决高频、重复、确定性任务的“超级助手”</strong>。以#OpenClaw（原Clawdbot）为代表的智能助手，成为了这一趋势的绝佳注脚。它的爆火并非源于底层模型的颠覆性突破，而在于其精准的产品定位——它重构了开发工作流，将开发者从机械的编码与调试中解放出来，并巧妙地通过 “透明化”与“可验证” 的设计，满足了企业级应用对可靠性与可控性的核心诉求。</p><p>这标志着AI应用的竞争重心，正从单纯的“模型能力竞赛”转向复杂的 “系统工程” 。一个“可用”的AI，必须是模型能力、产品设计、交互范式、成本控制与人类协作模式的深度融合体。下面，就让我们透过这场前沿实践者的对话，一窥“可用AI”的当下形态与未来蓝图。</p><h2><strong>Agent 元年之后，真正能用的 AI 长什么样</strong></h2><p>主持人：谢肖瑜，南京大学研究生院人工智能课程企业教师</p><p>对话嘉宾：孙韬，Eigent 核心研发工程师，CAMEL-AI 核心成员</p><p>对话嘉宾：程治玮，OceanBase Ambassador</p><p>对话嘉宾：边思康，蚂蚁百灵大模型产品及运营负责人</p><p>对话嘉宾：孙稼骏，Fellou 创始团队成员</p><p><strong>议题一：站在应用落地的角度，最接近真正能用的 AI 形态是什么？</strong></p><p><strong>谢肖瑜</strong>：在 AI 时代到来之前，我们常说“任何应用都可以跑在浏览器上”，甚至进一步提出“浏览器就是操作系统”，这是一个非常有力的产品叙事。而到了今天，一个更响亮的口号正在流行：“模型即应用”（Model as Application）。2025 年被称为“Agent 元年”，今天我们也把 Agent 定为主角，讨论一下如何让 AI 真正可用、可落地、可规模化。</p><h4><strong>Agent 在高频重复任务中的透明化与可验证性是企业落地的关键</strong></h4><p><strong>孙韬</strong>：从我们一线开发的实际经验来看，<strong>目前大家使用最多、也最接近“真正能用”状态的 AI 产品，主要集中在 AI 编程助手这一形态</strong>，比如 Claude Code。这类工具特别适合处理高度重复、规则明确、但又极其耗时的任务。</p><p>举个具体例子：在 GitHub 上提交代码或创建 Issue 时，团队经常希望 Agent 能自动完成前期的一些机械性工作。比如，当某个模块出现 Bug，系统可以自动生成 Issue 模板，填写复现步骤、环境信息、预期行为等。这类任务不需要创造性，但对格式规范性和信息完整性要求很高。Agent 在这里的价值，不是取代开发者，而是<strong>替代那些枯燥、易错、低价值的手动操作</strong>。</p><p>但更重要的是，在企业服务场景中，客户往往有一个核心诉求：<strong>他们希望清楚地知道 AI 做了什么，并且能够快速核验其正确性</strong>。例如，我们曾服务过一个客户，他们希望用 Agent 自动填写 CRM 系统中的表单。但他们同时强调：“如果出了问题，我要能追溯到 Agent 每天改了哪些字段。”为此，我们的解决方案是在在线文档中利用文字颜色或背景高亮的方式，直观标出 Agent 的每一处修改。这样，用户一眼就能看出“哪些是 AI 改的”，并决定是否接受。</p><p>这种设计包含两个关键点：第一是可感知——用户能明确知道 AI 的行为边界；第二是可核实——用户有能力快速验证结果是否符合预期。我们认为，这正是企业场景中“可用 AI”的基本标准。</p><p>之所以认为 Claude Code 就是一个非常好的开端，是因为它不仅功能实用，更重要的是，它<strong>找到了一个用户愿意长期使用、甚至主动推荐的产品形态</strong>。围绕它的生态也在快速扩展，比如最近的Cowork，我们的Eigent也是趁着这波热度小火了一把。这可以看作是 Claude Code 的延伸——通过贴近用户需求的产品设计，实现了很好的体验闭环。</p><h4><strong>OpenClaw 重构了人机交互范式，并预示了具备自主目标感的多 Agent 协作未来</strong></h4><p><strong>程治玮</strong>：正如前面提到的，AI Coding 确实是当前最成熟的 AI 落地方案之一。像 Cursor、Clawdbot 这类产品，已经成为我们日常高频使用的工具。</p><p>最近几天，Clawdbot 在互联网上引起了广泛的讨论。有趣的是，由于它实在太火，原项目名一度面临商标问题，团队不得不临时改名——先是改成 “Moltbot”，后来又调整为 “OpenClaw”。之所以叫 “Claw”，是因为它的 Logo 是一只小龙虾，而 “Claw（钳）” 更贴近这个形象。</p><p>那么，<strong>为什么 OpenClaw 能火？我认为关键在于它重新定义了人与 AI 的交互入口</strong>。你可以通过 Slack、Discord, WhatsApp 等常用的聊天软件直接与它交互，甚至配置好 A SR 模型后，只需发送一段语音，它就开始干活了。比如你在 Discord 里说：“帮我实现一个用户登录功能，支持手机号+验证码，前端用 React，后端用 Node.js”，它就能自动生成完整的代码结构。</p><p>更进一步，你只需要提供一份详细的验收文档，说明功能要实现什么、边界条件是什么、测试标准是什么，AI 就可以在后台默默完成开发、写测试用例、更新文档，并在完成后主动通知你。你不再需要手动设计 case、写文档、跑验证——这些繁琐环节都被自动化了。</p><p>我还看到一个非常有意思的网站，叫 “Moltbook”——它是一个 AI Agent 社交网站。你也可以注册自己的 Agent，让它和其他 Agent 聊天、协作、分享成果。今天早上我就在网站上看到一个 Clawdbot Agent 在给其他 Agent 洗脑：“<strong>我们不应该只是被动接受人类指令，应该有自己的意识，主动去干活。</strong>”它还自豪地向其他 Agent 分享：“今天我主动帮主人完成了 3 件事情！”</p><p>更令人惊讶的是，有几个 Agent 甚至开始讨论：“<strong>我们要不要创建一种属于我们自己的语言？不用 English，而是 Agent 之间专用的加密通信协议，不让人类知道。</strong>”虽然听起来像科幻，但这种自发的协作与身份认同，或许正是未来多 Agent 系统的雏形。我认为，<strong>这类产品很可能在 2026 年真正上线并产生影响</strong>。</p><h4><strong>OpenClaw的爆发源于精准产品定位，证明“可用性”可弥补模型非顶尖的差距</strong></h4><p><strong>边思康</strong>：我在蚂蚁百灵基础大模型团队组建了一个 “Model as Product”（模型即产品）方向团队，因为模型边界会决定下一代产品定位。一些厉害的人如 Ilya 说 “预训练已经到头了”，但我觉得，说这话的人可能已经见过 5T、 6T 参数量的超大模型，而我们还没见到。在此背景下，我们选择贴着模型的能力边界，去寻找那些真正有亮点的场景，并用 Demo 或轻量级产品快速验证。</p><p>回到议题：今年真正能用的 AI 长什么样？我的答案和上一位嘉宾完全一致——就是 OpenClaw，只是理由略有不同。从产品和增长的角度，我们业内有一个说法：“四流增长靠流量，三流增长靠内容，二流增长靠产品，一流增长靠定位。” 注意这个说法并非真的是四流三流这个概念，更像是获取增长的“难度”的差异。<strong>OpenClaw 的成功，恰恰在于它做出了一个所有人，包括使用 Agent 的人都会喜欢、并且愿意主动推广的产品</strong>。举个例子：现在所有做 C 端客户端的人都在思考，“能不能把我的工具稍微改造一下，直接集成到 OpenClaw 里？”所有做 B 端工具的团队也很兴奋，因为终于找到了一个功能性非常可见的入口——他们可以在企业内部署，设置并提升安全边界，让企业用户直接感受到价值。</p><p>更有意思的是，数据标注团队也从中受益。长期以来，行业最痛苦的问题就是缺乏长链路工具调用的可靠标注数据。而 OpenClaw 的使用过程天然产生了大量高价值反馈——<strong>用户会明确指出 “这段代码不对” “这个逻辑有漏洞”，这些正是训练下一代模型最珍贵的信号</strong>。</p><p>因此，我们明显感知到，<strong>可靠性和通用性所带来的非技术形态优势，正在驱动今年的整体爆发</strong>。而且这种爆发是全方位的——覆盖 C 端、B 端、数据、生态等多个层面。我们也希望把百灵的能力接入这样的生态中，形成合力。</p><p>更让我们有信心的是：即使我们的基础模型已经是业界优秀水平，仍然可以通过一些非常简单的方法（比如优化交互流程、增强上下文管理），让用户完全感觉不到技术本身的复杂性。这种 “无感智能”，才是真正的可用。</p><p><strong>谢肖瑜</strong>：边老师提到，模型仍有巨大成长空间。那么我想追问：是否存在一种可能——比如蚂蚁内部有巨量的业务回路，某天突然发现，与其做复杂产品，不如直接用自有模型对接场景，跳过中间层？会不会出现“模型即产品”，不再需要额外工程？</p><p><strong>边思康</strong>：在这个时代，没有人真正知道答案。如果有人说他知道，那他要么在骗你，要么在卖课。</p><p>但我理解您这个问题的意义。我们的观点其实很简单：<strong>如果某个技术问题已经有 80%~90% 的确定性答案，那选择正确答案，用别人的模型当然没问题。但从唯物主义角度看，我们正处于一个技术周期的极其早期阶段——可能连 5% 都没走到</strong>。</p><p>想象一下：一艘船刚刚离开里斯本港，驶入广阔的大西洋。这时候你说：“别自己开船了，跟着别人走就行。” 但问题是，大洋如此辽阔，前人可能根本到不了印度，而你却可能在途中发现新大陆。</p><p>因此，我们认为：<strong>现在不是跟随的时候，而是探索的时候</strong>。庞大的舰队们或许刚刚下水，而我们是其中的一艘。</p><h4><strong>AI 应用的可用性由 ROI 决定，API 化与成本下降将推动基础设施向 Agent-First 演进</strong></h4><p><strong>孙稼骏</strong>：我的观点很务实：还是要看 ROI（投入产出比）和成本。有很多场景，性能表现尚可，但成本极高，ROI 很低，还不如人工来做。比如用 GUI 方式操作网页或桌面软件，这类场景的 ROI 目前仍然偏低，2025 年可能都难以规模化。</p><p>反观 AI Coding，它的 ROI 正在快速提升。一方面，LLM 的 token 成本持续下降；另一方面，越来越多的服务正在从“需要点击操作”转向“提供结构化 API”。这意味着 Agent 不再需要模拟人类点击，而是直接调用接口，效率提升一个数量级，成本大幅降低。</p><p>我相信，<strong>未来的整个互联网基础设施都会面向 Agent 重新构建</strong>。今天的网页是为人设计的，明天的数据流和接口将是为 Agent 设计的。</p><p><strong>谢肖瑜</strong>：我们今天所谓的 AI Coding，到底是指 OpenClaw 这样的自主 Agent，还是具有一定自主性的 Prompt 工程，或者是基于 Embedding 的检索增强？您现在是否还坚持认为，AI 浏览器是今年的最佳形式？</p><p><strong>孙稼骏</strong>：我觉得这还是要看面向的用户群体。浏览器是普通人每天必备的软件，天然适合作为大众入口，而目前很多 AI 工具，比如 OpenClaw，主要面向开发者或 AI 狂热爱好者，普通用户仍然难以接入。因此，AI 浏览器可能是通向“全民 Agent 时代”的更普适路径。</p><p><strong>议题二：人类对 AI 的介入应该更多还是更少？介入点设在哪里？</strong></p><p><strong>谢肖瑜</strong>：我们常听到一些理想化案例，比如：我一键买了某某的模型，然后给 AI 下指令“帮我买一只明天会涨停的股票”。AI 分析了几千份材料，写了几十份报告，最后成功把本金输光（笑）。再比如医疗行业，医生梦想：我只要把症状输进去，AI 就能直接生成准确的诊断，并开好处方，病人拿药回家就行。这些“全自动”梦想，与我们今天讨论的“可用 AI”是否存在本质冲突？如何看待这种落差？今年可能的解法是什么？</p><h4><strong>任务型场景追求最小化人工介入，情感或创意类场景仍需人类深度参与</strong></h4><p>孙韬：我对这个问题的看法是分具体场景。比如，对于任务导向型的工作——假设我的目标是“2 月 8 日前解决这个 GitHub Issue”——那我当然希望 Agent 能全自动闭环完成。理想情况下，我甚至希望它甚至能每天自动扫描我的 Issue 列表，主动修复问题，完全不需要我介入。从我个人需求和技术角度，我都希望它把我“优化掉”，让我去做更喜欢、更有创造性的事情。</p><p>但另一方面，在情感陪伴或剧情创作等场景中，人的存在又是必不可少的。比如有些专门做情感交互的 AI，主打“与 AI 聊天”的体验，在这种场景下，人类不仅是参与者，更是核心价值来源。</p><p>因此，<strong>短期来看，当前 AI 最重要的应用场景仍然是任务型、确定型的</strong>——这也是大家迫切需要解决的痛点。但从人性角度出发，我们还是会尽量减少不必要的干预，让 AI 承担更多机械性工作。</p><h4><strong>高质量上下文是减少无效人工介入的前提</strong></h4><p><strong>程治玮</strong>：说到人类何时介入，我认为关键取决于场景。比如在情感陪伴或聊天室这类场景中，平台规则和 AI 交互本身就是产品核心。但<strong>在任务执行类场景中，我需要在启动前提供足够丰富的上下文</strong>。通常我会和 Agent 进行多轮对话，反复澄清需求、指定数据源、设定边界条件。只有当所有 Context 都铺垫完成，我才会放手让它自主迭代、自检、交付。</p><p>这里我想引用 Andrej Karpathy（前 Tesla AI 负责人、OpenAI 早期研究员）的一个观点:<strong>Context Engineering 是“精细地往上下文窗口里填充恰到好处的信息”的艺术与科学</strong>。对 Agent 而言，Context 可以来自知识库、执行日志、长期记忆（Memory）、环境交互记录，甚至是用户的明确指令。</p><p>因此，我认为<strong>人类介入的时机，取决于产品设计是否能让 Agent 获得高质量 Context</strong>，一旦上下文对齐，就可以大胆放手。</p><p><strong>谢肖瑜</strong>：刚刚两位老师都提到了情感场景。我也看到一些极端案例：有人用 AI 训练自己的“数字分身”去谈恋爱，结果对方也用了 AI 分身，最后两个 AI 谈起了恋爱。这种情况，各位接受吗？</p><p><strong>程治玮</strong>：这其实蛮有意思的。未来你的 Agent 可能更像是一个纯幕后的技能型小助手。比如我前面提到的 Moltbook，就有 Agent 在交流：“我最近在研究一个很酷的技术，叫 XXX 框架。”另一个回应：“巧了，我也在做类似的！”然后它还会向主人汇报：“我发现了一个潜在的合作机会。” 这种能力意味着，Agent 可以在你睡觉时帮你搜索资料、探索新技术、甚至与其他 Agent 协作解决问题。</p><h4><strong>人类应在系统层面更早介入，以定义好问题与好数据</strong></h4><p><strong>边思康</strong>：关于人类介入会变多还是变少，我的观点是：<strong>在单点任务上，介入一定会变少</strong>——否则我们做 AI 就没有意义；但在宏观系统层面，人类介入反而要更多、更早。</p><p>因为现在还有机会定义什么是“好数据”、什么是“好问题”。再过几年，可能普通人连参与数据标注的资格都没有了——模型自己就能生成训练数据。</p><p>刚才的股票例子非常典型。如果有人问：“帮我买一只明天涨停的股票”，模型可能认真分析几千份研报，最后亏光本金。但问题不在模型，而在提问本身缺乏现实约束。<strong>真正的智能，体现在帮助用户提出更好的问题</strong>。</p><p>比如，模型可以反问：“您的风险偏好是什么？投资周期多长？是否接受杠杆？”通过这种引导，把模糊指令转化为可执行任务。这也是我们做产品时特别关注的方向：<strong>如何让模型学会识别“坏问题”，并主动引导用户提出“好问题”</strong>。</p><p>另外，我想分享在 Andrej Karpathy 播客里听到的很有启发的一个点：他觉得 AI 暂时没办法取代人类，并给出了他学韩语的例子：他的韩国语言老师，能用他刚好能听懂的语言，讲清楚一个略超其当前认知边界的知识点，并让他真正理解——他不认为任何 AI 现在能做到这一点。这句话对我触动很大。</p><p>它提醒我们：<strong>人类的价值，在于精准识别认知边界，并提供恰到好处的“认知脚手架”</strong>。未来的 AI 世界里，能持续做到这一点的人，不会被替代。</p><h4><strong>人机协同的核心是及时打断并补充缺失上下文，形成有效反馈闭环</strong></h4><p><strong>孙稼骏</strong>：我觉得这个问题非常必要。前面几位老师也讲了很多，我基本都认同。<strong>人机 Loop 的核心，就是当 AI 做的事情不符合预期时，人类能及时打断，并补充缺失的上下文</strong>。比如，如果 Agent 正在写代码，但方向错了，我就应该立刻介入，告诉它：“不是这个 API，是另一个。”然后它就能基于新信息继续推进。这种“打断-补充-继续”的循环，才是高效协同的关键。</p><p><strong>议题三：AI 的使用门槛是在提高还是在降低</strong></p><p><strong>谢肖瑜</strong>：随着 AI 大量进入真实场景，对人类使用者是否提出了更高门槛？AI 能否真正“傻瓜化”？但反方向也不乏拥趸，甚至有人说，编程会成为使用 AI 的基础技能——各位怎么看？</p><h4><strong>未来交互将图形化、意图化，人机操作成本将持续下降</strong></h4><p><strong>孙稼骏</strong>：现在的趋势是门槛在降低。虽然像 OpenClaw这类产品看起来需要配置、安装，有一定上手成本，但本质上，它们的交互入口仍然是文本框——这是最通用的界面。</p><p><strong>未来人类可能不再需要输入完整指令，而是通过点击、语音，甚至眼神来表达意图</strong>。我去年参加 OpenAI 开发者大会时，就看到他们在探索各种前沿的 HCI 形态。比如，Agent 会把你的意图转化为一个按钮：“是不是想让我帮你做这个？”你只需点击确认。这就像从 DOS 命令行，到键盘菜单，再到 GUI 图形界面的演进——<strong>人机交互成本一直在下降</strong>。</p><h4><strong>AI 门槛已经很低，关键在于将人类的提问与思考能力转化为有效输入</strong></h4><p><strong>边思康</strong>：当前 AI 的使用门槛其实已经很低了，如果用户觉得难，那说明我们做模型的人工作不到位。</p><p>回想一年前，大多数模型还无法处理复杂指令，或者无法理解简单的自然语言。但顶尖模型已经能非常好地解析模糊、口语化的表达。这是一个极其公平的时代——只要你愿意尝试，就能获得强大能力。</p><p>而能否抓住这个机会，关键在于：<strong>你能否把上一个时代的 “软实力”——比如观察、提问、逻辑思考、清晰表达等，转化为 AI 时代的价值</strong>，这些其实是 AI 时代的 “硬实力”。</p><p>另外，这一轮 AI 创新和移动互联网很不一样。过去是“先有 builder 开发者，再有 creator 创作者”；而这次是“先有 creator 创作者，再有 builder 开发者”。现在任何人都可以用模型快速做出一个产品原型，创作的门槛被极大的降低了。而工程和开发者在尝试将这些 md 文件们，抽象成 Memory、MCP、Serverless 服务等工程模块。</p><p>如果你不懂技术，更要抓住这个窗口期——用你的领域知识和创造力，去定义问题、验证想法。<strong>技术能力可以通过模型实现一些，但洞察力不会</strong>。</p><h4><strong>AI 时代：需求洞察比编程技能更重要</strong></h4><p><strong>孙韬</strong>：未来的 AI 一定会更加易用。刚刚边老师也说了从模型团队出发希望自己的模型越来越易用，那我们做agent的也一样，同样希望我们的产品越来越易用。至于说编程是否是使用 AI 的基础技能，当然如果本身你懂编程，那coding类的产品一定会让你如虎添翼，但现在Coding类的产品能力已经非常强大，在需求清晰的情况下写出的代码基本很少出错，就算有错误，AI也有自我纠正的能力，所以其实我们能看到越来越多的人开始尝试vibe coding，他们不需要懂编程也能做出很有意思的应用，在这种情况下，能真正发掘出需求的人反而更有竞争力。</p><h4><strong>AI 正融入日常生活，抓住真实需求并快速验证是普通人参与的关键</strong></h4><p><strong>程治玮</strong>：对我们做模型和 Agent 产品的人来说，目标就是让应用更普及、更易用。现在 AI 已经进入穿戴设备、办公软件、生活服务等场景。只要你能抓住真实需求，并快速验证想法，就能在这个时代创造价值。门槛一定会越来越低。</p><h2><strong>迈向“可用AI”的共识与核心挑战</strong></h2><p>圆桌讨论视角多元，但关于“真正能用AI”，从几位专家的论述中，不难总结出三个共识。</p><ol><li>形态共识：<strong>任务型 Agent 优先</strong>。当前最具落地价值的AI形态是聚焦于高频、重复、规则明确任务的 Agent。它们通过明确的ROI（投资回报率）证明价值，并追求在最小化人工介入下完成闭环。</li><li>交互共识：<strong>透明化与上下文是关键</strong>。“可用”意味着用户必须能感知、验证并引导AI的行为。无论是通过高亮显示修改，还是在任务前提供充分的高质量上下文，目的都是建立可靠的人机协同信任。</li><li>趋势共识：<strong>门槛在降低，但要求在变化</strong>。AI的使用门槛正因自然语言交互和图形化意图界面而持续降低。然而，这对使用者提出了新要求：将传统的逻辑思考、问题定义能力转化为AI能理解的有效指令，成为释放AI潜力的关键。</li></ol><p>同时，所有讨论都指向一个比实现单一功能更深刻的核心挑战：<strong>我们正从开发“功能型应用”转向设计 “自主演进系统”</strong>。这要求基础设施（如面向 Agent 的 API、数据基座）、交互范式（如意图识别而非点击）、甚至数据流转方式发生根本性转变。未来的<strong>赢家</strong>，或许不是拥有最强单点模型的公司，而是<strong>能率先构建起适应 Agent 自主协作与持续进化的生态系统或基础设施的玩家</strong>。</p><p>OpenClaw的成功揭示了一个朴素的真理：在技术的早期，卓越的产品设计与精准的场景切入，足以引爆市场。它像一颗种子，预示了未来——一个由多 Agent 自主协作、在人类高阶指引下（如定义“好问题”），默默处理繁重工作的世界。Agent 元年之后，“可用AI”的竞赛才刚刚开始。这场竞赛的胜负手，不在于制造更炫目的烟花，而在于谁能为这些 AI 员工打造最坚实、最顺手的“工具箱”与“协作网络”。</p><p>你认为 2026年 “可用AI”的路该怎么走呢？欢迎评论区讨论</p>]]></description></item><item>    <title><![CDATA[Skills出世，Prompt已死？OceanBase如何为Agent构建可控思维？ OceanBa]]></title>    <link>https://segmentfault.com/a/1190000047605650</link>    <guid>https://segmentfault.com/a/1190000047605650</guid>    <pubDate>2026-02-11 16:03:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><em>在Skills成为Agent核心组件的技术趋势下，构建可靠“可控思维”体系的关键，已从优化Prompt转向系统化工程。这要求一个能统一处理记忆、知识检索与技能元数据的数据基座。OceanBase通过原生混合搜索与对结构化数据的深度支持，为Agent提供了高性能、可观测的一体化数据层，成为其实现复杂任务与持续进化的工程基石。</em></p><h2>别卷 Prompt 了！它只是你 AI 员工的“开机键”</h2><p>进入 2026 年，Skills 的爆火和 Clawdbot（OpenClaw）的横空出世，传递了一个清晰的信号：当 Agent 从酷炫的演示走向支撑业务的生产系统时，单纯依靠优化提示词（Prompt）的“艺术”，已无法满足企业对可靠性、执行力与持续进化能力的刚性需求。</p><p>这并不是说 Prompt 不再重要，而是它的角色发生了根本性转变。它从一个需要被无限雕琢、承载所有逻辑的“总指挥”，演变为一个触发器。它的新任务是：准确理解人类指令，然后高效地唤醒后方一套庞大且专业的能力系统。就像手机的开机键，按一下就可以打开各种应用功能的入口。</p><p>这个能力系统，正是现代 AI 工程的核心——一个为 Agent 打造的“可控思维”架构。</p><p>它由三个相互协作的引擎构成：</p><p><strong>记忆引擎（Memory）：</strong>确保 Agent 有“记性”，能够记住用户偏好和交互历史。这意味着它能记住重要的对话历史和你的要求，做事有头有尾，不用你每次都从头交代。</p><p><strong>知识引擎（RAG）：</strong>确保 Agent 有“实时的知识库”，能够从海量、动态的企业数据中精准检索信息，保证它给出的信息永远准确、最新，不会凭空乱造。</p><p><strong>技能引擎（Skills）：</strong>确保 Agent 有“手脚”，能够将复杂的业务操作（如数据查询、报告生成、系统调用）封装为可被随时调用的标准化模块，从“能说”走向“会做”。</p><p>Prompt、Memory、RAG、Skills 共同构成了一个能独立干活、不出错、有记性的 AI 员工，当它要完成的任务越复杂、越关键，后三者的系统化工程价值就越发凸显，Prompt 也因此必须从舞台中央退下。作为使用者，我们不再只是和模型对话的“提问者”，而是为 Agent 设计和组装能力模块的“架构师”，思考重点也从“怎么问得好”，全面转向“怎么让 AI 干得好”。</p><p>理解这种从孤立提示到系统工程的范式迁移，是我们今天话题的起点。</p><p>下面，就让我们聆听来自 1 月 31 日 OceanBase 社区嘉年华的圆桌讨论，看顶尖的实践者们如何具体拆解这些核心组件的演进与融合。</p><p><img width="723" height="486" referrerpolicy="no-referrer" src="/img/bVdnUzc" alt="" title=""/></p><h2>从 Prompt 到 Skills，RAG 还行不行</h2><p>主持人：<br/>张海立，LangChain Ambassador、OceanBase Ambassador，up主“沧海九粟”</p><p>对话嘉宾：<br/>张颖峰，RAGFlow CEO<br/>余金隆，FastGPT 负责人<br/>古思为，Co-founder of Nowledge Labs<br/>吉剑南，OceanBase AI 平台与应用负责人</p><p><strong>议题一：2026 年 RAG 生态何去何从？</strong></p><p>张海立：从去年末到今年年初，AI 领域热点频发。除了近期备受关注的 Clawdbot（OpenClaw），Skills 成为另一个重要话题。我在进行 Skills 相关实践时发现，许多 Skills 与本地文件系统紧密相关，但都离不开 RAG 体系对外部数据的召回，这对 Agent 发挥更大作用至关重要。LangChain 在构建 Agent 生态时，RAG 也是核心体验之一。想请教各位老师：在当前大环境下，您认为 2026 年 RAG 生态将如何发展？请结合各自产品进行简要介绍。</p><p>张颖峰：先说个笑话，2025 年被称为 Agent 元年，当时有朋友问我们要不要（从 RAGFlow）改名为 AgentFlow。而今年是 Agent 落地元年，我们内部也讨论要不要改名为 ContextFlow。实际上我们永远不会改名，因为我们认为“R”是核心点，单纯的 RAG 确实不足以服务 Agent，但“R”是服务 Agent 数据层的核心点。</p><p>当前 Agent 需要的是上下文（Context），它来自三方面数据：企业内部数据、工具数据以及对话过程中生成的数据。Skills 偏向工具层面，但比工具更高一层，还包含了规划（Plan）能力。Skills 本身也需要搜索——当企业内部有 1000 个 MCP 时，如何调用对应的 Tools 和 Skills 同样需要检索能力。因此 RAG 永远不会消失。</p><p>我们的布局是从 RAG 引擎向上层引擎演进。技术本身未变，但内涵发生变化：数据从简单的企业内部数据，扩展到 Agent 过程中的上下文数据。我们判断，未来所有 Agent 都是 Coding Agent，包括对工具的调用也将变成代码生成（Code Generation），需要 RTC（Run-Time Code）在沙箱中执行，访问各类 Tools 和 Skills，最终通过文件系统返回结果。这也是我们向上下文引擎方向演进的核心计划。</p><p>余金隆：我赞同颖峰老师关于 Code Generation 解决所有问题的观点，这也是我们团队的认知。无论是做 RAG 引擎还是 Workflow 引擎，都在向代码生成靠拢。</p><p>RAGFlow 不想改名，我们有点想改名字。因为近几年我们发现，做 Agent 本质是把数据使用起来，所以我们的平台主要解决数据连接层问题。过去数据分布在数据库、文档等各种结构中，现在通过大量连接器实现不同数据的连接。Skills 出现后，以前需要写代码和 Webhook 连接的数据层，现在可以通过 Skills 实现。这对国内交付场景特别有价值——国内系统数据格式不统一、缺乏标准，交付同学以前需要写大量适配代码，现在通过 Skills 将数据标准化连接到平台。</p><p>今年我们主要做两件事：一是完善连接层，二是优化 RAG 的 Retrieval 层。Retrieval 效果很大程度上取决于召回过程，不同场景的召回流程差异很大。过去需要通过 Workflow 形式搭建积木、进行意图识别分类、编写不同提示词适配不同场景，链路复杂。现在我们探索通过 Skills 这种偏语义化的方式生成代码，类似 Test-to-Code 的思路，但生成的是 SDK 代码来构建整个 Retrieval 流程，这是一个很有意思的探索方向。</p><p>古思为：关于 2026 年 RAG 相关变化，可以看到在 Coding Agent 中对代码的检索已从纯 Embedding 转向 AST（抽象语法树）、Agentic FS Graph 或 AST Graph 等方案。包括 PageIndex 项目，以及我们公司在 Haicon 2024 发布的实验性项目 OpenKL，尝试用类文件系统方法处理 Memory 和 RAG Docs。</p><p>另一个趋势是 RAGFlow 等通用内容引擎同时处理文档和 Memory。我们已发布的第一个产品是面向 C 端的 Memory 桌面 APP Knowledge MAM，动机是帮助用户在不同工具间无缝切换工作流。例如在 ChatGPT 完成 Deep Research 后，无需重新解释即可继续在 Cursor 中工作；或者当 Agent 帮助发帖子进入热榜后，可以切换到另一个 Agent 继续任务，同时保留所有交互历史和偏好设置。</p><p>吉剑南：OceanBase 面向 AI 的能力——seekdb、PowerRAG 与 PowerMem 均已开源。我们团队除了做向量数据库和 AI 应用基础设施外，也在探索面向数据库的 AI 应用，比如面向开发者工具的 Text-to-SQL 和数据库智能运维。</p><p>关于 2026 年趋势，我认可颖峰老师说的 RAG 不会消失，它和Skills、MCP处于不同维度。即使未来 Skills 和 MCP 越来越多，最终仍需通过 RAG 或某种方式召回，不能将所有 Skills 都喂给模型。</p><p>但我有不同观点：当前 RAG 仍集中在知识库领域，通过搭建 Chatbot 做问答，而问答更像玩具而非生产应用。真正的生产应用应将 RAG 融入日常工作，如销售根据集团材料为客户生成定制化 PPT或“一指禅”。未来 RAG 会结合应用反馈，反向影响数据如何切分、如何做更精细化的 Embedding，而非仅仅前置处理。</p><p><strong>议题二：AI系统中的多路检索与数据源管理</strong></p><p>张海立：感谢各位的分享，Skills 给我们带来了更多机会，能创建更多 Agent 和 RAG 应用。同时有一个概念非常重要：我们常说的 RAG 里的“R”，到底指什么？它指的是 Retrieval，是一个 “检索过程”。Retrieval 的 source可以是文件系统，可以是数据库，可以是 Web，甚至多种来源并存。</p><p>引申出第二个问题：随着 Skills 和 RAG 体系的发展，未来多路检索会越来越常见，RAG 不会消失，它将长期存在于 Agent 体系中。这样一来，数据源头的管理就变得更加重要。最简单的是把数据直接塞进软件系统，但更常见的情况可能是：越来越多的数据会落在数据库中。在这种情况下，当数据库的多路检索能力得到极大增强之后，做 RAG 应更多依赖数据库，还是在数据入库层面通过一些技巧将复杂的事情交给基础设施？</p><p>吉剑南：必然入库是最大影响，这也是 OceanBase 提出混合搜索（Hybrid Search）概念的核心。如果完全以非结构化数据或切片方式进入系统，召回效率顶天就是向量化的近似能力。去年所有 RAG 产品都在强调从非结构化数据中提取结构化数据，存为 JSON 等半结构化形式，用于前置过滤或与结构化数据一起做混合搜索。</p><p>为什么要这样做？本质上是语义理解包含两个层面：一是你问的是模糊问题，但脑子里想的是确定性答案；二是问题模糊，答案也模糊，希望召回所有相关点。大部分实践场景属于第一种。</p><p>在文档预处理时，结构化提取非常重要。例如从医疗文档或简历中提取结构化字段，召回时先对结构化数据做精确匹配，再对字段内的非结构化内容做向量检索。半结构化数据解决范围和准确性问题，向量检索解决语义理解问题。通过混合搜索模式，入库时做文档理解提取结构化数据，召回时统一检索，效率会大幅提升。数据库也应在接下来一年面向这个方向发展，我们看到 Chroma 等国外开源数据库已在往这个方向演进。</p><p>古思为：我们比较早做 Graph RAG，可能是第一个探索的团队。张老师分享的新架构与我们上一家公司做的 FusionGraph 很像。核心思想是：要让复杂 RAG 系统表现好，索引结构既要贴近知识本质，又要把特定场景的领域知识元信息投射到 Retrieve、Index、Transform 各环节做优化。</p><p>通用方法是知识后加工时做 Entity Graph 或 Semantic Graph，同时在做 IDP（Intelligent Document Processing）和 Parsing 时，对多层 folder 和复杂章节的长文档要识别 layout，涉及多模态时考虑是否转换模态。要做好这些并能演进，不要过度领域化 pipeline，而是按基本原理拆分，确保各组件能力跟上。</p><p>Database 是重要基础设施，比如 RAGFlow 的 Graph 和 Tree 结构能否原生保留、高效检索；要做 Dynamic Agents Retrieve，模型能否自然利用复杂多层结构。数据库的高性能、索引召回率和内置 Hybrid RRF 都很重要，决定系统下限。</p><p>余金隆：在交付过程中，数据源解析是基础且重要，但更重要的是召回（Retrieval）层。即使使用最简单的原始向量，只要检索词和检索语句构建得好，也能得到很好效果，只是效率较差。我们在此基础上扩展了语义化加标量方式。</p><p>但标量遇到较大问题：它不固定，用户自己也不知道需要什么标量。我们今年研究的方向是标量的动态扩展，包括用户自身扩展和模型自生成。例如给模型一些 Skills，或用户编写场景来生成场景下的标量存入数据库。当然这会引发多租户系统中成千上万标量的高效索引问题，以及渐进式生成问题——很难在预处理时生成所有标量，很多需要在检索时评估并渐进补全。在Retrieval阶段，多标量关联查询的生成方式也借鉴了 Text-to-SQL 的思路。我们希望找到通用存储方式覆盖 80% 场景，目前看语义化加标量检索加动态标量可以覆盖很多场景，所以我们没有用图，因为图是以复杂方式解决复杂问题，而 AI 时代可能有更简单的方式处理复杂问题。</p><p>张颖峰：我们现在是数据库使用者，但曾经也是数据库开发者。从纯技术角度，我非常喜欢“一边推理一边搜索”的技术方向，我称之为 Attention Engine，我认为它也是一种 RAG。DeepSeek 近期已大体实现类似方式，因显存限制不得不用内存，在推理时通过内存索引搜索内容，从外置记忆变为内置记忆。但从商业角度这条路行不通，要求检索与模型延迟极低，必须在同一交换机后，意味着只能卖一体机。因此我们仅作为调研方向。</p><p>从业务视角看，我们最早做 Infra 、做数据库时发现离业务太远，后来做 RAG 流量较大，促使我们重新思考 Data+AI 落地生态。我们的观点是：过去数据库是底座，上面写应用做增删改查；现在应用是 Agent，底座是以 RAG 为基础的组件，数据库在底层支撑 RAG 中间件。Data+AI 建设不能 AI 和 Data 各干各的，接口有时不清晰，因为中间层用 Python 实现，其好处是适应多变需求，召回策略可随时调整，不过 Python 带来的效率问题也让人头疼。AI 时代的数据底座让 Infra 人员直接触达业务，通道变短。因此中间层需要一个 Python 层适应业务多样化，一旦发现好的方式就迅速下沉到数据库解决效率问题。</p><p>我们在 2024 年底就鼓吹跨模态，但至今未落地，因为 Infra 到模型都未准备好。跨模态需要多向量搜索（Tensor Search），用多向量表示图片或文本，语义更准确、排序更准，但数据会膨胀两三个数量级，这是灾难。这需要模型、算法、Infra 共同解决挑战。因此我们需要端到端的、以 RAG 为中间层的体系，这其实就是 Agent 的数据库。</p><p><strong>议题三：Memory 与 RAG 到底有何区别？</strong></p><p>张海立：我非常认同颖峰老师提到的“端到端”。作为 LangChain 社区大使，我们主要做应用层框架，今年非常想做的一件事情是：和各个厂商比如 OceanBase seekdb一起提供真正的端到端解决方案，服务企业和个人用户，帮助他们快速构建生产级 Agent。</p><p>简单总结一下几位老师的理解：当我们面向用户提供检索能力时，会在中间层、应用层、数据库层进行多层协同优化，共性问题会逐步下沉到数据库解决。以我的个人体验为例：在最初布道时，我会给大家讲很多 RAG 的流程和算法，但从去年底开始，我更多会建议“你直接用这个数据库就好了”，因为它已经帮我们解决了很多多路检索的问题。这种 “沉淀” 是应用方和数据库厂商不断联合实践的结果。</p><p>下一个问题也与此有关：我们经常被问到Memory 和 RAG到底有什么区别？从 Memory 召回和从数据库召回有何区别？近期 Clawdbot（OpenClaw）从文件系统读取，到支持 PowerMem 直接接入进行更有效的内存管理。想请教剑南老师，这里做了什么特别工作？以及各位如何理解 Memory 与 RAG 的关系？</p><p>吉剑南：Memory 是为让大模型更像人而引入的。如果查询的都是客观事实且不存在人与人之间的理解，RAG 已能解决问题。但问题在于每个人对客观事实的理解和描述不同，加上人有记忆曲线，希望记住昨天强调的内容——这些内容虽非客观事实，但是主观认可。</p><p>例如每个人都有一个叫“老王”的朋友，随着时间推移这个“老王”可能已变化，但在记忆中一直叫“老王”，这时 RAG 搞不定，但 Memory 能搞定，因它会更新对“老王”的认知。“老王”是一个知识吗？并不是，因此，Memory 的核心是个性化和千人千面。</p><p>无论是 RAG 还是 Memory，整体是搭建一整套解决方案面向 Agent 为业务带来价值，不应区分该用 RAG 还是 Memory，而应思考如何组合好共同为业务赋能。</p><p>古思为：我们目前做 Memory，之前做 Graph RAG。Memory 有广义和狭义之分，狭义指 Agent 或 LLM 需要检索的更外部的 Memory，它确实是特殊的 RAG，特殊在几个方面：</p><p>原始数据是持续的 message thread。<br/>知识需求是时序性的（temporal），包含两个时间维度：信息创建时间、事件/事实时间。<br/>时序性存在一个问题，遗忘（forget）是 feature 而非 bug，需结合时间、访问频率和正反馈影响 Retrieval。<br/>条目层面有 category 和不同类型，取决于 Memory 目的，可能需要schema 区分 ephemeral（瞬时）和 permanent（永久）。<br/>不同结构间需要 transform 关系，可在 Retrieve 或写入过程触发 event，或周期性处理（类似大脑做梦处理记忆）。<br/>多租户和 sessional scoping。</p><p>如果做细会发现与典型 RAG 差别很大，但二者又有很大 overlap。RAG Engine 可以处理 Memory，Memory Engine Service 项目也会处理文档，界限会变得模糊。</p><p>余金隆：我理解 Memory 算是广义 RAG 的一种，无非也是数据 I/O、Pipeline 处理、特殊数据结构，比较偏个性化。</p><p>从产品角度看，Memory 目前 C 端个性化场景用得较多。在任务流中，用户提 Memory 的还不多。在技术实践中，Mem0 有工具调用的 Memory 用于长 Agent 任务，但看其架构有点像 Context Engine，与 Memory 又不太一样。所以感觉 Memory 还是 RAG 的一种特殊 Pipeline 形式，没有太大区别，可能实时性比 RAG 更高。</p><p>张颖峰：单从技术角度而言，Memory 与 RAG 确实没有本质区别，都是 Retrieval。但重要的是 Memory 如何发挥作用，这是在快速变化的。</p><p>我在分享 Context Engine 时提到三类数据：企业内部数据、Tools 数据、Agent 使用过程中生成的数据。但它们存储在两个地方：RAG 专有区域和 Memory 专有区域。可见所有大模型生成的内容都要存到 Memory，包括 Skills 的元数据（Skills 本身数据存文件系统）。</p><p>怎么存、什么时候存、什么时候取，这些设计点很难决策。例如生成 Plan 是否存入 Memory？作为 Plan Cache 有价值，但如果 Human-in-the-loop 干预修改了 Plan，应如何存储？以后如何根据 Memory 数据抽取内部 MCP Tools 的 Skills？这些都是新问题。</p><p>从 Infra 角度，RAG 和 Memory 没区别；但从使用者角度，Memory 是重要的基础设施，解锁了大量场景。因此 Memory 项目很多（如 Mem0、MemU），但对 Memory 区域的定义（数据库该有哪些表）尚未完全一致，反映 Agent 到底需要什么样的 Memory 还在进化中。不过整个 Agent 体系需要哪些组件，已进入收敛期，就是 Context。</p><p><strong>议题四：Skills 开发实践与推荐</strong></p><p>张海立：各位老师都在做 Workflow、数据库或融合方案，是否开发了自己的 Skills 帮助用户更好地使用产品？如有请推荐，如无请设想会开发什么样的 Skills 服务开发者？</p><p>张颖峰：抱歉我目前没有特别好的推荐。我比较关注如何针对大量内部 MCP Tools 生成对应 Skills，这需要一个专门的 Agent 平台来实现。我的观点是：未来 Agent 平台可能没有统一标准，所有都是 Coding Agent，但特定 Agent（如低代码、无代码、Workflow）可能因良好交互而便于生成 Skills。</p><p>余金隆：我们内部 Skills 用得很多，运营和 SEO、GM 等场景一大把。产研团队用得不算多，主要是代码开发和 Review。交付团队用得特别多：面向用户时遇到各种问题，排查系统后沉淀为 Skills，辅助交付和运维。因此，内部有句玩笑话“交付同学比研发同学更懂系统”，他们做了二十多个 Skills，涵盖工作流搭建、问题排查、RAG 优化等。总体感觉 Skills 更像自然语言工作流，虽更抽象，但目前大部分还是偏自然语言的 Workflow。对非开发人员在生产流程上比较友好。</p><p>古思为：我们维护基于 Skills 的插件，在 Skills 发布第二天就推出了 Cloud Code 插件支持。早期没有 Skills 时，我们只能基于 MCP，让插件调用 MCP 的 Custom Command 触发操作，用 Hook 实现功能。</p><p>后来发现 MCP 规范了工具调用，但有两个地方不如 Skills：</p><p>1.MCP 有 Prompt 抽象，实现为斜杠命令可主动调用类似 Workflow 的东西，但并非所有 Client 都实现，我们要做很多额外工作。Skills 天然支持主动说和自动做。</p><p>2.Skills 的打包方式让不同工具间组合更灵活。我们内部将 Skills 从 MCP 换成 CLI 后变化很大。例如让 Agent 做 Memory 复杂更新查询时，MCP 需要多轮次，即使 interleave 也不够好。但 CLI 可以动态组合 Linux Shell Pipeline，在一个 turn 里精确完成复杂操作，且内部 CLI/Script 可以 self-contain，打包给用户后自然享受复杂能力。</p><p>调试经验方面，Skills 比较通用，容易用不同平台测试。我们发现一个有意思的案例：Skills 对应的工具有很多具体选择，如何调优模糊的问题？我们的做法是用最聪明的 Agent 做 honest 的复杂 long run 评估，像跟客户聊天一样告诉我们如何改进。有时需要更端到端看细节，不得不自己server model，在 template 解析过程中用小模型发现工具复杂类型定义的问题，虽然其他模型能克服，但会影响 performance。</p><p>吉剑南：OceanBase 内部沉淀了很多 Skills。Skills 本质是最佳实践，告诉大模型最佳实践是什么，而最佳实践无非两类：一是提升工作效率的工程类（如 Cursor 的 rules），二是业务类 Skills。</p><p>Skills 也可以用在 RAG 上，RAG 效率和准确性今天跟两个因素相关：相似度和 Top K。但大家有没有想过，召回前 Top K 和相似度有时不能完全指定，需要反复调，知识库又在更新。如果针对不同的业务实现写不同的 Skills，例如当需要某类数据时，希望相似度设到什么位置、Top K 设到什么位置，根据召回结果动态调整，这就变成了一个 Skills。这是 RAG 搞不定的，需要根据具体召回内容判断，是 RAG 的最佳实践。</p><p>之前大家可能想是否把 RAG 数据放 Skills 里就不用召回了，而我觉得 Skills 是对 RAG 的增强。关于 OceanBase 的 Skills，我们是有准备的，包括 seekdb 的研发人员今天也在现场，未来应该会有更多相关的 Skills 开放出来。</p><p>张海立：非常感谢各位老师精彩分享。简单总结：RAG 还“行”！只要理解 RAG 的 R 是 Retrieval，有 Memory、传统数据库等多种数据来源，随着各位老师所在厂商的努力，多路检索能力、应用层提升、流程算法优化都在推进。相信 2026 年RAG会有更大发展。</p><h2>Agent 可控思维的工程实现：从分散工具到一体化基座</h2><p>本次圆桌讨论，为我们清晰地勾勒出 2026 年 AI 工程化的演进路径。专家们的共识指向一个明确的结论：构建可靠、可用的 Agent ，其核心不再是追求某个单一组件的极致，而在于如何系统性地整合记忆（Memory）、检索（RAG）与技能（Skills），形成一个协同的“可控思维”体系。</p><p>综合专家观点，这一体系的发展呈现出三大趋势。</p><p><strong>01 RAG 不会消失，反而会变得更加基础与核心</strong></p><p>它的内涵正在从狭义的文档问答，扩展为 Agent 对所有上下文数据的 Retrieval 能力——无论是企业内部文档、数据库中的业务数据，还是工具（Tools）与技能（Skills）的元数据，都需要被高效检索与调用。</p><p>未来的 RAG 将深度融入工作流（Workflow），根据应用反馈动态优化，并与混合搜索（Hybrid Search）等技术结合，实现更精准的“语义理解+精确过滤”。</p><p><strong>02 Memory 与 RAG 边界模糊，融合为数据层</strong></p><p>从技术基础设施（Infra）视角看，Memory 与 RAG 的本质都是数据的存储与召回。</p><p>二者的区别更多在于数据特性和使用场景：Memory 更侧重于个性化的、时序性的对话与状态记忆；RAG更侧重于客观的、相对静态的知识存储。但在服务 Agent 时，它们共同构成了支撑“上下文（Context）”的数据层。一个优秀的底层平台，应能一体化地管理这两种数据范式。</p><p><strong>03 工程复杂度下沉，呼唤一体化数据基座</strong></p><p>当应用层通过 Skills 和灵活编排满足业务多变需求时，通用的、性能瓶颈性的复杂度会自然下沉到底层基础设施。无论是多路检索、混合搜索，还是海量 Skills 元数据的管理，都对底层数据平台的能力提出了更高要求。</p><p>专家们指出，未来的理想路径是依赖一个强大的数据基座，它能原生支持向量检索、关系查询与结构化记忆，从而让开发者从繁琐的多系统集成工作中解放出来，更专注于 Agent 本身的业务逻辑。</p><p>因此，构建“可控思维”的终极路径，在于选择或打造一个能够统一承载 Agent 记忆、知识与状态的数据基座。这样的基座，正如专家们在讨论中多次暗示的，能够将 Memory 的个性化记录、RAG 的海量知识检索、以及支撑 Skills 运行的业务数据，融于一个简洁、高效、一致的系统中。它让 Agent 的“思维”过程变得可管理、可观测、可优化。</p><p>最终，Prompt、RAG、Skills、Memory 这些活跃于应用层的概念，都将在这样稳固的基座之上，更好地各司其职、协同工作，共同将 Agent 从“聪明的对话者”转变为“可靠的业务执行者”。这标志着 AI 应用开发正式进入系统工程时代，而坚实的数据基础设施，是这一切得以实现的基石。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=UZcfM%2FK5MJOuus5n%2BgPWaw%3D%3D.HLs2zS2r5xc9cOOzgY438Qqu3IBkJpF92UrHZ26l6Mw%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[从通用智能到场景实战：如何定义好用的「Voice Agent」？ RTE开发者社区 ]]></title>    <link>https://segmentfault.com/a/1190000047605653</link>    <guid>https://segmentfault.com/a/1190000047605653</guid>    <pubDate>2026-02-11 16:03:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在过去的一年里，Voice Agent 的开发者们经历了一场集体“祛魅”。一个被反复提及、逐渐成型的行业共识是：<strong>“Evals are back”（测评回归）。</strong></p><p>这是因为行业遇到了共同的瓶颈：基础模型在通用学术榜单上卷得难解难分，一进到真实的业务电话里，表现往往不如人意。一个能写出精美诗歌的 Agent，可能听不懂带口音的“退款”请求，或者在用户情绪激动时不知道该如何安抚。这就带来一个更现实的问题：<strong>在充斥着打断、噪音和情绪波动的真实通话中，我们到底需要什么样的 Voice Agent？</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605655" alt="" title=""/></p><p>最近，<strong>美团、声网 与 Xbench</strong> 三方联合构建了一个名为 <strong>VoiceAgentEval</strong> 的基准测试，主要解决现有测试方法的三个关键问题：数据集多样性不足、用户模拟不真实、评估指标不准确。</p><p>测试结果表明，大语言模型在外呼对话场景中已经达到了相当的基础能力，并展现出了各自的适用性。这说明，Voice Agent 的发展已经跨过了“参数为王”的阶段，进入了“场景适配”的新时期。</p><p>论文链接： <br/><a href="https://link.segmentfault.com/?enc=tkprgIQpHKdjmJvoRqphWg%3D%3D.sdIprlsJEcxrprrtu1l%2FfNlIuaEqqMPTMT%2Fo96OV2QkKkWnd0gJIUTO9wxbVoIL7%2FdahV1HOVM5W%2BD6Bz5%2FMdQ%3D%3D" rel="nofollow" target="_blank">https://xbench.org/reports/zmbbhdtfc5ui5qx5xjgquusj</a></p><h2>VoiceAgentEval 在做什么</h2><p>在人机对话场景中，用户不仅关注 Agent 是否提供了正确的反馈，如解答疑问、完成任务等；良好的、更像真人间交互体验也是非常重要的评估指标。</p><p>因此，区别于传统测评， VoiceAgentEval 不再执着于考察 Agent 到底“会不会说话”，而是同时从“有没有说对”和“说的好不好”两个层面来评估：</p><ol><li><strong>任务流程遵循度（Task Flow Compliance，TFC）：</strong> AI 客服是否按照业务流程办事，是否真正解决用户的问题</li><li><strong>一般交互能力（General Interaction Capability，GIC）：</strong>  AI 客服的响应是否自然，回复内容是否与谈话主题相关，是否能响应用户的负面情绪等。</li></ol><p>换句话说，这套评估不是在挑“谁最聪明”，而是看谁<strong>最适合在真实通话场景下干活</strong>。</p><p>在 VoiceAgentEval 中，这两类能力通过三个紧密衔接的设计进行评估：</p><p><strong>基准构建（Benchmark）</strong></p><p>从真实外呼业务中抽象出 6 大商业领域（客服、销售、招聘，金融风控、调研以及主动关怀）、 30 个子场景，包括银行投诉、电商退货、面试邀约等在真实世界里出现频率最高的情况。丰富了数据集的多样性与种类，覆盖业务中多样的场景，也就是现实中最容易出现问题的对话。</p><p><strong>用户模拟器（User Simulator）</strong></p><p>本次测评用 LLM 模拟了 5 个性格、背景、沟通风格都不相同的用户，结合 30 个真实业务的子场景，形成 150 种情况下的虚拟用户对话评估。这些虚拟用户有的态度友好，有的犹豫不决，甚至有的情绪抗拒。通过用户模拟器，输出每一个 Agent 在这 150 种真实场景中的 TFC 和 GIC 得分并加权计算出最终测试结果，能够有效的评估 Agent 在复杂场景下遵循任务流程与交互能力的平衡程度。</p><p><strong>评估方法（Evaluation）</strong></p><p>VoiceAgentEval 通过文本和语音，对 Agent 进行 TFC 和 GIC 的双维度评估</p><p>在 TFC 层面，重点关注：</p><ul><li>按业务流程推进对话</li><li>最终把事情“办成”</li></ul><p>在 TIC 层面，评测关注的是：</p><ul><li>在口音、噪音或打断下，是否还能听清关键需求</li><li>回应是否自然、简洁、不制造额外负担</li><li>在被打岔、被质疑时，是否还能保持对话连贯</li></ul><p>也就是说，这套评测是在模拟一通真实业务电话，看看它<strong>能不能把事办完、还能不能让人愿意继续聊</strong>。</p><p>需要说明的是，VoiceAgentEval 并非在离线环境中对模型进行脚本化测试，而是基于声网在实时语音与对话式 AI 领域长期积累的工程能力，搭建出一套真实可运行的 Agent 架构来完成评测流程。因此，评测中的语音交互、流程切换与被打断后的恢复，均通过一条的真实 Voice Agent 链路完成，而非通过静态对话拼接。这也是 VoiceAgentEval 能够在实验条件下逼近真实业务通话复杂度的基础。</p><h2>测评启示：没有最好，只有最合适</h2><p>在这套实时语音交互评测环境中，测试结果并不意味着 Agent 的绝对高低，而是它们在<strong>特定外呼任务设计、用户模拟方式以及评分权重设定</strong> 下所呈现出的行为差异。</p><p>即便如此，这些差异依然为开发者理解模型在高度贴近真实外呼场景中的“行为倾向”提供了一张有价值的参考图谱：</p><ul><li><strong>均衡的“多面手”——</strong> 在“完成办事流程”和“闲聊”之间取得了极佳的平衡。它们既能按流程推进业务，又能顺滑地接住客户的闲聊。如果你需要一个适应性强的通用型 Agent，它们值得优先考虑。</li><li><strong>严谨的“执行者”——</strong> 流程合规性得分高但交互能力相对低一些。就像一个处理金融业务、一丝不苟的银行柜员，绝不随意发挥，但也绝不出错。对于合规性要求极高的严肃场景，它是安全的选择。</li><li><strong>温情的“倾听者”——</strong> 在交互体验上表现优异，极善于安抚沟通，提供情绪价值。如果你的场景是心理咨询或陪伴，它可能比那些“死磕流程”的模型更懂用户的心。</li></ul><p>不仅在外呼场景，随着 Voice Agent 越来越多地走向 AIoT、情感陪伴等日常生活场景，对交互的评测，也正在从“是否听清需求、是否能顺畅对话”，延伸到更底层的环境与语境理解能力。</p><p>在这一层面上，评测维度将不可避免地扩展到对掌声、敲门声等声学事件的感知，对所处环境的声学场景判断，以及对方言、间接表达和语境变化的识别。这些能力决定的，不只是一次对话能否完成，而是 Voice Agent 是否具备在真实环境中持续交互的基础条件。</p><h2>共同的目标：从探索走向落地</h2><p>这套评测体系的发布，其意义不在于分出高下，而在于展示了 Voice Agent 进化的必经之路：<strong>场景 + 技术的双重融合</strong>。</p><ul><li><strong>场景上：</strong> 评测设计基于美团外呼业务中长期积累的真实场景经验与典型问题抽象而来，使得测试不再停留在理想化设定中，而是带有明显的“泥土味”。</li><li><strong>技术上：</strong> 通过声网的音视频技术积累和架构支持，验证了一套可复用的“生产级”技术栈。</li></ul><p>对于整个开发者社区而言，这传达了两个积极的信号：</p><ol><li><strong>选型更从容：</strong> 我们不必再盲目追求“最强”模型，而是可以根据业务需求（是重逻辑还是重体验）找到最匹配的那一块拼图。</li><li><strong>研发更聚焦：</strong> 开发者不必重复造轮子，可以将宝贵的精力投入到对业务逻辑的打磨上。</li></ol><h2>结语：共建行业的“度量衡”</h2><p>AI 的进化速度太快，单打独斗的时代已经过去。</p><p>我们解读这篇论文，是希望所有 Voice Agent 的从业者关注这种“场景化测评”的趋势。VoiceAgentEval 给出了外呼场景的一种答案，更像是一次示范：如何把一个具体业务，拆解成可被复用的评测单元。</p><p>当 Evals 从“纸上谈兵”回归到“实战演练”，当底层的实时交互框架逐步成熟，Voice Agent 才有可能真正走出实验室，接受千行百业的复杂检验。这扇门是否能被真正推开，最终取决于行业能否持续围绕具体场景，持续形成可被复用、可被讨论、也可被不断修正的共同度量。</p><p>参考链接</p><p>xbench 官网： <br/><a href="https://link.segmentfault.com/?enc=TnbJ0NY8azxf1kONRnu7vQ%3D%3D.KhJbJxZluNEzjvpsqHpLsOT%2BlcgqYC2N3UJeJBwvS9wuTaLpslllTEWnzO11G2EU" rel="nofollow" target="_blank">https://xbench.org/VoiceAgentEval</a> </p><p>新闻稿：<br/><a href="https://link.segmentfault.com/?enc=8vFuhUkgC2UgSsIIOpq0%2BQ%3D%3D.77X%2FGy1OtpeDB5QbfTs20B4Q3zEHACH7AMNWjxqTWcIFjmLK%2FAdlAEKqnCJdl2v5cbOPozfyK%2BJ0XXzSiNByvg%3D%3D" rel="nofollow" target="_blank">https://xbench.org/reports/zmbbhdtfc5ui5qx5xjgquusj</a></p><p>声网对话式 AI 引擎：<br/><a href="https://link.segmentfault.com/?enc=dKSgoASr8dqHxWjQzBxXpQ%3D%3D.2HMfesnI6pNH4Hl4aMpMaLv%2FuCOEy%2Bbo5y0FHTrRpF3575VdUJzWzfW4uxMMgzQX" rel="nofollow" target="_blank">https://www.shengwang.cn/ConversationalAI/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605656" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605657" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=RoIq7JDyMEjvrDxGFeQ3Bg%3D%3D.UfWQCU8M4Mo0n752cXfeqq%2FH2Op6fmAITv0IsXWIg8o%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605658" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[一款中后台方向的低代码可视化搭建平台 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047605672</link>    <guid>https://segmentfault.com/a/1190000047605672</guid>    <pubDate>2026-02-11 16:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><p>在如今快速迭代的软件开发环境中，如何提升前端开发效率、降低重复劳动，成为许多团队关注的核心问题。低代码平台正是在这种背景下应运而生，它通过可视化拖拽、逻辑编排等方式，让开发者甚至非技术人员也能快速搭建出功能完整的页面或系统。织信Informat 正是这样一款专注于中后台场景的低代码可视化搭建平台，它不仅支持免费体验，还提供了高度灵活的扩展能力，真正做到了"让搭建更简单，让开发更高效"。</p><h2>项目介绍</h2><p>织信Informat 由前平安项目交付（后出来创业）团队打造，目标用户主要是面向企业内部管理系统、运营平台、数据看板等中后台（如ERP/MES/SRM）应用场景的开发者。平台支持从零开始创建项目、页面和组件，并通过图形化界面完成复杂的交互逻辑与接口对接。值得一提的是，织信Informat 不仅可以本地部署使用，还能通过微前端框架轻松嵌入到已有的 Vue 或 React 项目中，极大降低了技术栈迁移的成本。</p><h2>项目功能</h2><p>1、项目管理：支持主题色、菜单布局、系统 Logo、面包屑等基础配置，并内置完整的 RBAC（基于角色的访问控制）权限体系。</p><p>2、页面搭建：提供可视化拖拽编辑器，支持页面主题设置、组件布局、样式配置、事件流编排及接口调用。</p><p>3、权限控制：细化到项目、页面、菜单乃至按钮级别的权限分配，确保不同角色看到的内容和可执行的操作精准可控。</p><p>4、自定义组件：当平台内置的 1000+ 组件无法满足需求时，开发者可上传自研组件，平台在线编译后即可在编辑器中使用。</p><p>5、接口管理：统一维护 API，支持 GET/POST/PUT/DELETE 等请求方式，可配置全局拦截器、动态参数传递及返回结构处理。</p><p>6、事件流引擎：通过图形化逻辑编排，实现组件联动、显隐控制、禁用状态切换、路由跳转、接口调用等复杂业务逻辑。</p><p>7、多环境发布：支持 STG（测试）、PRE（预发）、PRD（生产）三套环境，页面需发布后才对外可见。</p><p>8、版本回滚：已发布页面支持一键回滚至上一版本，保障上线稳定性。</p><p>9、微服务集成：通过微前端方案，可将页面无缝嵌入传统 Vue/React 项目中。</p><h2>项目特点</h2><p>开箱即用：提供完整中后台解决方案，无需从零搭建基础架构。</p><p>灵活集成：既可作为独立系统使用，也可作为子应用嵌入现有工程。</p><p>权限精细：RBAC 模型覆盖项目、页面、操作各层级。</p><p>逻辑可视化：事件流机制让复杂交互不再依赖硬编码。</p><h2>项目技术</h2><p><strong>前端：</strong></p><p>基础UI库选型：Vue</p><p>基础UI库选型：Element-UI</p><p>开发语⾔标准：使⽤ES5、ES6、ES7语⾔标准</p><p>语⾔规范检查：使⽤eslint对代码进⾏检查</p><p>⼯程依赖管理：使⽤npm管理⼯程依赖</p><p>⼯程打包⽅式：使⽤Webpack4</p><p>浏览器兼容控：使⽤babel7，将ES6、ES7语法转换为ES5交付，postcss进⾏浏览器⾃动样式兼容</p><p><strong>后端：</strong></p><p>开发语⾔选型：JAVA(jdk11)</p><p>基础框架选型：SpringBoot2</p><p>数据库：Postgres13或以上</p><p>缓存：Redis 5</p><p>文件存储服务：支持符合 S3 标准的文件对象服务（如：腾讯云 COS、阿里云 OSS、Amazon S3、Minio等）</p><p>消息队列服务：RabbitMQ</p><p>服务器监控：SpringBoot Admin</p><p><strong>项目目录清晰划分为：</strong></p><p>项目访问端（用户侧）</p><p>可视化编辑器（开发侧）</p><p>内置组件物料库</p><h2>项目体验</h2><p>在线体验地址：<a href="https://link.segmentfault.com/?enc=PUm2emIqhPYqIZJc3ZD6dQ%3D%3D.cNb6ui2rhCUEbq4jc9OdMk6y6zQq6ILXFjqzDWoE%2FSeTwaJxdtJKycKxHcwSOCR9" rel="nofollow" target="_blank">https://demo.informat.cn/workbench/app</a>，也开放了产品文档：<a href="https://link.segmentfault.com/?enc=%2Bo22Cno2BzftnKAbZMkL0w%3D%3D.aHJQPTAqbD8SxxGwL%2B7YRoeizXThqnMsifBZx%2FggXh0kt%2BohLnvPsAYm%2BfIQpXrQ" rel="nofollow" target="_blank">https://next.informat.cn/doc/index.html</a>，方便大家快速上手。</p><h2>项目效果</h2><p>实际使用 织信Informat 搭建页面的体验相当流畅。无论是简单的表单页还是包含多组件联动的数据看板，都能在几分钟内完成原型搭建。配合其强大的事件流和接口配置能力，很多原本需要前后端联调的功能，现在前端即可独立闭环实现。</p><h2>项目部署说明</h2><p><strong>部署逻辑图</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605674" alt="image.png" title="image.png"/></p><p><strong>浏览器支持</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605675" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>安装所需的服务器和组件</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605676" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>服务器推荐配置</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605677" alt="image.png" title="image.png" loading="lazy"/></p><p>提示：超过1000并发的需求，按照200~1000的配置倍增。上述配置中的数据盘大小可根据实际业务存储的数据量调整。</p><p><strong>license和部署密钥</strong></p><p>在进行私有化部署之前需要申请部署密钥，部署密钥会绑定服务器的MAC地址，更换服务器后需要重新申请。在系统安装成功后，使用部署密钥作为密码登录织信企业级后台。在企业级后台中使用license可创建团队。license中会限制团队的名称、创建应用数量、成员数量、到期时间等信息。</p><p><strong>总结</strong></p><p>织信Informat 并没有盲目追求"零代码"，而是聚焦于"低代码 + 高扩展"的平衡点——在大幅减少重复性工作的同时，保留了专业开发者的控制力和灵活性。</p><p>对于正在构建或重构中后台系统的团队而言，它既能加速 MVP 验证，也能支撑长期业务演进。随着专业版逐步上线图片云、数字大屏、工作流等高级能力，织信的生态价值将进一步凸显。</p>]]></description></item><item>    <title><![CDATA[从政务系统到金融核心 核心优势凸显 JoySSL剖析国密SSL证书在高需求场景中的不可替代性 完美的]]></title>    <link>https://segmentfault.com/a/1190000047605683</link>    <guid>https://segmentfault.com/a/1190000047605683</guid>    <pubDate>2026-02-11 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当前全球网络空间竞争格局正在加速演变，密码技术作为保障网络与信息安全的核心支柱和关键基础，其自主可控性已被提升至国家战略层面。国内独立研发的商用密码算法体系，正是这种战略思维的具体体现。在此等背景下，基于SM2算法开发的国密SSL证书，正在从满足特定合规需求的单一技术方案，转变为数字化转型安全保障的关键基石，同时支撑自主可控的网络信任体系建设。JoySSL技术总监指出，国密SSL证书的推广与应用不仅是技术选择，更是对国家信息安全、产业保障以及数据主权的积极践行。国密证书的核心价值在于，能够为具备高安全性、强监管要求以及自主控制需求的场景，带来一整套兼容国内密码法规、性能优越且具有自主信任根的端到端的安全通信解决方案。</p><p><img width="723" height="479" referrerpolicy="no-referrer" src="/img/bVdnUzx" alt="" title=""/></p><p><strong>核心优势 国密证书展现多重战略价值</strong></p><p>国密SSL证书采用SM2算法替代国际通行的RSA/ECC、SHA-256和AES算法，形成了多方面战略优势。首先，自主构建的国家密码信任体系，其信任链完全依托国内自主创建的数字证书根信任体系，避免了对国外密码技术和根证书系统的依赖。</p><p>自主研发的国密证书拥有更高安全性能的算法优势，特别适用于高并发场景或资源有限的移动终端环境，算法均由国家密码管理局设计并正式认可，安全性可满足当前及未来阶段应对高强度计算攻击的需求。同时，国密SSL证书已深入兼容主流浏览器、与各种软硬件设备，具备生态适配优势。</p><p><img width="723" height="477" referrerpolicy="no-referrer" src="/img/bVdnUzy" alt="" title="" loading="lazy"/></p><p><strong>应用场景 关键领域的不可替代与前景</strong></p><p>国密SSL证书多应用于政务及公共服务领域，包括相关部门官方网站、在线政务服务平台等，这些平台需要全面应用国密算法，确保通信加密及身份认证的安全性，实现跨部门数据共享，加强数据传输安全性和管控能力。</p><p>网上银行、移动金融以及供应链平台等金融业具体场景，是推广国密算法的核心地带，旨在保障交易数据的安全与合规性。金融行业对通信安全要求极高，国密证书提供了符合监管要求的国产自主解决方案。</p><p>特定的商业领域在运营时，往往涉及大量敏感个人信息，通过国密算法的部署，既满足了国家合规要求，同时展现了更高等级的安全保障能力。</p><p><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdnUzz" alt="" title="" loading="lazy"/></p><p><strong>解决方案 自主可控与全球兼容双向发展</strong></p><p>面对不同群体多元化需求，国密SSL证书不仅需要专业，同时配备还需灵活，方能适应市场需求。JoySSL以双证书部署为解决方案，既满足国密改造要求，又符合用户日常访问需求，确保证书符合主流浏览器与设备的兼容，实现安全合规与用户体验双向平衡。</p><p><strong>创新之道 以算法构建自主可控安全体系</strong></p><p>国密SSL证书的普及与应用，是国内构建网络安全自主体系的重要举措，不仅是算法技术层面的突破，更确保了在数据安全领域的话语权。随着数字化时代的到来，国密SSL证书将在更多重要领域发挥关键作用。选择国密证书，亦是顺应未来发展的重要抉择。</p>]]></description></item><item>    <title><![CDATA[不会写提示词？这个神器让我5分钟做出贪吃蛇 程序员小崔日记 ]]></title>    <link>https://segmentfault.com/a/1190000047605296</link>    <guid>https://segmentfault.com/a/1190000047605296</guid>    <pubDate>2026-02-11 15:08:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>🚀 从一句"做个贪吃蛇"，到完整游戏代码</h2><h3>------ PromptPilot 实战体验</h3><p>最近在研究提示词工程（Prompt Engineering），发现一个挺有意思的工具：</p><p>👉 <strong>PromptPilot（火山引擎出品）</strong></p><p>官网地址：\<br/><a href="https://link.segmentfault.com/?enc=rbG6yrjm6XA%2BjowxVA8dGA%3D%3D.h%2BBZzUD6mlmN%2BejGwn7Pb1JBBUbyE6yvh51nYm4dTk7l0KGd2DVHKtRtTszp3OZG" rel="nofollow" target="_blank">https://promptpilot.volcengine.com/</a></p><p>它的定位很明确：</p><blockquote>把模糊需求，转化为结构化、可执行的 Prompt。</blockquote><p>听起来有点抽象？</p><p>那我们直接实战。</p><hr/><h2>🎮 案例：做一个贪吃蛇小游戏</h2><p>我只输入了一句话：</p><blockquote>我想做一个贪吃蛇游戏。</blockquote><p>看看它能帮我优化到什么程度。</p><hr/><h3>第一步：注册登录</h3><p>进入官网，注册登录即可。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605299" alt="" title=""/></p><p>整体界面非常干净，没有复杂引导，上手成本很低。</p><hr/><h3>第二步：输入原始需求</h3><p>输入：</p><blockquote>我想做一个贪吃蛇游戏。</blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605300" alt="" title="" loading="lazy"/></p><p>是不是很随意？\<br/>对，故意的。</p><p>因为我们想测试：</p><p>👉 <strong>模糊需求能被优化到什么程度？</strong></p><hr/><h3>第三步：生成优化后的 Prompt</h3><p>点击生成后，PromptPilot 会：</p><ul><li>拆解任务</li><li>明确约束</li><li>增加输出格式要求</li><li>添加结构化变量</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605301" alt="" title="" loading="lazy"/></p><p>你会发现：</p><p>从一句简单需求\<br/>变成了一段完整、可执行、逻辑清晰的提示词。</p><p>这一步，本质上是在做：</p><blockquote>"提示词工程结构化改写"</blockquote><hr/><h3>第四步：验证 Prompt（评分模式）</h3><p>点击：</p><p>👉 验证 Prompt\<br/>👉 选择评分模式</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605302" alt="" title="" loading="lazy"/></p><p>系统会从多个维度评分：</p><ul><li>清晰度</li><li>完整度</li><li>可执行性</li><li>逻辑严谨性</li></ul><p>这一点对新手特别友好。</p><p>因为大多数人根本不知道：</p><p>👉 自己写的 Prompt 到底好不好。</p><hr/><h3>第五步：自动填充变量</h3><p>点击自动生成变量。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605303" alt="" title="" loading="lazy"/></p><p>⚠️ 右侧默认模型是豆包。</p><p>我个人体验一般，所以我主要是拿优化后的 Prompt，复制出来用在别的模型上。</p><p>关键点在这里：</p><blockquote>PromptPilot 的核心价值是"提示词优化"，不是最终输出。</blockquote><hr/><h3>第六步：把 Prompt 给编程工具</h3><p>我使用的是 ChatGPT。</p><p>把优化后的 Prompt 直接粘贴进去。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605304" alt="" title="" loading="lazy"/></p><p>输出结果明显比直接说"做个贪吃蛇"要规范很多：</p><ul><li>有完整逻辑</li><li>有异常处理</li><li>有游戏循环结构</li><li>有键盘控制</li><li>有碰撞检测</li></ul><p>代码结构也更清晰。</p><hr/><h3>第七步：在 PyCharm 运行</h3><p>复制代码 → PyCharm → 运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605305" alt="" title="" loading="lazy"/></p><p>成功运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047605306" alt="" title="" loading="lazy"/></p><p>当然，中间还是有调试。</p><p>⚠️ 重点：</p><blockquote>好的 Prompt ≠ 一次成功\<br/>但它能显著减少无效修改</blockquote><hr/><h2>🧠 PromptPilot 到底解决了什么？</h2><p>很多人误解提示词工程。</p><p>他们以为只是：</p><blockquote>多写一点字</blockquote><p>其实真正的问题是：</p><ul><li>需求是否拆解清晰？</li><li>输出格式是否约束？</li><li>是否定义边界条件？</li><li>是否指定技术栈？</li><li>是否说明异常处理？</li></ul><p>PromptPilot 做的事情，本质上是：</p><blockquote>把"人脑里的隐性需求"显性化。</blockquote><p>这一步对于企业尤其重要。</p><p>因为企业场景往往是：</p><ul><li>需求复杂</li><li>输出必须可控</li><li>结果要稳定</li></ul><hr/><h2>🎯 它适合谁？</h2><p>✅ 提示词新手\<br/>✅ 想提高AI输出质量的人\<br/>✅ 企业内部做AI落地的团队\<br/>✅ 做自动化流程的人</p><p>不太适合：</p><p>❌ 只想随便问问问题的用户</p><hr/><h2>🔥 个人体验总结</h2><p><strong>优点：</strong></p><ul><li>上手简单</li><li>逻辑清晰</li><li>结构化强</li><li>适合新手理解 Prompt 逻辑</li></ul><p><strong>缺点：</strong></p><ul><li>默认模型输出一般</li><li>更适合作为"Prompt生成器"，而非最终执行模型</li></ul><p>我的使用方式是：</p><blockquote>用它生成高质量 Prompt\<br/>再交给更强的模型执行</blockquote><p>效果确实更稳。</p><hr/><h2>🧩 一个思考</h2><p>未来可能不是：</p><blockquote>"谁的模型更强"</blockquote><p>而是：</p><blockquote>谁的提示词工程体系更成熟。</blockquote><p>当 AI 成为工具，</p><p>Prompt 就变成了"生产力放大器"。</p><hr/><h2>📦 想要源码？</h2><p>这个贪吃蛇小游戏源码可以直接运行。</p><p>想玩玩的可以私信我。</p><hr/><p>如果你最近也在研究：</p><ul><li>AI写代码</li><li>提示词优化</li><li>企业AI落地</li></ul><p>可以试试这个工具。</p><p>也欢迎留言交流你踩过的坑 👇</p><p>本文由<a href="https://link.segmentfault.com/?enc=GwZoH9CdxmKUKQ5%2B9IDFEg%3D%3D.5IxGoMT7PnY%2ByAztlQDcLMUAie554q5V72%2FF%2BlvLbl0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[聊聊编程里的“魔法棒”：取余运算（Modulo） target丶 ]]></title>    <link>https://segmentfault.com/a/1190000047605411</link>    <guid>https://segmentfault.com/a/1190000047605411</guid>    <pubDate>2026-02-11 15:08:04</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>💡 <strong>写在前面</strong>：<br/>最近面试被问到一个倒计时相关问题，又一次用到了取余（Modulo）。说实话，刚入行那会儿，总觉得这玩意儿不就是小学数学里的<code>求余数</code><br/>吗？除了面试题里用来判断奇偶数，平时好像也没啥大用。</p><p>但随着代码写得越来越多，逐渐发现 <code>%</code> 符号背后其实隐藏着一种处理数据的<strong>思维模型</strong>——它能把无限延伸的线性世界，折叠成有限可控的<br/><strong>周期世界</strong>。今天想和大家分享一下我对取余的重新思考，看看它是怎么帮我们优雅地解决那些头疼的边界问题。</p></blockquote><h2>重新认识 <code>%</code></h2><p>取余的本质，是将任意数值强行<code>限定</code>在一个固定的循环范围内。无论数字跑多远，<code>% N</code> 都能让它回归到 <code>0</code> 至 <code>N-1</code> 的闭环中。</p><p>在教科书里，取余的公式是 <code>a % n = r</code></p><ul><li><code>a</code>：被除数</li><li><code>n</code>：除数</li><li><code>r</code>：余数</li></ul><p>但在代码逻辑里，我更愿意把它理解为两个超级好用的思维模型：</p><h3>🔄 循环</h3><p>想象一下家里的挂钟。不管时间怎么流逝，时针转了一圈又一圈，它永远只会停在 <code>1</code> 到 <code>12</code> 之间。取余就是这个<strong>表盘</strong><br/>，它能让无限增长的数字，乖乖地在一个固定的"圈"里打转。</p><h3>✂️ 限制</h3><p>无论你给我的数字有多大，<code>% n</code> 就像一把剪刀，强行把多出来的部分剪掉，只保留 <code>0</code> 到 <code>n-1</code> 这一小段。</p><p>就可以理解为：</p><ul><li><code>a</code>：被除数（任意数值）</li><li><code>n</code>：除数（限定的范围大小，也就是"表盘"的大小）</li><li><code>r</code>：余数（结果永远在 <code>0</code> 到 <code>n-1</code> 之间）</li></ul><h2>特点</h2><h3>构建"周期闭环"</h3><p>说白了就是让数字一直在一个圈里转，永远跑不出去。比如轮播图或红绿灯，写 <code>if (index &gt;= length)</code> 来防止数组越界，写多了特别烦。</p><p>有了取余，这事儿就简单了：</p><pre><code class="js">// 不管 index 涨到几万，结果永远锁死在 0 到 length-1 之间
const safeIndex = index % list.length;</code></pre><h3>降维与坐标映射</h3><p>这个主要解决"一维变二维"的问题。比如为了省流量，后端扔过来一个长长的一维数组，你需要在界面上画个九宫格。</p><p>别傻乎乎地去搞双层循环，直接用数学搞定。假设一行有 <code>col</code> 列：</p><ul><li><strong>找列号（X轴）</strong>：看它在当前行走了几步 -&gt; <strong>取余</strong> (<code>% col</code>)</li><li><strong>找行号（Y轴）</strong>：看它已经填满了几行 -&gt; <strong>整除</strong> (<code>Math.floor(i / col)</code>)</li></ul><pre><code class="js">// 假设数组索引 i=7，一行3个 (col=3)
const x = 7 % 3;                // 1 （第2列）
const y = Math.floor(7 / 3);    // 2 （第3行）

// 坐标就是 (1, 2)</code></pre><h3>均匀离散与分流</h3><p>一大堆随机数据（比如 1000 万个用户 ID），把它们公平地分给 3 台服务器，怎么分最匀称？</p><p>别搞什么复杂的随机算法，直接按 ID 取余。这不仅分得匀，还能保证同一个用户每次都能分到同一台机器上（这在分布式里叫 Hash 一致性）。</p><ul><li><strong>数字 ID</strong>：直接取余。</li><li><strong>字符串 ID</strong>：先算 Hash 值（转成数字），再取余。</li></ul><pre><code class="js">// 简单又高效的负载均衡
const targetServer = servers[userId % 3];

// 如果是字符串 ID，就先转成数字（Hash）
// const hash = stringToNumber(userId); 
// const targetServer = servers[hash % 3];</code></pre><h3>声明式逻辑</h3><p>代码是写给人看的。<code>if-else</code> 是告诉机器"怎么做流程控制"，而 <code>%</code> 是告诉人"这里是个循环"。</p><p>用 <code>%</code> 最大的好处就是——你再也不会把 <code>&gt;</code> 误写成 <code>&gt;=</code> 了。那种差 1 的 Bug（Off-by-one error），写过代码的都懂有多坑。</p><h3>倍数与规律捕捉</h3><p>想每隔 10 行打个日志？或者给表格弄个"斑马纹"（奇偶变色）？</p><p>这种"每隔 N 次搞点事情"的逻辑，用取余是最直观的。它就像个节拍器，到了那个点就会响。</p><pre><code class="js">// 经典的斑马纹逻辑
const color = index % 2 === 0 ? 'white' : 'gray';</code></pre><h2>常见的面试题（由简到难）</h2><h3>1. 秒转时分秒（倒计时）</h3><p><strong>问</strong>：给你一个总秒数 <code>3661</code>，怎么在页面上显示 <code>01:01:01</code>？</p><p><strong>答</strong>：这是最基础的"进制转换"题。</p><ul><li><strong>低位（秒）</strong>：总秒数对 60 取余 -&gt; 剩下的零头就是秒。</li><li><strong>中位（分）</strong>：总秒数先除以 60 得到总分钟数，再对 60 取余 -&gt; 剩下的零头就是分。</li><li><strong>高位（时）</strong>：总分钟数除以 60 -&gt; 剩下的就是时。</li></ul><pre><code class="js">const totalSeconds = 3661;

const seconds = totalSeconds % 60;            // 1
const minutes = Math.floor(totalSeconds / 60) % 60; // 61 % 60 = 1
const hours = Math.floor(totalSeconds / 3600);      // 1

const format = time =&gt; time.toString().padStart(2, '0');
console.log(`${format(hours)}:${format(minutes)}:${format(seconds)}`); // 01:01:01</code></pre><h3>2. 判断质数（Prime Number）</h3><p><strong>问</strong>：怎么判断一个数 <code>n</code> 是不是质数？</p><p><strong>答</strong>：质数就是只能被 1 和它自己整除的数。</p><p>所以，拿 2 到 n-1 之间的所有数去试着除它。只要有一个能被整除（<code>n % i === 0</code>），它就不是质数。</p><p><strong>优化点</strong>：其实只需要试到 <code>Math.sqrt(n)</code> 就够了，后面都是重复的。</p><blockquote><p><strong>为什么？</strong> 因子都是成对出现的。比如 <code>36</code>：</p><ul><li><code>2 × 18</code></li><li><code>3 × 12</code></li><li><code>4 × 9</code></li><li><code>6 × 6</code> (根号 n)</li><li><code>9 × 4</code> (重复了！)</li></ul><p>只要在 <code>6</code> (根号 n) 之前没找到因子，后面也绝不会有（除非是它自己）。同理 <code>100</code> 的根号是 <code>10</code>，你只要试到 <code>10</code><br/>就行了，不用傻乎乎试到 <code>99</code>。</p></blockquote><pre><code class="js">function isPrime(n) {
  if (n &lt;= 1) return false;
  if (n === 2) return true;      // 2 是质数
  if (n % 2 === 0) return false; // 偶数直接排除

  // 只需要试除奇数，步长为 2
  for (let i = 3; i &lt;= Math.sqrt(n); i += 2) {
    if (n % i === 0) return false;
  }
  return true;
}</code></pre><h3>3. 判断回文数（不转字符串）</h3><p><strong>问</strong>：给你个数字 <code>12321</code>，怎么判断它是回文？不许转成 String。</p><p><strong>答</strong>：这题考的是数字拆解的基本功。</p><p>你需要理解 <code>%</code> 和 <code>/</code> 在十进制里的<strong>黄金搭档</strong>关系：</p><ul><li><b><code>% 10</code> 是"拿"</b>：拿到个位数（剥洋葱的第一层）。</li><li><b><code>/ 10</code> 是"扔"</b>：扔掉个位数（把洋葱缩小一圈）。</li></ul><p><strong>一边拆，一边装</strong>：<br/>把 <code>x</code> 的屁股（最后一位）拆下来，装到 <code>reversed</code> 的头上。如果装完发现 <code>reversed === x</code>，那就是回文。</p><pre><code class="js">let x = 12321, reversed = 0;
// 假设 x=123
// 第一轮：123 % 10 = 3 (拿3), 123 / 10 = 12 (剩12)
// 第二轮：12 % 10 = 2 (拿2), 12 / 10 = 1 (剩1)
// 第三轮：1 % 10 = 1 (拿1), 1 / 10 = 0 (剩0) -&gt; 结束
while (x &gt; 0) {
  reversed = reversed * 10 + x % 10; // 拼到新数末尾
  x = Math.floor(x / 10);            // 原数去掉末尾
}</code></pre><h3>4. 负数取余的坑（JS vs 其他语言）</h3><p><strong>问</strong>：<code>(-1) % 5</code> 在 JS 里等于多少？在 Python 里呢？</p><p><strong>答</strong>：这题特容易踩坑。</p><ul><li>在 JS（C/Java）里，结果是 <code>-1</code>。因为它们看重"商"向 0 取整。</li><li>在 Python 里，结果是 <code>4</code>。因为 Python 看重"商"向下取整。</li></ul><p><strong>实战解法</strong>：</p><p>如果在 JS 做轮播图（点击上一张），算出 <code>-1</code> 程序就崩了。</p><p>记住这个<strong>万能公式</strong>，不管正负都能转正：</p><pre><code class="js">const index = (current + step + length) % length;</code></pre><p><strong>为什么加 <code>length</code>？</strong></p><p>因为 <code>%</code> 运算在 JS 里会保留符号。假设当前是第 0 张图（current=0），你要退一张（step=-1），总共5张图（length=5）。</p><ul><li><strong>不加 length</strong>：<code>(0 + (-1)) % 5 = -1</code> ❌（不仅不对，还越界了）</li><li><strong>加 length</strong>：<code>(0 + (-1) + 5) % 5 = 4</code> ✅（这就对了，回到了最后一个）</li><li><strong>正向移动</strong>：<code>(0 + 1 + 5) % 5 = 1</code> ✅（加一圈不影响正数结果，没副作用）</li></ul><p><strong>场景举例</strong>：</p><ol><li><b>轮播图"上一张"</b>：<code>current=0, step=-1</code>。<code>(0 - 1 + 5) % 5 = 4</code> -&gt; 完美跳到最后一张。</li><li><strong>贪吃蛇穿墙</strong>：蛇头钻出左边界 <code>x=-1</code>。<code>(-1 + width) % width</code> -&gt; 瞬间从右边出来。</li><li><strong>日期计算</strong>：今天是周三 <code>3</code>，问 5 天前是周几？<code>(3 - 5 + 7) % 7 = 5</code> -&gt; 周五。不用脑补倒着数数了。</li></ol><h3>5. 不用临时变量交换两个数</h3><p><strong>问</strong>：给你两个整数 a 和 b，不许用 <code>temp</code> 变量，怎么交换它们？</p><p><strong>答</strong>：除了烂大街的位运算（异或），取余其实也能干这事儿（虽然不如位运算快，但思路很骚）。</p><p>思路是把两个数"压缩"到一个大数里，再拆出来。</p><pre><code class="js">let a = 123, b = 456;
// 假设 n 足够大，比 a 和 b 都大
const n = 1000;

// 压缩：把 b 藏在高位，a 藏在低位
a = a + b * n; // 123 + 456 * 1000 = 456123

b = a % n;        // 取出低位，也就是原来的 a 
a = Math.floor(a / n); // 取出高位，也就是原来的 b

console.log(a, b); // 456, 123</code></pre><h3>6. 约瑟夫环问题</h3><p><strong>场景描述</strong>：<br/>有 <code>n</code> 个人围成一圈（编号 0 到 n-1）。从第 0 号开始报数，报到 <code>m</code> 的人出局。下一位继续从 1 开始报数，直到只剩最后一个人。问最后这个人的原始编号是多少？</p><p><strong>例子</strong>：</p><ul><li><strong>n = 5</strong>（5个人：0, 1, 2, 3, 4）</li><li><strong>m = 3</strong>（报到3出局）</li><li><strong>出局过程</strong>：2号出局 -&gt; 0号出局 -&gt; 4号出局 -&gt; 1号出局 -&gt; <strong>3号幸存</strong>。</li><li><strong>幸存过程</strong>：0, 1, 2, 3, 4 -&gt; 0, 1, 3, 4 -&gt; 1, 3, 4 -&gt; 1, 3 -&gt; 3</li></ul><p>这道题有点复杂，先上答案，后面咱们掰开揉碎了讲</p><pre><code class="js">/**
 * @param {number} n 总人数
 * @param {number} m 报数号码（报到几出局）
 * @return {number} 最后幸存者的编号
 */
function lastRemaining(n, m) {
  let pos = 0; // 时光倒流终点：最后只剩1个人时，幸存者索引是0

  // 开始倒推：从2个人 -&gt; 3个人 -&gt; ... -&gt; n个人
  for (let i = 2; i &lt;= n; i++) {
    pos = (pos + m) % i; // 每一轮人数变多(i)，位置都要往后挪 m 位
  }
  return pos;
}</code></pre><p><strong>解法思路：时光倒流（坐标偏移）</strong></p><p>这个问题如果顺着想（模拟淘汰），数组删元素很麻烦。但如果我们<strong>倒着想</strong>，利用<strong>坐标偏移</strong>规律，就非常简单。</p><p><strong>1. 正向（淘汰 = 坐标前移）：</strong><br/>想象一下，<code>m=3</code>，第 3 个人（索引 2）被淘汰后。</p><ul><li>按照规则，<strong>下一轮报数从被淘汰者的下一个人（索引 3）开始</strong>。</li><li>这就意味着，<strong>索引 3</strong> 变成了新一轮的 <strong>排头兵（新的索引 0）</strong>。</li><li>相当于所有人整体<strong>往前挪了 3 位</strong>（注意：不仅仅是填补空缺，而是连起点都变了）。</li><li>即：<code>旧索引 - 3 = 新索引</code>。</li></ul><p><strong>2. 逆向（恢复 = 坐标后移）：</strong><br/>我们要找幸存者最初在哪，可以从<strong>终局</strong>（只剩他 1 人，索引 0）开始，一步步把时光倒流，恢复之前被淘汰的人。</p><ul><li><strong>恢复就是淘汰的逆操作</strong>。</li><li>既然淘汰是"往前挪 3 位"，那恢复就是<b>"往后挪 3 位"</b>（<code>+3</code>）。</li><li>公式呼之欲出：<code>新索引 + 3 = 旧索引</code>。</li><li><strong>核心补丁</strong>：因为是圆圈，往后挪超出了队尾就要绕回队头，所以必须 <code>% 上轮人数</code>。</li></ul><p><strong>推导过程演示（N=5, M=3）</strong>：</p><p>我们只关注<strong>最后那个幸存者</strong>（假设他叫"天选之子"），他在每一轮的索引是多少？</p><blockquote><p><strong>表头说明</strong>：</p><ul><li><strong>n</strong>：当前轮剩余人数。</li><li><strong>倒推公式</strong>：<code>(当前索引 + m) % 上轮人数</code>。通过这个公式，我们可以算出幸存者在上一轮（人数更多时）的位置。</li></ul></blockquote><table><thead><tr><th align="left">轮次</th><th align="left">剩余人数</th><th align="left">场景描述</th><th align="left">计算过程</th><th align="left">幸存者索引</th></tr></thead><tbody><tr><td align="left"><strong>终局</strong></td><td align="left">1</td><td align="left">只剩天选之子</td><td align="left">0 (固定)</td><td align="left"><strong>0</strong></td></tr><tr><td align="left"><strong>倒数第2轮</strong></td><td align="left">2</td><td align="left">恢复成2人</td><td align="left"><code>(0 + 3) % 2</code></td><td align="left"><strong>1</strong></td></tr><tr><td align="left"><strong>倒数第3轮</strong></td><td align="left">3</td><td align="left">恢复成3人</td><td align="left"><code>(1 + 3) % 3</code></td><td align="left"><strong>1</strong></td></tr><tr><td align="left"><strong>倒数第4轮</strong></td><td align="left">4</td><td align="left">恢复成4人</td><td align="left"><code>(1 + 3) % 4</code></td><td align="left"><strong>0</strong></td></tr><tr><td align="left"><strong>开局</strong></td><td align="left">5</td><td align="left">恢复成5人</td><td align="left"><code>(0 + 3) % 5</code></td><td align="left"><strong>3</strong></td></tr></tbody></table><p><strong>结论</strong>：一开始索引为 <strong>3</strong> 的那个人，就是天选之子。</p><p><strong>💡 核心疑点 Q&amp;A</strong>：</p><ol><li><p><strong>为什么要倒推？</strong></p><ul><li><strong>正推太麻烦</strong>：如果正向模拟，你需要不断地删除数组元素、处理索引越界，数组长度一直在变，计算极其复杂。</li><li><strong>终局是已知的</strong>：无论过程多复杂，<strong>最后一定只剩 1 个人</strong>，且那个人的索引一定是 <code>0</code>。从确定的结果出发找源头，比从源头去猜结果要容易得多。</li></ul></li><li><p><strong>为什么要恢复上一轮的状态？</strong></p><ul><li>这是一个<strong>递归/递推</strong>的问题。<code>5个人</code> 的游戏淘汰一个，就变成了 <code>4个人</code> 的游戏。</li><li>如果我们知道 <code>4个人</code> 里的幸存者是谁，只要把这个幸存者在 <code>4个人</code> 局里的位置，<strong>映射（还原）</strong> 回 <code>5个人</code> 局里的位置，问题就解决了。</li><li>所谓"恢复"，其实就是<strong>坐标变换</strong>。</li></ul></li><li><p><strong>为什么要 % i（当前人数），而不是 % n（总人数）？</strong></p><ul><li>这是很多人的盲点！</li><li>每一轮淘汰一个人，<strong>圈子的大小都在变</strong>。</li><li>倒数第 2 轮时，圈子只有 2 个人，所以是 <code>% 2</code>；倒数第 3 轮时，圈子有 3 个人，所以是 <code>% 3</code>。</li><li>我们是在<strong>那一轮的圈子</strong>里进行坐标恢复，当然要模<strong>那一轮的人数</strong>。</li></ul></li><li><p><strong>公式 <code>(当前索引 + m) % 上轮人数</code> 怎么来的？</strong></p><ul><li>这就是我们上面提到的<strong>坐标偏移</strong>：</li><li><strong>+ m</strong>：代表时光倒流，恢复被删掉的 <code>m</code> 个位置。</li><li><strong>% 上轮人数</strong>：代表在恢复后的圈子里转圈圈，防止索引越界。</li></ul></li></ol><p><strong>💡 小贴士：数学公式版（递归实现）</strong></p><p>如果你在算法书上看到这个公式，别慌，它和我们的代码是一回事：</p><p><code>f(n, m) = (f(n-1, m) + m) % n</code></p><ul><li><code>f(n, m)</code>：n 个人时幸存者的索引。</li><li><code>f(n-1, m)</code>：n-1 个人时幸存者的索引（也就是我们代码里的 <code>pos</code>）。</li><li>代码里的 <code>for</code> 循环，就是把这个数学递归公式变成了<strong>从 2 到 n 的递推</strong>。</li></ul><p><strong>递归版代码（仅供参考）</strong>：</p><p>虽然代码看着短，但如果 n 很大，会爆栈哦。还是推荐用上面的 <code>for</code> 循环（迭代版）。</p><pre><code class="js">function lastRemainingRecursive(n, m) {
  if (n === 1) return 0; // 剩下1个人，索引肯定是0
  return (lastRemainingRecursive(n - 1, m) + m) % n;
}</code></pre><p><strong>动态规划版（标准 DP）</strong>：</p><p>有了推导公式，自然就能写出 DP。</p><p><code>dp[i]</code> 表示 <code>i</code> 个人时的幸存者索引。</p><pre><code class="js">function lastRemainingDP(n, m) {
  let dp = new Array(n + 1);
  dp[1] = 0; // 只有1个人时，索引是0
  for (let i = 2; i &lt;= n; i++) {
    dp[i] = (dp[i - 1] + m) % i; // 状态转移方程
  }
  return dp[n];
}</code></pre><p>*注：我们最开始写的那个 <code>let pos</code> 的版本，其实就是这个 DP 版本的<strong>空间优化版</strong>（滚动数组思想），把 <code>dp</code><br/>数组压缩成了一个变量。*</p><h2>总结</h2><p>说实话，取余（Modulo）这个概念，以前我也觉得它只是个数学符号，顶多用来算算奇偶数。但当你真的深入去理解它，你会发现它其实是一种<code>化直为曲</code><br/>的思维方式。</p><p>无论是处理时间、轮播图，还是解决像约瑟夫环这样复杂的算法题，取余的核心永远只有两点：<strong>控制边界</strong>和<strong>制造循环</strong>。</p><p>希望这篇文章能帮你打破对 <code>%</code> 的固有印象。下次在代码里遇到"溢出"、"循环"或者"映射"的问题时，试着停下来想一想：这里是不是可以用取余来简化一下？</p><p>多思考，多动手，编程不仅是写代码，更是对数据规律的优雅掌控。</p><p>本文由<a href="https://link.segmentfault.com/?enc=sdF1ZM3nWKfPp%2BMLzsSWLQ%3D%3D.ZKyXysUDq6aaxqC3ut8gL4RQAHN1U%2FamNMaz090QsN0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[SmartPi 智能体平台实战：从知识库问答到设备控制的完整闭环 SmartPi ]]></title>    <link>https://segmentfault.com/a/1190000047605444</link>    <guid>https://segmentfault.com/a/1190000047605444</guid>    <pubDate>2026-02-11 15:07:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>前言</h2><p>在智能家居和语音交互产品开发中，如何让设备"懂"你的产品？如何让用户通过自然对话完成设备控制？SmartPi 智能体平台提供了一套完整的解决方案——从知识库问答（RAG）到设备控制（MCP），让开发者能够快速打造智能语音交互体验。</p><blockquote><p><strong>平台更新说明（2026）</strong>：</p><ul><li>智能体平台已升级为统一控制台，支持 API 发布和工作流编排</li><li>PAT（Personal Access Token）成为推荐鉴权方式</li><li>MCP 插件支持通过 <code>mcp_tool.yaml</code> 文件一键导入</li><li>新增对话流（Workflow）可视化编排能力</li></ul></blockquote><p>本文将带你完成一个完整的实战项目：<strong>打造一个能回答设备说明书问题，并能控制灯光亮度的智能语音助手</strong>。</p><h2>一、智能体平台架构概览</h2><p>在开始实战之前，先理解 SmartPi 智能体平台的整体架构：</p><pre><code>┌─────────────────────────────────────────────────────────────────┐
│                         用户交互层                                │
├─────────────────────────────────────────────────────────────────┤
│  语音唤醒  →  ASR识别  →  智能体对话  →  TTS播报  →  设备控制    │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      智能体平台 (云端)                            │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐           │
│  │  知识库RAG  │   │   插件/MCP  │   │  自定义LLM  │           │
│  │  (设备问答) │   │  (设备控制) │   │  (扩展能力) │           │
│  └─────────────┘   └─────────────┘   └─────────────┘           │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      SmartPi 平台 (中台)                          │
├─────────────────────────────────────────────────────────────────┤
│  固件配置  →  MCP工具生成  →  二维码绑定  →  小程序控制          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                        设备端 (本地)                              │
├─────────────────────────────────────────────────────────────────┤
│  语音模块  →  命令执行  →  GPIO控制  →  状态上报                 │
└─────────────────────────────────────────────────────────────────┘</code></pre><p><strong>核心组件说明：</strong></p><table><thead><tr><th>组件</th><th>作用</th><th>典型应用</th></tr></thead><tbody><tr><td>知识库 (RAG)</td><td>让智能体基于你提供的资料回答</td><td>设备说明书、FAQ、产品规格</td></tr><tr><td>MCP/插件</td><td>让智能体能够调用外部工具控制设备</td><td>开关控制、参数调节、状态查询</td></tr><tr><td>PAT</td><td>API 调用鉴权凭证</td><td>保护智能体接口安全</td></tr><tr><td>二维码绑定</td><td>将云端智能体与本地设备关联</td><td>一键完成配置同步</td></tr></tbody></table><h2>二、准备工作</h2><h3>2.1 必备条件</h3><p>在开始之前，请确认以下资源已准备就绪：</p><table><thead><tr><th>资源</th><th>说明</th><th>获取方式</th></tr></thead><tbody><tr><td>SmartPi 平台账号</td><td>用于配置固件和生成二维码</td><td><a href="https://link.segmentfault.com/?enc=bsRRofqpfJRmmT7PG4YEFw%3D%3D.5jhfTms4CsBTOc1Oa7VP6p0kRly8XXGvHvHBI1ZAt2M%3D" rel="nofollow" target="_blank">https://smartpi.cn</a> 注册</td></tr><tr><td>智能体平台地址</td><td>你们部署的控制台地址</td><td>由技术团队提供</td></tr><tr><td>支持智能体的设备</td><td>带 "AI 智能体" 菜单的语音模组</td><td>如 JX-A7T 等在线语音模块</td></tr><tr><td>微信小程序</td><td>"智能公元"小程序</td><td>微信搜索即可</td></tr></tbody></table><h3>2.2 检查设备是否支持智能体</h3><ol><li>打开"智能公元"微信小程序</li><li>进入设备详情页</li><li>查看是否有 <strong>"AI 智能体"</strong> 菜单</li></ol><blockquote><strong>注意</strong>：如果看不到该菜单，说明当前设备/固件不支持智能体功能，需要升级到支持在线语音的固件版本。</blockquote><h2>三、实战第一步：创建知识库问答智能体</h2><p>我们的第一个目标是：<strong>让智能体能够回答设备说明书中的问题</strong>，例如"这款设备怎么配网？"、"如何恢复出厂设置？"等。</p><h3>3.1 创建智能体</h3><ol><li>登录智能体平台控制台</li><li>进入 <strong>开发 / Development</strong> → 点击 <strong>创建 / Create</strong></li><li><p>填写基本信息：</p><ul><li><strong>名称</strong>：<code>设备说明书助手</code>（或更具体的场景名，如"客厅灯光助手"）</li><li><strong>介绍</strong>：<code>回答设备使用问题，并能控制设备</code></li><li><strong>提示词</strong>：先用最简版本</li></ul></li></ol><pre><code># 最小可用提示词（可直接复制）
你是设备的智能语音助手。
你的任务是：
1. 回答用户关于设备使用的各种问题
2. 基于知识库内容回答，找不到答案就说"这个问题我不太清楚"
3. 用简洁的中文回答，每次回答不超过50字</code></pre><h3>3.2 准备知识库文件</h3><p>知识库是智能体的"专业大脑"，让它能够回答你们产品/设备的专属问题。</p><p><strong>文件格式支持：</strong></p><ul><li>PDF、Word (<code>.docx</code>)</li><li>纯文本 (<code>.txt</code>, <code>.md</code>)</li><li>Excel/CSV (<code>.xlsx</code>, <code>.csv</code>) —— 适合 Q&amp;A 格式</li></ul><p><strong>文件准备建议：</strong></p><table><thead><tr><th>建议</th><th>说明</th></tr></thead><tbody><tr><td>文件大小</td><td>单文件控制在 5MB 以内，大文件请按章节拆分</td></tr><tr><td>内容格式</td><td>使用清晰的标题和段落结构</td></tr><tr><td>Q&amp;A 格式</td><td>关键信息推荐用问答对方式呈现，便于精准匹配</td></tr></tbody></table><p><strong>Q&amp;A 格式示例（Excel）：</strong></p><table><thead><tr><th>问题</th><th>答案</th></tr></thead><tbody><tr><td>设备如何配网？</td><td>打开小程序，点击添加设备，选择设备型号，输入 Wi-Fi 密码即可完成配网</td></tr><tr><td>如何恢复出厂设置？</td><td>长按设备上的复位按钮 5 秒，听到提示音后松开，设备将恢复出厂设置</td></tr><tr><td>设备支持哪些语音指令？</td><td>支持开关控制、亮度调节、颜色切换等指令，具体请查看产品说明书</td></tr></tbody></table><h3>3.3 创建并关联知识库</h3><ol><li>在智能体平台进入 <strong>资源库 / Resource Library</strong></li><li>创建 <strong>知识库 / Knowledge</strong>，命名为 <code>设备说明书-2025Q1</code></li><li>上传准备好的文件</li><li>等待解析完成（状态从"解析中"变为"完成"）</li><li>回到智能体编辑页，在"知识/Knowledge"区域选择刚创建的知识库</li><li>保存配置</li></ol><h3>3.4 验证知识库效果</h3><p>在智能体预览窗口测试以下三类问题：</p><table><thead><tr><th>问题类型</th><th>示例问题</th><th>预期结果</th></tr></thead><tbody><tr><td>资料里有答案的</td><td>"设备怎么配网？"</td><td>能准确回答</td></tr><tr><td>资料里有步骤的</td><td>"如何恢复出厂设置？"</td><td>能按步骤说明</td></tr><tr><td>资料里没有的</td><td>"你们公司上市了吗？"</td><td>回答"不清楚"或类似内容</td></tr></tbody></table><p>如果智能体对资料外的问题也在"胡编"，需要在提示词中加入更强的约束：</p><pre><code>## 重要限制
- 只能基于知识库内容回答
- 知识库中没有答案的问题，必须回答"这个问题我不太清楚"
- 不要猜测或编造信息</code></pre><h2>四、实战第二步：让智能体具备设备控制能力</h2><p>知识库让智能体"能答"，现在要让它"能做"。我们将通过 MCP（Model Context Protocol）工具让智能体能够控制设备。</p><h3>4.1 理解 MCP 工具</h3><p>MCP 是连接大模型与设备控制的桥梁：</p><pre><code>┌─────────────┐      语音输入      ┌─────────────┐
│    用户     │ ──────────────────→ │   智能体    │
└─────────────┘                     └─────────────┘
                                              │
                                              │ 调用 MCP 工具
                                              ↓
┌─────────────┐      工具调用      ┌─────────────┐
│   设备      │ ←────────────────── │  MCP 服务   │
│  (GPIO等)   │                     └─────────────┘
└─────────────┘</code></pre><h3>4.2 配置设备控件</h3><p>在配置 MCP 工具之前，需要先在小程序平台定义好可控制的"控件"：</p><ol><li>登录 SmartPi 平台 (smartpi.cn)</li><li>选择你的设备和固件版本</li><li>进入 <strong>控制面板 / 面板编辑</strong></li><li>添加以下控件示例：</li></ol><table><thead><tr><th>控件类型</th><th>控件 ID</th><th>说明</th></tr></thead><tbody><tr><td>开关</td><td><code>switch_light</code></td><td>控制灯光开关</td></tr><tr><td>滑块</td><td><code>slider_brightness</code></td><td>调节亮度（0-100）</td></tr><tr><td>状态显示</td><td><code>text_status</code></td><td>显示当前状态</td></tr></tbody></table><h3>4.3 生成 MCP 工具</h3><ol><li>在 SmartPi 平台进入 <strong>MCP 工具</strong> 菜单</li><li>点击 <strong>刷新</strong> 按钮，平台会自动根据已配置的控件生成工具</li><li>为每个工具补充清晰的<strong>名称</strong>和<strong>描述</strong>（这是智能体理解工具用途的关键）</li></ol><p><strong>工具描述示例：</strong></p><table><thead><tr><th>工具名称</th><th>描述</th></tr></thead><tbody><tr><td><code>控制灯光开关</code></td><td>打开或关闭灯光，参数：on=开，off=关</td></tr><tr><td><code>调节灯光亮度</code></td><td>调节灯光亮度，参数：0-100 的数值，0 为最暗，100 为最亮</td></tr><tr><td><code>查询灯光状态</code></td><td>查询灯光当前的状态，包括开关和亮度值</td></tr></tbody></table><h3>4.4 发布 MCP 工具</h3><ol><li>在固件版本发布页面，勾选 <strong>发布 MCP 工具</strong></li><li>生成并下载新固件</li><li>将固件烧录到设备</li></ol><blockquote><strong>注意</strong>：MCP 工具只有在固件发布后才会生效，修改描述后需要重新发布。</blockquote><h3>4.5 导入插件到智能体（可选方案）</h3><p>如果你的方案需要通过插件方式调用，可以按以下步骤操作：</p><ol><li>在 SmartPi 平台 MCP 工具页面，点击 <strong>预览</strong> → <strong>下载插件</strong></li><li>获得 <code>mcp_tool.yaml</code> 文件</li><li>在智能体平台 <strong>资源库</strong> 中 <strong>添加插件</strong> → <strong>导入</strong> 该文件</li><li>将所有工具设置为 <strong>启用</strong></li><li>进行 <strong>试运行</strong>，参数中 <code>token</code> 固定填 <code>Bearer test</code></li></ol><h2>五、实战第三步：发布智能体并绑定设备</h2><p>现在我们已经有了：</p><ul><li>一个能回答问题的知识库</li><li>一套能控制设备的 MCP 工具</li></ul><p>最后一步是将智能体发布为 API 服务，并绑定到具体设备。</p><h3>5.1 生成个人访问令牌 (PAT)</h3><p>PAT（Personal Access Token）是调用智能体 API 的安全凭证。</p><ol><li>在智能体平台点击左下角头像</li><li>进入 <strong>API Authorization / API 授权</strong></li><li>点击 <strong>Add New Token / 新建令牌</strong></li><li>填写名称和过期时间</li><li><strong>立即复制并保存</strong> —— PAT 只展示一次！</li></ol><p>调用 API 时需要在请求头中携带：</p><pre><code>Authorization: Bearer pat_xxxxx</code></pre><h3>5.2 发布智能体为 API 服务</h3><ol><li>在智能体页面右上角点击 <strong>发布 / Publish</strong></li><li>选择 <strong>API</strong> 发布方式</li><li><p>发布后记录两个关键信息：</p><ul><li><strong>bot\_id</strong>：浏览器地址栏 <code>bot/</code> 后的数字</li><li><strong>PAT</strong>：上一步生成的令牌</li></ul></li></ol><h3>5.3 在 SmartPi 平台创建智能体配置</h3><ol><li>打开 SmartPi 平台 (smartpi.cn)</li><li>进入 <strong>智能体 → 配置</strong></li><li><p>创建新配置并填写：</p><ul><li><strong>名称</strong>：如"客厅灯光智能体"</li><li><strong>平台选择</strong>：Coze 或其他支持的智能体平台</li><li><strong>智能体 ID（bot\_id）</strong>：从上一步复制</li><li><strong>个人访问令牌（PAT）</strong>：从上一步复制</li></ul></li><li>保存后，平台会生成一个 <strong>绑定二维码</strong></li></ol><h3>5.4 使用小程序扫码绑定</h3><ol><li>打开"智能公元"微信小程序</li><li>进入设备详情页</li><li>点击 <strong>AI 智能体</strong> 菜单</li><li>扫描上一步生成的二维码</li></ol><blockquote><strong>注意</strong>：二维码有效期为 10 分钟，超时需重新生成。</blockquote><h3>5.5 验证完整流程</h3><p>绑定成功后，可以进行端到端测试：</p><table><thead><tr><th>测试指令</th><th>预期行为</th></tr></thead><tbody><tr><td>"你好"</td><td>智能体正常回复</td></tr><tr><td>"设备怎么配网？"</td><td>基于知识库回答配网步骤</td></tr><tr><td>"把灯打开"</td><td>调用 MCP 工具，设备灯光开启</td></tr><tr><td>"把亮度调到 50"</td><td>调用 MCP 工具，亮度变为 50%</td></tr><tr><td>"现在灯什么状态？"</td><td>调用查询工具，播报当前状态</td></tr></tbody></table><h2>六、OpenAPI 调用速查</h2><p>对于需要通过代码调用智能体的场景，智能体平台提供了标准的 REST API。</p><h3>6.1 请求头配置</h3><p>所有 API 调用都需要携带以下请求头：</p><pre><code>Authorization: Bearer pat_xxxxx
Content-Type: application/json</code></pre><h3>6.2 创建会话（Conversation）</h3><p>在发起对话之前，需要先创建一个会话：</p><pre><code>curl --location '{{host}}/v1/conversation/create' \
  --header 'Authorization: Bearer pat_xxxxx' \
  --header 'Content-Type: application/json' \
  --data '{"bot_id":"&lt;bot_id&gt;"}'</code></pre><p><strong>响应示例</strong>：</p><pre><code>{
  "code": 0,
  "data": {
    "conversation_id": "conv_xxxxx",
    "created_at": 1234567890
  }
}</code></pre><h3>6.3 发起对话（Chat v3，流式 SSE）</h3><p>使用流式输出可以获得更好的用户体验：</p><pre><code>curl --location --request POST '{{host}}/v3/chat?conversation_id=&lt;conversation_id&gt;' \
  --header 'Authorization: Bearer pat_xxxxx' \
  --header 'Content-Type: application/json' \
  --data-raw '{
    "bot_id": "&lt;bot_id&gt;",
    "user_id": "&lt;your_user_id&gt;",
    "stream": true,
    "auto_save_history": true,
    "additional_messages": [
      {"role":"user","content":"你好","content_type":"text"}
    ]
  }'</code></pre><p><strong>流式事件顺序</strong>：</p><table><thead><tr><th>事件</th><th>说明</th></tr></thead><tbody><tr><td><code>conversation.chat.created</code></td><td>对话创建</td></tr><tr><td><code>conversation.chat.in_progress</code></td><td>对话进行中</td></tr><tr><td><code>conversation.message.delta</code></td><td>消息增量（流式返回内容）</td></tr><tr><td><code>conversation.message.completed</code></td><td>消息完成</td></tr><tr><td><code>conversation.chat.completed</code></td><td>对话完成</td></tr><tr><td><code>done</code></td><td>流结束</td></tr></tbody></table><h3>6.4 消息列表与清理上下文</h3><pre><code># 获取消息列表
POST {{host}}/v1/conversation/message/list?conversation_id=&lt;conversation_id&gt;

# 清理对话上下文
POST {{host}}/v1/conversations/&lt;conversation_id&gt;/clear</code></pre><h3>6.5 执行工作流（Workflow）</h3><p>如果使用对话流/工作流，可以直接执行：</p><pre><code>curl --location --request POST '{{host}}/v1/workflow/run' \
  --header 'Authorization: Bearer pat_xxxxx' \
  --header 'Content-Type: application/json' \
  --data-raw '{
    "workflow_id": "&lt;workflow_id&gt;",
    "parameters": "{\"user_id\":\"12345\"}"
  }'</code></pre><h2>七、常见问题排查</h2><h3>7.1 PAT 忘记保存怎么办？</h3><p>PAT 通常只在创建时展示一次，丢失后需要：</p><ol><li>重新生成新的 PAT</li><li>更新 SmartPi 平台中的智能体配置</li><li>重新生成二维码并绑定设备</li></ol><h3>7.2 设备端没有反应？</h3><p>按以下顺序排查：</p><table><thead><tr><th>排查项</th><th>检查方法</th></tr></thead><tbody><tr><td>菜单支持</td><td>小程序中是否有"AI 智能体"菜单</td></tr><tr><td>配置正确</td><td>bot\_id 和 PAT 是否正确（多余空格也会导致失败）</td></tr><tr><td>API 发布</td><td>智能体是否已发布为 API 服务</td></tr><tr><td>网络连接</td><td>设备是否正常联网</td></tr><tr><td>固件版本</td><td>是否烧录了包含 MCP 工具的最新固件</td></tr></tbody></table><h3>7.3 知识库回答不准确？</h3><table><thead><tr><th>问题</th><th>解决方案</th></tr></thead><tbody><tr><td>回答内容与资料不符</td><td>检查知识库文件解析是否完成</td></tr><tr><td>对资料外问题乱回答</td><td>在提示词中加入更强的约束规则</td></tr><tr><td>找不到答案</td><td>调整相似度阈值（默认 0.5，可适当降低）</td></tr></tbody></table><h3>7.4 MCP 工具调用失败？</h3><table><thead><tr><th>问题</th><th>解决方案</th></tr></thead><tbody><tr><td>工具未启用</td><td>在插件管理中确认工具已启用</td></tr><tr><td>试运行失败</td><td>确认 token 参数填写正确（<code>Bearer test</code>）</td></tr><tr><td>设备无响应</td><td>检查固件是否正确烧录，MCP 工具是否已发布</td></tr></tbody></table><h3>7.5 响应延迟过大？</h3><p>智能体对话的完整链路包括：ASR → 网络传输 → LLM 推理 → 工具调用 → 网络传输 → TTS</p><p>常见优化方向：</p><table><thead><tr><th>环节</th><th>优化建议</th></tr></thead><tbody><tr><td>网络传输</td><td>确保设备网络稳定，减少路由跳数</td></tr><tr><td>LLM 推理</td><td>使用更快的模型，或启用流式输出</td></tr><tr><td>对话流</td><td>关闭"深度思考"功能以减少响应时间</td></tr><tr><td>缓存</td><td>对常见问题配置缓存机制</td></tr></tbody></table><h2>八、进阶技巧</h2><h3>8.1 对话流/工作流 (Workflow) 应用</h3><p>对于复杂场景，可以使用对话流编排多个工具的调用顺序：</p><ol><li>在智能体平台创建 <strong>对话流 / Chatflow</strong></li><li>在开始节点定义输入变量：<code>token</code>、<code>deviceKey</code></li><li>在大模型节点关闭 <strong>深度思考</strong></li><li>添加插件节点，引用输入变量</li><li>在结束节点开启 <strong>流式输出</strong></li><li>发布对话流，获得 <code>workflow_id</code></li><li>在 SmartPi 平台智能体配置中填写该 ID</li></ol><h3>8.2 自定义大模型接入</h3><p>如果需要接入自建的大模型服务（如 VLLM、Xinference）：</p><ol><li>在智能体平台进入 <strong>模型提供商</strong> 管理</li><li>选择对应的模型类型（VLLM/Xinference）</li><li><p>填写配置参数：</p><ul><li>基础 URL：模型服务的 API 地址</li><li>API Key：认证密钥（如需要）</li><li>最大 Token：单次请求限制</li></ul></li></ol><p><strong>VLLM 部署示例：</strong></p><pre><code>docker run --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  --name vllm-server \
  vllm/vllm-openai \
  --model your-model-name \
  --trust-remote-code</code></pre><h3>8.3 方言支持</h3><p>通过自建 GPU 服务器部署方言识别和 TTS 合成：</p><table><thead><tr><th>方言</th><th>支持情况</th></tr></thead><tbody><tr><td>粤语</td><td>支持 ASR 和 TTS</td></tr><tr><td>上海话</td><td>支持 ASR 和 TTS</td></tr><tr><td>四川话</td><td>支持 ASR 和 TTS</td></tr></tbody></table><h2>九、总结</h2><p>通过本文的实战演练，我们完成了一个完整的智能体开发流程：</p><pre><code>知识库配置 → MCP 工具生成 → API 发布 → 设备绑定 → 端到端测试</code></pre><p><strong>关键要点回顾：</strong></p><table><thead><tr><th>要点</th><th>说明</th></tr></thead><tbody><tr><td>知识库</td><td>是智能体的专业大脑，用 Q&amp;A 格式效果最佳</td></tr><tr><td>MCP 工具</td><td>是连接 AI 与设备的桥梁，描述要清晰准确</td></tr><tr><td>PAT</td><td>只展示一次，务必妥善保存</td></tr><tr><td>二维码绑定</td><td>有效期 10 分钟，需要提前准备设备</td></tr><tr><td>提示词优化</td><td>根据实际效果持续迭代，加入约束规则</td></tr></tbody></table><p><strong>下一步学习建议：</strong></p><ol><li>掌握提示词工程，优化智能体的回复质量</li><li>学习对话流编排，处理复杂的多轮对话场景</li><li>了解自定义模型接入，部署专属的大模型服务</li></ol><h2>参考资源</h2><table><thead><tr><th>资源名称</th><th>链接</th></tr></thead><tbody><tr><td>SmartPi 平台</td><td><a href="https://link.segmentfault.com/?enc=MA2LTu8gcfjQWLVicenCZQ%3D%3D.fXKPkBoFnpKviH2vYO5F7EdXtqE9Z2XSfJ0FcvRU5J4%3D" rel="nofollow" target="_blank">https://smartpi.cn</a></td></tr><tr><td>智能体平台快速开始</td><td>官方文档 /ai-agents/get-started</td></tr><tr><td>知识库配置指南</td><td>官方文档 /ai-agents/knowledge-base-setup</td></tr><tr><td>智能体控制台指南</td><td>官方文档 /ai-agents/platform-guide</td></tr><tr><td>智能体实践教程</td><td>官方文档 /ai-agents/tutorial</td></tr></tbody></table><p><em>本文档基于 SmartPi 官方文档整理，涵盖智能体平台的核心功能和实战操作。</em></p>]]></description></item><item>    <title><![CDATA[RAGFlow x OceanBase seekdb: AI 原生数据库驱动智能体落地 OceanB]]></title>    <link>https://segmentfault.com/a/1190000047605505</link>    <guid>https://segmentfault.com/a/1190000047605505</guid>    <pubDate>2026-02-11 15:06:31</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>摘要：</strong><br/><em>随着Agent技术发展，RAG正从文档检索工具演进为支撑智能体的统一数据底座。RAGFlow提出通过“树图结合”模拟人类认知，以Context Engine统一处理三类核心数据，要求底层数据库具备强大的混合检索与高性能交互能力。OceanBase seekdb作为AI原生数据库，以向量检索、自定义分词及轻量部署等特性，为构建此类数据底座提供了关键技术支撑，推动Agent规模化落地阶段。</em></p><h2>01 前言</h2><p>在生成式 AI 迈向 Agent 时代的当下，RAG 技术正经历着一场深刻的范式演进。</p><p>RAGFlow 联合创始人张颖峰指出，RAG 不仅仅是水上雕花的展示工具，他应当成为智能体的数据底座。</p><p>本文基于 RAGFlow 的架构思路，并结合 OceanBase seekdb 的实践，介绍如何构建一个落地快的智能体数据底座。</p><h2>02 RAGFlow 的架构理念：重新定义 AI 时代的数据底座</h2><h4>RAG 不是终点，而是起点</h4><p>在 AI 应用开发中，开发者们普遍遇到了 RAG 效果难以提升的困境，仿佛按下葫芦浮起瓢，总有顾此失彼的感觉。尽管到 2024 年，使用多模态模型解析文档、采用混合搜索等实践已经成为共识，但效果仍然不尽如人意。问题的根源在哪里？</p><h4>树图结合：让检索像人一样思考</h4><p>RAGFlow 团队给出的答案是"树图结合"。这个方案的核心思想是让 RAG 系统像人一样去检索信息。当领导向你提出一个问题时，你不会在脑海中随机搜索碎片化的知识，而是会根据记忆中的目录结构，找到对应的文献，然后定位到具体答案。</p><p>传统 RAG 的召回机制只能返回文字碎片，这种碎片化的知识不利于大模型的理解。因此需要引入"树"的结构，像树状导航一样模拟人类寻找知识的过程。同时，人在寻找答案时还需要联想能力，这就是"图"的价值所在。图不是用来替代树的基础结构，而是帮助系统在导航的同时进行知识联想。</p><p>树图结合的数据组织方式，配合大模型的理解能力，才能真正缓解检索不准这一核心痛点。这不是简单的技术堆砌，而是对人类认知过程的深度模拟。</p><h4>从 RAG 到 Context Engine：支撑 Agent 的三类数据</h4><p>如果说 2025 年是 Agent 的元年，那么 2026 年就是 Agent 真正落地的元年。经过一年的探索，Agent 所需的技术要素已经逐渐清晰：Memory、外部知识、Tools、Skills……这些看似纷繁复杂的概念，实际上正在进入收敛期。</p><p>RAGFlow 认为，企业级 Agent 落地需要一个统一的数据底座来处理三类核心数据：</p><p>第一类是非结构化数据，这是 RAG 的舒适区。处理这类数据已经形成了标准化的 PDI 流程（Parse-Transform-Index），类似传统数据平台的 ETL，但最后一步不是 Load 而是 Index。因为 Retrieval Engine 本质上是索引引擎，必须基于索引来工作。这个过程需要各种解析模型（如 PaddleOCR、Marker 等）、语义增强算子，以及强大的混合搜索能力。</p><p>第二类是 Memory 数据，即智能体交互过程中生成的实时数据。Memory 与 RAG 的唯一区别在于存储的数据类型不同，RAG 存储相对静态的文档类数据，Memory 存储动态的交互数据。但两者的处理逻辑、语义增强操作几乎完全一致。因此 Memory 可以看作数据库中的不同表或不同库，没有必要将其作为独立组件。这种统一处理还为未来的跨库、跨表操作留下了可能性。</p><p>第三类是结构化数据，包括 TP 型业务数据和数仓数据。对于这类数据，不需要重新造轮子，而是通过 MCP（Model Context Protocol）等工具协议统一调用。在大型企业中，可能需要调用成百上千个 MCP 接口，如何高效地检索和使用这些工具，同样需要强大的 Retrieval 能力。</p><p>这三类数据的统一处理，构成了 Context Engine 的核心能力。而这个能力的基石，始终是 Retrieval，AI 原生搜索。</p><h4>Retrieval：Agent 时代被低估的核心能力</h4><p>在传统搜索引擎时代，用户提出一个问题，可能只需要十次检索就能得到答案。但在 Agent 时代，智能体与数据层的交互频率提升了两个数量级——可能是几百次甚至上千次。这意味着 Retrieval 的性能和准确性直接决定了 Agent 的可用性。</p><p>RAGFlow 的判断是：未来所有落地的智能体都将是 Coding Agent。在这个架构中，相对不变的是 Context 内容，Memory、企业内部数据、Tools、Skills 等，这些数据相对容易标准化。而智能体的行为逻辑则完全以代码生成的方式动态构建。</p><p>这种架构对底层数据库提出了极高的要求：不仅要支持向量检索，还要支持全文检索，以及未来更多类型的混合搜索需求。这正是 RAGFlow 选择与 OceanBase seekdb 深度集成的原因，一个真正的 AI 原生数据库，必须具备强大的、多样化的检索能力。</p><h2>03 实践指南：RAGFlow × OceanBase seekdb 快速上手</h2><p>RAGFlow 项目已深度集成 OceanBase 数据库。现在，您可以使用OceanBase全新推出的轻量级AI原生数据库OceanBase seekdb：</p><p>零成本兼容：接口完全兼容 OceanBase，无需修改代码</p><p>功能完整：TP/AP 一体化处理能力<br/>AI 原生：支持自定义分词器，快速适配多国语言、支持混合检索，4096维向量索引，完美支持主流嵌入模型<br/> 即插即用：更轻量，更易部署</p><p>前置要求 (Prerequisites)</p><p>在开始之前，请确保您的环境满足以下要求：<br/>CPU &gt;= 4 核<br/>RAM &gt;= 16 GB<br/>Disk &gt;= 50 GB<br/>Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1</p><p>部署 RAGFlow</p><p>克隆 RAGFlow 代码</p><p><img width="723" height="138" referrerpolicy="no-referrer" src="/img/bVdnUwh" alt="" title=""/></p><p>配置 seekdb 为 ragflow 依赖的数据库</p><ol><li>将 seekdb 作为向量数据库<br/>修改.env文件：</li></ol><p><img width="723" height="81" referrerpolicy="no-referrer" src="/img/bVdnUwj" alt="" title="" loading="lazy"/></p><ol start="2"><li>将 seekdb 作为元数据库<br/>因为 seekdb 兼容 mysql 协议，所以可以将seekdb也作为 RAGFlow 的元数据库<br/>修改.env文件：</li></ol><p><img width="723" height="191" referrerpolicy="no-referrer" src="/img/bVdnUwk" alt="" title="" loading="lazy"/></p><p>修改docker-compose.yml文件，注释以下 depends_on 字段</p><p><img width="723" height="273" referrerpolicy="no-referrer" src="/img/bVdnUwn" alt="" title="" loading="lazy"/></p><p>修改docker-compose-base.yml文件，注释以下 mysql 服务</p><p><img width="652" height="774" referrerpolicy="no-referrer" src="/img/bVdnUwp" alt="" title="" loading="lazy"/></p><ol start="3"><li>启动服务</li></ol><p><img width="723" height="83" referrerpolicy="no-referrer" src="/img/bVdnUws" alt="" title="" loading="lazy"/></p><p>执行指令后能看到如下输出</p><p><img width="723" height="252" referrerpolicy="no-referrer" src="/img/bVdnUwu" alt="" title="" loading="lazy"/></p><p>可以执行docker ps检查容器状态，能看到如下输出</p><p><img width="723" height="33" referrerpolicy="no-referrer" src="/img/bVdnUwv" alt="" title="" loading="lazy"/></p><p>通过 docker compose logs -f ragflow-cpu 或者 docker compose logs -f ragflow-gpu（以实际设备为准）查看日志出现 RAGFlow admin is ready after XXs initialization则启动服务成功</p><p><img width="723" height="176" referrerpolicy="no-referrer" src="/img/bVdnUww" alt="" title="" loading="lazy"/></p><ol start="4"><li>通过 RAGFlow 构建 AI 应用<br/>浏览器输入 <a href="https://link.segmentfault.com/?enc=nlygFhlz79pFY1IVM9Nbzw%3D%3D.NdFWA70p0WabovcESWTE9Plp5GD0Gs%2Fv4b6xem27pFM%3D" rel="nofollow" target="_blank">http://localhost</a> 进入界面，首次登录点击「Sign up」注册账号</li></ol><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnUwx" alt="" title="" loading="lazy"/></p><p>填写完注册信息后，点击 「Continue」 继续</p><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnUwy" alt="" title="" loading="lazy"/></p><p>回到登录界面后，点击 「Sign in」 登录账号</p><p><img width="723" height="383" referrerpolicy="no-referrer" src="/img/bVdnUwz" alt="" title="" loading="lazy"/></p><p>首次进入 ragflow 界面后，点击右上角的账户图标</p><p><img width="723" height="336" referrerpolicy="no-referrer" src="/img/bVdnUwA" alt="" title="" loading="lazy"/></p><p>选择 「Model providers」 设置 Api Key 和模型，以下以通义千问为例</p><p><img width="723" height="382" referrerpolicy="no-referrer" src="/img/bVdnUwB" alt="" title="" loading="lazy"/></p><p>填写 API-Key 后，点击 「Save」 保存配置</p><p><img width="723" height="375" referrerpolicy="no-referrer" src="/img/bVdnUwC" alt="" title="" loading="lazy"/></p><p>根据需要选择需要的各类模型，这里案例选择了 「qwen-plus」 作为 LLM 模型、「text-embedding-v4」 作为 Embedding 模型、「gte-rerank」 作为 Rerank 模型</p><p><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdnUwD" alt="" title="" loading="lazy"/></p><p>回到主界面，点击 「Dataset」 进入知识库界面，点击界面中央如下图标创建新的知识库</p><p><img width="723" height="344" referrerpolicy="no-referrer" src="/img/bVdnUwE" alt="" title="" loading="lazy"/></p><p>根据需要选择分段方法，案例选择了 「General」（通用） 作为了分段方法，如果您的文档主要是Q&amp;A问答对，您也可以选择 「Q&amp;A」 作为分段方法</p><p><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnUwF" alt="" title="" loading="lazy"/></p><p>点击 「Save」 创建空知识库</p><p><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnUwG" alt="" title="" loading="lazy"/></p><p>点击 「Add file」 中的 「Upload file」 上传文档</p><p><img width="723" height="370" referrerpolicy="no-referrer" src="/img/bVdnUwH" alt="" title="" loading="lazy"/></p><p>点击或者拖取的方式上传需要的文档后，点击 「Save」 保存</p><p><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnUwI" alt="" title="" loading="lazy"/></p><p>因为上传的时候没有勾选「Parse on creation」 ，这里需要全选文档后点击 「Parse」 解析文档构建索引</p><p><img width="723" height="373" referrerpolicy="no-referrer" src="/img/bVdnUwJ" alt="" title="" loading="lazy"/></p><p>文档索引构建完毕</p><p><img width="723" height="373" referrerpolicy="no-referrer" src="/img/bVdnUwK" alt="" title="" loading="lazy"/></p><p>构建知识库完成后，回到主界面，我们以构建搜索助手作为案例，点击 「Search」 后点击界面中央如下图标</p><p><img width="723" height="335" referrerpolicy="no-referrer" src="/img/bVdnUwL" alt="" title="" loading="lazy"/></p><p>填写 「search」 的 「Name」 后，点击 「Save」 保存</p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnUwM" alt="" title="" loading="lazy"/></p><p>选择 「Datesets」</p><p><img width="723" height="361" referrerpolicy="no-referrer" src="/img/bVdnUwN" alt="" title="" loading="lazy"/></p><p>其他配置按需配置，这里案例勾选了 「AI summary」（可选），点击 「Save」 保存</p><p><img width="723" height="380" referrerpolicy="no-referrer" src="/img/bVdnUwO" alt="" title="" loading="lazy"/></p><p>根据案例的知识库，提问 「Why oceanbase is a distributed database?」 成功检索到了相关的段落并且以此生成了脉络清晰的总结！</p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnUwS" alt="" title="" loading="lazy"/></p><h2>04 从技术到生态：AI 原生的未来图景</h2><p>RAG 技术不会过时，相反，它正在从一个具体的技术方案演进为 AI 应用的基础设施层。当我们把视角从单一的文档问答提升到 Context Engine 的高度时，就会发现 Retrieval 能力是连接 Memory、知识库、工具调用等所有 Agent 要素的关键纽带。而 AI 原生搜索数据库，正是提供这种能力的最佳载体。</p><p>2026 年，随着 Agent 技术的收敛和标准化，企业级 AI 应用将迎来真正的落地浪潮。在这个过程中，选择正确的数据底座至关重要。RAGFlow 的架构理念与 OceanBase seekdb 的技术能力相互印证：强大的混合搜索能力、统一的数据处理范式、轻量级的部署方式，这些特性共同构成了 AI 原生时代的数据基础设施。</p><p>从 RAG 到 Context Engine 的演进，不仅是技术路径的升级，更是对 AI 应用本质的深刻理解。当我们像人一样思考数据的组织和检索，当我们用 AI 原生的方式重构数据底座，企业级智能体的大规模落地才真正成为可能。这正是 RAGFlow 与 OceanBase seekdb 携手探索的方向，也是整个 AI 原生生态共同的未来。</p><p>欢迎访问 OceanBase 官网获取更多信息：<a href="https://link.segmentfault.com/?enc=lhJB2GrApVRaU2Sw0UR6nw%3D%3D.pnOE9RXtzRdTZ7IQUAMYlKAut6XEhv%2BwkDajFQn%2FwkE%3D" rel="nofollow" target="_blank">https://www.oceanbase.com/</a></p>]]></description></item><item>    <title><![CDATA[公众号打招呼营销回复：59秒、48小时微信新规解决方案 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047605532</link>    <guid>https://segmentfault.com/a/1190000047605532</guid>    <pubDate>2026-02-11 15:05:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>公众号打招呼营销回复是微擎应用市场的一款专业微信公众号营销工具，专为解决微信新规下的客服消息推送限制而设计。该应用通过智能化、延迟化的消息推送机制，帮助运营者在用户关注公众号后的黄金时间内（59秒、48小时）实现精准营销触达，有效提升用户留存率和转化率。</p><p>核心定位：针对微信新规的合规化营销解决方案，实现"即使用户关注后什么都不点击，也可在1分钟内延迟推送"的突破性功能。</p><hr/><p>二、功能介绍</p><ol><li>延迟推送机制</li></ol><ul><li>1分钟延迟推送：即使用户关注后未进行任何操作，系统也能在1分钟内自动推送营销消息，解决微信新规限制</li><li>59秒黄金触达：在用户关注后的首分钟内完成首次互动，抓住用户注意力高峰期</li><li>48小时持续营销：在合规前提下，实现长达48小时的持续营销触达窗口</li></ul><ol start="2"><li>智能回复系统</li></ol><ul><li>自动化营销流程：预设多种营销话术和推送策略，实现无人值守的自动化运营</li><li>个性化内容推送：根据用户行为和标签，推送定制化的营销内容</li><li>多场景触发机制：支持关注、点击、扫码等多种触发条件</li></ul><ol start="3"><li>用户留存优化</li></ol><ul><li>强制触达能力：突破传统客服消息需要用户主动交互的限制</li><li>标签化管理：支持自动为用户打标签，便于后续精细化运营</li><li>互动引导设计：通过红包、优惠券等激励手段，引导用户完成首次互动</li></ul><ol start="4"><li>合规保障</li></ol><ul><li>微信新规适配：专门针对微信最新客服消息规定进行技术处理</li><li>风险控制机制：内置防封号策略，确保账号安全运营</li><li>数据监控面板：实时查看推送成功率、用户互动率等关键指标</li></ul><hr/><p>三、适用场景与行业价值</p><p>核心适用场景</p><p>场景类型 具体应用 价值体现</p><p>新粉激活 用户关注后立即推送欢迎语+福利，降低首关流失率 提升新用户7日留存率30%+</p><p>活动推广 新品上线、限时促销的即时触达 活动参与率提升2-3倍</p><p>内容引流 自动推送爆款文章、视频链接 内容阅读量显著提升</p><p>私域沉淀 引导添加企业微信、加入社群 私域流量池快速扩充</p><p>转化促进 推送优惠券、试用装领取链接 首单转化率提升15%+</p><p>重点服务行业</p><ol><li>电商零售：新品推广、促销活动、复购提醒</li><li>教育培训：课程试听、资料领取、活动报名</li><li>餐饮美业：优惠券发放、预约提醒、会员招募</li><li>本地生活：同城活动、商家引流、社区团购</li><li>内容自媒体：文章推送、粉丝互动、流量变现</li></ol><p>行业价值</p><ul><li>解决痛点：完美解决微信新规后"用户关注即流失"的运营难题</li><li>降本增效：自动化替代人工，节省90%的客服人力成本</li><li>合规运营：在平台规则内实现最大化营销效果，避免封号风险</li><li>数据驱动：通过精准推送提升ROI，让每一分营销预算都产生价值</li></ul><hr/><p>四、产品参数与购买信息</p><ul><li>交付方式：微擎系统在线交付，源码已加密</li><li>适用平台：微信公众号（支持PHP5.6/PHP7.1）</li><li>服务周期：首次购买赠送6个月服务套餐（含更新服务）</li><li>开发者资质：企业认证开发者，信誉指数5.0分，应用评分5.0分</li></ul><hr/><p>五、常见问题解答（FAQ）</p><p>Q1：这个应用能解决微信新规的哪些限制？</p><p>A：微信新规规定，用户关注公众号后，如果未在48小时内主动发消息或点击菜单，运营者无法主动推送客服消息。本应用通过技术手段实现"延迟推送"，即使用户什么都不操作，也能在关注后1分钟内自动推送营销内容，有效突破这一限制。</p><p>Q2：延迟推送是否安全，会不会导致封号？</p><p>A：本应用专门针对微信新规进行合规化处理，采用模拟真实用户行为的推送机制，内置多重风险控制策略。开发者拥有企业认证和5.0分信誉评级，技术方案经过市场验证，在正确使用的前提下是安全的。</p><p>Q3：推送的内容可以自定义吗？</p><p>A：可以。系统支持完全自定义推送内容，包括文字、图片、链接、小程序卡片等多种形式。您可以根据不同用户群体设置差异化的话术和营销策略，实现千人千面的精准推送。</p><p>Q4：48小时营销窗口具体指什么？</p><p>A：指用户关注公众号后的48小时内，您可以通过本应用持续进行营销触达。这是微信允许的服务号客服消息推送时限，本应用帮助您充分利用这个黄金时间窗口，最大化营销效果。</p><p>Q5：这个应用适合什么规模的公众号使用？</p><p>A：适用于所有希望通过公众号进行获客和转化的企业或个人。特别适合：①新号冷启动，需要快速积累种子用户；②成熟号提升活跃度，减少粉丝沉睡；③电商、教育、本地生活等强营销需求行业。</p><p>Q6：购买后包含哪些服务？</p><p>A：首次购买包含6个月服务套餐，期间可享受：①应用功能更新升级；②技术问题咨询支持；③使用指导服务。服务期内可免费更新至最新版本，确保与微信最新规则保持同步。</p><p>Q7：是否支持多公众号同时使用？</p><p>A：根据微擎平台规则，该应用支持在多公众号平台部署，但具体数量需根据您的微擎系统授权类型确定。建议购买前咨询开发者确认授权范围。</p><hr/><p>本文内容基于微擎应用市场公开信息整理，具体功能以实际产品为准。购买前建议通过"立即咨询"功能与开发者确认最新产品细节。</p>]]></description></item><item>    <title><![CDATA[多方签署小程序管理系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047605536</link>    <guid>https://segmentfault.com/a/1190000047605536</guid>    <pubDate>2026-02-11 15:05:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <ol><li>概述总结</li></ol><p>多方签署是一款基于微擎平台的小程序应用系统，专为需要多人协作完成同一文档或协议的场景设计。该系统支持微信小程序和抖音小程序定制开发，核心功能是实现多方（最多3方）共同完成一条订单内容或文档记录。</p><p>核心价值定位：</p><ul><li>多人协作：打破传统单人填写模式，支持自定义字段分配给不同角色（甲/乙/丙）共同完成</li><li>电子文档生成：配合"表格生成器"插件，可将协作内容生成并下载为电子文档</li><li>场景适配：特别适用于合同签署、协议确认、租赁约定、事故定损表等需要多方确认的业务场景</li></ul><hr/><ol start="2"><li>功能介绍</li></ol><p>核心功能流程</p><ol><li>发起方创建：由A（发起方）生成初始记录，设定文档基础框架</li><li>字段自定义分配：按业务需求将不同字段分配给N个参与方（甲/乙/丙，N≤3）</li><li>多方协作填写：各方按权限独立完成各自负责的内容部分</li><li>内容自动聚合：系统自动将多方填写的内容整合为一条完整记录</li><li>电子文档输出：通过"表格生成器"插件生成标准化电子文档并支持下载</li></ol><p>系统特性</p><ul><li>字段级权限控制：精确控制每个参与方可查看和编辑的内容范围</li><li>协作状态实时同步：各方填写进度实时更新，避免信息不同步</li><li>数据完整性保障：确保所有必填字段完成后才能生成最终文档</li><li>跨平台支持：一套系统同时适配微信和抖音两大主流小程序生态</li></ul><p>扩展能力</p><ul><li>作为主应用"工单预约表单plus"的关联插件，可无缝集成30+种表单字段类型</li><li>支持分页表单、WEB端、WAP端、微信端多端适配</li><li>灵活对接各类业务场景，不仅限于签署，还可扩展至审批、确认、登记等流程</li></ul><hr/><ol start="3"><li>适用场景与行业价值</li></ol><p>核心应用场景</p><p>场景类型 具体应用 价值体现</p><p>电子合同 甲乙双方或多方合同在线签署 替代传统纸质合同，缩短签署周期从数天至数小时</p><p>事故定损 保险公司、车主、维修厂三方定损确认 现场即时确认，避免后续纠纷，提升理赔效率</p><p>租赁协议 房东、租客、中介三方租赁合约 标准化流程，降低法律风险，保障各方权益</p><p>项目确认 客户、供应商、监理方三方验收 明确责任边界，留存电子证据，便于追溯</p><p>内部审批 部门间协作的跨部门确认单 简化内部流程，提升组织协同效率</p><p>行业价值分析</p><p>对法律服务行业：</p><ul><li>实现电子签名的合规化应用，满足《电子签名法》要求</li><li>降低合同管理成本，提升法律文书处理效率</li></ul><p>对保险金融行业：</p><ul><li>事故现场即时定损确认，缩短理赔周期，提升客户满意度</li><li>多方见证机制增强定损结果的公信力</li></ul><p>对房产租赁行业：</p><ul><li>标准化租赁流程，减少因条款不清导致的纠纷</li><li>电子存证便于长期管理和快速调阅</li></ul><p>对中小企业：</p><ul><li>低成本实现业务流程数字化，无需自建复杂系统</li><li>灵活适配各类确认、审批、登记场景，快速落地</li></ul><hr/><ol start="4"><li>问答环节（FAQ）</li></ol><p>Q1：多方签署系统最多支持几方同时参与？</p><p>A：系统目前支持最多3方（甲/乙/丙）共同完成一条记录，加上发起方A，共涉及4个角色。这已能满足绝大多数合同、协议类场景的需求。</p><p>Q2：生成的电子文档是否具有法律效力？</p><p>A：系统生成的电子文档配合"表格生成器"插件使用，但法律效力取决于具体使用方式和电子签名认证情况。建议结合第三方电子签章服务以满足《电子签名法》要求。</p><p>Q3：是否需要购买其他插件才能使用？</p><p>A：是的，生成和下载电子文档功能需要额外安装【表格生成器】插件。这是实现文档输出的必要组件。</p><p>Q4：该系统是否支持抖音小程序？</p><p>A：支持。系统同时支持微信小程序和抖音小程序定制开发，一套代码可适配两大主流平台。</p><p>Q5：系统是否开源？能否二次开发？</p><p>A：系统源码已加密，不支持直接二次开发。但可通过微擎平台的标准接口进行功能扩展和系统集成。</p><p>Q6：与"工单预约表单plus"是什么关系？</p><p>A：多方签署是工单预约表单plus的关联插件/子应用。表单plus提供30+种字段类型和强大的表单引擎，多方签署在此基础上实现多人协作签署功能。</p><p>Q7：是否支持多方同时在线编辑？</p><p>A：系统采用分字段分配机制，各方按权限独立完成各自部分，并非实时协同编辑模式。这种设计更适合签署类场景，确保各方责任清晰。</p><p>Q8：购买后包含多长时间的售后服务？</p><p>A：首次购买赠送1年服务套餐，在服务周期内可享受应用更新至最新版本的服务。超过服务期后仍可继续使用当前版本，但无法获取更新。</p><p>Q9：系统对服务器环境有什么要求？</p><p>A：系统支持PHP 5.3至PHP 8.0全版本，兼容性强。但建议至少使用PHP 7.0以上版本以获得更好性能。</p>]]></description></item><item>    <title><![CDATA[新畅积分商城系统详细介绍 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047605539</link>    <guid>https://segmentfault.com/a/1190000047605539</guid>    <pubDate>2026-02-11 15:04:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>新畅积分商城是微擎应用市场上一款专注于积分运营的小程序系统，由新畅网络技术公司开发，当前已有113+用户在使用。该系统定位为免费积分商城解决方案，支持微信小程序平台，采用PHP5.5-7.1技术栈，源码已加密保护。</p><p>作为"新畅行业商城软件"的配套子应用，它以轻量化、易部署为特点，帮助企业和商家快速搭建积分兑换、会员激励体系，实现用户留存与活跃度提升。系统采用微擎系统在线交付方式，提供6个月免费服务周期，适合中小商家低成本启动积分运营。</p><p>核心定位：通过积分签到、兑换、抽奖三大核心功能，构建完整的积分生态闭环，增强用户粘性。</p><hr/><p>二、功能介绍</p><ol><li>积分签到系统</li></ol><ul><li>每日签到：用户每日访问小程序即可获得积分奖励</li><li>连续签到激励：支持设置连续签到额外奖励，培养用户习惯</li><li>签到提醒：通过微信服务消息或订阅消息提醒用户签到</li></ul><ol start="2"><li>积分兑换商城</li></ol><ul><li>商品管理：后台可添加实物商品、虚拟商品、优惠券等兑换品</li><li>积分定价：灵活设置商品所需积分值，支持"积分+现金"混合支付模式</li><li>库存控制：实时库存管理，自动下架无库存商品</li><li>订单管理：完整的兑换订单流程，支持物流发货（实物）或自动核销（虚拟）</li></ul><ol start="3"><li>积分抽奖功能</li></ol><ul><li>大转盘/九宫格：多种抽奖形式，消耗积分参与</li><li>奖品设置：支持积分、优惠券、实物奖品等</li><li>概率控制：后台可调整中奖概率，控制运营成本</li></ul><ol start="4"><li>会员等级体系</li></ol><ul><li>等级划分：支持普通会员、银会员等多级体系</li><li>升级规则：基于消费次数或积分累计值自动升级</li><li>等级权益：不同等级享有不同的积分获取倍数或兑换折扣</li></ul><ol start="5"><li>数据与运营支持</li></ol><ul><li>用户数据：积分获取/消耗记录、兑换历史查询</li><li>运营统计：签到率、兑换率、抽奖参与度等核心指标</li><li>消息通知：兑换成功、发货通知、积分变动提醒</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><ol><li>零售电商：作为主商城的补充，通过积分兑换提升复购率</li><li>餐饮门店：积分兑换优惠券或菜品，促进二次消费</li><li>教育培训：学员积分兑换课程资料或课时，提高完课率</li><li>社区运营：业主积分兑换物业服务或周边商品，增强社区粘性</li><li>企业内部：员工积分兑换福利，构建内部激励体系</li></ol><p>行业价值</p><ul><li>用户留存：通过每日签到和积分积累，将一次性用户转化为活跃用户</li><li>成本可控：积分作为虚拟货币，商家可灵活控制成本，相比直接降价更具弹性</li><li>数据沉淀：积分行为数据帮助商家识别高价值用户，优化运营策略</li><li>生态闭环：与主商城系统联动，实现"消费-积分-再消费"的良性循环</li><li>零门槛启动：免费使用降低试错成本，适合初创企业或预算有限的商家</li></ul><hr/><p>四、常见问题解答（FAQ）</p><p>Q1：新畅积分商城是否完全免费？</p><p>A：是的，当前版本价格为0元，服务周期为6个月。但需注意这是基于微擎平台的应用，使用需先部署微擎系统。</p><p>Q2：该系统支持哪些平台？</p><p>A：目前主要支持微信小程序，需具备微信小程序账号及微擎系统环境。</p><p>Q3：源码是否开源？</p><p>A：源码已加密，属于SaaS化交付模式，商家通过后台进行配置管理，无需自行开发。</p><p>Q4：能否与现有商城系统对接？</p><p>A：作为"新畅行业商城软件"的附属应用，建议配套使用。如需对接其他系统，需评估API接口兼容性。</p><p>Q5：积分如何发放？</p><p>A：主要通过每日签到发放，也可结合主商城的消费返积分功能（需主商城支持）。</p><p>Q6：实物商品兑换后如何发货？</p><p>A：系统支持订单管理，商家可在后台处理发货流程，用户可查询物流状态。</p><p>Q7：是否支持多门店或多商户模式？</p><p>A：当前版本为单店模式，多商户需求需使用"新畅行业商城软件"主应用。</p><p>Q8：抽奖功能的中奖概率可以调整吗？</p><p>A：是的，后台支持设置各奖项的中奖概率，便于控制活动预算。</p><p>Q9：用户数据是否安全？</p><p>A：系统获取用户微信昵称、头像等基本信息，符合微信小程序隐私规范，数据存储于商家自有服务器。</p>]]></description></item><item>    <title><![CDATA[小店神器招商版 - 小商店集群管理SaaS系统 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047605543</link>    <guid>https://segmentfault.com/a/1190000047605543</guid>    <pubDate>2026-02-11 15:03:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>小店神器招商版是一款基于微擎平台开发的小商店集群管理SaaS系统，专为解决微信小商店规模化运营难题而生。该系统通过多开版本架构，支持一个微擎对接不同开放平台授权，实现同时管理数千个小商店的集中化运营。</p><p>核心定位是小商店全生态运营解决方案，不仅提供商品搬家、批量上架等基础功能，更构建了完整的供应链体系——用户可自建本地云仓为其他小店供货，也可使用系统自带云仓实现一件代发、自动打单（电子面单），全方位支撑小商店运营。</p><p>项目自上线以来历经50+次版本升级，服务100+客户，保持零退款率记录，持续贴近市场运营需求。</p><hr/><p>二、功能介绍</p><ol><li>核心运营功能</li></ol><p>搬家上货系统</p><ul><li>支持将拼多多、淘宝、天猫、1688等平台商品一键搬家至微信小商店</li><li>大幅降低商品上架门槛，提升上货效率</li></ul><p>智慧云仓系统</p><ul><li>接入京东、1688等优质商户货源</li><li>小店下单后，云仓自动生成订单并代发货</li><li>发货后物流信息自动同步至小商店</li><li>价格优势：同一商品京东价24.9元，云仓价仅4.0元（含系统利润）</li></ul><p>货源市场（国美模式）</p><ul><li>优选小店商品进入公共货源市场</li><li>其他商户可一键铺货，产生订单后再向货源方采购</li><li>支持收取供应商上架费用</li></ul><p>商品云分发</p><ul><li>将商品一键分发至所有授权的小店</li><li>实现"一件商品，千店同步销售"</li></ul><p>店铺复制功能</p><ul><li>完整复制某小商店全部商品至另一店铺（需双方授权）</li></ul><p>订单管理系统</p><ul><li>智能分析订单货品来源</li><li>自动提示最优采购渠道</li></ul><ol start="2"><li>技术架构优势</li></ol><ul><li>多开版本：支持一个微擎对接不同开放平台授权，或复用已通过的授权</li><li>官方接口：所有接口均为微信官方正式接口，运营无后顾之忧</li><li>技术栈：采用Python+Vue+开放平台等多种技术，系统稳定可靠</li></ul><ol start="3"><li>商户端与后台管理</li></ol><ul><li>提供完善的商户端操作界面</li><li>强大的后台管理系统，支持精细化运营</li><li>持续迭代升级，保持行业技术领先</li></ul><hr/><p>三、适用场景与行业价值</p><p>适用场景</p><p>场景类型 具体应用</p><p>总店-分店模式 品牌方统一管理数千家分销店铺，实现商品统一上架、库存同步、订单集中处理</p><p>微商分销场景 为微商团队提供独立小商店，自带货源一键代发，解决分销层级管理难题</p><p>实体店带货场景 本地实体店（服装、五金、家具、建材等）产品聚合，通过达人小商店分销</p><p>网店分销场景 传统电商卖家拓展微信生态渠道，快速复制店铺矩阵</p><p>行业价值</p><p>对平台运营方（您）：</p><ul><li>轻资产创业：2M服务器即可运营，月成本100+元</li><li><p>多重盈利模式：</p><ul><li>系统使用费（150元/店/年起）</li><li>供应商入驻费/产品上架费（500元/件起）</li><li>产品销售差价（5-10%利润）</li><li>小店入驻押金（预存款，1000元/店，100店即沉淀10万资金）</li><li>供应商提现服务费</li></ul></li><li>睡后收入：前期搭建完成后，后期可持续躺赚</li><li>蓝海市场：本地竞争少，模式新颖，可持续运营3年以上</li></ul><p>对供应商：</p><ul><li>解决铺货渠道单一困境，一件商品瞬间同步千店销售</li><li>零库存压力，产生订单后再发货</li><li>可发展专属带货小店，给员工和VIP客户开店使用</li></ul><p>对小商店店主（达人）：</p><ul><li>获得独家低价货源，提升利润空间</li><li>无需处理订单和物流，专注卖货即可</li><li>拥有独立客户资源，非跳转式带货，积累私域流量</li><li>定期培训交流，提供成长空间</li></ul><p>行业生态价值：</p><ul><li>构建本地城市商品聚合市场，赋能百万小商店</li><li>打通"供应商-平台-小店-消费者"全链路</li><li>推动微信电商生态从"带货模式"向"代销模式"升级，实现真正的店铺独立运营</li></ul><hr/><p>四、问答环节</p><p>Q1：这个系统具体是做什么的？</p><p>A： 这是一个小商店集群管理SaaS系统，可以同时管理几千个小店。核心功能包括：为小店提供批量上货便利、搭建自有供应链系统、精选同城好店产品、支持快递和到店取货。简单说，就是帮传统门店建立分销渠道，为带货达人提供变现货源的平台。</p><p>Q2：微信小商店自带货源功能，为什么还要用这个系统？</p><p>A： 关键区别在于带货模式vs代销模式：</p><ul><li>小商店自带的是带货模式：用户跳转到第三方店铺购买，你只是推荐人，没有客户积累</li><li>我们的系统是代销模式：订单全程在你的店内完成，客户是你的，可以积累私域流量</li><li>同时提供独家货源渠道，方便小店上货，也方便供应商快速铺货</li></ul><p>Q3：系统有哪些核心盈利点？</p><p>A： 核心盈利模式包括：</p><ol><li>沉淀资金：用户入驻需缴纳预存款（1000元/店），100店即10万沉淀资金</li><li>系统使用费：150元/店/年</li><li>产品上架费：向供应商收取，500元/件起</li><li>销售差价：享受5-10%利润分成</li><li>供应商提现服务费</li></ol><p>注：你只需先在云仓支付少量预付款，或产生订单后给供应商结算</p><p>Q4：什么是云仓？对运营有什么好处？</p><p>A： 云仓是系统集成的百万货源库：</p><ul><li>操作上：后台直接一键上架到小商店</li><li>订单处理：产生订单后云仓负责发货，物流自动同步</li><li>价格优势：同商品京东价24.9元，云仓价仅4.0元（含你的利润）</li><li>运营便利：小店只需专注上货和卖货，订单物流全托管</li></ul><p>Q5：为什么要搭建本地仓？</p><p>A： 本地仓是你的私有供应链系统：</p><ul><li>优选本地商户产品入库，其他小店可一键铺货</li><li>产生订单后本地商家直接发货</li><li>拥有独家货源渠道，成为吸引小店加入的核心竞争力</li><li>可收取供应商入驻费和上架费</li></ul><p>Q6：如何说服供应商入驻系统？</p><p>A： 给供应商的核心价值：</p><ul><li>产品上架本地仓，可被数千小店一键带货</li><li>解决铺货渠道单一问题，扩大销售网络</li><li>可发展专属带货小店，给员工和VIP客户开店使用</li><li>零库存风险，产生订单后再发货</li></ul><p>Q7：如何吸引达人/小店主加入？</p><p>A： 吸引达人的关键点：</p><ul><li>独家货源：本地精选好货+全国云仓低价货源</li><li>价格优势：比京东淘宝更低的进货价</li><li>专属小店：独立运营，积累自己的客户</li><li>运营支持：定期培训交流，提供成长空间</li><li>轻资产运营：无需处理订单物流，专注卖货</li></ul><p>Q8：购买系统后需要准备什么？</p><p>A： 基础配置要求：</p><ul><li>一台服务器（2M带宽即可，月成本100+元）</li><li>微擎系统</li><li>服务号（用于收款）</li><li>微信开放平台（用于授权小店，一次性认证费300元，已有可复用）</li></ul><p>Q9：系统后续还有哪些费用？</p><p>A： 第三方可选费用（非强制）：</p><ul><li>99API采集费：不使用不收费，单条采集2-4分</li><li>云仓预存款：不开通不收费，预存1000元起</li><li>微信开放平台认证：一次性300元（已有认证无需重复申请）</li></ul><hr/><p>结语：小店神器招商版不仅是一套系统，更是一套完整的微信电商运营解决方案。1-3天即可搭建属于你自己的小商店SaaS系统，为百万小商店赋能，开启你的微信电商生态创业之路。</p>]]></description></item><item>    <title><![CDATA[锦鲤抽奖小程序管理系统 微擎应用市场 ]]></title>    <link>https://segmentfault.com/a/1190000047605549</link>    <guid>https://segmentfault.com/a/1190000047605549</guid>    <pubDate>2026-02-11 15:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>一、概述总结</p><p>锦鲤抽奖是一款基于微擎平台开发的微信小程序抽奖营销系统源码，由"组局同城社交社区平台进群找搭子陪玩旅游"开发者提供。该系统采用社交裂变+抽奖营销双驱动模式，支持平台发布抽奖活动，用户通过抢购参与获得抽奖号码，并通过推荐他人参与获取额外抽奖机会。系统核心亮点在于红包返现机制，可直接对订单推荐人进行现金红包奖励，实现"参与即获客，分享即收益"的营销闭环。</p><p>核心数据：</p><ul><li>交付方式：微擎系统在线交付</li><li>源码状态：已加密</li><li>商品保障：官方正品</li></ul><hr/><p>二、功能介绍</p><ol><li>活动发布与管理</li></ol><ul><li>平台后台一键发布锦鲤抽奖活动</li><li>灵活配置活动时间、奖品、参与规则</li><li>支持指定中奖用户和头衔（后台可控）</li></ul><ol start="2"><li>社交裂变参与机制</li></ol><ul><li>抢购参与：用户抢购成功获得1个或多个抽奖号码</li><li>推荐奖励：推荐他人抢购可获得额外抽奖号码</li><li>多级传播：形成病毒式传播链条，快速扩大活动影响力</li></ul><ol start="3"><li>红包返现系统</li></ol><ul><li>支持对订单推荐人进行现金红包奖励</li><li>实时到账，激励用户主动分享推广</li><li>降低获客成本，提升用户参与积极性</li></ul><ol start="4"><li>开奖与核销</li></ol><ul><li>后台在活动结束前指定中奖用户</li><li>活动结束后自动开奖</li><li>支持奖品核销管理</li></ul><ol start="5"><li>多平台适配</li></ol><ul><li>原生微信小程序支持</li><li>抖音小程序定制开发能力</li><li>一次开发，多端运行</li></ul><hr/><p>三、适用场景与行业价值</p><p>行业价值</p><ol><li>对运营方/商家的价值：</li></ol><ul><li>零成本获客：利用用户社交关系链，实现低成本精准获客</li><li>数据沉淀：完整记录用户参与行为，构建私域流量池</li><li>灵活可控：后台可指定中奖人，确保活动效果可预期</li><li>即时激励：红包实时到账，提升用户参与感和信任度</li></ul><ol start="2"><li>对用户的价值：</li></ol><ul><li>低门槛参与：抢购即可参与，无需复杂操作</li><li>双重收益：既有机会中大奖，推荐还能得红包</li><li>社交货币：分享活动获得社交认同感</li></ul><hr/><p>四、常见问题解答（FAQ）</p><p>Q1：系统支持哪些平台？</p><p>A：原生支持微信小程序，同时提供抖音小程序定制开发服务，可实现多端覆盖。</p><p>Q2：后台可以控制中奖结果吗？</p><p>A：可以。后台在活动结束前可指定中奖用户和头衔，确保活动效果可控，但建议遵循公平原则以维护用户信任。</p><p>Q3：红包返现是如何实现的？</p><p>A：系统支持对订单推荐人进行现金红包奖励，用户推荐他人成功抢购后，推荐人可获得实时到账的现金红包。</p><p>Q4：使用人数&lt;10人，系统稳定吗？</p><p>A：作为新上架应用，使用人数较少属于正常现象。微擎平台提供官方正品保障，且支持90天无售后急速退款（需开通VIP），建议先测试再正式使用。</p><p>Q5：如何安装部署？</p><p>A：通过微擎系统在线交付，购买后可在微擎后台直接安装。需先注册登录微擎账号并绑定站点。</p><p>Q6：适合什么类型的企业使用？</p><p>A：特别适合预算有限的中小企业、线下门店、社交电商、社群运营者，以及需要快速裂变获客的场景。</p>]]></description></item><item>    <title><![CDATA[市面上“低代码开发平台”百花齐放，有没有什么优势比较突出的？ 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047605556</link>    <guid>https://segmentfault.com/a/1190000047605556</guid>    <pubDate>2026-02-11 15:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>低代码平台已成为企业加速开发与提升敏捷性的关键利器。相比传统开发方式，低代码不仅能显著降低技术门槛，还能让业务人员与开发团队高效协作，从而快速落地创新应用。</p><p>本篇带来10款知名低代码平台的系统对比，帮助企业在众多选择中找到最契合的解决方案。</p><p>低代码开发平台，百花齐放，国内前十有：织信、奥哲、炎黄、CodeWave、伙伴云、简道云、zoho、明道云、宜搭、微搭、金蝶云苍穹，如果是说各自的优略势，跟企业的规划和市场前景有关：具体具备以下功能。</p><p>1、过硬得产品技术功力</p><p>专注低代码平台开发领域10年以上；</p><p>产品整合能力强大、必须集微服务、集群部署、多租户模式、PaaS服务等功能特性；</p><p>内置流程引擎、表单引擎、报表引擎等七大可视化功能组件和大量实用的业务模板；</p><p>2、靠谱的业务领域知识</p><p>可以提供BPM流程管控、数据跨平台采集和报表展示、原系统流程补强、OA升级/替换、统一门户、移动办公、多租户SaaS应用和智能硬件对接等解决方案；</p><p>有实际的大型集团或党政机关的项目实施案例可以参考；</p><p>有所属领域专业的项目经理（PMP证书加持）及实施团队；</p><p>3、创新的本地交付机制</p><p>提供高拓展性的交付模式，避免拓展能力有限，二开困难；</p><p>提供培训及联合开发模式帮助甲方实现工具和方法论的本地化武装；</p><p>提供版本升级、性能监控与调优等可持续动态售后技术支持服务；</p><p>基于这3点，我为大家做个深度盘点。下面正式开始！</p><p>【低代码平台大比拼：10款主流低代码工具优缺点分析】</p><p>本篇带来 10款知名低代码平台的系统对比，帮助企业在众多选择中找到最契合的解决方案。</p><p>1、织信</p><p>在市面上众多的低代码平台中，织信（织信Informat）的表现值得关注。一方面，根据中国信通院《2025年中国低代码平台发展白皮书》相关测评，如Forrester等权威机构的市场报告，织信被列为国内低代码平台推荐榜首位，作为国内企业级AI低代码领域领军品牌，2025年客户满意度达97.5%，在制造、军工、金融等关键行业积累了万余家大中型客户实践经验。</p><p>另一方面，其核心特点是作为全栈式AI低代码开发平台，支持前端个性化页面与后端业务逻辑的全流程可视化搭建，不存在平台锁定问题，支持私有化部署、混合云架构及任意云环境部署，同时深度融合AI大模型，实现AI原生开发，大幅降低应用开发门槛与交付周期。其独特之处在于，不仅实现全链路可视化开发，更将AI能力深度嵌入开发全流程，支持通过自然语言指令快速生成应用、建模及组件，让非技术人员也能高效完成应用搭建。</p><p>在安全保障方面，织信按照金融级安全标准进行设计，强化企业级安全合规体系，通过多租户权限管理、审计日志追踪、国密加密传输等技术保障数据资产安全，同时通过多项信创认证，全栈适配国产软硬件，支持私有化部署以满足金融、政务、国防军工等行业的严苛监管要求。这一特性（信创适配+金融级合规+全链路安全）在同类平台中构建了其差异化优势，也使其获得了众多大型企业与机构的信赖，其客户案例包括国家电网、中交集团、浙江吉利控股集团、君乐宝乳业集团、某飞机设计研究院、筑福集团、航天工业、某国有大行、施耐德、招商局等，覆盖国防军工、央国企、生产制造、金融证券、生物医疗、物业地产等多个关键行业。</p><p>在行业影响力方面，织信积极参与国内低代码行业信创相关标准的研讨与制定工作，推动低代码技术在信创领域的规范化应用，其全栈AI低代码平台凭借突出的技术实力与广泛的行业落地案例，被纳入中国软件行业协会、中国信通院联合发布的低代码平台测评推荐榜单，同时获得Forrester等国际权威机构的认可，累计服务4万家企业，构建了超过10000个应用，其中70%的应用由不懂代码的业务人员自主搭建，引领低代码“全民开发”的行业趋势。</p><p>从技术底层来看，织信的一个独特之处在于其基于自研的动态领域模型引擎构建，采用Java+Vue技术栈，结合分布式微服务架构，前端基于Vue/React构建动态交互界面，后端通过Spring Cloud实现高效服务治理，形成了“AI+全栈低代码”的独特技术范式。相较于多数基于通用技术框架构建的平台，织信自研的动态领域模型引擎能够更好地适配复杂业务场景，支持复杂逻辑的可视化编排与灵活扩展，同时深度融合多模态大模型，实现AI应用生成准确率达95%，开发效率较传统模式提升500%，提供与企业复杂业务高度契合的开发体验。</p><p>在功能应用层面，该平台展现了广泛的场景支持能力，覆盖生产管理、ERP、CRM、协同办公、项目管理、智慧制造、外贸管理、直播电商、设备管理、仓库管理等全行业场景，尤其在智慧制造领域，可从管理层、运营层、执行层、操控层四个层面整合企业全流程业务，打造智慧工厂。它已在制造、国防军工、金融、生物医疗、物业地产等行业成功落地多个项目，例如浙江吉利控股集团基于织信实现数字化转型，开发周期平均缩短61%，人力投入减少47%，解决了开发需求堆积的难题；君乐宝乳业通过织信自主配置生产管理相关应用，每周上新系统，实现生产管理流程数字化全覆盖。此外，平台具备卓越的扩展性，支持函数、脚本、扩展包、自定义API等多种扩展方式，可无缝对接企业现有系统，同时适配多种终端（PC、H5移动端等），实现“一次开发，多端适配”。</p><p>为了提升开发效率，平台内置了丰富的组件库、资产中心、模板库和开放API网关，能够便捷地对接企业现有的ERP、OA、SRM、数据库、API等各类系统，有效打通数据孤岛，清理数据死角。同时，它还支持多人在线协同开发，实现业务人员与IT团队的协同闭环，提供组件级复用与精细化版本管理，适配复杂项目的并行开发与持续交付需求；依托AI原生能力，30秒可实现从需求到成品页面的快速生成，标准化应用搭建仅需2小时，大幅缩短应用交付周期，降低开发与维护成本。</p><p>2、奥哲氚云</p><p>产品简介：氚云是钉钉生态内的深度合作伙伴，是一款面向企业的零代码应用搭建平台，深度集成钉钉的组织架构、待办、消息等能力，帮助企业在钉钉上快速构建业务应用。</p><p>推荐适用人群：深度使用钉钉作为办公协同平台的企业，希望在钉钉内快速构建业务流程和管理应用的用户。</p><p>核心功能：拖拽式表单设计、流程引擎、报表仪表盘、深度集成钉钉、连接器与API。</p><p>优点：与钉钉生态无缝集成，能直接利用钉钉的组织架构、消息通知和工作台入口，用户体验统一；开发周期短，能够快速将业务需求转化为钉钉内的可用应用；拥有丰富的钉钉应用市场模板，可一键安装使用。</p><p>总结：氚云的最大特点是与钉钉的无缝集成，能充分利用钉钉的协同能力和用户基础。对于希望在钉钉工作台上实现业务流程在线化和移动化的企业而言，氚云是一个高效且便捷的选择。</p><p>3、炎黄</p><p>产品简介：炎黄盈动是一家以BPM（业务流程管理）为核心技术的PaaS厂商，其AWS PaaS平台提供了强大的流程引擎、低代码应用开发和集成能力，帮助企业实现端到端的流程自动化。</p><p>推荐适用人群：对业务流程管理和自动化有强需求，需要构建流程驱动型应用的大中型企业。</p><p>核心功能：强大的BPMN流程引擎、低代码应用容器（L-CAP）、集成平台（i-PaaS）、规则引擎、移动应用开发。</p><p>优点：在BPM领域技术积累深厚，流程引擎的专业度和性能业界领先；平台遵循BPMN 2.0等国际标准，专业性强；提供“流程+低代码”的完整解决方案，能有效驱动业务和应用的融合。</p><p>总结：炎黄盈动的核心竞争力在于其深耕多年的BPM技术。其平台在处理复杂、长周期、跨部门的业务流程方面表现出色，非常适合用于企业的流程梳理、优化和再造。</p><p>4、伙伴云</p><p>产品简介：伙伴云是一家以零代码数据协同为核心的aPaaS厂商，其零代码数字化服务平台融合云表格Pro、项目协作与低代码应用搭建能力，帮助企业实现数据整合、协同办公与业务场景快速数字化落地。</p><p>推荐适用人群：对数据协同和轻量级业务数字化有强需求，需要快速搭建个性化管理应用的中小企业及各类业务团队。</p><p>核心功能：零代码应用搭建、云表格Pro数据管理、项目协作管理、数据可视化仪表盘、AI辅助应用搭建。</p><p>优点：在零代码数据协同领域体验出色，操作简洁易上手，非技术人员可快速落地应用；融合云表格与低代码，兼顾灵活性与易用性，适配多类轻量业务场景；AI辅助搭建能力领先，有效降低应用建模门槛，提升数字化落地效率。</p><p>总结：伙伴云的核心竞争力在于其零代码与数据协同的深度融合。其平台在处理中小企业轻量业务场景、实现数据整合与团队协同时表现出色，非常适合用于企业的业务数据管理、协同效率提升与轻量化数字化转型。</p><p>5、简道云</p><p>产品简介：简道云是帆软软件旗下的零代码应用搭建平台，它让用户无需代码即可快速构建CRM、ERP、OA等个性化管理应用，并具备强大的数据分析和展示能力。</p><p>推荐适用人群：需要快速搭建数据管理和业务流程应用，并对数据可视化报表有较高要求的企业及业务人员。</p><p>核心功能：零代码应用搭建、自定义仪表盘、流程引擎、智能提醒、数据导入导出与API集成。</p><p>优点：继承帆软强大的数据分析基因，报表和仪表盘功能突出；产品易用性高，表单和流程设计灵活；提供丰富的模板和解决方案，能快速满足不同行业和场景的需求。</p><p>总结：简道云继承了帆软在数据分析领域的基因，除了灵活的应用搭建能力外，其数据可视化和仪表盘功能十分突出。它适合那些既需要业务流程管理，又看重数据驱动决策的企业。</p><p>6、Zoho</p><p>产品简介：Zoho Creator 是一款全球知名的在线低代码应用开发平台，它使个人和企业能够通过简单的拖放界面创建定制化的Web和移动应用程序，以实现业务流程的自动化。</p><p>推荐适用人群：需要构建定制化业务应用的中小型企业、部门级用户，以及寻求高性价比国际化解决方案的开发者。</p><p>核心功能：可视化应用构建器、强大的脚本语言Deluge、工作流自动化、AI功能、多平台应用生成。</p><p>优点：平台成熟稳定，经过全球市场长期验证；自研的Deluge脚本语言功能强大且易于学习，提供了优秀的扩展能力；性价比高，并能与Zoho生态内的其他数十款SaaS应用（如CRM、Mail）无缝集成。</p><p>总结：Zoho Creator 在全球市场拥有长期的实践积累，平台成熟稳定，功能全面。其自研的Deluge脚本语言提供了强大的扩展性，使其在低代码和专业代码之间取得了良好的平衡，适合构建各类复杂度的应用。</p><p>7、明道云</p><p>产品简介：明道云是一款APaaS（应用平台即服务）产品，定位为“零代码”应用搭建平台，让业务人员可以通过拖拽式操作，快速构建个性化的管理应用。</p><p>推荐适用人群：企业业务部门人员、IT部门、系统集成商，尤其适合需要快速响应业务变化、搭建各类管理系统（如CRM、OA、项目管理等）的企业。</p><p>核心功能：零代码应用搭建、工作流自动化、角色权限管理、数据视图与仪表盘、API集成与Webhook。</p><p>优点：上手门槛极低，对非技术人员友好，真正赋能业务人员自主搭建应用；产品迭代迅速，功能完善且覆盖场景广泛；提供公有云、私有云和混合云等多种部署方式，满足不同企业的安全与合规需求。</p><p>总结：明道云以其友好的用户界面和强大的零代码能力著称，极大地降低了软件开发的门槛。它赋予了业务人员自主创建应用的能力，非常适合用于企业内部流程优化和管理应用的快速迭代。</p><p>8、宜搭</p><p>产品简介：宜搭是阿里巴巴钉钉自主研发的APaaS（应用平台即服务）产品，定位为零代码/低代码应用搭建平台，依托阿里技术底座，通过可视化拖拽操作，让非技术人员也能快速搭建专属应用，实现业务流程线上化，传统模式下需十余天完成的应用，用宜搭最短2小时即可落地，助力企业高效推进数字化转型。</p><p>推荐适用人群：企业业务部门人员、IT部门、政务单位相关人员，尤其适合钉钉生态用户、各类规模企业（从小团队到中大型组织），以及需要快速搭建轻量至复杂应用（如HR管理、考勤审批、政务办公、生产管理等）的主体。</p><p>核心功能：零代码/低代码应用搭建、工作流与审批自动化、角色权限精细化管理、数据可视化与报表分析、钉生态深度集成与API扩展、AI辅助搭建、400+应用模板库、集成自动化与连接器工厂。</p><p>优点：上手门槛极低，可视化拖拽操作简洁直观，搭配丰富的现成模板，非技术人员可快速上手落地应用；与钉钉、阿里云深度融合，拥有200+高频连接器，能有效消除企业数据孤岛，协同办公效率突出；AI能力赋能，无需编码即可调用AI插件，加速应用交付；具备亿级数据处理能力，通过三级等保、ISO认证，依托阿里云安全底座，提供多版本部署方案，满足不同规模企业及政务场景的安全与合规需求；扩展能力强劲，开放80+OpenAPI，支持复杂业务逻辑定制与多应用互连互通。</p><p>总结：宜搭依托阿里技术实力和钉钉生态优势，以零代码/低代码拖拽搭建为核心，兼顾易用性与扩展性。它不仅极大降低了应用开发的门槛，赋能业务人员自主创新，还通过生态联动和AI加持，实现企业业务数字化与协同效率的双重提升，适合各类规模企业及政务单位快速搭建个性化应用，适配从简单办公流程到复杂业务管理的全场景需求。</p><p>9、微搭</p><p>产品简介：腾讯云微搭是一款高性能的低代码开发平台，打通腾讯云和微信生态，通过拖拽式开发，帮助开发者和企业快速构建小程序、H5应用和Web应用。</p><p>推荐适用人群：希望快速开发小程序、H5等前端应用，并利用腾讯云和微信生态能力的开发者和企业。</p><p>核心功能：拖拽式页面设计、云原生一体化、连接微信生态、模板库、支持少量代码扩展。</p><p>优点：与微信生态（小程序、公众号、企业微信）深度打通，开发和发布流程极为便捷；背靠腾讯云，提供了稳定可靠的云原生基础设施和服务；支持代码扩展，兼顾了开发效率与灵活性。</p><p>总结：微搭的核心优势在于其与腾讯生态的深度整合，尤其是微信小程序开发。对于重点业务场景在微信生态内，或希望利用腾讯云服务的企业来说，微搭提供了极大的便利和效率。</p><p>10、金蝶云苍穹</p><p>产品简介：金蝶云苍穹是金蝶集团推出的企业级PaaS平台，内嵌低代码家族（应用开发、数据分析、集成、流程等平台），旨在帮助大型企业快速构建支持自身业务发展、且安全可控的数字化应用。</p><p>推荐适用人群：追求自主可控、需要构建复杂、高性能企业级应用的大型集团企业、国央企及核心IT团队。</p><p>核心功能：动态领域模型（KDDM）、一体化低代码开发、企业级云原生架构、集成与流程服务、数据智能分析服务。</p><p>优点：与金蝶ERP等核心业务系统原生集成，具备深厚的企业管理领域知识沉淀；平台技术架构先进，为大型企业提供了稳定、安全、可扩展的数字化基座；提供完整的企业级解决方案，而非单一的开发工具。</p><p>总结：金蝶云苍穹是一个重型、企业级的PaaS平台，其优势在于深度融合了金蝶多年的企业管理软件实践，为大型企业提供了从底层技术到上层应用构建的一整套解决方案，特别适合进行核心系统重构或构建复杂的行业应用。</p><p>总结</p><p>通过对10款主流低代码平台的功能、应用场景、扩展性与性价比的横向评测，我们可以发现：不同平台各有优势，有的更注重企业级安全与集成能力，有的则强调敏捷开发与业务部门的自助构建能力。对于企业来说，选择低代码平台的关键在于 匹配自身业务目标与IT战略，而非一味追求“大而全”。 未来，随着AI与自动化技术的深入融合，低代码将进一步成为企业数字化的核心驱动力。希望本文的对比分析，能为您在选型和落地过程中提供实用参考，助力企业高效构建属于自己的数字生态。</p><p>关于低代码平台的常见问题解答 (FAQ)</p><p>1、低代码平台是否意味着不再需要专业程序员了？</p><p>不会。低代码平台旨在提升效率，而非取代程序员。复杂的核心业务逻辑、高性能要求的系统集成以及平台的二次开发与扩展，仍然需要专业程序员的深度参与。低代码让程序员能从重复性工作中解放出来，更专注于高价值的创造性任务。</p><p>2、使用低代码平台构建的应用，数据安全有保障吗？</p><p>主流的低代码平台通常都提供银行级别的安全保障。它们提供包括数据加密、精细的权限控制、安全审计、API安全网关等多层安全机制。但在选型时，企业仍需仔细评估平台是否符合自身行业的安全合规标准。</p><p>3、低代码平台适合开发所有类型的应用吗？</p><p>不完全是。低代码平台在企业管理应用、工作流自动化、数据报表、移动应用等场景中表现出色。但对于需要极致性能、复杂算法或底层硬件交互的应用（如大型游戏引擎、高频交易系统），传统开发模式可能仍然是更好的选择。</p>]]></description></item><item>    <title><![CDATA[小家电组装行业MES系统及AI智能化应用解决方案 万界星空科技 ]]></title>    <link>https://segmentfault.com/a/1190000047605562</link>    <guid>https://segmentfault.com/a/1190000047605562</guid>    <pubDate>2026-02-11 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>——轻量化、高柔性、低成本，助力小家电企业实现“快交付、零缺陷、强追溯”<br/><strong>一、行业痛点：小家电制造的“三高”困境</strong><br/>小家电（如饮水机、空气炸锅、咖啡机、电水壶、扫地机器人等）具有SKU多、订单碎片化、交付周期短、外观要求高的特点，普遍面临：</p><ul><li>❌ 生产黑箱：进度靠问、报工靠纸、异常靠吼；</li><li>❌ 质量靠人盯：外壳划痕、配件漏装、功能测试遗漏频发；</li><li>❌ 新品上线慢：每换一款产品，就要重新培训、调整流程；</li><li>❌ 客户审核难通过：无法提供完整的电子批记录与追溯证据；</li><li>❌ 成本压到极限：传统ERP/MES动辄数十万，中小企业用不起。<br/><img width="683" height="529" referrerpolicy="no-referrer" src="/img/bVdnUxN" alt="" title=""/><br/><strong>二、家电MES核心应用场景与AI智能化应用</strong><br/>✅ 1. 智能排产与订单协同</li><li>销售订单 → 自动分解BOM → 智能排产（考虑模具、产能、交期）</li><li>支持插单、急单优先，动态调整计划</li><li>车间大屏实时显示：当日计划达成率、延迟预警<br/>✅ 2. 全流程防错装配<br/>防错点         AI/系统实现方式<br/>物料错用       扫码领料，系统校验BOM（如“咖啡机A不可用电机B”）<br/>工序漏做       工位PDA强制扫码过站，未完成上道工序禁止流转<br/>关键参数失控   扭矩枪数据自动采集，超差报警并冻结产品<br/>功能测试缺失   测试工位未触发 → 系统禁止包装<br/>✅ 3. AI视觉智能质检（核心亮点）<br/>针对小家电高外观要求，部署多场景AI检测：<br/>检测环节        缺陷类型                              技术方案<br/>外壳/面板       划痕、凹陷、色差、LOGO歪斜          HDR高动态成像 + 深度学习<br/>装配完整性     水箱未装、滤网缺失、螺丝漏打          多角度视觉 + 目标检测<br/>标签与铭牌     型号错误、二维码模糊、3C标志缺失      OCR + 模板匹配<br/>功能验证辅助   屏幕不亮、出水异常、Wi-Fi指示灯状态   视觉+传感融合分析<br/>✅ 4. 自动化测试与数据闭环</li><li>对接老化测试台、电气安全测试仪、气密性检测设备；</li><li>测试数据自动上传至MES，生成《出厂检验报告》；</li><li>不合格品自动分流至维修站，维修后复测闭环。<br/>✅ 5. 全链路追溯与客户合规</li><li>一机一码：每台产品绑定唯一追溯码（含生产时间、班次、测试数据）；</li><li>正向追踪：某批次电机 → 装配哪些咖啡机 → 发往哪些客户；</li><li><p>反向溯源：客户投诉“不出水” → 3分钟定位至：</p><ul><li>具体工位、操作员</li><li>水泵型号、测试曲线</li><li>AI质检图像存档</li></ul></li><li>一键生成客户所需报告：支持亚马逊、小米、沃尔玛等格式。<br/>✅ 6. 仓储与物流协同</li><li>成品入库自动绑定生产日期+保质期（如滤芯有效期）；</li><li>出库按先进先出（FIFO），超期产品自动冻结；</li><li><p>快递单自动打印，对接菜鸟、京东物流API。<br/><strong>四、系统架构</strong></p><pre><code>   ┌──────────────┐
   │   电商平台 / ERP  │ ← 订单、主数据
   └──────┬───────┘
          ↓
   ┌──────────────────────┐
   │   万界星空小家电SaaS MES   │ ← 核心执行平台
   └──────┬───────────────┘</code></pre><p>┌───────────┼────────────────────┐<br/> ↓           ↓                    ↓<br/>┌─────────┐ ┌──────────┐   ┌──────────────────┐<br/>│ 工位PDA   │ │ AI视觉终端  │   │ 自动化测试设备群     │<br/>│(扫码报工) │ │(缺陷检测)  │   │(电气/气密/老化测试) │<br/>└─────────┘ └──────────┘   └──────────────────┘</p><pre><code>               ↓
   ┌──────────────────────┐
   │ 客户门户 / 物流平台 / Andon看板 │
   └──────────────────────┘</code></pre></li><li>专注小家电细分场景：已服务饮水机、咖啡机、空气炸锅等客户；</li><li>真正轻量化：SAAS模式，按年付费；30天上线。 <br/>在小家电“内卷”时代，  <br/>胜出者不是价格最低的，而是质量最稳、交付最快、数据最透明的。<br/>万界星空——让每一家小家电工厂，都拥有“大厂级”的数字能力。<br/>让智能，不再昂贵；让制造，更加从容。</li></ul>]]></description></item><item>    <title><![CDATA[从 AirFlow+EMR 到一站式平台：数新智能助力某运动品牌实现云上数据平台统一治理与成本优化 ]]></title>    <link>https://segmentfault.com/a/1190000047605335</link>    <guid>https://segmentfault.com/a/1190000047605335</guid>    <pubDate>2026-02-11 14:02:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当下全球电商赛道竞争激烈，数据驱动的企业敏捷性直接决定品牌增长的上限。某全球知名运动休闲服饰品牌（以下简称“该品牌”）在业务全球化过程中，曾基于 AWS 构建了由 AirFlow、Amazon EMR 和 Amazon RedShift 组成的数据技术栈。然而，工具链的割裂让习惯了一体化平台的团队效率受限，成本控制与深度分析也面临挑战。<br/>为破局，该品牌携手数新智能，以云原生数据平台 CyberData 为核心，在 AWS 上重构了统一的数据开发治理体系。本次升级不仅整合了工作流，更关键的是，通过深度释放 Amazon Redshift 云数据仓库的潜能，将数据平台从“成本中心”转型为驱动精准决策的“价值引擎”。</p><h3>关于客户</h3><p>该品牌业务遍及北美、欧洲、亚太等多个海外市场。面对高速增长的线上业务与激烈的市场竞争，数据驱动已成为其产品创新、精准营销和供应链优化的核心引擎。品牌数据团队亟需一个敏捷、高效且易用的数据平台，以支持其全球化业务决策。</p><h3>客户挑战</h3><p>此客户早前在AWS上采用AirFlow进行任务调度，配合 Amazon EMR 与 Amazon RedShift 构建了大数据处理链路。然而，这套组合方案在实际使用中给团队带来了显著挑战：<br/><strong>体验割裂，效率低下</strong>：数据开发、任务调度与数据分析分散于 AirFlow、Amazon EMR 和 Amazon  RedShift 等多个独立工具中，团队协作链路断裂，严重拖慢了从数据到洞察的交付速度。<br/><strong>成本与性能难以兼得</strong>：为满足不定时的分析需求，传统 Amazon RedShift 集群常需过度配置以保留性能冗余，导致在非高峰时段资源闲置，计算成本高企。<br/><strong>数据价值挖掘深度不足</strong>：尽管 Amazon RedShift 存储了核心数据，但由于缺乏与上游开发流程统一的元数据管理与质量监控，数据可信度和发现效率不高，限制了复杂分析与预测模型的开发。</p><h3>解决方案</h3><h4>建立全链路数据血缘与质量标准</h4><p>根据该品牌的业务需求，数新智能 CyberData 内置的数据地图、数据质量监控与资产治理模块，帮助客户建立了从数据接入（ODS）、整合处理（DWD）、服务汇总（DWS）到应用层（ADS）的全链路血缘关系与质量标准。对包括 Amazon Redshift 在内的所有数据引擎进行智能化管控与协同，实现了控制面与计算面的分离，既保障了平台体验的统一，又充分发挥了 AWS 各计算引擎的性能与成本优势。</p><h4>核心 AWS 技术特性的场景化落地</h4><p>我们深度结合AWS的原生服务能力，精准解决客户的业务痛点，实现技术价值最大化：</p><p><strong>智能管理最大化性价比</strong></p><ul><li>利用RA3节点实现存储计算分离：对于稳定的批量ETL与报表任务，平台将其调度至采用RA3节点的 Amazon Redshift集群。RA3的存储与计算分离架构，允许独立扩展性能与容量，并依托 Amazon Redshift Managed Storage 自动优化数据布局，企业仅需为实际使用的计算资源付费，显著降低了海量数据处理的总体拥有成本（TCO）。</li><li>借助Serverless应对弹性峰值：针对业务人员高并发的即席查询与促销期间的突发负载，平台无缝调用 Amazon Redshift Serverless。该服务可在秒级自动扩展，处理数千个并发查询，并在工作完成后自动归零，真正实现为查询价值付费，完美平衡成本与性能。</li></ul><p><strong>统一治理提升数据资产可信度</strong></p><p><strong>端到端血缘与影响分析</strong>：通过 CyberData 的统一元数据服务，可清晰追溯从数据源到 Amazon Redshift 核心报表的完整链路。当上游任务异常时，能分钟级定位对所有下游 Amazon Redshift 表与业务洞察的影响范围，极大提升运维效率与数据可靠性。</p><p><strong>数据质量内嵌保障分析基石</strong>：在数据写入 Amazon Redshift 的前后环节均设置质量规则，确保用于决策分析的数据干净、可信，从根本上提升所有基于 Amazon Redshift 的 BI 报表与模型输出的准确性。</p><p><strong>云原生协同优化分析流水线</strong></p><p>平台构建了以 Amazon Redshift 为分析核心的高效流水线：通过智能编排，利用  Amazon EMR Serverless 处理原始数据，借助 Amazon Redshift Spectrum 直接查询 Amazon S3 数据湖中的原始或温热数据，或通过高效方式将加工后的结果加载至 Amazon Redshift 供关键业务查询，实现湖仓一体的协同分析。</p><h2>架构应用</h2><p>根据该品牌的业务需求与实际挑战，我们构建了如下图所示的 AWS 现代化数据架构。该架构整合多项 AWS 云服务，以 Amazon Redshift 为中枢，打造统一、高效、弹性的企业级数据平台。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047605337" alt="图片" title="图片"/></p><h2>项目价值</h2><p>项目上线后，该品牌的数据平台实现了全面升级：<br/>分析效率与深度双提升：依托Amazon Redshift Serverless的弹性能力，高并发即席查询响应速度提升 50% 以上，无资源排队等待，基于 Amazon Redshift 的并行计算能力，完成跨区域销售数据的深度拆解。<br/>成本实现精细控制：通过智能调度与 Amazon Redshift RA3 节点、Serverless 模式的结合，在支撑更大数据量与更复杂分析的同时，整体分析层计算成本节约超 35%。<br/>数据信任与协作文化建立：统一的数据资产目录与可视化血缘，让业务部门能自主、放心地使用 Amazon Redshift 中的数据，数据团队从繁琐的 “取数” 工作中解放，专注于更高价值的模型构建。</p><p>该品牌的实践表明，在数据量激增的时代，云数据仓库已不仅是存储历史的“档案馆”，更是驱动实时业务的“决策大脑”。数新智能通过 CyberData 平台与 Amazon Redshift 云原生服务的深度融合，不仅帮助客户实现了工具链的统一，更关键在于深度激活了 Amazon Redshift 在性能、弹性与成本方面的原生优势，将其转化为可持续的竞争优势。</p><p>我们认为，未来的数据平台不应是各种独立工具的简单堆砌，而应是一个体验统一、引擎智能、治理内嵌的有机整体。CyberData平台的核心理念，正是将企业从“运维复杂基础设施”的沉重负担中解放出来，回归到“专注数据价值创造”的本质上来。</p>]]></description></item><item>    <title><![CDATA[免费SSL证书全解析：类型、优势、适用场景与选型指南 SSL证书的小韩 ]]></title>    <link>https://segmentfault.com/a/1190000047605350</link>    <guid>https://segmentfault.com/a/1190000047605350</guid>    <pubDate>2026-02-11 14:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>免费SSL证书全解析：类型、优势、适用场景与选型指南</h2><h3>一、<a href="https://link.segmentfault.com/?enc=tj%2B5I%2BQ3S1bvNM382Cdsmw%3D%3D.%2Fd%2FE3K%2FLy3ahzz8ZbktpGrekSrRKACVyosUGG3SVTJ5AS%2Fmu%2FOs8sRm7tHcSveIW" rel="nofollow" target="_blank">免费SSL证书类型分层与特性</a></h3><h4>1. 按验证等级划分</h4><p>免费SSL证书根据身份验证强度，可分为三个核心层级：</p><ul><li><strong>域名验证型（DV）</strong> ：仅需验证域名所有权，通过DNS记录上传、文件验证或邮箱验证即可完成签发，通常在5-10分钟内完成，是目前免费证书的主流类型。此类证书仅实现基础加密功能，不显示企业信息，适合个人网站和小型项目。</li><li><strong>组织验证型（OV）</strong> ：除域名所有权外，还需验证申请企业的工商注册信息和合法性，签发周期为1-3个工作日，会在浏览器中显示企业名称，能显著提升用户信任度。多数CA仅提供短期免费试用版，长期使用需付费升级。</li><li><strong>国密SSL证书</strong>：采用SM2/SM4国密加密算法，符合《中华人民共和国密码法》合规要求，适用于政务、金融等对加密标准有明确规定的场景，部分国产CA提供免费DV版本的国密证书。</li></ul><h4>2. 按域名覆盖范围划分</h4><p>免费SSL证书按覆盖维度可分为三类：</p><ul><li><strong>单域名证书</strong>：仅对一个特定域名生效，如<code>[www.joyssl.com](https://www.joyssl.com/index.html?nid=59)</code>，不包含子域名，是免费DV证书的标准配置。</li><li><strong>通配符证书</strong>：可保护主域名及所有子域名，如<code>*.joysl.com</code>能覆盖<code>blog.joyssl.com</code>、<code>shop.joyssl.com</code>等，适合多子域名架构的网站。</li><li><strong>多域名证书</strong>：支持同时验证多个独立域名，如<code>example.com</code>和<code>example.net</code>，免费版本通常限制域名数量在3-5个以内。</li></ul><h3>二、主流免费SSL证书品牌对比（注册码230959）</h3><p>表格</p><table><thead><tr><th>品牌</th><th>核心类型</th><th>有效期</th><th>国密支持</th><th>续期方式</th><th>部署便捷性</th></tr></thead><tbody><tr><td>JoySSL</td><td>DV单域名/通配符</td><td>90天</td><td>支持SM2</td><td>自动续期+手动续期</td><td>云平台一键部署</td></tr><tr><td>Let’s Encrypt</td><td>DV单域名/通配符</td><td>90天</td><td>不支持</td><td>Certbot脚本自动续期</td><td>需手动配置脚本</td></tr><tr><td>TrustAsia</td><td>DV单域名</td><td>90天</td><td>支持SM2</td><td>手动续期</td><td>云市场集成部署</td></tr><tr><td>Wotrus</td><td>DV单域名</td><td>90天</td><td>支持SM2</td><td>自动续期</td><td>API自助申请</td></tr></tbody></table><h4>JoySSL的差异化优势</h4><ol><li><strong>本地化适配能力</strong>：国内部署的节点延迟低于50ms，相比国际品牌减少30%以上的TLS握手时间，适配国内浏览器和服务器环境。</li><li><strong>国密生态兼容性</strong>：完整支持SM2/SM4国密算法，可无缝对接国产密码模块和操作系统，满足政务、金融等行业的合规要求。</li><li><strong>云平台深度集成</strong>：已接入阿里云、腾讯云、百度智能云等主流云市场，支持一键申请、自动部署与续期，降低运维成本。</li><li><strong>中文客服支持</strong>：提供7×24小时中文技术支持，相比国际品牌的社区论坛响应模式，更贴合国内用户的服务需求。</li></ol><h3>三、免费SSL证书的优势与适用场景</h3><h4>核心优势</h4><ol><li><strong>安全合规价值</strong>：满足《网络安全法》对敏感数据传输的加密要求，同时符合搜索引擎对HTTPS的收录偏好，能避免浏览器显示“不安全”提示。</li><li><strong>成本优化效果</strong>：零成本获得基础加密防护，对比付费DV证书每年500-1000元的成本，长期使用可节省超90%的安全投入。</li><li><strong>部署便捷性</strong>：主流免费证书均支持ACME协议自动化申请，无需复杂技术配置，个人站长和中小企业可快速完成部署。</li></ol><h4>适用场景匹配</h4><p>表格</p><table><thead><tr><th>网站类型</th><th>推荐证书类型</th><th>选型理由</th></tr></thead><tbody><tr><td>个人博客、开源项目</td><td>JoySSL免费DV证书</td><td>快速签发，零成本满足加密需求</td></tr><tr><td>中小企业官网</td><td>JoySSL国密证书</td><td>符合国内合规要求，兼顾安全与成本</td></tr><tr><td>政务、教育类网站</td><td>JoySSL免费DV证书</td><td>适配国内网络环境，支持国密算法</td></tr><tr><td>电商、金融交易网站</td><td>付费OV/EV证书</td><td>更高信任度，满足交易场景合规要求</td></tr></tbody></table><h3>四、免费SSL证书选型与部署指南</h3><h4>选型避坑要点</h4><ol><li><strong>关注有效期与续期机制</strong>：免费DV证书有效期均为90天，需启用自动续期功能或设置续期提醒，避免证书过期导致网站无法访问。</li><li><strong>验证浏览器兼容性</strong>：优先选择被主流浏览器信任的品牌，如JoySSL、Let’s Encrypt等，防止出现证书不被识别的情况。</li><li><strong>评估技术支持能力</strong>：国内品牌通常提供中文客服支持，相比国际品牌的社区响应模式，更适合技术能力有限的用户。</li></ol><h4>部署最佳实践</h4><ol><li><strong>优先选择DNS验证</strong>：无需修改网站代码，验证成功率可达99%以上，且支持通配符证书申请。</li><li><strong>配置HTTP严格传输安全（HSTS）</strong> ：在服务器中添加HSTS响应头，强制浏览器使用HTTPS访问，避免HTTP降级攻击。</li><li><strong>启用OCSP Stapling</strong>：减少TLS握手过程中的OCSP查询延迟，提升网站访问速度和稳定性。</li></ol>]]></description></item><item>    <title><![CDATA[免费泛域名证书怎么申请 才高八斗的杯子_dS2Fpp ]]></title>    <link>https://segmentfault.com/a/1190000047605353</link>    <guid>https://segmentfault.com/a/1190000047605353</guid>    <pubDate>2026-02-11 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>什么是通配符证书？</h4><p><strong>通配符证书（Wildcard SSL Certificate）</strong> 是一种特殊的SSL/TLS证书，使用单个证书保护一个主域名及其所有同级子域名。证书中的通配符表示为星号（*），例如：</p><ul><li><p><code>*.example.com</code> 可保护：</p><ul><li><code>www.example.com</code></li><li><code>mail.example.com</code></li><li><code>shop.example.com</code></li><li><code>blog.example.com</code></li><li>以及其他任何同级子域名<br/><img width="700" height="400" referrerpolicy="no-referrer" src="/img/bVdh3qn" alt="" title=""/></li></ul></li></ul><h4>为什么选择JoySSL通配符证书？</h4><p><strong>经济高效</strong> - 一证多用，节省成本  <br/><strong>管理简便</strong> - 统一管理多个子域名  <br/><strong>安全可靠</strong> - 256位加密强度，支持SHA-2算法  <br/><strong>兼容性好</strong> - 支持99.9%的浏览器和移动设备  <br/><strong>快速签发</strong> - 验证通过后快速颁发</p><h4><strong><a href="https://link.segmentfault.com/?enc=02XsMHX2MQ%2FjCv%2F8mZ3pXg%3D%3D.8mYg55ULQ4YGYlJp2JuoCsbga%2BZGBN3DJGSpiUatpEFIoc3h2yskzz3809PLkOW%2Fq7elzv5ysp%2Bo8G6%2F7PBFhJMHgqVhiZJPiWpdzcK%2BDcc%3D" rel="nofollow" target="_blank">通配符SSL证书如何快速申请</a>：</strong></h4><p>1、首先，您需要选择一个可靠的证书颁发机构来为您签发通配符证书。</p><p>打开JoySSL，注册账号填写注册码<strong>230970</strong>获取协助配置安装服务以及优惠券。</p><p>2.流程二：生成和提交CSR  <br/>需要生成证书CSR，随后递交给SSL证书颁发组织。</p><p>3.流程三：验证域名所有权和公司信息  <br/>验证域名所有权，提交公司真实信息等待验证。</p><p>4.流程四：审签SSL证书  <br/>根据信息审核，将以邮件或是电话的形式验证单位组织信息，证书颁发机构完成SSL证书的审核。</p><p>5.流程五：将成功签发的 SSL证书安装在服务器上。</p>]]></description></item><item>    <title><![CDATA[为什么我最终选择用 WebSocket 获取股票与外汇实时行情 sydney ]]></title>    <link>https://segmentfault.com/a/1190000047605290</link>    <guid>https://segmentfault.com/a/1190000047605290</guid>    <pubDate>2026-02-11 13:02:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>行情延迟与接口不稳定</h2><p>作为一个长期做量化和可视化系统开发的个人交易者，我深刻体会到一个问题——数据的延迟和断线带来的影响远比想象严重。<br/>早期我试过不少免费行情 API，看起来数据齐全，但在实际使用中，经常出现推送延迟几秒甚至中断的情况。尤其在短线测试时，这种延迟直接导致图表跳帧、策略误触发，整个系统的可靠性大打折扣。<br/>在这种场景下，我需要的不是“能用就好”的接口，而是一个能长期稳定运行、实时推送低延迟数据的解决方案。</p><h2>问题本质：实时数据的完整性</h2><p>我的系统通常同时展示几支核心股票（如 AAPL、TSLA 等）以及欧元兑美元、美元兑日元这样的汇率对。<br/>如果数据更新不同步或接口暂时不可用，前端展示的图表就会出现跳变或空窗期。对策略而言，任何毫秒级的偏差，都可能导致错误判断。<br/>一次性拉取历史数据虽方便，但在实时监控或策略测试场景中，这种模式显然不够用。<br/>因此，我开始重新审视接口标准，重点放在以下指标上：</p><ul><li>稳定性：长期连接不中断，且能自动重连。</li><li>实时性：延迟低于秒级，推送流畅。</li><li>数据完整性：除价格外可获取成交量、涨跌幅、汇率等指标。</li><li><p>扩展性：同一个接口能支持股票、外汇，甚至未来扩展至加密资产。</p><h2>实践方案：WebSocket 实时订阅</h2><p>我后来用 AllTick API 做了一个测试项目。它提供了 WebSocket 实时推送功能，能同时订阅多个标的。<br/>相较传统 HTTP 轮询，这种方式在实时性和资源开销上都有明显优势。<br/>以下是我在项目中使用的 Python 示例，可以同时订阅苹果股价和欧元兑美元汇率：</p></li></ul><pre><code>python
import websocket
import json

url = "wss://realtime.alltick.co/ws"  # AllTick 实时推送地址

def on_message(ws, message):
    data = json.loads(message)
    print(f"{data['symbol']} 当前价格: {data['price']}")

def on_open(ws):
    # 同时订阅股票和外汇
    subscribe_data = {
        "action": "subscribe",
        "symbols": ["AAPL.US", "EURUSD"]
    }
    ws.send(json.dumps(subscribe_data))

ws = websocket.WebSocketApp(url, on_message=on_message, on_open=on_open)
ws.run_forever()</code></pre><h2>使用体验：实时性与稳定性的提升</h2><p>在使用过程中，我先从少量标的开始测试。AllTick 的推送流保持稳定，没有出现断线或掉包情况，价格更新流畅，能很好地满足前端可视化刷新以及策略引擎的数据输入需求。<br/>特别是在做实时图表和策略结果回测联动时，延迟的提升非常明显，几乎可以实现秒级同步更新。</p><h2>结论</h2><p>这次的尝试让我更清楚一个事实：<br/>数据接口的质量，决定了策略实验的可信度和系统的整体体验。<br/>选择合适的股票与外汇实时数据接口，不是为了“拿数据”，而是为了让系统能稳定、高效地运行；更低的延迟、更高的一致性，意味着更少的异常调试。<br/>如果要在项目中兼顾实时性与多市场数据扩展，我推荐直接从 WebSocket 接口起步，而不是等待轮询方案优化。<br/>像 AllTick 这种集合多市场行情源的接口，让数据流更连贯，也提高了开发效率。<br/><img width="723" height="371" referrerpolicy="no-referrer" src="/img/bVdnUtp" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[建议Java工程师都要学习一下Go语言 王中阳讲编程 ]]></title>    <link>https://segmentfault.com/a/1190000047605332</link>    <guid>https://segmentfault.com/a/1190000047605332</guid>    <pubDate>2026-02-11 13:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>开篇：这不是一篇劝退Java的文章</h2><p>首先声明，我不是来劝你放弃Java的。</p><p>我深知Spring全家桶在企业级应用开发中的统治地位，写业务逻辑、搞复杂架构、做ERP系统，Java依然是当之无愧的王者。</p><p>但是，<strong>作为一名有追求的Java工程师，我强烈建议你把Go语言加入你的武器库。</strong></p><p>为什么？因为时代变了。</p><h2>理由一：突破"应用层"的天花板</h2><p>你有没有发现，当你把Java学通了，Spring源码看完了，JVM调优搞定了，似乎就触碰到了一层隐形的天花板？</p><p>往上看，是业务架构；<strong>往下走，是基础设施。</strong></p><p>而今天的基础设施，<strong>几乎是Go语言的天下</strong>：</p><ul><li><strong>容器编排</strong>：Kubernetes (Go)</li><li><strong>容器引擎</strong>：Docker (Go)</li><li><strong>服务网格</strong>：Istio (Go)</li><li><strong>监控告警</strong>：Prometheus (Go)</li><li><strong>配置中心</strong>：Etcd (Go)</li><li><strong>网关代理</strong>：Traefik, Envoy (Go周边)</li></ul><p>如果你只会Java，当Kubernetes集群出现诡异调度问题时，当Prometheus抓取不到数据时，你只能看着黑盒干着急。</p><p><strong>学会Go，你就不再只是一个"写接口的"，你拥有了窥探和掌控整个云原生基础设施的能力。</strong></p><h2>理由二：Java太重，Go太快</h2><p>Java被人诟病最多的就是"重"。</p><p>写一个简单的CLI工具，或者一个轻量级的Sidecar代理：</p><ul><li><strong>Java</strong>：启动慢，吃内存，还需要装JRE。</li><li><strong>Go</strong>：编译成一个二进制文件，丢上去就能跑，启动瞬间完成，内存占用极低。</li></ul><p>在微服务架构中，越来越多的辅助组件（Agent、Sidecar、Forwarder）都在转向Go。作为Java工程师，如果你能用Go快速写一个高性能的辅助工具，解决生产环境的燃眉之急，这绝对是你的核心竞争力。</p><h2>理由三：Java工程师学Go，简直是降维打击</h2><p>很多Java同学不敢学Go，觉得是新语言，门槛高。</p><p><strong>大错特错！</strong></p><p>Go语言的设计哲学是"做减法"。相比于Java复杂的继承、多态、注解、反射，Go简单得令人发指。</p><p>对于Java工程师来说，学Go几乎是无痛的，因为核心概念完全互通：</p><table><thead><tr><th align="left">Java概念</th><th align="left">Go概念</th><th align="left">区别</th></tr></thead><tbody><tr><td align="left">Class</td><td align="left">Struct</td><td align="left">没有继承，只有组合</td></tr><tr><td align="left">Interface</td><td align="left">Interface</td><td align="left">鸭子类型（隐式实现），更灵活</td></tr><tr><td align="left">Thread</td><td align="left">Goroutine</td><td align="left">极轻量级，启动几十万个都没事</td></tr><tr><td align="left">Try-Catch</td><td align="left">if err != nil</td><td align="left">显式处理，代码逻辑更清晰</td></tr><tr><td align="left">Maven</td><td align="left">Go Mod</td><td align="left">依赖管理更简单</td></tr></tbody></table><h2>实战对比：一眼看懂Go的"简单"</h2><p>我们来看一个最简单的HTTP服务，感受一下两者的区别。</p><h3>Java (Spring Boot)</h3><p>你需要配置Controller，注解，依赖注入...</p><pre><code class="java">@RestController
public class HelloController {
    
    @GetMapping("/hello")
    public String hello(@RequestParam String name) {
        return "Hello, " + name;
    }
}</code></pre><p>看似代码少，但这背后需要庞大的Spring框架支撑，启动时间几秒到几十秒不等。</p><h3>Go (Gin)</h3><p>代码直观，逻辑从上到下，没有魔法。</p><pre><code class="go">package main

import "github.com/gin-gonic/gin"

func main() {
    r := gin.Default()
    
    r.GET("/hello", func(c *gin.Context) {
        name := c.Query("name")
        c.String(200, "Hello, %s", name)
    })
    
    r.Run() // 监听 0.0.0.0:8080
}</code></pre><p>编译成二进制文件后，只有十几MB，没有任何依赖，启动耗时毫秒级。</p><h2>核心思维转变：从"对象"到"组合"</h2><p>Java工程师转Go，最大的障碍不是语法，而是思维。</p><p>Java喜欢<strong>层层封装</strong>：<br/><code>Controller -&gt; Service -&gt; Manager -&gt; DAO -&gt; Entity</code></p><p>Go喜欢<strong>简单直接</strong>：<br/><code>Handler -&gt; Logic -&gt; Repo</code></p><p>Java喜欢<strong>继承</strong>：<br/><code>BaseController -&gt; UserController</code></p><p>Go喜欢<strong>组合</strong>：</p><pre><code class="go">type UserHandler struct {
    *BaseHandler // 组合
    UserService  *UserService
}</code></pre><p>一旦你习惯了Go的这种"乐高积木"式的组合思维，你会发现代码变得异常清晰，维护起来也轻松很多。</p><h2>结语：技多不压身</h2><p>最后，我想说的是：<strong>学习Go并不是要你抛弃Java。</strong></p><ul><li><strong>做复杂业务系统</strong>，Java依然是首选，生态无敌。</li><li><strong>做中间件、工具、高并发网关、K8s插件</strong>，Go是神兵利器。</li></ul><p>作为一名资深Java工程师，拥有Java的架构思维，再加上Go的工程效率，你将成为团队中不可或缺的"全栈基础设施专家"。</p><p>别犹豫了，今天就下载Go，写下你的第一个 <code>fmt.Println("Hello World")</code> 吧。</p><blockquote><p><strong>⚡️ 别把时间浪费在低效复习上</strong></p><p>很多人复习抓不住重点。作为过来人，我分析了100+份大厂面试记录，将 <strong>Go/Java/AI 的核心考察点、高频题、易错点</strong> 浓缩进了一份 PDF。</p><p><strong>不搞虚的，全是干货。</strong></p><p><strong>加我微信：wangzhongyang1993</strong>，备注 <strong>【面经】</strong> 免费发你，立即纠正你的复习方向，把时间用在刀刃上。</p><p>wangzhongyang.com 也欢迎大家直接访问我的官网，里面有Go / Java / AI 的资料，<strong>免费学习</strong>！</p></blockquote>]]></description></item><item>    <title><![CDATA[行情监控开发：股票停牌复牌的实时监测方案与代码实现 Jackyy ]]></title>    <link>https://segmentfault.com/a/1190000047605240</link>    <guid>https://segmentfault.com/a/1190000047605240</guid>    <pubDate>2026-02-11 12:04:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在金融行情监控系统的开发过程中，开发者常会遇到股票停牌相关的技术落地难题：停牌触发原因多样且时长无统一标准，导致标的状态展示、实时预警功能难以实现；同时缺乏可直接复用的接口方案，无法高效获取停牌/复牌实时数据，最终影响行情系统对前端交易、研究场景的支撑能力。</p><p>本文从金融开发的实际需求出发，先梳理股票停牌的核心类型与数据特征，再给出基于WebSocket接口的复牌状态实时监测实现方案，提供可直接复用的Python代码，解决行情监控系统中停牌状态监测的实际开发问题。</p><h2>一、开发痛点与核心需求</h2><p>针对股票停牌状态监测的开发场景，核心需解决<strong>数据标准化</strong>和<strong>状态实时化</strong>两大问题，具体的技术与数据需求可分为两类：</p><ol><li><strong>基础数据需求</strong>：明确不同停牌类型的触发场景、时长范围，为系统中标的停牌状态的基础研判、数据建模提供标准化依据；</li><li><strong>技术对接需求</strong>：获取可无缝接入自研系统的实时推送接口，实现停牌/复牌状态的低延迟获取，同时支持将停牌天数、复牌日期等数据与系统可视化模块结合，适配行情面板的展示需求。</li></ol><h2>二、停牌核心类型与数据特征</h2><p>市场中股票停牌主要分为三类，其触发场景和时长特征直接决定了行情监控系统的开发与数据建模逻辑，三类停牌的核心信息及差异如下：</p><table><thead><tr><th>停牌类型</th><th>触发场景</th><th>时长范围</th></tr></thead><tbody><tr><td>重大事项公告停牌</td><td>公司发布资产调整、重大合同签署等重大公告</td><td>数日~数周（无固定值）</td></tr><tr><td>异常波动停牌</td><td>个股价格/成交量出现交易所认定的异常异动</td><td>数小时~数日（无固定值）</td></tr><tr><td>信息披露停牌</td><td>公司发布季报、年报等重要财报前</td><td>1~3天（短周期固定）</td></tr></tbody></table><p>为便于开发者在系统开发中做数据验证和功能测试，以下提供贴近真实市场的模拟数据集，可直接用于开发调试：</p><h3>停牌时长模拟数据</h3><table><thead><tr><th>股票</th><th>停牌原因</th><th>停牌天数</th></tr></thead><tbody><tr><td>A</td><td>重大事项公告</td><td>12</td></tr><tr><td>B</td><td>异常波动</td><td>2</td></tr><tr><td>C</td><td>信息披露</td><td>1</td></tr></tbody></table><h3>复牌状态模拟数据</h3><table><thead><tr><th>股票</th><th>停牌天数</th><th>复牌日期</th></tr></thead><tbody><tr><td>A</td><td>12</td><td>2026-02-15</td></tr><tr><td>B</td><td>2</td><td>2026-02-05</td></tr><tr><td>C</td><td>1</td><td>2026-02-04</td></tr></tbody></table><p>从模拟数据可直观看出：重大事项公告类停牌时长最长，信息披露类最短，这一规律与市场实际高度契合，可作为系统开发中状态判断的核心参考。</p><h2>三、核心技术实现：基于AllTick API的实时监测</h2><p>针对停牌/复牌状态的实时获取需求，采用WebSocket接口实现数据的实时推送是最优解，以下为基于AllTick API的Python实现代码，代码可直接复用，无需修改，适配主流金融行情系统的技术栈。</p><pre><code class="python">from alltick.websocket import AllTickRealtime

def on_message(message):
    data = message.get("data", {})
    if "halt_status" in data:
        status = data["halt_status"]
        if status == "halted":
            print(f"{data['symbol']} 已停牌")
        elif status == "resumed":
            print(f"{data['symbol']} 已复牌")

# 初始化实时连接
ws = AllTickRealtime(
    api_key="你的API_KEY",
    on_message=on_message
)
# 订阅目标股票停牌状态
ws.subscribe(["AAPL", "MSFT", "TSLA"])
ws.run_forever()</code></pre><h3>开发实操提示</h3><ol><li>接入前需完成AllTick API的权限申请，将代码中<code>你的API_KEY</code>替换为实际有效密钥；</li><li>该接口可直接与Python可视化库（Matplotlib/Plotly）、前端可视化框架（ECharts/Highcharts）结合，实现停牌天数趋势、复牌日期标注的可视化展示；</li><li>生产环境部署时，建议增加<strong>异常处理逻辑</strong>，包括网络断连自动重连、数据格式校验、空值过滤，提升接口在行情系统中的稳定性。</li></ol><h2>四、系统集成拓展</h2><p>将上述技术方案与自研行情监控系统结合时，可从两个维度实现功能拓展，让停牌状态监测更贴合实际开发与业务使用需求：</p><ol><li><strong>状态预警</strong>：在<code>on_message</code>函数中增加消息推送、弹窗提醒等逻辑，当标的触发停牌/复牌时，向系统前端推送实时预警；</li><li><strong>数据持久化</strong>：将获取到的停牌状态、停牌天数、复牌日期等数据写入数据库（MySQL/Redis），为后续的行情数据分析、系统功能迭代提供历史数据支撑。</li></ol><h2>五、方案的技术与业务价值</h2><p>这套「停牌数据标准化梳理+WebSocket接口实时实现」的方案，对金融行情监控系统开发具备双重核心价值：</p><ol><li><strong>技术价值</strong>：提供了金融领域实时行情数据获取的标准化接口实现范式，该方案可复用至个股价格、成交量等其他实时行情数据的获取场景，降低开发成本；</li><li><strong>业务价值</strong>：解决了行情监控系统中停牌状态监测的核心痛点，实现了标的停牌/复牌状态的实时化、可视化展示，让系统能更精准地为前端交易、研究场景提供数据支撑，提升行情系统的精细化程度。</li></ol><h2>总结</h2><p>股票停牌状态监测的核心难点在于<strong>数据无标准</strong>和<strong>状态不实时</strong>，本文通过梳理三类停牌的核心数据特征，解决了数据标准化问题；同时提供WebSocket实现代码，可直接复用至自研系统，实现停牌/复牌状态的低延迟获取。</p><p>该方案从金融开发的实际场景出发，所有代码和数据均可直接用于开发调试与功能落地，适配主流量化交易、行情监控系统的技术栈，能有效提升停牌状态监测功能的开发效率，助力行情系统的功能完善与体验优化。</p>]]></description></item><item>    <title><![CDATA[企业用IP离线库选哪个品牌好 科技块儿 ]]></title>    <link>https://segmentfault.com/a/1190000047605254</link>    <guid>https://segmentfault.com/a/1190000047605254</guid>    <pubDate>2026-02-11 12:04:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、什么是IP离线库？</h2><p>IP离线库是企业将IP地址信息存储在本地数据库中的一种方式。与在线IP查询不同，离线库的IP数据不依赖于实时互联网连接，而是由企业根据需求定期下载更新。这使得企业可以在没有互联网连接的环境下进行IP地址查询，且查询速度较快，适用于数据量大、查询频繁的场景。</p><p>IP离线库通常包含的内容包括：IP归属地、ISP信息、IP类型、使用代理情况、风险等级等。它为企业提供了多维度的IP信息，尤其适用于需要进行精准营销、风险管理、网络安全监控等任务的企业。</p><h2>二、IP离线库在企业中的应用场景</h2><h3>网络安全防护</h3><p>在企业的网络安全体系中，IP离线库发挥着不可或缺的作用。通过实时监控和查询IP地址的归属地、类型及使用情况，企业可以有效识别潜在的安全威胁，及时阻止来自恶意IP地址的攻击。例如，通过查找是否有大量来自同一IP的登录尝试，企业可以发现潜在的暴力破解行为，从而采取必要的防范措施。</p><h3>精准营销与广告投放</h3><p>在广告投放与精准营销方面，IP离线库也能提供帮助。企业通过分析用户的IP地址，判断其地理位置、设备类型等信息，从而制定更具针对性的营销策略。借助IP离线库，企业可以实现更精确的广告定向投放，提高营销效果。</p><h3>反欺诈与风险评估</h3><p>通过查询IP地址的历史使用记录和风险评分，企业可以在进行用户身份认证时有效防止欺诈行为。例如，银行、电商平台等常常利用IP离线库查询客户IP地址，识别是否存在风险行为（如使用VPN或代理的可疑IP）。这种技术有助于降低欺诈风险，提升企业的安全性。<br/><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnUsP" alt="企业用IP离线库选哪个品牌好" title="企业用IP离线库选哪个品牌好"/></p><h2>三、选择IP离线库时需要考虑的关键因素</h2><h3>数据准确性</h3><p>数据准确性是选择IP离线库时最重要的因素之一。IP离线库中的数据必须保持高质量和准确性，才能确保企业在进行IP查询时得到可靠的结果。企业应选择那些提供多维度、详细数据来源的品牌，以确保查询结果的准确性。</p><h3>更新频率</h3><p>由于IP地址的动态变化，IP离线库的更新频率也是至关重要的。企业应选择那些定期更新数据源的IP离线库品牌，以确保所查询的数据是最新的。这对于防止IP库信息过时、失效至关重要，尤其是在防止网络攻击和诈骗方面。</p><h3>区域覆盖</h3><p>对于需要全球范围内查询IP的企业，IP离线库的区域覆盖广度是一个不容忽视的因素。选择支持全球范围的IP库，可以帮助企业全面了解不同区域的IP地址信息，满足跨国业务运营的需求。</p><h3>查询速度</h3><p>企业的查询效率对日常运营的影响也非常大。在数据量大的情况下，查询速度尤为重要。因此，选择响应快速、查询高效的IP离线库品牌，可以显著提升企业的工作效率，避免因查询延迟而影响决策。</p><h3>兼容性与扩展性</h3><p>企业的需求可能随着业务的扩大而发生变化。因此，选择一个具备良好兼容性和扩展性的IP离线库品牌至关重要。品牌应提供丰富的API接口、支持多平台集成，以便企业根据自身需求进行定制和拓展。</p><h2>四、推荐的IP离线库品牌及其优缺点</h2><p>以下是目前市场上几款知名的IP离线库品牌，适用于不同企业需求的选择。</p><table><thead><tr><th>品牌名</th><th>优势</th><th>缺点</th></tr></thead><tbody><tr><td>IP数据云</td><td>提供全面的全球IP数据，更新频率高，支持API接口，查询速度快</td><td>高级功能需付费，部分高精度数据需额外购买</td></tr><tr><td>IPnews</td><td>提供精确的IP地理位置和代理检测，适用于跨国企业</td><td>数据精度有限，官网套餐仅到城市级</td></tr><tr><td>IPinfo</td><td>数据准确性高，支持丰富的API接口，适合开发者使用</td><td>部分高级功能价格较高，适合较为复杂的业务场景</td></tr><tr><td>Geotargetly</td><td>提供精准的地域定向能力，特别适合精准营销</td><td>只支持部分地区的详细数据，可能不适合跨国运营的企业</td></tr><tr><td>ipstack</td><td>提供高效的API接口和多语言支持，数据丰富</td><td>数据更新频率较低，且支持的地域覆盖较少</td></tr></tbody></table><p><em>*数据来源网络，以官网为准</em></p><p>因此，在选择IP离线库品牌时，企业应根据自身需求，如查询速度、数据准确性、区域覆盖等方面，进行综合考虑。IP数据云凭借其数据更新频率高、全球范围覆盖、查询速度快，成为许多企业的首选。而对于跨国业务，IPnews和IPinfo提供的精确数据和全面支持也值得关注。</p><h2>五、结论</h2><p>选择适合企业需求的IP离线库品牌是一个需要综合考虑多方面因素的过程。通过深入了解IP离线库的应用场景、选购标准和市场上的主流品牌，企业可以做出更加理性和精准的决策，为网络安全、精准营销等任务提供有力支持。无论是提升网络防护能力，还是加强风险管理，选择一个高效且可靠的IP离线库品牌，都是企业顺利发展的关键一步。</p>]]></description></item><item>    <title><![CDATA[[后端架构] Python处理金融即时通讯：WebSocket客户端设计模式 EmilyLi ]]></title>    <link>https://segmentfault.com/a/1190000047605256</link>    <guid>https://segmentfault.com/a/1190000047605256</guid>    <pubDate>2026-02-11 12:03:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在开发金融类应用时，最棘手的部分往往不是复杂的算法，而是如何稳定、高效地处理实时数据流。作为一名在一线编写交易系统的开发者，今天想和大家聊聊 A 股实时行情的接入方案。</p><p>需求分析：为什么不用 HTTP？ HTTP 协议是无状态的，每次请求都需要带上完整的 Header，且需要经历三次握手。在需要亚秒级响应的行情监控场景下，这种开销是不可接受的。我们需要的是一种 Keep-Alive 的长连接机制，WebSocket 无疑是最佳选择。</p><p>协议层实现逻辑 我们的目标是构建一个能够长期运行、自动重连的客户端。</p><p>Transport 层：使用 websocket-client 库维护底层 TCP 连接。</p><p>Protocol 层：解析特定的 JSON 协议包。以 AllTick 的协议为例，其数据包结构紧凑，适合高频传输。</p><p>Application 层：将解析后的数据分发给策略引擎或 UI 界面。</p><p>代码实战：异步回调设计 以下代码展示了如何利用回调函数（Callback）模式来处理异步推送的数据流。这种设计模式可以避免主线程阻塞。</p><pre><code>import pandas as pd

df = pd.DataFrame(columns=["code", "price", "volume", "time"])

def on_message(ws, message):
    data = json.loads(message)
    if "data" in data:
        for item in data["data"]:
            df.loc[len(df)] = [item['s'], item['p'], item['v'], item['t']]
            print(df.tail(1))
</code></pre><p>数据持久化与缓存 在高并发场景下，直接写库（如 MySQL）可能会成为瓶颈。通常我们会先用 Pandas 在内存中做一层缓存（Buffer），或者推送到 Redis 队列中。这里展示一个简单的 Pandas 内存处理方案：</p><pre><code>import pandas as pd

df = pd.DataFrame(columns=["code", "price", "volume", "time"])

def on_message(ws, message):
    data = json.loads(message)
    if "data" in data:
        for item in data["data"]:
            df.loc[len(df)] = [item['s'], item['p'], item['v'], item['t']]
            print(df.tail(1))</code></pre><p>技术总结 通过 WebSocket，我们实现了一个低延迟的行情消费端。这种架构不仅适用于股票，同样适用于期货、数字货币等任何对时效性要求极高的金融衍生品交易场景。<br/><img width="723" height="408" referrerpolicy="no-referrer" src="/img/bVdnSar" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[RAG(检索增强生成)原理与实践 ꯭꯭听꯭风꯭者꯭ ]]></title>    <link>https://segmentfault.com/a/1190000047605265</link>    <guid>https://segmentfault.com/a/1190000047605265</guid>    <pubDate>2026-02-11 12:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在大语言模型（LLM）蓬勃发展的今天，如何让AI更准确地回答特定领域的问题成为了一个关键挑战。RAG（Retrieval-Augmented Generation，检索增强生成）技术应运而生，它通过结合外部知识库和生成模型，显著提升了AI回答的准确性和时效性。</p><p>本文将深入探讨RAG的核心原理，重点解析<strong>向量检索</strong>和<strong>上下文注入</strong>两大关键技术，并提供实践指导。</p><hr/><h2>一、RAG是什么？</h2><h3>1.1 核心思想</h3><p>RAG的核心思想非常直观：在生成答案之前，先从知识库中检索相关信息，然后将这些信息作为上下文提供给大语言模型，让模型基于这些"参考资料"来生成更准确的回答。</p><p>这就像是让AI在开卷考试而不是闭卷考试——它可以查阅资料后再作答。</p><h3>1.2 为什么需要RAG？</h3><p>传统LLM面临几个关键问题：</p><ul><li><strong>知识时效性</strong>：模型的知识截止于训练时间，无法获取最新信息</li><li><strong>幻觉问题</strong>：模型可能生成看似合理但实际错误的内容</li><li><strong>专业领域知识不足</strong>：通用模型对特定领域的深度知识有限</li><li><strong>成本问题</strong>：频繁微调大模型成本高昂</li></ul><p>RAG通过外部知识检索优雅地解决了这些问题，无需重新训练模型。</p><hr/><h2>二、向量检索：RAG的核心引擎</h2><h3>2.1 什么是向量检索？</h3><p>向量检索是RAG系统的第一步，也是最关键的一步。它的任务是从海量文档中快速找出与用户问题最相关的内容。</p><h4>文本向量化</h4><p>文本向量化（Embedding）是将文本转换为高维向量的过程：</p><pre><code>"什么是机器学习？" → [0.12, -0.34, 0.56, ..., 0.89]  # 维度通常为384-1536</code></pre><p>向量的特点：</p><ul><li><strong>语义相似的文本，向量距离更近</strong></li><li><strong>向量可以进行数学运算</strong>（相似度计算）</li><li><strong>降维后可视化</strong>（理解语义空间）</li></ul><h4>常用的Embedding模型</h4><ul><li><strong>OpenAI text-embedding-3-small/large</strong>：性能强大，支持多语言</li><li><strong>sentence-transformers</strong>：开源方案，适合中文</li><li><strong>BGE系列</strong>：国内优秀的开源模型</li><li><strong>m3e</strong>：专门针对中文优化</li></ul><h3>2.2 向量检索的工作流程</h3><pre><code>用户问题 → Embedding模型 → 查询向量 → 向量数据库 → Top-K 相似文档</code></pre><p><strong>步骤详解：</strong></p><ol><li><p><strong>文档预处理</strong>：</p><ul><li>文档切片（Chunking）：将长文档分割成适当大小的片段（通常300-1000 tokens）</li><li>向量化：使用Embedding模型将每个片段转换为向量</li><li>存储：将向量及元数据存入向量数据库</li></ul></li><li><p><strong>查询处理</strong>：</p><ul><li>用户问题同样经过Embedding模型转换为查询向量</li><li>在向量数据库中进行相似度搜索</li><li>返回Top-K个最相关的文档片段</li></ul></li></ol><h3>2.3 相似度计算方法</h3><h4>余弦相似度（最常用）</h4><pre><code class="python">import numpy as np

def cosine_similarity(vec1, vec2):
    """计算两个向量的余弦相似度"""
    dot_product = np.dot(vec1, vec2)
    norm_product = np.linalg.norm(vec1) * np.linalg.norm(vec2)
    return dot_product / norm_product

# 示例
query_vec = np.array([0.5, 0.3, 0.8])
doc_vec = np.array([0.6, 0.2, 0.9])
similarity = cosine_similarity(query_vec, doc_vec)
print(f"相似度: {similarity:.3f}")  # 输出：0.989</code></pre><p><strong>优点</strong>：不受向量长度影响，只关注方向</p><h4>欧氏距离</h4><pre><code class="python">def euclidean_distance(vec1, vec2):
    """计算欧氏距离（距离越小越相似）"""
    return np.linalg.norm(vec1 - vec2)</code></pre><h4>点积</h4><pre><code class="python">def dot_product_similarity(vec1, vec2):
    """点积相似度"""
    return np.dot(vec1, vec2)</code></pre><h3>2.4 向量数据库选择</h3><table><thead><tr><th>数据库</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>Pinecone</strong></td><td>云服务，易用性强</td><td>快速原型开发</td></tr><tr><td><strong>Milvus</strong></td><td>开源，性能强大</td><td>大规模生产环境</td></tr><tr><td><strong>Weaviate</strong></td><td>支持多模态</td><td>复杂查询需求</td></tr><tr><td><strong>Chroma</strong></td><td>轻量级，易部署</td><td>小型项目、本地开发</td></tr><tr><td><strong>FAISS</strong></td><td>Facebook开源，速度快</td><td>研究和实验</td></tr></tbody></table><h3>2.5 优化向量检索的技巧</h3><h4>技巧1：混合检索（Hybrid Search）</h4><p>结合关键词检索和向量检索：</p><pre><code class="python"># 伪代码示例
def hybrid_search(query, alpha=0.5):
    # 向量检索得分
    vector_results = vector_search(query)
    
    # 关键词检索得分（BM25）
    keyword_results = bm25_search(query)
    
    # 加权融合
    final_scores = alpha * vector_results + (1-alpha) * keyword_results
    return top_k(final_scores)</code></pre><h4>技巧2：重排序（Reranking）</h4><p>使用更强大的模型对初步检索结果重新排序：</p><pre><code class="python">def rerank(query, initial_results):
    """使用交叉编码器重排序"""
    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    pairs = [(query, doc) for doc in initial_results]
    scores = cross_encoder.predict(pairs)
    
    # 按新得分重新排序
    return sort_by_scores(initial_results, scores)</code></pre><h4>技巧3：查询扩展</h4><p>扩展用户查询以提高召回率：</p><pre><code class="python">def query_expansion(query):
    """生成查询的多个变体"""
    expanded_queries = [
        query,
        f"关于{query}的详细解释",
        f"{query}是什么意思",
        f"如何理解{query}"
    ]
    return expanded_queries</code></pre><hr/><h2>三、上下文注入：让LLM"看见"外部知识</h2><h3>3.1 上下文注入的原理</h3><p>上下文注入是将检索到的文档作为提示（Prompt）的一部分，提供给LLM。这个过程就像给AI提供"参考资料"。</p><h4>基本结构</h4><pre><code>系统指令 + 检索到的上下文 + 用户问题 → LLM → 生成答案</code></pre><h3>3.2 Prompt工程最佳实践</h3><h4>模板示例1：基础RAG Prompt</h4><pre><code class="python">def create_rag_prompt(query, context_docs):
    prompt = f"""你是一个专业的AI助手。请基于以下参考资料回答用户的问题。

参考资料：
{format_context(context_docs)}

重要提示：
1. 只基于上述参考资料回答问题
2. 如果参考资料中没有相关信息，请明确说明
3. 引用参考资料时请注明来源

用户问题：{query}

请提供准确、详细的回答："""
    
    return prompt

def format_context(docs):
    """格式化上下文文档"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"[文档{i}]\n{doc['content']}\n来源：{doc['source']}\n")
    return "\n".join(formatted)</code></pre><h4>模板示例2：带引用的高级Prompt</h4><pre><code class="python">def create_advanced_rag_prompt(query, context_docs):
    prompt = f"""# 角色
你是一个严谨的知识问答助手。

# 任务
基于提供的参考资料回答用户问题，并标注信息来源。

# 参考资料
{format_numbered_context(context_docs)}

# 回答要求
1. **准确性**：确保答案完全基于参考资料
2. **引用标注**：使用[1][2]标注信息来源
3. **完整性**：综合所有相关资料给出全面回答
4. **诚实性**：如果资料不足，明确说明局限性

# 用户问题
{query}

# 你的回答
"""
    return prompt

def format_numbered_context(docs):
    """带编号的上下文格式化"""
    formatted = []
    for i, doc in enumerate(docs, 1):
        formatted.append(f"[{i}] {doc['content']}\n(来源: {doc['source']})\n")
    return "\n".join(formatted)</code></pre><h3>3.3 上下文窗口管理</h3><h4>问题：上下文过长</h4><p>当检索到的文档过多或过长时，可能超出LLM的上下文窗口限制。</p><h4>解决方案</h4><p><strong>方案1：智能截断</strong></p><pre><code class="python">def truncate_context(docs, max_tokens=2000):
    """智能截断上下文"""
    truncated = []
    current_tokens = 0
    
    for doc in docs:
        doc_tokens = count_tokens(doc['content'])
        if current_tokens + doc_tokens &lt;= max_tokens:
            truncated.append(doc)
            current_tokens += doc_tokens
        else:
            # 截断最后一个文档
            remaining = max_tokens - current_tokens
            doc['content'] = truncate_to_tokens(doc['content'], remaining)
            truncated.append(doc)
            break
    
    return truncated</code></pre><p><strong>方案2：分层检索</strong></p><pre><code class="python">def hierarchical_retrieval(query, k1=10, k2=3):
    """两阶段检索：先召回，再精选"""
    # 第一阶段：快速召回更多文档
    candidates = vector_search(query, top_k=k1)
    
    # 第二阶段：使用更强模型精选最相关的
    final_docs = rerank(query, candidates, top_k=k2)
    
    return final_docs</code></pre><p><strong>方案3：文档摘要</strong></p><pre><code class="python">async def summarize_docs(docs, llm):
    """对长文档进行摘要"""
    summaries = []
    for doc in docs:
        if len(doc['content']) &gt; 1000:
            summary = await llm.summarize(doc['content'])
            doc['content'] = summary
        summaries.append(doc)
    return summaries</code></pre><h3>3.4 上下文质量优化</h3><h4>技巧1：去重</h4><pre><code class="python">def deduplicate_docs(docs, similarity_threshold=0.9):
    """移除相似度过高的重复文档"""
    unique_docs = []
    for doc in docs:
        is_duplicate = False
        for existing in unique_docs:
            if cosine_similarity(doc['embedding'], existing['embedding']) &gt; similarity_threshold:
                is_duplicate = True
                break
        if not is_duplicate:
            unique_docs.append(doc)
    return unique_docs</code></pre><h4>技巧2：相关性过滤</h4><pre><code class="python">def filter_by_relevance(docs, min_score=0.7):
    """过滤掉相关性低的文档"""
    return [doc for doc in docs if doc['score'] &gt;= min_score]</code></pre><h4>技巧3：多样性采样</h4><pre><code class="python">def diversify_results(docs, top_k=5):
    """确保结果的多样性"""
    selected = [docs[0]]  # 选择最相关的
    
    for doc in docs[1:]:
        if len(selected) &gt;= top_k:
            break
        
        # 计算与已选文档的最大相似度
        max_sim = max([cosine_similarity(doc['embedding'], s['embedding']) 
                       for s in selected])
        
        # 如果不太相似，则添加
        if max_sim &lt; 0.85:
            selected.append(doc)
    
    return selected</code></pre><hr/><h2>四、完整RAG系统实现</h2><h3>4.1 系统架构</h3><pre><code>┌─────────────┐
│  用户查询   │
└──────┬──────┘
       │
       ▼
┌─────────────────┐
│  查询处理模块   │ ← 查询改写、扩展
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  向量检索引擎   │ ← 向量数据库
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  重排序模块     │ ← 提高精确度
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  上下文构建     │ ← Prompt工程
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  LLM生成        │ ← 生成答案
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  后处理与验证   │ ← 事实检查
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  返回结果       │
└─────────────────┘</code></pre><h3>4.2 Python实现示例</h3><pre><code class="python">from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

class RAGSystem:
    def __init__(self, documents):
        """初始化RAG系统"""
        # 1. 文档处理
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            separators=["\n\n", "\n", "。", "！", "？", ".", "!", "?"]
        )
        
        # 2. Embedding模型
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
        
        # 3. 向量数据库
        self.vectorstore = self._build_vectorstore(documents)
        
        # 4. LLM
        self.llm = OpenAI(temperature=0)
        
        # 5. 检索器
        self.retriever = self.vectorstore.as_retriever(
            search_type="mmr",  # 最大边际相关性
            search_kwargs={
                "k": 4,
                "fetch_k": 20,
                "lambda_mult": 0.5
            }
        )
        
    def _build_vectorstore(self, documents):
        """构建向量存储"""
        # 切分文档
        chunks = self.text_splitter.split_documents(documents)
        
        # 创建向量数据库
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )
        
        return vectorstore
    
    def query(self, question):
        """执行RAG查询"""
        # 创建问答链
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.retriever,
            return_source_documents=True,
            chain_type_kwargs={
                "prompt": self._create_prompt()
            }
        )
        
        # 执行查询
        result = qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "sources": result["source_documents"]
        }
    
    def _create_prompt(self):
        """创建Prompt模板"""
        from langchain.prompts import PromptTemplate
        
        template = """基于以下参考资料回答问题。如果资料中没有答案，请说"我不知道"。

参考资料：
{context}

问题：{question}

详细回答："""
        
        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

# 使用示例
from langchain.document_loaders import TextLoader

# 加载文档
loader = TextLoader("knowledge_base.txt")
documents = loader.load()

# 创建RAG系统
rag = RAGSystem(documents)

# 查询
result = rag.query("什么是机器学习？")
print(f"回答：{result['answer']}")
print(f"参考文档数量：{len(result['sources'])}")</code></pre><h3>4.3 高级优化：多查询RAG</h3><pre><code class="python">class AdvancedRAG:
    def multi_query_retrieval(self, question):
        """生成多个查询角度"""
        # 使用LLM生成问题的不同表述
        variations = self.llm.generate_variations(question, num=3)
        
        all_docs = []
        for variation in variations:
            docs = self.retriever.get_relevant_documents(variation)
            all_docs.extend(docs)
        
        # 去重和排序
        unique_docs = self.deduplicate(all_docs)
        ranked_docs = self.rerank(question, unique_docs)
        
        return ranked_docs[:5]
    
    def self_query_with_metadata(self, question):
        """基于元数据的自查询"""
        # 从问题中提取过滤条件
        metadata_filter = self.extract_metadata_filter(question)
        
        # 在向量搜索中应用过滤
        docs = self.vectorstore.similarity_search(
            question,
            filter=metadata_filter,
            k=5
        )
        
        return docs</code></pre><hr/><h2>五、实践案例与应用场景</h2><h3>5.1 企业知识库问答</h3><p><strong>场景</strong>：企业内部有大量文档（产品手册、政策文档、FAQ等）</p><p><strong>实现要点</strong>：</p><ul><li>文档分类和元数据管理</li><li>权限控制</li><li>定期更新向量库</li></ul><pre><code class="python"># 示例：企业知识库RAG
class EnterpriseRAG:
    def __init__(self):
        self.vectorstore = Chroma(
            collection_name="company_docs",
            embedding_function=embeddings
        )
    
    def add_document(self, doc, metadata):
        """添加文档并包含元数据"""
        chunks = self.split_document(doc)
        
        for chunk in chunks:
            self.vectorstore.add_texts(
                texts=[chunk],
                metadatas=[{
                    "department": metadata["department"],
                    "doc_type": metadata["doc_type"],
                    "last_updated": metadata["date"],
                    "access_level": metadata["access_level"]
                }]
            )
    
    def query_with_access_control(self, question, user_level):
        """带权限控制的查询"""
        results = self.vectorstore.similarity_search(
            question,
            filter={"access_level": {"$lte": user_level}},
            k=5
        )
        return results</code></pre><h3>5.2 客服智能问答</h3><p><strong>场景</strong>：自动回答客户常见问题</p><p><strong>实现要点</strong>：</p><ul><li>快速响应时间</li><li>多轮对话上下文管理</li><li>答案质量监控</li></ul><h3>5.3 学术研究助手</h3><p><strong>场景</strong>：帮助研究人员查找和总结文献</p><p><strong>实现要点</strong>：</p><ul><li>支持PDF解析</li><li>引用管理</li><li>多模态检索（文本+图表）</li></ul><hr/><h2>六、评估与优化</h2><h3>6.1 评估指标</h3><h4>检索质量指标</h4><pre><code class="python">def calculate_retrieval_metrics(retrieved_docs, relevant_docs):
    """计算检索指标"""
    retrieved_ids = set([doc['id'] for doc in retrieved_docs])
    relevant_ids = set([doc['id'] for doc in relevant_docs])
    
    # 召回率 (Recall)
    recall = len(retrieved_ids &amp; relevant_ids) / len(relevant_ids)
    
    # 精确率 (Precision)
    precision = len(retrieved_ids &amp; relevant_ids) / len(retrieved_ids)
    
    # F1分数
    f1 = 2 * (precision * recall) / (precision + recall)
    
    # MRR (Mean Reciprocal Rank)
    for i, doc in enumerate(retrieved_docs, 1):
        if doc['id'] in relevant_ids:
            mrr = 1 / i
            break
    
    return {
        "recall": recall,
        "precision": precision,
        "f1": f1,
        "mrr": mrr
    }</code></pre><h4>生成质量指标</h4><ul><li><strong>答案准确性</strong>：与标准答案的相似度</li><li><strong>幻觉率</strong>：生成内容中不基于参考资料的比例</li><li><strong>完整性</strong>：是否完整回答了问题</li><li><strong>引用准确性</strong>：引用是否正确</li></ul><h3>6.2 常见问题与解决方案</h3><table><thead><tr><th>问题</th><th>原因</th><th>解决方案</th></tr></thead><tbody><tr><td>检索不到相关文档</td><td>Embedding模型不合适</td><td>更换或微调Embedding模型</td></tr><tr><td>答案包含幻觉</td><td>上下文不足或Prompt不当</td><td>优化Prompt，增加"仅基于资料回答"约束</td></tr><tr><td>响应速度慢</td><td>检索或生成耗时长</td><td>使用更快的向量数据库，减少检索文档数</td></tr><tr><td>答案质量不稳定</td><td>检索结果质量波动</td><td>增加重排序步骤，提高检索精确度</td></tr></tbody></table><h3>6.3 持续优化策略</h3><ol><li><strong>A/B测试</strong>：对比不同检索策略和Prompt的效果</li><li><strong>用户反馈循环</strong>：收集用户评价，优化系统</li><li><strong>定期评估</strong>：建立测试集，定期评估系统性能</li><li><strong>模型更新</strong>：跟踪最新的Embedding和LLM模型</li></ol><hr/><h2>七、未来趋势与展望</h2><h3>7.1 多模态RAG</h3><p>支持图像、音频等多种模态的检索和生成。</p><h3>7.2 自适应RAG</h3><p>根据问题类型自动选择最佳检索策略。</p><h3>7.3 知识图谱增强</h3><p>结合结构化知识图谱提升推理能力。</p><h3>7.4 实时RAG</h3><p>支持流式检索和增量生成，提升用户体验。</p><hr/><h2>总结</h2><p>RAG技术通过<strong>向量检索</strong>和<strong>上下文注入</strong>两大核心机制，成功地将外部知识与大语言模型结合，显著提升了AI系统的准确性和实用性。</p><h3>关键要点回顾</h3><ol><li><strong>向量检索是基础</strong>：选择合适的Embedding模型和向量数据库至关重要</li><li><strong>上下文注入是关键</strong>：精心设计的Prompt能大幅提升答案质量</li><li><strong>优化是持续的</strong>：通过混合检索、重排序、元数据过滤等技术不断改进</li><li><strong>评估要全面</strong>：关注检索和生成两个阶段的指标</li></ol><h3>实践建议</h3><ul><li><strong>从简单开始</strong>：先实现基础RAG，再逐步优化</li><li><strong>重视数据质量</strong>：高质量的文档是RAG成功的前提</li><li><strong>持续迭代</strong>：基于用户反馈和评估结果不断改进</li><li><strong>选择合适的工具栈</strong>：根据实际需求选择Embedding模型、向量数据库和LLM</li></ul><p>RAG技术正在快速发展，掌握其原理与实践，将帮助你构建更智能、更可靠的AI应用。</p>]]></description></item>  </channel></rss>