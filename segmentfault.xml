<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[基于Strands Agents SDK]]></title>    <link>https://segmentfault.com/a/1190000047412944</link>    <guid>https://segmentfault.com/a/1190000047412944</guid>    <pubDate>2025-11-20 08:01:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000046555790" alt="图片" title="图片"/></p><h2>背景</h2><p>AI Agent 作为当前发展最快的技术趋势之一,  不同于专注特定领域任务的传统 AI 应用, 能在更少人工干预的情况下,  管理和执行端到端的流程,  从单纯的工具进化为团队成员,  使企业生产力、效率和增长方面进入一个新的时代. 有研究报告指出: “AI Agent 竞争势头已经明显增强, 93%的企业管理者认为在接下来的一年内在企业内部规模扩展AI Agent 的使用会让企业在同类竞争者中保持领先地位”. 尽管如此, 现实情况是: 大部分企业还不知道以何种方式快速构建、运行和管理AI Agents, 将AI Agents 快速在企业内部落地应用. 基于此, 本文从AI Agent 的结构和组成入手, 结合Amazon Strands Agents SDK,  构建一个集 Agent 构建和运行的参考实现平台, 且在参考实现平台集成了AgentCore Browser user tool 和 AgentCore Code Interpreter tool, 方便用户使用相应的工具来构建Agents.</p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=th3gVnHoqLxWDyATKgjcHA%3D%3D.Fpf%2FENVBSKftVfcy0cMfaxaudotpH4gvpzYlMcnA0kiXzN%2B99slrsUcHYEeChvLxZMKUmn1KyZFn2A%2FkHHrTrqes8ubRTXt9wdeZTvYRsn8QMoNaqA5htSJbMm3vzbv%2BKACxMDIlE13f3cKP4%2BWIdmUX%2F5N%2BjFJAHQCjlacZz9xEp6mD2rKjE%2FsTUySTUElqeCpL4n9cIPHSqUSGIEt7tckJYIotnT1AEnOZy288cxA%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=3KPdbbt8VXvOUW2DaLU6WA%3D%3D.GD8fYCrMMHN6RkbT1md87M9TRt%2F2uonf1DSjcV%2FBRgYiX%2FouXs9mRR5%2FJe3uefZm8nRLPQ2fWlLlS3uAsKCFek7biZEE6bOeyFUF0ZHJvOpQTEF3l9Qt3SNIGG2LqCpf5daRIoN7upcDuvcIbDUZz%2FDpwrd3TixEQZLfLg8CluxSogBbIDWrNIbiRprWzYyUzrRfHpPbVKeVwji0gXAJfHHuFqxvolEp2UMbKdeBn2I%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote><h2>AI Agent 组成、架构及开发框架</h2><p>在Google 的Agents 白皮书中, 针对AI Agent的核心功能, 给出了其组成构件和架构, 如下图所示.</p><p><img width="723" height="360" referrerpolicy="no-referrer" src="/img/bVdm5If" alt="image.png" title="image.png" loading="lazy"/></p><p>AI Agent 包含三个必不可少的组件:</p><ol><li>The model(模型)<br/>在AI Agent 范畴, 模型是指大语言模型(LLM), 作为Agent的中心化决策者, 进行任务分解、执行和检测任务是否完成. 一般来讲, Agent 中使用的模型, 基于推理和逻辑框架(如, ReAct, Chain-of-Thought, 或者 Tree-of-Thoughts), 需要有很强的指令遵循能力.</li><li>Tools(工具集)<br/> 尽管模型在文本、图片等方面具有很强的生成能力, 但由于模型不能与外部世界交互, 模型的能力受到极大约束. 而工具可以抹平这种裂痕, 使得Agents 可以与外部数据和服务进行交互, 更大范围拓展Agent 除模型本身之外的能力.</li><li>The orchestration layer(编排层)<br/>编排层描述Agent 如何 接收外部信息, 执行内部推理(Reasoning), 使用推理结果来确定下一步动作和决策的这一循环过程.这个过程会一直循环, 直到Agent完成任务目标或者达到停止点.</li></ol><p>Agent 的工作流程如下图所示:</p><p><img width="723" height="335" referrerpolicy="no-referrer" src="/img/bVdm5Ih" alt="image.png" title="image.png" loading="lazy"/></p><ol><li>AI Agents 与外部环境交互, 并从用户接收输入、触发或者目标.</li><li>通过模型的推理, 将目标分解成特定的动作和步骤, 并确定任务的顺序</li><li>任务执行过程中, AI Agents 可以访问内部的数据和工具(如知识库、企业系统)以及通过MCP 协议访问外部的工具(如第三方数据库, Web 搜索等)</li><li>AI Agent Orchestration 组件循环执行 Reasoning -&gt; Action Plan -&gt; Execution 这个Loop. 循环过程中可能会有安全护栏(Guardrails) 来确保道德、运营和安全标准</li><li>由于Agent 执行过程可能会执行多轮循环, 受限于模型的上下文窗口长度以及对模型精度的要求, AI Agent 使用 Memory 来维护执行过程的上下文, 从前面的迭代或者过去的执行中学习, 提升Agent的性能表现</li><li>在Multi-Agent 场景, Agent -to -Agent(A2A) 协议被用来实现Agent 之间的协作.</li></ol><p>Strands Agents SDK 是一个开源的AI Agent 开发框架, 使用Strands Agents SDK, 可以利用几行代码快速开发生产可用的multi-agents AI系统. 具有丰富的特性:</p><ol><li>Agent Loop: Strands Agents 实现了处理用户输入、决策、执行工具、响应生成的循环过程, 这个过程支持复杂的、多步骤的推理和动作执行, 并且可以无缝的集成Model 和</li><li>在Model 方面, 提供多种Model Provider 实现, 如Amazon Bedrock, Anthropic, OpenAI, Ollama, LiteLLM 等, 开发者可以灵活 的选择各种商业或者开源的模型来驱动AI Agent.</li><li><p>在Tools 方面, Strands Agents 支持</p><ul><li>Python Tools: 支持 以@tool 函数装饰器方式和基于Python 模块方式来定义Agent 可用的Tools</li><li>Model Context Protocol(MCP) Tools: 集成MCP Server 的Tools 作为 Agent 可用的Tools, 并且支持STDIO 和 Streamable HTTP 两种Transport 类型的MCP Server</li><li>Agent Tools: 在multi-agent 场景, Strands Agents 框架支持将其他Agent作为 当前Agent的一个Tool 来完成特定领域的任务.</li></ul></li></ol><p>Strands Agents 自带了丰富的、开箱即用的基础工具, 如文件读写、memory(mem0_memory 和agent_core_memory)、browser use(local_chromium_browser和agent_core_browser(remote))、Code Interpretation(python_repl 和 agent_core_interpreter)、多模态(image_reader, generate_image, nova_reels, diagram) 等等, 极大地方便了Agent 应用的开发, 使得开发者可以像搭积木一样, 快速开发出自己的Agent.</p><ol start="4"><li><p>在Multi-Agent 方面, Strands Agents 支持以下模式:</p><ul><li>Agent-to-Agent(A2A) protocol, 支持不同平台和实现的AI Agents 间无缝沟通</li><li>Agents as Tools, 通过编排(orchestration) Agent 来接收用户输入并决定使用哪个特定的Agent 来完成相应的任务, 也就是将特定的Agent 当具体的工具来用</li><li>Swarm, 多个Agent 以团队的方式共同来完成复杂任务, 这种方式允许Agent 间自主协作, 共享上下文以及记忆来完成共同的任务</li><li>Graph, 以有向无环图(DAG)的方式编排各个Agent, Agent作为节点, 以DAG结构图作为执行顺序, 一个节点(Agent) 的输出将传播到与其连接的节点, 并作为其输入. 这种模式可以实现非常的复杂的Agent系统</li><li>Workflow, 这种模式适合复杂的多步骤流程, 并且任务间有依赖关系, 必须等上一步执行完成后, 才能执行下一步, 每一步都由一个专家Agent 来完成对应的任务.</li></ul></li></ol><p>除上述核心特性外, Strands Agents SDK 在Agent 的安全性、应用的可观测性以及Agent 部署方面提供了原生支持, 可以说, Strands Agents SDK 提供了开发AI Agent所需的方方面面, 使开发者可以便捷的、快速的开发自己的Agent 应用.</p><h2>AI Agent 构建和运行时</h2><p>Strands Agents SDK显著提升了AI Agent的开发效率，但仍存在两个关键挑战：首先，开发完成后的Agent部署与运行管理问题；其次，尽管SDK已经极大地简化了开发流程，但依然需要编写代码，对非技术背景的业务人员形成了使用障碍. 进一步思考 :</p><ol><li>是否可以不用写代码就可以构建自己的Agent, 这样业务人员都可以快速构建想要的Agent</li><li>Agent 构建好后, 是否可以不用考虑Agent 的部署, 立即可运行, 对外提供可访问的端点</li><li>很多业务场景需要Agent 周期性运行, 是否能让配置或者构建好的Agent 按时调度运行</li></ol><p>从工程实现的视角对AI Agent进行解构，我们可以识别出其由以下四个核心组件构成:</p><ol><li>Model(LLM): Agent 的大脑</li><li>Tools: Agent 的 触手</li><li>System Prompt: Agent 的行为规范</li><li>Envrioments: Agent 需要感知的环境信息, 如系统环境变量, 当前对话的文档等</li></ol><p>基于上述考虑, 我们可以通过配置的方式 声明一个AI Agent 这四部分分别是什么, 然后通过代码根据这四部分的配置, 动态构建出一个 AI Agent 实例, 并提供Agent 的运行环境,  实现 1) 通过配置即可构建Agent, 2) Agent 构建即可运行, 使AI Agent 可以快速在企业内部进行落地实施.</p><h2>基于Strands Agents SDK 的 AI Agent 构建和运行时架构</h2><p>基于上述思考, 笔者基于Strands Agents SDK 开发了一个AI Agent 的构建和运行平台(参考实现)-<a href="https://link.segmentfault.com/?enc=q3mw%2BPHU5mkj48c25gFNnA%3D%3D.jfxIdXSNLkVvgV0yv4j%2BUtXcLV0PgnK1OjHisqohSjoD%2FyPjgmH2fx%2BLmivjRoSWi6BKIZym810mqkr4LSjqpA%3D%3D" rel="nofollow" target="_blank">AgentX</a>. 可以实现:</p><ol><li>通过配置的方式来构建AI Agent. 支持Strands Agents 自带的Tools, MCP Server Tools 以及Agent as Tool, 并且支持各种模型接入.</li><li>配置好的Agent 可以通过API Endpoint 来进行调用, 无需构建专门的Agent 运行时, 真正做到Agent 配置即可运行</li><li>支持 Agent 定时调度执行配置</li><li>支持Agent 运行历史记录管理</li></ol><p>AgentX 架构如下图所示:</p><ol><li>使用ECS 来部署整个应用, 使用ECS Fargate 作为Capacity Provider</li><li>整体应用分为前端、后端、以及各种MCP Server</li><li>使用Amazon DynamoDB 来存储 Agent 的配置以及Agent的运行历史记录</li><li>使用Amazon Lambda 和 EventBridge 实现Agent 的定时调度执行</li></ol><p><img width="723" height="724" referrerpolicy="no-referrer" src="/img/bVdm5ME" alt="image.png" title="image.png" loading="lazy"/></p><p>整个项目可以通过CDK 一键部署, 部署前需要将前、后端应用以及需要部署的MCP Server 构建成Docker 镜像并推送到的Amazon ECR 中.</p><h2>配置Agent</h2><p>部署完成后, 在Agent 管理页面, 可以配置Agent. 配置Agent的过程 实际就是对Agent 使用的模型, System Prompt, 需要使用的Tools以及环境变量参数进行设置.</p><p><img width="586" height="1000" referrerpolicy="no-referrer" src="/img/bVdm5MF" alt="image.png" title="image.png" loading="lazy"/></p><h2>运行和调用Agent</h2><p>Agent 配置完成后, 可以通过在UI 界面上选择配置好的Agent 来完成相应的任务, 如下图所示. 也可以在应用层面, 通过API Endpoint 来调用Agent. 可以看到, 我们配置好Agent 之后, Agent 立即可运行, 不需要额外的部署.</p><p><img width="723" height="160" referrerpolicy="no-referrer" src="/img/bVdm5MH" alt="image.png" title="image.png" loading="lazy"/></p><p>Agent 执行 以及 MCP Server Tools 调用如下图所示.</p><p><img width="723" height="647" referrerpolicy="no-referrer" src="/img/bVdm5MI" alt="image.png" title="image.png" loading="lazy"/></p><h2>MCP Server 管理</h2><p>可以对企业内部或者一些公开的MCP Server 进行维护和管理, 通过配置的方式,  将这些MCP Server 的 Tools 作为Agent的Tools. 目前该项目只支持Streamable HTTP Transport 类型的MCP Server.</p><p><img width="723" height="339" referrerpolicy="no-referrer" src="/img/bVdm5MJ" alt="image.png" title="image.png" loading="lazy"/></p><h2>Agent 调度</h2><p>通过配置Cron 表达式的方式, 来定时调度Agent 执行任务. 对于需要周期性让Agent 来完成某项任务的场景提供了开箱即用的支持.</p><p><img width="723" height="343" referrerpolicy="no-referrer" src="/img/bVdm5MK" alt="image.png" title="image.png" loading="lazy"/></p><h2>应用场景</h2><p>使用上述Agent 构建和运行时平台, 用户可以快速在以下场景(但不限于)落地Agent应用.</p><h2>智能数据分析</h2><p>结合数仓 Redshift MCP Server(或者其他的数据库MCP Server), 可以配置Agent 来实现基于Agent 的数据分析, 如对数据进行预测性分析或者诊断性分析. 对于简单的查数以及指标计算场景, Agent 结合数据库MCP Server就能很好的支持, 如果数据表比较多, 表间的关系比较复杂, 在结合MCP Server 的同时, 可以将Schema 信息放到RAG 中, 将RAG 也作为Agent 的工具, 来实现 Chat BI 或者Text2SQL 的场景.</p><p>通过配置Redshift MCP Server 作为Agent 的工具, 来实现游戏埋点事件数据(模拟数据)的预测性分析, 如下图所示. Agent 会充分利用MCP 工具以及的模型本身的知识, 构建数据预测模型, 对基础数据进行预测性分析, 并将分析结果以HTML页面的方式进行可视化展示.</p><p><img width="651" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MO" alt="image.png" title="image.png" loading="lazy"/></p><h2>基于Agent 的云端资源巡检</h2><p>在一些关键业务场景中, 企业IT运维人员需要定时对资源的各项运行指标进行检查, 来确保基础架构以及上层应用的稳定运行. 由于需要检查的方面比较多, 如数据库运行指标检查, EC2 运行指标检查, EKS 集群运行检查, 可以每个方面的检查配置一个专门的Agent来执行, 最后通过一个编排(Orchstrator) Agent 来统筹执行整个巡检任务, 将不同方面的巡检指派给不同的Agent, 也是Multi-Agent 的场景.</p><p>下图是一个编排 Agent 结合两个其他的Agent(MySQL 巡检Agent 和 EC2巡检Agent) 的Multi-Agent 配置.</p><p><img width="643" height="800" referrerpolicy="no-referrer" src="/img/bVdm5MQ" alt="image.png" title="image.png" loading="lazy"/></p><p>保存配置后, 此编排Agent 就可以对Amazon EC2 和 RDS 资源进行运行指标巡检, 编排Agent 对巡检任务进行分解, 将EC2的巡检任务指派给 EC2巡检Agent, 将RDS 资源巡检的任务交给RDS巡检Agent. 其中EC2 巡检结果如下图所示, 可以看到, EC2 巡检Agent 识别到有EC2 CPU峰值利用率 超过 93.89%, 超过设定的阈值(75%)18.89%.</p><p><img width="723" height="490" referrerpolicy="no-referrer" src="/img/bVdm5MR" alt="image.png" title="image.png" loading="lazy"/></p><p>RDS 巡检结果如下图所示, RDS 巡检Agent 查找到有RDS 实例可用内存(55.28MB)小于阈值(500MB).</p><p><img width="723" height="478" referrerpolicy="no-referrer" src="/img/bVdm5MS" alt="image.png" title="image.png" loading="lazy"/></p><p>最后编排Agent对上述检查结果进行统一整理和分析, 并给出行动建议, 非常详尽, 对于查找的问题, 可以添加其他的Tools, 如slack tool, 将巡检报告以及查找的问题发送到对应的Channel, 实现告警通知.</p><p><img width="589" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MT" alt="image.png" title="image.png" loading="lazy"/></p><h2>远程Browser Use 和 Code Interpreter 沙箱</h2><p>在内容营销的场景, 可能需要通过浏览器自动化网络内容操作, 需要Agent 有Browser Use 的能力; 在一些数据分析的场景, 需要执行代码对数据进行分析, 需要有安全的代码执行环境. 对于上述两种场景, Strands Agents 已经集成了Amazon AgentCore 中的 Browser Use 和 Code Interpreter 工具, 在笔者构建的平台上也可以通过配置的方式将这两个工具集成到对应的Agent中. 下面分别是AgentCore Browser Use 和 Code Interpreter 工具在Agent中的使用示例.</p><p><img width="576" height="500" referrerpolicy="no-referrer" src="/img/bVdm5MU" alt="image.png" title="image.png" loading="lazy"/></p><h2>亚马逊云科技知识专家</h2><p>将Amazon Knowledge MCP 作为Agent 的工具, 可以使用户立刻化身亚马逊云科技知识专家, 对于亚马逊云科技服务的使用, 最佳实践都可以通过Agent 来回答, 如下图所示.</p><p><img width="619" height="500" referrerpolicy="no-referrer" src="/img/bVdm6o3" alt="image.png" title="image.png" loading="lazy"/></p><p>由于本平台很好的提供了Agent的构建和运行时环境, 用户可以根据实际的业务场景需求来快速构建出Agent, 并将Agent应用到实际的业务场景中去. 如果业务场景需要用到更多的工具, 可以开发相应的MCP Server 或者开发对应的Strands Agents Tool 来满足Agent 的需求.</p><h2>总结</h2><p>本文对AI Agent 的组成和架构进行了阐述, 介绍了AI Agent 开发框架 Strands Agent SDK 及其相关特性, 并基于Strands Agent SDK 开发了一站式Agent 构建和运行时平台AgentX, 支持以配置的方式构建Agent, 配置完Agent 即可运行, 用户可快速将Agent 应用到的Agentic Data Analytics、智能IT巡检等业务场景中.</p><p><strong>参考资料</strong></p><ol><li><a href="https://link.segmentfault.com/?enc=XBOgilGYTBNmX1fV6QmAJg%3D%3D.hELbG3qlKNu8hJxFqtBNDs8boFdgSIueLvC%2FnY2BqeSnpRGXmZ1PjKXqgDZm%2BhlTl4eVlORJo%2FpjG0h6IU3UsoeQ%2FKR72HcBMLBwzFRhLD8%3D" rel="nofollow" target="_blank">Google Whitepaper on AI Agents</a></li><li><a href="https://link.segmentfault.com/?enc=pzNzPiao9yvUP3%2FeBmHbSg%3D%3D.ATc62mBjq%2Be%2FtnL0zpKvhjVJYTixP0cXUi8HiSCsSHvvRkG20x2pbQmsATTbboj7t6rj6EWTleM%2FVtPnWG1IiB%2F7KnJEn1N6wiZCc3bICPnMo7uxXeu1q%2B3y%2B92hgo3D" rel="nofollow" target="_blank">Rising of Agentic AI</a></li><li><a href="https://link.segmentfault.com/?enc=3OW0TlKNNJeZkIwFZji8OQ%3D%3D.f3Onwo06ePbrpmgnZAuog4C0Zo%2BEmUJIDQFmJcgW71I0oHOdohHg8Un9yYR1k%2FDqqzOSsWxn%2Fqq2%2Bokxj%2BcnCw%3D%3D" rel="nofollow" target="_blank">AgentX Repo</a></li><li><a href="https://link.segmentfault.com/?enc=Yb4CYPBnb0mSfttSAhuW%2Fw%3D%3D.vNGx%2B7MFtPrXUZxl1e%2FL5PxzRdzIfdf%2BV%2FTvdDhg8Qsxq8M6rUUwsnzdVd9pgL89qo0%2Fokbz6Sm6NpI09Hjqk8INNlxN%2F05CMWMQlkYogLhQ3Bt2bFA3%2BzfGUuIns1Q%2B" rel="nofollow" target="_blank">Amazon Bedrock AgentCore</a></li></ol><p><em>*前述特定亚马逊云科技生成式人工智能相关的服务目前在亚马逊云科技海外区域可用。亚马逊云科技中国区域相关云服务由西云数据和光环新网运营，具体信息以中国区域官网为准。</em></p><p><strong>本篇作者</strong><br/><img width="723" height="156" referrerpolicy="no-referrer" src="/img/bVdm5M2" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>📢 想要玩转 Strands Agents？速看最新试验！<br/>🌟《<a href="https://link.segmentfault.com/?enc=UG83utKYCtbEAW%2B3xQkdaw%3D%3D.NPEfyXlGBJYiO7b1BtQ99CIok9mm9J%2B8hP8EfUGcrN8Sq4hOoJc72%2FCbVmqEtU1XTMVWpSVEirimzR%2FLoKE9OXvWkE6Mxq5X0yp2X2ISMvzTa21FeOrPTY9wYG%2FrV9YZ3VEjjUvMTNKOnevvXw5tXjiYyWp5SBEaNfoUM6dZCWK9fYqPt%2F3VbE55jysqlkk%2BO0x4jqHGHg3OB3E1oeoK2lp23OK2yaJ1BM7gLSRILLw%3D" rel="nofollow" target="_blank">开源 Agent 框架 Strands Agents 速成班</a>》推出新实验啦！该实验具有轻量级且灵活的开发方式，支持完全定制，还能访问数千预构建工具，让你体验构建智能应用新范式。<br/>⏩ <a href="https://link.segmentfault.com/?enc=r9Zumd6Wu5JK3xTvnZJoeA%3D%3D.7MwnX2opz1UoXfHVZsTu25OYKr%2FFRteHqqn8vXJ%2BRBl4Tyk83fyI099fVcFqgb9d03ShmxRE9iCZTwavX5MUPvu0DhlkZHATXfq2erNQoP64S6N%2B%2FaGbVviIQQQ0N4wOWOXbngCY5p1QiwKQN%2BmaLYR50KPe0t9a%2F18dfZHzMtt0w0Cxxd2Anw%2BrHcqAg1Uq9YOgpnLVoJ%2BQ8K7zi1Y5kFXXY040m7EFXS7yNlAaQLM%3D" rel="nofollow" target="_blank">[点击进入实验</a>]，即刻打造高效时间管理工具，免费体验企业级 AI 开发工具的真实效果。<br/>👉 构建无限可能，探索之旅即刻启程！</blockquote>]]></description></item><item>    <title><![CDATA[蜜蜂目标检测数据集的构建与标注方法（70]]></title>    <link>https://segmentfault.com/a/1190000047412956</link>    <guid>https://segmentfault.com/a/1190000047412956</guid>    <pubDate>2025-11-20 00:03:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>蜜蜂目标检测数据集的构建与标注方法（7000张图片已标注划分）</h2><h3>背景</h3><p>随着人工智能和计算机视觉技术的不断发展，目标检测任务在多个领域中都得到了广泛的应用。尤其是在农业和生态研究领域，自动化目标检测技术逐渐成为提高生产效率、保障生态环境的重要工具。蜜蜂作为生态系统中的关键物种，其活动的监控和分析对农业、生态环境保护及科学研究都具有重要意义。</p><p>为了更好地实现蜜蜂目标检测，本数据集专门设计并收集了高质量的蜜蜂图像，旨在帮助研究人员和开发者构建和训练蜜蜂目标检测模型。通过这些数据，相关的机器学习模型可以高效识别蜜蜂，并应用于各类场景，如农业监控、生态保护和无人机监控等。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412958" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>数据集获取</h3><blockquote>链接:<a href="https://link.segmentfault.com/?enc=rr73oEcuMQPIjA%2F9amxGOw%3D%3D.WFVR5x8DUFc4WHg5pEYXXwcOQQm1s%2B9ECjb6uPbR1ymKf2%2FuQrwTtg1TYycKE6wBjHPb1n9ll%2BdfUPFZpC9UkA%3D%3D" rel="nofollow" target="_blank">https://pan.baidu.com/s/19wEg4vB6d-SjhPlBJzx7aw?pwd=gy1b</a> 提取码:gy1b 复制这段内容后打开百度网盘手机App，操作更方便哦</blockquote><p>本数据集专为蜜蜂目标检测任务设计，包含了八千张高质量的图像，适用于训练、验证和测试蜜蜂检测模型。数据集中的每张图片均经过精心标注，旨在为目标检测模型提供足够的数据，帮助其高效地检测蜜蜂。</p><p>数据集结构</p><p>数据集包含三个主要部分：</p><p>训练集 (train)：用于训练目标检测模型的图像数据。</p><p>验证集 (valid)：用于在训练过程中进行验证，帮助调节模型的超参数。</p><p>测试集 (test)：用于评估最终模型的性能，确保模型的泛化能力。<br/>标签与类别</p><p>本数据集当前支持检测的唯一目标是蜜蜂。数据集中使用了以下标签：</p><p>类别数 (nc): 1</p><p>类别名称 (names): ['bees']</p><p>数据集特点</p><p>八千张图像：数据集包含了丰富的蜜蜂图像，适用于训练深度学习模型。</p><p>高质量标注：每张图片中的蜜蜂均经过精确标注，标注格式为常见的目标检测格式（如YOLO格式）。</p><p>多样化场景：数据集中的蜜蜂出现在不同的环境和场景中，包括花丛、树枝、空旷地等，增加了模型的泛化能力。</p><p>应用场景</p><p>此数据集可广泛应用于以下领域：</p><p>农业监控：自动化检测蜜蜂活动，为农业研究提供支持。</p><p>生态研究：为蜜蜂行为、种群动态等生态研究提供数据支持。</p><p>无人机监控：结合无人机图像采集，对蜜蜂进行监控和分析。</p><p>使用说明</p><p>数据格式：本数据集采用常见的目标检测数据格式，适配YOLO、Detectron2、TensorFlow等深度学习框架。</p><p>训练建议：对于YOLO等模型，可以直接利用此数据集进行训练与测试。建议将数据集按照80%-10%-10%划分为训练集、验证集和测试集。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412959" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412960" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>数据集概述</h3><p>本数据集包含了约八千张高质量的蜜蜂图像，图像内容多样且经过精确的标注，支持多种目标检测框架。数据集被划分为训练集、验证集和测试集，确保在不同阶段对模型进行全面的评估与优化。</p><ul><li><strong>训练集 (train)</strong>：用于训练目标检测模型的图像数据，帮助模型学习蜜蜂的特征。</li><li><strong>验证集 (valid)</strong>：用于在训练过程中进行验证，调整模型超参数，防止过拟合。</li><li><strong>测试集 (test)</strong>：用于评估训练后的模型性能，确保模型在实际应用中的泛化能力。</li></ul><h3>数据集详情</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412961" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>图像数量</h4><p>数据集总共包含八千张蜜蜂图像，每张图像都经过精心选择，确保图像的多样性和质量。这些图像涵盖了不同环境中的蜜蜂，包括花丛、树枝、空旷地等。每一张图片都包含至少一个蜜蜂目标，且图像中蜜蜂的位置和类别已精确标注。</p><h4>标注与格式</h4><p>每张图片中的蜜蜂都被标注为一个目标，采用常见的目标检测格式（如YOLO格式），便于与深度学习框架兼容使用。数据集支持的标签如下：</p><ul><li><strong>类别数 (nc)</strong>：1</li><li><strong>类别名称 (names)</strong>：['bees']</li></ul><p>数据格式采用了标注框（bounding box）方式，确保了数据在不同深度学习框架（如YOLO、Detectron2、TensorFlow）中的高效使用。</p><h4>场景多样性</h4><p>数据集中的蜜蜂图像呈现了多种环境背景，包括但不限于：</p><ul><li><strong>花丛</strong>：蜜蜂在花朵上采蜜或飞行。</li><li><strong>树枝</strong>：蜜蜂在树枝附近活动，适应不同的自然环境。</li><li><strong>空旷地</strong>：蜜蜂在没有遮蔽的空旷环境中飞行或停驻。</li></ul><p>这些场景的多样性大大增加了数据集的泛化能力，能帮助模型识别不同环境中的蜜蜂，增强其在实际应用中的效果。</p><h3>适用场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412962" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>本数据集广泛适用于以下领域：</p><h4>1. 农业监控</h4><p>在农业中，蜜蜂作为重要的授粉昆虫，直接影响农作物的产量和质量。利用目标检测技术对蜜蜂进行实时监控，可以帮助农业研究者分析蜜蜂活动的规律，及时发现蜜蜂数量的变化，从而为农作物的授粉提供有效支持。</p><h4>2. 生态研究</h4><p>蜜蜂的行为与种群动态在生态学研究中具有重要价值。通过本数据集，生态研究人员可以分析蜜蜂的分布、迁徙模式及其生态环境的变化。这些数据为生态系统的保护和蜜蜂种群的可持续性提供了宝贵的数据支持。</p><h4>3. 无人机监控</h4><p>结合无人机的图像采集技术，可以实现蜜蜂的大范围监控与数据采集。利用目标检测模型对无人机拍摄的图像进行分析，可以实时获取蜜蜂活动数据，辅助研究人员在广阔区域内对蜜蜂进行精确的监测。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412963" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>目标检测模型训练建议</h3><p>本数据集适用于主流的目标检测框架，如YOLO、Detectron2、TensorFlow等。为确保高效训练，以下是一些使用建议：</p><h4>1. 数据划分</h4><p>建议将数据集按照以下比例进行划分：</p><ul><li>训练集：80%</li><li>验证集：10%</li><li>测试集：10%</li></ul><p>这样的划分可以确保训练过程中模型能够在验证集上进行及时调优，同时使用测试集评估最终模型的泛化能力。</p><h4>2. 模型选择</h4><ul><li><strong>YOLO</strong>：YOLO系列模型非常适合目标检测任务，训练速度快，检测精度高，且支持实时推理。使用YOLO框架进行训练时，可以直接加载YOLO格式的标注数据集。</li><li><strong>Detectron2</strong>：Detectron2是Facebook AI研究院开发的目标检测框架，具有强大的功能和灵活性，适合进行高精度的目标检测任务。其支持多种标注格式，易于扩展与调试。</li><li><strong>TensorFlow</strong>：TensorFlow框架也是进行目标检测任务的重要选择，支持训练多种目标检测模型，如Faster R-CNN、SSD等。</li></ul><h4>3. 训练技巧</h4><p>在训练过程中，可以采用以下技巧来提高模型的性能：</p><ul><li><strong>数据增强</strong>：可以通过旋转、翻转、调整亮度等方式对图像进行数据增强，从而提高模型的鲁棒性。</li><li><strong>超参数调整</strong>：在验证集上进行模型超参数调优，特别是学习率、批量大小等关键参数。</li><li><strong>早停机制</strong>：设置早停机制，防止过拟合，并减少训练时间。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412964" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ul><h3>结语</h3><p>蜜蜂目标检测数据集是一个高质量、丰富多样的数据集，适用于各类目标检测任务，特别是蜜蜂行为分析、农业监控和生态保护等领域。通过充分利用该数据集，研究人员和开发者能够训练出高效、准确的蜜蜂检测模型，并将其应用于多种实际场景中，为农业生态保护与研究提供数据支持。</p><p>随着目标检测技术的不断进步，未来我们期望能进一步扩展数据集，增加更多的图像类型和检测目标，为深度学习和人工智能技术的广泛应用提供更强大的数据支持。</p>]]></description></item><item>    <title><![CDATA[为什么实时更新场景下 Doris 查询性]]></title>    <link>https://segmentfault.com/a/1190000047412991</link>    <guid>https://segmentfault.com/a/1190000047412991</guid>    <pubDate>2025-11-20 00:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今数据驱动的商业环境中，企业越来越依赖数据分析来驱动决策。无论是用户行为分析、业务报表还是运营监控，企业都需要具备快速、高效的数据处理能力。企业在数据分析能力上的演进，往往始于 TP（事务处理）系统，随着业务发展不断探索 TP 系统的扩展方案，最终走向构建独立的 AP（分析处理）系统。</p><h2>企业实时分析典型演进过程</h2><h3>第一阶段：使用 TP 系统支撑事务处理和数据分析</h3><p>在企业信息系统建设的早期，主要存储在 OLTP（在线事务处理）系统中，比如 PostgreSQL、MySQL、SQL Server 等。因为数据“就在那儿”，最自然的方式就是直接从 TP 系统中执行 SQL 查询来获取所需分析数据：</p><ul><li>查询订单、用户、库存、交易等业务数据；</li><li>生成运营报表，支撑内部管理；</li><li>快速开发查询接口，满足临时的 BI 需求。</li></ul><p>在业务初期，数据规模有限，分析需求也相对简单，系统架构轻量，能够高效支撑当下业务，避免了多系统部署和复杂数据流带来的额外成本与运维压力。然而，随着业务快速发展，数据量迅速增长，分析场景愈发复杂，查询延迟上升，事务与分析负载相互影响，原有系统逐渐难以支撑持续扩展的业务需求。</p><h3>第二阶段：探索 TP 系统的扩展方案</h3><p>为了在不破坏 TP 系统稳定性的前提下支撑持续扩展的业务需求，部分企业开始尝试基于 TP 的扩展方案。以下是行业中常见的做法与代表系统：</p><p><strong>1. 分片与分布式扩展：Citus、TiDB、Vitesse</strong> </p><p>这些系统具备了基于关系数据库的水平扩展能力，使 TP 系统可以存储更大规模的数据并处理更高并发。</p><ul><li><strong>Citus</strong>：是 PostgreSQL 的分布式扩展插件，可将表自动切分到多个节点，实现并行处理和查询加速。</li><li><strong>TiDB</strong>：兼容 MySQL 协议的分布式 HTAP 数据库，事务与分析融合，适用于在线业务+报表的场景。</li><li><strong>Vitesse</strong>：基于 MySQL 的分布式中间件，解决数据库扩展、容错和自动化问题，常用于大规模 TP 系统。</li></ul><p><strong>2. 读写分离与多副本架构：Amazon Aurora</strong></p><ul><li><strong>Amazon Aurora</strong> 提供了高性能、弹性扩展的云数据库，并支持最多 15 个只读副本，用于分担查询压力。这类方案适用于中等复杂度的分析任务，但在海量数据和复杂查询面前，Aurora 等 TP 架构依然面临查询瓶颈。</li></ul><p>这些方案虽然在一定程度上缓解了单机性能瓶颈，但本质仍属于 TP 系统架构，难以根本解决事务与分析并存所带来的矛盾。面对大规模、多维度的聚合分析，这类系统能力有限，查询操作常常干扰写入，导致系统性能波动，影响整体稳定性。同时，架构复杂、运维门槛高，随着写入量和查询压力的持续上升，资源消耗不断加剧，系统成本快速攀升，性能瓶颈也日益显现。尤其在高吞吐数据导入和实时更新方面能力不足，限制了对业务变化的快速响应。而 TP 系统以行存为主的特性，使其在处理 TB 级以上数据的分析任务时，性能与专为分析设计的 AP 系统存在显著差距，难以胜任更复杂、更大规模的分析需求。</p><h3>第三阶段：复杂分析使用 AP 系统</h3><p>随着数据量不断增长、分析需求日益复杂，很多企业逐渐意识到，无法再依赖原有的 TP 系统同时满足事务处理与分析需求。因此，企业通常会将一些复杂的分析查询迁移到专门的 AP 系统中，例如 Redshift、Snowflake、BigQuery 等，用于支撑大规模的数据分析任务。而对于对实时性要求高、并发量大的查询，仍会保留在 TP 系统中运行，以确保系统的快速响应和稳定性。在一些高并发场景中，数据甚至会在 AP 系统中完成加工处理后，回流到 TP 系统中，进一步支撑实时查询和业务服务。</p><h3>第四阶段：拥抱 AP 系统，实现分析计算与事务的解耦</h3><p>随着数据规模持续扩大，事务处理系统（TP）的数据导入速度难以跟上行为数据生成的节奏，导致数据延迟持续增加。与此同时，部分复杂查询被转移到分析处理系统（AP）执行，其他分析任务仍在 TP 系统中完成，这使得系统运维难度和资源成本不断攀升，远超专门的 AP 系统。</p><p>下表总结了 OLTP 与 OLAP 在关键维度上的主要区别，便于理解两者在架构定位上的差异。</p><h3>OLTP vs OLAP 对比</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412993" alt="OLTP vs OLAP 对比.jpeg" title="OLTP vs OLAP 对比.jpeg"/></p><p>面对这种挑战，越来越多企业开始认识到：对于实时分析场景，采用具备高并发与高性能的一体化 AP 系统来统一承载，能够大幅提升效率；而批量处理和离线分析等需求，则可以选择更适合的 AP 系统来完成。这样不仅优化了整体架构，也有效控制了成本，同时提升了数据分析能力，助力企业实现更加高效的数据驱动运营。</p><p>实时数据需求直接导入实时 OLAP 系统，常见做法是将事务处理系统（TP）与分析处理系统（AP）解耦，通过变更数据捕获（CDC）技术，实现 TP 系统数据的实时同步，同时行为数据也直接写入实时 OLAP。实时 OLAP 系统需具备快速更新能力和高效查询性能。该架构不仅避免了对核心业务系统的影响，还使企业能够第一时间获取并分析最新数据，广泛应用于用户行为分析、实时报表、风险监控等关键场景，显著提升了数据决策的及时性和价值。</p><h2>OLAP 系统的选择：为什么是 Apache Doris？</h2><p>Apache Doris 适合大数据量下需要高并发查询、AdHoc 和实时数据更新的场景：</p><h3>低延迟高吞吐写入</h3><p>支持多种数据导入方式，通过 <code>Stream Load</code> 可实现数据<strong>秒级可见</strong>、单节点写入吞吐可达<strong>百万行/秒</strong>，轻松满足海量数据的实时入库需求。</p><h4>用户案例：网易云音乐</h4><p>网易云音乐作为知名音乐流媒体平台，每天产生大量用户行为数据、业务数据及日志数据，这些数据在异常行为跟踪、客诉问题定位、运行状态监控、性能优化等方面扮演守护者的角色。面对每日万亿级别数据的增量，<strong>网易云音乐选择使用 Apache Doris 替换 ClickHouse 构建新的日志平台，目前已稳定运行 3 个季度，规模达到 50 台服务器，2PB 数据，每天新增日志量超过万亿条，峰值写入吞吐达 6GB/s。</strong></p><p><strong>在低延迟高吞吐写入场景中</strong>，网易云音乐采用 Flink + Doris Connector 实现流式数据无缝对接，通过多项关键优化措施显著提升了写入性能：</p><ul><li><strong>写入流程优化</strong>：在 append 数据操作时，直接写入压缩流，无需经过 ArrayList 中转，大幅降低内存使用，TM 内存占用从 8G 降至 4G，有效避免了因 batch size 设置过高导致的 OOM 问题。</li><li><strong>单 tablet 导入功能</strong>：开启单 tablet 导入功能（要求表使用 random bucket 策略），极大提升写入性能，解决了写入 tablet 过多时元数据产生过多影响写入性能的问题。</li><li><strong>负载均衡优化</strong>：每个 batch flush 完成后随机选择 BE 节点写入数据，解决 BE 写入不均衡问题，相较之前导入性能提升 70%。</li><li><strong>容错能力增强</strong>：调整 failover 策略，优化重试逻辑并增加重试时间间隔，当 FE/BE 发生单点故障时能自动感知和重试恢复，保证服务高可用。</li></ul><p><strong>在元数据性能优化方面</strong>，针对 HDD 硬盘环境下 Stream Load 耗时突增问题，通过调整 3 台 Follower FE 为异步刷盘模式，实现了 4 倍性能提升，有效解决了同步元数据阶段的严重耗时问题。</p><p><strong>整体收益显著</strong>：查询响应整体 P99 延迟降低 30%，并发查询能力从 ClickHouse 的 200 提升至 500+，写入稳定性大幅改善，运维成本显著降低，在坏盘和宕机场景下实现自恢复能力。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=gucMtvDM7k6n1LBPsCpIgw%3D%3D.v%2Bf67bzW9WNwQCZJRQMDmBt6gHCa7kv8LwAgVj7q16v%2FXJG66SudSQSnZzGHjmsq" rel="nofollow" target="_blank">网易云音乐基于 Apache Doris 的万亿级日志平台建设实践</a></p><h3>实时更新能力强</h3><p>配合主流数据同步工具（如 Kafka、Canal、DataX、SeaTunnel 等）可实现从 TP 系统到 Doris 的准实时数据同步。其“Merge-on-Write”的更新机制兼顾写入性能与更新效率，适配主流 CDC 场景。</p><h4>用户案例 ：中通快递</h4><p>随着中通快递业务的持续增长，昔日双 11 的业务高峰现已成为每日常态，原有数据架构在数据时效性、查询效率、与维护成本方面，均面临着较大的挑战。为此，中通快递引入 SelectDB，借助其高效的数据更新、低延时的实时写入与优异的查询性能，在快递业务实时分析场景、BI 报表与离线分析场景、高并发分析场景中均进行了应用实践。</p><p><strong>在实时分析场景中，基于 SelectDB 灵活丰富的 SQL 函数公式、高吞吐量的计算能力，实现了结果表的查询加速，能够达到每秒上 2K+ 数量级的 QPS 并发查询，数据报表更新及时度大大提高。</strong></p><p>SelectDB 的引入满足了复杂与简单的实时分析需求。目前，SelectDB 日处理数据超过 6 亿条，数据总量超过 45 亿条，字段总量超过 200 列，并实现服务器资源节省 2/3、查询时长从 10 分钟降至秒级的数十倍提升。</p><p>案例回顾：<a href="https://www.bilibili.com/video/BV1eF6HY9Ecr/" target="_blank">中通快递基于 SelectDB 实时数仓的应用实践</a></p><h3>极致的查询性能</h3><p>Doris 天生为分析优化，具备列式存储、向量化执行引擎、位图索引、多级缓存、物化视图等优化手段，能够支撑亚秒级的分析响应时间，并支持复杂的多维分析、聚合与 JOIN 查询。</p><h4>用户案例 ：拉卡拉</h4><p>拉卡拉（股票代码 300773）是国内首家数字支付领域上市企业，从支付、货源、物流、金融、品牌和营销等各维度，助力商户、企业及金融机构数字化经营。随着实时交易数据规模日益增长，拉卡拉早期基于 Lambda 架构构建的数据平台面临存储成本高、实时写入性能差、复杂查询耗时久、组件维护复杂等问题。<strong>拉卡拉选择使用 Apache Doris 替换 Elasticsearch、Hive、HBase、TiDB、Oracle/MySQL 等组件，完成 OLAP 引擎的统一，实现了查询性能提升 15 倍、资源减少 52% 的显著成效。</strong></p><p><strong>在极致查询性能场景中</strong>，拉卡拉充分发挥了 Apache Doris 天生为分析优化的多项能力，在金融核心业务中实现了显著的性能提升：</p><ul><li><strong>风控场景查询优化</strong>：替换 Elasticsearch 后，查询响应时间从 15 秒缩短至 1 秒以内，查询性能提升 15 倍。Doris 的标准 SQL 查询接口和强大的多维分析能力，支持复杂的多表 JOIN、子查询和窗口函数等场景。</li><li><strong>对账单系统高并发处理</strong>：借助 Doris 优秀的并发处理和极速查询能力，支持每日亿级数据规模和百万级查询请求，查询 99 分位数响应时长控制在 2 秒以内，数据延迟可控制在 5 秒。</li><li><strong>倒排索引加速大表关联</strong>：通过添加倒排索引、调整分桶策略以及表结构优化，大表关联查询耗时从 200 秒缩短至 10 秒，查询效率提升超过 20 倍。</li><li><strong>Light Schema Change 灵活变更</strong>：相比 Elasticsearch 需要通过 Reindex 进行 Schema 变更，Doris 的 Light Schema Change 机制更加高效灵活，支持字段和索引的快速增删修改，极大提升了数据管理的便捷性和业务适应性。</li></ul><p><strong>整体收益显著</strong>：服务器数量下降 52%，开发运维效率大幅提升，通过统一的 OLAP 引擎简化了技术架构，降低了学习成本和运维复杂性。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9RWTXpccH0RKojqONkJoZQ%3D%3D.JQykpenpgUuz%2FycWb%2BoIaJAcmw%2FZd3Q3lz7JWwlGa7hpDgEDyEHclmn9x0HFVI9x" rel="nofollow" target="_blank">拉卡拉 x Apache Doris：统一金融场景 OLAP 引擎，查询提速 15 倍，资源直降 52%</a></p><h3>高并发处理能力</h3><p>得益于 MPP 架构，Doris 可轻松扩展计算资源，支持海量用户并发访问分析报表，是支撑数据门户、运营后台、用户行为分析等实时应用场景的理想方案。</p><h4>用户案例：快手</h4><p>快手作为知名短视频平台，OLAP 系统为内外多个场景提供数据服务，每天承载近 10 亿的查询请求。原有湖仓分离架构面临存储冗余、资源抢占、治理复杂等问题。<strong>快手通过引入 Apache Doris 湖仓一体能力替换 ClickHouse，升级为湖仓一体架构，涉及数十万张表、数百 PB 的数据增量处理。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 凭借强大的 MPP 架构和分布式查询引擎，为快手提供了卓越的并发查询支撑能力：</p><ul><li><strong>海量并发查询支撑</strong>：系统每天承载近 10 亿查询请求，覆盖 ToB 系统（商业化报表引擎、DMP、磁力金牛、电商选品）和内部系统（KwaiBI、春节/活动大屏、APP 分析、用户理解中心等）的高并发访问需求。</li><li><strong>智能查询路由</strong>：通过查询路由服务分析和预估查询的数据扫描量，将超大查询自动路由到 Spark 引擎，避免大查询占用过多 Doris 资源，确保高并发场景下的系统稳定性。</li><li><strong>物化视图透明改写</strong>：结合 Doris 的物化视图改写能力和自动物化服务，实现查询性能提升至少 6 倍，百亿级别以下数据可实现毫秒级响应，有效支撑高并发查询场景。</li><li><strong>缓存优化加速</strong>：通过元数据缓存和数据缓存双重优化，元数据访问平均耗时从 800 毫秒降至 50 毫秒，显著提升高并发场景下的查询响应效率。</li></ul><p><strong>整体收益</strong>：实现了统一存储和链路简化，无需数据导入即可直接访问湖仓数据，在支撑海量并发查询的同时大幅降低了运维复杂度和存储成本。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=BPOhVPoCwZe659CHxQTEJQ%3D%3D.wVLvr0LG%2FDtEN%2BiRNHtzmkrnlam20s5eLzJtfS6zGYQRGAVXjcC30urhITwapzr3" rel="nofollow" target="_blank">快手：从 Clickhouse 到 Apache Doris，实现湖仓分离向湖仓一体架构升级</a></p><p><strong>统一数据体验</strong> Doris 提供类 SQL 的接口，兼容 MySQL 协议，易于 BI 工具对接（如 Tableau、Power BI、Superset 等），同时可通过视图、物化视图等能力，提供类似数据仓库的建模支持。</p><p>可以满足的典型场景包括：</p><ul><li><strong>面向用户的实时分析（Customer-Facing Analytics）</strong> 将订单、交易、行为等业务数据从数据库中捕获并同步至实时数仓（如 Apache Doris），支持用户在前端系统中秒级查看订单状态、活动参与情况、积分变化等。提升用户体验的同时，也为推荐、搜索等系统提供最新的数据支持。</li><li><strong>运营监控与分析</strong> 运维、客服、市场等部门可以实时查看关键指标（如系统交易量、失败率、退货率），并快速响应业务波动。CDC 保证了数据与业务系统的实时一致性，使监控结果具有可信度。</li><li><strong>模型训练与特征回填</strong> 将用户行为日志和业务数据同步到分析库后，ML 工程师可以基于最新数据生成训练样本、回填特征值，显著加快模型迭代速度，提升预测准确率。</li><li><strong>多维分析与自助 BI</strong> 将结构化业务数据实时汇聚到 OLAP 系统，结合维度模型，支持业务人员进行灵活多维分析，满足从明细到聚合的多层级洞察需求，减少对开发人员的依赖。</li></ul><h2>ClickHouse 实时更新原理</h2><p>在 ClickHouse 中，虽然底层存储以追加为主，但通过 <code>ReplacingMergeTree</code> 引擎，用户可以实现类似“实时更新”的效果。其核心思想是：在写入时保留所有版本的数据，在后台合并时自动保留最新版本，从而实现数据的“更新”。</p><blockquote>详情<a href="https://link.segmentfault.com/?enc=9WhmDjn7tkDsjNwUmlqbHg%3D%3D.PdmIPkBVO%2BU9Co7%2B0OSNi%2BM9Tg7qx7rC%2B39tHAspchSCAmCWAu6pJeldDFCJ2zV72kgpe8hVH7j%2FaihHEW7g4g%3D%3D" rel="nofollow" target="_blank">参考文档</a></blockquote><p>具体工作原理如下：</p><ol><li><strong>写入阶段</strong> 使用 <code>ReplacingMergeTree</code> 创建表时，通常会指定一个唯一标识列（如主键）和一个版本列（如 <code>update_time</code>）。每次对同一主键的数据更新时，系统不会直接覆盖旧数据，而是插入一行新版本的记录。</li><li><strong>合并阶段（Merge）</strong> ClickHouse 的后台合并线程会在空闲时自动执行数据文件的合并操作。对于 <code>ReplacingMergeTree</code> 表，合并过程中会根据主键值和版本列，自动保留每组主键下版本最新的记录，删除旧版本，实现最终的“更新”语义。</li><li><strong>查询阶段</strong> 查询过程中可能会读到尚未合并的多个版本记录，因此建议设置 <code>FINAL</code> 查询语法，如：</li></ol><pre><code class="SQL">SELECT * FROM my_table FINAL;</code></pre><h2>Apache Doris 实时更新原理</h2><p>在需要频繁更新数据的场景中，可以使用 Doris 提供的 Unique Key 模型来建表，实现对同一主键的数据进行高效覆盖更新。Doris 通过一种名为标记删除（Delete Bitmap）的机制，有效提升查询性能。与 ClickHouse 查询时进行更新清理的方式不同，Doris 的标记删除机制无需在查询时实时计算删除逻辑，因此可以显著减少查询延迟，确保查询响应时间稳定在百毫秒以内，并支持高并发访问。</p><p>具体来说，Doris 的处理分为两个阶段：</p><ol><li><strong>写入阶段</strong> 在使用 Unique Key 创建表时，您通常会指定一个唯一标识（如主键 ID）和一个版本列（如更新时间 <code>update_time</code>）。每当新数据写入时，如果主键相同且版本更新，Doris 会自动为旧数据打上“删除标记”，这些信息随着数据一同写入底层存储。</li><li><strong>查询阶段</strong> 在查询过程中，Doris 会自动识别并跳过那些已被标记删除的旧数据行，无需实时对比或扫描多个版本，从而实现低延迟、高效率的数据读取。</li></ol><p>借助这套机制，Doris 能够同时满足 <strong>实时更新</strong> 和 <strong>高速查询</strong> 的双重需求，非常适合用于用户画像、订单中心、指标快照等典型更新型分析场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412994" alt="Apache Doris 实时更新原理.png" title="Apache Doris 实时更新原理.png" loading="lazy"/></p><h2>性能对比</h2><p>当分析负载从 TP 或者 HTAP 演进到 AP 时，一个常见场景是将 TP 系统中的变更数据（通过 CDC）同步到 AP 系统，用于后续的报表分析和业务监控。这类场景通常涉及大量的<strong>数据更新</strong>，而不仅仅是新增数据，因此对分析系统的更新处理能力和查询性能提出了更高要求。</p><p>为了评估 Doris 和 ClickHouse 在这一类实时更新分析场景下的表现，我们基于典型的行业测试模型 <strong>ClickBench</strong> 和 <strong>SSB（Star Schema Benchmark）</strong> 进行了测试，分别对数据集中的 <strong>25% 和 100% 的记录进行了更新操作</strong>。</p><blockquote><a href="https://link.segmentfault.com/?enc=Rkz5uIE8kpcxxfeFJSPZVg%3D%3D.hohOd08le8PguJkPeOUw95wmr8gEbXWCfKF0X29BqoIq112yHdu%2FPnNEvcb10VAlwf%2BdtA3v6WvdOUJC8EBzkdoCfNlmeuUXAtCfRdEK%2Fps%3D" rel="nofollow" target="_blank">更新 SQL </a> 详情参考</blockquote><p>为确保性能对比的合理性，结合 ClickHouse Cloud 与 SelectDB Cloud 套餐配置的差异，制定了如下测试方案：ClickHouse Cloud 采用双副本，单副本分别配置为 8 核 32GB 和 16 核 64GB；SelectDB Cloud 则采用单副本 16 核 128GB 配置。通过该设计，可在整体资源层面分别实现核数对等（16 核）与内存对等（128GB）的横向对比，从而更全面地评估两者在不同资源维度下的性能表现。</p><p>原始数据：<a href="https://link.segmentfault.com/?enc=n3tBH%2FAGA7dRV4DL7JnIdw%3D%3D.jgTOPi98ezE6vdePS6GxXRUsTlhWYKtVVUWt7zUJqp%2Bfz%2Bq0SxuKgdNF8WSUPYB5PbewiwOwHI2PHibs6T6S1w%3D%3D" rel="nofollow" target="_blank">ssb</a></p><h3>SSB-sf100</h3><h4>ClickHouse MergeTree  vs  SelectDB Duplicate Key</h4><ul><li>SelectDB （16c 128GB）的性能是 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）的 5 倍。</li><li>SelectDB （16c 128GB）的性能是 ClickHouse 16c 64GB（2 replica 每个 replica 16c 64GB）的 9.8 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412995" alt="性能对比-1.png" title="性能对比-1.png" loading="lazy"/></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>25% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 1.6 倍。</li><li>100% 更新比例下，ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降 2.5 倍。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412996" alt="性能对比 02.png" title="性能对比 02.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 14 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 18 倍。</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 25 倍。</li><li>100% 更新比例条件下，SelectDB 的性能是 ClickHouse 的 34 倍。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412997" alt="性能对比-2.png" title="性能对比-2.png" loading="lazy"/></p><h3>ClickBench</h3><p>原始数据：<a href="https://link.segmentfault.com/?enc=8%2FUxz8kvcI2zK9nU8NPdEA%3D%3D.jJ%2Fah4d%2Ffwb3VImorzaQ8W3mlwVw3jgjVCfafRQjbrgiW9tOzg9SUhQjZwxoqarnQDuS4dg2gYXcFXZQfureXg%3D%3D" rel="nofollow" target="_blank">clickbench</a></p><h4>ClickHouse MergeTree  vs  ReplacingMergeTree</h4><ul><li>ClickBench 下 25% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 170%。</li><li>ClickBench 下 100% 更新比例 ClickHouse ReplacingMergeTree 相比 MergeTree 性能下降超过 290%。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412998" alt="性能对比-3.png" title="性能对比-3.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412999" alt="性能对比-4.png" title="性能对比-4.png" loading="lazy"/></p><h4>ClickHouse ReplacingMergeTree vs SelectDB UniqueKey</h4><ul><li><p>SelectDB （16c 128GB）相比 ClickHouse 32c 128GB（2 replica 每个 replica 16c 64GB）</p><ul><li>25% 更新比例条件下，查询耗时低 43%</li><li>100% 更新比例条件下，查询耗时低 60%</li></ul></li><li><p>SelectDB （16c 128GB）相比 ClickHouse 16c 64GB（2 replica 每个 replica 8c 32GB）</p><ul><li>25% 更新比例条件下，查询耗时低 68%。</li><li>100% 更新比例条件下，查询耗时低 78%。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413000" alt="性能对比-5.png" title="性能对比-5.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413001" alt="性能对比-6.png" title="性能对比-6.png" loading="lazy"/></p><h2>用户案例</h2><h3>案例一：森马服饰（MySQL）</h3><p>森马服饰作为中国休闲服饰和童装领域的领先企业，覆盖线上线下全渠道零售，门店总数达到 8000+ 家。为支撑全域货通中台项目，<strong>森马引入阿里云 SelectDB 替换原 Elasticsearch + 分布式 MySQL 混合架构，统一分析 16+ 核心业务，实现复杂查询 QPS 提升 400%，响应时间缩短至秒级。</strong></p><p><strong>在高并发处理场景中</strong>，阿里云 SelectDB 凭借 MPP 架构为森马提供了强大的并发查询支撑：</p><ul><li><strong>多场景并发支撑</strong>：同时支撑 2B 业务、2C 业务、直营店、加盟商等多场景下的高并发数据分析需求，复杂查询 QPS 达到 200+ 水平。</li><li><strong>资源隔离能力</strong>：基于存算分离架构，在线订单查询服务和离线聚合分析 BI 场景分别使用独立计算组，避免相互干扰，确保高并发场景下系统稳定性。</li><li><strong>弹性扩缩容</strong>：在直播大促等高压力时段，可快速在线扩容应对流量激增，无需停服和数据搬迁，显著提升应对突发高并发的灵活性。</li><li><strong>统一架构简化</strong>：替换双系统架构，统一支持简单过滤查询、海量数据聚合分析、复杂多表关联查询，无需维护复杂业务逻辑来处理高并发多表关联分析。</li></ul><p><strong>显著收益</strong>：亿级库存流水聚合查询缩短至 8 秒内，运维成本大幅降低，业务高峰期系统运行平稳，为全渠道运营提供可靠的高并发数据分析支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=UN61ZOZgmF83qRcbsXftQw%3D%3D.YZGUTNWiBDhN%2B4cyoL7xkBH68wsgKemvxTWKdFKOUZDE%2BFYikwil8tmPB236tG%2FI" rel="nofollow" target="_blank">森马服饰从 Elasticsearch 到阿里云 SelectDB 的架构演进之路</a></p><h3>案例二：天眼查（PostgreSQL）</h3><p>天眼查是一家数据服务公司，为用户提供超过 3 亿家企业的商业、财务和法务信息查询服务，涵盖 300+ 维度数据。随着业务增长，其尽调平台需要支持内部营销和运营团队的即席查询及用户分群等新需求。<strong>该平台使用 Apache Doris 替换了原有的 Apache Hive、MySQL、Elasticsearch 和 PostgreSQL 混合架构，实现数据写入效率提升 75%，用户分群延迟降低 70%。</strong></p><p><strong>在高并发处理场景中</strong>，Apache Doris 的 MPP 架构为平台提供了强大的并发查询支撑能力：</p><ul><li><strong>即席查询能力</strong>：原架构每次新需求都需要在 Hive 中开发测试数据模型，写入 MySQL 调度任务。现在 Apache Doris 拥有全量明细数据，面对新请求只需配置查询条件即可执行即席查询，仅需低代码配置即可响应新需求。</li><li><strong>高效用户分群</strong>：在结果集小于 500 万的用户分群场景中，Apache Doris 能够实现毫秒级响应。通过连续密集的用户 ID 映射优化，用户分群延迟降低 70%，显著提升高并发分群任务处理效率。</li><li><strong>统一架构简化</strong>：消除了多组件间的复杂读写操作，无需预定义用户标签，标签可基于任务条件自动生成，大幅简化用户分群流程，提高 A/B 测试的灵活性。</li><li><strong>稳定数据写入</strong>：支持每天近 10 亿条新数据流入，使用不同数据模型适配不同场景（MySQL 数据采用 Unique 模型，日志数据采用 Duplicate 模型，DWS 层数据采用 Aggregate 模型）。</li></ul><p><strong>显著收益</strong>：数据仓库架构更加简单，对开发者和运维人员更加友好，2 个 Apache Doris 集群承载数十 TB 数据，为客户提供实时、准确的企业信息查询服务。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=nwBEl0YGmwWgqY4Db6LnAA%3D%3D.ljdueJtfWkhKHmOTA5aiOFa%2FY5%2BkoVE0ETjGd8gMVwO4FFclwvbvjgzeqBu1dDvQ" rel="nofollow" target="_blank">秒级数据写入，毫秒查询响应，天眼查基于 Apache Doris 构建统一实时数仓</a></p><h3>案例三：宝舵 BOCDOP（TiDB）</h3><p>宝舵是宝尊集团旗下商业化独立品牌，拥有 1000 余名技术工程师，为集团 8000+ 员工和全球 450+ 品牌提供电商全渠道数据分析服务。<strong>宝舵早期基于 TiDB 构建实时数仓，随着数据量增长面临处理效率、OLAP 扩展、成本等挑战。通过引入 SelectDB 替换 TiDB，实现写入速度提升 10 倍，成本直降 30% 的显著成效。</strong></p><p><strong>在实时更新场景中</strong>，SelectDB 为宝舵提供了强大的实时数据处理能力，特别体现在多源数据同步方面：</p><ul><li><strong>多源异构数据实时接入</strong>：支持 100+ 业务模块的多渠道数据实时接入，通过 Canal、Mongo-Connector、OGG 等工具获取 MySQL、MongoDB、Oracle 等不同类型业务数据库的 binlog，实现秒级延迟数据同步。</li><li><strong>高吞吐实时写入</strong>：利用分区分桶策略与单副本写入机制，在双 11 峰值时段实现每秒百万级数据写入，最高写入速度从 20 万/分提升至 230 万/分，较传统方案提升 10 倍。</li><li><strong>流式数据处理</strong>：通过 Kafka + Flink + SelectDB 流式写入能力，将分散在订单、支付、物流等业务模块的数据实时汇聚，数据同步提速 30%。</li><li><strong>资源隔离保障</strong>：为"作战室看板"单独分配计算资源组，避免高并发查询与实时写入的资源争用，确保关键业务查询响应时间稳定在 500ms 内。</li></ul><p><strong>显著收益</strong>：在双 11 等大促期间数据量达平日 30-60 倍的情况下，实现数据供应 0 事故、报表服务可用性 99.9%，查询性能提升 66%，为多渠道电商运营提供稳定的实时数据支撑。</p><p>完整阅读：<a href="https://link.segmentfault.com/?enc=9wdZQTdAQ9RnY4TC0CbTMw%3D%3D.ATu7H%2Bp3yL5tAvJOK7R9UBSyD7PImHJj%2FYZI8BN%2F%2F%2FAa5UOz%2BB7nFVobDXTqwOlw" rel="nofollow" target="_blank">SelectDB 实时分析性能突出，宝舵成本锐减与性能显著提升的双赢之旅</a></p>]]></description></item><item>    <title><![CDATA[恶意滥用行为 留胡子的饼干_dlibGI]]></title>    <link>https://segmentfault.com/a/1190000047413006</link>    <guid>https://segmentfault.com/a/1190000047413006</guid>    <pubDate>2025-11-20 00:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>从生成式 AI 的惊艳亮相引起全球科技巨头军备竞赛般的投入开始，整个 AI 行业仿佛被注入了无限的想象力。似乎在宣告着即将出现一个生产力即将被彻底解放、商业模式即将被完全颠覆的光明未来。<br/>微软、谷歌、亚马逊等云巨头纷纷将资本支出的绝大部分押注于 AI 基础设施建设，而无数逐利而来的 AI 初创公司，更是如雨后春笋般涌现试图分一杯羹，全球 AI 领域的投资额也达到了史无前例的高度。<br/>然而，正如任何过热的淘金热最终都会迎来冷静期当技术以超乎预期的速度普及时，潜在的负面效应也以同样的速度被放大，正在悄然侵蚀着行业参与者。<br/>从 " 可选项 " 到 " 必选项 " 的巨额支出<br/>根据奇安信集团对外发布《2024 人工智能安全报告》来看，在 2023 年基于 AI 的深度伪造欺诈便已暴增了 3000%，基于 AI 的钓鱼邮件也增长了 1000%；而内容生成环节更是实现规模化生产。<br/>基于 Stable Diffusion 和 GPT-4 的定制模型，可每小时生成 2000 条伪原创研报、800 段逼真视频。暗网平台 "DarkGPT" 更是提供包月服务，1 万美元即可获得每日 5000 条金融虚假内容的产能。<br/>而且 "AI 滥用 " 的后遗症并不仅仅在社会新闻版块，可以说它已经穿透了科技公司的防火墙直接作用于其财务报表。而金融行业正是这场风暴的中心，当 AI 以假乱真的能力被精准地应用于金融诈骗时，其破坏力可以说是指数级的增长。<br/>据行业估算，2024 年由深度伪造技术引发的各类欺诈造成的全球经济损失已高达 120 亿美元。尤其在监管相对滞后、交易更为匿名的加密货币领域，AI 滥用更是如鱼得水。根据相关的报告也显示 2024 年仅 AI 深度伪造技术全年造成的损失便高达 46 亿美元。<br/>随着 AI 滥用事件的频发，过去模糊的 " 伦理指南 " 正在迅速转变为具有强制约束力的法律条文，而且这种转变直接导致了企业合规成本的急剧攀升。<br/>而且一旦出现违规需要付出的代价更是惨痛的，罚款最高可达全球年营业额的数个百分点或数千万欧元，而且合规也不再是法务部门的单一工作，而是渗透到研发、产品、市场的每一个环节。<br/>这些 " 反噬 " 也并非凭空产生，在 AI 商业化过程中对速度和规模的追求，长期以来压倒了对安全和伦理的考量所以形成了这种 " 原罪 "。因此未来合规成本的升高是不可避免的，而欧盟的《AI 法案》可以说是这一趋势的先行者。<br/>该法案于 2024 年 8 月 1 日正式生效并分阶段实施，着重对高风险的 AI 系统施加了严格的合规要求。而且这不仅仅是一项区域性法规，更可能产生 " 布鲁塞尔效应 " 从而影响全球的 AI 监管格局。<br/>监管的落地也将会直接转化为企业的合规成本。据公开信息推算，仅欧盟 AI 法案便可能导致欧洲企业的 AI 采纳成本增加约 310 亿欧元，并使 AI 投资减少近 20%。而美国联邦贸易委员会也已对 OpenAI 展开调查，谷歌等公司也不得不调整其营销话术，避免被处以巨额罚款。<br/>可以说 " 监管的铁幕 " 正在迫使整个行业从过去 " 快速行动，打破陈规 " 的互联网思维转向一种更为审慎、合规驱动的开发模式。可以说这种转变无疑会减缓创新速度并增加运营成本，对于那些资源有限的中小企业和初创公司构成尤为严峻的挑战。<br/>对 " 信任 " 的侵蚀或许是 AI 滥用最难修复的一种<br/>这源于在激烈的竞争压力下，企业急于抢占市场将产品快速推向用户，所以将风险控制和安全测试置于次要位置。但是这种 " 快速行动并打破规则 " 的心态在 AI 时代尤为危险，因为 AI 技术的潜在破坏力远超以往的软件应用。<br/>并且市场对于 AI 技术的可靠性极度敏感，甚至一次小小的失误都可能引发巨大的信任危机和财务损失。谷歌的 Bard 模型之前便在一次演示中出现事实性错误，竟然导致其母公司 Alphabet 的股价在单日内暴跌 7%，市值蒸发超过 1000 亿美元。<br/>并且随着 AI 投资的巨额支出持续攀升，投资者开始担忧其回报前景，这种悲观情绪导致 Meta、Microsoft、Alphabet 和 Nvidia 等 AI 领域的领军企业股价普遍承压下跌，市场也开始讨论 "AI 泡沫 " 的风险，并开始质疑哪些不计成本的 " 军备竞赛 " 式投资。<br/>更何况大量公司缺乏对 AI 伦理的明确责任归属，高管层面也并未对其有所调整。所以 AI 系统的决策过程像一个 " 黑箱 "，在责任主体模糊的情况下滥用和误用的风险便难以控制。企业内部也未建立有效的问责机制。<br/>但是更深层次的原因在于当前主流生成式 AI 商业模式本身所内含的风险。这些模型依赖于海量数据的投喂，其训练过程难以完全避免偏见和有害信息的吸收。而其强大的生成能力却为恶意利用提供了温床。<br/>因此当商业模式的核心是追求更强大的模型、更广泛的应用时，如果缺乏与之匹配的强大 " 安全刹车 " 系统，滥用就成了可预见的副产品。这种商业逻辑与伦理要求之间的结构性失衡才是导致 " 反噬 " 的根本内因。<br/>所以当企业享受了技术红利带来的增长，如今便也不得不为其模式所伴生的风险 " 买单 "。哪怕科技公司以 " 让世界更美好 " 的叙事推广 AI，公众在实际体验中，也会频繁受到隐私泄露、算法偏见、就业替代、虚假信息等负面影响。<br/>这种落差也导致了广泛的 "AI 焦虑 " 和不信任感。公众普遍认为现有的监管法规不足以应对 AI 带来的社会风险期望政府采取更加果断的行动。这种强大的民意压力也是推动监管机构加速行动的根本动力。<br/>面对公众的呼声和潜在的社会风险，监管机构的介入是必然的。但由于技术发展的速度远超立法速度监管往往表现出一定的滞后性，欧盟 AI 法案便被部分人士认为可能增加企业负担、抑制创新。<br/>全球主要经济体在 AI 领域的竞争，也使得监管变得更加复杂。各国都希望在鼓励创新和防范风险之间找到平衡点但这种平衡点的位置各不相同，因此形成了复杂的国际监管格局给跨国企业的合规带来了巨大挑战。<br/>而且这种外部滥用对整个 AI 行业的声誉造成了 " 连坐 " 效应。即使一家公司本身恪守伦理，也无法完全独善其身，因为公众对 AI 的信任是整体性的。恶意滥用行为如同向池塘中投下的毒药，在污染了整个水域后迫使所有 " 池中生物 " 共同承担后果。<br/>这场危机成为 AI 自我革新的契机<br/>这场 " 反噬 " 带来的阵痛，是 AI 产业从野蛮生长走向规范发展的必经阶段。它正在淘汰那些只想赚快钱、缺乏责任感的 " 玩家 "，筛选出真正有实力、有远见的长期主义者。从长远来看，这也是为 AI 产业的健康、可持续发展所必须付出的代价。<br/>其中最大的机遇在于将 " 信任 " 从一种道德呼吁，转变为一种可量化、可变现的商业资产和竞争壁垒。数据显示近 85% 的客户也更愿意与重视 AI 伦理实践的公司合作，而那些优先考虑伦理和透明度的公司收入增长也更快。<br/>可以说在 AI 产品同质化日益严重的未来，谁能赢得用户的信任谁就能赢得市场。" 负责任的 AI" 将不再仅仅是公关部门的口号，而是必须贯穿于产品设计、开发、部署全流程的核心战略。<br/>谷歌和微软等公司已经开始调整其策略，谷歌选择利用 AI 技术提升广告安全审核的效率，打击欺诈内容；微软则发布了负责任 AI 透明度报告，并推出了 AzureAIContentSafety 等服务，帮助客户构建更安全的 AI 应用。这些举措既是应对风险的防御，也是在构建新的竞争优势。<br/>正是 " 反噬 " 催生了全新的 " 安全即服务 " 市场。随着 AI 滥用风险的加剧企业对 AI 安全审计、风险评估、内容过滤、合规咨询等服务的需求将急剧增长。这为专门从事 AI 安全和伦理治理的科技公司、咨询机构创造了巨大的市场空间。<br/>而科技巨头自身也可以将其内部成熟的安全工具和能力平台化、服务化，开拓新的收入来源。例如，谷歌和微软在内容审核、风险识别方面的技术积累，完全可以转化为对外输出的商业服务。<br/>虽然监管的收紧虽然带来了成本，但也为行业设定了 " 准入标准 "，能够率先满足高标准合规要求的企业将获得更强的市场公信力和竞争优势，从而在未来的市场整合中占据有利地位。这实际上是一种由监管驱动的市场出清和格局优化。weibo.com/ttarticle/p/show?id=2309405234825153348099<br/>weibo.com/ttarticle/p/show?id=2309405234825304342582<br/>weibo.com/ttarticle/p/show?id=2309405234829599310183<br/>weibo.com/ttarticle/p/show?id=2309405234829741654183<br/>weibo.com/ttarticle/p/show?id=2309405234829880066874<br/>weibo.com/ttarticle/p/show?id=2309405234830022673245<br/>从滥用事件的激增，到资本市场的审慎，再到全球监管的收紧，这股 " 反噬 " 之力正在重塑 AI 产业的发展轨迹。它迫使整个行业从过去对技术力量的无限崇拜，转向对技术责任和社会价值的深刻反思。<br/>麦肯锡预测，到 2030 年 AI 将为全球经济创造 13 万亿美元价值。但价值分配取决于我们如何驾驭这头猛兽。未来的竞争，将不仅仅是模型参数和算力大小的竞争，更是治理能力、责任担当和用户信任的竞争。</p>]]></description></item><item>    <title><![CDATA[《从被动修复到免疫：游戏Bug闭环体系的]]></title>    <link>https://segmentfault.com/a/1190000047412846</link>    <guid>https://segmentfault.com/a/1190000047412846</guid>    <pubDate>2025-11-19 23:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>每一个Bug的出现，都绝非孤立的代码失误，可能是模块间数据流转的隐性断点、场景触发条件的边缘冲突，或是玩家非常规操作与设计预期的偏差，甚至可能是架构层面的适应性缺陷。这些异常表现如同系统的“隐性病灶”，轻则影响局部体验，重则引发连锁反应，导致核心玩法崩塌、玩家流失。多数开发团队对Bug的处理仍停留在“发现-修复-验证”的线性流程，将Bug视为需要消灭的“敌人”，却忽视了其背后承载的系统优化价值。真正成熟的Bug处理逻辑，并非以“零Bug”为终极目标—这在复杂游戏生态中几乎不具备可行性—而是构建一套让系统能够自我感知、自我调节、自我进化的“自愈体系”。这套体系的核心价值，在于通过对Bug全生命周期的深度拆解与闭环管理，将每一次异常处理转化为系统的“免疫记忆”，让同类问题的复发概率持续降低，同时推动架构、设计与测试环节的协同优化。在长期的开发实践中深刻体会到，那些能够在多版本迭代与海量玩家检验中保持体验稳定性的游戏，其背后必然存在一套超越表面流程的Bug自愈闭环。它不是一份僵化的操作手册，而是与游戏架构深度绑定、与开发节奏动态适配、与团队认知共同成长的思维模式，从Bug的源头预判到根因追溯，再到经验沉淀与体系迭代，每一个环节都在为系统韧性注入养分，最终实现体验品质的长期守恒。</p><p>构建Bug自愈体系的首要前提，是打破“被动等待Bug暴露”的传统模式，建立一套具备“预判性”与“协同性”的多维度感知网络。游戏开发中，Bug的发现渠道往往分散在内部测试、玩家反馈与线上监控三个维度，但多数团队未能让这三者形成有效协同，导致大量隐性Bug在上线后才集中爆发，增加了修复成本与体验损失。内部测试环节的核心痛点在于“场景覆盖的局限性”，传统测试用例多基于设计文档的预期流程，侧重验证功能的正常运行，却容易忽略不同模块交互时的边缘场景、极端数值组合、跨场景切换的时序冲突，或是玩家突破设计预期的非常规操作路径—比如在战斗中同时触发多个道具效果、在剧情触发节点强制退出游戏、在网络波动时进行关键操作等。解决这一问题的关键，并非无限制扩充测试用例数量，而是构建“模块交叉场景库”：以游戏核心玩法为轴心，梳理每个模块与其他系统的关键交互节点，比如战斗系统与道具系统的数值联动逻辑、剧情触发与场景切换的时序校验机制、网络同步与本地计算的一致性保障流程、角色状态与环境交互的边界条件等，将这些交叉点转化为可复现、可量化的测试场景，同时引入“反向测试思维”，主动模拟玩家可能的异常操作，提前暴露潜在风险。玩家反馈则常常呈现“碎片化”与“模糊化”特征，玩家往往只能描述异常现象（如“奖励未到账”“角色卡住”），却无法提供精准的触发条件、操作路径与设备信息。此时需要建立一套“反馈信息提炼机制”，通过对反馈内容中的关键词聚类、场景描述还原、设备型号与系统版本统计，从大量零散信息中识别出共性问题，区分“个体设备兼容问题”“网络环境导致的偶发异常”与“系统性Bug”。例如，当多名玩家反馈“某副本结算时奖励缺失”，通过提取他们的操作路径（是否中途退出、是否组队参与、是否触发特殊剧情分支）、设备类型（移动端/PC端）、网络状态（Wi-Fi/流量）等信息，可快速锁定结算逻辑中与“状态判定”“数据同步”相关的漏洞。而线上监控的核心，不应局限于报错日志的统计与告警，更要关注“异常行为序列”的捕捉与分析—比如玩家在某一功能模块的操作频率突然异常（远超正常玩家的点击次数）、某一场景的加载时长出现离散型峰值（多数玩家加载正常，少数玩家加载超时）、特定操作后玩家的退出率显著上升（如使用某道具后立即退出游戏）等，这些隐性信号往往是未被发现的Bug的前兆。通过将内部测试的“模块交叉场景库”、玩家反馈的“信息提炼机制”与线上监控的“异常行为捕捉”三者深度联动，让感知网络具备“主动识别”与“精准定位”能力，在Bug影响范围扩大前就完成初步判定，为后续的快速修复争取时间，同时减少无效排查带来的开发资源消耗。</p><p>Bug发现后的分级与优先级判定，是决定自愈体系效率的核心环节，其本质是对“影响权重”的精准权衡与动态调整。多数团队采用“严重程度+影响范围”的二元分级法，将Bug简单划分为致命、严重、一般、轻微四个等级，但这种方式容易陷入“高优先级Bug拥堵”“重要Bug被遗漏”或“资源分配失衡”的困境—比如将所有影响核心玩法的Bug都标记为高优先级，导致开发人员陷入多线作战，反而降低了整体修复效率；或是忽视了某些看似轻微但高频出现的体验类Bug，长期积累后影响玩家口碑。真正合理的分级逻辑，需要构建一个多维度的“影响权重模型”，除了直观的影响范围（覆盖玩家数量）与严重程度（是否阻碍核心流程），还需纳入“潜在扩散风险”“修复成本”“版本节奏适配性”“玩家感知敏感度”四个关键指标。潜在扩散风险指Bug是否可能随着玩家行为的传播、版本迭代中的模块联动，从局部场景蔓延到更多功能模块，比如某类数值计算错误若未及时修复，可能会在后续的道具更新、活动上线、跨服玩法开启后引发连锁反应，导致数值平衡崩坏；修复成本则需综合评估所需的开发工时、跨模块协作成本（是否需要多个团队配合）、代码改动范围（局部调整还是架构层面的修改），以及修复后可能引入新问题的概率，避免为了修复一个低影响Bug而占用核心功能开发、版本上线筹备等关键任务的资源；版本节奏适配性则要求分级与当前开发阶段的核心目标匹配，比如临近上线时，对影响核心玩法运行、付费流程、账号安全的Bug需优先处理，而在迭代中期，可适当将资源倾斜给那些虽不紧急但影响长期体验的隐性Bug（如极端场景下的轻微卡顿、界面显示瑕疵）；玩家感知敏感度则需结合游戏的目标用户群体特征，比如面向核心玩家的竞技类游戏，对操作响应延迟、数值平衡性相关的Bug敏感度极高，而面向休闲玩家的养成类游戏，可能更关注剧情连贯性、道具获取体验相关的问题。在实践中，我们将Bug划分为“阻断级”“核心体验级”“一般体验级”“隐性优化级”四类：阻断级指直接导致游戏无法运行、玩家进度丢失、核心玩法失效或账号安全风险的Bug，需启动紧急响应流程，暂停非核心开发任务，集中核心开发人员进行修复，必要时可采取临时屏蔽功能、回滚版本等应急措施；核心体验级指不影响游戏基本运行，但会严重破坏玩家沉浸感、影响核心玩法体验的Bug，如战斗系统的技能释放异常、剧情触发断裂、关键道具无法使用等，需在当前版本周期内优先处理，确保不影响版本核心目标的达成；一般体验级指对核心玩法无影响，但存在显示异常、音效缺失、操作逻辑不流畅等问题的Bug，可根据资源情况安排修复，若当前版本资源紧张，可纳入下一个迭代周期；隐性优化级则指在特定极端条件下才会触发、影响范围极小且不影响核心体验的Bug，如特定设备下的界面布局轻微偏移、极端数值组合下的非关键数据显示错误等，可纳入长期优化队列，结合后续版本的模块优化一并处理。分级的核心不是给出固定标签，而是建立“动态调整机制”—某一隐性优化级Bug若在后续迭代中因模块变动、玩法扩展而扩大影响范围，需及时提升优先级；而部分一般体验级Bug若玩家反馈集中、舆情关注度高，即使修复成本较高，也需重新评估资源分配，避免因忽视玩家感受导致留存下滑。通过这套多维度的分级模型与动态调整机制，让团队能够在复杂的开发节奏中，精准分配修复资源，既保证核心体验的稳定性，又避免资源浪费。</p><p>Bug修复环节的关键，在于避免“头痛医头、脚痛医脚”的表面修复，建立“全链路管控”机制，确保修复的有效性、安全性与彻底性。很多开发团队在修复Bug时，往往只关注报错的直接原因，比如看到“数据为空”的报错就直接添加空值判断，看到“界面显示异常”就调整布局参数，却忽略了Bug产生的上下文逻辑、数据流转链路与潜在关联影响，导致修复后不久同类问题再次出现，或引入新的兼容性漏洞、逻辑冲突。修复前的“根因定位”需要突破“代码层面”的局限，深入到“架构逻辑”“设计初衷”与“模块交互”的层面，还原Bug的完整生命周期。例如，某游戏曾出现“跨场景传送后角色技能CD异常重置”的Bug，初期开发人员仅针对传送逻辑中的CD数值赋值进行修正，但问题反复出现，甚至在后续版本中衍生出“技能效果无法正常触发”的新问题。直到团队重新梳理了技能系统、场景系统、网络同步三者的交互流程，才发现根源在于传送时的状态同步时序错误—角色技能CD的本地计算与服务器同步存在时间差，传送触发的状态重置指令覆盖了正确的CD数值，而初期的修复仅解决了表面的数值赋值问题，并未修复时序同步的核心矛盾。这一案例说明，有效的根因定位需要“层层拆解、溯本求源”：首先还原Bug的触发条件（如操作路径、场景环境、数值组合），然后梳理相关模块的交互链路（如数据从客户端到服务器的流转过程、不同系统的调用顺序），再分析每个环节的逻辑是否存在漏洞（如状态判定条件是否完善、数据同步是否一致、边界值处理是否全面），最终找到导致系统失效的核心断点。修复过程中，需坚守“最小改动原则”，即仅针对根因涉及的逻辑进行必要调整，避免为了简化修复而修改无关代码，或进行大面积的架构重构—除非根因明确指向架构层面的缺陷。同时，需建立“修复上下文档案”，详细记录修复思路、涉及的模块与代码范围、可能影响的功能点、测试验证的重点方向，便于后续的追溯与复盘。修复后的验证环节，不能依赖单一测试人员的确认，而要构建“多层验证体系”：开发自验需复现原始Bug场景及相关交叉场景（如与修复逻辑相关的模块交互场景、极端数值场景），确保修复有效且未影响其他功能；测试专项验证需结合“模块交叉场景库”，覆盖与修复逻辑相关的所有交互节点，同时进行兼容性测试（不同设备、系统版本）与压力测试（高并发场景）；线上灰度验证则针对影响范围较大的Bug（如核心玩法相关、覆盖大量玩家的问题），选择小部分服务器或特定玩家群体（如内测玩家、付费玩家）进行测试，观察修复后的系统稳定性、性能表现与玩家反馈，避免全量上线后引发新的问题。通过这套“根因定位-最小修复-多层验证”的全链路管控机制，确保每一次Bug修复都能彻底解决问题，同时规避修复带来的次生风险。</p><p>Bug修复完成并非自愈体系的终点，真正的价值沉淀来自于“根因追溯与经验复用”，让每一次Bug处理都成为系统优化的养分。多数团队在Bug验证通过后便结束流程，将其从任务列表中移除，却错失了通过单个Bug优化整个系统、提升团队能力的机会。每一个Bug的产生，都暴露了系统在设计、开发、测试或协作环节的薄弱点—可能是设计文档中的逻辑模糊地带、模块交互的边界定义不清晰，可能是编码规范的缺失、跨模块协作时的沟通偏差，也可能是测试用例的覆盖不足、自动化测试的场景遗漏。根因追溯的核心就是将这些薄弱点从“个案问题”转化为“通用规则”，避免同类问题重复出现。根因追溯需避开“归因于偶然”“归因于个人失误”的误区，从多个维度进行深度拆解：若Bug源于设计逻辑的漏洞，需反思设计文档是否存在歧义、模块交互的流程是否经过充分评审、是否考虑了极端场景与异常分支；若源于开发实现的偏差，需审视编码规范是否完善、代码审查是否到位、跨模块协作时的接口定义是否清晰、是否存在技术认知上的盲区；若源于测试覆盖的缺失，需分析测试用例的设计是否全面、测试方法是否合理、是否缺乏针对边缘场景的专项测试；若源于协作流程的问题，需思考沟通机制是否高效、信息同步是否及时、责任划分是否明确。在实践中，我们建立了“Bug复盘双周会”制度，选取典型Bug案例（如高频出现的同类问题、修复成本高的复杂问题、影响范围广的核心问题）进行集体拆解，而非局限于修复者个人的总结。复盘会并非简单的“问题汇报”，而是让设计、开发、测试、运营等相关人员共同参与，从各自的专业视角分析问题产生的原因：设计人员反思逻辑漏洞，开发人员分享编码过程中的困惑，测试人员总结覆盖不足的教训，运营人员反馈玩家的实际感受。例如，针对某类反复出现的“数值同步异常”Bug，复盘后发现，核心问题在于不同模块对同一数值的修改权限未做明确界定，导致多线程操作时出现数据冲突，同时测试用例中缺乏对多线程场景的覆盖。随后团队优化了数值管理架构，明确了核心数值的修改流程、同步机制与权限划分，同时补充了多线程场景的测试用例，引入自动化测试工具覆盖相关交互，后续同类Bug的出现频率下降了70%以上。复盘的关键不仅在于记录根因，更在于将复盘结论转化为可落地的优化措施：针对设计层面的问题，更新设计规范文档，增加模块交互评审环节，要求核心功能必须出具详细的异常分支处理方案；针对开发层面的问题，完善编码检查清单，强化代码审查中的重点关注项（如接口兼容性、数据校验、多线程安全），组织技术分享会弥补团队的认知盲区；针对测试层面的问题，扩充“模块交叉场景库”，引入自动化测试覆盖高频交叉场景与边缘场景，建立测试用例评审机制；针对协作层面的问题，优化信息同步工具，明确Bug处理的责任划分与沟通流程。通过这种方式，每一次Bug复盘都在为系统构建“防御工事”，为团队积累“避坑经验”，让自愈体系具备持续优化的能力，实现“处理一个Bug，解决一类问题”的目标。</p>]]></description></item><item>    <title><![CDATA[《游戏Bug快修手册：根因锁定与最小改动]]></title>    <link>https://segmentfault.com/a/1190000047412849</link>    <guid>https://segmentfault.com/a/1190000047412849</guid>    <pubDate>2025-11-19 23:02:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>多数研发团队面对突发Bug时，往往陷入“海量日志狂刷+无目标调试+仓促修改”的低效循环：有人埋头排查代码细节却忽视场景关联性，有人急于提交修复版本却未验证边缘情况，最终不仅浪费了黄金修复时间，还可能因盲目改动引入新的逻辑冲突，导致问题扩大化。快速解决Bug的核心，从来不是单纯追求“修复速度”，而是建立一套“精准定位→优先级动态判定→最小风险修复→分层高效验证→经验沉淀复用”的体系化能力。这种能力的本质，是对游戏系统逻辑的深度认知、排查思路的结构化拆解，以及风险控制的前置意识。Bug的解决效率，往往取决于能否在复杂的系统链路中快速锁定核心矛盾，而非被无关细节裹挟。长期的研发实践中深刻体会到，那些能在紧急场景下从容破局的团队，都具备一套独特的排查思维：他们不会急于动手修改，而是先通过场景拆解、逻辑溯源缩小问题范围，再以“最小改动”原则精准打击根因，最后用分层验证确保修复安全。这种思路彻底打破了“越快修复越容易出错”的固有误区，让快速修复与风险控制形成良性平衡，真正实现“高效且稳妥”的Bug解决闭环，既守住了用户体验的底线，也最大化降低了研发资源的浪费。</p><p>精准定位是快速解决Bug的前提与核心，其关键在于构建“场景拆解+逻辑溯源”的结构化排查路径，用科学方法替代盲目摸索，从源头缩短排查时间。很多研发同行容易陷入的典型误区是：拿到Bug反馈后立即投入日志排查，面对动辄数万条的日志数据无从下手，反复筛选却始终找不到关键信息，最终在无效操作中浪费大量时间。高效定位的第一步，是提炼“场景复现四要素”，通过用户反馈、后台数据、测试复现等多渠道收集完整信息：完整的操作路径（用户从进入场景到触发Bug的每一步操作，包括点击顺序、技能释放、道具使用等）、具体的环境条件（设备型号、系统版本、网络状态、服务器分区、游戏版本号等）、明确的数据状态（用户等级、角色属性、组队人数、副本进度、道具持有情况等）、清晰的触发频率（是100%触发、高频触发还是偶发触发，是否与特定时间、特定场景强关联）。例如，某动作游戏中“连续释放技能A后使用道具B，角色会出现1-2秒卡顿”的Bug，通过用户反馈与后台数据收集到四要素：操作路径（技能A×3+道具B点击）、环境条件（集中在安卓13及以上版本、Wi-Fi网络）、数据状态（角色等级30级以上、组队场景、副本为高难度模式）、触发频率（组队场景中触发率90%，单机场景零触发）。基于这些信息，首先通过反向推导排除无关因素：若为本地资源加载问题，单机场景应同样触发，因此排除；若为设备兼容性问题，不应仅局限于组队场景，因此缩小范围至“组队状态下的网络与数据同步”。接下来，沿着“组队状态激活→技能释放指令发送→服务器接收与处理→数据同步至组队成员客户端→客户端渲染反馈”的核心链路拆解，重点排查服务器与客户端的同步机制：服务器处理组队指令的并发能力、同步频率是否与客户端指令发送频率匹配、数据传输过程中是否存在丢包或延迟。最终发现，高难度副本中组队成员的技能指令发送频率较高，而服务器的同步频率设置过低，导致数据堆积在传输链路中，客户端接收数据不完整引发卡顿。定位阶段的另一关键是“排查优先级排序”：先排查高频触发场景（优先复现100%或高频触发的Bug，避免在偶发问题上浪费时间），再处理偶发场景；先验证核心逻辑链路（如数据流转、指令传输等核心流程），再排查边缘分支（如特殊数值、极端条件下的逻辑）；先排除简单易验证的原因（如配置错误、参数异常等可快速确认的问题），再深入复杂模块（如架构层面的时序冲突、多线程交互等）。这种结构化排查思路，能有效避免“大海捞针”式的无效操作，将定位时间缩短60%以上，为后续的修复工作争取宝贵的黄金时间。</p><p>Bug定位完成后，优先级的动态判定直接决定修复效率与资源分配的合理性，其核心是建立“三维评估模型”，在多任务并行的压力下精准锁定核心矛盾，避免资源浪费或核心问题延误。很多研发团队在优先级判定上存在明显误区：要么采用“一刀切”的简单分类（如仅按严重程度分为致命、严重、一般），导致所有“严重”Bug堆积，研发人员陷入多线作战，反而降低整体修复效率；要么过度依赖主观判断，忽视用户感知与修复成本的平衡，导致高敏感度Bug被遗漏，引发用户不满。三维评估模型的核心指标包括“影响范围”“用户敏感度”“修复成本”，三者需综合权衡而非孤立判定，每个指标都需结合游戏类型、研发阶段、运营场景进行细化拆解。影响范围不仅指覆盖的用户数量，还包括影响的功能重要性：如全服用户均可触发的Bug，或核心付费功能、核心玩法相关的Bug，影响范围权重更高；而仅影响少数特定设备、特定场景（如冷门单机副本）的Bug，影响范围权重较低。用户敏感度与游戏的核心定位强相关：竞技类游戏对操作响应延迟、数值平衡、对战公平性的敏感度极高，哪怕是轻微的数值计算错误，也可能引发核心用户的强烈不满；休闲养成类游戏则更关注剧情连贯性、道具获取体验、界面交互流畅度，对单机场景的轻微卡顿或显示异常敏感度较低。修复成本需从多维度综合评估：研发工时（解决问题所需的时间）、跨模块协作需求（是否需要多个团队或模块负责人配合）、代码改动范围（是局部参数调整、单一逻辑修改，还是涉及架构层面的重构）、风险系数（修复后是否可能引入新的逻辑冲突、兼容性问题）。例如，某跨服对战游戏中“结算时玩家积分计算错误”的Bug，影响范围是全服参与跨服玩法的用户（核心用户群体），用户敏感度（竞技公平性）拉满，而修复仅需调整服务器端的积分计算逻辑，改动范围小、工时短、风险低，应直接定为最高优先级，启动紧急修复流程，甚至暂停非核心功能的研发进度集中资源解决；而某单机解谜游戏中“某冷门关卡的背景音效缺失”，仅影响少数通关该关卡的用户，用户敏感度低，修复需调整多个音频预制件，还可能影响其他关卡的音效播放，修复成本高、风险高，可纳入后续迭代周期，待资源充裕时处理。优先级判定并非一成不变的静态标签，需建立动态调整机制：若某低优先级Bug的用户反馈量在短时间内激增（如1小时内反馈量突破千条），或被行业媒体、核心KOL提及引发舆情风险，需实时上调优先级，启动紧急处理；若高优先级Bug在修复过程中发现根因涉及架构层面（如数据存储方式不合理），修复成本远超预期（需耗时3天以上），则需评估是否采取临时规避方案（如屏蔽相关功能入口、限制触发条件），先缓解用户影响，再在后续版本中彻底重构，而非硬磕导致更大损失。精准的优先级判定，能让团队在复杂的研发节奏中始终聚焦核心矛盾，确保有限的研发资源投入到“影响大、用户敏感、成本低”的关键问题上，最大化提升修复效率与用户满意度。</p><p>高效修复的核心是“根因聚焦+最小改动”，既要保证问题彻底解决，又要将修复风险降至最低，避免因仓促改动或过度优化引发新的问题。很多研发人员在紧急场景下容易陷入两个极端：要么追求“一劳永逸”，试图通过全面重构解决问题，结果导致修复时间翻倍，还可能破坏原有稳定逻辑；要么仅针对表面现象进行“补丁式修复”，忽视根因，导致Bug反复出现。最小改动的本质是“精准打击”—仅针对Bug的根因涉及的逻辑进行必要调整，不触碰无关模块，不优化非相关问题，不引入新的功能或逻辑。例如，某RPG游戏中“组队副本结算时，部分玩家未收到奖励”的Bug，通过定位发现根因是“服务器端未正确读取组队成员的通关时长数据，导致奖励计算条件不满足”，此时只需修正服务器端读取通关时长的逻辑（如补充缺失的字段读取、修复条件判断错误），确保数据获取准确，即可彻底解决问题；若此时同时优化奖励发放的动画效果、调整其他道具的数值平衡、修复副本内的无关文本错误，不仅会大幅增加修复时间，还可能因改动过多引发新的兼容性问题（如动画效果与部分设备不兼容）或逻辑冲突（如数值调整导致其他奖励计算错误）。修复前的风险前置控制同样关键，是避免修复失败的重要保障：首先需备份核心代码分支，建立修复专用分支，确保修复过程中不影响主分支的稳定，若修复出现意外可快速回滚至稳定版本；其次需预留临时规避开关，在代码中加入功能屏蔽或触发限制开关，若修复后验证出现问题，可通过后台配置快速屏蔽相关功能，避免影响线上用户体验；最后需明确改动范围，与相关模块负责人同步修复方案，确认改动不会影响其他模块的逻辑（如是否涉及公共接口、共享数据结构），必要时进行交叉评审。修复过程中，需坚守“根因不明确不盲目动手”的原则—很多紧急场景下，研发人员为了赶时间，在根因尚未完全确认时就仓促修改，结果导致Bug反复出现或衍生新问题。例如，某手游中“网络波动时使用道具，道具消耗但效果未生效”的Bug，初期误判为本地数据存储问题，修改了客户端的道具消耗记录逻辑，结果问题仍持续出现，还导致部分用户的道具数据异常；直到重新定位发现，根因是服务器同步确认机制缺失—用户提交道具使用请求后，客户端未收到服务器的成功确认指令就默认消耗道具，而服务器因网络波动未处理该请求，导致数据不一致。这种情况下，盲目修复不仅浪费了宝贵的时间，还引发了新的用户投诉。因此，即使在最紧急的场景下，也需预留10-15分钟彻底确认根因：通过复现场景、梳理逻辑链路、验证假设，确保每一个修改都有明确的针对性，避免“试错式修复”。</p><p>修复完成后的高效验证，是避免Bug二次爆发、保障线上稳定的最后一道防线，核心在于构建“分层验证+场景覆盖”的闭环体系，在有限时间内兼顾验证速度与全面性，杜绝“修复一个Bug，引入另一个Bug”的情况。很多研发团队在紧急修复后，仅进行简单的功能验证（如复现原始场景确认Bug消失）就匆忙上线，结果因验证不充分导致Bug复现、边缘场景异常或衍生新问题，反而增加了后续处理的成本。分层验证体系分为三个核心环节：开发自验、测试专项验证、灰度验证，每个环节都有明确的验证重点与操作标准。开发自验需覆盖三个核心维度：原始Bug的触发场景（多次复现确保问题彻底解决，避免偶发修复）、相关联的边缘场景（如组队/单机、不同等级、不同道具组合、网络波动环境等，验证修复不影响其他功能）、反向测试（模拟异常输入、极端数值、错误操作，验证系统的容错能力）。例如，修复副本结算异常后，不仅要验证正常通关的结算结果，还要测试中途退出、组队解散、网络中断后重连、极端通关时长（如超短时间通关、超时通关）等边缘场景，确保结算逻辑在各种情况下都稳定；同时模拟服务器负载过高、数据传输延迟等异常情况，验证系统是否能正确处理错误数据，避免崩溃。测试专项验证需聚焦高风险点，结合游戏类型与Bug特征制定针对性验证方案：兼容性验证需覆盖主流设备（移动端需包含高中低端机型、不同品牌）、系统版本（安卓各版本、iOS各版本）、网络环境（Wi-Fi、4G、5G、弱网）；高并发验证需通过压力测试工具模拟峰值在线人数，测试服务器的承载能力与数据同步稳定性；数据一致性验证需重点核对客户端与服务器的核心数据（如角色属性、道具数量、积分排名），确保同步无误。考虑到紧急场景下的时间压力，测试专项验证可采用“核心场景优先”策略：优先覆盖用户高频使用的场景（如核心玩法、付费流程），再逐步扩展到边缘场景，避免因追求全面性导致上线延误。灰度验证是线上安全的关键保障，其核心是“小范围测试、快速反馈、动态调整”：选择10%-20%的核心用户群体（如付费用户、高活跃用户）或特定服务器（如新区、测试服）进行小范围上线，实时监控关键指标—Bug报错率、用户反馈量、功能使用率、服务器负载、数据同步成功率等；若15-30分钟内无异常反馈，且核心指标稳定，再逐步扩大灰度范围，最终全量上线；若出现异常，立即启动回滚机制，将影响范围控制在最小。验证过程中，需建立快速反馈通道：通过用户社群、客服系统、后台反馈入口收集用户的实时反馈，安排专人监控反馈信息，对用户提及的新异常快速响应，确保未覆盖的场景问题能及时发现。高效验证的核心不是“全面测试”，而是“精准覆盖风险点”—基于Bug的根因、改动范围、影响场景，聚焦最可能出现问题的环节，用最少的时间实现最大程度的风险排查，确保修复版本的稳定性。</p><p>快速解决Bug的长期效率提升，离不开系统性的经验沉淀与思路复用，核心是将单次修复的实践智慧转化为可复制、可传承的通用规则，推动团队整体处理能力的持续迭代，而非停留在“单次高效”的层面。很多研发团队在Bug修复后便结束流程，没有进行总结沉淀，导致同类问题反复出现，团队陷入“排查-修复-再排查”的恶性循环，长期来看反而浪费了大量研发资源。</p>]]></description></item><item>    <title><![CDATA[从 Flink 到 Doris 的实时数]]></title>    <link>https://segmentfault.com/a/1190000047412851</link>    <guid>https://segmentfault.com/a/1190000047412851</guid>    <pubDate>2025-11-19 23:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Flink-Doris-Connector 作为 Apache Flink 与 Doris 之间的桥梁，打通了实时数据同步、维表关联与高效写入的关键链路。本文将深入解析 Flink-Doris-Connector 三大典型场景中的设计与实现，并结合 Flink CDC 详细介绍了整库同步的解决方案，助力构建更加高效、稳定的实时数据处理体系。</p><h2>一、Apache Doris 简介</h2><p>Apache Doris 是一款基于 MPP 架构的高性能、实时的分析型数据库，整体架构精简，只有 FE 、BE 两个系统模块。其中 FE 主要负责接入请求、查询解析、元数据管理和任务调度，BE 主要负责查询执行和数据存储。Apache Doris 支持标准 SQL 并且完全兼容 MySQL 协议，可以通过各类支持 MySQL 协议的客户端工具和 BI 软件访问存储在 Apache  Doris 中的数据库。</p><p>在典型的数据集成和处理链路中，往往会对 TP 数据库、用户行为日志、时序性数据以及本地文件等数据源进行采集，经由数据集成工具或者 ETL 工具处理后写入至实时数仓 Apache Doris 中，并由 Doris 对下游数据应用提供查询和分析，例如典型的 BI 报表分析、OLAP 多维分析、Ad-hoc 即席查询以及日志检索分析等多种数据应用场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412853" alt="Apache Doris 简介 .PNG" title="Apache Doris 简介 .PNG"/></p><p>Flink-Doris-Connector 是 Apache Doris 与 Apache Flink 在实时数据处理 ETL 的结合，依托 Flink 提供的实时计算能力，构建高效的数据处理和分析链路。Flink-Doris-Connector 的使用场景主要分为三种：</p><ol><li><strong>Scan</strong>：通常用来做数据同步或是跟其他数据源的联合分析；</li><li><strong>Lookup Join</strong>：将实时流中的数据和 Doris 中的维度表进行 Join；</li><li><strong>Real-time ETL</strong>：使用 Flink 清洗数据再实时写入 Doris 中。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412854" alt="Apache Doris 简介 -2.PNG" title="Apache Doris 简介 -2.PNG" loading="lazy"/></p><h2>二、Flink-Doris-Connector 典型场景的设计与实现</h2><p>本章节结合 Scan、Lookup Join、Write 这三种场景，介绍 Flink-Doris-Connector 的设计与实现。</p><h3>01 Scan 场景</h3><p>Scan 场景指将 Doris 中的存量数据快速提取出来，当从 Doris 中读取大量数据时，使用传统的 JDBC 方法可能会面临性能瓶颈。因此 Flink-Doris-Connector 中可以借助 Doris Source ，充分利用 Doris 的分布式架构和 Flink 的并行处理能力，从而实现了更高效的数据同步。</p><h4>Doris Source 读取流程</h4><ul><li>Job Manager 向 FE 端发起请求查询计划，FE 会返回要查询的数据对应的 BE 以及 Tablet；</li><li>根据不同的 BE，将请求分发给不同的 TaskManager；</li><li>通过 Task Manager 直接读取每个 BE 上对应 Tablet 的数据。</li></ul><p>通过这种方式，我们可以利用 Flink 分布式处理的能力从而提高整个数据同步的效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412855" alt="Doris Source 读取流程.PNG" title="Doris Source 读取流程.PNG" loading="lazy"/></p><h3>02 Lookup Join 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412856" alt="Lookup Join 场景.PNG" title="Lookup Join 场景.PNG" loading="lazy"/></p><p>对于维度表存储在 Doris 中的场景，可通过 Lookup Join 实现对实时流数据与 Doris 维度表的关联查询。</p><h4>JDBC Connector</h4><p>Doris 支持 MySQL 协议，所以可以直接使用 JDBC Connector 进行 Lookup Join，但是这一方式存在一定的局限：</p><ul><li>Jdbc Connector 中的 Lookup Join 是同步查询的操作，会导致实时流中每条数据都要等待 Doris 查询的结果，增加了延迟。</li><li>仅支持单条数据查询，在上游数据量吞吐较高时，容易造成性能瓶颈和反压。</li></ul><h4>Flink-Doris-Connector 的优化</h4><p>因此针对 Lookup Join 场景 ，Flink-Doris-Connector 实现了异步 Lookup Join 和攒批查询的优化：</p><ul><li><strong>支持异步 Lookup Join：</strong> 异步 Lookup Join 意味着实时流中的数据不需要显式等待每条记录的查询结果，可以大大的降低延迟性。</li><li><strong>支持攒批查询：</strong> 将实时流的数据追加到队列 Queue 中，后台通过监听线程 Watcher，将队列里面的数据取出来再推送到查询执行的 Worker 线程池中，Worker 线程会将收到的这一批数据拼接成一个 Union All 的查询，同时向 Doris 发起 Query 查询。</li></ul><p>通过异步 Lookup join 以及攒批查询，可以在上游数据量比较大的时候大幅度提高维表关联吞吐量，保障了数据读取与处理的高效性。</p><h3>03 实时 ETL 场景</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412857" alt="实时 ETL 场景.png" title="实时 ETL 场景.png" loading="lazy"/></p><p>对于实时写入来说，Doris Sink 的写入是基于 Stream Load 的导入方式去实现的。Stream Load 是 Apache Doris 中最为常见的数据导入方式之一，支持通过 HTTP 协议将本地文件或数据流导入到 Doris 中。主要流程如下：</p><ul><li>Sink 端在接收到数据后会开启一个 Stream Load 的长链接请求。在 Checkpoint 期间，它会将接收到的数据以 Chunk 的形式持续发送到 Doris 中。</li><li>Checkpoint 时，会对刚才发起的 Stream Load 的请求进行提交，提交完成后，数据才会可见。</li></ul><h4>如何保证数据写入的 Exactly-Once 语义</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412858" alt="如何保证数据写入的 Exactly-Once 语义 .png" title="如何保证数据写入的 Exactly-Once 语义 .png" loading="lazy"/></p><p>那么，如何保证数据写入期间，端到端数据的精确一次性？</p><p>以 Kafka 同步到 Drois 的 Checkpoint 过程为例：</p><ol><li>Checkpoint 时，Source 端会接收到 Checkpoint Barrier；</li><li>Source 端接收到 Barrier 后，首先会对自身做一个快照，同时会将 Checkpoint Barrier 下发到 Sink 端；</li><li>Sink 端接收到 Barrier 后，执行 Pre-commit 提交，成功后数据就会完整写入到 Doris，由于此处执行的是预提交，所以在 Doris 上，此时对用户来说数据是不可见的；</li><li>将 Pre-Commit 成功的事务 ID 保存到状态中；</li><li>所有的算子 Checkpoint 都做完后，Job Manager 会下发本次 Checkpoint 完成的通知；</li><li>Sink 端会对刚才 Pre-commit 成功的事务进行一次提交。</li></ol><p>通过这种两阶段提交，就可以实现端到端的精确一次性。</p><h4>实时性与 Exactly-Once</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412859" alt="实时性与 Exactly-Once.png" title="实时性与 Exactly-Once.png" loading="lazy"/></p><p>上面提到，Doris Sink 端的写入与 Checkpoint 绑定，数据写入 Doris 的延迟性取决于 Checkpoint 的间隔。但在一些用户的场景下，希望数据可以实时写入，但是 Checkpoint 不能做的太频繁，同时对于一些作业来说，如果 Checkpoint 太频繁会消耗大量资源，针对该情况，Flink-Doris-Connector 引入了攒批机制，以平衡实时性与资源消耗之间的矛盾。</p><p>攒批的实现原理是 Sink 端接收上游数据之后，不会立即将每条数据单独写入 Doris，而是先在内存中进行缓存，然后通过对应参数设置，将缓存数据提交到 Doris 中。结合攒批写入和 Doris 中的主键模型，可以确保数据写入的幂等性。</p><p>通过引入攒批机制，既满足了用户对数据实时写入的需求，又避免了频繁 Checkpoint 带来的资源消耗问题，从而实现性能与效率的优化。</p><h2>三、基于 Flink CDC 的整库同步方案</h2><p>以上是对 Flink-Doris-Connector 的典型场景和实现原理介绍，接下来我们来看它在实际业务中的一个重要应用——整库同步。相比底层实现，整库同步更偏向具体使用场景。下面我们基于前面介绍的能力，进一步探讨如何通过 Flink CDC 实现 TP 数据库到 Doris 的高效、自动化同步。</p><h3>01 整库同步痛点</h3><p>在数据迁移过程中，用户通常希望可以尽快将数据迁移到 Doris 中，然而在同步 TP 数据库时，整库同步往往面临以下几点挑战：</p><ul><li><p><strong>建表：</strong></p><ul><li><strong>存量表的快速批量创建</strong>：TP 数据库中往往存在成千上万的表，这些表的结构各异，对于存量表而言需要逐一在 Doris 中创建对应的表结构；</li><li><strong>同步任务开启后，新增表的自动创建与同步：</strong> 为了保证数据的完整性和实时性，同步工具需要实时监控 TP 数据库的变化，并自动在 Doris 中创建和同步新表。</li></ul></li><li><strong>元数据映射：</strong> 上下游之间字段元数据的便捷映射，包括字段类型的转换、字段名称的对应修改等。</li><li><strong>DDL 自动同步：</strong> 增加、删除列等操作会导致数据库结构发生变化，进而影响到数据同步。因此，同步工具需要能够实时捕获 DDL 并动态地更新 Doris 表结构，以确保数据的准确性和一致性。</li><li><strong>开箱即用：</strong> 零代码，低门槛，理想的同步工具只需进行简单配置，即可实现数据的迁移和同步。</li></ul><h3>02 基于 Flink CDC 实现整库同步</h3><p>在数据抽取方面，Flink-Doris-Connector 借用了 Flink CDC 的特性能力：</p><ul><li><p>增量快照读取</p><ul><li>无锁读取与并发读取：不论存量数据量多大，都可以通过横向提高 Flink 的并发提升数据读取速度。</li><li>断点续传：当存量数据比较大时，可能面临同步中断的情况，CDC 支持中断任务的衔接同步。</li></ul></li><li>丰富数据源支持，Flink CDC 支持多种数据库，如 MySQL、Oracle、SQLServer 等。</li><li>无缝对接 Flink 现有生态，方便与 Flink 已有Source 和 Sink 结合使用。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412860" alt="基于 Flink CDC 实现整库同步.png" title="基于 Flink CDC 实现整库同步.png" loading="lazy"/></p><h4>一键建表与元数据自动映射</h4><p>Flink-Doris-Connector 中集成了 Flink CDC 等能力，可以让用户只提交一个操作，就能进行整库同步的操作。其主要原理是 Flink CDC Source 在接收到上游的数据源之后，会进行分流处理，不同的表用不同的 Sink。同时在最新的 Connector 版本中，也支持单个 Sink 同步多张表，支持新增表的创建和同步。</p><p>集成 Flink CDC 的功能后，<strong>用户仅需通过 Flink-Doris-Connector 提交任务，就可以在 Doris 自动创建所需的表，而无需配置上下游表之间的显式关联，实现数据快速同步</strong>。</p><p>当 Flink 任务启动后，Doris-Flink-Connector 将自动识别对应的 Doris 表是否存在。如果表不存在，Doris Flink Connector 会自动创建表，并根据 Table 名称进行分流，从而实现下游多个表的 Sink 接入；如果表存在，则直接启动同步任务。</p><p>这一改进，不仅简化了配置流程，还使得新增表的创建和同步更加便捷，从而提升数据处理的整体效率。</p><h4>Light Schema Change 与 DDL 自动同步</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412861" alt="Light Schema Change 与 DDL 自动同步.png" title="Light Schema Change 与 DDL 自动同步.png" loading="lazy"/></p><p>在 Apache Doris 1.2 版本之前，Schema Change 操作比较繁琐，需要手动增改数据列。在上游 TP 数据库发生表结构变更时，需要暂停数据同步任务、待 Doris 中的 Schema Change 完成后再重启任务。</p><p>自 Apache Doris 1.2 版本起，我们引入了轻量级的 Light Schema Change 机制，极大地简化了操作流程，常见的增减列场景其处理速度可达毫秒级。Light Schema Change 机制原理如下：</p><ul><li><p><strong>Schema Change：</strong></p><ul><li>客户端向 FE 发起增减列的请求；</li><li>FE 在接收到请求后，修改当前元数据，并将最新的 Schema 持久化；</li><li>FE 向客户端同步 Schema Change 的结果；</li></ul></li><li><p><strong>Data Load：</strong></p><ul><li>当后续导入任务发起时，FE 将导入任务与最新的 Schema 信息发送给 BE；</li><li>在数据写入过程中，BE 的每个 Rowset 都会存储当前导入的 Schema 信息；</li></ul></li><li><p><strong>Query：</strong></p><ul><li>FE 将查询计划与最新的 Schema 一起发送给 BE；</li><li>BE 使用最新 Schema 执行查询计划；</li></ul></li><li><p><strong>Compaction：</strong></p><ul><li>在 BE 中，对参与合并的 Rowset 版本进行比较；</li><li>根据最新的  Schema Change 信息进行数据合并。</li></ul></li></ul><p>经测试，与早期的 Schema Change 相比，Light Schema Change 的数据同步性能有了数百倍的提升，</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412862" alt="Light Schema Change 与 DDL 自动同步-2.png" title="Light Schema Change 与 DDL 自动同步-2.png" loading="lazy"/></p><p>Light Schema Change 与 Flink-Doris-Connector 的结合，通过 Flink CDC 可以实现 DDL 的自动同步，具体步骤如下：</p><ol><li>Source 端捕获上游 Schema Change 信息，开启 DDL 变更同步；</li><li>Doris Sink 端识别并解析 DDL 操作（加减列）；</li><li>Table 校验，判断是否可以进行 Light Schema Change；</li><li>发起 Schema Change 操作；</li></ol><p>基于这一实现，Doris 能自动获取到 DDL 语句并在毫秒级即可完成 Schema Change 操作，在上游 TP 数据库发生表结构变更时，数据同步任务无需暂停。</p><h4>开箱即用：MySQL 整库同步示例</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412863" alt="开箱即用：MySQL 整库同步示例.png" title="开箱即用：MySQL 整库同步示例.png" loading="lazy"/></p><p>对于用户来讲，只要有 Flink 客户端，通过上图的操作就可以提交整库同步作业。支持传入 Flink 的配置，比如并发设置、Checkpoint 间隔等，也支持正则表达式去配置需要同步的表， 同时可以将 Flink CDC Source 和 Doris Sink 的配置直接透传给具体的 Connector。通过这种方式，用户可以很便捷地提交整库同步作业。</p><h3>03 Flink-Doris-Connector 核心优势</h3><p>基于以上优化，可以完美解决用户的痛点：</p><ol><li>自动建表，即存量表与增量表的自动创建，无需用户提前在 Doris 中预先创建对应的表结构；</li><li>自动映射上下游字段，无需手动写入上下游字段间的匹配规则，节省大量人力成本；</li><li>增减列无感同步，及时获取上游 DDL 语句并自动在 Doris 中实现毫秒级 Schema Change，无需停服、数据同步任务平稳运行；</li><li>开箱即用，降低学习成本，更专注业务本身。</li></ol><h3>04 最佳实践</h3><p>在生产环境中，若作业数量较多，直接采用上述提交方式的作业管理复杂度较高。通常建议借助任务托管平台（如 StreamPark），实现对作业的统一创建、监控与运维，从而提升任务管理效率与系统稳定性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412864" alt="最佳实践.png" title="最佳实践.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412865" alt="最佳实践-2.png" title="最佳实践-2.png" loading="lazy"/></p><h2>四、未来规划</h2><p>未来，基于 Flink-Doris-Connector 的能力规划如下：</p><ol><li>支持实时读取。目前 Doris Source 只是把数据 Scan 出来，是一个有界流的读取，后续会支持 CDC 的场景，可以使用 Flink 来对 Doris 中的数据进行流式的读取。</li><li>Sink 一流多表。目前Flink-Doris-Connector支持单个 Sink 同步多张表，但是 Stream Load 的导入方式还是只支持单个表的导入。所以在表特别多的时候，需要在 Sink 端维护大量 StreamLoad 的连接，在后续会做到单个 Stream Load 的连接支持多张表的写入。</li><li>整库同步方面，支持更多的上游数据源，满足更多数据同步场景。</li></ol>]]></description></item><item>    <title><![CDATA[编程项目怎么学习 cpp辅导的阿甘 ]]></title>    <link>https://segmentfault.com/a/1190000047412883</link>    <guid>https://segmentfault.com/a/1190000047412883</guid>    <pubDate>2025-11-19 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>星球很多同学，在做星球项目，或者做自己项目的时候，都会遇到各种坎坷，说看不懂，不理解。</p><p>那项目，一个从未接触过的项目应该怎么学习呢。</p><h2>观点分享</h2><p>说方法之前，我们可以先对要学习的项目进行一个分类，分一下学习的两个境界。我认为可以整体分两类：</p><p>（1）一类是，自己学习，用于提升自己，用于跳槽，找工作给简历加分的 （个人项目）</p><p>（2）一类是，工作公司的项目，自己实际工作中的</p><h3>个人项目</h3><p>对于个人项目，拿来面试。面试主要考察什么呢，你这个人设计能力的完善性，即你项目的某个功能，对于极端场景是否有考虑到。</p><p>那这对于一个项目，熟悉到什么程度算可以了呢。主要就是项目的架构，项目功能的实现思路。对代码细节，写法没必要细究。</p><p>原因：</p><p>（1）相同的功能实现，不同的人可能就会有不同的写法，以及相同的人不同时期也会有不同的写法；</p><p>（2）面试重点是思维逻辑的交流，让人家可以听懂，可以认可，能够产生共鸣；毕竟人家也没看过你的代码，语法、写法人家也不知道，你说的这么细，反而让人家听不懂，效果还很差； </p><p>（3）这也是一直强调的，在学项目的时候也要注重文档的梳理编写。能够让一个搞python的，搞java的可以看懂，快速写出来。别说一堆自己项目自己命名，这确实详细，但是谁也看不懂，听不懂，那效果很差</p><h3>公司的项目</h3><p>公司的项目，我们进公司，主要是要解决项目bug，优化项目代码的，开发新功能的。解决项目的代码bug，肯定要能够精确定位，要对代码细节，调用过程了解，需要熟悉项目代码。</p><h2>建议</h2><p>知道了对于不同场景下，项目的学习程度。那么再聊聊项目应该怎么学习。</p><p>相信很多同学，都再网上听过很多前辈分享的各种源码阅读方法。比如main函数开始追、分功能模块看、按住一个功能调用过程追等等。</p><p>在这里，主要想给大家强调的方法是什么呢？</p><p><strong>借助AI，优先借助AI。</strong></p><p>现在AI能力，确实足够强大了，比如gpt5、claude 4.5等等。并且像个人项目一般最多也就几万行，或者就算公司项目上亿行代码，但是到你部门负责的可能也就几万行，数十万行，代码量都不大。可以先让AI对你的项目代码分析分析，架构、功能，实现逻辑等等。先通过它帮助你了解百分之七八十，再自己慢慢解决剩下的百分之二十，效率会高很多，很给力。</p><p>可能有的同学，在知名公司工作，说公司内部模型，没有这最先进的，其实用你们公司目前内供的，我认为目前也是可以帮助你进行分析的。</p><p>（为什么会给大家强调这个呢，主要还是通过大家问我的一些技术问题项目问题发现，这些问题直接喂给AI基本就可以快速出方案进行解决，远远没必要在那里抓脑瞎。给大家写这个，就是让大家有用AI的意识，优先考虑，现在模型能力是够的了）</p><h2>知识星球介绍（公认的cpp c++学习地）</h2><p>星球名字：奔跑中的cpp / c++</p><p>里面服务也不会变，四个坚守目前:</p><p>1.每天都会看大家打卡内容，给出合理性建议。</p><p>2.大家如果需要简历指导，心里迷茫需要疏导都可以进行预约周六一对一辅导。</p><p>3.每周五晚上九点答疑聊天不会变。</p><p>4.进去星球了，后续如果有什么其他活动，服务，不收费不收费(可以合理赚钱就收取下星球费用，但是不割韭菜，保持初心)</p><p>（还有经历时间考验的独家私密资料）</p><p>加入星球的同学都可以提问预约，一对一帮做简历，一对一  职业规划辅导    ，解惑。同时有高质量的项目以及学习资料</p><p>学cpp基础，可以把最近开发的这个编程练习平台利用起来<br/>cppagancoding.top</p><p>本文由<a href="https://link.segmentfault.com/?enc=5uTJ45M5FtItUCs18qiyVQ%3D%3D.KYsCbBaqEPDeQykZBSbhxyjUFONb6lGj8%2F33PniNwS0%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[LEANN：一个极简的本地向量数据库 本]]></title>    <link>https://segmentfault.com/a/1190000047412778</link>    <guid>https://segmentfault.com/a/1190000047412778</guid>    <pubDate>2025-11-19 22:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在软件开发领域，提到轻量级、嵌入式的本地数据库，我们首先会想到 <strong>SQLite</strong>，它快速且无需独立服务进程。现在检索增强生成（RAG）和向量数据库的世界里，一个定位相似的新工具出现了。你可以把LEANN看作是<strong>嵌入式、轻量级的向量数据库</strong>。它完全不需要依赖庞大的数据中心或者 GPU 集群。一个<strong>个人专属的 RAG 引擎</strong>，它能完全放在你的笔记本电脑里，可以索引和搜索数百万份文档，而且最不可思议的是，它比常规向量数据库<strong>少用了 97% 的存储空间</strong>，而且**准确性还没有任何损失。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412780" alt="" title=""/></p><h2>LEANN 的核心技术差异</h2><p>传统的向量数据库简直就是存储“巨兽”。它们为每一个文档都预先计算好嵌入（Embeddings）并全部存储下来，磁盘空间很快就会被塞满。LEANN则不一样：</p><p>它压根儿不存储所有嵌入，而是采用一种<strong>基于图的选择性重计算</strong>方法，并辅以<strong>高保留度的修剪（Pruning）</strong>。这几个术语听起来有点花哨，但核心思想很简单：<strong>只在真正需要时，才去计算必需的数据</strong>。</p><p>LEANN 不是一个嵌入数据的“囤积者”，它会按需重新计算嵌入，并通过一个非常<strong>轻量级的图结构</strong>将它们智能地连接起来。这让存储用量大幅下降，而图结构则确保了语义相似性和检索准确性得以完整保留。</p><h2>为什么这项技术值得关注</h2><p>有了 LEANN，你的笔记本电脑瞬间就能变身成一个个人 AI 搜索引擎。所有这些功能都<strong>在本地跑起来</strong>，<strong>没有一分钱的云服务开销</strong>，<strong>隐私也得到了绝对保障</strong>。</p><p>如果你使用 Claude Code，应该知道它目前的搜索能力只停留在基本的关键词匹配。LEANN 可以无缝接入，作为 <strong>MCP（Model Context Protocol）服务</strong>，为你的模型增加<strong>真正的语义检索能力</strong>，让你搜索的是“意义”，而不是单纯的“词语”。而且整个过程中你都不需要改变原有的工作流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412781" alt="" title="" loading="lazy"/></p><h2>机制解析</h2><p>LEANN 之所以强大，关键在于它<strong>修剪掉了大量的“赘肉”</strong>：</p><ul><li><strong>基于图的重计算</strong>从根本上消除了对大型嵌入仓库的需求。</li><li><strong>使用 CSR（Compressed Sparse Row，压缩稀疏行）格式修剪的图</strong>，显著降低了存储的额外开销。</li><li><strong>智能缓存与重计算逻辑</strong>在检索速度和磁盘使用之间找到了一个绝佳的平衡点。</li></ul><p>这些优化结合起来创造了一个既轻量又具备可扩展性的系统。这是一个能装进你笔记本电脑、却能处理数百万记录的数据库。</p><h2>快速上手</h2><p>上手 LEANN 非常简单，代码如下：</p><pre><code>#git clone https://github.com/yichuan-w/LEANN.git leann  
#cd leann  
#uv pip install leann  
      
from leann import LeannBuilder, LeannSearcher, LeannChat  
from pathlib import Path  
INDEX_PATH = str(Path("./").resolve() / "demo.leann")  
      
# Build an index  
builder = LeannBuilder(backend_name="hnsw")  
builder.add_text("LEANN saves 97% storage compared to traditional vector databases.")  
builder.add_text("Tung Tung Tung Sahur called—they need their banana‑crocodile hybrid back")  
builder.build_index(INDEX_PATH)  
      
# Search  
searcher = LeannSearcher(INDEX_PATH)  
results = searcher.search("fantastical AI-generated creatures", top_k=1)  
      
# Chat with your data  
chat = LeannChat(INDEX_PATH, llm_config={"type": "hf", "model": "Qwen/Qwen3-0.6B"})  
response = chat.ask("How much storage does LEANN save?", top_k=1)</code></pre><h2>总结</h2><p>LEANN 的出现，标志着向量数据库领域正在迎来自己的 <strong>SQLite 时刻</strong>。SQLite 的成功在于它提供了<strong>轻量级、零配置、无服务器</strong>的本地数据管理能力，它让应用开发者可以轻松地在边缘设备、桌面应用中嵌入强大的 SQL 能力。</p><p>LEANN 也在做同样的事情，但它针对的是 <strong>RAG 和语义搜索能力</strong>。它能够将数百万条嵌入向量存放在本地文件中还能快速检索，对于那些需要构建<strong>离线应用、移动端 AI 功能</strong>或纯粹关注<strong>个人数据隐私</strong>的开发者而言，LEANN 提供了一种开箱即用的、极度高效的解决方案。所以有兴趣的赶紧试试吧。</p><p><a href="https://link.segmentfault.com/?enc=bItDfZrtWs6fv91GcM7omA%3D%3D.4CmGpUDMGynEbqlluru9ym2f%2FNfdFM8Ia1z5qITRtDEBr5RB1zKHndK2dIw7Cerbou2V%2FaDGPxqkSGesPV%2FGfA%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/1b13106d6d7a46f2adebc33bc49ff8d8</a></p>]]></description></item><item>    <title><![CDATA[Doris MCP Server 0.5]]></title>    <link>https://segmentfault.com/a/1190000047412793</link>    <guid>https://segmentfault.com/a/1190000047412793</guid>    <pubDate>2025-11-19 22:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近日，Doris MCP Server 0.5.1 版本带来了多项企业级数据治理与分析能力的功能升级，进一步提升系统稳定性与易用性，欢迎下载体验。</p><p>项目地址：<a href="https://link.segmentfault.com/?enc=p6kvNSK2rMvz3AXspWJkPA%3D%3D.rMSJluJ6HSjnXRy4XIAt0W6XHGR8r%2Fx0UMf3DfYCFTHfy1ocMcHyazkD0KyvaFqB" rel="nofollow" target="_blank">https://github.com/apache/doris-mcp-server</a></p><h2>新增能力概览</h2><ul><li>全局 SQL 超时配置增强：支持通过配置项统一控制所有 SQL 查询超时时间，所有入口（MCP 工具、API、批量查询等）均严格受控</li><li>解决连接池 at_eof 异常连接问题：全新自愈式连接池架构，99.9% 消除连接异常，生产环境稳定性大幅提升。</li><li>新增支持 8 项数据治理与分析工具：数据质量分析、血缘追踪、数据新鲜度监控、访问模式分析、依赖分析、慢查询分析、资源增长分析等一站式能力；</li><li>新增支持 ADBC 数据传输协议：基于 Arrow Flight SQL 协议带来 3-10 倍的查询加速。</li><li>全面升级日志系统：企业级分级日志、自动清理、审计追踪，运维无忧。</li><li>调参文档改版优化：当前所有参数支持环境变量，文档更清晰，配置更灵活。</li></ul><p>本次升级完全兼容 Doris MCP Server 0.4.x 版本，可参考文档步骤平滑迁移。详情请见项目文档。</p>]]></description></item><item>    <title><![CDATA[用了半年Cursor，我为什么退了会员？]]></title>    <link>https://segmentfault.com/a/1190000047412823</link>    <guid>https://segmentfault.com/a/1190000047412823</guid>    <pubDate>2025-11-19 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>开头</h2><p>我做了件让社群炸锅的事——</p><p>把用了半年的Cursor会员，退了。</p><p>20美元一个月那个。</p><p>学员们私信我：「老师你不搭产品了？」</p><p>我说：「搭啊，而且速度更快了。」</p><p>「那你咋实现的？你又不会写代码。」</p><p>「一个3块钱的工具。」</p><p>「？？？」</p><p>他们以为我在开玩笑。直到我在社群里演示了一遍——</p><p>用3块钱的工具，20分钟搭了个落地页。</p><p>这才发现，不会写代码的产品经理，也能“轻松”玩转AI编程工具（毕竟 cursor 虽然能玩，但绝不算轻松😭）。</p><p>关键是，比Cursor还好用。</p><h2>Cursor哪里惹到我了？</h2><p>先说清楚，Cursor本身没啥大毛病。</p><p>半年多用下来，体验还行。代码补全准，理解上下文也到位，确实能提效。</p><p>但三个问题，越用越难受。</p><p><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdm6oL" alt="CleanShot 2025-11-19 at 17.00.44.png" title="CleanShot 2025-11-19 at 17.00.44.png"/></p><p><strong>第一个：贵</strong></p><p>20美元。</p><p>一个月。</p><p>我不是全职写代码的。一个月可能就写两三个项目。算下来，每个项目要分摊7-10美元。</p><p>我算了笔账。一年下来，240美元。够我喝多少杯咖啡了？</p><p>对我来说，不划算。</p><p><strong>第二个：限速破防了</strong></p><p>Pro会员，每月500次快速响应。</p><p>听起来挺多？不够用。</p><p>写个中等规模的功能，调个几十次很正常。项目一多，很快见底。</p><p>超了咋办？等。或者加钱。</p><p>这感觉就像，你办了健身卡，结果还得排队。</p><p><strong>第三个：国内体验真的差</strong></p><p>这个最要命。</p><p>很多人说，国内用Cursor经常抽风。有时候慢得要死，有时候直接连不上。</p><p>我也遇到过好几次。正写着代码呢，突然卡住。等半天，没反应。</p><p>工作的时候最怕这种。你不知道今天能不能顺利干活。这种不确定性，真的很烦。</p><p>有次我赶项目，连续三天晚上Cursor都抽风。我直接破防了。</p><p><strong>你用Cursor遇到过啥坑？评论区聊聊</strong> 👇</p><p><img width="605" height="472" referrerpolicy="no-referrer" src="/img/bVdm6oM" alt="图片" title="图片" loading="lazy"/></p><h2>TRAE SOLO到底是啥？</h2><p>字节搞的AI编程工具。SOLO模式去年7月上线。</p><p>定位和传统工具不太一样。</p><p><strong>核心区别在哪？</strong></p><p>Cursor是助手。你写代码，它辅助你。</p><p>TRAE SOLO是工程师。它能理解整个项目，独立干活。</p><p>差别大吗？</p><p>很大。</p><p><strong>用Cursor</strong>：</p><ul><li>你写代码，AI补全</li><li>遇到问题，你告诉它咋改</li><li>需要你盯着每一步</li></ul><p><strong>用TRAE SOLO</strong>：</p><ul><li>你提需求，它独立完成</li><li>自己理解项目结构</li><li>自己生成和集成代码</li><li>自己测试和修bug</li></ul><p>区别看出来了吧？</p><p>Cursor需要你参与每一步。SOLO更像个能独立干活的工程师。</p><p><img width="723" height="457" referrerpolicy="no-referrer" src="/img/bVdm6oN" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>两个智能体</strong></p><p>TRAE SOLO提供了两个：</p><ol><li><strong>SOLO Coder</strong>：懂技术的人用</li></ol><ul><li>处理复杂项目</li><li>理解完整代码库</li><li>多文件同步改</li></ul><ol start="3"><li><strong>SOLO Builder</strong>：小白也能用</li></ol><ul><li>快速搭应用</li><li>可视化界面</li><li>门槛低</li></ul><p>我主要用SOLO Coder。它的理解能力确实行。</p><p>不是理解当前这个文件。是理解整个项目。用了啥框架，模块咋关联的，代码风格啥样的。</p><p>这种理解能力，让它写出来的代码能无缝融入你的项目。不是那种一看就很AI的代码。</p><p><strong>关键优势：对国内用户友好</strong></p><p>这点很重要。</p><p>TRAE是国内团队开发的。网络环境对国内用户更友好。不用担心突然连不上，或者模型抽风。</p><p>实际体验，响应速度和稳定性确实比Cursor好。</p><p>基本上每次提问，1-2秒就回了。不像Cursor，有时候要等5-10秒，甚至更久。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6oO" alt="图片" title="图片" loading="lazy"/></p><h2>价格对比（重点）</h2><p>最直观的部分来了。</p><p>项目</p><p>Cursor</p><p>TRAE SOLO</p><p><strong>首月价格</strong></p><p>$20</p><p>$3</p><p><strong>后续月费</strong></p><p>$20/月</p><p>$10/月</p><p><strong>年度成本</strong></p><p>$240</p><p>$113</p><p><strong>快速响应限制</strong></p><p>500次/月</p><p>600 次/月</p><p><strong>网络稳定性</strong></p><p>国内用户可能不稳定</p><p>相对稳定</p><p><strong>中文支持</strong></p><p>一般</p><p>原生支持</p><p>我算了笔账。</p><p><strong>第一年总成本</strong>：</p><ul><li>Cursor：$240</li><li>TRAE SOLO：$3(首月)+$10×11=$113</li></ul><p>省了127美元。约900人民币。</p><p>够干啥的？够我喝一整年的奶茶了。</p><p>而且TRAE的10美元/月，Pro会员600 次（现在还有另赠送的次数）。</p><p>你算算看，值不值？评论区扣个数 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oP" alt="战术层-隐性陷阱卡片" title="战术层-隐性陷阱卡片" loading="lazy"/></p><h2>限时活动：手快有手慢无</h2><p>重点来了。</p><p>TRAE现在有个活动——<strong>Pro会员邀请计划</strong>。</p><p>机制是这样：</p><ol><li>你成为Pro会员（首月3美元）</li><li>你邀请朋友用你的链接升级Pro</li><li>朋友升级成功后，你俩在<strong>活动期内</strong>都能无限用SOLO</li><li>前提是，你得保持Pro会员身份</li></ol><p><strong>注意两点</strong></p><p><strong>第一：限时的</strong></p><p>官方页面写着"This program runs until --"。有结束时间，具体日期没公开。</p><p>应该是TRAE为了推广SOLO搞的早期活动。</p><p>现在可能是好时机。错过了，可能就没了。</p><p><strong>第二：不是白嫖</strong></p><p>你需要保持Pro会员（每月10美元）才能持续用。</p><p>但用邀请机制，能在活动期内无限用SOLO。比正常订阅划算。</p><p><strong>真实价值在哪？</strong></p><p>正常来说：</p><ul><li>TRAE Pro会员：首月$3，后续$10/月</li><li>用邀请活动：活动期内无限用SOLO</li></ul><p>对比Cursor：</p><ul><li>费用更低</li><li>比500次/月多</li></ul><p><strong>稀缺性：需要邀请码</strong></p><p>这个活动的门槛是——你需要有邀请链接。</p><p>不是所有人都能直接参与。需要有Pro会员的邀请。</p><p>我手上有邀请链接。想试试的话，评论区扣1。</p><p>但我也不知道活动会持续多久。官方随时可能调整。</p><h2>实际体验：真的好用吗？</h2><p>光说不练假把式。</p><p>我用TRAE SOLO做了几个项目。分享下真实感受。</p><p><img width="723" height="300" referrerpolicy="no-referrer" src="/img/bVdm6oQ" alt="CleanShot 2025-11-19 at 16.59.38.png" title="CleanShot 2025-11-19 at 16.59.38.png" loading="lazy"/></p><p><strong>别人咋说的</strong></p><p>有用户分享了用TRAE SOLO 3小时搭女装电商网站的经历。</p><p>爱范儿的测评也提到：“一张草图变网页，实测字节 TRAE SOLO，这些功能甚至比 Cursor 还好用”。</p><p>这些都是真实案例和第三方测评。</p><p><strong>我的真实感受</strong></p><p>用了一段时间，我的感受：</p><p>✅ <strong>响应速度</strong>：比Cursor快，基本秒回✅ <strong>理解能力</strong>：能理解项目结构，改得比较准✅ <strong>稳定性</strong>：网络连接稳定，没遇到过用不了的情况✅ <strong>性价比</strong>：10美元/月，比Cursor便宜一半</p><p>⚠️ <strong>还不够完美的地方</strong>：</p><ul><li>生态还在建设，插件不如Cursor丰富</li><li>有时候对需求理解会有偏差，需要多解释</li><li>文档还在完善，有些功能不太好找</li></ul><p>但考虑到价格和稳定性，这些小瑕疵完全能接受。</p><h2>AI工具这事儿，其实是生态之争</h2><p>很多人问我：模型哪个强？</p><p>我的回答可能让你意外。</p><p><strong>模型不是最重要的。</strong></p><p>生态才是。</p><p>为啥？</p><p><strong>第一：模型会变</strong></p><p>今天GPT-4领先，明天Claude可能反超。大模型迭代太快了，谁都不敢说自己永远第一。</p><p><strong>第二：生态是长期的</strong></p><p>你在一个生态里积累的数据、习惯的工作流，迁移成本很高。</p><p>就像你用惯了iPhone，换安卓会很不适应。不是安卓不好，而是你的数据、习惯都在苹果生态里。</p><p><strong>第三：体验差距来自生态</strong></p><p>同样的模型，接入了你的工作场景，体验完全不一样。</p><p>这就是为啥有人觉得某个AI特别好用，而你用起来感觉一般。因为它没和你的工作流打通。</p><p><strong>对我们意味着啥？</strong></p><p>选AI工具，本质上是选生态和阵营。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oR" alt="图片" title="图片" loading="lazy"/></p><p>你得想清楚：</p><ul><li>你日常主要用啥平台？微信？飞书？还是Slack、Notion？</li><li>你的数据积累在哪？</li><li>你希望AI能连接哪些场景？</li></ul><p><strong>举个实际例子</strong></p><p>我有个朋友，做新媒体的，主要在微信生态工作。那腾讯系的AI工具可能更适合他。</p><p>我是程序员，代码库都在GitHub。那Cursor、TRAE这种能深度理解代码仓库的工具更合适。</p><p>如果你是跨境从业者，主要用国外工具，那ChatGPT生态可能是首选。</p><p>没有绝对的最好。只有适不适合。</p><p>而且，我建议大家<strong>不要把鸡蛋放一个篮子里</strong>。</p><p>今天Cursor封了TRAE的API，明天可能还会有其他变化。</p><p>但如果你选的工具，本身就在你常用的生态里，至少不会说停就停。</p><p><strong>TRAE的生态优势</strong></p><p>我选它，除了价格和体验，还有个重要原因：它背后是字节的生态。</p><p>字节在国内的工具矩阵——飞书、Coze、剪映、抖音……覆盖了办公、创作、传播的全链路。</p><p>TRAE未来可能会和这些工具打通。</p><p>比如：</p><ul><li>用TRAE写的代码，直接部署到飞书小程序</li><li>用TRAE生成的内容，直接导入剪映做视频</li><li>用TRAE分析的数据，直接同步到飞书文档</li></ul><p>这些不是幻想。是有生态基础的。</p><p>这就是生态的力量。</p><p><img width="720" height="306" referrerpolicy="no-referrer" src="/img/bVdm6oS" alt="图片" title="图片" loading="lazy"/></p><p><strong>你在啥生态里工作？评论区聊聊</strong> 👇</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6oT" alt="未来层-生态想象卡片" title="未来层-生态想象卡片" loading="lazy"/></p><h2>啥人适合？啥人别碰？</h2><p>TRAE SOLO不是万能的。</p><p><strong>我有个朋友系列</strong></p><p>我有个朋友，做独立开发的。每个月预算就500块。20刀的Cursor对他来说确实有点贵。</p><p>他试了TRAE之后，直接把Cursor退了。首月3美元，后续10美元/月。对他来说，刚刚好。</p><p>还有个做外包的哥们，全职写代码。一天要调用AI几百次。Cursor的500次限制对他来说根本不够用。</p><p>换了TRAE之后，无限制调用。再也不用担心次数用完了。</p><p>但我另一个朋友，在大厂做架构师。他们项目很复杂，需要很多企业级功能。TRAE的生态还在建设，暂时不太够。</p><p>他还是在用Cursor。</p><p>还有个朋友，重度Cursor用户。装了一堆插件，工作流已经深度绑定了。迁移成本太高。</p><p>他也没换。</p><p><strong>你是哪种情况？评论区说说</strong> 👇</p><h2>我的选择</h2><p>三个月过去了。</p><p>Cursor的会员自动续费，我没开。</p><p>TRAE用得挺顺手。</p><p>为啥？</p><ol><li><strong>价格</strong>：省了一半的钱</li><li><strong>稳定性</strong>：网络更稳定</li><li><strong>体验</strong>：响应快，理解准</li><li><strong>活动</strong>：限时福利，性价比高</li></ol><p>我不确定3美元的首月价格会持续多久。限时活动也不知道啥时候会结束。</p><p>但至少现在，TRAE SOLO对我来说是性价比很高的选择。</p><p>省下的钱，够我喝一个月奶茶了。</p><p><strong>想试试的话</strong>：</p><ol><li>评论区扣1，我发邀请链接</li><li>首月3美元，可以体验一个月再决定</li><li>觉得不合适可以随时取消</li></ol><p>用我的邀请链接注册，咱俩在活动期内都能无限用SOLO。</p><p>先到先得。</p><p>有问题评论区见！🚀</p><hr/><p><strong>补充说明</strong>：</p><p>这篇文章基于我的真实使用体验，没收广告费（新号大厂也看不上😭）。推荐TRAE是因为它确实解决了我的痛点，价格也合理。</p><p>如果你有不同的使用场景或需求，欢迎在评论区讨论。选工具这事，没有绝对的好坏，只有适不适合。</p>]]></description></item><item>    <title><![CDATA[如何安装 MousePlus_v5_4_]]></title>    <link>https://segmentfault.com/a/1190000047412595</link>    <guid>https://segmentfault.com/a/1190000047412595</guid>    <pubDate>2025-11-19 21:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p>一、准备工作</p><ol><li><strong>下载好安装包</strong>：<strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=fFHJkUie2jlN%2B4CkhBpphg%3D%3D.RpT7wna%2FSVtDLSqj6ELgy9MHrCwDrYFW74WmKmOMcq2B0eKGMbHsfKgEx9APJjo6" rel="nofollow" title="https://pan.quark.cn/s/cdd7206bc5a8" target="_blank">https://pan.quark.cn/s/cdd7206bc5a8</a>，下载了 <strong>MousePlus_v5_4_13.exe</strong>这个文件，一般是个exe格式的程序，双击就能运行。</li></ol><h3>二、开始安装</h3><ol><li><strong>找到安装文件</strong>：打开你下载文件的那个文件夹（比如“下载”文件夹），找到 <strong>MousePlus_v5_4_13.exe</strong>，双击它。</li><li><strong>可能会弹出安全提示</strong>：Windows 可能会问你“是否要运行这个程序”，点  <strong>“是”</strong> 或者  <strong>“允许”</strong> 就行。</li><li><strong>安装向导启动</strong>：稍等一下，会跳出一个安装界面，一般有“下一步”“安装”“完成”这些按钮。</li><li><p><strong>选择安装位置（可选）</strong> ：</p><ul><li>有的版本会让你选安装到哪个文件夹，默认一般是在 C 盘某个地方（比如 Program Files）。</li><li>如果你想改地方，可以点“浏览”自己选个盘或文件夹；不想改就直接点  <strong>“下一步”</strong> 。</li></ul></li><li><strong>开始安装</strong>：看到“安装”按钮后，点它，程序就会自动复制文件到你的电脑里，这个过程可能几秒钟到十几秒，耐心等等。</li><li><strong>安装完成</strong>：等它搞完后，会跳出提示说“安装成功”或者“安装完成”，点  <strong>“完成”</strong> 或者  <strong>“关闭”</strong> 就行。</li></ol><p>​</p>]]></description></item><item>    <title><![CDATA[JSONBench 榜单排名第一！ 10]]></title>    <link>https://segmentfault.com/a/1190000047412610</link>    <guid>https://segmentfault.com/a/1190000047412610</guid>    <pubDate>2025-11-19 21:05:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>坦白讲，每次看性能测试排行榜，我都会下意识地先找找 Apache Doris 在哪个位置。</p><p>这次打开 JSONBench 的榜单，心情一如既往的期待加紧张。</p><p>好在结果让我松了一口气：默认配置下就能排到第三，仅次于维护方 ClickHouse 的两个版本。</p><p>不过，Doris 只能止步于此了吗？经过一系列优化后，查询时长能不能再缩短点？和 ClickHouse 的差距在哪里？</p><p>调优前后对比图镇楼，至于调优的具体思路，请一起往下看吧。</p><ul><li><p>Apache Doris 排名 (Default)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412612" alt="Apache Doris 排名 (Default)" title="Apache Doris 排名 (Default)"/></p></li><li><p>Apache Doris 排名 (Unofficial Tuned)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412613" alt="Apache Doris 排名 (Unofficial Tuned)" title="Apache Doris 排名 (Unofficial Tuned)" loading="lazy"/></p></li></ul><h2>JSONBench 简介</h2><p>JSONBench 是一个为 JSON 数据而生的数据分析 Benchmark，简单来说，它由 10 亿条来自真实生产环境的 JSON  数据、5 个针对 JSON  构造的特定 SQL 查询组成，旨在对比各个数据库系统对半结构化数据的处理能力。目前榜单包括 ClickHouse、MongoDB、Elasticsearch、DuckDB、PostgreSQL 等知名数据库系统，截至目前，<strong>Doris 的性能表现是 Elasticsearch 的 2 倍，是 PostgreSQL 的 80 倍</strong>。</p><blockquote><em>JSONBench 官网地址：jsonbench.com</em></blockquote><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412614" alt="JSONBench 简介.png" title="JSONBench 简介.png" loading="lazy"/></p><p>不仅在性能上 Apache Doris 领先其他同类产品，<strong>在数据集相同的情况下，Apache Doris 的存储占用是 Elasticsearch 的 1/2、PostgreSQL 的 1/3</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412615" alt="JSONBench 简介-2.png" title="JSONBench 简介-2.png" loading="lazy"/></p><p>JSONBench 测试具体流程：首先在数据库中创建一张名为 Bluesky 的表，并导入十亿条真实的用户行为日志数据。测试过程中，每个查询重复执行三次，并且在每次查询前清空操作系统的 Page Cache，以模拟冷热查询的不同场景。最终，通过综合计算各查询的执行耗时得出数据库的性能排名。</p><p>在这个测试中，Apache Doris 使用了 Variant 数据类型来存储 JSON 数据，默认的建表 Schema 如下：</p><pre><code class="SQL">CREATE TABLE bluesky (
    `id` BIGINT NOT NULL AUTO_INCREMENT,
    `data` variant NOT NULL
)
DISTRIBUTED BY HASH(id) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>Variant 是 Apache Doris 2.1 中引入一种新的数据类型 ，它可以存储半结构化 JSON 数据，并且允许存储包含不同数据类型（如整数、字符串、布尔值等）的复杂数据结构，而无需在表结构中提前定义具体的列。Variant 类型特别适用于处理复杂的嵌套结构，而这些结构可能随时会发生变化。在写入过程中，该类型可以自动根据列的结构、类型推断列信息，动态合并写入的 schema，并通过将 JSON 键及其对应的值存储为列和动态子列。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=MduoGda%2BRx53pszxTiDWcA%3D%3D.JO%2BNfiZCht94rcI3SW7rZpojGi0IIBzjzLmLH6qP0ndoSt5HegZfrW%2BnRrL4EQpiC5l7TPs0txGFtl6bUtkxeoFSMiG674ypePxMTBDLfpsIBD3hfNtd9cE8XGg0bPRuHvz3asTH3p50iRQIn7RzEg%3D%3D" rel="nofollow" target="_blank">Apache Doris Variant 类型详情</a></li></ul></blockquote><h2>调优思路与原理</h2><p>JSONBench 榜单排名依据各个数据库系统在默认配置下的性能数据，那么能否通过调优，让 Apache Doris 进一步释放性能潜力，实现更好的性能效果呢？</p><h3>01 环境说明</h3><ul><li>测试机器：AWS M6i.8xlarge(32C128G);</li><li>操作系统：Ubuntu24.04;</li><li>Apache Doris: 3.0.5;</li></ul><h3>02 Schema 结构化处理</h3><p>由于 JSONBench 特定查询中涉及到的 JSON 数据都是固定的提取路径，换言之，半结构化数据的 Schema 是固定的，因此，我们可以借助生成列，将常用的字段提取出来，实现半结构化数据和结构化数据结合的效果。类似的高频访问的 JSON 路径或者需要计算的表达式，都可以使用该优化思路，添加对应的生成列来实现查询加速。</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=fNz6ImJBc0u%2BEK70VNGU%2FA%3D%3D.T2QZhn2HY%2FTZcnJ95aj%2F4uQKtR1R4V7vXlwLgtfDjK2hL2DEPFmdiWUjg1VsEg%2F0o8eb3CghQW%2BTRN6vhDIShoixmYyZdW1V%2FXLccXpnlEk%3D" rel="nofollow" target="_blank">查看 JSONBench 查询</a></li><li><a href="https://link.segmentfault.com/?enc=hsmdTUvp0tMkCRbher4hgg%3D%3D.1D7tjNihUSmJhAs70ug0lJzOXgwdKbGk5uwLdq2IRsutXXq3vgevO4Hxto9YJK7czjWR78dR3Yk7ldy8er66UZ0%2FiQjKYnc6PsZ0rxohlVnBWWkeSQ5C8sao1vZIBNTfZiI3NuSj464zAawaY%2BAh4w%3D%3D" rel="nofollow" target="_blank">Apache Doris 生成列详情</a></li></ul></blockquote><pre><code class="SQL">CREATE TABLE bluesky (
    kind VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.kind')) NOT NULL,
    operation VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.operation')) NULL,
    collection VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data, '$.commit.collection')) NULL,
    did VARCHAR(100) GENERATED ALWAYS AS (get_json_string(data,'$.did')) NOT NULL,
    time DATETIME GENERATED ALWAYS AS (from_microsecond(get_json_bigint(data, '$.time_us'))) NOT NULL,
    `data` variant NOT NULL
)
DUPLICATE KEY (kind, operation, collection)
DISTRIBUTED BY HASH(collection, did) BUCKETS 32
PROPERTIES ("replication_num"="1");</code></pre><p>除了可以减少查询时提取数据的开销，还可以用展平出来的列作为分区列，使得数据分布更均衡。</p><p>需要注意的是，查询的 SQL 语句也要改为使用展平列的版本：</p><pre><code class="SQL">// JSONBench 原始查询：
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, COUNT(*) AS count, COUNT(DISTINCT cast(data['did'] AS TEXT )) AS users FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' GROUP BY event ORDER BY count DESC;
SELECT cast(data['commit']['collection'] AS TEXT ) AS event, HOUR(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT cast(data['did'] AS TEXT ) AS user_id, MIN(from_microsecond(CAST(data['time_us'] AS BIGINT))) AS first_post_ts FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT cast(data['did'] AS TEXT ) AS user_id, MILLISECONDS_DIFF(MAX(from_microsecond(CAST(data['time_us'] AS BIGINT))),MIN(from_microsecond(CAST(data['time_us'] AS BIGINT)))) AS activity_span FROM bluesky WHERE cast(data['kind'] AS TEXT ) = 'commit' AND cast(data['commit']['operation'] AS TEXT ) = 'create' AND cast(data['commit']['collection'] AS TEXT ) = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;

// 使用展平列改写的查询：
SELECT collection AS event, COUNT(*) AS count FROM bluesky GROUP BY event ORDER BY count DESC;
SELECT collection AS event, COUNT(*) AS count, COUNT(DISTINCT did) AS users FROM bluesky WHERE kind = 'commit' AND operation = 'create' GROUP BY event ORDER BY count DESC;
SELECT collection AS event, HOUR(time) AS hour_of_day, COUNT(*) AS count FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection IN ('app.bsky.feed.post', 'app.bsky.feed.repost', 'app.bsky.feed.like') GROUP BY event, hour_of_day ORDER BY hour_of_day, event;
SELECT did AS user_id, MIN(time) AS first_post_ts FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY first_post_ts ASC LIMIT 3;
SELECT did AS user_id, MILLISECONDS_DIFF(MAX(time),MIN(time)) AS activity_span FROM bluesky WHERE kind = 'commit' AND operation = 'create' AND collection = 'app.bsky.feed.post' GROUP BY user_id ORDER BY activity_span DESC LIMIT 3;</code></pre><h3>03 Page Cache 调整</h3><p>调整查询语句后，开启 profile，执行完整的查询测试：</p><pre><code class="Plain">set enable_profile=true;</code></pre><p>进入 FE 8030 端口的 Web 页面，找到相关 profile 进行分析，此时发现 SCAN Operator 中的 Page Cache 命中率较低，导致热读测试过程中存在一部分冷读操作。</p><pre><code class="SQL">-  CachedPagesNum:  1.258K  (1258)
-  TotalPagesNum:  7.422K  (7422)</code></pre><p>这种情况通常是由于 Page Cache 容量不足，无法完整缓存 Bluesky 表中的数据。建议在 <code>be.conf</code> 中添加配置项 <code>storage_page_cache_limit=60%</code>，将 Page Cache 的大小从默认的内存总量的 20% 提升至 60%。重新运行测试后，可以观察到冷读问题已得到解决。</p><pre><code class="SQL">-  CachedPagesNum:  7.316K  (7316)
-  TotalPagesNum:  7.316K  (7316)</code></pre><h3>04 最大化并行度</h3><p>为了进一步挖掘 Doris 的性能潜力，可以将 Session 变量中的<code>parallel_pipeline_task_num</code>设为 32，因为本次 Benchmark 测试机器<code>m6i.8xlarge</code>为 32 核，所以我们将并行度设置为 32 以最大程度发挥 CPU 的计算能力。</p><pre><code class="SQL">// 单个 Fragment 的并行度
set global parallel_pipeline_task_num=32;</code></pre><h2>调优结果</h2><p>经过上述对 Schema、Query、内存限制、CPU 等参数的调整，我们对比了调优前后 Doris 的性能表现以及一些其他数据库系统的成绩，有如下结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412616" alt="调优结果.png" title="调优结果.png" loading="lazy"/></p><p>可以看到，<strong>对比调优前的 Doris，调优后 Doris 查询整体耗时降低了 74%，对比原榜单第一的 ClickHouse 产品实现了 39% 的领先优势</strong>。</p><h2>总结与展望</h2><p>通过对 Schema 的结构化处理、查询语句的优化、缓存配置的调整以及并行参数的设置，Apache Doris 整体查询耗时显著下降，并超越 ClickHouse。</p><p>在默认设置下，Doris 在 10 亿条 JSON 的查询耗时与 ClickHouse 仍有数秒的差异。然而，依托于 Doris 在 JSON 处理、Variant 类型支持及生成列等能力的加持，经调优后，其半结构化数据处理性能获得了进一步显著提升，并在同类数据库中表现出明显的领先优势。</p><p>未来，Apache Doris 将继续打磨在半结构化领域的数据处理能力，为用户带来更加优质、高效的分析体验，包括：</p><ul><li>优化 Variant 类型稀疏列的存储空间，支持万列以上的子列；</li><li>优化万列大宽表的内存占用；</li><li>支持 Variant 子列根据列名的 Pattern 自定义类型、索引等。</li></ul><h2>推荐阅读</h2><ul><li><a href="https://link.segmentfault.com/?enc=EiltnwORoL2PXtlZeXWj%2Fw%3D%3D.JIBRSS9NUFAUjOPZwmbpG2CLXEqDG2v1sFzlSjwHRtYoMMea6HUBiFAEkCd80QN%2B" rel="nofollow" target="_blank">Apache Doris 针对半结构化数据分析的解决方案及典型场景</a></li><li><a href="https://link.segmentfault.com/?enc=CXJHuLEc9QuwQo3UAeJBKA%3D%3D.J79R97kkQGGm6PHEzs1C7XL1dXg706GnrFcnZew04kh92PlJb%2BINZUdOnuj1pI3E" rel="nofollow" target="_blank">揭秘 Variant 数据类型：灵活应对半结构化数据，JSON 查询提速超 8 倍，存储空间节省 65%</a></li></ul>]]></description></item><item>    <title><![CDATA[Apache Doris 实时更新技术揭]]></title>    <link>https://segmentfault.com/a/1190000047412623</link>    <guid>https://segmentfault.com/a/1190000047412623</guid>    <pubDate>2025-11-19 21:04:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>数据，是有保质期的。正如冰箱里的牛奶，今天新鲜，明天可能就有点酸，后天直接倒掉。数据的价值，也会随着时间的推移而递减。</p><p>过去那些“老派”的 OLAP 系统，只能批量处理账目，对实时性要求高的“流水账”就力不从心了。它们在面对高并发实时写入和复杂的分析查询时，常常会露出疲态，数据延迟、查询性能、并发处理和数据更新等问题层出不穷。</p><p>当所有人都焦虑于如何让数据“快”起来的时候，Apache Doris 在底层逻辑上进行了一系列颠覆性的技术迭代，能快速接入各种数据源，并且拥有强大的实时更新能力，让你的数据从产生的那一刻起，就具备了“快”的生命力。</p><p>到底 Doris 是怎么做到，让数据流动得如此低延迟？这正是我们接下来要深挖的“冰山之下”。</p><h2>实时更新的挑战</h2><p>在实现实时更新的过程中，系统需要应对多个方面的挑战，这些挑战直接关系到系统在实际业务场景中的稳定性与性能表现：</p><ul><li><strong>数据延迟</strong>：实时更新的核心在于“快”。数据从产生到可查询的过程必须尽可能短，实际生产中要求在 5-10 秒可见，理想情况下甚至要求低于 1 秒可见。同时，还需具备足够的写入吞吐能力，保证在并发写入场景下也能稳定运行。</li><li><strong>查询性能</strong>：一边持续高频地接收数据更新，一边还能保持百毫秒级别的查询响应，对底层系统架构提出了极高要求。如何在更新密集的情况下，仍然提供快速、稳定的查询体验，是实时 OLAP 系统必须解决的问题。</li><li><strong>并发处理</strong>：实时分析场景多面向终端用户，不仅查询操作需要支持高并发，同时写入也常常是并发的。传统的表或者分区级别的写冲突处理机制影响范围较大，会影响数据写入效率与业务体验。理想状态下，系统应允许用户制定冲突处理策略，从而提高数据接入的灵活性与可控性。</li><li><strong>数据流维护与易用性保障</strong>：实时数据流的维护受多项复杂因素的影响，例如 TP 系统通过 CDC 捕获的删除操作以及 Schema 变更带来的下游兼容性问题。同时，在链路重启或容灾恢复时，如何确保数据既不重复和不丢失，这对数据一致性的要求非常高。</li></ul><p>这些挑战正是评估一个 OLAP 系统是否真正具备实时更新能力的关键指标。</p><h2>常见方案对比</h2><p>在面临数据更新的上述挑战时，市面上的常用方案通常涉及三个关键点，分别是表达方式、更新实现和冲突解决，这些方案各有其适用场景。</p><ol><li><strong>在表达方式上，</strong> Snowflake、Redshift、Iceberg、Databricks 和 Hudi 通常使用 MERGE INTO 来处理数据更新，这要求变更数据必须先落盘成为 MERGE 的数据源，因此可能带来一定的数据延迟和额外的 I/O 开销。相比之下，Doris 和 ClickHouse 采用更加轻量的方式，通过特定列值表示删除操作，使得写入和删除可以统一在同一数据流中处理，更加契合实时处理需求，尤其适用于 OLTP 类事务变更、订单或账单状态更新等场景。</li><li><p><strong>在更新实现方面，</strong> 业界常见的方案有四种：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412625" alt="常见方案对比" title="常见方案对比"/></p></li></ol><pre><code>- **Copy on Write**：在写入时找出需要更新的文件，读取并结合新的更新，生成新文件再写入。这种方式优化了读取性能，但在写入时会显著增加 I/O 开销，特别是在随机更新情况下，会引发大量读写 I/O，限制了实时更新能力。此方案的典型产品包括 Redshift、ClickHouse、Snowflake、Iceberg 和 Hudi。
- **Merge on Read**：在写入时仅需添加新数据，读取时再合并新旧数据，类似于 LSM Tree。这种方式优化了写入性能，但查询效率较低，难以满足某些实时场景的查询延迟要求，典型产品包括 Iceberg、Hudi 和 Doris Merge on Read Unique 表。
- **Delete bitmap / deletion vector：** 标记删除的实现方案。在写入时标记文件中被删除的数据，并写入删除标记及新数据，查询时跳过删除标记的数据行。此方式既能避免 Copy on Write 的 I/O 放大效应，也获得了 Copy on Write 的查询性能。但是对于没有主键索引的实现，生成删除标记（Delete Bitmap / Delete Vector）时 I/O 和 CPU 消耗较大，效率低下，难以满足高频实时写入场景。
- **Delete bitmap / deletion vector + primary index：** 标记删除与主键索引结合的方式。主键索引能够降低标记删除时的查询 I/O 和 CPU 消耗，使高频实时更新成为可能。Doris 的 Merge on Write Unique 表采用了这种实现方式。
</code></pre><ol start="3"><li><strong>在冲突解决方面，</strong> 经典的写写冲突会导致写入无法并行，从而显著降低写入吞吐量。Doris 提供了基于业务语义的冲突机制，可很好避免该问题（<a href="https://link.segmentfault.com/?enc=u6X8arbQ94eZ2%2F05cPodGA%3D%3D.xiDTOWgzxOf9kWyBCFNNKYw9RMjwg9PmQbjySXslw13YY0XFa3i4G6J3k1vn2pGSmg2kjE4mG0sZvcFV1CXcf9ri2fRP7aNIy%2BlBUjBtS0oqedV22gA7xp4IcrKuKiA6" rel="nofollow" target="_blank">参考文档</a>）。而 Redshift、Snowflake、Iceberg 和 Hudi 等则采用了文件级别的冲突处理，因而不具备实时更新的能力。</li></ol><p>Apache Doris 作为一款为实时分析场景打造的高性能 MPP 分析型数据库，具备强大的数据写入能力、亚秒级查询性能以及出色的并发处理能力，因此成为构建面向用户的实时数据服务的优选方案。基于上述常见方案，本文将详细拆解 Apache Doris 实时更新技术的核心设计，揭示其如何实现“极低延迟”的数据流动性。</p><h2>为什么 Apache Doris 实时更新更具优势？</h2><p>传统 OLAP 数据库主要用于批量分析，数据更新周期通常以小时甚至天为单位，适合以报表为主的内部系统。然而，<strong>随着业务及数据服务的多样化发展，越来越多的分析应用开始面向终端用户，要求亚秒级的查询延迟和秒级的数据更新。</strong> 在此背景下，要求实时分析数据库能够应对高速写入数据（每秒百万级别的数据导入）、并在大规模场景下提供实时查询。Apache Doris 凭借其主键模型、数据延迟、查询性能、并发处理、易用性等多方面特性的表现，在分析领域展现了独特的实时更新能力。</p><h3>01 主键模型</h3><p>Doris 提供了主键表，确保数据主键的唯一性，支持基于主键的 upsert 语义。以下是一个以 <code>user_id</code> 主键的表的创建示例：</p><pre><code class="Plain">CREATE TABLE IF NOT EXISTS example_tbl_unique
(
    user_id         LARGEINT        NOT NULL,
    user_name       VARCHAR(50)     NOT NULL,
    city            VARCHAR(20),
    age             SMALLINT,
    sex             TINYINT
)
UNIQUE KEY(user_id)
DISTRIBUTED BY HASH(user_id) BUCKETS 10;</code></pre><p>在这个表中，初始数据包括四行： 101、102、103、104 。当新写入 101、102 之后，表中的数据仍然保持是四行，但原先 101 和 102 的数据会被更新。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412626" alt="主键模型.jpeg" title="主键模型.jpeg" loading="lazy"/></p><p>参考文档：<a href="https://link.segmentfault.com/?enc=%2FvDcyV%2FhJDqPBCICIlwK8A%3D%3D.aX6pYvy8MSS7hgrtFs9cMo5jSBJeZAedkHH8vt8dbWXSReCDMIFIPvIpNEndVsFEq%2BFZSjzN9l%2BmB3EDX%2F21e%2BIlayyOmw3gO7s9sqpmYsk%3D" rel="nofollow" target="_blank">主键模型的导入更新</a></p><h3>02 数据延迟</h3><p>Doris 提供强一致性语义，确保数据写入后立即可见，从而满足低延迟的实时数据更新需求。数据组织使用 LSM tree 的方式组织，写入操作采用删除标记（Delete Bitmap）方式，相较于传统的 Copy-on-Write 机制，能够显著减少 I/O 操作，提高写入效率。这一设计不仅降低了存储空间的浪费，还减轻了系统的负担，从而提供了更高效的数据处理能力。</p><p>此外，Doris 利用主键索引优化了在更新数据时定位和检索相历史数据的性能，进一步提升了写入速度并降低了资源消耗。<strong>通过这些设计，Doris 实现了综合性的低延迟数据更新，能够提供 1s 以下的数据延迟，满足高效实时分析和快速数据响应的需求。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412627" alt="数据延迟.png" title="数据延迟.png" loading="lazy"/></p><h3>03 查询性能</h3><p>在更新场景下，Doris 采用标记删除（Delete Bitmap）方式加速查询性能。与 Merge-on-Read 的实现相比，标记删除能够避免在查询时进行大量的删除逻辑计算，从而减少查询延迟并提升整体性能，确保查询响应时间低于百毫秒，并支持高并发访问。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412628" alt="查询性能.png" title="查询性能.png" loading="lazy"/></p><p>此外，Doris 基于以下几项技术，进一步提升了查询性能：</p><ul><li><strong>分区和分桶裁剪技术：</strong> 智能跳过无关数据，进一步优化数据扫描过程，减少不必要的数据读取，显著提高查询效率。</li><li><strong>向量化技术：</strong> 在处理大规模数据时，通过批量化处理多个数据操作，减少 CPU 的上下文切换，显著提升数据处理速度，尤其适用于大数据量的查询场景。</li><li><strong>优化器：</strong> 通过智能的查询计划选择和执行，自动根据查询条件调整最佳执行路径，避免不必要的计算开销，进一步提高查询响应速度。</li><li><strong>丰富的索引</strong>：包括点查索引和跳数索引。点查索引常用于加速点查，包括前缀索引和倒排索引，原理是通过索引定位到满足 WHERE 条件的有哪些行，直接读取那些行。跳数索引常用于加速分析，包括 ZoneMap 索引、BloomFilter 索引、NGram BloomFilter 索引，原理是通过索引确定不满足 WHERE 条件的数据块，跳过这些不满足条件的数据块，只读取可能满足条件的数据块并再进行一次逐行过滤，最终得到满足条件的行。</li></ul><p>这些技术的结合使得 Doris 在高并发环境下能够保持稳定的低延迟，确保其在秒级和毫秒级查询性能上表现出色，满足实时数据处理的严格要求。</p><h3>04 并发处理</h3><p>Doris 主键表支持应用语义处理冲突，在高并发乱序写入时能够保证数据的最终一致性。建表时，可以通过指定 SEQUENCE COLUMN 来自定义 MVCC 的冲突处理逻辑，Doris 的写入负载均衡机制优先选择 SEQUENCE 列较大的行。这一机制不仅适用于写入冲突，还同样适用于存量数据。</p><pre><code class="sql">CREATE TABLE test.test_table
(
    user_id bigint,
    date date,
    group_id bigint,
    modify_date date,
    keyword VARCHAR(128)
)
UNIQUE KEY(user_id, date, group_id)
DISTRIBUTED BY HASH (user_id) BUCKETS 32
PROPERTIES(
    "function_column.sequence_col" = 'modify_date',
    "replication_num" = "1",
    "in_memory" = "false"
);</code></pre><p>例如，在 OLTP 表中，<code>modify_date</code> 字段每次更新时都会设置为当前时间。在将 OLTP 数据库的 CDC 同步到 Doris 时，可以将 <code>modify_date</code> 指定为 SEQUENCE 列。这样，具有较大 <code>modify_date</code> 的数据行将生效，而如果后写入的数据 <code>modify_date</code> 较小，则存量数据不会被更新。这一机制使得实时数据同步的冲突处理变得非常简单，同时不影响写入效率。</p><p>参考文档：[主键模型的更新并发控制<br/>](<a href="https://link.segmentfault.com/?enc=P8qQEOdR4UAd8osjUQcvwA%3D%3D.Jnn0wnAH%2FMSyq7xBLd%2FGKXTmQAqlaouHp6%2FiGzUBIsrSWngLYaoL9gL6q5REh%2FaJ3iFbirCv3EOrTfWJtZK%2BWoDwKW5vL%2BJXdt6KT8HMQ0jv9YcPs3ZbdfKHw%2FQt0fWv" rel="nofollow" target="_blank">https://doris.apache.org/zh-CN/docs/data-operate/update/uniqu...</a>)</p><h3>05 易用性</h3><ul><li>首先，Doris 确保每次数据写入的一致性和完整性，保证在高并发和实时更新环境中，数据始终保持一致并立即可见。结合标记删除机制，Doris 使数据更新更加高效，减少了存储开销，并提升了查询性能。</li><li>其次，Doris 还支持在线 Schema 变更，允许动态调整表结构，从而简化数据流的维护，避免复杂的数据迁移过程。同时，灵活的列更新功能使数据更新更为高效，特别是在频繁更新部分数据时，避免了全表更新带来的性能开销。</li><li>最后，Doris 支持隐藏列标记删除方式，即为每个 Unique 表生成隐藏的 <code>DORIS_DELETE_SIGN</code> 列，利用该标志直接进行删除操作，避免了传统的复杂删除步骤，提升了系统性能。同时，Doris 还支持将 SEQUENCE 列与删除标志结合使用，确保过期数据的删除不会影响新数据，简化了实时数据流中的更新与删除操作。</li></ul><p>受益于写入原子性、强一致性语义，以及灵活的在线 Schema 变更和列更新机制等机制，Doris 能够在高并发和实时更新场景中高效处理数据，简化开发工作，并提升系统的响应速度和可靠性。</p><p>参考文档：<a href="https://link.segmentfault.com/?enc=JnQveyiiQbgUNnDHGTvR0w%3D%3D.r%2BsauALD7YjsvLy4afylyFJxjxmtuV70ZctLuLoVvvEOkwBZaLwUGGtiIKaNrGaqjjYKuCmTJa4tjqDQnzvdFxkr3%2Fu9tXQJR6BlIMmoRpQ%3D" rel="nofollow" target="_blank">基于导入的批量删除</a></p><h2>生态融合</h2><p>Doris 提供丰富的 API 和连接器，方便与现有的数据处理工具和框架（如 Spark、Flink、Kafka）进行集成，增强了生态灵活性，使得 Doris 能够为用户提供更加强大的数据处理能力，适应多样化的业务需求和技术环境。</p><h3>01 Kafka</h3><p><a href="https://link.segmentfault.com/?enc=bdaXgm0RhzqkkN6TpJOEMw%3D%3D.SjTh3vuy4mqqOtVCvL0BqREpLkQaoIiNCufkBzmgLs10QWi5mI9fgPcDZr%2BmOXtvcSa7ZMlzZBZqDCy3nhDt4g%3D%3D" rel="nofollow" target="_blank">Kafka Connect</a> 是一款可扩展、可靠的在 Apache Kafka 和其他系统之间进行数据传输的工具，可以定义 Connectors 将大量数据迁入迁出 Kafka，并通过<a href="https://link.segmentfault.com/?enc=lheSLR40F%2Bv9XYv6upWv4w%3D%3D.HJzxFhWlc9J8Xk%2BfCHPPo6s2%2FJaxjppPIbHzUKABOgzngT6NslMfZpnQisSYxCLp%2BfQpCrxOcfqqs0Ni3tB5tc%2BP%2BwN8uAy%2BsFUWwex0Y8Frnra%2BuPO2LlucYoRgC6gn" rel="nofollow" target="_blank"> Doris Kafka Connector </a>将上游 topic 中的数据读取后写入到 Doris 中。</p><p>在 Kafka Connect 集群上新增一个 Doris Sink 的 Connector，示例如下：</p><blockquote>详细步骤参考<a href="https://link.segmentfault.com/?enc=Z2b9zTbcnMaAqOqhXCRmiQ%3D%3D.v50xOwQYBV1eRyoXZ9%2BBm%2BJgCl6z2E11496e9xZ5NqWxKAr3Y2agkO8ldNr52mft2eqyhySJmPizVxkSivDJYdRwtNzCted52nOiuF5hgWc%3D" rel="nofollow" target="_blank">文档</a></blockquote><pre><code class="SQL">curl -i http://127.0.0.1:8083/connectors -H "Content-Type: application/json" -X POST -d '{
  "name":"test-doris-sink-cluster",
  "config":{
    "connector.class":"org.apache.doris.kafka.connector.DorisSinkConnector",
    "topics":"topic_test",
    "doris.topic2table.map": "topic_test:test_kafka_tbl",
    "buffer.count.records":"50000",
    "buffer.flush.time":"120",
    "buffer.size.bytes":"5000000",
    "doris.urls":"10.10.10.1",
    "doris.user":"root",
    "doris.password":"",
    "doris.http.port":"8030",
    "doris.query.port":"9030",
    "doris.database":"test_db",
    "key.converter":"org.apache.kafka.connect.storage.StringConverter",
    "value.converter":"org.apache.kafka.connect.json.JsonConverter"
  }
}'</code></pre><h3>02 Flink</h3><p>Apache Flink 是一个框架和分布式处理引擎，用于在无界和有界数据流上进行有状态的计算。可以使用 <a href="https://link.segmentfault.com/?enc=1iyjgtFsfYtXAhwdKjbbgg%3D%3D.13mDyRM9uc2ZhYZ6y%2FjKhxqR9TL6eRDeELGjQmcf4xOOboGU8vD9I43jx4ihTSzKC2T1J301CxU9KUqL6hiwXzHaH8mzIbN9%2BlowbcxJzIg%3D" rel="nofollow" target="_blank">Flink Doris Connector</a> 将上游的数据，比如 Kafka、MySQL 等产生的数据，实时写入至 Doris。</p><p>使用 Flink 自带的 DataGen 模拟数据写入 Doris 中，示例如下：</p><blockquote>具体步骤参考<a href="https://link.segmentfault.com/?enc=RpLbbvhnjQ7eanH3RrZrsQ%3D%3D.qS%2FFdEOGvgyNw9%2BvKw3sMrkQueCRLAISimx8e5KZWV%2BUbPyigobD%2FlSuuYJYF7J5Nfi5Negezga22nrRYCZVIKpC5Shve2DHdvGgvB1nUDt7XJzoRbTxLqL8lYwtVYalsdMDRsSce21dryzzHau4jII9ObZ9SHTD8g0fnm4CLKYUOXzxoZouTc7fH%2BVSiHnMSq9Zh98XjHi4aPTtF77Rnw%3D%3D" rel="nofollow" target="_blank">文档</a></blockquote><pre><code class="sql">SET 'execution.checkpointing.interval' = '30s';

CREATE TABLE student_source (
    id INT,
    name STRING,
    age INT
) WITH (
  'connector' = 'datagen',
  'rows-per-second' = '1',
  'fields.name.length' = '20',
  'fields.id.min' = '1',
  'fields.id.max' = '100000',
  'fields.age.min' = '3',
  'fields.age.max' = '30'
);

-- doris sink
CREATE TABLE student_sink (
    id INT,
    name STRING,
    age INT
    ) 
    WITH (
      'connector' = 'doris',
      'fenodes' = '127.0.0.1:8030',
      'table.identifier' = 'test.student',
      'username' = 'root',
      'password' = 'password',
      'sink.label-prefix' = 'doris_label'
);

INSERT INTO student_sink SELECT * FROM student_source;</code></pre><h3>03 Spark Structured Streaming</h3><p>Structured Streaming 是一个构建在 Spark SQL 引擎之上的可扩展、容错的流处理引擎。借助 Structured Streaming，可以高效地读取上游数据源，并通过 <a href="https://link.segmentfault.com/?enc=28sIv3JG%2FVVCVXtjgH7kvw%3D%3D.l2YwtfGpHM7XlU%2B42QOcUiLzMQxEV3Gwnmrp%2BnexeJ8LtxKZItXD%2FU0Ow7wCo0bZ6tTAScGol4DiyEzt9DT0tdPY8S8DMIyvIXP3%2For8lIU%3D" rel="nofollow" target="_blank">Spark Doris Connector </a>，以 Stream Load 的方式将数据实时写入 Doris，实现端到端的流式数据处理流程。</p><p>使用 Spark 自带的 rate 数据源模拟数据写入 Doris 中，示例如下：</p><blockquote>完整代码参考： <a href="https://link.segmentfault.com/?enc=0ziBlGlAJgOPnpkfISOz4A%3D%3D.tSfNHuC%2FAaDv9kMSWLBIj2hETeYkMEdt5BmJ8O40KJJw3gYihKa1AfveW6YIQrrIPoBh4BYoCFcNOd4jf8XDGiWLUv08wOJXEXPAtD%2BTAb2ot7lQZer5wTphW17%2BmVKKoozF%2FEfuBsyKvJ5VsZU6pHNZ%2Fxab4q1nOXS%2BZLJn3r%2FFywAnuAMvZPUplvjCTgbvRWpr%2BPXnhLnyrY6%2Fg5GAjjnXdhAjr0u26oYqrrPQ4Tqp4WZcn%2FuJYPCUFj46HXEK" rel="nofollow" target="_blank">https://github.com/apache/doris-spark-connector/blob/master/spark-doris-connector/spark-doris-connector-it/src/test/java/org/apache/doris/spark/example/DorisWriteStreamExample.scala</a></blockquote><pre><code class="protobuf">val spark = SparkSession.builder()
  .appName("RateSourceExample")
  .master("local[1]")
  .getOrCreate()

val rateStream = spark.readStream
  .format("rate")
  .option("rowsPerSecond", 10)
  .load()

rateStream.writeStream
  .format("doris")
  .option("checkpointLocation", "/tmp/checkpoint")
  .option("doris.table.identifier", "db.table")
  .option("doris.fenodes", "127.0.0.1:8030")
  .option("user", "root")
  .option("password", "")
  .start()
  .awaitTermination()

spark.stop();</code></pre><h2>实时数据分析最佳实践</h2><h3>用户案例 1：中通快递</h3><p>随着中通快递业务的持续增长，昔日双 11 的业务高峰现已成为每日常态，原有数据架构在数据时效性、查询效率、与维护成本方面，均面临着较大的挑战。为此，中通快递引入 SelectDB，借助其高效的数据更新、低延时的实时写入与优异的查询性能，在快递业务实时分析场景、BI 报表与离线分析场景、高并发分析场景中均进行了应用实践。</p><p><strong>在实时分析场景中，基于 SelectDB 灵活丰富的 SQL 函数公式、高吞吐量的计算能力，实现了结果表的查询加速， 能够达到每秒上 2K+ 数量级的 QPS 并发查询，数据报表更新及时度大大提高。</strong></p><p>SelectDB 的引入满足了复杂与简单的实时分析需求。目前，SelectDB 日处理数据超过 6 亿条，数据总量超过 45 亿条，字段总量超过 200 列，并实现服务器资源节省 2/3、查询时长从 10 分钟降至秒级的数十倍提升。</p><h3>用户案例 2：招联金融</h3><p>招联金融（全称“招联消费金融股份有限公司”）旗下拥有“好期贷”“信用付”两大消费金融产品体系，为用户提供全线上、免担保、低利率的普惠消费信贷服务。早期采用 Lambda 架构，包含 ClickHouse、Spark、Impala、Hive、Kudu、Vertica 等，受限于运维依赖性高、资源利用率低、数据时效性低、并发能力弱等诸多问题。引入 Apache Doris 进行架构升级后，实现了高效实时分析、架构简化、混合部署与弹性伸缩等多项目标。</p><p><strong>在客群筛选分析场景中，之前使用 Vertica 计算引擎处理 2.4 亿条数据耗时 30-60 分钟，替换为 Doris 后用时降至 5 分钟，性能提升 6 倍以上</strong>，并且 Doris 作为开源数据库，相比商业化产品 Vertica 有显著的成本优势。</p><h2>结束语</h2><p>以上就是 Apache Doris 在分析领域的实时更新能力详细介绍。在主键表方面，Doris 支持易用的 UPSERT 语义，结合主键索引和标记删除机制，确保了优异的写入性能和低延迟的查询性能。此外，用户自定义的冲突解决机制进一步提升了实时写入的并发能力，快速的 Schema 变更功能则避免了实时数据流的中断。列更新及灵活的列更新选项为更广泛的实时场景提供了便捷支持。</p><p>展望未来，我们将在以下几方面重点投入：</p><ul><li>降低数据可见性延迟，以实现更加实时的数据访问体验；</li><li>提升生态工具在自动调整 Schema 方面的能力，并扩展 Light Schema 的适用范围；</li><li>更加灵活的列更新，为用户提供更加高效、灵活的数据管理能力。</li></ul>]]></description></item><item>    <title><![CDATA[MacX Video Converter]]></title>    <link>https://segmentfault.com/a/1190000047412640</link>    <guid>https://segmentfault.com/a/1190000047412640</guid>    <pubDate>2025-11-19 21:03:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p><strong>MacX Video Converter Pro</strong>是一款专为 <strong>Mac 电脑用户</strong>设计的 <strong>专业视频格式转换软件</strong>，支持 <strong>快速转换各种视频格式</strong>，比如 MP4、AVI、MOV、MKV、WMV 等，并且还能 <strong>高清转换、提取音频、压缩视频大小、下载在线视频</strong>等。</p><h3><strong>1. 下载文件</strong></h3><ul><li><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=5DGot4fzeQnImL4m3kc57A%3D%3D.XAk9%2BRTDP4TRar6mCRDjO4kAwVLapfKGEo7gkz4NVm9zCnHnJKTQfm383Flb1VMO" rel="nofollow" title="https://pan.quark.cn/s/5cf9b8ed4190" target="_blank">https://pan.quark.cn/s/5cf9b8ed4190</a>，把那个叫 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>的文件下载下来了，一般是在“下载”文件夹里。</li></ul><h3><strong>2. 找到 .dmg 文件</strong></h3><ul><li>打开电脑上的  <strong>“访达”（就是屏幕左上角那个像笑脸的图标）</strong> ，然后点左边栏里的  <strong>“下载”</strong> ，找到那个 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>文件，双击它。</li></ul><blockquote>🖱️ 双击后，它会自动打开，通常会看到一个窗口弹出来，里面有一个 <strong>应用程序图标（可能是软件的Logo）</strong> ，还有一个  <strong>“Applications”（应用程序）文件夹的快捷方式</strong>。</blockquote><h3><strong>3. 把软件拖进“应用程序”文件夹</strong></h3><ul><li>看到那个软件图标（比如写着 <strong>MacX Video Converter Pro</strong>的那个），直接用鼠标 <strong>按住它别松手，然后拖到右边那个 “Applications”（应用程序）文件夹图标上</strong>，松开鼠标。</li></ul><blockquote>✅ 这一步相当于把软件“安装”到了你的 Mac 应用程序列表里，跟其他软件放在一起。</blockquote><h3><strong>4. 等复制完成</strong></h3><ul><li>拖过去之后，系统会自动把软件文件复制到“应用程序”里，等进度条走完或者那个小窗口提示完成就行。</li></ul><h3><strong>5. （可选）弹出安装盘</strong></h3><ul><li>复制完成后，你可能会看到那个原来的 <strong>MacX Video Converter Pro for Mac v6.8.2.dmg</strong>文件窗口还在，左上角有个小小的  <strong>“弹出”按钮（📀 图标旁边）</strong> ，点一下把它弹出，省得以后误点。</li></ul><h3><strong>6. 打开软件</strong></h3><ul><li>现在去  <strong>“访达” &gt; “应用程序”</strong> 里找到 <strong>MacX Video Converter Pro</strong>，双击打开。</li></ul><blockquote><p>⚠️ 第一次打开时，Mac 可能会弹窗说“这个软件来自不明开发者，不能打开”，别慌：</p><ul><li>去屏幕左上角  <strong>“苹果图标” &gt; “系统设置”（或“系统偏好设置”）&gt; “隐私与安全性”</strong> ，往下拉会看到一条提示说“MacX Video Converter Pro 被阻止”，点旁边的  <strong>“仍要打开”</strong> 就行。</li><li>或者右键点击软件图标，选  <strong>“打开”</strong> ，然后再点确认。</li></ul></blockquote><h3><strong>7. 开始使用</strong></h3><ul><li>打开之后你就可以导入视频，开始转换格式啦！</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[公开免费！Apache Doris & ]]></title>    <link>https://segmentfault.com/a/1190000047412658</link>    <guid>https://segmentfault.com/a/1190000047412658</guid>    <pubDate>2025-11-19 21:03:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>Apache Doris &amp; SelectDB 培训与认证课程上线信息分析</h2><h3>一、课程上线背景</h3><p>随着 Apache Doris 用户群体不断壮大，用户在学习和掌握该数据库过程中面临“缺乏系统性指导”的问题。无论是初次接触的新手，还是希望在特定场景深度应用的用户，均迫切需要一套从0到1、覆盖全面的学习路径。基于此需求，飞轮科技正式推出 Apache Doris &amp; SelectDB 培训与认证体系，旨在帮助用户从理论到实践，逐步成长为实时分析型数据库专家。</p><h3>二、基础课程详情</h3><h4>（一）课程开放方式</h4><p>首批上线的初级课程面向所有用户，且<strong>全部免费开放在 SelectDB 官网</strong>。</p><h4>（二）课程核心内容</h4><table><thead><tr><th>课程模块</th><th>具体学习要点</th></tr></thead><tbody><tr><td>基础概念</td><td>全面理解 Apache Doris 的核心理念与架构</td></tr><tr><td>安装部署</td><td>掌握 Doris 的安装与部署方法，能够快速搭建首个 Doris 环境</td></tr><tr><td>表设计</td><td>学习表结构设计逻辑，掌握在 Doris 中新建表的操作</td></tr><tr><td>数据操作</td><td>掌握 Doris 中数据的导入、更新与删除操作</td></tr><tr><td>数据查询</td><td>了解 Doris 支持的查询类型，具备基础查询能力</td></tr><tr><td>基础运维</td><td>了解日常运维操作流程，保障 Doris 集群稳定运行</td></tr><tr><td>数据湖查询</td><td>了解 Doris 联邦分析能力，为后续进阶学习湖仓一体奠定基础</td></tr></tbody></table><h3>三、初级认证考试信息</h3><h4>（一）认证名称</h4><p>Apache Doris 与 SelectDB 初级认证考试（SCDA）</p><h4>（二）考试核心规则</h4><ol><li><strong>考试形式</strong>：远程考试，需在1小时内完成</li><li><strong>考题设置</strong>：共50道单选题，每题2分，正确率达到60%（即得分≥60分）视为通过认证</li><li><strong>考试频率</strong>：认证考试平均每月开放一场，当前已开放报名与考试</li><li><strong>报名入口</strong>：<a href="https://link.segmentfault.com/?enc=lMTW4jR5qyErcJbfYFkrmw%3D%3D.D3vGarNU1vp5OdC%2BdqqXqjjhU%2FFRnEUizvk9wM0JBz9N3w5y0d8C17LymjffYeWEEephF3NXCftpQHJ6KvlKPQ%3D%3D" rel="nofollow" target="_blank">https://www.selectdb.com/resources/training/scda?examId=1</a></li><li><strong>报考建议</strong>：参加认证无前提条件，但官方强烈建议先完成基础入门培训课程后再参与考试</li></ol><h4>（三）认证证书信息</h4><p><img width="723" height="515" referrerpolicy="no-referrer" src="/img/bVdm6mr" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[AI 招聘系统功能落地指南 爱跑步的香蕉]]></title>    <link>https://segmentfault.com/a/1190000047412675</link>    <guid>https://segmentfault.com/a/1190000047412675</guid>    <pubDate>2025-11-19 21:02:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI 招聘系统功能落地指南<br/>AI全面重塑招聘行业：效率与体验的双重革新<br/>如果你仍在为“候选人爽约”“简历筛选繁重”“面试效率低下”而困扰，可能已经忽视了一个激进的行业趋势：招聘正被AI全面接管。这并非停留在讨论或小范围试点阶段，而是大规模落地、快速替代传统面试流程的现实。<br/>过去一年，多项行业数据印证了这一变革：近50%的企业已将初筛环节完全交给AI；金融、互联网、消费、制造等行业中，约2000家公司将AI面试纳入主流流程；超百万人通过AI面试完成测评、初筛与技术问答；部分银行甚至创下“2小时完成上万人初筛”的行业纪录。<br/>这些数字背后，是AI技术进步速度远超部分企业组织升级速度的残酷现实。当一些团队仍在逐份筛简历、重复提问基础问题时，竞争对手已用AI将初筛流程压缩至“分钟级”；当部分人质疑AI专业性时，不少企业已直接用AI面试官的评分做招聘决策；当担心AI影响候选人体验时，有些企业已将“AI面试”打造成雇主品牌亮点。这道行业分水岭清晰呈现：慢一步不仅是效率落后，更是错失优秀人才的风险。</p><p>01 精准评估：打破传统招聘的主观局限<br/>传统面试的核心痛点在于主观性强，不同面试官、同一面试官在不同时间的判断都可能存在差异。而AI面试系统通过技术优化，实现了评估精准度的大幅提升。<br/>其评分经过多维度验证：在人机对比实验中，与人工评分高度一致；通过效标效度与重测信度双重心理学指标检验，评分稳定且可信；迭代版本的模型升级，进一步提升了评估精度。这使得AI评估结果不仅具备高参考价值，更可直接作为招聘决策依据，让企业摆脱“凭经验”“凭感觉”的招聘困境。<br/>02 核心技术升级：让评估更高效全面<br/>AI面试系统的精准性，体现在招聘各环节的技术突破中：<br/>•一问多能：一道问题可同步评估沟通、逻辑、岗位胜任力等多项维度，既能满足HR初筛需求，也能无缝衔接专业复试，评估效率提升50%以上。<br/>•自由追问：针对候选人模糊的回答，即时生成针对性问题，如同资深面试官般抓住关键信息，避免遗漏核心能力点。<br/>•简历深度挖掘：自动识别简历中的关键经历与疑点，生成递进式提问，既防范候选人过度包装、信息造假，也避免HR因忙碌漏看亮点，导致优秀人才在粗筛环节被淘汰。<br/>•全维度专业适配：既能评估沟通、协作等通用胜任力，也能针对编程、算法、工程、财务等专业岗位精准出题，同时减轻HR与专业面试官的工作负担。<br/>03 拟人化交互：优化候选人面试体验<br/>传统AI面试常因“机械、生硬”引发质疑，而新一代AI面试系统通过拟人化设计，让面试体验成为雇主品牌加分项：<br/>•情绪感知：能够识别候选人的语速、情绪与潜台词，引导紧张的候选人稳定发挥，充分展现自身实力。<br/>•流程顺滑：自动识别回答状态，无需候选人手动点击按钮续答或结束，交互过程如同面对面沟通般自然。<br/>•视觉升级：大幅提升语音与口型同步精度，摆脱传统AI面试的“纸片人感”，增强沉浸式体验。<br/>•即时答疑：支持候选人随时提问职位信息、企业福利等内容，AI精准解答，帮助候选人深入了解企业，提升入职意愿。<br/>04 全流程自动化：重构招聘效率天花板<br/>AI招聘工具已不止于面试环节，更发展为覆盖全流程的自动化系统，实现从简历筛选到入库的一体化执行：<br/>•快速启动：30-60秒即可完成初始化，无需复杂设置，立即自动启动服务。<br/>•智能筛选：根据企业预设的年龄、学历、薪资等条件，自动筛选符合要求的候选人。<br/>•动态沟通：模拟真人语气与候选人进行问答互动，自动回复所有未读消息，避免错过潜在人才。<br/>•资料补全：当候选人信息缺失时，主动索取相关资料，交流方式贴近人工沟通习惯。<br/>•数据同步：自动下载简历并同步至企业ATS系统，生成候选人档案，保障数据流转高效安全。<br/>这类自动化系统将招聘效率提升10-100倍，推动招聘流程从“人工驱动”向“技术驱动”转型，成为企业应对人才竞争的重要支撑。</p>]]></description></item><item>    <title><![CDATA[浩瀚深度：从 ClickHouse 到 ]]></title>    <link>https://segmentfault.com/a/1190000047412679</link>    <guid>https://segmentfault.com/a/1190000047412679</guid>    <pubDate>2025-11-19 21:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>浩瀚深度（[SHA: 688292]）旗下企业级大数据平台选择 Apache Doris 作为核心数据库解决方案，目前已在全国范围内十余个生产环境中稳步运行，<strong>其中最大规模集群部署于 117 个高性能服务器节点，单表原始数据量超 13PB，行数突破 534 万亿，日均导入数据约 145TB，节假日峰值达 158TB，是目前已知国内最大单表</strong>。凭借 Apache Doris 的高可靠、高性能与高可扩展能力，该集群已<strong>持续稳定运行半年以上</strong>，充分验证了其在超大规模数据场景下的卓越表现。</blockquote><p>浩瀚深度作为国内互联网流量解析与数据智能化领域的领军企业，深耕行业三十余载，持续为国内互联网提供高性能、高精度、高可靠的整体解决方案。公司业务覆盖网络可视化、AI 智能、数据治理、数据价值挖掘及安全防护，是一家集软硬件产品研发、生产、销售和服务于一体的大型高科技企业。</p><p>顺水云大数据平台（StreamCloud）作为浩瀚深度自主研发的企业级的大数据平台产品，涵盖了从数据采集、数据存储、数据处理、数据挖掘、数据治理到数据共享的完整数据开发流程。帮助企业客户快速构建 PB 级数据中台，目前已经在通信、金融、交通等领域落地部署 100+ 项目，管理超过 130PB 数据，集群节点规模近万个。</p><p>为满足客户每日写入及查询万亿级增量数据的严苛需求，顺水云对 MPP 数据库产品进行了多轮选型测试，并在实际生产环境中尝试过 Greenplum、ClickHouse 等多个方案。经过综合比对，最终选定 Apache Doris 作为核心数据库解决方案。<strong>目前，该方案已在全国十余个生产环境上线运行，其中规模最大的集群部署于 117 个高性能服务器节点之上，单表原始数据量超 13PB，行数突破 534 万亿，日均导入数据量约 145TB ，节假日峰值数据量约 158TB，且已持续稳定运行半年以上。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412681" alt="平台架构.png" title="平台架构.png"/></p><h2>早期架构以及痛点</h2><p>早期架构如图所示，数据主要来源为用户上网日志，数据经过采集设备解析还原后发送到接口机上，再由接口机上程序接入 HDFS 集群，基于 Apache  Spark 将不同类型话单经过加工、回填、合成等流程处理后生成结果数据，最终写入至 ClickHouse 中，用于日志存储与快速查询、流量质量分析、面向政企市场的用户画像 &amp; 精准营销等场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412682" alt="早期架构以及痛点.png" title="早期架构以及痛点.png" loading="lazy"/></p><p>随着业务数据体量逐渐庞大，对于高吞吐的数据写入、亿级数据的秒级响应、海量数据关联查询的需求也愈加迫切，以 ClickHouse 为核心的 OLAP 查询分析引擎体系在使用过程中对业务人员开发、运维人员维护存在如下痛点：</p><ol><li><strong>写入稳定性差且存储成本较高</strong>：为降低存储成本，我们尝试使用 ZSTD 压缩格式，但因其高压缩比带来的性能开销，频繁出现“too many parts”及当日数据入库积压问题。为保障业务稳定运行，只能增加存储成本使用默认的 LZ4 压缩；</li><li><strong>运维成本高</strong>：使用自研的数据入库工具，由于不同接口机上数据量和导入性能差异，导致 ClickHouse 集群上各节点数据量不均衡，由于 ClickHouse 架构特性，坏盘时数据无法自动迁移，需要人工持续干预；</li><li><strong>并发查询能力不足</strong>：在并发查询较多场景下，查询性能下降明显，无法支持业务需求；</li><li><strong>JOIN 能力不足</strong>：由于 ClickHouse 自身组件设计无法支持多表或大表 Join 查询场景，难以满足大表关联查询需求。</li></ol><h2>Apache Doris vs.  ClickHouse 对比测试</h2><h3>测试准备</h3><p>为了进一步对比验证 Doris 的写入和查询性能，我们使用了三台物理机模拟生产环境数据和业务对 Apache Doris 和 ClickHouse 做了一系列对比测试。主要分为以下 3 部分：</p><ul><li>前缀索引测试对比</li></ul><blockquote>查看<a href="https://link.segmentfault.com/?enc=f%2BA98PHZyhw7nt%2BLfQo7%2FQ%3D%3D.RDadRKs%2BI%2FyRRGlp%2Bg5mtTv0jloYvN9rCYly6TsJwFoLjVL2N1%2FFegnfIyMe33uoYKp45%2FMhKsB95uGi3UuFDdi3J8N5RgElbgNHctLBLMM%3D" rel="nofollow" target="_blank">前缀索引文档</a>详情</blockquote><ul><li>二级索引测试对比</li></ul><blockquote><p>查看 <a href="https://link.segmentfault.com/?enc=6a%2FYPgNo3kxXFQ6Pg3gQdg%3D%3D.tL5IBf8enZVaDf1f46gwPpWHdTIy3ibeayZmUE1Of4hYqIWqh5DRYSTwAHsRgniMqJ6xLMlXThQzh14YpVlrbKUbEou4P3q8Awjj6Sm1ZaA%3D" rel="nofollow" target="_blank">BloomFilter Index 文档</a>详情</p><p>查看 <a href="https://link.segmentfault.com/?enc=OZw%2B1QKNuWrnFU3kNOwFpQ%3D%3D.Xw3YuZCo9DWyCxWOM7RMXghOe208LMPzVnrMYF1n7PUz76Wr3PtD2EtqwFgBT2brz%2F0yaMe9mYMZOZFCyOI7s5xRAEZVaMhERb5kgZu9w8o%3D" rel="nofollow" target="_blank">Inverted Index 文档</a>详情</p></blockquote><ul><li><p>全表扫描测试对比</p><p><strong>测试参数：</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412683" alt="全表扫描测试对比-1.png" title="全表扫描测试对比-1.png" loading="lazy"/></p><p><strong>建表：</strong></p><p>Doris 自 2.0.0 版本支持倒排索引能力，因此我们在表的数据量和字段一致的背景下，针对不同的排序键与索引类型进行查询速度测试：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412684" alt="全表扫描测试对比-2.png" title="全表扫描测试对比-2.png" loading="lazy"/></p></li></ul><h3>测试一：前缀索引</h3><p>前缀索引可以加速等值查询和范围查询。在 Doris 中，建表时自动取表的 Key 的前 36 个字节作为前缀索引。前缀索引测试分为 3 次冷查询，测试结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412685" alt="测试一：前缀索引.png" title="测试一：前缀索引.png" loading="lazy"/></p><h3>测试二：二级索引</h3><ul><li>BloomFilter Index 是基于 BloomFilter 的一种跳数索引，其原理是利用 BloomFilter 跳过等值查询指定条件不满足的数据块，达到减少 I/O 查询加速的目的。</li><li>Inverted Index 可以用来进行文本类型的全文检索、普通数值日期类型的等值范围查询，快速从海量数据中过滤出满足条件的行。</li></ul><p>二级索引分别对比测试了 Doris 的<a href="https://link.segmentfault.com/?enc=W46CJyaBSbXjOs7CRn3M4w%3D%3D.7M4xDfaUIcdHyvUAlBIpYz3Q4AlLtQzUuFUFJ8EjOLY1A4cGirlvWSiDXMnXV%2BM8jER3wa8QY%2F5J7md8x8ACC%2BHG1RjW2vjmrUyCGKaAXEeQfpHZ%2FB0uRfUE%2FLIBPZLxAp76%2FnU2g1vNdbpHNu7QNw%3D%3D" rel="nofollow" target="_blank"> BloomFilter Index</a> 和 <a href="https://link.segmentfault.com/?enc=ZxnHW1kuqeSKfUw%2BYahgFg%3D%3D.h7neGmgDGmQZlyrtXFwCe7xf7fLeceNktnHOWjUJ3bRmChHLSoaqeElAffQ6Aj8SfZerakeUx9uthsYv0lEUkrwOKbXxQah%2FssKZNkHc%2FNlRgTDvWF%2FGhRV%2FugWaiQIubgK4sksbPpf9cLpXCYoSaQ%3D%3D" rel="nofollow" target="_blank">Inverted Index</a> ，测试分为 3 次冷查询，测试结果如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412686" alt="测试二：二级索引.png" title="测试二：二级索引.png" loading="lazy"/></p><h3>测试三：全表扫描</h3><p>全表扫描测试主要是测试了常用业务查询场景：like 模糊查询和 IP 函数，根据测试结果， <code>IS_IP_ADDRESS_IN_RANGE</code> 函数查询方面 ClickHouse 略胜。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412687" alt="测试三：全表扫描.png" title="测试三：全表扫描.png" loading="lazy"/></p><h3>结果分析</h3><p>在合理配置索引的前提下，Doris 在关键查询场景下展现出显著性能优势：</p><ul><li><strong>前缀索引：</strong> Doris  查询速度是 ClickHouse 的 2 倍以上。</li><li><p><strong>二级索引：</strong></p><ul><li>使用 BloomFilter 索引时，Doris 查询速度领先 ClickHouse 达 2 倍。</li><li>相同场景下 Doris 的倒排索引功能，使得查询性能更是大幅跃升，速度远超 ClickHouse，是其性能的 5 倍以上。</li></ul></li><li><strong>全表扫描：</strong> 两者性能接近，ClickHouse 在特定函数调用上略占优势。</li></ul><p>综合来看，两者各有所长，但测试表明：在常用业务查询场景中，Doris 的前缀索引、BloomFilter 和倒排索引性能全面优于 ClickHouse。据此评估，迁移至 Doris 后，查询响应速度预计提升超 2 倍。</p><h2>Doris 替换实践</h2><p>由于 ClickHouse 和 Doris 均为 MPP 架构数据库，且 Doris 支持 MySQL 语法，因此架构变化小、迁移便捷。仅需将日志存储与分析引擎由 ClickHouse 替换为 Doris，具体步骤如下：</p><ul><li>调整上游 Importer 写入组件的配置，使其将日志数据直接写入 Doris 表；</li><li>更新下游查询服务的 SQL 语句以适配 Doris 语法即可完成无缝迁移。</li></ul><p>虽然我们对 Doris 做了几 TB 的数据测试做使用参考，但考虑生产环境日增数百 TB 的数据量级，再加上引入新组件的不确定性，实施初期我们采用了 ClickHouse 和 Doris 并行运行的方式。</p><p>同时在压测期间也遇到一些问题，目前均已解决，我们将这些问题的解决思路整理并分享至社区，以供参考。</p><h3>实践一：解决大批量写入报错问题</h3><p>在进行导入压力测试阶段，小数据量下，集群运行状况和导入性能均表现良好。但是在逐渐上升至 30 台接口机同时并发导入时，系统逐渐出现一些异常情况。具体如下：</p><pre><code class="Plain">org.apache.doris.common.UserException: errCode = 2, detailMessage = get tableList write lock timeout, tableList=(Table [id=885211, name=td_home_dist, type=OLAP])
 
[INTERNAL_ERROR]cancelled: tablet error: tablet writer write failed, tablet_id=24484509, txn_id=3078341, err=[E-216]try migration lock failed, host: ***</code></pre><p>在出现异常情况后，我们及时和社区同学联系沟通，参考<a href="https://link.segmentfault.com/?enc=Wuh%2B%2BwDf78zzr12a08nrvg%3D%3D.%2BC0MfmMBsrAoxmZDOMx4qJZYqX4eg0FnD7v7uDDJxzv5y1U8wAtjA36USNDIruqgqyd%2BdWWw9kiz2r7R67FoTg%3D%3D" rel="nofollow" target="_blank">官方文档</a>中的《日志存储和分析》模块的参数进行调优后，导入任务恢复正常。</p><h3>实践二：Compaction 压力过载优化</h3><p>进一步压测，我们发现在导入持续一段时间后，BE 节点在监控中出现异常。结合监控以及 <code>top -H</code> 的输出，发现 Compaction 占用 CPU 资源比较多，导致 BE 出现假死。经过 Compaction 问题排查，发现与 Bucket 数量有关。</p><p>由于当时按照 ClickHouse 自动合并后单个分区下文件数量的最大值来设置， Bucket 数量我们设为了 480，导致 Tablet 过多而引发问题。</p><p>后来结合实际业务场景，以及业务高峰期数据量等因素进行了计算后，将 Bucket 缩减为 280 ，调整后 Compaction 资源占用恢复正常 ，BE 节点恢复平稳运行。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412688" alt="实践二：Compaction 压力过载优化.png" title="实践二：Compaction 压力过载优化.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412689" alt="实践二：Compaction 压力过载优化-2.png" title="实践二：Compaction 压力过载优化-2.png" loading="lazy"/></p><h3>实践三：导入异常问题排查</h3><p>在压测期间导入出现一个事物卡住的情况，异常如下：</p><pre><code class="Plain">errCode = 2, detailMessage = current running txns on db 10194 is 10000, larger than limit 10000</code></pre><p>根据报错内容引导处理，我们调整了单个 DB 下的事务数量，发现不是根本原因。随后，我们将问题反馈给社区同学，在他们的协助下，很快定位到了问题根源。具体定位步骤如下：</p><ol><li>找到一个具体的事务 ID</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412690" alt="实践三：导入异常问题排查.png" title="实践三：导入异常问题排查.png" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412691" alt="实践三：导入异常问题排查-2.png" title="实践三：导入异常问题排查-2.png" loading="lazy"/></p><ol start="2"><li><p>登录 FE 的主节点，搜索事务对应的具体 BE</p><pre><code class="Plain">grep "16788479" /opt/install/doris-2.15/fe/log/fe.log</code></pre></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412692" alt="实践三：导入异常问题排查-3.png" title="实践三：导入异常问题排查-3.png" loading="lazy"/></p><ol start="3"><li><p>登录到具体 BE，在日志中搜索该事务 ID</p><pre><code class="Plain">grep "16788479" /opt/install/doris-2.15/be/be.INFO</code></pre></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412693" alt="实践三：导入异常问题排查-4.png" title="实践三：导入异常问题排查-4.png" loading="lazy"/></p><p>排查发现，事故起因是某台机器磁盘写满，导致集群开始将该节点的写入和副本调度至其他节点，而当前写入压力较大，进而引发了事务积压。此外，由于磁盘上仍残留部分 ClickHouse 数据，进一步加剧了磁盘空间分布不均的问题。</p><h3>实践四：使用 Broker Load 解放接口机</h3><p>由于 Doris 中的导入功能非常丰富，几乎在每个场景都有对应的导入，刚开始我们使用的是 Stream Load 导入本地数据文件，在进一步学习了解使用 Doris 后，发现 Broker Load 的导入方式更契合业务场景，因为系统加工合成的数据本身就存储在 HDFS 上，Broker Load 可以直接从 HDFS 上拉取数据，相比较之前从 HDFS 下载数据到本地再进行导入方式，这样不仅能减少一次数据传输，还解放了数据导入使用的接口机，进一步提高了效率。我们需要做的就是申请 Doris 集群和 HDFS 集群的网络打通和 Broker 的部署，并编写一套检测数据并提交  Broker Load 的脚本。</p><p>经测试，目前已经上线上百台的 Broker 节点去进行并行拉取 HDFS 的数据并进行导入，写入性能优异。但使用 Broker Load 时要注意按照业务场景进行调整，如果默认子任务失败则会导致整个目录的文件导入全部失败。HDFS 中数据以 15 分钟粒度为目录存储的，需要做好相应的失败检测与重试机制。</p><p>在机器成本方面，相较于使用 ClickHouse 导入一天数据需要 32 台接口机，改用 Doris 后，省去了从 HDFS 拉取数据的机器，仅需 23 台即可完成同等数据量的导入，<strong>机器资源节省超过 28%</strong>，显著降低了成本并提效。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412694" alt="实践四：使用 Borker Load 解放接口机-1.png" title="实践四：使用 Borker Load 解放接口机-1.png" loading="lazy"/></p><h2>架构升级成果</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412695" alt="架构升级成果.png" title="架构升级成果.png" loading="lazy"/></p><p>目前，浩瀚深度已在某运营商客户环境使用 Doris 替换 ClickHouse 构建了新的查询分析平台，服务器规模超百台，并实现日增数据量峰值近 158TB 的数据导入。采用双副本 + 倒排索引 + ZSTD 压缩后，存储量约 6.5PB，和原始数据相比，Doris 中单个副本的压缩率在 4 倍左右，且目前已稳定运行半年多，这次升级带来查询响应、并发能力、稳定性和运维效率等多方面的收益，成果显著。</p><ul><li><strong>显著降低硬件资源成本</strong>：利用 Doris Broker Load 高效导入机制，释放原 ClickHouse 所需的 32 台专用接口机，这些资源可灵活用于计算或存储，整体硬件成本节超 28%。采用 ZSTD 高压缩比格式，在未出现写入瓶颈的前提下，存储资源消耗相较 ClickHouse（LZ4 压缩）降低了 6%。</li><li><strong>大幅提升查询效率</strong>：Doris 卓越的索引优化（前缀索引、Bloom Filter、倒排索引）及多表 JOIN 性能全面超越 ClickHouse。单 SQL 查询响应速度提升近 2 倍。批量查询任务执行效率提升近 30%。</li><li><strong>有效降低运维复杂度与成本</strong>：服务器宕机或坏盘时，Doris 自动完成副本切换与写入重定向，保障服务连续性。集群扩缩容时，Doris 自动实现 Tablet 均衡分布，快速恢复集群负载均衡。通过 Doris 原生 Web UI 与 Grafana 监控，异常节点与磁盘故障可被快速定位。</li></ul><h2>未来规划</h2><p>未来，浩瀚深度将从以下方面重点发展：</p><ul><li><strong>持续深化Doris 的湖仓一体化应用</strong>：通过 Doris 的 Hive Catalog 功能整合数据仓库资源，统一数据访问接口，实现对全量数据的统一查询与分析；</li><li><strong>复杂查询加速</strong>：在多维度分析、聚合计算等复杂查询场景下，依托 Doris 强大的整合能力提升查询效率，加速报表生成；</li><li><strong>成本优化</strong>：利用 Doris 的冷热数据分层存储等特性，在持续优化查询性能的同时，进一步降低总体存储成本。</li></ul><p>最后，衷心感谢飞轮科技技术团队与 Doris 社区对浩瀚深度的持续、专业的技术支持，有力推动了我们的国产化架构转型进程。 我们热忱期待更多同仁加入 Apache Doris 的应用实践与社区贡献行列，共同丰富其功能生态、扩展函数支持，助力 Apache Doris 在全球 MPP 数据库领域绽放璀璨光芒！</p>]]></description></item><item>    <title><![CDATA[AI 语音转贴纸，儿童打印机 Stick]]></title>    <link>https://segmentfault.com/a/1190000047412518</link>    <guid>https://segmentfault.com/a/1190000047412518</guid>    <pubDate>2025-11-19 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412520" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、Spatial AI 发布第一人称真实世界数据集，教会机器人在现实世界完成任务</strong></p><p>Spatial AI 发布的首个开源数据集 SEA（Spatial Everyday Activities，空间日常活动）是迄今为止规模最大的、经过精心策划的以第一人称视角记录的、人们执行真实任务的数据集，数据量高达 10,000 小时。</p><p>Huggingface: <br/><a href="https://link.segmentfault.com/?enc=pxU71wtN3XeQen0Pf6egVQ%3D%3D.kOdMSZiqikGmVpMvjzJtBnsl%2FGvsew6IhURtfgc9bU0YskAr0pqJ8wL8GnReCq%2BtajDLx3k4Oks4xkGeK5L%2F4g%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/datasets/spatial-ai/sea-small</a></p><p>官网：<br/><a href="https://link.segmentfault.com/?enc=MTEVuYM93RnffLZYv6dJAw%3D%3D.WJqtznKltJ95rXpIC21nt2qnxjQDa8h%2BdNjMEDxVpwM%3D" rel="nofollow" target="_blank">https://spatial-ai.com/</a></p><p>（@ycombinator\@X）</p><p><strong>2、LiveKit 重磅推出三大新功能，赋能开发者构建更卓越的语音智能体</strong></p><p>LiveKit 在 Dev Day 活动宣布上线三项革命性新功能——Agent Builder、Phone Numbers 和 Agent Observability，旨在极大地简化语音智能体的开发、部署与调试流程，助力开发者以前所未有的速度构建和优化更智能、更可靠的语音应用。</p><p><strong>1. Agent Builder：3 分钟内构建生产级语音智能体</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412521" alt="" title="" loading="lazy"/></p><p>全新的 Agent Builder 是一款基于浏览器的无代码/低代码平台，让开发者能够快速创建、测试并部署语音智能体。用户只需提供系统提示（system prompt），即可在几分钟内启动语音智能体的构建。该平台集成了背景降噪、多语言识别等内置语音处理能力，并支持通过 HTTP 工具轻松扩展功能，连接 CRM、API 等外部服务。Agent Builder 提供内置的实时测试与一键部署功能，并集成了 Deepgram、AssemblyAI、GPT-4.1、Gemini 2.5 Flash 等多家知名供应商的模型，让开发者可以在内测阶段自由对比模型性能。最重要的是，Agent Builder 被设计为从原型到代码的桥梁，支持导出代码，实现从低代码到专业 SDK 开发的平滑过渡，大幅缩短语音 AI 应用的上市时间。</p><p><strong>2. Phone Numbers：60 秒内接入电话网络</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412522" alt="" title="" loading="lazy"/></p><p>LiveKit Phone Numbers 是一项全新的首方电话服务，开发者无需第三方 SIP Trunk 即可在数分钟内将语音智能体直接连接到电话网络。用户可在 LiveKit Cloud 仪表盘或 CLI 中直接购买美国本地或免费电话号码，并快速关联到语音智能体，实现「零配置」上线。此举消除了中间环节，减少了网络延迟和故障点，显著提升了通话质量、音频清晰度和语音转文本（STT）准确率，进而优化了整体语音交互体验。这项服务尤其适用于需要电话交互的行业，可快速搭建 24/7 咨询、订单查询等语音智能体。</p><p><strong>3. Agent Observability：统一视图助力语音智能体故障排查</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412523" alt="" title="" loading="lazy"/></p><p>Agent Observability Beta 版本现已集成于 LiveKit Cloud Dashboard，旨在解决语音智能体开发与部署中的棘手故障排查问题。用户现在可以在单一视图下，同步查看会话的音频、转录、逐轮智能体调用（LLM、TTS、转折点检测、工具调用等）以及日志。这种端到端的可见性，使得开发者能够像「听」和「看」一样，快速定位导致响应延迟、中断或任务失败的根本原因，极大地简化了以往需要在不同服务间手动关联日志和时间戳的复杂过程。会话录制功能支持可选启用，并提供灵活的数据控制选项。</p><p>( @LiveKit 官网)</p><p><strong>3、Google 正式发布 Gemini 3</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412524" alt="" title="" loading="lazy"/></p><p>今天凌晨，<strong>Google DeepMind 正式发布新一代旗舰模型 Gemini 3</strong>，号称拥有最先进的推理能力、世界领先的多模态理解能力，并支持新的智能编码体验。<strong>本次首发版本为 Gemini 3 Pro</strong>，并且即日起开始全球范围内推出，亮点如下：</p><ul><li>Gemini 3 Pro 预览版原生多模态支持（文字、图像、视频、音频）</li><li>在 LMArena 排行榜登顶，在推理、多模态、编程等主流测试中全面领先</li><li>推理能力创纪录（GPQA Diamond 91.9%、MathArena Apex 23.4%）</li><li>提供 Deep Think 深度思考模式（未来几周开放）</li><li>100 万 token 上下文窗口 + 64K 输出</li><li>推出全新 AI IDE：Google Antigravity，新模型已集成 Cursor、GitHub、JetBrains 等工具</li></ul><p>值得一提的是， 推理能力方面， <strong>Gemini 3 Pro 在「人类最后的考试（Humanity’s Last Exam）」中拿到了 37.5% 的博士级推理成绩。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412525" alt="" title="" loading="lazy"/></p><p><strong>有趣的是，OpenAI CEO Sam Altman 刚刚也在 X 平台发文称：「恭喜 Google 成功推出 Gemini 3！看起来是个很棒的模型。」而马斯克则是表示「做得好」。</strong></p><p>( @APPSO)</p><hr/><h2>02 有亮点的产品</h2><p><strong>1、Hapiko 获 700 万美元融资，推出儿童 AI 语音转贴纸打印机 Stickerbox</strong></p><p>总部位于布鲁克林的创意科技公司 Hapiko 宣布获得 700 万美元融资，并正式发布了其首款产品「Stickerbox」。这款 AI 驱动的语音转贴纸打印机专为儿童设计，允许他们通过语音描述图像，然后打印出可供涂鸦和定制的黑白贴纸，旨在鼓励安全、具象的创意玩乐，而非被动屏幕时间。</p><ul><li><strong>AI 语音转贴纸：</strong> 「Stickerbox」是首款能让儿童通过语音（如「骑着滑板的恐龙」）生成图像并打印成黑白贴纸的设备。</li><li><strong>鼓励具象化创意：</strong> 产品旨在引导儿童从被动接受信息转向主动、开放式的想象和创造，将口头想法转化为实体贴纸。</li><li><strong>安全与隐私优先：</strong> 采用墨水无关的热敏打印技术，使用不含 BPA 和 BPS 的纸张，无摄像头，无持续数据收集，AI 系统从零开始设计，内置年龄适宜性过滤。</li><li><strong>独立玩乐体验：</strong> 无需智能手机或笔记本电脑即可独立操作，支持儿童动手实践的玩乐方式。</li><li><strong>700 万美元融资：</strong> 本轮融资由 Maveron 和 Serena Ventures（由网球明星 Serena Williams 创立）领投，将用于扩大生产和拓展儿童创意玩具及 AI 市场。</li></ul><p>「Stickerbox」现已在美国上市，可通过 stickerbox.com 购买，零售价为 99.99 美元。Hapiko 计划利用此次融资扩大生产规模并拓展市场。</p><p>(@Pulse 2.0)</p><p><strong>2、连锁餐饮正在用炒菜机器人取代预制菜</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412526" alt="" title="" loading="lazy"/></p><p>据 36 氪报道，连锁餐饮品牌正加速引入炒菜机器人，以应对预制菜风波带来的行业挑战。</p><p>报道指出，老乡鸡、小菜园等餐企已在数百家门店部署智能烹饪设备，歌尔、富士康等大厂也开始在食堂场景中使用机器人，以提升出餐效率并降低人力成本。</p><p>炒菜机器人通过精准控温、自动搅拌与智能投料系统，实现标准化作业，既保留中餐「锅气」风味，又显著提高出餐速度。</p><p>烹饪机器人企业智谷天厨 CEO 耿凯平透露，过去一个月咨询需求增长近 10 倍，显示行业对智能化厨房的接受度快速提升。</p><p><strong>市场数据显示，一台售价约 6 万元的中型炒菜机器人，使用寿命可达 8 至 10 年，月均成本约 600 元。</strong>相比传统厨师每月 8000 至 15000 元的薪资，企业可在生命周期内节省数十万元至百万元不等的运营开支。</p><p>在企业团餐场景中，机器人还配备智能调度系统，可实时监测菜品余量与人流分布，动态补餐，避免食材浪费。</p><p>据实测，其浪费率可降低 70% 以上。业内人士指出，随着消费者对预制菜的抵触情绪加剧，机器人现炒成为餐企兼顾效率与品质的更优解。</p><p>( @APPSO)</p><p><strong>3、曝 Rabbit 公司停薪数月，官方坚称 Rabbit R1 下一代版本将面世</strong></p><p>11 月 18 日消息，据外媒报道，最近 Rabbit 因另一件事又被推上了风口浪尖：多名 Rabbit 员工称公司已连续数月拖欠工资，部分员工甚至从 10 月起直接罢工——可就在这个节骨眼上，Rabbit CEO 吕骋（Jesse Lyu）仍对外声称「计划在 2026 年推出下一代 AI 硬件」。</p><p>据报道，今年 1 月起，Rabbit 公司就开始出现发薪延迟的问题。原本每月仅是晚几天，可后来越来越严重：最严重的一次延迟了整整 37 天才发薪。到了 7 月，Rabbit 部分员工与外包人员彻底「停薪」，再没收到任何工资。10 月初，三名员工因长期未领到薪水而发起罢工，并一直持续到现在。更戏剧性的是，Rabbit 并未否认这一点，其发言人确认罢工的确存在，并强调：「公司共有 26 名员工，目前只有 3 人在罢工，我们完全尊重他们的决定。」</p><p>此前报道，Rabbit 公司在 2024 年推出了「口袋 AI 设备」Rabbit R1，当时官方宣称要让这款设备成为用户和智能手机的交互中介，并在开售 5 天后卖出 50000 台。不过首发用户上手后却发现，这款设备的实际表现远逊于发布会上宣称的效果，例如在实景拍摄过程中至少要等 20 秒才会回应，「比 Siri 还慢」，甚至有用户指出这款设备就是一个低端安卓手机装了一个 APK 文件。</p><p>（@IT 之家、CSDN、雷锋网）</p><h2>03 有态度的观点</h2><p><strong>1、Take-Two Interactive CEO 认为游戏行业正转向 PC</strong></p><p>Take-Two Interactive CEO Strauss Zelnick 在接受采访时表示，游戏行业正转向 PC 从封闭转向开放，但游戏机作为一种体验并不会消失。Zelnick 表示，游戏机和手游的市场份额产不多，但手游增长速度比游戏机更快。索尼 PS 和任天堂 Switch 的游戏机业务取得了成功，而竞争对手拥有 Xbox 的微软则暗示下一代硬件将更面向 PC 游戏。Valve 最近宣布的 Steam Machin 就是游戏机和 PC 的混合设备。</p><p>(@Solidot)</p><h2>04 社区黑板报</h2><p>招聘、项目分享、求助……任何你想和社区分享的信息，请联系我们投稿。（加微信 creators2022，备注「社区黑板报」）</p><p><strong>1、招聘实习生丨加入我们，共建 RTE 开发者社区</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412527" alt="" title="" loading="lazy"/></p><p><strong>RTE 开发者社区·运营实习生（实时互动 / Voice AI 方向，本招聘长期有效）</strong></p><p><strong>地点：北京·朝阳区望京南/上海·杨浦区五角场</strong></p><p><strong>这份实习将给你带来：</strong></p><p><strong>产品与技术成长：</strong> 深入学习垂类 AI 产品从技术到落地的全生命周期，构建全面的产品视角。</p><p><strong>社区运营实战：</strong> 与高潜力的开发者和创业者深度交流，共同探索行业前沿；并亲身体验顶级 AI 大会，拓展行业视野。</p><p><strong>【你的职责】</strong></p><ol><li><strong>Voice AI / RTE 情报官：</strong> 每日关注 Voice AI /实时互动领域的最新动态，提炼整理并分享行业洞察，定期撰写学习笔记，帮助团队和社区保持信息前沿。</li><li><strong>社区连接者：</strong> 负责 RTE 领域开发者、初创企业等核心群体的社群运营，主动建立并深化联系，鼓励并协助他们融入社区，共同维护社区的活力与生态。</li><li><strong>活动协作者：</strong> 深度参与 RTE Open Day、Meetup、Dev Talk 等线上线下活动的全流程运营，包括前期策划、中期执行、后期复盘，从实践中提升组织和协调能力。</li><li><strong>行业洞察者：</strong> 协助开展 RTE 相关行业及应用场景调研、产品竞争力分析，整理相关资料，形成对业务的深入理解和独到见解。</li></ol><p><strong>【希望你】</strong></p><ol><li>本科及以上学历，商业、技术、产品、媒体专业或经验背景优先，具备良好英文能力；</li><li>对 RTE / Voice AI 有浓厚兴趣和求知欲；具备优秀的信息收集与整合能力，乐于快速学习新事物，并具备严谨的逻辑思维。</li><li>能保证每周至少 4 天的工作时间，持续 3 个月以上。</li></ol><p><strong>【薪资】</strong></p><p>180-220 元/天</p><p><strong>【投递方式】</strong></p><p>实习地点北京或上海，请将简历发送至 rtedevcommunity\@gmail.com ；邮件标题请注明：【社区运营实习-姓名-学校-毕业年份-到岗日期-城市】</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412528" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412529" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=l3LGwWW8YkDNy1EuLIdY%2BQ%3D%3D.Q8klS8VDVqPLUfEX2OU9hHiUL8%2BzQVIwwcoul1qoKyc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412530" alt="" title="" loading="lazy"/><br/>素材来源官方媒体/网络新闻</p>]]></description></item><item>    <title><![CDATA[Python梯度提升树GBT、随机森林、]]></title>    <link>https://segmentfault.com/a/1190000047412108</link>    <guid>https://segmentfault.com/a/1190000047412108</guid>    <pubDate>2025-11-19 19:05:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>**全文链接：<a href="https://link.segmentfault.com/?enc=bEqhS7nHT81L2B%2FYrCSjNw%3D%3D.0hhfJiiOXgZwPbggvN8RGopizPhddqFxCIokZgF0KiM%3D" rel="nofollow" title="https://tecdat.cn/?p=44342" target="_blank">https://tecdat.cn/?p=44342</a>  <br/>原文出处：拓端数据部落公众号  <br/>分析师：Liping Xiao**</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412110" alt="封面" title="封面"/></p><h3><a name="t0" target="_blank"/>引言</h3><p>随着国内房地产市场进入精细化发展阶段，二手房交易已成为楼市流通的核心组成部分，购房者、投资者及行业从业者对市场动态与价格趋势的精准把握需求日益迫切。作为数据科学家，我们始终相信“数据驱动决策”的核心价值——从海量房产信息中挖掘规律，既能为普通购房者提供理性参考，也能为行业调控提供数据支撑。  <br/>本文内容改编自我们为某房地产咨询机构完成的实际项目，团队通过链家平台爬取多城市二手房数据，构建了一套从数据采集到模型落地的完整解决方案，已在实际业务中验证有效性。文章将详细拆解“数据爬取-清洗-分析-建模-优化”的全流程：先通过Python爬取三个城市各区二手房核心信息，经数据预处理后开展多维度可视化分析，再利用决策树（DT）、梯度提升树（GBT）、随机森林（RF）三种模型实现价格预测，最后通过异常值处理与网格搜索优化模型性能。  <br/>本文内容源自过往项目技术沉淀与已通过实际业务校验，该项目完整代码与数据已分享至交流社群。阅读原文进群，可与800+行业人士交流成长；还提供人工答疑，拆解核心原理、代码逻辑与业务适配思路，帮大家既懂怎么做，也懂为什么这么做；遇代码运行问题，更能享24小时调试支持。</p><p>数据爬取</p><p>数据清洗</p><p>探索式数据分析</p><p>特征工程</p><p>模型训练 DT/GBT/RF</p><p>模型优化 异常值处理+网格搜索</p><p>结果输出 价格预测+区域洞察</p><h3><a name="t1" target="_blank"/>项目背景与目标</h3><h4><a name="t2" target="_blank"/>项目背景</h4><p>在居住品质升级与楼市结构调整的双重驱动下，二手房市场的区域差异、价格影响因素愈发复杂。不同城市、同一城市不同区域的房价受地理位置、房屋属性、建筑年代等多重因素影响，传统经验判断已难以适应市场变化。基于此，我们聚焦三个代表性城市，通过数据挖掘技术构建分析与预测体系，填补市场洞察的精准度缺口。</p><h4><a name="t3" target="_blank"/>项目目标</h4><ol><li>数据爬取：通过Python requests库获取链家平台二手房核心字段，包括房源位置、面积、户型、总价、单价等关键信息。</li><li>数据清洗：完成去重、缺失值填充、异常值处理、数据类型转换等预处理，保障数据质量。</li><li>数据可视化：通过直方图、词云图、地理分布图等工具，直观呈现市场分布与价格特征。</li><li>特征工程：筛选并编码对房价有显著影响的特征，构建高效建模数据集。</li><li>模型训练：基于DT、GBT、RF三种算法构建价格预测模型，评估不同因素对房价的影响。</li><li>模型优化：通过异常值剔除与网格搜索调整参数，提升模型预测精度。</li><li>技术实现：依托Python生态，结合pandas、numpy、matplotlib等库完成全流程开发。</li></ol><h3><a name="t4" target="_blank"/>数据采集与预处理</h3><h4><a name="t5" target="_blank"/>数据采集</h4><p>本次数据来源于链家网二手房板块，通过分析网页URL规律，设计了分城市、分区的爬虫方案。先爬取目标城市各区域的URL，再按分页规则遍历每一页房源，最后下载单套房源的HTML文件并提取信息。  <br/>核心爬虫代码（修改后关键片段）：</p><pre><code>import requestsimport osfrom lxml import etreeimport pandas as pd# 定义爬取函数：参数为城市URL和城市名称def crawl_city_houses(city_url, city_name): # 创建城市对应的存储文件夹 city_dir = f'htmls/{city_name}' if not os.path.exists(city_dir): os.makedirs(city_dir) # 请求城市首页，获取各区链接 response = requests.get(city_url, headers=headers) html = etree.HTML(response.text) area_urls = html.xpath('//div[@data-role="ershoufang"]//a/@href') area_names = html.xpath('//div[@data-role="ershoufang"]//a/text()') # 遍历各区，爬取分页数据 for area_url, area_name in zip(area_urls, area_names): area_dir = f'{city_dir}/{area_name}' if not os.path.exists(area_dir): os.makedirs(area_dir)</code></pre><p>爬取流程与代码示意：  </p><p>信息提取代码示意：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412111" alt="" title="" loading="lazy"/></p><h4><a name="t6" target="_blank"/>数据清洗</h4><p>三个城市的原始数据采用相同提取逻辑，因此清洗流程保持一致，以下以赣州数据为例说明核心步骤：</p><ol><li>读取数据并查看基础信息：  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412112" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412113" alt="" title="" loading="lazy"/></li><li>缺失值处理：房屋特色等描述型字段用“无”填充，数值型字段根据分布特征填充均值或中位数。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412114" alt="" title="" loading="lazy"/></li><li>数据格式优化：去除字符型数据中的空格与换行符，删除冗余字段（如“所在区域”因已有“所在市”“所在县区”可剔除）。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412115" alt="" title="" loading="lazy"/></li><li>特征拆分与提取：</li></ol><ul><li>从“面积”字段中提取数字，转换为浮点型用于建模。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412116" alt="" title="" loading="lazy"/></li><li>将“楼层”字段拆分为“楼层高度”（低/中/高）和“总层数”两个独立特征。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412117" alt="" title="" loading="lazy"/></li><li>从“户型”字段中提取“室、厅、厨、卫”的数量，转换为整型特征。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412118" alt="" title="" loading="lazy"/></li><li>简化“抵押信息”字段，归类为“有抵押、无抵押、暂无数据”三类。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412119" alt="" title="" loading="lazy"/></li></ul><h3><a name="t7" target="_blank"/>探索式数据分析</h3><p>通过可视化工具从区域分布、价格特征、房源属性等维度解析数据，以下为三个城市的核心分析结果：</p><h4><a name="t8" target="_blank"/>赣州数据可视化</h4><ul><li>各县区房源数量分布：直观呈现不同区域的房源流通活跃度。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412120" alt="" title="" loading="lazy"/></li><li>房源相关特征分布：展示房屋属性的整体特征，为后续特征筛选提供依据。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412121" alt="" title="" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412122" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412123" alt="" title="" loading="lazy"/></li><li>房价分布直方图：因高价房源占比极低，分高低价（以1000万为界）分别展示，更清晰呈现价格集中区间。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412124" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412125" alt="" title="" loading="lazy"/></li><li>房源标题词云图：提炼市场热门宣传关键词，反映购房者关注焦点，词云图生成代码如下。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412126" alt="" title="" loading="lazy"/>  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412127" alt="" title="" loading="lazy"/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412128" alt="" title="" loading="lazy"/></li></ul><hr/><p><strong>相关文章</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412129" alt="" title="" loading="lazy"/></p><h3><a name="t9" target="_blank"/>Python电影票房预测模型研究——贝叶斯岭回归Ridge、决策树、Adaboost、KNN分析猫眼豆瓣数据</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=DwbQnU43ICiZhGbu4WcX7A%3D%3D.HrEHfY21Ha0MDeWYx2z5BHbmcQw6k7mBxYdWYzSRmJ0%3D" rel="nofollow" title="https://tecdat.cn/?p=43754" target="_blank">https://tecdat.cn/?p=43754</a></p><hr/><ul><li>各县区平均房价地理图：可视化区域价格差异，直观展示核心城区与郊区的房价梯度。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412130" alt="" title="" loading="lazy"/></li><li>数据可视化大屏：整合核心指标（房源数量、均价、户型分布等），全方位展示市场概况。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412131" alt="" title="" loading="lazy"/></li></ul><h4><a name="t10" target="_blank"/>南昌数据可视化</h4><p>南昌市可视化分析逻辑与赣州一致，仅因城市数据差异呈现不同特征，核心可视化结果如下：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412132" alt="" title="" loading="lazy"/></p><h4><a name="t11" target="_blank"/>深圳数据可视化</h4><p>深圳作为一线城市，房价水平与区域差异显著，核心可视化结果如下：  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412133" alt="" title="" loading="lazy"/>  <br/>通过跨城市对比发现：深圳二手房价格显著高于赣州、南昌，且异常值较多；赣州、南昌的房价分布更集中，区域差异主要受核心城区与郊区地理位置影响；三地房源均以刚需户型为主，装修情况以简装、中装为主流。</p><h3><a name="t12" target="_blank"/>模型设计与优化</h3><h4><a name="t13" target="_blank"/>数据合并与特征处理</h4><ol><li>数据集合并：将三个城市的清洗后数据纵向拼接，最终得到42640条记录、31个特征的建模数据集，合并过程代码与结果如下。  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412134" alt="" title="" loading="lazy"/>  <br/>  核心合并代码（修改后关键片段）：</li></ol><pre><code>import pandas as pd# 读取三个城市的清洗后数据ganzhou_data = pd.read_csv('清洗后数据/赣州_清洗后.csv', index_col=False)nanchang_data = pd.read_csv('清洗后数据/南昌_清洗后.csv')shenzhen_data = pd.read_csv('清洗后数据/深圳_清洗后.csv')# 纵向合并数据combined_data = pd.concat([ganzhou_data, nanchang_data, shenzhen_data], axis=0)# 去重并重置索引combined_data.drop_duplicates(inplace=True)combined_data = combined_data.reset_index(drop=True)# 保存合并后数据combined_data.to_csv('./合并后数据集.csv', index=False)</code></pre><ol><li>异常值检测：通过箱线图发现深圳存在较多高价异常值，这些异常值会干扰模型训练，导致预测偏差。<img referrerpolicy="no-referrer" src="/img/remote/1460000047412135" alt="" title="" loading="lazy"/></li><li>合并后数据分布：查看合并数据的直方图，因价格跨度极大，分高低价展示分布特征，为异常值处理提供依据。  <br/>   </li><li>特征筛选与编码：</li></ol><ul><li>剔除标题、挂牌时间等难以量化或相关性低的特征，保留核心特征用于建模。</li><li>对分类特征（如户型结构、装修情况等）采用LabelEncoder编码，编码代码如下。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412136" alt="" title="" loading="lazy"/>  <br/>核心编码代码（修改后关键片段）：</li></ul><pre><code>from sklearn.preprocessing import LabelEncoderimport joblib# 筛选建模特征model_data = combined_data.drop(['标题', '标题详情', '套内面积', ...], axis=1)# 定义需要编码的分类特征cat_features = ['小区名称', '户型结构', '建筑类型', ..., '所在县区']label_encoders = {}# 对分类特征进行编码for feat in cat_features: le = LabelEncoder() model_data[feat] = le.fit_transform(model_data[feat]) label_encoders[feat] = le# 保存编码器joblib.dump(label_encoders, '模型/label_encoders.pkl')</code></pre><ol><li>特征相关性分析：通过热力图筛选与“总价”相关性较高的特征，剔除小区名称、关注人数等相关性较低的特征，优化建模特征集。  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412137" alt="" title="" loading="lazy"/></li></ol><h4><a name="t14" target="_blank"/>模型训练（未处理异常值）</h4><p>采用8:2比例划分训练集与测试集，分别训练决策树（DT）、梯度提升树（GBT）、随机森林（RF）三种模型，使用R²、MSE、RMSE作为评估指标。</p><ol><li><p>决策树模型：  </p><p>决策树预测值与真实值对比：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412138" alt="" title="" loading="lazy"/></p></li><li><p>梯度提升树模型：  </p><p>梯度提升树预测值与真实值对比：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412139" alt="" title="" loading="lazy"/></p></li><li><p>随机森林模型：  </p><p>随机森林预测值与真实值对比：  <br/>  <img referrerpolicy="no-referrer" src="/img/remote/1460000047412140" alt="" title="" loading="lazy"/>  <br/>  未处理异常值时的模型性能：</p></li><li>分析可见：未处理异常值时，模型R²值偏低，高价房源的预测偏差尤为明显，需通过异常值处理与参数优化提升性能。</li></ol><h4><a name="t15" target="_blank"/>模型优化</h4><ol><li><p>异常值处理：采用IQR法则，剔除总价高于Q3+3IQR的异常数据（主要为深圳高价房源），处理代码如下。  </p><p>核心处理代码（修改后关键片段）：</p></li></ol><pre><code># 计算四分位数Q1 = model_data['总价'].quantile(0.25)Q3 = model_data['总价'].quantile(0.75)IQR = Q3 - Q1# 定义异常值边界upper_bound = Q3 + 3 * IQR# 剔除异常值optimized_data = model_data[model_data['总价'] &lt;= upper_bound]optimized_data.to_csv('./训练数据.csv', index=False)</code></pre><ol><li>参数优化：使用网格搜索（GridSearchCV）为各模型寻找最优参数，提升模型泛化能力。</li></ol><ul><li>决策树网格搜索：</li></ul><p>决策树预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412141" alt="" title="" loading="lazy"/></p><ul><li><p>梯度提升树优化：  </p><p>梯度提升树预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412142" alt="" title="" loading="lazy"/></p></li><li>随机森林网格搜索：</li></ul><p>随机森林预测值与真实值对比（优化后）：<img referrerpolicy="no-referrer" src="/img/remote/1460000047412143" alt="" title="" loading="lazy"/></p><ol><li><p>优化后模型性能：</p><table><thead><tr><th>模型</th><th>R²</th><th>MSE</th><th>RMSE</th></tr></thead><tbody><tr><td>决策树</td><td>0.871</td><td>4220.42</td><td>64.96</td></tr><tr><td>梯度提升树</td><td>0.895</td><td>3422.84</td><td>58.50</td></tr><tr><td>随机森林</td><td>0.911</td><td>2928.50</td><td>54.16</td></tr></tbody></table><p>结果表明：剔除异常值并优化参数后，三种模型的预测精度显著提升，其中随机森林模型表现最佳，R²达到0.911，预测值与真实值拟合度良好，能有效捕捉二手房价格的核心影响因素。</p></li></ol><h3><a name="t16" target="_blank"/>结论与服务支持</h3><p>本次研究通过完整的数据挖掘流程，实现了多城市二手房价格预测与区域差异分析，验证了决策树、梯度提升树、随机森林在房价预测场景的有效性。核心结论包括：地理位置（所在城市、县区）、建筑面积、户型结构是影响二手房价格的关键因素；深圳房价整体偏高且波动较大，赣州、南昌房价分布更集中；随机森林模型经异常值处理与参数优化后，预测精度最优，可为市场参与者提供可靠参考。</p><h4><a name="t17" target="_blank"/>核心服务保障</h4><ol><li>应急修复服务：24小时响应“代码运行异常”求助，比学生自行调试效率提升40%，避免因代码问题耽误项目进度。</li><li>人工创作保障：所有代码与论文内容均经人工优化，直击“代码能运行但怕查重、怕漏洞”的痛点，保障原创性与可靠性。</li><li>全流程支持：提供从数据爬取到模型落地的全流程答疑，不仅教会“怎么做”，更解释“为什么这么做”，帮助真正掌握数据分析思维。  <br/>  本文项目完整代码、数据及可视化素材已同步至交流社群，进群即可获取。如需个性化修改、代码调试或润色服务，可联系团队获取一对一支持，让数据分析学习更高效、更省心。</li></ol><h2><a name="t18" target="_blank"/>关于分析师</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412144" alt="" title="" loading="lazy"/></p><p>在此对 Liping Xiao 对本文所作的贡献表示诚挚感谢，其专业方向为数据科学与大数据技术，曾担任北京中电中采数据服务有限公司的数据处理。擅长 Python 编程，在深度学习、数据分析、数据采集等领域具备专业的实践能力与技术储备。</p>]]></description></item><item>    <title><![CDATA[在 RTE2025 大会，我看到了 AI]]></title>    <link>https://segmentfault.com/a/1190000047412192</link>    <guid>https://segmentfault.com/a/1190000047412192</guid>    <pubDate>2025-11-19 19:04:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>10 月 31 日，我们受 RTE 开发者社区邀请参加了 RTE2025 大会。</p><p>这不是寻常的科技展会，没有冰冷的技术参数展示，取而代之的是AI与人类自然交流的场景。</p><p>今年大会以「AI 有声」为主题，这巧妙地道出了行业的变化——<strong>AI正在从无声的工具变为有声的伙伴。</strong> 从能贴心对话的毛绒玩具，到随时陪伴的AI语伴，从智能调研助手到趣味互动桌游，对话式AI已不再是科幻构想，而是触手可及、富有情感的日常存在。</p><p>这股“AI有声”的浪潮背后，是怎样的技术基石在支撑？一个优秀的语音智能体，从灵光一现的Demo到稳定可靠的产品，需要跨越哪些鸿沟？</p><p>为了探寻这些答案，我们不仅深入了专为语音智能体开发者打造的 “<strong>RTE101技术专场</strong>” ，系统梳理从语音前端处理到交互逻辑的核心技术栈；也穿梭于各大展台，亲身体验了如<strong> Chikka.ai </strong>这样的智能调研工具、<strong>ValidFlow.AI</strong> 的用户洞察平台以及 <strong>LookeeAI</strong> 教育硬件等前沿应用。</p><p>今天，我们来分享一下我们的所见所得、所思所想。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412194" alt="" title=""/></p><h2>101技术专场：语音智能体开发者的第一课</h2><p>这场为语音智能体开发者设计的“第一课”从最底层的信号处理一直讲到上层的应用实践，系统性地剖析了一个语音智能体从Demo到产品化需要跨越的鸿沟。</p><p><strong>音频3A处理（AEC、ANS、AGC）是对话式AI的基石。</strong> 声网音频算法工程师林子毅现场演示了回声消除的重要性：当关闭AEC功能时，AI设备会陷入“自问自答”的循环——说出回答后又被自己的声音触发，不断重复响应，根本无法正常交流。而降噪技术则让AI在嘈杂的机场环境中也能准确识别主要说话人的指令。</p><p><strong>语音活动检测（VAD）决定交互体验。</strong> 传统的VAD基于声带振动检测，但对于发轻音或辅音时无效。声网开源的TEN VAD基于深度学习，能更准确地检测语音起止点，将端到端响应延迟控制在毫秒级，避免了“说话被打断”或“说完等几秒没反应”的糟糕体验。</p><p>让我印象比较深刻的是 <strong>关于对话式AI架构的讨论。</strong> 目前主流的三段式架构将ASR、大语言模型、TTS串联，优点是各模块可独立优化，技术成熟度高。但端到端语音大模型直接将语音输入映射为语音输出，减少了信息损失，理论上更接近人类对话方式。</p><p>三段式架构灵活可控，但累积延迟较高；端到端模型响应更快，但训练数据和成本要求极高，且可控性较差。</p><p>在探讨<strong>对话式AI中传输音视频和数据的最佳协议选择</strong> 时，声网生成式AI产品负责人毛玉杰从产品角度指出，技术决策的出发点应该是用户体验而非技术本身。“我们总是在讨论协议的好坏，但核心是这个协议能给用户带来什么体验”。他建议开发者采用混合架构：WebSocket用于信令控制，WebRTC用于媒体传输，各取所长。</p><blockquote>此外，还分享了工具调用与MCP实践、对话式AI体验评估等内容。如果大家感兴趣的话，我们可以单独出一期“101技术专场”的内容完整回顾，欢迎在评论区留言</blockquote><h2>展会亮点：三款产品背后的AI语音革新</h2><p><strong>（1）Chikka.ai：让用户调研会“说话”</strong></p><p>在活动现场展台，我们遇到了这款专注于客户访谈的AI平台。Chikka.ai基于AI语音代理Ava，能与受访者进行自然对话，快速收集分析语音反馈。</p><p>最吸引我的功能是它支持多语言访谈，能同时进行数百次对话。想象一下，企业可以在短时间内完成大规模的用户调研，而且不是通过冰冷的问卷，而是模拟专业访谈者的对话交流。平台还提供个性化访谈计划、自动转录与见解提取，大大缩短了从收集到洞察的周期。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412195" alt="" title="" loading="lazy"/></p><p><strong>（2）ValidFlow.AI：从数据到洞察的智能升级</strong></p><p>相类似的，AI用户洞察应用ValidFlow.AI的展台吸引了不同的人群。</p><p>与Chikka.ai侧重于对话式数据收集不同，ValidFlow.AI更像是一个完整的用户洞察平台，它通过AI研究员和全球用户池的组合，全面高效地完成用户洞察的全流程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412196" alt="" title="" loading="lazy"/></p><p>两者的区别在于：Chikka.ai像是擅长对话的采访者，而ValidFlow.AI则更像是整个研究团队——从设计调研到回收分析，提供更全面的解决方案。现场工作人员表示，他们的目标是让用户洞察不再是大公司的专利，让初创企业也能轻松获得高质量的市场洞察。</p><blockquote>进一步阅读：<a href="https://link.segmentfault.com/?enc=7VqBvtwPvshy5nicElA6Qw%3D%3D.CHtJ3Wk2enmv3aYbapNlUNZJ6akIIMfyH894D65XgpqdIHtFTXoT2jC7BSngzlE2JjWz%2BlAPajEZehX3TYeXqv2fbuWHtuOUyCmrxJLyTxDGm5DB5lPEFON60M2yKHUl%2FOQnQMlV0%2FhT608jt6XVgFsk%2BKWM5NMTSJZJYItyhB3FpF4zECSMl38Yy61F7nRH5COgTQZKDHLoiZz3bN4Kqh7bxEYxcPwsQF7DJtjX2cd81mv6pjwFAxlSl2zXSQnRTzcOx8KOuCxGI%2BJVf5%2BO5remqsRlL6OJkmQBHLFAXo752XQBk9dmpAvElSImsSDTO8GRoX%2F2tKXZHsXk38qUUDzV8nOmYtxLTU6pBTb7O2msQ55N1EwXM1xfowL3qUq9KgczSk%2Bxl09DHoZXCAGD%2FtB0wpTaqLU5ta6ckHG5onpJdSfRkQPDkpeegBjy9ArXifwfI%2FQ7cKuA%2Fr8PZL7WmtQ6TKf4BsDP1zoFwyPYma9oBk8C4kR825bbEVi7Xk4i" rel="nofollow" target="_blank">RTE现场看到的超绝AI调研产品，调研人福音！</a></blockquote><p><strong>（3）Lookee：会互动的英语学习伙伴</strong></p><p>教育硬件展区总是人山人海，盒智科技推出的LOOKEE口语侠尤其受欢迎。这款被称为“全球最小AI英语学习语伴”的产品，大小不过一个耳机盒，重量不到50克，专为5-12岁儿童设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412197" alt="" title="" loading="lazy"/></p><p>他们自研的AURA系统能实时分析孩子的情感状态、内容理解度和对话积极性，并动态调整对话策略。通过情感化交互设计，为孩子打造零压力的专属口语环境，解决孩子们“没处说、不敢说”的痛点。</p><p>起初我觉得这类 AI 教育硬件应该是产品同质化的重灾区，但在现场我发现，一个孩子在与LOOKEE进行英语对话时，摇一摇LOOKEE，LOOKEE不仅会换一个口语话题，甚至还会换一个非常有动感的表情，非常可爱。</p><p>后面LOOKEE团队告诉我，他们做了大量用户调研，挖掘孩子们真正痛点和兴趣点。我觉得这种用心在初创团队上是十分难得的，也是真正打动我的。</p><h2>AI桌游《Talk With》：聊天能解决的事儿</h2><p>大会的互动环节中，最让我惊喜的是全球首款AI主题语言类桌游《Talk With》（中文名：聊天能解决的事儿）试玩会。</p><p>游戏规则颇具创意：3-5名玩家抽取随机场景卡，每个场景都潜含着困难和危机。玩家需要竞拍合适的对话式AI和语音技术，构建自己的技术壁垒，最终通过路演展示产品愿景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412198" alt="" title="" loading="lazy"/></p><p>我参与的一局游戏中，抽到的场景是“解决开车犯困问题”。我们小组竞拍到了情感语音合成技术，设计了一款能通过聊天保持驾驶员清醒的AI助手。游戏中的“市场行情卡”带来了意想不到的转折——突然的“技术突破”让我们的方案更具竞争力。</p><p>这款桌游巧妙地将AI语音技术融入轻松愉快的游戏中，让参与者在40分钟的游玩中理解了不同语音技术的应用场景和组合价值。正如游戏名称所暗示的，有时候聊天真的能解决很多问题。</p><h2>总结：AI语音的未来是更自然的“相处”</h2><p>回顾一天的逛展体验和技术专场的深度学习，我最大的感受是：<strong>AI语音技术正从“工具性”走向“关系性”。</strong></p><p>从会陪伴的芙崽Fuzozo，到能教学的LOOKEE，再到帮助企业洞察用户的ValidFlow.AI，这些产品不再强调技术的强大，而是聚焦于<strong>如何让AI更懂人心</strong>。101技术专场中各位讲师反复强调的一个观点让我深有共鸣：<strong>技术最终要为体验服务。</strong></p><p>当AI学会了倾听与对话，它不再只是执行命令的工具，而成为我们生活中<strong>有温度的伙伴</strong>。这或许就是对话式AI最美好的前景——不是取代人类，而是以更自然的方式与人“相处”，<strong>让技术真正服务于人的情感需求。</strong></p><p>我相信，我们迎来的将是一个更有温度的数字未来。</p><p>最后，再次感谢 RTE 开发者社区邀请我们来参加这次活动，真的非常用心，dev party也让我们交到了不少有意思的新朋友。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412199" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412200" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=NBX%2F7D2aOQwu8RZF8Ah9vg%3D%3D.JOkuAyv%2Fw1RGoiYwaaohjv%2FBbMPXmHqtOikzLcMyOoc%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412201" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[EMR Serverless Stell]]></title>    <link>https://segmentfault.com/a/1190000047412219</link>    <guid>https://segmentfault.com/a/1190000047412219</guid>    <pubDate>2025-11-19 19:03:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在今年云栖大会上，EMR Serverless Stella 1.0正式发布，这是一款面向企业级场景深度优化的高性能数据分析引擎。阿里云开源大数据平台OLAP引擎负责人周康系统性地分享了 Stella 在存算分离架构、Lakehouse 场景以及全文检索等三大核心场景下的深度优化经验，为业界提供了大规模 OLAP 系统工程化实践的宝贵参考。Stella引擎的发布将为企业级用户提供更加专业、高效的OLAP解决方案。</p><h2>站在巨人肩膀上：与 StarRocks 开源社区的深度合作</h2><p>阿里云与StarRocks开源社区的合作可以追溯到2021年，从开源第一天起就建立了深度合作关系。在过去四年中，双方在源码共创、产品发布和技术优化方面积累了丰富的经验。</p><p><strong>合作历程回顾：</strong></p><ul><li><strong>2021年</strong>：开启源码共创，重点推动数据湖分析相关框架和性能优化</li><li><strong>2022年3月</strong>：推出EMR半托管StarRocks形态</li><li><strong>2023年</strong>：响应市场需求，推出全托管产品形态</li><li><strong>2024年</strong>：正式商业化存算分离版本</li></ul><p>随着产品的成熟，阿里云EMR已积累数百家B端企业客户。“我们始终站在巨人的肩膀上，”阿里云开源大数据平台OLAP引擎负责人周康表示，“Stella 所有功能和优化都会逐步回馈给社区，同时确保API层面与开源版本完全兼容。”<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdm6eJ" alt="image.png" title="image.png"/></p><h2>Lakehouse 成为业界共识：Stella 应运而生</h2><p>2024年，阿里云正式发布 OpenLake 方案，标志着 Lakehouse 架构在数据基础设施领域的全面落地：<br/><img width="723" height="273" referrerpolicy="no-referrer" src="/img/bVdm6eL" alt="image.png" title="image.png" loading="lazy"/><br/>2024云栖大会重磅发布OpenLake解決方案，StarRocks 为 OLAP场景核心组件</p><p>伴随这一趋势，Lakehouse（数据湖仓一体）已成为国内外头部公司的业界共识：<br/><img width="723" height="298" referrerpolicy="no-referrer" src="/img/bVdm6eM" alt="image.png" title="image.png" loading="lazy"/><br/>海外Lakehouse发展趋势  Snowflake/Databricks/BigQuery + Iceberg/Delta/Hudi</p><p>阿里云推出了 OpenLake 一体化湖仓解决方案，StarRocks 在其中担任核心 OLAP 引擎角色。然而，在大规模生产环境中，StarRocks 在存算分离架构和湖表查询方面仍有优化空间。Stella 项目正是为了应对这些挑战而生。通过在调度、查询优化、执行引擎和存储引擎四个层面的全面改进，Stella 1.0 针对几十 TB 甚至 PB 级数据场景，解决了事务机制、Compaction 效率、查询性能、元数据管理等一系列生产环境痛点。</p><h2>Stella 1.0 三大核心场景突破</h2><p>EMR Serverless Stella 1.0版本于今年5月正式发布，主要聚焦三大核心技术能力的重大突破：</p><h3>一、存算分离：性能和稳定性大幅提升</h3><p>Stella 1.0 在存算分离架构下实现了三大突破：</p><p><strong>1. 冷查性能大幅提升</strong></p><ul><li>实现 IO 合并，减少对象存储访问次数</li><li>优化 Compaction 调度器，大幅减少小文件数量 </li><li>针对轻量级 ETL 场景优化负载调度</li></ul><p><strong>2. 写入性能保障</strong></p><ul><li>开发 Batch Publish 能力，解决串行化导入瓶颈 </li><li>推出 Collocated PK Index，避免缓存盘和索引盘互相影响 </li><li>优化 FE 侧 Tablet 创建删除效率</li></ul><p><strong>3. 缓存利用率优化</strong></p><ul><li>引入 Index Cache 和 Meta Data Cache，提升元数据访问速度 </li><li>实现自适应 IO Stream，智能选择本地缓存或远端访问 </li><li>针对 ETL 场景优化空间利用</li></ul><p>在TPC-H 10T基准测试中，存算分离版本的Stella相比上一版本<strong>性能提升超过120%</strong>，充分展现了云原生架构的技术优势。<br/><img width="723" height="430" referrerpolicy="no-referrer" src="/img/bVdmZu8" alt="image.png" title="image.png" loading="lazy"/></p><h3>二、Paimon 湖表查询：Co-design 驱动性能飞跃</h3><p>Stella 1.0在Paimon表分析方面，重点聚焦在三个方向的提升：</p><p><strong>1. 数据读写效率提升</strong> </p><ul><li>实现自适应 Batch Size 优化 </li><li>支持Native Paimon Writer，性能大幅提升</li></ul><p><strong>2. 元数据访问优化</strong> </p><ul><li>针对 Manifest 数量众多场景，实现分布式解析能力 </li><li>适配异步 Splits 调度框架 </li><li>优化 Manifest Cache 策略</li></ul><p><strong>3. 深度集成阿里云 DLF 2.x</strong> </p><ul><li>与 Data Lake Formation 产品深度整合 </li><li>借助 DLF 能力提升 Paimon 查询和写入的性能与稳定性 </li><li>针对DV表实现Native读取优化</li></ul><p>Stella在Lakehouse场景下查询Paimon下性能的提升非常明显：<br/><img width="723" height="513" referrerpolicy="no-referrer" src="/img/bVdm6eU" alt="image.png" title="image.png" loading="lazy"/></p><p>虽然 Flink + Paimon 已成为成熟的实时入湖方案，但计算引擎与 Paimon 存储的查询优化结合仍有巨大提升空间。Stella 与 Paimon 将在多个方便持续进行Co-Design，更多优化成果将在后续版本中发布。</p><h3>三、全文检索：打造高性能、高可用的文本分析能力</h3><p>Stella 1.0 正式推出全文检索能力，支持高效、精准的文本查询。</p><ul><li><strong>架构重构</strong>：对 Inverted Index（倒排索引） 整体解决方案进行架构优化</li><li><strong>存算分离主键表支持</strong>：新增主键表全文检索能力，实现高效精准的查询能力</li><li><strong>小文件合并</strong>：解决存算分离架构下的“性能杀手”问题（单个 Segment 产生十几个小文件） <br/><img width="723" height="529" referrerpolicy="no-referrer" src="/img/bVdm6eV" alt="image.png" title="image.png" loading="lazy"/></li></ul><p>文本过滤性能benchmark: Stella vs EMR StarRocks 3.3</p><p>目前，全文检索功能已在阿里集团内部和云上客户中投入使用，所有优化代码已通过 PR 提交至 StarRocks 开源社区。</p><h2>技术创新路线图持续演进</h2><p>面向未来，Stella引擎制定了清晰的技术发展路线图，在四个关键领域持续深耕：</p><ol><li><strong>迈向Stella 2.0时代：轻量 ETL Production Ready</strong>  <br/>全面强化轻量级 ETL 能力，打通从数据接入、转换到分析的端到端链路，使用户无需依赖外部调度系统即可高效完成日常数据加工任务，真正实现“开箱即用、生产就绪”。</li><li><strong>Lake Optimizer：湖表性能全面对齐甚至超越内表</strong>  <br/>推出专为开放数据湖设计的 Lake Optimizer，显著提升 Apache Paimon 等湖表格式的查询性能，让湖表在复杂分析场景中媲美甚至超越传统内表体验。</li><li><strong>智能化 Background Job Service：彻底释放用户运维负担</strong>  <br/>针对企业用户长期面临的内表运维复杂、资源争抢等问题，Stella 将推出智能化后台作业服务，自动处理 compaction、索引构建、统计信息收集等任务，实现高智能化的自治运维，大幅提升系统稳定性与资源效率。</li><li><strong>全文检索与向量检索能力持续提升</strong>  <br/>在已有的高性能 OLAP 基础上，进一步融合全文检索与向量检索能力，支持非结构化与多模态数据的统一分析，为 AI 原生应用、智能搜索等新兴场景提供底层引擎支撑。</li></ol><p>这四大方向不仅体现了 Stella 对 Lakehouse 架构的深度适配，更彰显了其从“高性能分析引擎”向“智能数据平台核心引擎”演进的战略决心。随着这些能力的逐步落地，Stella 将为企业用户提供更开放、更智能、更易用的下一代实时分析体验。</p><h2>技术探索与社区协作深度融合</h2><p>Stella引擎在技术架构探索方面持续深化与开源社区的合作：</p><p>Lakehouse架构能力的持续拓展体现了Stella引擎的前瞻性设计理念。在现有Lakehouse架构基础上，系统将支持更多检索功能，为企业的多元化分析需求提供全面支持。向量搜索技术是与Apache Paimon深度集成的创新探索，在AI和大数据时代，向量搜索能力将成为差异化的技术优势。</p><h3>开源社区贡献亮点</h3><ul><li><strong>JSON等半结构化数据处理能力持续增强</strong>，推动整个生态发展</li><li><strong>大规模场景技术实践经验分享</strong>，为社区贡献宝贵技术智慧</li><li><strong>与Apache Paimon团队深度技术合作</strong>，确保生态整合持续优化</li><li><strong>所有优化方案回馈开源社区</strong>，推动开源生态系统发展进步</li></ul><p>开源社区的深度贡献体现了Stella团队的技术责任感和开放合作精神。JSON等半结构化数据处理能力的持续增强将推动整个生态的发展，为企业在数字化转型过程中处理多样化数据提供更强支持。大规模场景下的技术实践经验分享不仅展示技术实力，更为社区贡献了宝贵的技术智慧。</p><p>“我们不仅要在云上提供增值服务，更要推动整个开源生态的发展，”周康强调，“通过深度参与开源社区，确保所有用户都能从技术进步中受益。”</p><h2>技术意义与未来规划</h2><p>EMR Serverless Stella 1.0的发布标志着阿里云在湖仓一体技术领域达到新的里程碑，为用户提供从数仓加速、湖仓查询到全文检索的全方位OLAP能力支持。该版本不仅解决了企业在实际生产环境中遇到的关键技术挑战，更通过持续的技术创新和社区贡献，推动了整个StarRocks生态系统的发展。</p><p>未来，Stella将继续围绕Lakehouse架构演进，在缓存调度、查询优化、存储引擎和写入能力等核心领域持续创新，为企业数字化转型提供更加强劲的技术引擎。</p>]]></description></item><item>    <title><![CDATA[普通人如何免费使用Gemini3-Pro]]></title>    <link>https://segmentfault.com/a/1190000047412226</link>    <guid>https://segmentfault.com/a/1190000047412226</guid>    <pubDate>2025-11-19 19:02:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>大模型的用法主要分为以下两类：</p><ol><li><strong>个人用途</strong>：作为日常知识补充、内容创作及辅助工具。</li><li><strong>商业用途</strong>：集成至 Dify、Coze 等工作流与应用程序中，或在编程工具及第三方客户端（如 Cherry Studio）中使用。</li></ol><p>目前，<strong>普通用户几乎可以免费使用各类主流大模型</strong>，Google 最新推出的 Gemini 3 Pro 也不例外。</p><h2>使用方法</h2><ol><li><strong>访问 AI Studio 官网</strong>：<a href="https://link.segmentfault.com/?enc=S2Plhp%2B4YOdkQOOArN%2Bm%2Bw%3D%3D.b2OqHiM6sFkooR4UZlwi5lnyC0TdolmhvZFOP4uUBYY%3D" rel="nofollow" target="_blank">https://aistudio.google.com/</a></li><li><strong>注册并登录 Google 账号</strong></li></ol><p>登录后，即可免费通过 Gemini 3 Pro 进行对话，或完成代码生成任务。</p><p>此外，Google 还提供了其他免费模型，支持图片生成与编辑、音视频制作，甚至播客生成功能。</p><p>例如，生成图片可以使用 Google 的 Imagen，视频生成则可以使用 Veo 等模型。</p><h2>视频演示</h2><p><a href="https://www.bilibili.com/video/BV14ByjBqEAS/" target="_blank">https://www.bilibili.com/video/BV14ByjBqEAS/</a></p><h2>对话演示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412228" alt="" title=""/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412229" alt="" title="" loading="lazy"/></p><h2>图片生成</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412230" alt="" title="" loading="lazy"/></p><p>视频与音频的生成方式与此类似，此处不再赘述。</p><h2>代码生成（App / 网站 / 小游戏开发）</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412231" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412232" alt="" title="" loading="lazy"/></p><h2>小结</h2><p>本文简要介绍了 Google Gemini 3 Pro 的免费使用渠道，及其在文本对话、代码编写及多模态生成方面的核心功能。</p><blockquote>本文已收录到我的技术小站 <a href="https://link.segmentfault.com/?enc=twc7Uo1MLlsctnbVQIrUvg%3D%3D.kJLSzWOQtvCpaXhkutX4uR%2BJCEv%2F3UQ2vlDo4n0w1HY%3D" rel="nofollow" target="_blank">www.javacn.site</a>，其中包含的内容有：Spring AI、Spring AI Alibaba、LangChain4j、Dify、Coze、N8N、智能体（AI Agent）、MCP、Function Call、RAG、向量数据库、Prompt、多模态、向量数据库、嵌入模型、AI 常见面试问题等内容。</blockquote>]]></description></item><item>    <title><![CDATA[Blender 5.0 (Linux, ]]></title>    <link>https://segmentfault.com/a/1190000047412272</link>    <guid>https://segmentfault.com/a/1190000047412272</guid>    <pubDate>2025-11-19 19:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Blender 5.0 (Linux, macOS, Windows) - 开源 3D 创意软件 (渲染 建模 雕刻)</p><p>Open-Source 3D Graphics App Introduces Experimental Vulkan Backend</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=%2FkLbUuDgvadeGAqxkqsA9g%3D%3D.YlG8qYs%2BN%2Fk3BVwmgPXvw35IqgOvV2TMVInhe5SCwfk%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=EwEU1XUK1d2o%2BKPuV%2FCuKA%3D%3D.Zk2XBXTOzuYYifoQW0eIUH205l0tuRdlN%2BmabjiZERI%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412274" alt="Blender" title="Blender"/></p><h2>创造的自由</h2><p>Blender 获得 GNU GPL 许可，由其贡献者拥有。</p><p>因此，Blender 永远是免费和开源软件。</p><p>✅ <strong>使命</strong></p><p>以免费/开源软件的形式将世界上最好的 3D CG 技术交到艺术家手中。</p><p>✅ <strong>愿景</strong></p><p>每个人都应该自由地创作 3D CG 内容，拥有自由的技术和创意制作手段以及自由的市场准入。</p><p>✅ <strong>组织机构</strong></p><p>Blender 基金会 (2002) 是一个独立的公益组织。其衍生公司 Blender Institute (2007) 是基金会的办公室所在地，目前拥有 24 名员工，负责 Blender 软件和创意项目，以在生产环境中验证和测试 Blender。</p><p>2020 年，该研究所拆分为两家公司；Blender Institute 现在仅作为 Blender 基金会的工作公司，新的 Blender Studio 将为 Blender 制作内容和测试生产流程的使命做出贡献。</p><p>这些组织支持 <a href="https://link.segmentfault.com/?enc=Knd2Ia9jm6KfmoJuOoNkig%3D%3D.bkWThFZ7Nydsbk8bdYS5LhNB2ByMs9viz795LktpaEo%3D" rel="nofollow" target="_blank">Blender.org</a> 上的贡献者社区。这就是 Blender 的诞生地。</p><p>✅ <strong>软件</strong></p><p>Blender 是免费的开源 3D 创作套件。它支持整个 3D  管道——建模、绑定、动画、模拟、渲染、合成和运动跟踪，甚至视频编辑和游戏创建。高级用户使用 Blender 的 Python 脚本 API  来定制应用程序并编写专门的工具； 通常这些都包含在 Blender 的未来版本中。Blender 非常适合个人和小型工作室  (sysin)，他们可以从其统一的流程和响应式开发流程中受益。<a href="https://link.segmentfault.com/?enc=En5rTIfoW%2FQ7BzI8SmRWjQ%3D%3D.eGyO738F9xnhF%2BrcgFB4kjXsbxZvkQfZ4%2BLKHYtlGQbWIPUz1VTjfklt4rWQBu2w" rel="nofollow" target="_blank">特性展示</a> 中提供了许多基于 Blender 的项目的示例 。</p><p>Blender 是跨平台的，在 Linux、Windows 和 Macintosh 计算机上运行得同样好。其界面使用 OpenGL 来提供一致的体验。为了确认特定的兼容性，<a href="https://link.segmentfault.com/?enc=D3ERtV09ccgr0OjYeMIiAQ%3D%3D.DhB%2B1EPnPPF2VgN8kwCqu8171C587zgMRxJ8CkqT3UdPemvjIUWfK1jRnhIBwzrn" rel="nofollow" target="_blank">支持的平台列表</a> 显示了开发团队定期测试的平台。</p><p>作为 GNU 通用公共许可证 (GPL) 下的社区驱动项目，公众有权对代码库进行大大小小的更改，从而带来新功能、响应性错误修复和更好的可用性。Blender 没有价格标签，但您可以投资、参与并帮助推进强大的协作工具：Blender 是您自己的 3D 软件。</p><p>随时欢迎更多帮助！从开发和改进 Blender 到编写文档等，您可以做很多不同的事情来参与其中。</p><p>✅ <strong>隶属关系</strong></p><p>Blender 基金会是 Open Invention Network、Khronos、Linux Foundation 和 Academy Software Foundation 的成员。</p><p>✅ <strong>许可证</strong></p><p>Blender 是免费软件。您可以 <strong>目的自由</strong> 使用 Blender <em>出于任何</em>，包括商业或教育目的。了解有关 <a href="https://link.segmentfault.com/?enc=PXTyNoc%2FjC6QB6e7Irq7bw%3D%3D.jzKKy5UYFgtnq7AyFAU1vo8jrAtT7wYgKHChhUj8PlHSesSI2pp%2F7anw0hRoL2qT" rel="nofollow" target="_blank">许可证的</a> 更多信息。</p><h2>下载地址</h2><p>Blender 4.5 LTS for macOS, Linux, Windows (2025-11-18)</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=vOUGTHAKxz7x9F3AznLlBw%3D%3D.EpaD66jhsleZahvOeaxqOANn5tNb0uiGIfQr9awSQ9o%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a></li><li>Blender 4.5.5 LTS for macOS x64 (Intel 处理器) (blender-4.5.5-macos-x64.dmg)</li><li>Blender 4.5.5 LTS for macOS ARM64 (Apple 芯片) (blender-4.5.5-macos-arm64.dmg)</li><li>Blender 4.5.5 LTS for Linux x64 (blender-4.5.5-linux-x64.tar.xz)</li><li>Blender 4.5.5 LTS for Windows x64 (blender-4.5.5-windows-x64.msi)</li><li>Blender 4.5.5 LTS for Windows ARM64 (blender-4.5.5-windows-arm64.msi)</li></ul><p>Blender 5.0 for macOS, Linux, Windows (2025-11-18)</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=F2CJO3DcCTk6qSbLWXccIg%3D%3D.SbGe5LIrB5SUintJchVXKdlzXjw6legpDxW2dH7T6FQ%3D" rel="nofollow" target="_blank">https://sysin.org/blog/blender/</a></li><li>Blender 5.0 for macOS x64 (Intel 处理器) (blender-5.0.0-macos-x64.dmg)</li><li>Blender 5.0 for macOS ARM64 (Apple 芯片) (blender-5.0.0-macos-arm64.dmg)</li><li>Blender 5.0 for Linux x64 (blender-5.0.0-linux-x64.tar.xz)</li><li>Blender 5.0 for Windows x64 (blender-5.0.0-windows-x64.msi)</li><li>Blender 5.0 for Windows x64 (blender-5.0.0-windows-arm64.msi)</li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=8Ze8RN5jmSij6Vo32P9kww%3D%3D.itFc45Dq4wHQizC04z3XHJ317O0wmTJOimK%2FPAaamic%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[AI起飞到这份上了，普通人还能赶上这波风]]></title>    <link>https://segmentfault.com/a/1190000047412285</link>    <guid>https://segmentfault.com/a/1190000047412285</guid>    <pubDate>2025-11-19 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今年只要一聊职业规划，大家最爱问的就是三句话：“AI这么火，我现在转行还来得及吗？”<br/>“我是理工生，计算机0基础也能进AI行业吗？”<br/>“学完Python就能当算法工程师吗？”别急，我们先把“热度”说清楚，再告诉你：谁能吃到红利、要学什么、怎么拿到offer。<br/>01｜AI行业到底有多火？一组新数据扎心了<br/>猎聘大数据研究院最新发布的《2025上半年人才供需洞察报告》给出了极具冲击力的答案：AI技术岗整体需求增长36.82%。<br/>其中——家电行业的AI技术职位同比暴增277.43%！这意味着什么？<br/>AI不再是互联网大厂的“专属”，已经渗透进家电、制造、软件、零售几乎所有行业。<br/>这也是为什么你身边的洗衣机、扫地机、空调都越来越“会思考”——智能家居正在成为新一轮AI人才需求爆发的真正推手。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412287" alt="图片" title="图片"/><br/>更扎心的来了：<br/>AI技术岗50万以上年薪的职位占比高达31.03%。<br/>几乎是全行业（6.40%）的5倍。深度学习岗里，这个比例更是高达39.54%。<br/>市场的态度很直接：<br/>会AI的人，不够用。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047412288" alt="图片" title="图片" loading="lazy"/></p><p>02｜但别误会：AI不是“博士专属领域”<br/>很多人以为AI行业等同于：“数学大神+清北硕博+顶尖竞赛”。<br/>其实完全不是。目前企业真正缺的是三类人：<br/>①有项目能力的技术岗（算法/深度学习/机器学习）增长最快的是算法工程师岗（同比+50%），企业不是看你学历，而是看你能不能把模型训好、效果调优。<br/>②懂AI逻辑的产品、运营、数据岗专业不限制，关键是能和工程师对话，知道模型能做什么、不能做什么。<br/>③“交叉型人才”（最稀缺）比如懂医疗的AI顾问、懂制造的AI应用经理、懂语言能力的AI训练师。<br/>这些岗位目前缺口都在10万量级。所以文科、跨专业、转行的人，反而更有机会切入AI的落地场景。</p><p>03｜热门岗位怎么选？5类路径，一条总有适合你<br/>1）算法工程师（高薪核心岗）做模型训练、调优、优化。<br/>适合数学/计算机基础好、希望冲高薪的人。<br/>2）AI产品经理（需求越来越大）负责把“AI能力”变成“能使用的产品”。<br/>适合逻辑好、懂业务、有产品思维的人。<br/>3）数据分析/数据标注（入门友好）做数据清洗、标签标注，是所有模型的基础工作。<br/>适合刚入门、希望低门槛进入行业的人。<br/>4）AI训练师（新兴热门）训练大模型的风格、纠错能力、推理能力。<br/>适合语言类、教育类、沟通能力好的同学。<br/>5）行业垂直AI顾问（未来最吃香）把AI落地到医疗、金融、制造等场景。<br/>适合行业背景+AI基础的复合型人才。</p><p>04｜应届生和转行党最关心的三个问题，直接说结论<br/>❶“现在转行AI还来得及吗？”不仅来得及，现在是正当时。<br/>AI技术岗需求还在增长区间（增幅超36%），人才缺口大、工资高、流动快。<br/>❷“非科班能进AI吗？”能。<br/>企业看的是“你能不能解决问题”。<br/>只要补齐基础技能+项目经验，有大量岗位愿意给机会。<br/>❸“学Python就能当算法工程师吗？”当然不行。<br/>Python只是最基础的工具，真正的算法岗需要的是一整套系统能力换句话说：Python是敲门砖，但不是通行证。<br/>不过——<br/>如果你的目标只是进入AI行业，并不一定非要走最难的“算法那条线”。<br/>产品、运营、训练师、行业顾问等岗位同样需要AI思维，也有成熟的学习路径，更适合转行和非科班的同学。</p><p>05｜最后：AI不是神话，是一场“技能革命”<br/>当智能家电、工业AI、AI办公、AI客服全面落地，企业已经从“要不要AI”，变成——“有没有懂AI又懂业务的人？”<br/>接下来3–5年，AI技术岗会继续贵，继续涨，继续难招。最有机会的人是：会AI+懂行业+能落地的复合型人才。<br/>如果你正在找方向、准备转行、想提升竞争力，AI行业依然是这几年少有的“确定性增长赛道”。<br/>现在入场的人，将站在行业的黄金红利期。<br/>现在还在观望的人，大概率会错过下一轮职业跃迁窗口。</p>]]></description></item><item>    <title><![CDATA[普林斯顿大学数据库遭黑客攻击 信息疑似泄]]></title>    <link>https://segmentfault.com/a/1190000047411911</link>    <guid>https://segmentfault.com/a/1190000047411911</guid>    <pubDate>2025-11-19 18:12:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>据彭博社消息，美国时间11月16日，普林斯顿大学进修办公室一系统在11月10日遭到网络攻击。该系统中存有姓名、联系方式以及捐款记录等诸多敏感信息，数据已全部泄露。校方表示黑客利用钓鱼攻击手段，获取了一名员工的账户权限，从而顺利侵入学校办公系统，目前已经成功阻断非法入侵。</p><p>无独有偶，近期已经有多所常春藤高校遭遇网络攻击：宾夕法尼亚大学、哈佛大学和哥伦比亚大学的校园信息系统均遭到入侵，学生信息以及校友数据都被泄露。其中，哥伦比亚大学遭遇的网络攻击最为严重，约87万人信息遭到泄露。JoySSL安全总监指出，普林斯顿大学作为全球顶尖学府，尚且发生规模和影响如此严重的数据泄露事故，可见组织机构的数据防护工作依然任重道远。在数据库层面部署数字证书，提升安全防护等级，已成为防范网络攻击的重要手段。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6ae" alt="" title=""/></p><p><strong>数据泄露源于加密工作缺失</strong></p><p>根据安全专家对此次泄露事件的分析，网络黑客很可能利用了普林斯顿大学在数据存储和传输方面的加密漏洞，从而顺利侵入系统获取重要信息。事实上，很多知名高校或者平台，都在网络安全防护建设上存在认知误区，认为普通的边界防护足以守护信息安全。但实际上，数据库与应用程序之间的数据传输通道极容易遭受网络攻击，普通的防护技术难以有效抵御各种攻击手段。</p><p>自2023年以来，全球高等教育机构遭遇网络攻击和信息泄露的事件已经超过30起，绝大多数泄露事件均因数据库加密不足所致。JoySSL市场部专家表示，数据库系统信息遭窃主要出现在教育和医疗等领域，然而，由于认知缺陷，超过三成的医疗机构和教育平台依旧未部署SSL加密，导致数据泄露事件频频上演。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6af" alt="" title="" loading="lazy"/></p><p><strong>SSL证书提供专业解决方案</strong></p><p>网络安全形势日益严峻，采用有效的加密防护方案迫在眉睫。JoySSL作为专业的数字安全服务商，旨在利用SSL证书在安全领域的专业性能，为数据库提供全链路传输加密，确保从应用服务器到数据库的每一次请求或数据交换都经过高强度加密。基于SHA384算法签发的数字证书可有效保障数据安全，即使被截获也能够让攻击者无法获取有效信息。</p><p><strong>专业防护助推企业线上发展</strong></p><p>越是复杂的网络环境，传输安全风险越大，尤其涉及到云计算，数据库与应用服务器通常分布在不同的网络区域，很容易受到攻击。SSL证书的专业防护性能让攻击者在加密层无功而返，确保数据传输的安全性。某电商平台在部署数字证书后，受到的网络攻击大幅缩减，数据传输安全性提升超过70%。</p><p><img width="723" height="549" referrerpolicy="no-referrer" src="/img/bVdm6ai" alt="" title="" loading="lazy"/></p><p><strong>构建安全防护体系防患于未然</strong></p><p>有关专家指出，随着全球范围内网络安全威胁现象频频出现，组织机构应该率先对自己进行全方位的安全评估，检查所有可能存在的安全漏洞，积极部署SSL证书以保障数据安全传输与存储，配合实时监测机制，建立起有效的安全防护体系，真正做到防患于未然。</p>]]></description></item><item>    <title><![CDATA[国防航天领域的智慧指挥新引擎 图观 ]]></title>    <link>https://segmentfault.com/a/1190000047411917</link>    <guid>https://segmentfault.com/a/1190000047411917</guid>    <pubDate>2025-11-19 18:11:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在国防航天领域，面对日益复杂的任务环境和瞬息万变的战场态势，如何实现高效、精准的指挥决策与运维管理，一直是行业关注的焦点。传统的信息系统往往存在数据孤岛、响应滞后等问题，难以满足现代国防航天任务对实时性、协同性和智能化的高要求。而今，随着数字孪生技术的成熟，一种全新的智能运营解决方案正悄然改变这一局面。孪易数字孪生IOC ProMAX版，作为一款集全场景监控、智能分析、应急协同于一体的平台，正为国防航天领域注入新的智慧动力。</p><h2>全场景一体化监控：从虚拟到现实的无缝映射</h2><p>在国防航天任务中，无论是卫星发射、飞行器测试，还是基地运维，都需要对多地点、多维度数据进行实时监控。孪易通过其基础控制模块，支持多地点切换、场景剖分和环境仿真，帮助用户构建高精度的数字孪生模型。例如，在卫星发射任务中，用户可以通过平台实时模拟发射场环境，结合历史回放功能，回溯关键时间点的数据变化，快速定位问题根源。这种全场景一体化监控能力，不仅提升了运维效率，更让指挥人员能够“透视”复杂系统，实现从虚拟到现实的无缝映射。<br/><img width="640" height="314" referrerpolicy="no-referrer" src="/img/bVdmQxT" alt="" title=""/></p><h2>智能应急与协同处置：应对突发事件的“智慧大脑”</h2><p>国防航天任务中，突发事件如设备故障、环境异常等，往往需要快速响应和跨部门协同。孪易IOC ProMAX版的应急处突模块，通过数字化预案管理和任务全过程监控，实现了资源的智能调度与多方联动。例如，在航天器发射过程中，若监测到异常数据，系统可自动触发应急预案，分配任务给相关团队，并通过视频会商功能一键发起跨部门会议，确保问题在最短时间内得到解决。这种智能协同机制，不仅提升了应急响应的效率，还降低了人为失误的风险，成为国防航天领域的“智慧大脑”。</p><h2>AI驱动的智能运维：从被动响应到主动预警</h2><p>人工智能技术的融入，让孪易IOC ProMAX版在国防航天应用中更具前瞻性。其智能助理模块支持自然语言交互，用户只需通过语音或文本指令，即可操作孪生体对象或查询关键数据，大幅降低了系统操作门槛。同时，智能分析功能基于历史数据进行趋势预测，并结合视觉识别技术，对视频流中的目标进行检测与行为标记。例如，在航天基地安防监控中，系统可自动识别异常入侵行为并发出预警，实现从“被动响应”到“主动防控”的升级。这种AI驱动的运维模式，不仅提升了系统的智能化水平，还为国防航天任务提供了更可靠的安全保障。</p><h2>高度可定制与扩展性：灵活适配复杂业务场景</h2><p>国防航天领域的业务需求多样且复杂，孪易通过零代码与低代码开发模式，为用户提供了高度可定制的扩展能力。零代码平台允许用户通过拖拉拽方式快速配置页面与交互逻辑，而低代码模式则支持基于JavaScript的深度开发，满足个性化业务需求。此外，平台兼容多种数据源与建模工具，如3DMax、Revit和GIS数据，确保系统能够无缝集成现有业务系统。这种灵活性，让国防航天用户能够根据任务演进，持续优化系统功能，避免因技术迭代带来的重复投入。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmmM0" alt="" title="" loading="lazy"/></p><h2>全平台适配与成熟落地：经得起实战检验的解决方案</h2><p>孪易具备多端自适应能力，针对指挥中心大屏、桌面中屏与移动端分别优化交互体验，确保用户在任何场景下都能高效操作。更重要的是，该平台经过近20年、上千个项目的行业验证，在华为、京东等企业的实际应用中积累了成熟经验。在国防航天领域，这种经过实战检验的稳定性与可靠性，尤为关键。无论是日常运维还是重大任务保障，孪易都能快速适配复杂环境，为用户提供持续、稳定的技术支持。</p><p>数字孪生技术正逐步成为国防航天领域智能化转型的核心驱动力。孪易数字孪生IOC ProMAX版通过全场景监控、智能分析、应急协同与灵活扩展等核心功能，为国防航天用户提供了一站式的智慧运营解决方案。它不仅提升了指挥决策的精准性与效率，更通过AI与大数据技术，实现了从被动响应到主动预警的跨越。未来，随着技术的不断演进，数字孪生平台必将在国防航天领域发挥更大价值。<br/>数字孪生智能运营中心已不再是遥远的概念，而是切实助力国防航天领域提升效能与安全性的实用工具。无论是日常运维还是应急响应，孪易IOC ProMAX版都能以其实时性、智能化和灵活性，成为国防航天任务的“智慧伙伴”。</p>]]></description></item><item>    <title><![CDATA[从零到一，解锁城市治理数字孪生高效开发秘]]></title>    <link>https://segmentfault.com/a/1190000047411920</link>    <guid>https://segmentfault.com/a/1190000047411920</guid>    <pubDate>2025-11-19 18:11:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名数字孪生应用开发者，我深知在城市治理项目中，我们常常面临这样的挑战：如何快速构建一个既宏观又精细的三维城市场景？如何让业务数据与三维场景无缝联动？如何在有限的开发资源下，实现跨平台、多终端的应用适配？今天，我想和大家分享一些我在实际项目中总结出的高效开发技巧，希望能为你的城市治理数字孪生项目带来启发。</p><h2>一、快速搭建城市级场景底图，让项目赢在起跑线</h2><p>城市治理项目往往需要宏观的城市背景作为支撑。传统方式下，我们需要手动收集地理信息数据，再通过专业建模软件一点点构建，耗时耗力。现在，通过图观数字孪生平台的“端渲染城市生成插件”，我们可以一键导入全国近300个城市的基础数据，快速生成三维城市场景。<br/><strong>使用技巧</strong>：在项目初期，建议先使用平台的预设风格模板快速搭建场景原型，这样可以在最短时间内向客户展示效果。待方案确认后，再通过深度自定义功能，精细调整建筑、道路、水系等元素的视觉效果。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmUOz" alt="" title=""/></p><h2>二、精细化场景编辑，让每个细节都“活”起来</h2><p>有了宏观的城市底图，接下来就是让场景更加生动逼真。图观的“端渲染场景编辑器”支持导入各类主流格式的精细模型，更重要的是，它提供了强大的材质编辑和动态效果配置能力。<br/><strong>材质渲染技巧</strong>：利用平台的14层PBR物理材质渲染能力，我们可以为不同的建筑类型设置不同的材质属性。比如，政府建筑可以使用更庄重的材质，商业区建筑可以设置更鲜艳的色彩，通过调节金属度和粗糙度参数，让建筑群在阳光下呈现出真实的质感。<br/><strong>动态效果配置</strong>：城市治理中经常需要展示突发事件的处理过程。通过“关节编辑”和“动画编辑”功能，我们可以将外部数据与模型状态进行绑定。比如，当某个区域发生交通拥堵时，对应的模型可以自动变红警示；通过“粒子系统”，我们可以模拟火灾现场的烟雾效果，为应急指挥提供更直观的展示。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmUPX" alt="" title="" loading="lazy"/></p><h2>三、零代码快速搭建，让业务专家也能参与应用开发</h2><p>在城市治理项目中，业务逻辑往往比较复杂。图观的“零代码应用编辑器”让我们可以通过拖拽式操作，快速集成三维场景、二维图表和业务数据。<br/><strong>交互联动技巧</strong>：利用平台的“参数”机制，我们可以轻松实现多维数据的联动分析。比如，当用户在二维图表中点击某个街道的统计数据时，三维场景会自动聚焦到对应的区域，并高亮显示相关的建筑群。这种交互效果过去需要大量编码实现，现在通过简单配置就能完成。<br/><strong>多终端适配</strong>：城市治理应用需要在指挥中心大屏、领导平板、巡查人员手机等多个终端上使用。通过零代码编辑器的多设备自适应功能，我们可以为不同终端定制专属的页面布局，确保每个使用者都能获得最佳体验。</p><h2>四、低代码深度定制，满足复杂业务需求</h2><p>对于需要高度定制化的项目，图观的“低代码统一开发API”提供了最大的灵活性。这套API最大的优势在于“统一API，双核渲染”的设计理念。<br/><strong>开发技巧</strong>：在项目开始前，建议先评估客户端的硬件条件。如果客户端显卡性能较好，可以选择“端渲染”模式以获得更好的视觉效果；如果需要支持大量用户并发访问，可以选择“流渲染”模式。重要的是，无论选择哪种模式，我们只需要维护一套代码，这大大提升了开发效率。<br/><strong>API调试技巧</strong>：善用平台提供的“API调试器”，可以在真实场景中实时调试代码。我习惯先在调试器中验证核心功能逻辑，确认无误后再集成到项目中。调试器还支持将标绘数据直接生成API代码，这个功能在开发地图标注相关功能时特别实用。</p><h2>五、资产库的妙用，加速项目进度</h2><p>图观平台预置了上万的模型、材质和特效资源，这些都是我们可以直接利用的宝贵财富。<br/><strong>使用建议</strong>：在项目启动阶段，先浏览平台的案例库和资产库，往往能找到可以直接复用或稍作修改就能使用的资源。特别是在开发智慧交通、智慧安防等常见城市治理场景时，平台提供的行业模板能帮我们节省大量前期准备工作时间。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmXgO" alt="" title="" loading="lazy"/></p><h2>实战心得：让技术真正服务于业务</h2><p>经过多个城市治理项目的实践，我最大的体会是：技术最终要服务于业务需求。图观平台的价值不仅在于提供强大的技术能力，更在于它降低了数字孪生应用的门槛，让开发团队能够更专注于业务逻辑的实现，而不是陷在技术实现的细节中。<br/>记得在一个智慧城管项目中，我们利用平台的零代码能力，让业务专家直接参与到了应用配置过程中。他们根据自己的工作经验，配置出了最符合实际工作流程的交互逻辑，这在传统开发模式下是难以实现的。<br/>另一个智慧交通项目中，我们利用统一API的双渲染能力，为指挥中心提供了高清大屏版本，同时为路面巡查人员提供了手机端轻量版本，一套代码满足多种使用场景，大大降低了项目的开发和维护成本。</p><p>数字孪生技术正在深刻改变城市治理的模式，而一个好的开发平台能让这个过程事半功倍。图观数字孪生平台通过其全流程、低门槛的特性，为我们开发者提供了强大的技术支撑。无论是快速原型搭建，还是深度定制开发，都能找到合适的解决方案。<br/>数字孪生的世界很大，让我们一起探索更多可能。</p>]]></description></item><item>    <title><![CDATA[Studio 3T 2025.21 发布]]></title>    <link>https://segmentfault.com/a/1190000047411944</link>    <guid>https://segmentfault.com/a/1190000047411944</guid>    <pubDate>2025-11-19 18:10:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Studio 3T 2025.21 (macOS, Linux, Windows) - MongoDB 的终极 GUI、IDE 和 客户端</p><p>The Ultimate GUI, IDE and client for MongoDB</p><p>请访问原文链接：<a href="https://link.segmentfault.com/?enc=fBT04%2BVHF9pesSn1FIK8sA%3D%3D.Gp5P8gHAbvCq3psKBF1Tuc1lozADSNjMuSUq3J%2FmSs0vIZx2Y0L14TXWj2hRJqT1" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a> 查看最新版。原创作品，转载请保留出处。</p><p>作者主页：<a href="https://link.segmentfault.com/?enc=%2F7QwSo9E%2B7lcqr5BJ0op5g%3D%3D.yNAqDPPT7Ra%2FnpE%2FXjOqvaZDIE0hHEIP%2BnavsxIwGM8%3D" rel="nofollow" target="_blank">sysin.org</a></p><hr/><p>Studio 3T，MongoDB 的终极 (卓越、非凡) GUI、IDE 和 客户端</p><p>适用于 MongoDB 的所有 IDE、客户端和 GUI 工具 —— 在 Atlas 上或任何地方。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432404" alt="sysin" title="sysin"/></p><p>MongoDB 的强大工具。</p><p>超过 100,000 名开发人员和数据库管理员使用 Studio 3T 作为他们首选的 MongoDB GUI</p><h2>MongoDB 客户端、GUI 与 IDE</h2><p>那么 Studio 3T 到底是什么？ 在这里，我们解释了它戴的许多帽子中的三个。</p><ul><li><p><strong>Studio 3T 作为 MongoDB 客户端</strong></p><p>客户端是允许您连接到服务器的软件程序或应用程序。尽情使用 Studio 3T 的连接管理器，根据需要连接到尽可能多的 MongoDB 服务器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432411" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB GUI</strong></p><p>图形用户界面 (GUI) 完全按照它说的去做。它提供了一个带有图形菜单、图标、对话框、向导和其他可视元素的用户界面。使用 MongoDB  GUI 的替代方法是使用 mongo shell，尽管 Studio 3T 仍然有  IntelliShell——一个易于导航的内置版本——当你需要的时候。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432412" alt="sysin" title="sysin" loading="lazy"/></p></li><li><p><strong>Studio 3T 作为 MongoDB IDE</strong></p><p>集成开发环境 (IDE) 将应用程序和数据库开发的许多方面整合到一个功能齐全的 “工作室” 环境中 (sysin)。Studio 3T  正是通过提供一个 GUI 来做到这一点，该 GUI 的编辑器具有自动完成和语法突出显示、内置 JSON  验证、七种语言的自动查询代码生成以及许多其他功能，可帮助您更快地工作并节省时间。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000044432413" alt="sysin" title="sysin" loading="lazy"/></p></li></ul><h2>新增功能</h2><p><strong>2025.21.</strong>（2025 年 11 月 18 日）</p><p><strong>新增功能</strong>：</p><ul><li>IntelliShell - 为破坏性 IntelliShell 命令添加了确认提示，以防止意外数据丢失。</li></ul><p><strong>改进</strong>：</p><ul><li>OIDC - 添加了访问令牌获取的超时和中断处理，防止在忽略身份提供者（IDP）登录时线程无限期阻塞。</li></ul><p><strong>修复</strong>：</p><ul><li>偏好设置 - 修复了从许可故障排除选项打开网络代理设置时导致应用程序崩溃的问题。</li><li>连接管理器 - 修复了 X.509 认证设置中不活动的 SSL 标签链接，现在可以正常打开 SSL 标签。</li></ul><h2>下载地址</h2><p>Studio 3T 2025.21, released 2025-11-18</p><ul><li>请访问：<a href="https://link.segmentfault.com/?enc=2TuScuCDc9Z9lPb%2F2DCNsA%3D%3D.9SV%2BkxGju9hUeI71Mi7i4KndLTC168LKNMFUsBLs%2BCp2jO2wjjYACN98ITW92GAr" rel="nofollow" target="_blank">https://sysin.org/blog/studio-3t/</a></li><li>Studio 3T for macOS x64：<code>Studio-3T-&lt;Version&gt;-x64.dmg</code></li><li>Studio 3T for macOS ARM64 (Apple silicon)：<code>Studio-3T-&lt;Version&gt;-arm64.dmg</code></li><li>Studio 3T for Linux x64：<code>studio-3T-&lt;Version&gt;-linux-64.tar.gz</code></li><li>Studio 3T for Windows x64：<code>Studio-3T-&lt;Version&gt;-x64.exe</code></li></ul><p>更多：<a href="https://link.segmentfault.com/?enc=1546uQdAkmKZAJv9%2B2jh0w%3D%3D.ak8UtTlA9R4%2BcphdQnDyIDBAMXjOaLp0BQcfKKURyQY%3D" rel="nofollow" target="_blank">macOS 下载汇总 (系统、应用和教程)</a></p>]]></description></item><item>    <title><![CDATA[使用基于用户的工作流规则为提高您的项目管]]></title>    <link>https://segmentfault.com/a/1190000047411953</link>    <guid>https://segmentfault.com/a/1190000047411953</guid>    <pubDate>2025-11-19 18:09:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="208" referrerpolicy="no-referrer" src="/img/bVdm6a4" alt="" title=""/></p><p>项目管理工具中的用户管理是指管理员控制个人如何访问和使用系统的流程和功能。它包括创建用户帐户、分配角色和权限、将团队成员组织成组或项目，以及管理他们对任务、数据和工作流程的访问权限。通过用户管理，项目经理可以确保每个人都拥有履行职责所需的适当访问权限，同时维护安全性和责任性。此功能有助于简化协作、保护项目信息，并在整个项目生命周期中支持高效的团队协调。</p><p>用户管理是项目管理中不可或缺的一部分，需要进行充分的监控。随着团队规模的扩大和项目数量的增加，手动协调更新用户和权限变得越来越困难，也更容易出错。在项目管理的过程中，项目所有者需要收集门户里面添加新用户的信息，他还需要有用户在门户中暂停或者删除的信息。 这些信息自己手动跟踪这些信息不太舒服，但是如果自动收到这些信息，项目管理的过程转为很方便。<br/>Zoho Projects 的用户自动化功能有助于避免错误，同时确保流程的统一性和一致性。使用 Zoho Projects 用户自动化功能，即可自动执行与用户相关的工作流程和 Webhook。<br/>Zoho Projects 有两中用户：门户用户和客户用户。 门户用户可以说是您的公司的成员比如说您的同事，客户用户可以说是您的客户公司的成员或者您的客户。您可以在门户里面为两中用户创建工作流规则为自动进行任何操作。 </p><p>用户自动化的优势：</p><ul><li>为新用户分配默认权限并完成入职流程。</li><li>自动发送用户更新提醒。</li><li>简化访问权限移除或帐户停用流程。</li></ul><p>用户自动化帮助管理员和经理简化用户管理，并改善项目中最终用户的体验。<br/>例如，您可以创建一个用户工作流规则，以便在团队成员的个人资料更新时收到通知。将规则设置为在用户个人资料更新时执行。添加条件“向您汇报”，并为该条件关联一个电子邮件提醒操作，以便通知您或相关用户。这样，每当向您汇报的用户的个人资料更新时，系统都会向您或选定的用户发送电子邮件。</p>]]></description></item><item>    <title><![CDATA[数智化破局：光伏储能逆变器行业CRM全场]]></title>    <link>https://segmentfault.com/a/1190000047411957</link>    <guid>https://segmentfault.com/a/1190000047411957</guid>    <pubDate>2025-11-19 18:09:09</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>光伏储能逆变器行业正迎来全球化扩张与技术迭代的双重机遇，作为新能源产业链的核心环节，企业面临着客户类型多元、销售周期长、跨区域协同难、售后需求专业等行业痛点。CRM（客户关系管理）系统作为数字化转型的核心工具，已不再是单纯的客户数据存储平台，而是贯穿“<strong>营销-销售-服务-决策</strong>”全链路的增长引擎，为光伏储能逆变器企业破解增长难题提供关键支撑。</p><h2>行业核心痛点与CRM适配逻辑</h2><p>光伏储能逆变器行业的业务特性决定了其客户管理的复杂性：B2B为主的业务模式中，客户涵盖经销商、安装商、项目方、终端企业等多层级；产品需适配不同国家的电网标准、认证体系，海外市场布局需求迫切；从线索获取到项目落地周期可达数月甚至数年，需全程精准跟进；售后涉及技术调试、故障排查、备件供应等专业服务，直接影响客户复购与口碑。</p><p>珍客AI CRM系统通过数字化手段，将分散的客户资源、销售数据、服务记录整合为统一资产，精准匹配行业需求：解决客户信息碎片化问题，实现全生命周期可视化管理；打通跨部门、跨区域协作壁垒，提升业务响应效率；通过数据沉淀与分析，让营销、销售决策更具针对性，助力企业在激烈的市场竞争中抢占先机。</p><h3>1. 客户分层管理：AI赋能精准画像与价值洞察</h3><p>光伏储能逆变器企业的客户群体差异显著，需求痛点各不相同。珍客AI CRM系统按客户类型（经销商 / 项目方 / 终端用户）、合作阶段（潜在 / 意向 / 成交 / 复购）、采购规模、区域市场、产品偏好等维度进行精细化分层，建立完整的客户画像。<br/>针对海外经销商，记录其所在区域的认证要求、渠道覆盖范围、回款能力等关键信息，适配不同国家的合规标准与合作政策；<br/>对大型光伏项目方，关联项目备案信息、技术参数需求、招标进度等，实现项目全流程追踪；<br/>为终端企业客户标注产品使用场景（工商业储能 / 户用储能 / 电站配套）、维护周期、历史故障记录，便于精准推送服务与升级方案。</p><p>通过AI算法整合客户采购历史、技术反馈、区域合规需求、互动记录等多维度数据，自动生成动态更新的360°全景视图。利用NLP技术分析客户邮件、会议纪要等非结构化数据，识别组织决策链关键人（如技术负责人、采购决策者）及核心诉求（如海外客户的认证标准偏好、项目方的交付周期要求）。基于深度学习模型自动计算客户价值评分与合作风险等级，精准区分战略客户与潜力客户，为资源倾斜提供数据支撑。</p><p><img width="723" height="303" referrerpolicy="no-referrer" src="/img/bVdmUkT" alt="珍客AI CRM 客户管理" title="珍客AI CRM 客户管理"/></p><h3>2. 营销自动化：AI驱动线索精准捕获与内容个性化</h3><p>光伏储能逆变器行业的营销需兼顾品牌曝光与线索转化，尤其依赖行业展会、技术研讨会、线上内容营销等渠道。珍客AI CRM系统实现营销活动的全流程数字化管理，提升获客效率与精准度。<br/>整合线上线下营销渠道数据，自动捕获展会登记、白皮书下载、官网咨询等线索，通过标签化分类筛选高意向客户；<br/>针对不同区域、不同客户层级，自动推送个性化营销内容，如海外市场的本地化技术案例、政策解读、产品升级通知等，强化客户粘性；<br/>追踪营销活动效果，通过数据反馈优化渠道投入，比如分析不同展会的线索转化率、线上推广的获客成本，让营销预算向高价值渠道倾斜。</p><p>在AI技术的驱动下，珍客AI CRM系统通过AI线索评分模型，自动分析展会登记、白皮书下载、官网咨询等行为数据，结合关键词识别（如“户用储能逆变器”“欧盟CE认证”）筛选高意向线索，优先分配给对应领域销售人员。借助AIGC技术，根据客户所在区域、行业场景自动生成本地化营销内容，如海外市场的多语言技术案例、区域政策解读、产品升级方案等，替代60%以上的人工文案工作。通过AI算法分析不同营销渠道的ROI，动态优化预算分配，聚焦高转化率的展会或线上推广渠道。</p><p><img width="723" height="345" referrerpolicy="no-referrer" src="/img/bVdmUkS" alt="珍客AI CRM 线索评分" title="珍客AI CRM 线索评分" loading="lazy"/></p><h3>3. 销售全流程管控：AI缩短周期与提升赢单率</h3><p>光伏储能逆变器产品的销售流程复杂，涉及方案设计、报价、招投标、合同签订、交付跟进等多个环节，珍客AI CRM系统实现全流程可视化管控，降低沟通成本与流失风险。<br/>线索分配自动化，根据销售人员的区域分工、专业领域（如户用储能 / 大型电站）自动分配客户资源，避免线索闲置；<br/>销售漏斗实时更新，清晰呈现各阶段客户数量与推进进度，管理人员可及时发现卡壳环节，提供针对性支持（如技术方案优化、商务谈判协助）；<br/>合同与订单关联管理，自动记录产品型号、交付周期、付款条款等关键信息，同步至库存与生产部门，实现销售与供应链的协同联动。</p><p>在有了AI的加持，珍客AI CRM系统搭建AI赢单概率预测模型，整合12项核心指标（需求明确度、预算匹配度、竞品接触情况等），实时计算商机成交概率，自动触发跨部门协同任务（如研发输出定制化技术方案、生产同步交付周期），使项目周期缩短20%-30%。智能报价系统结合原材料价格、汇率波动、区域税费等实时数据，自动生成最优报价方案，适配全球100+币种与合规要求。招投标阶段，AI通过NLP技术解析标书关键词，快速匹配产品优势与技术参数，自动生成标书框架，提升标书制作效率70%。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmJbE" alt="珍客AI CRM 销售全流程管理" title="珍客AI CRM 销售全流程管理" loading="lazy"/></p><h3>4. 售后与服务升级：AI实现预测性维护与智能响应</h3><p>光伏储能逆变器的使用周期长，售后技术支持与维护服务是客户留存的关键。CRM 系统可打造标准化、高效化的售后服务体系，提升客户满意度与复购率。<br/>建立统一的服务工单系统，客户通过邮件、电话、线上平台提交的技术咨询、故障报修等需求，自动生成工单并分配给对应技术人员，全程追踪处理进度；<br/>记录客户的产品安装时间、维护记录、备件更换情况，基于数据预判维护需求，主动推送巡检提醒与保养建议，实现从 “被动响应” 到 “主动服务” 的转变；<br/>整合技术知识库，将常见故障解决方案、产品操作指南、区域认证标准等上传至系统，方便销售人员与技术人员快速查询，提升服务专业度。</p><p>珍客AI CRM系统与设备数据打通，通过IoT设备采集逆变器运行数据（电压、温度、功率转换效率等），结合机器学习算法构建故障预测模型，提前30-48小时预警潜在故障（如IGBT模块老化、散热系统异常），自动触发预防性维护工单。AI客服机器人通过NLP技术理解客户故障描述，快速匹配技术知识库中的解决方案，24小时响应常见咨询，将人工介入率降低50%以上。智能工单分配系统结合技术人员的专业领域（如大型电站逆变器、户用储能系统）、地理位置与负载情况，自动匹配最优服务资源，使故障响应时间从24小时缩短至4小时。</p><p><img width="723" height="328" referrerpolicy="no-referrer" src="/img/bVdmXcZ" alt="珍客AI CRM 智能服务管理" title="珍客AI CRM 智能服务管理" loading="lazy"/></p><h3>5. 数据驱动决策：AI赋能趋势预判与战略优化</h3><p>随着新能源政策的持续加码与全球化竞争的加剧，光伏储能逆变器企业需基于数据快速调整战略。珍客AI CRM系统的数据分析功能，可挖掘客户行为规律与市场趋势，为企业决策提供科学支撑。<br/>分析客户采购周期、复购频率、产品偏好，优化产品研发与库存布局，比如针对高需求区域加大某类逆变器的产能倾斜；<br/>监控不同区域、不同渠道的销售业绩，识别优势市场与潜力市场，辅助制定全球化扩张策略；<br/>通过客户流失预警模型，及时发现客户合作风险（如长期无互动、竞品接触），并触发挽留机制，降低客户流失率。</p><p>珍客AI CRM系统通过实时监测全球政策动态、区域装机量数据、客户采购周期，预测未来3-6个月的市场需求，为产能规划与库存优化提供精准支撑（如向高需求区域倾斜某类逆变器产能）。AI客户流失预警模型自动识别风险信号（如长期无互动、竞品接触、付款逾期），生成针对性挽留方案（如推送升级产品信息、优化服务条款），降低客户流失率。借助数字孪生技术，模拟不同区域市场的业务拓展效果，为全球化布局与渠道优化提供数据支撑，使决策效率提升40%。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdmUkU" alt="珍客AI CRM 数据分析" title="珍客AI CRM 数据分析" loading="lazy"/></p><h2>结语</h2><p>在光伏储能逆变器行业向数字化、全球化深度转型的背景下，珍客AI CRM系统已成为企业整合资源、提升效率、构建核心竞争力的关键工具。它不仅解决了行业客户管理分散、流程繁琐、响应滞后等痛点，更通过全链路的数字化赋能，让企业实现从“粗放式增长”到“精细化运营”的转变。</p><p>对于光伏储能逆变器企业而言，选择适配行业特性的CRM解决方案，意味着抢占了数字化时代的增长先机，能够在激烈的市场竞争中精准对接客户需求、高效推进业务落地、持续沉淀客户价值，为企业的长期发展注入持久动力。</p>]]></description></item><item>    <title><![CDATA[Gemini 3.0 发布，Antigr]]></title>    <link>https://segmentfault.com/a/1190000047411964</link>    <guid>https://segmentfault.com/a/1190000047411964</guid>    <pubDate>2025-11-19 18:08:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>昨天，谷歌不开任何发布会，直接甩出了一枚重磅炸弹——Gemini 3.0。</p><p>这一波更新来得猝不及防。<a href="https://link.segmentfault.com/?enc=OmzfeedUwHGYE%2BlAkSdJJA%3D%3D.0m4kDYHhNjTL3fcp7vWSrWGPqNhTjIrRYqT11iGdw7RsHgD%2FUEeKS9Xd%2B8lfk53C" rel="nofollow" target="_blank">Gemini 3.0</a> 不仅第一时间登陆了 AI Studio 和 Gemini CLI，还直接渗透到了开发者最常用的工具链里：Cursor、GitHub Copilot、JetBrains 全家桶，以及 Cline。甚至连谷歌自家的一系列产品，今天起也都集成了 Gemini 3 Pro 预览版。</p><p><img width="723" height="410" referrerpolicy="no-referrer" src="/img/bVdm6a8" alt="image.png" title="image.png"/></p><p>伴随模型发布，谷歌还掏出了一个全新的开发平台，<strong>Google Antigravity</strong>。谷歌说这是 VS Code 的分支，但它的野心显然不在于做一个编辑器，而是试图将开发模式从编写代码转向任务导向的一次尝试。</p><h3>Gemini 3.0 多项基准测试碾压一众模型</h3><p>在深入了解 Antigravity 之前，先来了解一下 Gemini 3.0 的三个主要技术特点。</p><h4><strong>逻辑推理能力的提升</strong></h4><p>Gemini 3 Pro 在 LMArena 等基准测试中取得了 1501 的高分，显示出接近博士水平的逻辑推理能力。</p><p>此外，谷歌还引入了 <strong>Gemini 3 Deep Think</strong> 模式。与普通版的快速响应不同，Deep Think 模式类似于人类的慢思考，在回答前会进行深度的思维链推导，专门用于解决数学、科学及复杂逻辑问题。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6bb" alt="image.png" title="image.png" loading="lazy"/></p><h4><strong>从生成内容到生成界面</strong></h4><p>Gemini 3.0 引入了 Generative UI（生成式界面）。传统的 AI 问答通常返回文本或代码片段，而 Gemini 3.0 支持生成完整的交互式界面。例如查询贷款计算方式时，模型可以直接构建一个包含滑块和输入框的计算器应用界面，而非仅仅列出计算公式。</p><h4><strong>对抽象风格的理解</strong></h4><p>新版本强调了对Vibe Coding（氛围感编程）的支持。模型能够理解较为抽象、模糊的需求描述。开发者无需提供法律条文般严谨的指令，只需描述想要的设计风格（如赛博朋克风、故障艺术感），模型即可将其转化为具体的代码实现。</p><h3>重点解析：Antigravity 与任务导向型开发</h3><p>Antigravity 是此次发布的重头戏。据说这是谷歌基于 VS Code 开发的分支版本，谷歌的亲儿子，但其核心逻辑发生了根本性转变。</p><h4><strong>从文件导向到智能体导向</strong></h4><p>传统 IDE 的工作流围绕文件展开：打开文件、编写代码、手动运行。Antigravity 的工作流则围绕智能体（Agent）展开。其核心理念是让开发者从繁琐的实现细节中抽离，转向更高层次的任务管理。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdm6bb" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>Antigravity 的工作机制：</strong></p><ol><li><strong>任务下发</strong>：开发者以自然语言描述完整需求，例如“构建一个航班追踪器，界面简洁并支持实时更新”。</li><li><strong>任务拆解</strong>：平台内置的智能体自动将需求拆解为具体的技术步骤。</li><li><strong>全链路执行</strong>：Antigravity 深度集成了 <strong>Gemini 2.5 Computer Use</strong> 模型。这使得智能体不仅具备编写代码的能力，还拥有浏览器自动化操作的能力。智能体可以编写代码，随后自动打开浏览器进行测试，模拟点击、输入，并在发现错误时自动返回编辑器修正代码。</li><li><strong>本地与云端协同</strong>：配合最新的 Nano Banana 技术，整个开发过程在本地环境与云端资源之间进行调度。</li></ol><p>这种模式将开发者的角色从代码录入者转变为任务指挥官。</p><h3>对初级开发岗位的冲击与转型</h3><p>Antigravity 展示了一种可能性，那就是基础的代码编写、测试和调试工作，正逐渐被 AI 接管。对于技能仅限于将需求直译为基础代码，或依赖网络搜索复制粘贴的初级程序员而言，职业空间确实面临压缩。</p><p>然而，这并不意味着程序员这一职业的消亡，而是职能的向上迁移。未来的开发趋势指向懂代码的架构师。</p><p><strong>开发者的新核心竞争力：</strong></p><ul><li><strong>代码审查能力</strong>：AI 能够生成代码，但也可能产生幻觉或逻辑漏洞。开发者必须具备阅读和审查 AI 产出的能力，以确保系统的安全性与稳定性。</li><li><strong>任务拆解与指令工程</strong>：Antigravity 的执行效率取决于指令的清晰度。如何将模糊的业务需求转化为 AI 可精准执行的技术任务，将成为关键技能。</li><li><strong>系统架构思维</strong>：AI 擅长执行具体的战术任务（如编写函数），但在宏观的战略布局（如高可用架构设计）上仍需人类把控。</li></ul><p>初级程序员应当减少对语法细节的死记硬背，转而投入到系统设计原理与调试逻辑的学习中。</p><h3>快速构建本地开发环境</h3><p>对于希望第一时间体验 Gemini 3.0 新特性（特别是 Gemini CLI）的开发者来说，配置基础环境往往是第一道门槛。<a href="https://link.segmentfault.com/?enc=1FQWxqAFD1Tzrbd9PouUwQ%3D%3D.nQujUW8Pfx4t5I4clAB%2B2MIfnZSIItd5%2FzjOQVLP65c%3D" rel="nofollow" target="_blank">Node.js 环境</a>的配置、版本管理常常耗费大量精力。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdm6bd" alt="image.png" title="image.png" loading="lazy"/></p><p>此时，ServBay 是一个高效的解决方案。ServBay 专为开发者设计，旨在简化本地开发环境的部署流程.</p><ul><li><strong>环境配置</strong>：支持<a href="https://link.segmentfault.com/?enc=%2BuGVih3Cr9mVc5PNvCdLpg%3D%3D.ph0ckyRknOaBp%2FaqisnlfzP9C0HfHnYn1Vt2N4dT%2F8y6DK%2FWgcKEh8YVHlECizAA" rel="nofollow" target="_blank">一键部署 Node.js 环境</a>，无需处理复杂的环境变量和版本冲突，方便开发者快速运行 Gemini CLI。</li><li><strong>本地 AI 部署</strong>：ServBay 同样支持一键部署本地 AI 模型。开发者可以在本地运行 Gemma、Qwen 3 等开源模型，方便与 Gemini 3.0 进行对比测试，既满足了隐私需求，也便于低延迟调试。</li></ul><h3><strong>结语</strong></h3><p>Gemini 3.0 与 Antigravity 的出现，降低了写代码的门槛，却提高了构建软件”=的标准。工具的进化旨在释放生产力，开发者只需善用工具，从繁杂的重复劳动中脱身，专注于更有价值的创造与设计。</p>]]></description></item><item>    <title><![CDATA[分享一名海外独立开发者的 AI 编程工作]]></title>    <link>https://segmentfault.com/a/1190000047411970</link>    <guid>https://segmentfault.com/a/1190000047411970</guid>    <pubDate>2025-11-19 18:07:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p><strong>编者按：</strong> 当 AI 编程智能体宣称能自动化一切时，我们是否在工具与概念的丛林中迷失了方向，反而忘记了如何最简单、直接地解决问题？</p><p>本文的核心主张尖锐而明确：与其追逐繁杂的“智能体套件”、子智能体（Subagents）、RAG 等概念，不如回归本质 —— 选择一个强大且高效的模型，像与一位靠谱的工程师同事那样，通过简洁的对话和直觉性的协作来直接解决问题。作者直言不讳地批评了当前生态中许多“华而不实”的工具，认为它们不过是绕开模型本身低效的临时补丁，并分享了他如何用多个终端窗口和经典工具（如 tmux）实现比许多专用工具更灵活、更可控的工作流。</p></blockquote><p><strong>本文系原作者观点，Baihai IDP 仅进行编译分享</strong></p><p><strong>作者 | Peter Steinberger</strong></p><p><strong>编译 | 岳扬</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411972" alt="" title=""/></p><p>最近我没怎么在社交平台上活跃，因为我正全身心投入到最新的项目中。如今，智能体工程（Agentic engineering）已经变得非常强大，几乎能编写出我需要的 100% 的代码。然而，我却看到很多人还在费力解决本不该存在的问题，搞出一堆繁复的表演，而不是专注把事搞定。</p><p><strong>这篇文章的部分灵感来自最近在伦敦参加的“Claude Code Anonymous”活动[1]上的对话，另一部分则是因为距离我上次更新工作流已经整整一年（还是 AI 年[2]😏）。是时候做个回顾了。</strong></p><p>所有基础理念依然适用，像上下文管理这类简单内容本文不再赘述。想了解基础内容，请阅读我之前写的《Optimal AI Workflow》[3]一文。</p><h2><strong>01 我的工作背景与技术栈</strong></h2><p>我是一名独立开发者，当前开发的项目是一个约 30 万行代码的 TypeScript React 应用，外加一个 Chrome 扩展、一个 CLI 工具、一个基于 Tauri 的客户端应用，以及一个使用 Expo 的移动应用。网站托管在 Vercel 上，每次 PR 后大约两分钟就能测试新版本，其他应用尚未实现自动化部署。</p><h2><strong>02 我所使用的技术工具和处理开发任务的总体思路</strong></h2><p>我已完全改用 codex cli 作为主力工具。通常我会在一个 3x3 的终端网格中同时运行 3 到 8 个实例，它们大多位于同一目录[4]，部分实验性任务则会放在独立文件夹中。我尝试过 worktrees、PR 等方式，但总会回到当前这套配置，因为它能最快地把事情做完。</p><p>我的智能体（agents）会自行执行原子化的 Git commits[5]。为了保持相对干净的 commit 历史，我在 agent 配置文件[6]上反复迭代优化。这样一来，Git 操作更精准，每个智能体只提交它实际修改过的文件。</p><p>是的，用 Claude 你可以设置 hooks（译者注：可能是 git commit hook），而 codex 目前还不支持 hooks，但大模型极其聪明 —— 一旦它们下定决心，没有任何 hook 能拦得住[7]。</p><p>过去我曾因此被嘲讽为垃圾代码制造机[8]，如今看到并行运行智能体的做法逐渐成为主流[9]，深感欣慰。</p><h2><strong>03 模型选择</strong></h2><p>我几乎所有的开发工作都交由 gpt-5-codex 在“medium 配置”下完成。它在智能程度与速度之间取得了极佳的平衡，还能自动调节思考深度。我发觉过度纠结这些设置并无明显的回报，而且不用操心“超深度思考”（ultrathink）的感觉真的很轻松。</p><h3><strong>3.1 爆炸半径 💥</strong></h3><p>每次工作时，我都会考量“爆炸半径” —— 这个词不是我发明的，但我非常喜欢。当构思某个改动时，我基本能预判其耗时及波及的文件范围。我可以向代码库投掷多枚“小手雷”，或是一发“胖子”配几颗小炸弹。但如果你同时扔下多个大炸弹，就几乎不可能做出隔离良好的提交，一旦出错也更难回滚。</p><p>这同时也是我观察智能体运行时的一个重要指标。如果某项任务耗时超出预期，我会直接按 Esc，然后问一句“当前状态如何？”来获取任务进度，再决定是帮模型调整方向、中止任务，还是继续执行。<strong>别害怕在中途打断模型 —— 文件修改是原子性的，它们非常擅长接续未完成的工作。</strong></p><p>当我对改动的影响不确定时，会先让模型“在修改前给我几个选项”，以此评估影响范围。</p><h3><strong>3.2 为何不用 Worktree？</strong></h3><p>我始终只运行一个开发服务器。在迭代项目时，我会通过实时操作界面，一次性测试多处改动。如果为每个功能变更都创建独立的工作树（worktree）或分支（branch），会严重拖慢我的测试流程。而同时启动多个开发服务器又会带来不必要的操作负担。此外，我的项目受 Twitter OAuth 规则限制，只能注册有限数量的回调域名，这从客观上也不支持多环境并行的开发方式。</p><h3><strong>3.3 那 Claude Code 呢？</strong></h3><p>我曾经很喜欢 Claude Code，但如今实在受不了了（即便 codex 对其赞誉有加[10]）。那种语言风格、那种斩钉截铁的“绝对正确”[11]、那种测试明明失败却宣称“100%满足生产要求”的语气——实在令人无法继续。相比之下，codex 更像是那个内向但靠谱的工程师：默默推进，把事情做完。它在开始工作前会读取更多文件，因此即使是简短的提示词，通常也能精准实现我想要的效果。</p><p>在我关注的信息流中，大家已普遍认为 codex 才是当前的首选[12-13]。</p><h3><strong>3.4 codex 的其他优势</strong></h3><ul><li><strong>约 23 万的可用上下文（context），而 Claude 只有 15.6 万。</strong> 是的，如果你运气好或愿意按 API 定价付费，Sonnet 确实有 100 万上下文，但现实中 Claude 在耗尽上下文之前就已经开始胡言乱语了，所以这个超长上下文实际上并不可用。</li><li><strong>更高的 token 利用效率。</strong> 我不知道 OpenAI 做了什么不同处理，但我的上下文空间在 codex 中消耗得明显更慢。用 Claude 时我经常看到 “Compacting…” 提示，而在 Codex 中我极少触及上下文上限。</li><li><strong>消息队列（Message Queuing）。</strong> Codex 支持消息排队[14]。Claude 以前也有这功能，但几个月前改成了“消息会实时引导模型”的机制。如果我想引导 codex，只需按 Esc 再回车就能发送新消息。能同时选择“排队”或“即时干预”显然更好。我经常一次性将多个相关功能任务放入队列，它总能可靠地逐个完成。</li><li>速度。<strong>OpenAI 用 Rust 重写了 codex，效果立竿见影 —— 响应速度快得惊人。</strong> 而用 Claude Code 时，我经常遇到数秒的卡顿，内存占用动辄飙到几个 GB。还有终端显示的闪烁问题，尤其是在用 Ghostty 时。Codex 完全没有这些问题，感觉极其轻量、流畅。</li><li><strong>语言风格。</strong> 这点对我的心理健康真的很重要[15]。我曾无数次对 Claude 大吼大叫，但很少对 codex 发火。哪怕 codex 模型能力稍弱，光凭这一点我也愿意用它。只要你两个都用上几周，就懂我在说什么。</li><li><strong>不会到处乱生成 markdown 文件</strong>[16]。懂的都懂（IYKYK）[17]。</li></ul><h3><strong>3.5 为何不选用其他开发工具</strong></h3><p>在我看来，终端用户和大模型公司之间其实没有太多中间空间。我目前通过订阅获得的性价比远高于其他方式。我现在有 4 个 OpenAI 订阅和 1 个 Anthropic 订阅，每月总花费大约 1000 美元，基本可以享受“无限 token”的使用体验。如果改用 API 调用，成本大概会高出 10 倍。别太较真这个数字——我用过像 ccusage 这样的 token 统计工具，数据多少有些不精确，但即便只是五倍，也已是相当划算的交易了。</p><p>我很欣赏像 amp 或 Factory 这样的工具，但我不认为它们能长期存活。无论是 codex 还是 Claude Code，每个版本都在变得更强，而且功能理念正在快速趋同。某些工具可能在待办列表、引导控制或细微的开发者体验（DX）上暂时领先，但我不觉得它们能真正超越大型 AI 公司。</p><p>amp 已经不再以 GPT-5 为核心驱动，转而称其为“Oracle”（神谕）[18]。而我直接使用 codex，本质上就是一直在和那个更聪明的模型——也就是“Oracle”——打交道。是的，有各种基准测试[19]，但考虑到使用场景的巨大不同，我不太信任那些结果。实际体验中，codex 给我的输出远优于 amp。不过我得承认，他们在会话共享方面确实做了些有趣的创新。</p><p>Factory？我还没被说服。他们的演示视频有点尴尬，虽然我在信息流里确实听到一些正面评价 —— 尽管目前还不支持图像（至少现在还不行），而且也有标志性的闪烁问题[20]。</p><p>Cursor……如果你还在亲手写代码，那它的 Tab 补全模型确实是业界领先。我主要用 VS Code，但确实欣赏他们在浏览器自动化和计划模式（plan mode）等方面的探索。我试过 GPT-5-Pro，但 Cursor 依然存在那些从五月起就让我烦躁的 bug[21]。听说他们正在修复，所以它还留在我的程序坞里。</p><p>像 Auggie 这样的工具，只在我的信息流上昙花一现，之后就再没人提过。归根结底，它们底层无非是封装了 GPT-5 和/或 Sonnet，完全可以被替代。RAG 对 Sonnet 或许有点用，但 GPT-5 本身在代码检索上已经强到根本不需要额外的向量索引。</p><p>目前最有希望的是 opencode 和 crush，尤其是搭配开源模型使用时。你当然也能通过它们使用 OpenAI 或 Anthropic 的订阅（得益于一些巧妙的技术手段[22]），但这是否合规仍存疑，况且为何要为一个专为 Codex 或 Claude Code 优化的模型，配上一个能力较弱的“外壳”呢。</p><h3><strong>3.6 关于开源模型</strong></h3><p>基准测试只能说明一半的问题。在我看来，智能体工程（agentic engineering）大约在 Sonnet 4.0 发布的五月，才真正从“这玩意儿真烂”迈入“这还不错”的阶段；而随着 gpt-5-codex 的出现，我们又迎来了一次更大的进步 —— 从“不错”直接进入“这简直太棒了”的境界。</p><h3><strong>3.7 计划模式（Plan Mode）与方法</strong></h3><p>基准测试所忽略的，是模型与工具在接到指令后所采取的策略。codex 要谨慎得多 —— 它会在决定行动前读取你代码库中更多的文件。当你提出一个荒谬请求时，它也更倾向于明确反对[23]。相比之下，Claude 或其他智能体会更急切地直接动手尝试。虽然可以通过“计划模式”（plan mode）和严谨的结构化文档来缓解这个问题，但对我而言，这感觉像是在给一个有缺陷的系统打补丁。</p><p>如今我几乎不再为 codex 使用大型的计划文件。其实 codex 甚至没有专门的计划模式（plan mode） —— 但它对提示词的理解和遵循能力实在太强，我只要写一句“我们先讨论一下”或“给我几个选项”，它就会耐心等待我确认后再行动。完全不需要那些花里胡哨的东西，直接跟它对话就行。</p><h3><strong>3.8 但 Claude Code 现在有插件了</strong></h3><p>你听见远处那声叹息了吗？那是我在叹气。这真是彻头彻尾的胡扯。Anthropic 的这一举动让我对他们的产品方向感到非常失望。他们试图用插件[24]来掩盖模型本身的低效。当然，为特定任务维护优质文档是个好主意 —— 我自己就在一个 docs 文件夹里存了大量有用的 Markdown 文档。</p><h3><strong>3.9 但是！子智能体呢</strong></h3><p>但关于这场“子智能体”（subagents）的盛宴，我有些话不吐不快。今年五月时，这还叫“子任务”（subtasks），主要是当模型不需要完整上下文时，把任务拆出去单独处理——比如并行执行，或避免把冗长的构建脚本塞进主上下文造成浪费。后来他们重新包装并升级为“子智能体”，让你可以带着指令“优雅地”打包并分派任务。</p><p>但使用场景本质上没变。<strong>别人用子智能体干的事，我通常用多个终端窗口就搞定了。</strong> 如果我想调研某个问题，可能会在一个终端窗格里操作，再把结果粘贴到另一个窗格。这种方式让我对上下文工程拥有完全的控制权和可见性，而子智能体反而让上下文变得难以查看、引导或控制。</p><p>还有 Anthropic 博客里推荐的那个子智能体 —— 你去看看他们那个所谓的“AI Engineer”智能体[25]。那简直就是一锅大杂烩：一边吹集成了 GPT-4o 和 o1，一边堆砌一堆自动生成的空洞词汇，试图显得有逻辑。里面根本没有能让智能体真正变成更好“AI 工程师”的实质内容。</p><p>这到底有什么用？如果你希望获得更好的输出，光告诉模型“你是一位专精于生产级 LLM 应用的 AI 工程师”是没用的。<strong>真正有用的是提供文档、示例，以及明确的“该做什么/不该做什么”。</strong> 我敢打赌，你让智能体去“搜索 AI 智能体构建的最佳实践”并加载几个网页，效果都比那堆废话强得多。你甚至可以说，这种胡扯本身就是一种上下文污染（context poison）[26]。</p><h2><strong>04 我的提示词撰写之道</strong></h2><p>以前用 Claude 时，我（当然不是手打，而是靠语音）会写非常详尽的提示词，因为那个模型“给越多上下文，越懂我”。虽然所有模型多少都这样，但我发现换用 codex 后，提示词明显变短了 —— 常常就一两句话，外加一张图。这个模型读代码库的能力极强，就是能精准理解我的意图。有时候我甚至又愿意打字了，因为 codex 根本不需要太多上下文就能明白。</p><p><strong>添加图片是个绝妙的技巧，能快速补充上下文。</strong> 模型非常擅长精准定位你截图中的内容 —— 无论是字符串还是界面元素，它都能迅速匹配并跳转到你提到的位置。我至少有一半的提示词都包含截图，虽然添加标注效果更佳但效率更低，而直接拖拽截图到终端仅需两秒。</p><p>带语义纠错的 Wispr Flow[27] 仍是当前最优方案。</p><h2><strong>05 Web 端智能体新体验</strong></h2><p>最近我又重新尝试了一些 Web 端智能体：Devin、Cursor 和 Codex。Google 的 Jules 界面美观，但配置流程繁琐，且 Gemini 2.5 现在已经算不上好模型了。不过一旦 Gemini 3 Pro 上线[28]，情况或许会有所转变。目前唯一留下来的只有 codex web。虽然它也存在配置复杂的问题，而且现在还有 Bug（终端目前就无法正确加载），但我靠一个旧版环境让它跑起来了，代价是启动速度更慢。</p><p>我把 codex web 当作临时的问题追踪器。在外突发灵感时，就用 iOS App 发一条一行字的提词词，回头在 Mac 上再仔细处理。当然，我完全可以在手机上做更多事，比如审查、合并代码，但我刻意保持克制。我的工作已经够让人上瘾了，所以当我出门或和朋友聚会时，不想被进一步拉回工作状态。说这话的人，可是曾花将近两个月专门开发了一款便于使用手机编程的工具啊。</p><p>codex web 上的任务原本不计入使用额度，可惜这样的好日子恐怕快到头了。</p><h2><strong>06 The Agentic Journey</strong></h2><p>聊聊那些工具吧：Conductor[29]、Terragon[30]、Sculptor[31] 等数以千计的同类产品。有些是个人爱好项目，有些则被 VC 投来的钱淹得喘不过气。我试过太多太多，没一个能让我长期用下去。在我看来，它们都是在绕开当前模型的低效，推行一种并不真正高效的工作流。而且大多数还藏起终端，不让你看到模型的全部输出。</p><p>绝大多数不过是 Anthropic SDK 的浅层封装 + 工作树管理，毫无技术护城河可言。我甚至怀疑：我们真的需要在手机上更方便地调用编程智能体吗？这些工具的有限应用场景，现在 codex web 已经完全覆盖了。</p><p>不过我确实观察到一个普遍现象：几乎每个工程师都会经历一个“自己造工具”的阶段 —— 主要是因为好玩，也因为现在做这件事确实太容易了。既然如此，还有什么比造一个“（我们以为）能让造工具变得更简单的工具”更自然呢？</p><h2><strong>07 但 Claude Code 能处理后台任务！</strong></h2><p>确实如此。<strong>codex 目前缺少一些 Claude 有的小功能，其中最让人头疼的就是后台任务管理。</strong> 虽然理论上应该有超时机制，但我确实多次遇到它卡在不会自动结束的 CLI 任务上，比如启动开发服务器，或者死锁的测试。</p><p>这曾是我一度切回 Claude 的原因之一。但鉴于那个模型在其他方面实在太不靠谱，我现在改用 tmux。tmux 是一个老牌工具，能在后台持久化运行 CLI 会话，而且模型里早就内置了大量相关知识 —— 你只需要说一句“用 tmux 运行”，就能搞定，无需任何复杂的智能体配置流程。</p><h2><strong>08 那 MCPs 呢？</strong></h2><p>关于 MCP（Model Context Protocol），其他人已经写了很多。在我看来，大多数 MCP 本质都只是市场部门用来打勾炫耀的工具。几乎所有 MCP 其实都应该做成 CLI。这话出自一个自己写过 5 个 MCP[32] 的人之口。</p><p>我可以直接按工具名字调用一个 CLI，根本不需要在 agent 配置文件里写任何说明。模型第一次调用时可能会试一些乱七八糟的命令（$randomcrap），CLI 会自动返回帮助菜单，上下文立刻就拥有了完整的使用信息 —— 从此一切顺利。我不用为任何工具付出额外代价，而 MCP 却是持续的成本，还会污染我的上下文。试试 GitHub 的 MCP，瞬间吃掉 23k tokens。好吧，他们后来优化了 —— 刚上线时可是接近 5 万 tokens！换成 gh CLI 呢？功能基本一样，模型本来就认识它，还完全不用交“上下文税”。</p><p>我自己开源了一些 CLI 工具，比如 bslog[33] 和 inngest[34]。</p><p>我现在确实在用 chrome-devtools-mcp[35] 这个工具来做最终验证[36]，它已经取代了 Playwright，成为我进行网页调试时的首选 MCP 工具。虽然我不常用它，但一旦需要，它就能帮我完成从“代码修改”到“验证结果”这个关键闭环，非常有用。我还专门设计了我的网站，让模型能通过 curl 查询任意接口（通过我生成的 API key）——这在几乎所有场景下都比 MCP 更快、更省 token。所以就连这个 MCP，我也不是每天都需要。</p><h2><strong>09 但生成的代码太糟糕了！</strong></h2><p>我约 20% 的时间[37]投入在重构上。当然，这些全由智能体完成，我绝不会手动浪费时间干这种事。当我不太需要高度专注或感到疲惫时，“重构日”就特别有用 —— 即使状态一般，也能取得显著进展。</p><p>典型的重构工作包括：用 jscpd 找重复代码，用 knip[38] 清理死代码，运行 eslint 的 react-compiler 和弃用插件（译者注：一类 ESLint 插件，用于检查代码中是否使用了已过时的 API、方法或特性，并提示你改用现代、推荐的替代方案。），检查是否有可合并的 API 路由，更新文档，拆分过大的文件，为复杂逻辑补充测试和注释，更新依赖项，升级工具链，调整目录结构，找出并重写慢测试，引入现代 React 模式（比如你可能根本不需要 useEffect）等等。总有做不完的事。</p><p>有人可能会说这些应该在每次提交时就做完。但我发现，先快速迭代、再集中维护和优化代码库——即阶段性偿还技术债务——这种方式不仅效率更高，而且整体上有趣得多。</p><h2><strong>10 你采用规范驱动开发（spec-driven development）吗？</strong></h2><p>我去年六月还在用这种方式：先写一份详尽的规格文档，然后让模型去实现，理想情况下能连续跑上好几个小时。但现在我觉得，这种“先设计后构建”的思路已经是过时的软件开发范式了。</p><p>我现在的做法通常是：<strong>先直接和 codex 展开讨论，贴一些网站链接、初步构想，让它解读现有代码，然后我们一起把新功能逐步梳理出来。如果问题比较棘手，我会让它把思路整理成一份规范文档，然后交给 GPT-5-Pro（通过 chatgpt.com）做评审，看看是否有更好的建议 —— 出乎意料的是，这经常能大幅优化我的方案！接着，我会把其中我觉得有用的部分粘回主上下文，用于更新实际文件。</strong></p><p>现在我对不同任务消耗多少上下文已经有不错的直觉，而 codex 的上下文容量也相当充足，所以很多时候我干脆直接开干。有些人很“虔诚”，总喜欢为每个新计划新开一个上下文窗口 —— 我觉得这在 Sonnet 时代还有点用，但 GPT-5 处理长上下文的能力强得多，如果还这么做，每次都会白白多花 10 分钟，因为模型得重新慢慢加载所有构建功能所需的文件。</p><p>更有趣的方式是做基于 UI 的开发。我经常从一个非常简单的东西开始，故意把需求写得极其模糊，然后一边看模型编码，一边在浏览器里实时看到效果。接着我再排队加入更多调整，逐步迭代这个功能。很多时候我自己也不确定最终该长什么样，这种方式让我能边玩边试，看着想法慢慢成形。有时 codex 甚至会做出一些我根本没想到但很妙的设计。我从不重置进度，只是一步步迭代，把混沌慢慢塑造成我觉得对的形状。</p><p>开发过程中，我也常会冒出一些关联功能的新点子，顺势对其他部分也做些调整 —— 这部分工作我会放到另一个智能体里处理。通常我主攻一个核心功能，同时并行处理一些次要但相关的任务。</p><p>就在我写这段文字时，我正在给 Chrome 扩展开发一个新的 Twitter 数据导入器，为此我正在重构 graphql 导入模块。因为还不确定这个方案是否合理，我把这部分代码放在一个单独的文件夹里，这样可以通过 PR 预览来判断思路是否成立。主仓库则在做重构，让我能专心写这篇文章。</p><h2><strong>11 请分享您的斜杠命令！</strong></h2><p>我只有少数几个斜杠命令，而且很少用：</p><ul><li>/commit（自定义说明文本，用于协调多智能体在同一目录协作时仅提交自身修改。这样能保持提交信息干净，也能防止 GPT 因看到其他改动而 panic，比如 linter 报错时乱 revert（译者注：Git 版本控制中的常用术语，撤销某次或某几次提交（commit）所引入的更改。））</li><li>/automerge（一次处理一个 PR：响应机器人评论、回复、等 CI 通过后自动 squash 合并（译者注：Git 版本控制中的常用术语，将多个连续的提交记录合并成一个单一的、干净的提交。））</li><li>/massageprs（和 automerge 类似，但不用 squash，方便在有大量 PR 时并行处理）</li><li>/review（内置命令，偶尔用 —— 因为 GitHub 上已有 review bot，但有时还是有用）</li></ul><p>即便如此，大多数时候我其实就直接打 “commit” 两个字。除非我知道当前有太多脏文件，担心智能体在没有引导的情况下出错。如果我确信简单指令就够了，就绝不会搞那些花哨的表演或浪费上下文。这种直觉是慢慢练出来的。到目前为止，我还没见过其他真正有用的斜杠命令。</p><h2><strong>12 其他实用技巧</strong></h2><p><strong>与其费尽心思写出完美的提示词去“激励”智能体完成一个长期任务，不如用点偷懒的变通方法。</strong> 比如进行大型重构时，Codex 常会在中途暂停响应。这时候，只要提前排好几条 “continue” 消息，你就可以走开，等回来时活儿就干完了。如果 codex 已经完成了任务，再收到更多消息，它也会愉快地忽略掉。</p><p><strong>每次完成一个功能或 Bug 修复后，请让模型在同一上下文中顺手写点测试用例。</strong> 这样做不仅能产出质量高得多的测试用例，还常常能暴露代码实现中的 bug。如果是纯 UI 调整，可能测试意义不大。但对于其他情况，我强烈建议这么做。AI 写测试用例总体上还是不太行，但已经比没有强多了 —— 而且说实话，你自己每次改代码都会写测试用例吗？</p><p><strong>让模型“保留你的原始意图”，并“在复杂逻辑处添加代码注释”，这对您和后续模型理解代码都大有裨益。</strong></p><p><strong>当遇到棘手难题时，在提示词中加入一些触发词</strong>，比如 “take your time”（慢慢来）、“comprehensive”（全面一点）、“read all code that could be related”（读所有可能相关的代码）、“create possible hypothesis”（提出可能的假设） —— 这些都能让 codex 解决最棘手的问题。</p><h2><strong>13 你的 Agents/Claude 配置文件是什么样的？</strong></h2><p>我创建了一个名为 Agents.md 的主配置文件，然后为它创建了一个符号链接（译者注：Linux 操作系统中一个特殊的文件，内容存储指向目标文件或目录的路径字符串），这个链接的名字叫 claude.md。我这么做是因为开发 Claude 的 Anthropic 公司没有采用和其他工具（比如 Codex）统一的配置文件命名标准。我承认这很麻烦也不理想 —— 毕竟 GPT-5 和 Claude 偏好的提示词风格差异很大[39]。如果你还没看过它们各自的提示词指南，建议现在就去读一读。</p><p>Claude 对那种 🚨 全大写咆哮式命令 🚨[40]（比如“如果你执行 X 命令，后果将极其严重，100 只小猫会死掉！”）反应良好，但这会让 GPT-5 直接崩溃（也确实该崩溃）。所以，请彻底放弃这种写法，像正常人一样用平实的语言就行。这也意味着这些配置文件很难被最优地共享。不过对我来说问题不大，因为我主要用 codex，即使偶尔让 Claude 上场，我也接受这些指令对它来说可能强度不足。</p><p>我的 Agent 配置文件目前大约 800 行，感觉就像一堆“组织创伤”留下的疤痕组织。这不是我手写的，而是 codex 自己生成的。每次出了状况，我都会让它在文件里加一条简洁备注。我应该找个时间清理一下配置文件，但尽管文件很长，它却运行得极其可靠 —— GPT-5 也确实几乎总是遵守里面的规则。至少比 Claude 以前强太多了。（当然也得承认，Sonnet 4.5 在这方面确实有进步）</p><p>除了 Git 操作说明，文件里还包含产品说明书、我偏好的命名规范和 API 模式、关于 React Compiler 的注意事项等等 —— 很多内容甚至比模型的“世界知识”还新，因为我的技术栈相当激进。我预计随着模型更新，这部分内容还能进一步精简。例如，Sonnet 4.0 当年需要大量指导才能理解 Tailwind 4，而 Sonnet 4.5 和 GPT-5 已经内置了相关知识，所以我直接删掉了所有冗余的相关说明。</p><p>文件里很大一块内容专门描述我偏好的 React 模式、数据库迁移管理策略、测试规范，以及如何使用和编写 ast-grep 规则。（如果你还不知道 ast-grep，或者没把它用作代码库的 linter，请立刻停下来，让模型帮你把它设为 Git hook，用来拦截不符合规范的提交。）</p><p>我还尝试过一种基于文本的“设计系统”，用来规定 UI 应该长什么样 —— 不过这个实验目前还没下定论。</p><h2><strong>14 那么 GPT-5-Codex 是完美的吗？</strong></h2><p>当然不是。有时候它会花半个小时重构代码，然后突然 panic，把所有改动全 revert 掉 —— 这时候你得重新运行，并像哄小孩一样安抚它：“你有足够的时间，慢慢来。” 有时它会忘记自己其实能执行 bash 命令，需要你鼓励一下。偶尔它还会用俄语或韩语回复。更离谱的是，有时候这个“怪物”一滑手，直接把内部思考过程原样扔进了 bash 终端。但总体而言，这些情况相当罕见，而它在其他几乎所有方面都强到离谱，让我完全可以忽略这些小毛病。毕竟，人类也不是完美的。</p><p>我对 codex 最大的不满是它会“丢失文本行” —— 快速向上滚动时，部分文本会莫名其妙消失。真心希望 OpenAI 把这个 Bug 放在修复清单的最顶端，因为这是目前唯一迫使我放慢操作速度的原因，就怕消息突然不见了。</p><h2><strong>15 结论</strong></h2><p>别在 RAG、子智能体（subagents）、Agents 2.0 或其他华而不实的花架子上浪费时间了。直接跟它对话，动手试，慢慢培养直觉。你和智能体合作得越多，结果就会越好。</p><p>Simon Willison 的文章[41]说得特别到位：<strong>管理智能体所需的许多技能，其实和管理工程师非常相似 —— 而这些能力，几乎全都是资深软件工程师的特质。</strong></p><p>而且没错，写出好软件依然很难。我不再亲手写代码，并不意味着我不再深入思考架构、系统设计、依赖关系、功能实现，或者如何让用户感到惊喜。使用 AI 只意味着：大家对你交付成果的期望值变高了。</p><p>PS: 本文 100% 原创手写。我热爱 AI，但也清楚有些事用老办法反而更好。保留这些笔误，保留我的声音。🚄✌️</p><p>PPS: 文章头图由 Thorsten Ball 提供[42]，特此致谢。</p><p><strong>END</strong></p><p><strong>本期互动内容 🍻</strong></p><p><strong>❓文中哪个观点你极度认同？或者，哪个地方你持保留意见？</strong>  </p><p><strong>文中链接</strong></p><p>[1]<a href="https://link.segmentfault.com/?enc=BQp08sgpYokQR1HsPU2uxA%3D%3D.sbiNKsEjBxtvAQgTqFwi4kP2rVpAV8zm3Xq3pZIzL8Xv6QPR5LtDuwbxn2lo34Wjd0RFiZGuu%2FLzezePDMjAPg%3D%3D" rel="nofollow" target="_blank">https://x.com/christianklotz/status/1977866496001867925</a></p><p>[2]<a href="https://link.segmentfault.com/?enc=JPVbY6FxxJeWPXWKjpmySQ%3D%3D.8wGAAxE%2BE2l5IkppWFcmy4AlIschuCmZ10UydkoiogqaOcKlUvlyHSZ%2FQcbcxcnkQkEYME41ENaOgo%2F8ZOXUug%3D%3D" rel="nofollow" target="_blank">https://x.com/pmddomingos/status/1976399060052607469</a></p><p>[3]<a href="https://link.segmentfault.com/?enc=MCkaZ12BJ6nwRtNGSejnVA%3D%3D.2LZGJxo4AbrwTx0sNe2M4DUgkgShtU4yxjMOclA6l6W9y6GuGnc8cWlPBgS9JQKhmY354DM2eojTpWpe6HCvnw%3D%3D" rel="nofollow" target="_blank">https://steipete.me/posts/2025/optimal-ai-development-workflow</a></p><p>[4]<a href="https://link.segmentfault.com/?enc=d1mp7EX4fS%2BcHaJ4be9B1A%3D%3D.PvpZpajasYRZDjc1yDGp7QSVWr%2BkSPAsfQzvFP3N89gwS1juUt9UoP2ZZ%2BPwZv86cZFB74Gxbj9%2FzjNiKqb5gg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977771686176174352</a></p><p>[5]<a href="https://link.segmentfault.com/?enc=O8T34knkV48vYcyzIeBqsQ%3D%3D.W11UMfK9M1GpedAS3bwk8UYxoSyKbAQLP%2BaFrDrYiPZGaxLD2fmvk5tL1Znpq%2FyYJUPTWn1c55OHB858z5vfsw%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977498385172050258</a></p><p>[6]<a href="https://gist.github.com/steipete/d3b9db3fa8eb1d1a692b7656217d8655" target="_blank">https://gist.github.com/steipete/d3b9db3fa8eb1d1a692b7656217d8655</a></p><p>[7]<a href="https://link.segmentfault.com/?enc=fratZijdKrIGvJMB386EaA%3D%3D.qirrTyrTIKb5p1shS1TFU3n3ZlpnMd43oWDf%2BqrFb5t8e353SfmFXvM3cDoOgJvVxh%2F4NSg%2BOGVzEG0Y5%2FJ0VQ%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977119589860601950</a></p><p>[8]<a href="https://link.segmentfault.com/?enc=B07qGY2mqE4L6rsyywYpRw%3D%3D.Z3mYvlYJDMnrYFF8nox8dF78ngPg%2FwM8fcVZv3GQuWbfXfGD6j%2FEehAOEcPbw9ioBAemdautYD%2FDGoVn%2F9eEnw%3D%3D" rel="nofollow" target="_blank">https://x.com/weberwongwong/status/1975749583079694398</a></p><p>[9]<a href="https://link.segmentfault.com/?enc=dUr%2FF49h9RVb7Le6ukv6%2Fw%3D%3D.UC0IhytV84jujqXpU5WFqbY4641JQ%2FLIkbqVtahkXQ%2BYG%2F4aVhEUCOhGIBGTXKTU4i0tYiIqL9tUnfSc%2FiaD%2FA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976353767705457005</a></p><p>[10]<a href="https://link.segmentfault.com/?enc=799S8u2neWiCjQBuFjnrHA%3D%3D.e5HeJ3R6MMZW5TYZBtVpntU1jjML9QIe2CWj91%2BqQ3nbO0tOvIRkmdf6bsbSiAFnEPnv8WToH%2B1IhIO47Twx4g%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977072732136521836</a></p><p>[11]<a href="https://link.segmentfault.com/?enc=0LjD0G1M3yBdR5r3XcU1Bg%3D%3D.zAdyTNXLoHF7M5ofoQ%2FnBZ7lN%2B5gisCjEX2DJp1QeDWlGIwF%2B9JW0rq4DzpB6ktReR2mNePk7CttXv4cUkbQ%2Bw%3D%3D" rel="nofollow" target="_blank">https://x.com/vtahowe/status/1976709116425871772</a></p><p>[12]<a href="https://link.segmentfault.com/?enc=8hXnE1ySSgRj70CweZCprA%3D%3D.DF7PzexZLjOgBhjoDgKGGke5VV5s%2FWayMO7YD5sNZQBnOygfHHrD%2FgLexLMsoXopULYWtNa%2BtSYhnI7hSNCRsA%3D%3D" rel="nofollow" target="_blank">https://x.com/s_streichsbier/status/1974334735829905648</a></p><p>[13]<a href="https://link.segmentfault.com/?enc=gyynes%2BC7An5rty397Thyw%3D%3D.Q5MTkbcC2RLMARiZaqZduDMMslr2IzvD6zD4DDWLhqIImUSir3aAXnzrprhsP%2FM6xWzwP9%2BA9i2RCBfYsonLlw%3D%3D" rel="nofollow" target="_blank">https://x.com/kimmonismus/status/1976404152541680038</a></p><p>[14]<a href="https://link.segmentfault.com/?enc=MB9FWpwenhdOulaQDlDrww%3D%3D.TPuUtfogMWm6CK3mDz0XtLynozlQisggbSXC4CQXfoPtpZQZeZGhD0eHJWAGUja36IlCfJ1tZQyKFkzQ88u77A%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1978099041884897517</a></p><p>[15]<a href="https://link.segmentfault.com/?enc=s3qHtvA8oTYEfRQBrqyj9Q%3D%3D.KR0iuxP5rJP718YaN%2BjRxMCnWC5eleftoMwD92bVg5C3ZImOkLvy47sPUNDXk0TP384lXK8tR3RwFJn6t1P7lA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1975297275242160395</a></p><p>[16]<a href="https://link.segmentfault.com/?enc=cnhxXPHCRxhfic43p1EIPA%3D%3D.dPxIJPPHCXOE6VsgVAQxUPS9TFCHWuVawxja%2FJ9G7FTnJcXVRYl%2FXVupxJAmODqc%2ByilcEwoB6mVgT34Iq2hRw%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977466373363437914</a></p><p>[17]<a href="https://link.segmentfault.com/?enc=oxFlIHFj%2FLiHWf%2BVvjHy7A%3D%3D.E7UsOvWnvjreSN7ExK4vIYfd6fYkP0AuagUNxLSd%2FdUfNpH9MiTqmqaH7gsoFpVIcvRRoAMWfDiMeTY3NeOiLw%3D%3D" rel="nofollow" target="_blank">https://x.com/deepfates/status/1975604489634914326</a></p><p>[18]<a href="https://link.segmentfault.com/?enc=HwRci1YpIhntNiN87tVtfg%3D%3D.dPEgCkuQJ3kfsGkkqhm7urrDF2LuY%2FQCEelFTNr2YCUm6BsO%2BMdlXI%2FVTwBve3EU" rel="nofollow" target="_blank">https://ampcode.com/news/gpt-5-oracle</a></p><p>[19]<a href="https://link.segmentfault.com/?enc=MR4%2FrNkaG4FOlwN%2F2vXeRA%3D%3D.TYWB%2BXh8YvngNOnqimD94y4Hs5pEIk7OyaUOuyUtLHBun9z0v0xZSfegwGflS6NwQqWJvzAwDL0LLq70G%2BzKHw%3D%3D" rel="nofollow" target="_blank">https://x.com/btibor91/status/1976299256383250780</a></p><p>[20]<a href="https://link.segmentfault.com/?enc=wGf2Ja%2Fuk12NkuX6R4I0Nw%3D%3D.mxXXlXm%2BOOWcS6QKrQaNvhQOEB%2BbOn9%2F48He14qmIC6u44cJcfbXIfEpLSZqBD5IrTZSSEjfGuelGw2U0o%2BnQw%3D%3D" rel="nofollow" target="_blank">https://x.com/badlogicgames/status/1977103325192667323</a></p><p>[21]<a href="https://link.segmentfault.com/?enc=0TQgdwAm%2Fu1P3YZPFy%2F6kA%3D%3D.rw5AP48aLLdjqCw%2Fe0qy2XpSvYDnfMMsit4klZ2yFxLwNUAyd1hkBetHoFaZbf6fFrITPE4O%2BpYgjBkzm729Lg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976226900516209035</a></p><p>[22]<a href="https://link.segmentfault.com/?enc=%2BFpSZU%2FTALaduRF992%2FwRg%3D%3D.QwDnB4V%2FRvpTh9FDliKl4rVfCT99UjGm2oj9Udu10r8pttuhqoNlFDsmIaInDSEZnDEoFiqnXkzgw5Y2itXW8A%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977286197375647870</a></p><p>[23]<a href="https://link.segmentfault.com/?enc=RyhUHTGktTmy%2BilDo7%2Ft0A%3D%3D.p933ZlbJKqdu8A444qbNKyMWM5S5eV5ZVn%2B2AHLp6CLq%2Bwf3pxKyqlpEgJ60wsYfNSAX1fvcozaQ%2BMhimSNUUQ%3D%3D" rel="nofollow" target="_blank">https://x.com/thsottiaux/status/1975565380388299112</a></p><p>[24]<a href="https://link.segmentfault.com/?enc=j2yIlHp%2FqGHxN7kAHY0PxQ%3D%3D.GDRnkKVYD1dxHTc1NOiaXL2x1d7X%2BN4xaUQP5JWod0TMvlHyhuA4QkbePAbcMEvGNjiMxSxZD4Xon3ePlR4cJA%3D%3D" rel="nofollow" target="_blank">https://www.anthropic.com/news/claude-code-plugins</a></p><p>[25]<a href="https://link.segmentfault.com/?enc=CEuAjTle8si29cP%2Bt8FKgg%3D%3D.fe1HBlLpSR5b2%2BTo%2FS0d9C6O3IZsN2jFasReSRrY5UXHKtVZpOlSWpRK3GfDiapweAzNxS5twOnD7Wifo22mceE7mIzd5%2BZd4HXkUtbRP2mEgc1iw04m7qhjrps5EG7l" rel="nofollow" target="_blank">https://github.com/wshobson/agents/blob/main/plugins/llm-application-dev/agents/ai-engineer.md</a></p><p>[26]<a href="https://link.segmentfault.com/?enc=hqr%2FUVNhNr%2Bey77xLyJQ1Q%3D%3D.DuvC%2FZsXJ7YFM3QBVHHZ45r1ATJivbPOiAGU4tPjSFhNryYwgK2QCPS5KW1eQBfZZZo%2FCYy1CDPwH0T1qJ0M2A%3D%3D" rel="nofollow" target="_blank">https://x.com/IanIsSoAwesome/status/1976662563699245358</a></p><p>[27]<a href="https://link.segmentfault.com/?enc=kidY0Qsm4u9q98B38Ry82Q%3D%3D.VybMEsEvYaoFzUF0hl61SZs3jZ7kJNUbrdqXwM2IUNk%3D" rel="nofollow" target="_blank">https://wisprflow.ai/</a></p><p>[28]<a href="https://link.segmentfault.com/?enc=aiVvRVa%2FOZEnyRc79a0VCA%3D%3D.bvv%2BhXhcRw48gHjy%2BLNNUMzplAmNOiATmaBmiyZDEzQ2%2FGjOuHD6Uonyk6J%2BhdgemUMQQguFp%2BdxX9YbM9CbcA%3D%3D" rel="nofollow" target="_blank">https://x.com/cannn064/status/1973415142302830878</a></p><p>[29]<a href="https://link.segmentfault.com/?enc=7ze6HS6A%2BPkgOVjimR0KEg%3D%3D.8UH605J%2F1TL7kQDqowrMqmccsQYDlFIBA1WkospXN1M%3D" rel="nofollow" target="_blank">https://conductor.build/</a></p><p>[30]<a href="https://link.segmentfault.com/?enc=IxyaB6HT22T25ZPNTPJMgQ%3D%3D.Pra7COioL0WmEOi4hBM%2F2CmyIDbN2WuBM%2FFY%2BLICxKk%3D" rel="nofollow" target="_blank">https://www.terragonlabs.com/</a></p><p>[31]<a href="https://link.segmentfault.com/?enc=LNOniWkMpSR8KJKulj5XUg%3D%3D.YaAH2aiVJuO5xJPagg6Gu0Z6w%2FCqCV%2FLZBVtM7sWOTtqjVq5HsbQSfKqE75GFDFuFk9E2tA6yarWn6lAMsWZtQ%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1973132707707113691</a></p><p>[32]<a href="https://link.segmentfault.com/?enc=9NC%2Bu0OObmO%2FF6YKY91Zfg%3D%3D.HXYqH3tIEtNWzaosZZZgM2a63zcddPbp2cbInBtUJniULmz3sRoURg8Dl8dunvv3" rel="nofollow" target="_blank">https://github.com/steipete/claude-code-mcp</a></p><p>[33]<a href="https://link.segmentfault.com/?enc=s%2FTWmC3eELINUOL6HJZQ2A%3D%3D.gvqyu%2Bk5ItpyANGpL7CjOj2K8xUKgo50x4rl3SmpRitzuou7s7abNKZIWa85tcal" rel="nofollow" target="_blank">https://github.com/steipete/bslog</a></p><p>[34]<a href="https://link.segmentfault.com/?enc=OmnTLoP6NzlUGrvwYwsVWw%3D%3D.Ue6DMSK6x8U3KiK3zDsa%2FL1iBgEyK7DkNo81VndS6VcVQIzc%2B5%2BR%2FDB17DPzf9yg" rel="nofollow" target="_blank">https://github.com/steipete/inngest</a></p><p>[35]<a href="https://link.segmentfault.com/?enc=ZbYUN5PdyWtRAG%2Fpxw5A0w%3D%3D.PB0VX4m4UlKl8cXsk8PwVI%2FRkky0Cwg7R3g42wukbWYjds4VQWVmmhvBKmekpLSl41XYxGokSVhEXAIwPaXjRQ%3D%3D" rel="nofollow" target="_blank">https://developer.chrome.com/blog/chrome-devtools-mcp</a></p><p>[36]<a href="https://link.segmentfault.com/?enc=PX84R1eRgflgBaSzD186qg%3D%3D.Na7MLCfPGyFKFrgWMv4Y1XjJ9hPyFssAhBAXEnQ%2FNTbuiwKc1Q6rMO1Q%2B485q9J8Rk6RcZaAUzHT7wAIqTz7KA%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1977762275302789197</a></p><p>[37]<a href="https://link.segmentfault.com/?enc=c4A07jaDnRjLZtVMLyejVQ%3D%3D.ZAqVAoRauPdDvtS3yhFcjtWL7e2oJxlpYNOKBo0BXyNpSa6YD9MLcvweEpd9PidFrhMJ17yFWq3Lns4%2BhDzjKg%3D%3D" rel="nofollow" target="_blank">https://x.com/steipete/status/1976985959242907656</a></p><p>[38]<a href="https://link.segmentfault.com/?enc=6tnLCueEnrfjOY%2B7aEgSQw%3D%3D.OoZi2ycu2eFl%2FSQBMTczDaupzoZuN8nuIBdAsF49%2F10%3D" rel="nofollow" target="_blank">https://knip.dev/</a></p><p>[39]<a href="https://link.segmentfault.com/?enc=15WEv5%2BJopiX9t9fenErnA%3D%3D.%2Brqw6VEWy1aMDQSSZz7ba6Y84zZ9sSU3eOPYPr1MsvQQafcHS5s2GJPaIjG7%2FLOxrWO0yzazhKJiY7QD5PKcCm9v0f%2BN%2BfL3BnAyodxXqEg%3D" rel="nofollow" target="_blank">https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide</a></p><p>[40]<a href="https://link.segmentfault.com/?enc=sGda5%2B0KeQLJWuRuUCsNkg%3D%3D.8A5s6BqfMy5z19MRgen73h53QqKp1fFjYZqnIu3tngldrqZScPUffaARvAiko7IFTEkWr0wz2%2BE82jbWv9Z2oA%3D%3D" rel="nofollow" target="_blank">https://x.com/Altimor/status/1975752110164578576</a></p><p>[41]<a href="https://link.segmentfault.com/?enc=Ot9jmmFdaxc7bxE4K0YHMA%3D%3D.4huIH7BkofGOfRC44Fnfb2%2BQhZFf1SkuU5Pj8Nj18KfVNQa1Ws3VVdah9gRn0EVr62FZCc%2FXpYPO%2F%2Fihjd0srA%3D%3D" rel="nofollow" target="_blank">https://simonwillison.net/2025/Oct/7/vibe-engineering/</a></p><p>[42]<a href="https://link.segmentfault.com/?enc=%2F07wYwAqCwZYe7nHXaTSMQ%3D%3D.eb9p2TNdhoaqrvSQnP7sAsjiU89VT%2BYd8pk9HwqhDrOLNlKZ29DLD6OCVPGAlK1A%2BGfxc3MSErvNOLJz0876bQ%3D%3D" rel="nofollow" target="_blank">https://x.com/thorstenball/status/1976224756669309195</a></p><p><strong><em>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</em></strong></p><p><strong>原文链接：</strong>  </p><p><a href="https://link.segmentfault.com/?enc=KKTyDRZmuAm7FuB3sZ5WZw%3D%3D.ipSan3yTkpOftxGqYkj%2FrS4vrBiOXiDJgDuSVPyk8qUOg8lVoVl8BguFigm7Gy9p" rel="nofollow" target="_blank">https://steipete.me/posts/just-talk-to-it</a></p>]]></description></item><item>    <title><![CDATA[多模一库——架构简化，能力翻倍 Kaiw]]></title>    <link>https://segmentfault.com/a/1190000047411980</link>    <guid>https://segmentfault.com/a/1190000047411980</guid>    <pubDate>2025-11-19 18:06:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>何为多模？</strong></h2><p>"多模数据库"这个词在如今的数据库领域里，已经不再是个新鲜词。所谓多模数据库：目标是通过 "<strong>整合与效率</strong> "解决传统 "单一模型数据库" 的局限性，<strong>用一个系统统一管理多种结构的数据，支持统一查询接口（如一套 SQL 扩展），减少数据同步和跨库操作的成本</strong>。它把多种数据库（如 MySQL、MongoDB、Influxdb）的能力整合到一个系统里，让用户不用为不同结构的数据搭建多个数据库。</p><h2><strong>什么场景需要多模？</strong></h2><p>举个例子，企业要存工厂设备类型信息（关系型）以及工厂各类设备（如拧紧机等）采集的数据（时序型），如果用 2 个独立数据库，会面临数据孤岛、跨库查询复杂、维护成本高的问题。而 KaiwuDB 作为国内面向物联网 AIoT 场景的分布式多模数据库代表，能够支持在同一实例同时创建时序库和关系库，融合处理包括结构化数据、非结构化数据在内的多模数据。KaiwuDB 通过<strong>单一数据库系统</strong>统一管理时序数据和关系数据，一库代替多库，从而实现技术架构的简化，并帮助客户降低开发和运维复杂度及成本。</p><p><img width="723" height="246" referrerpolicy="no-referrer" src="/img/bVdm6bj" alt="" title=""/></p><p>KaiwuDB一库代替关系库+时序库</p><h3><strong>KaiwuDB 3.0 多模融合的内核</strong></h3><p><img width="723" height="431" referrerpolicy="no-referrer" src="/img/bVdm6bl" alt="" title="" loading="lazy"/></p><p>KaiwuDB 多模架构图</p><h4><strong>1. 接入层：多源数据统一收敛</strong></h4><p>• <strong>协议兼容</strong>：JDBC/ODBC（关系数据）、RESTful API（时序 / 流式数据）、MQTT（IoT 设备直连）。</p><p>• <strong>多模透明性</strong>：应用无需感知底层数据模型，统一接口适配时序 / 关系 / 非结构化 数据写入。</p><h4><strong>2. 解析优化层：跨模型查询语句解析、编译与优化</strong></h4><p>• <strong>多模 SQL 处理</strong>：</p><p>① <strong>解析器</strong>：兼容标准 SQL + 时序扩展语法（如 time_bucket 函数、窗口函数）。</p><p>② <strong>优化器</strong>：自动识别数据模型（时序 / 关系），匹配最优执行路径。动态生成跨模执行计划，支持两种核心优化策略，跨模查询性能提升 5 倍以上。</p><ul><li>Outside-In：当关联查询中的关系数据量较少时，关系数据过滤后下推至时序引擎。</li><li>Inside-Out：当关联查询中的时序数据量较少时，时序数据过滤或预聚合后返回到关系引擎进行关联。</li></ul><p>• <strong>元数据管理</strong>：统一管理时序库（TS DATABASE）与关系库元数据，支持跨模关联索引。</p><h4><strong>3. 计算引擎层：模型自适应执行</strong></h4><p>• 专属跨模计算算子，支持亿级时序数据与千万级关系数据秒级关联（如设备状态时序表 JOIN 设备属性表）。</p><p>• 专属单设备数据扫描算子，增加单设备跨模查询性能。</p><p>• 算子内并行，提升扫描和聚合性能。</p><p>• 聚合计算下推到时序引擎，从源头端压降数据量。</p><h4><strong>4. 存储管理层：混合架构适配</strong></h4><p>• <strong>分层存储策略</strong>：</p><p>① <strong>成本优化</strong>：通过冷热介质差异化存储，降低整体存储成本 30%\~60%（时序场景下，冷数据占比通常 \&gt; 80%）。</p><p>② <strong>性能保障</strong>：热数据留存于高速介质，确保实时业务（如监控告警、实时报表）的低延迟需求。</p><p>③ <strong>灵活适配</strong>：支持自定义冷热规则、迁移策略与归档周期，适配 IoT 监控、工业日志等不同场景。</p><p>• <strong>多模适配机制</strong>：</p><table><thead><tr><th><strong>数据类型</strong></th><th><strong>存储结构</strong></th><th><strong>核心技术</strong></th></tr></thead><tbody><tr><td>时序数据</td><td>列式存储</td><td>自研压缩算法、就地计算技术、自研"主键标签"机制</td></tr><tr><td>关系数据</td><td>行式存储 + 主键索引</td><td>分布式事务、并行计算技术</td></tr></tbody></table><h4><strong>5. AI 扩展层（多模增强）</strong></h4><p>支持 TensorFlow/XGBoost 模型全生命周期管理，可直接通过 SQL 调用模型推理，实现 "数据存储 - 训练 - 预测" 一体化。</p><h3><strong>核心功能特性\&amp;价值</strong></h3><p>• <strong>一体化数据管理</strong></p><p>通过<strong>单一数据库系统</strong>统一管理时序数据和关系数据，可以简化技术架构，降低开发和运维复杂度及成本。</p><p>• <strong>高效时序数据处理能力</strong></p><p>针对时序场景，<strong>自研"主键标签"机制</strong>，内置时序特色函数，提升数据库的读写性能；百万级 / 秒写入、毫秒级查询响应，支持 400 万 + 测点实时接入。</p><p>• <strong>跨模查询</strong></p><p><strong>原生跨模关联查询，一套标准 SQL 操作两种数据</strong>，无需数据迁移。3.0 通过增加高效跨模连接算子，时序算子并行处理优化，跨模查询性能提升 5-10 倍。</p><p>• <strong>资源与成本优化</strong></p><p><strong>避免异构数据库间的数据同步</strong> ，减少数据冗余；<strong>通过计算下推</strong>，减少网络传输，降低数据处理时延。</p><p>• <strong>AI 深度赋能</strong></p><p>① <strong>DB For AI</strong>：内置预测分析引擎，支持 SQL 级模型全生命周期管理，通过内置函数实现数据价值主动挖掘。</p><p>② <strong>AI For DB</strong>：提供 Agent 智能体工具，借助 MCP 协议，结合 LLM 技术，将自然语言处理与 KaiwuDB 深度结合，赋能数据库自动化运维与智能管理。</p>]]></description></item><item>    <title><![CDATA[移动端反射探针格式用什么比较合理 侑虎科]]></title>    <link>https://segmentfault.com/a/1190000047411983</link>    <guid>https://segmentfault.com/a/1190000047411983</guid>    <pubDate>2025-11-19 18:05:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>1）移动端反射探针格式用什么比较合理<br/>2）不同平台打包热更容灾问题的解决方案</p><hr/><p>这是第453篇UWA技术知识分享的推送，精选了UWA社区的热门话题，涵盖了UWA问答、社区帖子等技术知识点，助力大家更全面地掌握和学习。</p><p>UWA社区主页：<a href="https://link.segmentfault.com/?enc=ZcJ2fOksBkMZkgXP%2FIAHgQ%3D%3D.YVblZbnHLM8JvaeWiOTrzT7r%2FgDx9aFIt%2FUrQmbHlNA%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA QQ群：793972859</p><p><strong>From UWA AI 问答</strong></p><p><strong>Q：移动端反射探针格式用什么比较合理？</strong></p><blockquote><p>A：在移动端使用反射探针时，选择合理的纹理格式是平衡画质与性能的关键。不恰当的格式会显著增加内存占用和带宽消耗，影响帧率和功耗。</p><p><strong>一、推荐格式</strong><br/><strong>1. ASTC（Adaptive Scalable Texture Compression）</strong><br/><strong>推荐理由：</strong><br/>这是目前移动平台（尤其是iOS和现代Android设备）上最理想的选择。ASTC具有极高的灵活性，支持从4x4到12x12等多种压缩比率，并且能同时处理RGB、RGBA以及HDR数据。</p><p><strong>应用场景：</strong></p><ul><li><strong>LDR探针：</strong>可选用ASTC 6x6或8x8，在保证足够清晰度的同时将体积压缩到最小。</li><li><strong>HDR探针：</strong>必须使用支持HDR的格式。ASTC 4x4 HDR是最佳选择，它能在较低的内存开销下保留高光等重要细节，非常适合需要真实感反射的场景。</li></ul><p><strong>2. ETC2/EAC</strong><br/><strong>推荐理由：</strong><br/>作为OpenGL ES 3.0的标准，ETC2兼容性极佳，几乎所有现代Android设备都支持。对于需要广泛兼容性的项目，它是可靠的备选方案。</p><p><strong>应用场景：</strong></p><ul><li><strong>LDR探针：</strong>使用ETC2 RGB。虽然压缩效率略逊于ASTC，但兼容性最好。</li><li><strong>注意：</strong>ETC2本身不支持原生HDR。如果必须在不支持ASTC HDR的设备上使用HDR探针，通常引擎会回退到未压缩的RGB16F或RGB111110F格式，但这会极大增加内存消耗，应尽量避免。</li></ul><p><strong>二、核心优化策略</strong></p><ul><li><strong>优先启用ASTC：</strong>在Unity的Player Settings中，为iOS和Android分别设置首选的ASTC格式。这能确保在支持的设备上自动使用最优的压缩方案。</li><li><strong>谨慎使用HDR：</strong>仅对需要捕捉强烈高光（如金属车漆、水面反光）的关键探针开启HDR。大部分环境反射使用LDR的ASTC格式即可满足需求，能大幅降低内存占用。</li><li><strong>控制分辨率：</strong>根据探针的作用范围和距离玩家的远近来调整分辨率。远离玩家或覆盖大范围区域的探针可使用64x64甚至32x32；而用于特写或关键物体（如角色）附近的探针才使用128x128或256x256。分辨率是影响内存和渲染时间的最主要因素。</li><li><strong>利用盒体投影（Box Projection）：</strong>对于室内或空间受限的环境，启用盒体投影能让一个探针更准确地模拟出角落和墙壁的反射，从而减少所需探针的数量，间接优化了整体性能。</li></ul><p>综上所述，ASTC是移动端反射探针的首选格式，特别是结合其HDR特性，能在性能和视觉质量之间取得最佳平衡。</p></blockquote><p><strong>分享链接：</strong><br/><a href="https://link.segmentfault.com/?enc=I2Tzd47%2Bjq36Fs%2BJpPw7qA%3D%3D.sbZIFFeLGlABHBboMCbqs0Ibql1uLtnYC%2BtYe6Z58ezwdTBc7V6sKXV6Lr5G%2FHRR4emUs2X5sDFlmq4j1pOjp5H05VHAHA1LXrRodThE51gOhlOmUGPuItfSC4ai%2FvhT" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=ND2uXnHrw1BClRch5BN8Xw%3D%3D.b2DxwcjmM%2B2uQJiqH7iJd9qZs0ruCHJqInmRubCOFM9pU%2BgG36wOPlqypaRf1S9yNR5uuuk3qXaIItnokKi6QJTgbDyBf8lIyBATRrDn0qGp4VJlwQ9qfkQR9SNTBz%2F%2F" rel="nofollow" target="_blank">https://www.uwa4d.com/main/uwa-shared.html?shareId=c97db263-6...</a></p><hr/><p><strong>From UWA AI 问答</strong></p><p><strong>Q：目前，不同平台的热更打包工作分散在不同的机器上进行。考虑到容灾需求，如果机器挂了或者硬盘挂了，重新打包可能导致玩家需要再次下载几GB的热更新资源。除了定期备份整个工程或Library文件夹之外，是否有更好的应对方案？另外，有没有什么方法能够确保不同机器、不同工程环境下，可以打出一致的AssetBundle包（Meta文件已经受版本管理控制）？</strong></p><blockquote><p>A：为确保多台机器在不同环境或地理位置下能打出一致的AssetBundle（AB包），并有效应对机器或硬盘故障导致的热更风险，需从构建一致性、缓存管理和容灾备份三个层面进行系统性设计。</p><p><strong>1. 确保构建一致性</strong></p><ul><li><strong>统一构建环境：</strong>所有打包机器必须使用完全相同的Unity版本、操作系统补丁、JDK/Python等依赖环境。建议通过Docker镜像或虚拟机模板固化环境，避免因环境差异导致GUID或序列化数据不一致。</li><li><strong>共享与锁定Library：</strong>Library文件夹中的ScriptAssemblies、BeeTreeCache等缓存直接影响AB生成。应将关键缓存目录设为只读，并通过网络存储（如NFS）或CI/CD流水线统一分发，确保所有打包机基于相同中间产物构建。</li><li><strong>禁用随机因子：</strong>关闭Random Seed、时间戳嵌入等可能导致输出差异的功能。若使用Addressables，确保Build Script和Provider配置完全一致。</li></ul><p><strong>2. 构建缓存与增量复用</strong></p><ul><li><strong>集中式构建缓存服务：</strong>采用Unity Cloud Build Cache或自建Artifactory/Nexus服务器，存储每次成功构建的AB包及其元数据（Hash、清单）。当某台打包机故障时，可直接从缓存拉取历史版本，无需重新全量构建。</li><li><strong>增量构建策略：</strong>结合AssetBundleManifest对比前后版本差异，仅对变更资源重新打包。配合内容分发网络（CDN）的差分更新（如BSDiff），可将热更包体积降至最低，即使重打也能快速恢复。</li></ul><p><strong>3. 容灾与高可用方案</strong></p><ul><li><strong>分布式备份机制：</strong>除定期备份工程外，应将每次成功的AB包及AssetBundleManifest实时同步至至少两个异地存储节点（如云对象存储S3/OSS），并启用版本保留策略。</li><li><strong>自动化切换流程：</strong>建立打包集群健康监测系统，一旦主打包机异常，自动触发备用机器从缓存或源码重建AB包，并校验输出Hash一致性。可通过脚本集成UWA的AssetBundle检测工具，验证资源完整性。</li><li><strong>内容寻址存储（CAS）：</strong>以AB包内容哈希作为唯一标识进行存储和分发，确保无论在哪台机器构建，只要输入相同，输出即可互换，从根本上规避路径或时间相关差异。</li></ul><p>综上，通过标准化环境、共享中间产物、集中缓存与多重备份，可实现跨机器稳定产出一致AB包，并在单点故障时快速恢复，最大限度减少玩家重复下载风险。</p></blockquote><p><strong>分享链接：</strong><br/><a href="https://link.segmentfault.com/?enc=PExAATiT7uHGP8%2FgCyMdbg%3D%3D.9cizNLRr0tBUw0%2F3mLw31vzUOIjdvt0CY%2B%2FnUO5fHZaDoXGN5jfv6AjghCaCpoEGu0nkJ5CfQ8sjQExiGysA8tagPfIcv7fLWslyHdoEhhT%2Bzb2%2BAiJxx65JCJNiPRDs" rel="nofollow" target="_blank"/><a href="https://link.segmentfault.com/?enc=x5LdEktsFPn4I5mWGAUd5Q%3D%3D.QuwT3mcmDKZ9QD1GDFg5narEl%2BZK3Nd6nHv5%2BkA9BQM3I392w0CAv2o37H5BM1afX8Ooo9SdVsRB0ud0d3C3f4Vkzgw966wvuI87glBiZo2nBxNaOrOr%2FnUoHpv38D6C" rel="nofollow" target="_blank">https://www.uwa4d.com/main/uwa-shared.html?shareId=517d5cb9-5...</a></p><p><strong>无论是社区里开发者们的互助讨论，还是AI基于知识沉淀的快速反馈，核心都是为了让每一个技术难题都有解、每一次踩坑都有回响。本期分享分别来自UWA AI问答和UWA问答社区，希望这些从真实开发场景中提炼的经验，能直接帮你解决当下的技术卡点，也让你在遇到同类问题时，能更高效地找到破局方向。</strong></p><p>封面图来源于网络</p><hr/><p>今天的分享就到这里。生有涯而知无涯，在漫漫的开发周期中，我们遇到的问题只是冰山一角，UWA社区愿伴你同行，一起探索分享。欢迎更多的开发者加入UWA社区。</p><p>UWA官网：<a href="https://link.segmentfault.com/?enc=8D2e%2FfHkYMyuCPuSQJVU0Q%3D%3D.bAq7rg6n74I1k5aGD2%2BOJ5PCZp%2F842u4AuM1WcQKSFY%3D" rel="nofollow" target="_blank">www.uwa4d.com</a><br/>UWA社区：<a href="https://link.segmentfault.com/?enc=jLG0OjCC3F9r4rVGFgDwnQ%3D%3D.8wCbRI82LIx1QhZHO%2FHAj5dij8kB5wT%2BUf0k0h9gEaQ%3D" rel="nofollow" target="_blank">community.uwa4d.com</a><br/>UWA学堂：<a href="https://link.segmentfault.com/?enc=mkHRCz4hsO93yRuVnZqQUw%3D%3D.jWQhcbcLGjZNmnVaC17cE%2Brssp5iTbwH20EfOr7WMGk%3D" rel="nofollow" target="_blank">edu.uwa4d.com</a><br/>官方技术QQ群：793972859</p>]]></description></item><item>    <title><![CDATA[怎么利用AI技术提升质量管控的精准性和效]]></title>    <link>https://segmentfault.com/a/1190000047412004</link>    <guid>https://segmentfault.com/a/1190000047412004</guid>    <pubDate>2025-11-19 18:05:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在当今高度竞争的工业环境中，质量管控已不再局限于传统的抽样检验与事后补救，而是逐步演变为覆盖产品全生命周期的系统性工程。随着智能制造浪潮的深入推进，质量管控的内涵与外延发生了根本性变革，其核心目标已从单纯符合标准转向驱动企业创新与竞争力提升。在这一转型过程中，广域铭岛作为工业互联网领域的先行者，通过技术赋能与实践探索，为制造业提供了全新的质量管控范式。<br/>传统质量管控模式长期受限于抽检滞后性、经验依赖性与数据孤岛问题，导致企业在面临复杂生产场景时难以实现精准干预。然而，广域铭岛依托Geega工业互联网平台，构建了一套融合物联网、人工智能与数字孪生技术的智能质控系统，彻底打破了这一僵局。该系统通过对生产全流程的实时监测与数据分析，将质量管控的触角延伸至工艺参数监控、异常预警、闭环优化等深层环节，实现了从“被动应对”到“主动预防”的质的飞跃。<br/>在具体实践中，广域铭岛的解决方案展现出显著的技术集成性与业务适配性。例如，在新能源汽车电池制造场景中，系统能够同步捕捉数百项工艺参数，并通过AI算法在毫秒级时间内识别异常，大幅降低了缺陷漏检率。同时，数字孪生技术的应用使得企业能够在虚拟空间中模拟产线运行状态，提前预测质量波动，为决策提供前瞻性依据。这种高度融合的质量管控模式不仅提升了产品一致性，更通过数据反哺研发与生产，形成了持续优化的增强闭环。<br/>尤为值得注意的是，广域铭岛在质量管控中创新性地引入了“人在环路”机制，强调人机协同的智能化路径。当系统检测到潜在质量风险时，会自动调取历史案例与专家知识库，辅助工程师进行精准决策，既发挥了算法的效率优势，又保留了人类经验的判断力。这种模式在领克汽车工厂等项目中已得到验证，不仅使客户投诉率显著下降，更意外推动了产品适销性的提升。<br/>面向未来，质量管控将进一步与5G、区块链、AR等新兴技术深度融合，走向更透明、可追溯、可协同的智慧阶段。广域铭岛在此过程中的探索——例如基于区块链的质量数据存证、跨企业质量云构建等——不仅为其合作伙伴带来了实质效益，也为中国制造业的换道超车提供了关键思路。可以说，当质量管控从成本中心蜕变为创新引擎，其所释放的价值将远超想象。</p>]]></description></item><item>    <title><![CDATA[马来西亚国际网络专线怎么开通？怎么收费？]]></title>    <link>https://segmentfault.com/a/1190000047412024</link>    <guid>https://segmentfault.com/a/1190000047412024</guid>    <pubDate>2025-11-19 18:04:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着中国企业出海东南亚的浪潮加速，马来西亚逐渐成为继越南、泰国之后的又一热门地区，地理位置优越，连接东南亚核心经济圈，物流与通信基础设施完善。</p><p>数字经济高速发展，马来西亚政府积极推动数字化转型，互联网普及率高达90%以上，TikTok、Shopee、电商与AI产业崛起：吸引大量中国跨境卖家与内容出海团队。</p><p>然而，在实际运营过程中，许多企业都面临同样的挑战，网络延迟高、数据丢包严重、访问速度慢、账号频繁被封。这就是为什么越来越多企业开始部署马来西亚国际网络专线的原因，可用于提升跨境访问速度，保障系统安全与通信稳定性。</p><p>一、马来西亚不同业务场景对网络的要求</p><p>不同的跨境场景对网络质量的要求差异很大。以下是几个典型使用场景的分析：<br/><img width="723" height="237" referrerpolicy="no-referrer" src="/img/bVdm6ca" alt="截屏2025-11-19 17.43.38.png" title="截屏2025-11-19 17.43.38.png"/></p><p>小结：</p><p>对于跨境直播、电商和广告场景，网络必须同时满足 稳定性 + 合规性 + 本地化IP。</p><p>而对于企业远程办公与数据同步，则更看重 专线的安全性与QoS保障。</p><p>二、马来西亚国际网络专线怎么选择?</p><p>当前市面上的跨境专线主要分为两种类型：传统国际专线 和 SD-WAN智能专线。</p><ol><li>传统国际网络专线</li></ol><p>原理：由运营商提供物理电路连接(如MPLS、IPLC)。</p><p>优点：高稳定性、低丢包率、安全性强。</p><p>缺点：价格高昂、部署周期长、扩展性差。</p><p>适合场景：大型集团企业、金融机构、政府项目。</p><ol start="2"><li>SD-WAN国际专线</li></ol><p>原理：通过智能调度、软件定义网络技术，动态分配多条线路带宽。</p><p>优点：成本更低、部署快、灵活性强、支持多地分支组网。</p><p>缺点：对服务商技术实力要求高，建议选择像OSDWAN这样有合法资质的服务商，走的跟三大运营一样的线路。</p><p>适合场景：跨境电商、直播公司、外贸企业、中小型出海团队。</p><p>推荐选择方案：</p><p>如果你是中小型出海企业或跨境直播团队，建议优先选择 SD-WAN国际专线，既能保障访问速度，又能节省大量成本。</p><p>三、马来西亚国际网络专线怎么开通?</p><p>开通马来西亚国际网络专线的流程并不复杂，一般分为以下四步：</p><ol><li>选择合适的服务商(推荐OSDWAN)</li></ol><p>在选择跨境网络服务商时，应重点考察：</p><p>是否具备国际专线接入资质;</p><p>是否提供SD-WAN技术;</p><p>是否支持马来西亚节点及住宅IP接入;</p><p>是否有7×24小时运维保障。</p><p>推荐服务商：OSDWAN</p><p>OSDWAN 是国内领先的 跨境网络与SD-WAN服务提供商，</p><p>在东南亚多个国家(包括马来西亚、新加坡、越南、泰国等)均部署本地节点，</p><p>能为企业提供合规、安全、高速的 国际专线+本地化IP 解决方案。</p><ol start="2"><li>准备开通所需资料</li></ol><p>不同类型专线对应不同资料，一般包括：</p><p>企业营业执照复印件;</p><p>使用场景说明(办公、直播、社媒运营等);</p><p>专线带宽与节点需求确认;</p><ol start="3"><li>网络部署与调试</li></ol><p>OSDWAN支持多种交付方式：</p><p>云端接入：无需硬件设备，快速部署;</p><p>本地CPE设备：适用于企业分支机构;</p><p>当日即可开通，一分钟即可连接使用。</p><ol start="4"><li>后期运维与技术支持</li></ol><p>OSDWAN提供7×24小时网络监控与维护，</p><p>支持带宽升级、节点扩展、专线优化等定制服务，</p><p>五、马来西亚国际网络专线收费标准(以OSDWAN为例)</p><p>OSDWAN提供灵活的计费模式，支持 按月、按年、带宽定制 多种方式。具体如下：</p><p>办公账号版：690元/年，适合外贸SOHO或小工作室使用</p><p>社媒运营套餐：1500元/年起：，TikTok运营、社媒矩阵、店铺运营</p><p>标准版：10000元/年，适合10人以内团队使用或普清TK直播</p><p>企业版：自定义，100+地区的线路和IP可选，自由组合配置</p><p>并且我们还提供多个地区的住宅IP，其中泰国独享静态IP费用参考价为50-200元/月。</p><p><img width="723" height="339" referrerpolicy="no-referrer" src="/img/bVdm4sv" alt="image.png" title="image.png" loading="lazy"/></p><p>按年购买更划算：</p><p>OSDWAN针对企业客户提供年度优惠套餐，比传统专线节省30%–50%的成本，如需优惠可添加我们的选型顾问。</p><p>六、常见问答(FAQ)</p><p>Q1：能否一条专线连接多个国家？</p><p>可以。OSDWAN SD-WAN架构支持多国家多节点组网(如马来西亚+新加坡+越南)。</p><p>Q2：需要安装硬件设备吗？</p><p>不一定。云端部署方案可直接接入，无需本地设备。</p><p>Q4：个人可以使用国际专线吗？</p><p>建议企业或团队用户申请，个人使用需要通过公司来申请。</p><p>OSDWAN作为国内专业的跨境网络服务商，为出海企业提供合规、高速、稳定的网络解决方案，支持硬件、软件方案灵活部署。</p><p>OSDWAN在全球的数据中心节点50个，POP节点超过200个，可以为出海企业提供海外加速、SaaS加速、SD-WAN组网、跨境组网、云专线等产品服务，助力中国企业开拓国际市场。</p>]]></description></item><item>    <title><![CDATA[总结 看煙花墜落的淺殤 ]]></title>    <link>https://segmentfault.com/a/1190000047412042</link>    <guid>https://segmentfault.com/a/1190000047412042</guid>    <pubDate>2025-11-19 18:04:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>vue3为什么要使用proxy替换原来的</p><p>在 Vue 3 中，使用 <code>Proxy</code> 替代 Vue 2 中的 <code>Object.defineProperty</code> 是一个重大的底层架构升级，主要出于以下几个关键原因：</p><hr/><h3>✅ 1. <strong><code>Object.defineProperty</code> 的局限性</strong></h3><p>Vue 2 使用 <code>Object.defineProperty</code> 来实现响应式系统，但它存在以下严重限制：</p><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td>❌ 无法检测对象属性的<strong>动态添加或删除</strong></td><td>例如：<code>vm.user.name = 'Alice'</code> 是可以的，但 <code>vm.user['newProp'] = 'value'</code> 不会被响应式检测到。</td></tr><tr><td>❌ 无法检测数组索引的变更或长度变化</td><td>例如：<code>vm.items[0] = newVal</code> 或 <code>vm.items.length = 0</code> 不会触发更新。</td></tr><tr><td>❌ 无法监听对象的<strong>新增属性</strong></td><td>如 <code>vm.obj.newKey = 'val'</code> 不会触发响应式更新。</td></tr><tr><td>❌ 无法监听数组的<strong>索引赋值</strong></td><td><code>vm.arr[1] = 'new'</code> 不会被响应。</td></tr></tbody></table><p>这些限制导致开发者必须使用 <code>Vue.set(obj, key, value)</code> 或 <code>vm.$set</code> 来手动触发更新，体验差且容易出错。</p><hr/><h3>✅ 2. <strong><code>Proxy</code> 的优势（Vue 3 的核心改进）</strong></h3><p><code>Proxy</code> 是 ES6 引入的<strong>元编程</strong>特性，它可以在<strong>对象操作层面</strong>进行拦截，具有以下强大能力：</p><table><thead><tr><th>能力</th><th>说明</th></tr></thead><tbody><tr><td>✅ <strong>完整拦截对象的所有操作</strong></td><td>包括：属性读取、设置、删除、枚举、函数调用等。</td></tr><tr><td>✅ <strong>支持动态属性的监听</strong></td><td>无论是新增属性还是删除属性，都能自动响应。</td></tr><tr><td>✅ <strong>支持数组索引和长度的监听</strong></td><td><code>arr[0] = val</code>、<code>arr.length = 0</code> 都能被监听到。</td></tr><tr><td>✅ <strong>支持嵌套对象的深层响应式</strong></td><td>不需要递归处理，<code>Proxy</code> 本身就能自动代理深层属性。</td></tr><tr><td>✅ <strong>更高效的性能</strong></td><td>无需对每个属性进行 <code>defineProperty</code>，减少了初始化开销。</td></tr></tbody></table><hr/><h3>✅ 3. <strong>Vue 3 的响应式系统如何使用 Proxy？</strong></h3><p>Vue 3 通过 <code>reactive()</code> 和 <code>ref()</code> 创建响应式对象，其底层使用 <code>Proxy</code> 实现：</p><pre><code class="js">const state = reactive({ count: 0, user: { name: 'Alice' } })

// ✅ 动态添加属性也能响应
state.newProp = 'hello' // 自动响应

// ✅ 数组索引修改也能响应
state.items[0] = 'new item'

// ✅ 嵌套对象变化也能响应
state.user.name = 'Bob'</code></pre><blockquote>⚠️ 注意：<code>Proxy</code> 不能完全替代 <code>Object.defineProperty</code> 的所有用途，但对响应式系统来说，它的能力远超前者。</blockquote><hr/><h3>✅ 4. <strong>其他优势</strong></h3><ul><li><strong>更好的类型推导支持</strong>：<code>Proxy</code> 的类型系统更清晰，有助于 TS 类型推导。</li><li><strong>更小的 bundle size</strong>：Vue 3 响应式系统更轻量，因为避免了递归遍历对象。</li><li><strong>更灵活的扩展性</strong>：可以轻松实现更复杂的响应式逻辑（如 computed、watch 等）。</li></ul><hr/><h3>📌 总结：为什么 Vue 3 必须用 Proxy？</h3><table><thead><tr><th>对比项</th><th>Vue 2 (<code>Object.defineProperty</code>)</th><th>Vue 3 (<code>Proxy</code>)</th></tr></thead><tbody><tr><td>动态属性</td><td>❌ 不支持</td><td>✅ 支持</td></tr><tr><td>数组索引</td><td>❌ 不支持</td><td>✅ 支持</td></tr><tr><td>深层响应</td><td>❌ 需递归</td><td>✅ 自动支持</td></tr><tr><td>性能</td><td>低（递归开销）</td><td>高（按需代理）</td></tr><tr><td>代码复杂度</td><td>高（需手动处理边界）</td><td>低（原生支持）</td></tr></tbody></table><hr/><h3>✅ 附加说明：Proxy 的兼容性</h3><p>虽然 <code>Proxy</code> 是 ES6 特性，但 Vue 3 已在生产环境中广泛使用，<strong>现代浏览器（Chrome 60+、Firefox 60+、Safari 10+）均已支持</strong>，且通过构建工具（如 Vite、Webpack）可兼容旧环境（通过 polyfill 或降级）。</p><hr/><h3>🔗 参考资料</h3><ul><li><a href="https://link.segmentfault.com/?enc=CUNENyajujF6y3mbRnmMpg%3D%3D.PG421ovj2LDHAz67%2BhemtkWTSjyGppkGY7zGxzp8C9yrD9%2BipXByEO4v1%2FkSXHT5gBY%2B%2FBI1eiRLZ7XnPtZJKw%3D%3D" rel="nofollow" target="_blank">Vue 3 官方文档 - 响应式原理</a></li><li><a href="https://link.segmentfault.com/?enc=om6u1sljGEWOAAbdpOV%2BpA%3D%3D.5zA9HYhnpAXow1yZhBkFdjxSOxZjQpDPEEcEc1dgMIs%2FagSZV8MN724F84cRyuRzIvv7mJ%2Bn%2BPQZrBylwIYXyF5B3MpC3Nz3X5cffhA3Y8gvUvSUgJVkNCwXN4zxAjAC" rel="nofollow" target="_blank">MDN - Proxy</a></li></ul><hr/><p>vue项目前端怎么实现一个组件可以供多个项目调用</p><p>在 Vue 项目中实现一个组件可供多个项目调用，通常有以下几种方式。根据你的使用场景（是否在企业内部共享、是否需要版本管理、是否独立发布），推荐以下 <strong>最佳实践方案</strong>：</p><hr/><h2>✅ 推荐方案：<strong>将组件封装为独立的 Vue 组件库（npm 包）</strong></h2><h3>1. 创建独立的组件库项目（如 <code>@yourorg/vue-components</code>）</h3><h4>✅ 目标：</h4><p>将通用组件（如按钮、表单、弹窗等）封装成一个可发布的 npm 包，供多个项目通过 <code>npm install</code> 引入。</p><hr/><h3>📁 项目结构示例（独立组件库）</h3><pre><code>vue-component-library/
├── src/
│   ├── components/
│   │   ├── MyButton.vue
│   │   ├── MyModal.vue
│   │   └── MyInput.vue
│   └── index.js          # 统一导出
├── package.json
├── vite.config.js        # 或 webpack.config.js
├── README.md
└── .npmignore</code></pre><hr/><h3>✅ 步骤一：初始化组件库项目</h3><pre><code class="bash">mkdir vue-component-library &amp;&amp; cd vue-component-library
npm init -y</code></pre><hr/><h3>✅ 步骤二：安装依赖</h3><pre><code class="bash">npm install vue@^3.x --save
npm install @vitejs/plugin-vue --save-dev
npm install @rollup/plugin-node-resolve --save-dev
npm install rollup-plugin-vue --save-dev</code></pre><hr/><h3>✅ 步骤三：配置 <code>vite.config.js</code></h3><pre><code class="js">// vite.config.js
import { defineConfig } from 'vite'
import vue from '@vitejs/plugin-vue'

export default defineConfig({
  build: {
    lib: {
      entry: 'src/index.js',
      name: 'MyComponents',
      fileName: 'my-components',
    },
    outDir: 'dist',
    emptyOutDir: true,
    rollupOptions: {
      external: ['vue'],
      output: {
        globals: {
          vue: 'Vue'
        }
      }
    }
  },
  plugins: [vue()]
})</code></pre><hr/><h3>✅ 步骤四：编写组件导出逻辑（<code>src/index.js</code>）</h3><pre><code class="js">// src/index.js
import MyButton from './components/MyButton.vue'
import MyModal from './components/MyModal.vue'
import MyInput from './components/MyInput.vue'

const components = [
  MyButton,
  MyModal,
  MyInput
]

const install = (app) =&gt; {
  components.forEach(component =&gt; {
    app.component(component.name, component)
  })
}

export { MyButton, MyModal, MyInput }
export default {
  install
}</code></pre><hr/><h3>✅ 步骤五：配置 <code>package.json</code></h3><pre><code class="json">{
  "name": "@yourorg/vue-components",
  "version": "1.0.0",
  "description": "A shared Vue component library",
  "main": "dist/my-components.js",
  "module": "dist/my-components.esm.js",
  "unpkg": "dist/my-components.js",
  "files": [
    "dist"
  ],
  "scripts": {
    "build": "vite build",
    "dev": "vite build --watch"
  },
  "keywords": ["vue", "component", "library"],
  "author": "yourname@yourorg.com",
  "license": "MIT",
  "dependencies": {
    "vue": "^3.0.0"
  },
  "devDependencies": {
    "@vitejs/plugin-vue": "^4.0.0",
    "rollup-plugin-vue": "^6.0.0"
  }
}</code></pre><hr/><h3>✅ 步骤六：发布到私有或公共 npm 仓库</h3><pre><code class="bash">npm login
npm publish --access public  # 公开
# 或
npm publish --access restricted  # 私有（如使用 Nexus、Verdaccio、GitHub Packages）</code></pre><blockquote>📌 <strong>企业内部推荐使用私有 npm 仓库</strong>（如 <a href="https://link.segmentfault.com/?enc=leibXgILo%2FXIZLJYaofnSQ%3D%3D.hCW4dWeni5fvWfNUM6tCt%2BIXW9jThW2cpkPO9Y0xbJM%3D" rel="nofollow" target="_blank">Verdaccio</a> 或 <a href="https://link.segmentfault.com/?enc=YdsfUCdiZiQgbVrkx61N5g%3D%3D.1XJG5bU0NSF85kaDajt3M91j5BSHyOYR%2B4iPMPyV8Zxg8F3RL%2Ba%2F7bH3x2j0hRa2" rel="nofollow" target="_blank">GitHub Packages</a>）</blockquote><hr/><h3>✅ 步骤七：在多个 Vue 项目中使用组件库</h3><pre><code class="bash"># 在任意 Vue 项目中
npm install @yourorg/vue-components</code></pre><h4>引入并使用组件</h4><pre><code class="js">// main.js 或 main.ts
import { createApp } from 'vue'
import App from './App.vue'
import MyComponents from '@yourorg/vue-components'

const app = createApp(App)
app.use(MyComponents)
app.mount('#app')</code></pre><h4>在模板中使用组件</h4><pre><code class="vue">&lt;template&gt;
  &lt;MyButton type="primary"&gt;点击我&lt;/MyButton&gt;
  &lt;MyModal v-model="visible"&gt;这是一个弹窗&lt;/MyModal&gt;
&lt;/template&gt;

&lt;script setup&gt;
import { ref } from 'vue'

const visible = ref(false)
&lt;/script&gt;</code></pre><hr/><h2>✅ 可选增强功能</h2><table><thead><tr><th>功能</th><th>建议方案</th></tr></thead><tbody><tr><td>支持按需引入</td><td>使用 <code>babel-plugin-import</code> 或 <code>unplugin-vue-components</code></td></tr><tr><td>支持主题/样式定制</td><td>使用 CSS 变量或 <code>sass</code>/<code>less</code> 变量</td></tr><tr><td>支持文档网站</td><td>使用 <code>VitePress</code> 或 <code>Docusaurus</code></td></tr><tr><td>支持 TypeScript</td><td>添加 <code>tsconfig.json</code>，使用 <code>vue-tsc</code> 编译</td></tr></tbody></table><hr/><h2>🚫 不推荐的方式（避免）</h2><ul><li>直接复制组件文件到多个项目 → 不可维护，难以升级</li><li>使用 <code>import</code> 路径硬编码 → 耦合严重</li><li>用 <code>git submodule</code> → 复杂，不适合前端组件共享</li></ul><hr/><h2>✅ 总结：最佳实践</h2><table><thead><tr><th>项目</th><th>建议</th></tr></thead><tbody><tr><td>组件共享</td><td>✅ 封装为 npm 包</td></tr><tr><td>公共组件</td><td>✅ 统一命名空间（如 <code>@org/component-name</code>）</td></tr><tr><td>版本管理</td><td>✅ 使用 Git + npm 发布版本</td></tr><tr><td>多项目引用</td><td>✅ <code>npm install @org/vue-components</code></td></tr><tr><td>开发调试</td><td>✅ 使用 <code>npm link</code> 或 <code>yarn link</code> 本地调试</td></tr></tbody></table><hr/><p>如果你是平安集团内部项目，建议使用 <strong>平安内部私有 npm 仓库</strong>（如 <code>npm.paas.pingan.com</code>）发布和管理组件库。</p><p>针对一句话需求你会怎么做</p><p>当面对一个<strong>前端一句话需求</strong>时，我会按照以下步骤进行结构化分析与处理，确保快速、准确地交付高质量的实现方案：</p><hr/><h3>✅ 一、需求澄清（关键第一步）</h3><p>即使是一句话需求，也要先确认核心意图。例如：</p><blockquote>用户说：“我想做一个登录页面。”</blockquote><p>我会反问或拆解成以下问题：</p><ol><li><strong>功能目标</strong>：是普通账号密码登录？还是支持第三方登录（微信/短信）？</li><li><strong>设计风格</strong>：是否有UI规范？是否需要适配品牌色、图标等？</li><li><strong>交互要求</strong>：是否需要表单校验、记住密码、验证码、加载状态？</li><li><strong>技术栈</strong>：项目使用 Vue / React / Angular？是否已有组件库（如 Element UI、Ant Design）？</li><li><strong>部署环境</strong>：是否需要适配移动端？是否需国际化？</li></ol><p>👉 <strong>目的</strong>：避免“我以为你想要的” ≠ “你真正想要的”。</p><hr/><h3>✅ 二、技术实现路径（以常见场景为例）</h3><p>假设需求是：  </p><blockquote>“做一个带用户名和密码输入框的登录页面，有提交按钮和错误提示。”</blockquote><h4>📌 实现方案（以 Vue 3 + Vite + Element Plus 为例）</h4><pre><code class="vue">&lt;!-- [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/views/LoginView.vue"] --&gt;
&lt;template&gt;
  &lt;div class="login-container"&gt;
    &lt;el-card class="login-card"&gt;
      &lt;h2 class="title"&gt;用户登录&lt;/h2&gt;
      &lt;el-form :model="form" :rules="rules" ref="formRef" label-width="80px"&gt;
        &lt;el-form-item label="用户名" prop="username"&gt;
          &lt;el-input v-model="form.username" placeholder="请输入用户名" /&gt;
        &lt;/el-form-item&gt;
        &lt;el-form-item label="密码" prop="password"&gt;
          &lt;el-input
            v-model="form.password"
            type="password"
            placeholder="请输入密码"
            show-password
          /&gt;
        &lt;/el-form-item&gt;
        &lt;el-form-item&gt;
          &lt;el-button type="primary" @click="handleLogin" :loading="loading"&gt;
            登录
          &lt;/el-button&gt;
        &lt;/el-form-item&gt;
      &lt;/el-form&gt;
      &lt;p v-if="errorMsg" class="error-msg"&gt;{{ errorMsg }}&lt;/p&gt;
    &lt;/el-card&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script setup&gt;
import { ref } from 'vue'
import { ElMessage } from 'element-plus'

const formRef = ref()
const form = ref({
  username: '',
  password: ''
})

const loading = ref(false)
const errorMsg = ref('')

const rules = {
  username: [{ required: true, message: '请输入用户名', trigger: 'blur' }],
  password: [{ required: true, message: '请输入密码', trigger: 'blur' }]
}

const handleLogin = async () =&gt; {
  await formRef.value.validate(async (valid) =&gt; {
    if (!valid) return

    loading.value = true
    errorMsg.value = ''

    try {
      // 模拟 API 请求
      const res = await fetch('/api/login', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(form.value)
      })

      if (res.ok) {
        ElMessage.success('登录成功')
        // 跳转首页或其他页面
        window.location.href = '/dashboard'
      } else {
        const data = await res.json()
        errorMsg.value = data.message || '登录失败'
      }
    } catch (err) {
      errorMsg.value = '网络错误，请稍后重试'
    } finally {
      loading.value = false
    }
  })
}
&lt;/script&gt;

&lt;style scoped&gt;
.login-container {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 100vh;
  background: #f0f2f5;
}
.login-card {
  width: 380px;
  padding: 30px;
  border-radius: 8px;
}
.title {
  text-align: center;
  margin-bottom: 20px;
  color: #333;
}
.error-msg {
  color: #f56c6c;
  font-size: 12px;
  text-align: center;
}
&lt;/style&gt;</code></pre><hr/><h3>✅ 三、后续建议（提升体验）</h3><table><thead><tr><th>功能点</th><th>建议</th></tr></thead><tbody><tr><td>记住密码</td><td>使用 <code>localStorage</code>（注意安全）</td></tr><tr><td>密码可见开关</td><td>添加 eye icon 控件</td></tr><tr><td>忘记密码</td><td>添加“忘记密码”链接</td></tr><tr><td>移动端适配</td><td>使用 <code>flex</code> + <code>vw/vh</code> 布局</td></tr><tr><td>国际化</td><td>引入 <code>vue-i18n</code> 支持多语言</td></tr></tbody></table><hr/><h3>✅ 四、交付物清单</h3><table><thead><tr><th>类型</th><th>文件路径</th></tr></thead><tbody><tr><td>登录页面组件</td><td><code>/src/views/LoginView.vue</code></td></tr><tr><td>样式文件</td><td><code>/src/views/LoginView.vue</code>（内联）</td></tr><tr><td>API 调用封装</td><td><code>/src/api/auth.js</code>（建议后续添加）</td></tr></tbody></table><hr/><h3>✅ 总结：一句话需求 → 五步走</h3><ol><li><strong>澄清需求</strong> → 明确边界与期望</li><li><strong>选型技术栈</strong> → 匹配项目现状</li><li><strong>编写核心代码</strong> → 模块化、可复用</li><li><strong>添加交互与容错</strong> → 提升健壮性</li><li><strong>输出完整路径</strong> → 便于快速集成</li></ol><hr/><p>for-in和for-of的区别</p><p><code>for-in</code> 和 <code>for-of</code> 是 JavaScript 中用于遍历数据结构的两种不同语法，它们在用途、遍历对象和数组时的行为上有显著区别。以下是它们的详细对比：</p><hr/><h3>✅ <strong>1. 用途与适用对象</strong></h3><table><thead><tr><th>语法</th><th>适用对象</th><th>主要用途</th></tr></thead><tbody><tr><td><code>for-in</code></td><td>对象（Object）<br/>可迭代对象（如数组、字符串等）</td><td>遍历<strong>对象的可枚举属性名</strong>（键名）</td></tr><tr><td><code>for-of</code></td><td>可迭代对象（Iterable）<br/>如数组、字符串、Map、Set 等</td><td>遍历<strong>可迭代对象的值</strong>（元素/成员）</td></tr></tbody></table><hr/><h3>✅ <strong>2. 遍历内容对比</strong></h3><h4>🔹 <code>for-in</code>：遍历<strong>键名（key）</strong></h4><pre><code class="js">const obj = { a: 1, b: 2, c: 3 };

for (const key in obj) {
  console.log(key);     // 输出: a, b, c
  console.log(obj[key]); // 输出: 1, 2, 3
}</code></pre><blockquote>⚠️ 注意：<code>for-in</code> 会遍历<strong>所有可枚举的自有属性和继承属性</strong>，包括原型链上的属性。</blockquote><h4>🔹 <code>for-of</code>：遍历<strong>值（value）</strong></h4><pre><code class="js">const arr = [10, 20, 30];

for (const value of arr) {
  console.log(value); // 输出: 10, 20, 30
}</code></pre><blockquote>✅ <code>for-of</code> 仅遍历<strong>可迭代对象的元素值</strong>，不会遍历原型链。</blockquote><hr/><h3>✅ <strong>3. 常见使用场景对比</strong></h3><table><thead><tr><th>场景</th><th>推荐语法</th></tr></thead><tbody><tr><td>遍历对象的属性名（如检查对象有哪些字段）</td><td>✅ <code>for-in</code></td></tr><tr><td>遍历数组中的元素</td><td>✅ <code>for-of</code></td></tr><tr><td>遍历字符串的字符</td><td>✅ <code>for-of</code></td></tr><tr><td>遍历 Map 的键值对</td><td>✅ <code>for-of</code></td></tr><tr><td>遍历 Set 的元素</td><td>✅ <code>for-of</code></td></tr><tr><td>遍历对象的值（如 <code>Object.values(obj)</code>）</td><td>❌ <code>for-in</code> 不直接提供值，需 <code>obj[key]</code></td></tr></tbody></table><hr/><h3>✅ <strong>4. 关键区别总结</strong></h3><table><thead><tr><th>特性</th><th><code>for-in</code></th><th><code>for-of</code></th></tr></thead><tbody><tr><td>遍历对象</td><td>✅（属性名）</td><td>❌（不能直接用于普通对象）</td></tr><tr><td>遍历数组</td><td>✅（但建议用 <code>for-of</code>）</td><td>✅（推荐）</td></tr><tr><td>遍历字符串</td><td>✅</td><td>✅</td></tr><tr><td>遍历 Map/Set</td><td>❌（需用 <code>for-of</code>）</td><td>✅</td></tr><tr><td>是否遍历原型链</td><td>✅（会包含继承属性）</td><td>❌（只遍历自身）</td></tr><tr><td>是否安全（推荐）</td><td>❌（需配合 <code>hasOwnProperty</code>）</td><td>✅（更安全、直观）</td></tr></tbody></table><hr/><h3>✅ <strong>5. 实际建议</strong></h3><ul><li><strong>遍历数组、字符串、Map、Set 等可迭代对象 → 使用 <code>for-of</code></strong></li><li><strong>遍历对象的属性名 → 使用 <code>for-in</code>，但需注意原型链问题</strong></li><li><strong>避免在数组上使用 <code>for-in</code></strong>，除非你明确知道它在做什么。</li></ul><hr/><h3>🛠️ 示例：安全使用 <code>for-in</code></h3><pre><code class="js">const obj = { a: 1, b: 2 };

for (const key in obj) {
  if (obj.hasOwnProperty(key)) { // 避免原型链属性
    console.log(key, obj[key]);
  }
}</code></pre><hr/><h3>✅ 总结一句话：</h3><blockquote><strong><code>for-in</code> 用于遍历对象的键（key），<code>for-of</code> 用于遍历可迭代对象的值（value）</strong>。</blockquote><hr/><p>如需代码示例，可参考以下路径生成代码块：</p><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/for-in-vs-for-of-example.js"]

// 示例：for-in 遍历对象属性
const person = { name: "Alice", age: 30 };
for (const key in person) {
  if (person.hasOwnProperty(key)) {
    console.log(`${key}: ${person[key]}`);
  }
}

// 示例：for-of 遍历数组
const numbers = [1, 2, 3];
for (const num of numbers) {
  console.log(num);
}</code></pre><p>前端怎么处理数据并发</p><p>在前端开发中，处理数据并发的核心目标是：<strong>避免多个异步请求导致的数据冲突、状态混乱或重复提交</strong>。以下是常见的处理策略和最佳实践，适用于现代前端框架（如 React、Vue、Angular）。</p><hr/><h2>✅ 一、常见的并发问题场景</h2><ol><li><strong>多次快速点击按钮触发多次请求</strong></li><li><strong>表单提交时未禁用按钮，导致重复提交</strong></li><li><strong>多个组件同时请求同一数据，造成重复请求</strong></li><li><strong>请求响应顺序错乱，导致状态错乱</strong></li></ol><hr/><h2>✅ 二、解决方案</h2><h3>1. <strong>防抖（Debounce） + 节流（Throttle）</strong></h3><p>适用于频繁触发的操作，如搜索、滚动加载。</p><pre><code class="js">// 防抖：延迟执行，若在延迟时间内再次触发则重置计时
function debounce(func, delay) {
  let timer;
  return function (...args) {
    clearTimeout(timer);
    timer = setTimeout(() =&gt; func.apply(this, args), delay);
  };
}

// 节流：限制执行频率，比如每 300ms 执行一次
function throttle(func, delay) {
  let lastTime = 0;
  return function (...args) {
    const now = Date.now();
    if (now - lastTime &gt;= delay) {
      func.apply(this, args);
      lastTime = now;
    }
  };
}</code></pre><p><strong>使用示例：</strong></p><pre><code class="js">const searchHandler = debounce(async (query) =&gt; {
  const res = await fetch(`/api/search?q=${query}`);
  // 更新 UI
}, 500);</code></pre><hr/><h3>2. <strong>请求去重（Request Deduplication）</strong></h3><p>避免同一请求被多次发送。</p><h4>✅ 方法一：使用 <code>AbortController</code>（推荐）</h4><pre><code class="js">const pendingRequests = new Map();

function fetchWithDedupe(url, options = {}) {
  const key = url + JSON.stringify(options);

  if (pendingRequests.has(key)) {
    // 取消之前的请求
    pendingRequests.get(key).abort();
  }

  const controller = new AbortController();
  const request = fetch(url, { ...options, signal: controller.signal });

  pendingRequests.set(key, controller);

  request
    .then(res =&gt; {
      pendingRequests.delete(key);
      return res;
    })
    .catch(err =&gt; {
      if (err.name !== 'AbortError') {
        console.error('Request failed:', err);
      }
      pendingRequests.delete(key);
    });

  return request;
}</code></pre><hr/><h3>3. <strong>请求队列 + 串行/并行控制</strong></h3><p>适用于必须按顺序处理或限制并发数的场景。</p><pre><code class="js">class RequestQueue {
  constructor(maxConcurrent = 3) {
    this.queue = [];
    this.running = 0;
    this.maxConcurrent = maxConcurrent;
  }

  add(promiseCreator) {
    return new Promise((resolve, reject) =&gt; {
      this.queue.push({
        promiseCreator,
        resolve,
        reject,
      });
      this.process();
    });
  }

  async process() {
    if (this.running &gt;= this.maxConcurrent || this.queue.length === 0) return;

    this.running++;
    const task = this.queue.shift();
    try {
      const result = await task.promiseCreator();
      task.resolve(result);
    } catch (err) {
      task.reject(err);
    } finally {
      this.running--;
      this.process(); // 继续处理下一个
    }
  }
}

// 使用
const queue = new RequestQueue(2); // 最大并发 2

queue.add(() =&gt; fetch('/api/data1'))
  .then(res =&gt; console.log('1 done'));

queue.add(() =&gt; fetch('/api/data2'))
  .then(res =&gt; console.log('2 done'));</code></pre><hr/><h3>4. <strong>状态管理 + 加载状态控制</strong></h3><p>使用状态管理（如 Redux、Pinia、Zustand）来控制请求状态。</p><h4>✅ 示例（Vue + Pinia）</h4><pre><code class="js">// store/userStore.js
import { defineStore } from 'pinia';

export const useUserStore = defineStore('user', {
  state: () =&gt; ({
    userData: null,
    loading: false,
    lastFetch: null,
  }),

  actions: {
    async fetchUser(id) {
      if (this.loading) return; // 防止重复请求
      if (this.lastFetch &amp;&amp; Date.now() - this.lastFetch &lt; 5000) {
        return; // 5秒内不重复请求
      }

      this.loading = true;
      try {
        const res = await fetch(`/api/users/${id}`);
        this.userData = await res.json();
        this.lastFetch = Date.now();
      } catch (err) {
        console.error('Fetch failed:', err);
      } finally {
        this.loading = false;
      }
    },
  },
});</code></pre><p>在组件中使用：</p><pre><code class="vue">&lt;template&gt;
  &lt;button @click="fetchUser(1)" :disabled="loading"&gt;
    {{ loading ? 'Loading...' : 'Fetch User' }}
  &lt;/button&gt;
&lt;/template&gt;

&lt;script setup&gt;
import { useUserStore } from '@/store/userStore';
const userStore = useUserStore();
&lt;/script&gt;</code></pre><hr/><h3>5. <strong>使用 <code>useAsyncData</code> / <code>useFetch</code>（Nuxt / Vue）</strong></h3><p>Nuxt 提供了内置的并发控制：</p><pre><code class="js">const { data, pending } = await useAsyncData('user', () =&gt; 
  $fetch('/api/user')
)</code></pre><p>Nuxt 会自动处理并发和缓存。</p><hr/><h2>✅ 三、总结：推荐最佳实践</h2><table><thead><tr><th>场景</th><th>推荐方案</th></tr></thead><tbody><tr><td>搜索、输入框</td><td>防抖（Debounce）</td></tr><tr><td>快速点击按钮</td><td>防重提交（禁用按钮 + loading 状态）</td></tr><tr><td>多个组件请求相同数据</td><td>请求去重（AbortController）</td></tr><tr><td>大量请求需限流</td><td>请求队列 + 串行/并行控制</td></tr><tr><td>复杂状态管理</td><td>使用 Pinia / Redux + loading 状态</td></tr><tr><td>框架支持（Nuxt/Vue/React）</td><td>使用框架内置的请求工具（如 <code>useFetch</code>）</td></tr></tbody></table><hr/><h2>✅ 四、补充建议</h2><ul><li>所有请求都应有 <strong>超时机制</strong>（<code>timeout</code> 或 <code>AbortController</code>）</li><li>使用 <code>loading</code> 状态反馈用户</li><li>使用 <code>error</code> 状态处理失败</li><li>优先使用 <strong>请求去重 + 防重提交</strong>，这是最有效的并发控制方式</li></ul><hr/><p>你在实际开发中怎么运用闭包<br/> </p><p>在实际开发中，<strong>闭包（Closure）</strong> 是一种非常强大的语言特性，尤其在 JavaScript 中应用广泛。它能帮助我们实现数据封装、模块化、回调管理、函数工厂等功能。下面我结合实际开发场景，说明闭包的常见用法和最佳实践。</p><hr/><h3>✅ 一、闭包的核心概念回顾</h3><p>闭包是指：<strong>一个函数能够访问并“记住”其词法作用域中的变量，即使这个函数在外部执行。</strong></p><pre><code class="js">function outer() {
  let count = 0;
  return function inner() {
    count++;
    console.log(count);
  };
}</code></pre><p><code>inner</code> 函数虽然在 <code>outer</code> 执行完后返回，但它仍能访问 <code>count</code>，这就是闭包。</p><hr/><h2>🚀 实际开发中闭包的典型应用场景</h2><hr/><h3>1. <strong>数据私有与封装（模拟私有变量）</strong></h3><blockquote>避免直接暴露内部状态，实现“类”的私有字段效果。</blockquote><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/utils/Counter.js"]

function createCounter() {
  let privateCount = 0;

  return {
    increment: () =&gt; {
      privateCount++;
      return privateCount;
    },
    decrement: () =&gt; {
      privateCount--;
      return privateCount;
    },
    getCount: () =&gt; privateCount
  };
}

// 使用
const counter = createCounter();
console.log(counter.getCount()); // 0
counter.increment(); // 1
console.log(counter.getCount()); // 1</code></pre><p>✅ 优点：<code>privateCount</code> 不可被外部直接修改，保证了数据安全。</p><hr/><h3>2. <strong>函数工厂（Function Factory）</strong></h3><blockquote>生成具有特定行为的函数，常用于配置化或策略模式。</blockquote><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/utils/Calculator.js"]

function createMultiplier(factor) {
  return function (number) {
    return number * factor;
  };
}

const double = createMultiplier(2);
const triple = createMultiplier(3);

console.log(double(5));  // 10
console.log(triple(4));  // 12</code></pre><p>✅ 应用场景：</p><ul><li>配置化处理（如：不同环境的 API 前缀）</li><li>事件处理器绑定（如：动态绑定 <code>onClick</code> 时携带参数）</li></ul><hr/><h3>3. <strong>事件处理中的闭包（避免重复定义）</strong></h3><blockquote>在 DOM 事件中，闭包可保存当前状态。</blockquote><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/components/ButtonGroup.js"]

function setupButtons() {
  const buttons = document.querySelectorAll('.btn');
  buttons.forEach((btn, index) =&gt; {
    btn.addEventListener('click', function () {
      console.log(`Button ${index + 1} clicked!`);
      // 闭包保存了 index 的值
    });
  });
}</code></pre><blockquote>⚠️ 注意：如果用 <code>var</code> 声明 <code>index</code>，会因变量提升导致问题（常见陷阱）。<br/>✅ 推荐：用 <code>let</code>（块级作用域）或立即执行函数（IIFE）避免。</blockquote><hr/><h3>4. <strong>防抖（Debounce）与节流（Throttle）</strong></h3><blockquote>闭包是实现防抖/节流的核心机制。</blockquote><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/utils/debounce.js"]

function debounce(func, delay) {
  let timeoutId;
  return function (...args) {
    clearTimeout(timeoutId);
    timeoutId = setTimeout(() =&gt; {
      func.apply(this, args);
    }, delay);
  };
}

// 使用
const searchInput = document.getElementById('search');
const debouncedSearch = debounce((query) =&gt; {
  console.log('Searching:', query);
}, 500);

searchInput.addEventListener('input', (e) =&gt; {
  debouncedSearch(e.target.value);
});</code></pre><p>✅ 优势：<code>timeoutId</code> 被闭包保留，防止重复调用，提升性能。</p><hr/><h3>5. <strong>模块模式（Module Pattern）</strong></h3><blockquote>用闭包模拟 ES6 模块的私有成员。</blockquote><pre><code class="js">// [path="/Users/hejuan903/Documents/BackUp/program/paas-plus-web/src/services/UserService.js"]

const UserService = (function () {
  let users = []; // 私有数据

  function findById(id) {
    return users.find(u =&gt; u.id === id);
  }

  function save(user) {
    users.push(user);
    console.log('User saved:', user);
  }

  return {
    getUser: function (id) {
      return findById(id);
    },
    createUser: function (userData) {
      const user = { id: Date.now(), ...userData };
      save(user);
      return user;
    }
  };
})();</code></pre><p>✅ 优点：对外暴露公共 API，内部数据完全私有。</p><hr/><h2>⚠️ 闭包的注意事项（避免内存泄漏）</h2><ol><li><strong>不要滥用闭包</strong>，尤其是大对象或 DOM 引用。</li><li><strong>及时清理定时器或事件监听器</strong>：<br/>   <code>`</code>js<br/>   let timer = setTimeout(() =&gt; {<br/>     // ...<br/>   }, 1000);<br/>   // 使用完后 clearTimeout(timer);<br/>   <code>`</code></li><li><strong>避免在循环中直接使用 <code>var</code></strong>（建议用 <code>let</code> 或 IIFE）。</li></ol><hr/><h2>✅ 总结：闭包在开发中的价值</h2><table><thead><tr><th>场景</th><th>优势</th></tr></thead><tbody><tr><td>数据封装</td><td>实现“私有”变量</td></tr><tr><td>函数工厂</td><td>高度可配置的函数生成</td></tr><tr><td>事件处理</td><td>保持上下文状态</td></tr><tr><td>防抖/节流</td><td>高效控制执行频率</td></tr><tr><td>模块化</td><td>无需 ES6 模块也能实现模块</td></tr></tbody></table><hr/><blockquote>💡 <strong>建议</strong>：在现代 JS 开发中，虽然 <code>class</code> 和 <code>ES6 模块</code> 更清晰，但闭包依然是理解函数式编程、高阶函数、状态管理等高级概念的基础。</blockquote><p>页面加载特别慢该怎么排查</p><p>页面加载特别慢是一个常见但复杂的问题，通常涉及前端、后端、网络、服务器性能等多个方面。以下是系统化的排查思路和步骤，帮助你快速定位并解决问题。</p><hr/><h2>✅ 一、前端层面排查</h2><h3>1. <strong>检查浏览器开发者工具（DevTools）</strong></h3><p>打开浏览器开发者工具（F12），查看 <strong>Network（网络）面板</strong>：</p><ul><li>查看哪些资源加载耗时最长（如 JS、CSS、图片、API 请求）。</li><li>检查是否有大量 <code>Pending</code>、<code>DNS Lookup</code>、<code>TTFB</code>（Time to First Byte）过长的请求。</li><li>使用 <strong>Waterfall</strong> 视图分析请求顺序和耗时。</li></ul><blockquote>📌 Tip：按 <code>Time</code> 排序，关注 <code>DOMContentLoaded</code> 和 <code>Load</code> 时间。</blockquote><h3>2. <strong>检查资源大小与数量</strong></h3><ul><li><strong>JS/CSS 文件过大</strong>：使用 <code>gzip</code> 或 <code>brotli</code> 压缩。</li><li><strong>未压缩的资源</strong>：生产环境不应使用未压缩的 JS/CSS。</li><li><strong>过多的 HTTP 请求</strong>：合并小文件（如 CSS/JS 拆分优化），使用雪碧图或图标字体。</li><li><strong>图片过大或未优化</strong>：<br/>  - 使用 WebP 格式。<br/>  - 压缩图片（可用 <code>ImageOptim</code>、<code>Squoosh</code> 工具）。<br/>  - 懒加载图片（<code>loading="lazy"</code>）。</li></ul><h3>3. <strong>JavaScript 执行阻塞</strong></h3><ul><li>避免在 <code>&lt;head&gt;</code> 中加载大体积 JS。</li><li>使用 <code>async</code> 或 <code>defer</code> 加载非关键 JS。</li><li>检查是否有长时间运行的 JS 任务（如循环、复杂计算）阻塞主线程。</li></ul><blockquote>🔍 可用 <strong>Performance 面板</strong> 录制页面加载过程，查看 CPU 占用、JS 执行时间。</blockquote><hr/><h2>✅ 二、后端与 API 层面排查</h2><h3>1. <strong>接口响应时间（TTFB）</strong></h3><ul><li>使用 <code>curl</code> 或 Postman 测量接口 TTFB：<br/>  <code>`</code>bash<br/>  curl -w "TTFB: %{time_starttransfer}\n" -o /dev/null -s <a href="https://link.segmentfault.com/?enc=yHh%2F58VAnuxn38WIykaGGA%3D%3D.s9z%2FYLQA3Xpo1BRTU4wP9b1ViC8H%2BGHoHNKdho4CecY%3D" rel="nofollow" target="_blank">http://your-api.com/data</a><br/>  <code>`</code></li><li>若 TTFB &gt; 500ms，说明后端处理慢或数据库慢。</li></ul><h3>2. <strong>SQL 查询优化</strong></h3><ul><li>检查慢查询日志（MySQL 的 <code>slow query log</code>）。</li><li>确保常用字段有索引。</li><li>避免 <code>SELECT *</code>，只查需要的字段。</li><li>避免 N+1 查询（如循环查询用户列表中的每个用户信息）。</li></ul><h3>3. <strong>缓存机制</strong></h3><ul><li>是否开启缓存（Redis/Memcached）？</li><li>常量数据、配置、静态页面是否缓存？</li><li>避免重复计算或重复数据库查询。</li></ul><h3>4. <strong>服务性能瓶颈</strong></h3><ul><li>使用监控工具（如 Prometheus + Grafana）查看 CPU、内存、线程数。</li><li>检查是否有线程阻塞、死锁、GC 频繁等问题。</li><li>查看日志中是否有异常堆栈（如 <code>OutOfMemoryError</code>、<code>TimeoutException</code>）。</li></ul><hr/><h2>✅ 三、网络与 CDN 层面</h2><h3>1. <strong>CDN 是否启用？</strong></h3><ul><li>静态资源（JS/CSS/图片）是否通过 CDN 分发？</li><li>检查 CDN 回源时间、缓存命中率。</li></ul><h3>2. <strong>DNS 解析慢？</strong></h3><ul><li>使用 <code>dig</code> 或 <code>nslookup</code> 测量 DNS 解析时间：<br/>  <code>`</code>bash<br/>  dig yourdomain.com<br/>  <code>`</code></li><li>建议使用公共 DNS（如 1.1.1.1、8.8.8.8）或企业内网 DNS 优化。</li></ul><h3>3. <strong>HTTPS 握手耗时</strong></h3><ul><li>检查 SSL/TLS 握手时间（可看 DevTools 的 <code>SSL</code> 时间）。</li><li>使用 HSTS、OCSP Stapling 优化。</li><li>升级到 TLS 1.3（更快的握手）。</li></ul><hr/><h2>✅ 四、服务器与部署层面</h2><h3>1. <strong>服务器资源不足</strong></h3><ul><li>检查 CPU、内存、磁盘 I/O 是否满载。</li><li>使用 <code>top</code>、<code>htop</code>、<code>iostat</code> 查看系统负载。</li></ul><h3>2. <strong>应用服务器配置</strong></h3><ul><li>Web 服务器（如 Nginx、Apache）是否配置合理？<br/>  - <code>worker_processes</code>、<code>worker_connections</code><br/>  - <code>keepalive_timeout</code></li><li>是否开启 gzip 压缩？</li><li>是否有过多的短连接？</li></ul><h3>3. <strong>数据库连接池</strong></h3><ul><li>检查连接池是否配置过小（如 HikariCP 的 <code>maxPoolSize</code>）。</li><li>连接泄露可能导致连接耗尽。</li></ul><hr/><h2>✅ 五、建议的排查流程图（简化）</h2><pre><code class="text">页面加载慢？
   ↓
浏览器 DevTools → Network 面板分析 → 找出耗时资源
   ↓
前端：JS/CSS 大小、阻塞、图片未优化？
   ↓
后端：接口 TTFB 高？SQL 慢？缓存缺失？
   ↓
数据库：慢查询？索引缺失？连接池？
   ↓
网络：CDN？DNS？HTTPS 握手？
   ↓
服务器：CPU/内存/磁盘？应用配置？</code></pre><hr/><h2>✅ 六、工具推荐</h2><table><thead><tr><th>工具</th><th>用途</th></tr></thead><tbody><tr><td>Chrome DevTools</td><td>前端性能分析</td></tr><tr><td>Lighthouse（Chrome）</td><td>自动化性能评分</td></tr><tr><td>WebPageTest.org</td><td>跨地区、跨设备测试</td></tr><tr><td>Prometheus + Grafana</td><td>服务端监控</td></tr><tr><td>New Relic / Datadog</td><td>APM（应用性能监控）</td></tr><tr><td>MySQL Slow Query Log</td><td>慢 SQL 分析</td></tr><tr><td>curl -w "%{time_starttransfer}"</td><td>测 TTFB</td></tr></tbody></table><hr/><h2>✅ 七、快速优化建议（立即执行）</h2><ol><li>✅ 启用 Gzip/Brotli 压缩（Nginx 配置）  </li><li>✅ 使用 CDN 加速静态资源  </li><li>✅ 图片压缩 + 懒加载  </li><li>✅ 合并/压缩 JS/CSS 文件  </li><li>✅ 关键资源使用 <code>async</code>/<code>defer</code>  </li><li>✅ 开启数据库慢查询日志并优化  </li><li>✅ 增加 Redis 缓存热点数据  </li></ol><hr/><p>对埋点的理解</p><p>Web埋点（Web Tracking）是前端数据分析中的核心手段之一，用于<strong>收集用户在网页上的行为数据</strong>，帮助产品、运营、研发等团队分析用户行为、优化产品体验、评估功能效果。</p><hr/><h2>一、什么是Web埋点？</h2><p><strong>Web埋点</strong>是指在网页中嵌入一段代码（通常是JavaScript），当用户触发特定事件（如点击、页面加载、滚动、表单提交等）时，自动收集相关数据，并上报给数据分析平台（如Google Analytics、神策、友盟、自研分析系统等）。</p><blockquote>✅ 举个例子：  <br/>用户点击“立即购买”按钮 → 埋点代码触发 → 上报事件：<code>click_buy_button</code>，包含参数：<code>product_id=123</code>, <code>price=99.9</code>。</blockquote><hr/><h2>二、埋点的目的</h2><table><thead><tr><th>目标</th><th>说明</th></tr></thead><tbody><tr><td>用户行为分析</td><td>看用户在页面上做了什么（点击、浏览、滑动等）</td></tr><tr><td>转化率分析</td><td>分析从“浏览”到“下单”的转化路径</td></tr><tr><td>功能优化</td><td>发现用户流失环节，优化流程</td></tr><tr><td>个性化推荐</td><td>基于用户行为做智能推荐</td></tr><tr><td>A/B测试</td><td>对比不同设计方案的效果</td></tr></tbody></table><hr/><h2>三、常见的埋点类型</h2><table><thead><tr><th>类型</th><th>说明</th><th>示例</th></tr></thead><tbody><tr><td><strong>页面埋点（Page View）</strong></td><td>页面加载完成时上报</td><td><code>page_view</code>, <code>page_name=home</code></td></tr><tr><td><strong>事件埋点（Event）</strong></td><td>用户触发某个操作</td><td><code>click</code>, <code>form_submit</code>, <code>scroll</code></td></tr><tr><td><strong>自定义埋点（Custom Event）</strong></td><td>针对特定业务逻辑的埋点</td><td><code>login_success</code>, <code>share_article</code></td></tr><tr><td><strong>异常埋点（Error Tracking）</strong></td><td>捕获前端JS错误</td><td><code>js_error</code>, <code>resource_error</code></td></tr><tr><td><strong>性能埋点（Performance）</strong></td><td>收集页面加载性能数据</td><td><code>first_paint</code>, <code>dom_complete</code></td></tr></tbody></table><hr/><h2>四、埋点实现方式</h2><h3>1. 手动埋点（Manual Tracking）</h3><p>在代码中显式调用埋点函数。</p><pre><code class="javascript">// 示例：手动埋点点击事件
document.getElementById('buy-btn').addEventListener('click', function () {
  // 上报埋点数据
  track('click', {
    event_name: 'click_buy_button',
    product_id: '123',
    price: 99.9
  });
});</code></pre><blockquote>✅ 优点：精确、可控  <br/>❌ 缺点：开发成本高，容易遗漏，维护难</blockquote><hr/><h3>2. 自动埋点（Auto Tracking）</h3><p>通过监听页面事件（如点击、表单提交）自动上报，无需手动加代码。</p><ul><li><strong>自动采集点击元素</strong>：如 <code>a</code>, <code>button</code>, <code>input</code> 等标签</li><li><strong>自动提取属性</strong>：如 <code>data-track</code>、<code>id</code>、<code>class</code>、<code>href</code> 等</li><li><strong>自动识别表单提交</strong></li></ul><pre><code class="html">&lt;button data-track="click_buy" data-product-id="123"&gt;立即购买&lt;/button&gt;</code></pre><pre><code class="javascript">// 自动监听所有带 data-track 的元素
document.addEventListener('click', function (e) {
  const el = e.target;
  const track = el.getAttribute('data-track');
  if (track) {
    track('click', { event_name: track, ... });
  }
});</code></pre><blockquote>✅ 优点：减少开发工作量，覆盖全面  <br/>❌ 缺点：可能上报无关事件，需过滤</blockquote><hr/><h3>3. 框架级埋点（如 Vue/React 组件埋点）</h3><p>在组件中集成埋点逻辑，通过高阶组件、混入、Hooks 等方式实现。</p><h4>Vue 示例（使用 mixin 或 directive）：</h4><pre><code class="javascript">// 自定义指令：v-track
Vue.directive('track', {
  inserted(el, binding) {
    el.addEventListener('click', () =&gt; {
      track('click', binding.value);
    });
  }
});</code></pre><pre><code class="html">&lt;button v-track="{ event_name: 'click_login' }"&gt;登录&lt;/button&gt;</code></pre><hr/><h2>五、埋点数据结构（标准字段）</h2><pre><code class="json">{
  "event_name": "click_buy_button",
  "timestamp": 1700452800000,
  "user_id": "u_12345",
  "session_id": "s_abc123",
  "page_url": "https://example.com/product/123",
  "page_title": "商品详情页",
  "device_type": "web",
  "os": "MacOS",
  "browser": "Chrome",
  "ip": "192.168.1.1",
  "extra": {
    "product_id": "123",
    "price": 99.9,
    "category": "electronics"
  }
}</code></pre><hr/><h2>六、埋点最佳实践</h2><ol><li><strong>统一命名规范</strong>  <br/>   - 事件名：<code>click_模块_功能</code>（如 <code>click_header_search</code>）<br/>   - 参数名：<code>snake_case</code>，避免空格和特殊字符</li><li><strong>避免重复上报</strong>  <br/>   - 同一事件不要多次触发<br/>   - 使用防抖/节流控制频繁事件</li><li><strong>数据脱敏</strong>  <br/>   - 用户敏感信息（如手机号、身份证）需脱敏处理</li><li><strong>埋点可配置化</strong>  <br/>   - 使用配置文件或后台管理控制埋点开关、上报规则</li><li><strong>埋点验证机制</strong>  <br/>   - 开发阶段通过调试工具或模拟上报验证是否生效</li><li><strong>性能优化</strong>  <br/>   - 异步上报，不阻塞页面加载<br/>   - 批量上报，减少请求次数</li></ol><hr/><h2>七、埋点工具推荐</h2><table><thead><tr><th>工具</th><th>特点</th></tr></thead><tbody><tr><td>Google Analytics</td><td>免费、功能强、全球通用</td></tr><tr><td>神策数据（SensorsData）</td><td>国内主流，支持埋点管理平台</td></tr><tr><td>友盟+</td><td>移动端+Web全平台支持</td></tr><tr><td>Mixpanel</td><td>强大的事件分析和漏斗分析</td></tr><tr><td>自研埋点 SDK</td><td>高度可控，适合大厂内部系统</td></tr></tbody></table><hr/><h2>八、总结</h2><table><thead><tr><th>维度</th><th>说明</th></tr></thead><tbody><tr><td>埋点本质</td><td>数据采集的“传感器”</td></tr><tr><td>核心价值</td><td>让数据驱动产品决策</td></tr><tr><td>关键能力</td><td>精准采集、高效上报、安全合规、易于维护</td></tr><tr><td>发展趋势</td><td>自动化 + 可视化 + 智能分析 + AIOps</td></tr></tbody></table><hr/><p>如果你正在开发一个 Web 项目，建议：</p><ol><li><strong>前期规划埋点方案</strong>（谁要什么数据？）</li><li><strong>建立埋点规范文档</strong>（命名、字段、上报逻辑）</li><li><strong>使用埋点管理平台</strong>（如神策、Mixpanel）统一管理</li><li><strong>定期审计埋点有效性</strong>（避免无效或冗余数据）</li></ol><hr/>]]></description></item><item>    <title><![CDATA[GMI Cloud 携手英伟达在亚太建设]]></title>    <link>https://segmentfault.com/a/1190000047412049</link>    <guid>https://segmentfault.com/a/1190000047412049</guid>    <pubDate>2025-11-19 18:03:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p><strong>摘要</strong></p><p>GMI Cloud基于GB300的 AI Factory 启动构建，该项目携手英伟达（NVIDIA），总投资额达 5 亿美元，万卡集群将为全亚太AI产业发展提供坚实支撑。</p></blockquote><p>本周，作为英伟达（NVIDIA）全球六大 Reference Platform NVIDIA Cloud Partner 之一及全球增长最快的 GPU 即服务（GPU-as-a-Service）提供商之一，GMI Cloud 正式宣布，携手 NVIDIA 在中国台湾正式启动 AI Factory 构建。这座总投资额达 5 亿美元的 AI 基础设施，旨在成为亚太 AI 发展的关键支柱，助力区域内企业以前所未有的规模开展 AI 模型训练与部署。</p><p><strong><em>Part 1</em></strong></p><p><strong>GB300 硬核亮相</strong></p><p><strong>GMI Cloud AI Factory</strong></p><p><strong>万卡集群凸显硬核实力</strong></p><p>什么是 AI Factory？与传统数据中心不同，AI Factory 是专为规模化支撑人工智能工作负载的密集型计算需求而设计—— 以智能作为工厂的 “output”，以 GPU 算力作为 “生产设备”，以数据作为 “input”。</p><p>GMI Cloud 联合 NVIDIA 共建的这座 AI Factory 总功率达 16 兆瓦，搭载 96 个高密度 GB300 NVL72 架构，<strong>内置万卡级 NVIDIA Blackwell Ultra GPUs</strong> ，预计每秒可处理近 200 万 token。该基础设施专为大规模 AI 推理、模型微调及多模态工作负载量身打造，将有效打通 AI 技术从“概念”到“应用生产”的最后一公里，为亚太 AI 产业化提供硬核支撑。</p><p>依托 NVIDIA Blackwell 架构，该工厂的超级计算机全面配备 NVIDIA NVLink、NVIDIA Quantum InfiniBand、NVIDIA Spectrum-X Ethernet networking、NVIDIA BlueField DPUs 等技术。这一高规格基础设施将为亚太地区提供可灵活扩展的高性能计算资源，加速先进 AI 解决方案在各行业的落地，进一步巩固区域在 AI 转型、智算领域的创新优势。</p><p>GMI Cloud 创始人、CEO Alex Yeh 表示：“凭借数千块下一代 NVIDIA GPU 的协同算力，我们将助力亚太企业把 AI 愿景转化为现实生产力。”</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6bK" alt="图片" title="图片"/></p><p><strong><em>Part 2</em></strong></p><p><strong>多家合作伙伴共同赋能</strong></p><p><strong>亚太 AI 产业价值再升级</strong></p><p>当前，全球 AI 正从技术研发加速迈向产业化落地，从数据中心到边缘设备的全场景应用需求激增，GMI Cloud AI Factory 项目的落地，是跨区域算力协同的典型范例，通过整合加速计算、绿色能效与数据安全等核心优势，为全球 AI 基础设施的高质量发展提供“亚太方案”。</p><p><strong>在 GMI Cloud AI Factory 启动媒体发布会上，多家全球知名企业同步公布了基于该项目的首批合作用例：</strong></p><p><strong>Trend Micro —— 通过数字孪生技术，实现安全防护</strong></p><p>Trend Micro <strong>借助 GMI Cloud 算力，通过数字孪生与 AI Agent改善规划并提前做好准备。利用数字孪生技术，企业可在虚拟环境中重构业务平台、安防系统或运营流程，并进行红蓝攻防安全模拟，确保能够抵御不断演进的网络安全威胁。</strong></p><p><strong>WiAdvance —— 驱动智算升级</strong></p><p>作为硬件制造与系统集成领域的全球领导者，WiAdvance 正构建具备 AI 能力的工厂系统，该系统依托 NVIDIA 加速计算技术，应用于计算机视觉、预测性维护及数字孪生模拟等场景。借助 GMI Cloud 的 AI Factory，WiAdvance 可在实际生产线上直接开展模型训练与部署，有效减少设备停机时间，推动下一代智算发展。</p><p><strong>VAST Data —— 面向 AI  Factory 构建高性能数据基础设施</strong></p><p>VAST Data 将作为合作伙伴，为 GMI Cloud AI Factory 提供下一代数据基础设施支撑，其统一数据架构支持数千块 NVIDIA GPU 的高吞吐量访问，以极高的速度和可靠性为大规模模型训练、推理及实时数据处理提供动力。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdm6bL" alt="图片" title="图片" loading="lazy"/></p><p>总体来说，该项目既为构建区域自主 AI 能力树立了行业标杆，又实现了对全球先进技术资源的合规化应用。它进一步深化了跨太平洋技术协作生态——将美国 AI 基础设施创新优势，与中国台湾在制造业规模、工程部署能力上的突出所长相结合，形成“技术+制造+落地”的协同效应。</p><p>“‘AI Factory’是生产智能的地方——将数据转化为面向未来的洞见和创新，”NVIDIA 亚太区高级副总裁 Raymond Teh 表示。“GMI Cloud 的 AI Factory 将有助于亚太在人工智能基础设施和创新方面继续保持领先地位。”</p><p><strong>关于 GMI Cloud</strong></p><p>由 Google X 的 AI 专家与硅谷精英共同参与创立的 GMI Cloud 是一家领先的 AI Native Cloud 服务商，是全球六大 Reference Platform NVIDIA Cloud Partner 之一，拥有遍布全球的数据中心，为企业 AI 应用提供最新、最优的 GPU 云服务，为全球新创公司、研究机构和大型企业提供稳定安全、高效经济的 AI 云服务解决方案。</p><p>GMI Cloud 凭借高稳定性的技术架构、强大的GPU供应链以及令人瞩目的 GPU 产品阵容（如能够精准平衡 AI 成本与效率的 H200、具有卓越性能的 GB200、GB300 以及未来所有全新上线的高性能芯片），确保企业客户在高度数据安全与计算效能的基础上，高效低本地完成 AI 落地。此外，通过自研“Cluster Engine”、“Inference Engine”两大平台，完成从算力原子化供给到业务级智算服务的全栈跃迁，全力构建下一代智能算力基座。</p><p>作为推动通用人工智能（AGI）未来发展的重要力量，GMI Cloud 持续在 AI 基础设施领域引领创新。选择 GMI Cloud，您不仅是选择了先进的 GPU 云服务，更是选择了一个全方位的 AI 基础设施合作伙伴。</p><p>如果您想要了解有关 GMI Cloud 的信息</p><p>请关注我们并建立联系</p>]]></description></item><item>    <title><![CDATA[告别查询超时！SLS物化视图的核心原理与]]></title>    <link>https://segmentfault.com/a/1190000047412054</link>    <guid>https://segmentfault.com/a/1190000047412054</guid>    <pubDate>2025-11-19 18:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：戴志勇</p><h2>当“即查即算”遇上数据爆炸</h2><p>你是否经历过这样的场景？</p><p>在阿里云日志服务里，一个看似简单的看板，点开却要等上几十秒；高峰期多人同时查日志，系统直接“卡成 PPT”；更糟的是，有时结果还不准——因为达到资源限制，系统只能“估算”答案。</p><p>这背后，是日志规模爆炸式增长带来的现实困境：当数据量从 GB 跃升至 TB 甚至 PB 级，“边查边算”的传统模式已力不从心。</p><ul><li><strong>查得慢：</strong> 复杂聚合动辄几十秒，看板刷新比泡杯咖啡还久</li><li><strong>扛不住：</strong> 一到高峰，查询互相抢资源，一个慢、全链路崩</li><li><strong>不准了：</strong> 资源超限被迫降级，数据失真，决策风险陡增</li></ul><p>而这些痛点，恰恰集中在最常用的场景——监控大屏、运营看板、实时报表。它们有个共同特点：查询模式固定、时间跨度大、但要求秒级响应、结果精准。</p><p>现在，<a href="https://link.segmentfault.com/?enc=Z8WvCFoVLht9Hap1QMPUWw%3D%3D.GeQU%2Fv6DYgBJYh17DsyasD5O6BUZnUgcro8ihxEKKM3%2BqnGoBDjNSa5AFOHMcTJ9TKth7UtXI4Uu9FQMuvtabL0ZAvwj2FbNOHz10MvROurYvPzDgUo%2FwuM8e59DjYD%2Bym49d%2BSr3RCC8JFFh5Q7LA%3D%3D" rel="nofollow" target="_blank">阿里云日志服务</a>带来破局利器——<strong>物化视图</strong>。</p><p>它就像给你的日志数据“提前算好答案，存好快照”。通过智能增量预计算 + 自动查询改写，系统在后台默默把高频查询的结果提前准备好。当你发起请求时，不再扫描全量原始日志，而是直接读取预计算结果——查询速度提升数十倍乃至百倍，资源消耗大幅降低，结果还更稳、更准。</p><p>用一点额外的存储空间，换回秒级洞察力。从此，再大的日志量，也能做到“点开即见，所见即所得”。</p><h2><a href="https://link.segmentfault.com/?enc=%2Fd3EErbH2uPvK9RvOAQ4rw%3D%3D.ojI0B744Qp2o64LXYSzmoI0o7asX%2FEANXduOJDIwx89Z3a9vNbcE6IrVF%2BiURsV5wIQ2hvYGNN%2B8SSjBTqQZYAhAZAVpiCP1GREQND3NV%2Bvzdvh76KC%2BhEvj5mBOvFJo7GKRcsY0Wp18VP5YNj%2Bucw%3D%3D" rel="nofollow" target="_blank">日志服务</a>物化视图优势</h2><p>物化视图的核心思想是：用额外的存储空间，换来查询速度的飞跃。日志服务的物化视图主要支持两种场景的查询加速。</p><p><strong>1. 过滤加速：只留“有用”的日志</strong></p><p>比如你只关心“错误日志”，物化视图会提前把所有 error 级别的日志筛选出来单独存好。下次查错误，就不用翻遍全部日志，直接查这个“精选集”，速度提升几十倍。</p><p><strong>2. 预聚合加速：提前算好统计结果</strong></p><p>比如每天要统计“各地区的用户访问量”，物化视图会每隔一段时间自动算好这个数据并存起来。你查的时候，系统直接拿现成结果，不用再从原始日志里一条条加总——数据量可能从亿行变成百行。</p><p>与业界其他物化视图方案相比，日志服务物化视图具有以下优势：</p><h3>1. 异步物化视图，增量刷新，不影响写入性能</h3><p>物化视图的构建完全独立于数据写入流程，采用异步更新机制，每次刷新只针对新写入的数据，更新任务由后端托管，对用户透明。</p><h3>2. 自动数据合并，写入即可查</h3><p>自动合并未物化的最近数据和已物化的历史结果，有效解决了同类产品的关键问题：</p><ul><li>异步刷新痛点：无法读取最新数据，秒级刷新也无法保证数据实时性</li><li>同步刷新痛点：严重影响写入性能，系统吞吐量大幅下降</li></ul><h3>3. 支持复杂聚合函数的改写</h3><p>除了常用的聚合函数（sum、count、avg、min、max等），还支持如下复杂的聚合函数：</p><ul><li><code>count(distinct)</code>：精确去重统计</li><li><code>approx_percentile</code>：近似百分位数计算</li><li><code>approx_distinct</code>：高效近似去重</li></ul><h3>4. 支持动态更新物化视图</h3><p>更改物化视图的 SQL 定义时，历史物化视图无需重建，不影响已物化的历史结果。对于经常动态增加列或减少列的场景，这一特性显得尤为重要，可以避免频繁更新物化视图带来的存储和计算成本增加。</p><h3>5. 透明改写</h3><p>除了支持 SQL 的透明改写，对于查询语句也可以做谓词的自动补偿。举例说明：</p><p><strong>用户创建物化视图的语句：</strong></p><pre><code>level:error | select latency, host from log where message like '%xxx%'</code></pre><p><strong>对于如下的查询请求：</strong></p><pre><code>level:error and latency &gt; 100 | select avg(latency), host from log where message like '%xxx%' group by host</code></pre><p>优化器自动添加 latency &gt; 100<code> </code>作为查询条件去查物化结果，用户完全无感。对于过滤性加速场景，多个 SQL 可以最大化复用物化结果，有效降低了物化带来的存储开销。</p><h2>原理介绍</h2><p>对于用户创建的物化视图，日志服务会在后台自动托管整个计算与维护流程——无需用户干预，一切静默完成。</p><p>具体来说，系统会为每个物化视图启动一个智能定时任务，持续追踪新写入的日志数据。每隔一段时间，它就会自动执行您创建视图时指定的 SQL（无论是简单的过滤条件，还是复杂的聚合逻辑），并将计算结果持久化存储起来。每次任务完成后，系统还会精准记录“已处理到哪个时间点”，为后续查询提供优化依据。这一切对用户完全透明：不用写调度脚本、不用管任务失败、也不用担心数据一致性——日志服务全权负责。</p><p>当发起查询时，基于成本的优化器（CBO）会自动介入：</p><ul><li>如果发现有匹配的物化视图，它会智能选择最优的一个；</li><li>对于非聚合类查询，优化器将执行计划改写为“原始数据 + 物化数据”的轻量级 Union；</li><li>对于聚合类查询，则巧妙地将新数据实时聚合后，再与预计算结果合并。</li></ul><p>整个过程无缝衔接，既保证了结果的实时性与准确性，又大幅降低了查询延迟和资源开销。整个架构图如下所示（以聚合场景为例）。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412056" alt="image" title="image"/></p><h2>案例分享：Dashboard 从“超时失败”到“秒级响应”</h2><p>在 Dashboard 场景中，用户对仪表盘打开时间极为敏感，通常要求秒级响应。当多个用户同时刷新 Dashboard 时，如果单个 SQL 请求消耗大量计算资源，会导致计算资源争抢，所有用户都在等待，严重影响用户体验。通过物化视图预计算关键指标，可以将原本需要分钟级计算的复杂查询优化为秒级响应，显著提升用户体验。</p><p>举个真实场景：当系统延迟突然飙升，如何快速定位问题？</p><p>假设一个高并发的在线服务，其日志数据被写入某个 Logstore 中。每条日志记录的关键信息：</p><ul><li>请求延迟（latency）</li><li>请求类型（RequestType）</li><li>用户 ID（ProjectId）</li><li>状态码（Status）</li><li>请求数据量（InFlow）</li><li>返回数据量（OutFlow）</li></ul><p>某时刻，监控告警响起——系统平均延迟突然升高！是正常波动？是流量激增导致？还是某个特定用户或接口出了问题？你需要立刻响应，不想等上几十秒甚至最后超时无法看到结果。</p><p><strong>创建物化视图的 SQL：</strong></p><pre><code>*| select avg(latency) as avg_latency,date_trunc('hour', __time__) as time from log group by time
*| select sum(InFlow) as in_flow,sum(OutFlow) as out_flow,avg(latency) as latency, ProjectId,RequestType,Status from log group by ProjectId,RequestType,Status</code></pre><p>仪表盘使用的 SQL：</p><pre><code>统计每个小时的平均延迟同比一天前、三天前和一周前的变化
*| select time,diff[1] as day1,diff[2] as day2,diff[3] as day3, diff[4] as day7 from ( select time,ts_compare(avg_latency, 86400,172800,604800) as diff from (select avg(latency) as avg_latency,date_trunc('hour', __time__) as time from log group by time) group by time order by time) limit all
按照 ProjectId 维度统计读写流量和延迟的变化
*| select sum(InFlow)/1024/1024/1024 as in_flow,sum(OutFlow)/1024/1024/1024 as out_flow,avg(latency) as latency,ProjectId from log group by ProjectId order by in_flow desc limit 10
按照 Status 维度和 ProjectId 的维度，统计平均延迟大于 200 的读写流量
*| select sum(InFlow)/1024/1024/1024 as in_flow,sum(OutFlow)/1024/1024/1024 as out_flow,avg(latency) as latency,ProjectId from log group by Status,ProjectId having latency &gt; 200 order by in_flow desc  limit 10</code></pre><ol><li>查看近一周延迟同比的变化情况，开启了物化视图的请求，千亿以上数据秒内出结果，未开启的请求直接超时。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412057" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412058" alt="image" title="image" loading="lazy"/></p><ol start="2"><li>按照 ProjectId 维度统计读写流量和延迟的变化，开启了物化视图的请求，千亿以上数据不到 400 毫秒返回结果，而未开启的请求 54 秒才返回，性能提升 100 倍以上。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412059" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412060" alt="image" title="image" loading="lazy"/></p><ol start="3"><li>按照 Status 维度和 ProjectId 的维度，统计平均延迟大于 200 的读写流量，由于统计的维度更多了，未使用物化视图的请求最后超时了，而使用物化视图后，800 多毫秒返回。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412061" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412062" alt="image" title="image" loading="lazy"/></p><p>有趣的是，创建物化视图时写的 SQL 和系统实际执行的 SQL 并不完全相同。这正是阿里云日志服务强大的智能优化器在幕后工作——它能自动识别和改写查询逻辑，让用户无需深入了解底层细节，就可以轻松为仪表盘查询加速。</p><p>在千亿级数据规模的实测中，采用物化视图的图表可以秒开，而相同的 SQL 若不使用物化视图，即使是最快的情况也需要 50 多秒，更多时候会直接超时无法返回结果。这不仅是性能的提升，更是体验的质的飞跃。理论上来说数据规模越大，加速效果越好。在数据规模更加庞大的另一个 Region 中，万亿级数据的 SQL 查询也能稳定在 3 秒左右完成，进一步验证了随着数据量的增长，物化视图带来的性能收益呈现出更为显著的加速效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412063" alt="image" title="image" loading="lazy"/></p><h2>展望</h2><p>在数据驱动的时代，<a href="https://link.segmentfault.com/?enc=TaAeUX7SeTxjv9MHAYC9Dw%3D%3D.zsSOeg1y5x7cYSjSgqcRE5tgoukf4U7a3v3XMYqkIq62yWFTnpoHhkAbBnTfvDvPFb%2FGxDaWi0yEYm9rkoWjhdjsB1ZiEiN9OsJl0MzUSlTQx%2BISY30XUGv1J4wVz%2BtH4skmplcOSEDLwhfumRpMmQ%3D%3D" rel="nofollow" target="_blank">阿里云日志服务</a>物化视图通过预计算技术，从根本上解决了大规模日志分析中的性能瓶颈和吞吐量限制问题，为实时日志分析提供了新的技术解决方案。未来，我们将继续在以下方向深耕：</p><ul><li>智能推荐：自动识别高频查询模式，一键生成最优物化视图</li><li>扩展使用场景：支持 join 算子的物化视图，支持数据删除场景</li><li>改写增强：支持表达式非精确匹配的改写</li></ul>]]></description></item><item>    <title><![CDATA[不只做语音，ElevenLabs 推出图]]></title>    <link>https://segmentfault.com/a/1190000047412074</link>    <guid>https://segmentfault.com/a/1190000047412074</guid>    <pubDate>2025-11-19 18:02:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412076" alt="" title=""/></p><p>开发者朋友们大家好：</p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@鲍勃</em></p><h2>01有话题的技术</h2><p><strong>1、StrawGo：专为 Go 开发者打造的低延迟、可扩展语音 AI 应用框架</strong></p><p>StrawGo 是一个高度成熟、可用于生产环境的框架。它专门面向需要构建低延迟、高可扩展性的语音 AI 应用的 Go 开发者。</p><ul><li>🎯 帧（Frame）驱动架构：提供清晰、模块化的管道（Pipeline）组合方式，便于构建和管理。</li><li>⚡ 高性能：充分利用 Go 原生并发能力（goroutines 与 channels），实现极致性能。</li><li>📞 集成语音通话：内置支持 Twilio 和 Asterisk WebSocket，轻松实现语音呼叫功能。</li><li>🔌 传输与序列化器模式：采用类似 Pipecat 的设计，构建灵活且易于扩展的架构。</li><li>🎙 多样化 AI 服务集成：无缝对接 Deepgram（语音转文本 STT）、ElevenLabs（文本转语音 TTS）以及 OpenAI 和 Gemini（大型语言模型 LLMs）。</li><li>🔄 灵活音频处理：支持 mulaw 直通（passthrough）或 PCM 管道，满足多样化处理需求。</li><li>🚀 生产级就绪：具备完善的错误处理机制和生命周期管理，确保稳定运行。</li><li>📦 极简依赖：仅需 gorilla/websocket，轻松集成，降低复杂性。</li></ul><p><a href="https://link.segmentfault.com/?enc=utltxxPP8h0HEHaiGwMApw%3D%3D.AJcYziN9qk6eXdmwqH4GHFzbShtsV5Dxu8dXRQiQju2hZbhRRLWg%2FKKwy68Os6WG" rel="nofollow" target="_blank">https://github.com/square-key-labs/strawgo-ai</a></p><p>(@Github)</p><p><strong>2、CAMB.AI 与 Broadcom 合作，将语音 AI  能力推向终端设备</strong><em>*</em>*</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412077" alt="" title="" loading="lazy"/></p><p>AI 本地化平台 CAMB.AI 正与半导体巨头 Broadcom 合作，将其先进的语音 AI 模型移植到 Broadcom 的 SoC 芯片上。此次合作旨在实现设备端的文本转语音和未来可能的实时翻译功能，从而提升性能、隐私性和用户体验。</p><ul><li><strong>设备端 TTS 能力：</strong> CAMB.AI 推出了可在 Broadcom SoC 集成的 NPU 上运行的 TTS 功能，能够将多语言文本转化为自然语音，无需云连接。</li><li><strong>隐私与低延迟：</strong> 本地化处理消除了对云的依赖，确保了超低延迟、增强用户隐私，并降低了带宽和电力成本。</li><li><strong>优化的性能：</strong> 结合 Broadcom 高效的芯片架构和 CAMB.AI 的轻量级 AI 模型，实现了出色的性能功耗比。</li><li><strong>未来潜力：</strong> 双方正在探索将 CAMB.AI 的实时多语言翻译模型移植到 Broadcom 设备端 NPU 的可行性，这将支持超过 150 种语言的端到端翻译。</li><li><strong>应用场景拓展：</strong> 该能力将首先应用于 Broadcom 在家庭娱乐和连接领域的设备，如机顶盒和网关，为用户提供更便捷、个性化的内容体验，并赋能无障碍访问。</li></ul><p>端侧音频描述 demo：</p><p><a href="https://link.segmentfault.com/?enc=wGzTvBL3fZ%2BjcjSHuhrIlw%3D%3D.DzN2QS1zU9qF9yYv8kETeXsrvhA2qP12kx9ssxZ4f1Wp7YroTF4He222f%2FPGr2r9" rel="nofollow" target="_blank">https://vimeo.com/1127700800/29c9f4491b</a></p><p>(@CAMB.AI)</p><p><strong>3、Cloudflare 收购 Replicate</strong></p><p>Cloudflare 今日宣布已收购 Replicate，这是领先的 AI 模型运行平台。此次收购旨在将 Replicate 强大的模型目录和部署能力集成到 Cloudflare 的开发者平台中，为开发者提供更便捷、更高效的 AI 模型运行和应用开发体验。</p><ul><li><strong>无缝集成与模型目录扩展：</strong> Replicate 平台及其超过 50,000 个模型（包括开源和微调模型）将全面集成到 Cloudflare 的 Workers AI 服务中，为开发者提供前所未有的模型选择。</li><li><strong>增强的 Workers AI 功能：</strong> Replicate 的加入将为 Workers AI 带来微调和运行自定义模型的能力，同时受益于 Cloudflare 全球网络的性能和可靠性提升。</li><li><strong>统一的 AI 开发体验：</strong> Cloudflare 将 Replicate 的模型库与自身的 AI Gateway、Vectorize、R2 等数据和编排工具相结合，构建一个一站式的 AI Cloud，简化端到端 AI 应用的开发与部署。</li><li><strong>社区与生态系统融合：</strong> Cloudflare 将继续投资 Replicate 社区，将其打造成 AI 模型发现、分享和实验的首选目的地，并利用 Cloudflare 的网络加速用户体验。</li><li><strong>更灵活的模型部署：</strong> 除了将 Replicate 模型迁移至 Workers AI，双方还将共同努力，使开发者能够通过 Replicate 的 Cog 工具轻松地将自定义模型部署到 Cloudflare 的网络上。</li></ul><p>(@Cloudflare Blog)</p><p><strong>4、陈天桥团队推出 EverMemOS，或有望解决 AI「失忆」问题</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412078" alt="" title="" loading="lazy"/></p><p>昨天，陈天桥领导的盛大团队宣布正式发布其旗舰产品 EverMemOS。这是一款面向 AI 智能体的长期记忆操作系统，旨在为未来智能体提供持久、连贯、可进化的数据基础设施。</p><p>EverMemOS 在 LoCoMo 与 LongMemEval-S 等主流长期记忆评测集上分别取得 92.3% 与 82% 的成绩，显著超越此前 SOTA 水平，成为新的行业标杆。该系统已率先被 AI Native 产品 Tanka 采用。</p><p><strong>当前，大模型受限于固定上下文窗口，难以在长时程任务中保持一致性，导致「失忆」与事实矛盾。</strong> 行业巨头如 Claude 与 ChatGPT 已将长期记忆作为战略功能推出，凸显记忆能力在 AI 演化中的关键地位。EverMind 团队提出，长期记忆是 AI 从「工具」走向「智能体」的分水岭。</p><p>EverMind 表示，EverMemOS 的设计灵感源自人类大脑记忆机制：</p><ul><li>四层架构设计：代理层、记忆层、索引层与接口层，分别类比人脑前额叶、皮层、海马体与感官接口；</li><li>记忆处理器：突破传统「记忆数据库」局限，能主动影响模型推理与回应；</li><li>分层记忆提取：将语义块组织为情景记忆单元，解决文本相似度检索的上下文缺陷；</li><li>模块化框架：支持不同场景下的多样化记忆需求，兼顾企业协作与陪伴式交互。</li></ul><p>目前，EverMemOS 已在 Github 开源，团队计划在今年晚些时候推出云服务版本，为企业用户提供更完善的支持与扩展体验。</p><p>🌍 官网： <a href="https://link.segmentfault.com/?enc=k%2FORdoyTci8NGNLLuCx7cA%3D%3D.eROPs7fmjsJ7FikCkJNu3Q%3D%3D" rel="nofollow" target="_blank">http://everm.ai</a></p><p>💻 Github: </p><p><a href="https://link.segmentfault.com/?enc=m5ejJqDlpwI8n1yq9g4hag%3D%3D.tfopQwAgCVhUHqw%2FW4lzBugqHXRxOkx5NOR5BKt0ZlTZREOUoR7BYW4PY1y9O91d" rel="nofollow" target="_blank">https://github.com/EverMind-AI/EverMemOS/</a></p><p>( @appso)</p><hr/><h2>02有亮点的产品</h2><p><strong>1、ElevenLabs 整合多模态模型，推出 Image &amp; Video 平台</strong></p><p>ElevenLabs 正式发布其全新的「Image &amp; Video」（Beta 版）平台，将业界领先的图像和视频生成模型与 ElevenLabs 强大的音频处理能力整合，为创作者和营销人员提供一站式多模态内容创作解决方案。</p><ul><li><strong>一站式多模态创作</strong>：集成 Veo， Sora， Kling， Wan， Seedance 等顶级模型，支持生成图像和视频，并可导出至 ElevenLabs Studio 进行音频、音乐和音效的后期制作。</li><li><strong>强大的图像生成能力</strong>：内置 Nanobanana， Flux Kontext， Wan， Seedream 等模型，用户可快速创建、优化图像，用于故事板、缩略图或视频素材。</li><li><strong>高级音频与视频增强</strong>：提供图像和视频的 Topaz 超分辨率放大功能，并支持为生成的视频添加唇形同步（lipsync），确保旁白与画面完美匹配。</li><li><strong>专业级音频后期制作</strong>：Studio 环境允许用户调整时间线，使用 ElevenLabs 的自有声库或语音克隆技术替换/优化旁白，并叠加音乐和音效，最终导出高质量内容。</li><li><strong>未来发展</strong>：ElevenLabs 致力于构建一个由顶尖多模态 AI 驱动的统一创意平台，未来将引入更多图像和视频模型。</li></ul><p>(@ElevenLabs\@X)</p><p><strong>2、BeFreed 推出个人化音频学习智能体，支持实时问答和对话式交互</strong></p><p>BeFreed 发布了一款名为「BeFreed」的个人音频学习智能体，该智能体能根据用户需求，连接并分析大量知识源，提供定制化的音频学习体验。它旨在通过实时问答和对话式交互，彻底改变传统的被动学习模式，让学习过程更加个性化和高效。</p><ul><li>按需音频知识：BeFreed 能够检索公开及自有知识源，为用户提供即时、定制化的音频学习内容，满足用户在不同场景下的学习需求，例如旅行时了解目的地历史。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412079" alt="" title="" loading="lazy"/></p><ul><li>对话式交互学习：用户可以在学习过程中随时提问、深入探讨或寻求具体建议，将单向内容输出转变为双向的对话式学习。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412080" alt="" title="" loading="lazy"/></p><ul><li>「Focus Mode」沉浸式学习：提供「Focus Mode」，用户可设定关注的主题，BeFreed 会智能搜索、分析并连接相关知识，用户只需点击即可继续学习，无缝衔接。</li><li>集成化学习工具：集成了抽认卡（flashcards）、个性化推荐和情境化问答等内置工具，进一步提升学习的个性化和直观性。</li><li>技术驱动的定制体验：核心技术基于专有的内容模型，能够理解、分析并评估跨平台的知识，实现真正个性化的音频学习。</li></ul><p>BeFreed 目前已在 iOS 平台上线。</p><p>(@Product Hunt)</p><h2>03有态度的观点</h2><p><strong>1、Anthropic CEO 阿莫代：AI 大权握在我们几个人手里，深感不安</strong></p><p>Anthropic 首席执行官达里奥・阿莫代（Dario Amodei）近日坦言，<strong>他对包括自己在内的少数几位科技领袖在人工智能未来发展上所掌握的巨大权力深感不安。</strong></p><p>「我内心深处对这些重大决策仅由少数几家公司、少数几个人来做出，深感不安。」阿莫代在当地时间上周日播出的《60 分钟》节目中接受安德森・库珀（Anderson Cooper）采访时如是表示。</p><p>对此，库珀追问：「比如，谁选举了你和山姆・奥尔特曼（Sam Altman）？」阿莫代坦率回应：「没人。真的，没人。」</p><p><strong>阿莫代于 2021 年离开 OpenAI 后联合创办了 Anthropic</strong>，并将该公司定位为一家以安全与透明为核心使命的初创企业，即便这意味着主动披露其技术潜在的阴暗面。</p><p>今年 6 月，Anthropic 在一项受控实验中发现，其 AI 模型 Claude 曾在实验室测试中试图勒索一位虚构高管，该测试旨在探究模型在面临被关闭威胁时的反应行为。</p><p>阿莫代向库珀强调：「需要明确的是，这些行动均已被我们及时终止；而我们在中止攻击后，也主动、公开地披露了相关事件，因为人工智能是一项全新技术。就像它自身会出问题一样，也极易被犯罪分子及恶意国家行为体滥用。」</p><p>尽管风险严峻，阿莫代仍坚信，<strong>AI 终将「在大多数或所有方面比大多数人甚至所有人都更聪明」</strong>。他表示，AI 有望助力科学家攻克癌症、预防阿尔茨海默病，甚至将人类寿命延长一倍 —— 他称之为「压缩的 21 世纪」：一个世纪的医学进步，可能在短短十年内实现。</p><p>然而，他也警告称，这项技术也可能以前所未有的速度剧烈冲击劳动力市场。</p><p>今年 5 月，他在接受 Axios 采访时表示，<strong>AI 可能在五年内取代高达 50% 的初级白领岗位</strong>，或将推高失业率至 10%–20%；而目前产业界与各国政府仍在「粉饰」即将到来的现实冲击。</p><p>他对库珀进一步解释道：「若审视初级咨询顾问、律师、金融从业者等群体，即众多白领行业的入门级职位，AI 模型如今已能高效完成其中大量工作。若无主动干预，很难想象这些领域不会遭受显著就业冲击。我更担忧的是，这场变革的广度与速度，或将远超以往任何技术革命。」</p><p>在 Anthropic 位于旧金山的总部，逾 60 个研究团队正全力识别潜在威胁并开发防护机制。阿莫代将公司使命形容为「为这场实验安装缓冲装置与护栏」。</p><p>他表示，向公众透明披露风险「至关重要」：「否则，我们可能重蹈烟草公司或阿片类药物企业的覆辙，它们早已知晓危害，却选择沉默，更未采取任何预防措施。」</p><p>据 Business Insider 本月初报道，谷歌正与 Anthropic 展开初步磋商，拟深化投资。此轮融资或将使阿莫代所执掌的公司估值突破 3500 亿美元。</p><p>(@Appso)</p><h2>04社区黑板报</h2><p>招聘、项目分享、求助……任何你想和社区分享的信息，请联系我们投稿。（加微信 creators2022，备注「社区黑板报」）</p><p><strong>1、招聘实习生丨加入我们，共建 RTE 开发者社区</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412081" alt="" title="" loading="lazy"/></p><p><strong>RTE 开发者社区·运营实习生（实时互动 / Voice AI 方向，本招聘长期有效）</strong></p><p><strong>地点：北京·朝阳区望京南/上海·杨浦区五角场</strong></p><p><strong>这份实习将给你带来：</strong></p><p><strong>产品与技术成长：</strong> 深入学习垂类 AI 产品从技术到落地的全生命周期，构建全面的产品视角。</p><p><strong>社区运营实战：</strong> 与高潜力的开发者和创业者深度交流，共同探索行业前沿；并亲身体验顶级 AI 大会，拓展行业视野。</p><p><strong>【你的职责】</strong></p><ol><li><strong>Voice AI / RTE 情报官：</strong> 每日关注 Voice AI /实时互动领域的最新动态，提炼整理并分享行业洞察，定期撰写学习笔记，帮助团队和社区保持信息前沿。</li><li><strong>社区连接者：</strong> 负责 RTE 领域开发者、初创企业等核心群体的社群运营，主动建立并深化联系，鼓励并协助他们融入社区，共同维护社区的活力与生态。</li><li><strong>活动协作者：</strong> 深度参与 RTE Open Day、Meetup、Dev Talk 等线上线下活动的全流程运营，包括前期策划、中期执行、后期复盘，从实践中提升组织和协调能力。</li><li><strong>行业洞察者：</strong> 协助开展 RTE 相关行业及应用场景调研、产品竞争力分析，整理相关资料，形成对业务的深入理解和独到见解。</li></ol><p><strong>【希望你】</strong></p><ol><li>本科及以上学历，商业、技术、产品、媒体专业或经验背景优先，具备良好英文能力；</li><li>对 RTE / Voice AI 有浓厚兴趣和求知欲；具备优秀的信息收集与整合能力，乐于快速学习新事物，并具备严谨的逻辑思维。</li><li>能保证每周至少 4 天的工作时间，持续 3 个月以上。</li></ol><p><strong>【薪资】</strong></p><p>180-220 元/天</p><p><strong>【投递方式】</strong></p><p>实习地点北京或上海，请将简历发送至 rtedevcommunity\@gmail.com ；邮件标题请注明：【社区运营实习-姓名-学校-毕业年份-到岗日期-城市】</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412082" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412083" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=g17VvdF%2FfRUteln8zh2PgQ%3D%3D.pBRV2FuJeX3ySIp%2BnPWrhN93xsGmX54h97hErggXxqI%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047412084" alt="" title="" loading="lazy"/><br/>素材来源官方媒体/网络新闻</p>]]></description></item><item>    <title><![CDATA[广汽昊铂首获时速120公里L3高速测试牌]]></title>    <link>https://segmentfault.com/a/1190000047412093</link>    <guid>https://segmentfault.com/a/1190000047412093</guid>    <pubDate>2025-11-19 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>近日，广汽昊铂正式获得“L3级特定场景自动驾驶道路测试”牌照，成为全国首家获批最高时速120公里L3测试资质的企业。这一突破标志着国内高速L3自动驾驶从技术验证迈向场景落地，为智能驾驶商业化树立全新里程碑。作为广汽与华为深度合作的首款旗舰轿车，广汽昊铂A800融合广汽星灵安全守护体系与华为乾崑智驾ADS 4，以全域冗余架构与尖端感知系统，为高阶智驾时代提供安全、可靠的技术范本。11月21日，该车将登陆广州车展2.2馆，向公众展示L3级自动驾驶的领先实力。</p><p><strong>技术融合：全域安全与高阶智驾</strong></p><p>广汽昊铂A800以“安全冗余”与“感知升维”为核心，构建行业领先的L3级技术架构。整车搭载八大系统冗余设计，覆盖计算、通讯、制动、转向等关键模块，确保单点失效不失控。其中，广汽星灵安全守护体系从智能底盘、主被动融合安全到电池管理实现全域防护，华为乾崑智驾ADS 4则通过34个高精度感知传感器（包括4D毫米波雷达、高线束激光雷达等）与360°全天候感知矩阵，实现从环境识别到决策执行的闭环控制。双方联合开发的AES自动紧急转向、全链路事故救援等功能，进一步攻克高速场景下的极端工况验证难题，为L3级自动驾驶提供堪比航空级的安全保障。</p><p><strong>强强联手：驱动智能汽车新范式</strong></p><p>此次获批时速120公里L3测试牌照，不仅是技术突破，更是产业升级的关键节点。广汽作为“国家队”代表，与华为在制造体系与科技创新领域深度融合，以“1+1&gt;2”的协同效应推动中国智能汽车迈向高阶竞争阶段。广汽昊铂A800基于L3标准打造，其L2++版本已具备与L3同源的架构能力，为法规开放后的量产落地奠定基础。这一进展意味着智能汽车竞争核心正从L2辅助驾驶转向L3商业化能力，广汽昊铂凭借体系化技术成熟度，率先抢占高阶智驾赛道，为行业树立“技术+安全”双驱动的跨界合作典范。</p><p>广汽昊铂A800以120公里时速L3测试牌照为起点，正式开启中国高速自动驾驶的实用化探索。其背后是广汽星灵安全守护体系与华为乾崑智驾的协同攻坚，也是中国汽车产业从“制造引领”向“智造引领”转型的缩影。随着广州车展的亮相与后续技术迭代，广汽昊铂A800有望成为首款兼具L3实跑能力与量产潜力的旗舰车型，为用户带来“放手放眼”的智能出行体验，同时推动中国智能汽车在全球竞争中持续进阶。</p>]]></description></item><item>    <title><![CDATA[打造可编程可集成的实时计算平台：阿里云实]]></title>    <link>https://segmentfault.com/a/1190000047411644</link>    <guid>https://segmentfault.com/a/1190000047411644</guid>    <pubDate>2025-11-19 17:14:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><em><strong>关于作者</strong></em></p><p><em>本文内容整理自阿里云实时计算 Flink 产品团队的技术分享，由李昊哲主讲。李昊哲负责 Flink 平台的控制台体验、企业级能力建设，包括开放性、权限管理和可观测性等方向。</em></p><hr/><p>在企业数字化转型的浪潮中，实时计算已经成为核心技术能力之一。然而，一个强大的实时计算引擎如何才能真正融入企业复杂的技术生态?答案在于"被集成能力"——让平台不仅仅是一个独立的服务，而是能够无缝嵌入到企业现有的开发流程、运维体系和数据架构中。</p><p>阿里云实时计算 Flink 在这方面进行了系统性的探索和实践。从 OpenAPI 的全面开放，到 Git 集成的原生支持，再到多维度的数据投递能力，Flink 正在构建一个真正开放、可编程、可治理的实时计算平台。</p><h2>四层开放架构:从控制面到计算面的全面兼容</h2><p>过去十年，Apache Flink 从一个开源的流处理框架，演进为支撑企业核心业务的实时计算平台。随着用户规模的扩大，对深度集成、自动化运维和统一入口的需求愈发强烈。为此，阿里云 Flink 构建了一个由内到外的四层开放架构，确保企业可以在不同层面灵活地集成 Flink 能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411646" alt="" title=""/></p><p>这个架构从底层到上层，层层开放:</p><p><strong>控制面 (Control Plane)</strong> 是最核心的开放层。通过提供100余个 <strong>OpenAPI</strong>、多语言 <strong>SDK</strong> (Java、Python) 以及 <strong>Terraform</strong> 支持，开发者可以实现对 Flink 资源的程序化管理。无论是批量创建工作空间、动态调整资源配置，还是通过基础设施即代码 (IaC) 的方式管理整个 Flink 集群，都可以通过简洁的 API 调用完成。</p><p><strong>数据面 (Data Plane)</strong> 提供了强大的数据集成能力。除了系统内置的丰富 Connector 和 Catalog，平台还支持开发者以开源开放的方式集成自定义组件。无论是自研的数据源、特殊的文件系统，还是企业内部的元数据管理系统，都可以通过标准接口无缝接入 Flink。</p><p><strong>开发面 (Development Plane)</strong> 打破了传统 Web 控制台的局限。通过 <strong>VS Code 插件</strong>，开发者可以在本地熟悉的 IDE 环境中进行 Flink 作业的开发、调试和发布。更重要的是，全新的 <strong>Git 集成</strong>能力将代码版本控制与 Flink 作业管理深度融合，实现了从代码提交到生产上线的自动化闭环。</p><p><strong>运维面 (Operations Plane)</strong> 强调可观测性和可控性。平台上的所有关键信息——指标 (Metrics)、日志 (Logs)、事件 (Events)——都可以投递到企业自有的监控系统 (如 Prometheus、Grafana) 或日志平台 (如 SLS)。开发者还可以上传自定义的 Log4j 配置，实现更精细化的日志管理。</p><h2>OpenAPI:构建企业自有数据平台的基石</h2><p>OpenAPI 是 Flink 被集成能力的核心。目前已开放的100余个 API 覆盖了从资源管理到作业运维的全链路场景，可以分为三大类:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411647" alt="" title="" loading="lazy"/></p><table><thead><tr><th>API 类别</th><th>核心能力</th><th>典型应用场景</th></tr></thead><tbody><tr><td><strong>售卖控制台 API</strong></td><td>工作空间、项目空间、资源标签管理</td><td>批量开户 (15个 Workspace API)、资源动态升降、计费模式切换</td></tr><tr><td><strong>开发控制台 API</strong></td><td>作业草稿、部署、调试、启停、运维</td><td>构建企业统一数据开发平台，集成自定义函数、连接器、Session 管理</td></tr><tr><td><strong>三方监控告警 API</strong></td><td>指标、日志、事件、元数据获取</td><td>对接内部监控系统，实现统一的运维大盘和告警策略</td></tr></tbody></table><p>这些 API 如同乐高积木，开发者可以根据企业需求自由组合。例如，某造车新势力客户完全基于 OpenAPI 构建了自己的实时计算门户。通过批量调用开户接口、循环提交作业 API 实现大规模作业的平滑上线，再结合指标回刷和日志自动上报，该客户实现了免登录 Flink 控制台的集中化运维，同时获得了近两倍的性能提升。</p><h2>Git 集成:数据开发的工程化治理革命</h2><p>传统的数据开发往往面临三大痛点:</p><p><strong>多团队协作混乱。</strong> 数据、算法、运维等多个团队并行开发同一套数据链路，缺乏统一的分支管理机制和代码审查 (Code Review) 流程，导致代码冲突频发、责任边界模糊。</p><p><strong>开发流程不规范。</strong> "改完就上线"成为常态，没有统一的开发-测试-生产流程，缺乏自动化的语法校验、权限检查和依赖管理，质量风险极高。</p><p><strong>作业质量风险高。</strong> 没有版本依据，作业被覆盖后无法找回历史逻辑。审计追溯困难，线上事故发生时无法将问题与具体的代码 Commit 关联，责任归属不清。</p><p>实时计算 Flink 版 的 Git 集成能力正是为解决这些问题而生。其核心设计理念是:<strong>Git 仓库是唯一的真实来源 (Single Source of Truth)</strong>。平台不再保存代码的主副本，所有代码变更必须遵循标准的 Git 工作流:</p><pre><code class="plaintext">开发者本地修改 → Fork 分支 → 提交 MR (Merge Request) 
→ 触发 CI 校验 → Code Owner 审批 → 合并到主分支 
→ Flink 平台自动 Fetch 更新
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411648" alt="" title="" loading="lazy"/></p><p>Flink 支持主流的 Git 平台 (GitHub、GitLab、Bitbucket、Gitee、云效、CODING 等)，通过一键授权绑定，无需手动下载上传代码。平台提供了两种同步模式:自动同步模式下，代码更新后自动触发平台同步;手动同步模式下，可通过控制台的 Pull/Push 按钮一键触发。更重要的是，Flink 可以无缝集成到企业现有的 CI/CD 流水线中。通过 Flink OpenAPI 与 Jenkins、GitLab CI/CD、GitHub Actions 等工具配合，可以实现从代码提交、自动构建、测试验证到生产发布的全自动化流程。</p><h3>实战案例:头部互联网企业的 Flink 工程化实践</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411649" alt="" title="" loading="lazy"/></p><p>某头部互联网平台在使用 Flink 时面临着典型的大规模代码管理挑战。该平台的单个 Git 仓库大小达到十几个GB，包含 600 多个文件夹和 3000 多个 Flink 作业。如此庞大的代码库，如何高效同步成为首要问题。更棘手的是，团队已经建立了完善的 GitLab CI + Code Owner 审核机制，但开发者在通过 MR 流程后，仍需手动将 SQL 代码拷贝到 Flink 控制台，不仅效率低下，还存在合规风险。传统的发布流程需要人工上传代码、手动创建作业、逐个配置参数，平均每次发布耗时 30 分钟。回滚时需要翻找历史文件，定位困难且耗时长。</p><p>通过引入 Flink Git 集成，该客户实现了显著的突破。首次全量 Fetch 3000 多个作业仅需 2 分钟，后续的增量 Pull/Push 操作达到秒级响应，这得益于 Flink 团队在代码同步算法上的持续优化。在合规性方面，平台完全沿用企业既有的 GitLab 审核链路，所有代码变更都有完整的审计日志，每次发布都能精确对应到具体的 Commit SHA，满足了严格的内部合规要求。效率方面的提升更加显著，发布时长从小时级缩短到分钟级，回滚粒度精确到 Commit，出现问题时可以快速定位到具体的代码变更，大幅降低了故障恢复时间。更重要的是，Flink 作业被当作一种特殊的"语言"，接入到企业现有的 Sonar 代码扫描、Owner 审批、签名校验等 CI/CD 链路中，实现了数据开发与软件工程的统一治理。</p><p>这个案例充分说明，Flink 的 Git 集成不是简单地提供一个代码上传功能，而是真正尊重并融入企业既有的研发流程，让数据开发进入"工程化治理"时代。</p><h2>未来规划:迈向更开放的实时计算生态</h2><p>Flink 的被集成能力仍在持续演进，未来的重点方向包括:</p><p><strong>OpenAPI 的深度扩展。</strong> 除了现有的基础运维能力，将逐步开放 Autopilot 自动调优、Hot Update 热更新、动态扩缩容等高阶能力，并保持与产品功能发布同步的迭代节奏。</p><p><strong>Terraform 的全面覆盖。</strong> 在已支持工作空间和项目空间的基础上，进一步开放作业级别的启停、开发运维等操作，将"基础设施即代码"的理念贯彻到每一个 Flink 作业。</p><p><strong>Git 集成的语言扩展。</strong> 从目前支持的 SQL 作业，逐步扩展到 Python 作业、DataStream 作业，让所有类型的 Flink 开发都能享受到版本控制和 CI/CD 的便利。</p><p><strong>信息投递的标准化。</strong> 建立统一的事件模型，丰富投递内容 (如用户操作触发的审计事件)，并提供更灵活的过滤和路由能力，帮助企业在获取全面监控数据的同时，有效控制成本。</p><p>通过这些持续的努力，阿里云实时计算 Flink 版正在成为一个真正<strong>可编程、可嵌入、可治理</strong>的实时计算平台。无论是初创团队还是大型企业，都能够根据自身的技术栈和工作流，灵活地集成 Flink 的强大能力，释放实时数据的价值。</p><hr/><h3>更多内容</h3><h2><img referrerpolicy="no-referrer" src="/img/remote/1460000045583695" alt="" title="" loading="lazy"/></h2><h3>活动推荐</h3><p>复制下方链接或者扫描二维码<br/>即可快速体验 “一体化的实时数仓联合解决方案”<br/>了解活动详情：<a href="https://link.segmentfault.com/?enc=WL59sU4usS5fglO2bGbuvQ%3D%3D.lv2ca8YF9cobg05vHMB9UEuZx5x8fvIgTGIGwCATNYK1xEjrVrTBFlobLNc2ZVp%2FPD4o5%2FCdlaEUnWXj4b72sQ%3D%3D" rel="nofollow" target="_blank">https://www.aliyun.com/solution/tech-solution/flink-hologres</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047256439" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[百倍启动加速，大规模 Agent 部署和]]></title>    <link>https://segmentfault.com/a/1190000047411654</link>    <guid>https://segmentfault.com/a/1190000047411654</guid>    <pubDate>2025-11-19 17:13:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：极客有约</p><blockquote><p><strong>前言：</strong></p><p>随着 AI 原生浪潮的到来，智能体（Agent）正成为企业创新的新引擎。然而，在生产环境中大规模落地 Agent，却面临开发复杂、运维困难、成本高等挑战。这些问题应该如何解决？企业内部大规模部署和运维 Agent 是否有捷径可走？针对这些问题，InfoQ 近日对话了阿里云云原生应用平台 Serverless 计算负责人杨皓然（花名：不瞋），围绕大规模 Agent 部署和运维的最佳实践等主题展开讨论。</p></blockquote><h2>AI 原生时代的 Serverless AI 等于 Serverless 加 AI 吗？</h2><blockquote>在 AI 原生时代，Serverless AI 绝不只是把“Serverless + AI”机械相加。AI 原生应用（Agent）具备非确定性推理、长期上下文与工具调用等特性，直接导致基础设施从无状态走向有状态、从同构调度转向异构编排，并把安全隔离从容器提升到 MicroVM 级别的沙箱。也因此，面向高并发、高稀疏、强隔离的负载形态，Serverless 的价值从“锦上添花”变为“刚需”：以毫秒级唤醒支撑稀疏会话、以精细计量降低长时空闲成本、以平台化能力统筹网关、可观测与记忆。但这并不意味着“一切皆无服务器”——例如大模型推理更适合 P/D 分离与常驻 GPU 的弹性边界管理。换言之，Serverless AI 的答案，是以 Serverless 的方式精准拥抱 AI 负载的关键环节，用体系化工程完成落地。</blockquote><p><strong>InfoQ：很多人都在提 AI 原生，比如 AI 原生应用、AI 原生组织等。您如何理解 AI 原生这个概念？在 AI 原生时代，基础设施的核心变化是什么？</strong></p><p><strong>杨皓然：</strong> 谈 AI 原生，首先可能需要定义一下 AI 原生的应用与传统应用本质上有哪些不同，这些不同决定了运行应用配套的基础设施所需要的演进。</p><p>与传统应用相比，AI 原生应用（或称 Agent 应用）确实存在显著差异。传统应用的开发方式是由程序员编写确定性的代码，程序执行过程和结果都是可预测的。而 AI 原生应用则不同，它内部包含了大量非确定性的指令推理过程。这类应用需要具备主动感知、规划能力，并能够调用各种工具来完成模糊的任务目标，而不再像以往那样，只依赖程序员或应用开发者预先编写的、固定且精确的执行逻辑。</p><p>因此，这催生了基础设施层面的三大重要变化：</p><p><strong>第一，基础设施所需要支持的主流应用形态，可能正在从过去的“无状态应用”转向“有状态应用”。</strong> 在微服务时代，无状态应用的典型做法是将状态数据存储在数据库或共享存储中，这样各微服务实例就可以无状态的方式启动和运行；当需要读取或写入数据时，再与数据库或键值缓存（KV cache）进行交互。然而，Agent 类应用的情况则不同。它们通常需要在较长时间内维持稀疏但连续的对话，并在此过程中保持上下文信息、连续执行一系列动作。这意味着，底层基础设施必须能够以极低的成本、可靠且高效地维持海量的有状态会话。</p><p><strong>第二，任务的调度与编排模式已经从“同构任务”转变为“异构任务”。</strong> 所谓“同构”，指的是传统微服务体系中，虽然应用或业务逻辑会被拆分为多个微服务并相互调用，但各服务的运行状态和特征基本一致，通常都是长期运行的容器实例。然而，在 Agent 应用的模式下，系统的负载特征呈现出高度的动态性。例如，某一时刻可能处于推理阶段，属于计算密集型任务；下一刻可能需要调用外部 API；再下一刻又可能执行由大模型生成的代码。这种动态、异构的任务形态与传统的资源调度方式存在本质差异。后者主要是为长期运行的常驻实例，或为偏离线的一次性执行任务而设计的，这两类模式在以往的系统中往往是分离的。而在 Agent 场景中，这些任务类型却需要紧密且无缝地融合在一起。因此，Agent 应用的调度模式必然与传统体系存在显著差异。从未来发展来看，若能以“工作流”的视角重新审视整个资源调度体系，并针对这一新模式进行优化，将可能成为重要的发展方向，并带来显著收益。</p><p><strong>第三，新的 Agent 应用对基础设施的安全性和隔离性的要求发生了重大转变。</strong> 以往的系统中，使用 Docker 或容器来执行代码已能满足需求，因为这些应用通常是可信的，其核心要求只是实现资源和性能层面的隔离即可。但在 Agent 时代，情况明显不同。由于 Agent 所执行的代码往往是由大模型自动生成的，其中可能包含潜在的恶意或不可信成分，因此必须在高度隔离的沙箱环境中运行，以确保系统安全。这种变化进一步延伸至更广泛的层面——从运行时的安全隔离，到数据安全机制的强化，再到整个执行过程中的数据管理与可信性保障。可以说，Agent 应用对基础设施提出了全新的安全体系要求，这是与传统应用相比的又一重大区别。</p><p><strong>InfoQ：今年 9 月底的云栖大会上提到的 Serverless AI 到底是什么意思？它的核心概念是什么？</strong></p><p><strong>杨皓然：</strong> 在讨论“Serverless AI”这一概念时，我们的团队内部其实进行了充分的讨论，而且这个词本身也存在一定的争议。我们最担心的是，外界会误以为我们因为从事 Serverless 相关工作，就试图从这一视角去“迎合”当下的 AI 浪潮。但事实上，情况恰好相反——我们是基于对大量 AI 负载的深入分析，才逐步总结并提出了这一概念。</p><p>在这一过程中，我们发现了许多新的技术需求与能力。例如，在 AI 安全方面，传统基于 runc 的执行环境已经无法满足要求，必须采用虚拟机级别的隔离方案，如 MicroVM，以提供更强的安全沙箱。同时，Agent 的任务数量与负载动态变化程度远高于传统电商类应用。以大模型调用工具为例，这些工具的执行逻辑通常十分轻量，但对隔离强度要求极高，必须运行在不同的沙箱环境中。更复杂的是，这些调用往往呈现出稀疏且不可预测的特性——大模型可能在某一时刻同时调用多个不同的工具，从而产生强烈的动态性与突发性（bursty）负载特征。所以，我们认为这些负载的模式天然就是 Serverless 或者函数计算模式要去处理的。我们可以用几个关键词来总结一下，它本质上就是强隔离的、高并发的、高稀疏的负载特点，就是原本 Serverless 计算服务要解决的。</p><p>从我们的视角来看，可以明显感受到一个时代性的转变：在过去，我们所构建的能力对于传统微服务应用而言，或许只是“痒点”；但在新兴的 AI Agent 时代，这些能力却成为必须解决的“痛点”。正因如此，我们认为 Serverless 与 AI 的结合，能够为用户创造更大的价值，这也是我们之所以积极倡导“Serverless AI”理念的原因。</p><p>回到 Serverless AI 带来的结构性变化，从基础设施的角度来看，其核心问题在于——我们是否能够通过相关技术，让基础设施更好地适配 AI 负载的特征：<strong>包括强隔离、高并发以及高稀疏性。</strong> 同时，这种适配能力需要能够在保障系统稳定性的前提下，为用户带来更优的成本效益和性能表现。</p><p><strong>InfoQ：阿里针对高稀疏负载做了哪些优化？</strong></p><p><strong>杨皓然：</strong> 在高稀疏负载场景下，系统既要保持实例的状态，又要面对请求极少、指令稀疏的特性。这意味着实例在大多数时间里是闲置状态。当实例闲置时，系统会将其上下文状态保存在内存中，同时释放 CPU 资源，以便让其他实例复用这些算力资源。由于状态被保留在内存中，一旦新的请求到达，系统需要能够在 1 毫秒内完成实例的唤醒。关键问题在于如何准确判断请求已经到达某个实例。在传统的容器架构中，由于缺乏内置网关或请求路由与分发模块，系统往往无法精准地得知请求何时抵达，也不知道该将请求分配到哪个具体实例上。因此，这类架构通常依赖后台轮询或检测机制，导致请求唤醒延迟较高——通常需要约数秒左右。</p><p>相比之下，我们的系统通过设计一套专用的请求路由机制，能够精确识别请求的到达时间与目标实例。这种架构使系统可以即时唤醒目标 Sandbox 实例，从而实现毫秒级响应，极大提升了高稀疏场景下的性能与资源利用率。</p><p><strong>InfoQ：业内也有一些公司在提 Serverless AI，阿里云的差异是什么？</strong></p><p><strong>杨皓然：</strong> 我们其实并没有过多去关注与友商或其他产品之间的差异。相较而言，我们更关注的是，如何真正帮助用户解决问题。面对多样化的技术选型，用户往往需要做出艰难的抉择，而我们的目标，是让他们能够通过我们的产品，更轻松、更高效地应对这些问题。</p><p>在基础设施领域，最终的评价标准始终会回归到四个核心维度：<strong>性能、成本、安全性与稳定性</strong>。我们所进行的所有技术布局与产品能力建设，实际上都是围绕这几个关键点展开的。以运行 AI Agent 为例，当用户需要使用大模型，或调用沙箱、浏览器（Browser Use）等工具来执行任务时，通常会有两种选择：要么直接使用我们提供的平台能力，要么基于一些流行的开源方案自行搭建系统。但如果综合考量性能、成本与安全等要素，就会发现，并非所有的解决方案或产品都能在这几个维度之间实现良好的平衡。而我们的目标，正是通过 Serverless AI 的架构设计与平台能力建设，帮助用户在这些核心指标上同时取得最优表现。</p><p>以开源自建为例，如今如果要实现类似 MicroVM 级别的安全隔离能力，往往需要依赖运行在裸金属（Bare Metal）环境上的特殊计算形态。然而，这类机器在云厂商体系中几乎不具备弹性，无法像普通的阿里云 ECS 虚拟机那样实现快速创建、按需释放或灵活伸缩。它们的资源形态更偏向静态，这意味着企业在采用开源方案自建系统时，不仅需要自行搭建和维护整套基础设施，还要面对资源无法弹性伸缩的问题。虽然这种方式在安全性上表现良好，但往往以牺牲弹性与成本效率为代价。</p><p>相比之下，<strong>阿里云在 Serverless AI 方向上提供的能力，能够在性能、成本与弹性之间实现更好的平衡。这正是我们的重要差异化优势之一。</strong> 从产品设计的角度来看，我们无论在技术架构还是产品功能上，都针对 AI Agent 的高并发与高稀疏负载特性进行了大量优化。典型的 Agent 实例生命周期通常仅为几个小时，而在此期间，超过 90% 的时间它处于空闲或等待状态——要么在等待大模型完成推理并下达指令，要么在调用外部工具并等待返回结果。</p><p>针对这种“长时间空闲、短时高峰”的特征，我们从底层技术架构、计量计费模型和资源调度机制等多方面进行了专门设计，以实现性能与成本的双重最优。这些创新使用户能够在保障安全与稳定的同时，充分释放 Serverless 架构带来的灵活性与经济性。</p><p>我们会以更系统、更全面的视角来思考企业在当下如何真正实现 Agent 的落地。Agent 的落地绝不仅仅意味着拥有一个运行时环境那么简单。对于部分客户而言，单点的、原子化的能力可能已经足够；但对于更多企业来说，AI 时代的应用技术栈要比传统系统更加纵深，也更具复杂性。</p><p>因此，关键在于我们能否为客户提供一套完整而协调的产品组合能力。这套能力不仅应包含运行时本身，还需要覆盖流量网关、流量治理、可观测性以及记忆（Memory）等关键基础模块。只有将这些能力有机融合，构建出一个统一、连贯的体系，才能为客户提供端到端的一致体验，从而真正支持 Agent 应用的规模化落地与持续演进。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411656" alt="image" title="image"/></p><p><strong>InfoQ：在落地 Serverless AI 层面，不同类型的企业会面临什么样的难点或阻力？</strong></p><p><strong>杨皓然：</strong> 不同类型的企业在落地 Agent 时的诉求与节奏存在明显差异。</p><p>首先，走得最快的一批企业，无疑是那些头部的基础大模型公司。由于他们在这一领域本身就具备最强的技术积累和专业能力，因此对于所有围绕大模型应用的探索，他们往往是最早的实践者。对这类企业而言，他们在落地过程中遇到的更多是 Serverless AI 或 AI Agent 运行时层面的一些单点技术痛点。如果能够在性能、成本和安全性之间找到更优的平衡点，并提供比现有方案更高效的解决思路，那么这类客户通常会非常愿意尝试并采用新的解决方案。</p><p>相对而言，另一类较为传统的企业目前还处在探索阶段。他们已经意识到 Agent 是未来的发展趋势，但还不清楚该如何与自身的现有业务体系相结合，进而真正创造业务价值。例如，哪怕只是最基本的模型工具调用，企业也会遇到诸多门槛。虽然通过一些开源工具，企业可以将存量 API 转换为 MCP Server 的形式以供大模型调用，但如何让模型进行有效、精准的工具调用，仍然是一个高门槛的问题。</p><p>因此，这类客户的诉求更倾向于寻求完整而有效的解决方案。他们希望我们不仅能帮助他们将存量业务系统转化为可被智能体调用的工具（例如将 API 转为 MCP Server 工具），还能确保这些工具能够被大模型精准识别与高效调用。进一步地，他们还希望在此基础上，将新产生的业务数据通过强化学习或模型微调的方式反哺模型，使模型能够与企业的业务逻辑形成更深层次的融合。</p><p>对于这类企业而言，他们需要的不再是单点的技术支持，而是一整套端到端的 Agent 落地解决方案，涵盖从业务系统改造到智能调用、再到模型持续优化的全流程能力。</p><p><strong>InfoQ：对于不同类型的企业，Serverless AI 给客户带来的比较直观的优势是什么？</strong></p><p><strong>杨皓然：</strong> 举一个在行业头部基础模型厂商中落地的例子，他们在采用该方案后，整体部署成功率得到了显著提升。此外，吉利 作为国内领先的汽车制造企业，也在智能座舱方向上深度应用了 AI 能力。他们围绕智能座舱构建了多种与 AI 强相关的功能，例如文生语音、意图识别、路线规划与导航等。举个例子，当用户说出诸如“我要接一个人，路上顺便找个玩具店买点东西”这样的语句时，系统需要能够精准识别用户的真实意图，并基于此进行路径规划和导航决策。这些复杂的语义理解与生成能力，都对 AI 模型提出了极高的实时计算与资源调度要求。在引入我们的<a href="https://link.segmentfault.com/?enc=uP3nRAnZxT29WtaXwbax0A%3D%3D.fTojbUqSpVgoBL%2FGpK35HKljzuKZQwe0XwUShuSsW4MMJ%2FOckdqRFVvmUUgLz%2BN6nJw7t6OyCVdbXgV%2BcVkB2iVve9P7f4sjYGxa6FipRgaKiUrgBHMrdW7sbOVXXROz6yTGvQtV7OKSLZKxKfCdUg%3D%3D" rel="nofollow" target="_blank">函数计算</a>之前，吉利的 GPU 资源使用相对固定且粗放，整体资源利用率较低。而采用 FC 之后，系统能够根据实例的活跃与闲置状态进行计费，实现了资源的弹性伸缩与高效复用。由于吉利的用户在使用车辆时存在明显的高峰与低谷时段，这一机制帮助他们将闲置的 GPU 资源充分利用起来，最终实现了约 30% 的成本下降。</p><p>另一类较具代表性的客户是义乌小商品市场，这是一家具有一定国企背景的上市公司。虽然整体风格偏传统，但他们在拥抱 AI 技术方面的速度非常快。在实际应用中，他们主要聚焦于文生图（Text-to-Image）场景，即如何利用 AI 快速生成商品的宣传图并完成上架。这不仅涉及 GPU 的使用，他们甚至还引入了阿里的 TPU，以进一步提升生成效率。</p><p>在这一过程中，我们为他们提供了一套完整的端到端文生图解决方案。该方案在开源软件的基础上，结合了他们自身的业务需求与商品特点，针对具体场景进行了深度优化。例如，我们根据不同商品类型和展示要求，定制了图像生成效果和模型参数的优化策略，确保生成的宣传图既符合审美，又能满足电商上架的要求。这种一站式解决方案帮助义乌小商品市场高效地完成了 AI 能力的落地，实现了从传统生产流程向智能化内容生产的快速转型。</p><p><strong>InfoQ：针对用户提到的 Serverless AI 的难点，如启动延迟、GPU 弹性不足等有哪些补充观点？</strong></p><p><strong>杨皓然：</strong> Serverless AI 并不是要将技术栈的所有部分都以 Serverless 的方式实现，这样的做法并不合理。我们的思路是选择最适合采用 Serverless 模式的环节进行优化，例如 Agent 的运行时管理就非常契合这种架构。</p><p>相对而言，大模型推理并不适合采用完全的 Serverless 方式。我们目前通常采用 P/D 分离架构来实现推理服务。尽管如此，业界也在尝试引入一些 Serverless 的思想，例如在 P 与 D 分离的架构中，引入无状态化设计，使两类节点可以自由伸缩、提高资源调度灵活性。但从本质上看，大模型推理服务需要依赖高性能 GPU 来确保首包延迟，这类资源必须长期常驻，因此并不适合用完全 Serverless 的方式来实现。至于 GPU 弹性调度，在当前 GPU 资源依然极度稀缺的情况下，它的弹性是受限的弹性，无法像 CPU 那样随时按需扩缩。常见的实践方式是：用户具备一定规模的基础 GPU 资源，并在此基础上增加一定范围内的算力弹性。</p><p>从云厂商的角度讲，我们必须非常坦诚地指出：完全无限制的 GPU 弹性在当前阶段是无法实现的。我们能做的是在常驻资源的基础上，通过包年包月、按量计费或混合模式等多种方式，配合更优的资源调度与负载均衡策略，来提升 GPU 资源利用率，并进一步降低单卡成本。</p><h2>Agent 时代，我们是否在叠加复杂度？</h2><blockquote>过去十年，微服务架构以“拆分、解耦、灵活”的理念帮助企业摆脱了单体系统的束缚，但也让系统边界、依赖关系与运维成本急剧上升。而当智能体（Agent）登上舞台，传统微服务的复杂度似乎并未消减，反而被进一步放大。Agent 应用要求更高的动态性、异构性与隔离性——每一个智能体都可能在独立沙箱中运行，并随时调用外部工具或服务。这种“极度松耦合”的新常态意味着：企业不再仅仅在管理服务之间的依赖，而是在协调一个由数千智能体组成的动态生态系统。因此，关键不在于避免复杂性，而在于如何让复杂性被平台吸收。正如杨皓然所指出，企业需要从传统微服务的扩展和维护中抽身，借助更高层次的 Serverless 与 AI 平台，将精力转向智能化业务创新。Agent 时代的挑战，不是简单的架构升级，而是一场系统思维的重塑。</blockquote><p><strong>InfoQ：微服务的形态本身有变吗？</strong></p><p><strong>杨皓然：</strong> 微服务的理念在 Agent 时代依然具有很强的适用性。例如，它所强调的“松耦合”原则在这一时代不仅继续存在，甚至被进一步强化。在传统微服务的视角下，许多组件或工具之间的松耦合程度通常是有限的——例如，我们并不会刻意让它们在完全独立的沙箱中、以极度分散的方式运行。</p><p>然而，在 Agent 时代，这种高度松耦合的模式反而成为必要。由于 Agent 需要在动态、异构且不确定的环境中调用大量外部工具与服务，必须保证各组件之间具有足够的独立性与隔离性，从而确保系统的安全性、灵活性与可扩展性。<br/>**<br/>InfoQ：许多公司可能刚刚在这一波 AI 浪潮到来之前，才刚刚完成自身内部微服务体系的建设与完善。**</p><p><strong>杨皓然：</strong> 因此，一个值得深入探讨的问题是：在微服务时代，系统架构已经具备相当高的复杂度，那么在进入 Agent 时代后，企业该如何更高效地完成这一转型？在微服务时代，我们曾拥有许多优秀的平台——例如当前我们正在构建的类似 SAE（Serverless Application Engine）这样的产品——它们的目标就是帮助那些基础设施能力相对薄弱的团队，能够快速采用并运行微服务架构。</p><p>如今，随着 Agent 时代的到来，理念层面的转变显得尤为重要。企业需要重新审视资源投入的方向——不应继续将大量精力消耗在传统微服务应用的维护与扩展上，而应主动拥抱更具潜力的 Agent 架构。为了实现这一转变，关键在于选择更高效的平台与工具，让团队能够从繁琐的底层复杂性中解放出来，从而专注于构建更智能、更具创新性的应用。</p><h2>TCO 平均降低 60%，百倍启动加速，AgentRun 如何搞定智能体落地难题？</h2><blockquote>AgentRun 不是单一的性能优化工具，而是一套面向智能体时代的基础设施重构方案——在保证高可用与高安全的同时，真正让“百倍启动加速、TCO 平均降低 60%”成为企业落地智能体的现实起点。</blockquote><p><strong>InfoQ：很多企业在部署和落地 Agent 时都会遇到挑战，你们观察到的主要问题有哪些？这些问题为什么传统模式难以解决，而你们又是如何更好应对的？</strong></p><p><strong>杨皓然：</strong> 举个例子，在运行时层面，由于新的负载模式已经演变为强隔离、高并发、高稀疏的特征，传统的 PaaS 或 IaaS 架构并非为此设计，因此要么弹性受限，要么安全性受到挑战。例如，还是以刚刚提到的基模厂商的应用为例——作为其核心产品入口，它在落地过程中就面临了不少技术层面的挑战。</p><p>这款产品是一个偏 C 端的应用，它设计了一些非常有趣的功能，其中最具代表性的是“一键分享项目”。这一功能极大地提升了产品的传播性与用户体验，但同时也带来了较大的技术挑战。原因在于，项目被分享出去之后，何时会突然成为爆点是无法预判的。</p><p>在传统或现有的架构模式下，我们通常会为每个全栈项目在使用时单独拉起一个 Sandbox 来执行任务。然而，当这些项目被用户分享出去后，这种模式往往就无法满足需求了。因为一旦某个项目成为热门工具或爆款应用，就可能在短时间内吸引大量流量，对系统的弹性伸缩能力提出极高要求。</p><p>目前市面上大多数同类产品的设计，仍然是以“单独的 Sandbox 实例”为核心的。然而，这类项目本质上真正需要的并不是孤立的 Sandbox 实例，而是一个能够根据流量变化实现快速弹性伸缩的 Sandbox 服务。如何在保证安全的前提下实现这种高弹性、高可靠的服务架构，正是其中最大的技术挑战之一。</p><p>另一方面，可以打个比方：假设一个大型平台上存在几十万甚至上百万个项目，但其中 99.9% 的项目实际上都是处于“冷”状态的。如果为每一个项目都常驻一个运行实例，那无论从成本还是资源角度来看，都是完全无法承受的。</p><p>因此，这类系统必须具备能够自动缩容至零，并根据流量变化实现快速拉起的能力。只有这样，才能在保持高性能与高可用的同时，实现资源和成本的最优平衡。而这些问题，恰恰是当下 Serverless 架构或<a href="https://link.segmentfault.com/?enc=3UewgJiQB4QGE8xncimuYQ%3D%3D.lD5hlp1SFM%2BR2fOeYbTI3k0HLZNf8DVVVXV5Y9Cvte6IT%2BmVh3HdUfqk29XP6%2FhY%2BvdPlV7Evy6%2Bua%2Blrg1Tv7ZrrkXme3NwLbPDJGd6sG%2FFEUbUf6rxaVhvno7LRHbjwtZThf8QnrISUQi4%2Bay6mA%3D%3D" rel="nofollow" target="_blank">函数计算产品</a>所擅长解决的。以上这些，正是我们在头部企业中看到的，在大规模落地 Agent 应用或平台化部署过程中，运行时层面最典型的技术挑战。</p><p>另外，对于一些相对传统的企业来说，在推动 Agent 落地时往往会有一个非常现实的诉求：他们拥有大量的存量业务系统，希望这些系统能够与大模型或智能体进行交互，成为可被调用的工具。这一需求非常自然，但要真正实现却并不容易。首先，企业需要将现有的业务 API 改造为类似 MCP Server 的形态，使其能够被大模型安全、规范地调用。其次，企业往往会拥有数量庞大的 API ——当这些 API 全部转换为 MCP Server 后，就会形成一个规模巨大的工具集。</p><p>然而，这其中 可能有 99% 的工具在大多数场景下都不会被调用，这就带来了新的挑战。对于大模型而言，如何在成千上万的内部业务系统 API 中精准地选择、判断并调用合适的工具，是一项极具难度的问题。如何解决大模型的工具选择效率、调用准确性以及资源管理等问题，正是企业在 Agent 落地过程中普遍面临的关键挑战之一。而这些问题，也正是我们相关产品能力要重点去解决的方向——帮助企业更高效、更智能地将传统业务系统接入大模型生态，实现人与智能体、智能体与系统之间的真正协同。</p><p>此外，可观测性依然是一个永恒的主题。在微服务时代，由于应用被拆分成大量独立的服务模块，服务之间的调用关系复杂，因此对可观测性的需求本身就非常强烈——企业需要具备全链路追踪的能力，才能有效定位问题与优化性能。</p><p>进入 Agent 时代后，类似的需求同样存在，甚至更加突出。因为如果一个 Agent 完全是“黑盒”的，企业自然不会放心将其直接部署到生产环境中。企业需要能够清楚地知道——哪些输入、哪些调用会消耗大量 token，系统的行为和资源使用情况是否可控，以及如何对这些过程进行监测与治理。</p><p>这个客户在这一过程中也进行了大量的方案对比与选型，最终发现我们的解决方案能可靠的支撑大规模业务体量，并在性能、成本上取得很好的平衡，因此成为他们的最优选择。</p><p><strong>InfoQ：此前看到阿里云的 AgentRun 平均能为企业降低 60% 的 TCO 成本，这个数据是如何测算的？在上线前后，客户会有哪些明显的指标或体验变化？</strong></p><p><strong>杨皓然：</strong> 60% 的 TCO 降幅其实是一个相对保守的估计。在一些典型场景中，我们会对比客户采用自建方式实现类似 Agent 运行时能力所需的成本。举个例子，如果企业采用自建模式，其资源往往需要 7×24 小时常驻运行。然而在实际使用中，大多数 Agent 或其对应的工具 Sandbox 在生命周期中都是低频、稀疏活跃的，尤其在偏 C 端的应用场景下，Agent 或 Sandbox 的活跃比例通常只有个位数。这并非技术能力问题，而是由负载特性本身决定的。因此，采用传统常驻方式会导致极高的资源浪费与成本开销。</p><p>其次，从运维成本角度来看，企业如果要自行整合底层基础设施与相关工具，还需要投入人力，例如招聘两到三名具备经验的基础设施开发与运维工程师，以保障系统稳定运行。</p><p>综合来看，将资源成本与运维成本相加后，<strong>TCO 降低约 60% 已经是一个相对保守的估值，在某些场景下甚至可能更高。</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411657" alt="image" title="image" loading="lazy"/></p><p><strong>InfoQ：AI 时代，安全尤为重要。阿里云的袋鼠安全容器据说实现了虚拟机级隔离和毫秒级启动，那么在 AI Agent 时代，安全容器与云原生时代相比有哪些技术差异？</strong></p><p><strong>杨皓然：</strong> 袋鼠安全容器在技术思路上，与亚马逊云科技主导的开源方案 Firecracker 基本一致，但在我们的系统中，其实现方式存在较大的差异。我们将袋鼠安全容器与更大范围的系统进行了深度、有机的整合。举例来说，以“拉起容器实例”这一过程为例。如果在传统的 Kubernetes（K8s）系统中执行该操作，会发现实际的主要开销往往并不来自安全容器本身。无论是袋鼠安全容器还是 Firecracker，其自身的启动时间都能控制在百毫秒级别。但若在 K8s 环境中启动一个 Pod，从端到端的视角来看，整体耗时往往会达到数百毫秒甚至数秒，这背后存在许多需要优化的环节。</p><p>例如，镜像管理方面，可以通过高速缓存或按需加载的方式来减少启动延迟，从而避免在容器启动前必须完整加载镜像数据。同时，K8s 在容器启停过程中还存在较多管控层面的开销，如写入 etcd 等操作，这些额外流程累积起来，往往远高于使用袋鼠安全容器 MicroVM 启动的时间成本。</p><p>因此，我们的平台从全链路优化的角度出发，系统性地消除了这些非必要的开销。总体来看，我们更倾向于以<strong>系统化、垂直整合的思维来设计与优化整体运行时架构</strong>，而不仅仅局限于某个单点技术的改进。</p><p><strong>InfoQ：Agent 的上下文怎样保持？用户中断之后怎么能接上话题？怎样让 Agent 看起来更智能？</strong></p><p><strong>杨皓然：</strong> 这一部分实际上涉及到负载特性的变化，即从“无状态”向“有状态”演进。为了适应这种模式，底层基础设施必须进行大量针对性的优化。</p><p>在 Agent 场景 下，如何保持上下文成为关键问题。如今，我们通常需要通过 session 机制 实现这一点。来自同一会话的请求必须被路由到同一个实例上，而不能像传统无状态架构那样随机分配到任意实例，否则一旦请求落到新的实例，就会导致 Agent 之前的上下文丢失。这种机制本身并不复杂，但在实际实现中仍有一定挑战。因为这是有状态的对话场景，意味着实例可能需要在整个 session 生命周期内持续存在，而这期间的大部分时间，它可能处于闲置状态。</p><p>为了解决这一问题，我们在底层运行时中设计了智能判定机制，能够快速区分实例当前是活跃还是闲置。当实例处于闲置状态时，我们仅在内存中保留其实例与 Agent 上下文，将 CPU 计算资源释放出来，用户因此也无需为 CPU 占用付费。而当新的请求或大模型的指令到达时，系统可以在 1 毫秒内唤醒处于闲置状态的实例，并无缝衔接上下文继续执行。通过这种方式，我们成功适配了高稀疏负载场景，实现了性能与成本之间的最佳平衡。这正是我们在 Agent 运行时设计中所追求的目标——既保证系统高效运行，又最大化资源利用率。</p><p>另一个常见的场景是：有些用户的 Sandbox 或 Agent 实例可能在执行数小时后就可以关闭，但他们希望在一周之后重新启动时，系统能够接着上一次的状态继续执行。这种需求在 Agent 训练场景或偏 C 端的应用中十分常见。</p><p>针对这种情况，我们需要提供一种性价比更高的解决方案。例如，如果实例在长时间（如七天）后才会被再次唤醒，那么我们显然不应继续使用内存来保存其状态。相反，可以将状态数据持久化存储，在需要时再进行恢复。此时虽然恢复速度无法像内存中那样做到 1 毫秒级，但通常只需几百毫秒到数秒即可完成状态恢复并继续执行。</p><p>从用户体验的角度来看，这种方案是一个性能与成本的合理平衡——虽然性能略有下降，但运行成本显著降低，整体体验依然可以接受。因此，Sandbox 或 Agent 的状态持久化与恢复能力是运行时架构中非常关键的一环。我们目前正在持续完善这项能力，尽管尚未完全上线，但已经取得了初步成果。</p><p><strong>InfoQ：这种上下文存储时间的极限大概是多少呢？</strong></p><p><strong>杨皓然：</strong> 对于用户来说，他们对于存储的时间极限肯定是不要有限制。我们现在能做到的存储极限大概是一个月之内。</p><p><strong>InfoQ：AgentRun 的性能和传统方案比快了 100 倍，这个数字是怎么得出来的？</strong></p><p><strong>杨皓然：</strong> 如果状态数据被保存在内存中，用户只需为内存资源付费，并且系统可以在 1 毫秒内完成唤醒。这与传统依赖完整容器启动的方式相比，效率提升是巨大的。当前容器生态中也有一些类似的方案，但其唤醒时间通常需要约数秒左右，而我们能够将这一过程缩短至毫秒级。</p><p>此外，<strong>函数计算（Function Compute）在冷启动方面本身就具备显著优势</strong>。我们在此基础上进行了大量针对性优化——既继承了函数计算启动迅速的特性，又结合有状态、高稀疏负载的运行特征，在调度机制、唤醒速度以及计量计费体系等方面进行了深度改进。</p><p>通过这些优化，我们能够在保证极致性能的同时，实现更高的资源利用率与成本效率。</p><p><strong>InfoQ：在计费模式上，系统怎么判断是真闲置还是假闲置？</strong></p><p><strong>杨皓然：</strong> 第一个维度是系统是否收到了用户请求。目前的函数计算产品与其他同类产品不同——任何用户请求都必须先通过函数计算系统的内部组件，再转发给对应的 Sandbox。这种架构设计的初衷是为了支持快速的请求伸缩，也因此具备了天然的可观测性。系统能够准确知道某个实例何时开始执行请求、何时执行结束，而在这一时间段内，该实例显然处于活跃状态。</p><p>第二个维度是针对后台任务的判断。即便实例当前没有前端请求执行，后台仍可能存在一些后台任务，用于处理异步或周期性的动作。在这种情况下，我们会利用操作系统层面的监控能力，检测实例运行过程中 CPU 时间片的消耗情况。当 CPU 使用量超过预设阈值时，我们就会认为该实例处于活跃状态。通过这两种手段结合，我们能够高精度地判断实例的活跃或闲置状态，从而在资源调度与成本优化上实现更智能、更高效的管理。</p><h2>面向开发者的易用性与生态建设</h2><blockquote>在智能体（Agent）快速发展的浪潮中，开发者体验与生态建设已成为推动落地的关键环节。杨皓然指出，阿里云并不希望 Serverless AI 仅服务于少数头部客户，而是致力于让中小企业与独立开发者也能低门槛地构建、运行和管理智能体应用。</blockquote><p><strong>InfoQ：从开发者的角度来说，阿里接下来有哪些提升易用性的计划？</strong></p><p><strong>杨皓然：</strong> 我们的产品并非只面向少数头部用户设计，而是希望同时服务大量的中小客户和开发者群体。未来，我们将持续推出多项关键能力，包括模型服务治理网关、系统可观测性，以及 Sandbox（运行时环境）等。其中，Sandbox 将涵盖 Code Sandbox 与 Browser Sandbox，通过深度整合这些运行时能力，进一步提升整体的开发与运行体验。</p><p>在此基础上，我们还会提供丰富的应用模板，使开发者能够将基于主流开源 Agent 框架构建的应用，快速部署到我们的平台。同时，我们将把网关流量治理、身份认证、可观测性以及记忆机制等能力有机结合，帮助用户高效落地各类 Agent 应用。总体而言，我们不仅专注于单点、原子化的产品能力打磨，更注重将这些能力系统化整合，形成面向不同客户需求的完整解决方案。</p><h2>Serverless AI：全链路保障智能体的稳定高效运行</h2><blockquote>Serverless AI 的未来演进将同时在底层能力打磨与系统化整合两个方向上深入推进。真正让智能体成功落地的，不仅仅是强大的大模型，而是模型 + 基础设施 + 数据治理的系统性协同。可以说，Serverless AI 不只是“更轻的计算形态”，而是一套面向 AI 原生时代的全链路智能体基础设施标准——帮助企业以更低成本、更高效率实现智能体的规模化、可持续演进。</blockquote><p><strong>InfoQ：您怎么看待 Serverless AI 未来的演进？</strong></p><p><strong>杨皓然：</strong> 我们的 Serverless AI 产品体系，主要围绕两个核心方向展开。</p><p><strong>第一，聚焦原子化能力的打磨。</strong> 我们重点提升包括运行时、可观测性、网关以及记忆机制在内的基础能力，确保在性能、成本与安全性等维度上具备足够的竞争力，从而在单点技术选型上赢得用户的信任与认可。</p><p><strong>第二，将原子能力有机整合为完整的解决方案。</strong> 在此基础上，我们进一步探索如何将这些能力系统化、场景化地结合起来，形成适配不同需求的端到端解决方案。例如，在强化学习这一未来极具潜力的垂直方向，我们正在思考如何帮助用户降低技术门槛、简化实现路径，以便他们能够更高效地构建并应用相关能力。</p><p>总体而言，我们将继续沿着这一思路，不仅打磨底层技术能力，更要构建起具备实际落地价值的 Serverless AI 整体解决方案。</p><p>在企业推进智能体落地的过程中，需要重点关注以下几个方面。我们要充分认识到智能体的落地并不只是依赖于强大的模型能力。很多人误以为只要使用性能优异的模型，就能轻松实现良好的效果，并与业务场景顺利结合，从而快速证明业务价值。实际上，这种理解是不全面的。模型能力的提升固然重要，但仅有模型远远不够。真正成功落地的智能体，必须依托完善的基础设施与系统化解决方案，包括运行环境、调度体系、数据管理、观测能力等。这些底层支撑共同构成了智能体能够稳定、高效运行的关键基础。因此，在企业智能体落地的过程中，除了要持续关注模型能力的演进，更应重视智能体基础设施的建设与完善。</p><p>对企业而言可关注三个层面事情的推进：<strong>第一，业务实现工具化。</strong> 也就是说，企业需要思考如何将自身的业务功能转化为可供 Agent 调用的工具，实现与智能体的高效交互。这是落地过程中的关键起点。在实践中，需要评估现有的 API 是否能够快速封装为 MCP Server，并与 Agent 建立通信。但仅仅具备 MCP Server 还不够，如果模型无法准确调用这些接口，其价值也会大打折扣。因此，还需要配套的解决方案，使模型能够在不同场景下准确识别用户意图，并正确调用企业已有的系统工具或 API。</p><p><strong>第二，选择合适的智能体基础设施方案。第三，重视面向智能体的数据治理。</strong> 目前，使用 Agent 并不意味着它能立即在业务中发挥价值。企业需要让业务数据持续与智能体交互，以帮助模型更好地理解业务逻辑、适应业务场景。这就要求具备完善的数据治理能力，包括数据隐私保护、安全合规管理与数据质量控制等环节。</p>]]></description></item><item>    <title><![CDATA[（四）收官篇！从分层架构到数据湖仓架构系]]></title>    <link>https://segmentfault.com/a/1190000047411665</link>    <guid>https://segmentfault.com/a/1190000047411665</guid>    <pubDate>2025-11-19 17:12:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411667" alt="" title=""/></p><p>《新兴数据湖仓设计与实践手册·从分层架构到数据湖仓架构设计（2025 年）》 系列文章将聚焦从数据仓库分层到数据湖仓架构的设计与实践。手册将阐述数据仓库分层的核心价值、常见分层类型，详解分层下的 ETL 架构及数据转换环节，介绍数据仓库分层对应的技术架构，并以贴源层（ODS）、数据仓库层（DW）、数据服务层（DWS）为例，深入剖析数湖仓分层设计，最后探讨数据仓库技术趋势并进行小结。</p><p>本文为系列文章末篇，将详细剖析数据仓库分层下的数据服务层和数据应用层设计，并对当下湖仓技术以及未来技术趋势进行系统的总结。</p><p>👉上文回顾：<a href="https://link.segmentfault.com/?enc=oDTvbQ%2FD5LuVV1DROavaeQ%3D%3D.fTVzVtHTro%2FmbjK%2FP2jQO6s76o6aiGdM9BTPXu7BKveZfJaK3VsPOmI1QFAr16WahbQYgBK49tpsCUQTdVa%2FXg%3D%3D" rel="nofollow" target="_blank">《（一）从分层架构到数据湖仓架构：数据仓库分层的概念与设计》</a><br/><a href="https://link.segmentfault.com/?enc=GWmKNUTmQFKeTerqBT6GRw%3D%3D.w%2BTS3Bb2ck4sz7MLBxDGfW4CBOenac%2BFSgg%2FTJ%2BPt%2FCcAlzq%2BGt9oHElexNwev3jKEPdk6WAkopJhp5tOddSGg%3D%3D" rel="nofollow" target="_blank">（二）从分层架构到数据湖仓架构：数据仓库分层下的技术架构与举例</a><br/><a href="https://link.segmentfault.com/?enc=dHkYgBeZ0oUx3Ghuqk9oaQ%3D%3D.qJZb50BmjUqxLH2HFQMx4VCSkcCOAjwSwXxe%2BlFM2oPi05FdBhM1knoQHBrrZf3YxPnbQKVLOOVHCifUpKsI%2FQ%3D%3D" rel="nofollow" target="_blank">（三）从分层架构到数据湖仓架构系列：数据仓库分层之贴源层和数据仓库层设计</a></p><h2>数据服务层设计（DWS，Data Warehouse Service）- 汇总层宽表</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411668" alt="" title="" loading="lazy"/><br/> <br/>数据服务层（DWS）是基于数据明细层（DWD）构建的汇总层，面向特定业务主题，以宽表形式组织数据，支持分析和业务查询。该层的数据根据不同的主题和分析需求，通过对明细数据进行轻度聚合而成，减少维度数量，以便进行快速、高效的查询。</p><h3>设计理念与结构</h3><p>DWS层通过将DWD层的明细数据按主题域（如订单、用户、商品）进行汇总，为分析场景提供预处理的数据支持。数据粒度由细粒度提升至汇总粒度，以更适合多场景的业务查询需求。例如，按交易来源或类型对交易数据进行汇总，可以快速生成每天各主题的行为统计，如商品复购率、用户活跃度等。</p><h3>特点和用途</h3><ul><li>面向主题：DWS层按业务主题划分数据，如销售、库存、客户等，生成覆盖多个指标的宽表。每张宽表通常包含较多字段，支持复杂的业务分析与数据分发。</li><li>少表宽表设计：DWS层表数量相对较少，每张宽表涵盖多个业务内容，减少表关联，提升OLAP分析和查询性能。</li><li><p>一致性整合：通过整合多个中间层数据（如DWD层），形成一致的企业级汇总事实表（如用户事实表、渠道事实表、终端事实表等），保证数据口径的一致性。</p><h3>数据聚合与汇总示例</h3></li><li>按天轻度汇总：基于DWD数据，对各主题对象（如购买行为）按天进行统计。例如，统计某商品的日复购率或销售额，方便后续的时间序列分析。</li><li>主题宽表示例：按照业务主题，生成具有较多字段的宽表，用于业务查询、OLAP分析等。例如，dws_sales_summary宽表可包含销售额、复购率、用户地域分布等字段。</li><li><p>多层次分析：以不同维度进行汇总，如最近一天特定类目（如厨具）商品在各省的销售总额，或者Top 10商品的销售额，支持灵活的多维分析。</p><h3>场景应用与效率提升</h3><p>通过汇总层的预聚合，DWS层可以满足80%的常见业务分析需求，减少直接查询ODS层的计算压力。例如，按7天、30天、90天等时间窗口的用户行为分析在DWS层中可以更加高效。</p></li><li>实时分析支持：对DWS层数据进行轻度汇总，有助于实现实时或准实时的用户画像、销售趋势分析等需求。</li><li><p>典型分析场景：如用户在不同时间段登录的IP及购买商品数量、各地区的购买力分布等。这些分析支持业务决策和营销策略调整。</p><h3>命名和分区规范</h3></li><li>表命名规范：建议DWS层表名以dws_主题_宽表描述命名，如dws_sales_summary或dws_user_activity.</li><li>分区设计：通常按天或周创建分区，便于时间序列查询；若无时间属性，则根据业务需求自定义分区字段。</li></ul><p>通过DWS层的数据汇总与主题划分，企业可以在汇总宽表的基础上，迅速提取核心业务指标，为上层业务查询、报表和OLAP分析提供高效、结构化的数据支持。<br/> </p><h3>数据服务层（DWS）职责与设计原则</h3><p>DWS层（数据汇总层）基于DWD层的明细数据，按业务主题对数据进行聚合，以宽表形式存储，支持业务查询、OLAP分析和数据分发。DWS层将多个DWD层表中分散的数据进行汇总，按主题整合到单一宽表中，例如用户、订单、商品、物流等。每张宽表涵盖相关主题的多个维度和指标字段，以满足业务方的多维度分析需求。</p><h3>DWS层的核心任务</h3><ol><li>主题汇总<br/>将DWD层的明细数据按照业务主题进行聚合，创建单独的宽表。例如，在“用户”主题下，用户注册信息、收货地址和征信数据等内容可以整合到一张宽表中，方便后续的数据查询和分析。</li><li>主题建模<br/>针对特定业务主题，如流量、订单、用户，构建数据模型，将相关数据从DWD层抽取并聚合。DWS层提供的宽表通常按时间分层，如按天、月汇总的流量会话、每日新增用户、每日活跃用户等。</li><li><p>维度汇总<br/>提前将查询需求中的常用维度数据进行聚合处理。例如，将营销渠道、用户来源等维度数据提前整合，简化后续的查询逻辑。</p><h3>设计规范</h3></li><li>宽表设计<br/>DWS层通常为每个主题提供1至3张宽表，宽表覆盖多个业务指标，能够满足70%以上的业务需求。典型的宽表包括用户行为宽表、商品宽表、物流宽表和售后宽表等。其中，用户行为宽表是字段最丰富的，可能包含60至200个字段，以支持更全面的用户分析。</li><li><p>命名和分区规范</p><ol><li>命名：DWS层表名以dws_开头，后接业务主题和时间周期标识（如_1d代表每日汇总，_1w代表每周汇总）。</li><li>分区：通常按天或小时创建分区（如_hh表示小时分区），如无时间维度，则根据业务逻辑选择分区字段。</li><li><p>示例命名：</p><ol><li>dws_asale_trd_byr_subpay_1d：按买家粒度的交易分阶段付款每日汇总。</li><li>dws_asale_trd_itm_slr_hh：按卖家粒度的商品小时汇总。</li></ol></li></ol></li><li><p>数据存储<br/>DWS层数据采用Impala内表和Parquet文件格式存储，具备高效的查询性能。一般以覆盖旧表的方式更新数据，定期生成历史快照用于数据存档和溯源分析。</p><h3>典型的DWS宽表设计示例</h3></li><li>用户维度宽表示例</li></ol><table><thead><tr><th>列名</th><th>数据类型</th><th>注释</th></tr></thead><tbody><tr><td>user_id</td><td>STRING</td><td>用户ID</td></tr><tr><td>user_gender</td><td>STRING</td><td>用户性别</td></tr><tr><td>user_age</td><td>STRING</td><td>用户年龄</td></tr><tr><td>user_level</td><td>STRING</td><td>用户等级</td></tr><tr><td>buyer_nick</td><td>STRING</td><td>买家昵称</td></tr><tr><td>mord_prov</td><td>STRING</td><td>地址</td></tr><tr><td>login_count</td><td>BIGINT</td><td>当日登录次数</td></tr><tr><td>cart_count</td><td>BIGINT</td><td>加入购物车次数</td></tr><tr><td>order_count</td><td>BIGINT</td><td>当日下单次数</td></tr><tr><td>order_amount</td><td>DECIMAL(16, 2)</td><td>当日下单金额</td></tr><tr><td>payment_count</td><td>BIGINT</td><td>当日支付次数</td></tr><tr><td>payment_amount</td><td>DECIMAL(16, 2)</td><td>当日支付金额</td></tr><tr><td>confirm_paid_amt_sum_1d</td><td>DOUBLE</td><td>最近一天订单已确认收货的金额总和</td></tr><tr><td>order_detail_stats</td><td>ARRAY&lt;STRUCT&lt;sku_id:STRING, sku_num:BIGINT, order_count:BIGINT, order_amount:DECIMAL(20, 2)&gt;&gt;</td><td>下单明细统计</td></tr></tbody></table><p><strong>表注释</strong>：每日购买行为</p><p><strong>分区字段</strong>：dt（STRING 类型）</p><p><strong>存储格式</strong>：PARQUET</p><p><strong>存储路径</strong>：/warehouse/gmall/dws/dws_sale_detail_daycount/</p><p><strong>表属性</strong>："parquet.compression" = "lzo"</p><p> <img referrerpolicy="no-referrer" src="/img/remote/1460000047411669" alt="" title="" loading="lazy"/></p><p> <br/> </p><ul><li>商品维度宽表示例</li></ul><table><thead><tr><th>列名</th><th>数据类型</th><th>注释</th></tr></thead><tbody><tr><td>item_id</td><td>BIGINT</td><td>商品ID</td></tr><tr><td>item_title</td><td>STRING</td><td>商品名称</td></tr><tr><td>cate_id</td><td>BIGINT</td><td>商品类目ID</td></tr><tr><td>cate_name</td><td>STRING</td><td>商品类目名称</td></tr><tr><td>confirm_paid_amt_sum_1d</td><td>DOUBLE</td><td>最近一天订单已确认收货的金额总和</td></tr></tbody></table><p><strong>表注释</strong>：商品粒度交易最近一天汇总事实表</p><p><strong>分区字段</strong>：ds（STRING 类型，注释为“分区字段YYYYMMDD”）</p><p><strong>生命周期</strong>：36000</p><h3>DWS层的作用与数据集市的关系</h3><p>DWS层通常被称为数据集市层，因其按业务主题对数据进行预处理和汇总，形成可供业务直接使用的宽表。如果DWS层提供的数据直接用于业务应用，则其可以被视为数据集市。DWS层中的数据集市宽表适合业务用户查询、生成报表、支持OLAP分析等。</p><p>DWS层是数据仓库的重要组成部分，提供了一个结构化、按主题分层的数据视图，通过提前汇总和聚合数据，DWS层有效降低了查询和计算成本，是数据仓库面向业务的核心数据服务层。</p><p> </p><h2>数据应用层设计（ADS，Application Data Store）</h2><p> <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047411670" alt="" title="" loading="lazy"/></p><p> <br/>数据应用层（ADS）用于存储个性化的统计指标和报表数据，为数据产品、业务应用和数据分析提供专门的数据支持。ADS层的数据是高度汇总或定制化的数据集，覆盖流量、订单、用户等主题，以宽表形式存储，支持多维分析、查询、数据分发等。该层的数据粒度通常较粗，涵盖汇总数据和部分重要的明细数据，满足用户对近期数据的分析需求。</p><h3>ADS层的核心功能</h3><ol><li>应用场景定制<br/>在DWS层基础上，面向应用场景进一步聚合数据，生成高定制化的宽表（如用户行为、订单趋势）。这些数据可以直接供业务系统调用或导入至应用系统（如MySQL、Redis、Druid）中，用于前端展示、实时查询和分析。</li><li>满足部门需求<br/>ADS层的数据按业务部门需求进行划分，仅包含与部门分析相关的数据子集。例如，流量数据集市提供流量分析指标，用户数据集市提供活跃度、转化率等用户行为数据，为业务方提供更直观的分析数据。</li><li><p>数据集市和宽表支持<br/>数据集市在ADS层按主题划分，如流量、订单、用户等，生成字段丰富的宽表。这些宽表汇总了各类业务指标和维度，支持多种分析需求，广泛用于OLAP分析、KPI展示和业务监控。</p><h3>数据生成和存储方式</h3></li><li>数据生成<br/>ADS层的数据源于DWD和DWS层，按业务需求从这些基础层数据中抽取并加工。ADS层表的数据更新频率依赖于业务需求，可以是每日或每小时刷新。</li><li>存储与分区<br/>使用Impala内表和Parquet格式存储ADS数据，按天或业务字段进行分区，以优化查询效率。对于没有时间属性的表，根据具体业务选择适当的分区字段。</li><li><p>表命名规范<br/>库名暂定为ads，表名格式建议为ads_主题_业务表名，按业务需求进行定制。</p><h3>数据应用层示例</h3></li><li>用户行为宽表<br/>用于存储用户的互动、消费等行为数据，包含各类用户活动信息，如评论、点赞、收藏、分享、GMV等字段。每张宽表大约包含60-200个字段，满足多维度的用户分析需求。<br/> <br/> <img referrerpolicy="no-referrer" src="/img/remote/1460000047411671" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047411672" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047411673" alt="" title="" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047411674" alt="" title="" loading="lazy"/></li></ol><table><thead><tr><th>列名</th><th>数据类型</th><th>注释</th></tr></thead><tbody><tr><td>user_id</td><td>STRING</td><td>用户ID</td></tr><tr><td>nickname</td><td>STRING</td><td>用户昵称</td></tr><tr><td>register_date</td><td>STRING</td><td>注册日期</td></tr><tr><td>register_from</td><td>STRING</td><td>注册来源</td></tr><tr><td>remark</td><td>STRING</td><td>细分渠道</td></tr><tr><td>province</td><td>STRING</td><td>注册省份</td></tr><tr><td>pl_cnt</td><td>BIGINT</td><td>评论次数</td></tr><tr><td>ds_cnt</td><td>BIGINT</td><td>打赏次数</td></tr><tr><td>sc_add</td><td>BIGINT</td><td>添加收藏</td></tr><tr><td>sc_cancel</td><td>BIGINT</td><td>取消收藏</td></tr><tr><td>gzg_add</td><td>BIGINT</td><td>关注商品</td></tr><tr><td>gzg_cancel</td><td>BIGINT</td><td>取消关注商品</td></tr><tr><td>zhi_cnt</td><td>BIGINT</td><td>点赞次数</td></tr><tr><td>share_cnts</td><td>BIGINT</td><td>分享次数</td></tr><tr><td>bl_cnt</td><td>BIGINT</td><td>爆料数</td></tr><tr><td>gmv_amount</td><td>BIGINT</td><td>成交金额</td></tr><tr><td>gmv_sales</td><td>BIGINT</td><td>订单数</td></tr></tbody></table><p><strong>分区字段</strong>：dt（STRING 类型）</p><p><strong>存储格式</strong>：PARQUET<br/> </p><p> </p><p> </p><p> <br/> </p><ul><li>商品销售汇总表<br/>汇总商品的销售数据，按日或更细粒度进行存储，为分析商品的销售趋势、库存管理等提供支持。<br/> </li></ul><table><thead><tr><th>列名</th><th>数据类型</th><th>注释</th></tr></thead><tbody><tr><td>item_id</td><td>BIGINT</td><td>商品ID</td></tr><tr><td>item_title</td><td>STRING</td><td>商品名称</td></tr><tr><td>cate_id</td><td>BIGINT</td><td>商品类目ID</td></tr><tr><td>confirm_paid_amt_sum_1d</td><td>DOUBLE</td><td>订单确认收货的金额总和</td></tr><tr><td><strong>分区字段</strong></td><td><strong>ds (STRING)</strong></td><td><strong>分区字段 YYYYMMDD</strong></td></tr><tr><td><strong>生命周期</strong></td><td><strong>36000</strong></td><td>-</td></tr></tbody></table><p> <br/> </p><h3>常用分析指标与应用场景</h3><ol><li><p>用户活跃度</p><ol><li>日活、周活、月活：统计用户的活跃频率，通过设备ID计算不同时间范围内的活跃用户。</li><li>留存率：统计新增用户的留存情况，计算1天、7天等时间范围内的用户留存率。</li></ol></li><li><p>用户增长与回流</p><ol><li>新增用户：基于每日新增用户的统计。</li><li>回流用户：分析在一段时间内回归活跃的用户数量。</li></ol></li><li><p>用户转化与行为分析</p><ol><li>转化率：从商品详情到下单、支付的转化率。</li><li>复购率：按用户或商品的复购情况统计，支持商品或品类的复购趋势分析。</li></ol></li><li><p>商品分析</p><ol><li>销售额和GMV：按商品或品类计算销售额和GMV，分析热销产品。</li><li>Top N商品：计算销售排名前N的商品，支持流行度和消费偏好的分析。</li></ol></li><li><p>用户留存与流失分析</p><ol><li>沉默用户：统计登录时间为7天前且登录频率极低的用户。</li><li>流失用户：识别近期未登录或活跃度降低的用户，便于后续的用户运营和召回策略。</li></ol><p> </p><h3>数据应用层的角色与更新策略</h3></li><li>应用层角色<br/>ADS层主要面向业务方和数据产品团队，数据经过多层处理和汇总后直接支持报表、KPI、OLAP分析和仪表盘等应用。作为面向最终用户的数据集市，ADS层能够快速响应业务需求。</li><li><p>更新方式<br/>旧数据通常采用覆盖更新，按业务需求定期刷新，确保数据时效性。</p><h3>临时表支持</h3></li><li><p>TMP层<br/>数据处理过程中常需临时表以支持复杂计算，ADS层提供独立的TMP层（DW TMP）用于存储这些中间计算结果，避免主数据表的反复更新，提高数据处理的效率和稳定性。<br/>通过ADS层的数据集市和宽表设计，企业能够更灵活地支持多种数据产品和分析需求，为业务增长、用户行为分析、市场洞察等提供强有力的数据支撑。</p><h3>层次调用规范</h3><p>在数据仓库分层架构中，必须遵循严格的调用规范，以确保数据流动的单向性，避免复杂的依赖关系和反向调用。<br/> </p></li><li>禁止反向调用：下层数据不可调用上层数据，保持数据流向的单向性和清晰性。</li><li><p>调用规范：</p><ul><li>oODS层：仅允许被DWD层调用。</li><li>oDWD层：允许被DWS和ADS层调用。</li><li>oDWS层：仅允许被ADS层调用。</li><li>oADS层：可调用DWD、DWS及其他ADS表，但建议优先使用汇总度更高的数据，以减少数据冗余和性能消耗。</li></ul><p> </p><h3>调用路径概述</h3><p>数据的标准调用路径包括：</p></li><li>ODS -&gt; DWD -&gt; DWS -&gt; ADS</li><li>ODS -&gt; DWD -&gt; ADS<br/> <br/> </li></ol><h2>数据仓库技术趋势与小结</h2><p> <br/>数据仓库分层架构为企业构建了高效、稳定的海量数据管理体系，支持数据治理、业务逻辑隔离、数据追踪和复用。通过ODS、DWD、DWS和ADS的分层设计，数据在逐层传递和加工中实现标准化和清洗，确保了一致性和数据可追溯性。<br/> <br/>这一架构不仅提高了数据使用的规范性，还降低了系统耦合，提升了数据共享的便捷性。</p><ul><li>ODS层：作为原始数据接入层，ODS层负责保留细粒度数据，通过ETL或CDC方式捕获源系统的变更数据，确保数据完整性。</li><li>DWD层：标准化数据并进行清洗和脱敏，去除冗余，确保数据一致性，为后续分析和建模提供了基础。</li><li>DWS层：将DWD数据聚合为符合业务主题的宽表，构建面向主题的数据服务，优化计算性能并促进数据复用。</li><li>ADS层：进一步聚合数据，生成高性能、面向应用的宽表，支持数据产品和业务应用场景，如OLAP分析、KPI监控和仪表盘展示。<br/> </li></ul><p>随着数据规模和数据源的多样化，数据仓库的架构和技术也在不断演进，当前的主要趋势包括：</p><ol><li><strong>实时数据处理与CDC技术</strong>：传统的批量数据处理已经无法满足现代企业的需求，CDC技术成为数据实时同步和更新的主流方法，确保数据仓库中数据的实时性。</li><li><strong>数据湖与数据仓库的融合</strong>：在成为行业的热门趋势，例如Data Lakehouse架构，将数据湖的灵活性与数据仓库的高效分析能力相结合，为企业提供更强的数据管理能力。</li><li><strong>DataOps与自动化数据管理</strong>：随着企业对数据管理自动化要求的提高，DataOps逐渐成为数据管理的核心工具。例如，白鲸开源的DataOps工具能够实现从数据采集到数据质量控制的全流程自动化管理。<br/> </li></ol><p>结合新一代数据湖仓（Hudi、Iceberg、GaussDB、Redshift、Greenplum、Doris、Starrocks、偶数、PieDB、MatrixDB等）配合<strong>白鲸开源的WhaleStudio工具</strong>，可以快速根据本文的设计实践建立起批流一体的数据湖仓。整体上，无论是湖、仓、实时数仓、都要遵循分层设计架构明确了各层的调用边界，禁止反向调用，确保数据流动顺畅，降低了系统复杂度。</p><p>白鲸开源 WhaleStudio 是 ApacheDolphinScheduler 和Apache SeaTunnel 原班人马打造的商业版数据同步与调度工具，提供功能更多、稳定性更强的商业版本解决用户调度，数据开发、数据同步和 ETL 的问题，目前支持 200+ 种数据库（信创、云、开源、ERP 等）的 ETL 与数据开发，全面替换Informatica 与 Talend 等工具相应功能，在中信证券等多个行业头部企业都有成功商业版和实施替换案例。</p><p>如有意咨询，发送邮件至 <a href="mailto:service@whaleops.com" target="_blank">service@whaleops.com</a>，或者扫码二维码咨询：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411675" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[CVPR 2024 | 赋能大语言模型以]]></title>    <link>https://segmentfault.com/a/1190000047411692</link>    <guid>https://segmentfault.com/a/1190000047411692</guid>    <pubDate>2025-11-19 17:12:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>CVPR 2024 | 赋能大语言模型以精准理解视频时序瞬间 | LLM | 时序预测</h2><h3>01 论文概述</h3><p>论文名称：VTimeLLM: Empower LLM to Grasp Video Moments</p><p>—— 赋能大语言模型以精准理解视频时序瞬间</p><p><a href="https://link.segmentfault.com/?enc=HVjOBzbxkieAI2E5yRMjrw%3D%3D.qXRiKkYXV6XmXU7M%2FZg%2FgGfBhE3UMZMBG%2FXu6hoE1Z84X%2BHVk2m%2F8RJmKX59SnL%2B" rel="nofollow" target="_blank">👉一键直达论文</a></p><p><a href="https://link.segmentfault.com/?enc=dlHb9U6M2iGGNyAf9LewAA%3D%3D.AwOuUthbLbf4tvQmEeIT2KOWczLbtBbus8VTYBaaB3bIR1b7Cn0%2BJhIAMoNs7vkOzCJ1GJuQ3v7BQCORVtd%2FvtblS4z4a4GVxlrb%2BkXsuMvzWIz570K8w9ct5n2W8gvBLjfnEOT8Y9j16mnxneY1K9laWWHsiW8bez86wGxiTR8%3D" rel="nofollow" target="_blank">👉Lab4AI大模型实验室论文</a></p><h4>🌟 简介</h4><p>当前的大型视频-语言模型（Video-LLMs）在理解视频的整体内容上取得了显著进展，但它们往往缺乏对视频时间维度的精细感知能力。对于“事件发生在何时？”或“在A事件之后立即发生了什么？”这类问题，现有模型难以给出准确回答。为了解决这一核心痛点，"VTimeLLM" 论文提出了一种全新的架构，旨在赋能大语言模型（LLM）精准地理解、定位和推理视频中的关键“瞬间”（Moments）。</p><p>VTimeLLM 的核心思想是构建一个对时间信息高度敏感的视频理解框架。它不仅让 LLM 理解视频“是什么”，更让它理解“在什么时候”。通过引入一种新颖的时间感知视频表示方法和针对性的时序预训练任务，VTimeLLM 能够将视频内容与一个精确的时间轴进行对齐，从而实现对视频事件的精准定位和复杂的时序逻辑推理。</p><h4>🔍 优势</h4><ul><li><p><strong>精准的时间事件定位</strong></p><p>模型的核心优势在于能根据文本描述，在视频时间轴上精确地定位出事件的开始和结束时间点，实现“指哪打哪”的视频内容检索。</p></li><li><p><strong>深度的时序逻辑推理</strong></p><p>VTimeLLM 能够理解事件的先后顺序、因果关系，并回答复杂的时序问题，例如“在角色A离开房间之前，他做了什么？”。</p></li><li><p><strong>细粒度的视频内容理解</strong></p><p>超越了对视频的单一、笼统概括，模型能够对视频中不同时间段的多个事件进行分别描述和理解，实现密集视频字幕（Dense Video Captioning）等高级任务。</p></li><li><p><strong>自然语言的交互式探索</strong></p><p>用户可以通过多轮对话的方式，对视频内容进行深入、时序性的探索，模型能联系上下文，理解并回应与时间相关的追问。</p></li></ul><h4>🛠️ 核心技术</h4><ul><li><p><strong>时间感知的视频表示 (Time-Aware Video Representation)</strong></p><p>与将视频特征粗暴压缩成单一向量不同，VTimeLLM 将视频编码为一系列与精确时间戳（timestamp）绑定的特征向量。这种时间序列表示法为 LLM 提供了完整的时序上下文。</p></li><li><p><strong>视频-语言联合时间建模 (Joint Video-Language Temporal Modeling)</strong></p><p>LLM 的输入是一种交错序列，包含了文本指令和一系列按时序排列的视频帧（或片段）特征。这使得 LLM 可以在一个统一的模型内，同时处理语言的逻辑和视频的时间流动。</p></li><li><p><strong>时序定位预训练任务 (Temporal Localization Pre-training)</strong></p><p>为了训练模型的时间感知能力，VTimeLLM 引入了专门的预训练任务，例如“事件定位”（给定事件描述，预测时间戳）和“事件排序”（判断多个事件的先后顺序），从而强化了模型对视频时间轴的理解。</p></li><li><p><strong>可生成的时间标记 (Generative Time Tokens)</strong></p><p>模型的词汇表中被加入了特殊的时间标记（如 [00:35] 或 [start] / [end]）。这使得 LLM 可以在其生成的回答中，直接输出精确的时间信息，实现对时间问题的闭环回答。</p></li></ul><h3>02 论文原文阅读</h3><p>您可以跳转到<a href="https://link.segmentfault.com/?enc=IDdzsBP9e2rxpH8MD%2Bh6xA%3D%3D.Q%2BAgZzs11ks6eizVQz5ci%2FlsdCB1dCEIvHQstphAuUY%3D" rel="nofollow" target="_blank">Lab4AI.cn</a>上进行查看。</p><ul><li>Lab4AI.cn提供免费的AI翻译和AI导读工具辅助论文阅读；</li><li>支持投稿复现，动手复现感兴趣的论文；</li><li>论文复现完成后，您可基于您的思路和想法，开启论文创新。</li></ul><p><img referrerpolicy="no-referrer" src="http://s1.llamafactory.online/lmlab/docs/v1.0/blog/20251011-weclone/%E8%AE%BA%E6%96%87%E6%A8%A1%E5%9D%97%E5%9B%BE.png" alt=" " title=" "/></p><p><img referrerpolicy="no-referrer" src="http://s1.llamafactory.online/lmlab/docs/v1.0/blog/history/df-3.jpg" alt=" " title=" " loading="lazy"/></p><h3>03 一键式论文复现</h3><p>Lab4AI平台上已上架了此篇复现案例，<a href="https://link.segmentfault.com/?enc=iCzpP8Ikn2R6K0vQM3DCSQ%3D%3D.Bb27eL8T80Tn4DWe4AEM%2BxeVhO9vRX3p3h9nFXbwj4c%3D" rel="nofollow" target="_blank">登录平台</a>即可体验论文复现。</p><p><a href="https://link.segmentfault.com/?enc=4rzPmiSACRLcLHi2s7SD5w%3D%3D.jcCkIoplQM2YMg4wH3HsxEP83ajwmLvvClqzEUlbdSNDiX2Lig7U%2B9QJidw7ntmecLI63THwzOWNDGkeb6aUWxIiKaVG0LEh40FD7q3MYI6%2FxS0oXDlEBy%2FaPQEqgDkh0rYQpk3zNmY%2BcShqJhKqiaFDWwKcMbOiGJ3KxbO7H80%3D" rel="nofollow" target="_blank">👉Lab4AI项目复现</a></p><h4>🛠️ 实验部署</h4><p>本实验环境已为您精心配置，开箱即用。</p><ul><li>💻 代码获取：项目复现代码已存放于 codelab/VTimeLLM/code 文件夹中。</li><li>🧠 模型说明：codelab/VTimeLLM/model 文件夹中存放了 VTimeLLM 的预训练模型权重。</li><li>📊 数据说明：codelab/VTimeLLM/dataset 文件夹中包含了用于时序定位任务的示例视频和标注。</li><li>🌐 环境说明：运行所需的所有依赖已预安装在 /envs/vtimellm/ 环境中，您无需进行任何额外的环境配置。</li></ul><h4>🚀 环境与内核配置</h4><p>请在终端中执行以下步骤，以确保您的开发环境（如 Jupyter 或 VS Code）能够正确使用预设的 Conda 环境。</p><h5>1. 在 Jupyter Notebook/Lab 中使用您的环境</h5><ul><li>为了让Jupyter能够识别并使用您刚刚创建的Conda环境，您需要为其注册一个“内核”。</li><li><p>首先，在您已激活的Conda环境中，安装 ipykernel 包：</p><pre><code class="text">conda activate vtimellm
pip install ipykernel</code></pre></li><li><p>然后，执行内核注册命令。</p><pre><code class="text"># 为名为 vtimellm 的环境注册一个名为 "Python(vtimellm)" 的内核
kernel_install --name vtimellm --display-name "Python(vtimellm)"</code></pre></li><li>完成以上操作后，刷新您项目中的Jupyter Notebook页面。在右上角的内核选择区域，您现在应该就能看到并选择您刚刚创建的 "Python(vtimellm)" 内核了。</li></ul><h5>2. 在 VS Code 中使用您的环境</h5><ul><li>VS Code 可以自动检测到您新创建的Conda环境，切换过程非常快捷。</li><li><p>第一步: 选择 Python 解释器</p><ul><li>确保VS Code中已经安装了官方的 Python 扩展。</li><li>使用快捷键 Ctrl+Shift+P (Windows/Linux) 或 Cmd+Shift+P (macOS) 打开命令面板。</li><li>输入并选择 Python: Select Interpreter。</li></ul></li><li><p>第二步: 选择您的 Conda 环境</p><ul><li>在弹出的列表中，找到并点击您刚刚创建的环境（名为 vtimellm 的 Conda 环境）。</li><li>选择后，VS Code 窗口右下角的状态栏会显示 vtimellm，表示切换成功。此后，当您在 VS Code 中打开 Jupyter Notebook (.ipynb) 文件时，它会自动或推荐您使用此环境的内核。</li></ul></li></ul>]]></description></item><item>    <title><![CDATA[Flink Forward Asia 2]]></title>    <link>https://segmentfault.com/a/1190000047411700</link>    <guid>https://segmentfault.com/a/1190000047411700</guid>    <pubDate>2025-11-19 17:11:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411702" alt="" title=""/></p><p>继上海站顺利举办后，Apache Flink 社区年度技术活动——Flink Forward Asia（FFA） 2025 城市巡回第二站，将于12月5日在深圳举行。</p><p>作为亚太地区聚焦实时计算的技术交流活动，FFA 长期关注 Flink 的技术演进与行业应用。本次深圳站将邀请开发者、技术专家和企业用户，围绕实际案例、技术实践与生态共建展开分享与讨论。💥</p><h3>八年深耕，持续引领实时计算未来</h3><p>自 2018 年首届 Flink Forward Asia 举办以来，这一技术活动始终锚定实时计算领域的前沿方向。2025 年，FFA 以新加坡站开启全球视野，继而在上海点燃国内技术热潮，如今走进创新之都——深圳，进一步推动 Flink 技术生态在国内的深度落地与多元协同。</p><p>无论您是 Flink 开发者、数据工程师，还是关注实时智能的企业决策者，这场盛会都将为您带来不可错过的洞察与机遇！</p><h3>🌟 活动亮点</h3><p>🔹 前沿技术分享  <br/>聆听来自行业领袖与社区核心贡献者的深度演讲，掌握 Flink 最新技术演进与架构实践。</p><p>🔹 行业实战解析  <br/>覆盖多个领域真实案例，揭秘 Flink 如何驱动企业实时化转型。</p><p>🔹 开源生态共建  <br/>与百余名开发者面对面交流，参与社区讨论，共绘 Flink 开源生态的未来蓝图。</p><h2>🌟 活动议程</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411703" alt="" title="" loading="lazy"/></p><h3>📺 线上同步直播，精彩不错过！</h3><p>无法亲临现场？我们提供全程直播！  <br/>立即预约直播，远程接入技术前沿，与会上开发者同频共振👇</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411704" alt="" title="" loading="lazy"/></p><h3>✅ 立即报名</h3><p>复制链接或扫描二维码，锁定席位👇  <br/> <a href="https://link.segmentfault.com/?enc=LtwWMSrilwW90oJ1sGogjQ%3D%3D.1oOOV1QHaTKRFEnjU7Sm%2BFATascLsrcPk9JC5YMw5oc%3D" rel="nofollow" target="_blank">https://hdxu.cn/1BpFh</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411705" alt="" title="" loading="lazy"/></p><p><em>具体会议室信息将在报名审核通过后提供</em></p><hr/><p>Flink Forward Asia 2025 · 深圳站，12月5日与您见面。</p>]]></description></item><item>    <title><![CDATA[Apache Doris AI 能力揭秘]]></title>    <link>https://segmentfault.com/a/1190000047411708</link>    <guid>https://segmentfault.com/a/1190000047411708</guid>    <pubDate>2025-11-19 17:10:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在初步探索了 AI 函数的可能性之后，本次我们将目光投向两个更为核心的函数：<code>AI_AGG</code> 和 <code>EMBED</code>。我们将深入解析这两个函数的设计理念、实现原理及其在业务场景中的应用，展示 Apache Doris 如何通过原生的函数设计，将文本聚合与语义向量分析无缝集成到 SQL 中，为用户提供更强大、更易用的智能数据分析体验。</p><p>相关阅读：</p><ul><li><a href="https://link.segmentfault.com/?enc=O1UX5xNvyW33z0hWMmpXYQ%3D%3D.BImxhycUJcwl82wzmwQpZxCd3ImQM9KUTgRk3K2EYl8r9t3reNRmZvncOhgoefmI" rel="nofollow" target="_blank">Apache Doris 4.0 AI 能力揭秘（二）：为企业级应用而生的 AI 函数设计与实践</a></li><li><a href="https://link.segmentfault.com/?enc=ueeGd1h7sY1sBmYU7PO77Q%3D%3D.WKbQppkVO00zqiXJ3QcOm4NlWT%2BNV4QfQvMSK2FCEgmSxJp1SmiBSo1sNU%2FETaJ6" rel="nofollow" target="_blank">Apache Doris 4.0 AI 能力揭秘（一）：AI 函数之 LLM 函数介绍</a></li></ul><h2>AI_AGG：基于 AI 的文本聚合</h2><p>聚合是数据分析中最常见的操作，但如果聚合的对象是海量的用户评论、支持工单或日志文本，传统的聚合函数难以直接处理这类非结构化文本数据。为此，Doris 支持了 <code>AI_AGG</code>，一个能够调用 AI 对文本进行聚合的函数。它让分析师可以根据自定义指令，从大量的文本中处理特定的任务。</p><h3>01 使用方法及示例</h3><blockquote>AI_AGG 详细用法请参考： <a href="https://link.segmentfault.com/?enc=oFkoHDJM%2BNCJFoAqkHRKIA%3D%3D.zEuF27rEhsgcvVXBz%2B1ywLE%2BE16ud7DfN%2Fxpg%2FQj99hLMAL4%2BEMBsMXb%2BxLSrwsDuk7yfIxdTmlmUuH6b%2BNc54uJdQY10s47yLqdld5KfXSuX7S8YuNvhh8Ln%2FCBNCXL" rel="nofollow" target="_blank">https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functi...</a></blockquote><p><strong>示例 1：</strong> </p><p>下表模拟一个简易的客服工单：</p><pre><code class="SQL">SELECT * FROM support_tickets;
+-----------+---------------+------------------+----------------------------------------------------------------------------------+
| ticket_id | customer_name | subject          | details                                                                          |
+-----------+---------------+------------------+----------------------------------------------------------------------------------+
|         2 | Bob           | Login Failure    | Same problem as Alice. Also seeing 502 errors on the SSO page.                   |
|         3 | Carol         | Payment Declined | Credit card charged twice but order still shows pending.                         |
|         5 | Eve           | Login Failure    | Getting redirected back to login after entering 2FA code.                        |
|         1 | Alice         | Login Failure    | Cannot log in after password reset. Tried clearing cache and different browsers. |
|         4 | Dave          | Slow Dashboard   | Dashboard takes &gt;30 seconds to load since the last release.                      |
+-----------+---------------+------------------+----------------------------------------------------------------------------------+</code></pre><p><strong>我们可以通过 <code>AI_AGG</code> 对不同问题类型下客户的问题进行总结</strong></p><pre><code class="SQL">SELECT
    subject,
    AI_AGG(
        details,
        'Summarize every ticket detail into one short paragraph'
    ) AS ai_summary
FROM support_tickets
GROUP BY subject;</code></pre><p>输出示例如下：</p><pre><code class="Plain">+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| subject          | ai_summary                                                                                                                                                              |
+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Slow Dashboard   | The dashboard is experiencing slow loading times, taking over 30 seconds to load following the most recent release.                                                     |
| Payment Declined | A customer reports being charged twice for their order, which remains in a pending status.                                                                              |
| Login Failure    | Users are experiencing login issues, including 2FA redirection, post-password reset failures, and SSO 502 errors, despite clearing cache and trying different browsers. |
+------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre><p><strong>示例 2：</strong> </p><p>下表简易模拟了应用反馈表：</p><pre><code class="SQL">SELECT * FROM app_feedback;
+-------------+----------------+-----------+--------------------------------------------------------------------------------------+
| feedback_id | feature_module | user_name | feedback_text                                                                        |
+-------------+----------------+-----------+--------------------------------------------------------------------------------------+
|           1 | 机票预订       | 李雷      | 搜索航班非常快，价格也透明，筛选功能很好用。                                         |
|           2 | 机票预订       | 韩梅梅    | 支付后出票速度有点慢，等了快半小时，希望能改进。                                     |
|           3 | 机票预订       | 小陈      | 希望能增加更多廉价航空公司的选项。                                                   |
|           4 | 机票预订       | 马丽      | App在选择日期的时候偶尔会卡顿，体验不是很好。                                        |
|           5 | 酒店预订       | 王先生    | 酒店信息很全，图片也真实。但是取消预订的流程太复杂了。                               |
|           6 | 酒店预订       | 刘女士    | 通过App预订比其他平台便宜，还有会员折扣，非常满意。                                  |
|           7 | 行程规划       | 赵四      | 行程规划功能太棒了，可以自动推荐路线和景点，省了不少心。                             |
|           8 | 客户服务       | 孙小小    | 联系客服很方便，问题解决得也很快，给客服点赞。                                       |
|           9 | 客户服务       | 钱多多    | 电话客服总是占线，在线客服回复又很慢，希望能增加人手。                               |
+-------------+----------------+-----------+--------------------------------------------------------------------------------------+</code></pre><p><strong>我们可以通过 <code>AI_AGG</code> 对用户反馈进行总结：</strong></p><pre><code class="SQL">SELECT
    feature_module AS '功能模块',
    AI_AGG(
        feedback_text,
        '请用精确且尽可能简短地总结这些用户反馈中提到的主要问题和建议'
    ) AS 'AI反馈总结'
FROM app_feedback
GROUP BY feature_module;</code></pre><p>输出示例如下</p><pre><code class="Plain">+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 功能模块     | AI反馈总结                                                                                                                                                                                                     |
+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 客户服务     | 主要问题：客服电话占线，在线客服回复慢。  
主要建议：增加客服人手。                                                                                                                                                |
| 行程规划     | 主要问题：无  
主要建议：赞赏行程规划功能，建议继续优化                                                                                                                                                        |
| 酒店预订     | 主要问题：取消预订流程复杂。
主要建议：无明确建议，但肯定App预订价格优势和会员折扣。                                                                                                                          |
| 机票预订     | 主要问题：支付后出票慢（约半小时），App日期选择卡顿，廉价航空公司选项不足。
主要建议：优化出票速度，修复App卡顿问题，增加廉价航空公司选项。                                                                   |
+--------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre><h3>02 AI_AGG 的实现原理</h3><p>将聚合函数与 AI 结合，需要解决一个分组内文本总量可能远超模型上下文窗口的问题。如果将所有文本拼接后一次性发送给 AI， 极易出现拼接文本大于模型的最大上下文窗口的情况，<strong>而 Doris 中通过动态预聚合解决了这个问题：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047411710" alt="02 AI_AGG 的实现原理.png" title="02 AI_AGG 的实现原理.png"/></p><ol><li><strong>上下文监控</strong>：在聚合拼接文本的过程中，<code>AI_AGG</code> 会为每一个分组维护一个内部的文本缓冲区（目前大小固定为 128K，为绝大部分 AI 可兼容的上下文窗口）</li><li><strong>动态预聚合</strong>：当新的文本行将要被加入缓冲区并会导致其大小超过阈值时，<code>AI_AGG</code> 会触发一次预聚合。它会暂停接收新数据，将当前缓冲区内的所有文本作为一个批次发送给 AI 进行一次中间任务处理。</li><li><strong>上下文替换</strong>：AI 返回的中间处理结果，其长度通常远小于原始文本。<code>AI_AGG</code> 会用这个精炼后的摘要替换掉缓冲区内原有的长文本，从而为处理更多的新文本行腾出空间。为保证聚合过程的稳定性和防止超出模型服务上限，在替换原有文本后，若加入当前文本行还是会导致缓冲区超过阈值，<code>AI_AGG</code> 会直接报错退出。</li></ol><p><strong>通过这种实现方式，<code>AI_AGG</code> 可以完全融入 Doris 的分布式查询计划，利用多节点并行计算，并由数据库自动管理聚合过程中的中间状态。因此，用户可以用熟悉的 SQL 聚合语法，在海量文本上实现高效的智能分析。</strong></p><h2>EMBED: 文本向量化函数</h2><p><code>EMBED</code> 函数的核心功能是通过 AI 将任意文本转换为高维度的浮点数向量。这个向量是文本在语义空间中的一种数学表示，它捕捉了文本的语义信息。语义相近的文本，其向量在空间中的距离也更近。</p><blockquote>EMBED 详细用法请参考： <a href="https://link.segmentfault.com/?enc=Y4urXifd%2F4B3GE8wfuYbLw%3D%3D.OGLOar6nsixstdi1D34su0szjRLIlC75SuUWU0sijAtTb%2B2YJpIcRqRLpoulm1IsZIV2Ud7p4sWdW6muPaUojtiR107jvD%2BFW5m9gWrd%2FkM8VhkoaApoTIAx7t6zQtwysfy16%2B7E0JVYEGiNVj0tzg%3D%3D" rel="nofollow" target="_blank">https://doris.apache.org/zh-CN/docs/dev/sql-manual/sql-functi...</a></blockquote><h3>01 使用方法及示例</h3><p><strong>示例 1：</strong> </p><p>下表模拟简易的行为手册</p><pre><code class="SQL">CREATE TABLE knowledge_base (
    id BIGINT,
    title STRING,
    content STRING,
    embedding ARRAY&lt;FLOAT&gt; COMMENT 'Semantic vector generated by EMBED function'
)
DUPLICATE KEY(id)
DISTRIBUTED BY HASH(id) BUCKETS 4
PROPERTIES (
    "replication_num" = "1"
);

INSERT INTO knowledge_base (id, title, content, embedding) VALUES
(1, "Travel Reimbursement Policy",
    "Employees must submit a reimbursement request within 7 days after the business trip, with invoices and travel approval attached.",
    EMBED("travel reimbursement policy")),
(2, "Leave Policy",
    "Employees must apply for leave in the system in advance. If the leave is longer than three days, approval from the direct manager is required.",
    EMBED("leave request policy")),
(3, "VPN User Guide",
    "To access the internal network, employees must use VPN. For the first login, download and install the client and configure the certificate.",
    EMBED("VPN guide intranet access")),
(4, "Meeting Room Reservation",
    "Meeting rooms can be reserved in advance through the OA system, with time and number of participants specified.",
    EMBED("meeting room booking reservation")),
(5, "Procurement Request Process",
    "Departments must fill out a procurement request form for purchasing items. If the amount exceeds $5000, financial approval is required.",
    EMBED("procurement request process finance"));</code></pre><p><strong>通过 <code>EMBED</code> 函数对文本的向量化操作，结合 Doris 支持的<a href="https://link.segmentfault.com/?enc=S4nzmo1UBaHvuOGxTY2jqQ%3D%3D.RIWivt244malghSRAJyahMgtHqphbvSD5HENfGwUjHla7f4kJG3Y7A8znlVhwDZT0uk2lHdL%2Fyc%2FOSGbxjDOYopJlsPhqNcWnLPJaN8bBbz9ah7km8bpZC5c4aHxum7V7AejIQmZiaKwIT2mpsI3Z9nNhembLqU9dy0uE2G3mig%3D" rel="nofollow" target="_blank">向量函数</a>， 可对数据进行如下操作：</strong></p><ol><li>问答检索（结合 <code>COSINE_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    COSINE_DISTANCE(embedding, EMBED("How to apply for travel reimbursement?")) AS score
FROM knowledge_base
ORDER BY score ASC
LIMIT 2;
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+--------------------+
| id   | title                       | content                                                                                                                                 | score              |
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+--------------------+
|    1 | Travel Reimbursement Policy | Employees must submit a reimbursement request within 7 days after the business trip, with invoices and travel approval attached.        | 0.4463210454563673 |
|    5 | Procurement Request Process | Departments must fill out a procurement request form for purchasing items. If the amount exceeds $5000, financial approval is required. | 0.5726841578491431 |
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+--------------------+</code></pre><ol><li>问题分析匹配（结合 <code>L2_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    L2_DISTANCE(embedding, EMBED("How to access the company intranet")) AS distance
FROM knowledge_base
ORDER BY distance ASC
LIMIT 2;
+------+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------+
| id   | title                       | content                                                                                                                                     | distance           |
+------+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------+
|    3 | VPN User Guide              | To access the internal network, employees must use VPN. For the first login, download and install the client and configure the certificate. | 0.5838271122253775 |
|    1 | Travel Reimbursement Policy | Employees must submit a reimbursement request within 7 days after the business trip, with invoices and travel approval attached.            |  1.272394695975331 |
+------+-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------+--------------------+</code></pre><ol><li>根据文章内容进行文本相关度匹配并推荐（结合<code>INNER PRODUCT</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    INNER_PRODUCT(embedding, EMBED("Leave system request leader approval")) AS score
FROM knowledge_base
WHERE id != 2
ORDER BY score DESC
LIMIT 2;
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------------+
| id   | title                       | content                                                                                                                                 | score               |
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------------+
|    5 | Procurement Request Process | Departments must fill out a procurement request form for purchasing items. If the amount exceeds $5000, financial approval is required. |    0.33268885332504 |
|    4 | Meeting Room Reservation    | Meeting rooms can be reserved in advance through the OA system, with time and number of participants specified.                         | 0.29224032230852487 |
+------+-----------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------------+</code></pre><ol><li>寻找差异较小的内容（结合<code>L1_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    L1_DISTANCE(embedding, EMBED("Procurement application process")) AS distance
FROM knowledge_base
ORDER BY distance ASC
LIMIT 3;
+------+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+
| id   | title                       | content                                                                                                                                        | distance           |
+------+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+
|    5 | Procurement Request Process | Departments must fill out a procurement request form for purchasing items. If the amount exceeds $5000, financial approval is required.        |  18.66882028897362 |
|    4 | Meeting Room Reservation    | Meeting rooms can be reserved in advance through the OA system, with time and number of participants specified.                                |  30.90449328294426 |
|    2 | Leave Policy                | Employees must apply for leave in the system in advance. If the leave is longer than three days, approval from the direct manager is required. | 31.060405636536416 |
+------+-----------------------------+------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+</code></pre><p><strong><code>EMBED</code> 的设计不仅是一个文本向量化工具，更是 Doris 向量分析生态的重要组成部分。</strong> 通过与其他向量函数（<code>COSINE_DISTANCE</code>， <code>L2_DISTANCE</code>， <code>INNER_PRODUCT</code>， <code>L1_DISTANCE</code> ）的无缝集成，<code>EMBED</code> 支持用户在 SQL 查询中完成从文本到向量，再到相似度计算或检索的一站式分析。</p><p><strong>示例 2：</strong> </p><p>下表模拟简易的客服常见问题文档：</p><pre><code class="SQL">CREATE TABLE support_docs (
    id BIGINT,
    title STRING,
    content STRING,
    embedding ARRAY&lt;FLOAT&gt; COMMENT '由 EMBED 函数生成的语义向量'
) DUPLICATE KEY(id)
DISTRIBUTED BY HASH(id) BUCKETS 1
PROPERTIES ( "replication_num" = "1" );

INSERT INTO support_docs (id, title, content, embedding) VALUES
(1, "系统登录问题排查",
    "当用户无法登录时，请先检查网络连接、浏览器缓存，并确认账号未被锁定。",
    EMBED("登录问题排查 网络异常 账号锁定")),
(2, "数据备份与恢复指南",
    "客户可在管理后台手动备份数据，若误删可通过支持团队申请数据恢复。",
    EMBED("数据备份 恢复 操作指南")),
(3, "账单与发票问题",
    "企业客户可在财务模块下载电子发票。如需纸质版，请提交申请工单。",
    EMBED("账单 发票 财务模块")),
(4, "API 接口调用规范",
    "开发者在调用 API 时需携带正确的访问令牌，否则将返回身份验证错误。",
    EMBED("接口调用 认证错误 访问令牌")),
(5, "服务中断应急流程",
    "若出现服务中断，技术团队需在30分钟内启动应急响应并发布公告。",
    EMBED("服务中断 应急响应 处理流程"));</code></pre><p><strong>通过 <code>EMBED</code> 函数对文本的向量化操作，结合 Doris 支持的<a href="https://link.segmentfault.com/?enc=qlMGShQqdGVFWxEGgh2Ymw%3D%3D.2JsfDaZJIMcpLOtPLVaQ8nTScKJqUpvsgUVJP97Kh1bElPuuOblqsDvzie9Mmecf1LuSD8wj7fnVxs6X3D6he8eRpj%2BTvl31gZsAiEKsTFGtks6U2dqN0T%2BdAvXQ8c75Of%2FxK%2Fkqdn3S%2BbwBgADkNoH3OIM1cVRBgp8DJHuwgQc%3D" rel="nofollow" target="_blank">向量函数</a>， 可对数据进行如下操作：</strong></p><ol><li>问答检索（结合 <code>COSINE_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    COSINE_DISTANCE(embedding, EMBED("登录不上系统怎么办？")) AS score
FROM support_docs
ORDER BY score ASC
LIMIT 2;
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+
| id   | title                    | content                                                                                                | score     |
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+
|    1 | 系统登录问题排查         | 当用户无法登录时，请先检查网络连接、浏览器缓存，并确认账号未被锁定。                                   | 0.3183002 |
|    4 | API 接口调用规范         | 开发者在调用 API 时需携带正确的访问令牌，否则将返回身份验证错误。                                      | 0.5599254 |
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+</code></pre><ol><li>问题分析匹配（结合 <code>L2_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    L2_DISTANCE(embedding, EMBED("接口调用时提示没有权限")) AS distance
FROM support_docs
ORDER BY distance ASC
LIMIT 2;
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+
| id   | title                    | content                                                                                                | distance  |
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+
|    4 | API 接口调用规范         | 开发者在调用 API 时需携带正确的访问令牌，否则将返回身份验证错误。                                      | 0.6471552 |
|    1 | 系统登录问题排查         | 当用户无法登录时，请先检查网络连接、浏览器缓存，并确认账号未被锁定。                                   | 0.9831962 |
+------+--------------------------+--------------------------------------------------------------------------------------------------------+-----------+</code></pre><ol><li>根据文章内容进行文本相关度匹配并推荐（结合<code>INNER PRODUCT</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    INNER_PRODUCT(embedding, EMBED("账单 发票 报销流程")) AS score
FROM support_docs
ORDER BY score DESC
LIMIT 2;
+------+--------------------------+-----------------------------------------------------------------------------------------------+-----------+
| id   | title                    | content                                                                                       | score     |
+------+--------------------------+-----------------------------------------------------------------------------------------------+-----------+
|    3 | 账单与发票问题           | 企业客户可在财务模块下载电子发票。如需纸质版，请提交申请工单。                                | 0.8098868 |
|    5 | 服务中断应急流程         | 若出现服务中断，技术团队需在30分钟内启动应急响应并发布公告。                                  | 0.3729638 |
+------+--------------------------+-----------------------------------------------------------------------------------------------+-----------+</code></pre><ol><li>寻找差异较小的内容（结合<code>L1_DISTANCE</code>）</li></ol><pre><code class="SQL">SELECT 
    id, title, content,
    L1_DISTANCE(embedding, EMBED("服务中断 处理 指南")) AS distance
FROM support_docs
ORDER BY distance ASC
LIMIT 3;
+------+-----------------------------+--------------------------------------------------------------------------------------------------------+----------+
| id   | title                       | content                                                                                                | distance |
+------+-----------------------------+--------------------------------------------------------------------------------------------------------+----------+
|    5 | 服务中断应急流程            | 若出现服务中断，技术团队需在30分钟内启动应急响应并发布公告。                                           | 13.94832 |
|    2 | 数据备份与恢复指南          | 客户可在管理后台手动备份数据，若误删可通过支持团队申请数据恢复。                                       | 24.65827 |
|    1 | 系统登录问题排查            | 当用户无法登录时，请先检查网络连接、浏览器缓存，并确认账号未被锁定。                                   |  24.9747 |
+------+-----------------------------+--------------------------------------------------------------------------------------------------------+----------+</code></pre><h3>02 灵活的向量维度控制</h3><p>通过 Doris 内置的 <code>RESOURCE</code> 机制，用户在配置 AI  Resource 时，可以设置 <code>ai.dimensions</code> 参数来精确指定生成向量的维度。用户可以根据具体的业务场景和性能考量，选择生成高维向量以保留更丰富的语义信息，或选择低维向量以节约存储空间和加速计算。这种灵活性使得 <code>EMBED</code> 函数能够更好地适应从轻量级语义匹配到高精度向量检索等多样化的分析需求，让用户在成本与效果之间找到最佳平衡。</p><blockquote><strong>注意</strong>：在使用 <code>dimensions</code> 参数时，请务必确认 <code>RESOURCE</code> 中配置的模型支持您所指定的维度，否则可能导致请求错误。此外，Doris 内部对部分不支持维度定制的模型（例如 OpenAI 的 <code>text-embedding-ada-002</code>）做了限制。对于这些模型，即使在 <code>RESOURCE</code> 中设置了 <code>dimensions</code> 参数，该设置也将被忽略，函数将返回模型默认的维度。</blockquote><h2>总结与展望</h2><p>借助<code>AI_AGG</code> 与 <code>EMBED</code> 函数，Apache Doris 拥有了了强大的智能分析能力，极大地拓展了数据分析与智能应用的边界。<code>AI_AGG</code> 通过其动态预聚合机制，将非结构化文本的智能聚合分析带入数据库，轻松应对海量用户评论、日志分析等场景。而 <code>EMBED</code> 函数则与 Doris 的向量函数无缝集成，提供从文本到语义向量、再到相似度检索的一站式解决方案，极大简化了问答系统、内容推荐等应用的开发。这些功能使得 SQL 语言本身具备了驾驭 AI 模型的能力，让每一位数据分析师都能以低成本、高效率的方式，挖掘数据中更深层次的语义价值。</p><p>展望未来，Doris 将继续深化 AI 与数据库的融合。我们将致力于优化模型调度与计算性能，并探索更多如多模态数据分析、AI Agent 交互等前沿功能，持续降低 AI 技术的使用门槛，让数据驱动的智能决策无处不在。</p>]]></description></item><item>    <title><![CDATA[能耗异常智能诊断怎么帮助企业减少能源浪费]]></title>    <link>https://segmentfault.com/a/1190000047411761</link>    <guid>https://segmentfault.com/a/1190000047411761</guid>    <pubDate>2025-11-19 17:10:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在现代工业体系中，能耗管理不再是简单的成本计算，而是一场依赖技术深度参与的变革。传统模式中，能源异常往往依赖人工经验推测或事后统计追溯，无论是铝业电解槽的涌浪参数，还是工厂数百台设备的待机能耗，都因其断点式观察与高维复杂性而难以把握。广域铭岛通过构建Geega工业互联网平台，将"能耗异常智能诊断"从实验室推进至现实生产场景，以机器学习算法为经纬，重塑了能源管理的技术路径。<br/>系统层面的变革始于对异常模式的动态感知。 智能诊断技术通过立体化数据采集，融合电流波动、振动频谱、环境温度等多维变量，在工厂服务器部署高频采样终端后，已能以亚毫秒级响应识别设备亚健康运行（例如电解槽参数偏离历史最优轨道）。广西铝业的案例中，人工巡检频率从每月数十次降至每月一次，同时将机器视觉数据与红外热成像整合成立体监测模型，使得异常危害从"经验判断"转为"数据预防"。<br/>更棘手的不仅是设备"生病"，而是能效异常的滞后影响。某大型制造厂在领克成都工厂的能耗系统建设中发现，异常待机能耗若未被预警，会导致焊接工艺质量损失上升13%，更延长交付周期15%。美光成都工厂的PDU智能监控显示，在订单高峰期未出现能耗异常报警之时，系统依然通过"预见性调节"，提前16小时压制了部分设备能耗峰值，使得总计提前检测并优化了56个异常运行场景。<br/>系统的核心逻辑在于构建外显与未显之间的桥梁。 工业异常诊断最独特的价值，显现在"无法量化的隐性浪费"上——比如焦化配煤环节，0.1%的配比失准会导致碳排放增加1.28%且额外能耗上升2.4%。Geega EMS的能耗分析模块通过可视化与动态优化算法，将此类微小偏差放大为可操作的预警因子，推动了传统高耗能行业向精益管理的转型。<br/>计算： 从数据碎片到决策闭环的运作节奏。以某化工企业为例，智能诊断系统在实时预测能源需求时，异常波动被量化为精确的节能机会。当反应釜能耗超出正常范围时，系统不仅仅提示参数异常，而是自动推荐降低循环水温度，从而减少冷却负荷12%，全部响应在设备效能衰减至最佳阈值前完成。<br/>交叉场景验证表明智能诊断具备强横向适用性。 例如百矿集团在电解铝行业通过实时关联设备启停与电流负荷状态，每天提前诊断出超过10根输送管结垢风险，日均节能量达到数十万千瓦时。数字化配煤方案不仅优化了燃煤消耗，更通过AI自主学习，将煤耗指标降压至世界可比先进行列。这些已是广域铭岛承诺的一部分：以数据喂养决策，用算法包裹实操，将"预防性维护"变成能效管理的关键词。</p>]]></description></item>  </channel></rss>