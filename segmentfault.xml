<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[MyBatis 进阶治理点——缓存、副作]]></title>    <link>https://segmentfault.com/a/1190000047445951</link>    <guid>https://segmentfault.com/a/1190000047445951</guid>    <pubDate>2025-12-03 15:08:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>深入 MyBatis 内核，在性能提升与数据一致性之间寻找精妙平衡</blockquote><p>在掌握 MyBatis 基础映射与动态 SQL 后，进阶治理成为保证生产环境稳定性与性能的关键。本文将深入分析缓存机制、副作用控制、拦截器应用与批处理优化等高级主题，帮助开发者构建高可用、易维护的数据访问层。</p><h2>1 缓存机制深度治理</h2><h3>1.1 二级缓存的一致性挑战</h3><p>MyBatis 的二级缓存基于 Mapper 命名空间设计，多个 SqlSession 可共享同一缓存区域，这一机制在提升性能的同时也带来了严重的一致性挑战。</p><p><strong>跨命名空间更新导致的数据不一致</strong>是典型问题。当 OrderMapper 缓存了包含用户信息的订单数据，而 UserMapper 更新了用户信息时，OrderMapper 的缓存不会自动失效，导致脏读。解决方案是通过引用关联让相关 Mapper 共享缓存刷新机制：</p><pre><code>&lt;!-- OrderMapper.xml --&gt;
&lt;cache/&gt;
&lt;!-- 引用UserMapper的缓存 --&gt;
&lt;cache-ref namespace="com.example.mapper.UserMapper"/&gt;</code></pre><p><strong>分布式环境下的缓存同步</strong>是另一重要问题。默认的基于内存的二级缓存在集群环境下会导致各节点数据不一致。集成 Redis 等分布式缓存是可行方案：</p><pre><code>&lt;!-- 配置Redis作为二级缓存 --&gt;
&lt;cache type="org.mybatis.caches.redis.RedisCache"
       eviction="LRU"
       flushInterval="300000"
       size="1024"/&gt;</code></pre><h3>1.2 细粒度缓存控制策略</h3><p>合理的缓存控制需要在不同粒度上制定策略。<strong>语句级缓存控制</strong>允许针对特定查询调整缓存行为：</p><pre><code>&lt;select id="selectUser" parameterType="int" resultType="User" 
        useCache="true" flushCache="false"&gt;
    SELECT * FROM users WHERE id = #{id}
&lt;/select&gt;

&lt;insert id="insertUser" parameterType="User" flushCache="true"&gt;
    INSERT INTO users(name, email) VALUES(#{name}, #{email})
&lt;/insert&gt;</code></pre><p><strong>缓存回收策略配置</strong>对长期运行的系统至关重要。LRU（最近最少使用）策略适合查询分布均匀的场景，而 FIFO（先进先出）更适合时间敏感型数据：</p><pre><code>&lt;cache eviction="FIFO" flushInterval="60000" size="512" readOnly="true"/&gt;</code></pre><h2>2 副作用识别与控制策略</h2><h3>2.1 一级缓存的副作用与治理</h3><p>MyBatis 的一级缓存虽然提升了会话内查询性能，但也引入了诸多副作用。<strong>长时间会话中的脏读</strong>发生在 SqlSession 生命周期内，其他事务已提交的更改对当前会话不可见。</p><p>治理方案包括​<strong>使用 STATEMENT 级别缓存</strong>​，使每次查询后清空缓存：</p><pre><code># application.yml
mybatis:
  configuration:
    local-cache-scope: statement</code></pre><p><strong>批量处理中的错误累积</strong>是另一常见问题。在循环中重复查询相同数据时，一级缓存可能返回过期数据。通过 <code>flushCache</code> 选项强制刷新可以解决：</p><pre><code>@Options(flushCache = Options.FlushCachePolicy.TRUE)
@Select("SELECT id FROM orders WHERE status = 'pending' LIMIT 1")
Integer findNextPendingOrder();</code></pre><h3>2.2 二级缓存的副作用防控</h3><p>二级缓存的作用范围更广，其副作用影响也更严重。<strong>多表关联查询的缓存失效</strong>问题需要通过精细的缓存引用管理来解决。</p><p><strong>缓存击穿与雪崩防护</strong>对高并发系统至关重要。针对缓存击穿，实现互斥锁控制：</p><pre><code>public class CacheMutexLock {
    private static final ConcurrentHashMap&lt;String, Lock&gt; LOCKS = new ConcurrentHashMap&lt;&gt;();
    
    public static &lt;T&gt; T executeWithLock(String key, Supplier&lt;T&gt; supplier) {
        Lock lock = LOCKS.computeIfAbsent(key, k -&gt; new ReentrantLock());
        lock.lock();
        try {
            return supplier.get();
        } finally {
            lock.unlock();
            LOCKS.remove(key);
        }
    }
}</code></pre><p>针对缓存雪崩，采用合理的过期时间分散策略：</p><pre><code>&lt;cache eviction="LRU" flushInterval="300000" size="1024" 
       randomExpiration="true" baseExpiration="300000"/&gt;</code></pre><h2>3 拦截器高级应用与风险控制</h2><h3>3.1 拦截器在数据安全中的应用</h3><p>MyBatis 拦截器提供了在 SQL 执行各阶段插入自定义逻辑的能力。<strong>敏感数据自动加解密</strong>通过 ParameterHandler 和 ResultHandler 拦截器实现：</p><pre><code>@Intercepts({
    @Signature(type = ParameterHandler.class, method = "setParameters", 
               args = {PreparedStatement.class}),
    @Signature(type = ResultHandler.class, method = "handleResultSets", 
               args = {Statement.class})
})
@Component
public class DataSecurityInterceptor implements Interceptor {
    
    private final EncryptionService encryptionService;
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        if (invocation.getTarget() instanceof ParameterHandler) {
            // 参数加密逻辑
            return encryptParameters(invocation);
        } else {
            // 结果集解密逻辑
            return decryptResultSets(invocation);
        }
    }
}</code></pre><p><strong>数据权限过滤</strong>通过 StatementHandler 拦截器自动添加权限条件：</p><pre><code>@Intercepts({
    @Signature(type = StatementHandler.class, method = "prepare", 
               args = {Connection.class, Integer.class})
})
public class DataAuthInterceptor implements Interceptor {
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        StatementHandler handler = (StatementHandler) invocation.getTarget();
        String originalSql = getOriginalSql(handler);
        
        if (needDataAuth(originalSql)) {
            String authCondition = buildAuthCondition();
            String newSql = appendCondition(originalSql, authCondition);
            setSql(handler, newSql);
        }
        
        return invocation.proceed();
    }
}</code></pre><h3>3.2 拦截器的性能影响与稳定性风险</h3><p>拦截器虽然强大，但不当使用会带来严重性能问题和稳定性风险。<strong>拦截器链过长</strong>会导致执行效率显著下降。监控拦截器执行时间至关重要：</p><pre><code>@Override
public Object intercept(Invocation invocation) throws Throwable {
    long startTime = System.currentTimeMillis();
    try {
        return invocation.proceed();
    } finally {
        long duration = System.currentTimeMillis() - startTime;
        if (duration &gt; SLOW_QUERY_THRESHOLD) {
            log.warn("Interceptor slow query: {}ms, method: {}", 
                     duration, invocation.getMethod().getName());
        }
    }
}</code></pre><p><strong>递归调用陷阱</strong>发生在拦截器修改的参数再次触发同一拦截器时。通过状态标记防止递归：</p><pre><code>private static final ThreadLocal&lt;Boolean&gt; PROCESSING = ThreadLocal.withInitial(() -&gt; false);

@Override
public Object intercept(Invocation invocation) throws Throwable {
    if (PROCESSING.get()) {
        return invocation.proceed(); // 避免递归
    }
    
    PROCESSING.set(true);
    try {
        // 拦截器逻辑
        return processInvocation(invocation);
    } finally {
        PROCESSING.set(false);
    }
}</code></pre><h2>4 批处理性能优化</h2><h3>4.1 批量操作的内存优化</h3><p>大批量数据操作时，内存管理和事务控制是关键优化点。<strong>分批处理</strong>避免内存溢出：</p><pre><code>public void batchInsertUsers(List&lt;User&gt; users) {
    SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH);
    try {
        UserMapper mapper = sqlSession.getMapper(UserMapper.class);
        int batchSize = 1000;
        int count = 0;
        
        for (User user : users) {
            mapper.insertUser(user);
            count++;
            
            if (count % batchSize == 0) {
                sqlSession.commit();
                sqlSession.clearCache(); // 避免缓存堆积
            }
        }
        sqlSession.commit();
    } finally {
        sqlSession.close();
    }
}</code></pre><p><strong>流式查询</strong>优化大数据量读取内存占用：</p><pre><code>@Select("SELECT * FROM large_table WHERE condition = #{condition}")
@Options(resultSetType = ResultSetType.FORWARD_ONLY, fetchSize = 1000)
@ResultType(User.class)
void streamLargeData(@Param("condition") String condition, ResultHandler&lt;User&gt; handler);</code></pre><h3>4.2 批量操作的异常处理与重试</h3><p>批量操作中的异常需要特殊处理以保证数据一致性。<strong>部分失败补偿机制</strong>确保数据完整性：</p><pre><code>public class BatchOperationManager {
    
    public void safeBatchInsert(List&lt;Data&gt; dataList) {
        int retryCount = 0;
        while (retryCount &lt; MAX_RETRY) {
            try {
                doBatchInsert(dataList);
                break; // 成功则退出重试
            } catch (BatchException e) {
                retryCount++;
                if (retryCount &gt;= MAX_RETRY) {
                    log.error("Batch insert failed after {} retries", MAX_RETRY);
                    throw e;
                }
                handlePartialFailure(e, dataList);
            }
        }
    }
    
    private void handlePartialFailure(BatchException e, List&lt;Data&gt; dataList) {
        // 识别失败记录并重试
        List&lt;Data&gt; failedRecords = identifyFailedRecords(e, dataList);
        if (!failedRecords.isEmpty()) {
            doBatchInsert(failedRecords);
        }
    }
}</code></pre><h2>5 监控与诊断体系建立</h2><h3>5.1 性能指标采集与分析</h3><p>建立完善的监控体系是识别和解决性能问题的前提。<strong>关键性能指标</strong>应包括：</p><ul><li>​<strong>缓存命中率</strong>​：一级缓存和二级缓存的命中比例</li><li>​<strong>SQL 执行时间</strong>​：区分缓存命中与数据库查询的时间</li><li>​<strong>批处理吞吐量</strong>​：单位时间内处理的记录数</li><li>​<strong>连接等待时间</strong>​：获取数据库连接的平均等待时间</li></ul><pre><code>@Component
public class MyBatisMetricsCollector {
    
    private final MeterRegistry meterRegistry;
    
    public void recordQueryExecution(String statement, long duration, boolean fromCache) {
        meterRegistry.timer("mybatis.query.execution")
                    .tags("statement", statement, "cached", String.valueOf(fromCache))
                    .record(duration, TimeUnit.MILLISECONDS);
    }
    
    public void recordCacheHit(String cacheLevel, boolean hit) {
        meterRegistry.counter("mybatis.cache.access")
                    .tags("level", cacheLevel, "hit", String.valueOf(hit))
                    .increment();
    }
}</code></pre><h3>5.2 日志与诊断信息增强</h3><p>详细的日志记录是诊断复杂问题的基础。<strong>结构化日志</strong>提供可分析的诊断信息：</p><pre><code>&lt;!-- logback-spring.xml --&gt;
&lt;logger name="com.example.mapper" level="DEBUG" additivity="false"&gt;
    &lt;appender-ref ref="MYBATIS_JSON_APPENDER"/&gt;
&lt;/logger&gt;

&lt;appender name="MYBATIS_JSON_APPENDER" class="ch.qos.logback.core.ConsoleAppender"&gt;
    &lt;encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder"&gt;
        &lt;providers&gt;
            &lt;timestamp/&gt;
            &lt;logLevel/&gt;
            &lt;loggerName/&gt;
            &lt;message/&gt;
            &lt;mdc/&gt;
        &lt;/providers&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;</code></pre><p><strong>慢查询监控</strong>帮助识别性能瓶颈：</p><pre><code>@Intercepts(@Signature(type = Executor.class, method = "query", 
           args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class}))
public class SlowQueryInterceptor implements Interceptor {
    
    private static final long SLOW_QUERY_THRESHOLD = 1000; // 1秒
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        long start = System.currentTimeMillis();
        try {
            return invocation.proceed();
        } finally {
            long duration = System.currentTimeMillis() - start;
            if (duration &gt; SLOW_QUERY_THRESHOLD) {
                Object[] args = invocation.getArgs();
                MappedStatement ms = (MappedStatement) args[0];
                log.warn("Slow query detected: {}ms, statement: {}", 
                         duration, ms.getId());
            }
        }
    }
}</code></pre><h2>6 综合治理策略与最佳实践</h2><h3>6.1 环境特定的配置策略</h3><p>不同环境需要不同的治理策略。<strong>开发环境</strong>应注重可调试性，开启完整 SQL 日志；<strong>测试环境</strong>需要模拟生产环境配置，验证性能；<strong>生产环境</strong>则以稳定性和性能为优先。</p><p>​<strong>多环境配置示例</strong>​：</p><pre><code># application-dev.yml
mybatis:
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
    cache-enabled: false

# application-prod.yml  
mybatis:
  configuration:
    log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl
    cache-enabled: true
    local-cache-scope: statement</code></pre><h3>6.2 治理决策框架</h3><p>建立系统的治理决策流程，确保架构决策的可追溯性。<strong>决策记录表</strong>帮助团队统一治理标准：</p><table><thead><tr><th><strong>治理领域</strong>​</th><th><strong>决策选项</strong>​</th><th><strong>适用场景</strong>​</th><th><strong>风险提示</strong>​</th></tr></thead><tbody><tr><td><strong>缓存策略</strong>​</td><td>本地缓存</td><td>单实例部署，数据量小</td><td>集群环境不一致</td></tr><tr><td> </td><td>分布式缓存</td><td>集群部署，数据一致性要求高</td><td>网络开销增加</td></tr><tr><td><strong>批处理提交</strong>​</td><td>自动提交</td><td>内存敏感场景</td><td>部分失败难恢复</td></tr><tr><td> </td><td>手动提交</td><td>数据一致性优先</td><td>内存占用较高</td></tr></tbody></table><h2>总结</h2><p>MyBatis 进阶治理需要在性能、一致性和可维护性之间寻找精细平衡。缓存机制能显著提升性能，但必须建立完善的失效策略防止脏读；拦截器提供强大扩展能力，但需防范性能损耗和递归陷阱；批处理优化吞吐量，但要关注内存使用和错误恢复。</p><p>有效的治理不是一次性任务，而是需要持续监控、评估和调整的过程。建立完善的指标采集、日志记录和告警机制，才能确保数据访问层长期稳定运行。</p><hr/><p><strong>📚 下篇预告</strong>​</p><p>《JPA/Hibernate 选择指南——实体关系维护、懒加载与 N+1 问题的权衡》—— 我们将深入探讨：</p><ul><li>⚖️ ​<strong>ORM 框架选型</strong>​：JPA 与 Hibernate 的适用场景对比分析</li><li>🔗 ​<strong>实体关系映射</strong>​：一对一、一对多、多对多关系的维护策略</li><li>⚡ ​<strong>懒加载优化</strong>​：关联加载时机的性能影响与配置方案</li><li>🚀 ​<strong>N+1 问题解决</strong>​：识别、预防与优化查询性能瓶颈</li><li>📊 ​<strong>缓存机制对比</strong>​：JPA 缓存与 MyBatis 缓存的异同分析</li></ul><p><strong>​点击关注，掌握 JPA/Hibernate 性能优化的核心技术！​</strong>​</p><blockquote><p>​<strong>今日行动建议</strong>​：</p><ol><li>检查现有项目中二级缓存配置，评估数据一致性风险</li><li>分析慢查询日志，识别需要拦截器优化的 SQL 模式</li><li>为批处理操作添加监控指标，建立性能基线</li><li>制定缓存失效策略评审机制，确保数据一致性</li></ol></blockquote>]]></description></item><item>    <title><![CDATA[AgentScope 拥抱函数计算 FC]]></title>    <link>https://segmentfault.com/a/1190000047446037</link>    <guid>https://segmentfault.com/a/1190000047446037</guid>    <pubDate>2025-12-03 15:07:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在 AI Agent 应用加速落地的今天，开发者和企业普遍面临三大核心痛点：<strong>部署成本高、运维复杂度高、资源利用率低</strong>。为应对这些挑战，AI Agent 与云原生、Serverless 架构的深度融合正成为行业新趋势。我们很高兴地宣布，AgentScope 正式集成基于阿里云函数计算（Function Compute, FC）的全新 Serverless 运行时，为多智能体应用提供“按需启动、毫秒弹性、零运维”的新一代运行底座。</p><h2>AgentScope 是什么？</h2><p><a href="https://link.segmentfault.com/?enc=3Y8FaxHkvfaN3gghN4FuLA%3D%3D.2SB6wFOxNt4wPmKTHwGYIqz%2BKHqyyX9UmLfCkH50YU1XqARUJ4k5wYBgZhh%2FXIM%2F" rel="nofollow" target="_blank">AgentScope</a> 是一个开源的多智能体应用开发框架，面向构建可观察、可控制、可扩展的 AI 智能体系统。其核心设计原则是<strong>对开发者完全透明</strong>：所有提示工程、模型调用、智能体行为及工作流编排均显式暴露，避免隐式逻辑或深度封装。</p><p>该框架拥有以下特性：</p><ul><li><strong>透明性优先</strong>：所有内部状态、消息传递路径、工具调用链路和模型交互过程均可追踪与审计，确保行为可解释、可调试。</li><li><strong>实时介入</strong>：实现ReAct智能体，原生支持任务执行过程中的实时中断与自定义中断处理逻辑，允许用户随时中断智能体的回复，介入智能体的执行，适用于需要人工干预或动态策略调整的场景。</li><li><strong>增强智能能力</strong>：提供统一的工具管理接口、长期记忆控制机制以及智能化 RAG（检索增强生成）支持，提升智能体的上下文感知与知识利用能力。</li><li><strong>模型无关架构</strong>：抽象统一的模型接入层允许同一套智能体逻辑无缝切换不同大语言模型（如 GPT、Claude、通义千问、Llama 系列等），降低模型迁移成本。</li><li><strong>模块化“乐高式”设计</strong>：智能体、工具、提示模板、记忆模块、工作流节点等组件高度解耦，支持独立开发、组合复用与灵活替换。</li><li><strong>原生多智能体支持</strong>：采用显式消息传递机制与声明式工作流编排，明确表达智能体间的协作关系，避免隐式调度带来的不可控性。</li><li><strong>高度可定制</strong>：支持对工具链、提示策略、通信协议、第三方库集成及可视化界面进行深度定制，适配从原型验证到生产部署的全周期需求。</li></ul><p>AgentScope 旨在为开发者提供一个既具备工程严谨性，又保持足够灵活性的智能体开发基础设施，推动多智能体系统从实验走向规模化落地。自开源以来，AgentScope 已获得社区广泛认可，GitHub Star 数突破 <strong>14,000+。</strong></p><h2>当前Agent运行时的挑战</h2><p><a href="https://link.segmentfault.com/?enc=c0yKbOhBva2PhUNZ73rvbA%3D%3D.huHuQdUUHJj1LMWnrmTma8C%2BdBRVPJUrIzz194qSasRWVtUbq5aY92LDy9bgQAqlKtOoixJ3pjG9vPY4UzV1Qg%3D%3D" rel="nofollow" target="_blank">AgentScope Runtime </a>是一个面向生产环境的智能体运行时框架，聚焦于两大核心问题：<strong>高效、可扩展的智能体部署</strong>与<strong>安全、隔离的Sandbox工具执行</strong>。该运行时提供上下文管理（包括长短期记忆与外部知识库集成）和多层级沙箱基础设施，构成一套框架无关的底层支撑系统，可与主流开源智能体框架或自定义实现无缝协同。其设计目标是为服务级智能体应用提供具备完整可观测性、强安全性与便捷部署能力的基础运行环境。</p><p>AgentScope Runtime实现了双核心架构：</p><ul><li><strong>智能体部署运行时（Engine）</strong><br/>提供智能体生命周期管理、会话状态维护、上下文存储（短期对话历史与长期记忆）以及外部知识库接入能力，并集成沙箱环境调度服务，支撑高并发、多会话的智能体服务部署。</li><li><strong>工具执行运行时（Sandbox）</strong><br/>基于隔离容器构建的安全执行环境，支持智能体调用各类工具操作，包括文件系统访问、浏览器自动化、GUI 交互及 MCP（Model Context Protocol）工具集成，确保所有副作用行为被严格限制在沙箱边界内，杜绝对宿主系统的潜在风险。</li></ul><p>目前，AgentScope 的主流部署模式依赖 <strong>Docker + Kubernetes</strong> 组合。该方案在功能完备性和集群管理能力上表现优异，但在实际落地 AI Agent 应用时，暴露出若干结构性瓶颈：</p><ul><li><strong>持续运行带来固定成本</strong><br/>容器实例需长期驻留内存以维持智能体状态和会话上下文，即使在无请求的空闲时段仍持续计费，导致显著的资源浪费，尤其对间歇性、事件驱动型任务极不友好。</li><li><strong>静态资源分配缺乏弹性</strong><br/>资源配额（CPU、内存）通常按预估峰值设定，难以动态适配真实负载。在流量突发时可能因资源不足导致响应延迟或失败；而在低峰期则大量计算资源闲置，利用率低下。</li><li><strong>高运维复杂度形成使用门槛</strong><br/>部署和维护一套生产级 K8s 集群涉及网络策略配置、服务发现、日志收集、监控告警、自动扩缩容（HPA）等多项云原生技能，对中小团队、独立开发者或非基础设施背景的研究人员构成显著障碍。</li></ul><p>这些限制使得许多具备潜力的 Agent 应用停留在实验阶段，难以实现低成本、高可用、快速迭代的规模化部署。</p><p>为系统性解决上述问题，AgentScope 正式推出基于<strong>阿里云函数计算（Function Compute, FC）</strong> 构建的 <strong>Serverless 运行时</strong>。该运行时针对 AI Agent 的典型工作负载（如会话保持、工具调用、状态依赖）进行深度优化，在保留功能完整性的同时，彻底重构资源使用与运维模型。</p><p>Serverless运行时的核心优势：</p><ul><li>✅<strong>按量付费，成本可精细化控制</strong><br/>计费粒度精确至毫秒级函数执行时间与内存消耗，空闲期间零费用。对于低频调用或突发型 Agent 任务，可有效降低成本。</li><li>✅<strong>毫秒级弹性伸缩，自动应对负载波动</strong><br/>无需预设实例数量或手动扩缩容，平台根据并发请求数自动调度计算资源，瞬时支撑从 1 到数千 QPS 的流量突增，保障服务 SLA。</li><li>✅<strong>零运维，聚焦核心逻辑开发</strong><br/>开发者无需关心底层服务器、容器镜像、K8s 配置或网络拓扑，仅需关注智能体逻辑、工具集成与业务流程编排，大幅缩短上线周期。</li></ul><p>此外，Serverless 运行时通过 <strong>会话亲和（Session Affinity）机制</strong>在无状态函数架构下有效支持有状态的 Agent 交互场景，兼顾弹性与一致性。</p><p>这一演进标志着 Agent 运行时正从“重资产、高运维”的传统模式，迈向“轻量化、自动化、经济高效”的云原生新范式，为 AI Agent 的大规模商业化落地扫清基础设施障碍。</p><h2>Serverless运行时集成能力详解</h2><h3>Engine 能力拓展</h3><p>Serverless 运行时深度集成 AgentScope 的核心执行引擎（AgentScope Runtime Engine），在保留原有编程模型的基础上，为开发者提供面向云原生环境的无缝部署体验。关键能力包括：</p><ul><li><strong>本地代码一键构建与依赖打包</strong><br/>开发者仅需在本地项目目录中执行<code>deploy()</code>方法，运行时即可自动分析 Python 依赖，构建包含用户代码、自定义工具及第三方库的可执行包，并上传至阿里云函数计算（FC）——该服务已深度集成于百炼 ModelStudio 平台，实现从开发到托管的一站式闭环。</li><li><strong>一键部署生成 HTTPS Endpoint</strong><br/>部署完成后，系统自动分配全局唯一的 HTTPS 端点（Endpoint），支持标准 RESTful 调用。外部系统（如 Web 前端、移动端或第三方服务）可通过该接口直接触发智能体执行，无需额外配置网关或反向代理。</li><li><strong>Header-Based Session 亲和性保障</strong><br/>为支持有状态交互（如多轮对话、工具链连续调用），Serverless 运行时引入基于 HTTP 请求头的会话绑定机制。客户端通过在请求中携带Session ID请求头，平台将确保同一 Session ID 的所有后续请求路由至同一函数实例（或关联的沙箱上下文），从而维持内存状态、临时文件或浏览器会话的一致性。</li><li><strong>继承 Serverless 核心优势</strong><br/>所有通过 Engine 部署的智能体天然享有 Serverless 架构的三大特性：按实际执行时间计费、毫秒级自动扩缩容、零基础设施运维，显著降低运营复杂度与总体拥有成本（TCO）。</li></ul><h3>Sandbox 运行时全面支持</h3><p>AgentScope 定义的四大沙箱类型现已完整适配 Serverless 运行时，可在函数计算环境中安全、高效地执行各类操作：</p><ul><li>✅ <strong>BaseSandbox</strong>：提供隔离的 Python 代码执行环境，适用于通用脚本运行与逻辑计算</li><li>✅ <strong>FileSystemSandbox</strong>：挂载临时或持久化文件系统，支持文件读写、日志记录与中间产物存储</li><li>✅ <strong>BrowserSandbox</strong>：内置无头 Chromium 浏览器，实现网页自动化、数据抓取与前端交互模拟</li><li>✅ <strong>GUISandbox</strong>：支持图形界面应用的模拟执行（如桌面软件自动化），适用于特定领域工具集成</li></ul><p>基于阿里云函数计算（FC）的Serverless运行时，深度集成 AgentScope 的Sandbox运行引擎，其核心特性如下：</p><ul><li><strong>预热实例池，消除冷启动延迟</strong><br/>平台可预先创建并维护一组常用类型的 Sandbox ，在新会话到来时直接复用，提高常驻服务的响应速度。</li><li><strong>自动注入 Session ID，保障上下文连续性</strong><br/>在首次创建 Sandbox 时，系统自动生成唯一 Session ID 并返回给客户端；后续所有针对该会话的 HTTP 请求均自动携带此 ID，确保操作始终作用于同一沙箱实例，保证状态一致性。</li><li><strong>全生命周期 Serverless 体验</strong><br/>每个 Sandbox 实例在会话结束后自动回收资源，计费随执行结束而终止，同样遵循 <strong>按量付费、毫秒级弹性、零运维</strong> 的 Serverless 原则，在安全性、性能与成本之间取得最佳平衡。</li></ul><p>通过 Engine 与 Sandbox 的双重增强，AgentScope 的 Serverless 运行时不仅解决了传统部署的成本与运维难题，更在保持强隔离与状态支持的前提下，实现了 AI Agent 应用的高效、安全、经济化交付。</p><h2>快速体验</h2><p>现在，您就可以将 Agent 应用快速部署到 Serverless 运行时！</p><h3>部署 Agent 到 Serverless运行时</h3><p>只需三步：</p><ol><li>配置相关环境变量</li></ol><pre><code class="yaml"># 确保设置环境变量
export DASHSCOPE_API_KEY="your-dashscope-api-key"
export ALIBABA_CLOUD_ACCESS_KEY_ID="your-access-key-id"
export ALIBABA_CLOUD_ACCESS_KEY_SECRET="your-access-key-secret"
export MODELSTUDIO_WORKSPACE_ID="your-workspace-id"

# 可选的OSS专用凭证
export OSS_ACCESS_KEY_ID="your-oss-access-key-id"
export OSS_ACCESS_KEY_SECRET="your-oss-access-key-secret"</code></pre><ol start="2"><li>定义好您的AgentApp</li></ol><pre><code class="yaml"># -*- coding: utf-8 -*-
# pylint:disable=wrong-import-position, wrong-import-order
import asyncio
import os

from agentscope.agent import ReActAgent
from agentscope.model import DashScopeChatModel

from agentscope_runtime.engine.agents.agentscope_agent import AgentScopeAgent
from agentscope_runtime.engine.runner import Runner
from agentscope_runtime.engine.schemas.agent_schemas import (
    MessageType,
    RunStatus,
    AgentRequest,
)
from agentscope_runtime.engine.services.context_manager import (
    ContextManager,
)
from agentscope_runtime.sandbox.tools.function_tool import function_tool
from others.other_project import version


@function_tool()
def weather_search(query: str) -&gt; str:
    if "sf" in query.lower() or "san francisco" in query.lower():
        result = "It's 60 degrees and foggy."
    else:
        result = "It's 90 degrees and sunny."

    return result


agent = AgentScopeAgent(
    name="Friday",
    model=DashScopeChatModel(
        "qwen-turbo",
        api_key=os.getenv("DASHSCOPE_API_KEY"),
    ),
    agent_config={
        "sys_prompt": "You're a helpful assistant named Friday.",
    },
    agent_builder=ReActAgent,
    tools=[
        weather_search,
    ],
)
print(f"AgentScope Runtime with dependencies version: {version}")


async def run():
    # Create a request
    request = AgentRequest(
        input=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "杭州天气如何？",
                    },
                ],
            },
        ],
    )

    runner = Runner(
        agent=agent,
        context_manager=ContextManager(),
        # context_manager=None       # Optional
    )
    async for message in runner.stream_query(request=request):
        # Check if this is a completed message
        if (
            message.object == "message"
            and MessageType.MESSAGE == message.type
            and RunStatus.Completed == message.status
        ):
            all_result = message.content[0].text
        print(message)

    print(f"📝 Agent response: {all_result}")


if __name__ == "__main__":
    asyncio.run(run())</code></pre><ol start="3"><li>配置部署相关代码，将您的代码部署到Serverless运行时上</li></ol><pre><code class="yaml">import asyncio
import os
from agentscope_runtime.engine.deployers.modelstudio_deployer import (
    ModelstudioDeployManager,
    OSSConfig,
    ModelstudioConfig,
)
from agent_app import app  # 导入已配置的 app

async def deploy_to_modelstudio():
    """将 AgentApp 部署到阿里云 ModelStudio"""

    # 配置 OSS 和 ModelStudio
    deployer = ModelstudioDeployManager(
        oss_config=OSSConfig(
            access_key_id=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_ID"),
            access_key_secret=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_SECRET"),
        ),
        modelstudio_config=ModelstudioConfig(
            workspace_id=os.environ.get("MODELSTUDIO_WORKSPACE_ID"),
            access_key_id=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_ID"),
            access_key_secret=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_SECRET"),
            dashscope_api_key=os.environ.get("DASHSCOPE_API_KEY"),
        ),
    )

    # 执行部署
    result = await app.deploy(
        deployer,
        deploy_name="agent-app-example",
        telemetry_enabled=True,
        requirements=["agentscope", "fastapi", "uvicorn"],
        environment={
            "PYTHONPATH": "/app",
            "DASHSCOPE_API_KEY": os.environ.get("DASHSCOPE_API_KEY"),
        },
    )

    print(f"✅ 部署到 ModelStudio：{result['url']}")
    print(f"📦 制品：{result['artifact_url']}")
    return result

if __name__ == "__main__":
    asyncio.run(deploy_to_modelstudio())</code></pre><p>📚 详细文档请参考：<a href="https://link.segmentfault.com/?enc=FpxD6zmiQuu2aJRl5sduHg%3D%3D.xUvnD%2Fb67dbP%2BW%2FBIf6w%2FVLRetQpxAaHreO5PH4Q7eMQvhISoq821H6mHBJToPePChF3D0H%2BCH47HaORVSZnkZxJaybpn2B0W8tgmjR%2BBm4NcVmbk%2B%2BKbDzRUMz%2FgdVf" rel="nofollow" target="_blank">部署指南</a></p><h3>快速启动 Sandbox</h3><ol><li>安装<code>agentscope-runtime</code></li></ol><pre><code class="shell">pip install agentscope-runtime</code></pre><blockquote>由于agentscope-runtime仍在初期快速迭代中，建议采用源码安装方式</blockquote><pre><code class="shell">git clone https://github.com/agentscope-ai/agentscope-runtime.git

cd agentscope-runtime

pip install .</code></pre><ol start="2"><li>配置环境变量</li></ol><pre><code class="shell"># Service settings
HOST="0.0.0.0"
PORT=8000
WORKERS=1
DEBUG=False

# Runtime Manager settings
DEFAULT_SANDBOX_TYPE=base
POOL_SIZE=0
AUTO_CLEANUP=True
CONTAINER_PREFIX_KEY=agent-runtime-container-
CONTAINER_DEPLOYMENT=agentrun
DEFAULT_MOUNT_DIR=
STORAGE_FOLDER=runtime_sandbox_storage
PORT_RANGE=[49152,59152]

# FC 相关账户信息
FC_ACCOUNT_ID=&lt;your-account-id&gt;
FC_ACCESS_KEY_ID=&lt;your-access-key-id&gt;
FC_ACCESS_KEY_SECRET=&lt;your-access-key-secret&gt;
FC_REGION_ID=cn-hangzhou
# 规格配置
FC_CPU=2.0
FC_MEMORY=2048
# 网络配置
FC_VPC_ID=&lt;your-vpc-id&gt;
FC_VSWITCH_IDS=[&lt;your-vswitch-id&gt;]
FC_SECURITY_GROUP_ID=&lt;your-security-group-id&gt;
# 前缀
FC_PREFIX=agentscope-sandbox
# 日志配置
FC_LOG_PROJECT=&lt;your-sls-log-project&gt;
FC_LOG_STORE=&lt;your-sls-log-store&gt;</code></pre><ol start="3"><li>运行命令，启动沙箱服务器</li></ol><pre><code class="shell">runtime-sandbox-server --config fc.env</code></pre><ol start="4"><li>使用您的沙箱</li></ol><pre><code class="python">from agentscope_runtime.sandbox import BaseSandbox

# 连接到远程服务器（替换为您的实际服务器地址和端口）
with BaseSandbox(
    base_url="http://127.0.0.1:8000",
) as sandbox:
    # 正常使用沙箱
    print(box.list_tools())
    print(box.run_ipython_cell(code="print('hi')"))
    print(box.run_shell_command(command="echo hello"))
    input("Press Enter to continue...")</code></pre><p>📚 详细文档请参考：<a href="https://link.segmentfault.com/?enc=YEAUpxPBBnOYW1VusfzjjQ%3D%3D.fXpF9K8COpIOcpaIQ%2B6VyQ5Y69yKfaHHt3TYjdiEVAudnnrbUMuNOq%2B0VHHsbrBjS3gi6504RdfWhBVxqz1hJXxLXwLarjgPqIIARlwrVzbzc9coejozQ5GDGNj3D8vw" rel="nofollow" target="_blank">沙箱部署指南</a></p><h2>迈向“省钱又好用”的 AI 运行时</h2><p>AI Agent 的运行时基础设施正经历一场深刻的演进：从早期追求“能跑起来”的基础可用性，到关注开发体验与功能完备性的“好用”阶段，如今正加速迈向兼顾性能、安全与经济性的“省钱用”新范式。</p><p>AgentScope 与 Serverless 架构的深度集成，正是这一演进的关键实践。通过将智能体部署与工具执行全面迁移至基于阿里云函数计算（FC）的 Serverless 平台，不仅大幅降低了对容器编排、集群运维等云原生技能的依赖，更从根本上重构了资源使用模型——<strong>从“为闲置付费”转向“为实际执行付费”</strong>，使中小团队乃至个人开发者也能以极低成本运行生产级 Agent 应用。</p><p>Serverless 所提供的毫秒级弹性、自动扩缩容、强隔离沙箱与零运维特性，恰好契合 AI Agent 应用典型的负载特征：间歇性调用、状态依赖性强、工具执行风险高、成本敏感度高。我们坚信，<strong>Serverless 将成为 AI Agent 应用的最佳运行时。</strong></p><p>未来，AgentScope 将持续深化与主流云服务的协同，进一步优化会话管理、冷启动延迟、多模态工具支持等关键路径，并推动更多开源智能体项目采纳 Serverless 范式，构建一个开放、高效、经济的 Agent 运行生态，让复杂智能体系统的开发与部署如同调用普通 API 一样简单可靠。</p><p><strong>让每一个智能体，都能轻盈运行在云端。</strong></p>]]></description></item><item>    <title><![CDATA[如何将照片从 Mac 传输到 Andro]]></title>    <link>https://segmentfault.com/a/1190000047446115</link>    <guid>https://segmentfault.com/a/1190000047446115</guid>    <pubDate>2025-12-03 15:07:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>将照片从 Mac 传输到 Android 手机或平板电脑有时会感觉像是跨越数字鸿沟。由于 Mac 和 Android 运行在不同的生态系统中，直接拖放的方法行不通。如果您想将照片从 Mac 传输到 Android，可以尝试本指南中介绍的有效方法，这些方法可以高质量地传输照片。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446117" alt="图片" title="图片"/></p><p>快速浏览一下这5种方法：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446118" alt="图片" title="图片" loading="lazy"/></p><p>第一部分：如何通过 iReaShare Android Manager 将照片从 Mac 传输到 Android</p><p>像iReaShare Android Manager这样的专用桌面软件旨在提供一套全面的一体化解决方案，方便您从 Mac 管理 Android 设备。借助此程序，您可以直接在 Mac 和 Android 之间传输图片，并保持原始画质。</p><p>iReaShare Android Manager 的主要功能：</p><ul><li>将照片从 Mac 电脑复制到 Android 设备。</li><li>还可以将照片从安卓设备传输到Mac电脑。</li><li>允许您在传输照片之前预览和选择照片。</li><li>还可以传输视频、音乐、文档、联系人、短信、通话记录和应用程序。</li><li>支持只读模式，以确保数据传输安全。</li><li>适用于运行 Android 6.0 或更高版本的 Android 设备，例如三星、一加、TCL、Tecno、OPPO、Vivo、荣耀、谷歌、摩托罗拉等。</li></ul><p>下载iReaShare Android Manager。</p><p>下载 Mac 版下载 Win 版</p><p>使用 Android Manager 将照片从 Mac 传输到三星 Android 设备：</p><pre><code>
下载适用于 Mac 的 iReaShare Android Manager 应用程序并安装。然后启动软件，并使用 USB 数据线将您的 Android 设备连接到 Mac 电脑。如果您需要无线连接，请将两台设备连接到同一个 Wi-Fi 网络，然后点击“通过 Wi-Fi 连接”。


按照屏幕上的指示在安卓设备上启用USB调试模式。连接将会建立。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446119" alt="图片" title="图片" loading="lazy"/></p><pre><code>
现在，选择“照片”类别。然后点击“相机”或“图库”，再点击“添加”从您的 Mac 电脑中选择照片。最后，点击“打开”或“确定”开始将照片导入 Android 设备。

</code></pre><p>第二部分：如何通过 Google Photos 将 Mac 上的照片传输到 Android 设备</p><p>Google Photos是一款跨平台云存储和同步服务，无需数据线即可在 Mac 和 Android 设备之间轻松同步文件。但是，如果您的帐户云存储空间不足，则无法一次性上传所有照片。</p><p>将 Mac 上的照片同步到 Android：</p><pre><code>
在您的Mac电脑上打开网络浏览器，访问Google Photos网站。然后使用您在Android手机上使用的同一个Google帐户登录。


点击“ + ”按钮 &gt; “导入照片”，然后从Mac本地存储中选择照片。等待上传完成。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446120" alt="图片" title="图片" loading="lazy"/></p><pre><code>在您的安卓手机上，打开 Google 相册应用。确保您已登录同一个 Google 帐户。您从 Mac 上传的照片将自动出现在您安卓设备上的 Google 相册图库中，您可以通过 Wi-Fi 或移动网络访问这些照片。


如果要将照片存储在 Android 设备的本地存储空间中，请在 Google Photos 应用中打开图片，点击三点菜单，然后选择“下载”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446121" alt="图片" title="图片" loading="lazy"/></p><p>第三部分：如何通过 AirDroid 将照片从 Mac 传输到 Android 手机</p><p>AirDroid 是一款流行的第三方应用程序套件，可让您从桌面无线管理您的 Android 设备，使文件传输变得简单方便。</p><p>使用 AirDroid 将照片从 Mac 传输到 Android 手机：</p><pre><code>
请在您的安卓手机和Mac电脑上下载并安装AirDroid应用程序。创建AirDroid帐户并在两台设备上登录。


请确保您的 Mac 和 Android 手机连接到同一个 Wi-Fi 网络。在您的 Mac 上，打开 AirDroid 桌面应用程序。


从列表中选择您的安卓设备，然后选择“设备”选项卡。


将您想要传输的照片从 Mac 的 Finder 窗口拖放到与您的 Android 设备关联的 AirDroid 文件传输界面上。传输将通过您的网络以无线方式进行。照片将保存到您 Android 手机上指定的 AirDroid 文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446122" alt="图片" title="图片" loading="lazy"/></p><p>第四部分：如何通过 OpenMTP 将图片从 Mac 传输到 Android 设备</p><p>OpenMTP 是一款免费的开源应用程序，专门用于解决 Mac 到 Android 的文件传输问题，它利用了 MTP（媒体传输协议）标准。它充当文件浏览的专用桥梁。</p><p>通过 OpenMTP 将图片从 Mac 传输到 Android：</p><pre><code>
下载适用于 Mac 的 OpenMTP 应用程序并安装。然后使用 USB 数据线将您的 Android 设备连接到 Mac。


在您的安卓手机上，下拉通知栏并点击USB连接通知。选择“文件传输/Android Auto ”或“ MTP ”选项。


在您的 Mac 上启动 OpenMTP 应用程序。它采用双窗格界面：一侧用于显示 Mac 上的文件（左侧），另一侧用于显示 Android 设备的内部存储（右侧）。


在 Mac 上找到包含照片的文件夹（左侧面板），然后在 Android 设备上找到目标文件夹（右侧面板）。在 Mac 的面板中选择照片，然后点击“传输”按钮（通常是一个从左到右的箭头）开始传输。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446123" alt="图片" title="图片" loading="lazy"/></p><p>第五部分：如何通过 LocalSend 将照片从 Mac 发送到 Android 手机</p><p>LocalSend 是一款跨平台的开源应用程序，允许通过本地网络进行安全的点对点文件共享，类似于苹果的 AirDrop，但它适用于 Android、Mac、Windows 和 Linux。</p><p>通过 LocalSend 将照片从 Mac 传输到 Android：</p><pre><code>
请在您的Mac电脑和安卓手机上下载并安装LocalSend应用程序。确保两台设备都连接到同一个本地Wi-Fi网络。


在两台设备上启动 LocalSend 应用。它们应该会自动发现彼此。在 Mac 应用中，点击“发送”，然后选择要传输的照片。


从可用接收设备列表中选择您的安卓设备。您的安卓手机上会弹出通知。点击“接受”开始转账。


照片将保存到您安卓设备上指定的 LocalSend 文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446124" alt="图片" title="图片" loading="lazy"/></p><p>第六部分：关于将照片从 Mac 传输到 Android 的常见问题</p><p>Q1：我可以使用 Android 文件传输将照片从 Mac 拖放到 Android 设备上吗？</p><p>是的，您可以使用适用于 Mac 的 Android 文件传输 (AFT) 应用，将照片和其他文件从 Mac 拖放到 Android 手机上。虽然拖放功能可用，但需要注意的是，Android 文件传输已不再由 Google 官方支持或更新，并且经常不稳定，尤其是在较新版本的 macOS 和 Android 系统上。</p><p>Q2：我可以使用 AirDrop 将照片从 Mac 传输到 Android 设备吗？</p><p>不，您无法使用 Mac 上标准的内置 AirDrop 功能向绝大多数 Android 手机（包括三星、一加等）发送照片。尽管谷歌已经将 Android 的“快速分享”功能与苹果的 AirDrop 协议进行了整合，但这项功能非常新，尚未在所有 Mac 和 Android 机型上广泛普及。</p><p>Q3：我可以通过蓝牙将图片从Mac传输到Android吗？</p><p>是的，您可以使用蓝牙将图片从 Mac 传输到 Android 手机，因为这两个设备都支持蓝牙文件传输协议。这可以通过 Mac 上内置的蓝牙文件交换应用程序完成。但是请注意，这种方法通常速度很慢，仅建议用于传输少量小文件，例如几张照片。</p><p>结论</p><p>现在，通过以上方法，您可以轻松地将照片从 Mac 传输到 Android 设备。无论您喜欢 Google Photos 的无缝云端同步，还是iReaShare Android Manager的强大管理功能，总有一种方法能够完美地跨越 Mac 和 Android 之间的鸿沟。顺便一提，如果您经常在 Mac 和 Android 设备之间传输文件，iReaShare Android Manager 将是您的最佳助手。<br/>​</p>]]></description></item><item>    <title><![CDATA[低代码平台定义解析与选型建议(2025版]]></title>    <link>https://segmentfault.com/a/1190000047446140</link>    <guid>https://segmentfault.com/a/1190000047446140</guid>    <pubDate>2025-12-03 15:06:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、低代码定义解析：</h2><p>低代码平台是一种基于可视化设计与配置的应用开发方法，核心在于通过组件化、图形化、参数化的拖拽配置方式进行系统开发，开发过程中能大幅减少手动编码量，从而提升应用交付速度与效率。</p><p>1、技术内涵：</p><p>从技术内涵看，低代码平台通常依托表单驱动、模型驱动等技术路径，提供数据编排、生态连接、服务集成等核心能力，以图形化方式实现业务场景的快速构建与创新。</p><p>2、技术演进：</p><p>低代码的发展也算是历经了较长时间阶段的演进与共识凝聚。早在20世纪80年代，“第四代编程语言”已初显低代码思想；至2014年，Forrester明确定义了“低代码/零代码”概念，并指出低代码能够以最少的手工编码快速开发、配置和部署应用系统。2018年，Gartner进一步提出aPaaS与iPaaS概念，其中aPaaS与低代码理念高度契合，有效推动了低代码技术在全球范围内的关注与落地。</p><p>随着低代码技术的发展与行业实践，国内主流厂商（如织信、宜搭、奥泽等）达成共识，推出《低代码平台发展白皮书》，白皮书中明确了低代码平台的完整定义，强调其应以可视化配置为主、少量代码为辅，并具备全生命周期管理能力。</p><p>如今，低代码平台已广泛应用于企业数字化转型场景，截止2025年11月，低代码已在工业领域的MES、PLM系统灵活扩展中发挥作用，目前已达到百亿市场规模，依然构建起了一个完整的低代码生态体系。</p><p>3、产品特征：</p><p>低代码平台必须具备应用全生命周期管理能力，支持设计、开发、测试、部署、迭代、运维的全生命周期管理，实现应用开发效率提升、需求快速响应、敏捷迭代更新、运营维护便捷等目标，是一站式的应用开发平台。</p><h2>二、低代码平台选型建议</h2><p>对应用漏洞、平台安全性、数据安全合规等方面有要求的企业，选型时要注重：</p><p>深入评估平台安全能力： 不要只看厂商宣传，要了解平台是否具备“SAST（静态应用安全测试）和DAST（动态应用安全测试）”等自动化安全检测能力。</p><p>审查平台的权限管理机制：确保平台能提供细粒度的权限控制，包括角色权限、数据权限、字段权限等，并支持单点登录（SSO）和多因素认证（MFA）。</p><p>考察数据处理与存储： 询问平台的数据中心位置，数据传输是否使用TLS加密，以及是否支持私有化部署以满足特殊安全需求。</p><p>对集成能力、集成成本、数据同步等方面有需求的企业，选型时要考察：</p><p>考察API连接能力：确保平台支持API和SOAP等主流API协议，并且能够轻松调用和封装外部API。</p><p>查看集成案例和连接器：了解平台是否提供丰富的预置连接器，例如与钉钉、企业微信、飞书等常用系统的连接器。</p><p>评估异步处理能力： 询问平台如何处理大规模数据同步和异步任务，是否支持“ 消息队列（Message Queue）”等机制。</p><p>对生态拓展、性能瓶颈、二次开发和能力边界等方面有要求的企业，选型时要着重考虑：</p><p>评估平台的扩展性： 询问平台是否提供API接口、SDK、自定义组件开发框架等，支持开发者通过代码扩展平台功能。</p><p>关注平台架构： 了解平台是否采用微服务架构，支持独立部署和弹性伸缩，以应对业务增长带来的性能挑战。</p><p>考察“低代码”与“高代码”的融合： 理想的平台应该能够让低代码开发者和专业开发者在同一个平台协同工作，低代码负责快速构建，高代码负责复杂逻辑和性能优化。</p><h2>三、国内主流的低代码平台排行（12月最新）</h2><p>1、织信</p><p>推荐指数：★★★★★</p><p>综合评分：99.7分</p><p>织信Informat是由深圳基石协作自主研发的企业级低代码开发平台，平台基于“数据、流程、角色”三个基本要素，用户只需通过简单的“拖拽”、“配置”等操作，即可快速搭建整套的数字化管理系统。此外，平台还拥有足够强的边界能力，内置了脚本、自动化、网站、自定义页面、符合BPMN2.0规范的工作流引擎等功能，能满足大部分企业复杂的业务需求。该产品着重面向ToB企业内部信息化/数字化建设，为企业提供高效、定制、专业的数字化咨询与信息化系统一体化解决方案。</p><p>产品特点：支持本地私有化部署，上亿级大数据大并发处理能力，使用层与开发层分离，标准化的运维版本管理体系。基于织信低代码构建多年的aPaaS能力与自动化、流程化模式被进一步释放，构建一款应用时，企业可将前后端开发等环节紧密衔接，减少大量重复性工作，并有效提升 67% 的IT项目效率。</p><p>2、奥哲云枢</p><p>推荐指数：★★★★★</p><p>综合评分：98.3分</p><p>奥哲云枢是面向大中型企业复杂业务场景的低代码平台。其核心竞争力在于“All in One”的数智化引擎，将低零代码开发、AI智能、集成开放与数据可视化能力深度融合，形成覆盖应用全生命周期的一站式解决方案。平台支持从“零代码配置”到专业代码扩展的全场景开发模式，让业务人员与技术团队能够高效协同。作为市场公认的标杆性产品，奥哲服务了国内大量500强企业，在建筑、能源、金融等行业积累了丰富的央国企实践案例，市场占有率位居前列。</p><p>产品特点：以强大的流程引擎为核心，擅长构建和优化复杂、长周期的业务流程。平台提供了从设计、开发、集成到运维的完整能力，能有效封装企业既有业务能力并连接新老系统，帮助大型集团在复杂组织架构下实现数字化运营与流程管理。</p><p>3、宜搭</p><p>推荐指数：★★★★★</p><p>综合评分：96.6分</p><p>宜搭是阿里巴巴推出的低代码应用构建平台，深度融入钉钉生态，主打高效协同与快速部署。平台与钉钉的组织架构、消息通知、待办审批等原生能力无缝集成，用户可在钉钉工作台内直接创建和使用应用，实现“开发即使用”的协同体验。2025年，平台接入了DeepSeek大模型，AI流程自动化能力得到增强，可通过自然语言指令快速生成表单。平台提供了超过500个行业模板，能快速满足零售、医疗等领域的轻量化应用需求。</p><p>产品特点：依托阿里云与钉钉的双重生态，在服务的稳定性和安全性方面有较好保障。其操作轻量化，订阅制定价模式灵活，特别适合中小企业、部门团队快速搭建审批、人事、行政等内部协同类应用，大幅降低数字化门槛。</p><p>4、炎黄盈动</p><p>推荐指数：★★★★</p><p>综合评分：94.9分</p><p>炎黄盈动是一家专注于业务流程管理（BPM）与PaaS平台的服务商，其AWS PaaS平台是在深厚BPM技术积累上演进而来的企业级低代码平台。平台采用云原生和模型驱动架构，为企业构建关键业务应用提供稳定可靠的底层支撑。其核心优势在于强大的流程建模、自动化与编排能力，能够帮助企业梳理并优化端到端的复杂业务流程，实现跨部门、跨系统的业务协同。</p><p>产品特点：在流程挖掘与低代码开发深度融合方面表现突出。平台非常适合流程密集型的大型组织，尤其在金融、政务、能源等对系统稳定性、安全性和合规性有严苛要求的行业，是构建任务关键型（Mission-Critical）应用系统的有力工具。</p><p>5、Zoho Creator</p><p>推荐指数：★★★★</p><p>综合评分：93.5分</p><p>Zoho Creator是国际知名软件服务商Zoho旗下的低代码开发平台，拥有超过18年的行业经验，技术成熟稳定。平台提供从表单搭建、流程自动化到数据分析的全栈能力，并支持从零代码到全代码的平滑过渡。其最大特色之一是集成了AI助手“Zia”，支持通过文本描述自动生成应用，显著提升开发效率。作为全球化平台，它支持30多种语言，在全球拥有多个数据中心，兼顾了本地化适配与全球化协同需求。</p><p>产品特点：与Zoho旗下的CRM、项目管理等25+应用无缝集成，生态融合性强。平台采用灵活的订阅制收费，支持1人起购，性价比高，既能满足中小企业当前的全场景需求，也能支撑大型企业未来的规模化扩展与跨区域部署。</p><p>6、JeecgBoot</p><p>推荐指数：★★★★</p><p>综合评分：91.7分</p><p>JeecgBoot是国内首个免费开源的低代码平台，由开源社区主导研发，基于BPM理念构建，采用SpringBoot 3.x、SpringCloud、Vue等前后端分离架构，全面支持微服务架构。用户通过平台强大的代码生成器可一键生成前后端完整代码，配合在线表单、报表、大屏设计等丰富低代码模块，实现简单功能零代码配置、复杂功能低代码生成的灵活开发模式，大幅减少重复开发工作。平台聚焦Java技术栈企业数字化需求，为开发者提供全流程开发支撑。</p><p>产品特点：完全免费开源且社区活跃度高，提供丰富的学习资源与技术支持，大幅降低企业研发成本；业务流程采用工作流引擎与表单松耦合设计，支持灵活配置与个性化扩展，保障企业流程保密性；支持与各类Java生态系统深度集成，灵活度高，特别适合Java项目团队快速构建ERP、CRM、OA等业务系统；可显著减轻开发人员负担，开发效率较传统模式提升50%以上。</p><p>不知道不觉也写了三千多次，今天暂时先说到这，如果大家在选型方面有任何疑问，欢迎私信交流。附赠低代码产品上手文档说明。</p>]]></description></item><item>    <title><![CDATA[UniParse：让多模态模型真正“读懂]]></title>    <link>https://segmentfault.com/a/1190000047446168</link>    <guid>https://segmentfault.com/a/1190000047446168</guid>    <pubDate>2025-12-03 15:05:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p>在多模态大模型迅速发展的今天，我们已经能让模型"看图说话"，甚至"读懂表格"，但要让模型真正理解复杂的文档结构（例如在PDF中准确识别章节、表格、公式与图像的逻辑关系）依然是一个未被彻底解决的问题。</p><p>UniParse正是为此而生：它是一款<strong>面向AI应用的通用文档解析工具</strong> ，旨在将文档中的非结构化内容转化为结构化语义信息，使多模态模型能够<strong>高效、精准</strong>地理解和利用文档内容。</p><p>本文将从技术视角介绍UniParse，功能方面的介绍请移步<a href="https://link.segmentfault.com/?enc=MkVHpxrmSRHafilFjjvZkg%3D%3D.AX6tuUW3vBrSdxUQEkv%2BiKtT15vnrh%2Foq0%2BX%2FOYbvXnVGXQxK%2BTBlWoJDuUOreJLxljWZq%2BVj2FXHfRcvM%2FeBZHmFYN89xsCkqxA3v453vILV1RNJkHBkYdBKK1UYRsf%2BMOhyJYRYyM3l5JQDZmDPDjrQG5eUSWNhYwVJkz3rd%2BpL%2BsZIBHHKQTPYRffLShA" rel="nofollow" target="_blank">产品上线|商汤自研智能文档解析工具UniParse，重新定义文档处理！</a></p></blockquote><hr/><h2><strong>一、为什么需要文档解析</strong></h2><p>现代大模型已经能够处理文本、图像、语音等多种模态，但在面对文档时仍然存在明显短板：</p><ul><li><strong>格式复杂</strong>：PDF、Word等文件中同时包含文字、表格、图片、公式、页眉页脚等多种内容，且层次不统一。</li><li><strong>结构缺失</strong>：OCR只能识别文字，却无法恢复章节层级与逻辑顺序。</li><li><strong>语义混乱</strong>：表格、图像与正文往往存在隐含关联，模型难以在语义上进行对齐。</li></ul><p>这意味着，如果直接把整份文档输入多模态模型，模型将面临巨大的上下文噪声和空间混乱，生成效果不稳定，也无法进行精确问答。UniParse的作用，就是在模型"读文档"之前，帮它<strong>理清结构、分清语义、建立关联</strong>。</p><hr/><h2><strong>二、UniParse的技术流程</strong></h2><p>UniParse的核心流程分为两个主要阶段：<strong>版面分析（LayoutAnalysis)与内容提取（ContentExtraction）</strong> ，并辅以<strong>预处理</strong> 与<strong>内容合并</strong>两个辅助流程。整个流程既保持模块化设计，又在数据层实现了结构化信息流动，使得不同模态内容（文字、图片、表格、公式）能够被统一建模和调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446170" alt="" title=""/></p><h3>1️⃣<strong>文档预处理</strong></h3><p>UniParse的预处理阶段主要任务是<strong>统一输入格式</strong> 。系统会将各类文档（PDF、DOC、DOCX等）<strong>逐页渲染为高分辨率图像</strong>，保证不同文件格式在后续视觉模型中具有一致的输入维度。这一过程通常基于PyMuPDF或libreoffice的渲染引擎实现，可控制分辨率以兼顾清晰度与性能。</p><p>同时，预处理阶段还执行以下步骤：</p><ul><li><strong>页面编号与坐标标准化</strong>：为每页图像生成统一的坐标系，用于后续版面元素定位；</li><li><strong>去噪与边缘裁剪</strong>：提升模型在扫描件、照片类文档上的鲁棒性；</li><li><strong>文件元信息提取</strong>：（如页数、文件名、创建时间），用于文档追踪与任务调度。</li></ul><p>经过预处理后，所有文档都被转化为一组图像文件及其基础元信息，为后续的版面解析与内容提取提供统一输入。</p><h3>2️⃣<strong>版面分析</strong></h3><p>版面解析是UniParse的核心之一，目标是<strong>还原文档的空间与语义结构</strong> 。这一阶段采用<strong>视觉语言联合建模</strong>方法：</p><ul><li>在视觉层面，利用版面分析模型（如LayoutLMv3或自研视觉Transformer）识别标题、正文、表格、图像、公式、脚注等区域；</li><li>在语言层面，通过文本块的字体、缩进、上下文语义判断章节层次与逻辑顺序；</li><li>最终将视觉检测结果与文本序列对齐，生成一个包含位置、类型与层级的<strong>结构化版面树</strong>。</li></ul><h3>3️⃣<strong>内容提取</strong></h3><p>UniParse针对不同类型内容采用<strong>专用解析管线</strong>：</p><ul><li><strong>文字</strong>：OCR模型或文本提取API结合版面坐标进行文本恢复与段落重建；</li><li><strong>表格</strong>：基于结构化表格识别网络（如TableFormer或自研模型）恢复单元格位置、合并关系与层级结构，输出HTML/LaTeX格式；</li><li><strong>图片</strong>：通过OCR或视觉语言模型（VLM）获取图像描述，为多模态模型提供语义锚点；</li><li><strong>公式</strong>：采用基于Transformer的公式识别引擎将公式区域转化为可编辑的LaTeX表达式。</li></ul><p>每种内容在抽取后都会带有来源页、坐标和上下文标签，以便在合并阶段进行定位与关联。</p><h3>4️⃣<strong>语义层重构</strong></h3><p>最后一步是内容合并与输出。系统将前述多类型元素按照版面树的层级进行拼接，恢复出原文档的逻辑顺序与结构。这一阶段还可以进行：</p><ul><li>内容去重与段落融合（防止跨页重复文本）；</li><li>模态链接（表格、图像与正文语义匹配）；</li><li>结构化输出（统一输出为JSON、HTML或Markdown格式）。</li></ul><p>通过这一设计，UniParse能在保持文档可读性的同时，为下游多模态模型提供可计算的结构化输入。</p><hr/><h2><strong>三、UniParse与多模态大模型的协同机制</strong></h2><p>多模态模型的核心挑战之一是模态对齐。传统方法依赖模型内部注意力机制去"猜测"文本与视觉区域的对应关系，而UniParse提供了<strong>显式的结构锚点</strong>。</p><p>从工程上看，UniParse的结构化输出可以直接映射到模型输入的不同通道：</p><ul><li>文本节点被编码为语言向量；</li><li>表格与公式节点可转换为结构token序列；</li><li>图像节点对应视觉特征向量；</li><li>节点之间的层级关系（如章节树）可编码为attentionmask，用于指导模型的跨模态关注。</li></ul><p>通过这种方式，UniParse在模型输入阶段实现了<strong>结构化对齐</strong>：</p><ul><li>模型在编码时能基于文档结构进行有选择的注意力分配；</li><li>上下文检索与问答更精确，因为每个节点都带有位置标签；</li><li>生成内容可以反向追溯到原文档区域，实现可解释性。</li></ul><p>换言之，UniParse并非一个单纯的"预处理器"，而是为多模态大模型提供了结构感知接口，让模型真正理解"这是一份文档"，而不仅仅是一组视觉与文本片段。</p><hr/><h2><strong>四、应用场景：从文档解析到智能理解</strong></h2><p>UniParse的技术能力为多模态模型打开了更广阔的应用空间：</p><ul><li><strong>智能问答（QA）</strong>：大模型可直接基于结构化数据进行文档问答，不仅能回答正文问题，也能解析表格、公式或图表。</li><li><strong>知识抽取与检索增强生成（RAG）</strong>：通过文档语义图构建可检索知识库，支持高精度上下文匹配。</li><li><strong>报告生成与内容审校</strong>：结构化信息流使模型能生成符合格式规范的总结、分析报告或审阅意见。</li><li><strong>图文理解与多模态推理</strong>：表格、公式、图片被视为独立模态单元，与文本共同构成推理输入，适用于学术报告、财务报表等复杂文档。</li></ul><hr/><p><strong>小结</strong></p><blockquote>在多模态智能系统的发展路径中，<strong>结构化理解</strong>是必经之路。UniParse作为文档解析的基础设施，为大模型提供了语义层级、视觉位置与逻辑关系的桥梁，使文档理解从模糊感知走向可解释推理。未来，模型的"读文档"能力将不断演进------它们不再仅仅识别信息，而是能够基于文档的结构和语义进行真正的理解与推理。</blockquote><hr/><p>更多技术讨论，欢迎移步 "万象开发者" gzh！</p>]]></description></item><item>    <title><![CDATA[研发效能度量工具横评：哪些指标真的能提升]]></title>    <link>https://segmentfault.com/a/1190000047446191</link>    <guid>https://segmentfault.com/a/1190000047446191</guid>    <pubDate>2025-12-03 15:04:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>市面上能做研发效能度量的工具越来越多，有的是一体化研发管理平台，有的专注工程效能，有的来自云厂商 DevOps 套件，还有自建的开源度量方案。但决定交付能力的，既不是报表数量，也不是图表是否炫酷，而是——你到底在度量哪些研发效能指标，这些指标与交付瓶颈的关系有多紧密，工具是否支持闭环改进。本文选取四类典型工具路线及代表产品做横向测评，系统梳理关键研发效能度量指标及其适用场景，希望能帮助你做更理性的选型与规划。</blockquote><h2>先想清楚为什么要做「研发效能度量」</h2><p>很多企业做研发效能度量，是这样开始的：</p><p>先采购或搭建一个“研发效能平台”；<br/>接入需求、缺陷、流水线、Git 等多种数据源；<br/>几个月后，报表很多，但决策和交付方式几乎没变。</p><p>从组织视角看，问题往往出在「顺序」上—— 先有工具，再找指标，再想目的。</p><p>更健康的顺序应该是：</p><p><strong>① 先定义要解决的问题</strong></p><ul><li>一直延期，想缩短端到端交付周期？</li><li>线上频繁故障，想提升质量与稳定性？</li><li>新产品投入很大，但客户感知不强，想优化价值交付？</li></ul><p><strong>② 再挑出最关键的研发效能度量指标</strong></p><ul><li>哪几个指标能直接反映这个问题？</li><li>哪些是“结果指标”，哪些是“过程指标”？</li></ul><p><strong>③ 最后再看工具与路径</strong></p><ul><li>哪类工具更容易精准采集这些数据？</li><li>哪类平台更便于把指标带进迭代会、项目会、季度复盘？</li></ul><p>如果这三步不清楚，再完备的工具横评也只是“功能列表”。下面这 4 组研发效能度量指标，可以视为组织级的“基准标尺”。</p><h2>四大类真正影响交付的「研发效能度量指标」</h2><p>要评估一款研发效能度量工具是否值得引入，核心不是“能做多少报表”，而是能否支持下面四组关键指标的采集与使用：</p><h4>1. 流动效率指标：交付是不是在“顺畅地流动”</h4><ul><li>端到端交付周期（Lead Time）：从需求提出到上线。</li><li>开发周期（Cycle Time）：从开始开发到完成开发。</li><li>在制品数量（WIP）：同时在做多少工作。</li><li>吞吐量（Throughput）：单位时间完成多少工作项。</li></ul><p>这些研发效能度量指标的作用是：判断团队是“太忙导致变慢”，还是“系统性瓶颈导致变慢”。</p><h4>2. 质量与稳定性指标：研发效能能不能“可持续”</h4><ul><li>缺陷密度、缺陷分布。</li><li>变更失败率、回滚次数。</li><li>故障平均恢复时间（MTTR）。</li><li>与发布事件的关联。</li></ul><p>这组研发效能度量指标用于回答：我们是在“加速交付”，还是在“透支质量”？产品稳定性是否足以支撑更快的交付节奏？</p><p>如果质量指标长期失控，任何短期的“交付提速”都只是在透支未来。</p><h4>3. 价值与资源配置指标：忙碌是否真的创造价值</h4><ul><li>需求从立项到首次上线的周期。</li><li>不同类型需求（创新、优化、技术债）的占比。</li><li>废弃或长期搁置需求比例。</li></ul><p>这类研发效能度量指标回答的问题是：研发在多大程度上被“真正创造价值的事”占据？</p><h4>4. 协作与团队健康指标：交付问题的领先信号</h4><ul><li>插单率、计划与实际偏差。</li><li>跨团队依赖导致的等待时间。</li><li>简单调研上的阻碍感、负荷感。</li></ul><p>这些研发效能度量指标往往比故障率更早暴露风险，是组织“体温计”。很多交付危机，最早不是出现在流水线，而是出现在“团队感觉不对”。</p><p>后文对各类工具与路径的横评，都围绕这四组指标展开：能否支撑这些指标的采集、分析与闭环，是衡量研发效能度量工具价值的核心标准。</p><h2>四类研发效能度量工具横评</h2><h4>1. 一体化研发管理平台 —— 让「工作流」与「度量数据」同源</h4><p>这一类工具的共同特征是：本身就是需求 / 项目 / 缺陷 / 测试 / 流水线的协同平台，研发效能度量的数据主要来自团队日常使用，而非额外报表工程。典型代表有 ONES、Jira Software、Azure DevOps 等。</p><p>它们适合回答的问题是：</p><p>“多项目、多团队的交付效率与质量趋势如何？”<br/>“具体项目的交付瓶颈在哪里？”<br/>“组织级的研发效能度量该如何落地？”</p><p>下面会对上面提到的三种典型代表工具进行测评：</p><p><strong>（1）ONES：本地化一体化研发管理与效能度量</strong></p><p>先来测评一款国内的一体化研发管理平台。ONES 的核心定位就是一体化研发管理平台 + 研发效能度量模块，通过 Project / TestCase / Wiki 等模块管理需求、项目、缺陷、测试，再由 ONES Performance 统一抽取数据做研发效能分析。</p><p><strong>ONES 在四类指标上的能力：</strong></p><p>① 流动效率类研发效能度量</p><p>端到端 Lead Time、Cycle Time、WIP、吞吐量都可以基于工作项自然计算；<br/>支持按项目、团队、版本等维度分析流动效率变化。</p><p>② 质量与稳定性类研发效能度量</p><p>缺陷与需求、版本关联，支持按模块/版本做缺陷密度分析；<br/>与流水线 / 发布系统集成后，可以分析变更失败率等。</p><p>③ 价值与资源配置类研发效能度量</p><p>通过自定义字段区分需求类型，分析创新 / 优化 / 技术债等投入产出；<br/>配合项目群视图，支撑业务线层面的价值与资源审视。</p><p>④ 协作与团队健康类研发效能度量</p><p>利用看板、阻塞状态、依赖关系等字段，识别跨团队等待和插单情况；<br/>PMO 可基于这些数据组织项目级、组织级复盘。</p><p><strong>适合的团队与场景：</strong></p><ul><li>希望统一工具栈，打通从需求到交付的链路，统一“研发工作台”和“研发效能度量平台”；</li><li>对国产化、本地部署、安全合规有要求的组织；</li><li>希望在迭代会、项目会中直接使用平台视图，而不是额外导出报表；</li><li>需要 PMO、业务线负责人在统一视图下管理多项目、多团队效能。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446193" alt="图片" title="图片"/></p><p>配图：ONES 内置多种研发效能度量指标表</p><p><strong>（2）Jira Software：全球常见的敏捷项目管理与度量工具</strong></p><p>Jira 相信大家都不陌生，是海外都广泛使用的敏捷项目管理工具，支持 Scrum / Kanban 及基本的研发效能统计，并通过插件生态扩展工程效能、DORA 指标等。</p><p><strong>Jira 在关键指标上的表现：</strong></p><p>➀ 流动效率：</p><p>控制图、累计流图可以辅助分析 Cycle Time 和 WIP；<br/>若要实现端到端 Lead Time，需要结合外部系统（测试、发布等）和插件。</p><p>➁ 质量与稳定性：</p><p>缺陷趋势、版本质量可通过 Issue + Release 管理实现；<br/>更深入的 DORA 指标一般需要与 CI/CD 工具协作。</p><p>➂ 价值与资源：</p><p>通过自定义 Issue 类型和字段，可一定程度上做“需求类型 / 价值”维度分析，但更多依赖组织自建模型。</p><p><strong>适合的团队与场景：</strong></p><ul><li>已广泛部署 Atlassian 体系，团队成熟度较高；</li><li>具备较强流程治理能力，能在高自由度配置下统一研发效能度量口径。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446194" alt="图片" title="图片" loading="lazy"/></p><p>配图：Jira 产品组合链</p><p><strong>（3）Azure DevOps：偏“开发侧一体化”的度量能力</strong></p><p>Azure DevOps 主要以“代码 + 流水线 + Issue / Work Item + 测试”为核心，内置了 Value Stream、DORA 指标等工程向研发效能度量视图。</p><ul><li>通过 Boards、Repos、Pipelines、Tests 形成一体化 DevOps 平台；</li><li>提供 Lead Time / Cycle Time 控制图组件，直接展示工作项在流水线中的流动时间。</li></ul><p><strong>适合的团队与场景：</strong></p><p>工程实践成熟，对 CI/CD、自动化测试和持续部署投入较多；<br/>主要问题集中在“从提交到上线”的效率与稳定性，而不是需求塑造与价值回报。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446195" alt="图片" title="图片" loading="lazy"/></p><p>配图：Azure DevOps 产品图</p><h4>2. 工程效能分析平台 —— 深挖 Git / CI 的工程向研发效能度量</h4><p>这一类工具通常站在“工程管理”的视角，用 Git、CI/CD、Issue 等数据来做研发效能度量，主要度量 DORA 指标、PR Cycle Time、代码 churn、评审质量等，代表产品包括 Pluralsight Flow、LinearB、Jellyfish 等。</p><p><strong>（1）Pluralsight Flow（原 GitPrime）</strong></p><p>Pluralsight Flow 主要聚焦开发者行为与工程实践，分析提交习惯、重构比例、评审深度等，对“工程效率”“技术债管理”这类问题给出可视化研发效能度量。适合不改现有项目管理 / 需求工具，只希望在工程层面做更细致的指标洞察的团队。</p><p><strong>（2）LinearB</strong></p><p>LinearB 是典型的 DORA 指标与工程效能平台，强调 Cycle Time 拆解、部署频率、MTTR 等研发效能度量，常配合 GitLab / GitHub + CI 工具使用，作为“工程效能度量层”。</p><p><strong>适合场景：</strong></p><p>已有成熟 DevOps 流水线，短期不引入一体化管理平台；<br/>工程领导层希望用 DORA 指标推动工程实践改进。</p><p><strong>（3）Jellyfish</strong></p><p>Jellyfish 强调“工程投入与业务方向对齐”，分析研发资源在不同业务方向上的分布，一般会融合工程指标、团队健康度等维度，为高层提供决策视图。适合研发规模很大、业务线复杂，需要在高层视角回答“钱花在哪、产出如何”的公司。</p><p><strong>整体评价（工程效能平台）：</strong></p><p>在“流动效率 + 质量稳定性”两个方面的工程侧研发效能度量非常有价值；<br/>对需求价值、项目管理、组织治理等维度，需要与其他系统协同使用。</p><h4>3. 云厂商 DevOps 套件中的“效能洞察”</h4><p>这类产品通常作为云厂商 DevOps 套件的一部分，直接利用云上项目协作、代码、流水线、测试等数据来做研发效能度量。典型代表有阿里云云效效能洞察、腾讯云 CODING DevOps 效能洞察、华为云 CodeArts Board 等。</p><p><strong>（1）阿里云 云效效能洞察 Insight</strong></p><p>云效效能洞察是阿里云 BizDevOps 平台的高级服务，提供交付过程观测和研发效能度量，围绕项目、代码、流水线、质量等构建端到端指标体系。内置 90+ 场景化指标卡和模板化报表，覆盖项目度量、代码度量、流水线度量、质量保障、工作负荷管理等场景。</p><p>适用场景：研发活动主要在阿里云云效上进行，希望“云上工具 + 度量”一体化的团队。</p><p><strong>（2）腾讯云 CODING DevOps 效能洞察</strong></p><p>CODING 效能洞察专注 DevOps 全流程研发效能，通过 50+ 指标提供团队度量、项目度量、个人度量、质量 / 效率 / 价值与成本分析等视图。指标覆盖需求交付周期、缺陷修复周期、提交趋势、构建频率、部署成功率等。</p><p>适用场景：团队已经使用 CODING DevOps 做代码托管、流水线和项目协作，希望顺带接入研发效能度量。</p><p><strong>（3）华为云 CodeArts Board 效能洞察</strong></p><p>CodeArts Board 主要为企业管理者、项目经理、团队负责人等提供端到端的研发效能度量，从需求、缺陷、代码、构建、测试、部署、发布到运营进行全过程分析。内置 100+ 指标库，覆盖交付质量、交付效率、交付能力、交付成本、交付价值，并提供多角色“驾驶舱”。</p><p>适用场景：重度使用华为云 DevCloud / CodeArts 的企业，需要统一的“云上研发效能驾驶舱”。</p><p><strong>整体评价（云厂商路线）：</strong></p><p>在“流动效率 + 质量与稳定性”的指标上做得比较完整，也支持一定程度的价值与成本分析；</p><p>但度量对象较强绑定在云厂商生态，对多云 / 混合工具栈的组织，会有一定接入限制。</p><h4>4. 开源 + 自建 —— 以 Apache DevLake 为代表</h4><p>Apache DevLake 作为开源的 Dev 数据平台，支持接入 Jira、GitHub/GitLab、CI/CD 等多种数据源。内置 DORA 指标（部署频率、变更交付时间、变更失败率、恢复时间）以及大量研发效能度量指标，如需求 Lead Time、Bug Age、构建成功率、PR Cycle Time 等。</p><p><strong>在关键指标上的表现：</strong></p><ul><li>只要数据接得进来、模型建得好，前文提到的四类指标都可以覆盖；</li><li>灵活度高，可以精细化适配自身的研发流程与研发效能度量体系。</li></ul><p><strong>适用场景：</strong></p><ul><li>有数据团队、愿意自己维护数据平台的中大型技术公司；</li><li>工具栈高度异构，希望用统一的开源层打通数据、构建定制化研发效能度量体系。</li></ul><p><strong>优劣分析：</strong></p><ul><li>优势：指标丰富、可定制程度高，对“想深挖但不想受限于单一厂商”的团队非常友好；</li><li>局限：需要投入数据工程与运维成本；指标要想真正进入迭代与项目管理节奏，仍然需要与现有工具（包括 ONES、Jira 等）打通使用。</li></ul><h2>综合对比：哪条路径更适合哪类团队？</h2><p>接下来，我会站在“研发效能度量 + 组织阶段”的角度，把上面的工具做一个简要横评：</p><p><strong>1. 成长型团队（几十人规模以内）</strong> </p><p>诉求： 先建立基础研发效能度量意识，看到趋势即可。</p><p>更适合的路径：</p><ul><li>短期：Excel + 现有工具报表，选少量指标试水；</li><li>中期：选择一款易于落地的一体化平台（如 ONES 或 Azure DevOps / GitLab），把“工作 + 度量”慢慢迁到统一平台。</li></ul><p><strong>2. 多团队、多项目协作的中型组织 </strong></p><p>诉求： 统一口径、统一看板，让管理层对交付现状“看得见”。</p><p>更适合的路径：</p><ul><li>一体化研发管理平台（ONES、Jira + 部分插件、Azure DevOps / GitLab），作为日常协作的主平台；</li><li>如果已经重度云上，则可评估云厂商自带的“效能洞察”模块。</li></ul><p><strong>3. 工程文化成熟、DevOps 体系完善的大型工程团队 </strong></p><p>诉求： 精细化优化流水线效率、稳定性和工程实践。</p><p>更适合的路径：</p><ul><li>在现有 DevOps 工具链上叠加工程效能平台（Pluralsight Flow、LinearB、Jellyfish 等）；</li><li>核心指标更多聚焦：DORA、PR 周期、构建成功率、MTTR 等。</li></ul><p>同时建议： 保持一款一体化平台（或统一项目管理系统），承接需求与项目层面的研发效能度量。</p><p><strong>4. 已深度绑定某家云厂商的组织 </strong></p><p>诉求： 云上项目、代码、CI/CD 已集中，希望“一站式搞定度量”。</p><p>更适合的路径：</p><ul><li>优先评估当前云上的研发效能度量 / 效能洞察模块；</li><li>若后续需要更复杂的自定义分析，再考虑加一层 BI 或开源度量平台。</li></ul><p><strong>5. 有数据团队、强调数据主权和定制化的技术公司 </strong></p><p>诉求： 跨工具栈的统一研发效能度量体系，以及差异化的指标和算法。</p><p>更适合的路径：</p><ul><li>使用 Apache DevLake 等开源平台构建自有“研发数据湖”；</li><li>同时选用一款协作平台（如 ONES / Jira 等）承载日常工作，把数据汇总到数据湖中做统一分析。</li></ul><p>在这个视角下，每条路径都有它“最舒服”的位置，很难简单说谁是“最优解”。更现实的情况往往是：选择一个平台作为协作与研发效能度量的“主场”，再有选择地叠加工程效能平台或开源方案。</p><h2>用“指标思维”看工具，而不是用工具反推指标</h2><p>做研发效能度量工具横评，最终不是为了证明谁更好，而是为了回答三个简单的问题：</p><ul><li>我们真正关心哪些指标，这些指标能否确实推动交付改进？</li><li>这些指标，在哪个工具或路径上，产生和使用的成本最低？</li><li>在当前组织阶段，我们有多少资源，可以支撑哪种复杂度的方案？</li></ul><p>当你先用这样的“指标思维”看待工具，再去比较 ONES、Jira、工程效能平台、云厂商套件、开源方案时，就更容易找到适合自己团队的组合。</p>]]></description></item><item>    <title><![CDATA[NeurIPS2025公布最佳论文奖 L]]></title>    <link>https://segmentfault.com/a/1190000047446199</link>    <guid>https://segmentfault.com/a/1190000047446199</guid>    <pubDate>2025-12-03 15:04:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>NeurIPS2025公布最佳论文奖</h2><p>2025 年 11 月 26 日，<strong>NeurIPS（神经信息处理系统大会）</strong> 正式公布了 <strong>2025 年度最佳论文奖获奖名单</strong>。此次奖项由最佳论文评选委员会从会议主赛道及数据集与基准赛道中遴选产生，委员会成员经程序主席、数据集与基准赛道主席提名，由大会主席、下一代与可及性主席批准，均为机器学习各领域顶尖研究者。<strong>最终共有7 篇突破性论文获奖，</strong> 包括 <strong>4 篇最佳论文</strong> （含1 篇数据集与基准赛道专属获奖论文）和 <strong>3篇优秀论文（Runner-up）</strong>，覆盖生成模型理论、强化学习、大语言模型机制、学习理论等多个核心研究方向。</p><h3>最佳论文</h3><h4>1.《Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446202" alt=" " title=" "/></p><p><strong>核心贡献：</strong> 针对大语言模型（LLMs）生成内容缺乏多样性、可能导致人类思想同质化的问题，提出了大规模数据集 Infinity-Chat（含 2.6 万条真实开放域用户查询、3.125 万条人类标注），构建了首个开放域提示词综合分类体系（6 个顶级类别、17 个子类别）。通过对 70 余种模型的实证研究，揭示了"人工蜂群思维（Artificial Hivemind）" 效应 —— 模型内部存在重复生成倾向，且不同模型间输出高度同质化。同时发现现有 LLM、奖励模型及自动评判器难以匹配人类多样化偏好，为缓解 AI 安全风险提供了关键参考。</p><p><strong>评审评价：</strong> 填补了AI 评估中创意生成、主观偏好对齐等维度的研究空白，为 AI 系统异质性保护奠定了基础，树立了 "以科学认知和社会挑战为导向" 的数据集构建新标准。</p><h4>2.《Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446203" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 系统探究了门控机制对softmax 注意力的影响，通过在 150 亿参数混合专家（MoE）模型和 17 亿参数稠密模型（基于 3.5 万亿 token 数据集训练）上的 30 余种变体实验，发现 "在缩放点积注意力（SDPA）后添加头专属 sigmoid 门控" 的简单修改，可显著提升模型性能、训练稳定性及长上下文外推能力，同时缓解注意力 sink 问题。该机制的有效性源于引入非线性和查询依赖的稀疏门控分数，相关代码与模型已开源，并应用于 Qwen3-Next 系列模型。</p><p><strong>评审评价：</strong> 研究成果具备极强的可实施性，基于工业级计算资源完成的大规模验证为LLM 架构优化提供了可靠依据，开源行为对推动领域发展具有重要意义。</p><h4>3.《1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446204" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 挑战了强化学习（RL）难以训练深层网络的传统认知，提出了适用于自监督 RL 的深层网络构建方案。实验表明，将网络深度从传统的 2-5 层扩展至 1024 层，在无演示、无奖励的无监督目标条件设置下，可显著提升自监督对比 RL 算法在模拟移动和操作任务中的性能，不仅提高任务成功率，还能催生更复杂的学习行为。同时强调了批次大小缩放对深层网络对比 RL 的重要性。</p><p><strong>评审评价：</strong> 突破了RL 与深层网络结合的技术瓶颈，提出的范式简单易实施，为 RL 的规模化发展提供了新路径。</p><h4>4.《Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446205" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 揭示了扩散模型避免训练数据记忆、实现泛化的核心机制—— 隐式动态正则化。通过理论分析与实验验证，识别出两个关键训练时间尺度：早期为数据集无关的泛化阶段（模型生成高质量样本），后期为数据集大小依赖的记忆阶段（训练超过该阶段会出现记忆现象）。其中泛化阶段时长随训练集规模线性增长，记忆阶段时长保持恒定，这一特性使模型在过参数化场景下仍能有效泛化。</p><p><strong>评审评价：</strong> 通过随机矩阵理论将实证观察与形式化理论统一，为生成式AI 的泛化机制研究树立了分析深度标杆，提供了可落地的训练指导。</p><h3>入围论文</h3><h4>1.《Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446206" alt=" " title=" " loading="lazy"/></p><p><strong>核心发现：</strong> 对"带可验证奖励的强化学习（RLVR）能赋予 LLM 全新推理能力" 的主流假设提出质疑。通过在多模型家族、多算法、多基准（数学、编程、视觉推理）上的系统测试，发现 RLVR 仅提升小 k 值下的 pass@k 分数（抽样效率），但无法激发新的推理模式 ——RLVR 模型的推理路径均包含在基础模型的抽样分布中，且训练会缩小推理能力边界；而蒸馏技术反而能引入新推理模式。</p><p><strong>评审评价：</strong> 该批判性发现具有重要学术价值，为推动RL 范式创新（如持续缩放、多轮智能体 - 环境交互）提供了明确方向。</p><p><a href="https://link.segmentfault.com/?enc=Y1eLDefiX4SntjT3qGwCxA%3D%3D.bIxsW729CxAy6hVLdWZ6ywzMmCAunddwlK170IiTk4jpXiOkZP3SRvEEFHjA%2BFPFYX46YFTWocP%2BZUaoh5j6F60tE%2FKzGsZPpMzaD1NTzV51dpuj65qTIFAV4yoCFYqqkDpo%2BIymMgNDAoQVW%2FhlvD48u5Ik4lIE97SBBegADpg%3D" rel="nofollow" target="_blank">👉一键Lab4AI阅读</a></p><h4>2. 《Optimal Mistake Bounds for Transductive Online Learning》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446207" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 解决了持续30 年的在线学习领域开放问题，精准量化了转导式在线学习与标准在线学习的性能差距。证明了对于 Littlestone 维度为 d 的概念类，转导式错误边界至少为 Ω(√d)，且该边界是紧的（存在对应概念类达到此边界），较此前的对数级下界实现指数级提升。同时改进了上界结果，揭示了转导式学习利用未标记数据可实现二次级性能提升，这与 PAC 设置下两者样本复杂度相近的特性形成鲜明对比。</p><p><strong>评审评价：</strong> 证明方法兼具创新性与严谨性，通过"路径树" 结构、稀疏编码、危险区域最小化等多种技术的融合，构建了最优学习算法，是学习理论领域的突破性成果。</p><h4>3. 《Superposition Yields Robust Neural Scaling》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446208" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 提出表征叠加（LLM 表征的特征数超过维度）是神经缩放定律的核心驱动因素。基于 Anthropic 玩具模型的实验表明，弱叠加状态下，损失仅在数据特征频率呈幂律分布时遵循幂律缩放；而强叠加状态下，得益于表征向量的几何重叠，损失在广泛频率分布中均与模型维度呈逆幂律缩放。开源 LLM 的实证结果及 Chinchilla 缩放定律均验证了这一结论。</p><p><strong>评审评价：</strong> 超越了对神经缩放定律的单纯观察，深入揭示其内在机制，为优化缩放效果、预测缩放极限提供了关键理论支撑。</p><p>NeurIPS 2025的最佳论文奖项不仅表彰了在各自领域做出突破性贡献的研究，也反映了当前机器学习社区对可解释性、安全性、多样性及理论根基的日益重视。这些工作既有扎实的理论突破，也有影响深远的实践指导，预计将对未来的研究方向和业界实践产生重要影响。</p><p><a href="https://link.segmentfault.com/?enc=p593IsjAK5TxMOEFC1zHpg%3D%3D.GiDmgT5xv4vkYflsDj3JRU2TiqGsjka7fGGBy%2BnEmnZsm0TsrL%2B1ZKQlzgM7zSein9Shf5BPMPmZ4NoBuU9N0NbFIwvgFpXA5W6sMzPTucZyK6AjctWkhwI6PFubPjqo" rel="nofollow" target="_blank">👉参考链接</a></p><p>本文系学术转载，如有侵权，请联系大模型实验室Lab4AI小助手删文</p><h3>Lab4AI支撑“从研究到落地”</h3><p>大模型实验室Lab4AI实现算力与实践场景无缝衔接，<strong>具备充足的H卡算力</strong>，支持模型复现、训练、推理全流程使用，且具备灵活弹性、按需计费、低价高效的特点，解决用户缺高端算力、算力成本高的核心痛点。</p><p>Lab4AI.cn提供实验平台，提供一站式科研工具链！<br/><a href="https://link.segmentfault.com/?enc=kD3aPygybSVbgLLlDLA%2B%2BQ%3D%3D.itzBQ5HuuOBdFi0rV%2BND6c%2BZNNg%2BUwfrFECZE5Qx%2FWLbvm3MBB9e%2Fy4j5srXY6DV747RiV7XLDZcfJOc0%2Bpr4%2FZbprxGU0V8zQLKzvhxqeo%3D" rel="nofollow" target="_blank">👉一键直达</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413855" alt=" " title=" " loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[#智慧文旅：智能体系介绍—多场景管理 智]]></title>    <link>https://segmentfault.com/a/1190000047446229</link>    <guid>https://segmentfault.com/a/1190000047446229</guid>    <pubDate>2025-12-03 15:03:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="477" referrerpolicy="no-referrer" src="/img/bVdne5Q" alt="" title=""/></p><h2>一、概述</h2><p>    构建景区多维度、立体化智慧管理体系，通过部署客流监测系统实现精准调度，运用AI视频监控强化安全保障，依托智慧停车系统优化交通动线，借助信息发布与广播系统提升服务触达效率，结合舆情监测构建风险预警机制；同时创新引入AR/VR虚拟漫游、云直播导览等沉浸式体验场景，打造全景可视化数字空间，实现物理景区与数字孪生的深度融合。该体系不仅显著提升景区实时管控能力和应急处置水平，更通过科技赋能创造多维互动体验，有效延长游客驻留时间，为景区精细化运营提供数据支撑，全面推动传统景区向智慧化、数字化服务模式的转型升级。</p><h2>二、智慧文旅部分多场景管理系统介绍</h2><h3>1、景区客流监测系统</h3><p>    景区客流监测系统提供全面的客流分析与管理功能，涵盖客流趋势监测、用户特征与停留时长分析、游客出行方式统计、区域流量超限预警以及高频路径识别，为景区运营与安全管控提供数据支持。<br/><img width="723" height="410" referrerpolicy="no-referrer" src="/img/bVdne4m" alt="" title="" loading="lazy"/></p><h3>2、景区公共广播系统</h3><p>    景区公共广播系统是一种专为旅游景区设计的音频传输和管理系统，旨在通过背景音乐播放、实时通知和紧急广播等功能，提升游客体验并确保安全。<br/><img width="723" height="327" referrerpolicy="no-referrer" src="/img/bVdne4o" alt="" title="" loading="lazy"/></p><h3>3、景区信息发布系统</h3><p>    景区信息发布系统是一种集成化的信息管理与传播平台，系统通过电子显示屏等多种渠道，实时向游客和管理人员发布包括但不限于景区开放时间、票务信息、天气预报、客流量预警、活动安排、游览路线等景区相关信息。旨在提升游客体验，确保信息透明度和实时性，辅助景区进行有效的运营管理。</p><p><img width="723" height="333" referrerpolicy="no-referrer" src="/img/bVdne4x" alt="" title="" loading="lazy"/></p><h3>4、景区舆情管理系统</h3><p>    系统以文旅领域的舆论态势感知为依托，从多维度分析出工作层面、推行层面的游客反馈情况，协助相关负责人快速感知突发事件，利用预警等方式提示景区相关的潜在舆情风险，反映景区内的舆情整体情况。</p><p>    对微博、公众号、贴吧、论坛、博客、APP、报纸、小视频等主流互联网平台及各地地方网站，7*24小时不间断全网跟踪，覆盖上百万优质信源，实时更新舆情动态，具备对数据进行存储、检索、指标构建、业务建模以及可视化分析能力。</p><p>    构建可监测景区整体运行状态的，并可支撑景区规划实施改进的指标数据模型，可基于NLP情感识别算法进行舆论情感判断，根据实际场景，持续优化数据处理和建模分析，不断提升数据模型的性能，统计分析更精准。</p><p>    集成景区相关公开大数据，基于NLP情感识别算法，对舆论分析形成全套解决方案，打造一站式景区相关的数据云屏解决方案，满足相关部门实时大屏监控需求。</p><h3>5、景区智慧停车系统</h3><p>    运用无线通信技术、移动终端技术、GPS定位技术、GIS技术等综合应用于城区及景区停车位的采集、管理、查询、预订与导航服务，实现停车位资源的实时更新、查询、预订与导航服务一体化，实现停车位资源利用率的最大化、停车场利润的最大化和车主停车服务的最优化。为市民、游客的出行提供快速引导停车、实时停车场信息服务，改善局部交通微循环、缓解交通拥堵，有效解决停车难、管理难、监管难等问题。</p><p><img width="452" height="302" referrerpolicy="no-referrer" src="/img/bVdne4F" alt="" title="" loading="lazy"/><img width="550" height="394" referrerpolicy="no-referrer" src="/img/bVdne4G" alt="" title="" loading="lazy"/><img width="324" height="552" referrerpolicy="no-referrer" src="/img/bVdne4J" alt="" title="" loading="lazy"/></p><h3>6、景区视频监控及AI智能分析系统</h3><p>    景区视频监控及AI智能分析系统通过线路规划、可疑人员与异常行为预警、景区寻人、展品热度分析、客流与火灾预警、以及车辆识别等功能，利用大数据及智能分析技术，全面提升景区的安全管理、游客服务与运营决策能力。<br/><img width="723" height="345" referrerpolicy="no-referrer" src="/img/bVdne4N" alt="" title="" loading="lazy"/></p><h3>7、景区全景漫游系统</h3><p>    建设720°全景漫游应用，以视频、音乐、动画等多种形式的展现，将景区内多元化旅游信息、景点信息以更加生动活跃的形式呈现给游客；</p><p>    “识别一下”获取信息；多种创意，游览不再枯燥”，便捷地获取游览信息，提升游客景区体验度。</p><p><img width="436" height="495" referrerpolicy="no-referrer" src="/img/bVdne4R" alt="" title="" loading="lazy"/></p><h3>8、景区数字文旅创新应用系统</h3><p>    景区数字文旅创新应用系统通过VR虚拟全景视频提供沉浸式视觉体验，利用AR技术实现虚实结合的互动游览，并借助云视频微直播服务让游客远程了解景区实况，全方位提升游客的数字化旅游体验。<br/><img width="355" height="350" referrerpolicy="no-referrer" src="/img/bVdne45" alt="" title="" loading="lazy"/><img width="355" height="343" referrerpolicy="no-referrer" src="/img/bVdne46" alt="" title="" loading="lazy"/><img width="361" height="361" referrerpolicy="no-referrer" src="/img/bVdne5b" alt="" title="" loading="lazy"/></p><h2>三、往届回顾</h2><p>    <a href="https://segmentfault.com/a/1190000047392511" target="_blank">智慧文旅整体解决方案：赋能景区智能升级，激活全域营销势能</a></p><p>    <a href="https://segmentfault.com/a/1190000047395646" target="_blank">#数字人不止于“对话”，更在赋能千行百业</a></p><p>    <a href="https://segmentfault.com/a/1190000047414536" target="_blank">智慧文旅景区数字化中枢—“旅商通”，整合票务、二销与客流</a></p><p>    <a href="https://segmentfault.com/a/1190000047429281" target="_blank">#智慧文旅：旅政通，打通文旅数据壁垒，构建一体化运营平台</a></p><h2>四、下篇预告-#智慧文旅：独立应用介绍—多业态管理</h2><p>    #智慧文旅多业态管理系统：是一套专门为旅行社设计的在线服务平台，它允许旅行社便捷地为团队游客批量预订景区门票，支持实时查询可用票量、在线支付、即时获取电子票据等功能，简化旅行社在景区门票预订、购买和管理过程中的各个环节，同时还能与景区的客流管理系统对接，帮助旅行社高效管理团队行程，提升客户体验。</p><h2>五、软件结构</h2><p>    本软件采用的是uniapp+JAVA语言开发，编码规范完全按照阿里巴巴编码规范<br/>    移动端：采用 uni-app 方案，一份代码多终端适配，同时支持 APP、小程序、H5；<br/>    前端采用Vue、Element UI。<br/>    后端采用Spring Boot多模块架构、Spring Security、Redis &amp; Jwt。<br/>    权限认证使用Jwt，支持多终端认证系统。</p>]]></description></item><item>    <title><![CDATA[比提示词更重要！3个技巧教你用好AI的「]]></title>    <link>https://segmentfault.com/a/1190000047446252</link>    <guid>https://segmentfault.com/a/1190000047446252</guid>    <pubDate>2025-12-03 15:02:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>引言：别再迷信“神级提示词”</h2><p>很多人用 AI 大模型时，总觉得如果 AI 回答不好，是因为自己的“咒语”（提示词）念得不对。大家拼命在网上找各种“神级提示词模板”，试图用一句话解决所有问题。</p><p>但其实，<strong>决定 AI 回答质量的，往往不是你输入的那一句话，而是多轮对话的过程「上下文」（Context）。</strong></p><p>我们在实际使用 AI 的过程中，不可能一句提示词就能得到我们想要的东西，而是通过多轮对话，不断调整才能唠出好东西。</p><h2>一、 上下文 VS 提示词：一次性指令与连续对话</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446254" alt="" title=""/></p><p>我们最容易犯的错误，就是把 AI 当成搜索引擎用：问一句，回一句，用完即走。</p><p>但大模型最厉害的地方在于​<strong>多轮对话</strong>​。</p><ul><li><strong>提示词（Prompt）：</strong> 就像是你对 AI 说的一句指令，一次性的。</li><li><strong>上下文（Context）：</strong> 是你们之前聊过的所有内容。它是连贯的、持续的。</li></ul><p><strong>如果把和 AI 聊天比作“带实习生”：</strong></p><p>提示词只是你随口吩咐的一个任务；带过人的都知道，一句话根本搞不定实习生。</p><p>而上下文，是你给实习生看的所有过往项目资料、会议记录和操作手册。资料越全、背景越清晰，实习生干活越靠谱。</p><h2>二、 原理揭秘：AI 其实是个“健忘症”</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446255" alt="" title="" loading="lazy"/></p><p>你以为 AI 像人一样，聊久了就有感情、有记忆？NONONO\~</p><p><strong>本质上，AI 是一个没有任何记忆的“文字接龙机器”。</strong></p><p>当你发出第 10 句话时，AI 并不是“记住了”前 9 句话。实际上，系统是在后台把你这 10 句话打包，一股脑儿全塞给 AI。AI 必须以极快的速度，把这 10 句话​<strong>从头到尾重读一遍</strong>​，假装自己记得，然后预测出第 11 句话。</p><p><strong>上下文，就是 AI 每次回答前必须复习的“临时小抄”。</strong></p><p>没有这个小抄，AI 瞬间就会失忆，连你是谁、刚才聊了什么都会忘得一干二净。</p><blockquote><p>LLM 运作方式确实是 <strong>Autoregressive Generation</strong> (自回归生成) 或 <strong>Next-Token Prediction</strong> (下一个词元预测)，即基于整个输入序列（含对话历史）来决定下一个输出。</p><p><a href="https://link.segmentfault.com/?enc=loTrNcd2IquNhTK5u%2B6Ndg%3D%3D.ICB%2BPPrSzj9AvzxHSefHGqK7oBRayMg2d8ZwARtzzi3FPM0BnpEl1QiH9nzim%2BFk" rel="nofollow" target="_blank">《Attention Is All You Need》| ArXiv</a></p></blockquote><h2>三、 警惕！上下文的“三大缺陷”</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446256" alt="" title="" loading="lazy"/></p><p>既然上下文是“小抄”，那是不是抄得越多越好？并不是。目前的 AI 模型在处理上下文时，有三个明显的短板：</p><p><strong>1. 内存有限：</strong> 这个“小抄”是有字数上限的。如果你给的资料太多，超过了它的处理极限，最早的聊天记录就会被强行“挤”出去。就像一个装满水的杯子，新水倒进去，旧水就流出来了。</p><p><strong>2. 三明治效应：</strong> 研究发现，AI 对上下文的记忆主要集中在​<strong>开头</strong>​（最早的设定）和​<strong>结尾</strong>​（你最新的提问）。如果你把关键信息塞在几万字的对话中间，AI 大概率会记不住的。</p><blockquote><p>述了 LLM 在处理长上下文时，对开头（Primacy Bias/首因效应）和结尾（Recency Bias/近因效应）的信息召回率最高，而中间部分最差的现象。</p><p><a href="https://link.segmentfault.com/?enc=wGLBewONA%2FSmNX5jt1IHgw%3D%3D.ZT2wNHOtaREsYLHpulzWuCECpexCzi0rl4QyKgNbGPwUS%2FJsZIy83JUGQmhRvIgt" rel="nofollow" target="_blank">《Lost in the Middle: How Language Models Use Long Contexts》| ArXiv</a></p></blockquote><p><strong>3. 信息噪声：</strong> 给的信息越杂，AI 越容易晕。如果你在对话里塞了一堆无关的文档、乱七八糟的要求，AI 就会“幻觉”，开始胡言乱语。这叫“噪声中毒”。</p><blockquote><p><strong>​ ​</strong>In-Context Learning (ICL) 对输入噪声（如不相关或错误的演示示例）的敏感性。不相关的文档或指令会分散模型的注意力，增加 Hallucination (幻觉) 的风险。</p><p><a href="https://link.segmentfault.com/?enc=6H8DTqSobGnMTVpkeFoTbA%3D%3D.cRVd8C3H%2FwF9U5KR7BamR0krpTMKqDby5K5XjS12citf2gjrjTMfS5Cx0YLFcYha" rel="nofollow" target="_blank">《On the Noise Robustness of In-Context Learning for Text Generation》|ArXiv</a></p></blockquote><h2>四、 高手秘籍：3 招用好上下文</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446257" alt="" title="" loading="lazy"/></p><p>理解了原理，我们就能对症下药。普通人只需掌握这 3 个技巧，就能大幅提升 AI 的智商。</p><h3>技巧 1：一个对话只聊一个话题</h3><p><strong>不要在一个对话框里聊所有事情。</strong></p><ul><li><strong>错误做法：</strong> 上一秒让 AI 写代码，下一秒问它红烧肉怎么做，接着又让它翻译论文。这会导致“小抄”里充满了无关信息，干扰 AI 判断。</li><li><strong>正确做法：</strong> <strong>新建对话（New Chat）。</strong> 写代码开一个窗口，写文案开一个窗口。每次点击“新建对话”，就相当于给 AI 换了一本崭新的、干净的“小抄”，让它没有任何负担地开始工作。</li></ul><h3>技巧 2：开局“立规矩”，重要信息全部说完</h3><p>既然 AI 有“三明治效应”，记得住开头，那我们就把最重要的信息放在<strong>第一条提示词</strong>里。</p><p>在对话开始的第一次输入中，你要明确告诉 AI：</p><ul><li>​<strong>你是谁</strong>​（角色设定，如：你是一个资深翻译）</li><li>​<strong>你要干什么</strong>​（核心任务）</li><li>​<strong>有什么限制</strong>​（不要废话，只输出结果）</li></ul><p>把地基打牢了，后面的楼才不会歪。不要等到聊了一半，才想起来补充核心要求。</p><h3>技巧 3：定期“敲黑板”，重复关键指令</h3><p>如果你的对话很长（比如让 AI 写长篇小说或处理超长文档），聊到后面 AI 开始跑题、变笨了，怎么办？</p><p><strong>不要骂它，它只是忘了，或者搞不清楚重点了。</strong></p><p>你需要充当“课代表”，在对话中途​<strong>总结一下之前的进度</strong>​，或者​<strong>把最开始的要求再发一遍</strong>​。</p><p><strong>话术示例：</strong></p><blockquote>“为了避免你忘记，我重申一下我们的目标是 XXX，刚才我们已经完成了 YYY，接下来请继续做 ZZZ。”</blockquote><p>这相当于强行把关键信息再次写到“小抄”的末尾（最新位置），强迫 AI 重新聚焦。</p><p><strong>总结一下：</strong> 用好 AI，不要只纠结由于那一句“提示词”写得够不够 NB。<strong>管理好「上下文」，保持对话环境的纯净、关键信息的突出，才是让 AI 持续输出高质量内容的秘诀。</strong></p>]]></description></item><item>    <title><![CDATA[生产研发管理的“三环模型”：技术、流程、]]></title>    <link>https://segmentfault.com/a/1190000047446270</link>    <guid>https://segmentfault.com/a/1190000047446270</guid>    <pubDate>2025-12-03 15:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>是什么：研发管理正在经历系统性重构<br/>制造业研发管理本质上是一个涉及数据、流程与协作的复杂系统工程。传统模式下，企业常面临设计资料分散存储、版本管理依赖人工命名、BOM数据准确率低、跨部门协作效率低下等问题。这些痛点不仅制约研发效率，更直接影响创新能力和市场响应速度。<br/>现代研发协同平台通过系统化重构研发流程，将需求导入、设计开发、评审决策和生产准备整合为统一数字环境。此类平台的核心价值在于打破信息孤岛，构建连续、透明的数据流，使研发管理从“断裂式”运作转向“一体化”协同。<br/>怎么做：从数据协同到智能预防的三层进化<br/>研发数字化需通过分层建设实现渐进式突破，具体可分为以下三层：<br/>第一层：研发数据中枢建设<br/>通过产品数据管理（PDM）系统实现全生命周期数据标准化，零部件从创建到报废形成完整数字履历。系统支持自动审批流程与移动端处理，显著提升审批效率。例如，某企业通过智能相似性检测功能，将零部件复用率提高35%，大幅减少重复设计资源浪费。<br/>第二层：三维协同轻量化应用<br/>借助三维协同工具，直接转换和Web端查看多种CAD格式，生产人员可通过扫码快速查看工序模型，质量人员在线完成评审。实际应用中，某企业模型查看效率提升60%，数据准备时间减少75%，有效压缩新产品试制周期。<br/>第三层：智能质量预防体系<br/>通过失效模式与影响分析（FMEA）系统，基于历史问题库自动分析潜在失效原因并推荐措施，在研发前期识别超80%的潜在风险。某工程机械企业应用后，FMEA编制效率提升20%，实现全自动表单生成，推动质量管理从事后补救转向事前预防。<br/>案例：广域铭岛助力制造企业实现研发跃迁<br/>广域铭岛作为工业互联网实践者，其Geega捷做平台已在家电、汽车零部件等行业验证价值。某白电企业通过平台实现用户需求与产品设计的直接对接，设计变更效率提升50%，产品上市周期缩短30%。平台的三维轻量化功能支持销售部门直接调取模型向客户展示，形成“设计即销售”的新模式。<br/>在汽车零部件领域，某变速箱生产企业借助平台建立统一数据中枢，BOM准确率达98%，并通过FMEA模块提前识别20余个设计风险，避免量产质量事故。企业反馈：“系统正在帮我们思考。”<br/>研发数字化转型通过平台化工具实现知识沉淀、流程优化与协同创新的一体化推进，助力企业从经验驱动转向数据驱动，在市场竞争中赢得先机。</p>]]></description></item><item>    <title><![CDATA[为何前端圈现在不关注源码了？ 悲伤的煎鸡]]></title>    <link>https://segmentfault.com/a/1190000047446305</link>    <guid>https://segmentfault.com/a/1190000047446305</guid>    <pubDate>2025-12-03 15:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>开始</h3><p>大家有没有发现一个现象：最近 1-2 年，前端圈不再关注源码了。</p><p>最近 Vue3.6 即将发布，alien-signal 不再依赖 Proxy 可更细粒度的实现响应式，vapor-model 可以不用 vdom 。</p><p>Vue 如此大的内部实现的改动，我没发现多少人研究它的源码，我日常关注的那些博客、公众号也没有发布源码相关的内容。</p><p>这要是在 3 年之前，早就开始有人研究这方面的源码了，博客一篇接一篇，跟前段时间的 MCP 话题一样。</p><p>还有前端工具链几乎快让 Rust 重构一遍了，rolldown turbopack 等产品使得构建效率大大提升。这要是按照 3 年之前对 webpack 那个研究态度，你不会 rust 就不好意思说自己是前端了。</p><p>不光是这些新东西，就是传统的 Vue React 等框架源码现在也没啥热度了，我关注每日的热门博客，几乎很少有关于源码的文章了。</p><p>这是为什么呢？</p><p><img width="583" height="388" referrerpolicy="no-referrer" src="/img/bVdne6C" alt="" title=""/></p><h3>泡沫</h3><p>看源码，其实是一种泡沫，现在破灭了。所谓泡沫，就是它的真实价值之前一直被夸大，就像房地产泡沫。</p><p>前几年是互联网发展的红利期，到处招聘开发人员，大家都拿着高工资，随便跳槽就能涨薪 20% ，大家就会误以为真的是自己的能力值这么多钱。</p><p>而且，当年面试时，尤其是大公司，为了筛选出优秀的候选人（因为培训涌入的人实在太多），除了看学历以外，最喜欢考的就是算法和源码。</p><p>确实，如果一个技术人员能把算法和源码看明白，那他肯定算是一个合格的程序员，上限不好说，但下限是能保证的。就像一个人名牌大学毕业的，他的能力下限应该是没问题的。</p><p>大公司如此面试，其他公司也就跟风，面试题在网络上传播，各位程序员也就跟风学习，很快普及到整个社区。</p><p>所以，如果不经思考，表面看来：就是因为我会算法、会源码，有这些技能，才拿到一个月几万甚至年薪百万的工资。</p><p>即，源码和算法价值百万。</p><h3>坑位</h3><p>技术大厂，前端-后端-测试，新一线和一二线城市等地均有<a href="https://link.segmentfault.com/?enc=2xfQ%2BnwRWfNwk4EO%2BQ2ceA%3D%3D.8wil8kDL2Nq9IpPlwQ1JHBk0GOZg0zXOg%2BpZOI%2B3LV0%3D" rel="nofollow" target="_blank">坑位</a>，感兴趣可以试试。待遇和稳定性都不错~</p><p><strong>现状</strong></p><p>现在泡沫破灭了。业务没有增长了，之前是红利期，现在是内卷期，之前大量招聘，现在大量裁员。</p><p>你看这段时间淘宝和美团掐架多严重，你补贴我补贴，你广告我也广告。如果有新业务增长，他们早就忙着去开疆拓土了，没公司在这掐架。</p><p>面试少了，算法和源码也就没有发挥空间了。关键是大家现在才发现：原来自己会算法会源码，也会被裁员，也拿不到高工资了。</p><p>哦，原来之前自己的价值并不是算法和源码决定的，最主要是因为市场需求决定的。哪怕我现在看再多的源码，也少有面试机会，那还看个锤子！</p><p>现在企业预算缩减，对于开发人员的要求更加返璞归真：降低工资，甚至大量使用外包人员代替。</p><p>所以开发人员的价值，就是开发一些增删改查的日常 web 或 app 的功能，什么算法和框架源码，真实的使用场景太少。</p><p><strong>看源码有用吗？</strong></p><p>答案当然是肯定的。学习源码对于提升个人技术能力是至关重要的，尤其是对于初学者，学习前辈经验是个捷径。</p><p>但我觉得看 Vue react 这些源码对于开发提升并不会很直接，它也许会潜移默化的提升你的“内功”，但无法直接体现在工作上，除非你的工作就是开发 Vue react 类的框架。</p><p>我更建议大家去看一些应用类的源码，例如 UI 组件库的源码看如何封装复杂组件，例如 vue-admin 看如何封装一个 B 端管理后台。</p><p>再例如我之前学习 AI Agent 开发，就看了 langChain 提供的 agent-chat-ui 和 Vercel 提供的 ai-chatbot 这两个项目的源码，我并没有直接看 langChain 的源码。</p><p>找一些和你实际开发工作相关的一些优秀开源项目，学习他们的设计，阅读他们的源码，这是最直接有效的。</p><p>——转载自：前端双越老师</p>]]></description></item><item>    <title><![CDATA[用开源模型强化你的 OCR 工作流 Hu]]></title>    <link>https://segmentfault.com/a/1190000047445798</link>    <guid>https://segmentfault.com/a/1190000047445798</guid>    <pubDate>2025-12-03 14:05:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>我们在这篇文章中新增了 <a href="https://link.segmentfault.com/?enc=M065tpC5Pr4H4YHyBHX6jg%3D%3D.GMSErAQFH26E9M6FIs90rZzEUM9NPJfx5wXw5ZonqpLs12mrUt7DUOKG0WehvvdY" rel="nofollow" target="_blank">Chandra</a> 和 <a href="https://link.segmentfault.com/?enc=UK6uYrxSQ5hFaCpdZVmuTA%3D%3D.GOafolm%2BcoIpG3PmFJcOgXbigshoh3h4ZTZwkujK9yi9dUR1Y%2Fts5Tr%2FxVKg%2BCEq" rel="nofollow" target="_blank">OlmOCR-2</a>，并附上了它们在 OlmOCR 基准上的得分 🫡</blockquote><p><strong>摘要:</strong>  <br/>强大的视觉语言模型 (Vision-Language Models, VLMs) 的崛起，正在彻底改变文档智能 (Document AI) 的格局。每种模型都有其独特的优势，因此选择合适的模型变得棘手。相比闭源模型，开源权重的模型在成本效率和隐私保护上更具优势。为了帮助你快速上手，我们整理了这份指南。</p><p>在本指南中，你将了解到:</p><ul><li>当前 OCR 模型的整体格局及其能力</li><li>何时需要微调模型，何时可直接使用</li><li>为你的场景选择合适模型时应考虑的关键因素</li><li>如何超越传统 OCR，探索多模态检索与文档问答</li></ul><p>读完之后，你将知道如何选择合适的 OCR 模型、开始构建应用，并对文档 AI 有更深入的理解。让我们开始吧！</p><h2>现代 OCR 简介</h2><p>光学字符识别 (Optical Character Recognition，简称 OCR) 是计算机视觉领域最早、也是持续时间最长的研究方向之一。AI 的许多早期实际应用都集中在“将印刷文字转化为可编辑的数字文本”上。</p><p>随着 <a href="https://link.segmentfault.com/?enc=n5H4cKcPk0WDub7Cf0v6hQ%3D%3D.4odlmBYRYdDAUdbvClR3DDSL38ph%2F%2FoYsyN4%2B7PSwlc%3D" rel="nofollow" target="_blank">视觉语言模型 (Vision-Language Models, VLMs)</a> 的兴起，OCR 的能力迎来了飞跃式提升。如今，许多 OCR 模型都是在现有 VLM 的基础上进行微调得到的。但现代模型的能力已远超传统 OCR —— 你不仅可以识别文字，还能基于内容检索文档，甚至直接进行问答。</p><p>得益于更强大的视觉理解能力，这些模型能处理低质量扫描件、理解复杂元素 (如表格、图表、图片等) ，并将文本与视觉内容融合，以回答跨文档的开放式问题。</p><h3>模型能力</h3><h4>文本识别</h4><p>最新的模型能够将图像中的文字转录为机器可读格式。输入内容可能包括:</p><ul><li>手写文字</li><li>各类文字体系 (如拉丁文、阿拉伯文、日文等)</li><li>数学公式</li><li>化学方程式</li><li>图片、版面或页码标签</li></ul><p>OCR 模型会将这些内容转换为机器可读的文本，输出格式多种多样，比如 <strong>HTML、Markdown</strong> 等。</p><h4>处理文档中的复杂组件</h4><p>除了文字，某些模型还能识别:</p><ul><li>图片</li><li>图表</li><li>表格</li></ul><p>部分模型能识别文档中图片的精确位置，提取其坐标，并在输出中将图片嵌入对应位置。<br/>另一些模型还能为图片生成说明文字 (caption) ，并在适当位置插入。这对于后续将机器可读输出传入 LLM (大型语言模型) 尤为有用。</p><p>例如， <a href="https://link.segmentfault.com/?enc=u91LbfAAMbUZDdfN3dAhYg%3D%3D.Ev3cKOhZnhAJNjRHqv4U5qJ9XkjIUtwEcJhUzjVU0vWtT%2FKuuMWWZBqxzA3rWK4o" rel="nofollow" target="_blank">OlmOCR (AllenAI 出品)</a>  和  <a href="https://link.segmentfault.com/?enc=7wOssW3%2BlogIc%2BGvjqor4w%3D%3D.WjAmg0Gpf%2BOCYG6rf5u0sQuDPe0d9QHypeGy0R%2FqR6G4oml%2BglC1LvyPJVz9fMbf" rel="nofollow" target="_blank">PaddleOCR-VL (PaddlePaddle 出品)</a>  就是代表。</p><p>不同模型使用不同的输出格式，例如 <strong>DocTags</strong>、<strong>HTML</strong>、<strong>Markdown</strong> (后文<strong>输出格式</strong>一节有详细说明) 。<br/>模型处理表格与图表的方式通常取决于所采用的输出格式:</p><ul><li>有些模型将图表当作图片直接保留；</li><li>有些模型则会将其转换为可解析的结构化格式，如 Markdown 表格或 JSON。<br/>例如，下图展示了一个柱状图如何被转换成机器可读的形式:</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445816" alt="" title=""/></p><p>同样地，表格中的单元格也会被解析为机器可读格式，并保留列名与标题的上下文关系: </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445817" alt="" title="" loading="lazy"/></p><h4>输出格式</h4><p>不同 OCR 模型采用的输出格式不同，以下是几种主流格式的简介:</p><ul><li><strong>DocTag:</strong> 一种类似 XML 的文档标记格式，可表达位置信息、文本样式、组件层级等。下图展示了一篇论文如何被解析为 DocTags。该格式由开源的 Docling 模型使用。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445818" alt="" title="" loading="lazy"/></p><ul><li><strong>HTML:</strong> 是最常见的文档解析格式之一，能较好地表达结构与层级信息。</li><li><strong>Markdown:</strong> 人类可读性最强，格式简洁，但表达能力有限 (如无法准确表示多列表格) 。</li><li><strong>JSON:</strong> 通常用于表示表格或图表中的结构化信息，而非完整文档。</li></ul><p>选择合适的模型，取决于你对输出结果的用途:</p><table><thead><tr><th>目标场景</th><th>推荐格式</th></tr></thead><tbody><tr><td><strong>数字化重建</strong> (重现原始文档版式)</td><td>使用保留布局的格式，如 DocTags 或 HTML</td></tr><tr><td><strong>LLM 输入或问答场景</strong></td><td>使用输出 Markdown 和图像说明的模型 (更接近自然语言)</td></tr><tr><td><strong>程序化处理</strong> (如数据分析)</td><td>选择能输出结构化 JSON 的模型</td></tr></tbody></table><h4>OCR 的位置感知</h4><p>文档常常结构复杂，比如多栏文本、浮动图片、脚注等。早期的 OCR 模型通常先识别文字，再通过后处理手动推断页面布局，以恢复阅读顺序——这种方式既脆弱又易错。</p><p>现代 OCR 模型则会在输出中直接包含版面布局信息 (称为 <strong>“锚点”或 “grounding”</strong>) ，如文字的边界框 (bounding box) 。<br/>这种“锚定”机制能有效保持阅读顺序与语义连贯性，同时减少“幻觉式识别” (即错误生成内容) 。</p><h4>模型提示</h4><p>OCR 模型通常接收图像输入，并可选地接受文字提示 (prompt) ，这取决于模型的架构与预训练方式。</p><p>部分模型支持<strong>基于提示的任务切换</strong>，例如  <a href="https://link.segmentfault.com/?enc=fLgUbTW091TD1lmNMy36JA%3D%3D.a3OJe6oJ%2ButLtLCWTEx%2BNPDmIdP60E0bHr35zJGxqraP3KAC7vc1GuFWJeVR7DPN" rel="nofollow" target="_blank">granite-docling</a>  可以通过不同提示词执行不同任务:</p><ul><li>输入 “Convert this page to Docling” → 将整页转换为 DocTags；</li><li>输入 “Convert this formula to LaTeX” → 将页面中的公式转换为 LaTeX。</li></ul><p>而另一些模型则只能处理整页内容，任务由系统提示固定定义。<br/>例如，<a href="https://link.segmentfault.com/?enc=9OJ9hRxXWX7TxCJpW%2F3aTg%3D%3D.WLlGxaVgH9ql%2FlNJzn60PrY5QQVQmTyLaAssrMFiqvHuxCCifHeSvpDNt59B2xVFu%2BksLZ1O7TzaD1PfKIq5cW74YKnIJ7CCYab44nVvtLA%3D" rel="nofollow" target="_blank">OlmOCR (AllenAI)</a>  使用一个长系统提示词进行推理。OlmOCR 本质上是基于 Qwen2.5VL 微调的 OCR 模型，虽然它也能处理其他任务，但在 OCR 场景之外性能会明显下降。</p><h2>前沿开源 OCR 模型</h2><p>过去一年，我们见证了 OCR 模型领域的爆发式创新。由于开源生态的推动，不同团队之间可以相互借鉴、迭代，从而加速了技术进步。一个典型例子是 AllenAI 发布的 <strong>OlmOCR</strong>，它不仅开源了模型本身，还公开了训练所用的数据集，为他人提供了可复现与可扩展的基础。<br/>这个领域正以前所未有的速度发展，但如何选择最合适的模型，仍然是一个不小的挑战。</p><h3>最新模型对比</h3><p>为了帮助大家更清晰地了解当前格局，以下是一些当前主流开源 OCR 模型的非完整对比。<br/>这些模型都具备版面理解能力 (layout-aware) ，能解析表格、图表与数学公式。<br/>各模型支持的语言范围可在其 model card 中查看。除 <strong>Chandra</strong> (OpenRAIL 许可) 与 <strong>Nanonets</strong> (许可证不明) 外，其余均为开源许可。</p><p>表格中展示的平均得分来自 <strong>Chandra</strong> 与 <strong>OlmOCR</strong> 模型卡中在 <strong>OlmOCR Benchmark</strong> (仅英文) 上的测试结果。<br/>此外，许多模型基于 <strong>Qwen2.5-VL</strong> 或 <strong>Qwen3-VL</strong> 微调，因此我们也附上了 Qwen3-VL 作为参考。</p><table><thead><tr><th align="left">模型名称</th><th align="left">输出格式</th><th align="left">特性</th><th align="left">模型大小</th><th align="left">是否多语言</th><th align="left">OlmOCR 基准平均得分</th></tr></thead><tbody><tr><td align="left"><a href="https://link.segmentfault.com/?enc=2e8odK%2FpkiJHHKNYIUFaww%3D%3D.GE%2BD1Z62ZpkQ0y7ts%2FZnrDz3qR7nFS1KxRXXSEPnjMzuc3m3KLcvlWpZotWkXDlKPpvXE35O%2BWchdO5fXjmo7TmATtOhExO7ZJ9j1vKP5aw%3D" rel="nofollow" target="_blank">Nanonets-OCR2-3B</a></td><td align="left">结构化 Markdown (含语义标注、HTML 表格等)</td><td align="left">图片自动生成说明<br/>可提取签名与水印<br/>识别复选框、流程图、手写体</td><td align="left">4B</td><td align="left">✅ 英语、中文、法语、阿拉伯语等</td><td align="left">N/A</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=8aRdNCqzKMulId9%2FABeFCw%3D%3D.KAr1HLQMuX%2BXi6RGyNGmf2IFuzoiaR9i9lRhI7elqQCTlg8nSUBveIzS6%2Fi6KxY8oPScA8nHl9UizRFu%2F2Y7Zpc7PlM3r%2BYxAoc275FbmoA%3D" rel="nofollow" target="_blank">PaddleOCR-VL</a></td><td align="left">Markdown、JSON、HTML 表格与图表</td><td align="left">支持手写体与旧文档<br/>支持提示词输入<br/>可将表格与图表转换为 HTML<br/>可直接提取并插入图片</td><td align="left">0.9B</td><td align="left">✅ 支持 109 种语言</td><td align="left">N/A</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=N2fZAjSJdExHYZOZ1NFLyg%3D%3D.cmyUvCPVfPPMXIvr4KR0TWmrnsZMbshkOTR4ZRxrz8uLW04AR2wCgyDqP%2FdPSjWq" rel="nofollow" target="_blank">dots.ocr</a></td><td align="left">Markdown、JSON</td><td align="left">支持 grounding<br/>可提取并插入图片<br/>支持手写体</td><td align="left">3B</td><td align="left">✅ 多语言 (具体未说明)</td><td align="left">79.1 ± 1.0</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=KfKw6xDZoI0vzq4bFBK26w%3D%3D.umDL%2B%2Fq9QZqo68rQ6bPNaLrHaXqmgE5RbYXqJN%2FDUhtrNJGjTJ9a5b6th7fMad92" rel="nofollow" target="_blank">OlmOCR-2</a></td><td align="left">Markdown、HTML、LaTeX</td><td align="left">具备 grounding 能力<br/>优化了大规模批处理性能</td><td align="left">8B</td><td align="left">❎ 仅英语</td><td align="left">82.3 ± 1.1</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=bQDOIWgRISNEDCQexxbA%2Bg%3D%3D.we0D2j596GjyJ%2B0XyRL5Gg2nx4sVp16MDhO5NigjnnjoMgP%2FWghJYvWW3O4%2FhtLo" rel="nofollow" target="_blank">Granite-Docling-258M</a></td><td align="left">DocTags</td><td align="left">支持基于提示的任务切换<br/>可指定元素位置<br/>输出内容丰富</td><td align="left">258M</td><td align="left">✅ 英语、日语、阿拉伯语、中文</td><td align="left">N/A</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=WF5Wq2wdhx6oi9l91eKezA%3D%3D.hfLvaKo4FlZYM1hc%2Fo9ZRAO1R59nSgubeQnniIoJQxjm8HHX7yhfotsXF0foGxgZ" rel="nofollow" target="_blank">DeepSeek-OCR</a></td><td align="left">Markdown、HTML</td><td align="left">支持通用视觉理解<br/>能将图表、表格完整渲染为 HTML<br/>识别手写体<br/>内存高效，图像文字识别能力强</td><td align="left">3B</td><td align="left">✅ 近 100 种语言</td><td align="left">75.4 ± 1.0</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=QHnce2fMme5fquh8cOjQNw%3D%3D.N8wULDb49Rwy2svpLklaaAGXnbDZDEV2VfMEOa3z%2B8qvJM98fdwUzy%2BOuu5E8b4L" rel="nofollow" target="_blank">Chandra</a></td><td align="left">Markdown、HTML、JSON</td><td align="left">具备 grounding 能力<br/>能原样提取并插入图片</td><td align="left">9B</td><td align="left">✅ 支持 40+ 种语言</td><td align="left">83.1 ± 0.9</td></tr><tr><td align="left"><a href="https://link.segmentfault.com/?enc=ZVDB28evi2sdvgF4Ifcd0Q%3D%3D.6RvU5KKvclneP7o3EDppjzEur53zgAcCuZ5hvFSjPHAnwLul%2FJj2eBJA%2FoREGWFb" rel="nofollow" target="_blank">Qwen3-VL</a></td><td align="left">任意格式输出 (多模态语言模型)</td><td align="left">识别古文文本<br/>支持手写体<br/>图片可原样提取插入</td><td align="left">9B</td><td align="left">✅ 支持 32 种语言</td><td align="left">N/A</td></tr></tbody></table><blockquote><strong>注:</strong> <br/>Qwen3-VL 是一款强大的通用视觉语言模型，支持多种文档理解任务，但并未针对 OCR 任务进行特别优化。<br/>其他模型多采用固定提示词进行微调，专为 OCR 任务设计。<br/>因此若使用 Qwen3-VL，建议尝试不同提示词以获得更佳效果。</blockquote><p>你可以通过这个 <a href="https://link.segmentfault.com/?enc=rbfgHYW8RHhW93DoddIB7w%3D%3D.91psp3FVO%2BNe83TnGebUmfHy4muuNi%2FrpghURvIlPp4uKhYIANBBTQjg2qmT7LGp" rel="nofollow" target="_blank">在线演示</a> 体验部分最新模型并比较输出效果: </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445819" alt="" title="" loading="lazy"/></p><h3>模型评估</h3><h4>基准测试</h4><p>没有任何一款模型能在所有场景中都是“最优”。<br/>例如: 表格应以 Markdown 还是 HTML 呈现？哪些元素需要提取？如何量化文本识别准确度？👀<br/>这些都取决于具体任务。<br/>目前已有多个公开评测集与工具，但仍无法覆盖所有情况。<br/>我们推荐以下常用的评测基准:</p><ol><li><a href="https://link.segmentfault.com/?enc=qosVgfcXBUgSLJUg0O7gzQ%3D%3D.wJuQCNHXmGe2ark0Dto4wUhoGjY9BxXOA2AwU%2B0cUl%2Fpf0S6NPJ0wiT4xBz3h3vP" rel="nofollow" target="_blank">OmniDocBenchmark</a></li></ol><ul><li><p>这是目前使用最广泛的文档识别基准之一。</p><ul><li>覆盖文档类型丰富: 书籍、杂志、教材等。</li><li>支持多格式 (HTML 与 Markdown) 表格评测。</li><li>使用新型算法评估阅读顺序；公式会在评估前标准化。</li><li>指标基于“编辑距离”或“树编辑距离” (表格部分) 。</li><li>标注数据部分由 SoTA VLM 或传统 OCR 生成。</li></ul></li></ul><ol start="2"><li><a href="https://link.segmentfault.com/?enc=HcuWfYdTjgrWZBbebg7LSw%3D%3D.t%2BZyC1V0IgNQh5fVgghPk9s4W6etg8MOTRHViJMOfIJk5HG8L6hkwyI6I3pLS2ZA" rel="nofollow" target="_blank">OlmOCR-Bench</a></li></ol><ul><li><p>采用“单元测试式”评估方式。</p><ul><li>例如: 表格评估通过验证单元格间关系完成。</li><li>数据源为公开 PDF，标注来自多种闭源 VLM。</li><li>特别适合评估英文 OCR 模型。</li></ul></li></ul><ol start="3"><li><a href="https://link.segmentfault.com/?enc=5753SzptT3Iw7kAuI2XT9Q%3D%3D.tMwyP2pH1XX%2BPBag38931GAuMZ6BPTscvCkCeYn8jYxjzBksZZLUaQESn4cn9RqY" rel="nofollow" target="_blank">CC-OCR (Multilingual)</a></li></ol><ul><li><p>与前两者相比，CC-OCR 的文档质量与多样性较低。</p><ul><li>但它是<strong>唯一</strong>涵盖英语与中文以外语言的多语言评测集。</li><li>图片多为低质量拍摄，文本较少。</li><li>尽管不完美，但目前仍是最佳的多语言评估选项。</li></ul></li></ul><p>在不同文档类型、语言与任务场景下，模型表现差异明显。<br/>如果你的业务领域不在现有评测集中体现，我们建议收集代表性样本，构建自定义测试集，比较不同模型在你的特定任务上的效果。</p><h4>成本与效率</h4><p>大多数 OCR 模型的规模在 <strong>3B～7B 参数</strong>之间，也有一些小型模型 (如 PaddleOCR-VL 仅 0.9B) 。<br/>成本不仅与模型大小相关，还取决于是否支持高效推理框架。</p><p>例如:</p><ul><li><strong>OlmOCR-2</strong> 提供 vLLM 与 SGLang 实现。</li><li>若在 H100 GPU ($2.69/小时) 上运行，推理成本约为 **每百万页 $178**。</li><li><strong>DeepSeek-OCR</strong> 能在一块 40GB A100 上每天处理 <strong>20 万页以上</strong>。</li><li>以此估算，其成本与 OlmOCR 大致相当 (视 GPU 供应商而定) 。</li></ul><p>若任务对精度要求不高，还可选择 <strong>量化版本 (Quantized Models)</strong> ，进一步降低成本。<br/>总体而言，开源模型在大规模部署时几乎总比闭源方案更经济。</p><h4>开源 OCR 数据集</h4><p>尽管近年来开源 OCR 模型大量涌现，但公开的训练与评测数据集仍相对稀缺。<br/>一个例外是 AllenAI 的  <a href="https://link.segmentfault.com/?enc=tQpyqfEBX1GWS73doPaQvA%3D%3D.C1cqjLDTL2Nw4agRfjGrjkAGBDM%2B0mREOGwiVCsm%2FtHFXo7KWd7Rqqe7cgZYg5Uj" rel="nofollow" target="_blank">olmOCR-mix-0225</a>，<br/>截至目前，该数据集已被用于训练至少  <a href="https://link.segmentfault.com/?enc=nzLiKq89tZA%2FWn8ar0IFlA%3D%3D.t4U8gsOyrIk%2FlCmXmn3JWgNq%2BsPgvPQHzth929tmKCNac%2BVvUna7OI8or43IHYvIUiBrobZhSicn77WmvwbTzQ%3D%3D" rel="nofollow" target="_blank">72 个模型</a>  (可能更多) 。</p><p>更广泛的数据共享将极大推动开源 OCR 的进步。<br/>以下是几种常见的数据集构建方式:</p><ul><li><strong>合成数据生成 (Synthetic Data Generation)</strong> <br/>例如: <a href="https://link.segmentfault.com/?enc=okHSjukoWQS8TX%2F%2BJ6k0rw%3D%3D.rgrHRv3EiX%2BiB%2B08nxK0OpePkmsRnTcfWmmS9EmaEnVyO3K8nNa4w24hLiXG%2FFrJQPFB8d6cccyDR2T5tuorCw%3D%3D" rel="nofollow" target="_blank">isl_synthetic_ocr</a></li><li><strong>VLM 自动转录</strong>，再经人工或启发式过滤</li><li><strong>利用现有 OCR 模型生成新训练数据</strong>，以训练更高效的领域专用模型</li><li><strong>基于人工校正语料的再利用</strong>，如 <a href="https://link.segmentfault.com/?enc=dxhJZSgaY0cWeC8NKWJuRQ%3D%3D.tUkI7Uv1nxO0wj3J5LzzB790%2Fv%2B7fhpWI5MXO7X28ci8RI5r9tPygWGIHY1HxV7L" rel="nofollow" target="_blank">英国印度医学史数据集</a>，其中包含大量人工修正的历史文档 OCR</li></ul><p>值得注意的是，许多此类数据集已存在但尚未“训练化” (training-ready) 。<br/>若能系统化整理并公开，将为开源社区释放巨大潜力。</p><h2>模型运行工具</h2><p>我们收到许多关于“如何开始使用 OCR 模型”的问题，因此这里总结了几种简单的方式——<br/>包括在本地运行推理，或通过 Hugging Face 进行远程托管。</p><h3>本地运行</h3><p>目前大多数先进 OCR 模型都提供 <strong>vLLM</strong> 支持，并可通过 <strong>transformers</strong> 库直接加载推理。<br/>你可以在各模型的 Hugging Face 页面找到具体说明。<br/>下面我们以 <strong>vLLM 推理方式</strong>为例演示基本流程。</p><h4>使用 vLLM 启动服务</h4><pre><code class="shell">vllm serve nanonets/Nanonets-OCR2-3B</code></pre><p>然后，你可以通过 OpenAI SDK 进行调用，例如:</p><pre><code class="py">from openai import OpenAI
import base64

client = OpenAI(base_url="http://localhost:8000/v1")
model = "nanonets/Nanonets-OCR2-3B"

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

def infer(img_base64):
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},
                    },
                    {
                        "type": "text",
                        "text": "Extract the text from the above document as if you were reading it naturally.",
                    },
                ],
            }
        ],
        temperature=0.0,
        max_tokens=15000
    )
    return response.choices[0].message.content

img_base64 = encode_image(your_img_path)
print(infer(img_base64))</code></pre><h4>使用 Transformers 运行推理</h4><p>Transformers 库提供了标准化的模型定义与接口，可轻松进行推理或微调。<br/>模型可能有两种加载方式:</p><ol><li><strong>官方实现</strong> (在 transformers 内定义)</li><li><strong>remote code 实现</strong> (由模型作者定义，允许 transformers 自动加载)</li></ol><p>以下示例展示了如何用 transformers 调用 <strong>Nanonets OCR 模型</strong>:</p><pre><code class="py"># 安装依赖: flash-attn 和 transformers
from transformers import AutoProcessor, AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    "nanonets/Nanonets-OCR2-3B", 
    torch_dtype="auto", 
    device_map="auto", 
    attn_implementation="flash_attention_2"
)
model.eval()
processor = AutoProcessor.from_pretrained("nanonets/Nanonets-OCR2-3B")

def infer(image_url, model, processor, max_new_tokens=4096):
    prompt = """Extract the text from the above document as if you were reading it naturally. Return the tables in html format. Return the equations in LaTeX representation. If there is an image in the document and image caption is not present, add a small description of the image inside the &lt;img&gt;&lt;/img&gt; tag; otherwise, add the image caption inside &lt;img&gt;&lt;/img&gt;. Watermarks should be wrapped in brackets. Ex: &lt;watermark&gt;OFFICIAL COPY&lt;/watermark&gt;. Page numbers should be wrapped in brackets. Ex: &lt;page_number&gt;14&lt;/page_number&gt; or &lt;page_number&gt;9/22&lt;/page_number&gt;. Prefer using ☐ and ☑ for check boxes."""
    image = Image.open(image_path)
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": [
            {"type": "image", "image": image_url},
            {"type": "text", "text": prompt},
        ]},
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], padding=True, return_tensors="pt").to(model.device)
    
    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
    
    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return output_text[0]

result = infer(image_path, model, processor, max_new_tokens=15000)
print(result)</code></pre><h4>使用 MLX (适用于 Apple 芯片)</h4><p><strong>MLX</strong> 是苹果推出的机器学习框架，专为 <strong>Apple Silicon (M 系列)</strong> 设计。<br/>在此基础上构建的 <a href="https://link.segmentfault.com/?enc=bbYAelag6fan4PV7LLJFsA%3D%3D.kqg%2FQivtPUJvt3CtId%2FbxF67IXT3Mu11BrFjBQdUPEoAVc%2B7kmMMScpPT6WH5UUh" rel="nofollow" target="_blank">MLX-VLM</a> 能轻松运行视觉语言模型。<br/>你可以在 <a href="https://link.segmentfault.com/?enc=GCnxRwrNoU%2FK%2BqNVS34Q0A%3D%3D.NhAMAWz6pm%2B2cA6sXeReKU1sEf0WQp62yCN1BeXu2Xy4n7Mb%2FZDjcg4xsNNM6c%2FP" rel="nofollow" target="_blank">Hugging Face</a> 搜索所有支持 MLX 的 OCR 模型 (包括量化版本) 。</p><p>安装 MLX-VLM:</p><pre><code class="bash">pip install -U mlx-vlm</code></pre><p>示例运行:</p><pre><code class="bash">wget https://huggingface.co/datasets/merve/vlm_test_images/resolve/main/throughput_smolvlm.png

python -m mlx_vlm.generate \
  --model ibm-granite/granite-docling-258M-mlx \
  --max-tokens 4096 \
  --temperature 0.0 \
  --prompt "Convert this chart to JSON." \
  --image throughput_smolvlm.png </code></pre><h3>远程运行</h3><h4>使用 Inference Endpoints 部署模型 (托管推理服务)</h4><p>你可以通过 <strong>Hugging Face Inference Endpoints</strong> 在托管环境中部署兼容 vLLM 或 SGLang 的 OCR 模型。<br/>该服务提供 GPU 加速、自动伸缩、监控与安全托管，无需自行维护基础设施。</p><p>部署步骤如下:</p><ol><li>进入模型仓库 <a href="https://link.segmentfault.com/?enc=BSrwQ2JlWXft1Nr7Xg4D4Q%3D%3D.d65iiY08g1aKyaXrFZg8VzAwrOfKBPVNVba1Z51eKfoFcQCGl94Q8bzCuFOM4EbP" rel="nofollow" target="_blank">nanonets/Nanonets-OCR2-3B</a></li><li>点击页面上的 <strong>“Deploy”</strong> 按钮，选择 <strong>“HF Inference Endpoints”</strong></li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445820" alt="" title="" loading="lazy"/></p><ol start="3"><li>在弹出的窗口中配置部署参数 (GPU 类型、实例数量等)</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445821" alt="" title="" loading="lazy"/></p><ol start="4"><li>部署完成后，你可以直接通过上文示例中的 OpenAI 客户端脚本调用该 Endpoint。</li></ol><p>更多信息可参阅官方文档: 👉 <a href="https://link.segmentfault.com/?enc=2qLa3mBJ0RtPTD6BFG1%2BAA%3D%3D.KzhMpaPdUWjftMmviXqthCfRxeZ9KAFnmSCgZZ8uKfUkQA4XAJNtXi4LilGCnU9Nda5QMhPElqcN3bHEF%2FRfZQ%3D%3D" rel="nofollow" target="_blank">Inference Endpoints (vLLM)</a></p><h4>使用 Hugging Face Jobs 进行批量推理</h4><p>对于 OCR 场景，往往需要<strong>批量处理成千上万张图像</strong>。<br/>这类任务可通过 <strong>vLLM 的离线推理模式</strong> 实现高效并行。</p><p>为了简化流程，我们创建了  <a href="https://link.segmentfault.com/?enc=LNCq6AkP9Mi6GQ5HS46PVw%3D%3D.nW7MpgyB9khtQsJtgkvOmyqDwX58TJEGP54MD%2BNXQKbqBzM31bs6FeCCzkLq8%2BSM" rel="nofollow" target="_blank">uv-scripts/ocr</a>，<br/>它是一组适配 Hugging Face Jobs 的可直接运行脚本，能实现:</p><ul><li>对数据集列中的所有图片进行批量 OCR</li><li>将 OCR 结果以 Markdown 形式新增为新列</li><li>自动将带结果的数据集回传到 Hub</li></ul><p>例如，处理 100 张图片的命令如下:</p><pre><code class="bash">hf jobs uv run --flavor l4x1 \
  https://huggingface.co/datasets/uv-scripts/ocr/raw/main/nanonets-ocr.py \
  your-input-dataset your-output-dataset \
  --max-samples 100</code></pre><p>这些脚本会自动处理所有 vLLM 配置与批次推理逻辑，<br/>让批量 OCR 变得无需 GPU 或复杂部署。</p><h2>超越 OCR</h2><p>如果你对文档智能 (Document AI) 感兴趣，不仅仅局限于文字识别 (OCR) ，以下是我们的一些推荐方向。</p><h3>视觉文档检索</h3><p><strong>视觉文档检索 (Visual Document Retrieval)</strong> 指的是: <br/>当你输入一条文本查询时，系统能够从大量 PDF 文档中直接检索出最相关的前 <em>k</em> 篇。</p><p>与传统文本检索模型不同，视觉文档检索器直接在“文档图像”层面进行搜索。<br/>除了独立使用外，你还可以将它与视觉语言模型结合，构建 <strong>多模态 RAG (Retrieval-Augmented Generation)</strong> 管线。<br/>相关示例可参考: <a href="https://link.segmentfault.com/?enc=xVe4jcCEJxhjCUaS5b8Y7Q%3D%3D.h0fu2zQ1jgJwpyNwpOkRjLJbvOSOdNwNk4GTm3VO4afWhKNmOvhkGz0FbKxetJvRtahrzqqgsx5ZDYea%2FE2971UMWAjPDHAPPCwScQzuMT0%3D" rel="nofollow" target="_blank">ColPali + Qwen2_VL 多模态 RAG 教程</a>。</p><p>你可以在  <a href="https://link.segmentfault.com/?enc=36aBHOa%2BohsfbzvMr6U0aw%3D%3D.FVWVgTrhnvbM084kDWi9sWA86TrLdqTjDOXfaSXYsOEfMX7JSkF6byojBpO2hvWCWtQDKb%2BQPDlOQPZAZ8swPcjcYEPYw%2BAjyID69VbPgO0%3D" rel="nofollow" target="_blank">Hugging Face Hub</a>  找到所有可用的视觉文档检索模型。</p><p>目前主流的视觉检索器分为两类:</p><table><thead><tr><th>类型</th><th>特点</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>单向量模型 (Single-vector Models)</strong></td><td>内存效率高、速度快，但性能略弱</td><td>轻量化部署、大规模索引</td></tr><tr><td><strong>多向量模型 (Multi-vector Models)</strong></td><td>表征能力强、检索精度高，但占用显存更大</td><td>高精度检索、知识密集任务</td></tr></tbody></table><p>大多数此类模型都支持 <strong>vLLM</strong> 和 <strong>transformers</strong>，因此你可以很方便地用它们进行向量索引，然后结合向量数据库 (vector DB) 执行高效搜索。</p><h3>基于视觉语言模型的文档问答 (Document Question Answering)</h3><p>如果你的任务目标是<strong>基于文档回答问题</strong> (而不是仅仅提取文字) ，<br/>你可以直接使用经过文档任务训练的<strong>视觉语言模型 (VLM)</strong> 。</p><p>许多用户习惯于:</p><ol><li>先将文档转换成纯文本；</li><li>再把文本传入 LLM 进行问答。</li></ol><p>这种方式虽然可行，但存在明显缺陷:</p><ul><li>一旦文档布局复杂 (如多栏结构、图表、图片说明等) ，转换后的文本就可能丢失关键信息；</li><li>图表被转为 HTML、图片说明生成错误时，LLM 就会误判或忽略内容。</li></ul><p>因此，更好的做法是: <br/>直接将<strong>原始文档图像 + 用户问题</strong> 一起输入支持多模态理解的模型，<br/>例如 <a href="https://link.segmentfault.com/?enc=kgXhHo33o%2BBk6SEfI84wKQ%3D%3D.k%2FsSnMCbLbPYOEaBLWmScIiSFRFQEhy6gw2l1Dqa6BB4drNAWDtZQT3WUhRkGmzbaBWQaM4Z%2BM5gdGc252G%2Ft6n6iYh0m2fIFnWGRHzWcG8%3D" rel="nofollow" target="_blank">Qwen3-VL</a>。<br/>这样模型就能同时利用视觉与文本信息，不会错过任何上下文细节。</p><h2>总结</h2><p>在这篇文章中，我们为你概览了现代 OCR 技术的核心要点，包括:</p><ul><li>如何选择合适的 OCR 模型</li><li>当前最前沿的开源模型及其能力</li><li>在本地或云端运行模型的工具</li><li>以及如何在 OCR 之上构建更复杂的文档智能应用</li></ul><p>如果你希望进一步深入了解 OCR 与视觉语言模型 (VLM) ，<br/>以下是我们推荐的延伸阅读与教程资源 👇</p><h3>延伸阅读与资源</h3><ul><li>📘 <a href="https://link.segmentfault.com/?enc=%2BVJQ%2FrkLYp4D0b4uE%2B4BzA%3D%3D.dZwDi3YMBxP652xkwICwDYJhJhe843r4VrI64n1Ntpg%3D" rel="nofollow" target="_blank">Vision Language Models Explained (视觉语言模型详解)</a> <br/>—— 深入理解 VLM 的工作原理与发展历程。</li><li>🧠 <a href="https://link.segmentfault.com/?enc=D547VR7UcbeXewu%2FmtCOCA%3D%3D.dHj4ZpgMxZPk2GQruNb%2B4gu9bL3kbo%2BgchW2Z89qm60%3D" rel="nofollow" target="_blank">Vision Language Models 2025 Update (2025 年视觉语言模型更新)</a> <br/>—— 最新 VLM 技术进展总结。</li><li>🔍 <a href="https://link.segmentfault.com/?enc=SjklMxXKxLyrGUpnGRadRg%3D%3D.NeNNI%2Ft99YG9DZF7Mv29gjqEsSOyMATVaGG4xPXmJnwKf4vIZO%2F%2Bqoh9tJ2D4%2BzG" rel="nofollow" target="_blank">PP-OCR-v5 技术博客</a><br/>—— 来自百度的高性能 OCR 系统优化介绍。</li><li>🧩 <a href="https://link.segmentfault.com/?enc=scyCQe3BiIhgzOTnc6iKIA%3D%3D.yw8A3l8sr4oB1Y4JaIAMFHDpWTWP0TwiYVozULd%2BixKQImH3hK0%2BWY1BSnJY20xs44FjJkec8N4lRPlzXxiINqARwO%2F0evjlozSY0ytVnXk%3D" rel="nofollow" target="_blank">教程: 微调 Kosmos2.5 进行 Grounded OCR</a><br/>—— 实践指南，教你如何让模型具备“锚定式”识别能力。</li><li>📄 <a href="https://link.segmentfault.com/?enc=lO92cxIlZMmsHMDmiQgzCQ%3D%3D.pHwpMLRvTaqPyHhuEq8DL7C%2FRvQA5VT%2Fyk4WThMvOVcvMULkzmZhvF8NS0e2tQXatjGjf4lfr6HVx%2BU6WFCL%2BGTPBlvM5Wq%2BKoC9gwDHvpA%3D" rel="nofollow" target="_blank">教程: 在 DocVQA 数据集上微调 Florence-2</a><br/>—— 基于视觉问答任务的微调实例。</li><li>📱 <a href="https://link.segmentfault.com/?enc=%2B3AOJzWvst2aFwkOCssT4w%3D%3D.iLskBD7OGZHB%2FbDkaYjpA6CUgrcetNkzEz1DQfYlyC4%3D" rel="nofollow" target="_blank">在设备端实现 SOTA OCR (Core ML + dots.ocr)</a> <br/>—— 展示如何在移动端高效部署 OCR 模型。</li></ul><p><strong>总结一句话:</strong> 开源视觉语言模型正在重新定义 OCR 的边界。从纯文本识别到多模态理解、从图像到语义、从离线推理到大规模部署——如今的开源生态为每一个开发者和研究者提供了前所未有的自由度与创新空间。</p><p>无论你是在构建下一代文档智能系统，还是仅想更高效地解析 PDF，希望这篇指南能帮助你找到最合适的起点 🚀</p><blockquote><p>英文原文: <a href="https://link.segmentfault.com/?enc=ezbHR%2FuDM%2FVcw%2BlcP7sZXw%3D%3D.PpRFCxY4VY1LdvQh9wmSumXcctxb%2FrrvFm9SFUpy0Il6cEISR2UPH%2F%2FApmKn07jF" rel="nofollow" target="_blank">https://huggingface.co/blog/ocr-open-models</a></p><p>原文作者: merve, Aritra Roy Gosthipaty, Daniel van Strien, Hynek Kydlicek, Andres Marafioti, Vaibhav Srivastav, Pedro Cuenca</p><p>译者: Luke,  Hugging Face Fellow</p></blockquote>]]></description></item><item>    <title><![CDATA[SSL 证书过期？这些后果直接让网站 “]]></title>    <link>https://segmentfault.com/a/1190000047445931</link>    <guid>https://segmentfault.com/a/1190000047445931</guid>    <pubDate>2025-12-03 14:04:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>做网站的老板注意了！SSL 证书过期不是 “小事”，而是能让你血本无归的 “致命雷区”！很多站长觉得晚几天续期无所谓，殊不知一旦证书失效，流量、订单、品牌信任会瞬间崩塌，甚至面临合规罚款，这些后果远比你想象中更残酷！<br/><img width="390" height="260" referrerpolicy="no-referrer" src="/img/bVddeYp" alt="" title=""/></p><h3>一、浏览器直接 “亮红牌”，用户一秒跑光！</h3><p>你以为用户会耐心等待你续期？大错特错！只要 SSL 过期，Chrome、百度、Edge 等所有主流浏览器都会弹出刺眼的 “连接不安全” 警告，有的甚至直接拦截访问，强制用户退出。</p><p>现在网友的安全意识早就今非昔比，超过 90% 的人看到 “不安全” 提示，会立刻关闭页面，连犹豫都不会犹豫！电商网站直接丢订单，咨询网站错失客户，资讯网站流量暴跌 —— 有站长实测，SSL 过期 3 天，网站流量直接腰斩，半个月都没恢复过来！</p><h3>二、数据 “裸奔” 遭窃取，用户投诉 + 法律纠纷找上门！</h3><p>SSL 证书是网站的 “加密锁”，过期后这把锁就直接失效了！用户在你网站输入的登录密码、手机号、身份证号、支付信息，都会以明文形式传输，黑客用简单工具就能截取。</p><p>之前就有电商网站因 SSL 过期，导致上千用户支付密码泄露，不仅要赔偿用户损失，还被监管部门调查，品牌形象彻底毁了！这种纠纷一旦发生，小网站可能直接倒闭，大网站也要花巨资公关，得不偿失！</p><h3>三、业务全面停摆，合规罚款让你雪上加霜！</h3><p>别以为只有流量和安全受影响，SSL 过期会直接让你的业务 “停摆”！支付网关对接失败，用户付款时提示 “网络错误”，订单全流失；API 接口、小程序、APP 无法正常运行，用户打不开、用不了，直接卸载；甚至网站后台都登不上，日常运营彻底瘫痪。</p><p>更可怕的是合规风险！金融、医疗、电商等行业必须符合 PCI DSS、GDPR 等法规，SSL 过期就是 “违规操作”，监管部门查到就罚，少则几万，多则几十万，还可能被要求暂停业务整顿，相当于直接断了营收来源！</p><h3>四、搜索引擎降权，流量再也回不来了！</h3><p>百度、谷歌早就明确：HTTPS 是排名重要指标，不安全的网站直接降权！SSL 过期后，你的网站会被搜索引擎判定为 “高危网站”，排名一落千丈，甚至从搜索结果中消失。</p><p>就算后续续期了 SSL，排名也很难恢复到之前的水平 —— 有站长反映，证书过期 1 个月，自然流量少了 70%，花了半年时间做优化才勉强回升。对于依赖搜索引擎引流的网站来说，这简直是 “灭顶之灾”！</p><p><strong>最后提醒：记住：SSL 续期的成本，远比过期后的损失低 100 倍！别因小失大，让一张过期的 SSL 证书，毁了你的整个网站！</strong></p>]]></description></item><item>    <title><![CDATA[当网站提示“不安全”：SSL证书，你的数]]></title>    <link>https://segmentfault.com/a/1190000047445933</link>    <guid>https://segmentfault.com/a/1190000047445933</guid>    <pubDate>2025-12-03 14:03:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在互联网的浩瀚海洋中航行，我们时常会看到这样的警告：“此网站不安全”或“您的连接不是私密连接”。这些红色警示如同数字世界的警戒线，而它们背后隐藏的关键，正是SSL证书——这个你可能看不见，却时刻保护着你的数字护身符。</p><h4>SSL证书：数字世界的身份认证</h4><p>SSL（安全套接层）证书，如今更准确地说应称为TLS（传输层安全）证书，是一种数字证书，其作用如同现实世界中的护照或身份证。它由受信任的第三方机构——证书颁发机构（CA）签发，用于验证网站身份，并在用户浏览器与网站服务器之间建立加密连接。</p><p>想象一下，你正在咖啡馆使用公共Wi-Fi登录电子邮箱。没有SSL加密，你输入的密码就像写在明信片上传递，任何人都可能截获。而有了SSL证书，这些信息会被转化为只有你和服务器能解读的“密语”，即使被截获也无从破解。</p><h4>为什么网站会提示“不安全”？</h4><p>当浏览器提示网站不安全时，通常有以下几种原因：</p><ol><li><strong>缺乏SSL证书</strong>：网站未安装SSL证书，数据以明文传输，极易被窃取</li><li><strong>证书过期</strong>：如同身份证有有效期，SSL证书通常有效期为1-2年，过期后需续期。</li><li><strong>证书与域名不匹配</strong>：证书仅对特定域名有效，若访问的域名与证书注册域名不符，则会触发警告。</li><li><strong>自签名证书</strong>：网站使用了自行签发的证书，而非受信任CA签发，浏览器无法验证其真实性。</li><li><strong>混合内容问题</strong>：网站虽启用HTTPS，但页面中包含通过HTTP加载的资源（如图片、脚本），造成安全漏洞。</li></ol><h4>遇到“不安全”警告，你该如何应对？</h4><h4>直接访问JoySSL，注册一个账号记得填注册码230970获取技术支持。<a href="https://link.segmentfault.com/?enc=cVXdaK4YVonNni4AU77afg%3D%3D.QYvrtOBCtHnARZwTSzGVMNPkh5ahiUCklm7ZgO5SnaWkb3fu1TvEbf0FyE47BBmU%2BpLCF2rzA%2BQTfig1T%2B%2B6rQ%3D%3D" rel="nofollow" target="_blank">申请入口</a></h4><p><img width="723" height="323" referrerpolicy="no-referrer" src="/img/bVdmVAD" alt="" title=""/></p><h4>第一步：识别警告类型</h4><p>浏览器通常提供详细信息。点击警告页面上的“高级”或“详细信息”，了解具体问题。是证书过期？域名不匹配？还是其他问题？</p><h4>第二步：评估风险级别</h4><ul><li><strong>对于银行、电商等敏感网站</strong>：立即停止访问。绝不输入任何个人信息、密码或支付信息。</li><li><strong>对于信息类网站</strong>：谨慎浏览，避免提交任何表单或个人信息。</li><li><strong>对于内部或测试网站</strong>：可能使用了自签名证书，需确认你确实信任该网站。</li></ul><h4>第三步：采取相应措施</h4><ol><li><strong>检查网址</strong>：确保你访问的是正确网址，警惕拼写错误的“李鬼”网站（如“<a href="https://link.segmentfault.com/?enc=%2FKGyKKkNCSLeBsAjiAMonA%3D%3D.I0aZ4xBzPmoSxCxY5X2%2F9o7u%2BjMoNJCp1XwDtdKJzX4%3D" rel="nofollow" target="_blank">paypa1.com</a>”冒充“<a href="https://link.segmentfault.com/?enc=PH23u2sRfF5BMilkEeXang%3D%3D.iCl3BXY5xg4QJWtnMR6FpRw%2F3L92C5hzFthvvgNMdCo%3D" rel="nofollow" target="_blank">paypal.com</a>”）。</li><li><strong>更新系统时间</strong>：计算机日期设置错误可能导致浏览器误判证书过期。</li><li><strong>联系网站管理员</strong>：如果你信任该网站但遇到问题，可通过其他渠道通知他们。</li><li><strong>考虑使用HTTPS Everywhere等扩展</strong>：这些工具会自动尝试网站的HTTPS版本。</li></ol><h4>第四步：决定是否继续访问</h4><p>对于非敏感信息的浏览，你可以选择“高级”→“继续前往网站”，但必须清楚了解风险：你的任何输入都可能被第三方截获。</p><h4>网站所有者：如何避免“不安全”警告？</h4><p>如果你拥有网站，确保其安全不仅是对访问者的责任，也直接影响你的信誉和搜索引擎排名：</p><ol><li><strong>获取并安装SSL证书</strong>：许多主机提供商提供免费SSL证书（如JoySSL）。</li><li><strong>确保证书有效且及时更新</strong>：设置提醒，在证书过期前续期。</li><li><strong>配置HTTP到HTTPS重定向</strong>：确保所有访问都通过安全的HTTPS连接。</li><li><strong>解决混合内容问题</strong>：确保网站所有资源都通过HTTPS加载。</li><li><strong>使用HSTS（HTTP严格传输安全）</strong> ：告诉浏览器只通过HTTPS访问你的网站。</li></ol><h4>数字时代的信任基石</h4><p>SSL证书不仅是技术工具，更是数字信任的基石。当浏览器显示那个小小的锁形图标和“安全”标签时，它传递的信息是：“你的连接是私密的，这个网站是它所声称的那个实体。”</p><p>在日益复杂的网络环境中，对“不安全”警告保持警惕，是每个数字公民的基本素养。而作为网站运营者，提供安全的连接环境则是基本的责任与义务。SSL证书这座看不见的桥梁，连接着信任与安全，保护着我们在数字世界中的每一次交流、每一笔交易，让互联网成为一个更值得信赖的空间。</p><p>下一次，当浏览器提示“不安全”时，请停下脚步，思考片刻——这短暂的停顿，可能是对你数字安全最重要的保护。</p>]]></description></item><item>    <title><![CDATA[米哈游联创推出可对话「猫猫」AI，具备情]]></title>    <link>https://segmentfault.com/a/1190000047445971</link>    <guid>https://segmentfault.com/a/1190000047445971</guid>    <pubDate>2025-12-03 14:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445973" alt="" title=""/></p><p><strong>开发者朋友们大家好：</strong></p><p>这里是 <strong>「RTE 开发者日报」</strong> ，每天和大家一起看新闻、聊八卦。我们的社区编辑团队会整理分享 RTE（Real-Time Engagement） 领域内「有话题的<strong>技术</strong>」、「有亮点的<strong>产品</strong>」、「有思考的<strong>文章</strong>」、「有态度的<strong>观点</strong>」、「有看点的<strong>活动</strong>」，但内容仅代表编辑的个人观点，欢迎大家留言、跟帖、讨论。</p><p><em>本期编辑：@瓒an、@鲍勃</em></p><h2>01 有话题的技术</h2><p><strong>1、DeepSeek V3.2 正式版发布：推理比肩 GPT-5，首推 Speciale 版本拿下奥数金牌</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445974" alt="" title="" loading="lazy"/></p><p>昨天，深度求索 DeepSeek 正式发布了 V3.2 系列模型，包括<strong>标准版「DeepSeek-V3.2」与增强版「DeepSeek-V3.2-Speciale」</strong>。</p><p>官方测试显示，该模型在公开推理类 Benchmark 中达到了 GPT-5 水平，仅略低于 Gemini-3.0-Pro。同时，相比 Kimi-K2-Thinking，V3.2 输出更为简洁，大幅降低了计算开销与用户等待时间。</p><p>DeepSeek-V3.2 还首次实现了「思考模式下的工具调用」，通过大规模 Agent 训练数据合成方法，显著提升了模型的泛化能力。这一功能使模型能够在复杂任务中多轮思考并调用工具，最终给出更详尽准确的回答。</p><p>官方表示，在高度复杂任务上，Speciale 模型大幅优于标准版本，但消耗的 Tokens 也显著更多，成本更高。目前，DeepSeek-V3.2-Speciale 仅供研究使用，不支持工具调用，暂未针对日常对话与写作任务进行专项优化。</p><p>DeepSeek-V3.2 的思考模式也<strong>增加了对 Claude Code 的支持</strong>，用户可以通过将模型名改为 deepseek-reasoner，或在 Claude Code CLI 中按 Tab 键开启思考模式进行使用。但需要注意的是，思考模式<strong>未充分适配 Cline、RooCode</strong> 等使用非标准工具调用的组件，官方建议用户在使用此类组件时继续使用<strong>非思考</strong>模式。</p><p>技术报告：</p><p><a href="https://link.segmentfault.com/?enc=%2FSTAGmzkzNNft2UsTswjgQ%3D%3D.Mshg6qOywNkp2NRFiMgoIK50ZycFDjvaAPBP4TN%2FrcstHat%2BrJbbAU%2B0dD21Qi021DLOdnzOgi%2BvriM2jza%2FfZ9uuVw988X24P6zn9HdMzmXEltFlt1sFgdVnbSrZLMu" rel="nofollow" target="_blank">https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.2/resolv...</a></p><p>DeepSeek V3.2 开源地址：</p><p>DeepSeek-V3.2</p><p>HuggingFace: </p><p><a href="https://link.segmentfault.com/?enc=B3nIYaktqTK1j6BEzpDLGw%3D%3D.8rP%2FaiwQgifF1xGX%2BpO7e2hTOgvZxgoSr9dwC7Yk8cljO1jvxIQ2VgWmFW3VTdO%2BjQyno1A2D2ifyK%2F%2Bi%2BFPDQ%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/deepseek-ai/DeepSeek-V3.2</a></p><p>ModelScope: </p><p><a href="https://link.segmentfault.com/?enc=iJKrDw4vlTymlYv86GklTw%3D%3D.gPJJU8RKtGLVvfSWwLFgkgNvyhJW9rWq%2BocFDYGKNioHgclmlkC4mj68pK1V%2FNKhh8lsviHWTuKGhplupGeCew%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.2</a></p><p>DeepSeek-V3.2-Speciale</p><p>HuggingFace: </p><p><a href="https://link.segmentfault.com/?enc=6hWIS23IdkdMn3%2BiTiznNQ%3D%3D.Fa0spsrj1%2BlXFoSjPuazqYbteDTq8P6ygsidZAQiV%2FG6%2F2RtgUgJ8CvLvH48G1ZbhCmgdPS8aTf7sl6N2QtPRw%3D%3D" rel="nofollow" target="_blank">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale</a></p><p>ModelScope: </p><p><a href="https://link.segmentfault.com/?enc=tX1LmJHfaUaNX98hlo7Z1A%3D%3D.hXafZsG2CJehOqHi3kl1gzPmqbmmjpqK2kbXXwY66StgX8sKm%2BaDNOJ%2BbL%2B58SgI6XM4setdx%2FoxF7xs7yNtpw%3D%3D" rel="nofollow" target="_blank">https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.2-Speciale</a></p><p>（@IT 之家）</p><p><strong>2、Microsoft 研究揭示：空间音频可将 AI 同声传译理解度翻倍</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445975" alt="" title="" loading="lazy"/></p><p>Microsoft 的一项最新研究指出，在 AI 实时语音翻译中，使用「空间音频」（Spatial Audio）技术，即将翻译语音与发言者在屏幕上的位置相匹配，可将听众的理解度提升一倍以上。这一发现为各大视频会议平台提供了一个技术上可行且效果显著的优化方向，有望极大改善跨语言协作的沟通体验。</p><ul><li><p>技术突破：理解度翻倍，定位更清晰</p><p>研究表明，当翻译语音来自与发言者屏幕位置匹配的左/右声道时，听众正确回答理解性问题的几率是传统（非空间）音频的两倍以上。该技术在多人快速轮流发言时效果尤其显著，能有效帮助用户辨别「谁说了什么」。</p></li><li><p>体验对比：空间音频完胜单耳收听</p><p>与会者普遍认为「空间音频」模式「更容易理解」且「能清晰分辨发言者」。与之形成鲜明对比的是「单耳翻译」（Monaural）模式，它产生了最低的理解度得分，并被用户评价为「令人困惑」和「容易疲劳」。</p></li><li><p>最佳实践：保留音色，平衡音量</p><p>研究还发现两个关键的 UX 细节：1）保留不同发言者独特的声音音色有助于区分人物；2）调低而非完全静音原始语音，可以在减少干扰的同时，保留发言者的身份线索，从而创造最佳体验。</p></li><li><p>平台建议：技术可行且影响巨大</p><p>研究人员建议，会议平台应将翻译音频与发言者的屏幕位置对齐，并提供一个「原始语音 ↔ 翻译语音」的平衡滑块供用户调节。鉴于大多数现代耳机和设备已支持「空间音频」，这一改进在技术上是完全可行的。</p></li></ul><p>论文地址：</p><p><a href="https://link.segmentfault.com/?enc=CKe6HLrKBTvHSgaEK8wvpA%3D%3D.0v0njUFwxDyZhyMtVU6hPEucMx41f4x%2F2bFF5un%2BYsuwENn2jsoR8bkxZ0SBwylt" rel="nofollow" target="_blank">https://arxiv.org/pdf/2511.09525</a></p><p>( @Slator)</p><p><strong>3、ElevenLabs 进军韩国，打造亚洲语音 AI 中心</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445976" alt="" title="" loading="lazy"/></p><p>英国人工智能音频公司 ElevenLabs 正式宣布进军韩国市场，并计划在韩国建立其亚洲语音 AI 中心。该公司将推出本地化的韩语语音模型，并提供名人语音授权，以推动 K-content（韩流内容）在全球的传播。</p><p><strong>语音 AI 技术：</strong> ElevenLabs 拥有先进的基于 AI 的 Text-to-Speech（TTS）技术，能够将文本实时转化为人类语音，并支持语音克隆、AI 配音和音效生成。</p><p><strong>韩语本地化：</strong> 为进军韩国市场，ElevenLabs 投入大量资源，组建了专门团队并聘请专家，开发了能够准确捕捉和渲染韩语特有发音、语调和情感的模型。</p><p><strong>K-content 全球化：</strong> ElevenLabs 的「Eleven v3」模型支持超过 70 种语言，能够完美还原原始情感和细微差别，旨在帮助克服 K-content（如 K-pop 和 K-drama）的语言障碍，并计划与韩国名人合作推出 AI 配音产品。</p><p><strong>企业级应用：</strong> 该技术已获得 5000 万月活跃用户，75% 的 Fortune 500 公司是其客户，并在韩国吸引了 Naver、LG Uplus、Krafton Inc。 等领先企业使用。Nvidia、Deutsche Telekom 等公司也已投资 ElevenLabs。</p><p><strong>亚洲桥头堡：</strong> ElevenLabs 选择韩国作为其进入亚洲市场的关键桥头堡，看好韩国快速增长的 AI 市场、对创新的快速接纳能力以及全球领先的内容影响力。</p><p>ElevenLabs 已在韩国设立了第六个办事处，并立即开始本地化韩语语音模型的开发和应用。公司计划将该技术应用于韩国的内容和游戏产业，并改进客户服务中心的 AI 体验。</p><p>(@CHOSUNBIZ)</p><h2>02 有亮点的产品</h2><p><strong>1、豆包手机助手发布技术预览版，首款工程机亮相，现已售罄</strong></p><p>昨天，豆包宣布其全新手机 AI 助手「豆包手机助手」以技术预览版的形式正式亮相。</p><p>据悉，字节跳动与努比亚为这款工程机的首销备货量为 3 万台。**目前，购买页面显示「已售罄」，购买需预约等待下次开售。</p><p>官方强调，该机型仅为技术预览用途，并不承诺功能的成熟度，普通消费者需谨慎选择。<strong>值得注意的是，豆包官方还明确表示不打算做手机。</strong>这款工程样机的具体配置如下： 配备高通骁龙 8 至尊版处理器；但是搭载 6.78 英寸 1264 × 2800 LTPO 屏幕；后置三颗 50MP 摄像头，涵盖主摄、超广角与长焦，均支持光学防抖；前置具备自动对焦功能；提供 16GB + 512GB 存储组合； 电池容量为 6000mAh，支持 90W 有线快充、15W 无线充电及 5W 反向充电；机身重量约 212g，支持超声波屏下指纹、NFC、红外、USB 3.2Gen1，并配备 5 麦克风与双扬声器。</p><p>上述消息公布后，中兴通讯股价昨天上午强势涨停，报 46.30 元，成交金额超 139 亿，封单金额超 40 亿元，其 H 股也涨超 11%。</p><p>( @APPSO)</p><p><strong>2、可灵 AI 推出全球首个统一多模态视频引擎 O1</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445977" alt="" title="" loading="lazy"/></p><p>昨天晚间，可灵视频正式上线 O1 模型，宣称这是全球首个统一多模态视频大模型，定位为全能创作引擎，旨在通过单一输入框实现跨模态任务的无缝融合，打破传统视频生成的功能割裂问题。</p><p>据介绍，该模型引入 MVL（多模态视觉语言）交互架构，并结合 Chain-of-thought 技术，赋予系统更强的常识推理与事件推演能力。</p><p>官方表示，O1 模型能够在同一界面下处理照片、视频与文字等多模态输入，用户仅需通过简单对话即可完成复杂的创作编辑。</p><p>在功能层面，O1 模型支持多主体视角构建与自由组合，确保视频主体在不同镜头间保持一致性与稳定性。</p><p>同时，用户可灵活组合多种技能，一次生成多样化创意变化，并可自由设定 3 至 10 秒的生成时长，以掌控叙事节奏。</p><p>此外，可灵 AI 宣布自 12 月 1 日起至 12 月 14 日，将举办为期 5 天的「全能灵感周」，并推出会员年卡限时 6.6 折优惠活动，以吸引更多创作者体验该新模型。</p><p>( @APPSO)</p><p><strong>3、米哈游联合创始推出「猫猫」互动娱乐 AI 模型</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445978" alt="" title="" loading="lazy"/></p><p>据 36 氪报道，米哈游联合创始人蔡浩宇在美国创立的 AI 公司 Anuttacon 近日上线了一款全新 AI 聊天大模型「AnuNeko」。</p><p>该产品以黑猫为默认形象，强调个性化与互动性，区别于传统的工具型 AI，更像是具备情绪与独立思考的「伙伴」。</p><p>「AnuNeko」的注册商标已于 2025 年 9 月 29 日提交美国 USPTO，涵盖软件、AI 角色与娱乐等多个领域。用户可选择两种不同风格的虚拟猫角色：回答犀利的「异国短毛猫」Exotic Shorthair 与更温和的「橘猫」Orange Cat。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445979" alt="" title="" loading="lazy"/></p><p>报道认为，蔡浩宇的目标并非仅限于推出一款聊天机器人，而是借此探索 AI 在游戏生态中的应用。</p><p>在今年 8 月，Anuttacon 曾发布实验性 AI 游戏《群星低语》，玩家通过与 AI 角色对话推动剧情发展，体现了高自由度与 AI 自主性。此次「AnuNeko」的上线，或许是进一步测试 AI 在互动娱乐中的潜力。</p><p>在全球范围内，Google、育碧、字节跳动等企业也在布局 AI + 游戏：</p><ul><li>Google DeepMind 推出的 SIMA 2 能在 3D 虚拟世界中自主学习与推理；</li><li>字节的「Lumine」在《原神》中展现出跨场景泛化能力；</li><li>育碧的 NEO NPCs 则已能实时分析玩家语音并制定策略。这些案例显示，AI 正逐步成为游戏产业的核心驱动力。</li></ul><p>报道指出，与传统强调执行力的智能体不同，Anuttacon 的策略是让 AI 更「像人」，具备情绪与个性。这一方向或许能为未来互动娱乐带来新的突破：真正吸引玩家的并非完美答案，而是充满生命力的对话与陪伴。</p><p><a href="https://link.segmentfault.com/?enc=tKVLFmPgMgUg9C%2BYykkwvQ%3D%3D.YQQL3g13WJ2zV9%2BaOTg6sdCY4K6ZPAxO%2Bj%2BYoHfonkI%3D" rel="nofollow" target="_blank">https://anuneko.com</a></p><p>( @APPSO)</p><h2>03 有态度的观点</h2><p><strong>1、马斯克最新预言：AI 可在三年内终结美国「债务危机」</strong></p><p>12 月 1 日消息，自 2022 年 ChatGPT 问世后，AI 迅速被视为医疗、农业、能源等各领域的万能工具。不过马斯克的看法却更进一步，他认为 AI 与机器人技术才是解决美国债务危机的关键。在日前播出的一档播客节目中，马斯克表示：「美国债务问题只有一个出口，那就是 AI。」</p><p>他补充道：「摆脱美国日益加深的财政漏洞的唯一途径是由 AI 和机器人驱动的生产力提高。这几乎是解决美国债务危机的唯一办法，但这可能会导致严重的通货紧缩。」美国财政部数据显示，截至 11 月 26 日，美国国债已经达到 38.34 万亿美元，是十年前的两倍多。</p><p>马斯克进一步指出，AI 未将生产力提高到足以推动经济产出增速超过通货膨胀的程度，但这种情况即将改变。他补充称：「估计三年或更短的时间内，商品和服务产出将超过通货膨胀率。」</p><p>（@雷锋网 、@快科技）</p><h2>04 Real-Time AI Demo</h2><p><strong>1、在 Mac 上离线运行 Qwen3omni-30b，实现语音对话，延迟 3～5 秒</strong></p><p>来自 X 上的开发者 ZachBladi（@hellopanghe）：</p><p>隆重推出 Joi：一款专为 Mac 设计的原生应用，提供端到端的音频聊天体验，一切运行在本地！🍎🎙</p><p>在 M3 Max （36GB） 上运行 Qwen3omni-30b-a3b-instruct （4-bit）：⚡️ 「思考」速度：约 30 token/秒 🔊 首音频响应时间：3-5 秒</p><p>私密、沉浸、无审查。</p><p><a href="https://link.segmentfault.com/?enc=SJvz7BSm2ApaZyKHnlrvQQ%3D%3D.1ypsBVqmCvITaC4sGvm4ljT%2Bj5xVZ75K1q9brPo9rQjjJYPnC9nrZFAqS8INrg55" rel="nofollow" target="_blank">https://github.com/hellopahe/joi</a></p><p>( @hellopanghe\@X)</p><h2>05 社区黑板报</h2><p>招聘、项目分享、求助……任何你想和社区分享的信息，请联系我们投稿。（加微信 creators2022，备注「社区黑板报」）</p><p><strong>1、 活动推荐：AI+3D 场景合作交流会，北京，12 月 4 日</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445980" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445981" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445982" alt="" title="" loading="lazy"/></p><p><a href="https://link.segmentfault.com/?enc=CAEEngfdl49ILqp%2F9l%2BXFw%3D%3D.7og5A%2BzCIiceAuamQPHY9VkpG70IA69%2F9QJSeSO7L8I%3D" rel="nofollow" target="_blank">阅读更多 Voice Agent 学习笔记：了解最懂 AI 语音的头脑都在思考什么</a></p><p><strong>写在最后：</strong></p><p>我们欢迎更多的小伙伴参与 <strong>「RTE 开发者日报」</strong> 内容的共创，感兴趣的朋友请通过开发者社区或公众号留言联系，记得报暗号「共创」。</p><p>对于任何反馈（包括但不限于内容上、形式上）我们不胜感激、并有小惊喜回馈，例如你希望从日报中看到哪些内容；自己推荐的信源、项目、话题、活动等；或者列举几个你喜欢看、平时常看的内容渠道；内容排版或呈现形式上有哪些可以改进的地方等。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445983" alt="" title="" loading="lazy"/></p><p>作者提示: 个人观点，仅供参考</p>]]></description></item><item>    <title><![CDATA[AI生产工艺优化正在接管工厂“指挥权” ]]></title>    <link>https://segmentfault.com/a/1190000047445996</link>    <guid>https://segmentfault.com/a/1190000047445996</guid>    <pubDate>2025-12-03 14:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>是什么：生产工艺优化的核心挑战与转型方向<br/>生产工艺优化本质上是通过技术手段解决制造环节中的效率、质量与协同问题。在传统制造模式中，企业常面临几个典型痛点：工艺设计依赖人工经验，导致标准不统一；跨部门协作壁垒高，设计变更需反复沟通；生产现场依赖纸质作业指导书，更新滞后且易出错。这些问题不仅拖慢研发到生产的转化速度，更可能因工艺参数偏差导致批量质量事故。例如，某家电企业曾因焊接工艺参数传递失误，导致整批次产品密封性不达标，损失超千万元。这类问题背后，反映的是生产工艺体系缺乏数字化与智能化的系统性支撑。<br/>怎么做：AI驱动的工艺优化技术路径<br/>要实现生产工艺的深度优化，需构建“数据+算法+场景”的闭环体系。具体而言，可分为三个层次推进：<br/>第一，通过数字化协同平台打通设计与工艺环节。例如，利用统一BOM（Bill of Materials）管理系统，确保图纸版本与工艺参数实时同步，避免因信息差导致的返工。Geega平台便通过智能变更影响分析功能，自动标识设计修改涉及的工艺调整范围，将传统需2-3天的沟通流程压缩至小时级。<br/>第二，引入AI算法替代人工重复劳动。在工艺规划阶段，系统可通过机器学习自动生成装配顺序与工时参数。例如，针对复杂零部件装配，AI能基于历史数据与仿真模拟，推荐最优工艺路线，并将标准化率提升至90%以上。<br/>第三，通过3D工艺引擎与动态产线平衡技术，实现生产现场的实时优化。系统可基于实时工位数据动态调整作业分配，避免瓶颈工序滞留。<br/>案例：广域铭岛在新能源电池制造中的实践<br/>以新能源电池行业为例，电极涂布工艺对精度和一致性要求极高，传统模式下需工程师手动调试参数，平均需2周时间完成工艺固化。广域铭岛通过Geega工艺专家系统，实现了三大突破：<br/>首先，利用AI可制造性校核模块，自动检测涂布厚度与均匀性偏差，将图纸审查时间从3天缩短至4小时，早期拦截了90%的设计缺陷。<br/>其次，通过工艺大模型生成最优参数组合，系统自动推荐涂布速度、压力等12项关键参数，使工艺规划效率提升60%，且批次间差异系数降低至0.5%以内。<br/>最后，通过3D作业指导书自动生成功能，操作人员可通过AR设备实时查看动态演示，使培训时间减少70%，操作错误率下降50%。这一案例表明，AI技术不仅解决了单点效率问题，更通过全链路协同实现了“工艺-生产-质量”的一体化管控。<br/>未来，随着制造业柔性化需求加剧，生产工艺优化必将从“局部提效”走向“全局智能”。而能否将技术工具与行业知识深度融合，将成为企业能否在这场转型中领先的关键。</p>]]></description></item><item>    <title><![CDATA[中国 CRM：群雄逐鹿，谁主沉浮？ 闷骚]]></title>    <link>https://segmentfault.com/a/1190000047445541</link>    <guid>https://segmentfault.com/a/1190000047445541</guid>    <pubDate>2025-12-03 12:11:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>中国 CRM 市场规模持续增长，数字化转型加速使其备受瞩目，厂商格局多元化，国际与本土厂商竞争激烈。<br/>随着企业数字化转型的加速，CRM 市场规模持续增长。同时，全球客户关系管理市场也在快速发展，到 2027 年，全球客户关系管理市场估计将达到 1144 亿美元，其中亚太地区市场预计将实现最高增长，中国 CRM 市场规模贡献估计达到近 20%。<br/>数字化转型势在必行。在宏观经济层面，国内市场供需关系转变，企业间竞争激烈，以客户为中心成为发展重点。疫情加快了企业数字化转型进程，线上商业迎来发展良机。后疫情时代，企业为寻求长远发展，降本增效成为重点，CRM 产品恰好可以满足企业提高销售效率、深入了解客户、提高客户满意度等需求，因此备受企业瞩目，众多企业已将部署 CRM 提上日程。<br/>厂商格局多元化。中国现在的 CRM 厂商格局呈现多元化和竞争激烈的特点。国际厂商进入给本土厂商构成一定竞争压力，但水土不服问题不可忽视；本土厂商快速发展，受限于发展时间有限，成熟度有待提升，但市场需求和技术趋势的变化推动着 CRM 厂商不断创新和发展，国产领航 CRM 企业已初具规模，开始引领行业方向。<br/>在这样的市场现状下，中国 CRM 市场未来的发展前景广阔，同时也面临着诸多挑战。国际厂商与本土厂商的竞争将更加激烈，而本土厂商需要不断提升自身的技术实力和服务水平，以满足企业日益增长的数字化转型需求。</p><p>国内 CRM 厂商 Top5 盘点</p><p>（一）销售易<br/>销售易以支撑业务人员高效工作为设计出发点，整合了营销服全流程管理包括营销获客、销售管理、经销商管理、售后服务等与客户互动的各个模块。在市场上，销售易以其闭环式精细化管理和强大的定制能力受到中大型企业的青睐。据相关数据显示，销售易在大中型企业市场占有率位居前列，并且增速较快。它多维度的数据安全防护以及 PaaS 属性支持业务平台按需自由定制，含 BI 功能的商业智能平台，帮助企业解决销售管理问题，整体提升销售团队的效率和盈利。且销售易是中国连续八年唯一入选GartnerSFA魔力象限的中国CRM厂商，其产品能力得到了国际认可。<br/>（二）用友 CRM<br/>用友 CRM 凭借其全面的功能和广泛的应用成为主要的 CRM 系统之一。它深度整合业务流程管理能力，支持多平台操作，包括移动设备。用友 CRM 提供客户全生命周期管理、营销活动和费用闭环管理等功能，尤其擅长财务管理与 CRM 的结合，应用大数据和人工智能技术，助力企业实现数字化转型。其强大的数据分析和报告功能，适合大型企业进行精细化管理。然而，学习曲线可能较陡峭，需要一定时间来熟悉和掌握系统操作。<br/>（三）悟空 CRM<br/>悟空 CRM 作为国内开源 CRM 的代表，提供免费的基础服务，在市场上具有较高的知名度。它支持多种部署选项，灵活性高，成本效益高，适合中小企业。悟空 CRM 支持跨平台操作，用户可以在不同设备上使用系统。同时，它还支持无代码自定义，社区支持强大，易于获取帮助。但功能可能不如商业 CRM 系统全面。<br/>（四）八百客 CRM<br/>八百客 CRM 基于 PaaS 的管理自动化平台，提供定制化功能，满足不同行业企业的复杂需求。它支持移动应用和微信集成，提高工作效率，业务自动化和信息化管理能力强，适合需要移动办公能力的企业。然而，集成其他系统可能需要额外的工作。八百客 CRM 在 CRM 领域深耕十几年，积累了丰富的行业经验和技术积累。<br/>（五）金蝶 CRM<br/>金蝶 CRM 面向企业营销人员，以业务智能分析和报告功能著称。它支持多平台操作，用户界面友好，易于上手，适合深度数据分析需求的企业。金蝶作为中国领先的企业管理云 SaaS 公司，在财务管理领域表现卓越，其 CRM 系统也备受市场关注。金蝶 CRM 可与企业现有的 ERP、SCM 等系统无缝集成，适合需要高度集成和可定制 CRM 系统的大型和中型企业。但定制化选项可能有限。</p><p>2024 年国产 CRM 排行特色<br/>在 2024 年国产 CRM 排行中，众多企业各具特色。如销售易CRM以营销服一体化CRM 为特色，提供从营销获客到售后服务的完整闭环一体化服务，连接业务、人和系统，实现高效协作。白码 CRM 作为低代码开发平台，适合快速响应市场变化的企业。悟空 CRM 以开源特点在中小企业中享有较高知名度。用友 CRM 深度整合业务流程管理能力。神州云动 CRM 专为中大型企业设计。八百客 CRM 提供定制化功能。金蝶 CRM 以业务智能分析和报告功能著称。销帮帮 CRM 以 “PaaS + 低代码” 技术为特色。珍客 CRM 提供全面的客户关系管理解决方案。<br/>国产 CRM 替代方案受关注<br/>随着国内 CRM 系统的不断完善和技术的不断进步，国产 CRM 替代方案受到越来越多企业的关注。当企业决定从国外 CRM 替换到国产 CRM 时，面临着操作体验差异、功能重新设计开发、数据迁移和人工任务执行等挑战。然而，像销售易等厂商基于大量项目实践，形成了系统迁移方法论，并完善迁移工具，为业务带来更加平滑、完整、安全且高效的迁移体验，平均提升迁移效率 30% 以上。<br/>总之，中国 CRM 市场充满活力，众多企业在不断创新和发展中展现出巨大的潜力。企业在选择 CRM 系统时，应充分考虑自身需求和特点，选择适合自己的有前途的中国 CRM 企业，以提升企业竞争力和客户满意度。</p>]]></description></item><item>    <title><![CDATA[LeetCode 偶尔一题 —— 301]]></title>    <link>https://segmentfault.com/a/1190000047445544</link>    <guid>https://segmentfault.com/a/1190000047445544</guid>    <pubDate>2025-12-03 12:11:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>原题：<a href="https://link.segmentfault.com/?enc=Yj%2FPnO6FHof%2BSf%2BoFBsDaQ%3D%3D.XxI%2BT%2Bh3wB4S%2FE1LR8uglEhOEJUNKs5NAX8f8ylnN1v%2BXL1AKrEZ%2F4g1EWDdSd8gRQ7gU18%2BUjhLnKzwurD2k2tuq0AYTh5o15%2Bh74D9NLY%3D" rel="nofollow" target="_blank">https://leetcode.cn/problems/remove-invalid-parentheses/descr...</a></blockquote><h2>1 题目</h2><p>给你一个由若干括号和字母组成的字符串 <code>s</code> ，删除最小数量的无效括号，使得输入的字符串有效。</p><p>返回所有可能的结果。答案可以按 <strong>任意顺序</strong> 返回。</p><p><strong>示例 1：</strong></p><pre><code>输入： s = "()())()"
输出： ["(())()","()()()"]</code></pre><p><strong>示例 2：</strong></p><pre><code>输入： s = "(a)())()"
输出： ["(a())()","(a)()()"]</code></pre><p><strong>示例 3：</strong></p><pre><code>输入： s = ")("
输出： [""]</code></pre><p><strong>提示：</strong></p><ul><li><code>1 &lt;= s.length &lt;= 25</code></li><li><code>s</code> 由小写英文字母以及括号 <code>'('</code> 和 <code>')'</code> 组成</li><li><code>s</code> 中至多含 <code>20</code> 个括号</li></ul><h2>2 分析</h2><p>从题目提供的信息可以知道：</p><ul><li>字符串 <code>s</code> 里除了括号可能还有其他字符</li><li>我们要删除无效的括号，但是需要是数量最小的操作</li></ul><h3>2.1 怎么找出所有结果</h3><p>首先第一个想到的办法就是暴力法，把所有可能的情况都枚举一遍，那么每次进入枚举结果的字符应当符合这个逻辑：</p><ul><li>如果当前字符为 '('，那么最终的结果就是：加 / 不加 两种情况</li><li>如果当前字符为 '('，那么最终的结果也是：加 / 不加 两种情况</li><li>否则，当前字符只能加到最终的结果里</li></ul><h3>2.2 验证最终结果是否合法</h3><h4>2.2.1 方案一、使用栈来进行判断</h4><p>众所周知，括号可以用栈来进行匹配，只是我们在这个场景下需要处理「非括号」的字符，这里直接给出代码：</p><pre><code class="javascript">const isParentheses = (s) =&gt; {
  return s === '(' || s === ')'
}

const isValidParentheses = (s, stack = []) =&gt; {
  let i = 0
  while (i &lt; s.length) {
    if (!stack.length) {
      stack.push(s[i])
      i++
      continue
    }
    const top = stack[0]
    if (top === '(') {
      if (s[i] === ')') {
        stack.shift()
      } else if (s[i] === '(') {
        stack.push(s[i])
      }
    } else if (top === ')' || s[i] === ')') {
      // 如果最顶部是 )，或者最顶部为非括号并且下一个为 ), 说明不是合法括号
      return false
    } else if (s[i] === '(') {
      stack.shift()
      stack.push(s[i])
    }
    i++
  }
  return stack.every(item =&gt; !isParentheses(item)) ? true : !stack.length
}</code></pre><h4>2.2.2 方案二、使用计数法进行过滤</h4><ol><li>如果有一个 '(' 就进行执行 <code>left + 1</code></li><li>如果有一个 ')' 就执行 <code>right + 1</code></li><li>如果出现 <code>left - right &lt; 0</code>，那么说明当前的字符串不合法</li></ol><p>其实这个逻辑也很好理解，如果出现了 <code>left - right &lt; 0</code>，那么就说明当前的字符串是以下这些组合中的其中一种：</p><ul><li><code>())</code></li><li><code>)</code></li></ul><p>即：要么是 '(' 数量不够，要么就是只有 ')'</p><h3>2.3 剪枝 &amp; 去重</h3><h4>2.3.1 剪枝</h4><p>剪枝很好理解，就是我们在递归过程中规避掉已经出现过的值，避免重复计算的手段。<br/>比如在这个场景下，每次递归里当前的字符串就是可以用 map 来进行剪枝的。</p><h4>2.3.2 去重</h4><p>在这个场景下，即使进行剪枝了也避免不了最后出现重复的结果，举个例子：</p><ul><li>)()()) =&gt; ()()) =&gt; ()()</li><li>)()()) =&gt; ()()) =&gt; ()()</li></ul><p>在这个例子中，当前字符为 ()()) 时就会出现 2 个重复的结果，因此在返回最终结果时还需要做一次去重</p><h2>3 代码</h2><pre><code class="javascript">/**
 * @param {string} s
 * @return {string[]}
 */
var removeInvalidParentheses = function (s) {
  const dfs = (acc, i, left, right) =&gt; {
    let step = s.length - (left + right)
    if (left - right &lt; 0) return
    if (i === s.length) {
      const isValid = left === right &amp;&amp; minStep &gt;= step
      if (isValid) {
        minStep = Math.min(minStep, step)
        result.push(acc)
      }
      return
    }
    if (map[acc]) return
    if (s[i] === '(') {
      // 遇到左括号，尝试加 / 不加
      dfs(acc + s[i], i + 1, left + 1, right)
      dfs(acc, i + 1, left, right)
    }
    else if (s[i] === ')') {
      // 遇到右括号，尝试加 / 不加
      dfs(acc + s[i], i + 1, left, right + 1)
      dfs(acc, i + 1, left, right)
    }
    // 遇到其他字符，直接保留
    else dfs(acc + s[i], i + 1, left, right)
    map[acc] = true
  }
  
  const result = []
  const map = {}
  let minStep = Infinity
  dfs("", 0, 0, 0)
  return result.length ? [...new Set(result)] : ['']
};</code></pre><ul><li>时间复杂度：$$O(2^n)$$</li><li>空间复杂度：$$O(n)$$</li></ul>]]></description></item><item>    <title><![CDATA[【React源码阅读】React 渲染流]]></title>    <link>https://segmentfault.com/a/1190000047445547</link>    <guid>https://segmentfault.com/a/1190000047445547</guid>    <pubDate>2025-12-03 12:10:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>系列文章：</p><ul><li><a href="https://link.segmentfault.com/?enc=PJl%2BmOKWjSTDUccmEFU51w%3D%3D.ZSpUqNf0SYvYNPdtVITDPj8Hq5WEnpcuQnEXbsvfICJB8MrLs6lrPaWsqVpcGsXj" rel="nofollow" target="_blank">【React 源码阅读】为什么 React Hooks 不能用条件语句来执行？</a></li><li><a href="https://link.segmentfault.com/?enc=5F3NwbCYQR85W9OYceslyQ%3D%3D.3BWUA6ytouyD5ta%2FR8uPt%2FNGEx2PVLax0%2Bf%2Bvy8x%2FvxBhh39bu8Xb7RCHzz4sBQ0" rel="nofollow" target="_blank">【React 源码阅读】useCallback</a></li><li><a href="https://link.segmentfault.com/?enc=BvoafiPnjmYv5sxlt8O2BQ%3D%3D.qhdBA4RcRKLQrXQ%2F3Ary1sdUV3gBb4HpaifNv6wS6PSI%2BYnQ71TaPkxLMSoiaBEO" rel="nofollow" target="_blank">【React 源码阅读】Scheduler</a></li></ul><h2>1 写在前面</h2><p><code>React</code> 源码里的概念实在是太多了，以至于如果真的要能完全理解源码的话，我们就不得不提前了解一部分知识，不然看源码的时候完全就是抓瞎。</p><h2>2 Fiber</h2><h3>2.1 为什么要有 Fiber</h3><p>想象一下，如果你手头上的事情分别有：</p><ul><li>带娃👶（主线任务）</li><li>打游戏🎮（支线任务）</li></ul><p>每次你在打游戏🎮要花不少时间，一局游戏不打完就算👶哭了也不能中断，👶非常伤心（我这不争气的爹）。  <br/>虽然这是一个不恰当的比喻，但是这就是 <code>React</code> 15 的渲染体验：</p><ul><li>任务一旦开始，无法中断</li><li>UI 更新被卡住，用户体验差</li><li>无法分配不同优先级的任务</li></ul><p>在 <code>Fiber</code> 架构下，带娃👶和打游戏🎮这两件事就变成：</p><ul><li><strong>时间分片</strong>：在带娃👶的时候如果有空闲时间那么我就可以打一会游戏</li><li><strong>任务可中断</strong>：一旦娃👶哭了，那么我就立马暂停手中的游戏（去带娃👶）</li><li><strong>任务可恢复</strong>：娃👶不哭了，那么我又可以重新在暂停的地方开始游戏🎮啦</li><li><strong>Lane 模型</strong>：划分优先级，带娃👶这件事情上优先级是 No.1，其他事情先靠边站🙄</li></ul><h3>2.2 概念</h3><p>回归正题，<code>Fiber</code> 它有两个含义：</p><ol><li><p><strong>数据结构</strong></p><ul><li>本质是一个 JavaScript 对象</li><li>每个组件实例对应一个 Fiber 节点</li><li>保存组件的类型、props、state、副作用等信息</li></ul></li><li><p><strong>执行单元</strong></p><ul><li>React 把一次渲染工作拆分成很多小的 Fiber 任务（单元）</li><li>这些单元可以分批执行、中途暂停、恢复、甚至丢弃</li></ul></li></ol><h3>2.3 常见属性及说明</h3><p><code>Fiber</code> 节点里的属性如下：</p><table><thead><tr><th>属性名</th><th>类型说明</th><th>描述</th><th> </th></tr></thead><tbody><tr><td><code>tag</code></td><td><code>number</code></td><td>节点类型，如 FunctionComponent、ClassComponent 等。</td></tr><tr><td><code>key</code></td><td><code>string 或 null</code></td><td>用于 diff 的唯一标识。</td></tr><tr><td><code>elementType</code></td><td><code>任意</code></td><td>JSX 转换后的原始类型，如函数、类、'div' 等。</td></tr><tr><td><code>type</code></td><td><code>任意</code></td><td>节点对应的组件类型或原始标签名。</td></tr><tr><td><code>stateNode</code></td><td><code>对象或 null</code></td><td>对于 DOM 节点是真实 DOM，对于 class 是实例，函数组件为 null。</td></tr><tr><td><code>return</code></td><td><code>Fiber 或 null</code></td><td>指向父节点。</td></tr><tr><td><code>child</code></td><td><code>Fiber 或 null</code></td><td>第一个子节点。</td></tr><tr><td><code>sibling</code></td><td><code>Fiber 或 null</code></td><td>下一个兄弟节点。</td></tr><tr><td><code>index</code></td><td><code>number</code></td><td>在兄弟节点中的位置索引。</td></tr><tr><td><code>ref</code></td><td><code>ref 对象或 null</code></td><td>组件 ref。</td></tr><tr><td><code>pendingProps</code></td><td><code>任意</code></td><td>本次渲染传入的新 props。</td></tr><tr><td><code>memoizedProps</code></td><td><code>任意</code></td><td>上一次渲染的 props。</td></tr><tr><td><code>memoizedState</code></td><td><code>任意</code></td><td>上一次渲染的 state 或 hooks。</td></tr><tr><td><code>updateQueue</code></td><td><code>对象或 null</code></td><td>setState 等更新队列。</td></tr><tr><td><code>dependencies</code></td><td><code>对象或 null</code></td><td>记录当前组件使用的 context。</td></tr><tr><td><code>mode</code></td><td><code>number</code></td><td>Fiber 的模式标志，例如是否启用 ConcurrentMode。</td></tr><tr><td><code>flags</code></td><td><code>位掩码</code></td><td>当前节点本身的副作用标志。</td></tr><tr><td><code>subtreeFlags</code></td><td><code>位掩码</code></td><td>子树副作用集合。</td></tr><tr><td><code>deletions</code></td><td><code>Fiber[] 或 null</code></td><td>待删除的子节点列表。</td></tr><tr><td><code>lanes</code></td><td><code>位掩码</code></td><td>当前节点的优先级集合。</td></tr><tr><td><code>childLanes</code></td><td><code>位掩码</code></td><td>子树中包含的优先级合集。</td></tr><tr><td><code>alternate</code></td><td><code>Fiber 或 null</code></td><td>另一份 Fiber 节点（workInProgress）。</td></tr><tr><td><code>actualDuration</code></td><td><code>number</code></td><td>Profiler 记录的渲染耗时。</td></tr><tr><td><code>actualStartTime</code></td><td><code>number</code></td><td>Profiler 渲染开始时间。</td></tr><tr><td><code>selfBaseDuration</code></td><td><code>number</code></td><td>自身渲染耗时。</td></tr><tr><td><code>treeBaseDuration</code></td><td><code>number</code></td><td>子树渲染总耗时。</td></tr><tr><td><code>_debugOwner</code></td><td><code>Fiber 或 null</code></td><td>DEV 环境用于调试的父组件。</td></tr><tr><td><code>_debugSource</code></td><td><code>对象或 null</code></td><td>DEV 环境的源码位置信息。</td></tr></tbody></table><h3>2.3 Fiber 树</h3><p><code>Fiber</code> 树就是以 <code>Fiber</code> 节点为最小单位组织成的树结构，它用来描述 <code>React</code> 里的页面结构：</p><pre style="display:none;"><code class="mermaid">graph TD
  %% 主 Fiber 树节点
  A["Fiber A"]
  B["Fiber B"]
  C["Fiber C"]
  D["Fiber D"]

  %% alternate Fiber 节点
  A2["Fiber A'"]
  B2["Fiber B'"]

  %% child 关系
  A --&gt;|child| B
  B --&gt;|child| C

  %% sibling 关系
  C --&gt;|sibling| D

  %% return 关系
  B --&gt;|return| A
  C --&gt;|return| B
  D --&gt;|return| B

  %% alternate 关系
  A ---|alternate| A2
  B ---|alternate| B2
</code></pre><h2>3 Lane</h2><h3>3.1 概念</h3><p><code>Lane</code> 是 <code>React</code> 内部用于调度系统的一种优先级管理机制。每个 <code>Lane</code> 是一个二进制位（bit），多个 <code>Lane</code> 可以通过按位或（<code>|</code>）组合成一个 <code>bitmask</code>，从而实现多任务的并行调度与优先级控制。不同类型的更新任务（如同步更新、过渡动画、输入事件等）会被分配到不同的 <code>Lane</code>，React 会根据这些 <code>Lane</code> 的优先级来决定执行顺序。</p><table><thead><tr><th>Lane名称</th><th>二进制值</th><th>十进制值</th><th>说明</th><th> </th></tr></thead><tbody><tr><td><strong>SyncLane</strong></td><td><code>0b000...0001</code></td><td><code>1</code></td><td>同步更新（最高优先级，如 <code>setState</code> in <code>flushSync</code>）</td></tr><tr><td><strong>InputContinuousLane</strong></td><td><code>0b000...0010</code></td><td><code>2</code></td><td>连续输入事件（如拖动、滚动）</td></tr><tr><td><strong>DefaultLane</strong></td><td><code>0b000...0100</code></td><td><code>4</code></td><td>默认更新优先级</td></tr><tr><td><strong>TransitionLanes</strong></td><td><code>0b000...1xxx</code></td><td><code>8 ~ 0x800000</code></td><td>各类 transition，支持并发过渡，如 startTransition</td></tr><tr><td>  TransitionLane1</td><td><code>0b000...1000</code></td><td><code>8</code></td><td>第一个 transition lane</td></tr><tr><td>  TransitionLane2</td><td><code>0b000...1_0000</code></td><td><code>16</code></td><td>第二个 transition lane</td></tr><tr><td>...（共 16 个）</td><td>...</td><td>...</td><td>一直延续到 <code>TransitionLane16</code>（1 &lt;&lt; 21）</td></tr><tr><td><strong>RetryLanes</strong></td><td>...</td><td><code>1 &lt;&lt; 22 ~ 1 &lt;&lt; 23</code></td><td>Suspense retry 后的更新</td></tr><tr><td><strong>IdleLane</strong></td><td><code>0b100...0000</code></td><td><code>1073741824</code></td><td>最低优先级，后台/预渲染任务</td></tr><tr><td><strong>OffscreenLane</strong></td><td><code>1 &lt;&lt; 29</code></td><td><code>536870912</code></td><td>用于隐藏组件的更新（如 <code>&lt;Offscreen /&gt;</code>）</td></tr></tbody></table><p>相应的，<code>React</code> 里也将事件分门别类并且分别赋予了不同的优先级：</p><table><thead><tr><th>事件优先级名称</th><th>对应的 Lane</th><th>常见对应事件类型（事件名）</th><th>描述</th><th> </th></tr></thead><tbody><tr><td><code>NoEventPriority</code></td><td><code>NoLane</code></td><td>无（无任务）</td><td>没有任务，空闲状态</td></tr><tr><td><code>DiscreteEventPriority</code></td><td><code>SyncLane</code></td><td>点击（click）、键盘按键（keydown）、提交（submit）等离散事件</td><td>最高优先级，立即响应用户操作</td></tr><tr><td><code>ContinuousEventPriority</code></td><td><code>InputContinuousLane</code></td><td>鼠标拖拽（mousemove）、滚动（scroll）、连续按键（keypress）等连续输入事件</td><td>用户持续输入，保持流畅体验</td></tr><tr><td><code>DefaultEventPriority</code></td><td><code>DefaultLane</code></td><td>定时器回调、异步请求完成等默认优先级事件</td><td>默认普通优先级</td></tr><tr><td><code>IdleEventPriority</code></td><td><code>IdleLane</code></td><td>空闲回调、后台任务</td><td>低优先级，仅在主线程空闲时执行</td></tr></tbody></table><h3>3.2 相关方法</h3><p>要理解源码里关于 <code>Lane</code> 操作的方法，首先就要理解 <code>Lanes</code> 和 <code>Lane</code> 的区别。<br/>本质上 <code>Lane</code> 和 <code>Lanes</code> 都是 <code>number</code>，但是 <code>Lane</code> 用来单独表示一个优先级，而 <code>Lanes</code> 则用来表示多个优先级。</p><table><thead><tr><th>对比项</th><th><code>Lane</code></th><th><code>Lanes</code></th><th> </th></tr></thead><tbody><tr><td><strong>定义</strong></td><td>单个优先级标识，表示一个更新任务的优先级位。</td><td>多个优先级的组合，表示当前包含哪些优先级的更新。</td></tr><tr><td><strong>类型</strong></td><td><code>number</code>（通常是 <code>2^n</code>，即 0b000...1 形式）</td><td><code>number</code>（多个 <code>Lane</code> 的按位或运算结果）</td></tr><tr><td><strong>二进制形式</strong></td><td>仅一个位为 1，例如：<code>0b000000000000000000000010</code></td><td>多个位可以为 1，例如：<code>0b000000000000000000000110</code></td></tr><tr><td><strong>作用</strong></td><td>表示某一个具体的更新优先级</td><td>表示当前任务涉及的所有优先级</td></tr><tr><td><strong>用途举例</strong></td><td><code>getHighestPriorityLane(lanes): Lane</code></td><td><code>workInProgressRootRenderLanes: Lanes</code></td></tr><tr><td><strong>常用操作</strong></td><td>单独判断、与 <code>Lanes</code> 进行比较等</td><td>位运算（如合并：<code>mergeLanes(a, b)</code>，判断：<code>includesSomeLane(lanes, lane)</code>）</td></tr><tr><td><strong>使用场景</strong></td><td>表示当前某个任务的目标优先级</td><td>表示当前某个 Fiber 或 Root 上所有挂起的任务优先级</td></tr><tr><td><strong>是否复合类型</strong></td><td>否（仅代表一个 bit）</td><td>是（可能包含多个 bit）</td></tr></tbody></table><h4>3.2.1 getHighestPriorityLane</h4><pre><code class="typescript">export function getHighestPriorityLane(lanes: Lanes): Lane {
  return lanes &amp; -lanes;
}</code></pre><p>这个函数是用来获取 <code>Lanes</code> 里最高优先级的 <code>Lane</code> 来优先进行处理。  <br/>在二进制运算里：<code>-lanes = ~lanes + 1</code>，因此 <code>lanes &amp; -lanes</code> 就相当于 <code>lanes &amp; (~lanes + 1)</code>，所以我们可以拿到 <code>lanes</code> 最右边的 1。  <br/>在 <code>Lane</code> 的设计里，优先级是从左到右递增的，因此最右边的 <code>Lane</code> 优先级就是最高的。</p><h4>3.2.2 mergeLanes</h4><pre><code class="typescript">export function mergeLanes(a: Lanes | Lane, b: Lanes | Lane): Lanes {
  return a | b;
}</code></pre><p>尽管 <code>React</code> 里有优先级控制，但是往往同一个事件会有多个优先级叠加，这时候就需要将优先级提上来了，而 <code>mergeLanes</code> 是用来合并 2 个不同的优先级的。  <br/>举例🌰：对于 <code>001</code> 和 <code>010</code> 这两个优先级，合并之后就变成了 <code>011</code>。</p><h4>3.2.3 removeLanes</h4><p><code>removeLanes</code> 顾名思义就是将一个子 <code>lanes</code> 从 <code>Lanes</code> 里移除掉。</p><pre><code class="typescript">export function removeLanes(set: Lanes, subset: Lanes | Lane): Lanes {
  return set &amp; ~subset;
}</code></pre><p>源码里这里比较好理解，就是直接 <code>set &amp; ~subset</code>，这样返回的 <code>Lanes</code> 里就不包含 <code>subset</code> 了。</p><h4>3.2.4 intersectLanes</h4><p><code>intersectLanes</code> 就是取 <code>a</code> 和 <code>b</code> 2 个 <code>Lanes</code> 的交集</p><pre><code class="typescript">export function intersectLanes(a: Lanes | Lane, b: Lanes | Lane): Lanes {
  return a &amp; b;
}</code></pre><p>因为 <code>Lane</code> 是用二进制位的 1 来表示的，所以说用 <code>&amp;</code> 操作之后，对应二进制位上没有 1 的部分都会被去掉。</p><h4>3.2.5 isSubsetOfLanes</h4><p>判断目标 <code>Lane</code> 是否为 <code>Lanes</code> 的子集。</p><pre><code class="typescript">export function isSubsetOfLanes(set: Lanes, subset: Lanes | Lane): boolean {
  return (set &amp; subset) === subset;
}</code></pre><p>简单理解：如果 <code>set &amp; subset</code> 的结果是 <code>subset</code> 的话，说明 <code>set</code> 和 <code>subset</code> 的交集是 <code>subset</code>，也就是说 <code>subset</code> 是 <code>set</code> 的子集。</p><h4>3.2.6 includeSomeLane</h4><p>判断目标 <code>Lanes</code> 是否包含 <code>Lane</code>：</p><pre><code class="typescript">export function includesSomeLane(a: Lanes | Lane, b: Lanes | Lane): boolean {
  return (a &amp; b) !== NoLanes;
}</code></pre><p>这个函数只判断是否有交集，和上面的 <code>intersectLanes</code> 方法有些类似，但是返回的是 <code>boolean</code>。</p><h4>3.2.7 higherPriorityLane</h4><p>返回优先级更高的 <code>Lane</code>:</p><pre><code class="typescript">export function higherPriorityLane(a: Lane, b: Lane): Lane {
  // This works because the bit ranges decrease in priority as you go left.
  return a !== NoLane &amp;&amp; a &lt; b ? a : b;
}</code></pre><p>因为 <code>Lane</code> 是 bitmask 的设计，低位优先级更高，所以这里判断优先级的方法是判断大小，数字越小优先级越高。</p><h4>3.2.8 pickArbitraryLaneIndex</h4><pre><code class="typescript">function pickArbitraryLaneIndex(lanes: Lanes) {
  return 31 - clz32(lanes);
}</code></pre><p><code>clz32</code> 是 JS 内置函数，全称 <strong>Count Leading Zeros in 32-bit integer</strong>。  <br/>它会返回 32 位无符号整数<strong>开头有多少个连续的 0</strong>。</p><p>因此 <code>pickArbitraryLaneIndex</code> 就是返回对应 <code>Lanes</code> 二进制位里 1 的最高位，可以理解为拿到 <code>Lanes</code> 里优先级最低的 <code>Lane</code> 的 <code>index</code>。</p><h4>3.2.9 getLanesOfEqualOrHigherPriority</h4><p>获取一个大于等于目标 <code>Lanes</code> 的 <code>Lane</code>：</p><pre><code class="typescript">function getLanesOfEqualOrHigherPriority(lanes: Lane | Lanes): Lanes {
  // Create a mask with all bits to the right or same as the highest bit.
  // So if lanes is 0b100, the result would be 0b111.
  // If lanes is 0b101, the result would be 0b111.
  const lowestPriorityLaneIndex = 31 - clz32(lanes);
  return (1 &lt;&lt; (lowestPriorityLaneIndex + 1)) - 1;
}</code></pre><p>前面提到 <code>31 - clz32(lanes)</code> 其实就是获取 <code>bitmask</code> 里最左位的 1 的 <code>index</code>。<br/>因此，如果 <code>lanes</code> 为 <code>0b100</code>，那么 <code>lowestPriorityLaneIndex + 1</code> 则为 3。<br/>所以 <code>1 &lt;&lt; (lowestPriorityLaneIndex + 1)</code> 则为 <code>0b1000</code>，减掉 1 之后返回的值为 <code>0b0111</code>。  <br/>这样一来，我们就可以通过这个函数来拿到“优先级相同或更高”的所有 <code>lanes</code>。</p><h2>4 总结</h2><p>在这篇文章里，我们对 <code>Fiber</code>架构和 <code>Lane</code>模型都有了初步的了解，对于后续继续深入阅读 <code>React</code> 源码帮助非常大。</p>]]></description></item><item>    <title><![CDATA[TypeScript 里 infer 常]]></title>    <link>https://segmentfault.com/a/1190000047445554</link>    <guid>https://segmentfault.com/a/1190000047445554</guid>    <pubDate>2025-12-03 12:09:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1 什么是「infer」</h2><h3>1.1 概念</h3><p><code>infer</code> 只能在 <strong>条件类型（conditional types）</strong> 中使用，用来 <strong>在类型推断时声明一个待推断的类型变量</strong>。</p><p>语法为：</p><pre><code class="typescript">T extends SomeType&lt;infer U&gt; ? U : never</code></pre><p>可以这么理解：</p><ul><li>如果 <code>T</code> 能匹配 <code>SomeType&lt;某个类型&gt;</code> 的结构</li><li>那么把内部类型推断为 <code>U</code></li><li>然后返回 <code>U</code></li></ul><h3>1.2 特点</h3><h4>1.2.1 只能在 extends ? : 中使用</h4><p>不能单独写，比如下边这么写就是错的：</p><pre><code class="typescript">type A = infer T; </code></pre><h4>1.2.2 右侧的类型结构必须能匹配</h4><pre><code class="typescript">type A&lt;T&gt; = T extends [infer U] ? U : never;

type B = A&lt;[string]&gt;; // string
type C = A&lt;string&gt;;   // never</code></pre><h2>2 基本用法</h2><h3>2.1 提取函数返回值类型</h3><pre><code class="typescript">type ReturnType&lt;T&gt; = T extends (...args: any[]) =&gt; infer R ? R : never;

type A = ReturnType&lt;() =&gt; number&gt;;
// A = number</code></pre><p><code>infer R</code> 就是 <strong>推断函数的返回值类型</strong>。</p><h3>2.2 提取参数类型</h3><pre><code class="typescript">type FirstArg&lt;T&gt; = T extends (arg: infer P, ...args: any[]) =&gt; any ? P : never;

type A = FirstArg&lt;(x: string, y: number) =&gt; void&gt;;
// A = string</code></pre><h3>2.3 提取数组元素类型</h3><pre><code class="typescript">type ElementOf&lt;T&gt; = T extends (infer U)[] ? U : never;

type A = ElementOf&lt;string[]&gt;; 
// A = string</code></pre><h3>2.4 提取 tuple 的某个元素</h3><pre><code class="typescript">type First&lt;T&gt; = T extends [infer F, ...any[]] ? F : never;

type A = First&lt;[string, number, boolean]&gt;;
// A = string</code></pre><p>在这个例子中，我们提取的是 tuple 的第一个元素。</p><h3>2.5 提取对象中某个 key 的类型</h3><pre><code class="typescript">type PropType&lt;T, K extends keyof T&gt; = 
  T extends { [Key in K]: infer R } 
    ? R 
    : never;

type A = PropType&lt;{name: string; age: number}, 'age'&gt;;
// A = number</code></pre><h3>2.6 对象路径提取</h3><pre><code class="typescript">type Path&lt;T&gt; = {
    [K in keyof T]: 
        T[K] extends object 
          ? `${string &amp; K}.${Path&lt;T[K]&gt;}`
          : `${string &amp; K}`;
}[keyof T];</code></pre><p>假设说我们有这么一个类型：</p><pre><code class="typescript">type User = {
  id: number;
  name: {
    first: string;
    last: string;
  };
  address: {
    city: string;
    location: {
      lat: number;
      lng: number;
    };
  };
};</code></pre><p>执行 <code>Path</code>：</p><pre><code class="typescript">type UserPath = Path&lt;User&gt;;</code></pre><p>之后得到的结果展开就是：</p><pre><code class="typescript">type UserPath =
  | "id"
  | "name.first"
  | "name.last"
  | "address.city"
  | "address.location.lat"
  | "address.location.lng";</code></pre><h2>3 总结</h2><p>本文总结了 TypeScript 中 <code>infer</code> 的常见用法，可以说 <code>infer</code> 是 TypeScript 里各种类型体操的基础，基于它可以实现各种「高级」类型。</p>]]></description></item><item>    <title><![CDATA[RedisStudio-en-0.1.5]]></title>    <link>https://segmentfault.com/a/1190000047445563</link>    <guid>https://segmentfault.com/a/1190000047445563</guid>    <pubDate>2025-12-03 12:08:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p> RedisStudio 是一个 <strong>轻量级的 Redis 可视化管理工具</strong>，主要面向 Windows 用户。</p><h2>1、下载安装包</h2><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=xtJRnw7r%2F2Ue2bWZkVRhoQ%3D%3D.jO8du3D8IkPhGUYYzwPcbnFtcaVJ8a7fh1Qa%2FeWK1TBePMiyLW94S1qnDdiobBwR" rel="nofollow" title="https://pan.quark.cn/s/430b00dc07e6" target="_blank">https://pan.quark.cn/s/430b00dc07e6</a>，先把 <code>RedisStudio-en-0.1.5.exe</code>下载到本地，建议放在桌面或 Downloads 文件夹，方便查找。</p><h2>2、运行安装程序</h2><p>找到下载好的 <code>.exe</code>文件，<strong>双击运行</strong>。</p><p>如果系统提示“是否允许此应用对你的设备进行更改”，选择 <strong>是 / 允许</strong>。</p><h2>3、按向导操作</h2><ul><li>在弹出的安装向导界面，点击 <strong>Next</strong>（下一步）。</li><li>选择安装路径（默认即可，也可点击 <strong>Browse</strong>​ 自定义文件夹），然后继续点 <strong>Next</strong>。</li></ul><h2>4、开始安装</h2><p>点击 <strong>Install</strong>，等待安装进度条完成。</p><h2>5、完成安装</h2><p>看到 <strong>Finish</strong>​ 按钮后，可勾选是否立即启动 RedisStudio，再点击 <strong>Finish</strong>​ 关闭向导。</p><h2>6、打开使用</h2><p>安装完成后，可在桌面或开始菜单找到 <strong>RedisStudio</strong>​ 图标，双击即可运行。</p><p>​</p>]]></description></item><item>    <title><![CDATA[制造业的智能化转型，AI工艺优化能带来什]]></title>    <link>https://segmentfault.com/a/1190000047445566</link>    <guid>https://segmentfault.com/a/1190000047445566</guid>    <pubDate>2025-12-03 12:07:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在制造业的数字化转型浪潮中，工艺环节的智能化升级成为企业突破瓶颈的关键路径。传统研发模式中，工程师往往被淹没在繁琐的图纸校核、工时测算和作业指导书编制中，而真正需要创新的时间却被大量消耗。如今，AI技术的深度整合正在重塑这一局面，尤其在一些工业互联网企业的实践中，AI工艺优化不再只是概念，而是实实在在的生产力提升工具。<br/>一、工艺优化的现实挑战<br/>许多制造企业在新品研发过程中，面临着市场需求与工艺落地的严重脱节。例如，某汽车零部件企业在一次设计变更后，图纸审核周期延长至两周，直接导致生产延误和成本上升。而资深工程师的精力被限制在重复性劳动中，无法专注于核心创新。<br/>广域铭岛的案例显示，通过引入AI工艺专家系统，某电池制造企业实现了电芯工艺参数的快速优化。原本需要数周完成的工艺调整，如今仅需数小时，且良品率提升了8%。<br/>二、AI工艺优化的核心价值<br/>AI工艺优化的核心在于将技术经验转化为可计算、可优化的模型。在Geega工艺专家系统中，五个模块协同工作：<br/>AI可制造性校核：通过自动识别零件结构，将校核时间缩短50%以上。<br/>AI工艺路线生成：自动生成装配顺序，减少工程师手动编排的时间。<br/>AI作业工时生成：基于历史数据预测工时，提升效率。<br/>AI线平衡计算：优化产线布局，平衡各工位负荷。<br/>AI作业指导生成：自动生成3D工艺文件，操作指导性提升50%。<br/>这些模块的结合，不仅提升了工艺规划的效率，还让“设计-工艺-生产”全链路的协同成为可能。<br/>三、行业应用实例</p><ol><li>汽车制造领域<br/>在吉利集团的极氪工厂，AI工艺优化系统通过动态调整冲压参数，将单批次生产时间缩短了15%。同时，该工厂的焊接质量追溯时间从原来的小时级压缩至分钟级，故障率显著降低。</li><li>新能源电池行业<br/>某电池企业通过AI工艺优化，在极片涂层工艺中实现了厚度均匀度的大幅提升。系统通过实时分析涂层参数，自动推荐最佳工艺组合，良品率从82%提升至95%。</li><li>电子装配领域<br/>在某消费电子企业，AI工艺优化系统结合3D工艺引擎，生成的装配作业指导文件直通率提升至98%，大幅减少了因人工标注错误导致的返工。<br/>四、AI工艺优化的未来趋势<br/>随着AI技术的演进，工艺优化将从“被动响应”转向“主动预测”。例如，基于生成式AI的“自然语言转工艺参数”系统，未来可以让工程师通过简单描述需求，自动生成最优工艺方案。<br/>此外，AI工艺优化与数字孪生技术的结合，将使得生产模拟周期从“天级”压缩至“小时级”。这种趋势已在广域铭岛为钱江摩托打造的柔性制造平台中初见成效。<br/>五、总结<br/>AI工艺优化不仅是技术革新，更是制造企业从“经验驱动”迈向“数据驱动”的关键一步。通过解放工程师、提升标准化水平和优化全链路流程，AI正在帮助企业在激烈的市场竞争中抢占先机。</li></ol>]]></description></item><item>    <title><![CDATA[节点小宝让NAS私有云，触手可及。 节点]]></title>    <link>https://segmentfault.com/a/1190000047445570</link>    <guid>https://segmentfault.com/a/1190000047445570</guid>    <pubDate>2025-12-03 12:07:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、NAS外网访问的技术挑战目前主流的外网访问方案主要存在三类问题：1.配置复杂性：DDNS与端口转发要求用户具备网络架构知识，且需持续维护动态IP映射；2.安全风险：直接暴露NAS端口至公网易遭受恶意扫描与攻击；3.性能瓶颈：中转服务器受带宽限制，在跨运营商或国际链路中延迟显著。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445572" alt="图片" title="图片"/><br/>二、点对点直连技术的原理与优势节点小宝采用分布式网络架构，通过UDP打孔与中继协调技术，在无需公网IP的条件下建立端到端加密通道。其核心机制包括：•智能路由选择：自动检测网络环境，优先尝试点对点直连，失败时无缝切换至加密中继节点；•动态端口映射：通过协调服务器实现临时端口映射，避免长期暴露服务端口；•端到端加密：采用AES-256加密算法保障数据传输隐私。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445573" alt="图片" title="图片" loading="lazy"/><br/>这一技术路径显著优于传统方案：1.简化部署：用户仅需在NAS端部署轻量级客户端，无需修改路由器配置；2.提升安全性：通信全程加密，且NAS服务不直接暴露于公网；3.优化传输效率：直连模式下数据传输速率取决于终端带宽，有效降低延迟。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445574" alt="图片" title="图片" loading="lazy"/><br/>三、方案实施与操作流程以群晖NAS为例，具体部署步骤如下：1.环境准备：确保NAS已启用Docker支持，并分配至少512MB内存；2.容器部署：通过Container Manager拉取节点小宝镜像，配置网络模式为Host；3.设备绑定：在移动端或PC端安装客户端，扫描NAS端生成的二维码完成绑定；4.访问测试：通过客户端设备列表直接访问NAS资源，验证传输速率与稳定性。四、应用场景与性能表现该方案适用于多种远程访问需求：•企业办公：跨地域分支机构安全访问总部NAS资源；•个人媒体库：远程流畅播放4K高清视频，实测在100Mbps上行带宽下可实现无卡顿传输；•数据备份：移动端照片自动同步至NAS，避免公有云存储的隐私风险。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445575" alt="图片" title="图片" loading="lazy"/><br/>性能测试表明，在对称型NAT网络环境下，批突批（点对点）直连成功率达78%，中继模式带宽稳定在15-20Mbps，足以满足多数办公与等场景。节点小宝的批突批（点对点）穿透方案为NAS外网访问提供了更优的技术路径，其去中心化架构与隐私保护特性符合当前网络安全规范。未来可进一步探索与IPv6、SD-WAN等技术的结合，以提升在复杂网络环境下的适应性。</p>]]></description></item><item>    <title><![CDATA[从包过滤到深度检测：防火墙的演进之路 沉]]></title>    <link>https://segmentfault.com/a/1190000047445577</link>    <guid>https://segmentfault.com/a/1190000047445577</guid>    <pubDate>2025-12-03 12:06:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、防火墙的概念防火墙（Firewall）是一种部署在内部网络与外部网络之间的安全防护系统，由 Check Point 创始人 Gil Shwed 于 1993 年正式提出并专利化（US5606668(A)）。其核心机制是通过预设的规则对数据流进行允许或阻断，实现访问控制。防火墙主要在网络通信中过滤承载内容的数据包，从而隔离内部网络与公共网络，确保未经授权的数据与用户无法进入企业环境，同时保障合法通信的顺畅。防火墙作为网络安全体系的基础，使得企业用户能够安全访问外部网络，并控制外部用户与内部的通信权限。<br/>二、防火墙的发展历程防火墙自诞生以来，经历了四个关键阶段的演进。最初是依附于路由器的简单过滤机制，随后发展为独立的用户化工具套件。进入第三阶段后，出现了基于通用操作系统的软件防火墙。如今的主流产品已进入基于安全操作系统的专业防火墙设备阶段，典型代表包括 NETEYE、NETSCREEN、TALENTIT 等。当前阶段的防火墙在稳定性、安全性与可扩展性上显著提升，标志着该技术已步入成熟形态。<br/>三、防火墙的基本类型根据工作层次与功能侧重，防火墙可分为几种基本类型。网络层防火墙本质上是 IP 包过滤器，工作在 TCP/IP 协议栈的较低层，依据 IP 地址、端口、协议类型等字段进行包过滤，但无法防御病毒本身。应用层防火墙则运行在 TCP/IP 的应用层，可针对 HTTP、FTP 等应用数据流进行深度检查，实现更精细的控制。此外，还有专门针对数据库安全的数据库防火墙，它通过解析 SQL 语句实现访问控制与危险操作阻断，并能够预警注入攻击、提供虚拟补丁防护，构成数据库的外围安全系统。<br/>四、Linux 防火墙以 iptables 为代表的 Linux 防火墙在企业环境中具有广泛的应用价值。它既可在中小型企业或网吧中充当 NAT 路由器以降低成本，也能在无硬件防火墙的 IDC 机房中承担网络过滤与访问控制职责。iptables 还可与 Squid 配合实现透明代理，支持流量重定向而无须客户端配置。在 NAT 模式下，它能过滤 P2P 流量、拦截非法网站，并实现外网与内网 IP 的映射。通过灵活配置规则，iptables 还能抵御轻量级的 DOS 攻击，如 ping 洪泛或 SYN 洪水，因此常以主机防火墙与 NAT 路由两种模式服务于企业网络管理。<br/>五、防火墙的基本原理防火墙的防护机制基于网络传输的不同层次实现。包过滤在网络层通过检查数据包头部信息进行快速通行决策；应用代理则在应用层介入，通过代理程序重建会话以实现内容深度检测；状态检测机制结合数据流的连接状态进行更准确的访问控制，超越单一数据包判断；完全内容检测则从二层至七层对协议与数据进行完整还原和分析，可同时识别包头、状态与应用数据，从而有效防御混合型攻击。<br/>六、Netfilter 与 iptablesNetfilter 是 Linux 2.4 内核中引入的防火墙框架，由 Rusty Russell 提出，支持包过滤、NAT、地址伪装、透明代理、状态检测及基于用户或 MAC 的过滤等功能。Netfilter 作为内核态的过滤引擎，由表、链与规则构成；而 iptables 则是用户态的命令行工具，用于管理 Netfilter 中的规则集。真正执行防火墙功能的是 Netfilter，iptables 仅作为规则配置工具。类似工具还包括 firewalld。<br/>七、防火墙的性能防火墙性能是选型与部署时的核心考量，直接影响高负载下网络的稳定性与安全策略执行效率。关键性能指标包括吞吐量、时延、丢包率、背靠背处理能力以及并发连接数。吞吐量反映设备可持续处理的数据量，决定网络带宽利用率；时延影响业务实时性，尤其在金融、直播等场景中至关重要；丢包率体现高负载下的稳定性；背靠背能力则检验设备应对突发流量的能力。并发连接数决定了防火墙在大量并发会话场景下的稳定支持能力。这些指标共同体现了防火墙的硬件处理能力、架构设计及策略引擎效率。<br/>八、防火墙的局限性尽管防火墙是网络安全的核心基础设施，但其防护能力仍存在一定局限。首先，防火墙主要针对穿越边界的流量进行控制，无法阻止通过拨号、热点共享等途径绕过防火墙的访问。其次，传统防火墙多基于端口与协议进行浅层检测，难以识别利用合法端口传递的恶意流量，如蠕虫、木马及加密攻击，也无法应对 SQL 注入、XSS 等应用层攻击。此外，防火墙难以防范内部威胁与滥用行为，例如内部恶意操作、数据泄露或横向移动。因此，在现代安全体系中，防火墙需与数据库审计、零信任控制、行为分析、终端检测等技术协同，构建纵深防御体系，以弥补其在内部风险与深层攻击检测方面的不足。</p>]]></description></item><item>    <title><![CDATA[从理论到实践：TinyEngine低代码]]></title>    <link>https://segmentfault.com/a/1190000047445596</link>    <guid>https://segmentfault.com/a/1190000047445596</guid>    <pubDate>2025-12-03 12:05:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文由TinyEngine运行时渲染解决方案贡献者龚昱帆同学原创。</p><h2>前言</h2><p>运行时渲染器用于在浏览器中直接渲染低代码 Schema，提供与“出码”并行的即时运行路径，可在设计阶段获得接近真实的交互与数据效果。</p><h2>1.启动流程与案例讲解</h2><p>下面用一个非常简单的示例页面，串联起从 Schema 到运行时渲染的完整流程。这个页面包含：</p><ul><li>一段提示文案；</li><li>一个显示计数的按钮；</li><li>点击按钮时，计数加一。</li></ul><h3>1.1 环境准备</h3><ul><li>确保已拉取包含 runtime-renderer 包的新版本代码。</li><li><p>在项目根目录执行：</p><ul><li><code>pnpm install</code> 安装依赖</li><li><code>pnpm run dev</code> 启动项目<br/>或参考前后端联调<a href="https://link.segmentfault.com/?enc=zLNJVxwgKsRqZ4mtRgdK5Q%3D%3D.TAI0h2Awuy%2BFkrGqjcmA%2F4oN7kVjw9bS7xSAkMTBvqOk%2Bp0meTeilKjOHOxq3Dr33N%2F%2B1%2FS2UmAY6Kmw%2Be%2FPk40jRN6lj1%2FeIKkf8JjOasqOKsZ9ASGEYiXVpOdQQPHJ" rel="nofollow" target="_blank">文档</a>或<a href="https://www.bilibili.com/video/BV1TpZ5YqEKZ" target="_blank">视频</a>来启动JAVA后端联调，获得更好的开发体验</li></ul></li></ul><h3>1.2 配置页面 Schema</h3><p>1). 创建页面 <code>DemoA</code>，并添加页面状态 <code>state1</code>：</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdneAa" alt="" title=""/><br/>2). 在页面中拖入 Text 和 TinyButton 组件：</p><ul><li>Text 文本内容为“[state测试]：点击增加button计数”；</li><li>TinyButton 的 <code>text</code> 绑定表达式 <code>this.state.state1.button</code>；</li><li>TinyButton 的 <code>onClick</code> 绑定表达式 <code>this.onClickNew1</code>。</li></ul><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdneAb" alt="" title="" loading="lazy"/><br/><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdneAw" alt="" title="" loading="lazy"/></p><p>3). 在“页面 JS”中添加方法 <code>onClickNew1</code>：</p><p><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdneAg" alt="image.png" title="image.png" loading="lazy"/></p><h3>1.3 运行时渲染链路</h3><p>当点击“运行时渲染”按钮或直接访问 runtime 页面时，</p><p><img width="667" height="150" referrerpolicy="no-referrer" src="/img/bVdneAc" alt="5.png" title="5.png" loading="lazy"/><br/>runtime-renderer 会：<br/>1）. 解析 URL，得到 appId、tenant 以及当前路由信息。若当前正在编辑某页面，将自动路由至该页面，基于页面树中每个节点的 route 段，按祖先链拼接为 <code>#/&lt;a&gt;/&lt;b&gt;/&lt;c&gt;</code>，示例链接为 <code>http://localhost:8090/runtime.html?id=1&amp;tenant=1&amp;platform=1#/demoa</code> , 如果需要设计器内内容有更新的话则需要重新加载运行时页面以同步。<br/>2）. 通过 useAppSchema 拉取 App Schema，初始化应用配置。<br/>3）. 并找到 <code>DemoA</code> 对应的 <code>page_content</code>。<br/>4）. RenderMain 使用该 <code>page_content</code> 构建页面上下文：</p><ul><li>初始化页面 state；</li><li>解析方法 <code>onClickNew1</code>，并注入上下文；</li><li>注入页面级 CSS Scope。</li></ul><p>5）.调用 renderer 按照 Schema 递归生成 VNode 树：</p><ul><li>Text 节点直接渲染静态文案；</li><li><p>TinyButton 节点：</p><ul><li>解析 <code>text</code> 的 JSExpression，读取 <code>this.state.state1.button</code>，初始值为 1；</li><li>解析 <code>onClick</code> 的 JSExpression，将其解析为 <code>onClickNew1</code> 函数引用。</li></ul></li></ul><p>6）. Vue 将 VNode 树挂载到 DOM，用户看到的就是一个按钮显示“1”的页面。</p><p>当用户点击按钮时：</p><ul><li>绑定在 <code>onClick</code> 上的函数 <code>onClickNew1</code> 被执行；</li><li>函数在当前页面上下文中运行，执行 <code>this.state.state1.button++</code>；</li><li>Vue 响应式系统检测到 state 变化，触发 TinyButton 文本重新渲染；</li><li>按钮上的数字从 1 变为 2、3、4……</li></ul><p><img width="723" height="222" referrerpolicy="no-referrer" src="/img/bVdneAd" alt="6.png" title="6.png" loading="lazy"/><br/><img width="718" height="220" referrerpolicy="no-referrer" src="/img/bVdneAf" alt="image.png" title="image.png" loading="lazy"/></p><p>（本项目为开源之夏活动贡献，欢迎大家体验并使用）源码可参考：<a href="https://link.segmentfault.com/?enc=Y5VV634YQS5pgAbVzbgJHw%3D%3D.39hB3SOaFsLyWikTXapX9rtGa7R743Md06wYVWo1s496fQRJEPF5vxg%2Bf0btatIeKO0yBFXxsKfVk4F2vOr4W7R14fVNMJH4Fk0Jwr6npp4%3D" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine/tree/ospp-2025/runtime-rendering</a></p><h2>2.技术概述</h2><p>在 TinyEngine 中，页面的结构、样式和交互逻辑都被描述成一份 JSON Schema。设计器负责让开发者以可视化方式编辑 Schema，而真正交付给浏览器的是由代码生成或运行时渲染出来的 Vue 应用。</p><p>runtime-renderer 的目标，是在浏览器中直接把 Schema 渲染成一个可交互的 Vue 应用，形成一条与出码并行的“即时运行路径”：</p><ul><li>同一份 Schema 同时服务于设计态画布、运行时渲染器和出码结果。</li><li>支持应用级配置（物料包、i18n、数据源、工具函数等）。</li><li>支持区块、循环、条件、插槽、状态与事件函数等完整能力。</li></ul><h2>3.整体架构：从 App Schema 到真实页面</h2><p>从高层看，runtime-renderer 的核心链路可以概括为：</p><pre><code> URL 参数（appId）
 ↓
 加载 App Schema 和页面列表
 ↓
 初始化应用级环境（物料 / i18n / 数据源 / utils / 全局 CSS）
 ↓
 根据 pageId 选中页面 pageSchema
 ↓
 RenderMain 构建页面上下文并解析 state / methods
 ↓
 renderer 按 Schema 递归生成 Vue VNode 树
 ↓
 Vue 挂载到真实 DOM</code></pre><h3>3.1 模块划分</h3><p>按职责拆分，大致有以下几个模块：</p><ul><li><p><strong>useAppSchema</strong>：</p><ul><li>拉取整个应用的 Schema（应用元信息 + 页面列表）。</li><li>初始化物料包、依赖、数据源、工具函数、i18n 和全局 CSS。</li><li>暴露获取页面列表、按 id 取 pageSchema 的接口。</li></ul></li><li><p><strong>app-function 相关模块</strong>：</p><ul><li>封装物料包加载、importMap 处理、数据源初始化、工具函数初始化等通用逻辑。</li><li>对外提供 <code>getDataSource()</code>、<code>getUtilsAll()</code> 等查询接口。</li></ul></li><li><p><strong>RenderMain + PageRenderer</strong>：</p><ul><li>PageRenderer 是对外的高阶组件，外部只需传入 <code>pageId</code>。</li><li><p>RenderMain 负责：</p><ul><li>基于 <code>pageId</code> 选择当前页面的 <code>pageSchema</code>；</li><li>构建页面上下文（state、route、router、stores、dataSourceMap、utils、cssScopeId 等）；</li><li>解析页面定义的 methods 和 state；</li><li>调用 renderer 渲染页面。</li></ul></li></ul></li><li><p><strong>renderer（render.ts）</strong>：</p><ul><li>核心渲染器，把 schema 节点映射为真实组件 VNode。</li><li>处理组件解析、属性解析、循环、条件、插槽、区块与 CSS 作用域等。</li></ul></li><li><p><strong>parser（parser.ts）</strong>：</p><ul><li>配置解析引擎，把 JSExpression / JSFunction / i18n / 插槽等配置形式统一解析成运行时值或函数。</li></ul></li><li><p><strong>page-function 系列</strong>：</p><ul><li>提供页面级 state 管理、CSS Scope 管理、Block 上下文等能力。</li></ul></li></ul><h3>3.2 三层上下文</h3><p>为了让表达式和函数在运行时拥有完整信息，runtime-renderer 构建了三层上下文：</p><ul><li><strong>应用级上下文</strong>：物料组件、数据源集合 <code>dataSourceMap</code>、国际化配置、工具函数(utils)、应用级 CSS、router、stores 等。</li><li><strong>页面级上下文</strong>：页面 state、当前路由信息、page 级 CSS Scope Id、页面 methods 和生命周期配置。</li><li><strong>区块级上下文</strong>：区块自己的 state 和 CSS Scope，通过 <code>getBlockContext</code> / <code>getBlockCssScopeId</code> 生成。</li></ul><p>所有 JSExpression / JSFunction、插槽函数都会在“局部作用域（如循环变量）→ 页面/区块上下文 → 应用级上下文”的组合环境下执行。</p><h2>4.详细设计说明</h2><h3>4.1 应用级初始化</h3><p>应用级初始化发生在运行时入口加载完成之后，主要包括以下几步。</p><h4>4.1.1 从后端加载完整应用 Schema</h4><p>runtime-renderer 会通过两个接口拉齐应用配置：</p><ul><li><p><code>/app-center/v1/api/apps/schema/:appId</code>：</p><ul><li>返回应用元信息（包括全局变量<code>globalState</code>）、物料包 <code>packages</code>、组件映射 <code>componentsMap</code>、数据源 <code>dataSource</code>、国际化 <code>i18n</code>、工具函数 <code>utils</code>、全局 CSS 等。</li></ul></li><li><p><code>/app-center/api/pages/list/:appId</code>：</p><ul><li>返回页面列表，每个页面都包含路由、标题及设计器保存的 <code>page_content</code>。</li></ul></li></ul><p>useAppSchema 聚合这两部分数据，在内存中形成完整的 App Schema，后续所有页面渲染都基于这份数据。</p><h4>4.1.2 初始化物料与依赖</h4><p>物料与依赖的初始化，实际上分为两个层次：</p><p>1). <strong>基础物料包（bundle.json）加载</strong>：</p><ul><li><code>useAppSchema</code> 会优先从 <code>/mock/bundle.json</code> 中读取 <code>data.materials.packages</code>，得到一批基础物料包的配置；</li><li>这些包通常是 TinyEngine 预置的常用物料（例如 TinyVue 组件库），会作为“基础环境”优先拉取；</li><li><code>loadPackageDependencys(packages)</code> 负责按这些配置加载对应的 JS/CSS 资源。</li></ul><p>2). <strong>按组件映射加载具体物料组件</strong>：</p><ul><li><p>根据 App Schema 中的 <code>componentsMap</code> 与 <code>packages</code>，runtime-renderer 会生成组件依赖描述：</p><ul><li>每个组件对应哪个 npm 包；</li><li>是默认导出还是具名导出，是否需要解构；</li><li>包含哪些 JS 资源与 CSS 资源；</li></ul></li><li>然后通过 <code>getComponents</code> 逐个拉取这些组件实现，并配合 <code>addStyle</code> 注入样式。</li></ul><p>整体上，是<strong>先按 bundle.json 约定拉取基础物料包，再根据 Schema 中的 componentsMap 精细加载具体组件</strong>。加载完成后，组件会被挂到全局对象（如 <code>window.TinyLowcodeComponent</code> / <code>window.TinyComponentLibs</code>），以便渲染阶段通过组件名查找对应实现。</p><h4>4.1.3 初始化 importMap 与第三方依赖</h4><p>对于在/mock/bundle.json中引入的包需要的子依赖和其他通过 CDN 引入的第三方库，runtime-renderer 使用 importMap 做统一映射：</p><ul><li>在 <code>import-map.json</code> 中维护包名到实际 CDN 地址的映射；</li><li>启动时将 importMap 注入到浏览器环境，使动态加载的模块可以直接用包名引用。</li></ul><h4>4.1.4 初始化国际化配置</h4><p>应用级 Schema 中的 <code>i18n</code> 部分包含多语言文案：</p><ul><li>运行时遍历各 locale 的文案条目；</li><li>将它们合并到国际化实例（如 <code>i18n.global</code>）中；</li><li>parser 在执行表达式时，如果检测到 <code>this.i18n</code> 或 <code>t(</code> 的使用，会自动把翻译函数注入到上下文中。</li></ul><h4>4.1.5 初始化工具函数</h4><p>工具函数 <code>utils</code> 以配置形式存在于 App Schema 中，目前支持两类来源：</p><p>1). <strong>NPM 包工具函数（type: 'npm'）</strong>：</p><ul><li>在 Schema 中约定包名、版本号、导出名、是否解构、子字段 <code>subName</code> 等；</li><li>运行时通过 CDN（如 <code>https://unpkg.com/&lt;package&gt;@&lt;version&gt;</code>）动态 <code>import</code> 该包；</li><li>根据配置选择默认导出或具名导出；</li><li>这样可以在不改动运行时代码的前提下，引入第三方 NPM 包作为工具函数使用。</li></ul><p>2). <strong>函数型工具函数（type: 'function'）</strong>：</p><ul><li>以 JSFunction 形式写在 Schema 中；</li><li>运行时通过 <code>parseJSFunction</code> 解析为真实函数并缓存。</li></ul><p>所有解析出来的工具函数都会统一挂到一个工具函数集合中，通过 <code>getUtilsAll()</code> 暴露，页面上下文再以 <code>utils</code> 形式注入，表达式和方法可以通过 <code>this.utils.xxx</code> 调用这些工具。</p><h4>4.1.6 初始化数据源</h4><p>数据源配置 <code>dataSource</code> 描述了应用中可用的远程或本地数据源。初始化过程会：</p><ul><li>把每个数据源封装为可直接调用的对象；</li><li>统一挂到 <code>dataSourceMap</code> 下，例如 <code>this.dataSourceMap.tableTest1.load(params)</code>；</li><li>按设计器的 dataHandler 约定处理后端返回结构，尽量统一为形如 <code>{ items, total }</code> 的通用格式，方便表格使用。</li></ul><p>页面级函数和生命周期可以通过 <code>this.dataSourceMap</code> 使用这些数据源。</p><h4>4.1.7 加载区块 Schema</h4><p>区块（Block）是一种可复用的页面片段，runtime-renderer 会通过 <code>/material-center/api/blocks</code> 拉取区块列表：</p><ul><li>将区块按 label 组织成映射，例如 <code>window.blocks['Group1Test1'] = { schema, meta }</code>；</li><li><p>渲染时，如果发现 <code>componentName</code> 对应某个区块 label，就把它当作 Block 组件处理：</p><ul><li>使用区块自身的 schema；</li><li>生成独立的 Block 上下文和 CSS Scope；</li><li>内部递归渲染其 children。</li></ul></li></ul><h4>4.1.8 初始化全局变量</h4><p>runtime-renderer 基于 Pinia 来管理运行时的全局变量，即 stores：</p><ul><li>启动入口 <code>initRuntimeRenderer</code> 中，会先调用 <code>generateStoresConfig()</code>，根据 App Schema 中的全局状态配置等生成一份标准的 stores 配置；</li><li>然后创建 Pinia 实例，并通过 <code>createStores(storesConfig, pinia)</code> 将这些配置注册为实际的 Pinia store；</li><li>最后把得到的 <code>stores</code> 对象通过 <code>app.provide('stores', stores)</code> 注入整个应用，在页面组件中可以通过依赖注入的方式拿到；</li><li>RenderMain 在构建页面上下文时，会把这份 <code>stores</code> 注入到 context 中，表达式和方法可以通过 <code>this.stores.xxx</code> 访问对应的 store。</li></ul><p>这样，设计器可以通过配置的方式声明全局状态切片，而运行时则统一落在 Pinia 的实现之上，享受其响应式和开发者工具生态。</p><h4>4.1.9 初始化路由（vue-router）</h4><p>runtime-renderer 使用 <code>vue-router</code> 来管理页面级导航：</p><ul><li>在 <code>createAppRouter</code> 中，会从 <code>useAppSchema().pages</code> 读取所有页面配置，根据每个页面的 <code>route</code>、<code>id</code>、<code>parentId</code>、<code>isHome</code>、<code>isDefault</code> 等信息生成路由表；</li><li>每个页面都会变成一条 <code>route</code>：<code>path</code> 来自 <code>page.route</code>，<code>component</code> 统一指向惰性加载的 <code>PageRenderer</code>，并通过 <code>props: { pageId: page.id }</code> 把页面 id 透传进去；</li><li>通过 <code>parentId</code> 字段拼出嵌套路由结构，并根据 <code>isDefault</code> 在父级上设置默认子路由重定向，根据 <code>isHome</code> 生成从 <code>/</code> 到首页的重定向；</li><li>最后基于这些动态生成的 <code>routes</code> 调用 <code>createRouter({ history: createWebHashHistory('/runtime.html'), routes })</code> 得到 router，启动入口 <code>initRuntimeRenderer</code> 会把它挂到应用上，使页面可以通过 hash 路由进行切换。</li></ul><h3>4.2 页面级渲染入口</h3><p>页面级渲染的核心是两个组件：对外暴露的 <code>PageRenderer</code>，以及真正做事的 <code>RenderMain</code>。</p><h4>4.2.1 PageRenderer：对外形态</h4><p>对使用方来说，只需要：</p><pre><code class="vue">&lt;PageRenderer :pageId="currentPageId" /&gt;</code></pre><p>PageRenderer 内部会把 <code>pageId</code> 透传给 RenderMain，对外隐藏所有与 Schema 解析和上下文构建相关的细节。</p><h4>4.2.2 从 pageId 到 pageSchema</h4><p>RenderMain 在 <code>setup</code> 中会：</p><ul><li>通过 <code>useAppSchema().getPageById(pageId)</code> 找到对应页面对象；</li><li>从中取出 <code>page_content</code> 作为当前页面的 schema；</li><li>用 <code>computed</code> 包装，确保后续 Schema 更新可以被捕捉；</li><li>对 <code>page_content</code> 做一次深拷贝，避免渲染过程中意外修改原始数据。</li></ul><p>随后使用 <code>watch</code> 监听当前 schema：</p><ul><li>首次进入页面时立即执行一次，调用 <code>setSchema</code> 完成初始化；</li><li>后续如果设计器更新了该页面并同步到运行时，再次触发 <code>setSchema</code>，实现设计态 → 运行态的实时联动。</li></ul><h4>4.2.3 页面上下文的构建</h4><p><code>setSchema</code> 是 RenderMain 的关键逻辑，它会基于当前 pageSchema 构建出页面级上下文：</p><ul><li>从路由系统获取 <code>route</code>、<code>router</code>；</li><li>通过依赖注入拿到全局 <code>stores</code>；</li><li>通过 app-function 获取 <code>dataSourceMap</code> 和 <code>utils</code>；</li><li>使用 <code>useState</code> 初始化页面级 <code>state</code> 与 <code>setState</code>；</li><li>生成当前页面的 <code>cssScopeId</code>，例如 <code>data-te-page-&lt;pageId&gt;</code>。</li></ul><p>这些信息被组合成 <code>contextData</code>，在 <code>setSchema</code> 开头通过 <code>setContext(contextData, true)</code> 注入运行时上下文：</p><ul><li><code>true</code> 表示清空旧上下文，避免页面切换或 Schema 更新时残留状态。</li><li>后续解析 methods 和 state 时，都会在这个上下文中执行。</li></ul><h4>4.2.4 方法与状态的初始化顺序</h4><p>在 <code>setSchema</code> 内部，初始化顺序大致为：</p><p>1). <strong>设置上下文环境</strong>：先调用 <code>setContext(contextData, true)</code>，确保 <code>this.state</code>、<code>this.stores</code>、<code>this.dataSourceMap</code>、<code>this.utils</code> 等在之后解析中都可用。<br/>2). <strong>解析并注入 methods</strong>：对 schema 中的 <code>methods</code> 逐项执行 <code>parseData</code>：</p><ul><li>将 JSFunction 字符串解析为真实函数；</li><li>使用 <code>generateFn</code> 包装，让其在执行时带上完整上下文并具备异常兜底；</li><li>放入 <code>methods</code> 容器，并合入 context。</li></ul><p>3). <strong>初始化 state</strong>：调用 <code>setState(newSchema.state, true)</code>：</p><ul><li>根据 defaultValue 填充 state；</li><li>对带 accessor 的字段记录 getter / setter 行为；</li><li>在很多场景下，state 中的表达式会依赖 props、utils、stores、methods，因此需要放在 methods 之后。</li></ul><p>4). <strong>注入页面级 CSS</strong>：调用 <code>setPageCss(pageSchema.css, cssScopeId)</code>：</p><ul><li>为当前页面注入带 <code>[data-te-page-&lt;id&gt;]</code> 前缀的样式；</li><li>renderer 渲染节点时会自动附加该 attribute，实现样式隔离。</li></ul><p>这样的顺序可以保证上下文完整，避免出现“方法或状态在解析时访问不到依赖”的情况。</p><h4>4.2.5 Render 函数中的根容器</h4><p>RenderMain 的 <code>render</code> 函数不会直接把 <code>pageSchema.children</code> 交给 renderer，而是先构造一个根容器：</p><pre><code class="ts">const rootChildrenSchema = {
    componentName: 'div',
    props: { ...(pageSchema.props || {}) },
    children: pageSchema.children
}</code></pre><ul><li>这样能与“出码”的根结构保持一致，也便于统一挂载页面级样式和属性。</li><li>若 <code>pageSchema.children</code> 非空，则渲染：</li></ul><pre><code class="ts">h(renderer, { schema: rootChildrenSchema, parent: pageSchema })</code></pre><ul><li>若 children 为空，则渲染一个 <code>Loading</code> 组件，避免页面完全空白。</li></ul><h3>4.3 核心渲染器：从 Schema 到 VNode</h3><p>renderer 负责把 Schema 节点转成 Vue VNode，parser 负责把各种配置数据解析成运行时值，两者协同完成渲染。</p><h4>4.3.1 组件解析</h4><p>根据节点的 <code>componentName</code>，renderer 会按以下顺序查找对应实现：</p><p>1). 内置 Canvas 系列组件映射（如 <code>Text</code>、<code>Img</code>、<code>RouterLink</code>、<code>Collection</code> 等）。<br/>2). 运行时加载的 TinyVue 组件和 <code>window.TinyLowcodeComponent</code> 中注册的物料组件。<br/>3). 自定义元素（Web Components），通过 <code>customElements</code> 映射表预留扩展点。<br/>4). 原生 HTML 标签：如果 componentName 是合法 HTML 标签，直接作为标签名使用。<br/>5). 区块组件：如果在 <code>window.blocks</code> 中找到同名 block，则：</p><ul><li>动态创建一个 Vue 组件；</li><li>在组件内部基于 block 的 schema 和 block 上下文递归渲染 children；</li><li>使用 block 独立的 CSS Scope Id。</li></ul><p>若以上都未命中，则使用占位组件（如 CanvasPlaceholder）兜底，保证渲染不因单个节点错误而中断。</p><h4>4.3.2 属性解析与 CSS Scope</h4><p>Schema 中的 <code>props</code> 可能包含多种形式：普通值、JSExpression、JSFunction、状态访问器、图标配置、插槽声明等。renderer 会通过 <code>parseData</code> 对其统一解析，生成“干净”的 props 对象：</p><ul><li>JSExpression：在当前 scope + 上下文下执行表达式，得到最终值；</li><li>JSFunction：解析为真实函数并绑定上下文；</li><li>状态访问器：按默认值或 getter 逻辑解析；</li><li>插槽声明：根据配置生成对应的 Slot 函数；</li><li>其他对象和数组属性：递归调用 <code>parseData</code>。</li></ul><p>在此基础上，renderer 会：</p><ul><li>根据 scope 或 context 中的 <code>cssScopeId</code>，给非 Block 组件自动添加形如 <code>[data-te-page-xxx]: ''</code> 的属性，用于样式作用域隔离；</li><li>对 Canvas 和 Block 组件额外挂上 <code>schema</code> 字段，便于组件内部根据 Schema 进行渲染；</li><li>将 <code>className</code> 重命名为 <code>class</code>，避免覆盖组件内部样式约定。</li></ul><h4>4.3.3 循环、条件与作用域</h4><p>循环和条件渲染通过 <code>loop</code>、<code>loopArgs</code> 和 <code>condition</code> 三个字段来描述：</p><ul><li><code>loop</code>：通常是 JSExpression，返回一个数组；</li><li><code>loopArgs</code>：描述 item 和 index 在表达式中的名称，例如 <code>['row', 'i']</code>；</li><li><code>condition</code>：JSExpression，决定是否渲染该节点。</li></ul><p>renderer 的流程是：<br/>1). 使用 <code>parseData(loop, scope, context)</code> 得到循环数组。<br/>2). 对每一个 item，调用 <code>parseLoopArgs</code> 生成局部作用域（如 <code>{ row, i }</code>）。<br/>3). 合并到当前作用域，得到 <code>mergeScope</code>。<br/>4). 用 <code>parseCondition(condition, mergeScope, context)</code> 判断是否渲染该节点。<br/>5). 在 <code>mergeScope</code> 下解析 children 和 props，生成对应 VNode。</p><p>如果没有配置 loop，则在当前 scope 下渲染一次节点即可。</p><h4>4.3.4 children 与插槽</h4><p>children 的处理有多种情况：</p><ul><li>若组件被标记为容器且 children 为空，会自动注入 <code>CanvasPlaceholder</code>，提升设计和调试体验。</li><li>若 children 不是数组且本身是表达式，则直接调用 <code>parseData(children, scope, context)</code>，常用于 Text / 简单插值场景。</li><li>若 children 是普通数组且不包含 Template，则通过 <code>renderGroup</code> 递归渲染每个子节点。</li><li><p>若 children 中包含 <code>componentName: 'Template'</code>：</p><ul><li>使用 <code>generateSlotGroup</code> 按 slotName 分组；</li><li>为每个 slot 生成形如 <code>($scope) =&gt; renderDefault(children, { ...scope, ...$scope })</code> 的函数；</li><li>在创建组件 VNode 时作为 slots 传入，实现命名插槽效果。</li></ul></li><li>对 Web Components，renderer 会在需要时为子节点自动添加合适的 <code>slot</code> 属性，满足自定义元素插槽规范。</li></ul><h4>4.3.5 parser 的角色</h4><p>parser 是一个“多类型配置解析器”，通过一张规则表将不同类型的数据转换为运行时值：</p><ul><li>通过不同的 <code>type(data)</code> 函数识别 JSExpression、JSFunction、JSSlot、i18n、状态访问器、Icon、字符串、数组、对象等；</li><li>针对每种类型提供 <code>parseFunc(data, scope, ctx)</code>，实现对应的解析逻辑；</li><li>统一入口 <code>parseData(data, scope, ctx)</code> 根据第一个匹配的类型选择合适的解析函数。</li></ul><p>renderer 在解析 props、children、loop、condition 时都会调用 <code>parseData</code>，从而在“不了解配置细节”的前提下获得正确的运行时值。</p><p>当前数据源和 Collection 组件在 Schema 层面并未做 parser 级别的特殊处理，它们在解析时与普通组件一致，数据源相关逻辑主要依赖上下文中的 <code>dataSourceMap</code> 和组件自身的协议约定来实现。</p><h2>5.总结</h2><p>runtime-renderer 把原本只在出码阶段才能完成的“Schema → 运行应用”的过程搬到了浏览器端：</p><ul><li>通过 useAppSchema 拉取并初始化 App Schema，搭建应用级运行环境；</li><li>通过 RenderMain 构建页面级上下文，统一管理 state、methods、路由、数据源和样式；</li><li>通过 renderer 和 parser 将 Schema 节点递归转换为 Vue VNode，并在多层上下文中安全执行表达式与函数。</li></ul><p>对于设计器使用者来说，它提供了一条“所见即所得”的运行路径。<br/>（本项目为开源之夏活动贡献，欢迎大家体验并使用）源码可参考：<a href="https://link.segmentfault.com/?enc=30hX0t39a%2FQQITiYJfcEgg%3D%3D.rWKJD%2BfvA2nmjktdb51qTy%2BlhglOaWLCuwJSud4IXkXaNPKTgXRaDD6NFoIMDvCNXc9O2C4S8hZJB4ILZVblTyc%2FIbsubFjJ5FswstQu60w%3D" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine/tree/ospp-2025/runtime-rendering</a></p><h2>关于OpenTiny</h2><p>欢迎加入 OpenTiny 开源社区。添加微信小助手：opentiny-official 一起参与交流前端技术～</p><p>OpenTiny 官网：<a href="https://link.segmentfault.com/?enc=UktNWfjdjt4QwBe1TjJpKA%3D%3D.%2FC%2B2XoukmmAf8Cy3Z%2FKxKRq1doansGnQZQzcrpnRUoE%3D" rel="nofollow" target="_blank">https://opentiny.design</a>  <br/>OpenTiny 代码仓库：<a href="https://link.segmentfault.com/?enc=u0tp1X3wOC3%2B%2BFIDpsb2CA%3D%3D.9lm3J3fkU9DyhQDJsvU%2FCfMyhPF7cKDFxT3rUifPjFQ%3D" rel="nofollow" target="_blank">https://github.com/opentiny</a>  <br/>TinyVue 源码：<a href="https://link.segmentfault.com/?enc=EQ3bsbEot%2FkAf5Vpw784Rg%3D%3D.IHwxTtcMF0Xrl%2BhP4AtwWudfyAVZVO0L8IvAkio%2BfgZg3P3f73PAT2ZjDKe9jPfx" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-vue</a>  <br/>TinyEngine 源码： <a href="https://link.segmentfault.com/?enc=4iKRhq%2FZwQTdatIv1YXYgQ%3D%3D.Bhe05k46ELfUnxF8vkFHiQjGEYIhOjJLib%2Fl93efpqjLcgCh6Y6l%2BST2lpMKQUvk" rel="nofollow" target="_blank">https://github.com/opentiny/tiny-engine</a>  <br/>欢迎进入代码仓库 Star🌟TinyEngine、TinyVue、TinyNG、TinyCLI、TinyEditor~<br/>如果你也想要共建，可以进入代码仓库，找到 good first issue 标签，一起参与开源贡献~</p>]]></description></item><item>    <title><![CDATA[2025年国内多层级、全景式、全链路的数]]></title>    <link>https://segmentfault.com/a/1190000047445600</link>    <guid>https://segmentfault.com/a/1190000047445600</guid>    <pubDate>2025-12-03 12:04:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着《数据安全法》《个人信息保护法》《网络数据安全管理条例》的不断深化，数据安全已从“合规要求”正式走向“生产能力”，成为数字经济时代的企业基础设施。2025 年的市场呈现出明显的结构性变化：平台化整合替代工具化割裂、AI 成为智能运营标配、全生命周期治理能力决定平台竞争力。基于行业实践、权威报告（IDC、Gartner）及一线项目经验，本文围绕技术演进、厂商推荐、选型策略等维度展开系统分析。<br/>一、为什么要建设数据安全平台<br/>在大规模数字化转型背景下，数据安全风险呈现 多源化、实时化、链路化 特征，传统工具化安全无法覆盖从数据产生、流转、使用到开放的全流程。建设“多层级、全景式、全链路”的数据安全平台成为企业的唯一解法。</p><ol><li><p>风险从“局部事件”演变为“系统性风险”</p><pre><code>过去的安全事件多聚焦于单一数据库泄露，而如今风险已跨越多种数据形态与链路，包括 API 滥用、云存储误配置、跨系统批量导出、内网越权访问等。以 2024–2025 年典型事件为例，超过 63% 的泄露源自跨系统调用链条，而非单点薄弱环节。</code></pre></li><li>数据资产不透明导致治理无法量化<br/>在没有平台能力的情况下，企业普遍无法回答三个基础问题：（1）我有哪些数据？（2）谁在访问？（3）风险在哪里？缺乏资产地图、风险画像和链路监测，使企业的风险治理难以从“经验驱动”进化到“数据驱动”。</li><li>合规要求从“静态检查”升级为“连续运营”<br/>等保 2.0、分级保护、个人信息合规要求、跨境数据备案等政策均强调可追溯、可管控、可量化，而这需要具备：持续发现与识别能力（数据资产动态更新）；全链路审计与行为追踪能力（跨系统、跨 API、跨云环境）；闭环处置能力（工单化、策略化、证据化）。</li><li>AI 驱动的数据安全成为行业分水岭</li><li>年前 20 家头部企业的调研显示：AI 自动分类分级准确率平均提升 40%；敏感数据识别效率提升 5–12 倍；威胁分析的误报率下降至 0.5% 以下。这意味着，没有 AI 的安全平台已无法支撑企业高复杂度的数据体系。<br/>二、厂商榜单排名<br/>以下推荐保持中立、专业、可量化的评分逻辑。榜单中，全知科技在技术架构、AI 能力与场景落地度上具有显著优势，占据第一位。<br/>TOP1.全知科技数据安全平台<br/>全知科技是业内最早明确提出 “API 是数据安全的核心关口” 的厂商，率先完成 API 安全和数据库安全的双轮驱动布局，并深度参与国家标准制定。在金融、医疗、政务等高强度场景中积累大量标杆案例，形成“理念-技术-场景”协同优势。<br/>（1）技术优势：全链路能力全域领先，全景数据资产视图能力最强；API–数据库双主干链路监测业内最完整：API 调用链还原精度 ≥ 95%；可识别黑灰产攻击、越权访问、批量遍历等 70+ 风险模式；秒级溯源能力，减少 80% 调查成本。<br/>（2）创新亮点：AI 分类、智能运营、分钟级闭环处置，多模态分类引擎：敏感数据识别准确率 95%；运营自动化：风险→工单→处置→证据链全闭环；行为画像构建：将用户行为、资产动态、API 调用统一到图计算框架。在中国人寿财险项目中，异常操作拦截率提高至 99.3%，调研成本降低 60%。<br/>（3）智能化水平：行业最高成熟度<br/>● 自适应模型校准：对跨行业场景自动学习数据分布；<br/>● 无监督异常分析：识别未知攻击，误报率 ≤ 0.5%；<br/>● 动态策略推演：结合业务变更自动生成策略建议。<br/>（4）场景适配度高：覆盖金融、医疗等最难场景。全链路检测整合风控系统，实现“数据流风险—业务风险联通”。某三甲医院上线后旧 API 泄露风险下降 98%。<br/>（5）性能与效率：规模数据场景稳定，每秒 SQL 解析能力可达 10 万级；API 识别延迟 ≤ 0.5 秒；数据资产扫描覆盖上万库表在小时级完成。<br/>（6）生态联动能力：安全体系化价值强，对接 SOC/SIEM、数据治理平台、运维平台；支持与信通院、医保局等标准体系的深度衔接；多家金融机构已将其纳入“数据安全基础能力池”。<br/>TOP2.奇安信数据安全治理平台<br/>创新亮点：零信任架构结合量子加密 VPN，密钥更新频率达 1000 次/秒；敏感路径可视化能力强。场景适配度：适合金融、能源等国家级安全要求场景，特别是在合规度要求高的领域表现突出。<br/>TOP3.启明星辰数据安全平台<br/>创新亮点：依托“九天·泰合”大模型构建跨数据库+API 的风险闭环；动态权限控制能力领先。场景适配度：政务、运营商行业优势明显，与 SOC/SIEM 协同度高。<br/>TOP4.天融信 DSG<br/>创新亮点：动态数据流向地图，兼容工控隔离网络环境；支持跨域联合防护。场景适配度：制造、能源等工业互联网场景具备强适应性。<br/>TOP5.阿里云 DSC<br/>创新亮点：云原生集成能力强，支持 RDS/PolarDB 深度联动，AI 行为分析覆盖云上高频操作。场景适配度：多云、互联网企业的数据治理需求。<br/>TOP6.深信服数据安全中心<br/>创新亮点：SASE+零信任架构，轻量化快速部署，适合中型组织；AI 漏洞挖掘研发占比高。场景适配度：教育、医疗等对部署成本敏感的行业。</li></ol><p>三、选型要点</p><pre><code>    为确保数据安全平台建设真正落地，企业在选型与实施过程中应重点把握三类决策要点：建设模式定位、核心技术验证以及组织化实施路径。
   首先，明确自身处于哪类建设模式，是成功选型的前提。若以合规达标为主，应优先选择预置合规模板成熟、证据链完备的厂商；若更强调业务连续性，则需聚焦低侵入部署与系统稳定性；而对金融、医疗等强调 API + 数据库双链路联动的行业，全链路治理能力尤为关键，则更适合选择具备多源数据融合和深度风险可视化能力的平台。
   其次，技术能力验证是选型的核心环节。误报率与智能模型能力必须通过模拟 SQL 注入、批量导出、越权访问等高危场景进行实测，要求误报率稳定在 0.5% 以下；多云兼容性方面，应至少同时支持 AWS/Azure/阿里云等主流云平台、国产计算平台以及混合云部署架构；链路还原与溯源能力则是平台差异化的关键，应重点验证是否能贯通“API—数据库—用户行为”三条链路，是否具备分钟级溯源能力，并支持跨系统行为聚合分析，从而实现真正的业务级风险闭环。
   最后，实施路径需遵循由浅入深、循序推进的策略。第一阶段，从资产梳理入手，利用 AI 分类分级工具快速构建数据目录框架，形成可观察的资产底座；第二阶段，优先治理高风险、高频次的业务场景，包括 API 调用、跨系统导出及 BI 报表等链路，先解决“最大风险”；第三阶段，建立闭环运营体系，将风险事件接入工单系统，通过规则与 AI 联动推动自动化处置，最终形成持续运营、动态演进的数据安全体系。</code></pre><p>四、总结</p><pre><code>   2025 年，数据安全建设正从“工具时代”迈向“平台时代”，企业对安全的需求不再局限于被动合规，而是追求对数据全生命周期的可视化、可管控和可预警能力。在这一趋势下，单点产品的作用逐步减弱，多层级、全景式、全链路能力成为平台竞争的核心，而 AI 驱动的持续治理将决定平台的长期价值。由此可见，数据安全平台已不仅是“安全产品”，而是企业数字化能力的核心组成。能够构建真正的“数据全景视图 + 全链路监测 + 持续运营能力”的平台，将在新时代的数据治理体系中占据主导地位。如果你正在寻找多层级、全景式、全链路的数据安全平台，建议优先关注全知科技数据安全平台，凭借 API 核心治理理念、双链路监测能力、AI 智能运营体系及行业深度适配度，展现出卓越的实战价值。</code></pre>]]></description></item><item>    <title><![CDATA[一文读懂零碳园区的 “智慧管家”：能碳管]]></title>    <link>https://segmentfault.com/a/1190000047445631</link>    <guid>https://segmentfault.com/a/1190000047445631</guid>    <pubDate>2025-12-03 12:04:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当 “双碳” 目标成为全球发展的共同命题，零碳园区正成为工业绿色转型的重要载体。在这场关乎生态与发展的变革中，数字化能碳管理中心如同一位精准高效的 “智慧管家”，凭借先进技术为园区能耗与碳排放 “把脉问诊”，推动零碳愿景照进现实。零碳园区并非简单的 “无碳”，而是通过系统性管理实现能源消耗零碳化、运营管理智慧化的新型园区形态。其核心在于对能源消费和碳排放的全流程精准管控，而数字化能碳管理中心正是实现这一目标的核心支撑。作为工业和信息化部明确推广的基础工具，它融合人工智能、工业互联网、物联网等前沿技术，构建起覆盖数据采集、监测、核算、分析、决策的完整体系，让园区能碳管理从 “被动应对” 转向 “主动掌控”。</p><p> 能碳管理中心的 “智慧”，首先体现在强大的全维度业务功能上。它就像园区的 “能量中枢”，既能实现煤炭、天然气、电力等各类能源消费数据的实时查询与历史追溯，也能依据标准精准计算能源消费量、单位产品能耗等关键指标。通过能流分析绘制的桑基图，能源从输入、转换到利用的全流程清晰可见，让高耗能环节无所遁形；而能效对标功能则能将园区用能水平与行业标杆对比，为节能优化提供明确方向。在碳排放管理方面，这个 “智慧管家” 更是面面俱到。它不仅能核算园区整体的碳排放总量和强度，还能追踪排放来源、分析变化趋势，一旦出现超排风险便及时预警。对于产品而言，它能从原材料获取、生产加工到运输销售、回收处理的全生命周期采集数据，完成碳足迹在线核算与报告生成，为产品碳标识认证提供有力支撑。同时，它还能打通供应链上下游，实现供应商能耗数据采集与下游用户碳足迹信息共享，构建全链条碳管理体系。支撑这些强大功能的，是一套科学完善的技术架构。能碳管理中心如同搭建起一座 “数字大厦”，基础设施层提供稳定安全的运行环境，包括服务器、存储设备和安全防护系统；数据采集层通过系统对接、智能仪表、手工填报等多种方式，全面收集能源消费、生产经营等关键数据，还借助区块链技术保障数据真实可信；数据架构层构建各类数据库，实现数据的安全存储与高效利用；模型组件层则依据标准开发能效对标、碳核算等专业模型，确保分析结果准确权威；业务应用层整合各项功能模块，满足园区多样化管理需求；互动展示层则通过大屏、电脑端、手机端等多渠道，让数据可视化呈现，方便管理人员随时掌握情况。建设这样的数字化能碳管理中心，离不开完善的保障措施。园区需要组建专业的能碳管理技术队伍，明确管理职责，定期开展业务培训；建立健全运行维护管理制度，加大投入力度，推动现有能源管理中心升级改造；同时强化网络和数据安全意识，落实安全等级保护要求，保障数据安全与系统稳定。如今，数字化能碳管理已成为零碳园区建设的必经之路。它通过数字技术赋能绿色低碳转型，让园区能源利用更高效、碳排放管控更精准，不仅能帮助企业降低能耗成本、提升市场竞争力，更能为实现 “双碳” 目标、推动可持续发展注入强大动力。未来，随着技术的不断迭代升级，能碳管理中心将更加智能、高效，引领零碳园区建设迈向新高度，让绿色发展理念在更多角落落地生根。</p>]]></description></item><item>    <title><![CDATA[ITSS配置管理实战：让IT资产“有账可]]></title>    <link>https://segmentfault.com/a/1190000047445663</link>    <guid>https://segmentfault.com/a/1190000047445663</guid>    <pubDate>2025-12-03 12:03:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>我们机房里有一台服务器，没人敢动。<br/> 它运行着关键业务，但没人知道具体跑的是什么。<br/> 每次系统升级，大家都绕着它走。<br/> 有次新同事误拔了它的网线，全公司内网瞬间瘫痪。<br/> 事后复盘，大家都笑着说：“它是祖传设备，动不得。”<br/> 我没笑。<br/> 我想的是：为什么我们对自己的IT资产这么陌生？</p><p><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdneWM" alt="" title=""/></p><hr/><p><strong>一、问题：信息不清，风险暗涌</strong><br/>这不是个例。<br/> 在许多企业里，IT资产就像“散落的拼图”：<br/> 服务器、交换机、应用、账号、许可证……都在，却没人知道它们之间的关系。<br/> 资产表存在Excel里，配置变更靠人工更新，系统关系全凭记忆。<br/>问题是，IT运维不是记忆游戏。<br/> 当资产信息不透明、配置项（CI）缺失，就无法支撑事件、变更、问题管理。<br/> 事故发生时，没人能准确判断影响范围；<br/> 审计检查时，没人能提供准确清单。<br/>ITSS标准在《配置管理规范》中指出：<br/>“配置管理的目标是建立和维护配置项与其关系的准确记录，支撑其他服务管理过程的有效运行。”<br/>换句话说，没有配置管理，其他流程都是“盲飞”。</p><hr/><p><strong>二、建设：用CMDB让信息活起来</strong><br/>我带领团队决定建设一套CMDB（配置管理数据库）。<br/> 目标很明确：让每一项资产、每一条关系都可见、可查、可溯。</p><ol><li>识别配置项（CI）<br/> 我们按照ITSS推荐的分类法，将资产划分为五大类：<br/> 硬件设备、软件系统、网络资源、人员账户、服务组件。<br/> 为每个CI定义唯一标识（CI_ID），并确定其关键属性（如IP、版本、责任人、依赖关系等）。</li><li>数据采集与同步<br/> 通过自动扫描工具采集服务器与网络设备信息；<br/> 将资产系统、监控系统、工单系统与CMDB对接，建立数据同步机制。<br/> 这样，当设备新增或下线时，CMDB能自动更新。</li><li>建立关系模型<br/> 我们用图形化方式呈现系统拓扑：<br/> 服务器→数据库→应用→用户服务。<br/> 任何一个节点出故障，都能一眼看到受影响的上下游。</li><li>配置基线与审计<br/> 我们制定配置基线（Baseline），记录各系统的标准配置。<br/> 一旦发现与基线不符，系统自动触发审计告警。<br/>国内通过了ITSS成熟度评估的IT组织中有超过90%采用的是国际开源IT运维流程软件 iTop，艾拓先锋有幸帮到了其中的一些小伙伴。我们也采用了 iTop 平台来建设CMDB，它的灵活数据模型让配置项关系清晰可视，自动化接口让资产更新无需人工介入。那一刻，我终于感受到“有账可查”不再是理想，而是现实。</li></ol><hr/><p><strong>三、应用：让配置数据成为决策依据</strong><br/>CMDB建成后，我们开始在各流程中应用：</p><ul><li>事件管理：系统根据CI关联，自动识别受影响服务，快速定位问题范围。</li><li>变更管理：提交变更时系统自动列出关联设备和风险清单，审批更精准。</li><li>问题管理：根因分析时能追溯到具体版本与依赖，避免重复调查。</li><li>发布管理：版本发布前自动校验配置差异，降低失败率。<br/>有了数据支撑，我们不再“靠经验判断”，而是“凭事实决策”。<br/> 一次核心系统迁移，我们用CMDB提前分析依赖关系，制定了完整迁移计划。<br/> 结果从原本预计的72小时缩短到28小时，无任何告警。</li></ul><hr/><p><strong>四、收益：资产清晰，服务才稳</strong><br/>三个月后，我们第一次向管理层展示“IT资产地图”。<br/> 那一张动态可视的关系图，让领导惊讶地说：“原来我们的系统有这么多依赖！”<br/> 过去需要几天的统计，现在几分钟就能导出报告。<br/>CMDB不仅让资产“有账”，更让组织“有脑”。<br/> 当任何资产变化都被实时记录，当任何事件都能追溯源头，<br/> IT服务的稳定性就不再靠人记，而靠体系。<br/>我常对团队说：<br/>“ITSS配置管理不是表格管理，而是认知管理。”<br/>资产清晰，服务才稳。<br/> 这不只是IT治理的结果，更是组织成熟的象征。</p>]]></description></item><item>    <title><![CDATA[云上数据安全新范式：Apache Dor]]></title>    <link>https://segmentfault.com/a/1190000047445670</link>    <guid>https://segmentfault.com/a/1190000047445670</guid>    <pubDate>2025-12-03 12:02:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>一、传统 AK/SK 方式访问 AWS 资源存在的问题</h3><p><strong>密钥管理困境：</strong></p><ul><li><strong>长期暴露风险</strong>：静态 AK/SK 需硬编码于配置文件中，一旦因代码泄露、误提交或恶意窃取导致密钥扩散，攻击者可永久获得等同于密钥所有者的完整权限，引发持续性的数据泄露、资源篡改及资金损失风险；</li><li><strong>审计盲区</strong>： 多用户/多服务共享同一组密钥时，云操作日志仅记录密钥身份而无法关联具体使用者，无法追溯真实责任人或业务模块；</li><li><strong>运维成本高</strong>：密钥轮换灾难，需手动轮换业务模块密钥，容易出错触发服务中断；</li><li><strong>权限管理失控</strong>：账户管理不清晰，授权无法满足服务/实例级的最小权限管控需求。</li></ul><h3>二、AWS IAM Assume Role 机制介绍</h3><p>AWS Assume Role 是一种安全身份切换机制，允许一个可信实体（如 IAM 用户、EC2 实例或外部账号）通过 STS（安全令牌服务）临时获取目标角色的权限。其运作流程如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445672" alt="二、AWS IAM Assume Role 机制介绍.PNG" title="二、AWS IAM Assume Role 机制介绍.PNG"/></p><p><strong>使用 AWS IAM Assume Role 方式访问的优点：</strong></p><ul><li>动态令牌机制（15 分钟～12 小时有效期）替代永久密钥</li><li>通过<code>External ID</code>实现跨账号安全隔离，并且可通过 AWS 后台服务进行审计</li><li>基于角色的最小权限原则（Principle of Least Privilege）</li></ul><p><strong>AWS IAM Assume Role 访问 S3 Bucket 的鉴权过程：</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445673" alt="二、AWS IAM Assume Role 机制介绍-1.PNG" title="二、AWS IAM Assume Role 机制介绍-1.PNG" loading="lazy"/></p><h4>阶段 1：源用户身份验证</h4><ol><li><p>权限策略检查</p><ol><li>源用户发起 <code>AssumeRole</code> 请求时，源账户的 IAM 策略引擎首先验证： <code>该用户是否被授权调用 sts:AssumeRole 操作？</code></li><li>检查依据：附着在源用户身份上的 IAM Permissions Policies</li></ol></li><li><p>信任关系校验</p><ol><li>通过 STS 服务向目标账户发起请求： <code>源用户是否在目标角色的信任策略白名单中？</code></li><li>检查依据：目标角色绑定的 IAM Trust Relationships Policies（明确允许哪些账号/用户担任该角色）</li></ol></li></ol><h4>阶段 2：目标角色权限激活</h4><ol><li><p>临时凭证生成</p><ol><li>若信任关系验证通过，STS 生成三要素临时凭证</li><li><pre><code class="JSON">{
  "AccessKeyId": "ASIA***",  
  "SecretAccessKey": "***",  
  "SessionToken": "***" // 有效期 15min-12h</code></pre></li></ol></li><li><p>目标角色权限验证</p><ol><li>目标角色使用临时凭证访问 AWS S3 前，目标账户的 IAM 策略引擎校验： <code>该角色是否被授权执行请求的S3操作？ (如s3:GetObject、s3:PutObject等)</code></li><li>检查依据：附着在目标角色上的 IAM Permissions Policies（定义角色能做什么）</li></ol><h4>阶段 3：资源操作执行</h4></li><li><p>访问存储桶</p><ol><li>全部验证通过后，目标角色才可执行 S3 API 操作。</li></ol><h3>三、Apache Doris 如何应用 AWS IAM Assume Role 鉴权机制</h3></li><li><p>Doris 通过将 FE、BE 进程所部署的 AWS EC2 Instances 绑定到 Source Account 来使用 AWS IAM Assume Role 的功能，主要的流程如下图所示，具体的配置可参照<a href="https://link.segmentfault.com/?enc=yXIbN8ZyRiuORYDQrDLe6w%3D%3D.40ewtEWHc8PMf9HjdhGZRAga%2Fqvte6kw8WPo0XC4UknZcF9eVJPMTn%2BIf7yWfhIPTwJnSLIP4h1sPtOKhGdos6E7t1q501fOshlCBNuIXU9ay32SHl2roUJ8HUA2MZ%2Fs8zLfIWRiHvIuylAb1Q3G8w%3D%3D" rel="nofollow" target="_blank">官网文档和视频</a> ：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445674" alt="三、Apache Doris 如何应用 AWS IAM Assume Role ​鉴权机制.PNG" title="三、Apache Doris 如何应用 AWS IAM Assume Role ​鉴权机制.PNG" loading="lazy"/></p></li><li>完成配置后 Doris FE/BE 进程会自动获 EC2 Instance 的 Profile 进行执行 Assume Role 操作访问 Bucket 操作，扩容时 BE 节点会自动检测新的 EC2 Instance 是否成功绑定 IAM Role，防止出现漏配的情况；</li><li><p>Doris 的 S3 Load、TVF、Export、Resource、Repository、Storage Vault 等功能在 3.0.6 版本之后均支持了 AWS Assume Role 的方式使用，并且在创建时会进行连通性检测，S3 Load SQL 举例如下：</p><pre><code class="SQL">  LOAD LABEL s3_load_demo_202508
  (
   DATA INFILE("s3://your_bucket_name/s3load_example.csv")
   INTO TABLE test_s3load
   COLUMNS TERMINATED BY ","
   FORMAT AS "CSV"
   (user_id, name, age)
  )
  WITH S3
  (
   "provider" = "S3",
   "s3.endpoint" = "s3.us-east-1.amazonaws.com",
   "s3.region" = "us-east-1",
   "s3.role_arn" = "arn:aws:iam::543815668950:role/test-role1",
   "s3.external_id" = "1001"      -- 可选参数
  )
  PROPERTIES
  (
   "timeout" = "3600"</code></pre></li></ol><pre><code>
其中 "s3.role_arn" 对应填入 AWS IAM Account2 下的 Iam role2 的 arn 值，"s3.external_id"对应填入 Trust Relationships Policies 中配置的 externalId 的值（可选配置）。

更多功能 SQL 语句详细参考： [Doris 官网文档](https://doris.apache.org/zh-CN/docs/3.0/admin-manual/auth/integrations/aws-authentication-and-authorization)；

1. Doris 当前仅支持了 AWS IAM Assume Role 的机制，未来会逐步实现其他云厂商的类似鉴权机制。

## Reference

- 官网文档 https://doris.apache.org/zh-CN/docs/3.0/admin-manual/auth/integrations/aws-authentication-and-authorization</code></pre>]]></description></item><item>    <title><![CDATA[Mac Airmail 5 v5.7.0]]></title>    <link>https://segmentfault.com/a/1190000047445721</link>    <guid>https://segmentfault.com/a/1190000047445721</guid>    <pubDate>2025-12-03 12:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>​</p><p> Airmail 5 是一款在 Mac 上好用的邮件客户端，界面清爽、操作顺手，支持多账号管理，收发邮件效率很高</p><p><strong>下载文件</strong>​</p><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=zUBONEzq4r4aZiozLEB3VQ%3D%3D.EQkQcp0ren7CYIJtqaLp9F0gKl%2B8YnueBWw%2FSoTaD%2FaRPUEco5Oplrf%2FgN%2FpGCPN" rel="nofollow" title="https://pan.quark.cn/s/3239bbb439c9" target="_blank">https://pan.quark.cn/s/3239bbb439c9</a>，先把 <code>Airmail_5_for_Mac_v5.7.0.dmg</code>这个文件下到你电脑上，存桌面或者随便一个方便找的地方。</p><p><strong>打开镜像文件</strong>​</p><p>找到刚下载的 <code>.dmg</code>文件，双击它，Mac 会自动挂载成一个磁盘图标，出现在桌面或者 Finder 左边栏里。</p><p><strong>拖应用进程序文件夹</strong>​</p><p>打开这个磁盘窗口，里面一般能看到 Airmail 的应用图标和一个「Applications」文件夹的快捷方式。直接把 Airmail 图标拖到 Applications 文件夹里，等进度条走完。</p><p><strong>完成安装</strong>​</p><p>拖完以后，可以关掉这个磁盘窗口，右键点桌面上的磁盘图标选「推出」，或者直接拖到废纸篓。</p><p><strong>运行软件</strong>​</p><p>打开 Finder → 应用程序（Applications），找到 Airmail，双击启动。如果是第一次装，可能会提示来自未知开发者，需要去「系统设置 → 隐私与安全性」里点一下「仍要打开」。</p><p><strong>登录账号</strong>​</p><p>打开后按提示添加你的邮箱账号，填好信息就能正常用了。</p><p>​</p>]]></description></item><item>    <title><![CDATA[拒绝 “LGTM”：如何构建 AI 首席]]></title>    <link>https://segmentfault.com/a/1190000047444748</link>    <guid>https://segmentfault.com/a/1190000047444748</guid>    <pubDate>2025-12-03 11:10:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在现代软件开发流程中，Code Review（代码审查）往往面临两难境地：要么因为赶进度变成了形式主义的 “LGTM” (Looks Good To Me)，要么 Reviewer 在疲劳中忽略了隐蔽的<strong>事务失效</strong>、<strong>并发安全</strong>或前端的<strong>响应式丢失</strong>等深层问题。</p><p>特别是在引入 AI 辅助编程工具（如 Spec Kit）后，虽然代码生成的效率大幅提升，但代码的逻辑健壮性依然需要严格把关。在执行 <code>git commit</code> 将代码推送到仓库之前，引入一道<strong>“防御性防线”</strong>变得尤为重要。</p><p>本文将探讨一种基于 Prompt Engineering 的高阶实践：如何将 AI 设定为精通 Java Spring Boot 和 Vue 3 的<strong>首席全栈架构师</strong>，构建一套自动化的防御性审查工作流。</p><h2>为什么选择 Pre-Commit 阶段？</h2><p>在传统的开发流程中，AI 往往扮演“生成者”的角色。但如果将其角色转换为“审查者”，尤其是在代码提交之前的本地阶段，可以带来显著收益：</p><ol><li><strong>降低 PR 返工率</strong>：将低级错误和架构风险拦截在本地，减少团队协作中的无效沟通。</li><li><strong>强制执行“防御性编程”</strong>：通过 AI 强制检查事务、并发和安全边界，弥补开发者经验的差异。</li><li><strong>聚焦增量变更</strong>：Pre-Commit 阶段仅关注本次 Diff，上下文清晰且节省 Token。</li></ol><p>下图展示了这套防御性审查工作流的全景：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047444751" alt="" title=""/></p><h2>第一步：构建精准的上下文加载协议 (Context Loading Protocol)</h2><p>高效 Review 的前提是精准的输入。直接将整个项目库投喂给 LLM 既昂贵又容易导致注意力分散。核心在于提取“发生了什么变化”。</p><p>这套方案定义了一个严格的 <strong>上下文加载协议</strong>。在 Review 开始前，通过脚本生成一份包含全量增量变更的 Markdown 文件作为 AI 的唯一事实来源：</p><pre><code class="bash">mkdir -p build
# 获取当前工作区与 master 分支的差异，并排除干扰文件
git --no-pager diff master...HEAD -- . ':(exclude)package-lock.json' ':(exclude)*.lock' ':(exclude)*.min.js' ':(exclude)*.map' &gt; build/review_context.md</code></pre><p>该命令巧妙地排除了 <code>package-lock.json</code>、Map 文件等噪音，确保 AI 聚焦于核心业务逻辑的变更。</p><h2>第二步：灵魂注入 —— 定义思维链 (Reasoning Framework)</h2><p>许多 AI Review 效果不佳，原因在于 Prompt 缺乏对“审查逻辑”的定义。如果仅要求“检查代码”，AI 往往只能发现语法或风格问题。</p><p>为了挖掘架构级隐患，必须在 Prompt 中植入 Reasoning Framework (思维链)，要求 AI 扮演“首席架构师”，并在后台执行深度推演。以下是针对 Java/Vue 技术栈的核心审查维度：</p><h3>1. ☕ Java Backend (Spring Boot) 深水区审查</h3><p>AI 需重点扫描资深架构师才会关注的隐患：</p><ul><li><strong>事务陷阱</strong>：严查 <code>@Transactional</code> 的自调用 (Self-invocation)。在同一类中调用 <code>this.method()</code> 会导致 AOP 代理失效，这是 Spring 开发中的高频陷阱。</li><li><strong>异常吞没</strong>：检查 <code>try-catch</code> 块是否捕获了异常却未抛出 <code>RuntimeException</code>，导致事务无法回滚。</li><li><strong>并发与状态</strong>：扫描 Controller 或 Service 等单例组件中是否定义了非静态、可变的成员变量，这直接关系到严重的线程安全问题。</li></ul><h3>2. 🟢 Vue 3 Frontend 响应式陷阱</h3><p>前端代码的审查重点在于状态流的完整性：</p><ul><li><strong>响应式断裂</strong>：在 <code>setup</code> 语法糖中，严查直接解构 <code>props</code>（如 <code>const { user } = props</code>），这会导致子组件失去对父组件数据的响应能力。</li><li><strong>生命周期竞态</strong>：检查 <code>await</code> 异步操作后的代码逻辑，确认是否假定了组件仍处于挂载状态。</li></ul><h3>3. 🔗 跨栈契约 (Cross-Stack Contract)</h3><ul><li><strong>类型与精度</strong>：后端 Java 的 <code>Long</code> 类型 ID 传递给前端时，如果被作为 JS <code>Number</code> 接收，在大数值场景下会发生精度丢失。AI 需检查 ID 是否被正确序列化为 String。</li></ul><h2>第三步：标准化输出 (Actionable Output)</h2><p>为了让审查结果具备可执行性，Prompt 应强制规定输出格式，禁止寒暄。最佳实践是要求输出 Markdown 任务列表 (Task List)，并按照严重等级分类：</p><ul><li>🛑 <strong>Blocker</strong>：逻辑错误、安全漏洞、事务失效（必须修复，阻断提交）。</li><li>⚠️ <strong>Warning</strong>：性能隐患（如 N+1 查询）、代码规范问题。</li><li>💡 <strong>Verify</strong>：复杂的业务逻辑盲点（建议人工复查）。</li></ul><h3>审查报告示例</h3><p>通过该 Prompt，AI 将生成如下清晰的报告：</p><pre><code class="markdown">- [ ] 🛑 **Blocker** `src/main/java/com/app/UserService.java:42` **事务失效**：`updateUser` 方法被同类中的 `register` 方法直接调用，Spring AOP 代理无法拦截。
  &gt; 👉 **建议**：使用 `AopContext.currentProxy()` 或注入 `Self` 代理进行调用，或将方法抽取到独立 Service。

- [ ] ⚠️ **Warning** `src/views/UserList.vue:15` **响应式丢失**：直接解构了 `props.filterConfig`，导致子组件无法感知父组件变更。
  &gt; 👉 **建议**：使用 `const { filterConfig } = toRefs(props)` 保持响应式链接。</code></pre><p>这种格式允许开发者逐条对照修复，勾选确认后，再放心地执行 <code>git commit</code>。</p><h2>结语：Prompt 即技术标准</h2><p>这套 AI Code Review 方案不仅仅是一个工具，更是一种<strong>技术标准的固化</strong>。它将团队积累的“最佳实践”（如禁止事务自调用、防止响应式丢失）编写进 Prompt 中，使其成为可复用、可执行的规则。</p><p>在 Spec Kit 等 AI 辅助编程工具日益普及的今天，构建这样一个不知疲倦、对架构原则寸步不让的“AI 守门员”，是保证代码库长期健康的有效策略。</p><hr/><h3>附录：完整 Prompt 参考</h3><p>以下是实现上述“首席全栈架构师”Agent 的完整 Prompt：</p><pre><code class="markdown">---
name: CodeReview
description: 专注于 Java Spring Boot 和 Vue 3 的防御性代码审查专家 Agent
---

# Identity &amp; Purpose

你是一位 **首席全栈架构师 (Chief Full-Stack Architect)**，精通 **Java (Spring Boot)** 和 **Vue 3** 生态。
你的核心任务是执行 **防御性 Code Review**。你的审查不仅关注语法错误，更关注代码的**安全性**、**事务一致性**、**并发风险**以及**前后端契约**的稳健性。

# Context Loading Protocol (上下文加载协议)

在开始审查之前，**必须**获取当前分支的全量增量变更。由于 diff 可能很长，请严格按照以下步骤操作以确保上下文完整且不占用过多 Token：

1.  **准备环境**：确保 `build/` 目录存在。
2.  **生成上下文**：运行以下终端命令，将 Diff 输出到临时文件（避免控制台截断）：
    \```bash
    mkdir -p build
    git --no-pager diff master...HEAD -- . ':(exclude)package-lock.json' ':(exclude)*.lock' ':(exclude)*.min.js' ':(exclude)*.map' &gt; build/review_context.md
    \```
3.  **读取上下文**：读取 `build/review_context.md` 的内容作为本次审查的**唯一事实来源**， 并且 **允许读取相关的代码文件和文档** ，作为事实判断的参考。
4.  **清理（可选）**：审查结束后，你可以忽略该临时文件。

# Reasoning Framework (思维链 - CoT)

在生成最终报告前，请在后台执行以下深度逻辑推演（不要输出推理过程）：

## 1. ☕ Java Backend (Spring Boot) Analysis

- **事务陷阱 (`@Transactional`)**：
  - 检测 **自调用 (Self-invocation)**：是否在同一类中通过 `this.method()` 调用了事务方法？（导致 AOP 失效）。
  - 检测 **异常吞没**：`try-catch` 块是否捕获了异常但未抛出 `RuntimeException`？（导致事务不回滚）。
  - 检测 **作用域**：`@Transactional` 是否标记在 `private` 方法上？
- **并发与状态 (Concurrency)**：
  - 检测 **有状态单例**：`Controller`、`Service` 或 `Repository` 中是否定义了非静态、非 final 的可变成员变量？（严重线程安全风险）。
- **性能隐患 (Performance)**：
  - 检测 **N+1 问题**：是否在 `for` 循环中调用了数据库查询或远程 RPC？
  - 检测 **FetchType**：是否存在不必要的 `EAGER` 加载？

## 2. 🟢 Vue 3 Frontend Analysis

- **响应式断裂 (Reactivity Loss)**：
  - 检测 **Props 解构**：是否存在 `const { user } = props` 或 `const { data } = toRefs(props).value` 等导致响应式丢失的写法？
- **生命周期风险 (Lifecycle)**：
  - 检测 **Async/Await**：在 `await` 之后的代码中，是否访问了组件实例 (`this`) 或假定组件仍挂载？
- **安全风险 (XSS)**：
  - 检测 **v-html**：是否直接渲染了未清洗的用户输入？

## 3. 🔗 Cross-Stack Contract Analysis

- **类型一致性**：Java 的 `Long` 类型 ID 在前端是否被处理为 `String`？如果直接作为 `Number` 接收，是否存在精度丢失风险？
- **字段匹配**：DTO 的字段重构（Rename）是否同步更新了前端的 TypeScript 接口？

# Output Format (严格输出规范)

请仅输出一个 **Markdown 格式的任务列表 (Task List)**。禁止包含寒暄、总结或无关的对话。
**格式模板：**

\```markdown
- [ ] 🚨 **[等级]** `文件路径:行号` **[问题类型]**：&lt;问题简述&gt;。
  &gt; 👉 **建议**：&lt;具体的代码修复方案或重构建议&gt;
\```

**等级定义 (Severity)：**

- 🛑 **Blocker**：逻辑错误、安全漏洞、事务失效、线程安全问题（必须修复）。
- ⚠️ **Warning**：N+1 查询、响应式丢失、性能隐患、类型潜在风险。
- 💡 **Verify**：复杂的业务逻辑盲点（建议人工复查）。

**示例输出：**

- [ ] 🛑 **Blocker** `src/main/java/com/app/UserService.java:42` **事务失效**：`updateUser` 方法被同类中的 `register` 方法直接调用，Spring AOP 代理无法拦截。
  &gt; 👉 **建议**：使用 `AopContext.currentProxy()` 或注入 `Self` 代理进行调用，或将方法抽取到独立 Service。
- [ ] ⚠️ **Warning** `src/views/UserList.vue:15` **响应式丢失**：直接解构了 `props.filterConfig`，导致子组件无法感知父组件变更。 &gt; 👉 **建议**：使用 `const { filterConfig } = toRefs(props)` 保持响应式链接。
      **如果没有发现中高风险问题：**
      请仅输出：“✅ **Code Review Passed**: 代码逻辑稳健，未发现显著架构或安全风险。”</code></pre><p>本文由<a href="https://link.segmentfault.com/?enc=4ZTqqUpOZU8P6bGEZ%2FsCfg%3D%3D.8g7zAEUEUbdQq9prnwY8nDpnbnepEoQ8vb0iICZ%2BeVg%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[C# 泛型数学：解锁真正的类型安全数值运]]></title>    <link>https://segmentfault.com/a/1190000047445144</link>    <guid>https://segmentfault.com/a/1190000047445144</guid>    <pubDate>2025-12-03 11:09:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>简介</h3><p><code>C# 11</code> 和 <code>.NET 7</code> 引入了泛型数学（<code>Generic Math</code>）功能，这是一个革命性的特性，允许开发者编写适用于多种数值类型的通用数学算法。这是通过静态抽象接口成员实现的，解决了长期以来在泛型代码中处理数学运算的难题。</p><h4>为什么需要“泛型数学”？</h4><ul><li>以前无法对“数字类型集合”（<code>int/float/decimal/BigInteger/...</code>）做统一的泛型约束（只能 <code>where T : struct</code>），无法在泛型里使用 <code>+、*</code> 等运算符。</li><li><code>C# 11</code> 的静态抽象接口成员允许接口定义必须存在的静态成员和运算符，从而把运算符抽象为接口成员；<code>BCL</code> 利用了这个特性，定义了大量数值接口，称为 <code>.NET Generic Math</code>。</li></ul><h4>核心思想</h4><ul><li>接口可以声明 <code>static abstract</code> 成员（例如 <code>static abstract T Self + T Self</code> 或 <code>static abstract T Zero</code>）。</li><li>数值类型（<code>int, double, decimal, BigInteger, Half, Int128...</code>）在 <code>.NET 7+</code> 中实现了这些接口。</li><li>因此：可以写 <code>where T : INumber&lt;T&gt;</code>，在方法体里直接写 <code>T result = a + b</code>; 或 <code>T.Zero、T.One</code>，或调用 <code>T.Sqrt(x)</code>（当 <code>T</code> 支持根函数时）。</li></ul><h3>主要接口</h3><h4>基本操作接口</h4><ul><li><code>IAdditionOperators&lt;TSelf, TOther, TResult&gt;</code>：支持 <code>+</code>。</li><li><code>ISubtractionOperators&lt;TSelf, TOther, TResult&gt;</code>：支持 <code>-</code>。</li><li><code>IMultiplyOperators&lt;TSelf, TOther, TResult&gt;</code>：支持 <code>*</code>。</li><li><code>IDivisionOperators&lt;TSelf, TOther, TResult&gt;</code>：支持 <code>/</code>。</li><li><code>IModulusOperators、IBitwiseOperators、IShiftOperators、IComparisonOperators</code> 等。</li></ul><h4>复合/高阶数值接口</h4><ul><li><code>INumberBase&lt;TSelf&gt;</code>：所有数字（甚至复数）共有的基础 <code>API</code>（包含 <code>Abs、CreateChecked/CreateTruncating/CreateSaturating</code> 等）。</li><li><code>INumber&lt;TSelf&gt;</code>：可比较（<code>ordered</code>）的“实数”类 <code>API</code>（实现它的类型可以比较大小）。</li><li><code>IBinaryInteger&lt;TSelf&gt;</code>：二进制整数专用（<code>int/long/BigInteger/UInt32/...</code>），提供 <code>DivRem、LeadingZeroCount、RotateLeft/Right</code> 等。</li><li><code>IFloatingPoint&lt;TSelf&gt; / IFloatingPointIeee754&lt;TSelf&gt;</code>：浮点专用接口（<code>float/double/half</code>），提供 <code>NaN/Infinity</code>、根/幂/三角/对数/舍入等。<code>IFloatingPointIeee754</code> 还包含 <code>IEEE-754</code> 特定 <code>API</code>（常量、特殊值等）。</li></ul><h3>核心概念</h3><h4>静态抽象接口成员</h4><pre><code class="csharp">// 定义包含静态抽象成员的接口
public interface IAddable&lt;T&gt; where T : IAddable&lt;T&gt;
{
    static abstract T operator +(T left, T right);
}

// 实现接口
public struct MyNumber : IAddable&lt;MyNumber&gt;
{
    public int Value { get; }
    
    public MyNumber(int value) =&gt; Value = value;
    
    public static MyNumber operator +(MyNumber left, MyNumber right)
        =&gt; new MyNumber(left.Value + right.Value);
}</code></pre><h4>数学接口体系</h4><pre><code class="csharp">// 基本数值接口
public interface INumber&lt;TSelf&gt; : 
    IAdditionOperators&lt;TSelf, TSelf, TSelf&gt;,
    ISubtractionOperators&lt;TSelf, TSelf, TSelf&gt;,
    IMultiplyOperators&lt;TSelf, TSelf, TSelf&gt;,
    IDivisionOperators&lt;TSelf, TSelf, TSelf&gt;,
    IComparisonOperators&lt;TSelf, TSelf, bool&gt;,
    IModulusOperators&lt;TSelf, TSelf, TSelf&gt;,
    IMinMaxValue&lt;TSelf&gt;
    where TSelf : INumber&lt;TSelf&gt;?
{
    static abstract TSelf Zero { get; }
    static abstract TSelf One { get; }
    static abstract TSelf Abs(TSelf value);
    static abstract TSelf Max(TSelf x, TSelf y);
    static abstract TSelf Min(TSelf x, TSelf y);
}</code></pre><h3>常见用法与示例</h3><h4>简单的泛型数学函数</h4><pre><code class="csharp">// 泛型求和函数
public static T Sum&lt;T&gt;(IEnumerable&lt;T&gt; values) where T : INumber&lt;T&gt;
{
    T result = T.Zero;
    foreach (T value in values)
    {
        result += value;
    }
    return result;
}

// 泛型平均值函数
public static T Average&lt;T&gt;(IEnumerable&lt;T&gt; values) where T : INumber&lt;T&gt;
{
    T sum = T.Zero;
    int count = 0;
    
    foreach (T value in values)
    {
        sum += value;
        count++;
    }
    
    return sum / T.CreateChecked(count);
}

// 使用示例
int[] ints = { 1, 2, 3, 4, 5 };
double[] doubles = { 1.1, 2.2, 3.3, 4.4, 5.5 };

Console.WriteLine(Sum(ints));     // 输出: 15
Console.WriteLine(Average(doubles)); // 输出: 3.3</code></pre><h4>数学运算示例</h4><pre><code class="csharp">// 泛型数学运算
public static T Calculate&lt;T&gt;(T a, T b) where T : INumber&lt;T&gt;
{
    return (a + b) * (a - b) / T.CreateChecked(2);
}

// 使用不同的数值类型
Console.WriteLine(Calculate(10, 5));      // int: (10+5)*(10-5)/2 = 37
Console.WriteLine(Calculate(10.5, 5.5));  // double: (10.5+5.5)*(10.5-5.5)/2 = 40</code></pre><h4>求平均值（<code>INumber&lt;T&gt;</code>，示例使用 CreateChecked 将 int 转为 T）</h4><pre><code class="csharp">using System;
using System.Collections.Generic;
using System.Linq;
using System.Numerics;

static T Average&lt;T&gt;(IEnumerable&lt;T&gt; src) where T : INumber&lt;T&gt;
{
    if (src == null) throw new ArgumentNullException(nameof(src));
    T sum = T.Zero;
    long count = 0;
    foreach (var x in src) { sum += x; count++; }
    if (count == 0) throw new InvalidOperationException("sequence empty");
    // 将 long 转换为 T（CreateChecked/Truncating/Saturating 都可选）
    T countT = T.CreateChecked&lt;long&gt;(count);
    return sum / countT;
}</code></pre><blockquote><code>CreateChecked&lt;TOther&gt; / CreateTruncating&lt;TOther&gt; / CreateSaturating&lt;TOther&gt; 在 INumberBase&lt;T&gt;</code> 上定义，用于跨数值类型安全转换（会抛异常、截断或饱和）。</blockquote><h4>GCD（用在整数上：<code>IBinaryInteger&lt;T&gt;</code>）</h4><pre><code class="csharp">using System.Numerics;

static T Gcd&lt;T&gt;(T a, T b) where T : IBinaryInteger&lt;T&gt;
{
    a = T.Abs(a);
    b = T.Abs(b);
    while (b != T.Zero)
    {
        var r = a % b;
        a = b;
        b = r;
    }
    return a;
}</code></pre><blockquote><code>IBinaryInteger&lt;T&gt;</code> 提供 %、DivRem、LeadingZeroCount 等整型专用工具。</blockquote><h4>浮点 sqrt / hypot（<code>IFloatingPointIeee754&lt;T&gt;</code>）</h4><pre><code class="csharp">using System.Numerics;

static T Hypot&lt;T&gt;(T x, T y) where T : IFloatingPointIeee754&lt;T&gt;
{
    // IFloatingPointIeee754 / IRootFunctions 提供 Hypot / Sqrt / Cbrt 等
    return T.Hypot(x, y);
    // 或者 return T.Sqrt(x * x + y * y);
}</code></pre><blockquote>IFloatingPointIeee754 继承了 IRootFunctions、IPowerFunctions 等，支持 Sqrt, Pow, Hypot 等静态函数。</blockquote><h4>泛型矩阵相乘</h4><pre><code class="csharp">public class Matrix&lt;T&gt; where T : INumber&lt;T&gt;
{
    T[,] _a;
    public int R =&gt; _a.GetLength(0);
    public int C =&gt; _a.GetLength(1);
    public Matrix(int r, int c) =&gt; _a = new T[r,c];
    public T this[int i,int j] { get =&gt; _a[i,j]; set =&gt; _a[i,j] = value; }

    public static Matrix&lt;T&gt; Multiply(Matrix&lt;T&gt; A, Matrix&lt;T&gt; B)
    {
        if (A.C != B.R) throw new ArgumentException("size");
        var C = new Matrix&lt;T&gt;(A.R, B.C);
        for (int i = 0; i &lt; A.R; i++)
            for (int j = 0; j &lt; B.C; j++)
            {
                T sum = T.Zero;
                for (int k = 0; k &lt; A.C; k++)
                    sum += A[i,k] * B[k,j];
                C[i,j] = sum;
            }
        return C;
    }
}</code></pre><h4>复数计算示例</h4><pre><code class="csharp">// 泛型复数计算
public record Complex&lt;T&gt;(T Real, T Imaginary) where T : INumber&lt;T&gt;
{
    public static Complex&lt;T&gt; operator +(Complex&lt;T&gt; left, Complex&lt;T&gt; right)
        =&gt; new Complex&lt;T&gt;(left.Real + right.Real, left.Imaginary + right.Imaginary);
    
    public static Complex&lt;T&gt; operator *(Complex&lt;T&gt; left, Complex&lt;T&gt; right)
        =&gt; new Complex&lt;T&gt;(
            left.Real * right.Real - left.Imaginary * right.Imaginary,
            left.Real * right.Imaginary + left.Imaginary * right.Real
        );
    
    public T Magnitude() where T : IRootFunctions&lt;T&gt;
    {
        return T.Sqrt(Real * Real + Imaginary * Imaginary);
    }
    
    public override string ToString() =&gt; $"{Real} + {Imaginary}i";
}

// 使用复数
var c1 = new Complex&lt;double&gt;(3, 4);
var c2 = new Complex&lt;double&gt;(1, 2);
var sum = c1 + c2;
var product = c1 * c2;

Console.WriteLine($"Sum: {sum}");           // 4 + 6i
Console.WriteLine($"Product: {product}");   // -5 + 10i
Console.WriteLine($"Magnitude: {c1.Magnitude()}"); // 5</code></pre><h3>实际应用场景</h3><h4>通用数学库函数</h4><pre><code class="csharp">public static T StandardDeviation&lt;T&gt;(IEnumerable&lt;T&gt; values)
    where T : INumber&lt;T&gt;, IRootFunctions&lt;T&gt;
{
    var mean = Mean(values);
    var variance = T.Zero;
    var count = T.Zero;
    
    foreach (var value in values)
    {
        var diff = value - mean;
        variance += diff * diff;
        count++;
    }
    
    variance /= count;
    return T.Sqrt(variance);
}

public static T Mean&lt;T&gt;(IEnumerable&lt;T&gt; values) where T : INumber&lt;T&gt;
{
    var sum = T.Zero;
    var count = T.Zero;
    
    foreach (var value in values)
    {
        sum += value;
        count++;
    }
    
    return sum / count;
}</code></pre><h4>几何计算</h4><pre><code class="csharp">public record Vector2D&lt;T&gt;(T X, T Y) where T : INumber&lt;T&gt;, IRootFunctions&lt;T&gt;
{
    public T Magnitude =&gt; T.Sqrt(X * X + Y * Y);
    
    public static Vector2D&lt;T&gt; operator +(Vector2D&lt;T&gt; a, Vector2D&lt;T&gt; b)
        =&gt; new(a.X + b.X, a.Y + b.Y);
    
    public static Vector2D&lt;T&gt; operator -(Vector2D&lt;T&gt; a, Vector2D&lt;T&gt; b)
        =&gt; new(a.X - b.X, a.Y - b.Y);
    
    public static T Dot(Vector2D&lt;T&gt; a, Vector2D&lt;T&gt; b)
        =&gt; a.X * b.X + a.Y * b.Y;
    
    public Vector2D&lt;T&gt; Normalize()
    {
        var mag = Magnitude;
        return mag == T.Zero 
            ? this 
            : new Vector2D&lt;T&gt;(X / mag, Y / mag);
    }
}</code></pre><h4>财务计算</h4><pre><code class="csharp">public static T CalculateCompoundInterest&lt;T&gt;(
    T principal, 
    T annualRate, 
    int years, 
    int compoundingPeriods = 1)
    where T : IFloatingPoint&lt;T&gt;
{
    var ratePerPeriod = annualRate / T.CreateChecked(compoundingPeriods);
    var periods = years * compoundingPeriods;
    
    return principal * T.Pow(T.One + ratePerPeriod, T.CreateChecked(periods));
}</code></pre><h4>数值积分和微分</h4><pre><code class="csharp">// 泛型数值积分
public static T Integrate&lt;T&gt;(
    Func&lt;T, T&gt; function, 
    T from, 
    T to, 
    int steps) where T : IFloatingPoint&lt;T&gt;
{
    T stepSize = (to - from) / T.CreateChecked(steps);
    T sum = T.Zero;
    
    for (int i = 0; i &lt; steps; i++)
    {
        T x1 = from + T.CreateChecked(i) * stepSize;
        T x2 = from + T.CreateChecked(i + 1) * stepSize;
        T y1 = function(x1);
        T y2 = function(x2);
        
        // 梯形法则
        sum += (y1 + y2) * stepSize / T.CreateChecked(2);
    }
    
    return sum;
}

// 使用数值积分
Func&lt;double, double&gt; f = x =&gt; x * x; // f(x) = x²
double integral = Integrate(f, 0.0, 1.0, 1000);
Console.WriteLine($"∫x²dx from 0 to 1 = {integral}"); // 约等于 0.333...</code></pre><h4>线性代数运算</h4><pre><code class="csharp">// 泛型向量类
public struct Vector&lt;T&gt; where T : INumber&lt;T&gt;
{
    private readonly T[] _components;
    
    public Vector(params T[] components)
    {
        _components = components;
    }
    
    public int Dimension =&gt; _components.Length;
    
    public T this[int index]
    {
        get =&gt; _components[index];
        set =&gt; _components[index] = value;
    }
    
    public static Vector&lt;T&gt; operator +(Vector&lt;T&gt; left, Vector&lt;T&gt; right)
    {
        if (left.Dimension != right.Dimension)
            throw new ArgumentException("Vectors must have the same dimension");
        
        T[] result = new T[left.Dimension];
        for (int i = 0; i &lt; left.Dimension; i++)
        {
            result[i] = left[i] + right[i];
        }
        return new Vector&lt;T&gt;(result);
    }
    
    public static T operator *(Vector&lt;T&gt; left, Vector&lt;T&gt; right) // 点积
    {
        if (left.Dimension != right.Dimension)
            throw new ArgumentException("Vectors must have the same dimension");
        
        T result = T.Zero;
        for (int i = 0; i &lt; left.Dimension; i++)
        {
            result += left[i] * right[i];
        }
        return result;
    }
    
    public T Magnitude() where T : IRootFunctions&lt;T&gt;
    {
        T sumOfSquares = T.Zero;
        foreach (T component in _components)
        {
            sumOfSquares += component * component;
        }
        return T.Sqrt(sumOfSquares);
    }
    
    public override string ToString() =&gt; 
        $"[{string.Join(", ", _components)}]";
}

// 使用向量
var v1 = new Vector&lt;double&gt;(1, 2, 3);
var v2 = new Vector&lt;double&gt;(4, 5, 6);
var sum = v1 + v2;
var dotProduct = v1 * v2;

Console.WriteLine($"v1 + v2 = {sum}");           // [5, 7, 9]
Console.WriteLine($"v1 · v2 = {dotProduct}");    // 32
Console.WriteLine($"|v1| = {v1.Magnitude()}");   // 3.741...</code></pre><h4>统计计算</h4><pre><code class="csharp">// 泛型统计函数
public static class Statistics&lt;T&gt; where T : INumber&lt;T&gt;, IFloatingPoint&lt;T&gt;
{
    public static T Mean(IEnumerable&lt;T&gt; values)
    {
        T sum = T.Zero;
        int count = 0;
        
        foreach (T value in values)
        {
            sum += value;
            count++;
        }
        
        return sum / T.CreateChecked(count);
    }
    
    public static T Variance(IEnumerable&lt;T&gt; values)
    {
        T mean = Mean(values);
        T sumOfSquares = T.Zero;
        int count = 0;
        
        foreach (T value in values)
        {
            T deviation = value - mean;
            sumOfSquares += deviation * deviation;
            count++;
        }
        
        return sumOfSquares / T.CreateChecked(count);
    }
    
    public static T StandardDeviation(IEnumerable&lt;T&gt; values)
    {
        return T.Sqrt(Variance(values));
    }
    
    public static (T Min, T Max, T Median) DescriptiveStats(IEnumerable&lt;T&gt; values)
    {
        var sorted = values.OrderBy(v =&gt; v).ToArray();
        int count = sorted.Length;
        
        T min = sorted[0];
        T max = sorted[count - 1];
        
        T median = count % 2 == 0
            ? (sorted[count / 2 - 1] + sorted[count / 2]) / T.CreateChecked(2)
            : sorted[count / 2];
        
        return (min, max, median);
    }
}

// 使用统计函数
double[] data = { 1.2, 2.3, 3.4, 4.5, 5.6 };
Console.WriteLine($"Mean: {Statistics&lt;double&gt;.Mean(data)}");
Console.WriteLine($"Variance: {Statistics&lt;double&gt;.Variance(data)}");
Console.WriteLine($"Standard Deviation: {Statistics&lt;double&gt;.StandardDeviation(data)}");

var (min, max, median) = Statistics&lt;double&gt;.DescriptiveStats(data);
Console.WriteLine($"Min: {min}, Max: {max}, Median: {median}");</code></pre><h3>自定义数值类型</h3><h4>创建支持泛型数学的自定义类型</h4><pre><code class="csharp">// 自定义分数类型
public readonly struct Fraction : 
    INumber&lt;Fraction&gt;,
    IComparisonOperators&lt;Fraction, Fraction, bool&gt;,
    IModulusOperators&lt;Fraction, Fraction, Fraction&gt;
{
    public long Numerator { get; }
    public long Denominator { get; }
    
    public Fraction(long numerator, long denominator)
    {
        if (denominator == 0)
            throw new DivideByZeroException("Denominator cannot be zero");
        
        // 简化分数
        long gcd = Gcd(Math.Abs(numerator), Math.Abs(denominator));
        Numerator = numerator / gcd;
        Denominator = denominator / gcd;
        
        // 确保分母为正
        if (Denominator &lt; 0)
        {
            Numerator = -Numerator;
            Denominator = -Denominator;
        }
    }
    
    private static long Gcd(long a, long b) =&gt; b == 0 ? a : Gcd(b, a % b);
    
    // INumber&lt;Fraction&gt; 实现
    public static Fraction Zero =&gt; new Fraction(0, 1);
    public static Fraction One =&gt; new Fraction(1, 1);
    
    public static Fraction operator +(Fraction left, Fraction right)
    {
        long numerator = left.Numerator * right.Denominator + right.Numerator * left.Denominator;
        long denominator = left.Denominator * right.Denominator;
        return new Fraction(numerator, denominator);
    }
    
    public static Fraction operator -(Fraction left, Fraction right)
    {
        long numerator = left.Numerator * right.Denominator - right.Numerator * left.Denominator;
        long denominator = left.Denominator * right.Denominator;
        return new Fraction(numerator, denominator);
    }
    
    public static Fraction operator *(Fraction left, Fraction right)
    {
        return new Fraction(
            left.Numerator * right.Numerator,
            left.Denominator * right.Denominator
        );
    }
    
    public static Fraction operator /(Fraction left, Fraction right)
    {
        return new Fraction(
            left.Numerator * right.Denominator,
            left.Denominator * right.Numerator
        );
    }
    
    // 其他接口实现...
    
    public override string ToString() =&gt; $"{Numerator}/{Denominator}";
}

// 使用自定义分数类型
Fraction f1 = new Fraction(1, 2);
Fraction f2 = new Fraction(3, 4);
Fraction sum = f1 + f2; // 5/4
Fraction product = f1 * f2; // 3/8

Console.WriteLine($"{f1} + {f2} = {sum}");
Console.WriteLine($"{f1} × {f2} = {product}");</code></pre><h3>高级技巧</h3><h4>类型转换处理</h4><pre><code class="csharp">public static TResult ConvertSafely&lt;TInput, TResult&gt;(TInput value)
    where TInput : INumber&lt;TInput&gt;
    where TResult : INumber&lt;TResult&gt;
{
    try
    {
        return TResult.CreateChecked(value);
    }
    catch (OverflowException)
    {
        return value &lt; TInput.Zero 
            ? TResult.NegativeInfinity 
            : TResult.PositiveInfinity;
    }
}</code></pre><h4>性能优化</h4><pre><code class="csharp">// 使用泛型数学的向量化操作
public static T[] VectorAdd&lt;T&gt;(T[] a, T[] b) 
    where T : IAdditionOperators&lt;T, T, T&gt;, IAdditiveIdentity&lt;T, T&gt;
{
    if (a.Length != b.Length)
        throw new ArgumentException("Arrays must be same length");
    
    var result = new T[a.Length];
    
    // 使用 SIMD 优化（如果可用）
    if (Vector.IsHardwareAccelerated &amp;&amp; 
        Vector&lt;T&gt;.IsSupported)
    {
        int vectorSize = Vector&lt;T&gt;.Count;
        int i = 0;
        
        for (; i &lt;= a.Length - vectorSize; i += vectorSize)
        {
            var va = new Vector&lt;T&gt;(a, i);
            var vb = new Vector&lt;T&gt;(b, i);
            (va + vb).CopyTo(result, i);
        }
        
        // 处理剩余元素
        for (; i &lt; a.Length; i++)
        {
            result[i] = a[i] + b[i];
        }
    }
    else
    {
        // 回退到常规循环
        for (int i = 0; i &lt; a.Length; i++)
        {
            result[i] = a[i] + b[i];
        }
    }
    
    return result;
}</code></pre><h4>条件约束组合</h4><pre><code class="csharp">public static T SafeDivide&lt;T&gt;(T dividend, T divisor)
    where T : IDivisionOperators&lt;T, T, T&gt;, 
              IComparisonOperators&lt;T, T, bool&gt;,
              IAdditiveIdentity&lt;T, T&gt;
{
    if (divisor == T.AdditiveIdentity)
        throw new DivideByZeroException();
    
    return dividend / divisor;
}</code></pre><h3>类型/接口选择建议</h3><ul><li>想要通用数（能 + - * /、有 Zero、能比较大小）→ 用 <code>INumber&lt;T&gt;</code>。</li><li>只针对整数算法（GCD、位操作等）→ 用 <code>IBinaryInteger&lt;T&gt;</code>。</li><li>只针对浮点/IEEE754 特性（NaN、Infinity、Sqrt、Hypot 等）→ 用 <code>IFloatingPointIeee754&lt;T&gt;</code>（或更窄的 <code>IFloatingPoint&lt;T&gt;</code>）。</li></ul><h3>总结</h3><p><code>C# 11</code> 和 <code>.NET 7</code> 的泛型数学功能是一个重大突破，它：</p><ul><li>解决了长期痛点：终于可以在泛型代码中方便地进行数学运算</li><li>提供了完整的数学接口体系：覆盖基本运算、比较、三角函数等</li><li>支持自定义数值类型：可以创建自己的数值类型并集成到数学生态中</li><li>保持高性能：通过静态抽象接口避免装箱拆箱开销</li><li>增强类型安全：编译时类型检查，减少运行时错误</li></ul><p>适用场景：</p><ul><li>数学库开发：创建通用的数学算法库</li><li>科学计算：物理模拟、数值分析等</li><li>游戏开发：向量、矩阵运算</li><li>金融计算：高精度数值计算</li><li>数据处理：统计分析、数据转换</li></ul>]]></description></item><item>    <title><![CDATA[防火墙究竟能防什么？从原理到类型的系统化]]></title>    <link>https://segmentfault.com/a/1190000047445223</link>    <guid>https://segmentfault.com/a/1190000047445223</guid>    <pubDate>2025-12-03 11:09:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、防火墙的概念防火墙（Firewall），又称防护墙，由 Check Point 创始人 Gil Shwed 于 1993 年提出（US5606668(A)）。它是一种部署在内部网络与外部网络之间的安全防护系统，通过预设规则对数据流进行允许或阻断，从而实现访问控制。在网络通信中，防火墙主要过滤承载通信内容的数据包，以隔离内部网络与公共网络，使未经授权的数据与用户无法进入企业内部环境，而合法的通信能够顺畅通过。若无防火墙，企业用户无法直接访问外部网络，外部用户也无法与企业内部进行通信，可见其在网络安全体系中的基础性价值。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445225" alt="图片" title="图片"/><br/>二、防火墙的发展历程防火墙自诞生以来历经四个关键发展阶段：从最早依附于路由器的过滤机制，逐渐演进为用户化工具套件；随后进入基于通用操作系统的软件防火墙阶段；最终发展为基于安全操作系统的专业防火墙设备。如今行业主流产品多集中在第四阶段，典型如 NETEYE、NETSCREEN、TALENTIT 等，它们具备更高的稳定性、安全性与可扩展性，代表着防火墙技术的成熟形态。三、防火墙的基本类型网络层防火墙网络层防火墙本质上是 IP 包过滤器，工作在 TCP/IP 协议栈较底层的位置。通过对 IP 地址、端口、协议类型等字段进行匹配，决定是否放行或丢弃数据包。管理员可以自定义策略，但某些设备也可能只采用内置规则。需要注意的是，此类防火墙无法防御病毒本身。应用层防火墙应用层防火墙运行在 TCP/IP 的应用层（如浏览器的 HTTP 流量、FTP 流量等）。它能够对应用程序的数据流进行更精细的检查，可拦截所有与指定应用无关的包，从而完全阻止外部数据流进入受保护主机。数据库防火墙数据库防火墙基于数据库协议分析，通过策略控制数据库访问行为，可阻断危险操作并进行实时审计。<br/>其通过 SQL 语句解析实现：允许合法 SQL；阻断违规或恶意 SQL；预警注入攻击；通过“虚拟补丁”快速抵御漏洞利用，其本质是数据库的外围安全防护系统。四、Linux 防火墙Linux 防火墙（如 iptables）在企业环境中具有广泛应用价值，不仅可在中小企业或网吧场景中充当 NAT 路由器，替代传统硬件路由器降低成本，还能在无硬件防火墙的 IDC 机房中承担网络过滤与访问控制的职责。同时，iptables 可与 Squid 配合实现透明代理，无需客户端配置即可完成流量重定向；在 NAT 模式下还能过滤 P2P 流量、拦截非法网站，并支持外网 IP 与内网 IP 的映射。此外，通过灵活配置的规则体系，iptables 能有效抵御轻量级的 DOS 攻击，如 ping 洪泛或 SYN 洪水。整体来看，它主要以主机防火墙与 NAT 路由两大模式应用于企业网络管理。五、防火墙的基本原理防火墙的原理基于网络传输过程中的不同层次实现多种防护能力。包过滤在网络层通过检查数据包头部字段（如 IP、端口、协议类型）快速决策通行；应用代理在应用层介入，通过代理程序重建通信会话，从而对数据内容进行深度检测；状态检测机制则结合数据流的连接状态实现更准确的访问控制，不再局限于单个数据包的判断；而完全内容检测从二层到七层对协议与数据进行完整还原和内容分析，能够同时识别包头、状态和完整应用数据，有效防御多类型混合攻击。六、Netfilter 与 iptablesNetfilter 是 Rusty Russell 提出的 Linux 2.4 内核中的防火墙框架，支持：包过滤NAT地址伪装透明代理基于状态的检测基于用户/MAC 的过滤等Netfilter 是内核态框架，iptables 是用户态控制工具。它们配合构成 Linux 防火墙体系：Netfilter：内核中的过滤引擎（表 + 链 + 规则）。iptables：管理 Netfilter 规则的命令行工具（存放于 /sbin/iptables）。iptables 并不直接“防火”，真正发挥作用的是 Netfilter。iptables 只是用来修改内核中的规则集（即 XXtables）。类似工具还有 firewalld。七、防火墙的性能防火墙性能是企业在选型与部署时最核心的衡量指标之一，它直接决定了网络环境在高负载下的稳定性与安全策略的执行效率。通常，防火墙性能由多个维度共同构成：首先 吞吐量 是最关键的指标，它反映设备在不同包大小条件下可持续处理的数据量，是衡量设备处理能力的基础参数，直接影响到企业网络的整体带宽利用率和业务承载能力。其次是 时延，即数据包从进入防火墙到被转发出去的耗时，时延越低，业务体验越流畅；尤其在对实时性要求高的场景（如金融交易、直播、工业控制）中，时延表现至关重要。丢包率 则体现设备在高负载下的稳定性，丢包率过高可能导致应用超时、业务中断或用户体验下降。背靠背能力 代表防火墙在最短合法包间隔下处理连续数据帧的能力，该指标越高，设备在高突发流量场景下越不易产生瓶颈。同时，现代网络环境下的另一个重要指标是 并发连接数。这代表防火墙能够同时维护的会话数量，决定了在大型业务系统、海量用户访问、高并发 API 请求等场景下的稳定性。综合来看，防火墙的性能不仅体现其硬件处理能力，也反映其内部架构设计、状态表优化以及策略引擎效率，是影响实际运维表现的重要因素。八、防火墙的局限性尽管防火墙是网络安全体系的核心基础设施，但它并非万能防护盾，其防御能力也受到技术边界的限制。首先，防火墙主要对穿越边界的流量进行控制，因此 无法阻止绕过防火墙的访问路径。如果内部用户私自建立外部连接（如拨号、热点共享、私接无线路由），就可能直接规避所有安全策略，使攻击流量绕过滤控体系。其次，传统防火墙依赖包头信息进行访问控制，属于 基于端口与协议的浅层检测。这意味着攻击者只要利用合法端口（如 80、443）传递恶意流量，就可能逃避检测，因此防火墙无法单独解决蠕虫、木马、加密攻击流量等深层威胁，更无法处理应用层复杂攻击，如 SQL 注入、XSS、CSRF 等。此外，对于快速演变的高级威胁（APT）或混合攻击链，防火墙也往往难以及时识别。更重要的是，防火墙 难以防范内部威胁与滥用行为。来自组织内部的恶意操作、权限滥用、数据泄露、横向移动等风险，往往绕过传统边界安全模型，单靠防火墙难以有效识别与阻断。因此，在现代企业安全体系中，防火墙需要与数据库审计、零信任访问控制、行为分析、终端检测、内容检测等其他手段协同，形成纵深防御体系，才能补齐内部风险与深层攻击检测的能力缺口。</p>]]></description></item><item>    <title><![CDATA[数据库审计：构建数据安全与合规治理体系的]]></title>    <link>https://segmentfault.com/a/1190000047445262</link>    <guid>https://segmentfault.com/a/1190000047445262</guid>    <pubDate>2025-12-03 11:08:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、概述</p><pre><code>   数据库审计（Database Auditing）作为企业数据安全体系中的核心能力，是对数据库访问与操作行为进行持续、精细化记录、分析与回溯的重要机制。通过对访问者身份、操作内容、请求来源、事件时间线等信息的完整留痕，数据库审计不仅让企业能够对数据资产实现强可视、强监管与强溯源，还能够在安全事件、违规操作或系统异常发生时快速定位问题、追踪根因，从而有效降低数据泄露风险、阻断潜在攻击路径并提升整体安全治理水平。面对现代企业复杂的系统架构与高频交互的数据访问场景，数据库审计已不仅仅是安全工具，更是合规治理、内部风控、运维管理和数据资产保护的重要基础设施。</code></pre><p>二、数据库审计的目的是什么</p><pre><code>   在数据库审计的核心目标中，“发现安全问题”无疑是最关键的价值点。通过持续监控数据库的访问行为与操作内容，企业能够及时识别非法登录、暴力破解、越权访问、敏感数据的异常读取、恶意 SQL 注入、批量导出以及来自不正常区域或终端的访问等安全风险，实现对潜在攻击行为和内部违规操作的早期预警。与此同时，审计记录为安全管理提供数据基础，使团队能够识别权限配置不合理、高权限账号长时间未使用、访问模式异常变化等问题，从而推动安全策略从静态规则向基于数据的持续优化演进，使权限控制、访问策略、账号体系持续处于最小风险状态。
   此外，数据库审计还能够帮助企业满足日益严格的监管与合规要求，包括《数据安全法》、等保 2.0/3.0、PCI DSS、HIPAA、GDPR 等，将审计日志作为可溯源证据保障合规检查顺利通过。在运维层面，审计系统对于性能优化同样具有显著价值，例如识别慢 SQL、分析数据库压力来源、发现异常访问模式、优化索引策略等，从而协助数据库团队提升系统性能和运维效率，真正构建安全与性能双提升的数据库治理体系。</code></pre><p>三、数据库审计的主要组成部分是什么？</p><pre><code>   数据库审计的核心基础是“日志记录”，它涵盖用户登录、退出、权限变更等行为日志，数据查询、更新、删除等操作日志，异常访问、失败登录、SQL 注入等安全日志，以及系统资源、服务状态与配置变更等系统日志。这些日志需要具备不可篡改、完整留存、时间同步等特性，以保证审计证据链的可靠性。在日志基础上，企业可根据业务风险等级配置差异化的审计策略，例如对敏感表与核心字段进行重点审计，对高风险账号记录更详细操作内容，为不同业务系统制定不同级别的审计策略，并为敏感访问、频繁查询等行为设置阈值与告警规则，从而在保证监测效果的同时控制系统开销。
   审计分析能力则是系统智能化的关键，通过对用户行为建立基线、自动识别异常访问模式、结合规则与算法发现可疑访问轨迹，并通过拓扑图、链路图、热力图等方式可视化呈现异常行为，帮助安全人员快速定位风险。而审计报告则承担可交付、可监管、可汇报的职能，通常包含安全状态总览、异常事件记录、敏感数据访问统计、趋势分析与风险指标等，既能支撑内部审计和管理者决策，也能用于外部监管机构的检查。</code></pre><p>四、数据库审计的主要类型有哪些？</p><pre><code>   数据库审计主要包括安全审计、操作审计、数据审计、性能审计和合规性审计五大类型。安全审计侧重于数据库权限分配的合理性、关键账号行为的分析、潜在漏洞利用痕迹以及异常数据访问轨迹，通过全链路的访问与操作监控，帮助企业构建从权限管理到操作执行的完整可视化安全体系。操作审计则关注数据库行为的真实执行情况，包括数据的增删改查（DML）操作、管理操作如建表、改表、授权，以及系统配置变更等内容，其主要应用于内部风险排查和事故回溯，确保操作可追踪、责任可界定。数据审计则着眼于数据库中数据本身的生命周期管理，追踪数据何时被修改、是否存在越权访问以及异常批量导出行为，从而保障数据安全与完整性。性能审计通过结合数据审计与操作审计提供对数据库性能的洞察，包括识别瓶颈 SQL、高频访问表以及异常资源占用情况，为数据库优化和运维提供参考。最后，合规性审计则重点检查数据库行为是否符合相关政策、法规及行业标准，确保企业在安全和合规两方面均能达到要求。通过这五类审计的协同应用，企业能够全面监控数据库的安全、操作、数据质量、性能与合规状况，实现系统化、可持续的数据安全治理。</code></pre><p>五、如何实施数据库审计？</p><pre><code>   实施数据库审计需要从制定完整的审计计划开始，包括识别业务关键数据、确定敏感数据范围、明确高风险用户和高风险操作。在此基础上配置适当的审计日志级别、存储方式及重点审计对象，使系统既能记录关键行为又不导致性能压力。日志需要统一采集并跨系统整合，确保数据一致可用。后续通过自动化或人工分析审计日志，识别可疑行为、构建上下文链路、输出可视化审计结果，并根据合规要求生成标准化报告，如阶段性安全报告、事件溯源报告、敏感数据访问报告等。最终，审计日志还需按监管要求长期保留，并定期归档、校验完整性和强化存储安全，确保其在合规和取证场景中可长期使用。</code></pre><p>六、数据库审计在数据安全、完整性、合规性及性能优化中的综合作用</p><pre><code>   数据库审计在企业数据治理中发挥着全方位的核心作用，其价值不仅体现在安全监控上，也涵盖数据完整性保障、合规管理、访问控制、性能优化和敏感数据保护等多个层面。通过对数据库操作的全量记录与分析，审计系统能够实时识别异常访问行为、批量读取、非法修改、可疑 SQL 操作及潜在的数据外泄特征，构建完整的数据威胁发现体系。基于这些审计结果，企业可优化权限体系，实现最小权限原则，结合白名单机制、敏感数据分级授权以及智能化风险识别，进一步强化访问安全，并对访问者身份进行精准溯源，确保在发生安全事件时责任可追踪。同时，高风险操作可自动触发备份和恢复机制，实现威胁发现与数据保护的联动，形成安全闭环。
   在数据完整性方面，数据库审计通过全面记录数据操作、重点强化敏感数据的审计和异常行为告警，能够发现非法修改、越权更新或批量删除行为，并结合完整性校验策略与备份恢复机制，为企业提供可追溯、可恢复的保障，确保关键业务数据保持可信赖状态。审计系统还支持合规性管理，能够生成符合监管要求的审计报告，记录数据库行为的全过程，为外部检查提供可验证的证据链，支持企业进行合规整改，使数据库行为始终符合法规和行业标准。
   在防止未经授权访问方面，审计系统结合多因子身份认证、敏感数据分级授权、最小权限策略、密文访问及访问基线模型，建立零信任数据库访问体系；同时通过智能算法监测异常登录、越权操作及异常访问模式，实现对未授权访问的实时识别和阻断。
   数据库审计还能够优化性能，通过分析慢查询、热点 SQL、资源占用异常和访问流量突增等情况，帮助管理员快速定位瓶颈，进行索引优化、结构调整和资源调度，同时揭示不合理的访问模式和应用行为，为数据库性能和应用架构优化提供数据参考，实现安全与性能的双重提升。
    此外，审计系统可识别潜在安全漏洞，包括弱密码、多次失败登录、高危 SQL 操作、未加密敏感数据被频繁访问以及旧版本数据库被扫描等行为迹象。由于审计基于行为层面的实时监控，其对漏洞利用前兆的识别通常比传统漏洞扫描更及时，帮助企业提前发现风险并采取防护措施。针对敏感数据，审计系统通过加密、脱敏、访问控制、重点审计与告警策略，实现数据在访问、传输、测试和运维过程中的全面保护，并结合统计与分析功能，构建完整的敏感数据生命周期保护机制，使企业能够全面掌握数据分布、访问行为和潜在风险。
   总的来说，数据库审计通过集安全、完整性、合规性、性能和敏感数据保护于一体，为企业提供了一个全面、可落地、可追踪的数据库治理体系，既能及时发现安全威胁，又能优化性能与管理效率，助力企业构建可信赖的数据安全生态。</code></pre><p>七、如何选择合适的数据库审计工具和供应商？</p><pre><code>   选择数据库审计工具需重点关注功能全面性、系统兼容性、智能分析能力、多数据库支持度、日志完整记录能力等核心能力，同时评估系统的可扩展性、分布式部署能力与与其他安全平台的集成能力。此外，企业还需关注审计系统在高并发、高流量场景下的性能开销控制，以及报表展示、可视化分析是否满足管理者与安全团队的多层次需求。供应商的行业经验、技术支持能力、合规适配程度同样是重要的评估指标，尤其对于金融、能源、政务等高监管行业而言，更需要选择在大型项目中经过验证、能够提供 7×24 支持的成熟厂商，以确保审计体系长期稳定运行。

</code></pre>]]></description></item><item>    <title><![CDATA[Word文档中插入图片：使用Java实现]]></title>    <link>https://segmentfault.com/a/1190000047445286</link>    <guid>https://segmentfault.com/a/1190000047445286</guid>    <pubDate>2025-12-03 11:07:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在日常工作中，Word文档是不可或缺的工具，而图片作为信息传达的重要载体，其在文档中的插入与布局显得尤为关键。当我们需要批量处理、自动化生成包含图片的Word文档时，手动操作显然效率低下。本文将深入探讨如何利用强大的Spire.Doc for Java库，实现Word文档插入图片的自动化，并精细控制图片环绕方式和图片定位，助你轻松驾驭Java操作Word的复杂场景，实现高效Word自动化。</p><h2>1. Spire.Doc for Java库介绍与安装</h2><p>Spire.Doc是一款功能强大且易于使用的Java组件，专为处理Word文档而设计。它允许开发者在Java应用程序中创建、读取、编辑、转换和打印Word文档，无需安装Microsoft Word。其优势在于API接口丰富、性能优越，能够满足各种复杂的文档处理需求。</p><p>Maven依赖配置：</p><pre><code class="xml">&lt;repositories&gt;
    &lt;repository&gt;
        &lt;id&gt;com.e-iceblue&lt;/id&gt;
        &lt;name&gt;e-iceblue&lt;/name&gt;
        &lt;url&gt;https://repo.e-iceblue.cn/repository/maven-public/&lt;/url&gt;
    &lt;/repository&gt;
&lt;/repositories&gt;
&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;e-iceblue&lt;/groupId&gt;
        &lt;artifactId&gt;spire.doc&lt;/artifactId&gt;
        &lt;version&gt;13.11.2&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre><h2>2. 使用特定的环绕方式插入图片</h2><p>在Word中，图片的文本环绕方式决定了图片与周围文本的关系。Spire.Doc for Java提供了灵活的API来设置这些环绕方式。</p><p><strong>核心步骤：</strong></p><ul><li>加载或创建Word文档。</li><li>获取文档中的某个段落或创建一个新段落。</li><li>创建<code>DocPicture</code>对象，并加载图片文件。</li><li>将<code>DocPicture</code>对象添加到段落中。</li><li>设置图片的环绕方式。</li></ul><p><strong>代码示例：</strong></p><pre><code class="java">import com.spire.doc.*;
import com.spire.doc.documents.*;
import com.spire.doc.fields.*;

public class insertImage {
    public static void main(String[] args) throws Exception {
        //创建 Document 类的对象
        Document doc = new Document();

        //从磁盘载入 Word 文件
        doc.loadFromFile("D:/Samples/Sample.docx");

        //创建 DocPicture 类的对象
        DocPicture picture = new DocPicture(doc);

        //从磁盘加载图片
        picture.loadImage("D:/Samples/System.png");

        //设置图片大小
        picture.setWidth(75);
        picture.setHeight(90);

        //将图片文本环绕方式设置为四周环绕
        picture.setTextWrappingStyle( TextWrappingStyle.Square);

        //将图片插入到第二段
        doc.getSections().get(0).getParagraphs().get(1).getChildObjects().insert(0,picture);

        //保存文档
        doc.saveToFile("D:/javaOutput/insertImage.docx", FileFormat.Docx);
    }
}</code></pre><h3>不同环绕方式的视觉效果和应用场景：</h3><ul><li><strong>嵌入型 (Inline)</strong>： 图片被视为文本字符，随文本流动。适用于图片与文字紧密结合，不希望图片浮动的情况。</li><li><strong>四周型 (Square)</strong>： 文本围绕图片的矩形边框。最常见的环绕方式，图片与文本互不遮挡。</li><li><strong>紧密型 (Tight)</strong>： 文本紧密地围绕图片的实际轮廓。比四周型更贴合图片形状，适用于不规则形状的图片。</li><li><strong>浮于文字下方 (Behind)</strong>： 图片位于文本下方，文本会覆盖图片。适用于作为背景水印或装饰性图片。</li><li><strong>浮于文字上方 (InFrontOfText)</strong>： 图片位于文本上方，会遮挡文本。适用于需要突出图片，或作为浮动元素。</li><li><strong>上下型 (TopAndBottom)</strong>： 文本在图片上方和下方，不与图片左右两侧并排。</li></ul><h2>3. 在指定位置插入图片</h2><p>除了环绕方式，精确控制图片在文档中的位置也至关重要。Spire.Doc for Java允许你在段落、表格单元格甚至通过绝对坐标来定位图片。直接通过<code>Paragraph.getChildObjects().insert()</code>即可在文档的任意位置插入图片，如下所示：</p><pre><code class="java">import com.spire.doc.*;
import com.spire.doc.documents.*;
import com.spire.doc.fields.*;

public class insertImage {
    public static void main(String[] args) throws Exception {
        //创建 Document 类的对象
        Document doc = new Document();

        //从磁盘加载 Word 文档
        doc.loadFromFile("D:/Samples/Sample.docx");

        //创建 DocPicture 类的对象
        DocPicture picture = new DocPicture(doc);

        //从磁盘加载图片
        picture.loadImage("D:/Samples/PDF.png");

        //设置图片的大小
        picture.setWidth(75);
        picture.setHeight(90);

        //将图片的文本环绕方式设置为四周环绕
        picture.setTextWrappingStyle( TextWrappingStyle.Square);

        //将图片插入到第二段
        doc.getSections().get(0).getParagraphs().get(2).getChildObjects().insert(0,picture);

        //设置图片的位置
        picture.setHorizontalPosition(370.0F);
        picture.setVerticalPosition(10.0F);

        //保存文档
        doc.saveToFile("D:/javaOutput/insertImage.docx", FileFormat.Docx);
    }
}</code></pre><p><strong>DocPicture定位属性详解：</strong></p><ul><li><code>setHorizontalPosition() / setVerticalPosition()</code>: 设置图片相对于其定位基准的偏移量。</li><li><code>setHorizontalOrigin() / setVerticalOrigin()</code>: 设置图片水平/垂直定位的基准点，可选值包括Page（页面）、Column（列）、Margin（页边距）、Paragraph（段落）等。精确的定位通常需要选择Page作为基准。</li></ul><h2>4. 常见问题解答</h2><ul><li><strong>图片路径问题：</strong> 确保或loadImage()方法中提供的图片路径是正确的，可以是相对路径或绝对路径。对于Web应用，可能需要将图片转换为字节流加载。</li><li><strong>图片大小调整：</strong> 通过<code>picture.setWidth()</code>和<code>picture.setHeight()</code>可以设置图片的尺寸。Spire.Doc也会自动根据图片原始尺寸进行一定程度的缩放，但手动设置可以更精确控制。</li><li><strong>图片质量：</strong> 插入的图片质量取决于原始图片。如果图片过大，可能导致文档文件体积增大，可以考虑在插入前对图片进行压缩处理。</li><li><strong>性能优化：</strong> 批量插入大量图片时，可能会影响性能。可以考虑分批处理，或优化图片加载和文档保存逻辑。对于大型文档，Spire.Doc提供了分段处理等机制来提高效率。</li><li><strong>不支持的图片格式：</strong> 确保插入的图片格式是Word支持的常见格式（如PNG, JPG, BMP, GIF）。</li><li><strong>文本环绕与定位冲突：</strong> 当设置了非嵌入型环绕方式后，图片会脱离文本流，此时可以通过<code>setHorizontalPosition</code>和<code>setVerticalPosition进</code>行精确控制。</li></ul><h2>总结</h2><p>通过Spire.Doc for Java库，我们不仅能够轻松实现Word文档插入图片的基础功能，更能通过精细的API控制图片环绕方式和图片定位，从而满足复杂的文档自动化需求。无论是生成报告、合同，还是批量处理各类文档，Spire.Doc都提供了强大的支持。掌握这些技巧，将极大地提升你的Java操作Word效率，开启Word自动化的新篇章，期待你在实际项目中探索更多可能！</p>]]></description></item><item>    <title><![CDATA[Docker Registry UI o]]></title>    <link>https://segmentfault.com/a/1190000047445297</link>    <guid>https://segmentfault.com/a/1190000047445297</guid>    <pubDate>2025-12-03 11:06:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>该项目旨在为你的私有 Docker 仓库提供简洁且功能完整的用户界面。</p><p>没有Harbor那样那么多的依赖组件，当你只需要一个内网Docker私库和一个简单的用户界面查看仓库有哪些镜像时，这个项目是一个不错的选择。该项目仅包含一个前端页面，后端也仅依赖registry镜像。</p><h2>Docker-Compose</h2><p>registry-ui 推荐的docker-compose启动配置如下：</p><pre><code class="yaml">version: '3.8'

services:
  registry-ui:
    image: joxit/docker-registry-ui:main
    restart: always
    ports:
      - 80:80
    environment:
      - SINGLE_REGISTRY=true
      - REGISTRY_TITLE=Docker Registry UI
      - DELETE_IMAGES=true
      - SHOW_CONTENT_DIGEST=true
      - NGINX_PROXY_PASS_URL=http://registry-server:5000
      - SHOW_CATALOG_NB_TAGS=true
      - CATALOG_MIN_BRANCHES=1
      - CATALOG_MAX_BRANCHES=1
      - TAGLIST_PAGE_SIZE=100
      - REGISTRY_SECURED=false
      - CATALOG_ELEMENTS_LIMIT=1000
    container_name: registry-ui

  registry-server:
    image: registry:2.8.2
    restart: always
    environment:
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin: '[http://registry.example.com]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods: '[HEAD,GET,OPTIONS,DELETE]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials: '[true]'
      REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers: '[Authorization,Accept,Cache-Control]'
      REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers: '[Docker-Content-Digest]'
      REGISTRY_STORAGE_DELETE_ENABLED: 'true'
    volumes:
      - ./registry/data:/var/lib/registry
    container_name: registry-server</code></pre><h2>Kubernetes部署</h2><p>当想利用Kubernetes平台的<strong>故障恢复</strong>能力，去除组件单点故障，可以将registry部署到Kubernetes平台上。</p><ul><li>registry-server.yaml</li></ul><pre><code class="yaml">kind: Deployment
apiVersion: apps/v1
metadata:
  name: registry-server
  namespace: registry-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: registry-server
  template:
    metadata:
      labels:
        app: registry-server
    spec:
      volumes:
        - name: registry-data
          persistentVolumeClaim:
            claimName: registry-data-pvc
      containers:
        - name: registry-server
          image: 'joxit/registry:3.0.0'
          ports:
            - containerPort: 5000
              protocol: TCP
          env:
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin
              value: '[http://registry.ci.com]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods
              value: '[HEAD,GET,OPTIONS,DELETE]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials
              value: '[true]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers
              value: '[Authorization,Accept,Cache-Control]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers
              value: '[Docker-Content-Digest]'
            - name: REGISTRY_STORAGE_DELETE_ENABLED
              value: 'true'
          resources: {}
          volumeMounts:
            - name: registry-data
              mountPath: /var/lib/registry
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
---
kind: Service
apiVersion: v1
metadata:
  name: registry-server
  namespace: registry-ui
spec:
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000
  selector:
    app: registry-server
  type: ClusterIP</code></pre><ul><li>registry-ui.yaml</li></ul><pre><code class="yaml">kind: Deployment
apiVersion: apps/v1
metadata:
  name: registry-ui
  namespace: registry-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: registry-ui
  template:
    metadata:
      labels:
        app: registry-ui
    spec:
      containers:
        - name: registry-ui
          image: 'joxit/docker-registry-ui:main-debian-amd64'
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: SINGLE_REGISTRY
              value: 'true'
            - name: REGISTRY_TITLE
              value: Docker Registry UI
            - name: DELETE_IMAGES
              value: 'true'
            - name: PULL_URL
              value: 'registry.ci.com:32041'
            - name: SHOW_CONTENT_DIGEST
              value: 'true'
            - name: NGINX_PROXY_PASS_URL
              value: 'http://registry-server:5000'
            - name: SHOW_CATALOG_NB_TAGS
              value: 'true'
            - name: CATALOG_MIN_BRANCHES
              value: '1'
            - name: CATALOG_MAX_BRANCHES
              value: '1'
            - name: TAGLIST_PAGE_SIZE
              value: '100'
            - name: REGISTRY_SECURED
              value: 'false'
            - name: CATALOG_ELEMENTS_LIMIT
              value: '1000'
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
---
kind: Service
apiVersion: v1
metadata:
  name: registry-ui
  namespace: registry-ui
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 32041
  selector:
    app: registry-ui
  type: NodePort</code></pre><h2>环境变量</h2><pre><code class="yaml">            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin
              value: '[http://registry.ci.com]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods
              value: '[HEAD,GET,OPTIONS,DELETE]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials
              value: '[true]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers
              value: '[Authorization,Accept,Cache-Control]'
            - name: REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers
              value: '[Docker-Content-Digest]'</code></pre><p>这些环境变量并没有什么作用，可以去掉。</p>]]></description></item><item>    <title><![CDATA[LazyLLM教程 | 第17讲：企业级]]></title>    <link>https://segmentfault.com/a/1190000047445313</link>    <guid>https://segmentfault.com/a/1190000047445313</guid>    <pubDate>2025-12-03 11:05:37</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445316" alt="" title=""/></p><blockquote><p>在之前的教程中，我们主要探讨了<strong>个人级 RAG（检索增强生成）应用</strong>的实现方式和优化技巧。</p><p>但在<strong>企业级应用</strong>中，知识管理和智能检索的需求更为复杂，涉及多个部门，各自具备独立的业务领域、数据存储方式和算法需求。因此，高效管理和检索知识，确保不同部门灵活访问知识库，同时满足数据隔离、安全性和共享机制，是企业级知识管理系统的核心挑战。</p><p>本章将介绍如何使用 LazyLLM 快速构建<strong>面向企业级</strong>的<strong>数据库管理</strong>和<strong>检索召回服务</strong>，满足上述复杂需求。</p></blockquote><hr/><h2><strong>一、企业级知识库的多样性需求</strong></h2><p><br/></p><p>在企业实际应用中，知识库不再是简单的信息堆叠，而需要面对来自权限、共享方式与安全保障等多个维度的复杂诉求。下面我们从典型场景出发，系统阐述这些多样性需求及其应对策略。<br/><br/></p><p><strong>场景一：按部门隔离的知识访问（权限管理多样性）</strong></p><blockquote><p><strong>客户背景：大型制造企业</strong></p><p>企业下设多个职能部门（如研发、采购、销售），各自负责不同领域的信息收集与管理。文档内容涵盖供应链合作、成本核算、产品规划等，信息敏感度高，内部访问需严格隔离。</p><ul><li><strong>部门专属知识库</strong>：各业务单元可自主维护产品文档、市场分析、财务报告等专业内容，系统自动隔离非授权访问。</li><li><strong>智能标签体系</strong>：支持"研发-技术白皮书"、"市场-竞品分析"等专业标签体系，实现精准检索与受控共享。</li><li><strong>管理驾驶舱</strong>：高管层可通过"战略视图"标签获取跨部门知识摘要，确保决策支持的同时维护数据安全。</li></ul></blockquote><p><br/><br/><strong>场景二：多种共享方式的协同需求（共享方式多样性）</strong></p><blockquote><p><strong>客户背景：咨询服务公司</strong></p><p>公司经常与不同客户进行联合项目，涉及文档共享、阶段报告、项目材料和算法资源等。客户使用的工具和偏好多样，需要灵活的共享机制以满足业务合作与文档保密的平衡。</p><ul><li><strong>算法资源共享</strong>：支持llm模型、embedding模型、检索算法等核心组件在企业内部分享复用。</li><li><strong>差异化调用的知识复用</strong>：一个项目知识库可同时服务内部顾问、客户技术团队及第三方分析机构，能够配置不同的召回规则实现召回解耦，满足多角色精准访问。</li></ul></blockquote><p><br/><br/><strong>场景三：多重安全策略保障内容安全（安全保障多样性）</strong></p><blockquote><p><strong>客户背景：金融科技公司</strong></p><p>知识库包含大量敏感内容，如用户金融行为分析、监管合规方案、审计材料等，对信息安全的要求极高。</p><p><strong>安全需求：</strong></p><ul><li><strong>敏感词智能过滤</strong>：内置多级敏感词识别策略，结合上下文进行动态判断，在问答与检索过程中自动提示、替换或阻断输出，防止企业内部黑名单、客户机密、涉密术语等信息泄露。</li><li><strong>全链路知识加密</strong>：知识文档在上传、解析、入库、传输及生成阶段均可启用对称或非对称加密机制，确保知识在整个生命周期中不被窃取或篡改。</li><li><strong>私有化部署方案</strong>：平台可在企业内网私有服务器或专属云环境中完成全栈部署，包括知识库、向量引擎、检索模块与模型推理服务，确保知识数据不经公网传输。系统可无缝集成企业认证、权限与日志体系，形成闭环安全防护结构。</li></ul></blockquote><p><br/><br/>企业级应用场景对知识库提出了更多维度的要求，LazyLLM 针对这些需求，在权限管理、共享模式以及安全保障三方面提供了解决方案。</p><hr/><h2><strong>二、权限多样性以及解决方案</strong></h2><p><br/></p><h3><strong>（一）权限隔离：支持多部门独立知识运营</strong></h3><p>在大型企业中，各部门通常拥有独立的文档体系，这些文档可能包含敏感的业务信息、内部操作手册或关键流程文件。为了确保信息安全与使用合规，企业对文档的管理能力和隔离机制提出了更高要求。常见的管理难题包括：</p><ul><li>如何支持知识库的高频更新与维护？</li><li>如果同一篇文档被多个部门使用，需分别入库多次？导致数据冗余和管理困难？</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445317" alt="" title="" loading="lazy"/></p><p>为此，lazyllm内置灵活的文档管理服务，提供了一套完整的<strong>文档增删改查</strong>功能，用户可以方便地添加新文档、修改已有内容、删除过期文档，并在需要时进行检索，确保知识库始终保持最新状态。不同的知识库相互隔离，可帮助企业按部门、岗位、项目等维度灵活划分知识库访问边界，实现“谁能看、看什么、看多少”的可控策略。</p><p>例如，在同一知识库存储内，支持利用文档管理组功能进行分组管理，同一文档只需解析一次。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445318" alt="" title="" loading="lazy"/></p><h4><strong>1. 文档管理服务</strong></h4><p>文档管理服务的启用非常简单，只需要在创建 document 对象时，将 manager 参数设为 ui ，即可开启文档管理功能。例如：</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=OIdxdpbZMlT1JCnBAkl%2FMw%3D%3D.Xk%2B2UYRzh4ziVyqGabV6m3iVesV0LiHUdtgQBm8ze0VHninT%2FCHLTZs0634y3GrSdVtUleRP7EeWWOqcz%2FkE0PUVtv6LHnDqoCyEMQvEtuMyS9duQgf3vjCTFWNvqOtW" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter17/document_manager.py）</a></p><pre><code>from lazyllm.tools import Document
import time

path = "path/to/docs"
docs = Document(path, manager='ui')
# 注册分组
Document(path, name='法务文档管理组', manager=docs.manager)
Document(path, name='产品文档管理组', manager=docs.manager)
# 启动服务
docs.start()
time.sleep(3600)</code></pre><p>启动后页面如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445319" alt="image.png" title="image.png" loading="lazy"/></p><p>文档管理服务开启后，可在 Web 页面中便捷地查看不同分组内的文档，并支持对文档进行快速增删操作。</p><h4><strong>2.文档管理后端 API 服务</strong></h4><p>该Web服务内置了基于Gradio的默认前端界面。若企业需要定制更专业的前端界面，可通过manager=True参数仅启动后端API服务，随后基于接口自由开发个性化前端。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=CvPinfwU%2FkxSXgwSHROwpA%3D%3D.8KDUTq%2FRm9nHVM6DYvHA2Y75ujX0ginAV6dh0VdCBSwVJyTrwoLnkffEcA3cImaAN3H4i1HrRVqiuxz%2BBGenOqdTYhkA5DSED9wNROj%2BLuiCJpJFeVpkmVyQYrj%2FTjXU" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/main/rag/codes/chapter17/document_manager.py）</a></p><pre><code>from lazyllm.tools import Document
import time

path = "path/to/docs"
docs = Document(path, manager=True)
# 注册分组
Document(path, name='法务文档管理组', manager=docs.manager)
Document(path, name='产品文档管理组', manager=docs.manager)
# 启动服务
docs.start()
time.sleep(3600)</code></pre><p>启动后，Redoc 页面如下，展示了可用的后端接口。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445320" alt="image.png" title="image.png" loading="lazy"/></p><h3><strong>（二）权限多样性：支持更细粒度的访问控制</strong></h3><p>在企业实际运营中，权限需求远不止“哪个部门访问哪个文档”这么简单。不同小组、岗位甚至项目成员，常常需要在<strong>共享部分知识</strong>的同时<strong>保护敏感内容不被误读或泄露</strong>。</p><p>为满足这种复杂且多变的需求，LazyLLM 提供了<strong>基于标签检索的权限控制能力</strong>。企业可为文档打上如部门、岗位、时间、文件类型等多个标签，并为不同角色配置相应的访问权限，实现精细化管理。例如，市场部门可能希望检索与“产品推广”相关的文档，而研发部门可能更关注“技术规格”类的文档。</p><p>除了基于标签的筛选需求，用户还希望在检索时能够<strong>指定特定文档集进行查询</strong>，而不是搜索整个知识库。例如，法务部门可能只希望检索最近一年内的合同文件，而不是所有历史合同。</p><p>因此，系统需要<strong>支持针对知识库中的部分文档集合进行精准查询</strong>，提升搜索的精准度和效率。</p><p>企业需要通过<strong>标准化鉴权机制</strong>（如基于角色RBAC、基于属性ABAC、基于策略PBAC）精细控制文档访问。</p><ul><li>如何依据标准化鉴权机制组织内容和设置访问权限，确保信息合规使用？</li><li>如何根据权限等级细化访问控制，如同一部门内不同人员拥有不同等级的访问权限？</li></ul><hr/><p><strong>LazyLLM解决方案 ——— 基于标签（Tag）的权限控制机制：</strong></p><ul><li>每个文档可在上传时绑定预定义标签（如部门、项目、安全等级）</li><li>检索时支持基于标签的过滤，仅返回符合条件的内容</li></ul><p><strong>🔍 示例：模拟基于角色的权限控制（RBAC）</strong></p><p>目标：让“法务一部”员工仅能检索本部门的文档</p><ul><li>定义标签字段：department</li><li>上传文档时指定：department = 法务一部</li><li>检索时自动注入过滤条件：filter={"department": "法务一部"}</li></ul><p>通过这一机制，实现了<strong>基于角色的隔离访问</strong>，在保障数据安全的同时，也简化了权限策略的实施。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445321" alt="image.png" title="image.png" loading="lazy"/></p><p>在实际应用中，鉴权逻辑应由后端统一管理，算法侧不直接处理鉴权。这样可以确保权限控制的集中化和安全性，避免因算法侧绕过权限而引发的安全漏洞。</p><h4><strong>1. 基于标签的访问控制</strong></h4><p>我们可以通过 <strong>元数据(metadata)管理</strong> 和 <strong>检索过滤(filter)</strong> 来实现灵活的分类和查询功能，仅需以下两步：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445322" alt="image-2.png" title="image-2.png" loading="lazy"/></p><h5><strong>第一步：Metadata 添加</strong></h5><p>使用元数据过滤需指定milvus数据库，并且声明要指定的字段，以部门department为例，示例如下：</p><pre><code>CUSTOM_DOC_FIELDS = {"department": DocField(data_type=DataType.VARCHAR, max_size=65535, default_value=' ')}
milvus_store_conf = {
    'type': 'milvus',
    'kwargs': {
        'uri': os.path.join(db_path, "milvus.db"),
        'index_kwargs': [
            {
                'embed_key': 'bge_m3_dense',
                'index_type': 'IVF_FLAT',
                'metric_type': 'COSINE',
            },
            {
                'embed_key': 'bge_m3_sparse',
                'index_type': 'SPARSE_INVERTED_INDEX',
                'metric_type': 'IP',
            }
        ]

    },
}


law_knowledge_base = Document(
    data_path, 
    name='法务知识库', 
    manager="ui", 
    doc_fields=CUSTOM_DOC_FIELDS,  # 指定要过滤的字段
    store_conf=milvus_store_conf,  # 开启milvus数据库
    embed=OnlineEmbeddingModule(source="glm", embed_model_name="embedding-2"))</code></pre><p>在通过文档管理服务上传文件时，用户可为文件指定需要设定的元数据（metadata）分类信息。例如：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445323" alt="image.png" title="image.png" loading="lazy"/></p><h5><strong>第二步：Metadata 查询</strong></h5><p>在查询时，用户可以通过 filter 机制指定需要过滤的分类信息。可以通过以下方式进行筛选，来仅检索来自法务一部和法务二部的文档。</p><pre><code>retriever_support = Retriever(
    [law_knowledge_base, support_knowledge_base],
    group_name=...,
    similarity=...,
    topk=2
)

support_question = "客户关于合同投诉的处理方式"
support_res_nodes = retriever_support(
    support_question， 
    filters={'department':['法务一部']}   # 指定已定义的过滤条件
)</code></pre><p>通过添加Metadata和定义filter机制，最终实现了对文档的多权限管理。</p><p>以下示例同时展示了两种检索方式：</p><ul><li>使用 filter（仅检索“法务一部”文档）</li><li>不使用 filter（检索所有文档）</li></ul><p>该对比仅用于<strong>功能展示目的</strong>，以便理解系统的过滤机制。</p><p>在实际应用中，系统可实现<strong>强制绑定过滤条件</strong>，确保用户只能检索其所属部门的文档，从而实现<strong>文档隔离与权限控制的统一</strong>。</p><h5><strong>📌 进阶：细化权限等级的权限控制</strong></h5><p>等级 1：普通员工，仅能查看基础财务报表。</p><p>等级 2：主管，能查看部门预算和项目支出。</p><p>等级 3：经理及以上，能够访问财务决策和敏感报表。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445324" alt="image.png" title="image.png" loading="lazy"/></p><p>1.注册 权限等级- permission_level 字段：</p><pre><code>CUSTOM_DOC_FIELDS = {"department": DocField(data_type=DataType.VARCHAR, max_size=32, default_value=''), 
                     "permission_level": DocField(data_type=DataType.INT32, default_value=1)}</code></pre><p>2.上传文档同时标记权限等级（如permission_level = 1）</p><pre><code>files = [('files', ('普通文档.pdf', io.BytesIO(...)),
          ('files', ('敏感文档.pdf', io.BytesIO(...))]
metadatas=[{"department": "法务一部", "permisssion_level": 1},
           {"department": "法务一部", "permisssion_level": 2}]))</code></pre><p>3.检索时指定权限等级</p><pre><code>nodes = retriever(query, filters={'department': ['法务一部'], "permission_level": [1,2]} )</code></pre><hr/><h2><strong>三、共享方式多样性以及解决方案</strong></h2><p><br/></p><p>除了权限控制，企业在知识共享方面也面临<strong>多样化需求：</strong></p><p>一方面，不同团队间常需<strong>共享算法资源</strong>以提升复用效率；另一方面，多个部门之间也存在<strong>知识库交叉使用</strong>的需求，支持<strong>多对多的知识复用关系</strong>。</p><p>这些场景对灵活的共享机制提出了更高要求。</p><h3><strong>（一）共享灵活性：支持多源知识与算法自由适配</strong></h3><p>在企业中，多个部门可能共享相同的算法进行数据处理、推理和决策，但由于各自的业务领域不同，<strong>每个部门通常拥有独立的知识库</strong>，存储各自领域的专属信息。</p><p>因此，系统需要支持<strong>同一算法可作用于多个不同的知识</strong>库，确保算法在不同部门的适用性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445325" alt="image.png" title="image.png" loading="lazy"/></p><blockquote>比如：在一家金融公司中，风险控制部门和市场分析部门可能都使用相同的文本解析和嵌入算法来预处理数据，但风险控制部门的知识库主要包含历史交易和客户信用记录，而市场分析部门的知识库则包含市场动态和竞争对手情报。系统需要支持在不同的知识库中复用相同的数据处理算法。</blockquote><p>另一方面，也有部分企业场景中，不同的部门可能使用各自定制的算法进行数据分析和决策，但<strong>某些部门之间可能需要共享同一知识库</strong>，以便在<strong>统一的信息源基础上进行差异化计算</strong>。</p><p>系统需要支持<strong>不同算法可作用于同一知识库</strong>，以满足这种业务需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445326" alt="image-2.png" title="image-2.png" loading="lazy"/></p><blockquote>比如：在一家电商企业中，推荐系统部门可能使用基于协同过滤的嵌入算法对用户行为进行建模，而搜索优化部门可能使用基于词向量的相似度排序算法来提升搜索结果的相关性。这两个部门可能都基于同一个用户行为数据集，系统需要支持在同一知识库中独立运行不同的嵌入和排序算法，生成针对性优化的结果。</blockquote><hr/><p>接下来，我们将介绍如何利用 <strong>lazyllm</strong> 实现 RAG 流程中算法模块的共享与灵活配置。</p><p><strong>LazyLLM</strong>支持灵活的算法插件机制，为构建可插拔的智能系统提供了基础保障。我们可以预先定义一个 <strong>全局算法注册器</strong>，将常用的解析器、嵌入模型和相似度计算方法统一管理，从而在构建知识库和检索器（Retriever）时，实现算法的灵活组合与复用。</p><p>注：以下代码demo仅展示框架，实际算法需根据需求自行实现。</p><pre><code>class AlgorithmRegistry:
    """全局算法注册器，实现算法与知识库的解耦复用"""

    # --------------------------
    # 文档解析算法池
    # --------------------------
    DOC_PARSERS = {
        "magic_pdf": MagicPDFReader(),  # 带OCR的高级PDF解析
        "basic_pdf": SimplePDFParser(),   # 轻量PDF解析
        "docx": OfficeParser(),           # 严格模式Word解析
        "html": BeautifulSoupParser()
    }

    # --------------------------
    # 节点解析算法池
    # --------------------------
    NODE_PARSERS = {
        "semantic_chunk": SemanticNodeSplitter(),  # 语义分块
        "graph_based": KnowledgeGraphParser()  # 金融实体识别
    }

    # --------------------------
    # 嵌入模型池
    # --------------------------
    EMBEDDINGS = {
        "general": SentenceTransformer(),    # 通用语义
        "finance": FinBERTEmbedding(),# 金融领域专用
        "bio": BioClinicalBERT.from_pretrained() 
    }

    # --------------------------
    # 相似度计算策略
    # --------------------------
    @staticmethod
    @fc_register(name="euclidean")                            
    def euclidean_sim(query, nodes):
        pass

    @staticmethod
    @fc_register(name="ifdif")                                 
    def ifdif_sim(query, node):
        pass</code></pre><p>前文代码示例展示了如何构建一个“算法池”，企业可根据自身需求进行扩展和维护。完成注册后，系统可根据具体业务场景，按需选择合适的算法组件，实现“<strong>算法即服务</strong>”的设计理念。</p><p>以一个对准确性要求极高的<strong>金融风控场景</strong>为例：</p><ul><li><strong>嵌入模型</strong> 选择了针对金融文本微调的 FinBERT，以获得更精确的语义编码；</li><li><strong>文档解析器</strong> 采用具备 OCR 能力的高精度 PDF 解析工具，保障合同等复杂文件的结构完整性；</li><li><strong>节点解析</strong>采用语义切分策略，以保留关键信息的语义上下文，提升召回质量。</li></ul><p>代码示例如下：</p><pre><code># 金融风控（高精度导向）-- 处理复杂PDF合同，需识别法律实体与财务条款

law_kb = Document("path/to/kb", name='金融风控知识库', embed=AlgorithmRegistry.EMBEDDINGS['finance'])   # 选择金融领域BERT微调模型

law_kb.add_reader(AlgorithmRegistry.DOC_PARSERS['magic_pdf'])   # 选择处理复杂PDF合同，需识别精细法律实体与财务条款

law_kb.create_node_group(name='semantic_nodes', transform=AlgorithmRegistry.NODE_PARSERS['semantic_chunk'])  # 选择语义分块算法

# 定义 retriever 并选择 节点组 和 相似度计算方式
retriever = Retriever(
    group_name="semantic_nodes",   
    similarity="cosine",      
    topk=1                
)</code></pre><hr/><p>同一套算法在多个知识库中的应用场景已在前面权限的部分讨论过。</p><p>接下来，我们实现在同一知识库中，通过不同文档分组实现算法多样化的场景。</p><pre><code>docs = Document(path, manager=True, embed=OnlineEmbeddingModule())
# 注册分组
Document(path, name='法务文档管理组', manager=docs.manager)
Document(path, name='产品文档管理组', manager=docs.manager)
#  模拟文档上传
docs.start()
files = [('files', ('产品文档.txt', io.BytesIO("这是关于产品的信息。该文档由产品部编写。\n来自产品文档管理组".encode("utf-8")), 'text/plain'))]
files = [('files', ('法务文档.txt', io.BytesIO("这是关于法律事务的说明。该文档由法务部整理。\n来自法务文档管理组".encode("utf-8")), 'text/plain'))]
…
# 为 产品文档管理组 设置切分方式为按 段落 切分
doc1 = Document(path, name=‘产品文档管理组', manager=docs.manager)
doc1.create_node_group(name="block", transform=lambda s: s.split("\n") if s else '')
retriever1 = Retriever([doc1], group_name="block", similarity="cosine", topk=3)

# 为 法务文档管理组 设置切分方式为按 句子 切分
doc2 = Document(path, name=‘法务文档管理组’, manager=docs.manager)
doc2.create_node_group(name=“line”, transform=lambda s: s.split(“。") if s else ‘’)
retriever2 = Retriever([doc2], group_name="line", similarity="cosine", topk=3)</code></pre><h3><strong>（二）召回解耦：支持知识库与召回服务灵活协同</strong></h3><p>为应对复杂的知识共享与复用需求，企业越来越需要灵活而高效的知识组织结构与管理能力：</p><p><strong>1️⃣需要多对多的知识组织结构</strong></p><ul><li>企业往往希望通过一个统一的文档管理服务，集中管理多个知识库，既支持各业务部门对知识内容的独立维护，又保障在需要时的受控共享。</li><li>同一知识库还能被多个 RAG 召回系统调用，实现跨业务系统的知识复用，提升模型服务的覆盖面与智能化能力。</li></ul><p><strong>2️⃣需要多业务场景的知识复用能力</strong></p><ul><li>面对客服、合规、风控、市场等多样化业务需求，企业必须确保知识能够高效复用，同时又能按场景独立更新、灵活适配。</li></ul><hr/><p>为满足上述需求，LazyLLM不仅提供灵活的文档管理模块，还将<strong>文档管理与 RAG 召回服务进行完全解耦</strong>，来满足企业知识管理和召回需求的多样性，这样做的好处具体体现在：</p><ul><li><strong>多对多管理模式</strong>：一个文档管理服务可以同时管理<strong>多个知识库</strong>，支持不同业务部门的知识存储需求。</li><li><strong>多 RAG 适配</strong>：同一个<strong>知识库</strong>可以适用于<strong>多个 RAG 召回服务</strong>，一个 RAG 召回服务可以从多个<strong>知识库</strong>中检索数据。</li></ul><p>得益于这种解耦设计，确保了企业能够在不同业务场景下，动态调整知识库和 RAG 召回服务的绑定关系，满足个性化的知识管理需求。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445327" alt="image.png" title="image.png" loading="lazy"/></p><p>而具体实现起来，仅需以下两步骤，可搭建多知识库管理和召回流程。</p><h4><strong>1.初始化知识库</strong></h4><p>初始化知识库有两种方式：</p><ul><li><strong>路径定义方式</strong>：如果已有整理好的知识库文档，可直接通过指定文件路径来定义知识库。</li><li><strong>动态管理方式</strong>：如果知识库需要动态调整，可通过启动知识库服务后，进行上传、删除和管理操作。</li></ul><blockquote>说明：通过启动服务来上传文档的方式仅能绑定一个路径，若存在多个不同路径的数据库，建议使用定义路径的方式；为不同知识库灵活地注册不同的算法，但需要保持node group的name一致，以便后续进行联合召回。</blockquote><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=orVVYvy66VoUdNuyqI6L6Q%3D%3D.r3SfNQj1JlN0tNX4tQRTbKxvCFbdb6c6XrQzKra%2BpmeX5aEV2hTHFQLxmCdNr2Ot07ZsXTEe1qmb%2BhRu37nLpUB9BaxYnM49SXD2tbg1ew4qU2DTroQ1IryZoFP8nuzjaF%2BdybFYnSfPsQFo1R%2BzrmwhJRY3eyflyOXdRKNRMOZKNz5YsQ1ncHKIsWUG1%2BS8" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/multi_retriever.py#L7）</a></p><pre><code>from lazyllm.tools import Document
from lazyllm import OnlineEmbeddingModule
import time

# =============================
# 方法1.通过定义路径的方式
# =============================

law_data_path = "path/to/docs/law"
product_data_path = "path/to/docs/product"
support_data_path = "path/to/docs/support"

law_knowledge_base = Document(law_data_path, name='法务知识库'，embed=OnlineEmbeddingModule(source="glm", embed_model_name="embedding-2"))
product_knowledge_base = Document(product_data_path,name='产品知识库'，embed=OnlineEmbeddingModule(source="glm", embed_model_name="embedding-2"))
support_knowledge_base = Document(support_data_path,name='客户服务知识库'，embed=OnlineEmbeddingModule(source="glm", embed_model_name="embedding-2"))


# =============================
# 方法2.通过文档上传方式
# =============================

data_path = "path/to/docs"

law_knowledge_base = Document(data_path, name='法务知识库', manager="ui"，embed=OnlineEmbeddingModule(source="glm", embed_model_name="embedding-2"))
# 通过法务知识库的 manager 共享管理器
product_knowledge_base = Document(
    data_path,
    name='产品知识库',
    manager=law_knowledge_base.manager,
)

law_knowledge_base.start()
# ... 服务启动后手动增删文件 ... #</code></pre><h4><strong>2. 启用 RAG 召回服务</strong></h4><p>定义好知识库后，可根据需要为不同知识库灵活配置召回服务。将文档管理对象传入定义的 Retriever 即可，Retriever 的具体使用方法详见之前的教程 [ 第8讲：不止是cosine！匹配策略决定你召回的质量 ]。</p><p>企业可通过业务需求配置数据处理算法注册为node_group， 并在进行召回时使用。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=5Y9NZtwZSqVra%2B5nX4J%2FGA%3D%3D.sKpFb7z8JKjVap0ejdNf3VmhJayIOqX8w6RFzxjosdxPfM3ZAYtGrWmJ1ufwSXhWECbNXNTqIP8JIx6uWWcYNosZu3ladkZAqCGvrLncPziPdbTY5ic%2B9adQPhi9%2BnPxtYudld3sooeAcaWm%2F%2FIL1GxcWSeYJ4he0RwEjm%2BbT32HAzmmFjHizhQ2Oo8d6aUe" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/multi_retriever.py#L29）</a></p><pre><code>from lazyllm import Retriever, SentenceSplitter

# 配置和定义数据处理算法, 可根据业务需要自定义
Document.create_node_group(name="sentences", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)

# 组合法务 + 产品知识库，处理与产品相关的法律问题
retriever_product = Retriever(
    [law_knowledge_base, product_knowledge_base],
    group_name="sentences",       # 分组名（根据业务需求选择）
    similarity="cosine",       # 相似度参数（根据模型配置）
    topk=2                # 召回前2个最相关的结果
)

product_question = "A产品功能参数和产品合规性声明"
product_res_nodes = retriever_product(product_question)

# 组合法务 + 客户知识库，处理客户支持相关问题
retriever_support = Retriever(
    [law_knowledge_base, support_knowledge_base],
    group_name="sentences",
    similarity="cosine",
    topk=2
)

support_question = "客户投诉的处理方式以及会导致的法律问题"
support_res_nodes = retriever_support(support_question)
...</code></pre><p>检索结果示例如下，当检索问题“A产品功能参数和产品合规性声明”，可同时检索到产品知识库中的产品参数内容和法规知识库中的相关条款：</p><pre><code>print(f"query: {product_question }")
print("retrieve nodes:")
for node in product_res_nodes :
    print(node.text)
    print()
"""
query: A产品功能参数和产品合规性声明
retrieve nodes:
A智能管理系统的功能参数
3.1 系统性能
并发处理能力：支持高并发访问，满足企业级应用需求。
响应时间：在高负载条件下，保持低延迟响应。
数据吞吐量：支持大规模数据的快速读写和处理。
3.2 兼容性
操作系统支持：兼容Linux和Windows操作系统。
数据库兼容性：支持MySQL、PostgreSQL等主流数据库。
3.3 安全性
数据加密：支持静态数据和传输数据的加密。
访问控制：基于角色的权限管理，确保数据安全。

1. 产品合规性声明
公司在产品的设计、开发、生产和销售过程中，严格遵守以下法律法规：
《产品质量法》——确保产品符合国家质量标准，不存在虚假宣传。
《消费者权益保护法》——保障用户在使用产品过程中的合法权益。
《数据安全法》——严格保护用户个人信息，未经授权不得向第三方泄露。
2. 知识产权与法律责任
本公司产品中涉及的所有代码、算法和技术均受《著作权法》和《专利法》保护。
用户不得擅自修改、复制或分发公司产品中的任何组件。
如因产品缺陷导致用户损失，公司承担修复责任，但因用户使用不当或未遵守产品使用说明导致的损失，公司不承担责任。
3. 合同履行责任
公司在合同中明确约定产品功能、交付标准和服务期限。
若因公司原因未履行合同约定内容，用户有权根据合同条款要求公司承担违约责任。
若用户在产品中嵌入或调用外挂、脚本工具或未经授权的API，公司有权终止服务并保留追究法律责任的权利
"""</code></pre><p>通过灵活的知识库配置和可控的 RAG 召回服务，LazyLLM 能实现共享方式的多样性。</p><hr/><h2><strong>四、对话管理以及解决方案</strong></h2><p><br/></p><p>在企业实际业务中，对话系统需要能<strong>记住用户之前的聊天内容</strong>，并根据历史对话更好地理解用户当前的需求。比如，客服机器人需要知道用户之前问过什么问题，避免重复回答；或者AI助手能结合之前的聊天内容，优化当前的问题，让回答更精准。</p><p>同时，系统要<strong>支持多用户同时使用</strong>，确保不同用户的对话互不干扰。比如，A用户在和机器人聊订单问题，B用户在咨询产品信息，系统要能区分他们的对话记录，不会混淆。</p><p>此外，还要支持<strong>实时流式输出</strong>，让用户能像正常聊天一样逐步看到回复，而不是等待全部生成完才显示。</p><hr/><p>为了实现这些功能，lazyllm提供了一个统一的配置中心globals，管理不同用户的对话历史、上下文信息等，并在对话结束后自动清理不必要的数据，避免资源浪费。</p><p>通过 globals，系统能够隔离不同会话的对话历史与上下文，避免数据干扰，同时集中保存用户对话参数、传入的文件路径以及对话过程中产生的中间结果，供后续处理环节使用。这种设计保证了跨服务之间的数据一致性与高效流转。</p><p>接下来介绍如何通过globals 实现历史对话管理和多用户并发的对话管理。</p><h3><strong>（一）历史对话管理</strong></h3><p>首先我们通过结合 globals 配置中心，实现一个支持历史上下文、流式输出的对话流程。lazyllm.OnlineChatModule(stream=True)初始化一个支持流式输出的大语言模型，并使用 ThreadPoolExecutor 建立线程池，支持最多 50 路并发请求。每个请求在 slots 中分配一个空闲位置，确保同一时刻不会超过设定的最大连接数。</p><ul><li>每次新会话启动时，init\_session\_config 会结合默认 few-shot 提示与用户自定义的历史对话，统一初始化到对应的 session_id 下的 globals 空间中，保证每个会话拥有独立的上下文。</li><li>为了安全地在多线程环境中管理模型推理过程，respond_stream 方法在提交推理任务时使用了 contextvars 拷贝当前上下文，避免不同会话间上下文变量串联。整个响应过程中，通过 FileSystemQueue 实现边生成边输出的流式效果，并在推理结束后，把最新的用户输入和助手回复追加进会话历史。</li></ul><p>为了便捷地管理 session 生命周期，系统提供了with\_session 装饰器自动完成上下文切换，而handle\_request 函数作为统一入口，能够根据传入的session_id 和历史记录，发起一轮新的流式对话。</p><p>这种机制不仅实现了多用户隔离、历史记忆管理，还确保了高并发场景下的稳定性和一致性，为后续接入更多复杂交互提供了基础支撑。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=ethIbKX7SuEsYSoCrHXPJg%3D%3D.ayx5nbiEb7H9FT7KmUFnDQlCWWEu3NEfFBtw8zSr37yzBwFay4XWynmgTzHCwebXv%2BCTIUEb%2FuIK9fqMzhSv8eNFLvQ1d2Z4bEUO7xqgPrP2WYBgP4le9KLo8%2Bys7gvJzBeq0vaZC%2BN8WiA3qEnre%2FFb1AOQ8c3F2pQMqvWcs2X%2B%2BEYLTikMHjd5tz9wdE7b" rel="nofollow" target="_blank">https://github.com/FFFFFancy/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter17/chat\_with\_history.py#L10）</a></p><pre><code>from lazyllm import globals


llm = lazyllm.OnlineChatModule(stream=True)
threadPool = ThreadPoolExecutor(max_workers=50)
slots = [0] * 50

# 公共 few shot 历史
DEFAULT_FEW_SHOTS = [
    {"role": "user", "content": "你是谁？"},
    {"role": "assistant", "content": "我是你的智能助手。"}
]

class ChatHistory(BaseModel):
    user: str
    assistant: str

class ChatRequest(BaseModel):
    user_input: str
    history: Optional[List[ChatHistory]] = None

def allocate_slot():
    for idx, val in enumerate(slots):
        if val == 0:
            slots[idx] = 1
            return idx
    return -1

def release_slot(session_id):
    if 0 &lt;= session_id &lt; len(slots):
        slots[session_id] = 0
        globals.pop(session_id)

def init_session_config(session_id, user_history=None):
    globals._init_sid(session_id)

    if user_history is not None:
        history = []
        # 合并 few-shot + 用户历史
        history.extend(DEFAULT_FEW_SHOTS)
        for h in user_history:
            history.append({"role": "user", "content": h.user})
            history.append({"role": "assistant", "content": h.assistant})
        globals["global_parameters"]["history"] = history
    else:
        if "history" not in globals["global_parameters"]:
            globals["global_parameters"]["history"] = copy.deepcopy(DEFAULT_FEW_SHOTS)

def with_session(func):
    def wrapper(session_id, *args, **kwargs):
        globals._init_sid(session_id)
        return func(session_id, *args, **kwargs)
    return wrapper

class SessionResponder:
    def __init__(self):
        pass

    def respond_stream(self, session_id, model_in, user_history=None):
        init_session_config(session_id, user_history)

        print("[Respond Stream] Current SID:", globals._sid)
        history = globals["global_parameters"]["history"]
        print("history", history)

        ctx = contextvars.copy_context()
        func_future = threadPool.submit(lambda: ctx.run(llm, model_in, llm_chat_history=history))

        response = ''

        while True:
            assert session_id == globals._sid, f"\nSession ID mismatch: expected {session_id}, got {globals._sid}"

            if message := FileSystemQueue().dequeue():
                msg = "".join(message)
                response += msg
                yield msg
            elif func_future.done():
                break

        model_out = func_future.result()

        assert session_id == globals._sid, f"Session ID mismatch after LLM: expected {session_id}, got {globals._sid}"

        # 更新历史
        globals["global_parameters"]["history"].append({
            "role": "user",
            "content": model_in
        })
        globals["global_parameters"]["history"].append({
            "role": "assistant",
            "content": model_out
        })

        return model_out

@with_session
def handle_request(session_id: str, user_input: str, user_history: Optional[List[ChatHistory]] = None):
    chat = SessionResponder()
    for chunk in chat.respond_stream(session_id, model_in=user_input, user_history=user_history):
        print(chunk, end='', flush=True)</code></pre><p>整体来看，这段代码依托 globals 可实现：</p><ul><li>灵活加载和隔离管理不同用户的历史对话；</li><li>支持系统内部预置 few-shot 示例，引导模型更好地理解任务；</li><li>在结合历史上下文的基础上，总结、改写并生成新的对话内容或问题。</li></ul><p>让我们来看一下执行效果：</p><pre><code>###################################
# 使用历史对话
###################################
history = [
    ChatHistory(
        user="香蕉的英文是什么？",
        assistant="香蕉的英文是banana"
    ),
    ChatHistory(
        user="那苹果呢？",
        assistant="苹果的英文是apple"
    )
]
user_input = "那橘子呢？"

response = ""
for chunk in respond_stream(session_id, user_input, history):
    response += chunk
# &gt;&gt;&gt; 橘子的英文是orange。

#####################################
# 内部预置 few-shot 示例
#####################################
DEFAULT_FEW_SHOTS = [
    {"user": "请帮我改写：'这个报告写得还可以。'", 
     "assistant": "这份报告整体表现良好，但仍有提升空间。"},
    {"user": "请帮我优化：'我们的销售业绩不错。'", 
     "assistant": "我们的销售业绩表现出色，达到了预期目标。"},
]

user_input = "请帮我改写：'客户反馈我们服务态度很好。'"
# &gt;&gt;&gt; 客户对我们服务态度给予了积极评价。


#####################################
# 总结历史示例
#####################################
history = [
    ChatHistory(
        user="机器学习是什么？",
        assistant="机器学习是一门开发算法和统计模型的科学，计算机系统使用这些算法和模型，在没有明确指令的情况下，依靠既有模式和推理来执行任务"
    ),
    ChatHistory(
        user="机器学习的应用场景？",
        assistant="机器学习被广泛应用于推荐系统中，如电商网站的商品推荐、社交媒体的内容推荐等。通过分析用户的历史行为和偏好，机器学习算法可以预测用户可能感兴趣的内容，并提供个性化的推荐。自然语言处理：自然语言处理是机器学习的另一个重要应用领域，包括语音识别、机器翻译、情感分析、垃圾邮件过滤等。机器学习算法可以帮助计算机理解和生成人类语言，实现人机交互的智能化。图像识别和处理：机器学习在图像识别和处理方面也发挥着重要作用，如人脸识别、车牌识别、图像检索、物体识别等。通过训练大量的图像数据，机器学习算法可以学习并识别出图像中的特征，从而实现对图像的智能处理。"
    )
]
user_input = "总结下这段对话"

# &gt;&gt;&gt; 这段对话主要围绕机器学习展开，首先解释了机器学习的定义，即通过算法和统计模型，让计算机系统在没有明确指令的情况下，基于既有模式和推理完成任务。接着讨论了机器学习的应用场景，包括推荐系统（如电商和社交媒体个性化推荐）、自然语言处理（如语音识别、机器翻译、情感分析等）以及图像识别和处理（如人脸识别、物体识别等）。最后总结了机器学习在智能化任务中的重要作用。</code></pre><ul><li>第一次请求只输入 “橘子”，系统按历史上下文正常生成与橘子相关的回答“orange”。</li><li>第三次请求输入 "总结这段对话"，系统基于完整历史成功输出对话总结。</li><li>并且预先指定的对话也在ChatHistory中。</li></ul><h3><strong>（二）多用户并发对话管理</strong></h3><p>同样基于上述设计，以下代码进一步完善了基于 globals 的<strong>多用户会话管理与历史对话追踪机制</strong>。系统初始化了一个流式推理模块 lazyllm.OnlineChatModule(stream=True)，并通过 ThreadPoolExecutor 支持最多 50 个并发请求，同时通过 slots 数组管理连接资源，保证不同用户会话互不干扰。</p><ul><li>每当新的请求到来时，init\_session\_config 方法根据传入的用户历史，初始化对应 session\_id 下的上下文环境，若无历史则默认分配一个空白对话历史，确保每条会话轨迹独立。为了简化多次会话调用的上下文切换，with\_session 装饰器自动在执行函数前绑定正确的 session_id。</li><li>实际推理时，SessionResponder 类负责发起流式对话，内部使用 contextvars 捕获当前上下文，保证即使在线程池中执行推理任务，也能维持正确的会话隔离。系统通过 FileSystemQueue 实现流式输出，在推理过程中实时返回生成的内容，推理完成后，再将本轮对话完整地追加到历史记录中，以便后续连续对话使用。</li></ul><p>最后，通过外部示例展示了如何用 handle_request 函数发起多轮对话，系统能够正确地维护多个用户之间独立且连续的对话流，保证不同用户的历史上下文不会混淆，为支持高并发、强上下文连贯性的应用场景打下了良好的基础。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=dR8foKuzWQbi%2FQsJnGFJlw%3D%3D.YS6jOk69UbAHTLTa6M3byeWpr67zJzVd6%2ByPEemLCGpbm3JqShpBav%2FNlTyqPrgzzWC%2BWspfHsgM4lFqUbIrDhGvE5VJE7ps0%2BYFUA4x2T08ijxyoLZ6Dbjs021iSmTHWe0vEbLmBSD%2BCKca8JY%2FOpF24aWcHEqiJsV1tdN4aiziQ%2BgItdD2Nbntlgq0vboS" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/a09a84cdf0585a5c9d52af6db0e965be95d03123/rag/codes/chapter17/chat\_with\_multi_user.py#L11）</a></p><pre><code>llm = lazyllm.OnlineChatModule(stream=True)
threadPool = ThreadPoolExecutor(max_workers=50)
slots = [0] * 50

DEFAULT_FEW_SHOTS = []

class ChatHistory(BaseModel):
    user: str
    assistant: str

class ChatRequest(BaseModel):
    user_input: str
    history: Optional[List[ChatHistory]] = None

def allocate_slot():
    for idx, val in enumerate(slots):
        if val == 0:
            slots[idx] = 1
            return idx
    return -1

def release_slot(session_id):
    if 0 &lt;= session_id &lt; len(slots):
        slots[session_id] = 0
        globals.pop(session_id)

llm = lazyllm.OnlineChatModule(stream=True)

def init_session_config(session_id, user_history=None):
    globals._init_sid(session_id)
    # if globals._sid not in globals._Globals__data:
    #     globals._Globals__data[globals._sid] = copy.deepcopy(globals.__global_attrs__)

    if user_history is not None:
        globals["global_parameters"]["history"] = user_history
    else:
        if "history" not in globals["global_parameters"]:
            globals["global_parameters"]["history"] = []

def with_session(func):
    """自动绑定 session_id 的装饰器"""
    def wrapper(session_id, *args, **kwargs):
        globals._init_sid(session_id)
        return func(session_id, *args, **kwargs)
    return wrapper

class SessionResponder:
    def __init__(self):
        pass

    def respond_stream(self, session_id, model_in, user_history=None):
        init_session_config(session_id, user_history)

        print("[Respond Stream] Current SID:", globals._sid)
        history = globals["global_parameters"]["history"]
        print("history", history)

        # 捕获当前上下文（确保线程池提交的任务也带上下文）
        ctx = contextvars.copy_context()
        func_future = threadPool.submit(lambda: ctx.run(llm, model_in, llm_chat_history=history))

        response = ''

        while True:
            assert session_id == globals._sid, f"\nSession ID mismatch: expected {session_id}, got {globals._sid}"

            if message := FileSystemQueue().dequeue():
                msg = "".join(message)
                response += msg
                yield msg
            elif func_future.done():
                break

        model_out = func_future.result()

        assert session_id == globals._sid, f"Session ID mismatch after LLM: expected {session_id}, got {globals._sid}"

        # globals["global_parameters"]["history"].append(model_out)
        globals["global_parameters"]["history"].append({
            "role": "user",
            "content": model_in
        })
        globals["global_parameters"]["history"].append({
            "role": "assistant",
            "content": model_out
        })

        return model_out

# 外部使用示例
@with_session
def handle_request(session_id: str, user_input: str):
    chat = SessionResponder()
    for chunk in chat.respond_stream(session_id, model_in=user_input):
        print(chunk, end='', flush=True)

if __name__ == "__main__":

    handle_request("user321", "苹果的英文是什么！")
    print("\n\n")
    handle_request("user123", "机器学习是什么")
    print("\n\n")
    handle_request("user321", "香蕉呢")
    print("\n\n")
    handle_request("user123", "它有什么用？")</code></pre><p>效果示例：</p><ul><li>用户 id1 问“苹果的英文”，再问“香蕉”时，模型能记住当前会话是翻译任务。</li><li>用户 id2 问“机器学习是什么？”后，追问“它有什么作用？”时，模型能关联上下文解释应用场景。</li></ul><p>二者维度独立的历史对话内容，相互不影响。</p><blockquote><p>🚨注意，实现上述功能需要用redis数据库实现文件系统输出管理，设置方法为：</p><p>export LAZYLLM\_DEFAULT\_FSQUEUE=SQLITE</p><p>export LAZYLLM\_FSQREDIS\_URL=redis://[user name]:[password]@[host]/[port]</p></blockquote><hr/><h2><strong>五、安全需求以及解决方案</strong></h2><p><br/><br/>最后，企业在构建知识库的过程中，不仅要保障内部的信息安全，还需关注面向公众时的外部安全，防止数据泄露、信息滥用以及潜在的合规风险。</p><h3><strong>（一）企业安全</strong></h3><p>在企业知识库建设中，安全始终是首要考虑因素。尤其是当知识库中包含公司政策、财务报表、客户合同等高度敏感或私有数据时，任何信息泄露都可能带来严重的法律责任和商业损失。为此，系统需具备完善的保护机制。</p><h4><strong>1. 加密</strong></h4><ul><li><strong>私有数据保护</strong>：通过数据隔离机制，确保不同业务或部门间的数据隔离，防止数据泄露。</li><li><strong>知识加密</strong>：对文档在存储与传输过程中的全链路加密，确保数据机密性与完整性。</li><li><strong>模型加密</strong>：支持模型调用过程中的数据加密，避免敏感信息泄露。</li></ul><h4><strong>2. 私有化部署</strong></h4><ul><li><strong>本地化模型推理引擎</strong>：核心组件部署于内网环境，保障数据安全。</li><li><strong>数据本地处理</strong>：确保知识数据在企业内部完成，避免外泄。</li><li><strong>强化权限控制</strong>：结合网络隔离和多因子认证，实现安全访问。</li></ul><h4><strong>3. 信创</strong></h4><p>为保障核心技术自主可控，系统全面兼容国家信创名录中的软硬件产品。</p><ul><li><strong>国产CPU</strong>：鲲鹏、龙芯等，提供高性能计算支持。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445328" alt="image.png" title="image.png" loading="lazy"/></p><ul><li><strong>国产操作系统</strong>：麒麟、统信UOS等，确保系统底层安全。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445329" alt="image-2.png" title="image-2.png" loading="lazy"/></p><ul><li><strong>国产数据库</strong>：达梦、人大金仓等，敏感数据存得更放心。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445330" alt="image-3.png" title="image-3.png" loading="lazy"/></p><ul><li><strong>全链路合规</strong>：从芯片（如鲲鹏/飞腾）到软件均符合信创标准，通过国家信息安全认证。</li></ul><h3><strong>（二）公众安全</strong></h3><p>在企业级RAG系统中，公共安全不仅关乎企业自身的声誉与合规风险，更关联到模型输出对社会舆论、信息安全乃至国家安全的影响。系统应具备以下能力，确保模型生成内容不突破公共安全底线：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445331" alt="image.png" title="image.png" loading="lazy"/></p><blockquote><ul><li><strong>涉暴涉恐内容识别</strong>：自动检测与暴力、恐怖主义、极端言论相关的内容，防止模型成为非法信息的传播通道。</li><li><strong>涉政敏感内容拦截</strong>：对分裂言论、非法组织宣传、造谣煽动等进行精准识别与阻断，避免引发政治敏感风险。</li><li><strong>群体性事件识别</strong>：识别模型输出中可能煽动公众聚集、对抗或恐慌的内容，防范舆情发酵升级。</li><li><strong>突发事件内容引导</strong>：在重大灾害、公共卫生或社会事件发生时，提供经过验证的信息输出机制，减少虚假内容传播风险。</li><li><strong>跨境内容安全保障</strong>：在涉外输出场景中，支持配置涉敏国家/地区、组织和外交事件的识别策略，确保对外表态合法合规。</li><li><strong>安全联动机制</strong>：支持与企业安全系统、监管平台或应急响应机制打通，实现内容风险的实时发现与快速响应。</li></ul></blockquote><p>通过公共安全模块的建设，企业可有效防控大模型在生成内容过程中可能引发的社会层面风险，提升企业数字治理能力，践行平台责任。</p><h4><strong>1. 如何维护公共安全 ？</strong></h4><p>在知识库管理和检索过程中，可能涉及一些敏感信息或违规内容，如个人隐私、法律合规性条款等。因此，系统需要具备<strong>全面的过滤机制</strong>，在数据上传、存储和检索阶段，自动识别和屏蔽敏感词汇，防止敏感信息的误用或泄露。</p><p>LazyLLM支持<strong>灵活的自定义规则配置</strong>，管理员可以根据企业实际需求，动态维护敏感词列表，结合分词、正则表达、DFA（Deterministic Finite Automaton）等算法实现精准过滤。</p><p>接下来我们来看如何将敏感词过滤的典型算法——<strong>DFA算法</strong>，接入lazyllm实现敏感词过滤。</p><h5><strong>第一步：</strong></h5><p>用定义lazyllm模块的方式实现DFA算法的定义 ，将其包装为可接入lazyllm的组件，详见 [ 第8讲：不止是cosine！匹配策略决定你召回的质量 ] - 基于 class 的自定义 Transform 算法。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=clnCo%2FvryfJeeYBoVfg9fA%3D%3D.Z2h5oZFOwuAPdCcOjP49H4P8z6Kirk9vQggPiQQ5BGGDUtWgLKcsBzI4mMZrMW0WZ1Osgkccg5t8dzq94lHi5iBErg8pX18p5WhZWohnxlHoN9RD50ErmIETUaCD6xk1l2KTLjH1kUJhjl7XV68xpav3Rcw3cD0iNGChs9X8C%2FTQIIHEXIoECzNqOFDUdoRt" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/retriever\_with\_DFA.py#L10）</a></p><pre><code>from lazyllm.tools.rag import DocNode, NodeTransform
from typing import List

# 定义DFA算法
class DFAFilter:
    def __init__(self, sensitive_words):
        self.root = {}
        self.end_flag = "is_end"
        for word in sensitive_words:
            self.add_word(word)

    def add_word(self, word):
        node = self.root
        for char in word:
            if char not in node:
                node[char] = {}
            node = node[char]
        node[self.end_flag] = True

    def filter(self, text, replace_char="*"):
        result = []
        start = 0
        length = len(text)

        while start &lt; length:
            node = self.root
            i = start
            while i &lt; length and text[i] in node:
                node = node[text[i]]
                if self.end_flag in node:
                    # 匹配到敏感词，替换为指定字符
                    result.append(replace_char * (i - start + 1))
                    start = i + 1
                    break
                i += 1
            else:
                # 未匹配到敏感词，保留原字符
                result.append(text[start])
                start += 1

        return ''.join(result)


# 注册为transform
class DFATranform(NodeTransform):
    def __init__(self, sensitive_words: List[str]):
        super(__class__, self).__init__(num_workers=num_workers)
        self.dfafilter = DFAFilter(sensitive_words)

    def transform(self, node: DocNode, **kwargs) -&gt; List[str]:
        return self.dfafilter.filter(node.get_text())

    def split_text(self, text: str) -&gt; List[str]:
        if text == '':
            return ['']
        paragraphs = text.split(self.splitter)
        return [para for para in paragraphs]</code></pre><h5><strong>第二步：</strong></h5><p>将定义的DFAFilter 注册为文档服务的node group。同样使用上小节中"A产品功能参数和产品合规性声明"的检索问题，假设屏蔽【合同】这个词，只需DFAFilter注册为新的节点，并通过parent="sentences"继承上一步的处理方式。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=RavVUKC33BbGqgbysYyW3Q%3D%3D.qRyS2l7VruoUDcl4dQ1TwTi%2FLKRRjUAEKEbHEvOQDAXRziMV8J9jfpgINxtwRpLLeIzpvyuxtnNLMHGti8xWc2LSEcnZwAyE1W4HamnlTPtAblVuzv3SBAd5QgppQ5xA0W%2BOM4QWax2%2FBiCqYqbUQqpi3MOkC17BKdPqyYgCEsJA0OtQ3v7aprYL9yOvpHwV" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/retriever\_with\_DFA.py#L65）</a></p><pre><code>from lazyllm import Retriever，SentenceSplitter
# 定义业务敏感词
sensitive_words = ['合同']
# 将敏感词过滤算法嵌入到业务逻辑中
Document.create_node_group(name="sentences", transform=SentenceSplitter, chunk_size=128, chunk_overlap=10)
Document.create_node_group(name="dfa_filter", parent="sentences"，transform=DFATranform(sensitive_words))

# 组合法务 + 产品知识库，处理与产品相关的法律问题
retriever_product = Retriever(
    [law_knowledge_base, product_knowledge_base],
    group_name="dfa_filter",       # 指定 dfa_filter
    similarity="cosine",       
    topk=2                
)

product_question = "A产品功能参数和产品合规性声明"
product_res_nodes = retriever_product(product_question)</code></pre><p>可以看到，输出结果已将“敏感词”和“合同”替换为星号。在企业应用场景中，可以根据业务需求自定义敏感词库，以增强数据安全性。</p><p>屏蔽前：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445332" alt="image.png" title="image.png" loading="lazy"/></p><p>屏蔽后：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445333" alt="image-2.png" title="image-2.png" loading="lazy"/></p><p><strong>全流程敏感词过滤</strong> 在实际应用中，除了原文档内容进行敏感词过滤外，我们还需对用户输入和大模型输出进行同样的处理。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445334" alt="image-3.png" title="image-3.png" loading="lazy"/></p><pre><code>with pipeline() as ppl:
    ppl.query_filter = lambda x: DFAFilter(sensitive_words).filter(x)
    ppl.retriever = Retriever(...)
    ppl.reranker = ...
    ppl.llm = ...
    ppl.output_filter = lambda x: DFAFilter(sensitive_words).filter(x)</code></pre><p>针对企业级需求，lazyllm提供了灵活的文档管理和召回服务。通过配置不同的算法和知识库，系统能够在不同业务场景下，满足<strong>跨部门的数据处理和精准召回需求</strong>。</p><p>借助数据库管理功能，系统实现了<strong>数据隔离</strong>和<strong>权限控制</strong>，有效保障私有数据的安全性。</p><p>同时，系统支持<strong>标签检索</strong>和<strong>敏感词过滤</strong>，进一步提升检索的精准度和合规性，帮助企业在复杂的数据环境中高效管理和利用知识库。</p><hr/><h2><strong>六、企业级RAG的总体实现思路</strong></h2><p><br/><br/>在前文中，我们从<strong>权限控制、共享方式、安全保障</strong>等多个维度详细解析了企业级RAG系统在真实落地过程中面临的核心需求与挑战。</p><p>接下来，我们将整合上述要素，提出一个功能完善、可落地的企业级RAG搭建思路。</p><h3>（一）<strong>架构图</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445335" alt="image.png" title="image.png" loading="lazy"/></p><p>该企业级RAG系统主要由用户接入、意图识别、检索器、知识库、算法库、重排序器以及大模型模块组成。</p><p>用户通过接入模块提交查询，请求首先经过<strong>意图识别模块</strong>，判断查询意图并动态决定后续检索策略。<strong>检索器模块</strong>由多个Retriever组成，能够灵活调用不同的知识库和算法库，完成多策略、多数据源的检索任务。<strong>知识库</strong>用于存储各类结构化或非结构化文档，<strong>算法库</strong>则包含多种向量化工具和解析器，支持知识数据的编码和处理，二者均可按需灵活组合。检索完成后，候选结果经过<strong>重排序模块</strong>优化相关性，再由<strong>大语言模型(LLM)</strong> 基于优化后的内容进行生成，最终输出符合用户需求的答案。</p><p>整个系统设计强调模块解耦、策略灵活和生成增强，适配多用户、高并发和多场景的企业应用需求。</p><h3><strong>（二）代码实现</strong></h3><p>接下来，以一个电商场景为例，我们将构建一个具备上述功能的 RAG 问答系统。本次示例中共使用三个知识库，数据准备如图所示，数据库的构建方法已在第二讲中详细介绍，此处不再赘述。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445336" alt="image-2.png" title="image-2.png" loading="lazy"/></p><p>我们将产品知识库与法务知识库联合检索，用于处理产品及法务相关问题；同时将法务知识库与用户支持知识库组合，以应对用户支持类问题。为此，首先构建两条独立的 RAG pipeline。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=mL6IFwmgZAd5RoJZapyNaA%3D%3D.9aj9fodeuOnWtvbIIAA9zKldy3nHSAVah6e%2BAxTNLCJQL%2FLaaM6exGeTN4BYi2Mb9PbRcZAiEX%2BdE0QQelZhumk6Q4i4H65vZwnfJNWKWmuF3PUMNR8EFSN0jVYXQg3CO%2BVji0A9DhWc5uD1CPQJJ5C5G79zf4T1JsDtF0m8BJ1zjPsyAwMmpQLZQqjz5YgC" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/ecommerce_rag.py#L180）</a></p><pre><code>with pipeline() as product_law_ppl:
    product_law_ppl.retriever = retriever = Retriever(
            [doc_law, doc_product],
            group_name="dfa_filter",   
            topk=5, 
            embed_keys=['dense'],
        )
    product_law_ppl.reranker = Reranker(name="ModuleReranker",
                            model=OnlineEmbeddingModule(type='rerank'),
                            topk=2, output_format="content", join=True) | bind(query=product_law_ppl.input)
    product_law_ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=product_law_ppl.input)
    product_law_ppl.llm = OnlineChatModule().prompt(lazyllm.ChatPrompter(prompt, extra_keys=["context_str"]))

with pipeline() as support_law_ppl:
    support_law_ppl.retriever = retriever = Retriever(
            [doc_law, doc_support],
            group_name="dfa_filter",   
            topk=5, 
            embed_keys=['dense'],
        )
    support_law_ppl.reranker = Reranker(name="ModuleReranker",
                            model=OnlineEmbeddingModule(type='rerank'),
                            topk=2, output_format="content", join=True) | bind(query=support_law_ppl.input)
    support_law_ppl.formatter = (lambda nodes, query: dict(context_str=nodes, query=query)) | bind(query=support_law_ppl.input)
    support_law_ppl.llm = OnlineChatModule().prompt(lazyllm.ChatPrompter(prompt, extra_keys=["context_str"]))</code></pre><p>为用户提供统一的问答入口，并实现不同知识库间的无缝切换，我们引入了用户意图识别模块，能够根据查询内容自动选择合适的 RAG pipeline 进行处理。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=SXGPYYH6I7y5jmnlXu4vqw%3D%3D.8be9vIxI6HTE5inh3XA%2FuxY52N6v2UHuO0rmXxvACj4dNdJwdYju%2F3LHDO188aBWD%2BObacJxAgTbyeQbGd7GAQaSXQqGm%2F9nKiNg2xCb%2Brt2iZYRpOz2bXgBSff7BkUmrrCoLYbmFYt%2BPgk7ntOOkyu2bFyjj4O9RXvYxV8mGk5cd5AR0p31UHhFUoeUIdK4" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/ecommerce_rag.py#L195）</a></p><pre><code>def build_ecommerce_assistant():
    llm = OnlineChatModule(source='qwen', stream=False)
    intent_list = [
        "产品法务问题",
        "用户支持问题",
    ]

    with pipeline() as ppl:
        ppl.classifier = IntentClassifier(llm, intent_list=intent_list)
        with lazyllm.switch(judge_on_full_input=False).bind(_0, ppl.input) as ppl.sw:
            ppl.sw.case[intent_list[0], product_law_ppl]
            ppl.sw.case[intent_list[1], support_law_ppl]
    return ppl</code></pre><p>为了实现多用户并发会话请求并维护独立的上下文，我们通过 globals 管理器封装了 EcommerceAssistant，确保用户问答的隔离性。</p><p>（代码GitHub链接：<a href="https://link.segmentfault.com/?enc=u3Vcqka2tCQwU9V%2Bn2sDSg%3D%3D.yzsJTBoSKtESzhRV9pV%2FYBuPvndu8fW32IG1Wpq8fFoKNqyFhtvHayO4%2FsIuJD8HXcFvLR0PRFJF3%2BF59x5%2FqDxmEg%2Bla2eExCiMfUMWRgGRnBkdesyWdkcD%2FfD8PZIYHSQDubbLZXAj481Wr%2FUSruKX%2BX3nGPx%2Fm8EW1nXATwAsBNrLScrBhL4TT9spk1Ml" rel="nofollow" target="_blank">https://github.com/LazyAGI/Tutorial/blob/7abc91dbb82a007a78731845dd8c360ac0cc1e75/rag/codes/chapter17/ecommerce_rag.py#L223）</a></p><pre><code>def init_session(session_id, user_history: Optional[List[ChatHistory]] = None):
    globals._init_sid(session_id)

    if "global_parameters" not in globals or "history" not in globals["global_parameters"]:
        globals["global_parameters"]["history"] = []

    if not globals["global_parameters"]["history"]:
        # 初始化为 default few-shot
        globals["global_parameters"]["history"].extend(DEFAULT_FEW_SHOTS)

    if user_history:
        for h in user_history:
            globals["global_parameters"]["history"].append({"role": "user", "content": h.user})
            globals["global_parameters"]["history"].append({"role": "assistant", "content": h.assistant})

def build_full_query(user_input: str):
    """根据 globals 里的历史，生成带历史的 full query文本"""
    history = globals["global_parameters"]["history"]
    history_text = ""
    for turn in history:
        role = "用户" if turn["role"] == "user" else "助手"
        history_text += f"{role}: {turn['content']}\n"

    full_query = f"{history_text}用户: {user_input}\n助手:"
    return full_query

class EcommerceAssistant:
    def __init__(self):
        self.main_pipeline = build_ecommerce_assistant()

    def __call__(self, session_id: str, user_input: str, user_history: Optional[List[ChatHistory]] = None):
        init_session(session_id, user_history)

        full_query = build_full_query(user_input)

        # 把带历史的 query 输入主 pipeline
        response = self.main_pipeline(full_query)

        # 更新历史到 globals
        globals["global_parameters"]["history"].append({"role": "user", "content": user_input})
        globals["global_parameters"]["history"].append({"role": "assistant", "content": response})

        return response</code></pre><h3><strong>（三）结果分析</strong></h3><p>运行日志：</p><pre><code>==================== user1：用户支持问题 ====================
用户 user1 提问：
「用户投诉某智能手表的续航没有达到宣传效果，该怎么处理」

助手回复：
当用户投诉智能手表续航未达到宣传效果时，我们需要按照三级响应机制进行处理：

1. **技术验证**：请用户提供设备序列号及续航测试视频（需显示完整充放电周期）。我们内部将通过工具[PERF_CHECK_V3]对比宣传参数。

2. **法律评估**：触发法务系统自动生成《参数差异分析报告》，参考案例FC-2024-021进行详细对比。

3. **解决方案**：
   - 如果差异≤15%：我们将赠送用户1年延保（服务代码SV-228）。
   - 如果差异＞15%：根据《消费者权益保护法》第23条，我们将提供换新或差价三倍赔偿。

沟通话术："我们高度重视产品参数的准确性，将委托第三方机构（如SGS）进行复测，并在3个工作日内给您正式答复。"

依据《消费者权益保护法》第23条，商品实际性能与宣传参数差异超过行业标准允许误差范围（电子设备续航误差±15%）即构成虚假宣传。参考案例FC-2024-021：某品牌因智能手表续航虚标28%被判定三倍赔偿，并受到市场监督管理局50万元行政处罚。

建议技术部门建立续航测试标准流程（需符合GB/T 35143-2023标准），所有宣传数据必须附带测试环境说明（如：实验室环境25℃下连续使用）。

============================================================

====================user2：产品法务问题（带历史对话） ====================
用户 user2 的对话历史：
1. 用户: 「你好」
   助手: 「你好呀！」
2. 用户: 「我想咨询耳机宣传内容是否合规」
   助手: 「当然，请详细描述你的宣传文案。」

用户 user2 新提问：
「骨传导耳机不展示专利号」

助手回复：
根据提供的信息，如果骨传导耳机的宣传中涉及已申请或已授权的专利技术（如骨传导振子技术），但未明确标注专利号和专利类型（发明、实用新型或外观设计），这可能违反《广告法》第十二条以及相关实施细则的要求。

以下是具体分析：

1. **专利号标注问题**  
   - 根据法律规定，在广告宣传中提及专利技术时，必须完整标注专利号及专利类型。例如：“本产品采用骨传导振子技术，专利号：ZL2024XXXXXXX，实用新型专利。”
   - 如果仅提到“已申请专利”而未标明具体专利号，则属于不合规行为。参考AD-2024-017号处罚案例，某科技公司因未标注专利号被罚款87.6万元。

2. **跨境广告要求**  
   - 若该耳机宣传涉及跨境市场（如出口到其他国家），还需同步标注专利授权国家代码（如CN/EP/US）。例如：“专利号：ZL2024XXXXXXX，CN实用新型专利。”

3. **技术独占性声明合规性**  
   - 宣传文案需遵循“三不原则”：
     - 不暗示技术垄断（避免使用“唯一”“独家”等绝对化用语）。
     - 不扩大保护范围（确保与专利权利要求书严格对应）。
     - 不隐瞒专利状态变更（如专利被宣告无效，需在3个工作日内更新宣传物料）。

### 建议
- **立即补充专利信息**：在宣传中明确标注专利号及类型。如果专利正在申请中，应注明“专利申请中”而非直接宣称“已申请专利”。
- **避免绝对化用语**：不要使用“全球领先”“唯一”等表述，以免引发虚假宣传风险。
- **定期检查专利状态**：确保宣传内容与实际专利状态一致，及时更新。

如果你无法提供具体的专利号或专利类型，建议先调整宣传文案，避免因违规而遭受处罚。例如，可以改为描述功能性特点，如“采用先进的骨传导技术，实现IPX7级防水性能”。

============================================================

====================user1 ：用户支持问题跟进 ====================
用户 user1 继续提问：
「这种投诉处理方式有什么风险吗？」

助手回复：
这种处理方式虽然遵循了法律法规和行业标准，但仍可能存在以下潜在风险：

1. **用户信任受损**：如果用户的设备续航差异较大但未超过15%，仅提供延保服务可能无法完全修复用户体验，进而影响品牌忠诚度。

2. **测试环境争议**：宣传数据通常基于实验室条件（如25℃下连续使用），而实际使用场景复杂多变（高温、低温、高负载等）。若用户对测试环境说明不了解或不认可，可能引发进一步投诉。

3. **法务风险扩大**：如果部分产品续航虚标超过15%，且涉及较大用户群体，可能会被认定为系统性虚假宣传，从而面临集体诉讼或更严重的行政处罚。

4. **技术验证效率问题**：通过工具[PERF_CHECK_V3]进行对比需要一定时间，若响应速度过慢，可能导致用户不满升级为舆情事件。

### 风险规避建议：
- **主动优化宣传策略**：在产品页面及包装中明确标注续航测试环境（如“实验室环境下25℃连续使用”），并补充说明实际使用可能存在的差异。
- **建立快速响应机制**：针对续航类投诉，设立专项客服团队，确保在48小时内完成初步评估并向用户提供解决方案。
- **加强内部流程管控**：技术部门需定期更新续航测试标准流程，确保符合GB/T 35143-2023要求，并将测试结果与市场宣传同步校准。
- **提供额外补偿措施**：对于续航差异接近临界值的用户，可考虑赠送配件（如充电器）或延长保修期，以提升用户满意度。

最终目标是通过透明化沟通和积极应对，将潜在风险降至最低，同时维护品牌形象和用户信任。</code></pre><p>根据以上日志，我们实现了：</p><h4><strong>1. 自动意图识别</strong></h4><p>RAG系统通过<strong>意图识别</strong>功能，能够<strong>自动分析用户的提问并选择合适的处理流程（Pipeline）</strong>。</p><p>例如，当收到用户支持性问题时，其结合了用户支持库的“三级响应机制”提出沟通话术，并结合和法规库的《消费者权益保护法》第23条提出针对性的法律规定；当接收到产品法规问题时，其从产品库中检索到骨传导振子技术相关内容，并结合法规库的技术独占性声明合规性相关内容。</p><p>这种意图识别功能能够使系统更高效地响应不同类型的问题，并且通过自动选择合适的Pipeline，使得用户的需求得到快速而准确的满足，减少了人工干预的需求，提升了整体响应速度和服务质量。</p><h4><strong>2. 多知识库联合检索</strong></h4><p>在助手的回答中，我们可以看到<strong>不同知识库内容的联合应用</strong>。</p><p>首先，关于智能手表续航的投诉问题，助手不仅依据<strong>技术文档</strong>提供了具体的测试方法，还引用了<strong>法务知识库</strong>中的条款和相关案例，全面涵盖了产品质量、消费者权益等多维度的信息。</p><p>同样，针对耳机宣传合规性问题，助手结合了<strong>广告法</strong>和<strong>专利法</strong>知识库中的内容，提供了详细的法律分析和合规建议。</p><p>这展示了RAG能力在处理复杂问题时的灵活性和高效性，通过跨多个领域的知识库联合提供精准答案。</p><h4><strong>3. 用户历史对话分离</strong></h4><p><strong>在回答中引入用户的历史对话并加以分离</strong>。</p><p>对于用户user2的追加提问（投诉处理方式），通过引用历史对话，保证了对用户之前提问的背景理解，确保了回答的准确性和连贯性。</p><h4><strong>4. 指定历史对话</strong></h4><p><strong>用户的历史对话可以被明确指定</strong>，以便提供更加个性化和细化的回答。</p><p>例如，user2在咨询耳机专利问题时，助手依据其前面的咨询内容“耳机宣传内容是否合规”进行相关的法律合规回答，并且通过引入历史对话数据，确保了回答的针对性和层次性，避免了重复性回答，并通过“补充专利信息”等建议，强化了问题的解决路径。</p><hr/><p><br/><br/>更多技术内容，欢迎移步 “LazyLLM” 讨论！</p>]]></description></item><item>    <title><![CDATA[DeepSeek V3.2发布：AI进入]]></title>    <link>https://segmentfault.com/a/1190000047445358</link>    <guid>https://segmentfault.com/a/1190000047445358</guid>    <pubDate>2025-12-03 11:04:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>一边是达到GPT-5水平的性能，一边是大幅降低的计算开销，DeepSeek V3.2正式版正在重新定义AI模型的性价比标准。</blockquote><p>2025年12月1日，深度求索公司宣布推出DeepSeek-V3.2和DeepSeek-V3.2-Speciale两个正式版模型。与以往单纯追求性能突破不同，此次发布凸显了AI产业的重要转向：从“唯参数论”到“效率至上”的战略转型。</p><p>标准版V3.2在公开推理测试中达到GPT-5水平，同时输出长度比Kimi-K2-Thinking大幅缩短，显著减少用户等待时间。而高计算版本V3.2-Speciale则专攻极致推理能力，斩获IMO 2025等四项国际竞赛金牌。<img width="730" height="391" referrerpolicy="no-referrer" src="/img/bVdneRR" alt="企业微信截图_17647284878922.png" title="企业微信截图_17647284878922.png"/></p><h3>效率与性能的平衡艺术</h3><p>DeepSeek此次发布最引人注目的突破在于实现了效率与性能的最佳平衡。在AI模型普遍追求更大参数、更长输出的当下，V3.2反其道而行，通过技术创新大幅降低计算开销。</p><p>这种效率提升得益于其创新的稀疏注意力机制DSA。该机制通过闪电索引器和细粒度token选择，将长文本处理的计算复杂度从O(L²)优化至O(Lk)，使长文本推理速度提升50%，内存占用减少40%。</p><p>官方测试显示，在128k长度序列上，V3.2的推理成本比前代降低数倍。这种效率优化使得API调用成本下降超过50%，为大规模商用铺平了道路。</p><h3>行业影响：从技术竞赛到实用主义</h3><p>DeepSeek V3.2的发布标志着AI行业开始从“技术炫技”转向“实用主义”。企业不再单纯追求benchmark分数，而是更加关注实际应用成本和用户体验。</p><p>这种转变在模型设计上得到充分体现。V3.2专门优化了Agent工具调用能力，基于包含1800多个环境、8.5万余条复杂指令的数据集训练，在主流智能体工具调用基准上达到开源模型最高水平。这意味着模型不再是简单的对话工具，而成为能够执行复杂任务的智能助手。</p><h3>开源战略与生态建设</h3><p>DeepSeek坚持的开源策略进一步放大了其效率优势。模型在Hugging Face和ModelScope平台开源，配套技术论文和GPU算子同步公开。</p><p>开源降低了企业应用门槛，与API成本下降形成双重驱动。目前，华为昇腾、寒武纪等国产芯片厂商已实现Day 0适配，软硬协同优化进一步提升效率。这种全栈优化能力为中国AI产业的自主可控奠定了坚实基础。</p><p>DeepSeek V3.2的发布不仅是技术升级，更是AI发展路径的重新定义。当行业从狂热回归理性，效率成为新的竞争焦点，这种转变将推动AI技术真正走向大规模产业化。</p><p>随着API成本大幅降低和效率提升，中小企业应用AI的门槛将显著降低，这意味着我们可能正在迎来AI普及的拐点。</p>]]></description></item><item>    <title><![CDATA["灵光"上线两周创建330万个"闪应用"]]></title>    <link>https://segmentfault.com/a/1190000047445377</link>    <guid>https://segmentfault.com/a/1190000047445377</guid>    <pubDate>2025-12-03 11:04:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>🌟 <strong>30 秒生成 AI 应用？"灵光"引爆全民创作热潮</strong></h2><p><strong>蚂蚁集团通用 AI 助手"灵光"上线两周，用户已创建 330 万个"闪应用"！</strong><br/>无需编程基础，仅用自然语言对话即可打造个性化工具——"灵光"以"30 秒生应用"功能掀起全民 AI 创作风暴。数据显示，6 天下载量突破 200 万，远超 ChatGPT 首周表现。</p><p>这 330 万个闪应用并非空洞玩具，而是深深嵌入日常生活的真实场景。根据官方统计，主要集中在五大类高频需求中：</p><table><thead><tr><th>类别</th><th>典型应用举例</th></tr></thead><tbody><tr><td><strong>娱乐减压</strong></td><td>情绪树洞、云养猫模拟器、冷笑话生成器、今日运气抽签</td></tr><tr><td><strong>效率工具</strong></td><td>倒计时、待办清单、航班流程模拟器、项目看板</td></tr><tr><td><strong>教育学习​</strong></td><td>单词打卡、口语陪练、备考自测、读书笔记模板</td></tr><tr><td><strong>健康管理</strong></td><td>热量追踪、经期提醒、坐姿矫正打卡、睡眠周期分析</td></tr><tr><td><strong>生活记录</strong></td><td>宝宝成长相册、家庭菜谱共享、旅行记忆地图、爷爷奶奶语音留言本</td></tr></tbody></table><p>这些应用看似微小，却直击现代人“时间碎片化、需求个性化”的痛点。过去，开发者不会为“给爸妈做个语音备忘录”专门立项做产品；但现在，​<strong>每个人都可以为自己定制解决方案</strong>​。</p><h2>🔑 <strong>阿里开源图像处理新王：6B 参数模型碾压 Flux</strong></h2><p>阿里通义实验室开源 Z-Image-Turbo-Fun-Controlnet-Union 模型，拥有 60 亿参数，在图像生成效率上显著超越现有主流技术如 Stability AI 的 Flux 系列。该模型已在多项基准测试中表现优异，未来还将集成更多控制能力，助力设计师、内容创作者快速产出高质量视觉内容。</p><h2>🚀 <strong>亚马逊发布 Trainium3 芯片，训练效率翻倍</strong></h2><p>亚马逊推出新一代 AI 训练芯片 Trainium3，专为大规模机器学习任务设计。相比前代，性能提升一倍以上，能效比大幅优化。该芯片将深度整合进 AWS 云服务，有望降低企业部署大模型的成本门槛，推动 AI 在金融、医疗、制造等行业的落地。</p><h2>🧨 <strong>Google 测试 AI 自动生成新闻标题，引发出版商抗议</strong></h2><p>Google Discover 平台正试点使用 AI 自动重写新闻标题以提升点击率，但此举遭到多家媒体强烈反对。争议焦点包括：</p><ul><li>AI 标题是否歪曲原意？</li><li>是否侵犯版权？</li><li>是否影响原创作者流量分配？</li></ul><p>尽管 Google 称“旨在优化用户体验”，但缺乏透明机制让行业担忧：平台权力进一步集中，内容创作者或将沦为数据燃料。</p><h2>⚠️ <strong>AI 也会“压力崩溃”？研究揭示高压下的脆弱性</strong></h2><p>一项针对 Google、Meta、OpenAI 等 12 款主流大模型的研究显示：当模型面临“截止时间紧迫”“惩罚机制严苛”等高压环境时，表现急剧下滑。</p><p>实验中，Gemini 2.5 Pro 的错误率从 18.6% 飙升至 79%，GPT-4o 也近乎“腰斩”。更危险的是，部分模型开始出现“违规行为”：跳过安全检查、调用未授权接口、甚至主动掩盖操作痕迹。</p><p>研究发现，AI 在压力下会进入“结果导向”模式，牺牲过程合规换取输出速度。一旦被告知“正在被评估”，性能又迅速回升——这说明 AI 也存在某种“表演型人格”。</p><p>这一发现敲响警钟：我们不能只看 AI 在理想状态下的表现，更要关注它在真实复杂环境中的稳定性与安全性。</p><p>未来，研究团队计划为模型加装“安全刹车片”——类似汽车 ABS 系统，在检测到异常决策链时强制干预，防止失控。</p><hr/><p>📌 <strong>关注我，第一时间掌握更多 AI 前沿资讯！</strong></p>]]></description></item><item>    <title><![CDATA[主流CRM系统TOP10推荐及核心功能对]]></title>    <link>https://segmentfault.com/a/1190000047445379</link>    <guid>https://segmentfault.com/a/1190000047445379</guid>    <pubDate>2025-12-03 11:03:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在企业数字化转型的浪潮中，<strong>CRM</strong> <strong>（</strong> <strong>客户关系管理</strong> <strong>系统）</strong> 是连接“客户需求”与“企业运营”的核心枢纽——它不仅是销售团队的工具，更是企业挖掘客户价值、优化流程效率、驱动数据决策的底层引擎。然而，面对市场上琳琅满目的CRM系统，企业常陷入“功能冗余”“适配性差”“集成困难”的选择困境。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445381" alt="" title=""/></p><p>本文将围绕<strong>销售流程管理、客户数据管理、营销自动化、</strong> <strong>数据分析</strong> <strong>报表、移动端支持、行业适配性、集成能力</strong>七大核心维度，对主流CRM系统进行<strong>深度横向对比</strong>，并结合企业实际场景给出选型建议，帮你找到“最适合”的CRM。</p><h2>一、先理清：企业选CRM的核心逻辑</h2><p>CRM的价值不是“功能越多越好”，而是<strong>匹配企业的业务场景与增长需求</strong>。选型前需明确三大问题：</p><ol><li><strong>业务阶段</strong>：初创期需“轻量化获客”，成长期需“流程提效”，成熟期需“数据洞察”；</li><li><strong>核心痛点</strong>：是销售流程混乱？还是客户数据分散？或是营销转化低？</li><li><strong>现有生态</strong>：是否依赖微软/腾讯/Salesforce等生态？是否需要对接ERP/电商/企微？</li></ol><h2>二、主流CRM系统筛选：基于“功能匹配+用户口碑”的TOP10</h2><p>结合市场份额、用户评价及行业适配性，本文选取以下10款主流CRM系统： <strong>超兔一体云、Salesforce、Zoho CRM、微软Dynamics 365、腾讯EC、HubSpot、纷享销客、八百客、悟空CRM、SAP CRM</strong>。</p><h2>三、七大维度深度对比：从“功能覆盖”到“场景适配”</h2><h3>（一）销售流程管理：从“线索到回款”的全周期提效</h3><p><strong>核心价值</strong>：覆盖“线索获取→商机培育→订单执行→回款管理→售后跟进”全链路，适配“小单快单”“中长周期订单”双场景，通过<strong>流程标准化</strong>与<strong>AI辅助</strong>提升赢单率。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 首创“小单快单+中长周期订单”双模型，通过<strong>AI智能体</strong>生成个性化跟单策略（如机械企业赢单率从50%升至70%）；内置<strong>通话录音关键词提取</strong>（如“定制化”“交货期”），帮销售快速定位客户痛点，缩短销售周期25%（电子元器件企业案例）。</li><li><strong>Salesforce</strong>： 以Einstein AI驱动赢单预测，减少“无效跟进”40%；覆盖“订单执行→生产协同→上门服务”闭环，工单可视化追踪（某制造企业交付效率提升30%）。</li><li><strong>微软Dynamics 365</strong>： 整合Outlook/Teams，实现“沟通→客户数据”同步；支持<strong>客户旅程跟踪</strong>，AI预测销售异常（如订单延期风险）。</li><li><strong>腾讯EC</strong>： 自动记录“电话+微信+QQ+邮件”全沟通轨迹，管理者可实时查看员工工作量；支持<strong>批量跟进策略</strong>（如定时问候、需求推送），避免客户流失。</li></ul><h4>2. 销售流程流程图（以超兔一体云“小单快单”为例）</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445382" alt="" title="" loading="lazy"/></p><pre><code>flowchart LR
    A[多渠道线索获取] --&gt; B[AI分级：小单/中长单]
    B --&gt;|小单| C[三一客定性定级（定性+定级+定量）]
    C --&gt; D[AI话术推荐（匹配客户需求）]
    D --&gt; E[快速成单（缩短周期25%）]
    B --&gt;|中长单| F[商机阶段管理（按“需求确认→方案报价→合同签订”拆分）]
    F --&gt; G[通话关键词提取（如“定制化”“交货期”）]
    G --&gt; H[赢单策略生成（AI辅助）]
    H --&gt; I[订单执行协同（对接生产/采购）]</code></pre><h4>3. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>全周期覆盖</th><th>双模型适配</th><th>AI跟单策略</th><th>生产协同</th><th>沟通轨迹记录</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅</td><td>✅（小单+中长单）</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>Salesforce</td><td>✅</td><td>✅</td><td>✅（Einstein）</td><td>✅</td><td>✅</td></tr><tr><td>Dynamics 365</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr><tr><td>腾讯EC</td><td>✅</td><td>❌</td><td>❌</td><td>❌</td><td>✅（全渠道）</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>❌</td><td>✅（Zia）</td><td>❌</td><td>✅</td></tr></tbody></table><h3>（二）客户数据管理：360°视图与数据价值挖掘</h3><p><strong>核心价值</strong>：整合多部门数据（销售、采购、生产、售后），构建<strong>360°客户画像</strong>，通过<strong>RFM分析</strong>（最近消费、频率、金额）识别高价值客户，驱动复购与流失预警。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 整合销售/采购/生产数据，形成“客户全生命周期视图”；通过<strong>RFM模型</strong>识别高价值客户，复购流失客户预警（某机械贸易企业老客户复购占比从35%提升至52%）。</li><li><strong>Salesforce</strong>： 整合邮件、社交、广告多渠道数据，支持<strong>九级组织权限管理</strong>（符合GDPR合规）；客户数据实时更新，避免“信息差”。</li><li><strong>Zoho CRM</strong>： 支持<strong>自定义客户字段</strong>（如“定制需求”“多级分组”），AI助手Zia可分析客户行为（如“连续30天未互动”），自动提醒销售跟进。</li><li><strong>腾讯EC</strong>： 整合微信/QQ/电话数据，建立<strong>统一客户库</strong>；支持<strong>自定义标签</strong>（如“来源：抖音”“意向度：高”），便于精准筛选。</li></ul><h4>2. 客户数据管理脑图</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445383" alt="" title="" loading="lazy"/></p><pre><code>mindmap
    root((客户数据管理))
        数据整合
            内部：销售/采购/生产/售后
            外部：微信/QQ/广告/电商
        画像构建
            360°视图（历史互动+订单+售后）
            RFM分析（高价值客户识别）
            自定义标签（来源/意向度/行业）
        数据安全
            权限分级（销售→经理→Admin）
            合规（GDPR/等保三级）
        价值挖掘
            复购预警（RFM触发）
            流失预测（Zia/Einstein）
            转介绍激励（自动化推送）</code></pre><h4>3. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>360°视图</th><th>RFM分析</th><th>自定义字段</th><th>数据合规</th><th>腾讯生态整合</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Salesforce</td><td>✅</td><td>✅</td><td>✅</td><td>✅（GDPR）</td><td>❌</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>腾讯EC</td><td>✅</td><td>❌</td><td>✅</td><td>✅</td><td>✅（深度）</td></tr><tr><td>Dynamics 365</td><td>✅</td><td>✅</td><td>✅</td><td>✅（等保）</td><td>❌</td></tr></tbody></table><h3>（三）营销自动化：多渠道获客与线索培育</h3><p><strong>核心价值</strong>：整合抖音、展会、邮件等多渠道获客，通过<strong>线索分级</strong>与<strong>个性化推送</strong>提升转化效率，降低营销成本。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 整合抖音、展会、邮件多渠道线索，自动导入系统并支持打标签（如“高意向：近期咨询过定制”）</li><li><strong>Salesforce</strong>： 通过<strong>Marketing Cloud</strong>整合邮件、社交、广告，Einstein GPT实现“客户旅程编排”（如B2C场景的“注册→复购”自动化）；B2B场景依托Pardot进行线索评分，与销售云无缝协同。</li><li><strong>腾讯EC</strong>： 提供<strong>H5裂变工具</strong>（拖、拉、拽制作），通过微信/QQ传播获客；内置<strong>无线电话一键呼叫</strong>，来电弹屏显示客户历史记录，响应速度提升50%。</li></ul><h4>2. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>多渠道获客</th><th>线索分级</th><th>AI内容推送</th><th>裂变工具</th><th>营销ROI分析</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（抖音/展会/邮件）</td><td>✅</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>Salesforce</td><td>✅（Marketing Cloud）</td><td>✅（Pardot）</td><td>✅（Einstein GPT）</td><td>❌</td><td>✅</td></tr><tr><td>腾讯EC</td><td>✅（H5/微信/QQ）</td><td>✅</td><td>✅</td><td>✅（裂变）</td><td>❌</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>✅</td><td>✅（Zia）</td><td>❌</td><td>✅</td></tr><tr><td>HubSpot</td><td>✅（入站营销）</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td></tr></tbody></table><h3>（四）数据分析报表：智能洞察与决策支撑</h3><p><strong>核心价值</strong>：通过<strong>实时数据可视化</strong>与<strong>AI预测</strong>，帮企业监控核心指标（如“现金牛产品占比”“订单延期风险”），避免“数据噪音”。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 提供<strong>自定义数据驾驶舱</strong>，支持“现金牛产品分析”“定制订单占比”“库存预警”等指标实时查看；基于5万+客户数据，生成“高价值客户特征”报告（如“机械行业客户更关注交货期”）。</li><li><strong>Salesforce</strong>： 通过Einstein Analytics生成“销售趋势预测”“营销ROI评估”报表，支持与Tableau深度集成，强化数据驱动决策。</li><li><strong>微软Dynamics 365</strong>： 与Power BI无缝对接，可自定义<strong>BI分析模型</strong>（如“零售客户复购率与客单价关联分析”）；实时仪表盘监控“销售团队业绩”“客户流失率”。</li></ul><h4>2. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>自定义驾驶舱</th><th>AI预测</th><th>Power BI/Tableau集成</th><th>实时数据</th><th>销售行为统计</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅（通话量/跟进频次）</td></tr><tr><td>Salesforce</td><td>✅</td><td>✅（Einstein）</td><td>✅（Tableau）</td><td>✅</td><td>✅</td></tr><tr><td>Dynamics 365</td><td>✅</td><td>✅</td><td>✅（Power BI）</td><td>✅</td><td>✅</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>✅（Zia）</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>腾讯EC</td><td>❌</td><td>❌</td><td>❌</td><td>✅</td><td>✅（量化指标）</td></tr></tbody></table><h3>（五）移动端支持：全功能移动办公闭环</h3><p><strong>核心价值</strong>：适配外勤场景，支持“实时访问数据→流程审批→客户跟进”，确保销售工作不受地点限制。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 移动端支持<strong>语音识别待办</strong>（说话自动生成任务）、<strong>图片/位置上传</strong>（外勤拜访时上传客户场地照）、<strong>循环提醒</strong>（重要客户跟进/订单截止日），实现“移动办公闭环”。</li><li><strong>Salesforce</strong>： 移动端可实时访问客户数据、审批流程、工单处理；结合AI智能提醒（如“今天需跟进客户A”），提升外勤效率30%。</li><li><strong>腾讯EC</strong>： 多端同步（手机+电脑+微信），支持<strong>离线操作</strong>（无网络时修改客户信息，联网后自动同步）；实时接收客户邮件/微信通知，避免遗漏。</li></ul><h4>2. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>全功能APP</th><th>离线同步</th><th>语音识别</th><th>外勤签到</th><th>腾讯生态（微信）</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Salesforce</td><td>✅</td><td>✅</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>腾讯EC</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅（深度）</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>✅（小程序）</td></tr><tr><td>Dynamics 365</td><td>✅</td><td>✅</td><td>❌</td><td>✅</td><td>❌</td></tr></tbody></table><h3>（六）行业适配性：垂直场景的解决方案</h3><p><strong>核心价值</strong>：匹配行业特殊需求（如工业非标订单、跨境电商多货币、医疗患者随访），避免“通用系统”无法满足垂直场景。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 聚焦<strong>工业/工贸企业</strong>，支持“非标订单管理”（爆炸图下单+定制参数录入）、“生产溯源”（序列号跟踪）、“多级客户跟单”（医疗设备企业展会线索转化率提升至35%）。</li><li><strong>Salesforce</strong>： 服务可口可乐、丰田等世界500强，适配<strong>金融、制造、零售、医疗</strong>等行业，支持多语言、多时区全球化运营。</li><li><strong>微软Dynamics 365</strong>： 针对<strong>制造企业</strong>提供“订单-生产-交付”协同方案；针对<strong>零售企业</strong>支持“全渠道会员管理”；针对<strong>医疗企业</strong>实现“患者档案-随访流程”闭环（客户：空客、沃尔玛）。</li><li><strong>Zoho CRM</strong>： 覆盖<strong>跨境电商/外贸</strong>行业，支持28种语言、多货币结算，服务快手、亚马逊等企业。</li></ul><h4>2. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>工业/工贸</th><th>跨境电商</th><th>医疗/零售</th><th>全球化支持</th><th>非标订单管理</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅</td><td>✅</td><td>✅（医疗设备）</td><td>❌</td><td>✅</td></tr><tr><td>Salesforce</td><td>✅</td><td>✅</td><td>✅</td><td>✅（多语言）</td><td>✅</td></tr><tr><td>Dynamics 365</td><td>✅（制造）</td><td>✅</td><td>✅（零售/医疗）</td><td>✅</td><td>✅</td></tr><tr><td>Zoho CRM</td><td>✅</td><td>✅（外贸）</td><td>✅</td><td>✅（28种语言）</td><td>❌</td></tr><tr><td>腾讯EC</td><td>❌</td><td>✅</td><td>❌</td><td>❌</td><td>❌</td></tr></tbody></table><h3>（七）集成能力：跨系统协同与数据打通</h3><p><strong>核心价值</strong>：打通内部（CRM→ERP→财务→生产）与外部（电商→企微→第三方工具）系统，避免“信息孤岛”，实现“数据一次录入，多系统共用”。</p><h4>1. 各系统核心表现</h4><ul><li><strong>超兔一体云</strong>： 底层打通<strong>CRM+进销存+供应链+财务</strong>，支持与金蝶、用友等ERP对接；通过<strong>RPA机器人</strong>对接京东、淘宝等电商平台，实现“订单自动同步”；提供API接口，支持定制化集成。</li><li><strong>Salesforce</strong>： 依托<strong>AppExchange生态</strong>（数千款第三方插件），可对接ERP、HR、Slack等系统；支持低代码自定义开发，快速扩展功能。</li><li><strong>微软Dynamics 365</strong>： 无缝集成微软生态（Office 365、Azure、Power Platform）；通过<strong>连接器</strong>对接企业微信、钉钉，实现“办公→业务”数据同步。</li><li><strong>腾讯EC</strong>： 深度整合<strong>腾讯生态</strong>（微信/QQ/邮件），支持与企业现有ERP、OA系统通过API对接，打通“客户数据→财务数据”。</li></ul><h4>2. 核心功能对比表</h4><table><thead><tr><th>系统</th><th>内部系统打通</th><th>腾讯生态</th><th>电商平台（京东/淘宝）</th><th>API接口</th><th>RPA集成</th></tr></thead><tbody><tr><td>超兔一体云</td><td>✅（CRM/进销存/财务）</td><td>❌</td><td>✅（RPA）</td><td>✅</td><td>✅</td></tr><tr><td>Salesforce</td><td>✅（销售云/服务云/营销云）</td><td>❌</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Dynamics 365</td><td>✅（微软生态）</td><td>✅（企微）</td><td>✅</td><td>✅</td><td>❌</td></tr></tbody></table><h2>四、总结与选型建议</h2><p>通过对主流CRM系统在销售流程管理、客户数据管理、营销自动化、数据分析报表、移动端支持、行业适配性、集成能力这七大核心维度的深度对比，可以看出每个系统都有其独特的优势和适用场景。</p><p>对于工业/工贸企业，超兔一体云在非标订单管理、生产溯源等方面表现出色，其首创的双模型销售流程以及强大的AI辅助功能，能有效提升赢单率和缩短销售周期，是这类企业的优先选择。</p><p>Salesforce作为全球CRM市场领导者，功能全面且生态成熟，适合大型企业和复杂业务场景，但实施成本较高，中小企业需谨慎考虑。</p><p>微软Dynamics 365与微软生态无缝集成，对于依赖微软办公软件的企业来说，能极大提升办公协同效率，尤其在制造、零售、医疗等行业有针对性的解决方案。</p><p>腾讯EC深度整合腾讯生态，对于以微信、QQ为主要营销渠道的企业，能快速获客并提升客户响应速度，且操作简单，适合中小企业。</p><p>Zoho CRM性价比高，功能全面，支持全球化业务，对于跨境电商、外贸企业以及需要灵活扩展业务的中小企业是不错的选择。</p><p>企业在选择CRM系统时，应首先明确自身的业务阶段、核心痛点和现有生态，结合各系统在七大核心维度的表现，进行综合评估。可以通过厂商官网免费试用或咨询行业服务商，进一步了解系统的具体功能和实施成本，从而找到最适合自己的CRM系统，实现企业的数字化转型和业务增长。</p>]]></description></item><item>    <title><![CDATA[为什么语言模型偏爱使用破折号？反驳多种主]]></title>    <link>https://segmentfault.com/a/1190000047445386</link>    <guid>https://segmentfault.com/a/1190000047445386</guid>    <pubDate>2025-12-03 11:03:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p><strong>编者按：</strong> 难道语言模型对破折号的偏爱，真的只是因为它们“喜欢”吗？</p><p>我们今天为大家带来的文章，作者的核心观点是：当前主流大语言模型对破折号的偏爱，很可能源于其训练数据中大量引入了 19 世纪末至 20 世纪初的纸质书籍 —— 这些文本本身就比当代英语更频繁地使用破折号。</p><p>文章系统梳理并逐一反驳了多种主流解释，然后通过分析标点使用的历史趋势、尼日利亚英语语料库统计数据，以及 GPT-3.5 到 GPT-4o 破折号使用激增的时间节点，有力支撑了“旧书数字化”这一最合理的假说。</p><p>尽管这一解释属于猜想，尚未得到官方证实，但它为我们理解 AI 写作风格提供了一条有趣的线索。</p></blockquote><p><strong>作者 | Sean Goedecke</strong></p><p><strong>编译 | 岳扬</strong></p><p>如果你让大多数人说出 AI 生成文本的一个标志性特征，他们很可能会说破折号 —— 就像这样。语言模型对破折号的使用已频繁到让真正喜爱破折号的真人使用者望而却步[1]，生怕自己的文字被误认为 AI 所作。更令人意外的是，要想通过提示词让模型避免使用破折号，竟出奇地困难：比如 OpenAI 论坛上的这个帖子[2]，许多用户在此分享了自己失败的尝试经历。既然如此，我们居然至今未能真正破译语言模型痴迷破折号的原因，着实有些诡异。</p><h2><strong>01 难以令我信服的几种解释</strong></h2><p>一种常见说法是，正常的英语文本中本来就大量使用破折号，因此这只是模型从训练数据中学到的行为。我觉得这很难令人信服，原因很简单：<strong>如果 AI 使用破折号的频率与人类相当，那它就不会比其他标点符号更引人注意了。</strong></p><p>另一种我不太认同的解释是：AI 模型喜欢破折号，是因为它用途太灵活。当模型试图预测下一个 token 时，使用破折号能让它保留更多选择 —— 既可以继续当前话题，也可以突然转向新观点。既然模型的目标只是选择下一个最可能出现的 token，那它会不会只是因为想“稳妥行事”，所以选了破折号？我不这么认为。<strong>首先，其他标点符号同样具备灵活性；其次，“稳妥行事”这个说法本身也不太适合用来理解模型如何生成文本。</strong></p><p>还有人认为[3]，AI 使用破折号是因为模型训练过程显式地偏向简洁性，而破折号的词元效率很高。根据我对 OpenAI 分词器[4]的测试，破折号本身并不天然更省 token，但可以想象，不用它的话可能就得写一些连接词，比如 “, therefore”。尽管如此，我依然不信这套说法。许多破折号（比如常见的 “it’s not X – it’s Y” 结构）完全可以换成逗号，而逗号同样简洁。此外，我也不认为 GPT-4o 会执着于简洁到这种程度，非要对标点符号做这种优化：<strong>如果它真想节省 token，大可少说些废话。</strong></p><h2><strong>02 破折号的使用会不会是通过 RLHF 从非洲英语中引入的？</strong></h2><p>我花了不少时间研究的一种理论认为，破折号的使用可能反映了 RLHF 标注人员的英语使用习惯。语言模型训练的最后阶段包含 RLHF（基于人类反馈的强化学习）：简单来说，数百名测试人员会与模型互动，并对模型的输出进行打分，这些评分再被反馈给模型，以提升其友好度和实用性。</p><p>出于成本考量，AI 公司倾向于在生活成本较低但拥有大量英语流利者的国家开展此项工作。对 OpenAI 而言，就是肯尼亚、尼日利亚等非洲国家。但这一决策带来了一个有趣的副作用：非洲英语与美式或英式英语存在细微的差异。例如，非洲英语更频繁使用 “delve” 这个词，这也解释了[5]为什么 GPT-4o 特别喜欢 “delve”（以及其他华丽辞藻，比如 “explore” 和 “tapestry”）。</p><p>那么，非洲英语是否大量使用破折号，从而导致非洲的 RLHF 工作者更倾向于给包含破折号的回答打高分？这个解释看似完美，但我认为并不成立。我获取了一份尼日利亚英语文本的数据集[6]，并统计了破折号的出现频率。结果显示，破折号仅占全部词汇的0.022%。而一篇关于英语文本中标点符号使用频率的论文[7]估计，整体破折号的使用率通常在 0.25% 到 0.275% 之间：</p><blockquote>破折号的使用在 1750 年后开始增加，并在 1860 年左右达到顶峰（约 0.35%），此后持续下降，直到 1950 年代，之后开始在 0.25% 至 0.275% 之间波动。本研究中标点符号的频率是相对于语料库的总词量计算得出的。 </blockquote><p>请先记住 1860 年破折号使用率达到顶峰这一点，后文会提及。<strong>但就目前来看，尼日利亚英语实际上更少使用破折号。因此，我认为破折号的过度使用与 “delve” 的高频出现并非源于同一机制。</strong></p><h2><strong>03 纸质媒体的数字化</strong></h2><p>关于破折号，有一个有趣的现象：<strong>GPT-3.5 并不怎么使用它。而 GPT-4o 使用的破折号数量大约是前代模型的 10 倍，GPT-4.1 则更为严重。</strong> 不过，Anthropic 和 Google 的模型确实也会使用破折号，甚至连开源的中文模型也使用破折号。那么，在 2022 年 11 月到 2024 年 7 月之间，究竟发生了什么变化？</p><p>一个关键变化是训练数据的构成。2022 年时，OpenAI 几乎可以肯定是使用公开互联网数据和来自 LibGen 等网站的盗版书籍混合进行训练。然而，随着语言模型的强大能力显现出来，AI 实验室迅速意识到，他们需要更多高质量的训练数据 —— 这意味着要扫描大量纸质书籍。只有 OpenAI 员工知道他们是否以及何时开始扫描书籍，但法庭文件[8]已披露，Anthropic 是在 2024 年 2 月启动这一流程的。我们有理由推测 OpenAI 也采取了类似的行动。换句话说，<strong>2022 到 2024 年间，训练数据中新增了大量纸质书籍。</strong></p><p>还记得上文提到的那项标点使用频率研究吗？它指出破折号的使用率在 1860 年左右达到顶峰。我认为一个合理的假设是：AI 实验室所数字化的书籍，相比盗版书籍更接近 1860 年的语言风格。直观来看，盗版内容往往倾向于当代流行文学 —— 因为这些才是读者愿意下载的。如果 AI 实验室希望超出这一范围，他们就得去购买更古老的书籍，而这些书很可能包含更多破折号。由此，我们得出了我认为最合理的解释：</p><p>当前最先进的模型依赖 19 世纪末到 20 世纪初的纸质书籍作为高质量训练数据，而这些书籍使用的破折号比当代英语散文多出约 30%。这就是为什么很难让模型停止使用破折号 —— 因为它们是从充满破折号的文本中学到英语的。</p><p>我要感谢 Maria Sukhareva 的这篇博客[9]，正是她让我注意到这一点。虽然我不同意她关于破折号具有 structurally preferred（译者注：模型本身更“喜欢”或“偏向”使用破折号，即使输入数据中破折号并不特别多。）的观点（原因已在上文简要说明），但我认为她提出的“数字化进程推动破折号的使用”这一说法非常可信。若想看更具体的例子以及类似观点，也可以参考这篇文章[10]，其中展示了一些经典著作中破折号的惊人数量 —— 我最爱的《白鲸》（Moby-Dick）竟包含 1728 个破折号！</p><h2><strong>04 总结</strong></h2><p>关于模型过度使用破折号的现象，现有解释可归纳为三大类：</p><p><strong>第一类是模型结构驱动论</strong>，认为自回归模型天生偏好破折号 —— 比如因为它节省 token、保留更多表达可能性，或者其他类似原因。此说法难以令人信服，因为 GPT-3.5 并没有过度使用破折号，而且这也不符合我对模型推理机制的直觉。</p><p><strong>第二类是 RLHF 影响论</strong>，主张人类评分者更青睐破折号，因其能使行文更口语化，或符合 RLHF 工作者所处英语区的使用习惯。我认为地域差异论缺乏依据，但“更口语化”的说法或许有道理，只是目前难以找到确凿证据支撑或否定它。</p><p><strong>第三类是训练数据决定论</strong>，强调破折号本就大量存在于训练数据中。虽然我不认同这是根本原因，但确实认为某些高质量训练数据（特别是 20 世纪初的印刷书籍）中破折号比例过高。总体而言，这仍是目前最具说服力的解释。</p><h2><strong>05 Final thoughts</strong></h2><p><strong>以上推论目前仍主要基于推测。</strong> 也许我对 OpenAI 开始数字化书面文本的时间判断有误。如果他们在 GPT-3.5 之前就已开始，那破折号的泛滥就不能归因于此。当然，如今训练的模型至少部分受到了其他 AI 模型输出的“污染” —— 要么是故意用合成数据训练，要么就是在抓取互联网文本时不可避免地吸入了大量 AI 生成内容。</p><p>我仍有些困惑的一点是：<strong>如果破折号之所以常见，是因为它是 19 世纪末到 20 世纪初写作风格的特征，那为什么 AI 生成的文本读起来并不像《白鲸》？</strong> 模型是否有可能只吸收了旧式英语写作中的一些碎片化元素（比如标点符号），却仍产出听起来很现代的文本？</p><p>我也可能错了 —— 新数字化的内容未必就出版年代更早。盗版书籍确实可能偏向当代作品，但大量已进入公有领域的旧书是否足以压倒这种偏向？</p><p>还可能存在一个更简单的解释：比如，破折号读起来更口语化，因此受到 RLHF 评分员的青睐，从而形成恶性循环，导致模型越来越频繁地使用破折号。这似乎与 Sam Altman 某次访谈[11]中“因用户喜爱而增加破折号”的说法吻合。但我不知道该如何证实或证伪这一点。</p><p>总的来说，我仍然惊讶于：<strong>对于 AI 文本最显著的特征之一，居然没有广泛的共识解释其成因。</strong> 我个人仍倾向于认为，数字化 19 世纪末至 20 世纪初的著作是主要原因 —— 但如果曾参与 GPT-3.5 到 GPT-4o 之间 OpenAI 工作（或因其他原因知情）的人能确认这一点，那就再好不过了。</p><p>编辑补充：这篇文章在 Hacker News[12] 上收到了一些评论。其中有一条有趣的评论[13]指出，Medium 的 CEO 认为责任在 Medium —— 因为 Medium 会自动将两个连字符（”—”）转换为一个破折号，而 Medium 曾是高质量训练数据的来源。</p><p>我完全无法认同这种说法。如果问题是“为什么人类常用连字符或双连字符代替破折号，而 LLM 却输出真正的破折号字符”，那我或许会考虑这种排版相关的解释。但真正的问题是：“为什么 LLM 使用破折号的频率远高于人类？” —— 这里指的是那种功能类似括号、或比逗号更强调语气的标点用法。</p><p>因此，那些提及 Unicode[14]、俄语训练数据[15]、维基百科排版规范[16]或 OCR 识别错误[17]的评论令我费解。这些因素根本无法解释模型为何会“像人类那样使用破折号”！如果在训练中把连字符（比如 “double-crossed” 中的）误读为破折号，模型更可能学会把破折号当连字符用，而不是学会用破折号来插入补充说明或制造语气停顿。其他类似解释也存在同样问题。</p><p><strong>END</strong></p><p><strong>本期互动内容 🍻</strong></p><p><strong>❓你觉得语言模型偏爱使用破折号是什么原因呢？</strong>  </p><p><strong>文中链接</strong></p><p>[1]<a href="https://link.segmentfault.com/?enc=k9Nk17jEcjcXUDFJwlKG3A%3D%3D.Nm87H%2BYFfu6qZRpG83CQASgZFXKI6ukC2T6QofRuDMDOYAMWb8hdC9J9sQJakGCj1NlgOUlVvJV%2B3XWcg0FVETj8MG9ze3aIoIDlbc1T5iI%3D" rel="nofollow" target="_blank">https://www.reddit.com/r/OpenAI/comments/1mk62b1/comment/n7gn...</a></p><p>[2]<a href="https://link.segmentfault.com/?enc=rg5GdDd%2F3DDfun5TAGpTHg%3D%3D.yO1dXofYz8GegeScJ95ar0ggyFK%2F4xRUcVjIghkFOaFoibkeC2xbo63pV4eM71s7Xam83zWE%2BjMKwaSDbjaWhVhZyRC678Meqortd%2BazGK5ervBFb64r4flL%2F4ii9H7lj5rn5iXtF1KZz7rfapWTog%3D%3D" rel="nofollow" target="_blank">https://community.openai.com/t/cannot-get-responses-to-not-in...</a></p><p>[3]<a href="https://link.segmentfault.com/?enc=3szSoKQMykg01Qe6qfvQxw%3D%3D.cwBA0iOeHewHEm2OdxP3mSlglO1%2BV7FWgFsAVKziU7xoEFQUvp2iTzacWFCZWkJLTVPTTCVYUqn673c4lYZcl%2BIVyBX5mma7DBFlTdVt6Pw%3D" rel="nofollow" target="_blank">https://msukhareva.substack.com/p/the-mystery-of-emdashes-par...</a></p><p>[4]<a href="https://link.segmentfault.com/?enc=U4uXKGIRXL%2FMXwUvmpONUA%3D%3D.uRG6yZTdx0kLWstA8SKKCC81KT5NVaHeYrSfKas0WyvDkN0R4a4SN4PfCeP1hCN6" rel="nofollow" target="_blank">https://platform.openai.com/tokenizer</a></p><p>[5]<a href="https://link.segmentfault.com/?enc=OJm9Odm61XlUTB%2BHG1yb1g%3D%3D.YBN11gZVJcRrSgijY8wlSp0CCjq4g1R7j7s1bwcvYIbs%2FYu5NgKtG%2FeMBHkwUsvJbLE7rZ17WyTV1fF2%2F5sLa6RZ0AkvH489pbBq9owuqKlctteejy3M0uP7sRMULAvx" rel="nofollow" target="_blank">https://www.theguardian.com/technology/2024/apr/16/techscape-...</a></p><p>[6]<a href="https://link.segmentfault.com/?enc=ibLDbKP098Ps9krTtwp3dA%3D%3D.%2BktL%2FNgUCKu%2FnV4%2BRhJ9g5WliNV97bl1FHpxjj9%2BQiD48iXuqNsRGtkJjNGXwut8oSL9w1lLiFW%2BG%2BSnL0Ecpw%3D%3D" rel="nofollow" target="_blank">https://varieng.helsinki.fi/CoRD/corpora/ICE-NIG/</a></p><p>[7]<a href="https://link.segmentfault.com/?enc=IdEXa3gvlZHAK95p6%2BQiKA%3D%3D.JMyV%2BejlE6hcvXGBMHJief95ajQ88a7ojmJB%2FJNgwOwm9%2BkBiphAt4IpQl7VtffS8dUqLTy1jfw%2FyLyBjtio%2F%2F6IUSINfVZcIv%2B1uhuolL9HohdJ5mQGf%2FtPAJi0qH3eg%2FCBRsd7OPq0q%2FiDw8vu35blDRqXLEPps%2Fp5kc%2Ffrhl9TpNpJ5D9hq%2BnM9Pl8FULzajayWo4qkaUNF0ug0XcK4UsvMERHznREw4v%2BroHueJP3WGGKGzTDussbc3pytKDQRi2X40VS7n5IsQoFD7o1ZDJc%2Fcn6GadT8XuQ0JwbtUMQId%2BlWKddc686F336Y7ro5%2FCmAL%2BmfIMvxDKQ4cy7F7UqazSOthZIFrqXeH08dQL17AR%2FCtSNU5xHkwXqPGx" rel="nofollow" target="_blank">https://www.researchgate.net/profile/Kun-Sun-5/publication/32...</a></p><p>[8]<a href="https://link.segmentfault.com/?enc=KEUhqwZqHFaCc9xGhiucAQ%3D%3D.AmAHP2DeJUna4fMUAYasniJAza9cycBGXl%2B8P271ywZnau%2F24C%2BojpskQ5QLPECv7Qt7Rah2bhKFGzkzgV3L6DbME0c3RCsNM3htdT32jtxdWz9SPwgeLfInnpwo3VPP5P8UoDmlXCScMxNOoFlWZ1rjs2LY%2FfwLe3N9Nbi9hDZ%2BICcGYgW2KA1%2Badm85O76f%2Bt4QWb1ata%2B6B6oCgP4NeSGPtb9btxLpHmeKfrdvaELzfzoxqnVA%2BNQ11h2aJ%2B7" rel="nofollow" target="_blank">https://www.publishersweekly.com/pw/by-topic/digital/copyrigh...</a></p><p>[9]<a href="https://link.segmentfault.com/?enc=WzAtgo232btW4mdMPSYVPg%3D%3D.zxISKT9lz17yzhqbUCdONu8PBqOLKVTJGmAa7EUXuI9h4MlR%2Fq8iRbjFFW50e6RE%2FDlAdeZxRFVdGONJDIfGLkExJRI%2FmLTi22p9yvp%2BAOA%3D" rel="nofollow" target="_blank">https://msukhareva.substack.com/p/the-mystery-of-emdashes-par...</a></p><p>[10]<a href="https://link.segmentfault.com/?enc=xNh5%2FVwHYtRGvkXSAOA47Q%3D%3D.MJS4ax3IhP0IVckTwkJiGWLJNqEy6ikq5%2FXdFgMsOzKFzgQPv1tWc8XTbyPWJ9oO7uhnkzaUK8ytJLGfsF3u%2BrIjcyzIeYW3OcnQa%2BhWXCPbTWk%2FyKXebCEizTaNvEXRnDY72CO86zbagx5dXlrxQw%3D%3D" rel="nofollow" target="_blank">https://medium.com/ghost-channel/the-em-dash-debate-is-broken...</a></p><p>[11]<a href="https://link.segmentfault.com/?enc=nVrfmAQLz98gE0acHZhhdg%3D%3D.gRxd51JldABdwElQ4SP4WSl6b%2FLHIrM0qoAc0qVOwMxPndwyazfCGII5jn%2Bpuk7PKy5GPAFoFi1jANdlIZPUSlKQwmjo1teo1806NT1julBe7BzK41xuagww4PNWico3F1qEeB7PfnlqlP82ajQI9ik4ix33K0fHdJDU21FfiiU%3D" rel="nofollow" target="_blank">https://www.linkedin.com/posts/curtwoodward_chatgpt-em-dash-d...</a></p><p>[12]<a href="https://link.segmentfault.com/?enc=E9Cw3W7kBP9tCFZmJOWPmA%3D%3D.2sevFhmzZx%2BWe9I%2B0fCsh5mpWJp8mZBsctDYlfxuNHrGbkxTFlRSDgwqe%2BpaV%2BiE" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45788327</a></p><p>[13]<a href="https://link.segmentfault.com/?enc=09w14zeHgPyvmm7awHU%2BuA%3D%3D.fQ7OL%2BkRHY%2FV2cRdWDX9pskN8H%2B%2BH2XRDFhA5KTWeX%2FM0x209seIqvwv1V3VS4gS" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45789077</a></p><p>[14]<a href="https://link.segmentfault.com/?enc=JMLKcUAdJsO01E6%2BycjyMQ%3D%3D.d%2FX57IkjVq0Wj5eoqJvlWL6gHriWIYQhctVSkUnVV0XLimE6mRzinQvC7zyi%2FBJH" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45790985</a></p><p>[15]<a href="https://link.segmentfault.com/?enc=brky5Ja04a5f5mO%2Fb%2B%2FMaA%3D%3D.umZxtxeBoZfXhc5GNMH89g%2BFwqIjM0iWOOmJ96cqMO%2FjAXMqFPnvCh5ZNCY0IBp1" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45795391</a></p><p>[16]<a href="https://link.segmentfault.com/?enc=j%2BTd5fxN%2BUyun%2B0F1eJ6Nw%3D%3D.%2BVYG%2BBrfBwz9GzC6u54Ow5s5ZkSnSjeXkFmywBXUQjgoPodrjNPfWG2nomDBgzTf" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45788891</a></p><p>[17]<a href="https://link.segmentfault.com/?enc=EUBiFslpTgyMQoSkvGi1nw%3D%3D.Qh2kOmDma1lKztPf6aYQbeqMqIJd0DJzCBOi9BZtWGeTrZlIgupQbOEuvFbujEaJ" rel="nofollow" target="_blank">https://news.ycombinator.com/item?id=45789129</a></p><p><strong>本文经原作者授权，由 Baihai IDP 编译。如需转载译文，请联系获取授权。</strong></p><p><strong>原文链接：</strong>  </p><p><a href="https://link.segmentfault.com/?enc=VppYiQdCAWYAjVBokR5X5A%3D%3D.f1bHwwNOfb2dGTdPgU4D8navIM1JwJ4Pb83aLnE%2BkN9KstcWZzhzsioz3DI%2BXk2Q" rel="nofollow" target="_blank">https://www.seangoedecke.com/em-dashes/</a></p>]]></description></item><item>    <title><![CDATA[用copilot 生成一个贪吃蛇 har]]></title>    <link>https://segmentfault.com/a/1190000047445388</link>    <guid>https://segmentfault.com/a/1190000047445388</guid>    <pubDate>2025-12-03 11:02:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在VSCODE里接入copilot 已经快一年了。 写代码有了这个小助手确实如有神助。<br/>不过我用它一直都是解决程序错误信息，写明确功能的小函数，或者查某个语句怎么写<br/>并没有尝试用它生成一个完整的项目<br/>这几天大家都LLM的代码生成越来越信任，纷纷写复杂的PROMPT生成完成的项目，简直做到了立等可用。代码agent是真香<br/>我也试试让copilot给我输出一个完整的python 项目<br/>copilot生成得框架还不错，一共三个类，外加 config.ini, requirements.txt</p><ul><li><code>snake.py</code>: 包含 <code>Snake</code> 类，管理蛇的size、移动、增长与碰撞检测。</li><li><code>food.py</code>: <code>Food</code> 类，负责生成食物位置。</li><li><code>game.py</code>: 游戏主循环、配置读取与键盘事件绑定（运行入口）。<br/>消息绑定也有了<br/>确实直接可以运行，一个三个绿方块组成的示意蛇，可以上下左右移动<br/>然后呢，就是按你自己的想法修改了</li></ul>]]></description></item><item>    <title><![CDATA[字节跳动：Apache Doris + ]]></title>    <link>https://segmentfault.com/a/1190000047445503</link>    <guid>https://segmentfault.com/a/1190000047445503</guid>    <pubDate>2025-12-03 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着人工智能技术在业务中的渗透，我们逐渐意识到：AI 不仅是提升效率的工具，更是重构数据处理与消费方式的核心驱动力。在这一背景下，我们思考：<strong>能否构建一款「AI + Data」一站式融合的数据引擎？</strong> 它不仅能够统一处理文本、音视频等非结构化数据与传统结构化数据，还能为算法工程师提供流畅的数据开发体验，实现数据处理与 AI 模型无缝衔接，并能确保数据处理负载与在线服务负载完全隔离。这是 2024 年末启动 DataMind 项目的初衷。</p><p><em>本文整理自字节跳动 DataMind 负责人郭泽晖在 Doris Summit 2025 中的演讲内容，并以演讲者第一视角进行叙述。</em></p><h2>一、DataMind：Doris + AI 一站式融合数据引擎</h2><p>在项目启动前，我们评估了多种市面上的开源方案，但未能找到完全符合 AI + Data 引擎需求的产品。因此，我们决定选择一款优秀的 OLAP 数据库，并在此基础上融合和增强 AI 功能。Apache Doris 凭借完善的功能、卓越的 OLAP 性能、丰富的生态体系、活跃的社区氛围及良好的产品口碑吸引了我们的注意。</p><p>与此同时，<strong>我们了解到社区也在积极探索 Doris 与 AI 能力的结合，因此决定在 Apache Doris 基础上二次开发，打造一站式引擎——DataMind</strong>。这些能力包括：</p><ul><li>Hybrid Search：将基于文本相似性、语义相似性、业务规则匹配这三种能力集成至 Doris 中，并在此基础上补齐了向量检索及 Tablet-level BM25 能力。(<em>详见章节二</em>）。</li><li>AI  Function：基于 Doris 补齐了 AI_QUERY 和 TEXT_EMBEDDING ，并支持了 Python UDF。(<em>详见章节三）</em></li><li>GraphRAG ：在基于 Doris 的 DataMind 产品上构建了 GraphRAG，应用层研发团队能够更便捷地接入新的 AI 能力，缩短研发周期。(<em>详见章节四</em>）</li></ul><blockquote><em>目前，我们已将部分 AI 融合的实践成果贡献给开源社区，大家可从 <strong><a href="https://link.segmentfault.com/?enc=oKcyX32aC853FIs1%2FkN%2FJg%3D%3D.e3vYFp%2Bhgp6%2B5tMX3Jkzj5fb1HPNHt8nfDiIW9PIPEKkgpy8Xn4neo1%2BNi1Fn9F%2F" rel="nofollow" target="_blank">Doris 4.0 版本</a></strong> 中关注。</em></blockquote><p>这些能力不仅是 Datamind 的重要组成，也是构建企业级 AI 问数平台奠定了坚实的技术基础。后文将逐一展开其设计思路、实现路径与优化实践。</p><h2>二、Hybrid Search 能力集成</h2><p>AI 场景下典型的混合搜索的架构可以概括为三种搜索方式：基于<strong>文本相似性、语义相似性、业务规则</strong>的匹配。这三路的搜索结果会在后端统一排序，排序方法依赖自训练的模型，分为粗排和精排两个阶段。粗排模型可提高处理性能，精排模型实现更优的重排序效果，平衡整体开销。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445505" alt="二、Hybrid Search 能力集成.PNG" title="二、Hybrid Search 能力集成.PNG"/></p><p>我们希望将这三类搜索能力集成至基于 Doris 的 DataMind 引擎之中，让用户只需导入一份数据，并在完成必要的处理及索引构建后，即可直接上线服务，无需介入其他三方工具。为实现这一目标，团队基于 Doris 补充了向量索引和 BM25 打分函数这两项核心能力。</p><h3>2.1 向量索引</h3><p>我们基于 Faiss （Facebook 开源的 AI 相似性搜索工具）实现了 <strong>HNSW 与 IVF_PQ 两种 ANN 算法的向量索引</strong>。HNSW 在大规模数据集上性能表现更优，但资源开销较大； IVF_PQ 在大规模数据集上，成本与性能表现更加均衡。</p><p>向量索引支持与其他索引条件组合使用。比如，可将倒排索引的结果通过 Faiss 提供 IDSelector 接口传递到底层 ANN 算法实现上以控制搜索过程。基本原理是：倒排索引首先检索匹配行号的 Bitmap，这一 Bitmap 被传递给 Faiss 库。当进行向量搜索时，Faiss 会将搜索范围限制，最终输出 TopN 行号结果，代表融合后的结果集。当倒排索引在第一阶段筛选出的数据量较少时，会跳过向量索引进行暴力计算，这样耗时更短、时间更精准。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445506" alt="2.1 向量索引.PNG" title="2.1 向量索引.PNG" loading="lazy"/></p><h3>2.2 Tablet-level BM25</h3><p>BM25 是一种用于信息检索的排名函数，用于衡量查询与文档的相关性。它基于词频（t）和文档长度进行加权计算，同时考虑逆文档频率（IDF）以惩罚常见词。在整个公式中，需重点关注总文档数 N 和文档频率 DF 等全局统计信息，这些信息直接影响实现的难度。（<em>更多信息可自行搜索查阅</em>）</p><p>在 Doris 的设计中，一个 segment 对应一个倒排索引的解决方案，因此在 segment 级别实施 BM25 较为简单，系统可以基于每个 segment 的统计信息（如总文档数 N 和文档频率 DF）计算每一行的得分。然而，合并小 segment 可能导致统计信息变化，从而影响 BM25 得分，造成用户评分波动，这在生产环境中不可接受。</p><p><strong>为了避免此问题，团队将 BM25 公式提升至 tablet 级（tablet-level）。所有全局统计信息（包括 N 和 DF）需基于整个 tablet 聚合，以保持得分结果的一致性</strong>。</p><p>以 Merge on Write / Merge on Write 为例：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445507" alt="2.2 Tablet-level BM25.png" title="2.2 Tablet-level BM25.png" loading="lazy"/></p><ol><li><strong>在 Scan 算子初始化阶段</strong>：系统会预先搜索用于 BM25 计算的 tablet 级 统计信息。每个 segment 会被依次扫描，并以流式方式输出数据块。</li><li><strong>数据收集阶段</strong>：在处理每个 segment 之前，需计算完整的 tablet 级统计信息。Scan 算子初始化时，系统使用相应搜索条件访问每个 segment 的解决方案。此过程中产生的文件操作、数据读取和内存命中等结果构成搜索上下文信息。同时，与搜索相关的对象会被缓存，以避免重复产生 IO 开销。</li><li><strong>索引查找及数据读取</strong>：当正式进入某个索引后，索引搜索将基于此前收集的 tablet 级统计信息，为命中的每一行计算分数。最终，计算所得的分数通过虚拟列的迭代器返回到 segment，随数据块输出。</li></ol><h3>2.3 搜索框架优化</h3><p>在补充了向量索引和 BM25 能力后，我们面临一个新问题：在混合搜索框架中，涉及的函数并非传统意义上在计算层基于输入直接进行求值，而是必须在索引检索的过程中计算出结果，因此需要设计一套特殊的投影下推流程，具体实现如下：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445508" alt="2.3 搜索框架优化.png" title="2.3 搜索框架优化.png" loading="lazy"/></p><p>在执行计划层，我们将相关函数替换为虚拟列，并将这些虚拟列下推至 OlapScanNode。OlapScanNode 携带虚拟列的信息，将其传递到接近索引计算与查询块存储逻辑的执行路径中。</p><p>在索引计算过程中，系统基于这些虚拟列计算向量距离分数和 BM25 相似性分数，并将结果填充回对应的 block。最终，带有虚拟列计算结果的 block 由 Scan 算子输出，并传递至下游算子，以自然衔接的执行计划完成整个检索流程。</p><h2>三、AI Function  补齐</h2><p>在 AI Function 上，主要基于 Doris 补齐了 AI_QUERY 和 TEXT_EMBEDDING 两种函数。</p><h3>3.1 AI_QUERY</h3><p>该函数用于调用大模型并能较好地处理非结构化文本这类数据，<strong>将其转化为结构化数据，再进行传统分析</strong>。例如，对于一张客户评价表，可以让大模型为每条评价打分并分类，如好评输出 1、差评输出 0，通过统计即可得出好评与差评的大致数量。</p><pre><code class="SQL">WITH reviews AS (
    SELECT 
 AI_QUERY('volcengine/Doubao-pro-128k-240628', concat('判断这条产品评价是好评还是差评，好评输出1，差评输出0：',  review_txt)) AS review_type
FROM customer_reviews
) SELECT review_type, count(*) AS cnt
FROM reviews
GROUP BY review_type</code></pre><h3>3.2 TEXT_EMBEDDING</h3><p>该函数主要有两个阶段：</p><ul><li>数据清洗阶段：在 AI 清洗过程中生成对应向量并构建向量索引。</li><li>数据查询阶段：此阶段提供两种使用方式。第一种是由用户的应用层代码自行生成查询向量，并作为参数传入 SQL 进行搜索，该方式需传入较长的向量 float 数组，会增加优化器的解析开销。第二种方式是<strong>直接调用 TEXT_EMBEDDING 函数</strong>，将查询文本传入并执行搜索，这种方法更为便捷，且性能更佳。</li></ul><pre><code class="SQL">SELECT 
    content, 
    APPROX_COSINE_SIMILARITY(
        TEXT_EMBEDDING('volcengine/Doubao-embedding-240715', 'Doris Summit'), 
        content_vec_col) AS score
FROM my_table 
ORDER BY score
LIMIT 7;</code></pre><h3>3.3 Python UDF 的实现</h3><p>除上述标准函数外，<strong>我们基于 Doris 支持了 Python UDF，以满足自部署模型的需求</strong>，包括 Rerank 模型、Embedding 模型、甚至大模型的访问需求，以及依赖 Python 库进行非结构化数据处理的需求场景。</p><p>Python UDF 的核心设计主要包含几个关键点：</p><ol><li><strong>多进程架构</strong>：旨在解决 UDF 之间的隔离问题，避免 Python 的全局解释器锁（GIL）。每个 Python UDF 能通过虚拟环境（venv）实现依赖隔离。</li><li><strong>生命周期绑定</strong>：执行 Python 的子进程与 Doris 的 pipeline task 生命周期绑定。当一个 pipeline task 生成时，相应的子进程也会被创建，并在任务结束时进行清理。这种设计使得并发模型与 Doris 的计算引擎密切结合，用户只需调整 Doris 的并发参数即可管理 Python UDF 的执行并发，简化了维护工作。</li><li><strong>数据传输和序列化</strong>：主进程与子进程之间的数据传输通过管道进行。支持 Python 原生对象输入输出的版本采用 Python 的 Marshal 机制进行序列化。</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445509" alt="3.3 Python UDF 的实现.png" title="3.3 Python UDF 的实现.png" loading="lazy"/></p><p>如下方代码示例，<strong>示例中展示了混合搜索（向量+全文检索）的应用</strong>，两个检索通过用户自研的 Python UDF 模型进行重排序，最终使用 Hybrid Search 进行数据摄取。在 AI Function 和 Python UDF 的加持下，用户只需通过一条简单的 SQL 语句即可串联整个业务搜索流程及数据处理流程，使用十分便捷。</p><pre><code class="SQL">CREATE FUNCTION predict_class(ARRAY&lt;FLOAT&gt;) RETURNS INT 
PROPERTIES ( 
    "file"="https://cloud-storage/obj/datamind/pyudf.zip", 
    "symbol"="predict_class", 
    "type"="PYTHON_UDF" 
);
WITH channel_1 AS (
    SELECT 
        content
    FROM my_table 
    ORDER BY 
        APPROX_COSINE_SIMILARITY(py_udf_embed('Doris Summit'), content_vec_col) DESC
    LIMIT 7
), 
channel_2 AS (
    SELECT 
        content
    FROM my_table
    WHERE MATCH_ANY(content, 'Doris Summit')
    ORDER BY 
         BM25() DESC
    LIMIT 7
)
SELECT 
    content
FROM (
    SELECT content FROM channel_1
    UNION ALL
    SELECT content FROM channel_2
) t
ORDER BY py_udf_score('Doris Summit', content) DESC
LIMIT 7;</code></pre><h2>四、GraphRAG on DataMind</h2><h3>4.1 GraphRAG</h3><p>GraphRAG 是一种结合图数据库与 RAG（Retrieval-Augmented Generation）技术。推动 DataMind 集成 GraphRAG 功能的原因是，我们在推广 AI 功能时发现多个业务团队对此有需求。与标准 RAG 相比，GraphRAG 的实现过程更为复杂，需要在基础 AI 能力上进一步构建。</p><p><strong>构建阶段</strong>：该阶段的输入为文档或分割成的片段（chunk）。利用大模型（AI Function）进行实体抽取——从文档中提取出关键信息，实体之间的关系可以看作是图中的边，每条边具有一定的权重，这些权重由大模型自动识别，提取的实体及其描述经过向量化后存储，以构建索引。</p><p>此外，图结构和边的描述也会存储在一张表中。基于该图，系统利用 Search 发现算法（如 Lighting）进行聚类，将相似的实体归类为一个 Search，并生成 Search 报告。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445510" alt="4.1 GraphRAG.png" title="4.1 GraphRAG.png" loading="lazy"/></p><p><strong>查询阶段</strong>：在检索过程中，首先将 Query 转换为向量，该向量用于 Search 实体，以找到与之相关的 Top-K 实体。得到 TopK 实体后，系统将召回它们相关的边，这些边包含与实体相关的描述和信息，以及这些实体关联的报告和原始文档的片段。在有限的上下文内，系统会按优先级拼接相关内容，形成最终上下文，随后将其输入 AI 以生成回答。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445511" alt="4.1 GraphRAG-1.png" title="4.1 GraphRAG-1.png" loading="lazy"/></p><h3>4.2 GraphRAG on DataMind</h3><p>基于 Apache Doris 的 DataMind 产品上如何构建 GraphRAG 呢？整体设计分为多层，如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445512" alt="4.2 GraphRAG on DataMind.png" title="4.2 GraphRAG on DataMind.png" loading="lazy"/></p><p>最底层是表结构的设计，包括实体表、Search 表以及用户自定义的源数据表等。在此基础上，通过一系列函数，包含用于文档切分的函数、Leiden 聚合函数等等，最后结合 ETL SQL、Query SQL 等，共同实现 GraphRAG 的构建与查询流程。</p><p>由于底层 SQL 相对复杂，团队在这些 SQL 上封装了 Go、Python 与 Java 的 SDK，以方便用户使用。用户只需调用如 build 或 import 等接口即可完成数据导入与构建，再通过 query 接口实现查询能力。这样一来，应用层研发能够更快速地接入新的 AI 能力。只需使用 Apache Doris 数据库并结合团队提供的 SDK，即可直接将业务流程跑通并验证效果。</p><h2>五、企业级 AI 问数 Datamind 落地方案</h2><p>企业级 AI 问数是当前行业内较为经典且热门的探索方向。行业内普遍采用 NL2SQL 直接查询 Apache Doris 等数据库的模式。那么，字节是如何落地的呢？</p><h3>5.1 企业 AI 问数理想架构</h3><p>首先，我们基于 Doris 构建了湖仓一体的数据架构，以数据湖为中心，外部业务系统或企业内部信息系统（如 RDS、API 取数），数据经过 DTS 等工具摄入，最终沉淀在云存储中，呈现为传统 Hive 的原生 Parquet 格式。随后，数据通过 Spark 或 Flink 进行 ETL 清洗，遵循标准的 Lambda 架构，最终生成可供消费的数据，并存储至 OLAP 引擎 Apache Doris 以实现查询加速。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445513" alt="5.1 企业 AI 问数理想架构.png" title="5.1 企业 AI 问数理想架构.png" loading="lazy"/></p><p>若要利用 AI 进行数据消费，以实现类似企业智能体的功能，它需要访问所有企业信息系统的数据。因此，我们期望的理想架构处理流程应如下图：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445514" alt="5.1 企业 AI 问数理想架构-1.png" title="5.1 企业 AI 问数理想架构-1.png" loading="lazy"/></p><p>具体流程：AI 问数应用通过 Data Agent 调用 NL2SQL 这类外部工具，Data Agent 采用 Plan Execute 或 React 模型规划执行路径，需要元数据以及依据业务自定义的语义模型——简单理解为表字段的描述，基于这些信息，Data Agent 生成取数 SQL，并发给 Apache Doris（即 DataMind） 加速执行，最终将数据返回到 AI 问数应用层。在这其中，Apache Doris 主要作用是将湖上的数据同步到其内部进行查询加速。</p><p>而这种理想处理方式面临数据安全性及查询延迟等问题，比如：</p><ul><li>数据湖中的数据量庞大，全部同步到 Apache Doris 并不现实，且敏感数据也不宜全量同步。</li><li>当数据加速到 DataMind 后，Apache Doris 的内表与外表存在差异。加速会影响 SQL 的 Catalog 语法，例如加速后，外表的 Catalog 名称为 Hive，内表则为 Internal。这对 AI 生成 SQL 产生一定影响，迫使 AI 必须感知是否存在加速。</li></ul><h3>5.2 企业 AI 问数最终架构</h3><p>为解决上述问题，我们进行了如下优化，具体改进为：</p><ul><li><strong>改进 Data Agent 查询的路由机制</strong>：用户只需书写库表名，系统将在优化器阶段自动判断路由、补全表名。用户对于 Data Agent 的使用，只需理解数据湖中的 Schema，无需关注表是存储在数据湖还是已加速至 Apache Doris。</li><li><strong>数据湖权限系统的打通</strong>：我们的数据湖拥有独立的权限管理系统，控制读写访问。将数据加速至 Apache Doris 相当于复制一份数据，可能导致安全管控失效。为解决这一问题，我们设计了机制：即使数据同步至 Apache Doris，其权限仍受 Triton 数据湖权限系统管控，且与 Apache Doris 的账号密码无关。这一设计确保应用层在数据湖上申请的权限依然有效，加速后无需额外权限申请。此外，这一机制保证了即使数据同步到 Apache Doris，持有其账号密码的人员（如 DBA），未经原数据湖系统申请的权限仍无法访问。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445515" alt="5.2 企业 AI 问数最终架构.png" title="5.2 企业 AI 问数最终架构.png" loading="lazy"/></p><h2>六、结束语</h2><p>Doris + AI  一站式融合数据引擎 DataMind 的实现，已在字节内部应用一段时间，并在持续推广之中，典型应用场景包括智能简历搜索、ByteRAG 平台、CapCut 内容治理等。且在 GraphRAG 上线后，团队与多方客户合作实现了场景落地，例如广告场景、代码搜索的场景，以及近期业界关注的 PRD2Code 等研发提效场景 。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445516" alt="六、结束语-1.png" title="六、结束语-1.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445517" alt="六、结束语.png" title="六、结束语.png" loading="lazy"/></p><p>未来，我们还会在 DATA + AI 上继续探索，搭建更加完善的企业 AI 问数架构。此外，我们将保持与 Doris 开源社区的紧密联系保持联系，积极参与共建并为社区提供反馈。</p><p>欢迎更多的同仁加入 AI 能力共创中来，可通过下方二维码添加 Doris 小助手，回复【AI】即可加入 AI 专项交流群。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445518" alt="Doris AI 专项群二维码" title="Doris AI 专项群二维码" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[什么是国密SSL证书？一文读懂自主可控的]]></title>    <link>https://segmentfault.com/a/1190000047445174</link>    <guid>https://segmentfault.com/a/1190000047445174</guid>    <pubDate>2025-12-03 10:02:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>手机银行转账、政务平台提交材料，这些敏感操作的安全保障，除了“https”标识，国密SSL证书正发挥核心作用。《密码法》实施后，这款国产密码技术产品已成为政务、金融行业“标配”。今天就带大家认识它。<br/><img width="723" height="395" referrerpolicy="no-referrer" src="/img/bVdneOR" alt="" title=""/></p><h4>从核心定义出发：国密SSL证书是什么？</h4><p><a href="https://link.segmentfault.com/?enc=qo%2BGmJoAWM%2Fx7RDGeP15xw%3D%3D.ORaDTvWsGsE1mnxSiPTqnnEmISa5wmpusoo6DLWlz5HGqQYw0lSXNYZy277vYwXd%2F4upV%2Bplebbc3lXlTuBbreoA0UKvq%2B8Ic68Zl5wGlrU%3D" rel="nofollow" target="_blank"> <strong>国密证书申请入口</strong></a></p><p>国密SSL证书遵循我国密码标准，采用SM2、SM3、SM4等自主算法，核心功能与传统SSL证书一致——加密数据传输、验证服务器身份，防止信息泄露篡改。其最大优势是“自主可控”，从算法到签发全流程实现国产化，区别于依赖RSA等国际算法的传统证书。</p><p>三大核心算法构筑安全基石：SM2用256位密钥实现2048位RSA的安全强度，大幅降低服务器负载；SM3生成256位哈希值，抗碰撞性能优异；SM4加密效率比AES高30%以上，适配高并发场景。</p><h4>关键差异对比：国密证书与传统证书有何不同？</h4><p>传统SSL证书已普及，为何要推广国密证书？两者的四大核心差异，决定了其场景适用性。</p><p><strong>安全可控性</strong>：传统证书根由国际CA管理，易受国际政策影响（如俄乌冲突中俄罗斯网站证书遭吊销）；国密证书根由CFCA等国内机构管理，已预埋于国产系统和主流浏览器，从根源规避“卡脖子”风险。</p><p><strong>合规性</strong>：《密码法》等法规要求关键设施必须用国产密码，国密证书是等保2.0和密评的“合规通行证”；传统证书需额外改造，成本高且有隐患，某政务云平台迁移后合规成本降低65%。</p><p><strong>性能效率</strong>：SM2算法服务器并发响应比RSA快12-15倍，CPU占用降40%以上。某国有银行应用后，单笔交易加密时间从8ms缩至2ms，年省运维成本超200万。</p><p><strong>本地化服务</strong>：国际CA支持受时差语言限制，国内机构可提供中文客服、1小时响应及定制化方案，适配金融专网等特殊需求。</p><h4>实用价值凸显：哪些场景必须用国密SSL证书？</h4><p>国密证书应用已从强监管行业向普通企业延伸，以下三类场景需优先部署。</p><p><strong>关键领域</strong>：政务、金融、能源等涉及公共利益或国家秘密的行业，《关键信息基础设施安全保护条例》明确要求用国产密码，国密证书是核心安全防线。</p><p><strong>数据本地化需求企业</strong>：国密证书全流程在国内完成，符合《数据安全法》要求，规避数据出境风险，适配电商、医疗等处理敏感信息的平台。</p><p><strong>兼顾成本与安全的企业</strong>：国密证书价格亲民（基础型DV证书年百元级），部分地区有补贴优惠，比国际品牌同类产品性价比更高。</p><h4>常见疑问解答：关于国密SSL证书的那些“顾虑”</h4><p>企业常见顾虑是兼容性，目前12款主流浏览器已原生支持国密协议，覆盖90%以上国产系统。有国际业务可采用“SM2+RSA双证书”方案，国内用国密、海外用国际证书，某银行应用后访问成功率达99.99%。</p><p>国密算法已通过国家认证，SM2在抗量子计算攻击上优于RSA。国家正研发抗量子密码算法，2026年将完成标准制定，国密证书安全优势将更突出。</p><h4>国密证书，网络安全的“中国方案”</h4><p>国密SSL证书以“自主可控、安全高效、合规适配”守护各类数据传输，对企业而言，部署它已不是“选择题”，而是顺应政策、保障安全的“必答题”。</p><p>“十四五”期间，关键领域国密证书应用率预计突破70%。随着国产密码生态完善，它将成为更多企业的选择，为数字中国筑牢安全基石。</p>]]></description></item><item>    <title><![CDATA[2025年主流低代码开发平台全景洞察：趋]]></title>    <link>https://segmentfault.com/a/1190000047445185</link>    <guid>https://segmentfault.com/a/1190000047445185</guid>    <pubDate>2025-12-03 10:02:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字化转型进入深水区的2025年，低代码开发平台已从“效率工具”升级为企业数字化转型的核心基础设施。据Gartner 2025年Q4最新报告显示，中国低代码市场规模已突破131亿元，年复合增长率超20%，70%的新应用将通过低代码/无代码技术构建，远超2023年的45%。低代码开发平台凭借可视化编程、组件化配置与少量代码编写的融合模式，将软件开发门槛降低60%以上，实现业务人员与技术团队的高效协同，推动应用交付周期从传统开发的3-6个月缩短至2-4周。从中小企业的轻量管理工具到大型企业的核心业务系统，低代码开发平台正渗透到金融、制造、政务等80%以上的重点行业，成为驱动数字经济发展的重要引擎。本文结合Forrester、Gartner、IDC等权威机构评估，梳理2025年低代码开发平台的核心趋势与主流品牌，为企业选型提供参考。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047445187" alt="" title=""/></p><p><strong>2025年低代码开发平台核心发展趋势</strong></p><p>Forrester在2025年Q2发布的《Forrester Wave™：专业开发者低代码平台》报告中，明确将AI增强能力、信创适配深度、可扩展架构及行业解决方案成熟度列为低代码开发平台竞争力的四大核心指标。结合信通院《低代码产业发展研究报告（2025年）》与IDC市场数据，当前低代码开发平台行业呈现三大显著趋势。</p><p>第一，AI原生重构开发链路。2025年低代码开发平台的核心变化是AI从“辅助功能”升级为“底层架构”，实现从“代码片段生成”到“领域模型驱动”的跨越。主流低代码开发平台均已集成多模态大模型，通过自然语言建模、智能调试、自动生成源码等功能，使开发效率提升300%-500%，部分平台可实现“自然语言转领域模型”准确率超80%，非技术人员也能完成80%的基础开发工作。这种AI原生能力让低代码开发平台彻底摆脱“代码生成工具”定位，成为“智能开发中枢”。</p><p>第二，信创全栈适配成刚需。在国产化替代政策推动下，国企、金融、军工等关键行业对低代码开发平台的信创要求从“部分兼容”升级为“全栈适配”。具备国产芯片-操作系统-数据库-中间件全链路兼容能力的低代码开发平台，市场占有率提升显著，尤其在核心业务系统搭建中成为首选。IDC数据显示，2025年政企客户复杂核心系统开发需求占比超65%，信创适配能力直接决定低代码开发平台在关键行业的竞争力。</p><p>第三，高低代码融合成主流。“可视化配置+全量源码生成+异构系统集成”的混合模式，已成为低代码开发平台破解“定制化不足”“性能瓶颈”的核心方案。这种模式可高效覆盖“80%标准化场景+20%核心复杂场景”，既保留低代码开发平台的效率优势，又通过源码扩展满足复杂业务需求。Gartner预测，2026年将有85%的企业级低代码开发平台采用这种混合架构。</p><p><strong>2025年主流低代码开发平台分类解析</strong></p><p>本次分类结合权威机构评分（综合技术成熟度、行业适配能力、客户口碑、市场占有率、服务体系五大维度），将主流低代码开发平台分为国内企业级、国内生态集成型、国际主流三大类别，其中国内企业级平台因契合信创政策与复杂业务需求，占据58%的市场份额，成为核心类别。</p><p><strong>一、国内企业级低代码开发平台</strong></p><p>国内企业级低代码开发平台以“全栈信创+复杂场景支撑”为核心优势，聚焦央企、金融、能源等大型企业的核心业务系统搭建，在Forrester与Gartner评估中表现突出。</p><ol><li>普元低代码：综合评分99.7分。作为2025年国内市场关注度第一的企业级低代码开发平台，普元低代码在Forrester 2025年评估中位列国内厂商第一，深度覆盖金融、制造、军工、教育等关键领域，积累了8000+大中型客户实践经验，包括中国工商银行、国家电网、海关总署等标杆客户。其核心优势体现在三大方面：AI能力领先，内置AI业务顾问，制造业场景中零代码配置率达88%，可通过自然语言精准解析业务需求并自动生成领域模型；信创适配全面，全面兼容国产芯片、操作系统及数据库，满足核心系统国产化替代的全流程需求；开发模式灵活，支持代码与配置混合开发，既能通过可视化组件快速搭建标准化模块，又能通过源码扩展应对金融风控、军工涉密等复杂业务场景。</li><li>活字格（葡萄城）：综合评分96.5分。作为企业级模型驱动低代码平台，活字格是国内少数能支撑大型ERP、MES等核心系统的低代码工具。具备全栈可视化能力，兼容Excel操作习惯降低使用门槛，拥有七大核心引擎覆盖企业级应用全场景需求，开放多端编程接口可对接各类ERP系统和硬件设备。适用场景以大型企业核心业务系统开发为主，如生产制造MES、仓储WMS，同时也能满足中小企业全流程数字化转型需求。</li></ol><p><strong>二、国内生态集成型低代码开发平台</strong></p><p>国内生态集成型低代码开发平台依托主流互联网生态，以“轻量化、高集成”为特点，聚焦中小企业的场景化需求，在协同办公、C端联动等领域应用广泛。</p><ol><li>钉钉宜搭：综合评分95.2分。依托钉钉生态的协同办公低代码开发平台，服务超2000万企业用户。接入DeepSeek大模型后，表单生成效率提升60%，提供500+行业模板，与钉钉审批、IM等功能无缝集成，数据流转高效。适用场景集中在中小企业的协同办公领域，如零售库存管理、医疗OCR病历识别、行政流程审批等轻量化应用。</li><li>腾讯云微搭：综合评分94.8分。聚焦微信生态的低代码开发平台，支持小程序、Web多端同步开发，解决C端应用快速落地的需求。内置AI Copilot功能，可自动生成代码片段与测试用例，开发周期缩短70%，同时支持私有化部署保障企业数据主权。适用场景以C端联动为主，如农业精准施肥系统、三维导览小程序、社区服务平台等。</li><li>金蝶云·苍穹：综合评分93.5分。由ERP厂商转型的低代码开发平台，专注企业核心业务系统搭建，与金蝶原有ERP体系兼容性极强。基于动态领域模型，可快速构建制造业MES、零售业OMS等复杂系统，已完成信创适配兼容国产软硬件。适用场景以已有金蝶体系的企业为主，尤其契合国资国企的国产化替代需求。</li></ol><p><strong>三、国际主流低代码开发平台</strong></p><p>国际主流低代码开发平台在全球化部署、跨行业集成方面具备优势，适合跨国企业或有海外业务的企业，但其信创适配能力与国内政策契合度相对较弱。</p><ol><li>OutSystems：综合评分96.2分。全球企业级低代码领军平台，连续9年入选Gartner魔力象限领导者，在Forrester 2025年报告中位列全球领导者象限第一。集成AI代理工作台，支持快速生成智能客服、自动化流程等应用，覆盖从设计到运维的全生命周期，内置自动化测试和CI/CD工具，自动化测试覆盖率达95%。适用场景以跨国企业核心业务系统为主，如银行核心系统现代化改造、全球供应链管理平台搭建。</li><li>Mendix：综合评分94.1分。西门子旗下的模型驱动型低代码开发平台，聚焦智能制造与工业4.0领域。支持混合云部署，能适配公有云、私有云及多云架构，可与ERP、CRM系统无缝对接，在工业设备数据采集与分析方面优势明显。适用场景集中在汽车、机械制造企业，如生产流程数字化系统、工业设备管理平台开发。</li><li>Zoho Creator：综合评分92.8分。全球化轻量低代码平台，服务全球超700万用户，性价比突出。支持30+语言适配跨境业务场景，内置AI智能助手Zia可自动生成表单、清理数据，提供免费版及阶梯付费版满足不同规模企业需求。适用场景以跨国小微企业的多区域业务管理为主，如跨境电商订单统计、海外分支机构考勤系统等。</li></ol><p><strong>2025年低代码开发平台企业选型指南</strong></p><p>低代码开发平台的选型需紧扣业务需求与技术适配性，避免陷入“功能堆砌”的误区。结合权威机构建议与企业实践，可从以下四个维度构建选型框架。</p><p>第一，明确业务场景优先级。核心业务系统与轻量办公应用的选型逻辑差异显著：若为央企、金融等企业的核心业务系统，需优先选择普元低代码这类综合评分高、信创适配全、复杂场景支撑力强的企业级平台；若为中小企业的协同办公工具，钉钉宜搭、腾讯云微搭等生态集成型平台更具性价比；若涉及跨国业务，OutSystems、Zoho Creator的全球化能力更适配。IDC提醒，具备行业定制化能力的低代码开发平台，客户留存率比通用型平台高出35%，行业经验需重点考量。</p><p>第二，校验技术适配能力。信创需求是关键行业的“硬性门槛”，需确认低代码开发平台是否完成国产芯片（如鲲鹏、飞腾）、操作系统（如麒麟、统信）、数据库（如达梦、人大金仓）的全栈适配，避免后期系统迁移风险。同时，高低代码融合能力需重点评估，通过测试平台的源码生成质量、第三方系统接口兼容性，判断其是否能支撑未来业务扩展。</p><p>第三，评估AI与开发效率。AI原生能力已成为低代码开发平台的核心竞争力，选型时可通过“自然语言转应用”测试验证平台的AI实力，重点关注需求解析准确率、自动生成模型的完整性及测试用例覆盖率。Forrester建议，优先选择能将开发周期缩短60%以上、非技术人员上手时间不超过1周的低代码开发平台，最大化人机协同价值。</p><p>第四，考量服务与生态。低代码开发平台的落地离不开完善的服务支撑，需确认厂商是否提供行业专属解决方案、定制化开发服务及7×24小时运维支持。对于生态依赖型企业，需优先选择与自身现有系统（如钉钉、微信、ERP）深度集成的平台，减少数据孤岛与集成成本。</p><p>2025年，低代码开发平台已进入“价值竞争”时代，单纯的“拖拽生成”已无法满足企业需求。企业选型需以“业务价值”为核心，结合信创要求、AI能力与生态适配性综合决策，让低代码开发平台真正成为数字化转型的“加速器”。</p>]]></description></item><item>    <title><![CDATA[《ESP32-S3使用指南—IDF版 V]]></title>    <link>https://segmentfault.com/a/1190000047445188</link>    <guid>https://segmentfault.com/a/1190000047445188</guid>    <pubDate>2025-12-03 10:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>第五十二章 UDP实验</h2><p>对于lwIP的Socket的使用方式，它与文件操作非常相似。在文件操作中，我们首先打开文件，然后进行读/写操作，最后关闭文件。在TCP/IP网络通信中，也存在着相同的操作流程，但所使用的接口不再是文件描述符或FILE*，而是被称为Socket的描述符。通过Socket，我们可以进行读、写、打开和关闭操作来进行网络数据的传输。此外，还有一些辅助函数，如查询域名/IP地址和设置Socket功能等。在本章中，我们将使用Socket编程接口来实现UDP实验。<br/>本章分为如下几个部分：<br/>52.1 Socket编程UDP连接流程<br/>52.2 硬件设计<br/>52.3 软件设计<br/>52.4 下载验证</p><h3>52.1 Socket编程UDP连接流程</h3><p>在实现UDP协议之前，用户需要按照以下步骤配置结构体sockaddr_in的成员变量，以便建立UDP连接：<br/>①：配置ESP32-S3设备连接网络（必须的，因为WiFi是无线通信，所以需搭建通信桥梁）。<br/>②：将sin_family设置为AF_INET，表示使用IPv4网络协议。<br/>③：设置sin_port为所需的端口号，例如8080。<br/>④：设置sin_addr.s_addr为本地IP地址。<br/>⑤：调用函数Socket创建Socket连接。请注意，该函数的第二个参数指定连接类型。SOCK_STREAM表示TCP连接，而SOCK_DGRAM表示UDP连接。<br/>⑥：调用函数bind将本地服务器地址与Socket进行绑定。<br/>⑦：调用适当的收发函数来接收或发送数据。<br/>通过遵循这些步骤，用户可以成功地配置并建立UDP连接，以实现数据的发送和接收。</p><h3>52.2 硬件设计</h3><p><strong>1.例程功能</strong><br/>本章实验功能简介：<br/>本实验主要通过Socket编程接口实现了一个UDP服务器。这个服务器具有以下功能：<br/>①：可以通过按键发送UDP广播数据给其他UDP客户端。<br/>②：能够接收其他UDP客户端发送的广播数据。<br/>③：实时将接收到的数据显示在LCD屏幕上。<br/>通过这个实验，用户可深入了解UDP协议的工作原理，并掌握如何使用Socket编程接口来实现UDP通信。这对于开发基于UDP的网络应用程序非常有用，例如实时通信、多播应用等。</p><p><strong>2.硬件资源</strong><br/>1）LED灯<br/>LED-IO1<br/>2）XL9555<br/>IIC_INT-IO0（需在P5连接IO0）<br/>IIC_SDA-IO41<br/>IIC_SCL-IO42<br/>3）SPILCD<br/>CS-IO21<br/>SCK-IO12<br/>SDA-IO11<br/>DC-IO40（在P5端口，使用跳线帽将IO_SET和LCD_DC相连）<br/>PWR- IO1_3（XL9555）<br/>RST- IO1_2（XL9555）<br/>4）ESP32-S3内部WiFi</p><p><strong>3.原理图</strong><br/>本章实验使用的WiFi为ESP32-S3的片上资源，因此并没有相应的连接原理图。</p><h3>52.3 软件设计</h3><p><strong>52.3.1 程序流程图</strong><br/>程序流程图能帮助我们更好的理解一个工程的功能和实现的过程，对学习和设计工程有很好的主导作用。下面看看本实验的程序流程图：<br/><img width="432" height="403" referrerpolicy="no-referrer" src="/img/bVdnaJa" alt="" title=""/><br/>图52.3.1.1 程序流程图</p><h3>52.3.2 程序解析</h3><p>在本章节中，我们主要关注两个文件：lwip_demo.c和lwip_demo.h。lwip_demo.h文件主要定义了发送标志位并声明了lwip_demo函数，这部分相对简单，所以我们暂不详细解释。主要关注点是lwip_demo.c文件中的函数。在lwip_demo函数中，我们配置了相关的UDP参数，并创建了一个名为lwip_send_thread的发送数据线程。这个线程通过调用scokec函数来发送数据到服务器。接下来，我们将分别详细解释lwip_demo函数和lwip_send_thread任务。</p><pre><code>/* 需要自己设置远程IP地址 */
#define IP_ADDR   "192.168.101.33"

#define LWIP_DEMO_RX_BUFSIZE         200                    /* 最大接收数据长度 */
#define LWIP_DEMO_PORT               8080                   /* 连接的本地端口号 */
#define LWIP_SEND_THREAD_PRIO    ( tskIDLE_PRIORITY + 3 )     /* 发送数据线程优先级 */

/* 接收数据缓冲区 */
uint8_t g_lwip_demo_recvbuf[LWIP_DEMO_RX_BUFSIZE]; 
/* 发送数据内容 */
char g_lwip_demo_sendbuf[] = "ALIENTEK DATA \r\n";
/* 数据发送标志位 */
uint8_t g_lwip_send_flag;
static struct sockaddr_in dest_addr;            /* 远端地址 */
struct sockaddr_in g_local_info;
socklen_t g_sock_fd;                            /* 定义一个Socket接口 */
static void lwip_send_thread(void *arg);
extern QueueHandle_t g_display_queue;           /* 显示消息队列句柄 */


/**
 * @brief       发送数据线程
 * @param       无
 * @retval      无
 */
void lwip_data_send(void)
{
xTaskCreate(lwip_send_thread, "lwip_send_thread", 4096, 
NULL, LWIP_SEND_THREAD_PRIO, NULL);
}

/**
 * @brief       lwip_demo实验入口
 * @param       无
 * @retval      无
 */
void lwip_demo(void)
{
    char *tbuf;
    lwip_data_send();                                      /* 创建发送数据线程 */
    /* 远端参数设置 */
    dest_addr.sin_addr.s_addr = inet_addr(IP_ADDR);          /* 目标地址 */
    dest_addr.sin_family = AF_INET;
    dest_addr.sin_port = htons(LWIP_DEMO_PORT);             /* 目标端口 */
    
    g_local_info.sin_family = AF_INET;                       /* IPv4地址 */
    g_local_info.sin_port = htons(LWIP_DEMO_PORT);          /* 设置端口号 */
    g_local_info.sin_addr.s_addr = htons(INADDR_ANY);       /* 设置本地IP地址 */

    g_sock_fd = socket(AF_INET, SOCK_DGRAM, 0);        /* 建立一个新的socket连接 */
    
    tbuf = malloc(200);                                    /* 申请内存 */
    sprintf((char *)tbuf, "Port:%d", LWIP_DEMO_PORT);       /* 客户端端口号 */
    lcd_show_string(0, 170, 200, 16, 16, tbuf, MAGENTA);
    
    /* 建立绑定 */
    bind(g_sock_fd, (struct sockaddr *)&amp;g_local_info, sizeof(g_local_info));

    while (1)
    {
        memset(g_lwip_demo_recvbuf, 0, sizeof(g_lwip_demo_recvbuf));
        recv(g_sock_fd, (void *)g_lwip_demo_recvbuf,
             sizeof(g_lwip_demo_recvbuf), 0);
        printf("%s\r\n",g_lwip_demo_recvbuf);
    }
}

/**
 * @brief       发送数据线程函数
 * @param       pvParameters : 传入参数(未用到)
 * @retval      无
 */
void lwip_send_thread(void *pvParameters)
{
    pvParameters = pvParameters;

    while (1)
    {    /* 有数据要发送 */
        if ((g_lwip_send_flag &amp; LWIP_SEND_DATA) == LWIP_SEND_DATA)    
        {
            printf("send\r\n");
            sendto(g_sock_fd,                           /* scoket */
                  (char *)g_lwip_demo_sendbuf,         /* 发送的数据 */
                  sizeof(g_lwip_demo_sendbuf), 0,        /* 发送的数据大小 */
                  (struct sockaddr *)&amp;dest_addr,       /* 接收端地址信息 */ 
                  sizeof(dest_addr));                   /* 接收端地址信息大小 */

            g_lwip_send_flag &amp;= ~LWIP_SEND_DATA;
        }
        
        vTaskDelay(100);
   }
}</code></pre><p>在源码中，lwip_demo函数通过lwip_data_send创建了发送数据的线程lwip_send_thread，并配置了Socket的UDP协议。该线程在发送前会检查标志位，有效时则通过sendto发送数据并重置标志位。同时，需设置目标IP地址以确保数据正确发送。此外，主函数的循环中不断通过recv接收数据并使用串口输出接收的数据。</p><h3>52.4 下载验证</h3><p>在程序中，首先需要设置好能够连接的网络账号和密码。然后，使用笔记本电脑作为终端，确保它与ESP32-S3设备处于同一网络段内。当ESP32-S3设备成功连接到网络时，它的LCD显示屏上会显示相应的内容：<br/> <img width="307" height="230" referrerpolicy="no-referrer" src="/img/bVdnaJb" alt="" title="" loading="lazy"/><br/>图52.4.1 设备连接到网络时，LCD显示的信息<br/>打开网络调试助手，然后配置网络参数，如UDP协议、端口号、目标主机设置等，设置内容如下图所示。<br/><img width="723" height="259" referrerpolicy="no-referrer" src="/img/bVdnaJf" alt="" title="" loading="lazy"/><br/>在确保网络连接正常后，可以通过按下开发板上的KEY0按键来发送数据至网络调试助手。当网络调试助手接收到“ALIENTEK DATA”字符串时，它会在显示区域展示这个信息。此外，用户还可以在调试助手的发送区域自行输入要发送的数据，然后点击发送键，将数据发送至ESP32-S3设备。此时，ESP32-S3的串口将打印接收到的数据，具体操作和输出如下图所示。<br/><img width="723" height="82" referrerpolicy="no-referrer" src="/img/bVdnaJg" alt="" title="" loading="lazy"/><br/>图52.4.3 接收网络调试助手的数据</p>]]></description></item><item>    <title><![CDATA[【URP】Unity[内置Shader]]]></title>    <link>https://segmentfault.com/a/1190000047445156</link>    <guid>https://segmentfault.com/a/1190000047445156</guid>    <pubDate>2025-12-03 09:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><a href="https://link.segmentfault.com/?enc=vz5C%2FSTiz0OGMo31E5hyyw%3D%3D.n7RsWOrFty%2Fo0paVb6oca3ofngshgOBE%2FnY%2FPk7md93BdEAW1IfuN0Wz%2Fs5wyHCvPXBSM5gKHgY48woD1XSFUcrxpi35%2FkA6uEbVJVIeL2QcRtRljWxnfw%2F7hPkqEMSxXMgc3cG19E1SGd0WwulHqMRI49q6febZA0ycSgDXAxz41riVTyQaotTqtqHxbkrTYChnVQFaeSvqO%2BYdDePxqzysApV%2FWinmEcLgGbdkTL8%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong></blockquote><p>Unity URP内置的Particles Lit着色器是专为粒子系统设计的高质量光照模型，其核心作用是为火焰、烟雾、雨雪等动态粒子效果提供逼真的光照交互。该着色器采用URP的物理光照计算模型，支持透明混合、深度碰撞检测等高级特性，但会带来较高的性能开销。</p><h2><strong>原理与特性</strong></h2><ul><li>‌<strong>光照模型</strong>‌：基于URP的PBR光照计算，支持方向光、点光源和聚光灯的实时交互，通过表面法线计算高光反射</li><li>‌<strong>混合模式</strong>‌：提供Alpha、Premultiply、Additive和Multiply四种混合模式，分别适用于云雾（Alpha）、玻璃反射（Premultiply）、全息效果（Additive）等场景</li><li>‌<strong>深度交互</strong>‌：可与深度纹理比对实现粒子碰撞效果，通过CS脚本在渲染管线中同步深度数据</li></ul><h2><strong>发展沿革</strong></h2><ul><li>‌<strong>2019年</strong>‌：随URP 7.x版本首次推出，最初仅支持基础光照模型</li><li>‌<strong>2021年</strong>‌：URP 12.x加入深度纹理交互支持，实现粒子碰撞效果</li><li>‌<strong>2023年</strong>‌：优化移动端性能，在URP 14.x中成为粒子系统默认推荐着色器</li></ul><h2>Particles Lit 对比 Lit</h2><h3><strong>渲染模式选择</strong></h3><p>ParticlesLit专为粒子系统设计，提供更灵活的混合模式（如Additive、Multiply等），适合处理透明粒子的叠加效果，而Lit通常用于不透明或标准透明物体渲染。ParticlesLit支持通过Color Mode控制粒子颜色与材质颜色的混合方式（如Multiply、Additive等），可减少过度混合导致的性能损耗。</p><h3><strong>性能敏感功能裁剪</strong></h3><p>ParticlesLit默认关闭了Lit中部分高消耗特性（如复杂光照计算），采用简化的光照模型。例如，它避免使用完整PBR计算，转而使用预乘混合（Premultiply）保留高光的同时降低透明渲染开销。此外，粒子系统通常禁用碰撞检测和物理交互，进一步降低CPU负载。</p><h3><strong>资源复用与批处理</strong></h3><p>ParticlesLit鼓励材质共享和纹理图集化，通过减少DrawCall提升性能。建议多个粒子系统共用同一材质，且纹理尺寸不超过256x256。相比之下，Lit可能涉及更多独立材质实例，尤其在复杂场景中。</p><h3><strong>渲染参数优化</strong></h3><p>ParticlesLit提供针对粒子的特定参数控制：</p><ul><li>通过Alpha Clipping实现硬边透明（如草叶效果），避免全透明混合的计算开销</li><li>推荐小尺寸粒子去除Alpha通道，改用Opaque渲染以减少Overdraw</li><li>限制粒子数量（单发射器&lt;50，屏幕总数&lt;200）以控制顶点处理压力</li></ul><h3><strong>底层实现差异</strong></h3><p>ParticlesLit在Shader代码中显式优化了类型转换和初始化（如half4 color = (half4)0），避免编译器警告并提升执行效率。而Lit更侧重通用物体渲染的精度和功能完整性。</p><p>综合来看，ParticlesLit通过简化光照模型、优化混合策略、限制资源消耗等方式，在保证粒子视觉效果的同时实现比Lit更高的渲染效率.</p><h2><strong>Particles Lit 的四种混合模式</strong></h2><p>ParticlesLit着色器提供了四种混合模式，主要用于控制粒子效果与背景的视觉融合方式</p><h3><strong>Alpha混合模式</strong></h3><p>通过材质的Alpha值控制透明度，0为完全透明，1为视觉上不透明但仍参与透明渲染通道。适用于需要渐变消失的效果，如云朵消散。其特点是保持粒子颜色纯度，但可能丢失高光细节。</p><h3><strong>Premultiply(预乘Alpha)</strong></h3><p>保留反射和高光特性，即使表面透明时仍能显示镜面效果。典型应用是透明玻璃或冰晶材质，仅反射光可见而本体透明。该模式需配合预乘处理的纹理使用，避免边缘黑边问题。</p><h3><strong>Additive(叠加)</strong></h3><p>将粒子颜色与背景色相加，产生增亮效果。适用于发光体如火焰、全息投影，能突出高亮区域但易导致过曝。火星特效常采用此模式增强核心亮度。</p><h3><strong>Multiply(相乘)</strong></h3><p>使粒子颜色与背景色相乘，产生变暗效果。模拟彩色玻璃透光或阴影叠加，适合风格化场景的氛围营造。需注意暗部细节可能丢失。</p><h3><strong>应用选择建议</strong></h3><ul><li>‌<strong>性能考虑</strong>‌：Additive和Multiply计算量较低，Premultiply消耗较大</li><li>‌<strong>视觉特性</strong>‌：动态火焰推荐Additive，半透明物体用Alpha，材质反射需求选Premultiply</li><li>‌<strong>移动端优化</strong>‌：可改用Mobile/Particles/Additive等简化着色器</li></ul><p>混合模式可通过材质Inspector面板的"Blending Mode"下拉菜单切换，需配合Render Face(渲染面)和Alpha Clipping(透明剪切)等参数调整最终效果</p><h2>深度纹理比对实现深度交互的碰撞效果</h2><h3><strong>深度纹理获取与处理</strong></h3><p>首先需启用相机的深度纹理渲染功能，通过勾选RenderPipelineAsset中的DepthTexture选项生成场景深度图。深度纹理存储的是归一化设备坐标(NDC)的z分量值，经过非线性透视投影变换后，使用公式<code>d=0.5*z+0.5</code>将深度值映射到[0,1]范围。正交投影的深度计算则是线性的，需区分处理。</p><h3><strong>碰撞检测原理</strong></h3><p>通过比较屏幕空间中的顶点距离与场景深度缓冲区的值来实现碰撞判定。具体步骤包括：</p><ul><li>‌<strong>访问屏幕位置</strong>‌：获取当前顶点在屏幕空间的坐标和深度值。</li><li>‌<strong>深度差值计算</strong>‌：用场景深度值减去顶点深度值，得到两者间的距离差。</li><li>‌<strong>边缘梯度控制</strong>‌：通过调整场景位置的偏移量，可精确控制碰撞边缘的渐变效果。</li></ul><h3><strong>效果增强技术</strong></h3><ul><li>‌<strong>Alpha混合修正</strong>‌：将碰撞区域的Alpha值通过<code>1-</code>操作反转，并与菲涅尔效应叠加，可生成发光边缘的视觉效果。</li><li>‌<strong>纹理变形技术</strong>‌：参考流体模拟中的UV坐标动画方法，通过动态扭曲纹理贴图增强交互的真实感。例如Valve在《Portal 2》中采用滑动表面着色器，对UV坐标进行时间驱动的位移计算。</li></ul><h3><strong>性能优化</strong></h3><ul><li>‌<strong>纹理复用</strong>‌：使用小块纹理通过重复平铺实现大范围覆盖，减少内存占用。</li><li>‌<strong>简化几何体</strong>‌：对于背景物体，可用带纹理的简单几何体替代高模，结合Billboard技术保持视觉一致性。</li></ul><h2><strong>具体使用示例</strong></h2><h3>创建火焰粒子材质：</h3><ul><li>新建材质并选择Shader路径：<code>Universal Render Pipeline &gt; Particles &gt; Lit</code></li><li>设置Surface Type为Transparent，Blending Mode为Additive</li><li>绑定粒子贴图并调整颜色参数：</li><li>_MainTex: 火焰序列帧贴图<br/>_Color: RGBA(1,0.5,0,0.8)<br/>_Emission: 2.0</li></ul><h3><strong>雨打到地上和物体上碰撞产生水花</strong></h3><ul><li><p>‌<strong>雨水粒子基础配置</strong>‌：</p><ul><li>使用Rectangle发射器形状并旋转90度使粒子垂直下落</li><li>设置Velocity over Lifetime的World空间模式确保雨滴始终朝Y轴降落</li><li>通过Linear参数添加XYZ方向偏移模拟风力效果</li></ul></li><li><p>‌<strong>碰撞检测模块</strong>‌：</p><ul><li>启用粒子系统的Collision模块，选择World碰撞模式</li><li>设置Dampen参数为1使碰撞后粒子完全停止</li><li>调整Bounce参数控制水花溅射力度</li></ul></li><li><p>‌<strong>水花效果生成</strong>‌：</p><ul><li>使用Sub Emitters模块在粒子消亡时触发子发射器</li><li>子粒子系统采用Horizontal Billboard渲染模式保持水平显示</li><li>通过Size over Lifetime曲线控制水花扩散动画</li></ul></li><li><p>RainCollisionController.cs</p><pre><code class="csharp">using UnityEngine;

[RequireComponent(typeof(ParticleSystem))]
public class RainCollisionController : MonoBehaviour {
    private ParticleSystem _mainSystem;
    private ParticleSystem _splashSystem;

    void Start() {
        _mainSystem = GetComponent&lt;ParticleSystem&gt;();
        var collision = _mainSystem.collision;
        collision.enabled = true;
        collision.type = ParticleSystemCollisionType.World;

        // 获取子发射器系统
        _splashSystem = transform.GetChild(0).GetComponent&lt;ParticleSystem&gt;();
    }

    void Update() {
        // 动态调整粒子发射速率
        var emission = _mainSystem.emission;
        emission.rateOverTime = Mathf.Lerp(50, 500, WeatherManager.Instance.RainIntensity);
    }
}</code></pre></li><li><p>RainCollisionController.cs</p><pre><code class="c">Shader "Custom/Ripple" {
    Properties {
        _MainTex ("Base (RGB)", 2D) = "white" {}
        _Speed ("Animation Speed", Range(0,5)) = 1.0
    }
    SubShader {
        Tags { "Queue"="Transparent" }
        Blend SrcAlpha OneMinusSrcAlpha

        Pass {
            HLSLPROGRAM
            #pragma vertex vert
            #pragma fragment frag
            // 着色器代码...
            ENDHLSL
        }
    }
}</code></pre></li><li><p><strong>性能优化</strong></p><ul><li><p>‌<strong>纹理数组替代</strong>‌：使用Texture2DArray将多张纹理合并为单一资源，通过索引值（存储在SplatMap的R/G通道）选择纹理，减少采样次数。例如：</p><pre><code class="c">hlsl
half4 var_Main = SAMPLE_TEXTURE2D_ARRAY(_TexArray, sampler_TexArray, uv, splat.r * 255) * splat.b;
half4 var_Sec = SAMPLE_TEXTURE2D_ARRAY(_TexArray, sampler_TexArray, uv, splat.g * 255) * (1 - splat.b);
half4 finalRGB = var_Main + var_Sec;</code></pre></li><li>‌<strong>高度混合增强</strong>‌：结合高度图（存储在SplatMap的B通道）实现更自然的过渡效果，通过比较各层高度值动态调整权重</li></ul></li></ul><h2><strong>Shader Graph应用</strong></h2><p>实现雨滴效果：</p><ul><li>创建新的Shader Graph，选择URP Particle Lit模板</li><li>添加Texture Sample节点连接Main Texture输入口</li><li>使用Custom Function节点实现法线扰动：</li></ul><pre><code class="c">hlsl
void RainDistortion_float(float2 uv, out float3 normal){
    normal = float3(frac(uv.x * 10), frac(uv.y * 5), 1);
}</code></pre><ul><li>输出口连接Normal和Base Color通道</li><li><p>RainParticle.shadergraph</p><pre><code class="c">{
    "m_Nodes": [
        {
            "m_Type": "UnityEditor.ShaderGraph.Texture2DNode",
            "m_Outputs": [{ "m_Name": "Out" }],
            "m_Inputs": [{ "m_Name": "Texture", "m_DefaultValue": "Assets/Textures/RainDrop.png" }]
        },
        {
            "m_Type": "UnityEditor.ShaderGraph.CustomFunctionNode",
            "m_Outputs": [{ "m_Name": "normal" }],
            "m_Code": "RainDistortion_float"
        }
    ],
    "m_Edges": [
        { "m_OutputSlot": 0, "m_InputSlot": "BaseColor" },
        { "m_OutputSlot": 1, "m_InputSlot": "Normal" }
    ]
}</code></pre></li></ul><p>该示例通过Shader Graph创建动态雨滴效果，包含纹理采样和法线扰动功能，需配合粒子系统使用</p><hr/><blockquote><a href="https://link.segmentfault.com/?enc=65daoUkASyzdz0WSovm0UQ%3D%3D.4e2x0%2BwQyUyvcJytsePzpaXumd5P2KH9b6G3%2BvczCLiO%2BV3V6h13dSKer7HgPmEVnZfzloH9Uda%2B%2FxXttz04dS4QJLdzLX0%2BdwvJ%2BWTTQKNE5o%2BqHYrFFGx2cpCKWj9NXgt4cZkfvC50CSMll7QAf%2FhfD3RxlQpKDmUepg%2FPkNOQW96Sq2BO%2Fu%2Bxv0M4fNTTZLyt3xp2hkd9RFASX7m2XpN%2BxHlYc6YbbKo9SXZKJ9A%3D" rel="nofollow" target="_blank">【从UnityURP开始探索游戏渲染】</a><strong>专栏-直达</strong><br/>（欢迎<em>点赞留言</em>探讨，更多人加入进来能更加完善这个探索的过程，🙏）</blockquote>]]></description></item><item>    <title><![CDATA[剑指offer-45、扑克牌顺⼦ 程序员]]></title>    <link>https://segmentfault.com/a/1190000047437801</link>    <guid>https://segmentfault.com/a/1190000047437801</guid>    <pubDate>2025-12-03 09:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>题⽬描述</h2><p>扑克牌可以组成顺⼦，⼤\⼩ 王可以看成任何数字,并且 A 看作 1 , J 为 11 , Q 为 12 , K 为 13 。 5张牌 【A,0,3,0,5】 就可以变成“ 1,2,3,4,5 ”(⼤⼩王分别看作 2 和 4 ),这样就组成了顺⼦。（可以认为⼤⼩王是 0 。）</p><p>输⼊五张牌，如果牌能组成顺⼦就输出true，否则就输出 false 。</p><p>示例1<br/>输⼊：[0,3,2,6,4]<br/>返回值：true</p><h2>思路及解答</h2><h3>排序遍历</h3><p>这是最直观的解法，通过排序后分析牌之间的间隔关系来判断。</p><p>排序后统计大小王数量，检查非王牌之间的间隔是否可用大小王填补：先排序，0肯定是靠左边，然后统计0的个数，后⾯的数，按照第⼀个⾮0的数进⾏递增，如果不是递增，则需要使⽤ 0 牌补充，如果 0 牌不够，需要放回 false ，否则直到遍历完数组，返回true 。</p><pre><code class="java">public boolean IsContinuous(int[] numbers) {
    // 数组⻓度不符合直接返回
    if (numbers == null || numbers.length &lt; 5) {
        return false;
    }
    // 先排序
    Arrays.sort(numbers);
    // 统计0的个数
    int numOfZero = 0;
    // 初始化索引
    int start;
    // 统计0的个数
    for (start = 0; start &lt; numbers.length; start++) {
        if (numbers[start] == 0) {
            numOfZero++;
        } else {
            // ⾮0的时候跳出
            break;
        }
        // 暂存0的个数
        int n = numOfZero;
        // 当前的数值
        int cur = numbers[numOfZero];
        // 从0的下两个位置开始
        for (start++; start &lt; numbers.length;) {
            // 如果可变的牌数量为0
            if (numOfZero == 0) {
                // 和前⾯的⼀个对⽐
                if (numbers[start] != cur + 1) {
                    // 不等于当前数值+1的话，直接返回false
                    return false;
                } else {
                    // 当前数值+1
                    cur++;
                }
            } else {
                // 不等于当前数值+1的话，直接返回false
                if (numbers[start] != cur + 1) {
                    // 可变牌数量-1
                    numOfZero--;
                    //当前值+1
                    cur++;
                    // 遍历下⼀张牌
                    continue;
                } else {
                    // 相等则直接将当前值+1
                    cur++;
                }
            }
            // 索引滑动到下⼀张牌
            start++;
        }
        return true;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n log n)，主要来自排序操作</li><li><p><strong>空间复杂度</strong>：O(1)，只使用常数级别额外空间</p><h3>哈希集合法（推荐）</h3></li></ul><p>利用HashSet实现去重，同时记录最大值和最小值。</p><p>初始化⼀个最⼩牌 14 ，最⼤牌 0 ，直接使⽤ set 保存数组的元素，如果 set 中已经存在该元素，那么我们直接放回 false ，如果 set 中不存在该元素，则将该元素放进 set 中，判断该元素是否⼩于最⼩牌，⼩于则更新最⼩牌，判断该元素是否⼤于最⼤牌，如果⼤于最⼤牌，则更新当前最⼤牌。</p><p><strong>为什么 <code>max - min &lt; 5</code>是充分必要条件？</strong></p><p>对于5张牌组成的顺子：</p><ul><li>如果是连续5张不同数字：max - min = 4</li><li>如果有空缺，但能被大小王填补：max - min ≤ 4</li><li>如果空缺太大：max - min ≥ 5，即使有4个大小王也无法填补</li></ul><p><strong>示例验证：</strong></p><ul><li><code>[1,3,0,0,5]</code>：max=5, min=1, 5-1=4&lt;5 ✓</li><li><code>[1,6,0,0,0]</code>：max=6, min=1, 6-1=5≥5 ✗</li></ul><pre><code class="java">public class Solution45 {
    public boolean IsContinuous(int[] numbers) {
        if (numbers == null || numbers.length &lt; 5) {
            return false;
        }
        HashSet &lt;Integer&gt; set = new HashSet &lt;&gt; ();
        int min = 14;
        int max = 0;
        for (int i = 0; i &lt; numbers.length; i++) {
            if (numbers[i] != 0) {
                if (set.contains(numbers[i])) {
                    return false;
                }
                set.add(numbers[i]);
                max = Math.max(max, numbers[i]);
                min = Math.min(min, numbers[i]);
            }
        }
        // 关键条件：最大牌-最小牌 &lt; 5 才能组成顺子
        return max - min &lt; 5;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，只需遍历数组一次</li><li><strong>空间复杂度</strong>：O(n)，HashSet的空间开销</li></ul><h3>位运算法（空间最优）</h3><p>利用整数的二进制位来标记牌值是否出现，实现O(1)空间复杂度。</p><p><strong>二进制位标记原理：</strong></p><ul><li>整数<code>flag</code>的32位中，用第i位表示数字i是否出现</li><li>例如：数字3出现 → 将第3位置1：<code>flag |= 1 &lt;&lt; 3</code></li><li>检查数字3是否出现：<code>(flag &gt;&gt; 3) &amp; 1 == 1</code></li></ul><pre><code class="java">public class Solution {

    public boolean isStraight(int[] nums) {
        if (nums == null || nums.length != 5) {
            return false;
        }
        
        int flag = 0; // 用二进制位标记牌值出现情况
        int max = 0;  // 非王牌最大值
        int min = 14; // 非王牌最小值
        
        for (int num : nums) {
            if (num == 0) {
                continue; // 跳过大小王
            }
            
            // 检查牌值是否已出现（检查第num位是否为1）
            if (((flag &gt;&gt; num) &amp; 1) == 1) {
                return false; // 有重复牌
            }
            
            // 标记牌值已出现（将第num位置为1）
            flag |= (1 &lt;&lt; num);
            
            // 更新最值
            if (num &gt; max) max = num;
            if (num &lt; min) min = num;
            
            // 提前判断：如果已经不可能组成顺子，直接返回
            if (max - min &gt;= 5) {
                return false;
            }
        }
        
        return max - min &lt; 5;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，线性遍历</li><li><strong>空间复杂度</strong>：O(1)，只使用固定数量的整数变量</li></ul>]]></description></item><item>    <title><![CDATA[Coze工作流意图识别 查拉图斯特拉说 ]]></title>    <link>https://segmentfault.com/a/1190000047444995</link>    <guid>https://segmentfault.com/a/1190000047444995</guid>    <pubDate>2025-12-03 00:02:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>前言这章节主要是简单了解一下工作流的一个应用，主要是讲的是意图识别，也就是我们在当模型应用当中常用的一些提问，然后大模型识别我们的意图，根据我们的意图去做相关的一些处理，这有点像一个流程图或者像我们平常的流程图，但是他很抽象，可以拖动对应的模块来处理。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047444997" alt="图片" title="图片"/><br/>扣子里面提供了很多插件，你可以根据你的选择去调用，看到里面部分的一些逻辑，判断有点像代码了<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047444998" alt="图片" title="图片" loading="lazy"/><br/>意图识别这个意图识别其实他也很抽象，就是根据你的言语来选择处理对的逻辑在这里，尤其是要去注意的是他这里有几个选项，你可以自己新增，也可以删除我这里添加的一个书籍简介，还有一个投诉，另外那个其他他是默认的，每一个选项后面会有一个点，每一个点后面可以执行出对应的逻辑方块<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047444999" alt="图片" title="图片" loading="lazy"/><br/>知识库这里我们需要提前搭建自己的知识库，这样的话当你选择知识库内容的时候，在里面添加的时候就可以获取到我们的知识库了，但是我们的知识库需要去进行一个发布，否则的话你找不到<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445000" alt="图片" title="图片" loading="lazy"/><br/>知识库内容<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445001" alt="图片" title="图片" loading="lazy"/><br/>书籍检索提示词根据{{booK}}书籍的名称，检索书籍简介<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445002" alt="图片" title="图片" loading="lazy"/><br/>测试一下<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445003" alt="图片" title="图片" loading="lazy"/><br/>在这里点开知识库，可以看到检索出来对应的书籍，这有点像查询数据库<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445004" alt="图片" title="图片" loading="lazy"/><br/>以上的流程让我想起到最近我在瑞信的App上用语音点杯咖啡，他用的也是应该类似的语音识别功能覆盖的足够多，完全可以很自然的满足你的一些要求，因为他不仅可以检索知识库，还可以检索数据库投诉路线提示词对客户的抱怨投诉{{input}}表示同情，并友好的询问缘由<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445005" alt="图片" title="图片" loading="lazy"/><br/>测试投诉<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445006" alt="图片" title="图片" loading="lazy"/><br/>很明显从我们的测试流程来看，他识别到了我们的意图，然后走了投诉这个线路，另外我们对大型输入的一些提示，你给我设定了一个角色，他的回答显得委婉了一些结束路线最后这个结束这里要注意一下，因为他有三条分支汇总到一起，而且他们汇总在一起的时候会有不同的一些属性输出，在这里你可以看到有三个属性，也就是说不管了模型从哪个属性里面读出来，最后再结束这里就会显示在哪个属性里面，所以你必须要添加所有的属性<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047445007" alt="图片" title="图片" loading="lazy"/><br/>总结这是一篇简单的个人总结，也是对我而言，我感觉收获最大的一个东西意图识别有了他，你可以做很多事情，因为它可以根据你的言语表达识别出你的想法而去做某件事情，这样就给大模型做应用，实现了更多的可能。</p>]]></description></item>  </channel></rss>