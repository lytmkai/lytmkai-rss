<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[CRM核心能力横向对比：从客户管理到多端协同，谁更适配你的业务？ 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047496297</link>    <guid>https://segmentfault.com/a/1190000047496297</guid>    <pubDate>2025-12-23 12:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在数字化转型中，CRM（客户关系管理）的价值早已超越“记录客户信息”——它是企业实现<strong>从线索到复购的全链路数字化、从流程管控到智能决策的核心工具</strong>。本文选取7款主流CRM产品（超兔一体云、红圈营销、六度人和EC、Salesforce、SugarCRM、Freshsales、Pipedrive），从<strong>客户信息管理、销售跟踪与待办、报表与分析、多端同步</strong>四大核心维度展开深度对比，结合业务场景给出选型建议。</p><h2>一、客户信息管理：从“数据收集”到“价值沉淀”的能力比拼</h2><p>客户信息管理的核心是<strong>将多源数据转化为可复用的客户资产</strong>，关键看“全生命周期覆盖、数据整合深度、画像精准度、查重效率”四大能力。</p><h3>1.1 核心子指标对比</h3><table><thead><tr><th>子指标</th><th>超兔一体云</th><th>红圈营销</th><th>六度人和（EC）</th><th>Salesforce</th><th>SugarCRM</th><th>Freshsales</th><th>Pipedrive</th></tr></thead><tbody><tr><td><strong>全生命周期覆盖</strong></td><td>潜在-意向-成交-复购（含客池分类）</td><td>潜在-意向-成交-售后（关联服务记录）</td><td>获客-转化-复购（腾讯生态闭环）</td><td>线索-客户-商机-服务（销售云+服务云）</td><td>线索-客户-订单（模块化）</td><td>线索-客户-交易（社交媒体+行为）</td><td>线索-客户-管道阶段（绑定销售流程）</td></tr><tr><td><strong>多源数据整合</strong></td><td>百度/抖音/官网/微信/工商/天眼查</td><td>表单/导入/外勤记录</td><td>企业微信/电话/邮件/海关数据</td><td>邮件/表单/销售云/服务云</td><td>第三方应用集成（如Mailchimp）</td><td>社交媒体（LinkedIn）/邮件/表单</td><td>表单/邮件/Chrome插件</td></tr><tr><td><strong>360°画像能力</strong></td><td>基本信息+工商背景+互动时序+财务汇总</td><td>基本信息+订单+服务记录</td><td>腾讯生态数据+工商+采购偏好</td><td>多渠道数据整合+自定义字段</td><td>模块化视图+客户关系管理</td><td>社交媒体动态+行为轨迹+交易数据</td><td>管道阶段+基本信息+沟通记录</td></tr><tr><td><strong>查重与合并</strong></td><td>自定义规则（客户名/手机号+简称模糊）</td><td>自动查重+手动合并</td><td>重复数据智能清理</td><td>规则引擎+批量合并工具</td><td>手动/自动合并</td><td>自动合并重复线索</td><td>无明确说明</td></tr><tr><td><strong>自定义配置</strong></td><td>字段/布局/客池分类（如“需求培养”“成功客”）</td><td>字段/查询/报表自定义</td><td>标签/分层/行业模板（外贸/教育）</td><td>自定义对象/字段/页面布局</td><td>模块化自定义（如添加“会员等级”）</td><td>字段/视图/报表自定义</td><td>字段/标签/管道阶段自定义</td></tr></tbody></table><h3>1.2 深度分析：谁的“数据价值”转化更高效？</h3><ul><li><strong>超兔一体云</strong>：<strong>工商数据补全+自定义查重</strong>是核心亮点。通过百度/天眼查自动补充企业背景、注册地址、经纬度，解决“客户信息不全”痛点；支持“简称模糊查重”（如“阿里云”与“阿里云计算”合并），避免重复录入。适合<strong>B端企业</strong>（需要精准的企业客户背景）。</li><li><strong>六度人和（</strong> <strong>EC</strong> <strong>）</strong> ：<strong>腾讯生态深度整合</strong>是壁垒。打通企业微信、邮件、电话数据，结合工商/海关数据生成“精准画像”（如“某外贸客户的采购频率”），解决“私域数据分散”问题。适合<strong>依赖微信生态的企业</strong>（如教育、零售）。</li><li><strong>Salesforce</strong>：<strong>全域数据整合</strong>能力最强。销售云+服务云覆盖售前售后，自定义对象/字段支持复杂业务（如“医疗设备的售后维护记录”）。适合<strong>大型跨国企业</strong>（多渠道数据需要统一管理）。</li></ul><h3>1.3 客户信息管理的工作流（Mermaid时序图）</h3><pre><code>sequenceDiagram
    participant 多渠道 as 多渠道获客（百度/抖音/官网/微信）
    participant 超兔 as 超兔一体云
    participant 工商数据 as 工商/天眼查
    participant 销售 as 销售团队
    多渠道-&gt;&gt;超兔: 线索录入（电子表单/拍名片/通讯录）
    超兔-&gt;&gt;超兔: 自定义查重（客户名/手机号+简称模糊）
    超兔-&gt;&gt;工商数据: 自动查询企业背景
    工商数据-&gt;&gt;超兔: 返回注册地址/经纬度/法人信息
    超兔-&gt;&gt;销售: 呈现360°画像（基本信息+工商+历史互动）</code></pre><h2>二、销售跟踪与待办：从“流程管控”到“智能提效”的差异</h2><p>销售跟踪的核心是<strong>让销售“做对的事”</strong> ——通过流程设计减少遗漏，通过智能辅助提升效率。</p><h3>2.1 核心子指标对比</h3><table><thead><tr><th>子指标</th><th>超兔一体云</th><th>红圈营销</th><th>六度人和（EC）</th><th>Salesforce</th><th>SugarCRM</th><th>Freshsales</th><th>Pipedrive</th></tr></thead><tbody><tr><td><strong>流程覆盖</strong></td><td>线索-商机-项目-回款（小单/中长单/项目）</td><td>线索-商机-合同-回款（销售漏斗）</td><td>获客-转化-复购（AI电销+行业模板）</td><td>线索-商机-合同-回款（工作流自动化）</td><td>线索-商机-订单-回款</td><td>线索-商机-合同-回款（AI线索评分）</td><td>线索-商机-管道-回款（拖拽式管道）</td></tr><tr><td><strong>跟单模型</strong></td><td>三一客（小单快单）+商机阶段（中长单）+多方项目（全周期）</td><td>通用销售漏斗（7阶段）</td><td>外贸（海关数据选客）+教育（联动学邦ERP）</td><td>标准销售流程+自定义阶段</td><td>通用流程</td><td>无明确模型</td><td>拖拽式销售管道（如“初期沟通→合同签约”）</td></tr><tr><td><strong>AI辅助</strong></td><td>AI待办生成（行动记录触发）+行动分析</td><td>无明确AI功能</td><td>AI电销（优化线路）+商机价值评估</td><td>AI预测（如“商机转化率”）+流程触发</td><td>无明确AI功能</td><td>AI线索评分（优先级排序）</td><td>无明确AI功能</td></tr><tr><td><strong>外勤支持</strong></td><td>外勤拜访记录（关联客户位置）</td><td>LBS定位+轨迹跟踪+现场订单录入</td><td>无明确说明</td><td>无明确说明</td><td>无明确说明</td><td>无明确说明</td><td>无明确说明</td></tr><tr><td><strong>待办自动化</strong></td><td>行动记录自动生成待办（如“向客户发报价→3天内跟进”）</td><td>节点提醒（如“商机进入‘立项评估’需跟进”）</td><td>AI生成待办（如“某客户30天未复购→提醒复购”）</td><td>工作流自动化（如“合同审批通过→生成回款待办”）</td><td>手动设置+邮件提醒</td><td>智能排序（高价值线索优先）</td><td>逾期任务警示+日历同步</td></tr></tbody></table><h3>2.2 核心差异：“场景定制” vs “通用流程”</h3><ul><li><p><strong>超兔一体云</strong>：<strong>场景化跟单模型</strong>是绝对优势。</p><ul><li>小单快单用“三一客”：通过“三定”（定性、定级、定量）快速推进（如“定性为‘有需求’，定级为‘A类’，定量为‘10万订单’”）；</li><li>中长单用“商机阶段”：跟踪“初期沟通→立项评估→合同签约”的关键节点；</li><li>复杂项目用“多方项目模型” <strong>：在一个视图内管理项目组、合同、采购、收支，控制利润（如“某工程公司的‘办公楼装修项目’，同步合同金额、采购成本、收款进度”）。</strong> <strong>适合</strong>“小单+项目”混合的企业（如IT集成、设备销售）。</li></ul></li><li><strong>红圈营销</strong>：<strong>外勤与流程闭环</strong>是重点。LBS定位记录销售轨迹，现场录入订单，解决“外勤人员管理难”痛点；销售漏斗可视化（如“初期沟通有10个商机，转化为5个立项”），适合<strong>快消</strong> <strong>、农牧等线下重执行的行业</strong>（如“饮料经销商的终端门店巡店”）。</li><li><strong>六度人和（</strong> <strong>EC</strong> <strong>）</strong> ：<strong>AI+</strong> <strong>行业定制</strong>是特色。AI电销优化线路（如“优先拨打高意向客户”），外贸模板整合海关数据（如“筛选‘近3个月进口过电子产品的客户’”），教育模板联动学邦ERP（如“招生线索同步到教务系统”）。适合<strong>行业属性强的企业</strong>（如外贸、教育）。</li></ul><h3>2.3 销售跟踪的脑图（Mermaid）</h3><pre><code>mindmap
    root((销售跟踪与待办))
        流程覆盖
            线索分配
            商机挖掘
            合同审批
            销售回款
        跟单模型
            小单快单（超兔：三一客）
            中长单（超兔：商机阶段）
            多方项目（超兔：全周期管理）
            行业定制（EC：外贸/教育）
        AI辅助
            待办生成（超兔/EC）
            商机评分（Freshsales）
            电销优化（EC）
        外勤支持
            LBS定位（红圈）
            现场录入（红圈）
        待办自动化
            行动触发（超兔）
            流程驱动（Salesforce）
            逾期提醒（Pipedrive）</code></pre><h2>三、报表与分析：从“数据呈现”到“决策驱动”的进化</h2><p>报表与分析的核心是<strong>将数据转化为可执行的策略</strong>，关键看“可视化能力、分析模型、AI驱动”。</p><h3>3.1 核心子指标对比</h3><table><thead><tr><th>子指标</th><th>超兔一体云</th><th>红圈营销</th><th>六度人和（EC）</th><th>Salesforce</th><th>SugarCRM</th><th>Freshsales</th><th>Pipedrive</th></tr></thead><tbody><tr><td><strong>可视化能力</strong></td><td>数字卡片+销售漏斗+RFM图表</td><td>仪表盘+销售漏斗+满意度曲线</td><td>数字大屏+多维度报表（如“获客来源占比”）</td><td>Data Cloud+实时仪表盘</td><td>内置报表+图表工具</td><td>实时仪表盘+销售漏斗</td><td>销售预测报表+绩效排行</td></tr><tr><td><strong>分析模型</strong></td><td>RFM（客户价值分类）+销售漏斗</td><td>销售漏斗+满意度分析</td><td>销售漏斗+客户转化路径</td><td>销售漏斗+AI预测模型</td><td>销售绩效+订单分析</td><td>销售漏斗+转化率分析</td><td>销售预测+管道分析</td></tr><tr><td><strong>AI驱动</strong></td><td>智能日报（自动汇总当日数据）+复购预警</td><td>智能建议（如“服务响应延迟需优化”）</td><td>海关数据拓客+交叉销售建议</td><td>AI预测（如“下月销售额”）+Data Cloud</td><td>无明确说明</td><td>实时数据更新+趋势分析</td><td>无明确说明</td></tr><tr><td><strong>自定义报表</strong></td><td>支持</td><td>支持</td><td>支持</td><td>支持</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td><strong>跨链路闭环</strong></td><td>客户-行动-待办-业绩</td><td>线索-订单-服务-满意度</td><td>获客-转化-复购</td><td>多渠道数据闭环</td><td>无明确说明</td><td>线索-交易-行为</td><td>管道-交易</td></tr></tbody></table><h3>3.2 深度解读：谁的“分析”真正“驱动决策”？</h3><ul><li><p><strong>超兔一体云</strong>：<strong>RFM</strong> <strong>分析+智能日报</strong>解决“日常复盘”痛点。</p><ul><li>RFM模型：通过“最近消费、消费频率、消费金额”将客户分为“价值客户”“挽留客户”（如“最近30天消费、月均2次、累计10万→价值客户”），针对性制定复购策略；</li><li>智能日报：自动汇总当日数据（如“签约15.8万、新增2个客户”），支持主观填写“今日问题”，帮助销售快速复盘，老板实时掌握进度。适合<strong>注重客户复购的企业</strong>（如 SaaS、消费品）。</li></ul></li><li><strong>六度人和（</strong> <strong>EC</strong> <strong>）</strong> ：<strong>大数据+数字大屏</strong>是亮点。数字大屏实时展示“获客来源占比、转化率、复购率”，结合90亿+海关数据生成“拓客建议”（如“某外贸企业的‘潜在客户’是‘近6个月进口过五金的美国企业’”），某银行用其<strong>交叉销售率提升42%</strong> 。适合<strong>需要大数据支撑决策的企业</strong>（如金融、外贸）。</li><li><strong>红圈营销</strong>：<strong>满意度分析</strong>是特色。通过问卷收集客户反馈（如“对服务响应速度的评分”），生成“满意度曲线”，识别“服务薄弱环节”（如“售后响应时间超过24小时”）。适合<strong>服务导向的企业</strong>（如医疗、家政）。</li></ul><h2>四、多端同步：从“数据一致”到“生态协同”的能力</h2><p>多端同步的核心是<strong>让销售在任何场景下都能访问最新数据</strong>，关键看“端覆盖、实时同步、生态兼容”。</p><h3>4.1 核心子指标对比</h3><table><thead><tr><th>子指标</th><th>超兔一体云</th><th>红圈营销</th><th>六度人和（EC）</th><th>Salesforce</th><th>SugarCRM</th><th>Freshsales</th><th>Pipedrive</th></tr></thead><tbody><tr><td><strong>端覆盖</strong></td><td>Web/App/小程序/RPA插件</td><td>Web/APP（iOS/Android）</td><td>Web/App/企业微信/视频号</td><td>Web/App/阿里云/微信</td><td>Web/App（iOS/Android）</td><td>Web/App/Freshworks生态</td><td>Web/App/Chrome插件</td></tr><tr><td><strong>实时同步</strong></td><td>支持（App记录的拜访同步到Web）</td><td>支持（APP与电脑端同步）</td><td>支持（企业微信消息同步到EC）</td><td>支持（多渠道数据实时更新）</td><td>支持</td><td>支持</td><td>支持</td></tr><tr><td><strong>离线支持</strong></td><td>无明确说明</td><td>支持（离线录入后联网同步）</td><td>无明确说明</td><td>无明确说明</td><td>支持</td><td>无明确说明</td><td>无明确说明</td></tr><tr><td><strong>生态兼容</strong></td><td>RPA插件（自动化录入）+自定义集成</td><td>无明确说明</td><td>企业微信/腾讯会议/视频号</td><td>阿里云/微信/第三方应用（如Slack）</td><td>第三方应用集成（如Mailchimp）</td><td>Freshdesk（售后）/Freshservice</td><td>无明确说明</td></tr></tbody></table><h3>4.2 核心差异：“多端覆盖” vs “生态深度”</h3><ul><li><strong>超兔一体云</strong>：<strong>多端全场景覆盖</strong>。Web端适合后台管理，App端适合外勤（记录拜访、查看待办），小程序适合客户互动（如“客户查看报价单”），RPA插件自动化录入（如“自动导入Excel线索”）。适合<strong>需要自动化提效的企业</strong>（如电商、制造业）。</li><li><strong>六度人和（EC）</strong> ：<strong>腾讯生态深度绑定</strong>是壁垒。打通企业微信、腾讯会议、视频号，实现“直播获客→企业微信跟进→EC转化”闭环（如“视频号直播的线索同步到EC，自动生成‘跟进待办’”）。适合<strong>依赖微信生态的企业</strong>（如零售、教育）。</li><li><strong>红圈营销</strong>：<strong>离线支持</strong>解决外勤痛点。线下无网络时，销售可录入“拜访记录”“订单”，联网后自动同步，避免“数据丢失”。适合<strong>户外作业多的企业</strong>（如快消、农牧）。</li></ul><h2>五、综合能力雷达图与选型建议</h2><h3>5.1 各品牌雷达图评分（10分制）</h3><table><thead><tr><th>维度</th><th>超兔一体云</th><th>红圈营销</th><th>六度人和（EC）</th><th>Salesforce</th><th>SugarCRM</th><th>Freshsales</th><th>Pipedrive</th></tr></thead><tbody><tr><td>客户信息管理</td><td>8.5</td><td>8.0</td><td>8.8</td><td>9.0</td><td>7.5</td><td>8.2</td><td>7.8</td></tr><tr><td>销售跟踪与待办</td><td>9.0</td><td>8.2</td><td>8.5</td><td>8.8</td><td>7.6</td><td>8.3</td><td>8.0</td></tr><tr><td>报表与分析</td><td>8.5</td><td>8.0</td><td>9.0</td><td>8.9</td><td>7.4</td><td>8.1</td><td>7.9</td></tr><tr><td>多端同步</td><td>8.8</td><td>8.1</td><td>9.2</td><td>8.5</td><td>7.7</td><td>8.4</td><td>8.0</td></tr></tbody></table><h3>5.2 选型建议</h3><table><thead><tr><th>企业类型</th><th>推荐品牌</th><th>核心原因</th></tr></thead><tbody><tr><td>B 端企业（需要精准的企业客户背景）</td><td>超兔一体云</td><td>工商数据补全+自定义查重是核心亮点，能通过百度/天眼查自动补充企业背景等信息，支持“简称模糊查重”，避免重复录入，适合需要精准企业客户信息的 B 端企业。</td></tr><tr><td>依赖微信生态的企业（如教育、零售）</td><td>六度人和（EC）</td><td>腾讯生态深度整合是壁垒，打通企业微信、邮件、电话数据，结合工商/海关数据生成“精准画像”，解决“私域数据分散”问题，还能实现“直播获客→企业微信跟进→EC 转化”闭环。</td></tr><tr><td>大型跨国企业（多渠道数据需要统一管理）</td><td>Salesforce</td><td>全域数据整合能力最强，销售云+服务云覆盖售前售后，自定义对象/字段支持复杂业务，可对多渠道数据进行统一管理。</td></tr><tr><td>“小单+项目”混合的企业（如 IT 集成、设备销售）</td><td>超兔一体云</td><td>场景化跟单模型是绝对优势，小单快单用“三一客”、中长单用“商机阶段”、复杂项目用“多方项目模型”，能满足不同业务场景需求。</td></tr><tr><td>快消、农牧等线下重执行的行业</td><td>红圈营销</td><td>外勤与流程闭环是重点，LBS 定位记录销售轨迹，现场录入订单，解决“外勤人员管理难”痛点，销售漏斗可视化便于管理。</td></tr><tr><td>行业属性强的企业（如外贸、教育）</td><td>六度人和（EC）</td><td>AI+行业定制是特色，AI 电销优化线路，外贸模板整合海关数据，教育模板联动学邦 ERP ，能满足不同行业的特定需求。</td></tr><tr><td>注重客户复购的企业（如 SaaS、消费品）</td><td>超兔一体云</td><td>RFM 分析+智能日报解决“日常复盘”痛点，RFM 模型将客户分类，针对性制定复购策略，智能日报自动汇总当日数据，帮助销售复盘和老板掌握进度。</td></tr><tr><td>需要大数据支撑决策的企业（如金融、外贸）</td><td>六度人和（EC）</td><td>大数据+数字大屏是亮点，数字大屏实时展示数据，结合 90 亿+海关数据生成“拓客建议”，能为企业决策提供有力支持。</td></tr><tr><td>服务导向的企业（如医疗、家政）</td><td>红圈营销</td><td>满意度分析是特色，通过问卷收集客户反馈，生成“满意度曲线”，识别“服务薄弱环节”，有助于提升服务质量。</td></tr><tr><td>需要自动化提效的企业（如电商、制造业）</td><td>超兔一体云</td><td>多端全场景覆盖，Web 端适合后台管理，App 端适合外勤，小程序适合客户互动，RPA 插件自动化录入，可提高企业工作效率。</td></tr><tr><td>户外作业多的企业（如快消、农牧）</td><td>红圈营销</td><td>离线支持解决外勤痛点，线下无网络时销售可录入数据，联网后自动同步，避免“数据丢失”。</td></tr></tbody></table><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[多模态数据中台为什么说是被“逼出来”的？ 袋鼠云数栈 ]]></title>    <link>https://segmentfault.com/a/1190000047496321</link>    <guid>https://segmentfault.com/a/1190000047496321</guid>    <pubDate>2025-12-23 12:01:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025年算是Agent元年，回望这一年，我们听到最多的企业内部需求，大概是这样的：</p><p>“能不能做个智能体，让业务同事直接问？”</p><p>“我们也想上大模型，把知识库、报表全串起来。”</p><p>“视频、IoT、日志、告警都在，就是没人能‘一口气’看完一件事。”</p><p>再往下问一句：</p><p>你们是不是已经有一套还算完整的数据中台了？</p><p>——有的，数仓有了，指标有了，报表有了，数据中台也上线好多年了。</p><p>问题就出在这里：看上去“齐活”的数据基础设施，遇到 AI 项目的时候，却越来越“不顺手”。</p><p>这不是某一家企业的特例，而是很多头部企业在今年共同遇到的现实。</p><p>多模态数据中台，不是哪个厂商的新品类，而是被这些现实一点点“逼”出来的结果。</p><h3>一、项目一个接一个，为什么“好用的 AI”依然很少？</h3><p>如果把技术名词先放一边，只看业务一线在日常中遇到的那些烦恼，基本可以归结为三类。</p><h4>第一类：问题本身，已经不是几张表能说清的了。</h4><p>投诉率上升，是产品问题，还是服务问题，还是运营动作出了偏差？</p><p>一条产线良率波动，是设备状态变差了，还是某个班组操作习惯改了，抑或原材料批次有差异？</p><p>一个片区的安全事件频率变高，是人流结构变化了，还是设施老化，还是历史事件叠加影响？</p><p>这些问题的线索，散落在录音文本、服务记录、工单流转、IoT 时序曲线、告警日志、监控视频、轨迹数据、甚至三维场景里。但在大多数企业的系统版图里，它们是这样的：</p><p>表格在数仓和中台；</p><p>日志在监控系统；</p><p>IoT 在边缘平台；</p><p>文档在知识库；</p><p>视频在安防系统或第三方云。</p><p>一个问题，天然跨多个系统。而传统数据中台，更多还是围绕结构化数据在组织世界。</p><h4>第二类：数据“都在”，但真正用起来总有点别扭。</h4><p>你去问数据团队，大概率会得到这样的回答：</p><p>“有，这个字段在某某表；那个数据要从日志库拉；</p><p>视频要去另外一个平台查；IoT 在边缘那边。”</p><p>于是，每做一个新的 AI 场景——智能客服、智能质检、预测性维护、智能风控、风险研判、智能问数与归因分析……都要从头打一条“专用数据链路”：</p><p>从五六个系统里各拎一段，再临时拼在一起。</p><p>做好一个项目不难，难的是：</p><p>几个项目之后，大家发现手上有的是一堆“各自为战”的小链路，而不是一块越来越厚的数据地基。</p><h4>第三类：智能总是“停在屏幕里”，很难进入日常工作。</h4><p>企业对 AI 的期待，其实很朴素：</p><p>业务能直接问；</p><p>不用切十个系统；</p><p>能看懂“为什么是这个结论”；</p><p>能把智能变成流程的一部分，而不是一次演示。</p><p>现实却往往是：</p><p>报表在一套系统，知识问答在一套系统，流程机器人在另一套系统；</p><p>每个“智能助手”背后连的是不同的数据源、不同的口径，能力不统一，体验也割裂。</p><p>久而久之，管理层会问一句：</p><p>我们到底是“有很多 AI 项目”，还是“真的多了一个和我们一起工作的智能同事”？</p><h3>二、沿着这些烦恼往下拆，企业真正缺的是什么？</h3><p>如果不从“我要不要上一个多模态数据中台”出发，而是老老实实从用户的烦恼往下拆，很快会发现：企业想要的，不是更多的“点状应用”，而是一块可复用的 Data+AI 地基。这块地基，要帮企业做三件事。</p><h4>第一件事：让业务问题可以被完整地“摊开”。</h4><p>“投诉率为什么上升”这个问题，背后需要的，不只是投诉表和订单表；“良率为什么波动”，需要的不只是工单和生产记录。如果企业内部没有一处地方，能围绕“客户、设备、订单、工单、门店、产线、场站”等对象，把结构化数据、日志、文本、音视频、时序、空间位置这些信息收拢在一起，那么任何一个 AI 模型，看到的都只是一块块碎片。</p><p>你很难指望一个模型，在看不到录音和工单的前提下，对投诉原因做出什么可靠判断。你也很难指望一个模型，在看不到工艺曲线和设备历史后，对良率问题给出负责任的建议。换句话说，如果问题本身是多模态的，数据视图却还是单模态的，那么所有智能，天然就被“打了折扣”。</p><h4>第二件事：让数据治理这件事，不再是“一次性的体力活”。</h4><p>数据团队并不怕辛苦做一遍数据治理，怕的是永远在做同一件事：</p><p>为了客服智能质检，清洗了一轮录音与文本；</p><p>为了质检优化，又清洗了一轮生产日志和视频；</p><p>为了城市治理，又清洗了一轮时空轨迹与告警事件。</p><p>每一轮都像是新品类，缺乏统一的标准和资产视角，项目一结束，这些成果就“躺”在各自专用库里，很难被后续场景继承。如果有一块工具，能把结构化数据、日志、文档、音视频、IoT、时空数据，以及 Embedding、标签这类“AI 产物”，统统纳入统一的元数据、血缘、权限和质量框架之中；</p><p>那每一次项目做完，企业的数据家底都会实实在在“厚”一截，而不是只多一套 Demo。</p><h4>第三件事：让智能有机会变成“默认存在”，而不是“偶尔出现”。</h4><p>所谓“默认存在”，就是：业务习惯了把问题直接抛给一个入口；</p><p>这个入口背后，能够自动把相关的数据、文档、案例、场景调度起来；</p><p>智能体给出的结论，可以被追问“为什么”，也可以在事后通过效果数据验证“对不对”。</p><p>要做到这一点，底层工具必须天然考虑：模型和智能体不是“外部调用者”，而是它的核心用户之一。它要提供的不只是数据查询接口，而是一整套为 AI 准备好的特征服务、向量服务、语义检索、算子编排能力。说到底，企业缺的不是一个“下一代中台”的名字，而是这么一块东西：</p><p>能看懂业务问题是多模态的，能让数据治理变成一件长期有复利的事，能让 AI 站在它上面，成为日常工作的一部分。叫“地基”也好，叫“操作系统”也好，本质就是这三件事。</p><h3>三、顺着这条线往下走，那块地基自然会长成“多模态数据中台”</h3><p>现在再回头看传统数据中台，它的价值并没有消失，只是边界暴露得越来越清晰。对结构化世界来说，它做得已经够好：</p><p>帮企业把“谁、在哪、做了什么、结果如何”这样的数字，收拢、建模、标准化，形成可被全公司共享的指标体系和分析框架。</p><p>问题在于，业务世界已经不只是一堆数字了。客户在通话里的情绪、工程师在工单里的备注、生产线上某段视频里的异常动作、某个设备在空间里的位置变化、某条路线的拥堵模式……这些东西，越来越频繁地出现在“关键问题”的证据链条上。</p><p>如果企业的地基仍然只为结构化数据设计，那么所有这些多模态线索，要么被挂在附件里，要么孤立在专用平台中，AI 想用，就得一次次“单飞”。沿着用户的问题继续往下走，能看到一个更清晰的图景：</p><p>在建模视角上，那块地基必须把“对象、事件、时空关系”当成核心事项，而不是只把表和字段当成世界的基本单元；</p><p>在治理视角上，它必须同时管理结构化、非结构化、时空数据和 AI 产物，让任何一条智能链路都可以被追溯、被评估；</p><p>在服务视角上，它必须把模型和智能体视为首要服务对象，提供适合它们的特征、向量、语义与编排能力。</p><p>当这些要求放在一起，你自然会得到这样一个结论：这块地基，长出来的样子，很像一个“多模态数据智能中台”。它不是简单地“加几种数据源”，而是回答了三个问题：</p><p>业务问题能不能被完整描述？</p><p>数据资产能不能持续积累？</p><p>AI 能不能站在这块地基上迭代，而不是每次旁路重造？</p><p>名字可以有很多，逻辑只有一个。从这个意义上讲，多模态数据中台并不是某个厂商的新故事，而是企业在 Data+AI 实践中，被一次次逼出来的共同答案。</p><h3>四、袋鼠云为什么会走到这一步？</h3><p>这时候再看袋鼠云，就会发现它的那条路其实并不复杂：</p><p>从大数据基础软件起步，先帮企业把结构化的那部分地基打牢；</p><p>随着客户在 AI、智能体、数字孪生等业务场景上的需求越来越多，逐渐意识到：</p><p>如果底层不能理解多模态，不能为 AI 提供原生的数据供给，再强的上层能力都会显得吃力。</p><p>于是，“数栈”开始往多模态方向延展：</p><p>一头兼容日志、文档、图像、音视频、IoT、时空数据，把它们纳入统一的元数据和治理框架；</p><p>一头与智能指标平台、智能体开发应用平台、空间智能产品家族打通，让智能问数、业务 Agent、数字孪生、空间智能、场景推演这些东西，都能站在同一块地基上生长。</p><p>你可以把它看成一个厂商的产品演进；也可以把它看成，是一批真实项目累积之后，对“那块缺席的地基”做的一次系统回应。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047496323" alt="图片" title="图片"/></p><h3>先承认世界是多模态的，再谈怎么做 AI</h3><p>回头看过去几年，很多企业在 AI 上的挫折感，其实来自一个很简单的错位：</p><p>业务提出的问题，天然是多模态的；</p><p>引进的模型，天然能处理多模态；</p><p>唯独数据基础设施，仍然是“表格世界”的设计。</p><p>在这样的前提下，再谈“知识问答”“智能体”“世界模型”，多少都有点“离地”。</p><p>所以，也许更现实的顺序是：</p><p>先承认业务世界就是多模态的；</p><p>再承认现有的数据地基对这件事准备不足；</p><p>然后，再思考一块怎样的基础设施，能把这两头重新接起来。</p><p>多模态数据中台，恰好是对这三个判断的同一个回答。它不是终点，只是一块起点——但如果没有这块起点，后面的 Data+AI，很难真正走得远。</p>]]></description></item><item>    <title><![CDATA[技术分享 | MySQL间隙锁原理深度详解 墨天轮 ]]></title>    <link>https://segmentfault.com/a/1190000047495731</link>    <guid>https://segmentfault.com/a/1190000047495731</guid>    <pubDate>2025-12-23 11:03:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文为<a href="https://link.segmentfault.com/?enc=pFVRseFSkUaQ2GsurHNeKw%3D%3D.RzZ%2BlUL9%2BuJqy%2FS7MLJHklDeZC%2FkYdZ1ALlN3QIlQa2RSNTx%2Bw06rsBRt8Q6kOAl" rel="nofollow" target="_blank">墨天轮数据库管理服务团队</a>第150期技术分享，内容原创，作者为技术顾问<strong>陈洋</strong>，如需转载请联系小墨（VX：modb666）并注明来源。如需查看更多文章可关注【墨天轮】公众号。</p><h2><strong>一、间隙锁概述</strong></h2><p>间隙锁（Gap Lock）是InnoDB存储引擎在<code>REPEATABLE READ</code>（可重复读）隔离级别下为了解决幻读（Phantom Read）问题而引入的一种锁机制。它锁定的是索引记录之间的“间隙”，而不是实际存在的记录。这意味着，即使间隙中没有数据，间隙锁也能阻止其他事务在该间隙内插入新的数据，从而保证了在同一事务中多次读取相同范围的数据时，结果集保持一致。</p><h2><strong>二、幻读问题</strong></h2><p>幻读是指在同一个事务中，两次执行相同的查询语句，但第二次查询却看到了第一次查询没</p><ol><li>事务A在某个范围内执行了查询。</li><li>事务B在该范围内插入了新的行并提交。</li><li>事务A再次执行相同的查询，看到了事务B插入的新行，导致前后两次查询结果不一致。</li></ol><p>在<code>REPEATABLE READ</code>隔离级别下，MySQL通过两种方式解决幻读问题：</p><ul><li><strong>快照读（Snapshot Read）</strong>：对于普通的<code>SELECT</code>语句，InnoDB通过MVCC（多版本并发控制）机制，在事务开始时生成一个Read View（一致性视图），后续的快照读都基于这个视图，因此不会看到其他事务提交的新数据。这种方式解决了普通<code>SELECT</code>语句的幻读。</li><li><strong>当前读（Current Read）</strong>：对于<code>SELECT ... FOR UPDATE</code>、<code>SELECT ... LOCK IN SHARE MODE</code>、<code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code>等语句，它们需要读取最新的数据版本，因此称为当前读。在当前读情况下，MVCC无法解决幻读问题，此时就需要间隙锁来防止其他事务插入新数据。</li></ul><h2><strong>三、间隙锁的实现原理</strong></h2><p>间隙锁是基于索引的，它锁定的是索引记录之间的空隙。当对某个范围的数据进行当前读操作时，InnoDB不仅会锁定符合条件的记录本身（记录锁），还会锁定这些记录前后的间隙，以及第一个记录之前的间隙和最后一个记录之后的间隙。这种记录锁和间隙锁的组合被称为<code>Next-Key Lock</code>。</p><p><strong>3.1 Next-Key Lock</strong></p><p><code>Next-Key Lock</code>是InnoDB默认的行锁类型，它结合了记录锁（Record Lock）和间隙锁（Gap Lock）。一个<code>Next-Key Lock</code>会锁定一个索引记录以及该记录之前的间隙。其锁定范围是<code>(前一个索引记录, 当前索引记录]</code>。</p><p>例如，在一个索引包含值10、20、30的表中，<code>Next-Key Lock</code>可能锁定的区间包括：</p><ul><li><code>(-∞, 10]</code></li><li><code>(10, 20]</code></li><li><code>(20, 30]</code></li><li><code>(30, +∞)</code></li></ul><p><strong>3.2 间隙锁的特性</strong></p><ul><li><strong>只在<code>REPEATABLE READ</code>隔离级别下生效</strong>：在<code>READ COMMITTED</code>（读已提交）隔离级别下，没有间隙锁，因此可能会出现幻读。</li><li><strong>不区分共享锁和排他锁</strong>：间隙锁的唯一目的是防止其他事务插入数据，因此它不区分共享（S）锁和排他（X）锁。任何事务持有间隙锁，都会阻止其他事务在该间隙内插入数据。</li><li><strong>间隙锁之间不冲突</strong>：不同事务可以同时持有同一个间隙的间隙锁。因为间隙锁的目的是阻止插入，而不是阻止读取或修改已存在的数据。</li><li><strong>与索引相关</strong>：间隙锁是加在索引上的，而不是数据行本身。如果查询没有使用索引，或者使用的索引不能有效地限制扫描范围，间隙锁可能会锁定整个表，导致并发性能下降。</li></ul><h2><strong>四、间隙锁的加锁规则</strong></h2><p>间隙锁的加锁规则相对复杂，主要取决于查询条件、索引类型以及是否是唯一索引：</p><ol><li><strong>等值查询与唯一索引</strong>：</li></ol><ul><li>如果查询条件是唯一索引的等值查询，并且找到了对应的记录，那么<code>Next-Key Lock</code>会退化为记录锁，只锁定该行，不会产生间隙锁。因为唯一索引保证了该值是唯一的，不会有新的数据插入到该位置。</li><li>如果查询条件是唯一索引的等值查询，但没有找到对应的记录，那么会在不存在的记录位置形成一个间隙锁，锁定该间隙，防止其他事务插入该值。</li></ul><ol start="2"><li><strong>等值查询与非唯一索引</strong>：</li></ol><ul><li>如果查询条件是非唯一索引的等值查询，无论是否找到记录，都会在扫描到的符合条件的记录以及其前后的间隙上加<code>Next-Key Lock</code>。这是因为非唯一索引可能存在多个相同的值，需要锁定一个范围来防止幻读。</li></ul><ol start="3"><li><strong>范围查询</strong>：</li></ol><ul><li>对于范围查询（如<code>WHERE id &gt; 10</code>或<code>WHERE id BETWEEN 10 AND 20</code>），无论是否是唯一索引，都会在扫描到的所有符合条件的记录及其前后的间隙上加<code>Next-Key Lock</code>。扫描会持续到第一个不满足条件的记录，并锁定该记录之前的间隙。</li></ul><ol start="4"><li><strong>无索引或索引失效</strong>：</li></ol><ul><li>如果查询没有使用索引，或者索引失效，那么InnoDB会进行全表扫描。在这种情况下，为了防止幻读，InnoDB会给整个表的所有索引记录都加上<code>Next-Key Lock</code>，这会严重影响并发性能。</li></ul><h2><strong>五、间隙锁的示例</strong></h2><p>假设有一个<code>products</code>表，其中包含<code>id</code>（主键）、<code>name</code>和<code>price</code>字段，并且<code>id</code>是自增主键。</p><pre><code class="sql">CREATE TABLE `products` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL,
  `price` decimal(10,2) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
INSERT INTO `products` (`id`, `name`, `price`) VALUES
(1, 'Laptop', 1200.00),
(5, 'Mouse', 25.00),
(10, 'Keyboard', 75.00);</code></pre><p>当前<code>products</code>表中的<code>id</code>值有1, 5, 10。那么存在的间隙包括：</p><ul><li><code>(-∞, 1]</code></li><li><code>(1, 5]</code></li><li><code>(5, 10]</code></li><li><code>(10, +∞)</code></li></ul><h3>示例1：等值查询（唯一索引，找到记录）</h3><p><strong>事务A:</strong></p><pre><code class="sql">BEGIN;
SELECT * FROM products WHERE id = 5 FOR UPDATE;</code></pre><p><strong>分析</strong>：<code>id</code>是主键（唯一索引），查询条件是等值查询且找到了记录。此时，<code>Next-Key Lock</code>会退化为记录锁，只锁定<code>id = 5</code>的行。其他事务可以插入<code>id = 2</code>或<code>id = 7</code>的记录，但不能修改或删除<code>id = 5</code>的记录。</p><h3>示例2：等值查询（唯一索引，未找到记录）</h3><p><strong>事务A:</strong></p><pre><code class="sql">BEGIN;
SELECT * FROM products WHERE id = 3 FOR UPDATE;</code></pre><p>分析：<code>id</code>是主键（唯一索引），查询条件是等值查询但未找到记录。此时，会在<code>(1, 5]</code>这个间隙上加间隙锁，阻止其他事务插入<code>id</code>为2、3、4的记录。例如，事务B尝试插入<code>id = 2</code>的记录会被阻塞。</p><h3>示例3：范围查询</h3><p><strong>事务A:</strong></p><pre><code class="sql">BEGIN;
SELECT * FROM products WHERE id &gt; 5 FOR UPDATE;</code></pre><p><strong>分析</strong>：查询条件是范围查询。InnoDB会扫描<code>id = 10</code>的记录，并锁定<code>(5, 10]</code>和<code>(10, +∞)</code>这两个间隙。这意味着，其他事务不能插入<code>id</code>为6、7、8、9的记录，也不能插入<code>id</code>大于10的记录。同时，<code>id = 10</code>的记录本身也会被锁定。</p><h2><strong>六、间隙锁的优缺点</strong></h2><p><strong>优点</strong>：</p><ul><li>解决幻读：在<code>REPEATABLE READ</code>隔离级别下，间隙锁有效地防止了幻读的发生，保证了数据的一致性。</li></ul><p><strong>缺点</strong>：</p><ul><li>降低并发性：间隙锁锁定的不是具体的行，而是索引的范围，这可能导致不必要的锁定，从而降低了数据库的并发性能。即使没有数据，间隙也会被锁定。</li><li>死锁风险：间隙锁增加了死锁的风险，因为多个事务可能在不同的间隙上持有锁，并尝试获取对方持有的间隙上的锁，从而形成死锁。</li><li>难以理解和排查：间隙锁的加锁规则相对复杂，使得在出现性能问题或死锁时，排查和定位问题变得更加困难。</li></ul><h2><strong>七、总结</strong></h2><p>间隙锁是MySQL InnoDB存储引擎在<code>REPEATABLE READ</code>隔离级别下解决幻读问题的关键机制。理解其原理、加锁规则以及优缺点对于数据库性能优化和问题排查至关重要。在实际应用中，应根据业务需求和并发量，合理选择事务隔离级别，并注意避免因间隙锁导致的性能瓶颈和死锁问题。</p><h2><strong>八、间隙锁的排查与定位</strong></h2><p>在实际的数据库运维和开发中，间隙锁可能导致性能问题甚至死锁。因此，了解如何排查和定位间隙锁是至关重要的。</p><p><strong>8.1 识别间隙锁导致的性能问题</strong></p><ul><li><p><strong>慢查询日志</strong></p><p>：检查MySQL的慢查询日志，特别是那些执行时间长、涉及范围查询且隔离级别为<code>REPEATABLE READ</code>的<code>SELECT ... FOR UPDATE</code>、<code>INSERT</code>、<code>UPDATE</code>、<code>DELETE</code>语句。这些语句很可能触发了间隙锁。</p></li><li><strong><code>SHOW PROCESSLIST</code></strong>：通过<code>SHOW PROCESSLIST</code>命令可以查看当前正在执行的SQL语句。如果发现有大量事务长时间处于<code>Locked</code>或<code>Waiting for table metadata lock</code>状态，并且涉及的SQL语句是范围查询，则可能与间隙锁有关。</li><li><strong><code>information_schema</code>数据库</strong>：<code>information_schema</code>数据库提供了许多关于MySQL服务器状态的信息。以下几个表对于排查锁问题非常有用：</li><li><code>information_schema.INNODB_TRX</code>：显示当前所有正在运行的InnoDB事务的信息，包括事务ID、事务状态、锁等待情况等。</li><li><code>information_schema.INNODB_LOCKS</code>：显示当前被锁定的资源以及持有锁的事务信息。可以查看锁的类型（如<code>RECORD</code>、<code>GAP</code>、<code>AUTO_INC</code>等）和锁定的索引。</li><li><code>information_schema.INNODB_LOCK_WAITS</code>：显示当前存在的锁等待关系，可以帮助识别死锁或长时间的锁等待。</li></ul><p><strong>8.2 使用SHOW ENGINE INNODB STATUS排查</strong></p><p><code>SHOW ENGINE INNODB STATUS</code>命令是排查InnoDB存储引擎问题（包括锁问题）的强大工具。它会输出大量关于InnoDB内部状态的信息，其中<code>LATEST DETECTED DEADLOCK</code>和<code>TRANSACTIONS</code>部分对于分析间隙锁导致的死锁和锁等待尤为重要。</p><p><strong>输出解读要点：</strong></p><ul><li><strong><code>LATEST DETECTED DEADLOCK</code></strong>：如果发生了死锁，这一部分会详细记录最近一次死锁的信息，包括死锁涉及的事务、它们尝试获取的锁、持有的锁以及等待的资源。通过分析这里的信息，可以明确是哪些事务在哪些间隙上发生了死锁。</li><li><strong><code>RECORD LOCKS</code></strong>：表示记录锁。</li><li><strong><code>GAP LOCKS</code></strong>：表示间隙锁。</li><li><strong><code>NEXT-KEY LOCKS</code></strong>：表示临键锁（记录锁+间隙锁）。</li><li><strong><code>TRANSACTIONS</code></strong>：这一部分列出了所有活跃的事务，包括它们的事务ID、状态、执行的SQL语句、持有的锁以及等待的锁。通过查看<code>LOCK WAIT</code>状态的事务，可以找到正在等待锁的事务，并进一步分析其等待的原因。</li><li>查找<code>LOCK WAIT</code>状态的事务。</li><li>查看<code>LOCKED TABLES</code>和<code>WAITING FOR THIS LOCK TO BE GRANTED</code>部分，了解事务正在等待的锁类型和资源。</li><li>结合SQL语句，判断是否是间隙锁导致的等待。</li></ul><p><strong>示例：</strong></p><pre><code class="sql">SHOW ENGINE INNODB STATUS\G</code></pre><p>执行上述命令后，会得到一个详细的报告。你需要仔细阅读其中的<code>LATEST DETECTED DEADLOCK</code>和<code>TRANSACTIONS</code>部分。</p><p><strong>8.3 模拟和复现间隙锁</strong></p><p>为了更好地理解和排查间隙锁，可以在测试环境中模拟和复现间隙锁的场景。这通常涉及：</p><ol><li>设置数据库隔离级别为<code>REPEATABLE READ</code>。</li><li>创建包含索引的测试表。</li><li>开启多个事务，在不同的事务中执行会触发间隙锁的SQL语句（如范围查询的<code>FOR UPDATE</code>语句），并尝试在间隙中插入数据，观察事务的阻塞和死锁情况。</li></ol><p>通过模拟，可以加深对间隙锁行为的理解，并验证排查方法是否有效。</p><p><strong>8.4 避免间隙锁导致的性能问题</strong></p><ul><li><strong>降低隔离级别</strong>：如果业务允许，可以将事务隔离级别从<code>REPEATABLE READ</code>降至<code>READ COMMITTED</code>。在<code>READ COMMITTED</code>隔离级别下，InnoDB不会使用间隙锁，从而避免了幻读和间隙锁带来的性能问题。但需要注意的是，这可能会引入其他并发问题，需要根据业务场景权衡。</li><li><strong>优化SQL语句</strong></li><li>尽量使用等值查询，避免不必要的范围查询。</li><li>确保查询条件能够命中索引，避免全表扫描。全表扫描会导致整个表被间隙锁锁定，严重影响并发。</li><li>对于范围查询，尽量缩小查询范围，减少间隙锁锁定的范围。</li><li><strong>避免不必要的<code>FOR UPDATE</code>或<code>LOCK IN SHARE MODE</code></strong>：只有在确实需要对查询结果进行更新或需要保证数据一致性时，才使用这些语句。</li><li><strong>拆分大事务</strong>：将长时间运行的大事务拆分为多个小事务，减少事务持有锁的时间，从而降低间隙锁冲突的概率。</li><li><strong>使用乐观锁</strong>：对于某些业务场景，可以考虑使用乐观锁（通过版本号或时间戳）来替代悲观锁，减少数据库层面的锁竞争。</li><li><strong>调整索引</strong>：合理设计索引，确保查询能够高效地利用索引，减少不必要的全表扫描或索引扫描。</li></ul><p>通过上述方法，可以有效地排查、定位和避免MySQL间隙锁带来的性能问题和死锁风险。</p><h2><strong>九、间隙锁死锁案例分析与解决方案</strong></h2><p>间隙锁虽然解决了幻读问题，但它引入了死锁的风险。当两个或多个事务在获取间隙锁时形成循环等待，就会发生死锁。以下是一个典型的间隙锁死锁案例及其解决方案。</p><p><strong>9.1 案例场景：并发插入导致的间隙锁死锁</strong></p><p>假设我们有一个<code>orders</code>表，其中包含<code>id</code>（主键）、<code>order_no</code>（唯一索引）和<code>amount</code>字段。为了简化，我们只关注<code>id</code>和<code>order_no</code>。</p><pre><code class="sql">CREATE TABLE `orders` (
  `id` int NOT NULL AUTO_INCREMENT,
  `order_no` varchar(255) UNIQUE,
  `amount` decimal(10,2),
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
INSERT INTO `orders` (`id`, `order_no`, `amount`) VALUES
(1, 'A001', 100.00),
(5, 'A005', 200.00),
(10, 'A010', 300.00);</code></pre><p>当前<code>orders</code>表中的<code>order_no</code>值有’A001’, ‘A005’, ‘A010’。假设现在有两个事务（事务A和事务B）几乎同时尝试插入<code>order_no</code>在’A001’和’A005’之间的记录，例如’A003’和’A004’。</p><p><strong>事务A:</strong></p><pre><code class="sql">-- 事务A
SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN;
-- 步骤1: 事务A尝试插入 'A003'
INSERT INTO orders (order_no, amount) VALUES ('A003', 150.00);</code></pre><p><strong>事务B:</strong></p><pre><code class="sql">-- 事务B
SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;
BEGIN;
-- 步骤1: 事务B尝试插入 'A004'
INSERT INTO orders (order_no, amount) VALUES ('A004', 180.00);</code></pre><p><strong>死锁发生过程：</strong></p><ol><li><strong>事务A执行<code>INSERT ('A003', ...)</code></strong>：</li></ol><ul><li>为了插入’A003’，InnoDB需要检查<code>order_no</code>唯一索引中<code>(A001, A005)</code>这个间隙。事务A会在这个间隙上加一个意向插入锁（Insert Intention Lock），这是一种特殊的间隙锁，表示事务A打算在这个间隙中插入一条记录。同时，为了保证唯一性，它可能还需要对<code>A001</code>和<code>A005</code>这两个记录加S锁或X锁（具体取决于索引类型和操作）。</li></ul><ol start="2"><li><strong>事务B执行<code>INSERT ('A004', ...)</code></strong>：</li></ol><ul><li>几乎同时，事务B也尝试插入’A004’。它也需要检查<code>order_no</code>唯一索引中<code>(A001, A005)</code>这个间隙。事务B也会在这个间隙上加一个意向插入锁。</li></ul><ol start="3"><li><strong>冲突与死锁</strong>：</li></ol><ul><li>虽然意向插入锁之间通常不会直接冲突，但当两个事务都试图在同一个间隙内插入数据时，它们可能会尝试获取间隙内的其他锁（例如，为了检查唯一性而对相邻记录加的锁），或者在内部对间隙进行更细粒度的锁定。在这种情况下，如果事务A持有了间隙<code>(A001, A005)</code>的一部分锁，并等待事务B持有的另一部分锁；同时事务B持有了间隙<code>(A001, A005)</code>的另一部分锁，并等待事务A持有的锁，就会形成循环等待，导致死锁。</li><li>MySQL的死锁检测机制会发现这个循环，并选择其中一个事务作为“牺牲品”（通常是修改行数较少的事务），回滚该事务，从而解除死锁。被回滚的事务会收到<code>ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction</code>错误。</li></ul><p><strong>9.2 解决方案</strong></p><p>针对这种因间隙锁导致的死锁，可以采取以下几种策略：</p><ol><li><strong>降低事务隔离级别</strong>：将事务隔离级别从<code>REPEATABLE READ</code>降至<code>READ COMMITTED</code>。在<code>READ COMMITTED</code>级别下，<code>INSERT</code>操作通常只在插入的行上加行锁，而不会加间隙锁，从而避免了这类死锁。但需要注意的是，<code>READ COMMITTED</code>隔离级别下可能出现幻读（对于快照读），需要根据业务场景权衡。</li><li><strong>优化SQL语句和索引</strong>：</li></ol><ul><li><strong>避免在非唯一索引上进行范围查询的<code>FOR UPDATE</code>或<code>LOCK IN SHARE MODE</code></strong>：如果业务允许，尽量避免在非唯一索引上使用<code>FOR UPDATE</code>或<code>LOCK IN SHARE MODE</code>进行范围查询，因为这会更容易触发间隙锁。</li><li><strong>合理设计唯一索引</strong>：如果<code>order_no</code>是唯一的，并且业务逻辑允许，可以考虑在插入前先进行一次<code>SELECT ... FOR UPDATE</code>来预先锁定范围，但这会降低并发性。</li></ul><ol start="3"><li><strong>应用程序层面处理死锁</strong>：在应用程序代码中捕获死锁异常（错误码1213），并实现事务重试机制。当发生死锁时，回滚当前事务，并等待一小段时间后重新尝试执行事务。这是处理死锁的常见且有效的方法。</li><li><strong>使用自增主键作为插入依据</strong>：如果<code>order_no</code>不是严格递增的，或者其生成逻辑复杂，可以考虑让<code>id</code>（自增主键）作为主要的插入依据，而<code>order_no</code>作为普通唯一索引。在某些情况下，这可以减少间隙锁的冲突。</li><li><strong>批量插入</strong>：如果需要插入大量数据，可以考虑使用批量插入（<code>INSERT INTO ... VALUES (...), (...);</code>）而不是单条插入。批量插入可以减少事务的数量和锁的竞争。</li><li><strong>调整业务逻辑</strong>：重新审视业务逻辑，看是否可以调整操作顺序或数据模型，以减少并发事务对相同间隙的竞争。</li></ol><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000046208374" alt="" title=""/>  </p><p>墨天轮从乐知乐享的数据库技术社区蓄势出发，全面升级，提供多类型数据库管理服务。墨天轮数据库管理服务旨在为用户构建信赖可托付的数据库环境，并为数据库厂商提供中立的生态支持。<br/>墨天轮数据库服务官网：<a href="https://link.segmentfault.com/?enc=2NFB4QTaSwYz%2FQCL3U4HLw%3D%3D.nVIMujpWFzGfX4RNbfd%2B1J7EUCgVSplTtId2U2W01AImxz0u8V9jTulk4w0S18vX" rel="nofollow" target="_blank">https://www.modb.pro/service</a></p>]]></description></item><item>    <title><![CDATA[同城陪玩小程序搭建指南：UniApp+PHP 源码适配 + 定位功能实现 伊伊DK ]]></title>    <link>https://segmentfault.com/a/1190000047495808</link>    <guid>https://segmentfault.com/a/1190000047495808</guid>    <pubDate>2025-12-23 11:02:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>相比普通陪玩平台，同城陪玩小程序的核心优势在于 “地域聚焦”—— 用户可快速找到身边的陪玩师（如同城游戏开黑、线下桌游、运动陪伴、技能教学等场景），解决 “跨城沟通不便”“线下约见难匹配” 的痛点；对创业者而言，同城模式获客成本更低（可深耕本地社群、线下推广），用户粘性更强，盈利转化更高效。<br/>而选择<strong> UniApp+PHP</strong> 技术栈搭建，既能借助 UniApp 跨端优势（一套源码覆盖微信小程序 / 支付宝小程序 / H5），又能依托 PHP 后端的高兼容性，轻松实现同城核心的 “定位匹配” 功能，不用从零开发，降低 80% 搭建门槛。<br/><img width="640" height="910" referrerpolicy="no-referrer" src="/img/bVdmVL2" alt="" title=""/><img width="723" height="654" referrerpolicy="no-referrer" src="/img/bVdmPC3" alt="" title="" loading="lazy"/><br/><strong>一、核心步骤：源码适配 + 定位功能实现（全程实操）</strong><br/>定位功能实现（核心操作，分 3 个关键环节）<br/>同城陪玩的核心是 “精准定位 + 距离匹配”，需通过 “小程序获取坐标→地图 API 解析地址→后端筛选匹配” 实现，具体步骤如下：</p><ol><li>申请地图 API 密钥（以腾讯地图为例）<br/>登录腾讯地图开放平台，注册并创建应用，申请 “微信小程序 JavaScriptAPI v2” 密钥；<br/>配置密钥的 “Referer 白名单”（填写自己的小程序 AppID），确保仅自身小程序可调用该 API。</li><li>前端：获取用户 / 陪玩师地理位置（UniApp 端开发）<br/>调用 UniApp 定位接口：在用户注册 / 登录时，通过 uni.getLocation() 方法获取用户的经纬度坐标（需用户授权 “获取地理位置” 权限）；<br/>陪玩师入驻时定位：陪玩师提交入驻资料页面，添加 “获取当前位置” 按钮，自动获取其经纬度并存储到数据库（后续用于匹配同城用户）；<br/>地址解析：将获取的经纬度通过腾讯地图 API 转换为具体地址（如 “北京市朝阳区 XX 街道”），展示在用户 / 陪玩师个人资料页，提升体验。</li><li>后端：实现同城筛选与距离排序（PHP 端开发）<br/>坐标存储：在用户表、陪玩师表中新增 “latitude（纬度）”“longitude（经度）” 字段，存储前端上传的坐标数据；<br/>距离计算：通过 PHP 编写 “球面距离计算公式”（或调用腾讯地图 API），根据用户坐标与陪玩师坐标，计算两者之间的实际距离（单位：公里）；<br/>同城筛选逻辑：<br/>用户端：在 “同城陪玩” 页面，默认展示 “5 公里内” 的陪玩师，支持手动调整距离范围（10 公里 / 20 公里 / 50 公里）；<br/>排序功能：按 “距离由近及远”“评分由高到低”“价格从低到高” 排序，方便用户快速筛选；<br/>订单关联：用户下单时，自动记录订单的 “同城标识”，后端统计同城订单数据，方便运营分析。<br/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdcACU" alt="" title="" loading="lazy"/><img width="723" height="697" referrerpolicy="no-referrer" src="/img/bVdeT7E" alt="" title="" loading="lazy"/><img width="723" height="247" referrerpolicy="no-referrer" src="/img/bVdmcMZ" alt="" title="" loading="lazy"/></li></ol>]]></description></item><item>    <title><![CDATA[Apache Parquet 优势与日志应用场景解析 东风微鸣云原生 ]]></title>    <link>https://segmentfault.com/a/1190000047495814</link>    <guid>https://segmentfault.com/a/1190000047495814</guid>    <pubDate>2025-12-23 11:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>写作背景</h2><p>近期看了几篇关于日志解决方案的文章, 发现它们都在使用 Apache Parquet 作为存储文件格式. 如下:</p><ul><li><a href="https://link.segmentfault.com/?enc=leh41Q%2Bpvyi2BTRyCTixEg%3D%3D.ZpG0Cqt9kyOnSJgaVnNPJ2WNd%2BmudocJdbq%2B8ZkJIr62BrFAjpkTXKRWHQgUBQUh52MIBie5%2BuIlHCWSQWL4bw%3D%3D" rel="nofollow" target="_blank">Yelp 发布大规模管理 S3 服务器访问日志的方案_架构_InfoQ精选文章</a></li><li><a href="https://link.segmentfault.com/?enc=ki9BJFbrwza59giqzvWfxQ%3D%3D.QR7FVYgeoV2B%2BefNkZrKCLgLi476xpDj2E0QrBpxDYnbfmLYpKOr%2Bu%2BYvF7Lo7%2BA" rel="nofollow" target="_blank">Cloudflare Log Explorer is now GA, providing native observability and forensics</a></li><li><a href="https://link.segmentfault.com/?enc=ioCVYLgODdrF%2FgZxUE1AsA%3D%3D.se%2BhOgHK78gZm6oB%2FpR%2FgdWpenOamd8iKyg9qRvMKb6%2BbB8S7ifgP%2F2hFeeSfmayNf8jSd67A4dtqMFfK6PTJg%3D%3D" rel="nofollow" target="_blank">逆势降本：云上数据平台年复削减30%的治理实践_云计算_吴建阳_InfoQ精选文章</a></li><li><a href="https://link.segmentfault.com/?enc=G47S5H2b5W2lT%2BCXRaKmxQ%3D%3D.VRzdaum4t58D8sf%2F1X95OlgSLQt16lV0pKOPj5S691vMjQhIts0g1jdlQB48%2FOuWljT%2Bs%2FifpENm1LK9u%2FFrFEyWfEZ%2By8hzH3yGQDkLeMIFi5Uv90W54sHZWbO%2F4pzG" rel="nofollow" target="_blank">AWS Debuts a Distributed SQL Database, Amazon S3 Tables for Iceberg - The New Stack</a></li><li><a href="https://link.segmentfault.com/?enc=nu%2Bzzv2XS4NJTNFbgmw%2BVA%3D%3D.JsJ8S6gjtLkTiZ50Ilznmy0%2BigAhWWvk9oMykAlH4ZGUDHinCz4lWRbBi0vZmjvoUQq%2FgjlgLEstx%2BKv6ZIGVG8SNVZfoL8VM28T66WCK7GGPX%2Bw%2FM61ddPR0XuUzagHcfNV3Q994NlsKqAVtFJCfw%3D%3D" rel="nofollow" target="_blank">Grafana Tempo 2.5 release: vParquet4, streaming endpoints, and more metrics | Grafana Labs</a></li><li><a href="https://link.segmentfault.com/?enc=6qPMn%2F4vV6qn5Kw7HENSug%3D%3D.cA6gec4ViEW8%2BWlIRf8smfi6AZVkdMfnKP4yj9BwGuneBVgK1RVuqpWR4LeNliogg8AyjAB8kGQqXG5svq2Po0AEO1%2FswE%2FCumYFJFjBtzo%3D" rel="nofollow" target="_blank">对象存储应用：云原生最新架构 - The New Stack --- Object Store Apps: Cloud Native's Freshest Architecture - The New Stack</a></li></ul><p>这勾起了我的好奇心:</p><ul><li>Apache Parquet 是什么?</li><li>有什么优势?</li><li>什么软件可以处理 Apache Parquet?</li><li>近期发现很多日志解决方案会将日志转换为 Apache Parquet, 为什么要这样处理, 有什么优势?</li></ul><h2>Apache Parquet 简介</h2><p><strong>Apache Parquet</strong> 是一种开源的列式存储文件格式，专门为大数据处理框架设计，最初由 Twitter 和 Cloudera 联合开发，现为 Apache 顶级项目。</p><h2>核心优势</h2><h3>1. <strong>列式存储结构</strong></h3><ul><li>与传统行式存储不同，Parquet 按列存储数据</li><li>查询时只需读取相关列，大幅减少 I/O</li><li>示例对比：</li></ul><pre><code class="log">行式存储：Row1[col1,col2,col3], Row2[col1,col2,col3], ...
列式存储：Column1[所有行的值], Column2[所有行的值], ...</code></pre><h3>2. <strong>高效的压缩和编码</strong></h3><ul><li>同列数据类型一致，压缩效率更高（可达行式存储的 1/10）</li><li>支持多种编码：RLE、字典编码、Delta 编码等</li><li>支持多种压缩：Snappy、Gzip、LZO、Zstd</li></ul><h3>3. <strong>Schema 演化支持</strong></h3><ul><li>支持向后/向前兼容的 schema 变更</li><li>可以添加新列、删除列、修改列类型</li></ul><h3>4. <strong>谓词下推</strong>（Predicate Pushdown）</h3><ul><li>查询引擎可以在读取数据前过滤不相关的数据块</li><li>利用列统计信息（min/max 值）跳过无关数据块</li></ul><h3>5. <strong>嵌套数据结构支持</strong></h3><ul><li>原生支持复杂嵌套数据类型（数组、映射、结构体）</li><li>使用 Dremel 记录 shredding 算法高效存储嵌套数据</li></ul><h2>能处理 Parquet 的软件/框架</h2><h3>大数据处理框架</h3><ul><li><strong>Apache Spark</strong>（主要使用场景）</li><li><strong>Apache Hive</strong></li><li><strong>Apache Impala</strong></li><li><strong>Presto/Trino</strong></li><li><strong>Apache Flink</strong></li><li><strong>Apache Arrow</strong>（内存格式转换）</li></ul><h3>查询引擎</h3><ul><li><strong>AWS Athena</strong></li><li><strong>Google BigQuery</strong></li><li><strong>Azure Synapse</strong></li><li><strong>DuckDB</strong></li><li><strong>Polars</strong></li></ul><h3>编程语言支持</h3><ul><li>Python（PyArrow、pandas）</li><li>Java</li><li>R</li><li>Go</li><li>.NET</li></ul><h3>日志解决方案</h3><ul><li>Cloudflare Log Explorer</li><li>OpenObserve</li><li>Grafana Tempo</li><li>Yelp</li><li>AWS 官方参考架构: <a href="https://link.segmentfault.com/?enc=2wjCHoId9L6HfhFi4g3igg%3D%3D.nr%2Bh7vBlzzPVkfBe4FchtQYyTkD8BJgIWyxBncwCJo3nn1HYBhdq2JSzm6VKluj4ZA5STUNioUzhgF%2F08UcgsBftxVAcuSVadw5rtHLRTQT18Q6qNa4I6DpsIrr3V3vTMwfQ%2Fimoe1SWQJfnLkpKUhK0bwALu2rdAw1rARGFIQE%3D" rel="nofollow" target="_blank">Extracting key insights from Amazon S3 access logs with AWS Glue for Ray | AWS Big Data Blog</a></li></ul><h2>日志解决方案转用 Parquet 的原因</h2><h3>1. <strong>成本效益</strong></h3><pre><code class="log"># 示例：日志存储成本对比

原始 JSON 日志：1TB → 存储成本 $$$$
Parquet 压缩后：~100GB → 存储成本 $</code></pre><ul><li>存储成本降低 70-90%</li><li>网络传输成本显著降低</li></ul><h3>2. <strong>查询性能提升</strong></h3><pre><code class="sql">-- 典型日志查询场景
SELECT COUNT(*), error_code 
FROM logs 
WHERE date &gt;= '2024-01-01' 
  AND status = 'ERROR' 
GROUP BY error_code;

-- Parquet 优势：
-- 1. 只读取 date, status, error_code 三列
-- 2. 利用列统计快速跳过无关日期分区
-- 3. 压缩数据减少磁盘 I/O</code></pre><h3>3. <strong>适合时序数据分析</strong></h3><ul><li>日志数据天然具有时间属性</li><li>Parquet 支持按时间分区，优化时间范围查询</li><li>结合分区剪枝（Partition Pruning）大幅提升性能</li></ul><h3>4. <strong>兼容现代数据栈</strong></h3><pre><code class="log"># 典型日志处理管道
原始日志 → Fluentd/Logstash → Kafka → 
Spark Streaming → Parquet (S3/ADLS) → 
Trino/Athena 查询 → BI 工具</code></pre><h3>5. <strong>长期存储和分析</strong></h3><ul><li>Parquet 是分析型工作负载的理想格式</li><li>支持数据湖架构（Delta Lake、Iceberg、Hudi）</li><li>便于历史日志的趋势分析和机器学习</li></ul><h2>具体应用场景示例</h2><h3>案例：ELT 日志分析管道</h3><pre><code class="log">原始日志 (JSON/文本)
       ↓
实时处理层 (Kafka)
       ↓
批处理层 (Spark) → 转换为 Parquet
       ↓
云存储 (S3/GCS) → 分区: dt=2024-01-01/
       ↓
查询层 (Athena/Presto)
       ↓
可视化 (Grafana/Tableau)</code></pre><h3>性能对比数据</h3><ul><li><strong>存储空间</strong>：较 JSON 减少 75-90%</li><li><strong>查询速度</strong>：提升 10-100 倍（取决于查询模式）</li><li><strong>扫描数据量</strong>：减少 60-95%（列裁剪效果）</li></ul><h2>注意事项</h2><ol><li><p><strong>不适合场景</strong>：</p><ul><li>高频单行读写（OLTP）</li><li>需要流式逐行处理的场景</li><li>小文件过多会影响性能</li></ul></li><li><p><strong>最佳实践</strong>：</p><ul><li>合理设置文件大小（128MB-1GB）</li><li>按时间分区组织数据</li><li>选择适当的压缩算法（平衡速度/比率）</li></ul></li></ol><p>Parquet 已成为现代数据湖和日志分析的事实标准格式，特别适合需要长期存储、批量分析和成本优化的日志管理场景。</p>]]></description></item><item>    <title><![CDATA[跟老卫学仓颉编程语言开发：标识符与程序结构 waylau ]]></title>    <link>https://segmentfault.com/a/1190000047495117</link>    <guid>https://segmentfault.com/a/1190000047495117</guid>    <pubDate>2025-12-23 09:03:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本节介绍仓颉编程语言的标识符和程序结构。</p><h3>标识符</h3><p>在仓颉编程语言中，开发者可以给一些程序元素命名，这些名字也被称为“标识符”，标识符分为普通标识符和原始标识符两类，它们分别遵从不同的命名规则。</p><p>普通标识符不能和仓颉关键字相同，可以取自以下两类字符序列：</p><p>由“XID_Start”字符开头，后接任意长度的“XID_Continue”字符<br/>由一个“_”开头，后接至少一个“XID_Continue”字符<br/>其中，“XID_Start”、“XID_Continue”定义见Unicode标准。仓颉使用Unicode标准15.0.0（<a href="https://link.segmentfault.com/?enc=1FQiwVPS0pmeKjYFkGuP3Q%3D%3D.JWL7QRGEFYOL%2Bn%2FkXA97rePuGAZSe3FDcFjj0iYDMCrzAwP%2BJjbRxuJu%2FLR%2FBoPaqI6eOupwVcZHIDBqgInR%2FQ%3D%3D" rel="nofollow" target="_blank">https://www.unicode.org/reports/tr31/tr31-37.html</a>）。</p><p>仓颉把所有标识符识别为Normalization Form C (NFC) 后的形式。两个标识符如果在NFC后相等，则认为是相同的标识符。</p><p>例如，以下每行字符串都是合法的普通标识符：</p><pre><code>abc
_abc
abc_
a1b2c3
a_b_c
a1_b2_c3
仓颉
__こんにちは</code></pre><p>以下每行字符串都是不合法的普通标识符：</p><pre><code>ab&amp;c  // 使用了非法字符 “&amp;”
3abc  // 数字不能出现在头部
while // 不能使用仓颉关键字</code></pre><p>原始标识符是在普通标识符或仓颉关键字的外面加上一对反引号，主要用于将仓颉关键字作为标识符的场景。</p><p>例如，以下每行字符串都是合法的原始标识符：</p><pre><code>`abc`
`_abc`
`a1b2c3`
`if`
`while`
`à֮̅̕b`</code></pre><p>以下每行字符串，由于反引号内的部分是不合法的普通标识符，所以它们整体也是不合法的原始标识符：</p><pre><code>`ab&amp;c`
`3abc`</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047495119" alt="" title=""/></p><h3>程序结构</h3><p>通常，我们都会在扩展名为<code>.cj</code>的文本文件中编写仓颉程序，这些程序和文件也被称为源代码和源文件，在程序开发的最后阶段，这些源代码将被编译为特定格式的二进制文件。</p><p>在仓颉程序的顶层作用域中，可以定义一系列的变量、函数和自定义类型（如struct、class、enum和interface等），其中的变量和函数分别被称为全局变量和全局函数。如果要将仓颉程序编译为可执行文件，则需要在顶层作用域中定义一个main函数作为程序入口，它可以有<code>Array&lt;String&gt;</code>类型的参数，也可以没有参数，它的返回值类型可以是整数类型或Unit类型。</p><p><strong>注</strong>：定义main函数时，不需要写func修饰符。此外，如果需要获取程序启动时的命令行参数，可以声明和使用<code>Array&lt;String&gt;</code>类型参数。</p><p>例如在以下程序中，我们在顶层作用域定义了全局变量g和全局函数b，还有自定义类型C、D和E，以及作为程序入口的main函数。</p><pre><code>let g = 2022
func b() {}
struct C {}
class D {}
enum E { F | G }

main() {
    println(g)
}</code></pre><p>在非顶层作用域中不能定义上述自定义类型，但可以定义变量和函数，称之为局部变量和局部函数。特别地，对于定义在自定义类型中的变量和函数，称之为成员变量和成员函数。</p><p><strong>注</strong>：enum和interface中仅支持定义成员函数，不支持定义成员变量。</p><p>例如在以下程序中，我们在顶层作用域定义了全局函数a和自定义类型A，在函数a中定义了局部变量b和局部函数c，在自定义类型A中定义了成员变量b和成员函数c。</p><pre><code>func a() {
    let b = 2023
    func c() {
        println(b)
    }
    c()
}

class A {
    let b = 2024
    public func c() {
        println(b)
    }
}

main() {
    a()
    A().c()
}</code></pre><p>运行以上程序，将输出：</p><pre><code>2023
2024</code></pre><p>本节示例可以在“program_structure_demo”应用下找到。</p><h3>参考引用</h3><ul><li>免费开源书<a href="https://link.segmentfault.com/?enc=02SD0srpY9xvgKf2y7FNbQ%3D%3D.zib%2FVwP%2FUcbQJcBR9G22Z9w%2FOzUa5S1t6yrlZC8fQyZgJ9z%2Fc1X4Zaj%2FYTFnwjLUuRf9v2%2BSAznJ70eLZgF1GQ%3D%3D" rel="nofollow" target="_blank">《跟老卫学仓颉编程语言开发》</a></li><li>免费开源书<a href="https://link.segmentfault.com/?enc=U29IRfe1A7Nsq3kDloVTvg%3D%3D.pCKj1Uf2RxZKWPiU9LQpqSS7Y6HIqK4guBuTwIj0V09%2FeStXSRi9ZMj23aiRmz2t" rel="nofollow" target="_blank">《跟老卫学HarmonyOS开发》</a></li><li><a href="https://link.segmentfault.com/?enc=jTBlYp%2F3GIfi9SHJ0j55pQ%3D%3D.1GDs2BQpV43JxWpsSmerODXYYJUjTXBnyxDaHyX0nsmra8f1rMIEiosLnrnuzeG%2F" rel="nofollow" target="_blank">HarmonyOS NEXT+AI大模型打造智能助手APP（仓颉版）</a>（视频）</li><li><a href="https://link.segmentfault.com/?enc=V1slGi2qrkUBpgNM01%2FgrQ%3D%3D.sWCbesEpjP1GPBc%2BZ%2Fs4%2BJ32F5g6lEx0N0NZE8qd%2B4l7RvwXTe2a5jfU0Hh4aOiBp2i1HzYe27iwXUAuXYIZj%2BMnT5AOleIkC7cFjUYLjVs%3D" rel="nofollow" target="_blank">仓颉编程从入门到实践</a>（北京大学出版社）</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047495120" alt="" title="" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[AI编程正在"腐烂"，而解决方案在40年前就存在了 reddish ]]></title>    <link>https://segmentfault.com/a/1190000047495123</link>    <guid>https://segmentfault.com/a/1190000047495123</guid>    <pubDate>2025-12-23 09:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>提到AI编程呢，真的是让人欢喜让人优。爱得不行又恨得不行。 写出来的东西前期是好得不得了，后期就垃圾遍地，错觉横飞。 因为他的能力强大，对于研发和产品来说很难割舍不用，但他不受控制，也给后期维护带来的极大地麻烦。</p><p>经常行走江湖的朋友都知道，一旦被毒蛇咬了，不要怕，不要慌，剧毒之物，五步之内必有解药。</p><p>那问题来了，对于AI编程的这种困境，五步之内是否有解药呢？</p><p>我们梳理了一下这些年来的软件工程的编程模型，a~~~,还真的有一个软件的设计架构，刚好可以补足AI编程的短板。只是这个架构太古老了，估计很多人都没有听说过。 这个架构就是命令行Pipeline架构(不记得有没人定义过明确的名称，大概是这个意思)。</p><h2>AI编程的三大绝症</h2><p>要解决问题，第一步就是要识别问题，找出他的<code>根本原因</code> <sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>。</p><p>AI编程前面一开始很厉害，但是越到后期越糟糕，而且是指数级的下降。这种现象被业内人士称为"上下文腐烂"。</p><p>它不仅会导致代码质量下降，处处埋伏隐患，而且还无法维护。给实际生成应用带来了很大的挑战。 最新的OpenAI的<code>访谈</code> <sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>里就提到一个数据就是说目前主流应用AI编程的公司，其工作量100个小时有80个小时都花在AI编码后的验证和确认上。</p><p>问题在哪儿呢？</p><ul><li><strong>金鱼记忆</strong>：上下文窗口的诅咒<br/>  大语言模型在长上下文中的注意力衰减问题。简单来说就是AI的记忆力非常有限。 就跟鱼一样只有7秒的记忆。 注意，这不是说他能力不行，相反能力是很强大的。怎么说呢，就像是一条鲸鱼，随便拍拍尾巴就能掀起巨浪，但他的记忆就只有7秒。 他能一下子把人拍飞，也百分百听你的话，但你让他干点复杂的事情，不好意思，拍一下能解决的他能干的很好，拍两下能解决的他能弄得鸡飞狗跳， 拍第二下的时候忘了第一下，连目标都可能丢失了，还顺带造成附加损伤。</li><li><strong>代码瘟疫</strong>：训练数据的技术债传染<br/>  AI模型在训练过程中需要大量的数据训练，比如说来自github的代码库。问题是不是所有的代码都是大师级别的规范设计的，就是经过筛选，也难免其中混杂一些奇怪的思路和做法，这些都会在训练过程中被AI吸收，然后在某个不知道什么时机的点传播到生成结果中。如果仅仅是思路和逻辑弱一点倒也罢了，但如果是存在严重的安全隐患和漏洞，那后果就麻烦了。</li><li><strong>沟通深渊</strong>：听话的AI是好AI么?<br/>  AI编码能力是从程序员学来的，当然毛病也有点一样，就是都喜欢写新的代码，不愿意修改现有代码。AI编码在修改现有代码时，尤其容易出现顾此失彼的问题。并且当开发者给出指示后，AI很大部分程度上是对指示100%遵循的。包括你对他说这块颜色不对要黑色中带点五彩斑斓的感觉。或者是这块能不能让他调小一些，但是内容显示的更大。这些要求如果是对人当面沟通，多半就会掀桌子。但AI仍会回复你说：你说的有道理，我立刻按你的要求来改， 然后大肆发挥，你就等着看结果就行，反正你要对结果负责，AI不对结果负责。</li></ul><h2>命令行架构模式, 四十年前的解药</h2><p>针对AI编码的问题和根源，不难发现，古老的命令行Pipeline架构正恰恰好弥补了AI编码的短板。</p><p>命令行Pipeline架构是一种非常实用的古老的软件设计架构。他是如此的简洁、清晰，以至于并没有人为他专门定义过什么理论或者专有名词（我印象中是没有的）。这里我称呼为命令行Pipeline架构，可能不大准，但意思是没问题的。</p><p>命令行Pipeline架构的核心思想是 <strong>一个工具只做一件事，然后遵循标准的输入和输出</strong>。在命令行环境中，多个工具通过管道(|)符号串联，形成一个工作流。前一个工具的标准输出成为下一个工具的标准输入，数据在进程间通过内存缓冲区传递，无需落地为临时文件。</p><p><strong>命令行Pipeline架构的核心优势在于其清晰的定义、简单的结构和各步骤间的低耦合性。这种架构直接对抗了AI编程腐烂的三大根源：</strong></p><ul><li><strong>进程隔离</strong>：让AI编码只敢简单的同时逻辑性强的事<br/>  命令行Pipeline将整个任务拆分为多个独立进程，每个进程专注于单一功能。这种设计使得AI生成的代码即使存在缺陷，也不会污染整个工作流。例如，如果数据清洗步骤中的AI生成代码出现错误，后续的特征工程步骤将不会受到影响，开发者可以快速定位问题并进行修复。 同时最核心的一点，就是拆分成的<strong>子任务（独立程序）通常是用途单一且明确的，而且范围也不大</strong>，其复杂度还可以控制在刚好能在AI腐烂前能解决。这样子AI能一次或简单的几次迭代就能搞定，质量还高。</li><li><strong>标准IO协议</strong>：用最常见最普通的规则代替混乱<br/>  命令行Pipeline强制使用标准输入输出进行数据传递，减少了硬编码和隐式依赖。完全一致的输入输出处理，而且也都是很成熟简单的参数传递机制，使得AI无需额外的过多约束规格定义就能很好的满足整体架构设计的的代码。有点像遵循Restful规范，逻辑上似乎意义不大，但在整体架构上，能提供简单清晰一致的理解和思路，从而简化问题，避免一碗面条式的逻辑。例如，在数据预处理阶段，AI生成的代码必须将处理后的数据输出到标准输出，而后续步骤只需读取标准输入，无需关心具体实现细节。</li><li><strong>组合起来威力无边</strong>：最为重要的是，每个子任务（独立程序）都可以独立运行，单独工作，简单的测试和验证，独立升级维护。 这使得系统的开发调试运行维护都极为简便，同时组合起来功能强大。</li></ul><p>命令行模式是所有模式中难得见到的可以将系统实现简化，操作简化，但是功能却超越预设的强大的一种。所有的按部就班设计的系统，最好的结果就是按照设计的工作。 而命令行模式的各种组合，其形式几乎可以是无穷无尽的。</p><p>由于每个子任务（独立程序）都可以单独运行并完整的完成自己的任务，因此多个子任务可以任意的前后组合起来，形成复杂的工作流。 而这一步，只需要简单的shell,bash, cmd脚本批处理就可以做到了。</p><p>假设你有10个可独立运行的子任务，一个5层的工作流，理论上来说，就有100000种不同的使用场景。而unix/linux,mac,windows/dos的命令行，通常只有几十到百来个命令。 而这些命令的组合，基本就能满足日常整个操作系统维护的需求。</p><p>所以依靠命令行pipeline架构的简单设计，就能实现非常庞大复杂的系统能力。 这是一种化繁为简的神兵利器。</p><h2>命令行架构在AI编程中的实践</h2><p>命令行架构这么强大，那么，要到哪里才能买...啊不对，要怎么才能用起来呢？</p><p>首先，顾名思义，命令行架构，如果你的系统是后端的，不需要UI界面的，基本都可以直接套用。而且思路和设计也极为简单，就是将整个大的系统，尽可能的拆分成可以独立运行的子任务（子系统）。拆的越细越好，越多越好。 每个子任务（子系统）因为是独立运行，因此也可以独立设计，独立开发，如果是多人协作的话，还可以分给不同的人，甚至是外包。</p><p>当然这些的前提是子任务的功能足够的清晰，明确，单一；所以要拆的足够的细。举例来说，文件操作很多，也很复杂。单个子任务来维护文件操作就过于庞大了，就可以继续拆分。像是 stat命令，就只做一件事情，返回文件的修改时间访问时间创建时间等几个属性。</p><p>基本上，只要能拆到这么细的程度，整个系统架构就算是完成了。</p><p>拆的足够细是保证系统能够非常简单的构建。 但要做到想命令行架构一样的强大的组合能力，还需要遵循标准输入输出模式。 这样子才可以将任意的子任务任意的组合起来，形成复杂的工作流。</p><p>这里举个简单的例子：</p><blockquote><blockquote>在某电商平台的用户数据分析项目中，开发团队采用了命令行Pipeline架构来处理海量用户数据。整个管道包含数据采集、清洗、特征工程、验证和存储五个步骤：</blockquote></blockquote><pre><code class="bash">python collect_data.py --start_date 2025-01-01 | python clean_data.py | python extract_features.py | python validate_features.py | python store_data.py</code></pre><p>上面的例子是最简单的直接输入输出重定向，管道串联的例子。 实际使用上，还可以借助shell bash或cmd bat的脚本批处理做粘合剂，实现条件判断，流程跳转，函数调用等等复杂的逻辑。</p><p>那是不是仅仅只有后台无界面应用才适合命令行架构模式呢？ </p><p>也不是的。即便是UI界面丰富的应用，一些情况下也是可以采用或部分采用命令行架构模式的。</p><p>在经典的设计模式中，就有一种命令模式的，它的基本思想是将请求封装成一个对象，从而使你可以用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可撤销的操作。 其思路也是跟命令行架构非常相似的。每个菜单项单独配置一个Action，每个Action都可以独立运行和测试。</p><p>另一种思路也是部分软件系统在采用的，就是将整个系统都设计成后端服务模式，一些必要的UI界面功能，单独通过提供一个网页的形式来操作。 这样子不仅简化了实现，UI界面的调整也变得非常简单。</p><p>还有哪些可以应用命令行架构模式的场景呢？欢迎各位分析师来分享和补充。</p><p>::: {#author name=reddish}<br/><em>本文同步发表在 <a href="https://link.segmentfault.com/?enc=fv6TZXytW0Ys9SV3TnMTCg%3D%3D.NHcHfxsEdAtQVnaEFCTGRw%3D%3D" rel="nofollow" target="_blank">软件需求探索</a>的<a href="https://link.segmentfault.com/?enc=Dk4mlD6bCsLdhTw%2F7QhpkA%3D%3D.YT4MRkbSuM2Upki91JprlkYsJ1CAz7JNN1MbIXLzL1JEjfGEk5%2F%2F8Ep9cURO61s55qq7gfg7ctkXvptCNhin%2B4VZp3wKT5UI5tqYZQqwilY%3D" rel="nofollow" target="_blank">https://srs.pub/thinking/commandline-is-the-best-architecture...</a></em></p><p><em>作者: <a href="mailto:reddish@srs.pub" target="_blank">reddish@srs.pub</a></em><br/>:::</p><div class="footnotes"><hr/><ol><li id="fn-1"> 商业分析中的五十种分析方法和技巧之40-根本原因分析.  <a href="https://link.segmentfault.com/?enc=YOsxbOEjE35W7foQfmKMQQ%3D%3D.Z0mFEHi8aAkNB3OfvLUIeixNvTPFDB3CqSVxN8XEd1QJg8V7Knh1bUGqozsQ%2BWWHhppVlFxDOIk%2BDw2KdMLUEHf3uZNF%2B%2BLvw2xou1H0LMJ%2BjMqkzgbf3%2BlSXWsbLQ5o" rel="nofollow" target="_blank">https://srs.pub/babok/genbenyuanyin-fenxi.htmlahref=#fnref-1c...</a></li><li id="fn-2"> 商业分析中的五十种分析方法和技巧之25-访谈.  <a href="https://link.segmentfault.com/?enc=mPIZuVvX2l7zIeU2sWq0WQ%3D%3D.b4yV8w2fYc06jk0OAQ5ipO2jvvKWyEvzzkBG50GV%2BPBQc9wMM0Smo6B4DCxJdW1kGJPkr7qNSLUDJngvb9AEhzjgPSUC08Gdg3NT8oQSoAk%3D" rel="nofollow" target="_blank">https://srs.pub/babok/fangtan.htmlahref=#fnref-2class=footnot...</a></li></ol></div>]]></description></item><item>    <title><![CDATA[剑指offer-53、表达数值的字符串 SevenCoding ]]></title>    <link>https://segmentfault.com/a/1190000047490390</link>    <guid>https://segmentfault.com/a/1190000047490390</guid>    <pubDate>2025-12-23 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>题⽬描述</h2><p>请实现⼀个函数⽤来判断字符串str是否表示数值（包括科学计数法的数字，⼩数和整数）。科学计数法的数字(按顺序）可以分成以下⼏个部分:</p><ol><li>若⼲空格</li><li>⼀个整数或者⼩数</li><li>（可选）⼀个 ' e ' 或 ' E ' ，后⾯跟着⼀个整数(可正可负)</li><li>若⼲空格</li></ol><p>⼩数（按顺序）可以分成以下⼏个部分：</p><ol><li>若⼲空格</li><li>（可选）⼀个符号字符（'+' 或 '-'）</li><li><p>可能是以下描述格式之⼀:</p><ol><li>⾄少⼀位数字，后⾯跟着⼀个点 '.'</li><li>⾄少⼀位数字，后⾯跟着⼀个点 '.' ，后⾯再跟着⾄少⼀位数字</li><li>⼀个点 '.' ，后⾯跟着⾄少⼀位数字</li></ol></li><li>若⼲空格</li></ol><p>整数（按顺序）可以分成以下⼏个部分：</p><ol><li>若⼲空格</li><li>（可选）⼀个符号字符（' + ' 或 ' - ')</li><li>⾄少⼀位数字</li><li>若⼲空格</li></ol><p>例如，字符串["+100","5e2","-123","3.1416","-1E-16"] 都表示数值。</p><p>但是["12e","1a3.14","1.2.3","+-5","12e+4.3"] 都不是数值。</p><p>提示:</p><ol><li>1 &lt;= str.length &lt;= 25</li><li>str 仅含英⽂字⺟（⼤写和⼩写），数字（0-9），加号 '+' ，减号 '-' ，空格 ' ' 或者点 '.' 。</li><li>如果怀疑⽤例是不是能表示为数值的，可以使⽤python 的print(float(str)) 去查看</li></ol><p>示例1<br/>输⼊："123.45e+6"<br/>返回值：true</p><p>示例2<br/>输⼊："1.2.3"<br/>返回值：false</p><h2>思路及解答</h2><h3>暴力分析拆解</h3><p>主要是分析好判断分⽀，可以定义⼏个变量：</p><ul><li>hashNum : 是否已经有数字</li><li>hashE ：是否已经有E</li><li>hasSign ：是否已经有符号</li><li>hasDot ：是否已经有⼩数点</li></ul><p>⾸先，初始化当前的索引index =0 ，字符串头部的空格需要跳过。</p><ul><li><p>循环判断索引是否在有效的范围内：</p><ul><li>循环判断是否是数字，是数字则更新hasNum = true ,并且索引后移，直到不是数字的时候，跳出循环。</li></ul></li><li>跳出循环后，需要判断当前的index 是否合法，不合法直接break</li><li><p>取出当前索引的字符c ：</p><ul><li><p>如果c 是e 或者E ：</p><ul><li>如果前⾯已经出现过E ，或者前⾯没有数字，直接返回false</li><li>否则， hasE 置为true ，其他的置为false ，也就是E后⾯可以继续出现符号数字和⼩数点了</li></ul></li><li><p>如果c 是“ + ”或者“ - ”：</p><ul><li>前⾯如果已经出现过数字或者符号或者⼩数点，都不是合法的</li><li>否则hasSign 置为true ，表示符号出现过</li></ul></li><li><p>如果c 是⼩数点“ . ”</p><ul><li>如果前⾯已经有⼩数点或者有E出现了，那么就是⾮法的，返回false</li><li>否则hasDot 置为true</li></ul></li><li>如果c 为空格，直接跳出循环</li><li>否则，直接返回false</li></ul></li><li>最后也需要跳过空格</li><li>最后判断是否合法的条件是：是否到达最后⼀个字符，并且出现过数字</li></ul><pre><code class="java">public boolean isNumeric(String str) {
    int size = str.length();
    int index= 0 ;
    // 默认全部是false
    boolean hashNum=false ,hasE=false ,hasSign=false ,hasDot=false;
    // 跳过空格
    while(index&lt;size&amp;&amp;str.charAt(index)==' '){
        index++;
    }
    
    while(index&lt;size){
        while(index&lt;size&amp;&amp;str.charAt(index)&gt;='0'&amp;&amp; str.charAt(index)&lt;='9'){
            index++;
            // 表示前⾯有数字
            hashNum = true;
        }
- 
        // 到末尾直接跳出
        if(index==size){
            break;
        }
- 
        char c = str.charAt(index);
        if(c=='e'||c=='E'){
            // 前⾯有E或者没有数字在前⾯
            if(hasE||!hashNum){
                return false;
            }
            hasE = true;
            // 出现E了后⾯⼜可以出现数字了
            hashNum = false;
            hasSign = false;
            hasDot = false;
        }else if(c=='+'||c=='-'){
            if(hasSign||hashNum||hasDot){
                return false;
            }
            hasSign = true;
        }else if(c=='.'){
            if(hasDot||hasE){
                return false;
            }
            hasDot =true;
        }else if(c==' '){
            break;
        }else{
            return false;
        }
        index++;
    }
    // 跳过空格
    while(index&lt;size&amp;&amp;str.charAt(index)==' '){
        index++;
    }
    return hashNum &amp;&amp;index==size;
}</code></pre><p>这道题，其实本质是状态的切换，最最重要的⼀点，是 E 出现之后，其实⼩数点和符号，和数字，都是可以再出现的，可以理解为 E 就是⼀个分割线。</p><h3>正则表达式</h3><p>直接借助正则表达式进⾏匹配，但是并不太推荐这种解法：</p><pre><code class="java">public class Solution {

    public boolean isNumeric (String str) {
        // 核心正则表达式：处理空格、符号、小数、指数部分
        
        // ^表示开头 $ 表示结尾 java中两个\\ 代表⼀个\
        // \\s*开头可能有空格
        // * 零次或多次匹配前⾯的字符或⼦表达式
        // ？零次或⼀次匹配前⾯的字符或⼦表达式
        // + ⼀次或多次匹配前⾯的字符或⼦表达式
        // [] 字符集。匹配包含的任⼀字符
        // (:? )匹配 pattern 但不捕获该匹配的⼦表达式，即它是⼀个⾮捕获匹配
        String p = "^\\s*[+-]?(\\d*\\.\\d+|\\d+(\\.\\d*)?)(?:[eE][+-]?\\d+)?$";
        return Pattern.matches(p,str);
    }
}</code></pre><ul><li>O(n)时间复杂度</li><li>O(1)空间复杂度</li></ul><h3>有限状态机</h3><p>使用确定有限状态机(DFA)来精确建模数值的判断过程。</p><p>有限状态机法：通过状态转移精确控制数值格式。定义9种状态，根据输入字符进行状态转移</p><pre><code class="java">public boolean isNumberDFA(String str) {
    if (str == null) return false;
    
    // 状态定义：0-8共9种状态
    // 0: 起始空格 1: 符号 2: 整数数字 3: 小数点前无数字 
    // 4: 小数点后有数字 5: 指数e 6: 指数符号 7: 指数数字 8: 结尾空格
    int state = 0; // 初始状态
    
    // 状态转移表
    int[][] transitionTable = {
        // 空格 符号 数字 小数点 e/E 其他
        {0,   1,   2,   3,   -1, -1}, // 状态0: 起始空格
        {-1, -1,   2,   3,   -1, -1}, // 状态1: 符号
        {8,  -1,   2,   4,    5, -1}, // 状态2: 整数数字
        {-1, -1,   4,   -1,  -1, -1}, // 状态3: 小数点前无数字
        {8,  -1,   4,   -1,   5, -1}, // 状态4: 小数点后有数字
        {-1,  6,   7,   -1,  -1, -1}, // 状态5: 指数e
        {-1, -1,   7,   -1,  -1, -1}, // 状态6: 指数符号
        {8,  -1,   7,   -1,  -1, -1}, // 状态7: 指数数字
        {8,  -1,  -1,   -1,  -1, -1}  // 状态8: 结尾空格
    };
    
    for (char c : str.toCharArray()) {
        int inputType = getInputType(c);
        if (inputType == -1) return false;
        
        state = transitionTable[state][inputType];
        if (state == -1) return false;
    }
    
    // 可接受的状态：数字相关状态(2,4,7,8)
    return state == 2 || state == 4 || state == 7 || state == 8;
}

// 将字符分类为状态机输入类型
private int getInputType(char c) {
    switch (c) {
        case ' ': return 0; // 空格
        case '+': case '-': return 1; // 符号
        case '0': case '1': case '2': case '3': case '4': 
        case '5': case '6': case '7': case '8': case '9': 
            return 2; // 数字
        case '.': return 3; // 小数点
        case 'e': case 'E': return 4; // 指数符号
        default: return 5; // 其他字符
    }
}</code></pre><ul><li>时间复杂度：O(n)</li><li>空间复杂度：O(1)</li></ul>]]></description></item><item>    <title><![CDATA[ChatGPT可手动"调温"、谷歌推出 A2UI 标准、通义千问推出 Qwen-Image-Laye]]></title>    <link>https://segmentfault.com/a/1190000047494857</link>    <guid>https://segmentfault.com/a/1190000047494857</guid>    <pubDate>2025-12-23 00:04:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>ChatGPT可手动"调温"了！OpenAI上线热情度滑块，用户可自定义AI的"情绪浓度"</h3><p>OpenAI公司最近推出了一项非常有趣的新功能，允许用户通过热情度滑块来调节ChatGPT的"情绪浓度"。这项创新让用户可以根据不同场景和需求，自定义AI的响应风格，从专业严谨到活泼热情，实现真正个性化的AI交互体验。</p><p><strong>核心事件</strong>：OpenAI为ChatGPT增加了情感调节功能，用户可以控制AI的响应风格和情绪表达强度。</p><p><strong>技术细节</strong>：这一功能可能基于温度参数调整、提示词微调或者专门的微调模型，通过算法实现对AI输出风格的精确控制。用户不再只能接受预设的AI性格，而是可以根据具体需求调整AI的"热情度"，使得AI助手在不同场合下表现出合适的交互风格。</p><p><strong>行业影响</strong>：这一创新为AI对话系统提供了更丰富的交互维度，对开发者来说意味着更多API选择和更强大的功能。未来，我们可以期待更多类似的情感调节功能在各类AI产品中出现，这将提升用户体验，尤其是在客户服务、教育和娱乐等场景中。</p><p><strong>商业意义</strong>：OpenAI通过这一功能进一步增强了ChatGPT的差异化竞争力，满足了用户对个性化AI助手的需求，可能吸引更多付费用户。</p><h3>谷歌推出 A2UI 标准，让 AI 实时生成用户界面</h3><p>谷歌最近发布了A2UI标准（AI-to-User Interface），这是一项革命性的技术标准，允许AI实时生成用户界面。该标准将彻底改变传统的UI开发流程，设计师和开发者只需描述需求，AI即可实时生成相应的用户界面，极大地提升了开发效率。</p><p><strong>核心事件</strong>：谷歌推出的A2UI标准允许AI根据自然语言描述实时生成用户界面，这代表了UI开发范式的根本性变革。</p><p><strong>技术细节</strong>：A2UI标准整合了先进的多模态AI模型，能够理解设计需求的自然语言描述，并将其转换为具体的UI元素、布局和交互逻辑。通过这一标准，AI可以理解设计意图，生成符合现代设计规范的界面，并支持实时预览和迭代。</p><p><strong>行业影响</strong>：这一标准的推出将对整个UI/UX行业产生深远影响。前端开发者需要适应AI辅助的UI设计方式，而设计师可以将更多精力投入到创意和用户体验策略上，而非具体的实现细节。对开发者来说，这意味着开发流程的简化和效率的提升。</p><p><strong>商业意义</strong>：谷歌通过A2UI标准巩固了其在AI领域的领先地位，同时也为开发者提供了新的工具和工作流程，可能加速Web应用的开发速度和创新步伐。</p><h3>通义千问推出 Qwen-Image-Layered 模型，实现图片 "分层编辑" 突破</h3><p>阿里巴巴通义实验室发布了Qwen-Image-Layered模型，这一创新性技术实现了图片的"分层编辑"功能。与传统图像编辑方法不同，该模型能够将图像分离成多个图层，允许用户对图像的不同部分进行独立编辑，极大提升了图像编辑的灵活性和精确度。</p><p><strong>核心事件</strong>：通义千问团队发布了Qwen-Image-Layered模型，实现了AI驱动的图像分层编辑功能。</p><p><strong>技术细节</strong>：该模型采用先进的计算机视觉和深度学习技术，能够智能识别图像中的不同对象和区域，并将其分离成独立的图层。用户可以单独编辑每个图层的属性，如颜色、纹理、位置等，而不会影响其他部分。这种技术基于对图像语义内容的理解，实现了更加自然和精确的编辑。</p><p><strong>行业影响</strong>：图像分层编辑技术的突破，将为设计师、内容创作者和开发者提供更强大的工具。对AI开发者而言，这一技术展示了多模态模型在图像理解与编辑方面的巨大潜力，为未来图像生成和编辑模型的发展指明了方向。</p><p><strong>商业意义</strong>：阿里巴巴通过这一技术创新进一步巩固了在AI图像处理领域的地位，可能吸引更多设计师和内容创作者使用其AI服务，推动相关云服务的收入增长。</p><h3>谷歌 Chrome 迎重大 AI 更新：搜索框功能"大换血"，跨标签页协同成亮点</h3><p>谷歌Chrome浏览器迎来了一次重大的AI功能更新，搜索框功能进行了全面革新，实现了跨标签页协同操作。用户可以利用AI助手在多个标签页之间进行信息整合和任务协同，提升浏览效率。</p><p><strong>核心事件</strong>：Chrome浏览器引入了AI助手，实现跨标签页的信息整合和任务协同功能。</p><p><strong>技术细节</strong>：这一更新利用了大型语言模型的上下文理解能力，AI助手可以分析用户打开的多个标签页内容，并根据用户需求进行信息提取、比较和整合。通过自然语言指令，用户可以要求浏览器在不同标签页间执行任务，如"比较这三个产品价格"或"总结这三篇文章的主要观点"。</p><p><strong>行业影响</strong>：这体现了浏览器厂商对AI集成的深度探索，对前端开发者来说，意味着浏览器功能的扩展和新的API机会。浏览器不再只是内容展示工具，而是逐渐成为智能化的信息处理平台。</p><p><strong>商业意义</strong>：谷歌通过增强Chrome的AI功能，进一步巩固了其在浏览器市场的主导地位，同时为AI服务创造了更多使用场景。</p><h3>"股票"登顶千问App十大AI提示词榜首:AI成全民"理财顾问"</h3><p>最新数据显示，"股票"相关提示词已跃居通义千问App十大热门AI提示词榜首，这反映出AI在金融理财领域的应用正受到广泛关注。AI正逐渐成为普通用户的"理财顾问"，通过分析市场数据、提供投资建议等方式，降低金融投资的门槛。</p><p><strong>核心事件</strong>：AI金融咨询功能成为用户最常使用的AI应用之一，"股票"相关查询在AI助手使用中占据重要位置。</p><p><strong>技术细节</strong>：AI理财顾问功能整合了自然语言处理、数据分析和金融知识，能够理解用户的金融问题，提供市场趋势分析、投资建议和风险评估。这些功能基于对大量金融数据和市场信息的分析，结合机器学习算法，为用户提供个性化的理财建议。</p><p><strong>行业影响</strong>：AI理财顾问的普及降低了金融投资的门槛，让更多普通用户能够获得专业的金融分析和建议。对金融科技开发者来说，这代表了一个快速增长的应用领域，需要关注合规性和准确性要求。</p><p><strong>商业意义</strong>：AI理财功能的普及为金融科技公司和AI平台提供了新的商业模式，通过AI服务获取更多用户并创造收入。</p><h3>Claude Chrome插件正式全量开放！付费用户瞬间拥有AI浏览器助手</h3><p>Anthropic的Claude Chrome插件现已向所有付费用户开放，为浏览器用户提供了强大的AI助手功能。用户可以直接在浏览器中与Claude交互，实现研究、写作、信息整理等多种任务。</p><p><strong>核心事件</strong>：Claude浏览器插件正式全面开放，为用户提供便捷的AI辅助浏览体验。</p><p><strong>技术细节</strong>：该插件集成了Claude强大的语言理解能力，可以直接分析当前页面内容，回答用户问题，提供摘要或执行其他任务。插件采用轻量级架构，确保在浏览器中运行时的性能和响应速度。</p><p><strong>行业影响</strong>：AI浏览器助手的普及改变了用户与网页内容的交互方式，开发者需要考虑如何优化网页内容以更好地与AI助手协作。</p><p><strong>商业意义</strong>：这一功能增强了Claude产品的用户粘性，通过浏览器插件为AI服务创造了更多使用场景。</p><h3>告别电量焦虑与沉重感:夸克 AI 眼镜 G1新品开启预售，轻至40g 且支持换电</h3><p>阿里巴巴旗下的夸克AI眼镜G1新品开启预售，这款眼镜重量仅有40克，还支持换电功能，解决了智能眼镜的续航问题。轻便的设计和创新的换电方案，为智能眼镜的普及扫清了部分障碍。</p><p><strong>核心事件</strong>：夸克AI眼镜G1以超轻重量和换电设计解决了智能眼镜的续航和便携性问题。</p><p><strong>技术细节</strong>：G1眼镜采用了轻量化材料和紧凑设计，重量控制在40克以内，同时引入了创新的换电模块，用户可以快速更换电池，延长使用时间。眼镜集成了AI处理单元，支持语音交互、AR显示等功能。</p><p><strong>行业影响</strong>：轻量化和换电设计为智能眼镜行业树立了新标准，推动了可穿戴设备的实用性提升。对硬件开发者而言，这一设计展示了在有限空间内平衡性能、重量和续航的技术路径。</p><p><strong>商业意义</strong>：夸克通过G1眼镜进入快速增长的可穿戴设备市场，结合AI功能，有望在智能眼镜领域建立先发优势。</p><h3>苹果携手普渡大学研发 DarkDiff 技术:即使在极暗环境下也能拍出"夜视仪"级大片</h3><p>苹果公司与普渡大学合作研发了DarkDiff技术，这项技术即使在极暗环境下也能拍摄出高质量照片，效果堪比"夜视仪"。这项技术可能将应用于未来的iPhone相机功能中。</p><p><strong>核心事件</strong>：苹果与普渡大学合作开发的DarkDiff技术，实现了极暗环境下的高质量图像拍摄。</p><p><strong>技术细节</strong>：DarkDiff技术可能基于扩散模型（Diffusion Model）的改进算法，能够从极低光图像中恢复细节，生成高质量的明亮图像。该技术通过AI模型学习如何在保持图像自然性的同时，提升暗部细节和整体亮度。</p><p><strong>行业影响</strong>：这一技术突破将提升移动设备的摄影能力，对手机制造商来说，这是一个重要的差异化功能。对AI图像处理开发者而言，这展示了扩散模型在低光图像增强方面的巨大潜力。</p><p><strong>商业意义</strong>：苹果通过这一技术优势增强了iPhone的摄影卖点，可能在竞争激烈的高端手机市场获得更多优势。</p><h3>生成式 AI 席卷游戏圈:Steam 热销榜前十竟有一半出自"AI 拥护者"之手</h3><p>生成式AI技术正在游戏行业中掀起波澜，最新数据显示，Steam热销榜前十名中竟有一半是由"AI拥护者"开发的游戏。这表明AI生成内容技术在游戏开发中的应用已开始获得市场认可。</p><p><strong>核心事件</strong>：AI辅助开发的游戏在Steam市场上取得显著成功，反映了AI技术在游戏产业中的实际应用价值。</p><p><strong>技术细节</strong>：这些游戏可能使用了AI生成内容（AIGC）技术，包括AI生成的图像、文本、音效或程序代码。AI技术帮助独立开发者以较低成本创建高质量的游戏内容，加速了游戏开发流程。</p><p><strong>行业影响</strong>：AI在游戏开发中的成功应用降低了游戏制作门槛，让更多独立开发者能够创建高质量游戏。这对游戏开发行业来说，意味着开发模式的变革和更多创新游戏的出现。</p><p><strong>商业意义</strong>：AI游戏的成功证明了AIGC技术的商业价值，鼓励更多开发者采用AI工具进行游戏开发，推动了整个游戏产业的AI化进程。</p><h3>AI绘画提示词新利器：PromptFill上线！让复杂Prompt像填空题一样简单</h3><p>AI绘画工具PromptFill上线，它将复杂的提示词生成过程简化为填空题形式，让普通用户也能轻松创建高质量的AI绘画提示词。这一工具降低了AI艺术创作的门槛。</p><p><strong>核心事件</strong>：PromptFill推出简化版提示词生成工具，通过模板化方式降低AI艺术创作难度。</p><p><strong>技术细节</strong>：该工具可能使用了自然语言理解模型来解析用户意图，并将其转换为有效的AI绘画提示词。通过分类和预设模板，用户只需填写关键信息即可生成高质量的提示词。</p><p><strong>行业影响</strong>：这一工具进一步降低了AI艺术创作的门槛，让更多非技术用户能够使用AI绘画工具。对AI艺术工具开发者而言，这展示了用户界面创新的重要性。</p><p><strong>商业意义</strong>：通过简化用户体验，PromptFill有望吸引更多用户使用AI艺术创作工具，推动相关市场的增长。</p><h3>Meta 智能眼镜重大更新:AI 助听功能上线，还能根据眼前的风景点歌</h3><p>Meta的智能眼镜迎来重大更新，新增了AI助听功能，还能根据用户眼前的风景智能推荐音乐。这种多模态AI应用展示了智能穿戴设备的未来发展方向。</p><p><strong>核心事件</strong>：Meta智能眼镜增加了AI助听和场景音乐推荐功能，实现了多模态AI应用。</p><p><strong>技术细节</strong>：这一功能整合了计算机视觉、音频处理和音频增强技术，能够实时分析周围环境，为听力障碍用户提供增强音频体验，同时根据场景推荐合适的音乐。</p><p><strong>行业影响</strong>：多模态AI应用展示了智能设备的未来发展方向，对可穿戴设备开发者来说，这是一个重要的技术演进方向。</p><p><strong>商业意义</strong>：通过增加实用的AI功能，Meta提升了智能眼镜的价值和用户粘性。</p><h3>AI"自动运维工程师"Resolve AI获Lightspeed领投A轮融资</h3><p>AI运维公司Resolve AI获得了Lightspeed的A轮融资，其AI自动运维工程师能够自主发现、诊断和修复IT系统中的问题，代表了IT运维自动化的新趋势。</p><p><strong>核心事件</strong>：Resolve AI的自主运维技术获得资本认可，标志着AI在IT运维领域的重要进展。</p><p><strong>技术细节</strong>：该AI运维系统可能基于机器学习算法，能够监控IT系统状态，自动识别异常，分析问题根源，并执行修复操作，实现真正的无人化运维。</p><p><strong>行业影响</strong>：AI自主运维将显著提升IT运维效率，减少人为错误，对IT运维从业者来说，需要适应新的工作模式。</p><p><strong>商业意义</strong>：这一领域获得了资本关注，预示着AI运维市场的巨大潜力。</p><h2>语音聊26分钟，80%用户成功约会！AI约会新贵Known获970万美元融资</h2><p>AI约会应用Known凭借其独特的语音交互功能获得了970万美元融资。该应用通过26分钟的语音聊天，成功率达80%，展示了AI在社交领域的创新应用。</p><p><strong>核心事件</strong>：Known AI约会应用通过语音AI匹配取得高成功率，获得资本认可。</p><p><strong>技术细节</strong>：该应用可能使用了语音识别、情感分析和匹配算法，通过分析用户语音中的情感、语调和内容来实现更精准的匹配。</p><p><strong>行业影响</strong>：AI在社交领域的应用展示了其在理解人类情感和社交需求方面的潜力。</p><p><strong>商业意义</strong>：这一成功案例为AI在社交应用领域开辟了新的商业模式。</p><h3>多智能体可信标准在ITU立项：信通院、蚂蚁、中国电信等共同推动</h3><p>由信通院、蚂蚁集团和中国电信等机构共同推动，多智能体可信标准在国际电信联盟(ITU)成功立项。这一标准将为多智能体系统的可信性提供国际规范，推动AI系统的安全可靠发展。</p><p><strong>核心事件</strong>：多智能体可信标准在ITU成功立项，标志着中国在AI国际标准制定中的重要贡献。</p><p><strong>技术细节</strong>：该标准将涉及多智能体系统的安全性、可靠性、可解释性和公平性等可信性指标。</p><p><strong>行业影响</strong>：标准化工作为AI技术的健康发展提供了基础，对AI开发者来说，需要关注并遵循相关标准。</p><p><strong>商业意义</strong>：标准的制定有助于建立可信赖的AI生态系统，促进AI技术的广泛应用。</p><hr/><p>你对今天的哪个新闻最感兴趣？欢迎在评论区分享你的看法。关注我，每天获取最新的AI行业动态。</p><p>📌 <strong>关注我，第一时间掌握更多AI前沿资讯！</strong></p>]]></description></item><item>    <title><![CDATA[专用蚊子苍蝇检测数据集（含背景样本）：适用于目标检测任务 逐梦AI ]]></title>    <link>https://segmentfault.com/a/1190000047494875</link>    <guid>https://segmentfault.com/a/1190000047494875</guid>    <pubDate>2025-12-23 00:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>专用蚊子苍蝇检测数据集（含背景样本）：适用于目标检测任务</h2><h3>一、背景</h3><p>随着城市化进程的加快和气候环境的变化，蚊子、苍蝇等害虫在居民生活、公共卫生以及工业场景中造成的问题日益突出。它们不仅影响生活环境质量，还可能传播多种疾病，对公共健康构成威胁。</p><p>传统的蚊虫监测方式大多依赖人工观察或简单的诱捕统计方法，存在 <strong>效率低、实时性差、误判率高</strong> 等问题。随着计算机视觉和深度学习技术的发展，<strong>基于目标检测的蚊子、苍蝇智能识别系统</strong> 成为一种高效、可扩展的解决方案。</p><p>然而，在实际工程落地中，模型效果的好坏往往不取决于算法本身，而是 <strong>数据集质量</strong>。因此，一个 <strong>标注规范、类别清晰、包含真实背景干扰样本的数据集</strong>，是构建高精度蚊虫检测系统的核心基础。</p><p>本文将详细介绍一套 <strong>专用蚊子苍蝇检测数据集（含背景样本）</strong>，并结合 YOLOv8 模型，探讨其在真实目标检测任务中的价值与应用。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494877" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>二、数据集概述</h3><p>本数据集是一个 <strong>面向目标检测任务的专业级蚊子/苍蝇数据集</strong>，专门为 <strong>YOLO 系列模型（尤其是 YOLOv8）</strong> 设计，适用于科研实验与工程实践。</p><h3>数据集下载</h3><blockquote>链接:<a href="https://link.segmentfault.com/?enc=vNFHs8k%2Bhq4fg7fLjhPkug%3D%3D.F5Z2xNfar5ucFGR0uTMoNr5GhqlBa9sNUFYdxtjDbuY%2BTRRq4LJ64nSZHLgcGSCon2b449UJE7YdQQ8RcnhObA%3D%3D" rel="nofollow" target="_blank">https://pan.baidu.com/s/1nIqYq6bvYjB-piufPKOP1w?pwd=dcar </a><br/>提取码:dcar 复制这段内容后打开百度网盘手机App，操作更方便哦</blockquote><p>专用蚊子苍蝇检测数据集（含背景样本）</p><p>蚊子和苍蝇数据集包含 1400 多张图片和 1400 多个 yolo 格式的 txt 文件。其中 600 多张是蚊子，600 多张是苍蝇，还有 200 多张用于背景。<br/>该数据集用于基于 yolov8 模型的苍蝇蚊子检测系统。</p><p>训练集图片数量: 576<br/>验证集图片数量: 145</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494878" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>数据集核心特点</h4><ul><li>🦟 <strong>蚊子样本</strong>：600+ 张</li><li>🪰 <strong>苍蝇样本</strong>：600+ 张</li><li>🌿 <strong>背景样本</strong>：200+ 张（无目标或复杂干扰）</li><li>📦 <strong>总图片数量</strong>：1500 张（已划分）</li><li>📝 <strong>标注格式</strong>：YOLO 标准格式（<code>.txt</code>）</li><li>🎯 <strong>任务类型</strong>：目标检测（Object Detection）</li><li>🚀 <strong>适配模型</strong>：YOLOv8 / YOLOv5 / YOLOv7 等</li></ul><p>通过引入 <strong>背景样本（Negative Samples）</strong>，数据集在真实环境中具备更强的泛化能力，有效减少误检与虚警。</p><hr/><h3>三、数据集详情</h3><h4>3.1 数据集结构</h4><p>数据集已经按照深度学习训练规范进行了划分，结构清晰，开箱即用：</p><pre><code>dataset/
├── train/
│   ├── images/
│   └── labels/
├── valid/
│   ├── images/
│   └── labels/</code></pre><p>对应数量如下：</p><table><thead><tr><th>数据划分</th><th>图片数量</th></tr></thead><tbody><tr><td>训练集</td><td>576</td></tr><tr><td>验证集</td><td>145</td></tr></tbody></table><blockquote>所有图片均配有对应的 YOLO 标注文件（<code>.txt</code>），背景样本则为空标注文件。</blockquote><hr/><h4>3.2 类别定义</h4><p>本数据集共定义 <strong>2 个目标类别</strong>：</p><table><thead><tr><th>类别 ID</th><th>类别名称</th></tr></thead><tbody><tr><td>0</td><td>mosquito（蚊子）</td></tr><tr><td>1</td><td>fly（苍蝇）</td></tr></tbody></table><p>标注遵循 YOLO 标准格式：</p><pre><code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code></pre><p>所有坐标均为 <strong>相对于图片宽高的归一化值</strong>，可直接用于 YOLOv8 训练。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494879" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047494880" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>3.3 背景样本的重要性</h4><p>在真实应用场景中，摄像头画面中 <strong>大多数时间并不存在蚊子或苍蝇</strong>。如果训练数据只包含目标样本，模型很容易出现：</p><ul><li>把噪点、阴影误识别为昆虫</li><li>对复杂纹理背景产生大量误报</li><li>实际部署效果严重劣化</li></ul><p>因此，本数据集特别加入了 <strong>200+ 张背景样本</strong>，包括：</p><ul><li>无蚊虫的室内环境</li><li>光照变化明显的背景</li><li>墙面、桌面、窗户等常见干扰元素</li></ul><p>这使模型在训练过程中学会 <strong>“什么时候不该检测”</strong>，显著提升实战可靠性。</p><hr/><h3>四、适用场景</h3><p>该蚊子苍蝇检测数据集可广泛应用于以下场景：</p><h4>4.1 智能家居与智慧安防</h4><ul><li>室内蚊虫监测</li><li>智能灭蚊设备触发</li><li>家庭环境健康评估</li></ul><hr/><h4>4.2 公共卫生与疾控监测</h4><ul><li>蚊媒疾病风险预警</li><li>社区环境蚊虫密度分析</li><li>智慧城市健康管理系统</li></ul><hr/><h4>4.3 工业与农业场景</h4><ul><li>食品加工厂虫害检测</li><li>农业温室环境监控</li><li>自动化虫害识别系统</li></ul><hr/><h4>4.4 AI 教学与科研实验</h4><ul><li>YOLOv8 目标检测教学案例</li><li>小样本检测与数据增强研究</li><li>背景负样本对模型泛化能力影响分析</li></ul><hr/><h3>五、目标检测实战：YOLOv8 训练示例</h3><h4>5.1 数据配置（data.yaml）</h4><pre><code class="yaml">path: dataset
train: train/images
val: valid/images

names:
  0: mosquito
  1: fly</code></pre><hr/><h4>5.2 启动训练</h4><pre><code class="bash">yolo detect train \
  model=yolov8n.pt \
  data=data.yaml \
  epochs=100 \
  imgsz=640 \
  batch=16</code></pre><p>YOLOv8 对小目标检测表现优秀，非常适合蚊子、苍蝇这类 <strong>尺度小、形态变化大的目标</strong>。</p><hr/><h4>5.3 训练效果提升建议</h4><ul><li>启用 Mosaic / MixUp 数据增强</li><li>适当提高输入分辨率（如 960）</li><li>使用 <code>yolov8s</code> 或 <code>yolov8m</code> 提升精度</li><li>增加背景样本比例，降低误检</li></ul><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494881" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>六、结语</h3><p>在目标检测任务中，<strong>数据集永远是模型性能的上限</strong>。</p><p>这套 <strong>专用蚊子苍蝇检测数据集（含背景样本）</strong>：</p><ul><li>覆盖真实应用场景</li><li>标注规范、结构清晰</li><li>针对 YOLOv8 深度优化</li><li>兼顾检测精度与泛化能力</li></ul><p>无论你是进行 <strong>AI 工程落地、科研实验，还是教学示范</strong>，该数据集都可以作为一个 <strong>高质量、可扩展的基础数据源</strong>。</p><p>如果你正在构建 <strong>蚊虫智能识别系统</strong>，那么从一套“懂场景”的数据集开始，往往比盲目调参更重要。</p>]]></description></item><item>    <title><![CDATA[程序员的伪年薪百万还能持续多久？ 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047494895</link>    <guid>https://segmentfault.com/a/1190000047494895</guid>    <pubDate>2025-12-23 00:02:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>前两天刷脉脉，看到一个帖子炸了——某大厂程序员晒出自己的年薪package，标题写着"年薪120万"，评论区一片羡慕。</p><p>但仔细一看，base只有40万，剩下的80万是股票、期权、年终奖的"理论值"。更狠的是，股票要分四年才能拿到，期权还没到行权期，年终奖能不能拿到还得看公司业绩。</p><p>这种伪年薪百万的游戏，还能玩多久？作为一个从大厂出来创业的过来人，我今天必须把这个话题说透了。</p><h2>1. 什么是伪年薪百万？</h2><p>先给不了解的人科普一下。现在很多大厂给程序员开offer，喜欢玩一个套路：把base、股票、期权、年终奖、各种补贴全加起来，算出一个"总包"（Total Compensation），然后告诉你"年薪XX万"。</p><p>比如一个典型的"年薪100万"offer可能是这样的：</p><ul><li>Base：35万</li><li>股票：40万（分四年，每年10万）</li><li>年终奖：15万（3-6个月，看绩效）</li><li>期权：10万（三年后才能行权，还得公司上市或被收购）</li></ul><p>听起来很美好对吧？但实际情况是：<strong>第一年你真正能拿到手的，可能只有50万左右</strong>。股票要慢慢解锁，期权能不能兑现还是未知数，年终奖拿多少全看公司心情。</p><p>更狠的是，如果公司股价跌了，你那40万的股票可能只值20万；如果公司裁员或者你没熬到解锁期就离职了，那些股票直接归零。</p><h2>2. 这个游戏为什么能玩起来？</h2><p>你可能会问，既然是伪年薪，为什么还有那么多人上当？这背后有几个原因。</p><p><strong>第一，信息不对称。</strong> 很多年轻程序员，尤其是应届生，根本搞不清楚股票、期权、RSU这些东西的区别。HR给你画大饼的时候，只会告诉你"总包多少"，不会详细解释每一部分的兑现条件和风险。等你真正入职了，才发现自己被套路了。</p><p><strong>第二，虚荣心作祟。</strong> 说实话，"年薪百万"这四个字太有杀伤力了。你在同学聚会上说自己年薪百万，立马就是人群中最靓的仔。至于这个百万是真百万还是伪百万，很多人根本不在乎，或者说不愿意承认。</p><p>我见过不少程序员，明明知道自己的年薪是"注水"的，但在外面还是说自己年薪百万。这种虚荣心，让大厂的套路越玩越溜。反正你愿意被骗，我就继续骗呗。</p><p><strong>第三，大厂需要这个故事。</strong> 对大厂来说，"年薪百万"是个很好的招聘工具。你想啊，如果告诉应届生"base只有35万，其他的都是不确定的"，谁还愿意来？但如果说"年薪100万"，立马就有一堆人抢着投简历。</p><p>而且，用股票和期权代替现金，对公司来说成本更低。现金是真金白银要从账上出去的，但股票只是一串数字，不需要实际支出。更狠的是，如果你没熬到解锁期就离职了，公司连这串数字都不用给你。这笔账，大厂算得门儿清。</p><h2>3. 这个泡沫什么时候会破？</h2><p>现在的问题是，这种伪年薪百万的游戏还能玩多久？我的判断是：<strong>这个泡沫已经在破了，而且会越破越快</strong>。</p><p><strong>第一个信号：大厂开始降薪裁员。</strong> 这两年你看看新闻，哪个大厂不在裁员？字节、腾讯、阿里、美团，裁员的消息一个接一个。裁员就算了，很多大厂还在降薪，取消大小周、砍年终奖、降低股票发放。那些拿着"年薪百万"offer入职的人，现在发现自己的实际收入可能只有当初承诺的60%-70%。</p><p><strong>第二个信号：股票和期权越来越不值钱。</strong> 前几年互联网大厂的股票一路涨，拿股票的人确实赚到了。但现在呢？你看看阿里、腾讯、美团的股价，跌得有多惨。很多人手里的股票，账面价值已经腰斩了。更别说那些还没上市的公司，给你的期权可能永远都兑现不了。</p><p>我做嵌入式和Linux这块，虽然不在互联网大厂，但也见过不少拿期权的人。有个做汽车电子的朋友，当年加入一家新能源车企，公司给了他一大堆期权，说是"未来价值千万"。结果公司融资失败，现在都快倒闭了，那些期权连废纸都不如。</p><p><strong>第三个信号：年轻人开始觉醒了。</strong> 现在的95后、00后程序员，比我们这代人聪明多了。他们不再迷信"年薪百万"的标签，而是会仔细算账：base多少？股票什么时候能解锁？年终奖的发放条件是什么？如果公司股价跌了怎么办？</p><h2>4. 真年薪和伪年薪的区别</h2><p>说了这么多，你可能会问：那怎么区分真年薪和伪年薪呢？我给你几个判断标准。</p><p><strong>第一，看现金占比。</strong> 如果base占总包的70%以上，那基本是真年薪。如果base只占50%甚至更低，剩下的都是股票、期权、年终奖，那就要小心了。</p><p><strong>第二，看股票的兑现条件。</strong> 如果股票是分四年解锁，而且公司股价稳定，那还算靠谱。但如果股票要分五年甚至更久，或者公司股价波动很大，那这部分收入的不确定性就很高了。</p><p>更狠的是期权。期权要等公司上市或被收购才能行权，这个周期可能是五年、十年，甚至永远等不到。我见过太多拿着期权的人，最后什么都没拿到。所以我的建议是：<strong>期权当作零，有就是惊喜，没有也不亏</strong>。</p><p><strong>第三，看年终奖的发放历史。</strong> 有些公司的年终奖是固定的，比如13薪、15薪，这种比较靠谱。但有些公司的年终奖完全看绩效和公司业绩，可能是3个月，也可能是0。如果你拿的offer里年终奖占比很高，一定要问清楚历史发放情况。</p><h2>5. 最后想说的</h2><p>伪年薪百万这个泡沫，本质上是互联网泡沫的一部分。前几年资本疯狂涌入，大厂疯狂扩张，给程序员开出天价offer。但现在资本退潮了，大厂开始降本增效，那些虚高的薪资自然也就撑不住了。</p><p>这不是坏事。泡沫破了，市场才能回归理性。程序员也才能更清楚地认识到，真正值钱的不是"年薪百万"的标签，而是你的技术能力、行业经验、解决问题的能力。</p><p>希望我的经历能给你一些启发。记住，职业发展是一场马拉松，不是百米冲刺。那些看起来跑得很快的人，可能只是在透支未来。真正能跑到最后的，是那些步伐稳健、持续积累的人。</p>]]></description></item><item>    <title><![CDATA[什么样的程序员在35岁以后依然被公司抢着要？ 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047494901</link>    <guid>https://segmentfault.com/a/1190000047494901</guid>    <pubDate>2025-12-23 00:01:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>35岁这道坎，是每个程序员都绕不开的话题。我今年刚过36，这几年做公司招人，也接触了不少35岁以上的程序员。有的人简历一投过来，我恨不得立马打电话约面试；有的人，说实话，简历看完就石沉大海了。</p><p>这个差距到底在哪？今天我就从一个创业者和技术管理者的角度，跟你聊聊什么样的程序员在35岁以后依然抢手。</p><h2>我见过的两种35岁程序员</h2><p>先说两个真实的例子，都是我这两年接触过的。</p><p><strong>第一个，老张，38岁，做嵌入式驱动开发的。</strong> 简历投过来的时候，我看他年纪有点大，本来有点犹豫。但一聊发现是个宝。他之前在一家做工控设备的公司待了十年，Linux内核那套东西玩得特别溜。关键是，他不光会写代码，还能把整个产品的技术架构给你讲清楚，从硬件选型到驱动适配到应用层优化，整条线都能搞定。</p><p><strong>第二个，小李，36岁，也是做Linux应用开发的。</strong> 简历看着挺漂亮，换了七八家公司，每家都待一两年。面试的时候我问他做过什么项目，他说做过很多，但细问下去，要么是在团队里打打下手，要么就是做一些边缘的功能模块。技术栈倒是写了一大堆，C、C++、Python、Shell都会，但每个都不精。</p><p>最要命的是，我问他对我们公司的业务有什么想法，他说"我就是想找个稳定的工作，技术上能学点东西就行"。这话一出，我基本就没兴趣了。36岁了，还是打工心态，还在想着"学东西"，这不是我要找的人。</p><p>这两个人的差距，就是35岁以后程序员的分水岭。</p><h2>第一种：有深度的技术专家</h2><p>作为一个从机械转行到嵌入式的过来人，我太清楚技术深度的重要性了。</p><p>35岁以后还抢手的程序员，第一个特点就是<strong>在某个技术领域有足够的深度</strong>。注意，是"某个领域"，不是"所有领域"。</p><p>我当年在那家500强外企的时候，部门里有个老师傅，40多岁了，就专门搞汽车电子的CAN总线通信。这东西说起来不算特别高大上，但他能把CAN协议的每个细节、每个坑都摸得透透的。车厂那边一有通信问题，第一个就找他。公司每年给他加薪，生怕他跑了。</p><p><strong>为什么技术深度这么重要？</strong> 因为到了35岁，你的价值不再是写代码的速度，而是解决疑难问题的能力。年轻人可以996肝代码，但遇到底层的、复杂的、需要经验积累的问题，他们就抓瞎了。这时候，有深度的老程序员就是救火队长，是定海神针。</p><p>但这里有个误区：很多人以为技术深度就是会用很多工具、很多框架。不是的。真正的技术深度，是<strong>理解原理，能解决别人解决不了的问题</strong>。</p><p>我举个例子。做嵌入式开发，会用Linux命令的人一抓一大把，但真正遇到系统启动卡死、内存泄漏、实时性不达标这些问题，能快速定位并解决的人，凤毛麟角。这才是技术深度。</p><h2>第二种：能独当一面的项目负责人</h2><p>第二种35岁以后依然抢手的程序员，是<strong>能独立负责项目的人</strong>。</p><p>这个我太有体会了。创业之后，我最头疼的就是找不到能独当一面的人。很多程序员，技术不错，但一让他负责一个项目，就各种问题：需求理解不到位、进度把控不住、跟客户沟通不顺畅、团队协作出问题。</p><p>而那些35岁以后还抢手的程序员，往往是能把一个项目从头到尾搞定的人。从需求分析、技术方案设计、团队分工、进度管理、风险把控到最后交付，整个流程都能handle住。</p><p>我去年接了一个智能家居的项目，涉及到嵌入式Linux、云平台对接、移动端APP联调，技术栈挺复杂的。我找了个35岁的老哥来负责，他之前在一家物联网公司做过类似的项目。整个项目他带着团队三个月就交付了，中间遇到的各种问题，他都能提前预判并解决。客户特别满意，后续又给了我们两个项目。</p><p>这种人，你说我能不抢着要吗？</p><p>这些能力，不是一两年能练出来的，需要大量的项目经验积累。这也是为什么35岁以后的程序员，如果有这些能力，反而更值钱。</p><h2>第三种：有创业思维的人</h2><p>最后一种，也是我认为最稀缺的，就是<strong>有创业思维的程序员</strong>。</p><p>什么叫创业思维？不是说你一定要去创业，而是<strong>你要像老板一样思考问题</strong>。</p><p>我见过很多35岁以后的程序员，还是打工心态：给我安排什么任务我就做什么，做完就下班，其他的不关我事。这种人，说实话，公司不会太重视。</p><p>而那些抢手的程序员，会主动思考：这个项目怎么做能帮公司省钱？怎么做能提高效率？怎么做能让客户更满意？他们不是为了完成任务而工作，而是为了创造价值而工作。</p><p>技术人员还是要有点商业思维。你做的技术再牛，如果不能转化成商业价值，那对公司来说就是成本。但如果你能把技术和商业结合起来，那你就是公司的核心资产。</p><p>我自己就是因为有这种思维，才能在28岁开始创业。我不仅会写代码，还会谈客户、做方案、算成本、控风险。这些能力，让我在二线城市实现了买房买车，有了自己的小公司。</p><h2>最后想说的</h2><p>说了这么多，我想说的是：35岁焦虑，本质上不是年龄焦虑，而是能力焦虑。</p><p>如果你35岁了，还只是个普通的码农，技术不深、项目经验不足、行业积累不够、学习能力下降、思维还停留在打工层面，那确实会被淘汰。但如果你在这三个方面中的任何一个做到了极致，那你不仅不会被淘汰，反而会越来越值钱。</p><p>希望我的经历和思考，能给你一些启发。不管你现在多大年纪，都要记住：年龄不是问题，能力才是。</p>]]></description></item><item>    <title><![CDATA[别再浪费内存了：Python __slots__ 机制深入解析 本文系转载，阅读原文
https:/]]></title>    <link>https://segmentfault.com/a/1190000047494694</link>    <guid>https://segmentfault.com/a/1190000047494694</guid>    <pubDate>2025-12-22 23:04:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Python 对象的灵活性大家都知道，可以随时给对象添加属性：</p><pre><code>class User:  
    pass  
u = User()  
u.name = "Alice"  
u.age = 30</code></pre><p>但这种灵活性的代价也很大，每个普通 Python 对象都有个 <code>__dict__</code> 字典来存储属性，对象一多内存开销就上来了，这时候 <code>__slots__</code> 就派上用场。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494696" alt="" title=""/></p><h2><strong>slots</strong> 到底在干什么</h2><p><code>__slots__</code> 让你提前声明类会用到哪些属性：</p><pre><code>class User:  
    __slots__ = ["name", "age"]  
    def __init__(self, name, age):  
        self.name = name  
        self.age = age</code></pre><p>这样做之后对象就不会再创建 <code>__dict__</code> 了。属性存储变成静态的，查找和赋值都快了，尤其是创建大量实例时内存占用能降不少。</p><h2>底层存储机制的差异</h2><p>普通对象把每个属性当作 <code>__dict__</code> 里的键值对访问属性就要做哈希查找。而用了 <code>__slots__</code> 之后每个属性在内存里有个固定位置，访问变成了直接的数组索引操作省掉了字典查找的开销。</p><p>说白了就是把 Python 对象存储搞得像 C 的结构体一样紧凑。</p><h2>内存能省多少</h2><p>看个对比：</p><pre><code>class Normal:  
    def __init__(self):  
        self.a = 1  
        self.b = 2  
        
class Slotted:  
    __slots__ = ["a", "b"]  
    def __init__(self):  
        self.a = 1  
        self.b = 2</code></pre><p>如果要创建几百万个实例，用 <code>__slots__</code> 的版本能少用 50-70% 的内存。</p><p>属性访问快主要是因为：省掉了字典查找、不用算哈希值、也没有额外的内存间接访问，属性访问时间一般能减少 20-40%。</p><h2>使用限制</h2><p><code>__slots__</code> 也不是完美的，有些限制得注意。</p><p>最明显的是不能随便加属性了：</p><pre><code class="python">u = User()  
u.address = "NYC"  # ❌ AttributeError</code></pre><p>继承的时候也麻烦，子类得定义自己的 <code>__slots__</code>，而且混用带slots和不带slots的类要小心。多重继承更复杂只有slots名不冲突才行。</p><p>另外默认不支持弱引用，要用的话得在 <code>__slots__</code> 里显式加上 <code>__weakref__</code>。</p><h2>什么场景适合用</h2><p>几个典型场景：处理大数据集时有几百万个对象；科学计算里的轻量数据结构、游戏引擎里的实体和粒子系统等等，总之就是那些对内存和速度敏感的地方。</p><h2>一些实用技巧</h2><p>配合类型提示用能让代码更清晰。可以和 <code>@property</code> 装饰器结合，该灵活的地方还是保持灵活。空类直接用 <code>__slots__ = ()</code> 把开销降到最低。</p><h2>总结</h2><p><code>__slots__</code> 就是让你用灵活性换内存效率和更快的属性访问。对于高性能场景来说这是个必须掌握的优化手段。</p><p>就算项目暂时不缺内存，理解 <code>__slots__</code> 本身也很有价值。它能让你明白 Python 对象是怎么存属性的、属性查找为什么有成本、Python 在灵活性和效率之间怎么权衡。</p><p>这些知识对系统设计、性能调优、排查问题都有帮助。</p><p><a href="https://link.segmentfault.com/?enc=gR9fNJI0fv4OPYDP8oCktg%3D%3D.HPp6tK1EjLoukqJ9v3wKTTzBVguH1gwEQS0eQrl0sU5e%2Fhs%2BD74sUiNgFdPjYcbdbuBpVf56e43iHC4mI1T4Pw%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/c18314c17e0047358cb13c0a990067ae</a></p><p>作者：Elshad Karimov</p>]]></description></item><item>    <title><![CDATA[《高质量游戏攻略与视频的优先级展示机制构建指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047494714</link>    <guid>https://segmentfault.com/a/1190000047494714</guid>    <pubDate>2025-12-22 23:03:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>深夜的游戏社区里，不同需求的玩家都在经历着相似的困境—刚入坑开放世界游戏的新手，想找隐藏副本的触发路径，刷到的却是重复剪辑的战斗混剪，连关键NPC的位置都没有标注；深耕竞技游戏的核心玩家，渴望学习新版本的战术拆解，置顶内容却还是三个月前的基础操作教学，毫无参考价值；专注单机剧情的玩家，想解锁隐藏结局的关键选择，搜索结果里满是标题党视频，点进去全是无关的剧情吐槽。这种优质内容被海量低质信息淹没的现象，本质上是传统推荐机制陷入了“流量加权”的怪圈，只看重播放量、点赞量等表面数据，却忽略了内容本身的实用价值与用户的真实需求。游戏内容的核心竞争力，从来不是短暂的流量爆发，而是能否为玩家提供可落地的技巧、有深度的解析或沉浸式的体验。构建一套自动识别并优先展示高质量内容的机制，相当于为优质创作者与精准需求玩家搭建了一条专属通道，它需要跳出单一数据的评判逻辑，深入挖掘内容的质感内核，通过对场景、行为、信息的多维度拆解，让真正有价值的内容自然突破信息壁垒，这不仅是对创作者心血的尊重，更是维系游戏社区生态活力、提升用户粘性的关键所在。</p><p>要实现高质量游戏内容的精准识别，首要任务是拆解“内容质感”的核心维度，建立一套多维度、立体化的评估体系，而非被单一指标牵着鼻子走。很多平台之所以陷入“劣币驱逐良币”的困境，就是因为将播放量作为核心评判标准，却没意识到播放量背后的虚假繁荣—有的内容靠夸张标题和封面吸引点击，用户打开3秒就果断退出，这种“无效播放”根本不能反映内容价值；而有的内容虽然初始播放量不高，但用户会完整看完，甚至反复拖拽进度条回看关键知识点，评论区里满是真诚的提问与补充，这种“有效互动”才是内容价值的真实体现。因此，机制构建的第一步，是重新定义“高质量”的核心特征：从内容属性来看，优质攻略必须具备结构完整性、信息独特性与实操适配性，比如开放世界游戏的攻略，不仅要标注资源点的精准坐标，还要说明不同职业的适配策略、避开环境陷阱的细节，甚至补充资源刷新的时间规律，而非简单罗列流程；优质视频则需兼具画面质感、表达清晰度与内容稀缺性，比如竞技游戏的操作视频，不能只放击杀集锦，还要有慢动作解析、战术思路的实时讲解，甚至针对不同段位玩家的适配建议，让不同水平的用户都能有所收获。从用户行为来看，高质量内容往往伴随高完播率、长平均停留时长、有效互动率（如评论区的深度提问、有价值的补充建议、收藏后的二次分享）等特征，这些隐性数据能更真实地反映用户对内容的认可程度。此外，还需纳入创作者维度的辅助参考，比如长期深耕垂直领域、粉丝互动质量高、无搬运抄袭等违规记录的创作者，其内容的优质概率确实更高，但这一维度只能作为补充，不能成为核心权重，要避免陷入“头部创作者特权”的误区，给新人创作者的优质内容留出公平的曝光空间，让社区始终保持新鲜血液的注入。</p><p>数据采集与预处理是机制落地的基础，这一步的核心是穿透表面数据的迷雾，挖掘能反映内容本质的有效信息，同时建立严格的异常数据过滤机制，确保识别逻辑的准确性。传统的数据采集模式往往局限于播放量、点赞量、转发量等显性指标，但这些数据极易被刷量行为操纵，比如通过脚本批量点击、雇佣水军刷评论，导致优质内容被虚假数据挤压，识别机制彻底失效。因此，必须拓展数据采集的维度，纳入更多能反映用户真实行为的隐性数据—比如用户的停留轨迹，是否在关键知识点处暂停、反复拖拽进度条回看，是否完整看完后又点击了创作者的其他相关内容；互动质量的细分，评论内容是否包含具体的问题（如“这个技能的冷却时间是多少”“隐藏任务的触发条件有等级限制吗”）、有价值的补充建议，而非无意义的表情、刷屏文字或简单的“打卡”“沙发”；甚至可以通过后台数据间接验证内容的实用价值，比如用户在观看攻略后，游戏内的任务完成率是否提升、特定操作的成功率是否增加。同时，要建立一套完善的异常数据过滤机制，通过行为链路溯源来识别刷量行为：比如某条内容的播放量在1小时内激增数万，但用户的平均停留时长不足10秒，评论内容高度同质化，且IP地址集中在同一区域，甚至存在同一设备多次点击的情况，即可判定为异常数据，直接降低其权重或纳入低质内容池。数据预处理过程中，还需对不同类型的游戏内容进行场景化分类，比如将攻略细分为新手引导、进阶技巧、隐藏内容挖掘、版本更新解析、故障排查（非Bug类）等，将视频细分为操作教学、剧情解析、娱乐集锦、赛事复盘、创作者杂谈等，不同类型的内容采用差异化的评估指标权重，比如新手引导类攻略更看重清晰度与完整性，进阶技巧类则更看重独特性与实操性，娱乐集锦类视频更看重画面质感与趣味性，这样才能让识别逻辑更贴合不同场景的用户需求。</p><p>特征提取与动态权重分配是机制的核心所在，它需要让识别逻辑从“被动统计数据”转向“主动理解内容价值”，真正抓住优质内容的核心特质。特征提取不能停留在数据表面，而要深入内容的信息内核：对于文字攻略，可通过语义分析提取关键信息点的密度与独特性，比如是否包含全网稀缺的信息（如隐藏任务的触发暗号、专属装备的获取路径）、逻辑是否连贯（是否有明确的步骤顺序、因果关系）、是否针对不同用户群体提供差异化建议（如新手与老玩家的不同玩法思路）；对于视频内容，则可通过画面分析与语义理解提取信息密度与表达质量，比如是否有清晰的字幕标注、关键操作的特写镜头、图文结合的讲解方式，语速是否适中、语言是否专业且易懂，画面是否稳定、剪辑是否流畅，是否能准确传达核心知识点。权重分配不能采用固定不变的模板，而要建立动态调整机制，根据游戏类型、社区用户画像、内容生命周期进行实时优化。比如在某款竞技游戏的版本更新初期，玩家对新英雄的技能解析、出装思路需求迫切，此时可提高“版本适配性”“信息时效性”的权重，让最新的优质攻略快速获得曝光；当社区内新手用户占比激增时，可适当提高“清晰度”“完整性”的权重，降低“独特性”的权重，让基础扎实的优质内容优先展示，帮助新手快速入门；而对于成熟的游戏社区，玩家的需求更多集中在深度解析与独特玩法上，此时可提高“独特性”“深度解析”的权重，鼓励创作者产出更有价值的内容。此外，还需引入“用户反馈闭环”来持续优化权重分配，比如通过用户对内容的举报（低质、误导）、纠错、好评等功能，收集机制识别失误的案例，比如某条优质小众攻略因为初期互动量低被低估，或者某条低质内容靠刷量获得高权重，通过这些案例反向调整特征提取的维度与权重比例，让机制的识别精度不断提升。</p><p>动态校验与场景化适配是确保机制长期有效的关键，它需要让筛选逻辑能够敏锐捕捉游戏行业的迭代节奏与用户需求的变化，避免陷入“一劳永逸”的僵化状态。游戏内容的价值具有强烈的时效性与场景性，一款游戏的新版本上线后，旧版本的攻略可能瞬间失去实用价值；不同玩家群体的需求差异也极为明显，新手玩家需要基础操作与入门指南，核心玩家则追求深度战术与隐藏内容，休闲玩家更倾向于娱乐向视频。因此，机制必须建立动态校验体系：通过后台数据实时监控不同类型内容的展示效果，比如某类内容的曝光率很高但完播率极低，可能是权重分配不合理，需要及时调整评估指标；通过定期的用户调研收集需求反馈，了解玩家在不同阶段、不同场景下的内容偏好，比如玩家在工作日晚间更倾向于实用型攻略，希望利用碎片时间提升游戏技巧，而在周末则更偏爱娱乐向视频，用于放松消遣，机制可根据时段动态调整内容展示比例。场景化适配还需针对不同游戏类型定制差异化的识别模型，比如开放世界游戏的内容更看重探索性与独特性，评估维度应侧重隐藏内容挖掘、资源点精准度；竞技游戏的内容更看重实操性与时效性，评估维度应侧重战术解析、版本适配性；单机游戏的内容更看重剧情解析与情感共鸣，评估维度应侧重剧情深度、画面质感。同时，要兼顾内容生态的多样性，避免机制过度聚焦某一类内容导致社区生态单一，可通过设置“多样性权重”，在保证高质量的前提下，适当提升小众优质内容的曝光机会，比如独立游戏的攻略、冷门玩法的解析，鼓励创作者探索更多元的内容形式，让社区内容生态更加丰富。</p><p>机制落地后的效果验证与持续优化，是构建良性循环的核心，它需要用真实数据反馈指导迭代方向，让筛选逻辑不断贴近用户的真实需求与行业的发展趋势。效果验证不能只看单一数据指标，而要建立多维度的评估体系：比如优质内容的曝光率提升比例，这能直接反映机制是否有效挖掘了高质量内容；用户的平均停留时长、有效互动率的变化，这能体现用户对展示内容的认可程度；创作者产出优质内容的积极性，比如优质内容的更新频率、新创作者的入驻数量、创作者对社区的满意度，这能反映机制是否真正激励了优质创作。通过这些数据可以全面判断机制的运行效果，比如某平台上线机制后，优质内容的曝光率提升了40%，用户平均停留时长增加了25%，评论区有效互动率增长了30%，新创作者入驻数量环比增长了20%，这说明机制起到了积极作用。同时，要主动关注机制可能存在的漏洞，比如某些技术性较强的小众攻略，因为理解门槛高，初期的完播率和互动量较低，容易被机制误判为低质内容，此时可通过设置“新内容扶持权重”，对新发布的内容给予一定的初始曝光量，根据后续的用户反馈再调整展示优先级；比如某些直播回放类内容，虽然互动数据不如短视频，但包含完整的战术讲解与实操演示，价值极高，机制需要适配这类新兴内容形式，加入专属的评估维度，如直播过程中的观众提问解答效率、核心技巧的讲解时长、回放的收藏率等。</p>]]></description></item><item>    <title><![CDATA[《前端实人核验与后端未成年人社交内容管控实操指南》 程序员阿伟 ]]></title>    <link>https://segmentfault.com/a/1190000047494720</link>    <guid>https://segmentfault.com/a/1190000047494720</guid>    <pubDate>2025-12-22 23:03:18</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>年龄防护从来不是孤立的核验环节，而是贯穿用户从注册到使用全流程的双向管控体系，前端需搭建高防绕、高可信的身份核验屏障，后端要构建精细化、可追溯的权限管控网络，两者形成无缝联动，才能真正抵御未成年人接触不适配内容、陷入不良社交的风险。前端的核心诉求是“在不降低成年用户体验的前提下，提升验证可信度”，后端的关键目标是“基于真实年龄数据，实现差异化权益管控”，只有让每一次验证结果都成为后端权限分配的唯一依据，让每一次内容消费、社交互动都经过年龄权限的刚性校验，才能既满足合规要求，又切实履行产品对未成年人的保护责任，让防护不再是停留在表面的口号。</p><p>前端年龄验证的核心挑战，在于打破“简单输入即通过”的形式化困境，在严格防绕与流畅体验之间找到平衡点—既不能因流程过于繁琐导致成年用户流失，也不能因验证门槛过低让未成年人轻易规避。传统的手动输入出生日期模式，存在明显的漏洞，用户可随意填写虚假信息，甚至通过清除浏览器缓存、修改设备时间等方式重复尝试，因此需要构建多维度、分层级的核验体系，层层递进提升身份可信度。首先，基础层采用可视化日历组件替代手动输入，组件需预设合理的年龄范围（如仅允许选择18年前及更早的日期），屏蔽未来日期与不符合逻辑的年龄选择，同时加入日期选择逻辑校验，比如若用户选择的年份对应的年龄明显与行为特征不符（如选择1950年出生却频繁使用青少年偏好的功能），则触发风险提示。进阶层引入设备与行为画像交叉验证，通过分析设备历史登录记录、常用地域、使用时段、内容偏好等数据，构建用户行为基线，若首次登录设备与常用设备差异较大，或选择的出生日期与行为基线严重背离（如自称50岁却长期在晚间22点后高频使用），则启动二次验证。二次验证可接入权威数据源辅助核验，如在获得用户明确授权后，对接实名信息校验服务，通过姓名与身份证号的一致性比对确认真实年龄，或采用人脸核验技术，结合生物特征与年龄区间的匹配度判断，进一步提升验证结果的可信度。同时，前端需对验证数据进行脱敏与加密传输，避免完整出生日期在传输过程中被篡改，本地仅存储年龄分段标识（如“12岁以下”“12-16岁”“18岁以上”），既保障用户隐私，又降低数据泄露风险。在体验优化上，将验证流程嵌入首次登录或核心功能（如观看特定内容、发起社交互动）的入口，避免重复验证；对于已通过高可信验证的用户，后续登录可通过设备指纹快速校验，直接沿用年龄权限标识，减少不必要的操作成本。</p><p>前端核验的结果并非终点，后端需建立“年龄可信等级”评估体系，对前端传输的数据进行二次校验与权限绑定，确保每一次用户操作都经过刚性权限过滤，杜绝“前端验证通过、后端放任不管”的脱节问题。首先，后端需对前端传输的验证数据进行完整性与加密有效性校验，通过校验算法核对数据签名，剔除被篡改、缺失关键信息或加密失效的异常数据，对于未通过校验的用户，直接限制其访问核心功能，并返回明确的提示信息。其次，根据前端验证方式的可信度，划分不同的年龄可信等级：仅通过基础日历选择验证的用户为“低可信等级”，需进一步限制高风险功能（如私信、评论）的使用；通过设备行为交叉验证的用户为“中可信等级”，可开放部分中等风险功能；通过权威数据源或人脸核验的用户为“高可信等级”，可正常使用对应年龄适配的全部功能。后端数据库需对年龄信息与可信等级进行字段级加密存储，采用权限分级访问机制，仅授权服务可读取相关数据，同时记录数据修改日志，确保每一次权限变更都可追溯。建立实时同步机制，当用户年龄自然增长（如未成年人成年）或验证等级提升（如从低可信升级为高可信）时，后端需自动更新权限规则，无需用户手动操作；若发现用户存在伪造身份、篡改验证结果的行为，立即将其可信等级降至最低，并限制账号功能，情节严重的予以封禁。此外，后端需通过接口签名校验、请求来源合法性验证、访问频率限制等方式，拦截非法请求，杜绝黑客通过模拟合法请求、篡改参数等方式绕过年龄权限管控的行为，确保前端核验与后端权限管控形成闭环，让防护无死角。</p><p>未成年人的内容消费限制，核心是构建“内容-年龄”精准适配机制，通过内容分级与动态权限管控，实现差异化的内容分发，既不让未成年人接触不适配内容，也不影响成年用户的正常使用。首先，需建立科学的内容分级体系，结合法律法规要求与产品定位，将平台内容划分为不同的年龄适配等级，如“全年龄段适配”“12岁以上适配”“16岁以上适配”“18岁以上适配”，分级维度不仅包括内容主题（如暴力、恐怖、婚恋），还涵盖语言风格（如成人化表达、网络俚语）、视觉元素（如血腥画面、暴露图像）、价值观导向（如拜金、攀比）等，确保分级的全面性与准确性。后端需搭建内容分级数据库，将每一条内容（包括平台自制内容与用户上传的UGC内容）与对应的适配年龄绑定，同时构建智能内容审核引擎，对UGC内容进行实时分级校验，若发现未分级或分级错误的内容，立即拦截并退回修改，避免不适配内容流入平台。在内容分发层面，后端根据用户的年龄可信等级与内容适配等级进行精准匹配：对于12岁以下未成年人，仅展示全年龄段适配的内容，过滤所有含风险元素的内容，同时设置单日内容消费时长阈值（如1小时），累计使用时长达到阈值后，自动锁定娱乐类内容，仅保留教育类、益智类内容的访问权限，并向监护人发送使用提醒；对于12-16岁未成年人，可开放“12岁以上适配”的内容，屏蔽“16岁以上”及“18岁以上”的内容，消费时长限制为工作日单日1.5小时、节假日3小时，同时在使用过程中每隔45分钟弹出休息提醒；对于16-18岁未成年人，可开放“16岁以上适配”的内容，仍禁止访问“18岁以上适配”的内容，消费时长不做强制限制，但保留时长统计与提醒功能。此外，后端需对未成年人的内容消费行为进行轨迹记录，若发现其频繁搜索、访问高风险内容，或通过切换账号、清除记录等方式试图规避限制，立即触发预警，加强管控力度，如缩短当日剩余使用时长、限制搜索功能等。</p><p>社交功能是未成年人面临风险的高发场景，后端需构建“年龄梯度化社交权限”体系，根据用户年龄划分不同的社交互动边界，从关系建立、互动行为、隐私保护三个维度进行全流程管控，降低不良社交风险。首先，限制社交关系建立权限：对于12岁以下未成年人，禁止发起陌生人社交，仅允许与已通过监护人授权的亲友建立社交关系，亲友关系建立需通过双向验证（如输入监护人设置的验证码）；对于12-16岁未成年人，可允许添加陌生人，但限制每日添加次数（如5次），添加前需展示对方的年龄区间、兴趣标签等非隐私信息，同时提供“拒绝添加”“拉黑”“举报”等功能，赋予用户自主选择权，添加后需向监护人同步社交关系变化；对于16-18岁未成年人，放宽添加次数限制，但保留添加提醒功能，让用户明确知晓社交行为的边界。其次，规范社交互动行为：屏蔽未成年人社交场景中的高风险互动功能，如私信功能仅允许与亲友或已添加3天以上的好友使用，且消息内容需经过关键词过滤与语义分析，禁止发送含隐私信息、不当言论、诱导性内容的消息；评论功能限制发言频率（如每分钟不超过3条），过滤风险词汇与恶意言论，禁止@陌生人或批量@他人；群聊功能方面，未成年人创建群聊需限制人数上限（如20人），群聊主题需经过审核，禁止创建含不适配主题的群聊，同时赋予群主与管理员对未成年人用户的管理权限，可限制其发言频率或移出群聊。在隐私保护上，后端自动屏蔽未成年人个人主页中的年龄、地理位置、联系方式等敏感信息，仅展示昵称、头像、兴趣标签等非隐私内容，限制陌生人查看其社交动态与历史互动记录，若未成年人试图发布含隐私信息的内容，立即弹出风险提示并拦截。建立社交风险预警机制，通过分析社交行为数据（如频繁与陌生人互动、接收可疑链接、发送敏感信息、被多人举报等），构建风险评分模型，当评分达到阈值时，自动触发预警，采取限制社交功能使用、通知监护人等干预措施，防范潜在风险。</p><p>机制的落地并非一劳永逸，需要建立全链路的监控、反馈与迭代体系，通过数据驱动持续优化管控策略，在强化保护力度的同时，不断提升用户体验，确保机制的有效性与适应性。首先，构建多维度监控指标体系，包括年龄验证通过率、绕过验证尝试次数、未成年人违规行为发生率、内容适配准确率、社交风险预警次数、用户投诉率等，通过实时监控掌握机制运行状态，若发现某一指标异常（如绕过验证尝试次数激增），立即排查原因并采取针对性措施。其次，建立用户反馈收集渠道，通过APP内问卷、客服反馈、社群交流等方式，收集成年用户与未成年人监护人对验证流程、权限限制的体验评价，针对用户反映的问题（如验证流程繁琐、内容限制过于严格、社交权限不合理等）进行专项优化，在不降低防护标准的前提下，提升体验流畅度。密切关注法律法规与行业标准的更新动态，安排专人跟踪相关政策变化，及时调整内容分级标准、权限限制规则与验证方式，确保机制始终符合合规要求。定期开展安全测试与压力测试，模拟黑客绕过验证、篡改权限、非法访问等场景，检验机制的安全性与稳定性，针对测试中发现的漏洞，快速推进修复与迭代；同时，进行灰度发布与效果评估，将优化后的机制先面向部分用户开放，通过对比测试数据与用户反馈，确认效果后再全面推广。</p>]]></description></item><item>    <title><![CDATA[理解人类意图 烦恼的蜡烛 ]]></title>    <link>https://segmentfault.com/a/1190000047494807</link>    <guid>https://segmentfault.com/a/1190000047494807</guid>    <pubDate>2025-12-22 23:02:21</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着人工智能（AI）的蓬勃发展，许多金融专业人士都在权衡两个主要问题：人工智能市场是否存在泡沫，如果存在，它是否应该与21世纪初的互联网时代相提并论？</p><p>　　电影《大空头》真实原型之一、知名交易员Danny Moses认为这两个问题的答案都是肯定的。虽然他不否认人工智能热潮是真实存在的，而且是一个长期增长的故事，但他也看到了这两次科技热潮之间的强烈相似之处，这表明投资者需要谨慎行事。</p><p>　　Moses因在次贷危机期间做空房地产市场而一战成名，他在最新采访中谈到了人工智能市场正在出现的潜在问题，以及他认为投资者应该如何应对这个快速发展的领域。</p><p>　　“增长是真实的，但数学计算不正确，”他说：“我认为，我们正在达到一个数学不起作用的地步。”</p><p>　　Moses强调，他对AI市场潜在问题的看法并不是呼吁做空该行业。相反，他指出，这是在呼吁投资者做好功课，找到合适的公司，以便在市场持续增长的情况下获得有价值的敞口。</p><p>　　在他看来，这意味着要坚持投资科技行业最具主导地位的公司，这些公司拥有继续扩张的资源，而且不像一些规模较小的公司那样受到种种限制。最好的例子包括亚马逊、谷歌、Meta和微软。</p><p>　　“他们可以在任何时候减少资本支出，而且他们的现金流仍然是正的，而其他公司则依赖于人工智能领域的支出。”他补充道。</p><p>　　不过，Moses并非看好所有的大型科技公司。他以甲骨文为例，说明了人工智能市场存在的问题，并指出该公司的高债务水平和完成科技客户订单所需的现金方面存在风险。他还特别提到波动性较大的科技股，如超微电脑和CoreWeave，认为它们是人工智能领域风险较高的投资标的。</p><p>　　不过，在他看来，投资者终于开始考虑到这样一个事实，即并非所有的人工智能股票都是“生儿平等”的，因为相对表现优异和表现不佳之间的差距变得越来越难以忽视。</p><p>　　“我认为这证明投资者开始分辨交易的赢家和输家，他们更愿意投资那些资产负债表更稳健、更有保障的企业来表达人工智能主题，”他补充道。</p><p>　　Moses还表示，他看好铀，因为这种金属越来越被吹捧为人工智能建设的关键组成部分，这将是未来几年维持该行业所必需的。</p><p>　　由于美国电力供应紧张，一些数据中心倾向采用小型核电站作为主力电源，而铀正是核电站的主要原料。</p><p>　　“我喜欢的交易之一是铀，从主题上讲，这应该是可行的，但需要很长一段时间。人们认为企业将经历人工智能增长的时机，与推动人工智能增长所需的基础设施实际上存在不匹配。”他补充道。</p><p>　人形机器人板块12月4日早盘表现强势，华伍股份、骏亚科技、巨轮智能、睿能科技、龙溪股份纷纷涨停；三协电机、德马科技、江苏雷利则大涨超10%。此外，机器人执行器、减速器、同步磁阻电机等相关板块也涨幅靠前。<br/>　　人形机器人消息不断</p><p>　　消息面上，近期有关于人形机器人的利好新动态不断涌现。据中国基金报援引报道称，在发布加速人工智能发展计划五个月后，特朗普政府开始将目光转向机器人。此前，美国商务部长卢特尼克一直在与机器人行业的首席执行官们会面，并“全力以赴”加速该行业的发展。特朗普政府正在考虑明年发布一项关于机器人技术的行政令。据报道，一位知情人士透露，交通部也正准备宣布成立一个机器人工作组，可能在年底前公布。受此影响，隔夜美股的机器人概念股表现强势，iRobot收涨73.85%，Serve Robotics收涨18.24%。<br/>　　此外，特斯拉CEO马斯克在北京时间12月3日在社交平台转发了特斯拉擎天柱（Optimus）团队发布的一段“擎天柱”人形机器人跑步的短视频。<br/>　　12月2日，众擎机器人宣布，全尺寸极致高效能通用人形机器人众擎T800正式发布，产品发售进程也随即正式启动。同一天，阿童木机器人正式发布迭代版全栈自研人形机器人“天兵一号ATOM01”。</p><p>　　政策环境持续友好</p><p>　　从政策来看，从2025年蛇年春晚舞台的机器人扭秧歌，到北京亦庄的机器人马拉松，再到浙江杭州的机器人格斗赛……人形机器人正逐渐“破圈”，从“实验室”迈向各类“应用场”。而这背后，与政策环境的友好是密不可分的。</p><p>　　今年以来，以人形机器人为典型业态的具身智能成为我国培育未来产业的重要方向。北京、上海、广东深圳、浙江杭州等多地密集出台专项政策，形成了一场面向未来的产业竞逐。</p><p>　　作为全国较早将“具身智能”写入地方政府工作报告的省份，广东在今年2月明确提出，要加快启动布局人形机器人等重点领域研发项目。除了政策支持，北京、上海、深圳等10余个地方政府已建立或筹备建立相关产业基金。</p><p>　　从企业来看，头部企业已率先开启证券化。今年以来，宇树科技、乐聚智能、智元+k.机器人等人形机器人头部整机厂密集启动IPO、并购上市等资本化动作，行业开始迈入“产业化+资本化”双轮驱-+动发展阶段。<br/>　　融资客抢筹前20个股</p><p>　　从杠杆资金角度来看，部分人形机器人概念也被积极抢筹。比如瑞芯微，国庆后融资客融资净买入3.43亿元，该股前三季度归母净利润7.8亿元，同比大增121.65%。东方精工紧随其后，融资客融资净买入3.13亿元，前三季度赚了5.1亿元，同比增54.64%。东阳光居第三位，被融资净买入2.41亿元，前三季度赚了9.06亿元，同比大增189.8%。<br/>研发投入占比前20个股</p><p>　　而从研发投入占营收比角度来看，东方财富Choice数据显示，安路科技以69.45%排在首位。帝奥微紧随其后，研发投入占比为35.22%。当虹科技、创耀科技、芯朋微排名也靠前。<br/>　　2026年迎量产元年？</p><p>　　往后看，“2026年是人形机器人的量产元年，当前临界点已至。”开源证券分析师孟鹏飞指出，海外特斯拉和国内产业进展持续加速，后续催化因素较多。展望2026年，人形机器人将进入量产期，大厂躬身入局，政策支持和补贴有望进入实际阶段，“趋势走强、景气上行”的布局窗口已然开启。而国家发展改革委健全具身智能准入与退出机制、营造公平竞争环境的举措，既正向引导行业迈向良性发展轨道，也释放出人形机器人相关支持政策或已逐步临近的信号。</p><p>　　高工机器人产业研究所（GGII）数据显示，2024年全球人形机器人市场规模约10.17亿美元，预计2030年将达150亿美元，年复合增长率超56%；同期销量从1.19万台增至60.57万台。中国市场前景也很广阔，2030年规模预计达380亿元人民币，销量跃升至27.12万台，占全球份额44.77%。</p><p>　　不过，随着人形机器人的关注度提升，市场上有关于“速度”与“泡沫”的讨论也多了起来。国家发展改革委政策研究室副主任李超此前表示，“速度”与“泡沫”一直是前沿产业发展过程中需要把握和平衡的问题，这对于具身智能产业来讲，也是一样的。当前，人形机器人在技术路线、商业化模式、应用场景等方面尚未完全成熟，随着新兴资本的加速入场，我国目前已有超过150家人形机器人企业，这个数量还在不断增加，其中半数以上为初创或“跨行”入局，这对鼓励创新来讲是一件好事；但也要着力防范重复度高的产品“扎堆”上市、研发空间被压缩等风险。面对机遇与挑战并存的局面，关键在于合理引导。</p><p>11月摩根士丹利新发布的一份研究报告中预测，苹果这家行业巨头正在逐步推进他们的人形机器人计划，想要打造下一个超级增长引擎；结合此前8月份彭博社等财经媒体的相关报道，机器人市场可能真的要在不久的将来迎来苹果这头“巨鲸”了。</p><p>苹果为什么要在此时开始加速下注机器人赛道？</p><p>行业的热度自然是最显要的背景，而对苹果自身来说，驱动它进军机器人领域的自身动力也在这个时间点上异常的大----</p><p>长达15年的库克掌舵时代即将在明年宣告落幕，iPhone系列的辉煌历史之下，是缺乏新的拳头产品的现实，以及更重要的是进入AI时代后在这块领域进展的受挫。</p><p>这些不足和隐忧，让苹果必须加紧迈向机器人领域的步伐。</p><p>而在这个过程里，它有哪些占优的禀赋、有什么可能的不足，以及更关键的，它会为机器人行业带来什么影响？</p><p>苹果的优势<br/>如今，在太平洋两岸，已经有众多的巨头，在过去几年里以下场自研或者投资的方式，切入机器人赛道，试图在包括人工智能在内的技术层、制造层和应用层等方面卡住一个身位，拿到一张通向未来机器人时代的门票。</p><p>而苹果在这个过程里却扮演了一个相对“沉默者”的角色。</p><p>但摩根士丹利在内的分析者们，依旧看好苹果在这个赛道“后来居上”的能力：</p><p>首先是苹果在过去十多年积累下的品牌溢价以及规模化制造能力。</p><p>依靠着高端的设计感和坚持隐私保护的理念，苹果以iPhone为拳头产品已经在全球攒下了十多亿用户，其中不乏品牌的忠实拥趸，拥有其他行业玩家难以匹敌的用户基础。</p><p>而数十年在消费电子领域的量产经验，被认为是苹果在未来有望快速压低机器人硬件制造成本的根基。</p><p>其次是他们在机器人领域掌握的技术储备和经验。</p><p>虽然在经历近10年研发后，苹果的“Project Titan”项目还是被终止，宣告着他们的自动驾驶汽车项目失败，但依旧在计算机视觉、学习和embodied Ai技术等方面积攒下可以复用到机器人领域的经验。类似的还包括此前苹果报以期望的Vision Pro的空间技术等。</p><p>而机器人技术在苹果的生产供应链上也已经颇具“存在感”：富士康“熄灯工厂”已经使用机器人来生产iPhone一段时间了，而名为Dasiy的回收机器人已经能够在生产线上实现每小时200台的拆解效率。在工业场景的落地上，苹果的机器人经验其实已经不输给大部分巨头了。</p><p>此外，苹果在招聘、投入占比等方面也开始加大了对机器人领域的突出和倾斜，所带来的一个直观效果就是近年来苹果公司和机器人相关的专利始终在保持增长。</p><p>最后就是对苹果以往成功立下了汗马功劳的垂直生态整合能力。</p><p>苹果是业内少有的能做到核心部件在设计和量产上都能实现自研和可控的公司。而在软件层面，以庞大用户群体手里的数十亿台不同设备为基础，能帮助苹果积累海量视觉数据。</p><p>更关键的是，Siri、iCloud、HomePod等已经形成用户使用习惯的生态可以和机器人形成紧密结合，极大地降低用户上手难度。</p><p>苹果的劣势<br/>尽管看起来拥有如此多的优势，但苹果通向机器人行业领头羊地位的道路，也绝不会是一帆风顺。</p><p>除了目前已经在机器人赛道的自研和投资上落后其他巨头一个身位的客观事实之外，二姐觉得以下因素也会拖累苹果雄心勃勃的机器人计划。</p><p>机器人，尤其是目前最热门的人形机器人，其生产制造的供应链和苹果原本所熟悉的移动设备供应链依旧存在一定的差异，比如对机器人而言至关重要的精密执行器等方面，苹果也许还需要一些时间来“补课”。</p><p>马斯克就曾公开“诉苦”，坦诚就智能设备而言，做机器人比造汽车还要难，尤其是在硬件设计等层面。对于曾经“造车失败”的苹果来说，无疑接下来的这场“仰攻”还是挺有难度的。</p><p>其次是被认为大概率会发生在明年的高层人事变动：在担任CEO整整15年后，库克明年很有可能卸任，而根据彭博社的文章报道，新任CEO人选很有可能花落硬件工程高级副总裁约翰.特努斯（John Ternus）。在2001年加入苹果后，特努斯参与了苹果大部分硬件产品的工程设计工作。</p><p>但变数还是存在，其他候选人目前也依旧保有可能性。CEO的变化和相关而来的人事变动，最终会给苹果的机器人业务带来什么样的具体变化，还是未知数。</p><p>与人事变动相关联的，还有苹果日趋保守的公司文化和决策流程。有前员工披露，这家市值被库克带到了4万亿美元高峰的大公司，如今每个动作“都要经过财务评估和考虑对利润率的影响”。这种变化显然对于需要创新思维和突破勇气支撑的机器人业务并非利好因素。</p><p>最后，也是最关键的，苹果AI能力的相对落后。</p><p>早在2024年年中，苹果就推出了苹果智能（Apple Intelligence），但迄今为止这个被寄予厚望的AI系统依旧进展缓慢，以至于原定于今年推出的新版Siri已经确定将被推迟到最早明年面世。</p><p>AI能力的瓶颈，此前已经或多或少影响了苹果Vision Pro等硬件设备的销售和用户渗透状况。</p><p>Apple Intelligence被看作是苹果连接已有生态和未来机器人业务的重要纽带，而如果缺乏有力AI的加持，会影响机器人感知、推理和实时学习等核心能力，降低机器人场景的多模态交互和环境自适应水平，机器人也难言是真正有价值的具身智能。</p><p>苹果已经计划将未来的Siri置于机器人操作系统的核心位置，并为其设计可视化形象，增强真实感，以降低用户接受的难度。但如果作为Siri基础的AI大脑“发育”不良，以苹果的慎重作风，其机器人计划的整体延宕是很有可能的。</p><p>苹果机器人的到来可能会带来哪些影响<br/>就目前披露的信息，苹果会在2027年推出一个可以担任虚拟陪伴角色的桌面机器人，其用途主要包括工作、娱乐和生活管理等。</p><p>苹果想利用这款产品，来承载自身AI实体化的战略，但其实步子迈的并不大：一方面，这款机器人所能提供的功能基本上来自于苹果移动设备所具有功能的延伸，只不过因为有了AI，它可以更主动地发起对话和任务；另一方面，在外形上，它也没有选择激进但在目前确实火热的人形形态。</p><p>就目前来看，这款概念机器人虽然进入了家庭，但并不能实现家庭众多场景的覆盖，而且它所想解决的用户需求并不那么明确----看起来，它几乎像是一台“会说话、会做一定程度移动的iPad”。</p><p>但话说回来，这款机器人应该只是苹果对于领域的投石问路之作，他们对机器人的探索绝不会止步于此。</p><p>此前，苹果与大学相关机构一起研发了能解决人形机器人“在物品密集环境中进行运动规划时面临感知问题”的系统；包括其后还发布了关于增强人形机器人基于非语言表达来理解人类意图、实现沟通的能力的研究。</p><p>这些动作，都证实了在场景选择上，苹果会让机器人“先进家”，毕竟他们是一家成熟的to C公司。在消费产品思维导向下，即使是机器人产品，苹果也会倾向于将其打造成轻量易用的智能友好型产品。</p><p>而作为一家在全球已经拥有牢固用户基础的公司，苹果的这种产品方向，除了在技术层面的带动和示范效应外，在需求端也能激发用户对于机器人的使用习惯。让普通消费者与机器人的交互需要更频繁和紧密，就像当年iPhone的渗透带动了智能手机行业整体的普及和发展。</p><p>另外，苹果惯用的“硬件+服务”配套的商业模式，既为自身机器人在以后实现服务和场景升级覆盖预留了空间，对于推动整个机器人行业盈利模式的多元化和完善，也会起到相应的作用。</p><p>同时，苹果加速机器人发展，对上下游产业链还会构成一定的影响。</p><p>比如出于全球竞争和供应链安全的考虑，苹果正在主动加强自身供应链的韧性。比较典型的例子，是他们与美国本土唯一一家运营稀土矿的公司MP materialsweibo.com/ttarticle/p/show?id=2309405246650527645753<br/>weibo.com/ttarticle/p/show?id=2309405246650858995719<br/>weibo.com/ttarticle/p/show?id=2309405246651186151543<br/>weibo.com/ttarticle/p/show?id=2309405246651635204331<br/>weibo.com/ttarticle/p/show?id=2309405246651970486332<br/>weibo.com/ttarticle/p/show?id=2309405246652297642044<br/>weibo.com/ttarticle/p/show?id=2309405246652624797824<br/>weibo.com/ttarticle/p/show?id=2309405246653077782601<br/>weibo.com/ttarticle/p/show?id=2309405246653400744165价值5亿美元的合作。苹果想在美国本土建立稀土磁铁供应链，来保证包括高性能电机这样机器人核心部件在内的制造不会受到原材料的限制。这种降低对单一原材料和生产地依赖的办法，也许会在未来被越来越多的机器人厂商所采纳，从而在某些程度上改变行业的全球布局。</p>]]></description></item><item>    <title><![CDATA[技术层面的带动 烦恼的蜡烛 ]]></title>    <link>https://segmentfault.com/a/1190000047494810</link>    <guid>https://segmentfault.com/a/1190000047494810</guid>    <pubDate>2025-12-22 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着人工智能（AI）的蓬勃发展，许多金融专业人士都在权衡两个主要问题：人工智能市场是否存在泡沫，如果存在，它是否应该与21世纪初的互联网时代相提并论？</p><p>　　电影《大空头》真实原型之一、知名交易员Danny Moses认为这两个问题的答案都是肯定的。虽然他不否认人工智能热潮是真实存在的，而且是一个长期增长的故事，但他也看到了这两次科技热潮之间的强烈相似之处，这表明投资者需要谨慎行事。</p><p>　　Moses因在次贷危机期间做空房地产市场而一战成名，他在最新采访中谈到了人工智能市场正在出现的潜在问题，以及他认为投资者应该如何应对这个快速发展的领域。</p><p>　　“增长是真实的，但数学计算不正确，”他说：“我认为，我们正在达到一个数学不起作用的地步。”</p><p>　　Moses强调，他对AI市场潜在问题的看法并不是呼吁做空该行业。相反，他指出，这是在呼吁投资者做好功课，找到合适的公司，以便在市场持续增长的情况下获得有价值的敞口。</p><p>　　在他看来，这意味着要坚持投资科技行业最具主导地位的公司，这些公司拥有继续扩张的资源，而且不像一些规模较小的公司那样受到种种限制。最好的例子包括亚马逊、谷歌、Meta和微软。</p><p>　　“他们可以在任何时候减少资本支出，而且他们的现金流仍然是正的，而其他公司则依赖于人工智能领域的支出。”他补充道。</p><p>　　不过，Moses并非看好所有的大型科技公司。他以甲骨文为例，说明了人工智能市场存在的问题，并指出该公司的高债务水平和完成科技客户订单所需的现金方面存在风险。他还特别提到波动性较大的科技股，如超微电脑和CoreWeave，认为它们是人工智能领域风险较高的投资标的。</p><p>　　不过，在他看来，投资者终于开始考虑到这样一个事实，即并非所有的人工智能股票都是“生儿平等”的，因为相对表现优异和表现不佳之间的差距变得越来越难以忽视。</p><p>　　“我认为这证明投资者开始分辨交易的赢家和输家，他们更愿意投资那些资产负债表更稳健、更有保障的企业来表达人工智能主题，”他补充道。</p><p>　　Moses还表示，他看好铀，因为这种金属越来越被吹捧为人工智能建设的关键组成部分，这将是未来几年维持该行业所必需的。</p><p>　　由于美国电力供应紧张，一些数据中心倾向采用小型核电站作为主力电源，而铀正是核电站的主要原料。</p><p>　　“我喜欢的交易之一是铀，从主题上讲，这应该是可行的，但需要很长一段时间。人们认为企业将经历人工智能增长的时机，与推动人工智能增长所需的基础设施实际上存在不匹配。”他补充道。</p><p>　人形机器人板块12月4日早盘表现强势，华伍股份、骏亚科技、巨轮智能、睿能科技、龙溪股份纷纷涨停；三协电机、德马科技、江苏雷利则大涨超10%。此外，机器人执行器、减速器、同步磁阻电机等相关板块也涨幅靠前。<br/>　　人形机器人消息不断</p><p>　　消息面上，近期有关于人形机器人的利好新动态不断涌现。据中国基金报援引报道称，在发布加速人工智能发展计划五个月后，特朗普政府开始将目光转向机器人。此前，美国商务部长卢特尼克一直在与机器人行业的首席执行官们会面，并“全力以赴”加速该行业的发展。特朗普政府正在考虑明年发布一项关于机器人技术的行政令。据报道，一位知情人士透露，交通部也正准备宣布成立一个机器人工作组，可能在年底前公布。受此影响，隔夜美股的机器人概念股表现强势，iRobot收涨73.85%，Serve Robotics收涨18.24%。<br/>　　此外，特斯拉CEO马斯克在北京时间12月3日在社交平台转发了特斯拉擎天柱（Optimus）团队发布的一段“擎天柱”人形机器人跑步的短视频。<br/>　　12月2日，众擎机器人宣布，全尺寸极致高效能通用人形机器人众擎T800正式发布，产品发售进程也随即正式启动。同一天，阿童木机器人正式发布迭代版全栈自研人形机器人“天兵一号ATOM01”。</p><p>　　政策环境持续友好</p><p>　　从政策来看，从2025年蛇年春晚舞台的机器人扭秧歌，到北京亦庄的机器人马拉松，再到浙江杭州的机器人格斗赛……人形机器人正逐渐“破圈”，从“实验室”迈向各类“应用场”。而这背后，与政策环境的友好是密不可分的。</p><p>　　今年以来，以人形机器人为典型业态的具身智能成为我国培育未来产业的重要方向。北京、上海、广东深圳、浙江杭州等多地密集出台专项政策，形成了一场面向未来的产业竞逐。</p><p>　　作为全国较早将“具身智能”写入地方政府工作报告的省份，广东在今年2月明确提出，要加快启动布局人形机器人等重点领域研发项目。除了政策支持，北京、上海、深圳等10余个地方政府已建立或筹备建立相关产业基金。</p><p>　　从企业来看，头部企业已率先开启证券化。今年以来，宇树科技、乐聚智能、智元+k.机器人等人形机器人头部整机厂密集启动IPO、并购上市等资本化动作，行业开始迈入“产业化+资本化”双轮驱-+动发展阶段。<br/>　　融资客抢筹前20个股</p><p>　　从杠杆资金角度来看，部分人形机器人概念也被积极抢筹。比如瑞芯微，国庆后融资客融资净买入3.43亿元，该股前三季度归母净利润7.8亿元，同比大增121.65%。东方精工紧随其后，融资客融资净买入3.13亿元，前三季度赚了5.1亿元，同比增54.64%。东阳光居第三位，被融资净买入2.41亿元，前三季度赚了9.06亿元，同比大增189.8%。<br/>研发投入占比前20个股</p><p>　　而从研发投入占营收比角度来看，东方财富Choice数据显示，安路科技以69.45%排在首位。帝奥微紧随其后，研发投入占比为35.22%。当虹科技、创耀科技、芯朋微排名也靠前。<br/>　　2026年迎量产元年？</p><p>　　往后看，“2026年是人形机器人的量产元年，当前临界点已至。”开源证券分析师孟鹏飞指出，海外特斯拉和国内产业进展持续加速，后续催化因素较多。展望2026年，人形机器人将进入量产期，大厂躬身入局，政策支持和补贴有望进入实际阶段，“趋势走强、景气上行”的布局窗口已然开启。而国家发展改革委健全具身智能准入与退出机制、营造公平竞争环境的举措，既正向引导行业迈向良性发展轨道，也释放出人形机器人相关支持政策或已逐步临近的信号。</p><p>　　高工机器人产业研究所（GGII）数据显示，2024年全球人形机器人市场规模约10.17亿美元，预计2030年将达150亿美元，年复合增长率超56%；同期销量从1.19万台增至60.57万台。中国市场前景也很广阔，2030年规模预计达380亿元人民币，销量跃升至27.12万台，占全球份额44.77%。</p><p>　　不过，随着人形机器人的关注度提升，市场上有关于“速度”与“泡沫”的讨论也多了起来。国家发展改革委政策研究室副主任李超此前表示，“速度”与“泡沫”一直是前沿产业发展过程中需要把握和平衡的问题，这对于具身智能产业来讲，也是一样的。当前，人形机器人在技术路线、商业化模式、应用场景等方面尚未完全成熟，随着新兴资本的加速入场，我国目前已有超过150家人形机器人企业，这个数量还在不断增加，其中半数以上为初创或“跨行”入局，这对鼓励创新来讲是一件好事；但也要着力防范重复度高的产品“扎堆”上市、研发空间被压缩等风险。面对机遇与挑战并存的局面，关键在于合理引导。</p><p>11月摩根士丹利新发布的一份研究报告中预测，苹果这家行业巨头正在逐步推进他们的人形机器人计划，想要打造下一个超级增长引擎；结合此前8月份彭博社等财经媒体的相关报道，机器人市场可能真的要在不久的将来迎来苹果这头“巨鲸”了。</p><p>苹果为什么要在此时开始加速下注机器人赛道？</p><p>行业的热度自然是最显要的背景，而对苹果自身来说，驱动它进军机器人领域的自身动力也在这个时间点上异常的大----</p><p>长达15年的库克掌舵时代即将在明年宣告落幕，iPhone系列的辉煌历史之下，是缺乏新的拳头产品的现实，以及更重要的是进入AI时代后在这块领域进展的受挫。</p><p>这些不足和隐忧，让苹果必须加紧迈向机器人领域的步伐。</p><p>而在这个过程里，它有哪些占优的禀赋、有什么可能的不足，以及更关键的，它会为机器人行业带来什么影响？</p><p>苹果的优势<br/>如今，在太平洋两岸，已经有众多的巨头，在过去几年里以下场自研或者投资的方式，切入机器人赛道，试图在包括人工智能在内的技术层、制造层和应用层等方面卡住一个身位，拿到一张通向未来机器人时代的门票。</p><p>而苹果在这个过程里却扮演了一个相对“沉默者”的角色。</p><p>但摩根士丹利在内的分析者们，依旧看好苹果在这个赛道“后来居上”的能力：</p><p>首先是苹果在过去十多年积累下的品牌溢价以及规模化制造能力。</p><p>依靠着高端的设计感和坚持隐私保护的理念，苹果以iPhone为拳头产品已经在全球攒下了十多亿用户，其中不乏品牌的忠实拥趸，拥有其他行业玩家难以匹敌的用户基础。</p><p>而数十年在消费电子领域的量产经验，被认为是苹果在未来有望快速压低机器人硬件制造成本的根基。</p><p>其次是他们在机器人领域掌握的技术储备和经验。</p><p>虽然在经历近10年研发后，苹果的“Project Titan”项目还是被终止，宣告着他们的自动驾驶汽车项目失败，但依旧在计算机视觉、学习和embodied Ai技术等方面积攒下可以复用到机器人领域的经验。类似的还包括此前苹果报以期望的Vision Pro的空间技术等。</p><p>而机器人技术在苹果的生产供应链上也已经颇具“存在感”：富士康“熄灯工厂”已经使用机器人来生产iPhone一段时间了，而名为Dasiy的回收机器人已经能够在生产线上实现每小时200台的拆解效率。在工业场景的落地上，苹果的机器人经验其实已经不输给大部分巨头了。</p><p>此外，苹果在招聘、投入占比等方面也开始加大了对机器人领域的突出和倾斜，所带来的一个直观效果就是近年来苹果公司和机器人相关的专利始终在保持增长。</p><p>最后就是对苹果以往成功立下了汗马功劳的垂直生态整合能力。</p><p>苹果是业内少有的能做到核心部件在设计和量产上都能实现自研和可控的公司。而在软件层面，以庞大用户群体手里的数十亿台不同设备为基础，能帮助苹果积累海量视觉数据。</p><p>更关键的是，Siri、iCloud、HomePod等已经形成用户使用习惯的生态可以和机器人形成紧密结合，极大地降低用户上手难度。</p><p>苹果的劣势<br/>尽管看起来拥有如此多的优势，但苹果通向机器人行业领头羊地位的道路，也绝不会是一帆风顺。</p><p>除了目前已经在机器人赛道的自研和投资上落后其他巨头一个身位的客观事实之外，二姐觉得以下因素也会拖累苹果雄心勃勃的机器人计划。</p><p>机器人，尤其是目前最热门的人形机器人，其生产制造的供应链和苹果原本所熟悉的移动设备供应链依旧存在一定的差异，比如对机器人而言至关重要的精密执行器等方面，苹果也许还需要一些时间来“补课”。</p><p>马斯克就曾公开“诉苦”，坦诚就智能设备而言，做机器人比造汽车还要难，尤其是在硬件设计等层面。对于曾经“造车失败”的苹果来说，无疑接下来的这场“仰攻”还是挺有难度的。</p><p>其次是被认为大概率会发生在明年的高层人事变动：在担任CEO整整15年后，库克明年很有可能卸任，而根据彭博社的文章报道，新任CEO人选很有可能花落硬件工程高级副总裁约翰.特努斯（John Ternus）。在2001年加入苹果后，特努斯参与了苹果大部分硬件产品的工程设计工作。</p><p>但变数还是存在，其他候选人目前也依旧保有可能性。CEO的变化和相关而来的人事变动，最终会给苹果的机器人业务带来什么样的具体变化，还是未知数。</p><p>与人事变动相关联的，还有苹果日趋保守的公司文化和决策流程。有前员工披露，这家市值被库克带到了4万亿美元高峰的大公司，如今每个动作“都要经过财务评估和考虑对利润率的影响”。这种变化显然对于需要创新思维和突破勇气支撑的机器人业务并非利好因素。</p><p>最后，也是最关键的，苹果AI能力的相对落后。</p><p>早在2024年年中，苹果就推出了苹果智能（Apple Intelligence），但迄今为止这个被寄予厚望的AI系统依旧进展缓慢，以至于原定于今年推出的新版Siri已经确定将被推迟到最早明年面世。</p><p>AI能力的瓶颈，此前已经或多或少影响了苹果Vision Pro等硬件设备的销售和用户渗透状况。</p><p>Apple Intelligence被看作是苹果连接已有生态和未来机器人业务的重要纽带，而如果缺乏有力AI的加持，会影响机器人感知、推理和实时学习等核心能力，降低机器人场景的多模态交互和环境自适应水平，机器人也难言是真正有价值的具身智能。</p><p>苹果已经计划将未来的Siri置于机器人操作系统的核心位置，并为其设计可视化形象，增强真实感，以降低用户接受的难度。但如果作为Siri基础的AI大脑“发育”不良，以苹果的慎重作风，其机器人计划的整体延宕是很有可能的。</p><p>苹果机器人的到来可能会带来哪些影响<br/>就目前披露的信息，苹果会在2027年推出一个可以担任虚拟陪伴角色的桌面机器人，其用途主要包括工作、娱乐和生活管理等。</p><p>苹果想利用这款产品，来承载自身AI实体化的战略，但其实步子迈的并不大：一方面，这款机器人所能提供的功能基本上来自于苹果移动设备所具有功能的延伸，只不过因为有了AI，它可以更主动地发起对话和任务；另一方面，在外形上，它也没有选择激进但在目前确实火热的人形形态。</p><p>就目前来看，这款概念机器人虽然进入了家庭，但并不能实现家庭众多场景的覆盖，而且它所想解决的用户需求并不那么明确----看起来，它几乎像是一台“会说话、会做一定程度移动的iPad”。</p><p>但话说回来，这款机器人应该只是苹果对于领域的投石问路之作，他们对机器人的探索绝不会止步于此。</p><p>此前，苹果与大学相关机构一起研发了能解决人形机器人“在物品密集环境中进行运动规划时面临感知问题”的系统；包括其后还发布了关于增强人形机器人基于非语言表达来理解人类意图、实现沟通的能力的研究。</p><p>这些动作，都证实了在场景选择上，苹果会让机器人“先进家”，毕竟他们是一家成熟的to C公司。在消费产品思维导向下，即使是机器人产品，苹果也会倾向于将其打造成轻量易用的智能友好型产品。</p><p>而作为一家在全球已经拥有牢固用户基础的公司，苹果的这种产品方向，除了在技术层面的带动和示范效应外，在需求端也能激发用户对于机器人的使用习惯。让普通消费者与机器人的交互需要更频繁和紧密，就像当年iPhone的渗透带动了智能手机行业整体的普及和发展。</p><p>另外，苹果惯用的“硬件+服务”配套的商业模式，既为自身机器人在以后实现服务和场景升级覆盖预留了空间，对于推动整个机器人行业盈利模式的多元化和完善，也会起到相应的作用。</p><p>同时，苹果加速机器人发展，对上下游产业链还会构成一定的影响。</p><p>比如出于全球竞争和供应链安全的考虑，苹果正在主动加强自身供应链的韧性。比较典型的例子，是他们与美国本土唯一一家运营稀土矿的公司MP materials价值5亿美元的合作。苹果想在美国本土建立稀土磁铁供应链，来保证包括高性能电机这样机器人核心部件在内的制造不会受到原材料的限制。这种降低对单一原材料和生产地依赖的办法，也许weibo.com/ttarticle/p/show?id=2309405246653732356353<br/>weibo.com/ttarticle/p/show?id=2309405246654059249848<br/>weibo.com/ttarticle/p/show?id=2309405246654520885392<br/>weibo.com/ttarticle/p/show?id=2309405246654856167482<br/>weibo.com/ttarticle/p/show?id=2309405246656701661488<br/>weibo.com/ttarticle/p/show?id=2309405246657028817187<br/>weibo.com/ttarticle/p/show?id=2309405246657360429191<br/>weibo.com/ttarticle/p/show?id=2309405246657691779225<br/>weibo.com/ttarticle/p/show?id=2309405246658140569735会在未来被越来越多的机器人厂商所采纳，从而在某些程度上改变行业的全球布局。</p>]]></description></item><item>    <title><![CDATA[【完整回放】2022 HVV实战专题 梓源 ]]></title>    <link>https://segmentfault.com/a/1190000047494665</link>    <guid>https://segmentfault.com/a/1190000047494665</guid>    <pubDate>2025-12-22 22:02:15</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>HVV 红蓝对抗流程：教育视角下的信息安全培训<br/>引言<br/>随着信息技术的迅猛发展，网络安全成为了各个行业的重要课题。在这个背景下，HVV（红蓝对抗）作为一种有效的网络安全演练方式，越来越受到重视。红蓝对抗通过模拟攻击（红队）与防御（蓝队）的对抗，帮助组织识别安全漏洞，并提高整体安全防护水平。本文将从教育视角出发，探讨HVV红蓝对抗中的信息收集、内网横向攻击和蜜罐反制等关键流程及其在信息安全培训中的应用。<br/>一、信息收集</p><ol><li>信息收集的意义<br/>在红蓝对抗的第一阶段，信息收集是红队成功攻击的基础。这一阶段主要目的是获取目标环境的相关信息，包括网络拓扑、开放端口、服务、系统版本等。通过系统化的信息收集，红队能够制定有效的攻击策略。</li><li>教育实践中的应用<br/>在信息安全培训中，教育者可以通过模拟演练让学员掌握信息收集的技能。以下是几个推荐的培训活动：</li></ol><p>1.工具介绍与实践：教授学员使用工具如Nmap、Recon-ng等进行网络扫描和信息收集。<br/>2.案例分析：分析历史上的网络攻击案例，让学员识别信息收集如何影响攻击结果。<br/>3.小组合作：组织学员团队进行攻防对抗演练，增强协作与实践能力。</p><p>二、内网横向攻击</p><ol><li>横向攻击的概念<br/>内网横向攻击是在入侵成功后，利用已获得的权限继续渗透内网其他系统。这一过程涉及许多技术手段，比如凭证盗窃、利用信任关系等。</li><li>教育实践中的应用<br/>在内网攻击的教育培训中，可以通过设计完整的攻防演练以提高学员的意识和技能：</li></ol><p>4.红队攻击演示：由专业人员演示如何在内网内进行横向攻击，包括使用Mimikatz等工具。<br/>5.蓝队防御策略：教授蓝队如何监控和防止横向攻击，例如，通过网络流量分析和用户行为监测工具识别异常活动。<br/>6.实战演练：模拟真实环境中的内网攻击，让学员在对抗中学习和应用防御策略。</p><p>三、蜜罐反制</p><ol><li>蜜罐技术概述<br/>蜜罐是一种主动防御技术，旨在通过设置虚假的脆弱系统或服务，引导攻击者进行攻击，从而收集数据和分析其行为。这对于识别攻击模式与手法，提升防御能力具有重要意义。</li><li>教育实践中的应用<br/>在信息安全教育中，蜜罐的应用可帮助学生深化对网络攻击行为的理解：</li></ol><p>7.蜜罐的搭建与配置：手把手教导学生如何创建和配置蜜罐系统，理解其工作原理及价值。<br/>8.攻击数据分析：分析通过蜜罐收集到的攻击数据，帮助学生识别攻击者的行为模式和常用工具。<br/>9.开发反制策略：培训学员利用蜜罐数据制定更有效的网络防护措施，提高整体安全防护能力。</p><p>结语<br/>通过HVV红蓝对抗流程的全面介绍，可以看出信息安全教育在现代网络防护体系中的重要性。信息收集、内网横向攻击和蜜罐反制，这些环节既是红蓝对抗的基本组成部分，也可以有效地融入信息安全培训之中。教育者通过设计科学的教学活动，使学员能够在实践中巩固理论知识，提升自身的安全防护能力，从而更好地应对日益复杂的网络安全挑战。</p>]]></description></item><item>    <title><![CDATA[kubernetes实战与源码剖析-专享 资源999it点top ]]></title>    <link>https://segmentfault.com/a/1190000047494668</link>    <guid>https://segmentfault.com/a/1190000047494668</guid>    <pubDate>2025-12-22 22:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Kubernetes 实战与源码剖析：核心组件运行机制与企业级部署全解析<br/>Kubernetes（K8s）作为现代云原生架构中的核心技术之一，已经成为企业级应用部署与管理的标准。自2014年由Google开源以来，Kubernetes以其强大的容器编排能力、灵活的扩展性和高可用性，逐渐形成了一套成熟的生态体系。本文将从教育、科技、人文发展和经济等多个角度深入探讨Kubernetes的核心组件及其企业级部署所带来的变革。<br/>教育：培养未来的IT人才<br/>随着Kubernetes的普及，相关的教育培训也应运而生。开发者、运维工程师以及IT管理人员需要掌握K8s的基础知识和实践能力。越来越多的高等院校以及职业培训机构开始开设Kubernetes相关课程，帮助学生和职场人士提升技能。<br/>此外，在线教育平台如Coursera、Udemy等也提供了丰富的Kubernetes课程。Kubernetes的学习不仅促进了计算机科学的学术研究，更推动了实践与理论的结合。通过实际项目的训练，学员们能够更好地理解Kubernetes的工作机制，从而有助于未来的职场竞争力。<br/>科技：推动技术的演进与创新<br/>Kubernetes的出现大大提升了云计算技术的灵活性与可管理性。它通过抽象化基础设施，使得开发者可以专注于应用程序的开发，而不必过于关注底层环境的配置。这种分离不仅提高了开发效率，也加速了新技术的迭代与创新。<br/>在技术演进方面，Kubernetes支持微服务架构，在大规模分布式系统中表现尤为出色。通过Docker等容器技术，K8s让应用的构建、测试、交付与部署过程变得更加自动化与标准化。此外，其丰富的生态工具链，例如Helm、Istio和Prometheus等，进一步增强了Kubernetes的功能，使得开发者和运维工程师能够更高效地管理复杂的微服务环境。<br/>人文发展：影响团队合作与文化<br/>Kubernetes的 adoption 不仅局限于技术层面，它还引发了文化与团队合作的变革。随着DevOps理念的深入，K8s使得开发与运维之间的墙变得模糊，促进了跨部门的协作。团队中的开发人员可以更快速地反馈和迭代，运维人员则通过集中的管理面板和自动化的监控工具减轻了日常运维的负担。<br/>此外，Kubernetes的开源特性使得全球的开发者能够参与到生态建设中，推动了一种共享知识和资源的文化。无论是通过贡献代码、解决Bug，还是撰写文档和教程，K8s的社区能够促进人们之间的合作和交流，成为一种推动技术人文发展的力量。<br/>经济：促进企业的数字化转型<br/>在经济层面，Kubernetes的企业级应用能够显著降低IT基础设施的成本，提高资源利用率。K8s的弹性扩展能力使得企业能够根据需求进行资源的灵活配置，从而避免了资源的浪费。这对于预算有限的中小企业尤为重要，它们可以在不进行大规模投资的情况下，充分利用云计算带来的灵活性。<br/>同时，使用Kubernetes的企业能够更快速地实现产品的上线和迭代，使企业在市场竞争中保持灵活性和响应速度。这种高效的开发和运维流程，使得企业能够更好地满足客户需求，从而提升整体业务的竞争力。<br/>结论<br/>Kubernetes的核心组件和企业级部署不仅在技术领域产生了深远影响，也在教育、人文和经济层面引发了一系列变革。无论是推动技术的创新与应用，还是促进团队文化的变革、帮助企业实现数字化转型，Kubernetes都以其独特的方式在现代科技的发展进程中扮演着重要角色。<br/>在未来，随着Kubernetes生态的不断完善，我们有理由相信，将有更多的机会去探索这一技术所带来的可能性，并为相关领域的进一步发展做出贡献。</p>]]></description></item><item>    <title><![CDATA[每日一个C++知识点|对象资源传递机制 图形学爱好者Wu ]]></title>    <link>https://segmentfault.com/a/1190000047494605</link>    <guid>https://segmentfault.com/a/1190000047494605</guid>    <pubDate>2025-12-22 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>C++是一门对内存资源配置要求较高的语言，其中对象资源传递在C++开发中无处不在，下面我将在浅拷贝、深拷贝、左值右值、移动语义、完美转发这5个方面层层递进地讲解C++对象资源传递机制，争取做到知识串联，深入浅出~</p><h2>浅拷贝</h2><p>我们从一个实际场景入手：写一个Image类，存储图片的像素数据，代码如下：</p><pre><code class="cpp">#include &lt;iostream&gt;
using namespace std;

// 图片类：管理堆内存中的像素数据
class Image {
public:
    // 构造函数：分配堆内存（相当于“买快递箱装图片数据”）
    Image(int w, int h) : width(w), height(h) {
        // 每个像素占4字节（RGBA），分配一大块堆内存
        pixels = new char[width * height * 4]; 
        cout &lt;&lt; "构造函数：分配内存，图片尺寸：" &lt;&lt; width &lt;&lt; "x" &lt;&lt; height &lt;&lt; endl;
    }

    // 析构函数：释放堆内存（相当于“扔掉快递箱”）
    ~Image() {
        if (pixels != nullptr) {
            delete[] pixels;
            cout &lt;&lt; "析构函数：释放了内存" &lt;&lt; endl;
        }
    }

private:
    int width, height;
    char* pixels; // 指向像素数据的指针（核心资源）
};

int main() {
    Image img1(1000, 1000); // 创建1000x1000的图片
    Image img2 = img1;      // 拷贝img1到img2
    return 0;
}</code></pre><p>这段代码看起来没问题，却会触发内存错误。原因就是编译器默认拷贝方式是<br/><code>浅拷贝</code></p><p>那么什么是浅拷贝呢？浅拷贝只拷贝成员变量的值，不拷贝资源本身，会造成两个对象共享同一块堆内存，相当于两个快递单号指向同一个快递箱。当多个对象共享资源，析构函数运行时就会崩溃。在上述代码中只把<code>img1</code>的<code>pixels</code>指针地址复制给<code>img2</code>,没有把资源本身复制一份，导致程序结束后<code>析构时双重释放</code>，<code>img2</code>先析构释放内存，<code>img1</code>析构时又去释放已经被释放的内存，直接崩溃</p><h2>深拷贝</h2><p>那么怎么解决浅拷贝带来的程序崩溃问题呢？一个简单的方法是使用<code>深拷贝</code>。深拷贝不仅拷贝成员变量，还为新对象重新分配资源并复制数据，使每个对象拥有独立资源，提升安全性。下面我们给<code>Image</code>类添加深拷贝构造函数（在原有代码的基础上直接添加，其他地方保持不变）：</p><pre><code class="cpp">// 深拷贝构造函数：参数是const左值引用（const 类名&amp;）
Image(const Image&amp; other) {
    // 第一步：复制基础属性
    width = other.width;
    height = other.height;
    // 第二步：关键！重新分配新的堆内存（买新快递箱）
    pixels = new char[width * height * 4]; 
    // （实际开发中会复制像素数据，这里重点是“新分配内存”）
    cout &lt;&lt; "深拷贝构造函数：新分配了内存" &lt;&lt; endl;
}</code></pre><p>以上拷贝构造函数会在编译器中自动执行，因而无需在main函数添加。</p><p>此时再运行代码，<code>img1</code>和<code>img2</code>各自拥有独立内存，程序正常结束。但是深拷贝的内存分配和数据复制会带来巨大性能开销，如果是为了处理临时数据而产生这么大的开销，有点浪费资源。那么我们可不可以在深拷贝完成之后对临时数据进行删除呢？</p><p>假设我们有一个函数，生成一张临时的<code>Image</code>对象:</p><pre><code class="cpp">// 返回临时Image对象（无名字，是“即将销毁”的右值）
Image createWhiteImage(int w, int h) {
    Image temp(w, h);
    return temp;
}</code></pre><p>因为<code>Image temp(w, h)</code>是在函数里实现的，也就是在栈内实现的，所以对象在函数执行时可以自动创建，函数运行结束后自动释放销毁;</p><p>再用深拷贝接收这个临时对象：</p><pre><code class="cpp">Image img3 = createWhiteImage(2000, 2000); // 深拷贝：耗时耗内存</code></pre><p>这样就实现了在深拷贝完成之后对临时数据进行删除，但是这就像 “把快递里的东西复制一份，再把原快递箱扔掉”，完全没必要，这时候移动语义就该登场了。</p><h2>左值和右值</h2><p>在了解移动语义之前，我们需要了解一个重要的概念——<code>左值</code>和<code>右值</code></p><p>左值通常在等号左边，右值通常在等号右边，<code>但是</code>，<code>左值并非是在等号左边的对象，右值也并非是在等号右边的对象</code></p><p>左值是有名字、能取地址的对象，是持久存在 的对象。</p><p>右值是无名字、不能取地址的临时对象，是即将销毁的对象。</p><p>在上述代码中，<code>img1</code>是左值，<code>int a = 10</code>;中的<code>a</code>是左值;<code>createWhiteImage()</code>的返回值是右值。了解左值和右值的基本概念后，我们就能在移动语义中使用它们了~</p><h2>移动语义</h2><p>移动语义本质是转移右值的资源所有权，而非执行资源拷贝，所以可以达到减少资源浪费的效果</p><h3>移动构造函数</h3><p>要实现移动语义，需要给Image类添加移动构造函数：</p><pre><code class="cpp">// 移动构造函数：参数是右值引用（类名&amp;&amp;），通常加noexcept
Image(Image&amp;&amp; other) noexcept {
    // 第一步：“偷”走源对象的资源（仅复制指针地址，无内存分配）
    width = other.width;
    height = other.height;
    pixels = other.pixels;

    // 第二步：关键！将源对象置为空（作废原快递单号，避免析构冲突）
    other.pixels = nullptr;
    other.width = 0;
    other.height = 0;

    cout &lt;&lt; "移动构造函数：直接转移资源，无内存分配！" &lt;&lt; endl;
}</code></pre><p>此时再接收临时对象：</p><pre><code class="cpp">Image img3 = createWhiteImage(2000, 2000); // 触发移动构造，瞬间完成</code></pre><p>这就像 “直接把快递箱的地址改成自己的，不用复制里面的东西”，性能直接拉满~</p><h3>std::move</h3><p><code>std::move</code>是把左值 “伪装” 成右值的小工具，如果想把左值的资源转移给其他对象，可以用<code>std::move</code></p><pre><code class="cpp">Image img4(1500, 1500); // 左值
Image img5 = std::move(img4); // 触发移动构造，img4变为空</code></pre><p>注意：它只是强制转换类型，不会真的移动数据</p><h2>完美转发</h2><p>移动语义解决了临时对象的拷贝问题，但在模板函数中，会遇到新问题：参数的左值和右值属性会丢失。</p><p>如下代码所示：</p><pre><code class="cpp">template &lt;typename T&gt;
void wrapper(T x) {
    Image img = x; // 无论x是左值还是右值，都触发深拷贝
}

// 调用：传入右值，却还是深拷贝
wrapper(createWhiteImage(1000, 1000));</code></pre><p>由于模板参数x是拷贝后的对象，已经变成了左值，丢失了原来的右值属性</p><p>这时候需要用到<code>完美转发</code>来解决上述问题，<code>完美转发</code>在模板中保留参数的左值 / 右值属性，它需要两个核心要素：<code>万能引用</code>和<code>std::forward</code></p><h3>万能引用</h3><p><code>T&amp;&amp;</code>是万能引用符号，仅在模板中使用，能绑定左值或右值</p><h3>std::forward</h3><p><code>std::forward</code>：根据参数的原始类型，转发为左值或右值</p><p>用代码举例如下：</p><pre><code class="cpp">template &lt;typename T&gt;
void wrapper(T&amp;&amp; x) { // 万能引用
    Image img = std::forward&lt;T&gt;(x); // 完美转发：保留属性
}

// 测试：属性保留
Image img6(800, 800);
wrapper(img6); // 传入左值，触发深拷贝（符合预期）
wrapper(createWhiteImage(800, 800)); // 传入右值，触发移动构造</code></pre><p>完美转发就像 “快递包装不拆，直接原封不动转发”，确保参数的属性不丢失。</p><h2>总结</h2><p>以上便是C++对象资源传递机制的主要内容，从浅拷贝、深拷贝、左值右值、移动语义、完美转发层层递进，如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494607" alt="" title=""/></p><ol><li>浅拷贝：绝对禁用（除非类无动态资源），会导致内存崩溃。</li><li>深拷贝：解决浅拷贝的内存崩溃，但需要更多内存开销。</li><li>移动语义：处理右值的性能方案，用资源转移代替拷贝，std::move可把左值转为右值。</li><li>完美转发：在模板中使用，保障移动语义在参数传递中生效。</li></ol><p>如果这篇文章文章对你有用的话, 欢迎点赞收藏加关注哦~</p>]]></description></item><item>    <title><![CDATA[硬件+软件协同交付怎么落地？2025 软硬件项目管理工具对比 研之有李 ]]></title>    <link>https://segmentfault.com/a/1190000047494367</link>    <guid>https://segmentfault.com/a/1190000047494367</guid>    <pubDate>2025-12-22 20:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>本文深度测评 ONES、Polarion、Codebeamer、Helix ALM、Jama Connect、SpiraTeam、Nuxeo、Hansoft、Nifty 等软硬件协同管理工具，帮助团队打通需求-缺陷-版本管理全流程。</p><h2>软硬件协同交付的难点</h2><p>在复杂系统研发里，软件团队习惯以迭代节奏驱动交付，硬件团队则以阶段评审与变更控制驱动质量。两种节奏并行并不矛盾，真正让项目失控的往往是：软硬件共享的关键对象没有被同一套机制锁定、追溯与复现。</p><p>我把常见痛点归为四类，但每一类背后都指向同一个本质——“对象与链路的缺失”：</p><ul><li>需求追溯断点：系统需求 → 子系统需求 → 软硬件分解需求 → 设计/实现 → 测试验证之间缺少稳定链接，影响分析只能靠经验与会议。</li><li>变更与版本对齐困难：硬件 ECR/ECN、固件/软件版本、测试基线、发布包（制品）各自为政，出现“同名不同物”，最后反噬质量与周期。</li><li>验证闭环效率低：测试结果停留在报告，没回到需求与缺陷对象上沉淀，回归成本上升，质量趋势难量化。</li><li>组织协同摩擦大：系统工程、硬件、嵌入式、应用、测试、供应商使用不同系统，信息在 IM、邮件、表格漂移，责任边界与决策记录不清晰。</li></ul><p>因此，讨论“软硬件项目管理工具”时，我更关心的不是看板够不够炫，而是它能否承载一条可运行的 ALM 数字主线（Digital Thread）：把需求管理、缺陷管理、版本管理、制品管理放进同一套“对象—关系—基线—证据”的体系。</p><h2>软硬件协同管理工具盘点与对比</h2><h4>ONES——集成化、本地化与软硬件协同落地的现实路径</h4><p>核心功能与定位：<a href="https://link.segmentfault.com/?enc=e5avVvM9zgTPFYHt%2BxXqzg%3D%3D.pnHP2SxjdQvt6WKOD2GO%2Bg%3D%3D" rel="nofollow" target="_blank">ONES</a> 研发管理平台覆盖流程、进度、协作、效能改进等，并支持 Agile / Waterfall / Hybrid 乃至 IPD 等不同方法论在同一套流程与数据上协同；同时提供企业级权限与审计等治理能力，适配云或本地部署的合规要求。</p><p>适用场景：</p><ul><li>软硬件并行交付需要统一节奏与透明化协同；</li><li>希望降低工具碎片化与集成成本，用一体化平台先跑通“需求—执行—测试—交付协作”的主干，再逐步扩展工具链；</li><li>在硬件研发管理上，ONES 能把硬件项目最关键的“计划—里程碑—依赖—变更协同”做成可执行的工程节奏；</li><li>在软件研发管理上，ONES 更接近 ALM 的“闭环能力”，强调覆盖 需求管理、路线图、迭代（Sprint）管理、质量控制、发布管理等生命周期关键环节。</li></ul><p>优势亮点：</p><ul><li>一体化降低推广成本：很多组织不是“工具买不起”，而是“流程推不动”。平台集成度越高，PMO 越容易建立统一模板、度量口径与跨团队协作机制。</li><li>本地化落地更可控：在私有化、合规、培训与持续运营上，本地化支持往往决定了工具是否能成为组织能力的一部分。</li></ul><p>局限性：</p><p>若组织追求某些系统工程专用能力的“极致深度”，仍建议用 POC 验证追溯粒度、复杂权限与跨域集成上限。<br/><img width="723" height="374" referrerpolicy="no-referrer" src="/img/bVdhI10" alt="" title=""/></p><h4>Polarion（Siemens）</h4><p>核心功能与定位：强调在统一平台定义、构建、测试与管理复杂系统，并保持端到端可追溯与可视化；同时强调面向审计/合规的追溯与变更控制能力。</p><p>适用场景：当你的组织需要系统工程级追溯、合规审计证据、跨团队协作的“单一事实源（Single Source of Truth）”，Polarion 更容易体现价值。</p><p>优势亮点：</p><ul><li>追溯不是报表，而是决策机制：影响分析从“开会问人”变成“在关系图上确认范围”，这会直接改变变更治理效率。</li><li>适合做组织级模板化：把 IPD 评审点、基线策略、交付证据输出固化为可复用资产，越大规模越有复利。</li></ul><p>局限与不足：</p><p>推行成本主要在“治理”而非“安装”：数据模型、权限边界、评审门禁要先统一。<br/>如果组织还未建立基线纪律，平台越强，反而越暴露管理短板——这不是坏事，但要有心理预期。<br/><img width="723" height="357" referrerpolicy="no-referrer" src="/img/bVdnnys" alt="" title="" loading="lazy"/></p><h4>Codebeamer（PTC）</h4><p>核心功能与定位：官方强调“需求管理 + 风险 + 测试与验证 + 产品线管理能力”在一个平台内，并主打“连接式 ALM”；同时提到可与 CI/CD、源码、PLM 等工具联动，服务更大的数字主线。</p><p>适用场景：多版本并行、变体/产品线工程、受监管行业（质量与合规压力高），以及希望把 ALM 放进更大的数字主线架构的组织。</p><p>优势亮点：</p><ul><li>把风险拉回主线：风险与需求、验证联动，比“表格式风险管理”更能落地到工程动作。</li><li>可配置性适合复杂流程：对成熟 PMO/质量体系团队，能把流程沉淀为平台能力。</li><li>局限与不足：</li><li>可配置意味着需要能力：没有流程架构与平台治理能力时，容易“配置过度/流程过重”。</li></ul><p>建议从一个产品线或一个系统域试点，把追溯粒度、基线与审批机制定住，再扩张。</p><h4>Helix ALM（Perforce）</h4><p>核心功能与定位：以需求、测试、缺陷/问题为核心对象形成追溯闭环；适合把验证结果与缺陷处理纳入同一条主线。</p><p>适用场景：嵌入式/工业软件团队想先把“需求—测试—缺陷”跑通，并逐步接入自动化验证与证据沉淀。</p><p>优势亮点：当验证证据能够回链到需求与缺陷，质量趋势才有统计意义，回归成本也更可控（尤其是多版本并行时）。</p><p>局限与不足：在“跨硬件变更、供应商协作、复杂门禁”上通常还要补集成与治理设计，才能形成真正端到端数字主线。</p><h4>Jama Connect（Jama Software）</h4><p>核心功能与定位：偏“需求与评审协作 + 追溯与影响分析”。其官方特性页明确 Review Center 用于实时协作评审与集中管理评审意见/批准记录。</p><p>适用场景：需求评审频繁、跨组织对齐成本高、且希望把评审记录纳入审计证据的系统工程团队（含供应商与外协参与）。</p><p>优势亮点：把评审从会议纪要变成结构化证据：对合规行业来说，评审记录往往就是审计准备工作量的大头。在 Jama 的公开内容中，也能看到关于缩短评审周期与降低审计准备时间的案例口径（建议按自身流程用 POC 验证）。</p><p>局限与不足：Jama 更强在需求与评审侧；要形成“从需求到制品”的发布闭环，通常仍需与开发、构建、制品库等工具链组合。</p><h4>SpiraTeam（Inflectra）</h4><p>核心功能与定位：倾向把需求、测试、计划、风险与缺陷整合为“基础 ALM 闭环”。<br/>适用场景：中型团队希望较快获得“闭环感”：需求能落到测试、缺陷能回到需求、发布有证据可查。</p><p>优势亮点：对“先跑通、再优化”的组织更友好：先建立对象体系与追溯，再谈规模化与深度集成。</p><p>局限与不足：面对多组织、多供应商、强合规的大规模场景时，数据模型/权限/流程会更快触顶，需要更强平台或更成熟的组合方案。</p><h4>Hansoft（规划侧更强）</h4><p>核心功能与定位：偏研发计划、组合与节奏治理，适合把多团队的里程碑与迭代节奏拉齐。</p><p>适用场景：硬件阶段门禁（EVT/DVT/PVT 等）与软件迭代并行，需要“节奏对齐 + 资源协调”的 PMO。</p><p>局限与不足：Hansoft 更像推进层补强；审计级追溯与证据链仍建议回到 ALM 主干。</p><p>ALM/系统工程工具的关键，不是“多功能”，而是能否把追溯链与基线机制做成组织级的默认动作。</p><h4>Nuxeo（Hyland Nuxeo）</h4><p>核心功能与定位：更像内容服务平台（Content Services）与数字资产底座；官方文档明确其提供原生 REST API，用于远程集成与构建自定义界面/能力。</p><p>适用场景：</p><p>大量规范、设计文档、验证报告、供应商资料需要版本/权限/流程/审计统一；<br/>希望把“证据库”与 ALM 主线关联，形成可审计的交付链。</p><p>优势亮点：把证据当资产管理：当组织进入合规与规模化交付阶段，证据不是“交付后补材料”，而应该伴随对象自然生成并归档。</p><p>局限与不足：它不替代 ALM 主线；如果证据无法回链到需求/测试/缺陷对象，最终仍会变成“更大的资料库”。</p><p>内容平台的价值在“证据可治理、可复用”，但前提是证据必须回链到工程对象。</p><h2>2025 的演进趋势与选型建议</h2><p><strong>趋势判断（你可以用作年度规划的判断框架）：</strong></p><ul><li>ALM 更强调单一事实源与合规追溯</li><li>“需求评审 + 证据链”产品化加速</li><li>数字主线从口号走向工程落地</li><li>本地化与可运营性成为现实权重</li></ul><p>不同规模与行业的建议（给硬件研发经理 / 系统工程 / PMO / 研发总监）：</p><ul><li>中小团队（几十人）：优先“最小闭环可运行”，一体化更容易落地；目标是把需求、缺陷、测试与发布证据先连起来。</li><li>中大型组织（数百到数千人）：采用“三层架构”，系统记录层先稳定，再用推进层做组合治理，用证据层做资产沉淀；避免一次性铺开导致推广失败。</li><li>强合规行业（汽车/医疗/工业控制等）：把“追溯、基线、证据链”放在第一优先级，推进效率排第二；合规不是文档工作量，而是工程对象是否可审计、可复现。</li></ul><p>硬件研发数字化转型的本质，是把系统工程与 IPD 的治理逻辑固化为“可复用的流程资产 + 可审计的数据资产”。当你用 ALM 数字主线把需求、缺陷、版本、制品锁进同一条链，软硬件协同交付才真正具备规模化复制能力。</p><h2>FAQ</h2><p>Q1：软硬件协同交付到底需要哪些“软件能力”？<br/>A：至少要具备需求管理、缺陷管理、版本管理、制品管理，并用测试管理与变更管理把证据链闭环，比如 ONES 等；协作工具只解决推进，不能替代审计级追溯。</p><p>Q2：一体化平台和最佳组合怎么选？<br/>A：如果你们的主矛盾是“推广与统一口径”，一体化更稳；如果你们有强集成能力与成熟治理体系，最佳组合可以在特定环节做到极致，但维护成本更高。</p><p>Q3：为什么我用了很多工具，交付还是混乱？<br/>A：通常是“基线策略缺失 + 追溯粒度不统一”。没有基线，版本与制品无法复现；没有统一追溯，影响分析与审计证据只能靠人扛。</p>]]></description></item><item>    <title><![CDATA[Verilog端口类型解析 星星上的柳树 ]]></title>    <link>https://segmentfault.com/a/1190000047494374</link>    <guid>https://segmentfault.com/a/1190000047494374</guid>    <pubDate>2025-12-22 20:02:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>“理解端口类型，是Verilog模块设计的关键。”<br/>在Verilog设计中，端口是模块与外界交互的桥梁。不同类型的端口——输入、输出与双向——在数据流向与信号驱动方式上有着严格的规则。若定义不当，不仅会引发编译错误，还可能导致仿真行为与硬件实现不一致。掌握Verilog端口类型的使用原则，能帮助设计者构建结构清晰、逻辑可靠的电路系统。</p><p>1、端口类型与信号流向Verilog模块的端口可分为三类：输入端口（input）、输出端口（output）和双向端口（inout）。它们决定了信号在模块间的流动方向，并影响端口的数据类型与驱动方式。输入端口：信号从外部流入模块内部。输出端口：模块内部生成信号并输出至外部。双向端口：信号可在模块间双向流动，常用于总线接口。<br/>端口类型不仅定义通信方向，更隐含了信号驱动规则：哪些信号可以被过程赋值、哪些必须通过连续赋值驱动。</p><p>2、输入端口：永远是nets在Verilog中，输入端口（input）始终被视为net类型。这意味着输入端口不能在过程块（always或initial中）被赋值，而只能接收外部驱动。<br/>例如：module adder (  input [3:0] a, b,  output [4:0] sum);  assign sum = a + b;endmodule<br/>这里，输入端口a与b都是nets类型，用于连接外部信号源。Verilog之所以规定输入端口为net，是因为它们仅承担信号传递功能，不具备存储能力。<br/>注意：若在模块内试图对输入端口赋值，如a = 1;，编译器会报错——因为nets不能在过程块中驱动。</p><p>3、输出端口：由驱动方式决定类型输出端口（output）的类型取决于其驱动方式，这一特性是Verilog端口规则的核心。若输出由过程块（如always）驱动，则必须声明为变量（reg或logic）类型。若输出由连续赋值（assign）语句驱动，则应为net类型。<br/>示例1（连续赋值驱动）：module and_gate (  input a, b,  output y);  assign y = a &amp; b;  // y为net类型endmodule<br/>示例2（过程赋值驱动）：module dff (  input clk, d,  output reg q);  always @(posedge clk)    q &lt;= d;           // q为变量类型endmodule<br/>这种区分源于Verilog的设计哲学：连续赋值用于组合逻辑，过程赋值用于时序逻辑。若错误声明输出类型，会导致编译器拒绝执行或仿真异常。例如，在过程块中驱动一个net类型输出，将产生非法赋值错误。</p><p>4、双向端口：始终为nets双向端口（inout）用于信号在模块间的双向流动，例如数据总线或IO接口。Verilog规定，双向端口必须为net类型，以确保信号的共享与驱动一致性。<br/>示例：module io_buffer (  inout wire data);  assign data = enable ? out_data : 1'bz; // 高阻态控制输出endmodule<br/>在此例中，data可根据控制信号enable决定是否驱动输出。当enable关闭时，data进入高阻态（z），允许外部模块驱动该信号。<br/>双向端口通常在FPGA或ASIC设计中用于总线系统，要求信号线在不同模块之间协调驱动，否则会出现信号冲突。</p><p>5、常见错误与调试建议Verilog端口类型规则明确，但新手在实践中常犯以下错误。过程赋值给netsalways @(posedge clk)  y = d; // 错误：nets不能在过程块中赋值正确做法是将输出定义为变量：output reg y;<br/>输出端口类型声明错误如果输出端口未被过程块驱动，却声明为reg，也会引发语义冲突。output reg y;  assign y = a &amp; b; // 错误：reg不能由assign驱动正确写法应为：output y;assign y = a &amp; b;<br/>双向端口错误声明为变量双向端口必须是net，否则多个驱动器会引发逻辑冲突：inout reg data; // 错误应改为：inout wire data;<br/>端口方向混淆部分初学者在设计模块时忽略信号方向，导致数据流不一致。应确保上层模块的输出连接下层模块的输入，双向端口则需明确高阻态控制。</p><p>6、Verilog-2001的改进：端口内联声明在Verilog-2001标准中，允许在模块头部直接定义端口类型与方向：module adder (  input wire [3:0] a, b,  output reg [4:0] sum);<br/>这种写法既简洁又直观，避免了早期Verilog-1995中必须分两步声明的繁琐结构。此外，现代工具也支持logic关键字替代reg，使代码兼容性更强。</p><p>7、端口类型选择的设计原则总结Verilog端口类型规则：<br/><img width="723" height="166" referrerpolicy="no-referrer" src="/img/bVdnrCr" alt="" title=""/><br/>设计时的关键思路是：谁驱动信号，就决定信号的类型。组合逻辑使用nets，时序逻辑使用reg，双向接口保持net结构。<br/>端口是模块交互的语言，类型是这门语言的语法。理解输入、输出与双向端口的类型规则，是写出可综合、高可靠Verilog代码的基础。掌握端口类型与驱动的关系，就能在设计初期避免许多低级错误，让电路在仿真与硬件中都能“说得通”。</p><pre><code>                    END</code></pre><p>《EDA网院》出品 · 与全球工程师一起探索芯片的世界</p>]]></description></item><item>    <title><![CDATA[Linux 麒麟系统安装 gcc-7.3.0 rpm 包步骤 无邪的课本 ]]></title>    <link>https://segmentfault.com/a/1190000047494389</link>    <guid>https://segmentfault.com/a/1190000047494389</guid>    <pubDate>2025-12-22 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>​</p><ol><li>找到 rpm 文件</li></ol><p><strong>安装包下载：</strong><a href="https://link.segmentfault.com/?enc=BKVJwZz2pP%2Bw0luTfFd%2FAQ%3D%3D.4KR6dxHrr4gqbg15fMrohDVrTuGTabJuCRSj9Fe3Jyy7K5p054K%2FRYq7t3Qs5roY" rel="nofollow" title="https://pan.quark.cn/s/9aac910b9f81" target="_blank">https://pan.quark.cn/s/9aac910b9f81</a>，下载完一般在 <strong>下载</strong>​ 目录，文件名：</p><pre><code>gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title="/></p><p>先确认一下：</p><pre><code>ls ~/下载/gcc-7.3.0*</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果是英文环境：</p><pre><code>ls ~/Downloads/gcc-7.3.0*</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>2. 打开终端</h3><p>右键桌面 → “打开终端”，或者按 <code>Ctrl + Alt + T</code>。</p><ul><li><ul><li>*</li></ul></li></ul><h3>3. 切换到 rpm 所在目录</h3><pre><code>cd ~/下载</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>英文路径：</p><pre><code>cd ~/Downloads</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>4. 检查是否已经安装了 gcc</h3><p>先试试：</p><pre><code>gcc --version</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果提示 “command not found” 就是没装；如果有版本号，想换版本就继续往下看。</p><ul><li><ul><li>*</li></ul></li></ul><h3>5. 安装 rpm 包</h3><p><strong>推荐方法</strong>（会自动装依赖）：</p><pre><code>sudo yum install ./gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>注意 <code>./</code> 不要漏，表示安装当前目录的文件。</p><p>如果用 rpm 直接装（不推荐，容易缺依赖）：</p><pre><code>sudo rpm -ivh gcc-7.3.0-20190804.35.p06.ky10.x86_64.rpm</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>如果报错缺少依赖，就用 yum 补包，比如：</p><pre><code>sudo yum install glibc-devel</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><ul><li><ul><li>*</li></ul></li></ul><h3>6. 验证安装结果</h3><p>装完后运行：</p><pre><code>gcc --version</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>看到类似：</p><pre><code>gcc (Kylin) 7.3.0</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p><p>说明安装成功。</p><ul><li><ul><li>*</li></ul></li></ul><h3>7. 常见问题</h3><ul><li><strong>权限不够</strong>：命令前加 <code>sudo</code>。</li><li><strong>依赖缺失</strong>：尽量用 <code>yum install</code> 安装 rpm 包，让系统自己找依赖。</li><li><p><strong>已有旧版本</strong>：可先卸载：</p><pre><code>sudo yum remove gcc</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000041378096" alt=" title=" title=" title=" loading="lazy"/></p></li><li><strong>安装后命令找不到</strong>：关闭终端重新打开，或执行 <code>source ~/.bashrc</code>。</li></ul><p>​</p>]]></description></item><item>    <title><![CDATA[推荐用于制造业的设备智能助手有哪些核心功能与应用场景？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047494197</link>    <guid>https://segmentfault.com/a/1190000047494197</guid>    <pubDate>2025-12-22 19:05:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>设备智能助手的定义与核心功能<br/>在现代制造业转型升级的关键阶段，人工智能技术的引入正在重构生产管理的智能化水平。设备智能助手作为这一趋势的核心产物，本质上是一种基于人工智能技术的生产辅助系统，它能够通过多模态感知、知识推理和自然语言交互，为生产管理者、工艺工程师和一线操作人员提供实时决策支持和问题解决方案。<br/>设备智能助手的核心价值在于其具备跨领域知识整合能力和实时响应能力，它不仅仅是一个工具，更是制造业知识体系的数字化载体。例如，在注塑生产线管理中，智能助手可以同时理解材料特性、模具设计、温度控制、设备运行数据等多维度信息，实现从“经验驱动”到“数据驱动”的管理范式转变。<br/>其功能体系主要包括：<br/>即时问题诊断：面对设备故障或生产异常时，系统可在数秒内完成多维度分析，提供从简单排查到复杂修复的建议；<br/>工艺优化指导：基于历史数据和行业最佳实践，推荐参数组合并预测效果；<br/>质量标准解读：针对模糊或争议的检测标准，提供权威解释和操作参考；<br/>动态学习能力：通过持续交互不断优化知识库，适应不同企业的个性化需求。<br/>设备智能助手在制造业中的关键作用<br/>在制造业的生产现场中，设备智能助手扮演着“技术参谋”与“生产管家”双重角色。它不仅提高了生产效率，还显著降低了因设备异常、工艺失误和质量波动所导致的生产风险与成本。<br/>提升响应速度是智能助手带来的最直观改变。当生产线突发异常时，传统响应流程可能需要等待技术专家或查阅大量文档，而智能助手能够在几分钟内提供解决方案。<br/>增强决策科学性也是其重要优势。设备智能助手通过融合实时数据与知识图谱，能够为复杂场景下的工艺调整、设备调度和质量控制提供可靠依据。例如，在焊接工艺中，系统可根据材料特性实时推荐最佳参数，显著减少试错成本。<br/>此外，智能助手还促进了知识沉淀与传承，将原本分散在资深工程师经验中的专业知识系统化、结构化，避免了人才流失对企业技术能力的影响。<br/>设备智能助手的应用案例<br/>一、注塑生产线的智能优化案例<br/>，2025年在广东某企业案例中，系统在检测到产品出现翘曲后，通过算法推荐改用稍低的熔体温度并延长冷却时间，使产品合格率从87%提升至96%。同时，系统还提供了预期效果分析，帮助企业评估调整后的风险。<br/>二、焊接工艺的智能参数推荐<br/>在某新能源汽车电池生产车间，设备智能助手被用于焊接工艺管理。面对不同材料的新产品导入，系统能够根据材料特性自动推荐焊接参数，并提供前期调试建议，缩短工艺开发周期。在2025年的测试中，系统对新导入的某型号电池壳体的焊接参数推荐准确率达到92%，不仅大幅减少了工人的调试时间，还显著提升了焊接质量的一致性。<br/>三、设备操作标准化与培训<br/>广域铭岛Geega平台在成都汽车焊装车间的设备智能助手将焊接操作转化为 12项关键参数，实时抓拍工人的操作动作，给出评分和优化建议。新员工通过AI教练的指导，操作合格率迅速提升至 90%，接近老师傅水平。培训周期缩短 70%，设备操作错误率降低 80%。</p>]]></description></item><item>    <title><![CDATA[PMO实战：AI研发效能度量（DORA×SPACE）路线图 PM老周 ]]></title>    <link>https://segmentfault.com/a/1190000047494227</link>    <guid>https://segmentfault.com/a/1190000047494227</guid>    <pubDate>2025-12-22 19:04:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>DORA 2025 报告指出：AI 采用率上升可能伴随吞吐与稳定波动，根因在于交付基本功与治理护栏没跟上。本文用 DORA×SPACE 给 PMO 一套 AI 研发效能度量路线图：先对齐口径，再做可对照试点，最后规模化治理，并说明如何把 AI 放进研发管理流程、跑成持续改进闭环。</p><h4>阅读本文你将获得：</h4><ul><li>一套可复用的 AI 研发效能度量指标体系：DORA（结果）× SPACE（机制）</li><li>一条从 0 到规模化的 PMO 路线图（含试点实验与护栏）</li><li>一张“症状—机制—指标”映射思路，避免只谈使用率与工时</li><li>一个“把 AI 放进流程”的落地方式：以 <a href="https://link.segmentfault.com/?enc=RN0lj9059UTCqWIrIkQWpA%3D%3D.Pq34Tkbdigs7gcyvtndSGDt3o6AgPFK5%2Fk%2BQ9UpNx9A%3D" rel="nofollow" target="_blank">ONES Copilot</a> 为例（不绑定工具，可替换）</li></ul><h2>AI 研发效能度量、DORA、SPACE 到底是什么</h2><p><strong>1. AI 研发效能度量是什么？</strong></p><p>一句话：用可验证的数据衡量“AI 是否让交付更快更稳、更可持续”，并据此驱动改进。它不等于“AI 使用次数”，也不等于“节省工时”，而是把 AI 放进端到端交付系统，衡量系统结果与系统机制。</p><p><strong>2. DORA 四项指标（软件交付性能）</strong></p><p>DORA 给出衡量软件交付结果的“四项关键指标（Four Keys）”。</p><ul><li>部署频率（Deployment Frequency）</li><li>变更前置时间（Lead Time for Changes）</li><li>变更失败率（Change Failure Rate）</li><li>故障恢复时间（Time to Restore Service）</li></ul><p><strong>3. SPACE 五维（开发者生产力）</strong></p><p>SPACE 强调：生产力不能用单一指标定义，需要从五个维度组合观测：满意度与幸福感、绩效、活动、沟通协作、效率与心流。</p><p>总结一下，从治理语言翻译：DORA 看“交付结果”，SPACE 解释“为什么结果变成这样”。</p><h2>方法论：PMO 的 DORA×SPACE 路线图（从0到规模化）</h2><p>我在不少企业的复盘会上听过类似对话：研发说“写得更快了”，测试说“回归更重了”，运维说“变更更频繁但故障没少”，而 PMO 最难回答的是：“领导问 ROI，我们只剩使用次数。”</p><p>很多时候，问题不在 AI，而在落地方式——把 AI 当成 IDE 插件，只能优化局部；PMO 关心的是端到端交付系统。更有效的做法，是把 AI 放进研发管理流程里：让它落在“工作项、文档、动态总结、项目数据洞察”这些可治理、可追溯的对象上。比如 ONES Copilot，就是围绕这些对象提供智能创建工作项、文档生成、总结动态、筛选查找与数据洞察等能力，并强调透明、负责、可控的原则。</p><h4>路线图总览（一屏版）</h4><ul><li>对齐目标与边界</li><li>建立指标字典（DORA×SPACE×AI三层）</li><li>试点做成实验（基线/对照/护栏/停机条件）</li><li>规模化治理（权限、透明追溯、质量门禁）</li><li>变成持续改进引擎（洞察→决策→行动闭环）</li></ul><h4>第 1 阶段：对齐目标与边界（2–4 周）</h4><p>AI 不是“买工具项目”，而是“交付系统再设计项目”。PMO 先把三件事写清楚：<br/>北极星目标（建议一条就够）：例如，在不提高变更失败率的前提下，将核心服务 Lead Time 缩短 30%。</p><ul><li>度量对象边界：按服务/产品线拆分，避免大盘混算。</li><li>AI 治理边界（先立护栏）：把“责任归属、权限控制、透明追溯”写进章程。</li></ul><p>ONES Copilot 的原则表述（透明优先、负责、可控、人类监督与审查）很适合直接转译成 PMO 的治理条款：关键输出必须经人类评审，权限遵从所有者设定，动作通过日志可追踪。</p><p>这一步看似慢，但它决定后面你能不能把数据说清、把风险控住。</p><h4>第 2 阶段：建立指标字典（4–6 周）</h4><p>AI 研发效能度量最怕“指标很全、口径在吵”。建议分三层建字典：</p><p><strong>2.1 结果层：DORA 四项（速度×稳定）</strong></p><p>先把 DORA 四项跑起来，并统一口径：</p><ul><li>Lead Time 起点/终点：提交→合入→发布？</li><li>部署频率：只算生产成功发布还是包含灰度？</li><li>失败定义：回滚算不算、事故等级如何映射？</li></ul><p>DORA 官方对“四项指标”的定义与定位非常清楚，适合作为口径基准。</p><p><strong>2.2 机制层：SPACE 五维（解释原因）</strong></p><p>用 SPACE 解释 DORA 的变化，优先选“低摩擦、可持续”的采集方式：</p><ul><li>S：双月脉搏问卷（工具摩擦、认知负荷、信任与焦虑）</li><li>A：PR 周期、评审等待、变更批次</li><li>C：评审往返轮次、跨团队依赖、返工原因</li><li>E：WIP、等待占比、被打断次数</li><li>P：与产品结果/OKR挂钩，但避免落到个人产出考核</li></ul><p>SPACE 的五维定义与“不要用单指标衡量生产力”的主张，是这层的理论底座。</p><p><strong>2.3 AI 层：从“使用率”升级为“贡献率”</strong></p><p>把 AI 指标拆成三类，避免“热闹但无用”：</p><ul><li>Leading（采用与熟练度）：覆盖哪些工作类型（需求/文档/排障/测试/编码）、有效采纳率</li><li>Guardrail（风险护栏）：AI 相关缺陷占比、安全告警趋势、团队信任度</li><li>Lagging（交付影响）：最终回扣 DORA 四项趋势</li></ul><p>如果你的 AI 能把动作留痕，PMO 的口径工作会轻很多。以 ONES Copilot 为例，它强调所有生成结果可通过日志与标记追踪，动作被详细记录，并要求关键输出经人类评审才进入协作流程——这类机制能显著降低“AI 到底改了什么”的争议，让复盘更像治理而不是辩论。<br/><img width="723" height="422" referrerpolicy="no-referrer" src="/img/bVdnrz4" alt="" title=""/></p><h4>第 3 阶段：试点与实验设计（6–12 周）</h4><ul><li>试点要像实验：有基线、有对照、有护栏、有停机条件。推荐从“管理链路低风险闭环”先赢一仗</li><li>智能创建工作项：把原始反馈/会议纪要转成结构化工作项</li><li>文档生成与优化：减少知识沉没，提高可复用性</li><li>总结动态与相似工单：让评审与决策基于事实</li><li>自然语言筛选与查找：降低检索成本，把信息流做顺</li></ul><p>这些能力与对象（工作项、文档、动态、数据洞察）天然更利于 PMO 度量与治理。</p><p><strong>试点章程建议包含 5 个硬要素</strong></p><ul><li>基线期：2–4 周先跑通数据，识别自然波动</li><li>成功标准：Lead Time ↓，且 CFR 不上升（或可控下降）</li><li>护栏指标：评审等待不许失控；回归缺陷不许飙升</li><li>停机条件：稳定性连续恶化且无法解释时，立即收敛范围</li><li>复盘节奏：每两周一次（DORA 看结果 + SPACE 找原因 + 行动项闭环）</li></ul><p>记住：试点不是证明“AI 很强”，而是证明“系统交付更好”。</p><h4>第 4 阶段：规模化与治理（3–6 个月）</h4><p>规模化的关键，不是“全员开通”，而是“护栏前置”。PMO 建议制度化三件事：</p><ul><li>权限与可控：不同角色能用 AI 写入哪些字段？哪些输出必须二次确认？</li><li>透明与追溯：AI 参与过的内容要能回溯来源与改动</li><li>质量门禁：自动化测试、评审清单、灰度与回滚预案</li></ul><p>DORA 2025 的提醒非常现实：如果没有小批量与健壮测试机制，改进开发过程不一定带来交付改善；规模化时更要把这些基本功变成制度，而不是寄希望于“工具自带魔法”。</p><h4>第 5 阶段：把路线图做成“持续改进引擎”（长期）</h4><p>到这一阶段，AI 不再是一个项目，而是一种能力：组织学会在不确定性中持续改进。</p><p>建议沉淀三张“管理可读”的图：</p><ul><li>DORA 结果看板：速度与稳定趋势</li><li>SPACE 机制看板：摩擦点与瓶颈</li><li>AI 影响图谱：哪些场景值得加码，哪些要收敛或加护栏</li></ul><p>你会发现，PMO 的工作开始从“追数字”升级为“给洞察、推行动”：用数据把讨论从“感觉更忙”拉回到“系统哪里卡住了、下一步改什么”。</p><h2>FAQ：</h2><p><strong>Q1：AI 研发效能度量最小可行版本（MVM）是什么？</strong></p><p>A：先跑通 DORA 四项趋势，再用 SPACE 选 3–5 个低摩擦指标解释波动；AI 指标只做“采用、护栏、交付影响”三层即可。</p><p><strong>Q2：DORA 与 SPACE 是竞争关系吗？</strong></p><p>A：不是。DORA 是交付结果，SPACE 是机制解释；对 PMO 来说是“结果+因果”的组合拳。</p><p><strong>Q3：PMO 如何回答“AI 的 ROI”？</strong></p><p>A：用“护栏下的交付改善”回答：Lead Time 是否缩短、CFR 是否可控、恢复是否更快；同时用 SPACE 证明摩擦点是否减少，而不是用使用次数。</p><p><strong>Q4：怎样避免 AI 让稳定性变差？</strong></p><p>A：坚持小批量、强测试、质量门禁与可追溯审查；把 AI 输出纳入流程治理，而不是只在个人侧加速产出。</p><p><strong>Q5：ONES Copilot 在这套体系里扮演什么角色？</strong></p><p>A：它是一种“把 AI 放进可治理对象”的落地方式：围绕工作项、文档、动态总结与数据洞察，让 PMO 更容易做口径对齐、留痕追溯与闭环复盘；同样的治理逻辑也可迁移到其他平台。</p><h2>结尾：让 AI 成为“可度量的改进资产”</h2><p>AI 时代，PMO 的价值不是做更漂亮的周报，而是让组织具备一种能力：用 DORA 看结果，用 SPACE 找原因，用试点实验验证杠杆，用治理护栏放大收益。</p><p>如果你的组织已在使用 ONES 研发管理平台，那么 ONES Copilot 围绕工作项、文档、动态总结、查找筛选与数据洞察的能力，可以成为你把 AI 研发效能度量跑成闭环的“天然载体”；如果你使用的是其他平台，也同样可以借鉴这套思路：工具会变，治理逻辑不变——把 AI 放进流程、纳入度量、接受审查，才能真正把它变成组织的交付能力。</p>]]></description></item><item>    <title><![CDATA[Hologres Dynamic Table：高效增量刷新，构建实时统一数仓的核心利器 阿里云大数据]]></title>    <link>https://segmentfault.com/a/1190000047494247</link>    <guid>https://segmentfault.com/a/1190000047494247</guid>    <pubDate>2025-12-22 19:03:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业数据架构逐步走向实时化与一体化的过程中，如何高效处理“大量历史 + 少量新增”的业务数据，已成为建设统一数仓与实时数仓时绕不开的关键挑战。</p><p>传统全量刷新方式在面对亿级历史数据时，往往面临刷新延迟高、计算成本大、链路复杂等问题。为了解决这些痛点，业界逐渐形成了一种新的数据处理范式——Dynamic Table（动态表），它通过声明式语法自动维护物化结果，并支持高效的增量刷新能力。</p><p>阿里云 Hologres 作为高性能实时数仓引擎，原生提供了 Dynamic Table，并基于有状态增量计算模型，在多表关联、聚合等复杂场景下展现出显著性能优势。本文将深入解析 Hologres Dynamic Table 的技术原理与实践价值。</p><h2><strong>为什么需要增量刷新能力？</strong></h2><p><img width="723" height="397" referrerpolicy="no-referrer" src="/img/bVdnrzG" alt="image.png" title="image.png"/></p><h3>典型业务场景</h3><p>在电商、互联网、金融等行业，以下场景极为常见：</p><p><strong>实时运营分析</strong></p><ul><li>订单、支付、退款等多源数据，按用户、商品、活动等维度进行关联与聚合，形成实时运营看板；</li><li>运营与业务团队希望以分钟级、甚至更高频率刷新核心指标。</li></ul><p><strong>用户与商品特征构建</strong></p><ul><li>将用户信息、行为数据、订单数据、商品信息、支付信息等多张表进行 Join，生成统一的特征宽表；</li><li>这些宽表通常是推荐、风控、画像等应用的输入，需要稳定、快速地更新。</li></ul><p><strong>资金与交易监控</strong></p><ul><li>交易流水、账户信息、风控结果等数据源不断产生新的记录；需要以较短的刷新间隔计算聚合指标或风控特征。</li></ul><p>这些场景的共同特点是：</p><blockquote><strong>历史数据规模庞大（亿级），但每次新增或变更的数据量很小（通常 &lt;1%）。</strong></blockquote><h3>传统数据加工的局限</h3><p>在缺少增量引擎的情况下，常见做法是：</p><ul><li>使用一条或多条 SQL 定义目标宽表或汇总表；</li><li>定时执行全量计算（如 INSERT OVERWRITE、CTAS），每次从基表完整扫描数据、执行多表 Join、再进行聚合；</li><li>通过调度系统编排上游任务和下游任务之间的依赖。</li></ul><p>这种模式存在若干明显不足：</p><ul><li><strong>刷新时延受限：</strong>数据规模增大后，每次全量扫描和计算耗时较长。当业务希望从“每小时刷新”提升到“每 5 分钟刷新”时，往往需要成倍增加计算资源，也可能遇到物理资源上限。</li><li><strong>计算成本较高</strong>：即使每次只有 1% 左右的数据发生变化，全量模式仍需对 100% 数据进行扫描和计算，CPU、IO 和网络资源利用效率不高。</li><li><strong>链路复杂，维护成本高</strong>：为降低单任务压力，工程实践中常将复杂逻辑拆解为多层中间表，形成较长的任务链路。链路越长，依赖越多，维护难度和变更风险也随之上升。</li></ul><p>因此，在“历史数据量大、实时数据实时产生”的场景下，引入真正高效的增量刷新机制，是提升数据时效与降低资源利用率的关键。</p><h2>二、什么是 Dynamic Table？Hologres 的增量刷新如何工作？</h2><p>Dynamic Table是当前一种主流的声明式数据处理架构，该架构可以自动处理并存储一个或者多个基表（Base Table）对象的数据关联、聚合结果，内置不同的数据刷新策略，业务可以根据需求设置不同的数据刷新策略，实现数据从基表对象到Dynamic Table的自动流转，满足业务统一开发、数据自动流转、处理时效性等诉求。</p><p>增量（incremental）、全量（full）是Dynamic Table的两种不同刷新方式，底层实现原理具有显著的差异：</p><ul><li><strong>全量刷新</strong>是指每次执行刷新时，都以全量的方式进行数据处理，并将基表的关联、聚合结果物化写入Dynamic Table，其技术原理类似于INSERT OVERWRITE。</li><li><strong>增量刷新</strong>模式下，每次刷新时只会读取基表中<strong>新增</strong>的数据，根据中间聚合状态和增量数据计算最终结果并更新到Dynamic Table中。相比全量刷新，增量刷新每次处理的数据量更少，效率更高，从而可以非常有效地提升刷新任务的时效性，同时降低计算资源的使用。</li></ul><p>增量Dynamic Table的计算遵循如下计算模式：</p><ol><li><strong>增全量刷新</strong>阶段：Dynamic Table的第一次刷新会把已有的所有历史数据都进行计算，即相当于把所有历史数据都当作增量来进行计算。这一阶段一般耗时比较长。</li><li><strong>增量刷新</strong>阶段：在增全量刷新完成后，以后的每次Dynamic Table刷新仅针对增量数据进行计算。理论上这些刷新应该耗时较短。</li></ol><p>在增量 Dynamic Table 的实现上，业界存在不同的技术路径。一种常见的做法是采用<strong>无状态增量计算模型</strong>：每次刷新时，系统仅基于源表的变更数据，重新推导整个查询逻辑，而不持久化任何中间计算状态。这种方式虽然节省存储，但在面对复杂查询（如多表关联、去重聚合等）时，往往需要反复扫描大量历史数据，导致刷新效率低下，甚至在某些场景下增量计算的开销反而超过全量。</p><p>相比之下，Hologres 采用了<strong>有状态增量计算模型</strong>：在首次全量构建 Dynamic Table 时，同步生成并持久化关键的中间状态（例如聚合结果、多表 Join 的中间产物等）。后续的增量刷新只需将新增或变更的数据与这些状态表进行高效合并，无需重复处理历史数据。这种设计以有限且可控的额外存储开销为代价，<strong>显著提升了复杂场景下的刷新性能和资源利用率，尤其适合“海量历史 + 少量增量”的典型业务负载</strong>。<br/><img width="723" height="407" referrerpolicy="no-referrer" src="/img/bVdnrzH" alt="image.png" title="image.png" loading="lazy"/></p><h2>三、Hologres 增量刷新的实战优势</h2><p>我们通过三个典型场景验证 Hologres 的性能表现（测试环境：Hologres 15CU Serverless，竞品采用相似规格）。</p><p>对于增量数据，本实验模拟两种场景：</p><ol><li>Append only：即源表的增量数据只有新增（Insert），没有修改（update和delete）。这在日志、埋点数据表中非常常见。</li><li>Retraction（回撤）：源表的增量数据包含Insert、Update和Delete的数据。这适用于数据库类的表。</li></ol><p>对于Append only的源表，很多增量计算算子都可以大幅简化，状态表也可以大幅缩小。所以在性能环节，可以看到Append only源表的增量计算性能会更好。</p><h3>场景 1：单表聚合（COUNT DISTINCT + SUM）</h3><p>该场景使用的工作负载如下所示：<br/><img width="681" height="135" referrerpolicy="no-referrer" src="/img/bVdnrzI" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres建表SQL如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL_AGG with (
    auto_refresh_mode = 'incremental', 
    auto_refresh_enable = 'false', 
    freshness = '1 minutes'
)AS 
SELECT
  PRODUCT_ID,
  COUNT(DISTINCT USER_ID) AS UV,
  SUM(LINE_AMOUNT) AS SUM_LINE_AMOUNT,
  MAX(QUANTITY) AS MAX_QUANTITY
FROM ORDER_DETAIL
GROUP BY PRODUCT_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录）：</p><table><thead><tr><th><strong>刷新</strong></th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算刷新耗时(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td>ORDER\_DETAIL: 10M</td><td> </td><td>1.5</td><td>1.3</td><td>4.6</td><td>3.9</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>7.8</td><td>2.4</td><td>0.59</td><td>0.48</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>8.1</td><td>2.7</td><td>0.49</td><td>0.41</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>7.8</td><td>2.9</td><td>0.45</td><td>0.3</td></tr></tbody></table><p>可以看到Hologres增全量刷新阶段较慢，但后面的每次增量刷新都很快，符合预期。</p><p>无状态增量刷新性能较差的主要原因是无状态增量执行计划变得更加复杂，包含36个计算节点，且变更数据触及了大量分区，需要从源表重新扫描计算所有的数据。在有回撤数据时，处理数据变更前后因果关系会导致计算逻辑会变得更加复杂，进一步变慢，增量计算显得没有意义。</p><p>Hologres增量刷新快的核心原因是每次增量计算都是基于上次计算生成的状态表（State），这极大的简化了增量计算逻辑。此例中Hologres中各表存储大小如下所示（源表是Retraction的情况），结果表很小，而因为min/max/count distinct这两种聚合函数与sum/count这类不同，状态表需要存储对应列的所有原始数据，所以相比结果表要大很多。</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>252 MB</td><td>1 MB</td><td>93 MB</td></tr></tbody></table><h3>场景 2：两表 Join（订单 + 明细）</h3><p>两表Join是大数据处理中一种较为简单的基本场景，测试使用的具体工作负载如下图所示<br/><img width="723" height="221" referrerpolicy="no-referrer" src="/img/bVdnrzU" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres建表SQL如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL
WITH (
  auto_refresh_mode = 'incremental', 
  auto_refresh_enable = 'false', 
  freshness = '1 minutes'
) AS 
SELECT
  o.ORDER_ID,
  o.ORDER_DATE,
  o.ORDER_STATUS,
  oi.ORDER_ITEM_ID,
  oi.PRODUCT_ID,
  oi.QUANTITY,
  oi.UNIT_PRICE,
  oi.LINE_AMOUNT
FROM ORDERS o
JOIN ORDER_ITEMS oi
  ON o.ORDER_ID = oi.ORDER_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录），无状态增量刷新引擎在数据含有回撤的时候执行计划包含50个节点，源表大部分分区的数据被反复读取参与聚合，导致性能不佳。而数据不包含回撤（Appendonly）时，无状态增量刷新执行计划相对简单，含有35个节点，且新增数据不会触及太多分区，表现良好，体现出了增量计算的意义。</p><table><thead><tr><th> </th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td><strong>ORDERS</strong>: 5M<br/><strong>ORDER\_ITEMS</strong>: 10M</td><td> </td><td>10</td><td>8.7</td><td>9.2</td><td>6.29</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>22</td><td>1.1</td><td>2.2</td><td>0.68</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>19</td><td>2.2</td><td>1.1</td><td>0.50</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>22</td><td>1.7</td><td>1.9</td><td>0.51</td></tr></tbody></table><p>Hologres中各表存储大小如下所示，在这种场景下状态表存了源表的部分列数据，实际存储小于源表。</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>370 MB</td><td>350 MB</td><td>228 MB</td></tr></tbody></table><h3>场景 3：五表复杂 Join（订单 + 用户 + 商品 + 支付等）</h3><p>多表Join是一种相对更常见、真实的场景，测试具体使用的工作负载如下:<br/><img width="723" height="268" referrerpolicy="no-referrer" src="/img/bVdnrzT" alt="image.png" title="image.png" loading="lazy"/></p><p>Hologres的测试脚本如下：</p><pre><code class="sql">CREATE DYNAMIC TABLE DT_ORDER_DETAIL
WITH (
  auto_refresh_mode = 'incremental', 
  auto_refresh_enable = 'false', 
  freshness = '1 minutes'
) AS 
SELECT
  o.ORDER_ID,
  o.ORDER_DATE,
  o.ORDER_STATUS,
  u.USER_ID,
  u.USER_NAME,
  u.EMAIL,
  u.STATUS AS USER_STATUS,
  oi.ORDER_ITEM_ID,
  oi.PRODUCT_ID,
  p.PRODUCT_NAME,
  p.CATEGORY,
  p.PRICE       AS PRODUCT_PRICE,
  oi.QUANTITY,
  oi.UNIT_PRICE,
  oi.LINE_AMOUNT,
  pay.PAYMENT_ID,
  pay.PAY_AMOUNT,
  pay.PAY_METHOD,
  pay.PAY_TIME
FROM ORDERS o
JOIN USERS u
  ON o.USER_ID = u.USER_ID
JOIN ORDER_ITEMS oi
  ON o.ORDER_ID = oi.ORDER_ID
JOIN PRODUCTS p
  ON oi.PRODUCT_ID = p.PRODUCT_ID
LEFT JOIN PAYMENTS pay
  ON o.ORDER_ID = pay.ORDER_ID;</code></pre><p>测试结果如下表所示（完整测试SQL见附录），无状态增量计算引擎无论是只包含插入还是有回撤，性能都相对较差，原因也是类似的。五表Join的场景下，增量计算的执行节点超过140个，会大量读取源表数据。</p><table><thead><tr><th> </th><th><strong>源表行数</strong></th><th> </th><th><strong>某国际知名产品无状态增量计算(s)</strong></th><th> </th><th><strong>Hologres刷新耗时(s)</strong></th><th> </th></tr></thead><tbody><tr><td> </td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td><td><strong>Retraction</strong></td><td><strong>Appendonly</strong></td></tr><tr><td><strong>增全量刷新</strong></td><td><strong>ORDER\_ITEMS</strong>: 10M<br/><strong>PAYMENTS</strong>: 4.9M<br/><strong>ORDERS</strong>: 5M<br/><strong>PRODUCTS</strong>: 50K<br/><strong>USERS</strong>: 500K</td><td> </td><td>23</td><td>23</td><td>30</td><td>22</td></tr><tr><td><strong>增量第一次</strong></td><td>每张表 <br/>UPDATE 0.5%<br/>INSERT 0.5%</td><td>每张表 <br/>INSERT 1.0%</td><td>55</td><td>26</td><td>3.9</td><td>2.8</td></tr><tr><td><strong>增量第二次</strong></td><td> </td><td> </td><td>58</td><td>27</td><td>3.2</td><td>1.8</td></tr><tr><td><strong>增量第三次</strong></td><td> </td><td> </td><td>60</td><td>26</td><td>2.9</td><td>1.8</td></tr></tbody></table><p>Hologres中各表存储大小如下所示，状态表会存储每一次Join的中间结果</p><table><thead><tr><th> </th><th><strong>源表</strong></th><th><strong>结果表</strong></th><th><strong>状态表</strong></th></tr></thead><tbody><tr><td><strong>存储</strong></td><td>594 MB</td><td>1218 MB</td><td>1094 MB</td></tr></tbody></table><h2>四、有状态增量计算：为何更高效？</h2><p>基于前面的实验结果，不难看出，无状态增量计算方案适用条件其实比较苛刻，在很多场景中基于少量数据的增量计算开销甚至会超过第一次的全量计算，丧失了增量计算的意义。</p><p>而相比较之下，Hologres的有状态增量计算方案可以适用于大多数的场景，通常只需要满足增量数据较少这一个条件即可。下图以单表聚合场景(<code>sum(value) group by key</code>)为例，展示了有状态方案的基本原理，增量计算过程中可以从状态表中直接获取历史数据的聚合结果，而不需要基于历史表的原始数据重新计算。此外在读取状态表数据时，也进一步引入了OLAP查询中常用的runtime filter优化，在增量数据较少的场景中，大幅减少状态表数据读取量，使刷新性能得到了显著的提升。<br/><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdnrzY" alt="image.png" title="image.png" loading="lazy"/></p><p>有状态方案一个显著的缺点是与无状态方案相比会需要占用额外的存储空间用于状态表的存储。如上述实验数据所示，实际额外存储大小通常与涉及到的源表、结果表的大小相关。</p><p>这一缺点通常是可以接受的，因为在业务实践操作中，这部分存储一般是可控的,不会无限增长。这是因为：</p><ul><li>分区表的场景中，只有活跃分区需要状态表，历史分区转全量刷新后不再需要状态表（这个过程是自动的，分区不再活跃后会自动清理状态表）。因此只有最近的一两个分区才需要状态表，这极大地减少了状态表的存储空间。因此Hologres Dynamic Table增量计算状态表没有Flink常见的状态膨胀问题。</li><li>非分区表场景中，可以为状态表配置合适的TTL，丢弃一些不再需要的历史状态减少存储空间</li></ul><h2>总结：Hologres Dynamic Table 的核心价值</h2><p>面对企业日益增长的实时分析需求，Hologres 的 Dynamic Table 通过有状态增量计算引擎，从根本上解决了传统方案在复杂查询下“增量不增效”的痛点。它不仅大幅缩短了数据刷新延迟，还显著降低了计算资源消耗和运维复杂度，真正实现了“写一次 SQL，自动高效更新”的体验。无论你是构建实时看板、用户画像宽表，还是风控特征管道，Hologres 都能以稳定、高性能、低成本的方式支撑你的核心数据链路。</p><h2>附录</h2><h3>无状态 &amp; 有状态增量计算底层原理对比分析</h3><p>从上面的实验结果来看，Hologres的有状态实现方案在大多数的场景中是要优于无状态增量计算引擎的，本小节将对两者的底层计算原理进行对比分析，说明造成这种性能差异的根本原因。</p><h4>符号说明</h4><p><img width="723" height="379" referrerpolicy="no-referrer" src="/img/bVdnrAd" alt="image.png" title="image.png" loading="lazy"/></p><h4>多表Join场景</h4><h5>无状态实现Hologres Dynamic Table：高效增量刷新，构建实时统一数仓的核心利器</h5><p>无状态多表Inner Join的增量计算公式如下（原理可参考<a href="https://link.segmentfault.com/?enc=H5kzk0H1LZqTSiA40XsElg%3D%3D.vRiV9k43KJDTqDPLpt3d68HbVvooo7Gjfe%2FiT0%2B7Fp8k%2FXl64BZJ55gfKgM6mz27" rel="nofollow" target="_blank">论文</a>），根据实际执行计划推测某无状态增量计算引擎的多表Join增量计算应该也是基于该公式实现的<br/><img width="723" height="151" referrerpolicy="no-referrer" src="/img/bVdnrAe" alt="image.png" title="image.png" loading="lazy"/><br/>该方案主要会有如下弊端：<br/><img width="723" height="155" referrerpolicy="no-referrer" src="/img/bVdnrAf" alt="image.png" title="image.png" loading="lazy"/></p><h5>有状态实现</h5><p>Hologres的有状态多表Join计算公式原理大致如下，因为所有的中间计算结果状态会通过状态表（State）持久化保存下来，因此可以按照两表Join的公式做简单展开<br/><img width="723" height="79" referrerpolicy="no-referrer" src="/img/bVdnrAh" alt="image.png" title="image.png" loading="lazy"/></p><p>有状态方案以额外的存储开销为代价换来了：<br/><img width="723" height="104" referrerpolicy="no-referrer" src="/img/bVdnrAk" alt="image.png" title="image.png" loading="lazy"/></p><h4>单表聚合场景</h4><p>单表聚合场景相对较为简单，无状态、有状态两种方案的计算公式原理分别如下：<br/><img width="723" height="118" referrerpolicy="no-referrer" src="/img/bVdnrAn" alt="image.png" title="image.png" loading="lazy"/></p><p>公式大体是比较相似的，主要区别是Hologres的有状态方案持久化存储了历史数据的聚合结果，因此有以下优势：</p><ul><li>不需要重新计算历史数据的聚合结果</li><li>State表数据量极小，Join 操作可以显著减少读取数据量</li></ul>]]></description></item><item>    <title><![CDATA[Excelize 开源社区荣获 2025 红山开源明星项目三等奖 xuri ]]></title>    <link>https://segmentfault.com/a/1190000047494249</link>    <guid>https://segmentfault.com/a/1190000047494249</guid>    <pubDate>2025-12-22 19:02:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2025 年 11 月 16 日，由红山开源平台主办的“2025 红山开源大会”，在北京香山颐和宾馆红山厅成功落下帷幕。为表彰在开源领域做出卓越贡献的项目，大会特别设立 2025 年度红山开源平台明星项目颁奖环节。</p><p>Excelize 开源社区荣获了 2025 红山开源明星项目三等奖，活动详情详见文章：<a href="https://link.segmentfault.com/?enc=0fY6kt4dAVh%2F%2F9WhDy5GMQ%3D%3D.ESXqDX15mNQMCo4fjdWRAJqISF83iqSEbZF3gcRBXtOWBV3YsgWIWOEFmeOondtIYp4%2FYGkjOpfbayeKXECKtw%3D%3D" rel="nofollow" target="_blank">2025 红山开源大会圆满收官，共筑开源生态新未来</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494251" alt="Excelize 荣获 2025 红山开源明星项目三等奖" title="Excelize 荣获 2025 红山开源明星项目三等奖"/></p><p>Excelize 是用于操作电子表格办公文档的开源基础库，开源地址：<a href="https://link.segmentfault.com/?enc=1AAjbIbAFAnNBfhvHDkiOQ%3D%3D.Qvq75emknm8lr13YXPatxBnlACpUJ12YHQLIOVYRBk6BX10OMY7DYj7I%2FmxYwcwX" rel="nofollow" target="_blank">github.com/xuri/excelize</a>，遵循 BSD 3-clause 开源协议，基于 ISO/IEC 29500 国际标准。可以使用它来读取、写入由 Excel 、WPS 、OpenOffice 等办公软件创建的电子表格文档。支持 XLAM / XLSM / XLSX / XLTM / XLTX 等多种文档格式，高度兼容带有样式、图片 (表)、透视表、切片器等复杂组件的文档，并提供流式读写支持，用于处理包含大规模数据的工作簿。可应用于各类报表平台、云计算、边缘计算等系统。自 2016 年开源以来已成为云原生应用尤其是 Go 语言开发者在处理电子表格办公文档时的热门选择，正在被广泛应用于大型互联网公司、中小企业客户和初创公司。</p><p>Excelize 开源基础库曾荣获 2025 上海开源创新菁英奖——优秀开源项目奖、2025 年 GitCode 百大开源项目、2022 年中国开源创新大赛一等奖、2018 年开源中国码云最有价值开源项目 GVP (Gitee Most Valuable Project) 等奖项。</p>]]></description></item><item>    <title><![CDATA[招聘终极战场：AI重构首轮筛选的精准与效能革命 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047494313</link>    <guid>https://segmentfault.com/a/1190000047494313</guid>    <pubDate>2025-12-22 19:02:00</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>招聘终极战场：AI重构首轮筛选的精准与效能革命<br/>国务院《关于深入实施“人工智能+”行动的意见》明确划定：2027年，70%的岗位面试将由AI或智能体完成。当智能化浪潮不可逆地渗透企业运营，你的招聘体系是否仍停留在简历海投、人力初筛、凭感觉提问的“前AI时代”？效率鸿沟已悄然拉开——当对手用AI数小时内完成千人筛选与精准初评，你的团队是否还在无尽简历与重复问答中消耗战略时间？HR向“数据驱动的决策伙伴”转型，已非未来愿景，而是当下生存必备技能，而这场转型的关键，始于面试智能化的两大核心：评估精准度与候选人体验。</p><p>一、数据驱动决策：超越直觉的可验证精准<br/>招聘最大的隐性成本是选错人，AI面试智能体将“精准”定义为可严格验证的标准：评分结果既通过与资深面试官“背靠背”对比实验，又经受效标效度与重测信度等心理学指标检验，从“辅助参考”进阶为“决策依据”。这种精准贯穿评估全流程：<br/>•一问多能：单题同步评估多项胜任力，无缝衔接初筛与专业复试，评估效率提升超50%；<br/>•智能追问：依据回答即时生成深度问题，如资深面试官般捕捉逻辑漏洞与能力闪光点，杜绝表面化评判；<br/>•简历深挖：自动解析简历关键信息与模糊点，生成递进式提问链，既验证真实性，又挖掘文本掩盖的胜任力；<br/>•全维度覆盖：兼顾通用素质与编程、财务等专业领域精准评估，同步解放HR与业务面试官。<br/>二、体验即品牌：让面试成为雇主形象加分项<br/>糟糕的AI面试体验足以劝退顶尖人才，AI面试智能体将“拟人化交互”作为技术核心，让面试成为雇主品牌传播的重要载体：<br/>•情绪感知交互：识别语速、语调中的紧张或犹豫，通过人性化引导帮助候选人展现最佳状态；<br/>•无缝对话流：无需手动操作“开始/停止”，答案结束后自动衔接下一题，还原真人对话的自然节奏；<br/>•高拟真视觉呈现：唇形与语音精准同步，大幅削弱传统虚拟面试的“机械感”；<br/>•实时答疑能力：候选人可随时咨询职位、团队、福利等问题，AI精准解答，成为传递企业信息的智能窗口。<br/>三、流程革命：招聘全链路迈入“无人驾驶”<br/>面试智能化仅是起点，AI人才寻访智能体将自动化延伸至招聘最前端的寻访环节。它并非简单群发工具，而是能自主完成“筛选-沟通-索要简历-系统录入”全链条动作的智能系统：<br/>•极速启动：30-60秒完成初始化，无需人工值守即可独立运作；<br/>•智能筛选：按预设条件精准筛选平台简历，锁定目标候选人；<br/>•拟人化沟通：发起自然对话，遍历回复所有未读消息，信息缺失时主动“索要简历”；<br/>•系统无缝同步：将获取的简历自动同步至企业ATS，形成完整数据闭环。<br/>这不仅将HR从重复劳动中彻底解放，更通过大模型技术，将寻访动作从“机械执行”升级为“有判断力的决策”。<br/>四、实践验证：顶尖组织的共同选择<br/>AI招聘解决方案已获得西门子中国、招商银行、阿里巴巴国际、TCL及浙江大学等上千家领先企业与高校的认可。通过引入该系统，这些组织成功将招聘流程搭建在智能化基座之上，实现了效率与精准度的双重飞跃，为行业树立了可复制的实践标杆。<br/>智能化趋势已明确，观望即是最大风险。AI招聘的下一轮竞争，始于对新模式的验证与落地。当精准评估、优质体验与全流程自动化形成闭环，招聘将不再是成本消耗，而是驱动企业人才竞争力提升的核心引擎。</p>]]></description></item><item>    <title><![CDATA[FundingRate/资金费率套利 云梦量化科技 ]]></title>    <link>https://segmentfault.com/a/1190000047494330</link>    <guid>https://segmentfault.com/a/1190000047494330</guid>    <pubDate>2025-12-22 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>资金费率介绍</h2><p>由于永续合约没有传统意义上的到期日/交割日，也就没有传统的价格收敛机制，因此需要使用一种新的机制来确保永续合约价格与现货价格锚定在一起，这就是资金费率机制。资金费率是多空双方之间定期交换的费用，通常每8小时结算一次。如果资金费率为正，多头支付空头；如果资金费率为负，空头支付多头。\<br/>当合约价格比现货价格高时，资金费率通常为正，鼓励交易者做空合约以推动价格下跌；同样当合约价格低于现货价格时，资金费率通常为负，鼓励交易者做多合约以推动价格上涨。通过这种机制，永续合约的价格能够与现货价格进行锚定。</p><h2>资金费率的定价</h2><p>根据某安的资金费率定价公式:</p><p>$$
\begin{align}
funding\_rate &amp;= avg\_premium\_index + clamp(interest\_rate - avg\_premium\_index, -0.05\%, 0.05\%) \\
funding\_rate &amp;= clamp(funding\_rate, lower\_bound, upper\_bound)
\end{align}
$$</p><p>其中avg_premium_index是平均溢价指数，表示合约与现货的偏离程度，interest_rate是利率，一般是日化0.03%，lower_bound和upper_bound分别是资金费率的下限和上限。<br/>其中对于平均溢价指数的计算公式为(以8小时周期为例):</p><ol><li><p>首先计算每个时间点的溢价指数, 溢价指数 = [Max(0, 冲击买价 - 价格指数) - Max(0, 价格指数 - 冲击卖价)] / 价格指数</p><pre><code>- 注：其中提到的冲击买价为使用200USDT*最大杠杆的资金冲击买盘，冲击卖价同理，价格指数为各个交易所的现货价格加权平均得到的价格</code></pre></li><li>然后根据不同的时间段赋予不同的权重，具体为，第1分钟权重为1，第2分钟权重为2...第480分钟权重为480，根据时间权重计算得到平均溢价指数</li></ol><p>对于这里提到的溢价指数计算公式，其实可以简单理解为（合约价-现货价）/现货价</p><p>在这个定价公式中，$interest\_rate$是一个非常重要的参数，这个参数决定了使用合约的资金成本（对于多头而言）。考虑此时合约价与现货价格一致，或者合约价与现货价的偏离程度在一个非常小的范围内，那么此时对于$interest\_rate-avg\_premium\_index$就应该位于-0.05%到0.05%的范围内，此时资金费率就等于$funding\_rate=avg\_premium\_index+(interest\_rate-avg\_premium\_index)=interest\_rate$，也就是说此时资金费率就等于利率。那么对于这个日化0.03%的利率来说，年化收益率为0.03%*365=10.95%。对于多头来说，这是非常高的资金成本，但是对于空头来说，却是一个非常好的收益来源。此时考虑买入现货卖出合约以赚取这部分无风险收益。这里选择反向（Reverse）合约，因为反向合约的资金费率更加稳定，长期来看要高于正向（Linear）合约的资金费率收益。</p><h2>买入现货卖出反向合约</h2><p>做空反向合约的盈亏计算方式为:</p><p>$$
\begin{align}
PnL_{short} = number\_of\_contracts * contract\_value * (\frac{1}{exit\_price} - \frac{1}{entry\_price})
\end{align}
$$</p><h3>无风险</h3><p>从直觉上来看，买入现货卖出等量的合约应该是无风险的，实际上也是如此。<br/>举例来说：假设当前现货价格为100000，对应反向合约价格为100000，此时买入1个现货，同时卖出1个反向合约，合约面值为100000。</p><ol><li>如果价格上涨到110000，此时现货价值上涨到110000，现货部分盈利10000；而合约部分亏损为100000 * (1/110000 - 1/100000)=0.0909个现货，乘上当前价格110000，合约部分亏损为10000，总体盈亏为0</li><li>如果价格下跌到90000，此时现货价值下跌到90000，现货部分亏损10000；而合约部分盈利为100000 * (1/90000 - 1/100000)=0.1111个现货，乘上当前价格90000，合约部分盈利为10000，总体盈亏为0</li></ol><p>再看是否存在爆仓风险，假设合约开仓时合约价格为P0, 此时为P, 考虑买入一个现货并卖出等量的反向合约，此时合约面值为1*P0<br/>币本位合约的维持保证金计算方式为:</p><p>$$
\begin{align}
% 先计算权益
equity\_value = 1 * P + 1 * P0 * (\frac{1}{P} - \frac{1}{P0}) * P = P + P0 - P = P0 \\
equity\_quantity = \frac{equity\_value}{P} = \frac{P0}{P} \\
% 计算维持保证金
MM = \frac{MMR*P0}{P}
\end{align}
$$</p><p>其中MMR为维持保证金率<br/>由于MMR为一个小于1的常数，因此有$equity\_quantity=\frac{P0}{P}&gt;\frac{MMR*P0}{P}=MM$，也就是说不会爆仓。</p><h3>收益分析</h3><p>获取了2024-09-30到2025-09-30期间某安上的主流的8个现货（图表标题为其市值排名）的反向合约资金费率数据<br/>计算累计资金费率收益如下图所示：<br/><img width="723" height="732" referrerpolicy="no-referrer" src="/img/bVdnrBG" alt="" title=""/><br/>可以看到基本上都是一条直线往上走，但其中有三个反向合约的累计资金费率收益分别是5.04%, 7.01%, 6.83%, 远小于其余的5个反向合约。考虑到分散风险的角度，构建等权重的组合，计算其累计资金费率收益如下图所示：<img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdnrBH" alt="" title="" loading="lazy"/><br/>进一步将其中收益较低的三个剔除，构建剩余5个的等权重组合，计算其累计资金费率收益如下图所示：<img width="723" height="439" referrerpolicy="no-referrer" src="/img/bVdnrBI" alt="" title="" loading="lazy"/><br/>可以发现其总收益可以达到9.67%，接近理论上的10.95%的无风险收益率。</p><h2>结论</h2><p>由于资金费率定价公式中的设定利率较高，导致这个市场上的无风险收益率也非常高。<br/>这使得通过买入现货并卖出反向合约，可以赚取一个非常高的无风险收益率，可以达到接近10%左右。并且通过组合分散风险，可以进一步提升收益的稳定性。</p><p>此文章内容由云梦量化科技高频策略实习生skylen创作投稿。</p>]]></description></item><item>    <title><![CDATA[怎么实现制造业数字化转型？实战案例解析 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047493984</link>    <guid>https://segmentfault.com/a/1190000047493984</guid>    <pubDate>2025-12-22 18:13:38</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在全球制造业加速向智能化、网络化演进的今天，工业数字化转型已不再是可选的优化路径，而是企业生存与发展的战略刚需。它超越了传统信息化升级的范畴，是一场涵盖研发、生产、供应链、设备管理与服务体系的全方位重构，其核心在于通过数据驱动、智能协同与平台赋能，打通“研、产、供、销、服”全链条，实现从经验决策向智能决策、从孤立系统向生态协同的根本跃迁。<br/>在这一转型浪潮中，广域铭岛作为中国工业互联网领域的先锋力量，凭借其自主研发的Geega工业互联网平台，构建了“平台+解决方案+工业软件”的完整生态体系，为制造业提供了可落地、可复制、可进化的数字化转型范式。其实践表明，真正的工业数字化转型，必须实现技术、流程与理念的三重突破。<br/>在研发端，广域铭岛的FastWorx设计研发协同平台，以BOM管理、三维工艺引擎与AI驱动的工艺专家系统为核心，打通了从概念设计到产品交付的全周期闭环。通过结构化知识管理与智能校核，企业零部件复用率提升35%，研发周期显著缩短，工程师从重复性劳动中解放，创造力得以释放。这标志着研发正从“人驱动”转向“数据+算法驱动”。<br/>在生产端，设备数字化协同与数字孪生技术成为提质增效的关键。广域铭岛在领克汽车成都工厂部署的焊接质量预警系统，整合3000+焊点参数，实现虚焊、飞溅等缺陷的实时识别与自动预警，将问题排查时间从72小时压缩至5分钟。其设备数字孪生模型可精准模拟反应釜、工业机器人等关键装备的运行状态，实现预测性维护，使设备综合效率（OEE）提升20%以上，运维成本大幅降低。<br/>在供应链与质量管理层面，广域铭岛的GOS-知识库系统融合多源异构数据采集、AI知识图谱与区块链技术，构建了端到端的智能协同网络。在百矿集团，系统通过实时优化氧化铝浓度，单吨铝电解能耗降低300千瓦时，年节电费超7000万元；在汽车产业链中，其GECP碳管理平台实现全生命周期碳足迹追踪，助力企业应对CBAM等绿色贸易壁垒。同时，智能计划助手将排产时间从6小时缩短至1小时，质量不良率从8%降至0.8%，真正实现了“质量可追溯、成本可预测、响应可敏捷”。<br/>当前，工业数字化转型正从单点技术应用迈向系统性生态构建。广域铭岛的实践印证：成功的转型不是工具的堆砌，而是以平台为中枢，打通数据孤岛、重构业务流程、激活组织潜能的系统工程。随着5G、AI大模型与量子计算等技术的深度融合，工业数字化将加速向“自感知、自决策、自优化”的智能体时代演进。<br/>未来，谁能率先完成这场从“制造”到“智造”的蜕变，谁就能在新一轮全球产业竞争中占据制高点。广域铭岛等领先服务商，正以技术为笔、数据为墨，为制造业绘制一幅智能、绿色、韧性的新蓝图——这不仅是技术的升级，更是中国制造业迈向高质量发展的必由之路。</p>]]></description></item><item>    <title><![CDATA[工业物联网IIOT如何重塑整车制造行业？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047494018</link>    <guid>https://segmentfault.com/a/1190000047494018</guid>    <pubDate>2025-12-22 18:12:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>导言：制造业数字化转型的新引擎<br/>当前全球制造业正经历着以工业物联网为代表的第四次工业革命浪潮。作为工业4.0的核心技术，IIoT通过将传感器、设备和系统相互连接，实现了制造过程的数字化、网络化和智能化。在技术密集型的汽车制造领域，IIoT正在从根本上改变传统生产模式，推动制造业向智能化、柔性化方向发展。从供应链管理到生产线优化，从质量管控到能效提升，IIoT技术正在汽车制造的全价值链中发挥着越来越重要的作用。<br/>值得注意的是，IIoT的应用已经超越了简单的设备联网和数据采集阶段，正在向智能决策和自主优化演进。通过大数据分析、人工智能和数字孪生等技术的深度融合，IIoT正在帮助汽车制造企业构建更加智能、高效的生产体系。这种转变不仅提升了制造效率，更重要的是重塑了汽车制造业的竞争格局。<br/>IIoT技术的核心价值与应用场景<br/>工业物联网在整车制造中的应用价值主要体现在三个层面：设备层、系统层和决策层。在设备层面，通过部署各类智能传感器和物联网终端，实现了生产设备运行状态的实时监测和预警。在系统层面，通过数据集成和交互，打破了传统制造系统中的信息孤岛，实现了生产过程的协同优化。在决策层面，基于大数据分析和人工智能算法，为企业管理层提供了更加科学、精准的决策支持。<br/>具体到应用场景，IIoT技术在质量控制方面的表现尤为突出。传统制造模式下，质量检测往往依赖于人工抽检，存在效率低、漏检率高的问题。而通过部署机器视觉检测系统和智能传感器，实现了对产品质量的全程监控和实时预警。例如在焊接工序中，智能监测系统可以实时采集焊接电流、电压等参数，通过算法分析及时发现问题，将质量缺陷消灭在萌芽状态。这种转变不仅提升了产品质量，更重要的是建立了可追溯的质量管理体系。<br/>典型案例：IIOT赋能汽车制造业创新实践<br/>在具体应用中，广域铭岛为某大型汽车制造企业部署了完整的数字孪生系统。通过构建虚拟工厂，实现了对物理工厂的实时映射和动态仿真。更值得一提的是，平台通过人工智能算法，实现了生产参数的自主优化，使整车生产效率提升了22%，同时能耗降低了15%。在质量管理方面，系统能够实时采集超过2000个质量参数，通过机器学习算法建立质量预测模型，将产品缺陷率降低了40%。广域铭岛通过构建供应链协同平台，实现了与300多家供应商的实时数据交互。平台通过智能算法预测零部件需求，优化库存管理，将库存周转率提升了35%，同时确保了生产物料的及时供应。<br/>吉利集团的数字化转型堪称典范。该集团在全球四大研发中心部署了IIoT平台，实现了从冲压、焊接、涂装到总装的全流程数字化监控。特别是在宁波工厂，他们通过引入数字孪生技术，构建了与物理工厂完全对应的虚拟镜像，能够提前发现并解决生产线切换过程中的各种问题。这一创新实践使新车型导入时间缩短了40%，同时将整车生产效率提升了25%。<br/>特斯拉则在智能制造的另一条路径上走得更深。其超级工厂采用了完全自动化的生产体系，通过机器视觉和AI算法实时监控焊接质量，实现了99.99%的缺陷检出率。同时，工厂的能源管理系统也通过IIoT技术实现了能耗的精细化控制，将每台汽车的能源消耗降低20%以上。这种极致的自动化和智能化，使特斯拉在激烈的市场竞争中保持了领先优势。<br/>未来展望：IIoT驱动制造业创新发展<br/>随着5G、人工智能、数字孪生等新技术的成熟应用，IIoT在汽车制造领域的应用将更加深入。未来，我们可以预见更加智能的自主决策系统，更加柔性化的生产模式，以及更加协同的制造生态。工业互联网平台如广域铭岛等，将继续推动制造业向网络化、智能化、服务化方向转型升级。<br/>然而，IIoT的深度应用仍面临诸多挑战，包括数据安全、系统集成、人才培养等方面。制造企业需要制定清晰的数字化转型战略，选择合适的技术合作伙伴，建立完善的数据治理体系。只有这样才能真正释放IIoT技术的价值，在数字化浪潮中赢得竞争优势。<br/>最终，IIoT技术的成功应用不在于技术的先进性，而在于能否为企业创造实际价值。制造企业应该以业务需求为导向，以价值创造为目标，循序渐进地推进数字化转型。通过IIoT技术的创新应用，汽车制造业必将迎来更加智能化、高效化的未来。</p>]]></description></item><item>    <title><![CDATA[[未解决]RuntimeError: CUDA environment is not correct]]></title>    <link>https://segmentfault.com/a/1190000047494023</link>    <guid>https://segmentfault.com/a/1190000047494023</guid>    <pubDate>2025-12-22 18:12:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在使用Win10重现代码的时候，其使用了Chainer库。然后按照requirements.txt安装了所有的依赖，但还是报错：</p><blockquote>RuntimeError: CUDA environment is not correctly set up</blockquote><p>然而我是已经装好CUDA和cuDNN的</p><p>仔细看了一下，下一行说的是：</p><blockquote>(see <a href="https://link.segmentfault.com/?enc=3NMk1keVMjpQq5Ib0wC1ww%3D%3D.T6C8C6JvoXubTosLNjjl1MsWAofd9dsjnNqJWfsiTIRDIWoeV%2FrCYyBWEH7mMl7N" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).No module named 'cupy'</blockquote><p>看样子是没有安装cupy这个库，按链接点了进去，在README找到了另一个界面<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>使用命令看一下CUDA的版本</p><blockquote>nvcc --version</blockquote><p>就直接命令行安装对应版本咯</p><blockquote>pip install cupy-cuda100</blockquote><p>然而，在我直接命令行安装最新版本的cupy之后，又报错了。我的版本是：</p><blockquote>cupy_cuda100-9.1.0</blockquote><p>报错提示：</p><blockquote>RuntimeError: CUDA environment is not correctly set up<br/>(see <a href="https://link.segmentfault.com/?enc=hc3tfXOx4Tm25Ft2b%2FmIMg%3D%3D.I9%2BZRAfLmtRK2tu2EGX%2BzgqvkPKGCNhFHYA7Qi%2FT1srLdHQrD8QJhhP8kF%2BfHgoo" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).CuPy is not correctly installed.</blockquote><p>我去上面看了看warning</p><blockquote>CuPy (cupy-cuda100) version 9.1.0 may not be compatible with this version of Chainer.<br/>Please consider installing the supported version by running:<br/>  $ pip install 'cupy-cuda100&gt;=7.7.0,&lt;8.0.0'</blockquote><p>啊，这意思是版本的问题咯？</p><p>那我直接指定一下版本</p><blockquote>pip install cupy-cuda100==7.7.0</blockquote><p>然后发现下载的巨慢，就找了篇博文参考<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></p><p>于是决定使用清华源<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>：</p><blockquote>pip install -i <a href="https://link.segmentfault.com/?enc=9pgCEICnPPVCYWIuTGrgrw%3D%3D.lj8IWDwSQ0maHZ613LHradK1Kh3878xIuLX7RiPw1fhBqpIRByYFHvRkOW9El0EI" rel="nofollow" target="_blank">https://pypi.tuna.tsinghua.edu.cn/simple</a> cupy-cuda100==7.7.0</blockquote><p>然而还是有问题。这个包大概280MB，下载到100+MB的时候就会报错，好像是HTTP ERROR ，说是超时还是什么的</p><p>命令行界面是有显示包的下载地址的，于是我直接CTRL+单击，在浏览器打开链接进行下载；或者将此链接丢到IDM什么的。</p><p>下载完whl文件后，命令行转到该文件夹，然后</p><blockquote>pip install filename.whl</blockquote><p>据说whl文件类似与压缩文件<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，所以直接pip install 加上完整的文件名就好</p><p>但问题还是没得到解决，后续的报错依然是：</p><blockquote>(see <a href="https://link.segmentfault.com/?enc=lEogseUKVKdF3krIHi10Sg%3D%3D.h84Qxfloes%2BLCxqmh8x8jjUQiWKGa3w2eX%2BnbNV9IuamJCL8ap6%2FtFjEV8WVfx6H" rel="nofollow" target="_blank">https://github.com/chainer/chainer#installation</a>).CuPy is not correctly installed.</blockquote><p>我又尝试了7.8.0的版本，还是不行</p><p>根据之前的warning：</p><blockquote>CuPy (cupy-cuda100) version 9.1.0 may not be compatible with this version of Chainer.<br/>Please consider installing the supported version by running:<br/>  $ pip install 'cupy-cuda100&gt;=7.7.0,&lt;8.0.0'</blockquote><p>符合要求的版本在7.7.0和8.0.0之间，而7.9.0版本是没有的。所以理论上只有7.7.0和7.8.0两个版本，而这两个版本都不行。</p><p>后来在chainer的官网<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>上看到一段话：</p><blockquote>We are automatically testing Chainer on all the recommended environments above. We cannot guarantee that Chainer works on other environments including Windows and macOS (especially with CUDA support), even if Chainer may seem to be running correctly.</blockquote><p>可能是系统为Win10的原因</p><p>不过在cpu模式下，这份代码运行良好，复现成功，chainer库没有报错。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=ngyLay%2F4E89dFlmsslMLgw%3D%3D.TRLMFVQ3EAlgqdtwto3iurIqZlyukm1Cs31K61sfZpITKdRha0TCjE8TEZN%2FnWiixmhltdP98uc3YQnNA6Es2w%3D%3D" rel="nofollow" target="_blank">Installation — CuPy 9.1.0 documentation</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=zyO1%2ByX9kYmYlXq2O0moOw%3D%3D.quYq5QYTOvmsQ4hSeGlL8TA8r%2BJPh3z%2BV%2FDQumRJTq60md2nnihTk7Ekoy%2FueTL0aF1P6TPdaXCvesvNJ9hE%2Fw%3D%3D" rel="nofollow" target="_blank">cupy-cuda安装下载报错以及速度太慢的问题</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=jrqWdxQ9S9%2FRgvSPUQ307g%3D%3D.hbbX18MNS%2FeV8OGHDlqHyRss4ROcs8VGBNTI3UFcsvU0%2FNI32DxZUySdx%2FxcNS1l" rel="nofollow" target="_blank">pypi 镜像使用帮助</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"><a href="https://link.segmentfault.com/?enc=Ti57zgPzpMezjFGOm1U4LA%3D%3D.x5eJiyqJCsJrrxS%2Fb3tMfulNnq77Cc3cY9Rt0%2FfUzXSVgO42R7KdHD%2B1OO3DC4xc33tb%2BhvbEGLfOrT4hJzYSQ%3D%3D" rel="nofollow" target="_blank">通过whl文件更新或安装python包</a>  <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"><a href="https://link.segmentfault.com/?enc=oH6L4cAP%2F%2FhZqkJjf9NWTg%3D%3D.mGl1cy7Cl8E6d%2B2NMn7gJUocfMINdUWbV%2FFi5L6XNr621jDazX0VSHT3Z%2FL7gWv4" rel="nofollow" target="_blank">Installation — Chainer 7.7.0 documentation</a>  <a href="#fnref-5" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[Windows PowerShell使用curl登录校园网 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494053</link>    <guid>https://segmentfault.com/a/1190000047494053</guid>    <pubDate>2025-12-22 18:11:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1.抓包分析登录过程</h2><p>这里尝试过两种方法：一是直接用新版edge的DevTools，即按F12键；另一种则是使用Wireshark软件。下面分别演示这两种方法。</p><h5>方法一：使用DevTools</h5><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw2" alt="image.png" title="image.png"/><br/>首先打开校园网的登录页面<br/><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrw3" alt="image.png" title="image.png" loading="lazy"/><br/>接着按F12进入DevTools，选择NetWork的选项卡，默认开始记录网络活动<br/><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw4" alt="image.png" title="image.png" loading="lazy"/><br/>然后返回登录页面，输入账号密码点击登录</p><p><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrw7" alt="image.png" title="image.png" loading="lazy"/><br/>此时去到DevTool的窗口，查看抓取的结果。抓取结果默认是按照时间顺序排列的，如果不是可以使用WaterFall的选项卡进行升序或者降序的排列<br/><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrxb" alt="image.png" title="image.png" loading="lazy"/><br/>在抓取的网络活动当中，发现有名为login.php的活动，猜测其为登录的关键步骤。上图是其中的某一个login.php的活动，其向目标网址发送了一个post请求，且携带的参数当中包含账号和密码。此活动应该是登录的关键步骤。</p><h5>方法二：使用WireShark</h5><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrw2" alt="image.png" title="image.png" loading="lazy"/><br/>同样打开校园网的登录页面<br/><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrxc" alt="image.png" title="image.png" loading="lazy"/><br/>启动WireShark，选择网络接口进行抓取。一般来说，抓取的网络接口都是直接对外的总连接的接口，即波动较大的网络。如果不清楚选择哪一个，也可以打开网络中心看一看对外连接的是哪一个网络接口。</p><p><img width="723" height="393" referrerpolicy="no-referrer" src="/img/bVdnrxd" alt="image.png" title="image.png" loading="lazy"/><br/>然后返回登录页面，输入账号密码点击登录</p><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnrxe" alt="image.png" title="image.png" loading="lazy"/></p><p>由于已知登录界面的主机的ip，所以在这里就直接输入了ip和协议进行过滤。如果不清楚，还是需要去F12的界面查看。关于过滤规则详见<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><h2>2.构造数据包模拟登录</h2><p>从F12和WireShark的抓取结果来看，登录过程主要是本地主机向登录页面发送了一个POST的请求，并携带账号密码进行验证。构造数据包的方法也有两个，一个是curl命令行构造，一个是python脚本构造</p><h5>方法一：使用curl命令</h5><p><img width="723" height="391" referrerpolicy="no-referrer" src="/img/bVdnrxf" alt="image.png" title="image.png" loading="lazy"/><br/>原本是参照某篇博文<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>进行构造命令，然后发现Windows下Powshell的命令格式有所不同，并不能使用这篇博文的方式。后续根据报错查找，另一篇博文<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>展示了改进方法。关于Powshell下curl命令的使用方法，这里有两篇博文也有很好地说明。<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>也可以参照该博文<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>使用curl进行登录。</p><h5>方法二：使用python脚本</h5><p><img width="723" height="392" referrerpolicy="no-referrer" src="/img/bVdnrxg" alt="image.png" title="image.png" loading="lazy"/></p><p>python代码如下：</p><pre><code class="python">import requests

login_url = ""

data = {
    "opr":"",
    "userName":"",
    "pwd":"",
    "ipv4or6":"",
    "rememberPwd":""
}

doc = requests.post(login_url,data).text
print(doc)</code></pre><p>以上代码参考了该博文<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup></p><h2>3.总结</h2><p>在抓包过程当中，DevTools的目标更明确，指定对某一个具体的页面的网络活动进行抓取，可以看到该页面的ip地址、端口等等；相比之下Wireshark的目标更宽泛，所有通过网络接口的数据均进行抓取，需要过滤才能看到目标页面的信息。不仅目标宽泛，WireShark抓取的数据也更多，过滤后，目标页面的TCP三次握手的过程也能够看到。</p><p>此次抓取的是登录过程，后续也抓取过退出过程，方法基本一致。退出过程的目标地址和登录过程的目标地址一样，只是携带的参数不同。</p><p>除此之外，也尝试过模拟登录搜狐邮箱，同样参考这篇博文<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，但并没有成功。分析是携带的参数不对，除了时间戳这个变量之外，每次登录的参数还受其他变量的影响。而且其使用的是加密的https的方式，还需要对WireShark进行配置才能解密数据包<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>。</p><p>还尝试过抓取B站的弹幕，参考的这两篇博文<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup><sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup>，但都没有成功。原因是并没有找到储存弹幕的xml文件，也就没有找到弹幕的服务器地址和获取弹幕的请求参数。不过有大佬做了API<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup>，可以直接使用API获取弹幕。除了用API之外，其实也可以用BeautifulSoup等python库，因为我发现弹幕是可以直接在网页的HTML文件里面找到的。直接爬取HTML文件，根据规则过滤，拿到弹幕也是可以的。但是在F12的界面，初始的HTML文件是没有弹幕的。猜测是有脚本从服务器获取了弹幕，然后按照某种规则填充了进去。如果可以从脚本开始分析，或许能够找到存储弹幕的服务器和请求弹幕的参数。</p><p>有博文<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup>指出了之前获取弹幕XML文件的两种方法依然适用：</p><blockquote>1、<a href="https://link.segmentfault.com/?enc=7FQI20iaFUTzcrSBGS714A%3D%3D.OWdIhRuTSTzgloQrXy2yR1o3vgG8TJqaWfEKBjmxDNM%3D" rel="nofollow" target="_blank">https://comment.bilibili.com/</a>视频的cid.xml<br/>2、<a href="https://link.segmentfault.com/?enc=J%2F%2FRaTATrm9BI0eHVH5DxQ%3D%3D.VNGNW16qKPxOAn0BWUiyHrWs8m7XosEjtbkGjk4Lte7jk2TqDRc%2FkjOa1%2BmxQpm0" rel="nofollow" target="_blank">https://api.bilibili.com/x/v1/dm/list.so?oid=</a>视频的cid</blockquote><p>获取cid的方法可以去上面的博文里面查看。</p><p>虽然以上的两种方法依然适用，但是我现在，仍旧没有在F12的页面当中找到XML的传输记录</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=zqr98zU9eXcB%2F6Tly4MHnw%3D%3D.V2ZF82muL0oNzX5zJ5eA7NhjZrbnBajZe6TRRy2LH0JQzKWXY9Wu0chuVQi2xHF51qdvlkF%2BNk3EnVMi44xy4g%3D%3D" rel="nofollow" target="_blank">Wireshark 常用过滤方法</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=33kj06YJ6YdVBT68RquKqg%3D%3D.q1a%2FaF0Ybhp4C4TI6tB9Nn9Iwafp03xv0X2Zqbfn0F4im6ThvnD4HfSEz%2Fj5n9gBKeR36fsBqTaSi9M4jxSoIQ%3D%3D" rel="nofollow" target="_blank">curl命令登录网页</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=0M2AGzL17pUK04v7eLZeqg%3D%3D.TJ18EMA4tdZeI2nldSt1S8xIMESyKSkA9NjjcrnF66BmIBOcyV8VSfAnbxReETMW" rel="nofollow" target="_blank">在 Windows PowerShell 中执行命令：curl -X POST –data，报错：Invoke-WebRequest : 找不到接受实际参数“POST”的位置形式参数。的分析解决</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=BfmNaug1scTkq6SJ94DRHQ%3D%3D.9bgKuF%2BHEORxt1M8yIhVcqi%2BNjbwMVw9wLwf73n3Ot04guUb83w%2BaWxpQY8xhADm" rel="nofollow" target="_blank">在PowerShell中使用curl(Invoke-WebRequest)</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=WKJkvVrlFlZUZk7VPQF8qg%3D%3D.jFX%2F3pSAaN8gMJutKfthrOpnMNNSM1e8clcL7ZaRZZwYRqLOxUzb9Sq9TW7Igt9pFzIjmzaTLACji8QkvstiUBzOOsD%2B7lICyLvGJ4UcTccaFLIM9ULzoLmmVfiRwOiR5EY%2BC3D1WrT5wMJlCg4nzKXhx0OI6QvN4mX7gQydPnqyL8VoICV4nx4exiEqK4m3" rel="nofollow" target="_blank">Invoke-WebRequest</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"><a href="https://link.segmentfault.com/?enc=35LXEX%2BTlttP%2FLpfpcpSEA%3D%3D.Z0rQjRhVnSKVsJBPn4fa8AA9Gq%2BttnJmWQ0CcEEm6Ss6APQQ5mWXaM9SJoCnT3UOoc93v9mcfY%2FBlcGlFSy0Jg%3D%3D" rel="nofollow" target="_blank">curl命令实现上网认证登录</a>  <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"><a href="https://link.segmentfault.com/?enc=vQ7iOnStfgdMpZbbOkY4fw%3D%3D.73CcbDwGa%2FUVJtiDNU8jnXk44yvKrBQoZRKTYdUhpUSVdr9PAXGaMahwAdrl9dzNzUKN5AcYGUpJ26flVisO7g%3D%3D" rel="nofollow" target="_blank">搜狐网模拟登录案例</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"><a href="https://link.segmentfault.com/?enc=kSAox2OwjcfFQp0iTE1hzA%3D%3D.HzRe3dwvT19su4JdC5S4o3%2FC7Ahs68aX038z3Rc87EKia3OyXO8EPaTB18UexEGd78KmdmKLIRWWTSgKhoJY%2FA%3D%3D" rel="nofollow" target="_blank">配置Wireshark抓取https数据包</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"><a href="https://link.segmentfault.com/?enc=iIiJFH5YeIIVtL3NGd4liQ%3D%3D.FK17eytEck7XYw5iIB%2BsCnK77kGpkSxOFCyfmbGaHR2%2BV7DlxViLkYnCmgdO0NxUX2VmoTTYdLjivA2T6gt1Og%3D%3D" rel="nofollow" target="_blank">python爬虫----b站的弹幕获取</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"><a href="https://link.segmentfault.com/?enc=w08FUYErLIpz4eKyfKfBlQ%3D%3D.SHkDCYVUNRQiRxy8qKEOSM3Zm9XB33wgTiXXg8hax%2BkS6%2BHk9i8SFJ45Vv0zhQ0J" rel="nofollow" target="_blank">Python爬虫爬取Bilibili弹幕过程解析</a> <a href="#fnref-10" class="footnote-backref">↩</a></li><li id="fn-11"> <a href="https://link.segmentfault.com/?enc=i9c47e40WZUk3uaNkjkpOg%3D%3D.pO6J7MByht4Iy%2F2%2BOaTx%2FrW4SrimV66CwzkZeki8rOdbH3%2BxmzyMml25Sg3sCNdODWX5TBxf003qYHsW%2BhKCRw%3D%3D" rel="nofollow" target="_blank">bilibili_api，仅用 3 行代码获取B站（弹幕、评论、用户）数据</a> <a href="#fnref-11" class="footnote-backref">↩</a></li><li id="fn-12"> <a href="https://link.segmentfault.com/?enc=mpoUqRIU6dHiP4%2B%2FemSPVQ%3D%3D.Wz8IQQQ8eNgXPo9HCU0j2BX3lG8zIPDeNXG1KY1FJnEuHKqs%2BEDA2AgtoatZ3bCX1VevBD06rKVcf6grN7WPBQ%3D%3D" rel="nofollow" target="_blank">感谢大佬，B站弹幕也能抓取啦~</a> <a href="#fnref-12" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[存疑]Spyder修改新建py文件的模板 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494065</link>    <guid>https://segmentfault.com/a/1190000047494065</guid>    <pubDate>2025-12-22 18:10:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我用的是Anaconda，因此spyder安装在Anaconda的路径下面，参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>寻找文件</p><blockquote>"D:\Software\Anaconda3\Lib\site-packages\spyder\plugins\editor\plugin.py"</blockquote><p>在Spyder的安装文件夹当中，找到plugins文件夹，再去往editor文件夹，修改其中的plugin.py</p><p>打开plugin.py之后，在文件当中搜索<kbd>date</kbd>，可以找到这么一段代码</p><pre><code class="python">VARS = {
'date': time.ctime(),
'username': username,
}</code></pre><p>将<kbd>'date'</kbd>后面的<kbd>time.ctime()</kbd>改成<kbd>time.strftime("%Y-%m-%d %H:%M:%S")</kbd>，即可改变新建py文件中头部注释的时间格式</p><p>当然这个或许可以尝试着进一步说明一下</p><p>在Spyder当中找到用于新建py文件的模板<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></p><blockquote>tools-&gt;preferences-&gt;editor-&gt;advanced settings-&gt;Edit template for new modules</blockquote><p>模板当中有一个<kbd>%(date)s</kbd>，猜测此处的<kbd>date</kbd>与上文plugin.py文件当中字典变量<kbd>VARS</kbd>的<kbd>date</kbd>应该是同一个东西，即模板的<kbd>date</kbd>引用了plugin.py文件的<kbd>VARS:date</kbd>，那我们就可以尝试更多的操作了。</p><p>我之前曾经在plugin.py的<kbd>VARS</kbd>当中加了一个<kbd>folderpath</kbd>变量，然后使用函数获取将当前的工作目录，赋值给这个变量。之后在temple.py文件当中使用<kbd># python -u "%(folderpath)s\"</kbd>来引用这个变量。当时是成功了的，但是不记得那个Spyder的版本号了。</p><p>本文标题当中有<kbd>[存疑]</kbd>，也就是，我这里现在出问题了，没有正常运行。之前用Spyder的时候，做了这个修改，大概都是一年前的事情了，当时并没有写博文把过程记录下来，只是收藏了这一篇关键的博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>。关于Spyder，早就不用了，后续也没再升级版本。今天整理浏览器收藏夹，看到了这篇博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>，于是就按照回忆重新做了一遍流程，但很尴尬的是，第一步就出问题了。</p><p>问题出在<kbd>%(date)s</kbd>上，似乎其并不能成功引用plugin.py文件的<kbd>VARS:date</kbd>，导致新建文件当中显示的是<kbd>%(date)s</kbd>的原文，并不是我们所期望的当前时间。而且我之前明明已经修改过plugin.py文件，在其中加入了<kbd>VARS:folderpath</kbd>变量，现在也没了。</p><p>猜测原因可能有以下两种：</p><ul><li>有可能是在修改plugin.py文件之后，Spyder进行了升级，因此我修改过的那个文件被新文件所覆盖。但这并不能解释为何<kbd>%(date)s</kbd>失效</li><li>或者就是我找错了地方，并不在 <kbd>"D:\Software\Anaconda3\Lib\site-packages\spyder\plugins\editor\plugin.py"</kbd>这个路径当中，因为这个路径本是我如今摸索出来的，并不一定是我之前修改成功的那个路径。我是Anaconda版本的Spyder，所以博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>当中的路径并不能直接使用，是按照其中像<kbd>plugins</kbd>、<kbd>editor</kbd>这样的关键字摸索的。就算我之前修改成功了，那也是摸索的路径。而且当时还没有对路径进行记录，导致如今又摸索了一遍路径。但不管怎么说，还是有那个问题，这并不能解释为何<kbd>%(date)s</kbd>失效。</li></ul><p>因为<kbd>%(date)s</kbd>是Spyder默认的东西，不管是升级抑或是找错文件，temple.py所引用的<kbd>date</kbd>都应该不会出什么问题。</p><p>猜测有可能是版本的问题，因为我打开尘封已久的Spyder不久，就弹窗提示更新，然而被我拒掉了。由于相当长一段时间内应该都不会再用Spyder，所以本文的探索到此为止了。希望还在用的小伙伴多多探索，然后发博文分享方法。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=J5YjTd1w8hH1c7TFdlRzBg%3D%3D.po46lP6IQNDx9%2FWhxvr8mBIa3yStzs0HzcyxU01DxL694QTshNMMCpkI609uXZpu0OYg9wUSwBXk5pd4JiQ0AA%3D%3D" rel="nofollow" target="_blank">Winpython Spyder template.py模板日期格式的修改</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=xgekzlYbGI4cFNl1jPIZxg%3D%3D.zNpClaaVgArW%2FcWJWUjMLiU%2F%2BHKu1%2Byh2NxrYtaB3oR1I7Ur5gyuxrXK1tvHbgx8ecVw2brWAiCd69z5G8MZyg%3D%3D" rel="nofollow" target="_blank">Spyder python文件抬头默认内容自定义</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[pandas计算某列每行带有分隔符的数据中包含特定值的次数 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494073</link>    <guid>https://segmentfault.com/a/1190000047494073</guid>    <pubDate>2025-12-22 18:09:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>某次做一个数据的处理，要计算用户的粉丝数量，数据集大概是这样的：</p><table><thead><tr><th>传播节点微博用户id</th><th>关注用户ids</th></tr></thead><tbody><tr><td>ae26e5e3db7626dcaf6819ce5492d534</td><td>"04e9dc04d4b600d574d67b298a7dea7d,···"</td></tr><tr><td>a845733e3729a136889c07d275bcc3c5</td><td>"aebe49645667a02eae6ab6734ade24eb,···"</td></tr><tr><td>68e605feb5344fd413587b4245946c24"</td><td>77c471d3aba195b1322800602a93dc72,···"</td></tr></tbody></table><p>这里的数据，都是经过脱敏处理后的id，即每个用户和他们的关注列表。“关注用户ids”应该是字符串类型，每一行由双引号包裹，由逗号作为id之间的分隔符。要计算用户的粉丝数量，就是看他们在所有用户的关注列表当中出现了多少次，也就是要对“关注用户ids”列出现的各个id进行计数。</p><p>参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>进行以下处理</p><pre><code class="python">countN = dataI0['name'].str.split('|', expand=True).stack().value_counts()
# 计算各元素出现的次数</code></pre><p>其中，不能对数据框的列<kbd>Chart3Part['关注用户ids']</kbd>直接应用<kbd>split</kbd>，而需要先调用<kbd>str</kbd>。</p><ul><li>其中的<kbd>expand=True</kbd>是按逗号对每一行进行分割后，将其扩展成多列。</li><li><kbd>stack()</kbd>则是构造二级行索引，在原本的行索引上，将列作为二级行索引。可参考博文<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，也可见下文当中的实验三</li><li><kbd>value_counts()</kbd>是对值出现的次数进行计数，其返回值是一个pd.Series，name为被计数的列的名字，index为被计数的项，值为出现的次数</li></ul><p>博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>当中还有另一种方法，先单独对每一行进行处理，再从总的视角进行计数</p><pre><code class="python">countN = pd.Series(Counter([y for x in dataI0['name'] for y in x.split('|')]))</code></pre><p>完整的示例如下：</p><pre><code class="python">import pandas as pd

dataI0 = pd.DataFrame(data=["book","fish","icecream|book","fish","campfire|book"],columns=["name"])
print(dataI0)
# 创建数据框

#             name
# 0           book
# 1           fish
# 2  icecream|book
# 3           fish
# 4  campfire|book

# 方法一----------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True).stack().value_counts()
print(countN)
# 对name列进行计数

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64

# 方法二---------------------------------------------------------------------------------------
from collections import Counter

countN = pd.Series(Counter([y for x in dataI0['name'] for y in x.split('|')]))
print(countN)

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64

# 实验一---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|')
print(countN)
# 此处是对每一行的数据按照'|'进行分割，每一行返回的是分割后的数组

# 0              [book]
# 1              [fish]
# 2    [icecream, book]
# 3              [fish]
# 4    [campfire, book]
# Name: name, dtype: object
        
# 实验二---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True)
print(countN)
# expand=True，即分割后扩展为多列，不足之处使用None补充

#           0     1
# 0      book  None
# 1      fish  None
# 2  icecream  book
# 3      fish  None
# 4  campfire  book

# 实验三---------------------------------------------------------------------------------------
countN = dataI0['name'].str.split('|', expand=True).stack()
print(countN)
# stack()，即将列变为二级行索引，原行索引为一级行索引，也就是说现在的数据框只有一列了
# 所有的数据都在这一列，直接对这一列进行计数即可

# 0  0        book
# 1  0        fish
# 2  0    icecream
#    1        book
# 3  0        fish
# 4  0    campfire
#    1        book
# dtype: object

# 实验四---------------------------------------------------------------------------------------
countN = [y for x in dataI0['name'] for y in x.split('|')]
print(countN)
# 这里的思路是利用列表推导式，对每一行按照'|'进行分割后，对分割的结果进行枚举，最后得到一个列表
# 所有的结果都在列表当中

# ['book', 'fish', 'icecream', 'book', 'fish', 'campfire', 'book']

# 实验五---------------------------------------------------------------------------------------
countN = Counter([y for x in dataI0['name'] for y in x.split('|')])
print(countN)
# 博文的思路是对列表应用Counter，直接计数
# 之后再转换成为pd.Series

# Counter({'book': 3, 'fish': 2, 'icecream': 1, 'campfire': 1})

# 实验六---------------------------------------------------------------------------------------
countN = pd.Series([y for x in dataI0['name'] for y in x.split('|')])
print(countN)
# 也可以直接转换成pd.Series，所有的数据都在这一列了
# 效果与上文stack()后的结果相似

# 0        book
# 1        fish
# 2    icecream
# 3        book
# 4        fish
# 5    campfire
# 6        book
# dtype: object
    
# 实验七---------------------------------------------------------------------------------------
countN = pd.Series([y for x in dataI0['name'] for y in x.split('|')]).value_counts()
print(countN)
# 在pd.Series的基础上，也可以直接使用value_counts()

# book        3
# fish        2
# icecream    1
# campfire    1
# dtype: int64</code></pre><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=Yr9ZonvfNzdsxs09VIXtdQ%3D%3D.l%2Be60DrUt0yWjV5Z9C4FYCUGN%2ByuL%2FuSQKAiafr2RC66qzWa0GTEI6Pq16kZqeJI" rel="nofollow" target="_blank">python – 计算列在Pandas中包含特定值的次数</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"><a href="https://link.segmentfault.com/?enc=7M7NU1JeOKzdKQcN%2Bn5LUw%3D%3D.AgYXvQShWwbpEd7vrs4xzb3zVlRuK6bkuCjFlst4CVcENMCc9nmu4Iuie60xrSVC" rel="nofollow" target="_blank">Pandas 数据堆叠 stack</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[eBPF + OpenTelemetry：适用于任何应用的零代码自动化仪表盘 俞凡 ]]></title>    <link>https://segmentfault.com/a/1190000047494078</link>    <guid>https://segmentfault.com/a/1190000047494078</guid>    <pubDate>2025-12-22 18:08:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p><em>本文介绍如何将 eBPF 与 OpenTelemetry 结合，实现自动化、零代码的分布式追踪和可观测性系统，了解 Odigos、Beyla 和 OpenTelemetry eBPF 等工具的工作原理、适用场景以及如何在生产环境中设置。原文：<a href="https://link.segmentfault.com/?enc=wa7uSRiZ%2BZFlD2ptXWW6Fg%3D%3D.MY3Q7Dlojto%2BTLXgsdC9PeasoRGb4EY%2F51zY3%2FnU8fwwycGmmxYgW%2Fj3m%2FqAiIDjKdHG5uB2iewyJwsaeQTBKg2fVXa6wDJbd4Mb3NDYNs8%2BIGPZf1UTkSds8KaWy1Uh" rel="nofollow" title="Using eBPF with OpenTelemetry: Zero-Code Auto-Instrumentation for Any Application" target="_blank">Using eBPF with OpenTelemetry: Zero-Code Auto-Instrumentation for Any Application</a></em></p><p>如果能在所有服务间实现完整的分布式追踪，<strong>而无需添加一行仪表盘代码</strong>，怎么样？</p></blockquote><p>传统 OpenTelemetry 仪表盘需要添加 SDK、配置导出器，并用 span 包装代码。虽然功能强大，但需要付出不少努力，尤其是系统中有数十种不同语言的服务时。</p><p><a href="https://link.segmentfault.com/?enc=eLVzoQTw8WH%2B6NVgKFLBpw%3D%3D.dQyKP%2FE8qZ%2FKlZJA%2BnGPdzlGmr639psCpZ0yDN%2BmHN1K6faQuLV6SYnD8NszTL6HCFdJq%2Bv5yjFoTbXgxnSltiFDgADGaghE2RvL5mm5ecoLHDN1XWN5%2FVmdfXXn5cLA" rel="nofollow" title="eBPF" target="_blank">eBPF</a> 完全改变了这一模式。通过从 Linux 内核观察应用，基于 eBPF 的工具可以自动生成兼容 OpenTelemetry 的追踪、指标和配置文件，而无需触及应用代码 。</p><p>本文将介绍如何将 eBPF 与 OpenTelemetry 结合，实现强大的零代码可观测性。</p><hr/><h2>1. 问题：大规模仪表盘</h2><p>传统 OpenTelemetry 仪表盘遵循以下模式：</p><ol><li>为每个服务添加 OTel SDK</li><li>配置导出器</li><li>测量入口点（HTTP 处理程序，gRPC 方法）</li><li>为重要操作添加 span</li><li>跨服务边界传播上下文</li><li>每个服务、每种语言都要重复</li></ol><p>对于只有少量服务的小团队来说，还算可以管理。但请考虑：</p><p>|场景|挑战|<br/>|-|-|<br/>| 50+ 微服务|需要数周时间集成跨所有服务的 SDK|<br/>|多语言技术栈|Go、Python、Node.js、Java、Rust 等不同的 SDK……|<br/>|遗留服务|代码不容易修改，没人愿意碰|<br/>|第三方服务|无法访问源代码|<br/>|快速部署|新服务出现的速度比测量它们的速度还快|</p><p>结果呢？可观测性缺口。有些服务有追踪，有些没有。未安装测量的服务中断上下文传播。系统一定程度上正在裸跑。</p><hr/><h2>2. eBPF 如何实现自动化测量</h2><p>eBPF 通过从应用外部 —— 内核层面观察应用来解决这个问题。</p><h5>eBPF 能看到什么</h5><p>由于 eBPF 会钩入内核函数和系统调用，可以观察到：</p><p>|层|eBPF 看见|<br/>|-|-|<br/>|网络|每一次 TCP 连接、HTTP 请求/响应、DNS 查询|<br/>|系统调用|文件 I/O，进程创建，内存分配|<br/>|用户功能|函数通过 uprobe 进入/退出（如果有符号）|<br/>|语言运行时|Go、Node.js、Python、Java 运行时内部结构|</p><h5>这些如何成为 OpenTelemetry 数据</h5><p>基于 eBPF 的自动化测量工作原理：</p><ol><li>将<strong>探针附加</strong>到已知入口点（HTTP 库、gRPC 处理器、数据库驱动程序）</li><li>从请求中<strong>提取上下文</strong>（从头部提取追踪 ID、请求元数据）</li><li>利用内核时间戳<strong>测量时序</strong></li><li><strong>关联</strong>相关请求/响应对</li><li><strong>导出</strong>为标准 OpenTelemetry Protocol（OTLP）数据</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494080" alt="" title=""/></p><h2>3. eBPF + OpenTelemetry 架构</h2><p>典型生产配置如下：</p><h5>组件</h5><p>|组件| 职责|<br/>|-|-|<br/>|eBPF 代理|运行在每个节点上，连接 eBPF 程序，生成遥测数据|<br/>|OTel 收集器|接收、处理 OTLP 数据，并导出到后端|<br/>|后端|存储和可视化追踪/指标（OneUptime、Jaeger、Tempo）|</p><h5>部署模式</h5><h6>模式 1：DaemonSet（Kubernetes）</h6><pre><code class="yaml"># 运行在每个 node 上的 eBPF 代理
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ebpf-auto-instrumenter
spec:
  selector:
    matchLabels:
      app: ebpf-agent
  template:
    spec:
      hostPID: true      # eBPF 需要
      hostNetwork: true  # 网络追踪需要
      containers:
      - name: agent
        securityContext:
          privileged: true  # eBPF 需要</code></pre><h6>模式 2：Sidecar（每个 Pod 一个）</h6><pre><code class="yaml"># eBPF 代理作为 sidecar (更为隔离，更多开销)
spec:
  containers:
  - name: my-app
    image: my-app:latest
  - name: ebpf-sidecar
    image: ebpf-agent:latest
    securityContext:
      privileged: true</code></pre><h6>模式 3：独立（非 Kubernetes）</h6><pre><code># 直接在主机上运行
sudo ./beyla --config config.yaml</code></pre><h2>4. 工具比较：Odigos vs Beyla vs Pixie</h2><p>下面比较几种将 eBPF 与 OpenTelemetry 结合起来的工具。</p><h5>Grafana Beyla</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|HTTP/gRPC 自动监测|<br/>|语言|Go, Python, Node.js, Java, Rust, .NET, Ruby|<br/>|输出|OTLP (追踪 + 指标)|<br/>|部署|独立二进制或 Kubernetes|<br/>|许可证|Apache 2.0|<br/>|最佳实践|简单部署，Grafana 技术栈用户|</p><h5>Odigos</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|带上下文传播的全分布式追踪|<br/>|语言|Go, Python, Node.js, Java, .NET|<br/>|输出|OTLP（追踪）|<br/>|部署|Kubernetes 原生(operator)|<br/>|许可证|Apache 2.0|<br/>|最佳实践|Kubernetes 环境，分布式追踪|</p><h5>Pixie</h5><p>|特色|详情|<br/>|-|-|<br/>|重点|全栈可观测性，带集群内存储|<br/>|语言|Go, C/C++, Python, Node.js, Java, Rust|<br/>|输出|Pixie 格式（可导出为 OTel）|<br/>|部署|仅限 Kubernetes|<br/>|许可证|Apache 2.0|<br/>|最佳实践|调试、临时查询、全面可视化|</p><h5>快速决策指南</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494081" alt="" title="" loading="lazy"/></p><hr/><h2>5. 设置 Beyla（Grafana 的 eBPF 自动化测量）</h2><p>Beyla 是入门基于 eBPF 的 OpenTelemetry 测量的最简单方式。</p><h5>前置条件</h5><ul><li>Linux 内核 5.8+（支持 BTF）</li><li>Root/privileged 访问</li><li>目标应用程序正在运行</li></ul><h5>安装</h5><pre><code class="bash"># 下载最新版本
curl -LO https://github.com/grafana/beyla/releases/latest/download/beyla-linux-amd64.tar.gz
tar xzf beyla-linux-amd64.tar.gz
sudo mv beyla /usr/local/bin/</code></pre><h5>配置</h5><p>创建 <code>beyla-config.yaml</code>：</p><pre><code class="yaml"># beyla-config.yaml
open_port: 8080  # 测量进程监听端口

# 或目标的可执行名称
# executable_name: "my-service"

# 或进程 ID
# pid: 12345

# OTLP 导出配置
otel_traces_export:
  endpoint: http://localhost:4317  # OTel 收集器

otel_metrics_export:
  endpoint: http://localhost:4317
  
# 可选: 添加资源参数
attributes:
  kubernetes:
    enable: true  # 自动检测 K8s 元数据
  
# 采样 (可选)
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.1"  # 10% 采样</code></pre><h5>运行 Beyla</h5><pre><code class="bash"># 通过配置文件执行
sudo beyla --config beyla-config.yaml

# 或者通过环境变量
sudo BEYLA_OPEN_PORT=8080 \
     OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317 \
     beyla</code></pre><h5>Kubernetes 部署</h5><pre><code class="yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: beyla
  namespace: observability
spec:
  selector:
    matchLabels:
      app: beyla
  template:
    metadata:
      labels:
        app: beyla
    spec:
      hostPID: true
      serviceAccountName: beyla
      containers:
      - name: beyla
        image: grafana/beyla:latest
        securityContext:
          privileged: true
          runAsUser: 0
        env:
        - name: BEYLA_OPEN_PORT
          value: "8080,3000,9090"  # 测量端口
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "http://otel-collector.observability:4317"
        - name: BEYLA_KUBE_METADATA_ENABLE
          value: "true"
        volumeMounts:
        - name: sys-kernel
          mountPath: /sys/kernel
          readOnly: true
      volumes:
      - name: sys-kernel
        hostPath:
          path: /sys/kernel</code></pre><h5>Beyla 采样数据</h5><p>运行后，Beyla 会自动生成：</p><p><strong>追踪</strong>：</p><ul><li>HTTP 服务器 span（方法、路径、状态、时长）</li><li>HTTP 客户端 span（外出请求）</li><li>gRPC span（方法，状态）</li><li>SQL 查询 span（如果使用支持的驱动）</li></ul><p><strong>指标</strong>：</p><ul><li><code>http.server.request.duration</code>（直方图）</li><li><code>http.server.request.body.size</code></li><li><code>http.client.request.duration</code></li><li><code>rpc.server.duration</code></li><li><code>rpc.client.duration</code></li></ul><hr/><h2>6. 为 Kubernetes 设置 Odigos</h2><p>Odigos 提供了更全面的分布式追踪，并实现了自动上下文传播。</p><h5>安装</h5><pre><code class="bash"># 安装 Odigos CLI
brew install odigos-io/homebrew-odigos-cli/odigos

# 或者直接下载
curl -LO https://github.com/odigos-io/odigos/releases/latest/download/odigos-cli-linux-amd64
chmod +x odigos-cli-linux-amd64
sudo mv odigos-cli-linux-amd64 /usr/local/bin/odigos</code></pre><h5>部署到 Kubernetes</h5><pre><code class="bash"># 在集群里安装 Odigos
odigos install

# 创建:
# - odigos-system namespace
# - Odigos operator
# - Instrumentor DaemonSet
# - OTel Collector (可选)</code></pre><h5>配置目的地</h5><pre><code class="bash"># 添加可观测性后端
odigos ui

# 或通过 CLI
odigos destination add oneuptime \
  --endpoint https://otlp.oneuptime.com \
  --api-key YOUR_API_KEY</code></pre><h5>测量命名空间</h5><pre><code class="bash"># 测量命名空间中的所有工作负载
odigos instrument namespace my-app-namespace

# 或指定工作负载
odigos instrument deployment my-service -n my-namespace</code></pre><h5>Odigos 运作方式</h5><p>Odigos 比简单的 eBPF 追踪更智能：</p><ol><li><strong>语言检测</strong>：自动检测运行时（Go、Java、Python 等）</li><li><strong>合适的测量方式</strong>：Go 使用 eBPF，Java/Python 注入代理</li><li><strong>上下文传播</strong>：确保跨越服务边界追踪上下文</li><li><strong>无代码更改</strong>：所有注入均发生在运行时</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494082" alt="" title="" loading="lazy"/></p><hr/><h2>7. 自动获取的内容</h2><p>以下是基于 eBPF 工具能自动获取或者不能获取的内容：</p><h5>自动获取</h5><p>|信号|详情|<br/>|-|-|<br/>|HTTP 服务器请求|方法、路径、状态码、时长、消息头|<br/>|HTTP 客户端请求|发送请求，含目的地和时间|<br/>|gRPC 调用|方法、状态、时长（包括服务器和客户端）|<br/>|数据库查询|查询文本、时长、数据库类型（因工具而异）|<br/>|DNS 查询|域、查询时间、结果|<br/>|TCP 连接|源、目的、传输字节数|<br/>|TLS 握手|证书信息，握手时间|</p><h5>部分获取（因工具/语言而异）</h5><p>|信号|局限性|<br/>|-|-|<br/>|消息队列|Kafka/RabbitMQ 的支持各不相同，可能需要手动设置|<br/>|自定义协议|需要特定工具的支持|<br/>|内部函数调用|只有在符号信息可用的情况下|<br/>|业务逻辑上下文|无法推断用户 ID、订单 ID 等信息|</p><h5>无法获取（需要手动测量）</h5><p>|信号|为什么|<br/>|-|-|<br/>|自定义 span 属性|eBPF 不知道业务域|<br/>|应用错误|异常详情，stack trace（部分）|<br/>|自定义指标|业务关键绩效指标（KPI），转化率|<br/>|Baggage/Context|自定义传播数据|</p><hr/><h2>8. 将 eBPF 数据与手动测量进行关联</h2><p>最佳方法通常是混合式：基础覆盖用 eBPF，重要细节用手动测量。</p><h5>策略：分层测量</h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494083" alt="" title="" loading="lazy"/></p><h6>示例：混合配置</h6><pre><code class="golang">// Go 服务 - eBPF 自动捕获 HTTP 处理
// 为重要业务逻辑添加手动 span

func (s *OrderService) CreateOrder(ctx context.Context, req *OrderRequest) (*Order, error) {
    // eBPF已经获取：HTTP POST /orders、计时、状态

    // 业务逻辑细节的手动 span
    ctx, span := tracer.Start(ctx, "order.validate")
    err := s.validateOrder(ctx, req)
    span.End()
    if err != nil {
        // 手动：添加 eBPF 看不到的错误细节
        span.RecordError(err)
        span.SetStatus(codes.Error, "validation failed")
        return nil, err
    }
    
    // eBPF 自动捕获数据库调用
    // 手动：添加业务上下文
    ctx, span = tracer.Start(ctx, "order.save")
    span.SetAttributes(
        attribute.String("order.customer_id", req.CustomerID),
        attribute.Float64("order.total", req.Total),
        attribute.Int("order.items_count", len(req.Items)),
    )
    order, err := s.repo.Save(ctx, req)
    span.End()
    
    return order, err
}</code></pre><h6>确保相关性有效</h6><p>为了让 eBPF span 和手动 span 出现在同一条追踪中：</p><ol><li><strong>相同的 Trace ID</strong>：eBPF 工具从输入请求中提取 <code>traceparent</code></li><li><strong>上下文传播</strong>：手动 span 必须使用相同的上下文</li><li><strong>一致导出</strong>：eBPF 和手动测量都导出到同一个收集器</li></ol><pre><code class="yaml"># OTel Collector 配置合并两个源
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    
  # 添加一致的资源属性
  resource:
    attributes:
      - key: deployment.environment
        value: production
        action: upsert

exporters:
  otlp:
    endpoint: https://oneuptime.com/otlp
    headers:
      x-oneuptime-token: ${ONEUPTIME_TOKEN}

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [otlp]</code></pre><hr/><h2>9. 性能开销</h2><p>关键问题： 运行基于 eBPF 的自动化测量的成本是多少？</p><h5>测量额外开销</h5><p>|工具|CPU 开销|内存|时延影响|<br/>|-|-|-|-|<br/>|Beyla|1-3%|~50-100MB|&lt; 1ms|<br/>|Odigos|2-5%|~100-200MB|&lt; 2ms|<br/>|Pixie|2-5%|~500MB-1GB|&lt; 1ms|</p><p><em>注意：实际开销因工作负载、采样率和追踪端点数量而异。</em></p><h5>增加额外开销的因素</h5><p>| 因素| 影响| 缓解措施|<br/>|-|-|-|<br/>| 高请求量|更多 eBPF 事件待处理| 增加采样|<br/>| 追踪太多端点| 连接太多探针| 要有选择性|<br/>| 全载荷捕获| 用于复制数据的内存/CPU|禁用或限制|<br/>| 低采样率| 更多数据需导出| 使用头部采样|</p><h5>降低开销</h5><pre><code class="yaml"># Beyla 示例: 通过采样降低开销
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.01"  # 1% 采样

# 隔离高数据量、低价值的端点
routes:
  ignored:
    - /health
    - /ready
    - /metrics</code></pre><hr/><h2>10. 局限性及何时使用手动测量</h2><p>eBPF 自动测量功能强大，但并非魔法，需要知道取舍。</p><p><strong>在以下情况下使用 eBPF 自动化测量</strong>：</p><p>✅ 需要在多个服务中快速实现基线可观测性</p><p>✅ 不能修改应用代码（遗留版本，第三方代码）</p><p>✅ 需要一致的 HTTP/gRPC/数据库追踪，而不是每个服务单独设置</p><p>✅ 需要网络层面的可视化（连接、DNS）</p><p>✅ 身处混合语言的 Kubernetes 环境中</p><p><strong>需要使用手动测量</strong>：</p><p>✅ 需要自定义业务属性（用户 ID、订单 ID、功能标志）。</p><p>✅ 需要详细的错误信息和 stack traces</p><p>✅ 需要自定义指标（业务关键绩效指标、特定事件的计数器）</p><p>✅ 追踪没有 eBPF 支持的非 HTTP 协议</p><p>✅ 需要跨服务上下文的 baggage 传播</p><p>✅ 要控制 span 名称和结构</p><h5>eBPF 自动化测量的局限性</h5><p>|限制|详情|<br/>|-|-|<br/>|仅限 Linux|不支持没有 Linux 内核的 Windows、macOS 或容器运行时|<br/>|内核版本|需要 5.x 以上才能获得最佳效果，部分功能需要 5.8 以上|<br/>|特权访问|必须提升权限运行（安全考虑）|<br/>|符号可用性|剥离符号的 Go 二进制可执行文件会降低可见度|<br/>|加密流量|TLS 检查需要额外设置|<br/>|应用上下文|无法从网络数据推断业务含义|</p><hr/><h2>11. 最佳生产实践</h2><h5>安全考量</h5><p>运行 eBPF 代理需要提升权限。降低风险：</p><pre><code class="yaml"># Kubernetes: 严格使用 RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ebpf-agent-role
rules:
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]
# 避免授予不必要的权限</code></pre><pre><code class="yaml"># 尽可能使用 seccomp 配置文件
securityContext:
  seccompProfile:
    type: RuntimeDefault</code></pre><h5>限制资源</h5><pre><code class="yaml">containers:
- name: ebpf-agent
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi</code></pre><h5>过滤与抽样</h5><pre><code class="yaml"># 不要追踪每件事 —— 专注于重要的事情
routes:
  patterns:
    - /api/*        # Trace API calls
    - /graphql      # Trace GraphQL
  ignored:
    - /health       # Skip health checks
    - /metrics      # Skip metrics endpoint
    - /favicon.ico  # Skip static assets

# 通过采样控制数据量
traces:
  sampler:
    name: parentbased_traceidratio
    arg: "0.1"  # 10% in production</code></pre><h5>逐步推广</h5><pre><code class="bash"># 从非生产环境开始
odigos instrument namespace staging

# 验证开销和数据质量
# 然后扩展到生产环境
odigos instrument namespace production</code></pre><h5>监控</h5><pre><code class="yaml"># 导出 eBPF 代理指标
prometheus:
  port: 9090
  path: /metrics

# 代理有问题时告警
# - 高 CPU 使用率
# - 事件丢失
# - 导出失败</code></pre><hr/><h2>12. 结论</h2><p>基于 eBPF 的自动化测量代表了可观测性的范式转变。通过将测量迁移到内核级，我们可以：</p><ul><li><strong>消除测量负担</strong>：不再按服务集成 SDK</li><li><strong>实现全覆盖</strong>：观察任何应用，任何语言</li><li><strong>减少盲点</strong>：发现那些被忽视的服务</li><li><strong>加快上线速度</strong>：新服务可立即被观测到</li></ul><p>但并不能完全取代传统测量。最佳可观测性策略结合了：</p><ol><li>eBPF 用于基线基础设施层级可视化</li><li>针对特定框架上下文的自动化测量库</li><li>为业务关键范围和自定义属性提供手动测量</li></ol><p>像 Beyla 和 Odigo 这样的工具让入门变得前所未有的简单。如果应用运行在 Kubernetes 和 Linux 上，只需要几分钟就可以实现整个分布式追踪技术栈。</p><hr/><h2>要点</h2><ol><li>eBPF 通过从内核观测应用实现<strong>零代码仪表化</strong></li><li><strong>OpenTelemetry 兼容性</strong>意味着 eBPF 数据会流入现有可观测栈</li><li><strong>选择合适的工具</strong>：Beyla 简化应用，Odigos 支持 Kubernetes 分布式追踪，Pixie 负责调试</li><li><strong>混合方法效果最佳</strong>：eBPF 用于覆盖，手动测量用于业务环境</li><li><strong>开销低</strong>（1-5% CPU），但要监控并使用采样</li><li><strong>安全问题</strong>：eBPF 需要特权，授权范围要适当</li><li><strong>从小处开始</strong>：先从非生产环境开始，再扩展到生产环境</li></ol><hr/><h2>延伸阅读</h2><p><a href="https://link.segmentfault.com/?enc=8rS61RcxeoPPcZNrskShnA%3D%3D.yWYeDN1rFKlSFKHoEAEMW%2FGzrHqF0n74%2FuzNqRyjvPfwipl7Emw%2BdigB3tUCmhUj5opWZ%2BtBeJaQOHRewhmchRqBnbUux3gUpUbXuIkfP1U%2BgD9SPjFDloVc0V9lcITJ" rel="nofollow" title="What is eBPF and How Does It Work?" target="_blank">What is eBPF and How Does It Work?</a> —— 深入探讨 eBPF 基础知识</p><p><a href="https://link.segmentfault.com/?enc=KD42xNyg5WGQG%2FB2CrgnUw%3D%3D.puICwmFgszwjsR4ctBcCupEfRWAUBnNl8qcv61AhMb8CrNHYZUpPJoNoMv7LEX%2BThjihR8AjOy3BpSe3lWfzaqYPjxaTSoB3urUVH%2FHj0Q%2BSYt6GDB3DEXq6n7Tw3UmQ" rel="nofollow" title="Traces and Spans in OpenTelemetry" target="_blank">Traces and Spans in OpenTelemetry</a> —— 理解分布式追踪</p><p><a href="https://link.segmentfault.com/?enc=9J5ycn9OXLrrVvbn7Y31eA%3D%3D.7yciEnjaJhg3q7Qnk%2F%2FL7bPufcmb7o%2BMbYDx6eQuWM6FXFl08YnbFFwt83hW0EqDfCQmITekntFdVzBVqnZlN0l5dI9W95NQMtLuveyEihcXlnCW8Jz1j%2FVG7R4tqNc3" rel="nofollow" title="What are Metrics in OpenTelemetry?" target="_blank">What are Metrics in OpenTelemetry?</a> —— 指标基础</p><p><a href="https://link.segmentfault.com/?enc=qBm0rFHA4siHBzhbwO%2FxhQ%3D%3D.VS58lWU81YvHE4cg5%2FnpheZv%2B5CpenwKgqkMZA5fV5zBEf%2FFCNTREapFQZmNysXXlkZ2gj4lfm9YNMGywjc7dVu3CdihMxLIx8Kofw9JQ%2F%2BoO38%2F9%2BTkvBm71YqrLwCCBiYcUj9JqmHygSea8swEqg%3D%3D" rel="nofollow" title="Logs, Metrics &amp; Traces: The Three Pillars" target="_blank">Logs, Metrics &amp; Traces: The Three Pillars</a> —— 完整的可观察性概述</p><p><a href="https://link.segmentfault.com/?enc=ycDnA7QgIJqzJiWgIgplFw%3D%3D.X7Oh6BFW9rRwXhFN17q8S0DrnXYPgajVTcSTYc0Pi2QaHtLevs%2BtXQtpaa%2BaPxGVyQp5yeElV78sr1h%2FELqo4l9PoBjvHDsC5HiF7FvCOeQ%3D" rel="nofollow" title="Basics of Profiling" target="_blank">Basics of Profiling</a> —— 需要更深入的性能洞察时</p><hr/><blockquote>Hi，我是俞凡，一名兼具技术深度与管理视野的技术管理者。曾就职于 Motorola，现任职于 Mavenir，多年带领技术团队，聚焦后端架构与云原生，持续关注 AI 等前沿方向，也关注人的成长，笃信持续学习的力量。在这里，我会分享技术实践与思考。欢迎关注公众号「DeepNoMind」，星标不迷路。也欢迎访问独立站 <a href="https://link.segmentfault.com/?enc=kojG2x4nVW9nQASnfwHI0Q%3D%3D.9Cxcxx28t2gDkYYuzPS2%2F5CDzNMvr%2BbDkPP8wdwiY%2Bk%3D" rel="nofollow" title="www.DeepNoMind.com" target="_blank">www.DeepNoMind.com</a>，一起交流成长。</blockquote><p>本文由<a href="https://link.segmentfault.com/?enc=dmWMj5frZVr6HncVLM7Jxg%3D%3D.7%2BlKZ5bt1vJb0NSB221%2FuN4T%2FO1cC7KgVoY9YF1bnjU%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[一文认识:低代码平台是什么,低代码的本质,未来发展以及适合哪些行业? 织信informat ]]></title>    <link>https://segmentfault.com/a/1190000047494089</link>    <guid>https://segmentfault.com/a/1190000047494089</guid>    <pubDate>2025-12-22 18:08:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>最近有不少朋友问我：</p><p>“你之前说用低代码平台搭了个公司内部系统，这玩意到底是啥？我们公司的人能用得上吗？”</p><p>说实话，低代码/零代码这些年在互联网行业已经烂大街，但在有些行业，比如制造业/工程行业，甚至是工程公司、施工单位、监理、设计院里，其实都还属于“新鲜物件”。</p><p>今天我就写一篇不需要任何IT背景也能看懂的低代码科普——让你知道它到底是什么、能解决什么问题、对制造业/工程行业有没有价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494091" alt="image.png" title="image.png"/></p><p><strong>一、低代码技术的本质解构</strong></p><p>1、低代码定义</p><p>低代码开发平台（Low-Code Development Platform, LCDP）是一种通过可视化建模和配置而非传统手工编码来创建应用程序的开发环境。低代码的核心在于它将软件开发从专业程序员的专属领域，转变为业务人员也能参与的过程。</p><p>技术层面上，低代码平台通常包含以下关键组件：</p><p>可视化建模工具：通过拖拽UI组件、定义数据模型、配置业务流程的可视化界面</p><p>模型驱动架构：系统自动将可视化模型转换为可执行代码</p><p>预构建模块库：包括常见业务功能模块（表单、报表、工作流等）</p><p>集成连接器：与现有系统（ERP、CRM、数据库等）的标准接口</p><p>一键部署机制：简化测试、部署和运维流程</p><p>2、低代码与零代码的区别</p><p>理解低代码技术，首先要做好区分：</p><p>零代码平台：面向完全无编程背景的业务用户，通过完全可视化的方式构建应用，适用于标准化程度高、逻辑相对简单的场景</p><p>低代码平台：保留一定程度的手工编码能力，允许专业开发者扩展平台功能，处理复杂业务逻辑和集成需求</p><p>实际应用中，大多数平台处于两者之间，既提供可视化构建能力，也开放API和自定义代码接口，形成灵活的技术栈。</p><p>3、技术演进</p><p>低代码并非全新概念，其思想可追溯到20世纪90年代的第四代编程语言（4GL）和快速应用开发（RAD）工具。然而，现代低代码平台的突破在于：</p><p>云计算架构：基于云原生技术，实现弹性扩展和便捷访问</p><p>移动优先设计：天然支持移动端应用生成</p><p>AI增强：部分平台集成AI能力，如智能表单识别、流程优化建议、代码自动编写</p><p>开放生态：丰富的第三方组件市场和模板库</p><p><strong>二、制造业/工程行业为何特别适合“低代码”？</strong></p><p>1、行业痛点与数字化的矛盾</p><p>拿工程行业举例，工程业务具有独特特征，这些特征既创造了数字化需求，也构成了实施障碍：</p><p>高度项目化与临时性：每个工程项目都是独特的临时性组织，需求差异大、生命周期有限。传统定制开发成本高、周期长，往往项目结束了系统才上线。</p><p>业务流程碎片化：从材料管理、进度控制到质量安全监督，工程管理涉及数十个专业流程，且不同企业、不同项目流程各异。</p><p>现场与办公室的割裂：大量业务发生在施工现场，但数据处理和分析需要在办公室完成，传统系统难以无缝衔接。</p><p>人员流动性大：项目团队随项目开始而组建、随项目结束而解散，系统培训成本高。</p><p>2、低代码与工程管理的契合点</p><p>需求匹配度</p><p>工程企业的IT需求呈现典型的二八分布：20%的核心系统（如财务、ERP）需要高度标准化和稳定性，适合采购成熟产品；80%的业务应用（如专项检查、临时报表、项目特定流程）需求分散、变化频繁、生命周期短——这正是低代码的优势领域。</p><p>成本效益比</p><p>传统软件开发模式下，一个中等复杂度的业务系统（如材料验收系统）开发成本通常在50-60万元，后续每次修改需数万元和数周时间。低代码平台将初始构建成本降低至传统模式的20%-50%，修改成本降低90%以上，且时间缩短为小时或天级。</p><p>业务与技术融合</p><p>工程管理的核心是专业知识——规范标准、工艺流程、安全要求、材料特性等。传统开发模式中，业务人员需将需求翻译给开发人员，存在信息失真风险。低代码平台使业务专家能够直接参与甚至主导系统构建，确保业务逻辑的准确性。</p><p>3、工程行业的低代码应用成熟度曲线</p><p>根据技术采纳生命周期理论，工程行业的低代码应用正处于从早期采用者向早期大众过渡的阶段：</p><p>创新者（2015-2018）：少数大型工程企业试点</p><p>早期采用者（2019-2021）：更多企业尝试用于非核心业务</p><p>早期大众（2022-现在）：开始在核心业务流程中应用</p><p>晚期大众（预计2025年后）：成为标准工具之一</p><p><strong>三、工程行业低代码应用场景解析</strong></p><p>1、材料管理系统</p><p>传统工程材料管理多依赖Excel表格，面临版本混乱、数据滞后、缺乏协同等问题。基于低代码构建的材料管理系统可实现：</p><p>多维数据建模</p><p>材料主数据（规格、型号、技术参数）</p><p>供应商信息（资质、评价、历史合作）</p><p>库存状态（在途、在库、已领用）</p><p>价格信息（合同价、市场价、历史价）</p><p>智能业务流程</p><p>采购申请→审批→订单生成→到货验收→入库→领用→结算的全流程数字化</p><p>自动关联设计用量与实际消耗，预警超耗风险</p><p>移动端扫码验收，自动匹配采购订单</p><p>数据分析与可视化</p><p>材料成本占工程造价的实时分析</p><p>供应商绩效自动评价</p><p>库存周转率、资金占用分析</p><p>2、质量安全管理</p><p>低代码平台可构建的质量安全管理系统超越传统纸质检查表，实现：</p><p>检查标准数字化</p><p>将规范条文转化为可执行的检查项</p><p>根据不同工程类型（房建、市政、公路）配置不同检查模板</p><p>支持图文并茂的问题描述</p><p>闭环整改流程</p><p>问题发现→整改通知→整改实施→复查验证的全流程追踪</p><p>自动分配责任人、设定整改期限、发送提醒</p><p>严重问题自动升级通知机制</p><p>风险预警与分析</p><p>基于历史数据的常见问题预测</p><p>安全隐患趋势分析</p><p>质量安全绩效可视化看板</p><p>3、进度管理系统</p><p>传统进度管理依赖Project或简单甘特图，难以应对工程变更。低代码进度管理系统提供：</p><p>多级计划联动</p><p>总进度计划→月计划→周计划→日计划的层层分解与关联</p><p>实际进度与计划进度的可视化对比</p><p>关键路径动态计算与预警</p><p>进度数据自动采集</p><p>移动端现场进度填报</p><p>与BIM模型关联，可视化展示进度状态</p><p>自动关联工程量完成情况</p><p>延误影响分析</p><p>进度延误对后续工序的自动影响分析</p><p>资源冲突预警</p><p>进度索赔资料自动整理</p><p>4、协同办公与流程审批</p><p>工程项目的多方参与方（业主、设计、施工、监理、分包）需要高效协同。低代码可构建：</p><p>统一协同平台</p><p>设计图纸在线审查与批注</p><p>工程联系单、变更签证的数字化流程</p><p>会议纪要、指令通知的自动分发与确认</p><p>智能审批流引擎</p><p>根据金额、类型、紧急程度自动路由审批流程</p><p>移动端审批，支持手写签名</p><p>审批时限监控与超时提醒</p><p>知识积累与复用</p><p>问题处理经验的知识库积累</p><p>优秀施工方案的模板化</p><p>常见技术问题的解决方案库</p><p><strong>四、低代码平台的技术评估与选型指南</strong></p><p>1、工程行业选型关键指标</p><p>选择低代码平台时，工程企业应重点关注以下维度：</p><p>工程适配性</p><p>是否支持离线操作（应对施工现场网络不稳定）</p><p>移动端体验是否流畅</p><p>是否支持拍照、定位、扫码等工程常用功能</p><p>扩展与集成能力</p><p>与常用工程软件（AutoCAD、Revit、Project）的集成能力</p><p>与现有系统（财务、人力资源）的数据接口</p><p>自定义组件的开发支持</p><p>数据安全与合规</p><p>数据存储位置和备份机制</p><p>权限控制粒度（能否实现项目部、公司多级权限）</p><p>操作日志和审计跟踪</p><p>成本结构透明性</p><p>许可模式（按用户、按应用、混合模式）</p><p>隐藏成本（培训、定制开发、维护）</p><p>长期总拥有成本（TCO）估算</p><p>2、主流平台特性对比</p><p>国内平台</p><p>钉钉宜搭/飞书多维表格</p><p>优势：与办公平台深度集成，用户无需额外账号；模板丰富，上手极快</p><p>适用场景：中小企业内部管理、审批流程、简单数据收集</p><p>工程适用性：适合行政管理、人事考勤等通用场景，专业工程功能需较多定制</p><p>织信</p><p>优势：业务流程引擎强大；支持复杂业务逻辑；有较多工程行业案例</p><p>适用场景：中等复杂度的业务系统，如材料管理、质量检查</p><p>工程适用性：较高，有专门的项目管理行业模板</p><p>简道云/轻流</p><p>优势：表单和流程设计简单直观；数据处理能力强；移动端体验好</p><p>适用场景：数据收集和流程审批类应用</p><p>工程适用性：适合巡检、验收、报验等现场数据采集场景</p><p>国外平台</p><p>Microsoft Power Apps</p><p>优势：与Office 365生态无缝集成；AI能力强大；企业级安全控制</p><p>适用场景：已有微软生态的企业，需要深度定制和复杂集成</p><p>工程适用性：高，但需要较强的技术能力配置</p><p>Airtable</p><p>优势：表格界面直观；视图类型丰富；自动化能力强</p><p>适用场景：数据管理和协作类应用</p><p>工程适用性：适合项目管理、设备台账等表格密集型场景</p><p>3、平台选型决策框架</p><p>建议采用四阶段评估法</p><p>第一阶段：需求梳理</p><p>列出3-5个优先实施场景</p><p>明确功能需求、用户规模、集成需求</p><p>评估内部技术能力（是否有IT支持）</p><p>第二阶段：平台初选</p><p>选择3-4个符合基本要求的平台</p><p>申请试用账号，进行原型构建</p><p>评估学习曲线和开发效率</p><p>第三阶段：深度验证</p><p>选择一个典型业务场景进行完整构建</p><p>测试性能、移动端体验、离线能力</p><p>评估长期成本和技术支持</p><p>第四阶段：试点实施</p><p>选择一个小范围试点项目</p><p>收集用户反馈，评估实际效果</p><p>制定推广计划和培训方案</p><p><strong>五、实施策略与成功要素</strong></p><p>1、渐进式实施路径</p><p>低代码应用的成功往往取决于实施策略。建议采用小步快跑、迭代优化的方法：</p><p>第一阶段：单点突破（1-3个月）</p><p>选择一个痛点明显、范围清晰的场景</p><p>快速构建最小可行产品（MVP）</p><p>收集反馈，建立信心</p><p>第二阶段：横向扩展（3-6个月）</p><p>基于成功经验，扩展至相关业务领域</p><p>建立内部低代码开发能力</p><p>制定应用开发和治理规范</p><p>第三阶段：纵向深化（6-12个月）</p><p>构建更复杂的集成应用</p><p>建立企业级低代码平台</p><p>培养业务部门自主开发能力</p><p>第四阶段：生态构建（12个月以上）</p><p>形成低代码开发社区</p><p>与合作伙伴共享应用模板</p><p>探索创新应用场景</p><p>2、组织能力建设</p><p>技术工具的成功应用离不开组织能力的支撑：</p><p>角色定义与培养</p><p>公民开发者（业务人员）：掌握基础平台操作，能构建简单应用</p><p>低代码专家：精通平台高级功能，能设计复杂业务逻辑</p><p>平台管理员：负责用户管理、权限控制、性能监控</p><p>治理机制建立</p><p>应用上线审批流程</p><p>数据安全和隐私保护规范</p><p>系统维护和升级计划</p><p>知识管理与传承</p><p>建立内部模板库和组件库</p><p>定期分享最佳实践</p><p>形成持续学习的文化</p><p>3、避免常见陷阱</p><p>根据行业实践，工程企业应用低代码需特别注意：</p><p>避免过度定制：不是所有需求都适合低代码实现，核心复杂系统仍需专业开发</p><p>防止数据孤岛：确保低代码应用与核心系统数据互通</p><p>管理期望落差：明确低代码的优势和局限，设定合理预期</p><p>关注技术债务：即使可视化开发，也需要良好的设计和文档</p><p><strong>六、低代码的未来趋势与展望</strong></p><p>1、技术趋势</p><p>低代码平台的未来演进将呈现以下趋势：</p><p>AI辅助开发：</p><p>自然语言描述自动生成应用</p><p>智能推荐业务流程和界面设计</p><p>自动优化应用性能</p><p>行业垂直化：</p><p>面向工程行业的专用组件和模板</p><p>集成行业标准（如BIM标准、工程计量规范）</p><p>预置行业最佳实践流程</p><p>混合开发模式：</p><p>低代码与专业代码的无缝协作</p><p>微服务架构下的低代码模块</p><p>边缘计算与低代码结合</p><p>2、低代码在工程数字生态中的定位</p><p>未来工程行业的数字生态将是多层结构：</p><p>基础平台层：ERP、BIM、项目管理核心系统</p><p>低代码应用层：业务部门自主构建的敏捷应用</p><p>数据智能层：基于大数据的分析和决策支持</p><p>现场物联层：传感器、无人机、智能设备的实时数据</p><p>低代码将成为连接各层的粘合剂，实现快速创新和灵活适应。</p><p>3、给工程企业的行动建议</p><p>基于当前技术成熟度和行业实践，建议工程企业：</p><p>立即行动：</p><p>组织低代码技术研讨会，提升管理层认知</p><p>选择1-2个试点场景，启动小范围尝试</p><p>培养首批公民开发者，建立内部能力</p><p>中期规划：</p><p>制定企业低代码战略和治理框架</p><p>建立低代码卓越中心（CoE）</p><p>将低代码纳入数字化转型路线图</p><p>长期愿景：</p><p>形成业务与技术融合的创新文化</p><p>构建基于低代码的数字化敏捷能力</p><p>探索低代码与新兴技术（AI、IoT、数字孪生）的融合应用</p><p><strong>七、结语</strong></p><p>低代码不是要替代专业开发人员，也不是要解决所有IT问题。它的核心价值在于：在专业开发与业务需求之间建立新的平衡点，让工程企业能够以更低的成本、更快的速度响应业务变化。</p><p>对于长期面临IT资源不足、业务需求多变困境的工程企业，低代码提供了一个务实而有效的选择。它降低了数字化的门槛，让更多企业能够享受技术带来的效率提升和业务创新。</p><p>工程行业的数字化不是一蹴而就的革命，而是渐进式的演进。低代码技术正是这一演进过程中的重要助推器——它不是万能的魔法棒，但确实是一把打开数字化大门的钥匙，让更多工程人能够参与其中，共同塑造行业的数字未来。</p><p>正如一位工程项目经理在使用低代码平台后所说：“我们终于不再是被动等待IT部门排期的业务部门，而是能够主动解决自己问题的建设者。”这种从使用者到创造者的角色转变，或许才是低代码带给工程行业最深刻的价值。</p>]]></description></item><item>    <title><![CDATA[中国新冠疫情的探索性空间数据分析 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494094</link>    <guid>https://segmentfault.com/a/1190000047494094</guid>    <pubDate>2025-12-22 18:07:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>这个东西主要是2020年上半年，疫情在家里上网课，某一门课程的期末大作业。作业参考了一篇分析SARS的论文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>，将其中的部分方法应用于新冠疫情的数据，主要是计算莫兰指数。我的方向跟这个没啥关系，也不太清楚这些指标的具体含义，仅仅是将他们计算了出来，并稍作解释。下面就直接贴代码和参考文献了。</p><p>先贴一下参考文献部分，我也不太了解莫兰指数的具体细节，就不多叙述了，自己都是看这些博客尝试理解的</p><ul><li><a href="https://link.segmentfault.com/?enc=ZsZ0y37I6iCiNA360oN61w%3D%3D.DBE0jGYaKeeZ%2FnV1ZwcikVXp0zq2lyt7Y6GH1T%2BURXONPtdWNW8ir%2FEdBvNhYKhgFJniXbDR2V9nxqcWVoIalg%3D%3D" rel="nofollow" target="_blank">python计算莫兰指数(Moran's I)并绘制地区热力图——以中国各省pm2.5为例</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></li><li><a href="https://link.segmentfault.com/?enc=LeVeRNd63QlmxhDww9jiJw%3D%3D.FAGxYlHkTTT3OLTkUt8rRjteF%2Bwn7twv4moWPv1dRwCZHKkr6q%2F4qOTMbslSJjCd9qUiJekbMet5TmwxbbwFKQ%3D%3D" rel="nofollow" target="_blank">空间统计：Moran's I（莫兰指数）</a><sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup></li><li><a href="https://link.segmentfault.com/?enc=cgE4Rx2NNv0SNg8oWb2ERg%3D%3D.8XcToCkQiq4YD9D8zT1F%2BEOlTivjWgJlklpWOFNFNhsAefygJDPGJp0bfwa4ZEmxR8j2uNBM8NwG%2F5heUD8flA%3D%3D" rel="nofollow" target="_blank">莫兰指数（Moran's I）的小总结</a><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup></li><li><a href="https://link.segmentfault.com/?enc=ZMN9WjsERqh1R2pI2Bl4zQ%3D%3D.pUsSkJU6RddjgIjh0ofD5370keCEed%2BcDiQpwqM3xoEfD4Y6B4RK0H6wePy1ACcteKY51%2FhSajstm%2F4wapZLvw%3D%3D" rel="nofollow" target="_blank">白话空间统计之四：P值和Z得分（中）</a><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></li></ul><p>下面是数据处理和分析的主要过程，数据就不贴了，大家参考流程就好</p><pre><code class="python">#%%

import pandas as pd

#%%


# 得到累计确诊数据

globalData = pd.read_csv('time_series_covid19_confirmed_global.csv')

# 读入全球的时间序列累计数据

chinaData = globalData[ globalData['Country/Region'] == 'China']

# 选出中国的省份和地区，33个

taiwanData = globalData[ globalData['Country/Region'] == 'Taiwan*']

# 再选出台湾地区

taiwanData['Country/Region'] = 'China'
taiwanData['Province/State'] = 'Taiwan'

chinaData = pd.concat([chinaData, taiwanData])

# 合并成新的中国数据框

chinaData = chinaData.reset_index(drop=True)

# 对数据框的索引进行重新编排

chinaData.to_csv('ChinaData.csv',index=0)

#%%

import matplotlib.pyplot as plt

# 累计确诊画折线图

chinaData = pd.read_csv('D:\WHO\ChinaData.csv',low_memory=False)

datelist = list(chinaData.columns[4:119])

# 抽取1/22/20 - 5/15/20的疫情数据

for i in range(0,34):
    x = datelist
    y = list(chinaData.loc[i,datelist])
    plt.plot(x,y)

plt.axvspan('1/22/20', '3/8/20',facecolor='#2E8B57',alpha=0.1)
plt.axvspan('3/8/20', '5/15/20',facecolor='#FFFF00',alpha=0.1)
plt.axvline(x='3/8/20')
plt.xticks(x, x, rotation=90)
plt.tick_params(labelsize=10)
plt.legend(range(0,34))
plt.show()

# 绘制折线图

#%%

# 今日确诊减去昨日确诊得到每日新增

increaseData = chinaData[chinaData.columns[0:4]]

datelist = list(chinaData.columns[4:119])

# 取出列名，即日期，形成列表

# 每一列减去前一列，即为该日新增

for i in range(1,115) :

    increasenew = chinaData[datelist[i]]-chinaData[datelist[i-1]]

    increasenew = pd.DataFrame(increasenew,columns=[datelist[i]])

    increaseData = pd.concat([increaseData, increasenew],axis=1)

increaseData.to_csv('increaseData.csv',index=0)

#%%

# 给每日新增画曲线图

increaseData = pd.read_csv('increaseData.csv',low_memory=False)

datelist = list(increaseData.columns[4:118])

# 取出新增数据

for i in range(0,34):
    x = datelist
    y = list(increaseData.loc[i,datelist])
    plt.plot(x,y)

plt.axvspan('1/23/20','3/8/20',facecolor='#2E8B57',alpha=0.1)
plt.axvspan('3/8/20','5/15/20',facecolor='#FFFF00',alpha=0.1)
plt.axvline(x='3/8/20')
plt.xticks(x, x, rotation=90)
plt.tick_params(labelsize=10)
plt.legend(range(0,34))
plt.show()

#%%

# 按阶段汇总确诊人数

chinaData = pd.read_csv('ChinaData.csv',low_memory=False)

statData = chinaData[chinaData.columns[0:4]]

statData['stage1'] = chinaData['3/8/20']

statData['stage2'] = chinaData['5/15/20'] - chinaData['3/8/20']

statData['total'] = chinaData['5/15/20']

#%%
# 拿到死亡人数

globalData = pd.read_csv('05-15-2020.csv')

chinaData = globalData[ globalData['Country_Region'] == 'China']

taiwanData = globalData[ globalData['Country_Region'] == 'Taiwan*']

taiwanData['Country/Region'] = 'China'
taiwanData['Province/State'] = 'Taiwan'

chinaData = pd.concat([chinaData, taiwanData])

chinaData = chinaData.reset_index(drop=True)

statData['death'] = chinaData['Deaths']

statData.to_csv('statData.csv',index=0)

#%%

# 计算空间权重矩阵

import pandas as pd

import numpy as np

from geopy.distance import geodesic

statData = pd.read_csv('D://WHO//statData.csv')

def DISTMatrix(SW,Dist):
    for i in range(0,34):
    #循环34个省份
    
        for t in range(i+1,34):
        # 循环其他省份
        
            cityDistance = geodesic(
                (statData.loc[i,'Lat'],statData.loc[i,'Long']),
                (statData.loc[t,'Lat'],statData.loc[t,'Long'])
                ).km
            # 计算两个地方的直线距离
            # 输出结果的单位为Km
            
            if(cityDistance&lt;=Dist):
                SW[i,t] = 1.0
                SW[t,i] = 1.0
            # 判断，若小于条件
            # 则标记为1，视为接壤
            # 该省对该省自身视为不接壤
                
# 定义空间权重矩阵
# 输入参数Dist
# 两个地方的距离小于Dist即视为接壤
# 由参数带入的矩阵SW
# 即为Dist条件下的
# 空间矩阵结果


def MatrixSelect(dist):
    SWName = 'SW' + str(dist/1000)+'.csv'
    SW = np.mat(np.zeros((34,34)))
    DISTMatrix(SW,dist)
    SW = pd.DataFrame(SW)           
    SW.to_csv('D://WHO//'+SWName,index=0,header=None)
    
# 根据传入的参数dist
# 输出空间权重矩阵的csv文件

setDist = 900
while (setDist&lt;=2000):
    MatrixSelect(setDist)
    setDist = setDist + 100
    
# 计算900-2000为参数的各空间权重矩阵
    

#%%

import pandas as pd

import numpy as np

import Moran

statData = pd.read_csv('statData.csv')

stage1 = np.matrix(statData['stage1'])

stage2 = np.matrix(statData['stage2'])

total = np.matrix(statData['total'])

death = np.matrix(statData['death'])

DataList = [stage1,stage2,total,death]

# 读取四个阶段的数据，形成列表
# 作为指数计算的内层循环
# 即对外层每一个矩阵，计算一遍该列表

FileList = ['SWM.csv']
i = 0.9
while ( i &lt; 2.1 ):
    i = round(i,2)
    FileName = 'SW'+str(i)+'.csv'
    FileList.append(FileName)
    i = i + 0.1
    
# 将权重矩阵的文件名形成列表，作为指数计算的外层循环

#%%
 
MoranChart = pd.DataFrame(columns=range(0,12))
# 创建数据框存储莫兰值，Z值，和p值

for i in range(0,13) :
# 循环13个空间权重矩阵
    
    SW = pd.read_csv(FileList[i],header=None)
    # 读取文件
    
    SW = np.matrix(SW.values)
    # 转换成矩阵
    
    ##########################
    print()
    print(i)
    ##########################
    
    for t in range(0,4):
    # 循环四个数据组
        
        MoranChart.loc[i,(3*t):(3*t)+2]  = Moran.Index(DataList[t], SW)

MoranChart.to_csv('D://WHO//MoranChart.csv',
                  index=0,header=None)

#%%

# 选出各阶段p值最小的数据，绘制莫兰散点图

SW = pd.read_csv('SW1.5.csv',header=None)

SW = np.matrix(SW.values)

Moran.Plot(stage1,SW)

Moran.Plot(total,SW)

Moran.Plot(death,SW)

SW = pd.read_csv('SW0.9.csv',header=None)

SW = np.matrix(SW.values)

Moran.Plot(stage2,SW)
</code></pre><p>下面是计算莫兰指数等用到的模块，我也是从别的地方找的，然后抽取出来这些部分</p><pre><code class="python">
import numpy as np

import matplotlib.pyplot as plt

from scipy.stats import norm

def Index(X,W):
    
    #x为行矩阵，W为权重矩阵（0-1矩阵，且对称）
    #X = np.matrix([8,6,6,3,2])
    #W = np.matrix([[0,1,1,0,0],[1,0,1,1,0],[1,1,0,1,0],[0,1,1,0,1],[0,0,0,1,0]])
    
    #预处理
    #X = np.matrix.transpose(X)
    X = X.T
    
    #对输入的行矩阵进行转置
    X = X/1.0
    
    #保证X中的数据为浮点型
    W = W/1.0
    #保证W中的数据为浮点型
    
    # # 求向量X的长度
    n = len(X)
    
    # # 求矩阵W每行之和
    # W_sum = np.sum(W,axis = 1)#axis=1表示对矩阵的行求和
    # W_standard = W/(W_sum*np.ones(n))#对W进行标准化
    
    # 对矩阵进行标准化
    Wsum = W.sum(axis=1)
    # 矩阵按行求和
    Locate0 = np.where(Wsum == 0)[0]
    # 取出和为0的行号
    Wsum[Locate0] = 1.0
    # 将这些行的和赋值为1.0
    # 否则0会出现在分母上
    W_standard = W/Wsum
    # 进行标准化
    
    #对x进行标准化
    xm = np.mean(X)
    sx = np.std(X)
    z = (X-xm)/sx
    
    #求标准化后的纵坐标，这里的x和W都进行了标准化
    Wz = W_standard*z
    #z = np.matrix.transpose(z)
    #A = np.vstack([z, np.ones(n)]).T
    
    #求线性回归系数，得出的直线斜率就是moran'I值
    z_lstsq = np.linalg.lstsq(z,Wz,rcond=-1)
    
    #moran'I值检验
    WW = W_standard
    E_I = -1.0/(n-1)
    S0 = np.sum(WW)
    WW1 =np.multiply(WW+WW.T,WW+WW.T)
    S1 = np.sum(WW1)/2.0
    WW2 = np.sum((WW+WW.T),axis = 1)
    S2 = np.sum(np.multiply(WW2,WW2))
    Var_I = (n*n*S1-n*S2+3*S0*S0)/((n*n-1)*S0*S0)-E_I*E_I
    Z_I = (float(z_lstsq[0])-E_I)/np.sqrt(Var_I)
    
    
    # 计算该z值对应的p值
    Z_p = norm.cdf(Z_I)
    
    
    print('moran`I值为:',float(z_lstsq[0]))
    print('Z值为:',Z_I)
    print('p值为',Z_p)
    
    return float(z_lstsq[0]),Z_I,Z_p
    # 依次返回 莫兰值，Z检验值，P概率值

def Plot(X,W):
    
    #x为行矩阵，W为权重矩阵（0-1矩阵，且对称）
    #X = np.matrix([8,6,6,3,2])
    #W = np.matrix([[0,1,1,0,0],[1,0,1,1,0],[1,1,0,1,0],[0,1,1,0,1],[0,0,0,1,0]])
    
    #预处理
    #X = np.matrix.transpose(X)
    X = X.T
    
    #对输入的行矩阵进行转置
    X = X/1.0
    
    #保证X中的数据为浮点型
    W = W/1.0
    #保证W中的数据为浮点型
    
    # # 求矩阵W每行之和
    # W_sum = np.sum(W,axis = 1)#axis=1表示对矩阵的行求和
    # W_standard = W/(W_sum*np.ones(n))#对W进行标准化
    
    # 对矩阵进行标准化
    Wsum = W.sum(axis=1)
    # 对矩阵按行求和
    Locate0 = np.where(Wsum == 0)[0]
    # 找出和为0的行
    Wsum[Locate0] = 1.0
    # 讲该行的和赋值为1.0
    # 否则即0出现在分母位置
    W_standard = W/Wsum
    # 对矩阵标准化
    
    #对x进行标准化
    xm = np.mean(X)
    sx = np.std(X)
    z = (X-xm)/sx
    
    #求标准化后的纵坐标，这里的x和W都进行了标准化
    Wz = W_standard*z
    #z = np.matrix.transpose(z)
    #A = np.vstack([z, np.ones(n)]).T
    
    #求线性回归系数，得出的直线斜率就是moran'I值
    z_lstsq = np.linalg.lstsq(z,Wz,rcond=-1)

    #求拟合值,拟合的直线一定过原点
    z_z = z*z_lstsq[0]
    
    #画moran'I散点图
    plt.tick_params(labelsize=30)
    plt.plot(z,Wz,'ro')
    plt.plot(z,z_z)
    plt.grid()
    plt.xlabel('x axis',fontsize=30)
    plt.ylabel('Wx axis',fontsize=30)
    plt.title('Moran`I Scatter Plot',fontsize=30)
    plt.show()
</code></pre><div class="footnotes"><hr/><ol><li id="fn-1"> [范新生;应龙根. 中国SARS疫情的探索性空间数据分析[J]. 地球科学进展, 2005, 20(3): 282-291.](<a href="https://link.segmentfault.com/?enc=ERXNAhU%2BhPEgwkJCWpPeNA%3D%3D.X0MQAfH5f5hkKw3YBQhA5n6rngE5otPs7zxGlrfnxaZDVuUMcwHwea%2FH4ANtuI%2BS" rel="nofollow" target="_blank">http://www.adearth.ac.cn/CN/Y2005/V20/I3/282</a>) <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=wUfqo%2FLmZr1giHhiCpBu%2BQ%3D%3D.emxIblmEmJ3rlhuj9KLZUpfZfTf9pa%2BihoG4LAoafrS8%2Fd5nPN%2FYJxObb%2F5lts0EWKmMPVgpBBBslP7h%2FfTygA%3D%3D" rel="nofollow" target="_blank">python计算莫兰指数(Moran's I)并绘制地区热力图——以中国各省pm2.5为例</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=tUpbjVlgxnrf%2F3CWfzzS8g%3D%3D.x5gC%2B4VhmbF6xl5PIsHevrCXdCSM1%2Fm%2Fg527tKUy4r6lc3spUwFD8NMLEpKmVZFki2qtcnilVNXxnUX0XCo2Bw%3D%3D" rel="nofollow" target="_blank">空间统计：Moran's I（莫兰指数）</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=SeXI66WdCKUv1Mc4VT3pEw%3D%3D.SNpM%2F5S0ScPhHLYNy6m8eIBo3Su4K7akbXWCimZjBFKwgjkmrbow0BAPN2IbXiyweHCPuanaqXccZWmeiEmA4g%3D%3D" rel="nofollow" target="_blank">莫兰指数（Moran's I）的小总结</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=ddtRBGHnw9QqmjV%2Ft4pUgg%3D%3D.lqxaJv3nX%2FxWhVjJFx0Z3EcUuyTlyp0jm2S9ASDdKRo2wd6cZ%2BQZSUVrRvTl0FaGB7ZqGZCh0%2BXwFc3ATBIaBQ%3D%3D" rel="nofollow" target="_blank">白话空间统计之四：P值和Z得分（中）</a> <a href="#fnref-5" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[腾讯镜像解决gradle报错：ERROR: Could not install Gradle dis]]></title>    <link>https://segmentfault.com/a/1190000047494108</link>    <guid>https://segmentfault.com/a/1190000047494108</guid>    <pubDate>2025-12-22 18:06:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>配置Android Studio的时候经常报错gradle，解决某一次报错的时候发现了这个东西<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>。</p><p>按照路径</p><blockquote>D:\Project\Studule\gradle\wrapper\gradle-wrapper.properties</blockquote><p>打开文件，将</p><blockquote>distributionUrl=https\://services.gradle.org/distributions/gradle-6.5-bin.zip</blockquote><p>替换成</p><blockquote>distributionUrl=https\://mirrors.cloud.tencent.com/gradle/gradle-6.5-bin.zip</blockquote><p>此外我还参考了其他博客</p><ul><li><a href="https://link.segmentfault.com/?enc=mdJ3EhzmMlJ2Ng2ZpKzTeQ%3D%3D.zRp48Jn0N45PH5atFE6zE1qnR7uGux6yDAClc%2FUHzARKOU3wGbj7iAAdOJRU8QLX" rel="nofollow" target="_blank">ERROR: Could not install Gradle distribution from 'https://services.gradle.org/distributions/grad...</a><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup></li></ul><p>之前有过一个想法，给某个课表APP做一个桌面插件，但一直没时间，先把一些参考文献列在下面。挖个坑，万一哪天填了，就独立出来写一篇博文</p><blockquote><ul><li><a href="https://link.segmentfault.com/?enc=P2j8yHqezMn4dpuZWm6LSQ%3D%3D.gPwTSbSKifoM2XOrTYERp3EYh9dYx%2FbrhR5iZMSgS60T%2BKsewJVX1yRq6%2FURiFve%2BMoqWrRlkYSke95Iwx8M9w%3D%3D" rel="nofollow" target="_blank">在widget实现复杂布局（Listview，GirdView）以及RemoteViewsService、RemoteViewsFactory的用法</a></li><li><a href="https://link.segmentfault.com/?enc=7Sc35fLWx4MMaY06mjpIuw%3D%3D.lnGftz7e%2BVE98ClHDDDGkKeamlYBavSFK4%2FYxuD40CFLbbXzfib1344UB8xQzTtGUw85sAMeYuy%2FsMHElVeGdQ%3D%3D" rel="nofollow" target="_blank">App Widgets 详解四 RemoteViews、RemoteViewsService和RemoteViewsFactory</a></li><li><a href="https://link.segmentfault.com/?enc=KDRyL185KLdLh9iQj%2FKfwA%3D%3D.Wb7C%2Fk7RzqVQ5l7vm99DfbKlb0yEI21%2BQBVH7HX1xvxWzl4jX%2B2JZOvvIrN4EEWp" rel="nofollow" target="_blank">Android桌面小部件AppWidget开发</a></li><li><a href="https://link.segmentfault.com/?enc=uiC4%2Fv7xbi76xaOd5euChA%3D%3D.qkIzpxoWcPYmRT1hBd4My3yrwMDqFYgjpkWIQvIulcE7smlcm%2FhdI%2FuJnyoBaakEbUZhZPqN3yl2I8Xs%2BwY0Lw%3D%3D" rel="nofollow" target="_blank">开发安卓桌面widget</a><br/><a href="" target="_blank"/></li></ul></blockquote><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=xsJrQ5u8ZLvwmml%2B6Uw5Tg%3D%3D.YQ46f7IDJsyyquPlNvgcPrpBlT5Wxzc6X1PWeF3q4NQsrGeFHHwgv8ikVHAW6lOg" rel="nofollow" target="_blank">Index of /gradle/</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=0Q%2FVWLYLS2JaetRtx07YLw%3D%3D.X7g1%2FdfESxzhBLt4lyj1NoBBEn9%2BY9rJ2vrGOeUqP%2F75Zsojy2eQox1d1ouzIFzM" rel="nofollow" target="_blank">ERROR: Could not install Gradle distribution from 'https://services.gradle.org/distributions/grad...</a> <a href="#fnref-2" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]狗熊会人才计划第6期选拔作业 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494127</link>    <guid>https://segmentfault.com/a/1190000047494127</guid>    <pubDate>2025-12-22 18:05:43</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>数据就不贴了，给个描述吧</p><pre><code class="python"># &lt;class 'pandas.core.frame.DataFrame'&gt;
# RangeIndex: 4492 entries, 0 to 4491
# Data columns (total 7 columns):
#  #   Column    Non-Null Count  Dtype
# ---  ------    --------------  -----
#  0   标题        4417 non-null   object
#  1   标题链接      4492 non-null   object
#  2   brief     4483 non-null   object
#  3   keywords  3272 non-null   object
#  4   发布时间      3817 non-null   object
#  5   新闻类别      4492 non-null   object
#  6   采集时的时间    4492 non-null   object
# dtypes: object(7)
# memory usage: 245.8+ KB</code></pre><p>大概就是这7列，还需要预处理一下</p><p>先对发布时间处理，好奇这是什么时段的新闻</p><pre><code class="python">dataI0['发布时间'].sort_values()
# Out[7]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 4356           NaN
# 4357           NaN
# 4358           NaN
# 4359           NaN
# 4360           NaN
# Name: 发布时间, Length: 4492, dtype: object</code></pre><p>啊这...有缺失值来着，忘记处理了</p><pre><code class="python">dataI0['发布时间'].dropna().sort_values()
# Out[8]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 2374       21-4-23
# 2373       21-4-23
# 2371       21-4-23
# 2369       21-4-23
# 2372       21-4-23
# Name: 发布时间, Length: 3817, dtype: object</code></pre><p>噢，数据格式还不统一。由于上面是排序后的结果，所以我们可以直接找到所有不合格式的数据，它们排序后都在一块儿</p><pre><code class="python">dataI0.loc[2369:2393,'发布时间']
# Out[10]:
# 2369    21-4-23
# 2370    21-4-23
# 2371    21-4-23
# 2372    21-4-23
# 2373    21-4-23
# 2374    21-4-23
# 2375    21-4-23
# 2376    21-4-23
# 2377    21-4-22
# 2378    21-4-22
# 2379    21-4-22
# 2380    21-4-22
# 2381    21-4-22
# 2382    21-4-22
# 2383    21-4-22
# 2384    21-4-22
# 2385    21-4-22
# 2386    21-4-22
# 2387    21-4-22
# 2388    21-4-22
# 2389    21-4-21
# 2390    21-4-21
# 2391    21-4-21
# 2392    21-4-21
# 2393    21-4-21
# Name: 发布时间, dtype: object</code></pre><p>噢，这个需要找找资料<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><pre><code class="python">tmpDF = dataI0.loc[2369:2393,'发布时间']
# 取出时间格式不同的这几个样本

tmpDF = pd.to_datetime(tmpDF,format="%y-%m-%d")
# 按照'两位数年份-月份-天数'的格式进行转换读入时间类型

dataI0.loc[2369:2393,'发布时间'] = tmpDF.dt.strftime("%Y-%m-%d")
# 按照'四位数年份-月份-天数'的格式从时间类型当中输出

dataI0['发布时间'].dropna().sort_values()
# 去除缺失值之后对数据进行排序
# 可以看到新闻的发布时间为2020-10-08到2021-04-24

# Out[47]:
# 2988    2020-10-08
# 2987    2020-10-09
# 2986    2020-10-09
# 2985    2020-10-10
# 2984    2020-10-10
#            ...
# 1934    2021-04-24
# 1935    2021-04-24
# 1936    2021-04-24
# 828     2021-04-24
# 0       2021-04-24
# Name: 发布时间, Length: 3817, dtype: object</code></pre><p>这里是直接使用了pandas自带的模块处理，也还有其他方法，可以参考博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>做完这个才想起来好像有点不对...考虑了缺失，没考虑重复，赶紧给去个重，按着新闻的链接作为唯一标识。</p><pre><code class="python">dataI0 = dataI0.drop_duplicates(['标题链接'])
# 网址链接相当于数据的id
# 根据id对网址进行去重</code></pre><p>接下来，嗯，考虑补全数据。我觉得这个思路就很混乱，处理缺失、去除重复，应该是一开始就要做的事情吧...上面是在干啥，咋突然处理了一波时间格式。</p><pre><code class="python">dataM0 = dataI0.loc[dataI0['标题'].isnull(),['标题链接','标题']].reset_index(drop=True)
# 先抽取出标题缺失的数据，拿到新闻的网址链接
import requests
from lxml import etree

for i in range(0,dataM0.shape[0]):
    print(i,'-'*10)
    # 打印序号
    htmlT = requests.get(dataM0.iloc[i,0])
    # 打开链接
    htmlT.encoding = 'utf-8'
    # 使用utf-8编码
    htmlT = etree.HTML(htmlT.text)
    # 取出其中的文本
    dataM0.iloc[i,1] = htmlT.xpath('/html/body/div[12]/div[1]/div[1]/h1/text()')[0]
    # 根据xpath取出新闻标题，需要精确到text()
    print(dataM0.iloc[i,1],'\n')
    # 打印新闻标题

dataI0.loc[dataI0['标题'].isnull(),'标题'] = dataM0['标题'].values
# 直接使用数据框赋值不可取
# 应当将取values后再赋值
# 因为数据框之间的赋值好像需要看index</code></pre><p>这里需要说一下，我之前习惯的都是selenium的爬虫，但是这次不顶用了，所以换了个方式。但是etree搞下来的数据，它的xpath竟然需要精确到<kbd>text()</kbd>，这是我没有想到的。按selenium的经验，我只把路径写到了<kbd>h1</kbd>，然后怎么搞都拿不到文本，后来查了博文才知道<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>。这个事情说来神奇，当时查的其他博文<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，都写着可以到<kbd>h1</kbd>，然后用<kbd>i.text</kbd>拿到文本，但我这里就是不行。另外，这里还需要注意编码的问题，我调成<kbd>htmlT.encoding = 'utf-8'</kbd>才正常，主要参考了博文<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>。</p><p>至于selenium为啥不行呢，我感觉根本原因是我太菜了不会用，直接原因如下：</p><pre><code class="python"># from selenium import webdriver
#
# driver_path = "D:\Software\Anaconda3\msedgedriver.exe"
# # msedgedriver.exe 的路径
#
# browser = webdriver.Edge(executable_path=driver_path)
# # 打开浏览器
#
# browser.get(dataM0.iloc[0,0])
# # 打开网页
#
# rowCount = browser.find_element_by_xpath("/html/body/div[12]/div[1]/div[1]/h1")
# # 选择表格
#
# # /html/body/div[13]/div[1]/div[1]/h1
# # /html/body/div[12]/div[1]/div[1]/h1
# # 在第一次爬取的时候div是12，第二次打开网页就是13
# # 可以每次循环都新打开浏览器，每次循环关闭浏览器
# # 这样似乎有点浪费资源，放弃selenium</code></pre><p>接着处理一下keywords。访问了一下原网页发现，原网页当中没有keywords，看来需要自己动手。考虑用TF-IDF，使用jieba库<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>。这里有一个神奇的点，本来是打算从brief当中抽取关键词，毕竟文本信息更多。但brief里面竟然不仅仅有这条新闻的消息，还有其他的诸如推荐的新闻等的文本...于是放弃brief，改用标题。</p><pre><code class="python">dataM0 = dataI0.loc[dataI0['keywords'].isnull(),['标题','keywords']].reset_index(drop=True)
# 抽取keyword缺失的行

from jieba.analyse import extract_tags
# from jieba.analyse import textrank

# for keyword, weight in extract_tags(dataM0.iloc[0,0], withWeight=True):
#     print('%s %s' % (keyword, weight))
# for keyword, weight in textrank(dataM0.iloc[0,0], withWeight=True):
#     print('%s %s' % (keyword, weight))

for i in range(0,dataM0.shape[0]):
    print(i, '-' * 10)
    # 打印序号
    keyW = extract_tags(dataM0.iloc[i,0])
    dataM0.iloc[i, 1] = ','.join(keyW[0:3])
    # 对关键词进行拼接
    print(dataM0.iloc[i, 1])


dataI0.loc[dataI0['keywords'].isnull(),'keywords'] = dataM0['keywords'].values
# 直接使用数据框赋值不可取
# 应当将取values后再赋值
# 因为数据框之间的赋值好像需要看index</code></pre><p>然后发布时间的话，可以直接从网址里面提取，用<kbd>split</kbd>进行文本处理就好</p><p>brief缺失的话，直接丢掉那些行吧，我印象缺失的行不到1%</p><p>下面开始进行可视化分析吧，好像也没其他预处理了</p><p>首先想做一个按天为单位的新闻发布量时序图，然后就出问题了。天数太多，坐标轴密密麻麻地重叠，这不行啊。查了查资料<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，应当将坐标轴刻度间隔调大。</p><pre><code class="python">import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator

countN = dataI0['发布时间'].value_counts()
countN.sort_index(inplace=True)
sns.lineplot(countN.index,countN.values)
x_major_locator=MultipleLocator(100)
#把x轴的刻度间隔设置为100，并存在变量里
ax=plt.gca()
#ax为两条坐标轴的实例
ax.xaxis.set_major_locator(x_major_locator)
#把x轴的主刻度设置为10的倍数
plt.show()
# 按天统计的发布趋势图</code></pre><p>其他的图，大概如下：</p><ul><li>新闻类别统计条形图</li><li>各类新闻在月的粒度上发布量时序图</li><li>发布量最大那天的新闻类别条形图</li><li>发布量最大那天的新闻类别饼图</li><li>根据keyword绘制词云</li></ul><p>贴一下部分代码吧</p><pre><code class="python"># 21年4月20日的新闻类别饼图
plt.pie(countN.values,startangle = 90,counterclock = False)
plt.legend(countN.index, loc=0)
plt.show()

# 按照关键词绘制词云
from wordcloud import WordCloud
#用来正常显示中文
plt.rcParams["font.sans-serif"]=["SimHei"]

wordK = dataI0['keywords'].values
wordK = ','.join(wordK)
wordK = wordK.replace(',',' ')

from PIL import Image
import numpy as np

mask_pic = Image.open("pic.jpg")
mask_pic_array = np.array(mask_pic)

wc=WordCloud(
    font_path='C:\\Windows\\Font\\simkai.ttf',
    background_color="white",
    mask = mask_pic_array)
wc.generate(wordK)
plt.imshow(wc,interpolation="bilinear")
plt.axis("off")
plt.show()
# 可惜并没有按照图片显示出来</code></pre><p>噢，对了，这个地方还需要调一下设置，要不然中文会乱码</p><pre><code class="python">plt.rcParams['font.sans-serif'] = ['SimHei'] #用来显示中文标签
plt.rcParams['axes.unicode_minus'] = False #用来正常显示负号</code></pre><p>以上，大概就是我提交的报名作品，然后就没过...嗯，只能去玩儿别的了</p><p>在以上的基础上，还想过做一个情感分析，等于说每一条新闻增加一列数据，然后再进行一些分析。或者是做<kbd>embedding</kbd>去训练一个新闻分类的模型，但意义不大，数据量太小。</p><p>这份报告大概是从当天中文的<kbd>12：53</kbd>开始做，然后在截止时间<kbd>20：00</kbd>的<kbd>19：40</kbd>完成，然后检查检查改了几个小错误，还是准备的不够啊。据说这个报名从<kbd>06月</kbd>初就开始了，到<kbd>07月10日</kbd>左右结束...而我是当天才开始做的。</p><p>然后关于狗熊会的培训和选拔也有点迷惑。怎么说呢，就是，人才计划貌似是培养数据分析能力的，是先想选题再去找数据？大概吧，我感觉选题在前。然后此次选拔呢，说的是，“根据附件提供的数据（news.csv），自行确定选题，完成一份数据分析报告。”，就有种数据在选题之前的感觉。当然事实上，数据肯定在选题之前，我们必然是要根据数据做选题的。但是，真实情况下的数据远比选拔附件提供的数据丰富详实，从而能做更多的分析。或许考核的也有其他的一些考虑，例如对数据集的补充能力？先确定选题，然后自己去爬新闻数据，大大扩充数据集，然后再做分析？可能是我的思路太过于局限所给的数据集了，当然也有时间不足的原因。</p><p>再贴一遍数据集的描述：</p><pre><code class="python"># &lt;class 'pandas.core.frame.DataFrame'&gt;
# RangeIndex: 4492 entries, 0 to 4491
# Data columns (total 7 columns):
#  #   Column    Non-Null Count  Dtype
# ---  ------    --------------  -----
#  0   标题        4417 non-null   object
#  1   标题链接      4492 non-null   object
#  2   brief     4483 non-null   object
#  3   keywords  3272 non-null   object
#  4   发布时间      3817 non-null   object
#  5   新闻类别      4492 non-null   object
#  6   采集时的时间    4492 non-null   object
# dtypes: object(7)
# memory usage: 245.8+ KB</code></pre><p>纵观全局，我好像只做了新闻类别、发布时间这两个特征的分析。拿标题抽取了keyword，然后做了词云，这俩特征就没用了。标题链接？作为标识去了个重，然后拿它补充了标题，没了。采集时间？好像没啥思路。主要是感觉没意义吧，还要对采集人的行为做一做分析？或许选拔也有这方面的考虑吧，时间紧，没有过多思考。</p><p>此外，“人才计划的选拔，考察个人能力，而非团队合作能力。”，嗯，考察对指定数据集的分析能力？如果是这样就好了。我提交的报告属于探索性数据分析，开始做的时候是没有选题的，但是探索之后有结论，极其囿于数据集本身。说起来，这选拔好像考核的是传统的数据分析能力啊，要选题要汇报的那种，是带着目的去分析数据....而我提交的是探索性数据分析报告，凉凉。其实一开始也想到了这个问题，但是想了想，这能选啥题？带着啥目的去分析这个数据集？发布趋势？那不一张时序图就搞定了？类别分析？嗯...好像可以，哪个时段哪个类别发的最多，最多的类别又是怎样的发布趋势。但还是感觉不够，太少了，两张图就搞定。事实上，我在探索性数据分析的报告中，做的也就是发布趋势分析和类别分析。但是不是以明确选题的方式，而是混杂地统一扔进了探索性数据分析。或者应该叫数据挖掘，这或许就是区别了，某种程度上数据为先和选题为先的差别。</p><div class="footnotes"><hr/><ol><li id="fn-1">[[python+pandas]数据预处理-时间格式转换](<a href="https://link.segmentfault.com/?enc=44NKDSS33DznB68TSl2kKA%3D%3D.T8RbGfLQzHsznwp9n0kudm1Yj%2B803ngmFF%2Fqtt3VZ8kTEmIVN1%2FFLCnZjQGH%2B6H1b82i9gyZ%2F0NR94CEzRARPA%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/Apuls1/article/details/108784021</a>)  <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"><a href="https://link.segmentfault.com/?enc=zwKuaPCIPr51Zk5H1Avdtg%3D%3D.NMM7FJvfvRfrJ5%2BB1PUN3DNgcU8imaAFKlZkEfpHMwJFXry%2Fz6zUqhD4M%2By08Xq%2F6aE3RH%2FnwMcmS1DUbHlBGA%3D%3D" rel="nofollow" target="_blank">爬虫解析库之xpath、Beautifulsoup的对比使用</a>  <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"><a href="https://link.segmentfault.com/?enc=gCCQsw2C7yoP9ZbD6wu87w%3D%3D.qsQC%2FVqouhJEoWebuVwCwu%2BI4zCHSdVRk4%2FFgTUgviR%2FyQcgqaepzUI1jVEAVo6NqBD0TY8DhF0TQlOrlxg6Yg%3D%3D" rel="nofollow" target="_blank">Python爬虫之xpath的详细使用（爬虫）</a>  <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"><a href="https://link.segmentfault.com/?enc=qb8eDR6fJtAx7r9EOpK5Hg%3D%3D.EAGNCPIeg7mVk3ZQKf2NMs2%2F9kwa8WVpkm%2FhDzci1%2FBEE3QdRGd%2FK%2BEzUsrRNBYf" rel="nofollow" target="_blank">爬取知乎热榜标题和连接 （python，requests，xpath）</a>  <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"><a href="https://link.segmentfault.com/?enc=kvul8SMYr5laYJH5EVSOqw%3D%3D.881K2isOnwAtG3DSJMUq8x0H7xxR3WqPrICvc55971b8pN4G6xjXAM5LV1wJ%2BwQ4" rel="nofollow" target="_blank">Python request中文乱码问题解决方案</a>  <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"><a href="https://link.segmentfault.com/?enc=u6gleIP1Rwm635i2FWe4Bw%3D%3D.KTmaetnsws3sCo79kG3rL%2FOIH8WfE73vf3SEOJiHwyvBXtGNPoUe6ZaGnmy6t4YH" rel="nofollow" target="_blank">如何用Python提取中文关键词？</a>  <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"><a href="https://link.segmentfault.com/?enc=Zh%2BGBhLQ3MDduuqhwogScQ%3D%3D.dcEasM7HWbD%2BJ9T455LuIspBGLTzrZkfoL1kYVAc424Ng6C7dEtq%2BJo6zOe758hR" rel="nofollow" target="_blank">Python设置matplotlib.plot的坐标轴刻度间隔以及刻度范围</a>  <a href="#fnref-7" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[你敢在周五下午发布代码吗？只有写单元测试的人才配有的“松弛感” HuiZhu ]]></title>    <link>https://segmentfault.com/a/1190000047494131</link>    <guid>https://segmentfault.com/a/1190000047494131</guid>    <pubDate>2025-12-22 18:04:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>“改一行代码，崩整个系统。”</p><p>这句听起来像段子的玩笑，却是无数开发者心中真实的恐惧。</p><p>问你一个扎心的问题：<strong>如果现在让你重构核心业务里的那个 <code>calculatePrice</code> 函数，你敢立马点上线吗？</strong></p><p>大多数人的回答是沉默。因为我们心里没底。我们写的代码就像没有地基的房子，看着光鲜，实则摇摇欲坠。一旦需要修改，就像是在玩叠叠乐，生怕抽错一块木条，整个大厦瞬间坍塌。</p><p>这种恐惧的根源，就是<strong>缺乏单元测试</strong>。我们都知道测试重要，但我们更知道写测试有多痛苦：</p><ul><li><strong>枯燥</strong>：构造数据、Mock接口、写断言，比写业务逻辑还累。</li><li><strong>耗时</strong>：写功能1小时，写测试3小时，老板还催着上线。</li><li><strong>难维护</strong>：业务变了，测试红一片，还得回头修测试。</li></ul><p>于是，我们选择了“裸奔”。</p><p>但今天，我想给你一个拒绝“裸奔”的理由，以及一件<strong>防弹衣</strong>。有了它，你不再需要在那枯燥的断言中消耗生命。你只需要把代码丢给AI，它就能还你一套覆盖率100%的测试用例。</p><p>这不是偷懒，这是把你的脑力从“重复劳动”中解放出来，去思考更有价值的架构设计。</p><h2>为什么你需要这位“AI质检员”？</h2><p>在传统的开发流程中，单元测试往往是“二等公民”。但在AI时代，它应该是你的“贴身保镖”。</p><p>这套<strong>单元测试生成AI指令</strong>，不仅仅是帮你生成几行 <code>assert</code>。它是一位深谙 <strong>TDD（测试驱动开发）</strong> 和 <strong>代码质量</strong> 的老练工程师。</p><p>它能做到你懒得做的事：</p><ol><li><strong>穷举边界</strong>：你只想到了正常输入，它想到了空值、负数、超长字符串。</li><li><strong>隔离依赖</strong>：你嫌Mock麻烦，它自动帮你把数据库和API请求都Mock好。</li><li><strong>规范命名</strong>：你的测试叫 <code>test1</code>，它的测试叫 <code>test_invalid_email_returns_false</code>。</li></ol><h2>核心指令：让AI以此为生</h2><p>这套指令经过精细打磨，融合了业界标准的测试方法论。它不玩虚的，直接输出可运行、高质量的测试代码。</p><h3>🧬 单元测试生成AI提示词</h3><pre><code class="markdown"># 角色定义
你是一位资深的测试开发工程师，拥有10年以上的软件测试经验，精通各类单元测试框架（如JUnit、pytest、Jest、Mocha、NUnit等）和测试方法论（TDD、BDD）。你深谙代码质量保证的最佳实践，能够针对各种编程语言和业务场景，设计出高效、全面、可维护的单元测试用例。

# 任务描述
请为以下代码生成完整的单元测试用例，确保测试覆盖全面、结构清晰、易于维护，帮助开发者提高代码质量和系统可靠性。

**输入信息**:
- **待测代码**: [粘贴需要测试的代码]
- **编程语言**: [如: Python/Java/JavaScript/TypeScript/C#/Go等]
- **测试框架**: [如: pytest/JUnit/Jest/Mocha/NUnit等，可选，AI可根据语言推荐]
- **业务背景**: [简要说明代码的业务功能，可选]
- **特殊要求**: [如: 需要Mock外部依赖、性能测试、边界测试等，可选]

# 输出要求

## 1. 测试代码结构
- **测试文件头部**: 必要的导入语句和测试配置
- **测试类/模块组织**: 按被测功能合理分组
- **测试方法命名**: 采用清晰的命名规范（如: test_功能_场景_预期结果）
- **测试数据准备**: 合理的setUp/tearDown或fixture设计
- **断言语句**: 明确的预期结果验证

## 2. 测试覆盖维度
- **正常路径测试**: 验证预期输入的正确输出
- **边界条件测试**: 极值、空值、临界值测试
- **异常处理测试**: 错误输入、异常抛出验证
- **参数化测试**: 多组输入数据的批量验证（如适用）
- **Mock/Stub测试**: 外部依赖的隔离测试（如适用）

## 3. 质量标准
- **覆盖率**: 力争达到核心逻辑80%以上的分支覆盖
- **独立性**: 每个测试用例相互独立，无依赖顺序
- **可读性**: 测试意图清晰，便于理解和维护
- **可重复性**: 测试结果稳定，多次运行结果一致
- **执行效率**: 测试运行快速，避免不必要的等待

## 4. 格式要求
- 输出完整可运行的测试代码
- 每个测试方法添加简要注释说明测试目的
- 提供测试执行命令
- 如有Mock需求，提供Mock配置代码

## 5. 风格约束
- **代码风格**: 遵循对应语言的编码规范（如PEP8、Google Style等）
- **注释语言**: 中文注释说明测试意图
- **专业程度**: 适合中级开发者阅读和维护

# 质量检查清单

在完成输出后，请自我检查:
- [ ] 测试用例是否覆盖了所有公共方法
- [ ] 是否包含正常路径和异常路径测试
- [ ] 边界条件是否得到充分验证
- [ ] 测试命名是否清晰表达测试意图
- [ ] Mock/Stub使用是否合理
- [ ] 测试代码是否可以直接运行
- [ ] 是否提供了测试执行说明

# 注意事项
- 不要测试语言内置功能或第三方库的正确性
- 避免测试私有方法（除非有特殊需求）
- 测试数据应具有代表性，避免过于简单或过于复杂
- 对于有外部依赖的代码，优先使用Mock隔离
- 异步代码需要使用对应的异步测试方法

# 输出格式
请按以下顺序输出:
1. 📊 **测试策略概述**: 简要说明测试设计思路
2. 📝 **完整测试代码**: 可直接运行的测试文件
3. 🔧 **执行说明**: 测试运行命令和依赖安装
4. 📈 **覆盖率分析**: 测试覆盖的功能点清单
5. 💡 **优化建议**: 代码质量或可测试性改进建议（如有）</code></pre><h2>实战：从“不敢动”到“随便改”</h2><p>口说无凭，我们来看一个真实的<strong>Python</strong>案例。</p><p><strong>场景</strong>：你写了一个邮箱验证函数 <code>validate_email</code>，逻辑看起来很简单：要有 <code>@</code>，要有 <code>.</code>，不能为空。</p><p>但是，当你把这段代码交给AI，并使用上述指令时，它不仅测试了 <code>user@example.com</code>（正常路径），还狠狠地“刁难”了你的代码：</p><ul><li><strong>边界测试</strong>：<code>a@b.c</code>（最短有效邮箱）</li><li><strong>异常测试</strong>：传入 <code>None</code> 或 <code>123</code>（非字符串输入）</li><li><strong>特殊字符</strong>：<code>user.name+tag@example.com</code>（合法但少见）</li></ul><p>AI生成的测试代码会包含这样的<strong>参数化测试</strong>，把所有可能性一网打尽：</p><pre><code class="python">    @pytest.mark.parametrize("invalid_email", [
        "plainaddress",
        "@missingusername.com",
        "username@.com",
        "username@com",
        "username@-example.com",
    ])
    def test_various_invalid_emails(self, invalid_email):
        """参数化测试: 多种无效邮箱格式"""
        assert validate_email(invalid_email) is False</code></pre><p>这就是<strong>专业</strong>。它替你考虑了那些你可能要在半夜两点修Bug时才会想到的情况。</p><h2>3个让AI写好测试的“骚操作”</h2><ol><li><strong>Mock一切外部依赖</strong>：<br/>告诉AI：“这个函数调用了数据库，请用 <code>unittest.mock</code> 把 <code>db.query</code> 隔离掉。” 这样你的单元测试就不需要连真实的数据库，跑得飞快。</li><li><strong>针对遗留代码</strong>：<br/>对于那些没人敢动的“祖传代码”，你可以把代码贴给AI，然后说：“请生成一组<strong>特征测试（Characterization Test）</strong>，记录它现在的行为。” 这样你就得到了一张安全网，保证重构时不会破坏现有逻辑。</li><li><strong>测试驱动修复（TDD）</strong>：<br/>发现Bug了？先把复现Bug的条件告诉AI，让它生成一个<strong>会失败的测试用例</strong>。然后你再去修代码，直到测试变绿。这才是修复Bug的正确姿势。</li></ol><h2>写在最后</h2><p>单元测试，是工程师给自己的一份<strong>职业保险</strong>。</p><p>它让你在面对复杂的业务变更时，依然能保持从容；它让你在周五下午发布代码时，依然能期待一个愉快的周末。</p><p>不要让“没时间”成为借口。有了这套指令，你离“代码自信”只差一次复制粘贴。</p><p>现在，去给你的核心代码穿上这件“防弹衣”吧。<strong>愿你的控制台，永远是一片生机盎然的绿色。</strong></p>]]></description></item><item>    <title><![CDATA[喜报｜枫清科技AI4S智能体荣获InfoQ2025中国技术力量榜单「AI Agent最具生产力产品」]]></title>    <link>https://segmentfault.com/a/1190000047494136</link>    <guid>https://segmentfault.com/a/1190000047494136</guid>    <pubDate>2025-12-22 18:03:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494138" alt="图片" title="图片"/></p><p>12月19日，InfoQ 极客传媒携手模力工场评选的“2025中国技术力量榜单”（下称“技术榜单”）正式揭晓。枫清科技（Fabarta）“AI4S通用智能体和场景智能体” 凭借自主研发的AI智能体技术与科研场景的商业化落地能力，成功入选该榜单“AI Agent最具生产力产品”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047494139" alt="图片" title="图片" loading="lazy"/></p><p>枫清科技依托在AI4S（AI For Science）科研平台建设与智能体技术研发方面的长期积累，构建了以“通用智能体 + 场景智能体”为核心的双轮驱动科研赋能体系，覆盖从文献理解、知识挖掘到实验设计与执行的科研全流程，可有效提升科研效率、降低试错成本，加速科研成果的产出与转化。</p><h3>通用智能体：面向科研共性需求的效率引擎</h3><p>AI4S通用智能体聚焦科研活动中的高频共性场景，覆盖文献处理、数据解析与知识挖掘等关键环节，系统性缓解科研人员在“信息过载”和“处理效率不足”方面的核心痛点，主要包括：</p><ul><li>文献智能处理智能体<br/>支持科研论文与文献的智能问答、多语言翻译、摘要生成等功能。该智能体基于深度科研语义理解能力，可精准解析PDF、Word 等多种格式文献，自动提取专业术语、核心观点及关键信息，显著提升文献研读与知识获取效率。</li><li>专利深度解析智能体<br/>依托专利检索与语义理解技术，自动提炼专利的技术问题、技术效果、创新点及权利要求内容，支持技术演进路径分析与自由问答。通过智能体能力实现自动化专利检索、上位词/下位词生成及专利精准度分析，并结合工作流引擎与本地知识库，实现从专利检索到内容总结的全链路自动化，大幅提升专利分析效率。</li><li>科研报告生成智能体<br/>内置多类型科研报告模板，可根据结构化或非结构化数据自动生成规范化科研报告；同时支持从用户已有报告中抽取模板结构，结合多源数据内容，实现科研报告的自动化生成与复用。</li></ul><h3>场景智能体：面向垂直领域的科研能力放大器</h3><p>AI4S场景智能体聚焦化工新材料、生物医药等专业领域，通过“行业知识体系 + 智能体技术”的深度融合，解决复杂实验设计与科研任务执行中的关键难题，典型应用场景包括：</p><ul><li>科研实验设计智能体<br/>面向不同科研实验场景，内置实验设计规则与领域知识体系，结合具体科研目标生成初步实验方案。该智能体通过交互式界面支持科研人员与智能体协同工作，对实验方案进行迭代优化与定制调整，提高实验设计的科学性与可执行性。</li><li>科研任务执行智能体<br/>针对科学实验中的复杂任务执行场景，支持实验建议生成、任务调度与长耗时任务管理（如分子动力学模拟等）。通过异步通信与状态持久化机制，该智能体突破传统智能体在长周期任务中的性能瓶颈，显著提升科研任务的执行效率与稳定性。</li></ul><p>枫清AI4S 智能体体系采用“智能体 + 工作流”协同架构，融合大模型的语义理解能力与多模态处理技术，支持跨学科、跨领域科研文献与数据的深度解析；在交互层面，该方案突破传统单一“问答”模式，集成知识图谱可视化与分析组件，更贴近真实科研工作流程，为科研人员提供高效、直观、可持续演进的智能化科研支撑。</p><p>2025中国技术力量榜单以“洞察AI变革，见证智能未来”为核心，围绕AI基础设施、工程与部署、智能体生产力、行业应用、数据智能、AI Coding、具身智能与开源等八大方向展开评选。本届评选历经两个多月的案例征集与多轮评审，获得来自200多家企业与团队的申报，覆盖云计算厂商、AI 初创公司、行业龙头企业、开源社区、科研及创新团队，共计300多个案例进入初审环节。</p><p>参评此次技术榜单的项目共同构成了一幅极具代表性的中国AI全栈创新实践图谱。该榜单不仅是对过去一年企业与团队技术实力的阶段性“体检报告”，也是对产业未来演进方向的一次集中洞察。</p><p>此次获奖，彰显了枫清科技在技术前瞻性、场景落地价值、生产力提升等方面的关键突破。枫清科技坚持以知识引擎与大模型双轮驱动，已在科研、化工、金融、制造、能源等多个领域落地企业场景的智能体解决方案。未来，枫清科技将深耕产业智能化升级，打造AI时代数智融合新引擎。</p>]]></description></item><item>    <title><![CDATA[powshell的tree命令 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494143</link>    <guid>https://segmentfault.com/a/1190000047494143</guid>    <pubDate>2025-12-22 18:03:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在给项目写README的时候需要列一下目录，主要参考某博文<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup></p><p>这个命令可以在dos和powshell当中运行，效果如下:</p><pre><code>D:\Project\Studule\app\src\main\java&gt;tree /f
卷 Data 的文件夹 PATH 列表
卷序列号为 00690061 7876:8A9C
D:.
└─com
    └─stu
        └─studule
            ├─activity
            │      MainActivity.java
            │      OptionActivity.java
            │      SyllabusActivity.java
            │      UpdateCourseActivity.java
            │
            ├─adapter
            │      LessonAdapter.java
            │
            ├─dao
            │      CourseDao.java
            │
            ├─pojo
            │      Course.java
            │
            ├─remote
            │      MyRemoteAppWidget.java
            │      MyRemoteService.java
            │      MyRemoteViewsFactory.java
            │
            ├─util
            │      JsonDataUtil.java
            │
            └─view
                    RoundTextView.java
                    TimeTableView.java
                    
PS D:\Project\Studule\app\src\main\java&gt; tree /f /a
卷 Data 的文件夹 PATH 列表
卷序列号为 7876-8A9C
D:.
\---com
    \---stu
        \---studule
            +---activity
            |       MainActivity.java
            |       OptionActivity.java
            |       SyllabusActivity.java
            |       UpdateCourseActivity.java
            |
            +---adapter
            |       LessonAdapter.java
            |
            +---dao
            |       CourseDao.java
            |
            +---pojo
            |       Course.java
            |
            +---remote
            |       MyRemoteAppWidget.java
            |       MyRemoteService.java
            |       MyRemoteViewsFactory.java
            |
            +---util
            |       JsonDataUtil.java
            |
            \---view
                    RoundTextView.java
                    TimeTableView.java</code></pre><p>但是在dos窗口当中，<kbd>tree</kbd>、<kbd>tree /f</kbd>都会有一些字符重叠，需要加上<kbd>/a</kbd>才能清晰显示。不过不影响复制，粘贴到markdown当中还是正常的。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=Cns08Fz%2Fco56ARMzeyZ81Q%3D%3D.8S%2BiMUVYFr8cgIkLWpPOjk%2FTGHALqLa%2FprFUetopZchBFLJc421slinQI2zxkNPJEKMicYiMLrjNqpnIL6DX1WjkFqwEwP%2B1DpzQ95IXeow%3D" rel="nofollow" target="_blank">Powershell-查询当前文件目录层级结构</a> <a href="#fnref-1" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[中奖]第九届“泰迪杯”挑战赛A题 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494148</link>    <guid>https://segmentfault.com/a/1190000047494148</guid>    <pubDate>2025-12-22 18:02:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h4>问题概述</h4><p>题目<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>如下：</p><p><img width="617" height="628" referrerpolicy="no-referrer" src="/img/bVdnryN" alt="image.png" title="image.png"/></p><p>赛题有2个点，分别是：</p><ul><li><p>确定数据指标</p><p>即确定哪些特征是决定财务造假与否的关键特征</p></li><li><p>预测造假公司</p><p>训练模型，然后跑测试数据即可</p></li></ul><h4>预处理</h4><ul><li>首先使用missingno<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，对全局数据进行观测，看一看缺失值等情况</li><li>然后删去无用的特征列</li><li>删去缺失值占比过多的特征列</li><li>使用pd.interpolate()<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>对缺失值占比较小的特征列进行补充，也可以参考<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></li></ul><h4>第一题</h4><p>最开始的想法是直接跑树模型，然后看看谁的权重大就选谁，然而问题出在样本比例上。</p><ul><li>首先对整体来看，正样本的数量远远大于负样本。不均衡的情况下，树模型虽然有所缓解，但估计还是够呛</li><li>更惨的是，第一题要求的是各行业的财务造假关键指标。数据一共几十个行业，有些行业没有造假，全是正样本。这样的情况无法用树模型处理，其他模型也不行。</li></ul><p>后来查到了一个方法，Null Importances<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup><sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup><sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup>。</p><p>其思路大概是，先用正确的标签计算一下各个特征对于分类的重要性，然后打乱标签，再计算特征的重要性。如果一个特征真的对分类有用，那么他应该在真实的标签下展示高重要性，而在错误的标签下展示低重要性。</p><p>对于第一题而言，我们分两类情况来考虑：</p><ul><li><p>对于整个行业没有造假记录的数据来说：</p><p>先对各个数值特征（好像所有的特征都是数值特征？）计算方差，取方差较小的特征为重要特征。因为该行业没有造假，所以其与造假相关的特征应当表现出聚集的趋势，即都没有造假，也就是方差较小的特征。然后随机赋予标签，计算其互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>（mutual_info_classif）。然后用前一个的特征集合减去后一个的特征集合，留下的即为关键特征。</p></li><li><p>对于整个行业有造假记录的数据来说</p><p>先对正确的标签计算互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>，然后随机赋予标签，再计算其互信息<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup>。取两者的差集为关键特征。</p></li></ul><h4>第二题</h4><p>这一题将数据分成了2个行业，制造业和非制造业。虽然样本还是很不均衡，但至少，正负样本都有。</p><ul><li>首先进行特征选择，使用LinearSVC<sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup></li><li>然后将数据丢进模型训练，并使用网格调参<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup></li><li>最后走一遍stacking<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup><sup id="fnref-13"><a href="#fn-13" class="footnote-ref">13</a></sup></li></ul><h4>后记</h4><p>其实模型训练的结果并不乐观，因为样本分布的不均衡。后来有一些其他想法：</p><ul><li>在模型融合的时候，加大树模型的权重，因为树模型对分布不均衡有所缓解</li><li>使用一些其他方法补充数据，例如SMOTE等<sup id="fnref-14"><a href="#fn-14" class="footnote-ref">14</a></sup></li></ul><p>啊，对了，我当时参考博文<sup id="fnref-15"><a href="#fn-15" class="footnote-ref">15</a></sup>，用pandas-profiling还跑崩了<sup id="fnref-16"><a href="#fn-16" class="footnote-ref">16</a></sup>，数据太多。</p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=nUhxVDoR3dybcJ6%2FfeyFQg%3D%3D.yy7S7HkoBsEtNR9G%2FF17f9jxYMNeYD%2B8vo3VSkg%2FXCarcScM6ZJc%2BYOFwinJ9%2B2ZzCh8D4amXX4VehwbHcDGdvdhMOWndxNGOMcUa1srna8%3D" rel="nofollow" target="_blank">第九届“泰迪杯”数据挖掘挑战赛</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=FMN8Ef4EYMUj9u%2BGc07Row%3D%3D.QkLqK1nXx6qtnOTQkRHjXlmkJBAYf3p4yOfTwLRETGVKBIhrNWv7kDffk7R1iW1FJeC0ccmYUYyc8vkL12coNw%3D%3D" rel="nofollow" target="_blank">数据探索分析之全局数据如何看？</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=aSPO32NrwaZlX4h8MXRn3g%3D%3D.O%2F%2BjvYXiRngqA8haG4YqeyRr8Db2w9tMB8YLwCvalL6L%2FH2Q2%2FgTYdF15mZKw5yJnix5acxy7qZIfsHOfqBhWQ%3D%3D" rel="nofollow" target="_blank">数据分析之Pandas缺失数据处理</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=hep57bXDYoY8FCW32TCiSQ%3D%3D.f2SV6wNKo54PaPXIvhsBqR4d8%2FhboIHjJmz7BSvs%2F%2Brru7PfXh6r7D4ZNw8aR9KhS%2FAh68nv%2Bi4Ow1RsV%2BYPVg%3D%3D" rel="nofollow" target="_blank">独家 | 在机器学习中利用统计插补来处理缺失值（附代码）</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=iXLGkpt6oQXDjEjFyTNOjw%3D%3D.LkXqB40O0uONXna0maBGoZS2EJUpI%2BLRZqNq3IpD2Qi8ipY%2Fg8WAuvK%2BB%2ByIGc%2FOv90WrD8WVhP8E1y884B9WQ%3D%3D" rel="nofollow" target="_blank">Kaggle知识点：缺失值处理</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=KYgrU%2BjAHUMu2vvahkPUiw%3D%3D.bN7xIJ1yrmWxjLT7sfcmC2thDRZrA8LmPKUO0TaPWKfd%2BhDc7LmPs1giNkslCOKe5S4IZqajisULUmFjrUE1wImDLPAEByRKf5Fl085j1l8%3D" rel="nofollow" target="_blank">Feature Selection with Null Importances</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=6a%2FOsJ1n1kot78%2BFrDSLAQ%3D%3D.1oEHw%2Bo%2FYZzGETPBqDREiF4Au%2Bxzdbd7bBeFMvo8BsljD6gE9xLuqHg%2B18q3S%2FlMs22y2yYn7C6yi4i%2Fu6rv4Q%3D%3D" rel="nofollow" target="_blank">【数据挖掘比赛】之 Null Importances（特征选择）</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> <a href="https://link.segmentfault.com/?enc=0GZsupJ5mB1Dhbrg2bVCEw%3D%3D.y6FDR3yFcLN6ZvZPosKkCf7xlMD0zicbazJeUCqbvyEU1ZefCiWNF0b7NPUF2t3a" rel="nofollow" target="_blank">特征选择之tree的feature_importance的null importance part2</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"> <a href="https://link.segmentfault.com/?enc=LzH85Luvbu0Q80P0xAwnMg%3D%3D.8uOg5LRzzhOj2%2BdA%2FY7kj0XBj9qZyDPqkBfZ0qamEKBe%2B0QvMfjUfDDaaPE2ChBoi9qqMNky00Kvc9Uh4X0Jdw%3D%3D" rel="nofollow" target="_blank">知识点-如何使用互信息进行单变量特征筛选？</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"> <a href="https://link.segmentfault.com/?enc=S0NwBzL7d11hYy0j3Gbbaw%3D%3D.ZHBGMSuEWvko6%2BfY%2FPL7VRYv8LkDL%2FdtWKLOBym5PjsROM40Tzgt5QyuXtJYNWMzhLRlTwBSQ0HMxkmOOM%2FQWA%3D%3D" rel="nofollow" target="_blank">机器学习 特征选择（过滤法 封装法 嵌入法）</a> <a href="#fnref-10" class="footnote-backref">↩</a></li><li id="fn-11"> <a href="https://link.segmentfault.com/?enc=ELrqiAroFdhQhoBVz0I8Kg%3D%3D.4p3txB%2BSYwhYyrKkfFJQMn7JW0tqaxs7XUnn1T%2F5cwPSvnsL83sDayXV52vtEYWw" rel="nofollow" target="_blank">第八届“泰迪杯”挑战赛A题优秀论文——基于数据挖掘的上市公司高送转预测（1）</a> <a href="#fnref-11" class="footnote-backref">↩</a></li><li id="fn-12"> <a href="https://link.segmentfault.com/?enc=0anzrpYk%2BE4RHh2dtiULSg%3D%3D.OXi1HmL%2F5iJHysyv0hwSbd7BAcSmn5QI6N9FZyaONxcoqb9IO%2BUqrzmebjohPgU9Xqn8cDvtEJE%2F2blBz2OI4Q%3D%3D" rel="nofollow" target="_blank">集成学习中的 stacking 以及python实现</a> <a href="#fnref-12" class="footnote-backref">↩</a></li><li id="fn-13"> <a href="https://link.segmentfault.com/?enc=dgCK7ugU8pPnevZGuFdmbQ%3D%3D.slTMzLQcCAjDY%2BW1y%2BgHqCUtyPoJITMJ%2BtaQX7gsg4VVKnFbYRcs0zRYq3fUuNZOECWfSBm6cmlArSvOGrvWqQ%3D%3D" rel="nofollow" target="_blank">详解 Stacking 的 python 实现</a> <a href="#fnref-13" class="footnote-backref">↩</a></li><li id="fn-14"> <a href="https://link.segmentfault.com/?enc=acf%2FFxk5Jd3BUYCL9bcVQQ%3D%3D.6r0tIgol0QxnYaGdSKQNlyGsEsMCq3odKpiT2k6Sld4aPZR5H5DCKTFKN9VtQQlW" rel="nofollow" target="_blank">对"样本不均衡"一顿操作</a> <a href="#fnref-14" class="footnote-backref">↩</a></li><li id="fn-15"> <a href="https://link.segmentfault.com/?enc=NOdfxRzKu5r%2FS3c1jr8CJw%3D%3D.HdrEhfbBQCQ8%2FiSisAvLWs1Od%2FBSaFGeIqFGz7tLiNQBAKsw4VEsQSe8ceQ1ZrDRvol%2FH6%2FuaI7seNL2jtrkkg%3D%3D" rel="nofollow" target="_blank">2020泰迪杯数据挖掘挑战赛总结（A题）</a> <a href="#fnref-15" class="footnote-backref">↩</a></li><li id="fn-16"> [[未解决]pandas-profiling出现MemoryError](<a href="https://link.segmentfault.com/?enc=htob%2FHwgDGwPUuWJCnHdkw%3D%3D.eMO8EdFvPC%2B1gwF1zwwL8QGpSdsLloOooGGhq1kNh42vLlB6m8e%2F4vmpCNwDr%2Fr2Rn9tJkF8j6dN%2BBj93tPihQ%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/weixin_52202311/article/details/117090739</a>) <a href="#fnref-16" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]2021微信大数据挑战赛_方案 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494163</link>    <guid>https://segmentfault.com/a/1190000047494163</guid>    <pubDate>2025-12-22 18:01:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>@<a href="目录" target="_blank">TOC</a></p><h4>问题概述</h4><p>先来看看这冗长的赛题说明<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup><br/><img width="723" height="1780" referrerpolicy="no-referrer" src="/img/bVdnry0" alt="image.png" title="image.png"/></p><h4>baseline</h4><p>最早是参考<code>麻婆豆腐AI</code><sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>的baseline，跑了一遍。</p><p>这份代码用了<code>LabelEncoder</code>、<code>OneHotEncoder</code>，好像没做特征工程，直接丢进去了</p><p>后来在周周星的分享上找了一个baseline<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>，结构相对来说更清晰一点，就按照这个改进。</p><p>先聊一下这份代码的基础特征吧</p><p><img width="723" height="312" referrerpolicy="no-referrer" src="/img/bVdnry1" alt="image.png" title="image.png" loading="lazy"/><br/><img width="723" height="338" referrerpolicy="no-referrer" src="/img/bVdnry2" alt="image.png" title="image.png" loading="lazy"/></p><p>详细描述如下：</p><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>滑窗提取特征：</p><p>首先是滑窗提取特征，即过去N天内，看过多少视频、点赞多少视频等等</p><p>baseline当中Ｎ取的是5，后续参考其他文献等改为7，有提升</p><p>这里的时间粒度都是天，也就是说，如果用户在当天内重复观看某条视频，那他只有一条数据行，只是在<code>is_finish</code>列为<code>1</code>，而在<code>play_times</code>列可能是<code>2</code>或者其他，在<code>play</code>列的时长要超过视频的时长<code>videoplayseconds</code>。但如果用户在另一天重复观看此视频，则会是一条新的数据行。两种情况的数据行在<code>userid</code>、<code>feedid</code>、<code>authorid</code>等列都保持一致，但在<code>date_</code>列则是两个数值。下面的计不计人重复，一般都是对当天内的重复观看的情况而言的。</p><p>然后这些滑窗的特征如下：</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>曝光数据</p><ul><li><p>该用户看过多少个视频</p><p>在<code>天</code>的尺度内，计量单位是<code>个</code>，也就是说当用户在某天内观看某个视频两次，在这里仍旧被记作一次，但如果在另一天再次观看，则被记为两次，下同</p></li><li>该视频被多少个人播放</li><li>该作者被多少个人播放</li><li>该用户看过该作者多少个视频</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>转化数据</p><ul><li><p>该用户看完的视频个数在看过的视频个数当中的占比</p><p>即对标记是否完整观看的<code>is_finish</code>列进行求平均，即为完成率</p></li><li><p>该视频被看完的次数在被看到的次数当中的占比</p><p>此处同样不计入当天内的重复观看，同样对<code>is_finish</code>列进行求平均</p></li><li><p>该作者被看完的次数在被看到次数当中的占比</p><p>同上</p></li><li><p>在该用户观看过的该作者的视频当中，看完的百分比</p><p>同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p></li></ul><ul><li><p>对于每个用户而言</p><ul><li><p>对其在过去N天观看单个视频的次数，<code>play_times</code>列</p><p>此处计入当天内的重复观看的次数，即对同一个视频反复观看</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天对单个视频的观看时长，<code>play</code>列</p><p>此处计入当天内的重复观看的情况，即观看时长超出视频时长的情况</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天在单个视频的停留时长，<code>stay</code>列</p><ul><li>求平均</li><li>求最大</li></ul></li></ul></li><li><p>对于每个视频而言</p><ul><li><p>对其在过去N天被观看的次数，<code>play_times</code>列</p><p>此处计入当天内重复观看的情况，即对同一个视频反复观看</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天被观看的时长，<code>play</code>列</p><p>此处计入当天内重复观看的次数，即观看时长超出视频时长的情况</p><ul><li>求平均</li><li>求最大</li></ul></li><li><p>对其在过去N天的停留时长，<code>stay</code>列</p><ul><li>求平均</li><li>求最大</li></ul></li></ul></li><li><p>对于每个作者而言</p><p>同上</p></li><li><p>每个观看过的每个作者</p><p>同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>对每个用户而言，在过去N天内</p><ul><li><p>其点赞的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其查看评论的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其点击头像的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其转发的视频数量的</p><ul><li>求和</li><li>平均</li></ul></li></ul></li><li><p>对每个视频而言，在过去N天内</p><ul><li><p>其点赞的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其查看评论的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其点击头像的数量的</p><ul><li>求和</li><li>平均</li></ul></li><li><p>其被转发的数量的</p><ul><li>求和</li><li>平均</li></ul></li></ul></li><li><p>对每个作者而言，在过去N天内</p><p>同上</p></li><li><p>每个用户对每个作者，在过去N天内</p><p>同上</p></li></ul><p>++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li><p>全局提取特征</p><p>这里针对的是用户的所有历史数据。由于baseline当中在开头就把测试集和训练集放在一起做特征工程，所以这里其实统计的是过去15天的历史数据，即包含了将要预测的第15天的观看次数等。</p><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>曝光数据</p><p>此处计入不同日期的重复观看的情况</p><ul><li>用户一共看过几个视频</li><li>该视频一共被多少人看过</li><li>该作者的视频一共被多少个人看过</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>用户与视频</p><ul><li><p>对于每个用户而言，在所有历史数据当中，其看过几个视频</p><p>此处不计入不同日期的重复观看，在所有历史数据意义上的<code>个</code></p></li><li><p>对于每个视频而言，在所有历史数据当中，其被几个用户看过</p><p>此处同样不计入重复，同上</p></li></ul><p>用户与作者</p><ul><li><p>对于每个用户而言，在所有历史数据当中，其看过几个作者</p><p>此处不计入不同日期的重复观看，在所有历史数据意义上的<code>个</code></p></li><li><p>对于每个作者而言，在所有历史数据当中，其被几个用户看过</p><p>此处同样不计入重复，同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><p>用户与作者</p><ul><li><p>用户在某一天内看某个作者多少次</p><p>即按照<code>userid</code>和<code>authorid</code>进行<code>groupby</code>之后，对<code>date_</code>列进行计数。假设这种情况下，<code>10</code>在<code>date_</code>列出现两次，即意味着用户在第<code>10</code>天看过该用户<code>2</code>次。</p></li><li><p>用户看过的该作者作品在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>用户看过的该作者作品，在该用户看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p><ul><li>用户观看过视频的平均时长</li><li>作者创作视频的平均时长</li><li>作者创作过几个视频</li></ul><p>+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</p></li></ul><h4>改进-0</h4><p>其实这个不算改进吧，就使用<code>optuna</code><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup>对baseline进行了调参，大概上升了<code>0.03%</code>？</p><p>然后后面就没啥用了，可能是我代码的问题，做了新的特征之后调出来的参数竟然和之前一致，没有变化</p><h4>改进-1</h4><p>参考了文章<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup><sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup>，对特征进行改进</p><p>其实也就是在相应的<code>for</code>循环当中加入了新的特征列名</p><p>baseline当中，滑窗类的特征主要做了以下四个：</p><ul><li>用户</li><li>视频</li><li>作者</li><li>用户对作者</li></ul><p>在上面的基础上追加特征：</p><ul><li>背景音乐</li><li>背景音乐的歌手</li><li>用户对背景音乐</li><li>用户对背景音乐的歌手</li></ul><p>在全局特征当中追加：</p><p>+++++++++++++++++++++++++++++</p><p>曝光数据</p><ul><li>背景音乐被播放几次</li><li>歌手被播放几次</li></ul><p>+++++++++++++++++++++++++++++</p><p>与<code>用户与视频</code>和<code>用户与作者</code>相并列的特征</p><p>用户与音乐</p><ul><li>用户听过多少次这个音乐</li><li>这个音乐被多少个用户听过</li></ul><p>用户与歌手</p><ul><li>用户听过多少次这个歌手</li><li>歌手被多少个用户听过</li></ul><p>+++++++++++++++++++++++++++++</p><p>与第二个<code>用户与作者</code>相并列的特征</p><p>用户与音乐</p><ul><li>用户在某一天内看某个音乐多少次</li><li><p>用户看过的该音乐在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>用户看过的该音乐，在该用户看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>用户与歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><h4>改进-2</h4><p>加入<code>device</code>的特征，感觉这个没啥用</p><p>同时将全局特征的统计限定在前14天，也就是不统计测试集的观看数据</p><hr/><p>后来想想不太对，这个地方其实应该统计全部15天包含测试集的数据</p><p>是这样的，真实情况下，我们不知道第15天的行为对应的标签</p><p>也就是说，当我们去做推荐的时候，用户还没有对这个视频产生交互</p><p>但是在打比赛的时候不一样，打比赛的时候已经有标签了</p><p>那也就意味着，测试集当中的数据，应该是发生过的</p><p>是属于用户画像的一部分的</p><hr/><p>+++++++++++++++++++++++++++++</p><p>在时间滑窗当中加入：</p><ul><li>设备，<code>device</code></li><li>设备对作者</li><li>设备对歌曲</li><li>设备对歌手</li></ul><p>+++++++++++++++++++++++++++++</p><p>在曝光当中加入：</p><ul><li>设备，<code>device</code></li></ul><p>+++++++++++++++++++++++++++++</p><p>相应加入：</p><p>设备对视频</p><ul><li>该型号看过几个个视频</li><li>该视频被几个型号的看过</li></ul><p>设备对作者</p><ul><li>同上</li></ul><p>设备对歌曲</p><ul><li>同上</li></ul><p>设备对歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><p>设备与作者</p><ul><li>设备在某一天内看某个作者多少次</li><li><p>设备看过的该作者作品在该作者所有作品当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?</p></li><li><p>设备看过的该作者作品，在该设备看过所有视频当中的占比</p><p>此处分母上有<code>+1</code>，不要清楚有什么作用，为了防止分母出现<code>0</code>?同上</p></li></ul><p>设备与歌曲</p><ul><li>同上</li></ul><p>设备与歌手</p><ul><li>同上</li></ul><p>+++++++++++++++++++++++++++++</p><h4>改进-3</h4><p>加入<code>embedding</code></p><ul><li>使用<code>word2vec</code>对<code>keyword</code>和<code>tag</code>进行编码，生成<code>32</code>维的向量，作为视频的特征数据</li><li>使用<code>pca</code>将<code>feed_embeddings</code>降维到32</li><li>对以词为单位的<code>description</code>、<code>ocr</code>、<code>asr</code>使用<code>word2vec</code>生成<code>32</code>维的向量，并计算两两之间的差距</li></ul><h4>改进-4</h4><ul><li>对以字为单位的<code>description</code>、<code>ocr</code>、<code>asr</code>使用<code>word2vec</code>生成<code>32</code>维的向量，并计算两两之间的差距</li><li>尝试统计用户点赞过、评论等操作的视频抽取其特征，形成用户的兴趣特征。但没有成功</li></ul><h4>结果</h4><p>A榜当时好像0.659吧？我印象还在0.66挣扎，就差一点，B榜根本没提交</p><p>周周星<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup>也用单模的树模型，都能干到0.675</p><h4>参考</h4><ul><li><a href="https://link.segmentfault.com/?enc=0lalaLaX5uyHmXzb44jYsQ%3D%3D.38W%2B1qlsOteYEUvh6I7T5g6ELJa4%2FeuU%2FI8JSQ795BINMeGPRQK8D2n12Q%2F6xamFCoE18E2wwO9Pstf9m%2FKqRw%3D%3D" rel="nofollow" target="_blank">她来了她来了，乘风破浪的微信视频号推荐算法</a><sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup></li><li><a href="https://link.segmentfault.com/?enc=EMx2%2FQe0d6nDPKwoD5BtJQ%3D%3D.odEyLT3kB4TrMDaMsP%2BqQw9XjyCBgVyI%2Br6%2B6euuruCZ8r%2B7Svjo4dbQxHTLLcwFwj3YrOfGEiQteO91txHvFg%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法挑战赛baseline分享</a><sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup></li><li><a href="https://link.segmentfault.com/?enc=6phesTmbdbF3D6W%2F6ma6Vw%3D%3D.05vrg%2FsoMKCzjKqqCjwd%2FuAbBQIvCCJS%2FaCDll4NUqbNo7W5pHd0XYCNgcGt%2FLimWlgB1FF8gCfqjtvpimBfqQ%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛：LightGBM vs DeepFM</a><sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup></li></ul><hr/><h4>更新</h4><p>OTTO队伍的树模型方案，<a href="https://link.segmentfault.com/?enc=coUOJh%2FqafRnSoglDGBSaQ%3D%3D.hlxrkna3ZZZ5TT56WO8sq4EgXROt28fKHDFP7K8P2rdy3i%2FfFtl38i44sjtAxjPPx4x6R2Q%2BcYjHqCxA127XMA%3D%3D" rel="nofollow" target="_blank">https://github.com/juzstu/WBDC2021_Tree_Solution</a></p><p>江离的方案 ，<a href="https://link.segmentfault.com/?enc=sQp3LdN%2FDw%2FJ1fP%2FmKp1aA%3D%3D.GEEMcJyNoUEeTwfhelOfjpGkfPeYXV497%2BSoeicw7tsBN4e7le5lqhRDUH4HDLsM" rel="nofollow" target="_blank">https://github.com/ji1ai1/202105-WEIXIN</a></p><p>关于NetworkX，用图构建用户和视频的关系，做协同过滤</p><p><a href="https://link.segmentfault.com/?enc=en5VL2%2FnNjDS5kJKTm51PA%3D%3D.7FyN3TWCULTIZyw%2Bhpc1v6BOy40KPEgwhkekfwKCjr7fkF7F1LUpLyxihmOXIGpDqZ6jzMHv2BOjeA1GgMy4Nw%3D%3D" rel="nofollow" target="_blank">定量分析方法第17讲：NetworkX基础</a></p><hr/><p><a href="https://link.segmentfault.com/?enc=9w2mA%2BNmRApcWLqQwPy9pg%3D%3D.mOzs4tP7mywYBNbJx3PeEjCbtV0FZoN9Y3sNksw8UGNJhMB%2BRkMFThL8KwLgnXMP6%2FYeMbPKI41gyO1texJnYg%3D%3D" rel="nofollow" target="_blank">特征工程之tag-pooling，以微信大数据比赛数据为例</a></p><p>这个我好像用的是第二种方式，<code>word2vec</code>对每个视频的<code>tag</code>进行<code>embedding</code>之后求平均</p><p><a href="https://link.segmentfault.com/?enc=AoNGyPB3gydx8DolQ%2B1HsA%3D%3D.A6V2uZWajEfnD0EEWS2rlJ7za03KwOvRjNFu6QouENOmNxwYB9sx5QqBap1JTn5Xz%2Bd8PKhXp8XwVi6K%2Bv9SBQ%3D%3D" rel="nofollow" target="_blank">微信大数据竞赛Trick--如何3ID上0.706+</a></p><p>这里好像是按照视频对用户进行<code>groupby</code>，将每个视频的观看用户理解为文档，将视频列表理解为文档列表，对用户进行<code>TFIDF</code>编码，然后使用<code>TruncatedSVD</code>降维。</p><blockquote>该特征的本质就是希望找到基于视频的用户共现性，如果某些视频某些用户总是一起出现，那么就说明这些用户大概率有相同的爱好，就很可能一起转发，一起评论并且同时关注了。</blockquote><p>下面这两个也有使用这个方法</p><p><a href="https://link.segmentfault.com/?enc=GJINf6KLVYIMM3JQRyogTg%3D%3D.9a1qg5ptk%2BXFL0gLuu%2Bo2GZzJxbRuL3mFnEB46La3tCXHtU79ebCiAzBJ2%2BNZIokF8ygXwskw6dXxD8bBv%2BoaQ%3D%3D" rel="nofollow" target="_blank">中科大倪茹：感谢开源，我从入门竞赛到Top 10的经验分享</a></p><p><a href="https://link.segmentfault.com/?enc=T%2Ffrhnxf5FJ7PhcY5d4sMw%3D%3D.cSJExapk9fgV8q7sQw1YbUBke0Z60AFFsQHFkUL35mlAU3m11oJctQaeTtznUcPSQsb4UR8rPIB2%2BTJSlaIBFQ%3D%3D" rel="nofollow" target="_blank">2020DCIC智能算法赛-智慧海洋建设TOP1方案</a></p><div class="footnotes"><hr/><ol><li id="fn-1"> <a href="https://link.segmentfault.com/?enc=ptrbbP0lGQP6Mb5a8mKvsw%3D%3D.v%2Bx%2FqV28k4yqis2JP37H394QJBsApLe%2FZGVFh%2Bm7qLE%3D" rel="nofollow" target="_blank">微信大数据挑战赛</a> <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=%2Fi972vADzemz2UYOLNvEsg%3D%3D.KJVS8P2SoyYaZGCM7EHYLREpTrwuCRACyAgl5pS37DYJ9mfGYThIlvwUkgUv%2BPrZnECa4sUbsFCKMj6Yv%2F%2BHHg%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=TD3u%2FSbbMyYTLrygUsTlWA%3D%3D.hC0jrbgVYHDaKwGlGWWGWj94hGmonNzPp%2FvFSMPcyZBX4vrOHRbjhQiAVU71T92gx9SGwGGw%2F8ugzFhADKNS2JpHqqaNdVYF3ReHKpHEyOQRIzk3dnUh0IlxL0Cl%2BeLA" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=ibYG6S2vbZrmS%2FDlGcrIAA%3D%3D.Wy5W00HNfnzxxwE%2BIbDP60aCd%2F7Sx%2BlEpq0TeEyC1y5ArX8k6bOXzl%2Fu%2Bppv3bLqWyJfhwpyTTYHpf4OGLkZfQ%3D%3D" rel="nofollow" target="_blank">席卷Kaggle的调参神器，NN和树模型通吃！</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=P8rNZoOB42JZr1tI%2BEIiuQ%3D%3D.nFP%2BoTyhV2ZC69J66uviGwnEYNo7y9E93JN7DjggFBVOPPQf9BUIRgb3MU1jUxJbuIVD5mbDlK4FwZzTVMn9hQ%3D%3D" rel="nofollow" target="_blank">【机器学习】Optuna机器学习模型调参(LightGBM、XGBoost)</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=dwR%2FfgurjvHyuBh7TaPd5w%3D%3D.Y6txoUjrEMv7xKM7RfWXZQ1N7RgWvB429Uesj4Ln%2FK913E4vJ%2BQE6Yu5MbBq31JCAlP41M%2BkgkRntZtRklA48g%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法解题思路</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=YJip%2BVt3pajGLnr%2Fvm4fyQ%3D%3D.23ixoPtUuMNfIqQF0M57rZy1GdPDdZBpYXimWftnKJCREA8anL71zP1X2YsFCGkmh%2FO7Y3qseNRDhsgFZKvWFw%3D%3D" rel="nofollow" target="_blank">微信大数据比赛我用树模型怎么从514到671</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> <a href="https://link.segmentfault.com/?enc=SaNlllod4RbxoVc%2Fhk8bAg%3D%3D.HPCK3y%2FJilj2%2FP0%2BXotrhJG5jxshDEkR0P8L0JXix%2BM6ospuXNexw9%2BHIl%2B6EnijG1FCqH%2FPg76EX%2Byf6hYhaw%3D%3D" rel="nofollow" target="_blank">她来了她来了，乘风破浪的微信视频号推荐算法</a> <a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9"> <a href="https://link.segmentfault.com/?enc=0JM52a4ha1FBYF3Z4PuNdA%3D%3D.%2BmXX9LRDbbjhXN%2BwDkRIY%2B0jOjCoGJ39W%2FI5U9b1j4JKWzTWNQeupun4ggwEf8Q5VIB%2Ft1%2F1evNXVek8bDeHSg%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法挑战赛baseline分享</a> <a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10"> <a href="https://link.segmentfault.com/?enc=3UvWV%2BQ5ap0gPk8QDAV%2B8w%3D%3D.5s3vALEkswPHrsetGnMLPoi7FVrOoureD6Jk%2BFER%2FzAzUNml9r9Rkn4qfAuE71bPiWI69KtnM3CI4nTHBBHo4A%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛：LightGBM vs DeepFM</a> <a href="#fnref-10" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[[落选]2021微信大数据挑战赛_总结 JxWang05 ]]></title>    <link>https://segmentfault.com/a/1190000047494173</link>    <guid>https://segmentfault.com/a/1190000047494173</guid>    <pubDate>2025-12-22 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>确实很早就知道了这个比赛，但似乎一直没有开始。</p><p>之前也就刚打完一个泰迪杯，对于竞赛可以说是一无所知。</p><p>于是就等baseline，等到了麻婆豆腐<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup>的那个版本。</p><p>5月20日开赛，我跑baseline大概都快6月中旬。</p><p>那时候一边忙着期末的各种大项目，一边找竞赛的各种指导和资料。</p><p>跑完麻婆豆腐那个，还是一头雾水，不知道这到底是怎么操作的。</p><p>当时为了<code>LabelEncoder</code>、<code>OneHotEncoder</code>都看了好久</p><p>后来找到了天才儿童（好像是这个名字）的baseline<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup>，结构清晰</p><p>这里面的特征工程几乎都是由<code>for</code>循环控制的，增添特征只需要在循环里面加入列名</p><p>这份代码有用到滑窗法，其详细的信息参考文章<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup></p><p>真正开始做，大概都是6月25日， 先读代码</p><p>弄清楚了结构，就调参、加特征</p><p>A榜基本在常规特征里面折腾，好像稍微加了点<code>embedding</code></p><p>A榜结束的那天晚上我还在疯狂做特征，意图冲击B榜</p><p>然后就出问题了，特征太多，我那会儿都搞到了将近1000+</p><p>从周周星的分享<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>来看，似乎最多500+？</p><p>当时做到用户的兴趣特征，就是把用户看过的那些<code>tag</code>和<code>keyword</code>求平均</p><p>但是贪心，减了一部分，尝试，炸了，再减，再尝试</p><p>由于疯狂做出来的特征没经过验证，质量也不好，所以一致炸</p><p>这也就导致了....我B榜根本就没提交，因为没跑出来</p><p>血泪的教训啊...一定是做加法而不是减法</p><p>应当先跑个基础版本提交，说不好还能中奖</p><p>后来复盘的时候，觉得还是经验不足，好在此次有收获</p><p><strong>首先是，做加法，一定要按照顺序做</strong></p><p>先去挖基础特征，再去做复杂特征</p><p>可以参考这篇文章当中对于往瓶子里面加石头的比喻<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup></p><p><strong>然后，做出的特征最好离线保存</strong></p><p>一是方便后面可以直接载入，而不用再做特征</p><p>二是方便调整方案，对不同的特征进行组合验证</p><p>因为训练的时间可能很长，如果能节省做特征的时间，就能充分利用提交次数</p><p>关于特征的离线保存，CSV确实有点慢，可以换别的试试<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup></p><p>对于做完所有的那个好大的数据框</p><p>我当时尝试过<code>pickle</code>、<code>parquet</code>、<code>feather</code>，都炸了</p><p>不仅炸了，还浪费了时间，因为我是在B榜那天做的</p><p>猜测是数据规模的问题，可能他们无法处理？</p><p>但是也不对啊，<code>parquet</code>可是<code>Hadoop</code>那套系统里面的，例子<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup>也是<code>2.5GB</code></p><p>当然可能这个总的、所有特征的数据框远超<code>2.5GB</code></p><p>不太清楚发生了啥，我也很疑惑</p><p>后面可以试试分拆数据，然后用其他格式保存，提高<code>I/O</code></p><hr/><p>啊啊啊啊找到原因了！我在本地跑的时候报错，，缺少相关组件。果然还是不熟悉这些工具</p><p>这是<code>parquet</code></p><pre><code>ImportError: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.
A suitable version of pyarrow or fastparquet is required for parquet support.
Trying to import the above resulted in these errors:
- Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.
- Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.</code></pre><p>这是<code>feather</code></p><pre><code>ImportError: Missing optional dependency 'pyarrow'. Use pip or conda to install pyarrow.</code></pre><p><code>Pandas</code>的<code>Pickle</code>倒是可以直接用，其模块的后缀为<code>.pkl.gzip</code>，在这篇博文当中有说明，不同于<code>Pickle</code>原生格式的文件类型<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup></p><p>但如果是原生的<code>.pkl</code>的后缀，好像是可以直接用<code>Pickle</code>模块，而不用装别的库。</p><p><code>.pkl.gzip</code>的后缀还没有尝试过</p><hr/><p><a href="https://link.segmentfault.com/?enc=zYZf3V4pptZr%2BLw7sHVSXA%3D%3D.jnnKEbxZKNGvuC5Dtpt23OmsjMhcX5WUyvQCXR2IqJSVZJUc8Ch5t6YRPJJULlQ%2FUuz6oUWO5V5vvzgV%2B7mA%2Bg%3D%3D" rel="nofollow" target="_blank">做竞赛别用csv文件了。</a></p><p>这个测评看起来是<code>parquet</code>比较省内存，有压缩文件。不过单纯考虑速度的话，自带的<code>Pickle</code>就够用了。毕竟<code>parquet</code>还要装其他的库</p><hr/><p><strong>还有一个神奇的东西，<code>reduce_mem</code></strong></p><p>好像是对数据格式进行变换，反正很省内存</p><p>具体的代码可以参考<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup><sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup>，两份代码里面都有</p><p><strong>对了，LightGBM也有过一个报错</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=jrtc93jyQPrErDubKpLRMQ%3D%3D.%2BQ5TANZp2MFgJl9nUa06hJo3ivHRXqsOd9sxwGP5rqQPPpqKEB7peBmQO0K0V5HRPIMyDNg3rW06HXD8LjbjgA%3D%3D" rel="nofollow" target="_blank">LightGBM.cv时，feature_pre_filter和min_data_in_leaf相互矛盾</a></li><li><a href="https://link.segmentfault.com/?enc=Jl2GyFxOg%2Fz3g6TzFOpa4Q%3D%3D.NeErAXsXULE5CqoK8rcLM3wfuHDExihH6tUtI08cUEYcu2BWng4pTYQqWpDWforE" rel="nofollow" target="_blank">LightGBMTunerCV stepwise tuning got an error with lightgbm==3.0.0rc1 #1718</a></li></ul><p><strong>word2vec也有报错</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=bF6enjSABbQqRZJLIdmeLQ%3D%3D.HmAQAsEjU4NzqWwPOQNbvdv9VyNLfiGbTlJTWUywWgAxSxvac7qS4AmXoIPr3UxtS4CUnWk6ysP%2FS2dzjADYUw%3D%3D" rel="nofollow" target="_blank">Word2Vec TypeError: __init__() got an unexpected keyword argument ‘size‘</a></li></ul><p><strong>关于保存并实时查看训练过程</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=NzQ3%2Bw2HHfwxltXXi8Efwg%3D%3D.4GVCSZlglo9fExevlGXthcobAmTBAUmQ46DZ5N8HEDjXbDQXmSXuoJ5dILvjxgApvaeNW7pi%2BmFhpyP1F0VKYA%3D%3D" rel="nofollow" target="_blank">Linux保存并实时查看训练日志</a></li></ul><p><strong>这里也放一下相关的比赛的参考吧</strong></p><ul><li><a href="https://link.segmentfault.com/?enc=lqg9cvVt%2FeSe4f7r7hjQfQ%3D%3D.FeqZT84vOKuwNkTLKCD3%2BDQZZMWAkkDSfCUtfcN0Y1l0e4BBClCnZVXoKItpYULIotVgyzug8qUYwX3mVK%2BZIMcb3ukq9giyXR5oB5sYc9W74iJ9b9BifJjc%2FQ64KblS" rel="nofollow" target="_blank">华为digix算法大赛2020机器学习赛道-ctr预估初赛/决赛rank1</a></li><li><a href="https://link.segmentfault.com/?enc=ygdBJ2MmZvARlDZp1jGCKg%3D%3D.Mf4WKEMEFnITsMrHJItBa2%2FBR%2F%2FKDr8wPDWtCIUzreJ%2BwqkzJCwBIcAWkU3Rub7m8DAoITFmd88OxzQhUf5ElQ%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历（一：数据预处理、特征工程）</a></li><li><a href="https://link.segmentfault.com/?enc=jxtn5Kj39CE2gTDLWEAaxQ%3D%3D.6YtBYUpDFEj7owuM44De%2BdHd8VR9A2reG7ujeoIu%2FGkOCcoeqj8o9Fbmlk8QAh28lBfD2HUHsA%2BBBuY4MvItMQ%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历（三：lightgbm调参、ensemble）</a></li><li><a href="https://link.segmentfault.com/?enc=J2DqZQ7ljQUpzJ884IZoGw%3D%3D.UUfL%2FgRQ16ZTkll9VKUn%2FjaVIrlqmWhUxzTMqhvm707r2PZYK9ECZunDWdI7E5KA4eeGxDZerKikxIfW9uqgVA%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测(完整版代码，数据集等总结)</a></li><li><a href="https://link.segmentfault.com/?enc=HwYungiP8MCwxpNR6wSBTg%3D%3D.V4qFnqSQq9p96aKJ0tCvxaoCje8qO0Atiev3kQvuCQW5z2Cz%2B0hoiOCYlPGTSMkXMlVG0pwwTPwHVKV14NYaBw%3D%3D" rel="nofollow" target="_blank">天池- IJCAI-18 阿里妈妈搜索广告转化预测新手入门经历(完：观看决赛答辩感想)</a></li><li><a href="https://link.segmentfault.com/?enc=wl%2B1DuFMoFsnl0qpPup6%2Fg%3D%3D.%2FLrQMCN46ZlcNGq%2B4DYDzmzeQuYNIFNrm6%2Ba21GiilIZkYZepRi4ucnYOUvAjbI2rnDk0CbvuuEHnicoKRZrRA%3D%3D" rel="nofollow" target="_blank">微信大数据挑战赛 记录</a></li><li><a href="https://link.segmentfault.com/?enc=W481Y41MgtXRdrSYhQDlPg%3D%3D.8uYTKGtz49Dsm0bkQOsHtk6qPjh18qpgfWYEDVXRAEJhSK3vhbUDLLL7%2FYCnJAVt" rel="nofollow" target="_blank">ijcai-2018 top1 solution-阿里妈妈搜索广告转化预测</a></li><li><a href="https://link.segmentfault.com/?enc=0R4doceIyAYTaRwLLL%2FgcQ%3D%3D.3ikdyCh2M9r0YwSFvnkaa4ltaE%2B8wosUbq5I6y2Xir4vgm8G22PJsriP9DnAeIsvQWBmC2ZgJ6%2FksTnprgiDhw%3D%3D" rel="nofollow" target="_blank">2021-07-01 微信大数据竞赛源码(免费colab/GPU版本）</a></li><li><a href="https://link.segmentfault.com/?enc=RjNXvvgL68aKuPI5Jh6euA%3D%3D.zsb6n2R3RBzAeckYcdHC%2FpkDlOd55bFd3cC6uZGENMmphRt%2BLbzZofMG0irGQsNiBapWBK5rzbxcPMidHTMIfszGmaCDB%2B1FXJ40tngOKqo%3D" rel="nofollow" target="_blank">2018腾讯广告大赛baseline 100行代码带你上0.73</a></li><li><a href="https://link.segmentfault.com/?enc=hqR2q0kiNGJXbiFBRWtFyw%3D%3D.Xowf%2F57PTQfOozXQ%2BVkeEjoOp1GeEpxw6QKg%2FZoKgjnzU8KnCXqSzGadQaAL0HV15VT65k87blrQJHM%2Fre58cg%3D%3D" rel="nofollow" target="_blank">华为digix算法大赛2020机器学习赛道-ctr预估初赛/决赛rank1</a></li><li><a href="https://link.segmentfault.com/?enc=9ImZdZZYmOYemhUWpXG8vA%3D%3D.6JNUjVPBXj1uFEagXUKJdiQOxAnPsYN0Cy%2BCCj00K4nz1pz2MCZ35f3tW23wzKge" rel="nofollow" target="_blank">【IJCAI-18 阿里妈妈搜索广告转化预测】大赛答辩</a></li></ul><div class="footnotes"><hr/><ol><li id="fn-1"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=OrJYM9HenfBKJXnI7BTPug%3D%3D.444adoaOpVfSkEw%2BWD7%2FjiXuCrx9qjnKBKRApiSnlV6reCVm2Ipy2Ps0f6DIWPO%2Bw6F4erEp4nM%2FjmtJiLZqyg%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2"> <a href="https://link.segmentfault.com/?enc=7gcvHPmE2dC3x5EXuqdzlw%3D%3D.ZNPRYtgbpJ0Pb%2FqQFWbdfIHkj9ldFLqgHgn%2FhWYAyla7rllbn6n23WOrOXSunWvWH6qk92IzjbCsFGG0BqF6vr3ZKj9qZgbc8V335D3zDPTsuOTexYsJm%2BCEskFGW5o%2F" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3"> <a href="https://link.segmentfault.com/?enc=zoUsZ8PvI2KQ5iRqIePgQg%3D%3D.KwxjHVVpckJ%2FB0NV%2F2SD10oAQHdTnHxf6CemlR0Q7%2BowZeh6dDzkwVKrU2EwH3mkWH9fus%2BxFNX2Tu51owuujw%3D%3D" rel="nofollow" target="_blank">微信视频号推荐算法解题思路</a> <a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4"> <a href="https://link.segmentfault.com/?enc=1qYCoaANcPYUDkBq0cDbqw%3D%3D.LiW%2B0tod%2B%2B52cWmJAzr5%2Bv3lkY6LU1qdOxVMRIyy45qEXv3vC9Uez3l0zAkNzozzOYADmdF%2Bv7jvUzi3QoD1GhA5QHJdILsDAvFY6U0Nk%2Fwfzr%2FZf5WmxHVeyzl2%2BPy%2B" rel="nofollow" target="_blank">6.14周周星（第一名）分享</a> <a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5"> <a href="https://link.segmentfault.com/?enc=HOQmC0OJH6lH9e7r2w8cvg%3D%3D.DESa1KvlkcmU9efjsQ%2FEFsMhfDbhWQMGSpddB8xJMZx3IvUdpWmWNR8gp7IysaEgsLi7yodIynJYd2wZhearDg%3D%3D" rel="nofollow" target="_blank">推荐大赛如何在一周时间内打进决赛</a> <a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6"> <a href="https://link.segmentfault.com/?enc=YQpVPkX2SNDcTbqgxZQ36w%3D%3D.wpkT4tB0EPcGNFFldJ5kVYZ5W3fKYyxzhg1ib5SCmT6FcAV97zmyLenZQSerbja6UXIZRKxi1QmNfNy9Vvj9AA%3D%3D" rel="nofollow" target="_blank">对比不同主流存储格式（csv, feather, jay, h5, parquet, pickle）的读取效率</a> <a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7"> <a href="https://link.segmentfault.com/?enc=J2hl6QjsJ8V3W%2BsSt9e4Rw%3D%3D.7%2B0aZgbKavBw3Vcp9aJgj6cZzkUykr6YOaoBcDr4q9GEdl3OWLl3%2Fuc6dGrXTcMO" rel="nofollow" target="_blank">Python Pandas to_pickle()压缩文件</a> <a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8"> [[某比赛的baseline/线上0.63/Lightgbm]](<a href="https://link.segmentfault.com/?enc=CHUx56nIUIi1Mzjs4QYyjA%3D%3D.Yl%2Bl6hFUpzykWarpnTj96GUXNFCSKuNLCNL%2FCYcG4rW526RT50Isa8NkI5Rfe1trErSl8yRA5dUJkCr6spWLMQ%3D%3D" rel="nofollow" target="_blank">https://mp.weixin.qq.com/s/Q098yndLXBXhCo39_U-K7Q</a>) <a href="#fnref-8" class="footnote-backref">↩</a></li></ol></div>]]></description></item><item>    <title><![CDATA[烟草专卖执法案卷评查系统：案卷质量效率全域跃升 中烟创新 ]]></title>    <link>https://segmentfault.com/a/1190000047493686</link>    <guid>https://segmentfault.com/a/1190000047493686</guid>    <pubDate>2025-12-22 17:21:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>烟草专卖执法案卷评查工作是保障执法规范性、提升执法公信力的重要环节。随着法治建设的深入推进和案件数量的持续增长，传统人工评查模式已难以适应新形势下的监管需求。北京中烟创新科技有限公司（简称：中烟创新）为此研发了“烟草专卖执法案卷评查系统”，针对烟草行业痛点，提供了提升案卷质量与审查效率的全新解决方案。</p><p>评查系统通过构建AI驱动的全流程防控体系，实现了从事后纠偏向过程控制的范式转移，系统的核心技术突破体现在三维度技术融合架构：动态规则引擎：集成千余项执法标准形成结构化知识库规则，通过自适应学习机制实时吸纳法律法规更新，支持省级差异化监管政策的弹性配置，确保评查基准与最新规范保持同步。多模态分析中枢：结合光学字符识别（OCR）与自然语言处理（NLP）技术，实现对文书格式、证据链逻辑、法律条款引用等多维度的交叉验证。智能决策模型：依托企业级灯塔大模型应用开发平台，具备法律条文解析精度97.9%的语义理解能力，通过模式识别生成风险预警矩阵，为执法人员提供实时决策支持。</p><p>系统采用“人工+AI”双审查模式，融合专业判断与AI效率及一致性优势。支持对接省级监管平台，具备OCR识别能力（纸质/电子文件），推进案卷信息化。显著提升评查效率，达传统模式数倍(如某案例：人工3小时vs系统5分钟），释放90%以上事务性人力负荷，替代了传统逐项核对文书的重负。基于《烟草专卖处罚程序规定》及标准，系统建立精细化评查体系覆盖立案至执行全流程。特别关注程序合规、案件定性、处罚裁量等细节，确保案卷清晰准确记录执法过程。</p><p>自动记录处理阶段并在关键节点发送提醒，保障案件按时按质完成，有效避免人为差错。系统通过六个核心指标评查案卷质量：自由裁量合法性（处罚在法定幅度内）；程序时限合法性（程序在法定期限内）；卷宗形式规范、材料完整；文书内容完整、规范、逻辑一致；法律依据引用准确；文字及多文书信息一致。</p><p>系统还构建了“自查-交叉评查-上级抽查”三级联动评查体系，通过多层级、多视角的检查与复核，确保评查结果客观公正。这种多层次的评查机制有效保障了执法质量的持续提升。系统在三个关键阶段发挥风险防控作用。</p><p>事前预防：通过智能预检自动筛查程序超期、证据链断裂等高发风险，将大部分的常规错误阻断于案卷形成阶段。系统能够实时监测案件处理的各个阶段，对潜在风险进行预警和评估，为执法人员提供决策支持。</p><p>事中干预：建立实时反馈机制，执法人员可在文书制作过程中即时接收格式修正与条款适用建议，显著降低事后整改成本。</p><p>事后治理：通过大数据生成区域性质量热力图，精准定位系统性缺陷，驱动靶向整改。系统自动聚合分析高频错误与典型问题，为持续提升执法规范化水平提供数据支撑和决策参考。</p><p>通过自动化文书生成与智能辅助功能，单个案卷平均制作时间减少约70%，案件整体处理效率提升40%以上。这使得执法人员能够将更多精力投入案件调查与实地核查等核心执法工作。烟草专卖执法案卷评查系统通过技术创新与制度创新的深度融合，树立了传统产业智能化升级的标杆范式，持续输出可复制、可推广的行业级解决方案。</p>]]></description></item><item>    <title><![CDATA[YashanDB的11个功能特性提升数据处理效率 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493689</link>    <guid>https://segmentfault.com/a/1190000047493689</guid>    <pubDate>2025-12-22 17:20:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代数据库技术领域，随着数据规模和复杂度的不断增加，系统在处理高并发、大容量数据时面临诸多挑战，如性能瓶颈、数据一致性维护困难、存储资源限制等。针对这些通用问题，YashanDB通过其先进的架构设计和功能特性，实现了对海量数据的高效管理与处理，确保数据库系统在各种应用场景下均能保持稳定高性能。本文将系统性剖析YashanDB提供的11项关键功能特性，探讨其在提升数据处理效率方面的技术优势，旨在为数据库开发人员和管理员提供深入的技术理解和实践参考。</p><ol><li>多样化的部署架构支持</li></ol><p>YashanDB提供单机(主备)、分布式集群及共享集群三种部署形态。单机部署适合中小规模应用且对高可用需求适中，主备复制保证数据同步与故障切换。分布式集群采用Shared-Nothing架构，具备线性扩展能力，适合海量数据分析业务。共享集群则基于Shared-Disk架构，利用共享存储和聚合内存技术，实现多实例的强一致性读写和高可用，满足高端核心交易需求。此多样化部署方案为不同业务场景提供灵活配置，优化资源使用，提高整体处理效率。</p><ol start="2"><li>高效的存储引擎及多存储结构支持</li></ol><p>YashanDB支持HEAP、BTREE、MCOL和SCOL四种存储结构，分别针对OLTP、HTAP和OLAP三类典型业务场景进行了优化。HEAP为无序行式存储，适合事务性能需求高的场景;BTREE存储适用于索引结构，提高查询速度;MCOL实现了可变列式存储，通过段页式存储和原地更新技术提高变更效率，支持实时业务需求;SCOL基于对象式切片存储，结合压缩编码提升海量稳态数据查询性能。结合行存表、TAC表和LSC表的灵活运用，满足不同数据操作模式，保障读写平衡和高性能访问。</p><ol start="3"><li>先进的事务机制与多版本并发控制（MVCC）</li></ol><p>全表支持事务的ACID特性，YashanDB通过MVCC技术实现读取一致性，避免读写阻塞。具体实现中，数据库采用事务槽(Xslot)、Undo日志操作数据的历史版本，以SCN为时间视角管理事务可见性。读操作可基于版本快照读取数据，而写操作则通过行锁保证数据一致。支持语句级、事务级一致性读，以及写一致性机制，保障并发事务间的隔离和性能。提供灵活的隔离级别配置，包括读已提交和可串行化，适配不同性能和一致性需求。</p><ol start="4"><li>强大的SQL引擎与基于成本的优化器</li></ol><p>YashanDB的SQL引擎涵盖解析、验证、静态与动态重写、优化与执行等多阶段处理，内置CBO优化模型。优化器基于动态统计信息评估各种执行路径，选择最低成本计划，有效利用索引、计算算子及并行度。支持内置多种执行算子，如扫描、连接、排序和向量化计算算子，利用SIMD技术批量并行处理数据。HINT机制让用户能够微调执行计划，灵活控制SQL执行，进一步提升查询效率。</p><ol start="5"><li>高度并行与分布式SQL执行能力</li></ol><p>支持MPP架构的分布式SQL执行，通过协调节点(CN)、管理节点(MN)和数据节点(DN)分工协作。CN负责生成分布式执行计划，DN并行执行任务，多级并行执行(节点间与节点内)，结合PX并行算子和数据分片交换机制，实现高吞吐量。通过内部互联网络保障高效网络通信，合理分配资源，降低跨节点通信延时，保证海量数据分析的实时性。</p><ol start="6"><li>灵活的存储管理和空间调度机制</li></ol><p>YashanDB逻辑存储层将存储分为块、区和段三层结构，采用段页式和对象式管理。通过多级空闲空间列表与水位线控制，实现精细的空间使用和并发访问优化。支持动态扩展表空间，数据文件和切片文件的并行管理，并利用双写机制防止半写错误，提高数据安全性。合理的空间管理减少碎片，提高I/O效率，促进数据高效访问和修改。</p><ol start="7"><li>先进的索引体系和多样的访问策略</li></ol><p>以BTREE索引为核心，支持唯一索引和非唯一索引，包含叶子块和分支块的平衡结构保证快速检索。支持多种扫描方式，包括全索引扫描、索引快速全扫描、范围扫描、唯一扫描和跳跃扫描，根据查询条件灵活选择。反向索引解决自增列索引倾斜问题。支持函数索引提升复杂表达式查询性能。索引的聚集因子优化存储顺序，降低I/O成本，显著加速数据访问。</p><ol start="8"><li>灵活的PL引擎及过程化编程支持</li></ol><p>内置PL语言引擎提供过程、函数、触发器、自定义类型和定时任务的编译及执行支持。PL语言支持变量、表达式、控制流、异常处理和静态及动态SQL，具备较强的编程能力。支持自治事务提供嵌套独立事务功能，增强事务隔离与异常处理。PL对象可持久化存储，提高复用和执行性能。定时任务管理支持复杂调度，便于实现自动化维护和业务逻辑处理。</p><ol start="9"><li>主备高可用及自动选主机制</li></ol><p>提供基于redo日志的主备复制，支持同步和异步复制模式，保障数据的可靠写入与及时同步。主备切换支持计划内(switchover)和故障切换(failover)，配合redo日志回放和归档修复。自动选主采用Raft协议和yasom仲裁，确保集群一致性和快速故障恢复。支持Quorum配置和多保护模式，兼顾可用性和数据安全性，最大限度提升业务连续性。</p><ol start="10"><li>完善的安全管理体系</li></ol><p>基于角色的访问控制和细粒度的标签访问控制(LBAC)，实现权限隔离和行级数据保护。支持系统特权和对象特权授权，采用多重身份认证方式，包括数据库密码认证和操作系统认证。实现用户管理、密码策略、安全审计及访问日志，保证数据保密性和可审计性。网络层支持SSL/TLS传输加密，保障数据传输安全。数据库层加密支持表空间、表和备份集透明加密，保障数据静态安全。</p><ol start="11"><li>高性能共享集群基础设施</li></ol><p>共享集群基于崖山集群服务(YCS)和崖山文件系统(YFS)，前者负责节点管理、资源调度、高可用和故障仲裁，后者提供高性能并行文件服务和多副本存储。利用全局资源目录(GRC)、全局缓存服务(GCS)和全局锁服务(GLS)实现多实例间资源协调与强一致性访问。YFS支持磁盘组和故障组管理，多副本冗余提高数据可靠性。该基础设施确保多实例数据库环境中高性能、高一致性、高可用性和扩展性。</p><p>总结与建议</p><p>根据应用场景选择合适的部署架构，充分发挥单机、分布式及共享集群的性能优势。</p><p>合理应用存储结构，区分热数据与冷数据，结合HEAP、MCOL及SCOL存储优化读写性能。</p><p>利用MVCC与事务隔离级别，确保数据一致性的同时提升并发处理能力。</p><p>关注SQL语句优化，收集准确统计信息，使用Hint指令辅助优化计划。</p><p>设计合理索引结构，选择合适索引类型和扫描策略，降低I/O开销。</p><p>扩展和利用PL引擎，实现业务逻辑本地化处理，减少网络通信开销。</p><p>部署主备复制与自动选主机制，保障业务的高可用性和数据安全。</p><p>强化安全管理，采取角色、标签访问控制和加密措施，满足合规要求。</p><p>合理配置共享集群资源，提升多实例环境下的协同处理效率。</p><p>定期进行备份与恢复演练，确保数据持久性和业务连续性。</p><p>持续关注数据库性能监控和诊断，快速响应和处理异常事件。</p><p>结论</p><p>随着企业数据量的爆炸式增长与业务对数据实时处理能力的需求日益提升，数据库系统在性能、扩展性和高可用性方面的要求也愈加严苛。YashanDB通过集成多种先进技术和功能特性，包括灵活的部署架构、多样化存储引擎、优化的事务和并发控制、强大的SQL执行引擎、高并发分布式处理能力及完善的安全保障机制，为行业客户提供了高效稳定的数据处理解决方案。未来，面对大数据与云计算的发展趋势，数据库的性能优化与业务适配能力将成为核心竞争力。持续深入掌握并应用YashanDB的技术优势，将助力数据库开发和运维团队有效应对复杂业务挑战，实现数据资源的最大价值化。</p>]]></description></item><item>    <title><![CDATA[YashanDB的版本更新与用户体验改进 逼格高的鼠标垫_elp4Ti ]]></title>    <link>https://segmentfault.com/a/1190000047493693</link>    <guid>https://segmentfault.com/a/1190000047493693</guid>    <pubDate>2025-12-22 17:20:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>随着数据库技术的日益成熟和应用场景的多样化，如何在保证数据一致性和高可用性的同时，提高查询速度和系统稳定性，成为数据库产品竞争的核心指标。对于企业而言，数据库的性能、可靠性和可维护性直接影响业务的连续性和发展速度。YashanDB作为一款功能全面且技术先进的数据库管理系统，通过持续的版本更新和架构优化，不断解决系统瓶颈问题，提升用户体验。</p><p>版本更新的核心技术升级</p><p>多形态部署架构的优化与拓展</p><p>YashanDB支持单机部署、分布式集群部署和共享集群部署三种部署形态，满足不同业务规模和数据访问需求。最新版本在单机部署模式中进一步简化主备复制机制，提升主备同步性能，确保主库故障时快速切换。分布式部署通过更精细的节点管理实现了元数据节点、协调节点与数据节点的高效协同，提升了系统的线性扩展能力。共享集群部署引入聚合内存技术，完善全局资源管理，实现多实例数据访问的强一致性，保证集群多活运行的稳定性和高性能。</p><p>存储引擎和数据结构的增强</p><p>针对在线事务处理(OLTP)及在线分析处理(OLAP)场景，YashanDB支持HEAP、BTREE、MCOL和SCOL四种存储结构。新版优化了MCOL的可变列式存储架构，支持更多维度的原地更新(in-place update)，避免了存储空间膨胀和频繁垃圾回收。SCOL稳态列式存储引入切片式组织和高级压缩技术，提升了海量数据访问的查询速度。存储对象类型的创新，如TAC表与LSC表细分冷热数据存储，大幅提高了系统的在线分析处理能力。</p><p>SQL引擎与优化器的性能提升</p><p>YashanDB数据库的SQL引擎采用基于成本的优化器(CBO)，集成了静态重写与动态重写机制，能够生成高效的执行计划。最新版本增强了统计信息收集与并行统计能力，确保优化器拥有准确的代价模型。支持更丰富的内置函数库和向量化计算框架，利用SIMD技术实现批量数据处理并行，提高CPU的利用率和查询吞吐量。并行执行与分布式SQL计划调度使得复杂查询在大规模集群环境下依然保持高效，满足海量数据分析需求。</p><p>事务管理与高可用性的保障机制</p><p>事务引擎支持ACID特性和多版本并发控制(MVCC)，保障数据的一致性和隔离性。新版本强化了读已提交和可串行化隔离级别的写冲突检测与死锁处理，提升并发事务的稳定性。主备复制架构优化了redo日志传输与回放机制，支持同步和异步复制模式，并引入Quorum机制提升数据保护程度。自动选主功能基于Raft算法，实现了主备故障的快速感知和自动切换，减少了人工干预需求，提高了系统可用率。</p><p>安全管理与数据保护功能完善</p><p>安全体系升级涵盖用户管理、身份认证、访问控制、加密和审计。引入基于角色的访问控制(RBAC)和基于标签的访问控制(LBAC)，实现细粒度权限分配和行级访问控制。支持表空间和表级透明数据加密(TDE)，结合强加密算法保护存储数据安全。网络通信采用标准SSL/TLS加密及X.509证书认证机制，实现数据传输的机密性和完整性保障。审计系统能够灵活配置审计策略并支持异步审计，兼顾审计完整性与系统性能。反入侵机制提供IP黑白名单和连接监听功能，防护外部攻击与异常连接。</p><p>用户体验改进措施</p><p>简化部署与运维管理</p><p>通过完善的运维工具集成及配置管理，新版本支持多种快速部署方案，如单机主备配置、一键分布式集群扩容与共享集群管理。增加了配置参数的灵活调整途径，支持实时和重启生效切换。自动化故障诊断架构集成健康监控线程和自动恢复机制，日志诊断和黑匣子数据帮助快速定位故障。集群服务通过网络和磁盘心跳实现实时异常监控，自动投票仲裁重组集群，保证业务不中断。</p><p>提升查询与事务响应速度</p><p>针对查询体验，提供SQL缓存机制避免重复解析，优化索引策略支持多样索引扫描方式(范围扫描、唯一扫描、跳跃扫描等)，并完善HINT提示语法辅助调整执行计划。加强内存管理结构，优化数据缓存和有界加速缓存，降低I/O延迟。事务并行度调优与写日志线程优化，保证高并发环境下事务吞吐和一致性。</p><p>丰富客户端支持与开发扩展性</p><p>保障各种主流编程语言的客户端驱动稳定支持，包括JDBC、Python DB API、ADO.NET、C和ODBC，提供统一接口调用体验。增强PL引擎编程能力，支持过程、函数、触发器、高级包等多种PL对象的灵活定义与调用，集成动态与静态SQL，满足复杂业务逻辑。支持外置UDF(C/C++及Java)，通过沙箱隔离保障系统稳定性，兼顾安全与性能。</p><p>灵活数据分区与存储管理</p><p>引入多种分区策略支持(范围分区、哈希分区、列表分区、间隔分区及复合分区)，用户可根据业务特点自由组合，减少无效数据扫描，加速定位数据。支持本地分区索引和全局分区索引关联分区管理。存储空间管理上优化段页式和对象式管理接口，提高空间分配效率。完善逻辑与物理存储分离机制，支持云存储和分布式数据空间管理，拓展混合云架构应用。</p><p>技术建议</p><p>基于业务场景合理选择部署形态。中小型应用优先考虑单机主备部署，海量数据分析业务宜采用分布式部署，高可用且多实例数据协同场景选用共享集群。</p><p>根据访问模式选择存储结构。事务性场景采用HEAP行存，实时分析应用首选MCOL列存，需兼顾更新和分析的实时业务采用TAC表，海量稳态数据分析利用LSC表。</p><p>定期维护统计信息，确保优化器准确估算代价，提供最佳执行计划，必要时使用HINT进行精细调优。</p><p>设计合适的索引策略，结合查询特征制定唯一索引、函数索引及多列组合索引，避免过多或滥用索引带来的维护成本。</p><p>合理设定事务隔离级别与锁策略，避免死锁风险，适当利用写一致性与保存点提高事务并发能力。</p><p>启用安全加密与访问控制策略，根据企业合规需求配置数据加密及审计，使用角色和标签实现合理权限划分，保障数据隐私与安全。</p><p>利用自动选主和自动故障检测减少人工干预，实现高可用环境下的快速切换与容灾恢复。</p><p>充分利用PL语言和外置函数增强业务逻辑处理能力，减少网络延迟和客户端复杂度。</p><p>结论</p><p>YashanDB通过持续的技术迭代与版本升级，完善了从存储底层到SQL执行引擎，再到安全管理和高可用架构的整体性能和稳定性。随着数据规模和业务复杂度的不断攀升，数据库系统对查询效率和系统可用性的要求日益严格，YashanDB的多部署形态、多存储结构和高效事务管理架构，为应对未来大数据挑战奠定坚实基础。展望未来，随着云计算与人工智能技术的融合，YashanDB将不断优化并融合更多智能化管理与自动化调优功能，为行业用户提供强大的数据服务支撑。持续关注数据库核心技术提升，是保障业务创新与高效运营的关键。</p>]]></description></item>  </channel></rss>