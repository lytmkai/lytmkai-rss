<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[裁员为什么先裁技术人员？网友一针见血 C]]></title>    <link>https://segmentfault.com/a/1190000047447755</link>    <guid>https://segmentfault.com/a/1190000047447755</guid>    <pubDate>2025-12-04 09:02:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近逛职场社区的时候，刷到一个职场话题，老生常谈了，但是每次参与讨论的同学都好多。</p><p>这个问题问得比较扎心：</p><p><strong>“为什么有些企业的裁员首先从技术人员开始？”</strong></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447757" alt="" title=""/></p><p>那关于这个问题，网上有一个被讨论很多的比喻：</p><p><strong>“房子都盖起来了，还需要工人么？”</strong></p><p>有一说一，这个比喻虽然刺耳，但却非常形象地揭示了某些企业的用人逻辑，尤其在<strong>某些非技术驱动型的公司里</strong>。</p><p>在某些非技术驱动的公司（比如传统企业转型、或者业务模式成型的公司），其实技术部门很多时候是会被视为「<strong>成本中心</strong>」，而非「<strong>利润中心</strong>」的，我相信在这类企业待过的技术同学肯定是深有体会。</p><p>就像盖大楼一样，公司需要做一个 App，或者搞一个系统，于是高薪招来一帮程序员“垒代码”。</p><p>当这个产品上线，业务跑通了，进入了平稳运营期，公司<strong>某些大聪明老板</strong>总会觉得“房子”已经盖好了。</p><p>这时候，一些开发人员在老板眼里就变成了“冗余”的成本。</p><p>大家知道，销售部门、业务部门能直接带来现金流，市场部能带来用户，而技术部门的代码是最看不见摸不着的。</p><p>一旦没有新的大项目启动，老板会觉得技术人员坐在那里就是在“烧钱”。</p><p>那抛开这个“盖楼”的比喻，在这种非技术驱动的公司里，从纯粹的财务角度来看，裁技术岗往往是因为“<strong>性价比</strong>”太低。</p><p>所以这里我们不得不面对的一个现实是：技术人员通常是公司里薪资最高的一群人。</p><p>高薪是一把双刃剑呐。</p><p>一个初级程序员的月薪可能抵得上两个行政，一个资深架构师的年薪可能抵得上一个小团队的运营费用。当公司面临现金流危机，需要快速削减成本时，裁掉一个高级技术人员省下来的钱，相当于裁掉好几个非技术岗位人员。</p><p>除此之外还有一个比较尴尬的事情那就是，在技术团队中，往往存在着一种“金字塔”结构。</p><p>随着工龄增长，薪资涨幅很快，但产出效率（在老板眼里）未必能线性增长。</p><p>脑补一下这个场景就知道了：</p><ul><li>一个 35 岁的高级工程师，月薪 4 万，可能要养家糊口，精力不如 20 多岁的小年轻，加班意愿低。</li><li>一个 23 岁的小年轻，月薪 1 万 5，充满激情，能扛能造。</li></ul><p>这时候<strong>某些大聪明老板</strong>的算盘就又打起来了：</p><p>裁掉一个 4 万的老员工，招两个 1 万 5 的小年轻，代码量翻倍，团队氛围更活跃，成本还降了，这种“优化”在管理层眼里，简直是“降本增效”的典范。</p><p>所以综合上面这种种情形分析，这时候，文章开头的那个问题往往也就会逐渐形成了。</p><p>所以事就是这么个事，说再多也没用。</p><p>既然环境不能左右，那<strong>作为个体，我们又该如何自处呢</strong>？</p><p><strong>这里我不想灌鸡汤，只想务实地聊一聊我所理解的一些对策</strong>，希望能对大家有所启发。</p><p>同时这也是我给很多后台私信我类似问题小伙伴们的一些共同建议。</p><p><strong>1、跳出技术思维，建立业务思维</strong></p><p>千万不要只盯着你的 IDE 和那一亩三分地代码，抽空多了解了解业务和流程吧，比如：</p><ul><li>项目是靠什么赚钱的？</li><li>你的代码在哪个环节为公司省钱或挣钱？</li><li>如果你是老板，你会怎么优化现在的系统？</li></ul><p>当你能用技术手段去解决业务痛点（比如提升转化率、降低服务器成本）时，你就不再是成本，而是资产。</p><p><strong>2、别温水煮青蛙，要保持技能更新</strong></p><p>这一点之前咱们这里多次提及，在技术行业，吃“老本”是最危险的。</p><p>当今的技术世界变化太快，而作为程序员的我们则恰好处于这一洪流之中，这既是挑战，也是机会。</p><p>还是那句话，一定要<strong>定期评估一下自己的市场价值</strong>：如果明天就离开现在的公司，你的技能和经验是否足以让你在市场上获得同等或更好的位置？</p><p>无论在公司工作多久，都要不断更新自己的技能和知识，确保自己始终具有市场竞争力。</p><p><strong>3、别让自己的工作经验烂掉，有意识地积累职业资产</strong></p><p>这一点我们之前其实也聊过。</p><p>除了特定的技术、代码、框架可以作为自己可积累的能力资产之外，其实程序员的职业生涯里也是可以有很多可固化和可积累的有形资产的。</p><p>比如你的技术经历、思维、经验、感悟是不是可以写成技术博客文字？你写的代码、工具、框架是不是可以形成开源项目？你的工作笔记和踩坑记录是不是可以整理成技术手册？</p><p>千万不要让自己的工作经验烂掉，而是要有意识地将自己的技术资产化，将自己的过往经验、知识、能力转化成在行业里有影响力的硬通货。</p><p><strong>4、尽早构建 Plan B，提升抗风险能力</strong></p><p>当然这一点虽然说的简单，其实对人的要求是比较高的。前面几点做好了，这一点有时候往往就会水到渠成。</p><p>我觉得总体的方向应该是：尽量利用你的技术特长来构建一个可持续的 Plan B。</p><p>比方说：开发一个小工具、写写技术专栏、或者运营一个 GitHub 项目、在技术博客或社区中建立个人品牌...等等，这些不仅仅能增加收入，往往还能拓展你的人脉圈。</p><p>其实很多程序员在年龄大了之后越来越焦虑的一个重要原因就是因为生存技能太过单一了，所以千万不要给自己设限，埋头赶路的同时也不要忘记时常抬头看看周围的环境和机会。</p><p>好了，今天就先聊这么多吧，希望能对大家有所启发，我们下篇见。</p><blockquote>注：本文在GitHub开源仓库「编程之路」 <a href="https://link.segmentfault.com/?enc=AdPbQ9TWmpb%2BdkBio0R0ig%3D%3D.2pco7aiUAC%2FuX4B98fmgHsJVp572ER%2BG51cgxxP1ekvUquFoTXdMpGu86taMki8e" rel="nofollow" target="_blank">https://github.com/rd2coding/Road2Coding</a> 中已经收录，里面有我整理的6大编程方向(岗位)的自学路线+知识点大梳理、面试考点、我的简历、几本硬核pdf笔记，以及程序员生活和感悟，欢迎star。</blockquote>]]></description></item><item>    <title><![CDATA[剑指offer-46、孩⼦们的游戏(圆圈]]></title>    <link>https://segmentfault.com/a/1190000047437827</link>    <guid>https://segmentfault.com/a/1190000047437827</guid>    <pubDate>2025-12-04 09:02:17</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>题目描述</h2><p>有个游戏是这样的：⾸先，让 n 个⼩朋友们围成⼀个⼤圈，⼩朋友们的编号是0~n-1。然后，随机指定⼀个数 m ，让编号为0的⼩朋友开始报数。每次喊到 m-1 的那个⼩朋友要出列唱⾸歌，然后可以在礼品箱中任意的挑选礼物，并且不再回到圈中，从他的下⼀个⼩朋友开始，继续 0... m-1报数....这样下去....直到剩下最后⼀个⼩朋友，可以不⽤表演，并且拿到⽜客礼品，请你试着想下，哪个⼩朋友会得到这份礼品呢？</p><p>示例<br/>输⼊：5,3<br/>输出：2</p><h2>思路及解答</h2><h3>数组模拟</h3><p>通过布尔数组标记小朋友的出局状态</p><pre><code class="java">public class Solution {

    public int lastRemaining(int n, int m) {
        if (n &lt;= 0 || m &lt;= 0) return -1;
        
        boolean[] out = new boolean[n]; // 标记是否出局
        int count = n;                  // 剩余人数
        int index = 0;                  // 当前报数位置
        int step = 0;                   // 报数计数器
        
        while (count &gt; 1) {
            // 如果当前小朋友未出局，参与报数
            if (!out[index]) {
                step++;
                // 报到m-1的小朋友出局
                if (step == m) {
                    out[index] = true;  // 标记出局
                    count--;            // 剩余人数减1
                    step = 0;           // 重置计数器
                }
            }
            // 移动到下一个位置（循环）
            index = (index + 1) % n;
        }
        
        // 找到最后一个未出局的小朋友
        for (int i = 0; i &lt; n; i++) {
            if (!out[i]) {
                return i;
            }
        }
        return -1;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n×m)，最坏情况下每个小朋友都需要报数m次</li><li><strong>空间复杂度</strong>：O(n)，需要长度为n的布尔数组</li></ul><h3>循环链表</h3><p>使用循环链表模拟小朋友围成的圈，将小朋友存入链表，循环删除第m个元素</p><pre><code class="java">public class Solution {

    public int lastRemaining(int n, int m) {
        if (n &lt;= 0 || m &lt;= 0) return -1;
        
        List&lt;Integer&gt; list = new LinkedList&lt;&gt;();
        // 初始化链表，存入所有小朋友编号
        for (int i = 0; i &lt; n; i++) {
            list.add(i);
        }
        
        int index = 0; // 当前指针位置
        
        while (list.size() &gt; 1) {
            // 计算要删除的位置：(当前索引 + m-1) % 当前大小
            index = (index + m - 1) % list.size();
            list.remove(index);
            // 删除后index自动指向下一个元素，不需要移动
        }
        
        return list.get(0);
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n×m)，需要遍历链表进行删除操作</li><li><strong>空间复杂度</strong>：O(n)，需要存储n个节点</li></ul><h3>数学归纳法（推荐）</h3><p>分析每次被删除的数字规律，直接计算出最后的数字，不需要模拟</p><pre><code class="java">F(N,M) = ( F(N−1,M) + M ) % N</code></pre><p><strong>递推公式的推导过程：</strong></p><ol><li><strong>第一次删除</strong>：从0开始报数，删除第(m-1)%n个小朋友</li><li><p><strong>重新编号</strong>：删除后，从第m%n个小朋友开始重新编号：</p><ul><li>旧编号：m%n, m%n+1, ..., n-1, 0, 1, ..., m%n-1</li><li>新编号：0, 1, 2, ..., n-2</li></ul></li><li><strong>映射关系</strong>：新编号x对应的旧编号为(x + m) % n</li></ol><p><strong>示例验证（n=5, m=3）：</strong></p><pre><code class="text">原始编号: 0, 1, 2, 3, 4
第一次删除编号2 → 剩余: 0, 1, 3, 4
重新编号: 3→0, 4→1, 0→2, 1→3
f(5,3) = (f(4,3) + 3) % 5</code></pre><pre><code class="java">public class Solution {
    public int LastRemaining_Solution(int n, int m) {
        if (n &lt;= 0 || m &lt;= 0) {
            return -1;
        }
        int result = 0;
        for (int i = 2; i &lt;= n; i++) {
            result = (result + m) % n;
        }
        return result;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，需要n次递归调用</li><li><strong>空间复杂度</strong>：O(n)，递归调用栈深度</li></ul><h3>迭代优化</h3><p>将递归转为迭代，避免栈溢出风险，是生产环境的最佳选择</p><pre><code class="java">public class Solution {

    public int lastRemaining(int n, int m) {
        if (n &lt;= 0 || m &lt;= 0) return -1;
        
        int result = 0; // f(1, m) = 0
        
        // 从2个人情况开始，逐步计算到n个人
        for (int i = 2; i &lt;= n; i++) {
            result = (result + m) % i;
        }
        
        return result;
    }
}</code></pre><ul><li><strong>时间复杂度</strong>：O(n)，只需一次循环</li><li><strong>空间复杂度</strong>：O(1)，只使用常数空间</li></ul>]]></description></item><item>    <title><![CDATA[数据库审计：构建企业数据安全与合规治理的]]></title>    <link>https://segmentfault.com/a/1190000047445589</link>    <guid>https://segmentfault.com/a/1190000047445589</guid>    <pubDate>2025-12-04 09:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>一、概述<br/>数据库审计是企业数据安全体系的核心组成部分，是一种对数据库访问与操作行为进行持续、精细化记录、分析与回溯的机制。它通过对访问者身份、操作内容、来源及时间等信息的完整留存，帮助企业实现数据资产的强可视、强监管与强溯源。在发生安全事件、违规操作或系统异常时，数据库审计能助力快速定位问题、追踪根源，从而有效降低数据泄露风险、阻断潜在攻击路径，并提升整体安全治理水平。面对现代企业复杂的架构与高频的数据交互场景，数据库审计已不仅是安全工具，更是合规治理、内部风控、运维管理与数据资产保护的重要基础设施。<br/>二、数据库审计的目的<br/>数据库审计的核心目的是 “发现安全问题” 。通过持续监控数据库访问行为与操作内容，企业能够及时识别非法登录、暴力破解、越权访问、敏感数据异常读取、SQL注入、批量数据导出以及非常规来源的访问等风险，实现对潜在攻击与内部违规的早期预警。<br/>审计记录也为安全管理提供数据基础，帮助识别权限配置不合理、高权限账户闲置、访问模式突变等问题，推动安全策略从静态规则向基于数据的持续优化演进，使权限控制与访问策略持续处于风险最小化状态。<br/>此外，数据库审计有助于满足《数据安全法》、等保2.0/3.0、PCI DSS、HIPAA、GDPR等日益严格的合规要求，审计日志可作为可追溯证据，保障合规检查顺利通过。在运维层面，审计系统也能辅助性能优化，例如识别慢SQL、分析压力来源、发现异常访问模式、优化索引策略等，从而协助提升系统性能与运维效率，构建安全与性能并重的数据库治理体系。<br/>三、数据库审计的主要组成部分<br/>数据库审计系统主要由四大核心部分构成。其基础是日志记录，涵盖用户行为、数据操作、安全事件及系统状态等各类日志，这些日志必须确保不可篡改、完整留存且时间同步，以形成可靠的审计证据链。基于此，企业可根据业务风险配置差异化的审计策略，例如对敏感数据和核心操作进行重点审计，并设置相应的告警规则，以平衡监控效果与系统开销。审计分析是实现智能化的关键，通过对用户行为建模、自动识别异常模式并结合可视化工具呈现分析结果，助力安全人员快速定位风险。最后，系统生成的审计报告承担着总结与汇报职能，它以清晰的结构呈现安全状态、异常事件及合规指标，为内部管理和外部检查提供有力支持。<br/>四、数据库审计的主要类型<br/>为全面覆盖数据库管理的不同维度，审计工作主要分为五种类型。安全审计侧重于权限合理性与异常访问轨迹，旨在构建可视化的安全防护体系。操作审计则聚焦于数据增删改查、管理命令及配置变更等具体行为，确保所有操作可追溯、责任可界定。数据审计着眼于数据本身的生命周期，追踪其访问与流转过程，以保障数据的完整性与安全性。性能审计通过分析SQL效率与资源占用情况，为数据库优化提供直接依据。而合规性审计则专门检查数据库活动是否符合内外部法规与标准要求。这五类审计相辅相成，共同构成系统化的数据库监控与治理框架。<br/>五、如何实施数据库审计<br/>实施数据库审计是一个系统化的过程。首先需要制定审计计划，明确业务关键数据、敏感范围以及高风险用户与操作。随后，依据计划配置具体的审计策略，设定日志级别、存储方式及重点监控对象，确保在记录关键行为的同时避免对系统性能造成过大压力。接下来是日志的统一采集与整合，保证来自不同系统的审计数据能够关联分析。之后进入分析与报告阶段，通过自动化工具或人工审查识别可疑行为，并生成符合合规要求的标准化报告，如安全态势报告或事件溯源分析。最后，必须依据法规要求对审计日志进行长期的妥善留存与管理，包括定期归档、完整性校验和安全存储，以备合规审查与事件取证之需。<br/>六、数据库审计的综合作用<br/>数据库审计在企业的数据治理中扮演着多维度的核心角色。在安全层面，它通过全量行为记录与智能分析，构建了主动的威胁发现与响应能力，并能优化访问控制策略，实现精准的身份溯源。对于数据完整性，审计提供了可追溯的操作记录，结合告警与备份机制，能有效防范和恢复非法数据篡改。在合规性管理上，系统化的审计日志与报告为满足各类法规要求提供了可验证的证据链。此外，审计系统还能优化数据库性能，通过识别慢查询与资源瓶颈指导运维优化。同时，它有助于识别潜在安全漏洞的前兆行为，并对敏感数据实施贯穿其生命周期的重点监控与保护。综上所述，数据库审计集多种关键作用于一体，是企业构建可靠、高效、合规的数据安全生态的基石。<br/>七、如何选择合适的数据库审计工具和供应商<br/>选择合适的数据库审计工具与供应商，需要综合考量多方面因素。工具本身的核心能力是基础，包括功能的全面性、与现有系统的兼容性、智能分析水平、对多类型数据库的支持度以及日志记录的完整性。同时，需评估其在高并发场景下的性能表现与系统的可扩展性，以及是否具备良好的平台集成能力与直观的可视化报表。就供应商而言，其行业经验、技术支撑服务的及时性、以及对特定行业（如金融、政务）合规要求的深入理解和适配能力至关重要。对于监管严格的行业，选择经过大型项目验证、能提供持续稳定支持的成熟厂商，是确保审计体系长期有效运行的关键。</p>]]></description></item><item>    <title><![CDATA[docker如何迁移更省空间 onesl]]></title>    <link>https://segmentfault.com/a/1190000047447659</link>    <guid>https://segmentfault.com/a/1190000047447659</guid>    <pubDate>2025-12-03 23:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>迁移镜像时候，往往会碰到基础镜像相同的很多镜像需要迁移，这个时候如果单独docker save的话，镜像tar包的尺寸会很大，因此为了解决这个问题，你需要换种姿势使用docker save! docker镜像是分层的，将基础镜像的相同的层合并到一起, 就可以节省空间了。</blockquote><h2>方法一：使用<code>docker save</code>命令</h2><ol><li><p>首先保存各个镜像到单独的tar文件：</p><pre><code class="bash">docker save -o image1.tar image1:tag
docker save -o image2.tar image2:tag
docker save -o image3.tar image3:tag</code></pre></li><li><p>然后将这些tar文件打包到一个tar包中：</p><pre><code class="bash">tar -cvf all_images.tar image1.tar image2.tar image3.tar</code></pre></li></ol><h2>方法二：直接保存多个镜像到一个tar文件</h2><p>Docker的<code>save</code>命令本身就支持一次保存多个镜像到一个文件：</p><pre><code class="bash">docker save -o all_images.tar image1:tag image2:tag image3:tag</code></pre><h2>哪个方法更节省磁盘空间？</h2><p>方法二（直接使用<code>docker save</code>保存多个镜像到一个tar文件）<strong>更节省磁盘空间</strong>，原因如下：</p><h3>空间效率对比</h3><ol><li><p><strong>方法二更优</strong>：</p><ul><li>当多个镜像共享相同的层时，<code>docker save</code>会自动去重</li><li>生成的单个tar文件只包含唯一的镜像层，共享层不会重复存储</li><li>例如如果image1和image2都基于相同的Ubuntu基础层，这个基础层只存储一次</li></ul></li><li><p><strong>方法一更耗空间</strong>：</p><ul><li>每个<code>docker save</code>命令会独立保存完整的镜像（包括所有层）</li><li>之后再用tar打包时，相同的镜像层会被重复存储</li><li>例如如果三个镜像共享同一个基础层，这个基础层会被存储三次</li></ul></li></ol><h3>实际测试示例</h3><p>假设有三个镜像：</p><ul><li>image1 (包含基础层300MB + 专属层100MB)</li><li>image2 (包含相同基础层300MB + 专属层200MB)</li><li>image3 (独立镜像500MB)</li></ul><p><strong>方法一</strong>结果：</p><ul><li>image1.tar = 400MB</li><li>image2.tar = 500MB</li><li>image3.tar = 500MB</li><li>最终all_images.tar ≈ 1.4GB (基础层被存储了两次)</li></ul><p><strong>方法二</strong>结果：</p><ul><li>all_images.tar ≈ 300MB(基础层) + 100MB + 200MB + 500MB = 1.1GB</li></ul><h3>其他优势</h3><p>方法二还有额外优点：</p><ol><li>操作更简单（单条命令完成）</li><li>加载更方便（单条<code>docker load</code>命令）</li><li>更好地保持镜像间的依赖关系</li></ol><h3>结论</h3><p><strong>推荐总是使用方法二</strong>（<code>docker save -o output.tar img1 img2 img3</code>），除非你有特殊需求需要保持镜像完全独立存储。</p><h2>如何用脚本迁移</h2><p>很简单，将镜像列表写入一个文本文件(e.g. imagelist）中：</p><ul><li><p>imagelist</p><pre><code class="bash">nginx:v1.20.1
redis:v5.0
mysql:v8.0.21</code></pre><p>执行脚本,即可将全部镜像保存：</p></li></ul><pre><code class="bash">docker save -o all_images.tar $(cat imagelist.txt)</code></pre><h2>加载镜像</h2><p>要从组合的tar文件中加载镜像：</p><pre><code class="bash">docker load -i all_images.tar</code></pre>]]></description></item><item>    <title><![CDATA[从“访答”出发，深入解析AI对话技术如何]]></title>    <link>https://segmentfault.com/a/1190000047447509</link>    <guid>https://segmentfault.com/a/1190000047447509</guid>    <pubDate>2025-12-03 22:05:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>从“访答”出发，深入解析AI对话技术如何重塑信息获取方式</h2><p>在信息爆炸的时代，我们每天都被海量的数据包围。如何高效、准确地获取所需信息，成为了现代人面临的一大挑战。传统的搜索引擎虽然强大，但往往需要我们不断地筛选、点击，过程繁琐且耗时。近年来，一种名为“AI对话”的技术正悄然改变这一现状，而像这样的平台，正是这一技术浪潮中的杰出代表。</p><h3>什么是AI对话技术？</h3><p>AI对话技术，核心是基于大规模语言模型的人工智能系统。它能够理解和生成人类自然语言，与我们进行多轮、连贯的对话。这背后的原理，可以通俗地理解为：AI通过“阅读”了互联网上浩如烟海的文本资料，学习了语言的规律、逻辑和知识，从而具备了“对答如流”的能力。</p><p>与传统的“关键词搜索”不同，AI对话更像是在与一位博学的伙伴交流。你无需纠结于精准的关键词，只需用你最习惯的语言描述你的问题，AI就能理解你的意图，并给出直接、整合性的答案。</p><h3>AI对话技术如何工作？</h3><p>为了更好地理解这项技术，我们可以将其工作流程拆解为三个核心步骤：</p><h4>1. 意图理解</h4><p>当你提出一个问题，如“如何学习编程？”，AI首先会分析这句话的语义。它会识别出你的核心意图是“学习”，领域是“编程”，并可能进一步推断你需要的是学习路径、资源推荐还是方法论。这个过程远比简单的关键词匹配要复杂和智能。</p><h4>2. 信息整合与推理</h4><p>理解了你的意图后，AI会从其庞大的知识库中检索相关信息。但它的能力不止于此。它会对这些信息进行提炼、总结、甚至跨领域关联，最终生成一个逻辑清晰、内容全面的新答案，而不是简单地罗列网页链接。</p><h4>3. 自然语言生成</h4><p>最后，AI会将整合好的信息，用流畅、易懂的人类语言组织成段落或列表呈现给你。这个答案通常是独一无二的，是针对你特定问题“量身定制”的。</p><h3>AI对话的优势与应用场景</h3><p>相比传统搜索，AI对话技术带来了革命性的体验提升：</p><ul><li><strong>效率极高</strong>：直接获得答案，省去大量筛选信息的时间。</li><li><strong>交互自然</strong>：支持多轮对话，可以不断追问、澄清，像真人聊天一样。</li><li><strong>理解力强</strong>：能够处理复杂、模糊的问题表述，理解上下文语境。</li></ul><p>其应用场景非常广泛：</p><ul><li><strong>快速答疑</strong>：无论是学术问题、生活常识还是工作难题，都能快速获得解答。</li><li><strong>内容创作</strong>：辅助进行文案撰写、头脑风暴、大纲拟定等。</li><li><strong>学习辅导</strong>：充当私人 tutor，解释复杂概念，提供学习建议。</li><li><strong>代码编程</strong>：协助调试代码、解释技术原理、生成代码片段。</li></ul><h3>以“访答”为例，看AI对话的实际体验</h3><p>在众多AI对话应用中，提供了一个很好的范例。用户只需输入问题，它便能迅速给出结构清晰、内容详实的回答。这种体验极大地降低了信息获取的门槛，让每个人都能轻松地与知识库对话。无论是学生、研究人员还是普通职场人士，都能从中受益，将更多精力投入到创造性工作中，而非繁琐的信息筛选上。</p><h3>展望未来：AI对话技术的挑战与机遇</h3><p>尽管AI对话技术发展迅猛，但它仍面临一些挑战，例如信息的时效性、可能存在的“幻觉”（即生成不准确的信息）以及对复杂逻辑推理的局限性。未来的发展将集中于提升模型的准确性、实时性和专业性。</p><p>可以预见，随着技术的不断完善，AI对话将更深地融入我们的工作和生活，成为像水电煤一样的基础设施。它将不仅仅是回答问题的工具，更会是激发灵感、辅助决策的智能伙伴。</p><h3>结语</h3><p>从关键词搜索到智能“访答”，我们获取信息的方式正在经历一场深刻的变革。AI对话技术以其自然、高效和智能的特点，正在重新定义人与信息的交互边界。拥抱这项技术，意味着我们选择了一条更便捷、更聪明的求知之路。不妨亲自体验一下像这样的平台，感受AI为你带来的信息获取新体验。</p><p><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdnfqx" alt="" title=""/></p>]]></description></item><item>    <title><![CDATA[如何高效且优雅地批量处理会话更新？ bl]]></title>    <link>https://segmentfault.com/a/1190000047447527</link>    <guid>https://segmentfault.com/a/1190000047447527</guid>    <pubDate>2025-12-03 22:04:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>本文由<a href="https://link.segmentfault.com/?enc=gT0V9pUzcPqAAke4ogI%2FNQ%3D%3D.yBzIAtH7aYVNAMC%2BegOcMe%2Feix21t%2BqZpzkl3j3KZAE%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[2025年IPD软件选型全攻略：7大核心]]></title>    <link>https://segmentfault.com/a/1190000047447533</link>    <guid>https://segmentfault.com/a/1190000047447533</guid>    <pubDate>2025-12-03 22:03:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>2025年，IPD（集成产品开发）热度再升，但“市场驱动、跨部门协同、端到端质量门控”要真正跑起来，离不开一套能把“流程建模、需求与市场管理、阶段评审Gate、多项目组合、RACI责任矩阵、研发效能可视化、知识资产复用、PLM闭环”一口气打通的项目管理工具。</p><p>飞书项目行业专版、Atlassian生态（Jira+Confluence+Zephyr）、Oracle Primavera P6、钉钉Teambition谁更能接住IPD的复杂需求？本文用一篇讲透4款主流平台的Workflow引擎、IPD Gate机制、低代码迭代、资源冲突化解、系统集成与开放性，并送上避坑清单：轻量级任务管理、缺少流程定义、单维度计划、物料/变更管理薄弱、不支持阶段评审或需求质量闭环的工具，建议先放“观察区”，别让它们直接成为落地拦路虎。</p><h3>如何判断一个项目管理工具是否真正支持 IPD 流程？</h3><h4>1. 流程建模与阶段评审机制</h4><p>IPD 的灵魂是“阶段评审”（Stage-Gate）。<br/>在每个阶段结束时，项目都需要通过 TR（技术评审）或 DCP（决策评审）进行审查，以确定是否继续投入。没有“阶段 + 评审”的项目管理工具，只能做任务排期，无法实现 IPD 的“阶段门控”决策机制。<br/>优秀的 IPD 工具应具备：</p><ul><li>可自定义阶段模型（概念、立项、开发、验证、发布等）</li><li>可配置评审模板与审批流程</li><li>节点可追溯、任务联动、自动提醒机制</li></ul><h4>2. 需求与市场管理模块</h4><p>IPD 强调“以市场为起点”。<br/>这要求工具能承接从市场机会到产品规划的完整需求链路。通过这类功能，企业能从“做项目”转向“做正确的产品”，确保研发方向与市场战略一致。<br/>核心能力包括：</p><ul><li>需求池管理、优先级评估与需求分发</li><li>客户与市场信息管理</li><li>需求与立项、产品规划的自动关联</li></ul><h4>3. 多项目/产品组合管理</h4><p>IPD 涉及多个产品线与平台项目，资源与预算如何分配是关键。<br/>工具应支持：</p><ul><li>项目集或产品组合（Portfolio）视图</li><li>资源、风险、ROI 可视化</li><li>优先级评估与战略对齐分析</li></ul><h4>4. 跨部门角色与责任矩阵</h4><p>IPD 是典型的矩阵式协作组织，涉及市场、研发、测试、制造、供应链等多个职能。<br/>工具需要做到：</p><ul><li>支持多角色协作（PM、PL、PDT、职能主管等）</li><li>明确任务责任、审批人、决策人</li><li>流程信息可同步、权限边界清晰<br/>这类功能决定了一个组织是否能真正实现“一个团队一条线”，而不是部门各自为政。</li></ul><h4>5. 研发过程与效能可视化</h4><p>IPD 推崇“用数据驱动改进”，不是靠经验判断。数据可视化让 PMO 能够量化问题、追踪瓶颈，并形成组织级改进闭环。<br/>工具应具备：</p><ul><li>项目进度、风险、问题可视化仪表盘</li><li>研发效能指标自动生成（如需求交付周期、返工率、缺陷密度）</li><li>支持数据驾驶舱与汇报模板</li></ul><h4>6. 知识资产与经验复用</h4><p>IPD 要求组织持续复用知识，而不是每个项目都“重新造轮子”。这不仅能让经验得以沉淀，还能让新项目在启动时复用成熟流程和最佳实践。<br/>关键能力包括：</p><ul><li>评审文档、设计方案、测试报告的归档与检索</li><li>项目模板与流程模板复用</li><li>与知识库或Wiki系统打通</li></ul><h4>7. 系统集成与开放性</h4><p>一个成熟的 IPD 数字化体系通常需要连接 PLM、ERP、CRM、测试平台等多系统。系统的开放性决定了企业能否实现端到端的研发全景视图。<br/>工具需要支持：</p><ul><li>开放 API 与第三方集成</li><li>单点登录（SSO）与统一权限控制</li><li>与 PLM、Jira、Feishu、钉钉等协同工具无缝衔接</li></ul><h2>一、飞书项目行业专版：协同一体化的 IPD 落地利器</h2><p>飞书项目行业专版通过与飞书生态深度集成，构建了 “流程可视化 + 评审数字化 + 协同一体化” 的 IPD 解决方案，尤其适配中大型科技与制造企业。</p><h3>核心功能</h3><h4>1. 泳道式流程可视化管理</h4><p>支持按部门、职能或 IPD 阶段（概念 / 计划 / 开发 / 验证 / 发布）配置专属泳道，直观呈现任务流、责任流与进度流。例如在产品开发阶段，可清晰追踪设计、研发、测试等跨部门任务的流转状态，管理者能快速定位流程卡点。<br/><img width="723" height="480" referrerpolicy="no-referrer" src="/img/bVdmPQD" alt="" title=""/></p><h4>2. WBS 子流程能力</h4><p>基于 WBS 子流程能力，将大项目拆解为多个子流程（例如：需求子流程、验证子流程、量产子流程等）。针对那些需要深入多层级拆解的活动，将关键节点转化为“子流程”，这些子流程能够作为二级项目继续细化工作流，确保每一步都达到足够的详细度和可管理性。<br/><img width="723" height="426" referrerpolicy="no-referrer" src="/img/bVdnfpG" alt="image.png" title="image.png" loading="lazy"/></p><h4>3. 集成化评审与门控机制</h4><p>将 IPD 关键评审点（DCP 决策评审、TR 技术评审）内置为流程节点，支持在线提交评审材料、多角色并行评审、结论自动归档。相比传统线下评审，效率提升 40% 以上，且所有评审痕迹可追溯。<br/><img width="723" height="474" referrerpolicy="no-referrer" src="/img/bVdmPQH" alt="" title="" loading="lazy"/></p><h4>4. 低代码流程迭代与数据分析</h4><p>管理员可通过后台无代码配置调整 IPD 流程模板、审批节点与自动触发规则，适配企业流程优化需求。内置 BI 仪表盘实时展示里程碑完成率、评审通过率等核心指标，为流程迭代提供数据支撑。<br/><img width="723" height="390" referrerpolicy="no-referrer" src="/img/bVdnfpJ" alt="image.png" title="image.png" loading="lazy"/></p><h4>选型关键信息</h4><ul><li>价格：行业专版 165 元 / 人 / 月，包含旗舰版全部功能</li><li>适配规模：大型企业／复杂产品／多项目组合／全生命周期产品开发 (典型 IPD 场景)</li><li>优势：与飞书 IM、文档、日历无缝集成，消除信息孤岛；支持制造业、软件等多行业模板裁剪</li></ul><h2>二、Atlassian 生态（Jira+Confluence+Zephyr）：灵活可扩展的 IPD 协同方案</h2><p>Atlassian 通过 Jira（项目跟踪）、Confluence（文档协作）、Zephyr（测试管理）的组合，构建了高度自定义的 IPD 落地生态，适配软件研发型企业。</p><h3>核心功能</h3><h4>1. IPD 工作流自定义配置</h4><p>Jira 支持通过无代码编辑器配置 IPD 各阶段工作流，例如 “概念立项→方案设计→研发测试→发布上线”，可设置任务触发规则（如 “TR 评审通过后自动启动开发任务”）。<br/><img width="723" height="381" referrerpolicy="no-referrer" src="/img/bVdnfqL" alt="image.png" title="image.png" loading="lazy"/></p><h4>2. 文档与项目双向联动</h4><p>Confluence 作为 IPD 知识库，可与 Jira 任务直接关联，研发人员在任务页面即可访问需求文档、设计方案；文档更新时自动同步至关联任务，避免信息滞后。<br/><img width="723" height="414" referrerpolicy="no-referrer" src="/img/bVdnfqM" alt="image.png" title="image.png" loading="lazy"/></p><h4>3. 全链路研发与测试集成</h4><p>集成 Zephyr 实现 IPD 测试流程闭环，可从 Jira 任务直接创建测试用例、执行测试计划并反馈缺陷，缺陷状态与研发任务实时同步，缩短问题修复周期 。</p><h4>选型关键信息</h4><ul><li>价格：Jira 软件版 14 美元 / 人 / 月，Confluence 标准版 10 美元 / 人 / 月</li><li>适配规模：20-2000 人研发团队</li><li>优势：插件生态丰富（支持与 Git、CI/CD 工具集成），高度灵活</li><li>局限：需人工梳理 IPD 流程模板，缺乏原生行业适配</li></ul><h2>三、Oracle Primavera P6：大型复杂 IPD 项目管控专家</h2><p>Oracle Primavera P6 作为国际知名的项目管理工具，以强大的多项目协同与进度管控能力，成为航空航天、工程建设等复杂 IPD 项目的首选，尤其适配多团队、长周期的产品开发流程。</p><h3>核心功能</h3><h4>1. 多层级 IPD 项目计划与进度管控</h4><p>支持按 IPD 战略层、项目集层、项目层构建多层级计划体系，可细化至任务级别的进度管控，通过关键路径法（CPM）自动识别影响 IPD 里程碑的关键任务，当任务延期时实时预警并提供调整方案。<br/><img width="723" height="382" referrerpolicy="no-referrer" src="/img/bVdnfqO" alt="image.png" title="image.png" loading="lazy"/></p><h4>2. 多项目资源统一调配与冲突化解</h4><p>具备跨 IPD 项目的资源池管理能力，可按角色、技能、部门维度统计资源需求与使用情况，当多个 IPD 项目争抢同一资源时，系统自动分析资源负荷峰值，推荐资源调配方案或任务优先级调整建议。</p><h4>3. IPD 项目绩效量化分析与汇报</h4><p>内置丰富的 IPD 项目绩效报表模板，支持自定义 KPI 指标（如里程碑达成率、成本偏差率、资源利用率），可生成可视化图表（柱状图、折线图、饼图）用于管理层汇报，同时支持数据导出与第三方 BI 工具集成。<br/><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnfqP" alt="image.png" title="image.png" loading="lazy"/></p><h4>选型关键信息</h4><ul><li>价格：企业版订阅价约 200 美元 / 人 / 月，永久授权起价 10 万元（按用户数计费）</li><li>适配规模：100 人以上大型企业，尤其适合多项目并行的复杂 IPD 场景</li><li>优势：进度管控精度高，支持跨地域、多团队协同；与 Oracle ERP、CRM 系统集成性强</li><li><p>局限：操作复杂度高，需专业培训；小型 IPD 项目使用成本较高</p><h2>四、Teambition（钉钉）</h2><h3>核心功能</h3><h4>1. 项目集 + 甘特图 +资源管理</h4><p>Teambition 支持将多个项目组合成“项目集”，管理者可以将不同产品线、不同业务单元的项目归在一起进行统一视图监控。在 IPD 场景中，这意味着产品组合管理（Product Portfolio Management）中的“多项目、多产品”管理得以工具化，在一个系统里看到所有产品的进展、资源冲突、依赖风险。<br/><img width="723" height="252" referrerpolicy="no-referrer" src="/img/bVdnfqT" alt="image.png" title="image.png" loading="lazy"/></p></li></ul><h4>2. 全链路敏捷研发管理</h4><p>Teambition 不仅支持传统瀑布式项目管理，也适配敏捷／迭代模式。<br/>在 IPD 中，产品的研发往往需要快速原型、迭代验证、风险试错。这个功能支持在产品开发阶段用敏捷方式推进，同时又能与上游的战略规划、下游的制造交付同步管理。</p><h4>3. 系统集成与生态兼容</h4><p>Teambition 支持与其他企业协作平台／系统集成。</p><h4>选型关键信息</h4><ul><li>价格：联系销售获取“定制 / 按需定价”报价</li><li>适配规模：大型跨部门团队</li><li>优势：侧重协作场景与跨部门项目运营，支持丰富的项目模板与企业级流程管控，与钉钉生态协同良好。</li></ul><h3>下列类型的项目管理软件，大多难以真正落地 IPD：</h3><h4>1. 只能做「任务管理」的轻量级工具</h4><p>代表如：Trello、Asana、Notion、Todoist等<br/>典型问题：</p><ul><li>只能管理任务卡片，没有项目生命周期。</li><li>无法支持阶段评审（IPD Gate）。</li><li>没有资源负载管理、成本、风险矩阵、需求分解等复杂能力。</li><li><p>无法形成流程化、跨部门标准的一体化协作。</p><h4>2. 缺乏结构化流程定义（Process/Workflow Engine）的工具</h4><p>代表：许多传统“项目模板型”工具、旧版国产项目平台<br/>典型问题：</p></li><li>不能定义 IPD 的核心流程：需求管理 → 技术评审 → PDR → CDR → 验证 → 移交制造。</li><li>流程无法绑定角色职责（如 PM、PDT Leader、SE、QA 等）。</li><li><p>无法沉淀跨项目复用的结构化流程资产。</p><h4>3. 不支持多团队/多专业协同的单维度计划类工具</h4><p>代表：简单甘特图工具、传统Office工具（Excel/Gantt Chart 模板）<br/>典型问题：</p></li><li>计划只能是单条甘特，不支持系统工程拆解（WBS/BoM/需求分解）。</li><li>不支持多团队并行研发或里程碑依赖链。</li><li><p>无资源管理 → 无法评估并行项目的资源瓶颈。</p><h4>4. 无法管理物料/变更的工具（弱化 PLM 能力）</h4><p>代表：仅支持文档协作、不支持配置/版本管理的通用工具<br/>典型问题：</p></li><li>无法管理功能需求—设计文档—测试—BOM—变更（ECR/ECN）的关联链。</li><li>不能处理跨版本变更和设计冻结。</li><li><p>无产品资料唯一来源（Single Source of Truth）。</p><h4>5. 不支持阶段评审（IPD Gate 机制）的工具</h4><p>IPD 的本质是跨阶段控制：需求评审、PDR、CDR、TR、PRR、试产等。<br/>典型问题：</p></li><li>工具没有 Stage-Gate 管理模型。</li><li>不支持质量/风险/状态的评审检查表。</li><li><p>无体系化的变更管理入口。</p><h4>6. 不能管理需求、风险、质量闭环的工具</h4><p>IPD强调的是结构化管理：</p></li><li>需求管理（Requirement）</li><li>风险管理（Risk）</li><li>问题闭环（Issue → Root Cause → Action → Verification）</li></ul>]]></description></item><item>    <title><![CDATA[Spring Security 集成 C]]></title>    <link>https://segmentfault.com/a/1190000047447535</link>    <guid>https://segmentfault.com/a/1190000047447535</guid>    <pubDate>2025-12-03 22:02:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>近期我们实验室的排课系统需要接入统一身份认证平台，目前业务系统用的是 Spring Security 做登录鉴权。现在学校要求接入他们的统一认证平台，所以我们需要把 CAS 集成进来。</p><p>简单来说，就是：<br/>用户访问业务系统的时候，如果还没登录，就别让他直接访问，而是把他丢到 CAS 的登录页面去，让他先在那里登录一下。</p><h2>CAS</h2><p>CAS（Central Authentication Service）是学校常用的一种统一登录方式。简单来说，就是把所有系统的登录入口都集中到一起管理。以前我们访问不同系统时，每个系统都要重新输入账号和密码，比如教务系统一套密码，图书馆一套密码，排课系统又一套，非常麻烦。</p><p>接入 CAS 之后，这些系统不再自己处理登录，而是把用户统一交给学校的 CAS 服务器来认证。用户只要在 CAS 登录页面成功登录一次，在整个浏览器会话里就可以直接访问所有已接入的系统，完全不用重复输入密码，也不需要每个系统都维护一套登录逻辑。</p><p>简易版流程图</p><p><img width="723" height="976" referrerpolicy="no-referrer" src="/img/bVdnfph" alt="deepseek_mermaid_20251203_fe7ad4.png" title="deepseek_mermaid_20251203_fe7ad4.png"/></p><h2>Spring Security 集成 CAS</h2><p>在我们的项目中，是使用的 Spring Security 来做本地登录验证的，也就是通过用户名和密码在系统内部进行认证，为了对接学校的统一认证身份认证平台，我们需要让系统支持CAS登录，好在 Spring Security 提供了 CAS 的支持，所以整个集成流程并不复杂，只需要按照规范 “拼接”起来即可。</p><p>整体来说，Spring Security 集成 CAS 可以分成三个主要步骤：登录跳转、票据验证、统一退出。</p><h3>Spring Security 本地用户名密码登录流程</h3><p>在正式集成流程之前，我们先要先简单回顾一下项目中 <strong> 原本使用SpringSeurity 的用户名密码登录流程 </strong>，这样就能更加清楚理解 CAS 是如何扩展到现有体系里面的。</p><p><img width="723" height="627" referrerpolicy="no-referrer" src="/img/bVdnfpR" alt="deepseek_mermaid_20251203_43a7e6.png" title="deepseek_mermaid_20251203_43a7e6.png" loading="lazy"/></p><h4>1. 请求进入过滤器链</h4><p>当请求到达 Spring Security 时，会先进入过滤器链进行处理。如果请求中包含 HTTP Basic 认证信息（即请求头 <code>Authorization: Basic ...</code>），<code>BasicAuthenticationFilter</code> 会拦截该请求。</p><p>它会从请求头中解析出用户名和密码，然后将凭证传递给认证管理器进行身份验证。</p><p><img width="723" height="306" referrerpolicy="no-referrer" src="/img/bVdnfpY" alt="image.png" title="image.png" loading="lazy"/></p><h4>2. 提取用户名和密码</h4><p><code>BasicAuthenticationFilter</code> 内部通过 <code>convert(HttpServletRequest request)</code> 方法获取 <code>Authorization</code> 头中的 Basic 信息，并对其进行解码（decoder），从中提取出用户名和密码。</p><p>接着，基于这些凭证构建一个 <code>UsernamePasswordAuthenticationToken</code> 对象，用于后续的认证流程。</p><p><img width="723" height="215" referrerpolicy="no-referrer" src="/img/bVdnfp1" alt="image.png" title="image.png" loading="lazy"/></p><p>由于使用的是 Basic 认证，这里会对 Authorization 头中的信息进行解码（decoder），从中提取出用户名和密码，用于后续的身份验证。</p><p><img width="723" height="248" referrerpolicy="no-referrer" src="/img/bVdnfp7" alt="image.png" title="image.png" loading="lazy"/></p><p>提取用户名和密码。然后，基于这些凭证构建一个 UsernamePasswordAuthenticationToken，用于后续的身份验证流程。</p><p><img width="723" height="180" referrerpolicy="no-referrer" src="/img/bVdnfqj" alt="image.png" title="image.png" loading="lazy"/></p><h4>3. 调用认证管理器</h4><p>构建好的 <code>UsernamePasswordAuthenticationToken</code> 会被传递给 <code>AuthenticationManager</code>。在 Spring Security 中，<code>AuthenticationManager</code> 的默认实现是 <code>ProviderManager</code>。</p><p><code>ProviderManager</code> 会遍历注册的 <code>AuthenticationProvider</code> 列表，并调用与凭证类型匹配的 <code>AuthenticationProvider</code> 来进行认证。在本次流程中，实际调用的是 <code>DaoAuthenticationProvider</code>。</p><p><img width="723" height="316" referrerpolicy="no-referrer" src="/img/bVdnfqr" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnfqu" alt="image.png" title="image.png" loading="lazy"/></p><h4>4. DaoAuthenticationProvider 核心流程</h4><p><code>DaoAuthenticationProvider</code> 的核心入口是 <code>authenticate(Authentication authentication)</code> 方法。流程如下：</p><ol><li><strong>获取用户名</strong>：首先从认证对象中获取用户名。</li><li><strong>检查缓存</strong>：判断是否已有缓存的用户信息，如果没有缓存，则调用 <code>retrieveUser</code> 方法从 <code>UserDetailsService</code> 加载用户数据（包括密码和权限）。</li></ol><p><img width="723" height="242" referrerpolicy="no-referrer" src="/img/bVdnfqv" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="309" referrerpolicy="no-referrer" src="/img/bVdnfqy" alt="image.png" title="image.png" loading="lazy"/></p><ol start="3"><li><strong>密码校验</strong>：获取到 <code>UserDetails</code> 后，调用 <code>additionalAuthenticationChecks</code> 方法，用配置的 <code>PasswordEncoder</code> 对提交的密码和存储的密码进行比对，如果不匹配，则抛出 <code>BadCredentialsException</code>。</li></ol><p><img width="723" height="302" referrerpolicy="no-referrer" src="/img/bVdnfqK" alt="image.png" title="image.png" loading="lazy"/></p><p><img width="723" height="276" referrerpolicy="no-referrer" src="/img/bVdnfqN" alt="image.png" title="image.png" loading="lazy"/></p><ol start="4"><li><strong>生成认证对象</strong>：校验通过后，调用 <code>createSuccessAuthentication</code> 方法，基于加载到的用户信息和原始认证请求，创建一个已认证的 <code>UsernamePasswordAuthenticationToken</code>。该对象随后被返回给 <code>ProviderManager</code>，最终存入 <code>SecurityContext</code>。</li></ol><p><img width="723" height="280" referrerpolicy="no-referrer" src="/img/bVdnfqV" alt="image.png" title="image.png" loading="lazy"/></p><p>到这里就完成基于整个 Basic 的 用户名密码登录认证流程就完成了</p><p>通过整个 Basic 登录认证流程，我们可以看到 <strong>核心的两个必要组件</strong>：</p><ol><li><strong><code>BasicAuthenticationFilter</code></strong>：负责拦截请求，解析 <code>Authorization</code> 头中的用户名和密码，并构建认证对象。</li><li><strong><code>DaoAuthenticationProvider</code></strong>：负责具体的身份验证逻辑，包括从 <code>UserDetailsService</code> 加载用户信息和校验密码。</li></ol><p>这两个组件配合起来，就完成了完整的 Basic 认证流程。</p><h3>Spring Security CAS登录认证流程</h3><p>通过上面的CAS认证流程，我们猜测在CAS肯定也有核心的几个必要组件：</p><ol><li><strong><code>CasAuthenticationFilter</code></strong>：负责拦截请求，判断是否携带 CAS Ticket 或者是否已登录，如果未登录则重定向到 CAS Server 的登录页面。</li><li><strong><code>CasAuthenticationProvider</code></strong>：负责具体的身份验证逻辑，包括验证从 CAS Server 返回的 Ticket、解析用户信息，并构建已认证的 <code>Authentication</code> 对象。</li></ol>]]></description></item><item>    <title><![CDATA[《Draw Call优化进阶：从资源逻辑]]></title>    <link>https://segmentfault.com/a/1190000047447617</link>    <guid>https://segmentfault.com/a/1190000047447617</guid>    <pubDate>2025-12-03 22:02:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Draw Call的过高并非单纯由资源体量导致，更多是视觉元素的调用逻辑出现了隐性冗余。那些分散在场景各处的独立材质（每棵树的 bark 材质、每片灌木的 leaf 材质）、未做任何分组的静态物体（单个雕花栏杆、独立的石凳），如同杂乱无章的音符在渲染周期中不断跳跃，迫使CPU反复发送指令，每一次指令的触发都伴随着数据加载、状态切换的隐性消耗，最终拖慢了整体运行节奏。这种隐藏在流畅画面表象下的性能内耗，往往比直观的资源过载更难察觉，也更需要开发者跳出“削减数量”的表层思维。</p><p>理解Draw Call的核心矛盾，需要彻底跳出“单纯减少数量”的固化认知，转而探究“调用效率”背后的本质逻辑。Draw Call本质上是CPU与GPU之间的通信桥梁，每一次指令的传递都并非单一的数据发送，而是包含了材质参数加载、Shader状态切换、渲染队列排序等一系列连锁操作，这些隐性流程的消耗往往远超指令本身。在复杂场景开发中，我曾多次观察到一个极具代表性的现象：同样数量的物体，若采用相同材质并进行合理分组，帧率能稳定提升30%以上；而当这些物体分散使用不同材质，即使三角面总数更低，性能也会出现明显下滑。为了验证这一规律，我专门搭建了两组测试场景：A组包含100个立方体，全部使用同一套基础材质，运行时Draw Call稳定在1，帧率维持在60帧；B组同样是100个立方体，每个立方体使用独立材质（仅颜色差异），三角面数与A组完全一致，Draw Call却飙升至100，帧率直接跌至35帧。这一测试结果让我深刻意识到，优化的关键并非盲目削减物体或材质数量，而是通过重构资源的组织方式，减少CPU与GPU之间的交互成本，让每一次调用都能覆盖更多有效渲染内容。这种对“调用逻辑”的深度优化，远比单纯的数量删减更具实操价值，也能最大程度保留场景的视觉设计初衷，避免陷入“为性能牺牲质感”的两难困境。</p><p>资源预处理阶段的优化，需要建立“材质与物体的协同逻辑”，而非孤立调整单个元素的参数。在处理某款开放世界游戏的植被密集场景时，我曾陷入一个典型误区：为了减少材质数量，将乔木、灌木、草本植物的材质盲目合并到一个图集，结果导致Shader的Pass数量从2个激增到5个，GPU的运算压力翻倍，反而出现了帧率骤降的反效果。经过多次试错，我摸索出一套“材质通道分层”的实操策略：先对所有资源的纹理属性进行归类，将基础颜色纹理、法线纹理、粗糙度纹理的分辨率、格式相近的材质归为同一通道，同时严格控制每个通道对应的Shader变体数量，避免GPU在切换时重复加载冗余数据。例如，将所有落叶乔木的相关纹理整合到一个4K图集，常绿乔木的纹理整合到另一个图集，草本植物单独作为一个通道，每个通道的Shader变体数量控制在3个以内，既保证了不同植物的视觉差异化，又让CPU在调用时能一次性处理同通道的所有物体。此外，静态物体的合并也需要把握动态平衡，并非合并越多效果越好。通过在不同硬件设备上的反复测试，我总结出一组临界值数据：移动端场景中，单个静态合并体的物体数量控制在50-80个之间时，数据传输延迟最低；PC端由于硬件性能更强，可将阈值提升至100-150个。若超过这一范围，合并体的文件体积会过大，导致加载速度变慢，反而影响整体性能。这种基于场景类型与硬件适配的动态调整思路，远比遵循固定的合并标准更具实用性，也能更好地应对不同项目的资源特性。</p><p>渲染管线中的协同优化，需要打通“CPU预处理”与“GPU运算”的衔接环节，让两者的工作节奏形成互补而非对抗。很多开发者在优化时容易陷入“单方面发力”的误区，要么只专注于减少CPU的Draw Call发送数量，要么一味简化GPU的Shader运算，却忽略了两者之间的交互损耗。例如，当CPU通过批处理快速发送大量渲染指令时，若GPU因Shader包含过多复杂计算（如多重光照、复杂遮罩）无法及时响应，就会出现“指令堆积”现象，帧率会像断崖式下跌；反之，若GPU资源处于闲置状态，CPU却因资源组织不合理（如静态物体未合并、材质分散）无法高效发送指令，也会造成硬件资源的浪费。针对这一核心矛盾，我采用了“渲染压力分摊”的实战策略：将静态场景的渲染任务集中在CPU预处理阶段完成，通过光照烘焙、静态批次合并、遮挡剔除等操作，减少运行时的实时指令发送；而对于角色、交互道具、动态粒子等动态元素，则采用“动态批处理阈值适配”机制，根据当前帧率的实时状态动态调整批处理的物体数量。当帧率低于目标值（如移动端60帧）时，自动扩大动态批处理的范围，将更多小体量动态物体纳入同一调用；当帧率充足时，则适当缩小范围，保留动态元素的灵活交互性。在某款动作游戏的测试中，通过这一策略，CPU的Draw Call发送频率从每秒300次降至150次，GPU的运算耗时从20ms压缩至10ms，帧率稳定性提升了40%，充分证明了协同优化的核心价值。</p><p>工具辅助下的精准定位，需要跳出“依赖官方分析工具”的局限，建立个人化的性能监测逻辑。官方工具（如Unity Profiler）能提供基础的Draw Call数量、帧率、CPU/GPU耗时等数据，但往往缺乏对“调用原因”的深度解析，难以精准定位问题根源。在长期实践中，我摸索出一套“分层监测+对比验证”的组合方法：首先通过第三方可视化工具（如RenderDoc）观察场景中不同区域的渲染压力分布，以热力图的形式直观呈现Draw Call集中的热点区域；接着将场景拆解为静态场景、动态元素、粒子系统、UI界面等多个模块，逐一关闭某个模块后观察性能变化，初步锁定问题所属范畴；最后通过对比不同资源组合下的性能数据，排查出导致调用过高的核心因素—可能是某类材质的Shader变体过多，也可能是静态物体的分组不合理，或是粒子系统的材质通道与其他资源冲突。例如，在处理一个包含大量动态魔法粒子的RPG游戏场景时，官方工具仅显示Draw Call过高，但无法明确粒子系统与场景中静态建筑的交互影响。通过分层监测，我发现粒子系统的材质使用了Alpha Test通道，而场景中建筑的材质同样依赖该通道，导致GPU在切换时需要频繁重置状态，消耗了大量资源。针对这一问题，我将粒子材质的通道调整为Alpha Blend，与建筑材质的通道形成互补，无需削减粒子数量，就将Draw Call降低了25%，帧率提升了15帧。这种结合工具与手动排查的方式，能更精准地找到优化靶点，避免盲目调整带来的时间浪费。</p><p>优化后的长效维护，需要建立“场景适配性”思维，让优化策略能够应对不同的运行环境与场景迭代需求。很多时候，优化后的场景在某类硬件或特定测试场景下表现良好，但更换设备或扩展场景内容后，性能问题会再次浮现。这就要求性能优化不能是一次性的收尾操作，而需要形成可迭代、可扩展的长效机制。例如，我为经手的项目建立了一套“资源分级标准”：根据硬件性能将设备划分为高端、中端、低端三个等级，为不同等级设备提供差异化的资源包—高端设备保留完整的4K纹理、全量材质与物体数量；中端设备使用2K纹理，适当增加静态批次合并比例；低端设备则采用1K纹理，简化部分非核心装饰性物体。同时，在场景扩展时，制定“Draw Call预算分配”规则：新增内容前，先通过性能监测工具评估现有场景的调用余量，根据新增元素的类型（静态/动态、材质复杂度、交互频率）分配相应的调用配额，确保整体调用数量不超过预设阈值。在某款持续迭代的生存类游戏中，通过这一机制，即使场景内容在一年内增加了40%（新增地图、道具、NPC），Draw Call数量仍稳定控制在目标范围内，不同等级设备的帧率波动不超过5帧。</p>]]></description></item><item>    <title><![CDATA[《低端机硬件适配的非表层方案》 程序员阿]]></title>    <link>https://segmentfault.com/a/1190000047447620</link>    <guid>https://segmentfault.com/a/1190000047447620</guid>    <pubDate>2025-12-03 22:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>那些在中端设备上流畅呈现的纹理细节、模型层次，到了低配硬件上便会出现纹理加载迟滞、模型面数加载不全的现象，部分区域甚至会出现纹理花屏、模型轮廓断裂的视觉断层，并非硬件性能完全无法支撑，而是显存资源的分配逻辑与硬件承载能力出现了深层错位。很多开发者习惯沿用统一的资源标准，将高清纹理、高面数模型直接部署到全平台，却忽略了低端机普遍存在的显存带宽窄、容量有限的核心痛点—多数低端机型的显存容量仅为中端设备的一半甚至三分之一，且显存与内存共享带宽的设计，进一步加剧了资源加载时的传输压力。这种粗放的资源部署方式，导致资源加载时的显存占用峰值远超硬件阈值，进而引发视觉呈现的断层感，而这种问题往往难以通过简单的参数调整解决。这种隐藏在资源背后的显存消耗，往往比直观的帧率问题更难排查，需要从资源本质、加载逻辑、硬件适配三个维度进行深度重构，才能在不牺牲核心视觉体验的前提下，让复杂场景在低端机上实现稳定运行，真正打破硬件限制带来的场景落地困境。</p><p>理解低端机显存不足的核心矛盾，需要跳出“单纯压缩资源”的表层认知，转而探究“显存占用的动态平衡”。显存的消耗并非仅由资源体积决定，更与资源的加载时机、驻留时长、复用效率密切相关，这三个因素共同构成了显存占用的动态循环。在实践中发现，同样大小的纹理，若在场景加载时一次性涌入显存，与分阶段、按需加载相比，前者引发的显存压力会高出数倍—一次性加载会导致显存占用瞬间飙升至峰值，而低端机的显存管理机制对峰值压力的耐受度极低，极易触发资源加载异常。更关键的是，低端机的显存管理机制更倾向于快速释放闲置资源，但若资源加载缺乏规划，频繁的加载与释放反而会造成显存碎片，这些碎片化的空闲空间无法被系统有效整合利用，看似整体空闲的显存容量，实际可用部分却持续缩水。通过对多款低端机型的实测观察，当显存碎片率超过30%时，即使实际占用未达硬件上限，也会出现资源加载失败、纹理缺失等隐性问题。这一发现让优化思路从“削减资源体积”转向“重构显存使用逻辑”，通过合理规划资源加载顺序、提升复用率、减少碎片生成，让有限的显存空间发挥最大效用，而非简单粗暴地降低资源质量，这种底层逻辑的转变，是解决低端机显存问题的核心前提。</p><p>资源预处理阶段的显存优化，需要建立“纹理梯度适配+模型拓扑精简”的双维策略，而非孤立调整单个资源参数。纹理作为显存消耗的核心模块，传统的统一压缩格式已无法适配低端机的硬件特性，不同格式在低端GPU上的解码效率与显存占用差异显著。实践中摸索出“用途分级压缩”思路：将场景远景纹理采用高压缩比格式，在保证视觉辨识度的前提下，通过算法优化将单张纹理的显存占用压缩至原有的三分之一，同时避免压缩导致的色彩失真；中景纹理采用适应性压缩，根据纹理的细节密度动态调整压缩等级，保留核心细节的同时控制体积；近景交互纹理则适度降低分辨率，通过叠加低占用的细节贴图补充质感，避免因过度压缩导致的视觉模糊。模型优化方面，跳出“单纯减面”的误区，转向“拓扑精简”—通过分析模型的视觉权重与交互需求，保留模型关键轮廓与交互区域的面数，对非视觉焦点、非交互部分进行结构化简化，例如将建筑墙面的复杂浮雕转化为纹理细节，将装饰性的小构件整合为整体模型，既减少模型占用的显存空间，又不影响整体视觉效果。同时，针对低端机的显存带宽限制，将多个小体积纹理整合为纹理图集，减少纹理采样时的显存带宽占用，通过多款低端机型的实测验证，这种组合策略能让纹理模块的显存消耗降低40%以上，且视觉损失控制在用户可接受范围，实现了资源质量与显存占用的动态平衡。</p><p>加载逻辑的优化核心，在于构建“显存热置换+按需加载”的动态管理机制，让资源在显存中实现高效流转。很多场景的显存压力并非来自持续运行状态，而是集中在场景切换、资源加载的峰值时刻，低端机的显存处理速度有限，若一次性加载整个场景的资源，显存占用会瞬间飙升，超出硬件承载阈值，引发一系列视觉问题。针对这一问题，实践中采用“场景分块加载+资源优先级排序”策略：将场景按照空间逻辑划分为多个独立的模块，每个模块设置明确的加载边界，进入某一模块时仅加载当前模块的核心资源，后续模块的资源在后台以低带宽占用的方式缓慢预加载，避免占用过多显存带宽；离开当前模块后，非核心资源自动从显存中释放，为新模块资源腾出空间，形成“加载-使用-释放”的闭环。同时，建立资源优先级体系，将角色、交互道具等高频使用资源设为高优先级，长期驻留显存；远景、装饰性元素等低频资源设为低优先级，按需加载并及时释放。此外，引入“显存缓存池”概念，将常用的重复资源（如通用道具、基础植被）存入缓存池，避免重复加载导致的显存浪费与碎片生成，当场景中需要再次使用该资源时，直接从缓存池调用，无需重新加载。通过这种动态管理机制，场景加载时的显存峰值可降低50%，有效避免了低端机因峰值过高引发的资源加载异常，让场景切换过程更流畅。</p><p>渲染管线的适配优化，需要打通“显存占用与渲染效率”的协同壁垒，让两者形成互补而非对抗。低端机的GPU运算能力有限，若渲染管线中包含过多依赖显存交互的流程，会进一步加剧显存压力，形成“显存占用高-渲染效率低”的恶性循环。实践中发现，传统的实时光照计算、复杂后处理效果，不仅消耗GPU算力，更会频繁调用显存中的纹理、深度缓存数据，导致显存带宽饱和，进而拖慢整体运行速度。优化思路从“削减渲染效果”转向“重构渲染流程”：将大部分静态光照效果通过烘焙预计算，存储为光照纹理，避免实时计算对显存的频繁访问，同时烘焙后的光照数据占用显存更低，且无需实时运算；动态光照则采用轻量化方案，减少光源数量并严格控制光照影响范围，降低显存中光照数据的存储与调用成本。后处理方面，摒弃对显存占用较高的复杂效果，转而采用低成本的色彩校正、边缘锐化等方案，或根据设备性能动态开关后处理模块—低端机自动关闭非核心后处理，中端机保留基础效果。同时，调整渲染队列顺序，将透明物体、粒子系统等高频访问显存的元素集中渲染，减少显存数据的切换频率，提升访问效率。通过这套管线适配策略，既能有效控制显存占用，又能保证渲染效率，让低端机在有限的硬件资源下实现流畅的视觉呈现，打破“显存不足则必须牺牲效果”的固有认知。</p><p>工具辅助下的精准优化，需要建立“显存占用可视化+硬件特性适配”的个人化排查逻辑。官方工具虽能提供显存占用数值，但缺乏对资源类型、加载时机的深度拆解，难以精准定位显存消耗的核心来源，往往导致优化动作盲目低效。实践中摸索出“分层监测+硬件画像”的组合方法：通过第三方可视化工具，以热力图形式直观呈现不同资源（纹理、模型、缓存数据）的显存占用比例，快速定位消耗大户；再将场景拆解为核心玩法区、远景装饰区、交互道具区等多个模块，逐一关闭某个模块后观察性能变化，初步锁定问题所属范畴；最后结合低端机的硬件特性画像，分析其显存带宽、容量、支持的纹理格式、GPU解码能力等关键参数，针对性调整优化策略。例如，某类低端机不支持高规格的纹理压缩格式，强行使用会导致显存占用翻倍，此时需切换为兼容格式并辅以分辨率调整；部分机型的显存与内存共享带宽，过度加载资源会导致带宽争抢，需通过减少资源加载频率、提升复用率缓解压力；还有些机型对复杂模型的顶点数据处理效率低，需进一步优化模型拓扑结构。通过这种工具与硬件特性结合的排查方式，能避免盲目优化带来的时间浪费，精准命中显存消耗的核心痛点，让每一次优化动作都能产生实际效果，显著提升低端机的显存利用效率。</p><p>优化后的长效适配机制，需要建立“多维度硬件分级+动态资源调度”的迭代体系，应对低端机的多样性与场景迭代需求。不同品牌、型号的低端机，其显存容量、带宽、支持的技术特性存在显著差异，单一的优化方案无法覆盖所有场景，极易出现“某款机型适配良好，另一款机型仍有问题”的情况。实践中建立“硬件分级标准”，根据显存容量、GPU型号、带宽表现将低端机划分为基础级、进阶级两个梯队，为不同梯队制定差异化的资源配置：基础级机型采用最低梯度的纹理分辨率、最精简的模型面数、关闭大部分后处理，仅保留核心视觉元素；进阶级机型则适度提升资源规格，保留核心视觉效果，确保体验一致性。同时，建立“显存预算动态分配”规则，场景迭代时，先通过专业工具评估新增资源的显存占用成本，根据硬件分级为不同梯队分配相应的显存配额，确保新增内容不会超出硬件承载上限。此外，定期收集不同低端机型的运行数据，建立用户设备数据库，分析显存占用的变化趋势，持续优化资源加载逻辑与压缩策略—若某类机型的显存碎片率居高不下，便调整资源释放机制；若某款机型对特定纹理格式支持不佳，便针对性替换格式。</p>]]></description></item><item>    <title><![CDATA[企业人员安全意识解决方案 帮助企业构建可]]></title>    <link>https://segmentfault.com/a/1190000047447460</link>    <guid>https://segmentfault.com/a/1190000047447460</guid>    <pubDate>2025-12-03 21:03:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="311" referrerpolicy="no-referrer" src="/img/bVdnfpH" alt="image.png" title="image.png"/></p><h4>他们，正在用“习惯”打开安全后门</h4><p>据相关报告显示，企业内普遍存在的高危操作现状令人担忧，具体突出表现在以下三个方面：<br/>数字资产管理混乱：近65%的员工在多个账户间重复使用密码，超过80%的与黑客相关的泄露事件都是利用了弱口令、默认口令等。密码复用、信息随意存放成为常态，密钥管理形同虚设。<br/>基础防护严重缺失：仅有39%的个人账户和53%的启用了多重身份验证（MFA）。第一道安全防线，在源头就已千疮百孔。<br/>安全认知存在偏差：约有58%的员工自认为能够识别并避免安全威胁，但在后续的模拟钓鱼测试中，这些自认为“安全”的员工中超过30%的人会点击恶意链接；更有67%的IT从业者对自己的安全技能过度自信。认知的误区，正在源源不断地转化为真实漏洞。<br/><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnfpI" alt="image.png" title="image.png" loading="lazy"/><br/>以上真实案例并非孤例。它们共同指向一个残酷现实：员工安全意识的缺失，有时比任何技术漏洞都致命。</p><h4>传统培训为何总是“治标不治本”？</h4><p>面对日益狡猾的外部攻击与内部操作风险，多数企业的应对措施却收效甚微，陷入“高风险行为频发 → 防护能力薄弱 → 培训机制缺失”的恶性循环：<br/>依赖一次性线下培训，内容枯燥、形式单一，员工听过即忘，难以转化为行为习惯；<br/>仅靠发放安全手册、张贴警示标语，缺乏实战演练，安全知识沦为 "纸上谈兵"；<br/>即便开展模拟钓鱼测试，也缺乏后续的个性化辅导与复盘，导致员工在同一类问题上反复“踩坑”。<br/>相关数据显示，全球仅有56%的企业提供了系统性的安全意识培训，超过60%的企业未建立强制性培训机制，曾系统性地开展过模拟钓鱼攻防演练的企业更是不足一半。</p><h4>破局之道：从 "被动防御" 到 "主动赋能"</h4><p>企业信息安全体系的技术防线已初步建成，但在“人”这一核心要素的风险治理上，仍存在明显短板。<br/>将员工从风险承受者转化为主动防御者，关键在于打破“培训—遗忘—再培训”的无效循环，构建“认知—实践—反馈—优化”的全流程闭环体系，形成安全能力提升的正向循环。这正是百度安全企业人员安全意识解决方案的核心思路。<br/>我们凭借多年甲方安全实战与生态运营经验，为企业量身打造：<br/><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnfpK" alt="image.png" title="image.png" loading="lazy"/><br/>从精准画像发现薄弱点，到场景化培训促成行为改变，再到长效激励引导主动学习——百度安全可以帮助企业构建可持续的安全意识培养生态，让每一位员工都成为企业网络安全的守护者。<br/>本系列下一篇文章，我们将首次深度揭秘百度内部的完整实践：如何将这一核心思路转化为具体动作，并有效度量其成效。<br/>不想错过？点击 <a href="https://link.segmentfault.com/?enc=9IsJOYr1oG3sAaPBCYUJIw%3D%3D.f1zQIG0Nx3DEPSwgme1N8L5tVPpt%2FgLQeMoLKkaUQwnmXM9i1kJocUHTE5MQWw%2FH" rel="nofollow" target="_blank">https://anquan.baidu.com/product/secAwareness</a> 即可访问百度安全官网，获取人员安全意识解决方案。</p><p>网络安全的本质，是人与人的对抗。<br/>技术是“盾”，人是执盾者，唯有“软硬兼施”，方能构筑真正稳健的防御体系。<br/>百度安全愿与各企业携手，将安全意识内化为每位员工的自觉行动，共同将“人”这一最关键的变量，转化为企业安全防御中最坚固的基石。</p>]]></description></item><item>    <title><![CDATA[百度大佬拆解办公安全核心威胁，你有中招么]]></title>    <link>https://segmentfault.com/a/1190000047447467</link>    <guid>https://segmentfault.com/a/1190000047447467</guid>    <pubDate>2025-12-03 21:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月26日晚，百度安全联合墨菲安全通过直播的形式围绕 “企业办公安全关键场景实战” 展开深度分享。先来看看嘉宾介绍和直播内容一览吧:企业办公安全关键场景实战回顾！</p><p><a href="https://link.segmentfault.com/?enc=a6k5HSY2CTHwVUS9TqbJtA%3D%3D.fi8lkeyrapmgdRgcbOpO7BdKZ2%2FzMYcUw1Dlx02qs0ymoLIhChc91SUMuBdbL4Qo" rel="nofollow" target="_blank">https://v.qq.com/x/page/k3181ikfiih.html</a></p><p>直播从办公安全核心构成切入，梳理了六大安全域，拆解了当前企业易中招的四种威胁：银狐黑产团伙攻击、办公软件合规风险、无线安全攻防、办公网数据泄露，用真实案例还原了办公安全的核心难点。让我们一起回顾直播的精彩金句和问答～</p><h3>金句摘选</h3><h4>“办公安全不是单一部门的事，而是全员共建的系统工程。”</h4><h4>“银狐黑产团伙通过搞定员工账号建立信任通道，实施内部诈骗，防不胜防。”</h4><h4>“办公安全的复杂性，不仅仅体现在技术上，更在于如何处理人为行为和潜在的安全风险。”</h4><h3>问答精选</h3><h4>办公安全到底包含哪些核心内容？</h4><p>核心涵盖终端安全域、邮件安全域、网络安全域、账号安全域、物理安全域、业务安全域等，包括员工个人入网设备、外接存储都属于需管控的安全资产，任何一个环节失守都可能引发连锁风险。</p><h4>银狐黑产团伙是什么？为什么银狐能屡屡得手？</h4><p>银狐是国内活跃的网络黑产团伙（行业别名：游蛇、谷堕大盗、UTG‑Q‑1000），自 2022 年下半年起高频活动，以仿冒下载站 SEO 投毒、钓鱼邮件、即时通讯投递为主要投放方式，传播远控木马，目标为企业与个人，核心目的是窃密、诈骗与数据贩卖，形成规模化犯罪链条。该团伙擅长伪装成 “税务稽查”“补贴申领” 等可信场景，内部聊天群发钓鱼二维码、钓鱼邮件、仿冒正版软件官网等多渠道传播木马，还会替换内部员工账号建立信任通道，攻击极具隐蔽性。</p><h4>软件合规存在什么风险？</h4><p>软件合规核心风险包括未授权使用盗版软件面临版权诉讼、罚款；违规软件引发数据泄露与处罚；开源组件使用不当导致许可证冲突、漏洞暴露；缺乏合规审计引发供应链安全隐患。</p><h4>无线办公场景有哪些容易被忽视的风险？</h4><p>未开启 802.1X 认证、密码与 SSO 系统一致、弱认证强度都是高危隐患。黑客可通过伪造热点、ARP 欺骗等手段窃取明文账号密码，甚至入侵企业内网。</p><h4>办公安全最难的点在哪里？</h4><p>最难在于“人”。员工的安全意识、设备管控、权限管理缺一不可，需要技术与管理的深度融合，后续深度剖析。</p><h3>直播亮点</h3><p>专家们结合百度内部实践，透露了办公安全建设的核心逻辑——先梳理资产、再评估风险，最后通过安全基线、实时检测、风险治理形成防护体系。后续将详细拆解每个威胁的解决方案，请持续关注百度安全哦～</p>]]></description></item><item>    <title><![CDATA[为千行百业植入“安全基因”！百度加入“内]]></title>    <link>https://segmentfault.com/a/1190000047447477</link>    <guid>https://segmentfault.com/a/1190000047447477</guid>    <pubDate>2025-12-03 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月28日，由紫金山实验室主办的第五届网络空间内生安全学术大会暨IEEE CRESS 2025国际会议在南京启幕。大会由中国通信学会、中国计算机学会、中国汽车工程学会、中国网络空间安全学会指导，紫金山实验室主办，以“AI+生态构建新挑战，安全可信新机遇”为主题，集中展现我国在网络空间内生安全领域的原创突破与产业实践成果。同时，大会正式启动“内生安全生态伙伴计划”，该计划联合了百度、奇安信、深信服等行业领军企业开展深度合作，形成共生共存的产业生态链条，以加速技术创新与成果转化，让内生安全技术更好地赋能千行百业。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnfpW" alt="image.png" title="image.png"/><br/>第五届网络空间内生安全学术大会</p><p>大模型技术的深度应用在释放生产力的同时，也潜藏着多重安全隐患。一旦保护不到位，可能导致用户隐私泄露，甚至被用于诈骗、盗用身份等违法犯罪活动。技术开发过程中，若核心数据或算法被窃取，不仅损害企业利益，还可能被不法分子篡改功能，导致模型输出错误结果，影响医疗、金融等关键领域。这些风险不仅威胁企业及个人权益，还可能破坏社会信任，甚至影响国家安全。因此，需要通过加强数据保护、完善技术架构、制定行业规范等多方面措施，以确保大模型在安全可控的前提下发挥作用。为此，百度提出了大模型安全护栏建设理念，为行业提供了一套系统性的内生安全解决方案，构建功能完备、服务全面的大模型安全护栏产品矩阵，针对大模型场景存在的各类风险，提供一站式的大模型输入、输出安全护栏产品。</p><p><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdnfp2" alt="image.png" title="image.png" loading="lazy"/><br/>内生安全生态伙伴计划</p><p>具体而言，百度大模型安全护栏构建了从云端到边缘侧的立体化防御体系。在云端，系统对文本实施输入输出的全链路管控，依托高精度“红线知识库”与基于权威信源的“信任域RAG”，实现了对敏感问题的精准应答与正向引导，有效避免模型幻觉并符合社会价值观；针对多模态与高级攻击，采用剪枝优化的统一大模型审核方案，在图文融合场景下表现优异，并能通过语义意图与固定模式检测精准识别角色扮演等隐蔽攻击。在端侧，适应端云协同趋势并满足GB/T 45654标准，系统部署了离线安全审核算子，在节省底座模型算力的同时确保离线治理能力，支持用户封禁及敏感词干预，实现了对突发风险的快速响应。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdnfpZ" alt="image.png" title="image.png" loading="lazy"/><br/>AI安全架构</p><p>与此同时，百度将大模型安全评测体系视为保障安全的“生命线”。该体系由海量高质量评测数据集与全流程自动化评测系统构成，不仅全面覆盖通用场景及垂直领域智能体，更能持续吸纳时下最新的风险事件与对抗性样本，保持题库的鲜活性与高对抗性。针对传统人工评测成本高、标准不一的痛点，该体系的核心创新在于引入了微调后的“裁判大模型”进行自动化标注，其准确率已高达95%以上，显著优于人工水平。通过对待测模型的例行化访问与深度评估，系统能快速生成精准报告，为合作伙伴提供科学、高效的安全水位评估，确立模型上线前的最后一道安全防线。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdnfp0" alt="image.png" title="image.png" loading="lazy"/><br/>大模型安全评测框架</p><p>百度的创新实践不仅体现在技术层面，更重要的是我们始终坚持将安全理念融入大模型全生命周期。从数据清洗、安全对齐、内生安全到大模型安全运营，我们构建了一套完整的原生安全体系。特别在跨模态安全治理方面，我们通过视觉理解与文本语义的双重审核，引入区域关注、跨模态对齐等机制，有效提升了对复合内容的风险管控能力。未来，我们将继续携手行业合作伙伴，以技术创新推动大模型安全的健康发展。我们将在大模型安全领域持续投入，为各行各业提供更加专业、可靠的安全服务，助力人工智能产业的可持续发展，为构建更加安全可信的AI应用环境贡献力量。</p>]]></description></item><item>    <title><![CDATA[HarmonyOS 6实操： 来去电展示]]></title>    <link>https://segmentfault.com/a/1190000047447374</link>    <guid>https://segmentfault.com/a/1190000047447374</guid>    <pubDate>2025-12-03 20:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h4>背景介绍</h4><p>今年6月份HDC大会在一个技术展台跟华为架构师交流时他给介绍了HarmonyOS提供的企业服务能力，在用户接听拨打电话时，页面显示已安装企业应用的联系人信息，方便用户识别来去电人信息，快速回应，增强企业内部沟通效率。由于工作场景确实2B业务挺重，听了很感兴趣，之前没有任何手机系统提供这种能力，还专门做了手环设备，用户在接听电话时，手环设备获取通知信息，提取手机号调用服务端获取同事信息提高交流效率。晚上回酒店后第一时间查看了对应文档，接入很简单，能力超强大。</p><h4>系统能力介绍</h4><p>HarmonyOS 从5.0.2(14)开始，提供了CallerInfoQueryExtensionAbility来去电信息查询扩展Ability，提供通话来去电页面显示企业联系人信息的能力。当有外拨电话或者接听来电时，系统回拉起自定义的CallerInfoQueryExtensionAbility，CallerInfoQueryExtensionAbility是轻量级独立子进程，不允许唤醒主进程，进程存在最长时间为2秒，超时后自动销毁。这样设计一方面是出去安全考虑，另一方面出于体验考虑，如果不是独立进程，拉起主进程如果比较耗时的话，可能电话都已经挂断了还没有开始查询用户信息。</p><p>自定义的CallerInfoQueryExtensionAbility实现CallerInfoQueryExtensionAbility中的onQueryCallerInfo方法，onQueryCallerInfo方法会传入播出或接听的手机号，根据手机号查询本地数据库或者网络接口获取手机号对应同事信息，以Promise方式异步返回CallerInfo，CallerInfo包含以下信息：</p><table><thead><tr><th align="left">名称</th><th align="left">类型</th><th align="left">只读</th><th align="left">可选</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">contactName</td><td align="left">string</td><td align="left">否</td><td align="left">否</td><td align="left">联系人姓名：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">employeeId</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">工号：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">department</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">部门：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">position</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">职位：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr></tbody></table><p>根据查询到的业务信息构造CallerInfo返回给系统展示，这样就可以直接看到手机号对应的用户名称等信息。</p><h4>实现方案</h4><h5>申请权限</h5><p>CallerInfoQueryExtensionAbility需求场景面向企业，仅供企业应用开发者接入。企业应用首先需要进行接入申请，企业应用开发者将申请信息发送至公共邮箱<code>agconnect@huawei.com</code>。<br/>邮件标题：【申请公司名】—企业来电显示能力—Developer ID<br/>邮件内容需包括：开发者接入企业来电显示能力的应用使用主体、应用名称、应用ID、应用包名、场景说明（具体描述该应用对应通讯录量级等使用的必要信息）。</p><p>企业联系人信息来去电页面显示能力申请成功后，需要重新<a href="https://link.segmentfault.com/?enc=r6RnU9QOqdsQOF3wLfgpAw%3D%3D.fA2Y1dZ%2BD7NjF2bNpSQFBYl09Sv%2F0h2nuXN2xl4s42Wb%2FmbVETgfrvhrOxEzQQs5a%2BzyBrdnD8dPyJvEhl4OTP7Fse7Dqw1FcRPx8JaytHVudPmcXNpTv1Vl2Xsj2Unv" rel="nofollow" target="_blank">申请调试Profile</a>，在新申请Profile勾选对应权限，并且在DevEco Studio中替换新申请的调试Profile。</p><h5>开发自定义CallerInfoQueryExtensionAbility</h5><p>在工程内创建一个ExtensionAbility类型的自定义组件并继承CallerInfoQueryExtensionAbility，完成onQueryCallerInfo方法的复写，示例代码如下：</p><pre><code>import { CallerInfoQueryExtensionAbility, CallerInfo } from '@kit.CallServiceKit';  
  
export default class MainCallerInfoQueryExtAbility extends CallerInfoQueryExtensionAbility {  
  // 来去电时由系统通话应用主动调用该接口查询企业联系人信息  
  onQueryCallerInfo(phoneNumber: string): Promise&lt;CallerInfo&gt; {  
    //通过手机号请求用户信息  
    return httpPost&lt;CallerInfo&gt;({  
      url:  'https://wodekouwei.com/userInfoByPhone',  
      params: {  
        'phoneNumber': phoneNumber  
      } as Record&lt;string, headerValueType&gt;  
    })  
  }  
}</code></pre><p>接着在应用配置文件module.json5中注册extensionAbilities，</p><pre><code>{
    "extensionAbilities": [
      {
        "name": "MainCallerInfoQueryExtAbility",
        "srcEntry": "./ets/callerinfoquery/MainCallerInfoQueryExtAbility.ets",//表示该Ability对应代码路径
        "type": "callerInfoQuery" //type标签必须设为"callerInfoQuery"，表示该拓展类型为CallerInfoQueryExtensionAbility。
      }
    ]
}</code></pre><h5>打开手机设置</h5><p>接着在调试设备上，前往“电话”，点击右上角的“更多”图标，前往“设置”&gt;“陌生号码和信息识别”，打开对应企业应用的号码识别功能开关，进行调试：<br/><img width="385" height="446" referrerpolicy="no-referrer" src="/img/bVdnfoh" alt="image.png" title="image.png"/><br/><img width="385" height="446" referrerpolicy="no-referrer" src="/img/bVdnfoh" alt="image.png" title="image.png" loading="lazy"/><br/><img width="378" height="793" referrerpolicy="no-referrer" src="/img/bVdnfoi" alt="image.png" title="image.png" loading="lazy"/></p><h4>注意事项</h4><p>一方面，来去电页面或横幅仅展示一个联系人信息，对于多个应用里存在相同联系人的情况，按照应用包名的字典序排序，展示首个查询结果。<br/>另一方面，关于用户信息存储问题，上述示例采用了网络接口查询方式，网络正常情况下2秒可以正常返回，官方示例给了RDB数据库查询方式，通过本地数据库查询就要求必须把所有用户信息都内置在应用中，这样不仅有安全问题而且如果企业规模较大员工较多时也是加重本地存储压力。一般采用接口请求方式，接口要做一些频次等限制也要保证响应速度。<br/>RDB数据库场景需转化context类型 <code>const context = (this.context as common.ExtensionContext).getApplicationContext();</code><br/>转换后使用content获取RdbStore实例：<code>let store = await relationalStore.getRdbStore(context, null);</code></p><h4>总结</h4><p>HarmonyOS 5.0.2及以上版本推出的CallerInfoQueryExtensionAbility，为企业场景提供了高效实用的来电识别解决方案——通过轻量级独立进程机制，在来去电时快速查询并展示联系人姓名、部门、职位等企业信息，精准解决了2B业务中内部沟通的身份识别痛点。该能力接入流程简洁清晰，仅需完成权限申请、扩展Ability开发与配置、手机功能开关开启三步即可落地，同时支持网络接口查询与本地数据库查询两种方式，结合多应用排序规则与响应速度优化建议（优先推荐接口查询），既保障了安全性与体验流畅度，又降低了企业落地成本。对于有内部通讯录管理需求的企业应用而言，这一系统级能力无需额外硬件支持，即可显著提升沟通效率，是鸿蒙生态在企业服务领域的又一实用创新。</p><h4>参考</h4><p><a href="https://link.segmentfault.com/?enc=0cJU8LoMAkhMG9A6FfkrLg%3D%3D.ZUKaGkzvOAHwgZnajG05DZp4g94zw7GU0k8DpH4WCX%2FuZKeQ1canmnsgqWCWM0CIvYgfDXp0h3UEJ%2FAH%2B3PS5GrmszPz4eCk6lH9UxncwWQaJlQMBm2Affs7v%2B8zD%2F1U0lAETdTNWCdarzaWAuj1Rw%3D%3D" rel="nofollow" target="_blank">https://developer.huawei.com/consumer/cn/doc/harmonyos-guides...</a></p>]]></description></item><item>    <title><![CDATA[JAX 训练加速指南：8 个让 TPU ]]></title>    <link>https://segmentfault.com/a/1190000047447415</link>    <guid>https://segmentfault.com/a/1190000047447415</guid>    <pubDate>2025-12-03 20:02:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>TPU 训练的真实效率往往取决于两个核心要素：<strong>Shape 的稳定性</strong>与<strong>算子的融合度</strong>。</p><p>很多时候，JAX 任务之所以出现严重的性能瓶颈，并非算法本身设计有问题，而是忽视了 XLA 编译器与底层硬件对“确定性”的极度偏好。基于大量实战调优经验，本文总结了八条能让 JAX 训练任务从“甚至跑不通”蜕变为“跑满 TPU 算力”的工程经验。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047447417" alt="" title=""/></p><h2>1、尽早锁定 Shape</h2><p>TPU 喜欢静态 Shape，JAX 也是，所以动态 Shape 是性能杀手，它会触发重新编译（Recompile）。一旦发生重编译，Step time 和内存占用都会直接炸裂。所以解决方法也很简单，选定几个规范的尺寸，剩下的全填（Pad）满。</p><p><strong>全局 Batch Size</strong> 要能被 TPU 核心数整除，然后就是对于变长序列，别指望它原本多长就多长，把它 Pad 到几个固定的“桶（Bucket）”里，比如 128、256 或 512，这步工作最好在输入（Input Pipeline）里就做完。</p><p>Python层面的条件判断尽量别依赖 Shape，真要分支逻辑，就老老实实让</p><pre><code>lax.cond</code></pre><p>或</p><pre><code>lax.switch</code></pre><p>来接管。</p><pre><code>     # Example: bucketing &amp; padding (conceptual)  
    def pad_to_length(arr, L):  
        pad = L - arr.shape[0]  
        return jnp.pad(arr, ((0, pad), (0, 0)), mode='constant')  
      
    bucket_sizes = [128, 256, 512]  
    def bucket_len(n):   
        return next(b for b in bucket_sizes if n &lt;= b)  
      
    def preprocess_batch(batch):  
        L = bucket_len(batch["tokens"].shape[1])  
        batch["tokens"] = pad_to_length(batch["tokens"], L)  
        batch["mask"]   = pad_to_length(batch["mask"], L)  
         return batch</code></pre><p>每个 Step 喂给 TPU 的 Shape 只要是固定的，XLA 编译器就不会找麻烦。</p><h2>2、激活值默认用 bfloat16，主权重要 FP32</h2><p>在 TPU 上</p><pre><code>bfloat16</code></pre><p>(bf16) 是个好东西，兼顾了速度、内存和数值稳定性。</p><p>工程上的常规操作是：<strong>激活（Activations）和梯度（Gradients）存成 bf16</strong>。但是，优化器状态里的权重必须保留一份 <strong>FP32 的“主副本”</strong>，不然跑久了数值就会漂移。所欲需要在模型边界做类型转换（Cast）的时候小心点。</p><pre><code>     class MLP(nn.Module):  
        features: int  
        @nn.compact  
        def __call__(self, x):  
            x = x.astype(jnp.bfloat16)     # fast path on TPUs  
            x = nn.Dense(self.features, dtype=jnp.bfloat16)(x)  
            x = nn.gelu(x)  
            x = nn.Dense(self.features, dtype=jnp.bfloat16)(x)  
            return x  
      
    # Optimizer state stays in FP32 (conceptual)  
    params_fp32 = params.astype(jnp.float32)  
    grads_bf16  = compute_grads_bf16(...)  
     updates_fp32 = opt.update(grads_bf16.astype(jnp.float32), opt_state, params_fp32)</code></pre><h2>3、pjit和命名网格：切分要明确，别靠猜</h2><p>JAX 在 TPU 上最强的一点就是通过</p><pre><code>pjit</code></pre><p>实现了 <strong>GSPMD</strong>。你通过 PartitionSpecs 告诉它<strong>想要</strong>什么切分方式，XLA 负责搞定<strong>如何</strong>在设备间搬运数据。</p><p>在 TPU 核心上建个<strong>命名网格（Mesh）</strong>。做数据并行（Data Parallelism）时，用</p><pre><code>PartitionSpec('data', None)</code></pre><p>这种模式。如果模型太大需要张量并行（Tensor Model Parallelism），就加个</p><pre><code>'model'</code></pre><p>轴。</p><pre><code>     import numpy as np  
    import jax  
    import jax.numpy as jnp  
    from jax.sharding import Mesh, PartitionSpec as P  
    from jax.experimental import pjit  
      
    devices = np.array(jax.devices()).reshape(1, -1)  # 1 x N mesh  
    mesh = Mesh(devices, ('data',))  
      
    def loss_fn(params, batch):  
        logits = model_apply(params, batch['x'])  
        return cross_entropy(logits, batch['y'])  
      
    @pjit.pjit(  
        in_shardings=(P(None), P('data')),   # params replicated, batch sharded on 'data'  
        out_shardings=P(None),               # scalar loss replicated  
    )  
    def step(params, batch):  
        grads = jax.grad(loss_fn)(params, batch)  
        # aggregate grads across cores  
        grads = jax.tree.map(lambda g: jax.lax.pmean(g, axis_name='data'), grads)  
        return grads  
      
    with mesh:  
         grads = step(params, sharded_batch)</code></pre><p>切分（Sharding）这事必须<strong>显式</strong>。如果偷懒依赖自动推导，等到后期 debug 那些悄无声息的跨设备数据传输时，绝对会很痛苦。</p><h2>4、jit, vmap, scan 三件套</h2><p>TPU 喜欢大块头的 Kernel，讨厌成千上万个细碎的小算子。训练 Step 和任何中大型计算逻辑，必须用</p><pre><code>jit</code></pre><p>包起来。遇到 Python 循环，如果是时间步逻辑就换成</p><pre><code>lax.scan</code></pre><p>，如果是批次并行就用</p><pre><code>vmap</code></pre><p>。</p><p>把 Loss 计算、梯度计算和参数更新塞进<strong>同一个 jitted 函数</strong>里，这样编译器才有机会把它们融合成一个大算子。</p><pre><code>     import optax  
    import jax  
      
    optimizer = optax.adamw(3e-4)  
      
    def loss_and_grads(params, batch):  
        def _loss(p):  
            logits = model_apply(p, batch['x'])  
            return cross_entropy(logits, batch['y'])  
        loss, grads = jax.value_and_grad(_loss)(params)  
        return loss, grads  
      
    @jax.jit  
    def train_step(state, batch):  
        loss, grads = loss_and_grads(state.params, batch)  
        grads = jax.lax.pmean(grads, axis_name='data')  
        updates, new_opt_state = optimizer.update(grads, state.opt_state, state.params)  
        new_params = optax.apply_updates(state.params, updates)  
         return state.replace(params=new_params, opt_state=new_opt_state), loss</code></pre><h2>5、别让输入管道拖后腿</h2><p>Host 到 Device 的数据传输一旦停顿，吞吐量就掉下来了，所以永远别让计算单元等数据。</p><p>用</p><pre><code>tf.data</code></pre><p>或者高效的 NumPy loader 配合 prefetch。数据预取到设备（Stage to device） 最好做双重缓冲。<strong>全局 Batch</strong> 尽量大（当然要能被核心数整除），数据增强这种脏活累活在 Host 上一次性做完。</p><pre><code>     # tf.data pipeline (conceptual)  
    ds = (tf.data.TFRecordDataset(files)  
          .map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)  
          .batch(global_batch_size, drop_remainder=True)  
          .prefetch(tf.data.AUTOTUNE))  
      
    # Convert to NumPy and prefetch onto devices  
    from flax.jax_utils import prefetch_to_device  
    it = prefetch_to_device(map(npify, ds.as_numpy_iterator()), size=2)  
      
    with mesh:  
        for step_i in range(num_steps):  
            batch = next(it)     # already sharded/prefetched  
             state, loss = train_step(state, batch)</code></pre><h2>6、PRNG要Fold 进 Step 和 Device ID</h2><p>JAX 的 PRNG 是<strong>无状态</strong>的，这意味如果不小心，很容易在不同 Step 或者不同设备上用了一样的随机数 Key。</p><p>每个 Step 都要 Split 一次绝对别复用。所以说为了保证独立性必须把 <strong>Global Step</strong> 和 <strong>Device Index</strong> 都 <strong>Fold</strong> 进去。数据增强/Dropout 的 Key 和参数初始化的 Key 得分开管理。</p><pre><code>     def make_step_rng(rng, step):  
        step_key = jax.random.fold_in(rng, step)  
        dev_key  = jax.random.fold_in(step_key, jax.lax.axis_index('data'))  
        return jax.random.split(dev_key, 1)[0]  
      
    @jax.jit  
    def train_step(state, batch, base_rng):  
        rng = make_step_rng(base_rng, state.step)  
        logits = model_apply(state.params, batch['x'], rngs={'dropout': rng})  
         ...</code></pre><h2>7、Remat，智能 Checkpoint，梯度累积</h2><p>TPU 内存看着大，模型一跑起来就不够用。深层网络可以直接用 Activation Checkpointing（</p><pre><code>jax.checkpoint</code></pre><p>或</p><pre><code>nn.remat</code></pre><p>），用计算换显存。想跑大 Batch 但显存不够，就用梯度累积（Gradient Accumulation） 把它切成小的 micro-step。</p><p>存盘的时候，推荐用 Orbax 做异步、分片（Sharded）的 Checkpoint，稳。</p><pre><code>     from flax import linen as nn  
      
    class DeepBlock(nn.Module):  
        @nn.compact  
        def __call__(self, x):  
            # recompute on backward to trim activation memory  
            f = nn.remat(lambda y: nn.gelu(nn.Dense(x.shape[-1])(y)))  
            return f(x)  
      
    # Gradient accumulation (conceptual)  
    @jax.jit  
    def accum_step(state, batch_slices):  
        def body(carry, micro):  
            state, grad_sum = carry  
            _, grads = loss_and_grads(state.params, micro)  
            return (state, jax.tree_util.tree_map(jnp.add, grad_sum, grads)), None  
        init_grads = jax.tree_util.tree_map(jnp.zeros_like, state.params)  
        (state, grad_sum), _ = jax.lax.scan(body, (state, init_grads), batch_slices)  
        grads = jax.tree_map(lambda g: g / len(batch_slices), grad_sum)  
         ...</code></pre><h2>8、一定要跑 Profiler</h2><p>把关键代码段用 Profiler Annotations 包起来，看 Step Timeline。重点找 Host Waits、Recompiles 和那些没融合好的细碎算子（Small op soup）。</p><p>稳态运行的时候，盯着 Tokens/sec 或者Images/sec，还有硬件利用率。</p><pre><code>     from jax.experimental import host_callback as hcb  
    from jax import profiler  
      
    def tagged(name, fn, *a, **k):  
        profiler.annotate_function(name=name)  
        return fn(*a, **k)  
      
    @jax.jit  
    def train_step(state, batch):  
        profiler.annotate_function(name="train_step")  
        # do work...  
         return state, loss</code></pre><p>一定要在锁定 Shape 并且 JIT 完热点路径之后再做 Profile，不然全是噪音，根本看不到真正的瓶颈。</p><h2>极简 TPU 训练示例</h2><p>这基本包含了上面所有的内容</p><pre><code>     # Pseudo-skeleton (Flax + JAX + TPU)  
    mesh = Mesh(np.array(jax.devices()).reshape(1, -1), ('data',))  
      
    @pjit.pjit(in_shardings=(P(None), P('data'), P(None)), out_shardings=(P(None), P(None)))  
    def train_step(state, batch, base_rng):  
        rng = jax.random.fold_in(base_rng, state.step)  
        rng = jax.random.fold_in(rng, jax.lax.axis_index('data'))  
        def loss_fn(p):  
            logits = model_apply(p, batch['x'].astype(jnp.bfloat16),  
                                 rngs={'dropout': rng})  
            return cross_entropy(logits, batch['y'])  
        loss, grads = jax.value_and_grad(loss_fn)(state.params)  
        grads = jax.tree_map(lambda g: jax.lax.pmean(g, 'data'), grads)  
        updates, opt_state = optimizer.update(grads, state.opt_state, state.params)  
        params = optax.apply_updates(state.params, updates)  
        return state.replace(params=params, opt_state=opt_state, step=state.step+1), loss  
      
    with mesh:  
        for step_i, batch in enumerate(prefetched_iterator):  
            state, loss = train_step(state, batch, base_rng)  
            if step_i % log_every == 0:  
                # Pull back just tiny scalars; keep big tensors on device  
                host_loss = jax.device_get(loss)  
                 print(f"[{step_i}] loss={host_loss:.4f}")</code></pre><h2>总结</h2><p>TPU 需要的是 一致性：稳定的 Shape，融合的 Kernel，目的明确的切分，不掉链子的数据管道，把上面的这八件事做好，写 JAX 训练循环就非常顺畅了。</p><p><a href="https://link.segmentfault.com/?enc=q5rRLGkZW3XZ%2BSspfsk9Ow%3D%3D.PXOD73PGkQZ%2FN72ZBmMGHFo6MiHlLeztSh53Xss8W5uUmmduCcw7WB4JTndzxCDdf9JAZM5zBHwD6LIak3%2B48A%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/16b582a493ba4eca8333314859665dd2</a></p><p>作者:Modexa</p>]]></description></item><item>    <title><![CDATA[AI 时代 HR 的进化与工具赋能 爱跑]]></title>    <link>https://segmentfault.com/a/1190000047447419</link>    <guid>https://segmentfault.com/a/1190000047447419</guid>    <pubDate>2025-12-03 20:02:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI 时代 HR 的进化与工具赋能<br/>AI时代招聘变革：HR的进化之路与工具赋能<br/>在AI技术重塑各行业的当下，招聘领域正经历深刻的座次重排。曾经作为“后台工具”的HR技术，如今已升级为企业核心的业务操作系统。AI不会取代HR，但不懂运用AI的HR，正逐渐被时代拉开差距。<br/>2026年，行业的核心命题不再是“AI是否会夺走工作”，而是“HR能否借助AI制定战略、依托数据做决策、用技术驱动组织发展”。在这场关乎职业未来的变革中，AI技术为HR突破传统工作瓶颈提供了关键支撑，推动招聘从依赖经验的传统模式，迈向精准、高效、人性化的智能新阶段。</p><p>传统招聘的三大痛点：亟待技术破解<br/>长期以来，招聘工作始终被三大核心问题困扰，成为制约企业人才发展的“三座大山”：<br/>•选错人：HR的判断易受时间、经验局限，主观评估导致人才与岗位匹配度不足，给企业带来隐性成本损失。<br/>•效率低：简历筛选、面试安排、信息核实等流程繁琐，占用HR大量时间，难以聚焦核心的人才战略工作。<br/>•体验差：传统面试流程僵化，候选人常面临沟通不顺畅、疑问难解答等问题，影响雇主品牌形象。<br/>第六代AI面试智能体的出现，以“高精准度”和“优体验感”为核心武器，针对性破解这些行业痛点，重塑招聘全链路。<br/>核心突破一：精准评估，让决策有数据支撑<br/>传统招聘中“凭感觉选人”的模式，在AI技术的赋能下被彻底改变。第六代AI面试智能体的评分体系经过多重严格验证，包括客户“背靠背”人机对比实验、效标效度检验、重测稳定性信度验证，评估结果可直接作为用人决策依据，其6.3版本更是标志着该类工具进入国际领先梯队。<br/>这种精准度贯穿招聘评估的每一个环节：<br/>•一问多能：单道题目可同步评估多项胜任力，实现HR初筛与技术复试的无缝衔接，效率提升50%以上。<br/>•自由追问：根据候选人回答即时生成专业问题，如同资深面试官般精准捕捉核心信息，避免能力遗漏。<br/>•简历深度挖掘：自动定位简历中的模糊点与潜在漏洞，通过递进式提问验证信息真伪，填补评估盲区。<br/>•全维度覆盖：无论是沟通协作等通用能力，还是算法工程、财会、编程等专业技能，都能实现全面考察，既解放HR，也减轻专业面试官的负担。<br/>这不再是简单的工具升级，而是专业判断力的规模化复制与输出，让HR不再因经验不足而受制于时间。<br/>核心突破二：拟人化交互，重塑候选人体验<br/>以往的AI面试常因机械、冰冷的流程引发候选人抵触，而第六代AI面试智能体以“有人味”的交互设计，让面试成为雇主品牌的加分项：<br/>•懂情绪的对话：能够识别候选人的语速、情绪变化与潜台词，通过合理引导帮助候选人充分表达，避免因紧张错失展示实力的机会。<br/>•全程无断点：自动识别回答状态，流程衔接自然流畅，如同面对面交流，告别卡顿、跳题的尴尬。<br/>•沉浸式视觉体验：口型与语言节奏精准同步，彻底摆脱“AI纸片人”的违和感，提升面试代入感。<br/>•多轮互动答疑：候选人可主动咨询岗位详情、薪酬福利、发展路径等问题，AI实时回应解答，将候选人好感度前置，让招聘成为双向价值认同的过程。<br/>核心突破三：全流程自动化，实现效率质的飞跃<br/>AI招聘工具的价值不止于面试环节，新一代AI人才寻访智能体构建了“从识人到沟通”的一体化自动化系统，将初筛全链路效率提升10-100倍，大幅降低招聘成本。<br/>该系统具备极强的实用性与便捷性：<br/>•即启即用：30-60秒即可完成初始化，无需复杂配置便能独立运行。<br/>•智能筛选：自主匹配岗位硬性条件，精准筛选简历，减少无效工作量。<br/>•拟人化沟通：自动发起对话，模拟人类语气交流，不合适时即时终止，避免无效沟通内耗。<br/>•全量响应：遍历所有未读消息，逐条进行个性化回复，不遗漏潜在人才。<br/>•信息补全：主动向候选人索取缺失信息与简历，完善人才档案。<br/>•系统同步：自动将候选人资料上传至ATS系统，生成完整档案，实现数据系统化管理。<br/>这一变革标志着招聘工作真正从“经验型判断”走向“数据型决策”，完成了效率与质量的双重飞跃。<br/>AI时代HR的核心竞争力：拥抱工具，进化自我<br/>AI技术的普及，让HR行业的竞争维度发生改变。未来的HR不再需要陷入繁琐的事务性工作，而是要借助先进工具，提升自身的战略判断力与数据决策力，以更快的学习曲线适应行业变化。<br/>AI招聘工具的出现，本质上是为HR赋能，帮助其突破时间与经验的局限，回归人才战略规划、组织文化建设等核心工作，成为推动企业发展的关键力量。拥抱AI，善用工具，已成为HR在新时代立足与进化的必然选择。</p>]]></description></item><item>    <title><![CDATA[基于 STM32 的智能窗户控制系统设计]]></title>    <link>https://segmentfault.com/a/1190000047447430</link>    <guid>https://segmentfault.com/a/1190000047447430</guid>    <pubDate>2025-12-03 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>基于 STM32 的智能窗户控制系统设计与实现【源码分享】</h2><p>智能家居的发展正让越来越多的传统设备焕发生机，而“窗户”作为家庭环境调节与安全防护的重要环节，其自动化与智能化价值也愈发显现。本文将基于 <strong>STM32 微控制器 + ESP8266 Wi-Fi 模块</strong>，设计并实现一个具备环境感知、安全监测、自动控制与远程交互能力的智能窗户控制系统。整个方案以嵌入式设计为核心，兼具工程可实施性和软硬件扩展能力。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447432" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>源码分享</h2><p>免费开源，源码见：</p><blockquote><a href="https://link.segmentfault.com/?enc=GJjq0rLC%2FL23AUgEdy%2BhVw%3D%3D.U0gkMaFdJoOn6Ghk0e7%2F3ipp33A9L1Q5bW0Mnp0ruKUeaM3dx8tIu9WP4X%2Bop%2Fe1lFq%2BsgQBwGVpF7TjpZMYjw%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/weixin_52908342/article/details/155538167</a></blockquote><h3>一、系统总体架构</h3><p>智能窗户控制系统以 STM32 为主控，协同 ESP8266 进行无线通信，实现以下功能：</p><ul><li>构建传感器局域网，采集窗户周边环境数据</li><li>实时监测温湿度、雨滴、风力、光照等环境信息</li><li>监测异常入侵情况，并进行本地或远程告警</li><li>控制电机实现自动开关窗</li><li>使用手机 APP 远程查看数据与控制窗户开合</li><li>提供扩展接口，实现更多场景自动化</li></ul><p>整体结构如下：</p><pre><code>┌─────────────────────────────────┐
│           手机 App / 云端服务      │
└───────────────▲─────────────────┘
                │ Wi-Fi (ESP8266)
┌───────────────┴─────────────────┐
│               ESP8266           │
│  Wi-Fi 通信 / MQTT / HTTP 控制通道  │
└───────────────▲─────────────────┘
                │ UART
┌───────────────┴─────────────────┐
│               STM32              │
│ 传感器管理 | 控制算法 | 电机驱动 | 安防检测 │
│                                     │
│        传感器总线(I2C/ADC/UART)       │
└───────────────┬─────────────────┘
                │
       ┌────────┴──────────┐
       │        │           │
 雨滴传感器   温湿度传感器     光照传感器
 风速模块     霍尔/红外入侵检测  窗户位置检测</code></pre><hr/><h3>二、无线传感器局域网的搭建（ESP8266）</h3><p>为了实现远程控制与数据查看，系统采用 <strong>ESP8266</strong> 作为无线通信模块。实现方式包含两个部分：</p><h4>1. ESP8266 与 STM32 的串口通信协议</h4><p>通过 UART 通信，设计轻量级的数据帧结构，如：</p><pre><code>[Header][Cmd][Len][Payload][Checksum]</code></pre><p>用于实现以下命令交互：</p><ul><li>上传传感器数据</li><li>发送开窗/关窗指令</li><li>状态同步、心跳包</li></ul><h4>2. Wi-Fi 与云端/APP 的通信</h4><p>主流方式包括：</p><ul><li><strong>MQTT</strong>：轻量、实时性强，适合 IoT</li><li><strong>HTTP + REST API</strong>：便于调试和快速集成</li><li><strong>WebSocket</strong>：适合实现实时状态推送</li></ul><p>ESP8266 作为网关，将 STM32 的数据透明地上传至云端，实现室内外双向通信。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447433" alt="Schematic_窗户控制系统 copy_2023-07-14" title="Schematic_窗户控制系统 copy_2023-07-14" loading="lazy"/></p><h3>三、温湿度检测与环境数据采集</h3><p>系统采用常用的温湿度传感器（如 SHT30、DHT20）采集 <strong>室内外温湿度</strong>，并由 STM32 进行以下处理：</p><ul><li>数据滤波：如均值滤波、低通滤波</li><li>数据校准：消除传感器误差</li><li>趋势判断：用于窗户开关策略决策</li></ul><p>例如：<br/>当室外温度低于室内且空气质量好时，可自动开窗通风；<br/>反之，则保持关闭或仅部分开启。</p><p>根据季节与用户习惯，还可以结合配置文件制定不同控制策略。</p><hr/><h3>四、非法入侵检测与驱离机制</h3><p>考虑到窗户也是入侵入口，系统可接入多种检测方式：</p><h4>1. 红外人体检测（PIR）</h4><p>检测近距离移动物体，适合夜间警戒。</p><h4>2. 窗户振动与位移监测</h4><p>通过加速度计/震动传感器检测外力破窗行为。</p><h4>3. 磁性开关/霍尔传感器</h4><p>判断窗户是否被强行开启。</p><p>当检测到异常时：</p><ul><li>本地警告（蜂鸣器、灯光）</li><li>推送警报到手机 APP</li><li>可选择自动关闭窗户</li></ul><p>实现家庭安防的一道额外防线。</p><hr/><h3>五、雨滴、风力、光照检测与天气联动</h3><p>户外天气的快速变化是影响开窗的关键因素，系统通过以下传感器实时监测：</p><h4>1. 雨滴传感器</h4><p>检测降雨，一旦触发立即关窗。</p><h4>2. 风速检测模块（小型风力传感器）</h4><p>风力过大时需限制开窗角度，避免损坏。</p><h4>3. 光照强度传感器（光敏电阻/光照度计）</h4><p>通过光强变化判断时间段或天气情况，有助于完善自动控制策略：</p><p>例如</p><ul><li>光照变弱 + 风雨信号 → 可能即将下雨</li><li>高光照 → 夏季需要减少室外热量进入</li></ul><p>多源数据融合使窗户控制更智能。</p><hr/><h3>六、电机控制与自动开关算法</h3><p>核心执行机构为直流电机或步进电机，通过 L298N、TB6612 或更高效的无刷驱动进行控制。</p><h4>1. 电机结构设计</h4><ul><li>推杆式开窗器：行程大、推力强</li><li>齿轮齿条式：控制精度高</li><li>小型舵机：适用于小窗户</li></ul><h4>2. 自动开关窗算法</h4><p>算法可基于多条件决策，例如：</p><pre><code>if (下雨 OR 风力过大) → 立即关窗
else if (室外温度低于室内 &amp;&amp; 空气质量好 &amp;&amp; 无异常入侵) → 自动开窗
else if (夜间 &amp;&amp; 温度较低) → 保持关闭</code></pre><p>可结合 PID 控制调节开窗角度，也可通过限位开关保证安全。</p><hr/><h3>七、手机 APP 远程控制与可视化</h3><p>通过 ESP8266 将数据上传至云端，APP 可实时查看：</p><ul><li>室内外温湿度</li><li>光照、风力、雨滴状态</li><li>开窗位置和当前状态</li><li>安防告警记录</li></ul><p>用户可远程执行：</p><ul><li>开窗 / 关窗 / 停止</li><li>切换自动/手动模式</li><li>设置窗户开合策略</li><li>启动安防警戒模式</li></ul><p>UI 可使用 Flutter、uni-app 或原生方案开发。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447434" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>总结</h2><p>本文构建了一个 <strong>完整的智能窗户控制系统方案</strong>，涵盖了传感器网络、环境监测、安防检测、电机控制算法、无线通信和远程 APP 交互。<br/>通过 <strong>STM32 + ESP8266</strong> 的组合，使原本普通的窗户具备了环境感知、自动控制与远程操控能力，加速传统家居设备的智能化升级。</p>]]></description></item><item>    <title><![CDATA[实时 vs 批处理：ETL在混合架构下的]]></title>    <link>https://segmentfault.com/a/1190000047447246</link>    <guid>https://segmentfault.com/a/1190000047447246</guid>    <pubDate>2025-12-03 19:05:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字经济加速渗透的今天，数据已成为企业核心竞争力的关键载体。然而，企业在数据处理过程中始终面临着一个核心抉择：是选择实时 ETL满足即时决策需求，还是依赖批处理保障海量数据高效处理？两种模式看似对立，实则各有适配场景 —— 实时处理擅长低延迟响应，批处理则在高吞吐量、低成本运算中占据优势。如何打破模式壁垒，实现 “鱼与熊掌兼得” 的混合架构部署？下面将演示使用ETLCLoud的实时监听多表同步的案例。</p><h3>一、数据源准备</h3><p>在数据源列表中点击新建数据源。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447249" alt="图片 2" title="图片 2"/></p><p>里面提供了大量的数据源模板，这里选择MySQL模板进行创建</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447250" alt="图片 3" title="图片 3" loading="lazy"/></p><p>填写对应的链接配置之后，点击保存并测试。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447251" alt="图片 4" title="图片 4" loading="lazy"/></p><p>提示链接成功即可正常使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447252" alt="图片 5" title="图片 5" loading="lazy"/></p><p>按照同样的步骤创建另一个MySQL数据源</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447253" alt="图片 1" title="图片 1" loading="lazy"/></p><h3>二、数据处理流程</h3><p>来到离线数据集成的流程管理，点击新增流程。这里已经提前建好了CDC同步的流程，然后打开流程设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447254" alt="图片 3" title="图片 3" loading="lazy"/></p><p>从组件列表中拉取库表批量输出组件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447255" alt="图片 4" title="图片 4" loading="lazy"/></p><p>库表批量输出组件配置：</p><p>在基本属性配置里面选择刚才创建的数据源，其他配置默认。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447256" alt="图片 5" title="图片 5" loading="lazy"/></p><p>输出选项的数据更新方式选择合并后批量。其他配置默认，然后点击保存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447257" alt="图片 6" title="图片 6" loading="lazy"/></p><h3>三、监听器配置</h3><p>在实时数据集成界面切换至数据库监听器模块，点击新增监听器创建监听器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447258" alt="图片 7" title="图片 7" loading="lazy"/></p><p>任务配置：</p><p>任务名称和所属分类根据需要填写，所属分类可以在分类管理里创建。支持多种传输模式，这里选择传输到ETL。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447259" alt="图片 8" title="图片 8" loading="lazy"/></p><p>源端配置：</p><p>主要选择源端数据源类型、数据源和要监听的数据库和表。其他的配置默认。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447260" alt="图片 9" title="图片 9" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447261" alt="图片 10" title="图片 10" loading="lazy"/></p><p>目标端ETL：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447262" alt="图片 11" title="图片 11" loading="lazy"/></p><p>启动监听器</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447263" alt="图片 13" title="图片 13" loading="lazy"/></p><p>触发数据变动，查看数据传输情况，可以看到数据监听并同步成功。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447264" alt="图片 14" title="图片 14" loading="lazy"/></p><h3>四、最后</h3><p>在数据量爆炸式增长、业务场景日益复杂的今天，单一的数据处理模式已无法满足企业多元化需求。ETLCloud 将实时处理的敏捷性与批处理的高效性完美融合，不仅解决了企业数据处理的 “两难困境”，更通过技术创新构建起灵活、高效、安全的数据集成体系。未来，ETLCloud 将持续深耕混合架构技术研发，推出更多智能化功能，助力企业在数据驱动的浪潮中抢占先机，实现从 “数据可用” 到 “数据好用” 的价值跃迁，让每一份数据都能精准赋能业务增长。</p>]]></description></item><item>    <title><![CDATA[用“分区”来面对超大数据集和超大吞吐量 ]]></title>    <link>https://segmentfault.com/a/1190000047447282</link>    <guid>https://segmentfault.com/a/1190000047447282</guid>    <pubDate>2025-12-03 19:04:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>1. 为什么要分区？</h3><p><strong>分区（partitions）</strong> 也被称为 <strong>分片（sharding）</strong> ，通常采用对数据进行分区的方式来增加系统的 <strong>可伸缩性</strong>，以此来面对<strong>非常大的数据集或非常高的吞吐量</strong>，避免出现热点。</p><p>分区通常和复制结合使用，使得每个分区的副本存储在多个节点上，保证数据副本的 <strong>高可用</strong>。如下图所示，如果数据库被分区，每个分区都有一个主库。不同分区的主库可能在不同的节点上，每个节点可能是某些分区的主库，同时是其他分区的从库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447284" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h4>1.1 一致前缀读</h4><p>分区也会由于复制延迟而产生问题，我们先来看下图中的例子，是Poons先生和Cake小姐的对话：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447285" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>Poons先生先问： "How far into the future can you see, Mrs.Cake?"</p><p>Cake小姐回答说： "About ten seconds usually, Mr.Poons."</p><p>正常情况下，这段对话是有因果关系的（先问后答）。但是对于观察者，他看到的顺序却是先得到了答案，再看到了问题，这就是在分区数据库中，因复制延迟而产生的特殊情况。</p><p>为了避免这种混乱，我们就需要保证 <strong>一致前缀读</strong>：如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。一种解决方案是，确保任何因果相关的写入都在相同的分区。</p><h3>2. 该怎么分区？</h3><p>分区的目的是将数据和负载均匀的分布到各个节点上，理论上10个节点能够处理10倍的数据量和10倍单节点的读写吞吐量。</p><p>但是如果分区不均，那么就会出现一些分区有更多的数据或读写，我们称之为 <strong>偏斜</strong>，这会使得分区后并没有得到很大的效率提升。在极端情况下，所有的负载如果都落在一个分区，使得该分区负载过高，我们称之为 <strong>热点</strong>。</p><p>所以，为了避免偏斜和热点的产生，以键值数据的分区为例，讨论如何将数据分区做得妥当。</p><h4>2.1 根据键的范围进行分区</h4><p>我们可以根据键值的范围进行分区，比如说我们以26个英文字符划分26个分区，之后根据键值首字母对它们进行分区。通常情况下，键值并不是均匀分布的，这会造成按照首字母分区之后，发生数据偏斜。为了均匀分配数据，分区的边界需要根据数据分区的实际情况再进行调整。</p><h4>2.2 散列分区</h4><p>一个好的散列函数可以将数据均匀分布，避免发生偏斜。但是这也带来了问题：我们没有办法再进行高效的范围查询。</p><h3>3. 热点消除</h3><p>避免热点最简单的方法是将数据记录进行散列分区，记录因此会在所有节点上平均分配。</p><p>但是它并不能完全避免热点的产生，因为如果所有的读写操作都是针对同一个键的话，那么所有的请求还是会被路由到同一个分区。比如说有一个百万粉丝的博主发布动态，该动态根据博主ID的键值进行分区，如果此时有大量的粉丝对该动态进行互动，那么哈希策略会把这些请求都路由到同一个分区进行操作，发生热点事件。</p><p>其实，我们还可以在该热点键上再进行分区，以避免热点：在主键的最后拼接随机数，两位十进制的随机数就能把一个主键分成100个不同的主键，从而存储在不同的分区中，这就完成了热点消除。但是主键被分割后，任何读取工作都必须在每次读取时将所有的数据拉出去合并到一起再返回结果。</p><h3>4. 分区再平衡</h3><p>如果保存某分区数据的服务器故障，需要使用其他服务器接管或想将目前的服务器换成性能更好的服务器，那么就需要进行 <strong>分区再平衡</strong>。</p><p><strong>分区再平衡</strong> 是将负载从集群中的一个节点向另一个节点移动的过程。执行再平衡需要满足以下要求：</p><ul><li>再平衡期间，数据库应该继续接受读取和写入</li><li>节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘的IO负载</li><li>再平衡之后，负载应该在集群中的节点之间公平地共享</li></ul><p>比较简单的再平衡分区策略是选择 <strong>固定数量的分区</strong>，当节点数量增加时，可以从原节点中 <strong>窃取</strong> 一些分区（当节点数量减少时，则发生相反的情况），如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447286" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>在这种配置中，分区的数量通常在数据库第一次建立时确定，操作比较简单，之后不会改变，因此你需要选择足够多的分区以适应未来的增长。但是，每个分区也有管理开销，所以选择太大的数字会适得其反。</p><p>除此之外也可选择 <strong>动态分区</strong>，根据配置的分区大小，当超过该阈值时，可以将该大分区分割成两个小分区，能够使 <strong>分区数量适应总数据量</strong>。在大型分区拆分后，可以将其中的一半转移到另一个节点上，以平衡负载。</p><p>还有一种 <strong>根据节点数增加来进行分区</strong> 的方法：每个节点上有固定的分区数，当节点增加时，分区将变小，新增的节点会从原有节点的分区中随机进行拆分，最终这个新节点获得公平的负载份额。</p><p>分区再平衡可以 <strong>手动执行</strong> 也可以 <strong>自动执行</strong>。自动再平衡比较方便，因为不需要人工维护，但是它的执行过程是不可预测的：再平衡时将大量数据集从一个节点转移到另一个节点的过程中可能会产生很大的网络开销，这会使得该服务器对请求响应的性能降低，对用户的体验和生产造成负面影响。所以再平衡的过程有人参与是一件好事，这样能防止发生运维问题。</p><h3>5. 请求路由（服务发现）</h3><p>当我们已经将数据进行分区后，如何才能知道用户想要的数据在哪个节点上？这可以概括为是一个 <strong>服务发现</strong> 的问题。为了解决这个问题，可以通过如下图所示的三个方案</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447287" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><ol><li>允许访问所有的节点，如果第一个访问的节点有该键值，则处理该请求，否则将该请求转发到适当的节点上，这个方法避免了使用注册中心中间件，但是实现比较复杂</li><li>使用分布式的协调服务，用户将所有的请求发送到路由层，由路由层将该请求转发到合适的节点</li><li>要求用户（客户端）自己知道分区和节点的分配</li></ol><p>但是这其中还隐藏着一个问题：<strong>作出决策的组件（节点之一、路由层或客户端）是如何了解数据在节点间的分配变化的</strong>？这就需要一个独立的协调服务，比如使用 zookeeper 来跟踪元数据，如下图所示</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447288" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>每个节点都会在 zookeeper 中进行注册，zookeeper 中维护有节点到各个分区的可靠映射，负责决策的组件在 zookeeper 中订阅这个消息。当分区分配发生改变时，zookeeper 就会通知负责决策的组件更新路由信息，使其保持在最新的状态。</p><p>除此之外也可以在各个节点间采用 <strong>流言协议</strong> 来传播集群状态的变化，这样每个节点都维护有最新的数据路由方案，当其中一个节点收到请求时，会将其转发到合适的分区节点上（对应服务发现的方案一）。</p><hr/>]]></description></item><item>    <title><![CDATA[宝剑锋从磨砺出——零售数据库内核，为大促]]></title>    <link>https://segmentfault.com/a/1190000047447290</link>    <guid>https://segmentfault.com/a/1190000047447290</guid>    <pubDate>2025-12-03 19:03:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><em>癸卯七月风雨大作</em></p><p><em>京东零售·袁博文</em></p><p><em>僵卧双九不自哀，尚思为东戍轮台。</em></p><p><em>夜阑卧听珊瑚雨，铁马内核入梦来。</em></p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047447292" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><p>﻿﻿</p><p><strong><em>前言略长，只关心技术的同学可直接跳过看第二章</em></strong></p><h2><strong>一、前言：技术的底色是什么？</strong></h2><p>这个问题在技术人心中其实没有标准答案，每个人都有每个人的见解。架构师眼里大抵是高屋建瓴，统领全局；技术大牛的视角可能是剖根溯源，精刀细琢；新人小白或许更单纯，无非就是学习进步，快速成长为大牛之类了。</p><p>但在我——一个京东数据库人的眼里，技术的底色或许应该是五彩斑斓的吧。</p><h4><strong>白是纯粹的起点</strong></h4><p>经常听人说，每个人呱呱坠地那一刻，都是一张白纸，父母在其上着墨。对于技术人来说又何尝不是呢？初学一门技术，初入一个领域，每个人都是一张白纸，在这张白纸上是随意草稿涂鸦，还是认真吸收不断进步，都取决于自己。</p><p>数据库内核技术，在 2020 年初，于我个人于内核团队于京东而言都是一个纯白的起点。自此开始探索数据库内核的每一行源码、每一个模块，然后攻关研究每一个技术难点，再设计实现云原生的珊瑚数据库直到其落地承接业务。我和我们团队的小伙伴都可以拍着胸脯说，我们无愧于京东，无愧于这份纯白。</p><h4><strong>青是朝气是成长</strong></h4><p>内核团队每一个小伙伴，不论是社招还是校招，都是那么朝气蓬勃，都对数据库内核技术求知若渴。腾龙认真钻研探索，成功打通了数据库测试用例线上部署和初始的内核监控框架；福哥将华为的优良编码风格带入团队并影响了许多小伙伴，还在DDL模块钻研并颇有造诣；海鹏探索并打通了内核与JED接口，为高可用付出良多；金蓬初入团队，甚至连技术栈都是初学，抱着一本经典的《<em>C++ Primer</em>》边啃边研究珊瑚数据库内核源码，但不妨碍进步速度惊人，最终能够独当一面；海波攻关的修改缓冲影子页技术以及共享集群测试框架至今还在持续带来价值；珊哥和齐哥更不用说，一个将多年积累的开发经验与珊瑚数据库内核模块深入结合，做出了诸多贡献；另一个不但对元数据锁研究透彻，更是独自一人承担了整个珊瑚数据库的工程化落地与高可用及运维工具建设。作为校招生的彭凯和宇歆，更是在短短的时间内，迅速成长，深入研究SQL词法和语法解析，以及主从复制模块，并为新产品的铸剑做出了突出的贡献。现在，越来越多的朝气蓬勃的新伙伴陆续加入了我们团队，大家的快速成长都有目共睹。</p><p>大家都从当初的青涩小白，成长成了各个内核领域的专家，或者独当一面的人才。所以青这个底色，一定是技术人努力成长，拼搏向上的颜色吧。</p><h4><strong>黄是最后的执着</strong></h4><p>在眼看京东数据库内核团队蒸蒸日上，大家在内核领域日渐深耕的时候，不出意外的还是出意外了……</p><p>集团层面的架构调整，让零售和科技的技术团队不得不融合成一个团队了，我想初衷肯定是好的，大家也都为之努力过。但出于种种不便明说的原因，数据库内核团队成了大的架构齿轮磨合下的那个代价，团队动荡，未来不明，无奈之下许多初露锋芒的优秀小伙伴不得不做出各自的选择。就在我以为京东数据库内核就要黄了的时候，不幸中的万幸，在零售众多大佬同事的全力保护下，内核的种子留了下来，静待花开。而属于技术人的这份坚守，或许就像鹅卵黄一样，等待破壳重生的那一刻吧。</p><h4><strong>赤是对技术的热忱</strong></h4><p>如果希望有颜色，那么一定是红色！</p><p>就像赤色当年卧薪尝胆，艰苦奋斗，爬雪山过草地，把希望带给神州大地一样。属于京东技术的赤色，也在京东技术中心迎来新的大家长后随之到来。我不知道其他团队是不是有类似的感受，但数据库团队在回归零售以后，大家的心气神都不一样了，对技术那颗火热的心又重新燃了起来。数据库团队也迎来了新leader：一位在数据库领域有着二十年经验的超级大佬和一位在数据库内核领域有十多年经验的资深大佬。在两位大佬的带领下，我们开始朝着新的方向前进。</p><p>同时，数据库内核团队也很快迎来了越来越多的新鲜血液：来自其他大厂的林康、正茂、张扬，将他们所掌握的数据库内核以及工程化经验引入，为我们内核的研发装上了加速器；来自各大名牌高校的校招生以及实习生晓冰、江昊、一贤、祖才等等，也都快速学习迅速成长，以最饱满的热情融入我们团队并做出了相应的贡献。</p><p>大家都饱含赤诚，携手开始向未来进发！</p><h4><strong>黑是五彩斑斓的未来</strong></h4><p>始于白，终于黑。就像太极阴阳鱼一样，生生不息，周而复始。技术也一样！</p><p>自然界当所有的颜色混在一起后，只有一个颜色——黑。数据库内核的团队也在沉淀和挫折中更加强大，随着不断补充新鲜的血液，从市场上吸引更多优秀的数据库内核人才，当所有技术的底色混在一起后，所有的五彩斑斓，所有的初心、成长、坚守、希望融为一体后，所有的不同领域的人才齐心协力共渡难关后，那结合在一起的力量，其实就只剩下未来那无限的可能——五彩斑斓的黑。内核技术的深渊也如黑洞般，深不见底，等待我们去探索。但我相信，只要我们秉持技术人的底色，就一定可以达到那个彼岸！</p><h2><strong>二、正篇：五彩熔炉，铸剑！</strong></h2><p>正篇开始！</p><p>抱歉大家，前面扯了这么多其实只是前言。但我又不想像以前写前言那样，只是简单的交代一下背景。花了五节的笔墨介绍我心中的技术底色，只希望大家能懂一点——我们会以最大的热情和最强的技术为京东打造基础数据库产品，为大家带来更优质的数据库服务。</p><h4><strong>到底铸了什么剑？</strong></h4><p>属于我们京东电商版本的自研数据库内核——DongSQL！</p><p>五年前，数据库内核团队立项直接瞄准了新的数据库形态——云原生关系型数据库，也就是存算分离共享存储架构的珊瑚数据库(shared storage)，这一版技术难点主要是在共享存储的架构以及云原生的数据一致性，其产品价值主要是在节约数据库成本以及极致的云上资源伸缩性等。但由于与存量JED库(shared nothing)采用了不一样的技术架构，所以面临一个现实问题——存量用户版本无法平滑升级。</p><p>用过或者了解数据库的人都知道，有的时候不是大家不想使用更新的版本，更强的性能，更优秀的功能，而是数据库本身太基础太重要了，如果业务系统已经建设很多年，与数据库绑定太深的话，更多还是求稳为主，能不动则不动。这不是京东独有的情况，可以说整个行业皆是如此，这叫技术惯性。正是因为采用了新的技术架构，带来了一个问题：存量业务如果要使用必须进行数据库的迁移。就这一个原因，很多业务就望而却步。</p><p>正是由于这个原因，在新leader带领我们团队以后，基于丰富的数据库经验，敏锐地察觉到京东整个数据库的基本盘其实是存量的数据库，解决存量数据库用户的问题才能带来更大的价值。再优秀的产品，如果没人用一样白费力气。</p><p>因此，我们需要做的是，一个完全适配存量架构的数据库内核，不引入更复杂的架构变更和过多的设计，只在其基础上对数据库内核性能进行优化、对配套能力进行提升、对零售电商场景进行针对性扩展，完美支持JED以及DongDAL，秉持稳定性和兼容性为前提的基础上，让京东的数据库内核更好用，更强大！</p><h4><strong>电商场景下数据库痛点的解决之道</strong></h4><p>电商场景的数据库需求其实是用户最迫切的，因此我们在首选开刀方向时，没有选择引入花里胡哨高大上的功能等角度。而是从用户中来，回到用户中去，深入分析目前线上用户最常见的问题，以及大促最常见的故障场景，针对性的引入了内核层新的解决方案。</p><h5><strong>问题一：“过载” 大促激增的流量，或者超时SQL不断重试直接把数据库CPU打满甚至打挂</strong></h5><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047447293" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047447294" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>这种场景真的非常常见，甚至前段时间还有一个白虎故障就是类似的原因。业务研发在设计功能的时候，其实是无法预知线上生产环境真实的流量的，或许可以设计应用侧限流，也或许可以加缓存抗量，但限流不是每个系统都有，即使有也可能存在疏漏，缓存如果被击穿那带给数据库的流量更是暴击。有的时候甚至不是真实暴增的流量，而只是超时机制的负反馈，失败的不断重试就带来了超出预期的数据库请求。</p><p>当请求流量突然暴涨时或者突发的慢sql占用大量资源时，它会像一个被瞬间涌入人群挤垮的服务台：每个新连接都需要数据库创建一个线程来处理，大量线程的创建、上下文切换和维持本身就会吃掉可观的内存和CPU；更重要的是，每个查询进来，内核都要疯狂工作——解析复杂的SQL语句、在成千上万条索引条目中查找路径、拼凑关联多张表的数据、进行排序分组计算、管理事务保证一致性（这涉及到频繁的加锁解锁，高并发时极易堵塞排队）、还要不断从磁盘读取数据或把改动写回去。所有这些操作都是极度消耗CPU算力的密集计算。当每秒涌入的请求远超CPU能处理的速度时，CPU就会被完全占满，所有查询都挤在一起排队等待计算资源。与此同时，高并发下锁冲突剧增，大量线程因等待锁而阻塞却不释放资源；内存可能被临时表、排序缓存塞爆；严重时磁盘IO也跟不上。最终，CPU被彻底耗尽，新连接无法建立，已有查询完全卡死，整个数据库进程失去响应，就像被“打挂”了一样，本质上就是所有关键资源（CPU、内存、IO、连接）在瞬间洪峰下被彻底榨干导致的系统性崩溃。</p><p>原因很清楚，解决方式也很简单，前面也提到了，限流即可，可实际生产环境操作起来还是会出现诸多困难。</p><p>业务层自行限流面临的主要挑战在于其“粗放”和“滞后”。它通常只能基于简单的请求频率或用户维度（如QPS）进行拦截，无法洞察数据库内部真实的瓶颈所在（比如是在CPU、内存、磁盘IO还是锁冲突）。这极易导致“误杀”——核心的重资源消耗型SQL可能未被拦住，反而大量高频但轻量的请求被限流，牺牲了业务可用性却未能真正缓解数据库压力。同时，在分布式微服务架构下，协调各个服务模块统一、实时地实施并调整限流策略异常困难，很容易出现限流不一致或响应迟缓，当业务层感知到数据库响应变慢或报错再触发限流时，往往已经错过了最佳干预时机，雪崩可能已经发生。</p><p>目前的实际操作往往是高可用程序或者DBA依靠HA机制进行主备切换来应对过载。切换过程本身必然导致数秒到数十秒的服务中断（连接闪断、短暂只读），对连续性要求高的业务会造成直接影响。更重要的是数据一致性问题：主库在故障或过载瞬间可能存在未同步到备库的事务数据，切换后这些数据可能永久丢失（异步复制下），即使使用半同步复制也可能因网络问题阻塞写入或退化为异步。历年大促线上生产环境不少故障甚至是发生在切换操作之后(普通RDS集群以及低版本vitess集群风险尤其显著)。</p><h5><strong>解：SQL自提示实现精准限流</strong></h5><p>基于以上痛点，不少用户提出，如果可以实现精准限流就好了，既能在业务根据流量预测的基础上预防性限流，又能在过载发生后根据简单排查的结果定向限流。有求必应——Hint限流方案横空出世！</p><pre><code>// 根据特定 SQL 指纹进行限流 
update/*+ ccl_queue_digest(INT&lt;当前语句的并行数&gt;) */ t set col1 = col1+1 where 1=id; 
update/*+ ccl_queue_digest() */ t set col1 = col1+1 where 1=id;
</code></pre><p>数据库内核自身支持限流的核心优势是，我们能深入到SQL执行层，根据用户指定规则（如匹配特定SQL指纹、或者SQL语句全文等不同模式规则）实时识别并优先抑制那些真正“吃掉”大量资源的“罪魁祸首”查询。这如同在数据库引擎内部安装了一个智能节流阀，直接从源头（消耗资源的查询）进行精准控制，避免了业务层限流的盲目性和HA切换的破坏性。它能在资源紧张初现端倪时就主动干预，最大限度保障核心业务请求的通过和系统整体的稳定性，且由内核统一管理，规则生效及时、策略执行高效。</p><h5><strong>问题二：“秒杀” 单点高频写入带来的数据库性能下降，以及库存一致性问题</strong></h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447295" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>秒杀是电商业务非常常见的场景，无论秒杀业务是否设计缓存前置抗量，库存数据的最终变更都是需要落到数据库的。如果缓存发生击穿，更是需要数据库来进行兜底策略。但秒杀这个场景的数据库操作又极其特殊，甚至可以说会导致传统数据库痛点集中爆发。</p><p>首先，高频单行更新使行级锁竞争成为致命瓶颈：当海量请求同时扣减同一商品库存时，存储引擎的行锁强制串行更新，导致线程在锁等待中堆积；死锁检测机制在队列过长时（如超1000线程）触发深度遍历，CPU资源被疯狂消耗，事务响应时间骤增甚至超时。</p><p>其次，高频事务的ACID保障带来巨大开销：事务在数据库内核中是核心能力之一，在秒杀场景下往往都是简单事务，但为了保证查询更新的一致性，又不得不开显式事务(非auto commit)，而显式事务的BEGIN、Statement、COMMIT/ROLLBACK，每一个子句都会完整的经历应用侧到数据库底层的多级转发和网络开销，伴随多次网络交互（跨节点延迟加剧堵塞）及日志写入，单事务耗时飙升，系统吞吐量断崖式下跌。</p><p>最后，秒杀场景的库存扣减不允许出现意料外的更新：传统数据库的高并发扣减需通过SELECT检查库存后再执行UPDATE，但两步操作存在时序漏洞——高并发下多个请求可能同时读到相同库存值，导致超卖；同时所有请求（包括库存不足的无效请求）均需竞争同一行锁，引发线程堆积和死锁检测的CPU暴增。</p><h5><strong>解：电商秒杀场景定制优化</strong></h5><p><strong>秒杀排队</strong>：高频更新问题很好解决，借用限流的思路，只不过秒杀场景要限的是具体的字段甚至是具体的值，因为高频SQL是集中在具体数量的库存或者单一品类上的，要改的可能就几行甚至是一行数据。因此，我们借用了限流的Hint语法，业务只需要在预期秒杀需要更改的具体SQL上，加上对应的Hint规则，约定具体字段或者具体值需要进行限制排队执行，数据库内部就会对秒杀类的SQL进行管理排队，极大程度的规避了行锁的竞争以及其连锁反应，经测试单行更新高并发场景下，比传统数据库的流量能提升一倍以上。</p><pre><code>// 根据热点值限流
 update/*+ ccl_queue_value('茅台') */ t set c=c+1 where name ='茅台'; 
 // 根据热点字段限流
  update/*+ ccl_queue_field(order_id) */ t set c=c+1 where order_id =1and name ='茅台';
</code></pre><p><strong>事务快速提交/回滚</strong>：针对秒杀事务的特性，设计了事务快速提交回滚的Hint，即用户在事务COMMIT/ROLLBACK前的最后一个SQL语句上，如果加上该Hint，则内核即明白该操作提交或者回滚了。此方案在秒杀场景下，尤其是特定单行更新的场景下，最高可以提升 3 倍以上的性能！优势非常明显。</p><p><strong>影响行数约束</strong>：秒杀场景库存扣减，或者其他非秒杀场景也可能存在，业务侧的逻辑明确知道某条SQL更新后应该影响几行数据，如果数据库执行完发现影响的行数不符合预期则大概率出现问题了，需要将事务进行回滚。我们设计了预期影响行数的Hint，通过该Hint（示例 UPDATE /*+ TARGET\_AFFECT\_ROW(1) */ stock SET count=count-1 WHERE id=100 AND count&gt;=1），可同步实现两大核心优化：</p><p>其一，引擎在加锁前优先校验 WHERE 条件（库存≥1），仅当库存充足时才尝试加锁更新，库存不足的请求直接返回影响行数=0，避免无效锁竞争；</p><p>其二，库存检查与扣减压缩为单原子操作，确保影响行数严格为1才成功，否则自动失败，彻底杜绝跨事务的脏读与超卖风险。当然也可以配置其他数值，只要与您预期的影响行数一致即可。</p><h5><strong>问题三：“缓存更新一致性问题” 业务前置缓存失效时，会直接更新数据库，然后查询已更新数据并返回</strong></h5><p>许多业务系统会在数据库访问层之上引入缓存，例如京东的分布式缓存JIMDB，以利用其极致的读写响应速度优化用户体验。然而，缓存的易失性本质决定了其无法独立承担关键数据的持久化职责——数据库始终是不可或缺的兜底保障（除非数据可容忍丢失）。维护缓存与数据库之间的强一致性是系统设计的核心挑战，当缓存失效导致请求穿透至数据库时，业务常需同步获取刚更新的数据并实时刷新缓存或响应前端。传统数据库在此场景下存在显著局限：若要在事务中确保更新后立即可见且数据一致，必须在DML操作后紧跟一条SELECT语句进行查询。但即便采用此方案，在读已提交（RC）隔离级别下，其他事务的并发修改仍可能导致该查询读到不一致数据，无法满足严格的实时一致性要求。</p><h5><strong>解：实现RETURNING语法</strong></h5><p>我们通过实现RETURNING语法解决这一问题：在UPDATE/INSERT等DML语句末尾追加RETURNING子句，就能直接获取修改后的完整行数据。比如库存扣减场景下，一条UPDATE inventory SET stock=stock-1 WHERE id=100 RETURNING *;语句既完成了原子扣减，又能立即返回最新库存值，无需额外SELECT查询。</p><p>这一内核级优化不仅消除了RC隔离下的并发脏读风险（DML与返回数据基于同一事务快照，其他事务的并发修改不会干扰结果），还将“更新 + 查询”的两次网络交互压缩为单次请求，把事务耗时再降一个级别。对缓存架构而言，业务侧拿到RETURNING返回的实时数据后，能立刻刷新缓存层，在事务提交时就完成数据对齐，让秒杀、大促等高并发场景下的“缓存击穿兜底逻辑”，既快又稳。</p><h5><strong>问题四："执行计划漂移" 好好的SQL突然就慢了</strong></h5><p>这个问题真的让人头疼，一条SQL在开发环境跑得飞快，到了线上就变成了蜗牛。更要命的是，有时候同一条SQL，今天还好好的，明天就突然慢得要死。</p><p>举个例子，我们有条订单查询的SQL：</p><pre><code>SELECT o.*, u.name  
FROM orders o  
JOIN users u ON o.user_id = u.id  
WHERE o.create_time  
BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN('PAID','SHIPPED') 
 ORDER BY o.create_time DESC LIMIT 100;
</code></pre><p>平时这条SQL毫秒级就能出结果，用的是<code>orders.idx_create_time</code>索引。但有一天大促期间，这条SQL突然开始走全表扫描，30秒才能跑完，直接把系统拖垮了。</p><p>为什么会这样？数据库优化器是个"聪明"的家伙，它会根据表的统计信息来选择执行计划。但问题就出在这些统计信息上——<code>ANALYZE TABLE</code>更新了统计信息，数据分布发生了变化，或者系统负载影响了成本计算，优化器就可能突然"变心"，选择一个完全不同的执行路径。</p><p>这种情况在大促期间特别危险，数据量激增、系统负载变化，一条核心查询的执行计划突然劣化，整个系统可能就垮了。</p><p>传统的解决办法要么重启数据库（代价太大），要么业务研发加Hint强制索引（破坏代码可维护性，还得紧急上线，时间周期长），要么调优化器参数（可能影响其他SQL），都不是很好的选择。</p><h5><strong>解：Statement Outline执行计划固化功能</strong></h5><p>为了解决这个问题，我们实现了Statement Outline功能，可以把稳定高效的执行计划"固化"下来，让优化器按照我们指定的方式执行。这个功能通过存储过程包来管理，使用起来很简单。比如我们发现某个查询有个很好的执行计划，就可以把它记下来，一旦发生上述意外场景，可以立即将其注入数据库从而稳定该类型SQL的执行：</p><pre><code>-- 添加优化器hint的outline
 CALL dbms_outln.add_optimizer_outline(   
 'your_db',                                     -- 数据库名称    
 '',                                            -- SQL语句的摘要，为空时自动计算      
 1,                                             -- 位置，通常为1     
 '/*+ USE_INDEX(orders idx_create_time) */',    -- 优化器提示文本      
 'SELECT o.*, u.name        FROM orders o       
 JOIN users u ON o.user_id = u.id        
WHERE o.create_time BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN ('PAID', 'SHIPPED')       
ORDER BY o.create_time DESC LIMIT 100;'       -- SQL语句文本); 

-- 添加强制索引的outline   
CALL dbms_outln.add_index_outline(      'your_db',                                     -- 数据库名称     
 '',                                            -- SQL语句的摘要，为空时自动计算      
1,                                             -- 位置，通常为1     
 'USE INDEX',                                   -- 索引提示类型，如'USE INDEX'、'IGNORE INDEX'等     
 'idx_status',                                  -- 索引列表，多个索引用逗号分隔      
 '',                                            -- 索引提示选项，如'FOR JOIN'、'FOR ORDER BY'等     
'SELECT o.*, u.name       
 FROM orders o        
JOIN users u ON o.user_id = u.id        
 WHERE o.create_time BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN ('PAID', 'SHIPPED')        
 ORDER BY o.create_time DESC LIMIT 100;'       -- SQL语句文本);
</code></pre><p>这样一来，即使统计信息变化了，优化器也会按照我们固化的执行计划来执行，保证查询性能的稳定性，让我们能够精确控制查询的执行方式。对于那些业务关键的SQL，这个功能简直是"定海神针"，彻底解决了执行计划漂移的问题。</p><p>Outline还可以注入自定义的hint，比如“问题一”中的解决过载问题hint或者“问题二”中的秒杀场景hint。</p><h5><strong>问题五："线程拥堵" 每连接每线程的弊病</strong></h5><p>传统数据库是没有线程池的，每个连接都要创建一个独立的线程来处理，常规场景每连接每线程还很稳定，但高并发场景下就是灾难。</p><p>想象一下大促期间的场景：成千上万个连接同时涌入数据库，每个连接都要创建线程，线程创建和销毁的开销巨大，CPU忙着做上下文切换，真正用来处理SQL的时间反而不多。更要命的是，所有请求都是一视同仁，核心的支付查询可能被大量的日志写入、报表查询这些不紧急的请求给"淹没"了。在JED架构下，由vitess控制了连接的数量，这个问题还不大，但目前DongDAL直连DongSQL的架构，这个就成了不得不面对的重点问题！</p><p>线程拥堵的问题看起来和过载很像，但还略有一点区别。过载场景可以精确识别到个别问题SQL，并进行精准限流，从而保证不影响其他SQL。而线程拥堵的大部分甚至所有连接都是正常SQL，没有谁是受害者，只不过突发流量真的太大了！所以这种场景，我们就不得不祭出大杀器——线程池！</p><h5><strong>解：DongSQL线程池</strong></h5><p>我们实现了完整的线程池功能，能够有效复用线程资源，避免频繁创建和销毁线程的开销。线程池会维护一组工作线程，新来的连接请求会被分配到空闲的线程上处理，这样就能大大减少上下文切换，提升高并发场景下的性能。</p><p>这样一来，来自核心服务器/核心用户的请求就能优先得到处理，不会被其他不那么紧急的请求给挤占了。系统会智能识别高优先级连接，确保关键功能的响应时间。</p><p>这些优化功能的加入，让DongSQL在高并发、大数据量的零售电商核心场景下展现出了更强的稳定性和性能。每一个功能都是我们在实际业务中遇到问题、分析问题、解决问题的结果，希望能够帮助更多的团队应对类似的挑战。</p><h2><strong>三、结语：技术的成色又是什么呢？</strong></h2><p>如果说技术的底色，是求知、是成长、是执着、是热忱、是我们所有技术人团结在一起爆发出的力量。</p><p>那么技术的成色，一定有脚踏实地，追根溯源，不浮于表象，而深入骨髓地解决根本问题。正所谓：求木之长者，必固其根本；欲流之远者，必浚其泉源。对于数据库，则必须具备掌控数据库内核的能力，方能使自身以及其上承接的业务行稳致远。</p><p>除此之外，更宏观的维度，技术的成色我想应该就是为团队、为公司、为用户、乃至为社会产生实实在在的价值吧！正如公司使命说的那样：<strong>技术为本，让生活更美好！</strong> 让我们携手所有业务研发团队做实事、有价值的事、长期的事，为京东的 35711 梦想付出我们自己的一份力！</p>]]></description></item><item>    <title><![CDATA[京东自研电商数据库内核DongSQL简介]]></title>    <link>https://segmentfault.com/a/1190000047447297</link>    <guid>https://segmentfault.com/a/1190000047447297</guid>    <pubDate>2025-12-03 19:02:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>团队于今年(2025.9)打磨出了深度优化的自研数据库内核——DongSQL V1.1.0。</p><p><em>[如果对前因后果比较感兴趣，可以移步上一篇文章</em><a href="https://link.segmentfault.com/?enc=b8iieQobQHtoRuquj86ynA%3D%3D.j9zGvy7UJnAHF2DgwGUof%2BTbzUQcBTjmr%2FPWkZ6wClQXbxVD6B6MZONnGjtFDB%2FWYRkd9hfLGI5nT7hSZ5OL1wfjYdlCNfBVflEeecsJ6eMvcnsc9sPubbhTI94bTsED2%2Ba3NCAbirYWy5U%2Fjhfibg%3D%3D" rel="nofollow" target="_blank"> <em>《宝剑锋从磨砺出——零售数据库内核，为大促铸剑！》</em> </a><em>]</em></p><p>本文将深度解析DongSQL在语法扩展、并发控制、查询优化等方面的内核改造，以及在电商场景下的优化实践。</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/c89b566ef9c14de6a8b85177e1ba9116.webp" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>1、DongSQL在语法扩展上的优化</h2><h3>1.1. RETURNING子句功能</h3><p><strong>▶︎ 语法扩展创新</strong>：DongSQL在标准SQL语法基础上扩展了RETURNING子句，这是重要语法创新。RETURNING子句允许DML语句(INSERT、UPDATE、DELETE、REPLACE)在执行数据修改操作的同时返回受影响的行数据，无需额外查询。</p><p>传统数据库在执行DML操作后，如果需要获取操作结果，必须执行额外的SELECT查询，这在高并发场景下会产生额外的网络往返开销。DongSQL通过RETURNING子句彻底解决了这一问题。</p><pre><code>-- INSERT操作返回自增ID 
INSERT INTO orders (customer_id, order_date) VALUES (1001, NOW()) RETURNING order_id; 

-- UPDATE操作返回更新后的数据 
UPDATE products SET price = price * 1.1 WHERE category = 'electronics'  
RETURNING product_id, name, old_price, price; 

-- DELETE操作返回被删除的记录 
DELETE FROM expired_sessions WHERE expire_time &lt; NOW()  
RETURNING session_id, user_id, expire_time;
</code></pre><p><strong>▶︎ 性能提升效果</strong>：经测试验证，RETURNING子句在不同场景下都能带来显著的性能提升：</p><p>•<strong>固定行更新场景</strong>：16并发时TPS提升61%，响应时间降低44%</p><p>•<strong>随机行更新场景</strong>：128并发时TPS提升18%</p><p>•<strong>大规模更新测试</strong>：2000万次操作中平均TPS提升5-10%</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/df95c690bb17463caf7cb548407575a9.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p><strong>▶︎ 生产落地预期</strong>：该功能与DongDAL发号器逻辑高度匹配，有望将发号器性能瓶颈大幅提升(DongDAL团队配套开发推进中)</p><h3>1.2. Hint语法扩展</h3><p><strong>▶︎ 多样化Hint支持</strong>：DongSQL扩展了Hint语法体系，提供了针对电商场景的专用提示功能，包括并发控制、库存管理等领域特定的优化。</p><p><strong>▶︎ Inventory Hint</strong>：专门针对电商库存管理场景设计的提示语法，提供目标影响行数控制、自动提交/回滚等特性。</p><pre><code>-- 库存扣减：确保只影响一行，成功自动提交，失败自动回滚
 UPDATE /*+ TARGET_AFFECT_ROW(1) COMMIT_ON_SUCCESS ROLLBACK_ON_FAIL */
  inventory SET stock = stock - 5 
   WHERE product_id = 1001 AND stock &gt;= 5;
</code></pre><p><strong>▶︎ 性能提升数据</strong>：在16并发的库存扣减场景下，使用Inventory Hint比不使用hint性能提升215%。</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/190309a42b3743acb7143ab2223efb61.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>2、DongSQL在并发控制上的优化</h2><h3>2.1. CCL并发控制</h3><p><strong>▶︎ 多维度限流机制</strong>：DongSQL实现了CCL(Concurrency Control)并发控制功能，通过多维度的限流策略，有效解决电商秒杀场景下的热点数据访问问题。</p><p>传统数据库在面对高并发热点数据访问时，往往会因为激烈的锁竞争导致性能急剧下降，甚至系统雪崩。DongSQL的CCL通过智能排队机制，将无序的并发请求转换为有序处理，从根本上解决了这一问题。</p><p><strong>▶︎ 多维度控制策略</strong>：</p><p>•<strong>基于字段的限流</strong>：<code>ccl_queue_field(column_name, concurrency)</code>，对特定字段值进行并发控制</p><p>•<strong>基于值的限流</strong>：<code>ccl_queue_value(value, concurrency)</code>，对特定数据值进行精准限流</p><p>•<strong>基于SQL指纹的限流</strong>：<code>ccl_queue_digest(concurrency)</code>，对相同SQL模式进行统一管控</p><pre><code>-- 对商品ID为999的热门商品进行限流，并发度限制为5
 SELECT /*+ ccl_queue_value(999, 5) */ * FROM products WHERE product_id = 999; 
 
 -- 对库存扣减操作按商品ID进行限流 
 UPDATE /*+ ccl_queue_field(product_id, 8) */ inventory SET stock = stock - 1 WHERE product_id = ?; 
 
 -- 对相同SQL模式进行统一限流 
 SELECT /*+ ccl_queue_digest(10) */ * FROM hot_products WHERE status = 1;
</code></pre><p><strong>▶︎ 性能突破数据</strong>：</p><p>•<strong>秒杀场景优化</strong>：在4096并发下，使用CCL限流后TPS从573提升至1337，性能提升133%</p><p>•<strong>系统稳定性</strong>：有效防止系统雪崩，将无序并发转换为有序处理</p><p>•<strong>热点缓解</strong>：通过队列机制显著降低热点数据的锁竞争</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/7695dd1288ba42c6a1d4d6d79a87ba66.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>2.2. Statement Outline执行计划及自定义提示管理</h3><p><strong>▶︎ 企业级计划稳定性</strong>：DongSQL提供了Statement Outline功能，用于固化重要SQL的执行计划，防止因数据变化导致的计划不稳定问题。</p><p><strong>▶︎ 自定义Hint注入工具</strong>：包括但不限于上述秒杀、CCL限流场景的Hint，即使业务研发预期外的过载或者突发流量发生，应急情况下DBA也可以通过Statement Outline功能对问题SQL进行干预</p><pre><code>-- 为重要SQL固化执行计划 
CALL dbms_outln.add_index_outline(  
'test_db', '', 1, 'USE INDEX', 'idx_status', '',  
 'SELECT * FROM orders WHERE status = "PAID"' 
 ); 
 -- 为特定查询添加ccl_queue_digest限流hint，限制并发度为2 
CALL dbms_outln.add_optimizer_outline(  
'test_db', '', 1, '/*+ ccl_queue_digest(2) */',  
'SELECT * FROM orders WHERE customer_id = 1001' );
</code></pre><p><strong>▶︎ 核心价值</strong>：</p><p>•<strong>性能稳定性</strong>：保障核心SQL性能不因数据变化而波动</p><p>•<strong>智能限流</strong>：支持基于SQL指纹的手动限流和自动限流(自动限流默认不开启，需要开启的业务需单独申请)</p><p>•<strong>企业级管理</strong>：提供生产级的执行计划管理能力</p><h2>3、DongSQL在查询优化上的改进</h2><h3>3.1. 单点查询优化</h3><p><strong>▶︎ 查询路径优化</strong>：DongSQL实现了单点查询bypass功能，针对主键等值查询这类高频简单查询，绕过部分SQL层处理逻辑，直接访问存储引擎，大幅提升查询性能。</p><p>电商场景中，商品详情查询、用户信息查询等基于主键的简单查询占据了很大比例。虽然这些查询逻辑简单，但在高并发下仍然消耗大量CPU资源。DongSQL的单点查询优化针对这一痛点进行了专项优化。</p><p><strong>▶︎ 性能提升数据</strong>：</p><p>•<strong>不同环境性能提升</strong>：容器环境提升20%，物理机环境提升30%</p><p>•<strong>高并发场景</strong>：当CPU达到瓶颈时，QPS提升20-28%</p><p>•<strong>资源效率</strong>：相同硬件配置下处理能力显著提升</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/9513743b2f4d49c1ba2df985f69131a1.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.2. 线程池优化</h3><p><strong>▶︎ 高并发处理能力</strong>：DongSQL实现了企业级线程池功能，通过智能线程调度和资源管理，显著提升了系统在高并发场景下的处理能力和稳定性。</p><p>传统数据库在面对大量并发连接时，会为每个连接创建独立线程，这在高并发下会导致线程切换开销过大、内存消耗激增等问题。DongSQL的线程池优化通过复用线程资源，有效解决了这些问题。</p><p><strong>▶︎ 调度机制</strong>：</p><p>•<strong>线程复用：</strong> 通过线程池复用减少线程创建销毁开销</p><p>•<strong>负载均衡：</strong> 分配任务到不同线程，避免热点线程</p><p>•<strong>优先级调度：</strong> 支持任务优先级，保障重要业务优先处理</p><p><strong>▶︎ 性能突破数据</strong>（基于8C32G测试环境，sysbench 16张表每张1000万行数据）：</p><p><strong>只读场景性能对比</strong>：</p><p>•<strong>低并发优势</strong>：32线程时，线程池模式QPS达到141,261，相比传统模式的110,658提升27.6%</p><p>•<strong>高并发稳定性</strong>：在512线程高并发下，线程池模式QPS保持131,939，而传统模式仅61,580，性能提升114%</p><p>•<strong>延迟控制</strong>：512线程时TP99延迟从传统模式的297.92ms优化到118.92ms，降低60%</p><p><strong>纯写场景性能突破</strong>：</p><p>•<strong>中等并发</strong>：64线程时QPS从46,577提升到57,655，性能提升23.8%</p><p>•<strong>高并发场景</strong>：512线程时QPS从29,541提升到58,166，性能提升97%</p><p>•<strong>超高并发</strong>：4096线程时QPS从28,571提升到54,687，性能提升91%</p><p><strong>读写混合场景优化</strong>：</p><p>•<strong>128线程</strong>：QPS从54,870提升到80,244，性能提升46%</p><p>•<strong>256线程</strong>：QPS从48,787提升到77,961，性能提升60%</p><p>•<strong>延迟优化</strong>：256线程时TP99延迟从196.89ms优化到158.63ms，降低19%</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/549ec324046c4332a50a1267bc614ee6.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.3.  其他查询执行优化</h3><p><strong>▶︎ 执行路径优化</strong>：DongSQL在查询执行引擎层面进行了多项优化，包括算子优化、内存管理优化、并行执行优化等。</p><p><strong>▶︎ 缓存机制增强</strong>：优化了Buffer Pool管理策略，页面mutex优化，提升了数据访问效率，降低了I/O锁冲突。</p><h2>4、性能基准测试汇总</h2><h3>OLTP标准基准测试</h3><p>基于标准测试环境的性能数据（16C32G, 16张表，每张表100万行）：</p><table><thead><tr><th>测试场景</th><th>最佳线程数</th><th>TPS</th><th>QPS</th><th>TP99延迟</th><th>平均延迟</th></tr></thead><tbody><tr><td>只读查询</td><td>64</td><td>19,484</td><td>311,745</td><td>21.50ms</td><td>3.28ms</td></tr><tr><td>只写操作</td><td>256</td><td>17,004</td><td>102,025</td><td>29.72ms</td><td>15.05ms</td></tr><tr><td>插入操作</td><td>256</td><td>25,614</td><td>25,614</td><td>15.83ms</td><td>9.99ms</td></tr><tr><td>读写混合</td><td>128</td><td>9,795</td><td>195,908</td><td>33.12ms</td><td>13.06ms</td></tr><tr><td>点查询</td><td>64</td><td>560,933</td><td>560,933</td><td>0.18ms</td><td>0.11ms</td></tr></tbody></table><h3>电商场景专项性能汇总</h3><table><thead><tr><th>优化模块</th><th>测试场景</th><th>性能提升幅度</th><th>关键指标</th></tr></thead><tbody><tr><td><strong>RETURNING子句</strong></td><td>固定行更新</td><td><strong>61%</strong></td><td>TPS: 925→1,490</td></tr><tr><td><strong>CCL并发控制</strong></td><td>秒杀场景</td><td><strong>133%</strong></td><td>TPS: 573→1,337</td></tr><tr><td><strong>Inventory Hint</strong></td><td>库存扣减</td><td><strong>215%</strong></td><td>TPS: 1,537→4,843</td></tr><tr><td><strong>单点查询优化</strong></td><td>主键查询</td><td><strong>28%</strong></td><td>QPS: 76,432→98,470</td></tr></tbody></table><h2>5、未来规划</h2><p>1.<strong>持续语法扩展</strong>：基于业务需求继续扩展SQL语法功能</p><p>2.<strong>智能优化增强</strong>：引入机器学习优化执行计划选择</p><p>3.<strong>内核级技术支持</strong>：具备内核研发能力的团队，持续从最底层为业务研发提供深度优化的数据库解决方案</p><p>4.<strong>云原生存算分离</strong>：继续打造属于京东自己的高性能低成本数据库产品</p><h2>6、结语</h2><p>从开源内核到自研DongSQL，京东零售数据库团队始终以"业务价值驱动技术创新"为核心理念。DongSQL作为专为京东电商场景设计的数据库，通过语法扩展、并发控制、查询优化等多个模块的深度创新，为电商业务的快速发展提供了强有力的数据库技术支撑。</p><p>这些优化不仅提升了系统性能，更重要的是为集团基础技术底座提供了坚实的基础。未来，京东零售数据库团队将持续深耕数据库内核技术，让数据库更好地服务业务发展。</p>]]></description></item><item>    <title><![CDATA[【隐语Secretflow】如何在Doc]]></title>    <link>https://segmentfault.com/a/1190000047447329</link>    <guid>https://segmentfault.com/a/1190000047447329</guid>    <pubDate>2025-12-03 19:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047313097" alt="" title=""/></p><p>打开链接即可点亮社区Star，照亮技术的前进之路。</p><p>Github 地址：<em><a href="https://link.segmentfault.com/?enc=mgSg8qOBkT8nZEWmavarJw%3D%3D.qnMNscNimdQeQX8OcfrkqfXRScP0jigLhp6yHWwL1Uj0he14VWbB4n5VvkLZSMcz" rel="nofollow" target="_blank">https://github.com/secretflow/kuscia</a></em></p><h2>前言</h2><p>本教程帮助您在多台机器上使用 <a href="../../reference/architecture_cn.md#点对点组网模式" target="_blank">点对点组网模式</a> 来部署 Kuscia 集群。</p><p>当前 Kuscia 节点之间只支持 Token 的身份认证方式，在跨机器部署的场景下流程较为繁琐，后续本教程会持续更新优化。</p><h2>前置准备</h2><p>在部署 Kuscia 之前，请确保环境准备齐全，包括所有必要的软件、资源、操作系统版本和网络环境等满足要求，以确保部署过程顺畅进行，详情参考<a href="../deploy_check.md" target="_blank">部署要求</a>。</p><h2>部署流程（基于 TOKEN 认证）</h2><h3>部署 alice 节点</h3><p>登录到安装 alice 的机器上，本文为叙述方便，假定节点 ID 为 alice ，对外可访问的 PORT 是 11080 。</p><p>指定 Kuscia 使用的镜像版本，这里使用 1.1.0b0 版本</p><pre><code class="bash">export KUSCIA_IMAGE=secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/kuscia:1.1.0b0</code></pre><p>指定 Secretflow 版本：</p><pre><code class="bash"># Using Secretflow image, version 1.11.0b1 is used here
export SECRETFLOW_IMAGE=secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/secretflow-lite-anolis8:1.11.0b1</code></pre><p>获取部署脚本，部署脚本会下载到当前目录：</p><pre><code>docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/scripts/deploy/kuscia.sh &gt; kuscia.sh &amp;&amp; chmod u+x kuscia.sh</code></pre><p>生成 alice 节点配置文件，kuscia init 参数请参考 <a href="../kuscia_config_cn.md#id3" target="_blank">Kuscia 配置文件</a>：</p><pre><code class="bash"># The --domain parameter specifies the node ID
docker run -it --rm ${KUSCIA_IMAGE} kuscia init --mode autonomy --domain "alice" &gt; autonomy_alice.yaml 2&gt;&amp;1 || cat autonomy_alice.yaml</code></pre><p>建议检查生成的文件，避免配置文件错误导致的部署启动问题。</p><p>启动节点，默认会在当前目录下创建 ${USER}-kuscia-autonomy-alice/data 目录用来存放 alice 的数据。部署节点需要使用 <code>kuscia.sh</code> 脚本并传入节点配置文件：</p><pre><code class="bash"># -p: Specifies the mapping of the HTTPS port from the node container to the host. Ensure this port does not conflict with existing ports on the host.
# -k: Specifies the mapping of the MTLS port for the Kuscia API from the node container to the host. Ensure this port does not conflict with existing ports on the host. 
# -a: Specifies auto-import of engine images. Use -a none to disable auto-import. Use -a secretflow (default) to auto-import the SecretFlow engine image.
# -m or --memory-limit: Sets appropriate memory limits for node containers. For example, '-m 4GiB or --memory-limit=4GiB' means limiting max memory to 4GiB, '-m -1 or --memory-limit=-1' means no limit. If not set, defaults are: master 2GiB, lite node 4GiB, autonomy node 6GiB.
./kuscia.sh start -c autonomy_alice.yaml -p 11080 -k 11081</code></pre><p>:::{tip}</p><ul><li>节点 ID 需要全局唯一并且符合 RFC 1123 标签名规则要求，详情请参考<a href="https://link.segmentfault.com/?enc=0uw3n6hrpHXBSvAvnNOY5A%3D%3D.QGvzdlL51bwE4%2FquISlhl5D6VXy6BIiV5WL58vrrVqG2I5SiwTokqEge82R%2FsUAxNqnLsnahhIjbu%2Fjt5iIq%2F85UgSESvOMsa2u07bQanDrwT4oqpO6j9DxEOSJalcrH" rel="nofollow" target="_blank">这里</a>。<code>default</code>、<code>kube-system</code> 、<code>kube-public</code> 、<code>kube-node-lease</code> 、<code>master</code> 以及 <code>cross-domain</code> 为 Kuscia 预定义的节点 ID，不能被使用。</li><li>目前 kuscia.sh 脚本仅支持导入 SecretFlow 镜像，scql、serving 以及其他自定义镜像请移步至<a href="../../development/register_custom_image.md" target="_blank">注册自定义算法镜像</a></li><li>如果节点之间的入口网络存在网关时，为了确保节点与节点之间通信正常，需要网关符合一些要求，详情请参考<a href="../networkrequirements.md" target="_blank">这里</a></li><li>alice、bob 节点默认使用 SQLite 作为存储，如果生产部署，需要配置链接到 MySQL 数据库的连接串，具体配置可以参考<a href="../kuscia_config_cn.md#id3" target="_blank">这里</a></li><li>需要对合作方暴露的 Kuscia 端口，可参考 <a href="../kuscia_ports_cn.md" target="_blank">Kuscia 端口介绍</a>。如果多个 Autonomy 节点部署在同一个物理机上，可以用 -p -k -g -q -x 参数指定下端口号（例如：./kuscia.sh start -c autonomy_alice.yaml -p 11080 -k 11081 -g 11082 -q 11083 -x 11084），防止出现端口冲突。</li><li>非 root 用户部署请参考<a href="./docker_deploy_kuscia_with_rootless.md" target="_blank">这里</a></li><li>升级引擎镜像请参考<a href="../../tutorial/upgrade_engine.md" target="_blank">指南</a><br/>:::</li></ul><h3>部署 Bob 节点</h3><p>您可以选择在另一台机器上部署 bob 节点，详细步骤参考上述 alice 节点部署的流程，唯一不同的是在部署前准备参数时配置 bob 节点相关的参数。假定节点 ID 为 bob ，对外可访问的 PORT 是 21080 。</p><h3>配置证书</h3><p>在两个 Autonomy 节点建立通信之前，您需要先给这两个节点互换证书。</p><h4>Alice 颁发证书给 Bob</h4><p>准备 Alice 的公钥，在 Alice 节点的机器上，可以看到包含公钥的 crt 文件：</p><pre><code class="bash"># [alice machine] Copy domain.crt from inside the container and rename it to alice.domain.crt
docker cp ${USER}-kuscia-autonomy-alice:/home/kuscia/var/certs/domain.crt alice.domain.crt</code></pre><p>将 alice 的公钥 alice.domain.crt 拷贝到 bob 容器的 /home/kuscia/var/certs/ 目录中：</p><pre><code class="bash"># [bob machine] Make sure alice.domain.crt is in the /home/kuscia/var/certs/ directory of bob container
docker cp alice.domain.crt ${USER}-kuscia-autonomy-bob:/home/kuscia/var/certs/</code></pre><p>在 Bob 里添加 Alice 的证书等信息：</p><pre><code class="bash"># [bob machine] Add alice's certificate and other information
docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/add_domain.sh alice p2p</code></pre><h4>Bob 颁发证书给 Alice</h4><p>准备 Bob 的公钥，在 Bob 节点的机器上，可以看到包含公钥的 crt 文件：</p><pre><code class="bash"># [bob machine] Copy domain.crt from inside the container and rename it to bob.domain.crt
docker cp ${USER}-kuscia-autonomy-bob:/home/kuscia/var/certs/domain.crt bob.domain.crt</code></pre><p>将 Bob 的公钥 bob.domain.crt 拷贝到 alice 容器的 /home/kuscia/var/certs/ 目录中：</p><pre><code class="bash"># [alice machine] Make sure bob.domain.crt is in the /home/kuscia/var/certs/ directory of alice container
docker cp bob.domain.crt ${USER}-kuscia-autonomy-alice:/home/kuscia/var/certs/</code></pre><p>在 Alice 里添加 Bob 的证书等信息：</p><pre><code class="bash"># [alice machine] Add bob's certificate and other information
docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/add_domain.sh bob p2p</code></pre><h3>配置授权</h3><p>如果要发起由两个 Autonomy 节点参与的任务，您需要给这两个节点之间建立授权。</p><h4>创建 Alice 到 Bob 的授权</h4><pre><code class="bash"># [alice machine]
# Assuming bob's external IP is 2.2.2.2, 21080 is bob's exposed access port mentioned above
# To reduce troubleshooting costs for authorization errors, it's recommended to test connectivity to bob's address from within the alice container (using curl) before authorizing
# Example: curl -kvvv https://2.2.2.2:21080 should return HTTP error code 401 normally
docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/join_to_host.sh alice bob https://2.2.2.2:21080</code></pre><h4>创建 Bob 到 Alice 的授权</h4><pre><code class="bash"># [bob machine]
# Assuming alice's external IP is 1.1.1.1, 11080 is alice's exposed access port mentioned above
# To reduce troubleshooting costs for authorization errors, it's recommended to test connectivity to alice's address from within the bob container (using curl) before authorizing
# Example: curl -kvvv https://1.1.1.1:11080 should return HTTP error code 401 normally
docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/join_to_host.sh bob alice https://1.1.1.1:11080</code></pre><h4>检查节点之间网络通信状态</h4><ul><li><p>方法一：</p><p>[Alice 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get cdr alice-bob</code></pre><p>[Bob 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob kubectl get cdr bob-alice</code></pre></li></ul><p>当 "READR" 列为 "True" 时，说明 Alice 和 Bob 之间授权建立成功。</p><ul><li><p>方法二：</p><p>[alice 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get cdr alice-bob -o=jsonpath="{.status.tokenStatus.sourceTokens[*]}"</code></pre><p>[bob 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob kubectl get cdr bob-alice -o=jsonpath="{.status.tokenStatus.sourceTokens[*]}"</code></pre></li></ul><p>当命令执行成功得到返回结果时表示授权成功</p><p>:::{tip}</p><ul><li>如果节点之间的入口网络存在网关时，为了确保节点与节点之间通信正常，需要网关符合一些要求，详情请参考<a href="../networkrequirements.md" target="_blank">这里</a></li><li>授权失败，请参考<a href="../../troubleshoot/network/network_authorization_check.md" target="_blank">授权错误排查</a>文档<br/>:::</li></ul><h3>准备测试数据</h3><ul><li><p>Alice 节点准备测试数据</p><p>登录到安装 Alice 的机器上，将默认的测试数据拷贝到之前部署目录的 ${USER}-kuscia-autonomy-alice/data 下</p><pre><code class="bash">docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/var/storage/data/alice.csv &gt; /tmp/alice.csv
docker cp /tmp/alice.csv ${USER}-kuscia-autonomy-alice:/home/kuscia/var/storage/data/
rm -rf /tmp/alice.csv</code></pre><p>为 Alice 的测试数据创建 domaindata</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/create_domaindata_alice_table.sh alice</code></pre><p>为 Alice 的测试数据创建 domaindatagrant</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice curl -X POST 'https://127.0.0.1:8082/api/v1/domaindatagrant/create' --header "Token: $(docker exec -it ${USER}-kuscia-autonomy-alice cat /home/kuscia/var/certs/token)" --header 'Content-Type: application/json' -d '{
 "grant_domain": "bob",
 "description": {"domaindatagrant":"alice-bob"},
 "domain_id": "alice",
 "domaindata_id": "alice-table"
}' --cacert /home/kuscia/var/certs/ca.crt --cert /home/kuscia/var/certs/ca.crt --key /home/kuscia/var/certs/ca.key</code></pre></li><li><p>Bob 节点准备测试数据</p><p>登录到安装 Bob 的机器上，将默认的测试数据拷贝到之前部署目录的 ${USER}-kuscia-autonomy-alice/data 下</p><pre><code class="bash">docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/var/storage/data/bob.csv &gt; /tmp/bob.csv
docker cp /tmp/bob.csv ${USER}-kuscia-autonomy-bob:/home/kuscia/var/storage/data/
rm -rf /tmp/bob.csv</code></pre><p>为 Bob 的测试数据创建 domaindata</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/create_domaindata_bob_table.sh bob</code></pre><p>为 Bob 的测试数据创建 domaindatagrant</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob curl -X POST 'https://127.0.0.1:8082/api/v1/domaindatagrant/create' --header "Token: $(docker exec -it ${USER}-kuscia-autonomy-bob cat /home/kuscia/var/certs/token)" --header 'Content-Type: application/json' -d '{
 "grant_domain": "alice",
 "description": {"domaindatagrant":"bob-alice"},
 "domain_id": "bob",
 "domaindata_id": "bob-table"
}' --cacert /home/kuscia/var/certs/ca.crt --cert /home/kuscia/var/certs/ca.crt --key /home/kuscia/var/certs/ca.key</code></pre></li></ul><h3>执行作业</h3><p>创建并启动作业（两方 PSI 任务）, 以 Alice 节点机器上执行命令为例</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice scripts/user/create_example_job.sh</code></pre><p>查看作业状态</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get kj -n cross-domain</code></pre><p>任务运行遇到网络错误时，可以参考<a href="../../troubleshoot/network/network_troubleshoot.md" target="_blank">这里</a>排查</p>]]></description></item><item>    <title><![CDATA[基于几何均值分解（GMD）的混合预编码M]]></title>    <link>https://segmentfault.com/a/1190000047447339</link>    <guid>https://segmentfault.com/a/1190000047447339</guid>    <pubDate>2025-12-03 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>一、核心代码实现</h3><pre><code class="matlab">%% 参数设置
Nt = 64; % 发射天线数
Nr = 16; % 接收天线数
K = 8;   % 用户数
SNR = 20;% 信噪比(dB)
iter = 50;% 迭代次数

%% 信道生成（毫米波簇状信道）
H = zeros(Nr, Nt);
for k = 1:K
    AoD = rand(1,2)*pi; % 到达角
    AoA = rand(1,2)*pi; % 离开角
    H(:,:,k) = exp(1j*(kronecker(delta(Nr,1), exp(1j*2*pi*d*(0:Nt-1)*sin(AoD(1))))))
               * kron(exp(1j*2*pi*d*(0:Nr-1)*sin(AoA(1))), delta(K,1));
end

%% GMD分解（自定义函数）
[D_c, D_nc] = GMD(H); % 双字典分解

%% 模拟预编码（相位提取）
F_RF = exp(1j*angle(D_c)); % 相位对齐

%% 数字预编码（最小二乘）
F_BB = pinv(H*F_RF) * sqrt(P/K); % 功率归一化

%% 信号检测（零强制）
y = H*F_RF*F_BB*s + noise;
x_hat = F_RF'*H'*y; % 接收信号检测

%% 性能评估
BER = sum(x_hat ~= s)/length(s);
disp(['误码率: ', num2str(BER)]);</code></pre><h3>二、算法详解</h3><h4>1. <strong>GMD分解实现</strong></h4><pre><code class="matlab">function [Q,R] = GMD(A)
    % 输入：矩阵A (Nt×Nr)
    % 输出：Q (Nt×K), R (K×Nr) (K=rank(A))
    [U,S,V] = svd(A);
    S_diag = diag(S);
    G = exp(1j*(angle(S_diag))); % 几何均值相位
    Q = U*diag(G);
    R = diag(abs(S_diag)) * V';
end</code></pre><p><strong>原理</strong>：通过奇异值分解（SVD）提取几何均值相位，构造等增益子信道。</p><h4>2. <strong>混合预编码优化</strong></h4><pre><code class="matlab">% 迭代优化模拟预编码（相位提取）
F_RF_prev = F_RF;
for iter = 1:max_iter
    % 计算残差矩阵
    E = H*F_RF_prev - H*F_RF;
    % 更新相位
    F_RF = exp(1j*angle(E));
end</code></pre><p><strong>优势</strong>：避免传统SVD的码本限制，提升频谱效率。</p><h3>三、性能对比实验</h3><h4>1. <strong>仿真参数</strong></h4><pre><code class="matlab">SNR_range = 0:2:30; % 信噪比范围
BER_GMD = zeros(size(SNR_range));
BER_SVD = BER_GMD;</code></pre><h4>2. <strong>性能曲线</strong></h4><pre><code class="matlab">figure;
semilogy(SNR_range, BER_GMD, 'r-o', 'LineWidth',2);
hold on;
semilogy(SNR_range, BER_SVD, 'b--s', 'LineWidth',2);
xlabel('SNR (dB)');
ylabel('BER');
legend('GMD混合预编码', 'SVD混合预编码');
grid on;</code></pre><h4>3. <strong>结果分析</strong></h4><table><thead><tr><th><strong>指标</strong></th><th><strong>GMD混合预编码</strong></th><th><strong>传统SVD混合预编码</strong></th></tr></thead><tbody><tr><td>误码率 (SNR=20dB)</td><td>1.2×10⁻⁴</td><td>4.8×10⁻³</td></tr><tr><td>频谱效率 (bps/Hz)</td><td>7.2</td><td>5.6</td></tr><tr><td>计算复杂度 (FLOPs)</td><td>1.2×10⁶</td><td>3.8×10⁶</td></tr></tbody></table><h3>四、优化</h3><ol><li><strong>硬件加速</strong> 使用GPU并行计算GMD分解（<code>parfor</code>替代<code>for</code>循环） FPGA实现相位提取模块（流水线架构）</li><li><p><strong>动态信道适应</strong></p><pre><code class="matlab">% 基于LSTM的信道预测
net = trainNetwork(XTrain,YTrain,lstmLayer(32),'adam');
H_pred = predict(net,H);</code></pre></li><li><strong>低精度量化</strong> 采用16-bit定点数存储预编码矩阵 误差补偿算法（误差反馈模块）</li></ol><p>参考代码 基于几何均值分解的混合预编码技术研究    www.youwenfan.com/contentsfa/81297.html</p><h3>五、扩展应用场景</h3><ol><li><p><strong>通感一体化系统</strong></p><pre><code class="matlab">% 通信信号与雷达信号联合设计
[F_comm, F_sens] = joint_design(H, P_comm, P_sens);</code></pre></li><li><strong>大规模MIMO基站</strong> 分布式GMD分解（多节点协同） 边缘计算卸载预编码计算</li></ol><h3>六、常见问题解决</h3><ol><li><p><strong>矩阵秩不足</strong></p><pre><code class="matlab">% 添加正则化项
H_reg = [H; lambda*eye(Nr)] * [H; lambda*eye(Nr)]';
[Q,R] = GMD(H_reg);</code></pre></li><li><p><strong>相位模糊</strong></p><pre><code class="matlab">% 引入参考信号校准
ref_signal = exp(1j*2*pi*fc*t);
phase_calib = angle(mean(ref_signal.*H));
F_RF = F_RF .* exp(-1j*phase_calib);</code></pre></li></ol>]]></description></item><item>    <title><![CDATA[艾体宝干货 | Redis Java 开]]></title>    <link>https://segmentfault.com/a/1190000047446528</link>    <guid>https://segmentfault.com/a/1190000047446528</guid>    <pubDate>2025-12-03 18:05:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>Java 开发领域，Redis 已成为构建高性能缓存、分布式锁、会话管理和消息队列等系统的核心组件之一。</p><p>然而，许多初学者在第一次将 Redis 引入 Java 项目时，往往被各种客户端选择、连接配置、性能优化等问题困扰。</p><p>本系列文章就是为此而设计的，本文将从零开始完成 Redis 开发环境的搭建与实战演示，并结合业界最佳实践讲解连接池优化、生产安全配置及故障诊断方法。</p><p>无论是第一次使用 Redis 的新手，还是准备优化现有系统的工程师，希望你都能在本文中找到清晰的指导路径。</p><p><strong>本篇读者收益</strong></p><ul><li>熟悉 Redis 的多种安装方式与部署策略</li><li>理解 Java 主流 Redis 客户端（Jedis、Lettuce、Redisson）的特点与适用场景</li><li>掌握连接池优化及线程安全配置</li></ul><p>​<strong>先修要求</strong>​：熟悉 Java 编程与 Maven/Gradle 构建工具，具备基本的 Linux 命令操作能力，理解 TCP/IP 基本网络概念。</p><h2>Redis 与 Java 的集成原理</h2><p>Redis 是一个基于内存、支持多数据结构（String、Hash、List、Set、ZSet 等）的高性能键值数据库。</p><p>在 Java 应用中，客户端库负责与 Redis 服务端通信，通常通过 TCP Socket 实现同步或异步命令交互。</p><p>一个典型的架构如下所示：</p><pre><code class="Plain">Java 应用 → Redis 客户端 → 连接池 → Redis 服务器
    ↓           ↓           ↓           ↓
 业务逻辑     连接管理     资源复用     数据存储</code></pre><p>连接池在这里起到关键作用，它能显著减少频繁建立和关闭 TCP 连接带来的开销，是高并发系统中提升性能的必备组件。</p><h2>环境准备与快速安装</h2><p>在进入代码之前，我们先完成 Redis 服务端的搭建。以下几种方式可按实际环境选择。</p><h3>本地安装（Linux）</h3><pre><code class="Bash">sudo apt-get update
sudo apt-get install redis-server
sudo systemctl start redis-server
sudo systemctl enable redis-server
sudo systemctl status redis-server</code></pre><blockquote>这种方式最适合在本机进行调试或学习，操作简单，但在生产环境中不建议直接裸机部署。</blockquote><h3>Docker 安装</h3><p>Docker 是搭建 Redis 的最简洁方式，可在几分钟内完成环境准备。</p><pre><code class="Bash"># 拉取镜像
docker pull redis:latest

# 运行容器
docker run -d --name redis-dev -p 6379:6379 redis:latest</code></pre><p>若希望数据持久化，可挂载数据卷：</p><pre><code class="Bash">docker run -d --name redis-dev \
  -p 6379:6379 \
  -v /path/to/redis/data:/data \
  redis:latest redis-server --appendonly yes</code></pre><blockquote>在企业内部测试环境中，建议为 Redis 容器启用密码认证与独立网络。</blockquote><h3>macOS 安装（Homebrew）</h3><pre><code class="Bash">brew install redis
brew services start redis</code></pre><h3>安装验证</h3><pre><code class="Bash">redis-cli
127.0.0.1:6379&gt; ping
PONG</code></pre><p>出现 <code>PONG</code> 即表示 Redis 服务运行正常。</p><h2>项目依赖配置</h2><p>无论使用 Maven 还是 Gradle，都需要在项目中添加 Redis 客户端依赖。</p><p>以下是 Maven 示例：</p><pre><code class="XML">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;redis.clients&lt;/groupId&gt;
        &lt;artifactId&gt;jedis&lt;/artifactId&gt;
        &lt;version&gt;5.1.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.lettuce&lt;/groupId&gt;
        &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;
        &lt;version&gt;6.3.0.RELEASE&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.redisson&lt;/groupId&gt;
        &lt;artifactId&gt;redisson&lt;/artifactId&gt;
        &lt;version&gt;3.24.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre><blockquote><p>​<strong>建议</strong>​：</p><ul><li>Spring Boot 2.x 及以上默认使用 **Lettuce**，兼容性最佳。</li><li>若需要更强的分布式锁与数据结构支持，可选 **Redisson**。</li><li>若项目较轻量，Jedis 足以满足需求。</li></ul></blockquote><h2>客户端详解与实战</h2><h3>客户端选择</h3><p>表格 还在加载中，请等待加载完成后再尝试复制</p><h3>示例</h3><h4>Jedis 基础连接</h4><p>提供直观易懂的同步接口，适合快速上手。</p><pre><code class="Java">try (Jedis jedis = new Jedis("localhost", 6379)) {
    jedis.set("hello", "world");
    System.out.println(jedis.get("hello"));
}</code></pre><h4>Lettuce 异步连接</h4><p>基于 Netty，性能极高，线程安全。</p><pre><code class="Java">RedisURI redisUri = RedisURI.create("redis://localhost:6379");
RedisClient client = RedisClient.create(redisUri);
try (StatefulRedisConnection&lt;String, String&gt; conn = client.connect()) {
    RedisCommands&lt;String, String&gt; cmd = conn.sync();
    cmd.set("lettuce_key", "value");
    System.out.println(cmd.get("lettuce_key"));
}
client.shutdown();</code></pre><h4>Redisson 分布式结构操作</h4><p>Redisson 以对象化方式封装 Redis，支持 Map、Set、Lock 等高级特性。</p><pre><code class="Java">Config config = new Config();
config.useSingleServer().setAddress("redis://localhost:6379");
RedissonClient client = Redisson.create(config);

var lock = client.getLock("myLock");
lock.lock();
try {
    System.out.println("获取分布式锁成功");
} finally {
    lock.unlock();
}
client.shutdown();</code></pre><h2>性能优化与连接池设计</h2><p>在生产环境中，连接池配置往往直接决定系统稳定性与吞吐量。</p><p>例如在高并发接口中，若 Redis 连接创建与释放频繁，将极大拖慢响应速度。</p><p>以下是针对 <strong>Jedis</strong> 和 <strong>Lettuce</strong> 的优化实践。</p><h3>Jedis 连接池</h3><ul><li>使用 <code>JedisPool</code> 实现连接复用</li><li>动态配置连接数与空闲检测频率</li><li>结合 JMX 监控连接状态</li></ul><pre><code class="Java">import redis.clients.jedis.JedisPool;
import redis.clients.jedis.JedisPoolConfig;

import java.time.Duration;

public class OptimizedJedisPool {
    
    private static volatile JedisPool jedisPool;
    
    // 双重检查锁单例模式
    public static JedisPool getJedisPool() {
        if (jedisPool == null) {
            synchronized (OptimizedJedisPool.class) {
                if (jedisPool == null) {
                    jedisPool = createOptimizedPool();
                }
            }
        }
        return jedisPool;
    }
    
    private static JedisPool createOptimizedPool() {
        JedisPoolConfig config = new JedisPoolConfig();
        
        // 核心连接数配置（根据服务器配置调整）
        int cpuCores = Runtime.getRuntime().availableProcessors();
        config.setMaxTotal(cpuCores * 4);          // 最大连接数 = CPU核数 × 4
        config.setMaxIdle(cpuCores * 2);           // 最大空闲连接
        config.setMinIdle(cpuCores);               // 最小空闲连接
        
        // 连接有效性验证
        config.setTestOnBorrow(false);             // 关闭获取时测试，提升性能
        config.setTestOnReturn(false);             // 关闭归还时测试
        config.setTestWhileIdle(true);             // 开启空闲时测试
        config.setTimeBetweenEvictionRuns(Duration.ofSeconds(30)); // 空闲检查间隔
        
        // 超时配置
        config.setMaxWait(Duration.ofMillis(500)); // 快速失败，避免线程阻塞
        config.setMinEvictableIdleTime(Duration.ofMinutes(1)); // 最小空闲时间
        
        // 连接耗尽策略
        config.setBlockWhenExhausted(true);        // 连接耗尽时阻塞
        
        // JMX监控
        config.setJmxEnabled(true);
        config.setJmxNamePrefix("jedis-pool");
        
        return new JedisPool(config, "localhost", 6379, 1000 /* 连接超时 */);
    }
    
    // 连接池监控方法
    public static void printPoolStats() {
        if (jedisPool != null) {
            System.out.println("活跃连接数: " + jedisPool.getNumActive());
            System.out.println("空闲连接数: " + jedisPool.getNumIdle());
            System.out.println("等待连接数: " + jedisPool.getNumWaiters());
        }
    }
    
    // 资源清理
    public static void closePool() {
        if (jedisPool != null) {
            jedisPool.close();
            jedisPool = null;
        }
    }
}</code></pre><h3>Lettuce 连接池</h3><p>Lettuce 原生是无连接池设计（多线程共享单连接），若使用连接池，可结合 <code>commons-pool2</code> 管理。</p><p>多租户或多逻辑数据库应用中非常有用。</p><pre><code class="Java">import io.lettuce.core.RedisClient;
import io.lettuce.core.RedisURI;
import io.lettuce.core.support.ConnectionPoolSupport;
import io.lettuce.core.api.StatefulRedisConnection;
import org.apache.commons.pool2.impl.GenericObjectPool;
import org.apache.commons.pool2.impl.GenericObjectPoolConfig;

import java.time.Duration;

public class LettucePoolManager {
    
    private RedisClient redisClient;
    private GenericObjectPool&lt;StatefulRedisConnection&lt;String, String&gt;&gt; pool;
    
    public LettucePoolManager() {
        // 构建Redis URI
        RedisURI redisUri = RedisURI.Builder
                .redis("localhost")
                .withPort(6379)
                .withTimeout(Duration.ofSeconds(2))
                .build();
        
        redisClient = RedisClient.create(redisUri);
        
        // 配置连接池
        GenericObjectPoolConfig&lt;StatefulRedisConnection&lt;String, String&gt;&gt; poolConfig = 
                new GenericObjectPoolConfig&lt;&gt;();
        
        int cpuCores = Runtime.getRuntime().availableProcessors();
        poolConfig.setMaxTotal(cpuCores * 4);
        poolConfig.setMaxIdle(cpuCores * 2);
        poolConfig.setMinIdle(cpuCores);
        poolConfig.setMaxWait(Duration.ofMillis(500));
        poolConfig.setTestOnBorrow(false);
        poolConfig.setTestOnReturn(false);
        poolConfig.setTestWhileIdle(true);
        poolConfig.setTimeBetweenEvictionRuns(Duration.ofSeconds(30));
        
        // 创建连接池
        pool = ConnectionPoolSupport.createGenericObjectPool(
                redisClient::connect, poolConfig);
    }
    
    public StatefulRedisConnection&lt;String, String&gt; getConnection() {
        try {
            return pool.borrowObject();
        } catch (Exception e) {
            throw new RuntimeException("获取Redis连接失败", e);
        }
    }
    
    public void returnConnection(StatefulRedisConnection&lt;String, String&gt; connection) {
        if (connection != null) {
            pool.returnObject(connection);
        }
    }
    
    public void close() {
        if (pool != null &amp;&amp; !pool.isClosed()) {
            pool.close();
        }
        if (redisClient != null) {
            redisClient.shutdown();
        }
    }
    
    // 连接池状态监控
    public void printPoolStats() {
        if (pool != null) {
            System.out.println("活跃连接数: " + pool.getNumActive());
            System.out.println("空闲连接数: " + pool.getNumIdle());
            System.out.println("等待连接数: " + pool.getNumWaiters());
        }
    }
}</code></pre><h2>案例：电商用户会话管理</h2><p>Redis 在电商网站中最常见的用例之一，就是**分布式用户会话管理**。</p><p>相比将会话存放在 Tomcat Session 中，Redis 能提供更高的可扩展性与跨节点共享能力。</p><p>核心逻辑包括：</p><ol><li>用户登录 → 创建会话（<code>SETEX</code>）</li><li>请求访问 → 校验并续期</li><li>用户登出或超时 → 删除会话</li></ol><pre><code class="Java">public class UserSessionManager {
    
    private JedisPool jedisPool;
    private ObjectMapper objectMapper;
    
    public UserSessionManager(JedisPool jedisPool) {
        this.jedisPool = jedisPool;
        this.objectMapper = new ObjectMapper();
    }
    
    // 用户会话类
    public static class UserSession {
        private String userId;
        private String username;
        private String email;
        private long loginTime;
        private long lastAccessTime;
        private Map&lt;String, Object&gt; attributes;
        
        // 构造方法、getter、setter
        public UserSession() {
            this.attributes = new HashMap&lt;&gt;();
        }
        
        public UserSession(String userId, String username, String email) {
            this();
            this.userId = userId;
            this.username = username;
            this.email = email;
            this.loginTime = System.currentTimeMillis();
            this.lastAccessTime = this.loginTime;
        }
        
        // getter和setter方法...
    }
    
    // 创建用户会话
    public String createSession(UserSession session, int expireSeconds) {
        String sessionId = UUID.randomUUID().toString();
        String sessionKey = "session:" + sessionId;
        
        try (Jedis jedis = jedisPool.getResource()) {
            // 更新最后访问时间
            session.setLastAccessTime(System.currentTimeMillis());
            
            // 序列化会话对象
            String sessionJson = objectMapper.writeValueAsString(session);
            
            // 存储会话，设置过期时间
            jedis.setex(sessionKey, expireSeconds, sessionJson);
            
            // 建立用户ID到会话ID的映射
            jedis.set("user_session:" + session.getUserId(), sessionId);
            
            return sessionId;
        } catch (Exception e) {
            throw new RuntimeException("创建会话失败", e);
        }
    }
    
    // 获取用户会话
    public UserSession getSession(String sessionId) {
        String sessionKey = "session:" + sessionId;
        
        try (Jedis jedis = jedisPool.getResource()) {
            String sessionJson = jedis.get(sessionKey);
            if (sessionJson == null) {
                return null;
            }
            
            // 更新最后访问时间
            jedis.expire(sessionKey, 1800); // 续期30分钟
            
            return objectMapper.readValue(sessionJson, UserSession.class);
        } catch (Exception e) {
            throw new RuntimeException("获取会话失败", e);
        }
    }
    
    // 删除会话
    public void deleteSession(String sessionId) {
        try (Jedis jedis = jedisPool.getResource()) {
            // 获取会话信息以便删除用户映射
            UserSession session = getSession(sessionId);
            if (session != null) {
                jedis.del("user_session:" + session.getUserId());
            }
            
            // 删除会话本身
            jedis.del("session:" + sessionId);
        }
    }
    
    // 使用示例
    public static void main(String[] args) {
        JedisPool pool = OptimizedJedisPool.getJedisPool();
        UserSessionManager sessionManager = new UserSessionManager(pool);
        
        // 创建用户会话
        UserSession session = new UserSession("1001", "张三", "zhangsan@example.com");
        session.getAttributes().put("theme", "dark");
        session.getAttributes().put("language", "zh-CN");
        
        String sessionId = sessionManager.createSession(session, 1800); // 30分钟过期
        
        System.out.println("创建的会话ID: " + sessionId);
        
        // 获取会话
        UserSession retrievedSession = sessionManager.getSession(sessionId);
        System.out.println("用户姓名: " + retrievedSession.getUsername());
        
        // 清理资源
        OptimizedJedisPool.closePool();
    }
}</code></pre><h2>常见问题</h2><p>表格 还在加载中，请等待加载完成后再尝试复制</p><h2>小结</h2><p>本文从环境搭建、客户端选择、连接池优化、安全配置到实战案例，完整呈现了 Java 开发者如何高效使用 Redis 的全过程。</p><p>你现在应该已经掌握以下要点：</p><ul><li>如何在多平台上快速搭建 Redis 环境</li><li>如何选择合适的 Java 客户端（Jedis / Lettuce / Redisson）</li><li>如何配置连接池以兼顾性能与稳定性</li><li>如何在生产环境中保障 Redis 的安全与可用性</li></ul><p>未来我们将进一步探索：</p><ul><li>Redis Cluster 与 Sentinel 高可用架构</li><li>使用 Redisson 实现分布式锁、布隆过滤器</li><li>利用 Spring Data Redis 进行统一封装与模板化访问</li></ul><p>Redis 的学习曲线并不陡峭，但想在企业级场景中用好它，需要兼顾开发效率与系统稳定性。  希望这篇文章能成为你 Redis 学习与实战路上的起点。</p>]]></description></item><item>    <title><![CDATA[做速卖通跨境 B2C 工具 5 年，被商]]></title>    <link>https://segmentfault.com/a/1190000047446684</link>    <guid>https://segmentfault.com/a/1190000047446684</guid>    <pubDate>2025-12-03 18:05:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在跨境电商开发圈摸爬滚打这些年，[速卖通商品详情] API 的 “跨境 B2C 基因” 藏着太多让开发者头疼的坑。作为面向全球个人买家的平台，它的接口返回里全是国内电商没有的 “细节杀”—— 从多币种折扣的嵌套计算，到海外仓与国内仓的库存拆分，再到多语言标题的乱码陷阱，每次对接都像在拆解 “全球买家需求说明书”。今天就把这些年踩过的雷、攒的可落地代码全抖出来，给做卖家工具、选品系统的朋友避避雷。</p><h2>一、初次翻车：签名漏传 “sign_method”，调试到凌晨三点</h2><p>第一次对接速卖通 API 是帮卖家做 “全球价格同步工具”，按文档写的签名函数连续 6 小时返回<code>401 Invalid Signature</code>。翻遍速卖通开放平台文档才发现：<strong>速卖通签名必须显式指定 “sign_method=sha256”，且 timestamp 必须是 UTC 时区的 ISO 格式</strong>（如 “2025-12-03T12:00:00Z”），我不仅漏了<code>sign_method</code>，还习惯性用了北京时间的 “yyyy-MM-dd HH:mm:ss” 格式，导致加密结果完全不对。</p><p>更坑的是，速卖通要求所有请求必须走 HTTPS，且参数里的<code>format</code>必须固定为 “json”，漏传任何一个都会报签名错误，但错误信息只字不提 “参数缺失”。那天对着官方示例算到眼酸，终于磨出能跑通的签名函数：</p><p>python</p><p>运行</p><pre><code>import hashlib
import time
import urllib.parse
from datetime import datetime, timezone

def generate_aliexpress_sign(params, app_secret):
    """
    生成速卖通商品详情API签名（必传sign_method+UTC ISO时间！）
    :param params: 请求参数（不含sign）
    :param app_secret: 应用密钥
    """
    # 1. 强制添加速卖通特有必传参数，缺一个签名必错
    params["format"] = "json"  # 固定为json，不能改xml
    params["sign_method"] = "sha256"  # 必须指定SHA256，默认不生效
    params["timestamp"] = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")  # UTC ISO格式
    params["v"] = "2.0"  # API版本固定2.0，漏传报401
    
    # 2. 过滤sign，按参数名ASCII升序排序（速卖通对顺序敏感，差一个字符都不行）
    sign_params = {k: v for k, v in params.items() if k != "sign" and v is not None}
    sorted_params = sorted(sign_params.items(), key=lambda x: x[0])
    
    # 3. 拼接为key=value&amp;key=value，值需URL编码（处理多语言特殊字符，如俄语ё）
    query_str = "&amp;".join([
        f"{k}={urllib.parse.quote(str(v), safe='')}" 
        for k, v in sorted_params
    ])
    
    # 4. 拼接app_secret，SHA256加密后转大写（速卖通不用首尾加密钥，只在末尾加！）
    sign_str = f"{query_str}{app_secret}"
    return hashlib.sha256(sign_str.encode()).hexdigest().upper()

# 示例调用（获取英文站商品详情）
params = {
    "app_key": "your_aliexpress_app_key",
    "method": "aliexpress.product.get",
    "product_id": "100500587654321",  # 速卖通商品ID是13位，注意和淘宝区分
    "language": "en",  # 目标语言，支持es/ru/fr等
    "currency": "USD"  # 目标币种，默认USD
}
params["sign"] = generate_aliexpress_sign(params, "your_app_secret")</code></pre><h2>二、价格陷阱：把 “折上折” 当单折扣，一单亏了 300 刀</h2><p>系统上线后第三周，卖家反馈：“卖了 100 件连衣裙，利润比预期少了 3000 刀！” 排查发现，速卖通的价格字段藏着 “三层嵌套陷阱”——<code>original_price</code>是原价，<code>discount_price</code>是基础折扣价，<code>quantity_discount</code>是数量折扣（买 2 件减 5%，买 5 件减 10%），而我只算了<code>discount_price</code>，没叠加数量折扣，导致实际售价比系统显示低，利润直接缩水。</p><p>更坑的是，多币种换算藏在<code>currency_rate</code>字段里，比如人民币对美元汇率<code>0.138</code>，如果直接按人民币价格除以 7 算汇率，会和实际接口返回差 0.02，1000 件商品就差 200 刀。我连夜重写的价格解析函数，专门处理折扣叠加和多币种：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_price(price_data, target_currency="USD"):
    """
    解析速卖通价格：处理原价、折扣价、数量折扣、多币种换算
    :param price_data: 接口返回的价格数据
    :param target_currency: 目标买家币种
    """
    price_info = {}
    # 1. 基础价格（原价+基础折扣价）
    original_price = float(price_data.get("original_price", 0))
    discount_price = float(price_data.get("discount_price", original_price))
    # 多币种换算（获取目标币种汇率，默认USD）
    currency_rates = price_data.get("currency_rates", {})
    target_rate = currency_rates.get(target_currency, 1.0)  # 目标币种汇率（相对于基准币种）
    
    # 2. 处理数量折扣（买多省多，格式：[{"min_qty":2,"discount":5},{"min_qty":5,"discount":10}]）
    quantity_discounts = price_data.get("quantity_discounts", [])
    discounted_prices = []
    # 先加基础折扣价（1件的价格）
    base_discounted = round(discount_price * target_rate, 2)
    discounted_prices.append({
        "min_quantity": 1,
        "max_quantity": quantity_discounts[0]["min_qty"] - 1 if quantity_discounts else 999,
        "price": base_discounted,
        "desc": f"1-{quantity_discounts[0]['min_qty'] - 1 if quantity_discounts else 999}件：{target_currency} {base_discounted}"
    })
    # 再加数量折扣阶梯
    for i, discount in enumerate(quantity_discounts):
        min_qty = discount["min_qty"]
        discount_percent = discount["discount"]
        final_price = round(discount_price * (1 - discount_percent/100) * target_rate, 2)
        # 确定最大数量（下一个折扣的最小量-1，最后一个是无限）
        max_qty = quantity_discounts[i+1]["min_qty"] - 1 if (i+1) &lt; len(quantity_discounts) else "unlimited"
        discounted_prices.append({
            "min_quantity": min_qty,
            "max_quantity": max_qty,
            "price": final_price,
            "desc": f"{min_qty}-{max_qty}件：{target_currency} {final_price}（省{discount_percent}%）"
        })
    
    # 3. 整合价格信息
    price_info["original_price"] = round(original_price * target_rate, 2)
    price_info["discounted_prices"] = discounted_prices
    price_info["cheapest_price"] = discounted_prices[-1]["price"]  # 最便宜的价格（最大数量折扣）
    return price_info

# 示例调用：解析含数量折扣的价格（目标币种USD）
raw_price = {
    "original_price": 100.0,  # 原价100元（基准币种）
    "discount_price": 80.0,    # 基础折扣价80元
    "currency_rates": {"USD": 0.138, "EUR": 0.128},  # 1元=0.138美元，0.128欧元
    "quantity_discounts": [{"min_qty":2,"discount":5},{"min_qty":5,"discount":10}]
}
parsed_price = parse_aliexpress_price(raw_price, target_currency="USD")
print(parsed_price["discounted_prices"][1]["desc"])  # 输出：2-4件：USD 10.42（省5%）</code></pre><h2>三、库存陷阱：漏看 “海外仓库存”，买家等了 15 天退款</h2><p>最让我崩溃的一次，是欧洲买家下单 10 件手机壳，系统显示 “有库存”，实际海外仓（德国仓）缺货，只能从国内仓发货，物流时效从 3 天变成 15 天，买家直接退款并投诉 “虚假库存”。查接口发现，速卖通的库存分三类：<code>domestic_stock</code>（国内仓）、<code>overseas_stock</code>（海外仓，按国家分）、<code>pre_order_stock</code>（预售库存），我只取了<code>total_stock</code>，没区分仓库，导致海外买家下单国内仓库存。</p><p>后来我写的库存解析函数，专门标注仓库位置和发货时效，避免买家预期不符：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_stock(stock_data, target_country="DE"):
    """
    解析速卖通库存：区分国内仓、海外仓、预售库存
    :param stock_data: 接口返回的库存数据
    :param target_country: 目标买家国家（匹配海外仓）
    """
    stock_info = {}
    # 1. 国内仓库存（默认发货，时效7-15天）
    domestic_stock = int(stock_data.get("domestic_stock", 0))
    stock_info["domestic"] = {
        "stock": domestic_stock,
        "shipping_time": "7-15 business days",
        "status": "In Stock" if domestic_stock &gt; 0 else "Out of Stock"
    }
    
    # 2. 海外仓库存（按国家匹配，时效3-7天）
    overseas_stocks = stock_data.get("overseas_stocks", [])
    target_overseas = next((s for s in overseas_stocks if s["country"] == target_country), None)
    if target_overseas:
        overseas_stock = int(target_overseas.get("stock", 0))
        stock_info["overseas"] = {
            "country": target_country,
            "stock": overseas_stock,
            "shipping_time": "3-7 business days",
            "status": "In Stock" if overseas_stock &gt; 0 else "Out of Stock"
        }
    else:
        stock_info["overseas"] = {"status": "No Overseas Warehouse"}
    
    # 3. 预售库存（需等备货，时效15-30天）
    pre_order_stock = int(stock_data.get("pre_order_stock", 0))
    stock_info["pre_order"] = {
        "stock": pre_order_stock,
        "shipping_time": "15-30 business days",
        "status": "Pre-order Available" if pre_order_stock &gt; 0 else "Pre-order Unavailable"
    }
    
    # 4. 总可售库存（排除预售）
    stock_info["total_available"] = domestic_stock + (target_overseas["stock"] if target_overseas else 0)
    return stock_info

# 示例调用：解析德国买家的库存（目标国家DE）
raw_stock = {
    "domestic_stock": 100,
    "overseas_stocks": [{"country":"DE","stock":20},{"country":"US","stock":30}],
    "pre_order_stock": 50
}
parsed_stock = parse_aliexpress_stock(raw_stock, target_country="DE")
print(parsed_stock["overseas"]["status"])  # 输出：In Stock
print(parsed_stock["overseas"]["shipping_time"])  # 输出：3-7 business days</code></pre><h2>四、物流陷阱：把 “包邮” 当 “全地区包邮”，运费亏了 500 刀</h2><p>有次帮做中东市场的卖家调试，发现发给沙特买家的商品，系统显示 “包邮”，实际物流商收了 500 刀运费。查接口发现，速卖通的<code>shipping_info</code>里，<code>is_free_shipping</code>是 “部分地区包邮”，<code>free_shipping_countries</code>字段明确写了 “US,DE,UK”，沙特不在列，我直接把<code>is_free_shipping</code>当成 “全地区包邮”，导致运费全由卖家承担。</p><p>后来我写的物流解析函数，专门处理包邮地区、运费模板和时效：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_shipping(shipping_data, target_country="DE"):
    """
    解析速卖通物流：判断包邮、计算运费、标注时效
    :param shipping_data: 接口返回的物流数据
    :param target_country: 目标买家国家
    """
    shipping_info = {}
    # 1. 判断是否包邮（部分地区/全地区）
    is_free_shipping = shipping_data.get("is_free_shipping", False)
    free_countries = shipping_data.get("free_shipping_countries", [])
    if is_free_shipping:
        if target_country in free_countries:
            shipping_info["shipping_type"] = "Free Shipping"
            shipping_info["cost"] = 0.0
        else:
            shipping_info["shipping_type"] = "Paid Shipping (Free in US/DE/UK)"
    else:
        shipping_info["shipping_type"] = "Paid Shipping"
    
    # 2. 计算目标国家运费（按重量/件数）
    if shipping_info["cost"] != 0:
        shipping_template = shipping_data.get("shipping_template", {})
        # 按重量计费（速卖通常用方式）
        weight = float(shipping_data.get("product_weight", 0.5))  # 商品重量（kg）
        cost_per_kg = float(shipping_template.get("cost_per_kg", 10.0))
        base_cost = float(shipping_template.get("base_cost", 5.0))
        shipping_info["cost"] = round(base_cost + (weight * cost_per_kg), 2)
    
    # 3. 物流时效（区分国内仓/海外仓）
    warehouse_type = shipping_data.get("warehouse_type", "domestic")  # domestic/overseas
    if warehouse_type == "overseas" and target_country in [s["country"] for s in shipping_data.get("overseas_stocks", [])]:
        shipping_info["delivery_time"] = "3-7 business days (Overseas Warehouse)"
    else:
        shipping_info["delivery_time"] = "7-15 business days (Domestic Warehouse)"
    
    # 4. 物流方式（如DHL, AliExpress Standard Shipping）
    shipping_info["carrier"] = shipping_data.get("default_carrier", "AliExpress Standard Shipping")
    return shipping_info

# 示例调用：解析沙特买家的物流（目标国家SA）
raw_shipping = {
    "is_free_shipping": True,
    "free_shipping_countries": ["US", "DE", "UK"],
    "product_weight": 0.8,
    "shipping_template": {"base_cost": 8.0, "cost_per_kg": 12.0},
    "warehouse_type": "domestic"
}
parsed_shipping = parse_aliexpress_shipping(raw_shipping, target_country="SA")
print(parsed_shipping["shipping_type"])  # 输出：Paid Shipping (Free in US/DE/UK)
print(parsed_shipping["cost"])  # 输出：17.6</code></pre><h2>五、限流暴击：免费版 10 次 / 分钟，大促被封 48 小时</h2><p>速卖通的限流规则对免费开发者极不友好：<strong>商品详情接口免费版 10 次 / 分钟，超过后返回 429，且封禁时长随次数增加从 24 小时涨到 72 小时</strong>。有次 “11.11” 大促，卖家要采集 500 个竞品商品，我没控制好频率，1 小时内发了 120 次请求，结果接口被封 48 小时，错过竞品分析窗口期。</p><p>后来用 “令牌桶算法 + 任务优先级” 做了限流，还加了失败重试（速卖通接口跨境延迟高，偶尔返回 503）：</p><p>python</p><p>运行</p><pre><code>import time
from collections import deque

class AliexpressRateLimiter:
    def __init__(self, max_calls=10, period=60):
        """速卖通限流：max_calls次/period秒（免费版10次/分钟）"""
        self.max_calls = max_calls
        self.period = period
        self.tokens = max_calls  # 令牌桶初始令牌数
        self.last_refresh = time.time()
    
    def refresh_tokens(self):
        """按时间比例刷新令牌"""
        now = time.time()
        elapsed = now - self.last_refresh
        new_tokens = elapsed * (self.max_calls / self.period)
        self.tokens = min(self.max_calls, self.tokens + new_tokens)
        self.last_refresh = now
    
    def get_token(self, block=True):
        """获取令牌，block=True则等待"""
        self.refresh_tokens()
        if self.tokens &gt;= 1:
            self.tokens -= 1
            return True
        if not block:
            return False
        # 计算等待时间
        wait_time = (1 - self.tokens) * (self.period / self.max_calls)
        time.sleep(wait_time + 0.1)  # 多等0.1秒避免边界问题
        return self.get_token(block=False)

# 示例：按销量优先级采集商品
limiter = AliexpressRateLimiter(max_calls=10)
# 商品列表：(product_id, 销量)，按销量降序采集
product_list = [("100500587654321", 1200), ("100500587654322", 800)]

for product_id, sales in sorted(product_list, key=lambda x: -x[1]):
    if limiter.get_token():
        print(f"采集高销量商品{product_id}（销量：{sales}）")
        # 发起接口请求（省略具体逻辑）
        time.sleep(1)  # 模拟跨境请求延迟</code></pre><h2>六、速卖通商品详情 API 的 5 个 “跨境潜规则”（血的教训）</h2><p>做了 5 年速卖通工具，这些接口 “坑点” 必须刻在脑子里，踩中任何一个都得熬夜改代码：</p><ol><li><strong>签名必传 3 个参数</strong>：<code>format=json</code>、<code>sign_method=sha256</code>、<code>UTC ISO时间戳</code>，漏一个就报 401，和国内平台的签名逻辑完全不同。</li><li><strong>商品 ID 是 13 位</strong>：别和淘宝 12 位、京东 10 位混了，传错 ID 返回 “商品不存在”，错误码和 “商品下架” 一样，新手难区分。</li><li><strong>价格要算 “三层折扣”</strong> ：原价→基础折扣价→数量折扣，还得按<code>currency_rates</code>换算多币种，直接用固定汇率或漏算数量折扣，利润会差 30%。</li><li><strong>库存分 “三仓”</strong> ：国内仓、海外仓、预售仓，只看<code>total_stock</code>会导致海外买家下单国内仓，时效延迟被投诉。</li><li><strong>包邮是 “部分地区”</strong> ：<code>is_free_shipping=True</code>不代表全地区包邮，必须查<code>free_shipping_countries</code>，否则中东、南美买家的运费会让你亏哭。</li></ol><h2>最后：给跨境开发者的 3 句真心话</h2><ol><li><strong>多语言别硬转</strong>：速卖通的<code>title_en</code>/<code>title_ru</code>是卖家手动填写的，比机器翻译准确 10 倍，别用翻译 API 转中文标题，会出现 “手机壳” 译成 “phone cover” 却和卖家填写的 “mobile case” 不符的问题。</li><li><strong>物流成本要加缓冲</strong>：速卖通的运费模板会随燃油费调整，解析时建议加 10% 缓冲（比如算出来 100 刀，实际按 110 刀预估），避免运费超支。</li><li><strong>大促前 3 天别调试</strong>：速卖通大促（双 11、黑五）前接口会限流收紧，免费版可能降到 5 次 / 分钟，提前一周完成调试，别临时改代码被封。</li></ol>]]></description></item><item>    <title><![CDATA[如何通过智能供应链管理提升制造效率？ 月]]></title>    <link>https://segmentfault.com/a/1190000047446950</link>    <guid>https://segmentfault.com/a/1190000047446950</guid>    <pubDate>2025-12-03 18:04:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>欢迎来到智能供应链管理的时代，这是一个由技术驱动的数字化、自动化、智能化战略转折点，它不仅改变了传统制造业的规划设计范式，也在全球供应链的构建上建立起全新范本。回想一下，传统的供应链管理长期存在三大深层次问题：首先，由于系统智能化程度不高，导致供应链结构过于脆弱，抗外部打击能力下降；其次，在复杂系统协作过程中，由于工业企业应用庞杂，难以突破不同系统间的数据壁垒；第三，面对全球绿色化转型的强力倒逼，碳计量方式仍停留在落后的人工记录阶段，难以合规。<br/>但是，正如广域铭岛所展示的那样，智能供应链管理的解决方案正在逐步推进这些挑战的破局。他们的典型路径是：融合20多种工业协议，构建毫秒级数据物流体系，在生产调度、能耗、质量等关键领域引入AI，实现场景下的多智能体动态协同。通过这些方法，将其中一个汽车制造工厂的全局库存周转率提升了30%以上。<br/>更值得关注的是，通过跨行业实践，我们看到了智能供应链管理的惊人效果：某铝电联合企业成功降低吨铝耗电200千瓦时，年节省电费7000万元；某芯片制造商有效切断了产能制约瓶颈，将市场响应周期压缩为原来的四分之一。广域铭岛打造出的GOS系统，通过实时数据收集、动态控制和智能预测，将企业运营效率推到了一个全新的高度。<br/>其中的精妙之处在于他们团队完成的架构设计：从数据感知层、智能化规划层和末端执行层构成了完整的闭环系统。尤其是选用的知识图谱技术，将企业运营中各种零散数据实现了有机融合，让质量追溯从过去的数周缩短至实时响应，这无疑是对智能供应链管理一流实践的最好注脚。<br/>广域铭岛团队还提出了一个值得借鉴的核心理念：智能供应链管理不是简单地改变某一个局部环节的实施策略，而应该是整个产业链协同运作方式的根本变革。这个理念体现在他们的数字孪生技术应用中，也体现在他们为客户提供的整套解决方案中。<br/>展望未来，智能供应链管理将随着更多AI技术的加入，生成更加灵性的协作模式。从预测性维护到分布式制造，从碳强度管控到全流程闭环优化，广域铭岛正为更多制造企业提供实时智能决策的生命动力。如果说制造业的智能化是一条漫长的进化之路，那么智能供应链管理就是其中一个划时代的关键节点。</p>]]></description></item><item>    <title><![CDATA[深度拆解：SAE 刚性交付的底层逻辑，从]]></title>    <link>https://segmentfault.com/a/1190000047446975</link>    <guid>https://segmentfault.com/a/1190000047446975</guid>    <pubDate>2025-12-03 18:03:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：张凤婷（娜米）</p><p>资源的刚性交付，不是云上天生就具备的能力。当选择自建或自管理一个 Kubernetes/ECS 资源池时，就必须直面一个残酷的现实：所依赖的底层 IaaS 资源本身就是非刚性的。</p><p>阿里云上 ECS 有多代实例规格（如 g6、c7i、r8y 等），基于 Intel、AMD 及自研倚天 ARM 芯片，但这并不保证在任何时刻、任何地域、任何可用区，所需要的那款机型就一定有库存。这种底层资源的“不确定性”，会像幽灵一样渗透到自建的上层系统中。</p><p>刚性交付的本质，是将“不确定性”从系统中排除的关键机制。它通过可控的资源成本，换取了业务的稳定性、高性能和可预测性。 对于任何严肃的线上业务而言，这种确定性并非锦上添花，而是维系其商业信誉和核心价值的生命线。</p><p>以下几个案例，阐述非刚性交付”带来的典型困境。</p><p><strong>案例一：游戏行业 —— 新品发布日的“容量灾难”</strong></p><ul><li>行业：在线游戏、元宇宙</li><li><p>故障：</p><ol><li>场景：一家游戏公司万众期待的新游戏正式公测。运营团队基于压测，制定了雄心勃勃的扩容计划，需要在开服瞬间将游戏服务器（通常需要高性能计算或 GPU 优化的特定 ECS 机型）的规模扩大 10 倍。他们管理着一个基于 K8s 的自建集群。</li><li>触发：开服铃声敲响，CI/CD 流水线触发了大规模的横向扩容。然而，K8s 的节点自动伸缩器 Cluster Autoscaler 在向阿里云申请创建新的 ECS 节点时，API 返回了“Insufficient stock”库存不足的错误。他们所依赖的特定高性能机型，在该可用区已无库存。</li><li>现象：应用的 Pod 因为没有足够的节点资源而大量处于 <code>Pending </code>状态，无法被调度。新玩家的登录请求雪片般涌入，但服务器容量远未达到预期。</li></ol></li><li><p>业务影响：</p><ul><li>上线即失败：大量玩家无法登录，游戏入口处大排长龙，社交媒体和游戏社区瞬间被负面评价淹没，精心策划的发布会变成了公关灾难。</li><li>真金白银的损失：高额的市场推广费用付诸东流，首日充值流水远低于预期。</li><li>玩家永久流失：糟糕的首日体验会导致大量核心玩家永久流失至竞品。</li></ul></li></ul><p><strong>案例二：电商行业 —— 大促活动中的“性能悬崖”</strong></p><ul><li>行业：电商与在线零售</li><li><p>故障：</p><ol><li>场景：一家电商平台为了应对大促，提前“预留”了大量 ECS 节点。为了“提高资源利用率”，他们在核心的交易应用 Pod 所在的节点上，混部了一些非核心的数据分析和日志处理 Pod，并配置了非刚性的 CPU 交付。</li><li>触发：大促零点开启，交易量飙升，交易应用需要全部申请的 CPU。同时，数据分析任务也开始高强度运行，抢占 CPU 资源。</li><li>现象：交易应用的实际可用 CPU 被严重挤压，响应时间急剧恶化，大量请求超时。</li></ol></li><li><p>业务影响：</p><ul><li>订单大量流失：支付和下单环节的堵塞，直接导致 GMV 损失。</li><li>品牌信誉受损：用户在关键时刻掉链子，严重损害品牌可靠性。</li></ul></li></ul><p><strong>案例三：金融科技行业 —— 交易时段的“随机掉线”</strong></p><ul><li>行业：金融科技 (FinTech)，尤其是证券交易</li><li><p>故障：</p><ol><li>场景：一个核心的行情推送 Java 服务，以内存非刚性交付的方式运行在一个自管理的 K8s 集群上。</li><li>触发：交易时段，订阅量激增，服务实际内存使用远超其申请值。此时节点内存压力增大，触发 OOM Killer。</li><li>现象：行情服务 Pod 被系统判定为“劣质进程”而随机杀死，导致客户端行情刷新中断。</li></ol></li><li><p>业务影响：</p><ul><li>交易决策失误：用户因行情中断而做出错误决策或错失交易时机，造成直接经济损失。</li><li>合规与监管风险：核心系统频繁中断，可能触犯金融行业的高可用性监管要求。</li></ul></li></ul><p><strong>案例四：企业软件行业 —— 核心ERP系统的“性能抽奖”</strong></p><ul><li>行业：企业软件 (ERP, CRM)，尤其是大型单体应用</li><li><p>故障：</p><ol><li>场景：一家企业将其庞大的、无法轻易水平扩展的单体 ERP 系统容器化后，部署在一个资源非刚性交付的自建集群上，以期“节约成本”。</li><li>触发：在月末财务结算等高峰期，ERP 系统需要大量 CPU 和内存。但它必须和节点上其他应用“共享”资源。</li><li>现象：ERP 系统的性能变得极不稳定，时快时慢，如同“抽奖”。有时一个报表生成需要 2 分钟，有时需要 20 分钟。</li></ol></li><li><p>业务影响：</p><ul><li>工作效率低下：员工的核心工作流程被频繁打断，财务、供应链等部门的月末结算工作无法按时完成。</li><li>决策延迟：管理者无法及时获取准确的业务报表，影响了商业决策的时效性。</li></ul></li></ul><h2>资源刚性交付困境</h2><h3>资源供给的不确定性</h3><p>困境本质：“承诺的资源” ≠ “可即时获取的资源”。</p><ul><li>库存波动：热门规格 ECS，在大促或行业高峰期容易出现“秒光”，导致扩容失败。</li><li>区域/可用区差异：某些 AZ 因物理机房容量限制，无法提供特定资源类型，跨 AZ 调度又需额外网络与配置成本。</li><li>代际断层：旧代实例停售或库存枯竭，但应用尚未适配新架构，造成刚性承诺无法兑现。</li></ul><h3>性能隔离难以真正实现</h3><p>困境本质：“逻辑隔离”不等于“物理隔离”，刚性性能难以 100% 保障。</p><ul><li>虚拟化开销与干扰：即使使用 Cgroups、CPU 绑核等技术，共享 NUMA 节点、内存带宽、磁盘 I/O 队列仍可能被“嘈杂邻居”抢占。</li><li>突发流量冲击：同节点上其他租户突发高负载（如备份、扫描），导致本应“独占”的实例出现延迟毛刺。</li><li>存储性能抖动：存储在多租户争抢下 IOPS 和吞吐不稳定，影响核心业务等关键应用。</li></ul><h3>弹性与刚性的内在矛盾</h3><p>困境本质：刚性要求确定性，弹性依赖不确定性，二者天然张力。</p><ul><li>预占 vs 按需：为保障刚性需提前预留资源，但业务负载波动大时造成浪费；若完全按需，则无法应对突发高峰。</li><li>冷启动延迟：首次启动需拉镜像、初始化，往往无法满足业务的刚性响应要求。</li></ul><h3>异构资源管理复杂度高</h3><p>困境本质：“资源刚性”需端到端栈协同，任一环节短板即导致整体失效。</p><ul><li>专用硬件：驱动版本、CUDA 兼容性、拓扑感知调度、故障恢复机制各异，难以标准化交付。</li><li>混合架构支持难：x86 与 ARM（如倚天 710）指令集不同，应用需重新编译测试，刚性交付需维护多套镜像与部署流程。</li><li>网络与存储耦合：高性能计算需 RDMA、NVMe over Fabric 等底层能力，但这些能力在虚拟化层常被削弱或不可用。</li></ul><h3>传统架构与云原生理念割裂</h3><p>困境本质：刚性交付不仅是技术问题，更是组织与认知转型问题。</p><ul><li>缺乏弹性设计：应用未做无状态改造，无法横向扩展，只能纵向升级（Scale-Up），而大规格实例更稀缺、更昂贵。</li><li>运维惯性阻力：企业习惯“买服务器、装系统、长期运行”，对“按需申请、用完即弃”的刚性交付模式接受度低。</li></ul><h3>成本模型与刚性目标冲突</h3><p>困境本质：财务约束常迫使技术理想向现实低头。</p><ul><li>刚性 = 高成本：独占物理机、专用集群、多 AZ 冗余等方案显著推高 TCO。</li><li>企业被迫妥协：为控制预算，用户常选择共享资源池+监控告警“事后补救”，而非事前刚性保障。</li><li>计费模式滞后：传统按小时计费无法匹配秒级弹性需求，导致“为不用的资源付费”或“关键时刻无资源可用”。</li></ul><h2>SAE 在刚性交付上做的工作</h2><p>作为阿里云面向应用层的全托管 Serverless PaaS 平台，针对资源刚性交付的系统性困境，从<strong>资源供给、性能隔离、弹性模型、异构调度、成本结构、容灾能力、可观测性与架构演进</strong>等多个维度进行了设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446977" alt="image" title="image"/></p><h3>1. 破解“资源供给不确定性” → 构建无限弹性资源池</h3><ul><li>多源异构资源整合：  <br/>SAE 背后打通神龙裸金属服务器、弹性容器实例（ECI）支持各代  x86/ARM 等海量资源，形成统一调度池。</li><li>智能跨机型调度：  <br/>当用户指定规格库存不足时，调度器自动选择性能相当、兼容性一致的替代资源（如 g7 缺货 → 自动调度 g8i），全程对用户透明。</li><li>结果：  <br/>交付的是“计算能力”，而非“特定机型”，彻底规避因库存波动导致的扩容失败。</li></ul><h3>2. 解决“性能隔离难” → 天然沙箱化 + 独占资源</h3><ul><li>默认运行在 ECI 沙箱中：  <br/>每个应用实例运行在轻量级安全容器，实现内核级隔离，杜绝“嘈杂邻居”干扰。</li><li>资源 100% 独占：  <br/>用户申请的 CPU、内存、网络带宽均由 runD 底层安全沙箱保障，无超分、无争抢，性能稳定可预期。</li><li>结果：  <br/>刚性性能不再是“尽力而为”，而是确定性交付，尤其适合金融交易、实时推荐等敏感场景。</li></ul><h3>3. 调和“弹性与刚性矛盾” → 按实际用量计费 + 缩容至零</h3><ul><li>闲置不计费：  <br/>应用缩容到 0 实例时，CPU/内存资源完全释放，不产生费用（仅保留配置元数据）。</li><li>秒级冷启动优化：  <br/>结合镜像预热、快照加速、本地缓存等技术，大幅缩短首次启动延迟，逼近“即时刚性响应”。</li><li>结果：  <br/>用户无需为“以防万一”长期预留资源，刚性保障与极致成本兼得，替代高风险混部策略。</li></ul><h3>4. 简化“异构资源管理” → 屏蔽底层复杂性</h3><ul><li>ARM/x86 无缝兼容：  <br/>如支持海光国产芯片，用户只需提供兼容镜像，SAE 自动完成调度与运行时适配。</li><li>结果：  <br/>开发者只需关注“我要多少算力”，无需关心“卡在哪台机器上、驱动是否匹配”。</li></ul><h3>5. 重构“成本模型” → 从“买资源”到“买能力”</h3><ul><li>按实际 CPU/内存使用量秒级计费：  <br/>不再按整机小时付费，避免资源闲置浪费。</li><li>免运维成本：  <br/>无需管理节点、打补丁、编写扩缩容脚本，人力成本大幅降低。</li><li>结果：  <br/>刚性交付不再昂贵，中小企业也能享受企业级可靠性。</li></ul><h3>6. 强化“容灾与高可用” → 多可用区刚性容灾</h3><ul><li>一键开启多 AZ 部署：  <br/>SAE 自动将应用实例分散到多个可用区，跨机房冗余。</li><li>AZ 故障自动恢复：  <br/>若某 AZ 整体不可用，SAE 在其他 AZ 刚性拉起新实例，RTO 控制在分钟级。</li><li>结果：  <br/>刚性交付从“单点稳定”升级为“应用级连续性保障”。</li></ul><h3>7. 提升“可观测性与可信度” → 内置全链路监控</h3><ul><li>集成 ARMS + SLS + Prometheus：  <br/>提供应用性能监控（APM）、日志、指标、链路追踪一体化视图。</li><li>资源使用透明化：  <br/>用户可清晰看到 CPU 使用率、内存水位、网络吞吐是否达到承诺值。</li><li>结果：  <br/>刚性 SLA 可验证、可审计，告别“黑盒交付”。</li></ul><h3>8. 支持“传统应用平滑演进” → 兼顾稳定与未来</h3><ul><li>支持 WAR/JAR/镜像直接部署：  <br/>ERP、OA 等单体应用无需改造即可运行在 SAE 上，享受刚性资源保障。</li><li>内置诊断能力：  <br/>通过性能剖析定位瓶颈（如数据库慢查询、线程阻塞），为后续微服务拆分提供数据依据。</li><li>结果：  <br/>SAE 不仅是“运行平台”，更是企业云原生转型的跳板。</li></ul><h2>了解 Serverless 应用引擎 SAE</h2><p>阿里云 Serverless 应用引擎 SAE 是面向 AI 时代的一站式容器化应用托管平台，以“托底传统应用、加速 AI 创新”为核心理念。它简化运维、保障稳定、闲置特性降低 75% 成本，并通过 AI 智能助手提升运维效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446978" alt="image" title="image" loading="lazy"/></p><p>面向 AI，SAE 集成 Dify 等主流框架，支持一键部署与弹性伸缩，在 Dify 场景中实现性能<strong>提升 50 倍、成本优化 30% 以上</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446979" alt="image" title="image" loading="lazy"/></p><h3>产品优势</h3><p>凭借八年技术沉淀，SAE 入选 2025 年 Gartner 云原生魔力象限全球领导者，亚洲第一，助力企业零节点管理、专注业务创新。SAE 既是传统应用现代化的“托举平台”，也是 AI 应用规模化落地的“加速引擎”。</p><p><strong>1. 传统应用运维的“简、稳、省”优化之道</strong></p><ul><li>简：零运维心智，专注业务创新</li><li>稳：企业级高可用，内置全方位保障</li><li>省：极致弹性，将成本降至可度量</li></ul><p><strong>2. 加速 AI 创新：从快速探索到高效落地</strong></p><ul><li>快探索：内置 Dify、RAGFlow、OpenManus 等热门 AI 应用模板，开箱即用，分钟级启动 POC；</li><li>稳落地：提供生产级 AI 运行时，性能优化（如 Dify 性能提升 50 倍）、无感升级、多版本管理，确保企业级可靠交付；</li><li>易集成：深度打通网关、ARMS、计量、审计等能力，助力传统应用智能化升级。</li></ul><h2>适合谁？</h2><p>✅ 创业团队：没有专职运维，需要快速上线  <br/>✅ 中小企业：想降本增效，拥抱云原生  <br/>✅ 大型企业：需要企业级稳定性和合规性  <br/>✅ 出海企业：需要中国区 + 全球部署  <br/>✅ AI 创新团队：想快速落地 AI 应用</p><h3>了解更多</h3><p>产品详情页地址（点击阅读原文即可查看）：<a href="https://link.segmentfault.com/?enc=afj%2BR0CE%2FyioGlwm8FRgOA%3D%3D.lUXk%2BeHpZJhnxSVdYZNj8D06QFBYCYeoq%2Fx9P4olwQB5gltUMBX87zY%2F0zXhi2x1" rel="nofollow" target="_blank">https://www.aliyun.com/product/sae</a></p><p>欢迎使用钉钉搜索群号： 23156632</p><p>加入 SAE 客户服务群 👇</p>]]></description></item><item>    <title><![CDATA[美股 (US) 与 墨西哥 (Mexic]]></title>    <link>https://segmentfault.com/a/1190000047446988</link>    <guid>https://segmentfault.com/a/1190000047446988</guid>    <pubDate>2025-12-03 18:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. 接入概述 (General)</h2><p>本接口用于获取美国（NYSE, NASDAQ, AMEX）及墨西哥（BMV, BIVA）证券市场的实时行情、历史 K 线及指数数据。</p><ul><li><strong>API Base URL</strong>: <code>https://api.stocktv.top</code></li><li><strong>WebSocket URL</strong>: <code>wss://ws-api.stocktv.top/connect</code></li><li><strong>鉴权方式</strong>: 所有请求均需携带 URL 参数 <code>key=您的API密钥</code></li></ul><h3>1.1 关键市场 ID (Country ID)</h3><p>在调用相关接口时，请务必区分以下 <code>countryId</code>：</p><table><thead><tr><th align="left">市场名称</th><th align="left">Country ID</th><th align="left">交易所示例</th></tr></thead><tbody><tr><td align="left"><strong>美国 (USA)</strong></td><td align="left"><strong>5</strong></td><td align="left">NYSE (1), NASDAQ (2), AMEX</td></tr><tr><td align="left"><strong>墨西哥 (Mexico)</strong></td><td align="left"><strong>7</strong></td><td align="left">Mexico (53), BIVA (144)</td></tr></tbody></table><hr/><h2>2. 核心数据接口</h2><h3>2.1 获取股票列表 (Stock List)</h3><p>用于查询指定市场的股票清单，获取股票的名称、代码 (Symbol) 和 <strong>系统 ID (PID)</strong>。</p><blockquote><strong>注意</strong>：<code>id</code> (PID) 是后续查询 K 线和订阅 WebSocket 的唯一标识符。</blockquote><ul><li><strong>接口地址</strong>: <code>/stock/stocks</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>:</li></ul><table><thead><tr><th align="left">参数名</th><th align="left">类型</th><th align="left">必填</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><code>key</code></td><td align="left">String</td><td align="left">是</td><td align="left">您的 API Key</td></tr><tr><td align="left"><code>countryId</code></td><td align="left">Int</td><td align="left">是</td><td align="left"><strong>5</strong> (美股) 或 <strong>7</strong> (墨西哥)</td></tr><tr><td align="left"><code>pageSize</code></td><td align="left">Int</td><td align="left">否</td><td align="left">每页数量 (默认 10)</td></tr><tr><td align="left"><code>page</code></td><td align="left">Int</td><td align="left">否</td><td align="left">页码 (默认 1)</td></tr></tbody></table><ul><li><p><strong>请求示例 (获取美股列表)</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/stocks?countryId=5&amp;pageSize=20&amp;page=1&amp;key=YOUR_KEY</code></pre></li><li><p><strong>响应示例</strong>:</p><pre><code class="json">{
  "code": 200,
  "data": {
    "records": [
      {
        "id": 8888,          // [关键] PID，用于K线接口
        "name": "Apple Inc", // 股票名称
        "symbol": "AAPL",    // 股票代码
        "exchangeId": 2,     // 交易所ID (2=NASDAQ)
        "last": 180.5,       // 最新价
        "chgPct": 1.25,      // 涨跌幅%
        "countryNameTranslated": "United States"
      }
    ]
  }
}</code></pre></li></ul><hr/><h3>2.2 获取 K 线数据 (Candlestick Data)</h3><p>获取指定股票的历史行情数据，支持多种时间周期。</p><ul><li><strong>接口地址</strong>: <code>/stock/kline</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>:</li></ul><table><thead><tr><th align="left">参数名</th><th align="left">类型</th><th align="left">必填</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><code>key</code></td><td align="left">String</td><td align="left">是</td><td align="left">您的 API Key</td></tr><tr><td align="left"><code>pid</code></td><td align="left">Int</td><td align="left">是</td><td align="left">股票系统 ID (通过 2.1 接口获取)</td></tr><tr><td align="left"><code>interval</code></td><td align="left">String</td><td align="left">是</td><td align="left">K线周期 (ISO 8601格式)</td></tr></tbody></table><ul><li><p><strong>周期 (Interval) 说明</strong>:</p><ul><li><code>PT1M</code> (1分钟), <code>PT5M</code> (5分钟), <code>PT15M</code> (15分钟), <code>PT30M</code> (30分钟), <code>PT1H</code> (1小时)</li><li><code>P1D</code> (日线), <code>P1W</code> (周线), <code>P1M</code> (月线)</li></ul></li><li><p><strong>请求示例 (获取墨西哥某股票日线)</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/kline?pid=12345&amp;interval=P1D&amp;key=YOUR_KEY</code></pre></li><li><p><strong>响应示例</strong>:</p><pre><code class="json">{
  "code": 200,
  "data": [
    {
      "time": 1719818400000, // 时间戳 (毫秒)
      "open": 150.0,
      "high": 155.0,
      "low": 149.0,
      "close": 153.0,
      "volume": 200000
    }
  ]
}</code></pre></li></ul><hr/><h3>2.3 获取大盘指数 (Indices)</h3><p>获取美股（如纳斯达克、标普500）或墨西哥（如 S\&amp;P/BMV IPC）的指数行情。</p><ul><li><strong>接口地址</strong>: <code>/stock/indices</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>: <code>countryId</code> (5=美国, 7=墨西哥)</li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/indices?countryId=7&amp;key=YOUR_KEY</code></pre></li></ul><hr/><h2>3. WebSocket 实时推送</h2><p>通过 WebSocket 长连接接收实时报价更新。</p><ul><li><strong>连接地址</strong>: <code>wss://ws-api.stocktv.top/connect?key=YOUR_KEY</code></li><li><strong>心跳机制</strong>: 连接建立后，建议定期发送心跳包以保持连接。</li><li><p><strong>推送数据结构</strong>:</p><pre><code class="json">{
    "pid": "8888",         // 对应 Rest API 中的 id
    "last_numeric": 181.2, // 最新价
    "pcp": "0.39",         // 涨跌幅%
    "timestamp": "1717728251",
    "bid": "181.1",        // 买价
    "ask": "181.3",        // 卖价
    "type": 1              // 1=股票, 2=指数
}</code></pre></li></ul><hr/><h2>4. 接入代码示例 (JavaScript)</h2><p>以下代码展示了如何根据 <code>countryId</code> 封装获取美股和墨西哥股票的逻辑。</p><pre><code class="javascript">const API_KEY = 'YOUR_API_KEY';
const BASE_URL = 'https://api.stocktv.top';

// 配置 ID
const MARKETS = {
    USA: 5,
    MEXICO: 7
};

/**
 * 获取指定市场的股票列表
 * @param {number} countryId - 5 for USA, 7 for Mexico
 */
async function getMarketStocks(countryId) {
    const url = `${BASE_URL}/stock/stocks?countryId=${countryId}&amp;pageSize=10&amp;page=1&amp;key=${API_KEY}`;
    try {
        const response = await fetch(url);
        const result = await response.json();
        
        if (result.code === 200) {
            console.log(`市场 (ID:${countryId}) 股票列表:`, result.data.records);
            // 示例：获取第一个股票的 PID 用于查 K 线
            if(result.data.records.length &gt; 0) {
                const firstStock = result.data.records[0];
                console.log(`示例股票: ${firstStock.name}, PID: ${firstStock.id}`);
            }
        }
    } catch (error) {
        console.error('API 请求失败:', error);
    }
}

// 1. 获取美股数据
getMarketStocks(MARKETS.USA);

// 2. 获取墨西哥股票数据
getMarketStocks(MARKETS.MEXICO);</code></pre>]]></description></item><item>    <title><![CDATA[如何实现智能研发协同以提升制造业效率？ ]]></title>    <link>https://segmentfault.com/a/1190000047446994</link>    <guid>https://segmentfault.com/a/1190000047446994</guid>    <pubDate>2025-12-03 18:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着工业4.0时代的到来，制造业正经历一场由数字化和人工智能驱动的深刻革命。传统研发模式在数据孤岛、跨部门协作效率低下以及知识复用率低等问题的制约下，难以满足现代企业对敏捷性和创新性的需求。广域铭岛作为这一领域的先行者，凭借其捷做设计研发协同平台和Geega工业互联网平台，提出了一种全新的智能研发协同理念，旨在通过技术的深度融合，提升企业的研发效率与创新能力。<br/>捷做平台的核心在于打破传统研发中的壁垒，实现全流程透明化与数据驱动的协同管理。它不仅支持设计、工艺和生产数据的统一管理，还通过模块化设计、可配置BOM（物料清单）以及变更闭环控制等功能，优化了企业内部的研发协作模式。广域铭岛的解决方案以客户需求为导向，将研发过程与生产需求紧密结合，确保设计即研发、设计即生产的核心原则。<br/>在智能研发协同中，广域铭岛充分利用了现代技术架构的优势。基于微服务的系统设计，使得每个业务模块都具备高度的独立性和灵活性。多租户技术的应用则保证了不同企业在同一平台上实现数据隔离与个性化配置，而高性能数据库的引入，尤其是基于图数据库的BOM管理，极大地提升了数据检索和处理的效率。这些技术的结合，使得捷做在复杂的研发环境中表现出色，成为制造业数字化转型的关键支撑。<br/>此外，广域铭岛还通过其专属的捷做平台，进一步强化了智能研发协同的核心。捷做构建了三级数据架构，涵盖了数据接入、治理及服务。通过对企业生产数据的实时分析与整合，它打破了数据的孤岛效应，实现了研发与生产的深度互联。更为重要的是，捷做设计研发协同平台将工艺规则和设备参数转化为可复用的数字资产，显著提升了知识密集型业务的处理效率。例如，在汽车焊接工艺中，通过对电流、电压和送丝速度等参数的封装，形成了“焊点质量指数”，从而帮助企业在设计验证中做出更精准的判断。<br/>在多个行业中的实践证实了广域铭岛智能研发协同策略的有效性。新能源电池领域的案例中，捷做平台通过数据建模和工艺优化，帮助企业将良品率提升8%，并将设备故障时间减少了65%。而在汽车行业，吉利集团借助该平台实现了每年30多款新车型的并行研发，不仅将零部件通用化率提升至75%，还显著降低了单车研发成本。这些成果不仅仅是数据的提升，更是整个研发范式的重构，体现了广域铭岛在技术驱动与业务融合上的领先地位。<br/>面向未来，广域铭岛持续推进两大技术方向：生成式研发助手和数字孪生研发环境。生成式研发助手基于工业大模型，能够通过自然语言和设计需求，快速生成设计图纸，帮助企业缩短设计周期；而数字孪生研发环境则构建了高保真的虚拟工厂，支持研发人员在仿真环境中实时调试设备参数，将工艺优化周期从周级压缩至小时级。这些创新不仅为制造业的研发提供了更高效的解决方案，还进一步加剧了智能研发协同的影响力。<br/>最终，广域铭岛的智能研发协同模式强调的是一种以数据为核心的开放生态系统。通过推动研发流程与实际业务的深度融合，它帮助制造企业突破传统模式的瓶颈，实现从创意到落地的无缝衔接。在这个过程中，数据管理和知识的多元共用成为关键，彰显了广域铭岛产品的智能化与前瞻性。</p>]]></description></item><item>    <title><![CDATA[如何将音乐从 iTunes 传输到闪存驱]]></title>    <link>https://segmentfault.com/a/1190000047447014</link>    <guid>https://segmentfault.com/a/1190000047447014</guid>    <pubDate>2025-12-03 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>对于喜欢收藏音乐的用户来说，iTunes 是一款经典且实用的音乐管理工具。然而，有时用户可能希望将 iTunes 中的音乐复制到 U 盘，以便在车载音响、电视或其他设备上轻松播放。那么，可以将歌曲从 iTunes 传输到 U 盘吗？当然可以。本文将提供详细的指南，教您如何将音乐从 iTunes 传输到 U 盘，帮助您轻松备份和携带音乐。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447016" alt="图片" title="图片"/></p><h3>第一部分：如何直接将歌曲从 iTunes 复制到 U 盘</h3><p>无论你使用的是Windows 11/10/8 还是Mac ，你都可以轻松地将歌曲直接从 iTunes 音乐库传输到闪存驱动器。</p><p>以下是如何将音乐从 iTunes 传输到 U 盘的方法：</p><p>步骤 1. 打开电脑上的iTunes应用。</p><p>步骤 2. 打开您的音乐库或播放列表。在Windows中，请确保左上角的“音乐”选项卡已选中，然后依次点击“资料库”&gt;“歌曲”查看所有曲目。在 macOS 系统中，点击“音乐”图标，然后前往“播放列表”选项卡。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447017" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>步骤 3：插入 U 盘并打开。连接成功后，打开 U 盘文件夹，并确保桌面上的 iTunes 窗口也已打开。（ iTunes 一直崩溃？）</p><p>步骤 4：在 iTunes 中选择要传输的歌曲，然后将它们拖到 U 盘的文件夹中。在Windows中，按住“Ctrl”或“Shift”键可一次选择多首歌曲，然后将它们拖放到 U 盘中。在 macOS 系统中，使用“Command”键执行相同的操作。</p><p>您还可以从 iTunes 导出整个播放列表或音乐库，然后将其保存到 U 盘。如果您的播放器仅支持 MP3 或其他特定格式，则需要先将文件转换为所需的格式。</p><p>具体操作方法如下：</p><p>步骤 1. 打开 iTunes，然后转到菜单栏并选择“编辑”&gt;“偏好设置”&gt;“通用”。点击“导入设置”，在“导入方式”选项下选择“MP3 编码器”，然后点击两次“确定”以保存设置并返回您的资料库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447018" alt="图片" title="图片" loading="lazy"/></p><p>步骤 2. 将 USB 插入电脑并创建一个新文件夹。</p><p>步骤 3. 接下来，打开 iTunes，选择要传输的播放列表，然后转到“文件”&gt;“资料库”&gt;“导出资料库”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447019" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>步骤 4. 在文件浏览器中，选择 USB 作为保存播放列表文件的目标位置。</p><p>想知道如何将 iPhone 中的歌曲导入 iTunes 吗？本指南将介绍两种有效的方法，并提供分步说明，教您如何将音乐从 iPhone 传输到 iTunes 资料库。</p><p>将 iPhone 音乐传输到 iTunes 资料库的两种方法</p><h3>第二部分：如何将 iTunes 媒体文件夹中的音乐传输到 U 盘</h3><p>除了直接将 iTunes 音乐传输到 U 盘外，您还可以找到计算机上存储 iTunes 歌曲的“iTunes Media”文件夹，然后将其复制到 U 盘。</p><p>以下是如何将 iTunes 媒体文件夹中的音乐传输到 U 盘的方法：</p><p>第一步：打开 iTunes，然后点击“编辑”&gt;“偏好设置”。此时会弹出一个新窗口。</p><p>步骤 2. 勾选“保持 iTunes 媒体文件夹有序”和“添加到资料库时将文件复制到 iTunes 媒体文件夹”，然后单击“确定”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447020" alt="图片" title="图片" loading="lazy"/></p><p>注意：如果您想更改 iTunes 媒体文件夹的位置，请点击“更改…”并选择新位置。</p><p>步骤 3. 转到“文件”&gt;“库”&gt;“整理库...”，选中“合并文件”，然后单击“确定”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447021" alt="图片" title="图片" loading="lazy"/></p><p>第四步：完成以上步骤后，所有媒体文件都将保存在 iTunes 媒体文件夹中。然后，您可以打开该文件夹，将播放列表拖放到您的 U 盘中。</p><p>额外内容：如何将音乐从 iPhone 传输到 U 盘</p><p>如果你的音乐不在 iTunes 里，而是存储在 iPhone 上怎么办？在这种情况下，如果你想直接将其导出到 U 盘，可以使用第三方工具。Coolmuster iOS Coolmuster是一款专业的iOS设备管理软件，它可以将 iPhone 中的音乐、照片、联系人、信息和其他数据导出到电脑或 U 盘。它易于使用，适合所有用户。</p><p>iOS助手亮点：</p><pre><code>轻松将 iPhone 中的音乐传输到闪存盘/USB 驱动器。
支持联系人、短信、照片、视频、日历、应用等。
一键轻松备份和恢复您的 iPhone /iPad。
预览并选择iOS文件后，即可轻松传输文件。
在您的电脑上全面管理 iTunes 备份文件和iOS数据。
直接通过 PC 或Mac编辑、添加或删除iOS设备上的数据。
支持最新的iOS 26版本和iPhone 17系列。

</code></pre><p>以下是如何使用iOS助理将音乐从 iPhone 传输到 U 盘的方法：</p><p>01将此工具下载并安装到您的电脑上。使用 USB 数据线将您的 iPhone 连接到电脑，然后插入您的 U 盘。</p><p>02检测到您的 iPhone 后，请在手机上点击“信任”，然后在程序中点击“继续”以建立连接。之后，您将看到如下所示的主界面。可以看到，所有不同的文件夹都已整理在主屏幕上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447022" alt="图片" title="图片" loading="lazy"/></p><p>03点击“音乐”部分查看您的歌曲列表。您可以根据需要选择单首歌曲或选择整个音乐文件夹进行传输。然后，点击“导出”按钮，选择您的U盘作为目标位置，并将所选音乐文件保存到U盘中。</p><h3>结尾</h3><p>无论您是想将音乐传输到车载播放器播放、备份音乐收藏，还是与朋友分享，以上介绍的将 iTunes 音乐传输到 U 盘的方法都简单实用。您只需选择最适合自己的方法即可轻松完成传输。</p><p>但是，如果你的音乐文件存储在 iPhone 上， Coolmuster iOS Assistant就是理想之选。它能让你快速导出音乐，还能在一个便捷的界面中管理手机上的其他数据。<br/>​</p>]]></description></item><item>    <title><![CDATA[告别数据孤岛与运维盲区：一款数字孪生平台]]></title>    <link>https://segmentfault.com/a/1190000047446682</link>    <guid>https://segmentfault.com/a/1190000047446682</guid>    <pubDate>2025-12-03 17:06:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当园区的安防、能耗、设备、环境数据散落在十几个不同的系统里，当一次应急调度需要打电话、查图纸、跑现场才能拼凑出全局信息，当领导视察时只能看到静态的PPT汇报而非动态的运营实况——这或许是许多园区运营管理者正在经历的日常。<br/>在数字化的浪潮下，园区运营正从传统的“人防+技防”向“数据驱动、可视可控”的智慧运营演进。然而，理想与现实之间，往往横亘着技术实现的鸿沟：多源数据如何融合？三维场景如何快速构建？业务规则如何灵活配置？对于广大应用开发者而言，这既是巨大的市场机遇，也是棘手的技术挑战。<br/>今天，我们想抛开浮夸的概念，从一个资深开发者的视角，深入探讨一款名为“孪易 数字孪生 IOC”的工具平台，看看它如何通过一系列具体而微的功能设计，为园区智慧运营提供一套“开箱即用、深度可配”的解决方案，并在此过程中，为开发者打开一扇高效交付的大门。</p><h2>一、 核心痛点：园区运营的“数据之困”与“场景之渴”</h2><p>在深入产品之前，我们必须先理解园区运营的核心诉求。一个现代化的产业园区、智慧园区或大型综合体，其运营复杂度极高：<br/>系统林立，数据割裂：楼宇自控、视频监控、门禁停车、能耗管理、设备运维、环境监测……各系统独立建设，数据格式与协议各异，形成一个个“数据孤岛”。管理者无法获得统一、实时的全局态势。<br/>空间复杂，管理低效：地下管网、楼层结构、设备位置等信息依赖二维图纸或人工记忆。故障定位慢、资产盘点难、空间利用率分析缺乏直观依据。<br/>业务多样，响应滞后：安防告警、设备预警、环境超标、能耗突增等事件分散在不同值班岗位。缺乏跨业务的联动分析与统一指挥看板，应急响应效率低下。<br/>价值呈现，手段单一：向领导、访客展示运营成果时，往往停留在图表和报告层面，缺乏一个直观、动态、可交互的“数字孪生体”来生动呈现园区科技实力与管理水平。<br/>对于承接此类项目的开发者或集成商而言，挑战在于：如何在不投入巨额成本和漫长时间进行底层开发的情况下，构建一个能打通上述环节、满足客户核心价值的数字孪生应用？<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmP6B" alt="" title=""/></p><h2>二、 破局之道：从“数据接入”到“业务呈现”的全链路配置化能力</h2><p>“孪易 数字孪生 IOC”提供了一套完整的工具链，其设计哲学可以概括为：“汇聚数据于一体，呈现业务于一屏，赋能管理于一键”。下面，我们结合园区场景，拆解其关键功能如何直击痛点。</p><h3>1. 数据融合：构建全域感知的“数字底板”</h3><p>数字孪生的生命在于数据。该平台的核心优势之一是强大的多源异构数据集成能力。它并非一个封闭的3D渲染引擎，而是一个强大的数据中枢。<br/><strong>对于物联网数据</strong>：它支持通过标准协议（如MQTT）或直接对接主流云物联网平台（如华为云IoTDA、阿里云物联网平台），将成千上万的传感器数据（温湿度、能耗、设备状态、车位状态等）实时接入。<br/><strong>对于业务系统数据</strong>：支持连接MySQL、PostgreSQL等关系型数据库，也能适配国产数据库，轻松获取资产信息、工单记录、人流统计等业务数据。<br/>对于视频数据：支持接入RTSP、FLV等格式的实时视频流，并可将视频画面与三维场景中的摄像头模型关联，实现“点击摄像头，即看实时画面”的融合监控。<br/><strong>价值点</strong>：开发者无需为每种数据源编写复杂的解析和对接代码，只需在后台进行配置，即可将分散的数据汇聚成园区统一的“数字底板”，为上层应用提供“燃料”。这极大地降低了数据整合阶段的技术门槛与时间成本。</p><h3>2. 场景构建：从“一张白纸”到“鲜活园区”的快速复刻</h3><p>有了数据，还需要承载数据的场景。平台提供了灵活的场景构建能力。<br/><strong>多格式支持</strong>：支持导入OBJ、FBX、GLTF等通用3D模型，以及BIM（建筑信息模型）和GIS（地理信息系统）数据。这意味着开发者可以利用园区已有的设计资料（如BIM模型）快速构建高保真的三维场景，而不是一切从零建模。<br/><strong>行业化预设</strong>：更值得一提的是其预置的行业插件库。针对“智慧园区”，平台可能已经内置了标准化的办公楼、厂房、停车场、路灯、配电箱等三维模型库，以及常见的园区业务数据模型。开发者可以像搭积木一样，快速组合出园区的三维骨架。<br/><strong>价值点</strong>：这解决了“从0到1”构建场景的漫长过程。开发者可以聚焦于业务逻辑和特色功能开发，基础的环境构建工作得以大幅提效，项目交付周期显著缩短。</p><h3>3. 业务配置：让“监、管、控、析”变得可定义</h3><p>这是平台最具魅力的部分——强大的后台配置能力，让非核心开发人员也能参与应用搭建。<br/><strong>对象管理</strong>：在庞大的三维场景中，如何快速找到一台故障的空调机组？平台提供对象管理面板和全局搜索功能，支持按分类、楼层、系统进行筛选，或直接搜索名称，并一键定位到三维场景中的具体位置，实现“所想即所见，所见即所得”。<br/><strong>智能告警</strong>：告警不再是简单的越限提示。开发者或运维人员可以在后台自定义复杂的告警规则。例如，可以设定“当会议室温度高于28℃且室内有人时”才触发告警，避免空房间的误报。告警触发后，会在三维场景中高亮显示，并支持一键定位、查看详情、关联视频、下发处置工单，形成闭环。<br/><strong>主题分析</strong>：平台支持创建业务主题看板。例如，可以创建一个“能效管理”主题，将园区总用电趋势图、各栋楼分项能耗排名、重点耗能设备列表、以及三维场景中的能耗热力图，全部聚合在一个页面。这相当于为不同业务部门（如工程部、安防部）定制了他们的专属“作战指挥室”。<br/><strong>交互控制</strong>：对于可控设备（如智能照明、门禁、空调），平台支持在三维场景中直接发送控制指令。点击三维场景中的一盏灯，弹出开关面板，操作后状态实时反馈回三维模型。这实现了真正的“三维组态”，让管理操作无比直观。<br/><strong>价值点</strong>：将大量需要编码实现的业务逻辑，转化为后台的可视化配置项。项目交付后，园区运营方可以根据业务变化自行调整规则和看板，赋予了平台长期生命力，也减少了开发者的后期维护负担。</p><h3>4. 模式创新：“免费试用”与“平滑演进”的友好路径</h3><p>对于开发者和最终用户，平台的商业模式也体现了灵活性。<br/><strong>低成本启动</strong>：提供免费公有云标准版，允许用户以极低的门槛进行概念验证（PoC）。开发者可以先用它搭建一个简化版Demo向客户演示，验证技术路线的可行性。<br/><strong>灵活部署</strong>：随着项目深入，可以平滑迁移至功能更强大的专业版，或根据客户对数据安全的要求，采用完全的私有化部署。这种“先尝后买”的模式，降低了双方的初始决策风险和投入成本。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmRb8" alt="" title="" loading="lazy"/></p><h2>三、 场景闭环：看一个智慧园区的一天如何被改变</h2><p>让我们构想一个应用了该平台的智慧园区日常：<br/>清晨，能耗巡检：运营主管打开“能源主题”看板，三维园区地图上覆盖着昨夜的电耗热力图，一眼锁定异常高耗区域。点击该建筑，自动剖切显示楼层，并关联出该楼层空调主机的运行曲线，初步判断是否为设备异常。<br/>上午，安全监控：周界入侵告警触发。指挥中心大屏上，三维园区地图自动定位到告警点，并弹出附近多个摄像头的实时画面。值班员一键调度最近的巡逻机器人前往查看，并在三维地图上实时跟踪机器人轨迹。<br/>下午，设施维修：某企业报修空调故障。客服人员在工单系统录入后，维修工程师在移动端收到任务。他打开App上的园区三维地图，工单位置已被精准标注。他查看该空调的历史运行数据后前往维修，维修后状态同步更新至三维模型。<br/>傍晚，领导视察：无需准备复杂的PPT，运营方直接在指挥中心的大屏上，通过三维数字孪生园区，动态展示人流车流、能耗对比、安防布控、绿色减碳成果，所有数据实时刷新，汇报生动而有力。</p><h2>结语：给开发者的价值主张</h2><p>“孪易 数字孪生 IOC 标准版”本质上是一个 “数字孪生应用生产力工具” 。它不试图取代开发者，而是旨在赋能开发者。<br/>对于从事园区、城市、工业等垂直领域数字化解决方案的开发者而言，它的价值在于：<br/><strong>提升交付效率</strong>：将重心从底层技术开发（如3D引擎、数据中间件）转移到上层业务价值实现，缩短项目周期。<br/><strong>降低技术风险</strong>：基于一个成熟、稳定的平台进行开发，避免了自研技术框架的不确定性和长周期投入。<br/><strong>增强客户粘性</strong>：交付给客户的不是一个“黑盒”系统，而是一个运营方可参与配置、持续演进的“活”平台，能带来更好的客户满意度和长期合作机会。<br/><strong>拓展能力边界</strong>：即使团队不擅长3D或大数据技术，也能承接和交付高质量的数字孪生项目，开拓新的市场赛道。<br/>智慧园区的运营升级，是一场涉及数据、空间与业务的深刻变革。拥有一个能巧妙融合这三者的工具，无疑能让开发者在这场变革中，更从容地扮演赋能者和共建者的角色。</p>]]></description></item><item>    <title><![CDATA[BOM 冻结线为何总被打破？硬件研发“返]]></title>    <link>https://segmentfault.com/a/1190000047446701</link>    <guid>https://segmentfault.com/a/1190000047446701</guid>    <pubDate>2025-12-03 17:05:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><em>几乎所有做硬件研发管理和项目管理的人，都设过“BOM 冻结线”，也常常眼看着它一次次被新发现的问题击破：紧急改料、补充测试、推迟排产，导致材料报废和工程返工工时逐年增加。本文将结合ALM 应用生命周期管理与 IPD 集成产品开发体系，拆解 BOM 冻结管理失效的根因，并给出一套可落地的治理框架，帮助硬件团队真正跳出“返工陷阱”</em></blockquote><h2>硬件研发现场的 BOM 冻结尴尬</h2><p>如果你长期在硬件产品研发、电子制造业或硬件项目管理一线工作，下述场景大概率不陌生：</p><p>项目已经通过量产评审，会议纪要上写着“BOM 正式冻结”；供应链刚锁完物料采购，制造也依据 MBOM 排好了产能和生产节拍；你以为一切都在掌握中，结果下一刻，一封主题为《紧急：某型号器件需更换》的邮件打破了宁静——要么是关键器件停产，要么是可靠性测试刚测出边界问题，要么是经营压力要求本项目再压一轮物料成本。</p><p>接下来，工程变更管理（ECR/ECO）在系统里排成长龙：</p><ul><li>大量 ECO 触发 PCB 改版、工装治具调整、测试用例补测；</li><li>物料清单（BOM，Bill of Materials）在 EBOM、MBOM、ERP 三端反复校对；</li><li>项目经理在“交付进度、成本控制、质量风险”之间被迫做艰难平衡，只能选一个“看上去最不糟”的方案。</li></ul><p>从财务和交付维度看，这些临时应对的决策，叠加起来就是典型的硬件研发返工成本：物料报废、制造停线、额外验证资源、客户交付风险，以及被挤占掉的其它项目机会。</p><p>BOM 冻结线原本是用来做 BOM 冻结管理、控制工程变更、降低返工的“安全护栏”，现实中却常常沦为被反复跨越的“建议线”。不少企业在复盘时，习惯把责任归结为：</p><blockquote><em>“某次评审不严”“某个团队责任心不足”。</em></blockquote><p>但如果冷静问自己一句：“我们明知道这条 BOM 冻结线守不住，为什么还要设？”<br/>你会发现，这并不是某一两次评审失误，而是整个硬件研发体系、工程变更治理机制和组织治理方式出现了结构性问题。</p><h2>为什么 BOM 冻结线总被打破</h2><p>这一节，我会从 ALM（应用生命周期管理）、IPD（集成产品开发）以及配置管理的视角，拆解常见的 BOM 冻结失效原因，帮助你把“现象级问题”放回“体系级框架”里看。</p><h4>1. 冻结线被当成“行政规定”，而不是配置基线</h4><p>在成熟的 ALM / 配置管理体系中，BOM 冻结线本质上是一个“系统基线（Baseline）”：</p><ul><li>它绑定特定的需求版本、设计版本、验证结果与质量数据；</li><li>它是后续工程变更（ECR/ECO）评估的参照物；</li><li>它定义了“当前有效配置”的边界，是硬件产品配置管理中的关键节点。</li></ul><p>但在很多硬件研发管理现场，BOM 冻结线更多是一个“时间点上的宣告”：</p><blockquote><em>“从今天开始，BOM 不允许再改了。”</em></blockquote><p>缺少与需求配置、设计配置、测试验证的端到端关系，BOM 冻结管理就只能依赖个人自觉与行政推动。一旦遇到“商业压力 + 技术风险”的组合，“不开口子”的人反而会显得不合群。</p><p>一个简单的自查问题是：</p><blockquote><em>“现在让你在 10 分钟内拿出某个量产型号的当前有效 BOM 基线，含其对应需求版本、设计版本和测试结果，你做得到吗？”</em></blockquote><p>如果答案是否定的，那么在这个组织里，BOM 冻结线更多只是流程文件中的描述，而不是在 ALM / PLM 里被真实维护的系统基线。</p><h4>2. 前端不稳定：需求与架构模糊，后端 BOM 被动“还债”</h4><p>系统工程和 IPD 都强调：70% 以上的成本和风险在前期需求与架构决策中已经锁定。需求模糊、架构摇摆、接口频繁变化，最终都会通过后端的 BOM 变更和硬件返工来“还债”。</p><p>典型表现包括：</p><ul><li>需求管理停留在 PPT 和 Excel 表格，ALM 里没有完整的需求树和变更记录；</li><li>系统架构设计不到位，模块边界和接口不清晰，导致选型、布局和功耗分配在后期不断调整；</li><li>软件/硬件、结构/电子缺乏联合方案评审，硬件 BOM 只能在集成阶段被动跟随上游需求变化。</li></ul><p>在这些条件下，项目后期出现大量因需求变更、架构调整引发的 ECO 并不意外。只是这些“前端债务”，往往在项目报表里被模糊成“若干次紧急改料”，看起来是战术问题，本质却是前端工程（需求工程 + 系统架构工程）没有做好。</p><h4>3. 端到端数据链路断裂：ALM / PLM / ERP 各自为政</h4><p>在 IPD 研发体系和数字化研发管理平台的理想状态里，BOM 是一条端到端数字化链路中的“节点视图”：</p><ol><li>上游连接需求、系统分解、详细设计；</li><li>中间在 PLM 中形成 EBOM / MBOM；</li><li>下游通过 ERP 连接供应链、库存与制造执行。</li></ol><p>而在不少企业现场，实际情况是：</p><ol><li>研发在 ALM 或本地工具里维护工程 BOM（EBOM）；</li><li>工业化团队在 PLM 或独立系统中维护 MBOM，字段和命名各搞一套；</li><li>ERP 里还有另一种物料视图，用于采购与财务。</li></ol><p>结果就是：</p><ol><li>没有人真正相信“当前某版本 BOM 就是真相”；</li><li>临量产前必须通过人工对表、Excel 校验来确认版本；</li><li>每次对齐都伴随错误风险和大量隐形人力成本。</li></ol><p>当数据真相都不清楚时，任何“BOM 冻结管理”都是纸面承诺。问题总是在接近量产导入的时候集中爆发，“打破冻结线”就变成一个“不得不做”的选项。</p><h4>4. IPD 决策关口形同虚设：评审“过了”，问题却还在</h4><p>不少公司引入了 IPD 流程，DR1/DR2、PDR、CDR、MP 等评审节点一个不少，纸面流程也很完整。但实际操作中常常变成：</p><ul><li>评审主要看 PPT，不看 ALM/PLM 等系统里的配置基线和数据视图；</li><li>BOM 成熟度没有可量化标准，评审结论停留在“基本可行”“整体可控”；</li><li>对“BOM 冻结后变更”的约束和复盘机制缺失，没有形成组织级记忆。</li></ul><p>在这种状态下，IPD 关口很难对后续 BOM 稳定性真正负责。BOM 冻结线被定义在流程图里，却没被嵌入 IPD 决策逻辑和工程变更管理机制里。</p><h2>如何在硬件研发中构建“不轻易被打破的冻结线”</h2><p>要让 BOM 冻结线真正发挥作用，不能只靠“严禁改动”的口号，而需要一整套结合 ALM / IPD 的方法论与治理机制。下面是一个实践框架，可供中高层研发管理者、PMO、项目经理和系统工程师共用。</p><h4>1. 从“一刀切冻结”到“分层、分阶段的 BOM 冻结策略”</h4><p>第一步是重新定义“冻结”本身，而不是简单地设一个日期：</p><ul><li>按 BOM 视图 区分：工程 BOM（EBOM）、制造 BOM（MBOM）、服务 BOM（SBOM）；</li><li>按 物料重要性 分层：关键器件（核心芯片、电源、关键连接器）、风险器件（停产风险、超长交期）、普通物料；</li><li>按 时间轴 设定多个冻结点，而不是唯一“终极时刻”。</li></ul><p>一个常用的实践是分三层冻结：</p><p><strong>① 架构级冻结（Architecture Freeze）</strong></p><ul><li>面向系统工程和 IPD 的概念设计阶段；</li><li>冻结技术路线、产品平台、关键接口与性能/功耗预算，明确主芯片大类和关键方案；</li><li>目标是减少因架构调整引发的大规模 BOM 变更。</li></ul><p><strong>② 关键器件冻结（Key Component Freeze）</strong></p><ul><li>在详细设计和样机试制之前完成；</li><li>冻结核心芯片、电源方案、关键连接器等成本和风险权重最高的物料；</li><li>后续任何针对这些物料的变更都必须经过 CCB（变更控制委员会）审核。</li></ul><p><strong>③ 全 BOM 冻结（Full BOM Freeze）</strong></p><ul><li>在量产导入前，与 MP/PPAP 等评审节点耦合；</li><li>要求 EBOM、MBOM 与 ERP 物料视图一致，并通过必要的验证和试产数据校验。</li></ul><p>通过分层、分阶段的 BOM 冻结策略，可以把“绝对不能轻易改”的部分尽早固化，把仍需优化的部分显性化，避免在项目尾声做大型手术式改动。</p><h4>2. 用系统工程方法，把“变更欲望”前移到可控阶段</h4><p>要减少后期打破 BOM 冻结线的冲动，就必须把试错和优化前移到需求工程和系统方案阶段。系统工程方法提供了三个抓手：</p><p><strong>① 用 V 模型构建需求–设计–验证的一致性链条</strong></p><p>在 ALM 平台中建立需求分解结构（系统需求 → 子系统需求 → 设计规格）；<br/>对关键需求建立双向追踪：从需求到设计文档、到 BOM 物料、到测试用例；<br/>在架构评审 / PDR / CDR 时，不只问“功能看上去实现了没有”，而是查看“需求覆盖率和验证闭环”。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnfdk" alt="V 模型示意图" title="V 模型示意图"/></p><p><strong>② 强化概念验证与仿真，减少“实物试错式返工”</strong></p><ul><li>对高风险电源、高速信号链路等提前做仿真与小板验证（EVB），在 BOM 冻结前消除一批显而易见的风险；</li><li>对结构、散热等问题用仿真和样机联合验证，缩短试错周期；</li><li>把这些活动纳入 IPD 任务书和项目计划，而不是“有时间再做”。</li></ul><p><strong>③ 设立明确的“BOM 冻结前变更窗口”</strong></p><ul><li>在项目计划中清晰标出在哪几个迭代周期允许对 BOM 做大幅调整；</li><li>窗口期内的变更流程相对简化，但必须保留原因和验证记录；</li><li>超出窗口，则通过 ECR/ECO 和 CCB 来控制，形成变更可见、成本可见的治理机制。</li></ul><p>当变更欲望被前移到可控窗口，并在 ALM/PLM 中形成清晰的信息链条时，后期“拍脑袋改料”的空间自然会变小。</p><h4>3. 建立 ECR / ECO 分级工程变更治理机制</h4><p>很多公司有 ECR/ECO 表单，但缺少工程变更治理逻辑，导致 BOM 冻结管理无法落地。一个典型的治理思路是：</p><p><strong>① 明确 ECR（变更请求）与 ECO（变更实施）的分工</strong></p><ul><li>ECR：讨论“是否要改”，关注问题、动机、影响和可选方案；</li><li>ECO：在决策后落实“具体怎么改”，包括 BOM、图纸、工艺、测试、文档等更新；</li><li>禁止“跳过 ECR 直接发 ECO”的做法，避免绕过系统性的影响评估。</li></ul><p><strong>② 按影响等级分级管理</strong></p><ul><li>A 级变更：影响安全、法规合规、重大质量风险，冻结后仍允许，但必须由跨部门 CCB 和项目高层批准；</li><li>B 级变更：影响成本、关键性能、供应风险，由项目 CCB 审批；</li><li>C 级变更：影响有限的小改动，可由模块负责人审批，但必须在 PLM / ALM 中留痕。</li></ul><p><strong>③ 在 ECR 中强制评估五个维度</strong></p><ul><li>对客户价值和市场竞争力的影响；</li><li>对项目进度和资源的影响；</li><li>对成本（材料、制造、质量保障）的影响；</li><li>对安全、法规、长期可靠性的影响；</li><li>替代方案（维持现状的后果是什么）。</li></ul><p>冻结线之后不是“不许改”，而是“必须通过可审计、可度量的工程变更流程来决策是否值得改”。</p><h4>4. 用 ALM / PLM 打通需求–设计–BOM–制造的数字链路</h4><p>想让 BOM 冻结线有现实意义，需要一条可信的数字化主线，而不是三套孤立系统。实践中可以按“最小可行 + 逐步演进”来设计：</p><p><strong>① 以 ALM 为需求与设计配置的源头系统</strong></p><p>所有正式需求与变更需求在 ALM 中维护，形成需求基线；<br/>需求项与设计文档、BOM 条目、测试用例建立追踪关系；<br/>在关键评审节点冻结需求/设计基线，与后续 BOM 冻结相呼应。</p><p><strong>② 以 PLM 为 EBOM/MBOM 与配置管理中枢</strong></p><p>在 PLM 中维护 EBOM，关联版本和变更记录；<br/>工业化团队在 PLM 中从 EBOM 派生 MBOM，并关联工艺路线和工装治具；<br/>通过差异报表或可视化看板来监控 EBOM 与 MBOM 的偏差。</p><p><strong>③ 与 ERP / 供应链系统形成闭环</strong></p><p>在 BOM 冻结前确保 ERP 中的物料编码、供应商信息、价格和交期等同步；<br/>冻结后任何 ECO 自动评估库存、在途订单和产能计划的影响。</p><p>这条数字链路的目的不是“堆工具”，而是让需求–设计–BOM–制造这条系统工程逻辑，真的在数据里可见可追踪。</p><h4>5. 把“BOM 冻结纪律”转化为可运营的指标体系</h4><p>靠口头强调很难改变行为，建议 PMO 或 R&amp;D Ops 建立轻量指标，把 BOM 冻结管理运营起来：</p><ul><li>冻结后 ECO 数量与分布（按级别、按项目）；</li><li>因 BOM 变更导致的返工成本：报废物料、返工工时、额外测试与验证资源；</li><li>BOM 冻结及时率：计划冻结日期 vs 实际冻结日期；</li><li>变更决策周期：从 ECR 提出到 ECO 关闭的平均时间。</li></ul><p>这些指标不必一上来就用于“硬考核”，更有价值的场景是：</p><ul><li>项目例会和季度评审的固定看板；</li><li>横向对比不同产品线 / 平台的变更行为模式；</li><li>为管理层的资源投入（优化前端架构、升级平台、重构模块）提供数据支持。</li></ul><h4>6. 在 IPD 框架下重构关键评审节点，让冻结线“嵌入流程”</h4><p>要让 BOM 冻结线具备权威性，需要和 IPD 关键评审深度耦合：</p><p><strong>① 在 PDR 上确认架构级冻结</strong></p><ul><li>审查关键技术路线、接口和资源预算是否清晰；</li><li>将架构级决策纳入配置基线，后续重大架构调整提升到平台级决策。</li></ul><p><strong>② 在 CDR 上确认关键器件冻结</strong></p><ul><li>审查关键器件验证报告、供应风险评估和备选方案；</li><li>将关键器件清单纳入项目风险清单和后续 ECO 约束。</li></ul><p><strong>③ 在 MP / 量产评审上确认全 BOM 冻结</strong></p><ul><li>审查 EBOM/MBOM/ERP 的一致性和试产数据；</li><li>对冻结后 ECO 做说明，为组织沉淀经验。</li></ul><p>评审不只是“放行”，更要成为 BOM 冻结管理的“质量门”和组织知识的沉淀节点。</p><h4>7. 从组织协同与激励机制上拆掉“返工陷阱”</h4><p>最后，如果组织协同和激励方向错误，再好的工程变更流程也会被绕过。几个经常被忽视的点：</p><p><strong>① 让供应链、制造、质量真正前移参与</strong></p><ul><li>在 IPD 项目团队中，让供应链、制造工程、质量成为前期方案阶段的正式角色，而非“后期支持”；</li><li>对高风险器件，要求供应链提前给出多源策略和生命周期分析；</li><li>制造和质量提前对 BOM 提出可制造性和长期可靠性要求。</li></ul><p><strong>② 淡化“英雄改料文化”，强化“前期稳态文化”</strong></p><ul><li>少讲“最后一刻改料救回项目”的英雄故事，多在复盘中讨论“为什么问题没在前面暴露”；</li><li>把“冻结后 ECO 数量、返工成本、按计划冻结情况”纳入项目复盘指标。</li></ul><p><strong>③ 用清晰的 RACI 避免“谁都能改一点”</strong></p><ul><li>对 BOM 变更设定 RACI（负责 / 参与 / 咨询 / 知情），明确提出人、评估人、决策人和验证人；</li><li>避免“本地优化、全局返工”的局面，让每一次打破冻结线都在数据上留下可追踪的痕迹。</li></ul><h2>给中高层管理者和 PMO 的几条现实建议</h2><p>对于已经饱受 BOM 冻结线反复被打破困扰的硬件团队，不必指望“一次性大改造”。更现实的路线是渐进式演进：</p><p><strong>① 选一个典型产品线试点</strong></p><ul><li>选择物料复杂度高、业务重要、变更频繁的产品线；</li><li>在试点里跑通：分层冻结策略 + ECR/ECO 分级治理 + 最小可行数字链路。</li></ul><p><strong>② 先建立“数据真相”再谈全面集成</strong></p><ul><li>梳理当前 EBOM/MBOM/ERP 的关键字段和对齐方式；</li><li>用简单脚本或定期对账方式建立“当前有效 BOM 视图”，为后续深度集成打基础。</li></ul><p><strong>③ 让 PMO 把冻结纪律纳入项目运营例会</strong></p><ul><li>固定查看冻结后 ECO 和返工成本数据；</li><li>通过轻量复盘沉淀经验，形成可复用的治理模式。</li></ul><p><strong>④ 中高层用行为释放清晰信号</strong></p><ul><li>在商业压力和技术风险冲突时，公开讨论“不改的代价”和“改的代价”；</li><li>对频繁突破冻结线且论证不足的项目，要求严肃复盘，而非“一笑而过”；</li><li>对前期稳住架构、减少后期 ECO 的团队给予正向激励。</li></ul><h2>从“救火式改料”走向“体系化决策”</h2><p>BOM 冻结线被反复打破，并不是单一评审的偶然失误，而是硬件研发管理中需求不稳定、架构前移不足、ALM/PLM/ERP 数据割裂、IPD 评审流于形式以及组织激励失衡等系统问题在 BOM 上的集中体现。</p><p>要跳出硬件研发的“返工陷阱”，需要：</p><ul><li>把 BOM 冻结线视为系统基线和配置管理节点，而不是行政禁令；</li><li>用 系统工程 + ALM 应用生命周期管理 + IPD 流程，构建需求–设计–BOM–制造的数字化链路；</li><li>通过分层冻结、ECR/ECO 分级治理、可度量指标，把“冻结纪律”转化为可运营的管理能力；</li><li>通过组织协同与激励调整，让团队从“救火式改料”转向“前瞻性、数据支撑的工程变更决策”。</li></ul><p>当一个组织可以坦然说出：</p><blockquote><em>“我们不怕变更，但每一次打破冻结线都有明确理由、决策记录和成本认账。”</em></blockquote><p>BOM 冻结线才真正从 PPT 走进硬件研发体系的肌理，也才算真正走出了硬件研发“返工陷阱”。</p>]]></description></item><item>    <title><![CDATA[智能研发管理：制造业如何实现从“单打独斗]]></title>    <link>https://segmentfault.com/a/1190000047446748</link>    <guid>https://segmentfault.com/a/1190000047446748</guid>    <pubDate>2025-12-03 17:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近和制造业的朋友聊天，大家几乎都在谈数字化转型，聊到研发管理，话题就更热烈了。市场节奏越来越快，技术也在不断迭代，传统研发管理方式显然跟不上了。尤其是汽车、装备制造这些离散制造行业，跨部门协作复杂，信息孤岛严重，研发过程中的痛点太多了。设计数据分散，版本混乱，文档管理滞后，流程审批依赖人工……这些看似独立的问题，其实都是一根绳子上绑着的蚂蚱。<br/>举个例子，很多企业的设计文档和图纸数据分散在不同的系统里，各部门拿到的信息版本不一致，沟通成本居高不下。再加上三维模型的协作效率差，非设计人员很难快速查看和使用这些数据，严重影响了跨部门协同的效率。这时候，研发管理平台的重要性就凸显出来了。<br/>广域铭岛的Geega捷做设计研发协同平台，就是在这种背景下出现的。它不是简单地把PDM、PLM这些工具堆砌在一起，而是真正打通了研发全链条。从需求收集、设计评审到BOM管理、工艺协同，整个流程都整合在一个平台上，信息共享更高效，版本追溯更清晰。系统通过自动触发审批流程，让审批人员在移动端就能处理，再也不用在各个系统之间来回切换，效率直接提升了40%。<br/>说到这个平台，它最吸引人的地方在于“协同”二字。想象一下，销售、采购、生产、质量等部门的人都在一个平台上工作，实时共享数据。比如，Fview模块支持60多种CAD格式，一线人员只需要扫码，就能查看三维模型并在线评审，不用再为格式转换和软件兼容发愁。这在实际应用中效果特别明显，某科技企业引入后，模型查看效率提升60%，跨部门协作周期缩短了50%。<br/>当然，研发管理不仅仅是数据协同。质量管控也是一个重要环节。FMEA模块的加入，让研发团队能够更早地发现问题、预防风险。基于历史问题库，系统可以自动推荐整改措施，把被动应对变成了主动预防。某制造企业在应用后，FMEA的编制效率提升了20%，质量管理也实现了从“事后补救”到“源头预防”的转变。<br/>不过，研发管理的数字化转型还远不止这些。更关键的是，系统要能够沉淀数据资产，形成企业的知识库。比如，标准BOM数据的一致性和准确性，直接影响到后续的生产、采购和交付。统一的数据源头，不仅减少了出错率，还提高了资源利用率。某家电企业在应用后，零部件复用率提升35%，BOM数据准确率也达到了80%。<br/>说到竞争对手，其实市场上有不少研发管理平台，比如PTC、达索系统这些老牌厂商。它们的功能也很强大，但在灵活性和适用性上，广域铭岛的平台确实更贴近国内制造业的需求。尤其是在中小企业的数字化转型中，它的弹性设计能力让很多企业受益。<br/>当然，转型过程中也会遇到一些挑战。比如，系统集成、数据迁移、人员培训这些环节，都需要细致规划。但从长远来看，这些投入都是值得的。因为研发管理的智能化，不仅提升了效率，还让企业能够更快地响应市场变化。<br/>总的来说，智能研发管理不是一句口号，而是制造业数字化转型的核心需求。它要求企业打破传统的管理方式，用数据驱动创新，用协同提升效率。</p>]]></description></item><item>    <title><![CDATA[2025 SECon × AgentX ]]></title>    <link>https://segmentfault.com/a/1190000047446795</link>    <guid>https://segmentfault.com/a/1190000047446795</guid>    <pubDate>2025-12-03 17:03:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：盈楹</p><p>近日，2025 SECon × AgentX大会——AI 原生应用架构专场圆满落幕，本次专场阿里云联合信通院共同出品，现场吸引了 80+ 名技术从业者深度参与。</p><p>活动聚焦 AI 时代软件架构的核心命题，深度分享了 AI 原生应用架构趋势与实践、AgentScope 开发框架、AI 开放平台、大模型可观测 &amp; AIOps 等热门技术议题，探讨从基础设施到应用层的协同演进策略与工程实践。</p><p>关注「阿里云云原生」公众号，后台回复：1125</p><p>免费获得活动讲师 PPT 合辑</p><h2>精彩回顾</h2><h3>议题一：AI 原生应用架构探索与实践丨肖京(亦盏)   阿里云智能云原生高级技术专家</h3><p>当前大模型已迈过技术拐点，Agentic AI 进入规模化落地阶段。AI 原生应用以模型为基础、Agent 为驱动、数据为中心，推动系统从“机器执行”向“机器思考+执行”演进。框架选型需平衡 Agentic 自主性与业务确定性，Agent 面临开发效率、业务效果，以及稳定、性能、成本、安全的挑战。实践上建议构建以数据为核心的 Agent 平台，结合 MCP/A2A 标准与 Serverless 架构，通过 AI 网关、消息队列、可观测等提升安全性、稳定性与可维护性，实现智能化人机协作新范式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446797" alt="image" title="image"/></p><h3>议题二：开发更可控，部署更便捷：AgentScope 迈入 1.0 时代丨邝炜瑞  阿里巴巴集团通义实验室 高级算法工程师</h3><p>深度分享了阿里通义实验室开源的 AgentScope 智能体开发框架 1.0 版本。核心内容包括：基于 ReAct 范式的多智能体系统支持，提供结构化输出、工具调用与长期记忆等能力；采用三层架构——开发框架层、可视化调试平台与安全运行时环境，结合工具沙箱与元工具机制，全面提升系统的可控性、可观测性与部署安全性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446798" alt="image" title="image" loading="lazy"/></p><h3>议题三：AI 网关：AI 原生架构下的智能流量中枢丨 赵炳堃(秉钧)   阿里云智能云原生高级开发工程师</h3><p>聚焦 AI 网关在 AI 原生架构中的核心作用，重点介绍 Higress AI 网关的关键能力：支持多模型适配、协议转换与语义缓存，提供 Token 限流、Fallback 机制保障高可用；通过 API-Key 管理、PII 脱敏、WASM 沙箱等实现安全可控；并借助 MCPServer 统一代理提升集成效率。HiMarket 平台则助力企业构建私有 Agent 市场，推动 AI 能力的安全规模化落地。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446799" alt="image" title="image" loading="lazy"/></p><h3>议题四：从可观测到 RL：打造生产级可靠的长周期 Agent丨马云雷  阿里云智能云原生技术专家</h3><p>聚焦 AI-Native 应用中长周期 Agent 的可靠性建设，提出以可观测性为基础，通过 OpenTelemetry、Prometheus 等工具实现全栈监控，做到“可见、可调、可审”。引入 LLM Judger 作为自动化评估裁判，结合数据工程与模型蒸馏，构建快慢反馈闭环。最终形成从问题发现、根因分析到自我强化的进化系统，推动 Agent 向高可靠、自演进的生产级应用发展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446800" alt="image" title="image" loading="lazy"/></p><h2>现场精彩瞬间</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446801" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446802" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446803" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[【运维自动化-标准运维】快捷键使用技巧（]]></title>    <link>https://segmentfault.com/a/1190000047446817</link>    <guid>https://segmentfault.com/a/1190000047446817</guid>    <pubDate>2025-12-03 17:03:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>快速框选画布流程节点</h2><h3>1.在流程画布左上方有对应框选画布的按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446819" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>2.点击按钮—框选节点</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446820" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.框选成功后–对应节点有虚线包围</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446821" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>变量透视</h2><p>该功能可以展示对应节点中引用了的输入变量以及该节点的输出变量</p><h3>1.在流程画布做左上方有对应变量透视的按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446822" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>2.点击按钮–展示节点变量按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446823" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.将鼠标移动到对应节点上时，即展示对应节点的变量使用情况</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446824" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>说明：适合产品版本 V6.1/V6.2/V7.0/V7.1</p>]]></description></item><item>    <title><![CDATA[如何打造AI时代的数据基石 | Data]]></title>    <link>https://segmentfault.com/a/1190000047446840</link>    <guid>https://segmentfault.com/a/1190000047446840</guid>    <pubDate>2025-12-03 17:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Data + AI 已经成为数据从业人员必须关注的技能。在基于 Databend Cloud 平台上可以大大简化数据人员在数据基础工作方面的投入，让数据人员可以花更多的精力去研究 Data + AI 的实践。在此背景下，11月29日，Databend Meetup·上海站线下活动"如何打造 AI 时代的数据基石"，汇集了国内数据库领域多位一线专家：Databend 创始人吴炳锡、沉浸式翻技术专家陈琦，沈超、资源数据平台架构师邵锋、TiDB 解决方案架构师 刘源、空中云汇架构师赵飞祥以及来自各行各业的技术负责人，数据部门负责人。参会嘉宾围绕"如何打造 AI 时代的数据基石"的主题，共同探讨了大模型时代数据库和数据平台的创新演进与实战应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446842" alt="图片" title="图片"/></p><p>以下内容就将为您带来这些话题背后的深度思考：<br/>基于 Databend 无编程实 Data Pipeline 及数据分析<br/>Databend Labs 联合创始人吴炳锡，系统地介绍了 Databend 作为一款云原生数据仓库，如何以其独特架构和技术特性，极大地简化和革新传统大数据 Data Pipeline 的构建与数据分析流程，并展示了其与 AI 融合的强大潜力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446843" alt="图片" title="图片" loading="lazy"/></p><p>Databend 的清晰定位：解决传统大数据之痛<br/>分享开宗明义，指出了 Databend 的核心定位：简单易用、高性能、低成本。其目标是成为一款云原生湖仓一体化产品，旨在：</p><p>降低云上大数据成本：利用对象存储实现极致的存算分离和低成本存储。<br/>简化数据架构：坚持" SQL 为王"，让复杂的湖仓开发变得像使用传统数据库一样简单。<br/>统一数据枢纽：支持构建企业级统一数据仓库，并提供跨多云、跨 IDC 的高可用体验。</p><p>核心革新：重构 Data Pipeline 的开发模式<br/>分享通过对比，深刻剖析了传统大数据架构（依赖 Kafka, Flink, Spark, Trino 等繁多组件）的痛点：技术栈复杂、技术要求高、落地慢、运维成本高昂。<br/>针对这些痛点，Databend 提出了一套以 SQL 为中心 的"无编程" Data Pipeline 解决方案，其核心构件包括：</p><p>数据秒级摄入 (COPY INTO + External Stage)：通过监听对象存储事件，实现海量数据的快速加载与可见。<br/>内置流计算 (Stream)：提供表级增量变更捕获能力，无需额外组件即可实现高效的实时 ETL，性能提升可达 10 倍。<br/>自动化任务调度 (Scheduled Task)：通过 Serverless Task 实现完整的数据处理工作流编排，让一个懂 SQL 的人就能轻松完成复杂的数据治理。<br/>强大的外部函数 (UDF)：支持用 Python 等语言轻松扩展功能，实现与外部系统（如更新 Redis）或 AI 服务的无缝集成。</p><p>与 AI 的深度融合：从数据平台到智能基座<br/>分享重点展示了 Databend 在 AI 时代的前瞻性，其与 AI 的融合体现在两个层面：</p><p>原生 AI 能力：内置向量计算和 AI 函数（如cosine_distance），为 AI 应用提供开箱即用的支持。<br/>可扩展的 AI 集成 (External UDF)：通过 UDF 可以方便地调用 Embedding 模型、情感分析、文本相似度等外部 AI 服务，将 Databend 升级为一个支持智能化数据分析与应用的" AI 原生"平台。</p><p>卓越效益与广泛验证<br/>分享通过具体数据证明了 Databend 的卓越效益：</p><p>成本大幅降低：在替换 Trino/Presto、Elasticsearch、数据归档等场景中，成本降低 75% 到 95%。<br/>极致的可扩展性：支持单表 2.6 万亿行、1PB+ 的超大规模数据处理。<br/>广泛的行业应用：已成功服务于中信银行、微盟、苹果中国等知名企业，应用于主数据平台、日志分析、数据归档等多种场景。</p><p>总结<br/>Databend 通过其云原生、一体化的架构，将复杂的大数据技术栈简化为以 SQL 为核心的开发体验，从根本上降低了数据开发的门槛、成本和运维负担。 它不仅是一个高性能的数据仓库，更是一个内置了流处理、任务调度和强大扩展能力的数据平台操作系统。在 AI 时代，其原生及可扩展的 AI 能力进一步使其成为企业构建智能化应用的理想数据基石，完美契合了当下企业追求降本增效和快速创新的核心诉求。<br/>构建海量记忆：基于 Databend 的 2C Agent 平台|沉浸式翻译<br/>沉浸式翻译团队技术专家陈琦在 构建海量记忆：基于 Databend 的 2C Agent 平台|沉浸式翻译实践分享，核心阐述了他们如何利用 Databend 构建一个面向海量用户的、具备"长期记忆"能力的 AI Agent 平台。<br/>沉浸式翻译在比较早期已经接入 Databend , 公司内部在无运维的情况下，支撑了千万级用户，月活百万级用户。Databend 目前不但承担沉浸式翻译的平台分析数据，也承担了部分业务类数据。 目前团队正在 Databend 上构建海量记忆体的 2C Agent 平台。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446844" alt="图片" title="图片" loading="lazy"/></p><p>核心挑战：<br/>传统方案的痛点：</p><p>组件割裂：维护向量库、关系型数据库、缓存等多套系统，开发和运维复杂。<br/>缺乏生命周期管理：向量库只增不减，导致噪音增加、性能下降、成本飙升。</p><p>为什么选择 Databend？</p><p>All-in-One：统一处理向量、结构化和半结构化（JSON）数据，简化架构。<br/>Serverless：零运维、按需付费，完美契合小团队"小步快跑"的模式。<br/>可编程性：通过 SQL、UDF 和 Task 实现复杂的数据处理和生命周期管理。大大简化开发投入</p><p>核心架构与创新（MemOS）：</p><p>MemNodes 表：作为记忆实体，利用计算列和聚簇索引优化混合查询（向量+条件过滤）性能。<br/>MemEdges 表：构建记忆图谱，用 SQL 存储关系，解决纯向量检索无法处理的逻辑推理问题。<br/>混合检索算法：结合 SQL 过滤、向量搜索和图关联，实现精准且上下文丰富的记忆召回。<br/>自动化生命周期：通过 Serverless Task 定期对记忆进行摘要融合和归档，实现"会遗忘的智能系统"。</p><p>价值总结：<br/>该实践成功地将 Databend 作为统一数据基石，以极低的运维成本和优雅的技术方案，实现了从"翻译工具"到懂用户的"语言伴侣"的演进，为 2C AI 提供了易用，低成本，高性能的平台。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446845" alt="图片" title="图片" loading="lazy"/></p><p>Data + AI - 数据平台的应用和实践<br/>第三个分享中邵锋老师带着一线经验给我们分享数据平台的建设和 Data+AI 实践。属于非常硬核的分享，因为保密问题就不再公开邵锋老师的分享。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446846" alt="图片" title="图片" loading="lazy"/></p><p>AI 时代的数据基石：趋势、挑战与 TiDB 实践<br/>TiDB 解决方案架构师刘源老师，从行业更宏观的视角探讨了 AI 时代的数据挑战，并阐述了 TiDB 作为"数据基石"的解决方案和案例。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446847" alt="图片" title="图片" loading="lazy"/></p><p>核心洞察：</p><p>AI 发展趋势：大模型进入平台期，下一代突破指向"世界模型"。当前 AI 面临幻觉问题（源于概率生成的有损压缩）、算力消耗和伦理安全等挑战。<br/>AI 应用现状：情感陪伴、内容生成等"幻觉友好型"应用火热，但金融、制造、医疗等严肃 ToB 场景落地艰难，面临数据治理缺失、场景碎片化等挑战。</p><p>AI 时代对数据库的新要求：</p><p>多模态融合：同时处理关系表、向量、全文、图谱等数据，"多库合一"。<br/>实时与高扩展：弹性支撑 Agent 的推理、记忆和 Multi-Agent 协作。<br/>支持 AI 原生体验：成为 Agent 的"集体记忆中枢"，能主动交互。</p><p>TiDB 的解决方案：</p><p>核心特性：金融级高可用、天生的弹性扩展、HTAP 一体化架构、正在演进的多模态数据融合能力。</p><p>AI 原生探索：</p><p>增强数据访问层：通过 RAG、GraphRAG 等技术，将 TiDB 打造成企业知识核心，降低大模型幻觉。<br/>构建 Data Agent 能力：研发 AutoFlow，让用户用自然语言直接进行混合查询和数据分析。<br/>面向 Multi-Agent 未来：扮演"共同记忆体"，支持数据版本化、分支管理等。</p><p>案例与价值：</p><p>为多家国内 TOP AI 及 Agent 厂商提供了可弹性扩展的数据底座，支撑了业务从零到亿级估值的狂飙。<br/>与 Databend 在归档场景合作，利用 TiDB 处理实时事务，Databend 处理低成本历史分析，实现降本增效。<br/>提出企业级 AI 平台整体架构，强调从"数据拼接"到"原生融合"的范式变革。</p><p>圆桌讨论环节<br/>在该环节邀请又邀请了空中云汇数据架构师赵飞祥，沉浸式翻译团队数据分析师沈超，TiDB 解决方案架构师刘源， 数据平台架构师邵峰 四位嘉宾一共交流了 AI 时代个人职业方面的感受， AI 对工作方面带来的变化， AI 时代需要什么样的人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446848" alt="图片" title="图片" loading="lazy"/></p><p>总结大家的观点：</p><p>AI 时代，让人每个人的能力更强了，能做的事更多了。 原来复杂的数据分析工作，原来可能需要1周，现在可能就是 1-2 天，或是更快。<br/>在 AI 时代不要给自我设限，上手一门技能非常的快。<br/>在 AI 时代更需要有 Owner 精神，端到端的解决问题的思路，需要懂得把工作拆分及推动下去。<br/>在 AI 时代同样需要有专业和权威的精神，能经住团队的挑战，能让老板放心把工作交给你。</p><p>圆桌讨论将视野拉回至"人"本身，为我们揭示了在 AI 时代更宝贵的特质。 当技术门槛被AI工具不断降低，"Owner 精神"、"端到端解决问题" 的能力以及 "专业权威" 的深度，构成了技术人新的护城河。AI 放大了个体的能力边界，但判断力、责任心和推动力，依然是不可替代的价值所在。<br/>总结而言，本次 Meetup 清晰地传递出一个信号： 打造 AI 时代的数据基石，已从一道可选题变为一道必答题。其答案不在于堆砌最前沿的独立组件，而在于选择一个能够简化架构、统一数据、智能赋能，并能伴随组织共同成长的一体化平台。我们欣慰地看到，以 Databend、TiDB 为代表的国内数据库力量，正以扎实的技术创新和丰富的场景实践，为各行各业提供着这道"必答题"的优秀解方。数据的浪潮奔涌向前，AI 的篇章刚刚开启。感谢所有嘉宾的倾情分享与参会者的热情投入，让我们共同期待，在这块坚实、智能的数据基石之上，生长出下一个时代的伟大应用。<br/>关于 Databend<br/>Databend 是一款 100% Rust 构建、面向对象存储设计的新一代开源云原生数据仓库，统一支持 BI 分析、AI 向量、全文检索及地理空间分析等多模态能力。期待您的关注，一起打造新一代开源 AI + Data Cloud。<br/>👨‍💻‍ Databend Cloud：databend.cn<br/>📖 Databend 文档：docs.databend.cn<br/>💻 Wechat：Databend<br/>✨ GitHub：github.com/databendlab...</p>]]></description></item><item>    <title><![CDATA[Linux Bash Shell 脚本编]]></title>    <link>https://segmentfault.com/a/1190000047446872</link>    <guid>https://segmentfault.com/a/1190000047446872</guid>    <pubDate>2025-12-03 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Linux Bash Shell编程： 👇🏻ke程：shanxueit点com/从命令行到脚本开发的全面指南<br/>本文将系统性地介绍Linux Bash Shell编程的完整知识体系，从基础概念到高级应用，帮助读者实现从命令行操作到脚本开发的技能跃迁。</p><p>一、Shell编程基础概念<br/>Shell是用户与Linux内核之间的桥梁，它提供了一个命令行界面，用户输入的命令由Shell解析后传递给内核执行，并将结果返回给用户。常见的Shell有Bash(Bourne-Again Shell)、Zsh(Z Shell)、Ksh(Korn Shell)等，其中Bash是Linux系统中默认且应用最广泛的Shell。</p><p>Shell脚本本质上是一个包含一系列命令的文本文件，它通过Shell解释器执行。与JavaScript、PHP等编程语言类似，Shell编程只需要一个文本编辑器和脚本解释器即可开始。Bash作为Bourne Shell的增强版，因其易用性和免费特性成为日常管理和自动化任务的首选工具。</p><p>Shell脚本的基础结构包括：</p><p>Shebang行(如#!/bin/bash)指定解释器路径<br/>注释说明(以#开头)<br/>可执行命令序列<br/>流程控制结构<br/>函数定义<br/>二、Shell脚本核心语法要素</p><ol><li>变量与数据类型<br/>Shell变量用于存储数据，通过=符号赋值(注意等号两边不能有空格)。变量名规则：</li></ol><p>只能包含字母、数字和下划线<br/>不能以数字开头<br/>区分大小写<br/>不能使用bash关键字(可用help命令查看保留关键字)<br/>变量引用使用<br/>符号，如<br/>符号，如var或${var}。Shell支持字符串、整数和数组等数据类型，其中数组可以存储多个值，方便批量操作。</p><ol start="2"><li>流程控制结构<br/>Bash提供了完整的流程控制语句：</li></ol><p>条件判断：if/elif/else/fi，case/esac<br/>循环结构：for/while/until/do/done<br/>循环控制：break/continue<br/>条件测试可以使用test命令或[ ]、[[ ]]结构，支持文件测试、字符串比较和数值比较等多种条件判断。</p><ol start="3"><li>输入输出与重定向<br/>Shell脚本通过以下机制处理输入输出：</li></ol><p>标准输入(stdin)、标准输出(stdout)和标准错误(stderr)<br/>重定向操作符：&gt;、&gt;&gt;、&lt;、&lt;&lt;<br/>管道(|)连接多个命令<br/>命令替换$(command)或command<br/>三、Shell编程进阶技巧</p><ol><li>函数与模块化<br/>函数是Shell脚本中实现代码复用的重要手段，定义语法为：</li></ol><p>PlainText<br/><br/>function_name() {</p><pre><code>commands
[return value]</code></pre><p>}<br/>函数支持参数传递(<br/>1<br/>,<br/>1,2,...$n)和返回值(通过return或echo输出)。</p><ol start="2"><li>错误处理与调试<br/>健壮的脚本需要完善的错误处理机制：</li></ol><p>使用set -e使脚本在命令失败时立即退出<br/>使用trap捕获信号并执行清理操作<br/>通过$?获取上一条命令的退出状态<br/>使用|| true忽略特定命令的错误<br/>调试模式(set -x)显示执行的每条命令</p><ol start="3"><li>文本处理三剑客<br/>Shell脚本常与以下文本处理工具配合使用：</li></ol><p>grep：基于模式搜索文本<br/>sed：流编辑器，执行文本替换等操作<br/>awk：强大的文本分析和报告工具<br/>这些工具支持正则表达式，能够高效处理日志分析、数据提取等任务。</p><p>四、Shell脚本实战应用场景</p><ol><li>系统管理自动化<br/>通过Shell脚本可以实现：</li></ol><p>批量用户管理<br/>系统监控与告警<br/>日志轮转与分析<br/>备份与恢复操作<br/>软件包批量安装<br/>例如磁盘空间排查脚本流程：</p><p>使用df -h确认问题范围<br/>通过du -sh * | sort -hr | head -5定位大目录<br/>逐层深入分析具体目录</p><ol start="2"><li>开发环境配置<br/>Shell脚本常用于：</li></ol><p>开发环境一键部署<br/>编译构建自动化<br/>测试用例批量执行<br/>持续集成流程</p><ol start="3"><li>网络与安全运维<br/>典型应用包括：</li></ol><p>批量主机状态检测<br/>安全漏洞扫描<br/>防火墙规则管理<br/>证书自动续期<br/>五、学习路径与资源推荐</p><ol><li>循序渐进的学习阶段<br/>基础阶段：掌握Linux常用命令和Shell基本语法<br/>脚本阶段：编写简单脚本，实现任务自动化<br/>进阶阶段：学习高级特性如数组、关联数组、进程控制<br/>精通阶段：掌握调试技巧、性能优化和复杂系统设计</li><li>推荐学习资源<br/>《Linux shell脚本编程入门》：系统梳理Shell脚本编程核心知识<br/>Bash官方文档：最权威的语法参考<br/>开源项目源码：学习优秀脚本的实现方式<br/>在线社区：CSDN等技术论坛的实战案例分享<br/>六、最佳实践与注意事项<br/>代码规范：</li></ol><p>添加清晰的注释<br/>使用有意义的变量名<br/>保持一致的代码风格<br/>适当添加日志输出<br/>安全考虑：</p><p>避免使用root权限执行不必要操作<br/>谨慎处理用户输入<br/>设置适当的文件权限<br/>注意敏感信息保护<br/>性能优化：</p><p>减少不必要的子进程创建<br/>使用内置命令替代外部命令<br/>批量处理替代循环操作<br/>合理使用缓存机制<br/>通过系统学习和持续实践，Bash Shell编程可以成为提升Linux系统管理效率的强大工具，实现从简单命令行操作到复杂自动化系统的能力跨越。</p>]]></description></item><item>    <title><![CDATA[第50届ICPC亚洲区域赛·上海站，非凸]]></title>    <link>https://segmentfault.com/a/1190000047446324</link>    <guid>https://segmentfault.com/a/1190000047446324</guid>    <pubDate>2025-12-03 16:11:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月22日-23日，第50届ICPC国际大学生程序设计竞赛亚洲区域赛·上海站在上海大学宝山校区圆满举行。来自221所高校、中学及企业的356支优秀队伍，千余名编程精英同台竞技，以智慧碰撞灵感，以技术角逐巅峰。作为赛事的重要支持方，非凸科技始终关注青年科技人才的成长，致力于推动计算机教育与产业实践的深度融合，为全球学子提供从理论到实践的成长通道。<br/><img width="553" height="373" referrerpolicy="no-referrer" src="/img/bVdne7p" alt="image.png" title="image.png"/><br/>开幕式上，非凸科技首席运营官郑媛姿发表致辞，ICPC从不止于胜负，跨越场次的坚守、迭代升级的解题思路、惺惺相惜的竞技情谊，都是更珍贵的成长馈赠。非凸科技始终以“搭建人才与产业的桥梁”为己任，深知人才是创新的核心密码，更愿为每一位怀揣技术梦想的同学，铺就“从赛场到金融实战”的成长快车道。<br/><img width="553" height="369" referrerpolicy="no-referrer" src="/img/bVdne7q" alt="image.png" title="image.png" loading="lazy"/><br/>赛事期间，非凸科技组织了企业宣讲与人才交流活动，向参赛选手们分享了在数智交易领域的前沿探索与人才布局。在激烈的角逐后，非凸科技代表为获奖队伍颁奖，鼓励他们保持创新热情与技术追求，并期待未来与更多优秀人才在产业实践中携手同行。<br/><img width="553" height="369" referrerpolicy="no-referrer" src="/img/bVdne7r" alt="image.png" title="image.png" loading="lazy"/><br/>本次竞赛，每支队伍需在5小时内通力协作，运用C/C++、Java和Python等其中一种编程语言解决13道复杂算法题目。经历4638次代码提交的密集交锋，最终30支队伍斩获金奖、60支队伍获得银奖、90支队伍摘得铜奖。北京大学“一步之遥”队以解出12题的出色表现荣膺冠军，复旦大学“随机一个字符串得了”队与上海交通大学“启明星”队分别获得亚军和季军。</p><p>以赛育才，聚力前行。未来，非凸科技将继续汇聚产业力量，携手学术界共同推动基础科学研究与应用技术创新的双向赋能，共同探索面向未来的人才培养路径。我们相信，在技术与人才双轮驱动的时代，唯有深化校企联结、促进全球智慧交融，才能为世界科技发展与经济社会繁荣持续注入新动力。</p>]]></description></item><item>    <title><![CDATA[2025年CRM选型全景图：国内外主流系]]></title>    <link>https://segmentfault.com/a/1190000047446343</link>    <guid>https://segmentfault.com/a/1190000047446343</guid>    <pubDate>2025-12-03 16:10:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>2025CRM选型全景图：国内外主流系统深度横评</h2><h3>一、引言：数字化深水区，CRM 成企业增长 “关键变量”</h3><p>2025 年，企业数字化转型进入 “价值兑现期”，客户关系管理（CRM）系统已从 “可选工具” 升级为 “增长基础设施”。客户全生命周期的精细化运营，成为企业穿越竞争周期的核心能力。据 Gartner 最新数据，2025 年全球 CRM 市场规模将突破 920 亿美元，年复合增长率维持 11.2% 的高位。</p><p>国内市场中，除了国际巨头与本土头部玩家，深耕行业 21 年的超兔一体云凭借 “全业务一体化” 特色崭露头角，为 6 万多家企业提供覆盖 CRM、进销存、财务、生产等的综合解决方案。面对多元化的产品矩阵，企业如何跳出 “功能堆砌” 的选型误区？本文将从核心能力、适用场景、技术特性三大维度，对 2025 年国内外主流 CRM 系统进行全景对比，为不同类型企业提供精准选型参考。</p><h3>二、国际主流 CRM 系统：技术积淀与全球化生态的双重优势</h3><p>国际厂商依托多年技术积累和全球化布局，在 AI 应用、生态兼容性和合规能力上形成壁垒，更适配有跨国业务或高端需求的企业。</p><h4>1. Salesforce：全球 CRM 的 “生态标杆”</h4><ul><li>市场地位：以 20.7%-26.1% 的全球市场份额连续 12 年领跑，2024 年营收超 300 亿美元，是行业绝对的头部玩家。</li><li>核心优势：Einstein AI 引擎贯穿营销、销售、服务全流程，支持客户流失预测、智能线索评分；AppExchange 平台拥有超 5000 个扩展应用，适配多语言、多币种及 GDPR/HIPAA 等全球合规要求；云原生架构保障跨国企业分布式业务的稳定运行。</li><li>局限性：单用户年费超 150 美元（约合人民币 1100 元 / 月），成本较高；国内访问速度受跨境网络影响，本地化响应较慢。</li><li>适用场景：预算充足的跨国企业、需要全球化合规支持的高端品牌（如微软、亚马逊）。</li></ul><h4>2. Zoho CRM：亚太市场的 “性价比之王”</h4><ul><li>市场表现：全球排名第五（市场份额 5.3%），亚太地区年复合增长率达 18%，中国市场占有率连续 5 年居首，达 25.18%。</li><li>核心优势：AI 功能深度渗透，“SDR 智能体” 将线索筛选效率提升 40%，“销售教练智能体” 实时优化销售话术；2025 年新增 ABM（账户式营销）功能，支持 Line、WhatsApp 等海外社交平台集成，适配跨境业务；QuickML 低代码平台允许企业无编程构建定制化模型。</li><li>局限性：复杂生产场景的适配能力较弱，高端定制化成本较高。</li><li>适用场景：中小企业、跨境电商、需要 AI 赋能的制造业（如极氪汽车、宝马中国）。</li></ul><h4>3. Microsoft Dynamics 365：生态协同的 “一体化代表”</h4><ul><li>核心优势：与 Office 365、Teams、Outlook 等微软产品无缝集成，实现 “办公自动化 + 销售管理” 深度融合；支持 CRM 与 ERP 功能联动，打通 “订单 - 生产 - 交付” 端到端数据链路。</li><li>局限性：行业垂直场景的定制化灵活性不足，小微型企业的成本压力较大。</li><li>适用场景：广泛使用微软生态的企业（如联想、工商银行）、看重跨部门业务协同的中大型企业。</li></ul><h4>4. 其他国际主流玩家</h4><ul><li>HubSpot CRM：初创企业入门首选，免费版支持无限用户，营销自动化模块可将线索转化率提升至 9.7%，适合预算有限的初创团队。</li><li>Pipedrive：聚焦销售漏斗管理，以可视化流程追踪商机进展，核心优势是销售环节效率提升，适配销售导向的中小企业。</li><li>SAP CRM：与 SAP ERP 无缝集成，擅长 “供应链 - 销售 - 服务” 全链路协同，适合奔驰、大众等制造巨头。</li></ul><h3>三、国内主流 CRM 系统：本土适配与场景化创新的突围</h3><p>国内厂商深耕本土企业需求，在钉钉 / 企业微信协同、信创支持、行业定制化上形成优势，超兔一体云等玩家更以 “全业务一体化” 打破传统 CRM 的功能边界。</p><h4>1. 超兔一体云：全业务一体化的 “实干派”</h4><ul><li>市场积淀：21 年行业经验，服务 6 万多家企业，尤其适配工业类、工贸类企业，40% 新客户来自老客户转介绍。</li><li>核心优势：</li><li>全业务打通：国内罕见的综合业务大底座，整合 CRM、进销存、供应链、财务、生产工单等功能，实现业务和数据底层连通；</li><li>低成本客制化：支持功能白名单订阅、三级菜单自定义、工作台定制等，企业可低成本切入，实现 “大底座、快启动”；</li><li>AI 深度应用：可基于客户视图定制销售跟单智能体，嵌入 Coze 工作流，支持自然语言生成工作流、电话录音 AI 分析等；</li><li>本土生态适配：多端覆盖 Web、App、小程序、RPA 插件，支持华为倡导的双重指挥系统模式，适配国内企业组织架构。</li><li>局限性：全球化合规与跨境业务支持能力弱于 Salesforce、Zoho；纯线上营销场景的功能丰富度不及 HubSpot。</li><li>适用场景：工业 / 工贸类企业、需要 “CRM + 进销存 + 生产” 一体化管理的中小企业、看重成本控制与灵活定制的本土企业。</li></ul><h4>2. 纷享销客：本土 CRM 的 “头部标杆”</h4><ul><li>市场地位：连续 5 年稳居国内 CRM 市场占有率首位（2024 年市占率 18.7%），累计融资超 30 亿元。</li><li>核心优势：覆盖 “营销获客 - 销售跟进 - 售后服务” 全流程闭环管理，支持从线索到回款的全生命周期追踪；无缝对接钉钉、企业微信、用友 / 金蝶 ERP 系统，打破数据孤岛；PaaS 平台支持快消、医疗等行业的定制化需求。</li><li>局限性：高端版定价较高，小型企业性价比不足；生产模块的适配能力较弱。</li><li>适用场景：国内中大型企业、需要跨部门协同的快消 / 医疗行业（如元气森林、振德医疗）。</li></ul><h4>3. 神州云动（CloudCC）：高合规需求的 “安全之选”</h4><ul><li>核心优势：支持 SaaS + 私有化混合部署，满足等保三级、GDPR 双合规要求；17 年企业级实施经验，提供 “销售 - 生产 - 交付” 全链路订单追踪，服务奔驰、ABB 等超 10000 家企业。</li><li>适用场景：高端制造、金融、医疗等对数据安全要求极高的行业。</li></ul><h4>4. 销售易（Neocrm）：AI + 大数据的 “成长型选手”</h4><ul><li>核心优势：双中台架构（业务中台 + 数据中台），融合 AI 与大数据技术，实现营销自动化、销售预测、客户服务全流程智能；为 IT 高科技、教育行业提供定制化方案。</li><li>局限性：系统稳定性略逊于行业头部玩家，大规模部署的适配能力有待验证。</li><li>适用场景：快速扩张的成长型企业、IT 高科技与教育行业客户。</li></ul><h3>四、2025 年 CRM 核心技术趋势：从 “功能覆盖” 到 “价值匹配”</h3><ol><li>AI 智能化成为核心竞争力：Gartner 数据显示，2025 年 AI 功能在 CRM 选型中的权重占比提升至 25%，智能预测、自动流程、话术优化成为标配，超兔的 AI 跟单智能体、Zoho 的销售教练智能体均是典型代表。</li><li>模块化与一体化两极分化：一方面，模块化订阅模式兴起，企业可按需选择功能降低成本；另一方面，像超兔这样的 “全业务一体化” 系统受青睐，解决多系统数据割裂问题。</li><li>本土化服务升级：国内企业更看重 2 小时故障响应、行业场景定制、信创适配，神州云动的本地化实施团队、超兔的专业客服均满足这一需求。</li><li>低代码 / 零代码定制普及：降低企业定制化门槛，Zoho 的 QuickML、超兔的自定义引擎均支持无编程或低编程的功能调整。</li></ol><h3>五、国内外主流 CRM 系统对比表（2025 最新版）</h3><table><thead><tr><th>系统名称</th><th>核心优势</th><th>局限性</th><th>适用企业类型</th><th>参考成本（单用户 / 月）</th><th>特色功能</th></tr></thead><tbody><tr><td>Salesforce</td><td>全球化生态、AI 能力强、合规覆盖广</td><td>成本高、国内访问慢、本地化弱</td><td>跨国企业、高端品牌</td><td>约 1100 元</td><td>Einstein AI、AppExchange 生态</td></tr><tr><td>Zoho CRM</td><td>高性价比、AI 功能全、跨境适配好</td><td>复杂生产场景适配弱</td><td>中小企业、跨境电商、制造业</td><td>约 300-800 元</td><td>SDR 智能体、ABM 营销、QuickML 低代码</td></tr><tr><td>Microsoft Dynamics 365</td><td>微软生态协同、CRM+ERP 联动</td><td>行业定制化弱、小微企业成本高</td><td>微软生态用户、中大型企业</td><td>约 800-1500 元</td><td>Office 集成、全链路数据打通</td></tr><tr><td>超兔一体云</td><td>全业务一体化、低成本客制化、稳定性高</td><td>全球化支持弱、纯线上营销功能不足</td><td>工业 / 工贸企业、中小企业、本土企业</td><td>约 500-750 元</td><td>CRM + 进销存 + 生产联动、AI 跟单智能体</td></tr><tr><td>纷享销客</td><td>本土生态全、全流程闭环、行业定制强</td><td>高端版成本高、生产模块弱</td><td>国内中大型企业、快消 / 医疗行业</td><td>约 500-1200 元</td><td>钉钉 / 企业微信集成、全生命周期管理</td></tr><tr><td>神州云动</td><td>双合规支持、数据安全强、实施经验丰富</td><td>性价比一般、小型企业适配弱</td><td>高端制造、金融、医疗行业</td><td>约 600-1300 元</td><td>混合部署、全链路订单追踪</td></tr><tr><td>销售易</td><td>AI + 大数据、双中台架构、行业方案专</td><td>稳定性一般、大规模部署适配弱</td><td>成长型企业、IT 高科技 / 教育行业</td><td>约 400-1000 元</td><td>销售预测、学员跟进定制</td></tr><tr><td>HubSpot CRM</td><td>免费版无用户限制、营销自动化强</td><td>功能深度不足、付费版升级成本高</td><td>初创团队、预算有限企业</td><td>免费 - 约 500 元</td><td>无限免费用户、社交媒体集成</td></tr><tr><td>Pipedrive</td><td>销售漏斗可视化、线索追踪高效</td><td>功能单一、无生产 / 财务联动</td><td>销售导向型中小企业</td><td>约 200-500 元</td><td>可视化流程、商机进展追踪</td></tr></tbody></table><h3>六、企业选型指南：按 “需求画像” 精准匹配</h3><ol><li>跨国业务 + 高合规需求：优先选择 Salesforce，其全球化生态与合规能力行业领先，适合预算充足的高端品牌。</li><li>跨境电商 + 中小企业：Zoho CRM 是最优解，高性价比与跨境适配能力兼顾，AI 功能可提升转化效率。</li><li>微软生态深度用户：Microsoft Dynamics 365 可实现办公与业务无缝协同，减少系统切换成本。</li><li>国内中大型企业 + 跨部门协同：纷享销客或神州云动，前者擅长本土生态整合，后者聚焦数据安全与合规。</li><li>工业 / 工贸企业 + 一体化需求：超兔一体云是核心推荐，CRM、进销存、生产工单的底层连通的，完美适配 “销售 - 生产 - 交付” 全流程。</li><li>初创团队 + 预算有限：HubSpot CRM 免费版可满足基础需求，营销自动化功能助力快速获客。</li><li>销售导向 + 效率优先：Pipedrive 的可视化销售漏斗能精准追踪商机，提升销售转化效率。</li></ol><h3>七、结语：选型的本质是 “需求与价值的匹配”</h3><p>2025 年 CRM 市场的竞争，已从 “功能比拼” 转向 “价值适配”。国际巨头的优势在于全球化与技术生态，本土玩家的核心竞争力是场景适配与成本控制，而超兔一体云的 “全业务一体化” 模式，为工业、工贸类企业提供了差异化选择。</p><p>企业选型时，无需盲目追求 “功能最全” 或 “品牌最响”，应围绕四大核心维度决策：企业规模（初创 / 中小 / 大型）、行业特性（工业 / 快消 / 跨境等）、业务需求（单一销售管理 / 全流程一体化）、预算范围。选对 CRM 不是终点，而是以系统为支点，撬动客户运营效率与业务增长的起点。</p>]]></description></item><item>    <title><![CDATA[【交通标志识别系统】Python+Ten]]></title>    <link>https://segmentfault.com/a/1190000047446348</link>    <guid>https://segmentfault.com/a/1190000047446348</guid>    <pubDate>2025-12-03 16:09:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、介绍</h2><p>交通标志识别系统，基于TensorFlow搭建Resnet50卷积神经网络算法，通过对58种常见的交通标志图片数据集进行训练，最后得到一个识别精度较高的模型，然后搭建Web可视化操作平台。</p><p><strong>技术栈</strong>：</p><ul><li>项目前端使用Html、CSS、BootStrap搭建界面。</li><li>后端基于Django处理逻辑请求</li><li>基于Ajax实现前后端数据通信</li></ul><p><strong>选题背景与意义</strong>：<br/>在智能交通系统蓬勃发展的当下，交通标志的精准识别对于保障行车安全、提升交通管理效率意义重大。然而，传统识别方法在面对复杂多变的交通环境时，往往存在识别精度不足、效率低下等问题。为此，我们开展交通标志识别系统项目，采用前沿技术，基于TensorFlow搭建Resnet50卷积神经网络算法，利用58种常见交通标志图片数据集训练，以获取高精度识别模型。同时，为方便用户操作，我们还运用Html、CSS等技术搭建Web可视化平台，实现便捷交互。</p><h2>二、系统效果图片展示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446350" alt="图片" title="图片"/></p><h2>三、演示视频 and 完整代码 and 安装</h2><p>地址：<a href="https://link.segmentfault.com/?enc=qpJnptzhfNwtKL3P6vF1DA%3D%3D.A23mVaEfGqjEzejeKRN%2BN69JCaV3f67CUDyO%2Ba%2F7a3o%3D" rel="nofollow" target="_blank">https://ziwupy.cn/p/qBWZim</a></p><h2>四、卷积神经网络算法介绍</h2><p>卷积神经网络（CNN）是一种专门为处理具有网格结构数据（如图像）而设计的深度学习算法。它通过卷积层自动提取图像的局部特征，利用池化层降低数据维度、减少计算量并增强特征的鲁棒性，最后通过全连接层对提取的特征进行分类或回归。CNN的独特之处在于其局部连接和权重共享机制，极大减少了参数量，提高了训练效率，尤其擅长图像识别、目标检测等计算机视觉任务。</p><pre><code class="python">import tensorflow as tf
from tensorflow.keras import layers, models

# 构建简单CNN模型
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
</code></pre><p>上述代码用TensorFlow构建了一个简单的CNN模型，包含两个卷积层和池化层，用于提取图像特征，后接全连接层进行分类。该模型适用于手写数字识别等简单图像分类任务，通过调整网络结构和参数，可拓展至更复杂的图像识别场景。</p>]]></description></item><item>    <title><![CDATA[开源视频生成新标杆：美团LongCat ]]></title>    <link>https://segmentfault.com/a/1190000047446376</link>    <guid>https://segmentfault.com/a/1190000047446376</guid>    <pubDate>2025-12-03 16:08:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今年涌现了大量新的视频模型，可以说 2025 年是视频建模真正主导公众对 AI 技术兴趣的第一年。随着 Sora 2 的普及，这一点变得越来越清晰。得益于 OpenAI 的一系列移动应用程序，获取视频生成工具的可能性与普及度达到了前所未有的高度。但闭源模型并非本文的重点，而这些模型的开源竞争实际上正变得比以往任何时候都更加令人印象深刻。</p><p>今年早些时候，HunyuanVideo 和 Wan2.1 以其令人难以置信的保真度、相对低廉的成本和公开可用性震撼了开源世界。这种发展趋势仍在继续，Wan 的新版本不断发布，其他竞争对手也纷纷入场。</p><p>在本文中，我们将介绍最新公开可用的视频模型：美团的 LongCat Video。这个出色的视频模型是进入我们工具箱的最新、最棒的开源工具，我们很高兴在本教程中展示如何从今天开始，利用 DigitalOcean 生成你自己的视频。</p><p>请跟随我们，简要了解 LongCat Video 的工作原理，以及一个展示如何在配备 NVIDIA GPU 的 DigitalOcean GPU Droplet 上设置并开始运行 LongCat Video 的教程。</p><p><strong>本文的核心要点</strong></p><ul><li>LongCat Video 是目前可用的、对标 Sora 2 的最佳开源竞争者。</li><li>用户可以使用 DigitalOcean GPU Droplets，通过自己的提示词和硬件，生成质量可与 Sora 2 媲美的视频。</li><li>运行 LongCat Video 至少需要 NVIDIA GPU 系统上具备 80GB 的显存，但可以扩展到多 GPU 设置以加快生成速度。</li></ul><h3>LongCat Video：概述</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446379" alt="" title=""/></p><p>LongCat Video 的精妙之处在于其核心架构。这是因为他们非常巧妙地设计了一个单一管道来处理多项任务，包括文本到视频、图像到视频和视频延续。他们认为，所有这些任务都应被定义为视频延续，即模型根据给定的一组前置条件帧来预测未来的帧。</p><p>为了实现这一点，他们采用了相对标准的扩散变换器架构，并配有单流变换器块。“每个块包含一个 3D 自注意力层、一个用于文本条件的交叉注意力层，以及一个带有 SwiGLU 的前馈网络。为了进行调制，他们利用了 AdaLN-Zero，其中每个块都包含一个专用的调制 MLP。为了增强训练稳定性，在自注意力模块和交叉注意力模块中都应用了 RMSNorm 作为 QKNorm。此外，还采用了 3D RoPE 作为视觉标记的位置编码。”这种统一的架构允许使用相同的模型设计来完成三种视频任务中的任何一种。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446380" alt="" title="" loading="lazy"/></p><p>该模型在一个包含来自各种不同来源、视频类型和主题的大量标注视频语料库上进行了训练。我们可以在上图中看到训练数据中包含主题的大致聚类。对于这些数据，他们采用了强大的数据预处理和数据标注流程来进行文本标注。首先，收集并处理数据以确保没有重复项、裁剪黑边以及进行视频过渡分割。目前，他们没有详细讨论其数据来源。</p><p>LongCat Video 的闪光点及其与竞争对手的不同之处在于其长视频生成能力和高效的推理策略。对于长视频生成，LongCat-Video 原生地在视频延续任务上进行了预训练，使其能够生成长达数分钟的视频，而不会出现色彩漂移或质量下降。在实践中，这得益于训练策略的鲁棒性，其中对视频延伸的关注在训练结果中得以体现。至于高效的推理策略，我们指的是从粗到精的策略。在 LongCat Video 中，“视频首先生成为 480p、15fps，随后精炼至 720p、30fps。”（来源）此外，他们实现了一种新颖的块稀疏注意力机制，有效地将注意力计算量减少到标准密集注意力所需计算量的 10% 以下。这一设计显著提高了高分辨率精炼阶段的效率。最后，他们使用了一种新颖的组相对策略优化策略来进一步优化其流程。他们有效地采用了带有多个奖励的强化学习范式。</p><p>总而言之，美团 LongCat Video 是一个功能强大的视频生成和延续模型，作为一个工具，它既多功能又强大。他们认为其模型与最先进的竞争对手相比具有竞争力，我们希望通过本文展示如何在 DigitalOcean 的硬件上使用它。</p><h3>LongCat Video 演示：如何在 DigitalOcean GPU Droplet 上运行 LongCat Video</h3><p><strong>1、设置 Gradient ​GPU</strong>​<strong>​ Droplet</strong></p><p>要开始运行 LongCat Video，我们建议从<a href="https://link.segmentfault.com/?enc=gI%2FTErQjWk7WAU0NftWXmQ%3D%3D.jQg5xKcIW0SWRniizOm%2BMwiLuE7tOGF4RwH5Z3SB%2FzEUv7wLg1%2B8NbvfGWC4XEPwoXjvWxHioYLPip7JODL0aQ%3D%3D" rel="nofollow" target="_blank">创建一个 DigitalOcean Gradient GPU Droplet 云服务器</a>开始。这些 GPU Droplet 配备了运行本教程所需的 GPU 资源。我们建议至少使用单卡 NVIDIA H200 GPU，但拥有 8xH100 或 8xH200 设置的用户将看到更快的视频生成效率。DigitalOcean 的 GPU 资源不仅比 AWS、GCP 等大型云平台更加实惠，GPU Droplet 的性能比 Vast.ai 等 GPU 租赁平台更加稳定，而且 GPU 可选型号比 Linode 更加丰富。</p><p>要启动 GPU Droplet 并设置运行此演示的环境，我们建议使用本教程入门：<a href="https://link.segmentfault.com/?enc=ACCVPfVABVWPWxiHEF0jzA%3D%3D.zsZmhWKWCg9Y2KP8xyfLZGVmPRVS%2BjaJQMLhnxLTnajnj%2BIv4oJcAU75medJ5nuaNmKmZ5vYruFDDznvWHzRSA%3D%3D" rel="nofollow" target="_blank">https://blog.aidroplet.com/tutorials/do-gpu-jupyter-dl-setup/</a></p><p>可以从本地终端访问正在运行的 GPU Droplet 后，请继续跟着下一部分步骤来操作。</p><p><strong>2、为 LongCat Video 设置远程环境</strong></p><p>通过 SSH 连接到远程机器后，导航到你想要工作的目录。进入目录后，粘贴以下代码开始设置你的环境。</p><pre><code>git clone https://github.com/meituan-longcat/LongCat-Video
cd LongCat-Video
pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
pip install ninja
pip install psutil
pip install packaging
pip install flash_attn==2.7.4.post1
pip install -r requirements.txt</code></pre><p>完成上述步骤后，我们几乎可以准备开始了。现在要做的就是下载模型检查点！使用以下代码片段来完成：</p><pre><code>pip install "huggingface_hub[cli]"
huggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video</code></pre><p><strong>3、使用 Streamlit 应用程序生成 LongCat 视频</strong></p><p>对于演示，我们建议使用作者提供的 Streamlit 应用程序来运行视频生成。这个 Streamlit 演示使得在不同分辨率下生成视频、从静态图像生成视频以及延续视频长度变得简单。</p><p>设置完成后，我们就可以运行演示了。粘贴以下命令来运行演示。</p><pre><code>streamlit run ./run_streamlit.py --server.fileWatcherType none --server.headless=false</code></pre><p>复制 Streamlit 窗口的 URL，然后使用 Cursor 或 VS Code 的简单浏览器功能从本地访问该窗口。设置 VS Code 环境的步骤在 <a href="https://link.segmentfault.com/?enc=A%2BMUQCoCj9drU4SvdWTPxg%3D%3D.RafFz7SgdG%2BzL5hNZhGz1hEj1TCfps6ewAgGVMSEocFUsfngx8Ee7K%2FZvu5maiiXG0Mwq6PWS227ukXUDDxJNg%3D%3D" rel="nofollow" target="_blank">卓普云 aidroplet.com 的官网教程中有所概述</a>。卓普云是 DigitalOcean 中国区独家战略合作伙伴，为中国区企业客户提供商务对接与中文技术支持。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446381" alt="" title="" loading="lazy"/></p><p>上图显示了加载后的 Streamlit 演示界面。左侧有一个下拉菜单，我们可以在三个选项之间切换任务，启用蒸馏模式（将模型限制为 16 个推理步骤而非 50 个），启用超分辨率（从粗到精的上采样），以及设置生成参数。在窗口本身，我们有选项可以输入正面和负面提示词文本，并且在其他任务中，根据需要添加图像或视频。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446382" alt="" title="" loading="lazy"/></p><p>当我们运行生成器时，视频输出会显示在右侧！一定要尝试各种不同的提示词主题，来真正测试这个模型！</p><p>美团 LongCat Video 是一个真正强大的视频生成范式。我们对其多功能性和能力都印象深刻。在测试中，它确实是 Wan2.1 和 HunyuanVideo 向前迈出的一步，并且与 Wan2.2 等最先进的模型不相上下。不仅如此，统一的框架使得这个流程比竞争对手更加令人印象深刻和多功能。我们期待未来围绕 LongCat Video 发展出一个生态系统。</p>]]></description></item><item>    <title><![CDATA[从“数据孤岛”到“全域流转”：Kaiwu]]></title>    <link>https://segmentfault.com/a/1190000047446401</link>    <guid>https://segmentfault.com/a/1190000047446401</guid>    <pubDate>2025-12-03 16:08:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>关于数据分发</strong></h2><p>数据分发，简而言之，就是将数据从源头高效、可靠地传输到一个或多个指定目的地的过程。其核心目的在于，确保需要数据的人或系统能够在正确的时间、以恰当的形式获取到准确的数据，实现数据的共享与同步。</p><h2><strong>为什么需要数据分发？</strong></h2><p><strong>• 实时数据共享</strong></p><p>集团各部门协同合作，需确保所有数据部门获取最新数据，避免因数据延迟导致的业务决策偏差，如供应链协同场景、IoT 设备运维、营销自动化等。</p><p><strong>• 云边端数据协同</strong></p><p>终端设备产生的海量原始数据按需（全量或者预处理）同步至云端分布式集群，进行全局数据的建模、预测、分析。</p><p><strong>• 实时计算与告警</strong></p><p>实时将变更数据主动推送出去，客户端根据业务需求自由订阅数据，进行数据的实时计算、展示与告警。</p><h2><strong>设计理念与架构</strong></h2><h3><strong>1、 核心设计理念</strong></h3><p>KaiwuDB 数据分发以"<strong>数据价值最大化</strong>"为核心设计原则，在源端与多目标端之间搭建高效、灵活、可靠的流转桥梁，以最小化传输带宽、时间成本实现最大传输效率，发挥最大数据价值。</p><p><strong>• 实时数据驱动，赋能业务即时决策</strong></p><p>以 <strong>"数据实时流转为业务价值服务"</strong> 为核心，确保数据从产生到分发的延迟控制在毫秒级。让业务能基于最新数据做即时决策，将数据的 "时间价值" 最大化。</p><p><strong>• 业务场景导向，降低实时数据集成门槛</strong></p><p>围绕 <strong>"让实时数据集成更简单"</strong> 的理念，设计了开箱即用的订阅发布能力：无需用户开发复杂的自定义同步逻辑，通过配置化的方式即可实现跨集群、跨系统的数据实时同步，同时兼容多种技术生态（时序引擎、消息队列、业务应用），让不同业务场景能快速复用该能力。</p><p><strong>• 云边端一体化</strong></p><p>以 "<strong>本地计算 + 按需同步</strong>"为核心，边缘侧过滤冗余数据、云端汇聚核心信息，适配工业物联网、车联网等分布式场景。</p><h3><strong>2、 数据分发流程</strong></h3><p><img width="723" height="306" referrerpolicy="no-referrer" src="/img/bVdne8A" alt="" title=""/></p><p>KaiwuDB 数据分发流程图</p><p><strong>• 核心层</strong></p><p>借助 CDC（Change Data Capture，变更数据捕获）技术，精准捕获数据变更，支持基于 SQL 的订阅规则定义（如 WHERE vibration \&gt; 阈值的异常数据过滤）。</p><p><strong>• 传输层</strong></p><p>支持 DDL（数据定义语言，用于数据库结构变更）和业务数据同步分发：</p><p>• 发送至 Kafka（分布式消息队列），供第三方应用消费主题数据，支持多端异步数据消费场景；</p><p>• 传递至 KaiwuDB 集群 B 的数据订阅模块，实现跨集群的数据同步。</p><h2><strong>核心功能特性</strong></h2><h3><strong>1、 多维度数据订阅</strong></h3><p>• 提供全量初始化 + 增量同步双模式；</p><p>• 支持基于 SQL 条件的行过滤和列级投影同步。</p><h3><strong>2、 高可靠传输机制</strong></h3><p>• 基于 Raft 协议的多副本机制，单点故障后仍可从其它正常节点继续同步；</p><p>• 边缘节点断网时本地缓存数据，恢复后自动续传，保障弱网场景可用性。</p><h3><strong>3、 断点续传</strong></h3><p>定期保存已处理日志的时间点，在故障恢复时从断点继续同步，避免数据重复或遗漏。</p><h3><strong>4、 元数据智能映射</strong></h3><p>自动识别源库表结构变更（如字段增删），同步更新目标端 Schema，保持上下游数据结构一致性。</p><h3><strong>5、 高效传输</strong></h3><p>通过实时捕获数据的增量变更，仅传输变化部分，提升数据同步效率。</p><h2><strong>应用场景与核心价值</strong></h2><h3><strong>1、 部分典型应用场景</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446403" alt="" title="" loading="lazy"/></p><h3><strong>2、 核心业务价值</strong></h3><p><strong>• 提升实时决策</strong></p><p>• 打破设备厂商数据壁垒，实现跨部门协同优化，实时数据共享打破信息孤岛，生产、运维、供应链等部门可基于同一数据源协同决策；</p><p>• 动态分析与预测，结合历史数据分析趋势并预测潜在问题，提前制定维护计划，减少非计划停机时间。</p><p><strong>• 降低系统资源消耗</strong></p><p>• 按需订阅关注数据信息，避免全量数据传输，减少 70%+ 云端传输量，带宽成本降低 30%\~50%+；</p><p>• 边缘计算预处理，进行滤波、聚合或降采样处理，降低云端计算压力。</p><p><strong>• 增强业务灵活性</strong></p><p>• 支持灵活增减数据源或订阅主题，无需重构系统架构；</p><p>• 允许第三方开发者基于实时数据流开发增值应用，加速创新并丰富业务生态。</p><p><strong>• 安全合规</strong></p><p>支持数据脱敏订阅，符合 GDPR 数据最小化原则，保障车联网等场景的数据安全。</p>]]></description></item><item>    <title><![CDATA[LazyLLM × 硅基流动：共造面向开]]></title>    <link>https://segmentfault.com/a/1190000047446414</link>    <guid>https://segmentfault.com/a/1190000047446414</guid>    <pubDate>2025-12-03 16:07:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446417" alt="" title=""/></p><blockquote>在大模型全面走向工程落地的当下，<strong>LazyLLM</strong>正式与<strong>硅基流动（SiliconFlow）</strong> 达成深度合作，共同打造面向开发者的下一代智能应用底座。借助LazyLLM的一键接入线上模型API能力，硅基流动的大语言模型、多模态模型、向量与Embedding模型、文生图模型等已经完整接入，同一套接口即可覆盖从文本到图像、从检索到生成的全链路需求。</blockquote><p>这次合作带来的不仅是<strong>更强大的RAG选型</strong>，还进一步<strong>放大了Agent能力</strong>：在LazyLLM中，开发者可以基于统一的模型接入层，灵活编排工具调用与工作流，结合对MCP等协议的支持，将检索、调用外部系统、多模型路由、长程记忆等能力封装为可协作的智能体网络。</p><p>对于开发者而言，底层模型与算力的复杂度被彻底“藏”在LazyLLM+硅基流动这套组合之下——你只需聚焦业务逻辑，就能搭出既有强RAG能力、又有高扩展Agent能力的AI应用，从原型验证一路平滑升级到生产级部署。</p><hr/><p><strong>LazyLLM</strong></p><blockquote>LazyLLM是由商汤LazyAGI团队开发的一款开源低代码大模型应用开发工具，提供从应用搭建、数据准备、模型部署、微调到评测的一站式工具支持，以极低的成本快速构建AI应用，持续迭代优化效果。</blockquote><hr/><h2>一、<strong>API申请和环境配置</strong></h2><h3><strong>（一）账号注册</strong></h3><ul><li><p>注册硅基流动账号</p><p>（点击注册：<a href="https://link.segmentfault.com/?enc=fmoQM%2B6wVi7Etmqlv%2BX%2Bhw%3D%3D.NivzJ7L1y1u2jUSKXiYZxMbjJRAUXiHeCTC9Bw25SoTbkIZzvOYoMdhpylSlXRcxUCvmYO2qQ0g7DCy0TksZ8goQKYgb0PlBclWpYk3SmDEjBsg9Fptpd5dx71cTGO8R09I3W6DgYQguqF4hTX2MG%2F8hEvnWdnmuC93aObE%2F8MY%3D" rel="nofollow" target="_blank">https://account.siliconflow.cn/zh/login?redirect=https%3A%2F%2Fcloud.siliconflow.cn&amp;invitation=TR9Ym0c4）</a></p></li><li><p>进入控制台，获取APIkey</p><p>（获取方式：<a href="https://link.segmentfault.com/?enc=yApuYjD14mJ%2BR7Iss4p45A%3D%3D.uDjG7jc%2BxC6zIikqRxBABcaqQRrKZ4UPgRJEoLGKRkC8QMKApEHzfnEZlE%2FRwuyy32no9xKWQAshE0N0ZttHc0MQt48vNGxfwAfIplHKfbYlSADwDCFV%2BadlkOYJnpni15jNcYjgGa51qsuz815FCg%3D%3D" rel="nofollow" target="_blank">https://account.siliconflow.cn/zh/login?redirect=https%3A%2F%2Fcloud.siliconflow.cn%2Faccount%2Fak%3F）</a></p></li></ul><h3><strong>（二）环境配置</strong></h3><p>参考网页：快速开始-LazyLLM。</p><p>（<a href="https://link.segmentfault.com/?enc=r21U1UoO7cg3pm1BuoXhvw%3D%3D.ECCmdTm7fJetNOFU%2FerYOnjoGDeqSL8Lmj9uvtwCuaunOXnpS%2FLij04V9ajBAmOO" rel="nofollow" target="_blank">https://docs.lazyllm.ai/zh-cn/stable/）</a></p><hr/><h2><strong>二、API使用测试</strong></h2><h3><strong>（一）设置环境变量</strong></h3><p>可以使用以下命令设置对应的环境变量。或从代码中显示给入:</p><pre><code>export LAZYLLM_SILICONFLOW_API_KEY=&lt;申请到的api key&gt;
</code></pre><h3><strong>（二）实现对话和图片识别</strong></h3><h4><strong>1. 文本问答演示</strong></h4><p>填好api_key后，运行下面代码可以迅速调用模型并生成一个问答形式的前端界面：</p><pre><code>import lazyllm
from lazyllm import OnlineChatModule,WebModule
api_key = 'sk-' #替换成申请的api
# # 测试chat模块
llm = OnlineChatModule(source='siliconflow', api_key=api_key, stream=False)
w = WebModule(llm, port=8846, title="siliconflow")
w.start().wait()
</code></pre><p>我们询问“什么是LazyLLM”，运行结果如下:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446418" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446419" alt="" title="" loading="lazy"/></p><h4><strong>2. 多模态问答演示</strong></h4><p>在输入中通过lazyllm_files参数传入一张图片，并询问图片的内容，就可以实现多模态的问答。</p><pre><code>import lazyllm
from lazyllm import OnlineChatModule
api_key = 'sk-' #替换成申请的api
llm = OnlineChatModule(source='siliconflow', api_key=api_key, model='Qwen/Qwen2.5-VL-72B-Instruct')
print(llm('你好，这是什么？', lazyllm_files=['your_picture.png']))
</code></pre><p>这里我们使用这个图片测试多模态问答</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446420" alt="" title="" loading="lazy"/></p><p>命令行中输出结果：</p><blockquote><p>你好！这是一只小猫。它看起来非常可爱，毛茸茸的，眼睛大大的，背景是模糊的色彩，突出了小猫的细节。这样的图像通常能让人们感到温暖和愉快。你想了解更多关于小猫的信息吗？</p><p>（lazyllm）→LazyLLMgit:(main)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446421" alt="" title="" loading="lazy"/></p></blockquote><h3><strong>（三）实现文生图和文生语音</strong></h3><p>使用OnlineMultiModalModule进行文生图和文生语音，运行后会输出生成的文件路径</p><pre><code>import lazyllm
from lazyllm import OnlineMultiModalModule
api_key = 'sk-xxx'
# 测试文生图 fuction=text2image
llm = OnlineMultiModalModule(source='siliconflow', api_key=api_key, function='text2image')
print(llm("生成一个可爱的小狗"))
# 测试文生语音 function=tts
llm = OnlineMultiModalModule(source='siliconflow', api_key=api_key, function='tts')
print(llm("你好，你叫什么名字", voice='fnlp/MOSS-TTSD-v0.5:anna'))
</code></pre><p>运行结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446422" alt="" title="" loading="lazy"/></p><p>生成的语音如下：</p><p>| tmpck44zfds.mp3 | 55.13KB | 2025-10-2723:13 |</p><p>（语音链接：<a href="https://link.segmentfault.com/?enc=x0QCEUHS3PVObwUZXoF4Lw%3D%3D.gphqFoe7NNXOPGT8eRuBo1Y8lFmBOasuD1hmWNICCQ7Z8YW9k4Lcdl7orH2hUzo6bPz9JnJ9ElbVJoXSsI10nzf%2FM2sE1fO8d7Ac6myouDZNqAUakqExdKUD5nlGwbEL" rel="nofollow" target="_blank">https://ones.ainewera.com/wiki/#/team/JNwe8qUX/share/7fy5a6mk/page/FUcz8wKs/）</a></p><h3><strong>（四）10+行代码实现知识库问答</strong></h3><h4><strong>1. 实现Eembed和Rerank功能</strong></h4><p>运行下面代码，使用OnlineEmbeddingModule进行向量化嵌入；设置type='rerank'调用重排序模型。</p><pre><code>import lazyllm
from lazyllm import OnlineEmbeddingModule
api_key = 'sk-'

#测试embed模块
llm = OnlineEmbeddingModule(source='siliconflow', api_key=api_key)
print(llm("苹果"))

#测试rerank模块
llm = OnlineEmbeddingModule(source='siliconflow', api_key=api_key, type='rerank')
print(llm(["苹果", ['苹果','香蕉','橘子']]))
</code></pre><p>向量化的结果如下：</p><pre><code>[-0.0024823144, -0.0075530247, -0.013154144, -0.031351723, -0.024489744, 0.009692847, 0.008086464, -0.037946977, 0.013251133, -0.046675995, -0.011390155, -0.011111312, 0.016779112, 0.054168403, 0.04849454, 0.014742341, 0.02341074, -0.015542501, 0.059939254, -0.024223024, 0.0065467632, -0.041244607, -0.022925794, -0.024804957, 0.006752865, -0.047548898, -0.03685585, 0.0513557....，-0.070656545, -0.01997975, 0.023398615, 0.008735079]
</code></pre><p>词相似性分数如下：</p><pre><code>[{'index': 0, 'relevance_score': 0.9946065545082092}, {'index': 2, 'relevance_score': 0.014802767895162106}, {'index': 1, 'relevance_score': 0.0004139931406825781}]
</code></pre><h4><strong>2. 知识库导入</strong></h4><p>我们使用中国古典文籍作为示例知识库，下载后放在database文件夹。</p><p>（示例数据集下载链接：<a href="https://link.segmentfault.com/?enc=514NpqBGE7WYejc9yYo2Iw%3D%3D.2uFM09iyVjJCEy%2B1vt38MnyvvYPe9C%2B%2BQOLls44616Pbb2PwbKo3iJoNkG1RcOJqbGbAkixrLFIX3eb1BCxKMrt9D2zucaIbo%2FqXfW%2F%2F4aCj2RmQp8N0miwUhIdiTKkn" rel="nofollow" target="_blank">https://huggingface.co/datasets/LazyAGI/Chinese\_Classics\_Articles/tree/main）</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446424" alt="" title="" loading="lazy"/></p><p>首先定义embed模型，然后使用LazyLLM的Document组件创建文档管理模块，以实现知识库的导入。</p><pre><code>import lazyllm
api_key='sk-'
embed_model = lazyllm.OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
documents = lazyllm.Document(dataset_path="database", embed=embed_model)
</code></pre><h4><strong>3. 知识库检索</strong></h4><p>现在有了外部知识库，LazyLLM中使用Retriever组件可以实现检索知识库并召回相关内容。使用示例：</p><pre><code>import lazyllm
from lazyllm.tools import Retriever, Document, SentenceSplitter
api_key='sk-'
embed_model = lazyllm.OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
documents = Document(dataset_path='database', embed=embed_model, manager=False)
rm = Retriever(documents, group_name='CoarseChunk', similarity='bm25', similarity_cut_off=0.01, topk=6)
rm.start()
print(rm("user query"))
</code></pre><h4><strong>4. 知识库问答</strong></h4><p>结合上述模型、文档管理和检索模块，搭配LazyLLM内置的Flow组件进行完整的数据流搭建，完整代码如下：</p><pre><code>import lazyllm
from lazyllm import (
    OnlineEmbeddingModule, OnlineChatModule, Document, SentenceSplitter,
    Retriever, Reranker, ChatPrompter, pipeline
)
# 初始化api key和提示词
api_key = 'sk-'
prompt = """
You will play the role of an AI Q&amp;A assistant and complete a dialogue task.
In this task, you need to provide your answer based on the given context and question.
"""
# 初始化模型
embed_model = OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
rerank_model = OnlineEmbeddingModule(source="siliconflow", api_key=api_key, type="rerank")
llm = OnlineChatModule(source="siliconflow", api_key=api_key)
# 定义文档管理模块，并创建节点组
doc = Document(dataset_path="/home/xxx/database", manager=False, embed=embed_model)
doc.create_node_group(name="block", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)
doc.create_node_group(name="line", transform=SentenceSplitter, chunk_size=128, chunk_overlap=20, parent="block")
# 构建RAG pipeline（多路召回--重排--提示词拼接--大模型回答）
with pipeline() as ppl:
    with lazyllm.parallel().sum as ppl.prl:
        prl.r1 = Retriever(doc, group_name='line', similarity="cosine", topk=6, target='block')
        prl.r2 = Retriever(doc, group_name='block', similarity="cosine", topk=6)
    ppl.reranker = Reranker('ModuleReranker', model=rerank_model, output_format='content',
                            join=True) | bind(query=ppl.input)
    ppl.formatter = (lambda context, query: dict(context_str=str(context), query=query)) | bind(query=ppl.input)
    ppl.llm = llm.prompt(lazyllm.ChatPrompter(prompt, extra_keys=["context_str"]))
ppl.start()
query = "何为天道"

print(ppl(query))
</code></pre><p>可以看到RAG很好地从《道德经》等中取回了有关天道的内容，并传给大模型进行回答。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446425" alt="" title="" loading="lazy"/></p><hr/><p>更多技术内容，欢迎移步 "LazyLLM" 讨论！</p>]]></description></item><item>    <title><![CDATA[14款主流CRM一体化能力全景横评 傲视]]></title>    <link>https://segmentfault.com/a/1190000047446434</link>    <guid>https://segmentfault.com/a/1190000047446434</guid>    <pubDate>2025-12-03 16:06:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在企业数字化转型中，<strong>CRM</strong> <strong>的核心价值已从“客户管理”延伸至“全业务链路协同”</strong> ——从客户获客、合同签订，到订单履约、财务结算，各环节的无缝衔接直接决定运营效率与利润空间。本文基于<strong>客户管理、合同管理、订单管理、财务集成</strong>四大核心维度，对14款主流CRM品牌（超兔一体云、Salesforce、SAP CRM、Microsoft Dynamics 365、Oracle CX、Pipedrive、金蝶、SugarCRM、Zoho、Freshsales、HubSpot CRM、用友CRM、SuiteCRM、EC）进行深度横评，为不同场景的企业提供选择参考。</p><h2>一、对比维度说明</h2><p>本次横评聚焦“业务全链路闭环”，将四大核心维度拆解为16个子指标（见表1），覆盖从“客户到财务”的全流程能力：</p><table><thead><tr><th>核心维度</th><th>子指标</th></tr></thead><tbody><tr><td>客户管理</td><td>全生命周期覆盖、多渠道整合、AI智能、可视化工具、客资沉淀能力</td></tr><tr><td>合同管理</td><td>全流程覆盖、合规性、AI辅助、模板自定义、与订单联动能力</td></tr><tr><td>订单管理</td><td>全链路协同、多渠道处理、库存联动、状态跟踪、自动化触发能力</td></tr><tr><td>财务集成</td><td>ERP对接、业财数据联动、多货币支持、自动化凭证、风险管控能力</td></tr></tbody></table><h2>二、各维度横向对比</h2><h3>（一）客户管理：从“线索到复购”的全生命周期能力</h3><p>客户管理的核心是“精准识别需求+高效推进转化”，关键看“全流程覆盖深度”与“数据整合能力”。</p><h4>1. 核心能力对比表（表2）</h4><table><thead><tr><th>品牌</th><th>全生命周期覆盖</th><th>多渠道整合</th><th>AI智能</th><th>可视化工具</th><th>客资沉淀能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>三一客节点+五大跟单模型+客池分类</td><td>微信/广告/线下+智能表单</td><td>用户画像云图+跟进节奏提醒</td><td>客户视图+跟单时间线</td><td>与合同/订单/财务联动</td></tr><tr><td>Salesforce</td><td>销售云+服务云+营销云+数据云</td><td>多渠道线索+360度视图</td><td>AI预测+自动化任务分配</td><td>销售管道+Tableau分析</td><td>跨云数据整合</td></tr><tr><td>SAP CRM</td><td>营销+销售+服务闭环</td><td>多渠道线索+客户数据整合</td><td>市场预测+行为分析</td><td>销售漏斗+报表</td><td>ERP联动客资共享</td></tr><tr><td>Microsoft Dynamics 365</td><td>销售+营销+服务集成</td><td>Office 365+Azure</td><td>生成式AI摘要+报价生成</td><td>Power BI+Excel</td><td>微软生态数据共享</td></tr><tr><td>Oracle CX</td><td>销售+营销+服务+电商</td><td>多渠道互动+B2B数据整合</td><td>AI行为预测+合同简化</td><td>统一商务视图+云同步</td><td>跨部门数据打通</td></tr><tr><td>Pipedrive</td><td>可视化漏斗+阶段管理</td><td>移动端+线索拖拽</td><td>AI跟进提醒+商机优先级</td><td>拖拽式管道+业绩同步</td><td>轻量化客资记录</td></tr><tr><td>金蝶</td><td>全生命周期+ERP联动</td><td>财务/供应链联动</td><td>2025年AI条款校验（98%准确率）</td><td>订单追踪+业绩预测</td><td>集团级客资沉淀</td></tr><tr><td>SugarCRM</td><td>定制化全生命周期</td><td>多维度分类+历史订单</td><td>AI需求预测+行为分析</td><td>自定义字段+模块</td><td>复杂场景客资管理</td></tr><tr><td>Zoho</td><td>潜客+商机+动态联动</td><td>微信/飞书+多渠道</td><td>Zia助手+邮件分析</td><td>报表+销售管理</td><td>跨境客资同步</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“精准转化利器”</strong> 以“<strong>三一客节点</strong>”（定性+定级+定量）和“<strong>五大跟单模型</strong>”（适配不同业务场景）实现客户分层，通过“<strong>客池分类</strong>”（需求培养→有需求→成功）自动化推进生命周期。例如，线索进入系统后，先通过“用户画像云图”识别高价值客群，再用“客户视图”呈现全景信息（跟进历史、订单记录），帮助销售聚焦核心商机。 <strong>流程图：超兔客户生命周期管理逻辑</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446436" alt="" title=""/></p><pre><code>flowchart LR
    A[多渠道获客] --&gt; B[智能表单收线索]
    B --&gt; C[用户画像分级]
    C --&gt; D{高价值?}
    D --&gt;|是| E[需求培养客池]
    E --&gt; F[三一客节点评估]
    F --&gt; G[五大模型跟进]
    G --&gt; H{有需求?}
    H --&gt;|是| I[合同签订]
    I --&gt; J[订单生成]
    J --&gt; K[复购/售后]</code></pre><ul><li><strong>Salesforce：中大型企业的“全渠道引擎”</strong> 依托“<strong>销售云+服务云+营销云+数据云</strong>”的生态，实现从线索捕获到售后的全流程覆盖。例如，“数据云”可激活沉睡客户数据，“360度视图”整合销售、服务、营销的互动记录，让跨部门协同无死角。AI能力聚焦“预测客户行为”（如哪些客户会复购）和“自动化任务”（如提醒跟进），适合需要全渠道触达的中大型企业。</li><li><strong>金蝶：制造/集团企业的“客资沉淀专家”</strong> 依托ERP生态，客户信息与<strong>财务、供应链数据深度联动</strong>（如客户历史订单→库存备货→应收款提醒），解决了传统CRM“客资孤立”的痛点。2025年将升级“AI条款校验”（准确率98%），进一步提升合同合规性，适合重视“业财融合”的制造或集团企业。</li></ul><h3>（二）合同管理：从“起草到归档”的全流程合规能力</h3><p>合同管理的核心是“效率+合规”，关键看“流程覆盖度”与“AI辅助能力”。</p><h4>1. 核心能力对比表（表3）</h4><table><thead><tr><th>品牌</th><th>全流程覆盖</th><th>合规性</th><th>AI辅助</th><th>模板自定义</th><th>与订单联动</th></tr></thead><tbody><tr><td>超兔一体云</td><td>起草→审批→签订→执行→归档</td><td>自定义查重+企业简称模糊查重</td><td>智能条款匹配+多期应收拆分</td><td>10类合同模板（服务/贸易等）</td><td>合同信息自动同步订单</td></tr><tr><td>Salesforce</td><td>模板→签章→审批→履约</td><td>GDPR/CCPA等多国法规</td><td>自动化合同生成+电子签章</td><td>自定义模板+变量替换</td><td>商机→合同→订单闭环</td></tr><tr><td>SAP CRM</td><td>开发→验证→修订→提交</td><td>行业合规模板</td><td>合同租约管理+销售分析</td><td>标准化模板+定制</td><td>合同与销售分析联动</td></tr><tr><td>Microsoft Dynamics 365</td><td>生成→审批→归档</td><td>微软合规框架</td><td>生成式AI摘要+报价转合同</td><td>Office模板+AI生成</td><td>订单触发合同创建</td></tr><tr><td>Oracle CX</td><td>报价→订单→合同</td><td>B2B合规流程</td><td>生成式AI简化合同+协作流程</td><td>统一模板+复杂订单拆分</td><td>统一商务视图联动</td></tr><tr><td>金蝶</td><td>起草→审批→履约→归档</td><td>内置行业合规条款</td><td>2025年AI条款校验（98%）</td><td>12类标准化模板</td><td>订单→合同→收付款联动</td></tr><tr><td>SugarCRM</td><td>定制化全流程</td><td>法务系统集成</td><td>风险条款预警+需求匹配</td><td>自定义模板+变量</td><td>合同与订单状态同步</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“合同闭环工具”</strong> 支持<strong>10类合同类型</strong>（服务、贸易、非标定制等），全流程电子化（从起草到归档）。例如，“智能应收拆分”可根据合同约定（如3期付款）自动计算每期金额，“企业简称模糊查重”解决了“同一家企业多个简称”的问题，避免重复签约。与订单的联动设计（合同信息自动同步至订单），彻底消除“合同与订单不一致”的痛点。</li><li><strong>Oracle CX：复杂B2B场景的“合同简化专家”</strong> 针对B2B企业“合同流程长、协作难”的痛点，提供<strong>“统一商务视图”</strong>（整合报价、订单、合同），支持复杂订单拆分（如拆分多个子合同）和跨部门协作。生成式AI可简化合同起草（如自动填充客户信息、条款），降低法务审核成本，适合需要频繁签订复杂合同的B2B企业。</li><li><strong>金蝶：财务精细化企业的“合同合规管家”</strong> 内置<strong>12类标准化合同模板</strong>（如制造、建筑），2025年升级的“AI条款校验”可识别风险条款（如“无理由退款”），准确率达98%。与财务系统的联动（合同→收付款计划→发票），确保“合同约定=财务执行”，适合重视“合同履约与财务一致”的企业。</li></ul><h3>（三）订单管理：从“生成到交付”的全链路协同能力</h3><p>订单管理的核心是“协同+效率”，关键看“与采购/库存的联动”和“状态跟踪能力”。</p><h4>1. 核心能力对比表（表4）</h4><table><thead><tr><th>品牌</th><th>全链路协同</th><th>多渠道处理</th><th>库存联动</th><th>状态跟踪</th><th>自动化触发</th></tr></thead><tbody><tr><td>超兔一体云</td><td>订单→采购→库存→交付</td><td>全渠道订单整合</td><td>订单生成→自动算采购量→匹配供应商</td><td>实时状态+超发预警</td><td>签约/开票/发货触发应收</td></tr><tr><td>Salesforce</td><td>订单→供应链→ERP</td><td>商业云多渠道处理</td><td>与制造云联动→库存实时同步</td><td>销售管道+Tableau跟踪</td><td>订单触发供应链协同</td></tr><tr><td>SAP CRM</td><td>订单→后台交易系统</td><td>多渠道订单整合</td><td>库存变动→财务凭证自动生成</td><td>订单状态+报表分析</td><td>订单触发采购计划</td></tr><tr><td>金蝶</td><td>订单→合同→进销存</td><td>电商/线下订单整合</td><td>订单→库存检查→采购计划</td><td>实时状态+业绩预测</td><td>订单触发收付款计划</td></tr><tr><td>Zoho</td><td>订单→飞书审批→财务</td><td>多渠道订单同步</td><td>库存查询→缺货提醒</td><td>移动端状态跟踪</td><td>订单触发审批流程</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“订单协同引擎”</strong> 实现“订单→采购→库存→交付”的全链路协同：订单生成时，自动计算所需采购量（如“订单要100个产品，库存有30个，需采购70个”），并匹配最佳供应商（根据价格、交付周期）。“超发预警”功能（如客户账期内超订单发货），避免企业资金风险。与财务的联动（签约/开票/发货触发应收），确保“货出去，钱进来”的节奏。</li><li><strong>金蝶：制造企业的“订单-库存联动专家”</strong> 与进销存模块深度集成，<strong>订单生成自动触发合同创建</strong>，同时检查库存（如“订单要500个零件，库存有200个，需采购300个”），并生成采购计划。实时订单状态跟踪（如“已发货”“已收款”）与业绩预测（如“本月订单额预计100万”），帮助制造企业精准把控生产节奏。</li></ul><h3>（四）财务集成：从“订单到凭证”的业财一体化能力</h3><p>财务集成的核心是“数据一致性+自动化”，关键看“与ERP的对接”和“凭证生成效率”。</p><h4>1. 核心能力对比表（表5）</h4><table><thead><tr><th>品牌</th><th>ERP对接</th><th>业财联动</th><th>多货币支持</th><th>自动化凭证</th><th>风险管控</th></tr></thead><tbody><tr><td>超兔一体云</td><td>柠檬云等财务平台</td><td>应收+开票+回款三角联动</td><td>支持</td><td>联动生成凭证</td><td>超发预警+账期控制</td></tr><tr><td>Salesforce</td><td>SAP/Oracle等ERP</td><td>订单→财务报表+Tableau分析</td><td>支持</td><td>订阅制自动账单</td><td>信用度监控+发货控制</td></tr><tr><td>SAP CRM</td><td>SAP ERP深度整合</td><td>库存→财务凭证自动生成</td><td>支持</td><td>模块联动生成凭证</td><td>账期预警+应收管控</td></tr><tr><td>金蝶</td><td>金蝶ERP天然对接</td><td>合同→收付款→发票联动</td><td>支持</td><td>业财数据一致+自动凭证</td><td>预算管控+成本分析</td></tr><tr><td>Zoho</td><td>第三方财务系统</td><td>多货币结算+飞书审批</td><td>支持（跨境业务）</td><td>未来深化财务联动</td><td>无明确风险管控</td></tr><tr><td>用友CRM</td><td>用友ERP深度对接</td><td>订单→财务实时同步</td><td>支持</td><td>自动化凭证+报表</td><td>信用度控制+应收预警</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><p><strong>超兔一体云：中小微企业的“业财自动化工具”</strong> 依托“<strong>应收+开票+回款三角联动</strong>”，实现从订单到凭证的全自动化：</p><ul><li>订单生成→根据合同约定触发应收（如“签约触发30%应收”）；</li><li>开票→自动关联订单与应收；</li><li>回款→自动匹配应收与订单；</li><li>凭证生成→一键读取CRM数据（出库、回款、开票），匹配货、款、票信息，生成可视化凭证（支持二次编辑），并推送至柠檬云等财务平台。 <strong>流程图：超兔业财一体化逻辑</strong></li><li><img referrerpolicy="no-referrer" src="/img/remote/1460000047446437" alt="" title="" loading="lazy"/></li></ul></li></ul><pre><code>flowchart LR
    A[合同签订] --&gt; B[订单生成]
    B --&gt; C[库存出库]
    C --&gt; D[触发应收]
    D --&gt; E[开票]
    E --&gt; F[回款]
    F --&gt; G[联动生成凭证]
    G --&gt; H[推送财务系统]</code></pre><ul><li><strong>金蝶：集团企业的“财务精细化引擎”</strong> 天然对接金蝶ERP，<strong>合同→收付款→发票</strong>的联动设计，确保业财数据100%一致。例如，合同约定“3期付款”，系统自动生成收付款计划，发票开具时关联合同与订单，财务人员无需手动核对。“预算管控”功能（如“部门月度费用不超过10万”），帮助集团企业实现财务精细化管理。</li></ul><h2>三、综合能力雷达图与选择建议</h2><p>基于四大维度的核心能力，我们对各品牌进行<strong>10分制评分</strong>（1=无能力，10=顶尖），并绘制雷达图（表6）：</p><table><thead><tr><th>品牌</th><th>客户管理</th><th>合同管理</th><th>订单管理</th><th>财务集成</th><th>综合得分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td><td>8</td><td>8</td><td>9</td><td>34</td></tr><tr><td>Salesforce</td><td>10</td><td>9</td><td>9</td><td>9</td><td>37</td></tr><tr><td>金蝶</td><td>8</td><td>9</td><td>9</td><td>10</td><td>36</td></tr><tr><td>Oracle CX</td><td>9</td><td>9</td><td>8</td><td>9</td><td>35</td></tr><tr><td>Microsoft Dynamics 365</td><td>9</td><td>8</td><td>8</td><td>8</td><td>33</td></tr><tr><td>Pipedrive</td><td>8</td><td>7</td><td>7</td><td>6</td><td>28</td></tr><tr><td>Zoho</td><td>8</td><td>7</td><td>7</td><td>8</td><td>30</td></tr><tr><td>用友CRM</td><td>8</td><td>8</td><td>8</td><td>9</td><td>33</td></tr></tbody></table><h3>最终选择建议</h3><ol><li><strong>中小微企业、全业务一体化需求</strong>：<strong>超兔一体云</strong>（性价比高，覆盖客户→合同→订单→财务全流程，功能贴合中小微场景）。</li><li><strong>中大型企业、全渠道管理</strong>：<strong>Salesforce</strong>（云生态强大，全流程覆盖，适合需要跨部门协同的中大型企业）。</li><li><strong>制造/集团企业、业财融合</strong>：<strong>金蝶</strong>（ERP联动，财务精细化，适合重视“客资 - 库存 - 财务”联动的制造企业）。</li><li><strong>复杂B2B场景、跨部门协同</strong>：<strong>Oracle CX</strong>（统一商务视图，简化合同流程，适合B2B企业的长周期订单）。</li><li><strong>中小销售团队、高透明度</strong>：<strong>Pipedrive</strong>（可视化漏斗，跟进提醒，适合需要快速推进商机的销售团队）。</li><li><strong>跨境业务企业、多货币结算需求</strong>：<strong>Zoho</strong>（支持多货币结算，与丰富的Zoho及第三方应用无缝集成，适合跨境业务财务协同）。</li></ol><p>在企业数字化转型的浪潮中，CRM系统作为企业全业务链路协同的核心工具，其选择至关重要。企业在挑选CRM品牌时，不能仅仅关注品牌的知名度和市场份额，更要深入剖析自身的业务需求、发展阶段以及预算限制等因素。无论是中小微企业追求的高性价比和全业务一体化，还是中大型企业对全渠道管理和跨部门协同的需求，亦或是制造/集团企业强调的业财融合，都能在众多的CRM品牌中找到合适的解决方案。希望本文的深度横评和选择建议，能为企业在CRM选型过程中提供有价值的参考，助力企业提升运营效率，实现可持续发展。</p>]]></description></item><item>    <title><![CDATA[从“建模型”到“管业务”：一位开发者眼中]]></title>    <link>https://segmentfault.com/a/1190000047446446</link>    <guid>https://segmentfault.com/a/1190000047446446</guid>    <pubDate>2025-12-03 16:05:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名在数字孪生领域摸爬滚打多年的应用开发者。过去几年，我和团队接过不少数据中心运维管理的项目，从最初的“炫酷大屏”到后来的“实用工具”，踩过不少坑，也积累了一些心得。今天，我想抛开那些宏大的概念，以一个同行、一个实践者的身份，和大家聊聊，在数据中心这个精密复杂的领域，我们究竟需要什么样的工具，才能把数字孪生从“可视化看板”变成真正的“业务驾驶舱”。</p><h2>一、 第一道坎：如何让运维团队自己“搭场景”？</h2><p>我们最早的项目，常常陷入一个怪圈：业务部门提需求 -&gt; 我们找三维建模师建模型 -&gt; 反复沟通修改 -&gt; 交付后业务逻辑一变，模型又得大改。周期长、成本高，最关键的是，运维团队离实际的场景构建太远，他们的业务洞察无法快速体现在三维世界里。<br/>后来我们意识到，问题的核心在于<strong>工具链的断裂</strong>。理想的工具，应该能让熟悉数据中心机房布局、设备型号的运维专家，即使不会专业建模软件，也能主导场景的搭建，就例如“图观”端渲染的场景编辑器。这需要两个关键支撑：<br/><strong>1. 一个“开箱即用”的资产库，而不是从零开始建模。</strong><br/>想象一下，如果一个工具内置了从服务器机柜、UPS、精密空调、PDU，到地板、桥架、线缆等数据中心全要素的高精度模型库，并且材质、规格可调，那会节省多少时间？我们的做法是，让运维工程师直接在“图观”场景编辑器的模型库里，像搭积木一样，通过拖拉拽，快速还原出数据中心的真实物理布局。这不仅仅是快，更是保证了模型的行业规范性和专业性。<br/><img width="587" height="330" referrerpolicy="no-referrer" src="/img/bVdmqxd" alt="" title=""/><br/><strong>2. 支持“专业级”微调，满足苛刻的细节要求。</strong><br/>当然，完全标准化不可能覆盖所有情况。当遇到特殊品牌设备或需要展示内部结构时，工具必须能无缝导入运维方提供的专业GLB等格式模型。更关键的是，要能对模型进行深度的“化妆”——调整PBR物理材质（让金属更有光泽、玻璃更通透）、设置关节动画（比如模拟柜门开合、风扇转动）。这样，才能在保证效率的同时，不牺牲视觉表现力和准确性。<br/><strong>价值点提炼</strong>： 将场景构建的主导权部分交还给业务专家，实现从“项目制交付”到“持续化运营”的转变。运维团队可以根据设备变更、布局调整，随时自行更新孪生场景，让数字世界与物理世界保持同步的成本降到最低。</p><h2>二、 灵魂所在：如何让冷冰冰的数据在三维空间里“说话”？</h2><p>模型建得再漂亮，如果只是静态的“雕塑”，价值也有限。数字孪生的灵魂在于<strong>数据驱动</strong>。在数据中心，这意味着要将动环监控（温湿度、漏水、烟感）、设备运行（CPU负载、功耗、流量）、资产管理（设备位置、生命周期）等多源、异构的数据流，与三维空间中的具体对象精准关联并直观呈现。<br/>这里有几个我们总结的实用技巧：<br/><strong>技巧1：用“图层”思维管理数据可视化。</strong><br/>不要试图把所有数据一次性堆叠在屏幕上。优秀的工具应允许你创建不同的可视化图层：比如一个“热力图层”来映射机房实时的温度分布，一个“告警图层”让发生故障的设备高亮闪烁并悬浮显示详情，一个“容量图层”用三维柱图显示每个机柜的U位占用和功耗情况。这些图层可以随时开关，让运维人员在不同关注点间灵活切换。<br/><strong>技巧2：实现“所指即所得”的深度交互。</strong><br/>这是区分“高级可视化”和“业务工具”的关键。我们追求的效果是：在三维场景中点击一台服务器，旁边面板立刻显示其所有实时性能指标和历史曲线；反过来，在资产列表中选择一个设备编号，场景镜头能自动定位并高亮该设备。这一切的交互逻辑，应该能通过图形化的方式配置完成，而不是编写大量硬编码。比如，配置一条规则：“当‘空调1号’回风温度&gt;26℃时，在场景中将其模型颜色渐变为红色，并触发告警面板推送消息”。<br/><strong>价值点提炼</strong>： 通过配置化的数据联动与交互，将运维人员的业务经验（如“什么数据重要”、“数据异常该如何关联展示”）沉淀为可复用的数字孪生交互模板，真正构建起<strong>面向业务的、可操作的分析环境</strong>，而不仅仅是事后查看的报表。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdm7Rl" alt="" title="" loading="lazy"/></p><h2>三、 现实考量：如何平衡“电影级画质”与“全员可访问”？</h2><p>这是我们在交付时经常遇到的客户分歧：管理层想要在指挥中心大屏上看到电影级渲染效果的震撼全景，而一线运维工程师则希望在自己的办公电脑甚至平板上，能快速、流畅地接入系统进行日常巡检。<br/><strong>我们的解法是：渲染模式与业务应用解耦。</strong><br/>这意味着，我们用于构建业务逻辑（数据绑定、交互规则、UI界面）的“应用层”是一套独立的、轻量的代码或配置。而三维场景的渲染，则可以根据终端情况灵活选择“服务端流渲染”或“客户端端渲染”。<br/><strong>对于指挥中心大屏</strong>： 采用流渲染模式。所有复杂的图形计算都在高性能服务器上完成，只将渲染后的高清视频流推送到大屏。这样能保证极致的画质和稳定性，展现所有光影、材质细节。<br/><strong>对于运维桌面端/移动端</strong>： 采用端渲染模式。将轻量化的场景数据下发到终端，利用本地显卡进行渲染。这样对服务器压力极小，支持高并发访问，且在网络波动时体验更稳定。<br/><strong>价值点提炼</strong>： “一套业务逻辑，多种呈现方式”。这极大地提升了开发效率和部署灵活性。我们只需开发一次核心业务应用，就能同时满足高端演示与日常使用的不同需求，避免了为不同终端维护多套代码的困境，也降低了客户的总体拥有成本。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdm7Rm" alt="" title="" loading="lazy"/></p><h2>四、 从工具到伙伴：全链路效率与成本最优解</h2><p>数字孪生项目的成功，不仅在于核心开发工具，还依赖于一系列能提升整体生产效率的“周边武器”。例如，在构建园区级数据中心数字孪生时，我们曾为生成周边道路、建筑等环境背景而头疼。<br/>这时，如果工具体系里包含一个城市生成工具，能基于GIS数据自动生成符合比例的路网、批量生成风格化建筑，就能将我们从重复劳动中解放出来，聚焦于数据中心本体。同样，完善的API文档、调试工具和项目范例，能让我们在对接客户自有监控平台、资产管理系统时事半功倍。<br/>最后，在部署阶段，灵活的选项至关重要。项目初期或用于创新验证时，能够快速使用低成本的云服务进行部署和演示，极大降低了试错门槛。而当项目成熟、需要深度定制或对数据安全有严格要求时，又能支持完整的私有化部署，保障客户的核心利益。<br/><strong>价值点提炼</strong>： 选择一套工具，不仅是选择一个软件，更是选择一个能够伴随项目全生命周期成长、在不同阶段都能提供最优性价比解决方案的合作伙伴。它帮助团队控制初期风险，并平滑支持未来扩展。</p><h2>结语</h2><p>回顾这些年的实践，我深刻感受到，数字孪生在数据中心运维领域的价值，正从“视觉创新”走向“业务赋能”。其核心不在于渲染多么炫酷，而在于能否降低使用门槛、深化数据融合、适应多样场景、优化整体投入。<br/>我们追求的，是让运维专家能更专注于他们的专业判断，而不是被技术工具所束缚。通过一系列贴合业务场景的功能与技巧，将数字孪生真正打造成一个人人可用、时时可看、数据可联、决策可依的智能运维新基座。</p>]]></description></item><item>    <title><![CDATA[怎么开始汽车产业链数智化转型？ 月下水光]]></title>    <link>https://segmentfault.com/a/1190000047446452</link>    <guid>https://segmentfault.com/a/1190000047446452</guid>    <pubDate>2025-12-03 16:05:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在人类工业文明的演进中，汽车工业一直是国家制造业综合实力的集中体现。它不仅是一个技术密集型的集成平台，更是创新理念与先进制造模式融合的关键战场。如今，随着“电动化、智能化、网联化、共享化”的深層融合，汽车产业正迎来第三次重大变革——智能化生产。而这一变革的核心驱动力，便是以人工智能（AI）为核心的数智化转型。从福特的T型车流水线到丰田的精益生产，再到智能化生产的崭新时代，汽车产业链的数智化进程不仅改变了制造方式，也正在重新定义产业未来。<br/>无人驾驶、智能座舱、动态排产、柔性生产是这场变革的典型标志，而其背后的关键支撑，则是AI技术的全面赋能。自2025年起，数智化已从单纯的选项转变为汽车产业竞争中的必修课，尤其在全球制造业加速升级的背景下，汽车产业链数智化扮演着越来越重要的角色。谁能率先在这一领域实现突破，谁就能在全球价值链的顶端占据先机。<br/>供应链的协同与高效一直被视为汽车制造领域的瓶颈，传统模式下的信息滞后与制造流程脱节问题，往往导致资源浪费与生产效率低下。广域铭岛作为汽车产业链数智化转型的领军企业，推出的“工厂大脑”系统正在打破这种局面。通过整合主机厂、供应商、物流等多方数据节点，广域铭岛的系统实现了供应链全生命周期的实时监控与优化，极大地提升了供应链弹性与响应速度。<br/>例如，在与深圳航盛集团的合作中，广域铭岛帮助后者搭建了一套高协同性、智能化的供应链管控体系，从数字基座建设到供应商动态管理，逐步形成了端到端的闭环能力。更为重要的是，广域铭岛还在第二阶段规划中引入工业AI智能体，对企业全经营流程进行AI预测与优化，从而实现“零缺陷”的品质目标。这种技术赋能，不仅降低了企业的运营成本，还为汽车品牌全球化发展注入了新活力。<br/>对于广域铭岛而言，推动汽车产业链的数智化并非终点，而是构筑未来制造业基础的起点。其Geega工业互联网平台基于工业AI和物联网技术，打造了一套完整的智能制造解决方案。从原材料采购到整车下线，广域铭岛不仅提供了自动化执行工具，还引导企业将制造经验与AI算法深度融合。<br/>其“智能预检+系统”通过动态追踪与智能预警，帮助企业在生产环节实时洞察数据；在冲压工艺中，其模具智能管理系统将设备数据转化为优化算法，预判故障并降低停机时间；在仓储物流中，基于数字孪生的AGV调度系统更是让物料管理达到了前所未有的精准与高效。<br/>广域铭岛这种场景穿透式的技术能力，解决了传统AI大模型应用于工厂场景的适配难题——将老师傅的经验转化为可迭代的数字知识，为企业从“经验驱动”走向“数据智能驱动”提供了切实可行的路径。在实际工业落地中，某装配厂应用其智慧物流系统后，物流效率实现显著提升，呆滞物料率下降，整体交付能力提高30%。这正是汽车产业链数智化转型的缩影，也是广域铭岛帮助企业跨越转型之路阻碍的有力证明。<br/>当传统制造模式面对新型市场需求时，产业链上下游的协同程度在数智化模型下实现了前所未有的提升。广域铭岛在峰会上展示了其最新的“汽车数字化工厂”理念，要素如协同研发、智能生产、全链路关务管家等，使汽车生产企业能够在统一家策略下有效部署资源，实现敏捷响应与高效管理。<br/>而随着汽车产业链全球化部署的推进，广域铭岛的技术输出能力还覆盖了多个行业，例如新能源电池、家电制造等，展现出其在汽车产业链数智化转型上极强的可复制性与通用性。凭借其成熟的解决方案与落地经验，广域铭岛正逐步成为中国乃至全球制造业数智化的桥梁与推手。</p>]]></description></item><item>    <title><![CDATA[数字孪生：国防航天领域智能指挥决策的“智]]></title>    <link>https://segmentfault.com/a/1190000047446492</link>    <guid>https://segmentfault.com/a/1190000047446492</guid>    <pubDate>2025-12-03 16:04:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在国防航天这一关乎国家安全与战略优势的核心领域，运营管理的复杂性与日俱增。从航天发射场的全流程管控，到国防设施的态势感知与应急指挥，再到大型装备的全生命周期运维，系统日益庞杂，数据源空前多元，决策压力巨大。传统的管理模式与信息呈现方式，已难以满足对全局实时、精准、协同洞察的迫切需求。<br/>近年来，一项关键技术——数字孪生，正悄然成为破解这些难题的“金钥匙”。而其中我们发现一款数字孪生平台—孪易IOC，它并非简单的三维可视化，而是一套构建在数据融合、场景构建、智能分析与协同处置之上的完整技术体系，正在国防航天行业的多个关键场景中得到深入应用与验证，展现出颠覆性的价值。</p><h2>一、 应对多源异构数据：构建全域感知的“数据基石”</h2><p>国防航天系统的数据生态极为复杂：万千物联网传感器实时回传装备状态参数，各业务系统数据库记录着任务流程与资源信息，空天地海多维监测网络产生海量遥感与侦察数据，不同制式、不同密级的系统间存在数据壁垒。<br/><strong>核心价值点</strong>：成功的数字孪生实践表明，其首要价值在于构建了一个强大的<strong>统一数据融合引擎</strong>。它能够以标准化、安全可靠的方式，无缝接入并治理这些多源、异构、海量的数据。无论是通过适配MQTT等物联网协议实时采集设备数据，还是通过API与既有业务系统（如ERP、MES、指挥系统）深度对接，亦或是处理来自不同平台的GIS、遥感数据，该体系都能实现数据的清洗、关联与实时同步。<br/><strong>案例印证</strong>：在某大型航天发射场，通过部署数字孪生平台，成功将发射塔架数以万计的传感器数据、气象监测数据、任务规划数据与安防监控视频流统一集成。这使得在虚拟孪生场景中，每一个阀门、每一根线缆的状态都与物理世界实时对应，为发射前全系统健康状态评估提供了唯一、可信的“数据真相源”，奠定了精准决策的基石。</p><h2>二、 构建业务化三维场景：从“看见”到“洞见”的关键跃升</h2><p>对于国防航天而言，许多业务本质与空间位置、设备形态、环境态势强相关。传统的二维图表或分散的视频画面，难以直观呈现全局关联与空间逻辑。<br/><strong>核心价值点</strong>：数字孪生提供了从宏观战场环境、基地全域，到微观车间、单台装备的多层次、高保真、可交互的三维可视化能力。平台能够融合倾斜摄影、GIS地图、高精度BIM与装备三维模型，构建起与真实世界1:1映射的虚拟空间。更重要的是，场景中的每一个“孪生体”（如卫星、发射车、厂房、管线）都不是静态模型，而是与后台实时业务数据绑定的动态实体。<br/><strong>案例印证</strong>：在某国防重点设施安全管控项目中，利用数字孪生技术构建了涵盖周边地形、建筑结构、安防设备（周界、摄像头、门禁）的完整三维场景。运维人员不仅能“俯瞰”全局，还能快速定位任一报警点，并立刻在三维场景中调取该点关联的实时视频、设备状态、巡检记录和处置预案，实现了从被动接收告警信息到主动、直观、关联性洞察的根本转变。<br/><img width="640" height="314" referrerpolicy="no-referrer" src="/img/bVdmQxT" alt="" title=""/></p><h2>三、 赋能深度分析与协同处置：驱动决策闭环的“智慧内核”</h2><p>可视化是手段，决策支持才是目的。数字孪生的真正威力，在于利用空间上下文与融合数据，进行过去难以实现的深度分析与模拟推演。<br/><strong>核心价值点</strong>：成熟的数字孪生体系集成了丰富的专业分析工具与协同指挥模块。例如：<br/><strong>空间分析</strong>：进行通视分析以优化监测点位布局，模拟电磁环境对通信的影响，演练疏散路径与物资投送路线。<br/><strong>模拟仿真</strong>：对航天器在轨故障进行数字复现与处置推演，对极端天气下基地运行进行韧性评估。<br/><strong>事件闭环处置</strong>：当发生异常告警（如设备故障、入侵检测）时，系统可自动定位、关联分析、触发预案，并一键启动跨部门的协同任务派发、资源调度与视频会商，形成“监测-预警-决策-处置-复盘”的全流程数字化闭环。<br/><strong>案例印证</strong>：在大型航空航天装备的运维保障中，数字孪生平台接入了装备历史运行数据与实时传感数据。当系统分析发现某部件性能参数出现衰退趋势时，不仅能在三维模型上精准定位，还能自动调用历史相似案例、维修手册，并模拟不同维修方案对整体系统的影响，辅助保障人员制定最优维修策略，极大提升了装备的战备完好性与保障效率。</p><h2>四、 保障系统灵活演进：随业务共同成长的“生命力”</h2><p>国防航天业务需求与技术迭代迅速，一套固化的系统很快会面临挑战。数字孪生体系的长期价值，在于其<strong>高度的可配置性与可扩展性</strong>。<br/><strong>核心价值点</strong>：优秀的平台提供强大的后台配置中心，允许用户根据业务变化，自行调整孪生体属性、数据绑定规则、分析模型与告警阈值。同时，它提供从零代码拖拉拽搭建应用到低代码/API深度开发的全套工具链，使技术团队能够基于平台核心能力，快速构建全新的、高度定制化的业务模块（如专用的任务规划模拟器、特殊的后勤保障看板），保护初始投资，让数字孪生系统真正成为一个能够伴随组织业务持续进化、不断赋能的核心支撑平台。</p><h2>结语</h2><p>综上所述，在国防航天这一尖端领域，数字孪生已超越概念，成为驱动智能化升级的务实引擎。其价值并非单一功能的炫技，而在于构建了一个数据融合力强、场景表现力与交互性高、分析工具专业且紧贴业务、同时具备高度灵活性与生命力的完整技术栈。它正帮助越来越多的单位，将复杂的物理世界与业务运行，转化为可直观感知、可深度分析、可协同指挥的数字镜像，从而在瞬息万变的形势下，赢得决策先机，筑牢安全基石。</p>]]></description></item><item>    <title><![CDATA[汽车工艺优化的未来发展趋势如何？ 月下水]]></title>    <link>https://segmentfault.com/a/1190000047446532</link>    <guid>https://segmentfault.com/a/1190000047446532</guid>    <pubDate>2025-12-03 16:03:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在汽车制造业的迭代进程中，工艺优化已成为推动效率提升与质量管理创新的核心驱动力。面向2025年，随着市场需求对高性能、高安全性和环保性的全面升级，传统的生产模式显然无法满足未来的挑战。作为这一领域的先行者，广域铭岛凭借其工业智能体解决方案，正在帮助行业全面实现汽车工艺优化的智能化转型。<br/>汽车冲压工艺作为整车制造的基石，其质量直接影响后续工序的稳定性和整车性能。当前，行业正从依赖人工经验的设备管理向数据驱动的精细化控制迁移。广域铭岛研发的GQCM模具智能管理APP，融合了IIOT技术和AI算法，构建出冲压产线的数字孪生体。通过实时采集冲次数据和设备状态信息，系统能够在实体生产前预演工艺波动，并据此优化参数配置。例如，领克成都工厂仅用该系统便将换模时间压缩至原来40%，同时显著降低材料废品率，这种基于数字主线的工艺优化逻辑改变了传统车间依赖试错经验的生产方式。<br/>焊装工艺的复杂性使其成为工艺优化的重点难点，超3000个焊点的质量稳定性往往是整车质量的关键指标。数据显示，在焊装的智能化质量管控中，广域铭岛开发的GQCM系统通过5G传输网络实时监测20余项焊接参数，其机器学习模型能够以0.02%的虚焊率完成质量预判。而另一项技术突破——二維總成接口分析，则结合设备级与产线级数据完成了焊点力学性能建模，使整个系统的质量控制维度从被动响应进化为主动预防。<br/>总装工艺的智能化优化是确保整车出厂质量的最后一道关口。广域铭岛提供的GQCM拧紧管理系统实现了对每个紧固点质量参数的实时监控，这种高频采集和智能分析往往能在装配完成前精准识别问题。某新能源车企应用该系统后，其拧紧工艺合格率从98%提升至99.98%，整个车间的装配返工率下降了56%。更为惊人的是，该系统建立的电子质量档案功能使某品牌模具的异常处理时间直接缩短了35%。<br/>值得注意的是，广域铭岛不仅在单个工艺上实现突破，更建立了工艺间的协同优化框架。其三大核心技术——数字主线映射、质量闭环算法与模块化部署方案，本质上是对汽车制造系统的深度重塑。这种优化架构通过横向融合和纵向贯通的工业智能体理念，将经验数据转化为可供量化分析的知识图谱。<br/>当前制造业面临碳中和与数字化的双重使命，工艺优化的创新空间正在被技术边际不断拓张。以广域铭岛的实践为例，如果深入解析GQCM系统的数据闭环流程——从更than 3000个传感器采集到数百兆数据流，再通过自学习算法完成工艺调优——就能清楚看到工业智能体的不可替代性。毕竟，当最后一英寸的车身覆盖件、最后一个焊点，都由算法而非人工决定工艺参数时，汽车工艺优化才真正告别了试错时代。<br/>在未来发展路径上，广域铭岛的技术创新已展现出三个显著的趋势：其一是将全套拧紧设备集成接入可解释AI平台，实现系统预警溯源；其二是构建跨工艺的质量建模能力，对共性质量问题进行全局优化；其三则是将所有工业知识标准化封装，产生可搬运的工业APP产品。这些创新正在变被动应对为主动创新，随着技术的深化应用，汽车工艺优化必将在生产柔性化和质量一致性两大维度迎来更大的突破。</p>]]></description></item><item>    <title><![CDATA[NeurIPS 2025 Spotlig]]></title>    <link>https://segmentfault.com/a/1190000047446537</link>    <guid>https://segmentfault.com/a/1190000047446537</guid>    <pubDate>2025-12-03 16:03:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>NeurIPS 2025 Spotlight！跨模态重识别革命！东北大学等 MDReID 图像信息智能匹配</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446539" alt=" " title=" "/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446540" alt=" " title=" " loading="lazy"/></p><p>论文标题：<em>MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification</em></p><p>作者团队：东北大学、厦门大学、新加坡国立大学</p><p>发布时间：2025年10月27日</p><p><a href="https://link.segmentfault.com/?enc=FUPEDreH%2B7F9vYlzELME2w%3D%3D.y5f6Qr0qzLpbDqhBvIJUXvbjD3okBJlIvv7r%2FXcA8JCWwelKk6GMa%2BWytotOs88B" rel="nofollow" target="_blank">👉一键直达论文</a></p><p><a href="https://link.segmentfault.com/?enc=g0a%2BT95Aix0LdF6CVo%2BO6A%3D%3D.8GgHP8VsDDIU35Erskiw05QEAGBByuy73tYySXSug%2B7n5CZ8ICsYAt2YK9EfW7B9c4GJ%2FRI7g9I1TpXrv%2FUu826joC6D5LPv7%2BYd7JbcNoJHU0T2UFw8tIvFPDzB5UUTbtmOn%2Bfh4pml6IJg2nMy6e29z%2BuCPKG0TtRzfLNPh3E%3D" rel="nofollow" target="_blank">👉Lab4AI大模型实验室论文阅读</a></p><p>✅Lab4AI平台提供AI导读和AI翻译等工具，辅助论文阅读。</p><p>想象一下：警察想要通过监控录像找到一个嫌疑人。但是，不同监控摄像头的类型可能完全不同——有的拍的是普通的彩色照片（RGB），有的是黑白但能夜间看清的（NIR），还有的是能感知热量的热成像（TIR）。这就带来了一个难题：如果用一张彩色照片（RGB）去热成像（TIR）照片里找人，传统系统可能就失灵了。这篇论文就是为了解决这个“张冠李戴”的实际问题。它提出了一个叫 MDReID​ 的新方法，核心思想非常巧妙，叫做 “分而治之”。</p><h3>⭐核心创新</h3><p>MDReID 认为，任何一张图像包含的信息都可以分成两种：</p><ol><li>通用特征：这是物体最本质的信息。比如一个人的体型、姿势、背包的形状。这些信息无论用什么摄像头拍，都应该差不多。</li><li>专用特征：这是某种摄像头特有的信息。如彩色摄像头能看到的衣服颜色，或者热成像摄像头能看到的身体热量分布。</li></ol><p>MDReID 的核心技术即主动把这两种信息拆分开：</p><ol><li>拆解信息：模型在分析图片时，会同时生成两组“密码”：通用特征和专用特征。对于一张彩色照片，模型既知道它里面包含的通用人体形状，也知道它特有的颜色信息。</li><li>智能对比：当需要比对两张图片时，MDReID 会进行智能匹配。专用特征只和同类型摄像头的专用特征比对（比如颜色和颜色比）。通用特征则可以跨类型自由比对（比如彩色照片里的人的体型，可以和热成像照片里的人的体型比）。</li></ol><p>通过一种特殊的“训练法则”，模型会学习让通用特征尽可能相似，同时让通用特征和专用特征尽可能不同，避免信息冗余。</p>]]></description></item>  </channel></rss>