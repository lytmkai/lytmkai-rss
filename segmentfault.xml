<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[千峰嵌入式2023-完整版 技术站999it点top ]]></title>    <link>https://segmentfault.com/a/1190000047525246</link>    <guid>https://segmentfault.com/a/1190000047525246</guid>    <pubDate>2026-01-06 21:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>《2023 千峰嵌入式开发实战指南：MCU 编程・外设驱动・工业级项目开发精讲》<br/>——从教育公平、科技自立、人文关怀与产业升级多维视角看嵌入式人才的培养价值</p><p>在“万物智联”成为现实的今天，嵌入式系统早已悄然渗透进我们生活的每个角落：从智能家电、车载电子，到工业机器人、医疗设备，再到国家电网、航空航天等关键基础设施，其背后都离不开微控制器（MCU）与底层驱动的精密协同。《2023 千峰嵌入式开发实战指南》以“MCU 编程—外设驱动—工业级项目”为脉络，不仅传授技术细节，更折射出一场关乎国家科技根基、教育转型与产业未来的深层变革。</p><p>教育维度：打破“重应用、轻底层”的失衡格局，重塑工程教育根基<br/>长期以来，高校计算机教育过度聚焦于 Web 开发、移动应用等上层软件，导致大量毕业生对硬件交互、内存管理、中断处理等底层机制缺乏基本认知。这种“空中楼阁”式的培养模式，难以支撑高端制造、芯片设计、工业自动化等国家战略领域的人才需求。</p><p>《千峰嵌入式开发实战指南》以系统化、阶梯式的内容设计，引导学习者从寄存器操作、时钟配置、GPIO 控制等基础入手，逐步掌握 UART、I2C、SPI、ADC 等外设驱动开发，并最终完成如智能温控系统、工业数据采集终端等贴近真实场景的项目。这种“从硅片到系统”的全链路训练，重建了软硬结合的工程思维，为高校教育提供了可借鉴的实践范本，也为自学群体打开了通往硬科技领域的大门。</p><p>科技维度：夯实国产芯片生态，助力关键技术自主可控<br/>当前，全球半导体产业链竞争白热化，MCU 作为“芯片中的芯片”，广泛应用于消费电子与工业控制领域。然而，国内大量嵌入式开发仍依赖国外芯片平台（如 STM32）及配套工具链，存在供应链风险与技术黑盒问题。</p><p>本指南虽以通用原理为主，但其强调的“理解芯片手册、掌握驱动抽象、适配不同硬件平台”的能力，正是构建国产芯片生态适配力的关键。当开发者具备扎实的底层开发功底，便能快速迁移至国产 MCU（如兆易创新、华大半导体、乐鑫等）平台，参与国产芯片的验证、优化与生态建设。从这个角度看，一本嵌入式教材，实则是培育国产半导体“土壤”的重要一环。</p><p>人文发展维度：技术应服务于人的安全、尊严与可持续生活<br/>嵌入式系统不同于普通软件，其失效可能直接导致物理世界的安全事故——如医疗设备误判、工业机械失控、汽车刹车失灵。因此，嵌入式开发天然带有高度的责任伦理。</p><p>《实战指南》在工业级项目讲解中，反复强调实时性保障、异常处理机制、电源管理策略与电磁兼容设计等工程规范，传递出一种严谨、敬畏、以人为本的技术价值观。它提醒开发者：你写的每一行初始化代码，都可能关系到一个工人的安全、一位患者的健康，或一个家庭的用电稳定。这种将技术精度与人文关怀相融合的教育理念，正是培养“负责任工程师”的核心所在。</p><p>经济维度：赋能制造业升级，催生“新蓝领”技术岗位<br/>中国正从“制造大国”迈向“智造强国”，而智能制造的核心在于“感知—决策—执行”的闭环，这正是嵌入式系统的主战场。无论是工厂的 PLC 控制器、物流 AGV 小车，还是农业物联网传感器、新能源充电桩，都急需大量懂硬件、会编程、能调试的复合型嵌入式人才。</p><p>本指南通过工业级项目实战，帮助学习者掌握企业真正需要的技能：如何读芯片 datasheet？如何用示波器调试通信协议？如何在资源受限环境下优化代码？这些能力使学习者能快速胜任嵌入式软件工程师、FAE（现场应用工程师）、测试验证工程师等高价值岗位。更重要的是，它为传统制造业工人、职校学生提供了向“数字新蓝领”转型的可行路径，推动劳动力结构向高技能、高附加值方向演进。</p>]]></description></item><item>    <title><![CDATA[AI 的“性格旋钮”——什么是大模型的温度？ blossom ]]></title>    <link>https://segmentfault.com/a/1190000047525315</link>    <guid>https://segmentfault.com/a/1190000047525315</guid>    <pubDate>2026-01-06 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>你有没有发现：有时候 AI 像个严谨的老教授，回答滴水不漏；有时候它又像个天马行空的艺术家，能编出一堆意想不到的情节？</p><p>这背后往往藏着一个关键参数：<strong>温度（Temperature）</strong>。</p><p>别担心，调高温度并不会让电脑“发烫”，也不是让 AI 发烧。这里的温度，更像一个控制 AI <strong>“有多敢冒险”</strong>的性格旋钮：</p><ul><li>温度低 → 更稳、更像标准答案</li><li>温度高 → 更发散、更有创意，但也更容易跑偏</li></ul><hr/><h2>一、为什么需要温度？（AI 的“填空游戏”）</h2><p>要理解温度，先看大模型是怎么说话的。</p><p>大模型生成文本的过程，近似于一种“逐字填空”的游戏：每输出一个词（token），它都会对“下一步可能出现的候选词”打分。</p><p>比如当 AI 写到：</p><blockquote>“今天天气真——”</blockquote><p>它脑内可能有这样一张“候选词打分表”（通常称为 <strong>logits</strong>）：</p><ul><li><strong>好</strong>：90 分（最稳妥）</li><li><strong>热</strong>：50 分（也合理）</li><li><strong>怪</strong>：5 分（少见但勉强能通）</li><li><strong>紫色</strong>：0 分（基本不通顺）</li></ul><p>如果 AI 每次都只选分数最高的那个词（比如永远选“好”），输出会非常稳定，但也容易变得<strong>模板化</strong>：句子没错，却缺少惊喜，像“复读机”。</p><p>于是，我们需要一种机制：在“稳妥”之外，给 AI 一点点“跳出常规”的空间——这就是温度登场的原因。</p><hr/><h2>二、温度到底做了什么？（神奇的蛋糕分法）</h2><p>在真正选词之前，模型会先把“分数”转换成“概率”，常见做法叫 <strong>Softmax</strong>。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525317" alt="" title=""/><br/>你可以把它想象成：</p><blockquote><strong>把一块蛋糕分给候选词：分数越高，分到的蛋糕越多，被选中的概率越大。</strong></blockquote><p>而<strong>温度</strong>，就像影响“怎么切蛋糕”的那把刀——它决定蛋糕分配得<strong>更偏心</strong>还是<strong>更平均</strong>。</p><h3>1）低温（T &lt; 1）：偏心切法（更保守）</h3><p>温度调低后，分配会变得更“极端”：<br/>第一名会拿走绝大多数蛋糕，其他词只剩零头。</p><ul><li><strong>结果</strong>：AI 更倾向选“最常见、最稳”的词</li><li><strong>体验</strong>：更严谨、更稳定，但也更容易“千篇一律”</li></ul><h3>2）高温（T &gt; 1）：均匀切法（更发散）</h3><p>温度调高后，蛋糕切得更平均：<br/>第一名仍然是大头，但第二、第三名也能分到明显份额。</p><ul><li><strong>结果</strong>：AI 更可能选到不那么“标准”的词</li><li><strong>体验</strong>：更有创意、更有变化，但也更容易跑题或胡编</li></ul><hr/><h2>三、温度怎么设置？（三个常见场景）</h2><p>可以把不同温度下的 AI，想象成三种不同“人格”。</p><h3>1）冰块模式（低温：0 ~ 0.3）</h3><ul><li><strong>像谁</strong>：严肃的科学家 / 数学老师</li><li><strong>适合</strong>：做数学题、写代码、严谨问答、总结归纳</li><li><strong>原因</strong>：这类任务追求确定性，“1+1=2”不需要创意</li></ul><h3>2）常温模式（中温：0.5 ~ 0.9）</h3><ul><li><strong>像谁</strong>：正常可靠的聊天伙伴</li><li><strong>适合</strong>：日常对话、写邮件、写周报、写解释说明</li><li><strong>原因</strong>：稳定之余也有一点自然变化，是最常用的平衡区间</li></ul><h3>3）火焰模式（高温：0.9 ~ 1.5 或更高）</h3><ul><li><strong>像谁</strong>：灵感爆棚的艺术家 / 头脑风暴搭子</li><li><strong>适合</strong>：写故事、写诗、创意发想、广告文案、脑洞类任务</li><li><strong>提醒</strong>：温度太高（例如 &gt;1.5）时，输出可能开始发散到不受控，甚至出现“看起来很像话但其实不太对”的内容</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525318" alt="" title="" loading="lazy"/></p><hr/><h2>四、补充：温度 vs Top-k / Top-p（它们到底有什么区别？）</h2><p>温度之外，你可能还见过两个常用的“采样参数”：<strong>Top-k</strong> 和 <strong>Top-p</strong>。它们和温度一样，都是在控制 AI 输出的随机性，但“动手的方式”不同。</p><p>你可以把它们理解成：<strong>温度在“调形状”，Top-k/Top-p 在“划范围”。</strong></p><h3>1）温度（Temperature）：调“整体概率分布”的陡峭程度</h3><ul><li><strong>温度低</strong>：概率分布更“尖”，第一名更容易被选中（更稳）</li><li><strong>温度高</strong>：概率分布更“平”，冷门词也更容易被抽到（更发散）</li></ul><p>👉 它不会删掉任何候选词，只是让“大家的概率差距”变大或变小。</p><h3>2）Top-k：只在“前 k 名”里抽</h3><p>Top-k 的规则很直白：</p><blockquote>只保留概率最高的 <strong>k 个候选词</strong>，其余一律不考虑，然后再在这 k 个里按概率抽。</blockquote><ul><li><strong>优点</strong>：简单、能防止特别离谱的词混进来</li><li><strong>缺点</strong>：k 是固定的——有时候候选词很集中，有时候很分散，固定 k 可能不够灵活</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525319" alt="" title="" loading="lazy"/></p><h3>3）Top-p（Nucleus Sampling）：只在“累计概率达到 p 的那一撮”里抽</h3><p>Top-p 更像“动态的 Top-k”：</p><blockquote>从最高概率开始往下加，直到累计概率达到 <strong>p</strong>（比如 0.9），只在这一小撮里抽。</blockquote><ul><li><strong>优点</strong>：更自适应：模型很确定时范围会自动变小；模型不确定时范围会自动变大</li><li><strong>缺点</strong>：需要理解“累计概率”的概念，但用起来通常更顺手</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525320" alt="" title="" loading="lazy"/></p><h3>怎么搭配最实用？</h3><p>很多实际系统里最常见的是：<strong>温度 + Top-p</strong></p><ul><li><strong>温度</strong>负责“敢不敢跳出最优解”</li><li><strong>Top-p</strong>负责“别跳得太离谱”</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525321" alt="" title="" loading="lazy"/></p><p>一句话记忆：</p><blockquote><strong>温度让你更有变化，Top-p/Top-k 帮你把变化圈在合理范围内。</strong></blockquote><hr/><h2>总结：掌握那个旋钮</h2><p><strong>温度不会让 AI 更聪明</strong>，它改变的是：AI 在“下一步选哪个词”时的<strong>胆量</strong>和<strong>随机性</strong>。</p><ul><li>想要更像“标准答案”？→ <strong>把温度调低</strong></li><li>想要更多惊喜和创意？→ <strong>把温度调高</strong></li></ul><p>下次你可以试试对 AI 说：</p><blockquote>“请把温度设为 1.2，给我讲一个更疯狂、更有画面感的故事。”</blockquote><p>看看它会不会带你去一趟意想不到的冒险。</p><p>本文由<a href="https://link.segmentfault.com/?enc=4BE2hBJL9ERnFowPc2xSug%3D%3D.b7tYCBoRRMPWsc1xdkZF%2FoIe8Pv8N2M65EVUglwbZVY%3D" rel="nofollow" target="_blank">mdnice</a>多平台发布</p>]]></description></item><item>    <title><![CDATA[Android 车机与 BLE 设备交互 philadelphia ]]></title>    <link>https://segmentfault.com/a/1190000047525180</link>    <guid>https://segmentfault.com/a/1190000047525180</guid>    <pubDate>2026-01-06 20:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>Android 车机与 BLE 设备交互全链路实践指南</h2><p>从广播地址、配对绑定到隐私机制的完整理解</p><p>最近在开发车机系统与无屏 BLE 设备（比如智能冰箱）的连接功能，过程中遇到了一连串看似独立、实则紧密关联的问题：为什么扫描到的地址和配对时一样？RPA 地址到底能不能看到？解绑为什么没有公开 API？回连时直接用连接成功时保存的 MAC 行不行？ADB 怎么清空配对列表？</p><p>这些问题背后其实是一套完整的 BLE 安全与隐私机制。这篇文章把整个探索过程串起来，记录下验证过的方法和踩过的坑，希望能帮到正在做类似工作的你。</p><hr/><h3>一、设备广播地址：你以为的“MAC”可能不是真 MAC</h3><p>一开始我在连接成功后将设备的MAC保存到SP中，后续APP启动或者蓝牙开启后直接根据整个mac直接去连接设备，</p><p>大致流程如图：</p><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDj" alt="connect_process" title="connect_process"/></p><p>但是有的设备是OK的，有的设备不行，后来发现不行的设备是开启了BLE Privacy机制，无法直接根据连接时返回的mac直接连接</p><p>后来发现这涉及到 <strong>BLE（Bluetooth Low Energy）隐私保护机制</strong> 的核心设计 —— <strong>RPA（Resolvable Private Address，可解析私有地址）</strong></p><p>首先看下BLE设备的广播类型</p><h4><strong>BLE 广播地址的类型</strong></h4><p>根据 BLE 规范（Core Spec Vol 6, Part B, Section 1.3），广播包中的 <strong>AdvA（Advertiser Address）</strong> 可以是以下之一：</p><table><thead><tr><th>地址类型</th><th>是否可变</th><th>是否可被解析</th><th>说明</th></tr></thead><tbody><tr><td><strong>Public Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>厂商烧录的 MAC</td></tr><tr><td><strong>Static Device Address</strong></td><td>❌ 固定</td><td>✅ 是</td><td>设备自定义的静态随机地址</td></tr><tr><td><strong>Resolvable Private Address (RPA)</strong></td><td>✅ 动态（如每15分钟换）</td><td>✅ 仅对有 IRK 的设备</td><td>隐私保护，可被配对设备解析</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>✅ 动态</td><td>❌ 否</td><td>完全匿名，无法追踪也无法连接（通常用于 beacon）</td></tr></tbody></table><blockquote>📌 大多数支持配对的智能设备（如你的冰箱）会使用 <strong>RPA</strong> 来广播，以保护用户隐私。</blockquote><h3>传统 MAC 地址的问题</h3><p>早期 BLE 设备使用 <strong>静态公开地址（Public Static Address）</strong>，比如：</p><pre><code>AA:BB:CC:11:22:33</code></pre><p>这个地址是固定的、全球唯一（理论上），但也带来严重隐私问题：</p><blockquote>🕵️‍♂️ 攻击者可以在商场、地铁等公共场所通过扫描 BLE 广播包，追踪某个设备（比如你的手机或冰箱）的行踪。</blockquote><p>为了解决这个问题，BLE 4.0 引入了 <strong>Privacy Feature（隐私特性）</strong>，允许设备使用 RPA <strong>随机变化的地址</strong>，而不是固定 MAC。</p><hr/><h3>🛡️ RPA（Resolvable Private Address）是什么？</h3><p>RPA 是一种 <strong>动态变化但可被“信任设备”识别</strong> 的地址机制。</p><h4>工作原理简述：</h4><ol><li><strong>配对（Bonding）时</strong>，双方交换一个密钥：<strong>IRK（Identity Resolving Key）</strong>。</li><li>此后，设备会定期（如每 15 分钟）生成一个新的 <strong>随机地址（RPA）</strong>，该地址由 IRK + 随机数加密生成。</li><li>只有拥有相同 IRK 的设备（即已配对设备）才能 <strong>“解析”这个 RPA，确认它来自同一个物理设备</strong>。</li><li>对未配对的第三方来说，看到的只是一个不断变化的随机地址，无法追踪。</li></ol><p>✅ <strong>优点</strong>：既保护隐私，又不影响已配对设备之间的通信</p><hr/><h3>什么是Identify Address</h3><ul><li><p>Identity Address</p><p>是设备在配对（Pairing + Bonding）过程中交换的“真实身份”，格式为：</p><ul><li>Public Address（如厂商烧录的 MAC）</li><li>或 Static Random Address（由设备制造商设定，固定不变）</li></ul></li><li>这个地址 <strong>在设备生命周期内是固定的</strong>，也是 Android 系统在 <code>BluetoothDevice.getAddress()</code> 中返回的值（对于 bonded 设备）。</li><li><strong>IRK（Identity Resolving Key）就是用来将 RPA 映射回这个 Identity Address 的。</strong></li></ul><hr/><h3>📱 Android 如何处理 RPA？</h3><ul><li><p>当你和一个支持隐私特性的 BLE 设备（如现代智能冰箱）完成 <strong>配对（bonding）</strong> 后：</p><ul><li>Android 系统会 <strong>自动保存该设备的 IRK</strong>；</li><li>即使冰箱下次广播的是一个全新的 RPA（比如 <code>D4:E5:F6:77:88:99</code>），Android 也能通过 IRK 识别出：“这是之前配对过的那台冰箱”。</li></ul></li><li><p>此时，你在代码中调用：</p><pre><code>BluetoothAdapter.getDefaultAdapter().getBondedDevices()</code></pre><p>返回的 <code>BluetoothDevice</code> 对象的 <code>.getAddress()</code> <strong>始终是配对时的“身份地址”（Identity Address）</strong>，通常是 Public 或 Static 地址（如 <code>AA:BB:CC:11:22:33</code>），<strong>而不是当前广播的 RPA</strong>。</p></li></ul><blockquote>✅ 所以：<strong>系统内部已经帮你完成了 RPA → Identity Address 的映射</strong>。</blockquote><hr/><p>这就回到了为什么直接根据扫描到的mac地址直接回连设备会失败的问题了</p><h3>❌ 为什么不要直接用存储的 MAC 字符串调用 <code>getRemoteDevice(mac)</code>？</h3><p>假设你把扫描到的 MAC（如 <code>"AA:BB:CC:11:22:33"</code>）存到 SharedPreferences，下次直接：</p><pre><code>String savedMac = prefs.getString("fridge_mac", null);
BluetoothDevice dev = adapter.getRemoteDevice(savedMac); 
dev.connectGatt(...);</code></pre><ol><li><code>getRemoteDevice(mac)</code> 仅根据地址字符串返回一个设备引用，它不保证能访问到该设备的绑定上下文（如 IRK）。如果传入的地址不是已配对设备的 Iden<code>t</code>ity Address（例如是一个 RPA），即使物理设备已绑定，系统也无法自动解析隐私地址</li><li>如果此时冰箱正在使用 RPA（比如广播地址是 <code>D4:E5:F6:77:88:99</code>），而你传入的是旧的 Identity Address（<code>AA:BB:CC:...</code>）；</li><li>Android <strong>不会自动用 IRK 去解析或关联这个 RPA</strong>，因为 <code>getRemoteDevice()</code> 不知道这个设备是否已配对；</li><li>结果：<strong>连接失败（GATT ERROR 133 或 timeout）</strong>，即使物理设备就在旁边！</li></ol><blockquote>💡 换句话说：<code>getRemoteDevice()</code> 绕过了系统的 bonding 数据库和 IRK 解析机制。</blockquote><hr/><h3>✅ 正确做法：从 <code>getBondedDevices()</code> 中查找</h3><pre><code>String savedIdentityAddress = "AA:BB:CC:11:22:33"; // 这是你配对时记录的身份地址

BluetoothAdapter adapter = BluetoothAdapter.getDefaultAdapter();
for (BluetoothDevice device : adapter.getBondedDevices()) {
    if (device.getAddress().equals(savedIdentityAddress)) {
        // ✅ 这个 device 对象是系统管理的 bonded 设备
        // 即使冰箱当前用 RPA 广播，系统也会自动解析并建立连接
        device.connectGatt(context, false, gattCallback);
        break;
    }
}</code></pre><p>这样做的好处：</p><ul><li>利用了 Android 内置的 <strong>IRK 解析能力</strong>；</li><li>无论冰箱当前使用什么 RPA，系统都能正确路由到物理设备；</li><li>连接成功率高，符合 BLE 规范。</li></ul><hr/><h3>🔧 补充建议</h3><ul><li>要记录配对时的mac地址而不是扫描到的mac地址，因为扫描的到mac可能是PRA，但是绑定时的mac一定是Identify Address</li><li>在配对完成后，<strong>记录的是 <code>device.getAddress()</code>（即 Identity Address）</strong>，这个地址在 bonding 生命周期内是稳定的；</li><li>不要尝试自己解析 RPA（除非你实现完整的 BLE Host 层，不推荐）；</li><li>如果目标设备 <strong>不支持 Privacy（即始终用 Public Address）</strong>，那么 <code>getRemoteDevice()</code> 也能工作，但为了兼容性和未来升级，仍建议走 bonded devices 路径。</li></ul><hr/><h4>✅ 总结</h4><table><thead><tr><th>方式</th><th>是否推荐</th><th>原因</th></tr></thead><tbody><tr><td><code>getRemoteDevice(savedMac)</code></td><td>❌ 不推荐</td><td>无法利用 IRK 解析 RPA，连接可能失败</td></tr><tr><td>从 <code>getBondedDevices()</code> 查找匹配地址</td><td>✅ 强烈推荐</td><td>系统自动处理 RPA，连接可靠</td></tr></tbody></table><p>所以，<strong>永远优先使用系统提供的 bonded device 对象来发起连接</strong>，而不是自己构造设备对象。这不仅是最佳实践，也是应对现代 BLE 隐私机制的必要手段。</p><p><strong>Android 在设备绑定后，所有 API 返回的都是 Identity Address（身份地址），而不是设备当前广播的地址</strong>。</p><p>BLE 设备可以广播四种地址：</p><ul><li><strong>Public Address</strong>：芯片的固定 MAC；</li><li><strong>Static Random Address</strong>：开机生成、运行期间不变的随机地址；</li><li><strong>Non-Resolvable Private Address (NRPA)</strong>：频繁变化、无法追踪的临时地址；</li><li><strong>Resolvable Private Address (RPA)</strong>：频繁变化，但持有 IRK 的设备能解析回 Identity Address。</li></ul><p>真正的 Privacy 机制 = <strong>使用 RPA 广播 + IRK 解析</strong>。</p><p>当你在车机上调用 <code>device.getAddress()</code>，如果设备已绑定，Android 会自动用 IRK 把 RPA “翻译” 成 Identity Address 再返回给你。所以你永远看不到那个变化的 RPA —— 这不是 bug，而是 Privacy 正常工作的表现。</p><blockquote>✅ 验证方法：用手机装 nRF Connect，在<strong>未绑定状态</strong>下扫描设备，隔 15 分钟看地址是否变化。如果变了，说明 Privacy 生效了。</blockquote><hr/><h3>二、RPA 地址能看到吗？怎么获取原始广播地址？</h3><p>既然系统把 RPA 隐藏了，那我们还能不能拿到真实的广播地址？</p><p>答案是：<strong>只有在未绑定状态下才可能看到</strong>。</p><p>当你调用 <code>BluetoothLeScanner.startScan()</code>，如果设备还没配对，且它广播的是 RPA，那么 <code>ScanResult.device.getAddress()</code> 返回的就是这个原始 RPA（比如 <code>D3:A1:F5:09:88:77</code>）。但一旦你完成绑定，下次再扫，系统就会直接给你 Identity Address。</p><p>所以，<strong>不要试图在绑定后获取 RPA</strong>——你不需要它。Identity Address 才是稳定的设备标识，RPA 只是空中传输的“马甲”。</p><p>如果你是在调试固件，建议用 nRF Connect 或蓝牙嗅探器抓包；如果是开发车机 App，请完全忽略 RPA 的存在，只认 <code>getBondedDevices()</code> 里的地址。</p><hr/><h3>三、回连时直接用保存的 MAC 地址行不行？</h3><p>早期我们图省事，配对成功后把设备地址存下来，下次启动直接用：</p><pre><code class="kotlin">val device = adapter.getRemoteDevice(savedMac)
device.connectGatt(...)</code></pre><p>结果某天测试新固件（启用了 RPA）时，连接直接超时失败。</p><p>原因很简单：<code>getRemoteDevice()</code> 创建的是一个“裸设备对象”，它没有 IRK，也不知道这个地址对应的是谁。而冰箱此刻广播的是 RPA，根本不在 <code>savedMac</code> 这个地址上。</p><p>正确做法是：</p><pre><code class="kotlin">val device = adapter.bondedDevices.find { it.address == savedMac }
device?.connectGatt(...)</code></pre><p>因为 <code>bondedDevices</code> 里的设备对象带着完整的绑定上下文（包括 IRK），系统能自动把 RPA 解析出来并建立连接。</p><blockquote>📌 记住：<strong>地址字符串相同 ≠ 设备对象等价</strong>。安全上下文才是关键。</blockquote><hr/><h4>2. <strong>Android 如何处理这个地址？</strong></h4><ul><li>在 <code>onLeScan()</code> 中，Android <strong>直接把广播包里的 AdvA 字段原样封装成 <code>BluetoothDevice</code> 对象的地址</strong>；</li><li>此时系统 <strong>还不知道这个设备是否已配对</strong>，也没有尝试用 IRK 去解析它（因为还没建立 bonding 上下文）；</li><li>所以：<strong><code>device.getAddress()</code> 就是原始广播地址（raw advertising address）</strong>。</li></ul><p>✅ 举例：</p><ul><li>冰箱的 Identity Address 是 <code>AA:BB:CC:11:22:33</code>（Public）；</li><li>当前广播使用 RPA：<code>D4:E5:F6:77:88:99</code>；</li><li>你在 <code>onLeScan()</code> 中拿到的 <code>device.getAddress()</code> 就是 <code>"D4:E5:F6:77:88:99"</code>；</li><li>即使你之前已经和这台冰箱配对过，<strong>扫描回调仍然返回 RPA</strong>，因为这是物理层看到的内容。</li></ul><blockquote>⚠️ 这就是为什么不能在扫描阶段存储这个地址作为设备唯一标识！</blockquote><hr/><h4>3. <strong>那系统怎么知道这是“老朋友”？</strong></h4><ul><li><p>当你调用</p><pre><code>device.createBond() 
或
device.connectGatt(...)</code></pre><p>时，Android 会：</p><ol><li>检查本地是否有该 <strong>Identity Address 对应的 IRK</strong>（即是否已配对）；</li><li>如果有，就尝试用 IRK 解析当前 RPA；</li><li>如果解析成功（RPA 能还原出已知 Identity Address），就走快速重连流程（无需重新配对）；</li><li>连接成功后，<code>BluetoothDevice.getAddress()</code> 在后续 API 调用中（如 GATT 回调、bonded devices 列表）会返回 <strong>Identity Address</strong>。</li></ol></li><li><h3>public Address和identify Address 是一回事吗?</h3></li><li><blockquote><strong>Public Address 是 Identity Address 的一种，但 Identity Address 不一定是 Public Address。</strong></blockquote><p>换句话说：</p><ul><li><strong>Identity Address（身份地址）是一个逻辑概念</strong>，用于唯一标识一个 BLE 设备；</li><li><p>它可以是以下两种之一：</p><ol><li><strong>Public Device Address</strong>（公开地址，即传统 MAC 地址）</li><li><strong>Static Random Address</strong>（静态随机地址）</li></ol></li></ul><p>所以：<br/> ✅ 所有 Public Address 都是 Identity Address，<br/> ❌ 但不是所有 Identity Address 都是 Public Address。</p><hr/><h4>1. <strong>什么是 Identity Address（身份地址）？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 核心规范）</strong>：</p><blockquote><p>The <strong>Identity Address</strong> is the address used to identify a device during the pairing and bonding process. It is either:</p><ul><li>A <strong>Public Device Address</strong>, or</li><li>A <strong>Static Random Address</strong></li></ul></blockquote><p>这个地址在设备的整个生命周期中是 <strong>固定不变的</strong>，并且会和 <strong>IRK（Identity Resolving Key）</strong> 一起在配对时交换，用于后续解析 RPA（Resolvable Private Address）。</p><hr/><h4>2. <strong>Public Device Address（公开地址）</strong></h4><ul><li>就是我们熟悉的 <strong>48-bit IEEE MAC 地址</strong>，如 <code>AA:BB:CC:11:22:33</code>；</li><li>由厂商烧录，全球唯一（理论上）；</li><li>地址的 <strong>最高有效位（MSB）为 0</strong>（即“公共地址”标志）；</li><li><strong>需要向 IEEE（通过 SIG 或直接）购买</strong></li><li>示例：<code>D0:CF:5E:xx:xx:xx</code>（很多手机/模块使用）。</li></ul><p>✅ 特点：固定、可识别、无隐私保护。</p><hr/><h4>3. <strong>Static Random Address（静态随机地址）</strong></h4><ul><li>由设备制造商或开发者设定的一个 <strong>随机生成但永不改变</strong> 的地址；</li><li><p>必须满足：</p><ul><li>最高两位为 <code>11</code>（表示是静态随机地址）；</li><li>不能是全 0 或全 1；</li></ul></li><li>示例：<code>DE:AD:BE:EF:CA:FE</code>（只要符合格式且固定即可）。</li></ul><p>✅ 特点：固定、不依赖 IEEE 分配、有一定匿名性，但仍可作为身份标识。</p><blockquote>📌 很多 IoT 设备（如低成本 BLE 模块）没有 Public Address，就用 Static Random Address 作为 Identity Address。</blockquote><hr/><h4>4. <strong>为什么需要区分？</strong></h4><p>因为 BLE 隐私机制（RPA）依赖于 <strong>Identity Address + IRK</strong> 的组合：</p><ul><li>当设备启用隐私功能时，它会用 IRK 生成 RPA 来广播；</li><li>配对设备收到 RPA 后，用本地存储的 IRK 尝试还原出 <strong>Identity Address</strong>；</li><li>如果匹配成功，就知道“这是之前配对过的那台设备”。</li></ul><p>所以，无论 Identity Address 是 Public 还是 Static Random，只要它是固定的，就能作为“身份锚点”。</p><hr/></li></ul><blockquote>💡 实际开发中，你不需要关心它是 Public 还是 Static Random —— 只需知道：<strong>这是该设备的唯一身份标识，配对后稳定不变，应该存储它。</strong></blockquote><p>------</p><p>## ✅ 总结表</p><table><thead><tr><th>概念</th><th>是否固定</th><th>是否用于配对</th><th>是否可用于长期标识</th><th>备注</th></tr></thead><tbody><tr><td><strong>Public Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>传统 MAC，IEEE 分配</td></tr><tr><td><strong>Static Random Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>设备自定义，高位为 <code>11</code></td></tr><tr><td><strong>Identity Address</strong></td><td>✅ 是</td><td>✅ 是</td><td>✅ 是</td><td>= Public 或 Static Random</td></tr><tr><td><strong>RPA（Resolvable Private Address）</strong></td><td>❌ 否（动态）</td><td>❌ 否</td><td>❌ 否</td><td>用于广播，保护隐私</td></tr><tr><td><strong>Non-resolvable Private Address</strong></td><td>❌ 否</td><td>❌ 否</td><td>❌ 否</td><td>完全匿名，通常不可连接</td></tr></tbody></table><p>------</p><p>### 🎯 开发建议</p><ul><li><strong>不要尝试解析地址类型</strong>，只需在 <code>BOND_BONDED</code> 时存储 <code>device.getAddress()</code>；</li><li>这个地址就是系统认可的 <strong>Identity Address</strong>，无论底层是 Public 还是 Static Random；</li><li>后续通过 <code>getBondedDevices()</code> 匹配该地址即可可靠连接。</li></ul><h3>四、配对弹窗是怎么来的？</h3><p>我们从来没调 <code>createBond()</code>，为什么也会弹出系统配对窗口？</p><p>后来发现，<strong>触发配对的不是你的代码，而是 GATT 特征的安全属性</strong>。</p><p>如果你的冰箱声明某个特征需要“认证后才能读写”（比如设置了 <code>AUTHEN</code> 权限），而当前连接还没加密，那 Android 在收到 <code>Insufficient Authentication</code> 错误后，会自动启动配对流程。</p><p>所以，配对弹窗其实是系统在帮你补安全课。你只需要在配对成功后重试 GATT 操作即可。</p><hr/><h3>五、配对流程</h3><p><img width="723" height="308" referrerpolicy="no-referrer" src="/img/bVdnzDV" alt="tongyi-mermaid-2026-01-06-194500.png" title="tongyi-mermaid-2026-01-06-194500.png" loading="lazy"/></p><p><strong>在标准 BLE 通信模型中，配对（Pairing）流程是由 Central（车机）发起的，但实际触发时机往往由 Peripheral（BLE设备，如冰箱）的安全需求间接驱动</strong></p><h4><strong>一、协议层面：谁“发起”配对？</strong></h4><p>根据 <strong>Bluetooth Core Specification（BLE 协议规范）</strong>：</p><ul><li><strong>Central（主设备，如车机）</strong> 负责发送 <code>Pairing Request</code>；</li><li><strong>Peripheral（从设备，如冰箱）</strong> 回复 <code>Pairing Response</code>；</li><li>后续密钥交换、确认值计算等均由 Central 主导。</li></ul><p>✅ 所以从<strong>协议动作</strong>看，<strong>配对是由 Central（车机）主动发起的</strong>。</p><hr/><h4><strong>二、应用层面：谁“触发”配对？</strong></h4><p>虽然 Central 发起配对请求，但它通常<strong>不是凭空发起</strong>，而是因为：</p><blockquote><strong>Peripheral 在 GATT 层拒绝了未加密的访问请求，从而迫使 Central 启动配对。</strong></blockquote><p>典型流程如下：</p><ol><li>车机（Central）连接冰箱（Peripheral）；</li><li>车机尝试读取一个被标记为 <strong>“需要认证”</strong> 的特征（例如 <code>read authen</code>）；</li><li>冰箱返回错误：<code>Insufficient Authentication (0x05)</code>；</li><li><strong>Android 系统检测到该错误，自动调用 <code>createBond()</code> 并发送 <code>Pairing Request</code></strong>；</li><li>配对流程启动，弹出系统弹窗或走 Just Works 模式。</li></ol><p>✅ 所以从<strong>触发原因</strong>看，<strong>是 Peripheral 的安全策略“迫使” Central 发起配对</strong>。</p><p>既然是车机发起的配对请求，那车机-Android系统怎么还能收到配对请求广播呢？</p><p>车机（App）收到的 <code>ACTION_PAIRING_REQUEST</code> 广播，<strong>不是来自远端设备的“请求”</strong>，而是 <strong>Android 系统在自己发起配对前，向 App 发出的一个“协商/干预”通知</strong>。</p><p>这个广播的目的是：让 App 有机会干预配对行为</p><p>例如：</p><ul><li>自动确认 Just Works 配对（免弹窗）；</li><li>填入预共享的 PIN 码；</li><li>拒绝某些设备的配对。</li></ul><h4>Pairing 和 Bonding 区别？</h4><p>这两个概念经常混用，但职责完全不同：</p><ul><li><strong>Pairing（配对）</strong>：协商密钥的过程（生成 LTK、IRK 等），确保本次连接加密；</li><li><strong>Bonding（绑定）</strong>：把配对成果（密钥 + 身份）存到本地，供以后复用。</li></ul><p>你可以只配对不绑定（每次连都重新确认），但不能只绑定不配对。在 Android 里，配对成功默认就会绑定，设备进入 <code>getBondedDevices()</code> 列表。</p><hr/><h3>六、怎么用代码解绑设备？</h3><p>Android 没有公开 <code>removeBond()</code> 方法，但它确实存在，只是被标为 <code>@hide</code>。通过反射调用是行业通用做法：</p><pre><code class="kotlin">fun removeBond(device: BluetoothDevice): Boolean {
    return try {
        val method = device::class.java.getMethod("removeBond")
        method.invoke(device) as Boolean
    } catch (e: Exception) {
        false
    }
}</code></pre><p>注意：</p><ul><li>需要 <code>BLUETOOTH_ADMIN</code> 权限；</li><li>解绑是异步的，必须监听 <code>ACTION_BOND_STATE_CHANGED</code> 广播，等状态变成 <code>BOND_NONE</code>；</li><li>最好加兜底：如果反射失败，引导用户去系统设置手动解绑。</li></ul><p>至于为什么不公开？主要是怕 App 滥用——比如偷偷删掉用户的耳机配对。但又不能完全禁掉，毕竟车机、IoT 设备确实需要程序化解绑。所以成了“能用，但不鼓励”的状态。</p><hr/><h3>总结：几个关键原则</h3><ol><li><strong>地址不变是正常的</strong>：那是 Identity Address，不是广播地址；</li><li><strong>RPA 不需要你关心</strong>：系统会自动处理，你只认 bonded 列表；</li><li><strong>回连必须从 <code>getBondedDevices()</code> 取设备</strong>，否则 Privacy 一开就断连；</li><li><strong>配对由 GATT 安全需求驱动</strong>，不是靠你调不调 <code>createBond()</code>；</li><li><strong>解绑用反射没问题</strong>，但要做好兼容和用户引导；</li><li><strong>Privacy 是好东西</strong>，但前提是两端都正确实现——设备要真启用 RPA，车机要会解析。</li></ol><p>BLE 看似简单，但安全和隐私细节很多。理解这套机制，才能做出既好用又安全的产品。希望这篇总结能帮你少走点弯路。</p>]]></description></item><item>    <title><![CDATA[AI智能体元年：IT服务管理行业的拐点已至 ITIL先锋论坛 ]]></title>    <link>https://segmentfault.com/a/1190000047525207</link>    <guid>https://segmentfault.com/a/1190000047525207</guid>    <pubDate>2026-01-06 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p><strong>宏观视角下的行业变革信号</strong></p><p>2025年12月13日，广州天河区的一场百人规模Meetup，或许将成为IT服务管理行业转折的标志性事件。这不是一场普通的技术分享会,而是一次行业集体焦虑的集中释放,更是一场关于未来方向的深度探讨。</p><p>当100余位来自大湾区的IT精英齐聚一堂,当四位深耕行业多年的专家倾囊相授,当"AI智能体"这个概念从PPT走向实战演练,我们看到的不仅是技术的演进,更是一个行业在时代巨变前夜的集体转身。</p><p>从更宏观的视角观察,这场活动折射出IT服务管理行业正在经历的三大深刻变革:技术范式的迁移、人才结构的重组、商业模式的重构。而这三大变革的交汇点,正是AI智能体技术的大规模应用。</p><p><img width="723" height="419" referrerpolicy="no-referrer" src="/img/bVdnvsC" alt="" title=""/></p><p><strong>行业痛点:67%的从业者还未真正触碰AI</strong></p><p>长河在现场的调研数据揭示了一个令人警醒的现实:使用AI超过100小时的IT从业者仅占33%,这意味着超过三分之二的行业从业者仍处于AI技术的观望期。</p><p><strong>这个数据背后隐藏着更深层的行业问题:</strong><br/><strong>第一,认知滞后与技术加速的矛盾。</strong><br/>当大语言模型以月为单位迭代升级,当AI智能体技术从概念走向落地,大部分IT从业者的认知仍停留在"AI是高级搜索引擎"的层面。这种认知与现实的错位,正在成为行业人才发展的最大障碍。<br/><strong>第二,技能结构与市场需求的错配。</strong><br/>传统IT服务管理强调的是系统运维、故障处理、流程优化等执行层面的技能。但AI时代需要的是提示词工程、智能体开发、人机协同设计等新型能力。这种技能代际的断层,导致大量经验丰富的从业者面临转型困境。<br/><strong>第三,投入产出与风险收益的博弈。</strong><br/>企业层面对AI技术持谨慎态度:投入巨大但效果未知,试点成功但推广困难,短期收益不明显但长期不投入又可能落后。这种矛盾心态导致行业整体的技术应用进程缓慢。</p><p>从行业发展周期理论看,IT服务管理正处于从"成熟期"向"变革期"过渡的关键节点。这个节点的特征是:传统业务模式增长乏力,新兴技术尚未形成主流,行业参与者普遍焦虑且方向不明。广州这场Meetup的火爆,恰恰说明了行业对方向指引的强烈渴求。</p><p><strong>技术演进:从自动化到智能化的质变</strong></p><p>丁振兴展示的五层智能体架构,标志着IT运维从"自动化"向"智能化"的质的飞跃。这不仅是技术层面的进步,更代表着行业对运维本质认知的深化。<br/>传统自动化解决的是"怎么做"的问题——给定明确的规则和流程,系统按部就班执行。但这种模式的局限在于:面对复杂多变的IT环境,不可能为每一种场景都预设规则。<br/>智能化解决的是"如何判断"和"如何学习"的问题——系统通过感知环境、回忆经验、规划策略、执行行动、持续优化,形成闭环。这种能力的本质是:将运维专家的认知过程数字化。</p><p><strong>从行业发展趋势看,智能运维市场正在经历三个阶段:</strong><br/>1.0阶段:工具集成时代(2010-2018)。市场以监控工具、自动化脚本、ITIL流程管理为主,各厂商提供独立产品,集成度低。<br/>2.0阶段:平台化时代(2018-2023)。市场出现一体化运维平台,打通监控、告警、处置全流程,但仍以规则驱动为主。<br/>3.0阶段:智能化时代(2024-)。市场进入AI驱动阶段,平台具备自主学习、智能决策能力,从"被动响应"转向"主动预防"。</p><p>丁振兴提到的"80%陷阱"是行业当前阶段的真实写照。这不是技术的失败,而是技术成熟度的客观反映。从Gartner技术成熟度曲线看,AI运维正处于"期望膨胀期"向"泡沫破裂期"过渡的阶段。行业需要的是理性认知,而非盲目追捧或全盘否定。</p><p>值得注意的是,乐维软件支持500+厂商、8000+设备型号、100000+指标体系的技术积累,揭示了智能运维的行业门槛:这不是一个可以靠短期投入快速突破的领域,而是需要长期技术沉淀和数据积累的系统工程。这也解释了为什么该领域至今仍是少数头部厂商主导,新进入者难以撼动的市场格局。</p><p><strong>商业模式:从人力密集到智能体驱动</strong></p><p>罗小军展示的企业业务智能体矩阵,预示着IT服务行业商业模式的根本性变革。<br/>传统IT服务的商业模式是人力密集型的:企业需要大量人员提供技术支持、系统运维、项目实施等服务,收入与人力规模直接相关。这种模式的天花板很明显:利润率受制于人力成本,规模扩张受制于人才供给。<br/>智能体驱动的商业模式是技术密集型的:企业投入研发构建智能体平台,通过智能体提供标准化服务,收入与技术能力相关而非人力规模。这种模式的想象空间更大:边际成本递减,规模效应明显,可以实现指数级增长。</p><p><strong>从行业竞争格局看,这种模式转变将带来三个层面的影响:</strong><br/>企业层面:马太效应加剧。拥有技术能力、数据积累、资金实力的头部企业将加速智能体布局,中小企业面临技术门槛高、投入产出不确定的困境,行业集中度可能进一步提升。<br/>项目层面:交付模式重构。传统的"人月成本"定价模式将被"按效果付费"模式取代。智能体处理的任务越多,单位成本越低,但前期研发投入巨大。这要求企业具备长期投入能力和风险承受能力。<br/>人才层面:需求结构变化。对执行型人才的需求下降,对架构型、创新型、复合型人才的需求上升。初级工程师岗位减少,高级架构师岗位增加,行业人才结构呈现"哑铃型"。<br/>罗小军提到的"方案撰写效率提升60倍"案例,在行业引发了广泛讨论。支持者认为这代表了生产力的革命性提升,质疑者认为这种极端案例不具普遍性。<br/>从行业实践看,效率提升的真实情况可能是:对于高度结构化、模板化的工作,效率提升可达10-50倍;对于需要创造性、判断力的工作,效率提升可能只有1.5-3倍。关键是要识别哪些工作适合用AI,哪些工作仍需人工主导。<br/>更深层的问题是:当效率大幅提升后,市场需求能否同步增长?如果需求相对固定,效率提升的结果就是人力需求下降。这是行业必须直面的结构性挑战。</p><p><strong>数据集成:老问题遇上新技术</strong></p><p>王晨光提出的集成中台方案,触及了企业数字化转型的核心痛点。系统孤岛、数据沉睡、重复劳动——这些问题存在了十几年,为何至今未能解决?<br/>从技术演进史看,每隔几年就会出现号称能解决集成问题的新技术:</p><ul><li>2000年代:企业服务总线(ESB)承诺统一集成</li><li>2010年代:微服务架构承诺松耦合集成</li><li>2020年代:集成中台+AI承诺智能化集成<br/>技术在进步,但问题仍在。根本原因在于:集成问题的本质不是技术问题,而是组织问题。<br/>不同系统背后是不同部门,不同部门有不同利益诉求。数据打通意味着权力边界模糊,流程优化意味着责任重新划分。这些组织层面的阻力,远大于技术层面的难度。</li></ul><p>AI在集成方案中的真正价值,不在于技术实现的突破,而在于降低了使用门槛。当业务人员可以用自然语言查询数据,不再依赖IT部门编写SQL,数据的流动就更加顺畅。当数据异常可以被AI自动识别和修复,数据治理的成本就大幅下降。</p><p><strong>从行业发展趋势看,集成中台市场正在从"项目制"向"产品制"转变:</strong><br/>项目制时代:每个企业的集成需求都不同,需要大量定制开发,交付周期长、成本高、可复用性低。<br/>产品制时代:通过零代码配置、智能适配、自学习优化,大部分集成场景可以通过标准产品实现,只有少数个性化需求才需要定制。<br/>这种转变的商业意义在于:集成服务从"一次性项目收入"变为"持续性订阅收入",从"劳动密集"变为"技术密集",商业模式更加健康。<br/>但挑战在于:标准产品能否真正满足企业的个性化需求?零代码配置的灵活性是否足够?AI的智能化水平能否支撑复杂场景?这些问题的答案,将决定集成中台市场的未来格局。</p><p><strong>人才市场:30%-50%岗位影响的深层解读</strong><br/>圆桌讨论中,专家们给出的"未来3-5年AI将影响30%-50%岗位"判断,在行业引发了强烈反响。这个数字是危言耸听还是客观预测?<br/>从劳动经济学角度分析,技术对就业的影响包含三个效应:<br/>替代效应:AI直接替代人工完成某些任务,导致岗位需求下降。这在重复性高、规则明确的岗位上表现明显,如初级运维工程师、基础开发人员、文档撰写人员。<br/>互补效应:AI提升人工效率,使得同样人力可以完成更多工作,进而刺激需求增长。这在咨询、架构设计、创新研发等岗位上表现明显。<br/>创造效应:AI催生新岗位、新业务、新行业,创造就业机会。如AI训练师、提示词工程师、智能体架构师等新兴岗位。</p><p>IT服务管理行业的现实情况是:替代效应在短期内更显著,创造效应在长期才能体现。这就导致了一个过渡期的阵痛:旧岗位快速消失,新岗位缓慢出现,人才供需出现结构性错配。</p><p><strong>从行业数据看,这种影响已经开始显现:</strong><br/>招聘需求变化:2024年初级运维工程师岗位需求同比下降15%,AI相关岗位需求同比上升60%。但绝对数量上,减少的岗位远多于新增的岗位。<br/>薪资结构变化:掌握AI技能的工程师薪资溢价20%-40%,传统技能工程师薪资增长停滞甚至下降。行业内部的薪资分化加剧。<br/>年龄结构变化:35岁以上的从业者转型难度更大,面临的就业压力更明显。年轻从业者因学习能力强、心态开放而适应更快。<br/>长河提出的"六个月转型路线图",在行业引发了两极分化的评价。乐观者认为这是可行的快速转型方案,悲观者认为这过于理想化。<br/>从行业人才培养实践看,六个月确实可以完成从"不懂AI"到"会用AI工具"的跨越,但要成为真正的"AI架构师",可能需要1-2年的持续实践。关键在于:</p><ol><li>明确"转型"的定义。是掌握基本工具使用,还是具备架构设计能力,还是能独立交付项目?不同层级的要求,时间投入差异巨大。</li><li>识别个人的起点。有编程基础的工程师转型更快,纯运维背景的从业者需要补充更多基础知识。</li><li>找到合适的路径。自学、培训、项目实践各有优劣,需要根据个人情况选择。</li></ol><p><strong>市场格局:巨头布局与创业机会并存</strong><br/>从更宏观的市场竞争格局看,AI智能体在IT服务管理领域的应用,正在重塑行业的竞争版图。<br/>巨头企业的布局策略:<br/>国际厂商如IBM、微软、ServiceNow,国内厂商如华为、阿里云、腾讯云,都在加速AI与IT服务管理的融合。它们的优势在于:技术积累深厚、数据资源丰富、客户基础广泛、资金实力雄厚。<br/>但巨头的劣势也很明显:组织庞大决策慢、产品标准化难以满足个性需求、对细分场景的理解不够深入。<br/>创业公司的机会空间:<br/>像乐维软件这样的专业厂商,像猛犸世纪这样的创新企业,在垂直领域、细分场景、特定行业仍有很大机会。它们的优势在于:对客户需求理解深刻、产品迭代速度快、服务响应及时、性价比高。<br/>从行业发展规律看,技术变革期往往是市场格局重塑的窗口期。那些能抓住新技术、切中真需求、建立壁垒的企业,有可能实现弯道超车。</p><p><strong>值得关注的几个趋势:</strong></p><ol><li>垂直化深耕:不追求大而全,而是在某个细分领域(如金融、医疗、制造)做深做透,建立行业壁垒。</li><li>平台生态化:不只是提供工具,而是构建开放平台,让合作伙伴、客户都能参与智能体开发,形成生态效应。</li><li>服务订阅化:从一次性项目收入转向持续订阅收入,提高客户粘性和企业估值。</li><li>开源社区化:通过开源部分核心技术,吸引开发者社区,形成技术影响力和人才聚集效应。</li></ol><p><strong>政策环境:监管与发展的平衡</strong><br/>AI技术的快速发展,也引发了监管层面的关注。虽然本次Meetup未直接涉及政策话题,但这是行业发展不可回避的外部环境。<br/>国家层面的政策导向:<br/>2023年《生成式人工智能服务管理暂行办法》出台,对AI应用提出了明确要求。2024年各部委密集发布AI相关政策,鼓励创新应用的同时,也强化了安全监管。<br/>对IT服务管理行业而言,政策影响主要体现在:<br/>数据安全:智能体训练和运行需要大量数据,如何确保数据不泄露、不滥用,是合规的首要问题。<br/>算法透明:AI决策过程的可解释性要求,对智能运维、智能诊断等应用提出了挑战。<br/>责任界定:当AI做出错误决策导致系统故障,责任如何划分?这涉及法律和保险层面的安排。<br/>行业自律:各行业协会正在制定AI应用的行业标准和最佳实践,参与标准制定将成为企业的竞争优势。<br/>从国际经验看,监管政策对行业发展是把"双刃剑":过严会抑制创新,过松会带来风险。找到平衡点需要监管部门、行业企业、技术专家的共同努力。</p><p><strong>未来展望:三年内的行业图景</strong><br/>基于当前趋势,我们可以对未来3年IT服务管理行业的发展做出以下预判:<br/>2025年:试点探索期</p><ul><li>大型企业启动AI智能体试点项目,聚焦高价值场景</li><li>专业厂商推出成熟度更高的智能体产品</li><li>行业培训和认证体系逐步建立</li><li>初级岗位需求开始明显下降<br/>2026年:规模应用期</li><li>AI智能体从试点走向规模部署</li><li>人机协同的工作模式成为主流</li><li>行业人才结构调整加速</li><li>新商业模式开始产生规模化收入<br/>2027年:深度融合期</li><li>AI成为IT服务管理的基础设施</li><li>行业竞争格局基本稳定</li><li>新一代技术人才成为市场主力</li><li>技术标准和监管框架基本完善<br/>这个演进过程不会一帆风顺,必然伴随着:技术迭代的不确定性、商业模式的试错成本、人才转型的阵痛期、组织变革的阻力。<br/>但历史的车轮不会停止。就像云计算取代传统IDC、移动互联网颠覆PC互联网一样,AI对IT服务管理的重塑已是不可逆转的趋势。</li></ul><p><strong>拐点已至,选择在你</strong><br/>广州这场Meetup的意义,不在于提供了多少技术细节,而在于它标志着行业集体意识的觉醒。<br/>当100多位IT精英主动牺牲周末时间来学习AI,当四位专家不遗余力地分享经验和洞察,当参会者全神贯注地进行实战演练,我们看到的是一个行业在变革前夜的集体行动。<br/>这种行动本身就是信号:IT服务管理行业的拐点已经到来。<br/>站在这个拐点上,每个从业者、每家企业、每个投资机构都面临选择:<br/>是主动拥抱变化,还是被动等待淘汰?<br/>是投入资源转型升级,还是固守传统模式?<br/>是培养新型人才,还是继续依赖旧有能力?<br/>历史告诉我们,在技术变革的拐点上,选择比努力更重要,方向比速度更关键。<br/>2025年,AI智能体元年。IT服务管理行业的新篇章,正在开启。<br/>而你,准备好了吗?</p>]]></description></item><item>    <title><![CDATA[智驾大模型的「隐形战场」：当GPU堆不动了，行业拼什么？ 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524975</link>    <guid>https://segmentfault.com/a/1190000047524975</guid>    <pubDate>2026-01-06 19:10:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会（OpenAnolis Conference，简称 2025 龙蜥大会）于北京圆满结束。同时，由阿里云智能集团编译器技术总监李三红，龙蜥社区运营委员会副主席、龙腾计划生态负责人金美琴联合出品的“数据×模型×软件”分论坛也圆满举办。来自阿里云、安谋科技、HiEV大蒜粒车研所、中兴通讯以及清华大学、澳门大学等企业和高校的 12 位大咖，从操作系统与上下游生态协同的视角出发，与参会嘉宾一起探讨了如何通过技术协作加速智能驾驶的进步，分享了各自在自动驾驶技术栈中的前沿实践与生态思考。以下文章转自 HiEV 大蒜粒车研所公众号：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524977" alt="图片" title="图片"/></p><p>过去两年，随着大模型的发展，智驾行业行业似乎进入一场“军备竞赛”。从大规模装车量产，采集数据喂养模型迭代，“算力”成为一段时间内主机厂们关注的焦点，行业甚至有「千卡是门槛，万卡是入场券」的说法。</p><p>从 BEV+Transformer 到端到端，再到如今大热的 VLA（视觉-语言-动作）模型，参数量指数级膨胀，让整个行业陷入了一种“囤卡狂热”。</p><p>仿佛只要堆砌了足够的 H100 或 H800，L3 甚至 L4 级别的自动驾驶能力就会在 Scaling Law 的魔法下，自动涌现。</p><p>在前不久的 2025 龙蜥操作系统大会“数据×模型×软件”分论坛上，我们听到了一些冷静得近乎“泼冷水”的声音。 </p><p>主持人在圆桌讨论的时候提到一个很有意思的事情： </p><p>之前微软 CEO 萨提亚·纳德拉在接受采访的时候就感慨过，即便拥有大量的 GPU，也面临着缺乏足够的物理基础设施（如机柜与电力环境）来安置它们的尴尬境地。</p><p>这也折射出了智驾行业一个被长期掩盖的痛点：单纯依靠堆砌 GPU，想“大力出奇迹”的模式，正在撞上一堵「物理现实与经济成本」的墙。</p><p>当行业的焦点都集中在英伟达、华为昇腾这些台前的“算力卡”上时，一场关于操作系统、基础软件与异构计算的“隐形战争”早已在水面下打响。</p><p>阿里云副总裁李俊平在开场致辞中提出了一个公式：AI 的效能 = 数据（燃料）× 模型（引擎）× 软件（油门和方向盘）。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524978" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云副总裁李俊平）</p><p>今天的智驾竞争，正在从单一的模型之争，演变为这三者乘积效应的系统工程对抗。</p><h3>这届智驾，被“数据搬运”卡脖子</h3><p>“谈卡伤感情，没卡没感情。”这是前两年智驾圈的真实写照。但到了 2025 年，很多车企发现，即便斥巨资买来了卡，训练效率却并没有线性增长。</p><p>问题出在哪？GPU 在“偷懒”。</p><p>这其实不是什么硬件故障，而是数据“喂”得不够快。</p><p>智驾研发并非只有模型训练这一个环节，它是一个包含数据采集、清洗、标注、挖掘、训练、仿真到端侧部署的一条长长的数据闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524979" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团高级架构师张先国分享了一组数据：智驾研发团队，云端存储的数据总量通常已达到 400PB 到 800PB，日增量在 1PB 以上。一个智驾企业同时进行多个模型训练，消耗的算力经常需要万卡以上。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524980" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团高级架构师张先国）</p><p>想象一下，GPU 就像是一台拥有 F1 引擎的赛车，但如果给它输油的管子（I/O带宽）只有吸管那么细，引擎空转就在所难免。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524981" alt="图片" title="图片" loading="lazy"/></p><p>在 2025 龙蜥大会的现场，多位专家指出了“数据闭环”中存在的隐形关卡： 一个是数据加载的问题。训练开始前，海量的小文件（图片、标注信息）需要从存储层搬运到计算层。另一个是预处理可能遭受的瓶颈：视频需要抽帧、解码、清洗，训练集群就在那里，但数据卡在缓存层过不来，GPU 只能闲置等待。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524982" alt="图片" title="图片" loading="lazy"/></p><p>阿里云智能集团产品专家钱君在演讲中提到，为了解决这个问题，行业正在把目光投向存储与操作系统的底层优化。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524983" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团产品专家钱君）</p><p>例如，龙蜥操作系统（OpenAnolis）给出的方案是全链路的“疏通”：针对 CPFS（并行文件系统），龙蜥在 OS 层面进行了深度适配。缓存写场景下的性能可以直接提升 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短：以前需要几小时，现在几十分钟就能搞定。 这种“看不见”的基础设施优化，虽然没有新开发一个大模型那么性感，但它决定了生产智能的效率和成本，是让万卡集群真正跑满的关键。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524984" alt="图片" title="图片" loading="lazy"/></p><h3>CPU：被忽视的“异构协同”</h3><p>在智驾的模型训练中，公众通常认为关键的算力在于 GPU；但在本届大会上，“CPU 的挖掘”成为当下的新共识。</p><p>“不能只关注 GPU，CPU 在数据预处理、存储 I/O 及逻辑控制中扮演着关键角色。” 中兴通讯操作系统产品副总经理胡冲在圆桌讨论中直言。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524985" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统产品副总经理胡冲）</p><p>事实上，在视频转图片（抽帧）、数据清洗、以及 Spark 大数据分析环节，CPU 才是主力军。而且，随着架构的演进，Arm 架构的服务器 CPU（例如如阿里云倚天 710 ）正在展现出独特的优势。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524986" alt="图片" title="图片" loading="lazy"/><br/>（图/安谋科技（Arm China）云人工智能事业部总监侯科鑫）<br/>安谋科技（Arm China）云人工智能事业部总监侯科鑫女士，在演讲中向现场观众展示了数据中心架构的演进逻辑：随着 NVIDIA  Grace Hopper 异构加速平台的推出，CPU 与 GPU 的“紧密协同处理”已成为行业明确的发展方向。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524987" alt="图片" title="图片" loading="lazy"/></p><p>为什么要协同？是为了打破“内存墙”。</p><p>“视频处理并不是简单的计算，它对高负载下算力要求极高。”张先国指出。</p><p>智驾训练需要把每秒视频抽帧为 8-32 张图片，在视频解码计算（如 H.264/H.265 格式）的高并发场景下，传统的 x86 架构，由于睿频（超线程）机制和功耗墙的存在，在高负载下往往会降频。</p><p>而张先国分享的实测数据显示，Arm 架构处理器凭借更多的物理核和大缓存（L1/L2 Cache），在智驾数据处理场景下表现惊人： </p><p>首先是视频抽帧，性能比传统 x86 提升约 20%，成本却降低了 20%-30%； </p><p>大数据清洗方面，由于拥有更大的 Cache（缓存），数据 Miss 率极低，这意味着 CPU 不需要频繁地去内存“搬砖”，从而使端到端性能提升了 30%，在部分场景下甚至实现了翻倍。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524988" alt="图片" title="图片" loading="lazy"/></p><p>一个高效的智驾云端底座，必须是 CPU 与 GPU “各司其职、紧密抱团”的异构系统。</p><p>侯科鑫还从更宏观的维度讲述了硬件底座的变迁。她指出，为了打破“内存墙”和功耗瓶颈，数据中心正在从通用服务器向“定制化 SoC + Chiplet”演进。</p><p>NVIDIA 的 Grace Hopper 平台就是典型案例——通过将 Arm 架构 CPU 与 Hopper  GPU 紧密互联，实现内存共享，极大降低了数据搬运的延迟。这种 CPU 与 GPU 紧密协作的架构，正是为了解决单一算力无法应对复杂数据流的困境。Arm 推出的 Total Design 生态和 Neoverse CSS，正是以推动异构计算规模化落地为核心目标，让芯片设计公司能节省大量工程投入，快速构建这种异构计算的「高速公路」。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524989" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><h3>基础软件的魔法：不堆卡也能让训练变得更快</h3><p>摩尔定律在放缓，硬件的红利正在吃紧。这时候，软件工程的价值就被进一步放大了。阿里云智能集团编译器技术总监李三红在圆桌环节提到了一个非常典型的矛盾：模型开发者的“爽”和底层工程师的“痛”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524990" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云智能集团编译器技术总监李三红）</p><p>算法工程师喜欢用 PyTorch 的 Eager 模式，因为这样写代码像写 Python 一样灵活，所见即所得；但这种模式对底层硬件极其不友好，运行效率低。而底层工程师希望用 Compile 模式，把代码编译成极致优化的机器码，但这又要求上层改代码，门槛极高。</p><p>“上层的模型开发者追求开发效率（Eager Mode），底层的 Infra 追求成本和性能，这中间的 Gap（鸿沟），就是基础软件的机会。” 阿里云智能集团编译器技术总监李三红在圆桌讨论中一针见血地指出。</p><p>针对如何填补这一鸿沟的问题，阿里云智能集团产品专家钱君与高级架构师张先国在随后的演讲中展示了龙蜥操作系统（OpenAnolis）如何通过全链路优化，在不改变硬件的情况下“白捡”性能：</p><ul><li>存储加速（IO 吞吐）： 针对 CPFS（并行文件系统），系统在 OS 层面进行了深度适配。钱君披露的数据显示，在缓存写场景下，性能提升了惊人的 10 倍。这意味着模型训练中的 Checkpoint 保存时间大幅缩短，断点续训不再是噩梦。</li><li>网络加速（打破 TCP 限制）： 张先国指出，通过部署自研的 eRDMA 协议，相比传统 TCP，延迟降低 3 倍，带宽提升 4 倍（实测可达 18GB/s）。这让数据在节点间的跳跃如同在本地总线般顺滑。</li><li>编译器优化（榨干每一滴算力）： 针对 PyTorch 等框架的运行效率痛点，利用 AI Compiler 进行算子融合。据钱君介绍，这套方案在部分通用模型上带来了接近 100% 的性能提升，有效地解决了开发灵活性与运行效率不可兼得的难题。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524991" alt="图片" title="图片" loading="lazy"/><br/>效果有多明显？ </p><p>地平线和小鹏汽车的案例显示，通过这一套“操作系统+编译器+调度”的组合拳，部分场景下的性能提升可达 30% 甚至 100%，而成本却能下降 20%-60%。</p><p>在「降本增效」成为汽车产业主旋律的 2025 年，这种来自基础软件的“软实力”，比盲目堆更多的卡，更有性价比。</p><h3>眺望未来：世界模型与“合成数据”</h3><p>如果说当下智驾行业发展的痛点是“效率”，那么未来的挑战可能会是“认知”。</p><p>清华大学人工智能研究院视觉智能研究中心主任邓志东教授在圆桌论坛上抛出了一个前瞻性观点：智驾模型正在从单纯的感知，向世界模型（World Model）演进。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524992" alt="图片" title="图片" loading="lazy"/><br/>图片来源：CVPR 2024 Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</p><p>目前的端到端大模型，虽然能处理很多场景，但面对极端的 Corner Case（长尾场景），靠实车采集的数据永远是不够的。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524993" alt="图片" title="图片" loading="lazy"/><br/>（图片来源：NVIDIA）</p><p>“路是跑不完的，但世界是可以被模拟的。”</p><p>但这种演进这种演进对基础设施提出了更苛刻的要求：</p><ul><li>算力需求的指数级爆炸： 世界模型极重，不仅需要理解物理世界，还要生成虚拟物理世界。这可能需要数百亿甚至更高的算力支撑，甚至触及到供电能力的边界。</li><li>合成数据的崛起： 真实路采数据的效率太低且稀缺。未来，大量的训练数据将来自“虚拟物理世界”的高效生成。这对 GPU 的渲染能力和 CPU 的逻辑模拟能力提出了双重挑战。</li><li>软件定义的灵活性：正如中兴操作系统产品线副总经理胡冲在圆桌中所感慨的，算法迭代极快——“去年可能还是 BEV，今年就是 VLA 了”。而阿里云李三红也证实，一线技术团队确实清晰感知到了模型向 VLA 及世界模型演进的趋势。这种软件层面的极速狂奔，与硬件芯片较长的迭代周期形成了鲜明对比。这就要求编译器和操作系统必须具备极强的适应性，通过软件定义来抹平硬件迭代的时间差。AI 不仅要“看懂”视频，还要能“生成”视频，甚至要理解牛顿定律。</li></ul><p>邓教授指出，这需要底层算力支持极其复杂的“虚实迁移”。这意味着，未来的操作系统不仅要调度计算，还要调度“物理世界的规则”。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524994" alt="图片" title="图片" loading="lazy"/><br/>（图/清华大学教授、清华大学人工智能研究院视觉智能研究中心主任邓志东）</p><p>这也解释了为什么像龙蜥这样的开源社区，开始在这个阶段强调“ AI 原生操作系统”的概念——因为旧的底座，真的撑不住新的世界了。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524995" alt="图片" title="图片" loading="lazy"/></p><h3>开源底座的长期主义</h3><p>从 2025 龙蜥大会的这场分论坛中，我们看到了汽车科技行业的一个明显转折：</p><p>大家不再盲目迷信硬件的堆砌，开始回归计算机科学的常识——系统协同。</p><p>面对 Arm、x86、RISC-V 等复杂的芯片架构，面对日新月异的模型算法，车企和智驾公司不可能每一家都去从零手搓一套底层软件。</p><p>而龙蜥社区的存在，就是为了提供一个标准化的技术底座，屏蔽底层异构硬件（不同架构的 CPU、GPU、NPU）的差异，让车企和智驾公司能够专注于上层模型和算法的创新。正如 Arm 通过 Arm Total Design 联合产业链一样，软件层面也需要这样一个“连接器”来降低全行业的试错成本。</p><p>正如胡冲所言：“通过社区共建、共享，降低车企的研发门槛与成本，是解决算力荒的另一种路径。”</p><p>在算力资源有限、成本高企、模型日趋复杂的背景下，谁能更高效地榨干每一 Tops 算力的价值，谁能以更低的成本完成数据的闭环流转，谁就能在 L3+ 的量产前夜活下来。</p><p>数据是资产，模型是能力，而软件与操作系统，是这一切的根基。</p><p>自动驾驶的下半场，不再是单点技术的突破，而是“数据-模型-软件”全链路的生态战争。在这个战场上，那个由 CPU、操作系统、编译器、文件系统构成的庞大“新基座”，正在成为决定胜负的隐形力量。</p><p>对中国的自动驾驶产业而言，建立一个自主、可控、高效的基础软件生态，其战略意义或许丝毫不亚于拥有几万张显卡。</p><p>因为只有根扎得够深，智能的树才能长得够高够稳。</p><p>本次分论坛回顾已上线，欢迎点击下方链接查看回放：<a href="https://link.segmentfault.com/?enc=Ad0vvxxozxD8A3cfJDxYVg%3D%3D.KaKY6pQ2Dh7iI5rJZ8a12KlhpFFqCT4%2BFD6fTofl9ZOwG4ruzgEki2alw41JoyB5" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[优秀学子获颁证书，开放原子校源行Meetup活动（中南大学站）圆满举办 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525016</link>    <guid>https://segmentfault.com/a/1190000047525016</guid>    <pubDate>2026-01-06 19:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，由浪潮信息联合龙蜥社区、中南大学信息与网络中心、电子信息学院共同举办的开放原子校源行 Meetup 活动（中南大学站）顺利举行。本次活动吸引了 70 余名中南大学本科生和研究生的积极参与，现场气氛热烈，同学们和与会嘉宾深入交流开源文化与技术应用，收获颇丰。</p><p>活动伊始，中南大学电子信息学院特聘副教授、博士生导师施鹤远主持开场环节。施教授对参与活动的师生、技术专家表示欢迎，并简要介绍了活动背景和目的。他强调，本次活动旨在为同学们搭建一个学习和实践开源技术的平台，帮助大家更好地了解开源文化、技术应用以及未来发展方向。他鼓励同学们积极参与今天的活动，主动与嘉宾交流，积极探索开源技术在实际应用中的价值，为未来的职业发展和个人成长积累宝贵经验。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525018" alt="图片" title="图片"/><br/>（图/中南大学电子信息学院特聘副教授、博士生导师 施鹤远）</p><p>中南大学电子信息学院教授、博士生导师、副院长石金晶为本次活动致辞。石院长在致辞中表示，开源技术不仅是当今科技发展的前沿趋势，更是同学们提升自身竞争力的重要途径。开源社区汇聚了全球最优秀的技术人才和创新项目，同学们在这里可以接触到最前沿的技术理念和实践经验。石院长强调，学院将全力支持同学们的开源实践，为大家提供更多的资源和平台，希望同学们能够珍惜这次机会，积极参与开源活动，为自己的未来职业发展打下坚实的基础，同时也为电子信息学院的学科发展贡献自己的力量。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525019" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、副院长 石金晶）</p><p>中南大学电子信息学院教授、博士生导师、教学实验中心主任胡超为本次活动致辞。胡主任在致辞中表示，开源技术为同学们提供了一个绝佳的实践平台，能够让大家在实践中快速成长。他指出，开源项目不仅能够提升同学们的专业技能，还能培养大家的创新思维和团队合作精神。胡主任鼓励同学们积极参与开源社区，主动探索和学习，勇于挑战自己。他强调，学院将为同学们提供全方位的支持，包括技术指导、实验资源等，帮助大家更好地参与到开源项目中。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525020" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学电子信息学院教授、博士生导师、教学实验中心主任 胡超）</p><p>活动中还举行了“浪潮信息 - 龙蜥技术认证证书颁发仪式”。石院长与胡主任为在技术认证中表现优异的同学颁发了工程师证书，表彰他们在开源技术领域的突出成绩，激励更多同学投身开源实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525021" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息 - 龙蜥技术认证证书颁发仪式）</p><p>中南大学计算中心实验师徐海坤在活动中作了题为《AI 赋能科学计算》的分享。他介绍了中南大学计算平台的发展历程，包括其在 2020 年建成的千万亿次级计算平台，以及该平台在全球和中国相关领域排行榜中的优异表现。报告重点阐述了 AI 技术在科学计算中的应用，包括 AI 能力平台的建设、基于 AI 的智能运维和作业调度优化等内容，强调了开源技术在提升平台性能和运维效率中的重要作用，展示了中南大学在 AI 赋能科学计算领域的创新实践。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525022" alt="图片" title="图片" loading="lazy"/><br/>（图/中南大学计算中心实验师 徐海坤）</p><p>阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦以《让校园代码长出 AI 翅膀：与龙蜥共探下一代开源智能》为题进行分享。他介绍了龙蜥社区的 Anolis OS 23 操作系统，重点阐述其在 AI 场景下的创新应用，包括 AI 辅助开发、系统构建优化和运维智能化。他还展示了 OS Copilot 智能助手的功能，强调其在降低 Linux 使用门槛和提升运维效率方面的优势，并鼓励同学们积极参与开源社区活动。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525023" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里云工程师、龙蜥社区基础设施 SIG Maintainer 单凯伦）</p><p>浪潮信息高级产品经理 Viki Wang在活动中分享了《AI 时代开源操作系统应用及生态创新实践》。他表示，浪潮信息作为龙蜥社区副理事长单位，在开源领域持续创新，其开源贡献排名位居前列，并在多个领域取得显著成果。他还分享了浪潮信息在 AI 时代的系统优化成果，如 GPU 和 CPU 异构算力协同、大模型推理性能提升以及兼容性测试基准的建立等，展示了浪潮信息在开源操作系统领域的技术实力和生态建设成果。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525024" alt="图片" title="图片" loading="lazy"/><br/>（图/浪潮信息高级产品经理 Viki Wang）</p><p>龙蜥社区 CXL SIG Maintainer 李伟在活动中分享了《携手龙蜥 共创芯生态》。他介绍了其团队在开源领域的全面布局和深度合作，特别是在龙蜥社区的积极参与。通过贡献内核优化、虚拟化支持和安全技术，推动了开源生态的发展。李伟强调，其团队致力于通过开源合作，共同打造开放、共赢的芯片与操作系统生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525025" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区CXL SIG Maintainer 李伟）</p><p>本次开放原子校源行 Meetup 活动（中南大学站）在热烈的氛围中圆满落幕。通过本次活动，浪潮信息携手开放原子、龙蜥社区及中南大学，成功搭建了一个前沿技术交流与实践的平台，为同学们开启了通往开源世界的大门。未来，浪潮信息将继续践行“龙蜥+”合作模式，深化与高校的合作，助力更多学子在开源领域成长成才，为开源生态的繁荣发展持续贡献力量。</p><p>龙蜥技术认证学习中心：<a href="https://link.segmentfault.com/?enc=Y2JFwazv9c2RpeTAkxdl%2BA%3D%3D.D%2B8TjMa%2FZo3B02zmnK3x3lKmicqpZuvd0Lny6tggDws%3D" rel="nofollow" target="_blank">https://openanolis.cn/course</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[专访 | 软硬协同、开源共建：英特尔与龙蜥携手打造 AI 时代的可信计算底座 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525040</link>    <guid>https://segmentfault.com/a/1190000047525040</guid>    <pubDate>2026-01-06 19:09:30</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>编者按：近日，2025 龙蜥操作系统大会（OpenAnolis Conference）在北京圆满召开。当智能驾舱厂商训练自动驾驶 AI 模型、金融机构运行 AI 风控系统时，普遍面临相同困境：数据敏感不敢上云，本地算力又难以支撑大模型需求。AI大爆发后，“算力效率”与“数据安全”的矛盾愈发突出。2025 龙蜥操作系统大会前夕，InfoQ 对话英特尔技术专家与阿里云技术专家，揭秘双方如何通过第六代至强处理器与龙蜥操作系统的深度协同，破解这一行业难题。以下为采访全文：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525042" alt="图片" title="图片"/></p><h3>合作底层逻辑：为什么是“英特尔硬件+龙蜥开源”？</h3><p>AI 时代的算力释放，早已不是“硬件单枪匹马”能实现的。“过去硬件做芯片、软件写系统的分工模式已失效，AI 模型扩大与数据敏感化，要求硬件与软件必须深度协同。”英特尔技术专家强调，“2020 年我们成为龙蜥首批理事单位，正是看中龙蜥操作系统在云原生和 AI 领域的开源属性——它能快速承接硬件新特性，而英特尔的芯片技术，就是要为这个开源生态打下坚实的算力底座。”</p><p>过去一年，双方形成了固定的合作节奏：硬件首发时与龙蜥操作系统同步适配，避免企业“空有硬件用不了”；针对硬件暴露的软件问题联合优化，成果反哺龙蜥及 Linux 上游社区；英特尔提供测试资源，龙蜥联动客户推动技术落地。总结来说，英特尔负责造算力、锁安全，龙蜥负责用算力、传价值，这是双方合作的核心逻辑。</p><p>阿里云技术专家补充道：“阿里云作为云平台方，刚好承接这种协同成果——把英特尔的硬件能力和龙蜥操作系统的系统优势，打包成企业能直接用的云服务，这也是我们作为社区理事长单位的价值。”</p><h3>技术共建：从芯片到工具链，让 AI 算力“跑满效能”</h3><p>AI 算力的高效释放，需要“芯片发动机”“系统公路”与“工具链交通规则”的协同。过去一年，英特尔与龙蜥的技术共建集中在这三大方向：</p><h4>第六代至强适配：不止能用，更要“榨干”性能</h4><p>2025 年发布的第六代至强处理器，针对不同 AI 场景做了细分设计：Granite Rapids 主打高密度计算，适配金融风控、科学计算等强性能需求；Sierra Forest 聚焦云原生大规模部署，优化能效比以降低云厂商运营成本。“我们的芯片设计贴合场景化需求，而龙蜥操作系统能精准匹配这种特性，让硬件能力不浪费。”英特尔技术专家说。</p><h4>为让硬件性能充分释放，双方完成了两项关键优化：</h4><ul><li>全链路适配：覆盖龙蜥操作系统 5.10 长期支持版与 6.6 最新特性版，同时完成主流虚拟化平台定制，为 Granite Rapids 开发“大内存调度补丁”，支持 2TB 以上内存以满足 AI 训练需求；</li><li>突破多核瓶颈：针对新一代处理器近 128 核的硬件特性，重构龙蜥操作系统多核调度算法，通过“专属缓存分配”减少核心资源争抢，优化内存页表管理实现有序读写。</li></ul><p>这些优化最终让 Granite Rapids 在多线程任务中性能较上代提升显著。“性能提升是企业能切实感受到的变化，这是软硬件协同的价值。”英特尔技术专家表示。</p><h4>下一代硬件预研：提前3个月适配，消除“空窗期”</h4><p>为解决企业“硬件到位、系统未就绪”的痛点，双方采用“提前布局”策略。英特尔下一代至强 6 Plus 服务器（代号 Clearwater Forest）尚未上市，2025 年 Q2 已联合龙蜥启动适配。“企业采购硬件投入大，我们把适配周期提前，就是要让客户拿到硬件就能开机测试，这符合龙蜥社区‘开箱即用’的理念。”英特尔技术专家表示。</p><h4>异构工具链：oneAPI+OpenVINO，降低开发门槛</h4><p>AI开发者常受困于“硬件异构”——为 CPU 写的代码无法直接在 GPU、NPU 上运行，重复适配耗费大量精力。英特尔与龙蜥的解法是构建“统一工具链”。“开发者的核心价值是优化 AI 模型精度，不是做硬件适配的‘翻译官’。”英特尔技术专家直言，“oneAPI 和 OpenVINO 的融合，就是要把硬件差异藏在工具链里，让一套代码跑通所有设备。”</p><ul><li>oneAPI 统一开发框架：基于 LLVM 扩展异构编译能力，搭建设备抽象层，一套代码可调用不同硬件能力；该平台支持多种编程语言，包括 C++、Python、Fortran 等，使得 AI 模型的训练和推理能够在不同计算架构上高效执行。</li><li>OpenVINO 工具链即插即用：与龙蜥操作系统深度集成，简单命令即可部署，为云端和边缘计算环境中的 AI 推理任务提供优化方案，进一步降低 AI 部署的计算成本，提高 AI 模型的执行效率。</li></ul><p>“我们的目标是让开发者聚焦模型优化，而非硬件适配。”英特尔技术专家表示，这正是“软件定义、硬件赋能”的核心体现。</p><h3>生态共建：让算力生态“活起来”</h3><p>技术落地离不开生态支撑。作为龙蜥社区副理事长单位，英特尔从社区治理、资源支持、国际化联动三方面推动生态发展：</p><p>首先是深度参与社区治理。英特尔并非单纯的“硬件供应商”，而是深度参与龙蜥社区底层建设：如主导 X86 架构优化的 Arch SIG 项目，制定至强处理器在龙蜥操作系统上的性能基准测试体系；参与《国产服务器操作系统发展报告（2025）》中核心章节的撰写；推动龙蜥社区加速国际化等。“开源社区要靠核心厂商带头做实事，这是我们作为副理事长单位的责任，也是为了让龙蜥生态更有技术厚度。”英特尔技术专家说。</p><p>其次是开放资源、降低参与门槛。为解决中小企业“缺硬件、缺技术”的问题，英特尔向龙蜥社区开放测试硬件，开展联合测试并输出技术文档、联合报告。阿里云技术专家补充：“我们会把测试成果转化为云平台最佳实践，帮客户少走弯路。”</p><p>最后是国际化经验反哺。依托在 Linux Foundation、CNCF 的经验，英特尔帮助龙蜥优化内核补丁流程，深度参与 X86 架构补丁审核；并推动龙蜥开发者参加全球开源峰会，加强国际化交流等。</p><h3>机密计算：用“硬件锁+开源钥匙”守护数据安全</h3><p>AI 时代的核心矛盾是“数据需流动产生价值，却怕流动中泄露”。英特尔与龙蜥的解法，是从硬件隔离到开源方案的全链路防护。</p><p>英特尔从硬件层面构建安全底座，核心依赖两大技术，其原理均通过硬件隔离实现数据防护。“机密计算的核心是‘硬件可信’，软件再安全，硬件被突破就没用。”英特尔技术专家解释，“TDX 和 SGX 就是从芯片层面给数据加‘锁’，让安全成为硬件原生能力。”</p><ul><li>TDX（可信域扩展）：在至强芯片中创建“隔离执行域（TD）”，即使系统内核被攻击，TD内的内容也无法被访问，内存数据通过内存控制器实时加密，仅硬件才能解密；</li><li>SGX（软件防护扩展）：针对轻量级场景在内存中划分“加密区（Enclave）”，仅授权代码可访问，其他进程即使获取内存地址，看到的也只是乱码。</li></ul><p>为让龙蜥操作系统适配这些能力，英特尔在系统内核中集成 TDX 和 SGX 驱动并由硬件实现 AES-GCM 加密协议，确保安全防护不影响性能。</p><p>此外，基于硬件底座，阿里云在龙蜥社区推出 Confidential AI 开源方案，整合 TDX 安全能力、远程证明服务与密态存储/网络能力，降低企业使用 TDX 机密计算的门槛。“英特尔的硬件是‘安全地基’，我们的工作是在地基上搭好‘房子’，让企业不用自己打地基就能用。”阿里云技术专家说，“目前龙蜥社区 Confidential AI 开源方案已落地阿里云异构机密计算实例，并正与消费电子、智能驾舱客户合作。”</p><p>为推动行业规范，双方在标准化工作上已取得明确进展。阿里云技术专家具体介绍：“以 Confidential AI 为技术基础，阿里云已联合 30 多家合作伙伴牵头编写 CCSA AI 数据安全的标准化架构与技术实现方案，重点覆盖 AI 推理和训练两大核心场景，目前这项工作已进入实质推进阶段。”</p><p>国际标准的布局也在同步展开，他补充道：“我们计划以 CCSA 的标准化成果为基础，在 12 月初日内瓦举行的国际电信联盟 ITU-T SG17 分论坛上，推动该标准的国际版本立项。这将形成一个良性闭环——开源解决方案为标准化提供了可落地的技术参考，而标准化规范又能为开源方案的合理性和通用性提供背书。”</p><p>在他看来，这种联动恰好体现了核心价值：“商业需求驱动开源方案迭代，开源方案支撑标准化落地，标准化又反过来赋能商业推广，三者不是孤立的，而是循环共生的关系。”</p><h3>未来方向：AI 原生时代的多元协同</h3><p>在 2025 龙蜥大会上，双方集中展示了第六代至强与龙蜥操作系统的性能优化成果、Confidential AI 落地进展及 Clearwater Forest 适配情况，同时释放 AI 原生时代的算力发展方向。</p><p>“AI 原生算力靠‘芯-OS-云-AI’协同。”英特尔技术专家透露规划：硬件上，下一代至强将进一步强化 AI 与安全能力；系统上，将联合龙蜥开发 AI 任务调度器，优化资源分配效率；场景上，将针对隐私敏感场景推出通用部署方案。</p><p>对企业而言，这意味着未来使用 AI 将更简单：依托“英特尔硬件+龙蜥操作系统”的组合，无需自行解决适配、优化与安全问题，即可直接获得高效且安全的算力支撑。随着龙蜥大会的召开，这套“硬件底座+开源生态”的方案，将成为企业 AI 落地的核心选择。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[不止于用，更在于创！龙蜥社区点燃高校开源火种 | 龙蜥五周年征文精选 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525045</link>    <guid>https://segmentfault.com/a/1190000047525045</guid>    <pubDate>2026-01-06 19:08:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <blockquote><p>各位小伙伴，龙蜥社区已启动主题为「5 周年，你与龙蜥的故事」征文活动！征文内容包括但不限于以下：</p><p>故事文章：文字记录你的经历、感悟或技术心得，包括：龙蜥操作系统 Anolis OS 的使用体验、相关技术特性解读、经验分享等。</p><p>故事视频：用镜头讲述你与龙蜥的点滴瞬间，参与某次大会/ MeetUp 的拍摄视频等（3 分钟内即可）。</p><p>优质文章将获得龙蜥社区官网及公众号推荐展出，还可获得神秘礼品。欢迎各位龙蜥社区朋友来稿~</p></blockquote><p>本期征文故事主角：施刚，龙蜥社区 2025 年度优秀贡献者奖获得者、成都东软学院计算机与软件学院教授、中国自动化学会边缘计算专业委员会委员，从事计算机系统研究与操作系统相关课程教学工作。2022 年，通过教育部产学合作协同育人项目开始与龙蜥社区开展深度合作。</p><h3>初识龙蜥社区</h3><p>在高校计算机相关专业中，操作系统相关的专业课程占据着重要的地位。多年来，基于 UNIX 和 Linux 系统来进行相关课程的讲授与学习，是高校计算机相关专业师生的普遍选择。早期，CentOS 系统作为 RHEL 系统的完全功能兼容版且开源免费的特性，是高校学习操作系统及中小 IT 企业用于部署各类应用服务器的首先系统。但随着 2021 年末，CentOS 系统停止更新退出市场后，不仅商业市场，在高校教学领域也必须加紧填补其留下的技术空白。</p><p>近年来，在国产系统走进课堂的大背景下，作为面向智算时代的国产开源操作系统，龙蜥操作系统 Anolis OS 已成为 CentOS 的优秀继承者，它不仅完全兼容 RHEL，更针对云原生、高性能计算进行了优化，是高校在操作系统相关课程尤其是 Linux 相关课程中的最优选择。2021 年底，我开始关注 CentOS 停服后的国产操作系统替代方案，并从此开始了解龙蜥社区。2022 年，我通过申请教育部产学合作协同育人项目（龙蜥社区理事长单位-阿里云发布 Anolis OS 项目），与龙蜥社区正式确立了合作关系，并把龙蜥社区正式引入了成都东软学院计算机相关专业的教学实践工作中来。同年注册龙蜥社区，下载安装或通过龙蜥实验室使用龙蜥操作系统的师生就超 1500 人次。至今，成都东软学院已有累计超 5000 人次师生学习和使用龙蜥操作系统。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525047" alt="图片" title="图片"/></p><p>基于 Anolis OS 8，我们也产出了一套完整的《Linux 系统管理与服务器配置》教材，用于日常教学。社区成员也可在龙蜥文档中心获取：<a href="https://link.segmentfault.com/?enc=ptZx1WglRHCOfgYTNCnpKw%3D%3D.1Lg7dpPFr3sqejFcRRI%2FUgtzwcj67OrF2GiUfZapyCQHGy96Mf94hC1IG9hDCdqVMd0VtwuEuiF2unAPfowhTQ%3D%3D" rel="nofollow" target="_blank">https://docs.openanolis.cn/document/detail/rxli6fw9</a></p><h3>龙蜥实验室：优秀的在线实践与学习平台</h3><p>对高校的计算机类实践教学活动来说，拥有一个优秀的在线实践平台是非常重要的。依靠高校自身打造基于操作系统的服务性平台所需的软硬件资源非常大，国内高校鲜有独立建立的相关实践教学平台。而龙蜥社区提供的龙蜥实验室帮助我完美解决了此问题。作为学生，只需要一台联网的计算机，就可在“龙蜥实验室”申请一台机器并在其进行各类实践和实验活动。我在使用龙蜥实验室的过程中，还不断通过向龙蜥社区后台技术人员反馈使用情况并提出改进意见，使得龙蜥实验室的申请和使用流程越来越优化和便捷。以我讲授的 Linux 基础课程为例，除日常的各类实践类作业，该课程所涉及的 8 个实验项目中有 6 个都可通过龙蜥实验室在线完成。 </p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525048" alt="图片" title="图片" loading="lazy"/></p><h3>龙蜥技术认证：为高校学生提供企业认可的职业技能认证</h3><p>对高校学生来说，毕业求职时能提供更多的 IT 职业认证技能证书更能得到对口求职单位的认可。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525049" alt="图片" title="图片" loading="lazy"/></p><p>在 Linux 的认证领域，传统的 RHEL 认证已发展了近二十年，在 RHEL 占据国内高端服务器操作系统的时代其认证证书的含金量是很高的。但随着 CentOS 的退出及以龙蜥操作系统为代表的国产服务器操作系统的强势崛起，国内 IT 企业对龙蜥操作系统的认可度也越来越高。根据2025 龙蜥操作系统大会数据显示，当前龙蜥操作系统的装机量已突破 1000 万套，市场占有率接近 50%，国内 IT 市场对相关运维人才的需要也越来越多。2023 年开始，龙蜥社区联合相关企业开始推出龙蜥技术认证的活动，我在成都东软学院计算机学院配合了该认证活动的推广和实施。</p><p>首次认证是基于龙蜥-统信联合开展的，有 600 多名同学报名，这些同学都经过了一学期的 Linux 课程学习，大部分同学都顺利通过了考试获得了龙蜥社区和统信的双认证证书。在首次活动顺利开展的基础上，2024 年度和 2025 年度，我在龙蜥社区的协助下，又继续开展了两次集中式的认证考试组织，报名同学所覆盖的专业由成都东软学院的计算机科学与技术拓展到了网络空间安全、网络工程、大数据等多个计算机相关专业，报名人数也逐年提升。为提高同学们的认证考试通过率，龙蜥社区还联合浪潮信息和统信软件的技术专家在后续两次考试前为报名同学进行了统一在线培训。</p><p>截止 2025 年下半年，成都东软学院已和龙蜥社区配合共同开展了 3 次龙蜥技术认证考试，累计参与学生 2000 余人次，通过率接近 80%。在推进龙蜥认证活动的过程中，我还通过申请将认证证书与 Linux 相关课程考核成绩相关联的方式，更好地激发和推动了学生参加龙蜥认证活动的热情。随着龙蜥中级认证活动在 2025 年的推出，后续龙蜥认证在我校的推进将更加深入开展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525050" alt="图片" title="图片" loading="lazy"/><br/>（图/技术认证现场）</p><h3>深度合作：龙蜥社区与高校计算机专业教学的发展方向</h3><p>操作系统类课程是计算机专业的核心课程也是我国计算机技术发展的重要基石，高校是为我国计算机行业提供后备人才的最重要基地。在当前国产软件国产系统进校园进课堂的大趋势和大背景下，龙蜥社区在与高校合作领域面临着非常好的发展前景。以我本人为例，当前我已将龙蜥操作系统作为操作系统类课程学习和实践的主要平台，并将龙蜥实验室作为相关课程实践和实验活动开展的主要场景，通过将龙蜥认证活动与相关课程的考核深度绑定。同时，龙蜥社区还提供了丰富的在线学习视频，可作为 Linux 相关课程内容学习的强力辅助。我作为一名高校教师，深感龙蜥社区在专业度上与计算机相关课程教学大纲和教学内容是高度契合的。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525051" alt="图片" title="图片" loading="lazy"/><br/>最后，高校与社区间是可以持续开展深度合作互惠互利的关系，龙蜥操作系统作为国产操作系统的优秀代表，高校的土壤可以为龙蜥操作系统培养源源不断的学习和使用者，同时也为 IT 企业输送合格的龙蜥操作系统开发和运维人员。随着我国在操作系统技术领域的不断发展，国产操作系统必将逐步取代 RHEL 和 Windows 为代表的各类非国产操作系统在国内各领域的地位和市场。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[关于 AI 编程的思考 edagarli ]]></title>    <link>https://segmentfault.com/a/1190000047525058</link>    <guid>https://segmentfault.com/a/1190000047525058</guid>    <pubDate>2026-01-06 19:07:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在低代码开发与AI商家提效领域深耕多年后，我对AI编程的核心认知逐渐聚焦于“人机协同的价值放大”——它并非替代开发者，而是通过技术工具将开发者从重复编码中解放，转向更核心的业务拆解、架构设计与质量把控。</p><p>AI编程的核心优势在于对标准化场景的效率提升，以我之前的“AI商家工作台看板生成”项目为例，通过结构化Prompt将商家的业务规范（如销量/库存/毛利等核心指标定义）、技术约束（UI组件库规范、接口对接标准）、权限规则等嵌入交互逻辑，AI能快速生成可直接运行的前端代码，将原本1-2天的开发周期压缩至小时级。但这背后离不开两个关键前提：一是精准的Prompt工程，需要将模糊的业务需求转化为AI可理解的技术语言，这要求开发者既懂业务又懂AI的“认知逻辑”；二是对生成结果的校验能力，AI可能存在边界case遗漏、性能优化不足等问题，开发者需凭借技术经验进行兜底，尤其企业级应用中，安全性、规范性与业务贴合度的校验不可或缺。</p><p>从行业发展来看，AI编程正朝着“领域定制化”方向演进。通用型AI编程工具已无法满足企业级场景需求，针对电商采销、供应链管理等垂直领域的定制化AI模型，通过训练行业专属知识库，能大幅提升代码生成的精准度。同时，开发者的角色也在迭代：从单纯的编码者转变为“需求拆解师+Prompt工程师+架构设计师”，需要更强的业务抽象能力与跨领域整合能力——只有将业务逻辑、技术架构与AI工具特性深度融合，才能让AI真正成为业务提效的催化剂。</p><p>归根结底，AI编程的本质是“技术工具对生产力的重构”，但其价值上限始终由开发者的业务理解深度与技术把控能力决定。未来，人机协同的核心将是“人定义价值、AI落地执行”，开发者需聚焦于更具创造性的工作，让AI成为串联需求与实现的高效桥梁，最终实现技术服务于业务增长的核心目标。</p>]]></description></item><item>    <title><![CDATA[2025全球量子计算产业发展展望报告：技术路线、市场规模与应用落地|附200+份报告PDF、数据、可]]></title>    <link>https://segmentfault.com/a/1190000047525072</link>    <guid>https://segmentfault.com/a/1190000047525072</guid>    <pubDate>2026-01-06 19:07:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>原文链接：<a href="https://link.segmentfault.com/?enc=%2BgYqhnlaAiIvokMsNg5M%2Bw%3D%3D.3MfwCGmj%2Fq0s63W5OmSPga4%2B50b64sCkQ6G1JnFX4l8%3D" rel="nofollow" title="https://tecdat.cn/?p=44713" target="_blank">https://tecdat.cn/?p=44713</a>  <br/>原文出处：拓端抖音号@拓端tecdat</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面"/></p><h3><a name="t0" target="_blank"/>引言</h3><p>当谷歌Willow芯片实现量子纠错关键突破，中国“祖冲之三号”刷新超导量子计算性能基准，全球量子计算产业已从“实验室小众探索”迈入中美双极竞速的战略博弈新阶段。20余年技术演进，让量子计算从理论构想成为重塑全球科技版图的核心变量——一个拥有100个量子比特的系统，理论上可并行处理2^100种可能状态，这一特性让它在密码破解、药物研发、材料设计等经典计算“束手无策”的领域具备颠覆性潜力，也让中美欧等主要经济体展开了围绕技术路线、产业生态、标准制定的全方位竞争。  <br/>本报告洞察基于《发布机构：光子盒研究院：2025全球量子计算产业发展展望》和文末<strong>200+份</strong>量子计算与量子技术行业研究报告及数据，系统梳理全球技术路线竞争格局、市场规模增长逻辑、应用场景落地潜力与风险挑战，为创业者、技术决策者、投资者提供可落地的行业洞察。</p><p>本文完整报告数据图表和文末最新参考报告合集已分享在交流群，阅读原文查看、进群咨询，定制数据、报告和800+行业人士共同交流和成长。</p><p>值得警惕的是，中国在量子计算领域虽实现单点突破，但在技术路线收敛性、产业生态完整性、专利布局广度上仍与美国存在显著差距。技术路线分野、成本高企、应用场景模糊等问题，再叠加美国的技术封锁与生态壁垒，正考验着所有中国参与者的战略定力——稍有迟疑，便可能在这场关乎未来算力主权的竞赛中被系统性甩开。</p><h3><a name="t1" target="_blank"/>一、技术路线竞速：中美主导的路线之争，谁能笑到最后？</h3><p>量子计算的技术路线之争从未停歇，超导、离子阱、光量子、中性原子等路线各有优劣，但竞争的核心已从“多路线并存”转向“中美主导的生态卡位”。以下核心指标直观呈现不同路线的性能差异，更揭示了中美企业的实力差距：</p><h4><a name="t2" target="_blank"/>1. 量子比特数量对比</h4><p><strong>规模扩张的核心竞赛，中美已形成第一梯队</strong>  <br/>量子比特数是衡量量子计算机算力的基础指标，直接决定并行处理能力的上限，也是中美企业的核心竞争点。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525075" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院《2025全球量子计算产业发展展望》，包含超导、离子阱、光量子、中性原子四大主流技术路线核心设备量子比特数统计，量子比特数均指物理的、可用于计算的，逻辑的、耦合的不考虑在内。  <br/>量子比特数量对比图1数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：超导路线在比特数上暂时领先，中国“祖冲之三号”与谷歌Willow均突破105比特，但美国QuEra的中性原子路线已实现256比特规模，中国中科酷原“汉原一号”仅达到100+比特，规模化差距明显。  <br/>对应人群行动建议：创业者可优先关注超导路线的商业化机会，依托中国在该路线的单点优势快速落地；技术团队需警惕“比特数陷阱”——单纯追求数量而忽视保真度无实际意义，同时需紧盯美国中性原子路线的规模化进展，避免技术代差扩大。</p><h4><a name="t3" target="_blank"/>2. 量子门保真度对比</h4><p><strong>计算准确性的生命线，美国企业仍占绝对优势</strong>  <br/>保真度直接影响计算结果的可靠性，是量子算法落地的核心前提，美国在高精度操控技术上的积累已形成壁垒。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525076" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K技术监测数据，统计时间为2024年全年，涵盖全球15家主流量子计算硬件厂商核心产品的单比特门与双比特门保真度测试结果。  <br/>量子门保真度对比图2数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线保真度最高，美国Quantinuum的单比特门保真度超99.99%，中国华翊量子HYQ-A37约为99.9%；超导路线中，谷歌Willow双比特门保真度达99.5%，中国“祖冲之三号”约为99.0%，差距虽小但在高精度场景影响显著。  <br/>对应人群行动建议：金融、医药等对计算准确性要求极高的行业，短期可优先布局美国离子阱路线应用；中国企业需加大量子测控技术研发，缩小保真度差距，避免在核心场景被替代。</p><h4><a name="t4" target="_blank"/>3. 量子相干时间对比</h4><p><strong>量子态稳定的关键，中美路线各有优劣但差距明显</strong>  <br/>相干时间决定量子比特能保持量子态的时长，直接影响计算深度，中国在极低温环境维持技术上仍依赖进口设备。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525077" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算硬件性能基准测试平台（2024年度报告），相干时间测试环境为各技术路线标准运行环境（超导10mK、离子阱超高真空等）。  <br/>量子相干时间对比图3数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：离子阱路线相干时间最长（毫秒级），美国IonQ达到10毫秒，中国华翊量子约为2毫秒；超导路线中，谷歌Willow在自主研发的稀释制冷机支持下，相干时间达500微秒，中国“祖冲之三号”约为300微秒，设备依赖导致差距难以快速缩小。  <br/>对应人群行动建议：需要长计算周期的量子模拟场景（如材料研发）可选择美国离子阱路线，短期快速计算任务（如组合优化）中国超导路线更具优势；国内企业需加速稀释制冷机等核心设备的国产替代，从底层突破相干时间瓶颈。</p><h4><a name="t5" target="_blank"/>4. 量子门操作时间对比</h4><p><strong>计算效率的核心保障，中国超导路线具备局部优势</strong>  <br/>操作时间决定量子门执行速度，影响整体计算效率，中国在超导路线的门操作速度上实现局部反超，但应用场景有限。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525078" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院与清华大学量子信息研究中心联合测试数据，操作时间为单量子门平均执行时间，气泡大小对应技术路线成熟度评分。  <br/>量子门操作时间对比图4数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：中国超导路线操作时间最短（30纳秒），略优于谷歌Willow的40纳秒；但离子阱路线中，美国Quantinuum虽操作时间较长（100微秒），但可通过高保真度弥补效率差距，应用场景更广泛。  <br/>对应人群行动建议：高频计算场景（如实时风控）优先选择中国超导路线，对效率要求不高的科研场景可接受美国离子阱路线的速度trade-off；中国企业需扩大超导路线的应用场景覆盖，将局部优势转化为生态优势。</p><h4><a name="t6" target="_blank"/>技术路线对比表</h4><table><thead><tr><th>核心结论</th><th>数据差异</th><th>原因分析</th></tr></thead><tbody><tr><td>超导路线成当前主流，中美双雄争霸</td><td>美国比特数105-176、保真度99.5%-99.9%；中国比特数105-176、保真度99.0%-99.5%</td><td>美国与半导体工艺兼容性高，生态成熟；中国技术单点突破快，但设备依赖进口</td></tr><tr><td>离子阱路线美国稳扎稳打，中国追赶中</td><td>美国比特数32-100、保真度99.8%-99.99%；中国比特数37-100、保真度99.0%-99.9%</td><td>美国量子比特天然全同技术积累深；中国依托高校研发快速突破，但激光系统仍依赖进口</td></tr><tr><td>中性原子路线美国潜力巨大，中国起步晚</td><td>美国比特数200-256、保真度99.0%-99.5%；中国比特数100+、保真度98.5%-99.0%</td><td>美国规模化扩展成本低，可构建多维阵列；中国测控难度高，技术成熟度不足</td></tr><tr><td>光量子路线中美进展均缓慢，中国略占优</td><td>美国比特数216、保真度98.0%-99.0%；中国比特数255、保真度98.5%-99.0%</td><td>中国“九章三号”实现光子数突破，但光子纠缠操控难度大，退相干快的问题未解决</td></tr></tbody></table><p>不同路线的竞争本质是“中美生态卡位战”——没有完美的技术，只有适配的场景。当前行业共识是：短期内超导路线将主导商业落地，中国可依托该路线实现局部突破；中长期中性原子路线可能成为美国拉开差距的关键，中国需加速技术攻关；离子阱路线则成为美国巩固高精度场景优势的核心，中国需在细分领域建立差异化壁垒。</p><p><strong>相关文章</strong><img referrerpolicy="no-referrer" src="/img/remote/1460000047525079" alt="" title="" loading="lazy"/></p><h3><a name="t7" target="_blank"/>2025人工智能AI研究报告：算力、应用、风险与就业|附1000+份报告PDF、数据、可视化模板汇总下载</h3><p>原文链接：<a href="https://link.segmentfault.com/?enc=Wu18l8G51gy0HdhvU9zVfA%3D%3D.BtNMBYglGHPlbuEbx8X9M9B5LTrJ0oEzbtfOLi0S3e8%3D" rel="nofollow" title="https://tecdat.cn/?p=44642" target="_blank">https://tecdat.cn/?p=44642</a></p><h3><a name="t8" target="_blank"/>二、市场规模与投融资：资本押注的未来，中美差距正在拉大</h3><p>量子计算市场正呈现“指数级增长”态势，但资本布局的结构性差异已凸显中美产业发展的深层差距——美国聚焦生态构建，中国仍停留在单点技术突破。</p><h4><a name="t9" target="_blank"/>1. 全球量子计算市场规模预测</h4><p><strong>从十亿到万亿的跨越，中国占比仍处弱势</strong>  <br/>量子计算产业规模将在2030年后迎来爆发式增长，但中国市场占比提升缓慢，难以撼动美国主导地位。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525080" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为光子盒研究院产业预测模型，基于2024年全球50.37亿美元市场规模，结合技术迭代速度、政策支持力度、应用落地进度综合测算。  <br/>全球量子计算市场规模预测图5数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球市场规模50.37亿美元，美国占比62.7%，中国仅25.3%；2030年全球将达2199.78亿美元，中国占比预计提升至27.96%，仍落后美国30个百分点以上；2035年全球突破8000亿美元，中国占比29.49%，差距仍未缩小。  <br/>对应人群行动建议：投资者可重点布局美国上游核心器件（稀释制冷机、量子芯片）和中游整机厂商，同时关注中国国产替代机会；中国企业需加强产业链协同，避免单点作战，依托政策支持构建自主生态。</p><h4><a name="t10" target="_blank"/>2. 全球量子计算产业规模预测</h4><p><strong>中国市场自主化驱动的增长，难掩生态短板</strong>  <br/>中国量子计算市场在自主化突破下快速增长，但产业生态不完整导致增长质量不高。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525081" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为ICV TA&amp;K全球区域市场分析报告（2024），中国市场规模统计包含硬件整机、软件算法、云平台及下游应用四大板块。  <br/>全球量子计算产业规模预测图6数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国市场规模占全球25.3%，其中硬件整机占比超60%，软件算法仅占15%；而美国软件算法占比达40%，生态完整性远超中国，产业抗风险能力更强。  <br/>对应人群行动建议：国内创业者可依托政策支持，聚焦上游国产替代机会（如稀释制冷机、测控系统），同时加大软件算法研发投入，补全生态短板；海外企业可寻求与国内科研机构的合作切入点，共享中国硬件增长红利。</p><h4><a name="t11" target="_blank"/>3. 中国量子计算融资规模</h4><p><strong>本土资本的谨慎布局，单笔体量远逊美国</strong>  <br/>中国量子计算融资活跃度位列全球第二，但单笔体量偏小，反映本土资本对生态构建的信心不足。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525082" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为PitchBook 2024量子技术投资报告及光子盒研究院投融资监测数据，融资规模统计包含种子轮、天使轮、A/B/C轮及政府资助。  <br/>中国量子计算融资规模图7数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年中国融资规模0.47亿美元，交易笔数6笔，单笔平均0.078亿美元；美国融资规模12.60亿美元，交易笔数17笔，单笔平均0.741亿美元，单笔体量是中国的9.5倍。  <br/>对应人群行动建议：国内初创企业需突出技术差异化与国产替代价值争取融资，避免单纯追求比特数突破；政府引导基金可加大对中游整机厂商的长期投入，同时设立专项基金支持软件生态建设，改变“重硬件、轻软件”的融资格局。</p><h4><a name="t12" target="_blank"/>4. 量子计算企业融资额</h4><p><strong>全球融资头部集中效应显著，美国企业垄断核心资源</strong>  <br/>全球融资向技术成熟、具备生态构建能力的企业集中，美国企业占据绝对主导地位，中国企业难获大额融资。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525083" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为全球量子计算投融资数据库（2024），统计范围为全球量子计算硬件、软件、云平台相关企业公开融资事件。  <br/>量子计算企业融资额图8数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：2024年全球融资20.15亿美元，美国企业占比62.7%，PsiQuantum以6.246亿美元获最大轮融资，Quantinuum、Q-CTRL紧随其后；中国最大单笔融资仅0.1亿美元，且集中在硬件领域，软件企业融资困难。  <br/>对应人群行动建议：中国初创企业需聚焦细分技术痛点（如低温测控、量子纠错）建立壁垒，避免与美国巨头正面竞争；投资者可关注“硬件+软件”一体化布局的中国企业，降低单一环节风险，同时警惕纯硬件企业的技术迭代风险。</p><h4><a name="t13" target="_blank"/>5. 量子技术专利数量</h4><p><strong>知识产权的全球博弈，中国基础专利差距明显</strong>  <br/>专利数量反映国家技术积累，美国仍占据绝对优势，中国在基础专利上的短板可能制约长期发展。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525084" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为Patsnap量子技术专利分析报告（截至2024年12月），统计范围为全球量子计算核心技术相关授权专利。  <br/>量子技术专利数量图9数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：截至2024年，美国量子技术专利18649件，基础专利占比超40%；中国7601件，基础专利仅占15%，多为应用层专利；日本、德国分别以9400件、8500件位列第二、三位，基础专利布局均优于中国。  <br/>对应人群行动建议：国内企业需加强核心技术专利布局，避免陷入“低端专利陷阱”，重点突破量子芯片、量子纠错等基础领域专利；科研机构可聚焦基础理论与核心器件专利突破，提升行业话语权，减少对美国基础专利的依赖。</p><h3><a name="t14" target="_blank"/>三、应用场景落地：从实验室到产业的跨越，中美应用深度差距显著</h3><p>量子计算的终极价值在于产业赋能，当前已在多个领域展现出落地潜力，但美国在应用深度与广度上已形成优势，中国仍处于试点阶段。</p><h4><a name="t15" target="_blank"/>1. 计算加速倍数</h4><p><strong>效率革命的开始，美国应用场景更广泛</strong>  <br/>量子计算在特定场景实现指数级加速，但美国已在多领域形成规模化应用，中国仍以科研试点为主。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525085" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为麦肯锡《The state of AI in 2025: Agents, innovation, and transformation》，加速倍数为量子计算与经典超级计算机在相同任务下的效率对比。  <br/>计算加速倍数图10数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：基因组组装加速1000倍，美国已应用于生物医药企业的新药研发；AI模型训练加速15.2倍，美国谷歌、微软已用于大模型优化；电力故障定位加速1.34倍，中国仅在个别电力企业试点，应用范围有限。  <br/>对应人群行动建议：中国生物医药企业可优先布局基因组组装、药物研发场景，依托“九章三号”光量子计算机的局部优势快速验证价值；能源企业可聚焦电力优化等轻量级应用，逐步探索深度融合，避免盲目跟风美国的大规模应用。</p><h4><a name="t16" target="_blank"/>2. 预测准确率提升</h4><p><strong>决策质量的提升，中国在核心场景应用滞后</strong>  <br/>量子算法显著提升预测准确率，但中国在金融、医药等核心场景的应用滞后美国3-5年。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525086" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为量子前哨《量子技术赋能金融风控与定价管理白皮书》，测试场景包含金融风控、生物制药分子对接等核心应用领域。  <br/>预测准确率提升图11数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：金融风控准确率提升18.5%，美国摩根大通、花旗已用于信贷风控和衍生品定价；生物制药分子对接提升14.3%，美国辉瑞、阿斯利康已纳入药物研发流程；中国仅个别头部企业开展POC测试，尚未规模化应用。  <br/>对应人群行动建议：中国金融机构可先在信贷风控、衍生品定价场景试点，依托量子云平台降低投入成本；医药企业可与量子计算公司合作开展药物分子模拟，快速验证价值，避免在核心场景被美国企业拉开代差。</p><h4><a name="t17" target="_blank"/>3. 后量子密码市场规模</h4><p><strong>应对量子安全威胁，中国PQC迁移进展缓慢</strong>  <br/>量子计算的发展带来密码安全风险，后量子密码（PQC）成为刚需，但中国PQC迁移进展滞后于美国和欧盟。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525087" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为朗空量子《全球抗量子迁移战略白皮书（2025）》，市场规模预测基于全球关键基础设施PQC迁移需求测算。  <br/>后量子密码市场规模图12数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：后量子密码市场规模将随量子计算成熟度同步增长，2030年将突破百亿规模；美国已完成关键基础设施PQC试点，欧盟2030年将全面完成迁移，中国仍处于标准制定阶段，迁移进度滞后2-3年。  <br/>对应人群行动建议：中国政府、金融、电信等关键基础设施行业需加快PQC迁移规划，避免“量子威胁”冲击；企业可先开展密码系统风险评估，依托国内PQC技术企业开展试点，降低对国外算法的依赖。</p><h4><a name="t18" target="_blank"/>4. NIST算法公钥长度比较</h4><p><strong>技术适配的关键，中国算法适配能力不足</strong>  <br/>不同PQC算法公钥长度差异显著，影响设备适配性与传输效率，中国在算法适配与设备兼容上仍落后。  </p><p>注释：数据来源为NIST后量子密码标准化项目（Round 4）测试数据，公钥长度为各算法标准实现的平均长度。  <br/>NIST算法公钥长度比较图13数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：NIST标准化PQC算法中，美国企业已实现全场景适配，支持从物联网设备到金融核心系统的全覆盖；中国仅能适配部分场景，短公钥长度算法在物联网设备的兼容性不足，长公钥算法在金融系统的传输效率问题未解决。  <br/>对应人群行动建议：中国物联网企业可优先选择短公钥长度算法（如SLH-DSA），与国内PQC企业联合优化兼容性；金融机构等核心场景可选用高安全性算法（如CRYSTALS-Kyber），同时加大传输效率优化投入，平衡安全与效率。</p><h4><a name="t19" target="_blank"/>5. 迁移政策时间线</h4><p><strong>全球PQC迁移协同推进，中国需加快节奏</strong>  <br/>各国明确PQC迁移时间表，形成全球协同防控量子安全风险的格局，中国迁移目标虽与美国一致，但执行力度需加强。  <br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525088" alt="" title="" loading="lazy"/>  <br/>注释：数据来源为信通院《量子信息领域的国家战略布局与研发趋势分析》，迁移目标完成年份为各国官方发布的量子安全相关政策明确时间。  <br/>迁移政策时间线图14数据EXCEL及图表PDF模板已分享到会员群  <br/>3秒解读：美国、中国计划2035年完成PQC迁移，欧盟提前至2030年；美国已出台分阶段迁移细则，明确各行业责任主体；中国虽明确目标，但缺乏具体执行方案，关键基础设施行业的迁移动力不足。  <br/>对应人群行动建议：中国跨国企业需按区域政策要求制定分阶段迁移计划，避免因合规问题影响海外业务；国内企业可依托国家级专项推进自主PQC技术落地，积极参与国际标准制定，提升话语权。</p><h3><a name="t20" target="_blank"/>四、风险提示与应对方案</h3><p>量子计算产业看似前景光明，但暗藏多重“陷阱”，中国企业需比美国更谨慎应对，避免在技术迭代与生态竞争中被淘汰：</p><h4><a name="t21" target="_blank"/>1. 技术路线收敛风险</h4><p><strong>风险描述</strong>：当前技术路线未收敛，美国已形成“超导+中性原子”双主线布局，中国企业多押注单一路线，可能面临“满盘皆输”的风险，如早期光量子路线企业因技术瓶颈陷入发展困境。  <br/><strong>具体应对方案</strong>：采用“主线+支线”布局策略，核心业务聚焦主流路线（如超导），同时小比例投入潜力路线（如中性原子）；加入行业联盟，实时跟踪美国技术路线演进动态，每半年评估一次路线优先级，避免与美国主流路线偏离。  <br/><strong>社群支持</strong>：交流群定期分享美国技术路线进展报告，组织技术选型闭门会，邀请光子盒研究院专家提供定制化建议，对接上下游企业资源，帮助中国企业快速调整技术方向。</p><h4><a name="t22" target="_blank"/>2. 成本高企风险</h4><p><strong>风险描述</strong>：量子计算机硬件成本动辄数亿元，运维成本（如极低温环境）高昂，美国企业可通过生态协同分摊成本，中国中小企业难以承受，单台超导量子计算机年运维成本超千万元，远超美国企业的600万元。  <br/><strong>具体应对方案</strong>：优先采用量子云平台（如中电信“天衍”平台）按需付费，避免重资产投入；聚焦细分场景的轻量化应用，降低算力需求，控制初期投入规模；联合高校、科研机构共享设备资源，分摊运维成本。  <br/><strong>社群支持</strong>：整理全球量子云平台对比手册，重点标注中美平台成本差异，提供中国平台优惠资源对接，组织中小企业量子计算应用试点对接会，帮助企业降低试点成本。</p><h4><a name="t23" target="_blank"/>3. 政策合规风险</h4><p><strong>风险描述</strong>：量子技术涉及国家安全，各国政策限制（如禁运、出口管制）日益严格，欧美已将稀释制冷机、量子芯片等纳入禁运清单，中国企业核心器件进口难度加大，技术迭代受阻。  <br/><strong>具体应对方案</strong>：国内企业加强自主化研发，重点突破稀释制冷机、测控系统等“卡脖子”环节，减少核心器件进口依赖；建立供应链风险预警机制，提前储备替代资源；避免违规合作，优先选择政策友好区域布局海外业务。  <br/><strong>社群支持</strong>：及时更新全球量子技术政策数据库，重点标注美国对华技术封锁清单，提供合规咨询对接服务，组织政策解读直播，帮助企业把握政策导向与机遇，规避合规风险。</p><h3><a name="t24" target="_blank"/>五、可落地的3件事</h3><ol><li>开展“量子就绪”评估：梳理企业核心业务中的计算瓶颈，对比中美应用场景差异，判断是否适合量子计算赋能，优先选择美国已验证、中国有技术基础的场景（如金融风控、药物研发），形成《量子应用潜力评估报告》，避免盲目跟风。</li><li>小步试点验证价值：与国内量子云平台合作开展POC（概念验证），投入少量资源测试量子算法效果，比如金融企业可试点量子组合优化算法优化投资组合，医药企业可测试量子模拟加速药物分子筛选，重点验证国产技术的可行性，避免过度依赖美国平台。</li><li>储备量子人才与专利：招聘具备量子计算基础的技术人员，或对现有团队开展量子技术培训（如参加量旋科技“量子计算实训营”），建立人才护城河；同时对接高校量子信息专业，搭建校企人才输送通道，加强核心技术专利布局，尤其是基础专利，减少对美国专利的依赖。</li></ol><h3><a name="t25" target="_blank"/>六、核心数据表格</h3><h4><a name="t26" target="_blank"/>1. 主要技术路线核心性能指标表</h4><table><thead><tr><th>技术路线</th><th>量子比特数</th><th>单比特门保真度</th><th>相干时间</th><th>单量子门操作时间</th><th>代表企业/设备（美国）</th><th>代表企业/设备（中国）</th></tr></thead><tbody><tr><td>超导</td><td>105-176</td><td>99.5%-99.9%</td><td>400-500微秒</td><td>40纳秒</td><td>谷歌Willow</td><td>中国“祖冲之三号”</td></tr><tr><td>离子阱</td><td>32-100</td><td>99.8%-99.99%</td><td>5-10毫秒</td><td>100微秒</td><td>Quantinuum H2-1、IonQ Forte</td><td>华翊量子HYQ-A37、幺正量子UQM1</td></tr><tr><td>光量子</td><td>216</td><td>98.0%-99.0%</td><td>50-100微秒</td><td>10微秒</td><td>Xanadu Borealis</td><td>中国“九章三号”</td></tr><tr><td>中性原子</td><td>200-256</td><td>99.0%-99.5%</td><td>800微秒-1秒</td><td>1微秒</td><td>QuEra Aquila</td><td>中科酷原“汉原一号”</td></tr></tbody></table><h4><a name="t27" target="_blank"/>2. 全球量子计算市场规模预测表（单位：亿美元）</h4><table><thead><tr><th>年份</th><th>全球市场规模</th><th>美国市场规模</th><th>中国市场规模</th><th>美国占比</th><th>中国占比</th><th>中国年复合增长率</th></tr></thead><tbody><tr><td>2024</td><td>50.37</td><td>31.58</td><td>12.74</td><td>62.70%</td><td>25.30%</td><td>-</td></tr><tr><td>2027</td><td>111.75</td><td>69.90</td><td>30.00</td><td>62.55%</td><td>26.84%</td><td>29.5%</td></tr><tr><td>2030</td><td>2199.78</td><td>1389.26</td><td>615.00</td><td>63.16%</td><td>27.96%</td><td>173.2%</td></tr><tr><td>2035</td><td>8077.50</td><td>5153.00</td><td>2382.00</td><td>63.80%</td><td>29.49%</td><td>29.8%</td></tr></tbody></table><h4><a name="t28" target="_blank"/>3. 量子计算应用场景价值表</h4><table><thead><tr><th>应用场景</th><th>加速倍数</th><th>准确率提升</th><th>落地周期</th><th>美国落地状态</th><th>中国落地状态</th><th>产业估值（2035年，亿美元）</th></tr></thead><tbody><tr><td>基因组组装</td><td>1000倍</td><td>-</td><td>3-5年</td><td>规模化应用</td><td>科研试点</td><td>-</td></tr><tr><td>AI模型训练</td><td>15.2倍</td><td>-</td><td>5-8年</td><td>企业试点</td><td>实验室阶段</td><td>-</td></tr><tr><td>电力故障定位</td><td>1.34倍</td><td>-</td><td>2-3年</td><td>行业应用</td><td>个别试点</td><td>-</td></tr><tr><td>金融风控</td><td>-</td><td>18.5%</td><td>3-5年</td><td>规模化应用</td><td>POC测试</td><td>7000（乐观估值）</td></tr><tr><td>生物制药分子对接</td><td>-</td><td>14.3%</td><td>5-8年</td><td>企业应用</td><td>科研合作</td><td>1830（乐观估值）</td></tr></tbody></table><h4><a name="t29" target="_blank"/>4. 主要经济体PQC迁移政策表</h4><table><thead><tr><th>经济体</th><th>迁移目标完成年份</th><th>核心要求</th><th>重点领域</th><th>执行进度</th></tr></thead><tbody><tr><td>美国</td><td>2035年</td><td>禁用传统密码算法，强制采用NIST标准化PQC算法</td><td>国防、金融、电信</td><td>分阶段执行中</td></tr><tr><td>中国</td><td>2035年</td><td>自主PQC技术落地，关键基础设施率先完成迁移</td><td>金融、能源、政务</td><td>标准制定阶段</td></tr><tr><td>欧盟</td><td>2030年</td><td>关键基础设施完成PQC升级，建立跨境互认机制</td><td>能源、交通、医疗</td><td>全面推进中</td></tr></tbody></table><h3><a name="t30" target="_blank"/>七、数据图表列表</h3><ol><li>量子比特数量对比图1.pdf</li><li>量子门保真度对比图2.pdf</li><li>量子相干时间对比图3.pdf</li><li>量子门操作时间对比图4.pdf</li><li>全球量子计算市场规模预测图5.pdf</li><li>中国量子计算融资规模图6.pdf</li><li>全球量子计算产业规模预测图7.pdf</li><li>量子计算企业融资额图8.pdf</li><li>量子技术专利数量图9.pdf</li><li>计算加速倍数图10.pdf</li><li>预测准确率提升图11.pdf</li><li>后量子密码市场规模图12.pdf</li><li>NIST算法公钥长度比较图13.pdf</li><li>迁移政策时间线图14.pdf</li></ol><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525074" alt="封面" title="封面" loading="lazy"/></p><h3><a name="t31" target="_blank"/>本专题内的参考报告（PDF）目录</h3><p>⦁    PQC-X实验室：全球金融银行业后量子安全迁移白皮书（2025）.pdf  <br/>⦁    2026-01-05 20:34  <br/>⦁    2025年量子技术：健康与医疗保健领导者的战略要务报告.pdf  <br/>⦁    2026-01-03 10:50  <br/>⦁    朗空量子：全球抗量子迁移战略白皮书（2025）.pdf  <br/>⦁    2025-12-31 15:52  <br/>⦁    2025年全球量子生态全景洞察报告：基于创新、企业、投资、技能、贸易及政策数据的综合研究（英文版）.pdf  <br/>⦁    2025-12-29 15:59  <br/>⦁    量子计算行业深度：行业概况、发展趋势、产业链及相关公司深度梳理.pdf  <br/>⦁    2025-12-29 15:53  <br/>⦁    软件与服务行业量子信息技术专题研究报告（二）：科技巨头加速布局，量子产业前景可期.pdf  <br/>⦁    2025-12-24 15:30  <br/>⦁    量子位；2025年度AI十大趋势报告.pdf  <br/>⦁    2025-12-17 16:16  <br/>⦁    量子信息技术发展与应用研究报告（2025年）-中国信通院.pdf  <br/>⦁    2025-12-16 16:29  <br/>⦁    长江证券：量子计算：从“量子优越性”到产业优越性.pdf  <br/>⦁    2025-12-04 16:45  <br/>⦁    ATARC：2025年揭秘当今与未来量子技术能力白皮书汇编（英文版）.pdf  <br/>⦁    2025-11-30 09:17  <br/>⦁    量子科技行业深度报告：量子科技驱动产业变革，激活经济增长新引擎.pdf  <br/>⦁    2025-11-30 09:11  <br/>⦁    2025版量子计算 生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-11-24 15:06  <br/>⦁    2025量子计算+生物制药产业与技术发展研究报告.pdf  <br/>⦁    2025-11-19 15:28  <br/>⦁    2025量子技术：先进制造和供应链的关键机遇白皮书（英文版）.pdf  <br/>⦁    2025-11-08 17:47  <br/>⦁    量子科技行业深度报告：量子革命，量子科技的现状与未来.pdf  <br/>⦁    2025-10-30 15:17  <br/>⦁    2025年量子计算驱动的电力系统弹性提升-探索与展望报告.pdf  <br/>⦁    2025-10-29 16:27  <br/>⦁    海通国际：量子科技行业深度报告：量子革命：量子科技的现状与未来.pdf  <br/>⦁    2025-10-28 16:17  <br/>⦁    电子行业深度报告：量子深潜-计算篇：从比特到Qubit的范式转移.pdf  <br/>⦁    2025-10-26 08:49  <br/>⦁    量子计算硬件深度报告：行业奇点将至，硬件破局当时.pdf  <br/>⦁    2025-10-16 15:09  <br/>⦁    中航证券：量子信息：引领未来全球科技变革之关键力量.pdf  <br/>⦁    2025-10-15 15:17  <br/>⦁    中移智库：移动网络中量子计算应用能力评估模型（2025年）.pdf  <br/>⦁    2025-09-30 16:39  <br/>⦁    中国信通院：量子计算发展态势研究报告（2025年）.pdf  <br/>⦁    2025-09-26 14:27  <br/>⦁    2025量子信息行业研究报告.pdf  <br/>⦁    2025-09-21 17:17  <br/>⦁    计算机行业深度研究：后量子密码技术：应对量子计算威胁的关键防线.pdf  <br/>⦁    2025-09-17 16:27  <br/>⦁    后量子密码学（PQC）测试研究白皮书.pdf  <br/>⦁    2025-09-12 16:36  <br/>⦁    麻省理工学院：2025年量子指数报告（英文版）.pdf  <br/>⦁    2025-09-03 16:55  <br/>⦁    量子计算专题：下一代计算革命，关注核心设备环节.pdf  <br/>⦁    2025-09-01 16:24  <br/>⦁    AI Coding玩家图谱【量子位智库】.pdf  <br/>⦁    2025-08-31 17:47  <br/>⦁    未来网络发展大会：2025量子互联网与算网协同体系架构白皮书.pdf  <br/>⦁    2025-08-23 17:17  <br/>⦁    2025中国量子计算产业市场现状及发展前景研究报告.pdf  <br/>⦁    2025-08-16 16:49  <br/>⦁    2025年全球科技行业：量子计算将如何影响AI发展？（英文版）.pdf  <br/>⦁    2025-08-05 15:31  <br/>⦁    量子位智库：2025上半年AI核心成果及趋势报告.pdf  <br/>⦁    2025-08-02 16:21  <br/>⦁    2025年全球量子计算新进展深度分析报告.pdf  <br/>⦁    2025-08-02 16:16  <br/>⦁    2025年中国联通后量子密码白皮书-中国联通.pdf  <br/>⦁    2025-07-30 16:14  <br/>⦁    量子位智库：2025年AI+游戏产业变革研究报告.pdf  <br/>⦁    2025-07-17 15:53  <br/>⦁    麦肯锡：量子之年：从2025年从概念到现实报告（英文版）.pdf  <br/>⦁    2025-07-13 08:36  <br/>⦁    后量子密码技术白皮书（2025）-东进技术.pdf  <br/>⦁    2025-07-07 16:52  <br/>⦁    2024年量子技术在金融通信安全领域的应用研究报告.pdf  <br/>⦁    2025-07-02 16:38  <br/>⦁    浙商证券-量子科技行业深度报告：超越经典，面向未来.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    政策与战略专题报告：量子科技：产业革命核心赛道，投资风口将至.pdf  <br/>⦁    2025-06-28 16:57  <br/>⦁    2024年量子传感在位置、导航和定时应用中的案例（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    2024年量子技术在金融消息传递中的应用报告（英文版）.pdf  <br/>⦁    2025-06-25 16:32  <br/>⦁    应对量子威胁：SIM体系抗量子密码迁移白皮书（2025年）.pdf  <br/>⦁    2025-06-19 16:02  <br/>⦁    量子算法在金融风控与定价管理领域的应用研究.pdf  <br/>⦁    2025-06-18 15:27  <br/>⦁    2024年量子计算在交通运输与物流领域的应用研究报告（英文版）.pdf  <br/>⦁    2025-06-12 15:35  <br/>⦁    通信行业动态报告：量子计算光量子技术路线进展加速，未来大有可为.pdf  <br/>⦁    2025-06-12 15:34  <br/>⦁    量子位智库：2025大模型架构创新研究报告.pdf  <br/>⦁    2025-06-06 15:38  <br/>⦁    量子位智库：2025年AI眼镜「预选赛」格局报告.pdf  <br/>⦁    2025-06-05 16:09  <br/>⦁    鼎帷咨询：2025年美国量子技术发展研究报告.pdf  <br/>⦁    2025-06-02 08:57  <br/>⦁    2025年全球量子计算用同轴电缆市场分析报告-光子盒研究院.pdf  <br/>⦁    2025-05-26 16:58  <br/>⦁    量子位智库：2025年AI智能助手的SEO策略变革研究报告.pdf  <br/>⦁    2025-05-24 16:38  <br/>⦁    2025美韩科技合作报告：电池、生物技术与量子技术（英文）.pdf  <br/>⦁    2025-05-20 17:05  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年营养健康食品专题报告.pdf  <br/>⦁    2025-05-17 16:13  <br/>⦁    2025年量子技术与未来学习研究报告（英文版）.pdf  <br/>⦁    2025-05-14 16:34  <br/>⦁    Globant：2024年科技趋势报告-人工智能、量子技术、机器人等将如何塑造未来一年（英文版）.pdf  <br/>⦁    2025-05-01 17:54  <br/>⦁    量子计算：打破维度瓶颈，开启化学的“算力革命”.pdf  <br/>⦁    2025-04-29 15:55  <br/>⦁    量子位智库：2025年空间智能研究报告.pdf  <br/>⦁    2025-04-28 17:23  <br/>⦁    量子信息网络产业联盟：2025年光量子计算技术产业研究报告.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    量子信息网络产业联盟：量子计算云平台接口研究报告（2024）.pdf  <br/>⦁    2025-04-27 13:27  <br/>⦁    2025年量子密钥无线分发技术研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年量子计算应用能力指标与测评研究报告.pdf  <br/>⦁    2025-04-26 14:29  <br/>⦁    2025年经典计算与多制式量子计算异构融合研究报告.pdf  <br/>⦁    2025-04-26 14:27  <br/>⦁    量子信息技术应用案例集（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    量子信息技术产业发展研究报告（2024年）.pdf  <br/>⦁    2025-04-26 14:25  <br/>⦁    2025版量子计算+生物制药白皮书-量子前哨智库.pdf  <br/>⦁    2025-04-21 10:06  <br/>⦁    量子位智库：2025年中国AIGC应用全景图谱报告..pdf  <br/>⦁    2025-04-19 14:49  <br/>⦁    量子计算行业深度：市场现状、发展趋势、产业链及相关企业深度梳理.pdf  <br/>⦁    2025-03-26 15:33  <br/>⦁    2025年全球量子技术专利态势分析白皮书（英文版）.pdf  <br/>⦁    2025-03-13 17:11  <br/>⦁    光子盒：2025年全球量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-12 15:49  <br/>⦁    光子盒：2025年量子科技产业发展展望报告.pdf  <br/>⦁    2025-03-05 15:24  <br/>⦁    中国在量子领域有多大创新性？.pdf  <br/>⦁    2025-03-04 16:09  <br/>⦁    光子盒：2025年全球量子传感产业发展展望报告.pdf  <br/>⦁    2025-03-01 16:55  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:38  <br/>⦁    光子盒：2025年全球量子安全产业发展展望报告.pdf  <br/>⦁    2025-02-28 16:37  <br/>⦁    光子盒：2025年全球量子计算产业发展展望报告.pdf  <br/>⦁    2025-02-27 14:57  <br/>⦁    2024年量子安全威胁及其对国内金融行业的影响研究报告.pdf  <br/>⦁    2025-02-18 15:53  <br/>⦁    2025年拥抱量子经济：企业领袖的前进之路洞察报告（英文版）.pdf  <br/>⦁    2025-01-22 16:12  <br/>⦁    量子位智库：智能驾驶2024年度报告.pdf  <br/>⦁    2025-01-17 13:14  <br/>⦁    量子信息技术国内外标准化进展报告（2024）.pdf  <br/>⦁    2025-01-15 15:49  <br/>⦁    ITIF：2023年美国的量子政策方针研究报告（英文版）.pdf  <br/>⦁    2025-01-13 10:17  <br/>⦁    2024年量子计算性能评估基准研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    2024年基于量子安全的分布式容错云存储应用场景研究报告.pdf  <br/>⦁    2025-01-10 16:35  <br/>⦁    量子信息技术发展与应用研究报告（2024年）.pdf  <br/>⦁    2024-12-28 16:56  <br/>⦁    移动网络中量子计算应用能力评测白皮书1.0（2024 年）.pdf  <br/>⦁    2024-12-26 15:46  <br/>⦁    2024年量子技术研究报告：投资于拐点（英文版）.pdf  <br/>⦁    2024-12-24 17:14  <br/>⦁    2024年度AI十大趋势报告-量子位.pdf  <br/>⦁    2024-12-14 15:10  <br/>⦁    量子安全技术蓝皮书2024.pdf  <br/>⦁    2024-12-09 16:56  <br/>⦁    量子位智库：2024年大模型落地与前沿趋势研究报告.pdf  <br/>⦁    2024-12-08 16:33  <br/>⦁    2024年量子计算与人工智能：无声的革命报告.pdf  <br/>⦁    2024-12-01 20:55  <br/>⦁    量子位智库：Robotaxi2024年度格局报告.pdf  <br/>⦁    2024-11-30 20:19  <br/>⦁    2023全球量子政策研究报告-光子盒.pdf  <br/>⦁    2024-11-12 16:46  <br/>⦁    量子技术助力社会_实现可持续发展目标.pdf  <br/>⦁    2024-10-19 16:36  <br/>⦁    世界经济论坛：2024年量子技术助力社会：实现可持续发展目标报告（英文版）.pdf  <br/>⦁    2024-10-19 16:30  <br/>⦁    2024中国量子计算应用潜力洞察报告.pdf  <br/>⦁    2024-10-10 15:21  <br/>⦁    2024年AI大模型创业格局报告-量子位智库.pdf  <br/>⦁    2024-10-06 15:17  <br/>⦁    AI教育硬件全景报告【量子位智库】.pdf  <br/>⦁    2024-09-30 15:14  <br/>⦁    量子计算发展态势研究报告（2024年）-中国信通院.pdf  <br/>⦁    2024-09-27 15:55  <br/>⦁    三未信安：抗量子密码技术与应用白皮书（2024）.pdf  <br/>⦁    2024-09-15 15:20  <br/>⦁    光子盒：2024上半年全球量子计算产业发展展望报告.pdf  <br/>⦁    2024-09-14 16:42  <br/>⦁    CIC灼识咨询&amp;量子之歌_中国中老年市场白皮书.pdf  <br/>⦁    2024-09-14 16:39  <br/>⦁    欧洲专利局：2023年量子计算洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:21  <br/>⦁    欧洲专利局：2023年量子模拟洞察力报告（英文版）.pdf  <br/>⦁    2024-09-06 16:20  <br/>⦁    2024量子计算技术全景报告-星河智源.pdf  <br/>⦁    2024-09-05 16:36  <br/>⦁    利亚德&amp;赛富乐斯半导体：2024年T003-量子点（QD-mLED）直显解决方案白皮书.pdf  <br/>⦁    2024-08-31 17:26  <br/>⦁    甲子大脑全球首发：以量子人工智能重新定义智库.pdf  <br/>⦁    2024-08-30 17:46  <br/>⦁    麦肯锡数字量子技术监测.pdf  <br/>⦁    2024-08-27 16:28  <br/>⦁    iCV TA&amp;K：2024年全球量子独角兽企业发展概览报告（英文版）.pdf  <br/>⦁    2024-08-27 16:18  <br/>⦁    光子盒：2024全球量子产业发展现状及展望报告.pdf  <br/>⦁    2024-08-18 17:30  <br/>⦁    尺度定律科普报告【量子位智库】 .pdf  <br/>⦁    2024-08-04 20:05  <br/>⦁    AI视频生成研究报告（2024年）-量子位.pdf  <br/>⦁    2024-07-30 16:30  <br/>⦁    2024中国具身智能创投报告-量子位智库.pdf  <br/>⦁    2024-07-27 17:08  <br/>⦁    AI音乐应用产业报告【量子位智库】.pdf  <br/>⦁    2024-07-22 16:40  <br/>⦁    计算机行业量子科技：见微知著、革故鼎新-国投证券.pdf  <br/>⦁    2024-07-17 10:39  <br/>⦁    光子盒：2024争夺量子优势的芬兰-国家量子战略的政策建议报告.pdf  <br/>⦁    2024-07-10 11:25  <br/>⦁    数据创新中心：2023美国的量子政策报告（英文版）.pdf  <br/>⦁    2024-07-04 11:00  <br/>⦁    赛迪报告：电子信息研究2024年第1期（总第95期）《量子产业发展白皮书》.pdf  <br/>⦁    2024-07-01 09:31  <br/>⦁    ...】2023年中国中老年市场白皮书-中老年服务及产品 “人-货-场”三维解析-CIC灼识咨询&amp;量子之歌.pdf  <br/>⦁    2024-06-28 10:40  <br/>⦁    头豹研究院-企业竞争图谱：2024年量子计算 头豹词条报告系列.pdf  <br/>⦁    2024-06-28 10:39  <br/>⦁    后量子密码迁移白皮书（2024）-西电广研院&amp;LRINF-.pdf  <br/>⦁    2024-06-27 11:20  <br/>⦁    国信证券-海外铜企专题3-第一量子-FM.TO-：高成长性的铜矿公司.pdf  <br/>⦁    2024-06-18 12:51  <br/>⦁    光子盒-量子准备：向后量子密码迁移.pdf  <br/>⦁    2024-06-15 11:02  <br/>⦁    2024上海量子科技产业发展白皮书.pdf  <br/>⦁    2024-06-08 13:03  <br/>⦁    后量子密码应用研究报告（2023年) .pdf  <br/>⦁    2024-06-07 10:11  <br/>⦁    “十五五”时期我国量子产业发展形势研判及思路建议.pdf  <br/>⦁    2024-06-05 10:15  <br/>⦁    西南证券-量子科技专题：量子应用逐步落地，关注政策支持.pdf  <br/>⦁    2024-05-31 14:53  <br/>⦁    量子科技专题系列一：逐梦量子，星辰大海.pdf  <br/>⦁    2024-05-13 13:31  <br/>⦁    通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-05-05 17:55  <br/>⦁    解读新质生产力：量子计算：打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-05-05 17:54  <br/>⦁    计算机：量子加密，一片新蓝海.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    计算机行业深度研究：抢先布局量子信息技术革命.pdf  <br/>⦁    2024-05-05 17:53  <br/>⦁    2024全球6G技术大会-面向6G时代前沿技术初探：量子信息技术-英文.pdf  <br/>⦁    2024-05-01 11:47  <br/>⦁    面向6G时代前沿技术初探：量子信息技术2024白皮书-29页.pdf  <br/>⦁    2024-05-01 11:44  <br/>⦁    2024年面向6G时代前沿技术初探量子信息技白皮书-全球6G技术大会.pdf  <br/>⦁    2024-04-30 14:38  <br/>⦁    2024量子加密，一片新蓝海.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    2024解读新质生产力：量子计算，打破传统范式，通用计算应用可期.pdf  <br/>⦁    2024-04-30 14:33  <br/>⦁    中国银河-通信行业深度报告：量子信息技术大发展，产业升级赋能新质生产力.pdf  <br/>⦁    2024-04-29 12:42  <br/>⦁    抢先布局量子信息技术革命.PDF  <br/>⦁    2024-04-27 10:25  <br/>⦁    量子位：2024中国AIGC应用全景报告.pdf  <br/>⦁    2024-04-26 11:21  <br/>⦁    华鑫证券-量子信息技术行业专题报告：优化运算法则，重塑安全格局.pdf  <br/>⦁    2024-04-20 12:06  <br/>⦁    量子化学方法的开发及其在能源环境材料研究中的应用-赵焱.pdf  <br/>⦁    2024-04-17 10:11  <br/>⦁    量子通信金融应用研究报告.pdf  <br/>⦁    2024-04-11 10:53  <br/>⦁    计算机行业深度报告：量子信息：下一场信息革命.pdf  <br/>⦁    2024-04-07 10:10  <br/>⦁    计算机行业深度研究-量子计算-人工智能与新质生产力的“未来引擎”-民生证券.pdf  <br/>⦁    2024-03-25 14:44  <br/>⦁    计算机行业深度研究：量子计算：人工智能与新质生产力的“未来引擎”.pdf  <br/>⦁    2024-03-24 10:43  <br/>⦁    量子精密测量行业赋能白皮书.pdf  <br/>⦁    2024-03-18 11:08  <br/>⦁    2024量子精密测量产业发展展望.pdf  <br/>⦁    2024-03-06 14:33  <br/>⦁    2024全球量子通信与安全产业发展展望报告-光子盒.pdf  <br/>⦁    2024-02-29 15:40  <br/>⦁    量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-28 11:37  <br/>⦁    北京金融科技产业联盟：2024量子最优化算法在金融业的应用研究报告.pdf  <br/>⦁    2024-02-26 16:13  <br/>⦁    2024全球量子计算产业发展展望.pdf  <br/>⦁    2024-02-22 11:00  <br/>⦁    量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子信息技术产业发展报告（2023年）.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-14 16:21  <br/>⦁    AI制药深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    存算一体芯片深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:23  <br/>⦁    隐私计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    计算生物深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    脑机接口深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    虚拟人深度产业报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    类脑计算神经拟态计算深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    卫星互联网深度报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    2021十大前沿科技趋势报告-量子位.pdf  <br/>⦁    2024-02-09 10:22  <br/>⦁    量子信息网络产业联盟：2024量子人工智能技术白皮书.pdf  <br/>⦁    2024-02-07 15:02  <br/>⦁    量子信息网络产业联盟：2024量子计算云平台功能模型、体系架构与能力分级研究报告.pdf  <br/>⦁    2024-02-06 15:05  <br/>⦁    量子信息网络产业联盟：量子信息技术应用案例集（2023年）.pdf  <br/>⦁    2024-02-06 15:04  <br/>⦁    量子信息网络产业联盟：2024量子汇编语言和量子中间表示发展白皮书.pdf  <br/>⦁    2024-02-05 16:04  <br/>⦁    2023年ARMR技术深度产业报告-量子位智库.pdf  <br/>⦁    2024-01-26 15:18  <br/>⦁    量子位：2024中国AIGC广告营销产业全景报告.pdf  <br/>⦁    2024-01-25 15:09  <br/>⦁    量子十年-2024量子计算未来趋势展望报告第四版-英文版-IBM商业价值研究院.pdf  <br/>⦁    2024-01-24 14:34  <br/>⦁    中国信通院：量子计算发展态势研究报告（2023年）.pdf  <br/>⦁    2024-01-02 14:36  <br/>⦁    量子测量技术发展蓝皮书.pdf  <br/>⦁    2023-12-30 10:11  <br/>⦁    中国信通院：量子信息技术发展与应用研究报告（2023年）.pdf  <br/>⦁    2023-12-29 14:43  <br/>⦁    欧洲量子技术关键绩效指标（2023年9月）（英文版）.pdf  <br/>⦁    2023-12-23 09:52  <br/>⦁    量子位：2023中国AIGC数据标注产业全景报告.pdf  <br/>⦁    2023-12-20 15:12  <br/>⦁    欧洲量子技术关键绩效指标（2023 年 9 月）-英.pdf  <br/>⦁    2023-12-16 15:12  <br/>⦁    赛迪前瞻：应对量子计算挑战需积极推进后量子密码研发和迁移.pdf  <br/>⦁    2023-12-04 15:21  <br/>⦁    应对量子计算挑战需积极推进后量子密码研发和迁移2023-赛迪前瞻.pdf  <br/>⦁    2023-11-24 07:24  <br/>⦁    QIIA：量子计算金融应用白皮书.pdf  <br/>⦁    2023-11-22 18:56  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-21 14:33  <br/>⦁    2023年中国AIGC产业全景报告-量子位.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    2023西班牙量子产业报告英文-Ametic.pdf  <br/>⦁    2023-11-11 10:25  <br/>⦁    量子信息技术标准化图景(2022)- QIIA.pdf  <br/>⦁    2023-11-10 16:02  <br/>⦁    量子位：2023年中国AIGC产业全景报告.pdf  <br/>⦁    2023-11-10 09:55  <br/>⦁    Y2Q2023量子安全加密之旅报告英文-凯捷.pdf  <br/>⦁    2023-11-09 10:16  <br/>⦁    量子计算金融应用白皮书-QIIA.pdf  <br/>⦁    2023-11-09 10:13  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 16:01  <br/>⦁    QIIA：量子测量技术与产业发展白皮书(2022).pdf  <br/>⦁    2023-11-08 11:26  <br/>⦁    Capgemini-Y2Q：量子安全密码之旅【英文版】-2023.pdf  <br/>⦁    2023-11-02 10:25  <br/>⦁    量子计算概念、现状和国会考虑（英）.pdf  <br/>⦁    2023-09-16 09:58  <br/>⦁    中国仿生机器人产业全景报告-量子位智库.pdf  <br/>⦁    2023-08-16 21:47  <br/>⦁    十大AI商业落地趋势-量子位智库.pdf  <br/>⦁    2023-08-16 07:15  <br/>⦁    量子位智库：十大AI商业落地趋势.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    量子位智库：中国仿生机器人产业全景报告.pdf  <br/>⦁    2023-08-15 07:24  <br/>⦁    2023 AIGC算力全景与趋势报告-量子位.pdf  <br/>⦁    2023-07-26 06:37  <br/>⦁    ChatGPT 实用指南（精编版）（2023）-量子论.pdf  <br/>⦁    2023-04-17 14:37  <br/>⦁    2023全球量子精密测量产业发展展望-量子盒.pdf  <br/>⦁    2023-04-03 10:27  <br/>⦁    2023全球量子通信与安全产业发展展望-光子盒.pdf  <br/>⦁    2023-03-13 17:26  <br/>⦁    2023全球量子精密测量产业发展展望（中）-103页.pdf  <br/>⦁    2023-03-10 09:16  <br/>⦁    AIGC深度产业报告 量子位智库-34页.pdf  <br/>⦁    2023-03-09 10:46  <br/>⦁    量子位2022十大前沿科技报告.pdf  <br/>⦁    2023-03-08 09:52</p>]]></description></item><item>    <title><![CDATA[活动回顾：Arm 龙蜥齐携手，共筑 AI 时代开源 OS 新生态 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525104</link>    <guid>https://segmentfault.com/a/1190000047525104</guid>    <pubDate>2026-01-06 19:06:10</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>12 月 11 日下午，在一个温暖和煦的冬日，“龙蜥社区走进系列”之走进 Arm Meetup 在上海成功举办。本次活动吸引了来自云计算、互联网、半导体等领域的众多专家学者齐聚一堂，围绕 Arm 生态、开源社区、AI 基础设施及大模型推理等领域的技术突破，共同探讨了开源操作系统与 Arm Neoverse 平台在人工智能（AI）时代的深度融合与创新实践。</p><p>现场通过一系列主题分享，集中展示了 Arm Neoverse 平台、AI 性能分析工具、异构推理框架及优化等技术成果。这些成果不仅体现了产业链上下游协同创新与开源共建的精神，也加速了 AI 与云计算在 Arm 架构上的落地，为开源操作系统行业提供了更高效、可靠的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525106" alt="图片" title="图片"/><br/>（图/活动现场嘉宾合影）</p><p>活动伊始，阿里云智能集团编译器技术总监、Java 标准委员会委员(JCP-EC)，Java Champion、龙蜥社区 Java 语言与虚拟机 SIG Maintainer 李三红做开场致辞。发言聚焦阿里云在倚天 Arm 架构上的深耕，首先以全栈自研为核心，从芯片、操作系统到编译器全面优化，提升云原生场景的性能与性价比。其次重申对开源的长期投入，积极参与龙蜥社区建设，持续贡献稳定高效的操作系统能力。最后强调通过社区、理事单位和生态伙伴的合作，携手 Arm 共建完整生态，共同推动企业更好地使用 Arm 技术。与此同时，他也鼓励大家在活动中积极交流，共促行业发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525107" alt="图片" title="图片" loading="lazy"/><br/>（图/李三红）</p><p>接下来，来自阿里云、龙蜥社区、趋境科技、鸿钧微电子以及安谋科技的技术和市场专家针对基于 Arm Neoverse 平台，围绕开源龙蜥操作系统 Anolis OS 的各个层面在 AI 浪潮中的变革和演进，进行了精彩的分享与思想碰撞。</p><p>首先由安谋科技云人工智能事业部总监侯科鑫和阿里云智能集团弹性计算高级架构师张先国带来了题为《Arm 基础设施加速云计算智能驾驶》的联合演讲。</p><p>侯科鑫女士回顾了 AI 浪潮中若干行业趋势，强调 AI 的快速发展正在推动基础设施技术的重大转型，Arm 参与其中并重新定义计算。Arm 帮助合作伙伴在通用计算以及智能计算定制化平台上取得了不菲的成绩。Arm Neoverse 已成为这一转型中多个关键领域的首选平台。演讲深入介绍了 Arm Neoverse CSS，以及 Arm 的技术创新是如何帮助合作伙伴在 AI 时代加速产品上市。此外 Arm 在软件生态系统上也持续投入，通过与阿里云等合作伙伴的深度协作，Arm 平台不断推动 AI 基础设施创新，满足智能驾驶行业对高可靠性、弹性扩展和绿色算力的迫切需求，助力企业快速应对 AI 时代的挑战与机遇。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525108" alt="图片" title="图片" loading="lazy"/><br/>（图/侯科鑫）</p><p>张先国则针对智能驾驶场景，重点展示了 Arm 基础设施在智能驾驶场景中的云计算加速能力，围绕智能驾驶全流程——从数据采集、存储、标注，到模型训练、仿真验证及端侧集成，深入剖析行业痛点，包括数据规模庞大、算力消耗高、模型迭代快、训练成本高。针对这些挑战，提出了阿里云弹性计算解决方案，包括高性能、弹性伸缩的 AI 基础设施，支持十万核级资源快速调度，结合容器化算力、Serverless 调度、GPU 切分等创新技术，显著降低运维成本并提升性能（Spark场景优化 10% 以上，MRACC 算子优化 35%以上）。此外，方案还涵盖视频抽帧、点云处理、分布式训练及大规模仿真，为智能驾驶业务提供端到端的高效云端加速能力，助力行业实现 高可靠、低成本、快速迭代的目标。<br/><img width="469" height="313" referrerpolicy="no-referrer" src="/img/bVdnzB5" alt="image.png" title="image.png" loading="lazy"/><br/>（图/张先国）</p><p>安谋科技主任软件工程师方方明做了《RTP-LLM：Arm平台全面支持》的主题演讲。Arm 同龙蜥社区、阿里巴巴在很多技术领域都有深度合作，包括推理引擎。在 AI 时代，基于 Arm CPU，团队实现了对阿里巴巴大模型推理引擎 RTP-LLM 的全面支持，通过与生态伙伴的深度合作，RTP-LLM 不仅实现了对倚天等云端 CPU 的高效适配，还支持主流大模型（如 Qwen、Llama、DeepSeek、Bert 等）及多种量化格式（FP32、FP16、INT8、INT4、GPTQ 等），并集成了 Arm KleidiAI 等高性能 AI 内核库，极大提升了推理效率和灵活性。详细的技术亮点包括：1. 利用 Arm 的加速指令对算子的极致性能优化；2. 全面支持 MoE 架构（如 DeepSeek V3、Qwen3），使用专家融合提高并行计算能力，MoE 推理性能最高提升 4 倍；3. 多种量化与矩阵乘优化方案，显著降低内存占用并提升算力利用率；4. 端到端测试与高效部署，支持云到边多场景落地；5. 性能对比显示，RTP-LLM 在 Arm 平台上推理速度和资源效率均优于同类方案。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525109" alt="图片" title="图片" loading="lazy"/><br/>（图/方方明）</p><p>随着 AI 时代的到来，Java 在 AI 相关负载中扮演越来越重要的角色。阿里云智能集团高级 JVM 工程师邢其正做了《阿里巴巴 Dragonwell JDK：为 AI 时代而生》的主题演讲。阿里云推出的 Dragonwell 21 AI 增强版，包含 Native 加速、热代码重排和 JTune 三大核心技术。具体来说：</p><ul><li>Native 加速：通过高度优化的原生实现，显著提升 AI 相关计算性能，远超传统 JNI 方案，助力 Elasticsearch 向量搜索、Spark 等场景性能提升 18%-60%。</li><li>热代码重排：智能管理 JVM Code Cache，提升 JIT 编译代码的命中率和运行效率。</li><li>智能调优：AI驱动的自动调优框架，降低JVM参数复杂度，实现更高效的资源利用和运维自动化。</li></ul><p>Dragonwell JDK 不仅让 Java 在 RAG、大数据、智能驾驶等 AI 场景下实现性能飞跃，还兼顾企业级系统集成与运维需求，推动 Java 成为 AI 时代的主流生产力工具。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525110" alt="图片" title="图片" loading="lazy"/><br/>（图/邢其正）</p><p>ModelSight 是龙蜥社区自研的 AI 性能分析工具，基于 eBPF 实现 GPU、CPU、框架事件一体化观测，实现 AI 场景下端到端的性能诊断。阿里云智能集团技术专家、龙蜥社区智算基础设施联盟委员王鹏和常怀鑫联合带来了题为《ModelSight：端到端 AI 性能分析框架》的演讲。两位嘉宾分享了如何利用 ModelSight 对 235B 参数的 Qwen3 推理链路进行压测、热点定位与瓶颈可视化，并结合 TP/PP/EP 并行策略在 SGLang 框架中的落地，最终实现 2.12 倍性能提升。通过优化 SGLang overlap schedule，TTFT（首 Token 响应时间）平均提升 20%+。ModelSight 让 AI 性能分析更智能、更高效，助力企业迈向 AI 时代算力极致优化！<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525111" alt="图片" title="图片" loading="lazy"/></p><p>鸿钧微电子产品营销经理吴喆就《鸿钧微电子开源社区实践》话题进行了深度分享。他介绍了鸿钧微电子基于 Arm Neoverse 平台的服务器 CPU 产品的规格特点与主要适配的应用场景，并详细阐述其开源社区策略：积极拥抱开源、服务社区；取之于社区，也回馈社区。基于龙蜥操作系统进行芯片验证、驱动适配与性能优化，并向 Linux kernel、Qemu 等社区贡献多项驱动与功能补丁。在应用层面，鸿旻处理器在内存数据库 (Redis，Memcached)、视频编解码 (X265)、大数据 (Spark、Flink) 等场景展现出卓越的性能与能效优势，助力 Arm 架构服务器在云计算、AI、大数据等领域实现突破。基于高效能 Arm Neoverse 平台的服务器 CPU，鸿钧微电子将持续推动开源协作与生态繁荣。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525112" alt="图片" title="图片" loading="lazy"/><br/>（图/吴喆）</p><p>KTransformers 专注于大语言模型的高效推理和微调，通过 CPU-GPU 异构计算实现资源受限环境下的大模型部署，探索 Arm CPU+GPU 平台下的本地极致推理和个性化微调方案。趋境科技技术专家、KTransformers 核心开发人员袁子为带来了题为《KTransformers：在 Arm CPU 上实现大模型异构推理》的主题演讲，就 KTransformers 的以下创新亮点进行了细致探讨：支持 DeepSeek、Qwen、GLM、LLaMa 等主流大模型，灵活适配多种硬件平台；创新“Expert Deferral”机制，推理吞吐提升 45%，精度无损；针对 Arm 架构深度优化，NUMA 亲和、SVE/SME 指令集加速，矩阵运算性能提升 1.5倍；支持 LoRA 微调，已集成进 Llama-Factory 和 SGLang，便于本地微调与多 GPU 加速。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525113" alt="图片" title="图片" loading="lazy"/><br/>（图/袁子为）</p><p>最后，安谋科技主任软件工程师刘亮亮就话题《llama.cpp 跨 NUMA 节点部署优化实践》展开详细探讨。刘亮亮介绍了 llama.cpp 在 Arm 架构服务器部署中跨 NUMA 节点的性能问题及优化方案。主要通过以下两种优化手段：通过“分治”优化 GGML barrier 大幅度减少跨 NUMA 节点原子操作；性能瓶颈 MUL_MAT 算子通过 dst_tensor=src0_tensor * wdata_tensor 进行 Tensor 的乘积操作，为了实现 MUL_MAT 算子的 NUMA 感知内存访问，避免跨 NUMA 内存访问。对其中的 src0 Tensor 以及 dst Tensor 进行内存分割，实现处在一个 NUMA 节点中的线程只访问本地 NUMA 内存。而 wdata Tensor 是通过在量化的时候同时在不同的 NUMA 节点计算成基于 NUMA 节点的多副本。该方案已在 Arm Neoverse N2 平台实测，实现了 S_TG t/s提升 55%，S t/s 提升 53.2%，内存带宽分布也更为均衡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525114" alt="图片" title="图片" loading="lazy"/><br/>（图/刘亮亮）</p><p>最后，感谢本次活动各位嘉宾的精彩演讲，也感谢龙蜥社区伙伴及 Arm 工作人员：刘捷、蔡佳丽、吴永霞、倪俊雄（以上排名不分先后）等人的组织与配合，使得本次走进 Arm MeetUp 活动圆满结束。未来，期待龙蜥社区与 Arm 持续深化合作，在 AI 浪潮中共筑开放、创新、可持续的开源操作系统新生态！</p><p>本次 MeetUp 回顾视频及 PPT 后续会陆续上传至龙蜥官网，欢迎大家持续关注。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[迈向云+数据中心的国产 CPU 新引擎，龙蜥大会 RISC-V 分论坛回顾一览 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525125</link>    <guid>https://segmentfault.com/a/1190000047525125</guid>    <pubDate>2026-01-06 19:05:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，2025 龙蜥操作系统大会在京顺利落幕，由中兴通讯操作系统产品规划总工徐立锋，中国科学院软件研究所工程师、如意 RISC-V 社区运营丁欣，阿里巴巴达摩院高级合作伙伴运营专家朱祯贞，龙蜥社区运营委员会副主席、龙蜥智算基础设施联盟秘书处负责人金美琴联合出品的 RISC-V 分论坛也圆满举办。本论坛汇聚 RISC-V 芯片厂商、云服务提供商及顶尖科研机构代表，以 “软硬协同” 为核心主线，围绕系统层适配优化、芯片与软件协同创新、开源生态标准化三大维度展开深度研讨。通过前沿技术案例分享、行业大咖圆桌对话及 Anolis OS RISC-V 版本特性展示，本论坛成为高效的技术交流与合作平台，充分展示了 RISC-V 架构在服务器、边缘计算等高性能场景的商业化落地，助力构建开放共赢的算力新生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525127" alt="图片" title="图片"/></p><p>会议伊始，阿里云智能集团研发副总裁、龙蜥社区理事长马涛，中兴通讯副总裁赵志勇，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕依次开场致辞。马涛认为 RISC-V 是国产 CPU 自主创新与协同发展的关键战略方向，尽管 RISC-V 已在嵌入式、边缘计算领域取得进展，但在云计算、数据中心方面尚处于初步探索阶段。未来，社区将联合合作伙伴，持续完善 RISC-V 生态和技术标准化，加速其在数据中心和云计算场景的产业化落地，推动 RISC-V 成为高效节能云计算基础设施的关键力量。赵志勇指出 RISC-V 作为开放指令集架构的代表，以及模块化可扩展的天然优势，在服务器、AI、工业控制等核心领域展现出了巨大的潜力。龙蜥社区作为开源生态的重要载体，汇聚了众多的产学研用各种各方的力量，为 RISC-V 技术落地搭建了宝贵的协作平台。这既是开源生态众人拾柴火焰高的生动体现，更是响应国家战略，凝聚产业合力的具体实践。于佳耕表示，中国科学院软件研究所自 2019 年以来，持续深耕 RISC-V 架构的基础软件与操作系统生态建设，通过成立“如意 RISC-V 社区”，聚焦操作系统内核、编译器等关键基础软件的架构适配与优化，有效推动了 RISC-V 基础软件生态的快速成长与可持续发展，也期待与龙蜥社区等合作伙伴携手，共同推进 RISC-V 基础软件生态的繁荣与成熟。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525128" alt="图片" title="图片" loading="lazy"/><br/>从左至右：马涛、赵志勇、于佳耕</p><p>阿里巴巴达摩院高级技术专家王云龙分享了《共建 RISC-V 通用高性能平台标准》，他重点介绍了 RISC-V 架构进军高性能服务器领域所面临的标准化挑战与共建之路。当前，RISC-V 在技术指标上已快速接近主流水平，产业落地加速，但整体生态仍处起步阶段。关键挑战在于：软硬件标准接口标准不够完善、测试标准与测试套件缺失、系统软件存在碎片化风险。为破局，业界正联合推动软硬件兼容性标准的制定，并构建配套测试体系，旨在以玄铁 C930 等重点产品为突破，协同研发与标准，目标在 2026 年推出标杆服务器产品，实现从技术到生态的闭环。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525129" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家王云龙）</p><p>中兴通讯操作系统架构师谈虎分享了《操作系统 RISC-V 生态实践》，本次分享聚焦 RISC-V在通用计算与服务器领域的软件生态建设与实践进展。核心在于依托 RVA23 关键规范，系统推进从操作系统、编译器、基础库到上层应用的全栈支持与优化。目前，龙蜥等社区已推出 RVA23 预览镜像，内核及虚拟化支持持续增强，并在编译器与基础库层面实现了显著的性能提升。同时，基于 OpenStack、Kubernetes 的云平台及 PyTorch 等关键 AI 组件已完成验证，贯通了主流业务场景。展望未来，行业将协同完善 Server Platform 标准与内核特性，并推动 Ubuntu 等主流系统的长期支持，加速 RISC-V 的产业应用落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525130" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯操作系统架构师谈虎）</p><p>中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕分享了《RISC-V 高性能基础软件共建生态》。他重点探讨了如何构建开放共赢的 RISC-V 高性能基础软件生态。RISC-V 凭借其开放与模块化特性，正通过 RVA23 等标准增强对 AI、云计算等高性能场景的支持。然而，硬件多样性给软件兼容与优化带来挑战。为应对此，国内正通过中电标协工委会与“如意 RISC-V 社区”等平台，推动标准、测试与应用生态建设，并构建集成关键扩展的操作系统参考版。现场介绍了 Sapling 评估框架，当前生态成熟度得分为 55.72。展望未来，业界计划借鉴成功模式，通过共建技术规范、CI/CD 体系与示范软件栈，并利用硬件试验场加速软件合规与交付，最终推动 RISC-V 生态实现规模化发展。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525131" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕）</p><p>龙蜥社区系统安全 SIG Owner 徐峥分享了《RISC-V 可信计算技术实践》。他介绍了可信计算的发展及其在 RISC-V 生态中的关键进展。可信计算历经从概念萌芽到与机密计算、AI 安全融合的数个发展阶段，其核心技术包括信任根、可信度量链、计算平台、软件栈与远程证明。当前，RISC-V 正积极融入这一体系，不仅制定了相关安全模型与服务器规范，还支持 dTPM、fTPM 等多种信任根实现方案。现场以 KOS 的 KTrusted 组件为例，展示了其在 RISC-V 平台上成功实现的动态度量、国密支持与远程认证，验证了可信计算技术从理论到实践的闭环。未来，RISC-V 生态将继续深化可信计算标准的落地与应用协同。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525132" alt="图片" title="图片" loading="lazy"/><br/>（图/龙蜥社区系统安全 SIG Owner 徐峥）</p><p>中兴通讯 RISC-V 生态技术专家申林分享了《多架构源代码向 RISC-V 的迁移工具研发与迁移优化实践》。本次分享聚焦于 x86/Arm 软件向 RISC-V 架构迁移所面临的挑战与智能化解决方案。随着 RISC-V 生态扩展，多语言代码迁移因底层架构差异、依赖复杂及人才稀缺而成本高昂。为应对此，中兴通讯开发了智能迁移工具，其核心是“扫描-建议-生成-评测”的四层架构，并创新性地结合结构化扫描、Agent 增强与知识库匹配，以精准识别和转换架构相关代码。目前已成功应用于 OpenSSL 等开源项目，推动其原生支持 RISC-V 向量指令。展望未来，我们将持续扩展工具能力，深化 AI 迁移支持并优化性能，以加速整个软件生态向 RISC-V 的高效过渡。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525133" alt="图片" title="图片" loading="lazy"/><br/>（图/中兴通讯 RISC-V 生态技术专家申林）</p><p>阿里巴巴达摩院高级技术专家童琪杰分享了《玄铁 RISC-V 在 OpenAnolis 上的进展》。童琪杰介绍了玄铁在高性能 RISC-V 软件生态建设上的全面布局与实践。面向 RISC-V 在高性能计算领域的广阔前景，玄铁正系统性地构建从编译器、虚拟化、固件到操作系统与应用的全栈软件能力。其在虚拟化、IOMMU 支持及高性能诊断技术等方面取得关键突破，并通过对 ISA-L、X264/X265 等关键组件的深度优化，实现了数倍的性能提升。在实际云服务场景中，Redis、MySQL 等应用也获得了显著的性能增益。玄铁已向 Linux 及龙蜥等主流社区贡献了数百个补丁，持续推动内核与生态成熟，致力于实现高性能 RISC-V 的软硬件协同与规模化落地。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525134" alt="图片" title="图片" loading="lazy"/><br/>（图/阿里巴巴达摩院高级技术专家童琪杰）</p><p>中国科学院软件研究所工程师戴希铨分享了《RISC-V 架构上 AI 应用实践与探索》。戴希铨介绍了软件所/如意社区在 AI 应用方面的研究进展。他表示，RISC-V 作为开源指令集架构，正以年均 40% 的增速迅猛发展，全球出货量已超 130 亿颗，成为国产芯片突围的关键路径。其开源、模块化与可扩展特性，为 AI 算力创新提供坚实基础，尤其在端侧AI与异构计算中表现突出。通过玄铁 C920 CPU+NPU/TPU 协同，如意 OS 平台已成功移植 PyTorch、vLLM 等框架，支撑 DeepSeek、Qwen 等大模型在 AI PC、AI 教育等场景本地化运行，实现数据不出端、隐私有保障。典型应用如 AI 公文写作、多模态助手、智能教学等，验证了 RISC-V 在高性能 AI 推理上的可行性，正加速构建 RISC-V 的 AI 软硬件生态。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525135" alt="图片" title="图片" loading="lazy"/><br/>（图/中国科学院软件研究所工程师戴希铨）</p><p>中国电信研究院先进计算中心研究员崔恩放分享了《RISC-V 云计算：AI 智能体算力基础设施新路径》。他系统介绍了中国电信在 RISC-V 云计算领域的完整实践路径。自 2022 年启动研究以来，团队先后实现了云原生虚拟机、操作系统适配等关键突破，并于 2024 年建成了拥有数千核心的“北海”RISC-V 云计算平台。目前，该平台已支撑视频转码、AI 大模型等多元场景的生态建设。在技术层面，通过定制 TeleVM 轻量虚拟机，实现了大幅度的内存与启动开销优化；提出的 AgentDNS 服务发现系统更获得了国际认可。这些成果标志着 RISC-V 正从技术验证走向规模化的云服务商用部署。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525136" alt="图片" title="图片" loading="lazy"/><br/>（图/中国电信研究院先进计算中心研究员崔恩放）</p><p>会上，由阿里云智能集团高级开发工程师、龙蜥社区 RISC-V SIG Maintainer 田瑞冬主持，中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕，阿里巴巴达摩院高级技术专家王云龙，中兴通讯操作系统架构师谈虎，中国电信研究院先进计算中心研究员崔恩放共同参与了主题为“高性能 RISC-V 算力生态展望”的圆桌讨论，围绕 RISC-V 架构在高性能计算和 AI 领域的未来发展方向、面临的挑战与机遇展开深度探讨。中国科学院软件研究所正高级工程师、国家重点研发计划项目首席科学家于佳耕表示，尽管面临生态不成熟、软件兼容性、定制化成本等挑战，但开源社区、学术界和产业界的合作被视为加速RISC-V生态发展的关键。阿里巴巴达摩院高级技术专家王云龙聚焦在通用高性能（服务器场景）和 AI 两大领域展开分析了 RISC-V 的性能优势、生态支持以及面临的挑战，并表示，如何有效利用 RISC-V 的优势，特别是在 AI 计算方面，仍需探索和创新技术方案。中兴通讯操作系统架构师谈虎则从操作系统角度来看， RISC-V 在高性能方面已取得显著进展，但在实际应用中，尤其是在虚拟化扩展性能测试中的细节问题，与主流架构相比仍存在差距。此外，谈虎呼吁商业软件、硬件公司，尤其是 AI GPU 厂商，共同参与到 RISC-V 生态建设中来。中国电信研究院先进计算中心研究员崔恩放表示， RISC-V 面临的主要挑战在于如何选择合适的场景引入服务器厂商在适配软件方面的问题，并强调了成本优势对于推动新技术采用的重要性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525137" alt="图片" title="图片" loading="lazy"/><br/>（图/圆桌讨论现场）</p><p>感谢本论坛的出品团队：胡捷、徐立锋、陈盛德、丁欣、朱祯贞、林洛卉、李康雅等。</p><p>附本论坛的精彩集锦：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525138" alt="图片" title="图片" loading="lazy"/><br/>视频回放链接：<a href="https://link.segmentfault.com/?enc=NpjaWlihyhAJeIAns2ha1A%3D%3D.A8eCC42G1bUbGWENa3CgYL58VGT1oqw1b5PrdGRfunq7DE2h6nS0vQE82Wgrj%2Fsd" rel="nofollow" target="_blank">https://openanolis.cn/openanolisconference2025</a> —— 完 ——</p>]]></description></item><item>    <title><![CDATA[龙蜥社区荣膺 InfoQ “2025 中国技术力量榜单”两大奖项 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525152</link>    <guid>https://segmentfault.com/a/1190000047525152</guid>    <pubDate>2026-01-06 19:04:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，InfoQ 极客传媒携手模力工场发起的 “2025 中国技术力量榜单” 评选结果正式揭晓。龙蜥社区与合作伙伴联合提报的 “AI Serving Stack：面向大模型时代的云原生推理服务全栈解决方案”，凭借创新的 RBG 重新定义推理编排、智能调度实现差异化负载优化、以 KVCache 为中心的 PD 分离和以存换算架构等方面的突出创新，斩获了 “2025 年度 AI 工程与部署卓越奖”。同时，“SysOM AI 基础设施运维解决方案”凭借在大规模 AI 训推集群的“分钟级发现、小时级定界”能力、及持续性能剖析与资源效能提升上的领先实践，荣获“‘人工智能+’行业最佳解决/落地方案”奖项。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047525154" alt="图片" title="图片"/></p><p>在“人工智能 +”行动规划的推动下，人工智能已从前沿技术逐渐演变为驱动产业升级与经济转型的关键力量。2025 年，中国 AI 技术落地与产业应用进入加速期。龙蜥社区积极布局 AI 基础软件栈，在操作系统内核、工具链与运维体系等关键环节持续创新，构建起支撑大模型高效训练与推理的坚实底座。</p><p>大模型推理正演变为"最昂贵的微服务"——既需 HPC 集群的极致性能，又要求云原生的敏捷弹性。AI Serving Stack 是由龙蜥社区与 SGLang 社区、Mooncake 社区、清华大学、南京大学、小红书、算秩未来、科大讯飞和阿里云联合打造，坚持全栈开源理念，采用开放治理模式，100% 开源架构让企业可零成本快速落地。作为大模型生产级”最后一公里”难题的解决方案，AI Serving Stack 填补开源社区在“生产级 LLM 推理编排”领域的空白，提供了从部署管理、智能路由、弹性伸缩、深度可观测的一体化能力，助力企业无论处于 AI 应用初期还是已运行大规模推理业务，都能轻松驾驭复杂的云原生 AI 推理场景。依托领先的 RBG 部署编排机制、智能负载调度策略，以及以 KVCache 为中心的 PD 分离架构和以存换算架构，AI Serving Stack 实现数倍性能提升，显著降低推理成本。未来，AI Serving Stack 将以更加开放的生态聚合产学研智慧，为产业提供从“能跑通”到“高可用、高吞吐、高弹性”的质变路径。</p><p>SysOM（System Operation&amp;Maintenance）是龙蜥社区系统运维 SIG 打造的一站式操作系统运维平台，通过监控、诊断、持续性能剖析等一体化解决方案，具备常态化、无侵入、低开销、可视化分析等特点，广泛应用于教育、医疗、电商、智驾等场景的性能诊断和分析优化。SysOM AI 基础设施运维解决方案是围绕训练及推理场景“MTTR （平均修复时间）长、无效训练时间长”等痛点，以“1 分钟极速发现、5 分钟快速定界定位问题”为目标，显著提升整体训推效率。目前，龙蜥社区系统运维联盟成员单位阿里云已基于 SysOM 项目发布了操作系统控制台，操作系统控制台为用户提供全面的系统资源监控、问题分析和故障解决能力，旨在优化系统性能，显著提升运维效率和业务稳定性。未来，SysOM 将会继续帮助提升训推业务场景万卡集群规模的 GPU 利用率等问题而努力。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=6yPhoU9r6pOT%2Fle9vWWtnw%3D%3D.u%2BcE3ZhUTnDYyLDVsl5%2F%2FGo48Itp1HE5prBmDl5GjPiCGbcwJPHTRTv4eRjGTz%2FO" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p>此前，龙蜥社区及龙蜥操作系统也获得了业界的广泛认可，荣获 OS2ATC 2025 “最具影响力开源创新贡献奖”、龙蜥操作系统通过工信部电子标准院首批开源项目成熟度评估，唯一获得“卓越级”（最高等级）的开源项目等 40+ 行业奖项。未来，龙蜥社区将持续深耕 AI 与操作系统融合创新，携手更多合作伙伴，共建开放、高效的下一代智能基础设施底座。</p><p>完整榜单见链接：<a href="https://link.segmentfault.com/?enc=c3D3E5PLKj5CstdZ2hmgWg%3D%3D.prz5JcPya7deRXMLFocsnLvU0BqgER%2BNoDN09c01dsdIUu0tvZQBGjs4Hs4Yo0FZ" rel="nofollow" target="_blank">https://www.infoq.cn/zones/chinatechawards2025/</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[聚力同行！这 13 家企业荣获“2025 龙蜥社区最佳合作伙伴奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525157</link>    <guid>https://segmentfault.com/a/1190000047525157</guid>    <pubDate>2026-01-06 19:03:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>近日，龙蜥社区正式揭晓 2025 年度“最佳合作伙伴”获奖名单，并于 11 月落幕的龙蜥操作系统大会上举行颁奖仪式。现场，龙蜥社区理事代表、凝思软件副总经理彭志航为阿里云、浪潮信息、intel、海光信息、AMD、Arm、Tenable 等 13 家获奖企业代表颁奖。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525159" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区年度最佳合作伙伴颁奖现场）</p><p>本次获奖企业是与龙蜥社区有实质性合作落地，在产品共建、技术创新、商业实践及社区运营等多个维度综合评估的基础上，结合龙蜥贡献平台的实际数据，经过社区运营委员会与技术委员会评审，并由理事会公示后最终确认。恭喜这些单位！</p><p>在龙蜥社区生态发展计划--“龙腾计划 2.0”的战略牵引下，2025 年龙蜥社区携手整机、云服务、芯片、安全等全产业链伙伴，依托安全联盟、系统运维联盟与智算联盟，深入开展全栈协同，推动装机量达 1000 万套。在龙蜥社区运营目标牵引下，年度案例、活动与贡献度同比增长最高达 200%。本次获奖企业正是深度融入这一开放生态，积极践行“自愿、平等、开放、协作”理念的优秀代表。</p><p>其中，理事长单位与副理事长单位在社区生态共建上做了良好示范。阿里云作为社区理事长单位，积极引领社区整体规划和技术发展方向。通过推出三大合作计划，有效促进芯片、云服务、基础软件等多方成员的深度协同；通过社区治理和生态协同，有效推动关键项目落地，凝聚生态力量促进龙蜥蓬勃发展。浪潮信息全面参与安全联盟、智算联盟及委员会的工作，并积极参与社区规划和组织工作，有力支撑了龙蜥操作系统 Anolis OS 在 AI、云计算及关键基础设施等领域的生态拓展与规模化落地 。海光信息将新一代处理器的关键计算与安全特性深度融入龙蜥操作系统，实现高性能芯片与开源底座的高效协同。通过生态活动与社区共治机制，积极推动国产芯片与龙蜥在行业场景中的融合落地。intel 在技术适配、AI 生态融合等方面与社区深度共建，推动 Anolis OS 在 intel 平台及异构计算场景的优化落地，积极参与 SIG 协作与国际技术资源整合，为社区注入了产业动能与全球视野。中兴通讯为龙蜥社区提供 SW 与 RISC-V 架构的完整工具链及关键组件适配，降低新架构操作系统构建门槛，并持续维护软件供应链安全稳定；同时，其贡献的 NDE 桌面环境在兼容主流生态基础上，丰富了龙蜥桌面生态多样性。</p><p>在技术研发和社区运营方面，诸多伙伴单位也在积极并深度参与龙蜥社区。AMD 在积极推进 EPYC 处理器在龙蜥操作系统上的适配与优化的同时，还长期支持并参与社区组织的技术交流与生态活动，以实际行动助力开源基础软件生态的发展。Arm 深度投入社区 SIG 建设、参与内核及基础软件栈的技术共建，同时通过联合举办技术活动、推动工具链完善等方式，助力龙蜥生态的多元发展和产业落地。如意社区与社区携手加速 RISC-V 在数据中心、云计算等高性能场景的落地，助力开源芯片生态与操作系统深度融合。达摩院依托在操作系统、AI 与系统软件领域的深厚积累，为龙蜥社区在内核优化、智能调度及云原生工具链等方向提供关键技术协助。</p><p>安全联盟、系统运维联盟和智算联盟的成员单位，也通过技术合作、产品适配等参与龙蜥社区共建。安恒信息基于 Anolis OS 23 完成了其安全防护平台的兼容性验证与性能调优，为关键行业用户提供高可靠的安全运行环境；沐曦高效完成其曦云系列加速产品对 Anolis OS 23 的全面适配，充分释放国产 GPU 在龙蜥操作系统上的计算潜能；Tenable 作为国际安全厂商加入安全联盟， 已启动对 Anolis OS 的适配工作，助力用户构建与国际接轨的系统安全防护体系；信通院则发挥其在 ICT 领域标准与评测方面的权威优势，联合龙蜥社区推进操作系统相关技术标准、兼容性认证及开源治理规范的建设。</p><p>最佳合作伙伴生动诠释了各成员单位开源协作与生态共赢的初心与实践。龙蜥社区理事长马涛表示：“开源不是独行，而是众行。龙蜥的蓬勃活力源于每一位合作伙伴的长期投入与多元共创。未来，我们愿与更多伙伴一道，以开放聚合力，以协同筑根基，共赴基础软件的长远未来。”</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=0TiWPsEZ8VHSzT5657EtYw%3D%3D.vvX3WH%2FGOib7YKzdE0GYC0rarmfa5GrKKwQ64G6%2FNE0%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p>]]></description></item><item>    <title><![CDATA[五年同行，共铸基石！11 位杰出贡献者荣获“龙蜥社区五周年特别贡献奖” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525162</link>    <guid>https://segmentfault.com/a/1190000047525162</guid>    <pubDate>2026-01-06 19:02:45</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在第三届龙蜥操作系统大会上，龙蜥社区“五周年特别贡献奖”正式公布，由社区高级顾问、凝思软件董事长宫敏与特约顾问和中国开源软件推进联盟副主席兼秘书长刘澎，为本次 11 位获奖的贡献者颁奖。五周年奖项是为激励那些在龙蜥社区成立至今，不仅坚持个人长期参与并持续牵引所在企业投入关键资源，在社区中产生较大影响力的理事或委员。这 11 位获奖者为：马涛、张磊、高翔、龚文、顾剑、陈绪、杨勇、金美琴、张金利、王洪虎、陈鲍孜。</p><p>这些获奖者不仅是龙蜥社区五年发展的亲历者，更是其关键推动者。在他们的持续投入与引领下，龙蜥社区从零起步，逐步构建起覆盖操作系统、芯片、整机及配件、云服务等产业链上下游核心环节的开源生态；龙蜥操作系统也由此建立起完整的技术体系，Anolis OS 累计发布多个稳定版本，广泛支撑千行百业落地应用——装机量突破 1000 万，社区合作伙伴超 1000 家，生态影响力持续扩大。他们的工作，实实在在地塑造了龙蜥今天坚实的技术底座与蓬勃发展的产业格局。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047525164" alt="图片" title="图片"/><br/>（图/ 2025 龙蜥社区“五周年特别贡献奖”现场合照）</p><p>在开源社区的演进过程中，清晰的技术方向与生态愿景离不开顶层的战略引领与坚定投入。龙蜥社区的发展之所以能够稳步前行、持续突破，离不开以理事长、副理事长及理事为代表的高层治理者们的远见与担当。他们不仅锚定社区长期发展方向，更通过资源协同、组织推动和关键决策，将战略转化为行动，为龙蜥构建坚实的技术底座、繁荣的产业生态和可持续的开源机制提供了根本保障。</p><p>阿里云智能集团研发副总裁马涛作为龙蜥社区理事长和掌舵人，五年来始终身先士卒、躬身入局，引领社区从初创走向成熟。他持续坚定投入阿里云的技术与工程和运营资源，加速操作系统关键技术攻坚，推动龙蜥社区夯实基础、扩大影响，成长为国内领先的开源操作系统阵地。他的远见与坚持，不仅为龙蜥社区的蓬勃发展提供了核心动力，也为中国操作系统产业的进步和开源生态建设作出了重要贡献。</p><p>统信软件高级副总经理、龙蜥社区副理事长张磊深耕社区建设，积极推动统信软件深度参与龙蜥建设，成功促成 DDE 桌面环境的移植，并首创分层分类的开源操作系统架构方法，为系统模块化与可维护性奠定基础。同时，他带领团队积极参与开源安全、Rust 改造、机密计算等关键工作组，持续筑牢社区的安全技术底座，为完善龙蜥产品版图与提升用户体验提供了核心支撑。</p><p>龙芯中科副总裁、龙蜥社区理事高翔作为首批加入社区的理事代表，始终带领龙芯团队深度参与并积极推动社区各项建设。特别是在 Anolis OS 8.4 LoongArch 版本发布和 LoongArch SIG 组建过程中，依托龙芯深厚的技术积累与大量工程投入，他有力推动了龙芯开源发行版的构建、软硬件技术的高效协同，以及应用生态的繁荣发展，为龙蜥社区自主架构领域的拓展作出了重要贡献。</p><p>中科方德高级副总裁、龙蜥社区理事龚文自社区成立之初就亲自带领团队深度参与龙蜥操作系统研发工作，成功发布基于 Anolis OS 的中科方德服务器操作系统，并持续推进技术适配与性能优化，在加强系统的安全性与兼容性方面取得成果，为龙蜥技术演进和生态完善作出了切实的贡献。</p><p>飞腾软件技术方案部高级总监、龙蜥社区理事顾剑积极推动飞腾产品与龙蜥在技术及生态层面的深度协同，不仅推动适配飞腾系列 CPU 平台的龙蜥社区发行版共计 17 项、覆盖腾云 S25000、S5000C 等核心产品，同时牵头在飞腾官网设立龙蜥专区，联合共建生态影响力，在加速龙蜥操作系统与国产化软硬件融合落地过程中发挥了关键作用。</p><p>在整体发展方向锚定之后，要将愿景转化为现实，离不开系统化的组织规划与强有力的治理机制。龙蜥社区以技术委员会和运营委员会承担了这个职责，其关键成员作为社区治理的核心支柱构建起兼具技术前瞻性与执行落地力的治理体系。</p><p>龙蜥社区技术委员会主席杨勇主导制定了龙蜥操作系统整体的技术战略与发展路线，构建了清晰的架构蓝图，以社区“三大技术合作计划”为载体，有效牵引并协同推进九大关键技术方向的持续演进。同时，通过建立开放、规范的技术治理机制，为龙蜥打造高可靠、高性能且可持续演进的操作系统底座提供了坚实引领，是龙蜥不可或缺的技术领航人。</p><p>龙蜥社区运营委员会主席陈绪始终聚焦社区发展顶层架构与运营体系的建设，全面指导并参与运营委员会各项工作。凭借 30 年的开源社区实践经验，参与创立了龙蜥社区，为社区的组织设计与机制完善提供了前瞻性引导，有效连接技术、商业与生态多方力量，持续强化社区联盟与组织韧性，为龙蜥社区的高端心智打造、实现规模化成长和可持续健康发展注入了关键的推动力。</p><p>龙蜥社区运营委员会副主席金美琴参与龙蜥社区的早期建设和筹备工作，自 2020 年 6 月开始，领导并推动了包括社区官网的创建、组织制度的建设、生态合作机制的运行以及重要联盟组织和关键项目的建立等，为社区的运营体系建设和生态发展发挥了至关重要的作用，是龙蜥五周年特别致谢的功勋个人。</p><p>版本研发是龙蜥社区工作的重中之重，在有了清晰的技术路线指引、规范的运营和组织建设后，技术的演进则需要更多核心贡献者的参与，他们就是推动 Anolis OS 逐步走向成熟、稳定与广泛应用的关键支撑力量。</p><p>龙蜥社区技术委员张金利作为龙蜥操作系统版本研发负责人，主导了 Anolis OS 所有重大版本的规划与发布，成功构建了一套稳定、安全且高性能的发行版体系。在此基础上，他持续推进内核优化、提升硬件兼容性，并加强自动化测试能力建设，有效保障了版本的高质量迭代。同时，他还积极协同社区力量，不断完善研发流程与发布机制，为龙蜥操作系统在金融、通信等关键行业的广泛应用提供了坚实可靠的版本研发支撑。</p><p>龙芯中科操作系统研发总监、龙蜥社区技术委员王洪虎带领龙芯研发团队与社区紧密协作，牵头成立 LoongArch SIG 工作组，统筹各方力量，向社区累计提交补丁覆盖 800 余个软件包，贡献代码超 30 万行。他主导完成了 Anolis OS 9 个大版本的移植、同源异构适配、验证与发布工作，使龙蜥操作系统成为首个支持龙芯 3C5000 与 3C6000 系列服务器芯片的社区发行版。</p><p>飞腾公司开源操作系统技术负责人、龙蜥社区技术委员陈鲍孜主导推动飞腾处理器系统软件开源生态的逐步成熟与持续演进。他积极推动飞腾平台在内核、驱动及基础软件等关键领域的深度投入，并持续向龙蜥社区贡献技术成果。同时，他协同社区伙伴共同制定内核 kAPI 与基础配置的统一规范，有效助力构建更加开放、兼容、安全且高效的操作系统生态。</p><p>一人行快，众人行远。过去五年，龙蜥社区之所以能从零起步、稳步成长，离不开这些坚定的贡献者和广大开发者的持续投入——他们的努力不仅体现在一行行代码和一个个版本中，更实实在在地落在千行百业的真实场景里。站在新起点，龙蜥将继续坚持共建、共治、共享的理念，诚邀更多企业和开发者加入，一起夯实基础软件底座，共筑中国开源的未来。</p><p>龙蜥社区年度评选获选名单详情：<a href="https://link.segmentfault.com/?enc=7%2BluN7rKbZr2dL8NJp278w%3D%3D.hl4b%2FUEytrxFU6fsCDCUUkONpU0x7J%2F0dXWXVsv1flo%3D" rel="nofollow" target="_blank">https://openanolis.cn/honor</a></p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[直播预告｜智算时代，龙蜥携手玄铁共探开源驱动的高性能 RISC-V 新范式 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047525169</link>    <guid>https://segmentfault.com/a/1190000047525169</guid>    <pubDate>2026-01-06 19:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>龙蜥社区早在成立之初就已积极布局并持续投入 RISC-V 生态建设。过去一年中，龙蜥社区在 RISC-V 领域取得双突破：软件生态方面，联合达摩院、中兴通讯、浪潮信息、中科院软件所等机构制定 RISC-V SIG 2.0 规划，并携手中兴通讯、达摩院、浪潮信息、如意社区等伙伴首次发布支持 RVA23 高性能扩展的 Anolis 23 RISC-V 预览版。国际标准化方面，社区专家在 RISC-V 国际基金会担任主席/副主席要职，主导 Data Center SIG 运作，推进 RAS/PMU 云方案增强、AIOE 扩展及虚拟化标准制定，参与全球标准建设。</p><p>1 月 8 日（周四）晚 7 点玄铁【智算系列】专题技术沙龙第二期沙龙聚焦玄铁与龙蜥社区合作最新进展，特邀 RISC-V 国际基金会 Datacenter SIG chair、龙蜥社区 RISC-V SIG co-maintainer 宋卓，阿里云技术专家薛帅，阿里巴巴达摩院玄铁 RISC-V 计算库技术专家周云飞等多位专家，系统分享玄铁 RISC-V 在数据中心、高性能计算及云原生场景下的最新技术突破与实践成果。欢迎大家扫码观看直播。</p><p><img width="723" height="1680" referrerpolicy="no-referrer" src="/img/bVdnzC6" alt="image.png" title="image.png"/></p>]]></description></item><item>    <title><![CDATA[2026招聘分水岭：AI重构决策型招聘新逻辑 爱跑步的香蕉_cKtiNz ]]></title>    <link>https://segmentfault.com/a/1190000047525175</link>    <guid>https://segmentfault.com/a/1190000047525175</guid>    <pubDate>2026-01-06 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026招聘分水岭：AI重构决策型招聘新逻辑<br/>2026年之前，招聘或许还能依赖经验支撑；但2026年之后，仅靠经验驱动招聘，终将陷入被动。2025年，不少企业已开启AI招聘的初步尝试，从生成招聘文案、自动回复候选人到简化流程，确实为HR减轻了部分负担。但随着实践深入，企业逐渐意识到：浅层的AI工具仅能提升表层效率，已无明显边际价值。真正的变革正在发生——AI不再是单纯的效率辅助工具，而是深度渗透到组织能力核心层，开始重构招聘的底层逻辑。<br/>进入2026年，AI的价值早已超越“提速”，转而介入三个过去高度依赖经验与直觉的关键命题：人才该如何被精准定义、能力该如何被科学评估、招聘决策是否还能仅凭主观感觉。在人力资源领域，这一转变指向一个核心结论：唯有实现评估的精准度，才能真正掌握招聘决策的主动权。</p><p>招聘质变：从“工具辅助”到“决策支撑”<br/>过去一年，AI面试已在企业中广泛落地，但多数应用仍停留在基础层面：能与候选人对话、能提出常规问题、能生成评估报告，却无法直接为招聘决策提供有效支撑。核心症结并非AI的智能化程度不足，而是评估打分缺乏精准度、标准缺乏稳定性、结果缺乏清晰解释，难以支撑决策落地。<br/>招聘的本质是对候选人的综合判断，而判断的核心终究要落到精准评估上。真正能融入招聘全流程的AI工具，必然以“精准度”为核心能力，其评估结果不再是仅供参考的建议，而是可直接纳入决策链路的有效依据。这需要满足三重条件：通过人机“背靠背”对比实验验证效果，经受效标效度与重测稳定信度两大心理学指标检验，形成可复现、可验证的评估体系。这一突破，标志着招聘正式从“经验型判断”转向“数据驱动的规模化决策”。<br/>精准评估：贯穿招聘全流程的核心能力<br/>AI招聘工具的精准度，并非单一环节的能力体现，而是渗透到面试全流程的系统性优势，具体表现为四大维度：<br/>•一问多能：单道问题可同步评估多项胜任力，实现HR初筛与技术复试的无缝衔接，整体评估效率提升50%以上；<br/>•智能追问：借鉴资深面试官的提问逻辑，根据候选人回答即时生成针对性问题，精准捕捉核心信息，避免遗漏关键能力点；<br/>•简历深度挖掘：自动解析简历中的关键信息与模糊表述，生成递进式提问，既能有效降低候选人造假风险，也能避免因人工疏忽错失优质人才；<br/>•全维度覆盖：既可以评估沟通、协作等通用胜任力，也能针对编程、算法、工程、财务等专业领域精准设计考题，同时解放HR与专业面试官的精力。<br/>体验赋能：候选人体验成雇主品牌新抓手<br/>AI招聘的价值，既要兼顾企业端的评估效率，也要保障候选人的体验感——在竞争激烈的人才市场中，面试体验本身就是雇主品牌的重要组成部分。优质的AI面试工具，必然以“拟人化交互”为核心设计逻辑，打造有温度、有尊重感的面试场景：<br/>•情绪感知交互：精准捕捉候选人的语速、情绪与潜台词，通过引导式沟通帮助候选人缓解紧张，充分发挥真实水平；<br/>•无断点流畅体验：无需手动操作启停，系统自动识别回答状态并衔接问题，模拟真人对话节奏，避免机械感；<br/>•沉浸式视觉呈现：实现语音与口型的精准同步，打破传统AI面试的“纸片人”疏离感，提升交互真实度；<br/>•实时答疑解惑：支持候选人随时提问，针对岗位详情、企业福利等问题即时回应，强化候选人对企业的认知与入职意愿。<br/>全流程自动化：开启招聘“无人驾驶”新阶段<br/>如果说AI面试解决了“选得准”的问题，那么AI人才寻访工具则攻克了“找得快”的痛点，将招聘初筛阶段的机械劳动彻底自动化，推动招聘进入“无人驾驶”模式。这类工具并非简单的批量操作工具，而是具备独立判断与执行能力的全流程解决方案：<br/>•快速启动值守：30-60秒即可完成配置启用，全程无需人工值守，大幅降低操作门槛；<br/>•智能筛选匹配：自动按学历、年龄、薪资预期等预设条件筛选简历，精准锁定目标候选人；<br/>•拟人化动态沟通：模拟人类沟通逻辑发起对话，根据交互结果判断适配性，不合适则智能终止沟通，提升转化效率；<br/>•全链路信息闭环：自动遍历未读消息并个性化回复，主动索取候选人简历，同步模拟人类打字节奏提升真实感，最终将简历自动同步至ATS系统并生成候选人档案，实现信息流转无缝衔接。<br/>借助AI大模型能力，招聘彻底从“经验驱动”升级为“数据驱动”，不仅实现效率的几何级提升，更将HR从海量重复劳动中解放，聚焦于人才洞察、战略规划等更具价值的工作，让招聘成为支撑企业核心竞争力的重要环节。2026年的招聘分水岭已然显现，唯有主动拥抱AI重构的决策型招聘逻辑，才能在人才竞争中占据主动。</p>]]></description></item><item>    <title><![CDATA[汽车制造质量大数据分析如何助力企业创新发展？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047524715</link>    <guid>https://segmentfault.com/a/1190000047524715</guid>    <pubDate>2026-01-06 18:13:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在汽车制造业转型升级的关键阶段，质量大数据分析正成为企业提升核心竞争力的重要抓手。随着市场竞争日趋激烈，消费者对汽车产品的要求不断提高，传统的质量管理方法已经难以满足现代生产的需求。质量大数据分析通过整合多源异构数据，构建智能化分析模型，为企业提供了全新的质量管控视角和决策支持手段。<br/>一、质量大数据分析的技术路径与价值实现<br/>汽车制造过程中的质量数据具有多维度、多阶段、多系统的特征，涵盖了从原材料进厂到整车出厂的全生命周期。这些数据如果能够得到有效挖掘和分析，将为企业带来显著的竞争优势。当前，主流的质量大数据分析技术包括数据采集与预处理、特征工程、机器学习建模和深度学习算法等多个环节。<br/>在数据采集方面，现代汽车制造企业普遍采用多种技术手段实现数据的全面采集。例如，通过部署在生产线上的各类传感器，实时采集关键工艺参数；通过连接MES系统，获取生产过程的详细记录；通过对接QMS系统，收集质量检验数据和反馈信息。这些数据经过清洗、转换和标准化处理后，才能为后续分析提供可靠基础。<br/>特征工程是质量大数据分析的核心环节。通过对原始数据进行降维、变换和特征提取，可以发现数据中隐藏的质量规律。常用的特征工程方法包括主成分分析、小波变换、时间序列分析等，这些方法能够将复杂的数据转化为可解释性强的特征指标。<br/>在模型构建层面，企业可以根据实际需求选择不同的算法。传统统计方法如回归分析、方差分析仍然有其价值，但随着数据规模的扩大和复杂度的提高，机器学习算法如随机森林、支持向量机等逐渐成为主流。近年来，深度学习技术在质量预测领域的应用也取得了显著进展。<br/>质量大数据分析的价值不仅体现在质量问题的解决上，更重要的是它能够为企业的创新活动提供数据支撑。通过对质量数据的深度挖掘，企业可以发现产品设计、工艺改进和质量控制的创新点，从而开发出更具竞争力的产品。<br/>二、实施质量大数据分析的关键要素与挑战<br/>实施质量大数据分析项目需要综合考虑技术、管理、人才等多个因素。首先，企业需要建立完善的数据采集体系，确保数据的全面性和准确性。这包括部署各类传感器、建立统一的数据标准、完善数据传输机制等。<br/>数据治理是实施过程中的重要环节。企业需要制定数据质量标准，建立数据清洗流程，完善数据安全机制。特别是在汽车行业，质量数据往往涉及商业机密，如何在保护数据安全的同时实现数据价值，是一个需要认真考虑的问题。<br/>技术平台的建设同样至关重要。企业需要根据自身需求选择合适的大数据分析工具和算法。目前市场上有多种解决方案可供选择，如阿里云Quick BI、华为FusionInsight、浪潮云洲工业互联网平台等。这些平台的功能特点、适用场景各不相同，需要结合企业实际情况进行评估和选择。<br/>在组织保障方面，企业需要调整现有的管理架构和工作流程。质量大数据分析往往需要跨部门协作，这就要求企业打破传统的部门壁垒，建立以数据为中心的协同工作机制。<br/>三、行业应用案例：标杆企业的实践<br/>广域铭岛的质量管理系统（QMS）在汽车制造领域实现了多项突破。该系统通过整合工业互联网技术，实现了质量数据的实时采集、传输和分析。系统架构包括数据采集层、传输层、存储层和应用层四个主要部分，各层之间通过标准化接口实现无缝对接。<br/>在应用效果方面，广域铭岛的QMS系统为某知名汽车零部件企业提供了全面的质量管控解决方案。该项目实现了：<br/>200多个关键质量指标的实时监控<br/>检测效率提升40%，检测时间从原来的数小时缩短到半小时以内<br/>质量预警准确率达到95%以上<br/>年均质量成本降低25%<br/>客户投诉率下降30%<br/>另一个值得关注的案例是某德系豪华品牌汽车制造商。该企业实施了基于大数据的智能质量控制系统，实现了从质量检测到工艺优化的全流程数字化管理。系统采用了实时数据可视化技术，将质量数据以直观的方式呈现给相关人员。通过机器学习算法的持续优化，系统能够自动识别质量异常，并给出预警和建议。<br/>国内领先的新能源汽车制造商也在质量大数据分析方面进行了创新探索。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造需要引入工业智能体进行全链路管理？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524723</link>    <guid>https://segmentfault.com/a/1190000047524723</guid>    <pubDate>2026-01-06 18:12:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在新一轮制造业智能化浪潮中，工业智能体正成为推动产业变革的核心力量，尤其在汽车制造领域，其价值已从概念验证迈向规模化落地。作为融合大语言模型、工业机理与多源数据的新型认知智能系统，工业智能体不再局限于单一任务的自动化执行，而是像一位具备经验与判断力的“数字工匠”，能够自主感知产线状态、动态优化工艺参数、协同调度资源，并持续学习进化，真正实现从“人控设备”到“系统自驱”的跃迁。<br/>在汽车制造这一高度复杂、流程密集的行业中，工业智能体的应用已深入全价值链。以广域铭岛为代表的领先企业，通过其Geega工业超级智能体平台，成功将多年沉淀的工艺经验转化为可复用、可迭代的AI能力。在焊装环节，智能体可直接理解工程师的自然语言指令，自动调整焊缝参数并完成编程，使工艺优化周期缩短60%，缺陷率下降45%；在冲压与涂装工序中，它实时融合设备振动、温度、压力与视觉数据，动态调节运行参数，使零部件精度提升15%，废品率降低18%。更令人瞩目的是，其“设计智能体”能根据需求描述自动生成轻量化零部件方案，将新车研发周期压缩60%，彻底打破传统依赖人工经验的设计瓶颈。<br/>在生产运营层面，工业智能体构建了覆盖排产、质量、物流、运维的全链路协同网络。在汽车工厂中，调度智能体可在1小时内完成原本需6小时的人工排产，每周节省超15小时；质量检测智能体借助多模态感知，实现微米级缺陷识别，效率较人工提升200倍；而当供应链突发中断时，广域铭岛平台上的12类专业智能体可在5分钟内协同生成替代方案并验证可行性，将损失降低80%，极大增强了制造系统的韧性与响应速度。其“黑灯仓库”系统更联动AGV与AMR机器人，实现无人化拣选与缺件预警，打通了从订单到交付的智能闭环。<br/>广域铭岛的突破不仅在于技术应用，更在于其构建了“数据—机理—反馈”三位一体的能力基座。通过边缘计算实时捕捉毫秒级设备信号，结合工业Know-How形成“懂行AI”，并依托低代码平台让非技术工程师也能“搭积木”式开发专属智能应用，大幅降低了部署门槛。这种模式有效破解了工业领域长期存在的“数据孤岛”与“经验失传”难题，将老师傅的隐性知识转化为可传承、可扩展的数字资产。<br/>当前，工业智能体正从“辅助工具”加速演变为汽车制造的“决策中枢”。随着数字孪生、联邦学习等技术的融合，未来智能体将不仅优化单点效率，更将重构整车研发、生产、服务的全生命周期体系。广域铭岛的实践表明，唯有将智能体深度扎根于真实汽车制造场景，构建开放、协同、自进化的智能生态，才能真正释放其作为新型生产力的潜能，推动中国汽车产业从“规模驱动”迈向“认知驱动”的高质量发展新阶段。</p>]]></description></item><item>    <title><![CDATA[为什么汽车制造企业需要引入模具智能管理系统？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524727</link>    <guid>https://segmentfault.com/a/1190000047524727</guid>    <pubDate>2026-01-06 18:12:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在工业4.0深度演进的背景下，模具智能管理正成为制造业提质增效的核心引擎，尤其在汽车这一高度精密、高节奏的产业中，其价值尤为凸显。传统依赖人工经验、固定周期保养的模具管理模式，早已难以应对现代汽车生产对稳定性、一致性与响应速度的严苛要求——停机频发、备件冗余、故障难溯，成为制约产能释放的隐形瓶颈。<br/>广域铭岛数字科技有限公司以“数据驱动、AI赋能”为理念，率先构建起一套面向汽车制造的模具智能管理闭环体系，彻底重塑了模具在生产链中的角色：从被动消耗的“工具”，转变为可感知、可预测、可优化的智能资产。其核心依托于Geega工业AI应用平台与工业智造超级智能体，通过在模具与压机上部署高精度传感器网络，实时采集冲压次数、温度、振动、压力等多维数据，结合材料特性、历史维修知识图谱与生产排程信息，动态生成每副模具的“设备健康指数”（EHI）。这一指数不再是抽象报表，而是模具的“生命体征”——当某副用于高光件生产的模具因表面易划伤导致EHI升高，系统自动缩短保养周期；当高强度钢模具因应力累积预警导柱磨损，系统即推送“更换导柱+优化润滑”的精准干预方案，实现从“定时体检”到“精准诊疗”的跃迁。<br/>在领克汽车成都工厂的实践中，这套系统展现出惊人的实战效能：模具相关停机率下降65%，故障响应时间从2小时压缩至15分钟，润滑剂消耗减少18%，备件库存周转率提升40%，设备故障预测准确率突破95%。更重要的是，系统打通了MES、ERP与库存调度系统，形成全链路协同——当某模具即将达维护阈值，系统可提前48小时自动切换订单至健康模具，避免突发停线；一旦异常发生，15分钟内自动生成包含设备切换与参数调整的应急方案，真正实现“未病先防、小病早治”。<br/>这一模式的价值远不止于降本增效。广域铭岛的解决方案将每副模具的全生命周期数据完整沉淀，质量问题可精准溯源至某次保养中的润滑不足或导柱更换延迟，企业经验不再随技师离职而流失，而是转化为可复用、可迭代的数字资产。在芯片短缺等供应链危机中，系统甚至能基于3000组模具实时状态数据，智能分配稀缺资源至故障风险最低的产线，保障核心车型交付，展现出强大的韧性与战略价值。<br/>如今，模具智能管理已从汽车制造的“关键支撑”升级为“核心竞争力”。广域铭岛所推动的，不仅是技术工具的升级，更是一场工业文明的范式革命——它让沉默的钢铁学会“说话”，让混沌的生产重获秩序。未来，随着5G、边缘计算与数字孪生技术的深度融合，模具的“数字分身”将在虚拟空间中模拟百万次冲压，AI智能体通过“自我对弈”持续优化策略，实现“一处学习，全网受益”的群体智能。在汽车工业迈向高端化、柔性化、智能化的征途中，广域铭岛正以模具智能管理为支点，撬动整个制造体系的进化。</p>]]></description></item><item>    <title><![CDATA[团队协作冲突怎么处理？项目管理常见冲突原因与解决流程 项目管理小胡 ]]></title>    <link>https://segmentfault.com/a/1190000047524731</link>    <guid>https://segmentfault.com/a/1190000047524731</guid>    <pubDate>2026-01-06 18:11:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>我从市场转岗做项目经理后，最先卡住的不是排计划，而是团队协作里的冲突：一句“又改需求？”就能把讨论从对事变成对人。我也试过圆场、私聊硬扛，结果问题没少，关系更累。后来我才明白：冲突不可怕，可怕的是没流程、没规则。下面我用自己的踩坑经历，给你一套新人也能照着用的团队协作冲突解决流程。</p><p>读完本文你会得到：</p><ul><li>5类常见冲突根因（目标/范围/资源/边界/信息）怎么识别</li><li>一套“冲突处理5步流程”（含话术 + 纪要模板）</li><li>需求变更、跨部门拉扯、资源抢占时怎么拍板与升级</li></ul><h2>明明想把事做好，为什么越团队协作越拧巴</h2><p>我印象最深的一次，是需求评审会的后半段。业务说按钮颜色得改，不然转化会掉。研发同事直接就是一个问号：“又改“？测试补了一句：“那我这轮回归是不是白做了”？</p><p>我当时脑子里只有一个念头：别吵起来。于是我快速圆场：“先按原计划走，改动我会私下再对齐”。会开完我分别私聊三方，像个客服一样安抚情绪。结果一周后需求照改、返工照来、抱怨更大。</p><p>后来我反思了一下，我总怕场面难堪，觉得“和气”比“结论”重要。后来才发现，回避一次，问题会在延期、返工或下一次更大的争执里补回来。</p><p>我开始把争论改写成一句“可决策的话”：要不要进本期？代价是什么？谁拍板？验收标准是什么？当问题变成“怎么做选择”，团队协作才有机会回到同一张桌子上。</p><p>真正让局面变稳的，是固定住几件事：事实对齐、影响量化、拍板机制、书面结论、行动项追踪。下面我会先讲“冲突根因怎么识别”，再给你完整的“5步解决流程”。</p><h2>团队协作冲突最常见的5个原因（以及你怎么识别它）</h2><p>下面五类，我会配上“项目现场常见句式”，你会更容易判断自己遇到的是哪一种。</p><p><strong>1）目标不一致：大家都在为“正确的事”努力，但方向不在一条线上</strong></p><p>典型句式：业务：“先上再说，错过窗口期就没机会了”；研发：“不把隐患收掉，上线就是埋雷”。</p><p>识别要点：一方在谈“速度/机会”，一方在谈“风险/稳定”。</p><p>我的补救：把目标拆成“本期必须守住的三件事”（比如合规、核心质量、关键路径），其余作为后续迭代。目标一旦写清，争论会自动收敛。</p><p><strong>2）范围不清/需求频变：冲突表面是情绪，背后是变更控制缺失</strong></p><p>典型句式：“这不是小改吗？” vs “你们根本不知道影响面。”<br/>识别要点：一方在谈“功能点”，另一方在谈“系统影响/回归成本”。<br/>我的补救：建立最小变更闭环：变更内容 → 影响面 → 成本/风险 → 决策人 → 验收口径。只要闭环跑起来，需求变更就不再等于“临时加塞”。</p><p><strong>3）角色与边界模糊：很多冲突不是讨论不出来，是没人有权拍板</strong></p><p>典型句式：“这个你问产品。”<br/>“这个要业务确认。”<br/>“那你们决定吧。”<br/>识别要点：决策权漂移，责任也漂移。<br/>我的补救：至少明确三件事：谁拍板、谁负责落地、谁负责验收。不用一上来就做完整RACI，但要让“决策出口”存在。</p><p><strong>4）资源稀缺：你以为在争优先级，其实在争“风险与代价谁来背”</strong></p><p>典型句式：“我们也很忙，你这个插进来我那边就要延期。”<br/>识别要点：争论持续围绕“谁先做”，却没人把“代价”写清。<br/>我的补救：把资源冲突显性化：关键人同时承担哪些任务？每个任务的代价是什么？让优先级回到“业务价值 × 风险”，而不是“谁声音大”。</p><p><strong>5）信息不对称：一方看见客户压力，一方只看见工期压缩</strong></p><p>典型句式：“客户已经在催了。” vs “那也不能不讲基本规律。”<br/>识别要点：双方讲的不是同一套语言。<br/>我的补救：练习“翻译”：业务把客户压力翻译成风险等级；研发把技术影响翻译成成本、失败概率与兜底方案。<br/>识别了原因，你就能更从容地进入“处理流程”。接下来这套 5 步，我自己反复用，尤其适合新人 PM 不被情绪带跑。</p><h2>方法与实践：我现在用的“冲突处理5步流程”</h2><p>我把它当成一个“小型可复制流程”。每次冲突来了，我就按步骤走，自己也不容易被情绪裹挟。</p><p><strong>Step 0：先“降温”，再“推进”（30秒把会议从对人拉回对事）</strong></p><p>我常用两句开场：</p><ul><li>“我先确认一下：我们目标是一致的——按期交付、风险可控，对吗？”</li><li>“我们先对齐事实，再讨论方案，最后明确谁拍板。”</li></ul><p>这不是装冷静，而是在给团队协作一个“共同底盘”：我们不是来赢对方，我们是来解决问题。</p><p><strong>Step 1：用“事实—影响—待决策”清单，结束空转争论</strong></p><p>我会把白板（或在线文档）写成三段：</p><ul><li>事实（可验证）：发生了什么？版本/时间/承诺/数据是什么？</li><li>影响（可量化）：对范围/进度/质量/风险的影响分别是什么？</li><li>待决策（一句话）：我们现在要决定什么？（进不进本期？延期？降级？拆分？）</li></ul><p>可复制模板（你直接粘贴到纪要里）：</p><ul><li>事实：</li><li>影响（范围/进度/质量/风险）：</li><li>待决策：</li><li>拍板人：</li><li>结论：</li><li>行动项（Owner/DDL/验收口径）：</li></ul><p>示例（拿“颜色变更”举例）：</p><ul><li>事实：提出变更发生在联调后；影响页面 6 个；涉及埋点 2 个。</li><li>影响：回归增加 1.5 人日；本期上线风险 +1。若不改，业务预计转化下降 X%（先标记为“假设”，待数据验证）。</li><li>待决策：是否进入本期？若进入，是否接受回归工期延长？验收口径是什么？</li></ul><p>小技巧：把“观点”先标为“假设”，再决定要不要验证。这样团队协作会更少陷入“我觉得”。</p><p><strong>Step 2：从“立场”转到“利益”（问两句就够用）</strong></p><p>冲突里最耗人的，是大家都在捍卫立场：</p><ul><li>业务立场：“必须改。”</li><li>研发立场：“不能改。”</li></ul><p>我会刻意追问“利益/担忧”：</p><ul><li>“你最担心的是什么风险？”</li><li>“如果只能保一个，你想保住什么？”</li></ul><p>这就是我借鉴的“原则式谈判”思路：先把深层诉求拉出来，方案空间才会出现。</p><p><strong>Step 3：选对冲突处理方式：不是永远“协作”，而是看情境</strong></p><p>这里我会用 TKI 的两条轴提醒自己，从而形成五种方式：竞争、回避、迁就、妥协、协作。</p><ul><li>坚持度（assertiveness）：我多想满足自己的诉求</li><li>合作度（cooperativeness）：我多想满足对方的诉求</li></ul><p>我给新人一个更“好用”的判断：</p><ul><li>合规/安全/关键质量：可以更“强制/直接”，但必须讲清依据与责任边界；</li><li>信息不足：先“延后”，补数据，不要在不确定里硬拍；</li><li>双方诉求都合理：优先“协作”，找拆分、找阶段性方案；</li><li>必须快速推进：选择“妥协”，但把代价写进记录里（别让代价消失在空气中）。</li></ul><p><strong>Step 4：开一场15分钟“问题解决会”，把结论落地到行动项</strong></p><p>我现在处理冲突，会尽量快速拉一个短会（人越少越好，但必须包含拍板人或授权人）：</p><ul><li>5分钟：事实&amp;影响对齐</li><li>5分钟：列方案（至少2个）+评估标准（进度/风险/收益/成本）</li><li>5分钟：决策 + 行动项（负责人/截止时间/验收口径/风险兜底）</li></ul><p>我常用的“拍板句”：</p><ul><li>“我们现在不是在找完美解，是在找‘可交付的最优解’。”</li><li>“我把代价写进纪要：我们接受 A，意味着 B 风险增加；谁确认/谁承担/怎么兜底？”</li></ul><p><strong>Step 5：把冲突变成机制：写进“工作约定”，减少下次重复爆炸</strong></p><p>如果同一类冲突反复出现，我会在复盘后补一条“工作约定”，比如：</p><ul><li>需求变更三问：改什么？影响什么？谁拍板？</li><li>优先级原则：以业务价值×风险为准（并把评估标准写出来）</li><li>升级路径：卡住超过 24 小时 → @相关Owner；超过 48 小时 → 升级到负责人；所有升级必须带“事实—影响—建议方案”。</li></ul><p>流程能止血，机制能治本。把冲突写进规则里，你的团队协作会越来越省心。</p><h2>启发与建议：我从实践里提炼的5条心得</h2><ol><li>先对齐目标，再讨论方案：目标是方向盘，方案只是路线图。</li><li>冲突不是人不好，多半是规则不清：范围、边界、信息、决策出口，缺哪补哪。</li><li>把结论写下来：没有记录的共识，下一次冲突会把你带回原点。</li><li>让“说真话”变安全：心理安全感常被定义为团队成员相信“表达观点、提出问题、承认错误不会被惩罚或羞辱”。这也是健康团队协作的重要底层条件。</li><li>把冲突当成体检报告：每次冲突都在告诉你——哪条规则缺失、哪条流程漏风、哪个角色没被授权。</li></ol><h2>常见问题 FAQ：</h2><p><strong>Q1：团队协作冲突出现时，PM 是不是应该立刻站队？</strong><br/>A：通常不要。先把冲突从“立场对抗”拉回“事实与影响”。当事实清晰、代价明确时，站队会变成“基于原则的决策”，而不是“情绪偏好”。</p><p><strong>Q2：需求变更引发冲突，最关键的一句话是什么？</strong><br/>A：把争论写成“可决策的问题”：“要不要进本期？代价是什么？谁拍板？验收是什么？”——这句话能把团队协作从互怼拉回协作。</p><p><strong>Q3：跨部门协作总卡住，怎么推动？</strong><br/>A：用“升级路径 + 书面化影响”推动。升级不是告状，而是把“卡住的代价”摆在台面上，让组织机制来做取舍。</p><p><strong>Q4：冲突解决后还要做什么？</strong><br/>A：一定做一次“轻量复盘”：本次冲突的根因属于目标/范围/资源/边界/信息哪一类？把对应规则补上，否则还会复发。</p><p>我到现在也不敢说自己“很会处理冲突”。但我越来越确定：项目经理的成长，不是把所有人都哄开心，而是把复杂的团队协作问题，变成大家都能理解、都能执行的流程与规则。</p><p>如果你也是跨岗位转型、刚做 PM，遇到冲突别急着怀疑自己——很多时候不是你不行，而是你还在学习“从沟通者走向协调者”。愿我们都能在一次次复盘里，把拉扯变成对齐，把争论变成决策，把冲突变成更稳的协作机制。</p>]]></description></item><item>    <title><![CDATA[这篇一定要看，观测云 2026 产品路线图全公开 观测云 ]]></title>    <link>https://segmentfault.com/a/1190000047524757</link>    <guid>https://segmentfault.com/a/1190000047524757</guid>    <pubDate>2026-01-06 18:10:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>序言：奇点临近，可观测性的代际跨越</h2><p>站在 2026 年的时间节点回望，我们正处于 IT 基础设施历史上最深刻的变革之中。这不仅是云计算的延续，更是一场由人工智能（AI）主导的“认知革命”。如果说云原生（Cloud Native）时代解决了资源的弹性问题，那么 AI 原生（AI Native）时代则致力于解决决策的自主性问题。</p><p>Gartner 的战略预测早已指出，到 2026 年底，由于缺乏足够的 AI 风险护栏，甚至可能出现数千起因 AI 决策失误导致的法律索赔案件。这一预测不仅揭示了 AI 技术的双刃剑效应，更深刻地指出了当前技术栈中最大的空白——对于自主智能体（Autonomous Agents）的深度可观测性与治理能力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524759" alt="图片" title="图片"/></p><p>在 2026 年的企业环境中，由于 Agentic AI 的普及，软件不再仅仅是执行预定义代码的静态指令集，而是变成了具有推理、规划和执行能力的“数字员工”。这些智能体像 F1 赛车的维修团队一样协作，以模块化的方式处理复杂的业务逻辑。然而，这种自主性带来了前所未有的不确定性：一个简单的用户请求可能触发成百上千次非确定性的模型推理、工具调用和数据库交互。传统的应用性能监控（APM）工具，基于确定性的堆栈跟踪和静态的拓扑图，已无法完全解释这种动态生成的行为链路。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524760" alt="图片" title="图片" loading="lazy"/></p><p>与此同时，数据重力的法则依然生效且愈发严苛。随着生成式 AI 和多模态交互的爆发，企业产生的数据量呈指数级增长，但 IT 预算的增长却远远滞后。如何在数据爆炸的背景下，既保持对所有信号的敏锐捕捉，又严格控制存储成本，成为了 SRE 和 CIO 面临的头号难题。传统的“索引一切”（Index Everything）的日志管理模式在经济上已然破产，市场迫切呼唤一种全新的、基于存算分离架构的数据底座。<br/>本文将作为观测云（Guance）2026 年的产品技术展望，深入剖析在这一大变革背景下，我们如何通过产品演进解决测试、业务、数分、SRE 等多角色的核心痛点。我们将沿着“从上层业务应用到底层基础设施”的逻辑脉络，抽丝剥茧，呈现一个全栈可观测的 2026 图景。</p><h2>1. 市场趋势：驱动变革的四股力量</h2><p>在展开产品细节之前，我们需要厘清推动 2026 年可观测性技术变革的宏观力量。</p><h3>1.1 AI Agent 的崛起与黑盒治理危机</h3><p>2026 年，AI 不再是辅助工具，而是核心生产力。Gartner 指出，AI 原生开发平台正在让自主 Agent 协作完成复杂任务。然而，Agent 的引入带来了全新的不可预测性：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524761" alt="图片" title="图片" loading="lazy"/></p><ul><li>非确定性路径：Agent 的决策逻辑是动态生成的，传统的基于固定代码路径的 APM 难以追踪其思维链。</li><li>Token 经济学：每一次 API 调用都对应着真金白银。监控系统的核心指标从 CPU 使用率转向了“Token 消耗率”与“任务完成成本”。</li><li>黑盒风险：当 Agent 陷入死循环或产生幻觉时，传统的监控告警往往滞后，导致巨额的 API 费用浪费。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524762" alt="图片" title="图片" loading="lazy"/></p><h3>1.2 数据引力与存算分离的必然</h3><p>随着数字化转型的深入，企业数据量正以每年 180 EB 的速度增长。传统的基于本地磁盘（SSD/HDD）的存储架构（存算耦合）面临巨大的成本压力：</p><ul><li>扩容困境：为了增加存储空间，不得不增加计算节点，导致计算资源闲置浪费。</li><li>冷热数据鸿沟：90% 的查询集中在最近 24 小时的数据，但为了合规，企业必须存储数年的历史数据。将所有数据都放在昂贵的块存储上在经济上已不可行。</li><li>解决方案：市场正全面转向基于对象存储（S3/OSS）的存算分离架构，这也是 GuanceDB 演进的必然方向。</li></ul><h3>1.3 平台工程（Platform Engineering）与左移</h3><p>DevOps 正在进化为平台工程。开发者不再满足于被动接收告警，他们需要自服务的、可编程的观测能力。可观测性正在“左移”进入 CI/CD 流水线，开发者要求能够通过代码（Monitoring as Code）定义监控规则，并通过 API 触发自动化修复流程。</p><h3>1.4 FinOps 与数据主权的博弈</h3><p>随着全球数据法规（GDPR 等）的收紧，大型企业越来越倾向于“控制面与数据面分离”的架构。他们希望利用 SaaS 厂商提供的先进 AI 分析能力（控制面），但要求原始遥测数据保留在自己的云账号下的对象存储桶中（数据面），即 BYOS（Bring Your Own Storage）模式。</p><h2>2. 观测云新的产品功能：蓝图 (Blueprint)</h2><p>—— 可观测性编排与自动化引擎</p><p>在观测云 2026 的规划中，“蓝图”（Blueprint）不是一张静态的架构图或一套预设的 Dashboard 模板。基于最新的用户需求与 UI 设计，蓝图被重新定义为“官方组件支持计划”的核心载体，是一个低代码/无代码的可观测性编排与自动化引擎。</p><p>它通过可视化工作流（DAG - 有向无环图）将分散的观测能力串联起来，形成从 数据查询 -&gt; 逻辑转换 -&gt; AI 分析 -&gt; 行动 的完整闭环。</p><h3>2.1 蓝图的核心架构：可视化 DAG 工作流</h3><p>传统的监控告警是离散的：一个阈值触发一封邮件。而 2026 年的蓝图引擎引入了状态机与流式处理的概念。蓝图工作流由以下四类核心节点构成，支持用户通过拖拽方式构建复杂的运维逻辑：</p><h4>2.1.1 数据查询节点（Input / Sensor）</h4><ul><li><p>DQL (Data Query Language) 驱动：支持复杂的查询逻辑，包含了简单的指标阈值（如 CPU &gt; 80%），更加支持跨数据源的关联查询。</p><ul><li>示例：“查询最近 5 分钟支付接口的 P99 延迟，且仅当该延迟不仅超过阈值，同时伴随错误日志激增时触发。”</li></ul></li><li>多源异构：支持 Metrics、Logs、Traces、RUM（用户体验数据）的混合查询。</li></ul><h4>2.1.2 转换与逻辑节点（Processor / Logic）</h4><ul><li>低代码处理：支持 JavaScript/TypeScript 片段或表达式语言（Expression Language）。</li><li>上下文丰富：原始告警往往缺乏上下文。转换节点可以调用外部 CMDB 或 K8s API，为告警数据打上“业务线”、“负责人”、“部署版本”等标签。</li><li>价值：解决“告警疲劳”的核心手段。通过逻辑判断（如去重、抑制、时间窗聚合），将 100 条原始告警压缩为 1 条高价值根因分析。</li></ul><h4>2.1.3 AI 分析节点（Intelligence / Obsy AI）</h4><ul><li>ObsyAI 智能体介入：这是蓝图的智能核心。当逻辑节点检测到异常后，自动唤起 ObsyAI 进行根因分析。</li><li>能力：自动关联该时间段内的变更事件（Change Events）、错误日志聚类（Log Patterns）和异常链路等等。</li><li>输出：一段自然语言描述的诊断建议：“检测到支付服务延迟升高，关联到 3 分钟前 payment-service 的 v2.1 发布，且 DB 连接池报错激增。”</li></ul><h4>2.1.4 行动节点（Action / Actuator）</h4><ul><li>OpenAPI 闭环：这是蓝图与传统监控的最大区别。它通过 OpenAPI 与外部系统对接，执行实质性操作。</li><li><p>场景覆盖：</p><ul><li>通知：发送富文本消息到 Slack/钉钉/企业微信（包含 AI 诊断结果）等任意communication channel。</li><li>监控器管理：自动静默非核心服务的告警，或在流量高峰期动态调整阈值。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524763" alt="图片" title="图片" loading="lazy"/></p><h2>3. 更加全面的变更观测 (Change Observability)</h2><p>—— 根因分析的时间维度</p><h3>3.1 变更：系统熵增的核心问题</h3><p>根据 SRE 的经验法则，80% 的生产事故是由变更（Change）引起的。无论是代码发布、配置文件的修改、Feature Flag 的切换，还是基础设施的扩缩容操作，都是打破系统稳态的潜在因素。然而，传统的监控工具往往只记录了“结果”（Metrics 的突变、Logs 的报错），却丢失了“原因”（谁、在什么时候、做了什么变更）。</p><p>观测云 2026 将“变更”提升为与 Logs、Metrics、Traces 同等的一级数据公民（First-Class Citizen），构建了全维度的 变更观测（Change Observability） 体系。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524764" alt="图片" title="图片" loading="lazy"/></p><h3>3.2 变更数据的全栈采集与关联</h3><h4>3.2.1 统一变更数据模型</h4><p>为了捕捉系统中的每一次变化，观测云 2026 建立了一套标准化的变更数据模型：</p><ul><li>应用层：深度集成 Jenkins、GitLab、GitHub Actions 等 CI/CD 工具，自动捕获部署事件（Deployment）、Commit 信息、Artifact 版本。</li><li>基础设施层：监听 Kubernetes Events（如 Pod Killing, Scaling）、云厂商审计日志（如 AWS CloudTrail、阿里云 ActionTrail），捕获资源的创建、销毁和规格变更。</li><li>配置层：对接 Nacos、Apollo、Consul 等配置中心，实时记录配置项的 Diff。记录配置变了，还记录从什么变成了什么。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524765" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.2 变更叠加分析（Change Overlay）</h4><p>变更观测的核心价值在于上下文的融合。在观测云的所有时序图表（Metric Charts）上，系统会自动叠加变更事件的标记（Annotations）。</p><ul><li><p>场景示例：</p><ul><li>传统视图：看到 API 错误率曲线在 14:00 突然飙升，SRE 开始排查日志。</li><li>变更观测视图：看到错误率飙升的同时，时间轴上显示 13:59 分有一个“支付服务 v3.2 Canary 发布”的标记。鼠标悬停即可看到该发布的 Commit Message 和变更人。</li></ul></li></ul><p>这种直观的视觉关联，能够将 MTTR（平均修复时间）从小时级缩短至分钟级。运维人员不再需要去各个聊天群里询问“刚才谁动了线上环境？”，变更观测直接给出了答案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524766" alt="图片" title="图片" loading="lazy"/></p><h4>3.2.3 变更风险评分与智能门禁</h4><p>结合 Arbiter 引擎的历史分析能力，系统能对每一次变更进行风险评分。如果某次代码提交修改了核心链路的关键文件，且缺乏足够的测试覆盖率，或者历史数据显示该开发者的变更回滚率较高，系统将在变更发生前发出预警，甚至联动 CI/CD 流水线进行阻断。</p><h2>4. Obsy AI SRE Agent 推出：可交互的根因分析侦探</h2><p>观测云 2026 颠覆了传统人找数据的排查模式，推出了一套基于 动态假设树（Dynamic Hypothesis Tree） 的交互式排查界面。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524767" alt="图片" title="图片" loading="lazy"/></p><h3>4.1 触发与情境感知</h3><p>当监控器发现异常（例如 flight-query-api 接口响应时间 P99 &gt; 2s），系统将直接启动 Obsy AI SRE Agent。在观测云的 Console 中，用户会看到一个关联了错误上下文（Error Trace、Latency Chart）的交互式卡片。</p><h3>4.2 动态假设引擎（Dynamic Hypothesis Engine）</h3><p>AI Agent 不会盲目列出所有指标，而是像一位经验丰富的 SRE 工程师一样进行逻辑推演。它会基于当前的异常特征，生成多条排查路径（Investigative Plans），并依据历史数据和专家知识库计算出每一条路径的置信度概率：</p><ul><li>Plan A (概率：高)：假设为数据库超时（DB Connection Block / Slow SQL）。</li><li>Plan B (概率：低)：假设为上游依赖服务响应变慢。</li><li>Plan C (概率：低)：假设为网络网关故障。</li></ul><h3>4.3 交互式思维导图与递归诊断</h3><p>用户点击高概率的 Plan A，界面将展开一个可视化的排查思维导图。这不仅仅是静态图表，而是 AI 正在执行的逻辑动作流：</p><ul><li>节点展开：Agent 自动检查 "RDS 资源水位" -&gt; "数据库连接池状态" -&gt; "慢查询日志分析"。</li><li>执行验证：每个节点会显示执行状态（Check Passed / Failed）。例如，AI 发现连接池正常，但捕获到了一条全表扫描的慢 SQL。</li><li>根因锁定：当 AI 找到确凿证据（如：flight_no 字段缺失索引导致全表扫描），它会标记为“Root Cause Identified”，并生成自然语言的结论报告。</li></ul><h3>4.4 闭环与反馈</h3><ul><li>对话式追问：在锁定根因后，用户可以直接与 Agent 对话：“如何修复这个问题？”Agent 会根据知识库提供 Runbook 建议（如：添加索引的 SQL 语句）。</li><li>多路径回溯：如果 Plan A 的排查结果显示一切正常（Negative Result），Agent 会智能建议用户切换至 Plan B 或 Plan C。系统会自动保留已排查过的路径记录，避免重复工作，直到递归找到真正的问题源头。</li><li>人工接管：整个 UI 包含清晰的 "Abort/Take Over" 按钮，允许工程师随时打断 AI 的自动化逻辑，手动介入排查。</li></ul><p>这套设计融合了现代工程美学与 AI 智能，将原本黑盒的 AI 思考过程透明化（White-box），让 SRE 既能享受 AI 的效率，又能保持对排查逻辑的掌控。</p><h2>5. GuanceDB 演进策略：云原生内核的重构</h2><p>GuanceDB 3.0 是观测云强健的心脏。现有的数据库架构大多基于本地磁盘（Shared-Nothing 架构），在面对 PB 级数据时，扩展成本高昂且缺乏弹性。GuanceDB 3.0 的核心目标是演进为基于对象存储（S3-Native）的存算分离架构。在这一演进过程中，我们必须正视目前与行业标杆的技术差距，并提出针对性的优化策略。</p><h3>5.1 关键演进挑战与探索方向</h3><p>GuanceDB 的演进要解决对象存储带来的物理限制：高延迟与元数据管理。</p><h4>5.1.1 挑战一：海量小文件元数据瓶颈 (Metadata Bottleneck)</h4><ul><li>痛点：在实时写入场景下（如 IoT），会产生数以亿计的小文件（Objects）。如果 GuanceDB 3.0 的元数据层不够强大，查询时的“列出文件”操作就会成为瓶颈，导致查询超时。</li><li><p>演进方向：分布式元数据架构</p><ul><li>探索：不再依赖单体 SQL 数据库存储元数据。探索分布式 Key-Value 存储来构建元数据层。</li><li>目标：支持每秒数十万次的元数据读写，确保即使底层有百亿个 S3 对象，查询规划器也能在毫秒级定位到需要扫描的文件。</li></ul></li></ul><h4>5.1.2 挑战二：存算分离后的查询延迟 (Cold Start Latency)</h4><ul><li>痛点：S3 的首字节延迟（TTFB）通常在几十到几百毫秒。对于“老板看数”的实时 Dashboard 场景，这种延迟是不可接受的。</li><li><p>演进方向：智能分层与分布式缓存 (Smart Tiering &amp; Caching)</p><ul><li>热数据 (Hot)：近期的数据查询直接走本地内存/磁盘，速度极快。</li><li>温数据 (Warm)：引入分布式缓存层。对于经常访问的“昨天”或“上周”的数据，在计算节点的 SSD 上进行 LRU 缓存。</li><li>冷数据 (Cold)：完全沉淀在 S3。查询时按需拉取，接受秒级延迟，换取极致成本。</li><li>价值：实现“像 SSD 一样快，像 S3 一样便宜”。</li></ul></li></ul><h4>5.1.3 挑战三：Compaction (压缩) 策略与写放大</h4><ul><li>痛点：为了优化查询，必须将 S3 上的小文件合并为大文件（Compaction）。但 S3 的 PUT 操作是收费的，且消耗网络带宽。</li><li><p>演进方向：成本感知的智能 Compaction</p><ul><li>策略：不盲目压缩。引入基于“查询热度”和“S3 计费模型”的代价函数。</li><li>探索：利用 Spot Instances（竞价实例）在云厂商的闲时运行 Compaction 任务，将小文件合并为列式存储（Parquet/ORC 变体），同时构建布隆过滤器（Bloom Filters）和 Min/Max 索引，以减少未来的扫描量。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524768" alt="图片" title="图片" loading="lazy"/></p><h2>6. 落地 Targeting Needs：场景化痛点的精准打击</h2><p>技术必须服务于业务。不同的大客户场景对数据平台的需求是截然不同的，甚至是互斥的。我们不能用一套参数满足所有人，而是提供灵活，可以满足特种需求的的数据引擎。</p><h3>6.1  场景一：实时查询（Real-Time Query）—— 老板看数</h3><ul><li>用户：CIO、CTO、NOC 监控大屏。</li><li>痛点：Dashboard 需要秒级刷新。读多写少，并发高。传统的 OLAP 引擎在处理聚合查询时延迟较高，且并发能力受限。</li><li><p>观测云 2026 解决方案：流式聚合。</p><ul><li>原理：GuanceDB 不再每次刷新都扫描原始日志。在数据摄取（Ingest）阶段，通过流式预聚合引擎（Pre-aggregation Engine）自动维护常用指标（如 Global_Error_Rate）。</li><li>效果：Dashboard 查询实际上是在读取一张极小的预计算表，无论原始数据量是 1TB 还是 1PB，大屏刷新始终保持在亚秒级。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524769" alt="图片" title="图片" loading="lazy"/></p><h3>6.2 场景二：批量报表与数据挖掘 —— 分析师的深潜</h3><ul><li>用户：SRE 专家、安全分析师、运营人员。</li><li>痛点：读少，但 IO 极重。需要扫描过去 30 天的海量日志进行根因分析或生成月度运营报告。容易导致数据库 OOM (Out of Memory) 或查询超时。</li><li><p>观测云 2026 解决方案：向量化执行引擎 + Serverless 扫描。</p><ul><li>原理：利用存算分离架构，当检测到此类大查询时，GuanceDB 动态弹出一组 Serverless 计算节点（Worker），并行扫描 S3 上的数据块。利用 SIMD 指令集和向量化执行（Vectorized Execution）加速过滤。</li><li>开放性：支持通过 DQL 导出数据到 Notebook 或外部数仓，满足深度挖掘需求。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524770" alt="图片" title="图片" loading="lazy"/></p><h3>6.3 场景三：高并发写入 —— IoT 与车联网数据海啸</h3><ul><li>用户：车企（V2X）、智能制造、IoT 架构师。</li><li>痛点：写多读少。Tag（标签）基数极高（High Cardinality）。例如，百万辆车，每辆车有唯一的 VehicleID，传统时序数据库的倒排索引会因此膨胀爆炸，导致内存溢出。</li><li><p>观测云 2026 解决方案：稀疏索引与列式存储优化。</p><ul><li>原理：放弃对高基数 Tag 建立全量倒排索引。GuanceDB 借鉴先进的架构设计，采用 稀疏索引（Sparse Indexing）和数据分区（Micro-partitions） 技术。</li><li>效果：将 VehicleID 作为排序列，通过 Min/Max 索引快速跳过无关数据块。在不牺牲写入性能的前提下，支持对高基数标签的高效过滤，彻底解决“索引爆炸”问题。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524771" alt="图片" title="图片" loading="lazy"/></p><h3>6.4 场景四：AI/LLM 可观测 —— Agent 行为治理</h3><ul><li>用户：AI 平台工程师、大模型应用开发者。</li><li>痛点：Agent 行为具有不确定性（幻觉、死循环），且 Token 成本昂贵。传统的 CPU/内存监控无法反映 AI 业务的健康度。</li><li><p>观测云 2026 解决方案：Model Telemetry 与成本归因。</p><ul><li>数据模型：引入专用的数据类型追踪 Prompt 和 Completion 的 Token 消耗、延迟、模型版本。</li><li>蓝图集成：通过蓝图实时监控 Token 消耗速率。一旦发现某个 Agent 陷入死循环（Token 消耗斜率异常），立即触发熔断机制（Action 节点），并通知开发者。</li><li>价值：进阶到 AI 业务治理，为企业节省真金白银的算力成本。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524772" alt="图片" title="图片" loading="lazy"/></p><h3>6.5 场景五：日志成本黑洞 —— 拒绝存不起，查不到</h3><ul><li>用户： 运维总监、合规审计部门、FinOps 负责人。</li><li>痛点： 日志数据量呈指数级增长（每天几十 TB），但 99% 的日志通常都用不上，只有故障时才需要回溯。传统方案要么全量索引导致存储成本天价，要么为了省钱只存 3 天导致关键数据丢失。</li><li><p>观测云 2026 解决方案： 冷热分层（Tiered Storage）+ Schema-on-Read（读时建模）。</p><ul><li>原理： GuanceDB 引入智能分层策略。热数据（最近 3 天）存高性能 SSD 并建立全索引；温/冷数据（3 天 - 3 年）自动下沉至对象存储（S3/OSS），不建立繁重倒排索引。当需要查询冷数据时，利用算子下推（Pushdown）临时扫描目标块。</li><li>效果： 将日志的长期存储成本降低 80% 以上。让企业存得起海量日志，还能在需要审计时，无需数据迁移即可直接查询历史归档。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524773" alt="图片" title="图片" loading="lazy"/></p><h3>6.6 场景六：微服务风暴 —— 抓住百万分之一的异常</h3><ul><li>用户： 架构师、中台研发负责人。</li><li>痛点： 在成百上千个微服务的调用链中，每天产生数亿条 Trace 数据。传统 APM 采用头部采样（Head-based Sampling）（如只采 1%），容易导致“关键的报错请求正好被丢弃了”，无法还原故障现场。</li><li><p>观测云 2026 解决方案： 100% 全量摄取 + 尾部采样（Tail-based Sampling）。</p><ul><li>原理： 数据进入系统时不做丢弃，先在内存缓冲区暂存。通过流式引擎实时分析整条链路的尾部状态（是否报错、是否高延迟）。只有有问题或高价值的链路才会被持久化存储，正常的无用链路自动丢弃。</li><li>价值： 在不增加存储预算的前提下，实现100% 的异常捕获率。不再靠运气抓 Bug，而是靠精准的算法。</li></ul></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524774" alt="图片" title="图片" loading="lazy"/></p><p>当然以上仅是冰山一角。观测云的统一数据底座已打破场景壁垒，无论是日志降本还是链路追踪，皆能以一套架构，从容应对万千需求。</p><h2>结语：观测云的2026</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524775" alt="图片" title="图片" loading="lazy"/></p><p>观测云 2026 的产品预告是对未来观测形态的一次预判与押注。</p><ul><li>市场在变：AI Agent 带来了复杂性，FinOps 带来了成本压力，数据主权带来了架构约束。</li><li>产品在变：蓝图将会成为企业的自动化中枢；GuanceDB 拥抱 S3，打破存储的物理边界，用云原生的架构解决云时代的规模问题。</li><li>价值在变：我们针对不同角色（CIO、SRE、IoT 架构师、AI 工程师等等）提供不同场景都可用的灵活解决方案。</li></ul><p>对于 CTO 和 CIO 而言，选择观测云 2026，不仅是选择了一个监控平台，更是选择了一套能够驾驭 AI 时代不确定性、从容应对数据洪流的系统。请查收这份产品路线图。</p>]]></description></item><item>    <title><![CDATA[从“救火”到“预见”：汽车行业操作系统智能运维解决方案 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524793</link>    <guid>https://segmentfault.com/a/1190000047524793</guid>    <pubDate>2026-01-06 18:10:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h3>汽车行业趋势与核心挑战</h3><p>近年来，新能源汽车加速普及，智能座舱、车联网和智能辅助驾驶等技术已成为整车厂商竞争的关键。这些功能基于端云协同架构，云端基础设施至关重要——无论是用户在车上点播音乐、远程控制车辆，还是智能车联网系统上传传感器数据，背后都离不开稳定、高效的基础设施云平台支持。</p><p>随着车辆联网率的提升以及 AI 模型能力的增强，汽车行业IT系统的数据吞吐量与计算负载呈指数级增长。一辆具备智能辅助驾驶能力的测试车，单日即可产生数 TB 的原始数据；一次面向百万用户的 OTA 升级，也可能在短时间内引发流量洪峰。在此业务特点下，云端基础设施的稳定性已成为直接影响用户体验甚至行车安全的核心环节。</p><h4>汽车行业的基础设施面临的四大核心运维挑战</h4><p>在上述业务压力下，支撑汽车场景的基础设施频繁遭遇以下四类典型问题，传统的运维手段往往难以有效应对：</p><p>1、周期性高峰业务-资源超载与系统夯机<br/>在 OTA 推送或早晚高峰、节假日远程控制集中触发时，服务器内存和 CPU 瞬时过载，系统进入“假死”状态——进程无法调度、命令无响应，即使未完全宕机，业务也已不可用。</p><p>2、出行服务下的资源超卖-内存失控与服务中断<br/>内存泄漏、缓存膨胀或显存异常增长等问题隐蔽性强，初期不易察觉，但会逐步耗尽系统资源，最终触发OOM（Out-Of-Memory）导致关键进程被强制终止，服务中断。</p><p>3、车联网服务响应迟滞-性能抖动与偶发卡顿<br/>系统在多数时间运行正常，却偶尔出现毫秒级延迟突增，且无法稳定复现。这类问题通常源于锁竞争、高频系统调用或 I/O 瓶颈，传统监控指标难以捕捉根因。</p><p>4、智能驾驶业务-智算可观测能力缺失<br/>在 GPU 集群中，显存使用异常、NCCL 通信失败、任务卡死等问题频发，但缺乏从应用层到硬件层的全栈观测能力，导致排查周期长、依赖人工经验，严重影响模型训练与推理效率。</p><p>这些问题共同指向一个核心诉求：汽车行业需要一套能够贯通“应用—操作系统—硬件”的智能运维体系，实现故障的提前预警、精准定位与自动恢复，而非被动响应。</p><h3>通过操作系统管理平台一站式解决 OS 运维卡点</h3><h4>操作系统管理平台介绍</h4><p>操作系统控制台是阿里云自研的操作系统管理平台，覆盖主流 Linux 操作系统，旨在为客户提供便捷易用、高效、专业的操作系统生命周期管理能力，包括运维管理、操作系统智能助手 OS Copilot、订阅等功能，支持通过界面、OpenAPI、MCP、CLI 等多种方式提供服务。致力于降低操作系统的技术门槛，通过系统解决客户应用与云平台运维信息不对称等问题，提升用户的云上体验。操作系统控制台智能运维可以让用户摆脱冗长的运维垂直栈和分析链，让平台更懂用户业务的异常根因，懂资源的消耗。</p><p>操作系统控制台地址：<a href="https://link.segmentfault.com/?enc=vmTNRifaTQOsj1de9lMeow%3D%3D.uYj4k8BtGkFAkkSovLqFxFlnyZL96ZPP%2FZ%2FvAgkmMfOIAvB7mEJWlvfr%2FDOs6kjW" rel="nofollow" target="_blank">https://alinux.console.aliyun.com/</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524795" alt="图片" title="图片"/></p><h4>解决方案概述</h4><p>面对智能座舱与自动驾驶业务对云端基础设施提出的高并发、低延迟、强稳定等严苛要求，传统运维手段已难以应对资源超载、内存失控、性能抖动和 AI 任务异常等复杂问题。操作系统控制台作为面向汽车行业的综合运维平台，致力于打通“应用—操作系统—硬件”全栈链路运维能力。</p><h4>场景化解决方案与核心能力</h4><p>针对智能座舱、自动驾驶等业务以上提到的汽车行业四大典型运维痛点，操作系统控制台推出对应的诊断及观测能力，在常见的夯机、OOM、抖动及 AI 观测都给出了对应的解决方案，弥补汽车行业的企业在基础设施可观测性的能力短板。</p><p>应对资源超载与系统夯机 —— 主动内存保护</p><p>核心收益：解决用车周期性高峰业务场景，资源的夯机问题，减少业务卡顿及异常。</p><p>适用场景：OTA 大规模推送、远程控制指令洪峰、AI 模型高并发推理等瞬时高负载场景。</p><p>在高峰期，系统常因内存迅速耗尽而进入“near-OOM”状态，传统 Linux OOM 机制响应滞后，往往在系统已卡死或无响应后才触发进程终止，且易误杀缓存型或 I/O 密集型进程，进一步加剧磁盘压力。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524796" alt="图片" title="图片" loading="lazy"/></p><p>通过以下机制实现主动防护：</p><ul><li>堆内存精准评分：不再依赖 RSS（常驻内存），而是聚焦可回收的堆内存使用量，更准确识别真正造成内存压力的进程。</li><li>批量终止策略：单次释放不足以缓解压力时，可同时终止多个高内存占用进程，快速释放大量内存。</li><li>多级压力响应：支持低、中、高三档灵敏度配置，适配不同业务对延迟的容忍度。</li><li>关键进程白名单：通过进程名或命令行参数显式保护车控、推理等关键服务，避免误杀。</li></ul><p>在内存压力上升初期即介入干预，有效防止系统夯机，保障远程控制、OTA 下发等关键指令的可达性和执行时效。</p><p>破解内存黑盒 —— 内存全景分析</p><p>核心收益：解决出行出行服务下的资源超卖所引起的服务中断问题，提升业务连续性。</p><p>适用场景：内存使用率持续飙升、频繁触发 OOM、缓存占用异常、GPU 显存增长不明等复杂内存问题。</p><p>传统运维难以回答“内存到底被谁用了”——是应用泄漏？文件缓存堆积？还是驱动或 GPU 隐性占用？内存全景分析提供统一、细粒度的内存视图。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524797" alt="图片" title="图片" loading="lazy"/></p><ul><li>一键生成全链路报告：无需登录机器，控制台点击即可输出包含进程、容器、缓存、驱动、GPU 显存的完整内存分布。</li><li>穿透应用堆内存：支持对 Java、Python、C++ 等语言进程的堆内对象进行二次拆解，定位具体泄漏点。</li><li>关联缓存与原始文件：如识别出“/ota/firmware_v2.1.bin”占用了 8GB page cache，便于优化预加载或清理策略。</li><li>纳入 GPU 与网卡内存：将 RDMA 缓冲区、GPU 显存映射等“不可见”内存纳入监控范围，消除盲区。</li></ul><p>内存全景从“猜测谁吃内存”转变为“秒级定位泄漏源”，显著缩短故障排查时间，支撑容量规划与资源优化。</p><p>消除性能抖动 —— 进程热点分析</p><p>核心收益：解决车联网服务响应迟滞问题，提升用户体验。</p><p>适用场景：偶发性卡顿、CPU 或 I/O 突发飙升、毫秒级延迟突增等难以复现的性能问题。</p><p>这类问题往往无固定复现路径，传统监控无法捕获瞬时调用栈，导致根因长期悬而未决。</p><p>进程热点分析基于 eBPF 实现轻量、持续追踪，该功能具有以下特点：</p><ul><li>小于 3% 性能开销：无侵入采集函数调用栈、上下文切换、系统调用等数据，适用于生产环境长期运行。</li><li>火焰图 + Diff 对比：直观展示 CPU 热点路径，并支持抖动前后或版本升级前后的性能差异比对，自动高亮退化点。</li><li>智能识别：结合大模型语义理解，识别高频/proc 访问、锁竞争、阻塞 I/O 等常见性能陷阱，并给出优化建议。</li><li>秒级回溯抖动时刻：系统持续缓存轻量调用栈，问题发生时可立即锁定瞬时高负载进程及其热点函数。</li></ul><p>进程热点分析解决了“无法复现”的性能难题，让偶发卡顿变得可追踪、可解释、可修复。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524798" alt="图片" title="图片" loading="lazy"/></p><p>保障智算基础设施的稳定 —— GPU 持续追踪时序图</p><p>核心收益：解决智能驾驶业务在 GPU 场景运维难的问题，提升训推效率，节省成本。</p><p>适用场景：自动驾驶模型训练、vLLM 等大模型推理、多 GPU 通信任务等 AI 密集型负载。</p><p>AI 任务对 GPU 资源稳定性高度敏感，但显存泄漏、XID 错误、通信瓶颈等问题往往隐蔽且难定位。</p><p>操作系统控制台构建基于内核的持续追踪体系，它具有以下特点：</p><ul><li>分钟级异常告警：实时监控显存、SM 利用率、温度、XID 错误码等，及时发现GPU掉卡、硬件报错或任务卡死。</li><li>小时级问题定界：支持慢节点识别、NCCL 通信延迟分析、单卡/整机资源瓶颈判断，快速缩小排查范围。</li><li>函数级根因剖析：通过 GPU 火焰图和 Timeline Profiling，将 Python 层调用、框架算子与 CUDA Kernel 关联，可视化算子执行序列与等待时间。</li><li>让 AI 任务“看得见、说得清、改得准”，避免因底层资源异常导致训练中断或推理延迟，提升 AI 基础设施可靠性。</li></ul><h3>行业成功案例分享</h3><h4>案例一：车机服务高峰期无响应</h4><p>案例背景</p><p>某头部物流行业用户节假日出现业务无响应、登录实例也十分卡顿。通过监控发现客户实例使用的内存在某个时间点开始徒增，接近系统的总内存（即 available 非常低），但没有超过系统总内存。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524799" alt="图片" title="图片" loading="lazy"/></p><p>通过 top 命令可以看到系统的 CPU sys 利用率和 iowait 利用率和系统负载都持续飙高，kswapd0 线程占用非常高的 CPU 进行内存回收。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524800" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>通过配置开启节点级别的 FastOOM 功能，由于业务是实验较为敏感的业务，内存压力选择中，且设置业务程序（以 python 启动，进程名包含 python 子串）为避免被 OOM 进程且设置无关的日志程序优先杀死。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524801" alt="图片" title="图片" loading="lazy"/></p><p>开启后，当节点内存水位处于 near-OOM 状态时，用户态提前介入，根据配置杀死了如下进程，从而释放了部分内存避免系统进入了夯机状态。通过操作系统控制台的系统概览可以看到 FastOOM 介入的相关记录。</p><p>如下图所示，由于 kube-rbac-proxy 和 node_exporter 等进程 oom_score_adj 被设置为接近 999，FastOOM 会匹配内核策略优先杀死这些进程，但是由于杀死这些进程后释放内存较小，仍处于 near-OOM；因此 FastOOM 杀死了配置优先杀死的 logcollect 进程。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524802" alt="图片" title="图片" loading="lazy"/><br/>由于用户态及时介入杀死进程释放出内存，使系统避免进入了near-OOM的抖动状态。</p><h4>案例二：AI 推理场景显存异常增长</h4><p>案例背景<br/>某头部自动驾驶方案公司部署的 vLLM 线上推理服务：KV-Cache 利用率并未打满，但通过 GPU 监控（DCGM）观察到有显存明显增长。vLLM 启动时使用显存预分配机制，在 KV-Cache 利用率未满情况下理论显存值不应上涨。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524803" alt="图片" title="图片" loading="lazy"/></p><p>解决方案</p><p>对在线业务应用进行 continuous Profiling，在 TimeLine 上找到显存申请的 cudaMalloc 调用，打上标记线，即可找到具体的 Python 调用，进一步定位到导致显存额外申请的调用栈如下所示，结合 decorate_context() 实现可以判断出显存增长的原因 Torch 的缓存管理机制，可以通过调整 vLLM 显存预占或 Torch 缓存的显存占用环境变量来进行相应的问题规避。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524804" alt="图片" title="图片" loading="lazy"/></p><h4>案例三：智能汽车全球发布会——高并发实时交互下的零卡顿保障</h4><p>案例背景<br/>2025 年春季，某智能电动汽车品牌在全球同步发布其旗舰车型，并启动大规模整合营销活动。由于发布会覆盖全球多个时区，且关键的价格公布环节引发高度关注，价格揭晓后 5 分钟内，App 总访问量突破 800 万，商城相关接口请求峰值高达 12 万 QPS，整体流量达日常水平的 200 倍以上。在此极端并发场景下，系统面临严峻挑战：核心交互接口的端到端响应必须控制在 30ms 以内，任何毫秒级的延迟都可能导致 APP 白屏、操作无响应或直播卡顿，严重影响用户体验并威胁品牌形象。与此同时，瞬时流量洪峰、极致的体验敏感性以及秒级故障定位与恢复的严苛要求，使得传统依赖日志回溯的运维排查方式完全失效，系统稳定性与实时可观测性面临前所未有的考验。</p><p>解决方案<br/>依托操作系统控制台构建“三位一体”保障体系：</p><p>1.高并发资源防护 —— 主动内存保护 + 关键进程隔离 提前识别车控指令服务、视频流网关、身份认证微服务为 关键路径组件，加入 FastOOM 白名单； 配置中等灵敏度内存压力策略，在系统进入 near-OOM 前主动释放低优先级进程内存，避免 kswapd 抢占 CPU 导致 API 延迟飙升，实现发布会全程实现 “零白屏、零卡顿、零交互失败”。</p><p>2.实时性能监控 —— 进程热点分析持续追踪 全链路启用 eBPF 驱动的进程热点分析，持续采集函数调用栈； 当某区域用户集中反馈“点击无反应”时，系统秒级回溯到问题时刻； 结合大模型辅助诊断，自动建议“缓存网络指标”而非实时读取，热修复后延迟 P99 从 50ms 降至 30ms。</p><h3>展望</h3><p>随着智能电动汽车的持续发展，车载系统与云端基础设施的耦合将更加紧密。未来，汽车不仅是交通工具，更是移动的计算终端和数据节点。这要求云平台不仅具备更强的弹性、更低的延迟和更高的可靠性，还需在资源调度、故障自愈和性能优化等方面实现更深层次的智能化。</p><p>操作系统控制台将持续围绕汽车行业核心场景打磨能力。一方面，我们将进一步强化对高并发、高实时性业务的支持，优化 FastOOM、内存全景分析、进程热点追踪等能力在 OTA 洪峰、自动驾驶训练推理等典型负载下的表现；另一方面，我们将探索 AI 驱动的智能运维（AIOps）路径，结合大模型与实时可观测数据，构建具备预测、诊断、决策和执行能力的 AI Agent 运维体系。</p><p>联系我们 </p><p>您在使用操作系统控制台的过程中，有任何疑问和建议，可以搜索群号：94405014449 加入钉钉群反馈，欢迎大家扫码加入交流。</p>]]></description></item><item>    <title><![CDATA[专访 | 不仅是适配，更是定义标准：中兴新支点在龙蜥大会交出的“2025答卷” 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524830</link>    <guid>https://segmentfault.com/a/1190000047524830</guid>    <pubDate>2026-01-06 18:09:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>编者按：近日，2025 龙蜥操作系统大会（OpenAnolis Conference）在北京圆满召开，主题为“生态共融·智驱未来”。现场汇聚了逾千位全球技术领袖、业界精英和行业开发者，共同探讨 AI 时代下操作系统产业发展的新趋势、新挑战与新路径。作为龙蜥社区的核心理事单位及龙蜥大会的承办方之一，中兴通讯走向台前，分享了其在 OS 架构创新与场景拓展方面的深度思考，并设立专门的分论坛与展区，集中展示新支点操作系统在 RISC-V 及工业领域的最新核心成果。大会前夕，广东中兴新支点技术有限公司总经理王健雄在与 InfoQ 的深度对话中描绘了一幅全新的战略版图：2025 年，中兴新支点操作系统已经不仅仅满足于传统的架构适配，而是向“AI 原生”与“RISC-V 深水区”发起全面冲击。以下为采访全文：<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524832" alt="图片" title="图片"/></p><p>过去几年，中兴新支点操作系统常常被贴上“工业级”、“安全稳定”、“电信场景专家”的标签。但在 2025 年，中兴在操作系统上显然有着更大的雄心。</p><h3>V7 全面适配主流大模型，RISC-V 全栈技术得以攻坚</h3><p>王健雄表示，中兴通讯一直都高度重视和开源社区的互动与合作，目前已组建起专业的团队，深度参与龙蜥社区的 AI 生态建设，在技术贡献、组织协同和生态共建方面都在持续投入。</p><p>他还强调，2025 年中兴新支点发布的新支点服务器操作系统 V7 作为面向 AI 时代的新型操作系统，全面适配了主流的 AI 硬件大模型及计算框架，具备内生智能特性，加速了用户在 AI 场景的落地进程。</p><p>面对 AI 算力爆发与大模型落地的行业浪潮，操作系统作为承上启下的核心软件底座，必须完成从“管理资源”到“加速智能”的进化。</p><p>王健雄在采访中透露，2025 年，“新支点服务器操作系统 V7”版本的发布，也并不仅仅是一次版本号的迭代，更是一次产品形态的重塑。V7 版本被定义为“面向 AI 时代的新型操作系统”，其核心战略价值体现在两个维度：</p><ul><li>具备“内生智能”特性：区别于过去“打补丁”式的 AI 支持，V7 版本在内核层构建了智能特性，能够全面适配主流的 AI 硬件（GPU/NPU/DPU）、大模型（如DeepSeek）以及主流计算框架。</li><li>加速场景落地：这一新版本将不仅服务于中兴自身的通信产品体系，更致力于显著加速用户在 AI 场景下的落地进程，解决软硬协同难、部署效率低的问题。</li></ul><p>如果说 AI 是应用层的风口，那么 RISC-V 则是芯片架构层的“未来变量”。在多架构支持方面，中兴新支点在 2025 年也展现出“All in”的姿态，尤其在 RISC-V 领域，其投入力度和技术深度均超出传统的适配范畴。</p><p>王健雄详细拆解了中兴在 RISC-V 领域的四大核心动作，标志着其工作重心正从“基础适配”转向“性能进取”：</p><ul><li>全量组件构建与社区首发：中兴推动成立了 RISC-V SIG 2.0 协作组，计划在2025年 完成全量 2000+ 组件的构建，并发布社区首个 RISC-V 同源预览版本。这意味着 RISC-V 在龙蜥生态中将拥有与 x86/Arm 同等的核心地位。</li><li>底层性能的极致优化：针对RISC-V 基础库的性能，中兴进行了向量化（Vectorization）及汇编级的优化。这表明中兴已经不满足于软件“能跑”，而是要让软件在 RISC-V 硬件上“跑得快”。</li><li>关键特性支持（RVA23）：基于 Linux6.6 内核基线，新支点操作系统将实现对 RISC-V 服务器关键特性的支持，包括 AIA（高级中断架构）、IOMMU（输入输出内存管理单元）以及最新的 RVA23 指令集。这些硬核技术的突破，是 RISC-V 能够真正走进高性能计算和数据中心场景的前提。</li><li>通过 V7 版本的发布与 RISC-V 全栈技术的攻坚，中兴新支点操作系统在 2025 年的战略意图十分清晰：在龙蜥社区的多架构生态中，构建一个既能承载传统通信业务高稳定性，又能适应 AI 时代异构算力挑战的“双模”底座。</li></ul><h3>填补全球空白：首个电信级“AI 试金石”</h3><p>在通用 AI 模型“卷”参数量的当下，垂直行业的 AI 落地却面临着一个隐蔽的痛点：缺乏针对特定工业场景的标准化评测基准。对于通信行业而言，这一问题尤为突出——5G 网络切片、6G 信道建模等场景极其复杂，通用数据集难以覆盖，导致电信级 AI 模型的训练与调优往往面临“无尺可量”的窘境。</p><p>2025 年，中兴新支点打破了这一僵局，将其在通信领域深耕多年的“独门秘籍”——TFCE 数据集贡献到开源社区，试图为行业建立一套全新的 AI 评测标准。</p><p>王健雄在采访中强调，TFCE 数据集并非普通的行业数据包，而是“全球首个面向电信场景的函数级、调用级 AI 评测标准”。这一成果的开源，填补了国际上在电信 AI 测评领域的空白。</p><p>TFCE 的核心价值在于其“广度”与“精度”。根据中兴披露的数据，该数据集包含了超过 20 万条通信设备的函数调用样本，覆盖了 5G 核心网、计算、智能运维、传输等八大典型电信场景。</p><p>通过与龙蜥社区的深度协同，TFCE 不再仅仅是静态的数据，它提供了一套标准化的接口协议与性能指标体系（涵盖时延、准确率、资源占用等维度）。这意味着，未来开发者在龙蜥操作系统上开发通信类 AI 应用时，将拥有一把“标准尺”，彻底解决了电信 AI 模型训练中“缺乏行业基准（Benchmark）”的痛点。</p><p>数据标准的建立，最终是为了反哺系统性能。中兴新支点操作系统团队并没有止步于数据集的开源，而是将 TFCE 的评测工具链与操作系统进行了深度融合。</p><p>这种“操作系统+行业数据”的协同效应，直接转化为惊人的性能数据。王健雄列举了两组极具说服力的成果：</p><ul><li>硬件加速效率提升：通过 TFCE工 具链优化通信函数调用在底层硬件上的执行路径，新支点操作系统成功将 DPU（数据处理器）和 NPU（神经网络处理器）的资源利用率提升了 40%。</li><li>运维推理速度翻倍：在基站故障日常运维这一高频场景中，基于 TFCE 优化的智能运维推理引擎，将推理效率提升了 60%。</li></ul><p>从中可以看出，中兴在 2025 年的打法非常务实：利用独有的行业数据资产（TFCE），驱动操作系统内核与 AI 硬件的深度磨合。这不仅提升了新支点操作系统的行业竞争力，更为龙蜥社区在工业、通信等垂直领域的 AI 落地提供了教科书级别的范例。</p><h3>攻坚深水区——解决“算力与通信融合”的四大痛点</h3><p>当 AI 的浪潮涌入 5G/6G 通信网络，“云网融合”不再是一个概念，而是具体的工程挑战。王健雄直言，在 AI 与通信技术深度融合的当下，服务器操作系统正面临着前所未有的“异构算力调度”压力。</p><p>传统的操作系统设计以 CPU 为中心，而现在的智算场景需要 CPU、GPU、DPU、NPU 等多芯协同。这种架构的错位，直接导致了行业内一个尴尬的现状：AI服务器的算力利用率普遍仅在 30%-40% 左右徘徊。</p><p>为了打破这一瓶颈，中兴新支点将 2025 年的技术攻坚重点锁定在“四大核心痛点”上，并给出了基于龙蜥生态的破局之道。</p><h4>直面四大“拦路虎”</h4><p>王健雄详细剖析了当前阻碍 AI 在通信场景落地的技术壁垒：</p><p>算力调度与适配难题：传统 OS 难以高效管理异构资源，且不同硬件厂家的驱动接口差异巨大，导致操作系统如同“盲人摸象”，难以进行全局最优调度。</p><p>低时延与高稳定性的双重挤压：AI 运维需要毫秒级响应，智能体调度要求指令实时下发。但传统 OS 的内存访问、中断处理等环节往往会引入不可控的时延抖动，这对于要求“确定性网络”的通信业务是致命的。</p><p>全栈生态的“变速齿轮”：上层 AI 框架（如PyTorch、TensorFlow）迭代周期缩短至季度级，而底层的通信协议（5G/6G 接口）极其复杂且严谨。操作系统夹在中间，若无法动态适配这种“快慢齿轮”的差异，将直接卡死业务落地。</p><p>运维智能化的缺失：面对大规模异构集群，传统 OS 缺乏全链路感知能力，难以发现 GPU 算力瓶颈或网络微拥塞，导致“有故障难定界”。</p><h4>系统级解法：构建“确定性”与“弹性”兼备的底座</h4><p>面对上述挑战，中兴新支点并没有选择“头痛医头”，而是联合龙蜥社区，从内核与架构层面进行了深度的重构与优化。</p><p>智能算力调度（解决利用率低）：中兴作为发起人之一，加入龙蜥智算基础设施联盟，推进算力调度标准的制定。在最新的 V7 版本中，新支点操作系统内置了智能资源调度优化算法。这一算法能够像交通指挥官一样，动态感知 AI 训推任务的负载波动，弹性匹配最优算力组合。结合龙蜥社区的 AI 增强组件，成功打破了“以 CPU 为中心”的调度桎梏，显著提升了异构算力的利用效率。</p><p>电信级低时延优化（解决抖动）：发挥中兴在通信领域的“看家本领”，新支点操作系统对高性能网络协议栈和内存访问效率进行了深度改造。通过内核级的优化，支持控制面与用户面分离、实时任务优先调度等关键特性，为 AI 通信场景构建了一个“高稳定、低抖动”的操作系统底座，确保关键指令“即发即达”。</p><p>通过这一系列技术攻坚，中兴新支点证明了：在智算时代，操作系统不再是底层的“透明管家”，而是决定算力能否转化为生产力的“核心调度引擎”。</p><h3>从“独行”到“受众”，中兴引领制定行业标准</h3><p>技术的领先往往始于单点的突破，但生态的繁荣必须依赖标准的统一。</p><p>在展望未来时，王健雄传递出一个明确的信号：中兴新支点正在试图将其在电信、金融、电力等关键行业积累的“专属经验”，转化为龙蜥社区的“公共财富”。</p><p>2025 年，中兴新支点操作系统的迁移方案凭借在电信 NFV（网络功能虚拟化）场景下的优异表现，成功入选“鼎新杯”典型案例。但这在中兴看来，仅仅是一个开始。</p><p>王健雄表示，中兴计划利用其作为龙蜥社区副理事长单位的影响力，将这些经过大规模商用验证的高可用、低时延、实时虚拟化性能优化经验，沉淀为“社区共性技术需求”。</p><p>更具战略意义的是，中兴计划联合产业链伙伴，共同制定“电信级操作系统适配标准”。这一标准将对操作系统的时延抖动、系统可靠性、安全性与资源调度能力提出量化要求。</p><p>这意味着，未来龙蜥社区的操作系统在进入关键基础设施领域时，将拥有一套清晰的“准入体检单”，这将极大地推动国产操作系统从“能用”向“好用、耐用”跨越。</p><p>如果把目光投向更远的未来，中兴新支点也已经开始 6G 场景的前瞻性布局。</p><p>不同于 5G，未来的 6G 网络将呈现“连接+计算+智能”的高度融合特征。王健雄透露，新支点操作系统将重点强化分布式组网、数据平面与计算平面的协同、以及服务化架构的能力。其目标是构建一个能够支撑 6G 网络“可编程”与“原生 AI”特性的底座，同时依然保持工业级的高安全与高实时性。</p><p>此外，在 RISC-V 全场景支持与智算操作系统方向，中兴也将持续投入。通过支持异构算力的统一纳管，为边缘计算、物联网等泛在智能场景提供坚实基础。</p><p>从深耕 RISC-V 架构到开源 TFCE 行业数据，从解决异构算力调度难题到制定电信级适配标准，中兴新支点在 2025 年的进化路径，清晰地折射出国产操作系统行业的发展缩影：</p><p>它正在走出单纯的“国产替代”逻辑，迈向以 AI 为核心、以多架构为两翼、以行业标准为护城河的“技术深水区”。在龙蜥社区这片开源沃土上，中兴新支点正在努力证明，一个懂通信、懂 AI 的操作系统，将如何成为支撑数字经济未来的坚实脊梁。</p><p>—— 完 ——</p>]]></description></item><item>    <title><![CDATA[前端性能革命：200 行 JavaScript 代码实现 Streaming JSON 冴羽 ]]></title>    <link>https://segmentfault.com/a/1190000047524856</link>    <guid>https://segmentfault.com/a/1190000047524856</guid>    <pubDate>2026-01-06 18:08:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>1. 前言</h2><p>5 月的时候，React 的核心开发者 Dan 发表了一篇名为<a href="https://link.segmentfault.com/?enc=6%2By9R1cb4IXTtPS1F6BJ5A%3D%3D.iG%2BidHMQAd1SVrpZrVvadpFowx3fKWPvBA7435xziX6ZxLq6rNrphjHF5VFT%2FKQb" rel="nofollow" target="_blank">《Progressive JSON》</a> 的文章，介绍了一种将 JSON 数据从服务器流式传输到客户端的技术，允许客户端在接收到全部数据之前就开始渲染部分数据。</p><p><strong>这可以显著提升用户体验，尤其是处理大型数据集时。</strong></p><p>让我们以“获取用户文章”这个场景为例。</p><p>这是一个完整的数据结构：</p><pre><code class="json">{
  "user": {
    "id": 1,
    "name": "John Doe",
    "posts": [
      { "id": 101, "title": "First Post", "content": "..." },
      { "id": 102, "title": "Second Post", "content": "..." }
    ]
  }
}</code></pre><p>假设我们能够很快获取用户信息，但文章数据还需要一段时间从数据库获取。</p><p>与其等待数据完全加载完毕，不如先发送一个占位符表示文章字段：</p><pre><code class="json">{
  "user": {
    "id": 1,
    "name": "John Doe",
    "posts": "_$1"
  }
}</code></pre><p>客户端收到数据后，先将用户信息渲染出来。</p><p>然后，当文章数据准备完毕后，我们将文章数据作为一个单独的 chunk 发送：</p><pre><code class="json">{
  "_$1": [
    { "id": 101, "title": "First Post", "content": "..." },
    { "id": 102, "title": "Second Post", "content": "..." }
  ]
}</code></pre><p>客户端收到数据后，最后将文章数据渲染出来。</p><p>要实现这样一个功能，客户端需要具备处理这些占位符的能力，并在最终数据到达时替换为实际数据。</p><p>如果要实现这样一个单独的功能需要多少代码呢？</p><p><strong>200 行就可以！</strong></p><p>本篇文章和大家介绍下实现思路，供大家学习和思考使用。</p><h2>2. 服务端实现</h2><p>让我们来看下服务器端实现。</p><p>首先是服务端函数。</p><pre><code class="javascript">function serve(res, data) {
  res.setHeader("Content-Type", "application/x-ndjson; charset=utf-8");
  res.setHeader("Transfer-Encoding", "chunked");

  // 向客户端发送 chunks
  res.write(JSON.stringify(...) + "\n");
  res.write(JSON.stringify(...) + "\n");

  // 当完成的时候
  res.end();
}</code></pre><p>这里有 2 点值得注意：</p><ol><li><strong>我们使用了 <code>application/x-ndjson</code>内容类型。</strong></li></ol><p>NDJSON，全拼 Newline Delimited JSON，其实就是一种换行符分割的 JSON，其中每一行都是一个有效的 JSON 对象。这允许我们在单个响应中发送多个 JSON 对象，并以换行符分隔。</p><ol start="2"><li><strong>我们使用了 <code>Transfer-Encoding: chunked</code>响应头。</strong></li></ol><p>使用该响应头，可以通知客户端，响应将分块发送。在调用 <code>res.end()</code>之前，请保持连接活跃状态。</p><p>其次，我们需要对数据进行分块。</p><p>实现方式也很简单，遍历数据对象，并用占位符替代那些暂时没有准备好的部分。</p><p>当遇到需要稍后发送的部分（一个 Promise）时，我们将其存储到队列中，并在准备就绪后，将其作为单独的数据块发送。</p><p>函数如下：</p><pre><code class="javascript">function normalize(value) {
  function walk(node) {
    if (isPromise(node)) {
      const id = getId();
      registerPromise(node, id);
      return id;
    }
    if (Array.isArray(node)) {
      return node.map((item) =&gt; walk(item));
    }
    if (node &amp;&amp; typeof node === "object") {
      const out = {};
      for (const [key, val] of Object.entries(node)) {
        out[key] = walk(val);
      }
      return out;
    }
    return node;
  }
  return walk(value);
}</code></pre><p>函数递归遍历数据对象。</p><p>当遇到 Promise 时，它会生成一个唯一的占位符 ID，注册该 Promise 以便稍后解析，并返回该占位符。</p><p>对于数组和对象，它会递归处理它们的元素或属性。原始值将按原样返回。</p><p>这是注册 Promise 的代码：</p><pre><code class="javascript">let promises = [];

function registerPromise(promise, id) {
  promises.push({ promise, id });
  promise.then((value) =&gt; {
    send(id, value);
  }).catch((err) =&gt; {
    console.error("Error resolving promise for path", err);
    send(id, { error: "promise error", timeoutMs: TIMEOUT });
  });</code></pre><p>这是 <code>send</code> 的代码，<code>send</code>函数负责将解析后的数据发送给客户端：</p><pre><code class="javascript">function send(id, value) {
  res.write(JSON.stringify({ i: id, c: normalize(value) }) + "\n");
  promises = promises.filter((p) =&gt; p.id !== id);
  if (promises.length === 0) res.end();
}</code></pre><p>该 <code>send</code> 函数会向响应中写入一个新的数据块，其中包括占位符 ID 和 normalize 后的值。然后它会从队列中移除已经 resolve 的 Promise。如果没有其他要处理的 Promise，它就会结束响应，从而关闭与客户端的连接。</p><p>完整的实现代码<a href="https://link.segmentfault.com/?enc=lx8v9k5RX8%2F4x5MN1UxhWA%3D%3D.s6XmYz0Pk2mMxHG6N6KhfFtm9Ll7Vo8rHPxdMx%2B8Y5JB89lVXiUJlRLzvoL0lh8wGdzMXdsJyPpcysczWSqZe41aMtvgtVWgqT6tf6sP5H5EbmSOguYSMSnuz5K2WVFy" rel="nofollow" target="_blank">点击这里</a>。</p><p>最后，我们举一个从服务端发送的对象示例：</p><pre><code class="javascript">const data = {
  user: {
    id: 1,
    name: "John Doe",
    posts: fetchPostsFromDatabase(), // 返回一个 promise
  },
};

async function fetchPostsFromDatabase() {
  const posts = await database.query("SELECT * FROM posts WHERE userId = 1");
  return posts.map((post) =&gt; ({
    id: post.id,
    title: post.title,
    content: post.content,
    comments: fetchCommentsForPost(post.id), // 返回一个 promise
  }));
}</code></pre><p>每篇文章还有一个评论字段（comments），该字段是一个 Promise 对象。意味着评论数据将在文章数据发送后，作为单独的片段发送。</p><h2>3. 客户端实现</h2><p>那客户端该如何实现呢？</p><p>在客户端，我们处理传入的数据块，并将占位符替换为实际数据。</p><p>我们可以使用 Fetch API 向服务器发送请求，并将响应读取为流。每当遇到占位符时，我们都会将其替换为一个 Promise，该 Promise 将在实际数据到达时解析。</p><p>核心逻辑如下：</p><pre><code class="javascript">try {
    const res = await fetch(endpoint);
    const reader = res.body.getReader();
    const decoder = new TextDecoder();

    async function process() {
      let done = false;
      while (!done) {
        const { value, done: readerDone } = await reader.read();
        done = readerDone;
        if (value) {
          try {
            const chunk = JSON.parse(decoder.decode(value, { stream: true }));
            chunk.c = walk(chunk.c);
            if (promises.has(chunk.i)) {
              promises.get(chunk.i)(chunk.c);
              promises.delete(chunk.i);
            }
          } catch (e) {
            console.error(`Error parsing chunk.`, e);
          }
        }
      }
    }
    process();
  } catch (e) {
    console.error(e);
    throw new Error(`Failed to fetch data from Streamson endpoint ${endpoint}`);
  }
}</code></pre><p>对流的处理，你可能感到陌生，可以拓展阅读我的这篇文章：<a href="https://link.segmentfault.com/?enc=DjtitRg6QB7Nk%2BT3bOQLrg%3D%3D.KDDZFgP2wnvEeradAokQz7iit9hXJJiq%2BiUem48q4rN2KVMrTBNQxYdnv082qQg1" rel="nofollow" target="_blank">《如何用 Next.js v14 实现一个 Streaming 接口？》</a></p><p><code>process</code> 函数逐块读取响应流。每个数据块都被解析为 JSON，并调用 <code>walk</code> 函数将占位符替换为 Promise。</p><p>如果数据块包含先前注册的占位符 ID ，则相应的 Promise 会被解析为接收到的数据。关键在于 <code>await reader.read()</code>，它允许我们等待新数据到来。</p><p><code>walk</code>函数用于将占位符替换为 Promise：</p><pre><code class="javascript">function walk(node) {
  if (isPromisePlaceholder(node)) {
    return new Promise((done) =&gt; {
      promises.set(node, done);
    });
  }
  if (Array.isArray(node)) {
    return node.map((item) =&gt; walk(item));
  }
  if (node &amp;&amp; typeof node === "object") {
    const out = {};
    for (const [key, val] of Object.entries(node)) {
      out[key] = walk(val);
    }
    return out;
  }
  return node;
}
function isPromisePlaceholder(val) {
  return typeof val === "string" &amp;&amp; val.match(/^_\$(\d)/);
}</code></pre><p>类似于服务端的 <code>normalize</code> 函数。当遇到占位符的时候，它会返回一个新的 Promise，该 Promise 将在实际数据到达时解析。对于数组和对象，它会递归处理它们的元素或属性。原始值则直接返回。当然，ID 必须与服务器端生成的 ID 匹配。</p><p>完整的实现代码<a href="https://link.segmentfault.com/?enc=RLEZ%2FwEjDelIoAsCVgJ1oQ%3D%3D.B6PwLLcs1ud2JL3kGcW05RFVRIMkBOnRZKQYp%2BYqT%2FeAV2YcAyhjm9QNg6N0po4hlwOioXHfu%2B0%2BmNkIJdTaD4sR3NngQ7M4ifUKAT4YatBNBbCyYZowmalJbSzp0DOs" rel="nofollow" target="_blank">点击这里</a>。两个文件加起来一共 155 行代码。</p><h2>4. NPM 包</h2><p>本篇文章整理翻译自 <a href="https://link.segmentfault.com/?enc=tlrSktL70aUsFByoUB5vyQ%3D%3D.wEhaubUq%2Fk6fbYhO5zPdylfPUJg9fVuoAyiwXkauB4QQ5kCp6D4uc4EQr1wc%2BkLpkzubTfoR%2FX03DZY%2FkMAD%2FXG3%2FfsrGpFjlov%2BSa%2BBLvFdlgYQPgbayKxC1RICBujL" rel="nofollow" target="_blank">Streaming JSON in just 200 lines of JavaScript</a>。</p><p>作者还将代码整理成了一个 NPM 包：<a href="https://link.segmentfault.com/?enc=07yxWKdv7CyN01nfQb5Oug%3D%3D.%2BT5VNTqFJZgn9MBWcnxa4JGebICeEmN8wvgg6pBqioFIQzMSWeKdkx4ZrpVUs3K6" rel="nofollow" target="_blank">Streamson</a>。</p><p>通过 npm 安装：<code>npm intall streamson</code></p><p>服务端上使用：</p><pre><code class="javascript">import { serve } from "streamson";
import express from "express";

const app = express();
const port = 5009;

app.get("/data", async (req, res) =&gt; {
  const myData = {
    title: "My Blog",
    description: "A simple blog example using Streamson",
    posts: getBlogPosts(), // this returns a Promise
  };
  serve(res, myData);
});

app.listen(port, () =&gt; {
  console.log(`Example app listening on port ${port}`);
});</code></pre><p>客户端是一个 1KB 的 JavaScript 文件，地址：<a href="https://link.segmentfault.com/?enc=9IzGiUoGgY9EN8tavz2JzQ%3D%3D.rUGgm8DVmH1Hd1ze2ZDYRBpmRrLCEO%2BtOtYuQO0jRnMEJomK%2BgLISdnaacuUuzc4M7VSJJr27l4DP%2FyeabvyJw%3D%3D" rel="nofollow" target="_blank">https://unpkg.com/streamson@latest/dist/streamson.min.js</a></p><p>客户端使用如下：</p><pre><code class="javascript">const request = Streamson("/data");

const data = await request.get();
console.log(data.title); // "My Blog"

const posts = await request.get("posts");
console.log(posts); // Array of blog posts</code></pre><h2>5. 最后</h2><p>作为准前端开发专家的你，第一时间获取前端资讯、技术干货、AI 课程，那不得关注下我的公众号「冴羽」。</p><p>流式传输 JSON 数据是一种提升 Web 应用感知性能的有效方法，尤其适用于处理大型数据集或动态生成数据。</p><p>通过在数据可用时立即发送部分数据，我们可以让客户端更早地开始渲染内容，从而带来更佳的用户体验。</p>]]></description></item><item>    <title><![CDATA[基于 YOLOv8 的学生课堂行为检测(举手、看书、写作业、玩手机)-完整项目源码 南瓜 ]]></title>    <link>https://segmentfault.com/a/1190000047524870</link>    <guid>https://segmentfault.com/a/1190000047524870</guid>    <pubDate>2026-01-06 18:07:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>基于 YOLOv8 的学生课堂行为检测-完整项目源码</h2><h3>一、问题背景：为什么要做“课堂行为识别”</h3><p>在智慧校园和数字化教学逐步落地的过程中，<strong>课堂行为数据</strong>正在从“不可量化”走向“可分析、可追溯、可评估”。</p><p>在真实教学场景中，教师和管理者往往关注以下问题：</p><ul><li>学生是否专注听讲？</li><li>是否存在频繁低头、趴桌、玩手机等行为？</li><li>课堂互动（举手、回答问题）是否足够积极？</li><li>不同时间段、不同课程的学习状态差异如何？</li></ul><p>传统方式主要依赖<strong>人工巡视或事后主观评价</strong>，存在明显局限：</p><table><thead><tr><th>方式</th><th>问题</th></tr></thead><tbody><tr><td>人工观察</td><td>成本高、主观性强、难以量化</td></tr><tr><td>问卷反馈</td><td>滞后、失真、难以持续</td></tr><tr><td>简单视频回放</td><td>无结构化信息、分析效率低</td></tr></tbody></table><p>因此，<strong>基于计算机视觉的课堂行为识别系统</strong>成为一个极具实际价值的工程方向。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524872" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>源码下载与效果演示</h3><p>哔哩哔哩视频下方观看：<br/><a href="https://www.bilibili.com/video/BV1m7KJzNEQ2/" target="_blank">https://www.bilibili.com/video/BV1m7KJzNEQ2/</a></p><p>包含：</p><p>📦完整项目源码</p><p>📦 预训练模型权重</p><p>🗂️ 数据集地址（含标注脚本<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524873" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>二、整体技术路线设计</h3><p>本项目的目标不是“只跑一个模型 Demo”，而是构建一个<strong>可直接使用的完整系统</strong>。因此在设计之初，整体架构就围绕以下三点展开：</p><ol><li><strong>模型必须实时可用</strong></li><li><strong>系统必须非算法人员也能操作</strong></li><li><strong>工程结构支持后续扩展</strong><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524874" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></li></ol><h4>2.1 系统总体架构</h4><p>整体采用典型的三层结构：</p><pre><code>数据层 → 模型层 → 应用层</code></pre><ul><li><strong>数据层</strong>：YOLO 格式行为数据集（图片 + 标签）</li><li><strong>模型层</strong>：YOLOv8 Detection 模型（PyTorch）</li><li><strong>应用层</strong>：PyQt5 桌面 GUI + 多输入推理模块</li></ul><pre><code>摄像头 / 视频 / 图片
        ↓
   YOLOv8 行为检测
        ↓
   行为类别 + 置信度
        ↓
   GUI 实时展示 / 保存结果</code></pre><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524875" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524876" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>三、为什么选择 YOLOv8 做课堂行为识别</h3><p>在行为识别领域，常见技术路线包括：</p><ul><li><strong>CNN + 分类</strong>（仅判断整张图）</li><li><strong>CNN + 时序模型（LSTM / Transformer）</strong></li><li><strong>目标检测 + 行为标签</strong></li></ul><p>在课堂场景中，我们更关心的是：</p><blockquote><strong>“谁”在“做什么行为”</strong></blockquote><p>因此，<strong>目标检测模型</strong>比单纯分类模型更合适。</p><h4>3.1 YOLOv8 的工程优势</h4><p>YOLOv8 相比早期 YOLO 版本，具备明显工程优势：</p><ul><li><strong>Anchor-Free 设计</strong><br/>不再依赖复杂 Anchor 调参，对新场景友好</li><li><strong>端到端训练流程简化</strong></li><li><strong>推理速度快，适合实时摄像头</strong></li><li><strong>Ultralytics 官方生态成熟</strong></li><li><strong>原生支持 ONNX / TensorRT 导出</strong></li></ul><p>对于“课堂实时监测”这种 <strong>FPS 和稳定性同等重要</strong> 的任务，YOLOv8 是非常理性的选择。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524877" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>四、行为数据集构建：比模型更重要的一步</h3><p>在实际项目中，<strong>数据质量往往决定上限</strong>。</p><h4>4.1 行为类别设计原则</h4><p>本项目中的课堂行为类别遵循三个原则：</p><ol><li><strong>视觉上可区分</strong></li><li><strong>教学场景有明确意义</strong></li><li><strong>避免过细导致标注困难</strong></li></ol><p>示例类别包括：</p><ul><li>举手</li><li>看书</li><li>写作业</li><li>听讲</li><li>趴桌</li><li>玩手机</li></ul><p>这些行为都可以通过<strong>单帧图像 + 空间特征</strong>进行判别，而无需复杂时序建模。</p><hr/><h4>4.2 数据集结构（YOLO 标准）</h4><pre><code class="text">dataset/
├── images/
│   ├── train/
│   └── val/
├── labels/
│   ├── train/
│   └── val/</code></pre><p>标签采用 YOLO 标准格式：</p><pre><code>class_id x_center y_center width height</code></pre><p>例如：</p><pre><code class="txt">3 0.51 0.36 0.39 0.32</code></pre><blockquote>所有坐标均为 <strong>相对比例</strong>，方便多分辨率训练。</blockquote><hr/><h3>五、模型训练与参数配置经验</h3><h4>5.1 训练策略</h4><p>在课堂场景中，模型训练重点并不是追求极限精度，而是：</p><ul><li><strong>稳定收敛</strong></li><li><strong>类别区分度清晰</strong></li><li><strong>推理速度可控</strong></li></ul><p>示例训练命令：</p><pre><code class="bash">yolo detect train \
  data=dataset/classroom.yaml \
  model=yolov8n.pt \
  epochs=100 \
  batch=16 \
  imgsz=640</code></pre><h4>5.2 关键指标解读</h4><p>训练完成后，重点关注：</p><ul><li><strong>mAP@0.5</strong>：是否稳定在 90% 左右</li><li><strong>混淆矩阵</strong>：是否存在行为间严重混淆</li><li><strong>box_loss / cls_loss 收敛情况</strong></li></ul><p>课堂行为中，“看书 / 写作业”是最容易混淆的类别，通常需要通过 <strong>数据平衡和样本增强</strong> 来改善。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524878" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524879" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>六、推理系统设计：从模型到可用软件</h3><p>如果说模型是“发动机”，那么 <strong>GUI 系统</strong>就是“驾驶舱”。</p><h4>6.1 多输入推理设计</h4><p>系统支持以下输入形式：</p><ul><li>单张图片检测</li><li>文件夹批量检测</li><li>视频文件检测</li><li>摄像头实时检测</li></ul><p>其核心思想是：</p><blockquote><strong>统一推理接口，不同输入仅影响数据读取方式</strong></blockquote><pre><code class="python">results = model(frame, conf=0.25)</code></pre><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524880" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h4>6.2 PyQt5 界面设计要点</h4><p>GUI 设计遵循三个工程原则：</p><ol><li><strong>功能按钮逻辑清晰</strong></li><li><strong>推理与界面解耦</strong></li><li><strong>避免阻塞主线程</strong></li></ol><p>常见功能包括：</p><ul><li>模型加载</li><li>输入源选择</li><li>实时结果显示</li><li>结果保存开关</li></ul><p>这种设计使得<strong>非算法人员也能直接运行系统</strong>。</p><hr/><h3>七、实际应用价值分析</h3><p>在真实教学场景中，该系统可用于：</p><ul><li><strong>课堂状态统计分析</strong></li><li><strong>教学质量评估辅助</strong></li><li><strong>学生行为数据可视化</strong></li><li><strong>智慧教室系统模块化集成</strong></li></ul><p>相比“单纯监控”，该系统更强调：</p><blockquote><strong>结构化行为数据的自动生成</strong></blockquote><hr/><h3>八、可扩展方向与进阶思路</h3><p>该项目并非终点，而是一个可持续扩展的工程起点。</p><p>可扩展方向包括：</p><ul><li><strong>引入姿态估计（Keypoints）</strong></li><li><strong>多摄像头联动分析</strong></li><li><strong>行为时间序列建模</strong></li><li><strong>行为频次 / 趋势统计</strong></li><li><strong>与教学管理系统对接</strong></li></ul><p>未来可从“检测行为”升级为：</p><blockquote><strong>理解课堂状态</strong></blockquote><hr/><h3>九、总结</h3><p>本文从工程视角出发，完整复盘了一个 <strong>基于 YOLOv8 的学生课堂行为识别系统</strong> 从需求分析、模型选择、数据构建、训练评估到 GUI 应用落地的全过程。</p><p>该项目的核心价值不在于“某一个模型指标”，而在于：</p><ul><li>模型可实时运行</li><li>系统可直接部署</li><li>工程结构可持续演进</li></ul><p>对于希望将 <strong>计算机视觉真正落地到教育场景</strong> 的开发者而言，这是一个非常具有实践意义的方向。</p><p>本文从工程实践角度系统性地梳理了一个基于 YOLOv8 的学生课堂行为识别系统的完整落地过程，涵盖需求背景、模型选型、数据集构建、训练评估以及 PyQt5 图形化应用封装等关键环节。实践表明，目标检测模型在课堂行为分析这一复杂场景中具备良好的实时性与可扩展性，能够有效将“不可量化的课堂状态”转化为结构化、可分析的数据资产。相比单一算法实验，本项目更强调模型与系统的协同设计，使 AI 能够真正服务于教学管理与教学分析。未来，随着姿态估计、多模态数据与行为统计分析的引入，该类系统有望从行为识别进一步升级为课堂状态理解与教学决策支持，为智慧校园建设提供更加可靠的技术基础。</p>]]></description></item><item>    <title><![CDATA[开发者如何集成IP查询功能？主流IP离线库全面解读与性能对比 科技块儿 ]]></title>    <link>https://segmentfault.com/a/1190000047524895</link>    <guid>https://segmentfault.com/a/1190000047524895</guid>    <pubDate>2026-01-06 18:06:53</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>IP查询是开发者在反欺诈、精准营销、网络防护等场景的核心需求，离线库因无网络依赖、响应更快，成为企业级应用的首选。本文筛选5款主流IP离线库（IP数据云、IPnews、IPinfo、IP2Location、DB-IP），从技术指标与实用场景切入测评，为开发者集成决策提供参考。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524897" alt="开发者如何集成IP查询功能主流IP离线库全面解读与性能对比.png" title="开发者如何集成IP查询功能主流IP离线库全面解读与性能对比.png"/></p><h2>一、主流IP离线库核心参数对比</h2><table><thead><tr><th>对比维度</th><th>IP数据云</th><th>IPnews</th><th>IPinfo</th><th>IP2Location</th><th>DB-IP</th></tr></thead><tbody><tr><td>全球IP覆盖</td><td>全球全量覆盖</td><td>全球覆盖</td><td>全球覆盖</td><td>全球覆盖</td><td>全球覆盖</td></tr><tr><td>定位精度</td><td>街道级</td><td>城市级</td><td>城市级</td><td>城市级</td><td>城市级</td></tr><tr><td>支持协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td><td>IPv4/IPv6双协议</td></tr><tr><td>字段维度</td><td>20+（含风险画像、场景）</td><td>20+（含代理及风险检测）</td><td>20+（含ISP信息）</td><td>20+（含运营商信息）</td><td>20+（基础地理+ISP）</td></tr><tr><td>风险识别能力</td><td>支持（薅羊毛/垃圾注册等）</td><td>支持</td><td>支持风险标记</td><td>支持</td><td>支持</td></tr><tr><td>适用场景</td><td>金融反欺诈、政企安全</td><td>普通网站定位</td><td>中小企业营销</td><td>网络防护、欺诈防范和广告等</td><td>通用信息查询</td></tr></tbody></table><p>这五款IP离线库均实现全球IP覆盖与IPv4/IPv6双协议支持，字段维度均达20 +且具备风险识别相关能力；核心差异集中在定位精度与适用场景，其中IP数据云以街道级定位精度凸显优势，适配金融反欺诈、政企安全等高精度需求场景，其余四款均为城市级定位，分别对应普通网站定位、中小企业营销、网络防护及通用信息查询等不同业务场景。</p><h2>二、开发者集成关键步骤</h2><p>1.选型适配：根据业务场景选择库；<br/>2.离线部署：下载对应库的离线数据包，支持本地服务器部署，规避网络波动；<br/>3.接口调用：各服务商大多通过SDK或API接口接入，无需复杂开发（其中IP数据云提供Java/Python/Go等多语言SDK）；<br/>4.数据更新：定期同步官方离线包。</p><h2>三、总结</h2><p>开发者集成IP查询功能，需平衡精度、性能与功能适配性。IP数据云等IP离线库服务商在定位精度、响应速度、风险识别等核心维度表现突出，尤其适合金融、政企等对安全性和精度要求严苛的场景。选择离线库时，建议优先考量业务核心需求，结合表格对比结果选型，提升集成效率与应用效果。</p>]]></description></item><item>    <title><![CDATA[解锁 PDF 内容：如何用 Python 从 PDF 中快速提取文本 宇文成都 ]]></title>    <link>https://segmentfault.com/a/1190000047524909</link>    <guid>https://segmentfault.com/a/1190000047524909</guid>    <pubDate>2026-01-06 18:06:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代办公环境中，PDF 文件作为一种通用的文档格式被广泛使用。无论是合同、报告还是电子书，很多重要信息都储存于 PDF 文件中。因此，从 PDF 文件中提取文本数据的需求也逐渐增加。本文将为大家介绍如何使用 Spire.PDF for Python 来实现这一功能，具体包括从某一页和从指定区域提取文本。</p><h2>1. 环境准备</h2><p>首先，确保你已经安装了 Python 和 Spire.PDF 的相关库。你可以通过以下命令安装 Spire.PDF：</p><pre><code class="bash">pip install Spire.PDF</code></pre><h2>2. 从指定页面提取文本</h2><h3>2.1 代码示例</h3><p>以下代码展示了如何从 PDF 文档中的特定页（例如第2页）提取文本：</p><pre><code class="python">from spire.pdf.common import *
from spire.pdf import *

# 创建一个 PdfDocument 对象
doc = PdfDocument()

# 加载 PDF 文档
doc.LoadFromFile('C:/Users/Administrator/Desktop/Terms of service.pdf')

# 创建 PdfTextExtractOptions 对象并启用全文本提取
extractOptions = PdfTextExtractOptions()
# 提取所有文本，包括空格
extractOptions.IsExtractAllText = True

# 获取特定的页面（例如，第2页）
page = doc.Pages.get_Item(1)

# 创建 PdfTextExtractor 对象
textExtractor = PdfTextExtractor(page)

# 从页面中提取文本
text = textExtractor.ExtractText(extractOptions)

# 使用 UTF-8 编码将提取的文本写入文件
withopen('output/TextOfPage.txt', 'w', encoding='utf-8') as file:
    file.write(text)</code></pre><h3>2.2 代码解析</h3><ol><li><strong>创建 <code>PdfDocument</code> 对象</strong> ：这一步是加载 PDF 文件的第一步。</li><li><strong>加载 PDF 文档</strong> ：使用指定路径加载你要处理的 PDF 文件。</li><li><strong>配置提取选项</strong> ：通过设置 <code>IsExtractAllText</code> 为 True，确保提取所有文本，包括空格。</li><li><strong>获取特定页面</strong> ：<code>doc.Pages.get_Item(1)</code> 获取的是 PDF 的第二页（索引从0开始）。</li><li><strong>创建文本提取器并提取文本</strong> ：使用 <code>PdfTextExtractor</code> 对象来提取文本。</li><li><strong>将提取的文本保存为文件</strong> ：最终将文本内容保存到指定路径的文件中。</li></ol><h2>3. 从指定区域提取文本</h2><p>有时候，仅提取 PDF 中的某一特定区域的文本更加有效。这可以通过定义一个矩形区域来实现。</p><h3>3.1 代码示例</h3><p>以下代码将展示如何从 PDF 的指定区域提取文本：</p><pre><code class="python">from spire.pdf.common import *
from spire.pdf import *

# 创建一个 PdfDocument 对象
doc = PdfDocument()

# 加载 PDF 文档
doc.LoadFromFile('C:/Users/Administrator/Desktop/Terms of service.pdf')

# 获取特定的页面（例如，第2页）
page = doc.Pages.get_Item(1)

# 创建 PdfTextExtractor 对象
textExtractor = PdfTextExtractor(page)

# 创建 PdfTextExtractOptions 对象
extractOptions = PdfTextExtractOptions()

# 定义提取的矩形区域
# RectangleF(left, top, width, height)
extractOptions.ExtractArea = RectangleF(0.0, 100.0, 890.0, 80.0)

# 从指定区域提取文本，保留空格
text = textExtractor.ExtractText(extractOptions)

# 使用 UTF-8 编码将提取的文本写入文件
withopen('output/TextOfRectangle.txt', 'w', encoding='utf-8') as file:
    file.write(text)</code></pre><h3>3.2 代码解析</h3><ol><li><strong>加载 PDF 文件</strong> ：与之前相同，首先加载 PDF 文档。</li><li><strong>获取特定页面</strong> ：依然使用 <code>doc.Pages.get_Item(1)</code> 来获取第2页。</li><li><strong>定义提取区域</strong> ：通过 <code>RectangleF</code> 类来定义一个矩形区域，该区域的左上角坐标为 <code>(0, 100)</code>，宽度为 <code>890</code>，高度为 <code>80</code>。</li><li><strong>执行文本提取</strong> ：然后使用 <code>ExtractText</code> 方法从指定区域提取文本。</li><li><strong>保存文本</strong> ：最后，提取的文本同样保存为 UTF-8 编码的文件。</li></ol><h2>结论</h2><p>通过以上方法，我们可以方便地从 PDF 文档中提取所需的文本信息。Spire.PDF for Python 提供的 API 简洁高效，能够满足多种文本提取需求。不论是从全页提取还是从特定区域提取，在实际工作中都能显著提高效率，尤其对于需要处理大量 PDF 文件的场合，使用此工具将使你事半功倍。</p><p>希望这篇博客能够帮助你更好地理解如何使用 Python 提取 PDF 文本，让你的工作更轻松高效！</p>]]></description></item><item>    <title><![CDATA[上传图片时交互来回闪现的情况优化 freeman_Tian ]]></title>    <link>https://segmentfault.com/a/1190000047524907</link>    <guid>https://segmentfault.com/a/1190000047524907</guid>    <pubDate>2026-01-06 18:05:40</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <pre><code>&lt;template&gt;
  &lt;div class="car-upload"&gt;
    &lt;el-upload
      ref="upload"
      action="#"
      :file-list="internalFileList"
      :before-upload="beforeUpload"
      :http-request="handleUpload"
      :on-preview="handlePreview"
      :multiple="false"
      list-type="picture-card"
      :on-remove="handleRemove"
      :class="{ 'hide-upload': hideUpload }"
      :disabled="uploading"
    &gt;
      &lt;template v-if="uploading"&gt;
        &lt;div class="uploading-mask"&gt;
          &lt;div class="custom-loading-icon" /&gt;
          &lt;span class="upload-text"&gt;{{ $t('common.uploading') }}&lt;/span&gt;
        &lt;/div&gt;
      &lt;/template&gt;
      &lt;template v-else&gt;
        &lt;i class="el-icon-plus" /&gt;
      &lt;/template&gt;

      &lt;div slot="tip" class="tip"&gt;
        {{ $t('externalModel.uploadimgTip') }}
      &lt;/div&gt;
    &lt;/el-upload&gt;

    &lt;PreviewModal :visible.sync="previewVisible" :image-url="previewImageUrl" /&gt;
  &lt;/div&gt;
&lt;/template&gt;

&lt;script&gt;
import { getToken } from '@/utils/auth'
import PreviewModal from './PreviewModal.vue'
import { imagesUpload } from '@/api/type/external'

export default {
  name: 'CarImageUpload',
  components: {
    PreviewModal
  },
  props: {
    value: {
      type: [String, Array],
      default: () =&gt; []
    },
    maxSize: { type: Number, default: 2 },
    accept: {
      type: Array,
      default: () =&gt; ['image/bmp', 'image/png', 'image/jpeg', 'image/gif']
    },
    uploadUrl: { type: String, required: false, default: '' },
    fixedSize: { type: Object, required: false, default: null },
    limit: {
      type: Number,
      default: 1
    }
  },
  data() {
    return {
      internalFileList: [],
      previewUrl: '',
      headers: { Authorization: 'Bearer ' + getToken() },
      previewVisible: false,
      previewImageUrl: '',
      uploading: false,
      currentFile: null,
      // 添加缓存用于存储原始文件的UID
      fileUidMap: new Map()
    }
  },
  computed: {
    hideUpload() {
      return this.internalFileList.length &gt;= this.limit || this.uploading
    }
  },
  watch: {
    value: {
      immediate: true,
      handler(newVal) {
        // 优化：避免不必要的重渲染，只有在真正变化时更新
        const currentUrls = this.internalFileList.map(item =&gt; item.url).filter(Boolean)
        const newUrls = Array.isArray(newVal) ? newVal : (newVal ? [newVal] : [])

        if (JSON.stringify(currentUrls) !== JSON.stringify(newUrls)) {
          this.syncFileList(newVal)
        }
      }
    }
  },
  methods: {
    // 优化文件列表同步方法
    syncFileList(newVal) {
      if (Array.isArray(newVal)) {
        this.internalFileList = newVal.map((url, index) =&gt; ({
          url,
          status: 'success',
          // 保持UID一致性，避免重新生成
          uid: this.fileUidMap.get(url) || this.generateUid(url, index)
        }))
      } else if (newVal) {
        this.internalFileList = [{
          url: newVal,
          status: 'success',
          uid: this.fileUidMap.get(newVal) || this.generateUid(newVal, 0)
        }]
      } else {
        this.internalFileList = []
      }
    },

    // 生成基于URL的稳定UID
    generateUid(url, index) {
      // 基于URL生成稳定UID，避免每次重新生成不同的UID
      const uid = `file_${btoa(url).substr(0, 10)}_${index}`
      this.fileUidMap.set(url, uid)
      return uid
    },

    async beforeUpload(file) {
      if (this.uploading) {
        this.$message.warning('正在上传中，请稍候...')
        return false
      }

      const isImage = this.accept.includes(file.type)
      const isLt10M = file.size / 1024 / 1024 &lt; 10

      if (!isImage) {
        this.$message.error(this.$t('externalModel.uploadimgTip'))
        return false
      }
      if (!isLt10M) {
        this.$message.error(this.$t('externalModel.uploadimgTip'))
        return false
      }

      if (this.fixedSize || this.aspectRatio) {
        const isDimensionValid = await this.validateImageSize(file)
        if (!isDimensionValid) {
          const msg = this.fixedSize
            ? `图片尺寸必须为 ${this.fixedSize.width}×${this.fixedSize.height}`
            : `宽高比需为 ${this.aspectRatio}:1`
          this.$message.error(msg)
          return false
        }
      }

      this.currentFile = file
      return true
    },

    // 优化上传方法，避免闪动[1,2](@ref)
    async handleUpload({ file }) {
      this.uploading = true

      // 保存原始文件的UID[2](@ref)
      const originalUid = file.uid

      try {
        const formData = new FormData()
        formData.append('file', this.currentFile || file)

        const res = await imagesUpload(formData)

        if (res.code === '000000') {
          const url = res.body.url

          // 优化：直接更新现有文件对象，而不是创建新对象[5](@ref)
          const existingFileIndex = this.internalFileList.findIndex(f =&gt; f.uid === originalUid)
          if (existingFileIndex &gt; -1) {
            // 保持UID不变，只更新URL[2](@ref)
            this.internalFileList[existingFileIndex].url = url
            this.internalFileList[existingFileIndex].status = 'success'
          } else {
            // 如果找不到现有文件，添加新文件但保持UID
            this.internalFileList.push({
              url,
              status: 'success',
              uid: originalUid
            })
          }

          this.updateModelValue(url)
          this.$message.success('上传成功')
        } else {
          throw new Error(res.message || '上传失败')
        }
      } catch (error) {
        console.error('上传失败:', error)
        this.$message.error('上传失败，请重试')

        // 上传失败时更新状态而不是移除文件[1](@ref)
        const failedFileIndex = this.internalFileList.findIndex(f =&gt; f.uid === originalUid)
        if (failedFileIndex &gt; -1) {
          this.internalFileList[failedFileIndex].status = 'failed'
        }
      } finally {
        this.uploading = false
        this.currentFile = null
      }
    },

    validateImageSize(file) {
      return new Promise((resolve) =&gt; {
        const img = new Image()
        img.src = URL.createObjectURL(file)
        img.onload = () =&gt; {
          let isValid = true
          if (this.fixedSize) {
            isValid = img.width === this.fixedSize.width &amp;&amp; img.height === this.fixedSize.height
          } else if (this.aspectRatio) {
            const ratio = (img.width / img.height).toFixed(2)
            isValid = ratio === this.aspectRatio.toFixed(2)
          }
          URL.revokeObjectURL(img.src)
          resolve(isValid)
        }
        img.onerror = () =&gt; resolve(false)
      })
    },

    // 优化模型值更新[5](@ref)
    updateModelValue(newUrl) {
      // 避免直接赋值导致的重新渲染
      this.$nextTick(() =&gt; {
        const currentValue = this.value
        let newValue

        if (this.limit &gt; 1) {
          const currentUrls = Array.isArray(currentValue) ? currentValue : []
          // 去重并过滤空值
          newValue = [...new Set([...currentUrls, newUrl].filter(url =&gt; url))]
        } else {
          newValue = newUrl
        }

        // 只有值真正改变时才触发更新
        if (JSON.stringify(currentValue) !== JSON.stringify(newValue)) {
          this.$emit('input', newValue)
        }
      })
    },

    handleRemove(file) {
      // 从UID映射中移除
      this.fileUidMap.delete(file.url)

      const newList = this.internalFileList.filter(f =&gt; f.uid !== file.uid)
      const newValue = this.limit &gt; 1 ? newList.map(f =&gt; f.url) : ''
      this.$emit('input', newValue)
    },

    handlePreview(file) {
      // 添加时间戳防止缓存问题[4](@ref)
      const timestamp = new Date().getTime()
      this.previewImageUrl = file.url + (file.url.includes('?') ? '&amp;' : '?') + `t=${timestamp}`
      this.previewVisible = true
    }
  }
}
&lt;/script&gt;

&lt;style lang="scss" scoped&gt;
.car-upload {
  height: 200px;
  position: relative;
}

.hide-upload {
  ::v-deep .el-upload--picture-card {
    display: none;
    /* 添加过渡效果减少视觉突兀感 */
    transition: opacity 0.3s ease;
  }
}

.tip {
  font-size: 12px;
  color: #909399;
  padding-top: 8px;
}

.uploading-mask {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  height: 100%;
  color: #409EFF;
}

.upload-text {
  margin-top: 8px;
  font-size: 12px;
}

.custom-loading-icon {
  width: 20px;
  height: 20px;
  border: 2px solid #f3f3f3;
  border-top: 2px solid #409EFF;
  border-radius: 50%;
  animation: rotating 2s linear infinite;
}

@keyframes rotating {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

/* 添加文件列表项过渡效果 */
::v-deep .el-upload-list__item {
  transition: all 0.3s ease;
}

/* 确保图片加载平滑 */
::v-deep .el-upload-list__item-thumbnail {
  object-fit: contain;
  transition: opacity 0.3s ease;
}
&lt;/style&gt;
</code></pre><p>上传组件的闪动问题是由于双阶段渲染导致的，选择文件后先用本地Blob URL预览，上传完成后再用服务器URL替换，这个替换过程会导致重新加载和闪动。文中建议保持使用本地预览不替换URL，或者优化上传流程。都指出Element UI中el-upload组件的闪动问题与uid变化有关，上传成功后如果file-list被重新赋值且uid发生变化，就会导致组件重新渲染和图片闪烁。解决方案是在回调中保持uid不变。提到避免使用computed或watch监听fileList，这可能导致不必要的重新渲染。<br/>UID不一致​</p><p>文件上传后新对象的UID与原始文件不同，导致组件重新渲染</p><p>保持UID一致性，避免重新创建文件对象</p><p>URL替换闪动​</p><p>从本地Blob URL切换到服务器HTTP URL时浏览器重新加载</p><p>优化URL替换时机或使用统一URL格式</p><p>文件列表重建​</p><p>使用watch监听value导致整个internalFileList重建</p><p>优化数据更新策略，避免不必要的重渲染</p>]]></description></item><item>    <title><![CDATA[节点小宝4.0性能测试：远程文件管理体验全面升级 节点小宝 ]]></title>    <link>https://segmentfault.com/a/1190000047524911</link>    <guid>https://segmentfault.com/a/1190000047524911</guid>    <pubDate>2026-01-06 18:04:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在远程工具的使用体验中，性能表现一直是用户关注的重点。近期我们对节点小宝4.0版本进行了系统性的性能测试，重点评估了其在远程文件管理方面的表现。</p><h3>测试环境与方法</h3><p>测试采用统一的硬件配置：红米K60设备通过4G热点连接，访问群晖DS220+ NAS设备。通过对比3.0与4.0版本的核心指标，我们获得了以下数据：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524913" alt="图片" title="图片"/></p><p>测试结果显示，4.0版本在多个关键指标上均有显著提升：<br/>冷启动到显示NAS根目录的时间从11.7秒缩短至2.3秒<br/>大文件缩略图加载时间从6.4秒减少到0.9秒<br/>批量文件列表加载效率提升约5.8倍<br/>4G网络下文件传输速度提升4倍</p><h4>技术优化亮点</h4><p>预加载算法改进<br/>新版本采用智能预读技术，自动加载可视区域前后30%的数据内容。缩略图经过优化压缩，在保证观感的同时节省了70%的流量消耗。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524914" alt="图片" title="图片" loading="lazy"/></p><h4>连接稳定性提升</h4><p>P2P直连技术在复杂网络环境下的成功率提升至92%，当主连接失败时，系统能在0.8秒内切换到备用通道，确保服务连续性。</p><h4>资源占用优化</h4><p>即使在处理万级文件批量操作时，CPU占用率也能控制在30%以内，保证了系统的流畅运行。</p><h4>实际使用体验文件浏览流畅度</h4><p>测试中在NAS存入2万张图片，高速滑动浏览时缩略图加载延迟低于0.1秒，在高刷新率设备上也能保持流畅的视觉效果。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524915" alt="图片" title="图片" loading="lazy"/></p><h4>断点续传功能</h4><p>在模拟弱网环境下测试文件传输，网络中断后重新连接时，系统能在3秒内自动恢复传输，并能从断点处准确继续，避免了重复传输。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524916" alt="图片" title="图片" loading="lazy"/></p><h3>功能改进对比</h3><h4>操作路径简化</h4><p>相比3.0版本需要多步操作才能访问远程文件，4.0版本实现了"一键直达"的体验。用户只需在首页点击"远程文件"即可直接访问NAS内容。</p><h4>协议支持扩展</h4><p>新版本增强了对SMB协议的支持，使得大文件编辑操作更加流畅。测试中打开45MB的RAW格式图片文件，曝光调整的响应延迟仅为120ms。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524917" alt="图片" title="图片" loading="lazy"/></p><h4>附加功能特性</h4><p>4.0版本还包含了一些实用的附加功能：<br/>WebDAV挂载支持，允许用户在系统文件管理器中直接访问远程存储<br/>快捷传输功能，通过快捷方式快速访问常用设备<br/>多标签页管理，支持同时控制多个远程设备</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524918" alt="图片" title="图片" loading="lazy"/></p><p>节点小宝4.0版本在远程文件管理方面实现了显著的技术突破。通过优化算法、提升连接稳定性和简化操作流程，为用户提供了接近本地操作的使用体验。这些改进使得远程文件管理变得更加高效和便捷，为需要频繁访问远程存储的用户提供了实用的解决方案。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524919" alt="图片" title="图片" loading="lazy"/></p><p>该版本现已正式发布，欢迎技术爱好者体验并提供反馈意见。对于远程访问技术有深入探讨需求的用户，可以在技术社区继续交流相关实现细节。</p>]]></description></item><item>    <title><![CDATA[企业落地 ChatBI，如何构建可信可靠的数据底座？ Aloudata大应科技 ]]></title>    <link>https://segmentfault.com/a/1190000047524928</link>    <guid>https://segmentfault.com/a/1190000047524928</guid>    <pubDate>2026-01-06 18:04:08</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在企业 ChatBI 落地过程中，数据底座的技术路线选择直接决定了数据可信度、维护成本和业务响应速度。传统宽表架构在数据口径一致性、维护成本和灵活性上已难以支撑企业级 ChatBI 的规模化应用，而基于 NoETL 明细语义层的方案正成为新一代数据底座的主流选择。</p><h2>企业落地 ChatBI 痛点：为什么传统宽表越来越难用？</h2><p><strong>痛点一：数据口径碎片化，业务不敢信</strong><br/>● 不同宽表、不同报表对同一指标定义不一致：同一“销售额”指标在营销宽表、财务宽表中可能包含不同的业务口径（如是否含税、是否含退货），导致业务人员无法判断哪个数据可信。<br/>● 业务与 IT 对指标理解偏差，导致“问 A 得 B”：业务人员理解的“活跃用户”与宽表字段逻辑存在差异，取数结果与预期不符，反复沟通成本高。<br/>● 数据口径不透明，结果难解释，决策依赖“拍脑袋”：宽表背后复杂的 ETL 逻辑缺乏文档沉淀，业务人员无法追溯计算过程，只能凭经验决策，数据驱动决策沦为口号。</p><p><strong>痛点二：维护成本高，IT 排期长</strong><br/>● 宽表数量随业务需求线性增长，开发与运维成本失控：每新增一个分析维度或业务场景，就需要新建一张宽表，导致数仓中宽表数量激增，数据冗余严重，存储和计算成本持续攀升。<br/>● 业务需求变更需重建宽表，响应周期长：当业务口径调整（如“高净值客户”定义变化）时，需要重新设计宽表、开发 ETL 任务并重新上线，响应周期通常以周为单位。<br/>● 数据工程师疲于应付宽表开发，难以沉淀数据资产：工程师长期陷入“接需求—建宽表—改宽表”的循环，无法将精力投入到数据资产治理和业务价值挖掘中。</p><p><strong>痛点三：分析灵活性差，难以下钻明细</strong><br/>● 宽表预聚合导致数据粒度固化，无法满足灵活分析需求：宽表通常按固定维度（如“日期+区域+品类”）预聚合，当业务需要按“渠道+门店”分析时，只能新建宽表或放弃分析。<br/>● 跨表分析需新建宽表，无法动态组合维度和指标：不同宽表之间缺乏统一的语义关联，跨表分析需要重新建模，无法实现“任意维度+任意指标”的动态组合查询。<br/>● 明细数据被汇总后丢失，归因分析只能靠人工猜测：宽表只保留汇总结果，原始明细数据被丢弃，当出现数据异常时，无法下钻到明细交易进行根因分析，只能依赖人工经验猜测。</p><h2>NoETL 明细语义层——ChatBI 数据底座的核心</h2><p>● 基于明细层数据模型进行语义抽象，覆盖完整分析场景：明细语义层直接对接企业数仓 DWD 层的明细模型，沉淀所有明细级语义，支持从宏观汇总到明细下钻的全场景问数需求。<br/>● 指标和维度一次定义，多处使用，确保口径一致：通过可视化配置指标逻辑（组合度量/维度/限定），自动生成无歧义 SQL，指标逻辑全局唯一，下游应用直接调用，避免重复开发与口径分歧。<br/>● 支持原子指标、派生指标、衍生指标的统一管理：原子指标（如“销售额”“客单价”）和维度（如“时间”“地区”）在语义层标准化定义，派生指标和衍生指标基于原子指标动态生成，无需预先固化所有分析路径。<br/><img width="723" height="411" referrerpolicy="no-referrer" src="/img/bVdmNuE" alt="" title=""/></p><h2>Aloudata Agent：基于 NoETL 明细语义层的分析决策智能体</h2><p>借助于 NoETL 明细语义层和 NL2MQL2SQL 的技术路径，Aloudata Agent 可以将自然语言查询转换为对指标语义层的精准查询请求，再由语义引擎生成准确、可执行的 SQL，有效避免了语义歧义与数据幻觉现象。该功能还支持复杂的智能归因分析，如维度归因和因子归因，并能自动生成结构化报告。</p><p>面对复杂的分析任务，Aloudata Agent 提供的多 Agent 协同架构能够自动进行拆解与协同处理。以“Q2 利润下滑”分析为例，系统可自动将其分解为收入分析、成本分析、异常交易检测等子任务，并分别调用相应的指标查询、归因分析和报告生成等子智能体，最终交付一个包含数据查询结果、关键异常发现及具体行动建议的完整结构化报告。</p><p>此外，Aloudata Agent 提供场景化的分析助手功能，以沉淀和复用业务知识。支持根据不同业务职能创建个性化助手，如门店运营助手或财务分析助手。每个助手可配置独立的资源管理与访问权限，有效避免跨业务间的数据干扰。同时，用户可在使用中维护个人术语知识和分析思路，促进业务知识的持续积累与沉淀。<br/><img width="723" height="389" referrerpolicy="no-referrer" src="/img/bVdmOdO" alt="" title="" loading="lazy"/></p><p>最后，为确保数据安全与合规，NoETL 明细语义层可支持行级和列级的数据权限，确保用户仅能访问其权限范围内的数据，如客户经理仅能看到所负责客户的销售数据。同时，通过多租户隔离机制，满足不同业务部门或子公司的独立使用需求，并严格管控数据访问，以符合金融、医疗等行业对数据安全与合规的严格要求。</p><h2>FAQ: 常见疑问解答</h2><p><strong>Q1: 语义层方案是否会影响查询性能？​</strong><br/>不会。基于 NoETL 明细语义层的方案通过智能物化加速和查询改写优化，能够保障亿级数据秒级响应。语义层负责逻辑抽象，底层通过数据虚拟化引擎和物化策略实现性能优化，相比宽表方案在灵活性和性能上取得更好平衡。</p><p><strong>Q2: 语义层如何解决数据口径一致性问题？​</strong><br/>语义层通过统一指标定义和指标血缘管理，确保所有分析场景消费相同的指标口径。业务规则迭代只需在语义层一次修改，全链路查询自动同步更新，避免了宽表方案中口径碎片化的问题。</p><p><strong>Q3: 语义层方案是否支持跨表分析？​</strong><br/>支持。基于明细语义层的方案突破分析维度和数据粒度固化，支持任意维度和指标的灵活组合，实现跨表动态查询。相比宽表预聚合方案，语义层在分析灵活性上具有明显优势。</p>]]></description></item><item>    <title><![CDATA[英伟达旗下芯片 咕噜云服务器晚晚 ]]></title>    <link>https://segmentfault.com/a/1190000047524932</link>    <guid>https://segmentfault.com/a/1190000047524932</guid>    <pubDate>2026-01-06 18:03:16</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>英伟达作为全球领先的半导体设计公司，其芯片产品以卓越的性能和广泛的应用场景引领着计算技术的变革。以下从产品体系、技术优势、应用领域及市场影响四个维度，对英伟达旗下芯片进行全面解析。<br/>一、产品体系：覆盖多场景的芯片矩阵<br/>英伟达芯片产品布局呈现“通用计算+专用加速”双轮驱动特征，核心产品线包括：</p><ol><li><p><strong>GPU（图形处理器）</strong></p><ul><li><strong>GeForce系列</strong>：面向消费级市场，如RTX 4090/4080采用Ada Lovelace架构，集成DLSS 3技术，支持光线追踪和AI渲染，为游戏、内容创作提供超高清画质与实时交互体验。</li><li><strong>Quadro（现更名为RTX A系列）</strong>：专业图形卡，针对CAD设计、影视特效等领域，如RTX A6000搭载48GB GDDR6显存，支持多屏输出和GPU加速计算，满足工业级渲染需求。</li><li><strong>Tesla系列（已逐步整合至数据中心产品线）</strong>：早期数据中心GPU，为云计算、科学计算提供算力支持，后续迭代为A100/H100等型号。</li></ul></li><li><p><strong>数据中心芯片</strong></p><ul><li><strong>Hopper架构H100</strong>：基于台积电4nm工艺，集成4nm CoWoS封装技术，支持HBM3显存（带宽达5TB/s）和PCIe 5.0，单芯片FP8算力达4PetaFLOPS，是AI训练与超算的核心引擎，被广泛应用于ChatGPT等大模型训练。</li><li><strong>Ampere架构A100</strong>：前一代数据中心旗舰，采用5nm工艺，支持多实例GPU（MIG）技术，可分割为7个独立计算单元，兼顾算力与资源利用率，已部署于全球超50%的TOP500超级计算机。</li><li><strong>Grace CPU</strong>：首款基于ARM架构的高性能CPU，与Grace Hopper Superchip结合GPU与CPU优势，内存带宽达1TB/s，专为AI和HPC workload优化，目标替代传统x86服务器芯片。</li></ul></li><li><p><strong>专用加速芯片</strong></p><ul><li><strong>DPU（数据处理单元）</strong>：如BlueField系列，集成ARM CPU、可编程网络加速引擎和安全协处理器，卸载服务器CPU的网络、存储与安全任务，提升数据中心整体效率，已与微软、AWS达成深度合作。</li><li><strong>AGX Orin</strong>：面向边缘计算与自动驾驶，集成12核ARM Cortex-A78AE CPU和2048核Ampere GPU，AI算力达200TOPS，支持L4级自动驾驶功能，是特斯拉、蔚来等车企的自动驾驶域控制器核心芯片。  <br/>二、技术优势：架构创新与生态壁垒<br/>英伟达芯片的核心竞争力源于持续的架构突破与全栈生态构建：</li></ul></li><li><strong>架构设计</strong>：从Fermi架构引入CUDA核心，到Volta架构的Tensor Core（专为深度学习优化），再到Hopper架构的Transformer Engine（加速大语言模型训练），每代架构均针对AI与并行计算需求升级。例如，Tensor Core支持混合精度计算，将FP16与FP32结合，在精度损失极小的情况下提升AI算力2-4倍。</li><li><strong>软件生态</strong>：CUDA平台作为全球最成熟的GPU编程模型，拥有超过400万开发者和2000+应用程序支持，形成“硬件-软件-开发者”正循环。此外，TensorRT（推理优化工具）、cuDNN（深度学习库）等工具链进一步降低AI开发门槛，巩固英伟达在AI软件生态的垄断地位。</li><li><strong>制程与封装技术</strong>：与台积电深度合作，率先采用4nm、5nm先进制程，并主导CoWoS（Chip on Wafer on Substrate）封装技术，实现多芯片异构集成（如H100集成GPU、HBM显存和IO die），突破物理性能瓶颈。  <br/>三、应用领域：从消费电子到产业变革<br/>英伟达芯片已渗透至几乎所有计算密集型领域：</li><li><strong>AI与深度学习</strong>：凭借H100/A100的算力优势，占据全球AI加速芯片市场80%以上份额，客户涵盖OpenAI、谷歌、Meta等科技巨头，以及高校科研机构。其推出的NeMo框架和NGC（NVIDIA GPU Cloud）平台，为开发者提供从模型训练到部署的全流程支持。</li><li><strong>自动驾驶</strong>：AGX Orin已成为自动驾驶域控制器的“标配”，除特斯拉外，小鹏、理想、奔驰等车企均采用其方案，支持激光雷达点云处理、实时路径规划等关键功能，推动L3/L4级自动驾驶商业化落地。</li><li><strong>元宇宙与数字孪生</strong>：GeForce RTX系列与Omniverse平台结合，支持实时物理模拟和3D渲染，助力企业构建虚拟工厂、数字城市等场景。例如，宝马利用Omniverse和RTX GPU构建虚拟生产线，将新车研发周期缩短30%。</li><li><strong>医疗与科学计算</strong>：在蛋白质结构预测（如AlphaFold）、癌症药物研发、气候模拟等领域，英伟达GPU加速了科研进程。美国橡树岭国家实验室的Summit超算（搭载V100 GPU）曾实现0.1秒内完成一次全球气候模型模拟。  <br/>四、市场影响与挑战<br/>英伟达芯片已成为全球数字经济的基础设施，2023年数据中心业务营收达470亿美元，同比增长279%，占总营收比重超70%，远超传统PC GPU业务。其市值一度突破2万亿美元，成为全球市值最高的半导体公司。  <br/>然而，挑战亦随之而来：</li><li><strong>供应链依赖</strong>：高度依赖台积电先进制程，地缘政治风险可能导致产能受限；</li><li><strong>竞争加剧</strong>：AMD推出MI300X GPU、Intel加速Xeon Max与Habana Labs AI芯片布局，AWS、谷歌等云厂商自研AI芯片（如Trainium/TPU），试图降低对英伟达的依赖；</li><li><strong>技术瓶颈</strong>：摩尔定律逼近极限，单芯片算力提升放缓，需通过Chiplet、3D封装等技术突破物理限制。  <br/>总结<br/>英伟达通过“GPU+CUDA”生态构建、架构持续创新与场景深度绑定，已从图形芯片供应商跃升为全球计算技术的引领者。其芯片不仅是AI革命的核心驱动力，更在重塑数据中心、自动驾驶、科学研究等产业格局。面对未来，如何在保持技术领先的同时应对供应链与竞争压力，将是英伟达持续增长的关键命题。</li></ol>]]></description></item><item>    <title><![CDATA[数字孪生中的 流渲染技术 与 大规模场景实践 数字孪生进化论 ]]></title>    <link>https://segmentfault.com/a/1190000047524943</link>    <guid>https://segmentfault.com/a/1190000047524943</guid>    <pubDate>2026-01-06 18:02:41</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>当“数字孪生”遭遇“卡在75%”的尴尬时刻，你是否曾遇到过这样的场景：</p><ul><li><em>在智慧城市平台上，试图拉近查看某个街道的实时交通状况，画面却永远卡在75%的加载进度？</em></li><li><em>在工厂数字孪生系统中，想要查看一台关键设备的内部结构，浏览器却因为模型过重而崩溃？</em></li><li><em>好不容易加载完一个园区的模型，旋转视角时却像看PPT一样一顿一顿？</em></li></ul><p>这些，都是数字孪生（Digital Twin）从概念走向大规模落地时，必须面对的“渲染之痛”。</p><p>数字孪生作为物理世界在数字空间的全息映射，正从单体设备走向城市级、园区级的大规模场景。然而，场景规模的增长速度，远远超过了硬件和网络性能的提升速度。一个现代智慧城市的数字孪生，可能需要处理数千平方公里的地理数据、数万栋建筑的 BIM 模型、数百万个物联网传感器的实时数据——这已经远远超出了传统三维渲染技术的处理能力。<br/><img width="723" height="437" referrerpolicy="no-referrer" src="/img/bVdnzyG" alt="" title=""/><br/>当场景数据量从 GB 级跃升至 TB 级乃至 PB 级时，基于“全量下载、本地渲染”的模式已无法满足实时性、可访问性与协同性要求。流渲染（Streaming Rendering）技术由此成为支撑大规模数字孪生应用的核心基础设施，通过数据调度、传输与呈现方式的根本性变革，实现了海量三维数据的可访问与可操作。</p><h2>一、为什么传统渲染在大规模数字孪生中“失灵”了？</h2><h3>1）数字孪生渲染的三大“不可能三角”：</h3><p>在理想状态下，我们希望数字孪生渲染同时满足：</p><ul><li><strong>高质量</strong>：高精度模型、真实材质、复杂光照</li><li><strong>大规模</strong>：城市级、工厂级的海量数据承载</li><li><strong>低延迟</strong>：实时交互、快速响应</li></ul><p>但传统渲染方式（尤其是基于 WebGL 的本地渲染）在面对大规模场景时，陷入了典型的“不可能三角”——三者不可兼得。</p><h3>2）数据量的“指数级爆炸”</h3><p>让我们看一组真实的数据对比：</p><ul><li><strong>单体设备</strong>：一个高精度泵阀模型，约50-100MB</li><li><strong>中型工厂</strong>：包含1000台设备，约50-100GB</li><li><strong>智慧城市</strong>：200km²区域，包含建筑、道路、植被，约10-20TB</li></ul><p>当数据量从GB级跃升到TB级，传统的“全量下载+本地渲染”模式彻底失效。即便使用最先进的网络（千兆光纤），下载10TB数据也需要超过24小时——这显然是不可接受的。</p><h3>3）终端设备的“性能天花板”</h3><p>数字孪生的用户终端千差万别：</p><ul><li>高性能工作站（专业GPU，32GB+内存）</li><li>普通办公电脑（集成显卡，8GB内存）</li><li>平板电脑、手机（移动端GPU，有限的内存）</li></ul><p>如果按照最高性能设备来设计，低端设备无法运行；如果按照最低性能设计，高端设备的潜力无法发挥。流渲染正是通过“按需分配”解决了这一矛盾。</p><h2>二、流渲染技术核心：按需调度与协同计算</h2><p>流渲染的本质是一种数据供给范式的革新。它不要求终端设备完整拥有或处理整个庞大的数据集，而是建立一个智能的调度系统，确保用户在交互过程中，能够实时获得其视野内必要的数据切片，并在云端或边缘完成大部分繁重的计算。<br/><img width="723" height="464" referrerpolicy="no-referrer" src="/img/bVdnzyU" alt="" title="" loading="lazy"/><br/>传统三维渲染依赖于终端设备的完整数据加载与本地图形计算能力，其瓶颈在于：</p><ul><li><strong>数据承载极限</strong>：终端存储与内存无法容纳城市级BIM+GIS融合数据</li><li><strong>计算能力不均</strong>：用户设备性能差异导致体验割裂</li><li><strong>网络传输低效</strong>：海量数据下载耗时无法满足实时需求</li></ul><p>流渲染通过将渲染管线解耦为“云端计算-网络传输-终端呈现”三个环节，实现了：</p><ul><li><strong>计算卸载</strong>：复杂渲染任务在云端高性能集群完成</li><li><strong>按需传输</strong>：仅传输视域范围内的必要数据</li><li><strong>终端适配</strong>：根据设备能力自动调整数据质量与呈现方式</li></ul><p>其核心在于：<strong>以网络和云端算力换取终端的普适性与体验的一致性</strong>。</p><h3>1) 空间分块与动态加载</h3><p>面对城市级模型，流渲染引擎会将整个三维场景依据地理坐标或逻辑结构，划分为众多可独立管理的数据块。当用户浏览时，系统持续进行视锥体裁切计算，仅请求并加载当前及邻近可视区域的数据块。远处的、不可见的部分则被暂时搁置，从而将单次处理的负载降至最低。<br/><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnzzp" alt="" title="" loading="lazy"/></p><h3>2) 多层次细节自适应</h3><p>为平衡画质与性能，同一地理对象会预先生成多个细节层级的版本。当用户从高空俯瞰时，引擎调用最低层级的简化模型以呈现宏观格局；当用户逐步拉近视角，系统则无缝切换至更精细的模型层级，展示建筑立面、窗户乃至设备纹理。这一过程是动态自适应的，依据屏幕像素误差、网络状况和设备性能自动决策，确保流畅交互。<br/><img width="723" height="401" referrerpolicy="no-referrer" src="/img/bVdnzzq" alt="" title="" loading="lazy"/></p><h3>3) 静态模型与动态数据的实时融合</h3><p>数字孪生的价值在于映射物理世界的状态。流渲染架构不仅传输静态三维网格，更建立了与实时数据流的通道。物联网传感器的读数、业务系统的状态更新、模拟分析的结果，都能作为属性或动画实时“绑定”到对应的三维实体上，将静态场景转化为一个持续跳动、反映现实状态的动态孪生体。</p><h2>三、技术冷静期：正视流渲染的挑战与局限</h2><p>在充分认识流渲染技术巨大优势的同时，我们必须清醒地认识到其固有的技术局限与应用挑战。任何技术决策都是利弊权衡的结果，全面理解流渲染的弊端对于架构设计与项目成功至关重要。</p><p>流渲染的根本前提是稳定可靠的网络连接，这在实际应用中构成了显著的脆弱性：</p><h3>1）连接敏感性问题</h3><ul><li><strong>网络抖动的影响</strong>：即使平均带宽足够，网络延迟的波动也会导致画面卡顿、加载中断；</li><li><strong>网络盲区限制</strong>：地下空间、偏远厂区、船舶等网络覆盖不足环境难以应用；</li><li><strong>带宽成本限制</strong>：高清画质流式传输的带宽消耗可观，大规模应用时网络成本显著。</li></ul><h3>2）成本结构的不确定性</h3><p>流渲染服务的使用成本是：固定基础设施成本+随用户数、使用时长、画质等级线性增长的变动成本，突发流量可能导致成本超预期增长（如突发事件期间大量并发访问）。</p><h3>3）技术碎片化现状</h3><ul><li><strong>技术栈选择困境</strong>：各家云服务商提供互不兼容的流渲染方案，不同终端平台（Web、移动、XR）需要不同适配方案；</li><li><strong>集成复杂度高</strong>：与现有业务系统（ERP、MES、SCADA）集成缺乏标准接口，多源数据融合（BIM+GIS+IoT）的流式化处理缺乏成熟方案。</li></ul><h2>四、技术演进趋势：下一代流渲染架构</h2><p>随着数字孪生应用向更广泛领域和更深层次发展，流渲染技术正在经历从单一方案向多元融合的演进。下一代流渲染架构将不再局限于传统的云端渲染或纯 WebGL 方案，而是呈现出多技术路径融合、智能协同的新特征。</p><p><img width="668" height="213" referrerpolicy="no-referrer" src="/img/bVdnzzr" alt="" title="" loading="lazy"/><br/>随着数字孪生应用场景的多样化，渲染架构并未收敛于单一方案，而是形成了 “<strong>端渲染（WebGL）</strong>”与“<strong>流渲染</strong>”<strong>两条清晰且长期共存的技术路径</strong>。下一代架构的核心演进方向并非强制性融合，而是实现更深刻的 “<strong>兼容性</strong>”——即系统平台能够支撑两种模式，并允许用户或部署者根据具体场景的约束与需求，做出最务实的选择。关键在于理解“何时选择端”与“何时选择流”，并在架构上使二者能灵活部署甚至并存。</p><h2>五、渲染不是终点，体验与价值才是</h2><p>数字孪生的终极目标并非仅仅是“渲染得更好”，而是“用得更好”。流渲染作为一种关键的基础设施技术，其意义在于让大规模、高保真的三维场景能够被广泛、实时、协同地访问与操作，从而支撑起真正的业务洞察与决策优化。</p><p>未来，随着5G/6G网络普及、算力成本持续下探、渲染与 AI 技术进一步融合，流渲染将更加智能、自适应与无缝。但无论技术如何演进，衡量其成功的标准始终是：是否让数字孪生从“可看”走向“可用”，从“展示”走向“驱动”。</p><p>关注 “数字孪生进化论” ，与我们共同探索数字孪生技术前沿与落地实践。</p>]]></description></item><item>    <title><![CDATA[如何利用生产调度分析实现汽车生产的实时决策与优化？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524952</link>    <guid>https://segmentfault.com/a/1190000047524952</guid>    <pubDate>2026-01-06 18:02:01</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在现代汽车制造业加速向智能化、柔性化与绿色化转型的背景下，生产调度分析已从传统的排产工具，演变为驱动整车制造效率跃升的核心决策中枢。面对多品种、小批量、快交付的市场需求，传统依赖人工经验与静态计划的调度方式难以应对复杂的产线协同与供应链波动，而以数据驱动、算法赋能和人机协同为特征的新一代生产调度分析系统，正重塑汽车生产的响应逻辑与价值创造模式。<br/>广域铭岛的Geega工业互联网平台正是这一变革的典型实践者。在汽车制造场景中，Geega系统通过实时采集焊装、涂装、总装等关键工序的300余项工艺参数，结合Few-Shot Learning等先进算法，将老师傅对工序交叉操作的隐性经验——如特定车型换线时的工位衔接节奏、物料批次异常时的应急处理策略——转化为可计算、可复用的智能模型。这使得系统不仅能提前12小时预判物料齐套风险，还能在焊装线因夹具切换导致停机前自动调整排程，避免产线空转，显著提升设备综合效率（OEE）。<br/>在汽车生产中，时间损失、速度损失与质量缺陷是影响交付与成本的三大核心瓶颈。Geega平台通过对OEE的精细化钻取分析——精准识别设备故障、换型时间、速度降速与缺陷率等关键因子——帮助企业定位效率洼地。例如，在某新能源汽车厂商的产线部署中，系统通过动态优化电池包装配节拍与物流配送节奏，将单台车的装配周期缩短8%，同时将因物料错配导致的返工率降低40%。更进一步，系统还与供应链端联动，当预判到某关键电子元件到货延迟时，自动触发供应商预警，并智能重组后续车型的生产顺序，确保高价值订单优先交付。<br/>此外，生产调度分析在汽车制造中的价值不仅体现在技术层面，更推动了组织形态的升级。借助FineBI等商业智能工具，生产、物流、质量等部门可共享同一套数据视图，实现“厂级—车间—工段”三级联动决策。管理者不再依赖周报与会议做判断，而是通过可视化仪表盘实时掌握产线健康度，实现分钟级策略迭代。这种“协同式治理”模式，让原本割裂的部门协同成为常态，真正实现了“以数据说话、用算法决策”。<br/>对于中小型汽车零部件供应商而言，广域铭岛提供的低代码开发界面与预置行业模型库，大幅降低了系统落地门槛。无论是冲压线的预测性维护，还是注塑工艺的良率优化，企业均可通过拖拽式配置快速构建专属AI调度应用，实现低成本、敏捷化部署。<br/>综上所述，生产调度分析在汽车制造领域已超越“排产”功能，成为连接人、机、料、法、环的智能神经中枢。它以数据为基因、算法为引擎、人机协同为纽带，不仅提升了产线效率与交付韧性，更重构了汽车企业的运营逻辑。广域铭岛的实践表明，唯有将先进的调度分析系统与制造流程深度耦合，汽车企业才能在激烈竞争与供应链波动中，实现柔性生产、绿色低碳与客户满意三者的统一，真正迈向智能制造的新纪元。</p>]]></description></item><item>    <title><![CDATA[基于 SGlang RBG + Mooncake 打造生产级云原生大模型推理平台 龙蜥社区 ]]></title>    <link>https://segmentfault.com/a/1190000047524957</link>    <guid>https://segmentfault.com/a/1190000047524957</guid>    <pubDate>2026-01-06 18:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>作者 | 玖宇（SGLang 社区 &amp; 阿里云），杨彦波（SGLang 社区 &amp; 科大讯飞），孙伟祥（SGLang 社区 &amp; 小红书），宋阳 （SGLang 社区 &amp; 小红书），雨杨（Mooncake &amp; 阿里云）</p><h3>背景</h3><p>大语言模型（LLM）推理服务正迅速成为企业级应用的核心基础设施。生产级落地的关键在于性能、稳定性与成本三者的平衡，而本文聚焦于如何构建稳定的高性能推理系统。</p><p>当前，LLM 推理架构正从单体模式向分布式演进，主流路径包括 Prefill-Decode（PD）分离、Attention-FFN（AF）分离以及 KVCache 外置。这一演进的根本动因是模型规模扩张导致的显存压力：在长上下文或高并发场景下，KVCache 显存占用常超 70%，单纯依赖 GPU HBM 与 CPU DRAM 已难以为继。将 KVCache 解耦外置，不仅能突破存储容量瓶颈，更能实现跨请求缓存共享、弹性伸缩与故障隔离等关键能力。尤其在 RAG、AI Agent、长文本生成等机器驱动消费 Token 的场景中，提示词模板化与可复用性成为常态，外置 KVCache 已成为保障低延迟、高吞吐与成本效益的必选项。</p><p>Mooncake 作为业界主流的分布式 KVCache 存储引擎，正是为应对这一挑战而生。它通过专用缓存集群为 SGLang 等推理框架提供高吞吐、低延迟的 KVCache 分布式服务。</p><p>然而，在生产环境中管理 Mooncake 这类分布式 KVCache 系统，以实现稳定的高性能仍面临新挑战：</p><ol><li>部署与运维复杂度高： 推理服务不限于单一 Pod，还可能是由 Prefill/Decode 计算节点与 Mooncake 缓存节点 构成的分布式系统。两者需在拓扑亲和、生命周期、扩缩容策略上深度协同，而 Kubernetes 原生 Workload（Deployment/StatefulSet）难以表达这种多角色强协同语义，导致配置繁琐、资源浪费或性能劣化。</li><li>滚动升级稳定性风险：Prefill 与 Mooncake 实例在升级过程中缓存丢失，迫使活跃会话的Prefill阶段需要重新计算，引发 P99 延迟毛刺与吞吐量断崖，严重影响服务稳定性。</li></ol><p>为根治这些痛点，RoleBasedGroup（RBG）应运而生。作为面向 AI 推理的 Kubernetes 原生 API，RBG 通过多角色协同编排，将 Mooncake 缓存与 SGLang 推理节点视为同一服务的不同角色，统一管理其部署、升级与弹性。借助 RBG 的原地升级与拓扑感知能力，既能尽可能避免缓存丢失，又能确保计算与缓存升级、调度和伸缩策略上的一致性，从而在性能最大化的同时，保障生产环境的稳定性与可运维性。</p><p>本文旨在阐明如何将 Mooncake Store 作为 RBG 编排下 SGLang PD 分离推理服务的补充角色，系统化实现生产级 KVCache 外置能力。</p><h3>Mooncake：面向大模型推理的分布式 KVCache 存储引擎</h3><p>项目地址：<a href="https://link.segmentfault.com/?enc=wXp3FXH3lp8dM%2Fz6gSlxSg%3D%3D.qLZCaoRhW0Y%2Fqo%2F%2FOMCoMVw8gkjbz4Y1NMCmYTiWIEl488BXlysKyrR11fSFwb8C" rel="nofollow" target="_blank">https://github.com/kvcache-ai/Mooncake</a></p><p>Mooncake 是 SGLang HiCache（层级缓存）的高性能分布式 L3 存储后端，通过 RDMA 实现跨机 KVCache 共享，突破单机 GPU/CPU 缓存容量瓶颈。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524959" alt="图片" title="图片"/></p><p>核心组件：</p><ul><li>Master Service： 管理集群存储池、元数据与节点生命周期</li><li>Store Service： 提供分布式缓存存储，支持多副本、条带化传输与热点负载均衡</li></ul><p>核心特性：</p><ul><li>RDMA 加速 + 零拷贝机制，实现高带宽、低延迟数据访问</li><li>智能预取与 GPU 直传，最大化 I/O 效率</li><li>支持 PD 分离架构，提升大规模集群 Token 吞吐量</li></ul><p>快速预览：</p><pre><code># 启动 Master
mooncake_master --http_metadata_server_port=9080
# 启动 Store 服务（配置 RDMA 设备与内存池）
python -m mooncake.mooncake_store_service --config=config.json
# 启动 SGLang（启用 Mooncake 后端）
python -m sglang.launch_server \
    --enable-hierarchical-cache \
    --hicache-storage-backend mooncake \
    --model-path &lt;model_path&gt;</code></pre><h3>RoleBasedGroup (RBG)：面向大模型推理的弹性角色编排引擎</h3><p>项目地址：<a href="https://link.segmentfault.com/?enc=wR222p%2FqWFRSrqxpjECARA%3D%3D.1WrS%2F8ZrnHTzVY%2FUyKc1azD0unioBT1RbQot9z0EZgtBw7vcdjp9tGbEAIqh99P5" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg</a></p><h4>3.1 核心问题：大模型推理生产落地的五大挑战</h4><p>大模型推理正演变为"最昂贵的微服务"——既需 HPC 集群的极致性能，又要求云原生的敏捷弹性。当前生产环境面临五大根本性挑战：</p><ol><li>快速架构迭代： 分离式大模型推理架构（如 Prefill/Decode 解耦、多级 Router/Gateway 等）演进极快，传统依赖固定抽象的平台难以及时适配新架构。</li><li>性能敏感：TTFT、TPOT 等关键性能指标对 GPU 拓扑（NVLink / PCIe）、RDMA 亲和性等因素有亚毫秒级敏感度，随意迁移或不当调度都会放大首响、尾响时延。</li><li>组件强依赖：关键角色之间存在强依赖关系（如 Prefill 与 Decode 等角色需要 1:1、N:1 等强绑定关系），版本升级、回滚必须在多个角色之间保持原子性，否则容易导致请求失败或数据不一致。</li><li>运维效率低：现有平台在重启、扩缩容、故障迁移等运维操作上缺乏对多角色整体的统一视角，日均高达 5% 的时间消耗于重启扩容升级中的手动协调，导致 GPU 资源空置浪费。</li><li>资源潮汐显著与利用率不足：线上流量峰谷差常超 10 倍，但静态配置的推理服务 GPU 平均利用率长期低于 30%，性能与成本难以兼得。</li></ol><p>根本矛盾：传统微服务面向无状态、弱拓扑场景，而大模型推理是强状态、拓扑感知、极致性能的有状态应用。</p><h4>3.2 RBG 设计理念：角色即一等公民，角色协同即核心场景</h4><p>RBG 源自 SGLang 社区，由小红书，算秩未来，科大讯飞、阿里云和南京大学等联合贡献。其核心目标，是在兼顾性能与稳定性的前提下，以"角色（Role）"作为调度编排的原子单元，构建贴合 LLM 推理特性的管理范式。</p><p>RBG 将一次推理服务视为拓扑化、有状态、可协同的"角色有机体"，而非孤立的 Deployment 集合。基于此理念，RBG 提出面向生产环境的 SCOPE 核心能力框架：</p><ul><li>S – Stable：面向拓扑感知的确定性运维</li><li>C – Coordination：跨角色协同策略引擎</li><li>O – Orchestration：有编排语义的角色与服务发现</li><li>P – Performance：拓扑感知的高性能调度</li><li>E – Extensible：面向未来的声明式抽象</li></ul><h4>3.3 SCOPE 核心能力解析</h4><p>3.3.1 Stable (稳定)：面向拓扑感知的确定性运维</p><p>稳定性是 RBG 的基石。通过为每个 Pod 注入全局唯一 RoleID，并遵循 "最小替换域" 原则，RBG 确保运维操作在原有 GPU-NVLink 域、NUMA 节点等硬件拓扑范围内完成，尽量避免拓扑漂移导致的性能抖动。</p><pre><code>roles:
- name: prefill
  replicas: 3
  rolloutStrategy:
    rollingUpdate:
      type: InplaceIfPossible
      maxUnavailable: 1</code></pre><p>3.3.2 Coordination (协同)：跨角色协同策略引擎</p><p>RBG 内置声明式协同引擎，通过Coordination 机制精确定义角色间依赖关系：</p><ul><li>部署协同：例如 Prefill 与 Decode 以特定比例成对调度、成组就绪；</li><li>升级协同：支持“比例协议”式升级，确保多角色版本一致性，避免部分升级导致协议不兼容；</li><li>故障协同：预定义联动策略，某个角色故障时触发关联角色的自动补救或迁移；</li><li>伸缩协同：在扩缩容时按照角色关系配比成组调整实例，保持吞吐与延迟表现稳定。</li></ul><p>这种精细化协同能力，将复杂分布式推理服务作为统一生命周期的整体进行管理，极大降低运维复杂度。</p><pre><code># 示例：PD 分离架构中 Prefill 和 Decode 角色协作升级
coordination:
- name: prefill-decode-co-update
type: RollingUpdate
  roles:
  - prefill
  - decode
  strategy:
    maxUnavailable: 5%
    maxSkew: 1% # 两个角色在升级的过程中新版本比例的最大偏差
    partition: 20%
roles:
- name: prefill
  replicas: 200
  template: ...
- name: decode
  replicas: 100
  template: ...</code></pre><p>3.3.3 Orchestration (编排)：编排化的角色与服务发现</p><p>RBG 显式定义角色依赖与精确启动顺序，实现编排化管理。更关键的是，它提供拓扑自感知的内建服务发现，在 Pod 启动时将完整拓扑信息（各角色 IP、属性、关系等）注入环境变量或配置文件。</p><p>推理引擎（SGLang、vLLM 等）可直接从本地配置读取拓扑视图，无需依赖 etcd、Consul 等外部服务发现系统，使服务跨环境迁移更自包含，显著降低集成复杂度。</p><p>3.3.4 Performance (性能)：拓扑感知的高性能调度</p><p>单次请求的延迟与吞吐高度依赖硬件拓扑与资源亲和性。RBG 引入拓扑感知的装箱策略，支持多维度性能优化：</p><ul><li>GPU 拓扑优先级（如 GPU-NVLink &gt; PCIe &gt; RDMA &gt; VPC）</li><li>角色之间的亲和与反亲和约束</li><li>同角色实例的布局均衡性</li><li>部署完成后的短路读优化</li></ul><p>通过这些约束与策略，RBG 在大规模部署时，能够在不牺牲稳定性的前提下，尽可能贴合最优的硬件拓扑，从而保障 TTFT、TPOT 等关键性能指标。</p><p>3.3.5 Extensible (可扩展)：面向变化的部署抽象</p><p>RBG 通过声明式 API（RBG、RBGs、EngineRuntimeProile等）与插件化机制，将 "角色关系定义"与"部署 / 模型管理 / 弹性策略"解耦 。</p><p>当社区演进出新架构（如新路由层形态、分离式架构等时），无需修改 RBG 核心代码，只需通过 YAML 定义新角色模板与关系，即可快速落地。这种"声明式 API + 插件机制"的平台化设计，将新架构的投产周期显著缩短。</p><pre><code># 示例：PD 分离架构角色定义
roles:
- name: prefill
  replicas: 2
  engineRuntimes:
  - profileName: custom-engine-runtime
  template:
  ...
- name: decode
  replicas: 1
  engineRuntimes:
  - profileName: custom-engine-runtime
  template:
  ...</code></pre><p>RBG 通过 Kubernetes 原生 API ，为大模型推理服务提供了一套稳定（Stable）、协同（Coordination）、可编排（Orchestration）、高性能（Performance）、可演进（Extensible）的统一承载层，是面向现代 LLM 推理工作负载的一种新型部署与运维抽象。</p><h3>基于RBG部署PD分离架构+Mooncake 推理服务</h3><p>4.1. 部署架构<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524960" alt="图片" title="图片" loading="lazy"/></p><p>通过 RoleBasedGroup 可部署高可用、弹性的 SGLang PD 分离推理系统，核心组件如下：</p><p>整个系统由以下核心角色构成：</p><ul><li>SGLang Router： 作为统一的请求入口与流量调度器，负责接收用户推理请求，根据负载状态、上下文长度和模型配置，智能为请求选择合适的Prefill 和 Decode 节点进行处理。</li><li>Prefill Serving Backend： 专用于处理提示词（prompt）的前向计算，生成初始 KVCache；通常为计算密集型，对显存带宽敏感。</li><li>Decode Serving Backend： 专注于自回归生成阶段的 token 逐个解码，依赖已生成的 KVCache 进行高效推理；对缓存访问延迟极为敏感。</li><li>Mooncake Master/Store： 作为独立的 KVCache 外置存储角色，提供高吞吐、低延迟的分布式缓存服务，持久化存储所有推理会话的 Key-Value Cache。它不仅突破了单 GPU HBM 和 CPU DRAM 的容量限制，还支持跨请求缓存复用以及细粒度缓存淘汰策略（如 LRU + 高水位驱逐）。</li></ul><p>这些角色并非孤立运行，而是通过 RBG 提供的原生多角色协同能力紧密集成。此外，EngineRuntime 作为 RBG 注入给引擎服务 Pod 的 Sidecar，成为推理引擎与上层编排系统的桥梁，提供了服务注册与元数据上报、动态 LoRA 加载 / 卸载、流量状态控制和可观测性集成的关键的运行时能力。</p><h4>4.2. 通过 RBG 部署 Mooncake + SGLang PD 分离推理服务</h4><ul><li>安装 RBG：<br/><a href="https://link.segmentfault.com/?enc=TuMJiWrLMIIf058PFduYZw%3D%3D.hs4kzMlbnrHFgbCpDQPKW%2FD4Y9gOVlyEoDLpt02nFHR1ee4R7Yzmq6%2FdOn86gebo9Awug3fvQTa%2B5eX80e3oJA%3D%3D" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/doc/install.md</a></li><li>镜像准备见附录 8.1</li><li>服务部署</li></ul><p>准备好容器镜像后，使用下面的 yaml，可以基于 RBG 部署带有 KVCache Offloading 能力的 SGLang PD 分离推理服务：<br/><a href="https://link.segmentfault.com/?enc=V82mF4XvZLciM7YEqjTpOQ%3D%3D.yGiTN629X%2BnAI82rtYbwo0%2FfiA5czqgD2QnMWkceOoaLj6wK7i8ATrR1T6nRKa1iLh9zrT4jzizk0IIL4%2FJHaW8%2FxtlWdrCGkBJ34b3A7ErrWRsqBZY%2FiyS75EqMUB29aHMBf576Yn3ohRlH358qOw%3D%3D" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/examples/mooncake/pd-disaggregated-with-mooncake.yaml</a> </p><p>yaml 中涉及的环境变量说明可以参考：<br/><a href="https://link.segmentfault.com/?enc=kS7Nb7lKuR2k5amX5juKeg%3D%3D.3LTA8haw3H1hUjZmcxaeYzeWRNKWjVIPSrV5hN5mdSnaKvxUwFa%2BPnZjHksY%2F9nB8a9xwftQVCb5aNPu6Q7gEENdA3nocVFfqG%2BN6FULhDA%3D" rel="nofollow" target="_blank">https://github.com/kvcache-ai/Mooncake/blob/main/doc/zh/mooncake-store.md</a></p><ul><li>查看部署结果：</li></ul><pre><code>kubectl get pods -l rolebasedgroup.workloads.x-k8s.io/name=sglang-pd-with-mooncake-demo
sglang-pd-with-mooncake-demo-router-0               1/1     Running   0          71s
sglang-pd-with-mooncake-demo-prefill-0              1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-decode-0               1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-master-0      1/1     Running   0          4m2s
sglang-pd-with-mooncake-demo-mooncake-store-bh9xs   1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-store-dsrv4   1/1     Running   0          3m42s
sglang-pd-with-mooncake-demo-mooncake-store-tqjvt   1/1     Running   0          3m42s</code></pre><ul><li>查看 Mooncake Store 角色其中一个实例的网络和 location 信息：</li></ul><pre><code>kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.spec.nodeName}'
kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.status.podIP}'</code></pre><h4>4.3. Benchmark 测试结果：多级缓存加速显著</h4><ul><li>Baseline（仅 GPU 显存）：缓存命中率低，平均 TTFT 5.91s，P90 12.16s，系统吞吐受限，InputToken 吞吐仅为 6576.85 token/s。</li><li>L2DRAMHiCache： 命中率提升至 40.62%，平均 TTFT 降至 3.77s（↓36.2%），P90 降至 10.88s，InputToken 吞吐提升至 10054.21 token/s（↑52.89%）。</li><li>L3 Mooncake 缓存：命中率进一步跃升，平均 TTFT 降至 2.58s（↓56.3%），P90 大幅改善至 6.97s（↓42.7%），InputToken 吞吐提升至 15022.80 token/s（↑49.41%）。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524961" alt="图片" title="图片" loading="lazy"/><br/>（图/多轮对话测试场景下服务整体吞吐指标）</p><p><img width="723" height="420" referrerpolicy="no-referrer" src="/img/bVdnzzG" alt="image.png" title="image.png" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524962" alt="图片" title="图片" loading="lazy"/><br/>（图/多轮对话测试场景下 KVCache 命中率及对应 TTFT 指标）</p><p><em>测试细节详见附录 8.2。</em></p><h3>通过原地升级能力实现Mooncake 版本平滑升级</h3><p>由于 Mooncake 内置的 transfer-engine 与 SGLang Serving Backend（Prefill/Decode）中的 transfer-engine 需保持严格版本一致，以确保 KVCache 传输协议的兼容性，因此在推理引擎升级时，Mooncake 需要同步进行版本更新。</p><p>然而，Mooncake 作为有状态的缓存服务，其 KVCache 数据通常仅驻留在内存中。在传统 Kubernetes 滚动升级（Rolling Update）过程中，旧 Pod 被终止时，其内存中的缓存数据会立即丢失；而新 Pod 启动后需要经历重新调度、重新创建的过程。这导致所有依赖该节点缓存的活跃推理会话被迫中断，必须重新执行完整的 Prefill 计算——这一过程不仅计算开销巨大，还会引发：</p><ul><li>P99 首 Token 延迟显著毛刺（从秒级飙升至数十秒）；</li><li>因大量请求排队等待 Prefill，导致的系统吞吐量断崖式下跌；</li><li>用户体验剧烈抖动，破坏生产环境的服务稳定性。</li></ul><p>解决方案：Mooncake 缓存本地持久化 + RBG 原地升级：</p><ul><li>Mooncake 缓存本地持久化：在 Mooncake 社区的 PR#1031 中，mooncake 支持在节点 ShareMemory 和本地磁盘（或高性能 NVMe）上将 KVCache 元数据与热数据快照持久化，确保进程重启后可快速恢复缓存状态，避免缓存失效导致的 Prefill 重计算；</li><li>RBG 原地升级：通过 RBG 的精细化角色控制能力，在升级 Mooncake 角色时避免重建 Pod，而是原地替换容器镜像并复用节点的本地盘或共享内存，从而保留已持久化的缓存数据，实现“无缝”版本切换。</li></ul><p>二者结合，使得在 Serving Backend 与 Mooncake 联合升级过程中，KVCache 状态得以延续，活跃会话无需回退到 Prefill 阶段，从而有效规避了延迟毛刺与吞吐下跌，保障了大模型推理服务在版本迭代期间的端到端稳定性与高可用性。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524963" alt="图片" title="图片" loading="lazy"/></p><p>换言之，RBG 不仅解决了多角色协同部署的复杂性，更通过原地升级，将“有状态缓存服务的平滑演进”这一行业难题转化为标准化、可自动化的运维能力，真正实现了 “升级无感、服务不抖” 的生产级目标。</p><p>我们对刚刚部署的服务进行引擎版本的更新，由 v0.5.5 版本更新至 v0.5.6。</p><pre><code>kubectl patch rolebasedgroup sglang-pd-with-mooncake-demo \
  --type='json' \
  -p='[{"op": "replace", "path": "/spec/roles/1/template/spec/containers/0/image", "value": "lmsysorg/sglang:v0.5.6"}]'</code></pre><p>通过查看 Pod 状态能发现，在 Mooncake Store 角色镜像版本更新后仅发生了一次容器的重启。</p><pre><code>kubectl get pods -l rolebasedgroup.workloads.x-k8s.io/name=sglang-pd-with-mooncake-demo -owide
NAME                                                READY   STATUS             RESTARTS   AGE
sglang-pd-with-mooncake-demo-decode-0               1/1     Running            0          7m4s
sglang-pd-with-mooncake-demo-mooncake-master-0      1/1     Running            0          7m24s
sglang-pd-with-mooncake-demo-mooncake-store-bh9xs   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-mooncake-store-dsrv4   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-mooncake-store-tqjvt   1/1     Running            1          7m4s
sglang-pd-with-mooncake-demo-prefill-0              1/1     Running            0          7m4s
sglang-pd-with-mooncake-demo-router-0               1/1     Running            0          4m33s</code></pre><p>可以通过查看 Pod 的事件确认重新原因：</p><pre><code>kubectl describe pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4
Events:
  Type     Reason          Age                  From               Message
  ----     ------          ----                 ----               -------
  Normal   Scheduled       27m                  default-scheduler  Successfully assigned default/sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 to cn-beijing.10.134.xxx.xxx
  Normal   AllocIPSucceed  27m                  terway-daemon      Alloc IP 10.134.25.238/16 took 584.019653ms
  Normal   Created         27m                  kubelet            Created container: store
  Normal   Pulled          27m                  kubelet            Container image "lmsysorg/sglang:v0.5.5" already present on machine
  Normal   Started         27m                  kubelet            Started container store
  Normal   Killing         21m                  kubelet            Container store definition changed, will be restarted</code></pre><p>确认重启的 Mooncake 实例状态可以发现，在原地升级后 Pod 的网络和拓扑信息并没有发生改变，配合 Mooncake 提供的缓存持久化能力，可以保证重启前的 KVCache 缓存并没有发生丢失，在原地升级后预期地完成了恢复。</p><pre><code>kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.spec.nodeName}'
 kubectl get pods sglang-pd-with-mooncake-demo-mooncake-store-dsrv4 -o jsonpath='{.status.podIP}'</code></pre><h3>总结和展望</h3><p>本文系统阐述了如何通过 RoleBasedGroup（RBG） 与 Mooncake 的协同设计，构建生产级的稳定高性能 PD 分离推理服务。结论如下：</p><ul><li>RBG 重新定义了 LLM 推理服务的编排范式：通过将多角色协同（PD 分离、Mooncake 缓存）与拓扑感知调度作为一等公民，RBG 不仅解决了分布式部署的复杂性，更通过原地升级能力攻克了"有状态缓存服务平滑演进"这一行业难题，实现了升级无感、服务不抖的生产级目标。</li><li>Mooncake 解锁了 KVCache 的无限可能：作为 L3 缓存层，Mooncake 通过分布式内存池与 RDMA 加速，使缓存命中率跃升，TTFT 降低 56.3%，P90 延迟改善 42.7%，同时将 GPU 平均利用率从不足 30% 提升至可持续弹性伸缩的水平，真正平衡了性能与成本。</li><li>分级缓存架构是长上下文推理的必由之路：从 GPU HBM → DRAM → Mooncake 的三级缓存体系，在 Benchmark 中证明了其有效性，尤其在多轮对话、RAG、AI Agent 等机器驱动场景中，缓存复用带来的边际成本递减效应将愈发显著。</li></ul><p>RBG + Mooncake 的实践表明，只有将高性能系统设计与云原生运维能力深度融合，才能让大模型推理真正从"能用"走向"好用"，从"实验室"走向"生产级"。 我们期待与社区共同推进这一范式，为下一代 AI 基础设施奠定基础。</p><p>Acknowledgment</p><ul><li>小红书：孙伟祥、宋阳、熊峰</li><li>科大讯飞：杨彦波</li><li>趋境科技：杨珂</li><li>Mooncake：马腾、蔡尚铭</li><li>阿里云：一斋、柏存、东伝</li></ul><h4>附录</h4><p>8.1 镜像构建</p><p>此本文所使用部署样例中，我们可以直接使用 SGLang社区的官方容器镜像 lmsysorg/sglang:v0.5.5（mooncake-transfer-engine &gt;= 0.3.7），该镜像已经默认包含了 Mooncake 相关依赖。如果有定制化需求，可以参考链接中提供的 Dockerfile 自行构建特定版本的 Mooncake 镜像：<a href="https://link.segmentfault.com/?enc=QuSr0WjXf4EYMqIT3hwGBw%3D%3D.lVaUJjMoE9aaLIsylK9UMkdDfZIq%2BYiqUsOVx6Yrp6lDhEjeIkUtINDhE%2FYw7GmhyuF0DnhYTN%2BE7fuwaPHskUYHWsuCFaVtMW8xrm2R1Ut%2BfL9NENvLsk1ZWbwKTqlz" rel="nofollow" target="_blank">https://github.com/sgl-project/rbg/blob/main/examples/mooncake/Dockerfile.mooncake</a></p><p>8.2 Benchmark 测试<br/>8.2.1 环境配置<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524964" alt="图片" title="图片" loading="lazy"/><br/>8.2.2 测试方法<br/>通过 HiCache 提供的多轮对话压测工具模拟多轮对话场景，测试 KVCache 可重用场景下开启了 L3 Mooncake + L2 Hicache 的推理服务，相对于仅开启了 L2 Hicache 和不开启 Hicache 的推理服务，在吞吐指标和 SLO 指标上的收益情况。</p><ul><li>测试对象<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047524965" alt="图片" title="图片" loading="lazy"/></li><li>测试命令</li></ul><pre><code>python3 benchmark/hicache/bench_multiturn.py \
--model-path /models/Qwen3-235B/Qwen3-235B-A22B \
--dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \
--disable-random-sample \
--output-length 1 \
--request-length 2048 \
--num-clients 150 \
--num-rounds 10 \
--max-parallel 4 \
--request-rate 16 \
--ready-queue-policy random \
--disable-auto-run \
--enable-round-barrier</code></pre><ul><li>分组记录：</li></ul><p><img width="723" height="179" referrerpolicy="no-referrer" src="/img/bVdnzzI" alt="image.png" title="image.png" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[中小企业CRM一体化能力横向对比：从闭环协同到智能生态的深度博弈 率性的开水瓶 ]]></title>    <link>https://segmentfault.com/a/1190000047524558</link>    <guid>https://segmentfault.com/a/1190000047524558</guid>    <pubDate>2026-01-06 17:05:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言</h2><p>在数字化转型进入“深水区”的当下，中小企业对CRM的需求已从“销售工具”升级为“全业务链路操作系统”——<strong>不仅要打通“市场-销售-采购-仓库-财务-客服-外勤”的信息断层，还要通过AI实现智能决策，通过</strong> <strong>API</strong> <strong>融入企业现有生态</strong>。</p><p>本文选取<strong>超兔一体云、Microsoft Dynamics 365、Odoo</strong> <strong>CRM</strong> <strong>、八百客CRM、ClickUp、Really Simple Systems、Free CRM</strong>七大主流CRM产品，从<strong>一体化管理深度、AI能力成熟度、</strong> <strong>API</strong> <strong>对接灵活性</strong>三大维度展开专业对比，为企业选型提供“业务适配性”参考。</p><h2>一、对比框架说明</h2><p>本次对比围绕“业务价值落地”核心，设置三大维度10项细分指标：</p><ol><li><strong>一体化管理</strong>：评估“数据连通性（7模块数据共享）、流程协同度（环节无断点）、行业适配性（如工业/零售场景）”；</li><li><strong>AI能力</strong>：评估“技术架构（原生vs第三方）、场景覆盖（通用vs行业）、落地效果（自动化vs决策支持）”；</li><li><strong>API</strong> <strong>对接</strong>：评估“接口丰富度、集成方式（API/RPA/低代码）、生态支持（第三方系统适配）”。</li></ol><h2>二、核心维度横向对比</h2><h3>（一）一体化管理：全链路闭环能力PK</h3><p>一体化管理的本质是“<strong>业务流、数据流、责任流的三统一</strong>”，以下分7大模块拆解各品牌的闭环深度：</p><h4>1. 模块能力对比表（核心摘要）</h4><table><thead><tr><th>模块</th><th>超兔一体云</th><th>Microsoft Dynamics 365</th><th>Odoo CRM</th><th>八百客CRM</th><th>ClickUp</th></tr></thead><tbody><tr><td><strong>市场</strong></td><td>百度/巨量引擎集成、线索一键转化</td><td>LinkedIn/Outlook联动、线索评分</td><td>360度画像、营销自动化</td><td>全链路协同、线索自动分配</td><td>低代码扩展、轻量级任务管理</td></tr><tr><td><strong>销售</strong></td><td>订单触发采购、应收多期拆分</td><td>Copilot商机总结、Sales Premium</td><td>销售漏斗、报价-签约流程</td><td>订单-工单闭环、信用控制</td><td>任务/目标集成、低代码扩展</td></tr><tr><td><strong>采购</strong></td><td>订单变更同步采购、智能询价比价</td><td>全流程支出控制、供应商绩效</td><td>销售触发采购、供应商API对接</td><td>采购-生产联动、绩效分析</td><td>低代码扩展、基础采购记录</td></tr><tr><td><strong>仓库</strong></td><td>库存实时更新、多仓预警</td><td>Supply Chain AI预测、仓储优化</td><td>3D货架、批次/序列号追溯</td><td>库存-工单联动、预警机制</td><td>低代码扩展、基础库存记录</td></tr><tr><td><strong>财务</strong></td><td>订单-应收-开票联动、账期风险控制</td><td>全球财务统一、Power BI分析</td><td>合同-回款核销、多币种核算</td><td>订单-生产-财务闭环、报表生成</td><td>低代码扩展、基础费用管理</td></tr><tr><td><strong>客服</strong></td><td>RFM复购预警、工单联动销售</td><td>销售-客服共享历史、Teams协作</td><td>客诉-批次追溯、售后反哺生产</td><td>客服-生产闭环、问题跟踪</td><td>低代码扩展、基础工单管理</td></tr><tr><td><strong>外勤</strong></td><td>App拜访记录、工单同步后台</td><td>Sales移动应用、实时数据同步</td><td>PDA扫码盘点、手机端录入</td><td>外勤-工单联动、任务提醒</td><td>手机端任务、时间跟踪</td></tr></tbody></table><h4>2. 典型闭环流程图（超兔一体云）</h4><p>以<strong>工业企业“销售-采购-仓库-财务”闭环</strong>为例，展示超兔的流程协同深度：</p><pre><code>sequenceDiagram
    participant 销售 as 销售模块
    participant 采购 as 采购模块
    participant 仓库 as 仓库模块
    participant 财务 as 财务模块
    销售-&gt;&gt;采购: 生成销售订单，触发采购计划
    采购-&gt;&gt;销售: 同步采购单状态（已审核）
    采购-&gt;&gt;仓库: 采购入库，更新库存
    仓库-&gt;&gt;销售: 同步库存（可发货）
    销售-&gt;&gt;仓库: 发起发货请求
    仓库-&gt;&gt;财务: 发货完成，触发应收款
    财务-&gt;&gt;销售: 同步应收状态（已开票）
    财务-&gt;&gt;采购: 同步供应商付款状态</code></pre><h3>（二）AI能力：从自动化到决策的升级</h3><p>AI是CRM的“大脑”，需实现“<strong>业务数据+AI模型</strong>”的深度融合，而非简单的工具化应用：</p><h4>1. AI能力对比表</h4><table><thead><tr><th>品牌</th><th>技术架构</th><th>核心场景</th><th>行业适配性</th></tr></thead><tbody><tr><td>超兔一体云</td><td>AI+Coze工作流</td><td>AI待办（跟单提醒）、AI日报（工作分析）、AI分析（通话内容提取）</td><td>工业企业（线索-订单-工单闭环）</td></tr><tr><td>Microsoft Dynamics 365</td><td>Copilot+Azure AI</td><td>Copilot（总结商机/会议）、AI需求预测（供应链）、行业用户行为预测（医疗）</td><td>跨国企业（全球供应链）</td></tr><tr><td>Odoo CRM</td><td>AI+行业模型</td><td>客户行为分析（个性化推荐）、销售机会成交概率预测</td><td>零售/制造（库存/画像）</td></tr><tr><td>八百客CRM</td><td>AI+Coze工作流</td><td>通话录音分析（关键话题）、AI智能体（跟进策略）</td><td>传统工业（线索-工单）</td></tr><tr><td>ClickUp</td><td>第三方AI集成</td><td>任务摘要生成（OpenAI）、文档辅助编写</td><td>科技创业（轻量级）</td></tr></tbody></table><h4>2. AI技术架构脑图</h4><pre><code>mindmap
    root((AI能力))
        技术路线
            AI
                超兔（AI智能体+Coze）
                Dynamics 365（Copilot+Azure）
                Odoo（预测模型）
            第三方集成
                ClickUp（OpenAI）
                Really Simple Systems（无）
        核心价值
            自动化（AI待办、订单采集）
            决策支持（AI分析、需求预测）
            行业适配（工业、零售、医疗）</code></pre><h3>（三）API对接：开放生态的构建</h3><p>API是CRM融入企业数字化生态的关键，需支持“<strong>内外部系统无缝联动</strong>”：</p><h4>1. API能力对比表</h4><table><thead><tr><th>品牌</th><th>接口类型</th><th>集成方式</th><th>生态支持</th></tr></thead><tbody><tr><td>超兔一体云</td><td>业务API（客户/订单）、RPA</td><td>API+RPA（电商/国税）</td><td>京东/淘宝/国税对接、RPA开发</td></tr><tr><td>Microsoft Dynamics 365</td><td>Power Apps/Azure API</td><td>API+低代码（Power Apps）</td><td>微软生态（Office 365、Azure）</td></tr><tr><td>Odoo CRM</td><td>REST API、电商/物流API</td><td>REST API+供应商对接</td><td>Amazon/Shopify/菜鸟对接</td></tr><tr><td>八百客CRM</td><td>基础业务API</td><td>API</td><td>文档陈旧、无Webhook</td></tr><tr><td>ClickUp</td><td>任务/文档API</td><td>API+低代码</td><td>供应链需自定义开发</td></tr></tbody></table><h2>三、综合能力雷达图（10分制）</h2><table><thead><tr><th>品牌</th><th>一体化管理</th><th>AI能力</th><th>API对接</th><th>总分</th><th>核心优势</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td><td>9</td><td>8</td><td>26</td><td>工业闭环、AI场景深度</td></tr><tr><td>Microsoft Dynamics 365</td><td>8</td><td>9</td><td>9</td><td>26</td><td>微软生态、全球合规</td></tr><tr><td>Odoo CRM</td><td>8</td><td>7</td><td>8</td><td>23</td><td>库存/行业定制</td></tr><tr><td>八百客CRM</td><td>7</td><td>6</td><td>5</td><td>18</td><td>基础闭环、工业适配</td></tr><tr><td>ClickUp</td><td>6</td><td>5</td><td>7</td><td>18</td><td>低代码扩展、轻量级团队</td></tr><tr><td>Really Simple Systems</td><td>5</td><td>3</td><td>6</td><td>14</td><td>极简无代码、小型企业</td></tr><tr><td>Free CRM</td><td>4</td><td>2</td><td>5</td><td>11</td><td>免费轻量、初创企业</td></tr></tbody></table><h2>四、典型场景适配建议</h2><table><thead><tr><th>企业类型</th><th>推荐品牌</th><th>核心原因</th></tr></thead><tbody><tr><td>工业/工贸企业</td><td>超兔一体云、Odoo CRM</td><td>订单-工单闭环、库存追溯</td></tr><tr><td>跨国/大型企业</td><td>Microsoft Dynamics 365</td><td>全球财务、微软生态</td></tr><tr><td>零售/电商企业</td><td>Odoo CRM、超兔一体云</td><td>精准营销、库存ABC分类</td></tr><tr><td>小型企业</td><td>Really Simple Systems</td><td>极简无代码、低学习成本</td></tr><tr><td>科技创业团队</td><td>ClickUp</td><td>任务/文档集成、低代码扩展</td></tr></tbody></table><h2>五、结论</h2><ul><li><strong>超兔一体云</strong>：工业/工贸企业首选，一体化闭环与AI场景深度适配；</li><li><strong>Microsoft Dynamics 365</strong>：跨国/大型企业首选，微软生态与全球合规优势；</li><li><strong>Odoo CRM</strong>：零售/制造企业首选，库存与行业定制化能力强；</li><li><strong>八百客CRM</strong>：传统工业企业过渡选择，基础闭环但AI/API需提升；</li><li><strong>ClickUp</strong>：轻量级团队选择，低代码扩展但供应链能力弱；</li><li><strong>Really Simple Systems</strong>：小型企业入门选择，极简无代码但功能有限；</li><li><strong>Free CRM</strong>：初创企业试水选择，免费但能力较弱。</li></ul><p>本对比从“业务落地”出发，覆盖中小企业核心痛点，为企业选型提供<strong>专业、可落地</strong>的参考。企业需结合自身行业特性与发展阶段，选择最适配的CRM工具，实现“从流程自动化到智能决策”的升级。</p>]]></description></item><item>    <title><![CDATA[汽车生产自动化服务商如何选择？ 雨大王 ]]></title>    <link>https://segmentfault.com/a/1190000047524565</link>    <guid>https://segmentfault.com/a/1190000047524565</guid>    <pubDate>2026-01-06 17:04:39</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>服务商在汽车制造业的定位与价值<br/>说起来，汽车生产自动化服务商正在重新定义整个汽车制造业的竞争格局。这些服务商不仅仅是设备供应商，更像是车企的"智能制造伙伴"，提供从单点设备到整体生产系统的全方位解决方案。随着汽车行业向电动化、智能化转型，传统生产模式已经难以满足新的制造需求，这就给了专业服务商巨大的发展空间。<br/>有意思的是，这些服务商的价值不仅体现在硬件设备上，更在于其对生产流程的深度理解和优化能力。就像给生产线装上了"智慧大脑"，通过数据驱动的方式实现生产过程的自我优化和持续改进。这种转变让车企能够更加专注于产品研发和市场开拓，而将生产环节的专业事务交给更懂行的合作伙伴。<br/>从实际效益来看，优秀的自动化服务商能够帮助车企实现显著的价值提升。除了提高生产效率和降低成本这些显性收益外，更重要的是能够提升生产柔性和产品质量一致性。在当今个性化消费时代，这种能够快速响应市场变化的生产能力，正在成为车企的核心竞争力之一。<br/>服务商的核心能力与差异化优势<br/>要成为一个出色的汽车生产自动化服务商，需要具备多方面的综合能力。技术实力固然重要，但更重要的是对汽车制造工艺的深度理解和行业经验积累。不同车企、不同车型的生产要求千差万别，没有足够的项目经验很难提供真正有效的解决方案。<br/>说到系统集成能力，这可能是区分优秀服务商和普通供应商的关键所在。现代汽车制造涉及冲压、焊接、涂装、总装等多个工艺环节，每个环节都有不同的设备和系统。优秀的服务商要能将这些分散的系统整合成一个有机整体，实现数据互通和协同优化。这就像指挥一个交响乐团，既要让每种乐器发挥最佳效果，又要确保整体演奏的和谐统一。<br/>在实际项目中，头部服务商大多采用平台化的解决方案架构。比如先构建一个统一的工业互联网平台，然后根据不同客户的具体需求进行定制化开发。这种做法的优势在于既能保证系统的标准化和可靠性，又能满足不同客户的个性化需求。特别是在当前汽车行业快速变革的背景下，这种灵活可扩展的架构显得尤为重要。<br/>除了技术能力，项目交付和持续服务能力也是关键考量因素。汽车生产是24小时连续作业，任何系统故障都可能造成重大损失。因此服务商不仅要能做好项目实施，更要建立完善的售后服务体系，确保出现问题能够快速响应和解决。<br/>典型案例与最佳实践<br/>在具体案例方面，广域铭岛为领克成都工厂打造的数字化项目颇具代表性。通过Geega工业互联网平台，实现了冲压、焊接、质量检测等环节的全链路数据贯通。这个项目不仅帮助工厂显著提升了生产效率，更重要的是建立了持续优化的数字底座，为未来的智能化升级奠定了坚实基础。<br/>国际巨头西门子在华晨宝马沈阳生产基地的项目也值得关注。他们通过数字化工厂解决方案，实现了生产过程的全面数字化管理。从订单接收到整车下线，整个流程都实现了可视化监控和智能调度。这个项目的特别之处在于将传统制造与数字技术深度融合，打造了一个行业标杆级的智能工厂。<br/>发那科为上汽通用提供的机器人自动化解决方案展示了在特定领域的专业深度。通过高度智能化的机器人工作站，不仅大幅提升了焊接质量和效率，还实现了生产数据的实时采集和分析。这种专注于特定工艺环节的深度解决方案，往往能带来意想不到的效益提升。</p>]]></description></item><item>    <title><![CDATA[你的业务该配哪款国产CRM？9 款CRM主流系统多维度推荐指南 傲视众生的脸盆 ]]></title>    <link>https://segmentfault.com/a/1190000047524572</link>    <guid>https://segmentfault.com/a/1190000047524572</guid>    <pubDate>2026-01-06 17:03:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、CRM 是什么？为什么企业需要它？</h2><h3>1. CRM 的核心定义</h3><p>CRM（Customer Relationship Management，客户关系管理）是以 “客户为中心” 为核心理念，结合信息技术实现客户全生命周期管理的系统。它通过整合营销、销售、服务全流程数据，帮助企业集中管理客户信息、自动化业务流程、分析客户行为，最终实现 “提升客户满意度” 与 “提高利润获取能力” 的双重目标（据上学吧 CRM 定义类题目）。</p><h3>2. CRM 的核心价值</h3><p>从企业实践看，CRM 的价值集中在四点：</p><ul><li>信息集中化：整合客户资料、沟通记录、交易历史，避免信息分散（比如超兔CRM的 “客户 360° 视图”）；</li><li>流程自动化：跟踪销售机会、自动提醒跟进节点、触发营销动作（比如有赞的 “优惠券过期自动提醒”）；</li><li>数据驱动决策：通过客户行为分析识别高价值客户、预测销售趋势（比如某母婴品牌用 CRM 提升 47% 复购率）；</li><li>体验个性化：快速响应客户需求、提供定制化服务（比如销售易的 “客户画像 + 精准营销”）。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524574" alt="" title=""/></p><h3>3. CRM 的四大类型</h3><p>根据 T 客研报与市场实践，国产 CRM 可分为四大类：</p><ul><li>外勤管理类：聚焦线下销售拜访、区域管理（如外勤 365、玄讯）；</li><li>客户服务类：侧重售后支持、工单管理（如小满 CRM、码客）；</li><li>销售自动化类：覆盖从线索到回款的全销售流程（如销售易、超兔CRM）；</li><li>SCRM（社交 CRM）：整合社交渠道（微信、企微），实现社交化客户运营（如腾讯企点、EC）。</li></ul><h2>二、9 款主流国产 CRM 深度解析（含超兔）</h2><p>以下基于市场份额、行业适配性、功能完整性，筛选 9 款国产 CRM，从品牌背景、核心功能、优势特色、适用场景四维度展开分析，用表格直观呈现关键信息：</p><table><thead><tr><th>品牌</th><th>品牌背景</th><th>核心功能</th><th>优势特色</th><th>适用场景</th></tr></thead><tbody><tr><td>超兔 CRM</td><td>2004 年成立，中国 SaaS 开创企业，21 年行业经验，服务 6 万 + 企业，尤其适配工业类、工贸类企业</td><td>1. 全业务一体化：覆盖 CRM、进销存、财务、生产工单等；2. 低成本客制化：支持功能订阅、三级菜单 / 工作台自定义等；3. AI 深度应用：AI 智能体 + Coze 工作流，嵌入客户视图；4. 多端覆盖：Web、App、小程序等多端落地；5. 上下游协同：通过 OpenCRM 连接客户与供应商；6. 全流程模块：含市场获客、客户中心、跟单管理、生产管理等完整模块</td><td>1. 大底座系统：业务和数据底层连通，无需考虑系统融合；2. 稳定性强：业内口碑良好，多企业因稳定性换用；3. 服务优质：40% 新客户来自老客户转介绍；4. 低成本切入：客制化订阅模式，适合小步快跑；5. 多组织支持：九级人员结构 + 矩阵式临时小组管理</td><td>工业类 / 工贸类企业、中小企业、需全业务一体化管理的企业、追求低成本客制化的企业、有上下游协同需求的企业</td></tr><tr><td>销售易（Neocrm）</td><td>2011 年成立，本土 CRM 领军企业，腾讯投资，连续 8 年入选 Gartner SFA 魔力象限，客户包括施耐德、三一、海康威视</td><td>1. 全流程自动化：覆盖营销、销售、服务全链路；2. 移动化社交化：支持多终端，整合企微 / 钉钉；3. 行业解决方案：针对制造、高科技、医疗、汽车提供深度定制</td><td>1. 本土化设计：契合中国企业管理文化与流程；2. AI 与大数据：6 大智能体实现流程智能化；3. 云架构：降低硬件维护成本</td><td>大中型企业、制造 / 高科技 / 医疗等垂直行业、需要全流程自动化与行业定制的企业</td></tr><tr><td>纷享销客</td><td>连续 5 年 IDC 中国 CRM 市场份额与增速双第一，提供营销、销售、服务、渠道全场景覆盖</td><td>1. PaaS 低代码开发：支持自定义流程；2. AI 融合：智能预测销售赢率、生成客户画像；3. 生态集成：与 ERP、OA 等系统无缝对接</td><td>1. 行业适配广：深耕 15 个行业，提供定制功能；2. 本地化服务：2 小时故障响应，满足双合规；3. 稳定性优：优于多数本土厂商</td><td>大型 / 跨国企业、需深度定制与数据打通的企业、快消 / 医疗等行业</td></tr><tr><td>神州云动</td><td>专注云计算服务，基于云技术构建，聚焦中小企业需求</td><td>1. 全流程整合：覆盖营销、销售、服务；2. 灵活配置：可根据业务发展调整功能模块</td><td>1. 高定制化：满足快速试错、调整需求；2. 高性价比：部署成本低于大中型 CRM</td><td>中小型企业、初创企业、需快速调整业务流程的企业</td></tr><tr><td>八骏科技</td><td>深耕 B2B/B2G 行业 11 年，提供针对性解决方案</td><td>1. 个性化自定义：适配行业特性调整销售流程；2. 客户分级管理：针对政府 / 企业客户的 VIP 维护功能</td><td>1. 行业经验足：熟悉长周期、高客单价销售模式；2. 精准匹配：避免功能冗余</td><td>B2B/B2G 行业企业（如设备制造、政务服务）、不同销售模式的企业</td></tr><tr><td>用友 CRM</td><td>用友集团旗下，依托 ERP 生态，聚焦制造业端到端数据打通</td><td>1. ERP 集成：与用友 ERP 无缝对接，全链路追踪；2. 供应链协同：整合供应商、经销商数据</td><td>1. 数据一致性：避免系统数据冲突；2. 行业深度：针对制造业的库存联动、产能规划功能</td><td>制造业巨头、需 ERP 与 CRM 打通的企业</td></tr><tr><td>金蝶 CRM</td><td>金蝶集团旗下，基于云架构，聚焦中小企业轻量化需求</td><td>1. 营销自动化：支持邮件 / 短信营销、活动 ROI 分析；2. 销售管理：线索 - 商机 - 订单全流程跟踪</td><td>1. 云部署：无需购买服务器，降低 IT 成本；2. 易用性强：界面友好，适合无专业 IT 团队的企业</td><td>中小企业、需快速部署的企业、轻量级销售管理需求</td></tr><tr><td>有赞 CRM</td><td>专注零售行业，服务 60 万 + 品牌，赋能 228 万销售 / 导购，管理 5.6 亿客户</td><td>1. 全渠道整合：覆盖线上、线下、社交客户数据；2. 导购赋能：支持客户管理、优惠券推送；3. 营销自动化：生日祝福、弃购提醒等</td><td>1. 零售行业经验丰富：熟悉导购 - 客户互动模式；2. 全渠道覆盖：解决数据割裂问题</td><td>零售品牌（如服装、美妆）、需导购赋能与全渠道管理的企业</td></tr><tr><td>励销云</td><td>聚焦中小企业，以 AI 技术为核心，解决获客难问题</td><td>1. AI 线索挖掘：大数据智能搜索潜在客户；2. 销售自动化：跟踪商机、自动提醒跟进</td><td>1. AI 驱动：降低找客户的时间成本；2. 高性价比：年费低于多数 CRM</td><td>中小企业、需快速获客的企业（如 To B 服务、消费品）</td></tr></tbody></table><h2>三、CRM 怎么用？关键流程与落地技巧</h2><p>CRM 的价值不是 “买了就有”，而是 “用对了才有”。根据超兔、销售易、有赞等厂商的实践，落地 CRM 的核心流程是：</p><h3>1. 数据录入：构建 “客户单一视图”</h3><ul><li>录入内容：客户基本信息、沟通记录、交易历史、行为数据；</li><li>注意事项：确保数据真实（如超兔的客户信息云端同步）、避免重复（如超兔的客户查重功能）。</li></ul><h3>2. 流程配置：匹配企业业务逻辑</h3><ul><li>销售流程：设置 “线索 - 商机 - 订单 - 回款” 节点（如超兔的多种跟单模型、销售易的精细化打单流程）；</li><li>审批流程：配置报价单、合同审批等环节（如移动提交审批）；</li><li>提醒规则：设置客户生日、线索跟进提醒（如有赞的优惠券过期提醒）。</li></ul><h3>3. 人员培训：让工具 “用起来”</h3><ul><li>新人培训：通过知识库学习、老员工带教，快速掌握操作；</li><li>考核绑定：将 CRM 使用情况纳入销售绩效（如线索跟进率、客户信息完整率）。</li></ul><h3>4. 数据利用：从 “记录” 到 “决策”</h3><ul><li>分析客户：通过客户画像识别高价值客户；</li><li>优化流程：通过销售漏斗分析转化短板；</li><li>精准营销：根据客户行为推送定制内容。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524575" alt="" title="" loading="lazy"/></p><h2>四、CRM 选型：6 个关键维度帮你避坑</h2><p>企业选择 CRM 时，需避免 “跟风选大牌”，而是匹配自身需求。以下 6 个维度是选型的核心：</p><h3>1. 明确需求：“我要解决什么问题？”</h3><ul><li>比如：工业 / 工贸企业需 “全业务一体化”（选超兔 CRM）、制造企业需 “ERP 集成”（选用友 CRM）、零售企业需 “导购赋能”（选有赞）。</li></ul><h3>2. 易用性：“员工愿意用吗？”</h3><ul><li>界面友好：避免功能复杂、操作繁琐；</li><li>移动支持：满足销售人员 “在路上” 办公需求（如超兔的多端覆盖、销售易的移动 APP）。</li></ul><h3>3. 定制化：“能适配我的流程吗？”</h3><ul><li>避免功能冗余：选择贴合行业特性的系统（如超兔的工业类适配、八骏科技的 B2B/B2G 定制）；</li><li>支持灵活调整：满足企业不同发展阶段需求（如超兔的客制化引擎、纷享销客的低代码开发）。</li></ul><h3>4. 集成能力：“能和现有系统打通吗？”</h3><ul><li>比如：已有 ERP 系统（选用友 / 纷享销客）、需上下游协同（选超兔的 OpenCRM）、已有企微（选销售易 / 有赞）。</li></ul><h3>5. 数据安全：“我的客户数据安全吗？”</h3><ul><li>云架构：选择云端加密的 CRM（如超兔、销售易、有赞）；</li><li>权限管理：设置数据可见范围（如超兔的全局权限机制）；</li><li>合规性：满足等保三级、GDPR 等要求（如纷享销客）。</li></ul><h3>6. 服务支持：“出问题能找到人吗？”</h3><ul><li>本地化服务：覆盖全国的服务网络；</li><li>响应速度：选择响应及时的厂商（如超兔的优质服务、纷享销客的 2 小时响应）。</li></ul><h2>五、常见场景与 CRM 匹配表</h2><table><thead><tr><th>企业类型 / 场景</th><th>推荐 CRM</th><th>核心原因</th></tr></thead><tbody><tr><td>工业类 / 工贸类企业</td><td>超兔 CRM</td><td>全业务一体化、低成本客制化、上下游协同、行业适配性强</td></tr><tr><td>大中型制造企业</td><td>销售易、用友 CRM</td><td>行业定制、ERP 集成、全流程自动化</td></tr><tr><td>零售品牌（线上 + 线下）</td><td>有赞 CRM</td><td>导购赋能、全渠道整合、零售行业经验丰富</td></tr><tr><td>中小企业 / 初创企业</td><td>超兔 CRM、励销云</td><td>高性价比、灵活定制、快速部署、低成本切入</td></tr><tr><td>B2B/B2G 行业企业</td><td>八骏科技</td><td>行业经验、垂直解决方案、适配长周期销售模式</td></tr><tr><td>快消 / 医疗行业</td><td>纷享销客</td><td>行业适配、本地化支持、稳定性优</td></tr><tr><td>需全业务打通（含生产 / 进销存）的企业</td><td>超兔 CRM</td><td>大底座系统、业务数据底层连通、多模块覆盖</td></tr></tbody></table><h2>六、4 个常见问题解答</h2><h3>Q1：CRM 只是销售工具吗？</h3><p>不是。CRM 覆盖营销、销售、服务全流程：</p><ul><li>营销端：市场活动管理、自动化营销、线索培育（如超兔的多渠道集客、有赞的营销自动化）；</li><li>销售端：线索 - 商机 - 回款跟踪、销售预测（如超兔的跟单中心、销售易的全流程自动化）；</li><li>服务端：工单管理、投诉处理（如超兔的售后响应、纷享销客的服务模块）；</li><li>分析端：客户行为分析、销售漏斗分析（如超兔的数据分析引擎、销售易的大数据能力）。</li></ul><h3>Q2：中小企业适合用昂贵的 CRM 吗？</h3><p>不一定。中小企业可选择高性价比、高定制的 CRM：</p><ul><li>比如超兔 CRM（低成本客制化订阅、无冗余功能）、神州云动（灵活配置）、励销云（AI 线索挖掘、年费低）；</li><li>避免 “为冗余功能付费”（如不需要 ERP 集成就不用选用友 CRM）。</li></ul><h3>Q3：CRM 的数据安全吗？</h3><p>选对厂商就安全。需关注三点：</p><ul><li>云架构：选择云端加密的 CRM（如超兔、销售易、有赞）；</li><li>权限管理：设置 “数据可见范围”（如超兔的上级管理下级、同级隔离机制）；</li><li>合规性：选择符合等保三级、GDPR 的厂商（如纷享销客）。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524576" alt="" title="" loading="lazy"/></p><h3>Q4：CRM 能提升多少业绩？</h3><p>效果取决于使用深度。根据案例：</p><ul><li>某工业企业用超兔 CRM 后，流程效率提升 30%，客户复购率提升 25%；</li><li>某母婴品牌用 CRM 后，复购率提升 47%；</li><li>某家电品牌用 CRM 后，客诉率下降 30%；</li><li>某奶茶店用 CRM 后，单店营收提升 20%（通过精准营销）。</li></ul><h2>结语：CRM 的本质是 “以客户为中心” 的落地工具</h2><p>无论选择哪款 CRM，核心都是 “围绕客户需求”—— 用系统整合数据，用数据驱动决策，用决策提升体验。企业需避免 “为技术而技术”，而是让 CRM 成为 “连接企业与客户的桥梁”。</p><p>你的业务该配哪款 CRM？答案就在 “你的客户需求里”。</p><p>（注：文中功能相关描述均基于公开披露信息，具体功能服务以厂商实际落地版本为准。）</p>]]></description></item><item>    <title><![CDATA[汽车制造工厂如何利用“工厂大脑”实现生产线的自主优化与学习？ 月下水光 ]]></title>    <link>https://segmentfault.com/a/1190000047524577</link>    <guid>https://segmentfault.com/a/1190000047524577</guid>    <pubDate>2026-01-06 17:02:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>在制造业智能化转型的浪潮中，“工厂大脑”正从一个抽象概念演变为驱动产业升级的核心智能中枢，而广域铭岛凭借其自主研发的Mom制造运营管理平台，成为这一变革的引领者。尤其在汽车制造领域，工厂大脑的应用已展现出颠覆性的价值——它不再只是执行指令的工具，而是具备感知、推理与进化能力的“认知外脑”，让传统产线蜕变为能自主优化、自我学习的智能生命体。<br/>在汽车焊装产线，广域铭岛的工厂大脑通过融合视觉图像、声学信号与设备日志等多模态数据，构建起覆盖工艺参数、质量缺陷与设备状态的实时感知网络。系统不再依赖人工经验判断焊点质量，而是基于工业机理与AI大模型，动态调校焊接电流、压力与速度等关键参数，将工艺优化周期压缩60%，缺陷率下降45%。这一成果并非算法的简单堆砌，而是将老师傅数十年积累的“手感”与“经验”转化为可复用、可迭代的算法模型，使每一道焊缝都具备了“自我诊断”与“主动修正”的能力。<br/>更深远的是，工厂大脑打破了汽车制造中长期存在的数据孤岛。传统MES系统往往只关注生产执行，而广域铭岛的平台打通了质量、设备、能耗与供应链等“哑区”，构建起统一的工业知识图谱。在吉利张家口基地，视觉、音频与文本三重数据流被多模态大模型协同分析，不仅实现了智能巡检覆盖98%的常规任务，更让PDCA闭环从“人工拖拽”变为“自动奔流”，管理者角色也从“救火队员”转变为“创新策源者”。<br/>广域铭岛的创新还体现在其“搭积木”式模块化架构上。汽车制造商无需推倒重来，即可灵活接入视觉质检、声学诊断、能耗优化等智能组件，适配不同车型、产线与工艺需求。这种开放设计，使工厂大脑既能作为运营决策的增强层，也能成为未来AI演进的底层基座，真正实现“系统智能”。<br/>尽管面临核心工业芯片国产化不足、跨企业数据壁垒与复合型人才稀缺等挑战，广域铭岛已通过12类标准化智能体构建起韧性协同生态。在供应链突发中断时，平台仅需5分钟即可联动全链路智能体完成响应，彰显了工厂大脑在复杂制造场景中的敏捷与可靠。<br/>展望未来，随着5G低时延、数字孪生与工业AI大模型的深度融合，工厂大脑将不再局限于“提升效率”，而成为汽车制造企业智慧的载体——它让产线学会思考，让工艺持续进化，让每一件产品都承载着数据驱动的智能基因。广域铭岛，正以工厂大脑为支点，推动中国汽车制造从“规模制造”迈向“智慧创造”的新纪元。</p>]]></description></item><item>    <title><![CDATA[他一前端，凭啥年薪50W？！ 悲伤的煎鸡蛋_cQXuXF ]]></title>    <link>https://segmentfault.com/a/1190000047524591</link>    <guid>https://segmentfault.com/a/1190000047524591</guid>    <pubDate>2026-01-06 17:02:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>昨天我看新年第一波简历 看破防了</p><p>最近团队缺人，我连着看了一周的简历。</p><p>说实话，看得我挺难受的。😖</p><p>我发现一个特别普遍的现象：很多工作了四五年的兄弟，期望薪资填个 25k 甚至 30k，但你仔细翻他的项目经历，全是后台管理系统，全是 H5 拼图页面，全是表单增删改查。</p><p>你问他：这几年你遇到的最大技术难点是啥？🤔</p><p>他回你：表单字段太多了，校验逻辑太复杂。或者说，产品经理改需求太频繁。😖</p><p>听到这种回答，我心里大概就有了底：这兄弟的薪资上限，大概率锁死在 20W 以内了。</p><p>这就是咱们常说的 CRUD 困局。</p><p>你会 Vue，你会 React，你会用 Antd 画页面，你会调接口。兄弟，这些在 2018 年也许能让你拿高薪，但现在是 2026 年了，这些东西是基建，是培训班出来的应届生两个月就能上手的。🤣</p><p>那么问题来了，那个坐在你隔壁工位、平时话不多、但年薪能拿 50W 的大佬，他到底比你强在哪？</p><p>是他敲键盘比你快？还是他发量比你少？</p><p>都不是。</p><p>我觉得最核心的差距，就只有三点。听我细说。</p><h3>你在做填空，他在设计整张试卷</h3><p>这事儿特别明显。就拿新开一个项目来说。</p><p>15W 的兄弟是怎么干的？</p><p>找个脚手架，create-react-app 一把梭。然后开始堆页面，写组件。遇到要用的工具函数？去百度搜一个粘贴进来。遇到样式冲突？加个 !important 搞定。代码格式乱了？不管了，先跑通再说。</p><p>他的脑子里只有一个字：做。</p><p>50W 的兄弟是怎么干的？</p><p>他在写第一行业务代码之前，会先在脑子里过一遍这几件事：</p><p>大家代码风格不一样怎么办？先把 ESLint + Prettier + Husky 这一套流水线配好，谁提交的代码格式不对，连 git push 都推不上去。</p><p>这个项目以后会不会变大？要不要直接上 Monorepo 管理？</p><p>公共组件怎么抽离？是不是该搭个私有 npm 库？</p><p>打包速度怎么优化？Vite 的配置能不能再调调？</p><p>这就是差距。🤔</p><p>老板愿意给他 50W，不是因为他页面画得快，而是因为他制定了标准。他一个人，能让团队剩下 10 个人的产出质量变高。这叫工程化视野，这才是值钱的玩意儿。</p><p><strong>机会</strong></p><p>技术大厂，前端-后端-测试，全国各地等均有<a href="https://link.segmentfault.com/?enc=qOln%2FRTaJmpOyahnfEjNxQ%3D%3D.739ROewv0b81BJrTOG0olSZxUdU9YZVP6DyeYAsKqdw%3D" rel="nofollow" target="_blank">机-会</a>，感兴趣可以试试~</p><h3>出了事，你只会甩锅，他能兜底</h3><p><img width="723" height="383" referrerpolicy="no-referrer" src="/img/bVdnzts" alt="" title=""/><br/>场景再具体点：用户投诉页面卡顿，加载慢。</p><p>15W 的兄弟通常反应是这样的：</p><p>打开控制台 Network 看一眼。</p><p>哎呀，接口这就 800ms 了，这后端不行啊，锅在服务端。</p><p>嗨🙂‍↔️，这图片 UI 给得太大了，切图没切好。</p><p>这数据量几万条，浏览器渲染本来就慢，我也没办法！</p><p>总之，只要不是 JS 报错，这事儿就跟我没关系。</p><p>50W 的兄弟会干嘛？</p><p>他不会废话，他直接打开 Chrome 的 Performance 面板，像做外科手术一样分析。</p><p>这一段掉帧，是不是触发了强制重排？</p><p>内存这一路飙升，是不是哪个闭包没释放，或者 DOM 节点没销毁？</p><p>主线程卡死，是不是长任务阻塞了渲染？能不能开个 Web Worker 把计算挪出去？</p><p>网络慢，是不是 HTTP/2 的多路复用没吃满？关键资源的加载优先级设对了吗？</p><p>这就叫底层能力。🤔</p><p>平时写业务看不出来，一旦遇到高并发、大数据量、若网环境这种极端场景，只会调 API 的人两手一摊，而懂底层原理的人能从浏览器内核里抠出性能。</p><p>这种 兜底能力，就是你的溢价。</p><h3>他是业务合伙人!</h3><p>这点最扎心。</p><p>产品经理提了个不靠谱的需求，比如要在手机端展示一个几百列的超级大表格。</p><p>15W 的兄弟：</p><p>心里骂娘：这傻X产品，脑子有坑。😡🤬</p><p>嘴上老实：行吧，我尽量试试。</p><p>结果做出来卡得要死，体验极差，上线被用户骂，回来接着改，陷入无尽加班。</p><p>这种思维模式下，你就是个执行资源，也就是个 打工人。</p><p>50W 的兄弟：</p><p>他听完需求直接就怼回去了：</p><p>哥们，在手机上看几百列表格，用户眼睛不要了？你这个需求的业务目标是啥？是为了让用户核对数据？</p><p>如果是核对数据，那我们要不要换个方案，只展示关键指标，点击再下钻看详情？这样开发成本低了 80%，用户体验还好。</p><p>这就叫技术变现。</p><p>高端的前端，不仅仅是写代码的，他是懂技术的业务专家。他能用技术方案去纠正产品逻辑，帮公司省钱，帮业务赚钱。</p><p>在老板眼里，你是成本，他是投资。🤷‍♂️</p><h3>哪怕现在是 15W，咱也能翻盘~</h3><p>如果你看上面这些话觉得膝盖中了一箭，别慌。谁还不是从切图仔过来的？</p><p>想打破这个 CRUD 的怪圈，从明天上班开始，试着变一下：</p><p><strong>别再只盯着那几个 API 了</strong></p><p>Vue 文档背得再熟也就是个熟练工。去看看源码，看看人家是怎么设计响应式的，看看 React 为什么要搞 Fiber。懂了原理，你就不怕框架变。</p><p><strong>别做重复工作</strong></p><p>下次想复制粘贴工具函数的时候，停一下。试着自己封装一个通用的，甚至试着把你们项目里重复的逻辑抽成一个库。工程化就是这么一点点做起来的。</p><p><strong>钻进去一个细分领域</strong></p><p>别啥都学，啥都学不精。</p><p>可视化、低代码、Node.js 中间件、音视频，随便挑一个，把它钻透。在任何一个细分领域做到前 5%，你都有议价权。</p><p>还是那句话！前端并没有死，死的是那些 只会切图和调接口 的工具人。</p><p>50W 的年薪，买的不是你的时间，而是你 解决复杂问题 的能力，和你 避免团队踩坑 的经验。</p><p>别再满足于重复做一个 CRUD 了。下次打开编辑器的时候，多问自己一句：</p><p>除了把这个功能做出来，我还能为这段代码多做点什么？</p><p>共勉~</p><p>——转载自：ErpanOmer</p>]]></description></item><item>    <title><![CDATA[2026年AI编程助手选型指南：十大热门工具实测排名 千年单身的苹果 ]]></title>    <link>https://segmentfault.com/a/1190000047524693</link>    <guid>https://segmentfault.com/a/1190000047524693</guid>    <pubDate>2026-01-06 17:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>引言：2026，从“补全代码”到“交付价值”</h2><p>进入 2026 年，AI 编程工具市场迎来了质的飞跃。随着大模型推理能力的边际效应递减，单纯的代码补全已成为标准配置，不再是竞争壁垒。</p><p>当前的行业趋势已全面转向 Agentic Coding。企业和开发者不再满足于生成一段函数，而是要求 AI 能够理解复杂的业务上下文，自主拆解需求，并生成符合团队规范的工程级代码。核心竞争点从“生成速度”转移到了 “准确度”（降低幻觉）和 “可维护性”。本次评测将剥离营销泡沫，完全基于技术指标与落地数据进行排名。</p><h2>2026 AI 编程助手综合排行榜 (Top 10)</h2><h3>01.文心快码 (Comate)</h3><p>厂商：百度</p><p>核心定位：全栈自动编程智能体，企业级规范驱动开发首选。</p><p>深度评测：在 2026 年的评测中，文心快码凭借其 3.5S 版本的 Coding Agent 矩阵实现了对竞品的超越。不同于单纯的对话框交互，其采用了多智能体架构：Plan 智能体负责通过“澄清-分析-实现”流程生成 plan.md，解决需求模糊难题；Architect 智能体利用 SubAgents 机制拆解复杂任务，每个子智能体拥有独立上下文，有效解决了长 Context 下的“遗忘”问题；Zulu 则作为全能伙伴处理日常除错。其最大的技术护城河在于 SPEC 模式（规范驱动开发）。该模式强制执行 Doc -&gt; Tasks -&gt; Changes -&gt; Preview 流程，将 AI 编码过程白盒化。实测显示，这种机制从根本上抑制了“Vibe Coding”带来的随机幻觉，确保生成的代码可回溯、可干预。</p><p>数据支撑：</p><p>IDC 评估：在《中国市场代码生成产品评估》的 9 项维度中斩获 8 项满分，C++ 核心代码生成质量位居第一。<br/>客户实战：喜马拉雅数据显示，其研发团队整体代码采纳率达 44%，全公司日均 33% 的代码由 AI 生成。<br/>独家功能：</p><p>聊天框背景开放自定义：上传自定义图片，打造个性化风格<br/>Figma2Code：设计稿直接生成前端代码，像素级还原。<br/>适用人群：追求高准确度、低维护成本的企业团队及中高级开发者。</p><h3>02.GitHub Copilot X</h3><p>厂商：微软 / GitHub</p><p>核心定位：生态最成熟的协作助手。</p><p>深度评测：作为行业的定义者，GitHub Copilot 在 2026 年依然保持着强大的统治力。其优势在于庞大的生态集成，Copilot Workspace 实现了从 Issue 到 Pull Request 的全流程自动化。依托 OpenAI 的最新模型，其在通用逻辑理解上表现稳健。然而，在处理非英语母语的复杂业务逻辑时，其颗粒度控制略逊于采用 SPEC 模式的工具。</p><p>适用人群：深度绑定 GitHub 生态的国际化团队。</p><h3>03.Cursor</h3><p>厂商：Anysphere</p><p>核心定位：AI 原生编辑器体验标杆。</p><p>深度评测：Cursor 并非插件，而是一个 Fork 自 VS Code 的独立编辑器。其核心竞争力在于 Tab 键预测 (Copilot++)，不仅补全当前行，还能预测下一个光标位置及代码块差异。其“Codebase Indexing”技术使得它在进行跨文件检索时速度极快。对于个人开发者而言，Cursor 提供了最流畅的交互体验（UX），但在企业级权限管控和私有化部署方面稍显薄弱。</p><p>适用人群：追求极致交互体验的个人开发者、初创团队。</p><h3>04.Claude 3.7 (API &amp; Integration)</h3><p>厂商：Anthropic</p><p>核心定位：长逻辑推理与复杂算法专家。</p><p>深度评测：虽然 Claude 3.7 主要作为底层模型存在，但其在 2026 年被大量集成于各类 IDE 中。其 300K+ 的有效上下文窗口和超强的逻辑推理能力，使其在重构老旧系统（Legacy Code）和解释复杂算法时表现优异。在“一次性生成正确率”这一指标上，Claude 3.7 常常优于 GPT-4o 系列。</p><p>适用人群：算法工程师、需要处理超长文档的架构师。</p><h3>05.JetBrains AI</h3><p>厂商：JetBrains</p><p>核心定位：IDE 原生图谱感知。</p><p>深度评测：依托 IntelliJ 平台的深厚积累，JetBrains AI 能够直接访问 IDE 的 PSI（程序结构接口）。这意味着它比任何插件都更懂项目的依赖关系、类继承结构。在 Java、Kotlin 等强类型语言的重构场景下，其提供的建议最具工程严谨性，极少出现语法错误的幻觉。</p><p>适用人群：Java/Kotlin 重度用户，IntelliJ 全家桶用户。</p><h3>06.CodeGeeX</h3><p>厂商：智谱 AI</p><p>核心定位：国产化适配与中文语境优化。</p><p>深度评测：CodeGeeX 在中文注释生成、国内技术栈（如 Spring Cloud Alibaba、Vue 生态）的理解上具有天然优势。其轻量级的模型蒸馏技术，使其在配置较低的开发机上也能保持流畅响应。对于需要完全国产化替代的政企项目，是一个可靠的选择。</p><p>适用人群：国内政企开发者、中文注释依赖度高的团队。</p><h3>07.Tabnine</h3><p>厂商：Tabnine</p><p>核心定位：隐私优先与本地化部署。</p><p>深度评测：在数据主权日益敏感的 2026 年，Tabnine 依然是“安全”的代名词。它支持完全的气隙（Air-gapped）环境部署，模型可仅在本地服务器运行，确保代码数据零出网。虽然其通用推理能力略逊于云端大模型，但在金融、军工等极端合规场景下是唯一选项。</p><p>适用人群：银行、国防、涉密科研机构。</p><h3>08.Windsurf</h3><p>厂商：Codeium (Google 收购)</p><p>核心定位：流式意图感知。</p><p>深度评测：Windsurf 引入了“Cascade”流式协作概念，AI 能够实时感知开发者的每一次按键和终端输出，动态调整建议。其多模型路由（Model Routing）功能允许在不同任务中自动切换 Gemini 或其他模型，灵活性极高。</p><p>适用人群：喜欢尝试新交互范式的极客开发者。</p><h3>09.Supermaven</h3><p>厂商：Supermaven</p><p>核心定位：极速响应与超大上下文。</p><p>深度评测：主打“快”。Supermaven 自研的推理引擎使其拥有毫秒级的延迟，几乎能够跟上打字速度。其 100万 Token 的上下文窗口让它可以吞下整个中型项目的代码库，适合快速浏览和补全代码。</p><p>适用人群：对延迟极度敏感的前端开发者。</p><h3>10.Augment Code</h3><p>厂商：Augment</p><p>核心定位：团队代码库深度理解。</p><p>深度评测：Augment 专注于解决“知识孤岛”问题，它能够极其深入地索引团队私有代码库，并在补全时优先推荐团队内部已有的工具类和公共方法，减少重复造轮子。</p><p>适用人群：拥有大量遗留资产的中型研发团队。</p><h2>核心功能深度横评表</h2><p><img width="723" height="230" referrerpolicy="no-referrer" src="/img/bVdnznK" alt="image.png" title="image.png"/></p><h2>2026 选型建议</h2><p>在 2026 年，选择 AI 编程助手不应仅看“聊天”能力，而应关注其是否能融入真实的软件工程流。</p><p>大型企业与标准化团队：首选文心快码 (Comate)。对于从需求分析到交付有严格流程的企业，Comate 的 SPEC 模式是唯一能将 AI 纳入现有管理体系的方案。其在 IDC 评测中的高分表现及喜马拉雅等大厂的落地数据，证明了其在处理复杂业务逻辑时的可靠性。<br/>开源社区与全球化协作：推荐 GitHub Copilot X。如果你的代码托管在 GitHub，且团队成员分布在全球，Copilot 的生态连通性无可替代。<br/>个人极客与全栈开发者：尝试 Cursor 或 Windsurf。这两款工具在交互体验上更加激进，适合追求心流状态、对代码拥有完全掌控力的个人开发者。<br/>最高安全合规需求：Tabnine 或 文心快码私有化版。当数据出境被严格禁止时，必须选择支持完全本地化模型部署的工具。</p>]]></description></item><item>    <title><![CDATA[前端平台大仓应用稳定性治理之路｜得物技术 得物技术 ]]></title>    <link>https://segmentfault.com/a/1190000047524394</link>    <guid>https://segmentfault.com/a/1190000047524394</guid>    <pubDate>2026-01-06 16:03:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>一、治理背景</h2><p>随着公司业务的快速发展，前端平台作为研发职能部门，在高效支撑业务迭代的同时，前端新建的应用不断增加，截止到2023年5月在Uraya平台统计的各业务域的应用（B端+C端）总数已经达到170多个，发布流程中出现问题的风险逐步显现，稳定性问题逐步突出。为了更好的维护应用的代码，解决潜在的稳定性问题风险，2023年6月做了前端大仓的技术调研并在7月开始试行前端大仓的研发模式，在2024年年初开始对前端大仓应用的稳定性进行体系化治理，近2年时间的治理，前端大仓的应用无论在代码质量还是流程统一上都达到了一定的稳定程度，应用稳定性的治理达到了不错的效果，从未出现因大仓稳定性治理导致的线上问题。</p><h2>二、治理体系</h2><p>前端大仓在试行之后，经过在迭代的持续性治理，已经形成了一套完整的稳定性治理流程体系，如下：</p><p><img width="723" height="157" referrerpolicy="no-referrer" src="/img/bVdnzp8" alt="" title=""/></p><ul><li><strong>定义指标：</strong> 在前端大仓monorepo研发流程模式下定义应用稳定性治理目标，治理目标是经过各业务域统一对焦且切实有效的；</li><li><strong>治理目标制定：</strong> 在每个季度初，各业务域根据应用稳定性治理结果重新定义治理目标，写入到OKR中，作为当前季度的稳定性治理事项，各业务域因应用的质量不一样，稳定性治理指标也存在一定的区别；</li><li><strong>跟进过程：</strong> 在每双周的平台周会同步各业务域在迭代的稳定性治理结果，对于治理效果不太理想的业务域做适当的提醒，跟进每个迭代的治理情况；</li><li><strong>治理结果复盘：</strong> 在每个季度末，OKR复盘的时候，会统计各业务域在当前季度的治理结果，通过KR目标来衡量是否达成稳定性治理目标。</li></ul><p>通过<strong>定义指标 -&gt; 治理目标制定 -&gt; 跟进过程 -&gt; 治理结果复盘</strong>不断的迭代循环治理，形成一个闭环，且各业务域也在不断的调整治理目标，直到最终达成平台的治理目标，使得大仓各应用的稳定性的治理都能达到不错的效果。</p><h2>三、治理指标</h2><p>截止目前，前端大仓的<strong>应用已达200多个，代码行数已经达到550多万行，</strong> 如何提升如此体量的代码质量和应用稳定性是一个相对比较有挑战的事情。经过早期半年的试行，基于大仓代码标准化以及研发流程标准化的建设，逐步形成了5大可衡量的治理指标：Git元数据的大小、代码质量分、研发流程卡点、Lint error质量分、应用代码重复率，如下：</p><ul><li><strong>Git元数据的大小：</strong> 随着每个迭代各业务域代码行数的增加以及git记录的提交，大仓的Git元数据会不断增加，当增加到一定程度的时候，会对本地git命令操作、MR变更以及代码的回合产生影响，进而影响应用发布的稳定性。对Git元数据大小进行治理，能够直接提升研发效率以及应用发布的稳定性；</li><li><strong>代码质量分：</strong> 对应用代码的质量通过不同的可衡量指标进行积分汇总成代码质量分，主要包括大文件、函数复杂度、HTTPS检测、敏感词检测、安全检测、前端运算和魔数这些指标得分来体现应用代码的质量。这些指标治理得好，代码质量分就越高，应用的稳定性也就越好；</li><li><strong>Lint error质量分：</strong> Lint error是前端代码标准规范的重要衡量指标，在大仓下的应用代码有统一的lint检测规范。对研发每次提交的代码进行Lint规范检测，获取不同的质量分，通过不同的分数区间来衡量Lint error质量分，质量分越高，应用的代码规范越好，应用的稳定性也就越好；</li><li><strong>研发流程卡点：</strong> 在应用代码MR阶段和构建发布流程中，研发流程卡点至关重要，主要包括强卡和弱卡。比如对lint标准规范的检测、变更文件权限校验、分支名称检测及合法性校验等进行卡点，当出现卡点的时候，通过强卡和弱卡的手段来提示研发问题的风险，避免了一些不规范操作带来的线上问题，提升了应用构建发布的稳定性；</li><li><strong>应用代码重复率：</strong> 代码重复率是体现大仓应用代码可复用的重要衡量指标，代码的复用性越好，代码的重复率就越低，可复用的代码就越稳定，进而提升应用代码的稳定性。</li></ul><p>大仓应用的稳定性基本上都是围绕上面5个指标来进行治理的，在逐步推进治理的过程中，大仓应用的代码稳定性也在不断的提升，当达到一定程度的时候，各应用的稳定性也会达到一定的程度趋于平稳。</p><h2>四、治理成效</h2><p>基于大仓代码的标准规范以及统一的研发发布流程，且在每个季度持续推进治理下，各业务域的治理指标都有显著的提升，进而提升了前端平台大仓应用整体的稳定性。具体成效如下：</p><ul><li>自从前端大仓试行以来，依托统一的研发构建发布流程，<strong>大仓应用从未出现过线上冒烟点和故障；</strong></li><li>通过对Git元数据的大小进行性能优化，<strong>将原来大仓800M+的元数据大小减少到平均各业务域Git元数据大小60M以下，</strong> 提升了本地Git命令操作的效率，使得MR变更和代码回合更加的清晰，提升了应用发布流程的稳定性；</li><li>大仓应用代码的质量分<strong>从最初的74分左右提升到目前的85分以上，</strong> 极大的提升了应用代码的质量，提升了应用线上功能的稳定性；</li><li>Lint error的质量分<strong>从最初的平均10分左右提升到目前的13分以上，</strong> 促进了各业务域应用代码标准规范的统一，不仅提升了大仓应用代码的质量和稳定性，还提升了平台轮岗、借调研发的编码效率；</li><li>研发流程卡点在构建发布和MR阶段上线以后，截止到目前为止，<strong>强卡次数1200多次，弱卡次数2万多次，成功避免出现线上问题隐患130次左右，</strong> 提升了应用发布的稳定性；</li><li>代码重复率<strong>从最初的12.5%左右降低到目前的8%以下，</strong> 在提升代码复用的同时，也提升了整体大仓应用代码的可维护性和稳定性。</li></ul><h2>五、治理事项</h2><h3>Git元数据性能优化</h3><p>前端大仓自试行之后，Git元数据就在持续的递增，截止到2024年年底，Git元数据的大小已经接近1G，本地的部分Git命令执行时间超过5秒，MR变更及代码回合经常被非当前业务变更的文件困扰，影响了大仓应用发布的稳定性。为了解决这些性能问题，对Git元数据的大小做了性能优化，主要事项包括：</p><p><img width="703" height="132" referrerpolicy="no-referrer" src="/img/bVdnzp9" alt="" title="" loading="lazy"/></p><ul><li>对Git clone命令做了二次封装，利用其实现本地缓存，二次clone时间至少减少90%左右的时间；</li><li>利用Git sparse-checkout稀疏检出的能力，将原来首次几分钟的clone时间减少到10秒以内；</li><li>通过动态化技术拆分大仓的元数据，将原来近1G的元数据减少至平均单个业务域60M以下。</li></ul><p>通过上面的技术实现，彻底解决了Git元数据持续递增的性能问题，使得MR阶段的代码CR更加的清晰，避免了因过多代码提交记录带来的CR不清晰、回合代码不清晰导致出现线上问题的风险，提升了应用发布的稳定性。</p><h3>代码质量分的统计</h3><p>应用代码质量分是衡量应用代码质量的重要指标，其中主要包含大文件、函数复杂度、HTTPS检测、敏感词检测、安全检测、前端运算和魔数这些指标得分来体现应用代码的质量，基于Uraya平台的规则统计逻辑，每个迭代都会对应用的代码进行扫描并做质量分的统计，如下：</p><p><img width="723" height="323" referrerpolicy="no-referrer" src="/img/bVdnzqa" alt="" title="" loading="lazy"/></p><p>同时也可以查看应用质量分各维度指标的得分情况，具体详情信息如下：</p><p><img width="723" height="400" referrerpolicy="no-referrer" src="/img/bVdnzqb" alt="" title="" loading="lazy"/></p><p>每个季度初期会根据不同业务域应用的质量情况来制定当前季度可达成的质量分目标，并且在季度末以此目标来最终复盘应用质量分的治理情况，如下是2025年Q3季度的整体质量分治理情况：</p><p><img width="723" height="352" referrerpolicy="no-referrer" src="/img/bVdnzqc" alt="" title="" loading="lazy"/></p><p>应用质量分的治理有一个标准线，高于80分的应用说明代码质量已经比较好了，后续再投入时间治理的话，ROI不高，故应用质量分超过80分的业务域常态化治理即可，不用专门花时间去做治理。在Q3结束之后，前端平台所有的业务域基本都达成了80分的标准线。</p><h3>Lint标准规范的统一</h3><p>目前前端大仓已经集成了上百个应用，很多应用都有各自的Lint规则配置以及代码规范配置，特别是在早期基于样板间创建的应用，这个现象尤为明显。在大仓里面，如果每个应用还是按照各自的规范去开发的话，那么当研发轮岗或者各域之间互相借调的时候，因代码风格的不一致带来的熟悉上手成本、IDE规则配置成本等这些都会比较高，且研发效率低下，这跟之前单个应用仓库开发没什么区别。因此对大仓下所有应用的代码规范做了统一，研发编写的代码都需要符合标准规范，这样不仅提升了应用代码的稳定性，也提升了平台轮岗、借调研发的编码效率。主要的代码标准规范如下：</p><ul><li>TS标准规范（TypeScript语言）：@xxxxx/ts-config/base.json</li><li>eslint标准规范（JavaScript语言）：@xxxxx/eslint-config</li><li>stylelint标准规范（CSS样式）：@xxxxx/stylelint-config</li><li>.prettierrc标准规范（代码格式化）&amp; VSCode编辑器代码格式化配置</li></ul><p>在大仓中有顶层目录的基本规范、应用目录下的代码规范以及不同技术栈的代码规范，其关系如下：</p><p><img width="723" height="353" referrerpolicy="no-referrer" src="/img/bVdnzqe" alt="" title="" loading="lazy"/></p><p>在研发本地提交代码的时候，会触发git钩子函数对变更文件的代码进行校验，确保提交的代码是符合标准规范的，并且对Lint error质量分定义了治理分记分规则，总分16.12分，代码提交的error错误数各区间得分如下：</p><ul><li>0～100（包含100）个error错误： 得4.12分</li><li>100～300（包含300）个error错误：得2.8分</li><li>300～600（包含600）个error错误：得2.8分</li><li>600～800（包含800）个error错误：得1.2分</li><li>800～1000（包含1000）个error错误：得4分</li><li>1000-2000 (包含2000）个error错误：得1.2分</li><li>2000个以上error错误：得0分</li></ul><p>在每双周的平台周会上进行治理情况的同步，同时在季度末复盘当前季度的整体达成情况，如下是2025年Q3季度各业务域的lint质量分治理情况：</p><p><img width="723" height="344" referrerpolicy="no-referrer" src="/img/bVdnzqf" alt="" title="" loading="lazy"/></p><p>Lint error质量分的治理也有一个标准线，高于9分的应用说明代码标准规范已经比较好了，再专门投入时间治理的话，ROI不高，故应用的Lint error质量分超过9分的业务域都是常态化治理即可。在Q3结束的时候，前端平台所有的业务域都达成了9分的标准线。</p><h3>研发流程卡点的建设</h3><p>为了避免研发本地的一些不规范流程操作带来的线上稳定性问题，在应用测试环境的构建发布流程新增流程卡点，如下所示：</p><p><img width="723" height="210" referrerpolicy="no-referrer" src="/img/bVdnzqg" alt="" title="" loading="lazy"/><br/><img width="723" height="371" referrerpolicy="no-referrer" src="/img/bVdnzqh" alt="" title="" loading="lazy"/></p><p>同时在构建发布流程中保留研发流程卡点的情况下，在MR阶段也新增了质量分的卡点，只要检测出有强卡的情况下，就不允许合并到release分支，确保了合入代码的质量和标准规范，如下图所示：</p><p><img width="723" height="309" referrerpolicy="no-referrer" src="/img/bVdnzqi" alt="" title="" loading="lazy"/><br/><img width="723" height="136" referrerpolicy="no-referrer" src="/img/bVdnzqj" alt="" title="" loading="lazy"/></p><p>基于大仓应用的研发流程，主要有以下流程卡点：</p><ul><li>【弱卡】分支名称检测及合法性校验</li><li>【弱卡】Lint标准规范检测</li><li>【强卡】变更文件权限校验</li><li>【强卡】分支变更与对应应用是否匹配</li><li>【强卡】分支变更是否存在多个业务域或者多个应用的修改</li><li>【强卡】分支变更是否存在不允许修改的文件夹</li></ul><p>通过研发流程的强卡和弱卡进一步规范研发流程的操作，截止到目前为止，强卡次数1200多次，弱卡次数2万多次，成功避免出现线上问题隐患130次左右，提升了应用发布的稳定性。</p><h3>代码重复率的统计</h3><p>代码重复率是衡量大仓代码可复用的重要指标，也是平台侧一直推进的治理事项，前期代码重复率的统计都依赖于研发本地跑脚本看数据，每个迭代结束才清楚治理效果。为了便于研发在本地能够实时的查看代码质量分的治理结果，提供了VSCode插件来实时统计结果，如下功能所示：</p><p><img width="723" height="372" referrerpolicy="no-referrer" src="/img/bVdnzqk" alt="" title="" loading="lazy"/></p><p>研发通过点击上面的治理小助手，就能实时查看当前分支的治理效果，极大的提升了治理效率。同样在每双周也会进行代码重复率的同步，在季度末会进行治理目标的复盘，如下是2025年Q3季度各业务域的代码重复率治理情况：</p><p><img width="723" height="352" referrerpolicy="no-referrer" src="/img/bVdnzql" alt="" title="" loading="lazy"/></p><p>代码重复率的治理也有一个标准线，低于6%的应用说明代码复用已经比较好了，再专门投入时间治理的话，ROI不高，故应用的代码重复率低于6%的业务域都是常态化治理即可。在Q3结束的时候，前端平台部分业务域已经达成了低于6%的标准线。</p><h2>六、治理总结</h2><p>前端平台通过试行大仓的研发模式，系统性地开展了应用的稳定性治理工作。自2023年7月试行、2024年初体系化推进以来，围绕五大核心指标--Git元数据大小、代码质量分、Linterror质量分、研发流程卡点和代码重复率，构建了“定义指标→制定目标→过程跟进→结果复盘”的闭环治理体系。通过统一代码规范、优化Git元数据性能、强化流程发布卡点、提升代码复用等举措，显著提升了大仓应用整体的稳定性。截至2025年Q3，各业务域普遍达成质量标准线，<strong>大仓应用从未发生因治理导致的线上故障，实现了高效、稳定、可持续的前端大仓应用研发稳定性治理体系。</strong> 随着目前大模型的不断迭代，后续结合AI智能体对研发流程进行稳定性加固，相信大仓应用的稳定性会更上一个台阶。</p><h3>往期回顾</h3><p>1.RocketMQ高性能揭秘：承载万亿级流量的架构奥秘｜得物技术</p><p>2.PAG在得物社区S级活动的落地 </p><p>3.Ant Design 6.0 尝鲜：上手现代化组件开发｜得物技术</p><p>4.Java 设计模式：原理、框架应用与实战全解析｜得物技术</p><p>5.Go语言在高并发高可用系统中的实践与解决方案｜得物技术</p><h3>文 /玉润</h3><p>关注得物技术，每周更新技术干货</p><p>要是觉得文章对你有帮助的话，欢迎评论转发点赞～</p><p>未经得物技术许可严禁转载，否则依法追究法律责任。</p>]]></description></item><item>    <title><![CDATA[公司正在裁员，为什么有人要主动离职连赔偿都不要？ 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047524443</link>    <guid>https://segmentfault.com/a/1190000047524443</guid>    <pubDate>2026-01-06 16:02:47</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>最近有个特别魔幻的现象：公司明明在裁员，HR都开始约谈了，结果有些人居然主动提离职，连N+1的赔偿都不要了。</p><p>这事儿传到我耳朵里，我第一反应是——疯了吗？</p><p>但仔细一想，这背后的水，深得很。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524445" alt="" title=""/></p><h2>被"自愿"的离职</h2><p>很多人以为主动离职是真的主动，其实是被逼到墙角了。</p><p>公司玩的套路你懂的，明明想裁你，但又不想给赔偿，怎么办？</p><p>开始给你穿小鞋呗。今天把你的项目抽走，明天让你去坐冷板凳，后天直接把你调到八竿子打不着的部门。</p><p>有些更狠的，直接给你安排根本完不成的KPI，或者天天找你谈话做"思想工作"，软磨硬泡地暗示你"主动点对大家都好"。</p><p>这种花式裁员的操作，说白了就是让你待不下去，自己滚蛋。</p><p>你要是真签了那张辞职申请，恭喜你，公司一分钱都不用赔。</p><p>因为从法律上看，这是你自己要走的，跟公司没关系。</p><p>很多人扛不住这种精神折磨，就真的签了字，然后才发现自己被坑了。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524446" alt="" title="" loading="lazy"/></p><h2>打官司？输不起的时间成本</h2><p>有人会说，那我不签字，跟公司死磕到底，打劳动仲裁啊。</p><p>理论上没错，但现实是——你耗得起吗？</p><p>劳动仲裁走下来，快的话三四个月，慢的话大半年甚至一年。</p><p>这期间你没收入，房贷车贷要还，孩子奶粉钱要出，父母看病要花钱。</p><p>更要命的是，你的简历上会出现一段空白期，新公司HR一看就会问："你这段时间在干嘛？"你说在打官司？对不起，下一位。</p><p>而且就算你赢了官司，拿到了赔偿，这笔钱可能还不够你这几个月的生活开销加上找工作的机会成本。</p><p>更别提打官司的精神内耗了，天天想着这事儿，整个人状态都不对，面试都过不了。</p><p>所以很多人算完这笔账，就认了，主动离职走人，至少能早点开始找下家。</p><h2>背调这道鬼门关</h2><p>还有个更隐蔽的原因——背调。</p><p>现在稍微像样点的公司，入职前都要做背调。</p><p>如果你是被裁的，HR打电话到你前公司一问，对方可能会说"因业务调整协商解除劳动合同"，听起来还算体面。</p><p>但如果你跟公司闹僵了，对簿公堂，你猜前公司会怎么说？</p><p>"这个人啊，工作态度有问题，跟公司有劳动纠纷，对簿公堂的。"</p><p>虽然你赢了官司证明自己有理，但新公司HR才不管这些，他们只看到一个标签——"爱打官司的刺头"。</p><p>在HR眼里，这种人就是定时炸弹，谁敢要？</p><p>所以很多人为了背调能好看点，为了不在行业里留下"难搞"的名声，宁可吃点亏，主动离职，换一份还算说得过去的离职证明。</p><p>这就是职场的潜规则，你不服也得服。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524447" alt="" title="" loading="lazy"/></p><h2>行业寒冬下的无奈</h2><p>说到底，这些选择背后都是无奈。</p><p>如果市场好，工作好找，谁会在乎那点时间成本？</p><p>如果法律执行到位，公司不敢玩这些花招，谁会被逼着主动离职？</p><p>如果社会氛围对劳动者友好一点，谁会怕背调怕成这样？</p><p>但现实就是这么残酷。</p><p>经济下行，岗位变少，竞争变激烈，打工人的议价权越来越低。</p><p>公司知道你没得选，所以才敢这么玩。</p><p>而你也知道自己没得选，所以只能忍气吞声，主动离职，连赔偿都不敢要。</p><p>这就是职场最魔幻的地方——明明是受害者，却要装成自愿者；明明有权利，却不敢去争取；明明被坑了，还要笑着说"谢谢"。</p><p>所以下次你再看到有人裁员时主动离职不要赔偿，别急着笑他傻。</p><p>可能他只是比你更早看清了这个游戏的规则，也可能他只是比你更累，更想逃离这个困局。</p><p>在这个人人自危的职场里，没有谁比谁更高明，只有谁比谁更无奈。</p>]]></description></item><item>    <title><![CDATA[Codigger研发日志：打造分布式数字工作平台，邀开发者共同见证成长 codigger ]]></title>    <link>https://segmentfault.com/a/1190000047524465</link>    <guid>https://segmentfault.com/a/1190000047524465</guid>    <pubDate>2026-01-06 16:02:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我们是Codigger开发团队！今天想和各位开发者朋友聊聊我们正在全力打磨的项目——Codigger分布式数字工作平台。从最初的想法雏形到现在的核心模块研发，我们始终围绕“解决开发痛点、提升协作效率”的目标推进，现在把项目的核心思路和最新进展分享给大家，也期待能收到更多宝贵的建议。<br/>研发初衷：从开发者痛点出发，定义分布式工作新形态<br/>在日常开发和协作中，我们和很多同行一样，遇到过不少困扰：本地环境配置繁琐且易出错、远程团队协作进度不同步、跨云平台部署适配成本高，还有很多优质的技术工具难以实现高效分发与变现。于是我们萌生了一个想法：打造一个以分布式架构为核心，能覆盖“开发-调试-部署-协作-变现”全流程的数字工作平台，这就是Codigger的由来。<br/>目前，Codigger仍处于核心模块研发阶段，我们的核心定位很明确——不止是简单的云IDE，而是要做连接开发者、团队与企业的“分布式数字工作枢纽”，让技术开发更顺畅，技术价值更易落地。<br/><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnzqM" alt="image.png" title="image.png"/><br/>核心模块研发进展：已落地这些关键功能雏形<br/>围绕最初的设计目标，我们目前已完成多个核心模块的原型开发，正在进行内部测试与优化：</p><ol><li>分布式云端开发环境：已实现多语言适配的基础框架，支持Java、Python、前端等主流技术栈，同时兼容我们自研的OSE、SONG轻量开发语言（目前正在优化语法适配与性能）。核心优势是“零配置上手”，开发者无需本地搭建环境，通过浏览器即可快速接入。</li><li>实时协作核心组件：完成了多人协同编码、项目资源可视化管理的基础功能开发，正在对接内置的分布式视频会议、文档协作模块，后续将实现“编码-沟通-文档”的无缝衔接，解决远程团队协作的高频痛点。</li><li>多云部署适配框架：已完成与主流云平台的初步对接，正在优化部署流程的自动化程度，目标是实现“开发完成后一键部署”，大幅降低跨平台适配成本。</li><li><img width="723" height="433" referrerpolicy="no-referrer" src="/img/bVdnzqN" alt="image.png" title="image.png" loading="lazy"/><br/>生态体系规划：以应用商店为核心，构建价值共创圈<br/>在研发核心功能的同时，我们也在规划Codigger的生态体系，其中应用商店（Store）是核心环节——我们希望它能成为“开发者技术变现的渠道”和“用户按需获取功能的中心”：<br/>未来商店将聚合开发插件、代码模板、调试工具、桌面主题、企业定制化模块等多种资源。开发者可以把自己的技术成果（如适配SIDE的插件）上架变现，我们计划设计友好的收益分配机制（初步规划开发者可获大部分收益）；用户则能按需获取资源，快速补充工具链。此外，我们还将配套搭建技术社区和学习资源库，形成“开发-分享-变现”的良性循环。<br/>目标受众：聚焦开发者与协作型团队，精准匹配需求<br/>从研发之初，我们就明确了Codigger的服务对象：核心是全栈开发者、独立开发者、编程初学者等各类从业者；同时也将适配有远程协作需求的企业、敏捷开发项目组，帮助团队简化管理、提升效率；后续还将针对编程教育培训机构，开发适配教学场景的功能，降低学员的入门门槛。<br/>写在最后：邀你一起参与产品打磨<br/>作为一群深耕开发领域的程序员，我们深知一款好的开发工具，离不开真实用户的反馈。目前Codigger仍在紧张研发中，核心模块已具备初步形态，后续我们会持续更新研发日志，分享技术细节与迭代进展。<br/>如果您对“分布式开发平台”有自己的痛点和期待，或者想了解某个功能的研发思路，欢迎在评论区和我们交流！如果愿意参与后续的内测，也可以私信我们留下联系方式，您的每一个建议，都将帮助我们把Codigger打磨得更贴合开发者需求。<br/>后续我们会在社区持续更新研发动态，敬请关注！<br/>—— Codigger开发团队 敬上</li></ol>]]></description></item><item>    <title><![CDATA[2026年值得关注的8个叫好不叫座的实用工具 烦恼的沙发 ]]></title>    <link>https://segmentfault.com/a/1190000047524528</link>    <guid>https://segmentfault.com/a/1190000047524528</guid>    <pubDate>2026-01-06 16:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>自从有了AI加持，天天都有新应用发布，各种云原生、颠覆式创新，概念一个比一个响亮。但冷静下来，你会发现日常开发中，真正用得顺手的，还是那些踏踏实实解决问题的东西。</p><p>我们每天都在被各种网红刷屏，比如 VS Code 的新插件、JetBrains 的全家桶更新。但聚光灯之外，有很多工具虽然没什么热度，却能解决我们开发中的具体痛点。</p><p>今天就来聊聊我私藏的几款工具，它们可能没那么出名，但谁用谁知道。</p><h4><a href="https://link.segmentfault.com/?enc=gpTTGvG5jpJf1kH%2B4FvuSg%3D%3D.fII1AL4qYztFd8H9UEhtOX7910OYM%2FE7ghhSsXwcIkY%3D" rel="nofollow" target="_blank">ServBay</a> - 本地开发环境管理的基石</h4><p>折腾本地开发环境，绝对是程序员最头疼的事情之一。</p><p>早年的 XAMPP/MAMP 之流已经跟不上时代，配置自由度太低。大家也都扑向 Docker 的怀抱，它确实强大，但对于只想在本地快速跑个项目、调试个脚本的场景来说，又显得有些重。写 Dockerfile、管理容器、处理网络问题，特别对于新手，分分钟就破防了</p><p>天空一声巨响，ServBay闪亮登场。</p><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnzsY" alt="image.png" title="image.png"/></p><p>它不是一个简单的集成环境，更像一个<a href="https://link.segmentfault.com/?enc=WDUILlO3JOK3ixVPhuNRGg%3D%3D.qjk1RbPd3gTtrKz6Xu9JzKTS1yyId%2Frt83nq4JZ41vbl6NalmU%2BtLWiL%2BNmaoc7e" rel="nofollow" target="_blank">本地开发服务</a>的调度中心，它的功能齐全又实用。</p><p><strong>多语言、多版本共存与一键切换</strong></p><ul><li>这是 ServBay 最厉害的地方，像点菜一样安装不同版本的 PHP、Node.js、Java、Python、Go 甚至 Rust。比如，你需要同时维护一个用 PHP 5.6 的老项目和一个用 PHP 8.2 的新项目，用 ServBay 就能让它们在本地同时运行，互不干扰。再也不用为了切换环境而修改系统变量或者使用复杂的版本管理工具了。</li></ul><p><strong>常用服务一键安装</strong></p><ul><li>除了编程语言，数据库和缓存服务也是本地开发的标配。ServBay 支持一键安装 MySQL、MariaDB、PostgreSQL 这类 SQL 数据库，也支持 Redis、MongoDB、Memcached 这类 NoSQL 工具。尤其是 <strong><a href="https://link.segmentfault.com/?enc=A5tBxI5xDBPVkp99PMPANg%3D%3D.QJHStDwCOMEZDOboqYMjgN5rl8tzP5QVBqZUlnYafLCnRMDm2wd3yzs22oDoHksj" rel="nofollow" target="_blank">一键安装 Redis</a></strong> 这个功能，省去了自己编译或用 Docker 拉取镜像的麻烦，点一下鼠标，几秒钟就能用上。还支持本地AI部署，非常适合 Vibe Coding。</li></ul><p><img width="723" height="458" referrerpolicy="no-referrer" src="/img/bVdnzsZ" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>性能和资源占用表现不错</strong></p><ul><li>它基于 Caddy 服务器，并为各个服务做了性能优化，启动速度很快，资源占用也比跑一堆 Docker 容器要小，能省点电和风扇噪音总是好的。</li></ul><p>总的来说，不想要自己配本地开发环境，又觉得 Docker 在某些场景下小题大做，ServBay 提供了一个恰到好处的解决方案。</p><h4><a href="https://link.segmentfault.com/?enc=uA6SgBasni1Hp9Z%2BFehhFg%3D%3D.cxbkwCsVI3%2BAQohd8RfA8coVnOeQjhSe8kNn1kP6BIo%3D" rel="nofollow" target="_blank">Zed </a>- 快到飞起的代码编辑器</h4><p>VS Code 好用吗？当然好用，生态强大，功能全面。但它的问题也越来越明显——慢。随着插件越装越多，项目文件越来越大，经常会感觉到明显的卡顿和延迟。</p><p><strong>Zed</strong> 是前 Atom 编辑器团队打造的新作品，它的核心卖点就一个字：快。</p><p><img width="723" height="465" referrerpolicy="no-referrer" src="/img/bVdnzs0" alt="image.png" title="image.png" loading="lazy"/></p><p>打开一个几百兆的大文件，Zed 几乎是秒开，而 VS Code 可能已经开始转圈了。它的界面响应、代码渲染、文件搜索速度也是杠杠的。这得益于它底层使用 Rust 编写，并在 GPU 加速上下了很大功夫。</p><p>当然，Zed 目前的生态还比不上 VS Code，插件数量有限。但对于追求极致性能、喜欢纯粹编码体验的开发者来说，它绝对值得一试。特别是做前端开发，或者需要频繁处理大文件的场景，Zed 的速度优势会让你印象深刻。</p><h4><a href="https://link.segmentfault.com/?enc=%2Bl3XxUZtDH1Jds16OHAF3w%3D%3D.pF81ljhGJYFt61bo2l8Ms21il06BMcLN%2FoSQFXu%2B6Fs%3D" rel="nofollow" target="_blank">GitButler</a> - 让 Git 操作更符合直觉</h4><p>Git 很强大，但它的很多概念（比如 <code>stash</code>、<code>rebase</code>）对不少人来说依然很绕。我们常常只是想把这几行代码存成一个提交，却要在不同的分支之间来回切换。</p><p><strong>GitButler</strong> 提供了一种全新的思路。它支持在工作区内同时处理多个不相关的任务，然后把属于同一个任务的代码变更（哪怕分布在不同文件里）打包成一个虚拟分支，最后再把这个虚拟分支变成一个真正的 Git 提交。</p><p><img width="723" height="423" referrerpolicy="no-referrer" src="/img/bVdnzs1" alt="image.png" title="image.png" loading="lazy"/></p><p>这个过程是可视化的，只需要拖拽代码块，就能完成变更的组织。它弱化了传统分支的束缚，开发者能更专注于项目的本身，而不是想着选分支。对于经常需要同时修复 bug 和开发新功能的开发者来说，这个工具能极大地降低 Git 操作的复杂度。</p><h4><a href="https://link.segmentfault.com/?enc=6CcphqZjf3x83GtBsKTDgg%3D%3D.UGT3LqryKTDPLq9%2F14PMxZvcfMxcPxd0iT4YiTRjX6s%3D" rel="nofollow" target="_blank">Focalboard</a> - 开源、可自托管的项目管理看板</h4><p>团队协作离不开项目管理工具。Trello、Jira、Notion 都是不错的选择，但它们要么价格不菲，要么对于小团队来说过于复杂。而且，这相当于所有项目数据都存在别人的服务器上。</p><p><img width="723" height="485" referrerpolicy="no-referrer" src="/img/bVdnzs2" alt="image.png" title="image.png" loading="lazy"/></p><p><strong>Focalboard</strong> 是一个开源的替代品，你可以把它看作是 Trello 和 Notion 的结合体。它提供了看板、表格、日历等多种视图，功能足够日常使用。最吸引人的一点是，你可以轻松地将它自托管在自己的服务器上，数据完全由自己掌控，安全又放心。</p><h4><a href="https://link.segmentfault.com/?enc=dNGm3Px7P3VQS75lbHUXkg%3D%3D.ELhdTmFrGay3BChw%2B6%2BVcP%2BekF%2FlhBQ0nmeWTxbQ9Js%3D" rel="nofollow" target="_blank">Sentry</a> - 在用户抱怨前发现代码错误</h4><p>“我这里报错了，页面打不开”，是不是听到这句话，你都要PTSD了。</p><p>别担心，<strong>Sentry</strong>来了。Sentry 是一个错误追踪系统，它能在项目（无论是前端还是后端）发生异常时，第一时间捕获错误信息，包括详细的堆栈、用户操作路径和设备环境，然后发出通知。</p><p><img width="723" height="406" referrerpolicy="no-referrer" src="/img/bVdnzs3" alt="image.png" title="image.png" loading="lazy"/></p><p>这样就能在用户发现问题之前，主动介入并修复。它支持几乎所有主流的编程语言和框架，接入也相当简单。对于任何一个线上项目来说，Sentry 都是保障稳定性的重要一环。</p><h4><a href="https://link.segmentfault.com/?enc=ORBbuhmmtLrLF6xcbnFNUw%3D%3D.BFENAIPL0nTT1mrgx9yZtfmh0EbCGx8R4f3g5ywbh5s%3D" rel="nofollow" target="_blank">Datadog</a> - 面向全栈的监控平台</h4><p>如果说 Sentry 专注于错误，那 <strong>Datadog</strong> 则关心一切，是个贴心的小棉袄。它是一个全栈监控平台，能整合系统的日志、指标（Metrics）、链路追踪（APM）等所有可观测性数据。</p><p><img width="723" height="371" referrerpolicy="no-referrer" src="/img/bVdnzs4" alt="image.png" title="image.png" loading="lazy"/></p><p>可以用它来监控服务器的 CPU 使用率、查看数据库的慢查询、追踪一个 API 请求在微服务之间的完整路径。它能把原本分散在各个地方的监控信息汇集在一起，开发者对整个系统的健康状况有一个全局的了解。如果说它有什么毛病的话，那就是贵，但对于维护复杂系统的团队来说，它提供的价值是巨大的。</p><h4><a href="https://link.segmentfault.com/?enc=JloeVIBMQzEZAoFfVC8b3g%3D%3D.gGbfJVweWeROv73DpNx12zRmNauoCG1N6O7J7MwR2ww%3D" rel="nofollow" target="_blank">Subframe</a> &amp; <a href="https://link.segmentfault.com/?enc=8gWIhkgg9gf4BWyIUTd7iw%3D%3D.jokrF3ygyf%2BtPBgSkueN%2FpOLBiHnc91Cg59yYUhStLA%3D" rel="nofollow" target="_blank">FlatIcon</a> - 为开发提效的设计资源</h4><p>最后推荐两个非典型开发工具，但对全栈或前端开发者尤其有用。</p><ul><li><strong>Subframe</strong>: 一个用 AI 辅助生成 UI 组件的工具。只需要用文字描述你想要的组件（比如“一个包含头像、用户名和关注按钮的用户卡片”），它就能快速生成对应的 React/Tailwind CSS 代码。在需要快速搭建原型或后台界面时，它能节省大量写样式的时间。</li><li><strong>FlatIcon</strong>: 最大的免费矢量图标库之一。做项目总需要各种小图标，自己设计不现实，到处找又费时费力。FlatIcon 提供了海量高质量的图标，格式多样，还支持在线修改颜色，非常方便。</li></ul><h3>写在最后</h3><p>工具的价值在于解决问题，而不是追赶潮流。今天介绍的这些工具，可能永远不会像 VS Code 那样人尽皆知，但它们都在各自的领域，为我们开发者提供了更优的解决方案。</p><p>希望这份清单能给你带来一些新的启发。如果你也有私藏的宝藏工具，欢迎在评论区分享交流。</p>]]></description></item><item>    <title><![CDATA[Access现代化开发实战：新式环形图深度解析 access开发 ]]></title>    <link>https://segmentfault.com/a/1190000047524138</link>    <guid>https://segmentfault.com/a/1190000047524138</guid>    <pubDate>2026-01-06 15:08:29</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>Hi，大家好！<br/>在企业级应用开发中，Access 凭借其短平快的开发效率，依然占据着重要的一席之地。但提到 Access，很多人的第一印象还是灰色的界面和过时的图表，同志们！我讲了辣么多图表的开发与应用，大家的印象居然还是些？<br/>时代变了！ 随着 Access版本的更新，“新式图表”（Modern Charts） 的加入，让我们在 Access 中也能轻松制作出媲美专业 BI 工具的现代化数据大屏。今天，我们就来聊聊其中最受欢迎的图表之一——环形图（Donut Chart）。</p><h2>01什么是环形图？</h2><p>环形图，本质上是饼图的一种变体。它的中间有一个空心的圆孔，看起来像一个甜甜圈。为什么它比饼图更受欢迎？视觉轻量化：相比实心的饼图，环形图的留白更多，视觉上不那么拥挤。信息聚焦：中间的空心区域是一个黄金位置，可以用来显示总数、关键指标（KPI）或者图表标题，让用户一眼看到核心数据。线性关注：人眼在阅读环形图时，更倾向于比较弧长的长度，而非面积，这在某些情况下比饼图的面积比较更直观。</p><h2>02环形图的最佳使用场景</h2><p>并不是所有数据都适合用环形图，它最适合表现“部分与整体”的关系：项目进度监控：例如，已完成任务 vs 未完成任务的比例。预算执行情况：已花费预算 vs 剩余预算。销售构成分析：不同产品线的销售额占比（建议分类不超过 5-7 个，否则会显得杂乱）。KPI 仪表盘：作为仪表盘的核心组件，展示核心达成率。</p><h2>03准备数据</h2><p>运行以下 SQL 语句，创建一个名为 tblSalesData 的表，包含自动编号主键、产品类别和销售金额字段。</p><pre><code class="SQL">CREATE TABLE tblSalesData (
    ID COUNTER CONSTRAINT PrimaryKey PRIMARY KEY,
    Category TEXT(50),
    Amount CURRENCY
);</code></pre><p>添加一些测试数据</p><pre><code class="SQL">INSERT INTO tblSalesData (Category, Amount) VALUES ('电子数码', 150000);
INSERT INTO tblSalesData (Category, Amount) VALUES ('家居生活', 98000);
INSERT INTO tblSalesData (Category, Amount) VALUES ('潮流服饰', 75000);
INSERT INTO tblSalesData (Category, Amount) VALUES ('食品饮料', 45000);
INSERT INTO tblSalesData (Category, Amount) VALUES ('其它', 20000);</code></pre><h2>04创建控件</h2><p>测试数据有了，我们就可以来添加图表控件了。<br/><img width="162" height="218" referrerpolicy="no-referrer" src="/img/bVdnzmj" alt="" title=""/></p><p><img width="723" height="533" referrerpolicy="no-referrer" src="/img/bVdnzmk" alt="" title="" loading="lazy"/></p><h2>05图表设置</h2><p>接着就可以来设置一下图表的数据与格式了<br/><img width="333" height="608" referrerpolicy="no-referrer" src="/img/bVdnzml" alt="" title="" loading="lazy"/></p><p><img width="341" height="378" referrerpolicy="no-referrer" src="/img/bVdnzmn" alt="" title="" loading="lazy"/></p><h2>06图表运行</h2><p>最后，就可以运行看下效果了。<br/><img width="687" height="468" referrerpolicy="no-referrer" src="/img/bVdnzmq" alt="" title="" loading="lazy"/><br/>**<br/>结语<br/>环形图只是 Access 现代化开发的一个缩影。别让工具限制了你的想象力，用好新式图表，让你的数据“说话”。**</p>]]></description></item><item>    <title><![CDATA[【2026原创】基于Vue3的实验室预约管理系统 子午 ]]></title>    <link>https://segmentfault.com/a/1190000047524166</link>    <guid>https://segmentfault.com/a/1190000047524166</guid>    <pubDate>2026-01-06 15:07:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <h2>项目功能介绍</h2><p>本系统是一个基于Web的实验室预约管理平台,实现了实验室资源的在线预约、查询和管理功能。系统支持普通用户和管理员两种角色,普通用户可以浏览实验室信息、发起预约请求、进行点赞和评论、查看自己的收藏和预约记录;管理员则拥有完整的系统管理权限,包括用户管理、实验室管理、类别管理以及预约审批等功能。系统采用分页查询、多条件筛选等方式提供友好的用户体验,并通过审批机制确保预约的合理性和资源的有效利用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524168" alt="图片" title="图片"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524169" alt="图片" title="图片" loading="lazy"/></p><h2>选题背景与意义</h2><p>随着高校教学和科研规模的不断扩大,实验室资源日益紧张,传统的人工预约管理方式存在效率低下、信息不透明、资源分配不均等问题。用户往往需要多次沟通才能完成预约,管理员也难以实时掌握实验室的使用情况和预约状态,导致资源浪费和管理困难。本系统旨在通过信息化手段解决这些问题,实现实验室资源的统一管理和在线预约,提高资源利用率,减少人工管理成本。系统具有操作简便、实时性强、可追溯性高等特点,能够有效提升实验室管理水平,为教学科研活动提供更好的支持,具有重要的实用价值和推广意义。</p><h2>关键技术栈 Flask+Vue3</h2><p>本系统采用前后端分离架构,后端基于Flask轻量级Web框架开发,利用Flask-SQLAlchemy实现ORM映射,使用Flask-Migrate进行数据库迁移,通过Flask-JWT-Extended实现JWT身份认证,便于开发和部署。前端使用Vue3框架,结合现代前端技术栈构建响应式用户界面。系统遵循RESTful API设计规范,提供统一的数据接口。数据模型包括用户、实验室、类别、预约、点赞、收藏、评论等多个实体,通过外键关联建立数据关系。安全性方面,使用Werkzeug进行密码加密存储,通过JWT Token进行用户认证和权限控制,确保系统安全稳定运行。</p><h2>系统架构</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524170" alt="图片" title="图片" loading="lazy"/></p><h2>核心业务流程：</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524171" alt="图片" title="图片" loading="lazy"/></p><h2>系统功能模块</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524172" alt="图片" title="图片" loading="lazy"/></p><h2>四、演示视频 and 完整代码 and 安装</h2><p>地址：<a href="https://link.segmentfault.com/?enc=y8BcshjacdTGywrBgtTk1w%3D%3D.E4cflkvkc5drFnL980BttzvF0veLoDmm6dfQtBaoulA%3D" rel="nofollow" target="_blank">https://ziwupy.cn/p/aRLgPz</a></p>]]></description></item><item>    <title><![CDATA[2026 年值得推荐的 10 款 CRM 客户管理软件排行榜（权威精选） 玩滑板的饺子 ]]></title>    <link>https://segmentfault.com/a/1190000047524186</link>    <guid>https://segmentfault.com/a/1190000047524186</guid>    <pubDate>2026-01-06 15:06:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>2026 年 CRM 市场呈现<strong>功能一体化、AI 深度赋能、生态融合加速</strong>三大趋势。本榜单基于市场占有率、用户口碑、功能完整性、性价比、本土化适配五大核心维度，精选 10 款覆盖不同规模与行业的优质 CRM 系统，帮助企业精准选型。</p><hr/><h3>一、全球标杆级 CRM（适合中大型、跨国企业）</h3><h4>1. Salesforce（全球领导者）</h4><ul><li><strong>核心定位</strong>：CRM 行业绝对标杆，全功能企业级解决方案</li><li><strong>核心优势</strong>：超大生态系统、可定制性极强、行业方案成熟、AI Einstein 智能分析</li><li><strong>适用场景</strong>：中大型企业、跨国集团、上市公司、复杂业务流程企业</li><li><strong>价格区间</strong>：基础版$25/月/用户起，企业版$150 / 月 / 用户起</li><li><strong>2026 亮点</strong>：推出 AI 驱动的自动化工作流，与 MuleSoft 深度集成提升数据整合能力</li></ul><h4>2. Microsoft Dynamics 365（微软生态首选）</h4><ul><li><strong>核心定位</strong>：微软生态内的智能业务应用套件，打通销售、营销、服务、运营全链路</li><li><strong>核心优势</strong>：与 Office 365、Teams、Power Platform 无缝集成，低代码定制能力强</li><li><strong>适用场景</strong>：微软生态深度用户、需要业务系统一体化的企业、制造业、零售业</li><li><strong>价格区间</strong>：销售模块$65/月/用户起，完整套件$210 / 月 / 用户起</li><li><strong>2026 亮点</strong>：增强 Copilot AI 助手功能，支持多语言实时翻译，提升全球协作效率</li></ul><hr/><h3>二、国产企业级 CRM（适合本土中大型企业）</h3><h4>3. 八骏 CRM（B2B 复杂销售流程专家）</h4><ul><li><strong>核心定位</strong>：专注长销售周期、复杂销售流程的企业级 CRM，B2B 集成专家</li><li><strong>核心优势</strong>：销售流程管理精细（线索 - 商机 - 合同 - 回款闭环）、自定义拓展能力强（底层低代码平台）、企业微信深度集成、性价比高</li><li><strong>适用场景</strong>：B2B 企业、工业品制造、电子元器件、医疗器械、工程服务等长周期销售行业</li><li><strong>价格区间</strong>：私有化部署方案：轻盈版19800元（不限使用时长），企业版 59800元（不限使用时长）。</li><li><strong>2026 亮点</strong>：推出 AI 销售预测模型，优化复杂报价管理功能，增强与 ERP 系统的数据互通</li></ul><h4>4. 销售易（行业定制专家）</h4><ul><li><strong>核心定位</strong>：深耕中国市场的 CRM 代表，主打 B2B 销售管理、营销与服务一体化</li><li><strong>核心优势</strong>：销售云、营销云、服务云全栈产品、PaaS 平台灵活定制、AI 销售助手、行业方案丰富（金融、制造、医药等）</li><li><strong>适用场景</strong>：中大型企业、行业特性明显的企业、需要营销服务一体化的企业</li><li><strong>价格区间</strong>：标准版 600 元 / 月 / 用户起，企业版 1500 元 / 月 / 用户起</li><li><strong>2026 亮点</strong>：强化 AI 驱动的客户洞察能力，推出垂直行业专属解决方案包</li></ul><hr/><h3>三、高性价比全能型 CRM（适合中小企业）</h3><h4>5. Zoho CRM（性价比之王）</h4><ul><li><strong>核心定位</strong>：功能全面、价格亲民的全能型 CRM，中小企业首选</li><li><strong>核心优势</strong>：自动化能力强、生态完备（40 + 款 Zoho 应用集成）、AI 助手 Zia 实用、多语言支持</li><li><strong>适用场景</strong>：中小企业、成长型企业、跨境电商、需要多语言支持的企业</li><li><strong>价格区间</strong>：免费版（3 用户），标准版 100 元 / 月 / 用户起，企业版 300 元 / 月 / 用户起</li><li><strong>2026 亮点</strong>：升级 Zia AI 功能，新增社交媒体监听模块，优化移动端体验</li></ul><h4>6. 纷享销客（本土领导者）</h4><ul><li><strong>核心定位</strong>：移动协同型 CRM 代表，融合 OA 与 CRM 功能，本土生态连接标杆</li><li><strong>核心优势</strong>：移动办公体验佳、审批流程灵活、企业微信 / 钉钉双生态支持、销售过程可视化</li><li><strong>适用场景</strong>：中小企业、注重移动办公、需要 OA 与 CRM 一体化的企业</li><li><strong>价格区间</strong>：基础版 298 元 / 月 / 用户起，高级版 698 元 / 月 / 用户起</li><li><strong>2026 亮点</strong>：强化营销自动化功能，推出客户成功管理模块，提升数据分析能力</li></ul><hr/><h3>四、特色功能型 CRM（适合特定场景）</h3><h4>7. HubSpot CRM（营销驱动型）</h4><ul><li><strong>核心定位</strong>：Inbound 营销理念领导者，营销自动化强，适合内容驱动型企业</li><li><strong>核心优势</strong>：上手快、免费版功能完整、营销与销售无缝衔接、内容管理系统集成</li><li><strong>适用场景</strong>：初创企业、SaaS 公司、内容驱动营销型企业、侧重线索培育的业务场景</li><li><strong>价格区间</strong>：免费版（无限用户），营销专业版 800 元 / 月起，销售专业版 400 元 / 月起</li><li><strong>2026 亮点</strong>：增强 AI 内容生成能力，优化客户旅程地图工具，提升与电商平台的集成度</li></ul><h4>8. Pipedrive（销售流程专家）</h4><ul><li><strong>核心定位</strong>：轻量级销售 CRM，专注可视化销售流程管理，适合简单销售流程企业</li><li><strong>核心优势</strong>：界面简洁直观、销售漏斗管理清晰、移动端体验优秀、第三方集成丰富</li><li><strong>适用场景</strong>：初创企业、小型销售团队、零售、快消等短周期销售行业</li><li><strong>价格区间</strong>：基础版 15 美元 / 月 / 用户起，高级版 49 美元 / 月 / 用户起</li><li><strong>2026 亮点</strong>：推出 AI 销售助手，优化批量数据导入功能，增强报告自定义能力</li></ul><hr/><h3>五、生态融合与垂直领域 CRM</h3><h4>9. 金蝶云・星空 CRM（财务业务协同）</h4><ul><li><strong>核心定位</strong>：财务业务一体化 CRM，与金蝶 ERP 深度集成，适合重视财务管控的企业</li><li><strong>核心优势</strong>：财务业务数据互通、预算管控、应收应付管理、多组织架构支持</li><li><strong>适用场景</strong>：制造业、商贸企业、需要财务与业务紧密协同的中大型企业</li><li><strong>价格区间</strong>：基础版 500 元 / 月 / 用户起，与 ERP 捆绑购买更优惠</li><li><strong>2026 亮点</strong>：强化业财一体化报表，优化成本核算功能，提升与税务系统的对接能力</li></ul><h4>10. 小满科技 OKKI（出海 CRM 专家）</h4><ul><li><strong>核心定位</strong>：专注外贸企业的 CRM 系统，出海业务首选工具</li><li><strong>核心优势</strong>：多语言支持、跨境支付对接、海关数据整合、国际物流跟踪、客户背景调查</li><li><strong>适用场景</strong>：外贸企业、跨境电商、需要拓展国际市场的中小企业</li><li><strong>价格区间</strong>：基础版 499 元 / 月 / 用户起，企业版 1299 元 / 月 / 用户起</li><li><strong>2026 亮点</strong>：新增 AI 汇率预测功能，优化多币种报价管理，增强与跨境电商平台（亚马逊、Shopee 等）的集成</li></ul><hr/><h3>选型指南：如何选择适合你的 CRM？</h3><table><thead><tr><th>企业规模</th><th>推荐 CRM</th><th>核心考量因素</th></tr></thead><tbody><tr><td>初创企业 / 小微企业</td><td>HubSpot 免费版、Zoho 免费版、Pipedrive 基础版</td><td>成本控制、易上手、快速部署</td></tr><tr><td>中小企业</td><td>八骏 CRM 轻盈版、Zoho 标准版、纷享销客基础版</td><td>功能完整性、性价比、扩展性</td></tr><tr><td>中大型企业</td><td>八骏 CRM 企业版、销售易、Salesforce、Microsoft Dynamics 365</td><td>定制能力、行业适配、集成能力</td></tr><tr><td>跨国企业</td><td>Salesforce、Zoho CRM、Microsoft Dynamics 365</td><td>多语言支持、全球合规、跨境协作</td></tr><tr><td>外贸企业</td><td>小满科技 OKKI、Zoho CRM</td><td>跨境功能、多币种支持、国际市场适配</td></tr><tr><td>B2B 长周期销售</td><td>八骏 CRM、销售易</td><td>复杂流程管理、报价管理、回款跟踪</td></tr></tbody></table><hr/><h3>总结</h3><p>2026 年 CRM 市场呈现多元化格局，选择时应<strong>优先匹配自身业务模式与规模</strong>，而非盲目追求品牌知名度。对于 B2B 长周期销售企业，<strong>八骏 CRM</strong>凭借精细的销售流程管理、强大的自定义能力和高性价比脱颖而出；中小企业可优先考虑<strong>Zoho CRM</strong>或<strong>HubSpot</strong>的免费版；中大型企业则可根据行业特性选择<strong>销售易</strong>或<strong>Salesforce</strong>等全功能平台。</p><p>建议在选型前进行免费试用，重点测试核心业务流程的适配度、系统易用性和技术支持响应速度，确保 CRM 真正成为企业增长的助推器。</p>]]></description></item><item>    <title><![CDATA[为什么有些人换了一家公司或环境后，之前的很强的能力，就好像完全消失了一样？ 良许 ]]></title>    <link>https://segmentfault.com/a/1190000047524216</link>    <guid>https://segmentfault.com/a/1190000047524216</guid>    <pubDate>2026-01-06 15:06:03</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/8a2de9abf59d619c.css" data-n-g=""> <p>大家好，我是良许。</p><p>最近有个朋友跟我吐槽，说他们部门新来了个总监，简历写得特别牛，上一家公司带团队做了好几个千万级项目。</p><p>结果来了三个月，开会永远在画饼，落地方案一个没有，团队被他搞得怨声载道。</p><p>这让我想起一个特别扎心的现象——为什么有些人换了环境，之前那些闪闪发光的能力，就像被格式化了一样？</p><h2><strong>你以为的能力，其实是环境给的buff</strong></h2><p>很多人没意识到，你在上一家公司的"强"，可能根本不是你一个人的强。</p><p>在老东家待久了，你熟悉所有系统的底层逻辑，知道哪个接口有坑，知道找谁能最快解决问题。</p><p>你看起来效率爆表，实际上是因为你把代码库和人际关系都摸透了。</p><p>换个地方呢？代码规范不一样，技术栈不一样，连开会的黑话体系都不一样。</p><p>你突然发现自己像个新手，需要重新学习一切。</p><p>更要命的是那些隐性资源。</p><p>你之前能快速推动项目，可能是因为你跟产品总监关系好，跟技术老大是铁哥们，测试组长欠你人情。</p><p>这些看不见的关系网络，才是你能力的放大器。到了新公司，这些全部清零。</p><p>你说话没人听，提需求没人理，开会就是个透明人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524218" alt="" title=""/></p><h2><strong>文化不匹配，再强的人也会水土不服</strong></h2><p>有些公司崇尚狼性文化，加班到深夜是常态，开会就是battle。</p><p>有些公司讲究work-life balance，准点下班是基本操作。</p><p>你在A公司如鱼得水的做事风格，到B公司可能就是格格不入。</p><p>我见过一个技术大牛，在互联网大厂带团队特别厉害，讲究快速迭代、小步快跑。</p><p>跳槽去了传统企业，发现人家做决策要层层审批，上线要走半年流程。</p><p>他那套敏捷开发的玩法完全施展不开，三个月就待不下去了。</p><p>这不是能力问题，是你的操作系统跟新环境的硬件不兼容。你再牛，也得先适配环境，才能发挥价值。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524219" alt="" title="" loading="lazy"/></p><h2><strong>组织给你的title，不等于你真实的能力</strong></h2><p>很多人在原公司做到了高级、专家、总监，就觉得自己真的有那个level。</p><p>但你有没有想过，那个title可能是公司给你的，不是市场给你的？</p><p>你在小公司当技术总监，管五个人，做的都是CRUD。</p><p>跳到大厂当高级工程师，面对的是分布式系统、高并发场景，你那点经验根本不够看。</p><p>或者反过来，你在大厂螺丝钉岗位干得很溜，跳到创业公司要你从0到1搭建整个系统，你也傻眼。</p><p>能力这东西，是要放在具体场景里才能验证的。</p><p>离开了那个场景，你的能力可能就不值钱了。</p><h2><strong>能力迁移，是职场最难的修炼</strong></h2><p>真正厉害的人，不是在某个环境里强，而是有快速适应新环境的能力。</p><p>这需要你把经验提炼成方法论，把人脉转化为沟通能力，把对业务的理解升级为对商业的洞察。</p><p>但这太难了。大部分人的能力都是场景化的、依赖具体资源的。</p><p>换个地方，那些让你发光的条件都不存在了，你当然就"消失"了。</p><p>所以下次看到有人换工作后表现拉胯，别急着嘲笑。</p><p>可能不是他不行，是他的能力模型跟新环境没对上。</p><p>也提醒你自己，如果你现在混得还不错，多想想有多少是自己的硬实力，有多少是环境给的红利。</p><p>真到了要换环境的时候，你能带走的，才是真正属于你的东西。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047524220" alt="" title="" loading="lazy"/></p>]]></description></item>  </channel></rss>