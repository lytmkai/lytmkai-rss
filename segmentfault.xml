<?xml version="1.0" encoding="UTF-8"?><rss version="2.0">  <channel>      <title>SegmentFault - 最近文章</title>      <link>https://segmentfault.com/blogs/newest</link>      <description>SegmentFault 思否</description>      <generator>python segmentfault.py @Pi20</generator>      <item>    <title><![CDATA[企业人员安全意识解决方案 帮助企业构建可]]></title>    <link>https://segmentfault.com/a/1190000047447460</link>    <guid>https://segmentfault.com/a/1190000047447460</guid>    <pubDate>2025-12-03 21:03:07</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img width="723" height="311" referrerpolicy="no-referrer" src="/img/bVdnfpH" alt="image.png" title="image.png"/></p><h4>他们，正在用“习惯”打开安全后门</h4><p>据相关报告显示，企业内普遍存在的高危操作现状令人担忧，具体突出表现在以下三个方面：<br/>数字资产管理混乱：近65%的员工在多个账户间重复使用密码，超过80%的与黑客相关的泄露事件都是利用了弱口令、默认口令等。密码复用、信息随意存放成为常态，密钥管理形同虚设。<br/>基础防护严重缺失：仅有39%的个人账户和53%的启用了多重身份验证（MFA）。第一道安全防线，在源头就已千疮百孔。<br/>安全认知存在偏差：约有58%的员工自认为能够识别并避免安全威胁，但在后续的模拟钓鱼测试中，这些自认为“安全”的员工中超过30%的人会点击恶意链接；更有67%的IT从业者对自己的安全技能过度自信。认知的误区，正在源源不断地转化为真实漏洞。<br/><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnfpI" alt="image.png" title="image.png" loading="lazy"/><br/>以上真实案例并非孤例。它们共同指向一个残酷现实：员工安全意识的缺失，有时比任何技术漏洞都致命。</p><h4>传统培训为何总是“治标不治本”？</h4><p>面对日益狡猾的外部攻击与内部操作风险，多数企业的应对措施却收效甚微，陷入“高风险行为频发 → 防护能力薄弱 → 培训机制缺失”的恶性循环：<br/>依赖一次性线下培训，内容枯燥、形式单一，员工听过即忘，难以转化为行为习惯；<br/>仅靠发放安全手册、张贴警示标语，缺乏实战演练，安全知识沦为 "纸上谈兵"；<br/>即便开展模拟钓鱼测试，也缺乏后续的个性化辅导与复盘，导致员工在同一类问题上反复“踩坑”。<br/>相关数据显示，全球仅有56%的企业提供了系统性的安全意识培训，超过60%的企业未建立强制性培训机制，曾系统性地开展过模拟钓鱼攻防演练的企业更是不足一半。</p><h4>破局之道：从 "被动防御" 到 "主动赋能"</h4><p>企业信息安全体系的技术防线已初步建成，但在“人”这一核心要素的风险治理上，仍存在明显短板。<br/>将员工从风险承受者转化为主动防御者，关键在于打破“培训—遗忘—再培训”的无效循环，构建“认知—实践—反馈—优化”的全流程闭环体系，形成安全能力提升的正向循环。这正是百度安全企业人员安全意识解决方案的核心思路。<br/>我们凭借多年甲方安全实战与生态运营经验，为企业量身打造：<br/><img width="723" height="369" referrerpolicy="no-referrer" src="/img/bVdnfpK" alt="image.png" title="image.png" loading="lazy"/><br/>从精准画像发现薄弱点，到场景化培训促成行为改变，再到长效激励引导主动学习——百度安全可以帮助企业构建可持续的安全意识培养生态，让每一位员工都成为企业网络安全的守护者。<br/>本系列下一篇文章，我们将首次深度揭秘百度内部的完整实践：如何将这一核心思路转化为具体动作，并有效度量其成效。<br/>不想错过？点击 <a href="https://link.segmentfault.com/?enc=9IsJOYr1oG3sAaPBCYUJIw%3D%3D.f1zQIG0Nx3DEPSwgme1N8L5tVPpt%2FgLQeMoLKkaUQwnmXM9i1kJocUHTE5MQWw%2FH" rel="nofollow" target="_blank">https://anquan.baidu.com/product/secAwareness</a> 即可访问百度安全官网，获取人员安全意识解决方案。</p><p>网络安全的本质，是人与人的对抗。<br/>技术是“盾”，人是执盾者，唯有“软硬兼施”，方能构筑真正稳健的防御体系。<br/>百度安全愿与各企业携手，将安全意识内化为每位员工的自觉行动，共同将“人”这一最关键的变量，转化为企业安全防御中最坚固的基石。</p>]]></description></item><item>    <title><![CDATA[百度大佬拆解办公安全核心威胁，你有中招么]]></title>    <link>https://segmentfault.com/a/1190000047447467</link>    <guid>https://segmentfault.com/a/1190000047447467</guid>    <pubDate>2025-12-03 21:02:13</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月26日晚，百度安全联合墨菲安全通过直播的形式围绕 “企业办公安全关键场景实战” 展开深度分享。先来看看嘉宾介绍和直播内容一览吧:企业办公安全关键场景实战回顾！</p><p><a href="https://link.segmentfault.com/?enc=a6k5HSY2CTHwVUS9TqbJtA%3D%3D.fi8lkeyrapmgdRgcbOpO7BdKZ2%2FzMYcUw1Dlx02qs0ymoLIhChc91SUMuBdbL4Qo" rel="nofollow" target="_blank">https://v.qq.com/x/page/k3181ikfiih.html</a></p><p>直播从办公安全核心构成切入，梳理了六大安全域，拆解了当前企业易中招的四种威胁：银狐黑产团伙攻击、办公软件合规风险、无线安全攻防、办公网数据泄露，用真实案例还原了办公安全的核心难点。让我们一起回顾直播的精彩金句和问答～</p><h3>金句摘选</h3><h4>“办公安全不是单一部门的事，而是全员共建的系统工程。”</h4><h4>“银狐黑产团伙通过搞定员工账号建立信任通道，实施内部诈骗，防不胜防。”</h4><h4>“办公安全的复杂性，不仅仅体现在技术上，更在于如何处理人为行为和潜在的安全风险。”</h4><h3>问答精选</h3><h4>办公安全到底包含哪些核心内容？</h4><p>核心涵盖终端安全域、邮件安全域、网络安全域、账号安全域、物理安全域、业务安全域等，包括员工个人入网设备、外接存储都属于需管控的安全资产，任何一个环节失守都可能引发连锁风险。</p><h4>银狐黑产团伙是什么？为什么银狐能屡屡得手？</h4><p>银狐是国内活跃的网络黑产团伙（行业别名：游蛇、谷堕大盗、UTG‑Q‑1000），自 2022 年下半年起高频活动，以仿冒下载站 SEO 投毒、钓鱼邮件、即时通讯投递为主要投放方式，传播远控木马，目标为企业与个人，核心目的是窃密、诈骗与数据贩卖，形成规模化犯罪链条。该团伙擅长伪装成 “税务稽查”“补贴申领” 等可信场景，内部聊天群发钓鱼二维码、钓鱼邮件、仿冒正版软件官网等多渠道传播木马，还会替换内部员工账号建立信任通道，攻击极具隐蔽性。</p><h4>软件合规存在什么风险？</h4><p>软件合规核心风险包括未授权使用盗版软件面临版权诉讼、罚款；违规软件引发数据泄露与处罚；开源组件使用不当导致许可证冲突、漏洞暴露；缺乏合规审计引发供应链安全隐患。</p><h4>无线办公场景有哪些容易被忽视的风险？</h4><p>未开启 802.1X 认证、密码与 SSO 系统一致、弱认证强度都是高危隐患。黑客可通过伪造热点、ARP 欺骗等手段窃取明文账号密码，甚至入侵企业内网。</p><h4>办公安全最难的点在哪里？</h4><p>最难在于“人”。员工的安全意识、设备管控、权限管理缺一不可，需要技术与管理的深度融合，后续深度剖析。</p><h3>直播亮点</h3><p>专家们结合百度内部实践，透露了办公安全建设的核心逻辑——先梳理资产、再评估风险，最后通过安全基线、实时检测、风险治理形成防护体系。后续将详细拆解每个威胁的解决方案，请持续关注百度安全哦～</p>]]></description></item><item>    <title><![CDATA[为千行百业植入“安全基因”！百度加入“内]]></title>    <link>https://segmentfault.com/a/1190000047447477</link>    <guid>https://segmentfault.com/a/1190000047447477</guid>    <pubDate>2025-12-03 21:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月28日，由紫金山实验室主办的第五届网络空间内生安全学术大会暨IEEE CRESS 2025国际会议在南京启幕。大会由中国通信学会、中国计算机学会、中国汽车工程学会、中国网络空间安全学会指导，紫金山实验室主办，以“AI+生态构建新挑战，安全可信新机遇”为主题，集中展现我国在网络空间内生安全领域的原创突破与产业实践成果。同时，大会正式启动“内生安全生态伙伴计划”，该计划联合了百度、奇安信、深信服等行业领军企业开展深度合作，形成共生共存的产业生态链条，以加速技术创新与成果转化，让内生安全技术更好地赋能千行百业。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnfpW" alt="image.png" title="image.png"/><br/>第五届网络空间内生安全学术大会</p><p>大模型技术的深度应用在释放生产力的同时，也潜藏着多重安全隐患。一旦保护不到位，可能导致用户隐私泄露，甚至被用于诈骗、盗用身份等违法犯罪活动。技术开发过程中，若核心数据或算法被窃取，不仅损害企业利益，还可能被不法分子篡改功能，导致模型输出错误结果，影响医疗、金融等关键领域。这些风险不仅威胁企业及个人权益，还可能破坏社会信任，甚至影响国家安全。因此，需要通过加强数据保护、完善技术架构、制定行业规范等多方面措施，以确保大模型在安全可控的前提下发挥作用。为此，百度提出了大模型安全护栏建设理念，为行业提供了一套系统性的内生安全解决方案，构建功能完备、服务全面的大模型安全护栏产品矩阵，针对大模型场景存在的各类风险，提供一站式的大模型输入、输出安全护栏产品。</p><p><img width="723" height="487" referrerpolicy="no-referrer" src="/img/bVdnfp2" alt="image.png" title="image.png" loading="lazy"/><br/>内生安全生态伙伴计划</p><p>具体而言，百度大模型安全护栏构建了从云端到边缘侧的立体化防御体系。在云端，系统对文本实施输入输出的全链路管控，依托高精度“红线知识库”与基于权威信源的“信任域RAG”，实现了对敏感问题的精准应答与正向引导，有效避免模型幻觉并符合社会价值观；针对多模态与高级攻击，采用剪枝优化的统一大模型审核方案，在图文融合场景下表现优异，并能通过语义意图与固定模式检测精准识别角色扮演等隐蔽攻击。在端侧，适应端云协同趋势并满足GB/T 45654标准，系统部署了离线安全审核算子，在节省底座模型算力的同时确保离线治理能力，支持用户封禁及敏感词干预，实现了对突发风险的快速响应。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdnfpZ" alt="image.png" title="image.png" loading="lazy"/><br/>AI安全架构</p><p>与此同时，百度将大模型安全评测体系视为保障安全的“生命线”。该体系由海量高质量评测数据集与全流程自动化评测系统构成，不仅全面覆盖通用场景及垂直领域智能体，更能持续吸纳时下最新的风险事件与对抗性样本，保持题库的鲜活性与高对抗性。针对传统人工评测成本高、标准不一的痛点，该体系的核心创新在于引入了微调后的“裁判大模型”进行自动化标注，其准确率已高达95%以上，显著优于人工水平。通过对待测模型的例行化访问与深度评估，系统能快速生成精准报告，为合作伙伴提供科学、高效的安全水位评估，确立模型上线前的最后一道安全防线。</p><p><img width="723" height="412" referrerpolicy="no-referrer" src="/img/bVdnfp0" alt="image.png" title="image.png" loading="lazy"/><br/>大模型安全评测框架</p><p>百度的创新实践不仅体现在技术层面，更重要的是我们始终坚持将安全理念融入大模型全生命周期。从数据清洗、安全对齐、内生安全到大模型安全运营，我们构建了一套完整的原生安全体系。特别在跨模态安全治理方面，我们通过视觉理解与文本语义的双重审核，引入区域关注、跨模态对齐等机制，有效提升了对复合内容的风险管控能力。未来，我们将继续携手行业合作伙伴，以技术创新推动大模型安全的健康发展。我们将在大模型安全领域持续投入，为各行各业提供更加专业、可靠的安全服务，助力人工智能产业的可持续发展，为构建更加安全可信的AI应用环境贡献力量。</p>]]></description></item><item>    <title><![CDATA[HarmonyOS 6实操： 来去电展示]]></title>    <link>https://segmentfault.com/a/1190000047447374</link>    <guid>https://segmentfault.com/a/1190000047447374</guid>    <pubDate>2025-12-03 20:03:44</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h4>背景介绍</h4><p>今年6月份HDC大会在一个技术展台跟华为架构师交流时他给介绍了HarmonyOS提供的企业服务能力，在用户接听拨打电话时，页面显示已安装企业应用的联系人信息，方便用户识别来去电人信息，快速回应，增强企业内部沟通效率。由于工作场景确实2B业务挺重，听了很感兴趣，之前没有任何手机系统提供这种能力，还专门做了手环设备，用户在接听电话时，手环设备获取通知信息，提取手机号调用服务端获取同事信息提高交流效率。晚上回酒店后第一时间查看了对应文档，接入很简单，能力超强大。</p><h4>系统能力介绍</h4><p>HarmonyOS 从5.0.2(14)开始，提供了CallerInfoQueryExtensionAbility来去电信息查询扩展Ability，提供通话来去电页面显示企业联系人信息的能力。当有外拨电话或者接听来电时，系统回拉起自定义的CallerInfoQueryExtensionAbility，CallerInfoQueryExtensionAbility是轻量级独立子进程，不允许唤醒主进程，进程存在最长时间为2秒，超时后自动销毁。这样设计一方面是出去安全考虑，另一方面出于体验考虑，如果不是独立进程，拉起主进程如果比较耗时的话，可能电话都已经挂断了还没有开始查询用户信息。</p><p>自定义的CallerInfoQueryExtensionAbility实现CallerInfoQueryExtensionAbility中的onQueryCallerInfo方法，onQueryCallerInfo方法会传入播出或接听的手机号，根据手机号查询本地数据库或者网络接口获取手机号对应同事信息，以Promise方式异步返回CallerInfo，CallerInfo包含以下信息：</p><table><thead><tr><th align="left">名称</th><th align="left">类型</th><th align="left">只读</th><th align="left">可选</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">contactName</td><td align="left">string</td><td align="left">否</td><td align="left">否</td><td align="left">联系人姓名：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">employeeId</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">工号：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">department</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">部门：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr><tr><td align="left">position</td><td align="left">string</td><td align="left">否</td><td align="left">是</td><td align="left">职位：为保证页面最佳显示效果，字数建议限制在20字以内</td></tr></tbody></table><p>根据查询到的业务信息构造CallerInfo返回给系统展示，这样就可以直接看到手机号对应的用户名称等信息。</p><h4>实现方案</h4><h5>申请权限</h5><p>CallerInfoQueryExtensionAbility需求场景面向企业，仅供企业应用开发者接入。企业应用首先需要进行接入申请，企业应用开发者将申请信息发送至公共邮箱<code>agconnect@huawei.com</code>。<br/>邮件标题：【申请公司名】—企业来电显示能力—Developer ID<br/>邮件内容需包括：开发者接入企业来电显示能力的应用使用主体、应用名称、应用ID、应用包名、场景说明（具体描述该应用对应通讯录量级等使用的必要信息）。</p><p>企业联系人信息来去电页面显示能力申请成功后，需要重新<a href="https://link.segmentfault.com/?enc=r6RnU9QOqdsQOF3wLfgpAw%3D%3D.fA2Y1dZ%2BD7NjF2bNpSQFBYl09Sv%2F0h2nuXN2xl4s42Wb%2FmbVETgfrvhrOxEzQQs5a%2BzyBrdnD8dPyJvEhl4OTP7Fse7Dqw1FcRPx8JaytHVudPmcXNpTv1Vl2Xsj2Unv" rel="nofollow" target="_blank">申请调试Profile</a>，在新申请Profile勾选对应权限，并且在DevEco Studio中替换新申请的调试Profile。</p><h5>开发自定义CallerInfoQueryExtensionAbility</h5><p>在工程内创建一个ExtensionAbility类型的自定义组件并继承CallerInfoQueryExtensionAbility，完成onQueryCallerInfo方法的复写，示例代码如下：</p><pre><code>import { CallerInfoQueryExtensionAbility, CallerInfo } from '@kit.CallServiceKit';  
  
export default class MainCallerInfoQueryExtAbility extends CallerInfoQueryExtensionAbility {  
  // 来去电时由系统通话应用主动调用该接口查询企业联系人信息  
  onQueryCallerInfo(phoneNumber: string): Promise&lt;CallerInfo&gt; {  
    //通过手机号请求用户信息  
    return httpPost&lt;CallerInfo&gt;({  
      url:  'https://wodekouwei.com/userInfoByPhone',  
      params: {  
        'phoneNumber': phoneNumber  
      } as Record&lt;string, headerValueType&gt;  
    })  
  }  
}</code></pre><p>接着在应用配置文件module.json5中注册extensionAbilities，</p><pre><code>{
    "extensionAbilities": [
      {
        "name": "MainCallerInfoQueryExtAbility",
        "srcEntry": "./ets/callerinfoquery/MainCallerInfoQueryExtAbility.ets",//表示该Ability对应代码路径
        "type": "callerInfoQuery" //type标签必须设为"callerInfoQuery"，表示该拓展类型为CallerInfoQueryExtensionAbility。
      }
    ]
}</code></pre><h5>打开手机设置</h5><p>接着在调试设备上，前往“电话”，点击右上角的“更多”图标，前往“设置”&gt;“陌生号码和信息识别”，打开对应企业应用的号码识别功能开关，进行调试：<br/><img width="385" height="446" referrerpolicy="no-referrer" src="/img/bVdnfoh" alt="image.png" title="image.png"/><br/><img width="385" height="446" referrerpolicy="no-referrer" src="/img/bVdnfoh" alt="image.png" title="image.png" loading="lazy"/><br/><img width="378" height="793" referrerpolicy="no-referrer" src="/img/bVdnfoi" alt="image.png" title="image.png" loading="lazy"/></p><h4>注意事项</h4><p>一方面，来去电页面或横幅仅展示一个联系人信息，对于多个应用里存在相同联系人的情况，按照应用包名的字典序排序，展示首个查询结果。<br/>另一方面，关于用户信息存储问题，上述示例采用了网络接口查询方式，网络正常情况下2秒可以正常返回，官方示例给了RDB数据库查询方式，通过本地数据库查询就要求必须把所有用户信息都内置在应用中，这样不仅有安全问题而且如果企业规模较大员工较多时也是加重本地存储压力。一般采用接口请求方式，接口要做一些频次等限制也要保证响应速度。<br/>RDB数据库场景需转化context类型 <code>const context = (this.context as common.ExtensionContext).getApplicationContext();</code><br/>转换后使用content获取RdbStore实例：<code>let store = await relationalStore.getRdbStore(context, null);</code></p><h4>总结</h4><p>HarmonyOS 5.0.2及以上版本推出的CallerInfoQueryExtensionAbility，为企业场景提供了高效实用的来电识别解决方案——通过轻量级独立进程机制，在来去电时快速查询并展示联系人姓名、部门、职位等企业信息，精准解决了2B业务中内部沟通的身份识别痛点。该能力接入流程简洁清晰，仅需完成权限申请、扩展Ability开发与配置、手机功能开关开启三步即可落地，同时支持网络接口查询与本地数据库查询两种方式，结合多应用排序规则与响应速度优化建议（优先推荐接口查询），既保障了安全性与体验流畅度，又降低了企业落地成本。对于有内部通讯录管理需求的企业应用而言，这一系统级能力无需额外硬件支持，即可显著提升沟通效率，是鸿蒙生态在企业服务领域的又一实用创新。</p><h4>参考</h4><p><a href="https://link.segmentfault.com/?enc=0cJU8LoMAkhMG9A6FfkrLg%3D%3D.ZUKaGkzvOAHwgZnajG05DZp4g94zw7GU0k8DpH4WCX%2FuZKeQ1canmnsgqWCWM0CIvYgfDXp0h3UEJ%2FAH%2B3PS5GrmszPz4eCk6lH9UxncwWQaJlQMBm2Affs7v%2B8zD%2F1U0lAETdTNWCdarzaWAuj1Rw%3D%3D" rel="nofollow" target="_blank">https://developer.huawei.com/consumer/cn/doc/harmonyos-guides...</a></p>]]></description></item><item>    <title><![CDATA[JAX 训练加速指南：8 个让 TPU ]]></title>    <link>https://segmentfault.com/a/1190000047447415</link>    <guid>https://segmentfault.com/a/1190000047447415</guid>    <pubDate>2025-12-03 20:02:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>TPU 训练的真实效率往往取决于两个核心要素：<strong>Shape 的稳定性</strong>与<strong>算子的融合度</strong>。</p><p>很多时候，JAX 任务之所以出现严重的性能瓶颈，并非算法本身设计有问题，而是忽视了 XLA 编译器与底层硬件对“确定性”的极度偏好。基于大量实战调优经验，本文总结了八条能让 JAX 训练任务从“甚至跑不通”蜕变为“跑满 TPU 算力”的工程经验。<br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047447417" alt="" title=""/></p><h2>1、尽早锁定 Shape</h2><p>TPU 喜欢静态 Shape，JAX 也是，所以动态 Shape 是性能杀手，它会触发重新编译（Recompile）。一旦发生重编译，Step time 和内存占用都会直接炸裂。所以解决方法也很简单，选定几个规范的尺寸，剩下的全填（Pad）满。</p><p><strong>全局 Batch Size</strong> 要能被 TPU 核心数整除，然后就是对于变长序列，别指望它原本多长就多长，把它 Pad 到几个固定的“桶（Bucket）”里，比如 128、256 或 512，这步工作最好在输入（Input Pipeline）里就做完。</p><p>Python层面的条件判断尽量别依赖 Shape，真要分支逻辑，就老老实实让</p><pre><code>lax.cond</code></pre><p>或</p><pre><code>lax.switch</code></pre><p>来接管。</p><pre><code>     # Example: bucketing &amp; padding (conceptual)  
    def pad_to_length(arr, L):  
        pad = L - arr.shape[0]  
        return jnp.pad(arr, ((0, pad), (0, 0)), mode='constant')  
      
    bucket_sizes = [128, 256, 512]  
    def bucket_len(n):   
        return next(b for b in bucket_sizes if n &lt;= b)  
      
    def preprocess_batch(batch):  
        L = bucket_len(batch["tokens"].shape[1])  
        batch["tokens"] = pad_to_length(batch["tokens"], L)  
        batch["mask"]   = pad_to_length(batch["mask"], L)  
         return batch</code></pre><p>每个 Step 喂给 TPU 的 Shape 只要是固定的，XLA 编译器就不会找麻烦。</p><h2>2、激活值默认用 bfloat16，主权重要 FP32</h2><p>在 TPU 上</p><pre><code>bfloat16</code></pre><p>(bf16) 是个好东西，兼顾了速度、内存和数值稳定性。</p><p>工程上的常规操作是：<strong>激活（Activations）和梯度（Gradients）存成 bf16</strong>。但是，优化器状态里的权重必须保留一份 <strong>FP32 的“主副本”</strong>，不然跑久了数值就会漂移。所欲需要在模型边界做类型转换（Cast）的时候小心点。</p><pre><code>     class MLP(nn.Module):  
        features: int  
        @nn.compact  
        def __call__(self, x):  
            x = x.astype(jnp.bfloat16)     # fast path on TPUs  
            x = nn.Dense(self.features, dtype=jnp.bfloat16)(x)  
            x = nn.gelu(x)  
            x = nn.Dense(self.features, dtype=jnp.bfloat16)(x)  
            return x  
      
    # Optimizer state stays in FP32 (conceptual)  
    params_fp32 = params.astype(jnp.float32)  
    grads_bf16  = compute_grads_bf16(...)  
     updates_fp32 = opt.update(grads_bf16.astype(jnp.float32), opt_state, params_fp32)</code></pre><h2>3、pjit和命名网格：切分要明确，别靠猜</h2><p>JAX 在 TPU 上最强的一点就是通过</p><pre><code>pjit</code></pre><p>实现了 <strong>GSPMD</strong>。你通过 PartitionSpecs 告诉它<strong>想要</strong>什么切分方式，XLA 负责搞定<strong>如何</strong>在设备间搬运数据。</p><p>在 TPU 核心上建个<strong>命名网格（Mesh）</strong>。做数据并行（Data Parallelism）时，用</p><pre><code>PartitionSpec('data', None)</code></pre><p>这种模式。如果模型太大需要张量并行（Tensor Model Parallelism），就加个</p><pre><code>'model'</code></pre><p>轴。</p><pre><code>     import numpy as np  
    import jax  
    import jax.numpy as jnp  
    from jax.sharding import Mesh, PartitionSpec as P  
    from jax.experimental import pjit  
      
    devices = np.array(jax.devices()).reshape(1, -1)  # 1 x N mesh  
    mesh = Mesh(devices, ('data',))  
      
    def loss_fn(params, batch):  
        logits = model_apply(params, batch['x'])  
        return cross_entropy(logits, batch['y'])  
      
    @pjit.pjit(  
        in_shardings=(P(None), P('data')),   # params replicated, batch sharded on 'data'  
        out_shardings=P(None),               # scalar loss replicated  
    )  
    def step(params, batch):  
        grads = jax.grad(loss_fn)(params, batch)  
        # aggregate grads across cores  
        grads = jax.tree.map(lambda g: jax.lax.pmean(g, axis_name='data'), grads)  
        return grads  
      
    with mesh:  
         grads = step(params, sharded_batch)</code></pre><p>切分（Sharding）这事必须<strong>显式</strong>。如果偷懒依赖自动推导，等到后期 debug 那些悄无声息的跨设备数据传输时，绝对会很痛苦。</p><h2>4、jit, vmap, scan 三件套</h2><p>TPU 喜欢大块头的 Kernel，讨厌成千上万个细碎的小算子。训练 Step 和任何中大型计算逻辑，必须用</p><pre><code>jit</code></pre><p>包起来。遇到 Python 循环，如果是时间步逻辑就换成</p><pre><code>lax.scan</code></pre><p>，如果是批次并行就用</p><pre><code>vmap</code></pre><p>。</p><p>把 Loss 计算、梯度计算和参数更新塞进<strong>同一个 jitted 函数</strong>里，这样编译器才有机会把它们融合成一个大算子。</p><pre><code>     import optax  
    import jax  
      
    optimizer = optax.adamw(3e-4)  
      
    def loss_and_grads(params, batch):  
        def _loss(p):  
            logits = model_apply(p, batch['x'])  
            return cross_entropy(logits, batch['y'])  
        loss, grads = jax.value_and_grad(_loss)(params)  
        return loss, grads  
      
    @jax.jit  
    def train_step(state, batch):  
        loss, grads = loss_and_grads(state.params, batch)  
        grads = jax.lax.pmean(grads, axis_name='data')  
        updates, new_opt_state = optimizer.update(grads, state.opt_state, state.params)  
        new_params = optax.apply_updates(state.params, updates)  
         return state.replace(params=new_params, opt_state=new_opt_state), loss</code></pre><h2>5、别让输入管道拖后腿</h2><p>Host 到 Device 的数据传输一旦停顿，吞吐量就掉下来了，所以永远别让计算单元等数据。</p><p>用</p><pre><code>tf.data</code></pre><p>或者高效的 NumPy loader 配合 prefetch。数据预取到设备（Stage to device） 最好做双重缓冲。<strong>全局 Batch</strong> 尽量大（当然要能被核心数整除），数据增强这种脏活累活在 Host 上一次性做完。</p><pre><code>     # tf.data pipeline (conceptual)  
    ds = (tf.data.TFRecordDataset(files)  
          .map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)  
          .batch(global_batch_size, drop_remainder=True)  
          .prefetch(tf.data.AUTOTUNE))  
      
    # Convert to NumPy and prefetch onto devices  
    from flax.jax_utils import prefetch_to_device  
    it = prefetch_to_device(map(npify, ds.as_numpy_iterator()), size=2)  
      
    with mesh:  
        for step_i in range(num_steps):  
            batch = next(it)     # already sharded/prefetched  
             state, loss = train_step(state, batch)</code></pre><h2>6、PRNG要Fold 进 Step 和 Device ID</h2><p>JAX 的 PRNG 是<strong>无状态</strong>的，这意味如果不小心，很容易在不同 Step 或者不同设备上用了一样的随机数 Key。</p><p>每个 Step 都要 Split 一次绝对别复用。所以说为了保证独立性必须把 <strong>Global Step</strong> 和 <strong>Device Index</strong> 都 <strong>Fold</strong> 进去。数据增强/Dropout 的 Key 和参数初始化的 Key 得分开管理。</p><pre><code>     def make_step_rng(rng, step):  
        step_key = jax.random.fold_in(rng, step)  
        dev_key  = jax.random.fold_in(step_key, jax.lax.axis_index('data'))  
        return jax.random.split(dev_key, 1)[0]  
      
    @jax.jit  
    def train_step(state, batch, base_rng):  
        rng = make_step_rng(base_rng, state.step)  
        logits = model_apply(state.params, batch['x'], rngs={'dropout': rng})  
         ...</code></pre><h2>7、Remat，智能 Checkpoint，梯度累积</h2><p>TPU 内存看着大，模型一跑起来就不够用。深层网络可以直接用 Activation Checkpointing（</p><pre><code>jax.checkpoint</code></pre><p>或</p><pre><code>nn.remat</code></pre><p>），用计算换显存。想跑大 Batch 但显存不够，就用梯度累积（Gradient Accumulation） 把它切成小的 micro-step。</p><p>存盘的时候，推荐用 Orbax 做异步、分片（Sharded）的 Checkpoint，稳。</p><pre><code>     from flax import linen as nn  
      
    class DeepBlock(nn.Module):  
        @nn.compact  
        def __call__(self, x):  
            # recompute on backward to trim activation memory  
            f = nn.remat(lambda y: nn.gelu(nn.Dense(x.shape[-1])(y)))  
            return f(x)  
      
    # Gradient accumulation (conceptual)  
    @jax.jit  
    def accum_step(state, batch_slices):  
        def body(carry, micro):  
            state, grad_sum = carry  
            _, grads = loss_and_grads(state.params, micro)  
            return (state, jax.tree_util.tree_map(jnp.add, grad_sum, grads)), None  
        init_grads = jax.tree_util.tree_map(jnp.zeros_like, state.params)  
        (state, grad_sum), _ = jax.lax.scan(body, (state, init_grads), batch_slices)  
        grads = jax.tree_map(lambda g: g / len(batch_slices), grad_sum)  
         ...</code></pre><h2>8、一定要跑 Profiler</h2><p>把关键代码段用 Profiler Annotations 包起来，看 Step Timeline。重点找 Host Waits、Recompiles 和那些没融合好的细碎算子（Small op soup）。</p><p>稳态运行的时候，盯着 Tokens/sec 或者Images/sec，还有硬件利用率。</p><pre><code>     from jax.experimental import host_callback as hcb  
    from jax import profiler  
      
    def tagged(name, fn, *a, **k):  
        profiler.annotate_function(name=name)  
        return fn(*a, **k)  
      
    @jax.jit  
    def train_step(state, batch):  
        profiler.annotate_function(name="train_step")  
        # do work...  
         return state, loss</code></pre><p>一定要在锁定 Shape 并且 JIT 完热点路径之后再做 Profile，不然全是噪音，根本看不到真正的瓶颈。</p><h2>极简 TPU 训练示例</h2><p>这基本包含了上面所有的内容</p><pre><code>     # Pseudo-skeleton (Flax + JAX + TPU)  
    mesh = Mesh(np.array(jax.devices()).reshape(1, -1), ('data',))  
      
    @pjit.pjit(in_shardings=(P(None), P('data'), P(None)), out_shardings=(P(None), P(None)))  
    def train_step(state, batch, base_rng):  
        rng = jax.random.fold_in(base_rng, state.step)  
        rng = jax.random.fold_in(rng, jax.lax.axis_index('data'))  
        def loss_fn(p):  
            logits = model_apply(p, batch['x'].astype(jnp.bfloat16),  
                                 rngs={'dropout': rng})  
            return cross_entropy(logits, batch['y'])  
        loss, grads = jax.value_and_grad(loss_fn)(state.params)  
        grads = jax.tree_map(lambda g: jax.lax.pmean(g, 'data'), grads)  
        updates, opt_state = optimizer.update(grads, state.opt_state, state.params)  
        params = optax.apply_updates(state.params, updates)  
        return state.replace(params=params, opt_state=opt_state, step=state.step+1), loss  
      
    with mesh:  
        for step_i, batch in enumerate(prefetched_iterator):  
            state, loss = train_step(state, batch, base_rng)  
            if step_i % log_every == 0:  
                # Pull back just tiny scalars; keep big tensors on device  
                host_loss = jax.device_get(loss)  
                 print(f"[{step_i}] loss={host_loss:.4f}")</code></pre><h2>总结</h2><p>TPU 需要的是 一致性：稳定的 Shape，融合的 Kernel，目的明确的切分，不掉链子的数据管道，把上面的这八件事做好，写 JAX 训练循环就非常顺畅了。</p><p><a href="https://link.segmentfault.com/?enc=q5rRLGkZW3XZ%2BSspfsk9Ow%3D%3D.PXOD73PGkQZ%2FN72ZBmMGHFo6MiHlLeztSh53Xss8W5uUmmduCcw7WB4JTndzxCDdf9JAZM5zBHwD6LIak3%2B48A%3D%3D" rel="nofollow" target="_blank">https://avoid.overfit.cn/post/16b582a493ba4eca8333314859665dd2</a></p><p>作者:Modexa</p>]]></description></item><item>    <title><![CDATA[AI 时代 HR 的进化与工具赋能 爱跑]]></title>    <link>https://segmentfault.com/a/1190000047447419</link>    <guid>https://segmentfault.com/a/1190000047447419</guid>    <pubDate>2025-12-03 20:02:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>AI 时代 HR 的进化与工具赋能<br/>AI时代招聘变革：HR的进化之路与工具赋能<br/>在AI技术重塑各行业的当下，招聘领域正经历深刻的座次重排。曾经作为“后台工具”的HR技术，如今已升级为企业核心的业务操作系统。AI不会取代HR，但不懂运用AI的HR，正逐渐被时代拉开差距。<br/>2026年，行业的核心命题不再是“AI是否会夺走工作”，而是“HR能否借助AI制定战略、依托数据做决策、用技术驱动组织发展”。在这场关乎职业未来的变革中，AI技术为HR突破传统工作瓶颈提供了关键支撑，推动招聘从依赖经验的传统模式，迈向精准、高效、人性化的智能新阶段。</p><p>传统招聘的三大痛点：亟待技术破解<br/>长期以来，招聘工作始终被三大核心问题困扰，成为制约企业人才发展的“三座大山”：<br/>•选错人：HR的判断易受时间、经验局限，主观评估导致人才与岗位匹配度不足，给企业带来隐性成本损失。<br/>•效率低：简历筛选、面试安排、信息核实等流程繁琐，占用HR大量时间，难以聚焦核心的人才战略工作。<br/>•体验差：传统面试流程僵化，候选人常面临沟通不顺畅、疑问难解答等问题，影响雇主品牌形象。<br/>第六代AI面试智能体的出现，以“高精准度”和“优体验感”为核心武器，针对性破解这些行业痛点，重塑招聘全链路。<br/>核心突破一：精准评估，让决策有数据支撑<br/>传统招聘中“凭感觉选人”的模式，在AI技术的赋能下被彻底改变。第六代AI面试智能体的评分体系经过多重严格验证，包括客户“背靠背”人机对比实验、效标效度检验、重测稳定性信度验证，评估结果可直接作为用人决策依据，其6.3版本更是标志着该类工具进入国际领先梯队。<br/>这种精准度贯穿招聘评估的每一个环节：<br/>•一问多能：单道题目可同步评估多项胜任力，实现HR初筛与技术复试的无缝衔接，效率提升50%以上。<br/>•自由追问：根据候选人回答即时生成专业问题，如同资深面试官般精准捕捉核心信息，避免能力遗漏。<br/>•简历深度挖掘：自动定位简历中的模糊点与潜在漏洞，通过递进式提问验证信息真伪，填补评估盲区。<br/>•全维度覆盖：无论是沟通协作等通用能力，还是算法工程、财会、编程等专业技能，都能实现全面考察，既解放HR，也减轻专业面试官的负担。<br/>这不再是简单的工具升级，而是专业判断力的规模化复制与输出，让HR不再因经验不足而受制于时间。<br/>核心突破二：拟人化交互，重塑候选人体验<br/>以往的AI面试常因机械、冰冷的流程引发候选人抵触，而第六代AI面试智能体以“有人味”的交互设计，让面试成为雇主品牌的加分项：<br/>•懂情绪的对话：能够识别候选人的语速、情绪变化与潜台词，通过合理引导帮助候选人充分表达，避免因紧张错失展示实力的机会。<br/>•全程无断点：自动识别回答状态，流程衔接自然流畅，如同面对面交流，告别卡顿、跳题的尴尬。<br/>•沉浸式视觉体验：口型与语言节奏精准同步，彻底摆脱“AI纸片人”的违和感，提升面试代入感。<br/>•多轮互动答疑：候选人可主动咨询岗位详情、薪酬福利、发展路径等问题，AI实时回应解答，将候选人好感度前置，让招聘成为双向价值认同的过程。<br/>核心突破三：全流程自动化，实现效率质的飞跃<br/>AI招聘工具的价值不止于面试环节，新一代AI人才寻访智能体构建了“从识人到沟通”的一体化自动化系统，将初筛全链路效率提升10-100倍，大幅降低招聘成本。<br/>该系统具备极强的实用性与便捷性：<br/>•即启即用：30-60秒即可完成初始化，无需复杂配置便能独立运行。<br/>•智能筛选：自主匹配岗位硬性条件，精准筛选简历，减少无效工作量。<br/>•拟人化沟通：自动发起对话，模拟人类语气交流，不合适时即时终止，避免无效沟通内耗。<br/>•全量响应：遍历所有未读消息，逐条进行个性化回复，不遗漏潜在人才。<br/>•信息补全：主动向候选人索取缺失信息与简历，完善人才档案。<br/>•系统同步：自动将候选人资料上传至ATS系统，生成完整档案，实现数据系统化管理。<br/>这一变革标志着招聘工作真正从“经验型判断”走向“数据型决策”，完成了效率与质量的双重飞跃。<br/>AI时代HR的核心竞争力：拥抱工具，进化自我<br/>AI技术的普及，让HR行业的竞争维度发生改变。未来的HR不再需要陷入繁琐的事务性工作，而是要借助先进工具，提升自身的战略判断力与数据决策力，以更快的学习曲线适应行业变化。<br/>AI招聘工具的出现，本质上是为HR赋能，帮助其突破时间与经验的局限，回归人才战略规划、组织文化建设等核心工作，成为推动企业发展的关键力量。拥抱AI，善用工具，已成为HR在新时代立足与进化的必然选择。</p>]]></description></item><item>    <title><![CDATA[基于 STM32 的智能窗户控制系统设计]]></title>    <link>https://segmentfault.com/a/1190000047447430</link>    <guid>https://segmentfault.com/a/1190000047447430</guid>    <pubDate>2025-12-03 20:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>基于 STM32 的智能窗户控制系统设计与实现【源码分享】</h2><p>智能家居的发展正让越来越多的传统设备焕发生机，而“窗户”作为家庭环境调节与安全防护的重要环节，其自动化与智能化价值也愈发显现。本文将基于 <strong>STM32 微控制器 + ESP8266 Wi-Fi 模块</strong>，设计并实现一个具备环境感知、安全监测、自动控制与远程交互能力的智能窗户控制系统。整个方案以嵌入式设计为核心，兼具工程可实施性和软硬件扩展能力。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447432" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>源码分享</h2><p>免费开源，源码见：</p><blockquote><a href="https://link.segmentfault.com/?enc=GJjq0rLC%2FL23AUgEdy%2BhVw%3D%3D.U0gkMaFdJoOn6Ghk0e7%2F3ipp33A9L1Q5bW0Mnp0ruKUeaM3dx8tIu9WP4X%2Bop%2Fe1lFq%2BsgQBwGVpF7TjpZMYjw%3D%3D" rel="nofollow" target="_blank">https://blog.csdn.net/weixin_52908342/article/details/155538167</a></blockquote><h3>一、系统总体架构</h3><p>智能窗户控制系统以 STM32 为主控，协同 ESP8266 进行无线通信，实现以下功能：</p><ul><li>构建传感器局域网，采集窗户周边环境数据</li><li>实时监测温湿度、雨滴、风力、光照等环境信息</li><li>监测异常入侵情况，并进行本地或远程告警</li><li>控制电机实现自动开关窗</li><li>使用手机 APP 远程查看数据与控制窗户开合</li><li>提供扩展接口，实现更多场景自动化</li></ul><p>整体结构如下：</p><pre><code>┌─────────────────────────────────┐
│           手机 App / 云端服务      │
└───────────────▲─────────────────┘
                │ Wi-Fi (ESP8266)
┌───────────────┴─────────────────┐
│               ESP8266           │
│  Wi-Fi 通信 / MQTT / HTTP 控制通道  │
└───────────────▲─────────────────┘
                │ UART
┌───────────────┴─────────────────┐
│               STM32              │
│ 传感器管理 | 控制算法 | 电机驱动 | 安防检测 │
│                                     │
│        传感器总线(I2C/ADC/UART)       │
└───────────────┬─────────────────┘
                │
       ┌────────┴──────────┐
       │        │           │
 雨滴传感器   温湿度传感器     光照传感器
 风速模块     霍尔/红外入侵检测  窗户位置检测</code></pre><hr/><h3>二、无线传感器局域网的搭建（ESP8266）</h3><p>为了实现远程控制与数据查看，系统采用 <strong>ESP8266</strong> 作为无线通信模块。实现方式包含两个部分：</p><h4>1. ESP8266 与 STM32 的串口通信协议</h4><p>通过 UART 通信，设计轻量级的数据帧结构，如：</p><pre><code>[Header][Cmd][Len][Payload][Checksum]</code></pre><p>用于实现以下命令交互：</p><ul><li>上传传感器数据</li><li>发送开窗/关窗指令</li><li>状态同步、心跳包</li></ul><h4>2. Wi-Fi 与云端/APP 的通信</h4><p>主流方式包括：</p><ul><li><strong>MQTT</strong>：轻量、实时性强，适合 IoT</li><li><strong>HTTP + REST API</strong>：便于调试和快速集成</li><li><strong>WebSocket</strong>：适合实现实时状态推送</li></ul><p>ESP8266 作为网关，将 STM32 的数据透明地上传至云端，实现室内外双向通信。</p><hr/><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447433" alt="Schematic_窗户控制系统 copy_2023-07-14" title="Schematic_窗户控制系统 copy_2023-07-14" loading="lazy"/></p><h3>三、温湿度检测与环境数据采集</h3><p>系统采用常用的温湿度传感器（如 SHT30、DHT20）采集 <strong>室内外温湿度</strong>，并由 STM32 进行以下处理：</p><ul><li>数据滤波：如均值滤波、低通滤波</li><li>数据校准：消除传感器误差</li><li>趋势判断：用于窗户开关策略决策</li></ul><p>例如：<br/>当室外温度低于室内且空气质量好时，可自动开窗通风；<br/>反之，则保持关闭或仅部分开启。</p><p>根据季节与用户习惯，还可以结合配置文件制定不同控制策略。</p><hr/><h3>四、非法入侵检测与驱离机制</h3><p>考虑到窗户也是入侵入口，系统可接入多种检测方式：</p><h4>1. 红外人体检测（PIR）</h4><p>检测近距离移动物体，适合夜间警戒。</p><h4>2. 窗户振动与位移监测</h4><p>通过加速度计/震动传感器检测外力破窗行为。</p><h4>3. 磁性开关/霍尔传感器</h4><p>判断窗户是否被强行开启。</p><p>当检测到异常时：</p><ul><li>本地警告（蜂鸣器、灯光）</li><li>推送警报到手机 APP</li><li>可选择自动关闭窗户</li></ul><p>实现家庭安防的一道额外防线。</p><hr/><h3>五、雨滴、风力、光照检测与天气联动</h3><p>户外天气的快速变化是影响开窗的关键因素，系统通过以下传感器实时监测：</p><h4>1. 雨滴传感器</h4><p>检测降雨，一旦触发立即关窗。</p><h4>2. 风速检测模块（小型风力传感器）</h4><p>风力过大时需限制开窗角度，避免损坏。</p><h4>3. 光照强度传感器（光敏电阻/光照度计）</h4><p>通过光强变化判断时间段或天气情况，有助于完善自动控制策略：</p><p>例如</p><ul><li>光照变弱 + 风雨信号 → 可能即将下雨</li><li>高光照 → 夏季需要减少室外热量进入</li></ul><p>多源数据融合使窗户控制更智能。</p><hr/><h3>六、电机控制与自动开关算法</h3><p>核心执行机构为直流电机或步进电机，通过 L298N、TB6612 或更高效的无刷驱动进行控制。</p><h4>1. 电机结构设计</h4><ul><li>推杆式开窗器：行程大、推力强</li><li>齿轮齿条式：控制精度高</li><li>小型舵机：适用于小窗户</li></ul><h4>2. 自动开关窗算法</h4><p>算法可基于多条件决策，例如：</p><pre><code>if (下雨 OR 风力过大) → 立即关窗
else if (室外温度低于室内 &amp;&amp; 空气质量好 &amp;&amp; 无异常入侵) → 自动开窗
else if (夜间 &amp;&amp; 温度较低) → 保持关闭</code></pre><p>可结合 PID 控制调节开窗角度，也可通过限位开关保证安全。</p><hr/><h3>七、手机 APP 远程控制与可视化</h3><p>通过 ESP8266 将数据上传至云端，APP 可实时查看：</p><ul><li>室内外温湿度</li><li>光照、风力、雨滴状态</li><li>开窗位置和当前状态</li><li>安防告警记录</li></ul><p>用户可远程执行：</p><ul><li>开窗 / 关窗 / 停止</li><li>切换自动/手动模式</li><li>设置窗户开合策略</li><li>启动安防警戒模式</li></ul><p>UI 可使用 Flutter、uni-app 或原生方案开发。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447434" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>总结</h2><p>本文构建了一个 <strong>完整的智能窗户控制系统方案</strong>，涵盖了传感器网络、环境监测、安防检测、电机控制算法、无线通信和远程 APP 交互。<br/>通过 <strong>STM32 + ESP8266</strong> 的组合，使原本普通的窗户具备了环境感知、自动控制与远程操控能力，加速传统家居设备的智能化升级。</p>]]></description></item><item>    <title><![CDATA[实时 vs 批处理：ETL在混合架构下的]]></title>    <link>https://segmentfault.com/a/1190000047447246</link>    <guid>https://segmentfault.com/a/1190000047447246</guid>    <pubDate>2025-12-03 19:05:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在数字经济加速渗透的今天，数据已成为企业核心竞争力的关键载体。然而，企业在数据处理过程中始终面临着一个核心抉择：是选择实时 ETL满足即时决策需求，还是依赖批处理保障海量数据高效处理？两种模式看似对立，实则各有适配场景 —— 实时处理擅长低延迟响应，批处理则在高吞吐量、低成本运算中占据优势。如何打破模式壁垒，实现 “鱼与熊掌兼得” 的混合架构部署？下面将演示使用ETLCLoud的实时监听多表同步的案例。</p><h3>一、数据源准备</h3><p>在数据源列表中点击新建数据源。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447249" alt="图片 2" title="图片 2"/></p><p>里面提供了大量的数据源模板，这里选择MySQL模板进行创建</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447250" alt="图片 3" title="图片 3" loading="lazy"/></p><p>填写对应的链接配置之后，点击保存并测试。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447251" alt="图片 4" title="图片 4" loading="lazy"/></p><p>提示链接成功即可正常使用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447252" alt="图片 5" title="图片 5" loading="lazy"/></p><p>按照同样的步骤创建另一个MySQL数据源</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447253" alt="图片 1" title="图片 1" loading="lazy"/></p><h3>二、数据处理流程</h3><p>来到离线数据集成的流程管理，点击新增流程。这里已经提前建好了CDC同步的流程，然后打开流程设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447254" alt="图片 3" title="图片 3" loading="lazy"/></p><p>从组件列表中拉取库表批量输出组件。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447255" alt="图片 4" title="图片 4" loading="lazy"/></p><p>库表批量输出组件配置：</p><p>在基本属性配置里面选择刚才创建的数据源，其他配置默认。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447256" alt="图片 5" title="图片 5" loading="lazy"/></p><p>输出选项的数据更新方式选择合并后批量。其他配置默认，然后点击保存。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447257" alt="图片 6" title="图片 6" loading="lazy"/></p><h3>三、监听器配置</h3><p>在实时数据集成界面切换至数据库监听器模块，点击新增监听器创建监听器。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447258" alt="图片 7" title="图片 7" loading="lazy"/></p><p>任务配置：</p><p>任务名称和所属分类根据需要填写，所属分类可以在分类管理里创建。支持多种传输模式，这里选择传输到ETL。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447259" alt="图片 8" title="图片 8" loading="lazy"/></p><p>源端配置：</p><p>主要选择源端数据源类型、数据源和要监听的数据库和表。其他的配置默认。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447260" alt="图片 9" title="图片 9" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447261" alt="图片 10" title="图片 10" loading="lazy"/></p><p>目标端ETL：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447262" alt="图片 11" title="图片 11" loading="lazy"/></p><p>启动监听器</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447263" alt="图片 13" title="图片 13" loading="lazy"/></p><p>触发数据变动，查看数据传输情况，可以看到数据监听并同步成功。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447264" alt="图片 14" title="图片 14" loading="lazy"/></p><h3>四、最后</h3><p>在数据量爆炸式增长、业务场景日益复杂的今天，单一的数据处理模式已无法满足企业多元化需求。ETLCloud 将实时处理的敏捷性与批处理的高效性完美融合，不仅解决了企业数据处理的 “两难困境”，更通过技术创新构建起灵活、高效、安全的数据集成体系。未来，ETLCloud 将持续深耕混合架构技术研发，推出更多智能化功能，助力企业在数据驱动的浪潮中抢占先机，实现从 “数据可用” 到 “数据好用” 的价值跃迁，让每一份数据都能精准赋能业务增长。</p>]]></description></item><item>    <title><![CDATA[用“分区”来面对超大数据集和超大吞吐量 ]]></title>    <link>https://segmentfault.com/a/1190000047447282</link>    <guid>https://segmentfault.com/a/1190000047447282</guid>    <pubDate>2025-12-03 19:04:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>1. 为什么要分区？</h3><p><strong>分区（partitions）</strong> 也被称为 <strong>分片（sharding）</strong> ，通常采用对数据进行分区的方式来增加系统的 <strong>可伸缩性</strong>，以此来面对<strong>非常大的数据集或非常高的吞吐量</strong>，避免出现热点。</p><p>分区通常和复制结合使用，使得每个分区的副本存储在多个节点上，保证数据副本的 <strong>高可用</strong>。如下图所示，如果数据库被分区，每个分区都有一个主库。不同分区的主库可能在不同的节点上，每个节点可能是某些分区的主库，同时是其他分区的从库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447284" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h4>1.1 一致前缀读</h4><p>分区也会由于复制延迟而产生问题，我们先来看下图中的例子，是Poons先生和Cake小姐的对话：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447285" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>Poons先生先问： "How far into the future can you see, Mrs.Cake?"</p><p>Cake小姐回答说： "About ten seconds usually, Mr.Poons."</p><p>正常情况下，这段对话是有因果关系的（先问后答）。但是对于观察者，他看到的顺序却是先得到了答案，再看到了问题，这就是在分区数据库中，因复制延迟而产生的特殊情况。</p><p>为了避免这种混乱，我们就需要保证 <strong>一致前缀读</strong>：如果一系列写入按某个顺序发生，那么任何人读取这些写入时，也会看见它们以同样的顺序出现。一种解决方案是，确保任何因果相关的写入都在相同的分区。</p><h3>2. 该怎么分区？</h3><p>分区的目的是将数据和负载均匀的分布到各个节点上，理论上10个节点能够处理10倍的数据量和10倍单节点的读写吞吐量。</p><p>但是如果分区不均，那么就会出现一些分区有更多的数据或读写，我们称之为 <strong>偏斜</strong>，这会使得分区后并没有得到很大的效率提升。在极端情况下，所有的负载如果都落在一个分区，使得该分区负载过高，我们称之为 <strong>热点</strong>。</p><p>所以，为了避免偏斜和热点的产生，以键值数据的分区为例，讨论如何将数据分区做得妥当。</p><h4>2.1 根据键的范围进行分区</h4><p>我们可以根据键值的范围进行分区，比如说我们以26个英文字符划分26个分区，之后根据键值首字母对它们进行分区。通常情况下，键值并不是均匀分布的，这会造成按照首字母分区之后，发生数据偏斜。为了均匀分配数据，分区的边界需要根据数据分区的实际情况再进行调整。</p><h4>2.2 散列分区</h4><p>一个好的散列函数可以将数据均匀分布，避免发生偏斜。但是这也带来了问题：我们没有办法再进行高效的范围查询。</p><h3>3. 热点消除</h3><p>避免热点最简单的方法是将数据记录进行散列分区，记录因此会在所有节点上平均分配。</p><p>但是它并不能完全避免热点的产生，因为如果所有的读写操作都是针对同一个键的话，那么所有的请求还是会被路由到同一个分区。比如说有一个百万粉丝的博主发布动态，该动态根据博主ID的键值进行分区，如果此时有大量的粉丝对该动态进行互动，那么哈希策略会把这些请求都路由到同一个分区进行操作，发生热点事件。</p><p>其实，我们还可以在该热点键上再进行分区，以避免热点：在主键的最后拼接随机数，两位十进制的随机数就能把一个主键分成100个不同的主键，从而存储在不同的分区中，这就完成了热点消除。但是主键被分割后，任何读取工作都必须在每次读取时将所有的数据拉出去合并到一起再返回结果。</p><h3>4. 分区再平衡</h3><p>如果保存某分区数据的服务器故障，需要使用其他服务器接管或想将目前的服务器换成性能更好的服务器，那么就需要进行 <strong>分区再平衡</strong>。</p><p><strong>分区再平衡</strong> 是将负载从集群中的一个节点向另一个节点移动的过程。执行再平衡需要满足以下要求：</p><ul><li>再平衡期间，数据库应该继续接受读取和写入</li><li>节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘的IO负载</li><li>再平衡之后，负载应该在集群中的节点之间公平地共享</li></ul><p>比较简单的再平衡分区策略是选择 <strong>固定数量的分区</strong>，当节点数量增加时，可以从原节点中 <strong>窃取</strong> 一些分区（当节点数量减少时，则发生相反的情况），如下图所示：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447286" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>在这种配置中，分区的数量通常在数据库第一次建立时确定，操作比较简单，之后不会改变，因此你需要选择足够多的分区以适应未来的增长。但是，每个分区也有管理开销，所以选择太大的数字会适得其反。</p><p>除此之外也可选择 <strong>动态分区</strong>，根据配置的分区大小，当超过该阈值时，可以将该大分区分割成两个小分区，能够使 <strong>分区数量适应总数据量</strong>。在大型分区拆分后，可以将其中的一半转移到另一个节点上，以平衡负载。</p><p>还有一种 <strong>根据节点数增加来进行分区</strong> 的方法：每个节点上有固定的分区数，当节点增加时，分区将变小，新增的节点会从原有节点的分区中随机进行拆分，最终这个新节点获得公平的负载份额。</p><p>分区再平衡可以 <strong>手动执行</strong> 也可以 <strong>自动执行</strong>。自动再平衡比较方便，因为不需要人工维护，但是它的执行过程是不可预测的：再平衡时将大量数据集从一个节点转移到另一个节点的过程中可能会产生很大的网络开销，这会使得该服务器对请求响应的性能降低，对用户的体验和生产造成负面影响。所以再平衡的过程有人参与是一件好事，这样能防止发生运维问题。</p><h3>5. 请求路由（服务发现）</h3><p>当我们已经将数据进行分区后，如何才能知道用户想要的数据在哪个节点上？这可以概括为是一个 <strong>服务发现</strong> 的问题。为了解决这个问题，可以通过如下图所示的三个方案</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447287" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><ol><li>允许访问所有的节点，如果第一个访问的节点有该键值，则处理该请求，否则将该请求转发到适当的节点上，这个方法避免了使用注册中心中间件，但是实现比较复杂</li><li>使用分布式的协调服务，用户将所有的请求发送到路由层，由路由层将该请求转发到合适的节点</li><li>要求用户（客户端）自己知道分区和节点的分配</li></ol><p>但是这其中还隐藏着一个问题：<strong>作出决策的组件（节点之一、路由层或客户端）是如何了解数据在节点间的分配变化的</strong>？这就需要一个独立的协调服务，比如使用 zookeeper 来跟踪元数据，如下图所示</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447288" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>每个节点都会在 zookeeper 中进行注册，zookeeper 中维护有节点到各个分区的可靠映射，负责决策的组件在 zookeeper 中订阅这个消息。当分区分配发生改变时，zookeeper 就会通知负责决策的组件更新路由信息，使其保持在最新的状态。</p><p>除此之外也可以在各个节点间采用 <strong>流言协议</strong> 来传播集群状态的变化，这样每个节点都维护有最新的数据路由方案，当其中一个节点收到请求时，会将其转发到合适的分区节点上（对应服务发现的方案一）。</p><hr/>]]></description></item><item>    <title><![CDATA[宝剑锋从磨砺出——零售数据库内核，为大促]]></title>    <link>https://segmentfault.com/a/1190000047447290</link>    <guid>https://segmentfault.com/a/1190000047447290</guid>    <pubDate>2025-12-03 19:03:32</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><em>癸卯七月风雨大作</em></p><p><em>京东零售·袁博文</em></p><p><em>僵卧双九不自哀，尚思为东戍轮台。</em></p><p><em>夜阑卧听珊瑚雨，铁马内核入梦来。</em></p><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047447292" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><p>﻿﻿</p><p><strong><em>前言略长，只关心技术的同学可直接跳过看第二章</em></strong></p><h2><strong>一、前言：技术的底色是什么？</strong></h2><p>这个问题在技术人心中其实没有标准答案，每个人都有每个人的见解。架构师眼里大抵是高屋建瓴，统领全局；技术大牛的视角可能是剖根溯源，精刀细琢；新人小白或许更单纯，无非就是学习进步，快速成长为大牛之类了。</p><p>但在我——一个京东数据库人的眼里，技术的底色或许应该是五彩斑斓的吧。</p><h4><strong>白是纯粹的起点</strong></h4><p>经常听人说，每个人呱呱坠地那一刻，都是一张白纸，父母在其上着墨。对于技术人来说又何尝不是呢？初学一门技术，初入一个领域，每个人都是一张白纸，在这张白纸上是随意草稿涂鸦，还是认真吸收不断进步，都取决于自己。</p><p>数据库内核技术，在 2020 年初，于我个人于内核团队于京东而言都是一个纯白的起点。自此开始探索数据库内核的每一行源码、每一个模块，然后攻关研究每一个技术难点，再设计实现云原生的珊瑚数据库直到其落地承接业务。我和我们团队的小伙伴都可以拍着胸脯说，我们无愧于京东，无愧于这份纯白。</p><h4><strong>青是朝气是成长</strong></h4><p>内核团队每一个小伙伴，不论是社招还是校招，都是那么朝气蓬勃，都对数据库内核技术求知若渴。腾龙认真钻研探索，成功打通了数据库测试用例线上部署和初始的内核监控框架；福哥将华为的优良编码风格带入团队并影响了许多小伙伴，还在DDL模块钻研并颇有造诣；海鹏探索并打通了内核与JED接口，为高可用付出良多；金蓬初入团队，甚至连技术栈都是初学，抱着一本经典的《<em>C++ Primer</em>》边啃边研究珊瑚数据库内核源码，但不妨碍进步速度惊人，最终能够独当一面；海波攻关的修改缓冲影子页技术以及共享集群测试框架至今还在持续带来价值；珊哥和齐哥更不用说，一个将多年积累的开发经验与珊瑚数据库内核模块深入结合，做出了诸多贡献；另一个不但对元数据锁研究透彻，更是独自一人承担了整个珊瑚数据库的工程化落地与高可用及运维工具建设。作为校招生的彭凯和宇歆，更是在短短的时间内，迅速成长，深入研究SQL词法和语法解析，以及主从复制模块，并为新产品的铸剑做出了突出的贡献。现在，越来越多的朝气蓬勃的新伙伴陆续加入了我们团队，大家的快速成长都有目共睹。</p><p>大家都从当初的青涩小白，成长成了各个内核领域的专家，或者独当一面的人才。所以青这个底色，一定是技术人努力成长，拼搏向上的颜色吧。</p><h4><strong>黄是最后的执着</strong></h4><p>在眼看京东数据库内核团队蒸蒸日上，大家在内核领域日渐深耕的时候，不出意外的还是出意外了……</p><p>集团层面的架构调整，让零售和科技的技术团队不得不融合成一个团队了，我想初衷肯定是好的，大家也都为之努力过。但出于种种不便明说的原因，数据库内核团队成了大的架构齿轮磨合下的那个代价，团队动荡，未来不明，无奈之下许多初露锋芒的优秀小伙伴不得不做出各自的选择。就在我以为京东数据库内核就要黄了的时候，不幸中的万幸，在零售众多大佬同事的全力保护下，内核的种子留了下来，静待花开。而属于技术人的这份坚守，或许就像鹅卵黄一样，等待破壳重生的那一刻吧。</p><h4><strong>赤是对技术的热忱</strong></h4><p>如果希望有颜色，那么一定是红色！</p><p>就像赤色当年卧薪尝胆，艰苦奋斗，爬雪山过草地，把希望带给神州大地一样。属于京东技术的赤色，也在京东技术中心迎来新的大家长后随之到来。我不知道其他团队是不是有类似的感受，但数据库团队在回归零售以后，大家的心气神都不一样了，对技术那颗火热的心又重新燃了起来。数据库团队也迎来了新leader：一位在数据库领域有着二十年经验的超级大佬和一位在数据库内核领域有十多年经验的资深大佬。在两位大佬的带领下，我们开始朝着新的方向前进。</p><p>同时，数据库内核团队也很快迎来了越来越多的新鲜血液：来自其他大厂的林康、正茂、张扬，将他们所掌握的数据库内核以及工程化经验引入，为我们内核的研发装上了加速器；来自各大名牌高校的校招生以及实习生晓冰、江昊、一贤、祖才等等，也都快速学习迅速成长，以最饱满的热情融入我们团队并做出了相应的贡献。</p><p>大家都饱含赤诚，携手开始向未来进发！</p><h4><strong>黑是五彩斑斓的未来</strong></h4><p>始于白，终于黑。就像太极阴阳鱼一样，生生不息，周而复始。技术也一样！</p><p>自然界当所有的颜色混在一起后，只有一个颜色——黑。数据库内核的团队也在沉淀和挫折中更加强大，随着不断补充新鲜的血液，从市场上吸引更多优秀的数据库内核人才，当所有技术的底色混在一起后，所有的五彩斑斓，所有的初心、成长、坚守、希望融为一体后，所有的不同领域的人才齐心协力共渡难关后，那结合在一起的力量，其实就只剩下未来那无限的可能——五彩斑斓的黑。内核技术的深渊也如黑洞般，深不见底，等待我们去探索。但我相信，只要我们秉持技术人的底色，就一定可以达到那个彼岸！</p><h2><strong>二、正篇：五彩熔炉，铸剑！</strong></h2><p>正篇开始！</p><p>抱歉大家，前面扯了这么多其实只是前言。但我又不想像以前写前言那样，只是简单的交代一下背景。花了五节的笔墨介绍我心中的技术底色，只希望大家能懂一点——我们会以最大的热情和最强的技术为京东打造基础数据库产品，为大家带来更优质的数据库服务。</p><h4><strong>到底铸了什么剑？</strong></h4><p>属于我们京东电商版本的自研数据库内核——DongSQL！</p><p>五年前，数据库内核团队立项直接瞄准了新的数据库形态——云原生关系型数据库，也就是存算分离共享存储架构的珊瑚数据库(shared storage)，这一版技术难点主要是在共享存储的架构以及云原生的数据一致性，其产品价值主要是在节约数据库成本以及极致的云上资源伸缩性等。但由于与存量JED库(shared nothing)采用了不一样的技术架构，所以面临一个现实问题——存量用户版本无法平滑升级。</p><p>用过或者了解数据库的人都知道，有的时候不是大家不想使用更新的版本，更强的性能，更优秀的功能，而是数据库本身太基础太重要了，如果业务系统已经建设很多年，与数据库绑定太深的话，更多还是求稳为主，能不动则不动。这不是京东独有的情况，可以说整个行业皆是如此，这叫技术惯性。正是因为采用了新的技术架构，带来了一个问题：存量业务如果要使用必须进行数据库的迁移。就这一个原因，很多业务就望而却步。</p><p>正是由于这个原因，在新leader带领我们团队以后，基于丰富的数据库经验，敏锐地察觉到京东整个数据库的基本盘其实是存量的数据库，解决存量数据库用户的问题才能带来更大的价值。再优秀的产品，如果没人用一样白费力气。</p><p>因此，我们需要做的是，一个完全适配存量架构的数据库内核，不引入更复杂的架构变更和过多的设计，只在其基础上对数据库内核性能进行优化、对配套能力进行提升、对零售电商场景进行针对性扩展，完美支持JED以及DongDAL，秉持稳定性和兼容性为前提的基础上，让京东的数据库内核更好用，更强大！</p><h4><strong>电商场景下数据库痛点的解决之道</strong></h4><p>电商场景的数据库需求其实是用户最迫切的，因此我们在首选开刀方向时，没有选择引入花里胡哨高大上的功能等角度。而是从用户中来，回到用户中去，深入分析目前线上用户最常见的问题，以及大促最常见的故障场景，针对性的引入了内核层新的解决方案。</p><h5><strong>问题一：“过载” 大促激增的流量，或者超时SQL不断重试直接把数据库CPU打满甚至打挂</strong></h5><p>﻿<img referrerpolicy="no-referrer" src="/img/remote/1460000047447293" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/><br/><img referrerpolicy="no-referrer" src="/img/remote/1460000047447294" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>这种场景真的非常常见，甚至前段时间还有一个白虎故障就是类似的原因。业务研发在设计功能的时候，其实是无法预知线上生产环境真实的流量的，或许可以设计应用侧限流，也或许可以加缓存抗量，但限流不是每个系统都有，即使有也可能存在疏漏，缓存如果被击穿那带给数据库的流量更是暴击。有的时候甚至不是真实暴增的流量，而只是超时机制的负反馈，失败的不断重试就带来了超出预期的数据库请求。</p><p>当请求流量突然暴涨时或者突发的慢sql占用大量资源时，它会像一个被瞬间涌入人群挤垮的服务台：每个新连接都需要数据库创建一个线程来处理，大量线程的创建、上下文切换和维持本身就会吃掉可观的内存和CPU；更重要的是，每个查询进来，内核都要疯狂工作——解析复杂的SQL语句、在成千上万条索引条目中查找路径、拼凑关联多张表的数据、进行排序分组计算、管理事务保证一致性（这涉及到频繁的加锁解锁，高并发时极易堵塞排队）、还要不断从磁盘读取数据或把改动写回去。所有这些操作都是极度消耗CPU算力的密集计算。当每秒涌入的请求远超CPU能处理的速度时，CPU就会被完全占满，所有查询都挤在一起排队等待计算资源。与此同时，高并发下锁冲突剧增，大量线程因等待锁而阻塞却不释放资源；内存可能被临时表、排序缓存塞爆；严重时磁盘IO也跟不上。最终，CPU被彻底耗尽，新连接无法建立，已有查询完全卡死，整个数据库进程失去响应，就像被“打挂”了一样，本质上就是所有关键资源（CPU、内存、IO、连接）在瞬间洪峰下被彻底榨干导致的系统性崩溃。</p><p>原因很清楚，解决方式也很简单，前面也提到了，限流即可，可实际生产环境操作起来还是会出现诸多困难。</p><p>业务层自行限流面临的主要挑战在于其“粗放”和“滞后”。它通常只能基于简单的请求频率或用户维度（如QPS）进行拦截，无法洞察数据库内部真实的瓶颈所在（比如是在CPU、内存、磁盘IO还是锁冲突）。这极易导致“误杀”——核心的重资源消耗型SQL可能未被拦住，反而大量高频但轻量的请求被限流，牺牲了业务可用性却未能真正缓解数据库压力。同时，在分布式微服务架构下，协调各个服务模块统一、实时地实施并调整限流策略异常困难，很容易出现限流不一致或响应迟缓，当业务层感知到数据库响应变慢或报错再触发限流时，往往已经错过了最佳干预时机，雪崩可能已经发生。</p><p>目前的实际操作往往是高可用程序或者DBA依靠HA机制进行主备切换来应对过载。切换过程本身必然导致数秒到数十秒的服务中断（连接闪断、短暂只读），对连续性要求高的业务会造成直接影响。更重要的是数据一致性问题：主库在故障或过载瞬间可能存在未同步到备库的事务数据，切换后这些数据可能永久丢失（异步复制下），即使使用半同步复制也可能因网络问题阻塞写入或退化为异步。历年大促线上生产环境不少故障甚至是发生在切换操作之后(普通RDS集群以及低版本vitess集群风险尤其显著)。</p><h5><strong>解：SQL自提示实现精准限流</strong></h5><p>基于以上痛点，不少用户提出，如果可以实现精准限流就好了，既能在业务根据流量预测的基础上预防性限流，又能在过载发生后根据简单排查的结果定向限流。有求必应——Hint限流方案横空出世！</p><pre><code>// 根据特定 SQL 指纹进行限流 
update/*+ ccl_queue_digest(INT&lt;当前语句的并行数&gt;) */ t set col1 = col1+1 where 1=id; 
update/*+ ccl_queue_digest() */ t set col1 = col1+1 where 1=id;
</code></pre><p>数据库内核自身支持限流的核心优势是，我们能深入到SQL执行层，根据用户指定规则（如匹配特定SQL指纹、或者SQL语句全文等不同模式规则）实时识别并优先抑制那些真正“吃掉”大量资源的“罪魁祸首”查询。这如同在数据库引擎内部安装了一个智能节流阀，直接从源头（消耗资源的查询）进行精准控制，避免了业务层限流的盲目性和HA切换的破坏性。它能在资源紧张初现端倪时就主动干预，最大限度保障核心业务请求的通过和系统整体的稳定性，且由内核统一管理，规则生效及时、策略执行高效。</p><h5><strong>问题二：“秒杀” 单点高频写入带来的数据库性能下降，以及库存一致性问题</strong></h5><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447295" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>秒杀是电商业务非常常见的场景，无论秒杀业务是否设计缓存前置抗量，库存数据的最终变更都是需要落到数据库的。如果缓存发生击穿，更是需要数据库来进行兜底策略。但秒杀这个场景的数据库操作又极其特殊，甚至可以说会导致传统数据库痛点集中爆发。</p><p>首先，高频单行更新使行级锁竞争成为致命瓶颈：当海量请求同时扣减同一商品库存时，存储引擎的行锁强制串行更新，导致线程在锁等待中堆积；死锁检测机制在队列过长时（如超1000线程）触发深度遍历，CPU资源被疯狂消耗，事务响应时间骤增甚至超时。</p><p>其次，高频事务的ACID保障带来巨大开销：事务在数据库内核中是核心能力之一，在秒杀场景下往往都是简单事务，但为了保证查询更新的一致性，又不得不开显式事务(非auto commit)，而显式事务的BEGIN、Statement、COMMIT/ROLLBACK，每一个子句都会完整的经历应用侧到数据库底层的多级转发和网络开销，伴随多次网络交互（跨节点延迟加剧堵塞）及日志写入，单事务耗时飙升，系统吞吐量断崖式下跌。</p><p>最后，秒杀场景的库存扣减不允许出现意料外的更新：传统数据库的高并发扣减需通过SELECT检查库存后再执行UPDATE，但两步操作存在时序漏洞——高并发下多个请求可能同时读到相同库存值，导致超卖；同时所有请求（包括库存不足的无效请求）均需竞争同一行锁，引发线程堆积和死锁检测的CPU暴增。</p><h5><strong>解：电商秒杀场景定制优化</strong></h5><p><strong>秒杀排队</strong>：高频更新问题很好解决，借用限流的思路，只不过秒杀场景要限的是具体的字段甚至是具体的值，因为高频SQL是集中在具体数量的库存或者单一品类上的，要改的可能就几行甚至是一行数据。因此，我们借用了限流的Hint语法，业务只需要在预期秒杀需要更改的具体SQL上，加上对应的Hint规则，约定具体字段或者具体值需要进行限制排队执行，数据库内部就会对秒杀类的SQL进行管理排队，极大程度的规避了行锁的竞争以及其连锁反应，经测试单行更新高并发场景下，比传统数据库的流量能提升一倍以上。</p><pre><code>// 根据热点值限流
 update/*+ ccl_queue_value('茅台') */ t set c=c+1 where name ='茅台'; 
 // 根据热点字段限流
  update/*+ ccl_queue_field(order_id) */ t set c=c+1 where order_id =1and name ='茅台';
</code></pre><p><strong>事务快速提交/回滚</strong>：针对秒杀事务的特性，设计了事务快速提交回滚的Hint，即用户在事务COMMIT/ROLLBACK前的最后一个SQL语句上，如果加上该Hint，则内核即明白该操作提交或者回滚了。此方案在秒杀场景下，尤其是特定单行更新的场景下，最高可以提升 3 倍以上的性能！优势非常明显。</p><p><strong>影响行数约束</strong>：秒杀场景库存扣减，或者其他非秒杀场景也可能存在，业务侧的逻辑明确知道某条SQL更新后应该影响几行数据，如果数据库执行完发现影响的行数不符合预期则大概率出现问题了，需要将事务进行回滚。我们设计了预期影响行数的Hint，通过该Hint（示例 UPDATE /*+ TARGET\_AFFECT\_ROW(1) */ stock SET count=count-1 WHERE id=100 AND count&gt;=1），可同步实现两大核心优化：</p><p>其一，引擎在加锁前优先校验 WHERE 条件（库存≥1），仅当库存充足时才尝试加锁更新，库存不足的请求直接返回影响行数=0，避免无效锁竞争；</p><p>其二，库存检查与扣减压缩为单原子操作，确保影响行数严格为1才成功，否则自动失败，彻底杜绝跨事务的脏读与超卖风险。当然也可以配置其他数值，只要与您预期的影响行数一致即可。</p><h5><strong>问题三：“缓存更新一致性问题” 业务前置缓存失效时，会直接更新数据库，然后查询已更新数据并返回</strong></h5><p>许多业务系统会在数据库访问层之上引入缓存，例如京东的分布式缓存JIMDB，以利用其极致的读写响应速度优化用户体验。然而，缓存的易失性本质决定了其无法独立承担关键数据的持久化职责——数据库始终是不可或缺的兜底保障（除非数据可容忍丢失）。维护缓存与数据库之间的强一致性是系统设计的核心挑战，当缓存失效导致请求穿透至数据库时，业务常需同步获取刚更新的数据并实时刷新缓存或响应前端。传统数据库在此场景下存在显著局限：若要在事务中确保更新后立即可见且数据一致，必须在DML操作后紧跟一条SELECT语句进行查询。但即便采用此方案，在读已提交（RC）隔离级别下，其他事务的并发修改仍可能导致该查询读到不一致数据，无法满足严格的实时一致性要求。</p><h5><strong>解：实现RETURNING语法</strong></h5><p>我们通过实现RETURNING语法解决这一问题：在UPDATE/INSERT等DML语句末尾追加RETURNING子句，就能直接获取修改后的完整行数据。比如库存扣减场景下，一条UPDATE inventory SET stock=stock-1 WHERE id=100 RETURNING *;语句既完成了原子扣减，又能立即返回最新库存值，无需额外SELECT查询。</p><p>这一内核级优化不仅消除了RC隔离下的并发脏读风险（DML与返回数据基于同一事务快照，其他事务的并发修改不会干扰结果），还将“更新 + 查询”的两次网络交互压缩为单次请求，把事务耗时再降一个级别。对缓存架构而言，业务侧拿到RETURNING返回的实时数据后，能立刻刷新缓存层，在事务提交时就完成数据对齐，让秒杀、大促等高并发场景下的“缓存击穿兜底逻辑”，既快又稳。</p><h5><strong>问题四："执行计划漂移" 好好的SQL突然就慢了</strong></h5><p>这个问题真的让人头疼，一条SQL在开发环境跑得飞快，到了线上就变成了蜗牛。更要命的是，有时候同一条SQL，今天还好好的，明天就突然慢得要死。</p><p>举个例子，我们有条订单查询的SQL：</p><pre><code>SELECT o.*, u.name  
FROM orders o  
JOIN users u ON o.user_id = u.id  
WHERE o.create_time  
BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN('PAID','SHIPPED') 
 ORDER BY o.create_time DESC LIMIT 100;
</code></pre><p>平时这条SQL毫秒级就能出结果，用的是<code>orders.idx_create_time</code>索引。但有一天大促期间，这条SQL突然开始走全表扫描，30秒才能跑完，直接把系统拖垮了。</p><p>为什么会这样？数据库优化器是个"聪明"的家伙，它会根据表的统计信息来选择执行计划。但问题就出在这些统计信息上——<code>ANALYZE TABLE</code>更新了统计信息，数据分布发生了变化，或者系统负载影响了成本计算，优化器就可能突然"变心"，选择一个完全不同的执行路径。</p><p>这种情况在大促期间特别危险，数据量激增、系统负载变化，一条核心查询的执行计划突然劣化，整个系统可能就垮了。</p><p>传统的解决办法要么重启数据库（代价太大），要么业务研发加Hint强制索引（破坏代码可维护性，还得紧急上线，时间周期长），要么调优化器参数（可能影响其他SQL），都不是很好的选择。</p><h5><strong>解：Statement Outline执行计划固化功能</strong></h5><p>为了解决这个问题，我们实现了Statement Outline功能，可以把稳定高效的执行计划"固化"下来，让优化器按照我们指定的方式执行。这个功能通过存储过程包来管理，使用起来很简单。比如我们发现某个查询有个很好的执行计划，就可以把它记下来，一旦发生上述意外场景，可以立即将其注入数据库从而稳定该类型SQL的执行：</p><pre><code>-- 添加优化器hint的outline
 CALL dbms_outln.add_optimizer_outline(   
 'your_db',                                     -- 数据库名称    
 '',                                            -- SQL语句的摘要，为空时自动计算      
 1,                                             -- 位置，通常为1     
 '/*+ USE_INDEX(orders idx_create_time) */',    -- 优化器提示文本      
 'SELECT o.*, u.name        FROM orders o       
 JOIN users u ON o.user_id = u.id        
WHERE o.create_time BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN ('PAID', 'SHIPPED')       
ORDER BY o.create_time DESC LIMIT 100;'       -- SQL语句文本); 

-- 添加强制索引的outline   
CALL dbms_outln.add_index_outline(      'your_db',                                     -- 数据库名称     
 '',                                            -- SQL语句的摘要，为空时自动计算      
1,                                             -- 位置，通常为1     
 'USE INDEX',                                   -- 索引提示类型，如'USE INDEX'、'IGNORE INDEX'等     
 'idx_status',                                  -- 索引列表，多个索引用逗号分隔      
 '',                                            -- 索引提示选项，如'FOR JOIN'、'FOR ORDER BY'等     
'SELECT o.*, u.name       
 FROM orders o        
JOIN users u ON o.user_id = u.id        
 WHERE o.create_time BETWEEN '2025-10-01' AND '2025-10-30' AND o.status IN ('PAID', 'SHIPPED')        
 ORDER BY o.create_time DESC LIMIT 100;'       -- SQL语句文本);
</code></pre><p>这样一来，即使统计信息变化了，优化器也会按照我们固化的执行计划来执行，保证查询性能的稳定性，让我们能够精确控制查询的执行方式。对于那些业务关键的SQL，这个功能简直是"定海神针"，彻底解决了执行计划漂移的问题。</p><p>Outline还可以注入自定义的hint，比如“问题一”中的解决过载问题hint或者“问题二”中的秒杀场景hint。</p><h5><strong>问题五："线程拥堵" 每连接每线程的弊病</strong></h5><p>传统数据库是没有线程池的，每个连接都要创建一个独立的线程来处理，常规场景每连接每线程还很稳定，但高并发场景下就是灾难。</p><p>想象一下大促期间的场景：成千上万个连接同时涌入数据库，每个连接都要创建线程，线程创建和销毁的开销巨大，CPU忙着做上下文切换，真正用来处理SQL的时间反而不多。更要命的是，所有请求都是一视同仁，核心的支付查询可能被大量的日志写入、报表查询这些不紧急的请求给"淹没"了。在JED架构下，由vitess控制了连接的数量，这个问题还不大，但目前DongDAL直连DongSQL的架构，这个就成了不得不面对的重点问题！</p><p>线程拥堵的问题看起来和过载很像，但还略有一点区别。过载场景可以精确识别到个别问题SQL，并进行精准限流，从而保证不影响其他SQL。而线程拥堵的大部分甚至所有连接都是正常SQL，没有谁是受害者，只不过突发流量真的太大了！所以这种场景，我们就不得不祭出大杀器——线程池！</p><h5><strong>解：DongSQL线程池</strong></h5><p>我们实现了完整的线程池功能，能够有效复用线程资源，避免频繁创建和销毁线程的开销。线程池会维护一组工作线程，新来的连接请求会被分配到空闲的线程上处理，这样就能大大减少上下文切换，提升高并发场景下的性能。</p><p>这样一来，来自核心服务器/核心用户的请求就能优先得到处理，不会被其他不那么紧急的请求给挤占了。系统会智能识别高优先级连接，确保关键功能的响应时间。</p><p>这些优化功能的加入，让DongSQL在高并发、大数据量的零售电商核心场景下展现出了更强的稳定性和性能。每一个功能都是我们在实际业务中遇到问题、分析问题、解决问题的结果，希望能够帮助更多的团队应对类似的挑战。</p><h2><strong>三、结语：技术的成色又是什么呢？</strong></h2><p>如果说技术的底色，是求知、是成长、是执着、是热忱、是我们所有技术人团结在一起爆发出的力量。</p><p>那么技术的成色，一定有脚踏实地，追根溯源，不浮于表象，而深入骨髓地解决根本问题。正所谓：求木之长者，必固其根本；欲流之远者，必浚其泉源。对于数据库，则必须具备掌控数据库内核的能力，方能使自身以及其上承接的业务行稳致远。</p><p>除此之外，更宏观的维度，技术的成色我想应该就是为团队、为公司、为用户、乃至为社会产生实实在在的价值吧！正如公司使命说的那样：<strong>技术为本，让生活更美好！</strong> 让我们携手所有业务研发团队做实事、有价值的事、长期的事，为京东的 35711 梦想付出我们自己的一份力！</p>]]></description></item><item>    <title><![CDATA[京东自研电商数据库内核DongSQL简介]]></title>    <link>https://segmentfault.com/a/1190000047447297</link>    <guid>https://segmentfault.com/a/1190000047447297</guid>    <pubDate>2025-12-03 19:02:55</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>团队于今年(2025.9)打磨出了深度优化的自研数据库内核——DongSQL V1.1.0。</p><p><em>[如果对前因后果比较感兴趣，可以移步上一篇文章</em><a href="https://link.segmentfault.com/?enc=b8iieQobQHtoRuquj86ynA%3D%3D.j9zGvy7UJnAHF2DgwGUof%2BTbzUQcBTjmr%2FPWkZ6wClQXbxVD6B6MZONnGjtFDB%2FWYRkd9hfLGI5nT7hSZ5OL1wfjYdlCNfBVflEeecsJ6eMvcnsc9sPubbhTI94bTsED2%2Ba3NCAbirYWy5U%2Fjhfibg%3D%3D" rel="nofollow" target="_blank"> <em>《宝剑锋从磨砺出——零售数据库内核，为大促铸剑！》</em> </a><em>]</em></p><p>本文将深度解析DongSQL在语法扩展、并发控制、查询优化等方面的内核改造，以及在电商场景下的优化实践。</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/c89b566ef9c14de6a8b85177e1ba9116.webp" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h2>1、DongSQL在语法扩展上的优化</h2><h3>1.1. RETURNING子句功能</h3><p><strong>▶︎ 语法扩展创新</strong>：DongSQL在标准SQL语法基础上扩展了RETURNING子句，这是重要语法创新。RETURNING子句允许DML语句(INSERT、UPDATE、DELETE、REPLACE)在执行数据修改操作的同时返回受影响的行数据，无需额外查询。</p><p>传统数据库在执行DML操作后，如果需要获取操作结果，必须执行额外的SELECT查询，这在高并发场景下会产生额外的网络往返开销。DongSQL通过RETURNING子句彻底解决了这一问题。</p><pre><code>-- INSERT操作返回自增ID 
INSERT INTO orders (customer_id, order_date) VALUES (1001, NOW()) RETURNING order_id; 

-- UPDATE操作返回更新后的数据 
UPDATE products SET price = price * 1.1 WHERE category = 'electronics'  
RETURNING product_id, name, old_price, price; 

-- DELETE操作返回被删除的记录 
DELETE FROM expired_sessions WHERE expire_time &lt; NOW()  
RETURNING session_id, user_id, expire_time;
</code></pre><p><strong>▶︎ 性能提升效果</strong>：经测试验证，RETURNING子句在不同场景下都能带来显著的性能提升：</p><p>•<strong>固定行更新场景</strong>：16并发时TPS提升61%，响应时间降低44%</p><p>•<strong>随机行更新场景</strong>：128并发时TPS提升18%</p><p>•<strong>大规模更新测试</strong>：2000万次操作中平均TPS提升5-10%</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/df95c690bb17463caf7cb548407575a9.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p><strong>▶︎ 生产落地预期</strong>：该功能与DongDAL发号器逻辑高度匹配，有望将发号器性能瓶颈大幅提升(DongDAL团队配套开发推进中)</p><h3>1.2. Hint语法扩展</h3><p><strong>▶︎ 多样化Hint支持</strong>：DongSQL扩展了Hint语法体系，提供了针对电商场景的专用提示功能，包括并发控制、库存管理等领域特定的优化。</p><p><strong>▶︎ Inventory Hint</strong>：专门针对电商库存管理场景设计的提示语法，提供目标影响行数控制、自动提交/回滚等特性。</p><pre><code>-- 库存扣减：确保只影响一行，成功自动提交，失败自动回滚
 UPDATE /*+ TARGET_AFFECT_ROW(1) COMMIT_ON_SUCCESS ROLLBACK_ON_FAIL */
  inventory SET stock = stock - 5 
   WHERE product_id = 1001 AND stock &gt;= 5;
</code></pre><p><strong>▶︎ 性能提升数据</strong>：在16并发的库存扣减场景下，使用Inventory Hint比不使用hint性能提升215%。</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/190309a42b3743acb7143ab2223efb61.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>2、DongSQL在并发控制上的优化</h2><h3>2.1. CCL并发控制</h3><p><strong>▶︎ 多维度限流机制</strong>：DongSQL实现了CCL(Concurrency Control)并发控制功能，通过多维度的限流策略，有效解决电商秒杀场景下的热点数据访问问题。</p><p>传统数据库在面对高并发热点数据访问时，往往会因为激烈的锁竞争导致性能急剧下降，甚至系统雪崩。DongSQL的CCL通过智能排队机制，将无序的并发请求转换为有序处理，从根本上解决了这一问题。</p><p><strong>▶︎ 多维度控制策略</strong>：</p><p>•<strong>基于字段的限流</strong>：<code>ccl_queue_field(column_name, concurrency)</code>，对特定字段值进行并发控制</p><p>•<strong>基于值的限流</strong>：<code>ccl_queue_value(value, concurrency)</code>，对特定数据值进行精准限流</p><p>•<strong>基于SQL指纹的限流</strong>：<code>ccl_queue_digest(concurrency)</code>，对相同SQL模式进行统一管控</p><pre><code>-- 对商品ID为999的热门商品进行限流，并发度限制为5
 SELECT /*+ ccl_queue_value(999, 5) */ * FROM products WHERE product_id = 999; 
 
 -- 对库存扣减操作按商品ID进行限流 
 UPDATE /*+ ccl_queue_field(product_id, 8) */ inventory SET stock = stock - 1 WHERE product_id = ?; 
 
 -- 对相同SQL模式进行统一限流 
 SELECT /*+ ccl_queue_digest(10) */ * FROM hot_products WHERE status = 1;
</code></pre><p><strong>▶︎ 性能突破数据</strong>：</p><p>•<strong>秒杀场景优化</strong>：在4096并发下，使用CCL限流后TPS从573提升至1337，性能提升133%</p><p>•<strong>系统稳定性</strong>：有效防止系统雪崩，将无序并发转换为有序处理</p><p>•<strong>热点缓解</strong>：通过队列机制显著降低热点数据的锁竞争</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/7695dd1288ba42c6a1d4d6d79a87ba66.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>2.2. Statement Outline执行计划及自定义提示管理</h3><p><strong>▶︎ 企业级计划稳定性</strong>：DongSQL提供了Statement Outline功能，用于固化重要SQL的执行计划，防止因数据变化导致的计划不稳定问题。</p><p><strong>▶︎ 自定义Hint注入工具</strong>：包括但不限于上述秒杀、CCL限流场景的Hint，即使业务研发预期外的过载或者突发流量发生，应急情况下DBA也可以通过Statement Outline功能对问题SQL进行干预</p><pre><code>-- 为重要SQL固化执行计划 
CALL dbms_outln.add_index_outline(  
'test_db', '', 1, 'USE INDEX', 'idx_status', '',  
 'SELECT * FROM orders WHERE status = "PAID"' 
 ); 
 -- 为特定查询添加ccl_queue_digest限流hint，限制并发度为2 
CALL dbms_outln.add_optimizer_outline(  
'test_db', '', 1, '/*+ ccl_queue_digest(2) */',  
'SELECT * FROM orders WHERE customer_id = 1001' );
</code></pre><p><strong>▶︎ 核心价值</strong>：</p><p>•<strong>性能稳定性</strong>：保障核心SQL性能不因数据变化而波动</p><p>•<strong>智能限流</strong>：支持基于SQL指纹的手动限流和自动限流(自动限流默认不开启，需要开启的业务需单独申请)</p><p>•<strong>企业级管理</strong>：提供生产级的执行计划管理能力</p><h2>3、DongSQL在查询优化上的改进</h2><h3>3.1. 单点查询优化</h3><p><strong>▶︎ 查询路径优化</strong>：DongSQL实现了单点查询bypass功能，针对主键等值查询这类高频简单查询，绕过部分SQL层处理逻辑，直接访问存储引擎，大幅提升查询性能。</p><p>电商场景中，商品详情查询、用户信息查询等基于主键的简单查询占据了很大比例。虽然这些查询逻辑简单，但在高并发下仍然消耗大量CPU资源。DongSQL的单点查询优化针对这一痛点进行了专项优化。</p><p><strong>▶︎ 性能提升数据</strong>：</p><p>•<strong>不同环境性能提升</strong>：容器环境提升20%，物理机环境提升30%</p><p>•<strong>高并发场景</strong>：当CPU达到瓶颈时，QPS提升20-28%</p><p>•<strong>资源效率</strong>：相同硬件配置下处理能力显著提升</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/9513743b2f4d49c1ba2df985f69131a1.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.2. 线程池优化</h3><p><strong>▶︎ 高并发处理能力</strong>：DongSQL实现了企业级线程池功能，通过智能线程调度和资源管理，显著提升了系统在高并发场景下的处理能力和稳定性。</p><p>传统数据库在面对大量并发连接时，会为每个连接创建独立线程，这在高并发下会导致线程切换开销过大、内存消耗激增等问题。DongSQL的线程池优化通过复用线程资源，有效解决了这些问题。</p><p><strong>▶︎ 调度机制</strong>：</p><p>•<strong>线程复用：</strong> 通过线程池复用减少线程创建销毁开销</p><p>•<strong>负载均衡：</strong> 分配任务到不同线程，避免热点线程</p><p>•<strong>优先级调度：</strong> 支持任务优先级，保障重要业务优先处理</p><p><strong>▶︎ 性能突破数据</strong>（基于8C32G测试环境，sysbench 16张表每张1000万行数据）：</p><p><strong>只读场景性能对比</strong>：</p><p>•<strong>低并发优势</strong>：32线程时，线程池模式QPS达到141,261，相比传统模式的110,658提升27.6%</p><p>•<strong>高并发稳定性</strong>：在512线程高并发下，线程池模式QPS保持131,939，而传统模式仅61,580，性能提升114%</p><p>•<strong>延迟控制</strong>：512线程时TP99延迟从传统模式的297.92ms优化到118.92ms，降低60%</p><p><strong>纯写场景性能突破</strong>：</p><p>•<strong>中等并发</strong>：64线程时QPS从46,577提升到57,655，性能提升23.8%</p><p>•<strong>高并发场景</strong>：512线程时QPS从29,541提升到58,166，性能提升97%</p><p>•<strong>超高并发</strong>：4096线程时QPS从28,571提升到54,687，性能提升91%</p><p><strong>读写混合场景优化</strong>：</p><p>•<strong>128线程</strong>：QPS从54,870提升到80,244，性能提升46%</p><p>•<strong>256线程</strong>：QPS从48,787提升到77,961，性能提升60%</p><p>•<strong>延迟优化</strong>：256线程时TP99延迟从196.89ms优化到158.63ms，降低19%</p><p><img referrerpolicy="no-referrer" src="https://i-blog.csdnimg.cn/direct/549ec324046c4332a50a1267bc614ee6.webp" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.3.  其他查询执行优化</h3><p><strong>▶︎ 执行路径优化</strong>：DongSQL在查询执行引擎层面进行了多项优化，包括算子优化、内存管理优化、并行执行优化等。</p><p><strong>▶︎ 缓存机制增强</strong>：优化了Buffer Pool管理策略，页面mutex优化，提升了数据访问效率，降低了I/O锁冲突。</p><h2>4、性能基准测试汇总</h2><h3>OLTP标准基准测试</h3><p>基于标准测试环境的性能数据（16C32G, 16张表，每张表100万行）：</p><table><thead><tr><th>测试场景</th><th>最佳线程数</th><th>TPS</th><th>QPS</th><th>TP99延迟</th><th>平均延迟</th></tr></thead><tbody><tr><td>只读查询</td><td>64</td><td>19,484</td><td>311,745</td><td>21.50ms</td><td>3.28ms</td></tr><tr><td>只写操作</td><td>256</td><td>17,004</td><td>102,025</td><td>29.72ms</td><td>15.05ms</td></tr><tr><td>插入操作</td><td>256</td><td>25,614</td><td>25,614</td><td>15.83ms</td><td>9.99ms</td></tr><tr><td>读写混合</td><td>128</td><td>9,795</td><td>195,908</td><td>33.12ms</td><td>13.06ms</td></tr><tr><td>点查询</td><td>64</td><td>560,933</td><td>560,933</td><td>0.18ms</td><td>0.11ms</td></tr></tbody></table><h3>电商场景专项性能汇总</h3><table><thead><tr><th>优化模块</th><th>测试场景</th><th>性能提升幅度</th><th>关键指标</th></tr></thead><tbody><tr><td><strong>RETURNING子句</strong></td><td>固定行更新</td><td><strong>61%</strong></td><td>TPS: 925→1,490</td></tr><tr><td><strong>CCL并发控制</strong></td><td>秒杀场景</td><td><strong>133%</strong></td><td>TPS: 573→1,337</td></tr><tr><td><strong>Inventory Hint</strong></td><td>库存扣减</td><td><strong>215%</strong></td><td>TPS: 1,537→4,843</td></tr><tr><td><strong>单点查询优化</strong></td><td>主键查询</td><td><strong>28%</strong></td><td>QPS: 76,432→98,470</td></tr></tbody></table><h2>5、未来规划</h2><p>1.<strong>持续语法扩展</strong>：基于业务需求继续扩展SQL语法功能</p><p>2.<strong>智能优化增强</strong>：引入机器学习优化执行计划选择</p><p>3.<strong>内核级技术支持</strong>：具备内核研发能力的团队，持续从最底层为业务研发提供深度优化的数据库解决方案</p><p>4.<strong>云原生存算分离</strong>：继续打造属于京东自己的高性能低成本数据库产品</p><h2>6、结语</h2><p>从开源内核到自研DongSQL，京东零售数据库团队始终以"业务价值驱动技术创新"为核心理念。DongSQL作为专为京东电商场景设计的数据库，通过语法扩展、并发控制、查询优化等多个模块的深度创新，为电商业务的快速发展提供了强有力的数据库技术支撑。</p><p>这些优化不仅提升了系统性能，更重要的是为集团基础技术底座提供了坚实的基础。未来，京东零售数据库团队将持续深耕数据库内核技术，让数据库更好地服务业务发展。</p>]]></description></item><item>    <title><![CDATA[【隐语Secretflow】如何在Doc]]></title>    <link>https://segmentfault.com/a/1190000047447329</link>    <guid>https://segmentfault.com/a/1190000047447329</guid>    <pubDate>2025-12-03 19:01:59</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047313097" alt="" title=""/></p><p>打开链接即可点亮社区Star，照亮技术的前进之路。</p><p>Github 地址：<em><a href="https://link.segmentfault.com/?enc=mgSg8qOBkT8nZEWmavarJw%3D%3D.qnMNscNimdQeQX8OcfrkqfXRScP0jigLhp6yHWwL1Uj0he14VWbB4n5VvkLZSMcz" rel="nofollow" target="_blank">https://github.com/secretflow/kuscia</a></em></p><h2>前言</h2><p>本教程帮助您在多台机器上使用 <a href="../../reference/architecture_cn.md#点对点组网模式" target="_blank">点对点组网模式</a> 来部署 Kuscia 集群。</p><p>当前 Kuscia 节点之间只支持 Token 的身份认证方式，在跨机器部署的场景下流程较为繁琐，后续本教程会持续更新优化。</p><h2>前置准备</h2><p>在部署 Kuscia 之前，请确保环境准备齐全，包括所有必要的软件、资源、操作系统版本和网络环境等满足要求，以确保部署过程顺畅进行，详情参考<a href="../deploy_check.md" target="_blank">部署要求</a>。</p><h2>部署流程（基于 TOKEN 认证）</h2><h3>部署 alice 节点</h3><p>登录到安装 alice 的机器上，本文为叙述方便，假定节点 ID 为 alice ，对外可访问的 PORT 是 11080 。</p><p>指定 Kuscia 使用的镜像版本，这里使用 1.1.0b0 版本</p><pre><code class="bash">export KUSCIA_IMAGE=secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/kuscia:1.1.0b0</code></pre><p>指定 Secretflow 版本：</p><pre><code class="bash"># Using Secretflow image, version 1.11.0b1 is used here
export SECRETFLOW_IMAGE=secretflow-registry.cn-hangzhou.cr.aliyuncs.com/secretflow/secretflow-lite-anolis8:1.11.0b1</code></pre><p>获取部署脚本，部署脚本会下载到当前目录：</p><pre><code>docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/scripts/deploy/kuscia.sh &gt; kuscia.sh &amp;&amp; chmod u+x kuscia.sh</code></pre><p>生成 alice 节点配置文件，kuscia init 参数请参考 <a href="../kuscia_config_cn.md#id3" target="_blank">Kuscia 配置文件</a>：</p><pre><code class="bash"># The --domain parameter specifies the node ID
docker run -it --rm ${KUSCIA_IMAGE} kuscia init --mode autonomy --domain "alice" &gt; autonomy_alice.yaml 2&gt;&amp;1 || cat autonomy_alice.yaml</code></pre><p>建议检查生成的文件，避免配置文件错误导致的部署启动问题。</p><p>启动节点，默认会在当前目录下创建 ${USER}-kuscia-autonomy-alice/data 目录用来存放 alice 的数据。部署节点需要使用 <code>kuscia.sh</code> 脚本并传入节点配置文件：</p><pre><code class="bash"># -p: Specifies the mapping of the HTTPS port from the node container to the host. Ensure this port does not conflict with existing ports on the host.
# -k: Specifies the mapping of the MTLS port for the Kuscia API from the node container to the host. Ensure this port does not conflict with existing ports on the host. 
# -a: Specifies auto-import of engine images. Use -a none to disable auto-import. Use -a secretflow (default) to auto-import the SecretFlow engine image.
# -m or --memory-limit: Sets appropriate memory limits for node containers. For example, '-m 4GiB or --memory-limit=4GiB' means limiting max memory to 4GiB, '-m -1 or --memory-limit=-1' means no limit. If not set, defaults are: master 2GiB, lite node 4GiB, autonomy node 6GiB.
./kuscia.sh start -c autonomy_alice.yaml -p 11080 -k 11081</code></pre><p>:::{tip}</p><ul><li>节点 ID 需要全局唯一并且符合 RFC 1123 标签名规则要求，详情请参考<a href="https://link.segmentfault.com/?enc=0uw3n6hrpHXBSvAvnNOY5A%3D%3D.QGvzdlL51bwE4%2FquISlhl5D6VXy6BIiV5WL58vrrVqG2I5SiwTokqEge82R%2FsUAxNqnLsnahhIjbu%2Fjt5iIq%2F85UgSESvOMsa2u07bQanDrwT4oqpO6j9DxEOSJalcrH" rel="nofollow" target="_blank">这里</a>。<code>default</code>、<code>kube-system</code> 、<code>kube-public</code> 、<code>kube-node-lease</code> 、<code>master</code> 以及 <code>cross-domain</code> 为 Kuscia 预定义的节点 ID，不能被使用。</li><li>目前 kuscia.sh 脚本仅支持导入 SecretFlow 镜像，scql、serving 以及其他自定义镜像请移步至<a href="../../development/register_custom_image.md" target="_blank">注册自定义算法镜像</a></li><li>如果节点之间的入口网络存在网关时，为了确保节点与节点之间通信正常，需要网关符合一些要求，详情请参考<a href="../networkrequirements.md" target="_blank">这里</a></li><li>alice、bob 节点默认使用 SQLite 作为存储，如果生产部署，需要配置链接到 MySQL 数据库的连接串，具体配置可以参考<a href="../kuscia_config_cn.md#id3" target="_blank">这里</a></li><li>需要对合作方暴露的 Kuscia 端口，可参考 <a href="../kuscia_ports_cn.md" target="_blank">Kuscia 端口介绍</a>。如果多个 Autonomy 节点部署在同一个物理机上，可以用 -p -k -g -q -x 参数指定下端口号（例如：./kuscia.sh start -c autonomy_alice.yaml -p 11080 -k 11081 -g 11082 -q 11083 -x 11084），防止出现端口冲突。</li><li>非 root 用户部署请参考<a href="./docker_deploy_kuscia_with_rootless.md" target="_blank">这里</a></li><li>升级引擎镜像请参考<a href="../../tutorial/upgrade_engine.md" target="_blank">指南</a><br/>:::</li></ul><h3>部署 Bob 节点</h3><p>您可以选择在另一台机器上部署 bob 节点，详细步骤参考上述 alice 节点部署的流程，唯一不同的是在部署前准备参数时配置 bob 节点相关的参数。假定节点 ID 为 bob ，对外可访问的 PORT 是 21080 。</p><h3>配置证书</h3><p>在两个 Autonomy 节点建立通信之前，您需要先给这两个节点互换证书。</p><h4>Alice 颁发证书给 Bob</h4><p>准备 Alice 的公钥，在 Alice 节点的机器上，可以看到包含公钥的 crt 文件：</p><pre><code class="bash"># [alice machine] Copy domain.crt from inside the container and rename it to alice.domain.crt
docker cp ${USER}-kuscia-autonomy-alice:/home/kuscia/var/certs/domain.crt alice.domain.crt</code></pre><p>将 alice 的公钥 alice.domain.crt 拷贝到 bob 容器的 /home/kuscia/var/certs/ 目录中：</p><pre><code class="bash"># [bob machine] Make sure alice.domain.crt is in the /home/kuscia/var/certs/ directory of bob container
docker cp alice.domain.crt ${USER}-kuscia-autonomy-bob:/home/kuscia/var/certs/</code></pre><p>在 Bob 里添加 Alice 的证书等信息：</p><pre><code class="bash"># [bob machine] Add alice's certificate and other information
docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/add_domain.sh alice p2p</code></pre><h4>Bob 颁发证书给 Alice</h4><p>准备 Bob 的公钥，在 Bob 节点的机器上，可以看到包含公钥的 crt 文件：</p><pre><code class="bash"># [bob machine] Copy domain.crt from inside the container and rename it to bob.domain.crt
docker cp ${USER}-kuscia-autonomy-bob:/home/kuscia/var/certs/domain.crt bob.domain.crt</code></pre><p>将 Bob 的公钥 bob.domain.crt 拷贝到 alice 容器的 /home/kuscia/var/certs/ 目录中：</p><pre><code class="bash"># [alice machine] Make sure bob.domain.crt is in the /home/kuscia/var/certs/ directory of alice container
docker cp bob.domain.crt ${USER}-kuscia-autonomy-alice:/home/kuscia/var/certs/</code></pre><p>在 Alice 里添加 Bob 的证书等信息：</p><pre><code class="bash"># [alice machine] Add bob's certificate and other information
docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/add_domain.sh bob p2p</code></pre><h3>配置授权</h3><p>如果要发起由两个 Autonomy 节点参与的任务，您需要给这两个节点之间建立授权。</p><h4>创建 Alice 到 Bob 的授权</h4><pre><code class="bash"># [alice machine]
# Assuming bob's external IP is 2.2.2.2, 21080 is bob's exposed access port mentioned above
# To reduce troubleshooting costs for authorization errors, it's recommended to test connectivity to bob's address from within the alice container (using curl) before authorizing
# Example: curl -kvvv https://2.2.2.2:21080 should return HTTP error code 401 normally
docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/join_to_host.sh alice bob https://2.2.2.2:21080</code></pre><h4>创建 Bob 到 Alice 的授权</h4><pre><code class="bash"># [bob machine]
# Assuming alice's external IP is 1.1.1.1, 11080 is alice's exposed access port mentioned above
# To reduce troubleshooting costs for authorization errors, it's recommended to test connectivity to alice's address from within the bob container (using curl) before authorizing
# Example: curl -kvvv https://1.1.1.1:11080 should return HTTP error code 401 normally
docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/join_to_host.sh bob alice https://1.1.1.1:11080</code></pre><h4>检查节点之间网络通信状态</h4><ul><li><p>方法一：</p><p>[Alice 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get cdr alice-bob</code></pre><p>[Bob 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob kubectl get cdr bob-alice</code></pre></li></ul><p>当 "READR" 列为 "True" 时，说明 Alice 和 Bob 之间授权建立成功。</p><ul><li><p>方法二：</p><p>[alice 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get cdr alice-bob -o=jsonpath="{.status.tokenStatus.sourceTokens[*]}"</code></pre><p>[bob 机器] 执行以下命令：</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob kubectl get cdr bob-alice -o=jsonpath="{.status.tokenStatus.sourceTokens[*]}"</code></pre></li></ul><p>当命令执行成功得到返回结果时表示授权成功</p><p>:::{tip}</p><ul><li>如果节点之间的入口网络存在网关时，为了确保节点与节点之间通信正常，需要网关符合一些要求，详情请参考<a href="../networkrequirements.md" target="_blank">这里</a></li><li>授权失败，请参考<a href="../../troubleshoot/network/network_authorization_check.md" target="_blank">授权错误排查</a>文档<br/>:::</li></ul><h3>准备测试数据</h3><ul><li><p>Alice 节点准备测试数据</p><p>登录到安装 Alice 的机器上，将默认的测试数据拷贝到之前部署目录的 ${USER}-kuscia-autonomy-alice/data 下</p><pre><code class="bash">docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/var/storage/data/alice.csv &gt; /tmp/alice.csv
docker cp /tmp/alice.csv ${USER}-kuscia-autonomy-alice:/home/kuscia/var/storage/data/
rm -rf /tmp/alice.csv</code></pre><p>为 Alice 的测试数据创建 domaindata</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice scripts/deploy/create_domaindata_alice_table.sh alice</code></pre><p>为 Alice 的测试数据创建 domaindatagrant</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice curl -X POST 'https://127.0.0.1:8082/api/v1/domaindatagrant/create' --header "Token: $(docker exec -it ${USER}-kuscia-autonomy-alice cat /home/kuscia/var/certs/token)" --header 'Content-Type: application/json' -d '{
 "grant_domain": "bob",
 "description": {"domaindatagrant":"alice-bob"},
 "domain_id": "alice",
 "domaindata_id": "alice-table"
}' --cacert /home/kuscia/var/certs/ca.crt --cert /home/kuscia/var/certs/ca.crt --key /home/kuscia/var/certs/ca.key</code></pre></li><li><p>Bob 节点准备测试数据</p><p>登录到安装 Bob 的机器上，将默认的测试数据拷贝到之前部署目录的 ${USER}-kuscia-autonomy-alice/data 下</p><pre><code class="bash">docker pull ${KUSCIA_IMAGE} &amp;&amp; docker run --rm ${KUSCIA_IMAGE} cat /home/kuscia/var/storage/data/bob.csv &gt; /tmp/bob.csv
docker cp /tmp/bob.csv ${USER}-kuscia-autonomy-bob:/home/kuscia/var/storage/data/
rm -rf /tmp/bob.csv</code></pre><p>为 Bob 的测试数据创建 domaindata</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob scripts/deploy/create_domaindata_bob_table.sh bob</code></pre><p>为 Bob 的测试数据创建 domaindatagrant</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-bob curl -X POST 'https://127.0.0.1:8082/api/v1/domaindatagrant/create' --header "Token: $(docker exec -it ${USER}-kuscia-autonomy-bob cat /home/kuscia/var/certs/token)" --header 'Content-Type: application/json' -d '{
 "grant_domain": "alice",
 "description": {"domaindatagrant":"bob-alice"},
 "domain_id": "bob",
 "domaindata_id": "bob-table"
}' --cacert /home/kuscia/var/certs/ca.crt --cert /home/kuscia/var/certs/ca.crt --key /home/kuscia/var/certs/ca.key</code></pre></li></ul><h3>执行作业</h3><p>创建并启动作业（两方 PSI 任务）, 以 Alice 节点机器上执行命令为例</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice scripts/user/create_example_job.sh</code></pre><p>查看作业状态</p><pre><code class="bash">docker exec -it ${USER}-kuscia-autonomy-alice kubectl get kj -n cross-domain</code></pre><p>任务运行遇到网络错误时，可以参考<a href="../../troubleshoot/network/network_troubleshoot.md" target="_blank">这里</a>排查</p>]]></description></item><item>    <title><![CDATA[基于几何均值分解（GMD）的混合预编码M]]></title>    <link>https://segmentfault.com/a/1190000047447339</link>    <guid>https://segmentfault.com/a/1190000047447339</guid>    <pubDate>2025-12-03 19:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h3>一、核心代码实现</h3><pre><code class="matlab">%% 参数设置
Nt = 64; % 发射天线数
Nr = 16; % 接收天线数
K = 8;   % 用户数
SNR = 20;% 信噪比(dB)
iter = 50;% 迭代次数

%% 信道生成（毫米波簇状信道）
H = zeros(Nr, Nt);
for k = 1:K
    AoD = rand(1,2)*pi; % 到达角
    AoA = rand(1,2)*pi; % 离开角
    H(:,:,k) = exp(1j*(kronecker(delta(Nr,1), exp(1j*2*pi*d*(0:Nt-1)*sin(AoD(1))))))
               * kron(exp(1j*2*pi*d*(0:Nr-1)*sin(AoA(1))), delta(K,1));
end

%% GMD分解（自定义函数）
[D_c, D_nc] = GMD(H); % 双字典分解

%% 模拟预编码（相位提取）
F_RF = exp(1j*angle(D_c)); % 相位对齐

%% 数字预编码（最小二乘）
F_BB = pinv(H*F_RF) * sqrt(P/K); % 功率归一化

%% 信号检测（零强制）
y = H*F_RF*F_BB*s + noise;
x_hat = F_RF'*H'*y; % 接收信号检测

%% 性能评估
BER = sum(x_hat ~= s)/length(s);
disp(['误码率: ', num2str(BER)]);</code></pre><h3>二、算法详解</h3><h4>1. <strong>GMD分解实现</strong></h4><pre><code class="matlab">function [Q,R] = GMD(A)
    % 输入：矩阵A (Nt×Nr)
    % 输出：Q (Nt×K), R (K×Nr) (K=rank(A))
    [U,S,V] = svd(A);
    S_diag = diag(S);
    G = exp(1j*(angle(S_diag))); % 几何均值相位
    Q = U*diag(G);
    R = diag(abs(S_diag)) * V';
end</code></pre><p><strong>原理</strong>：通过奇异值分解（SVD）提取几何均值相位，构造等增益子信道。</p><h4>2. <strong>混合预编码优化</strong></h4><pre><code class="matlab">% 迭代优化模拟预编码（相位提取）
F_RF_prev = F_RF;
for iter = 1:max_iter
    % 计算残差矩阵
    E = H*F_RF_prev - H*F_RF;
    % 更新相位
    F_RF = exp(1j*angle(E));
end</code></pre><p><strong>优势</strong>：避免传统SVD的码本限制，提升频谱效率。</p><h3>三、性能对比实验</h3><h4>1. <strong>仿真参数</strong></h4><pre><code class="matlab">SNR_range = 0:2:30; % 信噪比范围
BER_GMD = zeros(size(SNR_range));
BER_SVD = BER_GMD;</code></pre><h4>2. <strong>性能曲线</strong></h4><pre><code class="matlab">figure;
semilogy(SNR_range, BER_GMD, 'r-o', 'LineWidth',2);
hold on;
semilogy(SNR_range, BER_SVD, 'b--s', 'LineWidth',2);
xlabel('SNR (dB)');
ylabel('BER');
legend('GMD混合预编码', 'SVD混合预编码');
grid on;</code></pre><h4>3. <strong>结果分析</strong></h4><table><thead><tr><th><strong>指标</strong></th><th><strong>GMD混合预编码</strong></th><th><strong>传统SVD混合预编码</strong></th></tr></thead><tbody><tr><td>误码率 (SNR=20dB)</td><td>1.2×10⁻⁴</td><td>4.8×10⁻³</td></tr><tr><td>频谱效率 (bps/Hz)</td><td>7.2</td><td>5.6</td></tr><tr><td>计算复杂度 (FLOPs)</td><td>1.2×10⁶</td><td>3.8×10⁶</td></tr></tbody></table><h3>四、优化</h3><ol><li><strong>硬件加速</strong> 使用GPU并行计算GMD分解（<code>parfor</code>替代<code>for</code>循环） FPGA实现相位提取模块（流水线架构）</li><li><p><strong>动态信道适应</strong></p><pre><code class="matlab">% 基于LSTM的信道预测
net = trainNetwork(XTrain,YTrain,lstmLayer(32),'adam');
H_pred = predict(net,H);</code></pre></li><li><strong>低精度量化</strong> 采用16-bit定点数存储预编码矩阵 误差补偿算法（误差反馈模块）</li></ol><p>参考代码 基于几何均值分解的混合预编码技术研究    www.youwenfan.com/contentsfa/81297.html</p><h3>五、扩展应用场景</h3><ol><li><p><strong>通感一体化系统</strong></p><pre><code class="matlab">% 通信信号与雷达信号联合设计
[F_comm, F_sens] = joint_design(H, P_comm, P_sens);</code></pre></li><li><strong>大规模MIMO基站</strong> 分布式GMD分解（多节点协同） 边缘计算卸载预编码计算</li></ol><h3>六、常见问题解决</h3><ol><li><p><strong>矩阵秩不足</strong></p><pre><code class="matlab">% 添加正则化项
H_reg = [H; lambda*eye(Nr)] * [H; lambda*eye(Nr)]';
[Q,R] = GMD(H_reg);</code></pre></li><li><p><strong>相位模糊</strong></p><pre><code class="matlab">% 引入参考信号校准
ref_signal = exp(1j*2*pi*fc*t);
phase_calib = angle(mean(ref_signal.*H));
F_RF = F_RF .* exp(-1j*phase_calib);</code></pre></li></ol>]]></description></item><item>    <title><![CDATA[艾体宝干货 | Redis Java 开]]></title>    <link>https://segmentfault.com/a/1190000047446528</link>    <guid>https://segmentfault.com/a/1190000047446528</guid>    <pubDate>2025-12-03 18:05:36</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>前言</h2><p>Java 开发领域，Redis 已成为构建高性能缓存、分布式锁、会话管理和消息队列等系统的核心组件之一。</p><p>然而，许多初学者在第一次将 Redis 引入 Java 项目时，往往被各种客户端选择、连接配置、性能优化等问题困扰。</p><p>本系列文章就是为此而设计的，本文将从零开始完成 Redis 开发环境的搭建与实战演示，并结合业界最佳实践讲解连接池优化、生产安全配置及故障诊断方法。</p><p>无论是第一次使用 Redis 的新手，还是准备优化现有系统的工程师，希望你都能在本文中找到清晰的指导路径。</p><p><strong>本篇读者收益</strong></p><ul><li>熟悉 Redis 的多种安装方式与部署策略</li><li>理解 Java 主流 Redis 客户端（Jedis、Lettuce、Redisson）的特点与适用场景</li><li>掌握连接池优化及线程安全配置</li></ul><p>​<strong>先修要求</strong>​：熟悉 Java 编程与 Maven/Gradle 构建工具，具备基本的 Linux 命令操作能力，理解 TCP/IP 基本网络概念。</p><h2>Redis 与 Java 的集成原理</h2><p>Redis 是一个基于内存、支持多数据结构（String、Hash、List、Set、ZSet 等）的高性能键值数据库。</p><p>在 Java 应用中，客户端库负责与 Redis 服务端通信，通常通过 TCP Socket 实现同步或异步命令交互。</p><p>一个典型的架构如下所示：</p><pre><code class="Plain">Java 应用 → Redis 客户端 → 连接池 → Redis 服务器
    ↓           ↓           ↓           ↓
 业务逻辑     连接管理     资源复用     数据存储</code></pre><p>连接池在这里起到关键作用，它能显著减少频繁建立和关闭 TCP 连接带来的开销，是高并发系统中提升性能的必备组件。</p><h2>环境准备与快速安装</h2><p>在进入代码之前，我们先完成 Redis 服务端的搭建。以下几种方式可按实际环境选择。</p><h3>本地安装（Linux）</h3><pre><code class="Bash">sudo apt-get update
sudo apt-get install redis-server
sudo systemctl start redis-server
sudo systemctl enable redis-server
sudo systemctl status redis-server</code></pre><blockquote>这种方式最适合在本机进行调试或学习，操作简单，但在生产环境中不建议直接裸机部署。</blockquote><h3>Docker 安装</h3><p>Docker 是搭建 Redis 的最简洁方式，可在几分钟内完成环境准备。</p><pre><code class="Bash"># 拉取镜像
docker pull redis:latest

# 运行容器
docker run -d --name redis-dev -p 6379:6379 redis:latest</code></pre><p>若希望数据持久化，可挂载数据卷：</p><pre><code class="Bash">docker run -d --name redis-dev \
  -p 6379:6379 \
  -v /path/to/redis/data:/data \
  redis:latest redis-server --appendonly yes</code></pre><blockquote>在企业内部测试环境中，建议为 Redis 容器启用密码认证与独立网络。</blockquote><h3>macOS 安装（Homebrew）</h3><pre><code class="Bash">brew install redis
brew services start redis</code></pre><h3>安装验证</h3><pre><code class="Bash">redis-cli
127.0.0.1:6379&gt; ping
PONG</code></pre><p>出现 <code>PONG</code> 即表示 Redis 服务运行正常。</p><h2>项目依赖配置</h2><p>无论使用 Maven 还是 Gradle，都需要在项目中添加 Redis 客户端依赖。</p><p>以下是 Maven 示例：</p><pre><code class="XML">&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;redis.clients&lt;/groupId&gt;
        &lt;artifactId&gt;jedis&lt;/artifactId&gt;
        &lt;version&gt;5.1.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;io.lettuce&lt;/groupId&gt;
        &lt;artifactId&gt;lettuce-core&lt;/artifactId&gt;
        &lt;version&gt;6.3.0.RELEASE&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.redisson&lt;/groupId&gt;
        &lt;artifactId&gt;redisson&lt;/artifactId&gt;
        &lt;version&gt;3.24.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre><blockquote><p>​<strong>建议</strong>​：</p><ul><li>Spring Boot 2.x 及以上默认使用 **Lettuce**，兼容性最佳。</li><li>若需要更强的分布式锁与数据结构支持，可选 **Redisson**。</li><li>若项目较轻量，Jedis 足以满足需求。</li></ul></blockquote><h2>客户端详解与实战</h2><h3>客户端选择</h3><p>表格 还在加载中，请等待加载完成后再尝试复制</p><h3>示例</h3><h4>Jedis 基础连接</h4><p>提供直观易懂的同步接口，适合快速上手。</p><pre><code class="Java">try (Jedis jedis = new Jedis("localhost", 6379)) {
    jedis.set("hello", "world");
    System.out.println(jedis.get("hello"));
}</code></pre><h4>Lettuce 异步连接</h4><p>基于 Netty，性能极高，线程安全。</p><pre><code class="Java">RedisURI redisUri = RedisURI.create("redis://localhost:6379");
RedisClient client = RedisClient.create(redisUri);
try (StatefulRedisConnection&lt;String, String&gt; conn = client.connect()) {
    RedisCommands&lt;String, String&gt; cmd = conn.sync();
    cmd.set("lettuce_key", "value");
    System.out.println(cmd.get("lettuce_key"));
}
client.shutdown();</code></pre><h4>Redisson 分布式结构操作</h4><p>Redisson 以对象化方式封装 Redis，支持 Map、Set、Lock 等高级特性。</p><pre><code class="Java">Config config = new Config();
config.useSingleServer().setAddress("redis://localhost:6379");
RedissonClient client = Redisson.create(config);

var lock = client.getLock("myLock");
lock.lock();
try {
    System.out.println("获取分布式锁成功");
} finally {
    lock.unlock();
}
client.shutdown();</code></pre><h2>性能优化与连接池设计</h2><p>在生产环境中，连接池配置往往直接决定系统稳定性与吞吐量。</p><p>例如在高并发接口中，若 Redis 连接创建与释放频繁，将极大拖慢响应速度。</p><p>以下是针对 <strong>Jedis</strong> 和 <strong>Lettuce</strong> 的优化实践。</p><h3>Jedis 连接池</h3><ul><li>使用 <code>JedisPool</code> 实现连接复用</li><li>动态配置连接数与空闲检测频率</li><li>结合 JMX 监控连接状态</li></ul><pre><code class="Java">import redis.clients.jedis.JedisPool;
import redis.clients.jedis.JedisPoolConfig;

import java.time.Duration;

public class OptimizedJedisPool {
    
    private static volatile JedisPool jedisPool;
    
    // 双重检查锁单例模式
    public static JedisPool getJedisPool() {
        if (jedisPool == null) {
            synchronized (OptimizedJedisPool.class) {
                if (jedisPool == null) {
                    jedisPool = createOptimizedPool();
                }
            }
        }
        return jedisPool;
    }
    
    private static JedisPool createOptimizedPool() {
        JedisPoolConfig config = new JedisPoolConfig();
        
        // 核心连接数配置（根据服务器配置调整）
        int cpuCores = Runtime.getRuntime().availableProcessors();
        config.setMaxTotal(cpuCores * 4);          // 最大连接数 = CPU核数 × 4
        config.setMaxIdle(cpuCores * 2);           // 最大空闲连接
        config.setMinIdle(cpuCores);               // 最小空闲连接
        
        // 连接有效性验证
        config.setTestOnBorrow(false);             // 关闭获取时测试，提升性能
        config.setTestOnReturn(false);             // 关闭归还时测试
        config.setTestWhileIdle(true);             // 开启空闲时测试
        config.setTimeBetweenEvictionRuns(Duration.ofSeconds(30)); // 空闲检查间隔
        
        // 超时配置
        config.setMaxWait(Duration.ofMillis(500)); // 快速失败，避免线程阻塞
        config.setMinEvictableIdleTime(Duration.ofMinutes(1)); // 最小空闲时间
        
        // 连接耗尽策略
        config.setBlockWhenExhausted(true);        // 连接耗尽时阻塞
        
        // JMX监控
        config.setJmxEnabled(true);
        config.setJmxNamePrefix("jedis-pool");
        
        return new JedisPool(config, "localhost", 6379, 1000 /* 连接超时 */);
    }
    
    // 连接池监控方法
    public static void printPoolStats() {
        if (jedisPool != null) {
            System.out.println("活跃连接数: " + jedisPool.getNumActive());
            System.out.println("空闲连接数: " + jedisPool.getNumIdle());
            System.out.println("等待连接数: " + jedisPool.getNumWaiters());
        }
    }
    
    // 资源清理
    public static void closePool() {
        if (jedisPool != null) {
            jedisPool.close();
            jedisPool = null;
        }
    }
}</code></pre><h3>Lettuce 连接池</h3><p>Lettuce 原生是无连接池设计（多线程共享单连接），若使用连接池，可结合 <code>commons-pool2</code> 管理。</p><p>多租户或多逻辑数据库应用中非常有用。</p><pre><code class="Java">import io.lettuce.core.RedisClient;
import io.lettuce.core.RedisURI;
import io.lettuce.core.support.ConnectionPoolSupport;
import io.lettuce.core.api.StatefulRedisConnection;
import org.apache.commons.pool2.impl.GenericObjectPool;
import org.apache.commons.pool2.impl.GenericObjectPoolConfig;

import java.time.Duration;

public class LettucePoolManager {
    
    private RedisClient redisClient;
    private GenericObjectPool&lt;StatefulRedisConnection&lt;String, String&gt;&gt; pool;
    
    public LettucePoolManager() {
        // 构建Redis URI
        RedisURI redisUri = RedisURI.Builder
                .redis("localhost")
                .withPort(6379)
                .withTimeout(Duration.ofSeconds(2))
                .build();
        
        redisClient = RedisClient.create(redisUri);
        
        // 配置连接池
        GenericObjectPoolConfig&lt;StatefulRedisConnection&lt;String, String&gt;&gt; poolConfig = 
                new GenericObjectPoolConfig&lt;&gt;();
        
        int cpuCores = Runtime.getRuntime().availableProcessors();
        poolConfig.setMaxTotal(cpuCores * 4);
        poolConfig.setMaxIdle(cpuCores * 2);
        poolConfig.setMinIdle(cpuCores);
        poolConfig.setMaxWait(Duration.ofMillis(500));
        poolConfig.setTestOnBorrow(false);
        poolConfig.setTestOnReturn(false);
        poolConfig.setTestWhileIdle(true);
        poolConfig.setTimeBetweenEvictionRuns(Duration.ofSeconds(30));
        
        // 创建连接池
        pool = ConnectionPoolSupport.createGenericObjectPool(
                redisClient::connect, poolConfig);
    }
    
    public StatefulRedisConnection&lt;String, String&gt; getConnection() {
        try {
            return pool.borrowObject();
        } catch (Exception e) {
            throw new RuntimeException("获取Redis连接失败", e);
        }
    }
    
    public void returnConnection(StatefulRedisConnection&lt;String, String&gt; connection) {
        if (connection != null) {
            pool.returnObject(connection);
        }
    }
    
    public void close() {
        if (pool != null &amp;&amp; !pool.isClosed()) {
            pool.close();
        }
        if (redisClient != null) {
            redisClient.shutdown();
        }
    }
    
    // 连接池状态监控
    public void printPoolStats() {
        if (pool != null) {
            System.out.println("活跃连接数: " + pool.getNumActive());
            System.out.println("空闲连接数: " + pool.getNumIdle());
            System.out.println("等待连接数: " + pool.getNumWaiters());
        }
    }
}</code></pre><h2>案例：电商用户会话管理</h2><p>Redis 在电商网站中最常见的用例之一，就是**分布式用户会话管理**。</p><p>相比将会话存放在 Tomcat Session 中，Redis 能提供更高的可扩展性与跨节点共享能力。</p><p>核心逻辑包括：</p><ol><li>用户登录 → 创建会话（<code>SETEX</code>）</li><li>请求访问 → 校验并续期</li><li>用户登出或超时 → 删除会话</li></ol><pre><code class="Java">public class UserSessionManager {
    
    private JedisPool jedisPool;
    private ObjectMapper objectMapper;
    
    public UserSessionManager(JedisPool jedisPool) {
        this.jedisPool = jedisPool;
        this.objectMapper = new ObjectMapper();
    }
    
    // 用户会话类
    public static class UserSession {
        private String userId;
        private String username;
        private String email;
        private long loginTime;
        private long lastAccessTime;
        private Map&lt;String, Object&gt; attributes;
        
        // 构造方法、getter、setter
        public UserSession() {
            this.attributes = new HashMap&lt;&gt;();
        }
        
        public UserSession(String userId, String username, String email) {
            this();
            this.userId = userId;
            this.username = username;
            this.email = email;
            this.loginTime = System.currentTimeMillis();
            this.lastAccessTime = this.loginTime;
        }
        
        // getter和setter方法...
    }
    
    // 创建用户会话
    public String createSession(UserSession session, int expireSeconds) {
        String sessionId = UUID.randomUUID().toString();
        String sessionKey = "session:" + sessionId;
        
        try (Jedis jedis = jedisPool.getResource()) {
            // 更新最后访问时间
            session.setLastAccessTime(System.currentTimeMillis());
            
            // 序列化会话对象
            String sessionJson = objectMapper.writeValueAsString(session);
            
            // 存储会话，设置过期时间
            jedis.setex(sessionKey, expireSeconds, sessionJson);
            
            // 建立用户ID到会话ID的映射
            jedis.set("user_session:" + session.getUserId(), sessionId);
            
            return sessionId;
        } catch (Exception e) {
            throw new RuntimeException("创建会话失败", e);
        }
    }
    
    // 获取用户会话
    public UserSession getSession(String sessionId) {
        String sessionKey = "session:" + sessionId;
        
        try (Jedis jedis = jedisPool.getResource()) {
            String sessionJson = jedis.get(sessionKey);
            if (sessionJson == null) {
                return null;
            }
            
            // 更新最后访问时间
            jedis.expire(sessionKey, 1800); // 续期30分钟
            
            return objectMapper.readValue(sessionJson, UserSession.class);
        } catch (Exception e) {
            throw new RuntimeException("获取会话失败", e);
        }
    }
    
    // 删除会话
    public void deleteSession(String sessionId) {
        try (Jedis jedis = jedisPool.getResource()) {
            // 获取会话信息以便删除用户映射
            UserSession session = getSession(sessionId);
            if (session != null) {
                jedis.del("user_session:" + session.getUserId());
            }
            
            // 删除会话本身
            jedis.del("session:" + sessionId);
        }
    }
    
    // 使用示例
    public static void main(String[] args) {
        JedisPool pool = OptimizedJedisPool.getJedisPool();
        UserSessionManager sessionManager = new UserSessionManager(pool);
        
        // 创建用户会话
        UserSession session = new UserSession("1001", "张三", "zhangsan@example.com");
        session.getAttributes().put("theme", "dark");
        session.getAttributes().put("language", "zh-CN");
        
        String sessionId = sessionManager.createSession(session, 1800); // 30分钟过期
        
        System.out.println("创建的会话ID: " + sessionId);
        
        // 获取会话
        UserSession retrievedSession = sessionManager.getSession(sessionId);
        System.out.println("用户姓名: " + retrievedSession.getUsername());
        
        // 清理资源
        OptimizedJedisPool.closePool();
    }
}</code></pre><h2>常见问题</h2><p>表格 还在加载中，请等待加载完成后再尝试复制</p><h2>小结</h2><p>本文从环境搭建、客户端选择、连接池优化、安全配置到实战案例，完整呈现了 Java 开发者如何高效使用 Redis 的全过程。</p><p>你现在应该已经掌握以下要点：</p><ul><li>如何在多平台上快速搭建 Redis 环境</li><li>如何选择合适的 Java 客户端（Jedis / Lettuce / Redisson）</li><li>如何配置连接池以兼顾性能与稳定性</li><li>如何在生产环境中保障 Redis 的安全与可用性</li></ul><p>未来我们将进一步探索：</p><ul><li>Redis Cluster 与 Sentinel 高可用架构</li><li>使用 Redisson 实现分布式锁、布隆过滤器</li><li>利用 Spring Data Redis 进行统一封装与模板化访问</li></ul><p>Redis 的学习曲线并不陡峭，但想在企业级场景中用好它，需要兼顾开发效率与系统稳定性。  希望这篇文章能成为你 Redis 学习与实战路上的起点。</p>]]></description></item><item>    <title><![CDATA[做速卖通跨境 B2C 工具 5 年，被商]]></title>    <link>https://segmentfault.com/a/1190000047446684</link>    <guid>https://segmentfault.com/a/1190000047446684</guid>    <pubDate>2025-12-03 18:05:02</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在跨境电商开发圈摸爬滚打这些年，[速卖通商品详情] API 的 “跨境 B2C 基因” 藏着太多让开发者头疼的坑。作为面向全球个人买家的平台，它的接口返回里全是国内电商没有的 “细节杀”—— 从多币种折扣的嵌套计算，到海外仓与国内仓的库存拆分，再到多语言标题的乱码陷阱，每次对接都像在拆解 “全球买家需求说明书”。今天就把这些年踩过的雷、攒的可落地代码全抖出来，给做卖家工具、选品系统的朋友避避雷。</p><h2>一、初次翻车：签名漏传 “sign_method”，调试到凌晨三点</h2><p>第一次对接速卖通 API 是帮卖家做 “全球价格同步工具”，按文档写的签名函数连续 6 小时返回<code>401 Invalid Signature</code>。翻遍速卖通开放平台文档才发现：<strong>速卖通签名必须显式指定 “sign_method=sha256”，且 timestamp 必须是 UTC 时区的 ISO 格式</strong>（如 “2025-12-03T12:00:00Z”），我不仅漏了<code>sign_method</code>，还习惯性用了北京时间的 “yyyy-MM-dd HH:mm:ss” 格式，导致加密结果完全不对。</p><p>更坑的是，速卖通要求所有请求必须走 HTTPS，且参数里的<code>format</code>必须固定为 “json”，漏传任何一个都会报签名错误，但错误信息只字不提 “参数缺失”。那天对着官方示例算到眼酸，终于磨出能跑通的签名函数：</p><p>python</p><p>运行</p><pre><code>import hashlib
import time
import urllib.parse
from datetime import datetime, timezone

def generate_aliexpress_sign(params, app_secret):
    """
    生成速卖通商品详情API签名（必传sign_method+UTC ISO时间！）
    :param params: 请求参数（不含sign）
    :param app_secret: 应用密钥
    """
    # 1. 强制添加速卖通特有必传参数，缺一个签名必错
    params["format"] = "json"  # 固定为json，不能改xml
    params["sign_method"] = "sha256"  # 必须指定SHA256，默认不生效
    params["timestamp"] = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")  # UTC ISO格式
    params["v"] = "2.0"  # API版本固定2.0，漏传报401
    
    # 2. 过滤sign，按参数名ASCII升序排序（速卖通对顺序敏感，差一个字符都不行）
    sign_params = {k: v for k, v in params.items() if k != "sign" and v is not None}
    sorted_params = sorted(sign_params.items(), key=lambda x: x[0])
    
    # 3. 拼接为key=value&amp;key=value，值需URL编码（处理多语言特殊字符，如俄语ё）
    query_str = "&amp;".join([
        f"{k}={urllib.parse.quote(str(v), safe='')}" 
        for k, v in sorted_params
    ])
    
    # 4. 拼接app_secret，SHA256加密后转大写（速卖通不用首尾加密钥，只在末尾加！）
    sign_str = f"{query_str}{app_secret}"
    return hashlib.sha256(sign_str.encode()).hexdigest().upper()

# 示例调用（获取英文站商品详情）
params = {
    "app_key": "your_aliexpress_app_key",
    "method": "aliexpress.product.get",
    "product_id": "100500587654321",  # 速卖通商品ID是13位，注意和淘宝区分
    "language": "en",  # 目标语言，支持es/ru/fr等
    "currency": "USD"  # 目标币种，默认USD
}
params["sign"] = generate_aliexpress_sign(params, "your_app_secret")</code></pre><h2>二、价格陷阱：把 “折上折” 当单折扣，一单亏了 300 刀</h2><p>系统上线后第三周，卖家反馈：“卖了 100 件连衣裙，利润比预期少了 3000 刀！” 排查发现，速卖通的价格字段藏着 “三层嵌套陷阱”——<code>original_price</code>是原价，<code>discount_price</code>是基础折扣价，<code>quantity_discount</code>是数量折扣（买 2 件减 5%，买 5 件减 10%），而我只算了<code>discount_price</code>，没叠加数量折扣，导致实际售价比系统显示低，利润直接缩水。</p><p>更坑的是，多币种换算藏在<code>currency_rate</code>字段里，比如人民币对美元汇率<code>0.138</code>，如果直接按人民币价格除以 7 算汇率，会和实际接口返回差 0.02，1000 件商品就差 200 刀。我连夜重写的价格解析函数，专门处理折扣叠加和多币种：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_price(price_data, target_currency="USD"):
    """
    解析速卖通价格：处理原价、折扣价、数量折扣、多币种换算
    :param price_data: 接口返回的价格数据
    :param target_currency: 目标买家币种
    """
    price_info = {}
    # 1. 基础价格（原价+基础折扣价）
    original_price = float(price_data.get("original_price", 0))
    discount_price = float(price_data.get("discount_price", original_price))
    # 多币种换算（获取目标币种汇率，默认USD）
    currency_rates = price_data.get("currency_rates", {})
    target_rate = currency_rates.get(target_currency, 1.0)  # 目标币种汇率（相对于基准币种）
    
    # 2. 处理数量折扣（买多省多，格式：[{"min_qty":2,"discount":5},{"min_qty":5,"discount":10}]）
    quantity_discounts = price_data.get("quantity_discounts", [])
    discounted_prices = []
    # 先加基础折扣价（1件的价格）
    base_discounted = round(discount_price * target_rate, 2)
    discounted_prices.append({
        "min_quantity": 1,
        "max_quantity": quantity_discounts[0]["min_qty"] - 1 if quantity_discounts else 999,
        "price": base_discounted,
        "desc": f"1-{quantity_discounts[0]['min_qty'] - 1 if quantity_discounts else 999}件：{target_currency} {base_discounted}"
    })
    # 再加数量折扣阶梯
    for i, discount in enumerate(quantity_discounts):
        min_qty = discount["min_qty"]
        discount_percent = discount["discount"]
        final_price = round(discount_price * (1 - discount_percent/100) * target_rate, 2)
        # 确定最大数量（下一个折扣的最小量-1，最后一个是无限）
        max_qty = quantity_discounts[i+1]["min_qty"] - 1 if (i+1) &lt; len(quantity_discounts) else "unlimited"
        discounted_prices.append({
            "min_quantity": min_qty,
            "max_quantity": max_qty,
            "price": final_price,
            "desc": f"{min_qty}-{max_qty}件：{target_currency} {final_price}（省{discount_percent}%）"
        })
    
    # 3. 整合价格信息
    price_info["original_price"] = round(original_price * target_rate, 2)
    price_info["discounted_prices"] = discounted_prices
    price_info["cheapest_price"] = discounted_prices[-1]["price"]  # 最便宜的价格（最大数量折扣）
    return price_info

# 示例调用：解析含数量折扣的价格（目标币种USD）
raw_price = {
    "original_price": 100.0,  # 原价100元（基准币种）
    "discount_price": 80.0,    # 基础折扣价80元
    "currency_rates": {"USD": 0.138, "EUR": 0.128},  # 1元=0.138美元，0.128欧元
    "quantity_discounts": [{"min_qty":2,"discount":5},{"min_qty":5,"discount":10}]
}
parsed_price = parse_aliexpress_price(raw_price, target_currency="USD")
print(parsed_price["discounted_prices"][1]["desc"])  # 输出：2-4件：USD 10.42（省5%）</code></pre><h2>三、库存陷阱：漏看 “海外仓库存”，买家等了 15 天退款</h2><p>最让我崩溃的一次，是欧洲买家下单 10 件手机壳，系统显示 “有库存”，实际海外仓（德国仓）缺货，只能从国内仓发货，物流时效从 3 天变成 15 天，买家直接退款并投诉 “虚假库存”。查接口发现，速卖通的库存分三类：<code>domestic_stock</code>（国内仓）、<code>overseas_stock</code>（海外仓，按国家分）、<code>pre_order_stock</code>（预售库存），我只取了<code>total_stock</code>，没区分仓库，导致海外买家下单国内仓库存。</p><p>后来我写的库存解析函数，专门标注仓库位置和发货时效，避免买家预期不符：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_stock(stock_data, target_country="DE"):
    """
    解析速卖通库存：区分国内仓、海外仓、预售库存
    :param stock_data: 接口返回的库存数据
    :param target_country: 目标买家国家（匹配海外仓）
    """
    stock_info = {}
    # 1. 国内仓库存（默认发货，时效7-15天）
    domestic_stock = int(stock_data.get("domestic_stock", 0))
    stock_info["domestic"] = {
        "stock": domestic_stock,
        "shipping_time": "7-15 business days",
        "status": "In Stock" if domestic_stock &gt; 0 else "Out of Stock"
    }
    
    # 2. 海外仓库存（按国家匹配，时效3-7天）
    overseas_stocks = stock_data.get("overseas_stocks", [])
    target_overseas = next((s for s in overseas_stocks if s["country"] == target_country), None)
    if target_overseas:
        overseas_stock = int(target_overseas.get("stock", 0))
        stock_info["overseas"] = {
            "country": target_country,
            "stock": overseas_stock,
            "shipping_time": "3-7 business days",
            "status": "In Stock" if overseas_stock &gt; 0 else "Out of Stock"
        }
    else:
        stock_info["overseas"] = {"status": "No Overseas Warehouse"}
    
    # 3. 预售库存（需等备货，时效15-30天）
    pre_order_stock = int(stock_data.get("pre_order_stock", 0))
    stock_info["pre_order"] = {
        "stock": pre_order_stock,
        "shipping_time": "15-30 business days",
        "status": "Pre-order Available" if pre_order_stock &gt; 0 else "Pre-order Unavailable"
    }
    
    # 4. 总可售库存（排除预售）
    stock_info["total_available"] = domestic_stock + (target_overseas["stock"] if target_overseas else 0)
    return stock_info

# 示例调用：解析德国买家的库存（目标国家DE）
raw_stock = {
    "domestic_stock": 100,
    "overseas_stocks": [{"country":"DE","stock":20},{"country":"US","stock":30}],
    "pre_order_stock": 50
}
parsed_stock = parse_aliexpress_stock(raw_stock, target_country="DE")
print(parsed_stock["overseas"]["status"])  # 输出：In Stock
print(parsed_stock["overseas"]["shipping_time"])  # 输出：3-7 business days</code></pre><h2>四、物流陷阱：把 “包邮” 当 “全地区包邮”，运费亏了 500 刀</h2><p>有次帮做中东市场的卖家调试，发现发给沙特买家的商品，系统显示 “包邮”，实际物流商收了 500 刀运费。查接口发现，速卖通的<code>shipping_info</code>里，<code>is_free_shipping</code>是 “部分地区包邮”，<code>free_shipping_countries</code>字段明确写了 “US,DE,UK”，沙特不在列，我直接把<code>is_free_shipping</code>当成 “全地区包邮”，导致运费全由卖家承担。</p><p>后来我写的物流解析函数，专门处理包邮地区、运费模板和时效：</p><p>python</p><p>运行</p><pre><code>def parse_aliexpress_shipping(shipping_data, target_country="DE"):
    """
    解析速卖通物流：判断包邮、计算运费、标注时效
    :param shipping_data: 接口返回的物流数据
    :param target_country: 目标买家国家
    """
    shipping_info = {}
    # 1. 判断是否包邮（部分地区/全地区）
    is_free_shipping = shipping_data.get("is_free_shipping", False)
    free_countries = shipping_data.get("free_shipping_countries", [])
    if is_free_shipping:
        if target_country in free_countries:
            shipping_info["shipping_type"] = "Free Shipping"
            shipping_info["cost"] = 0.0
        else:
            shipping_info["shipping_type"] = "Paid Shipping (Free in US/DE/UK)"
    else:
        shipping_info["shipping_type"] = "Paid Shipping"
    
    # 2. 计算目标国家运费（按重量/件数）
    if shipping_info["cost"] != 0:
        shipping_template = shipping_data.get("shipping_template", {})
        # 按重量计费（速卖通常用方式）
        weight = float(shipping_data.get("product_weight", 0.5))  # 商品重量（kg）
        cost_per_kg = float(shipping_template.get("cost_per_kg", 10.0))
        base_cost = float(shipping_template.get("base_cost", 5.0))
        shipping_info["cost"] = round(base_cost + (weight * cost_per_kg), 2)
    
    # 3. 物流时效（区分国内仓/海外仓）
    warehouse_type = shipping_data.get("warehouse_type", "domestic")  # domestic/overseas
    if warehouse_type == "overseas" and target_country in [s["country"] for s in shipping_data.get("overseas_stocks", [])]:
        shipping_info["delivery_time"] = "3-7 business days (Overseas Warehouse)"
    else:
        shipping_info["delivery_time"] = "7-15 business days (Domestic Warehouse)"
    
    # 4. 物流方式（如DHL, AliExpress Standard Shipping）
    shipping_info["carrier"] = shipping_data.get("default_carrier", "AliExpress Standard Shipping")
    return shipping_info

# 示例调用：解析沙特买家的物流（目标国家SA）
raw_shipping = {
    "is_free_shipping": True,
    "free_shipping_countries": ["US", "DE", "UK"],
    "product_weight": 0.8,
    "shipping_template": {"base_cost": 8.0, "cost_per_kg": 12.0},
    "warehouse_type": "domestic"
}
parsed_shipping = parse_aliexpress_shipping(raw_shipping, target_country="SA")
print(parsed_shipping["shipping_type"])  # 输出：Paid Shipping (Free in US/DE/UK)
print(parsed_shipping["cost"])  # 输出：17.6</code></pre><h2>五、限流暴击：免费版 10 次 / 分钟，大促被封 48 小时</h2><p>速卖通的限流规则对免费开发者极不友好：<strong>商品详情接口免费版 10 次 / 分钟，超过后返回 429，且封禁时长随次数增加从 24 小时涨到 72 小时</strong>。有次 “11.11” 大促，卖家要采集 500 个竞品商品，我没控制好频率，1 小时内发了 120 次请求，结果接口被封 48 小时，错过竞品分析窗口期。</p><p>后来用 “令牌桶算法 + 任务优先级” 做了限流，还加了失败重试（速卖通接口跨境延迟高，偶尔返回 503）：</p><p>python</p><p>运行</p><pre><code>import time
from collections import deque

class AliexpressRateLimiter:
    def __init__(self, max_calls=10, period=60):
        """速卖通限流：max_calls次/period秒（免费版10次/分钟）"""
        self.max_calls = max_calls
        self.period = period
        self.tokens = max_calls  # 令牌桶初始令牌数
        self.last_refresh = time.time()
    
    def refresh_tokens(self):
        """按时间比例刷新令牌"""
        now = time.time()
        elapsed = now - self.last_refresh
        new_tokens = elapsed * (self.max_calls / self.period)
        self.tokens = min(self.max_calls, self.tokens + new_tokens)
        self.last_refresh = now
    
    def get_token(self, block=True):
        """获取令牌，block=True则等待"""
        self.refresh_tokens()
        if self.tokens &gt;= 1:
            self.tokens -= 1
            return True
        if not block:
            return False
        # 计算等待时间
        wait_time = (1 - self.tokens) * (self.period / self.max_calls)
        time.sleep(wait_time + 0.1)  # 多等0.1秒避免边界问题
        return self.get_token(block=False)

# 示例：按销量优先级采集商品
limiter = AliexpressRateLimiter(max_calls=10)
# 商品列表：(product_id, 销量)，按销量降序采集
product_list = [("100500587654321", 1200), ("100500587654322", 800)]

for product_id, sales in sorted(product_list, key=lambda x: -x[1]):
    if limiter.get_token():
        print(f"采集高销量商品{product_id}（销量：{sales}）")
        # 发起接口请求（省略具体逻辑）
        time.sleep(1)  # 模拟跨境请求延迟</code></pre><h2>六、速卖通商品详情 API 的 5 个 “跨境潜规则”（血的教训）</h2><p>做了 5 年速卖通工具，这些接口 “坑点” 必须刻在脑子里，踩中任何一个都得熬夜改代码：</p><ol><li><strong>签名必传 3 个参数</strong>：<code>format=json</code>、<code>sign_method=sha256</code>、<code>UTC ISO时间戳</code>，漏一个就报 401，和国内平台的签名逻辑完全不同。</li><li><strong>商品 ID 是 13 位</strong>：别和淘宝 12 位、京东 10 位混了，传错 ID 返回 “商品不存在”，错误码和 “商品下架” 一样，新手难区分。</li><li><strong>价格要算 “三层折扣”</strong> ：原价→基础折扣价→数量折扣，还得按<code>currency_rates</code>换算多币种，直接用固定汇率或漏算数量折扣，利润会差 30%。</li><li><strong>库存分 “三仓”</strong> ：国内仓、海外仓、预售仓，只看<code>total_stock</code>会导致海外买家下单国内仓，时效延迟被投诉。</li><li><strong>包邮是 “部分地区”</strong> ：<code>is_free_shipping=True</code>不代表全地区包邮，必须查<code>free_shipping_countries</code>，否则中东、南美买家的运费会让你亏哭。</li></ol><h2>最后：给跨境开发者的 3 句真心话</h2><ol><li><strong>多语言别硬转</strong>：速卖通的<code>title_en</code>/<code>title_ru</code>是卖家手动填写的，比机器翻译准确 10 倍，别用翻译 API 转中文标题，会出现 “手机壳” 译成 “phone cover” 却和卖家填写的 “mobile case” 不符的问题。</li><li><strong>物流成本要加缓冲</strong>：速卖通的运费模板会随燃油费调整，解析时建议加 10% 缓冲（比如算出来 100 刀，实际按 110 刀预估），避免运费超支。</li><li><strong>大促前 3 天别调试</strong>：速卖通大促（双 11、黑五）前接口会限流收紧，免费版可能降到 5 次 / 分钟，提前一周完成调试，别临时改代码被封。</li></ol>]]></description></item><item>    <title><![CDATA[如何通过智能供应链管理提升制造效率？ 月]]></title>    <link>https://segmentfault.com/a/1190000047446950</link>    <guid>https://segmentfault.com/a/1190000047446950</guid>    <pubDate>2025-12-03 18:04:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>欢迎来到智能供应链管理的时代，这是一个由技术驱动的数字化、自动化、智能化战略转折点，它不仅改变了传统制造业的规划设计范式，也在全球供应链的构建上建立起全新范本。回想一下，传统的供应链管理长期存在三大深层次问题：首先，由于系统智能化程度不高，导致供应链结构过于脆弱，抗外部打击能力下降；其次，在复杂系统协作过程中，由于工业企业应用庞杂，难以突破不同系统间的数据壁垒；第三，面对全球绿色化转型的强力倒逼，碳计量方式仍停留在落后的人工记录阶段，难以合规。<br/>但是，正如广域铭岛所展示的那样，智能供应链管理的解决方案正在逐步推进这些挑战的破局。他们的典型路径是：融合20多种工业协议，构建毫秒级数据物流体系，在生产调度、能耗、质量等关键领域引入AI，实现场景下的多智能体动态协同。通过这些方法，将其中一个汽车制造工厂的全局库存周转率提升了30%以上。<br/>更值得关注的是，通过跨行业实践，我们看到了智能供应链管理的惊人效果：某铝电联合企业成功降低吨铝耗电200千瓦时，年节省电费7000万元；某芯片制造商有效切断了产能制约瓶颈，将市场响应周期压缩为原来的四分之一。广域铭岛打造出的GOS系统，通过实时数据收集、动态控制和智能预测，将企业运营效率推到了一个全新的高度。<br/>其中的精妙之处在于他们团队完成的架构设计：从数据感知层、智能化规划层和末端执行层构成了完整的闭环系统。尤其是选用的知识图谱技术，将企业运营中各种零散数据实现了有机融合，让质量追溯从过去的数周缩短至实时响应，这无疑是对智能供应链管理一流实践的最好注脚。<br/>广域铭岛团队还提出了一个值得借鉴的核心理念：智能供应链管理不是简单地改变某一个局部环节的实施策略，而应该是整个产业链协同运作方式的根本变革。这个理念体现在他们的数字孪生技术应用中，也体现在他们为客户提供的整套解决方案中。<br/>展望未来，智能供应链管理将随着更多AI技术的加入，生成更加灵性的协作模式。从预测性维护到分布式制造，从碳强度管控到全流程闭环优化，广域铭岛正为更多制造企业提供实时智能决策的生命动力。如果说制造业的智能化是一条漫长的进化之路，那么智能供应链管理就是其中一个划时代的关键节点。</p>]]></description></item><item>    <title><![CDATA[深度拆解：SAE 刚性交付的底层逻辑，从]]></title>    <link>https://segmentfault.com/a/1190000047446975</link>    <guid>https://segmentfault.com/a/1190000047446975</guid>    <pubDate>2025-12-03 18:03:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：张凤婷（娜米）</p><p>资源的刚性交付，不是云上天生就具备的能力。当选择自建或自管理一个 Kubernetes/ECS 资源池时，就必须直面一个残酷的现实：所依赖的底层 IaaS 资源本身就是非刚性的。</p><p>阿里云上 ECS 有多代实例规格（如 g6、c7i、r8y 等），基于 Intel、AMD 及自研倚天 ARM 芯片，但这并不保证在任何时刻、任何地域、任何可用区，所需要的那款机型就一定有库存。这种底层资源的“不确定性”，会像幽灵一样渗透到自建的上层系统中。</p><p>刚性交付的本质，是将“不确定性”从系统中排除的关键机制。它通过可控的资源成本，换取了业务的稳定性、高性能和可预测性。 对于任何严肃的线上业务而言，这种确定性并非锦上添花，而是维系其商业信誉和核心价值的生命线。</p><p>以下几个案例，阐述非刚性交付”带来的典型困境。</p><p><strong>案例一：游戏行业 —— 新品发布日的“容量灾难”</strong></p><ul><li>行业：在线游戏、元宇宙</li><li><p>故障：</p><ol><li>场景：一家游戏公司万众期待的新游戏正式公测。运营团队基于压测，制定了雄心勃勃的扩容计划，需要在开服瞬间将游戏服务器（通常需要高性能计算或 GPU 优化的特定 ECS 机型）的规模扩大 10 倍。他们管理着一个基于 K8s 的自建集群。</li><li>触发：开服铃声敲响，CI/CD 流水线触发了大规模的横向扩容。然而，K8s 的节点自动伸缩器 Cluster Autoscaler 在向阿里云申请创建新的 ECS 节点时，API 返回了“Insufficient stock”库存不足的错误。他们所依赖的特定高性能机型，在该可用区已无库存。</li><li>现象：应用的 Pod 因为没有足够的节点资源而大量处于 <code>Pending </code>状态，无法被调度。新玩家的登录请求雪片般涌入，但服务器容量远未达到预期。</li></ol></li><li><p>业务影响：</p><ul><li>上线即失败：大量玩家无法登录，游戏入口处大排长龙，社交媒体和游戏社区瞬间被负面评价淹没，精心策划的发布会变成了公关灾难。</li><li>真金白银的损失：高额的市场推广费用付诸东流，首日充值流水远低于预期。</li><li>玩家永久流失：糟糕的首日体验会导致大量核心玩家永久流失至竞品。</li></ul></li></ul><p><strong>案例二：电商行业 —— 大促活动中的“性能悬崖”</strong></p><ul><li>行业：电商与在线零售</li><li><p>故障：</p><ol><li>场景：一家电商平台为了应对大促，提前“预留”了大量 ECS 节点。为了“提高资源利用率”，他们在核心的交易应用 Pod 所在的节点上，混部了一些非核心的数据分析和日志处理 Pod，并配置了非刚性的 CPU 交付。</li><li>触发：大促零点开启，交易量飙升，交易应用需要全部申请的 CPU。同时，数据分析任务也开始高强度运行，抢占 CPU 资源。</li><li>现象：交易应用的实际可用 CPU 被严重挤压，响应时间急剧恶化，大量请求超时。</li></ol></li><li><p>业务影响：</p><ul><li>订单大量流失：支付和下单环节的堵塞，直接导致 GMV 损失。</li><li>品牌信誉受损：用户在关键时刻掉链子，严重损害品牌可靠性。</li></ul></li></ul><p><strong>案例三：金融科技行业 —— 交易时段的“随机掉线”</strong></p><ul><li>行业：金融科技 (FinTech)，尤其是证券交易</li><li><p>故障：</p><ol><li>场景：一个核心的行情推送 Java 服务，以内存非刚性交付的方式运行在一个自管理的 K8s 集群上。</li><li>触发：交易时段，订阅量激增，服务实际内存使用远超其申请值。此时节点内存压力增大，触发 OOM Killer。</li><li>现象：行情服务 Pod 被系统判定为“劣质进程”而随机杀死，导致客户端行情刷新中断。</li></ol></li><li><p>业务影响：</p><ul><li>交易决策失误：用户因行情中断而做出错误决策或错失交易时机，造成直接经济损失。</li><li>合规与监管风险：核心系统频繁中断，可能触犯金融行业的高可用性监管要求。</li></ul></li></ul><p><strong>案例四：企业软件行业 —— 核心ERP系统的“性能抽奖”</strong></p><ul><li>行业：企业软件 (ERP, CRM)，尤其是大型单体应用</li><li><p>故障：</p><ol><li>场景：一家企业将其庞大的、无法轻易水平扩展的单体 ERP 系统容器化后，部署在一个资源非刚性交付的自建集群上，以期“节约成本”。</li><li>触发：在月末财务结算等高峰期，ERP 系统需要大量 CPU 和内存。但它必须和节点上其他应用“共享”资源。</li><li>现象：ERP 系统的性能变得极不稳定，时快时慢，如同“抽奖”。有时一个报表生成需要 2 分钟，有时需要 20 分钟。</li></ol></li><li><p>业务影响：</p><ul><li>工作效率低下：员工的核心工作流程被频繁打断，财务、供应链等部门的月末结算工作无法按时完成。</li><li>决策延迟：管理者无法及时获取准确的业务报表，影响了商业决策的时效性。</li></ul></li></ul><h2>资源刚性交付困境</h2><h3>资源供给的不确定性</h3><p>困境本质：“承诺的资源” ≠ “可即时获取的资源”。</p><ul><li>库存波动：热门规格 ECS，在大促或行业高峰期容易出现“秒光”，导致扩容失败。</li><li>区域/可用区差异：某些 AZ 因物理机房容量限制，无法提供特定资源类型，跨 AZ 调度又需额外网络与配置成本。</li><li>代际断层：旧代实例停售或库存枯竭，但应用尚未适配新架构，造成刚性承诺无法兑现。</li></ul><h3>性能隔离难以真正实现</h3><p>困境本质：“逻辑隔离”不等于“物理隔离”，刚性性能难以 100% 保障。</p><ul><li>虚拟化开销与干扰：即使使用 Cgroups、CPU 绑核等技术，共享 NUMA 节点、内存带宽、磁盘 I/O 队列仍可能被“嘈杂邻居”抢占。</li><li>突发流量冲击：同节点上其他租户突发高负载（如备份、扫描），导致本应“独占”的实例出现延迟毛刺。</li><li>存储性能抖动：存储在多租户争抢下 IOPS 和吞吐不稳定，影响核心业务等关键应用。</li></ul><h3>弹性与刚性的内在矛盾</h3><p>困境本质：刚性要求确定性，弹性依赖不确定性，二者天然张力。</p><ul><li>预占 vs 按需：为保障刚性需提前预留资源，但业务负载波动大时造成浪费；若完全按需，则无法应对突发高峰。</li><li>冷启动延迟：首次启动需拉镜像、初始化，往往无法满足业务的刚性响应要求。</li></ul><h3>异构资源管理复杂度高</h3><p>困境本质：“资源刚性”需端到端栈协同，任一环节短板即导致整体失效。</p><ul><li>专用硬件：驱动版本、CUDA 兼容性、拓扑感知调度、故障恢复机制各异，难以标准化交付。</li><li>混合架构支持难：x86 与 ARM（如倚天 710）指令集不同，应用需重新编译测试，刚性交付需维护多套镜像与部署流程。</li><li>网络与存储耦合：高性能计算需 RDMA、NVMe over Fabric 等底层能力，但这些能力在虚拟化层常被削弱或不可用。</li></ul><h3>传统架构与云原生理念割裂</h3><p>困境本质：刚性交付不仅是技术问题，更是组织与认知转型问题。</p><ul><li>缺乏弹性设计：应用未做无状态改造，无法横向扩展，只能纵向升级（Scale-Up），而大规格实例更稀缺、更昂贵。</li><li>运维惯性阻力：企业习惯“买服务器、装系统、长期运行”，对“按需申请、用完即弃”的刚性交付模式接受度低。</li></ul><h3>成本模型与刚性目标冲突</h3><p>困境本质：财务约束常迫使技术理想向现实低头。</p><ul><li>刚性 = 高成本：独占物理机、专用集群、多 AZ 冗余等方案显著推高 TCO。</li><li>企业被迫妥协：为控制预算，用户常选择共享资源池+监控告警“事后补救”，而非事前刚性保障。</li><li>计费模式滞后：传统按小时计费无法匹配秒级弹性需求，导致“为不用的资源付费”或“关键时刻无资源可用”。</li></ul><h2>SAE 在刚性交付上做的工作</h2><p>作为阿里云面向应用层的全托管 Serverless PaaS 平台，针对资源刚性交付的系统性困境，从<strong>资源供给、性能隔离、弹性模型、异构调度、成本结构、容灾能力、可观测性与架构演进</strong>等多个维度进行了设计。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446977" alt="image" title="image"/></p><h3>1. 破解“资源供给不确定性” → 构建无限弹性资源池</h3><ul><li>多源异构资源整合：  <br/>SAE 背后打通神龙裸金属服务器、弹性容器实例（ECI）支持各代  x86/ARM 等海量资源，形成统一调度池。</li><li>智能跨机型调度：  <br/>当用户指定规格库存不足时，调度器自动选择性能相当、兼容性一致的替代资源（如 g7 缺货 → 自动调度 g8i），全程对用户透明。</li><li>结果：  <br/>交付的是“计算能力”，而非“特定机型”，彻底规避因库存波动导致的扩容失败。</li></ul><h3>2. 解决“性能隔离难” → 天然沙箱化 + 独占资源</h3><ul><li>默认运行在 ECI 沙箱中：  <br/>每个应用实例运行在轻量级安全容器，实现内核级隔离，杜绝“嘈杂邻居”干扰。</li><li>资源 100% 独占：  <br/>用户申请的 CPU、内存、网络带宽均由 runD 底层安全沙箱保障，无超分、无争抢，性能稳定可预期。</li><li>结果：  <br/>刚性性能不再是“尽力而为”，而是确定性交付，尤其适合金融交易、实时推荐等敏感场景。</li></ul><h3>3. 调和“弹性与刚性矛盾” → 按实际用量计费 + 缩容至零</h3><ul><li>闲置不计费：  <br/>应用缩容到 0 实例时，CPU/内存资源完全释放，不产生费用（仅保留配置元数据）。</li><li>秒级冷启动优化：  <br/>结合镜像预热、快照加速、本地缓存等技术，大幅缩短首次启动延迟，逼近“即时刚性响应”。</li><li>结果：  <br/>用户无需为“以防万一”长期预留资源，刚性保障与极致成本兼得，替代高风险混部策略。</li></ul><h3>4. 简化“异构资源管理” → 屏蔽底层复杂性</h3><ul><li>ARM/x86 无缝兼容：  <br/>如支持海光国产芯片，用户只需提供兼容镜像，SAE 自动完成调度与运行时适配。</li><li>结果：  <br/>开发者只需关注“我要多少算力”，无需关心“卡在哪台机器上、驱动是否匹配”。</li></ul><h3>5. 重构“成本模型” → 从“买资源”到“买能力”</h3><ul><li>按实际 CPU/内存使用量秒级计费：  <br/>不再按整机小时付费，避免资源闲置浪费。</li><li>免运维成本：  <br/>无需管理节点、打补丁、编写扩缩容脚本，人力成本大幅降低。</li><li>结果：  <br/>刚性交付不再昂贵，中小企业也能享受企业级可靠性。</li></ul><h3>6. 强化“容灾与高可用” → 多可用区刚性容灾</h3><ul><li>一键开启多 AZ 部署：  <br/>SAE 自动将应用实例分散到多个可用区，跨机房冗余。</li><li>AZ 故障自动恢复：  <br/>若某 AZ 整体不可用，SAE 在其他 AZ 刚性拉起新实例，RTO 控制在分钟级。</li><li>结果：  <br/>刚性交付从“单点稳定”升级为“应用级连续性保障”。</li></ul><h3>7. 提升“可观测性与可信度” → 内置全链路监控</h3><ul><li>集成 ARMS + SLS + Prometheus：  <br/>提供应用性能监控（APM）、日志、指标、链路追踪一体化视图。</li><li>资源使用透明化：  <br/>用户可清晰看到 CPU 使用率、内存水位、网络吞吐是否达到承诺值。</li><li>结果：  <br/>刚性 SLA 可验证、可审计，告别“黑盒交付”。</li></ul><h3>8. 支持“传统应用平滑演进” → 兼顾稳定与未来</h3><ul><li>支持 WAR/JAR/镜像直接部署：  <br/>ERP、OA 等单体应用无需改造即可运行在 SAE 上，享受刚性资源保障。</li><li>内置诊断能力：  <br/>通过性能剖析定位瓶颈（如数据库慢查询、线程阻塞），为后续微服务拆分提供数据依据。</li><li>结果：  <br/>SAE 不仅是“运行平台”，更是企业云原生转型的跳板。</li></ul><h2>了解 Serverless 应用引擎 SAE</h2><p>阿里云 Serverless 应用引擎 SAE 是面向 AI 时代的一站式容器化应用托管平台，以“托底传统应用、加速 AI 创新”为核心理念。它简化运维、保障稳定、闲置特性降低 75% 成本，并通过 AI 智能助手提升运维效率。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446978" alt="image" title="image" loading="lazy"/></p><p>面向 AI，SAE 集成 Dify 等主流框架，支持一键部署与弹性伸缩，在 Dify 场景中实现性能<strong>提升 50 倍、成本优化 30% 以上</strong>。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446979" alt="image" title="image" loading="lazy"/></p><h3>产品优势</h3><p>凭借八年技术沉淀，SAE 入选 2025 年 Gartner 云原生魔力象限全球领导者，亚洲第一，助力企业零节点管理、专注业务创新。SAE 既是传统应用现代化的“托举平台”，也是 AI 应用规模化落地的“加速引擎”。</p><p><strong>1. 传统应用运维的“简、稳、省”优化之道</strong></p><ul><li>简：零运维心智，专注业务创新</li><li>稳：企业级高可用，内置全方位保障</li><li>省：极致弹性，将成本降至可度量</li></ul><p><strong>2. 加速 AI 创新：从快速探索到高效落地</strong></p><ul><li>快探索：内置 Dify、RAGFlow、OpenManus 等热门 AI 应用模板，开箱即用，分钟级启动 POC；</li><li>稳落地：提供生产级 AI 运行时，性能优化（如 Dify 性能提升 50 倍）、无感升级、多版本管理，确保企业级可靠交付；</li><li>易集成：深度打通网关、ARMS、计量、审计等能力，助力传统应用智能化升级。</li></ul><h2>适合谁？</h2><p>✅ 创业团队：没有专职运维，需要快速上线  <br/>✅ 中小企业：想降本增效，拥抱云原生  <br/>✅ 大型企业：需要企业级稳定性和合规性  <br/>✅ 出海企业：需要中国区 + 全球部署  <br/>✅ AI 创新团队：想快速落地 AI 应用</p><h3>了解更多</h3><p>产品详情页地址（点击阅读原文即可查看）：<a href="https://link.segmentfault.com/?enc=afj%2BR0CE%2FyioGlwm8FRgOA%3D%3D.lUXk%2BeHpZJhnxSVdYZNj8D06QFBYCYeoq%2Fx9P4olwQB5gltUMBX87zY%2F0zXhi2x1" rel="nofollow" target="_blank">https://www.aliyun.com/product/sae</a></p><p>欢迎使用钉钉搜索群号： 23156632</p><p>加入 SAE 客户服务群 👇</p>]]></description></item><item>    <title><![CDATA[美股 (US) 与 墨西哥 (Mexic]]></title>    <link>https://segmentfault.com/a/1190000047446988</link>    <guid>https://segmentfault.com/a/1190000047446988</guid>    <pubDate>2025-12-03 18:02:50</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>1. 接入概述 (General)</h2><p>本接口用于获取美国（NYSE, NASDAQ, AMEX）及墨西哥（BMV, BIVA）证券市场的实时行情、历史 K 线及指数数据。</p><ul><li><strong>API Base URL</strong>: <code>https://api.stocktv.top</code></li><li><strong>WebSocket URL</strong>: <code>wss://ws-api.stocktv.top/connect</code></li><li><strong>鉴权方式</strong>: 所有请求均需携带 URL 参数 <code>key=您的API密钥</code></li></ul><h3>1.1 关键市场 ID (Country ID)</h3><p>在调用相关接口时，请务必区分以下 <code>countryId</code>：</p><table><thead><tr><th align="left">市场名称</th><th align="left">Country ID</th><th align="left">交易所示例</th></tr></thead><tbody><tr><td align="left"><strong>美国 (USA)</strong></td><td align="left"><strong>5</strong></td><td align="left">NYSE (1), NASDAQ (2), AMEX</td></tr><tr><td align="left"><strong>墨西哥 (Mexico)</strong></td><td align="left"><strong>7</strong></td><td align="left">Mexico (53), BIVA (144)</td></tr></tbody></table><hr/><h2>2. 核心数据接口</h2><h3>2.1 获取股票列表 (Stock List)</h3><p>用于查询指定市场的股票清单，获取股票的名称、代码 (Symbol) 和 <strong>系统 ID (PID)</strong>。</p><blockquote><strong>注意</strong>：<code>id</code> (PID) 是后续查询 K 线和订阅 WebSocket 的唯一标识符。</blockquote><ul><li><strong>接口地址</strong>: <code>/stock/stocks</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>:</li></ul><table><thead><tr><th align="left">参数名</th><th align="left">类型</th><th align="left">必填</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><code>key</code></td><td align="left">String</td><td align="left">是</td><td align="left">您的 API Key</td></tr><tr><td align="left"><code>countryId</code></td><td align="left">Int</td><td align="left">是</td><td align="left"><strong>5</strong> (美股) 或 <strong>7</strong> (墨西哥)</td></tr><tr><td align="left"><code>pageSize</code></td><td align="left">Int</td><td align="left">否</td><td align="left">每页数量 (默认 10)</td></tr><tr><td align="left"><code>page</code></td><td align="left">Int</td><td align="left">否</td><td align="left">页码 (默认 1)</td></tr></tbody></table><ul><li><p><strong>请求示例 (获取美股列表)</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/stocks?countryId=5&amp;pageSize=20&amp;page=1&amp;key=YOUR_KEY</code></pre></li><li><p><strong>响应示例</strong>:</p><pre><code class="json">{
  "code": 200,
  "data": {
    "records": [
      {
        "id": 8888,          // [关键] PID，用于K线接口
        "name": "Apple Inc", // 股票名称
        "symbol": "AAPL",    // 股票代码
        "exchangeId": 2,     // 交易所ID (2=NASDAQ)
        "last": 180.5,       // 最新价
        "chgPct": 1.25,      // 涨跌幅%
        "countryNameTranslated": "United States"
      }
    ]
  }
}</code></pre></li></ul><hr/><h3>2.2 获取 K 线数据 (Candlestick Data)</h3><p>获取指定股票的历史行情数据，支持多种时间周期。</p><ul><li><strong>接口地址</strong>: <code>/stock/kline</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>:</li></ul><table><thead><tr><th align="left">参数名</th><th align="left">类型</th><th align="left">必填</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left"><code>key</code></td><td align="left">String</td><td align="left">是</td><td align="left">您的 API Key</td></tr><tr><td align="left"><code>pid</code></td><td align="left">Int</td><td align="left">是</td><td align="left">股票系统 ID (通过 2.1 接口获取)</td></tr><tr><td align="left"><code>interval</code></td><td align="left">String</td><td align="left">是</td><td align="left">K线周期 (ISO 8601格式)</td></tr></tbody></table><ul><li><p><strong>周期 (Interval) 说明</strong>:</p><ul><li><code>PT1M</code> (1分钟), <code>PT5M</code> (5分钟), <code>PT15M</code> (15分钟), <code>PT30M</code> (30分钟), <code>PT1H</code> (1小时)</li><li><code>P1D</code> (日线), <code>P1W</code> (周线), <code>P1M</code> (月线)</li></ul></li><li><p><strong>请求示例 (获取墨西哥某股票日线)</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/kline?pid=12345&amp;interval=P1D&amp;key=YOUR_KEY</code></pre></li><li><p><strong>响应示例</strong>:</p><pre><code class="json">{
  "code": 200,
  "data": [
    {
      "time": 1719818400000, // 时间戳 (毫秒)
      "open": 150.0,
      "high": 155.0,
      "low": 149.0,
      "close": 153.0,
      "volume": 200000
    }
  ]
}</code></pre></li></ul><hr/><h3>2.3 获取大盘指数 (Indices)</h3><p>获取美股（如纳斯达克、标普500）或墨西哥（如 S\&amp;P/BMV IPC）的指数行情。</p><ul><li><strong>接口地址</strong>: <code>/stock/indices</code></li><li><strong>请求方式</strong>: <code>GET</code></li><li><strong>请求参数</strong>: <code>countryId</code> (5=美国, 7=墨西哥)</li><li><p><strong>请求示例</strong>:</p><pre><code class="http">GET https://api.stocktv.top/stock/indices?countryId=7&amp;key=YOUR_KEY</code></pre></li></ul><hr/><h2>3. WebSocket 实时推送</h2><p>通过 WebSocket 长连接接收实时报价更新。</p><ul><li><strong>连接地址</strong>: <code>wss://ws-api.stocktv.top/connect?key=YOUR_KEY</code></li><li><strong>心跳机制</strong>: 连接建立后，建议定期发送心跳包以保持连接。</li><li><p><strong>推送数据结构</strong>:</p><pre><code class="json">{
    "pid": "8888",         // 对应 Rest API 中的 id
    "last_numeric": 181.2, // 最新价
    "pcp": "0.39",         // 涨跌幅%
    "timestamp": "1717728251",
    "bid": "181.1",        // 买价
    "ask": "181.3",        // 卖价
    "type": 1              // 1=股票, 2=指数
}</code></pre></li></ul><hr/><h2>4. 接入代码示例 (JavaScript)</h2><p>以下代码展示了如何根据 <code>countryId</code> 封装获取美股和墨西哥股票的逻辑。</p><pre><code class="javascript">const API_KEY = 'YOUR_API_KEY';
const BASE_URL = 'https://api.stocktv.top';

// 配置 ID
const MARKETS = {
    USA: 5,
    MEXICO: 7
};

/**
 * 获取指定市场的股票列表
 * @param {number} countryId - 5 for USA, 7 for Mexico
 */
async function getMarketStocks(countryId) {
    const url = `${BASE_URL}/stock/stocks?countryId=${countryId}&amp;pageSize=10&amp;page=1&amp;key=${API_KEY}`;
    try {
        const response = await fetch(url);
        const result = await response.json();
        
        if (result.code === 200) {
            console.log(`市场 (ID:${countryId}) 股票列表:`, result.data.records);
            // 示例：获取第一个股票的 PID 用于查 K 线
            if(result.data.records.length &gt; 0) {
                const firstStock = result.data.records[0];
                console.log(`示例股票: ${firstStock.name}, PID: ${firstStock.id}`);
            }
        }
    } catch (error) {
        console.error('API 请求失败:', error);
    }
}

// 1. 获取美股数据
getMarketStocks(MARKETS.USA);

// 2. 获取墨西哥股票数据
getMarketStocks(MARKETS.MEXICO);</code></pre>]]></description></item><item>    <title><![CDATA[如何实现智能研发协同以提升制造业效率？ ]]></title>    <link>https://segmentfault.com/a/1190000047446994</link>    <guid>https://segmentfault.com/a/1190000047446994</guid>    <pubDate>2025-12-03 18:02:06</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>随着工业4.0时代的到来，制造业正经历一场由数字化和人工智能驱动的深刻革命。传统研发模式在数据孤岛、跨部门协作效率低下以及知识复用率低等问题的制约下，难以满足现代企业对敏捷性和创新性的需求。广域铭岛作为这一领域的先行者，凭借其捷做设计研发协同平台和Geega工业互联网平台，提出了一种全新的智能研发协同理念，旨在通过技术的深度融合，提升企业的研发效率与创新能力。<br/>捷做平台的核心在于打破传统研发中的壁垒，实现全流程透明化与数据驱动的协同管理。它不仅支持设计、工艺和生产数据的统一管理，还通过模块化设计、可配置BOM（物料清单）以及变更闭环控制等功能，优化了企业内部的研发协作模式。广域铭岛的解决方案以客户需求为导向，将研发过程与生产需求紧密结合，确保设计即研发、设计即生产的核心原则。<br/>在智能研发协同中，广域铭岛充分利用了现代技术架构的优势。基于微服务的系统设计，使得每个业务模块都具备高度的独立性和灵活性。多租户技术的应用则保证了不同企业在同一平台上实现数据隔离与个性化配置，而高性能数据库的引入，尤其是基于图数据库的BOM管理，极大地提升了数据检索和处理的效率。这些技术的结合，使得捷做在复杂的研发环境中表现出色，成为制造业数字化转型的关键支撑。<br/>此外，广域铭岛还通过其专属的捷做平台，进一步强化了智能研发协同的核心。捷做构建了三级数据架构，涵盖了数据接入、治理及服务。通过对企业生产数据的实时分析与整合，它打破了数据的孤岛效应，实现了研发与生产的深度互联。更为重要的是，捷做设计研发协同平台将工艺规则和设备参数转化为可复用的数字资产，显著提升了知识密集型业务的处理效率。例如，在汽车焊接工艺中，通过对电流、电压和送丝速度等参数的封装，形成了“焊点质量指数”，从而帮助企业在设计验证中做出更精准的判断。<br/>在多个行业中的实践证实了广域铭岛智能研发协同策略的有效性。新能源电池领域的案例中，捷做平台通过数据建模和工艺优化，帮助企业将良品率提升8%，并将设备故障时间减少了65%。而在汽车行业，吉利集团借助该平台实现了每年30多款新车型的并行研发，不仅将零部件通用化率提升至75%，还显著降低了单车研发成本。这些成果不仅仅是数据的提升，更是整个研发范式的重构，体现了广域铭岛在技术驱动与业务融合上的领先地位。<br/>面向未来，广域铭岛持续推进两大技术方向：生成式研发助手和数字孪生研发环境。生成式研发助手基于工业大模型，能够通过自然语言和设计需求，快速生成设计图纸，帮助企业缩短设计周期；而数字孪生研发环境则构建了高保真的虚拟工厂，支持研发人员在仿真环境中实时调试设备参数，将工艺优化周期从周级压缩至小时级。这些创新不仅为制造业的研发提供了更高效的解决方案，还进一步加剧了智能研发协同的影响力。<br/>最终，广域铭岛的智能研发协同模式强调的是一种以数据为核心的开放生态系统。通过推动研发流程与实际业务的深度融合，它帮助制造企业突破传统模式的瓶颈，实现从创意到落地的无缝衔接。在这个过程中，数据管理和知识的多元共用成为关键，彰显了广域铭岛产品的智能化与前瞻性。</p>]]></description></item><item>    <title><![CDATA[如何将音乐从 iTunes 传输到闪存驱]]></title>    <link>https://segmentfault.com/a/1190000047447014</link>    <guid>https://segmentfault.com/a/1190000047447014</guid>    <pubDate>2025-12-03 18:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>对于喜欢收藏音乐的用户来说，iTunes 是一款经典且实用的音乐管理工具。然而，有时用户可能希望将 iTunes 中的音乐复制到 U 盘，以便在车载音响、电视或其他设备上轻松播放。那么，可以将歌曲从 iTunes 传输到 U 盘吗？当然可以。本文将提供详细的指南，教您如何将音乐从 iTunes 传输到 U 盘，帮助您轻松备份和携带音乐。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447016" alt="图片" title="图片"/></p><h3>第一部分：如何直接将歌曲从 iTunes 复制到 U 盘</h3><p>无论你使用的是Windows 11/10/8 还是Mac ，你都可以轻松地将歌曲直接从 iTunes 音乐库传输到闪存驱动器。</p><p>以下是如何将音乐从 iTunes 传输到 U 盘的方法：</p><p>步骤 1. 打开电脑上的iTunes应用。</p><p>步骤 2. 打开您的音乐库或播放列表。在Windows中，请确保左上角的“音乐”选项卡已选中，然后依次点击“资料库”&gt;“歌曲”查看所有曲目。在 macOS 系统中，点击“音乐”图标，然后前往“播放列表”选项卡。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447017" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>步骤 3：插入 U 盘并打开。连接成功后，打开 U 盘文件夹，并确保桌面上的 iTunes 窗口也已打开。（ iTunes 一直崩溃？）</p><p>步骤 4：在 iTunes 中选择要传输的歌曲，然后将它们拖到 U 盘的文件夹中。在Windows中，按住“Ctrl”或“Shift”键可一次选择多首歌曲，然后将它们拖放到 U 盘中。在 macOS 系统中，使用“Command”键执行相同的操作。</p><p>您还可以从 iTunes 导出整个播放列表或音乐库，然后将其保存到 U 盘。如果您的播放器仅支持 MP3 或其他特定格式，则需要先将文件转换为所需的格式。</p><p>具体操作方法如下：</p><p>步骤 1. 打开 iTunes，然后转到菜单栏并选择“编辑”&gt;“偏好设置”&gt;“通用”。点击“导入设置”，在“导入方式”选项下选择“MP3 编码器”，然后点击两次“确定”以保存设置并返回您的资料库。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447018" alt="图片" title="图片" loading="lazy"/></p><p>步骤 2. 将 USB 插入电脑并创建一个新文件夹。</p><p>步骤 3. 接下来，打开 iTunes，选择要传输的播放列表，然后转到“文件”&gt;“资料库”&gt;“导出资料库”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447019" alt="图片" title="图片" loading="lazy"/><br/>​</p><p>步骤 4. 在文件浏览器中，选择 USB 作为保存播放列表文件的目标位置。</p><p>想知道如何将 iPhone 中的歌曲导入 iTunes 吗？本指南将介绍两种有效的方法，并提供分步说明，教您如何将音乐从 iPhone 传输到 iTunes 资料库。</p><p>将 iPhone 音乐传输到 iTunes 资料库的两种方法</p><h3>第二部分：如何将 iTunes 媒体文件夹中的音乐传输到 U 盘</h3><p>除了直接将 iTunes 音乐传输到 U 盘外，您还可以找到计算机上存储 iTunes 歌曲的“iTunes Media”文件夹，然后将其复制到 U 盘。</p><p>以下是如何将 iTunes 媒体文件夹中的音乐传输到 U 盘的方法：</p><p>第一步：打开 iTunes，然后点击“编辑”&gt;“偏好设置”。此时会弹出一个新窗口。</p><p>步骤 2. 勾选“保持 iTunes 媒体文件夹有序”和“添加到资料库时将文件复制到 iTunes 媒体文件夹”，然后单击“确定”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447020" alt="图片" title="图片" loading="lazy"/></p><p>注意：如果您想更改 iTunes 媒体文件夹的位置，请点击“更改…”并选择新位置。</p><p>步骤 3. 转到“文件”&gt;“库”&gt;“整理库...”，选中“合并文件”，然后单击“确定”。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447021" alt="图片" title="图片" loading="lazy"/></p><p>第四步：完成以上步骤后，所有媒体文件都将保存在 iTunes 媒体文件夹中。然后，您可以打开该文件夹，将播放列表拖放到您的 U 盘中。</p><p>额外内容：如何将音乐从 iPhone 传输到 U 盘</p><p>如果你的音乐不在 iTunes 里，而是存储在 iPhone 上怎么办？在这种情况下，如果你想直接将其导出到 U 盘，可以使用第三方工具。Coolmuster iOS Coolmuster是一款专业的iOS设备管理软件，它可以将 iPhone 中的音乐、照片、联系人、信息和其他数据导出到电脑或 U 盘。它易于使用，适合所有用户。</p><p>iOS助手亮点：</p><pre><code>轻松将 iPhone 中的音乐传输到闪存盘/USB 驱动器。
支持联系人、短信、照片、视频、日历、应用等。
一键轻松备份和恢复您的 iPhone /iPad。
预览并选择iOS文件后，即可轻松传输文件。
在您的电脑上全面管理 iTunes 备份文件和iOS数据。
直接通过 PC 或Mac编辑、添加或删除iOS设备上的数据。
支持最新的iOS 26版本和iPhone 17系列。

</code></pre><p>以下是如何使用iOS助理将音乐从 iPhone 传输到 U 盘的方法：</p><p>01将此工具下载并安装到您的电脑上。使用 USB 数据线将您的 iPhone 连接到电脑，然后插入您的 U 盘。</p><p>02检测到您的 iPhone 后，请在手机上点击“信任”，然后在程序中点击“继续”以建立连接。之后，您将看到如下所示的主界面。可以看到，所有不同的文件夹都已整理在主屏幕上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047447022" alt="图片" title="图片" loading="lazy"/></p><p>03点击“音乐”部分查看您的歌曲列表。您可以根据需要选择单首歌曲或选择整个音乐文件夹进行传输。然后，点击“导出”按钮，选择您的U盘作为目标位置，并将所选音乐文件保存到U盘中。</p><h3>结尾</h3><p>无论您是想将音乐传输到车载播放器播放、备份音乐收藏，还是与朋友分享，以上介绍的将 iTunes 音乐传输到 U 盘的方法都简单实用。您只需选择最适合自己的方法即可轻松完成传输。</p><p>但是，如果你的音乐文件存储在 iPhone 上， Coolmuster iOS Assistant就是理想之选。它能让你快速导出音乐，还能在一个便捷的界面中管理手机上的其他数据。<br/>​</p>]]></description></item><item>    <title><![CDATA[告别数据孤岛与运维盲区：一款数字孪生平台]]></title>    <link>https://segmentfault.com/a/1190000047446682</link>    <guid>https://segmentfault.com/a/1190000047446682</guid>    <pubDate>2025-12-03 17:06:27</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>当园区的安防、能耗、设备、环境数据散落在十几个不同的系统里，当一次应急调度需要打电话、查图纸、跑现场才能拼凑出全局信息，当领导视察时只能看到静态的PPT汇报而非动态的运营实况——这或许是许多园区运营管理者正在经历的日常。<br/>在数字化的浪潮下，园区运营正从传统的“人防+技防”向“数据驱动、可视可控”的智慧运营演进。然而，理想与现实之间，往往横亘着技术实现的鸿沟：多源数据如何融合？三维场景如何快速构建？业务规则如何灵活配置？对于广大应用开发者而言，这既是巨大的市场机遇，也是棘手的技术挑战。<br/>今天，我们想抛开浮夸的概念，从一个资深开发者的视角，深入探讨一款名为“孪易 数字孪生 IOC”的工具平台，看看它如何通过一系列具体而微的功能设计，为园区智慧运营提供一套“开箱即用、深度可配”的解决方案，并在此过程中，为开发者打开一扇高效交付的大门。</p><h2>一、 核心痛点：园区运营的“数据之困”与“场景之渴”</h2><p>在深入产品之前，我们必须先理解园区运营的核心诉求。一个现代化的产业园区、智慧园区或大型综合体，其运营复杂度极高：<br/>系统林立，数据割裂：楼宇自控、视频监控、门禁停车、能耗管理、设备运维、环境监测……各系统独立建设，数据格式与协议各异，形成一个个“数据孤岛”。管理者无法获得统一、实时的全局态势。<br/>空间复杂，管理低效：地下管网、楼层结构、设备位置等信息依赖二维图纸或人工记忆。故障定位慢、资产盘点难、空间利用率分析缺乏直观依据。<br/>业务多样，响应滞后：安防告警、设备预警、环境超标、能耗突增等事件分散在不同值班岗位。缺乏跨业务的联动分析与统一指挥看板，应急响应效率低下。<br/>价值呈现，手段单一：向领导、访客展示运营成果时，往往停留在图表和报告层面，缺乏一个直观、动态、可交互的“数字孪生体”来生动呈现园区科技实力与管理水平。<br/>对于承接此类项目的开发者或集成商而言，挑战在于：如何在不投入巨额成本和漫长时间进行底层开发的情况下，构建一个能打通上述环节、满足客户核心价值的数字孪生应用？<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmP6B" alt="" title=""/></p><h2>二、 破局之道：从“数据接入”到“业务呈现”的全链路配置化能力</h2><p>“孪易 数字孪生 IOC”提供了一套完整的工具链，其设计哲学可以概括为：“汇聚数据于一体，呈现业务于一屏，赋能管理于一键”。下面，我们结合园区场景，拆解其关键功能如何直击痛点。</p><h3>1. 数据融合：构建全域感知的“数字底板”</h3><p>数字孪生的生命在于数据。该平台的核心优势之一是强大的多源异构数据集成能力。它并非一个封闭的3D渲染引擎，而是一个强大的数据中枢。<br/><strong>对于物联网数据</strong>：它支持通过标准协议（如MQTT）或直接对接主流云物联网平台（如华为云IoTDA、阿里云物联网平台），将成千上万的传感器数据（温湿度、能耗、设备状态、车位状态等）实时接入。<br/><strong>对于业务系统数据</strong>：支持连接MySQL、PostgreSQL等关系型数据库，也能适配国产数据库，轻松获取资产信息、工单记录、人流统计等业务数据。<br/>对于视频数据：支持接入RTSP、FLV等格式的实时视频流，并可将视频画面与三维场景中的摄像头模型关联，实现“点击摄像头，即看实时画面”的融合监控。<br/><strong>价值点</strong>：开发者无需为每种数据源编写复杂的解析和对接代码，只需在后台进行配置，即可将分散的数据汇聚成园区统一的“数字底板”，为上层应用提供“燃料”。这极大地降低了数据整合阶段的技术门槛与时间成本。</p><h3>2. 场景构建：从“一张白纸”到“鲜活园区”的快速复刻</h3><p>有了数据，还需要承载数据的场景。平台提供了灵活的场景构建能力。<br/><strong>多格式支持</strong>：支持导入OBJ、FBX、GLTF等通用3D模型，以及BIM（建筑信息模型）和GIS（地理信息系统）数据。这意味着开发者可以利用园区已有的设计资料（如BIM模型）快速构建高保真的三维场景，而不是一切从零建模。<br/><strong>行业化预设</strong>：更值得一提的是其预置的行业插件库。针对“智慧园区”，平台可能已经内置了标准化的办公楼、厂房、停车场、路灯、配电箱等三维模型库，以及常见的园区业务数据模型。开发者可以像搭积木一样，快速组合出园区的三维骨架。<br/><strong>价值点</strong>：这解决了“从0到1”构建场景的漫长过程。开发者可以聚焦于业务逻辑和特色功能开发，基础的环境构建工作得以大幅提效，项目交付周期显著缩短。</p><h3>3. 业务配置：让“监、管、控、析”变得可定义</h3><p>这是平台最具魅力的部分——强大的后台配置能力，让非核心开发人员也能参与应用搭建。<br/><strong>对象管理</strong>：在庞大的三维场景中，如何快速找到一台故障的空调机组？平台提供对象管理面板和全局搜索功能，支持按分类、楼层、系统进行筛选，或直接搜索名称，并一键定位到三维场景中的具体位置，实现“所想即所见，所见即所得”。<br/><strong>智能告警</strong>：告警不再是简单的越限提示。开发者或运维人员可以在后台自定义复杂的告警规则。例如，可以设定“当会议室温度高于28℃且室内有人时”才触发告警，避免空房间的误报。告警触发后，会在三维场景中高亮显示，并支持一键定位、查看详情、关联视频、下发处置工单，形成闭环。<br/><strong>主题分析</strong>：平台支持创建业务主题看板。例如，可以创建一个“能效管理”主题，将园区总用电趋势图、各栋楼分项能耗排名、重点耗能设备列表、以及三维场景中的能耗热力图，全部聚合在一个页面。这相当于为不同业务部门（如工程部、安防部）定制了他们的专属“作战指挥室”。<br/><strong>交互控制</strong>：对于可控设备（如智能照明、门禁、空调），平台支持在三维场景中直接发送控制指令。点击三维场景中的一盏灯，弹出开关面板，操作后状态实时反馈回三维模型。这实现了真正的“三维组态”，让管理操作无比直观。<br/><strong>价值点</strong>：将大量需要编码实现的业务逻辑，转化为后台的可视化配置项。项目交付后，园区运营方可以根据业务变化自行调整规则和看板，赋予了平台长期生命力，也减少了开发者的后期维护负担。</p><h3>4. 模式创新：“免费试用”与“平滑演进”的友好路径</h3><p>对于开发者和最终用户，平台的商业模式也体现了灵活性。<br/><strong>低成本启动</strong>：提供免费公有云标准版，允许用户以极低的门槛进行概念验证（PoC）。开发者可以先用它搭建一个简化版Demo向客户演示，验证技术路线的可行性。<br/><strong>灵活部署</strong>：随着项目深入，可以平滑迁移至功能更强大的专业版，或根据客户对数据安全的要求，采用完全的私有化部署。这种“先尝后买”的模式，降低了双方的初始决策风险和投入成本。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdmRb8" alt="" title="" loading="lazy"/></p><h2>三、 场景闭环：看一个智慧园区的一天如何被改变</h2><p>让我们构想一个应用了该平台的智慧园区日常：<br/>清晨，能耗巡检：运营主管打开“能源主题”看板，三维园区地图上覆盖着昨夜的电耗热力图，一眼锁定异常高耗区域。点击该建筑，自动剖切显示楼层，并关联出该楼层空调主机的运行曲线，初步判断是否为设备异常。<br/>上午，安全监控：周界入侵告警触发。指挥中心大屏上，三维园区地图自动定位到告警点，并弹出附近多个摄像头的实时画面。值班员一键调度最近的巡逻机器人前往查看，并在三维地图上实时跟踪机器人轨迹。<br/>下午，设施维修：某企业报修空调故障。客服人员在工单系统录入后，维修工程师在移动端收到任务。他打开App上的园区三维地图，工单位置已被精准标注。他查看该空调的历史运行数据后前往维修，维修后状态同步更新至三维模型。<br/>傍晚，领导视察：无需准备复杂的PPT，运营方直接在指挥中心的大屏上，通过三维数字孪生园区，动态展示人流车流、能耗对比、安防布控、绿色减碳成果，所有数据实时刷新，汇报生动而有力。</p><h2>结语：给开发者的价值主张</h2><p>“孪易 数字孪生 IOC 标准版”本质上是一个 “数字孪生应用生产力工具” 。它不试图取代开发者，而是旨在赋能开发者。<br/>对于从事园区、城市、工业等垂直领域数字化解决方案的开发者而言，它的价值在于：<br/><strong>提升交付效率</strong>：将重心从底层技术开发（如3D引擎、数据中间件）转移到上层业务价值实现，缩短项目周期。<br/><strong>降低技术风险</strong>：基于一个成熟、稳定的平台进行开发，避免了自研技术框架的不确定性和长周期投入。<br/><strong>增强客户粘性</strong>：交付给客户的不是一个“黑盒”系统，而是一个运营方可参与配置、持续演进的“活”平台，能带来更好的客户满意度和长期合作机会。<br/><strong>拓展能力边界</strong>：即使团队不擅长3D或大数据技术，也能承接和交付高质量的数字孪生项目，开拓新的市场赛道。<br/>智慧园区的运营升级，是一场涉及数据、空间与业务的深刻变革。拥有一个能巧妙融合这三者的工具，无疑能让开发者在这场变革中，更从容地扮演赋能者和共建者的角色。</p>]]></description></item><item>    <title><![CDATA[BOM 冻结线为何总被打破？硬件研发“返]]></title>    <link>https://segmentfault.com/a/1190000047446701</link>    <guid>https://segmentfault.com/a/1190000047446701</guid>    <pubDate>2025-12-03 17:05:28</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><em>几乎所有做硬件研发管理和项目管理的人，都设过“BOM 冻结线”，也常常眼看着它一次次被新发现的问题击破：紧急改料、补充测试、推迟排产，导致材料报废和工程返工工时逐年增加。本文将结合ALM 应用生命周期管理与 IPD 集成产品开发体系，拆解 BOM 冻结管理失效的根因，并给出一套可落地的治理框架，帮助硬件团队真正跳出“返工陷阱”</em></blockquote><h2>硬件研发现场的 BOM 冻结尴尬</h2><p>如果你长期在硬件产品研发、电子制造业或硬件项目管理一线工作，下述场景大概率不陌生：</p><p>项目已经通过量产评审，会议纪要上写着“BOM 正式冻结”；供应链刚锁完物料采购，制造也依据 MBOM 排好了产能和生产节拍；你以为一切都在掌握中，结果下一刻，一封主题为《紧急：某型号器件需更换》的邮件打破了宁静——要么是关键器件停产，要么是可靠性测试刚测出边界问题，要么是经营压力要求本项目再压一轮物料成本。</p><p>接下来，工程变更管理（ECR/ECO）在系统里排成长龙：</p><ul><li>大量 ECO 触发 PCB 改版、工装治具调整、测试用例补测；</li><li>物料清单（BOM，Bill of Materials）在 EBOM、MBOM、ERP 三端反复校对；</li><li>项目经理在“交付进度、成本控制、质量风险”之间被迫做艰难平衡，只能选一个“看上去最不糟”的方案。</li></ul><p>从财务和交付维度看，这些临时应对的决策，叠加起来就是典型的硬件研发返工成本：物料报废、制造停线、额外验证资源、客户交付风险，以及被挤占掉的其它项目机会。</p><p>BOM 冻结线原本是用来做 BOM 冻结管理、控制工程变更、降低返工的“安全护栏”，现实中却常常沦为被反复跨越的“建议线”。不少企业在复盘时，习惯把责任归结为：</p><blockquote><em>“某次评审不严”“某个团队责任心不足”。</em></blockquote><p>但如果冷静问自己一句：“我们明知道这条 BOM 冻结线守不住，为什么还要设？”<br/>你会发现，这并不是某一两次评审失误，而是整个硬件研发体系、工程变更治理机制和组织治理方式出现了结构性问题。</p><h2>为什么 BOM 冻结线总被打破</h2><p>这一节，我会从 ALM（应用生命周期管理）、IPD（集成产品开发）以及配置管理的视角，拆解常见的 BOM 冻结失效原因，帮助你把“现象级问题”放回“体系级框架”里看。</p><h4>1. 冻结线被当成“行政规定”，而不是配置基线</h4><p>在成熟的 ALM / 配置管理体系中，BOM 冻结线本质上是一个“系统基线（Baseline）”：</p><ul><li>它绑定特定的需求版本、设计版本、验证结果与质量数据；</li><li>它是后续工程变更（ECR/ECO）评估的参照物；</li><li>它定义了“当前有效配置”的边界，是硬件产品配置管理中的关键节点。</li></ul><p>但在很多硬件研发管理现场，BOM 冻结线更多是一个“时间点上的宣告”：</p><blockquote><em>“从今天开始，BOM 不允许再改了。”</em></blockquote><p>缺少与需求配置、设计配置、测试验证的端到端关系，BOM 冻结管理就只能依赖个人自觉与行政推动。一旦遇到“商业压力 + 技术风险”的组合，“不开口子”的人反而会显得不合群。</p><p>一个简单的自查问题是：</p><blockquote><em>“现在让你在 10 分钟内拿出某个量产型号的当前有效 BOM 基线，含其对应需求版本、设计版本和测试结果，你做得到吗？”</em></blockquote><p>如果答案是否定的，那么在这个组织里，BOM 冻结线更多只是流程文件中的描述，而不是在 ALM / PLM 里被真实维护的系统基线。</p><h4>2. 前端不稳定：需求与架构模糊，后端 BOM 被动“还债”</h4><p>系统工程和 IPD 都强调：70% 以上的成本和风险在前期需求与架构决策中已经锁定。需求模糊、架构摇摆、接口频繁变化，最终都会通过后端的 BOM 变更和硬件返工来“还债”。</p><p>典型表现包括：</p><ul><li>需求管理停留在 PPT 和 Excel 表格，ALM 里没有完整的需求树和变更记录；</li><li>系统架构设计不到位，模块边界和接口不清晰，导致选型、布局和功耗分配在后期不断调整；</li><li>软件/硬件、结构/电子缺乏联合方案评审，硬件 BOM 只能在集成阶段被动跟随上游需求变化。</li></ul><p>在这些条件下，项目后期出现大量因需求变更、架构调整引发的 ECO 并不意外。只是这些“前端债务”，往往在项目报表里被模糊成“若干次紧急改料”，看起来是战术问题，本质却是前端工程（需求工程 + 系统架构工程）没有做好。</p><h4>3. 端到端数据链路断裂：ALM / PLM / ERP 各自为政</h4><p>在 IPD 研发体系和数字化研发管理平台的理想状态里，BOM 是一条端到端数字化链路中的“节点视图”：</p><ol><li>上游连接需求、系统分解、详细设计；</li><li>中间在 PLM 中形成 EBOM / MBOM；</li><li>下游通过 ERP 连接供应链、库存与制造执行。</li></ol><p>而在不少企业现场，实际情况是：</p><ol><li>研发在 ALM 或本地工具里维护工程 BOM（EBOM）；</li><li>工业化团队在 PLM 或独立系统中维护 MBOM，字段和命名各搞一套；</li><li>ERP 里还有另一种物料视图，用于采购与财务。</li></ol><p>结果就是：</p><ol><li>没有人真正相信“当前某版本 BOM 就是真相”；</li><li>临量产前必须通过人工对表、Excel 校验来确认版本；</li><li>每次对齐都伴随错误风险和大量隐形人力成本。</li></ol><p>当数据真相都不清楚时，任何“BOM 冻结管理”都是纸面承诺。问题总是在接近量产导入的时候集中爆发，“打破冻结线”就变成一个“不得不做”的选项。</p><h4>4. IPD 决策关口形同虚设：评审“过了”，问题却还在</h4><p>不少公司引入了 IPD 流程，DR1/DR2、PDR、CDR、MP 等评审节点一个不少，纸面流程也很完整。但实际操作中常常变成：</p><ul><li>评审主要看 PPT，不看 ALM/PLM 等系统里的配置基线和数据视图；</li><li>BOM 成熟度没有可量化标准，评审结论停留在“基本可行”“整体可控”；</li><li>对“BOM 冻结后变更”的约束和复盘机制缺失，没有形成组织级记忆。</li></ul><p>在这种状态下，IPD 关口很难对后续 BOM 稳定性真正负责。BOM 冻结线被定义在流程图里，却没被嵌入 IPD 决策逻辑和工程变更管理机制里。</p><h2>如何在硬件研发中构建“不轻易被打破的冻结线”</h2><p>要让 BOM 冻结线真正发挥作用，不能只靠“严禁改动”的口号，而需要一整套结合 ALM / IPD 的方法论与治理机制。下面是一个实践框架，可供中高层研发管理者、PMO、项目经理和系统工程师共用。</p><h4>1. 从“一刀切冻结”到“分层、分阶段的 BOM 冻结策略”</h4><p>第一步是重新定义“冻结”本身，而不是简单地设一个日期：</p><ul><li>按 BOM 视图 区分：工程 BOM（EBOM）、制造 BOM（MBOM）、服务 BOM（SBOM）；</li><li>按 物料重要性 分层：关键器件（核心芯片、电源、关键连接器）、风险器件（停产风险、超长交期）、普通物料；</li><li>按 时间轴 设定多个冻结点，而不是唯一“终极时刻”。</li></ul><p>一个常用的实践是分三层冻结：</p><p><strong>① 架构级冻结（Architecture Freeze）</strong></p><ul><li>面向系统工程和 IPD 的概念设计阶段；</li><li>冻结技术路线、产品平台、关键接口与性能/功耗预算，明确主芯片大类和关键方案；</li><li>目标是减少因架构调整引发的大规模 BOM 变更。</li></ul><p><strong>② 关键器件冻结（Key Component Freeze）</strong></p><ul><li>在详细设计和样机试制之前完成；</li><li>冻结核心芯片、电源方案、关键连接器等成本和风险权重最高的物料；</li><li>后续任何针对这些物料的变更都必须经过 CCB（变更控制委员会）审核。</li></ul><p><strong>③ 全 BOM 冻结（Full BOM Freeze）</strong></p><ul><li>在量产导入前，与 MP/PPAP 等评审节点耦合；</li><li>要求 EBOM、MBOM 与 ERP 物料视图一致，并通过必要的验证和试产数据校验。</li></ul><p>通过分层、分阶段的 BOM 冻结策略，可以把“绝对不能轻易改”的部分尽早固化，把仍需优化的部分显性化，避免在项目尾声做大型手术式改动。</p><h4>2. 用系统工程方法，把“变更欲望”前移到可控阶段</h4><p>要减少后期打破 BOM 冻结线的冲动，就必须把试错和优化前移到需求工程和系统方案阶段。系统工程方法提供了三个抓手：</p><p><strong>① 用 V 模型构建需求–设计–验证的一致性链条</strong></p><p>在 ALM 平台中建立需求分解结构（系统需求 → 子系统需求 → 设计规格）；<br/>对关键需求建立双向追踪：从需求到设计文档、到 BOM 物料、到测试用例；<br/>在架构评审 / PDR / CDR 时，不只问“功能看上去实现了没有”，而是查看“需求覆盖率和验证闭环”。</p><p><img width="723" height="488" referrerpolicy="no-referrer" src="/img/bVdnfdk" alt="V 模型示意图" title="V 模型示意图"/></p><p><strong>② 强化概念验证与仿真，减少“实物试错式返工”</strong></p><ul><li>对高风险电源、高速信号链路等提前做仿真与小板验证（EVB），在 BOM 冻结前消除一批显而易见的风险；</li><li>对结构、散热等问题用仿真和样机联合验证，缩短试错周期；</li><li>把这些活动纳入 IPD 任务书和项目计划，而不是“有时间再做”。</li></ul><p><strong>③ 设立明确的“BOM 冻结前变更窗口”</strong></p><ul><li>在项目计划中清晰标出在哪几个迭代周期允许对 BOM 做大幅调整；</li><li>窗口期内的变更流程相对简化，但必须保留原因和验证记录；</li><li>超出窗口，则通过 ECR/ECO 和 CCB 来控制，形成变更可见、成本可见的治理机制。</li></ul><p>当变更欲望被前移到可控窗口，并在 ALM/PLM 中形成清晰的信息链条时，后期“拍脑袋改料”的空间自然会变小。</p><h4>3. 建立 ECR / ECO 分级工程变更治理机制</h4><p>很多公司有 ECR/ECO 表单，但缺少工程变更治理逻辑，导致 BOM 冻结管理无法落地。一个典型的治理思路是：</p><p><strong>① 明确 ECR（变更请求）与 ECO（变更实施）的分工</strong></p><ul><li>ECR：讨论“是否要改”，关注问题、动机、影响和可选方案；</li><li>ECO：在决策后落实“具体怎么改”，包括 BOM、图纸、工艺、测试、文档等更新；</li><li>禁止“跳过 ECR 直接发 ECO”的做法，避免绕过系统性的影响评估。</li></ul><p><strong>② 按影响等级分级管理</strong></p><ul><li>A 级变更：影响安全、法规合规、重大质量风险，冻结后仍允许，但必须由跨部门 CCB 和项目高层批准；</li><li>B 级变更：影响成本、关键性能、供应风险，由项目 CCB 审批；</li><li>C 级变更：影响有限的小改动，可由模块负责人审批，但必须在 PLM / ALM 中留痕。</li></ul><p><strong>③ 在 ECR 中强制评估五个维度</strong></p><ul><li>对客户价值和市场竞争力的影响；</li><li>对项目进度和资源的影响；</li><li>对成本（材料、制造、质量保障）的影响；</li><li>对安全、法规、长期可靠性的影响；</li><li>替代方案（维持现状的后果是什么）。</li></ul><p>冻结线之后不是“不许改”，而是“必须通过可审计、可度量的工程变更流程来决策是否值得改”。</p><h4>4. 用 ALM / PLM 打通需求–设计–BOM–制造的数字链路</h4><p>想让 BOM 冻结线有现实意义，需要一条可信的数字化主线，而不是三套孤立系统。实践中可以按“最小可行 + 逐步演进”来设计：</p><p><strong>① 以 ALM 为需求与设计配置的源头系统</strong></p><p>所有正式需求与变更需求在 ALM 中维护，形成需求基线；<br/>需求项与设计文档、BOM 条目、测试用例建立追踪关系；<br/>在关键评审节点冻结需求/设计基线，与后续 BOM 冻结相呼应。</p><p><strong>② 以 PLM 为 EBOM/MBOM 与配置管理中枢</strong></p><p>在 PLM 中维护 EBOM，关联版本和变更记录；<br/>工业化团队在 PLM 中从 EBOM 派生 MBOM，并关联工艺路线和工装治具；<br/>通过差异报表或可视化看板来监控 EBOM 与 MBOM 的偏差。</p><p><strong>③ 与 ERP / 供应链系统形成闭环</strong></p><p>在 BOM 冻结前确保 ERP 中的物料编码、供应商信息、价格和交期等同步；<br/>冻结后任何 ECO 自动评估库存、在途订单和产能计划的影响。</p><p>这条数字链路的目的不是“堆工具”，而是让需求–设计–BOM–制造这条系统工程逻辑，真的在数据里可见可追踪。</p><h4>5. 把“BOM 冻结纪律”转化为可运营的指标体系</h4><p>靠口头强调很难改变行为，建议 PMO 或 R&amp;D Ops 建立轻量指标，把 BOM 冻结管理运营起来：</p><ul><li>冻结后 ECO 数量与分布（按级别、按项目）；</li><li>因 BOM 变更导致的返工成本：报废物料、返工工时、额外测试与验证资源；</li><li>BOM 冻结及时率：计划冻结日期 vs 实际冻结日期；</li><li>变更决策周期：从 ECR 提出到 ECO 关闭的平均时间。</li></ul><p>这些指标不必一上来就用于“硬考核”，更有价值的场景是：</p><ul><li>项目例会和季度评审的固定看板；</li><li>横向对比不同产品线 / 平台的变更行为模式；</li><li>为管理层的资源投入（优化前端架构、升级平台、重构模块）提供数据支持。</li></ul><h4>6. 在 IPD 框架下重构关键评审节点，让冻结线“嵌入流程”</h4><p>要让 BOM 冻结线具备权威性，需要和 IPD 关键评审深度耦合：</p><p><strong>① 在 PDR 上确认架构级冻结</strong></p><ul><li>审查关键技术路线、接口和资源预算是否清晰；</li><li>将架构级决策纳入配置基线，后续重大架构调整提升到平台级决策。</li></ul><p><strong>② 在 CDR 上确认关键器件冻结</strong></p><ul><li>审查关键器件验证报告、供应风险评估和备选方案；</li><li>将关键器件清单纳入项目风险清单和后续 ECO 约束。</li></ul><p><strong>③ 在 MP / 量产评审上确认全 BOM 冻结</strong></p><ul><li>审查 EBOM/MBOM/ERP 的一致性和试产数据；</li><li>对冻结后 ECO 做说明，为组织沉淀经验。</li></ul><p>评审不只是“放行”，更要成为 BOM 冻结管理的“质量门”和组织知识的沉淀节点。</p><h4>7. 从组织协同与激励机制上拆掉“返工陷阱”</h4><p>最后，如果组织协同和激励方向错误，再好的工程变更流程也会被绕过。几个经常被忽视的点：</p><p><strong>① 让供应链、制造、质量真正前移参与</strong></p><ul><li>在 IPD 项目团队中，让供应链、制造工程、质量成为前期方案阶段的正式角色，而非“后期支持”；</li><li>对高风险器件，要求供应链提前给出多源策略和生命周期分析；</li><li>制造和质量提前对 BOM 提出可制造性和长期可靠性要求。</li></ul><p><strong>② 淡化“英雄改料文化”，强化“前期稳态文化”</strong></p><ul><li>少讲“最后一刻改料救回项目”的英雄故事，多在复盘中讨论“为什么问题没在前面暴露”；</li><li>把“冻结后 ECO 数量、返工成本、按计划冻结情况”纳入项目复盘指标。</li></ul><p><strong>③ 用清晰的 RACI 避免“谁都能改一点”</strong></p><ul><li>对 BOM 变更设定 RACI（负责 / 参与 / 咨询 / 知情），明确提出人、评估人、决策人和验证人；</li><li>避免“本地优化、全局返工”的局面，让每一次打破冻结线都在数据上留下可追踪的痕迹。</li></ul><h2>给中高层管理者和 PMO 的几条现实建议</h2><p>对于已经饱受 BOM 冻结线反复被打破困扰的硬件团队，不必指望“一次性大改造”。更现实的路线是渐进式演进：</p><p><strong>① 选一个典型产品线试点</strong></p><ul><li>选择物料复杂度高、业务重要、变更频繁的产品线；</li><li>在试点里跑通：分层冻结策略 + ECR/ECO 分级治理 + 最小可行数字链路。</li></ul><p><strong>② 先建立“数据真相”再谈全面集成</strong></p><ul><li>梳理当前 EBOM/MBOM/ERP 的关键字段和对齐方式；</li><li>用简单脚本或定期对账方式建立“当前有效 BOM 视图”，为后续深度集成打基础。</li></ul><p><strong>③ 让 PMO 把冻结纪律纳入项目运营例会</strong></p><ul><li>固定查看冻结后 ECO 和返工成本数据；</li><li>通过轻量复盘沉淀经验，形成可复用的治理模式。</li></ul><p><strong>④ 中高层用行为释放清晰信号</strong></p><ul><li>在商业压力和技术风险冲突时，公开讨论“不改的代价”和“改的代价”；</li><li>对频繁突破冻结线且论证不足的项目，要求严肃复盘，而非“一笑而过”；</li><li>对前期稳住架构、减少后期 ECO 的团队给予正向激励。</li></ul><h2>从“救火式改料”走向“体系化决策”</h2><p>BOM 冻结线被反复打破，并不是单一评审的偶然失误，而是硬件研发管理中需求不稳定、架构前移不足、ALM/PLM/ERP 数据割裂、IPD 评审流于形式以及组织激励失衡等系统问题在 BOM 上的集中体现。</p><p>要跳出硬件研发的“返工陷阱”，需要：</p><ul><li>把 BOM 冻结线视为系统基线和配置管理节点，而不是行政禁令；</li><li>用 系统工程 + ALM 应用生命周期管理 + IPD 流程，构建需求–设计–BOM–制造的数字化链路；</li><li>通过分层冻结、ECR/ECO 分级治理、可度量指标，把“冻结纪律”转化为可运营的管理能力；</li><li>通过组织协同与激励调整，让团队从“救火式改料”转向“前瞻性、数据支撑的工程变更决策”。</li></ul><p>当一个组织可以坦然说出：</p><blockquote><em>“我们不怕变更，但每一次打破冻结线都有明确理由、决策记录和成本认账。”</em></blockquote><p>BOM 冻结线才真正从 PPT 走进硬件研发体系的肌理，也才算真正走出了硬件研发“返工陷阱”。</p>]]></description></item><item>    <title><![CDATA[智能研发管理：制造业如何实现从“单打独斗]]></title>    <link>https://segmentfault.com/a/1190000047446748</link>    <guid>https://segmentfault.com/a/1190000047446748</guid>    <pubDate>2025-12-03 17:04:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>最近和制造业的朋友聊天，大家几乎都在谈数字化转型，聊到研发管理，话题就更热烈了。市场节奏越来越快，技术也在不断迭代，传统研发管理方式显然跟不上了。尤其是汽车、装备制造这些离散制造行业，跨部门协作复杂，信息孤岛严重，研发过程中的痛点太多了。设计数据分散，版本混乱，文档管理滞后，流程审批依赖人工……这些看似独立的问题，其实都是一根绳子上绑着的蚂蚱。<br/>举个例子，很多企业的设计文档和图纸数据分散在不同的系统里，各部门拿到的信息版本不一致，沟通成本居高不下。再加上三维模型的协作效率差，非设计人员很难快速查看和使用这些数据，严重影响了跨部门协同的效率。这时候，研发管理平台的重要性就凸显出来了。<br/>广域铭岛的Geega捷做设计研发协同平台，就是在这种背景下出现的。它不是简单地把PDM、PLM这些工具堆砌在一起，而是真正打通了研发全链条。从需求收集、设计评审到BOM管理、工艺协同，整个流程都整合在一个平台上，信息共享更高效，版本追溯更清晰。系统通过自动触发审批流程，让审批人员在移动端就能处理，再也不用在各个系统之间来回切换，效率直接提升了40%。<br/>说到这个平台，它最吸引人的地方在于“协同”二字。想象一下，销售、采购、生产、质量等部门的人都在一个平台上工作，实时共享数据。比如，Fview模块支持60多种CAD格式，一线人员只需要扫码，就能查看三维模型并在线评审，不用再为格式转换和软件兼容发愁。这在实际应用中效果特别明显，某科技企业引入后，模型查看效率提升60%，跨部门协作周期缩短了50%。<br/>当然，研发管理不仅仅是数据协同。质量管控也是一个重要环节。FMEA模块的加入，让研发团队能够更早地发现问题、预防风险。基于历史问题库，系统可以自动推荐整改措施，把被动应对变成了主动预防。某制造企业在应用后，FMEA的编制效率提升了20%，质量管理也实现了从“事后补救”到“源头预防”的转变。<br/>不过，研发管理的数字化转型还远不止这些。更关键的是，系统要能够沉淀数据资产，形成企业的知识库。比如，标准BOM数据的一致性和准确性，直接影响到后续的生产、采购和交付。统一的数据源头，不仅减少了出错率，还提高了资源利用率。某家电企业在应用后，零部件复用率提升35%，BOM数据准确率也达到了80%。<br/>说到竞争对手，其实市场上有不少研发管理平台，比如PTC、达索系统这些老牌厂商。它们的功能也很强大，但在灵活性和适用性上，广域铭岛的平台确实更贴近国内制造业的需求。尤其是在中小企业的数字化转型中，它的弹性设计能力让很多企业受益。<br/>当然，转型过程中也会遇到一些挑战。比如，系统集成、数据迁移、人员培训这些环节，都需要细致规划。但从长远来看，这些投入都是值得的。因为研发管理的智能化，不仅提升了效率，还让企业能够更快地响应市场变化。<br/>总的来说，智能研发管理不是一句口号，而是制造业数字化转型的核心需求。它要求企业打破传统的管理方式，用数据驱动创新，用协同提升效率。</p>]]></description></item><item>    <title><![CDATA[2025 SECon × AgentX ]]></title>    <link>https://segmentfault.com/a/1190000047446795</link>    <guid>https://segmentfault.com/a/1190000047446795</guid>    <pubDate>2025-12-03 17:03:56</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：盈楹</p><p>近日，2025 SECon × AgentX大会——AI 原生应用架构专场圆满落幕，本次专场阿里云联合信通院共同出品，现场吸引了 80+ 名技术从业者深度参与。</p><p>活动聚焦 AI 时代软件架构的核心命题，深度分享了 AI 原生应用架构趋势与实践、AgentScope 开发框架、AI 开放平台、大模型可观测 &amp; AIOps 等热门技术议题，探讨从基础设施到应用层的协同演进策略与工程实践。</p><p>关注「阿里云云原生」公众号，后台回复：1125</p><p>免费获得活动讲师 PPT 合辑</p><h2>精彩回顾</h2><h3>议题一：AI 原生应用架构探索与实践丨肖京(亦盏)   阿里云智能云原生高级技术专家</h3><p>当前大模型已迈过技术拐点，Agentic AI 进入规模化落地阶段。AI 原生应用以模型为基础、Agent 为驱动、数据为中心，推动系统从“机器执行”向“机器思考+执行”演进。框架选型需平衡 Agentic 自主性与业务确定性，Agent 面临开发效率、业务效果，以及稳定、性能、成本、安全的挑战。实践上建议构建以数据为核心的 Agent 平台，结合 MCP/A2A 标准与 Serverless 架构，通过 AI 网关、消息队列、可观测等提升安全性、稳定性与可维护性，实现智能化人机协作新范式。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446797" alt="image" title="image"/></p><h3>议题二：开发更可控，部署更便捷：AgentScope 迈入 1.0 时代丨邝炜瑞  阿里巴巴集团通义实验室 高级算法工程师</h3><p>深度分享了阿里通义实验室开源的 AgentScope 智能体开发框架 1.0 版本。核心内容包括：基于 ReAct 范式的多智能体系统支持，提供结构化输出、工具调用与长期记忆等能力；采用三层架构——开发框架层、可视化调试平台与安全运行时环境，结合工具沙箱与元工具机制，全面提升系统的可控性、可观测性与部署安全性。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446798" alt="image" title="image" loading="lazy"/></p><h3>议题三：AI 网关：AI 原生架构下的智能流量中枢丨 赵炳堃(秉钧)   阿里云智能云原生高级开发工程师</h3><p>聚焦 AI 网关在 AI 原生架构中的核心作用，重点介绍 Higress AI 网关的关键能力：支持多模型适配、协议转换与语义缓存，提供 Token 限流、Fallback 机制保障高可用；通过 API-Key 管理、PII 脱敏、WASM 沙箱等实现安全可控；并借助 MCPServer 统一代理提升集成效率。HiMarket 平台则助力企业构建私有 Agent 市场，推动 AI 能力的安全规模化落地。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446799" alt="image" title="image" loading="lazy"/></p><h3>议题四：从可观测到 RL：打造生产级可靠的长周期 Agent丨马云雷  阿里云智能云原生技术专家</h3><p>聚焦 AI-Native 应用中长周期 Agent 的可靠性建设，提出以可观测性为基础，通过 OpenTelemetry、Prometheus 等工具实现全栈监控，做到“可见、可调、可审”。引入 LLM Judger 作为自动化评估裁判，结合数据工程与模型蒸馏，构建快慢反馈闭环。最终形成从问题发现、根因分析到自我强化的进化系统，推动 Agent 向高可靠、自演进的生产级应用发展。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446800" alt="image" title="image" loading="lazy"/></p><h2>现场精彩瞬间</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446801" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446802" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446803" alt="image" title="image" loading="lazy"/></p>]]></description></item><item>    <title><![CDATA[【运维自动化-标准运维】快捷键使用技巧（]]></title>    <link>https://segmentfault.com/a/1190000047446817</link>    <guid>https://segmentfault.com/a/1190000047446817</guid>    <pubDate>2025-12-03 17:03:05</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>快速框选画布流程节点</h2><h3>1.在流程画布左上方有对应框选画布的按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446819" alt="在这里插入图片描述" title="在这里插入图片描述"/></p><h3>2.点击按钮—框选节点</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446820" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.框选成功后–对应节点有虚线包围</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446821" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h2>变量透视</h2><p>该功能可以展示对应节点中引用了的输入变量以及该节点的输出变量</p><h3>1.在流程画布做左上方有对应变量透视的按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446822" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>2.点击按钮–展示节点变量按钮</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446823" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><h3>3.将鼠标移动到对应节点上时，即展示对应节点的变量使用情况</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446824" alt="在这里插入图片描述" title="在这里插入图片描述" loading="lazy"/></p><p>说明：适合产品版本 V6.1/V6.2/V7.0/V7.1</p>]]></description></item><item>    <title><![CDATA[如何打造AI时代的数据基石 | Data]]></title>    <link>https://segmentfault.com/a/1190000047446840</link>    <guid>https://segmentfault.com/a/1190000047446840</guid>    <pubDate>2025-12-03 17:02:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Data + AI 已经成为数据从业人员必须关注的技能。在基于 Databend Cloud 平台上可以大大简化数据人员在数据基础工作方面的投入，让数据人员可以花更多的精力去研究 Data + AI 的实践。在此背景下，11月29日，Databend Meetup·上海站线下活动"如何打造 AI 时代的数据基石"，汇集了国内数据库领域多位一线专家：Databend 创始人吴炳锡、沉浸式翻技术专家陈琦，沈超、资源数据平台架构师邵锋、TiDB 解决方案架构师 刘源、空中云汇架构师赵飞祥以及来自各行各业的技术负责人，数据部门负责人。参会嘉宾围绕"如何打造 AI 时代的数据基石"的主题，共同探讨了大模型时代数据库和数据平台的创新演进与实战应用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446842" alt="图片" title="图片"/></p><p>以下内容就将为您带来这些话题背后的深度思考：<br/>基于 Databend 无编程实 Data Pipeline 及数据分析<br/>Databend Labs 联合创始人吴炳锡，系统地介绍了 Databend 作为一款云原生数据仓库，如何以其独特架构和技术特性，极大地简化和革新传统大数据 Data Pipeline 的构建与数据分析流程，并展示了其与 AI 融合的强大潜力。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446843" alt="图片" title="图片" loading="lazy"/></p><p>Databend 的清晰定位：解决传统大数据之痛<br/>分享开宗明义，指出了 Databend 的核心定位：简单易用、高性能、低成本。其目标是成为一款云原生湖仓一体化产品，旨在：</p><p>降低云上大数据成本：利用对象存储实现极致的存算分离和低成本存储。<br/>简化数据架构：坚持" SQL 为王"，让复杂的湖仓开发变得像使用传统数据库一样简单。<br/>统一数据枢纽：支持构建企业级统一数据仓库，并提供跨多云、跨 IDC 的高可用体验。</p><p>核心革新：重构 Data Pipeline 的开发模式<br/>分享通过对比，深刻剖析了传统大数据架构（依赖 Kafka, Flink, Spark, Trino 等繁多组件）的痛点：技术栈复杂、技术要求高、落地慢、运维成本高昂。<br/>针对这些痛点，Databend 提出了一套以 SQL 为中心 的"无编程" Data Pipeline 解决方案，其核心构件包括：</p><p>数据秒级摄入 (COPY INTO + External Stage)：通过监听对象存储事件，实现海量数据的快速加载与可见。<br/>内置流计算 (Stream)：提供表级增量变更捕获能力，无需额外组件即可实现高效的实时 ETL，性能提升可达 10 倍。<br/>自动化任务调度 (Scheduled Task)：通过 Serverless Task 实现完整的数据处理工作流编排，让一个懂 SQL 的人就能轻松完成复杂的数据治理。<br/>强大的外部函数 (UDF)：支持用 Python 等语言轻松扩展功能，实现与外部系统（如更新 Redis）或 AI 服务的无缝集成。</p><p>与 AI 的深度融合：从数据平台到智能基座<br/>分享重点展示了 Databend 在 AI 时代的前瞻性，其与 AI 的融合体现在两个层面：</p><p>原生 AI 能力：内置向量计算和 AI 函数（如cosine_distance），为 AI 应用提供开箱即用的支持。<br/>可扩展的 AI 集成 (External UDF)：通过 UDF 可以方便地调用 Embedding 模型、情感分析、文本相似度等外部 AI 服务，将 Databend 升级为一个支持智能化数据分析与应用的" AI 原生"平台。</p><p>卓越效益与广泛验证<br/>分享通过具体数据证明了 Databend 的卓越效益：</p><p>成本大幅降低：在替换 Trino/Presto、Elasticsearch、数据归档等场景中，成本降低 75% 到 95%。<br/>极致的可扩展性：支持单表 2.6 万亿行、1PB+ 的超大规模数据处理。<br/>广泛的行业应用：已成功服务于中信银行、微盟、苹果中国等知名企业，应用于主数据平台、日志分析、数据归档等多种场景。</p><p>总结<br/>Databend 通过其云原生、一体化的架构，将复杂的大数据技术栈简化为以 SQL 为核心的开发体验，从根本上降低了数据开发的门槛、成本和运维负担。 它不仅是一个高性能的数据仓库，更是一个内置了流处理、任务调度和强大扩展能力的数据平台操作系统。在 AI 时代，其原生及可扩展的 AI 能力进一步使其成为企业构建智能化应用的理想数据基石，完美契合了当下企业追求降本增效和快速创新的核心诉求。<br/>构建海量记忆：基于 Databend 的 2C Agent 平台|沉浸式翻译<br/>沉浸式翻译团队技术专家陈琦在 构建海量记忆：基于 Databend 的 2C Agent 平台|沉浸式翻译实践分享，核心阐述了他们如何利用 Databend 构建一个面向海量用户的、具备"长期记忆"能力的 AI Agent 平台。<br/>沉浸式翻译在比较早期已经接入 Databend , 公司内部在无运维的情况下，支撑了千万级用户，月活百万级用户。Databend 目前不但承担沉浸式翻译的平台分析数据，也承担了部分业务类数据。 目前团队正在 Databend 上构建海量记忆体的 2C Agent 平台。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446844" alt="图片" title="图片" loading="lazy"/></p><p>核心挑战：<br/>传统方案的痛点：</p><p>组件割裂：维护向量库、关系型数据库、缓存等多套系统，开发和运维复杂。<br/>缺乏生命周期管理：向量库只增不减，导致噪音增加、性能下降、成本飙升。</p><p>为什么选择 Databend？</p><p>All-in-One：统一处理向量、结构化和半结构化（JSON）数据，简化架构。<br/>Serverless：零运维、按需付费，完美契合小团队"小步快跑"的模式。<br/>可编程性：通过 SQL、UDF 和 Task 实现复杂的数据处理和生命周期管理。大大简化开发投入</p><p>核心架构与创新（MemOS）：</p><p>MemNodes 表：作为记忆实体，利用计算列和聚簇索引优化混合查询（向量+条件过滤）性能。<br/>MemEdges 表：构建记忆图谱，用 SQL 存储关系，解决纯向量检索无法处理的逻辑推理问题。<br/>混合检索算法：结合 SQL 过滤、向量搜索和图关联，实现精准且上下文丰富的记忆召回。<br/>自动化生命周期：通过 Serverless Task 定期对记忆进行摘要融合和归档，实现"会遗忘的智能系统"。</p><p>价值总结：<br/>该实践成功地将 Databend 作为统一数据基石，以极低的运维成本和优雅的技术方案，实现了从"翻译工具"到懂用户的"语言伴侣"的演进，为 2C AI 提供了易用，低成本，高性能的平台。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446845" alt="图片" title="图片" loading="lazy"/></p><p>Data + AI - 数据平台的应用和实践<br/>第三个分享中邵锋老师带着一线经验给我们分享数据平台的建设和 Data+AI 实践。属于非常硬核的分享，因为保密问题就不再公开邵锋老师的分享。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446846" alt="图片" title="图片" loading="lazy"/></p><p>AI 时代的数据基石：趋势、挑战与 TiDB 实践<br/>TiDB 解决方案架构师刘源老师，从行业更宏观的视角探讨了 AI 时代的数据挑战，并阐述了 TiDB 作为"数据基石"的解决方案和案例。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446847" alt="图片" title="图片" loading="lazy"/></p><p>核心洞察：</p><p>AI 发展趋势：大模型进入平台期，下一代突破指向"世界模型"。当前 AI 面临幻觉问题（源于概率生成的有损压缩）、算力消耗和伦理安全等挑战。<br/>AI 应用现状：情感陪伴、内容生成等"幻觉友好型"应用火热，但金融、制造、医疗等严肃 ToB 场景落地艰难，面临数据治理缺失、场景碎片化等挑战。</p><p>AI 时代对数据库的新要求：</p><p>多模态融合：同时处理关系表、向量、全文、图谱等数据，"多库合一"。<br/>实时与高扩展：弹性支撑 Agent 的推理、记忆和 Multi-Agent 协作。<br/>支持 AI 原生体验：成为 Agent 的"集体记忆中枢"，能主动交互。</p><p>TiDB 的解决方案：</p><p>核心特性：金融级高可用、天生的弹性扩展、HTAP 一体化架构、正在演进的多模态数据融合能力。</p><p>AI 原生探索：</p><p>增强数据访问层：通过 RAG、GraphRAG 等技术，将 TiDB 打造成企业知识核心，降低大模型幻觉。<br/>构建 Data Agent 能力：研发 AutoFlow，让用户用自然语言直接进行混合查询和数据分析。<br/>面向 Multi-Agent 未来：扮演"共同记忆体"，支持数据版本化、分支管理等。</p><p>案例与价值：</p><p>为多家国内 TOP AI 及 Agent 厂商提供了可弹性扩展的数据底座，支撑了业务从零到亿级估值的狂飙。<br/>与 Databend 在归档场景合作，利用 TiDB 处理实时事务，Databend 处理低成本历史分析，实现降本增效。<br/>提出企业级 AI 平台整体架构，强调从"数据拼接"到"原生融合"的范式变革。</p><p>圆桌讨论环节<br/>在该环节邀请又邀请了空中云汇数据架构师赵飞祥，沉浸式翻译团队数据分析师沈超，TiDB 解决方案架构师刘源， 数据平台架构师邵峰 四位嘉宾一共交流了 AI 时代个人职业方面的感受， AI 对工作方面带来的变化， AI 时代需要什么样的人。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446848" alt="图片" title="图片" loading="lazy"/></p><p>总结大家的观点：</p><p>AI 时代，让人每个人的能力更强了，能做的事更多了。 原来复杂的数据分析工作，原来可能需要1周，现在可能就是 1-2 天，或是更快。<br/>在 AI 时代不要给自我设限，上手一门技能非常的快。<br/>在 AI 时代更需要有 Owner 精神，端到端的解决问题的思路，需要懂得把工作拆分及推动下去。<br/>在 AI 时代同样需要有专业和权威的精神，能经住团队的挑战，能让老板放心把工作交给你。</p><p>圆桌讨论将视野拉回至"人"本身，为我们揭示了在 AI 时代更宝贵的特质。 当技术门槛被AI工具不断降低，"Owner 精神"、"端到端解决问题" 的能力以及 "专业权威" 的深度，构成了技术人新的护城河。AI 放大了个体的能力边界，但判断力、责任心和推动力，依然是不可替代的价值所在。<br/>总结而言，本次 Meetup 清晰地传递出一个信号： 打造 AI 时代的数据基石，已从一道可选题变为一道必答题。其答案不在于堆砌最前沿的独立组件，而在于选择一个能够简化架构、统一数据、智能赋能，并能伴随组织共同成长的一体化平台。我们欣慰地看到，以 Databend、TiDB 为代表的国内数据库力量，正以扎实的技术创新和丰富的场景实践，为各行各业提供着这道"必答题"的优秀解方。数据的浪潮奔涌向前，AI 的篇章刚刚开启。感谢所有嘉宾的倾情分享与参会者的热情投入，让我们共同期待，在这块坚实、智能的数据基石之上，生长出下一个时代的伟大应用。<br/>关于 Databend<br/>Databend 是一款 100% Rust 构建、面向对象存储设计的新一代开源云原生数据仓库，统一支持 BI 分析、AI 向量、全文检索及地理空间分析等多模态能力。期待您的关注，一起打造新一代开源 AI + Data Cloud。<br/>👨‍💻‍ Databend Cloud：databend.cn<br/>📖 Databend 文档：docs.databend.cn<br/>💻 Wechat：Databend<br/>✨ GitHub：github.com/databendlab...</p>]]></description></item><item>    <title><![CDATA[Linux Bash Shell 脚本编]]></title>    <link>https://segmentfault.com/a/1190000047446872</link>    <guid>https://segmentfault.com/a/1190000047446872</guid>    <pubDate>2025-12-03 17:01:23</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>Linux Bash Shell编程： 👇🏻ke程：shanxueit点com/从命令行到脚本开发的全面指南<br/>本文将系统性地介绍Linux Bash Shell编程的完整知识体系，从基础概念到高级应用，帮助读者实现从命令行操作到脚本开发的技能跃迁。</p><p>一、Shell编程基础概念<br/>Shell是用户与Linux内核之间的桥梁，它提供了一个命令行界面，用户输入的命令由Shell解析后传递给内核执行，并将结果返回给用户。常见的Shell有Bash(Bourne-Again Shell)、Zsh(Z Shell)、Ksh(Korn Shell)等，其中Bash是Linux系统中默认且应用最广泛的Shell。</p><p>Shell脚本本质上是一个包含一系列命令的文本文件，它通过Shell解释器执行。与JavaScript、PHP等编程语言类似，Shell编程只需要一个文本编辑器和脚本解释器即可开始。Bash作为Bourne Shell的增强版，因其易用性和免费特性成为日常管理和自动化任务的首选工具。</p><p>Shell脚本的基础结构包括：</p><p>Shebang行(如#!/bin/bash)指定解释器路径<br/>注释说明(以#开头)<br/>可执行命令序列<br/>流程控制结构<br/>函数定义<br/>二、Shell脚本核心语法要素</p><ol><li>变量与数据类型<br/>Shell变量用于存储数据，通过=符号赋值(注意等号两边不能有空格)。变量名规则：</li></ol><p>只能包含字母、数字和下划线<br/>不能以数字开头<br/>区分大小写<br/>不能使用bash关键字(可用help命令查看保留关键字)<br/>变量引用使用<br/>符号，如<br/>符号，如var或${var}。Shell支持字符串、整数和数组等数据类型，其中数组可以存储多个值，方便批量操作。</p><ol start="2"><li>流程控制结构<br/>Bash提供了完整的流程控制语句：</li></ol><p>条件判断：if/elif/else/fi，case/esac<br/>循环结构：for/while/until/do/done<br/>循环控制：break/continue<br/>条件测试可以使用test命令或[ ]、[[ ]]结构，支持文件测试、字符串比较和数值比较等多种条件判断。</p><ol start="3"><li>输入输出与重定向<br/>Shell脚本通过以下机制处理输入输出：</li></ol><p>标准输入(stdin)、标准输出(stdout)和标准错误(stderr)<br/>重定向操作符：&gt;、&gt;&gt;、&lt;、&lt;&lt;<br/>管道(|)连接多个命令<br/>命令替换$(command)或command<br/>三、Shell编程进阶技巧</p><ol><li>函数与模块化<br/>函数是Shell脚本中实现代码复用的重要手段，定义语法为：</li></ol><p>PlainText<br/><br/>function_name() {</p><pre><code>commands
[return value]</code></pre><p>}<br/>函数支持参数传递(<br/>1<br/>,<br/>1,2,...$n)和返回值(通过return或echo输出)。</p><ol start="2"><li>错误处理与调试<br/>健壮的脚本需要完善的错误处理机制：</li></ol><p>使用set -e使脚本在命令失败时立即退出<br/>使用trap捕获信号并执行清理操作<br/>通过$?获取上一条命令的退出状态<br/>使用|| true忽略特定命令的错误<br/>调试模式(set -x)显示执行的每条命令</p><ol start="3"><li>文本处理三剑客<br/>Shell脚本常与以下文本处理工具配合使用：</li></ol><p>grep：基于模式搜索文本<br/>sed：流编辑器，执行文本替换等操作<br/>awk：强大的文本分析和报告工具<br/>这些工具支持正则表达式，能够高效处理日志分析、数据提取等任务。</p><p>四、Shell脚本实战应用场景</p><ol><li>系统管理自动化<br/>通过Shell脚本可以实现：</li></ol><p>批量用户管理<br/>系统监控与告警<br/>日志轮转与分析<br/>备份与恢复操作<br/>软件包批量安装<br/>例如磁盘空间排查脚本流程：</p><p>使用df -h确认问题范围<br/>通过du -sh * | sort -hr | head -5定位大目录<br/>逐层深入分析具体目录</p><ol start="2"><li>开发环境配置<br/>Shell脚本常用于：</li></ol><p>开发环境一键部署<br/>编译构建自动化<br/>测试用例批量执行<br/>持续集成流程</p><ol start="3"><li>网络与安全运维<br/>典型应用包括：</li></ol><p>批量主机状态检测<br/>安全漏洞扫描<br/>防火墙规则管理<br/>证书自动续期<br/>五、学习路径与资源推荐</p><ol><li>循序渐进的学习阶段<br/>基础阶段：掌握Linux常用命令和Shell基本语法<br/>脚本阶段：编写简单脚本，实现任务自动化<br/>进阶阶段：学习高级特性如数组、关联数组、进程控制<br/>精通阶段：掌握调试技巧、性能优化和复杂系统设计</li><li>推荐学习资源<br/>《Linux shell脚本编程入门》：系统梳理Shell脚本编程核心知识<br/>Bash官方文档：最权威的语法参考<br/>开源项目源码：学习优秀脚本的实现方式<br/>在线社区：CSDN等技术论坛的实战案例分享<br/>六、最佳实践与注意事项<br/>代码规范：</li></ol><p>添加清晰的注释<br/>使用有意义的变量名<br/>保持一致的代码风格<br/>适当添加日志输出<br/>安全考虑：</p><p>避免使用root权限执行不必要操作<br/>谨慎处理用户输入<br/>设置适当的文件权限<br/>注意敏感信息保护<br/>性能优化：</p><p>减少不必要的子进程创建<br/>使用内置命令替代外部命令<br/>批量处理替代循环操作<br/>合理使用缓存机制<br/>通过系统学习和持续实践，Bash Shell编程可以成为提升Linux系统管理效率的强大工具，实现从简单命令行操作到复杂自动化系统的能力跨越。</p>]]></description></item><item>    <title><![CDATA[第50届ICPC亚洲区域赛·上海站，非凸]]></title>    <link>https://segmentfault.com/a/1190000047446324</link>    <guid>https://segmentfault.com/a/1190000047446324</guid>    <pubDate>2025-12-03 16:11:12</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>11月22日-23日，第50届ICPC国际大学生程序设计竞赛亚洲区域赛·上海站在上海大学宝山校区圆满举行。来自221所高校、中学及企业的356支优秀队伍，千余名编程精英同台竞技，以智慧碰撞灵感，以技术角逐巅峰。作为赛事的重要支持方，非凸科技始终关注青年科技人才的成长，致力于推动计算机教育与产业实践的深度融合，为全球学子提供从理论到实践的成长通道。<br/><img width="553" height="373" referrerpolicy="no-referrer" src="/img/bVdne7p" alt="image.png" title="image.png"/><br/>开幕式上，非凸科技首席运营官郑媛姿发表致辞，ICPC从不止于胜负，跨越场次的坚守、迭代升级的解题思路、惺惺相惜的竞技情谊，都是更珍贵的成长馈赠。非凸科技始终以“搭建人才与产业的桥梁”为己任，深知人才是创新的核心密码，更愿为每一位怀揣技术梦想的同学，铺就“从赛场到金融实战”的成长快车道。<br/><img width="553" height="369" referrerpolicy="no-referrer" src="/img/bVdne7q" alt="image.png" title="image.png" loading="lazy"/><br/>赛事期间，非凸科技组织了企业宣讲与人才交流活动，向参赛选手们分享了在数智交易领域的前沿探索与人才布局。在激烈的角逐后，非凸科技代表为获奖队伍颁奖，鼓励他们保持创新热情与技术追求，并期待未来与更多优秀人才在产业实践中携手同行。<br/><img width="553" height="369" referrerpolicy="no-referrer" src="/img/bVdne7r" alt="image.png" title="image.png" loading="lazy"/><br/>本次竞赛，每支队伍需在5小时内通力协作，运用C/C++、Java和Python等其中一种编程语言解决13道复杂算法题目。经历4638次代码提交的密集交锋，最终30支队伍斩获金奖、60支队伍获得银奖、90支队伍摘得铜奖。北京大学“一步之遥”队以解出12题的出色表现荣膺冠军，复旦大学“随机一个字符串得了”队与上海交通大学“启明星”队分别获得亚军和季军。</p><p>以赛育才，聚力前行。未来，非凸科技将继续汇聚产业力量，携手学术界共同推动基础科学研究与应用技术创新的双向赋能，共同探索面向未来的人才培养路径。我们相信，在技术与人才双轮驱动的时代，唯有深化校企联结、促进全球智慧交融，才能为世界科技发展与经济社会繁荣持续注入新动力。</p>]]></description></item><item>    <title><![CDATA[2025年CRM选型全景图：国内外主流系]]></title>    <link>https://segmentfault.com/a/1190000047446343</link>    <guid>https://segmentfault.com/a/1190000047446343</guid>    <pubDate>2025-12-03 16:10:26</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>2025CRM选型全景图：国内外主流系统深度横评</h2><h3>一、引言：数字化深水区，CRM 成企业增长 “关键变量”</h3><p>2025 年，企业数字化转型进入 “价值兑现期”，客户关系管理（CRM）系统已从 “可选工具” 升级为 “增长基础设施”。客户全生命周期的精细化运营，成为企业穿越竞争周期的核心能力。据 Gartner 最新数据，2025 年全球 CRM 市场规模将突破 920 亿美元，年复合增长率维持 11.2% 的高位。</p><p>国内市场中，除了国际巨头与本土头部玩家，深耕行业 21 年的超兔一体云凭借 “全业务一体化” 特色崭露头角，为 6 万多家企业提供覆盖 CRM、进销存、财务、生产等的综合解决方案。面对多元化的产品矩阵，企业如何跳出 “功能堆砌” 的选型误区？本文将从核心能力、适用场景、技术特性三大维度，对 2025 年国内外主流 CRM 系统进行全景对比，为不同类型企业提供精准选型参考。</p><h3>二、国际主流 CRM 系统：技术积淀与全球化生态的双重优势</h3><p>国际厂商依托多年技术积累和全球化布局，在 AI 应用、生态兼容性和合规能力上形成壁垒，更适配有跨国业务或高端需求的企业。</p><h4>1. Salesforce：全球 CRM 的 “生态标杆”</h4><ul><li>市场地位：以 20.7%-26.1% 的全球市场份额连续 12 年领跑，2024 年营收超 300 亿美元，是行业绝对的头部玩家。</li><li>核心优势：Einstein AI 引擎贯穿营销、销售、服务全流程，支持客户流失预测、智能线索评分；AppExchange 平台拥有超 5000 个扩展应用，适配多语言、多币种及 GDPR/HIPAA 等全球合规要求；云原生架构保障跨国企业分布式业务的稳定运行。</li><li>局限性：单用户年费超 150 美元（约合人民币 1100 元 / 月），成本较高；国内访问速度受跨境网络影响，本地化响应较慢。</li><li>适用场景：预算充足的跨国企业、需要全球化合规支持的高端品牌（如微软、亚马逊）。</li></ul><h4>2. Zoho CRM：亚太市场的 “性价比之王”</h4><ul><li>市场表现：全球排名第五（市场份额 5.3%），亚太地区年复合增长率达 18%，中国市场占有率连续 5 年居首，达 25.18%。</li><li>核心优势：AI 功能深度渗透，“SDR 智能体” 将线索筛选效率提升 40%，“销售教练智能体” 实时优化销售话术；2025 年新增 ABM（账户式营销）功能，支持 Line、WhatsApp 等海外社交平台集成，适配跨境业务；QuickML 低代码平台允许企业无编程构建定制化模型。</li><li>局限性：复杂生产场景的适配能力较弱，高端定制化成本较高。</li><li>适用场景：中小企业、跨境电商、需要 AI 赋能的制造业（如极氪汽车、宝马中国）。</li></ul><h4>3. Microsoft Dynamics 365：生态协同的 “一体化代表”</h4><ul><li>核心优势：与 Office 365、Teams、Outlook 等微软产品无缝集成，实现 “办公自动化 + 销售管理” 深度融合；支持 CRM 与 ERP 功能联动，打通 “订单 - 生产 - 交付” 端到端数据链路。</li><li>局限性：行业垂直场景的定制化灵活性不足，小微型企业的成本压力较大。</li><li>适用场景：广泛使用微软生态的企业（如联想、工商银行）、看重跨部门业务协同的中大型企业。</li></ul><h4>4. 其他国际主流玩家</h4><ul><li>HubSpot CRM：初创企业入门首选，免费版支持无限用户，营销自动化模块可将线索转化率提升至 9.7%，适合预算有限的初创团队。</li><li>Pipedrive：聚焦销售漏斗管理，以可视化流程追踪商机进展，核心优势是销售环节效率提升，适配销售导向的中小企业。</li><li>SAP CRM：与 SAP ERP 无缝集成，擅长 “供应链 - 销售 - 服务” 全链路协同，适合奔驰、大众等制造巨头。</li></ul><h3>三、国内主流 CRM 系统：本土适配与场景化创新的突围</h3><p>国内厂商深耕本土企业需求，在钉钉 / 企业微信协同、信创支持、行业定制化上形成优势，超兔一体云等玩家更以 “全业务一体化” 打破传统 CRM 的功能边界。</p><h4>1. 超兔一体云：全业务一体化的 “实干派”</h4><ul><li>市场积淀：21 年行业经验，服务 6 万多家企业，尤其适配工业类、工贸类企业，40% 新客户来自老客户转介绍。</li><li>核心优势：</li><li>全业务打通：国内罕见的综合业务大底座，整合 CRM、进销存、供应链、财务、生产工单等功能，实现业务和数据底层连通；</li><li>低成本客制化：支持功能白名单订阅、三级菜单自定义、工作台定制等，企业可低成本切入，实现 “大底座、快启动”；</li><li>AI 深度应用：可基于客户视图定制销售跟单智能体，嵌入 Coze 工作流，支持自然语言生成工作流、电话录音 AI 分析等；</li><li>本土生态适配：多端覆盖 Web、App、小程序、RPA 插件，支持华为倡导的双重指挥系统模式，适配国内企业组织架构。</li><li>局限性：全球化合规与跨境业务支持能力弱于 Salesforce、Zoho；纯线上营销场景的功能丰富度不及 HubSpot。</li><li>适用场景：工业 / 工贸类企业、需要 “CRM + 进销存 + 生产” 一体化管理的中小企业、看重成本控制与灵活定制的本土企业。</li></ul><h4>2. 纷享销客：本土 CRM 的 “头部标杆”</h4><ul><li>市场地位：连续 5 年稳居国内 CRM 市场占有率首位（2024 年市占率 18.7%），累计融资超 30 亿元。</li><li>核心优势：覆盖 “营销获客 - 销售跟进 - 售后服务” 全流程闭环管理，支持从线索到回款的全生命周期追踪；无缝对接钉钉、企业微信、用友 / 金蝶 ERP 系统，打破数据孤岛；PaaS 平台支持快消、医疗等行业的定制化需求。</li><li>局限性：高端版定价较高，小型企业性价比不足；生产模块的适配能力较弱。</li><li>适用场景：国内中大型企业、需要跨部门协同的快消 / 医疗行业（如元气森林、振德医疗）。</li></ul><h4>3. 神州云动（CloudCC）：高合规需求的 “安全之选”</h4><ul><li>核心优势：支持 SaaS + 私有化混合部署，满足等保三级、GDPR 双合规要求；17 年企业级实施经验，提供 “销售 - 生产 - 交付” 全链路订单追踪，服务奔驰、ABB 等超 10000 家企业。</li><li>适用场景：高端制造、金融、医疗等对数据安全要求极高的行业。</li></ul><h4>4. 销售易（Neocrm）：AI + 大数据的 “成长型选手”</h4><ul><li>核心优势：双中台架构（业务中台 + 数据中台），融合 AI 与大数据技术，实现营销自动化、销售预测、客户服务全流程智能；为 IT 高科技、教育行业提供定制化方案。</li><li>局限性：系统稳定性略逊于行业头部玩家，大规模部署的适配能力有待验证。</li><li>适用场景：快速扩张的成长型企业、IT 高科技与教育行业客户。</li></ul><h3>四、2025 年 CRM 核心技术趋势：从 “功能覆盖” 到 “价值匹配”</h3><ol><li>AI 智能化成为核心竞争力：Gartner 数据显示，2025 年 AI 功能在 CRM 选型中的权重占比提升至 25%，智能预测、自动流程、话术优化成为标配，超兔的 AI 跟单智能体、Zoho 的销售教练智能体均是典型代表。</li><li>模块化与一体化两极分化：一方面，模块化订阅模式兴起，企业可按需选择功能降低成本；另一方面，像超兔这样的 “全业务一体化” 系统受青睐，解决多系统数据割裂问题。</li><li>本土化服务升级：国内企业更看重 2 小时故障响应、行业场景定制、信创适配，神州云动的本地化实施团队、超兔的专业客服均满足这一需求。</li><li>低代码 / 零代码定制普及：降低企业定制化门槛，Zoho 的 QuickML、超兔的自定义引擎均支持无编程或低编程的功能调整。</li></ol><h3>五、国内外主流 CRM 系统对比表（2025 最新版）</h3><table><thead><tr><th>系统名称</th><th>核心优势</th><th>局限性</th><th>适用企业类型</th><th>参考成本（单用户 / 月）</th><th>特色功能</th></tr></thead><tbody><tr><td>Salesforce</td><td>全球化生态、AI 能力强、合规覆盖广</td><td>成本高、国内访问慢、本地化弱</td><td>跨国企业、高端品牌</td><td>约 1100 元</td><td>Einstein AI、AppExchange 生态</td></tr><tr><td>Zoho CRM</td><td>高性价比、AI 功能全、跨境适配好</td><td>复杂生产场景适配弱</td><td>中小企业、跨境电商、制造业</td><td>约 300-800 元</td><td>SDR 智能体、ABM 营销、QuickML 低代码</td></tr><tr><td>Microsoft Dynamics 365</td><td>微软生态协同、CRM+ERP 联动</td><td>行业定制化弱、小微企业成本高</td><td>微软生态用户、中大型企业</td><td>约 800-1500 元</td><td>Office 集成、全链路数据打通</td></tr><tr><td>超兔一体云</td><td>全业务一体化、低成本客制化、稳定性高</td><td>全球化支持弱、纯线上营销功能不足</td><td>工业 / 工贸企业、中小企业、本土企业</td><td>约 500-750 元</td><td>CRM + 进销存 + 生产联动、AI 跟单智能体</td></tr><tr><td>纷享销客</td><td>本土生态全、全流程闭环、行业定制强</td><td>高端版成本高、生产模块弱</td><td>国内中大型企业、快消 / 医疗行业</td><td>约 500-1200 元</td><td>钉钉 / 企业微信集成、全生命周期管理</td></tr><tr><td>神州云动</td><td>双合规支持、数据安全强、实施经验丰富</td><td>性价比一般、小型企业适配弱</td><td>高端制造、金融、医疗行业</td><td>约 600-1300 元</td><td>混合部署、全链路订单追踪</td></tr><tr><td>销售易</td><td>AI + 大数据、双中台架构、行业方案专</td><td>稳定性一般、大规模部署适配弱</td><td>成长型企业、IT 高科技 / 教育行业</td><td>约 400-1000 元</td><td>销售预测、学员跟进定制</td></tr><tr><td>HubSpot CRM</td><td>免费版无用户限制、营销自动化强</td><td>功能深度不足、付费版升级成本高</td><td>初创团队、预算有限企业</td><td>免费 - 约 500 元</td><td>无限免费用户、社交媒体集成</td></tr><tr><td>Pipedrive</td><td>销售漏斗可视化、线索追踪高效</td><td>功能单一、无生产 / 财务联动</td><td>销售导向型中小企业</td><td>约 200-500 元</td><td>可视化流程、商机进展追踪</td></tr></tbody></table><h3>六、企业选型指南：按 “需求画像” 精准匹配</h3><ol><li>跨国业务 + 高合规需求：优先选择 Salesforce，其全球化生态与合规能力行业领先，适合预算充足的高端品牌。</li><li>跨境电商 + 中小企业：Zoho CRM 是最优解，高性价比与跨境适配能力兼顾，AI 功能可提升转化效率。</li><li>微软生态深度用户：Microsoft Dynamics 365 可实现办公与业务无缝协同，减少系统切换成本。</li><li>国内中大型企业 + 跨部门协同：纷享销客或神州云动，前者擅长本土生态整合，后者聚焦数据安全与合规。</li><li>工业 / 工贸企业 + 一体化需求：超兔一体云是核心推荐，CRM、进销存、生产工单的底层连通的，完美适配 “销售 - 生产 - 交付” 全流程。</li><li>初创团队 + 预算有限：HubSpot CRM 免费版可满足基础需求，营销自动化功能助力快速获客。</li><li>销售导向 + 效率优先：Pipedrive 的可视化销售漏斗能精准追踪商机，提升销售转化效率。</li></ol><h3>七、结语：选型的本质是 “需求与价值的匹配”</h3><p>2025 年 CRM 市场的竞争，已从 “功能比拼” 转向 “价值适配”。国际巨头的优势在于全球化与技术生态，本土玩家的核心竞争力是场景适配与成本控制，而超兔一体云的 “全业务一体化” 模式，为工业、工贸类企业提供了差异化选择。</p><p>企业选型时，无需盲目追求 “功能最全” 或 “品牌最响”，应围绕四大核心维度决策：企业规模（初创 / 中小 / 大型）、行业特性（工业 / 快消 / 跨境等）、业务需求（单一销售管理 / 全流程一体化）、预算范围。选对 CRM 不是终点，而是以系统为支点，撬动客户运营效率与业务增长的起点。</p>]]></description></item><item>    <title><![CDATA[【交通标志识别系统】Python+Ten]]></title>    <link>https://segmentfault.com/a/1190000047446348</link>    <guid>https://segmentfault.com/a/1190000047446348</guid>    <pubDate>2025-12-03 16:09:42</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、介绍</h2><p>交通标志识别系统，基于TensorFlow搭建Resnet50卷积神经网络算法，通过对58种常见的交通标志图片数据集进行训练，最后得到一个识别精度较高的模型，然后搭建Web可视化操作平台。</p><p><strong>技术栈</strong>：</p><ul><li>项目前端使用Html、CSS、BootStrap搭建界面。</li><li>后端基于Django处理逻辑请求</li><li>基于Ajax实现前后端数据通信</li></ul><p><strong>选题背景与意义</strong>：<br/>在智能交通系统蓬勃发展的当下，交通标志的精准识别对于保障行车安全、提升交通管理效率意义重大。然而，传统识别方法在面对复杂多变的交通环境时，往往存在识别精度不足、效率低下等问题。为此，我们开展交通标志识别系统项目，采用前沿技术，基于TensorFlow搭建Resnet50卷积神经网络算法，利用58种常见交通标志图片数据集训练，以获取高精度识别模型。同时，为方便用户操作，我们还运用Html、CSS等技术搭建Web可视化平台，实现便捷交互。</p><h2>二、系统效果图片展示</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446350" alt="图片" title="图片"/></p><h2>三、演示视频 and 完整代码 and 安装</h2><p>地址：<a href="https://link.segmentfault.com/?enc=qpJnptzhfNwtKL3P6vF1DA%3D%3D.A23mVaEfGqjEzejeKRN%2BN69JCaV3f67CUDyO%2Ba%2F7a3o%3D" rel="nofollow" target="_blank">https://ziwupy.cn/p/qBWZim</a></p><h2>四、卷积神经网络算法介绍</h2><p>卷积神经网络（CNN）是一种专门为处理具有网格结构数据（如图像）而设计的深度学习算法。它通过卷积层自动提取图像的局部特征，利用池化层降低数据维度、减少计算量并增强特征的鲁棒性，最后通过全连接层对提取的特征进行分类或回归。CNN的独特之处在于其局部连接和权重共享机制，极大减少了参数量，提高了训练效率，尤其擅长图像识别、目标检测等计算机视觉任务。</p><pre><code class="python">import tensorflow as tf
from tensorflow.keras import layers, models

# 构建简单CNN模型
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
</code></pre><p>上述代码用TensorFlow构建了一个简单的CNN模型，包含两个卷积层和池化层，用于提取图像特征，后接全连接层进行分类。该模型适用于手写数字识别等简单图像分类任务，通过调整网络结构和参数，可拓展至更复杂的图像识别场景。</p>]]></description></item><item>    <title><![CDATA[开源视频生成新标杆：美团LongCat ]]></title>    <link>https://segmentfault.com/a/1190000047446376</link>    <guid>https://segmentfault.com/a/1190000047446376</guid>    <pubDate>2025-12-03 16:08:51</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>今年涌现了大量新的视频模型，可以说 2025 年是视频建模真正主导公众对 AI 技术兴趣的第一年。随着 Sora 2 的普及，这一点变得越来越清晰。得益于 OpenAI 的一系列移动应用程序，获取视频生成工具的可能性与普及度达到了前所未有的高度。但闭源模型并非本文的重点，而这些模型的开源竞争实际上正变得比以往任何时候都更加令人印象深刻。</p><p>今年早些时候，HunyuanVideo 和 Wan2.1 以其令人难以置信的保真度、相对低廉的成本和公开可用性震撼了开源世界。这种发展趋势仍在继续，Wan 的新版本不断发布，其他竞争对手也纷纷入场。</p><p>在本文中，我们将介绍最新公开可用的视频模型：美团的 LongCat Video。这个出色的视频模型是进入我们工具箱的最新、最棒的开源工具，我们很高兴在本教程中展示如何从今天开始，利用 DigitalOcean 生成你自己的视频。</p><p>请跟随我们，简要了解 LongCat Video 的工作原理，以及一个展示如何在配备 NVIDIA GPU 的 DigitalOcean GPU Droplet 上设置并开始运行 LongCat Video 的教程。</p><p><strong>本文的核心要点</strong></p><ul><li>LongCat Video 是目前可用的、对标 Sora 2 的最佳开源竞争者。</li><li>用户可以使用 DigitalOcean GPU Droplets，通过自己的提示词和硬件，生成质量可与 Sora 2 媲美的视频。</li><li>运行 LongCat Video 至少需要 NVIDIA GPU 系统上具备 80GB 的显存，但可以扩展到多 GPU 设置以加快生成速度。</li></ul><h3>LongCat Video：概述</h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446379" alt="" title=""/></p><p>LongCat Video 的精妙之处在于其核心架构。这是因为他们非常巧妙地设计了一个单一管道来处理多项任务，包括文本到视频、图像到视频和视频延续。他们认为，所有这些任务都应被定义为视频延续，即模型根据给定的一组前置条件帧来预测未来的帧。</p><p>为了实现这一点，他们采用了相对标准的扩散变换器架构，并配有单流变换器块。“每个块包含一个 3D 自注意力层、一个用于文本条件的交叉注意力层，以及一个带有 SwiGLU 的前馈网络。为了进行调制，他们利用了 AdaLN-Zero，其中每个块都包含一个专用的调制 MLP。为了增强训练稳定性，在自注意力模块和交叉注意力模块中都应用了 RMSNorm 作为 QKNorm。此外，还采用了 3D RoPE 作为视觉标记的位置编码。”这种统一的架构允许使用相同的模型设计来完成三种视频任务中的任何一种。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446380" alt="" title="" loading="lazy"/></p><p>该模型在一个包含来自各种不同来源、视频类型和主题的大量标注视频语料库上进行了训练。我们可以在上图中看到训练数据中包含主题的大致聚类。对于这些数据，他们采用了强大的数据预处理和数据标注流程来进行文本标注。首先，收集并处理数据以确保没有重复项、裁剪黑边以及进行视频过渡分割。目前，他们没有详细讨论其数据来源。</p><p>LongCat Video 的闪光点及其与竞争对手的不同之处在于其长视频生成能力和高效的推理策略。对于长视频生成，LongCat-Video 原生地在视频延续任务上进行了预训练，使其能够生成长达数分钟的视频，而不会出现色彩漂移或质量下降。在实践中，这得益于训练策略的鲁棒性，其中对视频延伸的关注在训练结果中得以体现。至于高效的推理策略，我们指的是从粗到精的策略。在 LongCat Video 中，“视频首先生成为 480p、15fps，随后精炼至 720p、30fps。”（来源）此外，他们实现了一种新颖的块稀疏注意力机制，有效地将注意力计算量减少到标准密集注意力所需计算量的 10% 以下。这一设计显著提高了高分辨率精炼阶段的效率。最后，他们使用了一种新颖的组相对策略优化策略来进一步优化其流程。他们有效地采用了带有多个奖励的强化学习范式。</p><p>总而言之，美团 LongCat Video 是一个功能强大的视频生成和延续模型，作为一个工具，它既多功能又强大。他们认为其模型与最先进的竞争对手相比具有竞争力，我们希望通过本文展示如何在 DigitalOcean 的硬件上使用它。</p><h3>LongCat Video 演示：如何在 DigitalOcean GPU Droplet 上运行 LongCat Video</h3><p><strong>1、设置 Gradient ​GPU</strong>​<strong>​ Droplet</strong></p><p>要开始运行 LongCat Video，我们建议从<a href="https://link.segmentfault.com/?enc=gI%2FTErQjWk7WAU0NftWXmQ%3D%3D.jQg5xKcIW0SWRniizOm%2BMwiLuE7tOGF4RwH5Z3SB%2FzEUv7wLg1%2B8NbvfGWC4XEPwoXjvWxHioYLPip7JODL0aQ%3D%3D" rel="nofollow" target="_blank">创建一个 DigitalOcean Gradient GPU Droplet 云服务器</a>开始。这些 GPU Droplet 配备了运行本教程所需的 GPU 资源。我们建议至少使用单卡 NVIDIA H200 GPU，但拥有 8xH100 或 8xH200 设置的用户将看到更快的视频生成效率。DigitalOcean 的 GPU 资源不仅比 AWS、GCP 等大型云平台更加实惠，GPU Droplet 的性能比 Vast.ai 等 GPU 租赁平台更加稳定，而且 GPU 可选型号比 Linode 更加丰富。</p><p>要启动 GPU Droplet 并设置运行此演示的环境，我们建议使用本教程入门：<a href="https://link.segmentfault.com/?enc=ACCVPfVABVWPWxiHEF0jzA%3D%3D.zsZmhWKWCg9Y2KP8xyfLZGVmPRVS%2BjaJQMLhnxLTnajnj%2BIv4oJcAU75medJ5nuaNmKmZ5vYruFDDznvWHzRSA%3D%3D" rel="nofollow" target="_blank">https://blog.aidroplet.com/tutorials/do-gpu-jupyter-dl-setup/</a></p><p>可以从本地终端访问正在运行的 GPU Droplet 后，请继续跟着下一部分步骤来操作。</p><p><strong>2、为 LongCat Video 设置远程环境</strong></p><p>通过 SSH 连接到远程机器后，导航到你想要工作的目录。进入目录后，粘贴以下代码开始设置你的环境。</p><pre><code>git clone https://github.com/meituan-longcat/LongCat-Video
cd LongCat-Video
pip install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu124
pip install ninja
pip install psutil
pip install packaging
pip install flash_attn==2.7.4.post1
pip install -r requirements.txt</code></pre><p>完成上述步骤后，我们几乎可以准备开始了。现在要做的就是下载模型检查点！使用以下代码片段来完成：</p><pre><code>pip install "huggingface_hub[cli]"
huggingface-cli download meituan-longcat/LongCat-Video --local-dir ./weights/LongCat-Video</code></pre><p><strong>3、使用 Streamlit 应用程序生成 LongCat 视频</strong></p><p>对于演示，我们建议使用作者提供的 Streamlit 应用程序来运行视频生成。这个 Streamlit 演示使得在不同分辨率下生成视频、从静态图像生成视频以及延续视频长度变得简单。</p><p>设置完成后，我们就可以运行演示了。粘贴以下命令来运行演示。</p><pre><code>streamlit run ./run_streamlit.py --server.fileWatcherType none --server.headless=false</code></pre><p>复制 Streamlit 窗口的 URL，然后使用 Cursor 或 VS Code 的简单浏览器功能从本地访问该窗口。设置 VS Code 环境的步骤在 <a href="https://link.segmentfault.com/?enc=A%2BMUQCoCj9drU4SvdWTPxg%3D%3D.RafFz7SgdG%2BzL5hNZhGz1hEj1TCfps6ewAgGVMSEocFUsfngx8Ee7K%2FZvu5maiiXG0Mwq6PWS227ukXUDDxJNg%3D%3D" rel="nofollow" target="_blank">卓普云 aidroplet.com 的官网教程中有所概述</a>。卓普云是 DigitalOcean 中国区独家战略合作伙伴，为中国区企业客户提供商务对接与中文技术支持。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446381" alt="" title="" loading="lazy"/></p><p>上图显示了加载后的 Streamlit 演示界面。左侧有一个下拉菜单，我们可以在三个选项之间切换任务，启用蒸馏模式（将模型限制为 16 个推理步骤而非 50 个），启用超分辨率（从粗到精的上采样），以及设置生成参数。在窗口本身，我们有选项可以输入正面和负面提示词文本，并且在其他任务中，根据需要添加图像或视频。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446382" alt="" title="" loading="lazy"/></p><p>当我们运行生成器时，视频输出会显示在右侧！一定要尝试各种不同的提示词主题，来真正测试这个模型！</p><p>美团 LongCat Video 是一个真正强大的视频生成范式。我们对其多功能性和能力都印象深刻。在测试中，它确实是 Wan2.1 和 HunyuanVideo 向前迈出的一步，并且与 Wan2.2 等最先进的模型不相上下。不仅如此，统一的框架使得这个流程比竞争对手更加令人印象深刻和多功能。我们期待未来围绕 LongCat Video 发展出一个生态系统。</p>]]></description></item><item>    <title><![CDATA[从“数据孤岛”到“全域流转”：Kaiwu]]></title>    <link>https://segmentfault.com/a/1190000047446401</link>    <guid>https://segmentfault.com/a/1190000047446401</guid>    <pubDate>2025-12-03 16:08:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2><strong>关于数据分发</strong></h2><p>数据分发，简而言之，就是将数据从源头高效、可靠地传输到一个或多个指定目的地的过程。其核心目的在于，确保需要数据的人或系统能够在正确的时间、以恰当的形式获取到准确的数据，实现数据的共享与同步。</p><h2><strong>为什么需要数据分发？</strong></h2><p><strong>• 实时数据共享</strong></p><p>集团各部门协同合作，需确保所有数据部门获取最新数据，避免因数据延迟导致的业务决策偏差，如供应链协同场景、IoT 设备运维、营销自动化等。</p><p><strong>• 云边端数据协同</strong></p><p>终端设备产生的海量原始数据按需（全量或者预处理）同步至云端分布式集群，进行全局数据的建模、预测、分析。</p><p><strong>• 实时计算与告警</strong></p><p>实时将变更数据主动推送出去，客户端根据业务需求自由订阅数据，进行数据的实时计算、展示与告警。</p><h2><strong>设计理念与架构</strong></h2><h3><strong>1、 核心设计理念</strong></h3><p>KaiwuDB 数据分发以"<strong>数据价值最大化</strong>"为核心设计原则，在源端与多目标端之间搭建高效、灵活、可靠的流转桥梁，以最小化传输带宽、时间成本实现最大传输效率，发挥最大数据价值。</p><p><strong>• 实时数据驱动，赋能业务即时决策</strong></p><p>以 <strong>"数据实时流转为业务价值服务"</strong> 为核心，确保数据从产生到分发的延迟控制在毫秒级。让业务能基于最新数据做即时决策，将数据的 "时间价值" 最大化。</p><p><strong>• 业务场景导向，降低实时数据集成门槛</strong></p><p>围绕 <strong>"让实时数据集成更简单"</strong> 的理念，设计了开箱即用的订阅发布能力：无需用户开发复杂的自定义同步逻辑，通过配置化的方式即可实现跨集群、跨系统的数据实时同步，同时兼容多种技术生态（时序引擎、消息队列、业务应用），让不同业务场景能快速复用该能力。</p><p><strong>• 云边端一体化</strong></p><p>以 "<strong>本地计算 + 按需同步</strong>"为核心，边缘侧过滤冗余数据、云端汇聚核心信息，适配工业物联网、车联网等分布式场景。</p><h3><strong>2、 数据分发流程</strong></h3><p><img width="723" height="306" referrerpolicy="no-referrer" src="/img/bVdne8A" alt="" title=""/></p><p>KaiwuDB 数据分发流程图</p><p><strong>• 核心层</strong></p><p>借助 CDC（Change Data Capture，变更数据捕获）技术，精准捕获数据变更，支持基于 SQL 的订阅规则定义（如 WHERE vibration \&gt; 阈值的异常数据过滤）。</p><p><strong>• 传输层</strong></p><p>支持 DDL（数据定义语言，用于数据库结构变更）和业务数据同步分发：</p><p>• 发送至 Kafka（分布式消息队列），供第三方应用消费主题数据，支持多端异步数据消费场景；</p><p>• 传递至 KaiwuDB 集群 B 的数据订阅模块，实现跨集群的数据同步。</p><h2><strong>核心功能特性</strong></h2><h3><strong>1、 多维度数据订阅</strong></h3><p>• 提供全量初始化 + 增量同步双模式；</p><p>• 支持基于 SQL 条件的行过滤和列级投影同步。</p><h3><strong>2、 高可靠传输机制</strong></h3><p>• 基于 Raft 协议的多副本机制，单点故障后仍可从其它正常节点继续同步；</p><p>• 边缘节点断网时本地缓存数据，恢复后自动续传，保障弱网场景可用性。</p><h3><strong>3、 断点续传</strong></h3><p>定期保存已处理日志的时间点，在故障恢复时从断点继续同步，避免数据重复或遗漏。</p><h3><strong>4、 元数据智能映射</strong></h3><p>自动识别源库表结构变更（如字段增删），同步更新目标端 Schema，保持上下游数据结构一致性。</p><h3><strong>5、 高效传输</strong></h3><p>通过实时捕获数据的增量变更，仅传输变化部分，提升数据同步效率。</p><h2><strong>应用场景与核心价值</strong></h2><h3><strong>1、 部分典型应用场景</strong></h3><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446403" alt="" title="" loading="lazy"/></p><h3><strong>2、 核心业务价值</strong></h3><p><strong>• 提升实时决策</strong></p><p>• 打破设备厂商数据壁垒，实现跨部门协同优化，实时数据共享打破信息孤岛，生产、运维、供应链等部门可基于同一数据源协同决策；</p><p>• 动态分析与预测，结合历史数据分析趋势并预测潜在问题，提前制定维护计划，减少非计划停机时间。</p><p><strong>• 降低系统资源消耗</strong></p><p>• 按需订阅关注数据信息，避免全量数据传输，减少 70%+ 云端传输量，带宽成本降低 30%\~50%+；</p><p>• 边缘计算预处理，进行滤波、聚合或降采样处理，降低云端计算压力。</p><p><strong>• 增强业务灵活性</strong></p><p>• 支持灵活增减数据源或订阅主题，无需重构系统架构；</p><p>• 允许第三方开发者基于实时数据流开发增值应用，加速创新并丰富业务生态。</p><p><strong>• 安全合规</strong></p><p>支持数据脱敏订阅，符合 GDPR 数据最小化原则，保障车联网等场景的数据安全。</p>]]></description></item><item>    <title><![CDATA[LazyLLM × 硅基流动：共造面向开]]></title>    <link>https://segmentfault.com/a/1190000047446414</link>    <guid>https://segmentfault.com/a/1190000047446414</guid>    <pubDate>2025-12-03 16:07:19</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446417" alt="" title=""/></p><blockquote>在大模型全面走向工程落地的当下，<strong>LazyLLM</strong>正式与<strong>硅基流动（SiliconFlow）</strong> 达成深度合作，共同打造面向开发者的下一代智能应用底座。借助LazyLLM的一键接入线上模型API能力，硅基流动的大语言模型、多模态模型、向量与Embedding模型、文生图模型等已经完整接入，同一套接口即可覆盖从文本到图像、从检索到生成的全链路需求。</blockquote><p>这次合作带来的不仅是<strong>更强大的RAG选型</strong>，还进一步<strong>放大了Agent能力</strong>：在LazyLLM中，开发者可以基于统一的模型接入层，灵活编排工具调用与工作流，结合对MCP等协议的支持，将检索、调用外部系统、多模型路由、长程记忆等能力封装为可协作的智能体网络。</p><p>对于开发者而言，底层模型与算力的复杂度被彻底“藏”在LazyLLM+硅基流动这套组合之下——你只需聚焦业务逻辑，就能搭出既有强RAG能力、又有高扩展Agent能力的AI应用，从原型验证一路平滑升级到生产级部署。</p><hr/><p><strong>LazyLLM</strong></p><blockquote>LazyLLM是由商汤LazyAGI团队开发的一款开源低代码大模型应用开发工具，提供从应用搭建、数据准备、模型部署、微调到评测的一站式工具支持，以极低的成本快速构建AI应用，持续迭代优化效果。</blockquote><hr/><h2>一、<strong>API申请和环境配置</strong></h2><h3><strong>（一）账号注册</strong></h3><ul><li><p>注册硅基流动账号</p><p>（点击注册：<a href="https://link.segmentfault.com/?enc=fmoQM%2B6wVi7Etmqlv%2BX%2Bhw%3D%3D.NivzJ7L1y1u2jUSKXiYZxMbjJRAUXiHeCTC9Bw25SoTbkIZzvOYoMdhpylSlXRcxUCvmYO2qQ0g7DCy0TksZ8goQKYgb0PlBclWpYk3SmDEjBsg9Fptpd5dx71cTGO8R09I3W6DgYQguqF4hTX2MG%2F8hEvnWdnmuC93aObE%2F8MY%3D" rel="nofollow" target="_blank">https://account.siliconflow.cn/zh/login?redirect=https%3A%2F%2Fcloud.siliconflow.cn&amp;invitation=TR9Ym0c4）</a></p></li><li><p>进入控制台，获取APIkey</p><p>（获取方式：<a href="https://link.segmentfault.com/?enc=yApuYjD14mJ%2BR7Iss4p45A%3D%3D.uDjG7jc%2BxC6zIikqRxBABcaqQRrKZ4UPgRJEoLGKRkC8QMKApEHzfnEZlE%2FRwuyy32no9xKWQAshE0N0ZttHc0MQt48vNGxfwAfIplHKfbYlSADwDCFV%2BadlkOYJnpni15jNcYjgGa51qsuz815FCg%3D%3D" rel="nofollow" target="_blank">https://account.siliconflow.cn/zh/login?redirect=https%3A%2F%2Fcloud.siliconflow.cn%2Faccount%2Fak%3F）</a></p></li></ul><h3><strong>（二）环境配置</strong></h3><p>参考网页：快速开始-LazyLLM。</p><p>（<a href="https://link.segmentfault.com/?enc=r21U1UoO7cg3pm1BuoXhvw%3D%3D.ECCmdTm7fJetNOFU%2FerYOnjoGDeqSL8Lmj9uvtwCuaunOXnpS%2FLij04V9ajBAmOO" rel="nofollow" target="_blank">https://docs.lazyllm.ai/zh-cn/stable/）</a></p><hr/><h2><strong>二、API使用测试</strong></h2><h3><strong>（一）设置环境变量</strong></h3><p>可以使用以下命令设置对应的环境变量。或从代码中显示给入:</p><pre><code>export LAZYLLM_SILICONFLOW_API_KEY=&lt;申请到的api key&gt;
</code></pre><h3><strong>（二）实现对话和图片识别</strong></h3><h4><strong>1. 文本问答演示</strong></h4><p>填好api_key后，运行下面代码可以迅速调用模型并生成一个问答形式的前端界面：</p><pre><code>import lazyllm
from lazyllm import OnlineChatModule,WebModule
api_key = 'sk-' #替换成申请的api
# # 测试chat模块
llm = OnlineChatModule(source='siliconflow', api_key=api_key, stream=False)
w = WebModule(llm, port=8846, title="siliconflow")
w.start().wait()
</code></pre><p>我们询问“什么是LazyLLM”，运行结果如下:</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446418" alt="" title="" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446419" alt="" title="" loading="lazy"/></p><h4><strong>2. 多模态问答演示</strong></h4><p>在输入中通过lazyllm_files参数传入一张图片，并询问图片的内容，就可以实现多模态的问答。</p><pre><code>import lazyllm
from lazyllm import OnlineChatModule
api_key = 'sk-' #替换成申请的api
llm = OnlineChatModule(source='siliconflow', api_key=api_key, model='Qwen/Qwen2.5-VL-72B-Instruct')
print(llm('你好，这是什么？', lazyllm_files=['your_picture.png']))
</code></pre><p>这里我们使用这个图片测试多模态问答</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446420" alt="" title="" loading="lazy"/></p><p>命令行中输出结果：</p><blockquote><p>你好！这是一只小猫。它看起来非常可爱，毛茸茸的，眼睛大大的，背景是模糊的色彩，突出了小猫的细节。这样的图像通常能让人们感到温暖和愉快。你想了解更多关于小猫的信息吗？</p><p>（lazyllm）→LazyLLMgit:(main)</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446421" alt="" title="" loading="lazy"/></p></blockquote><h3><strong>（三）实现文生图和文生语音</strong></h3><p>使用OnlineMultiModalModule进行文生图和文生语音，运行后会输出生成的文件路径</p><pre><code>import lazyllm
from lazyllm import OnlineMultiModalModule
api_key = 'sk-xxx'
# 测试文生图 fuction=text2image
llm = OnlineMultiModalModule(source='siliconflow', api_key=api_key, function='text2image')
print(llm("生成一个可爱的小狗"))
# 测试文生语音 function=tts
llm = OnlineMultiModalModule(source='siliconflow', api_key=api_key, function='tts')
print(llm("你好，你叫什么名字", voice='fnlp/MOSS-TTSD-v0.5:anna'))
</code></pre><p>运行结果：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446422" alt="" title="" loading="lazy"/></p><p>生成的语音如下：</p><p>| tmpck44zfds.mp3 | 55.13KB | 2025-10-2723:13 |</p><p>（语音链接：<a href="https://link.segmentfault.com/?enc=x0QCEUHS3PVObwUZXoF4Lw%3D%3D.gphqFoe7NNXOPGT8eRuBo1Y8lFmBOasuD1hmWNICCQ7Z8YW9k4Lcdl7orH2hUzo6bPz9JnJ9ElbVJoXSsI10nzf%2FM2sE1fO8d7Ac6myouDZNqAUakqExdKUD5nlGwbEL" rel="nofollow" target="_blank">https://ones.ainewera.com/wiki/#/team/JNwe8qUX/share/7fy5a6mk/page/FUcz8wKs/）</a></p><h3><strong>（四）10+行代码实现知识库问答</strong></h3><h4><strong>1. 实现Eembed和Rerank功能</strong></h4><p>运行下面代码，使用OnlineEmbeddingModule进行向量化嵌入；设置type='rerank'调用重排序模型。</p><pre><code>import lazyllm
from lazyllm import OnlineEmbeddingModule
api_key = 'sk-'

#测试embed模块
llm = OnlineEmbeddingModule(source='siliconflow', api_key=api_key)
print(llm("苹果"))

#测试rerank模块
llm = OnlineEmbeddingModule(source='siliconflow', api_key=api_key, type='rerank')
print(llm(["苹果", ['苹果','香蕉','橘子']]))
</code></pre><p>向量化的结果如下：</p><pre><code>[-0.0024823144, -0.0075530247, -0.013154144, -0.031351723, -0.024489744, 0.009692847, 0.008086464, -0.037946977, 0.013251133, -0.046675995, -0.011390155, -0.011111312, 0.016779112, 0.054168403, 0.04849454, 0.014742341, 0.02341074, -0.015542501, 0.059939254, -0.024223024, 0.0065467632, -0.041244607, -0.022925794, -0.024804957, 0.006752865, -0.047548898, -0.03685585, 0.0513557....，-0.070656545, -0.01997975, 0.023398615, 0.008735079]
</code></pre><p>词相似性分数如下：</p><pre><code>[{'index': 0, 'relevance_score': 0.9946065545082092}, {'index': 2, 'relevance_score': 0.014802767895162106}, {'index': 1, 'relevance_score': 0.0004139931406825781}]
</code></pre><h4><strong>2. 知识库导入</strong></h4><p>我们使用中国古典文籍作为示例知识库，下载后放在database文件夹。</p><p>（示例数据集下载链接：<a href="https://link.segmentfault.com/?enc=514NpqBGE7WYejc9yYo2Iw%3D%3D.2uFM09iyVjJCEy%2B1vt38MnyvvYPe9C%2B%2BQOLls44616Pbb2PwbKo3iJoNkG1RcOJqbGbAkixrLFIX3eb1BCxKMrt9D2zucaIbo%2FqXfW%2F%2F4aCj2RmQp8N0miwUhIdiTKkn" rel="nofollow" target="_blank">https://huggingface.co/datasets/LazyAGI/Chinese\_Classics\_Articles/tree/main）</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446424" alt="" title="" loading="lazy"/></p><p>首先定义embed模型，然后使用LazyLLM的Document组件创建文档管理模块，以实现知识库的导入。</p><pre><code>import lazyllm
api_key='sk-'
embed_model = lazyllm.OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
documents = lazyllm.Document(dataset_path="database", embed=embed_model)
</code></pre><h4><strong>3. 知识库检索</strong></h4><p>现在有了外部知识库，LazyLLM中使用Retriever组件可以实现检索知识库并召回相关内容。使用示例：</p><pre><code>import lazyllm
from lazyllm.tools import Retriever, Document, SentenceSplitter
api_key='sk-'
embed_model = lazyllm.OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
documents = Document(dataset_path='database', embed=embed_model, manager=False)
rm = Retriever(documents, group_name='CoarseChunk', similarity='bm25', similarity_cut_off=0.01, topk=6)
rm.start()
print(rm("user query"))
</code></pre><h4><strong>4. 知识库问答</strong></h4><p>结合上述模型、文档管理和检索模块，搭配LazyLLM内置的Flow组件进行完整的数据流搭建，完整代码如下：</p><pre><code>import lazyllm
from lazyllm import (
    OnlineEmbeddingModule, OnlineChatModule, Document, SentenceSplitter,
    Retriever, Reranker, ChatPrompter, pipeline
)
# 初始化api key和提示词
api_key = 'sk-'
prompt = """
You will play the role of an AI Q&amp;A assistant and complete a dialogue task.
In this task, you need to provide your answer based on the given context and question.
"""
# 初始化模型
embed_model = OnlineEmbeddingModule(source="siliconflow", api_key=api_key)
rerank_model = OnlineEmbeddingModule(source="siliconflow", api_key=api_key, type="rerank")
llm = OnlineChatModule(source="siliconflow", api_key=api_key)
# 定义文档管理模块，并创建节点组
doc = Document(dataset_path="/home/xxx/database", manager=False, embed=embed_model)
doc.create_node_group(name="block", transform=SentenceSplitter, chunk_size=1024, chunk_overlap=100)
doc.create_node_group(name="line", transform=SentenceSplitter, chunk_size=128, chunk_overlap=20, parent="block")
# 构建RAG pipeline（多路召回--重排--提示词拼接--大模型回答）
with pipeline() as ppl:
    with lazyllm.parallel().sum as ppl.prl:
        prl.r1 = Retriever(doc, group_name='line', similarity="cosine", topk=6, target='block')
        prl.r2 = Retriever(doc, group_name='block', similarity="cosine", topk=6)
    ppl.reranker = Reranker('ModuleReranker', model=rerank_model, output_format='content',
                            join=True) | bind(query=ppl.input)
    ppl.formatter = (lambda context, query: dict(context_str=str(context), query=query)) | bind(query=ppl.input)
    ppl.llm = llm.prompt(lazyllm.ChatPrompter(prompt, extra_keys=["context_str"]))
ppl.start()
query = "何为天道"

print(ppl(query))
</code></pre><p>可以看到RAG很好地从《道德经》等中取回了有关天道的内容，并传给大模型进行回答。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446425" alt="" title="" loading="lazy"/></p><hr/><p>更多技术内容，欢迎移步 "LazyLLM" 讨论！</p>]]></description></item><item>    <title><![CDATA[14款主流CRM一体化能力全景横评 傲视]]></title>    <link>https://segmentfault.com/a/1190000047446434</link>    <guid>https://segmentfault.com/a/1190000047446434</guid>    <pubDate>2025-12-03 16:06:33</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在企业数字化转型中，<strong>CRM</strong> <strong>的核心价值已从“客户管理”延伸至“全业务链路协同”</strong> ——从客户获客、合同签订，到订单履约、财务结算，各环节的无缝衔接直接决定运营效率与利润空间。本文基于<strong>客户管理、合同管理、订单管理、财务集成</strong>四大核心维度，对14款主流CRM品牌（超兔一体云、Salesforce、SAP CRM、Microsoft Dynamics 365、Oracle CX、Pipedrive、金蝶、SugarCRM、Zoho、Freshsales、HubSpot CRM、用友CRM、SuiteCRM、EC）进行深度横评，为不同场景的企业提供选择参考。</p><h2>一、对比维度说明</h2><p>本次横评聚焦“业务全链路闭环”，将四大核心维度拆解为16个子指标（见表1），覆盖从“客户到财务”的全流程能力：</p><table><thead><tr><th>核心维度</th><th>子指标</th></tr></thead><tbody><tr><td>客户管理</td><td>全生命周期覆盖、多渠道整合、AI智能、可视化工具、客资沉淀能力</td></tr><tr><td>合同管理</td><td>全流程覆盖、合规性、AI辅助、模板自定义、与订单联动能力</td></tr><tr><td>订单管理</td><td>全链路协同、多渠道处理、库存联动、状态跟踪、自动化触发能力</td></tr><tr><td>财务集成</td><td>ERP对接、业财数据联动、多货币支持、自动化凭证、风险管控能力</td></tr></tbody></table><h2>二、各维度横向对比</h2><h3>（一）客户管理：从“线索到复购”的全生命周期能力</h3><p>客户管理的核心是“精准识别需求+高效推进转化”，关键看“全流程覆盖深度”与“数据整合能力”。</p><h4>1. 核心能力对比表（表2）</h4><table><thead><tr><th>品牌</th><th>全生命周期覆盖</th><th>多渠道整合</th><th>AI智能</th><th>可视化工具</th><th>客资沉淀能力</th></tr></thead><tbody><tr><td>超兔一体云</td><td>三一客节点+五大跟单模型+客池分类</td><td>微信/广告/线下+智能表单</td><td>用户画像云图+跟进节奏提醒</td><td>客户视图+跟单时间线</td><td>与合同/订单/财务联动</td></tr><tr><td>Salesforce</td><td>销售云+服务云+营销云+数据云</td><td>多渠道线索+360度视图</td><td>AI预测+自动化任务分配</td><td>销售管道+Tableau分析</td><td>跨云数据整合</td></tr><tr><td>SAP CRM</td><td>营销+销售+服务闭环</td><td>多渠道线索+客户数据整合</td><td>市场预测+行为分析</td><td>销售漏斗+报表</td><td>ERP联动客资共享</td></tr><tr><td>Microsoft Dynamics 365</td><td>销售+营销+服务集成</td><td>Office 365+Azure</td><td>生成式AI摘要+报价生成</td><td>Power BI+Excel</td><td>微软生态数据共享</td></tr><tr><td>Oracle CX</td><td>销售+营销+服务+电商</td><td>多渠道互动+B2B数据整合</td><td>AI行为预测+合同简化</td><td>统一商务视图+云同步</td><td>跨部门数据打通</td></tr><tr><td>Pipedrive</td><td>可视化漏斗+阶段管理</td><td>移动端+线索拖拽</td><td>AI跟进提醒+商机优先级</td><td>拖拽式管道+业绩同步</td><td>轻量化客资记录</td></tr><tr><td>金蝶</td><td>全生命周期+ERP联动</td><td>财务/供应链联动</td><td>2025年AI条款校验（98%准确率）</td><td>订单追踪+业绩预测</td><td>集团级客资沉淀</td></tr><tr><td>SugarCRM</td><td>定制化全生命周期</td><td>多维度分类+历史订单</td><td>AI需求预测+行为分析</td><td>自定义字段+模块</td><td>复杂场景客资管理</td></tr><tr><td>Zoho</td><td>潜客+商机+动态联动</td><td>微信/飞书+多渠道</td><td>Zia助手+邮件分析</td><td>报表+销售管理</td><td>跨境客资同步</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“精准转化利器”</strong> 以“<strong>三一客节点</strong>”（定性+定级+定量）和“<strong>五大跟单模型</strong>”（适配不同业务场景）实现客户分层，通过“<strong>客池分类</strong>”（需求培养→有需求→成功）自动化推进生命周期。例如，线索进入系统后，先通过“用户画像云图”识别高价值客群，再用“客户视图”呈现全景信息（跟进历史、订单记录），帮助销售聚焦核心商机。 <strong>流程图：超兔客户生命周期管理逻辑</strong></li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446436" alt="" title=""/></p><pre><code>flowchart LR
    A[多渠道获客] --&gt; B[智能表单收线索]
    B --&gt; C[用户画像分级]
    C --&gt; D{高价值?}
    D --&gt;|是| E[需求培养客池]
    E --&gt; F[三一客节点评估]
    F --&gt; G[五大模型跟进]
    G --&gt; H{有需求?}
    H --&gt;|是| I[合同签订]
    I --&gt; J[订单生成]
    J --&gt; K[复购/售后]</code></pre><ul><li><strong>Salesforce：中大型企业的“全渠道引擎”</strong> 依托“<strong>销售云+服务云+营销云+数据云</strong>”的生态，实现从线索捕获到售后的全流程覆盖。例如，“数据云”可激活沉睡客户数据，“360度视图”整合销售、服务、营销的互动记录，让跨部门协同无死角。AI能力聚焦“预测客户行为”（如哪些客户会复购）和“自动化任务”（如提醒跟进），适合需要全渠道触达的中大型企业。</li><li><strong>金蝶：制造/集团企业的“客资沉淀专家”</strong> 依托ERP生态，客户信息与<strong>财务、供应链数据深度联动</strong>（如客户历史订单→库存备货→应收款提醒），解决了传统CRM“客资孤立”的痛点。2025年将升级“AI条款校验”（准确率98%），进一步提升合同合规性，适合重视“业财融合”的制造或集团企业。</li></ul><h3>（二）合同管理：从“起草到归档”的全流程合规能力</h3><p>合同管理的核心是“效率+合规”，关键看“流程覆盖度”与“AI辅助能力”。</p><h4>1. 核心能力对比表（表3）</h4><table><thead><tr><th>品牌</th><th>全流程覆盖</th><th>合规性</th><th>AI辅助</th><th>模板自定义</th><th>与订单联动</th></tr></thead><tbody><tr><td>超兔一体云</td><td>起草→审批→签订→执行→归档</td><td>自定义查重+企业简称模糊查重</td><td>智能条款匹配+多期应收拆分</td><td>10类合同模板（服务/贸易等）</td><td>合同信息自动同步订单</td></tr><tr><td>Salesforce</td><td>模板→签章→审批→履约</td><td>GDPR/CCPA等多国法规</td><td>自动化合同生成+电子签章</td><td>自定义模板+变量替换</td><td>商机→合同→订单闭环</td></tr><tr><td>SAP CRM</td><td>开发→验证→修订→提交</td><td>行业合规模板</td><td>合同租约管理+销售分析</td><td>标准化模板+定制</td><td>合同与销售分析联动</td></tr><tr><td>Microsoft Dynamics 365</td><td>生成→审批→归档</td><td>微软合规框架</td><td>生成式AI摘要+报价转合同</td><td>Office模板+AI生成</td><td>订单触发合同创建</td></tr><tr><td>Oracle CX</td><td>报价→订单→合同</td><td>B2B合规流程</td><td>生成式AI简化合同+协作流程</td><td>统一模板+复杂订单拆分</td><td>统一商务视图联动</td></tr><tr><td>金蝶</td><td>起草→审批→履约→归档</td><td>内置行业合规条款</td><td>2025年AI条款校验（98%）</td><td>12类标准化模板</td><td>订单→合同→收付款联动</td></tr><tr><td>SugarCRM</td><td>定制化全流程</td><td>法务系统集成</td><td>风险条款预警+需求匹配</td><td>自定义模板+变量</td><td>合同与订单状态同步</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“合同闭环工具”</strong> 支持<strong>10类合同类型</strong>（服务、贸易、非标定制等），全流程电子化（从起草到归档）。例如，“智能应收拆分”可根据合同约定（如3期付款）自动计算每期金额，“企业简称模糊查重”解决了“同一家企业多个简称”的问题，避免重复签约。与订单的联动设计（合同信息自动同步至订单），彻底消除“合同与订单不一致”的痛点。</li><li><strong>Oracle CX：复杂B2B场景的“合同简化专家”</strong> 针对B2B企业“合同流程长、协作难”的痛点，提供<strong>“统一商务视图”</strong>（整合报价、订单、合同），支持复杂订单拆分（如拆分多个子合同）和跨部门协作。生成式AI可简化合同起草（如自动填充客户信息、条款），降低法务审核成本，适合需要频繁签订复杂合同的B2B企业。</li><li><strong>金蝶：财务精细化企业的“合同合规管家”</strong> 内置<strong>12类标准化合同模板</strong>（如制造、建筑），2025年升级的“AI条款校验”可识别风险条款（如“无理由退款”），准确率达98%。与财务系统的联动（合同→收付款计划→发票），确保“合同约定=财务执行”，适合重视“合同履约与财务一致”的企业。</li></ul><h3>（三）订单管理：从“生成到交付”的全链路协同能力</h3><p>订单管理的核心是“协同+效率”，关键看“与采购/库存的联动”和“状态跟踪能力”。</p><h4>1. 核心能力对比表（表4）</h4><table><thead><tr><th>品牌</th><th>全链路协同</th><th>多渠道处理</th><th>库存联动</th><th>状态跟踪</th><th>自动化触发</th></tr></thead><tbody><tr><td>超兔一体云</td><td>订单→采购→库存→交付</td><td>全渠道订单整合</td><td>订单生成→自动算采购量→匹配供应商</td><td>实时状态+超发预警</td><td>签约/开票/发货触发应收</td></tr><tr><td>Salesforce</td><td>订单→供应链→ERP</td><td>商业云多渠道处理</td><td>与制造云联动→库存实时同步</td><td>销售管道+Tableau跟踪</td><td>订单触发供应链协同</td></tr><tr><td>SAP CRM</td><td>订单→后台交易系统</td><td>多渠道订单整合</td><td>库存变动→财务凭证自动生成</td><td>订单状态+报表分析</td><td>订单触发采购计划</td></tr><tr><td>金蝶</td><td>订单→合同→进销存</td><td>电商/线下订单整合</td><td>订单→库存检查→采购计划</td><td>实时状态+业绩预测</td><td>订单触发收付款计划</td></tr><tr><td>Zoho</td><td>订单→飞书审批→财务</td><td>多渠道订单同步</td><td>库存查询→缺货提醒</td><td>移动端状态跟踪</td><td>订单触发审批流程</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><strong>超兔一体云：中小微企业的“订单协同引擎”</strong> 实现“订单→采购→库存→交付”的全链路协同：订单生成时，自动计算所需采购量（如“订单要100个产品，库存有30个，需采购70个”），并匹配最佳供应商（根据价格、交付周期）。“超发预警”功能（如客户账期内超订单发货），避免企业资金风险。与财务的联动（签约/开票/发货触发应收），确保“货出去，钱进来”的节奏。</li><li><strong>金蝶：制造企业的“订单-库存联动专家”</strong> 与进销存模块深度集成，<strong>订单生成自动触发合同创建</strong>，同时检查库存（如“订单要500个零件，库存有200个，需采购300个”），并生成采购计划。实时订单状态跟踪（如“已发货”“已收款”）与业绩预测（如“本月订单额预计100万”），帮助制造企业精准把控生产节奏。</li></ul><h3>（四）财务集成：从“订单到凭证”的业财一体化能力</h3><p>财务集成的核心是“数据一致性+自动化”，关键看“与ERP的对接”和“凭证生成效率”。</p><h4>1. 核心能力对比表（表5）</h4><table><thead><tr><th>品牌</th><th>ERP对接</th><th>业财联动</th><th>多货币支持</th><th>自动化凭证</th><th>风险管控</th></tr></thead><tbody><tr><td>超兔一体云</td><td>柠檬云等财务平台</td><td>应收+开票+回款三角联动</td><td>支持</td><td>联动生成凭证</td><td>超发预警+账期控制</td></tr><tr><td>Salesforce</td><td>SAP/Oracle等ERP</td><td>订单→财务报表+Tableau分析</td><td>支持</td><td>订阅制自动账单</td><td>信用度监控+发货控制</td></tr><tr><td>SAP CRM</td><td>SAP ERP深度整合</td><td>库存→财务凭证自动生成</td><td>支持</td><td>模块联动生成凭证</td><td>账期预警+应收管控</td></tr><tr><td>金蝶</td><td>金蝶ERP天然对接</td><td>合同→收付款→发票联动</td><td>支持</td><td>业财数据一致+自动凭证</td><td>预算管控+成本分析</td></tr><tr><td>Zoho</td><td>第三方财务系统</td><td>多货币结算+飞书审批</td><td>支持（跨境业务）</td><td>未来深化财务联动</td><td>无明确风险管控</td></tr><tr><td>用友CRM</td><td>用友ERP深度对接</td><td>订单→财务实时同步</td><td>支持</td><td>自动化凭证+报表</td><td>信用度控制+应收预警</td></tr></tbody></table><h4>2. 关键能力解读</h4><ul><li><p><strong>超兔一体云：中小微企业的“业财自动化工具”</strong> 依托“<strong>应收+开票+回款三角联动</strong>”，实现从订单到凭证的全自动化：</p><ul><li>订单生成→根据合同约定触发应收（如“签约触发30%应收”）；</li><li>开票→自动关联订单与应收；</li><li>回款→自动匹配应收与订单；</li><li>凭证生成→一键读取CRM数据（出库、回款、开票），匹配货、款、票信息，生成可视化凭证（支持二次编辑），并推送至柠檬云等财务平台。 <strong>流程图：超兔业财一体化逻辑</strong></li><li><img referrerpolicy="no-referrer" src="/img/remote/1460000047446437" alt="" title="" loading="lazy"/></li></ul></li></ul><pre><code>flowchart LR
    A[合同签订] --&gt; B[订单生成]
    B --&gt; C[库存出库]
    C --&gt; D[触发应收]
    D --&gt; E[开票]
    E --&gt; F[回款]
    F --&gt; G[联动生成凭证]
    G --&gt; H[推送财务系统]</code></pre><ul><li><strong>金蝶：集团企业的“财务精细化引擎”</strong> 天然对接金蝶ERP，<strong>合同→收付款→发票</strong>的联动设计，确保业财数据100%一致。例如，合同约定“3期付款”，系统自动生成收付款计划，发票开具时关联合同与订单，财务人员无需手动核对。“预算管控”功能（如“部门月度费用不超过10万”），帮助集团企业实现财务精细化管理。</li></ul><h2>三、综合能力雷达图与选择建议</h2><p>基于四大维度的核心能力，我们对各品牌进行<strong>10分制评分</strong>（1=无能力，10=顶尖），并绘制雷达图（表6）：</p><table><thead><tr><th>品牌</th><th>客户管理</th><th>合同管理</th><th>订单管理</th><th>财务集成</th><th>综合得分</th></tr></thead><tbody><tr><td>超兔一体云</td><td>9</td><td>8</td><td>8</td><td>9</td><td>34</td></tr><tr><td>Salesforce</td><td>10</td><td>9</td><td>9</td><td>9</td><td>37</td></tr><tr><td>金蝶</td><td>8</td><td>9</td><td>9</td><td>10</td><td>36</td></tr><tr><td>Oracle CX</td><td>9</td><td>9</td><td>8</td><td>9</td><td>35</td></tr><tr><td>Microsoft Dynamics 365</td><td>9</td><td>8</td><td>8</td><td>8</td><td>33</td></tr><tr><td>Pipedrive</td><td>8</td><td>7</td><td>7</td><td>6</td><td>28</td></tr><tr><td>Zoho</td><td>8</td><td>7</td><td>7</td><td>8</td><td>30</td></tr><tr><td>用友CRM</td><td>8</td><td>8</td><td>8</td><td>9</td><td>33</td></tr></tbody></table><h3>最终选择建议</h3><ol><li><strong>中小微企业、全业务一体化需求</strong>：<strong>超兔一体云</strong>（性价比高，覆盖客户→合同→订单→财务全流程，功能贴合中小微场景）。</li><li><strong>中大型企业、全渠道管理</strong>：<strong>Salesforce</strong>（云生态强大，全流程覆盖，适合需要跨部门协同的中大型企业）。</li><li><strong>制造/集团企业、业财融合</strong>：<strong>金蝶</strong>（ERP联动，财务精细化，适合重视“客资 - 库存 - 财务”联动的制造企业）。</li><li><strong>复杂B2B场景、跨部门协同</strong>：<strong>Oracle CX</strong>（统一商务视图，简化合同流程，适合B2B企业的长周期订单）。</li><li><strong>中小销售团队、高透明度</strong>：<strong>Pipedrive</strong>（可视化漏斗，跟进提醒，适合需要快速推进商机的销售团队）。</li><li><strong>跨境业务企业、多货币结算需求</strong>：<strong>Zoho</strong>（支持多货币结算，与丰富的Zoho及第三方应用无缝集成，适合跨境业务财务协同）。</li></ol><p>在企业数字化转型的浪潮中，CRM系统作为企业全业务链路协同的核心工具，其选择至关重要。企业在挑选CRM品牌时，不能仅仅关注品牌的知名度和市场份额，更要深入剖析自身的业务需求、发展阶段以及预算限制等因素。无论是中小微企业追求的高性价比和全业务一体化，还是中大型企业对全渠道管理和跨部门协同的需求，亦或是制造/集团企业强调的业财融合，都能在众多的CRM品牌中找到合适的解决方案。希望本文的深度横评和选择建议，能为企业在CRM选型过程中提供有价值的参考，助力企业提升运营效率，实现可持续发展。</p>]]></description></item><item>    <title><![CDATA[从“建模型”到“管业务”：一位开发者眼中]]></title>    <link>https://segmentfault.com/a/1190000047446446</link>    <guid>https://segmentfault.com/a/1190000047446446</guid>    <pubDate>2025-12-03 16:05:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作为一名在数字孪生领域摸爬滚打多年的应用开发者。过去几年，我和团队接过不少数据中心运维管理的项目，从最初的“炫酷大屏”到后来的“实用工具”，踩过不少坑，也积累了一些心得。今天，我想抛开那些宏大的概念，以一个同行、一个实践者的身份，和大家聊聊，在数据中心这个精密复杂的领域，我们究竟需要什么样的工具，才能把数字孪生从“可视化看板”变成真正的“业务驾驶舱”。</p><h2>一、 第一道坎：如何让运维团队自己“搭场景”？</h2><p>我们最早的项目，常常陷入一个怪圈：业务部门提需求 -&gt; 我们找三维建模师建模型 -&gt; 反复沟通修改 -&gt; 交付后业务逻辑一变，模型又得大改。周期长、成本高，最关键的是，运维团队离实际的场景构建太远，他们的业务洞察无法快速体现在三维世界里。<br/>后来我们意识到，问题的核心在于<strong>工具链的断裂</strong>。理想的工具，应该能让熟悉数据中心机房布局、设备型号的运维专家，即使不会专业建模软件，也能主导场景的搭建，就例如“图观”端渲染的场景编辑器。这需要两个关键支撑：<br/><strong>1. 一个“开箱即用”的资产库，而不是从零开始建模。</strong><br/>想象一下，如果一个工具内置了从服务器机柜、UPS、精密空调、PDU，到地板、桥架、线缆等数据中心全要素的高精度模型库，并且材质、规格可调，那会节省多少时间？我们的做法是，让运维工程师直接在“图观”场景编辑器的模型库里，像搭积木一样，通过拖拉拽，快速还原出数据中心的真实物理布局。这不仅仅是快，更是保证了模型的行业规范性和专业性。<br/><img width="587" height="330" referrerpolicy="no-referrer" src="/img/bVdmqxd" alt="" title=""/><br/><strong>2. 支持“专业级”微调，满足苛刻的细节要求。</strong><br/>当然，完全标准化不可能覆盖所有情况。当遇到特殊品牌设备或需要展示内部结构时，工具必须能无缝导入运维方提供的专业GLB等格式模型。更关键的是，要能对模型进行深度的“化妆”——调整PBR物理材质（让金属更有光泽、玻璃更通透）、设置关节动画（比如模拟柜门开合、风扇转动）。这样，才能在保证效率的同时，不牺牲视觉表现力和准确性。<br/><strong>价值点提炼</strong>： 将场景构建的主导权部分交还给业务专家，实现从“项目制交付”到“持续化运营”的转变。运维团队可以根据设备变更、布局调整，随时自行更新孪生场景，让数字世界与物理世界保持同步的成本降到最低。</p><h2>二、 灵魂所在：如何让冷冰冰的数据在三维空间里“说话”？</h2><p>模型建得再漂亮，如果只是静态的“雕塑”，价值也有限。数字孪生的灵魂在于<strong>数据驱动</strong>。在数据中心，这意味着要将动环监控（温湿度、漏水、烟感）、设备运行（CPU负载、功耗、流量）、资产管理（设备位置、生命周期）等多源、异构的数据流，与三维空间中的具体对象精准关联并直观呈现。<br/>这里有几个我们总结的实用技巧：<br/><strong>技巧1：用“图层”思维管理数据可视化。</strong><br/>不要试图把所有数据一次性堆叠在屏幕上。优秀的工具应允许你创建不同的可视化图层：比如一个“热力图层”来映射机房实时的温度分布，一个“告警图层”让发生故障的设备高亮闪烁并悬浮显示详情，一个“容量图层”用三维柱图显示每个机柜的U位占用和功耗情况。这些图层可以随时开关，让运维人员在不同关注点间灵活切换。<br/><strong>技巧2：实现“所指即所得”的深度交互。</strong><br/>这是区分“高级可视化”和“业务工具”的关键。我们追求的效果是：在三维场景中点击一台服务器，旁边面板立刻显示其所有实时性能指标和历史曲线；反过来，在资产列表中选择一个设备编号，场景镜头能自动定位并高亮该设备。这一切的交互逻辑，应该能通过图形化的方式配置完成，而不是编写大量硬编码。比如，配置一条规则：“当‘空调1号’回风温度&gt;26℃时，在场景中将其模型颜色渐变为红色，并触发告警面板推送消息”。<br/><strong>价值点提炼</strong>： 通过配置化的数据联动与交互，将运维人员的业务经验（如“什么数据重要”、“数据异常该如何关联展示”）沉淀为可复用的数字孪生交互模板，真正构建起<strong>面向业务的、可操作的分析环境</strong>，而不仅仅是事后查看的报表。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdm7Rl" alt="" title="" loading="lazy"/></p><h2>三、 现实考量：如何平衡“电影级画质”与“全员可访问”？</h2><p>这是我们在交付时经常遇到的客户分歧：管理层想要在指挥中心大屏上看到电影级渲染效果的震撼全景，而一线运维工程师则希望在自己的办公电脑甚至平板上，能快速、流畅地接入系统进行日常巡检。<br/><strong>我们的解法是：渲染模式与业务应用解耦。</strong><br/>这意味着，我们用于构建业务逻辑（数据绑定、交互规则、UI界面）的“应用层”是一套独立的、轻量的代码或配置。而三维场景的渲染，则可以根据终端情况灵活选择“服务端流渲染”或“客户端端渲染”。<br/><strong>对于指挥中心大屏</strong>： 采用流渲染模式。所有复杂的图形计算都在高性能服务器上完成，只将渲染后的高清视频流推送到大屏。这样能保证极致的画质和稳定性，展现所有光影、材质细节。<br/><strong>对于运维桌面端/移动端</strong>： 采用端渲染模式。将轻量化的场景数据下发到终端，利用本地显卡进行渲染。这样对服务器压力极小，支持高并发访问，且在网络波动时体验更稳定。<br/><strong>价值点提炼</strong>： “一套业务逻辑，多种呈现方式”。这极大地提升了开发效率和部署灵活性。我们只需开发一次核心业务应用，就能同时满足高端演示与日常使用的不同需求，避免了为不同终端维护多套代码的困境，也降低了客户的总体拥有成本。<br/><img width="640" height="360" referrerpolicy="no-referrer" src="/img/bVdm7Rm" alt="" title="" loading="lazy"/></p><h2>四、 从工具到伙伴：全链路效率与成本最优解</h2><p>数字孪生项目的成功，不仅在于核心开发工具，还依赖于一系列能提升整体生产效率的“周边武器”。例如，在构建园区级数据中心数字孪生时，我们曾为生成周边道路、建筑等环境背景而头疼。<br/>这时，如果工具体系里包含一个城市生成工具，能基于GIS数据自动生成符合比例的路网、批量生成风格化建筑，就能将我们从重复劳动中解放出来，聚焦于数据中心本体。同样，完善的API文档、调试工具和项目范例，能让我们在对接客户自有监控平台、资产管理系统时事半功倍。<br/>最后，在部署阶段，灵活的选项至关重要。项目初期或用于创新验证时，能够快速使用低成本的云服务进行部署和演示，极大降低了试错门槛。而当项目成熟、需要深度定制或对数据安全有严格要求时，又能支持完整的私有化部署，保障客户的核心利益。<br/><strong>价值点提炼</strong>： 选择一套工具，不仅是选择一个软件，更是选择一个能够伴随项目全生命周期成长、在不同阶段都能提供最优性价比解决方案的合作伙伴。它帮助团队控制初期风险，并平滑支持未来扩展。</p><h2>结语</h2><p>回顾这些年的实践，我深刻感受到，数字孪生在数据中心运维领域的价值，正从“视觉创新”走向“业务赋能”。其核心不在于渲染多么炫酷，而在于能否降低使用门槛、深化数据融合、适应多样场景、优化整体投入。<br/>我们追求的，是让运维专家能更专注于他们的专业判断，而不是被技术工具所束缚。通过一系列贴合业务场景的功能与技巧，将数字孪生真正打造成一个人人可用、时时可看、数据可联、决策可依的智能运维新基座。</p>]]></description></item><item>    <title><![CDATA[怎么开始汽车产业链数智化转型？ 月下水光]]></title>    <link>https://segmentfault.com/a/1190000047446452</link>    <guid>https://segmentfault.com/a/1190000047446452</guid>    <pubDate>2025-12-03 16:05:25</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在人类工业文明的演进中，汽车工业一直是国家制造业综合实力的集中体现。它不仅是一个技术密集型的集成平台，更是创新理念与先进制造模式融合的关键战场。如今，随着“电动化、智能化、网联化、共享化”的深層融合，汽车产业正迎来第三次重大变革——智能化生产。而这一变革的核心驱动力，便是以人工智能（AI）为核心的数智化转型。从福特的T型车流水线到丰田的精益生产，再到智能化生产的崭新时代，汽车产业链的数智化进程不仅改变了制造方式，也正在重新定义产业未来。<br/>无人驾驶、智能座舱、动态排产、柔性生产是这场变革的典型标志，而其背后的关键支撑，则是AI技术的全面赋能。自2025年起，数智化已从单纯的选项转变为汽车产业竞争中的必修课，尤其在全球制造业加速升级的背景下，汽车产业链数智化扮演着越来越重要的角色。谁能率先在这一领域实现突破，谁就能在全球价值链的顶端占据先机。<br/>供应链的协同与高效一直被视为汽车制造领域的瓶颈，传统模式下的信息滞后与制造流程脱节问题，往往导致资源浪费与生产效率低下。广域铭岛作为汽车产业链数智化转型的领军企业，推出的“工厂大脑”系统正在打破这种局面。通过整合主机厂、供应商、物流等多方数据节点，广域铭岛的系统实现了供应链全生命周期的实时监控与优化，极大地提升了供应链弹性与响应速度。<br/>例如，在与深圳航盛集团的合作中，广域铭岛帮助后者搭建了一套高协同性、智能化的供应链管控体系，从数字基座建设到供应商动态管理，逐步形成了端到端的闭环能力。更为重要的是，广域铭岛还在第二阶段规划中引入工业AI智能体，对企业全经营流程进行AI预测与优化，从而实现“零缺陷”的品质目标。这种技术赋能，不仅降低了企业的运营成本，还为汽车品牌全球化发展注入了新活力。<br/>对于广域铭岛而言，推动汽车产业链的数智化并非终点，而是构筑未来制造业基础的起点。其Geega工业互联网平台基于工业AI和物联网技术，打造了一套完整的智能制造解决方案。从原材料采购到整车下线，广域铭岛不仅提供了自动化执行工具，还引导企业将制造经验与AI算法深度融合。<br/>其“智能预检+系统”通过动态追踪与智能预警，帮助企业在生产环节实时洞察数据；在冲压工艺中，其模具智能管理系统将设备数据转化为优化算法，预判故障并降低停机时间；在仓储物流中，基于数字孪生的AGV调度系统更是让物料管理达到了前所未有的精准与高效。<br/>广域铭岛这种场景穿透式的技术能力，解决了传统AI大模型应用于工厂场景的适配难题——将老师傅的经验转化为可迭代的数字知识，为企业从“经验驱动”走向“数据智能驱动”提供了切实可行的路径。在实际工业落地中，某装配厂应用其智慧物流系统后，物流效率实现显著提升，呆滞物料率下降，整体交付能力提高30%。这正是汽车产业链数智化转型的缩影，也是广域铭岛帮助企业跨越转型之路阻碍的有力证明。<br/>当传统制造模式面对新型市场需求时，产业链上下游的协同程度在数智化模型下实现了前所未有的提升。广域铭岛在峰会上展示了其最新的“汽车数字化工厂”理念，要素如协同研发、智能生产、全链路关务管家等，使汽车生产企业能够在统一家策略下有效部署资源，实现敏捷响应与高效管理。<br/>而随着汽车产业链全球化部署的推进，广域铭岛的技术输出能力还覆盖了多个行业，例如新能源电池、家电制造等，展现出其在汽车产业链数智化转型上极强的可复制性与通用性。凭借其成熟的解决方案与落地经验，广域铭岛正逐步成为中国乃至全球制造业数智化的桥梁与推手。</p>]]></description></item><item>    <title><![CDATA[数字孪生：国防航天领域智能指挥决策的“智]]></title>    <link>https://segmentfault.com/a/1190000047446492</link>    <guid>https://segmentfault.com/a/1190000047446492</guid>    <pubDate>2025-12-03 16:04:52</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在国防航天这一关乎国家安全与战略优势的核心领域，运营管理的复杂性与日俱增。从航天发射场的全流程管控，到国防设施的态势感知与应急指挥，再到大型装备的全生命周期运维，系统日益庞杂，数据源空前多元，决策压力巨大。传统的管理模式与信息呈现方式，已难以满足对全局实时、精准、协同洞察的迫切需求。<br/>近年来，一项关键技术——数字孪生，正悄然成为破解这些难题的“金钥匙”。而其中我们发现一款数字孪生平台—孪易IOC，它并非简单的三维可视化，而是一套构建在数据融合、场景构建、智能分析与协同处置之上的完整技术体系，正在国防航天行业的多个关键场景中得到深入应用与验证，展现出颠覆性的价值。</p><h2>一、 应对多源异构数据：构建全域感知的“数据基石”</h2><p>国防航天系统的数据生态极为复杂：万千物联网传感器实时回传装备状态参数，各业务系统数据库记录着任务流程与资源信息，空天地海多维监测网络产生海量遥感与侦察数据，不同制式、不同密级的系统间存在数据壁垒。<br/><strong>核心价值点</strong>：成功的数字孪生实践表明，其首要价值在于构建了一个强大的<strong>统一数据融合引擎</strong>。它能够以标准化、安全可靠的方式，无缝接入并治理这些多源、异构、海量的数据。无论是通过适配MQTT等物联网协议实时采集设备数据，还是通过API与既有业务系统（如ERP、MES、指挥系统）深度对接，亦或是处理来自不同平台的GIS、遥感数据，该体系都能实现数据的清洗、关联与实时同步。<br/><strong>案例印证</strong>：在某大型航天发射场，通过部署数字孪生平台，成功将发射塔架数以万计的传感器数据、气象监测数据、任务规划数据与安防监控视频流统一集成。这使得在虚拟孪生场景中，每一个阀门、每一根线缆的状态都与物理世界实时对应，为发射前全系统健康状态评估提供了唯一、可信的“数据真相源”，奠定了精准决策的基石。</p><h2>二、 构建业务化三维场景：从“看见”到“洞见”的关键跃升</h2><p>对于国防航天而言，许多业务本质与空间位置、设备形态、环境态势强相关。传统的二维图表或分散的视频画面，难以直观呈现全局关联与空间逻辑。<br/><strong>核心价值点</strong>：数字孪生提供了从宏观战场环境、基地全域，到微观车间、单台装备的多层次、高保真、可交互的三维可视化能力。平台能够融合倾斜摄影、GIS地图、高精度BIM与装备三维模型，构建起与真实世界1:1映射的虚拟空间。更重要的是，场景中的每一个“孪生体”（如卫星、发射车、厂房、管线）都不是静态模型，而是与后台实时业务数据绑定的动态实体。<br/><strong>案例印证</strong>：在某国防重点设施安全管控项目中，利用数字孪生技术构建了涵盖周边地形、建筑结构、安防设备（周界、摄像头、门禁）的完整三维场景。运维人员不仅能“俯瞰”全局，还能快速定位任一报警点，并立刻在三维场景中调取该点关联的实时视频、设备状态、巡检记录和处置预案，实现了从被动接收告警信息到主动、直观、关联性洞察的根本转变。<br/><img width="640" height="314" referrerpolicy="no-referrer" src="/img/bVdmQxT" alt="" title=""/></p><h2>三、 赋能深度分析与协同处置：驱动决策闭环的“智慧内核”</h2><p>可视化是手段，决策支持才是目的。数字孪生的真正威力，在于利用空间上下文与融合数据，进行过去难以实现的深度分析与模拟推演。<br/><strong>核心价值点</strong>：成熟的数字孪生体系集成了丰富的专业分析工具与协同指挥模块。例如：<br/><strong>空间分析</strong>：进行通视分析以优化监测点位布局，模拟电磁环境对通信的影响，演练疏散路径与物资投送路线。<br/><strong>模拟仿真</strong>：对航天器在轨故障进行数字复现与处置推演，对极端天气下基地运行进行韧性评估。<br/><strong>事件闭环处置</strong>：当发生异常告警（如设备故障、入侵检测）时，系统可自动定位、关联分析、触发预案，并一键启动跨部门的协同任务派发、资源调度与视频会商，形成“监测-预警-决策-处置-复盘”的全流程数字化闭环。<br/><strong>案例印证</strong>：在大型航空航天装备的运维保障中，数字孪生平台接入了装备历史运行数据与实时传感数据。当系统分析发现某部件性能参数出现衰退趋势时，不仅能在三维模型上精准定位，还能自动调用历史相似案例、维修手册，并模拟不同维修方案对整体系统的影响，辅助保障人员制定最优维修策略，极大提升了装备的战备完好性与保障效率。</p><h2>四、 保障系统灵活演进：随业务共同成长的“生命力”</h2><p>国防航天业务需求与技术迭代迅速，一套固化的系统很快会面临挑战。数字孪生体系的长期价值，在于其<strong>高度的可配置性与可扩展性</strong>。<br/><strong>核心价值点</strong>：优秀的平台提供强大的后台配置中心，允许用户根据业务变化，自行调整孪生体属性、数据绑定规则、分析模型与告警阈值。同时，它提供从零代码拖拉拽搭建应用到低代码/API深度开发的全套工具链，使技术团队能够基于平台核心能力，快速构建全新的、高度定制化的业务模块（如专用的任务规划模拟器、特殊的后勤保障看板），保护初始投资，让数字孪生系统真正成为一个能够伴随组织业务持续进化、不断赋能的核心支撑平台。</p><h2>结语</h2><p>综上所述，在国防航天这一尖端领域，数字孪生已超越概念，成为驱动智能化升级的务实引擎。其价值并非单一功能的炫技，而在于构建了一个数据融合力强、场景表现力与交互性高、分析工具专业且紧贴业务、同时具备高度灵活性与生命力的完整技术栈。它正帮助越来越多的单位，将复杂的物理世界与业务运行，转化为可直观感知、可深度分析、可协同指挥的数字镜像，从而在瞬息万变的形势下，赢得决策先机，筑牢安全基石。</p>]]></description></item><item>    <title><![CDATA[汽车工艺优化的未来发展趋势如何？ 月下水]]></title>    <link>https://segmentfault.com/a/1190000047446532</link>    <guid>https://segmentfault.com/a/1190000047446532</guid>    <pubDate>2025-12-03 16:03:54</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在汽车制造业的迭代进程中，工艺优化已成为推动效率提升与质量管理创新的核心驱动力。面向2025年，随着市场需求对高性能、高安全性和环保性的全面升级，传统的生产模式显然无法满足未来的挑战。作为这一领域的先行者，广域铭岛凭借其工业智能体解决方案，正在帮助行业全面实现汽车工艺优化的智能化转型。<br/>汽车冲压工艺作为整车制造的基石，其质量直接影响后续工序的稳定性和整车性能。当前，行业正从依赖人工经验的设备管理向数据驱动的精细化控制迁移。广域铭岛研发的GQCM模具智能管理APP，融合了IIOT技术和AI算法，构建出冲压产线的数字孪生体。通过实时采集冲次数据和设备状态信息，系统能够在实体生产前预演工艺波动，并据此优化参数配置。例如，领克成都工厂仅用该系统便将换模时间压缩至原来40%，同时显著降低材料废品率，这种基于数字主线的工艺优化逻辑改变了传统车间依赖试错经验的生产方式。<br/>焊装工艺的复杂性使其成为工艺优化的重点难点，超3000个焊点的质量稳定性往往是整车质量的关键指标。数据显示，在焊装的智能化质量管控中，广域铭岛开发的GQCM系统通过5G传输网络实时监测20余项焊接参数，其机器学习模型能够以0.02%的虚焊率完成质量预判。而另一项技术突破——二維總成接口分析，则结合设备级与产线级数据完成了焊点力学性能建模，使整个系统的质量控制维度从被动响应进化为主动预防。<br/>总装工艺的智能化优化是确保整车出厂质量的最后一道关口。广域铭岛提供的GQCM拧紧管理系统实现了对每个紧固点质量参数的实时监控，这种高频采集和智能分析往往能在装配完成前精准识别问题。某新能源车企应用该系统后，其拧紧工艺合格率从98%提升至99.98%，整个车间的装配返工率下降了56%。更为惊人的是，该系统建立的电子质量档案功能使某品牌模具的异常处理时间直接缩短了35%。<br/>值得注意的是，广域铭岛不仅在单个工艺上实现突破，更建立了工艺间的协同优化框架。其三大核心技术——数字主线映射、质量闭环算法与模块化部署方案，本质上是对汽车制造系统的深度重塑。这种优化架构通过横向融合和纵向贯通的工业智能体理念，将经验数据转化为可供量化分析的知识图谱。<br/>当前制造业面临碳中和与数字化的双重使命，工艺优化的创新空间正在被技术边际不断拓张。以广域铭岛的实践为例，如果深入解析GQCM系统的数据闭环流程——从更than 3000个传感器采集到数百兆数据流，再通过自学习算法完成工艺调优——就能清楚看到工业智能体的不可替代性。毕竟，当最后一英寸的车身覆盖件、最后一个焊点，都由算法而非人工决定工艺参数时，汽车工艺优化才真正告别了试错时代。<br/>在未来发展路径上，广域铭岛的技术创新已展现出三个显著的趋势：其一是将全套拧紧设备集成接入可解释AI平台，实现系统预警溯源；其二是构建跨工艺的质量建模能力，对共性质量问题进行全局优化；其三则是将所有工业知识标准化封装，产生可搬运的工业APP产品。这些创新正在变被动应对为主动创新，随着技术的深化应用，汽车工艺优化必将在生产柔性化和质量一致性两大维度迎来更大的突破。</p>]]></description></item><item>    <title><![CDATA[NeurIPS 2025 Spotlig]]></title>    <link>https://segmentfault.com/a/1190000047446537</link>    <guid>https://segmentfault.com/a/1190000047446537</guid>    <pubDate>2025-12-03 16:03:20</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>NeurIPS 2025 Spotlight！跨模态重识别革命！东北大学等 MDReID 图像信息智能匹配</h2><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446539" alt=" " title=" "/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446540" alt=" " title=" " loading="lazy"/></p><p>论文标题：<em>MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification</em></p><p>作者团队：东北大学、厦门大学、新加坡国立大学</p><p>发布时间：2025年10月27日</p><p><a href="https://link.segmentfault.com/?enc=FUPEDreH%2B7F9vYlzELME2w%3D%3D.y5f6Qr0qzLpbDqhBvIJUXvbjD3okBJlIvv7r%2FXcA8JCWwelKk6GMa%2BWytotOs88B" rel="nofollow" target="_blank">👉一键直达论文</a></p><p><a href="https://link.segmentfault.com/?enc=g0a%2BT95Aix0LdF6CVo%2BO6A%3D%3D.8GgHP8VsDDIU35Erskiw05QEAGBByuy73tYySXSug%2B7n5CZ8ICsYAt2YK9EfW7B9c4GJ%2FRI7g9I1TpXrv%2FUu826joC6D5LPv7%2BYd7JbcNoJHU0T2UFw8tIvFPDzB5UUTbtmOn%2Bfh4pml6IJg2nMy6e29z%2BuCPKG0TtRzfLNPh3E%3D" rel="nofollow" target="_blank">👉Lab4AI大模型实验室论文阅读</a></p><p>✅Lab4AI平台提供AI导读和AI翻译等工具，辅助论文阅读。</p><p>想象一下：警察想要通过监控录像找到一个嫌疑人。但是，不同监控摄像头的类型可能完全不同——有的拍的是普通的彩色照片（RGB），有的是黑白但能夜间看清的（NIR），还有的是能感知热量的热成像（TIR）。这就带来了一个难题：如果用一张彩色照片（RGB）去热成像（TIR）照片里找人，传统系统可能就失灵了。这篇论文就是为了解决这个“张冠李戴”的实际问题。它提出了一个叫 MDReID​ 的新方法，核心思想非常巧妙，叫做 “分而治之”。</p><h3>⭐核心创新</h3><p>MDReID 认为，任何一张图像包含的信息都可以分成两种：</p><ol><li>通用特征：这是物体最本质的信息。比如一个人的体型、姿势、背包的形状。这些信息无论用什么摄像头拍，都应该差不多。</li><li>专用特征：这是某种摄像头特有的信息。如彩色摄像头能看到的衣服颜色，或者热成像摄像头能看到的身体热量分布。</li></ol><p>MDReID 的核心技术即主动把这两种信息拆分开：</p><ol><li>拆解信息：模型在分析图片时，会同时生成两组“密码”：通用特征和专用特征。对于一张彩色照片，模型既知道它里面包含的通用人体形状，也知道它特有的颜色信息。</li><li>智能对比：当需要比对两张图片时，MDReID 会进行智能匹配。专用特征只和同类型摄像头的专用特征比对（比如颜色和颜色比）。通用特征则可以跨类型自由比对（比如彩色照片里的人的体型，可以和热成像照片里的人的体型比）。</li></ol><p>通过一种特殊的“训练法则”，模型会学习让通用特征尽可能相似，同时让通用特征和专用特征尽可能不同，避免信息冗余。</p>]]></description></item><item>    <title><![CDATA[从零开始：新手下载MT4的完整流程 邱米]]></title>    <link>https://segmentfault.com/a/1190000047446558</link>    <guid>https://segmentfault.com/a/1190000047446558</guid>    <pubDate>2025-12-03 16:02:46</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>对于刚接触外汇或差价合约交易的新手而言，下载并安装MT4交易软件是开启交易的第一步。为确保软件安全性和下载效率，以下将详细介绍通过官方渠道下载MT4的完整流程，帮助新手快速完成安装。</p><p>第一步：精准定位官方下载入口</p><p>打开浏览器，在地址栏输入智能跳转链接： v.3px.cc/EKQG</p><p>该页面采用智能识别技术，可自动匹配您的设备类型（Windows/Mac/iOS/Android），并跳转至适配的加密下载界面。此路径整合官方资源，跳过广告干扰，实测下载速度较常规渠道提升3倍以上，且文件经过双重加密传输，杜绝恶意软件风险。</p><p>第二步：选择适配版本并下载</p><p>电脑端（Windows/Mac）：<br/>进入下载页后，Windows用户点击「下载PC版」按钮，浏览器将自动下载.exe格式安装包；Mac用户则下载.dmg镜像文件。若下载中断，可关闭占用带宽的程序或更换浏览器（推荐使用IDM下载管理器）。</p><p>移动端（iOS/Android）：<br/>iOS用户可扫描页面二维码跳转至App Store，或直接打开App Store搜索“MetaTrader 4”，认准开发者为“MetaQuotes Software Corp.”的官方应用，点击“获取”完成安装；Android用户可扫描二维码跳转至华为/小米应用市场，或手动打开应用商店搜索“MT4”安装。若通过官网下载APK文件，需在安装前开启“允许安装未知来源应用”权限，安装完成后立即关闭该权限以保障安全。</p><p>第三步：完成安装并验证</p><p>电脑端：双击下载文件，按安装向导提示点击“下一步”直至完成，推荐勾选“创建桌面快捷方式”。安装完成后，桌面将生成MT4图标，双击打开软件，若显示实时行情图表、市场报价等界面，即表示安装成功。</p><p>移动端：打开APP后，若能正常浏览实时行情并完成模拟交易下单，则说明安装成功。首次使用时，建议先通过模拟账户熟悉操作界面，再逐步过渡到实盘交易。</p><p>通过以上步骤，新手即可在3分钟内完成MT4的安全下载与基础设置。切记，选择官方渠道下载是保障账户安全的第一步，避免因误点不明链接导致设备感染病毒或信息泄露。</p>]]></description></item><item>    <title><![CDATA[云监控 UModel Explorer：]]></title>    <link>https://segmentfault.com/a/1190000047446560</link>    <guid>https://segmentfault.com/a/1190000047446560</guid>    <pubDate>2025-12-03 16:01:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>作者：隰宗正(霜键)</p><p>点击<a href="https://www.bilibili.com/video/BV1jjUrBYEum/" target="_blank">此处</a>查看相关视频！</p><p>在复杂可观测系统的构建过程中，数据建模往往是“从混沌到秩序”的关键一步。传统的建模方式往往依赖配置文件或代码定义，这种方式虽然精确，但缺乏直观性，难以让团队成员快速理解和协作。UModel Explorer 正是为了改变这一现状而设计。它构建了一个完整的可视化建模环境，让工程师可以像绘制架构图一样，通过拖拽、连线等直观操作来构建可观测数据模型。</p><h2>为什么需要可视化的 UModel 建模</h2><h3>1.1 问题与痛点</h3><p>在当今的云原生和微服务架构下，系统的复杂性呈指数级增长。一个看似简单的用户请求，背后可能流经数十甚至上百个服务组件。这种复杂性带来了可观测领域的巨大挑战：数据孤岛现象严重。指标（Metrics）、追踪（Traces）、日志（Logs）这三大支柱分散在不同的系统中，彼此割裂。当故障发生时，工程师不得不在多个系统之间来回跳转，试图通过人脑将这些碎片化的信息拼凑成完整的故障现场。这个过程不仅效率低下，而且对工程师的经验和系统熟悉度要求极高。</p><p>传统的解决方案试图通过数据建模来解决这个问题，但往往引入了新的痛点。基于代码或 YAML 的建模方式（如 Terraform、Prometheus Operator）虽然功能强大且易于版本控制，但其陡峭的学习曲线和高度的抽象性，使得数据模型变成了少数专家的“私有物品”。业务开发人员难以理解，新入职的 SRE 也需要花费大量时间才能上手。模型与现实系统之间的映射关系不够直观，导致模型更新滞后，最终沦为“僵尸模型”。我们迫切需要一种更直观、更低门槛的方式来定义和管理可观测数据模型。</p><h3>1.2 业界现状</h3><p>业界解决这一问题的思路主要分为两类。一类是“分析时关联”，即在查询和分析阶段，通过特定的关联 ID（如 trace\_id）将不同数据源关联起来。这种方式在特定场景下有效，但它是一种“事后”关联，无法在建模阶段提供全局的、结构化的系统视图。</p><p>另一类是“建模时定义”，通过各种 DSL（领域特定语言）或配置文件来预先定义实体及其关系。这类方案虽然提供了结构化的能力，但其交互体验往往与建模过程本身是脱节的。可视化通常只是建模结果的一种“只读”展示，而不是建模过程的一部分。工程师在文本编辑器中修改复杂的配置文件，然后通过命令行工具应用变更，最后在一个 Web 界面上查看结果。这个“编辑-编译-运行”的循环，在复杂的模型构建过程中显得非常笨拙和低效。</p><h3>1.3 阿里云可观测解决思路：可视化即建模</h3><p>面对上述挑战，可观测开发团队的思路是：将可视化的“终点”前移，让它成为建模的“起点”和“过程”。UModel Explorer 的核心设计理念是“可视化即建模”(Visualization as Modeling)。我们认为，描述一个复杂系统的最佳方式，就是像在白板上画架构图一样，直观地把它呈现出来。</p><p>UModel Explorer 提供了一个交互式的画布，用户不再是面对冰冷的配置文件，而是直接与代表着实体（EntitySet）、指标（MetricSet）等可观测元素的图形化节点进行交互。创建 UModel，就是从工具栏点击新建节点；建立关系，就是用鼠标在两个节点之间画一条线。所有的修改都会实时地在画布上反映出来，提供了一种所见即所得（WYSIWYG）的建模体验。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446562" alt="image" title="image"/></p><p>用户可前往可观测 2.0 控制台进行体验：<a href="https://link.segmentfault.com/?enc=ZjydEjSkXfPKUs7h10LrFw%3D%3D.TIhSNHeoabQzxoLKoeg0ISnWQgQJnQX9okpi6DJ9wttI6rUeeqpNBj6p9l4WGtXR" rel="nofollow" target="_blank">https://cmsnext.console.aliyun.com/next/home</a></p><p>底层支撑这一体验的是统一可观测模型（UModel）。UModel 是我们提出的核心概念，它通过一套标准的 Schema，将指标、日志、追踪、事件等多种可观测数据统一抽象为“实体”和“关系”，从根本上打破了数据孤岛。UModel Explorer 正是这一统一模型的可视化交互界面。</p><h3>1.4 独特优势</h3><p>相比于业界现有方案，UModel Explorer 的核心优势在于其体验的革命性，这种体验带来了效率和协作方式的巨大提升。</p><ul><li>极低的认知门槛：图形化的交互方式符合人类的直觉。无论是经验丰富的架构师，还是刚入职的开发人员，都可以通过画布快速理解系统的可观测模型。这使得模型不再是少数专家的专利，而成为团队共享的知识资产。</li><li>建模与分析一体化：UModel Explorer 不仅仅是一个“画图”工具。画布上的每一个节点都是一个“活”的入口。例如，用户可以直接在 MetricSet 节点上发起指标分析，或者在 EntitySet 节点上查询实体列表。这种将建模、探索、分析无缝集成的设计，打通了从“定义模型”到“使用模型”的最后一公里，实现了真正的“建模即服务”。</li><li>高效的协作平台：可视化的画布成为了团队之间沟通和协作的共同语言。当需要调整监控策略或排查问题时，相关人员可以围绕着同一张“活”的架构图进行讨论，所有的变更意图都清晰可见。分享功能更是让跨团队协作变得轻而易举。</li><li>强大的工程化能力：在直观的交互体验背后，UModel Explorer 具备完整的工程化能力。所有的可视化操作最终都会转化为结构化的 UModel 定义。提交工作流提供了清晰的变更审查（Diff）机制，支持撤销/重做，确保了所有修改都是可追溯、可管理的。</li></ul><p>综上所述，UModel Explorer 并非简单地为数据模型增加了一个可视化层，而是从根本上重构了可观测建模的交互范式。它将复杂的建模过程转变为一种直观、高效且富有创造性的体验，旨在将每一位工程师从繁琐的配置工作中解放出来，更专注于可观测性带来的真正价值。</p><h2>系统概述与入口</h2><p>UModel Explorer 作为可观测 2.0 平台的核心组件，提供了统一的可观测数据建模能力。它支持多种数据模型类型，包括 entity_set（实体集）、metric_set（指标集）、log_set（日志集）、trace_set（追踪集）等，以及它们之间的关联关系。这种统一的建模框架，让原本分散在不同系统中的数据有了统一的管理视角。</p><p>进入系统的方式很简单：进入<a href="https://link.segmentfault.com/?enc=OGS8BbyYoolJ%2BOa5ulmXEg%3D%3D.aqY8WBYGBFxiI%2FA6gG9U0Rb0ZgAXxTzzlwerY5ot5GFImycNkGLLJxrbI78iEyIvbGtCYHdtL3fj7%2BjtBcPQI9PlNHWDKpBqwaUHY5%2F36Ewua84CuVm0i1O7qwRvSUbG1qZgJiXLD6YunyR8iAakrw%3D%3D" rel="nofollow" target="_blank">阿里云云监控 2.0</a> 产品控制台后选择一个Workspace并进入，在“应用中心”找到 UModel Explorer，推荐将其固定到 Workspace App 侧边栏，这样可以在日常工作中快速访问。首次进入时，系统会自动加载当前 Workspace 中的所有 UModel 数据，并以图形化的方式展示在画布上。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446563" alt="image" title="image" loading="lazy"/></p><h2>主界面布局</h2><p>UModel Explorer 的主界面采用了清晰的功能分区设计，这种布局既保证了信息的层次性，又兼顾了操作的便利性。整个界面可以大致分为几个区域：左上角的控制面板、右侧的详情面板、右下角的迷你图，以及右上角的操作工具栏。中央的画布区域占据了最大空间，这是用户进行建模操作的核心区域。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446564" alt="image" title="image" loading="lazy"/></p><h3>3.1 控制面板：全局视图与筛选</h3><p>控制面板位于界面左上角，是管理整个 UModel 集合的中央控制台。面板采用标签页设计，包含概览、筛选、CommonSchema 信息和设置四个页面。用户可以通过点击面板右侧的折叠按钮来收起或展开控制面板，当画布上的操作需要更多空间时，这个功能特别有用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446565" alt="image" title="image" loading="lazy"/></p><h4>3.1.1 概览页：数据总览与导航</h4><p>概览页提供了当前 Workspace 中 UModel 数据的全局统计信息。这些统计信息分为两个维度：节点统计和链接统计。节点统计展示各类 UModel 元素（entity_set、metric_set 等）的数量分布，链接统计则展示了它们之间的关联关系数量。特别需要注意的是，entity_set_link 类型在统计中同时算作节点和链接，因为它既是一个节点实体，又代表了一种关联关系。</p><p>统计信息支持两种模式：全局统计和应用过滤器统计。当用户设置了筛选条件后，可以切换到应用过滤器模式，此时统计信息会只计算符合筛选条件的元素，这让用户能够快速了解当前视图范围内的数据分布。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446566" alt="image" title="image" loading="lazy"/></p><p>概览页还提供了一个重要的导航功能：UModel 列表视图。点击“查看 UModel 列表”按钮，会进入一个表格视图。这个表格不仅展示所有 UModel 元素的通用信息（名称、域、类型等），还根据不同的元素类型展示专属的关键信息。例如，对于 metric_set，会展示其包含的指标数量；对于 entity_set，会展示实体类型等。表格支持多维度的筛选，包括按类型、按域筛选，还支持全文搜索，用户可以在搜索框中输入关键词（支持中文名、英文名或 ID），系统会在所有字段中搜索匹配的内容。</p><p>列表视图的一个重要功能是定位。当用户找到某个 UModel 元素后，点击操作栏中的定位按钮，画布会自动聚焦到该元素，并将其置入聚焦筛选条件。这个功能在数据量很大的场景下特别有用，可以帮助用户快速从一个庞大的模型图中找到目标元素。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446567" alt="image" title="image" loading="lazy"/></p><h4>3.1.2 筛选页：精确数据定位</h4><p>筛选页提供了强大的数据过滤能力，让用户能够从海量的 UModel 数据中快速定位到需要查看或编辑的部分。筛选功能分为四种类型，它们的组合逻辑需要理解清楚才能高效使用。</p><p>前三种筛选类型（节点类型筛选、域筛选、全文查找筛选）作为一组，它们之间的逻辑关系是：同一项内为“或”关系，不同项之间为“且”关系。举个例子，如果用户在节点类型筛选中选择了 entity_set 和 metric_set，在域筛选中选择了 domain1 和 domain2，那么系统会显示所有类型为 entity_set 或 metric_set，且域为 domain1 或 domain2 的元素。全文查找筛选的使用需要注意，输入关键词后需要按回车键才能提交。</p><p>第四种筛选是聚焦筛选，这是一种特殊的筛选模式。当存在聚焦筛选时，其他所有筛选条件都会被忽略，系统只显示聚焦筛选选中的元素。聚焦筛选通常是临时性的，用于快速查看某个特定的子图。用户不管是画布上的节点操作菜单，还是列表视图的定位功能，都可以设置聚焦筛选。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446568" alt="image" title="image" loading="lazy"/></p><p>所有筛选条件修改后，需要点击“应用”按钮才会生效。这个设计让用户可以同时修改多个筛选条件，然后一次性应用，避免了频繁刷新画布带来的性能问题。</p><h4>3.1.3 CommonSchema 信息页：公共模型管理</h4><p>在阿里云可观测业务中，系统内置了一些标准化的 UModel 数据，这些数据作为公共 UModel（CommonSchema）默认存在于 Workspace 中。公共 UModel 数据以引用方式（CommonSchemaRef）配置，在查询时动态生成 UModel 实例，并在元素的 metadata 字段中附加 commonschemainfo 字段作为额外说明。</p><p>CommonSchema 信息页展示了当前 Workspace 使用了哪些 CommonSchema，以及是否存在本地定义与 CommonSchema 的冲突。这种冲突检测非常重要，因为如果本地定义的 UModel 与 CommonSchema 的定义不一致，可能会导致查询或分析时出现问题。当检测到冲突时，系统会清晰地标示出来，用户需要决定是使用 CommonSchema 的版本，还是保留本地的自定义版本。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446569" alt="image" title="image" loading="lazy"/></p><p>需要注意的是，CommonSchema 元素在画布上会显示特殊的标识（云朵图标），它们不支持直接修改。如果用户需要自定义，应该创建本地的 UModel 元素，而不是试图修改 CommonSchema。</p><h4>3.1.4 设置页：显示与性能控制</h4><p>设置页允许用户控制 UModel Explorer 的显示行为和性能参数。这些设置都是会话级别的，不会持久化保存，每次重新打开页面时会恢复到默认值。</p><p>最重要的设置是“强制全量显示”开关。当画布上的节点数量超过一定阈值（默认 50 个）时，系统会在页面底部显示提示，告知用户当前只展示了部分节点。这是出于性能考虑的设计，因为当节点数量很大时，全部渲染会导致严重的卡顿，影响使用体验。如果用户确实需要查看全部节点，可以在设置页打开“强制全量显示”开关。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446570" alt="image" title="image" loading="lazy"/></p><p>但需要注意的是，强制显示大量节点可能会带来明显的性能问题。最佳实践是：先用筛选条件缩小范围，比如使用聚焦筛选只看某个子图，然后再打开全量显示。完成查看后，及时关闭全量显示开关，避免影响后续操作。</p><p>最佳实践：将范围缩小到固定聚焦、筛选后，打开全量显示开关。观察调整后，及时关闭全量显示开关。</p><p>背景样式可以根据需要进行最优的显示，用户根据当前的关注点调整界面，获得最佳的查看体验。</p><p>实体链接显示方式是一种针对于 entity_set_link 的显示模式调整。</p><p>真实视图：entity_set_link 作为独立节点链接相关 data_set，entity_set 与 entity_set 之间的链接直接连线。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446571" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446572" alt="image" title="image" loading="lazy"/></p><p>逻辑视图：entity_set_link 作为实体链接边上节点，entity_set 与 entity_set_link 之间存在虚拟逻辑连接。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446573" alt="image" title="image" loading="lazy"/></p><h3>3.2 右侧详情面板：表单编辑能力</h3><p>右侧详情面板是编辑 UModel 元素的核心区域。当用户点击画布上的任意节点或边时，右侧面板会自动滑出，展示该元素的完整属性表单。</p><p>UModel Explorer 为每种类型的元素都设计了专门的表单 Schema，这些 Schema 不仅包含通用的元数据字段（名称、域、描述等），还包含该类型特有的配置项。例如，metric_set 的表单会展示指标列表和标签定义，entity_set 的表单会展示实体字段和索引配置。每个字段的标题旁都有提示图标，鼠标悬停可以查看详细的字段说明，这对于理解复杂的配置项非常有帮助。</p><p>表单编辑支持实时校验。当用户修改某个字段时，系统会立即进行格式和逻辑校验，如果发现错误，会在表单底部以红色文字提示。这种即时反馈让用户能够快速发现问题，而不需要等到提交时才发现错误。完成修改后，点击“提交”按钮即可保存。注意，这里的提交只是保存到本地状态，真正的持久化需要通过右上角的“提交”功能将变更批量提交到 Workspace。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446574" alt="image" title="image" loading="lazy"/></p><p>对于 CommonSchema 元素，表单会显示为只读模式，用户无法直接修改。这是为了保护公共模型的一致性。如果需要修改，应该创建本地的副本。</p><h4>3.2.1 复杂嵌套表单展示</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446575" alt="image" title="image" loading="lazy"/></p><p>针对复杂对象，UModel Explorer 支持嵌套表单展示和分段式表单校验。</p><h4>3.2.2 Json 格式编辑</h4><p>支持表单视图与 Json 格式编辑切换。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446576" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446577" alt="image" title="image" loading="lazy"/></p><h3>3.3 迷你图：快速导航</h3><p>右下角的迷你图提供了整个画布的缩略视图。当模型图很大时，迷你图特别有用。用户可以通过点击迷你图中的任意位置，快速将主视图移动到对应的区域。迷你图会实时反映主视图的位置，主视图移动时，迷你图中的视口指示器也会同步移动。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446578" alt="image" title="image" loading="lazy"/></p><h3>3.4 操作工具栏：核心功能集合</h3><p>界面右上角的操作工具栏集成了几个最常用的功能。最左侧是操作说明按钮，点击后可以查看快捷键和基本操作提示。</p><p>创建节点按钮是快速建模的入口。点击后会弹出一个对话框，若点击创建新节点，则让用户选择要创建的节点类型（entity_set、metric_set、log_set 等）。创建完成后，新节点会自动出现在画布中央，并且会自动聚焦和选中，用户可以立即开始编辑。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446579" alt="image" title="image" loading="lazy"/></p><p>若点击批量导入 UModel，则支持 Yaml 或 Json 的文件导入，需要注意：</p><ul><li>支持上传规范的 UModel 文件，包含 JSON 和 YAML 格式。</li><li>可批量上传多个文件，每个文件可以包含多个 UModel 对象。</li><li>必须包含完整的 UModel id。系统会自动验证必需字段。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446580" alt="image" title="image" loading="lazy"/></p><p>导入完成后，新节点会自动出现在画布中央，并且会自动聚焦和选中，用户可以继续编辑做调整。</p><p>注意：导入或创建新节点，都需要最终提交（右上角的提交按钮），才能最终在 Workspace 中生效。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446581" alt="image" title="image" loading="lazy"/></p><p>刷新按钮会重新从服务器获取最新的 UModel 数据，并用新数据替换画布上当前的内容。这个功能在网络环境不稳定或怀疑数据有更新时特别有用。需要注意的是，刷新会丢失所有未保存的本地修改，所以刷新前要确保已经通过提交功能保存了重要更改。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446582" alt="image" title="image" loading="lazy"/></p><p>分享功能允许用户将当前视图状态（包括筛选条件、显示配置、视图位置等）生成为一个 URL，用户可以将这个 URL 分享给团队成员。接收者打开链接后，会看到与用户完全相同的视图状态，这对于协作和问题沟通非常方便。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446583" alt="image" title="image" loading="lazy"/></p><p>撤销和重做功能支持对每一步操作进行回退。这对于复杂的建模过程特别有用，当用户发现某次操作不对时，可以立即撤销。系统会记录所有的本地修改操作（包括节点的创建、删除、属性的修改、边的连接等），用户可以多次撤销和重做。</p><p>最后的提交功能是建模流程的关键一步。在画布上进行的所有编辑操作（创建节点、修改属性、删除元素等）都只是保存在本地状态中，不会影响 Workspace 中的数据。只有通过提交功能，才会将变更正式持久化到 Workspace。</p><p>点击提交按钮后，系统会打开提交预览对话框。这个对话框以三个标签页分别展示要删除、要新增、要修改的元素。每种操作都用不同颜色标识：删除用红色（D），新增用绿色（A），修改用黄棕色（M）。对于修改操作，会展示详细的 Diff 视图，让用户能够清楚地看到每个字段的变化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446584" alt="image" title="image" loading="lazy"/></p><p>在提交预览界面，用户可以点击每个标签页右上角的定位按钮，将对应的元素在画布上进行聚焦定位，这样可以再次确认修改是否正确。如果担心在编辑过程中，Workspace 中的数据被其他用户修改，可以点击“同步最新数据”按钮，系统会重新获取最新数据并更新 Diff 视图，这样用户就能看到是否有新的冲突。</p><p>如果所有变更都符合预期，点击“执行变更”按钮，系统会开始批量提交。提交会按照删除、添加、修改的顺序执行，删除操作每次只提交一个元素（最保守的策略），添加和修改操作每次批量提交 10 个元素。如果某个批次执行失败，后续批次会停止执行，并显示详细的错误信息。用户可以根据错误信息修正问题后，重新提交剩余的变更。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446585" alt="image" title="image" loading="lazy"/></p><p>提交过程中，已经成功提交的批次会从预览列表中移除，失败的批次会保留并显示错误信息。即使关闭提交对话框，用户也可以通过操作工具栏再次查看提交状态和错误信息，直到所有变更都成功提交完成。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446586" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446587" alt="image" title="image" loading="lazy"/></p><h2>编辑操作详解</h2><h3>4.1 节点与边的选中</h3><p>在画布上，用户可以通过鼠标点击来选中节点或边。选中的元素会有视觉高亮效果（通常是边框变粗或背景色改变），同时右侧详情面板会显示该元素的属性表单。用户可以通过点击画布的空白区域来取消选中。</p><h3>4.2 键盘快捷键</h3><p>UModel Explorer 支持多个键盘快捷键，熟练掌握可以显著提升操作效率。最常用的是 Delete 或 Backspace 键，用于删除选中的节点或边。删除操作是即时生效的，但同样支持撤销。</p><h3>4.3 可视化连线：直观的关系构建</h3><p>UModel Explorer 最核心的创新之一，是提供了可视化的连线操作，让用户可以通过拖动鼠标来创建节点之间的关联关系。这种交互方式比传统的表单填写方式更加直观，也更符合人类对关系图的认知习惯。</p><p>连线操作都是从节点的右侧操作盘开始的。当用户将鼠标移动到节点的右侧边缘时，会出现一个圆形的操作盘，鼠标变成十字状，表示可以开始连线。按住鼠标左键，然后拖动到目标节点的左侧，松开鼠标，即可创建一条边。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446588" alt="image" title="image" loading="lazy"/></p><p>拖动过程中，用户可以看到一条临时的连线跟随鼠标移动，这让用户能够清楚地看到要创建的连接关系。当用户拖动到某个节点的左侧区域时，目标节点会有视觉提示（比如边框高亮），表示可以连接到该节点。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446589" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446590" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446591" alt="image" title="image" loading="lazy"/></p><p>连线创建后，连接关系使用默认值，通常需要进一步编辑才能满足实际需求。默认的连接可能缺少关键的关联字段配置，或者关联类型不正确。所以建议连线后立即点击该边，在右侧面板中完善配置。</p><h3>4.4 高级连线模式</h3><p>除了直接连接两个节点，系统还提供了两种高级连线模式，用于处理更复杂的场景。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446592" alt="image" title="image" loading="lazy"/></p><p>第一种是批量连接模式。当用户从一个节点开始连线，但拖动到画布空白区域时，会弹出一个“选择或创建链接对象”的虚拟节点对话框。在这个对话框中，用户可以搜索并选择多个已有的节点，一次性创建多条边。这对于需要将一个节点连接到多个节点的情况特别有用。例如，一个 entity_set 可能需要同时连接到多个 metric_set，使用批量连接模式可以一次性完成所有连接。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446593" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446594" alt="image" title="image" loading="lazy"/></p><p>已选中的内容会在底部显示。若选择完毕，点击确认。此时会聚焦到刚刚选择的几条新建边。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446595" alt="image" title="image" loading="lazy"/></p><p>第二种是创建并连接模式。同样是在虚拟节点对话框中，切换到“创建新节点”标签页。用户可以选择要创建的节点类型，填写基本的名称和域信息，然后确认。系统会同时创建新节点和连接边，新节点会自动出现在画布的合适位置，并且已经连接到源节点。这种模式适用于边建模边建立关系的工作流，避免了先创建节点再逐个连接的繁琐过程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446596" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446597" alt="image" title="image" loading="lazy"/></p><p>无论是批量连接还是创建并连接，创建完成后都需要点击对应的边进行详细配置，才能确保关联关系能够正常工作。</p><h3>4.5 节点操作菜单</h3><p>每个节点在鼠标悬停时，左上角都会显示一个操作菜单。这个菜单提供了该节点类型特有的快捷操作。所有节点都至少有一个“聚焦”操作，点击后会将该节点设为聚焦筛选条件，画布会自动调整视图，只显示该节点及其相关的邻居节点。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446598" alt="image" title="image" loading="lazy"/></p><p>不同类型的节点还有各自的特有操作。例如，entity_set 节点可能有“查询实体列表”、“快速查询”等操作，metric_set 节点有“分析”操作，log_set 节点有“跳转日志查询”操作等。这些操作将建模和分析功能无缝集成，让用户可以在建模的同时直接查看和分析数据。</p><h3>4.6 节点级联操作</h3><p>节点的操作菜单中还包含一些级联操作选项，这些选项允许用户对节点及其相关的边进行批量操作。例如，用户可能需要删除一个节点及其所有出边和入边，或者只删除出边而保留入边。这些操作在重构模型时特别有用，可以避免手动逐个删除边的繁琐过程。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446599" alt="image" title="image" loading="lazy"/></p><h2>特化操作：节点类型的高级能力</h2><p>不同类型的 UModel 节点除了基本的建模功能外，还提供了各自特有的高级能力。这些能力将数据查询、分析等功能直接集成到建模环境中，实现了“建模即使用”的理念。</p><h3>5.1 entity_set：实体查询与探索</h3><p>entity_set 节点代表实体数据的集合，比如服务实例、容器、主机等。在 UModel Explorer 中，entity_set 节点提供了两个核心的特化操作：快速查询和实体列表查询。</p><p>快速查询（Usearch）功能让用户能够直接在画布上查询 entity_set 中包含的实体数据。点击操作菜单中的“快速查询”，会打开一个查询面板。在这个面板中，用户可以输入查询条件（基于实体的字段），系统会实时返回匹配的实体列表。查询结果支持进一步筛选和排序，用户可以快速找到感兴趣的实体。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446600" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446601" alt="image" title="image" loading="lazy"/></p><p>实体列表查询功能则会跳转到专门的实体管理页面，提供更完整的实体数据管理和分析能力。这个页面通常包含表格视图、详情视图、关联关系视图等多种展示方式，支持复杂的查询条件和批量操作。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446602" alt="image" title="image" loading="lazy"/></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446603" alt="image" title="image" loading="lazy"/></p><p>这两种查询方式各有优势。快速查询适合快速验证和探索，实体列表查询适合深入的查询和分析。用户根据场景选择合适的方式即可。</p><h3>5.2 metric_set：智能指标分析</h3><p>metric_set 节点代表指标数据的集合，是 UModel Explorer 中最具分析能力的节点类型。点击 metric_set 节点的“分析”按钮，会打开一个大型的分析面板，集成完整的 metric_set Explorer 智能分析功能。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446604" alt="image" title="image" loading="lazy"/></p><p>分析面板占据屏幕 90% 的宽度，提供了充足的空间来展示图表和分析结果。面板顶部集成了时间选择器，用户可以选择要分析的时间范围。面板内部包含了指标概览、下钻分析、智能分组、智能下钻等多个分析标签页。</p><p>在指标概览模式下，metric_set 中定义的所有指标都会以卡片形式展示，每个卡片包含时序曲线预览。系统支持两种视图：普通视图按照黄金指标和基础指标分类展示，异常视图则运行异常检测算法，按照异常评分排序，将最有问题的指标优先展示。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446605" alt="image" title="image" loading="lazy"/></p><p>下钻分析功能允许用户从整体到局部逐层深入。选择一个维度（比如服务名、地域、实例 ID 等），系统会按该维度分组展示指标曲线。用户可以继续选择下一层维度，形成多级下钻的分析路径。系统还支持 ALL 模式下钻，自动分析所有维度，找出数据分布差异最大的维度，这对于不确定从哪个角度分析时特别有帮助。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446606" alt="image" title="image" loading="lazy"/></p><p>智能分组功能基于时序聚类算法，将所有时间序列按照形态相似度进行聚类。这让用户能够发现数据中的模式或群组，比如高负载、中负载、低负载三类实例，便于进行容量规划或资源优化。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446607" alt="image" title="image" loading="lazy"/></p><p>智能下钻是最具技术含量的功能。用户先在时间轴上框选一个异常时间段，系统会运行根因定位算法，自动分析所有维度组合，找出对异常贡献最大的维度取值。结果会按置信度排序展示，每个结果包含根因模式、置信度、影响曲线和对比基线。这个功能将原本需要人工尝试十几种维度组合的工作，缩短到几秒钟就能完成。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446608" alt="image" title="image" loading="lazy"/></p><p>除了核心分析功能，metric_set 分析还支持多指标对比、时间对比（环比分析）、准星联动、查询语句查看等高级功能。这些功能的设计都围绕一个核心目标：让指标分析从“人工排查”转变为“算法驱动”，大幅提升问题定位的效率。</p><p>关于 metric_set 分析的详细使用方法，可以参考专门的 metric_set explorer 使用文档。这里只是简要说明其在 UModel Explorer 中的入口和基本能力。</p><h2>最佳实践与注意事项</h2><p>在使用 UModel Explorer 进行建模时，有一些最佳实践值得遵循。</p><p>首先，建议先用筛选功能缩小范围，只关注当前需要编辑的部分。这不仅能提升性能，也能减少视觉干扰，让用户更专注于当前任务。</p><p>其次，合理使用聚焦筛选。当用户需要查看某个节点的局部视图时，使用聚焦筛选比全局筛选更高效。聚焦筛选会自动包含相关的邻居节点，形成完整的子图，这对于理解局部的关联关系特别有用。</p><p>第三，定期提交变更。虽然系统支持撤销重做，但这些操作只在当前会话中有效。如果用户进行了大量修改，建议分阶段提交，避免因为浏览器崩溃或其他意外导致工作丢失。提交前一定要仔细查看 Diff 预览，确保修改符合预期。</p><p>第四，注意 CommonSchema 与本地定义的区分。CommonSchema 是系统提供的标准定义，不应该直接修改。如果确实需要自定义，应该创建本地的 UModel 元素。当发现冲突时，要及时处理，避免影响后续使用。</p><p>第五，合理使用批量操作。无论是批量创建连接、批量删除元素，还是批量提交变更，都要确认影响范围。特别是级联删除操作，要确保不会误删重要的关联关系。</p><p>最后，善用分享功能进行协作。当用户需要与团队成员讨论某个模型时，可以使用分享功能生成 URL，这样对方能够看到完全相同的视图状态，提高了沟通效率。</p><h2>总结</h2><p>UModel Explorer 通过可视化的方式重新定义了可观测数据建模的体验。它将原本需要编写配置文件或代码的建模过程，转变为直观的图形操作。同时，它还将查询、分析等功能无缝集成到建模环境中，实现了从建模到使用的一体化体验。</p><p>对于正在构建可观测系统的团队，UModel Explorer 不仅能提升建模效率，更能帮助团队成员更好地理解和协作。随着系统的不断完善，相信它会成为云原生可观测性领域的重要基础设施。</p>]]></description></item><item>    <title><![CDATA[活动预告｜Oracle 到 Postgr]]></title>    <link>https://segmentfault.com/a/1190000047446665</link>    <guid>https://segmentfault.com/a/1190000047446665</guid>    <pubDate>2025-12-03 16:01:24</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在全球数据库架构向 PostgreSQL 转型的浪潮中，如何低成本解决 Oracle 存量业务的兼容性难题？IvorySQL 开源社区特邀欧洲技术专家，结合最新发布的 IvorySQL 5.0 版本，通过实战演示为您剖析异构数据库迁移的破局之道。</p><h2>📅 会议概况</h2><ul><li>主题： Oracle 迁移挑战与 IvorySQL 解决方案探讨</li><li>时间： 2025年12月12日 (周五) 15:00 - 16:00 (UTC+8)</li><li>形式： Zoom 在线会议 (需注册账号)</li><li>语言： 英语 / English (本次为国际化技术交流)</li></ul><h2>💡 精彩议程（60分钟）</h2><ol><li>IvorySQL 5.0 版本关键特性简述（5分钟）</li></ol><p>概要介绍 11月25日发布的 IvorySQL 5.0 版本。</p><p>重点说明基于 PostgreSQL 新内核的升级，以及在 Oracle 兼容性层面（包括新增的兼容函数、系统包、数据类型支持等）的技术改进点。</p><ol start="2"><li>特邀分享：欧洲市场 Oracle 迁移挑战透视（15分钟）</li></ol><p>嘉宾：法国 Data Bene 代表</p><p>分享欧洲企业在执行“去O”战略时的特定痛点，如数据合规要求、特定行业遗留系统的复杂性，以及对平滑迁移工具的迫切需求。</p><ol start="3"><li>迁移技术痛点与实战演示（20分钟）</li></ol><p>聚焦 PL/SQL 存储过程重构、Oracle 专有 SQL 方言适配、驱动兼容性等典型高难度迁移场景。</p><p>基于 5.0 环境进行实时对比演示，直观展示 IvorySQL 兼容层在降低应用代码改造量、缩短迁移周期方面的实际效果。</p><ol start="4"><li>用户需求研讨与路线图规划（15分钟）</li></ol><p>通过定向投票与互动问答，收集参会者在实际项目中的具体技术需求与痛点。</p><p>探讨社区后续功能开发优先级，确保产品的演进方向贴合企业一线生产场景。</p><ol start="5"><li>Q&amp;A 与总结（5分钟）</li></ol><h2>👥 适合人群</h2><p>正在规划或实施数据库“去O”项目的企业 DBA、数据库架构师、后端技术负责人及生态合作伙伴。</p><h2>🚀 立即报名</h2><p>注册参会：<a href="https://link.segmentfault.com/?enc=xyVkRyOEIC0%2BidT3pDADKA%3D%3D.15vvS8JxeeZz422L%2BS76YDepkBGENFwqbUdFEOZcLz0%3D" rel="nofollow" target="_blank">https://jsj.top/f/idJjBf</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446667" alt="二维码.png" title="二维码.png"/></p><h2>⚠️ 注意事项</h2><p>本活动为闭门技术分享，全程使用英文交流。报名后请务必注册/登录 Zoom 账号以顺利入会。欢迎各位感兴趣的社区伙伴报名参与！</p>]]></description></item><item>    <title><![CDATA[MyBatis 进阶治理点——缓存、副作]]></title>    <link>https://segmentfault.com/a/1190000047445951</link>    <guid>https://segmentfault.com/a/1190000047445951</guid>    <pubDate>2025-12-03 15:08:48</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>深入 MyBatis 内核，在性能提升与数据一致性之间寻找精妙平衡</blockquote><p>在掌握 MyBatis 基础映射与动态 SQL 后，进阶治理成为保证生产环境稳定性与性能的关键。本文将深入分析缓存机制、副作用控制、拦截器应用与批处理优化等高级主题，帮助开发者构建高可用、易维护的数据访问层。</p><h2>1 缓存机制深度治理</h2><h3>1.1 二级缓存的一致性挑战</h3><p>MyBatis 的二级缓存基于 Mapper 命名空间设计，多个 SqlSession 可共享同一缓存区域，这一机制在提升性能的同时也带来了严重的一致性挑战。</p><p><strong>跨命名空间更新导致的数据不一致</strong>是典型问题。当 OrderMapper 缓存了包含用户信息的订单数据，而 UserMapper 更新了用户信息时，OrderMapper 的缓存不会自动失效，导致脏读。解决方案是通过引用关联让相关 Mapper 共享缓存刷新机制：</p><pre><code>&lt;!-- OrderMapper.xml --&gt;
&lt;cache/&gt;
&lt;!-- 引用UserMapper的缓存 --&gt;
&lt;cache-ref namespace="com.example.mapper.UserMapper"/&gt;</code></pre><p><strong>分布式环境下的缓存同步</strong>是另一重要问题。默认的基于内存的二级缓存在集群环境下会导致各节点数据不一致。集成 Redis 等分布式缓存是可行方案：</p><pre><code>&lt;!-- 配置Redis作为二级缓存 --&gt;
&lt;cache type="org.mybatis.caches.redis.RedisCache"
       eviction="LRU"
       flushInterval="300000"
       size="1024"/&gt;</code></pre><h3>1.2 细粒度缓存控制策略</h3><p>合理的缓存控制需要在不同粒度上制定策略。<strong>语句级缓存控制</strong>允许针对特定查询调整缓存行为：</p><pre><code>&lt;select id="selectUser" parameterType="int" resultType="User" 
        useCache="true" flushCache="false"&gt;
    SELECT * FROM users WHERE id = #{id}
&lt;/select&gt;

&lt;insert id="insertUser" parameterType="User" flushCache="true"&gt;
    INSERT INTO users(name, email) VALUES(#{name}, #{email})
&lt;/insert&gt;</code></pre><p><strong>缓存回收策略配置</strong>对长期运行的系统至关重要。LRU（最近最少使用）策略适合查询分布均匀的场景，而 FIFO（先进先出）更适合时间敏感型数据：</p><pre><code>&lt;cache eviction="FIFO" flushInterval="60000" size="512" readOnly="true"/&gt;</code></pre><h2>2 副作用识别与控制策略</h2><h3>2.1 一级缓存的副作用与治理</h3><p>MyBatis 的一级缓存虽然提升了会话内查询性能，但也引入了诸多副作用。<strong>长时间会话中的脏读</strong>发生在 SqlSession 生命周期内，其他事务已提交的更改对当前会话不可见。</p><p>治理方案包括​<strong>使用 STATEMENT 级别缓存</strong>​，使每次查询后清空缓存：</p><pre><code># application.yml
mybatis:
  configuration:
    local-cache-scope: statement</code></pre><p><strong>批量处理中的错误累积</strong>是另一常见问题。在循环中重复查询相同数据时，一级缓存可能返回过期数据。通过 <code>flushCache</code> 选项强制刷新可以解决：</p><pre><code>@Options(flushCache = Options.FlushCachePolicy.TRUE)
@Select("SELECT id FROM orders WHERE status = 'pending' LIMIT 1")
Integer findNextPendingOrder();</code></pre><h3>2.2 二级缓存的副作用防控</h3><p>二级缓存的作用范围更广，其副作用影响也更严重。<strong>多表关联查询的缓存失效</strong>问题需要通过精细的缓存引用管理来解决。</p><p><strong>缓存击穿与雪崩防护</strong>对高并发系统至关重要。针对缓存击穿，实现互斥锁控制：</p><pre><code>public class CacheMutexLock {
    private static final ConcurrentHashMap&lt;String, Lock&gt; LOCKS = new ConcurrentHashMap&lt;&gt;();
    
    public static &lt;T&gt; T executeWithLock(String key, Supplier&lt;T&gt; supplier) {
        Lock lock = LOCKS.computeIfAbsent(key, k -&gt; new ReentrantLock());
        lock.lock();
        try {
            return supplier.get();
        } finally {
            lock.unlock();
            LOCKS.remove(key);
        }
    }
}</code></pre><p>针对缓存雪崩，采用合理的过期时间分散策略：</p><pre><code>&lt;cache eviction="LRU" flushInterval="300000" size="1024" 
       randomExpiration="true" baseExpiration="300000"/&gt;</code></pre><h2>3 拦截器高级应用与风险控制</h2><h3>3.1 拦截器在数据安全中的应用</h3><p>MyBatis 拦截器提供了在 SQL 执行各阶段插入自定义逻辑的能力。<strong>敏感数据自动加解密</strong>通过 ParameterHandler 和 ResultHandler 拦截器实现：</p><pre><code>@Intercepts({
    @Signature(type = ParameterHandler.class, method = "setParameters", 
               args = {PreparedStatement.class}),
    @Signature(type = ResultHandler.class, method = "handleResultSets", 
               args = {Statement.class})
})
@Component
public class DataSecurityInterceptor implements Interceptor {
    
    private final EncryptionService encryptionService;
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        if (invocation.getTarget() instanceof ParameterHandler) {
            // 参数加密逻辑
            return encryptParameters(invocation);
        } else {
            // 结果集解密逻辑
            return decryptResultSets(invocation);
        }
    }
}</code></pre><p><strong>数据权限过滤</strong>通过 StatementHandler 拦截器自动添加权限条件：</p><pre><code>@Intercepts({
    @Signature(type = StatementHandler.class, method = "prepare", 
               args = {Connection.class, Integer.class})
})
public class DataAuthInterceptor implements Interceptor {
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        StatementHandler handler = (StatementHandler) invocation.getTarget();
        String originalSql = getOriginalSql(handler);
        
        if (needDataAuth(originalSql)) {
            String authCondition = buildAuthCondition();
            String newSql = appendCondition(originalSql, authCondition);
            setSql(handler, newSql);
        }
        
        return invocation.proceed();
    }
}</code></pre><h3>3.2 拦截器的性能影响与稳定性风险</h3><p>拦截器虽然强大，但不当使用会带来严重性能问题和稳定性风险。<strong>拦截器链过长</strong>会导致执行效率显著下降。监控拦截器执行时间至关重要：</p><pre><code>@Override
public Object intercept(Invocation invocation) throws Throwable {
    long startTime = System.currentTimeMillis();
    try {
        return invocation.proceed();
    } finally {
        long duration = System.currentTimeMillis() - startTime;
        if (duration &gt; SLOW_QUERY_THRESHOLD) {
            log.warn("Interceptor slow query: {}ms, method: {}", 
                     duration, invocation.getMethod().getName());
        }
    }
}</code></pre><p><strong>递归调用陷阱</strong>发生在拦截器修改的参数再次触发同一拦截器时。通过状态标记防止递归：</p><pre><code>private static final ThreadLocal&lt;Boolean&gt; PROCESSING = ThreadLocal.withInitial(() -&gt; false);

@Override
public Object intercept(Invocation invocation) throws Throwable {
    if (PROCESSING.get()) {
        return invocation.proceed(); // 避免递归
    }
    
    PROCESSING.set(true);
    try {
        // 拦截器逻辑
        return processInvocation(invocation);
    } finally {
        PROCESSING.set(false);
    }
}</code></pre><h2>4 批处理性能优化</h2><h3>4.1 批量操作的内存优化</h3><p>大批量数据操作时，内存管理和事务控制是关键优化点。<strong>分批处理</strong>避免内存溢出：</p><pre><code>public void batchInsertUsers(List&lt;User&gt; users) {
    SqlSession sqlSession = sqlSessionFactory.openSession(ExecutorType.BATCH);
    try {
        UserMapper mapper = sqlSession.getMapper(UserMapper.class);
        int batchSize = 1000;
        int count = 0;
        
        for (User user : users) {
            mapper.insertUser(user);
            count++;
            
            if (count % batchSize == 0) {
                sqlSession.commit();
                sqlSession.clearCache(); // 避免缓存堆积
            }
        }
        sqlSession.commit();
    } finally {
        sqlSession.close();
    }
}</code></pre><p><strong>流式查询</strong>优化大数据量读取内存占用：</p><pre><code>@Select("SELECT * FROM large_table WHERE condition = #{condition}")
@Options(resultSetType = ResultSetType.FORWARD_ONLY, fetchSize = 1000)
@ResultType(User.class)
void streamLargeData(@Param("condition") String condition, ResultHandler&lt;User&gt; handler);</code></pre><h3>4.2 批量操作的异常处理与重试</h3><p>批量操作中的异常需要特殊处理以保证数据一致性。<strong>部分失败补偿机制</strong>确保数据完整性：</p><pre><code>public class BatchOperationManager {
    
    public void safeBatchInsert(List&lt;Data&gt; dataList) {
        int retryCount = 0;
        while (retryCount &lt; MAX_RETRY) {
            try {
                doBatchInsert(dataList);
                break; // 成功则退出重试
            } catch (BatchException e) {
                retryCount++;
                if (retryCount &gt;= MAX_RETRY) {
                    log.error("Batch insert failed after {} retries", MAX_RETRY);
                    throw e;
                }
                handlePartialFailure(e, dataList);
            }
        }
    }
    
    private void handlePartialFailure(BatchException e, List&lt;Data&gt; dataList) {
        // 识别失败记录并重试
        List&lt;Data&gt; failedRecords = identifyFailedRecords(e, dataList);
        if (!failedRecords.isEmpty()) {
            doBatchInsert(failedRecords);
        }
    }
}</code></pre><h2>5 监控与诊断体系建立</h2><h3>5.1 性能指标采集与分析</h3><p>建立完善的监控体系是识别和解决性能问题的前提。<strong>关键性能指标</strong>应包括：</p><ul><li>​<strong>缓存命中率</strong>​：一级缓存和二级缓存的命中比例</li><li>​<strong>SQL 执行时间</strong>​：区分缓存命中与数据库查询的时间</li><li>​<strong>批处理吞吐量</strong>​：单位时间内处理的记录数</li><li>​<strong>连接等待时间</strong>​：获取数据库连接的平均等待时间</li></ul><pre><code>@Component
public class MyBatisMetricsCollector {
    
    private final MeterRegistry meterRegistry;
    
    public void recordQueryExecution(String statement, long duration, boolean fromCache) {
        meterRegistry.timer("mybatis.query.execution")
                    .tags("statement", statement, "cached", String.valueOf(fromCache))
                    .record(duration, TimeUnit.MILLISECONDS);
    }
    
    public void recordCacheHit(String cacheLevel, boolean hit) {
        meterRegistry.counter("mybatis.cache.access")
                    .tags("level", cacheLevel, "hit", String.valueOf(hit))
                    .increment();
    }
}</code></pre><h3>5.2 日志与诊断信息增强</h3><p>详细的日志记录是诊断复杂问题的基础。<strong>结构化日志</strong>提供可分析的诊断信息：</p><pre><code>&lt;!-- logback-spring.xml --&gt;
&lt;logger name="com.example.mapper" level="DEBUG" additivity="false"&gt;
    &lt;appender-ref ref="MYBATIS_JSON_APPENDER"/&gt;
&lt;/logger&gt;

&lt;appender name="MYBATIS_JSON_APPENDER" class="ch.qos.logback.core.ConsoleAppender"&gt;
    &lt;encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder"&gt;
        &lt;providers&gt;
            &lt;timestamp/&gt;
            &lt;logLevel/&gt;
            &lt;loggerName/&gt;
            &lt;message/&gt;
            &lt;mdc/&gt;
        &lt;/providers&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;</code></pre><p><strong>慢查询监控</strong>帮助识别性能瓶颈：</p><pre><code>@Intercepts(@Signature(type = Executor.class, method = "query", 
           args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class}))
public class SlowQueryInterceptor implements Interceptor {
    
    private static final long SLOW_QUERY_THRESHOLD = 1000; // 1秒
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        long start = System.currentTimeMillis();
        try {
            return invocation.proceed();
        } finally {
            long duration = System.currentTimeMillis() - start;
            if (duration &gt; SLOW_QUERY_THRESHOLD) {
                Object[] args = invocation.getArgs();
                MappedStatement ms = (MappedStatement) args[0];
                log.warn("Slow query detected: {}ms, statement: {}", 
                         duration, ms.getId());
            }
        }
    }
}</code></pre><h2>6 综合治理策略与最佳实践</h2><h3>6.1 环境特定的配置策略</h3><p>不同环境需要不同的治理策略。<strong>开发环境</strong>应注重可调试性，开启完整 SQL 日志；<strong>测试环境</strong>需要模拟生产环境配置，验证性能；<strong>生产环境</strong>则以稳定性和性能为优先。</p><p>​<strong>多环境配置示例</strong>​：</p><pre><code># application-dev.yml
mybatis:
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl
    cache-enabled: false

# application-prod.yml  
mybatis:
  configuration:
    log-impl: org.apache.ibatis.logging.slf4j.Slf4jImpl
    cache-enabled: true
    local-cache-scope: statement</code></pre><h3>6.2 治理决策框架</h3><p>建立系统的治理决策流程，确保架构决策的可追溯性。<strong>决策记录表</strong>帮助团队统一治理标准：</p><table><thead><tr><th><strong>治理领域</strong>​</th><th><strong>决策选项</strong>​</th><th><strong>适用场景</strong>​</th><th><strong>风险提示</strong>​</th></tr></thead><tbody><tr><td><strong>缓存策略</strong>​</td><td>本地缓存</td><td>单实例部署，数据量小</td><td>集群环境不一致</td></tr><tr><td> </td><td>分布式缓存</td><td>集群部署，数据一致性要求高</td><td>网络开销增加</td></tr><tr><td><strong>批处理提交</strong>​</td><td>自动提交</td><td>内存敏感场景</td><td>部分失败难恢复</td></tr><tr><td> </td><td>手动提交</td><td>数据一致性优先</td><td>内存占用较高</td></tr></tbody></table><h2>总结</h2><p>MyBatis 进阶治理需要在性能、一致性和可维护性之间寻找精细平衡。缓存机制能显著提升性能，但必须建立完善的失效策略防止脏读；拦截器提供强大扩展能力，但需防范性能损耗和递归陷阱；批处理优化吞吐量，但要关注内存使用和错误恢复。</p><p>有效的治理不是一次性任务，而是需要持续监控、评估和调整的过程。建立完善的指标采集、日志记录和告警机制，才能确保数据访问层长期稳定运行。</p><hr/><p><strong>📚 下篇预告</strong>​</p><p>《JPA/Hibernate 选择指南——实体关系维护、懒加载与 N+1 问题的权衡》—— 我们将深入探讨：</p><ul><li>⚖️ ​<strong>ORM 框架选型</strong>​：JPA 与 Hibernate 的适用场景对比分析</li><li>🔗 ​<strong>实体关系映射</strong>​：一对一、一对多、多对多关系的维护策略</li><li>⚡ ​<strong>懒加载优化</strong>​：关联加载时机的性能影响与配置方案</li><li>🚀 ​<strong>N+1 问题解决</strong>​：识别、预防与优化查询性能瓶颈</li><li>📊 ​<strong>缓存机制对比</strong>​：JPA 缓存与 MyBatis 缓存的异同分析</li></ul><p><strong>​点击关注，掌握 JPA/Hibernate 性能优化的核心技术！​</strong>​</p><blockquote><p>​<strong>今日行动建议</strong>​：</p><ol><li>检查现有项目中二级缓存配置，评估数据一致性风险</li><li>分析慢查询日志，识别需要拦截器优化的 SQL 模式</li><li>为批处理操作添加监控指标，建立性能基线</li><li>制定缓存失效策略评审机制，确保数据一致性</li></ol></blockquote>]]></description></item><item>    <title><![CDATA[AgentScope 拥抱函数计算 FC]]></title>    <link>https://segmentfault.com/a/1190000047446037</link>    <guid>https://segmentfault.com/a/1190000047446037</guid>    <pubDate>2025-12-03 15:07:58</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>在 AI Agent 应用加速落地的今天，开发者和企业普遍面临三大核心痛点：<strong>部署成本高、运维复杂度高、资源利用率低</strong>。为应对这些挑战，AI Agent 与云原生、Serverless 架构的深度融合正成为行业新趋势。我们很高兴地宣布，AgentScope 正式集成基于阿里云函数计算（Function Compute, FC）的全新 Serverless 运行时，为多智能体应用提供“按需启动、毫秒弹性、零运维”的新一代运行底座。</p><h2>AgentScope 是什么？</h2><p><a href="https://link.segmentfault.com/?enc=3Y8FaxHkvfaN3gghN4FuLA%3D%3D.2SB6wFOxNt4wPmKTHwGYIqz%2BKHqyyX9UmLfCkH50YU1XqARUJ4k5wYBgZhh%2FXIM%2F" rel="nofollow" target="_blank">AgentScope</a> 是一个开源的多智能体应用开发框架，面向构建可观察、可控制、可扩展的 AI 智能体系统。其核心设计原则是<strong>对开发者完全透明</strong>：所有提示工程、模型调用、智能体行为及工作流编排均显式暴露，避免隐式逻辑或深度封装。</p><p>该框架拥有以下特性：</p><ul><li><strong>透明性优先</strong>：所有内部状态、消息传递路径、工具调用链路和模型交互过程均可追踪与审计，确保行为可解释、可调试。</li><li><strong>实时介入</strong>：实现ReAct智能体，原生支持任务执行过程中的实时中断与自定义中断处理逻辑，允许用户随时中断智能体的回复，介入智能体的执行，适用于需要人工干预或动态策略调整的场景。</li><li><strong>增强智能能力</strong>：提供统一的工具管理接口、长期记忆控制机制以及智能化 RAG（检索增强生成）支持，提升智能体的上下文感知与知识利用能力。</li><li><strong>模型无关架构</strong>：抽象统一的模型接入层允许同一套智能体逻辑无缝切换不同大语言模型（如 GPT、Claude、通义千问、Llama 系列等），降低模型迁移成本。</li><li><strong>模块化“乐高式”设计</strong>：智能体、工具、提示模板、记忆模块、工作流节点等组件高度解耦，支持独立开发、组合复用与灵活替换。</li><li><strong>原生多智能体支持</strong>：采用显式消息传递机制与声明式工作流编排，明确表达智能体间的协作关系，避免隐式调度带来的不可控性。</li><li><strong>高度可定制</strong>：支持对工具链、提示策略、通信协议、第三方库集成及可视化界面进行深度定制，适配从原型验证到生产部署的全周期需求。</li></ul><p>AgentScope 旨在为开发者提供一个既具备工程严谨性，又保持足够灵活性的智能体开发基础设施，推动多智能体系统从实验走向规模化落地。自开源以来，AgentScope 已获得社区广泛认可，GitHub Star 数突破 <strong>14,000+。</strong></p><h2>当前Agent运行时的挑战</h2><p><a href="https://link.segmentfault.com/?enc=c0yKbOhBva2PhUNZ73rvbA%3D%3D.huHuQdUUHJj1LMWnrmTma8C%2BdBRVPJUrIzz194qSasRWVtUbq5aY92LDy9bgQAqlKtOoixJ3pjG9vPY4UzV1Qg%3D%3D" rel="nofollow" target="_blank">AgentScope Runtime </a>是一个面向生产环境的智能体运行时框架，聚焦于两大核心问题：<strong>高效、可扩展的智能体部署</strong>与<strong>安全、隔离的Sandbox工具执行</strong>。该运行时提供上下文管理（包括长短期记忆与外部知识库集成）和多层级沙箱基础设施，构成一套框架无关的底层支撑系统，可与主流开源智能体框架或自定义实现无缝协同。其设计目标是为服务级智能体应用提供具备完整可观测性、强安全性与便捷部署能力的基础运行环境。</p><p>AgentScope Runtime实现了双核心架构：</p><ul><li><strong>智能体部署运行时（Engine）</strong><br/>提供智能体生命周期管理、会话状态维护、上下文存储（短期对话历史与长期记忆）以及外部知识库接入能力，并集成沙箱环境调度服务，支撑高并发、多会话的智能体服务部署。</li><li><strong>工具执行运行时（Sandbox）</strong><br/>基于隔离容器构建的安全执行环境，支持智能体调用各类工具操作，包括文件系统访问、浏览器自动化、GUI 交互及 MCP（Model Context Protocol）工具集成，确保所有副作用行为被严格限制在沙箱边界内，杜绝对宿主系统的潜在风险。</li></ul><p>目前，AgentScope 的主流部署模式依赖 <strong>Docker + Kubernetes</strong> 组合。该方案在功能完备性和集群管理能力上表现优异，但在实际落地 AI Agent 应用时，暴露出若干结构性瓶颈：</p><ul><li><strong>持续运行带来固定成本</strong><br/>容器实例需长期驻留内存以维持智能体状态和会话上下文，即使在无请求的空闲时段仍持续计费，导致显著的资源浪费，尤其对间歇性、事件驱动型任务极不友好。</li><li><strong>静态资源分配缺乏弹性</strong><br/>资源配额（CPU、内存）通常按预估峰值设定，难以动态适配真实负载。在流量突发时可能因资源不足导致响应延迟或失败；而在低峰期则大量计算资源闲置，利用率低下。</li><li><strong>高运维复杂度形成使用门槛</strong><br/>部署和维护一套生产级 K8s 集群涉及网络策略配置、服务发现、日志收集、监控告警、自动扩缩容（HPA）等多项云原生技能，对中小团队、独立开发者或非基础设施背景的研究人员构成显著障碍。</li></ul><p>这些限制使得许多具备潜力的 Agent 应用停留在实验阶段，难以实现低成本、高可用、快速迭代的规模化部署。</p><p>为系统性解决上述问题，AgentScope 正式推出基于<strong>阿里云函数计算（Function Compute, FC）</strong> 构建的 <strong>Serverless 运行时</strong>。该运行时针对 AI Agent 的典型工作负载（如会话保持、工具调用、状态依赖）进行深度优化，在保留功能完整性的同时，彻底重构资源使用与运维模型。</p><p>Serverless运行时的核心优势：</p><ul><li>✅<strong>按量付费，成本可精细化控制</strong><br/>计费粒度精确至毫秒级函数执行时间与内存消耗，空闲期间零费用。对于低频调用或突发型 Agent 任务，可有效降低成本。</li><li>✅<strong>毫秒级弹性伸缩，自动应对负载波动</strong><br/>无需预设实例数量或手动扩缩容，平台根据并发请求数自动调度计算资源，瞬时支撑从 1 到数千 QPS 的流量突增，保障服务 SLA。</li><li>✅<strong>零运维，聚焦核心逻辑开发</strong><br/>开发者无需关心底层服务器、容器镜像、K8s 配置或网络拓扑，仅需关注智能体逻辑、工具集成与业务流程编排，大幅缩短上线周期。</li></ul><p>此外，Serverless 运行时通过 <strong>会话亲和（Session Affinity）机制</strong>在无状态函数架构下有效支持有状态的 Agent 交互场景，兼顾弹性与一致性。</p><p>这一演进标志着 Agent 运行时正从“重资产、高运维”的传统模式，迈向“轻量化、自动化、经济高效”的云原生新范式，为 AI Agent 的大规模商业化落地扫清基础设施障碍。</p><h2>Serverless运行时集成能力详解</h2><h3>Engine 能力拓展</h3><p>Serverless 运行时深度集成 AgentScope 的核心执行引擎（AgentScope Runtime Engine），在保留原有编程模型的基础上，为开发者提供面向云原生环境的无缝部署体验。关键能力包括：</p><ul><li><strong>本地代码一键构建与依赖打包</strong><br/>开发者仅需在本地项目目录中执行<code>deploy()</code>方法，运行时即可自动分析 Python 依赖，构建包含用户代码、自定义工具及第三方库的可执行包，并上传至阿里云函数计算（FC）——该服务已深度集成于百炼 ModelStudio 平台，实现从开发到托管的一站式闭环。</li><li><strong>一键部署生成 HTTPS Endpoint</strong><br/>部署完成后，系统自动分配全局唯一的 HTTPS 端点（Endpoint），支持标准 RESTful 调用。外部系统（如 Web 前端、移动端或第三方服务）可通过该接口直接触发智能体执行，无需额外配置网关或反向代理。</li><li><strong>Header-Based Session 亲和性保障</strong><br/>为支持有状态交互（如多轮对话、工具链连续调用），Serverless 运行时引入基于 HTTP 请求头的会话绑定机制。客户端通过在请求中携带Session ID请求头，平台将确保同一 Session ID 的所有后续请求路由至同一函数实例（或关联的沙箱上下文），从而维持内存状态、临时文件或浏览器会话的一致性。</li><li><strong>继承 Serverless 核心优势</strong><br/>所有通过 Engine 部署的智能体天然享有 Serverless 架构的三大特性：按实际执行时间计费、毫秒级自动扩缩容、零基础设施运维，显著降低运营复杂度与总体拥有成本（TCO）。</li></ul><h3>Sandbox 运行时全面支持</h3><p>AgentScope 定义的四大沙箱类型现已完整适配 Serverless 运行时，可在函数计算环境中安全、高效地执行各类操作：</p><ul><li>✅ <strong>BaseSandbox</strong>：提供隔离的 Python 代码执行环境，适用于通用脚本运行与逻辑计算</li><li>✅ <strong>FileSystemSandbox</strong>：挂载临时或持久化文件系统，支持文件读写、日志记录与中间产物存储</li><li>✅ <strong>BrowserSandbox</strong>：内置无头 Chromium 浏览器，实现网页自动化、数据抓取与前端交互模拟</li><li>✅ <strong>GUISandbox</strong>：支持图形界面应用的模拟执行（如桌面软件自动化），适用于特定领域工具集成</li></ul><p>基于阿里云函数计算（FC）的Serverless运行时，深度集成 AgentScope 的Sandbox运行引擎，其核心特性如下：</p><ul><li><strong>预热实例池，消除冷启动延迟</strong><br/>平台可预先创建并维护一组常用类型的 Sandbox ，在新会话到来时直接复用，提高常驻服务的响应速度。</li><li><strong>自动注入 Session ID，保障上下文连续性</strong><br/>在首次创建 Sandbox 时，系统自动生成唯一 Session ID 并返回给客户端；后续所有针对该会话的 HTTP 请求均自动携带此 ID，确保操作始终作用于同一沙箱实例，保证状态一致性。</li><li><strong>全生命周期 Serverless 体验</strong><br/>每个 Sandbox 实例在会话结束后自动回收资源，计费随执行结束而终止，同样遵循 <strong>按量付费、毫秒级弹性、零运维</strong> 的 Serverless 原则，在安全性、性能与成本之间取得最佳平衡。</li></ul><p>通过 Engine 与 Sandbox 的双重增强，AgentScope 的 Serverless 运行时不仅解决了传统部署的成本与运维难题，更在保持强隔离与状态支持的前提下，实现了 AI Agent 应用的高效、安全、经济化交付。</p><h2>快速体验</h2><p>现在，您就可以将 Agent 应用快速部署到 Serverless 运行时！</p><h3>部署 Agent 到 Serverless运行时</h3><p>只需三步：</p><ol><li>配置相关环境变量</li></ol><pre><code class="yaml"># 确保设置环境变量
export DASHSCOPE_API_KEY="your-dashscope-api-key"
export ALIBABA_CLOUD_ACCESS_KEY_ID="your-access-key-id"
export ALIBABA_CLOUD_ACCESS_KEY_SECRET="your-access-key-secret"
export MODELSTUDIO_WORKSPACE_ID="your-workspace-id"

# 可选的OSS专用凭证
export OSS_ACCESS_KEY_ID="your-oss-access-key-id"
export OSS_ACCESS_KEY_SECRET="your-oss-access-key-secret"</code></pre><ol start="2"><li>定义好您的AgentApp</li></ol><pre><code class="yaml"># -*- coding: utf-8 -*-
# pylint:disable=wrong-import-position, wrong-import-order
import asyncio
import os

from agentscope.agent import ReActAgent
from agentscope.model import DashScopeChatModel

from agentscope_runtime.engine.agents.agentscope_agent import AgentScopeAgent
from agentscope_runtime.engine.runner import Runner
from agentscope_runtime.engine.schemas.agent_schemas import (
    MessageType,
    RunStatus,
    AgentRequest,
)
from agentscope_runtime.engine.services.context_manager import (
    ContextManager,
)
from agentscope_runtime.sandbox.tools.function_tool import function_tool
from others.other_project import version


@function_tool()
def weather_search(query: str) -&gt; str:
    if "sf" in query.lower() or "san francisco" in query.lower():
        result = "It's 60 degrees and foggy."
    else:
        result = "It's 90 degrees and sunny."

    return result


agent = AgentScopeAgent(
    name="Friday",
    model=DashScopeChatModel(
        "qwen-turbo",
        api_key=os.getenv("DASHSCOPE_API_KEY"),
    ),
    agent_config={
        "sys_prompt": "You're a helpful assistant named Friday.",
    },
    agent_builder=ReActAgent,
    tools=[
        weather_search,
    ],
)
print(f"AgentScope Runtime with dependencies version: {version}")


async def run():
    # Create a request
    request = AgentRequest(
        input=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": "杭州天气如何？",
                    },
                ],
            },
        ],
    )

    runner = Runner(
        agent=agent,
        context_manager=ContextManager(),
        # context_manager=None       # Optional
    )
    async for message in runner.stream_query(request=request):
        # Check if this is a completed message
        if (
            message.object == "message"
            and MessageType.MESSAGE == message.type
            and RunStatus.Completed == message.status
        ):
            all_result = message.content[0].text
        print(message)

    print(f"📝 Agent response: {all_result}")


if __name__ == "__main__":
    asyncio.run(run())</code></pre><ol start="3"><li>配置部署相关代码，将您的代码部署到Serverless运行时上</li></ol><pre><code class="yaml">import asyncio
import os
from agentscope_runtime.engine.deployers.modelstudio_deployer import (
    ModelstudioDeployManager,
    OSSConfig,
    ModelstudioConfig,
)
from agent_app import app  # 导入已配置的 app

async def deploy_to_modelstudio():
    """将 AgentApp 部署到阿里云 ModelStudio"""

    # 配置 OSS 和 ModelStudio
    deployer = ModelstudioDeployManager(
        oss_config=OSSConfig(
            access_key_id=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_ID"),
            access_key_secret=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_SECRET"),
        ),
        modelstudio_config=ModelstudioConfig(
            workspace_id=os.environ.get("MODELSTUDIO_WORKSPACE_ID"),
            access_key_id=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_ID"),
            access_key_secret=os.environ.get("ALIBABA_CLOUD_ACCESS_KEY_SECRET"),
            dashscope_api_key=os.environ.get("DASHSCOPE_API_KEY"),
        ),
    )

    # 执行部署
    result = await app.deploy(
        deployer,
        deploy_name="agent-app-example",
        telemetry_enabled=True,
        requirements=["agentscope", "fastapi", "uvicorn"],
        environment={
            "PYTHONPATH": "/app",
            "DASHSCOPE_API_KEY": os.environ.get("DASHSCOPE_API_KEY"),
        },
    )

    print(f"✅ 部署到 ModelStudio：{result['url']}")
    print(f"📦 制品：{result['artifact_url']}")
    return result

if __name__ == "__main__":
    asyncio.run(deploy_to_modelstudio())</code></pre><p>📚 详细文档请参考：<a href="https://link.segmentfault.com/?enc=FpxD6zmiQuu2aJRl5sduHg%3D%3D.xUvnD%2Fb67dbP%2BW%2FBIf6w%2FVLRetQpxAaHreO5PH4Q7eMQvhISoq821H6mHBJToPePChF3D0H%2BCH47HaORVSZnkZxJaybpn2B0W8tgmjR%2BBm4NcVmbk%2B%2BKbDzRUMz%2FgdVf" rel="nofollow" target="_blank">部署指南</a></p><h3>快速启动 Sandbox</h3><ol><li>安装<code>agentscope-runtime</code></li></ol><pre><code class="shell">pip install agentscope-runtime</code></pre><blockquote>由于agentscope-runtime仍在初期快速迭代中，建议采用源码安装方式</blockquote><pre><code class="shell">git clone https://github.com/agentscope-ai/agentscope-runtime.git

cd agentscope-runtime

pip install .</code></pre><ol start="2"><li>配置环境变量</li></ol><pre><code class="shell"># Service settings
HOST="0.0.0.0"
PORT=8000
WORKERS=1
DEBUG=False

# Runtime Manager settings
DEFAULT_SANDBOX_TYPE=base
POOL_SIZE=0
AUTO_CLEANUP=True
CONTAINER_PREFIX_KEY=agent-runtime-container-
CONTAINER_DEPLOYMENT=agentrun
DEFAULT_MOUNT_DIR=
STORAGE_FOLDER=runtime_sandbox_storage
PORT_RANGE=[49152,59152]

# FC 相关账户信息
FC_ACCOUNT_ID=&lt;your-account-id&gt;
FC_ACCESS_KEY_ID=&lt;your-access-key-id&gt;
FC_ACCESS_KEY_SECRET=&lt;your-access-key-secret&gt;
FC_REGION_ID=cn-hangzhou
# 规格配置
FC_CPU=2.0
FC_MEMORY=2048
# 网络配置
FC_VPC_ID=&lt;your-vpc-id&gt;
FC_VSWITCH_IDS=[&lt;your-vswitch-id&gt;]
FC_SECURITY_GROUP_ID=&lt;your-security-group-id&gt;
# 前缀
FC_PREFIX=agentscope-sandbox
# 日志配置
FC_LOG_PROJECT=&lt;your-sls-log-project&gt;
FC_LOG_STORE=&lt;your-sls-log-store&gt;</code></pre><ol start="3"><li>运行命令，启动沙箱服务器</li></ol><pre><code class="shell">runtime-sandbox-server --config fc.env</code></pre><ol start="4"><li>使用您的沙箱</li></ol><pre><code class="python">from agentscope_runtime.sandbox import BaseSandbox

# 连接到远程服务器（替换为您的实际服务器地址和端口）
with BaseSandbox(
    base_url="http://127.0.0.1:8000",
) as sandbox:
    # 正常使用沙箱
    print(box.list_tools())
    print(box.run_ipython_cell(code="print('hi')"))
    print(box.run_shell_command(command="echo hello"))
    input("Press Enter to continue...")</code></pre><p>📚 详细文档请参考：<a href="https://link.segmentfault.com/?enc=YEAUpxPBBnOYW1VusfzjjQ%3D%3D.fXpF9K8COpIOcpaIQ%2B6VyQ5Y69yKfaHHt3TYjdiEVAudnnrbUMuNOq%2B0VHHsbrBjS3gi6504RdfWhBVxqz1hJXxLXwLarjgPqIIARlwrVzbzc9coejozQ5GDGNj3D8vw" rel="nofollow" target="_blank">沙箱部署指南</a></p><h2>迈向“省钱又好用”的 AI 运行时</h2><p>AI Agent 的运行时基础设施正经历一场深刻的演进：从早期追求“能跑起来”的基础可用性，到关注开发体验与功能完备性的“好用”阶段，如今正加速迈向兼顾性能、安全与经济性的“省钱用”新范式。</p><p>AgentScope 与 Serverless 架构的深度集成，正是这一演进的关键实践。通过将智能体部署与工具执行全面迁移至基于阿里云函数计算（FC）的 Serverless 平台，不仅大幅降低了对容器编排、集群运维等云原生技能的依赖，更从根本上重构了资源使用模型——<strong>从“为闲置付费”转向“为实际执行付费”</strong>，使中小团队乃至个人开发者也能以极低成本运行生产级 Agent 应用。</p><p>Serverless 所提供的毫秒级弹性、自动扩缩容、强隔离沙箱与零运维特性，恰好契合 AI Agent 应用典型的负载特征：间歇性调用、状态依赖性强、工具执行风险高、成本敏感度高。我们坚信，<strong>Serverless 将成为 AI Agent 应用的最佳运行时。</strong></p><p>未来，AgentScope 将持续深化与主流云服务的协同，进一步优化会话管理、冷启动延迟、多模态工具支持等关键路径，并推动更多开源智能体项目采纳 Serverless 范式，构建一个开放、高效、经济的 Agent 运行生态，让复杂智能体系统的开发与部署如同调用普通 API 一样简单可靠。</p><p><strong>让每一个智能体，都能轻盈运行在云端。</strong></p>]]></description></item><item>    <title><![CDATA[如何将照片从 Mac 传输到 Andro]]></title>    <link>https://segmentfault.com/a/1190000047446115</link>    <guid>https://segmentfault.com/a/1190000047446115</guid>    <pubDate>2025-12-03 15:07:11</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <p>将照片从 Mac 传输到 Android 手机或平板电脑有时会感觉像是跨越数字鸿沟。由于 Mac 和 Android 运行在不同的生态系统中，直接拖放的方法行不通。如果您想将照片从 Mac 传输到 Android，可以尝试本指南中介绍的有效方法，这些方法可以高质量地传输照片。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446117" alt="图片" title="图片"/></p><p>快速浏览一下这5种方法：</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446118" alt="图片" title="图片" loading="lazy"/></p><p>第一部分：如何通过 iReaShare Android Manager 将照片从 Mac 传输到 Android</p><p>像iReaShare Android Manager这样的专用桌面软件旨在提供一套全面的一体化解决方案，方便您从 Mac 管理 Android 设备。借助此程序，您可以直接在 Mac 和 Android 之间传输图片，并保持原始画质。</p><p>iReaShare Android Manager 的主要功能：</p><ul><li>将照片从 Mac 电脑复制到 Android 设备。</li><li>还可以将照片从安卓设备传输到Mac电脑。</li><li>允许您在传输照片之前预览和选择照片。</li><li>还可以传输视频、音乐、文档、联系人、短信、通话记录和应用程序。</li><li>支持只读模式，以确保数据传输安全。</li><li>适用于运行 Android 6.0 或更高版本的 Android 设备，例如三星、一加、TCL、Tecno、OPPO、Vivo、荣耀、谷歌、摩托罗拉等。</li></ul><p>下载iReaShare Android Manager。</p><p>下载 Mac 版下载 Win 版</p><p>使用 Android Manager 将照片从 Mac 传输到三星 Android 设备：</p><pre><code>
下载适用于 Mac 的 iReaShare Android Manager 应用程序并安装。然后启动软件，并使用 USB 数据线将您的 Android 设备连接到 Mac 电脑。如果您需要无线连接，请将两台设备连接到同一个 Wi-Fi 网络，然后点击“通过 Wi-Fi 连接”。


按照屏幕上的指示在安卓设备上启用USB调试模式。连接将会建立。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446119" alt="图片" title="图片" loading="lazy"/></p><pre><code>
现在，选择“照片”类别。然后点击“相机”或“图库”，再点击“添加”从您的 Mac 电脑中选择照片。最后，点击“打开”或“确定”开始将照片导入 Android 设备。

</code></pre><p>第二部分：如何通过 Google Photos 将 Mac 上的照片传输到 Android 设备</p><p>Google Photos是一款跨平台云存储和同步服务，无需数据线即可在 Mac 和 Android 设备之间轻松同步文件。但是，如果您的帐户云存储空间不足，则无法一次性上传所有照片。</p><p>将 Mac 上的照片同步到 Android：</p><pre><code>
在您的Mac电脑上打开网络浏览器，访问Google Photos网站。然后使用您在Android手机上使用的同一个Google帐户登录。


点击“ + ”按钮 &gt; “导入照片”，然后从Mac本地存储中选择照片。等待上传完成。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446120" alt="图片" title="图片" loading="lazy"/></p><pre><code>在您的安卓手机上，打开 Google 相册应用。确保您已登录同一个 Google 帐户。您从 Mac 上传的照片将自动出现在您安卓设备上的 Google 相册图库中，您可以通过 Wi-Fi 或移动网络访问这些照片。


如果要将照片存储在 Android 设备的本地存储空间中，请在 Google Photos 应用中打开图片，点击三点菜单，然后选择“下载”。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446121" alt="图片" title="图片" loading="lazy"/></p><p>第三部分：如何通过 AirDroid 将照片从 Mac 传输到 Android 手机</p><p>AirDroid 是一款流行的第三方应用程序套件，可让您从桌面无线管理您的 Android 设备，使文件传输变得简单方便。</p><p>使用 AirDroid 将照片从 Mac 传输到 Android 手机：</p><pre><code>
请在您的安卓手机和Mac电脑上下载并安装AirDroid应用程序。创建AirDroid帐户并在两台设备上登录。


请确保您的 Mac 和 Android 手机连接到同一个 Wi-Fi 网络。在您的 Mac 上，打开 AirDroid 桌面应用程序。


从列表中选择您的安卓设备，然后选择“设备”选项卡。


将您想要传输的照片从 Mac 的 Finder 窗口拖放到与您的 Android 设备关联的 AirDroid 文件传输界面上。传输将通过您的网络以无线方式进行。照片将保存到您 Android 手机上指定的 AirDroid 文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446122" alt="图片" title="图片" loading="lazy"/></p><p>第四部分：如何通过 OpenMTP 将图片从 Mac 传输到 Android 设备</p><p>OpenMTP 是一款免费的开源应用程序，专门用于解决 Mac 到 Android 的文件传输问题，它利用了 MTP（媒体传输协议）标准。它充当文件浏览的专用桥梁。</p><p>通过 OpenMTP 将图片从 Mac 传输到 Android：</p><pre><code>
下载适用于 Mac 的 OpenMTP 应用程序并安装。然后使用 USB 数据线将您的 Android 设备连接到 Mac。


在您的安卓手机上，下拉通知栏并点击USB连接通知。选择“文件传输/Android Auto ”或“ MTP ”选项。


在您的 Mac 上启动 OpenMTP 应用程序。它采用双窗格界面：一侧用于显示 Mac 上的文件（左侧），另一侧用于显示 Android 设备的内部存储（右侧）。


在 Mac 上找到包含照片的文件夹（左侧面板），然后在 Android 设备上找到目标文件夹（右侧面板）。在 Mac 的面板中选择照片，然后点击“传输”按钮（通常是一个从左到右的箭头）开始传输。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446123" alt="图片" title="图片" loading="lazy"/></p><p>第五部分：如何通过 LocalSend 将照片从 Mac 发送到 Android 手机</p><p>LocalSend 是一款跨平台的开源应用程序，允许通过本地网络进行安全的点对点文件共享，类似于苹果的 AirDrop，但它适用于 Android、Mac、Windows 和 Linux。</p><p>通过 LocalSend 将照片从 Mac 传输到 Android：</p><pre><code>
请在您的Mac电脑和安卓手机上下载并安装LocalSend应用程序。确保两台设备都连接到同一个本地Wi-Fi网络。


在两台设备上启动 LocalSend 应用。它们应该会自动发现彼此。在 Mac 应用中，点击“发送”，然后选择要传输的照片。


从可用接收设备列表中选择您的安卓设备。您的安卓手机上会弹出通知。点击“接受”开始转账。


照片将保存到您安卓设备上指定的 LocalSend 文件夹中。
</code></pre><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446124" alt="图片" title="图片" loading="lazy"/></p><p>第六部分：关于将照片从 Mac 传输到 Android 的常见问题</p><p>Q1：我可以使用 Android 文件传输将照片从 Mac 拖放到 Android 设备上吗？</p><p>是的，您可以使用适用于 Mac 的 Android 文件传输 (AFT) 应用，将照片和其他文件从 Mac 拖放到 Android 手机上。虽然拖放功能可用，但需要注意的是，Android 文件传输已不再由 Google 官方支持或更新，并且经常不稳定，尤其是在较新版本的 macOS 和 Android 系统上。</p><p>Q2：我可以使用 AirDrop 将照片从 Mac 传输到 Android 设备吗？</p><p>不，您无法使用 Mac 上标准的内置 AirDrop 功能向绝大多数 Android 手机（包括三星、一加等）发送照片。尽管谷歌已经将 Android 的“快速分享”功能与苹果的 AirDrop 协议进行了整合，但这项功能非常新，尚未在所有 Mac 和 Android 机型上广泛普及。</p><p>Q3：我可以通过蓝牙将图片从Mac传输到Android吗？</p><p>是的，您可以使用蓝牙将图片从 Mac 传输到 Android 手机，因为这两个设备都支持蓝牙文件传输协议。这可以通过 Mac 上内置的蓝牙文件交换应用程序完成。但是请注意，这种方法通常速度很慢，仅建议用于传输少量小文件，例如几张照片。</p><p>结论</p><p>现在，通过以上方法，您可以轻松地将照片从 Mac 传输到 Android 设备。无论您喜欢 Google Photos 的无缝云端同步，还是iReaShare Android Manager的强大管理功能，总有一种方法能够完美地跨越 Mac 和 Android 之间的鸿沟。顺便一提，如果您经常在 Mac 和 Android 设备之间传输文件，iReaShare Android Manager 将是您的最佳助手。<br/>​</p>]]></description></item><item>    <title><![CDATA[低代码平台定义解析与选型建议(2025版]]></title>    <link>https://segmentfault.com/a/1190000047446140</link>    <guid>https://segmentfault.com/a/1190000047446140</guid>    <pubDate>2025-12-03 15:06:22</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>一、低代码定义解析：</h2><p>低代码平台是一种基于可视化设计与配置的应用开发方法，核心在于通过组件化、图形化、参数化的拖拽配置方式进行系统开发，开发过程中能大幅减少手动编码量，从而提升应用交付速度与效率。</p><p>1、技术内涵：</p><p>从技术内涵看，低代码平台通常依托表单驱动、模型驱动等技术路径，提供数据编排、生态连接、服务集成等核心能力，以图形化方式实现业务场景的快速构建与创新。</p><p>2、技术演进：</p><p>低代码的发展也算是历经了较长时间阶段的演进与共识凝聚。早在20世纪80年代，“第四代编程语言”已初显低代码思想；至2014年，Forrester明确定义了“低代码/零代码”概念，并指出低代码能够以最少的手工编码快速开发、配置和部署应用系统。2018年，Gartner进一步提出aPaaS与iPaaS概念，其中aPaaS与低代码理念高度契合，有效推动了低代码技术在全球范围内的关注与落地。</p><p>随着低代码技术的发展与行业实践，国内主流厂商（如织信、宜搭、奥泽等）达成共识，推出《低代码平台发展白皮书》，白皮书中明确了低代码平台的完整定义，强调其应以可视化配置为主、少量代码为辅，并具备全生命周期管理能力。</p><p>如今，低代码平台已广泛应用于企业数字化转型场景，截止2025年11月，低代码已在工业领域的MES、PLM系统灵活扩展中发挥作用，目前已达到百亿市场规模，依然构建起了一个完整的低代码生态体系。</p><p>3、产品特征：</p><p>低代码平台必须具备应用全生命周期管理能力，支持设计、开发、测试、部署、迭代、运维的全生命周期管理，实现应用开发效率提升、需求快速响应、敏捷迭代更新、运营维护便捷等目标，是一站式的应用开发平台。</p><h2>二、低代码平台选型建议</h2><p>对应用漏洞、平台安全性、数据安全合规等方面有要求的企业，选型时要注重：</p><p>深入评估平台安全能力： 不要只看厂商宣传，要了解平台是否具备“SAST（静态应用安全测试）和DAST（动态应用安全测试）”等自动化安全检测能力。</p><p>审查平台的权限管理机制：确保平台能提供细粒度的权限控制，包括角色权限、数据权限、字段权限等，并支持单点登录（SSO）和多因素认证（MFA）。</p><p>考察数据处理与存储： 询问平台的数据中心位置，数据传输是否使用TLS加密，以及是否支持私有化部署以满足特殊安全需求。</p><p>对集成能力、集成成本、数据同步等方面有需求的企业，选型时要考察：</p><p>考察API连接能力：确保平台支持API和SOAP等主流API协议，并且能够轻松调用和封装外部API。</p><p>查看集成案例和连接器：了解平台是否提供丰富的预置连接器，例如与钉钉、企业微信、飞书等常用系统的连接器。</p><p>评估异步处理能力： 询问平台如何处理大规模数据同步和异步任务，是否支持“ 消息队列（Message Queue）”等机制。</p><p>对生态拓展、性能瓶颈、二次开发和能力边界等方面有要求的企业，选型时要着重考虑：</p><p>评估平台的扩展性： 询问平台是否提供API接口、SDK、自定义组件开发框架等，支持开发者通过代码扩展平台功能。</p><p>关注平台架构： 了解平台是否采用微服务架构，支持独立部署和弹性伸缩，以应对业务增长带来的性能挑战。</p><p>考察“低代码”与“高代码”的融合： 理想的平台应该能够让低代码开发者和专业开发者在同一个平台协同工作，低代码负责快速构建，高代码负责复杂逻辑和性能优化。</p><h2>三、国内主流的低代码平台排行（12月最新）</h2><p>1、织信</p><p>推荐指数：★★★★★</p><p>综合评分：99.7分</p><p>织信Informat是由深圳基石协作自主研发的企业级低代码开发平台，平台基于“数据、流程、角色”三个基本要素，用户只需通过简单的“拖拽”、“配置”等操作，即可快速搭建整套的数字化管理系统。此外，平台还拥有足够强的边界能力，内置了脚本、自动化、网站、自定义页面、符合BPMN2.0规范的工作流引擎等功能，能满足大部分企业复杂的业务需求。该产品着重面向ToB企业内部信息化/数字化建设，为企业提供高效、定制、专业的数字化咨询与信息化系统一体化解决方案。</p><p>产品特点：支持本地私有化部署，上亿级大数据大并发处理能力，使用层与开发层分离，标准化的运维版本管理体系。基于织信低代码构建多年的aPaaS能力与自动化、流程化模式被进一步释放，构建一款应用时，企业可将前后端开发等环节紧密衔接，减少大量重复性工作，并有效提升 67% 的IT项目效率。</p><p>2、奥哲云枢</p><p>推荐指数：★★★★★</p><p>综合评分：98.3分</p><p>奥哲云枢是面向大中型企业复杂业务场景的低代码平台。其核心竞争力在于“All in One”的数智化引擎，将低零代码开发、AI智能、集成开放与数据可视化能力深度融合，形成覆盖应用全生命周期的一站式解决方案。平台支持从“零代码配置”到专业代码扩展的全场景开发模式，让业务人员与技术团队能够高效协同。作为市场公认的标杆性产品，奥哲服务了国内大量500强企业，在建筑、能源、金融等行业积累了丰富的央国企实践案例，市场占有率位居前列。</p><p>产品特点：以强大的流程引擎为核心，擅长构建和优化复杂、长周期的业务流程。平台提供了从设计、开发、集成到运维的完整能力，能有效封装企业既有业务能力并连接新老系统，帮助大型集团在复杂组织架构下实现数字化运营与流程管理。</p><p>3、宜搭</p><p>推荐指数：★★★★★</p><p>综合评分：96.6分</p><p>宜搭是阿里巴巴推出的低代码应用构建平台，深度融入钉钉生态，主打高效协同与快速部署。平台与钉钉的组织架构、消息通知、待办审批等原生能力无缝集成，用户可在钉钉工作台内直接创建和使用应用，实现“开发即使用”的协同体验。2025年，平台接入了DeepSeek大模型，AI流程自动化能力得到增强，可通过自然语言指令快速生成表单。平台提供了超过500个行业模板，能快速满足零售、医疗等领域的轻量化应用需求。</p><p>产品特点：依托阿里云与钉钉的双重生态，在服务的稳定性和安全性方面有较好保障。其操作轻量化，订阅制定价模式灵活，特别适合中小企业、部门团队快速搭建审批、人事、行政等内部协同类应用，大幅降低数字化门槛。</p><p>4、炎黄盈动</p><p>推荐指数：★★★★</p><p>综合评分：94.9分</p><p>炎黄盈动是一家专注于业务流程管理（BPM）与PaaS平台的服务商，其AWS PaaS平台是在深厚BPM技术积累上演进而来的企业级低代码平台。平台采用云原生和模型驱动架构，为企业构建关键业务应用提供稳定可靠的底层支撑。其核心优势在于强大的流程建模、自动化与编排能力，能够帮助企业梳理并优化端到端的复杂业务流程，实现跨部门、跨系统的业务协同。</p><p>产品特点：在流程挖掘与低代码开发深度融合方面表现突出。平台非常适合流程密集型的大型组织，尤其在金融、政务、能源等对系统稳定性、安全性和合规性有严苛要求的行业，是构建任务关键型（Mission-Critical）应用系统的有力工具。</p><p>5、Zoho Creator</p><p>推荐指数：★★★★</p><p>综合评分：93.5分</p><p>Zoho Creator是国际知名软件服务商Zoho旗下的低代码开发平台，拥有超过18年的行业经验，技术成熟稳定。平台提供从表单搭建、流程自动化到数据分析的全栈能力，并支持从零代码到全代码的平滑过渡。其最大特色之一是集成了AI助手“Zia”，支持通过文本描述自动生成应用，显著提升开发效率。作为全球化平台，它支持30多种语言，在全球拥有多个数据中心，兼顾了本地化适配与全球化协同需求。</p><p>产品特点：与Zoho旗下的CRM、项目管理等25+应用无缝集成，生态融合性强。平台采用灵活的订阅制收费，支持1人起购，性价比高，既能满足中小企业当前的全场景需求，也能支撑大型企业未来的规模化扩展与跨区域部署。</p><p>6、JeecgBoot</p><p>推荐指数：★★★★</p><p>综合评分：91.7分</p><p>JeecgBoot是国内首个免费开源的低代码平台，由开源社区主导研发，基于BPM理念构建，采用SpringBoot 3.x、SpringCloud、Vue等前后端分离架构，全面支持微服务架构。用户通过平台强大的代码生成器可一键生成前后端完整代码，配合在线表单、报表、大屏设计等丰富低代码模块，实现简单功能零代码配置、复杂功能低代码生成的灵活开发模式，大幅减少重复开发工作。平台聚焦Java技术栈企业数字化需求，为开发者提供全流程开发支撑。</p><p>产品特点：完全免费开源且社区活跃度高，提供丰富的学习资源与技术支持，大幅降低企业研发成本；业务流程采用工作流引擎与表单松耦合设计，支持灵活配置与个性化扩展，保障企业流程保密性；支持与各类Java生态系统深度集成，灵活度高，特别适合Java项目团队快速构建ERP、CRM、OA等业务系统；可显著减轻开发人员负担，开发效率较传统模式提升50%以上。</p><p>不知道不觉也写了三千多次，今天暂时先说到这，如果大家在选型方面有任何疑问，欢迎私信交流。附赠低代码产品上手文档说明。</p>]]></description></item><item>    <title><![CDATA[UniParse：让多模态模型真正“读懂]]></title>    <link>https://segmentfault.com/a/1190000047446168</link>    <guid>https://segmentfault.com/a/1190000047446168</guid>    <pubDate>2025-12-03 15:05:34</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote><p>在多模态大模型迅速发展的今天，我们已经能让模型"看图说话"，甚至"读懂表格"，但要让模型真正理解复杂的文档结构（例如在PDF中准确识别章节、表格、公式与图像的逻辑关系）依然是一个未被彻底解决的问题。</p><p>UniParse正是为此而生：它是一款<strong>面向AI应用的通用文档解析工具</strong> ，旨在将文档中的非结构化内容转化为结构化语义信息，使多模态模型能够<strong>高效、精准</strong>地理解和利用文档内容。</p><p>本文将从技术视角介绍UniParse，功能方面的介绍请移步<a href="https://link.segmentfault.com/?enc=MkVHpxrmSRHafilFjjvZkg%3D%3D.AX6tuUW3vBrSdxUQEkv%2BiKtT15vnrh%2Foq0%2BX%2FOYbvXnVGXQxK%2BTBlWoJDuUOreJLxljWZq%2BVj2FXHfRcvM%2FeBZHmFYN89xsCkqxA3v453vILV1RNJkHBkYdBKK1UYRsf%2BMOhyJYRYyM3l5JQDZmDPDjrQG5eUSWNhYwVJkz3rd%2BpL%2BsZIBHHKQTPYRffLShA" rel="nofollow" target="_blank">产品上线|商汤自研智能文档解析工具UniParse，重新定义文档处理！</a></p></blockquote><hr/><h2><strong>一、为什么需要文档解析</strong></h2><p>现代大模型已经能够处理文本、图像、语音等多种模态，但在面对文档时仍然存在明显短板：</p><ul><li><strong>格式复杂</strong>：PDF、Word等文件中同时包含文字、表格、图片、公式、页眉页脚等多种内容，且层次不统一。</li><li><strong>结构缺失</strong>：OCR只能识别文字，却无法恢复章节层级与逻辑顺序。</li><li><strong>语义混乱</strong>：表格、图像与正文往往存在隐含关联，模型难以在语义上进行对齐。</li></ul><p>这意味着，如果直接把整份文档输入多模态模型，模型将面临巨大的上下文噪声和空间混乱，生成效果不稳定，也无法进行精确问答。UniParse的作用，就是在模型"读文档"之前，帮它<strong>理清结构、分清语义、建立关联</strong>。</p><hr/><h2><strong>二、UniParse的技术流程</strong></h2><p>UniParse的核心流程分为两个主要阶段：<strong>版面分析（LayoutAnalysis)与内容提取（ContentExtraction）</strong> ，并辅以<strong>预处理</strong> 与<strong>内容合并</strong>两个辅助流程。整个流程既保持模块化设计，又在数据层实现了结构化信息流动，使得不同模态内容（文字、图片、表格、公式）能够被统一建模和调用。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446170" alt="" title=""/></p><h3>1️⃣<strong>文档预处理</strong></h3><p>UniParse的预处理阶段主要任务是<strong>统一输入格式</strong> 。系统会将各类文档（PDF、DOC、DOCX等）<strong>逐页渲染为高分辨率图像</strong>，保证不同文件格式在后续视觉模型中具有一致的输入维度。这一过程通常基于PyMuPDF或libreoffice的渲染引擎实现，可控制分辨率以兼顾清晰度与性能。</p><p>同时，预处理阶段还执行以下步骤：</p><ul><li><strong>页面编号与坐标标准化</strong>：为每页图像生成统一的坐标系，用于后续版面元素定位；</li><li><strong>去噪与边缘裁剪</strong>：提升模型在扫描件、照片类文档上的鲁棒性；</li><li><strong>文件元信息提取</strong>：（如页数、文件名、创建时间），用于文档追踪与任务调度。</li></ul><p>经过预处理后，所有文档都被转化为一组图像文件及其基础元信息，为后续的版面解析与内容提取提供统一输入。</p><h3>2️⃣<strong>版面分析</strong></h3><p>版面解析是UniParse的核心之一，目标是<strong>还原文档的空间与语义结构</strong> 。这一阶段采用<strong>视觉语言联合建模</strong>方法：</p><ul><li>在视觉层面，利用版面分析模型（如LayoutLMv3或自研视觉Transformer）识别标题、正文、表格、图像、公式、脚注等区域；</li><li>在语言层面，通过文本块的字体、缩进、上下文语义判断章节层次与逻辑顺序；</li><li>最终将视觉检测结果与文本序列对齐，生成一个包含位置、类型与层级的<strong>结构化版面树</strong>。</li></ul><h3>3️⃣<strong>内容提取</strong></h3><p>UniParse针对不同类型内容采用<strong>专用解析管线</strong>：</p><ul><li><strong>文字</strong>：OCR模型或文本提取API结合版面坐标进行文本恢复与段落重建；</li><li><strong>表格</strong>：基于结构化表格识别网络（如TableFormer或自研模型）恢复单元格位置、合并关系与层级结构，输出HTML/LaTeX格式；</li><li><strong>图片</strong>：通过OCR或视觉语言模型（VLM）获取图像描述，为多模态模型提供语义锚点；</li><li><strong>公式</strong>：采用基于Transformer的公式识别引擎将公式区域转化为可编辑的LaTeX表达式。</li></ul><p>每种内容在抽取后都会带有来源页、坐标和上下文标签，以便在合并阶段进行定位与关联。</p><h3>4️⃣<strong>语义层重构</strong></h3><p>最后一步是内容合并与输出。系统将前述多类型元素按照版面树的层级进行拼接，恢复出原文档的逻辑顺序与结构。这一阶段还可以进行：</p><ul><li>内容去重与段落融合（防止跨页重复文本）；</li><li>模态链接（表格、图像与正文语义匹配）；</li><li>结构化输出（统一输出为JSON、HTML或Markdown格式）。</li></ul><p>通过这一设计，UniParse能在保持文档可读性的同时，为下游多模态模型提供可计算的结构化输入。</p><hr/><h2><strong>三、UniParse与多模态大模型的协同机制</strong></h2><p>多模态模型的核心挑战之一是模态对齐。传统方法依赖模型内部注意力机制去"猜测"文本与视觉区域的对应关系，而UniParse提供了<strong>显式的结构锚点</strong>。</p><p>从工程上看，UniParse的结构化输出可以直接映射到模型输入的不同通道：</p><ul><li>文本节点被编码为语言向量；</li><li>表格与公式节点可转换为结构token序列；</li><li>图像节点对应视觉特征向量；</li><li>节点之间的层级关系（如章节树）可编码为attentionmask，用于指导模型的跨模态关注。</li></ul><p>通过这种方式，UniParse在模型输入阶段实现了<strong>结构化对齐</strong>：</p><ul><li>模型在编码时能基于文档结构进行有选择的注意力分配；</li><li>上下文检索与问答更精确，因为每个节点都带有位置标签；</li><li>生成内容可以反向追溯到原文档区域，实现可解释性。</li></ul><p>换言之，UniParse并非一个单纯的"预处理器"，而是为多模态大模型提供了结构感知接口，让模型真正理解"这是一份文档"，而不仅仅是一组视觉与文本片段。</p><hr/><h2><strong>四、应用场景：从文档解析到智能理解</strong></h2><p>UniParse的技术能力为多模态模型打开了更广阔的应用空间：</p><ul><li><strong>智能问答（QA）</strong>：大模型可直接基于结构化数据进行文档问答，不仅能回答正文问题，也能解析表格、公式或图表。</li><li><strong>知识抽取与检索增强生成（RAG）</strong>：通过文档语义图构建可检索知识库，支持高精度上下文匹配。</li><li><strong>报告生成与内容审校</strong>：结构化信息流使模型能生成符合格式规范的总结、分析报告或审阅意见。</li><li><strong>图文理解与多模态推理</strong>：表格、公式、图片被视为独立模态单元，与文本共同构成推理输入，适用于学术报告、财务报表等复杂文档。</li></ul><hr/><p><strong>小结</strong></p><blockquote>在多模态智能系统的发展路径中，<strong>结构化理解</strong>是必经之路。UniParse作为文档解析的基础设施，为大模型提供了语义层级、视觉位置与逻辑关系的桥梁，使文档理解从模糊感知走向可解释推理。未来，模型的"读文档"能力将不断演进------它们不再仅仅识别信息，而是能够基于文档的结构和语义进行真正的理解与推理。</blockquote><hr/><p>更多技术讨论，欢迎移步 "万象开发者" gzh！</p>]]></description></item><item>    <title><![CDATA[研发效能度量工具横评：哪些指标真的能提升]]></title>    <link>https://segmentfault.com/a/1190000047446191</link>    <guid>https://segmentfault.com/a/1190000047446191</guid>    <pubDate>2025-12-03 15:04:57</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <blockquote>市面上能做研发效能度量的工具越来越多，有的是一体化研发管理平台，有的专注工程效能，有的来自云厂商 DevOps 套件，还有自建的开源度量方案。但决定交付能力的，既不是报表数量，也不是图表是否炫酷，而是——你到底在度量哪些研发效能指标，这些指标与交付瓶颈的关系有多紧密，工具是否支持闭环改进。本文选取四类典型工具路线及代表产品做横向测评，系统梳理关键研发效能度量指标及其适用场景，希望能帮助你做更理性的选型与规划。</blockquote><h2>先想清楚为什么要做「研发效能度量」</h2><p>很多企业做研发效能度量，是这样开始的：</p><p>先采购或搭建一个“研发效能平台”；<br/>接入需求、缺陷、流水线、Git 等多种数据源；<br/>几个月后，报表很多，但决策和交付方式几乎没变。</p><p>从组织视角看，问题往往出在「顺序」上—— 先有工具，再找指标，再想目的。</p><p>更健康的顺序应该是：</p><p><strong>① 先定义要解决的问题</strong></p><ul><li>一直延期，想缩短端到端交付周期？</li><li>线上频繁故障，想提升质量与稳定性？</li><li>新产品投入很大，但客户感知不强，想优化价值交付？</li></ul><p><strong>② 再挑出最关键的研发效能度量指标</strong></p><ul><li>哪几个指标能直接反映这个问题？</li><li>哪些是“结果指标”，哪些是“过程指标”？</li></ul><p><strong>③ 最后再看工具与路径</strong></p><ul><li>哪类工具更容易精准采集这些数据？</li><li>哪类平台更便于把指标带进迭代会、项目会、季度复盘？</li></ul><p>如果这三步不清楚，再完备的工具横评也只是“功能列表”。下面这 4 组研发效能度量指标，可以视为组织级的“基准标尺”。</p><h2>四大类真正影响交付的「研发效能度量指标」</h2><p>要评估一款研发效能度量工具是否值得引入，核心不是“能做多少报表”，而是能否支持下面四组关键指标的采集与使用：</p><h4>1. 流动效率指标：交付是不是在“顺畅地流动”</h4><ul><li>端到端交付周期（Lead Time）：从需求提出到上线。</li><li>开发周期（Cycle Time）：从开始开发到完成开发。</li><li>在制品数量（WIP）：同时在做多少工作。</li><li>吞吐量（Throughput）：单位时间完成多少工作项。</li></ul><p>这些研发效能度量指标的作用是：判断团队是“太忙导致变慢”，还是“系统性瓶颈导致变慢”。</p><h4>2. 质量与稳定性指标：研发效能能不能“可持续”</h4><ul><li>缺陷密度、缺陷分布。</li><li>变更失败率、回滚次数。</li><li>故障平均恢复时间（MTTR）。</li><li>与发布事件的关联。</li></ul><p>这组研发效能度量指标用于回答：我们是在“加速交付”，还是在“透支质量”？产品稳定性是否足以支撑更快的交付节奏？</p><p>如果质量指标长期失控，任何短期的“交付提速”都只是在透支未来。</p><h4>3. 价值与资源配置指标：忙碌是否真的创造价值</h4><ul><li>需求从立项到首次上线的周期。</li><li>不同类型需求（创新、优化、技术债）的占比。</li><li>废弃或长期搁置需求比例。</li></ul><p>这类研发效能度量指标回答的问题是：研发在多大程度上被“真正创造价值的事”占据？</p><h4>4. 协作与团队健康指标：交付问题的领先信号</h4><ul><li>插单率、计划与实际偏差。</li><li>跨团队依赖导致的等待时间。</li><li>简单调研上的阻碍感、负荷感。</li></ul><p>这些研发效能度量指标往往比故障率更早暴露风险，是组织“体温计”。很多交付危机，最早不是出现在流水线，而是出现在“团队感觉不对”。</p><p>后文对各类工具与路径的横评，都围绕这四组指标展开：能否支撑这些指标的采集、分析与闭环，是衡量研发效能度量工具价值的核心标准。</p><h2>四类研发效能度量工具横评</h2><h4>1. 一体化研发管理平台 —— 让「工作流」与「度量数据」同源</h4><p>这一类工具的共同特征是：本身就是需求 / 项目 / 缺陷 / 测试 / 流水线的协同平台，研发效能度量的数据主要来自团队日常使用，而非额外报表工程。典型代表有 ONES、Jira Software、Azure DevOps 等。</p><p>它们适合回答的问题是：</p><p>“多项目、多团队的交付效率与质量趋势如何？”<br/>“具体项目的交付瓶颈在哪里？”<br/>“组织级的研发效能度量该如何落地？”</p><p>下面会对上面提到的三种典型代表工具进行测评：</p><p><strong>（1）ONES：本地化一体化研发管理与效能度量</strong></p><p>先来测评一款国内的一体化研发管理平台。ONES 的核心定位就是一体化研发管理平台 + 研发效能度量模块，通过 Project / TestCase / Wiki 等模块管理需求、项目、缺陷、测试，再由 ONES Performance 统一抽取数据做研发效能分析。</p><p><strong>ONES 在四类指标上的能力：</strong></p><p>① 流动效率类研发效能度量</p><p>端到端 Lead Time、Cycle Time、WIP、吞吐量都可以基于工作项自然计算；<br/>支持按项目、团队、版本等维度分析流动效率变化。</p><p>② 质量与稳定性类研发效能度量</p><p>缺陷与需求、版本关联，支持按模块/版本做缺陷密度分析；<br/>与流水线 / 发布系统集成后，可以分析变更失败率等。</p><p>③ 价值与资源配置类研发效能度量</p><p>通过自定义字段区分需求类型，分析创新 / 优化 / 技术债等投入产出；<br/>配合项目群视图，支撑业务线层面的价值与资源审视。</p><p>④ 协作与团队健康类研发效能度量</p><p>利用看板、阻塞状态、依赖关系等字段，识别跨团队等待和插单情况；<br/>PMO 可基于这些数据组织项目级、组织级复盘。</p><p><strong>适合的团队与场景：</strong></p><ul><li>希望统一工具栈，打通从需求到交付的链路，统一“研发工作台”和“研发效能度量平台”；</li><li>对国产化、本地部署、安全合规有要求的组织；</li><li>希望在迭代会、项目会中直接使用平台视图，而不是额外导出报表；</li><li>需要 PMO、业务线负责人在统一视图下管理多项目、多团队效能。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446193" alt="图片" title="图片"/></p><p>配图：ONES 内置多种研发效能度量指标表</p><p><strong>（2）Jira Software：全球常见的敏捷项目管理与度量工具</strong></p><p>Jira 相信大家都不陌生，是海外都广泛使用的敏捷项目管理工具，支持 Scrum / Kanban 及基本的研发效能统计，并通过插件生态扩展工程效能、DORA 指标等。</p><p><strong>Jira 在关键指标上的表现：</strong></p><p>➀ 流动效率：</p><p>控制图、累计流图可以辅助分析 Cycle Time 和 WIP；<br/>若要实现端到端 Lead Time，需要结合外部系统（测试、发布等）和插件。</p><p>➁ 质量与稳定性：</p><p>缺陷趋势、版本质量可通过 Issue + Release 管理实现；<br/>更深入的 DORA 指标一般需要与 CI/CD 工具协作。</p><p>➂ 价值与资源：</p><p>通过自定义 Issue 类型和字段，可一定程度上做“需求类型 / 价值”维度分析，但更多依赖组织自建模型。</p><p><strong>适合的团队与场景：</strong></p><ul><li>已广泛部署 Atlassian 体系，团队成熟度较高；</li><li>具备较强流程治理能力，能在高自由度配置下统一研发效能度量口径。</li></ul><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446194" alt="图片" title="图片" loading="lazy"/></p><p>配图：Jira 产品组合链</p><p><strong>（3）Azure DevOps：偏“开发侧一体化”的度量能力</strong></p><p>Azure DevOps 主要以“代码 + 流水线 + Issue / Work Item + 测试”为核心，内置了 Value Stream、DORA 指标等工程向研发效能度量视图。</p><ul><li>通过 Boards、Repos、Pipelines、Tests 形成一体化 DevOps 平台；</li><li>提供 Lead Time / Cycle Time 控制图组件，直接展示工作项在流水线中的流动时间。</li></ul><p><strong>适合的团队与场景：</strong></p><p>工程实践成熟，对 CI/CD、自动化测试和持续部署投入较多；<br/>主要问题集中在“从提交到上线”的效率与稳定性，而不是需求塑造与价值回报。</p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446195" alt="图片" title="图片" loading="lazy"/></p><p>配图：Azure DevOps 产品图</p><h4>2. 工程效能分析平台 —— 深挖 Git / CI 的工程向研发效能度量</h4><p>这一类工具通常站在“工程管理”的视角，用 Git、CI/CD、Issue 等数据来做研发效能度量，主要度量 DORA 指标、PR Cycle Time、代码 churn、评审质量等，代表产品包括 Pluralsight Flow、LinearB、Jellyfish 等。</p><p><strong>（1）Pluralsight Flow（原 GitPrime）</strong></p><p>Pluralsight Flow 主要聚焦开发者行为与工程实践，分析提交习惯、重构比例、评审深度等，对“工程效率”“技术债管理”这类问题给出可视化研发效能度量。适合不改现有项目管理 / 需求工具，只希望在工程层面做更细致的指标洞察的团队。</p><p><strong>（2）LinearB</strong></p><p>LinearB 是典型的 DORA 指标与工程效能平台，强调 Cycle Time 拆解、部署频率、MTTR 等研发效能度量，常配合 GitLab / GitHub + CI 工具使用，作为“工程效能度量层”。</p><p><strong>适合场景：</strong></p><p>已有成熟 DevOps 流水线，短期不引入一体化管理平台；<br/>工程领导层希望用 DORA 指标推动工程实践改进。</p><p><strong>（3）Jellyfish</strong></p><p>Jellyfish 强调“工程投入与业务方向对齐”，分析研发资源在不同业务方向上的分布，一般会融合工程指标、团队健康度等维度，为高层提供决策视图。适合研发规模很大、业务线复杂，需要在高层视角回答“钱花在哪、产出如何”的公司。</p><p><strong>整体评价（工程效能平台）：</strong></p><p>在“流动效率 + 质量稳定性”两个方面的工程侧研发效能度量非常有价值；<br/>对需求价值、项目管理、组织治理等维度，需要与其他系统协同使用。</p><h4>3. 云厂商 DevOps 套件中的“效能洞察”</h4><p>这类产品通常作为云厂商 DevOps 套件的一部分，直接利用云上项目协作、代码、流水线、测试等数据来做研发效能度量。典型代表有阿里云云效效能洞察、腾讯云 CODING DevOps 效能洞察、华为云 CodeArts Board 等。</p><p><strong>（1）阿里云 云效效能洞察 Insight</strong></p><p>云效效能洞察是阿里云 BizDevOps 平台的高级服务，提供交付过程观测和研发效能度量，围绕项目、代码、流水线、质量等构建端到端指标体系。内置 90+ 场景化指标卡和模板化报表，覆盖项目度量、代码度量、流水线度量、质量保障、工作负荷管理等场景。</p><p>适用场景：研发活动主要在阿里云云效上进行，希望“云上工具 + 度量”一体化的团队。</p><p><strong>（2）腾讯云 CODING DevOps 效能洞察</strong></p><p>CODING 效能洞察专注 DevOps 全流程研发效能，通过 50+ 指标提供团队度量、项目度量、个人度量、质量 / 效率 / 价值与成本分析等视图。指标覆盖需求交付周期、缺陷修复周期、提交趋势、构建频率、部署成功率等。</p><p>适用场景：团队已经使用 CODING DevOps 做代码托管、流水线和项目协作，希望顺带接入研发效能度量。</p><p><strong>（3）华为云 CodeArts Board 效能洞察</strong></p><p>CodeArts Board 主要为企业管理者、项目经理、团队负责人等提供端到端的研发效能度量，从需求、缺陷、代码、构建、测试、部署、发布到运营进行全过程分析。内置 100+ 指标库，覆盖交付质量、交付效率、交付能力、交付成本、交付价值，并提供多角色“驾驶舱”。</p><p>适用场景：重度使用华为云 DevCloud / CodeArts 的企业，需要统一的“云上研发效能驾驶舱”。</p><p><strong>整体评价（云厂商路线）：</strong></p><p>在“流动效率 + 质量与稳定性”的指标上做得比较完整，也支持一定程度的价值与成本分析；</p><p>但度量对象较强绑定在云厂商生态，对多云 / 混合工具栈的组织，会有一定接入限制。</p><h4>4. 开源 + 自建 —— 以 Apache DevLake 为代表</h4><p>Apache DevLake 作为开源的 Dev 数据平台，支持接入 Jira、GitHub/GitLab、CI/CD 等多种数据源。内置 DORA 指标（部署频率、变更交付时间、变更失败率、恢复时间）以及大量研发效能度量指标，如需求 Lead Time、Bug Age、构建成功率、PR Cycle Time 等。</p><p><strong>在关键指标上的表现：</strong></p><ul><li>只要数据接得进来、模型建得好，前文提到的四类指标都可以覆盖；</li><li>灵活度高，可以精细化适配自身的研发流程与研发效能度量体系。</li></ul><p><strong>适用场景：</strong></p><ul><li>有数据团队、愿意自己维护数据平台的中大型技术公司；</li><li>工具栈高度异构，希望用统一的开源层打通数据、构建定制化研发效能度量体系。</li></ul><p><strong>优劣分析：</strong></p><ul><li>优势：指标丰富、可定制程度高，对“想深挖但不想受限于单一厂商”的团队非常友好；</li><li>局限：需要投入数据工程与运维成本；指标要想真正进入迭代与项目管理节奏，仍然需要与现有工具（包括 ONES、Jira 等）打通使用。</li></ul><h2>综合对比：哪条路径更适合哪类团队？</h2><p>接下来，我会站在“研发效能度量 + 组织阶段”的角度，把上面的工具做一个简要横评：</p><p><strong>1. 成长型团队（几十人规模以内）</strong> </p><p>诉求： 先建立基础研发效能度量意识，看到趋势即可。</p><p>更适合的路径：</p><ul><li>短期：Excel + 现有工具报表，选少量指标试水；</li><li>中期：选择一款易于落地的一体化平台（如 ONES 或 Azure DevOps / GitLab），把“工作 + 度量”慢慢迁到统一平台。</li></ul><p><strong>2. 多团队、多项目协作的中型组织 </strong></p><p>诉求： 统一口径、统一看板，让管理层对交付现状“看得见”。</p><p>更适合的路径：</p><ul><li>一体化研发管理平台（ONES、Jira + 部分插件、Azure DevOps / GitLab），作为日常协作的主平台；</li><li>如果已经重度云上，则可评估云厂商自带的“效能洞察”模块。</li></ul><p><strong>3. 工程文化成熟、DevOps 体系完善的大型工程团队 </strong></p><p>诉求： 精细化优化流水线效率、稳定性和工程实践。</p><p>更适合的路径：</p><ul><li>在现有 DevOps 工具链上叠加工程效能平台（Pluralsight Flow、LinearB、Jellyfish 等）；</li><li>核心指标更多聚焦：DORA、PR 周期、构建成功率、MTTR 等。</li></ul><p>同时建议： 保持一款一体化平台（或统一项目管理系统），承接需求与项目层面的研发效能度量。</p><p><strong>4. 已深度绑定某家云厂商的组织 </strong></p><p>诉求： 云上项目、代码、CI/CD 已集中，希望“一站式搞定度量”。</p><p>更适合的路径：</p><ul><li>优先评估当前云上的研发效能度量 / 效能洞察模块；</li><li>若后续需要更复杂的自定义分析，再考虑加一层 BI 或开源度量平台。</li></ul><p><strong>5. 有数据团队、强调数据主权和定制化的技术公司 </strong></p><p>诉求： 跨工具栈的统一研发效能度量体系，以及差异化的指标和算法。</p><p>更适合的路径：</p><ul><li>使用 Apache DevLake 等开源平台构建自有“研发数据湖”；</li><li>同时选用一款协作平台（如 ONES / Jira 等）承载日常工作，把数据汇总到数据湖中做统一分析。</li></ul><p>在这个视角下，每条路径都有它“最舒服”的位置，很难简单说谁是“最优解”。更现实的情况往往是：选择一个平台作为协作与研发效能度量的“主场”，再有选择地叠加工程效能平台或开源方案。</p><h2>用“指标思维”看工具，而不是用工具反推指标</h2><p>做研发效能度量工具横评，最终不是为了证明谁更好，而是为了回答三个简单的问题：</p><ul><li>我们真正关心哪些指标，这些指标能否确实推动交付改进？</li><li>这些指标，在哪个工具或路径上，产生和使用的成本最低？</li><li>在当前组织阶段，我们有多少资源，可以支撑哪种复杂度的方案？</li></ul><p>当你先用这样的“指标思维”看待工具，再去比较 ONES、Jira、工程效能平台、云厂商套件、开源方案时，就更容易找到适合自己团队的组合。</p>]]></description></item><item>    <title><![CDATA[NeurIPS2025公布最佳论文奖 L]]></title>    <link>https://segmentfault.com/a/1190000047446199</link>    <guid>https://segmentfault.com/a/1190000047446199</guid>    <pubDate>2025-12-03 15:04:14</pubDate>    <description><![CDATA[<link rel="stylesheet" href="https://static.segmentfault.com/main_site_next/prod/_next/static/css/95c21119582c7926.css" data-n-g=""> <h2>NeurIPS2025公布最佳论文奖</h2><p>2025 年 11 月 26 日，<strong>NeurIPS（神经信息处理系统大会）</strong> 正式公布了 <strong>2025 年度最佳论文奖获奖名单</strong>。此次奖项由最佳论文评选委员会从会议主赛道及数据集与基准赛道中遴选产生，委员会成员经程序主席、数据集与基准赛道主席提名，由大会主席、下一代与可及性主席批准，均为机器学习各领域顶尖研究者。<strong>最终共有7 篇突破性论文获奖，</strong> 包括 <strong>4 篇最佳论文</strong> （含1 篇数据集与基准赛道专属获奖论文）和 <strong>3篇优秀论文（Runner-up）</strong>，覆盖生成模型理论、强化学习、大语言模型机制、学习理论等多个核心研究方向。</p><h3>最佳论文</h3><h4>1.《Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446202" alt=" " title=" "/></p><p><strong>核心贡献：</strong> 针对大语言模型（LLMs）生成内容缺乏多样性、可能导致人类思想同质化的问题，提出了大规模数据集 Infinity-Chat（含 2.6 万条真实开放域用户查询、3.125 万条人类标注），构建了首个开放域提示词综合分类体系（6 个顶级类别、17 个子类别）。通过对 70 余种模型的实证研究，揭示了"人工蜂群思维（Artificial Hivemind）" 效应 —— 模型内部存在重复生成倾向，且不同模型间输出高度同质化。同时发现现有 LLM、奖励模型及自动评判器难以匹配人类多样化偏好，为缓解 AI 安全风险提供了关键参考。</p><p><strong>评审评价：</strong> 填补了AI 评估中创意生成、主观偏好对齐等维度的研究空白，为 AI 系统异质性保护奠定了基础，树立了 "以科学认知和社会挑战为导向" 的数据集构建新标准。</p><h4>2.《Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446203" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 系统探究了门控机制对softmax 注意力的影响，通过在 150 亿参数混合专家（MoE）模型和 17 亿参数稠密模型（基于 3.5 万亿 token 数据集训练）上的 30 余种变体实验，发现 "在缩放点积注意力（SDPA）后添加头专属 sigmoid 门控" 的简单修改，可显著提升模型性能、训练稳定性及长上下文外推能力，同时缓解注意力 sink 问题。该机制的有效性源于引入非线性和查询依赖的稀疏门控分数，相关代码与模型已开源，并应用于 Qwen3-Next 系列模型。</p><p><strong>评审评价：</strong> 研究成果具备极强的可实施性，基于工业级计算资源完成的大规模验证为LLM 架构优化提供了可靠依据，开源行为对推动领域发展具有重要意义。</p><h4>3.《1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446204" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 挑战了强化学习（RL）难以训练深层网络的传统认知，提出了适用于自监督 RL 的深层网络构建方案。实验表明，将网络深度从传统的 2-5 层扩展至 1024 层，在无演示、无奖励的无监督目标条件设置下，可显著提升自监督对比 RL 算法在模拟移动和操作任务中的性能，不仅提高任务成功率，还能催生更复杂的学习行为。同时强调了批次大小缩放对深层网络对比 RL 的重要性。</p><p><strong>评审评价：</strong> 突破了RL 与深层网络结合的技术瓶颈，提出的范式简单易实施，为 RL 的规模化发展提供了新路径。</p><h4>4.《Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446205" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 揭示了扩散模型避免训练数据记忆、实现泛化的核心机制—— 隐式动态正则化。通过理论分析与实验验证，识别出两个关键训练时间尺度：早期为数据集无关的泛化阶段（模型生成高质量样本），后期为数据集大小依赖的记忆阶段（训练超过该阶段会出现记忆现象）。其中泛化阶段时长随训练集规模线性增长，记忆阶段时长保持恒定，这一特性使模型在过参数化场景下仍能有效泛化。</p><p><strong>评审评价：</strong> 通过随机矩阵理论将实证观察与形式化理论统一，为生成式AI 的泛化机制研究树立了分析深度标杆，提供了可落地的训练指导。</p><h3>入围论文</h3><h4>1.《Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446206" alt=" " title=" " loading="lazy"/></p><p><strong>核心发现：</strong> 对"带可验证奖励的强化学习（RLVR）能赋予 LLM 全新推理能力" 的主流假设提出质疑。通过在多模型家族、多算法、多基准（数学、编程、视觉推理）上的系统测试，发现 RLVR 仅提升小 k 值下的 pass@k 分数（抽样效率），但无法激发新的推理模式 ——RLVR 模型的推理路径均包含在基础模型的抽样分布中，且训练会缩小推理能力边界；而蒸馏技术反而能引入新推理模式。</p><p><strong>评审评价：</strong> 该批判性发现具有重要学术价值，为推动RL 范式创新（如持续缩放、多轮智能体 - 环境交互）提供了明确方向。</p><p><a href="https://link.segmentfault.com/?enc=Y1eLDefiX4SntjT3qGwCxA%3D%3D.bIxsW729CxAy6hVLdWZ6ywzMmCAunddwlK170IiTk4jpXiOkZP3SRvEEFHjA%2BFPFYX46YFTWocP%2BZUaoh5j6F60tE%2FKzGsZPpMzaD1NTzV51dpuj65qTIFAV4yoCFYqqkDpo%2BIymMgNDAoQVW%2FhlvD48u5Ik4lIE97SBBegADpg%3D" rel="nofollow" target="_blank">👉一键Lab4AI阅读</a></p><h4>2. 《Optimal Mistake Bounds for Transductive Online Learning》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446207" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 解决了持续30 年的在线学习领域开放问题，精准量化了转导式在线学习与标准在线学习的性能差距。证明了对于 Littlestone 维度为 d 的概念类，转导式错误边界至少为 Ω(√d)，且该边界是紧的（存在对应概念类达到此边界），较此前的对数级下界实现指数级提升。同时改进了上界结果，揭示了转导式学习利用未标记数据可实现二次级性能提升，这与 PAC 设置下两者样本复杂度相近的特性形成鲜明对比。</p><p><strong>评审评价：</strong> 证明方法兼具创新性与严谨性，通过"路径树" 结构、稀疏编码、危险区域最小化等多种技术的融合，构建了最优学习算法，是学习理论领域的突破性成果。</p><h4>3. 《Superposition Yields Robust Neural Scaling》</h4><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047446208" alt=" " title=" " loading="lazy"/></p><p><strong>核心贡献：</strong> 提出表征叠加（LLM 表征的特征数超过维度）是神经缩放定律的核心驱动因素。基于 Anthropic 玩具模型的实验表明，弱叠加状态下，损失仅在数据特征频率呈幂律分布时遵循幂律缩放；而强叠加状态下，得益于表征向量的几何重叠，损失在广泛频率分布中均与模型维度呈逆幂律缩放。开源 LLM 的实证结果及 Chinchilla 缩放定律均验证了这一结论。</p><p><strong>评审评价：</strong> 超越了对神经缩放定律的单纯观察，深入揭示其内在机制，为优化缩放效果、预测缩放极限提供了关键理论支撑。</p><p>NeurIPS 2025的最佳论文奖项不仅表彰了在各自领域做出突破性贡献的研究，也反映了当前机器学习社区对可解释性、安全性、多样性及理论根基的日益重视。这些工作既有扎实的理论突破，也有影响深远的实践指导，预计将对未来的研究方向和业界实践产生重要影响。</p><p><a href="https://link.segmentfault.com/?enc=p593IsjAK5TxMOEFC1zHpg%3D%3D.GiDmgT5xv4vkYflsDj3JRU2TiqGsjka7fGGBy%2BnEmnZsm0TsrL%2B1ZKQlzgM7zSein9Shf5BPMPmZ4NoBuU9N0NbFIwvgFpXA5W6sMzPTucZyK6AjctWkhwI6PFubPjqo" rel="nofollow" target="_blank">👉参考链接</a></p><p>本文系学术转载，如有侵权，请联系大模型实验室Lab4AI小助手删文</p><h3>Lab4AI支撑“从研究到落地”</h3><p>大模型实验室Lab4AI实现算力与实践场景无缝衔接，<strong>具备充足的H卡算力</strong>，支持模型复现、训练、推理全流程使用，且具备灵活弹性、按需计费、低价高效的特点，解决用户缺高端算力、算力成本高的核心痛点。</p><p>Lab4AI.cn提供实验平台，提供一站式科研工具链！<br/><a href="https://link.segmentfault.com/?enc=kD3aPygybSVbgLLlDLA%2B%2BQ%3D%3D.itzBQ5HuuOBdFi0rV%2BND6c%2BZNNg%2BUwfrFECZE5Qx%2FWLbvm3MBB9e%2Fy4j5srXY6DV747RiV7XLDZcfJOc0%2Bpr4%2FZbprxGU0V8zQLKzvhxqeo%3D" rel="nofollow" target="_blank">👉一键直达</a></p><p><img referrerpolicy="no-referrer" src="/img/remote/1460000047413855" alt=" " title=" " loading="lazy"/></p>]]></description></item>  </channel></rss>