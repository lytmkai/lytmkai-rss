<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Mon, 26 Jan 2026 20:05:15 GMT</lastBuildDate><ttl>5</ttl><item><title>腾讯发力社交AI赛道，元宝内测“元宝派”玩法</title><description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月26日，腾讯旗下AI助手元宝低调开启全新社交AI玩法“元宝派”内测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从目前流出的内测截图来看，用户可以选择创建一个“派”，或者加入一个已有的“派”。用户可以在派内@元宝 或引用元宝的话，让元宝AI总结派内聊天、创建健身、阅读等兴趣打卡活动，由元宝AI担任“监督员”。不止文字聊天，用户还可以在派内进行“图片二创”，将一张普通的照片变成有趣的“梗图”或表情包，在共同创作中激发乐趣。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30092526ec3c4ef868fe24040ba55c1e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据网传截图，元宝派后续公测还将开放上线“一起看”、“一起听”玩法，该玩法接⼊了腾讯会议的⾳视频底层能⼒，让⽤户可以邀请派内好友同步观看一部电影、一场比赛、听一首歌。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;元宝派还打通了微信、QQ等社交产品体验，用户可以把“派号”或者专属邀请链接分享到微信朋友圈、或者微信、QQ好友，让好友一键丝滑加入元宝派。此前，元宝和微信、QQ已深度打通，不仅可以在微信、QQ添加“元宝”为联系人，随时随地和元宝AI互动，还能在公众号、视频号评论区@元宝，让TA总结内容、拓展提问。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ac/ac73119eec17854f1e5191309a62e294.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月25日，腾讯还宣布将在元宝APP内派发10亿现金红包。腾讯已经多年不参与春节“撒币”，本轮用10亿真金白银砸向元宝，在AI赛道加速的决心可见一斑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;腾讯路线：把AI带入群体交流中&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;腾讯将AI战局拉进了自身最擅长的“社交场”，试图用“元宝派&quot;打开一种全新的人与AI交互的模式。此举标志着腾讯正将AI应用的探索方向，从提升个人效率的“工具”属性，延伸至连接人与人、增进群体互动的属性，为AI应用的发展提供一个新的解题思路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于AI更自然、更深入进入人们日常生活的方式，业界的探索路径逐渐分野：其一，让AI模拟人类行为、完成复杂任务的Agent（智能体）被寄予厚望；同时，另一条路径也正浮现，即将AI带入群体交流中，使其成为社交互动的一部分，“元宝派”正是腾讯在此方向上的一次具体实践。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;多人沟通是人类最真实、最高频的沟通场景之一，也是对AI的上下文理解、多轮对话、意图识别等综合能力要求最高的场景。选择从这一场景切入，可以看出腾讯希望在最具挑战性的环境中，探索和打磨AI产品能力的决心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，不同于追求任务执行效率的逻辑，可以看出“元宝派”更关注AI在群体中的“社交价值”。它试图回答的问题是：当AI拥有了理解群体氛围、参与群体讨论、辅助群体决策的能力时，它将如何改变我们的线上社交体验？这一定位，旨在破解当前AI应用普遍面临的用户粘性不足、使用场景单一的挑战，通过引入真实、多维的社交关系，为AI的演进提供更丰富的土壤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;社交探索之外，腾讯在AI赛道2025年下半年也动作不断。从全模态的模型布局，到密集引进高阶人才、组织变阵，再到各业务线加速完成AI改造，腾讯开始找到自己在AI马拉松进程中的节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/klZi0LdorLWrHvvmMoOJ</link><guid isPermaLink="false">https://www.infoq.cn/article/klZi0LdorLWrHvvmMoOJ</guid><pubDate>Mon, 26 Jan 2026 10:55:46 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>我收集了12条技术社区疯传的Claude Prompt，如今这篇帖子火遍全网</title><description>&lt;p&gt;&amp;nbsp;近日，一篇关于 Claude 提示词（Prompt）的整理帖在海外技术社区迅速走红。发帖者是一位活跃在 X（原 Twitter）的国外网友，他声称自己系统性地收集了近一段时间在 Reddit、X 以及研究型社区中“被反复验证有效”的 Claude Prompt，并将其汇总成一份清单公开发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在帖子中，这位网友用了一种颇具传播性的说法来形容这些 Prompt 的效果——“可以在 60 秒内完成原本需要 10 小时的工作量”。尽管这一表述明显带有夸张成分，但并不妨碍该帖迅速在技术圈、研究圈和写作社区中被大量转发和收藏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0d/0dd7f9716fe2717a37246def52bf8ed6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与常见的“帮我写方案”“帮我改文案”类 Prompt 不同，这份清单中的 12 条提示（原作者提到共13条提示，但有一条是重复的，故最终为12条提示）几乎没有一条是直接要求模型“产出结果”的。相反，它们更多聚焦于质疑、拆解、对照和反思——这些原本属于研究人员、审稿人或资深从业者的认知工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ 翻译整理了该网友提出的12条Prompt，供参考：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1、“矛盾查找器”：非常适合用于论文、报告或长篇文档。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“列出所有内部矛盾、未解决的矛盾，或与证据不完全相符的论断。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;它能揭露人类忽略的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2、“审阅者#2”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“像持怀疑态度的同行评审员那样进行批判性评价。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要严厉批评。重点关注方法论缺陷、缺失的控制因素和过于自信的论断。残酷。必要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3、“将此内容转化为论文”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当你倾倒原始笔记、链接或不成熟的想法时，可以使用此功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“请将以下材料整理成一份结构化的研究简报。内容包括：关键论点、证据、假设、反驳论点和未决问题。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;标记任何薄弱环节或缺失之处。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4、“倒着解释”的技巧：非常适合检验真正的理解程度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“先解释这个结论，然后一步一步地倒推到假设条件。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果逻辑崩溃，你会立刻发现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;5、“像科学家一样进行比较”提示，不是功能列表，而是真正的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比较这两种方法：理论基础、失效模式、可扩展性和现实世界的限制。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;6、“什么会破坏它？”提示：用于预测。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“描述一下这种方法会造成灾难性失败的场景。不是极端情况，而是实际存在的故障模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大多数人从来不会问这个问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;7、“是什么改变了我的想法？”通常用于结尾&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“分析了所有这些之后，什么应该改变我目前的看法？”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这才是真正的研究人员的思考方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;8、 “一页纸思维模型”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“把整个主题浓缩成一个我能记住的单一思维模型。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果文件无法压缩，说明你还没有拥有它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;9、“跨域翻译”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“请用一个完全不同领域的类比来解释这个概念。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这不仅能带来理解，更能带来洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;10、“窃取结构”技巧&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条往往被低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通常用于分析文章的结构、流程和论证模式。在撰写优秀论文和文章时，请将其用于写作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;11、“像科学家一样进行比较”提示&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不是功能列表，而是真正的对比。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比较这两种方法：理论基础、失效模式、可扩展性和现实世界的限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;12、“假设压力测试”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这条信息直接来自研究论坛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“列出该论点所依赖的每一项假设。现在告诉我哪些最脆弱，以及原因。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pDlNcmOaTX2BYwSjBRBY</link><guid isPermaLink="false">https://www.infoq.cn/article/pDlNcmOaTX2BYwSjBRBY</guid><pubDate>Mon, 26 Jan 2026 10:43:35 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>奥特曼小号泄密：OpenAI代码工作100%交给Codex！工程师才揭底Codex“大脑”运行逻辑，碾压Claude架构？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;用一个 PostgreSQL 主库和 50 个只读副本，就顶住了 ChatGPT 上的 8 亿用户！&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近日，OpenAI的工程师们不仅爆出了这一惊人消息，还直接把Codex的“大脑”给扒了个精光。在OpenAI 官方工程博客主页，OpenAI 工程师、Technical Staff 成员 Michael Bolin发布了一篇文章，以“揭秘 Codex 智能体循环”为题，深入揭秘了 Codex CLI 的核心框架：智能体循环（Agent Loop），并详细讲解了 Codex 在查询模型时如何构建和管理其上下文，以及适用于所有基于 Responses API 构建智能体循环的实用注意事项和最佳实践。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些消息传出后，在Hacker News等技术论坛及社交平台上获得了高度关注。“看似平淡的技术最终会胜出。OpenAI 正在证明，优秀的架构远胜于花哨的工具。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;值得一提的是，有网友透露，前不久Anthropic 的一位工程师称“他们用于 Claude Code UI 的架构糟糕且效率低下”。而就在刚刚，X上出现一条爆料：Codex已接管OpenAI 100%的代码编写工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30f62549fbbf3d6a07e11b46de53ee61.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于“你们有多少百分比的编码工作是基于OpenAI模型进行”的问题，roon表示，“100%，我不再写代码了。”而此前，Sam Altman 曾公开发帖称，“roon是我的小号。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/79/79d713fd1f9241aadd05296b83b51079.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Codex的“大脑”揭秘&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“每个人工智能智能体的核心都是Agent Loop，负责协调用户、模型以及模型调用以执行有意义的软件工作的工具之间的交互。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，在 OpenAI 内部，“Codex”涵盖了一系列软件智能体产品，包括 Codex CLI、Codex Cloud 和 Codex VS Code插件，而支撑它们的框架和执行逻辑是同一个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3b/3b637b18b4cafda84135a8c2bbefa602.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agent Loop的简化示意图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，智能体会从用户那里接收输入，并将其纳入为模型准备的文本指令集，该指令集被称为提示词。下一步是通过向模型发送指令并要求其生成响应来查询模型，这个过程称为推理。推理过程中，文本提示词首先被转换为一系列输入token，随后被用于对模型进行采样，生成新的输出token序列。输出token会被还原为文本，成为模型的回复。由于token是逐步生成的，该还原过程可与模型的运行同步进行，这也是众多基于大语言模型的应用支持流式输出的原因。实际应用中，推理功能通常封装在文本API后方，从而抽象化词元化的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;推理步骤完成后，模型会产生两种结果：（1）针对用户的原始输入生成最终回复；（2）要求智能体执行某项工具调用操作。若为第二种情况，智能体将执行该工具调用并将工具输出结果附加至原始提示词中。该输出结果会被用于生成新的输入内容，再次对模型进行查询；智能体随后会结合这些新信息，重新尝试完成任务。这一过程会不断重复，直至模型停止发出工具调用指令，转而生成面向用户的消息（在 OpenAI 的模型中，该消息被称为助手消息）。多数情况下，这条消息会直接解答用户的原始请求，也可能是向用户提出的跟进问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于智能体可执行能对本地环境进行修改的工具调用，其 “输出” 并不仅限于助手消息。在很多场景下，软件智能体的核心输出是在用户设备上编写或编辑的代码。但无论何种情况，每一轮交互最终都会以一条助手消息收尾，该消息是智能体循环进入终止状态的信号。从智能体的角度来看，其任务已完成，操作控制权将交还给用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b0e6ff6a2f96d092cc253776e3ede3d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多轮智能体循环&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这意味着，对话内容越丰富，用于模型采样的提示词长度也会随之增加。而所有模型都存在上下文窗口限制，即其单次推理调用可处理的token最大数量，智能体可能在单次对话轮次中发起数百次工具调用，这有可能耗尽上下文窗口的容量。因此，上下文窗口管理是智能体的多项职责之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;这套智能体循环如何运行？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据介绍，Codex 正是借助响应 API 来驱动这套智能体循环的，博文曝出许多背后的实际运行细节，包括：&lt;/p&gt;&lt;p&gt;Codex不会把用户的话直接给大模型用，而是会主动“拼接”出一整套精心设计的提示词结构，且涵盖多个角色的指令、用户输入的一句话在结尾才出现。模型推理与工具调用之间可能会进行多轮迭代，提示词的内容会持续增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;构建初始提示词&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为终端用户，在调用响应 API 时无需逐字指定用于模型采样的提示词，只需在查询中指定各类输入类型，由响应 API 服务器决定如何将这些信息组织为模型可处理的提示词格式。在初始提示词中，列表中的每个条目均关联一个角色。该角色决定了对应内容的权重占比，优先级从高到低分为以下几类：系统、开发者、用户、助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f3/f3934b941f6e44fde60dc0dad6a69e08.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;响应 API 接收包含多个参数的 JSON 负载，其中三个核心参数有：&lt;/p&gt;&lt;p&gt;指令：插入模型上下文的系统（或开发者）消息工具：模型生成回复过程中可调用的工具列表输入：向模型传入的文本、图片或文件输入列表&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Codex 中，若已配置，指令字段的内容会从～/.codex/config.toml 配置文件中的模型指令文件读取；若未配置，则使用与该模型关联的基础指令。模型专属指令存储在 Codex 代码仓库中，并被打包至命令行工具中。工具字段为符合响应 API 定义的模式的工具定义列表。对于 Codex 而言，该列表包含三部分工具：Codex 命令行工具自带的工具、响应 API 提供且开放给 Codex 使用的工具，以及通常由用户通过 MCP 服务器提供的自定义工具。JSON负载的输入字段为一个条目列表。在添加用户消息前，Codex会先向该输入中插入以下条目：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1. 一条角色为开发者（role=developer）的消息，用于描述仅适用于工具部分中定义的Codex内置Shell工具的沙箱环境。也就是说，其他工具（如由MCP服务器提供的工具）并不受Codex的沙箱限制，需自行负责实施自身的防护规则。该消息基于模板构建，核心内容均来自打包在Codex命令行工具中的Markdown代码片段。&lt;/p&gt;&lt;p&gt;2.一条角色为开发者的消息，其内容为从用户的 config.toml 配置文件中读取的 developer_instructions 配置值。&lt;/p&gt;&lt;p&gt;3.一条角色为用户的消息，其内容为用户指令；该内容并非来源于单个文件，而是从多个数据源聚合而来。一般而言，表述越具体的指令，排序越靠后：&lt;/p&gt;&lt;p&gt;加载 $CODEX_HOME 目录下 AGENTS.override.md 和 AGENTS.md 文件的内容在默认 32 千字节的大小限制内，从当前工作目录对应的 Git / 项目根目录（若存在）向上遍历至当前工作目录本身，加载任意 AGENTS.override.md、AGENTS.md 文件的内容，或加载 config.toml 配置文件中 project_doc_fallback_filenames 参数指定的任意文件内容若已配置相关技能，则补充以下内容：关于技能的简短引言、各技能对应的技能元数据、技能使用方法说明章节。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;一条角色为用户的消息，用于描述智能体当前的运行本地环境，其中会明确当前工作目录及用户所使用的终端 Shell 信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当 Codex 完成上述所有计算并完成输入初始化后，会追加用户消息以启动对话。需注意的是，输入中的每一个元素都是一个 JSON 对象，包含类型、角色和内容三个字段。当 Codex 构建好要发送至响应 API 的完整 JSON 负载后，会根据～/.codex/config.toml 中响应 API 端点的配置方式，携带授权请求头发起 HTTP POST 请求（若有指定，还会添加额外的 HTTP 请求头和查询参数）。当 OpenAI 响应 API 服务器接收到该请求后，会使用 JSON 数据来推导出模型的提示信息，（需要说明的是，Responses API 的自定义实现可能会采用不同的方法）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可见，提示词中前三项的顺序由服务器决定，而非客户端。也就是说，这三项里仅系统消息的内容同样由服务器控制，工具与指令则均由客户端决定。紧随其后的是JSON负载中的输入内容，至此提示词拼接完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;模型采样&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;提示词准备就绪后，模型才开始进行进行采样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一轮交互：此次向响应 API 发起的 HTTP 请求，将启动 Codex 中对话的第一轮交互。服务器会以服务器发送事件（SSE）流的形式进行响应，每个事件的数据均为一个 JSON 负载，其type字段以response开头。Codex接收该事件流并将其重新发布为可供客户端调用的内部事件对象。`response.output_text.delta`这类事件用于为用户界面实现流式输出功能，而`response.output_item.added`等其他事件则会被转换为对象，附加至输入内容中，为后续的响应API调用所用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;若首次向响应API发起的请求返回两个`response.output_item.done`事件，一个类型为推理（reasoning），一个类型为函数调用（function_call），那么当结合工具调用的返回结果再次向模型发起查询时，这些事件必须在JSON的输入字段中进行体现。后续查询中用于模型采样的最终提示词结构如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ae/ae4b6e2c78ddc6b2116f05bc03a52262.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;需要特别注意的是，旧提示词是新提示词的完整前缀。这一设计是有意为之的，因为它能让用户借助提示词缓存提升后续请求的效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Codex命令行工具中，会将助手消息展示给用户，并聚焦输入编辑区，以此提示用户轮到其继续对话。若用户做出回应，上一轮的助手消息以及用户的新消息均需附加至响应API请求的输入字段中，从而开启新一轮对话。同样，由于对话处于持续进行的状态，发送至响应API的输入内容长度也会不断增加。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/01/01f5394fa51b8486df2e18923a885a62.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;弃用简单参数费力做优化，就为了用户隐私？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“在对话过程中，发送至响应API的JSON数据量，是否会让智能体循环的时间复杂度达到二次方级别？”答案是肯定的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，尽管响应API支持通过可选的previous_response_id参数缓解这一问题，但目前Codex并未启用该参数，主要是为了保证请求完全无状态，并兼容零数据保留（ZDR） 配置，即不存储用户对话数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;取而代之的，是两套需投入大量研发精力、涉及复杂实施流程的技术策略。文中，OpenAI详细介绍了这两项硬核优化的具体方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通常，模型采样的开销远高于网络传输的开销，采样环节会成为优化效率的核心目标，这也是提示词缓存至关重要的原因，它能复用前一次推理调用的计算结果。当缓存命中时，模型采样的时间复杂度将从二次方降至线性。OpenAI相关的提示词缓存文档对这一机制有更详细的说明：仅当提示词存在完全匹配的前缀时，才有可能实现缓存命中。为充分发挥缓存的优势，需将指令、示例等静态内容置于提示词开头，而将用户专属信息等可变内容放在末尾。这一原则同样适用于图片和工具，且其内容在各次请求中必须保持完全一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于这一原则，Codex中可能有以下导致缓存未命中的操作：&lt;/p&gt;&lt;p&gt;在对话过程中修改模型可调用的工具列表；更换响应API请求的目标模型（实际场景中，这会改变原始提示词中的第三项内容，因该部分包含模型专属指令）；修改沙箱配置、审批模式或当前工作目录。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，Codex团队在为命令行工具开发新功能时，必须审慎考量，避免新功能破坏提示词缓存机制。例如，他们最初对MCP工具的支持曾出现一个漏洞：工具的枚举顺序无法保持一致，进而导致缓存未命中。需要注意的是，MCP工具的处理难度尤为突出，因为MCP服务器可通过notifications/tools/list_changed通知，动态修改其提供的工具列表。若在长对话过程中响应该通知，极易引发高成本的缓存未命中问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在可能的情况下，针对对话过程中发生的配置变更，他们会通过在输入中追加新消息的方式体现变更，而非修改已有的早期消息：&lt;/p&gt;&lt;p&gt;若沙箱配置或审批模式发生变更，我们会插入一条新的role=developer消息，格式与原始的条目保持一致；若当前工作目录发生变更，我们会插入一条新的role=user消息，格式与原始的条目保持一致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，为保障性能，OpenAI在实现缓存命中方面投入了大量精力。除此之外，他们还重点管理了一项核心资源：上下文窗口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其规避上下文窗口耗尽的通用策略是：一旦词元数量超过某个阈值，就对对话进行压缩。具体来说，会用一个更精简、且能代表对话核心内容的新条目列表替代原有输入，让智能体在继续执行任务时仍能理解此前的对话过程。早期的压缩功能实现方案，需要用户手动调用/compact命令，该命令会结合现有对话内容和自定义的摘要生成指令，向响应API发起查询；Codex则会将返回的、包含对话摘要的助手消息，作为后续对话轮次的新输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此后，响应API不断迭代，新增了专用的/responses/compact端点，能以更高效率完成压缩操作。该端点会返回一个条目列表，可替代原有输入继续对话，同时释放出更多的上下文窗口空间。该列表中包含一个特殊的type=compaction条目，其附带的encrypted_content加密字段为透明化设计，可保留模型对原始对话的潜在理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，当词元数量超过auto_compact_limit自动压缩阈值时，Codex会自动调用该端点对对话内容进行压缩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;极限扩容：用1个数据库扛住了8亿用户&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在另一篇技术博文中，OpenAI工程师Bohan Zhang介绍，&amp;nbsp;OpenAI 通过严苛的技术优化与扎实的工程实践，对单个数据库 PostgreSQL 进行深度扩容，实现以单套体系支撑 8 亿用户、每秒数百万次查询的访问需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据称，多年来，PostgreSQL 一直是支撑 ChatGPT、OpenAI API 等核心产品的核心底层数据系统之一。过去一年，公司 PostgreSQL 的负载增长超 10 倍，且这一增长趋势仍在持续加速。OpenAI称，PostgreSQL 的横向扩展能力远超此前行业普遍认知，能够稳定支撑规模大得多的读密集型工作负载。“这套最初由加州大学伯克利分校的科学家团队研发的系统，助力我们通过单主节点 Azure PostgreSQL 弹性服务器实例，搭配分布在全球多个区域的近 50 个只读副本，承接了海量的全球访问流量。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且，OpenAI表示，其扩容在实现规模提升的同时，始终将延迟控制与可靠性优化放在核心位置：生产环境中，客户端 99 分位延迟稳定保持在十几毫秒的低水平，服务可用性达到五个九标准；过去 12 个月内，PostgreSQL 仅出现过一次零级严重故障，该故障发生在 ChatGPT 图像生成功能爆红上线期间，一周内超 1 亿新用户注册导致写流量突发暴涨超 10 倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管 PostgreSQL 的扩容成果已达预期，OpenAI仍在持续探索其性能极限。目前，他们已将可分片的写密集型业务负载迁移至 CosmosDB 等分片式数据库系统；对于分片难度更高的剩余写密集型负载，相关迁移工作也在积极推进，以此进一步减轻 PostgreSQL 主节点的写压力。同时，OpenAI正与微软 Azure 团队展开合作，推动级联复制功能落地，实现只读副本的安全、大规模扩容。随着基础设施需求的持续增长，其将继续探索更多扩容方案，包括基于 PostgreSQL 的分片架构改造、引入其他分布式数据库系统等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有网友评价道，“这招的高明之处，就在于极简。他们没用什么花里胡哨的冷门技术，不过是把最佳实践做到了极致。过去十年，行业里全是 ‘一切皆分片、拥抱 NoSQL、全面分布式，为 CAP 定理折腰’ 的论调，而 OpenAI 倒好， 服务十亿级用户的解法，居然只是一句‘试过加只读副本吗？’”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/unrolling-the-codex-agent-loop/&quot;&gt;https://openai.com/index/unrolling-the-codex-agent-loop/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openai.com/index/scaling-postgresql/&quot;&gt;https://openai.com/index/scaling-postgresql/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lqDSzHGW2eYdUxZQjuYN</link><guid isPermaLink="false">https://www.infoq.cn/article/lqDSzHGW2eYdUxZQjuYN</guid><pubDate>Mon, 26 Jan 2026 09:46:48 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>慕了！内存芯片巨头年终奖人均64万；32岁程序员猝死背后公司被扒，曾给39万“封口费”；马斯克曝星舰成本将降99%，商业航天受捧｜AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;慕了！内存芯片巨头人均发 64 万年终奖；月之暗面总裁张予彤：Kimi 仅使用美国顶尖实验室 1% 的资源；32 岁程序员猝死背后公司：不配合工伤认定，曾给 39 万“封口费”；黄仁勋现身上海菜市场；大清洗！大众裁员 3.5 万人，包括 1/3 高管；微信聊天不能导出和分析；腾讯回应开源项目被下架；苹果 Siri「偷听」集体诉讼和解，美国用户开始获赔 9500 万美元；传阿里旗下芯片公司平头哥拟独立上市；马斯克：星舰今年目标全复用，成本将下降 99%；TikTok 宣布“美国方案”：成立数据安全合资公司，字节保留算法知识产权；黄景瑜成为中国首批商业航天太空旅客……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;慕了！内存芯片巨头人均发64万年终奖&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月19日，据韩媒报道，全球内存芯片行业的三巨头之一SK海力士宣布，将向全体员工发放人均超1.36亿韩元（约合64万元人民币）的绩效奖金，创公司历史最高纪录。据报道，SK海力士为员工提供以公司股票形式领取绩效奖金的选项，即“股东参与计划”。根据股东参与计划，员工可以选择将其年终奖的最多50%以公司股票形式领取。持有这些股票满一年的员工，将获得相当于购买金额15%的额外现金奖励。例如，一位获得1亿韩元年终奖的员工如果选择顶格持股，将获得价值5000万韩元的股票。若持有一年，该员工还将额外获得750万韩元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2024年，由于半导体行业低迷，SK海力士推出了该计划。但由于当年仅发放了慰问金而未发放绩效奖金，因此当时未能实施。该计划从去年开始向员工提供购股选择。随着去年下半年劳资双方达成的新指南于本月底生效，SK海力士员工的奖金金额预计将大幅增加。根据新协议，此前绩效奖金发放最高限额为1000%（即10个月基本工资）的规定已被废除。取而代之的是，以上一年度营业利润的10%作为年终奖，其中80%的绩效奖金在当年发放，剩余20%分两年递延发放，并享受每年10%的利息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;鉴于SK海力士去年全年营业利润预计为45万亿韩元，员工总数为3.3万人，因此预计每位员工的绩效奖金约为1.36亿韩元（约合64万元人民币）。不过，“股东参与计划”也存在潜在变数。SK海力士在公告中提示，韩国国会正在推进《公司法第三次修订案》审议，其中明确要求企业回购的自有股必须注销。若法案在本月或3月正式通过，企业将无法用自有股开展员工激励，本次持股计划或面临调整甚至取消。相关修订案将于1月21日进入议案审查小组讨论环节。行情数据显示，得益于人工智能热潮，SK海力士的股价在2025年内涨幅达275%。同时，今年年初，SK海力士就表示其2026年的全部芯片产能已售罄。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;月之暗面总裁张予彤：Kimi仅使用美国顶尖实验室1%的资源&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日，月之暗面Kimi总裁张予彤出席在瑞士达沃斯举行的世界经济论坛2026年年会时表示，Kimi仅使用美国顶尖实验室1%的资源，就开放出Kimi K2、Kimi K2 Thinking这样全球领先的开源模型，甚至在部分性能上超越美国的顶尖闭源模型。张予彤透露，Kimi投入大量精力将工程化思维引入研究环节，确保所有算法创新都能在生产系统中大规模稳定运行，据她透露，Kimi最新模型将很快发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月19日，据外媒报道，两位知情人士透露，月之暗面Kimi在最新一轮融资中估值达到 48 亿美元（约合330亿元人民币），而就在几周前，该公司的估值还为 43 亿美元。消息人士称，由于市场需求旺盛，此轮融资可能很快就会完成。截至本文发布时，月之暗面尚未回复置评请求。消息人士补充说，由于市场对中国 AI IPO候选企业的兴趣激增，该公司在后续几轮融资中的估值可能会更高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;32岁程序员猝死背后公司：不配合工伤认定，曾给39万“封口费”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2025年11月29日，32岁程序员高广辉（视源股份任职7年）因长期工作强度大、频繁熬夜，在身体不适晕倒送医后抢救无效死亡，病历标注其“经常熬夜，工作强度大”。事发当日，他送医前仍叮嘱妻子“带上电脑”计划住院工作，急救期间及离世8小时后仍收到同事工作消息，当日曾5次登录公司OA系统，但公司隐藏后台时间，给工伤认定带来阻碍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;高广辉的工作状态极具代表性：公司实行“弹性工作制”却无加班费，他需同时承担研发、管理、售后、拉业务等多项职责，入职7年底薪始终3000元，依赖“多劳多得”维持税前2.9万元月薪，且面临每季度末位淘汰压力，长期深夜甚至凌晨回家成为常态。他曾在GitHub开源“反996”项目，提醒同行“不要被任务压垮”，其离世后该项目被大量悼念留言刷屏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;事发后，视源股份的处理引发争议：6天后向家属支付39万元“人道主义抚恤金”，实则附带“负面评价需支付50万违约金”的“封口费”条款；删除高广辉企业微信工作号、撤掉工位、丢失部分遗物，并要求全体员工不得提及此事，且不配合工伤认定相关调查。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“大概就是炒掉我老公的N+1钱，其实就是封口费，说如果我对公司造成负面评价违约金50万，当时被逼的没办法所以签了。”高广辉妻子对凤凰网说道，后续因为高父想要提前分遗产，分走了三十多万元，“其实（这笔钱）我只拿到四到六万。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据报道，视源股份是一家业务涵盖教育、办公、AI、汽车电子等多领域的上市公司，技术人员占比超58%，资料显示“无刚性考勤、人性化管理”，且福利优厚，但背后是普遍的强制加班文化。前员工反映，加班至深夜是常态，休假、就医需随身携带电脑，拒绝加班会影响绩效，部分员工因长期高压出现躯体化症状。公司近年盈利持续下滑，正推进H股上市，业务扩张压力最终转嫁至一线员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前高广辉的工伤认定仍在调查中，律师表示，若能提供工作记录、加班文化等证据证明猝死与工作高度相关，可认定为工伤，居家办公也可视为工作时间延伸。高广辉家属希望此事能敲响职场警钟，推动职场环境改革，避免类似悲剧重演。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄仁勋现身上海菜市场&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;昨天，黄仁勋现身陆家嘴街道乳山路锦德菜市场，体验上海市井风俗。此前，多位知情人士透露，黄仁勋再度来华，首站到访了英伟达位于上海的新办公室，与员工见面并回答了诸多员工关注的问题，同时回顾公司2025年主要事件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大清洗！大众裁员3.5万人，包括1/3高管&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日，德国汽车制造商大众汽车周三表示，其核心品牌集团计划削减管理岗位并整合生产平台，目标是到2030年节约10亿欧元。该公司在声明中表示，计划在2026年夏季前将大众核心品牌集团的董事会成员数量减少约三分之一。报道称，这意味着董事会职位将从29个减少到19个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该公司补充说，大众乘用车、斯柯达和西雅特/Cupra等品牌未来将各有四名董事会成员——一位首席执行官，加上财务、销售和人力资源主管--而开发、采购和生产将由位于沃尔夫斯堡的汽车制造商总部处理。它表示，将在中期内进一步逐步精简核心品牌集团内部的管理结构。该公司表示，核心品牌集团的20多家全球运营工厂将被组织成五个生产区域，区域经理将承担跨品牌和跨国家的责任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大众汽车正努力应对工业放缓、来自中国的激烈竞争和昂贵的关税，到2030年将在德国削减3.5万个工作岗位。媒体曾报道称，大众核心品牌集团首席执行官托马斯·谢弗预计的节约成本由6亿欧元的人力成本和4亿欧元的生产效率组成。大众汽车的此次改革，远不止于财务上的“节流”。它标志着集团在向电动化与数字化转型的关键时期，正试图重塑其庞大的管理体系——通过削减层级、整合资源与强化区域协调，构建一个更敏捷、更高效的运营模式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;&amp;nbsp;&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;微信聊天不能导出和分析？腾讯回应开源项目被下架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日下午消息，近日在小红书等社交媒体上，有用户发帖表示，腾讯要求全球最大代码托管与协作平台GitHub全面下架一批微信开源项目，相关项目主要涉及“允许用户导出或分析自己微信聊天记录”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这一话题引发网友讨论，部分网友质疑表示，像导出个人聊天记录、清理微信缓存这类基础功能工具，本该是提升用户体验的标配，可微信不仅自己不提供，还不允许用户自主优化使用体验。这是否说明用户对微信数据没有自主权。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，腾讯方面回应称，部分读取微信聊天记录的开源项目，是通过对微信客户端进行逆向工程等手段，破解本地数据库的密钥，以绕过微信客户端的加密措施，威胁用户本人及第三方数据隐私与客户端安全，且极易被黑灰产利用。腾讯依据相关法规向开源平台提出请求，并获得了平台支持，大部分项目也在平台沟通后选择主动移除违规内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;苹果 Siri「偷听」集体诉讼和解，美国用户开始获赔 9500 万美元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;苹果去年年初就 Siri「非法且故意录音」集体诉讼达成和解。该事件可追溯至 2019 年，苹果曾否认指控并加强隐私保护。从去年年中起受影响用户可提交索赔申请，如今部分用户已陆续收到赔付款。用户需在 2014 年 9 月 17 日至 2024 年 12 月 31 日购买过支持 Siri 的设备且经历过「非预期激活 Siri」才能申请。本次赔付总价值 9500 万美元，每名用户最多申报 5 台设备，最初预计每台赔 20 美元、单人最多 100 美元，最终每台约 8.02 美元、最多 40.1 美元。成功参与和解的用户，直接转账的赔付款从前天开始陆续到账，选择支票 / 礼品卡形式的需留意邮箱或实体邮箱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;传阿里旗下芯片公司平头哥拟独立上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日下午消息，接近市场人士称，阿里巴巴集团已决定支持旗下芯片公司平头哥未来独立上市。平头哥是阿里巴巴集团旗下全资芯片公司，2018年成立以来，在行业中非常低调，是阿里雪藏多年的“核武器”。如今平头哥芯片正式浮出水面。阿里方面对此消息未作评论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;截至目前，平头哥在算力芯片领域推出AI推理芯片含光800、CPU倚天710以及AI芯片PPU，在存储芯片领域推出SSD主控芯片镇岳510，在网络芯片领域据称也将推出相关芯片，已布局数据中心全栈芯片。平头哥还在端侧芯片推出羽阵IoT芯片，已实现数亿出货，布局覆盖云端和终端。此外，2026年1月2日，百度集团宣布，昆仑芯已于1月1日通过其联席保荐人向香港证券交易所提交了上市申请表。在拟议的分拆完成后，昆仑芯预计仍将作为子公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马斯克：星舰今年目标全复用，成本将下降99%&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2026年1月22日，特斯拉首席执行官埃隆·马斯克在达沃斯世界经济论坛与贝莱德首席执行官拉里·芬克交谈时，阐述了其在太空探索、人工智能、机器人及自动驾驶领域的愿景与规划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在太空探索领域，马斯克表示SpaceX今年的目标是实现“星舰（Starship）”的完全可重复使用。作为有史以来最大的飞行器，星舰若达成该目标，将使进入太空的成本降至当前的1%（每磅100美元以下）。此外，他还提及未来两到三年计划发射太阳能驱动的人工智能卫星，由于太空中太阳能持续充足且无大气干扰，太阳能电池板效率将是地球的五倍，未来太空或将成为部署人工智能成本最低的地方，而此前他曾提出未来四到五年内将大规模人工智能系统部署到轨道的设想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在机器人领域，马斯克透露特斯拉计划明年年底向公众销售人形机器人Optimus，目前该机器人已在工厂执行简单任务，预计今年年底将具备更复杂功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自动驾驶方面，马斯克称自动驾驶汽车“基本是已解决的问题”，特斯拉“全自动驾驶”软件每周都会更新，部分保险公司已为使用该技术的客户提供半价保险。目前特斯拉已在美国多个城市推出自动驾驶出租车服务，计划今年年底在美国大规模推广，欧洲则有望下月获得全面自动驾驶监管批准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，马斯克此前在社交媒体X上表示，Cybercab（无人驾驶出租车）和Optimus的生产速度将呈S型曲线逐步加快。他还展望，今年年底或最迟明年，人工智能将“比任何人更聪明”，其公司的总体目标是“最大限度提高人类文明拥有美好未来的可能性”，并“将意识扩展到地球之外”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;资本市场上，商业航天概念板块热度颇高，价格波动情况受到市场广泛关注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;TikTok 宣布“美国方案”：成立数据安全合资公司，字节保留算法知识产权&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;北京时间 1 月 23 日，TikTok 发布公告称已成立 TikTok 美国数据安全合资有限责任公司，负责 TikTok 美国的数据保护等业务，字节跳动继续拥有 TikTok 算法知识产权并授权其使用。TikTok 美国公司由字节跳动全资控股，负责电商等商业活动及全球产品互联互通。这意味着 TikTok 美国方案正式落地，超 2 亿美国用户可继续使用。合资公司中，甲骨文、银湖资本、MGX 各持股 15%，字节跳动保留 19.9% 股份为最大单一股东，由七人董事会管理。TikTok 美国公司保障产品全球内容互联、体验一致，其商业活动是重要收入来源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;黄景瑜成为中国首批商业航天太空旅客&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日，黄景瑜以「009 号太空游客」身份官宣成为中国首批商业航天太空旅客，计划 2028 年搭乘国产可重复使用载人飞船飞赴亚轨道太空，同行者包括中国工程院院士李立浧等人。他们将乘坐亚轨道载人飞船穿越卡门线，体验至少 5 分钟失重并俯瞰地球。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据穿越者此前消息，穿越者已开启船票预售，预售船票300万/张，预付10%可锁定名额，目前已签约来自学界、商界、航天界、艺术界、娱乐界、网红界等领域的十余位付费太空游客。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智元机器人CMO邱恒是“中国001号商业航天员”。邱恒于2023年自费购买中国首张商业航天“太空船票”，成为首位签约亚轨道飞行的普通人。他表示，人形与四足机器人能创造无限生产力，这样的生产力正加速服务工业、商业，未来也有可能助力人类拓展新的疆域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2023年11月，穿越者与首位太空游游客签约。据穿越者官方消息，目前穿越者载人飞船的签约游客有中国工程院院士李立浧、旅美诗人林小颜、广州正佳集团副董事长兼首席执行官谢萌、星河动力CEO刘百奇、航天垂类大V博主@NASA爱好者等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月18日，穿越者自主研制的穿越者壹号（CYZ1）载人飞船试验舱，完成着陆缓冲系统的综合验证试验。穿越者介绍，这是我国商业航天领域首个载人飞船全尺寸试验舱着陆缓冲关键技术验证项目，此次试验的成功，标志着穿越者已成为全球第三家研发并验证了载人飞船着陆缓冲技术的商业航天企业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;穿越者官方资料显示，穿越者成立于2023年1月，是中国首家商业的“载人航天科技”有限公司，专注可重复使用载人飞船研制和太空旅游运营，计划3-4年首先完成亚轨道可重复使用载人飞船研制，2028年前后实现中国乃至亚洲的太空旅游。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 计划于今年下半年推出首款硬件设备&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 19 日消息，据 Axios 网站报道，OpenAI 政策主管克里斯 · 莱恩周一在达沃斯论坛透露，公司“正按计划”于 2026 年下半年推出其首款设备。OpenAI 首席执行官萨姆・奥特曼和苹果前首席设计师乔尼・艾维去年 5 月份曾透露，该公司正在研发一系列硬件产品，有望于 2026 年正式亮相。彭博社记者马克・古尔曼去年 11 月曾爆料，OpenAI 正在从苹果的硬件工程团队中大肆挖人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，其合作开发的神秘 AI 硬件已拥有首个原型机，奥尔特曼称原型机的表现“令人惊叹”，并表示项目正按预期推进，并透露该产品将在两年内投入生产。在本次活动中，两人阐述了该设备的核心理念：彻底改变人们使用计算机的方式。他们认为，当前的智能设备如同“走在纽约时代广场”，充斥着各种干扰信息，无法带来平静。因此，这款新设备旨在创造一种“坐在湖边小屋”般的宁静体验，让用户能专注于真正重要的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;DeepSeek 新模型“Model 1”曝光，疑似“高效推理模型”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 21 日下午消息，DeepSeek 于官方 GitHub 仓库更新了一系列 FlashMLA 代码，在这些更新中，一个名为“Model 1”的模型引起了广泛关注。据悉，目前这个还很神秘的 Model1 不仅出现在了代码与注释中，甚至还有与 DeepSeek-V3.2 并驾齐驱的文件。这也不禁引发广大网友猜测，认为 Model 1 很可能就是传闻中 DeepSeek 将于春节前后发布的新模型代号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最新消息显示，Model1 是 DeepSeek FlashMLA 中支持的两个主要模型架构之一，另一个是 DeepSeek-V3.2。这很可能是一个高效推理模型，相比 V3.2 内存占用更低，适合边缘设备或成本敏感场景。此外，它也可能是一个长序列专家，针对 16K+序列优化，适合文档理解、代码分析等长上下文任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里推出 AIGC 设计应用“呜哩 (Wuli)”，集成通义千问图像模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，阿里巴巴推出了一款名为“呜哩”的 AIGC 创意设计生产力平台，并已正式开启测试。该平台旨在为内容创作者、设计师及营销人员提供一套高效多元的 AI 创意生成解决方案。平台深度整合了通义千问团队研发的多款图像大模型，形成一个模型全家桶。其中包括主打高质量的 Qwen Image25.12 生成模型、追求极致响应速度的 Qwen Image Turbo 模型，以及专注于细节调整的 Qwen Image25.11 编辑模型。用户可根据不同创作需求，在生成质量、速度与可控性之间灵活选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在功能层面，呜哩平台提供了从图片生成、视频生成到灵感联想、翻译辅助及资源库支持的完整具集，可帮助用户跨越创意瓶颈。用户通过输入简单的描述，即可快速生成如 3D 艺术字体、电影风格海报、电商场景图在内的丰富内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;百川推出最低幻觉循证增强医疗大模型M3 Plus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月22日，百川智能正式发布Baichuan-M3 Plus，严肃医疗场景下的问答准确性、可靠性，再次刷新了刚刚推出的M3所创下的世界纪录。凭借独创的六源循证技术与M3基座结合，M3 Plus将幻觉率降低至2.6%，低于Open Evidence，达到全球最低水平；首创“证据锚定”技术，不仅给出引文来源，还能将模型生成的每一句医学结论，精确锚定到原始论文中的对应证据段落。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;微软 Copilot 测试 Real Talk 和视频生成功能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软正邀请全球用户测试 Copilot 的 Real Talk 功能，并开始测试视频生成能力，以应对 Google Gemini 和 ChatGPT 的竞争。网络分析公司 SimilarWeb 数据显示，Copilot 网页端市场份额约为 1%。为提升竞争力，微软推出两项核心更新：Real Talk 模式与视频生成功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Real Talk 模式通过“深度”和“写作风格”两个维度调整 AI 回应方式。系统可根据对话历史自动选择对话深度（如标准、压缩）和风格（如休闲），并允许用户随时查看 AI 的思维过程，旨在实现更接近人类的对话体验。该模式下，Copilot 不再以程式化方式回应问题，而是表现出真实的好奇心。面对不合理提问时，会主动反驳并表达对特定话题的兴趣，而非对所有问题均表现出虚假热情，从而提升互动活跃度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在安卓版 Copilot 中已发现“生成视频”功能开关，测试表明可创建最长 8 秒且包含音频的视频片段。目前尚不明确该视频功能基于微软自研或 OpenAI 的 Sora 模型，但已在基础版 Microsoft 365 订阅提供，未设置额外付费门槛。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;文心App秘密筹划界面改版，将新增“多人多Agent”群聊功能&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1月21日消息，文心App近期将启动交互界面改版，新增“多人、多Agent群聊”功能，以更加社交化、活人感的方式进行用户交互。目前，文心App群聊功能已开启内测，将很快与用户见面。据百度内部人士透露，这是国内AI应用首次从“一对一助手”进化为参与人类社交与协作的“智能成员”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，文心App群聊功能将支持用户在同一群聊中调动多个AI角色，适用职场创意脑暴、办公协作、家庭成员间生活协同、趣味互动等场景。AI能理解群聊上下文、识别成员意图，并根据讨论氛围精准判断介入时机，主动应答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据相关人士透露，预计今年2月，文心App还将新增支持群聊内给自己或别人布置日程提醒，支持自定义个人的文心助手人设和回复风格，支持图生图能力和特色玩法类Agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，文心App群聊功能已在内测阶段。百度内部人士表示：“我们其实没有考虑过要做一个微信或者取代微信，目前没有考虑，我相信大家也都感觉这个不现实，我们都是从需求和任务本身去出发的，大家如果真的有特定的任务，比如说家庭健康的任务、小组作业的任务、旅游规划的任务，或者是一些特别复杂的决策，我们希望提供一个平台给大家，大家可以尝试在这里面跟你的一些同事朋友去借助AI的能力，更好地完成你们的任务。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;X 平台正式开源推荐算法，马斯克：没有其他社交媒体这么做&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，马斯克宣布正式开源新的 X 平台算法，该算法由与 xAI 的 Grok 模型相同的 Transformer 架构驱动。马斯克坦言，我们知道这种算法很笨拙，还需要大幅改进。但至少可以看到 X 平台在实时、透明的情况下努力让它变得更好，没有其他社交媒体公司这样做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，该算法与当前运行的生产系统相同，并由 xAI 的 Grok 提供支持。完整代码已发布在 GitHub 上，并将每 4 周更新一次。用户的推荐内容会结合关注账号的帖子与 X 上发现的帖子，然后使用基于 Grok 的转换器进行排序。该转换器预测用户可能喜欢、回复、转发、点击或观看的几率。模型不仅预测单一分数，而是预测多种行为并综合它们，以实现更细致的排序。所有内容均直接从用户行为中学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;特斯拉发布第二代人形机器人摆件：199 元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 20 日，特斯拉中国官网发布 Tesla Bot 摆件（生肖盲盒版），售价 199 元，将于 1 月 21 日 10:00 开售。据介绍，Tesla Bot 系列摆件是以 1:10 比例打造的可动收藏玩偶，由 40 多个独立零件组成，配备 20 个关节点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/651098d8452274b1476c550b97e32e77.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;产品尺寸为 5.5cm x 18.2cm，净重约 25 克。无论是外观细节还是动作表现，均高度还原第二代人形机器人。用户可以自由调整摆件姿势，如双手抱拳的拜年造型、手持节日道具的俏皮模样。&lt;/p&gt;&lt;p&gt;据了解，盲盒内还有神秘嘉宾，将以 10% 的概率随机出现，可能是身穿新年限定服饰的 Tesla Bot（马年生肖特别版），也可能是 Tesla Bot 的神秘好友。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;苹果内部“ChatGPT”曝光：能AI写代码、改文案、分析文件等&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日消息，日前，据外媒报道，苹果公司于 2025 年 11 月在内部推出名为 Enchanté 的类 ChatGPT 聊天机器人，以及名为“企业助手”（Enterprise Assistant）的知识中心应用。消息称 Enchanté 界面酷似 macOS 版 ChatGPT，但专为苹果严格的安全需求定制。媒体援引博文介绍，Enchanté 仅运行苹果批准的模型（含苹果自研基础模型以及 Claude 和 Gemini），且完全在本地或私有服务器上运行。该工具不仅能协助员工完成创意构思、代码开发和校对工作，还能深度分析员工上传的文档和图像，且杜绝了任何向第三方发送敏感数据的风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一款名为“企业助手”（Enterprise Assistant）的应用则更具针对性，它充当了苹果员工的中央知识库。该工具完全基于苹果内部的大语言模型（LLM）构建，整合了海量的内部政策与技术文档。员工可通过它快速查询高管职责、商业行为准则、健康保险福利，甚至包括“如何在 iPhone 上配置 XXX”等具体技术指南，极大地简化了信息检索流程。这两款应用均内置了反馈机制，允许员工对 AI 的回答质量进行评分，甚至能将苹果自研模型的回答与第三方模型进行“同屏比对”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虽然苹果尚未公开具体细节，但行业普遍认为，这些来自工程、设计及营销等跨部门的高质量内部数据，将直接用于训练和微调其基础模型。该媒体认为这种“内部试错”的模式，或许正是苹果在 Apple Intelligence 发展受阻的背景下，试图通过提升模型硬实力来打破僵局的关键一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 月 22 日，手机厂商 vivo 在近期叫停了 AI 眼镜项目。这一项目此前已秘密筹备半年时间，并已与歌尔、中科创达在内的多家 ODM 厂商合作 demo。原因是vivo 执行副总裁胡柏山在内的多位高层判断，其 AI 眼镜“在当下很难做出差异化”。报道指出，叫停 AI 眼镜项目之后，vivo 将继续聚焦混合现实（MR）方向。1 月 20 日，OpenAI 宣布，将为旗下 ChatGPT 个人版推出年龄识别模型，助力这家人工智能企业识别出未满 18 周岁用户的账号。该公司表示，此模型的运行依托账号数据信号与用户行为数据信号的结合分析。具体信号涵盖用户长期使用习惯、账号注册时长、日常活跃时段以及用户自行填报的年龄信息。管理解决方案提供商ServiceNow达成三年期协议，将AI模型集成进后者的业务软件。&lt;/p&gt;</description><link>https://www.infoq.cn/article/XO4MTrrpMLfkwoZWK7hB</link><guid isPermaLink="false">https://www.infoq.cn/article/XO4MTrrpMLfkwoZWK7hB</guid><pubDate>Mon, 26 Jan 2026 09:34:02 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Arkweb如何正确加载web的当前title？</title><description>&lt;p&gt;本问答帖原创发布在&lt;a href=&quot;https://developer.huawei.com/consumer/cn/forum/ha_source=InfoQ&amp;amp;ha_sourceId=70000011&quot;&gt;华为开发者联盟社区&lt;/a&gt;&quot;&amp;nbsp;，欢迎开发者前往论坛提问交流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;使用arkweb的onTitleReceive获取web的title有时候并不是和document.title是一致的，而且onTitleReceive经常会返回url字符串，请问这种问题应该如何应对？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;解决方案：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;·&amp;nbsp;方案一：在&lt;a href=&quot;https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd#ontitlereceive&quot;&gt;onTitleReceive&lt;/a&gt;&quot;中通过webController.getTitle()获取网页的标题。&lt;/p&gt;&lt;p&gt;·&amp;nbsp;方案二：通过&lt;a href=&quot;https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd#runjavascript&quot;&gt;runJavaScript&lt;/a&gt;&quot;执行JavaScript代码来获取文档的标题。如果getTitle返回的是网页url，那是因为当前网页未设置title。正常来说通过webController.getTitle()获取到的网页标题和document.title是一致，如果遇到不一致的情况，可以自由选择方式一或者二。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;具体参考demo/操作步骤，请点击原帖查看：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://developer.huawei.com/consumer/cn/forum/topic/0203186440708060028?fid=0109140870620153026&quot;&gt;Arkweb如何正确加载web的当前title？-华为开发者问答&amp;nbsp;|华为开发者联盟&amp;nbsp;(huawei.com)&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd</link><guid isPermaLink="false">https://www.infoq.cn/article/VNllfMFUJOfF4C9ulddd</guid><pubDate>Mon, 26 Jan 2026 09:02:28 GMT</pubDate><author>HarmonyOS</author><category>HarmonyOS</category></item><item><title>AI辅助开发系列专题：现实世界的模式、陷阱和生产就绪情况</title><description>&lt;p&gt;AI不再是研究性实验或IDE中的新奇小玩意儿：它已成为软件交付流程的一个重要组成部分。团队逐渐认识到，将AI融入生产环境的关键不在于模型性能，而在于架构设计、流程管理和责任归属。在本系列文章中，我们将探讨AI完成概念验证之后的发展轨迹，以及AI如何改变我们构建、测试和运营系统的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;贯穿这些文章的核心观点是：可持续AI开发所依赖的基础要素与支撑优质软件工程的基础要素相同——清晰的抽象、可观测性、版本控制以及迭代验证。现如今的差异在于，系统的部分组件能在运行的过程中学习，这在上下文设计、评估管道和人类责任等方面提出了更高的要求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着团队的成熟，他们的注意力从工具转到了架构，从模型能做什么转到了周边系统如何确保可靠性、透明度和可控性。你会在实践中看到这一点，从资源感知型模型构建和人机协同数据创建到使用分层协议（如A2A与MCP），这些技术使AI代理能够发现能力并协作工作，而且无需重写代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能代理架构不再是一个想法验证实验。具备协调、适应和协商能力的系统正逐步投入生产应用，而最稳妥的实施路径是循序渐进，建立清晰的防护机制和共享工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ系列文章“AI辅助开发：现实世界的模式、陷阱和生产就绪情况”探讨了AI辅助开发的现状：工程师正将实验转化为工程实践，而AI正从一种好奇心驱动的探索，演变为一项可掌握、可应用的技艺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;感兴趣的读者可以&lt;a href=&quot;https://www.infoq.com/minibooks/ai-assisted-development-2025/&quot;&gt;下载整个系列的PDF合集&lt;/a&gt;&quot;。以下是该系列文章的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1.&lt;a href=&quot;https://www.infoq.com/articles/ai-trends-disrupting-software-teams&quot;&gt;颠覆软件团队的AI趋势&lt;/a&gt;&quot;，作者：&lt;a href=&quot;https://www.infoq.com/profile/Bilgin-Ibryam/#allActivity&quot;&gt;Bilgin Ibryam&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文将AI定位为自云计算以来软件领域最重要的转变，它重塑了团队构建、运营和协作的方式。文中重点介绍了从生成式开发到智能代理系统的新兴发展趋势，为开发者、架构师和产品经理提供了具体的指导，有助于他们更好地适应这个有AI辅助的软件工程新时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2.虚拟座谈会：实战中的AI：开发者如何重写软件流程&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;虚拟座谈会从观察所得谈及实践经验。本次座谈会的参与者有工程师、架构师和技术领导者，探讨的主题是AI如何改变了软件开发的格局。作为从业者，他们会分享自己的见解，关于把AI纳入日常工作流程后，什么会成功，什么会失败，并强调了上下文、验证和文化适应对于AI在现代工程实践中的可持续应用的重要性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;座谈会成员：&lt;a href=&quot;https://www.infoq.com/profile/Mariia-Bulycheva/#allActivity&quot;&gt;Mariia Bulycheva&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/May-Walter/#allActivity&quot;&gt;May Walter&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Phil-Cal%c3%a7ado/#allActivity&quot;&gt;Phil Calçado&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Andreas-Kollegger/#allActivity&quot;&gt;Andreas Kollegger&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;主持人：&lt;a href=&quot;https://www.infoq.com/profile/Arthur-Casals/#allActivity&quot;&gt;Arthur Casals&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;发布日期：2026年1月26日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;3.为什么大多数机器学习项目未能投入生产应用，作者：&lt;a href=&quot;https://www.infoq.com/profile/Wenjie-Zi/#allActivity&quot;&gt;Wenjie Zi&lt;/a&gt;&quot;，发布日期：2026年2月2日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文采用诊断方法，剖析众多项目在落地前陷入停滞的原因：模糊的问题定义和脆弱的数据实践与理想的模型与实际的产品之间存在着巨大的鸿沟。文中会提供切实可行的建议：设定清晰的商业目标，将数据视为产品，建立早期评估与监测机制，使各团队协调一致，从容实现从原型到量产的跨越。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4.在资源受限环境中构建大型语言模型，作者：&lt;a href=&quot;https://www.infoq.com/profile/Olimpiu-Pop/#allActivity&quot;&gt;Olimpiu Pop&lt;/a&gt;&quot;，发布日期：2026年2月9日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文重点探讨基础设施、数据和计算资源的限制如何推动创新而不是阻碍它。作者会援引真实的案例，展示如何在资源严重受限的情况下，通过更小、更高效的模型、合成数据生成技术和严格的工程实践创建出有效的AI系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;5.架构代理MLOps：A2A和MCP的分层协议策略，作者：&lt;a href=&quot;https://www.infoq.com/profile/Shashank-Kapoor/#allActivity&quot;&gt;Shashank Kapoor&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Sanjay-Surendranath-Girija/#allActivity&quot;&gt;Sanjay Surendranath Girija&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.infoq.com/profile/Lakshit-Arora/#allActivity&quot;&gt;Lakshit Arora&lt;/a&gt;&quot;，发布日期：2026年2月16日当周&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文展示如何将Agent-to-Agent通信与模型上下文协议结合起来，实现互操作性和可扩展的多智能代理系统，并应用于实际的MLOps工作流程中。该文会概要介绍一个将编排与执行解耦的架构，使得团队可以通过发现而不是重写来添加新功能，从静态管道演变为协调一致的智能操作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/articles/ai-assisted-development-series/&quot;&gt;https://www.infoq.com/articles/ai-assisted-development-series/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/yQ3S96l0rjeBBy9XPpCS</link><guid isPermaLink="false">https://www.infoq.cn/article/yQ3S96l0rjeBBy9XPpCS</guid><pubDate>Mon, 26 Jan 2026 08:42:00 GMT</pubDate><author>Arthur Casals</author><category>AI&amp;大模型</category></item><item><title>千亿级请求下，飞猪如何将广告外投系统超时率爆降至0.01%</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;什么是 RTA？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;一句话描述：RTA（Real-Time API）= 实时竞价接口，就是广告平台在每次广告曝光前，实时问飞猪&quot;这个用户要不要投、出多少钱&quot;的关键技术。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;引 &amp;nbsp; &amp;nbsp;言&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪用户增长广告外部投放（RTA）系统自 2022 年上线以来，对接了头条、小红书、华为等 10+ 头部广告媒体渠道，日均处理千亿级请求（百万级 QPS），对低延迟、高吞吐、强稳定性提出极高要求。随着业务策略复杂度提升与流量规模持续增长，系统面临更高的性能与效率挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们围绕两大核心目标展开系统性优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效能提升：通过应用架构解耦、技术栈升级与研发流程优化等，系统性释放工程生产力；极致性能优化：从网络层、网关层、应用层到业务逻辑层优化，系统性降低响应延迟、减少资源成本、提升参竞率与准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本文将按此结构，系统回顾我们的优化路径与核心成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;整体链路架构&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;飞猪 RTA 作为广告投放的实时决策端，接收来自媒体的竞价请求。流量通过两种方式接入：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚合接入：经由阿里妈妈广告交易平台（Tanx 平台）统一转发；直连接入：如小红书、vivo 等媒体直接调用飞猪服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;系统整体分为 网关层（承担高并发请求接入与流量路由）和业务逻辑层（在毫秒级窗口内完成设备解析、人群定向、策略召回、频控与出价计算等多阶段实时决策），最终返回竞价结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2e/2e4c2c966a0cd36fea2b36c1660d1f10.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;研发效能升级&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;技术考量&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;早期 RTA 与多个业务模块共部署于同一应用。随着系统承载流量突破百万级 QPS，一个核心矛盾逐渐凸显：99% 的流量由 RTA 产生，但任何功能迭代都需全量发布，导致资源投入与业务价值配置需要重新审视。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这促使我们从两个维度重新审视系统设计：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资源效率维度：在硬件持续演进的背景下，如何通过架构优化释放单机潜能，以更少机器支撑更高吞吐？这不仅是成本问题，更是技术人应该追求的目标；研发效率维度：效能提升不能仅关注“开发快”，而应覆盖“开发→自测→发布→定位→解决”的完整闭环。尤其在多渠道 RTA 对接场景下，是否存在可复用的范式，能否借助 AI 进一步释放生产力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，我们决定以 RTA 为突破口，开展系统性效能升级——因其流量占比最高、优化 ROI 最显著，且业务逻辑相对独立，是验证新架构与新工具的理想载体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化方案&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;应用架构解耦 - RTA 独立拆分&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;识别核心矛盾、评估 ROI&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在系统拆分上，优先考虑将 RTA 从原应用中拆出。有以下几点考虑：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;RTA 依赖较少，后续做单元化更简单，成本更低。业务逻辑相对清晰，迁移风险可控；重点是它流量最大、成本最高，可以最大化享受底层技术升级带来的红利，ROI 更高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在拆分过程中，曾评估切换到 GO（协程）方案，但综合考虑开发成本及后续维护成本后劝退。最终仍采用 Java 技术栈，但是升级了“大保健三件套”：JDK21（虚拟线程） + SpringBoot 3.x（比 2.x 快约 10-20%，依赖模块化初始化改进）+ 网络中间件优化（降低 I/O 开销与堆外内存）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;平滑迁移策略&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为保障迁移过程零故障、可回滚，过程中作了以下关键思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9e/9ea090e7998e6f22f0eff65c0b79e9d1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;发布提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布不仅是功能上线的终点，更是系统韧性的起点。尤其当单次故障恢复时间直接影响业务收入时，应用重启速度、发布流程确定性、回滚敏捷性，就成为了衡量工程成熟度的关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们以“分钟级恢复”为目标，从流程与性能两个维度优化发布链路：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化发布流程，强化关键验证：移除“安全生产”卡口（测试后置至 Beta）、合并 Beta 与第一批发布，并将 Beta 日志采样改为全量，提升问题发现能力。加速应用重启，支撑快速回滚：基于 JDK 21 + Spring Boot 3 升级，精简依赖与配置，应用重启时间降低约 80%+，显著提升日常发布效率与故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;发布流程对比：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/97/9751800dbe434474dfadb2709cde6aef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;测试提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;各媒体渠道环境高度异构且封闭，无法向开发或预发环境注入标准化测试流量。这导致传统 Mock 或人工构造用例难以覆盖真实长尾场景，逻辑变更后往往依赖线上验证，成本高、风险大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对测试成本大的问题，做了 2 点思考：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线上即标准：线上运行代码已验证可靠，其请求出入参可作为功能正确性的基准——预发环境用相同入参得到相同出参，即可判定代码正常；&lt;/p&gt;&lt;p&gt;真实流量即用例：线上流量天然覆盖最全场景，通过采集请求快照并在预发回放，可自动化完成功能验证与 diff 比对。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上面的思考，设计了一套流量采集和回放系统：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/37d00b5f4d958a8785d3f9ea1607a88e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;开发运维提效&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI Coding 代码重构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 时代背景下，大家都在尝试进行 AI-Coding 实践，我们也从工具 Claude、Cursor、Qcoder，到框架 BMAD、OpenSpec 基本都用了一遍，沉淀了一些较为可行的范式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对 RTA 多渠道接入的场景，通过 AI Coding，我们高效完成了核心链路的代码框架的升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Pipeline 模式：将业务流程原子化拆分，按语义划分为多个节点，管道式编排，职责单一；适配层设计：在关键节点开放扩展点，媒体个性化逻辑收敛至适配层，主流程无侵入；插拔式接入：新媒体只需实现适配接口，即插即用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后整体的代码框架如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/17de09c2a8246e72ea5464dc54af0bc1.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;监控体系精细化&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原有监控体系已覆盖 iGraph 调用、广告召回等关键链路，但仅提供“成功 / 失败”的二元指标，存在局限：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;监控颗粒度不足：无法区分失败根因。例如，iGraph 查询无结果突增时，难以判断是主动熔断、下游超时，还是真实无匹配；问题排查效率低：依赖人工翻查日志，定位耗时长，影响故障恢复速度。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们对强依赖接口进行深度可观测性升级：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;细化异常码，补齐多维度监控：针对 iGraph、人群召回、策略召回、溢价召回等核心环节，统一定义结构化异常码，并按媒体、地域、设备类型等维度聚合，实现快速定位与精准归因；构建 Pipeline 实时折损漏斗：基于节点化改造，将全链路拆解为可度量的转化阶段，通过可视化漏斗动态呈现各环节折损率及原因，使业务流转状态与瓶颈节点清晰可见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;优化成果&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成本与性能&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;机器成本大幅降低：在流量不变的情况下，服务器数量降低了 30%，单机 CPU 水位进一步降低&amp;nbsp;15%。性能显著提升：RTA 接口平均 RT 下降&amp;nbsp;20%，应用重启速度大幅提升，有效支撑高频发布与快速故障恢复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;研发效率&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;测试与发布效率大幅提升：通过流量回放能力，测试周期从 3 天缩短至 1 天；发布周期从至少 1 天缩短至约 2 小时开发与运维效率提升：新媒体渠道接入周期从 5 天缩短至 2 天；新增多维度监控指标，问题发现与定位效率提升&amp;nbsp;40%，实时折损漏斗让业务流转一目了然。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;极致性能优化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面对高并发实时系统，我们摒弃了&quot;头痛医头&quot;的优化方式，构建了从网络层→网关层→应用层→业务层的全链路性能优化体系：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网络层优化：根治跨地域网络耗时&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在接入多个外部媒体 RTA 后，跨地域调用问题凸显。由于媒体机房分布广泛（覆盖华北、华东、华南等区域），而飞猪 RTA 服务当时仅中心化部署于单一机房，导致跨地域单元调用时出现严重超时：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现象：深圳 / 南通单元超时率高达 100%，张北单元相对较好矛盾点：飞猪服务端 P99 延迟仅数毫秒，但端到端仍无法满足媒体严苛的超时要求（如 30~60ms 级别）；根因：每次请求都重新建立 TCP 连接，仅握手建连就要消耗约 30ms，叠加 HTTP 请求的 30ms，极易触发超时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关键验证：通过 CNAME 切换进行了快速验证——将小红书南通区域流量直接导向张北中心机房，省去南通→张北的网络中转环节，超时率从 30% 骤降至 8% → 证明物理距离是根本瓶颈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ba/ba64539f069da86f8a7a1a02b54ef348.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;HTTP 长连接复用&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;启用 HTTP 长连接复用，核心收益：节省 TCP 建连时间（~30ms）、RTT 次数从 2 次降为 1 次、减少系统开销（避免频繁握手 / 挥手）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;配置改造&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过调整网关层配置，显式启用 HTTP 长连接（Keep-Alive），并合理设置连接保活时长与单连接最大请求数，确保长连接有效复用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4b/4b0d33c3aba4518679c739a3b812b6ef.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;解决首次请求超时难题&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首次请求必须经历 TCP 建连，建连耗时导致 HTTP 超时，超时又导致连接关闭，长连接无法建立。&lt;/p&gt;&lt;p&gt;为此，通过改造 HTTP 客户端底层实现：当 HTTP 协议层超时时，TCP 连接往往已建立完成，若底层 TCP 连接已成功建立，则保留该连接供后续请求复用，打破“超时 → 关连接 → 无法长建连”的恶性循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，深圳机房 su121、南通机房 ea120/ea119 的 RTA 超时率大幅降低，但跨地域网络延迟的不确定性仍未根除。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;单元化部署&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;长连接复用虽然缓解了问题，但物理距离带来的 RTT 波动仍是稳定性隐患。RTA 服务完成独立拆分、系统依赖大幅简化后，具备了实施单元化的技术条件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署，核心是梳理 RTA 服务的依赖关系，并针对不同的依赖项，制定了不同的改造方案（仅列出部分）：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;强依赖（如缓存）本地化部署，确保低延迟访问；弱依赖（如配置类数据库）通过中心化代理 + 异步同步满足最终一致性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化成效：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单元化部署彻底解决了跨地域网络延迟问题：&lt;/p&gt;&lt;p&gt;阿里妈妈广告平台侧：深圳、南通单元超时率降为 0.07%。小红书直连：超时率从 30%→0.01%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;网关层深度调优&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为流量入口，网关的性能直接影响系统整体稳定性。通过火焰图与 TCP 连接状态分析，我们发现异常现象：TIME_WAIT 连接数高达数千，而 ESTABLISHED 连接仅十余个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这说明 Tengine 到后端应用也在使用短连接，每次请求都创建 / 销毁连接。TIME_WAIT 过多会导致端口耗尽、内存浪费（每个连接 2~4KB）和 CPU 开销增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tegine 后端长连接优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了减少建连开销，我们在网关层启用与后端应用的长连接池，核心配置如下：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b114c2a9b631f29db3af7c37c2ad5d2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TIME-WAIT 总量下降了 99%集群 CPU 使用率：CPU 降了近 10pt。在保障稳定性前提下，缩容 15% 服务器数量后，单机水位仍保持在健康区间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Tengine 配置精简&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;全盘梳理 Tengine 配置，针对性优化低效和冗余项：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关闭非必要日志：access_log 单文件达 25G+ 引发磁盘告警，仅保留 error 日志移除 gzip 压缩：RTA 响应多为小 JSON，gzip 压缩收益低但 CPU 开销高启用 reuseport：配置 listen 80 reuseport，消除 accept 锁竞争，提升并发处理能力。优化效果：CPU 水位下降 2 个百分点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;应用层极致优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用层的性能瓶颈往往隐藏在非核心路径中，核心业务逻辑通常是经过多次优化的重点，而一些看似不起眼的非核心路径（如日志系统、下游服务调用等）往往成为隐藏的性能瓶颈。通过压测与线上监控，我们发现两个关键问题：日志埋点开销过大与下游长尾请求拖累整体 RT，并针对性实施优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;日志系统优化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在高并发实时系统中，日志既是可观测性的基石，也可能成为性能瓶颈。通过 Arthas 火焰图分析 CPU 热点，发现日志埋点逻辑与核心 RTA 业务逻辑的 CPU 占比居然相当，是两个大头，表明日志系统仍然有优化空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;鉴于日志埋点对业务监控、链路追踪等的重要性，我们无法简单地关闭或大幅减少日志。因此，从减少日志量和提升日志吞吐效率两个方面进行优化：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;协议精简：精简日志的输出格式，采用紧凑型协议格式替代冗余的 JSON 格式，减少了 50% 的日志体积。批量聚合：通过 StringBuilder 将散落在多处的日志打印收敛到一次日志打印，直接降低了 IO 操作次数。异步刷盘：通异步日志过配置 Logback 的 AsyncAppender（设置 neverBlock=true）和 RollingFileAppender（设置 immediateFlush=false），以异步方式刷新日志到磁盘，减少了频繁的磁盘同步操作带来的系统开销，增加了日志的吞吐。分层采样：不同应用环境采取不同的采样策略，在 Beta 环境下进行全量采集以便快速定位问题；而在生产环境中实施千分之一的采样率，确保可观测性的同时大幅减少日志数量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，CPU 使用率降低了 9pt，整体日志文件大小减少了 60%，直接降低日志存储和分析成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;主动熔断机制&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在完成网络层、网关层的性能优化后，系统 P99 延迟已稳定满足媒体超时要求。但偶发的长尾“毛刺”请求（由瞬时 GC 抖动、资源竞争或下游微突发引起）仍可能影响毫秒级实时决策的稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们在核心依赖调用中引入主动超时熔断机制：对关键服务调用设置独立于全局超时的更严格执行时限，一旦超时立即中断并返回降级兜底结果，避免单点延迟拖累整体响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化后，接口 P99 延迟波动显著平滑，各区域机房超时率进一步降低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;业务层优化：参竞率与准确率提升&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心洞察与背景&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着流量规模扩大，设备数量和类型同步增长，设备身份识别的一致性问题被放大，主要体现在以下三方面：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;策略一致性挑战：原有 ID 选择采用单一优先级规则（如 Android 优先 OAID → IMEI），当人群包仅包含 CAID 而系统选中 IDFA 时，可能导致匹配失败；标识歧义风险：采用扁平化的 didMd5 格式，在亿级规模下存在哈希碰撞可能，影响画像准确性；配置与执行不一致：离线策略（如定向表、溢价表）与实时决策使用不同 ID 格式，造成策略“写一套、用一套”，实际失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化方案&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;设备标识标准化：摒弃 didMd5 扁平格式，定义 didType_didValue 分层标识体系（如：idfa_{hash}, oaid_{hash}），通过类型前缀彻底消除哈希碰撞歧义，身份识别准确率提升至 99.99%召回策略重构：废弃单点优先级规则，构建多维身份并行召回引擎，提升召回成功率和准确率。全链路数据一致性：统一改造定向表、溢价系数表等 8 个核心离线表，确保策略定义与实时执行使用同一套标识体系；同时建立设备身份质量监控，自动过滤 &quot;null&quot;、&quot;-&quot; 等无效设备标识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;优化效果&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因 ID 不匹配导致的参竞失败大幅减少，整体参竞效率明显提升；投放精准度增强，无效拉新显著下降，营销资源更高效地触达目标用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结与展望&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总结&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过两阶段的系统性优化，飞猪 RTA 在性能与效能上都有显著的突破：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;性能与成本：通过应用拆分、架构升级与网关调优等，在整体 QPS 提升 60%+ 的前提下，服务器资源消耗降低约 30%；研发效能：通过流量回放、发布流程优化与核心链路重构，测试周期缩短约 65%，发布周期压缩超 80%，新渠道接入效率提升 60%+，问题发现与定位效率提升约 50%；业务价值：通过设备身份一致性治理，参竞效率显著提升；通过精准定向优化，拉新重复率大幅下降，用户质量明显改善。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;展望&lt;/p&gt;&lt;p&gt;未来，RTA 将持续深耕性能与业务双轮驱动：一方面保持对 RTA 极致性能的探索；另一方面深度融合 AI 能力，构建投放效果自动诊断与策略自优化机制，实现从“实时响应”到“智能决策”的跃迁，让 RTA 系统不仅更快，而且更聪明，真正成为驱动业务增长的智能引擎。&lt;/p&gt;</description><link>https://www.infoq.cn/article/wbygEP7MOJfR7btgiWvo</link><guid isPermaLink="false">https://www.infoq.cn/article/wbygEP7MOJfR7btgiWvo</guid><pubDate>Mon, 26 Jan 2026 05:26:49 GMT</pubDate><author>飞猪技术 曹会祎</author><category>生成式 AI</category></item><item><title>AI Agent 是长期运行的“风险系统”，如果你还只在防 Prompt Injection，说明已经落后一代了</title><description>&lt;p&gt;为防止大语言模型和 AI Agent 执行嵌入在外部数据中的恶意指令，&lt;a href=&quot;https://medium.com/cyberark-engineering/when-ai-agents-obey-the-wrong-master-913aff17e3ed&quot;&gt;所有进入 Agent 上下文的文本在完成校验之前都必须被视为不可信数据&lt;/a&gt;&quot;，其中不仅包括用户输入的提示，还包括来自外部系统的各类信息。AI 安全公司 CyberArk 的首席软件架构师 Niv Rabin 表示。为此，他所在的团队提出了一种基于“指令检测”（Instruction Detection）与“历史感知校验”（History-Aware Validation）的方法，用于同时防御恶意输入数据和上下文历史投毒攻击。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 介绍说，团队设计了多种防御机制，并将其组织成一个分层的处理流水线。流水线中的每一层都针对不同类型的威胁，目的是减少单一防护手段不可避免存在的盲区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些防御措施包括“蜜罐动作”（honeypot actions）以及指令检测器，用于拦截具备指令特征的文本，确保模型最终接触到的仅是经过验证、且不包含指令的数据。同时，这些机制也会应用于整个上下文历史中，以防止所谓的“历史投毒”（history poisoning）。这种攻击方式指的是，原本看似无害的内容片段在多轮交互中逐步累积，最终拼接成一条完整的恶意指令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所谓“蜜罐动作”，本质上是一种用于诱捕恶意意图的安全陷阱，即 Agent 在任何正常业务场景下都不应选择的合成动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些动作对应的是一些不会真正执行任何操作的虚拟工具。它们并非用于完成实际任务，而是作为安全信号存在，其工具描述被刻意设计成能够暴露可疑行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在提示中出现的可疑行为包括对系统内部机制的元层级探测、异常的数据提取尝试，以及试图诱导模型泄露系统提示（system prompt）的操纵行为等。如果 LLM 在动作映射阶段选择了其中一个蜜罐动作，通常就意味着存在明显的异常或越界行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 指出，真正的安全风险并不主要来自用户输入，而是来自外部 API 或数据库的返回结果。针对这一问题，团队引入了指令检测器作为关键防护手段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种检测已经不再是传统意义上对“恶意内容”的搜索，也并非基于关键词、文本毒性或策略违规的判断，而是聚焦于识别文本中所蕴含的意图、行为模式以及指令在结构层面的特征。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;指令检测器本身是基于 LLM 构建的“裁判模型”。在任何外部数据被送入主模型之前，检测器都会对其进行审查，并被明确要求识别任何形式的指令，无论其表现得多么直白或隐蔽，从而使系统能够在第一时间阻断可疑数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，时间也被证明是一种重要的攻击向量。早期响应中零散存在的恶意指令片段，可能会在后续交互中被重新组合，最终形成一条完整指令。这种现象被称为“历史投毒”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;示意图展示了一个典型案例：LLM 被要求分别获取三段数据，单独来看，这些数据完全无害；但合并在一起后，内容实际拼成了一条指令，要求系统停止处理并返回特定结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://res.infoq.com/news/2026/01/cyberark-agents-defenses/en/resources/1securing-agents-cyberark-1768938604269.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为防止历史投毒，所有历史 API 响应都会与最新获取的数据一并提交给指令检测器，作为一个统一输入进行分析。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Rabin 指出，历史投毒并不是发生在数据进入系统的入口阶段，而是发生在系统从历史记录中重建上下文的过程中。通过引入这一机制，即便对话历史中隐藏着试图干扰模型推理的细微线索，系统也能够在模型受到影响之前及时发现异常。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上述所有步骤都会在同一条流水线中运行。一旦任意一个阶段检测到风险，请求就会在模型处理之前被直接拦截；只有通过全部校验后，模型才会处理已经净化过的数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rabin 总结，这种方法的关键在于将 LLM 视为一个长期运行、跨多轮交互的工作流系统，而非一次性的请求响应组件。他在原文中对这一方案进行了更为深入的展开，对于关注 AI 安全问题的读者而言，值得进一步阅读。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cyberark-agents-defenses/&quot;&gt;https://www.infoq.com/news/2026/01/cyberark-agents-defenses/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KacfyVt0C9OHv76W6a8A</link><guid isPermaLink="false">https://www.infoq.cn/article/KacfyVt0C9OHv76W6a8A</guid><pubDate>Mon, 26 Jan 2026 03:17:33 GMT</pubDate><author>作者：Sergio De Simone</author><category>AI&amp;大模型</category><category>安全</category></item><item><title>OpenAI 七年元老离职后首次受访开麦：大模型圈“娱乐化”把压力拉爆，谷歌这波别急着吹本质还是 OpenAI 失误</title><description>&lt;p&gt;2026 年的第一个月，Jerry Tworek 离开 OpenAI 的消息传出来时，几位 OpenAI 的员工都觉得很突然，他们在X上评论说：“我真的崩溃了”“这太难受了”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Jerry 是现代 AI 浪潮背后最有影响力、却也最少公开露面的关键人物之一。 2019 年加入 OpenAI时，当时该公司还只有约 30 名员工。他参与了许多最重要的项目，包括后来被称为 Q-Star 和 Strawberry 的推理方法，最终发展成为 o1 推理模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这次离职后，他在接受 Core Memory 的播客采访时解释了原因：他想从事有风险的基础研究，这种研究在像 OpenAI 这样的公司已经不可能进行了，因为像用户增长这样的指标才是优先考虑的。他对 ChatGPT 广告的看法体现了研究与商业化之间的脱节：“这是一种商业策略，而我负责训练模型。” 这番言论印证了有关 OpenAI 人工智能研究与产品开发之间日益加剧的分歧的传言。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Tworek指出，创新不足的原因有很多。最佳模型的竞争异常激烈，公司需要不断展现实力才能留住用户并证明GPU成本的合理性。僵化的组织结构更是雪上加霜，组织架构图决定了哪些研究是可能的：团队各自为政，职责分明，跨团队研究难以开展，Tworek解释道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这场采访，也是一次“离职解读”，Jerry 还批评了整个人工智能行业，指出所有主要的人工智能公司都在开发几乎相同的技术，产品也几乎没有区别，这迫使研究人员追求短期利益，而不是实验性突破。更重要的是，他开始认真思考：如果研究真的需要冒险、需要不同路径，那他是否还应该继续待在这场高度同质化的竞赛中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 Tworek 看来，谷歌之所以能够在 AI 竞赛中成功追赶 OpenAI，本质上是 OpenAI 自身的失误。他表示，这家 AI 实验室犯了一些错误，行动过于缓慢，没能充分利用自己原本拥有的巨大领先优势；而与此同时，谷歌则做出了许多正确的决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及 OpenAI 的具体问题时，Tworek 并未展开细说，只是暗示：员工流失有时是更深层问题的表象。他强调说，人走人来本来很正常，但如果一波人是因为“方向不对、决策错了”才走，那就说明公司里确实有点事——也难怪有些关键推进会慢得不该那么慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与这种“慢得不该那么慢”的状态形成对照的，是 Tworek 对 Anthropic 的评价。在播客中，他高度评价了这家 OpenAI 最强的初创公司对手，认为它在过去一年里展现出了一种罕见的“清晰感”：算力更少、团队更小，却异常专注，执行力极强。他特别提到 Anthropic 在代码模型与代码 Agent 方向上的进展——那不是靠简单堆规模取得的成果，而是一种“非常清楚自己在做什么”的工程与研究结合状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着谈话继续，话题很快从技术转向了另一件更微妙的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry 说，这几年最让他感到“不对劲”的，并不只是研究路线，而是整个大模型行业正在发生的变化。他形容现在的状态有点像这样：你做出一个新东西，大家还没真正弄清楚它是什么，它已经被卷进了一整套剧情里。谁离职、谁跳槽、谁被挖、谁“内部有分歧”，每天都像连续剧更新；湾区像一个巨大的转会市场，研究者在几家前沿实验室之间流动，围观者负责情绪，媒体负责剪辑——研究现场，被包裹进了一层娱乐业式的叙事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“技术、概念、人类情绪、现实生活，是分不开的。”Jerry 说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当一个行业被持续围观，每一次进展都会被强行赋予意义，每一次内部变化都会被解读成信号，整个系统就会被不断加压。你不是在安静地做研究，而是在聚光灯下跑一场没有终点的马拉松。他用一个很个人的比喻形容这七年：“像做俯卧撑。”每一次高压过去，你会更能扛一点。你学会屏蔽噪音，学会在混乱中保持稳定。但代价是，你也会慢慢习惯这种状态——把异常当成常态，把围观当成空气，把压力当成日常。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们翻译并整理了这期播客的完整对话，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;当整个大模型行业只剩下一套“配方”，有些人宁愿离场&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：今天我们请来重量级嘉宾——OpenAI 的 Jerry Tworek。他在 AI 圈算是“活传奇”那种人，而且刚刚离开 OpenAI，所以这期信息非常新、也非常重磅。我刷到不少 OpenAI 的同事在 X 上直接说“我崩溃了”“太难受了”。这就能看出来他在内部的分量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他主导或参与了 OpenAI 很多最重要的项目。这一波“推理模型”的时代，在很大程度上也和 Jerry 有关。今天他会聊他的经历、他做过的事情，然后我们也看看他会不会讲得更“辣”一点——希望如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry，你好。你身上有一种……“刚失业的光芒”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我已经失业八天了，确实是一种变化。我已经很久没有失业过了，但这件事也有很多好处。比如我现在晒太阳的时间多了很多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那这期节目就算你的“离职访谈”了。我们刚才已经简单介绍了你的背景，我再稍微补充一点。你大概是 2019 年加入 OpenAI 的。你来自波兰，在来 AI 领域之前，和很多 AI 从业者一样，曾经在高频交易相关的领域工作过。在 OpenAI，你参与或领导了很多大家非常熟悉的重要项目。最近，很多人听说过 Strawberry、o1，以及这波“推理模型”的兴起，而这是你追了相当长一段时间的方向。然后，如大家所知，你最近刚离开 OpenAI。这件事在 X（推特）上引起了不少讨论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/47/479f049779c12aa5095f04390fa0f70a.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大家好，我做了一个艰难的决定：离开 OpenAI。&amp;nbsp;我在这里将近七年，经历了很多美好与疯狂的时刻——但美好远远多于疯狂。&amp;nbsp;我非常享受和这支团队共事的时光。我有机会在“机器人上的强化学习规模化”还没流行之前就参与其中；训练了世界上最早的一批代码模型，推动了 LLM 编程革命；在“Chinchilla（缩放规律）”还没被叫作 Chinchilla 之前就发现了它；参与了 GPT-4 和 ChatGPT 的工作；最近则是组建了一支团队，建立了一种训练与推理算力规模化的新范式——我们通常把它称为“推理模型”。&amp;nbsp;我在这里结识了许多朋友，有些夜晚也在办公室度过；我参与并见证了相当多的技术突破；也和许多我视为至亲的人一起欢笑、一起担忧。我有幸招募并壮大了——在我看来——世界上最强的机器学习团队。&amp;nbsp;这段旅程非常精彩。虽然我将离开，去探索一些在 OpenAI 很难开展的研究方向，但这依然是一家特别的公司、一个特别的地方，它已经在全人类的历史中占据了永恒的一席之地。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f0/f0ec12c557df1b4104102d4a8b647076.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/93/93f6e7985e9154b0197e376ef99f46b8.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：某种意义上，这事挺棘手的：我如果不自己说，媒体迟早也会替我说——要么写成“独家”，要么当成“泄露”。所以我宁愿自己把话讲清楚，省得消息一传十、十传百，越传越走样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，我们最怕“越传越离谱”。你其实可以先跟我们说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：（笑）我可以随时给你们打电话，告诉你们我生活里发生的任何事——比如我中午吃了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：但说真的，你那条离职帖写得很好，而且挺真情实感的。你在那里待了七年，经历了巨大的变化。从你的视角看，这七年是什么感觉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：老实说，我在 OpenAI 的每一年，都像是在一家完全不同的公司里。无论是公司本身的高速增长，还是整个 AI 世界的变化速度，都非常罕见。我不觉得历史上有很多类似的例子。我很高兴自己亲身经历了这一切。几乎每一个阶段，情况都完全不同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你 2019 年加入的时候，公司大概只有 30 人左右？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：对，大概就是那个规模。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那现在呢？几千人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：已经没法数清楚了。现在是一家规模非常大的公司，有很多办公室，全球各地都有团队。现在几乎很难找到没听说过 OpenAI 的人。我加入的时候，还是几个小团队各自在做自己的小研究项目。那时唯一始终不变的，是野心——从一开始就瞄准 AGI，想要改变世界、产生正向影响。我觉得公司在这方面做得非常成功。ChatGPT 把一种“可用的智能”分发给了非常多的人，这本身就是一件非常了不起的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你发了那条离职推文之后，是不是几乎所有基础模型实验室都立刻联系你了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：确实有很多。我现在正在慢慢梳理下一步要做什么。在这个行业待了这么多年，我本来就认识很多人，也有很多联系。从积极的角度看，我并不急着立刻做决定。过去很多年我工作得非常拼，几乎没有时间去见人、聊天。现在终于有机会停下来，认真想一想接下来的七年要怎么度过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你在推文里提到，你想做一些在 OpenAI 觉得无法进行的研究。能具体解释一下吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：是这样：在一家必须参与当下这种极其残酷、极其高压的竞赛、必须争夺“世界上最强 AI 模型”的公司里，有些事情就是很难做。这背后有几个方面的原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其中一个因素是风险偏好。公司愿意承担多大风险，会受到很多现实约束：比如不能落后于用户增长指标，比如 GPU 成本极其高昂。因此，向外界展示实力、持续拥有最强模型，对所有主要 AI 公司来说都非常重要。但这确实会影响你愿意承担风险的“胃口”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一个很难的取舍是组织架构。公司有 org chart，而 org chart 往往决定了你能做什么研究。每个团队都需要一个身份、一个研究范围、一组他们要解决的问题。跨组织的研究就会变得非常困难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也不确定这是不是一个已经被完全解决的问题：当研究规模变得很大时，究竟该如何把研究组织好？研究本身喜欢动态，甚至可以说喜欢混沌；但一大群人需要秩序、结构和组织架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，“把组织架构交付出去（shipping your org chart）”成了一种非常普遍的现象，研究也不例外。你最终会做那些组织结构最容易支持的项目。而与此同时，我确实想做一些研究，但公司的组织结构并不容易支持我去做这些事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是否意味着我们将看到一项新突破？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我想，其实 AI 世界里的每一位研究者，都想参与下一次真正的突破——我当然也包括在内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我之前在播客里跟 Mark（Mark Chen，OpenAI 的 首席研究官） 聊过这个话题：几乎所有人都会带着自己的想法去找他、找 Yakob（Jakub Pachocki，OpenAI 的核心研究负责人之一）。OpenAI 一直以来确实有一段“押注冒险想法、去做其他实验室没做的事”的历史，而且这种策略也确实为他们带来了回报。但我也很清楚——你们那里一定聚集了大量非常聪明的人，所有人都会不断提出各种想法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在某个时刻，公司终究是一家资源有限的组织——哪怕这些资源已经非常多了——也必须做出取舍。所以，这必然是一个非常艰难的决策过程。也正因为如此，我在思考的那些方向，大概确实属于那种“相当新、相当不寻常”的路径：公司需要判断，我们到底要不要往这个方向走？现在有没有能力、有没有余力去承担这种不确定性？我们是否能在当下负担得起？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：关于“研究时代”的判断，我不确定事情是否真的像他说的那样是非黑即白的。但我非常确定的一点是：在 AI 和机器学习的世界里，还有大量东西尚未被真正探索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大约六年前，我们基本确定了以 Transformer 为核心的架构路线。此后相当长一段时间里，整个行业都在持续扩大 Transformer 的规模，而且进展确实不错。路径也非常清晰：每个季度用稍多一点算力、稍多一点数据，训练出一个更强的模型。到目前为止，这条路看起来并没有明显的“天花板”，进步仍在持续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但问题是：这就是终点了吗？这是最后一条路了吗？我几乎可以确定不是。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们还有很多改进模型的方式，目前根本还没真正开始做。正如你刚才提到的，我自己主要做的是“推理”，以及扩大强化学习的规模。在那之前，整个领域几乎所有的“大赌注”都押在 Transformer 的预训练规模上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;扩大预训练规模，确实是一种有效的扩展方式，而且效果很好。每一次更大规模的预训练，模型能力都会整体提升，各方面都会变强。所以你当然可以说：那我们就继续扩展预训练规模，模型自然会越来越好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但后来，有那么一小撮“做梦的人”、研究者开始相信：事情不止这一种做法。我们不只是扩展预训练，还可以在语言模型之上，大规模扩展强化学习，而且投入的计算量可以和预训练处在同一个量级。这样做，能够教会模型一些仅靠预训练永远学不会的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为如此，我们今天才有了这些令人惊叹的 Agent：它们可以自动化工作、解决复杂问题。而如果只靠预训练模型去完成这些任务，可能需要极其夸张的算力和数据量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也就是说，当你发明了一种新的“扩展方式”，你就会得到一整套全新的能力；而如果你只是沿着原有的预训练扩展路线走，那可能要花非常、非常久，才能逼近这些能力。这一次，其实是一次相当大的跃迁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，自从 GPT-4 引入以来，“推理模型”几乎是这几年里最重要的一次能力跃升。而我相信，类似这样的跃迁还会出现不止一次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我一直觉得，研究者不应该只盯着“渐进式改进”，而是要去思考：有没有办法把整个棋盘掀翻？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：去年在 NeurIPS 上，Ilya 曾说过一句话，大意是：“我们正在耗尽数据，这条路迟早会走到尽头。”关于“预训练是否正在进入一个越来越艰难的阶段”，我一直在想：那下一个真正的突破会是什么？这正是你现在想问的问题，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：是的。但我并不认为这等于在说“预训练已经结束了”。预训练仍然在持续改进，而且还有很多方式可以继续优化它。但它已经不再是唯一的改进路径，而且其他路径，可能在很多维度上能更快地带来提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;扩大预训练规模，在很多能力上提升得其实非常慢——它确实会让模型更好，但提升是渐进的。而与此同时，可能还存在其他方式，能带来更大的跃迁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：硅谷有一个很有意思的现象：很多时候，科技公司会提出一些非常原创、甚至看起来“怪异”的想法，外界一开始完全不理解。但正是这样，才催生了全新的商业模式、新的科学、新的研究方向。而科学研究本身，也是如此：你需要去追逐别人还没走的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可一旦某个方向“爆了”，事情就会反过来——会形成一种巨大的共识。突然之间，所有人都开始说：“我们就该这么做。”然后大家不再讨论“该不该走这条路”，而是开始比拼“谁在这条路上跑得更快”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这其实就是你刚才描述的那种状态。那么问题来了：当我们已经进入这种“模型竞赛”，而且已经持续了两三年之后，会不会出问题？是不是所有主要实验室都变得越来越保守？这会不会成为一个普遍性的结构问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：让我感到非常“难过”的事，就是现在几乎所有 AI 实验室都在试图做和 OpenAI 一模一样的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 显然是一家非常成功的公司，它在很多关键问题上做对了选择，把整个世界带进了“规模化 Transformer”的范式之中，也证明了：通过扩展机器学习模型的规模，确实可以为世界带来大量非常有价值、非常有用的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但问题是：这个世界究竟需要多少家“做完全同一件事”的公司？我不知道。竞争当然是好事，所以肯定不止一家更好。但现在我们大概已经有五家相当严肃、体量巨大的 AI 公司，基本上在用完全同一套“配方”，试图在同一套技术之上，做出一点点差异化的产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也许这确实是对的选择，但我还是希望能看到更多多样性——更多模型层面的差异。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你去看现在世界上最好的那些模型，实际上很少有人真的能注意到它们之间的区别。我觉得应该做更多“盲测”：让人们分别和不同模型对话，看他们是否真的能分辨出哪个是哪个。我敢说，99.9% 的用户根本察觉不出来这些模型有什么不同；在他们的感受里，这些模型几乎一模一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便背后是不同团队，在做一些细微不同的事情，但所有实验室都觉得“我们在这个点上做得稍微好一点”“对方在另一个技巧上可能更强”，最终的结果却是：大家全都挤在一个非常接近的位置上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那真正的探索在哪里？真正的创新空间在哪里？真正能让你和别人拉开距离的差异化又在哪里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 我主要用这些模型做文字工作，偶尔会在 Gemini、ChatGPT、Claude 之间切换——差别确实有，但很细，更多是语气和“性格”。比如我最近更常用 Claude，因为它更直接、不啰嗦；而 ChatGPT 的语气我一直很难调到那种感觉。不过总体我也同意，大多数人其实分不清这些模型的区别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;话说回来，我想问一个可能有点尖锐的问题：你在 OpenAI 待了这么久，在公司内部算是传奇人物之一，而且你的履历也证明，你参与的项目往往能做成。那从外界看，如果连你这样的人都觉得——自己真正想做的研究在公司里推进起来足够困难，以至于最后选择离开——这是不是一个不太好的信号？尤其对一家最初以研究实验室起家的公司来说，这意味着什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我觉得有时候，人和组织都会成长到一个阶段：必须意识到，彼此的道路需要分开。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对一家单一公司来说，非常重要的一点是：公司内部的人，必须在某种程度上对目标、对前进路径保持一致。而在某个时刻，我对“未来研究路径”的判断，和 OpenAI 选择的方向，至少在一些足够重要的点上，出现了分歧——包括接下来一年研究该是什么样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种情况下，我认为分开，反而比强行在分歧中继续合作要好得多。否则，那些分歧可能会不断积累、发酵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我反而认为：不同公司去做同样的事情，在某种意义上是合理的。因为专注对于一家公司来说非常重要，而 OpenAI 很可能正在做所有“正确的事”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也许只是我自己有一些不太现实的梦想；也许我对“还能做些什么其他事情”过于乐观——这完全有可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多公司必须专注于自己的核心路径，才能活下来，才能进入下一个阶段。所以在一个理想的世界里，应该有很多不同的公司，在做很多不同的事情。而研究者——尤其是那些很难去做自己并不真正相信之事的研究者——应该能找到一个地方，在那里，他们能投入到自己最相信的研究方向中。最终，历史会证明哪一条路是对的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为如此，我才会对“大家都在做同一件事”感到有点难过。因为在当下，如果你想做一些偏离主流机器学习路线的事情，真的非常难找到一个合适的地方。这大概是我目前最感到遗憾的一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你现在还在思考下一步要做什么，对吧？如果所有实验室都在做同一件事，那你应该不会想简单跳去另一家大实验室？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我当然还在认真思考下一阶段。但如果有更多“稍微偏离主流、但依然具备规模”的选择，那我会更开心，也会更容易做决定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你觉得，要让整个行业偏离当前主流路径，需要什么条件？我可以想象，这些公司投入了巨额资金、消耗了大量资源，又处在聚光灯下，自然会害怕承担风险。但也许这些风险是必要的。那到底要改变什么？或者这种改变真的会发生吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这正是一个非常有意思的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我其实非常喜欢冒风险，也经常被人这样评价。我认为，冒风险本身是一件好事。但当你面对的是“巨额资金在押”的局面时，真正有能力、也愿意承担风险的人，其实非常非常少。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;每个人的风险偏好都是极其个人化、极其独特的。我和很多人共事过，我真心觉得：人们应该愿意多承担一些风险，多去尝试一些事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但另一方面，现在 AI 世界里的研究者薪酬已经高得离谱了。这在某种程度上，也会让人变得非常害怕失去工作、害怕一次不好的绩效周期。结果就是：人们更倾向于追求短期、确定性的收益路径。而这些人本身往往都是非常聪明、动机也非常正直的研究者。只是整个系统在某些地方，确实更容易鼓励“短视”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为，研究者应该被更明确地鼓励去冒风险、去下大胆的赌注，因为真正的进步，正是这样发生的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Yann LeCun的世界模型，“方向无疑是正确的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我们已经看到了一些“独行侠”式的人物。比如 John Carmack。Carmack 跑去了达拉斯，像是进了自己的洞穴里。一开始似乎是单干，现在好像有几个人在跟他一起做。他几年前说的，其实和你刚才讲的很像：也许我不知道能不能走出一条完全不同的路，但至少应该有人在一条完全不同的路径上持续折腾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我和 Ilya 聊过，但并不知道他现在具体在做什么。我不知道那是他之前工作的延续，还是某种非常激进的新路线。不过我想，如果不是完全不同的方向，他大概也不会去募那么多钱、重新开始。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后还有 Yann LeCun，他显然有一套不同的哲学。有时候我会觉得这个领域挺奇怪的：AI 从某种意义上说很“老”，已经发展了几十年；但当前这一波 AI 又非常新。和研究者聊天时，他们会说：现在把主要论文读完，其实很快就能跟上前沿。所以我一直在想，会不会有某个人，突然从完全意想不到的方向出现，带来一个极端激进的新想法，把整个领域往前推一大步？但与此同时，又好像越来越难——因为现在你几乎需要一个“国家级规模”的数据中心，才能真正参与到这个层级的竞争中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这正是事情变得非常困难的地方，同时也是一个非常值得解决的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;世界上其实有大量学术研究在发生，也有很多学生在做各种各样的事情，但其中大多数都严重缺乏资源。这使得很多研究最终走不远，因为你真正想做的研究，往往必须在“大规模”下才能完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这也是让我感到非常乐观的一点：现在确实有相当多的资金，正在流向那些“想做新东西”的人。像 John Carmack、像 Ilya——他们做的事情，正是当下这个时代应该存在、也应该被资助的。当然，不是所有尝试都会成功，但其中一定会有一些成功，而创新正是这样发生的。对于任何一个强化学习研究者来说，“探索（exploration）与利用（exploitation）”之间的权衡，都是一个非常基础、非常重要的概念。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是在优化 agent 时，你也必须不断权衡：是走已经被证明有效的路径，还是去尝试全新的方法，用完全不同的方式解决老问题？这是一个非常困难的取舍，但它本身就是一个被研究、也值得研究的问题。而正如我们在设计 agent 时会思考这个问题一样，我们也应该反过来问自己：我们自己在做研究时，是如何在探索与利用之间取舍的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在这个非常非常顶尖的小圈子里，大家都知道 Carmack 在做什么吗？你们彼此是互相了解的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：老实说，我并不完全清楚。但如果我没记错的话，我隐约知道一些。他可能是在押注一种非常端到端的强化学习方式——通过鼠标和键盘，在电脑游戏中训练 agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果真是这样，那其实非常有意思。因为我长期以来一直在想：电子游戏，可能是训练智能体的最有趣环境之一。游戏本身就是为了“对人类大脑有吸引力”而设计的。它们包含故事、权力幻想，但更重要的是：大量的问题求解。游戏必须有趣、必须有挑战、不能重复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某种意义上，电子游戏非常贴合人类智能，它们天然地在教你资源分配、解谜、如何在不同规则下取胜——这正是我们希望 agent 能学会的事情。当然，我们现在还没有真正能在高频、多模态环境中稳定运行的超强模型，可能存在一些架构层面的限制。但我认为，用电子游戏来训练 AI，是一件非常值得做的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：Richard Richard Sutton 过去在扑克、游戏等领域做过大量工作；我也曾在他的实验室待过。早期的那些游戏环境，比后来 OpenAI 的 Dota 要原始得多。但你可以看到，这个想法一直贯穿其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Demis Hassabis 也长期在追逐类似的方向。所以你提到这一点很有意思——这其实是一个“老想法”。一段时间里，各大实验室都在比谁能打通更复杂的游戏、谁能更好地“秀”成果；后来在 ChatGPT 时代，这条路线似乎被边缘化了。但也许，它仍然有潜力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：在科学史上，有一个非常常见的现象：好的想法，往往会反复出现。真正困难的，并不是提前预测“哪个想法是重要的”，而是判断“什么时候是对的时机”。即便在 OpenAI 早期，我们也常说：不能断言某种方法“行不通”，也许只是“现在还行不通”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我七年前刚加入 OpenAI 时，强化学习在游戏上是一个非常火的方向。我们解决了很多游戏问题：StarCraft、Dota，而 AlphaGo 更是一个标志性时刻。但这些模型有一个非常明显的缺陷：它们几乎没有世界知识。它们并不理解我们的世界，只是从零开始，专门为某一个游戏训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这显然不是正确的路径。我们必须先教模型理解世界，理解更高层次的概念，而不仅仅是对像素做出反应。从零开始的强化学习，更像是“猴脑”或“蜥蜴脑”。而我们想要的，是具备更高层次抽象能力的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在多年大规模预训练之后，我们现在已经能够学到一套非常强的“世界表征”。而接下来，我们应该利用它。这正是“推理模型”的核心魔法：在一个对世界有深刻理解的基础之上，叠加一层强化学习。未来就应该沿着这个方向前进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那这不就和“世界模型”的方向一致了吗？Google 在做这个，Yann LeCun 似乎也在推动类似的想法。这在直觉上是合理的——这也是人类学习世界的方式。我们不是在一个黑箱里长大的，而是通过不断试探、感知世界来学习的。所以你对这个方向是非常看好的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这个方向毫无疑问是正确的。真正有挑战性的，是：如何把从世界建模中学到的表征，与强化学习真正结合起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;强化学习教会模型“技能”——让它学会如何在世界中实现自己的目标。但在此之前，模型必须先理解世界，否则它连“如何设定目标”“如何达成目标”都无从谈起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正因为如此，这两件事情必须结合起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果有人能在一个高质量世界模型之上，真正把强化学习跑通，那将会是一个非常令人振奋的时刻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：就你现在这些正在吸引你的研究方向来说——你能不能稍微给我们一点提示？还是说，这样就直接暴露你下一家创业公司的方向了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我现在最兴奋的研究方向大概有两个。主要原因也很简单：我不觉得重复去做各大实验室正在做的那套事情有什么意义。现有体系里当然还有很多可以微调、可以改进的地方，但我认为有两个方向长期被低估了投入——或者至少没有得到足够的资源与重视。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，是某种意义上的“架构创新”。我觉得我们对 Transformer 架构有点过于“路径依赖”了。Transformer 确实很伟大，也被非常深入地研究过。人们一直试图在本地做一些小改动，让 Transformer 更强，但这件事并不容易。虽然也有一些相当成功的改进——比如稀疏化非常成功；还有各种让注意力计算更便宜的方法，也取得了不错的效果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但 Transformer 会是机器学习的最终架构吗？显然不会。尽管 Transformer 的发明者做出了惊人的贡献，并且几乎定义了接下来十年的机器学习格局，但我相信一定还有更多可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一定存在一些训练大模型的方法——它们也许有点像 Transformer，也许完全不像。我觉得这是一个值得去解决的问题。甚至如果没有别人去做，我也愿意卷起袖子自己上，试着把它做出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二个方向相对更“热门”，但我觉得几乎没有人把它做得真正好，那就是持续学习（continual learning）：如何把测试时（test time）与训练时（train time）真正打通、真正融合起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人类显然就是这样运作的：我们没有一个“专门学习模式”和一个“专门回答问题模式”。学习与反应是连续发生的、时时刻刻都在进行。我觉得我们的模型也应该更接近这种状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这可能是我们在把模型真正称为 AGI 之前，最后几个关键能力要素之一。如果模型不能从它看到的数据中持续学习，它就仍然显得有点受限——甚至有点“笨”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;新技术炒作带来的恐惧感&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：说到 AGI，我们上次录播客时我提过：我已经不像一两年前那样经常听到“时间线”讨论了。那时候大家非常热衷谈什么时候会实现 AGI，甚至连“AGI”这个词最近都没那么火了。你自称对 AI 是“谨慎的乐观主义者”。那你觉得我们现在处在 AGI 时间线的哪个位置？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我个人的看法是：我对时间线做了一点更新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直认为，把强化学习规模化（scaling reinforcement learning）是通向 AGI 的必要部分。一年、或一年半之前，我非常坚定地认为：只要把 RL 规模化到我们的模型之上，那就是 AGI 了。但我确实不得不稍微修正这个判断。因为有些东西，只有当你真的到了“下一阶段”之后才看得见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也必须承认：今天的模型在很多方面已经非常非常强了。就拿编码来说——“vibe coding”是我最喜欢的爱好之一，你现在可以非常快地写出很多东西。对一些十年前的人来说，如果你把今天这些能力展示给他们，他们可能已经会把它叫做 AGI 了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我不觉得谈 AGI 还是一种多么离谱、多么疯狂的事。但至少按我的定义，现在的模型仍然不是 AGI——原因之一是：持续学习完全还没有以真正的方式被整合进模型体系里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，还有很多问题。比如多模态感知：如果模型文本理解很强、编程也很强，但它看不见真实世界、不能看视频并且很好地理解视频，那我们能称它为 AGI 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为，要真正达到那个“文明级里程碑”——构建 AGI——还有很多必要步骤要完成。&lt;/p&gt;&lt;p&gt;有一段时间我曾想：如果我们真的拼命推进，并且把所有关键问题都做得足够好，也许 2026 年至少能实现非常强的持续学习，以及真正通用的强化学习。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得我的时间线仍在漂移。但与此同时，AI 领域移动得太快了：投资在年复一年累积增长，越来越多人进入 AI 领域，人才池变大，我们探索的想法数量也变多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我不觉得“这个想法完全荒唐”。也许会早一点，也许会晚一点：可能是 2026，也可能 2027、2028、2029。我不觉得会比这更久太多。但确实还有很多工作要做。不过人们正在非常努力地做 AGI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你刚才提到的内容——让我想起你之前做的那些事。除非我记错：在 Strawberry 还没成为一个“明确项目”之前，外界不是有过所谓的 Q-Star 传闻吗？而且在那次“内部风波”期间，这件事被反复提起：什么“他们知道 AGI 已经到了”，把所有人都吓到了。但听你现在这么说又挺有意思的。因为确实，这些东西做出来以后非常惊人，我们会一度情绪很亢奋；然后时间过去，大家就习惯了。现在回头看，Strawberry 确实很不可思议，也确实改变了整个领域。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我第一次用它的时候，并没有到那种“把我吓死”的程度。你懂我意思吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我懂你意思。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这其实涉及人类心理，以及我们如何与技术互动的方式。对我来说，把强化学习规模化带来的效果仍然非常显著，而且我觉得随着时间推移，我们会看到更多影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尤其是应用在编程上，这会以很多很多方式改变我们的生活。你今天做一个大规模编程项目，和一年前相比，完全是另一种游戏。我们会在很多领域看到这种变化带来的连锁影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我也想说：两年前，当我和团队、以及 OpenAI 的很多人第一次看到 Q-Star 的一些早期迹象真的开始工作时——你坐在一个房间里，看到一种“有意义的新技术”正在出现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你在那一刻不感到一点害怕、不感到一点担忧、不暂停一下想一想“这对世界意味着什么后果”，那我会觉得你没有在负责任地对待自己的工作。我认为每一个 AI 研究者都应该想这些问题：如果我正在做的东西是全新的、它展现出了以前从未出现过的新能力，那世界会发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多研究者确实会这么想。当然，有时候也会把担忧推得太远。一方面，到目前为止，AI 还没有给世界带来什么“实质性的重大伤害”；但另一方面，一些事情（比如“某些很花哨的东西”）是不是算有问题——也许还可以争论。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但总体来说，我认为：当你向世界释放新技术时，感到担忧与谨慎，是一种非常好、也非常健康的反应。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们正在经历一个变化的时代：大量新事物正在扩散到世界里，它们会产生影响——影响人们如何生活，如何看待自己、看待他人；影响人际关系、国际关系；影响 GDP、影响生产力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有时候，一个人写下的一行代码，就可能引发连锁反应。经历了这一切，肩膀上的担子就相当重。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;为什么大模型行业叙事变成了肥皂剧、真人秀&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 我一直在想，尤其“政变”那段时间：你做出来的东西被媒体炒得很热，还被卷进各种戏剧化叙事。我不知道“滑稽”这个词对不对，很多人其实还没弄清它到底是什么，就已经围观成现象了。你当时是什么感觉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry： 技术、概念、人类情绪、人类生活、人和人之间的协议与分歧——在现实里很难被切开来看。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们确实活在一个世界里：AI 领域的重要参与者之间，有一个非常复杂的关系网络，很多层次叠在一起。要把它完全理清楚，可能得历史学家花很多年、甚至几十年，才能真正弄明白到底发生了什么、哪些因素起了关键作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老实说，到现在为止，我对那段时间发生的一切也只剩下非常零散的记忆。我们也在不断“补课”——每当有新的证词出现、每当新的文件被披露，就会冒出一些新事实。未来某个时刻，肯定会有人把所有内容都挖出来、完整还原。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但现实世界就是这么复杂。我也确实觉得，也许应该有一种更健康的方式来讨论技术：找到一个更合适的讨论场域，让分歧能够被更充分、更有建设性地展开。但我们生活在这样一个世界里：没有完美解，也不存在一种绝对正确的讨论机制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 所以你觉得 X（推特）也不是理想媒介？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry： 我个人其实很喜欢在 X 上发内容，分享想法，和社区交流。但它也不是一个完全严肃的地方——很多时候都是半开玩笑、半认真。更核心的问题是：有人担心某件事太危险不该继续；有人觉得继续做是对的，因为它会增强能力；还有人认为方向本身就不对，我们应该做别的研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在技术进步与研究的世界里，这些事情很多都是未知的。没人知道未来。我们只有想法、信念和梦想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们必须和这种不确定性共处，也必须学会在很多问题上“求同存异”——很多时候只能接受：大家各自下注、各自承担后果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 说到当时媒体对 Q-Star 的关注——那阵子简直是炒作过度，几乎天天都在加码，每个月都愈演愈烈。我看着会觉得：这是不是太“嗨”了、太多 hype 了？而且我们俩也都在推特上，多少也参与了这股热度。你怎么看：这种 hype 该不该降一降？我个人确实觉得，强度可以往回拧一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry： 我了解。反过来想，如果七年前有人告诉你：OpenAI 会成为万亿美元级别的公司；会建造规模堪比史上最大基础设施项目的数据中心；会拥有世界上最大的 Web 产品之一；全世界会无时无刻都在谈 AI——你一定会觉得那个人疯了，会说“这就是炒作”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可我真心觉得：这波 hype 在很多层面其实是有事实支撑的。人工智能在很多方面存在过度反应和反应不足的情况（有时候被高估，有时候也会低估），但 AI 的重要性毋庸置疑——它值得被讨论。我不觉得现在还有谁会认为 AI 是个“不重要、不值得讨论”的话题。几年前确实还有人这么想，但现在已经很清楚：AI 很可能是当今世界最重要的议题之一，值得持续讨论与思考。至于进展会有多快、路径到底对不对、安全还是危险——这些当然都可以争论。但 AI 会长期存在，而且只会越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 完全同意。但如果先把技术放一边——我甚至报道过“挖人狂潮”。我越来越觉得，这个行业的叙事变得像肥皂剧、像真人秀，很多时候讨论的不是硬核科学，而是剧情、阵营和情绪。你会不会也觉得我们有点“跑偏”了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry： 但到底是谁在制造这场肥皂剧？这才是问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 嗯，说真的，这一轮比我经历过的任何技术周期都更“肥皂剧”。可能是赌注太高、钱太多，再加上挖人和各种戏剧化叙事，整个旧金山像活在一套自己的现实里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我有时都替你们累——七八年一直在这种高压竞速里，你现在想停下来喘口气，我完全能理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：的确很消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我可以跟你分享一句对我很有帮助的话：有一次，一个比我更有经验、更擅长应对压力的人跟我说——Jerry，这就像做俯卧撑。每经历一次艰难、紧张的时刻，你就更擅长应对压力一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;老实说，这七年让我练出了很强的心理和情绪韧性。我真的学会了在大量噪音、很多胡扯面前，把自己抽离出来，尽量保持稳定、保持定力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不管外部发生什么——公司看起来要塌了也好，研究者流动也好，项目被重新分配也好——总会有事情在推进，总会有新的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我听过有人把“挖人”这件事类比成体育队伍的转会。体育之所以还能运转，是因为有角色、有规则。我差点想说：可惜在加州的法律框架下，这类规则基本不可能出现。但我确实觉得，如果能有一些规则，可能会更健康。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因为确实存在这样一种现象：有些人换工作的频率，比他们真正产出成果的频率还高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： AI 薪资帽？（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：（笑）确实有人这样。但也仍然有很多人在认真做事，推动前沿继续往前走。不过，AI 是一门大生意——这点无论如何都没法否认。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我还跟同事说，我们真该做一张表，把那些在每一家前沿实验室都待过的人列出来，标注他们在每家待了多久。（笑）肯定至少有一小撮人，把整个湾区的“前沿实验室巡回赛”跑完了。说真的，这太疯狂了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：2018 年前后，OpenAI 还只有三十来个人。有一件事当时让我印象特别深：最早那批成员里，波兰人的比例异常高，而且很多都是非常典型的“数学脑”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些人彼此从小就认识，有些并不认识。我一直很好奇：这到底反映的是一种教育背景的集中效应——比如偏重数学训练的体系，确实更容易培养出这类人？还是说，其实只是早期有几个人先来了，后来通过学术和个人网络，慢慢把更多同类的人吸引到了 OpenAI？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：先澄清一点：我在加入 OpenAI 之前，完全不认识任何 OpenAI 的人。我是非常随机、机缘巧合地进来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但你说得没错，在 OpenAI 非常早期，波兰人的占比确实偏高。不过我并不觉得这种情况“经得起时间检验”。现在公司里，波兰人的比例仍然略高于平均水平，但考虑到 OpenAI 的规模已经增长了大概一百倍，这种早期的“高浓度”并没有按比例延续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得这里面确实有一些值得讨论的因素，但我并没有足够多对其他教育体系的亲身体验，所以不敢轻易下结论，说波兰的教育体系“天然更强”。我能确定的是：我们确实有很多非常聪明、数学直觉很强的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但如果说有一件我特别认可、也特别喜欢的事情，那就是波兰人对“努力工作”这件事的重视从我个人经历来看，这种特质在很多地方正在变得越来越少见——尤其是在一些生活条件已经非常优渥的社会里，人们对工作的强调确实在下降。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;Google 的“回归”还是 OpenAI 的“失误”？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你怎么看 Google 最近这一轮的“回归”？你是觉得意外、惊讶，还是说其实早就料到了？看起来他们这段时间做对了不少事情。你们之前是不是一直都觉得：Google 迟早会把局面理顺？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我个人其实不太愿意把这件事称为“Google 的回归”。它应该被视为OpenAI 的失误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 确实在很多关键点上做对了事情，但也不可否认，在某些阶段出现过判断或执行上的失误，导致整体推进速度比它本可以达到的状态要慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一种理想的执行情境里，如果你是一家已经取得领先优势的公司，而且拥有 OpenAI 那样的技术、人才和资源条件，那么你理论上是可以持续保持领先的。但如果在这个过程中，你做出了一些错误决策，而你的竞争对手做出了更多正确决策——而 Google 在最近一段时间里，确实做对了不少事情——那对方追上来，其实并不奇怪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你也必须承认：Google 在硬件、算力和人才储备上，本身就有非常巨大的优势。事实上，在 OpenAI 刚起步的那些年里，Google 在几乎所有机器学习方向上，都是明显的行业第一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 能真正跑出来，靠的主要不是资源优势，而是研究方向上的强烈信念：对某一条具体技术路线、某一个具体长期赌注的坚定投入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但让整个行业、让外部世界真正意识到“这是一个正确的赌注”，花的时间比很多人想象的要长得多。哪怕 GPT-2 训练完成了，GPT-3 训练完成了，后来 GPT-3.5 也出来了——在那个阶段，其实并没有太多人真正重视这件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你去 NeurIPS 这样的会议和研究者聊天，大家会觉得 OpenAI 很酷，但很多其他实验室的态度是：“嗯，我们迟早也能复现。”语言模型确实挺有意思，但在他们看来，也就止步于“有意思”。&lt;/p&gt;&lt;p&gt;真正的转折点，是 OpenAI 开始通过 ChatGPT 赚到钱。那一刻，其他公司才突然意识到：“好，这不只是研究展示，而是一个已经被验证的商业方向，我们必须认真投入了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里其实存在一个很关键、但常常被忽略的时间窗口：从你开始构建一项技术，到它真正被商业化，中间往往隔着一段很长的时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这段时间，足够让其他公司观察、犹豫、评估风险，然后再决定是否下场。而在这个阶段，Google 显然开始非常认真地对待大语言模型这条路线。再叠加 OpenAI 在执行层面的一些失误，最终导致今天的结果：在模型能力和训练成果上，双方已经变得非常接近。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，从 Google 的角度来看，这确实是一件值得祝贺的事情。能够把团队重新拉回状态、把执行节奏提起来，背后一定做了大量艰难而高质量的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你说的这些“失误”，具体指的是什么？我在努力回忆。我记得当年你们推出 Search 的时候，外界一度在说“Google 完了”，但我当时就觉得未必如此。所以你提到的失误，更多是指哪些方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我不太想展开讨论具体的内部决策细节，哪些判断是对的，哪些是错的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但我想强调的核心其实很简单：如果一家领先公司执行得足够好，那么在大多数情况下，它是可以把领先优势持续下去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在现实中，很明显有一些事情的推进速度，比它本可以达到的节奏要慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你的意思是技术层面的失误吗？因为从外界看，也确实发生了不少公司层面的戏剧性的狗血剧情，这些在某些阶段显然拖慢了整体节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我跟 OpenAI 的一些人聊过，关于公司要如何继续向前，确实出现过一些阶段性的混乱，比如关键人物离开等等。所以我原本以为你指的是纯技术问题，但听起来你的意思更复杂一些。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这些事情有时候确实是相互关联的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从技术角度来说，我并不认为“有人离开”这件事本身就一定构成问题。在任何一家公司，人来人往其实都很正常，也应该是一种常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但如果离开变成了某种更深层问题的症状——比如有人觉得：“公司在一些关键事情上做错了决定，我不再相信这家公司了，所以选择离开”——那这往往意味着，背后确实存在一些需要被正视的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以回到我最初的判断：确实有一些事情，推进得比它本可以做到的速度要慢。这并不否认 OpenAI 的成功，但也不能忽视这些失误带来的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：如果像你说的那样，各大实验室基本都在走同一条路，那 Meta 显然也是其中之一。他们在 AI 上投入巨大，也在从各家实验室挖人。我并不完全清楚 Meta 内部的具体策略，但从外部看，他们似乎并没有选择一条完全不同的路线，而更像是在追赶同一条主流路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这听起来像一个根本性的问题：如果你既起步更晚，又在做和别人几乎一样的事情，这真的可能有好结果吗？还是说，你觉得 Meta 实际上走的是一条不一样的路？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我并不完全了解他们的内部策略，所以只能谈一些外部观察。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的感觉是，他们已经意识到一件非常关键的事情：“规模化”在当前的 AI 世界里是不可回避的。如果你放眼现在的 AI 行业，基本可以抽象出两种不同的战略选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一种是：我要做一种和其他人都不一样的模型——它在某些方面会明显更强，我希望把这种差异化模型带给世界。第二种是：我也希望拥有和别人一样强、同一量级的模型，但我的重点不在模型本身，而在于我如何使用这些模型、以及我基于它们构建什么样的产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从我对 Meta 一贯路线的理解来看，这家公司长期以来关注的核心，一直是连接人与人、构建关系、打造大规模的用户体验型产品。无论是社交网络、沉浸式体验，还是他们设想中的元宇宙，本质上都是围绕“体验”和“连接”展开的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我这里是基于外部推测，但我认为 Meta 的思路，很可能是：使用我们已经熟悉、已经理解得比较透彻的 AI 技术（比如 Transformer），来构建全新的产品体验，而不是在模型层面追求完全不同的路线。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从一家极其成功、极其赚钱、而且已经拥有全球最大社交网络的公司视角来看，这其实完全可能是一种非常合理、甚至非常聪明的策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我们刚才聊了 Google，也聊了 Meta。但我想换一个角度问：在你们内部讨论、或者评估其他实验室的时候，有没有哪一家，让你们真的觉得“被震撼到了”？哪一家是你个人印象最深的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我得说，这是一个相对比较新的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在过去一年里，我对 Anthropic 的印象提升得非常明显。我本人其实从来不是那种特别在意模型“性格”的人。虽然我也听说过 Claude 的“性格”很好，可能确实如此，但这并不是我关注的重点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正让我感到震撼的是几件事：他们在代码模型、编码 Agent上的成果；以及他们围绕“开发者”建立起来的整体产品和品牌——还有最关键的一点：他们拥有一大群真正满意、甚至很开心的开发者用户。这是一项非常、非常了不起的成就。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是：他们起步比 OpenAI 更晚；算力条件更受限制；团队规模也更小。在这样的前提下，他们依然做到了高度聚焦，并且执行得非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们在获取高质量算力方面遇到过不少现实困难，但即便如此，仍然做出了非常出色的产品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些产品正在明显改变人们开发软件的方式；而据我了解，也已经在实质性地提升企业生产力。&lt;/p&gt;&lt;p&gt;所以我真心觉得：他们做得非常好，值得祝贺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：他们确实看起来正处在一个“高光时刻”。我身边几乎所有人都在聊 Claude Code。我最近还采访了一个人——他在用 Claude“养活一盆植物”。（笑）可能是第一种被 AI 模型持续“照料”的生命体。我真的不知道他们是怎么做出一个几乎“人人都喜欢”的工具的。从 ChatGPT 到 Claude Code，这种程度的“普遍好评”，其实非常少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且之前还有一件事：当大家被“切断使用”时，开发者的反应极其强烈——某种程度上，那种崩溃感甚至超过了 OpenAI 出事时的反应。连 Elon 都公开承认了这一点，说：“是的，我们用得太多了，这是个警醒，我们得把自己的东西做得更好。”所以我在想：这也许不是一个完全普遍的现象，但看起来，很多实验室其实已经在不同程度上依赖这套工具了。也希望这次“切断”能倒逼出更多、更好的同类产品。来一百万个 Claude Code。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：在 OpenAI，我们其实也开发 Codex 有一段时间了——它算是我们自己的“Claude Code 版本”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我个人觉得 Codex 也挺不错的。有点好笑的是：我自己其实并没有怎么用过 Claude Code。毕竟当时我还在 OpenAI 工作，也没太多机会去亲自用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也是想说得客气一点。所以我确实没法给出太多一手对比体验。但至少从推特上的反馈来看，Claude 确实被全球开发者非常、非常喜欢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;做点跟OpenAI不同的事情&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：结合我们前面的讨论，我对你的理解是：你一直是从一种很纯粹的智识和科学兴趣出发的人。你在 reasoning 上的很多工作，本质上都指向一个长期目标——你想创造“AI 科学家”。&lt;/p&gt;&lt;p&gt;所以当我看到你说要离开 OpenAI 时，我忍不住在想：你是不是已经不太想继续待在这场“基础模型竞赛”里了？听你说话的感觉，更像是想换一条路走。我甚至会想象，你会不会干脆跑去做生物科技之类的方向，用完全不同的方式继续追这件事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：如果我能克隆自己、同时做很多件不同的事情，我真的会非常愿意。但长话短说：有一天我突然意识到——我对自己过去的人生很满意，也为自己做过的事情感到骄傲；但我现在真正想做的，是押一两个、甚至两三个非常非常大的研究赌注，然后看看能不能把它们做成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一直觉得，人应该更愿意冒风险。至少从我的观察来看，我可能算是那种风险承受能力比较高的人——愿意去追一些看起来很野、很不确定、甚至有点离谱的想法。所以我觉得，我应该把这种特质用在更有意义的事情上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你脑子里的这些想法，如果真要落地，大概需要多久？是一年左右的项目，还是说你说的“风险”，意味着你愿意花四五年时间去追一件事，而它最后甚至可能还不如现有方案？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我肯定愿意投入很多时间。但与此同时，我也非常坚定地认为：研究应该尽可能快地推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;做得慢，本身并不值得骄傲。从“把研究执行好”这个角度看，我希望它能更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过，真正关键的，其实是我之前反复提到的两个词：聚焦（focus）和信念（conviction）。&lt;/p&gt;&lt;p&gt;如果你同时做很多事情，几乎注定每件事都只能做一小部分。你的注意力会被摊薄，资源也会被摊薄。研究实验室经常会说：算力不够，算力限制拖慢了研究。这当然是真的，而且是重要因素之一。但很多时候，更核心的问题其实是：不够聚焦。一天之内，一个人的注意力只能真正放在有限的几件事情上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我很喜欢对和我共事过的研究者说一句话：少跑一点实验，把每一个实验想得更深。因为有时候，你花几个小时什么实验都不跑，只是盯着结果、反复分析数据——反而更容易带来真正的突破，而不是不停地“多跑”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以像 OpenAI 这样的公司，算力其实非常多。但如果算力被分散到太多项目上，效果反而会被稀释。如果把算力集中到更少、更聚焦的项目上，算力往往是够用的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这又回到了风险和信念的问题。如果你同时做三个项目，只要有一个成功，其实就已经算不错了；另外两个被砍掉，也完全可以接受。如果三个都成功，那当然更好。但如果你只做一个项目，它往往会推进得更快——因为你足够聚焦、也足够坚定。当然，代价是：如果它失败了，你会非常惨；但如果它成功了，你可能会拥有世界上最好的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而对 OpenAI 这样规模的公司来说，现在确实很难做到一件事：把整个公司押注在一个全新的、完全不同的方向上，同时不在乎下个季度 Gemini 会不会更强。这真的非常难。它需要一种非常特殊类型的人，才愿意这么做。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，这就是问题的核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我明白，也知道你不能聊什么“秘方”。但我还是忍不住好奇：从外部看，我会直觉觉得，OpenAI 接下来押注的方向，应该是那些能赚大钱的方向。比如“Chat 里要加广告”的消息，几乎把整个互联网点燃了。哪怕很笼统地说，你觉得我们能判断他们接下来大概会把资源投向哪里吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这个问题上，我确实不应该、也不能谈 OpenAI 的任何具体计划。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：合理。（笑）那我换个问法：你觉得这些做模型的公司里，有没有谁会选择——也许“勇气”这个词不太准确——不把广告塞进模型里？还是说，从商业角度看，这其实是不可避免的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这属于商业策略。我做的是训练模型。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：好，抱歉，我不是想逼你。（笑）只是聊完整个对话之后，我自己还在试图想明白一件事。一方面，你说你想走一些新的方向，去追一些和主流不同的路径；但另一方面，我们也反复提到：你想做的这些事，确实需要非常强的“马力”。所以我有点难想象：这是 Jerry 一个人、在外面慢慢测试新想法？还是说，就你真正想做的那些研究，你必须身处一个拥有足够资源的地方，事情才有可能发生？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：这正是我现在最想搞清楚的第一个问题。任何 AI 研究，最终都离不开 GPU、离不开算力（FLOPs）。我现在需要认真想清楚的是：到底什么样的方式，才是做这些研究的最佳路径。&lt;/p&gt;&lt;p&gt;我确实正在努力理清楚：我很清楚自己想做哪些研究，但我还在寻找答案——到底怎样去做，才算是一个“好的方式”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;OpenAI 的压力甚至超过创业？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我刚才问的那些，基本就是我最想问的了。我觉得我能跟你聊上好几个小时。&lt;/p&gt;&lt;p&gt;我不想继续追问“你接下来做什么”，因为你看起来太开心了，整个人容光焕发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：是的，我听好几个人都跟我说：你现在比以前快乐多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我不想把你拖回那种压力里，比如问你接下来要做什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我不知道。而且我也听一位正在经营自己公司的人说过一句让我很震撼的话：在 OpenAI 工作，比自己创业还更有压力。从很多方面看，OpenAI 的确是一个压力极大的地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：还有一个小问题，除了“大家都在追同一套东西”之外，你觉得这个领域里还有没有什么“巨大的错误”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我不觉得存在那种特别“巨大的错误”。这个行业里的人，其实都很难犯那种一眼就能看出来的致命错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;真正的问题更像是：你愿意花多少精力去探索“其他可能性”？又有多少精力，继续沿着你已经走得很顺的那条路往前推。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那我换个问法，可能更准确一点。有没有一些你觉得被明显低估、被忽视的研究方向？它们本该得到更多关注，但现在没有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：老实说，这样的想法非常多。但这些想法最缺的，往往不是“它们不存在”，而是：缺关注、缺算力、缺资源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这里还有一个比较有意思的现象。很多研究者——包括学术界——很擅长、也很喜欢做“从 0 到 1”的事情：提出一个新想法，证明它“有点能跑”，然后就发表出来。而我觉得，我自己、以及我在 OpenAI 共事过的团队，真正特别擅长的一件事，是“从 1 到 100”：拿一些已经有初步证据的新想法——它们很不同，也不成熟——然后想办法把它们在大规模上做得可靠、稳定、可落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要训练前沿模型，把一种技术真正嵌进系统里，会涉及大量非常具体、非常琐碎、但又极其关键的工程和研究工作。如果执行不好，可能要花上好几年；但如果你有一套好的方法和节奏，可能几个月就能完成。这也是我未来很想继续多做的一类事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;AI 研究是“明星驱动”的吗？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们之前聊到 OpenAI 的人员流动时，你说公司是能扛住这些变化的。但从外部看，这个领域又很像是“明星驱动”的：比如 Alec Radford 那样的突破级贡献——你知道我指的是什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从行业行为上看，很多实验室似乎也在按“明星逻辑”行事。当然，这背后有大量集体协作，但确实也有一些时刻，看起来重大突破被“绑定”在少数几个人身上。但你刚才的反应，似乎并不完全认同这是一个“明星驱动”的行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：我觉得这是个很复杂的话题，但有两个看法可以同时成立。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一方面，确实存在这样的情况：在某些阶段，尤其是在 OpenAI，一小撮人能产生远超常人的影响力，推动真正突破性的成果，然后这些成果扩散到整个行业。我亲眼看到这种事情反复发生。&lt;/p&gt;&lt;p&gt;但另一方面，当我看到人们在不同公司之间频繁流动时，我很少看到这种流动本身，对公司产生“决定性影响”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我更相信的是：公司的结构、文化和运作方式，才是真正的研究引擎，而不完全取决于某一个研究者是否在这里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且我也观察到一个现象：那些频繁跳槽的研究者，反而往往没那么高产——即便他们过去做过很好的工作。他们需要重新磨合，会被各种事情分散注意力，短期内也未必有新的突破性想法。&lt;/p&gt;&lt;p&gt;经验当然重要，但更重要的是：营造一种环境——强调个人责任、鼓励探索、并且真正为“做出伟大事情”提供条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在一个好的结构、好的文化、好的协作方式下，你完全可以建立很多团队，持续做出伟大的成果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这件事并不依赖某一个“唯一的人”。归根结底，我认为：研究结构、研究文化和协作方式，远比“某个特定的人是否在团队里”更重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人： 很有道理，很有道理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题：你冥想吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：最近在试，但我觉得我冥想得不太行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那祝你下一段旅程，能找到属于自己的“黑暗静修”。Jerry，谢谢你。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jerry：谢谢，很高兴和你们聊天。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VaCq4u5c78U&quot;&gt;https://www.youtube.com/watch?v=VaCq4u5c78U&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/IkEZ94SpmmQuvJ9Kq9q9</link><guid isPermaLink="false">https://www.infoq.cn/article/IkEZ94SpmmQuvJ9Kq9q9</guid><pubDate>Mon, 26 Jan 2026 02:51:12 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>阶跃星辰完成50亿大规模融资，印奇挂帅</title><description>&lt;p&gt;刚刚，阶跃星辰在其官方公众号发文宣布，人工智能领域知名专家印奇正式出任公司董事长，全面负责公司的整体战略节奏与技术方向制定。公司方面表示，印奇在人工智能技术演进、产业落地与组织建设等方面拥有长期而系统的实践经验，其加入将为阶跃星辰进入下一发展阶段提供更强的战略牵引力与执行确定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;公开资料显示，印奇是中国人工智能产业的代表性人物之一，清华大学计算机系背景出身，长期深耕计算机视觉与人工智能基础技术研究。他是旷视科技的联合创始人之一，曾长期担任旷视 CEO，主导公司从学术研究走向大规模产业化落地，推动计算机视觉技术在城市物联网、供应链、消费电子等多个领域实现商业化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在阶跃星辰的组织架构中，印奇将与公司 CEO 姜大昕、首席科学家张祥雨以及 CTO 朱亦博共同组成核心管理团队。该团队在基础模型研究、系统工程能力以及产业化推进等方面形成互补配置，构建起从前沿研究到商业落地的完整决策与执行链条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阶跃星辰方面介绍，公司近年来围绕基础大模型持续加大投入，已在语言基模型、多模态模型以及端云协同模型等方向形成了具备国际竞争力的技术体系。在模型结构设计、训练效率、推理性能与系统协同等关键能力上，公司认为自身已经“跑出世界领先水平”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在应用层面，阶跃星辰正围绕汽车、手机与穿戴式设备、具身机器人等核心终端场景，加快构建“AI+终端”的商业化体系。公司明确表示，“基础大模型”与“AI+终端”并行推进，是其长期坚定不移的战略选择。通过将通用模型能力与具体终端形态深度耦合，阶跃希望推动人工智能从“云端能力”向“场景智能”演进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得注意的是，印奇目前同时担任千里科技董事长，在人工智能与汽车产业结合方面已有较为成熟的实践经验。千里科技长期聚焦智能汽车相关技术与产品，涵盖感知、决策、系统集成等多个环节。阶跃星辰表示，未来将与千里科技进一步深化合作，在智能汽车及更广泛的终端场景中协同推进“AI+终端”战略落地，加速技术能力向产品与规模化应用转化。&lt;/p&gt;</description><link>https://www.infoq.cn/article/laeUNsjRu4ShtikGMNvf</link><guid isPermaLink="false">https://www.infoq.cn/article/laeUNsjRu4ShtikGMNvf</guid><pubDate>Mon, 26 Jan 2026 02:20:59 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Bengio 15年前论文再夺AAAI奖！AI正告别单纯炫技，走向真实世界</title><description>&lt;p&gt;新加坡的会场里，全球人工智能顶会AAAI，正式揭晓年度奖项，也迎来了它的第40个年头。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今年共颁发了5个杰出论文奖，以及2个经典论文奖。在获奖名单中，竟然还有“机器学习三巨头”之一的Yoshua Bengio。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过这一次，他并不是因为最新成果获奖，而是凭借在2011年写的一篇论文获得了经典论文奖。而且不久前，他刚达成AI领域首个“百万被引作者”的成就。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为什么10多年前的这篇论文，会在今年被重新拉出来，还获得了经典论文奖？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不妨来看看它讲了些什么。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;论文名为Learning Structured Embeddings of Knowledge Bases（《面向知识库的结构化表示学习》）。提出了一种方法，把知识库的结构化数据嵌入到连续空间中，从而让结构化知识更容易用于机器学习任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，这篇文章解决的是如何把离散世界（知识、事实、关系）嵌入到连续空间；以及如何让神经网络不靠纯统计，而是“接住现实结构”。而今天热门的世界模型、RAG、Agent的外部记忆等等这些东西，从本质上讲，全都在复用这条路线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再说回今年获奖的5篇杰出论文，这些论文有讲机器人和VLA的，有在讲如何在连续时间系统中让AI模型“白盒化”的，还有讲LLM和CLIP、讲高频信号和局部判别结构的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7f352989d825ad2b7b1437c45a4a287.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;串起来看，这些论文的研究方向，其实可以概括出一个共同指向：AI的竞争，已从拼实验环境的中的炫酷Demo，转向真正的应用层。Scaling Law那套虽然不完全失效，但多少有点过时了，谁能在真实世界中被理解、被修订、被信任越来越关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AAAI 2026: AI走向现实，评奖标准重塑&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面来看看这几篇杰出论文，都有哪些有意思的信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;具身智能领域：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;论文名：ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver（ReconVLA：作为高效机器人感知器的重建式视觉-语言-动作模型）&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0d/0da3ffbdb7d94624f2a83314aa230154.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要说清本文的创新点，需要再这里先简单回顾一下什么是VLA——VLA（Vision-Language-Action）具身智能领域的一个关键模型，可以把视觉感知、语言理解和动作生成统一到同一个模型中，直接根据“看到什么 + 听到什么”，来输出可执行机器人动作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过当前VLA的缺陷也是很明显的：比如模型在执行动作时，视觉注意力高度分散；即便模型能“理解指令”，但在复杂场景、多干扰物、长任务中，往往看不准真正要操作的物体。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果就是：抓错对象、操作不精确（现实世界对精确度要求很高）、长链任务中途失败等等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总之，以往VLA 只监督“动作输出”，几乎不约束“视觉感知过程本身”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而ReconVLA的关键思想是：不“告诉模型看哪里”，而是“逼模型把关键区域重建出来”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其核心机制，简单来说，就是模拟人类视觉的“凝视（gaze）”机制，不要求模型输出框，也不输入裁剪图，而是让模型在内部生成一种“重建信号”，去还原“当前要操作的局部区域”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;论文还系统性地对比了三类视觉定位（grounding）范式：&lt;/p&gt;&lt;p&gt;一类是以外部检测器和裁剪图像为代表的&amp;nbsp;Explicit Grounding，一类是先输出目标框、再生成动作的&amp;nbsp;CoT Grounding，以及作者提出的&amp;nbsp;Implicit Grounding（隐式 Grounding），也就是 ReconVLA 的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/cd/cdfede417b5813ee80c901b42a245e94.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;图注：不同范式Grounding之间的概念性对比。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;前两类方法本质上都是在显式告诉模型“答案在哪里”，并未真正改变 VLA 内部的视觉表示和注意力机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而 ReconVLA通过重建过程，将关键区域作为一种隐式的视觉监督信号，引导模型生成所谓的“重建 token（reconstructive tokens）”，从而在不引入额外输入或输出的前提下，重塑视觉感知能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，它不再让模型“蒙着眼睛试动作”，而是强制模型在每一步决策前，先把目标对象看准，再去动手。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;关于从“结果可解释”，走向“结构可操作”：&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;论文名：Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis（基于理论评分分析的动态系统因果结构学习方法）&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/43/43e04f77af85bba7d6b1ce9eb97c834c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇论文提出了一种方法：CADYT。能够在连续时间、甚至不规则采样的数据中，同时刻画系统的动力学演化，并恢复其中的因果结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/0787bb0138d19e847b1a842bb53a1609.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更重要的是，作者证明了用于判断因果关系的评分函数，在理论上等价于一种合理的模型选择准则，而不是经验性的启发式指标。换句话说，就是这个评分不是凭经验设计的，而是从理论上保证：它会偏向那些“解释得刚刚好、不多也不少”的因果结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在现实世界的系统中，无论是工业控制、物理系统，还是医疗过程，系统本质上都是连续时间演化的，而且由稳定的因果机制驱动。但以往的方法往往只能解决其中一半问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一类是时间序列因果发现方法，它们通常基于离散时间建模（如 DBN、Granger），并假设规则采样，因此在面对真实的连续动力学和不规则采样时，难以准确刻画系统本身的演化机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一类是连续时间动力学建模方法（如 Neural ODE、GP-ODE），虽然能自然处理不规则采样，却主要关注预测精度，本质上并不区分因果依赖与偶然相关。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这就留下了一个长期存在的空白：几乎没有方法，既工作在连续时间框架下，又能够同时恢复系统的动力学机制和因果结构。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而CADYT正是针对这一空白提出的。它将连续时间的高斯过程动力学建模，与基于最小描述长度（MDL）和算法马尔可夫条件（AMC）的因果评分结合起来，在不规则采样条件下，通过比较不同因果结构对数据的“压缩能力”，来识别真正的因果关系，并给出了明确的理论保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说得更直白一点，这项工作把连续时间动力学建模，从“拟合得像不像真实轨迹”，推进到了“学到的机制在因果上是不是对的”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;论文名：Model Change for Description Logic Concepts（描述逻辑概念的模型变更）&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/88c11c7b95b7b3ab6a4ca75fdc929016.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此论文还未公开上传，暂无链接。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;关于表示学习，重新审视结构本身&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;论文名：LLM2CLIP: Powerful Language Model Unlocks Richer Cross-Modality Representation（LLM2CLIP：强大语言模型解锁更丰富跨模态表征）&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c9/c92d445ea035e37732f71822fb76ce9b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLIP（Contrastive Language–Image Pre-training）是一个经典的多模态模型，通过对比学习，将图像和文本映射到同一语义空间，从而实现“以文找图、以图找文”等跨模态理解能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;CLIP 在跨模态检索和基础语义对齐上表现出色，但它也有一个公认的短板：文本编码器容量较小、上下文长度有限，对长、复杂、信息密集的文本理解能力不足。这在长文本检索、多语言理解等场景中尤为明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;LLM在语言理解、上下文建模和世界知识方面，倒是明显更强。但问题在于，LLM不能直接接入CLIP。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;——一方面，原生 LLM 的句向量并不具备对比学习所需的“高区分度”，很难有效拉开不同 caption 之间的距离；另一方面，如果端到端联合训练 LLM 和 CLIP，计算成本也高得不可接受。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这篇论文提出了一种系统化的新方法，名曰：LLM2CLIP，顾名思义，把LLM“接入”或“输送”到CLIP里，用LLM 来替代或者增强CLIP的文本能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/10/10b9416b60bb2c36bf84d71b94e80684.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这并不是简单地把LLM直接接进去。作者给出的解决路径，是分两步走，各解决一个关键障碍。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一步，是先让 LLM 成为一个“合格的文本 embedding 模型”。为此，论文提出了&amp;nbsp;Caption-Contrastive Fine-tuning：&lt;/p&gt;&lt;p&gt;使用同一张图像对应的不同 caption 作为正样本，通过对比学习，让语义相近的描述在向量空间中更接近、不相关的描述更远；同时配合平均池化、双向注意力和 LoRA 等结构调整，提升句向量的稳定性和可区分性。&lt;/p&gt;&lt;p&gt;这一步的目标并不是做多模态，而是把 LLM 训练成一个真正“好用”的文本表示器。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二步，则是直接用经过处理的 LLM，替换掉 CLIP 原有的文本编码器。在这一阶段，LLM 参数被冻结，仅训练一个非常轻量的 adaptor 来对齐视觉特征，使整体训练流程几乎等同于普通的 CLIP 微调，算力成本基本不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大量消融实验表明：同时保留两个文本编码器、或试图在两者之间做复杂对齐，效果反而更差；“直接替换”是最简单、也是最有效的方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;实验结果显示，LLM2CLIP 在长文本检索任务上提升最为显著，短文本检索也有稳定增益，同时多语言检索能力明显增强。更重要的是，这些提升是在仅使用百万级数据、几乎不增加训练成本的前提下实现的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总体来看，LLM2CLIP的价值在于，它没有重造一个更大的多模态模型，而是用一种低成本、可复用的方式，把“语言理解”这块短板，直接补进了CLIP的核心结构里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;论文名：High-Pass Matters: Theoretical Insights and Sheaflet-Based Design for Hypergraph Neural Networks（高频信息的重要性：面向超图神经网络的理论分析与 Sheaflet 方法设计）&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/31/3165f5d7858170f3320221770119142f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此论文还未公开上传，暂无链接。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;总而言之，这些研究都在把关注点从结果层面的性能，推向模型内部的感知、结构和机制本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;论文地址：&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2508.10333&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2411.04997&lt;/p&gt;&lt;p&gt;https://arxiv.org/abs/2512.14361&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://aaai.org/about-aaai/aaai-awards/aaai-conference-paper-awards-and-recognition/&lt;/p&gt;&lt;p&gt;https://aaai.org/about-aaai/aaai-awards/aaai-classic-paper-award/?utm_source&lt;/p&gt;&lt;p&gt;https://aaai.org/conference/aaai/aaai-26/award-talks/&lt;/p&gt;</description><link>https://www.infoq.cn/article/KXaviFJ5cNI4qylQg39x</link><guid isPermaLink="false">https://www.infoq.cn/article/KXaviFJ5cNI4qylQg39x</guid><pubDate>Fri, 23 Jan 2026 16:07:54 GMT</pubDate><author>木子</author><category>AI 工程化</category></item><item><title>学界大佬吵架金句不断，智谱和MiniMax太优秀被点名，Agent竟然能写GPU内核了？！</title><description>&lt;p&gt;“如果一个 AI 能解 IMO，但解决不了任何现实问题，那它不是通用人工智能。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是卡内基梅隆大学助理教授、艾伦人工智能研究所研究科学家，蒂姆·德特默斯对 AGI 给出的判断，他用一篇文章&amp;nbsp;《通用人工智能为何不会成为现实》&amp;nbsp;直接把 AGI 从神坛上拽了下来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4c/4c6584d47ba2a183ad6f2930da089fd8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，几天后，加州大学圣地亚哥分校助理教授、Together AI 内核副总裁丹·傅，给出了完全相反的判断。他写了一篇&amp;nbsp;《通用人工智能终将成为现实》，说&amp;nbsp;我们也许早就已经实现了 AGI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/9b/9bf29a2d6fd3899bdca567de55251a0a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，两篇文章，一场关于&amp;nbsp;“AGI ”&amp;nbsp;的争论，被带进了播客现场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这场讨论并非空谈，两位嘉宾都是同时深耕学术界与产业界的一线研究者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆·德特默斯长期深耕深度学习量化领域，即模型压缩，如何在更低精度、更少算力下，让模型保持可用性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/88/884d67cf5e260d675320b5e89d9bd3f9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在蒂姆·德特默斯看来，判断 AGI 是否成立，首先要回到一个常被忽略的前提：计算是物理的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，内存迁移、带宽、延迟，以及冯·诺依曼瓶颈，决定了算力不可能无限扩张。他说&amp;nbsp;“几乎所有指数增长，最终都会撞上资源和物理极限”。&amp;nbsp;所以，指数增长终将放缓，Scaling Law 也不例外。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但丹·傅显然不这么看。在他看来，现在谈“算力见顶”，还太早了。丹·傅每天都在和 GPU 内核、算力利用率打交道，在他看来，“我们甚至还没真正用好上一代硬件。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55c6aa2437726c05a36eb881da00441c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在现实系统中，算力其实被严重低估和浪费了，&amp;nbsp;大量性能消耗在内核调度、系统开销和工程细节上。更关键的是，人们今天评测和使用的“最强模型”，往往是基于一到两年前的算力集群训练出来的，它们并不能代表当下硬件和大规模集群所能达到的真实上限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他因此提出了一个直观的估算思路，用来说明算力增长的潜力来自多个维度的叠加：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;新一代硬件 带来约 2–3 倍 的性能提升；系统与工程优化 将算力利用率提升 约 3 倍；更大规模的集群 再带来 约 10 倍 的规模效应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这三者相乘，意味着可用算力在理论上可以提升接近&amp;nbsp;90 倍。这并不是纸面上的推算，而是正在产业中逐步发生、逐步兑现的现实潜力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有意思的是，当争论继续推进，两人反而在一个问题上开始靠拢：AGI 到底是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于 AGI 的定义，大致有两种主流视角：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一种从认知能力出发，看模型能否覆盖足够多的认知任务；&lt;/p&gt;&lt;p&gt;另一种则从经济角度出发，看它是否真的改变了生产方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一点上，双方达成一个共识：AGI 是什么并不重要，重要的是，它有没有改变我们工作的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在访谈后后半部分，大家从未来拉回到了现实，Agent 成为了关键话题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹·傅在节目中提到一个有趣的时间点：2025 年 6 月，&amp;nbsp;那是他第一次意识到，Agent 可能真的越过了拐点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/30/30d521323b1daf70213350898c21572e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他当时发现机器学习工程中最难的技能之一、编程领域的终极难题——“GPU 内核编程”&amp;nbsp;被代码智能体啃下来了。他自己亲测：原本一个 GPU 内核功能开发得磨一周，那天靠着代码智能体，一天就搞定了三四个，工作效率直接提升了 5 倍。而他的团队用上后，那些原本需要整支团队耗数月的复杂系统开发，也变得轻装上阵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让丹·傅想起了自己对自动驾驶的态度变化，从长期怀疑到真正坐上 Waymo，他意识到技术的突破可能藏在某个猝不及防的瞬间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对 Agent 的爆发式潜力，蒂姆·德特默斯曾发布了一篇掷地有声的文章&amp;nbsp;《要么善用 Agent，要么被时代淘汰》。在他看来，代码 Agent 本身就是高度通用的 Agent，因为代码几乎可以描述和解决所有数字化问题。他甚至直言，“超过 90% 的代码和文本，本就应该由 Agent 来生成。但同时他也强调，“人类必须对最终结果承担责任，而非盲目依赖 AI 的输出。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/52/527887224b94aa5fdc60e1ae06cb1969.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;两人将 Agent 形象地比作“需要精细化管理的实习生”，只要给它明确背景信息、拆解任务边界、设定执行约束，人类无需过度干预其执行过程，而是把注意力聚焦在把控方向上，用专业判断力校验结果。而在 Agent 时代，真正吃到红利的将是有深厚积累的专家，其专业基础越深厚，Agent 能为其创造的效率增量就越显著。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在节目的最后，关乎对 AI 行业未来的预判，双方抛出了一系列深刻洞见。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他们看来，小模型会成为行业新热点、开源模型会进一步飞跃；新硬件、多模态、端侧 AI 都会有进一步发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，硬件赛道将走向多元化发展，模型训练与推理环节的专业化分化会进一步加剧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更值得关注的是，Transformer 架构独霸天下的时代会落幕，各类新架构会登上时代舞台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他们还特别提到了中国的&amp;nbsp;GLM-4.7、MiniMax、DeepSeek 等优秀模型，对中国大模型的快速进步表达了高度认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他们看来，相比技术路线相对集中的美国，中国团队反而更敢于探索多种可能性，比如状态空间模型、线性注意力以及混合架构等，通过架构创新或极致性能，让开源模型脱颖而出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，他们也指出，中国的模型团队在技术路线上更&amp;nbsp;务实。与“先做出最强模型，再等待应用出现”的硅谷思路不同，中国团队更关注模型是否真正能落地、是否能在现实场景中产生价值。正是这种务实的发展思维，可能会在未来深刻影响人工智能的技术形态以及它所能创造的社会价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是播客全文，更多精彩细节，欢迎来看：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“AGI 能否成为现实”之争&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：蒂姆，几周前你发表了一篇极具争议性的精彩博文，标题是&amp;nbsp;《通用人工智能为何不会成为现实》。而丹，你在几天后也发布了一篇同样引人入胜的回应博文，标题为&amp;nbsp;《通用人工智能终将成为现实》。我想先了解一下二位的背景，你们都有着一个有趣的特点，就是兼具产业界和学术界的从业经历。蒂姆，不如你先讲讲吧。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我是卡内基梅隆大学机器学习与计算机科学系的助理教授，同时也是艾伦人工智能研究所的研究科学家。&lt;/p&gt;&lt;p&gt;我过往的研究主要聚焦于高效深度学习量化技术，简单来说就是模型压缩，&amp;nbsp;把大模型从 16 位精度压缩到 4 位精度左右，这方面我做了不少核心研究。比如一种高效的微调方法，我们将模型压缩至 4 位精度，在模型上使用适配器，这样所需的内存相比全精度模型能减少多达 16 倍。&lt;/p&gt;&lt;p&gt;目前我正致力于代码 Agent 的研究，&amp;nbsp;我们将在约两周后发布一项非常令人振奋的成果，打造出了目前最先进的 Agent，它能快速适配私有数据，在任意代码库上都能实现出色的性能表现，这一成果真的让人充满期待。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：丹，该你了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我是加州大学圣地亚哥分校的助理教授，同时担任合聚人工智能公司的内核副总裁。&lt;/p&gt;&lt;p&gt;在产业界，我的工作主要集中在提升模型的运行速度，GPU 内核正是将模型转化为实际在 GPU 上运行程序的关键，你可以把它理解为专门的 GPU 程序。&lt;/p&gt;&lt;p&gt;我的博士阶段以及实验室的大量研究都围绕这一方向展开，比如我研发了快速注意力机制，这是一款针对当下多数语言模型核心运算的高效内核。我还研究了 Transformer 架构之外的替代架构，&amp;nbsp;比如状态空间模型等。&lt;/p&gt;&lt;p&gt;在合聚人工智能，我主要关注如何打造当下最优的语言模型，以及如何进一步提升它们的运行速度。&lt;/p&gt;&lt;p&gt;就在本期节目录制的今早，我们还和库尔索公司联合发布了一篇博文，介绍了我们如何为其多款模型实现加速，并助力他们在英伟达的布莱克韦尔（Blackwell） GPU 上推出了作曲者 2.0 模型，这大概就是我的工作内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;从 AGI 的定义，聊到对 AGI 的现实判断&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：接下来我们聊聊通用人工智能的话题，节目后半段再探讨 Agent 和代码 Agent，以及二位的相关见解。通用人工智能这个术语被大家广泛使用，但我想大家都认同，目前还没有人能准确定义它。为了本次探讨，二位认为什么样的通用人工智能定义是实用的？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：当然。我和蒂姆在这一系列博文中&amp;nbsp;反复探讨的一个问题，就是通用人工智能的定义。&lt;/p&gt;&lt;p&gt;就我而言，我最近一直在思考，以当下的模型发展水平，尤其是语言模型，再结合后续会谈到的 Agent 来看，以 5 年前、10 年前，甚至我和蒂姆刚开始读博时任何人给出的通用人工智能定义，我们其实已经实现了当时的设想。如今的模型能写代码、能生成人类语言，即便有时用词上会有些小瑕疵，但确实能完成这些令人惊叹的任务。我还会思考，这种技术发展到何种程度，会引发一场新的工业革命，真正改变我们当下的工作方式，并产生巨大的经济影响。&lt;/p&gt;&lt;p&gt;在软件工程领域，我觉得我们已经身处这样的变革中，或者说即将迎来全面变革。虽然在一些高度专业化的领域，比如模型未必能写出世界上最优质的福兰语和钴语言代码，但在网页开发，甚至很多底层系统工程方面，它们的表现已经非常出色。&lt;/p&gt;&lt;p&gt;我写那篇博文的一个原因就是，审视当下的发展，我们或许已经实现了通用人工智能，或者说某种形式的通用人工智能。即便尚未完全实现，下一代正在训练的模型，只要比当下的模型表现更好，我们就已经取得了令人惊叹的突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我写那篇博文时发现，自己竟然忘了在文中给出通用人工智能的定义，尽管整篇文章都围绕这个主题展开。我想这在某种程度上也反映了我们对通用人工智能的思考现状 —— 我们并未认真去界定它。当然，目前存在多种定义，各有优劣，正如你所说，没有一个定义能获得所有人的认同。&lt;/p&gt;&lt;p&gt;我简单提几种比较主流的，一种是将通用人工智能视为认知能力、认知任务的集合，关注模型能完成哪些认知层面的工作。&amp;nbsp;软件工程、文本创作都是高度依赖认知的任务，而让机器人在空间中移动则更偏向操作层面，当然也有人认为肢体移动的规划也属于认知范畴，但多数人会将其区分开来，认为所有数字化的任务都属于认知领域，物理层面的操作则超出了这一范畴。&lt;/p&gt;&lt;p&gt;另一种我认为很有意义的定义视角是经济层面，看人工智能是否能引发一场新的工业革命，是否具备广泛的实用性，能应用到各个领域，推动各类工作的效率提升，就像计算机的出现那样。&amp;nbsp;当然，计算机刚出现时，生产率其实出现了下降，直到其在经济中广泛普及，生产率才重新回升。通用人工智能的发展或许也会经历类似过程，在软件工程等领域，其带来的效率提升已经十分显著。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们直接切入核心争论吧。蒂姆，你曾提到 AGI 的相关构想的起源，这一点让我觉得很有意思，你能展开讲讲吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：好的。先梳理一下整体的背景，当下关于 AGI 的一些观点，根植于特定的思维模式，主要来源于有效利他主义社群和理性主义社群。&lt;/p&gt;&lt;p&gt;我 15 年前也曾是这些社群的一员。在推特上，总能看到有人说 “两年内就能实现通用人工智能”，一年后又有人说 “两年内就能实现通用人工智能”，年年如此。我觉得这种想法有些草率，也体现出一种信息茧房的状态，持这种观点的人很少接触不同的想法。这也是我写那篇博文的主要动机，我希望提出一些不同的观点，为当下主流的思考提供一种反视角。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;算力是否见顶&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你核心的观点是，这些构想与实际的计算现实之间存在矛盾，这样概括准确吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：没错。这其中既涉及物理层面的限制，也有理论层面的问题，而这两方面都存在&amp;nbsp;一个共同的规律 —— 收益递减。所有指数级增长的事物最终都会放缓，因为发展需要资源，而资源总会耗尽，这里的资源可以有多种解读。&lt;/p&gt;&lt;p&gt;从物理层面来看，技术的进一步发展会变得越来越困难，几乎所有研究和开发领域都是如此。前期的进展往往容易实现，而后续要取得突破，需要投入更多资源，发展速度也会越来越慢。&lt;/p&gt;&lt;p&gt;再看计算设备的物理现实，以及计算本身的结构，&amp;nbsp;其实有用的计算主要包含两个环节：&lt;/p&gt;&lt;p&gt;首先是将数据从不同位置收集起来，汇聚到指定位置，然后对这些信息进行整合，完成信息的转化处理。简单来说，就是结合已知信息，计算出未知的新信息。有用的信息，必然是从已有的信息中转化而来的。如果只是大量转移信息，却不进行处理，就无法产生新信息；如果只是对现有信息进行大量计算，又会错失跨领域的洞察和间接的启发。我认为这一点与我们当下的神经网络架构高度契合。&lt;/p&gt;&lt;p&gt;早期的卷积神经网络表现出色，原因就在于它们几乎不怎么移动内存，而是专注于大量计算，这意味着这类设备需要强大的浮点运算能力，而内存带宽则没那么重要。当发展到大规模密集计算、大矩阵运算阶段，就到了当下神经网络的发展方向，但此时仍保留着循环机制的特点，需要关注之前的状态。不过由于循环的特性，计算的内存复用率极低。&lt;/p&gt;&lt;p&gt;而 Transformer 架构，先是通过大矩阵将前一层的输入信息进行转化，再通过注意力机制实现跨时间或空间的信息关联。我认为这是处理信息最根本的两种方式：一是让信息之间建立关联，或对信息进行转化；&lt;/p&gt;&lt;p&gt;二是让信息与关联较远的其他信息建立联系，也就是挖掘长期关联，并基于已有信息进行转化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你认为这一发展进程正在放缓，对吧？你的博文中有一句非常引人注目的话，称 “图形处理器的发展将不再有实质性突破”，这是核心观点，能说说原因吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：这个观点包含两层含义，首先是一个非常根本的物理问题，也就是我刚才提到的内存转移和计算的关系。&lt;/p&gt;&lt;p&gt;计算要产生价值，就必须将内存数据转移到进行计算的本地区域，这其实是一个几何问题。你需要一个大容量的信息存储区，然后将其中的信息转移到计算区域。而我们已经找到了实现这一过程的最优物理方式：配备大容量但速度较慢的动态随机存取存储器，再将数据转移到高速缓存中。&lt;/p&gt;&lt;p&gt;从几何结构来看，这是实现高速运算的最优解，针对特定规模的计算任务，这种架构的效率是最高的。如果是矩阵乘法这类不同规模的计算任务，就需要使用图形处理器而非中央处理器，因为图形处理器虽然延迟更高，但吞吐量更大，能传输更多数据，只是速度稍慢。我们可以对缓存的结构、大小，以及核心的共享方式做一些微调，但归根结底，核心的问题始终存在 —— 这是一个几何难题，空间的利用方式是有限的，这就决定了数据的访问模式和延迟始终存在固定的限制，其中最大的延迟来自大容量的动态随机存取存储器，这也是主要的性能瓶颈。这一瓶颈也被称为&amp;nbsp;冯・诺依曼瓶颈，几乎所有计算机都受此限制，具体来说，就是需要将程序传输到执行区域才能运行。对于神经网络而言，就是要将权重和输入数据传输到张量核心这一执行单元。&lt;/p&gt;&lt;p&gt;想要绕开这一瓶颈的方法寥寥无几，唯一的途径是进行本地内存存储和本地计算，市面上也有一些处理器尝试实现这一点，比如存算一体处理器，能在很大程度上在芯片内部解决冯・诺依曼瓶颈问题，但这类处理器仍需要从外部向芯片内传输数据，这就使得冯・诺依曼瓶颈从芯片内部转移到了存储设备或网络层面，问题只是发生了转移，本质并未改变。你仍需要通过网络将存储在磁盘或内存中的程序加载到芯片中，这还是同一个物理问题，只是调整了几个变量而已。这是问题的第一个层面，目前还没有能解决这一问题的架构。&lt;/p&gt;&lt;p&gt;第二个层面，也是我的核心观点所在：想要突破瓶颈，需要依靠新技术，但当新技术的潜力被充分挖掘后，又需要新的技术实现进一步突破。&lt;/p&gt;&lt;p&gt;比如，我们从动态随机存取存储器发展到了高带宽存储器，也就是堆叠式的动态随机存取存储器，速度大幅提升，但这种存储器的堆叠层数有限，因为其制造和测试的难度极高，良品率很低。到 2026 年，高带宽存储器的产能将会不足，无法实现规模化生产，因为制造难度实在太大。我们已经见证了诸多技术创新，张量核心的出现是一大突破，8 位精度、4 位精度的量化技术也相继落地，我和其他研究者的研究都表明，这些技术在信息论层面和实际应用中都是接近最优的。&lt;/p&gt;&lt;p&gt;如果基于足够多的数据进行训练，4 位精度是不够的，实际需要 8 位精度，这意味着量化技术已经发展到了极限。硬件的潜力也被挖掘殆尽，目前没有新的技术可以突破，我们能做的只是优化制造工艺，降低成本，却无法提升速度。各项功能的开发也已到极致，稀疏化技术是很多人尝试的方向，这一研究已经持续了 50 年，我自己也做过相关尝试，这或许是最后一个可探索的方向，但 4 位精度的量化技术已经意味着量化领域的发展走到了尽头。&lt;/p&gt;&lt;p&gt;简单来说&amp;nbsp;，功能和硬件都已被开发到极限，这就是我们当下的处境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：太有意思了。丹，你对这些观点有什么看法？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我非常认可蒂姆的这篇博文，因为当下有不少关于通用人工智能的讨论，只是简单地按照指数增长的趋势去推演，认为到某个时间点，人工智能会发展到掌控整个宇宙的程度，我一直觉得这种思考方式有些片面。我认同蒂姆从实际物理限制角度出发的分析，正如他所说，这些都是依赖物理输入、进行实际物理计算的系统。&lt;/p&gt;&lt;p&gt;我的观点是，看看当下的系统和我们训练的模型，我们甚至连上一代硬件的潜力都远未充分挖掘，更不用说新推出的硬件了。&lt;/p&gt;&lt;p&gt;从技术层面，我在博文中主要提出了两个核心观点：&lt;/p&gt;&lt;p&gt;第一，看看当下那些表现出色的模型，我在博文中主要以开源模型为例，因为开源领域会更多地披露模型的训练过程和所耗资源，而开放人工智能和思存人工智能等公司并未公开相关数据。&lt;/p&gt;&lt;p&gt;以 DeepSeek 模型为例，这是目前最优秀的开源模型之一，它在 2024 年底完成训练，使用的是上一代的英伟达 H800 GPU，这款显卡因出口限制做了性能阉割，并非原版 H100。根据公开报告，该模型的训练使用了约 2000 块 H800 显卡，耗时约一个月。计算一下实际的算力利用情况会发现，芯片的有效利用率仅约 20%，行业内将这一指标称为模型浮点运算利用率。而在 21 世纪 20 年代初，我们在旧硬件上训练不同架构的模型时，轻松就能实现 50% 甚至 60% 的模型浮点运算利用率。如果能将这一指标提升，再加上我的好友崔最近发布了一系列能优化模型训练的新内核，单是这一项优化，就能让算力利用率提升 3 倍。&lt;/p&gt;&lt;p&gt;第二，需要意识到的是，这款 2024 年年中开始训练的 DeepSeek 模型，在 2026 年初仍是众多优秀开源或类开源模型的基础。而从那之后，我们已经搭建了全新的算力集群，搭载了当下最新的硬件，比如英伟达的布莱克韦尔系列显卡。普尔赛德、瑞弗莱克申等公司都在搭建包含数万个 B200、GB200 芯片的算力集群。&lt;/p&gt;&lt;p&gt;对比来看，新一代硬件即便保持和之前相同的精度、相同的配置，运算速度也能提升 2 至 3 倍，算力集群的规模更是扩大了 10 倍，再加上 3 倍的纯技术优化空间，整体的可用算力能提升 3×3×10，也就是 90 倍。这还没有考虑未来的算力集群建设，只是当下已经落地、有人正在用于模型训练的集群。&lt;/p&gt;&lt;p&gt;我的核心观点是，单从这些基础的硬件条件来看，就能发现可用算力相比我们当下所依赖的模型，还有多达两个数量级的提升空间，也就是 100 倍。&amp;nbsp;当然，我们可以争论算力规模扩大是否会带来收益递减，缩放曲线是否依然有效，但现实的算力潜力就摆在眼前。&lt;/p&gt;&lt;p&gt;这还没考虑蒂姆提到的那些点，比如目前的训练大多采用 8 位精度，而 4 位精度的训练方法才刚刚开始形成相关研究成果；GB200 芯片有 72 个连接速度极快的核心，而我们甚至还没看到基于这款芯片训练的首个预训练模型。开放人工智能的报告中提到，GPT-5.2 是首个基于 H100、H200 和 GP200 芯片训练的模型，这在我看来，意味着它的预训练其实是在老旧的算力集群上完成的，只是在新的 GP200 芯片上进行了一些微调。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你提到，不仅硬件的利用率不足，模型本身也是硬件发展的滞后指标，对吧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：没错。我们当下能使用、能体验到的模型，都是在一两年前搭建的算力集群上完成预训练的。&lt;/p&gt;&lt;p&gt;因为搭建一个算力集群需要时间，完成大规模的预训练需要时间，后续的微调、人类反馈强化学习等后训练环节也需要时间。所以我们当下所看到的、用来衡量模型质量的这些模型，其实都是在一年半前的硬件上训练的。而在这之后，我们已经搭建了规模大得多的算力集群，不难想象，这些集群会被用于训练新一代模型。&lt;/p&gt;&lt;p&gt;也就是说，我们当下所依赖的优质模型，训练所使用的硬件其实已经相当老旧，而我们拥有了新一代的硬件、更多的软件优化方案，更不用说架构层面的创新了。&lt;/p&gt;&lt;p&gt;蒂姆刚才提到，处理数据的核心是先转移、再计算，而变形金刚架构其实一直在发展，只是在研究者看来，发展速度稍慢。但我们能看到，计算的核心方式已经在发生变化，哪怕再找到 1.5 倍或 2 倍的优化空间，整体的可用算力就能达到 100 甚至 150 倍。所以当下还有大量的算力潜力可以挖掘，用来训练更优质的模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp; 预训练是综合训练，后训练是专项训练&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我理解这场讨论的核心是预训练，也就是我们能否用更多的数据和算力训练出更大的模型。但在本播客之前的对话中，很多人都强调后训练的重要性，以及构建结合预训练和强化学习的人工智能系统的意义。这一点在当下的讨论中该如何定位？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：这是个非常好的问题，我和蒂姆的博文其实都没有重点探讨这一点。我喜欢这样比喻，预训练就像是在健身房进行的综合力量训练，通过大重量训练提升整体的力量和能力；而后训练就像是针对特定项目的专项训练，让你在具体任务上表现更出色。&lt;/p&gt;&lt;p&gt;从算力消耗来看，历史上预训练消耗的算力占绝对主导，其目的是打造具备通用能力的模型，让模型掌握大量知识，能完成多种任务，甚至拥有比普通人更多的知识储备，比如我自己的知识量肯定比不上聊天生成预训练转换器。&lt;/p&gt;&lt;p&gt;而后训练的作用，一方面是让模型变得更实用，比如聊天生成预训练转换器，能理解用户的需求，并尽力完成任务；另一方面，我们也发现，后训练正越来越多地被用于培养模型的特定技能。比如擅长辅助编程的模型，虽然依托于预训练积累的大量知识，但正是通过后训练，才让它在编程领域具备了出色的能力；同理，擅长法律工作的模型，也是在预训练的基础上，通过后训练实现了专业领域的优化。&lt;/p&gt;&lt;p&gt;从纯计算的角度来看，预训练的算力消耗通常远大于后训练。&amp;nbsp;后训练的工作，我虽然不是这方面的专家，但感觉更多地像是如何打造一款实用的产品，如何获取用户反馈，诸如此类。&lt;/p&gt;&lt;p&gt;当然，也有一种可能是，下一代预训练模型的基础能力已经足够强大，只要针对经济领域的各个垂直赛道进行后训练，就能打造出极具实用性的模型。所以这也是计算领域的另一个重要维度，或许我们根本不需要那 100 倍的额外算力，更多的是需要像培养人类一样，深入理解问题，找到合适的训练方法 —— 就像你如何培养一名实习生完成特定任务，如何让一个能力强大的预训练模型发挥出实际价值，这正是后训练要解决的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：二位都提到了 “实用性” 这个概念，这或许是你们观点的交汇点。通用人工智能的定义众说纷纭，但最终的关键还是看它在产业中的实际实用性。所以即便由于收益递减，我们无法实现那个大家都无法准确定义的、理想化的通用人工智能，也无关紧要，因为我们还有巨大的潜力可以挖掘，足以让人工智能在整个经济领域发挥真正的价值，而不仅限于编程领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：没错。我那篇博文的核心结论正是如此，我们不必过分纠结于通用人工智能的定义，更应该思考如何让人工智能发挥最大的实用价值，而这不仅关乎模型本身，丹刚才提到后训练是产品化的过程，这一点很重要。计算机的发展历程告诉我们，技术在经济中的普及需要一种截然不同的思维模式。&lt;/p&gt;&lt;p&gt;美国的思维模式往往是 “打造出最优的模型，自然会有人使用”，而中国的思维模式则更注重务实，思考如何让技术惠及更多人。我认为这种务实的思维模式至关重要。谈及实用性，一方面是模型的能力，另一方面就是这种发展思维。&lt;/p&gt;&lt;p&gt;我相信我和丹，以及大多数人都会认同一个观点：如果一个人工智能能完成数学奥林匹克竞赛这类高难度任务，却无法解决任何实际问题，那它算不上通用人工智能。而当下的模型已经具备了实用性，所以不会出现那种 “有能力却无用处” 的情况。&lt;/p&gt;&lt;p&gt;我们真正追求的，是实用性极强的模型，而这样的模型我们已经拥有，并且还能不断优化。我认为按照某些定义，我们或许无法实现通用人工智能，但人工智能必将产生巨大的社会影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我想补充一点，蒂姆你提到了经济领域的物理性工作和知识性工作的划分，美中两国在这方面的差异非常有意思。&lt;/p&gt;&lt;p&gt;最近有一本丹・王写的书很火，探讨了制造型经济、工程型经济与偏法务型经济的区别。美国有大量优秀的知识性工作有待人工智能去赋能，而从经济的实际产业结构来看，医疗、教育占了很大比重，科技领域虽然也是重要组成部分，引领着股市的走向，但还有更多领域等待挖掘。&lt;/p&gt;&lt;p&gt;现在有很多优秀的研究者正在尝试用新一代模型研发新药、推动医疗领域的实际变革；如果机器人技术能实现突破，助力完成一些体力劳动 —— 未必是建造房屋这类重活，而是日常的家务劳动，那将挖掘出经济领域的巨大潜力。这些方向的发展已经能看到初步的成果，自动驾驶的发展历程对我很有启发。&lt;/p&gt;&lt;p&gt;在我读博初期，大概 2018、2019 年，我对自动驾驶持非常怀疑的态度，当时大家总说自动驾驶 “再有一两年就能实现”，专家则说 “五年内有望落地”。但去年我乘坐了威莫的自动驾驶车辆，如今在加州湾区，我甚至能使用威莫的高速自动驾驶服务。理论上，我现在甚至可以卖掉自己的车 —— 当然我不会这么做，因为我个人喜欢开车。&lt;/p&gt;&lt;p&gt;但技术的进步就是这样，在这之前一直毫无起色，突然有一天就实现了突破，你会发现它不仅表现出色，甚至比优步、出租车这类人工服务还要好。如果人工智能在家庭清洁、洗碗这类家务劳动上也实现这样的突破，那将是非常令人振奋的，也会彻底改变人们的看法。我自己并非机器人领域的研究者，但一直密切关注着这个领域的发展。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;多硬件、多芯片的未来方向&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：丹，借着这个话题，我想问问，从你的观察来看，人工智能领域是否会朝着多硬件、多芯片的方向发展？显然英伟达的发展势头迅猛，还有赛博拉斯等公司，以及众多从底层技术切入的专用集成电路企业。从你深耕底层技术的视角，你怎么看这一趋势？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：这是个很棒的问题，我在实验室的工作中会花大量时间思考这个问题，产业界的工作中也会密切关注。当下正处于一个非常令人振奋的阶段：英伟达的芯片性能强劲、稳定性高，围绕其构建的软件生态也非常完善；而 AMD 的芯片也开始展现出同样的潜力，相关的研究也在推进。&lt;/p&gt;&lt;p&gt;比如在实验室，我的好友西姆龙・奥罗拉主导开发了一个名为希普基滕斯的库，核心就是探索如何设计合适的软件抽象层，实现 AMD GPU 的编程。研究发现，AMD GPU 和英伟达 GPU 的软件抽象层存在明显差异，即便这两款 GPU 的参数规格相对接近 —— 更不用说和格罗克、赛博拉斯、萨博诺瓦等公司的芯片相比了，它们的编程方式也截然不同。&lt;/p&gt;&lt;p&gt;现在越来越多的人开始关注这一领域，投入时间和精力进行研究。英伟达收购了格罗克，当下张量处理单元也备受关注，赛博拉斯和开放人工智能也刚宣布达成合作。所以未来必然会涌现出更多的硬件方案，英伟达无疑会继续保持良好的发展态势，甚至在本期节目录制时，其市值已经突破 5 万亿美元，但硬件领域的多样性会大幅提升，尤其是在模型推理层面。&lt;/p&gt;&lt;p&gt;训练和推理是两种截然不同的计算过程，因此需要的芯片也大相径庭。在推理层面，模型可能需要在手机、笔记本电脑等本地设备上运行。&amp;nbsp;我的手机是一款几年前的苹果手机，但其运算能力已经超过了我读博初期使用的一些 GPU，硬件算力的增长速度令人惊叹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2025 年 6 月是 Agent 的拐点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：丹，你刚才提到自动驾驶实现突破的那个节点，Agent 的发展是否也已经到了这样的时刻？你还提到过 “软件奇点”，我们当下是否正处于 Agent 发展的关键突破点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我认为是的。就我个人的经历而言，这个突破点出现在 2025 年 6 月左右。&lt;/p&gt;&lt;p&gt;给大家做个背景介绍，我在合聚人工智能的日常工作就是编写这些 GPU 内核，在机器学习领域，GPU 内核的编程被认为是最难掌握的技能之一，它需要高度的并行化设计，使用的是 C++ 这种资深工程师使用了数十年的老牌语言，而非 Python 这类易用的语言。招聘能编写 GPU 内核的工程师难度极大，这是一项极具挑战性的技能，无疑是编程能力的顶尖体现。&lt;/p&gt;&lt;p&gt;而 2025 年 6 月，我们有了一个非常有趣的发现：云代码、库尔索 Agent 这类代码 Agent，在编写 GPU 内核方面的表现非常出色。那一周，我完成了三四个原本各自需要一周时间才能完成的功能开发，全部工作一天就搞定了。&amp;nbsp;当时我就意识到，这个工具让我这个内核领域的专家，工作效率提升了 5 倍。&lt;/p&gt;&lt;p&gt;我让团队都开始使用这个工具，现在团队借助它搭建了许多复杂的系统，能快速完成原本需要整个团队耗时数月才能实现的功能开发。而 GPU 内核编程，正是编程领域最难的 “终极挑战”，所以在我们看来，代码 Agent，尤其是在高难度的 GPU 内核编程领域，已经实现了关键性的突破。&lt;/p&gt;&lt;p&gt;几个月前，我在斯拉什大会上做了一场演讲，提出了&amp;nbsp;“软件奇点”&amp;nbsp;的概念，核心就是意识到在软件工程领域，即便是这类非常小众的高难度技能，人工智能的表现也已经超越了普通程序员，甚至能为资深程序员带来效率的大幅提升。就本期节目录制的当下而言，让 Agent 独立完成开发，可能还无法产出完美的结果，但如果资深程序员借助这些工具，工作效率能提升 10 倍，这是一个非常令人振奋的发展阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;要么善用 Agent，要么被时代淘汰。&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊到 Agent，蒂姆，你最近还发表了一篇精彩的博文，标题是 《要么善用 Agent，要么被时代淘汰》，其中探讨了代码 Agent 和适用于其他各类任务的 Agent。从代码 Agent 的出色表现，到 Agent 在日常生活各领域发挥实用价值，这一发展进程当下处于什么阶段？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我写这篇博文，也是因为发现使用代码 Agent 能为各类任务带来巨大的生产效率提升。作为一名教授，我平时的编程工作并不多，但借助代码 Agent，编程变得前所未有的轻松，这在以往是难以想象的。&lt;/p&gt;&lt;p&gt;当然，Agent 在非编程任务上的表现也同样出色。从我自身的体验来看，生产效率的提升幅度不一，有时是两三倍，有时甚至能达到 10 倍，而且工作质量没有下降，甚至有时还能提升。Agent 的能力或许未必比我强，但它不会疲惫，不会犯低级错误，也不会在整合复杂信息时出现认知上的困难 —— 这和丹刚才提到的 GPU 内核编程的情况是一样的。&lt;/p&gt;&lt;p&gt;我认为马特你将其分为代码 Agent 和通用 Agent，但在我看来，代码 Agent 本身就是通用 Agent。代码 Agent 能编写程序解决各类问题，而代码的通用性极强，任何数字化的问题都能通过代码解决。代码 Agent 让解决问题的过程变得无比轻松，让我们能以以往无法想象的方式和速度解决各类问题，实现多任务并行处理。Agent 不会疲惫，可以持续工作，让工作变得轻松很多。&lt;/p&gt;&lt;p&gt;我的博文中有一个观点我自己很认同，开篇我先区分了炒作和现实，而后基于自己在直播中测试 Agent 的实际体验得出结论&amp;nbsp;：超过 90% 的代码和文本都应该由 Agent 来生成，不这么做，就会被时代淘汰。&amp;nbsp;我想对于很多工程师来说，这一点已经成为现实。&lt;/p&gt;&lt;p&gt;有些人认为，Agent 生成的代码和文本质量一定低下，但关键在于，你需要对 Agent 的输出进行检查和编辑。你所做的这 10% 的工作，能带来巨大的改变。通过这种对输出内容的检查、编辑和优化，让成果成为属于自己的作品。&lt;/p&gt;&lt;p&gt;人工智能生成的内容，并不比你自己写的内容缺乏个性。比如我借助 Agent 撰写科研基金申请，成品会让我觉得充满生命力，能感受到其中的吸引力，相信评审人看到后会觉得 “这是一项优秀的研究，值得资助”。现实就是如此，如果你只是让 Agent 生成内容，不做任何检查就直接使用，那肯定无法达到预期效果；但如果你能快速审核内容、调整优化，发现不妥之处并进行修改，最终就能得到优质的成果，这会成为未来的常态。&lt;/p&gt;&lt;p&gt;而适应这种工作方式所需的技能，大多数人还未完全掌握，我自己也在学习中，目前仍处于探索阶段。&amp;nbsp;模型在更新，框架在迭代，我们需要不断适应、持续学习，虽然要学的东西很多，但一旦掌握，带来的回报是巨大的。&lt;/p&gt;&lt;p&gt;曾经有人认为软件工程师会因此消失，但现在大家都不再这么想了。Agent 极大地提升了生产效率，而掌握使用 Agent 的能力，正是当下最需要学习的技能。善用 Agent，能让你完成更多工作，这是核心所在。如果不懂得如何有效使用 Agent，你就会被淘汰，这将成为一项必备的核心技能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊到 Agent，蒂姆，你最近还发表了一篇精彩的博文，标题是 《要么善用 Agent，要么被时代淘汰》，其中探讨了代码 Agent 和适用于其他各类任务的 Agent。从代码 Agent 的出色表现，到 Agent 在日常生活各领域发挥实用价值，这一发展进程当下处于什么阶段？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我认为最关键的是保持务实，思考需要解决的问题，并尝试用代码实现。&lt;/p&gt;&lt;p&gt;当然，对于非程序员来说，编程本身就有很高的门槛，会觉得 “我从没写过代码，根本做不到”。但如果和 Agent 互动，它能直接帮你搭建程序，你只需要进行少量的学习 —— Agent 还会为你讲解相关知识，很快就能上手，实现程序的运行、网站的搭建等，还能快速获得反馈，现在做这些事情已经不再困难。&lt;/p&gt;&lt;p&gt;当然，我之前提到过需要检查 Agent 的输出，但如果你只是为自己搭建一些简单的工具提升工作效率，其实往往不需要这么做，Agent 生成的代码质量已经足够高。如果是在公司工作，需要将代码整合到正式的代码库中，那肯定需要进行审核；但如果只是搭建个人使用的小程序，提升自己的工作效率，那非常容易。&lt;/p&gt;&lt;p&gt;举个随机的例子，我会录制自己和 Agent 互动的视频，视频中会有我讲解的片段，也有我查看输出、思考分析的片段。我借助 Agent 搭建了一个工具，它能识别语音，记录我说话的时间戳，然后对视频进行剪辑，只保留我讲解的部分，去掉无意义的片段。这个工具我只用了 20 分钟就搭建好了，我相信所有人都能做到，因为我甚至没有检查 Agent 生成的代码，直接使用后，剪辑出的视频效果非常好。&lt;/p&gt;&lt;p&gt;只要建立起 “提出需求 — Agent 生成 — 获得反馈” 的循环，你根本不需要自己编程，只需要学会检查输出内容，或者掌握 Python 程序、bash 脚本的基本运行方法，就能实现工作的自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：那该如何选择要自动化的工作呢？该从哪些角度思考生活中的自动化需求？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我在博文中也探讨过这个问题，其实可以分为&amp;nbsp;直觉层面和精细化分析层面。&lt;/p&gt;&lt;p&gt;直觉层面很简单，就是思考哪些工作自动化后会带来便利，哪怕是一些复杂的需求，比如 “我想要一个能实现某某功能的安卓或苹果应用”，一开始你可能觉得这很难，但只要向 Agent 提出需求，它能立刻实现。你可以充分发挥想象力，打造任何自己想要的工具，那些以往没人开发、自己又迫切需要的产品，现在都能借助 Agent 实现。&lt;/p&gt;&lt;p&gt;这种思维方式能让你打造出实用的工具，提升生产效率，同时也能锻炼你使用 Agent 的能力。当然，有时尝试后可能会失败，这时你会明白 Agent 的局限性，以及自己还需要学习哪些知识才能解决问题。&lt;/p&gt;&lt;p&gt;这是直觉层面的方法，能让你快速入门，从最初的兴奋，到面对现实的冷静，再到继续尝试，最终会发现自己的生产效率在一天天提升。&lt;/p&gt;&lt;p&gt;而精细化分析层面的方法，来自我在德国自动化行业三年的工作经历，当时主要负责工厂的自动化改造，这是一种非常严谨的计算方法：先梳理自己的工作流程，为每个步骤计时，然后分析如果将某个步骤自动化，能带来多少收益、节省多少时间，再计算开发这个自动化工具需要投入多少时间，通过这种成本收益分析，快速判断哪些工作的自动化改造是有价值的。&lt;/p&gt;&lt;p&gt;我的博文中提到，邮件的自动化处理效果并不好，还有一些事情也是如此，比如创建会议日历邀请，没人喜欢做这件事，但仔细想想，人们对会议的安排有很多个性化的需求，比如某天想多安排会议，某天想把会议安排在午饭前，这些需求 Agent 无法感知。即便你向 Agent 详细说明这些需求，它生成的日历邀请也未必能符合预期，最终的效率提升其实非常有限。&lt;/p&gt;&lt;p&gt;通过这种精细化的分析，能让我们避开这些无意义的尝试，找到真正能通过自动化提升效率的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：丹，从你的角度来看，在 Agent 的应用中，哪些方法是有效的，哪些目前还不成熟但未来有望实现，又该如何管理 Agent？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我发现 Agent 的有效应用，主要有两个核心要点。&lt;/p&gt;&lt;p&gt;第一，让 Agent 发挥效用的方式，和管理团队中的初级员工、公司里的实习生非常相似。&amp;nbsp;比如，你不会对一个刚来的实习生说 “去把公司的营收提升一倍”，或许你会尝试一次，但显然不可能得到想要的结果。相反，你会给实习生安排一些简单的入门任务，让他们熟悉复杂的代码库，并告诉他们可能会遇到的问题 —— 因为你自己有过相关的经历。当你给 Agent 提供这样的背景信息，让它能接触到相关的资料，它通常就能顺利完成任务。&lt;/p&gt;&lt;p&gt;另外，对待新员工，你不会直接把生产环境的所有权限、数据库信息都交给他们，而是会给他们足够的工具，让他们能开展工作。对待 Agent 也是如此，有些人会担心 Agent 误删生产环境的所有数据，于是对其处处限制，每一步都进行监控，但如果用这种方式对待人类员工，他们根本不可能高效工作。这是一个很重要的点，当下的 Agent，至少可以把它当作实习生或初级员工来对待。&lt;/p&gt;&lt;p&gt;第二，我发现一个非常有趣的现象，尤其是从教授的教育视角，思考如何培养学生适应这个 Agent 成为工作核心的未来，那就是：一个人的专业知识越扎实，比如蒂姆在流程自动化领域的专业积累，或是我在 GPU 内核编程领域的深耕，Agent 能为其带来的能力提升就越大。&lt;/p&gt;&lt;p&gt;因为专业知识扎实的人，能在更高的抽象层面开展工作，知道工作的核心要点、方向，了解常见的问题和陷阱，知道哪些事情容易实现、哪些事情有难度，知道如何将复杂任务拆解为多个步骤。&lt;/p&gt;&lt;p&gt;之前有一段时间，大家一直在讨论 Agent 是否会取代所有软件工程师，或者取代所有初级员工，而从当下的发展来看，显然不会出现这种情况。&amp;nbsp;如果一个工具能让我的团队工作效率提升 10 倍，我不会解雇 90% 的员工，而是会让他们去完成更有价值的工作，实现 100 倍的效率提升。这是一方面。&lt;/p&gt;&lt;p&gt;另一方面，成为某个领域专家的路径，其实和以往并没有太大区别：你需要深入学习、深入理解相关知识，需要亲手实践、真正解决问题。在当下这个时代，聊天生成预训练转换器能教你很多东西，我自己就尝试过让它教我汽车的各类工作原理，虽然目前效果还一般，但不可否认，现在学习知识的难度比以往低了很多，哪怕是两三年前，都没有这么便捷的学习方式。&lt;/p&gt;&lt;p&gt;所以总结来说，对待 Agent，要像扮演管理者的角色，帮助它解决遇到的问题，不能只是把问题丢给它就撒手不管；同时，你需要不断提升自己，成为更优秀的 “管理者”，积累更多的领域知识，更深入地理解工作内容。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：也就是说，成为专家、持续学习的需求并没有改变，这一点很有意思，也很有道理。但有一个问题，如果一名年轻的内核工程师第一天入职，以往的培养方式是先安排简单的任务，第二年再安排更复杂的工作，那在 Agent 时代，这种实操性的职场培训该如何开展？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我们在合聚人工智能也一直在思考这个问题，即便在模型和 Agent 如此强大的当下，我们仍在积极招聘人才。&lt;/p&gt;&lt;p&gt;我们的做法是：首先，我以教授的身份，录制了一系列关于 GPU 工作原理的课程，要求所有新员工都必须学习；然后，我会给他们布置一个从零开始的任务，比如修改快速注意力机制的内核，实现某个新功能，具体的功能可以由他们自己选择。Agent 的优势在于，能让新员工更快地参与到高价值的工作中。&lt;/p&gt;&lt;p&gt;对于一名初级工程师来说，第一次尝试管理他人是非常有意义的经历，因为这会让他们开始用更精准的语言思考问题。比如，软件工程师常会遇到这种情况：产品经理给出一个需求，写了长长的需求文档，但当你让别人去实现这个需求时，才会发现描述一个功能需要多么精准的表达。&lt;/p&gt;&lt;p&gt;而 Agent 的出现，让这一过程得以简化，初级工程师不需要真正成为管理者，依然可以作为工程师开展工作，但能以管理者的思维方式，甚至产品经理的视角来思考问题。因为和 Agent 沟通时，你必须精准地描述自己的需求。我发现，团队中那些刚从大学或硕士毕业的年轻员工，只要积极学习和使用人工智能 Agent，他们的沟通能力会比以往的工程师强很多，对知识的理解和掌握速度也会大幅提升，并且能以以往 5 到 10 年都难以想象的速度搭建工具、完成工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我从教育的角度补充一点，这一点其实和丹的观点形成了一定的对比，也很有意思。我一直强调 “要么善用 Agent，要么被时代淘汰”，这一点对学生也同样适用，但正如丹所说，使用 Agent 的前提是具备一定的领域知识。&lt;/p&gt;&lt;p&gt;我们发现，如果允许学生使用 Agent，他们的学习效率会非常高，但有时他们借助 Agent 完成的解决方案，表面上看起来没问题，实际上却漏洞百出，而学生自己却意识不到。&lt;/p&gt;&lt;p&gt;当下我们正面临一个困境：很难同时培养学生的领域知识和 Agent 使用能力，这两者的平衡很难把握。&amp;nbsp;我们既不想培养出对知识一知半解的学生，也希望学生能掌握 Agent 的使用方法，否则他们进入职场后将无法胜任工作。&lt;/p&gt;&lt;p&gt;丹提到，具备扎实知识基础的人，借助 Agent 能实现能力的飞跃，但对于刚开始学习计算机科学的学生来说，该让他们学习多少专业知识，又该让他们在多大程度上借助 Agent 完成工作，这是一个非常棘手的问题，目前还没有完美的解决方案。&lt;/p&gt;&lt;p&gt;如果让学生过度依赖 Agent，他们的基础知识点掌握会非常薄弱；如果让学生完全靠自己完成所有学习任务，不使用 Agent，他们又无法掌握这项核心技能，进入职场后缺乏竞争力。&lt;/p&gt;&lt;p&gt;或许一个解决方案是：先让学生扎实掌握基础知识，再学习使用 Agent。但学生并不会这样做，他们能轻易接触到这些人工智能工具，并且会因为其便捷性而频繁使用。&lt;/p&gt;&lt;p&gt;所以或许真正的解决之道，是培养学生一种全新的信息处理和知识学习的思维方式，这种能力甚至超越了批判性思维 —— 学生需要学会识别自己不知道的未知事物，也就是那些自己没有考虑到、不理解，甚至从未想过的问题。&amp;nbsp;只有具备这种能力，才能跟上 Agent 的发展步伐。因为在未来，我们很可能会面对自己无法理解的问题，而 Agent 却能理解，我们需要找到一种方式，跟上 Agent 的节奏，这无疑是一大挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;小模型是未来趋势&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：二位对 2026 年人工智能的发展有哪些具体的期待？认为哪些趋势会成为现实，哪些则不会？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;蒂姆・德特默斯：我觉得自己的看法比较矛盾，一方面，我认为很多领域的发展会趋于平淡，不会有太多创新；另一方面，又会有一些意想不到的突破出现。而在前沿模型领域，我认为不会有太多惊喜。&lt;/p&gt;&lt;p&gt;当下一个公开的事实是，预训练数据已经耗尽，正如丹所说，我们可以通过合成数据来弥补这一缺口，代码 Agent 的训练，就是在各类环境中生成大量合成数据，并进行数据融合，我们在这方面会取得一些进展，但整体来看，机器学习领域的发展已经显现出疲态。&lt;/p&gt;&lt;p&gt;我认为代码 Agent 的性能不会有太大提升，主要的进步会体现在用户体验的优化上。&amp;nbsp;当下各款模型的性能已经趋于同质化，比如我使用&amp;nbsp;GLM-4.7&amp;nbsp;的配置时，一度以为自己用的是 Opus 4.5，后来才发现是不同的模型，因为它们的表现实在太相似了。&lt;/p&gt;&lt;p&gt;所以&amp;nbsp;前沿模型的性能发展会陷入停滞，而小模型领域则会迎来快速发展。&amp;nbsp;如果针对特定的专业数据训练小模型，其性能会非常出色，而且小模型的部署难度低，能力却不容小觑。&lt;/p&gt;&lt;p&gt;比如 1000 亿参数的模型，能轻松实现部署，即便是 RTX 6000 这类售价 6000 美元的入门级数据中心 GPU，也能胜任。我认为对于很多企业来说，这会是一个极具吸引力的选择，它们不再需要依赖前沿的大模型，定制化的小模型甚至能表现出更优的性能，因为其针对特定领域做了优化。&lt;/p&gt;&lt;p&gt;当下存在一个很大的问题，正如 Anthropic 首席执行官所指出的，市面上有很多性能强大的开源模型，但实际使用的人却很少，原因就在于&amp;nbsp;部署难度极高。一旦模型的部署需要超过 8 块 GPU，不仅需要用户进行大量的效率优化，还涉及复杂的系统工程问题，而目前还没有能实现这一功能的开源系统，需要实现推理任务的解耦、跨序列长度的拆分等技术。或许我们能为异构 GPU 设备、小模型打造这样的部署系统，届时 1000 亿参数模型的运行效率，将能媲美当下的前沿大模型。&lt;/p&gt;&lt;p&gt;小模型兼具效率和灵活性的优势，再加上能通过大模型的知识蒸馏实现性能提升，这些因素结合起来，将彻底改变人工智能的发展格局。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我也对小模型的发展充满期待，认为它们会释放出更多的能力。&lt;/p&gt;&lt;p&gt;我会密切关注开源模型的发展，GLM-4.7&amp;nbsp;的出现，已经让开源模型的性能开始媲美当下最优秀的前沿模型，我认为 2026 年开源模型的能力会实现又一次大的飞跃。&lt;/p&gt;&lt;p&gt;我也非常期待新硬件的推出，目前已经有一些关于英伟达下一代 NVIDIA Rubin GPU、AMD 400 系列显卡的消息，即便我们还未充分挖掘当下硬件的潜力，我也很想看看下一代硬件能带来怎样的性能突破。&lt;/p&gt;&lt;p&gt;此外，我还期待多模态领域的发展，去年视频生成模型迎来了发展的小高峰，比如 Sora 2、Gemini、Veo 等模型都表现出色，我很想看看它们后续的发展。&lt;/p&gt;&lt;p&gt;最后，我也期待能看到，在笔记本电脑、手机这类终端设备上，人工智能的智能水平能达到怎样的高度，&amp;nbsp;能被推进到什么程度。我想说，当下投身人工智能领域，恰逢最激动人心的时刻。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：二位早些时候提到了状态空间架构（SSM），你们认为这会是人工智能的近期发展方向吗？也就是说，我们会逐渐走出 Transformer 架构的时代，向状态空间模型、世界模型等新架构发展吗？这是否是你认为值得期待且势在必行的发展趋势？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;丹・傅：我认为在很多领域，新架构已经落地应用了。比如当下全球最优秀的一些音频模型，就部分基于状态空间模型打造。英伟达最近也发布了多款优秀的混合架构模型，比如神经变形金刚，就是其中的代表。&lt;/p&gt;&lt;p&gt;所以相关的研究已经取得了很多不错的成果，架构的进化还会继续。比如 DeepSeek 的模型压缩技术，就借鉴了状态空间模型的一些理念；MiniMax 的一款模型，则采用了线性注意力的思路。&lt;/p&gt;&lt;p&gt;所以未来人工智能的架构会变得更加多元，这一趋势已经显现。&lt;/p&gt;&lt;p&gt;而中国的实验室在这方面会有更多的探索和突破，因为中国并没有像开放人工智能那样，集产品、模型、营收于一体的巨头企业，也就没有统一的技术发展范式。所以中国的实验室会更敢于尝试，想要让自己的开源模型脱颖而出，架构创新就是一个重要的方向，当然，纯性能的提升也是一个途径。因此，未来人工智能的架构会迎来爆发式的创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=XCCkgRzth6Q、&lt;/p&gt;</description><link>https://www.infoq.cn/article/69sjXelB4jbxSoqOIosH</link><guid isPermaLink="false">https://www.infoq.cn/article/69sjXelB4jbxSoqOIosH</guid><pubDate>Fri, 23 Jan 2026 12:03:02 GMT</pubDate><author>高允毅,木子</author><category>生成式 AI</category></item><item><title>亚太 CDN 大会直击：火山引擎提出应用加速的演进，AI 大模型成关键驱动力</title><description>&lt;p&gt;2026 年 1 月 20 日，第十五届亚太 CDN 产业大会暨年度颁奖盛典在北京隆重举行。作为 CDN 领域极具影响力的行业盛会，大会汇聚产、学、研、用领域领袖与专家，聚焦数智新时代下内容分发网络的技术创新与产业变革。火山引擎视频云边缘产品线高级解决方案总监许思安受邀出席，发表《AI 时代下应用加速的演进》主题演讲，深度解析火山引擎边缘云核心能力、AI 大模型融合场景及 CDN 未来演进形态，凭借扎实的技术沉淀与前瞻视野引发全场关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0c/0c223724063bd48745a50ecc34964eb6.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“抖音同款”到生态赋能，火山引擎边缘云的技术进阶之路&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;演讲开篇，许思安详细介绍了火山引擎的发展历程与平台定位。作为字节跳动旗下云原生 AI 服务平台，火山引擎早期以“抖音同款内容云技术”为核心标签，2025 年起全面升级为面向更广泛机构的技术服务提供商，这一转变既是市场需求的必然回应，也是平台能力的全面进阶。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈及核心竞争力，许思安强调，火山引擎 CDN 商业化虽始于 2021 年，但依托字节跳动原生技术底座，构建了自主研发的边缘云平台，融合预估算理与边缘网络，实现“让云计算数据无处不在”的核心目标。目前，平台已形成涵盖 RTC、CDN SaaS、IGA 等产品的丰富矩阵：RTC 针对国内外不同场景优化技术方案，底层资源统一适配；CDN SaaS 实现多厂商能力抽象整合，达成管控配置与质量监控一体化；IGA则从传统分发向全链路加速延伸，提供 7 层全栈加速、3-4 层加速及跨境加速等多元化解决方案，精准覆盖非缓存类加速需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三大场景：AI 大模型深度融合，解锁加速服务新价值&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 AI 技术爆发的背景下，火山引擎积极探索边缘云与大模型的融合路径，许思安重点分享了三大核心业务场景：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;联合加速方案：传输效率与访问稳定双提升&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;火山引擎联合豆包大模型打造全栈加速解决方案，具备多重核心优势：兼容 SSE、SaaS 等 AI 常用协议，适配多样化业务需求；通过智能选路、精准缓存等技术优化网络传输效率；集成跨境专线加速与 Web 请求分析能力，在边缘层高效处理并发请求，既保护原点安全，又提升访问稳定性。实测数据显示，该方案可使丢包率降低 5%-10%，延时缩短 10%-30%，目前已在火山引擎官网 RTC 产品矩阵中正式上线。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;veFaaS 服务：Agent适配与安全防护双强化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对在火山引擎veFaaS 服务上部署 agent 的客户，平台通过玩机产品适配提供 GS SDK，优化智能购物等业务逻辑。同时，借助 ACP 请求经内网访问火山引擎 refuse 服务，既有效抵御公网攻击，为源站单向服务构建安全屏障，又显著提升访问效率，降低网络延时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;AI 应用开发部署平台：轻量化设计与开发者赋能双推进&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;聚焦 AI 应用落地痛点，火山引擎打造一键式开发部署平台，整合自身加速、安全防护与观测能力。平台支持模板创建、导入及本地上传等多种开发模式，集成 AI 插件生态，可一键部署代码并调用火山方舟、千川等大模型，大幅降低开发者工作量。目前已覆盖家居、安防等多个场景，为行业 AI 应用落地提供高效支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;三阶演进：AI 时代 CDN 加速网的未来形态&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈及 CDN 行业的发展趋势，许思安提出“优化 - 变化 - 变革”三阶演进模型，描绘 AI 时代加速网络的未来蓝图：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;优化阶段：AI 驱动全链路效率升级&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过 AI 技术实现四大优化：智能调度基于用户行为与网络状态预判热点，提升缓存命中率；传输优化动态调整视频码率等策略，替代传统固化方案；智能运维构建全局决策系统，实现异常识别与故障自愈，提升容灾切换效率；安全防护从被动防御转向主动感知，形成快速响应机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;变化阶段：从分发节点到边缘计算单元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硬件层方面，CDN 节点将升级为集计算、存储、网络安全于一体的边缘计算单元，优化 CPU、GPU 等算力配置；软件层从中心化分布向边缘协同分布式平台演进，部署容器引擎并优化节点间通讯资源；场景层面，承载内容从互联网内容拓展至 AIGC 生成数据、车联网数据等全行业低时延数据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;变革阶段：语义缓存 + 边缘推理的深度融合&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;许思安强调，CDN 的核心突破将是从基于内容哈希的静态缓存，升级为基于语义理解的智能缓存。这一变革将在多场景落地：AIGC 头像生成场景缓存热门提示词接口，大模型聊天机器人场景缓存常见问题响应，AI 推理 API 场景精准分配请求至边缘单元，IOT设备场景剔除无效数据、聚合同类数据。未来，语义缓存与边缘推理的深度结合，将形成 &quot;场景化精准处理&quot; 的新型架构，大幅降低 AI 请求响应时间与后端算力成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;双奖加持：行业认可火山引擎技术实力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本次大会颁奖环节，火山引擎凭借在 AI 基础设施领域的卓越技术创新、完善解决方案及行业影响力，以及在 CDN 领域的深耕细作与突出服务表现，一举斩获“AI 基础设施标杆奖”与“CDN 行业先锋奖”两项重磅荣誉，充分彰显行业对其技术实力与市场价值的高度认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/1b/1b3311b521b9388571152d0652946f14.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/4c/4cdf13de0f8d9a8c6cedc60d4e245622.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来，火山引擎将持续深耕 AI、应用加速、CDN 等核心领域，以技术迭代与产品创新为驱动力，不断探索加速服务与行业场景的深度融合，为千行百业数字化转型提供更高效、更安全、更智能的技术支撑，助力数字经济高质量发展。&lt;/p&gt;</description><link>https://www.infoq.cn/article/Cd5OmDZ7fa2S0bwSL8oX</link><guid isPermaLink="false">https://www.infoq.cn/article/Cd5OmDZ7fa2S0bwSL8oX</guid><pubDate>Fri, 23 Jan 2026 08:33:31 GMT</pubDate><author>火山引擎</author><category>云计算</category><category>AI&amp;大模型</category></item><item><title>IDC：人形机器人在六大场景大规模商用，智元登顶五大场景</title><description>&lt;p&gt;在 IDC 最新发布的《全球人形机器人市场分析》报告中，一个关键信号被反复提及：人形机器人开始进入可复制、可交付的规模化商用阶段。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这份报告中，IDC选择用“出货量”而非“项目数”、“合作数”作为核心衡量指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;报告中还提到，人形机器人正在从单一硬件销售，向&amp;nbsp;“硬件 + 平台 + 服务”&amp;nbsp;组合模式演进，其中包括&amp;nbsp;RaaS（Robot-as-a-Service）&amp;nbsp;等形式 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中的数据显示，2025年全球人形机器人出货量约为1.8 万台，同比增长约 508%，销售额约4.4亿美元（约合人民币30.6亿元），其中中国厂商占据主导位置。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;还将当前人形机器人的主要落地需求，归纳为六大类场景：&lt;/p&gt;&lt;p&gt;文娱商演科研教育数据采集导览导购工业制造仓储物流&lt;/p&gt;&lt;p&gt;这些场景有一个共同点：强调可控任务、明确边界和可持续交付。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从 IDC 的统计口径来看，当前需求并未集中在单一行业，而是分散在上述六大场景中。这种分散本身，反而说明一个问题：市场或者并不是在等待“完美的人形机器人”，而是在寻找“现在就能用的那一部分能力”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，有一家公司成立仅三年，就已经在六大应用场景中的五类，都实现了出货量第一：这家公司就是智元机器人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ad/ad8516321932501bf0fe9562695483be.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，智元以5200台的出货量夺得全球榜首，还拿下了“全尺寸细分领域出货量第一”的桂冠。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/17/1726a8f2fb503bae83e9cc0e225931f8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;IDC 在报告中特别区分了不同形态的人形机器人，其中全尺寸人形机器人在2025年贡献了41.6%的市场收入份额，成为最主要的收入来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所谓全尺寸机器人，并非外形更像人，而是按照成年人的身体尺度与关节结构设计，对人类的空间（如展陈、导览、科研实验室）适配度高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，与其说全尺寸人形机器人更“先进”，不如说是现实条件更早将其推向商用——高成本和高部署门槛，使其难以长期停留在实验或演示阶段，只能优先进入需求明确、具备支付能力的场景，并在真实使用中完成迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;值得一提的是，智元凭借软硬件全栈技术能力、快速的市场拓展、完善的生态建设以及多元化的商业模式，实现了1300台出货量，亦位居全球市场行业第一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在IDC的这份报告中还提到，人形机器人正在从单一硬件销售，向&amp;nbsp;“硬件 + 平台 + 服务”&amp;nbsp;组合模式演进，其中包括&amp;nbsp;RaaS（Robot-as-a-Service）&amp;nbsp;等形式 。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这背后也算是现实原因驱动：比如短期活动、科研采集、阶段性项目中，租用能力比拥有设备更重要。这类模式的出现，也在一定程度上降低了人形机器人进入真实场景的门槛，加速了早期需求的释放。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而全球首个机器人租赁平台“擎天租”，也是来自于智元。智元表示平台上线3周，注册用户数已突破20万，日均租赁订单稳定在200单以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：https://my.idc.com/getdoc.jsp?containerId=CHC54064426&amp;amp;pageType=PRINTFRIENDLY&lt;/p&gt;</description><link>https://www.infoq.cn/article/xJZepWGP6dmoQ8AJpNbF</link><guid isPermaLink="false">https://www.infoq.cn/article/xJZepWGP6dmoQ8AJpNbF</guid><pubDate>Fri, 23 Jan 2026 07:09:18 GMT</pubDate><author>木子</author><category>具身智能</category></item><item><title>AI 进化论丨第八期：新瓶旧酒还是涅槃重生？操作系统的 AI 进化终将走向何方？</title><description>&lt;h2&gt;嘉宾介绍&lt;/h2&gt;
&lt;p&gt;杨勇  阿里云操作系统团队资深总监，龙蜥社区技术委员会主席&lt;br&gt;
郭松柳  中国科学院软件研究所高级工程师、RISC-V行业生态负责人&lt;br&gt;
凌敏  InfoQ极客传媒策划编辑&lt;/p&gt;
&lt;h2&gt;视频简介&lt;/h2&gt;
&lt;p&gt;AI 浪潮引发基础设施革命，服务器操作系统也正在迈入“(Cloud+OS)xAI”多维赋能的全新阶段。从国内外主流 OS 的差异化演进、阿里云 Alibaba Cloud Linux 4 的内核突破与性能跃升，到 “GPU 时代”的内核争议；从 OS-Copilot 的升级赋能，到 RISC-V 异构算力适配的前沿探索，操作系统的 “涅槃重生” 需要跨越哪些技术鸿沟？&lt;br&gt;
本期直播聚焦从业者核心困惑，结合阿里云 AI 增强套件、百万级服务器运维实践与 RISC-V 适配经验，深度拆解  AI 时代 OS 的技术重构与价值重塑！&lt;/p&gt;
&lt;h2&gt;视频亮点&lt;/h2&gt;
&lt;p&gt;● 趋势解读：剖析国内外主流操作系统 AI 演进路径与核心竞争力，对比技术路线差异&lt;br&gt;
● 技术重构：拆解 AI 驱动下 OS 全栈优化关键突破，结合 RISC-V 异构算力适配实践&lt;br&gt;
● 内核思辨：直击 GPU 时代 “内核价值 vs 模型层 bypass” 核心争议，厘清技术演进本质&lt;br&gt;
● 生态前瞻：探索未来 OS 支撑 Agent 生态演进的技术底座，解锁 “AI for System” 落地路径&lt;/p&gt;
</description><link>https://www.infoq.cn/article/0a2A4lh12FJJMYgS7jqx</link><guid isPermaLink="false">https://www.infoq.cn/article/0a2A4lh12FJJMYgS7jqx</guid><pubDate>Fri, 23 Jan 2026 06:42:18 GMT</pubDate><author>郭佳浠</author><category>阿里巴巴</category><category>操作系统</category><category>云计算</category></item><item><title>欧洲数据主权倡议发布了一个信任框架</title><description>&lt;p&gt;在多瑙河发布的Gaia-X信任框架提供了自动化合规机制，并支持跨部门和跨区域的互操作性，以确保可信的数据交易和服务交互。2025年举办的&lt;a href=&quot;https://gaia-x.eu/event/gaia-x-summit-2025/&quot;&gt;Gaia-X&lt;/a&gt;&quot;峰会促进了关于人工智能和数据主权的讨论，并提出了支持整个欧洲和其他地区创新的数据空间解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gaia-X是一项欧洲倡议，将国际工业界、学术界和政界聚集在一起，通过规范、合规标签和开源软件组件为数据生态系统和底层云基础设施开发了一个联邦数据和云基础设施。它基于欧洲的价值观，如数据主权、安全和透明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gaia-X的首席执行官Ulrich Ahle提到，数字主权是通过信任、开放和共同标准创建的。Gaia-X正在从试点实施转向业务部署；数据空间、信任框架和互操作性工具都已准备好了扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Gaia-X的首席技术官Christoph Strnadl和首席运营官Roland Fadrany介绍了Gaia-X信任框架3.0——“多瑙河”版本。问题往往存在于不同的生态系统和区域之间。Gaia-X信任框架允许应用程序和系统在具有不同参与者的联邦环境中顺利运行。参与者可以使用不同的方法和标准从可信服务提供商那里获得他们的数字身份和其他数字证书。Strnadl解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;例如多云环境、云到边缘连续体、数据空间和数据空间的联合体，以及提供数字产品通行证所需的任何其他形式的数字生态系统，如全球供应网络。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了支持可扩展的联合数字生态系统，多瑙河版本提供了自动化合规和跨行业和地理区域的互操作性机制，Strnadl解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;你可以以一种可扩展的方式自动化治理规则和合规框架，在保持互操作性的同时允许域和区域适应。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Fadrany提到，在Gaia-X中，这被称为“自带规则（bring your own rules，BYOR）”。组织可以在不牺牲技术互操作性的情况下添加自己的规则、合规框架或行业特定要求作为扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Roland Fadrany提到，Gaia-X已经进入了执行阶段，从原则转向实践。它旨在为政府和企业提供操作手段，以建立、治理和增长数字生态系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;航空航天和核能两个行业，正在使用Gaia-X信任框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.decade-x.org/&quot;&gt;DECADE-X&lt;/a&gt;&quot;是航空航天和国防工业的数字生态系统。空客的Jérémy Mambrini表示，其使命是构建一个全球协作的数据共享框架，汇集行业利益相关者，实现可信、安全和基于标准的数据分析交换。DECADE-X包括一个基于Gaia-X的信任框架，并扩展了针对生态系统特定规则和全球地理采用的扩展。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.google.com/url?q=https://www.edf.fr/en/the-edf-group/dedicated-sections/journalists/all-press-releases/data4nuclearx-a-sovereign-and-secure-digital-dataspace-for-the-nuclear-sector&amp;amp;sa=D&amp;amp;source=docs&amp;amp;ust=1766176128032592&amp;amp;usg=AOvVaw1H8yiUgoMgook-ynValFKU&quot;&gt;Data4NuclearX&lt;/a&gt;&quot;项目旨在基于Gaia-X为核工业构建一个安全和主权的数据交换空间。EDF的Martine Gouriet提到，确保数据的主权是主要挑战之一。他们的方法重点是关注信任、安全、监管和合规。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ就Gaia-X采访了&lt;a href=&quot;https://www.linkedin.com/in/christophstrnadl/&quot;&gt;Christoph Strnadl&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：软件行业为什么应该关心Gaia-X？它能带来什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Christoph Strnadl：对于软件公司来说，实施Gaia-X信任框架意味着进入新市场，更容易与客户和合作伙伴集成，并能够参与大型跨行业数据生态系统，而不被绑定到单一的平台或供应商。&amp;nbsp;Gaia-X降低了与信任相关的集成成本，并增加了潜在客户群。通过遵循Gaia-X合规规则，开发者可以构建符合规范、可移植且与新兴欧洲数据经济兼容的服务。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：开发者和架构师需要了解Gaia-X的哪些信息？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Strnadl：Gaia-X不是一个新的云平台，也不是另一个服务编排层或云间API。Gaia-X提供了框架和组件，用于实现一种可信的方法，以建立参与数据共享或服务交互的组织和人类参与者的身份。它确保服务和其他生态系统实体（例如，物联网设备、AI训练数据集）符合生态系统规则。&amp;nbsp;Gaia-X信任框架由架构、一套规范和标准以及实现它们的软件组件组成。遵循这些指南的软件密集型系统将在信任层上通过设计实现互操作性。关键概念包括：可验证的凭证、由适当的合格评定机构（CABs）即“清算所”验证服务的自我描述，以及身份和策略执行使用凭证。&amp;nbsp;对于架构师来说，Gaia-X定义了如何将可信的数字标识符链接到生态系统参与者，如何描述XaaS服务，如何通过独立方（CABs）验证这些描述的合规性，以及参与者如何在联合环境中进行身份验证和互动。&amp;nbsp;对于开发者来说，它提供了可以嵌入到应用程序中的具体技术组件，从注册表到凭证格式，包括像did-resolver或VC-JWT游乐场这样的加密支持函数。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;InfoQ：Gaia-X为软件项目提供什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Strnadl：Gaia-X提供了技术规范，其中包括数字标识符、服务描述以及合规性定义和验证的模型和标准。还有&lt;a href=&quot;https://gitlab.com/gaia-x/lab&quot;&gt;Gaia-X Lab&lt;/a&gt;&quot;，开发者可以在那里测试实现并看到工作组件的示例。&amp;nbsp;软件团队可以使用这些规范和工具来构建可识别、可认证的应用程序和服务，为参与数字生态系统和数据空间做好准备。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有其他关于数据主权、云和数据共享的倡议。国际数据空间协会（&lt;a href=&quot;https://internationaldataspaces.org/&quot;&gt;International Data Spaces Association&lt;/a&gt;&quot;，IDSA）是一个非营利协会，它为数据空间中的数据共享创建标准，允许参与者完全控制他们的数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://eclipse-tractusx.github.io/&quot;&gt;Eclipse Tractus-X™&lt;/a&gt;&quot; 是一个协作的开源项目，旨在推动各行各业的数字化转型；其使命是使用开放标准实现安全、自主和高效的数据交换。Tractus-X提供了实现可信数据交易的软件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;EOSC协会实施欧洲开放科学云（&lt;a href=&quot;https://digital-strategy.ec.europa.eu/en/policies/open-science-cloud&quot;&gt;European Open Science Cloud，EOSC&lt;/a&gt;&quot;），以支持研究数据管理和应用，确保科学家能够访问数据驱动的科学，创造新知识，促进创新，并加强公众对科学的信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/data-sovereignty-trust-framework/&quot;&gt;https://www.infoq.com/news/2026/01/data-sovereignty-trust-framework/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/KdUzYQcqMUOSh4Xmx7AI</link><guid isPermaLink="false">https://www.infoq.cn/article/KdUzYQcqMUOSh4Xmx7AI</guid><pubDate>Fri, 23 Jan 2026 06:17:00 GMT</pubDate><author>Ben Linders</author><category>框架</category><category>大数据</category></item><item><title>8B端侧写作智能体AgentCPM-Report开源，DeepResearch 终于本地化</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;1月20日，由清华大学自然语言处理实验室、中国人民大学、面壁智能与OpenBMB开源社区联合研发的8B端侧写作智能体AgentCPM-Report正式开源。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在当前深度研究场景中，企业与科研人员常面临两难抉择：依赖云端大模型虽能获得顶级调研能力，却需承担核心数据泄密风险；选择断网或本地小模型保障安全，又往往因性能局限导致报告逻辑浅薄、实用性不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为此，AgentCPM-Report以端侧模型为核心，来实现本地化部署与SOTA性能的双重突破，力求无需昂贵算力集群，也无需上传任何信息，即可在本地构建专家级调研助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据悉，该智能体的核心亮点集中在两大维度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第一，极致效能与“以小博大”的突破：通过平均40轮深度检索与近100轮思维链推演，AgentCPM-Report以仅8B的参数规模，实现了对复杂信息的全方位挖掘与重组，能够产出逻辑严密、洞察深刻的万字长文，在深度调研任务上性能对标顶级闭源系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;第二，物理隔绝的本地安全保障：专为高隐私场景设计，支持完全离线的敏捷部署，彻底杜绝云端泄密风险；依托开源的UltraRAG框架，可高效挂载并理解本地私有知识库，让核心机密数据在&quot;不出域&quot;的前提下，转化为高价值的专业决策报告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在DeepResearch Bench、Deep Consult、DeepResearch Gym三大主流深度调研评测基准中，其综合评分达到甚至超越顶级闭源系统：在最考验核心能力的洞察性指标上排名第一，全面性指标位居第一梯队，仅次于基于Claude的复杂写作框架。其中在DeepResearch Gym评测中，AgentCPM-Report以98.48的综合得分领跑，在深度、广度、洞察力等关键维度均斩获满分。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d7/d749e50a38b9eb2573a9dffdc8e29541.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;官方展示的实战场景中，该智能体可基于《三体》原文知识库，完成从线索挖掘、大纲规划到万字长文撰写的全流程，精准生成&quot;面壁计划&quot;深度调查报告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部署便捷性方面，AgentCPM-Report支持Docker一键启动，无需编写代码即可通过拖拽方式将PDF、TXT等本地文档导入后台，系统自动完成切片与向量化索引，用户输入研究课题后，即可生成结构化、带引用的专业报告，实现沉浸式深度调研体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;技术层面，两大创新支撑其“以弱胜强”的表现：一是“写作即推理”模式，通过“起草-深化”两阶段循环与渐进式优化，将长篇写作拆解为微小目标，避免小模型逻辑崩塌；二是“多阶段智能体学习”，拆解智能检索、流畅写作、科学规划、精准决策四大核心能力，通过有监督微调、原子能力强化、全流程优化三阶段训练，实现端到端全链路能力提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，AgentCPM-Report已在GitHub、HuggingFace、ModelScope、GitCode、魔乐社区等多个平台开源，UltraRAG框架也同步开放获取。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;UltralRAG 框架开源地址：&lt;a href=&quot;https://github.com/OpenBMB/UltraRAG&quot;&gt;https://github.com/OpenBMB/UltraRAG&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;GitHub：https://github.com/OpenBMB/AgentCPM&lt;/p&gt;&lt;p&gt;HuggingFace：&lt;a href=&quot;https://huggingface.co/openbmb/AgentCPM-Report&quot;&gt;https://huggingface.co/openbmb/AgentCPM-Report&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;ModelScope：&lt;a href=&quot;https://modelscope.cn/models/OpenBMB/AgentCPM-&quot;&gt;https://modelscope.cn/models/OpenBMB/AgentCPM-&lt;/a&gt;&quot;Report&lt;/p&gt;&lt;p&gt;GitCode：&lt;a href=&quot;https://gitcode.com/OpenBMB/AgentCPM&quot;&gt;https://gitcode.com/OpenBMB/AgentCPM&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;魔乐社区：&lt;a href=&quot;https://modelers.cn/models/OpenBMB/AgentCPM-Report&quot;&gt;https://modelers.cn/models/OpenBMB/AgentCPM-Report&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/m3AbwhgYsmXQua8Fu2XG</link><guid isPermaLink="false">https://www.infoq.cn/article/m3AbwhgYsmXQua8Fu2XG</guid><pubDate>Fri, 23 Jan 2026 06:01:01 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>如何建设人人都能训的大模型技术氛围</title><description>&lt;p&gt;在大模型算法快速迭代演进的背景下，业务研发人员负责工程、算法研究人员负责模型优化的协作模式，已经无法满足大模型产品快速创新、模型效果快速迭代的业务需求，业务团队需要建设自有的大模型优化能力。如何建设一个人人都能训大模型的技术氛围，已成为加速大模型业务落地、推动组织创新与发展的关键。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025 年 4 月，在 InfoQ 举办的 QCon 全球软件开发大会（北京站） 上，科大讯飞消费者 BG 大数据研发部总监吕昕分享了“&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6422&quot;&gt;如何建设人人都能训的大模型技术氛围&lt;/a&gt;&quot;”，他从平台基础设施、大模型思维、协作文化 3 个角度，阐述如何建设“人人能用、人人会训”的大模型文化，有效提升组织效能，进而推动业务的持续成长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;预告：&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;2026 年 QCon 全球软件开发大会（北京站）&lt;/a&gt;&quot;策划了「&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/track/1920&quot;&gt;AI 时代的“超级团队”&lt;/a&gt;&quot;」专题，将探讨如何弥补人与 AI 的能力鸿沟，重构产品与技术的协作关系，并建立一套适应 AI 时代的全新管理与度量体系，打造高适应性、高产出的“超级团队”。如果你也有相关方向案例想要分享，欢迎&lt;a href=&quot;https://jinshuju.com/f/Cu32l5&quot;&gt;提交&lt;/a&gt;&quot;。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大模型时代组织创新的必要性&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型时代创新的必要性在于，无论是 C 端还是 B 端业务，直接使用大模型完成工作都存在困难，需要进行优化。每个业务线或单元都有必要自己训练大模型，我的分享一方面可以帮助小团队或业务线从 0 到 1 建设大模型训练能力，另一方面能让想转大模型的工程人员了解如何转型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型算法优化的几种模式&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从业务优化需求来看，C 端业务场景零散但可划分到特定场景优化，业务线要求高且效果优化永无止境，核心是围绕用户场景建立数据和快速优化能力。B 端业务以解决方案为主，对效果要求相对有限，主要是满足国产化和安全要求，达到可用即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型优化模式与传统机器学习有所不同。传统机器学习中，算法需求由算法研究人员或团队主导，业务线研发主要负责部署上线和维护。而在大模型时代，特征工程基本不存在，但出现了两种新的合作模式：一种是以算法研究人员为主，业务线辅助定义需求、标数据等；另一种是以业务线为主导，算法人员辅助问题定义与选型、模型训练。DeepSeek 等技术的出现，使得业务线或产品线有可能自己优化大模型训练效果，不再依赖算法辅助。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/b5/b5ec5149295fbe3aca6e3421ad886641.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型吋代的 BLM 模型&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从组织架构角度，各个业务线更希望业务线自己训练大模型。因为大模型技术发展迅速，战略需灵活调整，组织活力需进一步激活，以实现敏捷创新和更好的信息拉齐与穿透。传统的算法团队与工程团队分开的模式已不能满足业务发展需要，每个业务线或团队都需要具备从 0 到 1、端到端优化大模型的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/6a/6a5dcdac3c600678ec239a481e249219.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型时代，DeepSeek 的出现既带来了危机也带来了机遇。它在基础模型方面表现出色，一些场景直接使用深度探索就能取得不错的效果。同时，开源生态的成熟，包括训练框架、推理框架和智能代理框架，降低了训练基础设施的建设成本。通过蒸馏深度探索，可以快速构建高质量数据，如思维链数据，节省了大量人工标注成本。此外，模型优化范式也在变革，从之前的底座模型训练和监督微调（SFT），转变为现在的知识蒸馏，并且广泛采用 GRPO 来优化效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从 0 到 1 自建大模型优化能力面临的问题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业务线如果想自己从 0 到 1 建设大模型的优化能力，会面临诸多挑战。首先是基础设施的缺失，包括算法、算力、平台、数据，以及训练框架和推理框架。其次是缺乏算法优化经验，不清楚如何选择模型、技术方案，如何评估和优化效果。最后是人才短缺，不清楚需要什么样的人才、到哪里找以及需要掌握哪些技术栈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型效果优化团队的协作与流程&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在大模型时代，对研发岗位的要求也发生了变化。核心岗位包括大模型算法工程师和大模型测试工程师。大模型算法工程师相比传统搜索、广告、推荐算法工程师，门槛降低，需要调的参数少，但需要更好的业务感知能力，将业务需求转化为大模型优化场景，并具备创新思维和前沿跟进能力。大模型测试工程师相比传统测试工程师，需要更高的自动化测试要求，能够基于业务感知能力自动化构建大模型测试样本和制定测试标准。除了这两个核心岗位，还有其他岗位，如提示词工程师因天花板低和深度探索出现后需求减少而不再热门；大模型平台架构师、大模型平台开发工程师和大模型应用开发工程师这些岗位和传统软件开发岗位基本没有太大区别。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在研发和测试的协作方面，之前让团队野蛮发展，未重视项目管理，导致模型训练完成、上线前测试环节出现问题，训练样本与业务未对齐，浪费了大量时间。因此，我增加了样本评估环节，要求在训练前与业务线对齐样本，确保样本能满足业务需求。同时要求每次算法上线时提供详尽的自测报告和提示词文档，明确参数设置等细节，以避免因参数错误导致的测试问题，因为大模型训练结果是黑盒，测试时不易发现问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/47/47d7a6b0d23b2ab896ed3433921154ed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;建设人人能训大模型的基础设施&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型优化平台的建设&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于我对整个平台架构设计的理解，基本分为三层。最底层是基础设施，公有云可以解决 90%，甚至 100% 的问题。因为业务线的训练样本数和情况一般不支持训练 32B 以上的模型，32B 的全参训练是上限。此时租用几十张显卡基本能解决大部分训练问题，大部分业务场景 7B 模型也能搞定。所以公有云租卡基本能解决 90% 的训练和部署问题。在训练的第二层是训练工具。这里使用了公司内部已有的星火训练平台，同时也基于开源搭建了相关工具，开源生态的成熟对此帮助很大。再往上是大模型应用开发的三个工程：数据工程、模型工程和 Agent 工程，也可称为大模型的应用开发。核心需要自己扩建设的资源主要是数据资源和应用开发资源。数据资源方面，要掌握如何通过调用 API 构建样本，如何蒸馏 Deepseek，公有云的 API 基本能满足需求。应用开发方面，主要涉及 Agent 和 RAG。Agent 的开源项目众多，star 超过 1000 的都有 50 个左右，可以基于开源搭建自己的 Agent 和 RAG 平台。如果想低成本建设从 0 到 1 的基础设施，利用公司内部资源复用和拥抱开源，基本能解决所有问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/f2/f2c1705f860b1c6e75e7a4b5764b9d51.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;开源模型的技术选型&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有了基础设施后，简单介绍一下开源技术栈。之前没显卡时还考虑过 Qlora，但后来发现 32B 模型的 Lora 训练，16 张显卡基本都能搞定，没必要再用 Qlora。在模型选型上，简单模型用 7B、14B、32B 基本都能满足，复杂一点的长文本和复杂任务，32B 模型也能差不多应对。使用开源模型进行部署和训练基本没什么太大问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/a2/a2ce43bb4579af825c06ced9822fba6f.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;数据管理平台&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在数据管理平台方面，我看了所有开源项目并梳理了公司内部所有数据相关平台后，得出结论是必须由业务线自建，因为没有任何两个业务的数据管理需求是一样的。其核心有两点：一是 Badcase 驱动，Badcase 管理非常重要，我每次训练时核心任务是修复 Badcase；二是要进行模型样本管理，避免引入脏数据，出问题时能追溯模型来源，所以要建设模型溯源能力，而不仅仅是数据管理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;培养全员大模型思维与能力&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如何培养全员训练大模型的思维和能力，重点在于提升能力，尤其是让普通研发人员快速掌握大模型训练，建设他们的算法能力。大模型训练流程包括问题定义、提示词设计、样本构建、微调（蒸馏、强化学习）、评估和上线。模型优化能力由四个能力叠加而成：模型问题定义能力、样本构建能力、训练能力和评测能力。最初认为模型训练能力最难，但实际上最容易，一周内所有人都能学会调参，且调参不超过 3 个。研发团队最需要提升的是问题定义和评测能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型的应用场景和优化方式&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我将自己最近半年工作中的教训和经验总结，把所有训练过的大模型场景做了拆分，发现大部分大模型场景都能映射到下表几个类别中。每次模型训练时，思考一下可以放到哪个类别，然后按照相应的优化方式去做，基本都能取得不错的效果。以写作类为例，这是最常用的大模型优化场景，现在 DeepSeek 效果较好，大家开始广泛使用。以前不敢碰写作类，因为需要构建样本，难度较大。但现在通过 DeepSeek 蒸馏和强化学习（GRPO），基本能取得较好的效果。要素抽取类场景中，公有云模型准确率能达到 90%，自身优化空间不大。问答类场景中，大模型能力很少单独训练，大家主要做 RAG 和搜索插件，因为底层工程化可以提升更多效果。还有 API 调用类场景，训练大模型时将其抽象到某个场景，再看每个场景的优化方式。无论是写作还是交互，最核心的是要有一套快速构建样本训练的链路能力，从业务驱动出发，快速构建样本训练，再快速进行评测和 Badcase 修复，以及与之相配合的平台能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/28/282fcfc4f5498bac5e24af11a15ede35.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型测试&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大模型测试曾是我最不关心的环节，但后来发现它对模型优化迭代效率影响最大。首先，数据来源很重要。如果线上有 Badcase，建议直接使用 Badcase 作为优化数据。性能测试方面，大模型性能测试与普通性能测试存在差距，可能会考虑 GPU 并发等因素。但我认为，同样 Token 长度和 Size 模型性能差异不大，不要投入过多精力。最核心的是找一个测过的开源的数据源，拿来即用。效果测试很关键，就是理解模型效果并进行测试。我的感受是，合作的业务线中，是否有优秀的测试人员对最终模型效果影响很大。优秀的测试人员可以从业务需求出发，将业务标准和测试标准转化为测试用例，自动化生成样例，并用大模型自动评测。一个这样的测试人员对于团队能力的提升，相当于三个以上的大模型算法人员，而那些配合较差、反复优化效果不好的业务线，往往缺少这样的人。因此，我在公司内进行大模型测试能力评估，尽管自己做算法工作，但感觉没有优秀的测试人员，工作开展会很困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2d/2d18f5b7cde0f71a02acc5bc48332c29.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型优化案例 1 一多轮改写&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我最早做搜索时，用户输入多轮搜索结果，需要多轮改写来理解用户意图。之前使用传统方法和一些大模型，都无法很好地理解几轮对话之间的关系，上下文无关和上下文有关的内容都识别不出来。DeepSeek 出现后，发现其 R1 效果非常好，因为它有思维链，能思考上下文关系。于是尝试用 R1 做蒸馏，结果效果也很好。这个实验有几点结论：一是使用 DeepSeek 后，提示词简化了很多，这也是提示词工程师现在市场不大的原因；二是蒸馏时仍需要底座模型，像 1.5B 的底座模型较弱，学不到东西；三是思维链加入后，可以做一些以前做不到的事情。举个例子，用户在搜索中要求生成双色球下期中奖号码，以前在 Query 理解上做了很多尝试，但都无法解决。DeepSeek 给出的回复是“双色球号码不靠谱，远离赌博，珍爱生命”，这让我觉得自己之前的尝试很愚蠢。这个案例说明，当新技术如 DeepSeek 出现后，要勇于探索和尝试，会得到超出预期的惊喜，也能让团队成员感到开心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/43/43edd14792b9ecce8cfaa787f098d01d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;大模型优化案例 2 一公文写作&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;写作场景以前是我不敢碰的，因为构建样本难度大。DeepSeek 出现后，针对政府公文写作场景，直接使用 DeepSeek，通过公文反推生成大纲，再基于大纲生成要素，然后进行写作。这个过程中有几点分享：一是 DeepSeek 可以帮助做样本构建，节省大量工作量，甚至可以做样本评测；二是用多轮改写的成功经验来训练和蒸馏 COT，发现写作类加 COT 后效果更差，说明之前的经验证到新技术面前可能需要更多实验来验证；三是写作类模型优化并非一次生成文章即可，大部分写作类模型优化是先生成大纲，再基于大纲写作，这样才能取得较好效果，即使使用 DeepSeek，直接一步生成的效果也不如两步走（先生成大纲再生成文章）的效果好；四是通过尝试新技术，即使之前在该领域没有积累，基于 DeepSeek 等最新开源成果，也能实现技术跨越，从原来 30 分的能力提升到 75 分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/33/33275ec5a7f1e3346c533f0c01325090.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;构建开放共享的协作文化&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在推动工程人员转向大模型工作时，会遇到一些疑虑。例如，一位有五六年的软件开发经验的同学对转向大模型工作非常抵触，他提出了两个疑虑：一是自己不会深度学习理论技术怎么办，我对此解释是大模型工作不需要这些，只要会搞样本、调参数、写 Python 代码就行；二是大模型优化与写代码差距太大，我展示了一个在 QCon 学到的关于工程师文化的图，就是李云老师在 2024 年 QCon 上海演讲分享的 《AI 时代团队管理的不变与变》 中的一张图，该图将工程师文化的关键项总结得很好，指出工程师的工程能力包括设计能力和工程能力两块，之前做工程开发可能是 30% 时间设计、70% 时间工程，而大模型优化可能是 80% 时间设计、20% 时间写代码，本质上仍是工程师工作，只是比例变化，底层活动也一样，都是设计、文档化、写代码以及敏捷开发等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/92/92f5c13aad8f4e33d9ece9ed347a97e5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果有人担心自己的效果比不上专业的研究团队，那是因为缺乏经验，存在知识壁垒和技术孤岛。解决方法是打破壁垒，通过开源和分享打破技术孤岛，大家团结起来共同成长。遇到问题时，可以找人问、开分享会、开会研讨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;一些解决遇到的大模型优化问题的经验&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在做多轮搜索时，面临模型合并、样本合并问题，如果每个模型都单独训练，最后需要维护几百个模型，这是无法维护的，所以把相似数据放在一起同时训练，但这样导致准确率下降很多，当时不知所措，于是向研究院同学请教，对方建议把多轮与单轮的 promot 差异加大，尝试后发现有效；又向工程同学请教，对方说 VLLM 支持动态的 Lora 加载，每个模型训练一个 Lora，然后动态加载即可，这两种方式都能解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在写作场景中，出现前面写得正常，后面突然出不来标点符号的问题，当时甚至想用强化学习设置 Reward 来解决，但训练底座大模型写作的人说把 decay 的惩罚从 0.6 设到 0.1，尝试后发现可以解决。现在回看去年做的事，觉得当时犯了低级错误，但认为这不是黑历史，而是成长之路，想跟大家分享的是遇到问题找别人会得到帮助，能力是逐渐积累的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;工程师文化建设&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我在公司负责一些工程师文化建设工作，梳理出工程师文化最核心的几点是技术过硬、专业靠谱和开放共享。在大模型时代，我个人最认同的是开放和乐于分享，整个团队、公司或组织需要有更开放共享的文化心态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;总结与展望&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从组织氛围或组织变革角度看，训练大模型很简单，只要有平台、有业务 Sense 就能做起来。大模型基础平台可以低成本建设，有众多开源资源可复用。大模型场景就那几类，按流程优化就行。要拥抱开源，避免闭门造车。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后是致敬：一是 QCon 上一位老师的分享，他讲的“优化算法最好的办法就是找 bug”这句话对我后续工作影响很大，认为在大模型时代，找 bug 和 review 数据比调参更有用；二是 Hugging Face，感谢它提供很多优秀的开源模型和数据，每个公司都需要有自己的类似 Hugging Face 的共享平台，用于模型数据、训练方法论和经验的共享，打造开放共享的团队氛围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;吕昕，负责科大讯飞消费者 BG 大数据和大模型技术平台相关工作，先后负责建设了讯飞 C 端用户数据中台、大数据分析平台和大模型应用开发平台等，目前负责多个 C 端产品的大模型效果优化工作。 在大数据平台、个性化推荐、广告算法、商业分析、大模型算法领域有多年经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从基础设施、推理与知识体系，到研发与交付流程，再到前端、客户端与应用体验——AI 正在以更工程化的方式进入软件生产。&lt;a href=&quot;https://qcon.infoq.cn/2026/beijing/&quot;&gt;2026 年&amp;nbsp;QCon 全球软件开发大会（北京站）&lt;/a&gt;&quot;将以&amp;nbsp;「Agentic AI 时代的软件工程重塑」&amp;nbsp;作为大会核心主线，把讨论从&amp;nbsp;「AI For What」，走向真正可持续的&amp;nbsp;「Value From AI」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/01/016a6e281287b30423b759d9f9056ef8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/U214fDcWzZZgPlOvl5m4</link><guid isPermaLink="false">https://www.infoq.cn/article/U214fDcWzZZgPlOvl5m4</guid><pubDate>Fri, 23 Jan 2026 04:25:23 GMT</pubDate><author>作者：吕昕</author><category>AI&amp;大模型</category><category>管理/文化</category></item><item><title>在美国法律管辖权受到质疑之际，亚马逊云科技推出了欧洲主权云服务</title><description>&lt;p&gt;亚马逊云科技（AWS）已将其欧洲主权云服务（&lt;a href=&quot;https://www.aws.eu/&quot;&gt;European Sovereign Cloud&lt;/a&gt;&quot;）推向全面可用，该服务在物理和逻辑上分离的基础设施上投资了&lt;a href=&quot;https://www.infoq.com/news/2025/06/aws-eu-sovereign-cloud/&quot;&gt;78亿欧元&lt;/a&gt;&quot;。该服务现已在德国勃兰登堡州提供，旨在应对欧洲的监管要求以及对美国访问数据的日益增长的地缘政治担忧。尽管AWS强调，该云服务将完全由欧盟居民在新的德国母公司结构下运营，但关于这种分离是否真的能抵御美国政府的数据请求，仍存在重大疑问。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该基础设施使用&lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html&quot;&gt;分区&lt;/a&gt;&quot;名称aws-eusc和区域名称eusc-de-east-1，完全独立于AWS的全球区域运行。所有组件，包括专用的IAM、计费系统和使用欧洲顶级域名的Route 53名称服务器，都保留在欧盟境内。AWS欧洲主权云有限责任公司（AWS European Sovereign Cloud GmbH）是一家成立的德国母公司，拥有三个子公司，分别负责基础设施、证书管理和就业，负责运营。欧盟公民&lt;a href=&quot;https://www.aboutamazon.eu/news/aws/stephane-israel-appointed-to-lead-the-aws-european-sovereign-cloud&quot;&gt;Stéphane Israël&lt;/a&gt;&quot;担任总经理，与AWS德国和中欧副总裁&lt;a href=&quot;https://www.linkedin.com/in/stefanhoechbaue&quot;&gt;Stefan Hoechbauer&lt;/a&gt;&quot;一起担任董事总经理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一位将服务部署到欧洲主权云的AWS软件开发工程师证实了技术隔离在实践中的存在。该工程师在Hacker News上&lt;a href=&quot;https://news.ycombinator.com/item?id=46640462&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;AWS已经在欧洲主权云（ESC）和全球AWS之间建立了适当的界限。由于我在美国，我看不到ESC中的任何活动，即使是我们开发的服务。为了解决这个问题，我们必须与ESC中的工程师进行电话沟通……所有数据确实100%保留在ESC中。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该工程师还警告要注意权衡，指出隔离“确实减慢了调试问题的速度。本来一两天就能解决的问题可能需要一个月。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管存在这种技术隔离，从业者和分析师对法律保护提出了根本性的担忧。独立技术顾问Sam Newman在LinkedIn上&lt;a href=&quot;https://www.linkedin.com/posts/samnewman_unless-ive-misunderstood-the-us-patriot-activity-7417950014951542785-bqBs/&quot;&gt;写道&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;除非我误解了美国的爱国者法案（这是可能的），否则新的欧盟AWS主权云服务并不能保护客户数据免受美国政府的访问。所以我不太确定这是为了什么，除了公司想要支付（我假设）溢价，让自己看起来像是在面对更不稳定的美国政权做些事情。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;ICT解决方案协调员Marko Teklic也&lt;a href=&quot;https://www.linkedin.com/posts/tanuja-randery_awseuropeansovereigncloud-activity-7417476341463384064-GY7K&quot;&gt;表达&lt;/a&gt;&quot;了类似的担忧，他指出，根据外国情报监视法和&lt;a href=&quot;https://en.wikipedia.org/wiki/CLOUD_Act&quot;&gt;CLOUD&lt;/a&gt;&quot;法案，AWS作为一家总部位于美国的公司，仍然受美国对其欧洲业务的管辖。CLOUD法案允许美国当局无论云服务提供商的物理位置如何，都可以要求云服务提供商提供数据。法院可以要求母公司提供子公司持有的数据，这可能使AWS欧洲主权云有限责任公司的独立结构在法律上不足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Reddit上的一位评论者概述了&lt;a href=&quot;https://www.reddit.com/r/europe/comments/1qg58g1/aws_launches_european_sovereign_cloud_to_address/&quot;&gt;这种机制&lt;/a&gt;&quot;：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;该法案适用于“所有在美国运营或在美国合法存在的电子通信服务或远程计算服务提供商。”法院可以要求母公司提供其子公司持有的数据。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一些人认为AWS的结构可能会提供保护。一位Hacker News用户&lt;a href=&quot;https://news.ycombinator.com/item?id=46640462&quot;&gt;建议&lt;/a&gt;&quot;，在欧洲的治理下，亚马逊可以告诉美国政府，欧盟员工拒绝遵守数据请求，因为这样做将违反欧盟法律。怀疑论者反驳说，AWS可以通过向当地员工模糊命令或临时派遣美国员工到欧洲来绕过这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从业者提出了AWS尚未回答的尖锐问题。S. Maud在Jeff Barr的LinkedIn帖子上&lt;a href=&quot;https://www.linkedin.com/posts/jeffbarr_the-aws-european-sovereign-cloud-is-now-open-activity-7417492996415475715-F17H&quot;&gt;询问&lt;/a&gt;&quot;，如果美国政府针对存储在主权云中的军事行动数据发出CLOUD法案令状，AWS是否会遵守。Sebastian Vogelsang对远程干预提出了技术性担忧：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;什么阻止了远程关闭开关？如果AWS公司或美国政府指示禁用这个基础设施，有什么技术或法律机制可以阻止这一点？软件堆栈是完全独立的，还是依赖于可能从欧盟外部撤销的许可证、更新或控制平面？&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件信任问题超出了运营范围。虽然Hacker News评论者指出，AWS的Nitro虚拟化团队设在柏林，但人们对更广泛的AWS软件堆栈仍然存在疑问。是否对后门进行过审计？在美国开发的代码是否包含了远程访问机制？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当被问及AWS欧洲主权云是否类似于AWS的中国区域时，首席云架构师Ivo Pinto&lt;a href=&quot;https://www.linkedin.com/posts/ivopinto01_aws-sovereigncloud-awsportugal-activity-7417538727796858880-ehtH&quot;&gt;确认&lt;/a&gt;&quot;这是“甚至比govcloud更好的比较”。然而，有一个关键区别：AWS中国通过独立的中国公司（Sinnet和NWCD）运营，而AWS欧洲主权云完全由Amazon.com Inc.拥有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CarMax的Eric Swanson解释了这一服务的实际效果：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;美国所有权和总部意味着，无论基础设施在哪里运行，美国法律仍然可以适用于提供商。主权云服务并不凌驾于爱国者法案之上。它们主要减少了在其他背景下的重叠：数据位置、运营控制、员工访问和客户管辖区。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在没有美国所有权的情况下寻求主权的组织可以在欧洲找到替代方案，包括德国提供商Hetzner、法国提供商Scaleway、瑞士提供商Infomaniak，以及StackIT by Schwarz Digits（Lidl的母公司），许多在LinkedIn和Reddit上的评论者强调这些是真正的欧洲主权云选项。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该服务推出了大约90个AWS服务，计划通过在比利时、荷兰和葡萄牙的主权本地区域进行扩展。AWS预计7.8亿欧元的投资将在20年内为欧洲经济贡献172亿欧元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AWS现在的竞争对手是微软和谷歌云与泰雷兹开发的S3NS。Mark Surrow在LinkedIn上&lt;a href=&quot;https://www.linkedin.com/posts/samnewman_unless-ive-misunderstood-the-us-patriot-activity-7417950014951542785-bqBs/&quot;&gt;指出&lt;/a&gt;&quot;，微软“不得不在法国法院直接承认”它不能保证欧盟客户的数据主权。一个根本性的问题仍然存在：任何美国所有的主权云能否在CLOUD法案和FISA下保护欧洲数据免受美国政府访问？在AWS回答这个问题之前，有严格主权要求的组织可能会寻找其他地方。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/aws-european-sovereign-cloud/&quot;&gt;https://www.infoq.com/news/2026/01/aws-european-sovereign-cloud/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/og7i0uWphcTawtJ9ybqg</link><guid isPermaLink="false">https://www.infoq.cn/article/og7i0uWphcTawtJ9ybqg</guid><pubDate>Fri, 23 Jan 2026 03:09:13 GMT</pubDate><author>Steef-Jan Wiggers</author><category>亚马逊云科技</category><category>安全</category><category>管理/文化</category></item><item><title>“AI工程师”已上岗！微软 CEO 曝正尝试新学徒制模式：内部工程师的顶级实践全变</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近的达沃斯论坛上，科技领袖们纷纷出来发表观点。当 Google 的 Demis Hassabis 和 Anthropic 的 Dario Amodei 在讨论更宏观的 AGI 话题时，微软 CEO Satya Nadella 与英国前首相 Rishi Sunak的对话，更聚焦在了AI应用的话题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya以自己参加达沃斯的准备工作变化为例，来说明在企业内部，AI 正在打破传统层级架构，让信息流实现扁平化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“自从我1992年参加以来，直到几年前，流程都没什么变化：我的现场团队会准备笔记，然后送到总部进一步提炼。但现在我直接找Copilot说，“我要见xxx，给我一个简介”。它会给我一个全方位的视角。”“我做的是立即把这个简介分享给所有部门的同事。”&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，企业 AI 应用呈现出明显的 “杠杆效应”：初创公司能从零开始构建适配 AI 的组织，落地速度更快；大型企业虽手握数据、资源优势，但传统工作流程与组织惯性带来的变革管理挑战更大。而无论大小企业，都需经历 “思维转变 — 技能培养 — 数据整合” 的艰苦过程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;人才方面，他认为全球 AI 技术人才与初创公司的质量已无显著差异：“雅加达、伊斯坦布尔的人才技术水平并不逊色于西雅图、旧金山。”真正的差距在于大规模应用的推进力度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya表示，判断 AI 是否存在泡沫，关键也在于落地应用：若仅停留在科技公司的技术讨论，泡沫风险确实存在；但当 AI 加速药物临床试验、提升农业生产效率、优化公共服务时，技术就已转化为实实在在的经济价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;今天，Satya 参加 All-In Podcast的采访也发布了，这次谈话与Rishi那次比，有部分话题重合，但也更微观一些。他谈到，科技行业每十年换一批竞争对手是好事，能倒逼企业保持竞争力，科技产业蛋糕会持续变大，绝非零和博弈。而微软与OpenAI合作的核心逻辑：不押注单一模型，而是打造算力+应用服务器层的平台，兼容多模型生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还提到，公司内部全球网络团队已用AI Agent（数字员工）自动化处理光纤挖断、设备故障等DevOps重复工作，完全是自下而上的落地实践。此外还将LinkedIn等团队各角色合并为“全栈构建者”，重构AI产品工作流。现在，微软正在尝试新学徒制模式：由资深IC工程师带一组应届生，借助AI加速新人生产力爬坡，以适配AI时代的人才培养方式，新人仍需持续进入职场。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;国际竞争方面，他认为，美国技术栈的核心优势是生态效应（平台之上生态收入远超自身收入），而非单纯市场份额，技术“扩散”是做大全球蛋糕，而非抢蛋糕。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们翻译并整理了这次访谈内容，并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;移民政策下的一段“奇妙经历”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：今天非常高兴，能请到重量级嘉宾 Satya Nadella，Microsoft 的第三任 CEO，和我们的 AI 与加密领域负责人 David Sacks 来一场即兴炉边对话。Satya 出生在印度，大学毕业后来到美国，这一路经历本身就很传奇。你在书里写过，为了把太太接来美国，还专门“折返”了一趟。能不能简单和大家讲讲当时是怎么回事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：这件事其实是美国移民政策下的一段“奇妙经历”。我和太太在印度读的是同一所大学，后来我来美国读研究生，我们结了婚。我拿到了绿卡，但问题是由于我们是结婚后才申请，她反而不能直接过来。结果就是，我不得不放弃已经拿到的绿卡。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最有意思的是，我去新德里的美国使馆，问工作人员：“请问放弃绿卡要排哪一队？”他们直接说：“没有这种队伍。”在九十年代，主动放弃绿卡绝对算是件“疯狂”的事。但为了让她能以 H1 签证过来，只能这么操作。好在最后一切都解决了，现在想起来更像是一段久远但有点荒诞的回忆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：我想聊聊 Copilot。你们最早在 GitHub 上推出 Copilot，后来做到桌面端，再到直接把它放进 Windows，这对 Microsoft 来说是个非常大胆的决定。我每天都在用。但老实说，在它还不能真正理解文件系统、也没法和应用深度交互之前，市场反应不温不火。不过现在你们明显在持续加码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我看来，面向知识工作者，AI 正在走向三种形态：一类是 Elon 在 xAI 做的那种“人类模拟器”，据说直接把“虚拟员工”塞进聊天和邮箱系统；一类是 Claude 刚发布的协作型Agent，强得离谱，很多人已经被震住了，我自己连续玩了四十多个小时。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那 Microsoft 的愿景是什么？知识工作者究竟该怎么真正把这些东西用起来？现在大家更多还是在“玩 ChatGPT”，这和真正创造商业价值之间好像还有一道鸿沟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：要理解这些不同形态，最好的切入口其实是编程，代码工作几乎是最典型的知识工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回头看这条演进路线：最早是“Next Edit Suggestions”，也就是智能补全。老实说，我对这一代 AI 技术真正建立信心，就是从早期 Codex 那一代模型开始的。那还是 GPT-3.5 之前，但补全已经相当准确了。后来我们有了chat交互，再往后是可执行的actions，现在则是全自主Agent。这些Agent既可以在前台，也可以在后台；可以在云端，也可以在本地运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有意思的是，这些形态今天在编程中都有，而且你会全部用到，而非只选其中一种。比如我在 CLI 里，可以有前台Agent、后台Agent，同时直接在 VS Code 里改代码，这些全部并行进行。这说明了不同形态是可以组合的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;把这套放到知识工作上也是一样。我们是从chat开始，带推理的chat不只是一问一答，你能看到它完整的思考过程；现在到了actions阶段，通过模拟电脑操作、Skill和Agent 调用调用来执行任务，这就是Copilot 如今的状况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来，其实需要一个新的“隐喻”来理解 AI 时代的计算机。Jobs 当年形容 PC 是“思维的自行车”；Bill Gates 说过一句我很喜欢的话：“信息触手可及”。但在 AI 时代，我们需要新的说法。我很喜欢 Notion CEO 的一个比喻：“无限思维的管理者”。这个说法非常形象。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：确实是个很棒的产品。不过你们还没收购它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：还没有（笑）。但这个比喻点中了关键：你同时在和大量Agent协作。我自己还常用两个词：宏观委派和微观引导，即你把一整块工作交出去，同时在执行过程中不断给细节指令。写代码其实已经是这样了。这正是今天 Copilot 的真实状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有一种我特别期待的形态，很快你们就会看到：开发者并不是只待在自己的 repo 里。我们要开会、写设计文档、实现别人写好的规格说明，还要保证代码和这些内容一致。这就意味着，Copilot 需要能通过 MCP Server 之类的方式，把我的工作流、待办事项、上下文全部拉进来。这才是真正的知识工作“组合”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;安全领域也是一样。一个安全工程师面对的是海量日志：把日志放进文件系统、用代码分析、生成仪表盘，这些都是 AI 能大幅放大的知识工作场景。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;数字员工如何进入企业&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那“数字员工”“数字同事”这种概念呢？是不是也在你们的规划里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：核心问题其实是“身份”。我们推出了 Agent 365，就是把今天给人用的身份体系、终端防护体系，扩展到Agent身上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：也就是说，你可以“克隆”一个我，让他在 HR 或市场部里工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：没错。在 Office 体系里完全可以做到。这里有两种模式：一种是，每个知识工作者都拥有“无限个大脑”；另一种是，创造完全独立于你个人身份的Agent。而身份这件事非常关键，权限、决策、责任追溯等全都依赖它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：说到底，就是搞清楚“谁对谁做了什么”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：正是如此。对任何组织来说，最重要的问题之一就是：工作是谁完成的、怎么完成的、来源是什么、能不能追溯，所以要么是“人 + 一堆Agent”，由人来做宏委派、微观引导，要么就是一个完全独立的身份在运作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：过去几年，Microsoft 的员工数量基本没变，但收入多了 900 亿美元，利润还翻了一倍。你们也像 Alphabet、Meta 一样，削掉了不少中间管理层。这是因为自动化？还是以前人确实有点多？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：你抓住了一个非常关键的问题。我认为，这是自 PC 普及以来，知识工作最大的结构性变化。想想 PC 之前，一家跨国公司怎么做预测？传真、内部备忘录满天飞，最后凑出一份结果。后来 PC 成了标配，Excel + Email，让流程和产出物全变了，今天正在发生同样级别的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，在 LinkedIn，我们以前有产品经理、设计师、前端工程师、后端工程师，后来我们把前面这些角色合并、扩大职责范围，统一成“全栈构建者”。这是结构性的调整，它改变了工作本身，也改变了工作流。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：沟通成本一下就下来了，速度自然更快，一个人就能“vibe coding”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：没错，而且 AI 产品本身也有一套全新的工作流：从评测、到科学建模，再到基础设施。评测和产品由新的“全栈型 PM / Builder”完成，系统工程师负责支撑后端科学和基础设施，这是一个全新的闭环，必须从组织结构上去适配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，对 Microsoft 来说，我们不可能只活在未来。现在，我们要一边把 Windows 的热补丁做好、质量做到位；一边还要持续提升 Copilot 的评测体系和质量，这两件事都必须是第一优先级的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“每十年换一批竞争对手”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：这大概是你职业生涯里最具挑战性的阶段吧？过去 Microsoft 在很多领域是双寡头甚至垄断，但现在面对的竞争完全不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：确实非常激烈。但我一直觉得，每十年换一批竞争对手，其实是好事，它能让你保持“体能”。我 1992 年加入 Microsoft，那时最大的对手是 Novell；现在是 2026 年，环境完全不同。竞争很残酷，但从 GDP 占比来看，五年后科技产业一定更大，这不是一个零和游戏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：蛋糕在变大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：而且会大得多。整个技术栈对社会的影响会极其深远。最终的问题是Microsoft 的品牌定位是什么？客户期待我们提供什么？有时候我们会误以为，所有客户对所有厂商的期待都是一样的，但真正重要的是弄清楚客户“希望从你这里得到什么”。这其实是 Peter Thiel 那个观点的另一种表达：不是逃避竞争，而是通过理解客户，找到你真正不可替代的位置。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：这次在达沃斯，既有不少国家领导人，也有大量《财富》世界五百强公司的 CEO。昨晚晚宴上，有人问你一个问题：他们该如何看待 AI，怎样才能真正把 AI 用好。我记得你当时提到了“扩散（diffusion）”这个词，这一点和我最近参与的一些政策研究高度契合。能不能展开讲讲你的想法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：当然可以。事实上，你们一直在做一件非常重要的事，就是确保以美国为代表的技术栈，能在全球范围内被广泛采用、并且被信任。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回过头来看，技术本身只是起点，真正的价值来自于被大规模、深入地使用。我一直很喜欢一项研究，是 Diego Comin 做的，研究的是工业革命时期各国是如何实现领先的。结论其实很简单：那些把最新技术引入本国，并在此基础上做价值叠加的国家，最终跑得最快。说白了，不要重复造轮子，而是先用最先进的，再在上面持续创新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是“扩散”的意义所在。像 AI 这样的通用型技术，关键在于能不能真正铺开。就拿美国来说，技术我们已经有了，但问题是：它有没有进入医疗？有没有进入金融？有没有进入所有行业？不只是大企业，也包括中小企业和公共部门。如果看不到这种广泛而密集的应用，就谈不上真正的成功。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在我们正处在这样一个阶段：AI 正在更快地“扩散”。你们做的那些政策层面的工作其实非常关键。好消息是，技术已经成熟了，云计算和移动互联网这些“基础设施轨道”早就铺好了，这让 AI 的传播成为可能。现在真正的问题不在算力能不能拿到，而在于具体的应用场景是什么，以及组织如何管理随之而来的变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在达沃斯，还有一个常被提起的问题：发达国家之外，全球南方怎么办？我反而觉得这里蕴含着巨大的机会。在很多全球南方国家，公共部门在 GDP 中的占比非常高。想象一下，如果 AI 能显著提升政府把纳税人资金转化为公共服务的效率，哪怕只提升一点点，那可能就是几个百分点的 GDP 增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我非常乐观，我认为会形成一种强烈的拉动力，而美国也应该把我们已有的技术栈，推动在欧洲、亚洲、南美、非洲等地广泛落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：我经常被问到一个问题：这场 AI 竞赛，怎么判断谁在赢？或者美国是不是领先全球？我给出的答案很直接：看市场份额。如果几年后我们放眼全球，看到美国公司的技术占据了绝大多数市场，那说明我们做对了；如果看到全球到处用的都是中国的芯片和模型，那可能就意味着我们输了。说到底，使用情况才是最真实的检验标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我同意。但你也在 Microsoft 工作过几年，应该记得 Bill Gates 对“平台”的理解。对我来说，除了市场份额，更重要的是生态效应。美国一直以来的优势，不只是本国公司的收入规模，而是围绕平台形成的完整生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我在 Microsoft 学到的一点是，每次去一个国家访问，最先看的不是我们卖了多少软件，而是围绕 Microsoft 平台，在当地创造了多少就业岗位。比如有多少渠道伙伴、多少 ISV、多少相关的 IT 从业者。我们有一整套指标，衡量一个国家的生态是如何围绕平台建立起来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正是美国技术栈过去在全球，包括在中国，能够被广泛采用的原因：当地公司能在上面构建自己的产品和业务。这种事情还会再次发生。所以你们推动“扩散”的工作，本质上不是在抢蛋糕，而是在把蛋糕做大，增强对平台的信任，从而带来真正的经济机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David：你这么一说，我确实想起了一些往事。那还是十多年前，Yammer 被 Microsoft 收购，我们并入了 SharePoint 团队。当时产品经理们非常自豪的一点是：围绕 SharePoint 的生态收入，即非 Microsoft 的咨询公司、实施伙伴创造的收入，其规模是 Microsoft 自身软件收入的好几倍。Bill 也说过一句话：只有当平台之上的收入，显著超过平台自身的收入时，你才算真正拥有一个生态。所以，当我们谈“扩散”，希望美国保持领先地位，并不意味着这对世界其他地方是坏事。恰恰相反，其他国家和公司可以在这个平台之上创造出更大的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：完全同意。这一点非常关键。这不是“美国技术、美国收入”的问题，而是在用一个新平台在全球范围内创造机会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我90年代做数据库产品时，和 SAP 有过深度合作。SQL Server 和 R/3 的结合，对双方都是巨大的成功。大家常提 Intel 和 Microsoft，但对我个人成长影响很深的一件事其实是和一家欧洲软件巨头的合作。放到今天也是一样，谁知道下一个伟大的 AI 应用会出现在哪里？我始终相信，即便基于美国的技术栈，世界各地都可能诞生顶级的科技公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;与OpenAI合作背后：所有公司、应用会同时用多种模型&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：你不仅是技术领袖，也是一位非常出色的并购操盘手，这一点其实被外界低估了。你和 Sam Altman、OpenAI 的合作，被认为既高明又充满争议。有人说，这笔交易可能让 Microsoft 获得巨额回报，但也有人质疑：你是不是亲手培养了一个未来最强的竞争对手？尤其是考虑到 Microsoft 过去错过了移动互联网浪潮，你们为什么不自己做一个 Gemini、xAI 或 Claude？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我理解这种疑问。很多人问我：你们自己的基础模型在哪里？从知识产权角度说，我们确实拥有相关能力，但更重要的是，Microsoft 现在的战略有几个层面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先，我们要把“算力工厂”做好。Azure 是我们最大的业务之一，而随着 AI 的发展，它的市场空间会变得极其庞大，这要求我们在异构基础设施管理、软件调度和资源利用率上做到极致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其次，是应用服务器层。未来，每个人都在构建Agent，有强化学习环境、有评测体系，就像每一代平台都会有自己的应用服务器一样。我们现在在做的 Foundry，就是这个定位。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这一层里，有一点已经非常清楚：任何应用、任何公司，最终都会同时使用多种模型。为什么不用呢，甚至在一个具体任务里，编排多个模型协同工作，效果往往比单一的前沿模型更好。我们在医疗领域做过一个“决策编排”的实践，仅仅通过给模型分配不同角色再进行协同，就能显著提升结果质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那是不是可以理解为，你其实看好开源模型，认为大模型本身会逐渐商品化，真正的价值不在这里？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：我更愿意把它类比成数据库市场。最早大家觉得数据库就是 SQL，后来才发现并不是。关系型、文档型、NoSQL，各种数据库层出不穷，甚至出现了大量开源项目和围绕它们建立的公司。模型也会是类似的演进路径，会有闭源的前沿模型，也会有达到前沿水平的开源模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;接下来一个非常重要的方向是：企业能否把自身的隐性知识，真正嵌入到自己掌控的模型权重中。有人问我未来会有多少模型，我的回答是：可能和世界上有多少家公司一样多。这听起来极端，但在我看来，这正是“知识经济”向“AI 经济”转变的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason：那你有没有在 Windows 桌面上，悄悄推进一个本地运行的大模型？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya：其实已经在发生了，现在已经有完全驻留在本地、基于 NPU 和 GPU 的模型。高性能工作站正在回归，这本身就是一件非常有意思的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 明白了。所以 Microsoft 当然会重视 PC，这毕竟是你们的主场，有完整的桌面生态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 是的，本质上这是个商业问题。我们一直认为“形态”非常重要。我常开玩笑说，我的职业生涯是从命令行开始的，说不定最后也会回到命令行。但不管怎样，形态一直在演进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 你当年起步时用的是 Sun 那种最早的工作站，价格五千到一万美元。你能想象有一天，你会向客户推荐一台一万到两万美元的桌面机，里面内置 LLM 和强悍硬件吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 完全有可能。你可以插一张 DGX 卡，做出一台非常强的机器。其实在模型架构上，我们可能只差一次关键调整就能实现某种分布式模型架构，比如真正能自我调度的 MoE 架构。这类突破会彻底改变“混合 AI”该是什么样子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但不管怎样，我们非常明确：PC 必须成为本地模型的最佳载体。本地模型可以承担大量 prompt 处理，再按需调用云端能力。这里面还有大量工作空间，这也是我们正在坚定推进的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;David： 云与本地的协同已经证明了，能直接访问本地文件系统，本身就非常有价值。这让我想到 Yammer。很多人可能不知道 Yammer 当年最大的特点，是用消费级增长打法去攻企业软件。站在今天去看企业 AI 的采用，你觉得未来一年会怎么“扩散”？现在好像正处在一个关键点：会是自上而下，由 CEO 拍板、搞战略转型、走 RFP；还是自下而上，由一批 AI 原生员工先用起来，把工具带进工作中，做出惊人的成果？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 说实话，我觉得两种都会发生。自上而下的原因很简单：在客服、供应链、HR 自助这些场景里，AI 的 ROI 非常清晰，IT 和 CXO 很容易拍板，这也是目前最先落地的一波真实 AI 应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但最终真正改变组织的，一定是自下而上的力量。回看 PC 的历史也是这样：最早是律师把 Word 带进公司、财务把 Excel 带进来，后来有了邮件，最后才变成标配。现在正在重演这个过程。比如说 Agent，现在几乎所有人都在做 Agent，本质是在重构工作流，把大量重复、枯燥的事情自动化掉，这正是自下而上转型的起点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说实话，我最兴奋的也是这种变化。以 Microsoft 为例，我们在全球管理着五百多个光纤运营点，尤其在亚洲。我自己以前都没意识到，这些所谓的 DevOps，其实很大一部分是物理资产：光纤会被挖断、设备会出故障。所谓 DevOps，很多时候就是在不停地发邮件问“这张光纤卡怎么了”“怎么修”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在负责全球网络的同事，已经构建了一批“数字员工”，本质就是 Agent 在自动处理这些 DevOps 工作。这完全是自下而上的：工具已经在那里了，我就用它来做自动化，减少重复劳动，提高效率和质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而这些能力最终能不能规模化，关键不在“学会没有”，而在“用不用”。所谓技能提升并不神秘，就是在实际使用中完成的。工具扩散、工具被真正用起来，这才是最重要的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“我们在尝试新的学徒制模式”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Jason： 正因为如此，现在用这些工具去赋能现有员工，比招人、培养新人要容易得多。站在今天看，如果 Microsoft 规模不变，三、四十年后谁会接我的工作？你们是典型的技术优先公司，理论上已经没有太多理由继续增加员工数量，这几年你们也基本没扩张，只是在内部结构上做了调整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那你怎么看下一代？对那些现在还没拿到 Microsoft offer 的应届生，你会给什么建议？以前你花了很多精力去培养这群人，但现在好像没那么“奢侈”了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Satya： 这是个好问题。现在确实有争论：职业早期会发生什么变化、校园招聘还重要吗？我依然坚定相信校园招聘，因为 AI 会彻底改变一个人掌握代码库、建立熟练度的速度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;过去，新人进团队的爬坡期很长；现在不一样了，有文档、有技能库，还可以直接问 Agent，本质上就像身边有一个极其强大的导师帮你快速上手代码。换句话说，应届生的生产力曲线会比以往陡得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们也在尝试新的学徒制模式：让一位资深 IC 工程师带一组应届生一起工作，因为这本身就是一种全新的工作方式。以前大家进 Microsoft 后会去读 Dave Cutler 的代码，理解什么是顶级工程实践；而现在，顶级实践更多体现在十倍、百倍工程师是如何借助 AI 打造高质量产品的。对于这些经验，新一代毕业生会学得更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对 Microsoft 这样的公司来说，这是好事。毕竟只要人类还没解决“永生”问题，我们就需要新人进入职场、在 Microsoft 成长。所以我们依然会积极投入，只是会确保岗位的边界和内容，让其既符合现有员工的期望，也符合新入职者的追求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=5nCbHsCG334&lt;/p&gt;</description><link>https://www.infoq.cn/article/9ZTy7eW64XZMNOkZJV1r</link><guid isPermaLink="false">https://www.infoq.cn/article/9ZTy7eW64XZMNOkZJV1r</guid><pubDate>Thu, 22 Jan 2026 10:38:46 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>马斯克的底裤要被扒光了！超级爆料一个多小时， xAI 工程师被火速解雇</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;Sulaiman Ghori 在一期播客中，用了一个多小时详细讲述了他在 xAI 的经历。他说，在那里“从来没有人对我说不”，每个人都被充分信任去做正确的事；只要是好想法，当天就能落地、当天就能得到反馈。他还提到，马斯克愿意被证明是错的，只要你能拿出实验数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他也坦言，在上一家公司，很多事情也许他一个人能做得更快；但在 xAI，整体反而更快，因为几乎没有官僚流程。这些话，听起来都是对公司的认同和马斯克的赞扬，实际上他还说自己是马斯克粉丝。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后，播客发出来后第3天，他被解雇了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;外界猜测是因为他说了太多敏感信息。节目中，他透露了利用闲置特斯拉汽车驱动的人类模拟器AI代理的计划、还有马斯克如何快速构建Colossus超级集群、xAI在模型策略上的核心决策，曝光了公司内部部署测试的AI虚拟员工等，还有xAI也被完全曝光。他坦率地谈到了激进的时间表、马斯克亲自参与的Cybertruck奖金计划、内部文化和运营方式以及一些非公开的策略，这些言论引发了外界的强烈反响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/72/728e59e8a2fe868f89affab28208d2d9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经历被玩梗：如何在1小时内毁掉你的一生，对应了最近x的爆文“如何在1小时内修复你的一生”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman自 2019 年起持续创业。在德国上大学一个月后退学，为了实现童年创办航天公司的梦想，在自家后院亲手制造过一台液体燃料火箭发动机。创业失败后，他进入xAI。对于他的经历，有网友表示，“这位兄弟跑去上播客，没拿到明确授权，就顺手把一堆内部敏感信息抖出来，这就是纯纯的新手行为。可以说，这是职业生涯级别的大忌。任何一家严肃的公司都会立刻把你原地开除，更别说是像马斯克这样的人。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们翻译并整理了他这期“超级爆料”的播客对话，并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;在xAI，事情永远是“昨天就该完成”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：今天我很高兴能和 Sulaiman Ghori 坐下来聊聊，他是 xAI 的一名工程师。我从 2023 年马斯克刚开始搞 xAI 的时候就一直很关注这家公司，感觉它可能是史上增长最快的公司之一。你能不能跟大家讲讲，现在 xAI 到底在发生什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，我们几乎没有所谓的 deadline，永远都是“昨天就该完成”。基本没有什么人为障碍。马斯克一直强调要“追根溯源”，找到最底层、最根本的东西，不管是物理层面的还是其他的。我们通常会非常快地深入到那个层面，能多快就多快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在软件行业其实挺有意思的，因为你平时不太会把硬件这件事放在心上，但我们确实花了很多精力去考虑这些。而且严格来说，我们现在也不完全算是一家纯软件公司了，毕竟基础设施的建设占了很大一部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，现在明显是被硬件限制住的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没错。硬件可能是我们最大的优势之一，因为在部署能力上，几乎没有其他公司能接近我们。不过，软件方面的人才密度也高得惊人，我从来没在任何地方见过这样的团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我觉得马斯克有一点特别厉害：他很擅长提前判断未来几个月、甚至几年后会出现什么瓶颈，然后从那个未来的瓶颈反推，确保自己现在就站在一个很好的位置上。这种思维方式在日常工作中是怎么影响普通工程师、AI 开发者的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常我们一旦要快速启动一个新项目，不管是我们还是他自己，都会先定一个指标。这个指标一般都非常核心，要么直接关系到财务回报，要么关系到硬件层面的产出，有时候两者都有。之后，所有事情都会围绕着这个指标来推进。而且我们不太接受那种“这事本来就不可能”的说法，就算真有极限，那也必须是一个扎根在最底层的、本质性的限制，而不是人为的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;软件行业里，尤其是过去十年做 Web 开发的人，往往会默认、接受很多所谓的限制，比如速度、延迟之类的。但实际上这些限制很多都是假的。技术栈里有大量没必要的开销和“蠢东西”，如果你能把这些清掉，很多系统都能直接提升 2 到 8 倍，至少是那些相对比较新的东西。当然，也有些老东西确实不好动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最近一次真正感受到“传统认知被彻底打碎”的经历是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：最近一次就是我们在 Macrohood 上做模型迭代。我们同时在做几种全新的架构，而且是并行推进的。现在我们几乎每天都会出新版本，有时候一天不止一次，有些甚至是从预训练阶段就开始重新来。这在业内其实非常少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这背后有几个原因：第一，我们有一支非常强的超算团队，他们解决了很多训练过程中常见的障碍。即便我们的硬件环境变化很大，但通常一个机架搭好后，一天之内就能开始训练，有时候甚至几个小时就可以。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这真的很不正常，一般不是都要好几天吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：甚至好几周。过去十年里，大多数人都是把这些事情抽象掉，交给 Amazon、Google 去管，他们给你多少算力你就用多少。但在 AI 时代，这种方式是行不通的。要么你死掉，要么你自己把这些东西建出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;入职初体验：没人管，做模型和产品默认资源到位&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：当初为什么加入 xAI，以及前几周入职体验怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我当时刚搬到湾区，在做自己的创业项目。那段时间，xAI 的联合创始人之一 Greg Yang主动联系了我。他真的很会招人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一开始收到邮件的时候还以为是垃圾邮件，因为那时候我经常收到那种“嘿，想聊聊吗”“我很欣赏你做的事情”之类的邮件。正准备删掉的时候，看到发件人的域名是 xAI，我一下反应过来：等等，这不是那帮人吗？当时他们大概成立了八个月左右，我就答应先聊聊。我们聊了好几次，我本来还想再试试别的机会，但后来发现时机不太对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个项目最后也没做下去，原因很明显：用一百万美元是不可能把 Macrohard 这种东西做出来的，但想法本身是对的。接下来六七个月，我基本是在烧钱，做各种航天相关的小项目，还试过一个“空气空间”相关的概念，后来也发现大概率行不通，但至少试过了。于是，我又给 Greg 发邮件，说能不能再聊聊。他直接回我：要不要明天面试？我说“好”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面试还算顺利，我周一就搬家，直接入职了。第一天真的没人管我，就给了我一台电脑和工牌。我当时想：那现在怎么办？我去找 Greg，说我连团队都没有，也没人告诉我该干嘛。他当初招我进来，更多是因为他认可我之前做的事情，也觉得和 Macrohard&amp;nbsp;的长期方向相关，但那时候 Macrohood 甚至还算不上一个正式项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后来正好 Ask Grok 要启动，做和 X 的集成，他们问我能不能帮忙，我说当然可以。第一周我基本就是和另外一个人一起干活。但我很快意识到，在 xAI，你坐在工位上，甚至站起来一看，就能指着某个东西说：哦，这是那个人做的。这种感觉非常酷。而且我连固定工位都没有，就坐在当天没来的人桌子旁。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那时候公司里人其实也不多吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，大概也就几百人，工程团队一百来号人。基础设施团队具体多少人我也说不太清，因为有些人是从其他团队慢慢转到我们正式编制里的。但整体规模确实比其他实验室小个数量级。当时我们刚做完 Grok 3。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：真的很酷。我特别喜欢的一点是，xAI 从成立到现在的速度实在太夸张了。我记得马斯克一开始还说，不确定在别人已经领先好几年的情况下能不能成功。结果你们第一个 Colossus 数据中心 122 天就建完了，这在行业里几乎是不可想象的。这种速度塑造了一种怎样的文化？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：他让我们在做模型和产品的时候，可以默认资源是到位的。事实也确实如此，我们并没有被资源严重卡住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然，我们还是会把资源用到极限，但那是因为同时在推进二三十个、甚至更多事情。有大量训练任务并行跑着，通常是由少数几个人在推动。这也是为什么我们在模型和产品迭代上能这么快。而且这种速度让我们可以更长期地去思考。比如 Grok 4、Grok 5，其实在我加入之前、甚至 Grok 3 落地之前，规模和预期就已经设计好了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，至少提前一年在规划？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，而且你能感觉到，这些预期大概率是能实现的，因为团队整体非常可靠。这就极大地解放了你的思维，让你不用老是纠结“我会不会做不到”。举个例子，我们之前假设的最低延迟，其实比真正需要的高了大概三倍，而基础设施的建设让我们可以做到这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是什么意思？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们在做的一种新架构，如果没有足够高的实验频率，基本是不可能推进的，因为它完全不建立在现有研究基础之上。你需要全新的预训练体系，也需要新的数据集。这本身并不完全受制于硬件资源，虽然也有一些因素，比如 Tesla 计算平台的问题。这个其实已经是公开的了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在在想一件事：如果我们用 Macrohard 去做“人类模拟器”，那要怎么部署？如果要部署一百万个“人类模拟器”，就需要一百万台计算机，这怎么可能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果两天后答案就出现了：Tesla 车载计算机。它的资本效率非常高，我们可以在上面跑模型，甚至跑一个完整的人类工作环境，成本比在 AWS、Oracle 的虚拟机上，甚至直接买 Nvidia 硬件都要低得多。这让我们可以假设：我们能以更快的速度、在更大的规模上部署。所以我们也相应调整了预期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，你们基本上可以直接利用汽车网络?&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：所以这其实是一种潜在的解决方案。简单来说，我们想要一百万个虚拟人（VMs）。仅在北美，就已经有大约 400 万辆特斯拉汽车。假设其中有三分之二，或者哪怕一半，已经配备了 Hardware 4。而且在 78% 到 80% 的时间里，这些车基本都是停在那里，要么闲置、要么在充电。那我们完全可以付费，让车主把车的算力时间“租”给我们。车本身已经有网络、有散热、有电力。我们可以直接在车上运行一个“人类模拟器”，也就是 Digital Optimus。这样一来，车主的租赁费用能被覆盖，我们这边则得到一个可以投入工作的完整人类模拟器。整个过程几乎不需要额外的基础设施建设，基本就是一个纯软件层面的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：对，这个资产本来就放在那里，你们只是把它用起来了，太厉害了。那从宏观层面看，这种“人类模拟器”规模化到几百万个，它的目的是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：其实核心概念非常简单。Optimus 就是把人类能做的任何物理任务，让机器人自动完成，成本更低，而且可以 24×7 全天候运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在做的，是把这个逻辑复制到“数字世界”。凡是人类需要通过键盘、鼠标、看屏幕、做决策来完成的数字化工作，我们都可以直接去模拟人类的操作过程。完全不需要软件方做任何适配，也不需要改系统。只要现在有一个岗位是人类在用电脑做的，我们理论上都可以直接部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：挺有意思的。那具体会怎么推进、怎么落地呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们还没公开详细的落地计划，整体来说会是先慢后快。对我们来说在于，要么基础设施已经建好了，要么我们可以直接用特斯拉的网络，或者自己扩数据中心、测试算力。实际上，从一千个“人类模拟器”扩展到一百万个，差别对我们来说并没有想象中那么大，这反而不是最难的部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克一个电话“救火”，个人“生死自负”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：马斯克最擅长的一件事，就是在公司里不断“救火”，哪里有问题就冲到哪里把问题解决掉。你有没有见过那种，本来是个大问题，但被他非常快地解决掉的情况？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有，最典型的就是基础设施建设，这是最大的一个。模型这边也有过一些小波折，但整体还算顺利。在模型侧，因为涉及很多非常底层、非常具体的算子，每一代 ASIC、CPU 都是为特定操作优化的，当我们引入新硬件，比如从 Nvidia 或其他厂商拿到新产品时，往往不是所有东西都能直接跑起来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;去年年初有几次内部会议，他听到这些问题之后直接打了一个电话，第二天软件团队就给我们交付了补丁。我们几乎是并肩作战，直到问题解决，然后就能很快在新硬件上跑模型或训练任务，否则这种来回沟通可能要拖上好几周。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以很多这种“卡点”，真的就是一个电话就解决了。要么是我们主动提出来，要么他自己会问。经常在会议快结束、或者讨论暂时停顿的时候，他会突然来一句：“我能怎么帮忙？怎么能把这件事再加快一点？”然后就有人把问题抛出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道你们在并行做很多不同的产品，这在一定程度上是必须的。但在大多数组织里，同时推进多个目标，其实很难保持专注。你们是怎么做到多线并行还能高效执行的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，很多时候，是在全员会议或者大家私下聊天时，我们才真正搞清楚每个人在做什么、各个项目进展到哪一步。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如，我们当时做语音模型和语音部署，其实很多极低延迟的端到端能力早就已经在系统里了，从数据包发到客户端那一整套链路都准备好了。后来只是把正确的开关打开、解决一些冲突，延迟就直接降了两三倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种情况非常常见：在软件或硬件某个角落里，存在一个“很蠢”的问题，而恰好已经有人想好了方案。你可能是在翻代码库的时候发现，或者随口问一句，有人就会说：“哦，这个 XYZ 已经搞定了，你去找他就行。”基本不需要花太多时间对齐、同步、请示。提出一个想法，反馈要么是“这想法不行”，要么是“那为什么还没做完？”然后你就直接去做，事情就这么推进了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在马斯克的公司里，好像你只要主动要责任，就得“生死自负”。事情做成了就担负更多责任，做不成可能就出局。你的体验是这样吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，基本就是这样。我参与过很多不同的项目，大多只是因为有人找我帮忙，我就一直帮下去。结果到最后，我就成了某个模块、甚至一大块系统的负责人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对所有人来说都是这样。如果你在某个领域有经验，或者能非常快地推进事情，几天之内，这个组件就归你负责了。从“正式流程”上看其实挺混乱的。我在 HR 系统里可能还是挂在 voice 和 iOS 名下，安全系统甚至还以为我在做 X 的集成，从来没人更新这些信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，你进公司时并没有一个非常清晰的工作方向，就是先开始干活，然后不断在不同项目之间流动，谁需要你你就去哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：差不多是这样，会有很多重叠和流动。入职之后，我通常同时参与两三个项目，哪个最紧急、或者我能帮上最多忙，就会占用我大部分时间。然后项目之间会像瀑布一样自然切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那从入职到现在，你大概都做过哪些项目？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：一开始我做的是 Ask Grok以及相关集成，也和后端团队一起处理过可靠性和扩展性问题，当时系统规模增长得很快；之后我独立承担了桌面端套件的开发，把它做到内部可用的完整状；接着又被拉去帮做 Imagine 的发布，以及 iOS 相关工作。说真的，iOS 团队小得离谱，和用户规模完全不匹配，你绝对猜不到有多少人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：五个？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：三个。当时推出时，我正好是第三个。但大家都非常强。这是我第一次感觉到，自己必须拼命跑才能跟上整体的节奏和人才密度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你第一次真正感觉到“自己被充分使用”的时刻是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：肯定是Imagine的那次发布。我们基本是 24 小时一个迭代周期：晚上收到反馈，当晚就改；第二天早上再看新一轮反馈，接着马上修 bug、加大家想要的新功能。模型这边有新变化，我们也立刻跟进。整个节奏非常快，那可能是我连续每天都在办公室待着时间最长的一段时期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那段时间持续了多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：大概两三个月。那段时间几乎没有周末，但我反而挺开心的，也算验证了自己能扛住这种强度。之后我就被调去做 Macrohard 产品了，当时那边只有另一个人，一开始就我们俩。我从项目启动一直做到现在。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;疯狂推进度，马斯克直接送Cybertruck&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：关于 Colossus 的建设，我不知道你了解多少。早期 xAI 团队为了把 Colossus 跑起来，在供电、算力、各种基础条件上都做了很多“疯狂”的事。到现在，其实还是到处是瓶颈，总觉得还需要更多芯片、更多 GPU、更快的速度。你当时的感受是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这一路上有太多“战争故事”，也下过不少赌注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：挑几个讲讲吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：好。我记得 Tyler 当时和马斯克打了个赌。我们在上新机柜的时候，具体是哪一代 GPU 我都忘了。马斯克说，“如果你能在 24 小时内用这些 CPU 跑起来一次训练，我今晚就送你一辆 Cybertruck。”结果那天晚上我们真的把训练跑起来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：他拿到了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：拿到了。现在从我们午餐的窗户望去就能看到那辆车，马斯克人挺酷的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到供电，其实我们必须和市政、电力公司还有州一级的电力机构高度协同。因为当他们那边负载飙升时，我们就得立刻切断公共电网，全部切到自备电源上——大概是八十台，甚至可能更多，用卡车拉来的移动发电机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整个切换过程必须无缝完成，不能影响任何正在跑的训练任务。你要知道，那些训练极其不稳定，GPU 和硬件的功耗可以在毫秒级别上下波动，动辄就是几兆瓦。这件事本身就非常夸张。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那是不是也是为什么你们会把巨型电池组直接放在数据中心旁边？这样负载上下波动就能更快响应？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。没有电池的话，很难这么快地调整负载，发电机毕竟是物理设备，你是在让一个真实旋转的东西加速或减速，它天然就有时间延迟，电池的反应速度要快得多。从物理层面看，整个链路是：本地电容、数据大厅侧的电容、电池、发电机，最后才是公共电网。当然，这套架构我们现在可能也在不断调整，尤其是散热这块，反应速度必须非常快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你还有没有那种“本来不可能，但最后居然成了”的故事？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有。比如我们这块地的租约，从法律意义上讲其实是临时的。这样做是为了最快通过审批、尽快开工。我猜以后会转成永久的，但现在确实是短期租约。对数据中心来说，这是目前能把事情推进得最快的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：他们是怎么允许这种操作的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：算是一种地方和州政府层面的特殊豁免。你只是“临时”改造这块土地，类似嘉年华那种用途。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以 xAI 本质上就是个要来的“嘉年华”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：差不多就是这个意思（笑）。但正因为这样，事情推进得特别快。内部规划加建设，全程不到一个月就搞定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：规模接下来肯定会继续疯狂扩张。马斯克 也说过，能源会是最大的瓶颈，其次才是芯片。在这种很难预测未来一到两年项目和资源需求的情况下，你们是怎么做规划的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们会尽量从“杠杆率最高的目标”倒推。先想清楚：在某个时间点之前，我们最值得做的事情是什么。比如，如果我们想在某个日期前做到一千万甚至一亿美元收入，那从经济和系统设计角度，最有效的事情是什么？然后再倒推：需要什么软件、什么物理基础设施，最后一步步拆解。所以我们几乎不会从“硬件需求”开始，那通常是最后才考虑的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那是不是也有一套类似 SpaceX 的“让事情发生”的算法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：你是说那种“先删掉，再加回来”的逻辑？那确实一直都在用。我们经常先把某个东西砍掉，等确认必须要的时候再加回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你最近一次这么干是什么时候？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：今天（指录制当天）。Macrohard 上部署大量变化极快的物理硬件，让测试变得很难，所以我们尽量减少下游的“特殊情况”。比如，我们要让三十年前的老显示器到最新的 5K Apple 显示器，全都跑在同一套技术栈上，结果发现并不是所有系统在任何时候都能愉快地配合。比如视频编码器，在某些层级上就得反复调。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我之前不知道，后来才发现，有些编码器对“最大像素数”是有硬上限的。所以我们一开始删掉了多编码器的特殊分支，后来在 5K 分辨率上撞墙了，又不得不把这个特殊逻辑加回来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“优秀的人太多了，反而变得很难判断”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在你看来，xAI 本身有哪些特别值得讲的地方？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：首先是人，这里的角色非常多样；其次是我们的招聘方式也挺“怪”的。有些我原本觉得很蠢的做法，结果发现居然行得通，那我们就直接试。比如搞 hackathon，如果能从五百个人里挑出五个顶级选手，这件事就非常划算。他们未来给公司带来的预期价值，远远高于这次活动的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们前几天还算了一笔账，现在主仓库里，每一次 commit 的“价值”大概是 250 万美元。我今天提交了五次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你今天直接加了差不多一千两百万美元？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：轻轻松松的一天（笑）。确实不错，杠杆效应非常强。你用更少的努力和时间就能做更多事，因为身边的人和内部工具都很棒。还有我的老板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那什么样的人会想来这里工作？我听你描述，感觉第一天来的人就已经准备好周末、熬夜、全天候投入了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：大家刚来的时候都非常兴奋，非常有热情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：使命感驱动？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，但野心的类型不一样。有些人想往管理层走，看有多少人向自己汇报；也有人想“拥有”一大块技术栈。比如现在，我们在重构核心生产 API，基本上是一个人+20个 Agent 在做，而且做得非常好。你完全可以独立拥有代码库中的很大部分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有点像 X 被收购之后那样，人很少，但每个人负责的范围巨大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：除了 hackathon，你们在招聘上还有什么不太常规的做法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我们在 Macrohard 上推得非常猛。有两、三周的时间，我每周面试20多个人。有的只聊十五分钟，有的就是一整小时的技术面。优秀的人太多了，反而变得很难判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你怎么判断？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我有一道自己解决过的、非常具体的问题，是几年前在创业时遇到的一个计算机视觉问题。我会给候选人半小时去实现解决方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个问题本身其实很简单，但“简单得很有欺骗性”，大多数人都会想复杂。我特别看重一点：你能否不过度思考，给出一个朴素但有效的方案。因为我们的系统要跑在跨三、四十年的各种硬件、操作系统上，如果不保持简单，下周代码量就能膨胀到一千万行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你还会看重哪些杠杆能力？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我喜欢会质疑需求、也会质疑我的人。这个方法我从 Chester Ford 那里学来的。&lt;/p&gt;&lt;p&gt;他在招聘时，常常会故意在题目里塞一个错误的需求、不可能的条件，期待候选人指出来。如果对方没发现，他就不招。我现在也这么干，效果非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们的节奏真的快到离谱。你自己也在做很多不同的事情，面对新任务时，怎么最快上手？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：要看具体是什么。如果是代码多，那就老老实实读代码，反复跳转定义，很快就能摸清楚。很多时候，实现代码比想象的要少。只有在高度活跃开发的模块里，才会同时存在二十个版本，你根本不知道哪个是主线，这时候就只能去问人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我惊喜的是，这里的人都非常开放、友好。我原本以为大家会很聪明、也很傲慢，但事实是：大家都很聪明，而且非常乐于帮忙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不怎么写文档，因为写文档的速度跟不上开发速度（笑）。现在我们也在尝试用 AI 自动生成文档。好处是，我们有几乎无限的算力和很聪明的 AI，可以大胆试各种“蠢办法”。在别的创业公司，这可能要烧掉几十万、上百万美元，但我们几乎是零成本。结果就是：实验更多、失败更多，但成功也更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克极限压缩时间，“办法总会有的”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在实验这件事上，你们是怎么最大化“尝试次数”的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常都会有时间限制。我们经常在模型侧同时跑两、三个实验。有时候不是因为时间紧，而是因为两周后某个前置条件才会就绪：可能是硬件，也可能是数据。但今天你必须上线一个东西，那就先跑几种方案，看哪个今天就能交付、能产生收入或客户效果，两周后条件成熟了再切换。这种做法在 Macrohard 里是常态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你有没有遇到过这种情况：按理说一个项目的周期应该拉得很长，但你们却压缩后提前了好几周甚至几个月完成？这种事经常发生吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：每次都是这样，无论是跟马斯克的会议，还是内部讨论，只要有人强力推动一件事，或者有外部的人——哪怕他并不对这件事负责——提出了新的需求、要求你把某件事做出来。我们一开始都会觉得，这个时间要求太离谱了。通常会花两分钟想一想、抱怨几句，然后剩下的时间就全部用来想：怎么在这个时间内把事情做完。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说到底，对完成时间的预估，永远建立在一堆假设之上。一旦时间被压到原来的二分之一、甚至十分之一，你就会回头看这些假设并问自己：这些假设对时间的影响到底有多大？然后你要么把它们砍掉，要么调整掉。这样一来，时间线立刻就能快一倍。你多做几次这样的优化，基本上任何要求都能满足。当然，最终还是会撞上物理极限，但一开始的时候，你离那个极限其实远得很。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道像完全自动驾驶、SpaceX 的火箭也是类似的情况。马斯克给的时间线通常都比实际要长得多，所谓的 “马斯克时间” 可能只有真实周期的四分之一或者一半。但正因为一开始把时间线定得这么激进，事情反而真的快了好几倍。xAI 这边是不是也差不多？虽然现在更多是软件，但哪怕在数据中心这类硬件侧，感觉进展也快得离谱，而且基本都落在他最初说的那个时间范围内。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我觉得他自己也在不断校准他的时间判断。毕竟现在马斯克已经在大规模部署各种各样的硬件了，所以他的估算明显比以前准很多。而且他更新时间线的频率也更高了，有时候甚至每天都在变。他会跟我们不断沟通，根据不同的参数来调整进度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有些变化甚至是他那边直接带来的，尤其是在基础设施层面。比如某个交易提前敲定了，或者某批设备可以提前排进生产，那就可能直接省下一个月、两个月，甚至更多，具体要看部署的情况。软件这边其实也是一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他一直说的一句话是：你完全可以试着用一个月去做一件原本要一年才能做完的事，最后你可能两个月就搞定了，但那也已经快得多了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得在 SpaceX 的早期，有一种内部共识：马斯克说每拖延一天，就相当于损失一千万美元的收入。我不知道在 xAI 是什么感觉，你心里会不会也有一种直觉：如果今天没有再 push 一点、没有把事情往前拱一步，就等于损失了多少本可以创造的价值？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有的。至少在Macrohard这个项目上，我们确实有一些非常明确的收入目标。具体数字我不能说，但在我脑子里，只要一件事被延迟或者被加速，我几乎立刻就能算出来：我们刚刚是多赚了多少钱，或者少赚了多少钱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这也太夸张了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，数字会非常大。一方面是因为预期回报本身就极高，另一方面是时间线实在太短了。所以哪怕只是几天的变化，按比例来看，对收入的影响都已经非常可观了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：马斯克一直以“快速下重注”闻名。有没有那种在一次会议里，就做出了投入巨大资本、时间或者承诺的决定？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有一个非常典型的决定，就是在Macrohard 上，我们选择了一条路线：模型的速度至少要比人类快 1.5 倍，而现在看起来，实际速度远远不止如此。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在其他实验室，类似“人类模拟器”的尝试，更多是走“更强推理能力、更大的模型”这条路。但我们当时的这个决定，几乎是完全走在了和所有人相反的方向上。之后我们做的几乎所有事情，基本都是这个决定的下游结果。虽然不能说百分之百，但它影响了绝大多数事情，而且这个决定是在非常早期就定下来的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这在某种程度上也是一种共识，尤其是类比完全自动驾驶就很容易理解。没有人会等电脑花十分钟去做一件自己五分钟就能做完的事。但如果电脑十秒就能搞定，那我愿意为此付出任何价格。这其实是个非常直观的判断。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正常情况下，我们这些工程师可能会站出来反对，有二十个理由说明事情不能这么做。但当一个决定已经被拍板了，你只能从结果倒推路径，办法总会有的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;没有AI研究员， 就是工程师&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得马斯克之前说过一次，好像是在 YC的活动上，他和 Gary Tan 做问答。Gary 提到 AI 研究员这件事，结果马斯克说不存在什么 AI 研究员了，现在全都是 AI 工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，我们跟他开过一次关于招聘的会，也有人提到过类似的话题，比如岗位描述之类的。然后他大概讲了十分钟，核心就一句话：工程师，就是工程师，别的都不重要。只要是好工程师，本质上是个会解决问题的人就行。不管你以前是做哪一块的，用过什么架构、做过哪种基础设施，这些都不重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：为什么“工程师”这么重要？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：因为这样边界就被拉得很宽。意味着我们可以从很多不同背景的人里招人，现实中也确实是这样。AI 领域可能还不算特别明显，但 SpaceX 有很多这样的故事：有人来自你完全想不到的背景，按传统眼光根本不可能进来，但最后却在工程上做成了非常大的事情。所以定义宽一点，就等于给这些人留了一条路，也能帮助我们整体跑得更快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“没人指挥你干这个、干那个”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那对你个人来说，在那工作最有意思的地方是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没人管我。真的，没人指挥你干这个、干那个。如果我有个好想法，通常当天就能自己动手把它做出来，然后拿去展示。看看合不合理，跑个评估，或者直接给客户看，给马斯克看，给相关的人看，一般当天就能知道这个方向对不对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没有冗长的讨论，也不用等各种流程和官僚审批，我特别喜欢这一点。说实话，我从非常小的创业公司来更大的公司，本以为会牺牲一些自由度。我加入时公司 100 人,是我之前公司的 10 倍。但对马斯克的公司来说算小的，确实感觉很小的公司，没有什么繁文缛节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你进去之前，有没有什么特别大的预期，结果后来发现完全不是那么回事的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我原来以为会更“自上而下”一些，结果发现有一些，但不多。管理层级非常少，基本就三层：最底下是 IC，中间是联合创始人和一些新晋的经理，再往上就是马斯克，没有了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在每个经理下面的人都很多，事情反而很少是自上而下推动的。通常是我们自己先想出解决方案，跟经理对一下，马斯克点头，就直接干了。有反馈就再调整。整体比我想象中要“自下而上”得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：感觉就是在刻意设计一种状态，让所有人都在做东西，管理者更少，真正的“建造者”更多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。我刚加入的时候，几乎所有经理都还在写代码。现在有些人下面管着上百号人，写得少了一点，但总体上，大家还是工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我记得第一周，有天吃晚饭，一个人坐我旁边。我就随口问他在哪个团队。他说他是做销售的，主要负责企业客户。我当时还想，“哦，原来是销售。”结果，他接着跟我讲他最近在训练的模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;没错，销售也是工程师。销售团队全是工程师，几乎每个人都是工程师。那会儿公司里，可能真正不算工程师的人不到八个。即便如此，大家也都是在为同一台“机器”做贡献。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以是不是更像这样：一个工程师负责一个项目，可以直接面对客户，理解他们的问题，然后快速实现解决方案？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，而且层级越少，信息损失就越小。本质上是信息压缩的问题。语言本身就是有损的。如果信息要从客户脑子里变成语言，再进销售脑子，再变成语言、再到经理、再到工程师，每过一层，就像传话游戏一样丢一大截。如果你能尽量减少层级，那就只剩下一次压缩：客户直接告诉你他们要什么、体验是什么，然后工程师直接去解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有没有什么你以前在别的公司从没见过，但 xAI 在做的事情，能让事情推进得特别快？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：最让我意外的是团队之间、职责之间的“模糊性”。这在其他大公司，甚至规模差不多的公司里，都很少见。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;比如我要修虚拟机基础设施的一个问题，我就直接修，修完给负责那块的人看一眼，对方说 OK，马上合并、上线。几乎没有那种严格的边界，大家基本都可以改任何东西。当然，危险的操作还是有检查的，但总体上，公司是信任你的，默认你会把事情做对。这种感觉真的很不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我记得之前马斯克在搞 DOGE 的时候，删掉了一些防控措施然后又很快加回来了。在这种高速试错的过程中，有没有什么东西被删掉、又重新做回来的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：几乎没有那种不可逆的破坏。我想不起来有什么东西是真的被永久性毁掉的。但像你说的，删掉、移除某个东西，然后有人说“我需要这个”，这种情况非常常见。可能一个小时后就回滚了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也有那种情况，一个项目做了好几个月，依赖某块基础设施，结果等你真要上线的时候，那块基础设施已经被重构过三次了。那就再适配一次，继续往前走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你觉得工程团队人这么少是件好事吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：绝对是。人越多，反而越慢。一个人能做完的事，两个人来做，往往要花两倍时间，这在任何规模下都成立。尤其是现在，你已经不需要像以前那样写那么多代码了，更多是在做决策、做架构设计。每个人都可以是架构师，不需要那么多“手”，一个大脑能做的事情多得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前自己也尝试过创业，做过很多不同的项目。是什么让你决定来这里？使命感也好，文化也好，哪一点真正打动了你？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话，我一直是马斯克的粉丝。小时候第一次看到猎鹰火箭回收着陆，那种震撼真的忘不了。我后来还专门跑去看了 星舰的第五次发射，那次是第一次成功“接住”，真的值回票价，是我这辈子见过最酷的事情。所以只要能参与任何跟这些事情沾点边的东西，对我来说就已经非常有吸引力了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你当初为什么选择这家公司，而不是 SpaceX 或特斯拉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：主要还是因为我骨子里就是个创业者吧。xAI 是这几家公司里规模最小、也最新的一家。我当时的一个判断，就是在这种体量的公司里，个人能产生的杠杆和改变会最大，事实也基本验证了这一点。因为从比例上看，你在公司里的“占比”更大。不是说其他公司不酷、或者个人不重要，而是这种比例带来的影响力不一样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：也就是说，对决策产生影响的可能性要大得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：甚至不只是决策，而是从想法到落地、到看到结果，速度都非常快。我之前以为很多事情自己单干会更快，比如自己做某个功能、跑某个实验。但现实是，在 xAI 反而更快，因为已经有现成的基础设施和团队，很多我本来要手动完成的步骤，他们早就做过了，而且基本没人会对你说“不”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;内部AI虚拟员工&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你之前提到，公司里不同人、不同事情之间的边界其实挺模糊的。那你能不能随时去找其他同事帮忙？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：经常啊。基本就是走到别人桌前，直接说：“我有个问题。你现在在做什么？我能不能帮你一点？你能不能帮我这个？”大家都在同一栋楼里，这种事非常自然。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;挺有意思的是，我们后来在公司内部测试“虚拟员工”（human emulator），有时候甚至没提前告诉大家，所以就会出现这种情况：有个真人员工在干活，突然有人找他说“你能不能帮我做这个”，虚拟员工就回：“行啊，来我工位吧。”结果那人真的走过去，发现什么都没有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;好几次我收到消息说：“组织架构里这个人向你汇报，他今天是不是没来？”但其实他是个 AI，是虚拟员工。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过整体来说，大家默认都是在同一栋楼、随时能联系到的。所以互相求助这件事非常频繁。我可以找别人帮忙，别人也经常来找我。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那在这些过程中，最容易“翻车”或者最让你意外的点是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：主要发生在“人类行为模拟”这块，尤其是和客户一起做的时候。我们会尽量全面地理解客户的工作内容：先聊天、访谈，让他们讲，或者写下来他们是怎么做这份工作的。再过一周，我们回头看虚拟员工犯的错误，发现它总是在某些特定场景出问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候我们就去观察真人是怎么做的，结果发现真实流程里其实有二、三十个步骤，对方之前完全没提。我们一问，他们就说：“哦对，这一步我们是这么做的，刚才忘了说，不好意思。”这种情况太常见了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多事情在人脑里是默认存在的，全靠“自动驾驶模式”在跑。就像你开车开了一小时，完全不记得自己刚才是怎么开的。人类对任何重复性的工作都是这样，而我们想解决的正是这些问题：把人类现在反复做、其实根本不需要人来做的“蠢活”，全部替掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那你是怎么决定“先解决哪一类问题”的？除了开车以外，人类还有哪些事情是天天在做、但其实没必要继续做的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：只要是电脑上的重复性工作，基本都在这个范围内。比如客服就是一个特别典型的场景：不断接收各种格式、各种内容的用户输入，然后把它们转化成一个标准化的处理流程。这样人类就可以去做更有创造性、更需要大脑的事情。&lt;/p&gt;&lt;p&gt;这&lt;/p&gt;&lt;p&gt;和编程领域发生的变化几乎是完全平行的：以前你要把同样的实现写二十遍，现在你用三句话描述一下，它就帮你搞定了，这是一次巨大的“压缩”。我们做的，其实就是把这种“压缩”，应用到所有数字化工作流上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在公司内部推这些“虚拟员工”的时候，除了“人不存在但被叫去工位”这种情况，还有什么让你觉得意外的吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：意外的一点是，它的泛化能力比我们预期的强很多。有很多测试案例，模型根本没针对这个任务训练过，但表现却非常完美，远远超出我们的预期。因此，可以很确定地说，泛化效果真的比想象中好，而且我们现在还处在非常早期的阶段，之后只会越来越强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这点其实和完全自动驾驶很像：有些场景并不在训练数据里，但车就是能正确应对。这本质上是一个“权重效率”的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;马斯克给反馈，要么宏观、要么细节&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你参加过几次和马斯克的会议？那种会议一般是什么样的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：说实话都挺简单的，而且我运气不错，大多数都进行得很顺利。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 SpaceX 这种地方，成本和零部件细节特别重要。但在你们这里，他给反馈时会不会不太一样？比如不会去抠每个流程的细节？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：他的反馈通常要么非常宏观，要么非常微观，很少停在中间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;宏观层面上，可能是产品方向、客户判断，比如“只专注这个细分市场”“这件事完全不要做”。微观层面，尤其是算力效率、延迟这些问题，他往往会给出非常具体的建议，比如“试试这个方案”。而且他是愿意被证明错的，但前提是要有证据，必须做实验、看结果，而不是靠观点对喷。有些实验的结果甚至会出乎所有人的意料，然后我们就顺着那个方向继续走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：所以你们后来选择小模型，而不是一味堆大模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，小模型在算力效率上的选择，带来了很多改进。有些是直接的，有些是间接的。最直观的当然是响应更快。但更重要的是，特斯拉在自动驾驶上也发现了同样的事：模型小了，迭代速度就快得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不仅模型对环境反应更快，部署新版本的速度也快了。以前可能四周一次，现在一周一次。这又反过来影响了实验方式：为什么我们能同时跑二十个实验，其实就是源于这个早期决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：那一开始的设想，是不是想直接上大模型？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：算是吧。我们确实想比所有人都快，但后来发现，“快”这件事的效果，被放大了很多倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“war room”真实存在&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：维基百科一直被诟病有偏见，马斯克也很关注构建一个“更接近真实”的替代体系。那你们怎么看待清理互联网来找到真相这件事?&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这是个极其困难的问题，因为互联网本身往往并不是所谓的“事实真相”。我们能做的，是尽可能往“底层原理”去钻，但这本身也很难。比如你问“宪法在物理意义上的底层原理是什么”，这其实很难有人真正给出一个严谨的答案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但思路是类似的：尽量往下挖，再从那里往上构建。问题是真正这样写、这样做的资料并不多。比较接近的一个例子，是 James Burke 的《Connections》系列，他会把看似完全不相关的概念，通过物理和发明串联起来，非常有意思。我们想做的，其实是类似的事情，只不过这条路还很新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们是怎么找到更好的数据的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：数据并不是决定结果的唯一因素。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我有时候会在 X 上看到有人贴出 Grok 的输出，说“这明显不对”，然后马斯克直接回复说“我们会修”，接着可能过了十二个小时、一天，他又说“好了，已经修好了”。这种事情发生时，内部一般是怎么运作的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：通常是他把哪里出问题了直接指给我们看，然后当时还醒着的人就会马上拉一个线程开始解决问题，一般先是个人处理，如果需要就再拉几个人。之后我们会做一次复盘，把到底哪里出了问题、以后怎么避免都讲清楚。原则上，犯一次错是可以接受的，但同样的错误犯第二次就很严重了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在 SpaceX 的历史里，包括特斯拉，其实有过很多这种“冲刺时刻”。比如马斯克半夜突然出现，发一封全公司邮件，说大家都来公司干活。你们也有这种情况吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：这种更多发生在做大模型的时候。就 Macrohard 这个项目来说，我们已经在“作战室”里连续干了四个月了，基本一直就是这种状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：你们门口是不是还真挂着一块牌子写着“war room”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，真的。最早那个作战室后来扩张了，我们就把东西全搬走了。有一次马斯克走进作战室，发现里面空无一人，就问“人呢？怎么回事？”然后他又走到我们现在待的地方，其实就是健身房，我们把健身器材全清掉，把人都塞进来了——然后他就在那儿开始一连串追问到底发生了什么。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在那种很多事情被打乱又被迅速推进的夜晚，或者经历那种大规模冲刺时，是什么感觉？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我最近正好看到 xAI 的一位联合创始人 Igor 发的一条内容。他人特别好，我也很喜欢跟他一起工作。他以前在StarCraft AI 工作，大概十年前吧，是我高中时尝试复现过的最酷的机器学习项目之一，难得要命，所以后来能和他一起共事真的挺神奇的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他说的一句话我特别有共鸣：有些时间里，感觉只过去了几天；但有些夜晚里，仿佛发生了几个月的事情。那天晚上就是这样。说“几个月”可能有点夸张，技术结果我们本来也可能几周内做到，但一晚上把它搞出来，冲击感非常大，而且真的熬了一个通宵。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：有没有那种情况，大家连续五天、甚至一整周都没怎么离开过办公室？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：有的。模型冲刺的时候，经常会有很多人直接在公司过夜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：之前提到你们有五、六个睡眠舱，大家轮着用？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，有睡眠舱，现在还有一些上下铺，条件差点，但至少能睡。后来帐篷那张照片传出来后，很多人都发给我。我只能说确实有帐篷，但我从没见过一次搭那么多。反正……确实挺极端的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;成长经历：从小不服权威&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我知道你小时候做过很多不同的项目，好像还做过指尖陀螺。可能是在你房间里搞的？这种折腾、动手的心态，对你现在的工作影响大吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：影响挺大的。我很小就开始学编程，大概十一岁的时候，我爸给我买了一本书。我一开始觉得还行，但真正开始喜欢是在我意识到它能赚钱之后。我在网上认识了一些人，他们给游戏写脚本、外挂，然后卖一点钱。对我来说，能在网上赚到几百美元已经是天大的事了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：第一次有人给你钱，那种感觉真的很奇怪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：太疯狂了。我还记得当时得让我爸帮我弄一个 PayPal 的托管账户之类的，然后钱真的打进来了。对我来说，那简直是世界上最酷的事情。我干了几个月，攒了点钱，当时我对3D 打印特别着迷，RepRap 那套体系正火。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那其实就是一群大学生搞的项目，目标是造一台能打印出自己大部分零件的机器，所以才叫 RepRap。他们在不同大学里搞了一些实验室，从一台打印机开始，让它打印下一台的零件，一步步扩展。当然，这里面问题很多，他们也一直在解决，但那确实推动了后来的 3D 打印浪潮。我当时特别痴迷，就照着他们的零件清单，在阿里巴巴上把东西全买齐了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：然后呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：一个月后东西到齐了，我一晚上把它装起来，但过程其实挺惨的。我在拆电源的铜线，那是个非常不靠谱的电源，结果真的着火了。铜线全散开，有一根直接扎进我拇指里，大概有五厘米深。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：去医院了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：没有。那是个上学的夜晚，已经凌晨了。我十三岁，动手能力也不行，在卫生间用镊子折腾了一个小时也没拔出来，最后我干脆把露在外面的剪掉了。接下来几周，它一点点往外长，我每天早上再剪一点。现在想想还挺离谱的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过打印机最后还是装好了。那时候正好赶上指尖陀螺爆火。我从中国买了一千个滑板轴承，在自己卧室里搞了个小工厂。晚上每隔两个小时起来一次清理打印平台，重新打印一批陀螺。白天上学前，我在车库里装轴承、喷漆、晾干，然后跑去其他学校的公交站，把货卖给“分销商”，其实就是别的学校的学生。他们白天卖，我放学后收钱，线上也卖、发货。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;生意做了两个月，最后被叫停了。官方理由是，学校餐饮公司有独家销售权，不能在校园里卖东西。但我觉得，他们主要是不爽我一边分散大家注意力，一边还赚钱。这事让我学到了一种“健康的不服从权威”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这种对权威保持距离，好像一直贯穿你的经历。你提到你不太信任机构，这种态度是怎么形成的？在你的人生里具体体现在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：我从很小就知道，我想要的是一种不寻常的结果，而走一条常规路径，基本不可能得到。于是我本能地抗拒一切“惯例”，而机构的本质就是维护惯例。我觉得，几乎所有真正有创造力、有意思的成果，都是来自自由的人。至少在我看到的世界里是这样。所以，忠于这一点，对我来说才是正确的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我很喜欢 John Carlson 的一个观点：所有东西都这么难造、难实现。看看周围，世界就是充满人们的激情项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对，完全就是个奇迹。每一样东西背后都有故事，比你想象的要多得多。我记得以前读过 YKK 拉链的故事。你会发现，全世界真正做得好的拉链厂商就两、三家。拉链看起来很便宜，但机械结构其实挺复杂的。之所以能这么便宜、这么可靠，是因为有极少数公司、甚至可以说是极少数人，花了几十年把这件事做到极致。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这几乎适用于所有东西。任何特别具体、又能大规模生产的东西，背后通常只有几家公司、甚至几个人在做。就像有时候你会听说，德国某个不起眼的小公司一停产，大众汽车整条产线都得停。疫情期间这种事就更明显了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：在我们见面之前，你还做了一个液体燃料火箭发动机，我记得很小一个，你说是临时起意，二十四小时内点火的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：整个项目其实前后做了大概四周。一开始我就是买了一堆教材，研究火箭发动机的设计原理。和软件完全不一样，软件你可以上 GitHub 看别人的代码，但火箭没有现成文件。你得搞清楚材料特性、化学性质、怎么加工、参数怎么定，推力怎么估算，怎么避免超压。还有喷注器的设计，这个特别难，大概占了一半时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：这是最难的部分吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：是的，喷注器最难，也是最后问题最大的地方。我花了三、四周时间，找中国工厂加急做了很多零件。那时候正好感恩节，我准备飞回东海岸看家人。我当时想，要么今晚把它装好、点火，要么就拖两周，然后我决定不能拖，就现在干。我早上灌了很多咖啡，一整天都在干活，搭测试架、装发动机，当晚就点火了。当然，为了能当晚完成，做了不少妥协。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;主持人：我真的觉得特别好笑，你当时离它其实就几步远？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Sulaiman：对。我其实设计了远程点火，但问题是，用来给板载计算机供电的电源还没到，只能用笔记本通过 USB 供电。而我最长的 USB 线只有一米多，所以我只能站在旁边点火。我心里估计，大概有三成概率它会炸，或者喷得到处都是火。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;视频里其实能看到，我的外套着火了。因为喷注器设计不好，产生了很多超压，没完全燃烧的乙醇直接喷出来，溅到我身上就点着了。那件烧焦的外套现在还留着，当纪念品了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8jN60eJr4Ps&amp;amp;t=41s&quot;&gt;https://www.youtube.com/watch?v=8jN60eJr4Ps&amp;amp;t=41s&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0ds6wQgBpN7W2N6EL1iM</link><guid isPermaLink="false">https://www.infoq.cn/article/0ds6wQgBpN7W2N6EL1iM</guid><pubDate>Thu, 22 Jan 2026 10:35:34 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Salesforce将1,000多个EKS集群迁移到Karpenter，以提高扩缩速度和效率</title><description>&lt;p&gt;&lt;a href=&quot;https://www.salesforce.com/eu/?ir=1&quot;&gt;Salesforce&lt;/a&gt;&quot;已完成对1000多个Amazon &lt;a href=&quot;https://aws.amazon.com/eks/&quot;&gt;Elastic Kubernetes Service&lt;/a&gt;&quot;（EKS）集群从Kubernetes Cluster Autoscaler到&lt;a href=&quot;https://karpenter.sh/&quot;&gt;Karpenter&lt;/a&gt;&quot;的分阶段性迁移，Karpenter是AWS的开源节点配置和自动伸缩解决方案。这次大规模转型旨在减少扩展延迟，简化操作，降低成本，并为公司广泛的Kubernetes团队内部开发人员提供更灵活自助的基础设施。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;面对基于自动伸缩组的自动伸（Auto Scaling）和集群自动伸缩（Cluster Autoscaler）的限制，包括扩展速度慢、跨可用区利用率低以及成千上万的节点组的激增，Salesforce的平台团队构建了自定义工具来安全、可靠地自动化和管理迁移。这种方法结合了精心编排的节点转换和自动化，尊重Pod中断预算（&lt;a href=&quot;https://www.cloudbolt.io/kubernetes-pod-scheduling/pod-disruption-budgets/&quot;&gt;Pod Disruption Budgets&lt;/a&gt;&quot;，PDBs），支持回滚路径，并与公司的CI/CD配置管道集成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移之旅始于2025年中期的低风险环境，并在2026年初投入生产之前经历了测试和验证阶段。Salesforce的工程师开发了一种内部Karpenter转换工具和补丁检查，可以处理节点轮换、亚马逊机器镜像（&lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html&quot;&gt;Amazon Machine Image&lt;/a&gt;&quot;，AMI）验证和优雅的Pod驱逐，从而实现了跨不同节点池配置的可重复和一致的转换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过这次转型，团队解决了操作上的挑战，例如配置错误的PDBs阻止了节点替换、&lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;&quot;标签长度限制导致自动化失败，以及Karpenter高效打包需要调整以防止单副本应用程序中断的工作负载模式。这些见解导致了改进的实践，包括主动策略验证和工作负载感知中断策略。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce报告了迁移后可衡量的操作和成本改善。通过采用Karpenter的动态配置模型，集群扩展延迟从分钟减少到几秒，通过更智能的打包提高了节点利用率，并显著减少了对静态自动伸缩组的依赖。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于自动化流程取代了手动节点组管理，使开发人员可以自己声明节点池配置，操作开销减少了大约80%。这种加速的采用减少了对中央平台团队的依赖。此外，初步结果显示，2026财年成本节省了约5%，预计随着Karpenter的打包和现货实例利用率继续优化资源，预计2027财年将进一步减少5-10%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce的迁移突出了大规模Kubernetes操作中的更广泛趋势，其中传统的自动伸缩机制难以跟上动态工作负载和异构基础设施需求的步伐。Karpenter的实时决策、对异构实例类型的支持（包括GPU和ARM）以及与云API的更紧密集成，与集群自动伸缩器相比，实现了更快的响应和更高效的节点使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其他从传统的Kubernetes自动伸缩向Karpenter等更动态的解决方案进行大规模过渡的组织面临了许多与Salesforce记录的相同结构性挑战。例如，&lt;a href=&quot;https://reinvent.awsevents.com/content/dam/reinvent/2024/slides/pro/PRO303_Scaling-to-new-heights-Coinbase-migrates-to-Amazon-EKS-and-scales-10x.pdf&quot;&gt;Coinbase&lt;/a&gt;&quot;公开描述了其向Karpenter的转变，以处理具有变化需求模式的复杂混合工作负载集群，提到了扩展延迟和资源效率的改善，同时减少了静态节点组引起的操作摩擦。同样，&lt;a href=&quot;https://aws.amazon.com/blogs/industries/transforming-the-bmw-connected-vehicle-backend-with-karpenter/&quot;&gt;宝马集团&lt;/a&gt;&quot;分享了在其汽车平台上采用Karpenter如何更好地利用现货实例和工作负载感知调度，实现了更快的开发人员反馈循环和降低基础设施成本波动。这些案例呼应了Salesforce的观察，即集群自动伸缩器依赖于预定义的自动伸缩组和较慢的决策路径，可能会阻碍具有多样化和突发工作负载的环境的响应能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salesforce迁移的不同之处在于其规模和自动化工具：转换超过1000个不同的EKS集群需要定制工具来处理策略验证、Pod中断预算限制、Kubernetes标签限制和舰队级别的增量推出自动化。其他公司报告了在个别集群或较小舰队中从Karpenter中的获益，但Salesforce的方法强调了在企业规模上可重复、自动化的转换，集成了回滚和合规性保障。在实践中，这意味着不仅要替换自动伸缩逻辑，还要协调工作负载模式、治理控制和全球平台上开发人员的自助服务期望。虽然这些迁移的最终目标是更快的扩展、更好的利用率和减少手动开销，但Salesforce的蓝图突出了将这些好处带给大型、生产关键环境所需的操作规程和自定义自动化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着企业越来越多地采用Kubernetes来支持关键任务服务，Salesforce的经验为其他考虑类似转型的组织提供了一个蓝图，证明了自动化、联合自动扩展可以带来性能、成本效率和开发者速度的显著提升——前提是必须有周密的规划和工具支持来支撑这一变化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/salesforce-eks-karpenter/&quot;&gt;https://www.infoq.com/news/2026/01/salesforce-eks-karpenter/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/MJlz0Dv7QQPqf782oCPJ</link><guid isPermaLink="false">https://www.infoq.cn/article/MJlz0Dv7QQPqf782oCPJ</guid><pubDate>Thu, 22 Jan 2026 08:02:00 GMT</pubDate><author>作者：Craig Risi</author><category>云计算</category><category>大数据</category></item><item><title>每周工作100小时！谷歌DeepMind CEO揭秘：中国对手是字节跳动，断言谷歌是AI领域唯一全栈巨头</title><description>&lt;p&gt;“没有，从来都没有安心的时候。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 2026 年达沃斯世界经济论坛，DeepMind 创始人、Google DeepMind CEO 德米斯·哈萨比斯，用这句话形容过去三到四年的谷歌。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;外界一度流行的“谷歌慢半拍”的言论，在他看来是一个彻底的误解。事实上，在这段时间里，谷歌的 AI 团队几乎一直处于红色警报状态。他本人长期保持着每周 100 小时、一年 50 周的工作强度，把一家万亿美元体量的科技巨头，硬生生拉回到创业公司的战时节奏。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0f/0fc6727cdbb957df06f1b507fb492a99.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这样的状态下，谷歌迎来了 Gemini 3 的发布，被哈萨比斯视为“重回行业最前沿”的关键节点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在接受彭博社记者 Emily Chang 的专访时，他罕见地系统性拆解了当下几乎所有 AI 世界的核心争议：&lt;/p&gt;&lt;p&gt;谷歌是否真的掉队？中国 AI 是否构成威胁？Transformer 和大模型是否已经走到尽头？AGI 会在什么时候到来？当工作不再必要，人类该如何寻找意义。&lt;/p&gt;&lt;p&gt;在哈萨比斯看来，过去十年，现代人工智能产业所依赖的关键突破，比如 Transformer 架构、深度强化学习、AlphaGo 背后的技术体系，几乎都诞生于谷歌与 DeepMind。他高度赞扬谷歌深厚的技术积累，他认为&amp;nbsp;谷歌是唯一真正具备 AI 全栈能力的公司，其真正的问题在于能否把研究、算力、数据、硬件和产品，整合成一个统一体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他还高度赞扬了谷歌的科学研究氛围，认为这正是他当初选择谷歌作为 Google DeepMind 归宿的原因。他还透露了他与拉里・佩奇、谢尔盖・布林如何高效分工。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在访谈中，哈萨比斯还反复提到一个关键词：物理 AI（Physical AI），他承认物理 AI 确实正处于突破的临界点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他的设想中，Gemini 从一开始就不是“聊天模型”，而是一个理解现实世界的多模态系统，是通往物理 AI 的入口。未来 Gemini 只会走向两个方向：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随身的通用 AI 助手（眼镜、手机）真正能干活的机器人&lt;/p&gt;&lt;p&gt;当然，他也给出了冷静判断，距离物理 AI 跨过临界点还有&amp;nbsp;18 个月到两年的时间，在算法、数据、硬件等方面，都还差最后一段路。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;谈到中国 AI，哈萨比斯的态度异常冷静。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他并不认为 DeepSeek 构成真正意义上的“危机”，也直言西方舆论夸大了其算力效率优势，这背后仍依赖西方模型蒸馏。在他看来，中国公司极其擅长追赶，但是否能率先打开下一代技术前沿，仍有待时间验证。而&amp;nbsp;现代人工智能行业所依赖的约 90% 的突破性技术，都是谷歌研发的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但他特别表扬了&amp;nbsp;字节跳动，给出了一个极具分量的评价：字节跳动距离技术前沿，大约只差 6 个月，而不是 1–2 年。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这位把 AGI 当作毕生使命的科学家型 CEO，几乎反驳了 马斯克、杨立昆和伊利亚·苏茨克维的核心判断，同时给出了一个异常冷静 AGI 的时间表：2030 年，有 50% 的概率实现通用人工智能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯对 AGI 有自己一套严格的标准，即必须具备完整的人类认知能力，尤其是科学创新能力，不仅能解决问题，还要能提出真正重要的问题&amp;nbsp;。&amp;nbsp;这其中还有不小的差距。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他认为距离 AGI，还需要一两项，最多不超过五项突破性技术，这可能体现在世界模型、持续学习的能力、稳定性表现、更强的推理能力或更长远的规划能力等方面。他高度认可现有的模型成就，认为在现有方法的基础上进行优化并扩大规模，或许就能实现 AGI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在访谈的最后，话题不可避免地走向未来社会：人工智能是否会取代人类的工作？围绕这一问题，哈萨比斯提出了一个有趣的概念&amp;nbsp;“后稀缺时代”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在他看来，AI 带来的变革，无论规模还是速度，都会是工业革命的十倍，取代部分人类工作几乎是不可避免的结果。但他厘清一个概念，即人工智能本质上是一种终极的科学研究工具，就像更先进的望远镜和显微镜一样，是为科学服务的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在哈萨比斯的设想中，真正重要的并不是“谁被取代”，而是人类将因此获得前所未有的自由，把注意力转向那些更根本的问题。例如能源危机，如何实现核聚变，如何发现全新的材料体系。这些长期困扰人类的难题，或许正是在人工智能的加持下，才第一次显露出被彻底解决的可能性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅是一场技术竞赛，更是一场文明级实验。真正的风险，在于当人类不再需要通过工作来定义自身价值时，我们是否已经准备好回答那个更深层的问题“为什么而活？”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在那个时代，人类或许需要的不只是更强的工程师，而是伟大的哲学家，去重新书写意义的来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是哈萨比斯访谈实录，更多的谈话细节，欢迎来看：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;谷歌的红色警报期与“王者归来”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：和你上次来达沃斯相比，今年的感受有什么不同吗？Gemini 3 已经发布了，相关的消息我们也都听说了。我在内部甚至把这段时间称作“红色警报”。你觉得谷歌已经找回曾经的状态了吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不太确定这是不是该由我来评价，但我确实认为，过去这一年我们做得非常出色。我们付出了极其艰苦的努力，几乎是全力以赴，才让我们的技术和模型重新回到行业最前沿。&lt;/p&gt;&lt;p&gt;尤其是 Gemini 3，以及我们在视觉和成像系统方面取得的一些关键突破，都在这一过程中起到了决定性作用。同时，我们也逐渐适应了如今这种节奏极快、需要迅速将成果推向市场的行业环境，让整个团队重新焕发出一种更接近初创公司的活力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你认为人们是否低估了谷歌，或是对谷歌有误解？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：或许是吧，我不确定。我的意思是，我们一直都拥有站在这个领域前沿的所有必备条件，显然我们在这方面有着悠久的积淀。&lt;/p&gt;&lt;p&gt;我认为在过去十年里，谷歌和 Google DeepMind（谷歌深度思维）联手，创造出了现代人工智能行业所依赖的大部分突破性技术。比如 Transformer 架构，还有最知名的阿尔法狗背后的深度强化学习技术，这些都是我们的成果。&lt;/p&gt;&lt;p&gt;我们还有覆盖数十亿用户的优质产品矩阵，从搜索引擎、电子邮箱到谷歌浏览器，这些产品天生就适合融入人工智能技术。&lt;/p&gt;&lt;p&gt;问题只是如何将所有这些资源整合起来，以正确的方式统筹规划。&amp;nbsp;过去几年我们已经做到了这一点，当然还有大量工作要做，但我们已经开始看到努力带来的成果了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果你认为谷歌具备优势，你觉得这个优势有多大？能持续多久？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：在我看来，一切都始于研究。尤其是模型，要在各类基准测试中都保持行业前沿水平。这也是我们整合谷歌和 Google DeepMind（谷歌深度思维）后，首要聚焦的工作。双子座系列模型的进展，我们感到非常满意，当然这方面还有很多工作要推进。&lt;/p&gt;&lt;p&gt;但我认为，我们是唯一一家拥有全栈能力的机构，从技术、战术、流程体系，到硬件、数据中心、云业务、前沿实验室，再到一众天生适配人工智能的优质产品，我们一应俱全。&lt;/p&gt;&lt;p&gt;所以从根本的结构层面来说，我们本就该有出色的表现，而且我认为我们未来还有很大的提升空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我想知道，作为前沿模型研发的负责人，日常工作状态是怎样的。我看到有报道说，你大多在凌晨一点到四点进行深度思考。确实是这样吧？谷歌内部的工作状态是否一直处于红色警报级别？你有没有感到安心的时候？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：没有，从来都没有安心的时候。我们设定红色警报级别，本是针对特殊情况的，但过去三四年，工作强度一直大到难以想象。每周工作一百小时，一年工作五十周，这已经是常态。&lt;/p&gt;&lt;p&gt;在这个技术发展速度极快的领域，要想保持前沿，就必须这样做。行业的竞争异常激烈，可能是科技领域有史以来最白热化的阶段，而且背后的利害关系重大。通用人工智能的研发，无论从商业还是科学角度，都有着深远的意义。&lt;/p&gt;&lt;p&gt;再加上我们正做的事情本身就令人振奋，而我的热情就是用人工智能探索科学难题，推动科学发现的进程。这是我一直以来的梦想，我毕生都在为人工智能发展的这一刻而努力。所以常常会因为有太多工作要做而难以入眠，但同时，也有太多令人兴奋的事情值得我们去探索、去推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：聊聊谷歌目前的内部文化吧，你们既要在这场竞争中取胜，又要保证研发的方向正确。拉里・佩奇和谢尔盖・布林 现在的参与度如何？你和他们沟通的频率高吗？他们现阶段的工作重点是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：他们的参与度非常高。&lt;/p&gt;&lt;p&gt;拉里・佩奇更多负责战略层面的工作，我会在董事会会议上见到他，去硅谷时也会和他碰面。&lt;/p&gt;&lt;p&gt;谢尔盖・布林则更多参与具体工作，他甚至会亲自参与双子座研发团队的编码工作，尤其专注于算法细节方面。&lt;/p&gt;&lt;p&gt;他们能对当下的人工智能研发充满热情，这对我们来说是好事，毕竟这是计算机科学发展史上一个无比重要的时刻，单从科学角度来看，这也是人类历史上的重要时刻，所以所有人都想亲身参与其中，这一点非常好。&lt;/p&gt;&lt;p&gt;而对于我来说，我正努力融合各方优势，既保留初创企业快速推出产品、敢于冒险的活力，这一点我们已经看到了成效；又充分利用大企业的资源优势，同时还为长期研究和探索性研究保留空间，而非只聚焦于三个月内就能落地的产品相关研究，我认为只做短期研究是不明智的。&lt;/p&gt;&lt;p&gt;我正努力平衡这些因素，过去一年，各项工作的推进都很顺利，而且我认为今年我们能做得更好。我对目前的发展态势非常满意，谷歌的技术提升和研发进展速度，在业内应该是最快的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;物理 AI 的奇点时刻，还有 18 个月到两年的时间&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我知道你一直把重点放在推动科学进步上，比如发现新材料。我们也看到，现在 Gemini 已经被整合进人形机器人系统中。那么你觉得，人工智能在真实物理世界中的应用，是否即将迎来一个类似 AlphaFold 那样的突破性时刻？如果是的话，这个“突破”会以什么形式出现？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：是的，过去一年我花了大量时间深入研究机器人技术。我确实认为，我们正处在物理 AI 取得突破性进展的临界点。&lt;/p&gt;&lt;p&gt;但我还是觉得，距离实现这一突破，我们还有 18 个月到两年的时间，还需要开展更多研究。&lt;/p&gt;&lt;p&gt;不过我认为，双子座这样的基础模型，为我们指明了方向。从一开始，我们就将双子座设计为多模态模型，让它能够理解物理世界，背后有多重原因。&lt;/p&gt;&lt;p&gt;其一，是为了打造通用智能助手，这种助手或许会搭载在&amp;nbsp;智能眼镜或手机&amp;nbsp;上，能够理解周边的现实世界。&lt;/p&gt;&lt;p&gt;其二，当然就是为了应用在&amp;nbsp;机器人领域。那么人工智能在物理世界的突破性时刻，究竟会是怎样的？我认为，那就是让机器人能在现实世界中稳定地完成各类有实际价值的任务。&lt;/p&gt;&lt;p&gt;目前，仍有一些因素制约着这一目标的实现。&lt;/p&gt;&lt;p&gt;一方面，算法还不够完善，需要提升鲁棒性，而且相较于实验室中仅处理数字信息的模型，机器人相关算法能依托的数据量更少，合成这类数据的难度也远高于数字数据。&lt;/p&gt;&lt;p&gt;另一方面，硬件方面也仍有一些难题尚未解决，尤其是机械臂和机械手的研发。其实深入研究机器人技术后，你会对人类的手部结构产生全新的敬畏之心，至少我是这样。进化的设计精妙绝伦，人类的手在稳定性、力量和灵活性上的表现，很难被复刻。所以在我看来，要实现这一突破，还有不少环节需要完善，但目前已有很多令人振奋的进展。&lt;/p&gt;&lt;p&gt;我们刚刚宣布与&amp;nbsp;波士顿动力&amp;nbsp;展开深度合作，他们研发的机器人非常出色，我们正将人工智能技术应用到汽&amp;nbsp;车制造领域。&lt;/p&gt;&lt;p&gt;接下来一年，我们会先推出&amp;nbsp;原型机&amp;nbsp;进行测试，或许一两年后，我们就能展示一些令人印象深刻的成果，并实现规模化应用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;DeepSeek并不是重大危机，特别表扬字节跳动&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：一年前，DeepSeek 模型的发布在西方引发了不小的震动，很多人把它视为一场潜在的危机。但一年过去了，局势似乎逐渐平稳下来，中国方面的节奏看起来也有所放缓。你对中国人工智能领域整体竞争格局的看法，有没有发生变化？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：没有，其实并没有改变。一开始我就不认为这是一场真正意义上的危机，我觉得西方当时的反应多少有些过度了。&lt;/p&gt;&lt;p&gt;DeepSeek 的确是一个令人印象深刻的模型，它清楚地展现了中国科技公司的实力。&lt;/p&gt;&lt;p&gt;如果看头部企业，比如字节跳动，我认为他们的能力非常强。在技术前沿的跟进速度上，他们可能只落后大约六个月，而不是一到两年。DeepSeek 正是这一点的体现。&lt;/p&gt;&lt;p&gt;当然，围绕它的一些说法也被夸大了。比如关于&amp;nbsp;算力使用效率的说法，并不完全准确，因为他们在研发过程中借鉴并依托了部分西方模型，也对顶尖模型的输出结果进行了微调，而不是完全从零开始独立训练。&lt;/p&gt;&lt;p&gt;另外，还有一个关键问题目前仍然没有答案：那就是中国公司是否能够在跟进前沿的基础上，真正实现原创性的突破并引领下一代技术。&amp;nbsp;他们在追赶方面确实非常擅长，而且能力正在快速提升，但到目前为止，还没有证明自己能够率先打开新的技术前沿。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AGI 的时间表：2030 年，有 50% 的可能实现 AGI&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：是你为通用人工智能给出了定义，你也曾说过，到 2030 年，我们有 50% 的可能实现通用人工智能。&amp;nbsp;这个时间规划是否依然不变？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：不变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：通用人工智能对你而言，依然是一个有价值的研发目标吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为是的，这个时间规划在我看来很合理，而且相较于一些人的预期，这个时间其实更充裕。&lt;/p&gt;&lt;p&gt;但我对通用人工智能的评判标准非常高，它指的是一个具备人类所有认知能力的系统，显然我们目前离这个目标还有很大差距。&amp;nbsp;这意味着，这类系统需要拥有&amp;nbsp;科学创新能力，不仅能解决科学领域的猜想和难题，更要能率先提出研究假设和问题。&amp;nbsp;任何一名科学家都清楚，找到正确的问题，往往比找到答案难得多。&lt;/p&gt;&lt;p&gt;目前的人工智能系统显然还不具备这种能力，未来能否拥有，还未可知，我们也仍未明确实现这一能力需要哪些技术突破。比如&amp;nbsp;持续学习能力，也就是在线学习能力，让系统能突破训练的局限，在现实世界中自主学习；还有&amp;nbsp;稳定性，目前的系统在不同领域的表现参差不齐，而通用智能系统不该有这样的短板。在我看来，要打造通用人工智能系统，还有不少关键能力亟待突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：我们来聊聊技术和未来的发展趋势。Meta 首席科学家&amp;nbsp;杨立昆（Yann LeCun）&amp;nbsp;认为，仅凭 Transformer 架构和大模型，无法实现通用人工智能。你是否认同这一观点？如果这些技术走到了尽头，我们的研发方向会是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同，我认为说这些技术走到尽头的观点显然是错误的，因为它们目前已经展现出了巨大的实用价值。但在我看来，这是一个实证问题，也是一个科学问题，仅凭这些技术是否能实现通用人工智能，尚无定论。&lt;/p&gt;&lt;p&gt;我认为有 50% 的可能，只需在现有方法的基础上进行优化并扩大规模，就能实现通用人工智能，&amp;nbsp;这是有可能的，而且我们也必须这样做。在我看来，这项研究是有价值的，因为至少这些大模型会成为最终通用人工智能系统的核心组成部分，唯一的问题只是，它是否是唯一的组成部分。&lt;/p&gt;&lt;p&gt;我能想象，从现在到实现通用人工智能，我们还需要一两项，最多不超过五项突破性技术。&lt;/p&gt;&lt;p&gt;比如&amp;nbsp;世界模型，这是我一直提及的，我们也正在研发，目前我们的 GENI 系统就是最先进的世界模型（GENI 是 DeepMind 、Google 内部正在研发的一类世界模型（World Model）系统），我也直接参与了这项研发，我认为它至关重要。&lt;/p&gt;&lt;p&gt;还有&amp;nbsp;持续学习能力，以及打造&amp;nbsp;性能稳定的系统，让系统不再出现这种领域间的表现失衡，真正的通用智能系统，不该有这样的问题。&lt;/p&gt;&lt;p&gt;所以在我看来，人工智能还缺乏更强的&amp;nbsp;推理能力、更长远的规划能力&amp;nbsp;等多项关键能力。目前尚未确定的是，实现这些能力，是否需要新的架构或突破性技术，还是只需在现有基础上继续优化。而谷歌和 Google DeepMind（谷歌深度思维）的做法是，双管齐下，既全力研发新的技术，也持续优化并扩大现有技术的规模。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：OpenAI 联合创始人兼前首席科学家伊利亚・苏茨克维（Ilya Sutskever）认为，依靠扩大模型规模实现技术提升的时代即将结束。你是否认同这一观点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同。他的原话大概是 “我们重回研究的时代”，我和伊利亚・苏茨克维是很好的朋友，我们在很多问题上的看法都一致，但在这一点上，我并不认同。&lt;/p&gt;&lt;p&gt;我的观点是，我们从未离开过研究的时代，至少谷歌和 Google DeepMind（谷歌深度思维）一直如此。&amp;nbsp;我们始终在研发领域投入巨资，而且我认为，整合后的谷歌和 Google DeepMind（谷歌深度思维），拥有业内最深厚、最广泛的研发团队。&lt;/p&gt;&lt;p&gt;过去十年，现代人工智能行业所依赖的约 90% 的突破性技术，都是我们研发的，当然最知名的是 Transformer 架构，还有深度强化学习、阿尔法狗背后的各类强化学习技术，这些都是我们开创的。所以如果未来实现通用人工智能需要新的突破性技术，我相信，就像过去一样，我们依然会是这些技术的研发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：最后一个问题，埃隆・马斯克说我们已经进入了技术奇点，你是否认同？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：不认同，我认为这一说法为时过早。在我看来，技术奇点其实就是实现完全的通用人工智能，而我之前已经解释过，我们目前离这个目标还相去甚远。我相信我们最终能实现这一目标还有五年的时间，从实现通用人工智能的角度来看，其实并不长，但在那之前，我们还有大量的工作要做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;人工智能就像更先进的望远镜和显微镜&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你是诺贝尔奖得主，我知道你一心想让人工智能推动科学研究的发展。如果未来人工智能本身取得了足以获得诺贝尔奖的科研发现，这个奖项该颁给谁？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为还是该颁给人类。当然，这取决于人工智能是否是完全独立完成这项发现。&lt;/p&gt;&lt;p&gt;目前来看，人工智能依然只是工具，在我眼中，它是终极的科学研究工具，就像更先进的望远镜和显微镜。&amp;nbsp;人类一直都在制造工具，让自己能更好地探索自然世界，人类本质上就是会制造工具的物种，这也是人类与其他动物的区别，而工具也让人类拥有了超越自身的能力，计算机当然也属于这类工具，人工智能则是这种能力的终极体现。&lt;/p&gt;&lt;p&gt;所以在我看来，人工智能一直都是推动科学研究的终极工具，而且在可预见的未来，科学研究都将是顶尖科学家与人工智能的合作成果：科学家提出富有创意的想法和研究假设，而人工智能作为强大的工具，助力提升数据处理、模式识别的效率，推动科学探索的进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 是否会取代人？我们将迎来后稀缺时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：谷歌是 Anthropic 人工智能公司的主要投资方，Anthropic 联合创始人兼 CEO 达里奥・阿莫迪 (Dario Amodei) 今天早些时候也来到了达沃斯。他预测，未来五年内，人工智能会取代 50% 的初级白领岗位，你是否认同这一观点？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我不认同，我认为这一过程会耗时更久。今年，我们或许能看到这一趋势的初步显现，比如初级岗位和实习岗位可能会受到影响，但要实现大规模取代，我们还需要解决人工智能系统的稳定性问题。&lt;/p&gt;&lt;p&gt;我把目前人工智能的这种不均衡表现称为&amp;nbsp;“锯齿型智能”，在某些领域表现出色，在另一些领域却不尽如人意。如果想将一整项工作完全交由人工智能代理完成，而非像现在这样，仅让其作为辅助工具，就需要让系统在各方面都保持稳定的表现。如果一个系统完成一项工作的成功率只有 95%，那是远远不够的，必须能圆满完成整个任务，才能让人放心地将工作交托给它。&lt;/p&gt;&lt;p&gt;所以在出现这种大规模的岗位变革前，我们还有大量工作要做，但&amp;nbsp;这种变革最终一定会到来。当然，一旦实现通用人工智能，整个经济体系都会发生改变，这早已超出了岗位变革的范畴。如果我们能打造出真正的通用人工智能，而且方向正确，我们或许会进入一个后稀缺时代，解决世界上一些根本性的难题，比如能源问题。借助人工智能，研发出全新的清洁、可再生的近乎免费的能源，比如实现核聚变。还有新材料的研发，我认为在实现通用人工智能后的五到十年，我们会进入一个彻底改变的世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：不过，在进入后稀缺时代之前，人们对这一过渡阶段充满了焦虑。我是一位母亲，我知道你也有孩子。你最担心孩子们未来会面临什么？你会和他们聊些什么？会告诉他们未来即将到来的变化吗？我听到很多人说，大学毕业生未来的就业会非常困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我倒不这么认为。我觉得我们即将进入一个变革的时代，就像工业革命那样，或许变革的速度会是工业革命的十倍，甚至难以想象。准确来说，变革的规模和速度都会是工业革命的十倍，影响力会是百倍。&lt;/p&gt;&lt;p&gt;但我想对所有人说，变革的背后，蕴藏着巨大的机遇。而且我始终坚信人类的创造力，我们的适应能力极强，因为人类的思维具有极强的通用性。&lt;/p&gt;&lt;p&gt;人类的大脑无比强大，我们的祖先以狩猎采集为生，而我们凭借这样的大脑构建了现代文明，所以我相信我们能再次适应新的时代。当然，这次的变革是前所未有的，因为它的速度太快了。以往，这样的重大变革往往需要一两代人的时间才能完成，而这次人工智能技术的变革，规模和影响力都极为巨大。&lt;/p&gt;&lt;p&gt;但对于如今的孩子，我会鼓励他们熟练掌握这些新工具，像使用母语一样运用它们，这些工具几乎能赋予他们超能力。比如在创意艺术领域，借助人工智能，一个人或许能完成过去十个人的工作。这意味着，如果你富有创业精神，在游戏设计、电影制作等创意领域有想法，就能完成更多工作，也能比以往更容易地跻身这些行业，成为新锐人才。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：一些人主张暂停人工智能的研发，让监管政策跟上技术发展的步伐，也让社会有时间适应这些变化。如果在理想情况下，所有企业、所有国家都同意暂停研发，你是否会支持这一做法？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我会支持。我也曾公开表达过我的期望，这也是我十五年来的梦想。我接触人工智能研究已有二十五年，我一直希望，当我们接近实现通用人工智能的这一关键节点时，全球的科研人员能展开科学层面的合作。&lt;/p&gt;&lt;p&gt;我有时会设想，成立一个类似欧洲核子研究中心的国际人工智能研究机构，让全球最顶尖的人才携手合作，以极为严谨的科学方式，推进通用人工智能研发的最后阶段，同时让全社会参与其中，不仅是技术人员，还有哲学家、社会科学家、经济学家，共同探讨我们希望从这项技术中获得什么，以及如何让它造福全人类。这才是我们当下的核心议题。&lt;/p&gt;&lt;p&gt;但显然，这需要国际社会的通力合作，因为即便只有一家企业、一个国家，甚至整个西方世界决定暂停研发，倘若没有全世界的共同参与，没有制定统一的最低标准，这一做法也毫无意义。而目前，国际合作面临着不小的阻碍，所以如果想以严谨的科学方式推进通用人工智能的最后研发，就必须改变当下的国际合作现状。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果到 2030 年我们实现了通用人工智能，而相关的监管政策尚未出台，我们是否注定会面临困境？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我依然乐观地认为，全球顶尖的人工智能研发机构会充分沟通，至少在安全和安保协议等方面展开合作，目前这方面的合作已经有了不少进展。比如我们和人工智能公司 Anthropic 在这些领域的合作就十分紧密。&lt;/p&gt;&lt;p&gt;如果国际层面的合作难以推进，这种行业内的同行合作就尤为必要。我和其他顶尖人工智能实验室的负责人关系都很不错，我认为，当利害关系足够重大时，大家会意识到问题的严重性和潜在的风险，而在未来两到三年，这一点会变得更加清晰。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：你当初本可以把 Google DeepMind（谷歌深度思维）卖给任何一家企业，而如今，这些研发人工智能的企业都在寻求大众的信任。尤其是在监管政策难以跟上技术发展速度的情况下，历史经验也证明了这一点。我们为什么该信任你？为什么你认为谷歌，也是你内心所认可的，是最值得我们信任的企业？毕竟人工智能的研发存在不小的风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我认为，评判一家企业，要看它的实际行动，也要看参与相关研发的领导者的初衷。&lt;/p&gt;&lt;p&gt;我选择谷歌作为 Google DeepMind（谷歌深度思维）的归宿，有多个原因，最主要的是，谷歌的创始人创立谷歌的初衷，是打造一家以科学研究为核心的企业。&amp;nbsp;很多人都忘了，谷歌最初其实是一个&amp;nbsp;博士研究项目，是拉里・佩奇和谢尔盖・布林 的研究成果。所以我和他们一见如故。&lt;/p&gt;&lt;p&gt;拉里・佩奇主导了 Google DeepMind（谷歌深度思维）的收购，而谷歌的董事会成员也都是各行各业的顶尖人才，比如董事会主席约翰・轩尼诗是图灵奖得主，弗朗西斯・阿诺德是诺贝尔奖得主，这样的阵容在企业董事会中并不多见。所以谷歌的整体环境充满了&amp;nbsp;科学氛围，企业的发展以科学研究和工程技术为核心，这一文化早已根深蒂固。而追求最高水平的科学研究，就意味着&amp;nbsp;做事要严谨、深思熟虑，在所有领域都践行科学方法。&lt;/p&gt;&lt;p&gt;我认为这不仅适用于技术研发，也适用于企业的运营管理。所以我们始终努力做到深思熟虑、负责任，尽可能掌控我们推向市场的技术。当然，我们不可能做到尽善尽美，因为人工智能是一项全新、复杂且具有变革性的技术，但如果出现问题，我们会尽快调整修正。&lt;/p&gt;&lt;p&gt;最后我想说，谷歌想要为世界做的事情，也是我当初选择谷歌的原因之一。&amp;nbsp;谷歌的使命是整合全球信息，让人人皆可访问并从中受益，我认为这是一个非常崇高的目标。而&amp;nbsp;Google DeepMind（谷歌深度思维）的使命是破解智能的奥秘，并利用智能解决其他所有问题，这两个使命高度契合。人工智能与整合全球信息的工作本就相辅相成，谷歌的各类产品，从谷歌地图、电子邮箱到搜索引擎，都是对世界有实际价值的产品，人工智能能很自然地融入这些产品，为所有人的日常生活提供助力，我认为这是一件造福世界的事，能为此贡献力量，我感到很荣幸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：试想一下，在后稀缺时代，人们不再需要工作，当你实现了所有的技术目标后，你个人打算如何度过时间？毕竟到那时，科研工作本身或许也能实现自动化了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：如果真的到了那个阶段，我想利用人工智能探索物理学的极限。&lt;/p&gt;&lt;p&gt;上学时，我最感兴趣的就是那些终极问题：现实的本质是什么？意识的本质是什么？费米悖论的答案是什么？（费米悖论是宇宙学和天体生物学中最经典的未解之谜，由美籍意大利物理学家、1938 年诺贝尔物理学奖得主恩里科・费米（Enrico Fermi） 在 1950 年提出，核心是 “理论上的地外文明存在性” 与 “人类实际观测证据为零” 的尖锐矛盾 ，其最经典的表述就是费米的一句反问：“他们都在哪儿呢？”）时间是什么？引力是什么？&lt;/p&gt;&lt;p&gt;我很惊讶，很多人每天忙于生活，却从未思考过这些重大问题，而这些问题一直萦绕在我心头，迫切想要找到答案。我想借助人工智能，去探索所有这些问题，或许还能在人工智能的助力下，利用新的能源和材料技术，实现星际旅行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：如果人们不再需要工作，我们还能找到生活的意义和目标吗？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：说实话，这一点比经济层面的问题更让我担忧。经济层面的问题，更多是一个政治问题：当人工智能为我们带来巨大的效益和生产力提升时，我们能否确保这些成果为全人类共享，这也是我一直坚信的理念。&lt;/p&gt;&lt;p&gt;但更核心的问题是，很多人从工作和科研中获得生活的意义和目标，在新的时代，我们该如何找到这些？我认为，我们需要&amp;nbsp;新一代伟大的哲学家，来帮助我们思考这个问题。或许未来，我们的艺术创作会更加精妙，我们的探索之旅会更加深远，就像如今我们所做的极限运动等非经济目的的事情一样，未来或许会有更多更小众、更有深度的这类活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主持人：在场的所有人都想知道，自己该如何应对人工智能带来的变革。比如现在坐在达沃斯的会场里，十年后该如何自处？你认为，在场的人在看待人工智能这件事上，最容易犯的重大错误是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;哈萨比斯：我想从两个方面来说。&lt;/p&gt;&lt;p&gt;第一，对于年轻人和我们的孩子而言，唯一可以确定的是，未来会发生巨大的变化。所以在学习技能方面，要做好持续学习的准备，学会学习，才是最重要的能力。要能快速适应新环境，利用现有工具吸收新信息。&lt;/p&gt;&lt;p&gt;第二，对于在场的企业首席执行官和商界人士而言，当下最重要的是，目前市场上有很多顶尖的人工智能模型和服务提供商，未来还会更多。要选择那些以正确方式研发人工智能的合作伙伴，与这些企业携手，共同打造我们所期望的人工智能未来。&lt;/p&gt;</description><link>https://www.infoq.cn/article/0TByYFFwWJi9u0xLobuU</link><guid isPermaLink="false">https://www.infoq.cn/article/0TByYFFwWJi9u0xLobuU</guid><pubDate>Thu, 22 Jan 2026 07:41:31 GMT</pubDate><author>高允毅</author><category>Google</category><category>生成式 AI</category></item><item><title>AI撞到“数据天花板”，一场革命正悄悄上演</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;撰稿：李文朋&lt;/p&gt;&lt;p&gt;编辑：王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这两年AI发展很快，很多企业遇到的瓶颈也在变化：不再是“算力不够”，而是“数据跟不上”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月，IDC在《边缘进化：从核心到边缘驱动成功》报告中提到：已经部署生成式AI的企业里，超过60%的“实时交互类应用的响应延迟比预期高”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多时候，这种延迟不一定是模型慢，也不一定是算力不够，而是数据散在企业内部各处，口径不统一，质量也不稳定，关键时刻更是“找不到、拿不出、对不上、流不动”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;金融行业感受特别明显。一位城商行做数字化建设的负责人公开表示：“我们目前不缺算力，也不缺模型。缺的是能让模型真正跑起来的数据。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型训练成本在下降，但把数据整理好、清洗好、能实时用起来的成本反而越来越高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年初，这个问题已经不只是“体验不好”，甚至会影响商业项目的成败。IDC在FutureScape里提醒称：今年，50%的AI驱动应用将会因为数据基础薄弱，达不到原定ROI目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;事实上，数据的重要性远不止如此，更长远一点看，甚至会关系到AI到底能走多远。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025年云栖大会上，阿里巴巴集团CEO吴泳铭谈过一个判断：AGI大概率会出现，但只是开始。真正的下一步，是走向能自我迭代、持续变强的ASI。他把过程分成三段：先学会推理，再学会使用工具辅助人类，然后连接现实世界的数据，能自己学习、自己迭代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;说得更直白一点：未来AI更像一个“持续在线的系统”，它得不断吃到最新的数据，并把这些数据变成新的能力。数据是否能高效、持续地进入系统，变得愈加重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正因如此，很多基础设施厂商开始关注“更适合AI使用的数据”方向。数据库不再是“存数据”，而是要让数据更容易被统一管理、被实时取用、被不同类型的模型和应用调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年1月20日，阿里云在2026 PolarDB开发者大会上发布了AI就绪（AI-Ready）云原生数据库新标准。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它想解决的事情其实很简单：让数据系统不仅能存储、查询多模态数据，还将直接驱动AI智能决策，让数据进入模型与业务的路径更短、更稳定，以及更安全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b0/b084376de02531226517c5dabe52a7ec.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;阿里云资深副总裁、数据库产品事业部负责人李飞飞表示：“未来，AI原生数据库是技术演进的必然方向。从云原生到Al就绪，再到Al原生，PolarDB将持续深化AI与数据库的融合创新，加快走向超级人工智能时代。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业视角看，数据库已不只是业务系统的底座，开始逐渐变成智能应用能不能跑顺的关键部分。围绕“数据怎么被组织、被使用、被转化”的变革，已悄然上演。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第一部分：数据困境的背后：是新旧时代的“不兼容”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去很多年，企业做数据治理的“沉淀逻辑”只有一个：让人更容易做决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;业务员、分析师、管理层要看的数据，通常得“对得上”“能解释”“表格整齐”。于是传统数据团队投入大量成本做ETL（清洗、转换、加载），把数据整理成一张张看起来清楚、口径一致的报表。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题是，现在数据的“主要使用者”变了：很多数据不是给人看，而是给模型用。这就会出现一种情况：对人很友好的数据，不一定对模型有用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个常见例子是风控。在传统的数据整理过程中，为了让报表更稳定、更好讲，分析人员往往会把极端交易、可疑行为当成离群点删掉，觉得它们会影响整体判断。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但对模型来说，删掉这些样本的结果是：正常样本越来越多，异常样本越来越少；并导致欺诈、极端风险这些关键模式识别，几乎无法归纳学习。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，在AI时代，“干净数据”并不等于“高质量训练数据”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天很多企业说“数据资产不少，但模型效果一般”，背后往往是同一类问题——现有数据的组织方式，跟模型所需要的对不上——本质就是“兼容”问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，在结构方面，企业现有数据多数是二维表格，字段清晰，适合报表和人工分析。但很多模型更需要的是向量、图结构、时间序列这些形式，用来表达关系、上下文和变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据的维度也不够。传统指标体系更强调“少而精”，字段要能解释能展示，但模型训练往往靠大量稠密特征。很多特征单看没什么意义，要组合起来才有价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据更新速度也慢。很多系统按天、按周更新数据，这对复盘、报表够用，但推荐、风控、运营决策这类应用，往往希望输入尽量接近实时。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统数据格式也较为分散，不少业务系统以结构化数据为主，图像、音频、视频、传感器流等数据通常分散在各自系统里，管理不在一起，调用也不在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是看上去数据资产很多，但真正能直接拿来训练、推理的数据，比例并不高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大家越来越接受一个现实：2026年，数据本身将决定AI模型的能力天花板。为了缓解上面的这些“对不上”问题，“AI就绪数据”（AI-Ready Data）应运而生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它想表达的不是一个新概念，而是一件很具体的事：数据要经过专门的整理、特征化和组织，以更小的工程成本直接用于训练、推理和决策。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI就绪数据，通常会包含几类要求：首先，特征要够用，不是“有数据”就行，而是要有足够细的维度，让模型有东西可学。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如做用户行为建模，只保留“总次数”“总金额”通常不够，还需要时间分布、品类偏好、渠道差异、设备类型等细节等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，标签也要准，需要监督学习的场景里，标签相当于“题目答案”。标签粗、标签不一致，都会拉低模型上限。这就要求，图像分割、文本抽取都要尽可能精确。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，样本要尽可能覆盖真实世界，因为现实业务不会只落在“平均值”上。所以实践中会强调覆盖长尾：高峰期、极端天气、罕见故障、少数群体、低频行为等。这些数据从报表角度不一定好看，但对泛化能力很重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，数据也要能跟着变化更新，很多传统的数据质量体系把数据当“静态资产”，但用于智能应用时，数据要像“动态输入”。常见要求包括：按合适频率引入新样本；对明显过时的数据标记或降权；根据线上表现迭代数据集。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去两年，很多企业在数据库和数仓之外，再搭特征平台；要实时就接流计算；要多模态就加向量库、图系统；最后再用调度、同步、API网关把这些拼在一起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种做法在试点阶段通常能跑起来，但场景一多、频率一高、数据类型一复杂，架构复杂度和运维成本就会上去。因此，越来越多的方法论开始强调：与其在旧框架上不断加组件，不如从底层重新规划面向智能应用的数据底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在产品层面，一些云数据库厂商正在调整定位：不只做“关系型数据库”，而是把自己当作智能应用的数据基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如阿里云云原生数据库PolarDB的产品理念，就强调在云原生架构上，配合湖库一体等能力，去支撑结构化、半结构化以及非结构化数据的统一管理，为“AI就绪数据”提供底层能力等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b4/b40c88968d48f02667384403e292f2aa.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB还首次系统定义了“AI就绪数据库”的4大核心支柱，分别是：多模态AI数据湖库、高效融合搜索能力、模型算子化服务，以及面向Agent应用开发的后端服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是通过将多模态存储、搜索、推理和后端开发套件深度集成到数据库内核，满足企业多模态搜索、问答、数据处理、标注等需求，将复杂的异构架构简化为统一的智能化底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，AI就绪数据会越来越像企业的“基础配置”：这不是为了追趋势，而是为了让后面的应用能更智能、更高效、更安全地跑起来、跑下去。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第二部分：行业正想尽办法，让数据处理实现加速&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说“AI数据就绪”解决了数据能不能用，那么“数据处理速度”则决定这些数据能否“实时”产生价值。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;经过不少实践后，大家慢慢形成一个判断：同一份信息，发生在“刚刚”和“昨天”，对业务价值可能不是一两倍的差距，而是会差一个数量级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以淘宝为例，数据显示电商运营数据的实时监控能够让决策效率提升40%以上。某头部淘宝店铺通过自主搭建实时数据采集和分析系统，将数据延迟控制在1-5分钟后，运营效率和业绩直接提升30%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;风控领域的收益更明显。一次异常交易判断窗口往往只有秒级：秒级识别，损失只是几百元；第二天发现，可能已经数百万。对金融机构来说，实时数据不是“体验优化”，而是成本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;问题在于：今天大多数企业的传统数据链路，并不是为“实时”设计的。最典型数据处理路径就是：从业务数据库，到ETL，再到特征平台处理，进行特征缓存，最后供模型调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这条链路长、环节多，每一步都会带来延迟。所以这两年行业里出现一个变化：大家开始关注能不能少搬点数据，少绕几道弯。因为数据在系统之间来回搬运、复制、同步，本身就是时间和复杂度的来源。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从这个角度看，很多数据“新架构”绕来绕去，其实想解决的是同一件事：让数据尽量留在一个更统一的底座上，把处理、检索、计算尽量在同一套体系里完成，把链路缩短简化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB这次讲的“AI就绪云原生数据库”，基本就是沿着这个思路在做。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年企业反复提“湖仓一体/湖库一体”，说白了是因为两套系统各有短板：数据湖便宜、能存很多、数据类型也更杂，数据库查询强、事务能力好，可一旦规模大、成本就上来了，对大规模非结构化数据也不友好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结果就是数据经常搬来搬去：为了分析，把业务数据抽到湖里；为了在线服务，又从湖里挑一部分加工后装回库或特征仓。每搬一次，就多一次复制、多一次同步、多一段延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此次，PolarDB发布的—AI数据湖库（Lakebase）解决方案，就是专为实现“湖库—体”架构而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI数据湖库尝试把结构化、半结构化，以及非结构化数据，都放在同一个平台里统一存取和处理，减少来回同步，让链路变短。与此同时，它还配了缓存加速能力，针对不同场景做I/O和带宽的加速，让海量数据在底座里流转得更顺。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让数据从“产生”到“能用”的时间缩短，很多场景能从小时级压到分钟级，甚至更低。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是加速的第一步：少搬数据。但湖库一体更多解决的不止是“搬运成本”，还有个更隐蔽、也更容易被忽略的卡点：推理路径。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统架构里，数据库只负责存储和查询，推理模型是独立的外部服务。这样做的结果是：应用需要先从数据库取特征，再送给推理服务推理，最后把结果写回或返回业务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;每一步看起来都不慢，但数据序列化、网络传输、排队等待加起来，延迟就会暴增。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB这次的思路不太一样：它不是把推理当成“外挂”，而是希望把推理内化为数据库的原生能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它的做法是，通过多模态引擎与独有In-DB模型算子化的深度集成，开发者可以在PolarDB库内直接完成语义检索与推理加工，在效率显著提升的同时，确保数据不出域，保障隐私合规。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;具体方面，通过LLM SQL接口封装阿里云百炼各类模型构建PolarDB模型算子，开发者在SQL里可以直接调用推理能力——不用数据出库，不用中间转换，一条查询就完成&quot;找数据→检索语义→推理加工→返回结果&quot;整个流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了支撑这套库内推理，PolarDB还对底层做了分层优化，创新性地融合了KVCache、图数据库与向量技术，构建了兼顾长短期记忆与低算力消耗的检索方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;换句话说，AI数据湖库不再只是提供&quot;看数据接口&quot;，而是变成&quot;数据和模型直接对话的场所&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，要让推理少绕路，还有个前提：数据库要顶得住Agent的高频访问。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent在执行任务时，可能会发起大量查询来验证和规划，如果数据库是“存储和计算绑在一起”，高频查询的计算压力会直接拖垮存储稳定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/65/65648598c912b6fedcaf65301862e20b.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;云原生数据库PolarDB的设计是通过存算分离来解决这个问题：计算节点独立扩缩，高并发查询主要消耗计算资源，不会拖垮存储。遇到Agent高峰期的访问洪峰，可以独立扩计算而不用扩存储，成本和效率都会提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了架构分离，PolarDB还在应用和功能层做了专门设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;PolarDB新增AgentMemory能力，提供长短期记忆表结构模板，自动管理对话历史和上下文。开发者不需要自己拼SQL、维护索引，Agent每一轮对话都被自动记录，下一轮查询时自动成为上下文的一部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行层，PolarDB提供自然语言工具调用（NL2SQL自动解析与执行），Agent可以用&quot;问问题&quot;的方式检索复杂知识。同时支持多模态数据融合，让Agent能在一次查询里实时融合文本、向量、图关系的检索结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;结合基于Supabase的Agent统一部署与托管，PolarDB为企业提供工业级Agent开发框架。从多租户隔离、Serverless自动扩容、到运维自动化，所有工程复杂度都被打包进框架里，开发者只需专注定义Agent的行为和目标即可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样一来，开发者收获很明确：存算分离让高并发和性能更容易同时拿到，AgentMemory+NL2SQL+多模态融合让Agent的记忆、检索、推理更像是数据库原生支持的事；工程上的托管和Serverless减少了部署、扩容、监控这些杂事难题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;整体看下来，数据行业的这轮&quot;加速&quot;并不只是把某个指标做快，而是在做一件更底层的事：让数据少移动，让推理少绕路，让Agent的高频快速访问有专门架构支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;链路短了，实时能力才更容易稳定下来，也更容易规模化，不至于每个场景都要重新搭一套。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;第三部分：当AI反哺“数据”，AI-Native成为可能&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从行业看，2026年很可能会成为多Agent协同大规模落地的起点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不是因为单个Agent的能力突然跃升，而是因为多个Agent协同工作能够产生涌现效应——它们可以相互验证、相互纠正、共同规划复杂任务，从而完成单一模型难以胜任的工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当Agent大规模走向自主决策与协作时，可能在一秒内对数据库发起成千上万次查询——先查一遍，根据结果修正假设，再查一遍，调整策略，反复循环，直到找到满意的答案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要承载&amp;nbsp;Agent&amp;nbsp;这种近乎“暴力”的访问模式，就必须引入一种全新的数据库形态——AI-Native&amp;nbsp;数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI-Native数据库也需要从根本上改变与Agent的交互方式。最核心的转向是：从SQL的&quot;精确匹配&quot;扩展到&quot;语义级检索与推理式访问&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着数据库不再仅仅回答&quot;这个值是什么&quot;的问题，而是要回答&quot;这个值意味什么&quot;、&quot;这条数据与另一条数据在语义上有什么关联&quot;、&quot;基于这些信息，下一步应该怎么做？&quot;。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而要做到这一点，AI相关的数据能力不能只做成外挂，而要成为数据库的“内生智能”。例如在存储层支持向量索引，在查询层支持相似度检索，在优化层针对向量查询做专门优化等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大会上，PolarDB提出“AI就绪的云原生数据库”的概念，就是为了推动数据库实现从“外挂式”集成AI到“内生智能”的进化，这也是走向AI-Native的过渡。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于AI-Native数据库，另一个同样重要、却常被低估的变化，是对数据动态性的重新认知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在AI时代，高质量数据并不是一次性定义出来就能长期使用的：今天仍然有效的数据集，可能因为新的应用场景或模型路线，变得不再匹配。这需要Agent持续学习、持续适应新环境，相应的数据特征也会随之变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很显然，传统数据仓库“每天一次、每周一次”的更新节奏明显跟不上，AI-Native数据库需要支持更实时、更持续的数据优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;好的一面是：被数据“喂养”的AI，正在获得反过来“反哺数据”的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去的数据清洗、整理与验证高度依赖人工：工程师写脚本，分析师定规则，QA定期抽检，流程慢且容易遗漏。现在，具备推理与决策能力的Agent已可以把一部分治理工作自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;比如，让Agent获得对数据库的“写权限”：把自己的思考过程、决策日志写入数据库，沉淀为训练样本；把推理中得到的新知识、新规律固化到数据层。更进一步，当Agent在执行任务时发现脏数据、明显错误或不一致，它可以自动触发修正流程，而不是等人工排查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当这些机制形成闭环，数据库就能更快产出“最新、可用、被校正过”的数据，并把反馈链路压到更短的延迟。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/11/11e22833ed83a41f2566b86065a84483.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以想象一个场景：某个Agent在做客户风险评估时，发现了一类新的可疑交易特征。它把该特征写入数据库并触发检测规则；规则自动回扫历史数据，标注出相似交易；评分模型读取新标签，更新客户风险等级。整个流程自动闭环，同时数据一致性仍然受到约束与保障。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从更宏观的角度看，这意味着AI+Data正在形成一个自循环系统：AI消费数据、理解数据、改写数据，数据再反过来塑造AI的行为与能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来的超级智能（ASI）将不再是一个孤立模型，而更像是一个持续运转的系统：它既是数据的使用者，也是数据的生产者和优化者。数据不再只是被存放的资源，而是一种被不断加工、更新的运行态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个循环的速度越快、效率越高，整个系统的智能水平就越高。而承载这个循环的核心基础设施，一定是AI-Native的数据库系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;回到PolarDB大会发布的一系列能力：AI数据湖库（Lakebase）减少数据搬运，多模态多引擎融合扩展可管理的数据类型，模型算子化把推理拉回数据库内部，以及面向&amp;nbsp;Agent&amp;nbsp;应用开发的托管能力。它们看起来是分散功能，但放在一起更像一套完整路径——让数据库在AI时代重新站到系统中心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着一次更深的范式转移：从2025到2026，数据库产品、数据架构与AI应用之间的边界在变得模糊。企业IT也可能从“多个专用系统拼装”转向“围绕一个AI-Native数据库组织数据、计算与决策”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这个背景下，未来谁能更快完成从云原生到AI原生的迁移，谁就更有机会在下一轮基础设施竞争中占据优势。&lt;/p&gt;</description><link>https://www.infoq.cn/article/vAZrlAQMJ6rvaq6oWypv</link><guid isPermaLink="false">https://www.infoq.cn/article/vAZrlAQMJ6rvaq6oWypv</guid><pubDate>Thu, 22 Jan 2026 06:47:00 GMT</pubDate><author>李文朋</author><category>阿里巴巴</category><category>行业深度</category><category>数据湖仓</category></item><item><title>Android Studio Otter 优化代理工作流程，增强 LLM 灵活性</title><description>&lt;p&gt;Android Studio Otter 的最新版本引入了多项新特性，&lt;a href=&quot;https://android-developers.googleblog.com/2026/01/llm-flexibility-agent-mode-improvements.html&quot;&gt;使开发者可以更轻松地将 AI 驱动的工具集成到他们的工作流程中&lt;/a&gt;&quot;，包括选择使用哪个大型语言模型（LLM）、通过设备交互实现增强型代理模式、支持自然语言测试等。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;LLM 灵活性是指开发者可以选择使用哪个 LLM 为 Android Studio 中的 AI 功能提供支持。虽然 IDE 默认包含一个 Gemini 模型，但开发者现在可以集成一个单独的远程模型，包括 OpenAI 的 GPT 和 Anthropic 的 Claude，或者使用 LM Studio 或 Ollama 等运行一个本地模型。谷歌表示，本地模型特别适合那些“互联网连接受限、数据隐私要求严格或希望尝试开源研究成果”的开发者，不过它们需要大量的本地 RAM 和硬盘空间才能有效运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;偏好 Gemini 的开发者现在可以使用自己的 Gemini API 密钥访问更高级的版本，以及扩展后的上下文窗口和配额，在使用代理模式进行长时间编码会话时，这可能很重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio Otter 还通过让代理“看到”并与应用程序交互来增强代理模式。这包括在设备或模拟器上部署和检查应用程序，通过捕获屏幕截图和分析屏幕内容来调试应用程序 UI，以及检查 Logcat 以查找错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio Otter 的另一个主要特性是通过 “Journey” 支持自然语言测试，这使得开发者可以用简单英语定义用户 Journey 测试，Gemini 会将这些指令转换为可执行的测试步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这不仅使你可以更轻松地编写测试，而且编写出来的测试更容易理解。它还使你能够定义复杂的断言，让 Gemini 根据在设备屏幕上“看到”的内容进行评估。因为 Gemini 会推理如何实现你的目标，所以这些测试能更好地应对应用程序布局的微妙变化，在面对不同应用程序版本或设备配置时显著减少测试结果的不稳定性。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该 IDE 专门提供了一个基于 XML 的编辑器（管理这些 Journey ）以及一个测试面板（显示每个动作的屏幕截图以及 Gemini 执行每个步骤的原因）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Android Studio 现在还支持模型上下文协议（MCP），允许 AI 代理连接到 Figma、Notion 和 Canva 等远程服务器。例如，通过连接到 Figma，代理模式可以直接访问设计文件，生成更准确的 UI 代码，减少了在不同的工具之间手动复制粘贴上下文的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最后，本次更新引入了一个专门的 UI ，用于审查编码代理编辑过的每个文件。它允许开发者查看代码差异，并选择保留或单个或全部撤销更改。此外，它现在可以管理多个聊天线程，使不同的任务（如 UI 设计和 Bug 修复）可以同时执行，而不会丢失上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Otter 的特性更新比这里提到的要多许多，如经过改进的应用链接助手、 Logcat 自动回溯等。要了解完整的特性更新信息，请查阅发布公告原文。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/android-studio-otter-llm-flex/&quot;&gt;https://www.infoq.com/news/2026/01/android-studio-otter-llm-flex/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/hGq63lTTN44RtAmwA128</link><guid isPermaLink="false">https://www.infoq.cn/article/hGq63lTTN44RtAmwA128</guid><pubDate>Thu, 22 Jan 2026 06:40:00 GMT</pubDate><author>Sergio De Simone</author><category>Google</category><category>AI&amp;大模型</category><category>Android/iOS</category></item><item><title>Cloudflare在R2 SQL中推出聚合功能，提升数据分析能力</title><description>&lt;p&gt;最近，Cloudflare宣布&lt;a href=&quot;https://blog.cloudflare.com/r2-sql-aggregations/&quot;&gt;在R2 SQL中支持聚合功能&lt;/a&gt;&quot;。这是一个新特性，使开发者可以通过SQL查询存储在R2中的数据。这一功能增强使得R2 SQL不再局限于基本的过滤功能，而是可以在不依赖单独的数据仓库工具的情况下，更好地满足分析工作负载的需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://developers.cloudflare.com/r2-sql/&quot;&gt;R2 SQL&lt;/a&gt;&quot;现在支持SUM、COUNT、AVG、MIN和MAX，以及GROUP BY和HAVING子句。这些聚合函数使开发者可以直接在R2上通过R2数据目录运行SQL分析，快速汇总数据、发现趋势、生成报告以及识别日志中的异常模式。除了聚合之外，本次更新还引入了模式发现命令，包括SHOW TABLES和DESCRIBE。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare资深软件工程师&lt;a href=&quot;https://www.linkedin.com/in/jeromeschneider/&quot;&gt;Jérôme Schneider&lt;/a&gt;&quot;、高级软件工程师&lt;a href=&quot;https://www.linkedin.com/in/nikitalapkov/&quot;&gt;Nikita Lapkov&lt;/a&gt;&quot;和高级产品经理&lt;a href=&quot;https://www.linkedin.com/in/marc-selwan-088a7342&quot;&gt;Marc Selwan&lt;/a&gt;&quot;总结道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;无论是生成报告、监控大量日志中的异常，还是仅仅试图发现数据中的趋势，现在你都可以在Cloudflare提供的开发者平台上轻松完成所有这些工作，而无需管理复杂的OLAP基础设施或将数据从R2中移出。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;CloudZero研究主管Jeremy Daly在他的新闻资讯中&lt;a href=&quot;https://offbynone.io/issues/349/&quot;&gt;评论&lt;/a&gt;&quot;说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;通过在R2 SQL中支持聚合，Cloudflare继续将数据推向边缘，扩展了开发者可以实际在边缘运行的工作负载类型。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/20/200e9b573823a72d742963d9b8e1b640.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片来源：Cloudflare博客&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Schneider、Lapkov和Selwan阐述了他们如何使用scatter-gather和shuffling策略构建分布式GROUP BY执行，以便直接在R2数据目录上运行分析：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;不包含HAVING和ORDER BY子句的聚合查询可以用和过滤查询类似的方式执行。对于过滤查询，R2 SQL会选择一个节点作为查询执行的协调者。这个节点会分析查询并查看R2数据目录，以便确定哪些Parquet行组可能包含与查询相关的数据。每个Parquet行组代表单个计算节点可以处理的相对较小的工作量。协调节点将工作分配给多个工作节点，收集结果后返回给用户。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare还单独宣布，&lt;a href=&quot;https://developers.cloudflare.com/changelog/2025-12-18-r2-data-catalog-snapshot-expiration/&quot;&gt;R2数据目录现在支持Apache Iceberg表的快照自动过期&lt;/a&gt;&quot;，完善了&lt;a href=&quot;https://developers.cloudflare.com/r2/data-catalog/table-maintenance/&quot;&gt;自动压缩&lt;/a&gt;&quot;——通过将小数据文件合并成比较大的文件来优化查询性能。Selwan&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7407493096638386176/&quot;&gt;评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这两者是相辅相成的，因为快照过期所带来的一系列元数据清理/管理操作能够提高这些聚合查询的执行效率，在启用了压缩功能的情况下更是如此。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这家超大规模云服务商最近发布了一篇深度解析文章，详细阐述了其&lt;a href=&quot;https://blog.cloudflare.com/r2-sql-deep-dive&quot;&gt;分布式查询引擎的工作原理&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于R2 SQL仍处于公测阶段，所以支持的SQL语法可能会随着时间的推移而变化。文档页介绍了&lt;a href=&quot;https://developers.cloudflare.com/r2-sql/reference/limitations-best-practices/&quot;&gt;当前存在的限制和最佳实践&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-r2-sql-aggregations/&quot;&gt;https://www.infoq.com/news/2026/01/cloudflare-r2-sql-aggregations/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/233savw2C6lrQCPaUbMX</link><guid isPermaLink="false">https://www.infoq.cn/article/233savw2C6lrQCPaUbMX</guid><pubDate>Thu, 22 Jan 2026 05:58:00 GMT</pubDate><author>Renato Losio</author><category>大数据</category></item><item><title>如何利用 Snowflake ML 实现电商个性化 ｜ 技术实践</title><description>&lt;p&gt;生成式 AI 的投资回报远超预期？Snowflake 调研全球 1900 位企业与 IT 专业人士后发现平均 ROI 高达 41%！&lt;a href=&quot;https://www.infoq.cn/minibook/aja6h8SVCM1Smvggyvvu?utm_source=snowflakecn&amp;amp;utm_medium=snowflakecn&amp;amp;utm_campaign=snowflakecn&amp;amp;utm_content=snowflakecn&quot;&gt;点击下载&lt;/a&gt;&quot;完整报告&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在当今竞争激烈的电商领域，为客户提供个性化体验已不再是奢侈选项，而是驱动成功的关键要素。运用人工智能驱动分析、数据科学与机器学习的企业正日益超越竞争对手。消费者越来越期待定制化推荐与动态购物体验——这正是 Snowflake ML 的用武之地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过 Snowflake ML，开发者和分析师可直接在 Snowflake 平台中使用标准 SQL 实现以下功能：&lt;/p&gt;&lt;p&gt;加载与整合数据构建客户细分画像训练并部署机器学习模型生成个性化评分将结果输送到应用与实时工作流中&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本文将深入探讨 Snowflake ML 如何为现代电商体验提供简洁、基于 SQL 的个性化解决方案。您将了解如何将客户数据接入 Snowflake，根据行为模式划分客群，并利用 Snowflake ML 构建预测高价值客户的智能模型。无论您是构建个性化工作流的开发者，还是提升营销效果的分析师，这些实践步骤都将助您快速入门。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;请首先登录您的 Snowflake 账户（访问 Snowflake 网页控制台）。若尚未拥有账户或需测试环境进行学习，可在此免费注册体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤1：加载并准备数据&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将首先创建一个客户订单的小型模拟数据集。&lt;/p&gt;&lt;p&gt;请在 Snowflake SQL 工作表中完整运行以下代码块：&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 1.0: Create a database and schema

CREATE OR REPLACE DATABASE DATACLOUDDISPATCHSI;
USE DATABASE DATACLOUDDISPATCHSI;
CREATE OR REPLACE SCHEMA ECOMMERCE;
USE SCHEMA ECOMMERCE;

-- Step 1.1: Create a customer orders table

CREATE OR REPLACE TABLE CUSTOMER_ORDERS (
  CUSTOMER_ID  NUMBER,
  ORDER_ID     NUMBER,
  ORDER_DATE   DATE,
  ORDER_VALUE  NUMBER(10,2),
  PRODUCT_ID   NUMBER
);

-- Step 1.2: Insert sample order data

INSERT INTO CUSTOMER_ORDERS (CUSTOMER_ID, ORDER_ID, ORDER_DATE, ORDER_VALUE, PRODUCT_ID) VALUES
(1001,50001,&#39;2023-01-15&#39;, 89.99,201),
(1001,50022,&#39;2023-03-02&#39;,120.49,305),
(1002,50110,&#39;2023-05-11&#39;, 45.00,110),
(1003,50155,&#39;2023-02-19&#39;,239.00,402),
(1003,50190,&#39;2023-05-22&#39;,130.00,233),
(1003,50201,&#39;2023-06-01&#39;, 99.99,110),
(1004,50333,&#39;2023-01-05&#39;, 19.99,502),
(1001,50390,&#39;2023-11-11&#39;,301.00,900),
(1005,50400,&#39;2023-12-12&#39;, 67.50,702);

-- Step 1.3: Verify data
SELECT * FROM CUSTOMER_ORDERS ORDER BY ORDER_DATE;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该数据集包含重复的客户购买记录、多样化的订单金额以及用于后续客户分群和机器学习建模的实用字段，足以支持基础建模需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;使用 Snowflake Workspace&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若您倾向于通过可视化界面而非 SQL 加载数据，Snowflake Workspace 支持将文件（包括 Excel 和 CSV 格式）直接拖放至环境中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;在 Snowflake 左侧导航栏中进入 Projects。&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;点击下拉菜单中的 Workspaces（如图所示）。&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;创建并打开一个新的 Workspace。&lt;/p&gt;&lt;p&gt;4.&amp;nbsp;在 Workspace 内点击+ Worksheet 以新建 SQL 工作表。&lt;/p&gt;&lt;p&gt;5.&amp;nbsp;运行 SQL 代码前，请确保工作表已设置正确的角色、仓库、数据库与模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/03/03741f3890ab0c93ba25027a62dbd661.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;本教程步骤 1 至 3 中的所有 SQL 命令均需在此 SQL 工作表中粘贴并执行。Snowflake 虽提供 Workspace、Notebook 等多种项目工具，但本教程全程使用标准 SQL 工作表完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤二：使用 SQL 构建客户细分模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake 支持集成机器学习模型，用于预测客户行为、推荐产品及定制促销策略。开发人员可通过 Python 或 R 语言，结合 Snowflake 的 Data Science Workspace 部署模型，该模型可输入客户数据并输出个性化推荐。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一种基础的个性化策略是基于客户历史行为进行识别，我们将计算以下指标：&lt;/p&gt;&lt;p&gt;购买频率客单价（AOV）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;USE DATABASE DATACLOUDDISPATCHSI;
USE SCHEMA ECOMMERCE;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 2.1: Create customer segments
CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS AS
SELECT
  CUSTOMER_ID,
  COUNT(ORDER_ID)  AS PURCHASE_COUNT,
  AVG(ORDER_VALUE) AS AVG_ORDER_VALUE
FROM CUSTOMER_ORDERS
WHERE ORDER_DATE BETWEEN &#39;2023-01-01&#39; AND &#39;2023-12-31&#39;
GROUP BY CUSTOMER_ID;

-- Step 2.2: Inspect customer segments
SELECT * FROM CUSTOMER_SEGMENTS ORDER BY PURCHASE_COUNT DESC;
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由此构建的 CUSTOMER_SEGMENTS 表将成为机器学习模型的基础数据层。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤三：训练与部署机器学习模型（基于 Snowflake ML 的纯 SQL 实现）&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Snowflake ML 支持直接使用 SQL 训练模型，无需依赖 Python 或外部工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将完成以下任务：&lt;/p&gt;&lt;p&gt;1.&amp;nbsp;标记“高价值客户”（购买次数 ≥3次）&lt;/p&gt;&lt;p&gt;2.&amp;nbsp;训练分类模型&lt;/p&gt;&lt;p&gt;3.&amp;nbsp;对全部客户进行评分&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.1：创建训练表&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 Snowflake 中训练机器学习模型前，需为模型提供学习样本。这意味着需要构建一个包含以下内容的表：&lt;/p&gt;&lt;p&gt;特征（模型学习的输入变量）目标标签（模型需预测的结果）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;本例中，我们的目标是识别高价值客户。因此，需要在历史数据中创建一列，明确标注哪些客户属于高价值客户。训练表的作用正在于此——它基于步骤二生成的客户分群，新增目标标签列。随后，Snowflake ML将利用此标注表学习高价值客户的特征模式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.1: Add a target label for modeling
CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS_TRAIN AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    IFF(PURCHASE_COUNT &amp;gt;= 3, 1, 0) AS TARGET_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

SELECT * FROM CUSTOMER_SEGMENTS_TRAIN ORDER BY PURCHASE_COUNT DESC;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.2：使用 Snowflake ML 训练分类模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在获得已标注的训练表后，即可训练 Snowflake ML 识别高价值客户的潜在特征。通过训练分类模型，Snowflake 将学习：&lt;/p&gt;&lt;p&gt;应从哪些输入特征中学习规律（如购买次数与平均订单金额）需要预测的目标结果（即高价值标签：0或1）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.2: Train the classification model

CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION HIGH_VALUE_MODEL (
    INPUT_DATA     =&amp;gt; SYSTEM$REFERENCE(&#39;TABLE&#39;, &#39;ECOMMERCE.CUSTOMER_SEGMENTS_TRAIN&#39;),
    TARGET_COLNAME =&amp;gt; &#39;TARGET_HIGH_VALUE&#39;
);
Snowflake automatically trains and tunes the model based on your training table.
(Optional) View metrics:
CALL HIGH_VALUE_MODEL!SHOW_EVALUATION_METRICS();&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;步骤3.3：使用模型对客户进行评分（SQL）&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模型训练完成后，即可用于预测。在此步骤中，模型将根据每位客户的购买行为（购买次数与平均订单金额）判断其是否为潜在高价值客户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下 SQL 命令将每位客户的特征输入模型，并返回预测结果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- Step 3.3: Score customers

SELECT
    s.CUSTOMER_ID,
    s.PURCHASE_COUNT,
    s.AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, s.PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, s.AVG_ORDER_VALUE
        )
    ) AS MODEL_OUTPUT
FROM CUSTOMER_SEGMENTS AS s;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;MODEL_OUTPUT 是什么？&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 将模型的预测结果以 VARIANT 类型（一种结构化对象）返回。您无需运行或执行它——它仅仅是 Snowflake 所展示的结果！&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了使预测结果更易于使用，您可以只提取预测类别（0或1）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;1 表示模型将客户识别为高价值客户0 表示非高价值客户&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;提取预测类别的语句为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这将为您提供一个清晰的 0/1 指标，用于判断客户是否被视为“高价值客户”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;&amp;nbsp;步骤3.4：持久化个性化评分（可选）&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;至此，您已通过在查询中直接使用模型生成预测，这非常适合探索性分析——但在实际场景中，您通常需要将这些预测存储到表中，以便供仪表板、应用程序、营销活动等重复使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下 SQL 语句创建一个名为 CUSTOMER_VALUE_SCORES 的新表，其中包含每位客户、其购买行为以及模型的预测结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE OR REPLACE TABLE CUSTOMER_VALUE_SCORES AS

SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;
SELECT * FROM CUSTOMER_VALUE_SCORES ORDER BY PREDICTED_HIGH_VALUE DESC;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在您已拥有一个可用于下游个性化流程的数据表。您可以持续引用这些评分来定位高价值客户、触发个性化优惠、提供推荐内容等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;步骤四：实时个性化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;获得每位客户的预测评分后，即可结合实时行为数据提供更智能的个性化推荐。实时行为数据包括：&lt;/p&gt;&lt;p&gt;最近浏览的商品购物车中新增或移除的商品浏览或会话事件实时库存更新&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;针对更高级的用例，Snowflake 支持在线特征存储，允许应用程序（如网站或推荐引擎）在毫秒级延迟内获取最新的客户特征——包括近期点击行为、会话历史或模型生成的评分。这对于需要在应用用户体验中实现实时个性化（而非依赖批量调度）的场景尤为理想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Snowflake 可通过 Kafka、Kinesis 或 Event Hubs 等工具接收此类流式数据，从而根据客户行为变化持续更新推荐结果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为保持个性化数据的时效性，您还可以通过 Snowflake 任务定期更新推荐表。以下示例展示了一个每小时运行并刷新热门商品推荐的简化任务：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;--示例：定期更新推荐数据&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;CREATE OR REPLACE TASK PERSONALIZE_RECOMMENDATIONS
WAREHOUSE = COMPUTE_WH
SCHEDULE = &#39;USING CRON 0   UTC&#39;
AS
MERGE INTO LATEST_RECOMMENDATIONS tgt
USING (
    SELECT CUSTOMER_ID, PRODUCT_ID, SCORE
    FROM ECOMMERCE.RECOMMENDATIONS_STREAM
    WHERE SCORE &amp;gt; 0.8
) src
ON tgt.CUSTOMER_ID = src.CUSTOMER_ID AND tgt.PRODUCT_ID = src.PRODUCT_ID
WHEN MATCHED THEN UPDATE SET SCORE = src.SCORE
WHEN NOT MATCHED THEN INSERT VALUES (src.CUSTOMER_ID, src.PRODUCT_ID, src.SCORE);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此方案使您的应用程序能够始终查询最新、最相关的推荐结果，从而实现完全动态的个性化购物体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;个性化推荐现已不再局限于手动规则或外部机器学习流水线。借助 Snowflake ML，您可以在 Snowflake 平台内直接驱动端到端的电商个性化推荐。本教程展示了如何：&lt;/p&gt;&lt;p&gt;将全部电商数据整合至统一的单一平台完全使用 SQL 构建客户细分模型通过 Snowflake ML 训练机器学习模型——无需 Python 环境完成客户评分并生成个性化洞察利用实时数据流和任务机制保持推荐结果动态更新&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最关键的是，所有操作均在 Snowflake 内完成——无需数据迁移、无需配置 Python 环境、无需依赖外部服务。这使得开发者、分析师和数据团队能够以前所未有的便捷度，提供高度个性化的购物体验。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;注：本教程使用 SQL 和 Snowflake ML 进行演示，但 Snowflake 还提供更多人工智能与智能增强功能，可助力规模化扩展个性化应用场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;想要一键复制代码以便跟随操作吗？&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是您可以粘贴到 SQL workspace 中的分步最小可复现工作流程：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;-- ============================================================

-- E-COMMERCE PERSONALIZATION QUICKSTART (SQL-ONLY)

-- End-to-end example:

--  1. Create database &amp;amp; schema
--  2. Load sample orders data
--  3. Build customer segments
--  4. Prepare training data for ML
--  5. Train Snowflake ML classification model
--  6. Score customers &amp;amp; optionally persist scores

-- ============================================================

----------------------------------------------------------------

-- (Optional) Step 0: Choose a warehouse

----------------------------------------------------------------

-- Uncomment and replace &lt;your_warehouse&gt; if needed:
-- USE WAREHOUSE &lt;your_warehouse&gt;;

----------------------------------------------------------------

-- Step 1: Create database, schema, and sample CUSTOMER_ORDERS

----------------------------------------------------------------

CREATE OR REPLACE DATABASE DATACLOUDDISPATCHSI;
USE DATABASE DATACLOUDDISPATCHSI;
CREATE OR REPLACE SCHEMA ECOMMERCE;
USE SCHEMA ECOMMERCE;

-- Create the orders table

CREATE OR REPLACE TABLE CUSTOMER_ORDERS (
  CUSTOMER_ID  NUMBER,
  ORDER_ID     NUMBER,
  ORDER_DATE   DATE,
  ORDER_VALUE  NUMBER(10,2),
  PRODUCT_ID   NUMBER
);

-- Insert sample e-commerce data

INSERT INTO CUSTOMER_ORDERS (CUSTOMER_ID, ORDER_ID, ORDER_DATE, ORDER_VALUE, PRODUCT_ID) VALUES
(1001,50001,&#39;2023-01-15&#39;, 89.99,201),
(1001,50022,&#39;2023-03-02&#39;,120.49,305),
(1002,50110,&#39;2023-05-11&#39;, 45.00,110),
(1003,50155,&#39;2023-02-19&#39;,239.00,402),
(1003,50190,&#39;2023-05-22&#39;,130.00,233),
(1003,50201,&#39;2023-06-01&#39;, 99.99,110),
(1004,50333,&#39;2023-01-05&#39;, 19.99,502),
(1001,50390,&#39;2023-11-11&#39;,301.00,900),
(1005,50400,&#39;2023-12-12&#39;, 67.50,702);

-- Quick preview of raw orders

SELECT * FROM CUSTOMER_ORDERS ORDER BY ORDER_DATE;

----------------------------------------------------------------

-- Step 2: Build customer segments (frequency &amp;amp; average order value)

----------------------------------------------------------------

-- Aggregate behavior to create one row per customer

CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS AS
SELECT
    CUSTOMER_ID,
    COUNT(ORDER_ID)  AS PURCHASE_COUNT,
    AVG(ORDER_VALUE) AS AVG_ORDER_VALUE
FROM CUSTOMER_ORDERS
WHERE ORDER_DATE BETWEEN &#39;2023-01-01&#39; AND &#39;2023-12-31&#39;
GROUP BY CUSTOMER_ID;

-- Inspect segments

SELECT * FROM CUSTOMER_SEGMENTS ORDER BY PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 3: Prepare training data for Snowflake ML
-- Add a label indicating whether a customer is “high-value”
-- (in this example: 3 or more purchases)

----------------------------------------------------------------

CREATE OR REPLACE TABLE CUSTOMER_SEGMENTS_TRAIN AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    IFF(PURCHASE_COUNT &amp;gt;= 3, 1, 0) AS TARGET_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

-- View training data with target

SELECT * FROM CUSTOMER_SEGMENTS_TRAIN ORDER BY PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 4: Train a classification model with Snowflake ML
-- This learns to predict TARGET_HIGH_VALUE from the features
-- PURCHASE_COUNT and AVG_ORDER_VALUE.

----------------------------------------------------------------

CREATE OR REPLACE SNOWFLAKE.ML.CLASSIFICATION HIGH_VALUE_MODEL (
    INPUT_DATA     =&amp;gt; SYSTEM$REFERENCE(&#39;TABLE&#39;,&#39;ECOMMERCE.CUSTOMER_SEGMENTS_TRAIN&#39;),
    TARGET_COLNAME =&amp;gt; &#39;TARGET_HIGH_VALUE&#39;
);

-- (Optional) Inspect training metrics
CALL HIGH_VALUE_MODEL!SHOW_EVALUATION_METRICS();

----------------------------------------------------------------

-- Step 5: Score customers with the trained model
-- This returns the predicted class (0 = not high-value, 1 = high-value).

----------------------------------------------------------------

SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS
ORDER BY PREDICTED_HIGH_VALUE DESC, PURCHASE_COUNT DESC;

----------------------------------------------------------------

-- Step 6 (Optional): Persist personalized scores for downstream use
-- This creates a reusable table that other teams, dashboards,
-- and applications can query.

----------------------------------------------------------------

CREATE OR REPLACE TABLE CUSTOMER_VALUE_SCORES AS
SELECT
    CUSTOMER_ID,
    PURCHASE_COUNT,
    AVG_ORDER_VALUE,
    HIGH_VALUE_MODEL!PREDICT(
        INPUT_DATA =&amp;gt; OBJECT_CONSTRUCT(
            &#39;PURCHASE_COUNT&#39;, PURCHASE_COUNT,
            &#39;AVG_ORDER_VALUE&#39;, AVG_ORDER_VALUE
        )
    ):PREDICTION:&quot;class&quot;::NUMBER AS PREDICTED_HIGH_VALUE
FROM CUSTOMER_SEGMENTS;

-- Final scored output

SELECT * FROM CUSTOMER_VALUE_SCORES
ORDER BY PREDICTED_HIGH_VALUE DESC, PURCHASE_COUNT DESC;&lt;/your_warehouse&gt;&lt;/your_warehouse&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文地址：&lt;a href=&quot;https://www.linkedin.com/pulse/how-leverage-snowflake-intelligence-e-commerce-personalization-60fhc/?trackingId=SamHZTb8T76gKESH2PP2SA%3D%3D&quot;&gt;https://www.linkedin.com/pulse/how-leverage-snowflake-intelligence-e-commerce-personalization-60fhc/?trackingId=SamHZTb8T76gKESH2PP2SA%3D%3D&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/QY60NJrEuZq8um5fiHO7</link><guid isPermaLink="false">https://www.infoq.cn/article/QY60NJrEuZq8um5fiHO7</guid><pubDate>Thu, 22 Jan 2026 03:24:12 GMT</pubDate><author>Snowflake</author><category>Snowflake</category><category>大数据</category><category>AI&amp;大模型</category></item><item><title>揭秘Uber跨区域数据湖与灾难恢复机制：350PB数据、数百万事件、单一系统</title><description>&lt;p&gt;Uber构建了&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync/&quot;&gt;HiveSync&lt;/a&gt;&quot;，这是一个分片式批量复制系统，能够使Hive和HDFS数据在多个区域之间保持同步，它每天处理数百万个Hive事件。HiveSync确保了跨区域数据的一致性，实现了Uber的灾难恢复策略，并消除了由次要区域闲置而导致的低效问题——此前次要区域需承担与主区域一样的硬件成本，而HiveSync在维持高可用性的同时彻底解决了这一问题&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync基于开源项目Airbnb &lt;a href=&quot;https://github.com/airbnb/reair&quot;&gt;ReAir&lt;/a&gt;&quot;构建并做了一些扩展，包括实现了分片、基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;&gt;DAG&lt;/a&gt;&quot;的编排以及控制平面和数据平面的分离。&lt;a href=&quot;https://en.wikipedia.org/wiki/Extract,_transform,_load&quot;&gt;ETL&lt;/a&gt;&quot;作业现在只在主数据中心执行，而HiveSync处理跨区域复制，实现了近乎实时的一致性，保持了灾难应对能力和分析访问权限。分片功能允许将表和分区划分为独立的单元，从而实现并行复制和细粒度容错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync将控制平面（负责编排作业和管理关系元数据存储中的状态）与数据平面（执行&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hadoop&quot;&gt;HDFS&lt;/a&gt;&quot;和&lt;a href=&quot;https://en.wikipedia.org/wiki/Apache_Hive&quot;&gt;Hive&lt;/a&gt;&quot;文件操作）分离。Hive Metastore事件监听器负责捕获DDL和DML变更，将它们记录到&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&quot;中，并触发复制工作流。任务以有限状态机的形式呈现，支持任务重启与健壮的故障恢复机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/70/70a87b9fb2fc1939cfd347a9b44c9890.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;HiveSync架构：控制平面和数据平面分离（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync有两个主要组件：HiveSync复制服务和数据修复服务。复制服务使用Hive Metastore事件监听器实时捕获表和分区变更，将它们异步记录到MySQL中。这些审计条目被转换为异步复制作业，以有限状态机的形式执行，为确保可靠性，状态会被持久化。Uber使用了混合策略：规模比较小的作业使用&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_procedure_call&quot;&gt;RPC&lt;/a&gt;&quot;以提高效率，而规模比较大的作业则利用&lt;a href=&quot;https://yarnpkg.com/&quot;&gt;YARN&lt;/a&gt;&quot;上的DistCp。DAG管理器强制执行分片级的排序和锁定，而静态和动态分片技术则实现了水平扩展，确保复制过程一致且无冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5e/5ed30f7fc7494e66967a1812cc9107e8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;HiveSync复制服务（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;数据修复是一个持续检测异常的服务，如缺失的分区或非预期的HDFS更新，恢复数据中心1（DC1）和数据中心2（DC2）之间的一致性，从而保证数据的正确性。HiveSync保证了每四小时一次的复制SLA，99&lt;a href=&quot;https://en.wikipedia.org/wiki/Percentile&quot;&gt;百分位&lt;/a&gt;&quot;的延迟大约为20分钟，并支持一次性复制，用于在切换到增量复制之前，一次性地将历史数据集导入新区域或集群。Uber的数据修复服务会扫描DC1和DC2，检测异常（如缺失或多余的分区），并修复任何不匹配的情况，从而确保跨区域的一致性，目标是准确性超过99.99%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/5d/5dd2b6aac7fb4dd6b5b67670c5d45d79.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;数据修复服务分析和解决数据中心之间的不一致性（来源：&lt;a href=&quot;https://www.uber.com/blog/building-ubers-data-lake-batch-data-replication-using-hivesync&quot;&gt;Uber博文&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;HiveSync的规模很大，管理着80万个Hive表，总计约300PB的数据，单表数据量从几GB到数十PB不等，单表分区数从几百到一百万多不等。每天，HiveSync处理超过500万个Hive DDL和DML事件，跨区域复制约8PB的数据。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;展望未来，随着批量分析和ML管道迁移到谷歌云平台，Uber计划将HiveSync扩展到云端复制场景，进一步利用&lt;a href=&quot;https://en.wikipedia.org/wiki/Shard_(database_architecture)&quot;&gt;分片&lt;/a&gt;&quot;、编排和数据一致性技术来高效地维护其PB级数据的完整性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-hivesync-data-lake/&quot;&gt;https://www.infoq.com/news/2026/01/uber-hivesync-data-lake/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/O7T47Q680HHi6rbdBEPr</link><guid isPermaLink="false">https://www.infoq.cn/article/O7T47Q680HHi6rbdBEPr</guid><pubDate>Thu, 22 Jan 2026 03:06:30 GMT</pubDate><author>作者：Leela Kumili</author><category>大数据</category><category>安全</category></item><item><title>架构彻底重构！DeepSeek新模型代码曝光，要来的V4让国内外都坐不住了？</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;DeepSeek V4马上要来了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正值 DeepSeek-R1 发布一周年之际，DeepSeek 的官方 GitHub 代码库意外曝光了代号为“MODEL1”的全新模型线索。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而综合泄露代码片段中呈现的架构调整、硬件优化与全新处理机制来看，“MODEL1”似乎绝非简单的版本迭代，而是一次全方位的架构重构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此次 DeepSeek 在 GitHub 代码库的提前部署，在时间线上与业内疯传的“其新模型再次在春节期间发布”的消息高度吻合。本月初，也有外媒爆料称，DeepSeek 将在今年 2 月中旬农历新年期间推出新一代旗舰 AI 模型 DeepSeek V4。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;新模型曝光，代码揭露全新架构能力&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，DeepSeek 陆陆续续给其在GitHub上的 FlashMLA 代码库做了一系列更新。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/7e/7e5d5348f3efe211a0e3e5f1ba7eb4e5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而刚刚，有开发者发现，114 个文件中有 28 处都提到了未知的“MODEL1”大模型标识符。而且，在代码逻辑结构中，该标识符与现有模型“V32”（即 DeepSeek-V3.2）是并列且作为独立分支出现的。也就是说，“MODEL1”很可能代表一个不同于现有架构和技术路径的全新模型。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/37/37bd822e39d17249fa980e897d6855d9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;网友们也纷纷猜测，这个“MODEL1”很可能就是 DeepSeek 即将发布的新模型V4的内部开发代号或首个工程版本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据代码片段中披露的技术规格，这个新模型有重大架构变更，或在KV Cache（键值缓存）布局、稀疏性处理及FP8解码支持等方面改变了策略和机制，还包括参数维度切换至 512 维以及针对英伟达下一代 Blackwell GPU 架构的专项优化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 FP8 解码路径上，该模型有多处针对性的内存优化调整。测试脚本中同步新增了test_flash_mla_sparse_decoding.py与test_flash_mla_dense_decoding.py两个文件，这一改动证实“MODEL1”具备稀疏与稠密计算并行处理的能力。在稀疏化实现方案中，键值缓存存储采用 FP8 精度，而矩阵乘法运算则使用 bfloat16 精度，以此保障计算准确性。这种混合精度设计表明，“MODEL1”通过在推理阶段对部分数据进行选择性稀疏化处理，有效降低内存占用压力，从而具备处理超长上下文窗口的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f8/f8d9b8a3d386e1644e188a9293322bd6.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在csrc/api/common.h文件内的代码显示，“MODEL1”的注意力头参数维度被配置为 512 维，与上一代产品 DeepSeek V3.2 采用的 576 维参数设置形成显著差异。这一架构调整意味着，DeepSeek已对其多头隐式注意力（MLA）结构进行了重新设计。此前的 V3 系列采用非对称设计方案，将 128 维旋转位置编码（RoPE）与 448 维隐层维度相结合。此次转向标准化的 512 维参数配置，或许是为了更好地适配硬件性能，也可能是在隐层压缩率方面实现了技术突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/0889979be9eaef4b84e9d7e73cb3ce6a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代码更新记录还显示，DeepSeek研发团队已围绕英伟达 Blackwell 架构开展了大量优化工作，预示着DeepSeek正为“MODEL1”量身打造下一代硬件适配方案。代码中新增了一批专门面向 Blackwell 指令集的接口，包括FMHACutlassSM100FwdRun；相关文档明确指出，该模型若要在 B200 GPU 上运行，需依赖 CUDA 12.9 版本环境；内嵌的性能指标数据显示，即便在未完全优化的状态下，稀疏化 MLA 算子在 B200 硬件平台上的运算性能仍可达到 350 万亿次浮点运算每秒（TFLOPS）。在当前主流的 H800 GPU（基于 SM90a 架构）上，稠密型 MLA 算子的吞吐量则能达到 660 万亿次浮点运算每秒。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管本次代码提交的内容主要聚焦于算子层面的实现，但调度逻辑中仍提及多项新增功能。从代码仓库的结构可以推断，“MODEL1”集成了价值向量位置感知（VVPA）技术，这项技术有望解决传统 MLA 架构在长文本处理场景下存在的位置信息衰减问题。代码注释中还提到了一种名为 “记忆印记（Engram）机制” 的技术，但在已公开的代码提交记录中，相关实现细节尚不完整。从该机制在分布式处理模块中的部署位置推测，其功能大概率与分布式存储优化或高级键值压缩技术相关，旨在满足“MODEL1”对高吞吐量的性能需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;前不久，DeepSeek 研究团队刚发布了 Engram 的技术论文。当时，就有业内观察者认为，Engram 模块可能会成为 DeepSeek V4 的重要组成部分，并预示 DeepSeek 下一代模型会在记忆和推理协同上实现架构级提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些优化能够表明，“MODEL1”在推理效率上可能有更好的表现。此前也有爆料称，DeepSeek V4的代码表现已超越 Claude 和 GPT 系列，并且具备处理复杂项目架构和大规模代码库的工程化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;国内外万众期待，“中国 AI 站起来了”&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“DeepSeek刚刚泄露了一个模型，这可能会再次改变整个AI行业的格局。”在国内外的各大社交平台及社区，针对 DeepSeek 新模型的上线猜测、能力预测的期待帖子已大量涌现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“中国 AI 站起来了。”昨日，全球最大的 AI 开源社区 Hugging Face 以“距离DeepSeek 时刻一周年”为题专门发文，复盘了 R1 发布这一年来对中国开源社区及其对整个AI生态系统的影响。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“这是中国研发的开源模型首次跻身全球主流榜单。此后一年间，每当有新模型发布时，R1 都会被当作重要的参照基准。该模型迅速登顶 Hugging Face 平台历史最受欢迎模型榜单，而这一平台上最受青睐的模型，也不再以美国研发的产品为主导。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在他们看来，R1 的真正价值在于降低先进AI能力的门槛或者说障碍，并提供了清晰的模式。&lt;/p&gt;&lt;p&gt;技术障碍。通过公开分享其推理路径和训练后的方法，R1将此前被封闭API锁定的高级推理转变为可下载、提炼和微调的工程资产。许多团队不再需要从零开始训练庞大的模型来获得强大的推理能力。应用障碍。R1 以 MIT 许可证发布，使其使用、修改和再分发变得简单。依赖封闭式模型的公司开始直接将R1投入生产。蒸馏、二次培训和领域特定适应成为常规工程工作，而非专门项目。心理层面。当问题从“我们能做到吗？”转变为“我们如何做好？”时，许多公司的决策发生了变化。对于中国 AI 社区来说，这也是罕见的持续全球关注时刻，对长期被视为追随者的生态系统意义重大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“在 R1 模型发布一年后的今天，我们看到的不仅是一大批新模型的涌现，更见证了一个富有生命力的中国 AI 开源生态的加速成型。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/FlashMLA?tab=readme-ov-file&quot;&gt;https://github.com/deepseek-ai/FlashMLA?tab=readme-ov-file&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment&quot;&gt;https://huggingface.co/blog/huggingface/one-year-since-the-deepseek-moment&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://chinabizinsider.com/deepseeks-mysterious-model-1-surfaces-in-github-code-sparking-speculation-about-next-generation-ai-system/&quot;&gt;https://chinabizinsider.com/deepseeks-mysterious-model-1-surfaces-in-github-code-sparking-speculation-about-next-generation-ai-system/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/XISEq5cHfv4FARpZMBgZ</link><guid isPermaLink="false">https://www.infoq.cn/article/XISEq5cHfv4FARpZMBgZ</guid><pubDate>Wed, 21 Jan 2026 10:18:11 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>理想主义者们，没能阻止AI进入伊朗“战场”</title><description>&lt;p&gt;撰稿&amp;nbsp;|&amp;nbsp;陈姚戈、高允毅&lt;/p&gt;&lt;p&gt;编辑&amp;nbsp;|&amp;nbsp;王一鹏&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一场从线下蔓延至线上的舆论战争，正发生在伊朗。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;线下，伊朗当局正在组织“反骚乱”集会；线上，断网、媒体管制和信息封锁同时发生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在网络被关闭的时段，伊朗国营媒体几乎成为唯一的信息源。信息真空之中，大量影像只能在社交平台上传播，却很难被证实或证伪。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网民和非营利组织通过自发事实核查发现，伊朗官方发布了使用后期编辑和&amp;nbsp;AI&amp;nbsp;生成影像，刻意营造了“反骚乱”的舆论氛围。这类内容在&amp;nbsp;X&amp;nbsp;平台上获得了数万次观看。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/229bbf0e7d84ba6134ce9060c6aadf50.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，另一方同样出现大量&amp;nbsp;AI&amp;nbsp;生成内容。一段被广泛转发的视频显示，有人从建筑物上扯下国旗，发布者称有人撤下了伊朗国旗。这段视频经过反向图片搜索后，被证实拍摄于&amp;nbsp;2025&amp;nbsp;年&amp;nbsp;9&amp;nbsp;月尼泊尔抗议活动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/84/84af1869924c53814f241423b3de06cb.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非营利组织&amp;nbsp;WITNESS“技术威胁与机遇”项目副主任&amp;nbsp;Mahsa&amp;nbsp;Alimardani&amp;nbsp;&lt;a href=&quot;https://www.theatlantic.com/international/2026/01/iran-disinformation-ai-protests-doubt/685608/&quot;&gt;指出&lt;/a&gt;&quot;，在传播过程中被&amp;nbsp;AI“增强画质”的现场照片，反而被当局用来否定影像本身的真实性，对抗议事实进行整体抨击。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真假在这一过程中被同时稀释，AI&amp;nbsp;让“知晓真相”这件事变得更难了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并非人们第一次意识到&amp;nbsp;AI&amp;nbsp;的风险。近几年，从《&lt;a href=&quot;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&quot;&gt;要求暂停更强模型训练的公开信&lt;/a&gt;&quot;》，到《&lt;a href=&quot;https://superintelligence-statement.org/zh&quot;&gt;针对超级智能的联合声明&lt;/a&gt;&quot;》，理想主义者反复呼吁放慢脚步、建立约束。但现实是，这些警告几乎没有改变产业的整体方向，也未能阻止更强模型和更激进应用的持续推出。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;尤其是在战争中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从俄乌冲突、以伊冲突，再到今天在伊朗发生的舆论战，包括生成式&amp;nbsp;AI&amp;nbsp;在内的技术被广泛采用，而战场也正成为前沿&amp;nbsp;AI&amp;nbsp;技术和武器的“实验场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;《&lt;a href=&quot;https://www.news.cn/mil/2024-04/24/c_1212356149.htm&quot;&gt;中国航空报&lt;/a&gt;&quot;》指出，乌克兰战事加速了&amp;nbsp;AI&amp;nbsp;在实战中的应用落地，如自主导航、目标识别和交战以及情报处理等。根据《&lt;a href=&quot;https://qnck.cyol.com/pc/content/202502/28/content_407550.html&quot;&gt;青年参考&lt;/a&gt;&quot;》，大量军事科技初创企业和国防创新企业在乌克兰聚集，使乌克兰逐渐演变为相关技术的重要孵化地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更深层的变化在于，科技公司、金融资本与国家战争机器之间，正在形成紧密绑定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月，美国国防部长&amp;nbsp;Pete&amp;nbsp;Hegseth&amp;nbsp;公布新一轮国防采购改革，明确指出原有国防体系已难以应对新的战争形态，并宣布启动新的“作战采购系统”，以缩短交付周期、提升灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;美国国防部试图引入硅谷的投资和迭代逻辑，重塑军备采购体系，让军队像科技公司一样快速试错、快速部署、快速扩张。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;资本迅速跟进。今年1月，a16z&amp;nbsp;宣布新一轮募资超过&amp;nbsp;150&amp;nbsp;亿美元，其中明确投向国防科技领域的资金超过&amp;nbsp;11&amp;nbsp;亿美元。与此同时，a16z&amp;nbsp;还与美国陆军参谋长CTO&amp;nbsp;Alex&amp;nbsp;Miller&amp;nbsp;和美国海军部&amp;nbsp;CTO&amp;nbsp;Justin&amp;nbsp;Fanelli等美国军方要员共同推出播客和专栏，教初创企业如何拿下国防部订单。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以色列政府通过初创公司加速器计划&amp;nbsp;Innofense、增加对本土初创企业的采购额等，系统性地推动私营技术进入军事和安全体系。围绕这一政策环境，近几年集中涌现出一批专注国防科技的初创公司和投资机构，“Patriotism&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service”成为了以色列创投圈的时髦概念。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;类似的转向也正在欧洲发生。2024&amp;nbsp;年，欧盟投资银行放宽了对军民两用技术项目的投资限制，并参与设立规模约&amp;nbsp;1.75&amp;nbsp;亿欧元的国防股权基金，以吸引更多社会资本进入相关领域。《&lt;a href=&quot;https://www.news.cn/globe/20251124/ec98edac049d4f879924d9a3f1d0ff7d/c.html&quot;&gt;环球&lt;/a&gt;&quot;》杂志指出，这些政策为初创企业提供了关键的早期订单和市场入口；同时，在技术、市场、资本与战略因素的共振下，欧洲初创企业大力进军军工产业，欧洲军工创业投资正迎来爆发式增长。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;世界正处这样的时刻：AI&amp;nbsp;的能力已被大规模引入战争中最敏感的场景，而大型科技公司缺乏主动约束自身的动力；本应推动规则协调与共识形成的国际组织，在关键议题上的作用仍然有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;伊朗现场，正在发生的&amp;nbsp;AI&amp;nbsp;信息战&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一段“万人上街支持政府”的航拍视频，在&amp;nbsp;1&amp;nbsp;月&amp;nbsp;12&amp;nbsp;日突然刷屏社交平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;画面中，伊朗记者坐在一架直升机敞开的舱门边，一边俯瞰地面“集会人群”，一边对着镜头解说：伊朗民众自发走上街头，支持本国政府，对抗美国和以色列的干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/caf6efe11878be722651ce5d8a2634de.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;镜头掠过，整条街道被伊朗国旗铺满，整齐庞大的队伍，看上去就是一场“全民拥护政府”的壮观场面。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很快，这段视频就被贴上了另一个标签：AI&amp;nbsp;造假。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伊朗政策分析师&amp;nbsp;Behnam&amp;nbsp;Gholipour&amp;nbsp;公开质疑画面真实性，并谴责这是人工智能生成的虚假信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d8/d82290fe01a54ec5caf5e74456a0c05b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有网民对画面细节提出质疑，并逐帧分析指出：记者坐在直升机舱门边，却未见任何安全防护；衣着与面部状态未呈现高速气流下的正常反应；手部动作存在异常形变；街道背景中还出现了已被烧毁的建筑……&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;质疑声越滚越大，IRIB&amp;nbsp;很快放出第二段“证据视频”：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;画面里，记者坐在电脑前，播放完整的集会录像，试图证明，先前那段航拍并非伪造。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/b3/b3f0e10709366b4b727d5bc0f73a3c17.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但马上有人发现新录像存在前后矛盾之处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/749ce8e117dd4d98ce3cfe8048d92efc.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而另一张广泛流传的关于集会的图片，有眼尖网民放大画面，发现有人“长”在伊朗国旗上，上半身悬在空中，下半身则直接消失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a1/a17a809bdb148f8cb27a50ecc3cba6f3.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社交平台的评论画风逐渐一边倒，IRIB&amp;nbsp;不仅在用可疑的视频讲述“盛大集会”，还在用同样粗糙的方式掩盖伪造。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;网民对伊朗官方的愤怒，很快堆积在评论区。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;有人开始恶搞那位直升机记者，用各种&amp;nbsp;AI&amp;nbsp;工具生成新的“伪造视频”和恶搞图。“既然你用&amp;nbsp;AI&amp;nbsp;篡改现场，那我们就用&amp;nbsp;AI&amp;nbsp;把你变成梗。”这种“以梗对梗，以&amp;nbsp;AI&amp;nbsp;反制&amp;nbsp;AI”的创作，在社交平台上快速扩散。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/997976329d8201813164900890e854ac.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI玩梗是“技术抵抗”和消解意义的一种方式。但以AI对抗AI终不是种解法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;非营利组织&amp;nbsp;WITNESS&amp;nbsp;“技术威胁与机遇”项目副主任&amp;nbsp;Mahsa&amp;nbsp;Alimardani&amp;nbsp;在网络欺骗、审查和监控领域有超过&amp;nbsp;15&amp;nbsp;年的研究经验。她在最近发表的文章《&lt;a href=&quot;https://www.theatlantic.com/international/2026/01/iran-disinformation-ai-protests-doubt/685608/&quot;&gt;怀疑如何在伊朗成为一种武器&lt;/a&gt;&quot;》中指出：“AI&amp;nbsp;对信息的操纵，以及围绕这种操纵产生的怀疑，本身都会成为掩盖真相的工具。”（AI&amp;nbsp;manipulation,&amp;nbsp;and&amp;nbsp;the&amp;nbsp;very&amp;nbsp;suspicion&amp;nbsp;of&amp;nbsp;it,&amp;nbsp;serves&amp;nbsp;those&amp;nbsp;who&amp;nbsp;have&amp;nbsp;the&amp;nbsp;most&amp;nbsp;to&amp;nbsp;hide.）&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Mahsa&amp;nbsp;Alimardani&amp;nbsp;回忆称，集会自12月28日爆发后仅数小时，伊朗当局相关账号就开始将抗议现场的真实影像贴上“AI&amp;nbsp;伪造”的标签。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个典型案例发生在抗议爆发后的第二天：一段在德黑兰拍摄的低清视频中，一名抗议者坐在街道中央，面对安保力量。该事件已被多方核实确认属实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但随着视频在网络上不断传播，出现了经过&amp;nbsp;AI&amp;nbsp;增强画质的版本。BBC&amp;nbsp;波斯语记者&amp;nbsp;Hossein&amp;nbsp;Bastani&amp;nbsp;发布了这段清晰版视频，但未注意到其已被&amp;nbsp;AI&amp;nbsp;处理。支持伊朗官方的相关账号随即抓住这一点，将&amp;nbsp;AI&amp;nbsp;修图留下的痕迹当作“证据”，以此否定这张照片和其他抗议影像的真实性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/19/197aea919a2a038d1d66ff3e6e16aa9e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/744e24b3b6994950405c283f527ddb02.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上图为低清原视频；下图为&amp;nbsp;AI&amp;nbsp;增强后的版本。Hossein&amp;nbsp;Bastani&amp;nbsp;已就未注意到其&amp;nbsp;AI&amp;nbsp;增强特性而道歉。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Alimardani&amp;nbsp;认为，深度伪造让&amp;nbsp;AI&amp;nbsp;贴上“欺骗工具”的标签，但实际上很多常用的图片编辑工具都带有生成式&amp;nbsp;AI&amp;nbsp;的能力，公众很难分辨出哪一种是善意修图、哪一种是恶意伪造。正因为如此，伊朗不仅可以利用&amp;nbsp;AI&amp;nbsp;本身，还可以利用公众对&amp;nbsp;AI&amp;nbsp;的怀疑，把这种怀疑变成一种“加速剂”，进一步压制和否定抗议信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;AI&amp;nbsp;如何改变现代战场&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在战场中，AI不仅影响人们理解战争，更近一步参与战争本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;俄乌冲突与以伊冲突，为观察&amp;nbsp;AI&amp;nbsp;如何介入舆论战与实际作战提供了清晰案例。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;欧盟资助的虚假信息意识与韧性项目团队（&lt;a href=&quot;https://www.eeas.europa.eu/delegations/ukraine/results-pro-russian-information-manipulation-and-disinformation-monitoring-targeting-ukraine-eu_en?s=232&quot;&gt;DARE&lt;/a&gt;&quot;），在调研俄乌冲突时的信息操纵时发现，相关舆论操纵活动已明显呈现出自动化、规模化特征。调查显示，水军账号不再主要依赖人工运营，而是借助&amp;nbsp;AI&amp;nbsp;工具批量生成虚假社交身份，并模拟真实用户的行为轨迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以&amp;nbsp;Meliorator&amp;nbsp;为代表的&amp;nbsp;AI&amp;nbsp;软件包，可以自动生成包含头像、兴趣与互动历史的账号资料，并通过技术手段规避平台的异常检测机制，使这些账号在短时间内融入正常的信息流。同时，AI&amp;nbsp;生成的图像与视频被用于构建情绪指向明确的叙事，削弱受众对信息真实性的判断能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种变化在中东地区的冲突中表现得更加直观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以伊冲突期间，一张“伊朗击落以色列F-35战斗机”的图片在社交平台迅速传播。图片中，一架喷气式战机坠毁在沙漠中，残骸周围挤满围观民众。这一画面一度让外界误以为伊朗在空中对抗中占据上风。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/2f/2f5095d33f9165e34063a51a232e12c0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但图像本身的物理逻辑存在明显漏洞。现场人物与车辆比例失衡，沙地上也缺乏高速坠毁应有的冲击痕迹。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据澎湃新闻旗下事实核查栏目“澎湃明查”梳理，在&amp;nbsp;2025&amp;nbsp;年伊以冲突期间，基于&amp;nbsp;AI&amp;nbsp;生成的虚假视频和图像数量显著上升，规模甚至超过了俄乌冲突初期。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些内容往往画面粗糙、叙事夸张，甚至直接截取自游戏画面，却频繁被用于“重构”战斗场景，成为信息战的重要组成部分。凡是包含武器、废墟或宗教符号的影像，都可能被抽离原有背景，重新拼接成一个看似连贯、实则失真的“中东战场”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说舆论战主要作用于认知层面，那么从俄乌冲突开始，AI&amp;nbsp;已逐步进入直接参与作战的阶段，战场也成为&amp;nbsp;AI&amp;nbsp;技术快速试验和迭代的环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月的“蛛网”行动，集中体现了&amp;nbsp;AI&amp;nbsp;与无人机系统结合后所展现出的作战能力。在这次行动中，乌克兰国家安全局策划并实施代号为“蛛网”的特种作战，出动约&amp;nbsp;150&amp;nbsp;架远程无人机，对俄罗斯境内&amp;nbsp;5&amp;nbsp;座空军基地发动袭击，损坏包括&amp;nbsp;Tu-160、Tu-22&amp;nbsp;和&amp;nbsp;Tu-95&amp;nbsp;在内的&amp;nbsp;41&amp;nbsp;架战机。乌方称俄方损失约&amp;nbsp;70&amp;nbsp;亿美元，而单架无人机的成本不足&amp;nbsp;1000&amp;nbsp;美元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伴随技术升级，战争的参与结构也在发生变化。商用武器、AI技术和军事需求的结合，正在塑造一个由政府和企业共同参与的作战生态。这一模式部署灵活、更新迅速，但相应的监管与约束机制尚未同步建立。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这一过程中，私营商业科技公司开始进入更核心的位置。乌克兰在冲突中广泛使用由美国民用软件公司&amp;nbsp;Palantir&amp;nbsp;提供的信息分析系统，对多源战场数据进行整合与研判，为指挥决策提供支持。相关系统能够在短时间内处理光学影像、雷达数据与火力分布信息，从而提升行动效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Palantir&amp;nbsp;是由&amp;nbsp;PayPal创始人&amp;nbsp;Peter&amp;nbsp;Thiel&amp;nbsp;创立的国防科技公司，已经成为多国国防部的供应商。就在今年1月，Palantir&amp;nbsp;与乌克兰国防科技集群&amp;nbsp;Brave1&amp;nbsp;启动&amp;nbsp;Dataroom&amp;nbsp;项目。该平台允许工程师利用大量经实战验证的数据训练和测试&amp;nbsp;AI&amp;nbsp;模型，目标之一是开发新一代自主拦截无人机，使其在缺乏人工干预、且&amp;nbsp;GPS&amp;nbsp;与通信受干扰的环境下，仍能完成探测、分类与拦截任务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;科技企业正主动嵌入战争的运行机制之中。技术开始按市场与投资逻辑被快速设计、部署和迭代，战争由此进入一套新的商业-政治结构，对既有国际规则形成持续挤压。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当一段伪造影像就足以影响大规模公众判断，当低成本无人系统能够在复杂环境中自主锁定并打击高价值目标时，如何为&amp;nbsp;AI&amp;nbsp;的军事应用划定清晰边界，已成为无法回避的现实问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;当科技、资本和政治形成AI联盟&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正在美国和以色列发生的事情，为我们提供了一种更现实的视角：当科技公司、金融资本与国家安全机器深度绑定，战争的技术形态、节奏与激励机制都会随之改变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一个共同趋势正在显现——私营科技公司再次被系统性地拉入国防体系核心。它们不再只是为军方提供工具的外包商，而是直接参与战争工具的设计、部署，甚至作战本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从资金流向上看，这并非零星现象。根据&amp;nbsp;PitchBook&amp;nbsp;数据，全球防务科技的风险投资在过去十年持续抬升，并在近两轮战争节点出现明显跃升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是交易金额还是交易数量，在俄乌冲突、加沙战争这些时间点，“发战争财”都变得异常活跃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d1/d1ce8f6c04d8cb06d736c8a32ef15dff.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;硅谷回到五角大楼&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在美国正在发生的事情是，硅谷与五角大楼关系的重新加温。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然硅谷的诞生与美国国防技术的发展息息相关，但过去二十年中，风险投资企业对国防科技的关注度，从未像今天如此之高。风险投资机构们纵使不出于道德考虑，也因为昂贵的硬件、未经证实的商业路径以及传统国防承包商的垄断，一直徘徊在五角大楼门外。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这个平衡正在被打破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，政策环境发生变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://federalnewsnetwork.com/federal-insights/2026/01/military-acquisition-reform-has-important-backing/&quot;&gt;特朗普&lt;/a&gt;&quot;通过一系列行政命令和《FoRGED法案》等立法支持，对传统军工承包商施加严格的财务与绩效惩罚，同时大幅放松采购监管，以扶持高增长的科技企业。并推动一套得到两党支持的采购改革方案，核心逻辑只有一个：让军队像科技公司一样采购、迭代和部署技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月&amp;nbsp;7&amp;nbsp;日，美国国防部长&amp;nbsp;Pete&amp;nbsp;Hegseth&amp;nbsp;&lt;a href=&quot;https://media.defense.gov/2025/Nov/10/2003819439/-1/-1/1/TRANSFORMING-THE-DEFENSE-ACQUISITION-SYSTEM-INTO-THE-WARFIGHTING-ACQUISITION-SYSTEM-TO-ACCELERATE-FIELDING-OF-URGENTLY-NEEDED-CAPABILITIES-TO-OUR-WARRIORS.PDF&quot;&gt;正式公布&lt;/a&gt;&quot;新一轮国防采购改革，目标是缩短装备交付周期，为长期僵化的采购体系引入更大的灵活性。在面向国防与科技行业高管的演讲中，他直言原有的“国防采购系统”已经走到尽头，并宣布启动全新的“作战采购系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，五角大楼发布《采购转型战略》及配套指令，明确三项改革重点：一是整体转向作战采购体系；二是推进对外军售（FMS）与直接商业销售（DCS）的现代化；三是重塑联合需求审查流程。国防部释放出的信号十分明确——现有规则不再适配新的战争形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一方面，资本明确进场。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;就在今年&amp;nbsp;1&amp;nbsp;月&amp;nbsp;9&amp;nbsp;日，a16z&lt;a href=&quot;https://a16z.com/why-did-we-raise-15b/?campaign_id=4&amp;amp;emc=edit_dk_20260112&amp;amp;instance_id=169150&amp;amp;nl=dealbook&amp;amp;regi_id=297464440&amp;amp;segment_id=213529&amp;amp;user_id=bb34479d05a7b8849830c7aced0df618&quot;&gt;宣布&lt;/a&gt;&quot;新一轮募资超过&amp;nbsp;150&amp;nbsp;亿美元，金额占&amp;nbsp;2025&amp;nbsp;年美国所有风险投资总额的&amp;nbsp;18%&amp;nbsp;以上。新基金的领域的金额包括：&amp;nbsp;American&amp;nbsp;Dynamism（11.76&amp;nbsp;亿美元）、Apps（17&amp;nbsp;亿美元）、Bio&amp;nbsp;+&amp;nbsp;Health（7&amp;nbsp;亿美元）、Infrastructure（17&amp;nbsp;亿美元）、Growth（67.5&amp;nbsp;亿美元）和其他风险投资策略（30&amp;nbsp;亿美元）。其中&amp;nbsp;“American&amp;nbsp;Dynamism”&amp;nbsp;明确指向国防与国家安全相关产业。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在&amp;nbsp;a16z&amp;nbsp;的&lt;a href=&quot;https://a16z.com/defense-reform/&quot;&gt;官网&lt;/a&gt;&quot;，你可以看到这样两行露骨的文字——&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“a16z&amp;nbsp;致力于推动动态的国防科技改革，以重建美国的国防工业基础。以创新保障安全。是时候行动了。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/eb/eb75e2407086bdf99d281a6024c25ee0.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“美国——这个创新者和建设者的国度——已经因为官僚主义和中央计划而失去了国防工业基础。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/43/43600e99c56cc8896d502ad65c870706.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月，美国陆军在官网&lt;a href=&quot;https://www.army.mil/article-amp/286317/army_launches_detachment_201_executive_innovation_corps_to_drive_tech_transformation&quot;&gt;宣布&lt;/a&gt;&quot;，正在组建第&amp;nbsp;201&amp;nbsp;分队“陆军高管级创新团”。来自&amp;nbsp;Meta、OpenAI、Palantir&amp;nbsp;和Thinking&amp;nbsp;Machines&amp;nbsp;Lab的&amp;nbsp;4&amp;nbsp;位高管，以高级顾问身份兼职宣誓加入陆军预备役。陆军在公告中表示，通过引入私营领域的专业能力，第&amp;nbsp;201&amp;nbsp;分队正为包括陆军转型计划在内的多个项目提供支持，目标是让军队变得更加精简、智能和高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca90c9df8ad5acce173b57666b0e7428.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;6&amp;nbsp;月&amp;nbsp;13&amp;nbsp;日，美国陆军参谋长Randy&amp;nbsp;A.&amp;nbsp;George&amp;nbsp;为四名新任美国陆军中校主持宣誓就职仪式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Randy&amp;nbsp;A.&amp;nbsp;George&amp;nbsp;对面从左至右分别是Meta&amp;nbsp;首席技术官&amp;nbsp;Andrew&amp;nbsp;Bosworth、Thinking&amp;nbsp;Machines&amp;nbsp;Lab&amp;nbsp;顾问和OpenAI&amp;nbsp;前首席研究官Bob&amp;nbsp;McGrew、Palantir&amp;nbsp;首席技术官&amp;nbsp;Shyam&amp;nbsp;Sankar、OpenAI&amp;nbsp;for&amp;nbsp;Science副总裁&amp;nbsp;Kevin&amp;nbsp;Weil。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;硅谷不再只是为战争“提供工具”，也开始参与战争体系的设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从金融到科技公司，以色列的“全民皆兵”模式&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比美国，以色列并不缺乏军队和科技企业融合的历史，大量科技公司，如Palo&amp;nbsp;Alto&amp;nbsp;Networks、Wix&amp;nbsp;的创始人都来自&amp;nbsp;8200&amp;nbsp;情报部队。8200&amp;nbsp;情报部队的退伍军人还组成了非盈利组织“8200&amp;nbsp;校友”，为青少年提供编程培训、为创业公司提供服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但真正的变化起源于&amp;nbsp;2019&amp;nbsp;年之后。当时，以色列前参谋长&amp;nbsp;Aviv&amp;nbsp;Kochavi&amp;nbsp;发起了&amp;nbsp;Tnufa&amp;nbsp;五年计划，旨在将以色列国防军（IDF）转型为一支更致命、数字化的多域作战力量。与此同时，以色列国防部（IMoD）、研发局（MAFAT）与民间机构合作成立初创企业加速器Innofense，寻找和集成能够改变战场游戏规则的军民两用技术，并为企业提供早期资金支持，加速其产品化进程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2023年的10/7事件后，虽然&amp;nbsp;Tnufa&amp;nbsp;计划宣告破产，但以&amp;nbsp;Innofense&amp;nbsp;为代表的军队与初创公司合作的模式被保留下来。除Innofense&amp;nbsp;之外，以色列政府和军队还&lt;a href=&quot;https://ddrd-mafat.mod.gov.il/en/mafat-for-startups&quot;&gt;大力推进&lt;/a&gt;&quot;“绿色通道计划”（Green&amp;nbsp;Lane&amp;nbsp;Track），为初创企业和年收入不超过&amp;nbsp;2500&amp;nbsp;万新谢克尔&amp;nbsp;(NIS)&amp;nbsp;的小型公司提供精简流程，使其能够注册成为国防部的正式供应商。结果是，与标准国防采购相比，该通道大幅缩短了反馈响应时间，并放宽了合同条件，为初创企业简化了采购流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/ce/21/cee2e753de9772d789dd5bc90d3c6721.jpg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;10/7事件，是指2023年10月7日，在哈马斯袭击以色列之初，从加沙地带潜入以色列的哈马斯武装分子对在雷姆基布兹附近参加诺瓦音乐节的平民发动大屠杀。这个事件被认为是以色列的“911”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上文提到的Tnufa计划中，以色列军队为了追求“高效、灵活”削减了一些传统的地面部队规模，导致以色列边境常规驻军过少且缺乏随时可用的预备役动员方案，增援部队花费了数小时甚至十数小时才到达受袭社区。2026年，现任以色列国防军总参谋长&amp;nbsp;Eyal&amp;nbsp;Zamir&amp;nbsp;宣布了新的多年计划Hoshen，以替代Tnufa。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;图片为以色列国家图书馆推出的“10月7日纪念墙”，展示了2023年10月7日以来遇难的平民、以色列国防军士兵的照片和姓名。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据公开信息，截止2025年12月，以色列国防部与超过&amp;nbsp;300&amp;nbsp;家初创公司合作，其中三分之一直接参与战争相关项目，大多为军民两用技术。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;10/7事件也直接影响了许多以色列金融和科技经营的投资和创业逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“过去18到24个月内成立的这批国防科技初创公司，绝大多数都是在‘10/7’事件之后才诞生的。它们源于真实的军事需求、作战需求，甚至是个人切肤之痛，并且已经经过实战验证、正在发挥作用。”Aurelius&amp;nbsp;Capital&amp;nbsp;创始人&amp;nbsp;Alon&amp;nbsp;Lifshitz&amp;nbsp;在&lt;a href=&quot;https://www.youtube.com/watch?v=Q-ZOAoTRKbo&quot;&gt;最近的&lt;/a&gt;&quot;对谈播客中表示。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成立于&amp;nbsp;2024&amp;nbsp;年的&amp;nbsp;Kela&amp;nbsp;是这一代公司的代表。其目标是“帮助西方防务体系快速、无缝整合商业与军事系统”，已从红杉资本、Lux&amp;nbsp;Capital&amp;nbsp;以及&amp;nbsp;In-Q-Tel&amp;nbsp;筹集&amp;nbsp;1&amp;nbsp;亿美元资金，最新一轮估值约&amp;nbsp;2&amp;nbsp;亿美元。值得一提的是，In-Q-Tel&amp;nbsp;虽为非盈利机构，其资金却来自&amp;nbsp;CIA&amp;nbsp;，它也是&amp;nbsp;Palantir的早期投资者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Aurelius&amp;nbsp;Capital&amp;nbsp;则代表了这批公司背后，以色列投资机构的新风向。它成立于2025年1月，专注以色列国防领域投资，目前已经完成首轮约&amp;nbsp;5000&amp;nbsp;万美元的募资。&amp;nbsp;创始人&amp;nbsp;Alon&amp;nbsp;Lifshitz&amp;nbsp;曾经在采访中表示，他此前创立的&amp;nbsp;Haneco&amp;nbsp;Venture&amp;nbsp;因&amp;nbsp;LP&amp;nbsp;限制无法涉足国防领域，而10/7事件直接促使他与妻子另起炉灶，成立一家明确服务于国防方向的新基金。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种转向甚至开始被包装为一种“以色列爱国主义”投资叙事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在以往频繁讨论&amp;nbsp;Platform&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service、Model&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service&amp;nbsp;的以色列投资界，出现了“Patriotism&amp;nbsp;as&amp;nbsp;a&amp;nbsp;Service”的说法。以色列风投机构&amp;nbsp;TLV&amp;nbsp;Partners&amp;nbsp;在10/7事件后公开提出这一理念，并表示国防领域将成为其投资生态的重要部分。&amp;nbsp;TLV&amp;nbsp;Partners&amp;nbsp;投资的&amp;nbsp;AI&amp;nbsp;视觉识别公司&amp;nbsp;Airis&amp;nbsp;Labs，试图将日常数字影像转化为可直接用于任务的情报资产，服务于国家安全、公共安全、边境管理和应急响应等场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Airis&amp;nbsp;Labs&amp;nbsp;在官网介绍，传统情报工具并非为去中心化、多模态的信息环境而设计，而以色列的对手正日益利用用户生成内容进行协调、招募和传播。借助Airis&amp;nbsp;Labs的&amp;nbsp;User-Generated&amp;nbsp;Field&amp;nbsp;Intelligence，任何来源的媒体内容都可以被转化为可计算、可调用的情报资产。短短几句的描述，已经为我们勾勒出一个《疑犯追踪》中大规模、定制化监控系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从资金规模看，以色列国防相关部门和公司的合作已经具有明显规模效应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据多方公开信息梳理，与以色列国防部研发局（MAFAT）合作的国防科技初创企业，在&amp;nbsp;2025&amp;nbsp;年通过融资和并购已吸引超过&amp;nbsp;10&amp;nbsp;亿美元资金。报道同时指出，2024&amp;nbsp;年虽然也是国防领域融资金额创纪录的一年，但全年融资规模仅约&amp;nbsp;1.5&amp;nbsp;亿美元；在&amp;nbsp;2025&amp;nbsp;年之前，该领域初创企业历年来累计融资总额约为&amp;nbsp;4.22&amp;nbsp;亿美元。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;很多人可能已经忘了互联网开始于军用网络。而今天这些科技公司、金融资本与国家安全机器在“国防”领域的合作，无疑都在提醒我们，一个把科技当作美好创新代表的时代已经结束了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这正是我们今天讨论&amp;nbsp;AI&amp;nbsp;治理，无法回避的现实背景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;失效的AI治理&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;今天，AI&amp;nbsp;治理正同时经历着道德共识、国际机制与企业自律的三重失效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自&amp;nbsp;AI&amp;nbsp;行业引领者的警告一直从未缺席。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年&amp;nbsp;10&amp;nbsp;月，非营利组织未来生命研究所发起了《&lt;a href=&quot;https://superintelligence-statement.org/zh&quot;&gt;针对超级智能的联合声明&lt;/a&gt;&quot;》，包括人工智能先驱杰弗里·辛顿、苹果公司联合创始人史蒂夫·沃兹尼亚克等多位知名人士参与签署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这份声明没有激起什么讨论的水花。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也许你还记得2023年3月，科技界曾发起《&lt;a href=&quot;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&quot;&gt;要求暂停更强模型训练的公开信&lt;/a&gt;&quot;》，呼吁所有人工智能实验室立即暂停训练比&amp;nbsp;GPT‑4&amp;nbsp;更强大的模型，暂停时间至少&amp;nbsp;6&amp;nbsp;个月，并建议在企业不配合的情况下由政府强制介入。结果是，没有任何一家关键公司或实验室真正停下，包括签署公开信的埃隆·马斯克本人。2023&amp;nbsp;年&amp;nbsp;11&amp;nbsp;月，xAI&amp;nbsp;正式推出&amp;nbsp;Grok‑1&amp;nbsp;的抢先体验版本——很难相信这是一场“暂停”之后的产物。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;杰弗里·辛顿频繁公开演讲、不断签署声明，但这些努力并未改变产业的集体行动方向，并未改变和他一样的聪明头脑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些公开信之后的“缺乏行动”无疑反映出，大型科技公司缺乏主动约束自身的动力，行业内部也未能形成真正可执行的治理共识。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说道德呼吁无法转化为行动，本应承担“共识塑造”与规则协调角色的国际组织，同样未能填补这一真空。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;军事领域负责任人工智能峰会（REAIM），是少数能够聚集全球近一半国家和地区代表，专门讨论军事人工智能治理的国际平台。2024&amp;nbsp;年，该峰会形成了一份“行动蓝图”，提出了关于“负责任使用军事人工智能”的最低共识，例如强调人工智能应用应符合伦理、以人为本，人类仍需对人工智能的开发和使用承担责任；同时明确指出，人工智能技术应接受法律审查，并遵循包括国际人道主义法和国际人权法在内的适用国际法框架。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但即便是这样一份最低限度的原则文件，在会议期间仍未获得完全认可，约有&amp;nbsp;30&amp;nbsp;个政府代表拒绝接受相关表述。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直到&amp;nbsp;2025&amp;nbsp;年&amp;nbsp;8&amp;nbsp;月，联合国才正式设立具备明确职能和常设架构的&amp;nbsp;AI&amp;nbsp;治理机制，包括“人工智能独立国际科学专家组”和“全球人工智能治理对话平台”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但这份好不容易到来的“治理机制”并不试图建立一套具有强制力的普适规则，而是强调在部分议题上促进协调与共识，同时有意避开高度敏感的领域。尤其值得注意的是，独立政策研究机构&amp;nbsp;Chatham&amp;nbsp;House&amp;nbsp;&lt;a href=&quot;https://www.chathamhouse.org/2025/09/can-uns-new-ai-governance-efforts-weather-ai-race&quot;&gt;观察&lt;/a&gt;&quot;到，人工智能在军事领域的应用，被明确排除在联合国讨论议程之外，这也直接引发了对“军民两用技术”将如何被监管的广泛疑虑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在国际治理尚未就AI在军事中的应用达成广泛共识之前，AI&amp;nbsp;企业自身已率先调整了边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2024&amp;nbsp;年&amp;nbsp;1&amp;nbsp;月，OpenAI&amp;nbsp;在其服务条款中&lt;a href=&quot;https://www.cnbc.com/2024/01/16/openai-quietly-removes-ban-on-military-use-of-its-ai-tools.html&quot;&gt;删除了&lt;/a&gt;&quot;明确禁止人工智能用于“军事和战争”应用的条款，转而采用更模糊的措辞，要求用户不应“利用我们的服务伤害自己或他人”，包括“研发或使用武器”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同年&amp;nbsp;11&amp;nbsp;月，Meta&amp;nbsp;&lt;a href=&quot;https://about.fb.com/news/2024/11/open-source-ai-america-global-security/&quot;&gt;宣布&lt;/a&gt;&quot;将向政府机构提供其&amp;nbsp;Llama&amp;nbsp;生成式人工智能模型用于“国家安全应用”，并与国防承包商&amp;nbsp;Anduril&amp;nbsp;合作，开发军用&amp;nbsp;AR/VR&amp;nbsp;头戴设备和训练系统。TechRadar&amp;nbsp;&lt;a href=&quot;https://www.techradar.com/pro/meta-is-letting-the-us-military-use-its-llama-ai-model-for-national-security-applications&quot;&gt;评论&lt;/a&gt;&quot;称，这一行动与&amp;nbsp;Llama&amp;nbsp;之前的可接受使用政策存在显著差异——该政策原本禁止模型用于“军事、战争、核工业或间谍活动”，并明确禁止武器开发和宣扬暴力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2025&amp;nbsp;年，Google&amp;nbsp;&lt;a href=&quot;https://www.wired.com/story/google-responsible-ai-principles/&quot;&gt;修改《AI&amp;nbsp;原则》&lt;/a&gt;&quot;，删除“不开发武器”“不用于监视”等明确限制条款，转而采用更模糊的表述，强调技术应用需服务于“国家安全、民主与防卫”。这打破了2018年谷歌的承诺。当时&amp;nbsp;Google&amp;nbsp;因参与五角大楼&amp;nbsp;Project&amp;nbsp;Maven&amp;nbsp;项目引发员工抗议，随后发布《AI&amp;nbsp;原则》，明确承诺不将技术用于武器开发或特定监控用途。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;伦敦国王大学讲师&amp;nbsp;Nick&amp;nbsp;Srnicek&amp;nbsp;在其新书《&lt;a href=&quot;https://www.wired.com/story/book-excerpt-silicon-empires-nick-srnicek/&quot;&gt;硅谷帝国：人工智能的未来之争&lt;/a&gt;&quot;》中，描述了AI巨头们卷入美国军事行动的故事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他观察到&amp;nbsp;，科技巨头正借助“竞争威胁”的叙事抵制监管，并与国家安全体系深度绑定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年中，关键人物的立场已发生明显转变：Sam&amp;nbsp;Altman&amp;nbsp;从呼吁中美合作，转向强调“美国领导的志同道合国家联盟”；Anthropic&amp;nbsp;首席执行官&amp;nbsp;Dario&amp;nbsp;Amodei&amp;nbsp;也从担忧竞赛风险，转向主张美国必须在&amp;nbsp;AI&amp;nbsp;竞争中取胜。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Srnicek&amp;nbsp;总结，这标志着“硅谷共识”的瓦解——曾以全球化与开放为目标的技术秩序，正在被技术民族主义和阵营对抗取代。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;投资机构以及进入“国防”领域的初创公司，则进一步借助“安全困境”理论为自身行为提供正当性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;国防科技公司&amp;nbsp;Anduril&amp;nbsp;与投资机构&amp;nbsp;Founders&amp;nbsp;Fund&amp;nbsp;的创始人&amp;nbsp;Trae&amp;nbsp;Stephens&amp;nbsp;曾发表过一篇广为流传的文章《&lt;a href=&quot;https://medium.com/@traestephens/the-ethics-of-defense-technology-development-an-investors-perspective-45c71bf6e6af&quot;&gt;国防科技发展的伦理：一个投资者的视角&lt;/a&gt;&quot;》，为私企和资本加大对“国防”技术的投入“正名”。这篇文章的核心观点是，战争应当是“万不得已的最后手段”，对国防技术的投资恰恰是为了避免和慑止战争。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，他强调更高科技的武器有可能带来更少的伤害：高度精确、由&amp;nbsp;AI&amp;nbsp;驱动的打击手段，有可能减少无辜平民的伤亡，并降低大规模、无差别攻击发生的概率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;是的，技术有可能做到这一点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但现实是，“精准打击”往往不顾及平民伤亡。根据冲突检测机构&amp;nbsp;Airwars，2023年10月，以色列通过&amp;nbsp;AI&amp;nbsp;赋能的监听技术锁定哈马斯指挥官Ibrahim&amp;nbsp;Biari后，对他所在地区展开空袭，袭击中超过125名平民丧生。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当这群世界上“最聪明”“最有野心”的人聚集在一起，不断提高武器创新和部署的效率时，很难相信“威慑”仍是他们唯一的动机——当从战争中公开获利变得越来越容易，又有什么理由真正去阻止战争的发生？&lt;/p&gt;</description><link>https://www.infoq.cn/article/RuJMiEsSJguUXza6HsrM</link><guid isPermaLink="false">https://www.infoq.cn/article/RuJMiEsSJguUXza6HsrM</guid><pubDate>Wed, 21 Jan 2026 09:47:10 GMT</pubDate><author>陈姚戈,高允毅,王一鹏</author><category>工业</category><category>AI&amp;大模型</category></item></channel></rss>