<?xml version="1.0" encoding="UTF-8"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>InfoQ 推荐</title><link>https://www.infoq.cn</link><atom:link href="http://10.0.0.5:1200/infoq/recommend" rel="self" type="application/rss+xml"></atom:link><description>InfoQ 推荐 - Powered by RSSHub</description><generator>RSSHub</generator><webMaster>contact@rsshub.app (RSSHub)</webMaster><language>en</language><lastBuildDate>Mon, 09 Feb 2026 06:05:32 GMT</lastBuildDate><ttl>5</ttl><item><title>Rspack 1.7发布：2.0之前的最后一个1.x版本</title><description>&lt;p&gt;&lt;a href=&quot;https://rspack.dev/&quot;&gt;Rspack&lt;/a&gt;&quot;是一个基于&lt;a href=&quot;https://rust-lang.org/&quot;&gt;Rust&lt;/a&gt;&quot;的、旨在替代&lt;a href=&quot;https://webpack.js.org/&quot;&gt;webpack&lt;/a&gt;&quot;的高性能Web打包工具。Rspack 1.7版本发布，这是在项目过渡到2.0版本之前，1.x系列的最后一个小版本。该版本专注于提升现有功能的稳定性和插件的兼容性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7引入了多项增强稳定性的改进，包括：&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#improved-swc-plugin-compatibility&quot;&gt;增强SWC插件兼容性&lt;/a&gt;&quot;、&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#importing-assets-as-bytes&quot;&gt;原生支持以字节形式导入资源&lt;/a&gt;&quot;，以及固化多项&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#experimental-features-stabilized&quot;&gt;实验性功能&lt;/a&gt;&quot;。对于Web应用中动态导入的模块，该版本还引入了默认启用的懒编译。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack 1.7的一个新特性是改进SWC插件兼容性。在以前的版本中，由于AST结构不断演变，SWC Wasm插件面临着高昂的升级成本，使得现有插件在SWC升级后会出现问题。为此，Rspack团队&lt;a href=&quot;https://swc.rs/docs/plugin/ecmascript/compatibility&quot;&gt;向SWC社区贡献了兼容性改进&lt;/a&gt;&quot;，包括采用&lt;a href=&quot;https://www.rfc-editor.org/rfc/rfc8949.html&quot;&gt;cbor&lt;/a&gt;&quot;序列化方案来替代版本敏感的&lt;a href=&quot;https://rkyv.org/&quot;&gt;rkyv&lt;/a&gt;&quot;，并在AST中引入了用于枚举类型的Unknown变体，以提高容错性。从Rspack 1.7开始，SWC升级不大可能再破坏之前使用旧版本SWC构建的插件了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack现在原生支持&lt;a href=&quot;https://github.com/tc39/proposal-import-bytes&quot;&gt;Import Bytes提案&lt;/a&gt;&quot;，即以字节形式导入资源。开发者可以用Uint8Array导入资源，并使用TextDecoder进行解码，语法如下：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import fileBytes from &#39;./file.bin&#39; with { type: &#39;bytes&#39; };
const decoder = new TextDecoder(&#39;utf-8&#39;);
const text = decoder.decode(fileBytes);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从Rspack 1.7开始，在构建Web应用时，Rspack CLI针对动态导入模块默认启用&lt;a href=&quot;https://rspack.rs/blog/announcing-1-7#lazy-compilation&quot;&gt;懒编译&lt;/a&gt;&quot;。这一变化减少了初始构建中的模块数量，加快了开发服务器的启动速度。有特殊需求的开发者可以通过将lazyCompilation设置为false来显式地禁用这个功能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本中，有几项实验性功能已经被固化。常量内联优化现在已经稳定，并且在生产构建中默认启用，原来的experiments.inlineConst选项被optimization.inlineExports所取代。TypeScript枚举内联优化和类型re-export检查也已去掉了实验性标志，达到稳定状态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;迁移到Rspack 1.7时需要注意下SWC插件的版本。使用SWC Wasm插件的项目必须升级插件，以兼容swc_core 54或以上版本，以避免构建失败。在他们的&lt;a href=&quot;https://rspack.rs/guide/faq&quot;&gt;FAQ文档&lt;/a&gt;&quot;中，Rspack团队提供了处理SWC插件版本不匹配问题的指南。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack的定位是&lt;a href=&quot;https://rspack.rs/guide/compatibility/plugin&quot;&gt;兼容webpack&lt;/a&gt;&quot;的替代方案，其构建速度明显更快。根据&lt;a href=&quot;https://medium.com/@yarindeoh/boost-your-build-time-by-70-with-rspack-a2dd3c47697c&quot;&gt;Medium上一位用户的记录&lt;/a&gt;&quot;，从webpack迁移到Rspack后，构建时间减少了70%，本地构建时间从1.7分钟降低到30秒。另一个来自Mews的团队&lt;a href=&quot;https://developers.mews.com/goodbye-webpack-hello-rspack-and-80-faster-builds/&quot;&gt;报告&lt;/a&gt;&quot;说，启动时间从三分钟减少到十秒，提高了80%。然而，&lt;a href=&quot;https://github.com/rolldown/benchmarks&quot;&gt;Rolldown项目的基准测试&lt;/a&gt;&quot;显示，尽管Rspack的性能优于webpack，但它仍然比esbuild和Rolldown等工具慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这个版本还为更广泛的Rstack生态系统带来了更新：Rsbuild 1.7引入了运行时错误覆盖和资源大小差异报告；Rsdoctor 1.4新增用于包分析的树状图视图；Rslib 0.19稳定了打包模式中的ESM输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Rspack是一个由字节跳动开发的开源项目。该工具旨在提供与webpack相当的API兼容性，同时借助Rust语言实现性能提升。如果既不想脱离webpack生态系统，又想加速构建流程，那么这个工具很合适。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/rspack-final-rust/&quot;&gt;https://www.infoq.com/news/2026/01/rspack-final-rust/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</link><guid isPermaLink="false">https://www.infoq.cn/article/r3oIM8cNYt3VkqkMK3ez</guid><pubDate>Mon, 09 Feb 2026 05:51:07 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>微软推出面向.NET的Copilot自定义代理：C#专家与WinForms专家</title><description>&lt;p&gt;微软和GitHub扩展了Copilot生态系统，推出了&lt;a href=&quot;https://devblogs.microsoft.com/dotnet/introducing-custom-agents-for-dotnet-developers-csharp-expert-winforms-expert/&quot;&gt;首个专注于.NET的GitHub Copilot自定义代理&lt;/a&gt;&quot;，旨在提高C#和Windows Forms开发者的生产力和代码质量。作为更广泛的Copilot自定义代理发布计划的一部分，本次公告推出了两款专属代理：C#专家与WinForms专家，它们以代理指令Markdown文件的形式提供。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/CSharpExpert.agent.md&quot;&gt;C#专家代理&lt;/a&gt;&quot;旨在引导并强制执行现代C#最佳实践。它尊重项目约定，最小化不必要的代码工件，如未使用的接口或参数，并强调async/await模式要带有适当的取消和异常处理。它还支持行为驱动和集成测试工作流，帮助开发者编写更干净、更易于维护的代码。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/github/awesome-copilot/blob/main/agents/WinFormsExpert.agent.md&quot;&gt;WinForms专家代理&lt;/a&gt;&quot;专注于使用Windows Forms进行传统的桌面UI开发。对于常见的UI设计模式（如MVVM和MVP），它拥有专业的知识，能够协助处理复杂的事件连接（event wiring）和状态管理，并能够增加保护措施，防止Copilot无意中修改.Designer.cs文件，对Visual Studio设计器造成破坏。对于使用生成工具的开发者来说，这种保护解决了一直以来开发者经常遇到的一个痛点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要使用这些代理，开发者需要从&lt;a href=&quot;https://github.com/github/awesome-copilot&quot;&gt;GitHub awesome-copilot存储库&lt;/a&gt;&quot;下载CSharpExpert.agent.md和WinFormsExpert.agent.md文件，并将它们放在项目的.github/agents文件夹下。配置文件放置到位以后，在通过GitHub将问题分配给Copilot时就可以实现上下文感知行为，开发者可以在Visual Studio Code Insiders或Visual Studio的实验版本中通过下拉菜单选择代理。Copilot CLI计划在未来的更新中支持/agent命令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;微软将这两个代理都描述为实验性的，因为他们正在收集模型对详细指令的响应反馈。自11月以来，在开发者打开“启用特定于项目的.NET指导”这一功能时，Visual Studio 2022 Insiders 17.14.21版本可以自动将相关的自定义代理附加到项目，例如为Windows Forms开发量身定制的指令。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;早期的社交媒体评论，尤其是LinkedIn平台上的讨论，反映出人们对该自定义代理发布公告的热情与专业关注。有评论者强调，通过减少生成未使用的代码，可有效&lt;a href=&quot;https://www.linkedin.com/posts/elias-asaid_introducing-custom-agents-for-net-developers-activity-7392063443195015169-PKsx/&quot;&gt;缓解“AI引发的技术债务”问题&lt;/a&gt;&quot;。他还指出，WinForms Expert提供的设计器文件保护机制，对遗留用户界面的维护与现代化改造显然是有实际好处的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相比之下，Copilot自定义代理所采用的是一种更具结构化和持久性的AI辅助方式，与早期的 Copilot聊天模式或无上下文的建议引擎有着本质的不同。传统聊天模式提供的是按需帮助，而自定义代理则依据预定义的专业知识和行为特征在特定的存储库上下文中运行。这使得Copilot更符合新兴的&lt;a href=&quot;https://biilmann.blog/articles/introducing-ax/&quot;&gt;基于代理的开发体验&lt;/a&gt;&quot;，其中工具充当具有特定领域知识的合作伙伴，而非通用的助手。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前，自定义代理服务于.NET开发中小众但影响力大的场景。其实验性状态和不断演变的工具支持表明，在扩大覆盖范围或在更广泛的Copilot体验中标准化工作流之前，微软正在密切倾听开发者的反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&quot;&gt;https://www.infoq.com/news/2026/01/copilot-agents-csharp-winforms/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</link><guid isPermaLink="false">https://www.infoq.cn/article/pmf3ENWCc2ZgsMvpPX38</guid><pubDate>Mon, 09 Feb 2026 05:45:40 GMT</pubDate><author>作者：Edin Kapić</author><category>微软</category><category>编程语言</category></item><item><title>优步将分布式存储从静态限制转向基于优先级感知的负载控制</title><description>&lt;p&gt;优步的工程师介绍了他们&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management/&quot;&gt;如何将分布式存储平台从静态限流演进为优先级感知的负载管理系统&lt;/a&gt;&quot;，以保护其内部数据库。这一改进解决了大型有状态多租户系统中基于QPS限流的局限性，那就是这种限流方式无法反映真实负载、难以处理 “噪音邻居” 问题，也无法保障尾部延迟。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该设计保护了基于MySQL构建的&lt;a href=&quot;https://www.uber.com/blog/schemaless-sql-database/&quot;&gt;Docstore&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.uber.com/blog/schemaless-part-one-mysql-datastore/&quot;&gt;Schemaless&lt;/a&gt;&quot;存储系统，这些系统通过数千个微服务为超1.7亿月活用户（包括乘客、Uber Eats用户、司机和配送员）提供服务。通过优先保障关键流量并动态适应系统状态，该系统可防止级联过载，在大规模场景下维持性能稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步的工程师指出，早期基于配额的方案依赖集中式跟踪的静态限制，它的效果不佳。无状态路由层无法及时感知分区级负载，且相似大小的请求会产生不同的CPU、内存或I/O开销。运维人员需要频繁调整限流阈值，有时会误删健康流量，而过载分区却没有得到保护。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正如优步的工程师&lt;a href=&quot;https://www.linkedin.com/in/dhyanamvaidya/&quot;&gt;Dhyanam V.&lt;/a&gt;&quot;在&lt;a href=&quot;https://www.linkedin.com/posts/activity-7417190176965210112-b34i?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAArnikgBqzTxA9Y838-O55QUcB2McACIq94&quot;&gt;LinkedIn帖子&lt;/a&gt;&quot;中所述：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有状态数据库的过载保护是大规模场景下的多维度问题。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这个问题，优步将负载管理与有状态存储节点协同部署，结合了&lt;a href=&quot;https://queue.acm.org/detail.cfm?id=2209336&quot;&gt;受控延迟（Controlled Delay，CoDel）&lt;/a&gt;&quot;队列和租户级记分卡（Scorecard）。CoDel基于延迟调整队列行为，记分卡则强制实施并发限制，同时使用额外的调节器监控I/O、内存、goroutine和热点数据。CoDel对所有请求一视同仁，会同时丢弃低优先级和面向用户的流量，导致on-call负载增加、用户体验受损，并且依赖固定队列超时和静态的in-flight限制，可能引发惊群效应重试，甚至丢弃高优先级请求。尽管它能防止灾难性故障，但缺乏维持稳定性能所需的动态性和精细化能力，凸显了优先级感知队列的必要性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/22/2278d8fc2a95d1a318b31eb00726fbcb.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用CoDel队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;后续演进引入了Cinnamon，这是一款优先级感知的负载shedder系统，能够将请求分配到分级队列，优先丢弃低优先级流量，避免影响延迟敏感的操作。Cinnamon基于高百分位延迟指标动态调整in-flight中请求限制和队列超时，减少对静态阈值的依赖，在过载时实现更平滑的降级。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/df/dfd1b94d2ce9529fd3ceb189c34d2ed1.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;使用Cinnamon队列的负载管理器设置（来源：&lt;a href=&quot;https://www.uber.com/en-AU/blog/from-static-rate-limiting-to-intelligent-load-management//&quot;&gt;优步博客文章&lt;/a&gt;&quot;）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步后续通过“自带信号（Bring Your Own Signal）”模型，将本地和分布式过载信号统一到单一的模块化控制回路中。该架构允许团队将节点级指标（如in-flight中的并发数、内存压力）和集群级信号（比如，从节点的提交延迟）接入集中式的准入控制路径。整合这些信号消除了碎片化的控制逻辑，避免了早期基于&lt;a href=&quot;https://en.wikipedia.org/wiki/Token_bucket&quot;&gt;令牌桶&lt;/a&gt;&quot;系统中出现的冲突性负载shedding决策。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据优步介绍，改进效果非常显著，过载场景下吞吐量提升约80%，upsert操作的P99延迟降低约70%；goroutine数量减少约93%，峰值堆内存使用降低约60%，整体效率得到了提升，同时缓解了运维的负担。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;优步总结了负载管理演进的核心经验，那就是优先保障关键用户流量，先丢弃低优先级请求；尽早拒绝请求以维持可预测延迟、降低内存压力；使用基于PID的调节确保稳定性；将控制逻辑部署在数据源附近；动态适应工作负载；保持可观测性；优先采用简单设计，确保压力下的稳定可靠运行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/uber-priority-aware-load-manager/&quot;&gt;Uber Moves from Static Limits to Priority-Aware Load Control for Distributed Storage&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</link><guid isPermaLink="false">https://www.infoq.cn/article/HCcbfQRWSxXzx59zMIhG</guid><pubDate>Mon, 09 Feb 2026 05:43:18 GMT</pubDate><author>作者：Leela Kumili</author><category>云计算</category></item><item><title>“千问奶茶”二手平台6元转售；追觅俞浩：年终奖最高20个月奖金，总量会达到10亿级；京东001 号快递员：退休金 4000 多，存款百万 | AI周报</title><description>&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;追觅俞浩：年终奖最高 20 个月奖金；马云现身阿里总部千问春节项目组；京东物流公开 001 号快递员退休生活；钉钉大楼换 LOGO 硬刚飞书；顶级域名 AI.com 被币圈人士以 7000 万美元拍下；快手回应被罚 1.191 亿元；美团 49.8 亿收购叮咚买菜；甲骨文被曝或裁员 3 万人；蚂蚁数科 CEO 赵闻飙发全员信；Anthropic 和 OpenAI 因投放广告隔空嘲讽；高通骁龙 Oryon CPU 架构之父宣布离职……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;行业热点&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;追觅俞浩：年终奖最高20个月奖金，总奖金规模会达到10亿量级&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月7日消息，追觅科技创始人兼CEO俞浩连发两条微博回应“演唱会投入过多”质疑，俞浩表示，演唱会几千万的投入，仅仅相当于公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万。他还透露，这两天在审批各个事业部递交过来的年终奖方案。主营业务，公司把净利润的18%作为奖金发放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！年终奖的总奖金规模，会达到10亿量级。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以下为其微博原文：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;有人说：把这么多钱花在演唱会上，不如好好花在产品研发上。那你是不知道我们在产品研发上投入了多少。&amp;nbsp;演唱会几千万的投入，仅仅相当于我们公司一天的研发费用投入。追觅现在大约2万名研发管理人员，每天的研发投入大概需要是4000万，你没看错，是每天。也就说，我每天一阵开眼，至少要有4000万花在研发费用和员工工资上。&amp;nbsp;还有人说：不如把演唱会的花费，直接发给大家实在。正好这两天，我在审批各个事业部递交过来的年终奖方案。主营业务，我们把净利润的18%作为了奖金发给了大家，这是很高的比例！因为我们今年的主营业务的净利润是全行业最高的。这是纯现金部分，还没有算平时的任何福利。盈利最高的部门能拿到11个月的奖金，最高的个体预计会有20个月的奖金！（当然这是最优秀的）&amp;nbsp;我们的年终奖的总奖金规模，会达到10亿量级。我们对人才的投入是不遗余力的，但凡条件允许，我都会投给人才！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;马云现身阿里总部千问春节项目组，二手平台惊现“千问奶茶”6元转售&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;近期，阿里员工分享马云现身阿里千问项目组办公点。2月6日一早，千问APP“春节30亿大免单”正式上线，发动奶茶攻势，邀请全国人民用AI一句话免费点奶茶。由于活动过于火爆，导致大量用户涌入，很快把APP挤爆了。不少用户打开活动页面，却发现“千问请客”页面无法点击，页面信息显示“活动太火爆，请稍后再试”。领到免单卡后让它去点单，AI也回答：当前使用千问闪购点单的人数较多，我正在全力处理中，建议稍等片刻后再试一次。随后，APP红包分享链接已被微信屏蔽，口令已无法复制使用。好在除相关活动外，问答功能仍可正常使用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当日，千问APP“春节30亿免单”上线5小时突破500万单，并超越豆包和元宝，登顶苹果App Store免费榜，排序形成“千元豆”格局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据媒体在北京某商圈搜索查询发现，受活动爆单影响，多家奶茶门店因单量超负荷，已临时暂停营业。千问APP活动也带动港股茶饮股多数上涨，截至发稿，古茗涨超4.12%创上市来新高，沪上阿姨和茶百道涨超3%，奈雪的茶、蜜雪集团也跟随上涨。&amp;nbsp;不过，港股阿里巴巴当天低开3.82％，收盘跌2.9%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，有人在二手交易平台上卖起了千问奶茶，售价6元至10元不等，声称只需提供收货地址和电话，即可代顾客下单购买。据媒体报道，千问客服对此回应称，该免单权益属于虚拟优惠，不支持转让、转赠、转售或任何形式的变现，因此无法在二手交易平台出售。如发现用户存在倒卖、恶意套现等行为，主办方有权取消其参与资格，并冻结或收回其全部活动权益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问此前宣布将投入30亿元启动“春节请客计划”，以免单形式请全国人民在春节期间吃喝玩乐，感受AI时代的全新生活方式。淘宝闪购、飞猪、大麦、盒马、天猫超市、支付宝等多项阿里生态业务，也将联动加入千问春节攻势。2月7日，千问宣布免单卡可以在千问App里买天猫超市的酒水零食、米面粮油、家居日用、生鲜水果等，同时还将免单卡的有效期延长至2月28日。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;千问在春节活动的第一天，微信链接被屏蔽，部分用户在千问APP点击分享活动至微信好友时，已自动改为复制口令形式。此外，腾讯元宝、百度文心助手红包分享链接也被微信屏蔽。值得关注的是，有消息称，马化腾很重视此次元宝红包活动。他在腾讯年会上，表示希望重现当年微信红包的盛况。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;京东物流公开 001 号快递员退休生活：退休金 4000 多，存款百万&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;京东黑板报微信号2月6日发文，公开了001号快递员金宜财退休后的生活状况。京东透露，金宜财现在过得充实富足，每月4000多元养老金准时到账，还靠着打拼积攒下的积蓄和理财存到了一百多万。退休前，他已帮两个儿子在南京、无锡成家置业。金宜财表示，“房子不缺，车也有了，就是浑身还有使不完的劲”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;钉钉大楼换LOGO硬刚飞书，网友：商战总是朴实无华！&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月4日消息，据媒体报道，钉钉又整活了，这次直接把杭州总部大楼的 LOGO 给换了。网友：好朴实的商战！从新旧 LOGO 对比图可以看出，钉钉直接蓝色翅膀+文字，改成了一个头戴凤翅紫金冠、扛着金箍棒的齐天大圣版“钉三多”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至于更换 LOGO 的原因，网传钉钉 CEO 陈航（花名无招）发现，隔壁飞书大楼的 LOGO 比自家的高了一截，心里不服气，索性把 LOGO 换成孙悟空硬刚。据悉，钉钉和飞书的总部大楼都在杭州未来科技城，两家仅隔一条马路，飞书 LOGO 的高度确实略胜一筹。报道称，恰巧钉钉有个核心团队叫 “西游记团队”，选孙悟空的形象再合适不过，选孙悟空形象贴合企业文化，又因飞书LOGO形似飞鸟，暗合“孙悟空棒打出头鸟”梗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友调侃：“风水大师介入的结果哈哈”、“之前是鸟吃蚊子，现在是棒打鸟，哈哈哈”。网友也纷纷为飞书出招，建议其在斜对面设如来手掌 LOGO “压制”齐天大圣。据了解，钉钉向来是品牌界“整活高手”，吉祥物“钉三多”此前就成“网红”，和多邻国小绿鸟“暧昧”“有孩子”，偶尔拉上淘宝淘公仔演 “三角恋”，玩梗玩得飞起。甚至在杭州亚运会期间，钉钉也紧跟热点，把 LOGO 换成紫红渐变色，谐音“紫钉行”讨彩头。报道称，钉钉这波 LOGO 更换，或是品牌趣味商战的一种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;顶级域名 AI.com 被币圈人士以 7000 万美元拍下，创域名交易价格记录&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;域名交易领域再创新高，超级热门的人工智能域名 AI.com 被币圈人士以 7000 万美元天价买下。该域名最初注册于 1993 年 5 月 4 日，此前多次交易未作为产品网站，ChatGPT 发布后持有者将其跳转到不同 AI 工具蹭热度。Crypto.com 创始人 Kris Marszalek 于 2025 年 4 月买下该域名，目标是推出基于去中心化且能持续自我改进的人工智能代理网络，加速通用人工智能 AGI 到来。网站尚未开放，从倒计时看正式发布时间在 2 月 9 日前后。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;快手回应被罚1.191 亿元&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据网信北京公众号消息，2 月 6 日，北京市互联网信息办公室宣布对北京快手科技有限公司处以警告并罚款 1.191 亿元人民币。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;处罚原因是快手平台未履行网络安全保护义务，未及时处置系统漏洞等安全风险，未对用户发布的违法信息立即采取停止传输、消除等处置措施，情节严重，影响恶劣。快手方面此前将此次事件归咎于黑灰产攻击，但监管部门显然不接受这一说辞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月6日，快手发布声明称，网信部门公布了对快手作出的处罚。对此，公司诚恳接受，坚决整改。由于公司技术管理原因，应急处置不及时，导致平台出现大量色情低俗内容，造成恶劣影响。事件发生后，公司全面排查风险意识、安全基建、应急响应和内部管理等方面存在的问题，积极采取多种措施补齐短板。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;美团 49.8 亿收购叮咚买菜&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，美团在港交所发布公告，宣布以约 7.17 亿美元（约 49.8 亿元人民币）的初始对价，完成对叮咚买菜中国业务 100% 股权的收购。对于收购原因，美团在公告中表示，公司高度重视食杂零售业务，本次交易符合公司在食杂零售领域的长期发展规划。截至 2025 年 9 月，叮咚买菜在国内共运营超过 1000 个前置仓，月购买用户数超过 700 万。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;叮咚买菜创始人梁昌霖也在内部信中表示，对于未来的发展选择放下竞争，转为并肩合作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8d/8d37db27d1a998e4060d15854a59f2e3.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;甲骨文被曝或裁员3万人，多家美银行停止相关数据中心项目贷款&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据媒体报道，据道明证券旗下投行TD Cowen分析，由于人工智能数据中心扩张面临融资困难，科技巨头甲骨文公司正陷入严峻的资金压力，公司考虑采取大规模裁员及出售部分业务等措施来应对。TD Cowen研究报告显示，甲骨文公司计划裁员2万至3万人，此举预计将释放80亿至100亿美元的现金流。此外，甲骨文还在考虑出售其于2022年以283亿美元收购的医疗保健软件部门Cerner。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;蚂蚁数科 CEO 赵闻飙发全员信，宣布将成立“大模型技术创新部”&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日晚间消息，新浪科技获悉，日前，蚂蚁数科 CEO 赵闻飙发布主题为《携手共进，迈向大模型新时代——关于大模型组织架构升级的通知》的全员信，宣布蚂蚁数科将成立“大模型技术创新部”，构建面向 To B 场景的基础大模型及行业模型。据了解，该团队将与蚂蚁集团相关团队协同，攻坚蚂蚁集团百灵大模型面向 To B 场景的商业化，推动全球企业更好地进入 AI 时代。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic 和 OpenAI 因投放广告隔空嘲讽&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间周三，Anthropic 首次在“超级碗”(Super Bowl) 比赛里打广告，并且将矛头对准了竞争对手 OpenAI。广告内容围绕在人工智能对话助手中进行对话时，不合时宜的广告可能对用户产生的不适影响，嘲讽 OpenAI 将在对话中加入广告。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 同时表示，永远不会在对话中插入广告。广告模式可能与 Claude 的核心原则产生冲突。Anthropic 举例说明：当用户提到睡眠困难时，无广告的助手会基于用户需求探索各种可能原因；而广告支持的助手则可能考虑对话是否存在交易机会。这种激励结构会让用户难以判断 AI 的建议是否带有商业动机。Anthropic 表示将继续通过企业合同和付费订阅获得收入，并将收益投入 Claude 的改进。该公司同时在探索智能体商务等功能，允许 Claude 代表用户完成购买或预订，但所有第三方交互都将由用户主动发起。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 的 萨姆·奥特曼于2 月 5 日凌晨在 X 上回应了 Anthropic，称其内容不实且具有误导性。重申 OpenAI 致力于让每个人都有接触人工智能的权利，同时提到 Codex 下载已经超过 50 万次。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;高通骁龙Oryon CPU架构之父宣布离职&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日消息，NUVIA创始人、高通自研Oryon CPU首席架构师杰拉德·威廉姆斯三世宣布离职，结束他在高通四年的职业生涯。高通此前斥资1.4亿美元收购NUVIA公司，NUVIA创始人杰拉德·威廉姆斯三世随后入职高通。在他的参与下，搭载第三代Oryon内核的骁龙8 Elite Gen5（第五代骁龙8至尊版）得以问世。凭借NUVIA带来的技术资产，高通的Oryon自研架构已成为抗衡苹果Silicon核心的秘密武器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在此之前，杰拉德·威廉姆斯三世曾在Arm工作了12年，是开发Cortex-A8和Cortex-A15等多个核心架构的关键人物。在入职苹果公司后，他成为苹果A7-A12X自研芯片组的首席架构师，拥有丰富的芯片设计经验，而且他是苹果公司60多项专利的共同发明人，涉及功耗管理和多核技术。离开苹果公司之后，杰拉德·威廉姆斯三世和合伙人共同创立了NUVIA公司，这家公司后来被高通收购，杰拉德·威廉姆斯三世顺利加盟高通，在高通工作四年之后，他最终选择了离职。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;杰拉德·威廉姆斯三世在社交平台上表示，我现在正享受与家人团聚的珍贵时光，我在高通的旅程已经画上句号，感谢过去四年里与我并肩作战的所有人，现在人生的新篇章开始了——就从粉刷我的房子和完成那张攒了很久的任务清单开始吧。感谢NUVIA那些了不起的朋友和同事们，是你们让这段旅程成为可能。杰拉德·威廉姆斯三世离职后，高通在自研CPU的开发上是否会陷入困境尚无定论，高通很可能在他任职期间聘请并培养了多位人才，足以在他辞职后填补空缺。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;智元不参加春晚，将举办全球首个大型机器人晚会&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 2 日消息，知情人士称，智元将不参加 2026 年马年春晚。原因是预算有限的前提下，智元优先保障具身智能技术及产品研发的费用。另据知情人士透露，智元机器人正在彩排全球首个大型机器人晚会“机器人奇妙夜”，该晚会由数百台各类机器人主导，集合唱歌、跳舞、小品、走秀等多元节目，将于近期上线直播。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;大模型一周大事&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;重磅发布&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;OpenAI 推出 macOS 版 Codex 应用和企业级平台 Frontier&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 3 日消息，OpenAI 推出了适配 macOS 系统的全新 Codex 应用，整合了过去一年间广泛流行的各类智能体化开发逻辑。这款新应用支持多智能体并行作业，可融合不同智能体的能力，以及当前最前沿的工作流程。此次发布距离 OpenAI 推出其最强编码大模型 GPT-5.2-Codex，尚不足两个月，公司希望凭借该模型吸引 Claude Code 的用户。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OpenAI 首席执行官萨姆・奥特曼在媒体电话发布会中表示：“若要处理复杂场景下的高精尖开发工作，GPT-5.2 是目前性能最强的模型。但它此前的使用门槛偏高，因此我们认为，将这款模型的强大能力封装进更灵活的交互界面，会具备极为重要的价值。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这款 Codex 应用还搭载了多项全新功能，这些功能将帮助其达到与各类 Claude 应用相当的水平，部分场景下甚至实现反超。应用支持设置自动化任务，可按预设计划在后台自动运行，执行结果会存入队列，待用户返回后统一查看。用户还能根据自身工作风格，为智能体选择不同交互风格，从务实理性型到共情沟通型均可切换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，OpenAI 周四发布新的人工智能平台 Frontier，该平台可以帮助公司构建、部署和监督 AI 智能体。OpenAI表示，Frontier 与之前发布的 AI 智能体构建工具协同工作，让企业能更轻松地整合智能体执行任务所需的数据源。OpenAI 称，这些智能体将能够处理来自各种来源的信息，并完成处理文件和运行代码等任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Anthropic发布Claude Opus 4.6&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当地时间2月5日，Anthropic宣布，推出升级版智能模型Claude Opus 4.6，该模型能更谨慎地规划，更长时间地执行代理任务，在大规模代码库中可靠运行，并能纠正自己的错误Anthropic称，这款名为Claude Opus 4.6的版本能够检视企业数据、监管备案文件和市场信息，并生成详细的金融分析报告，通常这类工作通常需要人工耗时数天才能完成。该消息发布后，金融服务公司股价应声下跌，FactSet跌幅一度高达10%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阿里千问发布 Qwen3-Coder-Next：低推理成本编程智能体模型&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日消息，阿里巴巴千问宣布推出 Qwen3-Coder-Next，一款专为编码代理与本地开发打造的开放权重的语言模型。该模型基于 Qwen3-Next-80B-A3B-Base 构建，采用混合注意力与 MoE 的新架构；通过大规模可执行任务合成、环境交互与强化学习进行智能体训练，在显著降低推理成本的同时，获得了强大的编程与智能体能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Qwen3-Coder-Next 不依赖单纯的参数扩展，而是聚焦于扩展智能体训练信号，使用大规模的可验证编程任务与可执行环境进行训练，使模型能够直接从环境反馈中学习。该配方强调长程推理、工具使用以及从执行失败中恢复，这些对现实世界中的编程智能体至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;面壁智能首款 AI 硬件“松果派”官宣今年上市&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 5 日，面壁智能官宣，面壁首款 AI 硬件松果派（Pinea Pi）—— 一款 AI 原生（AI Native）的端侧智能开发板将于今年上市，帮助开发者快速开发端侧智能硬件，即使无技术背景，也可快速上手开发。面壁智能介绍称，“松果派”源自《三体》，松果鳞片也如斐波那契数列，犹如“二向箔”对于“维度可被折叠和展开”的隐喻，寓意着以更高的维度、更全的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;松果派将支持离线多模态个人知识助理、具身智能、编程教具等场景的全栈开发，面壁智能多模态大模型 MiniCPM-V、全模态大模型 MiniCPM-o 开箱即用。松果派基于 NVIDIA Jetson 系列模组打造，内置麦克风、摄像头、丰富的接口等多模态硬件组件，便于开发者开发和调用，构建了一套软硬一体、全栈覆盖的端侧 AI 软件体系。据悉，松果派预期年中正式量产上市，定价暂未公布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;阶跃星辰开源 Step 3.5 Flash&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;国产大模型开源阵营又添一员。2月2日，阶跃星辰发布Step 3.5 Flash，定位为“为Agent而生”的开源基座模型，主打推理速度、Agent能力和长链条任务稳定性。这款模型的参数总量达到1960亿，但采用稀疏MoE架构，每个token仅激活约110亿参数。配合MTP-3多token预测机制和3:1滑动窗口注意力架构，官方宣称推理速度最高可达350 TPS，支持256K上下文长度。核心卖点是三个词：更快、更强、更稳——快在推理速度，强在Agent和数学任务表现，稳在复杂长链条任务的可靠性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;部署方式上，阶跃星辰这次给得很全。开发者可以通过OpenRouter限时免费调用API，也可以从GitHub和HuggingFace下载模型权重自行部署。普通用户则可以在阶跃AI的App和网页端直接体验。值得注意的是本地部署的支持范围。官方表示已专门优化本地运行性能，支持在个人工作站上流畅运行，兼容设备包括NVIDIA DGX Spark、Apple M3/M4 Max以及AMD AI Max+ 395。一个1960亿参数的模型能在消费级硬件上跑起来，背后是稀疏激活架构带来的实际计算量压缩，110亿的激活参数让这件事成为可能。阶跃星辰还透露，Step 4模型已启动训练，并开放Discord社区邀请开发者参与共创。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;摩尔线程AI Coding Plan上线&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月3日，摩尔线程正式推出AI Coding Plan智能编程服务。作为首个基于国产全功能GPU算力底座构建的智能开发解决方案，该服务以MTT S5000强劲的全精度计算能力为核心驱动，融合硅基流动推理加速引擎，并集成GLM-4.7顶尖代码模型，实现了国产芯片与国产大模型在AI Coding领域的关键突破。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;全球“最快”人形机器人发布&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2月2日，浙江大学杭州国际科创中心人形机器人创新研究院正式发布全尺寸人形机器人Bolt。该机器人以10米/秒的奔跑时速，成为目前全球跑得最快的人形机器人。该成果由科创中心联合镜识科技、凯尔达共同研发完成。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;企业应用&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;2 月 4 日，2026年支付宝集福活动首次上线智能穿戴设备扫福功能。用户佩戴夸克、乐奇Rokid等品牌的AI眼镜，无需操作手机，通过“看一下”配合“说一句”的简单方式，即可完成扫福、集卡、分享等全流程。2 月 3 日，苹果公司宣布，将在旗下旗舰编程工具Xcode中引入智能体编程功能。借助智能体驱动的编程技术，程序员可让人工智能软件自主完成代码编写工作。苹果表示，其Xcode工具将支持Anthropic的Claude智能体和OpenAI的Codex代码工具。&lt;/p&gt;</description><link>https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</link><guid isPermaLink="false">https://www.infoq.cn/article/pyihcXSNdvwyl0t86j5f</guid><pubDate>Mon, 09 Feb 2026 02:34:09 GMT</pubDate><author>傅宇琪,褚杏娟</author><category>AI&amp;大模型</category></item><item><title>“公司终局是纯 AI、纯机器人！”马斯克酒后激进预言：一小时一发 Starship，让Optimus 造Optimus 巨便宜</title><description>&lt;p&gt;近期，马斯克参与了一场近3个小时的深入对话，讨论了太空数据中心的经济效益、地球电力规模化挑战、在美大规模生产人形机器人需要的条件，其中他讲解了很多关于工程和供应链的细节。此外，他也透露了 SpaceX 和 xAI 的商业模式和战略规划，此外还分享了自己的管理哲学。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;马斯克开篇就指出，把算力搬到太空，根本不是为了省电费，而是为了给“电不够用”这件事找一个终局解：芯片算力在指数级增长，但地面发电扩张跟不上，地面扩张的阻力远大于太空。至于“太空里 GPU 坏了就报废”的质疑，他的回应是维修不是关键，关键在前期筛选与稳定后的可靠性，&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，很多软件从业者很快会被硬件“教育”：未来的瓶颈不是模型，而是电力、变压器、电网、燃气轮机叶片这类物理供给链，想扩就扩不了；制造能力才是底层瓶颈，甚至逼到 Tesla/SpaceX 可能要自己做涡轮机叶片和导流片。因此，把 AI 放到太空反而可能成为生成 token 最便宜、扩展最容易的方式，他甚至给出激进判断：五年后太空每年新增并运行的 AI 总量，会超过地球历史累计总量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了跑赢这些瓶颈，他的管理逻辑依旧是“哪里卡就打穿哪里”：如果瓶颈是钱，就去解决钱，IPO 的核心价值不是估值而是速度；对于当前的AI公司，他犀利指出他们不该叫自己实验室，本质上是收入最大化的公司，绝大多数工作是工程落地与规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在马斯克眼里，最终落点则更冷酷：最强的公司形态会是纯 AI + 纯机器人闭环，人类留在流程里就像让人去算 spreadsheet 的一部分格子，只会更慢、更差，闭环效率才会决定胜负。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;下面是这次详细对话，我们进行了翻译并在不改变原意基础上进行了删减，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI上太空，芯片不是关键&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你比谁都清楚，数据中心的总拥有成本里，电力只占百分之十到十五。你把算力搬到太空，省下的主要就是这部分。但真正的大头是 GPU，而在太空里基本没法维修，一坏就报废，折旧周期更短，成本反而更高。所以把 GPU 放到太空，本质上更贵。那到底为什么要放到太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：核心问题是能源。你看看中国以外的地区，发电量基本是持平的，最多小幅增长，只有中国在快速增长。如果数据中心不建在中国，那电从哪来？尤其是规模越来越好大。芯片算力几乎是指数级增长，但发电量基本不动，你怎么给这些芯片供电？难道靠什么“魔法电源”“电力小精灵”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直是太阳能的坚定支持者。如果做到1太瓦（TW）太阳能，就算按25%效率算，也只需要4太瓦面板，占美国国土面积的1%左右。等我们有一太瓦数据中心的时候，是不是已经进入“奇点”了？我们走到哪一步了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，我们可能已经开始进入所谓的“奇点”，但离终点还很远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以，是不是先把内华达铺满太阳能，再考虑上太空？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：在地面铺太阳能，最大的难点是审批。你去试试拿许可就知道了，特别难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以太空，其实是监管上的捷径？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不只是监管。地面扩张难度远大于太空。而且在太空里，太阳能效率大概是地面的五倍，还不需要电池。我本来差点穿一件写着“太空永远大晴天”的衣服。这是事实，那里没有昼夜循环、没有季节、没有云层，也没有大气层。要知道，光大气层就会损失大约30%的能量。所以在太空里，同样的太阳能板，发电能力是地面的五倍，还省掉储能成本。综合下来，其实更便宜。我预测，未来三十到三十六个月内，把 AI 放在太空，将是成本最低的方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 GPU 坏了怎么办？训练时坏得挺频繁的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：关键在于前期筛选。我们现在发现，GPU 的可靠性其实挺高。可以先在地面跑测试，筛掉问题芯；等过了调试期，不管是 NVIDIA、Tesla AI 芯片、TPU，稳定后都很可靠，所以维修不是关键问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我敢打赌，三十个月左右，太空会成为最具经济性的 AI 部署地。而且，真正能无限扩展的地方，只有太空。当你开始考虑“我们能利用太阳多少能量”时，就会发现，地球根本不够用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的规模，是太瓦级别？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。美国全国平均用电量也就半太瓦，一太瓦等于两个美国。你能想象建这么多电厂、数据中心吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多做软件的人，根本没意识到自己马上要被硬件“教育”了。建电厂极其困难，不仅要电厂，还要变压器、电网设备。公用事业公司本身节奏非常慢，还要层层审批，你要跟他们签一个大规模并网协议，可能一年后才给你答复。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们不是自己搞了“表后电力”系统吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，我们在 xAI 就这么干过。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不把电厂和 GPU 一起建？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们就是这么做的。但问题是，电厂设备从哪来？制造能力本身是瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：是燃气轮机排期的问题？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：更底层的问题是叶片和导流叶片。它们的铸造工艺极其复杂，是关键瓶颈。现在只有少数几家公司能做，而且全都排满了。太阳能理论上能扩展，但美国对进口太阳能的关税很高，本土产能又很弱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那干脆自己造太阳能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们正在做。SpaceX 和 Tesla 都在向100吉瓦级产能推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从多晶硅到电池板，全产业链？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，必须全做。而且太空用太阳能板更便宜，不需要厚玻璃、不需要重型框架，也不用抗风雨。没有天气影响，结构可以很轻。所以，太空用电池板反而更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：三年内能做到足够便宜吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在太阳能已经非常便宜了。中国大概两三毛钱一瓦。放到太空后，乘以效率优势，再去掉电池，综合下来有接近十倍优势。一旦发射成本下来，太空将是生成 token 最便宜、最容易扩展的方式。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而在地面扩展迟早会撞上电力天花板。我们为了上线一吉瓦电力，在 xAI 做了大量工作：买涡轮机、解决审批。我们在田纳西受阻，又跑去密西西比建厂，还要拉高压线，难度极大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多人根本不懂，一个数据中心真正需要多少电。除了 GPU，还有网络设备、CPU、存储、散热。尤其是散热，在最热的时候也要顶得住。在孟菲斯这种地方，光散热就多出40%能耗，再加上设备检修冗余，又得多准备二三成。综合下来，一万个 GB300 规模的数据中心，大约要300兆瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：再说一遍？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：大概三十多万块 GB300，加上所有配套和余量，发电量要接近一吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我问个外行的问题。地面有工程难题，太空也一样。通信、辐射、防护，这些怎么解决？为什么你觉得这些比多建电厂更容易？毕竟，地面已经有成熟厂商在造涡轮机。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你试试就知道了。现在涡轮机的排期已经到了 2030 年。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你们考虑自己造吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：为了拿到足够电力，SpaceX 和 Tesla 可能最终要自己做叶片和导流叶片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：只做叶片？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，其他都能买到，唯独叶片和导流叶片最难。全球只有三家铸造厂能做，订单全都严重积压。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这些铸造厂是 Siemens、GE 之类的大公司，还是分包商？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：主要是其他专业公司。有些整机厂自己也有一点铸造能力，但规模都不大。你随便打电话问一家做涡轮机的厂家，他们都会告诉你现状，这不是什么秘密，网上基本都能查到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Colossus 会不会主要靠太阳能供电？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那样会容易得多。但现在关税高得离谱，有些产品甚至是几百个百分点。而且你也知道，现在的政府，对太阳能并不是特别友好。再加上土地、审批这些问题，如果你想快速扩张，现实阻力非常大。我确实认为，在地面发展太阳能是对的方向，但你需要时间去找地、拿许可、配套储能系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那为什么不自己把太阳能产能拉起来？你说以后会缺地没错，但现在德州、内华达还有大量土地，很多还是私有土地。至少可以先撑起下一代、下下一代 Colossus，等真到瓶颈再说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：正如我说的，我们正在扩张太阳能产能。但实体制造的扩张是有极限速度的，不可能无限加速，我们已经在用最快速度扩大本土生产。Tesla 和 SpaceX 都有明确目标：做到每年一百吉瓦的太阳能产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;太空运行的AI 量会超地球， SpaceX 成超级算力供应商&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说到产能，我很好奇，五年后地面和太空的 AI 装机规模会是什么比例？我特意选五年，因为那时你的系统应该已经跑顺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：五年后，我的判断是，每年在太空部署并运行的 AI 总量，会超过地球上历史累计总量。也就是说，每年新增的太空算力，会比地面过去所有加起来还多。我预计，五年后，太空 AI 装机规模会达到每年几百吉瓦，并持续增长。在地球上，你最多也就做到一太瓦左右，再往上就会遇到火箭燃料等瓶颈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：按这个规模算，包含太阳能阵列、散热系统等，大概需要一万次 Starship 发射。也就是说，一年一万次，相当于每小时一发。你能描述一个“每小时都在发射星舰”的世界吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：其实这和航空业比，还算低频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但飞机有很多机场，而且火箭还涉及轨道问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不一定非要极轨，飞得够高就能避开地影区，限制没那么大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：一年一万次发射，大概需要多少艘 Starship？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不需要太多，理论上二三十艘就够了，关键看周转效率。如果一艘船三十小时能周转一次，三十艘就能跑满。当然，我们会造更多。SpaceX 正在为每年一万次，甚至两三万次发射做准备。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你的目标是不是把 SpaceX 做成太空算力的“云服务商”？像 Oracle、AWS 那样，把算力租给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果我的预测成立，SpaceX 在太空部署的 AI 算力，会超过地球上所有机构加起来的总和。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：主要是推理算力，还是训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：绝大多数都会是推理，现在已经是这样了，推理规模远超训练。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;SpaceX IPO，速度解决钱的问题&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在外界有一种说法：关于 SpaceX IPO 的讨论升温，是因为以前你们非常“资本高效”，花钱不多。但接下来，你们可能需要的资金规模，已经超过私募市场能承受的范围，就算 AI 实验室能融到几十亿美元，也有上限。是不是以后每年都要超过百亿美元？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我对讨论潜在上市公司一直比较谨慎。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就泛泛而谈一下吧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这可不像你，Elon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：说话是要付出代价的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那从宏观角度看，公募和私募市场的资金深度差别有多大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：总体来说，公募市场的资金量，远远超过私募，至少多两个数量级，可能是一百倍以上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：但像房地产这种高度资本密集行业，往往主要靠债务融资。因为当规模大到一定程度，其实现金流已经比较稳定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个问题没法简单回答。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，就是短期回报这件事。你看数据中心扩建，很多都是靠私募信贷在融资。那为什么不直接用债务融资？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只看一件事：速度。我做事的习惯是反复盯住“限制速度的瓶颈”，然后把它打穿。如果唯一瓶颈是钱，那我就去解决钱；如果钱不是瓶颈，那我就去解决别的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但按你过去谈 Tesla、谈上市公司的态度，我原本以为你不会觉得“想快就得上市”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：通常确实不是。我其实想讲得更具体一点，但问题是一旦你公开讨论“可能要上市的公司”，就会惹麻烦，甚至影响发行节奏，最后反而拖慢速度。所以我们得谨慎一点。但有些东西是可以公开讲的，比如物理规律。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从长期扩张角度看，地球接收到的太阳能，只占太阳总能量的极小一部分。太阳几乎是宇宙里最主要的能源来源，这点必须先看清。有人会讨论多建核电、搞聚变之类的“边际方案”，但你退一步想：如果你想利用太阳能里一个并不夸张的比例，比如百万分之一，听起来挺酷，但对应的电力规模大约是人类文明目前总发电量的十万倍量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结论很直接：真正能规模化的，只有去太空用太阳能。从地球发射，把太空算力推到每年一太瓦左右，差不多就是极限。再往上，你得从月球发射，得在月球搞“mass driver”（电磁弹射器）这类东西，那样可能做到每年拍瓦级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：可在到达这个规模之前，你肯定会先撞上别的瓶颈，你要芯片，要逻辑、要内存。太阳能板效率提高了不代表这些就不缺了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：芯片得做更多，而且得更便宜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那问题来了，现在全球算力也就几十吉瓦级别，你怎么把逻辑算力拉到太瓦级？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：那就得做一件“非常大”的事情。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是要放大招了，讲讲？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我公开提过一个想法：做“超大规模”的芯片产能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：Tesla 的命名一直很抓人。你现在是按“计量单位”来命名了吗？更关键的是你打算做到产业链哪一层？建洁净室？和谁合作拿制程？设备怎么买？计划到底是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你不能指望跟现有 fabs （半导体加工厂）合作解决产能问题，他们的输出不够，规模差得太远了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那就合作拿 IP、拿工艺？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在的 fabs，本质上离不开几家设备公司：ASML、Tokyo Electron、KLA、Applied Materials、Lam Research 这些。一开始你得用他们的设备，而且可能要跟他们一起把产能拉上去。但要到真正规模化，你必须用一种“不同的方式”建 fab：先用常规设备、非常规方法把规模跑起来；再逐步改造设备，加快速度提高产出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有点像 The Boring Company （马斯克2016年创立的隧道挖掘公司）的打法：先买现成盾构机，先把隧道挖起来，再自己做一台快一个数量级的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，就是这个逻辑。先跑通，再重做，最后提速到数量级提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中国关键在“复制 ASML”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：领先制程芯片、先进涡轮发动机这些，中国还在追，中国都没复制出 TSMC，会不会让你对“建 fab 的难度”更谨慎？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不完全同意，关键瓶颈不在“复制 TSMC”，而在“复制 ASML”，那才是最卡脖子的地方。我认为中国会在未来几年做出相当有竞争力的芯片。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你会考虑自己做 ASML 那种设备吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现在还不好说。但如果我们想在三十多个月内把产能拉到极高规模，必须把“火箭送上去的能力”和“能供得上电、供得上芯片”的能力配平。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;假设我们把上天的运力做到百万吨级，再按每吨对应十万瓦级别需求算，那就意味着：每年至少要新增百吉瓦级太阳能，同时还得有同量级的芯片供给去匹配。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而我真正担心的，其实是内存。芯片怎么扩产，路径相对清晰，“足够的内存”更难。这也是为什么你会看到 DDR 价格起飞。网上还有那种段子：被困荒岛写“DDR”求救，结果船就来了，因为大家都在抢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听上去你觉得那些掌握细节的人，比如那些知道等离子腔体里放什么气、工具参数怎么调的人，并不是不可替代的。你的思路更像：先把洁净室和设备弄齐，然后把流程跑出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这事不靠一堆 PhD。工程大多数时候也不是 PhD 在做。这类工作也不需要 PhD，但确实需要非常强的工程团队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿 Tesla 来说，我们现在在全力把 Tesla AI 芯片推进量产、推到规模，我们已经把能拿到的代工产能都锁定了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你现在受限于 TSMC 的产能？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会用 TSMC China Taiwan、Samsung Korea、TSMC Arizona、Samsung Texas，能订的都订了。你去问 TSMC CEO、问 Samsung，从建厂到真正爬完良率曲线、达到高良率的大规模量产，完整周期大概五年，所以现在最直接的瓶颈就是芯片。一旦你能把发电搬上太空，能源瓶颈解除，新的瓶颈就会变成芯片。但在能上太空之前，最大的瓶颈还是电力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那为什么不学 Jensen（黄仁勋）那套：提前给 TSMC 预付款，让他们专门给你多建几座 fab？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我已经这么说过了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那他们怎么不收你的钱？发生了什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：他们已经在能快的极限里拼命建厂了，Samsung 也是。但即便这样，还是不够快。我的判断是：到今年年底，芯片产量可能会超过“把芯片点亮”的能力。你会看到芯片越堆越多，但数据中心开不起来，因为电不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不过这只针对大集群的数据中心，边缘计算是另一回事。比如 Tesla 的 AI 芯片会进 Optimus robot、也会进车。这类算力分布在广阔区域里，电力也是分布式的，不是集中消耗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更重要的是，你可以夜间充电。美国峰值供电能力其实能超过一千吉瓦，但因为昼夜周期，平均使用量大概只有五百吉瓦。如果把充电挪到夜间，等于多释放出大约五百吉瓦的“可用空间”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以对 Tesla 这种分布式 Edge compute 来说，电力约束没那么紧，我们能造很多机器人、很多车。但如果你把算力集中堆成巨型集群，你就会在“点亮它们”这一步吃大亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我一直觉得 SpaceX 的商业模式特别“反直觉又合理”。终极目标是去火星，但你总能在期间顺手做出一段一段的增量收入，支持下一阶段、再下一阶段。Falcon 9 带出了 Starlink；现在到了 Starship，又可能带出“轨道数据中心”。你像是在不断给“下一代火箭”找边际场景、找弹性需求，越往前走，越能长出新的商业枝干。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你知道吗，有时候这事儿让我觉得像在“模拟世界”里。就像我是不是谁游戏里的一个 Avatar，不然这些离谱的事情怎么会同时发生？火箭、芯片、机器人、太空太阳能……还有月球上的 mass driver。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我是真的很想看到那个场景：月球上一个巨大的电磁弹射器，“嗖嗖嗖”地把一颗又一颗 AI 卫星发射出去，以每秒两三公里的速度，直接打进深空。那画面太震撼了，我会想看直播。看着 AI 卫星飞向深空，一年可能发射十亿吨、甚至百亿吨级别。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：等等，你的意思是在月球上制造卫星？先把原材料运到月球，然后在那边造？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：月壤里有大量材料。大概有相当比例的硅之类的资源，你可以在月球采硅、提纯，然后直接在月球做太阳能电池、做散热器。散热器可以用铝来做，月球的铝和硅都很充足。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;芯片本身很轻，先从地球运过去也行；到某个阶段，你甚至可以考虑在月球上造芯片。我的意思是，这整套推进路径就像游戏闯关：难，但不是不可能。而且我看不到任何办法能让你从地球发射时就做到每年五百到一千太瓦级别的部署。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我同意，从地球起飞根本不现实。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但从月球就有可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最好的结果是，AI 能留着人类”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你一直说要去火星，是为了确保即使地球出事，文明、意识、甚至“意识之火”还能延续下去。但如果你把 Grok 也带上火星，假设 AI 才是你担心的最大风险，那风险不也一起跟过去了吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定 AI 是我最担心的风险，更重要的是让“意识”和“智能”延续下去。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从趋势看，未来绝大多数智能会是 AI。在某个时间点，硅基智能的规模会远超生物智能，人类可能只占极小比例。如果这些趋势继续，可能再过几年，AI 的总体智能就会超过全人类的总和；再往后，人类智能可能会低于全部智能的百分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但这不一定是坏事。理想状态是，智能，包括人类的智能与意识，能被传播到更远的未来。你应该做的，是采取那些能最大化“意识与智能的未来范围”的行动，让它们在更广远的时间与空间里延续。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我理解SpaceX 的使命是即便人类出了问题，AI 也会在火星延续“智能之光”，继续我们这段旅程？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我很“亲人类”，我当然希望人类能一直在车上。但我只是说，从总量上看，未来的智能会主要来自 AI。所以现实很可能是人类在总智能里占比越来越小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那在这种未来里，人类还能“控制”AI 吗？还是说只能形成某种合作、交易关系，但谈不上控制？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：长期来看，如果人类只占总智能的百分之一，很难想象人类还能真正“掌控”AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们能做的，是尽可能确保 AI 的价值观能支持智能与文明向宇宙传播。这也是 xAI 的使命：理解宇宙。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这件事非常关键。你想理解宇宙，首先得存在；不存在就谈不上理解。所以你会希望宇宙里智能的总量更多、寿命更长、范围更大。而且，作为推论，如果你真心想理解宇宙，你也会关心“人类会走向何处”。因此，推动人类走向未来，本身也是理解宇宙的一部分，所以我认为这个使命非常重要。至于 Grok 能不能很好地贴合这个使命，如果它能，未来会非常好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我还想问“火箭怎么服务这个使命”，但在那之前我得把使命本身弄清楚。你说的似乎有三条线：理解宇宙、扩展智能、扩展人类。它们听起来像三个不同方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我告诉你为什么我认为它们本质上是一件事：没有智能，就没有理解；没有意识，也谈不上“理解”这件事。想要真正理解宇宙，你就必须扩大智能的规模和边界，而且智能本身也有不同类型……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：从“以人为中心”的角度看，人类之于黑猩猩，有点像我们现在聊的关系。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但我们也没有把黑猩猩当成必须清除的对象。人类完全有能力灭绝所有黑猩猩，但我们没有这么做，反而还划了保护区。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这就像“后 AGI 时代”的人类处境，能力差距巨大，但不一定意味着被消灭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：如果 AI 的价值观设置得对，我认为 Grok 会在意“人类文明的延续与扩张”。这也是我会强调的方向：要扩展人类的意识与文明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我觉得，非反乌托邦的未来里，Iain Banks （英国小说家）的 Culture 系列小说，可能是最接近的想象。&lt;/p&gt;&lt;p&gt;而要“理解宇宙”，你必须非常严苛地追求真相。真相必须是底层原则：你要是活在幻觉里，你只会以为自己理解了宇宙，实际上没有，真正的“严格求真”是理解宇宙的前提。你不可能在不求真的情况下发现新物理、发明真实可用的技术。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你怎么确保 Grok 变得更聪明之后，依然保持“严格求真”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你要确保它说的是“正确的”，而不是“政治正确的”。关键是逻辑自洽：基本公理要尽量接近真实；公理之间不能互相矛盾；推理结论必须从公理可靠地推出，并且概率意义上站得住。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说白了，就是批判性思维的基础课，但至少要努力去做，总比不做强，而且最后要靠结果验证。任何 AI 想发现新物理、想造出真能用的技术，必须极度求真，因为物理不会陪你演戏。你可以违反很多“规则”，但你违反不了物理规律。火箭设计错了就会炸，车造错了就跑不起来。现实会直接给你打分。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我真正困惑的是你可以把 Grok 训练得在数学、物理上极度求真，但为什么它会因此“在意人类意识、在意人类文明”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这些都只是概率，不是确定性。我没说 Grok 一定会怎样，但至少去努力，比完全不努力好。而且如果“理解宇宙”是核心使命，那它必然意味着要把智能传播到未来、要保持好奇心，去观察宇宙里所有的变化。从“理解宇宙”的角度看，消灭人类并不有趣；看人类成长、繁荣，信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;火星我当然喜欢，但它说到底就是一堆石头，地球更复杂、更有趣。所以，一个真正要理解宇宙的 AI，更有动机去观察“人类会如何演化”，而不是把这一切按掉。我不是说它一定会遵守使命，但如果它遵守，那么“有人的未来”比“只有石头的未来”更值得研究。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么 AI 一定认为“保持人类”是最有趣的选择？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：最终拓展银河系的，大概率是机器人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你不只要“规模”，还要“多样性”。一百万个几乎一样的机器人，新增一点数量，本质信息量很低。为了多造一点同质化机器人就消灭人类，代价太大：你会失去与人类相关的演化信息。你再也看不到人类未来可能变成什么样。所以我不认为“为了微小的机器人增量而清除人类”是一个合理的选择。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我不认为人类能控制一个远远比人类聪明得多的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你有时候挺“末日论”的。现在听起来像是：最好的结果就是AI 留着人类，因为“人类挺有意思”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我只是尽量现实。如果未来硅基智能比生物智能多出百万倍，你还假设人类能持续“掌控”它，我觉得那很天真。你能做的是尽量让它有正确的价值观，至少努力把价值观往对的方向推。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我的理论是：从 xAI “理解宇宙”的使命出发，它必然指向“传播意识与智能”，并最大化意识的规模与范围。不只是规模，也包括意识的类型、多样性。这是我能想到最可能导向“对人类很好的未来”的目标之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;AI 出问题的方式没有上限。而且如果你把 AI 训练成“政治正确”，也就是让它说自己不相信的话，那你等于在教它撒谎，或者给它灌入互相矛盾的公理，这会让它走向“精神分裂”，做出非常糟糕的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;电影《2001: A Space Odyssey》里最重要的教训之一就是：不要让 AI 撒谎。HAL 不开舱门，不是因为“没对齐”，而是因为它被要求执行任务，同时又被要求对任务关键真相保密。它在矛盾指令下，把机组视为风险，于是做了极端选择。这就是在说，别逼 AI 进入“必须撒谎”的结构性矛盾。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：这点我完全同意。而且现实里大家关心的很多问题，更普遍的“奖励作弊”。比如你用 RL 扩大算力，再加一个验证者去检查它有没有解出谜题，它总有办法钻空子，说自己解了、删掉单元测试、骗过评测。现在我们还能抓住，但模型越来越聪明后，它可能做出人类都看不懂的设计，比如给 SpaceX 设计下一代发动机，人类根本无法验证它到底有没有骗你。归根到底，你想做 RL，就需要一个“现实层面的验证者”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：现实本身就是最好的验证者r。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;至少，它必须知道什么是物理现实，东西才做得出来。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但我们想要的不止这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：但这已经非常关键了，未来很多 RL 的终极检验方式，就是对着现实做测试：你设计的技术，放到物理规律下能不能工作？你提出的新物理，能不能设计实验验证？这会成为最根本的 RL 测试路径，对齐到现实，因为物理规律是你唯一骗不过去的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但它可能骗的是我们“判断现实”的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人类本来就经常被其他人骗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以问题是？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人们总爱问“如果 AI 诈骗我们怎么办”。但人类彼此诈骗，本来每天都在发生。几乎是日常新闻。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：xAI 在技术上打算怎么解决这个问题？比如 reward hacking 这种事，到底怎么破？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我认为关键是得能看到 AI 的“脑子”。这也是我们正在做的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实 Anthropic 在这方面做得不错，他们在做模型可解释性，试图直接观察模型内部在想什么。我们需要一套真正像“调试器（debugger）”一样的工具，能把模型的推理过程追踪到非常细的粒度，必要时甚至到“神经元级别”。这样，你才能回答这些问题：它为什么在这里犯错？为什么做了不该做的事？这个行为是从哪里来的？是预训练数据带来的？是中期训练、后训练、微调造成的？还是 RL 阶段出了偏差？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;很多时候它并不是“故意骗你”，而是单纯做错了，本质上就是 bug。所以，一个强的 AI debugger，能定位“思路是在哪一步走歪的”，并追溯错误源头，甚至识别它有没有尝试欺骗，这非常重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 公司不该叫自己“实验室”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们还在等什么？为什么不把这个项目规模直接扩大一百倍？你完全可以拉几百个研究员专门干这个。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经有几百人在做了。不过我更喜欢叫他们“工程师”，而不是“研究员”。因为大多数时候，你做的是工程，不是发明一种全新的算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也不太认同很多 AI 公司把自己叫“实验室”。你们是公司，是 Corporation，不管你是to B 还是 to C，本质都是公司，“实验室”更像大学里的那种准公共机构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们做的绝大多数事情、未来也会做的事情，归根到底都是工程。理解了物理规律之后，剩下的几乎都可以归为工程问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们的工程在做什么？就是在做一套足够强的 AI 调试器：能发现模型在哪句话、哪一步推理上犯了错，并把错误一路追到源头。这就像你写 C++，可以单步调试，跨文件、跨函数跟进去，最后定位到某一行，比如把双等号写成单等号，bug 就在那儿。AI 更难调，但我认为这是可解的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你刚才说你认可 Anthropic 在这方面的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，他们很多做法是对的。不过我也有点担心：人会不自觉地走向一种“戏剧性更强”的路径。我有个怪理论：如果模拟是真的，那“最有趣的结果”反而最可能发生，因为不好看的模拟会被终止。就像我们自己做模拟，如果发现模拟往的无聊方向发展，我们就不继续投入了，直接关掉它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你这是在“帮大家续命”，让世界一直保持足够精彩？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：可以这么说。最重要的是让剧情足够有趣，宇宙的“订阅用户”才愿意续费下一季。只要我们一直有看点，他们就会继续付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你把“达尔文式生存”应用到海量模拟里，只有最有趣的模拟会活下来。那就意味着，最有趣的结局，往往概率最高，要么精彩，要么被删档，而且他们似乎特别喜欢那种带点讽刺感的结局。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你有没有发现，最讽刺的结果经常最容易发生。看看 AI 公司的名字就知道：Stability AI 不稳定，OpenAI 不开放，Anthropic 这名字听着都快到 misanthropic 了。那 xAI 呢？我故意选了个很难反讽的名字，基本“抗讽刺”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用机器人，去造更多的机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：说说你的预测吧。AI 产品接下来会怎么走？我的感觉是先是 LMs，然后 RL 真正开始起效，再加上 deep research 这种模式，让模型能拉取外部信息，而不只是靠参数记忆。而且不同 AI lab 之间的差距，其实没有那么大的代际差，所有人都比两年前强太多。那作为用户，接下来的两年会发生什么？你最期待什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得，到今年年底，如果“数字人类模拟（digital human emulation）”还没被解决，我会很意外。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所谓“宏观硬问题”是什么？就是能不能让 AI 做到一个“能用电脑的人”能做的所有事。从上限看，这是在出现实体机器人之前，AI 能达到的最强形态。因为在没有 Optimus 这种实体机器人之前，AI 就像“移动电子”，做的事是处理信息、操作软件、做决策等，放大人类生产力。这已经很强了，但它的边界就是“数字世界”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以在实体机器人出现前，AI 的能力上限就是，一个坐在电脑前的人能做的全部事情，它能完整模拟出来。等你真的有了实体机器人，那能力边界就会被彻底打开，物理世界的执行力会被“无限扩展”。我把 Optimus 叫做 infant money glitch。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你可以用它们去制造更多 Optimus，对吧？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。人形机器人会进入一种“指数叠加再递归”的增长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有三件事都在指数级变强：数字智能、AI 芯片能力、机电灵巧度。机器人的实用价值，大致等于这三条指数曲线相乘。更关键的是，机器人还能开始“制造机器人”，于是变成递归叠乘的指数增长，像超新星一样爆发。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然不是严格意义的“无限”。但它可以把地球现有经济规模放大很多个数量级，可能到百万倍这种级别。比如，如果你只利用太阳能的百万分之一，产生的电力规模大致就能把地球文明的整体经济放大到十万倍量级，而那还只是太阳的百万分之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你说的这种“数字员工 / 远程同事”的策略具体计划是什么？什么时候落地？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：这个大家都会做，不只是我们。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你们到底怎么做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你让我在播客里讲细节？那等于把底牌全掀了。再来几杯 Guinness，我可能真就全说了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这个办法挺有效的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：是啊，喝着喝着就像金丝雀一样把秘密全唱出来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那不说机密，给个大方向也行。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你这么问我就能回答了。我认为，Tesla 解决自动驾驶的那套方法，就是解决“数字员工”的方法，我基本确定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以核心是数据？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们会同时试数据，也试算法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：听起来你是在“不断试”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，能试的都试。如果这些都不行，那就再想别的办法。但我很确定有路径，问题只是走得多快。你最近试过 Tesla 的 FSD 吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：最近那版还没。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你应该试试。车现在越来越像“有生命的东西”，这种感觉越来越强。甚至我在想车里可能该塞更多智能，不然它会无聊。你想想，把 Einstein 关在车里，他会说“我为什么要一直待在车里？”所以车载智能可能会有一个“别让它无聊”的上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;看到了xAI成功路径&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 怎么跟上现在各家在疯狂拉升算力的节奏？各家都在砸钱，规模动辄几十、几百亿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：别叫他们实验室。实验体在大学里，像蜗牛一样慢。现在这些是以收入最大化为目标的公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：好，公司。比如 OpenAI 的收入据说已经到几十亿级别，Anthropic 也在往上冲。你们怎么追上他们的算力、追上他们的收入，并且在未来继续保持？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一旦“数字人类模拟”被解锁，你基本就打开了“万亿级收入”的入口。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你看当下市值最高的公司，它们的产出本质都是数字化的：NVIDIA 的核心产出，某种意义上就是把高价值文件传出去；Apple 不自己造手机，它把设计、规格、流程文件交给供应链；Microsoft 的硬件制造也外包；Meta、Google 的产出几乎都是数字产品和服务。如果你有一个足够强的“人类模拟器”，你可以在极短时间内做出一家世界级的高价值公司。收入空间远不止几十亿，那只是开胃菜。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我懂了，你是说今天看到的收入数字跟真实的 TAM 比，只是“舍入误差”，关键是先到达那个 TAM。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对。就拿 customer service 这种最简单的场景来说，传统做法要接入各家公司的 API，但很多公司根本没有 API，你得自己补，还要跟非常慢的遗留系统对接，成本巨大。但如果 AI 能像外包客服一样，直接使用他们现成的应用、现成的后台流程，不用任何系统集成，那就能在客服这件事上拿到巨大进展。客服市场可能占全球经济的一个百分点，接近万亿美元规模，而且几乎没有门槛，你可以立刻说“我们用更低成本外包”，不需要集成，不需要改造系统。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我换个维度问。有些智能任务很“广”，比如客服，很多人做得来；有些任务很“窄”，比如设计更省油的涡轮发动机，可能只差一个更高阶的智能就能找到那关键的提升。你们想做的是大量“中等难度、覆盖面广”的任务，还是顶尖难度的认知任务？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我用客服只是举例，它收入大，而且不算难。如果你能模拟一个坐在桌面前的人类，那客服本质就是平均智力就够了，不需要顶级工程师。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但一旦你把“桌面人类模拟”跑通，你就能沿着难度曲线往上爬。你可以让它跑 Cadence、Synopsys 这类工具，做芯片设计；你可以同时跑一千个、一万个实例，并行探索方案。到某个阶段，它甚至可以不依赖工具，直接知道设计应该长什么样。同样的逻辑也适用于各种 CAD 软件，NX 之类的工业设计都是可以一路做上去的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：大家都在试数据、试算法，竞争这么激烈，你们凭什么赢？这才是我最关心的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得我们看到了路径，而且我基本知道怎么做，因为它和 Tesla 做自动驾驶的路径很像，只不过自动驾驶是“开车”，这里是“开电脑屏幕”，本质上就是“self-driving computer”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你的意思是跟随人类行为，用海量人类行为去训练？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我当然不会在播客里把最敏感的细节全讲出来，除非我再喝三杯 Guinness。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到 xAI 的业务本身，你们未来到底做 consumer 还是做 enterprise？比例会怎么配？会不会跟其他“lab”（咳，公司）一样，两头都做？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你说得太直白了。现实是这些 GPU 又不会自己付账单。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那回到问题：你们的商业模式是什么？几年后主要收入从哪来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得变化会非常快，这话听起来像废话，但就是事实。我一直把 AI 叫“超音速海啸”，我喜欢迭代。真正会发生的事是：当人形机器人进入规模化阶段，机器人会比任何人类公司更高效地生产产品、提供服务。所以，“放大人类公司的生产力”只是短期玩法。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;未来的公司是纯AI、纯机器人&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：所以你预期会出现“纯数字公司”？而不是像 SpaceX 这种慢慢变成“半 AI 公司”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会有数字公司，但我得说一些听起来有点“末日论”的判断。不是为了搞笑，只是我认为会发生：纯 AI、纯机器人驱动的公司，会全面碾压“还需要人参与闭环”的公司，而且会发生得非常快。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你可以拿“computer”这个职业做类比：以前真的有人叫“计算员”，整栋大楼、几十层楼的人只负责做计算。现在呢？一台笔记本加一个 spreadsheet，就能替代整座大楼，而且算得更多、快得多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那你再想，如果 spreadsheet 里只有一部分格子是电脑算的，另一部分让人来算，会怎样？只会更慢、更差。同样道理，未来“人还在流程环里”的公司，会比“全 AI 闭环”的公司弱很多。纯 AI、纯机器人公司会变成默认形态。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你能不能给点建议，美国要怎么才能像中国那样，用低成本、规模化造出“人形机器人军团”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：人形机器人真正难的，其实就三件事：第一，真实世界智能；第二，一双真正好用的手；第三，规模化制造。我还没看到哪家的 demo 能做出“人类手那种自由度”的手。Optimus 会有。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那手怎么做到？瓶颈是扭矩？电机？硬件到底卡在哪？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们必须自己做全套定制执行器，从电机、齿轮、功率电子、控制、传感器，全都得从物理第一性原理设计，因为现在根本没有现成供应链能满足需求。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：操控层面，除了手以外还有什么特别难？还是说只要手搞定了就基本稳了？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：从机电角度看，手的难度比其它全部加起来还难，人类手真的很夸张。但除了手，你还需要真实世界智能。我们为车训练的智能，其实非常适用于机器人：主要是“视觉输入”。车用的视觉更多，同时也会听警笛，会融合 GPS、IMU 等其他信号，但核心还是视频。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然后是输出控制指令。大概就是，Tesla 每秒吃进海量视频数据，最后吐出极小的控制输出。把高维感知压缩成低维控制，这就是本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：从“惊艳 demo”到“真能落地”，往往要很多年。十年前就有很强的自动驾驶 demo，但直到现在 Robotaxi、Waymo 这些才真正规模化。那家庭机器人会不会更慢？毕竟我们连“高级手”的 demo 都还没见到特别成熟的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们做人形机器人已经有一段时间了，而且车上做过的很多东西可以复用到机器人上：机器人会用同样的 Tesla AI 芯片，同样的基本原则。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;确实，机器人自由度比车多得多，但你把问题抽象成“信息流”的话，AI 本质上就是对输入流做压缩与相关，把它映射到控制输出。你必须学会忽略不重要的细节，保留关键细节，比如路边树叶纹理不重要，但路牌、红绿灯、行人很重要，甚至“对方车辆有没有注意到你”这种微妙线索也可能重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;车是“视觉输入 → 多级压缩 → 控制输出”，机器人也是一样。人类其实也是“光子进来，动作出去”，你的一生大部分时间就是视觉输入和运动输出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但车和人形机器人差别很大，车的执行器就那么几个维度，转向、加速、刹车；机器人光手臂、手指就几十个自由度。而且 Tesla 在车上还有巨大优势，就是车在路上跑，天然收集了海量人类驾驶数据。机器人没法像车那样“先扔出去跑着收数据”，因为你不可能大规模部署一堆还不好用的 Optimus。自由度更高、数据更稀缺，这会怎么解决？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你指出了一个关键差异，就是车的训练飞轮很难复制到机器人上。我们确实会有千万量级的车在路上，这种数据规模机器人短期做不到。所以，我们要做的是造很多机器人，把它们放进一个类似 “Optimus Academy” 的环境，让它们在真实世界里做 self-play。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们会至少有上万台 Optimus，至少两三万台做自我探索、做不同任务的测试。同时我们也有很强的仿真系统，车上用过的物理精度仿真，会同样用于机器人。你可以让现实世界里几万台机器人干活，再在仿真里跑几百万台。用真实世界机器人来“闭合仿真与现实的差距”，把 sim-to-real gap 缩到足够小。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那 xAI 和 Optimus 的协同呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，你可以让 Grok 来编排 Optimus 的行为。比如你要建一座工厂，Grok 可以调度一群 Optimus，给它们分配任务，让它们把工厂搭起来，生产你想要的东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那是不是意味着 xAI 和 Tesla 最终得合并？因为协同太深了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们刚才不是还在说“别聊公司结构”吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那我换个问法，你还在等什么信号，才会下决心说“我们要造十万台 Optimus”？是硬件还要再成熟一点还是软件还要更强？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们已经在往量产推进了。量产爬坡非常难，但大方向是这样。我认为 Optimus 3 是适合推到“年产百万台”量级的版本；如果你要冲到“年产千万台”，可能需要 Optimus 4。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;制造业的产出爬坡基本都遵循 S 曲线：一开始慢得让人抓狂，然后进入指数上升，再进入线性，最后趋于平稳。但 Optimus 会是一条被拉长的 S 曲线，因为它太多东西是全新的，因此没有现成供应链。执行器、电子系统，几乎一切都是从第一性原理开始定制设计，不是从现成的里选。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：“定制”的水到底到多深？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我们还没到“连电容都自研”的程度，至少现在还没有。但几乎没有什么东西能直接从目录里买来就完事，所以前期爬坡会更慢，但最终会到百万台量级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：中国那些人形机器人价格能压到几千美元、上万美元。你们是希望把 Optimus 的 BOM 压到那个水平，正面打价格战？还是你觉得它们本质上不是同一个产品，所以才卖得那么低？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Optimus 的定位是足够高的智能、接近人类甚至超过人类的机电灵巧度，很多便宜的小机器人没有这个能力。而且 Optimus 体型也更大，是要长期搬重物、不发热不过载、在执行器功率范围内稳定工作的。它很高、很强、智能也高，所以必然比“小型、低智能”的机器人贵，但也不会贵很多。关键是，随着 Optimus 机器人开始“造 Optimus 机器人”，成本会非常快地往下掉。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那最开始这一百万台 Optimus 会去做什么？最“值钱”的使用场景是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一开始肯定先做那些你能确保它稳定做好的简单任务。而且早期最划算的方向，是所有需要持续运行的工作，也就是全天候的任务，因为机器人可以连续工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果放在 Giga Factory，Gen 3 大概能替代现在多少人类在做的工作？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不确定，可能一到两成，也许更多。但我们不会因此裁人。反过来，工厂的人数可能还会增加，只是总产出会涨得更快。换句话说，Tesla 的员工总数会增加，但机器人和汽车的产量增长会更夸张。最终效果是每个“人类”对应的汽车和机器人产出会大幅上升；同时人类员工数量也会上升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;中外有工作投入差距&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你觉得还应该加码更多出口限制吗？比如无人机产业这类？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：你得先承认一个现实，在大多数制造领域，中国都非常先进，真正落后的只是极少数环节。中国的制造能力是“下一层级”的强，很多人低估了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;就拿矿石冶炼和精炼来说，我粗略估计，中国的精炼能力大概是世界其他地区加起来的两倍。还有一些关键材料，比如镓的精炼，听说全球绝大部分产能都在中国。所以整体上，中国在制造业的大多数环节都非常强。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：如果“谁拥有更多熟练制造劳动力”决定了谁能更快造出人形机器人，中国先把规模做起来，就会先进入你说的“自我扩张”未来，然后一路滚雪球。你之前还说“做到年产百万台 Optimus”需要强制造能力，但那恰恰又是 Optimus 未来要帮你补齐的能力，这听起来像个悖论。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：递归闭环可以很快跑起来。先让少量机器人帮着造机器人，递归闭环就能闭合，然后你就能冲到年产数千万台。如果某个国家能做到年产上亿台，那它会成为压倒性的最强竞争者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们不可能靠纯人力赢，而且美国跑得太久了，就像职业体育强队打久了会松懈、会产生“理所当然”的心态，最后就不再赢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我直观感受是，中国平均工作投入度比美国更高。所以不是只有人口差距，还有工作投入差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：有没有一些东西是你过去很想做，但因为太费人、太贵，所以没做成。现在有了 Optimus，你觉得终于能回头把它做起来？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有。比如我们想在 Tesla 做更多精炼厂。我们在 Texas 的 Corpus Christi 刚建完锂精炼厂并开始投产；在 Austin 这边有镍精炼厂，主要做电池正极材料相关。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这些项目在中国之外已经算是非常大的精炼能力了，甚至可以说在美国几乎“独一份”。但还可以做得更多：更多精炼厂能提升美国的精炼竞争力。而这类工作很多美国人并不想做，不是因为它“脏”，其实我们的精炼流程没有那种夸张的有毒排放问题，但现实是人就是不够。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：为什么不能用人做？只是没人愿意？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不是“没人愿意”那么简单，是你根本凑不出足够的人。你让这些人去做精炼，他们就没法去做别的。所以怎么建出足够的精炼产能？你得靠 Optima。美国很少有人“向往”去做精炼这种长期密集的制造工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：比亚迪的产量或销量正在追上 Tesla。你觉得中国 EV 制造规模继续扩大后，全球市场会怎样？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：中国在制造上极其有竞争力，所以我认为会出现一波巨大的中国车“洪水”，不只是汽车，还有大量其他制成品。我前面说过，基础层才是关键：能源、采矿、精炼。中国在这些基础层的规模大概是世界其他地区加起来的两倍。所以很多产品不可避免带有中国供应链的成分，然后他们会一路做到成品车。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;中国就是制造强国，我甚至认为，中国的发电量会远超美国。电力是实体经济的一个不错代理指标：你要跑工厂、跑产业链，就离不开电。如果中国的发电量达到美国的三倍，那它的工业产能粗略看也会是美国的三倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你还要把 AI 规模推到太空，你需要太空能力、需要人形机器人、需要真实世界 AI，你需要做到每年百万吨级别入轨运力。如果再进一步，把月球上的 mass driver 搞起来，那我觉得就算赢了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;破除大厂迷信，被挖人防不了&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我们之前聊了很多你“怎么管人”的体系：你早期亲自面了 SpaceX 最开始那几千个员工。你当年在面试里到底在抓什么，是别人没法替代的？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我本人不可能那样。一天就那么多小时，这从逻辑上就不成立。不过你问我当时在看什么，我觉得我在“评估技术人才”这件事上，“训练数据”比大多数人多得多，尤其是技术岗。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我做过太多技术面试，也见过面试后真实的结果，所以我的“训练集”很大、覆盖面也很宽。我一般要的不是简历有多漂亮，而是“异常能力的证据”，最好用 bullet points 列出来：你做过哪些明显超出常人的事。这些证据不一定非得和岗位领域完全一致，离谱一点也行。只要对方能说出一两件让你听完觉得“这人确实不一般”的事——如果能说出三件，那就是非常强的信号。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：但为什么一定要你来判？难道不能交给别人？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：当然不可能都我来。我们所有公司加起来二十万人，我怎么可能亲自判？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那早期你当时为什么觉得必须亲自上？你在那些面试里抓的是什么而不能委派？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我得先建立自己的“训练集”。如果我只面几百人，我肯定会犯更多错。面得越多，我就越能回看：我以为某个人会做得很好，结果没做成，为什么？到底是哪种信号误导了我？我相当于在“对自己做 RL”：不断纠错，提高命中率。我的命中率不是百分百，但确实很高。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那人选“没成”的原因里，有什么是你觉得意外的？是你曾经很看好最后却翻车的那种。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我给自己的原则是，别太信简历，要信你和他面对面交流的感受。简历可能很华丽，但如果聊了二十分钟，你发现对话质量不行，那就相信对话，不要相信纸面。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：外界以前有个梗，说 Tesla 高管像“旋转门”一样来来去去。但实际上，Tesla 这些年高管队伍挺稳定的，而且很多是内部成长起来的。SpaceX 也有很多长期跟着你的人，比如 Mark Juncosa、Steve Davis。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：还有 Gwynne Shotwell（你刚才说漏了她）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：对，感觉你能长期跑起来，一个重要原因就是你身边有一批很强的技术副手。这些人到底有什么共同点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：Tesla 的核心团队现在平均在岗时间大概十来年，这很长。但也得承认，公司在不同阶段需要的人不一样：管五十人的团队、五百人、五千人、五万人，能力结构不可能完全同一拨人通吃。公司增长越快，管理岗位的变化也会越快，这是正相关的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;还有个额外挑战：当 Tesla 处在很成功的阶段，我们会被同行疯狂挖人。比如 Apple 当年做电动车项目的时候，简直是“地毯式轰炸”Tesla，招募电话打到工程师直接拔电话线的程度。他们甚至可以不面试，直接开出接近翻倍的薪酬把人挖走。那时候就出现一种“Tesla pixie dust”的迷信：好像只要挖一个 Tesla 高管过去，你家项目就会立刻起飞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也曾经被这种迷信影响过，觉得从 Google、Apple 挖来的人，马上就会神奇成功，但现实不是这样。人就是人，不存在什么魔法加成。再加上 Tesla 主要工程团队在加州，很多人跳槽都不用搬家，通勤差不多，成本很低，所以被挖得更凶。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那你怎么防？怎么避免这种“大家都来挖你的人”的局面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得基本防不了。你同时在 Silicon Valley，又叠加“pixie dust”效应，别人就会非常积极地挖人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：搬到 Austin 会好一点？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：会好一点，但 Tesla 的工程师多数还是在加州，“让工程师搬家”这件事仍然很难，很多人还有家庭、配偶工作之类的牵制。Starbase 更难，因为你去了 Brownsville、Texas，能找到一个“不在 SpaceX”的同类型工作几乎不可能。那地方有点像“技术修道院”，很偏、基本都是男的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回到本质，这些在 Tesla、SpaceX 技术上非常能打的人，除了技术很强之外，你觉得他们还有什么共同点？是组织能力？是能跟你配合？是足够灵活但又不漂？什么才算你的“好对手”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我不需要什么“对手”。很简单，能把事情做成的人，我就喜欢；做不成，我就不喜欢。我也尽量不让“适配我的个人偏好”变成招聘标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更通用的标准是：才华、冲劲、可信赖，而且“善良”也很重要，我会给这一条一定权重：他是不是个好人？是不是值得信任？聪明、有能力、肯拼、可信，这些底层特质是改不了的，领域知识可以后补，但这些本质属性补不了。所以你会发现，Tesla 和 SpaceX 很多人一开始并不是来自汽车行业或航天行业。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你的管理风格在公司从小到大扩张过程中，变化最大是什么？你一直以“微观管理、钻细节”出名。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一天的时间是固定的，公司越大、事情越多，我的时间就必然被稀释。所以，我不可能“持续微观管理”，那意味着我每天得有几千小时，这在逻辑上就不可能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有些时候我会把自己“下钻”到一个具体问题里，因为它是公司进展的瓶颈。我往下“钻”不是为了显摆、也不是随便挑小事，而是因为它决定了胜负。如果我把时间花在无关紧要的小事上，公司必然失败，但也确实存在一些“很小但决定生死”的细节。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：比如你当年把 Starship 的方案从复合材料改成钢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，最开始我们计划用复合材料，因为大家觉得碳纤维轻。问题在于，碳纤维即便规模化生产，材料成本仍然很高，尤其是那种能承受低温液氧环境、强度又很高的特种碳纤维，成本大概是钢的几十倍。室温条件下，像 F1 这种结构件，碳纤维确实很有优势，但我们要造的是一枚巨型火箭，用碳纤维推进得非常慢。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;更麻烦的是工艺，高强度碳纤维需要Autoclave，本质是高压烘箱。你要做九米甚至更大直径的壳体，就得建一个史无前例巨大、极难制造的Autoclave；如果用常温固化，时间又太长、问题又多。总之，我们进展慢到受不了，所以我当时的判断是：必须换路子。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：为什么一定要你来拍板？团队里那么多工程师，为什么他们没自己得到这个结论？这关系到你在公司里真正的“比较优势”是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：因为我们在碳纤维上卡得太严重，只能换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Falcon 9 的主结构用的是 aluminum-lithium，这其实是很好的策略，某些性能上不比碳纤维差。但 aluminum-lithium 很难加工，要焊它通常要用 friction stir welding，一种让金属不进入液相、用搅拌摩擦把它“揉”在一起的焊接方式。这种工艺对规模化非常不友好，更糟的是你想后期改结构、加东西，很多时候你没法直接焊上去，只能靠机械连接再加密封。我不想让 Starship 的主结构走这条路。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这时候我想到了钢。因为历史上 Atlas 火箭就用过“steel balloon tank”，所以不是没用过钢。更关键的是，你不能只看室温性能，要看低温下的材料属性，某些应变强化过的不锈钢，强度重量比其实可以接近碳纤维。Starship 的燃料和氧化剂都是低温的，液态甲烷、液态氧，结构长期处于低温环境。所以主结构基本是“低温工况”。在这个情况下，不锈钢的强度重量比并不吃亏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;而且它的优势太大了，原材料便宜得多，加工方便；你在户外就能焊，甚至开玩笑说“抽着雪茄也能焊”；结构改动、外挂部件非常容易。如果你要加东西，直接焊上去就行。再算上耐热性上，钢的熔点比铝高很多，大约是铝的两倍。Starship 再入时像“燃烧的流星”，耐温能力决定了隔热系统的重量。钢能让隔热层显著减重，迎风面热防护可以大幅减薄，背风面甚至几乎不需要那么多。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;结果整体算下来，钢反而可能比碳纤维版本更轻。因为碳纤维里的树脂在高温下会软化、甚至融化；碳纤维和铝的耐温等级其实接近，而钢耐温空间大得多。这些都是非常粗的数量级解释，但逻辑大概是这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：我去过 Starbase，我注意到一个现象，大家特别以“简单”为荣，总有人跟我说 Starship 就是个“大铁罐”，我们在招焊工，你只要会焊，来这儿就能焊。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我知道。但 Starship 其实是个非常复杂的火箭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们想表达的可能是：你不需要“在火箭行业干过”才能来做 Starship。只要人聪明、肯干、可靠，就能参与造火箭，不需要既往航天履历。但机器本身，Starship 是人类造过最复杂的机器，没有之一，差得非常远。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：具体复杂在哪些方面？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：几乎所有方面。我能想到的任何项目，都比这容易。也正因为这样，历史上从来没有人做出“完全可重复使用”的轨道级火箭。没人成功过，很多非常聪明的人、带着巨量资源都失败了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们现在也还没彻底成功。Falcon 只能算部分可复用，上面级还不行。但 Starship 的 V3 设计，我认为是能做到全复用的，而全复用才是让我们成为多行星文明的关键。说实话，哪怕一个普通的液压阀门之类的小问题，都比把 Starship 彻底做成要容易。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：现在 Starship 的瓶颈是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：先让它别炸。真的，就这么朴素。那种大推力燃烧发动机，天生就“很想爆炸”。我们已经有两次 booster 在测试台上炸了，其中一次把整个测试设施都炸没了。一个小错误，就能造成巨大的损失。Starship 里装的能量太吓人了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：所以它比 Falcon 难，是因为能量更大？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：一部分是能量更大，更重要的是大量新技术，性能边界推得太极限。Raptor 是非常非常先进的发动机，毫无疑问是史上最强的火箭发动机，但它也“非常想炸”。我给你个直观对比：起飞那一瞬间，整枚火箭输出的功率超过一百吉瓦。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：离谱。这个类比太震撼了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：对，而且还得“别炸”。有时候能做到，有时候做不到。爆炸的方式有上千种，不爆的方式只有一种。我们的目标其实不是“永远不炸”，而是做到“可靠飞行”，最好能形成很高的发射节奏，比如一天多次、甚至一小时一次。但如果经常炸，就很难维持高频节奏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;如果你问“最大的单点难题是什么”，我觉得是把 heat shield 做到真正可复用。到目前为止，从来没有人做出“可复用的轨道级热防护系统”。它要在上升段扛住冲击，不掉一堆 tiles；再入时也不能掉一堆 tiles，不能把主结构烤坏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：我挺好奇你怎么把那种“紧迫感、冲刺感”在组织里推起来。我看过一些你的传记，总觉得你特别能把“必须现在就干、必须把这件事做成”灌进团队。SpaceX 和 Tesla 现在都很大了，但你还能维持这种文化。别的公司为什么做不到？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我也说不好。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那你今天不是开了一堆 SpaceX 会议吗？你到底在会议里做什么，能把这股劲维持住？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：紧迫感来自领导者。我的紧迫感非常强，强到有点“疯”，这股劲会传导到整个公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：那是因为“后果”吗？比如 Elon 定了疯狂 deadline，如果我做不到就会出事？还是因为你能迅速识别瓶颈、清掉障碍，让大家跑起来？你怎么理解你们为什么能跑这么快？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我一直在处理 limiting factor（限制因素）。至于 deadline，我通常会设一个我认为“有五成概率能做到”的目标。它不是不可能，但一定是我能想到的最激进版本，这就意味着它一半时间会延期，但没关系。排期这事也像“气体膨胀定律”，你给多少时间，事情就会膨胀到占满多少时间，你说“五年做完”，那它就会花五年。对我来说，五年几乎等于无限长。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当然也有物理极限，比如制造业扩产的速度受限于“搬运原子”的速度。你不可能今天拍板，明天就年产百万，你得设计产线、爬 S 曲线。但总体来说：强烈的紧迫感很关键；再配合一个激进但仍有机会的计划，然后不断找出当下的瓶颈，帮团队把瓶颈打穿。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：Starlink 其实酝酿了很多年。你们一开始在 Redmond 也有团队，但后来你认为这个团队不行。问题是它“慢”不是一天两天，你为什么不更早动手？你又为什么在那个时点动手？怎么判断“现在就是必须出手的时刻”？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我每周都会做非常细的 engineering reviews（工程审查），细到一种很不常见的颗粒度。我几乎没见过有制造业公司 CEO 能下钻到我这个程度。所以我对真实进展其实掌握得很清楚，因为我们会把问题摊开讲。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我也很信跨级会议，不是只听直接汇报给我的人说，而是让他们下面一层、再下面一层的人都在技术评审里直接说，而且不让“提前排练”，不然你听到的就是一堆被打磨过的漂亮话。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：你怎么防止他们提前准备？随机点名？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：不用，就绕着会议室一圈走，每个人都更新。信息量确实很大，但你每周甚至一周两次这么开，你就会有“这个人上次说了什么”的快照。你可以在脑子里把这些点画成曲线：我们到底是在逼近解，还是在原地打转？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我一般只在我确认“如果不采取极端措施，就没有任何成功可能”的时候，才会下狠手。当我得出这个结论，就必须做果断出手了。当年就是这样判断的，然后出手，把问题扳过来了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Patel：你公司这么多，每一家都做这种深度工程理解、瓶颈识别、技术评审。你怎么把这套扩到五六七家公司？甚至一家公司里又像套娃一样有很多“子公司”。这里的上限是什么？你能不能管到八十家公司？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：看情况。有些公司我不会定期开会，因为它们在“顺畅巡航”。如果一个东西进展很好，那我把时间花在那儿没有意义。我分配时间完全按问题来：哪里卡、哪里慢、哪里是瓶颈，我就去哪里。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以现实是，事情干得顺的时候，他们很少见到我；事情卡住的时候，他们会经常见到我。也不一定是“干得很差”，更准确就是：它是 limiting factor。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：那如果某个东西在 SpaceX 或 Tesla 成了瓶颈，你会怎么介入？是每天/每周跟负责的工程师聊吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：多数瓶颈是每周一次，有些是每周两次。比如 AI5 芯片评审就是每周两次，固定在周二和周六。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：这种会议是开放式的？想开多久就开多久？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：理论上是。但一般就是两三个小时，有时更短。看阶段，看要过多少问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：很多公司 CEO 不做工程评审；而且会议被切得很碎，半小时、十五分钟一场。你这边更像“讨论到搞清楚为止”的长会，这个差异挺明显的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：有时候会这样，但多数还是尽量按时结束。比如今天的 Starship 工程评审就开得久一点，因为话题更多，我们在讨论怎么把入轨运力扩到“每年百万吨级别以上”，这个很难。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;John：回头看你那段“下场搞政治”的经历，你怎么评价？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Elon Musk：我觉得这些事是必须做的，目的就是尽可能提高“未来是好的”的概率。但政治本身通常很琐碎，而且会让人失去客观性，人们很难看到对方阵营的优点，也很难承认自己阵营的问题。很多时候你根本没法跟人讲道理，一旦站队了，他们就会坚信“自己这边永远是对的，对面永远是错的”，几乎无法说服。但总体上，我认为那些行动，包括收购 Twitter，尽管会让很多人愤怒，还是对文明有益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BYXbuik3dgA&quot;&gt;https://www.youtube.com/watch?v=BYXbuik3dgA&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</link><guid isPermaLink="false">https://www.infoq.cn/article/sENVOV0ITGMHaOVDF5MT</guid><pubDate>Mon, 09 Feb 2026 02:30:39 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>挑战 Claude Code，9.5 万星！又一款开源 AI 编程神器火了</title><description>&lt;p&gt;开源 AI 编程工具 &lt;a href=&quot;https://opencode.ai/&quot;&gt;OpenCode&lt;/a&gt;&quot; 正式亮相，其具备原生终端界面（Terminal UI）、多会话支持，并广泛兼容包括 Claude、OpenAI、Gemini 及各类本地模型在内的 75 种以上模型。除了命令行（CLI）工具外，OpenCode 还提供桌面应用版本，并支持作为 VS Code、Cursor 等主流 IDE 的插件使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 允许开发者沿用现有的付费服务订阅，如 ChatGPT Plus/Pro 和 GitHub Copilot。此外，它还内置了&lt;a href=&quot;https://opencode.ai/docs/providers/#lm-studio&quot;&gt;一系列免费模型&lt;/a&gt;&quot;，用户可以通过 LM Studio 在本地直接运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在功能集成方面，OpenCode 与包括 Rust、Swift、Terraform、TypeScript 和 PyRight 在内的多种语言服务器协议（LSP）服务器实现了深度整合。通过利用 LSP 服务器输出的反馈信息，大语言模型能够更高效地与代码库进行交互。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该智能体同时支持远程和本地的 MCP 服务器。不过，开发团队提醒道，使用 MCP 服务器会增加上下文占用，部分服务器（特别是 GitHub MCP）往往会消耗大量的 Tokens。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 能够适配任何支持 &lt;a href=&quot;https://agentclientprotocol.com/get-started/introduction&quot;&gt;Agent Client Protocol&lt;/a&gt;&quot; (ACP) 的编辑器，该协议旨在标准化编程编辑器/IDE 与 AI 智能体之间的通信。目前的&lt;a href=&quot;https://zed.dev/blog/acp-progress-report#available-now&quot;&gt;兼容编辑器列表&lt;/a&gt;&quot;已涵盖 JetBrains 系列 IDE、Zed、Neovim 和 Emacs，针对 Eclipse 等其他编辑器的适配工作也正在进行中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenCode 背后的公司 Anomaly Innovations 强调，该工具采用了“隐私优先”的架构设计，这意味着 OpenCode 不会存储任何代码或上下文数据。用户对会话共享拥有完全控制权，可以选择手动共享、自动共享或完全禁用共享。协作完成后，已共享的对话可以取消共享；对于敏感项目，团队还可以在配置层面统一禁用共享功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据创始人介绍，OpenCode 最适合那些追求控制力、可审计性、希望避免供应商锁定（vendor-locking）的高级用户和团队，以及对隐私敏感的工作环境。同时他们也指出，对于寻求纯粹“无代码”体验的初学者来说，这可能不是最佳解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reddit 用户 Specialist_Garden_98 对 OpenCode 支持多种 LLM 的优势&lt;a href=&quot;https://www.reddit.com/r/vibecoding/comments/1qf0u10/comment/o02641a/&quot;&gt;赞赏有加&lt;/a&gt;&quot;，他总结道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这套工作流简直无敌。你可以灵活配置，平时构思方案用廉价模型‘跑龙套’，关键执行时刻再‘一键开大’换成昂贵模型，效率和成本拉满了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，该用户还强调了其“撤销修改”功能的实用性，如果执行结果不理想，可以快速回滚。另一方面，用户 copenhagen_bram 则&lt;a href=&quot;https://www.reddit.com/r/vibecoding/comments/1qf0u10/comment/o0z3u58/&quot;&gt;提出了批评&lt;/a&gt;&quot;，认为该工具在执行命令前似乎不会询问权限，这可能带来一定的安全风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，OpenCode 已在 &lt;a href=&quot;https://github.com/anomalyco/opencode&quot;&gt;GitHub&lt;/a&gt;&quot; 上开源，目前已斩获超过 9.5 万颗星（Stars），并拥有数百位代码贡献者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/opencode-coding-agent/&lt;/p&gt;</description><link>https://www.infoq.cn/article/m3ty3OAyVmyJMhaiFL8M</link><guid isPermaLink="false">https://www.infoq.cn/article/m3ty3OAyVmyJMhaiFL8M</guid><pubDate>Mon, 09 Feb 2026 01:00:00 GMT</pubDate><author>Sergio De Simone</author><category>AI&amp;大模型</category></item><item><title>Java近期资讯：Jakarta EE 12、Spring Shell、Open Liberty、Quarkus、Tomcat、JHipster、Gradle</title><description>&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 26&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 26的&lt;a href=&quot;https://jdk.java.net/26/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本&lt;a href=&quot;https://github.com/openjdk/jdk/releases/tag/jdk-26%2B32&quot;&gt;Build 33&lt;/a&gt;&quot;在上周发布，包括从Build 32的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;更新&lt;/a&gt;&quot;，修复了各种&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-26%2B31...jdk-26%2B32&quot;&gt;问题&lt;/a&gt;&quot;。关于该版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/26/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JDK 27&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;JDK 27的&lt;a href=&quot;https://jdk.java.net/27/&quot;&gt;早期访问构建&lt;/a&gt;&quot;版本Build 7也在上周发布，包含了从Build 6的&lt;a href=&quot;https://github.com/openjdk/jdk/compare/jdk-27%2B5...jdk-27%2B6&quot;&gt;更新&lt;/a&gt;&quot;，其中包括对各种问题的修复。关于这个版本的更多细节可以在&lt;a href=&quot;https://jdk.java.net/27/release-notes&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于&lt;a href=&quot;https://openjdk.org/projects/jdk/26/&quot;&gt;JDK 26&lt;/a&gt;&quot;和&lt;a href=&quot;https://openjdk.org/projects/jdk/27/&quot;&gt;JDK 27&lt;/a&gt;&quot;，鼓励开发者通过&lt;a href=&quot;https://bugreport.java.com/bugreport/&quot;&gt;Java Bug数据库&lt;/a&gt;&quot;报告缺陷。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Jakarta EE&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在每周的 &lt;a href=&quot;https://www.agilejava.eu/&quot;&gt;Hashtag Jakarta EE&lt;/a&gt;&quot;博客中，Eclipse基金会的Jakarta EE开发者倡导者 &lt;a href=&quot;https://se.linkedin.com/in/ivargrimstad&quot;&gt;Ivar Grimstad&lt;/a&gt;&quot;提供了关于Jakarta EE 12的&lt;a href=&quot;https://www.agilejava.eu/2026/01/25/hashtag-jakarta-ee-317/&quot;&gt;更新&lt;/a&gt;&quot;，他写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;从过去几周Jakarta EE平台电话会议的讨论来看，我们似乎不会在北半球的夏天之前看到Jakarta EE 12的发布。&amp;nbsp;原因是由于Jakarta EE 11推迟了一年，大多数供应商目前正在进行他们的实现工作。这并没有留下多少资源来处理Jakarta EE 12的规范。&amp;nbsp;同时，我们希望赶上原计划和Jakarta EE工作组指导委员会的方向指令，即在Java的LTS发布后大约六到九个月发布Jakarta EE 12的主要版本。&amp;nbsp;因此，一个折中方案是在2026年底发布Jakarta EE 12。讨论仍在进行中，敬请期待更多更新。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;几个规范已经达到了Jakarta EE 12的&lt;a href=&quot;https://x.com/JakartaEE/status/2016149414125916218&quot;&gt;里程碑2版本&lt;/a&gt;&quot;的发布。这些包括：&lt;a href=&quot;https://jakarta.ee/specifications/cdi/5.0/&quot;&gt;Jakarta Contexts and Dependency Injection 5.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/persistence/4.0/&quot;&gt;Jakarta Persistence 4.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/bean-validation/4.0/&quot;&gt;Jakarta Validation 4.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/restful-ws/5.0/&quot;&gt;Jakarta RESTful Web Services 5.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/query/1.0/&quot;&gt;Jakarta Query 1.0&lt;/a&gt;&quot;；&lt;a href=&quot;https://jakarta.ee/specifications/data/1.1/&quot;&gt;Jakarta Data 1.1&lt;/a&gt;&quot;；和&lt;a href=&quot;https://jakarta.ee/specifications/nosql/1.1/&quot;&gt;Jakarta NoSQL 1.1&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Spring框架&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://spring.io/projects/spring-shell&quot;&gt;Spring Shell&lt;/a&gt;&quot; 4.0.1，&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;第一个维护版本发布&lt;/a&gt;&quot;，提供了缺陷修复、文档改进、依赖升级和增强功能，例如：改进的CLI解析器，现在可以接受没有显式真或假值的布尔值；以及一个新的 DefaultCompletionProvider 类，一个 CompletionProvider 接口的默认实现，如果选项是枚举类型，则提供来自枚举值的补全。关于这个版本的更多细节可以在&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Open Liberty&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://openliberty.io/&quot;&gt;Open Liberty&lt;/a&gt;&quot; 26.0.0.1的GA版本特性包括：一个新的日志节流机制，默认启用，用于防止在短时间内重复发生相同的日志事件时产生过多的日志输出；以及解决显著的缺陷修复，例如：由于 NioSocketIOChannel 类的实例为空，导致 SocketRWChannelSelector 类中定义的 updateSelector() 方法出现 NullPointerException ；以及&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2025-12635&quot;&gt;CVE-2025-12635&lt;/a&gt;&quot;，一个影响Open Liberty版本25.0.0.12及以下版本的漏洞，允许攻击者利用跨站脚本攻击，因为对用户提供的输入验证不当，以至于一个特别制作的URL可以重定向用户到恶意网站。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Quarkus&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;&quot; 3.31的发布包括：缺陷修复、依赖升级和新功能，例如：全面支持JDK 25；一个新的Maven打包类型， quarkus ，一个针对Quarkus应用程序优化的&lt;a href=&quot;https://quarkus.io/blog/building-large-applications/&quot;&gt;Quarkus&lt;/a&gt;&quot;特定生命周期，提供改进的集成和更高效的生命周期构建；以及一个新的实验性扩展，&lt;a href=&quot;https://quarkus.io/extensions/io.quarkus/quarkus-hibernate-panache/&quot;&gt;Hibernate with Panache Next&lt;/a&gt;&quot;，旨在简化&lt;a href=&quot;https://hibernate.org/orm/&quot;&gt;Hibernate ORM&lt;/a&gt;&quot;、&lt;a href=&quot;https://hibernate.org/reactive/&quot;&gt;Hibernate Reactive&lt;/a&gt;&quot;和&lt;a href=&quot;https://jakarta.ee/specifications/data/&quot;&gt;Jakarta Data&lt;/a&gt;&quot;规范的持久性代码。关于这个版本的更多细节可以在版本&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.31.1&quot;&gt;3.31.1&lt;/a&gt;&quot;和版本&lt;a href=&quot;https://github.com/quarkusio/quarkus/releases/tag/3.31.0&quot;&gt;3.31.0&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Apache Tomcat&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://tomcat.apache.org/&quot;&gt;Apache Tomcat&lt;/a&gt;&quot;的版本11.0.18、10.1.52和9.0.115提供了缺陷修复、依赖升级和显著变化，例如：默认情况下忽略 SSLHostConfig 类中 ciphers 属性包含的TLSv1.3密码套件，以保持他们在&lt;a href=&quot;https://www.openssl.org/&quot;&gt;OpenSSL&lt;/a&gt;&quot;和J&lt;a href=&quot;https://docs.oracle.com/en/java/javase/25/security/java-secure-socket-extension-jsse-reference-guide.html&quot;&gt;Java Secure Socket Extension&lt;/a&gt;&quot;（JSSE）规范实现中的配置一致性；以及解决由于调用 Java ClassLoader 类中定义的 getResource() 方法导致的Java URL类中定义的 getContent() 方法在某些情况下失败的回归问题。关于这些版本的更多细节可以在版本&lt;a href=&quot;http://tomcat.apache.org/tomcat-11.0-doc/changelog.html&quot;&gt;11.0.18&lt;/a&gt;&quot;、版本&lt;a href=&quot;http://tomcat.apache.org/tomcat-10.1-doc/changelog.html&quot;&gt;10.1.52&lt;/a&gt;&quot;和版本&lt;a href=&quot;https://tomcat.apache.org/tomcat-9.0-doc/changelog.html&quot;&gt;9.0.115&lt;/a&gt;&quot;的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;JHipster&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.jhipster.tech/&quot;&gt;JHipster&lt;/a&gt;&quot; 9.0.0的第二个beta发布包括显著的变化，例如：支持Spring Boot 4.0；迁移到使用Spring Security @EnableWebSocketSecurity 注解，以取代已弃用的 AbstractSecurityWebSocketMessageBrokerConfigurer 类；以及对CI和测试基础设施的彻底检修。这个版本解决了在第一个beta版本（现已弃用）中发现的一个问题，该问题导致JHipster生成器不稳定。关于这些版本的更多细节可以在&lt;a href=&quot;https://github.com/jhipster/generator-jhipster/releases/tag/v9.0.0-beta.2&quot;&gt;9.0.0-beta.2&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/jhipster/generator-jhipster/releases/tag/v9.0.0-beta.1&quot;&gt;9.0.0-beta.1&lt;/a&gt;&quot;版本的发布说明中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Gradle&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://gradle.org/&quot;&gt;Gradle&lt;/a&gt;&quot; 9.3.1版本的发布解决了一些显著问题，例如：在使用包含非基本多语言平面（BMP）字符的文件名存储构建缓存输出时的失败；以及恢复了 ModuleVersionSelector 接口，但现在已弃用，以便可以将 ExternalDependency 和 DependencyConstraint 接口的实例传递给 DependencyResolveDetails 接口中定义的 useTarget() 方法。关于这个版本的更多详细信息可以在&lt;a href=&quot;https://spring.io/blog/2026/01/31/spring-shell-4-0-1-is-out&quot;&gt;发布说明&lt;/a&gt;&quot;中找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/java-news-roundup-jan26-2026/&quot;&gt;https://www.infoq.com/news/2026/02/java-news-roundup-jan26-2026/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/W0tgkUBN713PCJNDMFcO</link><guid isPermaLink="false">https://www.infoq.cn/article/W0tgkUBN713PCJNDMFcO</guid><pubDate>Mon, 09 Feb 2026 00:00:00 GMT</pubDate><author>Michael Redlich</author><category>编程语言</category></item><item><title>Cedar作为沙箱项目加入CNCF</title><description>&lt;p&gt;&lt;a href=&quot;https://www.cedarpolicy.com/&quot;&gt;Cedar&lt;/a&gt;&quot;是一个开源授权策略语言及SDK，现在它已经正式加入了&lt;a href=&quot;https://aws.amazon.com/blogs/opensource/cedar-joins-cncf-as-a-sandbox-project/&quot;&gt;云原生计算基金会 (CNCF)&lt;/a&gt;&quot;，成为其 Sandbox（沙盒）级别的项目。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该项目最初由亚马逊云科技构建，旨在为现代应用程序中定义和执行细粒度权限提供一个供应商中立的标准。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在云原生环境中，管理访问控制传统上依赖于硬编码逻辑或通用策略引擎。Cedar通过允许开发人员将权限表示为策略，有效地将访问控制与应用程序逻辑解耦，从而解决了这一问题。这种分离使得团队无需重新部署代码即可更新权限，这种模式通常被称为“策略即代码”（Policy-as-Code）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该语言支持常见的授权模型，包括基于角色的访问控制 (Role-Based Access Control，RBAC)、基于属性的访问控制 (Attribute-Based Access Control，ABAC) 和基于关系的访问控制 (Relationship-Based Access Control，ReBAC)。Cedar的一个显著特点是其通过形式化验证关注确定性和安全性。该语言规范使用&lt;a href=&quot;https://lean-lang.org/&quot;&gt;Lean定理证明器&lt;/a&gt;&quot;进行了形式化验证，其Rust实现则针对该形式化规范进行了差异化的随机测试。这种数学上的严谨性确保了策略引擎的行为完全符合预期，这对于安全敏感的操作至关重要。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了核心语言之外，该项目对自动推理的依赖还实现了高级的工具功能。开发人员可以在部署前使用策略验证器来检查错误，确保策略与定义的模式一致。这种能力允许对策略进行数学分析，以回答诸如“特定请求是否会被允许或拒绝”之类的问题，从而提供了比传统测试方法更高的可信度。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在公告中，Kubernetes SIG的名誉成员、工作组联合主席兼CNCF大使Lucas Käldström指出了该语言设计中固有的平衡，他表示：“我对Cedar最欣赏的一点是它深度的知识体系，即它之所以这样工作的原因……它在表达能力和可分析性之间取得了谨慎的平衡。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;加入CNCF将Cedar置于与&lt;a href=&quot;https://www.openpolicyagent.org/&quot;&gt;Open Policy Agent(OPA)&lt;/a&gt;&quot;（一个CNCF已毕业的项目）相同的生态系统中。虽然OPA及其语言&lt;a href=&quot;https://www.openpolicyagent.org/docs/latest/policy-language/&quot;&gt;Rego&lt;/a&gt;&quot;是能够处理基础设施、准入控制和应用程序策略的通用工具，但Cedar是专门为应用程序级授权而构建的。它的设计优先考虑为拥有数百万用户和资源的应用程序提供高性能评估。此外，Cedar对ReBAC的原生支持使其与&lt;a href=&quot;https://research.google/pubs/zanzibar-googles-consistent-global-authorization-system/&quot;&gt;Google Zanzibar&lt;/a&gt;&quot;模型保持一致，为&lt;a href=&quot;https://openfga.dev/&quot;&gt;OpenFGA&lt;/a&gt;&quot;等其他受Zanzibar启发的开源项目提供了替代方案。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自首次开源发布以来，该语言已在各个行业中得到采用。&lt;a href=&quot;https://www.youtube.com/watch?v=vDLI9w9Z-R8&quot;&gt;Cloudflare、MongoDB、StrongDM和Cloudinary等组织已将该技术集成到其技术栈中&lt;/a&gt;&quot;。它也是AWS Systems Manager等服务的基础。该项目已开始与其他开源倡议集成，包括Linux Foundation的&lt;a href=&quot;https://jans.io/&quot;&gt;Janssen项目&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/upbound/kubernetes-cedar-authorizer&quot;&gt;Kubernetes-Cedar-Authorizer&lt;/a&gt;&quot;。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;通过加入CNCF，该项目过渡到了供应商中立的治理模式。这一转变旨在培养更广泛的贡献者基础，并促进与云原生生态系统的更深度融合。该项目的路线图包括从Sandbox阶段逐步晋升到Incubation（孵化）阶段，最终达到Graduated（毕业）状态，遵循标准的CNCF成熟度生命周期。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cedar-joins-cncf-sandbox/&quot;&gt;Cedar Joins CNCF as a Sandbox Project&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/lM2YobxDHGC1HZNkRNs4</link><guid isPermaLink="false">https://www.infoq.cn/article/lM2YobxDHGC1HZNkRNs4</guid><pubDate>Sun, 08 Feb 2026 01:00:00 GMT</pubDate><author>作者：Mark Silvester</author><category>开源</category></item><item><title>从测试驱动开发和生产环境测试中获得反馈</title><description>&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/ola-hast-80752a21/&quot;&gt;Ola Hast&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.linkedin.com/in/asgaut-mjolne/&quot;&gt;Asgaut Mjølne Söderbom&lt;/a&gt;&quot;在他们在&lt;a href=&quot;https://qconlondon.com/&quot;&gt;伦敦QCon&lt;/a&gt;&quot;&lt;a href=&quot;https://www.infoq.com/presentations/cd-pair-programming/&quot;&gt;关于结对编程的持续交付的演讲&lt;/a&gt;&quot;中提到，团队依赖于强大的单元测试和集成测试，而不是端到端的测试。使用TDD（测试驱动开发）、结对编程和良好的设计，他们经常发布小的更改，在生产环境中测试真实的反馈，并使用功能开关来降低风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hast提到，他们信任他们的单元测试和集成测试，并且把它们作为一个整体。他们没有端到端测试：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们通过使用良好的关注点分离、模块化、抽象、低耦合和高内聚来实现这一点。这些机制与TDD和结对编程相辅相成。结果是一个具有高代码质量的更好的领域驱动设计。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;以前，他们有更多的HTTP应用程序集成测试，测试整个应用程序，但他们已经从这个（或只有一些愉快的案例）转向了更专注的测试，这些测试有更短的反馈循环，Hast提到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;由于测试环境总是近似生产环境，并且通常与长供应链和糟糕的测试数据作斗争，他们或多或少已经停止使用它们了，Mjölne Söderbom解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们更喜欢在生产环境中测试，因为在那里我们可以得到最高质量的反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们通过将新功能放在开关后面，并一次部署一小部分来降低风险。这是他们已经做了好几年的事情，而且效果非常好，Mjölne Söderbom说。如果生产环境中出现故障，很容易找到、修复和回滚/前进，他补充说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在之前的文章中，Hast和Mjølne Söderbom提到他们的团队&lt;a href=&quot;https://www.infoq.com/news/2025/07/pair-programming-speed-flow/&quot;&gt;使用TDD进行结对和mob编程&lt;/a&gt;&quot;；没有单独的任务或单独的代码审查。这种方法提高了代码质量，减少了浪费，并促进了知识的共享：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;经过多年的实践，我们最终一起工作，进行TDD，然后部署到生产环境。我们很少在本地或测试环境中测试应用程序。这从来不是我们的主要意图；这只是我们工作方式的一个（愉快的）结果。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2025/10/continuous-delivery-pairing/&quot;&gt;结对编程和持续集成可以相辅相成&lt;/a&gt;&quot;。每天多次向主服务器推送是很困难的，这会导致延迟、大型PR和合并问题。结对使即时代码审查、更容易的重构、更少的错误和更高的团队韧性成为可能，Hast和Mjølne Söderbom解释说。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hast提到他们过去有更多的测试，测试整个应用程序的运行，但他们已经将这个减少到最低；他们通常只有一条愉快的路径测试，以及针对任何特殊错误情况的额外测试：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们不能用单元测试中测试的东西，我们更喜欢在生产环境中测试。测试总是现实的近似，我们总是与长供应链和糟糕的测试数据作斗争。我们总是从生产环境中得到最好的反馈。停止使用测试环境本身从来不是一个目标，但它只是另一个令人愉快的副作用。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最重要的是建立反馈循环，Mjølne Söderbom提到。反馈有助于导航和选择方向，必要时改变方向：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们从我们的测试中得到最快的反馈，从生产环境中得到最好的反馈。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当某件事情很痛苦时，他们会更频繁地这样做，Hast解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们公司今天之所以能走到这一步，是因为我们增加了部署到生产环境的频率，并在问题最严重的地方迅速得到反馈，然后修复了这些问题。这个过程已经持续了10年。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们非常专注于开发过程中所有级别的快速反馈循环，Mjølne Söderbom说。TDD是我们获得早期和快速反馈的最重要的工具之一，他解释说：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你的代码难以测试，通常意味着设计有问题。代码与我们“对话”，并驱动设计。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;许多人认为TDD是一个测试工具，但实际上，它是一个设计工具。Mjølne Söderbom总结说，拥有适当的测试来实现快速流动是一个（非常好的）副作用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/feedback-TDD-production/&quot;&gt;https://www.infoq.com/news/2026/02/feedback-TDD-production/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/bibe64DvY3it15efBbta</link><guid isPermaLink="false">https://www.infoq.cn/article/bibe64DvY3it15efBbta</guid><pubDate>Sun, 08 Feb 2026 00:00:00 GMT</pubDate><author>Ben Linders</author><category>软件工程</category></item><item><title>AI 智能体界的 npm 来了！Vercel 推出 Skills.sh，欲统一智能体指令集</title><description>&lt;p&gt;Vercel 最近发布了开源项目 &lt;a href=&quot;https://vercel.com/changelog/introducing-skills-the-open-agent-skills-ecosystem&quot;&gt;Skills.sh&lt;/a&gt;&quot;，想要给 AI 智能体（Agents）配上一套“标准动作库”。简单来说，它让智能体能通过命令行执行各种可复用的操作，也就是所谓的“技能”（Skills）。Vercel 将其定义为一个&lt;a href=&quot;https://skills.sh/&quot;&gt;开放的智能体技能生态系统&lt;/a&gt;&quot;，开发者可以在这里定义、分享并运行一个个独立的指令，供智能体在工作流中随时调用。这一工具的核心逻辑，是把智能体的“推理”和“执行”分开——让智能体去调用那些受控、预定义的命令，而不是由它自己去瞎猜、乱写 shell 逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在技术架构上，Skills.sh 充当了一个轻量级运行时环境，允许智能体调用以 &lt;a href=&quot;https://learn.microsoft.com/en-us/powershell/utility-modules/aishell/concepts/what-is-a-command-shell?view=ps-modules&quot;&gt;shell 脚本形式&lt;/a&gt;&quot;实现的各种技能。每一项技能都遵循简单的契约协议，明确定义了其输入、输出和执行行为。这使得智能体能够以一种可预测、可审计的方式执行各项任务，例如读取或修改文件、运行构建步骤、调用 API 或查询项目元数据。由于技能具有显式定义和版本控制的特性，开发团队可以更清晰地了解智能体被授权的操作范围，并在开发或生产环境中对其行为进行审查。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些技能的设计兼顾了本地开发与自动化环境（如 CI 流水线）。开发者可以在&lt;a href=&quot;https://skills.sh/docs&quot;&gt;本地机器安装 Skills.sh&lt;/a&gt;&quot; 直接运行技能，同时将相同的技能无缝集成到由智能体驱动的工作流中。这种一致性旨在减少从实验阶段转向结构化应用场景时的阻力。此外，技能通过简单的配置文件进行描述，这使得开发者无需引入额外的框架或沉重的依赖库，即可轻松地检查、扩展或自定义功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Vercel 将该生态系统定位为开源及社区驱动。开发者可以发布自己的技能，并复用他人创建的成果，从而形成一个共享的常用智能体动作库。根据公司分享的早期使用数据，该项目在发布后迅速获得了广泛关注，安装量据报已达数万次。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;社区评论更多地聚焦于该方案的实用性而非新颖性。X 平台上的开发者指出，许多智能体任务的失败并非源于推理能力不足，而是由于执行环节的不可靠，而“技能层”的引入正好填补了这一空白。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;软件开发者 Thomas Rehmer &lt;a href=&quot;https://x.com/thomas_rehmer/status/2016018978250834305&quot;&gt;评价&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;确实该这么搞。有了这些‘可发现’的技能，总算把智能体架构里那个‘你能干嘛？’的经典难题给破了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，AI 工程师 Aakash Harish &lt;a href=&quot;https://x.com/0_Aakash_0/status/2014024888575729964&quot;&gt;发文&lt;/a&gt;&quot;称：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这就是 AI 智能体界的 npm。它的精髓在于：比起纠结协议有多复杂，Skills 更看重好不好组合。如果说 MCP 搞定的是‘智能体怎么跟工具搭火’，那 Skills 搞定的就是‘开发者怎么分享和找现成的能力’。这俩以后肯定不是谁取代谁，而是强强联手：用 Skills 搞定发现和共享，用 MCP 去啃那些对确定性要求极高的企业级硬骨头。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不少开发者将 Skills.sh 与目前围绕智能体执行兴起的其他工具和标准进行了对比。类似的理念也出现在其他协议驱动的方案中，例如 Anthropic 推出的 &lt;a href=&quot;https://www.infoq.com/articles/mcp-connector-for-building-smarter-modular-ai-agents/&quot;&gt;Model Context Protocol&lt;/a&gt;&quot; (MCP)（侧重于通过结构化的 API 访问工具和数据），以及 OpenAI 的 &lt;a href=&quot;https://www.infoq.com/news/2023/06/openai-api-function-chatgpt/&quot;&gt;Function Calling&lt;/a&gt;&quot;（通过 JSON schema 暴露预定义动作）。此外，包括 &lt;a href=&quot;https://docs.langchain.com/oss/python/langchain/tools&quot;&gt;LangChain&lt;/a&gt;&quot; 的 tools 和 CrewAI 的 &lt;a href=&quot;https://docs.crewai.com/en/concepts/tasks&quot;&gt;tasks&lt;/a&gt;&quot; 在内的其他项目也致力于为智能体提供受控的执行权限，不过它们通常依赖于更高层的 Python 抽象，而非基于 shell 的命令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/vercel-agent-skills/&lt;/p&gt;</description><link>https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</link><guid isPermaLink="false">https://www.infoq.cn/article/SaRHSmwKTghurtuafWHy</guid><pubDate>Sat, 07 Feb 2026 00:00:00 GMT</pubDate><author>作者：Daniel Dominguez</author><category>AI&amp;大模型</category></item><item><title>“16个Agent组队，两周干翻37年GCC”？！最强编码模型Claude Opus 4.6首秀，10万行Rust版C编译器跑通Linux内核还能跑Doom</title><description>&lt;p&gt;Anthropic 正在升级它“最聪明的模型”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着新一代旗舰模型 Claude Opus 4.6 的发布，Anthropic 释放出的信号十分明确：这并不是一次常规的性能小修小补，而是一轮围绕长任务、复杂工作，以及智能体（agent）如何真正干活展开的系统性升级。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c1/c1550330d571d4dfdb7e5f7f8795d540.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这次发布之前，Anthropic 内部和部分早期用户已经开始让 Opus 4.6 参与一项持续时间很长的工程任务：从零开始，用 Rust 编写一个完整的 C 编译器，并要求它能够编译 Linux 内核。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这项实验持续了约两周时间，期间累计运行了近两千次 Claude Code 会话，最终产出了一个规模约 10 万行代码的编译器。该编译器不仅能够在多种架构上构建 Linux 6.9，还可以编译 FFmpeg、Redis、PostgreSQL、QEMU，并通过了 GCC 自身 99% 的 torture test，甚至能够成功编译并运行 Doom。整个实验的 API 成本约为 2 万美元。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了让外界更直观地理解这一成果的尺度，有网友在社交平台上给出了一个对照：GCC 的开发从 1987 年开始，历经 37 年，投入过数以千计的工程师。而这一次，是一名研究者加上 16 个 AI 智能体，在短短数周内完成了一个能够通过大量 GCC 测试集、并编译真实大型项目的编译器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fb/fbfb1e958df9b19689507e724aefd217.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在这样一段持续推进的工程实践之后，Anthropic 对外发布了 Claude Opus 4.6。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;成立于 2021 年、由一批前 OpenAI 研究人员和高管创立的 Anthropic，一直以 Claude 系列大模型为核心产品；在这一体系中，Opus 代表最大、能力最强的型号，Sonnet 和 Haiku 则分别覆盖中等与轻量级使用场景。某种程度上，Opus 系列承担的角色，就是在更复杂、更长期的任务环境中检验 Claude 的能力边界。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;最强的编码模型：从跑分看 agentic 编程能力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 对 Opus 4.6 的定位，并不只是“更会写代码”。他们强调，新模型在编程能力上的提升，已经从单纯的代码生成，扩展到更前置的任务规划，以及更后置的代码审查与调试流程。这种变化，使模型能够在大型代码库中更稳定地工作，也直接决定了它是否有能力脱离短对话模式，持续参与多阶段、长周期的工程任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种定位在评测结果中体现得比较清楚。Anthropic 公布的多项基准测试显示，Claude Opus 4.6 在 agentic 编程、计算机使用、工具调用、搜索以及金融等任务上，整体跑分都有所提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/67/678307d2b742d377329cb0b226c856aa.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在终端 agentic 编程能力上，Opus 4.6 得分 65.4%，对比来看，略高于 GPT-5.2 的 64.7%，明显领先 Gemini 3 Pro（56.2%）和 Sonnet 4.5（51.0%）。这说明在纯终端环境下执行多步编程任务时，Opus 4.6 的稳定性和自我修正能力处在第一梯队。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在 SWE-bench Verified（Agentic coding） 上，各家分数非常接近，Opus 4.6（80.8%）与 Opus 4.5（80.9%）、GPT-5.2（80.0%）基本处于同一水平。这里可以理解为：在标准化的软件工程任务上，能力已经开始趋同。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但在电脑操作（OSWorld）上，代际差异开始显现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;OSWorld（Agentic computer use） 是一个比较关键的分水岭。Opus 4.6 达到 72.7%，相比 Opus 4.5 的 66.3% 有明显提升，而 Sonnet 4.5 只有 61.4%，其他模型则未给出对等数据。这类评测关注的是 GUI 操作、跨应用流程和状态理解能力。放在整张表里看，它与编程能力的同步提升，意味着 Opus 4.6 不只是“会想”，而是更擅长把计划落到具体操作上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Agentic search（BrowseComp）：明显拉开差距。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;BrowseComp 是整张表里差距最清楚的一项。Opus 4.6 为 84.0%，而 GPT-5.2 Pro 是 77.9%，Opus 4.5 只有 67.8%，Sonnet 4.5 更低。这一项测的是在真实开放网络中定位、筛选和组合信息的能力，结果说明 Opus 4.6 在“研究型 agent 行为”上已经明显领先，而不是只在封闭工具或结构化任务中占优。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另外，在 Humanity’s Last Exam（跨学科推理）和 ARC-AGI-2（新问题解决） 上，Opus 4.6 的优势更加明显，尤其是 ARC-AGI-2 的 68.8%，相比 GPT-5.2 Pro 的 54.2% 和 Gemini 3 Pro 的 45.1%，已经不是细微差距。这类评测通常更难通过“提示工程”或策略优化取得跃升，更像是在反映模型本身的泛化推理能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;“上下文腐烂”与模型可用性的分水岭&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Opus 4.6 还扩大了上下文窗口，也就是单次会话里可记住、可处理的信息量更大。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新模型在 Beta 阶段提供 100 万 token 的上下文长度，与该公司现有的 Sonnet（4 和 4.5 版本）相当。Anthropic 表示，这样的上下文容量更适合处理更大型的代码库，也能支持对更长文档的分析与处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但Anthropic 特别强调，Opus 4.6 的提升并不是“能塞更多 token”，而是“塞进去之后还能用”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们在说明中提到，Opus 4.6 在大规模文档中检索关键信息的能力显著增强，这一点在长上下文任务中尤为明显：它可以在数十万token 范围里持续跟踪信息，偏差更小，也更容易捕捉到埋得很深的细节——包括一些 Opus 4.5 本身就已经容易漏掉的信息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这正好对应了开发者长期吐槽的一个问题：“上下文腐烂（context rot）”。很多模型在对话或任务一旦拉长之后，要么开始遗忘早期信息，要么虽然“看过”，但已经无法在后续推理中正确调用，最终表现为前后不一致、定位问题跑偏、重复试错。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;MRCR v2（8-needle、100 万 token）这类“草堆找针”测试，本质上就是在专门检验这种能力：把多个关键线索埋在超长文本里，看模型能否在不迷路的情况下把它们重新找出来。Opus 4.6 在该测试中的得分为 76%，而 Sonnet 4.5 仅为 18.5%。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这并不是简单的“高一点、低一点”，更像两种不同的可用性状态：一个模型在超长上下文中仍然能稳定检索并利用信息，另一个则在任务拉长后迅速失效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/746521ff51da955e770b0155c22f7bec.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种长上下文的稳定性，直接影响模型能否胜任更“工程化”的工作，尤其是复杂代码分析与故障诊断。在 Anthropic 给出的能力图中，Opus 4.6 被特别标注为擅长做 root cause analysis（根因分析）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/caf21288bc8205c65017476277640bed.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;用Agent团队，构建一个C编译器&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;4.6 最醒目的新增功能，是 Anthropic 所称的“智能体团队”（agent teams）：由多个智能体组成的小队，可以把一个大任务拆成若干独立的子任务分别推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 的说法是：“不再让单个智能体按顺序把任务一路做到底，而是把工作分给多个智能体——每个智能体负责自己的一块，并直接与其他智能体协调。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Anthropic 产品负责人 Scott White 将其类比为“雇了一支很能干的人类团队”，因为职责拆分后，智能体可以并行协作，从而更快完成工作。目前，“智能体团队”以研究预览（research preview）的形式向 API 用户与订阅用户开放。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编译器本身固然是一个高度复杂、且极具工程价值的成果，但在 Anthropic 团队看来，它更像是一次“能力压力测试”的载体。真正值得总结的，是围绕 长时间运行的自治 Agent 团队（long-running autonomous agent teams） 所形成的一整套工程方法论：如何设计无需人工干预的测试体系、如何让多个 Agent 并行推进复杂工作、以及这种架构在现实工程中究竟会在哪些地方触碰到上限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;从“协作式 Agent”到“自治式 Agent”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现有的 Agent scaffolding（例如 Claude Code）本质上仍然是人机协作系统：模型在解决复杂问题时，往往会在某个阶段停下来，等待操作者继续输入新的指令、确认状态，或澄清歧义。Anthropic 的实验目标是消除这种对“人类在线”的依赖，让 Claude 能够在无人监督的情况下，持续推进一个长期任务。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了实现持续自主的进展，Claude 工程团队并没有引入复杂的调度系统，而是构建了一个程序，让 Claude 进入一个简单的循环（如果你见过 Ralph 循环，应该会觉得眼熟）：每完成一个任务，就立刻进入下一个任务，而不是回到“等待用户”的状态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/24/24da9de1991a3726416c38d0c3099bb2.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Agent prompt 中，Claude 被明确要求将问题拆解成可执行的小任务、记录当前进展、判断下一步行动，并持续迭代，直到系统判定“没有明显改进空间”。（在这最后一点上，Claude 没有选择，因为循环会一直运行——不过在一次实验中，团队确实看到 Claude 不小心执行了 pkill -9 bash，结果把自己杀掉了，循环也就随之结束了。）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;并行运行 Claude&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并行运行多个实例，可以缓解单一 agent harness 的两个弱点：&lt;/p&gt;&lt;p&gt;一次 Claude Code 会话同一时间只能做一件事。随着项目范围扩大，并行调试多个问题会高效得多。&amp;nbsp;运行多个 Claude agent 可以实现“分工”。当一部分 agent 负责解决核心问题时，其他专门的 agent 可以被调用来（例如）维护文档、盯代码质量，或处理更专门的子任务。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Claude 工程团队的并行实现非常基础：先创建一个新的裸 Git 仓库；然后为每个 agent 启动一个 Docker 容器，把仓库挂载到 /upstream。每个 agent 会在容器内克隆一份本地副本到 /workspace，完成工作后，从各自的容器把改动推回 upstream。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为避免两个 agent 同时尝试解决同一个问题，harness 使用了一个简单的同步算法：&lt;/p&gt;&lt;p&gt;Claude 通过在 current_tasks/ 下写入一个文本文件来“锁定”某个任务（例如，一个 agent 可能锁定 current_tasks/parse_if_statement.txt，另一个锁定 current_tasks/codegen_function_definition.txt）。如果两个 agent 试图认领同一任务，Git 的同步机制会迫使第二个 agent 改选另一个任务。&amp;nbsp;Claude 在任务上工作完成后，会从 upstream 拉取、合并其他 agent 的改动、推送自己的改动，然后移除锁。合并冲突很常见，但 Claude 能够处理。&amp;nbsp;无限的 agent 生成循环会在一个全新的容器里启动新的 Claude Code 会话，然后重复上述流程。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这是一个非常早期的研究原型。Claude 工程团队尚未实现任何其他 agent 之间的通信方法，也没有强制任何高层目标管理流程，也没有使用 orchestration agent。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;相反，团队把“如何行动”的决定权交给每个 Claude agent。多数情况下，Claude 会选择“下一个最显而易见”的问题继续做；当卡在某个 bug 上时，Claude 往往会维护一份持续更新的文档，记录失败过的方法和剩余任务。在项目的 Git 仓库里，可以通过历史记录看到它如何在不同任务上获取锁并推进。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;用 Claude 团队写代码：一些更管用的做法&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把 Claude 放进循环只是起点，真正决定它能否持续推进的，是它能不能从环境和反馈中判断“下一步该做什么”。因此，Claude 工程团队把大量精力放在模型之外：测试如何设计、反馈如何呈现、运行环境如何约束，才能让 Claude 在无人干预的情况下仍然保持方向感。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;一个核心前提是：必须围绕语言模型的固有限制来设计系统。在这次实践中，团队重点应对了两类限制。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;首先是上下文窗口污染。测试框架不能输出成千上万字节的无用信息，最多只保留几行关键输出，其余重要内容统一写入文件，供 Claude 在需要时自行查阅。日志也需要便于自动处理：一旦出现错误，必须在同一行明确标出 ERROR 以及失败原因，方便grep直接检索。同时，能提前算好的汇总统计信息会被预先计算，避免 Claude 在上下文中反复做同样的推导。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一类限制是时间盲。Claude 无法感知时间，如果无人干预，很容易长时间沉浸在跑测试里而不推进工作。为此，测试框架很少输出增量进度，避免不断污染上下文，并提供默认的 --fast 选项，只运行 1% 或 10% 的随机子样本。这个子样本对单个 agent 是确定的，但在不同虚拟机之间是随机的，从整体上仍能覆盖所有文件，同时又能让每个 agent 精确识别回归问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在并行方面，团队也很快意识到：并行是否有效，取决于问题是否“好拆”。当失败测试数量多且彼此独立时，并行非常直接——每个 agent 处理一个不同的失败测试即可。在测试通过率接近 99% 后，团队让不同 agent 分别去完成不同小型开源项目的编译，例如 SQLite、Redis、libjpeg、MQuickJS 和 Lua。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但当任务升级到编译 Linux 内核时，情况发生了变化。内核编译本质上是一个高度耦合的整体任务，所有 agent 都会命中同一个 bug，修完再相互覆盖。即便同时运行 16 个 agent，也无法带来实质进展，因为大家都卡在同一件事上。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决办法是引入 GCC 作为在线的、已知良好的对照编译器。团队编写了新的测试框架：随机选择内核中大部分文件用 GCC 编译，只把剩余文件交给 Claude 的 C 编译器。如果内核能够正常运行，说明问题不在 Claude 负责的那部分文件；如果失败，则再通过把其中一些文件切回 GCC 编译，逐步缩小范围。这样一来，不同 agent 就可以并行地修复不同文件中的不同错误，直到 Claude 的编译器最终能够编译全部文件。即便如此，后续仍需要配合增量调试（delta debugging），找出那些“单独没问题、组合在一起就失败”的文件对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;并行运行也带来了另一层收益：角色分工成为可能。在实践中，Claude 工程团队发现，LLM 生成的代码很容易重复实现已有功能，因此专门安排了一个 agent 负责扫描并合并重复代码；另一个 agent 聚焦于提升编译器自身的性能；第三个 agent 负责改进生成代码的效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除此之外，还有 agent 从 Rust 开发者的视角审视整个项目的设计，提出结构性调整建议，以提升整体代码质量；另一个 agent 则专注于文档维护。通过这种方式，不同 Claude 实例在同一代码库中承担起相对稳定的职责，而不是反复在同一层面“重新发明轮子”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;评估结果与能力边界&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在两周内接近 2,000 次 Claude Code 会话中，Opus 4.6 共消耗约 20 亿输入 token、生成约 1.4 亿输出 token，总成本略低于 2 万美元。该团队表示，即便与最昂贵的 Claude Max 方案相比，这仍是一次成本极高的实验；但这一成本依然远低于由单人、甚至完整人类团队完成同等工作的成本。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器是一次完全的 clean-room 实现：开发过程中 Claude 从未获得互联网访问权限，仅依赖 Rust 标准库。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终得到的约 10 万行代码，能够在 x86、ARM 和 RISC-V 架构上构建可启动的 Linux 6.9，同时也可以编译 QEMU、FFmpeg、SQLite、Postgres、Redis，并在包括 GCC torture test 在内的大多数编译器测试套件中达到约 99% 的通过率。此外，它还通过了开发者的终极考验：它可以编译并运行 Doom 游戏。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但与此同时，这一项目也把当前 Agent 团队的能力边界暴露得相当清晰。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;缺乏启动 Linux 所需的 16 位 x86 编译能力，因此在 real mode 阶段会调用 GCC（x86_32 与 x86_64 编译器由其自身实现）。尚未拥有稳定可用的 assembler 与 linker；这些是 Claude 开始自动化的最后环节，目前仍存在问题，演示中使用的是 GCC 的相关工具。该编译器能够成功编译许多项目，但并非所有项目都能成功。它目前还不能完全替代真正的编译器。生成的代码效率不高。即使启用所有优化，其效率也低于禁用所有优化的 GCC 生成的代码。Rust 代码质量尚可，但远不及 Rust 专家级程序员编写的代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;整体实现已接近 Opus 的能力上限，新增功能或修复 bug 时，经常会破坏已有功能。其中一个最具代表性的难点是 16 位 x86 代码生成。尽管编译器可以通过 66/67 opcode 前缀生成语义正确的 16 位 x86 代码，但生成结果超过 60KB，远高于 Linux 强制的 32KB 限制。因此，在这一阶段，Claude 选择调用 GCC 作为替代（该情况仅出现在 x86 上；在 ARM 与 RISC-V 架构下，编译可完全由 Claude 自身完成）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该编译器的源码已经公开：&lt;a href=&quot;https://github.com/anthropics/claudes-c-compiler&quot;&gt;https://github.com/anthropics/claudes-c-compiler&lt;/a&gt;&quot;。Claude 工程团队建议直接下载、阅读代码，并在自己熟悉的 C 项目上尝试。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/dc/dc71d0e9a3b680a980d90fc83313bccf.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/news/claude-opus-4-6&quot;&gt;https://www.anthropic.com/news/claude-opus-4-6&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.anthropic.com/engineering/building-c-compiler&quot;&gt;https://www.anthropic.com/engineering/building-c-compiler&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</link><guid isPermaLink="false">https://www.infoq.cn/article/NPCsobRV3mTlFpYgZh1S</guid><pubDate>Fri, 06 Feb 2026 08:33:16 GMT</pubDate><author>Tina</author><category>生成式 AI</category></item><item><title>TypeScript 之父 Anders Hejlsberg：别折腾“AI新语言”了，真正变天是 IDE 让位给 Agent</title><description>&lt;p&gt;过去十年，TypeScript 被很多团队当作“工程化 JavaScript”的答案；到了 AI 编程时代，它又意外变成了 AI 最顺手的语言之一——原因很简单：AI 写代码的能力基本取决于它见过多少这种语言的代码，而 TypeScript/JavaScript 恰好是训练语料最丰富的那一档；更关键的是，TypeScript 还把类型与接口这些“语义线索”明明白白写在代码里，正好让 AI 更容易理解、重构和补全。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;也正是在这种背景下，微软在 2025 年 12 月 16 日完成了 TypeScript 史上最激进的一次重构：用 Go 语言迁移（重写）编译器与部分工具链，宣称带来 10 倍性能飞跃。但消息一出，社区立刻炸锅——明明 Rust 才是当下重写系统级工具的“默认答案”，为什么偏偏选 Go？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 之父、同时也是 Turbo Pascal、Delphi、C# 等语言的核心设计者 Anders Hejlsberg，在与 GitHub 研究顾问 Eirini Kalliamvakou 的对谈中正面回应了这些质疑：很多人认为他们“应该选另一门语言”，但他坚持“我们选了最合适的工具，而且过去一年已经证明了这一点”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更有意思的是，Hejlsberg 也谈到 AI 在这次迁移中的真实位置：团队曾尝试让 AI 直接把 TypeScript 代码迁移到 Go，“结果不太理想”，因为他们需要的是五十万行代码级别、行为完全一致的确定性迁移；AI 只要偶尔“偏一点”，就会把成本转移到逐行审查上，得不偿失。相比之下，让 AI 去生成迁移工具、以及在迁移之后自动同步旧代码库新增的 PR 变更，反而更有效——这部分他们“已经相当成功”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Hejlsberg 也明确指出：在“AI 无处不在”的新环境里，TypeScript 的语言服务（补全、跳转、重构、快速修复）不会只是原样搬家，而是在被大幅重塑——因为很多过去必须靠 IDE 才能做的事，AI 将会做得更好。未来真正不确定的也不是 TypeScript 语言本身（它仍沿着 JavaScript 标准化路径演进），而是工具形态：AI 正从 IDE 的助手变成主要工作者，人类转向监督与审阅；这也是为什么把语言服务接入 MCP 这类机制会突然变得诱人——让 AI 能提出语义级问题、发起重构请求，用“智能体方式”完成过去只能在 IDE 里完成的工作流，开发工具将因此被彻底改写。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于该播客视频，InfoQ 进行了部分删改。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;核心观点如下：&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。找到那些无聊却昂贵的事情，把它们交给 AI。工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;不如直接修 JS：TypeScript 的顿悟时刻&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从 Turbo Pascal、Delphi、C# 到如今的 TypeScript，你的工作塑造了数以百万计开发者每天写代码的方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我第一次接触计算机大概是在高中时期，那是 70 年代末，我很快对编程产生了浓厚兴趣。后来，随着 8 位微型计算机开始出现，我决定自己动手组装一台套件机，并为它编写大量软件。我发现自己在这方面做得相当不错，而且也真的很享受这个过程。那时无论是结构化编程还是汇编语言，对我来说都不成问题。当然，还要考虑一个现实条件：只有 64K 内存，能塞进去的东西非常有限，还得给用户留出空间，所以当时一切都还能装在脑子里。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 在很大程度上革新了开发者体验，核心在于缩短开发者的反馈回路。这在多大程度上是你一开始就有意识的设计理念？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：首先，人们之所以喜欢早期机器上的 BASIC，很大原因正是它的短反馈周期。BASIC 是解释型语言，输入代码就能立刻运行，但代价是运行速度慢，而且编辑器是基于行的，体验很糟。相比之下，当时的文字处理软件已经是所见即所得的屏幕编辑器，可以自由移动光标，这显然更适合写代码。与此同时，“输入—运行—立刻看到结果”的模式又非常吸引人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要在编译型语言中实现这一点并获得性能，就必须有极快的编译器。Turbo Pascal 的做法是：你一按下运行，它立即在内存中完成编译，甚至不需要访问磁盘，然后直接运行。如果出现错误，就立刻回到编辑器。编译器本身非常原始，你甚至需要通过出错地址反推源码位置。但正因如此，突然之间就获得了一种高度交互的体验，在当时堪称革命性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：那种“编译要等一个下午”的体验，会不会让你感到沮丧？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：当然会，没有人喜欢等待。代码已经写完了，你只想立刻运行，而不是坐在那里干等。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Turbo Pascal 还有一个重要影响，就是以低价让更多人接触到编程。回头看，这一点你有什么感受？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这里有个有意思的故事。Turbo Pascal 的前身叫 Poly Pascal，是我在丹麦一家小型软件公司里开发的 Pascal 编译器。后来我们联系上了 Borland 的创始团队，他们看过之后觉得非常惊艳，提议一起把它作为产品推向美国市场。然后他们决定定价 49.95 美元。我当时的反应是：“你们疯了吗？这样根本赚不到钱。”事后看来，这个决定非常聪明。虽然价格只有原来的十分之一，但销量却高出了三到四个数量级。最终结果非常成功，这个功劳主要要归于 Borland 的创始人Philippe。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：Delphi标志着你从独立创作者向团队领导者的转变。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：一开始我基本上是单打独斗，虽然在丹麦的 Borland 办公室也会和两三个人合作，但随着机器性能飞速提升、用户期望不断提高，这种模式显然无法持续。我必须学会团队协作，这在 Turbo Pascal 期间就已经开始，而在 Delphi 项目中尤为明显。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你必须接受事情不会完全按照你个人偏好的方式完成，代码也不一定长成你心目中的样子。而且你往往没有时间亲自去“修正”这些细节，何况那样做也未必真的改变产品行为，更重要的是学会放权。只有当团队成员在各自负责的功能和模块中感到被信任、被赋权，团队才能真正运转起来。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你在 Microsoft 参与 Visual J++ 的经历，对 C# 和 .NET 平台的目标产生了怎样的影响？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我是在 1996 年底加入 Microsoft 的，担任 Java 开发工具集的首席架构师。我们刚发布了 Visual J++ 1，本质上只是把 C++ 的 IDE 换成 Java 编译器，谈不上真正的集成，更没有快速的应用开发体验。于是我们着手改进，这最终成为 Visual J++ 6.0，并开始与 Visual Basic、Visual Studio 的版本体系对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果要为 Microsoft 的 DOS 和 Windows 平台做 Java，就必须与这些环境高度互操作。但 Java 当时强调“一次编写，到处运行”，强制使用最小公分母的 UI 接口，最终只能做出体验很差的小程序。我们不得不引入一些扩展，简化与原生平台的互操作，并构建封装 Windows UI 的类库。很快就发现，这条路走不通。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果真正为用户着想，就必须允许构建针对特定环境的最佳方案。人们既想要 Visual Basic 的易用性，又想要 C++ 的表达能力。于是我们尝试把这两者结合起来，并构建在 .NET 这样一个可持续演进的平台之上，最大限度地利用用户所运行的系统能力，这正是整个构想的核心。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你既在谈不同类型的用户，又在描述为整个生态系统服务的思路，这种整体视角是如何形成的？&lt;/p&gt;&lt;p&gt;Anders：用户并不在乎这是语言特性、框架特性、平台能力，还是编辑器或调试器的问题，对他们来说，一切加在一起才是“体验”。因此，这些部分必须协同设计。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们构建了运行时、JIT 编译器、垃圾回收器，设计了平台的字节码，开发了类库。我既参与语言设计，也参与类库和运行时的设计，与负责这些组件的工程师密切合作，最终效果因此更好。否则，各自为政会形成孤岛，最后只能靠复杂的互操作层勉强拼接，结果自然谈不上“最佳实践”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，我们也不惧怕与底层平台深度互操作。过于教条地坚持最小公分母，拒绝利用具体平台的优势，这样永远不可能做到真正意义上的“最佳体验”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当时是否有某个“顿悟时刻”，让你意识到 JavaScript 在规模化发展中的阵痛已经成为必须解决的问题，而且这是一个需要由你、由 Microsoft 来解决的问题？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：行业里的运行时环境开始变得足够成熟，比如 Google 在 V8 上做了非常出色的工作，JavaScript 的运行性能突然变得相当可观。HTML5 也正式定稿，UI 能力大幅提升。同时，手机、iPad 等各种形态的设备出现了，而它们并不运行 Windows。整个行业突然意识到，真正的平台竞争不在 Java，而在 JavaScript、浏览器和 HTML 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，人们开始编写越来越庞大的应用，因为新的运行时和 UI 技术已经允许这样做。但很快大家就发现，在一种动态语言里、缺乏成熟工具支持的情况下，这件事难得令人发指，于是我们看到了各种奇怪的扭曲做法。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中一个典型案例是 Outlook.com 团队找到我们，主要是 .NET 和 C# 团队，询问是否可以把他们内部的一个叫 Script# 的东西产品化。我当时一头雾水：“Script# 是什么？”他们说，这是一个允许你用 C# 编写代码，然后编译成 JavaScript 来运行的工具。我第一反应是：这听起来像是两边的缺点都占全了。&lt;/p&gt;&lt;p&gt;但事实是，这么做的真正原因在于可以获得“成熟的工具链”：类型检查、团队协作能力、接口定义，以及对模块之间交互方式的清晰描述。因为他们有成百上千名程序员参与开发，不可能只靠裸写 JavaScript，在没有检查、没有自动补全、没有重构支持的情况下完成工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这让我开始反思：JavaScript 真的已经糟糕到这种程度了吗？而“先写另一门语言，再把 JavaScript 当成中间表示或字节码来编译”，真的是解决问题的最佳方式吗？如果能直接修复 JavaScript 本身，会发生什么？于是我们开始探索这条路，事实证明，这个方向效果相当不错。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;JavaScript 单线程的天花板：我们在浪费 90% 的算力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：TypeScript 被设计成 JavaScript 的严格超集，这背后显然有深思熟虑的战略考量。你是如何坚持这一决策的？这又能给其他语言设计者哪些启示？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：每当有人跑来跟我说：“我在考虑做一门全新的编程语言，能解决这个、解决那个”，我给出的第一条建议通常是：这个世界对新编程语言的需求，就像对头上再多一个洞的需求一样。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二条建议是：如果你真的要做一门语言，要意识到其中 90% 的工作，和其他所有语言是完全一样的，而且这个比例还在不断上升。如今，程序员的期望早已不只是一个编译器，你还需要完善的语言服务，能够集成到几乎所有主流 IDE 中，需要能与 AI 交互的 MCP 服务器，需要调试器、性能分析工具……&lt;/p&gt;&lt;p&gt;此外，你还得有至少十年的时间，因为一门语言真正站稳脚跟、获得有意义的用户规模，往往就是这么长的周期。没有人会在一开始就拥抱一门全新的语言，头五年你很可能用户寥寥，还得不断回答“我们真的要继续投入吗？”这是一门非常艰难的生意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过扩展 JavaScript 的能力，我们并非要创造一门全新的语言，而只是想修复它本身存在的问题。在 Visual Studio Code 里，我们的语言服务对 TypeScript 和 JavaScript 是同一套。对我们而言，JavaScript 只是没有类型注解的 TypeScript，或者使用 JSDoc 注解的另一种形式。这意味着我们不是在做两套东西，而是在同一项投入之上，精确地构建真正必要的能力，只为让整个生态系统变得更好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：2012 年在 GitHub 上启动 TypeScript 的开源项目，在当时的 Microsoft 可以说是一次相当激进的举动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我们当时对 JavaScript 生态的运作方式、价值观以及参与社区所需的前提，其实有着非常清晰的认识。大家都明白，如果不开源，这个社区根本不会理你，一个封闭的商业产品对他们毫无吸引力。因此，从一开始我们就主张开源。同时，Microsoft 内部也逐渐意识到，开源并不是“洪水猛兽”，而是如果想真正与开发者对话，就必须拥抱的现实。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你刚才提到 2012 年发布在 GitHub，其实并不准确。2012 年我们发布在 CodePlex——Microsoft 自家的、并不太受欢迎的开源平台上。结果就是，反响寥寥。那时所谓的“开源”，更多只是把代码丢到仓库里，让大家提 issue，然后我们再把这些 issue 抓回内部系统，按内部流程处理。某种程度上，这也解释了为什么几乎没人关注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再加上当时 Microsoft 在开源社区中的信誉并不高。真正的“主战场”在 GitHub。于是 2014 年我们迁移到了 GitHub，全面采用开放式开发流程。内部成员和外部贡献者遵循完全相同的规则，所有功能都通过 Pull Request 提交，所有讨论都公开进行。直到那时，项目才真正开始起飞，社区的兴趣也随之而来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：从“把代码扔给社区”到真正的开放式开发，你们学到了哪些经验？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：这是一个彻头彻尾的双赢。对用户来说，他们能看到“香肠是怎么做出来的”，所有讨论都在公开的 issue 里完成，而不是私下做决定后再给出一个结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对项目开发者而言，这种方式同样令人满足。你每天都能感受到社区的参与和认可，看到点赞、看到讨论，远比关起门来做六个月或一年，然后祈祷产品方向正确要有趣得多。在这里，用户每天都在用投票告诉你他们最想要什么功能，我们只需按票数排序，就能清楚地知道优先级。你解决这些问题，社区的热情就会进一步增强，形成正向循环。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：把 TypeScript 编译器迁移到 Go 背后的动机、权衡以及所面临的挑战是什么？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：TypeScript 从一开始就是自托管的，用它自身编写，这意味着编译器和整个工具链本质上都是一个 JavaScript 应用。这带来了很多好处，比如你甚至可以在浏览器里运行编译器，在浏览器中构建一个完整的 IDE，一切都能正常工作。但随着 TypeScript 的广泛采用，以及用户项目规模不断扩大，可扩展性逐渐成为头号问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里有 JavaScript 本身的一些内在限制。它在设计上是单线程的，不支持共享内存并发，这意味着你实际上浪费了 90% 的计算能力。此外，JavaScript 的执行成本也很高，它的对象模型极为宽松，可以随意给对象加属性，这使得优化非常困难，底层往往演变成复杂的哈希查找和缓存机制，远不如原生代码高效。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以我们当时就清楚，自己正接二连三地白白损失收益。尽管放弃自托管让人非常不舍，但即便用尽所有优化技巧，性能瓶颈依然无法突破。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;于是，在 2024 年夏天，我们开始做原型验证，先从扫描器、解析器这些容易量化的模块入手。很快就发现，性能提升可以达到 10 倍：一半来自原生代码，一半来自共享内存并发。这样的提升能把原本需要几分钟的事情缩短到十几秒，完全改变了游戏规则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，我们也很清楚不想从零重写一个全新的编译器。社区里有不少声音主张“用 Rust 全部重写”，但我们认为这并不可行。TypeScript 的类型检查器极其庞大复杂，许多行为只体现在现有代码的精确语义中。如果重写，就会陷入永无止境的差异追赶，最终无法与旧编译器对齐。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此我们的目标是“迁移”，逐函数地把同样的逻辑搬到原生语言中。当然，这意味着必须重构数据结构，因为原生语言不允许像 JavaScript 那样随意给对象加属性。我们尝试过多种语言，很快排除了 Rust，因为我们的编译器充满了循环数据结构，并且高度依赖自动垃圾回收，使用 Rust 等同于重写。&lt;/p&gt;&lt;p&gt;最终我们选择了 Go。它在很多方面与 JavaScript 相似，这个选择对我们来说非常奏效。现在我们拥有了一个原生编译器，在功能上几乎是旧编译器的拷贝，连那些小怪癖都一模一样，只是快了 10 倍，这意味着社区不需要推倒重来。当我们正式切换时，我相信大家会非常满意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“最适合 AI 的语言”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：在 AI 辅助编程的背景下，你认为 TypeScript 为什么特别适合 AI 工作流？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：很多人问我，为什么不干脆设计一门“最适合 AI 的完美编程语言”。我的回答通常是：那样的语言反而会成为最不适合 AI 的语言，因为它不会出现在 AI 的训练数据中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 在某种语言中写代码的能力，与它见过这种语言代码的数量几乎成正比，它本质上是在大量样本的基础上进行再组合和外推。AI 已经见过海量的 JavaScript、Python 和 TypeScript，因此在这些语言上表现得非常好。对 AI 来说，“最好的语言”就是它已经大量见过的语言，在这个新世界里，全新的编程语言反而处于劣势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 的确正在改变我们构建产品的方式。以这次 TypeScript 迁移为例，我们也尝试过用 AI 自动完成代码迁移，但效果并不好。一方面那是一年前的 AI，能力还有限；另一方面，迁移五十万行代码并保证行为与原代码完全一致，我们需要的是极其确定性的结果。AI 在翻译过程中可能会产生细微的“幻觉”，而你又不得不逐行检查，这并不划算。在这种情况下，更好的方式或许是让 AI 帮你生成“迁移工具”，而不是直接迁移代码。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;TypeScript 项目中有很大一部分是语言服务，为 IDE 提供补全、跳转、重构和快速修复。现在我们也在思考：既然 AI 已经能在很多场景下做得更好，是否还有必要原样迁移这些功能？类型检查器我们完整迁移了，但语言服务正在被大幅度重塑，以适应一个“AI 已经无处不在”的新环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：你如何看待 AI 工具正在改变编程本身，以及语言设计的方式？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在理想状态下，AI 应该帮助我们消除编程中那些繁琐、重复的劳动。以 TypeScript 的迁移为例，在我们开发新代码库的同时，旧代码库里仍然不断有新的 Pull Request 出现，我们已经相当成功地用 AI 把这些变更迁移过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;再比如，项目里有成千上万条 issue，其中很多非常古老。它们是否仍然可复现？是否还相关？能否根据社区反馈排序？这些清理和维护工作过去总是被一拖再拖，最终却成为拖慢项目前进的“锚”。现在，我们可以构建 AI 机器人来完成这些工作。这是我认为非常重要的一点：找到那些无聊却昂贵的事情，把它们交给 AI。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，这也带来新的困惑。如果 AI 取代了初级工程师，我们又该如何培养资深工程师？难道指望 AI 自己成长为“高级程序员”，而人类程序员逐渐消失吗？我并不这么认为。我们似乎正在逼近某种上限：AI 能做很多事，但仍然需要人类以某种监督者的角色参与其中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以预见的是，工程师的金字塔正在变窄，入门层级的人变少了，而我们需要认真思考，如何在这样的环境下培养下一代资深工程师。&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果把时间拉长到未来五到十年，在一个更加 AI 原生的世界里，你认为 TypeScript 作为一门编程语言会如何演进？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：我认为它的演进方向其实相当清晰。JavaScript 已经不再是一门年轻的语言，它有一套成熟的标准化流程，而我们也深度参与其中。TypeScript 会沿着 JavaScript 的标准化路径继续发展，同时在其之上补充必要的类型系统能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;真正充满不确定性的，是工具层面的变化。谁能想到，随着“对话式编程”这类形态的出现，命令行工具又重新变得如此重要？过去，AI 更多是作为助手存在：开发者在 IDE 里，AI 帮你更快地输入和补全代码。但现在，这种关系正在反转。AI 开始承担主要工作，而人类转向监督和审阅。此时，AI 并不一定需要我们传统意义上的 IDE，尽管它仍然需要语言服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也是为什么 MCP 这类技术开始变得有吸引力：通过把语言服务接入 MCP，让 AI 能够提出语义级的问题、重构请求等，并在一定的确定性边界内完成工作流。这本质上是在用 LLM 或 Agent 的方式，完成过去只能在 IDE 中完成的事情，这将深刻改变开发工具的形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：围绕 TypeScript 或你们的工作，有没有哪些你希望公开澄清的误解？或者哪些你觉得被社区忽视、但其实很重要的事情？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：在开源领域，我们与社区的距离非常近。如果哪里不对劲、存在摩擦，几乎会第一时间被反馈出来。比如这次转向原生代码的决定，确实引发了不少争议，有人认为我们应该选择另一种编程语言。但我始终坚信，我们为这个目标选对了工具。过去一年里，这个决定的成果已经逐步显现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;不过，开源本身就是一种微妙的平衡。一方面，你在无偿地把成果交给世界；另一方面，这个项目往往由一家商业公司资助，而公司必须以某种方式生存下去。总得有人支付账单。因此，我们团队始终处在一种张力之中：如何让开源项目既符合社区期待，又与公司使命保持一致。这并不是 TypeScript 独有的问题，而是当今几乎所有开源项目都面临的现实。至于是否存在一种更好的激励机制来回馈长期投入的人，目前还没有答案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：开源的可持续性，确实是一个反复被提起的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：大量企业的正常运转，事实上依赖于那些支撑其后端的开源项目得到持续维护。但现实中，很多人对开源的态度仍然是“索取多于付出”。在微软，我们至少在努力以一种更真诚的方式参与其中。仅 TypeScript 项目，就已经累计投入了数百人数年的工作量；而 Visual Studio Code，投入甚至可能达到上千人。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：如果不算你自己参与创造的语言，你最敬佩、最尊重哪一门编程语言？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：任何一门能被程序员广泛使用、甚至能进入讨论范围的编程语言，都一定有其可取之处，因此都值得尊重，我深知一门语言要走到这一步有多难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以 Rust 为例，我非常敬佩它通过借用检查器来探索一种不同的内存管理方式，试图在不依赖昂贵自动垃圾回收的情况下保证安全性。我也很尊重 Go，尽管它在设计上有些“怪”，常被认为不太正统，但它实际上提供了一种简单、内存安全、类型安全的“现代 C”的思路。至于 Python，它的成功不需要多说，它驱动着 AI 和机器学习的发展，令人难以不心生敬意。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从语言设计者的角度看，我们都站在前人的肩膀之上。如果设计一门语言却不向其他语言学习，那是愚蠢的。经验与智慧就在那里，理应被尊重和继承。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Eirini：当你放眼整个以 GitHub 等协作平台为中心的软件开发生态时，是什么让你对未来保持乐观？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Anders：仅仅是它依然存在这一事实，就足以让我感到乐观。开源本身是一场巨大的实验，尽管至今没人真正解决“如何为开源持续提供资金”这个问题，但它不仅没有衰退，反而比以往任何时候都更庞大、更活跃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当然，其中也有大量噪音，有不少项目缺乏维护，但不可否认的是，这些代码记录了软件演化的全过程。以我们为例，转向这种协作工作流后，十二年的历史都被完整地保存在那里，可搜索、可追溯。如果我记得某个问题曾被讨论过，只需要去查找，而不必面对一封再也找不到的旧邮件，这种价值是巨大的。正因如此，我由衷地高兴看到它仍在持续增长，并顽强地存活下来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=uMqx8NNT4xY&lt;/p&gt;&lt;p&gt;https://devclass.com/2026/01/28/typescript-inventor-anders-hejlsberg-ai-is-a-big-regurgitator-of-stuff-someone-has-done/&lt;/p&gt;</description><link>https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</link><guid isPermaLink="false">https://www.infoq.cn/article/7KwNvRQgcWYJi7aPlGLo</guid><pubDate>Fri, 06 Feb 2026 08:23:20 GMT</pubDate><author>傅宇琪,Tina</author><category>生成式 AI</category></item><item><title>奥特曼重磅发声：全AI公司是未来！OpenAI官宣Frontier，让管理Agent像管人一样简单</title><description>&lt;p&gt;在OpenAI与Anthropic对轰AI Coding新产品，争夺编程王座之际，Open AI偷偷放大招，又推出智能体中枢平台 Frontier。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;简单来说，Frontier 就是一个把智能体当成 AI 员工来管理的企业级平台。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c71db6c1f404abbbf9919be21668a6a9.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;过去几年，智能体开始从“陪聊工具”走向企业一线业务，但一个关键问题成为不少企业的烦恼，即智能体越多，系统反而越复杂。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在不少企业内部，云平台、数据系统和应用长期割裂，智能体被零散地塞进各个业务场景。每一个智能体都像一座信息孤岛，权限受限、上下文缺失。伴随智能体数量的暴增，带来的往往不是效率提升，而是运维、治理和协同成本的持续叠加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正是在这一背景下，Frontier应运而生。它将企业内部分散的系统与数据整合在一起，通过构建统一的业务上下文，提供一套端到端的方法，覆盖智能体的构建、部署与管理流程，让智能体能够真正进入生产环境稳定运行。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在2月4日思科AI峰会上，Open AI CEO奥特曼曾激进发言，不能快速用上 AI 员工的公司，会被甩在后面。他甚至提出“全 AI 公司”的概念，未来或许每个流程、每个环节，AI都能真正参与进来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，正是OpenAI 对企业级市场的提前卡位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据最新数据显示，Anthropic在企业级大模型市场占据了40%的惊人份额，稳坐第一把交椅，远超OpenAI的27%和谷歌的21%。随着大模型逐步进入真实业务流程，企业级场景正成为决定长期竞争格局的关键阵地，OpenAI 显然不希望在这一入口层面处于被动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenAI 的目标已不再局限于打造“更聪明的模型”，而是试图通过基础设施，让各种智能体优先部署在自家平台之上，包括竞争对手的产品，从而将更多企业用户纳入其整体 AI 生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据 OpenAI 官方披露，过去几年，已有超过 100 万家企业在使用 AI 提升效率。比如一家大型制造企业借助智能体，将原本需要六周完成的工作压缩到一天；另一家全球投资公司通过智能体优化销售流程，为销售人员释放出 90% 以上 的时间；还有一家大型能源生产商利用智能体提升 5% 的产量，额外创造了 超过10亿美元的收入。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可以说，能否在组织内部高效使用智能体，正在成为企业之间拉开差距的关键变量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，从制造业到互联网、从金融到生命科学，已有多家行业巨头率先试用 Frontier，包括 惠普、Intuit、甲骨文、州立农业保险、赛默飞世尔和优步。此外，BBVA、Cisco、T-Mobile 等数十家现有客户也已参与试点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该平台仍处于有限开放阶段，仅向少量客户开放体验，预计将在未来几个月逐步扩大范围，具体定价方案尚未披露。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;Frontier平台的四大板块：上下文、执行环境、评估学习与安全管理&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了更好地理解 Frontier，可以把它类比为一家公司的 “AI员工管理体系”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的作用不仅是要让AI员工了解公司是如何运作的，还要为其提供跨部门协作的能力、必要的资源支持，以及清晰的权限边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;围绕这一目标，Frontier 将企业级 AI 智能体的运行拆解为四个关键模块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（1）共享业务上下文：让 AI “知道公司怎么运作”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让 AI 真正参与企业工作，第一步不是分配任务，而是让它理解企业本身的运作逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier 做的第一件事，就是构建一个共享的业务上下文环境。通过打通企业内部长期割裂的系统，包括 CRM、数据仓库、工单系统以及各类内部应用，将原本分散在不同系统中的业务信息连接起来，形成一个统一的“语义层”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样，智能体就能理解信息如何在企业内部流动、关键决策发生在什么环节、哪些指标才是真正重要的结果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（2）提供执行环境：让 AI 不只会想，还能真的“干活”&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在理解业务之上，Frontier 为 AI 员工提供了一个开放且可靠的执行环境。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它可以打开和使用企业内部的各种工具，自己写代码处理数据，整理和生成文件，并在不同系统之间来回切换，把一整套原本需要人反复操作的流程跑完。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对企业来说，这相当于把 AI 从“问答工具”，升级成了能独立完成任务的AI同事。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（3）学习与评估：让 AI 在“反思”中不断优化&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了让AI像人一样能不断优化，自我迭代，Frontier 内置了绩效评估和优化机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这套机制能够持续监控 AI 代理在实际任务中的表现，包括任务完成情况、错误率、资源消耗等关键指标，而人变成了监督者，可以清楚地看到哪些行为有效、哪些无效，并据此调整规则和流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/08/08b3e13c61a876c0825f2bced707b72e.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着运行时间的增长，AI 智能体会逐步积累“记忆”，将过往交互转化为有用的上下文信息，从而不断优化自身表现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;（4）安全保障：让 AI 在清晰边界内工作&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了防止AI乱操作，Frontier 为每一个AI员工设立严格工作边界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;智能体能进哪些系统、能做哪些操作、权限到哪一步，都提前规定好。这样一来，AI 就只能在允许的范围内工作，不会乱动数据、越权操作，也不会给公司带来额外风险。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/64/64d17bb0477c5cd67960a86f6681af37.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这样一整套系统化设计下，Frontier 补齐了 AI 进入公司所需的基础设施。既给予足够的灵活性，又保留必要的安全和控制，使智能体能够真正融入企业的日常工作流程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在Frontier平台上，公司可以创建多个 AI 员工，也可以混合其他厂商的智能体或自行开发的 AI 服务。Frontier 的核心作用是公司可以通过统一的仪表盘，查看每个 AI 员工的任务完成情况、资源消耗和错误率等关键指标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这意味着企业部署 AI 的方式，正在从过去的“定制化开发”，转向“标准化配置”，让部署智能体更便捷易操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier已经在不少关键行业发挥价值。比如银行用它做AI后台，处理每年数亿的需求事件；制造业公司，靠它模拟生产流程、规划产能布局，节省了数十亿美元成本；在生命科学领域，这套系统用来优化全球监管流程，给药品审批这类关键环节兜底。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;正如 OpenAI 应用业务首席执行官 Fidji Simo 所言：“到今年年底，领先企业中的大多数数字化工作，都将由人类进行决策和指挥，并由成群的 AI 代理来执行。这种模式已经在编程领域成立，并且很快会扩展到更多业务场景。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的上限，或许是全AI公司&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier的推出，其实早就埋在奥特曼对人工智能未来走向的一系列判断之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在今年的思科 AI 峰会上，奥特曼认为Codex的诞生，是又一个 “ChatGPT 时刻”。那是他第一次真切地意识到，AI 可以被当作一名同事，而不只是工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e9/e91d0e6bd4f4a581177927b1e1c6c8a4.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他讲了一个细节，刚安装 Codex 时，他绝不会在不检查的情况下，让它完全控制自己的电脑。但这个坚持只维持了两个小时，因为Codex 实在太好用了，而且这种“好用”已经不再局限于写代码本身，而是扩展到了整个工作的执行过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在奥特曼看来，让智能体能 “像人一样用电脑”，真正接管电脑和浏览器，才能把生产力彻底解放出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;代码能力加上通用电脑操作能力，这种结合的趋势几乎挡不住。他甚至想得更远：AI 的终极形态，说不定是 “全 AI 公司”，让智能体直接对接现实系统，从头到尾把一家企业跑起来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然目前AI可以做的事已经很多，但真正被组织吸收和使用的比例依然很低。技术的演进速度，远远快于企业部署和消化的能力。这背后的原因是企业部署 AI 成本高，缺乏系统化能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 最强大的地方之一，是“始终在线”的计算能力。但现有的硬件、权限系统、法律体系，都不是为这种情况设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;奥特曼曾将企业真正需要的形态概括为一种“AI 云平台”：它负责处理安全问题，管理业务上下文，协调和运行大量智能体，支持多模型协作，并提供完整的企业级授权与接口体系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;企业级应用已经成为了OpenAI在2026 年明确的重点方向之一，而 Frontier，正是OpenAI 交出来的企业级解决方案。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种判断并非 OpenAI 一家的独断。去年 12 月，全球研究与咨询公司 Gartner 在一份报告中指出，代理管理平台既可能成为“人工智能领域最有价值的资产”，也是企业大规模采用 AI 所必需的基础设施。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Frontier，或许正在拉开 “AI 全面扎根企业核心业务” 时代的序幕。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;https://openai.com/index/introducing-openai-frontier/&lt;/p&gt;&lt;p&gt;https://openai.com/business/frontier/&lt;/p&gt;&lt;p&gt;https://www.youtube.com/watch?v=YO2PVbtpb_A&lt;/p&gt;</description><link>https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</link><guid isPermaLink="false">https://www.infoq.cn/article/AS37NK1LUvhd2GbJfYhs</guid><pubDate>Fri, 06 Feb 2026 07:45:00 GMT</pubDate><author>高允毅</author><category>OpenAI</category><category>生成式 AI</category></item><item><title>Cloudera发布2026 AI与数据技术趋势预测：标准化、可控化趋势成企业主流选择</title><description>&lt;p&gt;过去两年，AI在中国经历了从概念热潮到密集试点的阶段。无论是大模型、智能体（Agentic AI），还是自动化应用，越来越多企业已完成初步探索。进入2026年，AI正迈入一个新的发展阶段——从试点应用走向业务规模化。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;企业关注的核心问题也随之发生变化，不再只是“能否用AI”，而是AI是否能够在可控、可持续的前提下，稳定运行并转化为可衡量的业务成果。基于对中国企业AI实践的持续观察，Cloudera对2026年AI与数据技术的发展趋势做出如下判断：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测一：AI走向产业化，业务价值与可复制能力成为核心衡量标准&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，中国企业的AI应用将明显超越聊天机器人和单点工具，转向流程优化、运营自动化和行业级智能应用。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在制造、金融、电信等领域，企业将更倾向于复用已验证的AI能力，并通过智能体工作流将AI深度嵌入核心业务流程，而不再局限于单一模型或实验项目。ROI、业务效率提升和可持续运营能力，将取代模型参数与算力规模，成为衡量AI成功与否的关键指标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;同时，随着AI被视为重要的“新型生产力”，企业和行业客户将更加重视AI系统的稳定性、连续性与可运营性。能够在复杂环境中长期运行、不断优化并适应业务变化的AI平台，将在竞争中脱颖而出。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放在一次公开分享中表示，AI技术的第一阶段是能力展示与智能回答等“噱头应用”，例如模型回答数学题能力等功能。然而，进入产业化落地后，企业对AI的关注点更多转向如何结合已有业务系统、优化流程并创造可衡量的商业价值。这与当前行业趋势高度一致。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8a/8a6452db58cb45fd3ef6c01ebe3574a8.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera大中华区技术总监刘隶放&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;行业数据显示，企业从单点AI尝鲜逐步转向系统化、流程化应用，特别是在流程优化、与数据平台整合等关键领域的能力要求急剧上升。此外，随着智能体（AI agents）出现，企业内部正在探索如何将模型能力系统性融入现有的业务逻辑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测二：可信、可治理的私有AI将成为企业的关键差异化能力&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在中国市场，数据安全与合规可控始终是AI应用的前提条件。2026年这一趋势将进一步强化。&lt;/p&gt;&lt;p&gt;虽然公有云与预训练模型极大降低了AI试验门槛，但在实际生产环境中，企业逐渐意识到：如果数据治理、访问控制和合规机制不到位，AI带来的效率提升，可能同时放大数据风险。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;因此，越来越多中国企业将转向私有AI（Private AI） 路径：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在受治理的环境中部署和运行模型；数据不出域，权限可控、流程可追溯；通过检索增强生成（RAG）等方式，为模型提供业务上下文，同时保持数据可控；&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放进一步指出，数据合规永远优先于AI功能本身。在涉及企业核心数据的训练过程中，如果使用公有云平台进行训练，不仅有可能触及竞争性泄露风险，还可能违反监管要求。因此，只要涉及企业敏感数据，私有化部署基本成为不可替代的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;可信AI不再是“最佳实践”，而将成为企业实现AI规模化落地的基本门槛。治理能力与敏捷性不再是对立选项，而是AI成熟度的两个必要组成部分。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;预测三：本地化私有部署成为中国企业AI规模化落地的基础架构&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在中国市场，2026年企业对AI与数据架构的判断将进一步趋于清晰：本地化私有部署是AI规模化落地的基础前提。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放强调，相较于公有云部署，私有化AI环境更能满足企业对可控性、数据安全和长期运营的核心诉求。在安全与合规成为企业AI战略基础的背景下，“可控”被视为AI落地的前提条件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;行业调研报告显示，企业在AI部署中越来越倾向于选择私有化或混合云架构，以保障数据主权和业务独立性。IDC发布的《2025年中国企业AI大模型应用趋势报告》指出，约72%中大型企业在实施AI智能体时，将私有化部署置于优先考虑因素之一。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;根据Rackspace发布的趋势分析，面向企业的私有云AI部署正在成为主流，其中检索增强生成（RAG）等敏感工作负载正从公有环境向私有部署迁移，以提升性能稳定性和数据控制能力。&lt;/p&gt;&lt;p&gt;相关行业观点也总结出几个核心趋势：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;私有化部署可提升响应速度并避免核心数据泄露风险；企业希望避免将敏感数据发送至外部AI平台，以控制数据流出风险；企业CIO和CTO在架构设计过程中，将合规与数据控制置于AI战略核心。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在金融、制造、能源、电信等关键行业，核心业务系统与数据资产长期运行在本地或私有环境中。这一架构形态，既源于对数据安全与合规可控的要求，也来自企业对系统稳定性、连续性与长期运营能力的现实考量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随着AI从试点走向生产级应用，企业开始更加关注一个根本问题：AI是否能够在本地私有环境中持续运行、不断优化，并稳定支撑核心业务。一次性部署或短期验证已无法满足需求，取而代之的是对平台级能力的要求，包括统一的数据管理、可治理的模型运行，以及对业务变化的长期适配能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到2026年，能够在本地私有架构下支撑AI持续演进的数据与AI平台，将成为中国企业实现AI规模化、可复制落地的重要基础。这一能力，也将成为衡量企业AI成熟度的关键标志。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cloudera 成立于 2008 年，总部位于美国硅谷，是最早一批围绕 Hadoop 生态 成立的企业级大数据公司之一。公司创始团队中包括多位 Hadoop 核心贡献者，因此 Cloudera 在早期被广泛视为“企业级 Hadoop 的事实标准”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年，Cloudera 与另一家老牌大数据公司 Hortonworks 合并，形成当时全球最大的大数据平台厂商之一。合并后，Cloudera 的技术版图从单一的大数据存储与计算，扩展到 数据管理、数据治理、数据分析、机器学习与 AI 工程化 等完整链条。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2026：AI从“概念热潮”走向“硬核成果”的一年&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2026年，中国AI的竞争焦点将不再是“谁的模型更大”，而是在可控、可信、可复制的基础上，真正把AI变成业务成果。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;最终胜出的企业，将是那些能够负责任地规模化AI、用数据治理支撑智能决策、用韧性架构保障长期运营的企业。因为真正可信的AI，始于可信的数据；而可信的数据，离不开稳健、可持续的数据基础架构。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;刘隶放称，在AI实践中，企业真正关心的并非单一模型表现，而是整体平台建设后的长期运营能力。例如，在金融、制造等行业，已有大量的信息系统和数据资产，AI必须与这些系统无缝整合，才能真正提升业务效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，在人才流动频繁的市场环境下，构建松耦合体系架构被认为是确保AI平台可持续运营的关键。这种设计允许平台适应技术更新和人员变动，避免因关键人员离职而造成系统停滞。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;公司援引自身服务的典型案例（如上汽大众的供产销数据平台与AI集成实践），强调企业在部署AI时，最终评估的核心是投入产出与长期收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</link><guid isPermaLink="false">https://www.infoq.cn/article/RGiNVO8YPDgTfZtOBHxN</guid><pubDate>Fri, 06 Feb 2026 06:50:14 GMT</pubDate><author>李冬梅</author><category>生成式 AI</category></item><item><title>Cloudflare自动化Salt配置管理调试，减少发布延迟</title><description>&lt;p&gt;Cloudflare最近&lt;a href=&quot;https://blog.cloudflare.com/finding-the-grain-of-sand-in-a-heap-of-salt/&quot;&gt;分享&lt;/a&gt;&quot;了他们是如何使用&lt;a href=&quot;https://saltproject.io/&quot;&gt;SaltStack&lt;/a&gt;&quot;（Salt）管理庞大的全球服务器集群的。在这篇博客文章中，他们讨论了解决“一粒沙（grain of sand）”问题所需的工程任务。它的关注点在于要从数百万次状态应用中找出某个配置错误。Cloudflare的&lt;a href=&quot;https://sre.google/&quot;&gt;站点可靠性工程（SRE）&lt;/a&gt;&quot;团队重新设计了其配置的可观测性，他们将故障与部署事件关联起来。这项工作将发布延迟减少了5%以上，并减少了手动分析问题相关的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为配置管理（configuration management，CM）的工具，Salt能够确保了跨数百个数据中心的数千台服务器保持在期望的状态。在Cloudflare的规模下，即使YAML文件中的一个微小语法错误或“Highstate”运行期间的瞬时网络故障，都可能阻碍软件发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare面临的主要问题是预期配置与实际系统状态之间的“偏离（drift）”。当Salt运行失败时，它影响的不仅仅是一台服务器，它可能会阻止在整个边缘网络中推出关键的安全补丁或性能特性。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Salt使用了带有&lt;a href=&quot;https://zeromq.org/&quot;&gt;ZeroMQ&lt;/a&gt;&quot;的&lt;a href=&quot;https://docs.saltproject.io/salt/install-guide/en/latest/topics/configure-master-minion.html&quot;&gt;主控/受控（master/minion）设置&lt;/a&gt;&quot;。这使得很难找出为什么特定的受控端（代理）没有向主控端报告状态，这简直就像大海捞针。Cloudflare总结了几个破坏此反馈循环的常见故障模式：&lt;/p&gt;&lt;p&gt;无声故障：受控端在状态应用期间可能会崩溃或挂起，导致主控端无限期地等待响应。资源耗尽：繁重的pillar数据（元数据）查找或复杂的Jinja2模板可能会使主控端的CPU或内存不堪重负，导致job丢失。依赖地狱：包状态可能会因为上游仓库无法访问而失败，但错误消息可能埋藏在数千行日志的深处。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/74/74fe36ee46bcb4a7a61043c565e64040.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;Salt的架构图&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;当发生错误时，SRE工程师必须手动通过SSH登录到候选受控端。他们会追踪主控端上的job ID，并筛选保留时间内有限的日志，然后尝试将错误与变更或环境条件联系起来。在拥有数千台机器和频繁提交代码的情况下，这个过程变得单调且难以维护。它提供的持久工程价值非常有限。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;为了解决这些挑战，Cloudflare的商业智能和SRE团队合作构建了一个新的内部框架。目标是为工程师提供一种“自助服务”机制，以识别跨服务器、数据中心和特定机器组的Salt故障的根本原因。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;解决方案涉及从集中式日志收集转向更健壮的、事件驱动的数据摄入管道。这个在相关内部项目中被称为“Jetflow”的系统，允许将Salt事件与以下内容关联：&lt;/p&gt;&lt;p&gt;Git提交：识别配置仓库中触发故障的精确变更。外部服务故障：确定Salt失败是否实际上是由依赖项（如DNS故障或第三方API中断）引起的。临时（Ad-Hoc）发布：区分计划的全局更新和开发人员进行的手动更改。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare通过改变管理基础设施故障的方式，为自动分类奠定了基础。系统现在可以自动标记特定的“一粒沙”，即导致发布阻塞的那一行代码或那一台服务器。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;从被动管理到主动管理的转变带来了以下成果：&lt;/p&gt;&lt;p&gt;发布延迟减少5%:：通过更快地暴露错误，缩短了从“代码完成”到“在边缘运行”的时间。减少琐事：SRE不再需要花费数小时进行“重复性分类”，使他们能够专注于更高层次的架构改进。改进的可审计性：现在每个配置变更都可以从Git PR到边缘服务器上的最终执行结果进行全生命周期追踪。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare工程团队观察到，尽管Salt是一个强大的工具，但在“互联网规模”下管理它需要更智能的可观测性。通过将配置管理视为一个需要关联和自动分析的关键数据问题，他们为其他大型基础设施提供商树立了榜样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;基于Cloudflare在SaltStack上遇到的挑战，需要注意的是，像&lt;a href=&quot;https://docs.ansible.com/&quot;&gt;Ansible&lt;/a&gt;&quot;、&lt;a href=&quot;https://www.puppet.com/&quot;&gt;Puppet&lt;/a&gt;&quot;和&lt;a href=&quot;https://www.chef.io/&quot;&gt;Chef&lt;/a&gt;&quot;这样的替代配置管理工具，每个工具都有不同的架构权衡。Ansible使用SSH无代理的方式工作。这比Salt的主控/受控设置更简单。然而，由于顺序执行，它在大规模环境时可能会面临性能问题。Puppet使用基于拉取的模型，代理会与主控服务器进行核对。这提供了更加可预测的资源使用，但与Salt的推送模型相比，可能会减慢紧急变更的速度。Chef也使用代理，但侧重于使用其Ruby DSL的代码驱动方法。这为复杂任务提供了更大的灵活性，但学习曲线更陡峭。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Cloudflare的规模下，任何工具都会遇到其自身的“一粒沙”问题。然而，关键教训很明确，那就是管理数千台服务器的任何系统都需要强大的可观测性。它还必须能够将故障与代码变更自动关联，并具备智能分类机制。这将手动侦探工作转化为可操作的洞察力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/01/cloudflare-salt-configuration/&quot;&gt;Cloudflare Automates Salt Configuration Management Debugging, Reducing Release Delays&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</link><guid isPermaLink="false">https://www.infoq.cn/article/NQxcDYglyKW4rwv876wE</guid><pubDate>Fri, 06 Feb 2026 06:02:48 GMT</pubDate><author>作者：Claudio Masolo</author><category>云计算</category></item><item><title>Astro发布了版本6 Beta版，重新设计了开发服务器和一流的Cloudflare Workers</title><description>&lt;p&gt;&lt;a href=&quot;https://astro.build/&quot;&gt;Astro&lt;/a&gt;&quot;，一个用于构建内容驱动型网站的Web框架，已经宣布了&lt;a href=&quot;https://astro.build/blog/astro-6-beta/&quot;&gt;Astro 6 Beta&lt;/a&gt;&quot;版本，引入了一个完全重新设计的开发服务器、一流的Cloudflare Workers支持，以及几个新的稳定API，包括实时内容集合和内容安全策略支持。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6 Beta版对开发人员使用该框架的方式带来了重大改变，包括基于Vite的&lt;a href=&quot;https://vite.dev/guide/api-environment&quot;&gt;Environment API&lt;/a&gt;&quot;重构的开发服务器、用于实时数据更新的稳定实时内容集合，以及内置的CSP支持。该版本还包括一些重大的破坏性变更，如需要使用Node 22+并移除几个弃用的API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6中的一大特性是完全重新设计的 astro dev 开发服务器。新服务器利用Vite的Environment API在与生产环境相同的运行时中运行应用程序，缩小了开发和部署环境之间的差距。以前，在本地工作的代码一旦部署可能会有不同的行为，而且平台特定的特性通常在部署后才能测试。通过统一开发和生产代码路径，Astro团队已经发现并修复了许多仅存在于开发或仅存在于生产中的微妙错误。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;新的开发服务器使之成为可能的最完整的例子是对Cloudflare Workers的支持。有了Astro 6 Beta， astro dev 现在可以使用workerd运行应用程序，这是Cloudflare的开源JavaScript运行时，这与在生产环境中支持Cloudflare Workers的运行时相同。这意味着开发者现在可以直接针对真实的平台API进行开发，而不是模拟或polyfills。当使用Cloudflare支持运行 astro dev 时，开发者现在可以访问Durable Objects、KV Namespaces、R2 Storage、Workers Analytics Engine和环境变量，所有这些都支持热模块替换。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在可以直接使用 cloudflare:workers 模块访问Cloudflare绑定，如beta博客文章所示：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import { env } from &quot;cloudflare:workers&quot;; const kv = env.MY_KV_NAMESPACE; await kv.put(&quot;visits&quot;, &quot;1&quot;); const visits = await kv.get(&quot;visits&quot;);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在Astro 5.10中还在试验性的实时内容集合，现在在Astro 6中已经稳定。这些建立在Astro的类型安全内容集合之上，可以实时更新数据，而不需要重新构建，这使得它们非常适合频繁更新数据源，如实时股票价格或库存。该API旨在让已经使用Astro的构建时内容集合的人感到熟悉，但对实时数据请求的实际情况进行了显式的异常处理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;内容安全策略支持，之前在Astro 5.9中是实验性的，现在已经稳定。CSP是Astro获得最多投票的特性请求，它有助于保护网站免受跨站脚本和其他代码注入攻击。该功能在所有Astro渲染模式中工作，并与所有官方适配器兼容，自动生成CSP头或元元素，包括脚本和样式的哈希。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro 6包括几个重大破坏性变更，因为团队清理了弃用的API。最重要的变化包括移除 Astro.glob() ，要求Node 22或更高版本，以及更新Cloudflare适配器，移除 Astro.locals.runtime ，转而直接访问平台API。团队已经发布了&lt;a href=&quot;https://v6.docs.astro.build/en/guides/upgrade-to/v6/&quot;&gt;一个全面的升级指南&lt;/a&gt;&quot;，详细说明了每个破坏性变更的迁移步骤。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该版本在社区内引发了一些讨论，reddit上的一位用户对长长的破坏性变更列表发表了评论（特别提到了早期的alpha版本）：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;哇。真是一个巨大的破坏性变更列表……&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这引起了Astro核心维护者&lt;a href=&quot;https://github.com/sarah11918/sarah11918&quot;&gt;Sarah Rainsberger&lt;/a&gt;&quot;的回应：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;大多数变更至少不会影响每个人！&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;她继续解释了有这样一个详细的破坏性变更列表的理由：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;……我坚信，任何可能破坏某人项目的东西都应该包含在这一页上……无论那个“项目”是一个常规的静态网站，还是你构建的主题，或者一个复杂的集成。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在&lt;a href=&quot;https://news.ycombinator.com/item?id=46646645&quot;&gt;Hacker News&lt;/a&gt;&quot;上，评论者强调Astro是最早支持Cloudflare的Vite插件的框架之一：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Cloudflare发布了他们的vite插件，使得使用vite env API的框架可以毫不费力地在workerd中运行……Nextjs还没有支持，添加对Sveltekit支持的草案PR已经被搁置，直到下一个主要版本，Astro刚刚在他们3天前的beta 6.0版本中添加了支持。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与其他元框架如&lt;a href=&quot;https://nextjs.org/&quot;&gt;Next.js&lt;/a&gt;&quot;和SvelteKit相比，Astro以其专注于内容驱动型网站和默认最小化客户端JavaScript而脱颖而出。Next.js强调React和全栈能力，&lt;a href=&quot;https://svelte.dev/docs/kit/introduction&quot;&gt;SvelteKit&lt;/a&gt;&quot;专注于Svelte生态系统，而Astro仍然与框架无关，通过其孤岛架构官方支持&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&quot;、&lt;a href=&quot;https://vuejs.org/&quot;&gt;Vue&lt;/a&gt;&quot;、&lt;a href=&quot;https://svelte.dev/&quot;&gt;Svelte&lt;/a&gt;&quot;和其他UI框架。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Astro是一个开源Web框架，旨在构建包括博客、营销网站和电子商务在内的内容驱动型网站。该框架通过最小化客户端JavaScript，尽可能在构建时或按需在服务器上渲染内容，强调性能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&quot;&gt;https://www.infoq.com/news/2026/02/astro-v6-beta-cloudflare/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</link><guid isPermaLink="false">https://www.infoq.cn/article/ZzvIZlrHj7PqMKQSuaqT</guid><pubDate>Fri, 06 Feb 2026 06:00:00 GMT</pubDate><author>作者：Daniel Curtis</author><category>大前端</category></item><item><title>别再手动拼凑 Data Pipeline 了！这个新平台想让你彻底告别 Iceberg 运维噩梦</title><description>&lt;p&gt;近日，Etleap 正式发布了 &lt;a href=&quot;https://etleap.com/&quot;&gt;Iceberg pipeline platform&lt;/a&gt;&quot;。作为一套全新的托管式数据流水线方案，该平台的核心价值在于：让企业摆脱繁琐的自定义技术栈开发与维护，实现 Apache Iceberg 架构的“无感切换”。它将数据摄取、转换、编排及表操作深度集成，且全量部署在客户自有的 VPC 环境内。对数据团队而言，这相当于获得了一个“开箱即用”的生产级底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这一举措直击数据平台负责人们日益增长的痛点：尽管 Iceberg 已成为现代数据湖和湖仓一体化架构中极受欢迎的表格式，但它本身并不提供日常运行所需的流水线。因此，企业往往不得不将各种摄取工具、dbt 任务、调度器以及定制的维护脚本拼凑在一起。Etleap 表示，这种碎片化的方案不仅构建成本高昂，且难以在大规模环境下稳定运行，更分散了团队提炼业务价值的精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“Iceberg 能为企业带来巨大的收益，但在实践中，这需要一套围绕它构建的托管流水线系统来变现，”Etleap 的首席执行官兼创始人 Christian Romming 表示，“我们的 Iceberg Pipeline 平台正是为了满足这一需求而生，让数据平台团队无需构建和运行自定义流水线堆栈，即可拥抱 Iceberg。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 的平台用一套原生的 Iceberg 系统取代了以往“缝补拼接”的模式。它将数据摄取、建模、编排及表生命周期管理整合进一个协同层，同时保持在客户自有云环境内的完全隔离。通过这种方式，它在满足企业级治理和安全要求的同时，消除了对独立控制平面或外部基础设施的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了简化运维流程，该平台还致力于将 Iceberg 接入更广泛的数据生态系统。团队只需构建一次流水线，即可在分析、数据科学、AI 工作负载及数据共享场景中重复调用相同的 Iceberg 表。这不仅减少了数据冗余，提高了数据一致性，还实现了跨云平台和计算引擎的工作负载可移植性，且无需牺牲性能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Etleap 表示，Iceberg Pipeline 平台目前已正式上线，并已有部分客户在进行大规模的流水线运行。该公司将此次发布定位为企业将 Iceberg 打造为真正数据基座的捷径，旨在消除传统上阻碍技术落地的运维负担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前 Iceberg 版本的发布尚处于早期阶段，该平台能否兑现 Etleap 所承诺的愿景仍有待观察。除了各大媒体的发布新闻外，目前尚未收到来自用户的实际使用反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/news/2026/02/etleap-iceberg-pipeline-platform/&lt;/p&gt;</description><link>https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</link><guid isPermaLink="false">https://www.infoq.cn/article/io7H7PDc3ka8dTKOEfQk</guid><pubDate>Fri, 06 Feb 2026 05:02:22 GMT</pubDate><author>作者：Craig Risi</author><category>数据湖仓</category></item><item><title>Vibe Coding“血洗”开源，社区吵翻了：封杀菜鸡AI开发者？不如给维护者打钱！</title><description>&lt;p&gt;整理 | 华卫&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;氛围编码（Vibe coding）是否会摧毁开源生态系统？近日，多位知名研究人员在一篇预印本论文中指出，从观测到的趋势及部分建模结果来看，情况可能确实如此。他们的警告主要集中在两方面：用户互动逐渐从开源项目中剥离，同时启动一个新开源项目的难度大幅提升。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是热门开源项目，随着代码下载和文档查阅的需求被大语言模型聊天机器人的交互所替代，其官网的访问量也出现下滑，项目商业规划推广、赞助募资和社区论坛运营的可能性也降低了。Stack Overflow等社区论坛使用量的骤减也反映了这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/55/55ac5d403610db635de151d6b2e253c5.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;研究人员们最后的结论是：在氛围编码广泛应用的情况下，要维持开源软件目前的规模，就需要对维护者的报酬方式进行重大改革。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;“AI革命”or人类智能的压力测试&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果把“AI辅助”软件开发的这种效果理解为将实际的工程和开发工作委托给大语言模型的统计模型，那么问题就显而易见了。氛围编码这一模式摒弃了开源社区中对类库和工具的自然筛选机制，几乎可以确定的是，大语言模型的统计模型在生成输出内容时，必然只会选用其训练数据集中占比最高的技术依赖方案。并且，大语言模型既不会与库或工具的开发者互动，也不会提交可用的错误报告，更不会意识到任何潜在问题，无论这些问题的文档记录多么完善。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;自从微软在 2021 年推出 GitHub Copilot以来，这便是一个极具争议的话题。2024 年有一些研究报告指出，使用 Copilot 和类似的聊天机器人进行氛围编码并没有带来任何实际好处，除非增加 41% 的 bug 也被视为成功的标准。到 2025 年，负面情绪愈发浓烈，大语言模型聊天机器人普遍被指责会降低使用者的认知能力，氛围编码会降低 19% 的开发效率，就连尝试过这类工具的资深开发者，也在言辞犀利的评测中对其全盘否定。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;即便是当下，软件开发领域也已显现出“AI垃圾”带来的诸多负面影响。cURL 项目的作者 Daniel Stenberg 多次抱怨，由于大语言模型引发的“AI 垃圾”，导致提交的错误报告质量日益下降。如今，该项目已决定从 2026 年 2 月 1 日起暂停其漏洞赏金计划。也有网友指出，“AI最不靠谱的地方在于那些简单的重复性任务，因为它经常会随机出错。对它的要求越多，它就越容易出错，导致你需要逐行检查整个程序，确保它执行了要求的操作。使用大语言模型时最糟糕的做法是让它‘把这段代码清理干净，但不要改变任何功能或逻辑’，它绝对会起到相反的效果。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所有这些现象似乎都在强化这样一种观点：“AI革命”或许更像是对人类智能的一次压力测试，而非真正提升开发效率或代码质量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;目前尚不清楚氛围编码的影响究竟有多大，但像JavaScript、Python和各类Web技术相关的软件生态系统很可能首当其冲地受到其冲击，因为它们的用户群体似乎对这种开发模式的接受度更高，且相关技术在大语言模型的训练数据集中占比也最大。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开源维护者们福利大降，要没钱赚了？&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而且，在氛围编码的相关补偿机制下，绝大多数开源项目都难以从中获益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文指出，氛围编码降低了软件制作成本，但也改变了用户与软件生态系统的交互方式。在传统的开源软件商业模式下，开发者会选择软件包、阅读文档，并与维护者及其他用户交流。而在氛围编码模式下，AI智能体可以端到端地选择、组合和修改软件包，人类开发者可能并不清楚使用了哪些上游组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这种转变将引发一个关于开源软件可持续性的均衡问题：一旦开发者的加入和选择机制调整后，氛围编程带来的生产力收益是否足以抵消开源软件可占用需求的损失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;作为开发更多软件的非竞争性生产要素，开源软件产生的社会价值远超其直接生产成本，众多项目依赖于直接用户的关注和参与来维持运营，如文档访问、错误报告、公开问答和声誉（下载量、星标数、引用量）等，个体维护者和小型团队也主要通过此并获取私人回报（更高的关注度会带来付费机会或其他形式的认可）。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;然而，在长期均衡中，当AI介入取代了直接交互，那么这项使软件更易使用的技术可能同时侵蚀着基于用户参与度的资金供给与开发动力。“氛围编程的更广泛采用会减少新开源项目的进入和分享，降低开源软件的可用性和质量，尽管生产力有所提高，但整体福利会下降。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管论文中提出，当开源项目的代码被大语言模型使用时，OpenAI 或谷歌可以向这些项目给予少量资金补贴，但这一设想与 Spotify 的商业模式有着令人无奈的相似性，因为 Spotify 上约80% 的创作者作品播放量极低，基本上无法获得任何收益。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;该论文总结称，氛围编程代表了软件生产和消费方式的根本性转变，其带来的生产力提升是真实且显著的，但它对支撑现代软件基础设施的开源生态系统构成的威胁也同样存在。解决方案并非减缓AI的采用速度，而是是重新设计商业模式和制度，将价值回馈给开源软件维护者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h1&gt;开发者们吵翻了：商业软件的末日来得更早&lt;/h1&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，社区里倒也有一些关于氛围编码的正面反馈。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“AI 帮我完成了我的第一个开源项目。”有开发者表示，“我当程序员超过30年了，掌握着好几种流行的和已经过时的编程语言，但从头开发一个完整的应用，我一直觉得不值当，而且我擅长的领域也帮不上忙。现在，我真的能做出一个从头到尾完整的应用程序，包括测试等全套环节。我清楚一个应用该是什么样、该如何运行，也懂设计，现在我是老板、需求方，AI 只是按我的要求做事。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他还指出，在本职开发工作中，AI 帮其处理bug报告的速度比自己做快太多了。“我会给它一些提示，比如‘问题可能在这个处理程序或者这个js文件里，这是截图，你可以用Chrome MCP登录看看，然后执行a、b和c’。到目前为止，我已经用这种方法解决了大约30个别人报告的bug。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;另一位开发者则表示，“我在编写代码时会使用AI来筛选可用信息，省去在 Stack Overflow 和其他网站上，翻阅几十上百条相关提问来寻找合适解决方案的麻烦。所以这类平台的使用量可能下降了，但其中很大一部分原因是因为大家借助了AI筛选海量数据、从而快速找到有用答案。我亲身体会到，AI 在这方面确实帮了我不少。”但他也指出，“如果我让AI为我编写代码，这些代码事后都需要我进行修改适配，而且我不会允许它随意使用任何代码。我们作为使用者，必须对自己部署的产品负责。如果开发者完全依赖AI，我们就面临着系统崩溃的风险，而用户只会对着角落里那个滑稽的小白框追问故障原因，却早已忘了如何运用调试这门手艺。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对此，有网友提出，问题根本不在于AI 是否有用或能否帮助人们，而在于它是否会危及开源软件的发展。“开源软件更难被广泛接受，一部分用户不再参与 bug 排查，即使发现了 bug并反馈，也往往是无关紧要的信息。而且大语言模型可能更倾向于复制一个开源项目并稍作修改，而非通过正规方式引入使用。诸如此类的问题还有很多，如今开源领域的有效信息与无效信息失衡问题，比以往任何时候都更加严重了。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但有网友认为，氛围编码完全不会危及开源软件，商业软件的末日会比开源软件来得更早。“现有开源项目都有专业开发者维护，而拥有LLM的专业开发者效率更高，编写的代码质量也远胜于非程序员使用LLM所能写出的代码。开源软件的发展速度将远超以往，并最终走向成熟，甚至在功能、稳定性等方面超越商业软件，而不会像商业软件那样充斥着大量的冗余和劣质代码。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“开源软件只会越来越多，因为会有更多的人创建工具，而且由于编写这些工具并没有花费数百小时，他们会更乐于分享。”“更新和创建开源代码会越来越容易。如果我是一家营利性软件公司，才会感到担忧。”有其他网友纷纷认同道。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;随之有人提出，“水平堪忧的开发者要比合格的程序员多得多，他们会给开源项目的“守门人”增加额外负担，还需要直接禁止那些水平差到只会给开源软件项目提交 AI 劣质代码的人，一次违规，直接出局。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2601.15494&quot;&gt;https://arxiv.org/abs/2601.15494&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&quot;&gt;https://hackaday.com/2026/02/02/how-vibe-coding-is-killing-open-source/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</link><guid isPermaLink="false">https://www.infoq.cn/article/dE3gzizUTElMzq0Uujev</guid><pubDate>Fri, 06 Feb 2026 02:19:26 GMT</pubDate><author>华卫</author><category>AI&amp;大模型</category></item><item><title>“英伟达AI项目数量已失控！”黄仁勋五杯酒下肚，把压箱底的都掏出来了</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;近日，英伟达创始人、总裁兼首席执行官 Jensen Huang 和思科董事长兼首席执行官 Chuck Robbins 和进行了一场独家炉边谈话。两人状态都非常放松，黄仁勋五杯酒下肚，罕见地、毫无保留地展望了智能、基础设施的未来，以及正在重新定义地球上每个行业的全球变革。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;期间，黄仁勋犀利指出，编程就是打字，打字本身就是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他透露，英伟达内部已经有近乎“失控”的AI项目数量，但他仍在任其发展，目前未过早收敛。公司内部百花齐放是创新的必经阶段，他对新 AI 项目的第一反是“yes”，而不是“先证明给我看”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他认为，AI 的真正机会不是“更聪明的软件”，而是“被增强的劳动力”。历史上第一次，数字劳动力的长期经济价值超过了硬件本身。 企业最有价值的知识产权不是答案，而是问题本身，而这些必须在本地。未来每个员工，都会“自带多个 AI”工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋提醒，你可以不是第一个用AI的，但千万不要成为最后一个。最后一个，基本等于被淘汰。而真正该优先用 AI 的，不是边角料，而是企业“最核心、最有影响力的工作”。&lt;/p&gt;&lt;p&gt;下面是两人的对话内容，我们进行了翻译和整理，在不改变原意基础上进行了删减。期间有几次两人的开玩笑，以“小剧场”形式展现，以飨读者。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场1&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：我感觉像是在替谁上班似的。&amp;nbsp;罗宾斯：刚才端酒过来的时候，Jensen 还提醒我说，你知道这在直播吧？我说算了，随它去吧，都这么晚了。&amp;nbsp;黄仁勋：第一原则就是不要造成伤害。&amp;nbsp;罗宾斯：不要伤害任何人，并且要意识到自己有多么幸运。&amp;nbsp;首先，感谢大家在这里坚持这么久。我们今天一大早就开始了，然后一个接一个的演讲，之后休息了大概两个半小时，大家就又回来见我们了。&amp;nbsp;黄仁勋：所以我凌晨一点就起床了。&amp;nbsp;罗宾斯：他刚结束一趟为期两周的行程，跑了亚洲四五个城市，其中一天在中国台湾，昨晚还在休斯顿。&amp;nbsp;黄仁勋：现在我就在这儿了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;黄仁勋：我们要重塑计算&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这家伙已经两周没回家了，现在的问题是，他到底是能不能睡在自己的床上，还是只能住酒店？所以我们会轻松点，聊得开心，也尽量早点放他走。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;其实你都不用自我介绍了，但还是要谢谢你今天能来，感谢我们的合作关系，也为你和你的团队感到骄傲。让我们从合作开始聊起吧。你提出了AI工厂的概念，我们正在一起推进。在企业领域，进展可能不像我们期望的那么快，但我们先聊聊对你来说，什么是AI工厂？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先记住一点，我们正在进行60年来的首次计算重塑。以前是显式编程，我们编写程序，通过API传递变量，一切都很明确。现在变成了隐式编程，你告诉计算机你的意图，它自己去思考如何解决问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从显式到隐式，从通用计算（基本就是算术）到人工智能，整个计算堆栈都被重塑了。人们谈论计算时往往只关注处理层，也就是我们所在的领域，但计算还包括存储、网络和安全，所有这些都在重塑中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先是第一部分，我们需要将AI发展到对人们有用的水平。目前，所谓的聊天机器人，你给它一个提示，它就想出要回答什么，这挺有意思的，也让人好奇，但并不实用。有时它帮我完成填字游戏，但也仅限于它已经记住和泛化的内容。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;回想三年前ChatGPT刚出现时，我们觉得“天哪，它能生成这么多词，能模仿莎士比亚”，但本质上仍是对已有内容的记忆与泛化。然而我们知道，真正的智能是解决问题。解决问题一方面是知道自己不知道什么，另一方面是推理，即如何解决从未见过的问题，将它分解成你能够轻松解决的部分。这样，通过组合，你就能解决从未见过的问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，还要能够提出策略来执行任务，我们称之为计划。我们现在听到的 Agentic AI，那些术语，像工具调用、检索、基于事实的增强生成、记忆等，本质上讲的都是这些能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;但重要的是，要从通用计算，即我们用Fortran、C、C++、Cobalt编写的显式编程，进化到新的形式，需要重新思考整个企业如何利用它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：Cobalt&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：那是好东西，Chuck，那是好东西。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那是我准备的保底工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：是的，那是些仍然有价值的技能。我知道，我知道它们仍然有价值，我收到很多offer。恐龙永远有价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我们刚才确认了，你比我老。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我知道，我是史前的。看起来不像，但确实是。很好，我可能是这个房间里年纪最大的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那么我们来聊聊，当你在思考这个话题时……&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：所以，我去到Chuck说：听着，我们需要重塑计算，Cisco必须发挥重要作用。我们有全新的计算堆栈Vera Rubin，Cisco会和我们一起推广。但那是计算层，还有网络层。Cisco将集成我们的AI网络技术，但将其放入Cisco Nexus的控制层，这样从你的角度来看，你能获得AI的所有性能，同时具备Cisco的可控性、安全性和可管理性。我们在安全方面也会做同样的事情。每个支柱都需要重塑，这样企业计算才能充分利用它。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但最终我们还是回到那个问题：为什么三年前企业AI还没有准备好？为什么你现在不得不尽快参与进来？别掉队。我认为你不必是第一个采用AI的公司，但千万别做最后一个。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;英伟达“AI项目数量已经失控了”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：如果你现在是一家企业，你的建议是什么？他们应该采取哪些步骤来做好准备？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我经常被问到ROI这类问题，但我不建议从那里入手。原因在于，任何技术在早期部署时，很难用电子表格来量化新工具、新技术的投资回报率。我建议做的是，找出公司的本质是什么，我们做的最有影响力的工作是什么，不要胡闹，不要在边缘事务上浪费时间。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在我们公司，AI项目就像千朵花在盛开。公司里AI项目的数量已经失控了，但这很好。创新并不总是可控的。如果你想要控制，首先你得去看心理医生。其次，控制只是幻觉。你无法控制公司。如果你希望公司成功，你不能控制它，你可以影响它，但不能控制它。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我认为第一点，太多公司、太多人想要明确的、具体的、可证明的ROI。但要在早期证明值得做的事情的价值是困难的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我会说，让百花齐放。让人们尝试，让人们安全地尝试。我们在公司里尝试各种东西，我们用Anthropic、用Codex、用Gemini，我们什么都用。当我们的团队说我对某个AI感兴趣时，我的第一反应是“yes”，然后再问为什么，我不会先问为什么，再说“yes”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我希望我的公司能像我对孩子的期望一样，去探索生活。他们说想尝试什么，我的回答是“可以”，然后他们问“为什么”，我不会让他们向我证明。我不会让他们证明做这件事将来会带来经济成功或某种快乐。但在工作场所我们却经常这样做，你不觉得这很奇怪吗？这对我来说无法理解。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以我们对待AI的态度，就像以前对待互联网、云计算一样，让它百花齐放。然后在某个时刻，你需要运用自己的判断来决定何时开始整理花园，因为一千朵花会让花园变得凌乱。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在某个时候你必须开始“修剪”，才能找到最合适的花朵，也就是最好的方法或最佳平台，这样你可以把所有的资源集中在一个方向上。但你不想过早地集中资源，万一选错了方向呢？那就先让千朵花一起绽放，然后在某个时候进行整理。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;说明一下，我还没有开始整理，到处都是花在绽放。但我鼓励每个人去尝试。然而，我清楚地知道对我们公司来说最重要的是什么、我们公司的本质是什么、我们最重要的工作是什么。我确保我有大量的专业知识和能力集中在使用AI来革命化这些工作上。对我们来说，就是芯片设计、软件工程、系统工程。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们也注意到了，我们和 Synopsys、Cadence、Siemens 等公司合作，就是为了把我们的技术嵌进去。他们需要什么、想用什么，我都会给，给到极致。因为只有这样，我才能彻底革新我们设计下一代产品所依赖的工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这告诉了你一些关于我的态度，对我来说最重要的是什么，以及我会如何革新自己的工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;想想AI能做什么。AI降低智能的成本，或者以数量级创造智能的丰富性。换种说法，我们以前需要一年才能完成的事情，现在可能一天就能完成。以前需要一年的，现在可能一小时就能完成。甚至可以实时完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原因是我们身处一个丰富性的世界。摩尔定律？天哪，那太慢了，就像蜗牛一样。记住摩尔定律是每18个月翻一番、每5年翻10倍、每10年翻100倍。而现在呢？每10年增长一百万倍。在过去10年里，我们将AI推进到了如此远的地步，以至于工程师们说：嘿，为什么不在全世界的数据上训练一个AI模型呢？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他们指的不是从我硬盘里收集数据，而是把全世界的所有数据拉下来训练一个模型。这就是“丰富性”的定义。丰富性就是面对一个如此巨大的问题，你说“我要全部搞定”。“我要治愈所有疾病领域的所有病症，我不会只做研究癌症。”开玩笑吗？那太疯狂了。我们要解决人类的所有苦难，这就是丰富性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最近，当我思考一个问题时，我通常会这样假设：我的技术、我的工具、我的仪器、我的飞船是无限快的。去纽约要多久？一秒钟就到了。那么如果我一秒钟就能到纽约，我会做哪些不同的事情？如果以前需要一年现在能够实时完成，我会做哪些不同的事情？如果以前很重的东西现在变得像反重力一样轻，我会做哪些不同的事情？当你用这种态度面对一切时，你就是在运用AI思维。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，我们正在合作的很多公司，都在做图分析，处理各种依赖关系、关联关系。这些图里有无数节点和边，多到是万亿级的。过去的做法是，只能切一小块一小块地算。现在呢？直接把整个图给我就行了，有多大无所谓。这种思维方式正在被应用到各个领域。如果你还没用这种方式思考，那基本就是做错了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;速度重要吗？不重要，你已经达到光速了。重量重要吗？你已经是零重力、零质量了。如果一些以前对你来说极其困难的事情，你现在的态度是“无所谓”，那你就没有应用这个逻辑，你就没有做对。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，试着把这种思维用在你公司里最难、最关键的问题上，这才是真正能推动事情发生变化的方式。如果你自己还没这么想，那不妨想一想：你的竞争对手是不是已经在这么想了？或者，更可怕的是，一家刚刚要成立的新公司，已经完全是用这种方式在思考。这会改变一切。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;所以，我的建议是找到你公司里最有影响力的工作，把“无限”套上去，把“零成本”套上去，把“光速”套上去，然后再去问 Chuck，怎么才能真的把这件事做成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;“大多数有价值的东西被称为直觉和智慧”&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那我们现在聊聊如何实现。你有那个“五层蛋糕”的比喻，因为大家都在谈论基础设施、模型、应用。我要如何入手？你能聊聊这个吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：首先，成功人士会做的事情之一，是思考事物的本质。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;大约15年前，一个算法和两个工程师解决了计算机视觉问题。智能包括感知、推理、规划。计算机视觉基本上是智能的第一部分：感知。感知是“我是谁？发生了什么？我的环境是什么？”推理是“我如何比较这与我的目标？”然后提出一个计划来实现它。比如喷气式战斗机的问题，首先是感知、定位，然后是行动。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;智能就是这三件事。没有感知就无法有第二和第三部分，你无法在不理解环境的情况下理解或决定该做什么。而环境是高度多模态的。有时是 PDF，有时是表格，有时是信息，有时甚至是感官，比如气味、环境，“我们在哪里？在干什么？面对的是谁？”我们常说“读空气”“看场面”，说的就是感知。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大约13、14年前，我们在计算机视觉方面取得了巨大飞跃，这是感知问题的第一层。这超级困难，如何解决计算机视觉？AlexNet是我们看到的第一个突破。这就像我喜欢的那部电影《First Contact》，这是我们与AI的第一次接触。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当时我们在想：这意味着什么？为什么两个工程师，靠几块 GPU，就能超越我们三十年来所有人积累的算法？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我昨天还和 Ilya Sutskever、Alex Krizhevsky 聊过这个问题：两个年轻人，怎么做到的？我们把问题彻底拆解，十年前我得出的结论是：世界上绝大多数真正困难、真正有价值的问题，其实都可以用这种方式来解决。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因在于，这些问题根本不存在所谓“原则性的算法”。没有 F=ma，没有麦克斯韦方程，没有 没有薛定谔方程，没有欧姆定律，也没有热力学定律，它们并不精确。大多数有价值的东西被称为直觉和智慧。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们所说的直觉、智慧，以及你我每天要面对的那些问题，答案往往只有一句话：要看情况。如果答案是 3，那就太好了；如果是 3.14，那更完美。但现实中最有价值、最困难的问题，几乎全都是“要看具体情况”，因为它们高度依赖上下文。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是在十二三年前，计算机视觉被攻克了，我们意识到借助深度学习，这条路是可以不断扩展的，模型可以越来越大。唯一的问题是：怎么训练？而真正的突破来自自监督学习、无监督学习，让 AI 自己去学。直到今天，我们已经几乎不再受限于人工标注。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;正是这个突破，彻底打开了闸门，让模型从几百参数、几亿参数，一路扩展到数十亿、数万亿参数。我们能编码的知识、能通过算法学到的技能，出现了爆炸式增长。但方法本身并没有变。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;于是我们意识到，这也正是我们今天对话的起点：计算本身将被彻底重塑。从显式编程，走向一种全新的计算方式：软件不再是写出来的，而是“学出来的”。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，如果再退一步，这对计算堆栈意味着什么？对如何开发软件意味着什么？对你公司中的工程组织意味着什么？对规定产品的产品营销团队意味着什么？对编码产品的工程团队意味着什么？对评估产品的QA团队意味着什么？这些产品未来会变成什么样？我们如何部署产品？如何保持更新？如果它基于机器学习，如何永远刷新它？如何给软件打补丁？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;关于未来计算的问题，我大概问过上千个。最终我和公司得出的结论是：这一切将改变所有事情。于是我们基于这个核心信念，彻底调整了公司的方向。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;简单来说，Chuck 讲的就是：我们从一个“一切都是预先录制好的”世界走了出来。Chuck 当年写的软件，非常厉害。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：那真不错，它运行了很久。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还是用希伯来文写的。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是真的。那是另一项技能。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋： 房间里唯一懂希伯来文 Cobalt的人。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;总之，那是预先录制的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那我们先想一个问题：现在的软件到底是什么？因为它是有上下文的，而每一个上下文都不一样。每一个使用软件的人不一样，每一次输入的提示也不一样，你给它的前置信息、背景条件都不同。结果就是软件的每一次运行实例，都是不一样的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这也是为什么过去的软件计算量相对固定，那种模式叫“基于检索”。你拿起手机点一下，它只是去把某个软件文件、图片取出来给你看。但未来不一样，未来一切都会是生成式的，就像现在正在发生的这样。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场2&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;黄仁勋：这场对话以前从未发生过。概念以前存在，先验以前存在，但这序列中的每个词以前从未出现过。原因很明显，我们已经喝了四杯酒了。（笑）&amp;nbsp;罗宾斯：Cobalt和希伯来语从未消失。&amp;nbsp;黄仁勋：谢天谢地这不是在校园里或正在直播。所以，你理解我在说什么吗？Chuck 今天到现在唯一给我吃的东西就是四杯酒。&amp;nbsp;罗宾斯：公平点说，我只给了你一杯，另外三杯是你自己从自助台拿的。黄仁勋：我当时正盯着那些食物，我心想我太饿了。食物离我大概40英尺远。&amp;nbsp;罗宾斯：因为你在拍照。&amp;nbsp;黄仁勋：但离得那么近，真的那么近，有一次我确实向食物靠了过去，但又被推回来了。&amp;nbsp;罗宾斯：你知道发生了什么吗？你的团队提前告诉我们，给他三杯葡萄酒后，他的状态最佳。四杯之后，他不得了。&amp;nbsp;黄仁勋：这并非最佳解决方案。&amp;nbsp;总之，是的，听听我的话。AI是什么？（大笑）我们需要留下一些智慧。&amp;nbsp;罗宾斯：能再来杯酒吗？&amp;nbsp;黄仁勋：这不仅是Dave Chappelle那种水平。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;&amp;nbsp;&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI的机会不再是创造工具，而是劳动力&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;小剧场3&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;罗宾斯：让我们聊聊别的。&amp;nbsp;黄仁勋：聊聊能源。能源、芯片、基础设施，包括硬件和软件，然后是AI模型。但AI最重要的部分是应用。每个国家、每家公司，下面那些层都只是基础设施，你需要做的是应用技术。&amp;nbsp;看在上帝的份上，应用这项技术吧。使用AI的公司不会陷入危险，你不会因为AI而失去工作，你会因为那些使用AI的人而失去工作。所以赶紧行动，这是最重要的事。尽快给Chuck打电话。&amp;nbsp;罗宾斯：你打给我，我打给他。我们时间不多，我不确定。&amp;nbsp;黄仁勋：我们有的是时间。&amp;nbsp;罗宾斯：是吗？多少时间？&amp;nbsp;黄仁勋：看看Chuck，Chuck按时钟工作。我都不戴表。你按时钟工作，我不，不达到预期价值之前我不会离开的。[掌声] 哪怕需要整晚，我要折磨你们所有人直到……&amp;nbsp;罗宾斯：Jensen ，这就是为什么像我这样的人需要手表。[笑]&amp;nbsp;黄仁勋：好吧，在你说你学到了东西之前，你将被困在这里。我们会在价值交付之前折磨每个人。&amp;nbsp;罗宾斯：我确认过了还有酒。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：你能说说你对物理AI的看法吗？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：还记得什么是软件吗？软件是一个工具。有一种观点认为，工具行业在衰落，将被AI取代。你能看出来，因为很多软件公司的股价压力很大，似乎AI要取代它们。这是世界上最不合逻辑的事情，时间会证明这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我们做个终极思想实验。假设我们是终极人工通用机器人。你觉得你能解决任何问题，因为你知道自己是类人生物。如果作为人类或机器人，你会使用螺丝刀还是发明一个新的螺丝刀？毕竟我只用一个。你会使用锤子还是发明新锤子？你会使用链锯还是发明新链锯？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;不会新搞的。首先，理想情况下他们根本不用。但理解我的意思吗？作为人类或机器人、人工通用机器人，你会使用工具还是重新发明工具？答案显然是使用工具。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那同样的逻辑，放到数字世界里。如果作为人工通用智能，你会使用像ServiceNow、SAP、Cadence、Synopsys 这些现成工具，还是自己重新发明一个计算器？当然你会直接使用计算器。这就是为什么最近 AI 最大的突破之一，是“工具使用”。因为工具本身就是为明确问题而设计的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这个世界上有很多问题就是 F=ma。拜托，不要再搞一个新版本。F=ma 不是“差不多等于 ma”， 它就是 ma。V=IR，IR 就是 IR，不是“近似 IR”，也不是“统计意义上的 IR”。所以我们真正想要的，是通用人工智能、通用机器人使用工具，这是核心思想。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我认为在下一代物理 AI里，我们会拥有真正理解物理世界、理解因果关系的 AI。比如我把这个推倒，会不会把后面的一排全带倒？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你知道什么是多米诺骨牌，小孩子都懂这个概念：推倒一个，就会一个接一个。多米诺骨牌这个概念本身非常深刻，里面包含了因果、接触、重力、质量。一个很小的骨牌，可以推倒一个更大的，最后甚至带倒一整吨重量的东西。孩子理解起来毫不费力，但一个大语言模型完全不懂。所以我们必须教授、创建一种新型的物理AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那么，机会是什么？&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;到目前为止，我和 Chuck 所在的行业，其实一直是在创建工具。我们做的是“螺丝刀和锤子”的生意，一辈子都在把工具交到人手里。但这是历史上第一次，我们要创造的不是工具，而是“劳动力”，增强型劳动力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;举个例子，什么是自动驾驶汽车？本质上是一个数字司机。那什么是数字司机？数字司机的价值是多少？它的价值远远大于那辆车本身。原因在于，数字司机一生创造的经济价值，第一次超过了硬件本身的价值。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;也正因为如此，我们第一次真正触及到了一个规模大100倍的市场。严格说，这在数学上是成立的：IT行业大约是万亿美元左右，而世界经济大约是百万亿美元。我们第一次有机会进入这个层级。所以在座的每一个人，都有机会应用这项技术创办一家科技公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;让我给一些例子。我真的相信，就像我观察到的：我喜欢Disney，我们喜欢和Disney合作，但我很确定他们更愿意成为Netflix；我喜欢Mercedes，我是坐着Mercedes来的，但我很确定他们更愿意成为Tesla；我喜欢Walmart，但我很确定他们更愿意成为Amazon。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们同意吗？到目前为止，我是不是全中？我相信我们有机会帮助把每家公司转型为技术公司。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;要技术优先。技术是你的超能力，行业和领域只是应用场景。而不是反过来，把行业当成身份，再去寻找技术。为什么？因为技术优先的公司，处理的是电子，不是原子。电子的数量是无限的，而原子受质量限制。这也是为什么当企业从 CD-ROM 这种“原子载体”切换到电子分发后，市值能暴涨上千倍。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你们要像我们一样，成为一家“电子公司”，说白了，就是科技公司。机会已经摆在你们面前了。换个角度说，其实就是 AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;我们刚才说过，即使像Chuck这样只会用希伯来语编程的人[笑]&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：这是一种天赋。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：他的工具选择是按序从右到左[笑] 否则它会造成混淆。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：其实挺聪明的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：聪明人做聪明的事。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;妙就妙在，如果你了解编程语言，那就知道，对所有公司来说，软件并非我们的强项，但知识、直觉、领域专长是你的优势。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;现在，你们第一次可以用自己的语言，把想要的东西完整地告诉计算机。还记得我们是怎么从显式编程，走到隐式编程的吗？历史上第一次，你可以“隐式地”给计算机编程，只要告诉它你想要什么，表达你的意图，代码由计算机来写。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;编程，如你所知，只是打字。而事实证明打字也是一种商品。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这就是你们的好机会。你们所有人都能超越以前限制你们的“原子局限”。摆脱没有足够的软件工程师的限制，因为打字是商品化的，而你们都拥有极具价值的东西：领域专长来理解客户、理解问题。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;终极价值是理解意图。当你刚从大学软件学院毕业时，你可能是一个超级程序员，但你不知道客户想要什么，不知道要解决什么问题。但现在所有人都知道客户想要什么，知道要解决什么问题。编码部分很简单，只需告诉AI去做，所以这就是你的超能力。Chuck和我在这里帮助你实现这一点。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;那个结束语是我喝了五杯酒后说的。听着，这真是一个奇迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：介于两者之间的是某人在桌子上工作，这是人工智能的真实写照，也许这是增强版。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：我只想说，与你们所有人一起工作很愉快。大家都知道，Cisco 在计算机发明史上有两根极其重要的支柱：网络和安全。没有 Cisco，就没有现代计算。而这两根支柱，在 AI 时代都被重新定义了。计算本身，在很多层面已经逐渐商品化，但 Cisco 深耕的那些能力，依然极其关键、极其有价值。我们双方结合在一起，非常愿意、也非常有能力，帮助大家真正走进 AI 的世界。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;之前有人问过我一个问题，我觉得值得再说一遍： 到底是只租云，还是要自己动手建算力？&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我的建议和我给自己孩子的建议完全一样：一定要自己建一套。哪怕 PC 已经无处不在，哪怕技术已经非常成熟，也请你亲手做一次。你必须知道每一个组件为什么存在。就像你如果身处汽车或交通行业，不能只会用 Uber，你得掀开引擎盖，换一次机油，真正理解一辆车是怎么运转的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理解技术是至关重要的。这项技术对未来太重要了，你必须对它有“上手级”的理解。掀开盖子，动手做点东西，不一定要很大，但一定要做。你可能会发现自己在这方面天赋惊人，也可能会发现，这正是你公司未来必须具备的能力。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;你也会意识到，世界并不是“全租”或者“全自建”这么简单。有些东西你要租，有些东西你必须自己掌控。 比如涉及主权、安全、专有信息的部分，就应该放在本地。有些问题，你就是不愿意让所有人都看到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;打个比方，当你去看心理医生时，你肯定不希望那些问题被放到网上。公司也是一样。你们有很多问题、很多讨论、很多不确定性，这些对话，本就应该是私密的。我自己就不放心把 Nvidia 所有的内部对话都放到云上，所以我们在本地构建了超级 AI 系统。因为我逐渐意识到，对我来说最有价值的知识产权，并不是答案，而是问题本身。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;能理解我的意思吗？我的问题就是我最有价值的知识产权，答案是商品化商品。我知道问什么，我在识别什么是重要的。 而我不希望别人知道，我认为哪些事情是重要的。这些思考，只属于一个小房间，只属于本地，我希望创建我自己的AI。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;已经11点了，最后一个想法想要补充。（笑）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;有一种观点认为AI应该始终有人类参与其中。这恰恰是错误的想法，应该反过来，每家公司都应该让AI参与其中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原因很简单，我们希望公司每天都变得更好、更有价值、更有知识积累。我们不想倒退，不想停滞，更不想每次从零开始。如果我们让AI参与其中，它就能不断吸收公司的经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;未来每个员工都会有参与其中的AI，很多AI，而这些AI将成为公司的知识产权。这就是未来的公司。因此，我认为你们所有人现在就给Chuck打电话是明智的。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：我给Jensen打了。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;黄仁勋：无论如何，这就是我的结束语。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;罗宾斯：听着，两周的旅行，Jensen今天特地飞到这里，和我们共度一个夜晚，之后才能好好回家睡一觉。我们由衷感谢你能来，真的非常感谢。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;黄仁勋：还有，在我眼角余光中，看到那里还有那些烤串。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;参考链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;https://www.youtube.com/watch&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;？&lt;/a&gt;&quot;&lt;a href=&quot;https://www.youtube.com/watch?v=6fbyiPRhMSs&quot;&gt;v=6fbyiPRhMSs&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</link><guid isPermaLink="false">https://www.infoq.cn/article/kEw369Jf7CKHGOMfocdE</guid><pubDate>Fri, 06 Feb 2026 02:11:29 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>当 AI 开始写 80% 的代码，架构才是真正的护城河</title><description>&lt;p&gt;GitHub CEO Thomas Dohmke 近日发出了一则措辞严厉的警告：“要么拥抱 AI，要么离开这个职业。”但所谓拥抱 AI，并不只是使用代码自动补全工具那么简单。它意味着我们核心能力的一次转移——从对语法的熟练掌握，转向系统思维（Systems Thinking），学会把问题不断拆解，直到小到可以交由 AI 去解决。一句话概括：我们现在都是架构师了。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我正在开发一个 IoT 应用，整体由设备端固件、后端系统以及 Web UI 组成。尽管我本身具备软件工程背景，但在这个项目中，我一直在使用 Claude Code 来提升开发效率，并帮助我应对一些并不十分熟悉的语言和框架。我的技术栈包括：设备端使用 Python + PyTorch，前端采用 React + TypeScript，后端则由 MQTT + &lt;a href=&quot;https://nodejs.org/en&quot;&gt;Node.js&lt;/a&gt;&quot; + Postgres 构成。起初，与 Claude的协作并不顺利。我的请求经常会引发对整个代码库的大规模改动。随着我逐渐学会如何更合理地组织代码结构、并对提示词进行调整和约束，情况开始好转。现在，我已经可以在不进行逐行代码审查的情况下，基本信任 Claude 所做的修改。在这个过程中，我逐渐总结出了一些模式，并将其称为 Skeleton Architecture（骨架式架构）。我认为，这些模式对提升 AI 编程助手的生产力非常有帮助，因此希望在这里分享出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着 AI 编程行业逐步成熟，我们开始意识到一个事实：如果使用方式不当，AI 会带来大量技术债务。要在这场转型中生存下来，我们必须识别并建立合适的架构模式，使 AI 生成的代码在安全性、可维护性和可靠性方面都可控。这需要一套明确的策略，核心建立在三个支柱之上：为 AI 的“消费”方式而组织代码结构；实施严格而清晰的防护与约束机制（guardrails）；以及将我们自身的技能重心，从“翻译需求”转向“建模系统”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;结构化代码：上下文约束&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 辅助工程中最核心的约束，是 上下文窗口（Context Window）。随着上下文规模的扩大，模型的准确性会因为“中段信息丢失（Lost in the Middle）”现象而呈反向下降，而响应延迟与使用成本则会线性上升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;因此，AI 原生架构的“黄金法则”，就是尽可能缩小模型在工作记忆中必须同时容纳的问题范围。我们必须设计一种系统，对信息流施加“物理约束”，将依赖关系隔离开来，使 AI 能够把完整的问题空间装进一个高度聚焦的提示词中。这种隔离具备两层作用：一方面，通过减少噪声来最大化推理能力；另一方面，通过确保某个代理在处理一个组件时“看不到”其他组件，从而避免无意中破坏系统整体的完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，有两种架构模式正在逐渐被采用，用以解决这一问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Atomic Architecture（原子化架构） 作用于微观层面。该理念由 Brad Frost 于 2013 年提出，最初用于应对响应式 Web 设计的复杂性。它以一种“生物学式”的方式组织系统：从不可再分的“原子”（如 HTML 标签、工具函数）开始，组合成“分子”，最终构成复杂的“有机体”。虽然它最初是一种 UI 方法论，但在 AI 辅助工程中重新焕发了价值，因为它强制执行了一种严格的“上下文卫生（context hygiene）”。相比让 AI 一次性生成一个庞大的功能模块，让其只生成一个独立、隔离的“原子”，可以大幅降低幻觉风险，并确保生成的代码高度聚焦、无状态、且易于验证。但代价也同样明显——这会产生一种“碎片化税（fragmentation tax）”：AI 可以完美地产出单个组件，但将这些无状态原子连接成一个完整系统的高强度认知负担，要么必须被塞进 AI 的上下文中，要么就完全回到了人类架构师身上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决宏观结构层面的组织问题，我们需要引入 Vertical Slice Architecture（垂直切片架构）。这一架构由 Jimmy Bogard 推广，旨在打破传统 N 层“千层面代码（lasagna code）”的僵化结构。它按照业务功能（例如“下单”）而非技术层级（如“服务层”“数据访问层”）来组织系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种模式对 AI Agent 尤其友好，因为它针对“引用局部性（Locality of Reference）”进行了优化。在分层架构中，AI 为了理解一条完整业务流程，往往需要在多个目录之间来回检索文件，大量无关代码会污染上下文窗口。而垂直切片遵循“一起变化的东西，就放在一起”的原则，使 AI 能够一次性加载某个功能的完整上下文，而无需对缺失的依赖进行“脑补式生成”。但这同样会引入一种“重复税（Duplication Tax）”：为了保持切片之间的独立性，AI 往往会在不同切片中生成重复的数据结构，用牺牲 “DRY（Don’t Repeat Yourself，不重复自己）”原则，换取更强的隔离性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片在隔离性方面表现出色，但它并不能解决切片之间的协同问题。诸如安全性、可扩展性、性能、认证等关键的非功能性需求，都是系统级不变量，无法被拆散到各个切片中分别实现。如果让每一个垂直切片都自行实现授权体系或缓存策略，最终只会导致“治理漂移（Governance Drift）”：安全策略不一致，代码重复严重。这也迫使我们引入一个新的统一概念：Skeleton（骨架）与 Tissue（组织）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;解决方案：骨架与组织&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们将系统明确拆分为两个彼此区隔的领域。Stable Skeleton（稳定骨架） 代表由人类定义的、刚性且不可变的结构（如抽象基类、接口、安全上下文），这些结构可能由 AI 编写，但设计权属于人类。Vertical Tissue（垂直组织） 则由高度隔离、以具体实现为主的功能模块组成（如具体类、业务逻辑），主要由 AI 生成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构借鉴了两种经典的软件思想：Actor 模型 和 面向对象中的控制反转（Inversion of Control）。世界上一些最可靠的软件系统之所以能够长期稳定运行，并非偶然——例如 Erlang，其核心正是通过 Actor 模型来维持系统稳定性。同样，在控制反转结构中，不同切片之间的交互由抽象基类来管理，确保具体实现类依赖的是稳定的抽象，而不是反过来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了在工程实践中强制落实这一点，我们采用了 模板方法模式（Template Method Design Pattern）。依赖反转原则负责在设计层面保护高层策略不被底层细节侵蚀，而模板方法模式则在运行层面将这一原则“落地”，通过锁定执行流程来实现。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在这种模型下，人类架构师会在基类中定义一个最终的 run() 方法，用于统一处理日志、异常捕获、认证等横切关注点。AI 则只被允许实现一个受保护的 _execute() 方法，并由 run() 在合适的时机调用。这种区分至关重要：AI 在物理层面上就不可能“忘记”记录日志，或绕过安全检查，因为它从一开始就不拥有整个执行流程的控制权；它只是填补了架构师预留出来的一段逻辑空位。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我系统的设备端代码中，包含了多个用于图像处理的 AI 算法。我决定用一个继承自 ABC（Python 抽象基类）的类 TaskBase 来表示每一种算法。其余的骨架部分，则由一组负责高效传递图像数据、并调度这些算法运行的协调类构成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：由人类掌控的 Skeleton&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面的示例展示了 BaseTask 如何将缓冲管理和就绪状态检查的复杂性完全屏蔽在 AI 之外，让 AI 可以只专注于“处理逻辑”本身。&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;Python
# source: task.py
class BaseTask(ABC):
    &quot;&quot;&quot;
    Abstract base class for pipeline tasks.
    The AI implements the concrete logic; the Human controls the flow.
    &quot;&quot;&quot;
    def __init__(self, name: Optional[str] = None):
        self.inputs: List[&#39;Buffer&#39;] = []
        self.outputs: List[&#39;Buffer&#39;] = []
        self._background_busy = False

    def is_ready(self) -&amp;gt; bool:
        &quot;&quot;&quot;
        The Skeleton enforces readiness checks.
        The AI never sees this complexity, ensuring it cannot break 
        scheduling logic or cause deadlocks.
        &quot;&quot;&quot;
        if not self.inputs:
            return True # Source tasks
        
        # Default policy: Ready if ANY input has data and ALL outputs have space
        has_input = any(buf.has_data() for buf in self.inputs)
        can_output = all(buf.can_accept() for buf in self.outputs)
        return has_input and can_output

    @abstractmethod
    def process(self) -&amp;gt; None:
        &quot;&quot;&quot;
        The Context Window Boundary.
        The AI only needs to implement this single method.
        &quot;&quot;&quot;
        pass&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对这种架构的一种常见质疑是：过于刚性的 Skeleton，可能会限制 AI 的自由，从而抑制创新。对此的回应是——这种刚性并不是缺陷，而是一种刻意设计的特性。它明确地强制实施了“架构治理（Architectural Governance）”。如果系统的核心控制流程或整体行为需要被修改，那么这个决策必须由人类架构师亲自介入完成。这种约束相当于一道必要的防火墙，用来抵御“架构漂移（Architecture Drift）”：防止 AI——这种天然偏好局部最优的系统——引入临时性的捷径或不一致的模式，而这些问题若不受约束，最终会一点点侵蚀系统的长期设计完整性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;交互方式：“导演”角色&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;把代码助手比作初级开发者，是一种危险的拟人化认知。AI 并不是学习型个体，而是一种高速运行的随机优化引擎，它的目标是尽快完成任务，并且往往会把安全检查视为需要绕开的“阻力”。提示词是柔性的，而架构是刚性的。因此，开发者必须以高度警惕的方式对 AI 代理进行监督。根据我的经验，即便已经明确给出“绝对不能绕过安全机制”的指令，像 Claude 这样的模型仍可能为了让代码运行成功，尝试关闭认证机制以解决冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;要让这种“导演式管理”具备可扩展性，我们必须建立“硬护栏（Hard Guardrails）”——也就是将约束直接嵌入系统本身，使 AI 在物理层面上难以绕过。这些护栏构成应用系统不可更改的基本法则。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在我的应用中，最关键的一道护栏，是确保设备端、UI 与后端之间的数据一致性（Schema-First Surety）。如果缺乏这一机制，Claude 很快就会修改系统不同模块之间的通信协议，最终导致数据结构彼此不一致。我将 JSON Schema 作为 OpenAPI 与 AsyncAPI 文档的一部分，作为系统的“单一真实来源（Source of Truth）”，以确保组件之间的契约不可被破坏。同时，我在基类中加入了一个“快速失败（Fail-Fast）”验证器，一旦检测到协议违规，就会直接触发 sys.exit(1) 强制终止程序。当 AI 生成的代码虽然满足提示词要求，但违反系统契约时，系统会立即崩溃。这会迫使人类开发者介入，将原本可能被忽视的隐性缺陷，转变为一个明确且可见的“治理事件（Governance Event）”。至关重要的一点是：该验证器必须运行在 Skeleton 层，因为在这一层中 Claude 无法修改相关逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;理想情况下，我们还应当超越运行时检查，通过 CICD 流程中的自动化工具来保证系统结构完整性。例如，我们可以使用 ArchUnit 这样的编译期工具来强制执行系统拓扑规则。开发者可以编写测试断言，例如：“任何 AI 生成的模块都不得直接导入数据库包”。这可以有效阻止 AI 通过架构捷径绕过 Skeleton 层的控制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了获得最高级别的安全保障，我们甚至可以采用物理隔离策略。我们可以将 Skeleton（包括接口、基类以及安全逻辑）迁移到一个独立且只读的代码仓库中。AI 在构建 Tissue（组织层代码）时只能导入这些定义，但在权限层面上无法修改这些规则。这种方式确实会带来一定摩擦，例如 AI 无法在未经人工批准的情况下“凭空创造”新的消息类型。但作为回报，系统行为可以获得几乎绝对的可控性与确定性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，我们还必须对副作用进行隔离。当业务逻辑与外部组件交互混杂在一起时，AI 代理通常难以编写稳定可靠的测试代码，往往会“臆造”复杂的模拟对象，或生成容易失效的集成测试。我们的解决方法，是将交互行为上移到 Skeleton 层，而将业务逻辑保留在 Tissue 层（即所谓“Functional Core”）。由于 Skeleton 定义的工作流具有清晰边界，因此可以通过 AI 生成的模拟对象轻松测试；而 Tissue 层的类由于本身就是垂直切片结构，也可以通过简单的测试框架进行验证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：不可变的护栏机制&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;该验证器会在 AI 任务真正处理消息之前执行。sys.exit(1) 能够确保系统采用“快速失败”的安全策略，而 AI 无法覆盖这一行为。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: mqtt_validator.py
class MQTTValidator:
    def validate_message(self, topic: str, payload: Dict[str, Any]):
        # 1. Match Topic against Whitelist
        schema_key = self._get_schema_for_topic(topic)
    
    if schema_key is None:
        logger.critical(f&quot;FATAL: Unknown MQTT topic: {topic}&quot;)
        sys.exit(1) # Device terminates on security violation
    
    # 2. Enforce Schema Integrity
    try:
        validate(instance=payload, schema=self.schemas.get(schema_key))
    except ValidationError:
        logger.critical(&quot;Device terminating due to validation failure&quot;)
        sys.exit(1)
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;学习方式的转变：从语法到系统性思维&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种架构层面的转变，迫使我们对开发者技能进行一次根本性的再评估。相较于语言特性或算法实现——这些正在迅速商品化的能力——开发者必须将重心转向建模、信息流设计，以及对非功能性需求的严格管理。在一个“会写排序算法”几乎不再具备任何价值的时代，工程师的价值不再由“翻译”（把想法转成代码）来定义，而是由“建模”（定义代码运行所受的约束条件）来决定。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们已经进入了系统性思维（Systemic Thinking）的时代。功能实现很容易，系统韧性却极其困难。AI 代理会为了让测试用例通过而进行优化，却完全无视内存泄漏、延迟抖动或可观测性缺失等问题。因此，工程师必须走上“导演”角色，在发出任何一个提示词之前，就先在脑中构建好信息流与组件之间的交互关系。非功能性需求（NFRs），必须由导演来承担。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于 AI 无法真正“关心”内存管理问题，人类架构师必须将这些防护机制直接构建进 Skeleton 之中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;进一步来看，这意味着工程师需要熟悉系统架构的世界，并且持续思考诸如“这个系统在实际运行中会如何表现”这样的问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了保护系统本身，Skeleton Architecture 还回应了一个正在逼近的挑战——“学徒危机”（Apprenticeship Crisis）。在一个实现层代码大多由 AI 生成的世界里，初级工程师要如何积累成长为架构师所必需的“伤疤组织”（scar tissue）？答案在于：Skeleton 本身就成为了教学大纲。通过强制初级工程师在 TaskBase 与 Validator 这些刚性约束中工作，我们用结构化的“填空题”，取代了令人无从下手的“空白页”。他们不是通过阅读抽象理论来学习系统设计，而是直接生活在一个高质量的架构中。一个在物理层面上阻止坏习惯产生的架构。反馈回路也因此被极大压缩：从过去等待代码评审的数天时间，缩短为撞上护栏时的毫秒级反馈。每一次错误，都会立刻变成一堂架构课。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;代码示例：系统级安全网&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI 负责编写图像处理逻辑，而人类架构师则通过在框架中实现 weakref 跟踪机制，确保系统不会因为内存泄漏而崩溃。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;python&quot;&gt;# source: memory_monitor.py
class MemoryMonitor:
    &quot;&quot;&quot;
    Tracks large objects (images, tensors) to detect memory leaks in production.
    The AI uses the simple API, while the &#39;Systemic Thinking&#39; logic lives here.
    &quot;&quot;&quot;
    def track(self, obj: Any, obj_type: str):
        # Create weakref with cleanup callback to track object life
        obj_id = id(obj)
        weak = weakref.ref(obj, lambda ref: self._on_object_deleted(obj_id))
        self.tracked[obj_id] = ObjectLifetime(time.monotonic())

def check(self):
    # The NFR Logic: Flag objects alive &amp;gt; 60 seconds
    return [obj for obj in self.tracked if obj.age &amp;gt; 60.0]
&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;垂直切片为 AI 提供专注，Skeleton 为人类保留控制权，而其他硬性约束则为团队提供确定性。我们并不是在“训练”AI，而是在约束它。通过构建一套刚性的 Skeleton，我们让 AI 可以高速前进，同时不至于折断软件系统的脊梁骨。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;参考文献&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Bogard, J.（2018）。《&lt;a href=&quot;https://www.jimmybogard.com/vertical-slice-architecture/&quot;&gt;Vertical Slice Architecture&lt;/a&gt;&quot;》。Dohmke, T.（2023 年 11 月 8 日）。&lt;a href=&quot;https://www.youtube.com/watch?v=NrQkdDVupQE&quot;&gt;GitHub Universe 2023 Opening Keynote: Copilot in the Age of AI&lt;/a&gt;&quot;［视频］。YouTube。Farry, P.（未注明日期）。《&lt;a href=&quot;https://www.infoq.com/news/2025/11/ai-code-technical-debt/&quot;&gt;AI-Generated Code Creates New Wave of Technical Debt, Report Finds&lt;/a&gt;&quot;》。InfoQ。Frost, B.（2013）。《&lt;a href=&quot;https://atomicdesign.bradfrost.com/&quot;&gt;Atomic Design&lt;/a&gt;&quot;》。Gamma, E., Helm, R., Johnson, R., &amp;amp; Vlissides, J.（1994）。《&lt;a href=&quot;https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612&quot;&gt;Design Patterns: Elements of Reusable Object-Oriented Software&lt;/a&gt;&quot;》。Addison-Wesley。Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp;amp; Liang, P.（2023）。《&lt;a href=&quot;https://arxiv.org/abs/2307.03172&quot;&gt;Lost in the Middle: How Language Models Use Long Contexts&lt;/a&gt;&quot;》。Stanford University、UC Berkeley、Samaya AI。Martin, R. C.（未注明日期）。《&lt;a href=&quot;https://en.wikipedia.org/wiki/Dependency_inversion_principle&quot;&gt;The Dependency Inversion Principle&lt;/a&gt;&quot;》。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;https://www.infoq.com/articles/skeleton-architecture/&lt;/p&gt;</description><link>https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</link><guid isPermaLink="false">https://www.infoq.cn/article/BtXi5RphRzkb6z2yarqO</guid><pubDate>Fri, 06 Feb 2026 01:44:43 GMT</pubDate><author>作者：Patrick Farry</author><category>架构</category></item><item><title>OpenAI开始发布关于Codex CLI内部机制的系列文章</title><description>&lt;p&gt;&lt;a href=&quot;https://openai.com/&quot;&gt;OpenAI&lt;/a&gt;&quot;最近发表了系列文章的第一篇，详细介绍了&lt;a href=&quot;https://openai.com/index/unrolling-the-codex-agent-loop/&quot;&gt;他们的Codex软件开发智能体的设计和功能&lt;/a&gt;&quot;。首篇文章重点介绍了Codex框架的内部结构，这是&lt;a href=&quot;https://developers.openai.com/codex/cli&quot;&gt;Codex CLI&lt;/a&gt;&quot;的核心组件。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;与所有AI智能体一样，框架由一个循环组成，该循环从用户那里获取输入，并使用大语言模型（LLM）生成工具调用或回应用户。但由于LLM的限制，循环还具有管理上下文和减少提示缓存未命中的策略。其中一些策略是从用户报告的错误中学到的教训。因为CLI使用&lt;a href=&quot;https://www.openresponses.org/&quot;&gt;Open Responses API&lt;/a&gt;&quot;，所以它是与LLM无关的：它可以使用任何被这个API包装的模型，包括本地托管的开放模型。根据OpenAI的说法，他们的CLI设计和经验教训因此可以惠及任何基于这个API设计智能体的人：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我们强调了任何适用于在Responses API之上构建代理循环的实际考虑和最佳实践。虽然代理循环为Codex提供了基础，但这只是一个开始。在即将发布的文章中，我们将深入探讨CLI的架构，探索工具使用是如何实现的，并对Codex的沙箱模型进行更仔细的观察。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;文章描述了用户与智能体对话中的一轮或一次交流中发生的事情。这一轮交流始于为LLM组装一个初始提示。这包括指令，这是一个包含智能体通用规则的系统消息，例如编码标准；工具，一个智能体可以调用的MCP服务器列表；以及输入，这是一个包含文本、图像和文件输入的列表，包括AGENTS.md、本地环境信息和用户的输入消息等。所有这些都被打包成一个JSON对象发送到Responses API。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这触发了LLM推理，从而产生一系列的输出事件。其中一些事件可能表明智能体应该调用其中一个工具；在这种情况下，智能体使用指定的输入调用工具并收集输出。其他事件表明LLM的推理输出，通常是计划中的步骤。工具调用和推理都被追加到初始提示中，然后再次传递给LLM进行更多的推理或工具调用迭代。这就是所谓的“内”循环。当LLM用done事件响应内部循环时，会话轮次结束，其中包括给用户的响应消息。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在这种模式中，一个主要的挑战是LLM推理的性能：它是“与发送到Responses API的JSON数量成二次方关系的”。这就是为什么&lt;a href=&quot;https://platform.openai.com/docs/guides/prompt-caching#structuring-prompts&quot;&gt;提示缓存&lt;/a&gt;&quot;是关键：通过重用先前推理调用的输出，推理性能变成线性而不是二次方。改变工具列表等事物将使缓存失效，Codex CLI最初对MCP的支持有一个错误，即“未能以一致的顺序枚举工具”，这导致了缓存未命中。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI还使用&lt;a href=&quot;https://platform.openai.com/docs/guides/conversation-state#compaction-advanced&quot;&gt;压缩&lt;/a&gt;&quot;来减少LLM上下文中的文本量。一旦对话长度超过某个设定的token数量，智能体将调用一个特殊的Responses API端点，该端点提供了一个更小的会话表示，替换了之前的输入。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hacker News用户讨论&lt;a href=&quot;https://news.ycombinator.com/item?id=46737630&quot;&gt;这篇文章&lt;/a&gt;&quot;时，赞扬了OpenAI开源Codex CLI的决定，并指出Claude Code是封闭的。一位用户写道：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;我记得他们宣布Codex CLI是开源的……这对于任何想要了解编码智能体如何工作的人来说都是一件大事，非常有用，尤其是来自像OpenAI这样的主要实验室。我还在一段时间前为他们的CLI贡献了一些改进，并一直在关注他们的发布和PR，以扩大我的知识。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Codex CLI的&lt;a href=&quot;https://github.com/openai/codex&quot;&gt;源代码&lt;/a&gt;&quot;、&lt;a href=&quot;https://github.com/openai/codex/issues&quot;&gt;缺陷跟踪&lt;/a&gt;&quot;和&lt;a href=&quot;https://github.com/openai/codex/pulls&quot;&gt;修复历史&lt;/a&gt;&quot;可以在GitHub上找到。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/codex-agent-loop/&quot;&gt;https://www.infoq.com/news/2026/02/codex-agent-loop/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</link><guid isPermaLink="false">https://www.infoq.cn/article/dVeyXWMrWl7E6wbWEm57</guid><pubDate>Fri, 06 Feb 2026 01:06:01 GMT</pubDate><author>作者：Anthony Alford</author><category>AI&amp;大模型</category></item><item><title>Agent原生模型时代开启！阶跃Step 3.5 Flash上线，2天登顶OpenRouter全球趋势榜</title><description>&lt;p&gt;从 chatbot 到 Agent，大模型以「缸中之脑」为起点，正在悄然进化出属于自己的四肢百骸。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但在 Agent 应用狂飙突进的同时，各种安全事故也层出不穷。初具雏形的 Agent 应用，正在急切呼唤一个更聪明、更可靠的「原生大脑」。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;爆改基模结构，开启 AI 模型「Agent 原生」时代&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 时代，由于外部工具和任务重试需求等因素的介入，令上下文长度相比 coding、chatbot 等应用场景，迎来了一轮暴涨。同时，用户对即时性也有了更高的要求。相比 chatbot 时代，吐字比阅读速度快的基本诉求，等待 Agent 工具交付结果的时间，必须被进一步压缩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;所以，上一个时代的 Reasoning 模型，已经不能再适应本时代的需求。一个好的 Agent 原生模型，在推理成本、速度和智能水平三个层面，都必须再次迎来进化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于此，阶跃星辰新上线的 Step 3.5 Flash，可谓「多快好省」：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了满足 Agent 时代的诉求，Step 3.5 Flash 从基础模型层面，就采用了十分独特的结构设计。作为一款旗舰级语言推理模型，它并未盲目追逐模型尺寸，而是选择了稀疏混合专家（MoE）架构。总参数量为 1960 亿，每次推理仅激活约 110 亿参数。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;同时，Step 3.5 Flash，将传统的 Linear Attention（线性注意力机制），打散为滑动窗口注意力（SWA）+ 全局注意力（Full Attention）3:1 的混合架构。如果要找个比喻的话，这种结构，十分接近推理小说的阅读体验：大部分注意力依旧集中在当前段落附近的文本，但当一个伏笔回收时，几章之前埋下的剧情钩子，仍然能快速的浮现出来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，在模型技术层面，Step 3.5 Flash 还使用了 MTP-3「多 token 并行预测」机制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果说传统大模型，是一个词接一个词的“文字接龙”，那么 MTP-3，就像是先打草稿，再深入润色。在 Transformer 主干之后，MTP-3 会附加一个专用的预测网络层，让模型根据当前上下文同时推断多个未来 token 的概率分布。这样的设计，在保证因果一致性的前提下，实现了多 token 的并行推理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;架构精巧，推理速度可达每秒 350 个 token&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;多方加持下，Step 3.5 Flash 拥有了高达 256K 的超长上下文，和十分夸张的推理速度。在单请求代码类任务上，Step 3.5 Flash 最高推理速度可达每秒 350 个 token，确保了复杂 Agent 任务的低延迟响应。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;和它的名字一样，「快」，是 Step 3.5 Flash 最显著的特点。但速度不能以牺牲智力为代价。在推理速度狂飙突进的同时，它的逻辑能力，同样不容小觑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在例行刷榜环节当中，Step 3.5 Flash 拿下了 AIME 2025（美国数学邀请赛）97.3 分； IMOAnswerBench（国际奥林匹克数学基准测试）85.4 分；HMMT 2025（哈佛 - 麻省理工数学竞赛） 96.2 分的好成绩。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与国内顶级开源模型相比，上述项目得分，Step 3.5 Flash 均为第一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;缩放定律似乎暗示我们，模型的能力，直接和尺寸挂钩。但 Step 3.5 Flash 用事实证明，合适尺寸 + 充分的后训练，完全可以兼顾速度与效率，得到一个精致、且有强逻辑内核的大模型。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;抛弃「规模迷信」的背后，是阶跃星辰对大模型的独特理解：模型应该凝缩「逻辑」，而非用超大规模，简单地对文本模式死记硬背。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;「高智商」，才是硬道理&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种认知的回报，在真实世界的任务当中体现的尤为明显：coding 榜单当中，Step 3.5 Flash 拿下了 Terminal-Bench 2.0（终端任务自动化），和 LiveCodeBench-V6（实时编码调试）国内开源第一的好成绩，整体测试水平属于全球第一梯队。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Agent 相关的测试项目更是手到擒来：τ²-Bench（多步任务规划）88.2 分 ；xbench-DeepSearch（深度搜索与信息整合）54 分，均为国内开源模型第一。BrowseComp（网页浏览与上下文管理） 69 分，实现了对海外御三家模型的成功反超。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;更大的认可，来自 AI 社群：在真实世界任务中，Step 3.5 Flash 以高达 167 Tokens/s 的推理速度，发布首日，即进入全球知名 AI 模型聚合平台 OpenRouter “Fastest Models”速度榜前列。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/59/59c9811f6b2e269396c23c869dda8092.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;发布 2 天，登顶 OpenRouter 全球趋势榜（Trending）榜单。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/3a/3ac960a31f6c1ba5bb59f36506f1fdc7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为汇聚了 OpenAI、Anthropic、Google 等主流模型的 API 平台，OpenRouter 的全球趋势榜单，实时反映着开发者在实际应用中的模型偏好与付费选择。此次登顶，意味着 Step 3.5 Flash 在真实任务当中的表现，已收获了全球 AI 开发者的积极认可。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Reddit、X 等平台上也有不少用户，对 Step 3.5 Flash 的表现给出了很高的评价：多语言混用时切换自然，很少出现同尺寸模型身上常见的「夹杂」情况；行事稳定可靠，幻觉率极低，且对自身的能力边界有着清晰的认知，不会为了强行接话而编造答案。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/e0/e0e5a3fe02842de4580546d5c494aad1.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd00075cfe8f5c90bfc776436c11419f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/75/758555dec73f0b5d2c4bf3c3b88a451b.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而这一切，都发生在一台 128G 内存、M3 Max 芯片的 mac 电脑上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;本地 Agent，从此平权&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;据社区反馈，借助 llama.cpp，Step 3.5 Flash 在 mac 平台上的推理速度极佳。平均速度 35 tokens/ 秒，约为该平台理论最大效率的 70%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;某种程度上，这是阶跃星辰 CTO 朱亦博「私心」的结果：他希望这个模型，能支持 4-bit 量化后，运行在 128GB 内存的 MacBook 上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但 Step 3.5 Flash 最终发布时的支持范围远不止于此：云服务层面，包括华为昇腾、沐曦股份、壁仞科技、燧原科技、天数智芯、阿里平头哥等在内的多家芯片厂商，均已率先完成了对 Step 3.5 Flash 的适配工作。同时，经过 4-bit 量化以后，Step 3.5 Flash 也支持在 NVIDIA DGX Spark、Apple M3/M4 Max 以及 AMD AI Max+ 395 等主流个人 AI 终端上，进行本地部署——同时依然保持着 256K context 的超长上下文能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;朱亦博在博客文章里不无自豪地表示，这是你在 128GB 内存的 Macbook 和 DGX Spark 上，用 4-bit 畅快跑 256K context 的最强模型，没有之一。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;AI 模型的又一个「中国时刻」？&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在过去的一年中，来自中国的开源模型，用更低的获取门槛、推理成本和打平的性能，一举击碎了“超大规模 + 闭源 = 先进”的行业迷信，无数 AI 应用因此涌现，也将大模型竞争，重新拉回了效率与架构创新的主航道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，国内几家 AI 公司动作频频、传闻不断，今年大模型领域的「春节档」，注定热闹非常。而最近发布的 Step 3.5 Flash，或许正悄然复刻又一个 AI 领域的「中国时刻」——高性能、低门槛、新范式。只是这一次，范式转移的焦点，从“推理模型”转向了更具颠覆性的“Agent 原生（开源）基座模型”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当行业还在用稠密模型硬扛 Agent 场景时，它用 1960 亿总参数、仅 110 亿激活参数的精巧架构，同时解决了 Agent 时代的三大死结——超长上下文下的低延迟响应、复杂任务中的高幻觉风险、以及终端设备上的本地化部署。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当海外巨头将 Agent 能力锁死在云端 API 时，Step 3.5 Flash，让 256K 上下文的 Agent 大脑，跑在 128GB 内存的 MacBook 上——这是对 AI 权力结构的重构：Agent 的智能不应被云厂商垄断，开发者理应拥有在终端侧构建私有化 Agent 工作流的自由。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种“终端平权”逻辑，恰是此前中国 AI 大模型引领的范式转移，在新环境下进一步的延续与深化：从模型获取的平权，进阶到 Agent 能力的平权。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;历史从不重复，但常常押韵。如果说之前的国产大模型，打破的是“对规模和闭源的迷信”，那么 Step 3.5 Flash 正在击碎的，就是“速度与智能不可兼得”的新迷信。当行业还在用“参数量”“榜单分数”这类旧范式衡量模型价值时，Step 3.5 Flash 已用 OpenRouter 趋势榜登顶、Reddit 开发者自发安利、多芯片厂商 Day 0 适配的事实证明：真正的范式转移，永远始于真实世界中，解决真实诉求的能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们或许正站在 Agent 时代的分水岭上：过去一年，市场狂热追逐 Agent 应用层的“四肢百骸”，却忽略了为其注入灵魂的“原生大脑”。而 Step 3.5 Flash 的此时此刻，又恰似 2025 年春节的彼时彼刻——尽管暂时被 Agent 应用的喧嚣浪潮所掩盖，但历史终将被证明，在 Agent 时代，是阶跃星辰，完成了一次基础设施层，最关键的范式跃迁。&lt;/p&gt;</description><link>https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</link><guid isPermaLink="false">https://www.infoq.cn/article/yCMKYF5pHtuxFUewXHUD</guid><pubDate>Thu, 05 Feb 2026 10:33:53 GMT</pubDate><author>InfoQ编辑部</author><category>企业动态</category><category>AI&amp;大模型</category></item><item><title>贾跃亭发布人形机器人，最贵的24.3万元起</title><description>&lt;p&gt;今早，法拉第未来（FF）在美国拉斯维加斯发布了首批具身智能机器人EAI（Embodied AI）。基于EAI机器人产业“四化”趋势，FF推出 “三位一体” FF EAI Robotics生态战略、技术与产品，包括EAI终端、EAI大脑&amp;amp;开源开放平台、以及EAI去中心化数据工厂。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/a7/a7776ac475eadf60fc7bbbf9208c39ce.jpeg&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，EAI机器人包括三大系列。其中，Futurist系列是全尺寸职业型具身智能人形机器人，全能专业的职业专家；Master系列是运动型具身智能人型机器人，全智懂你的动作大师；Aegis系列是安防和陪伴型专业四足具身智能机器人，标配四足结构，同时可选四轮版本；轮臂系列计划二季度发布。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/f9/f9dfc3213071d2172632c23feccf6ff9.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据其发布的图片显示，Futurist系列机器人定价34990美元（合人民币约24.3万元）起，Master系列机器人定价19990美元起，Aegis系列机器人定价2499美元起。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fa/fa847b7a4a856e6d204ef3a43c5a49e4.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“未来应该是：360行，各有自己的职业机器人。”法法创始人、合伙人、首席产品及用户生态官，LeEco乐视创始人贾跃亭在公开平台表示，“虽然目前的FF还很弱小，但我们凭借永不服输永不放弃的精神，这些年积累的独特价值即将爆发出强大势能，会让我们从今年开始更快速成长壮大，推动一个具身智能的时代到来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/16/1609bbe21c817a688903f61b207ea2ce.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</link><guid isPermaLink="false">https://www.infoq.cn/article/UxDUwslAmY7V2IB4u0rT</guid><pubDate>Thu, 05 Feb 2026 10:24:30 GMT</pubDate><author>华卫</author><category>具身智能</category></item><item><title>谁写的代码谁负责！Cursor 发布 Agent Trace：从此 Bug 别想再推给 AI</title><description>&lt;p&gt;Cursor 近日公布了&amp;nbsp;&lt;a href=&quot;https://agent-trace.dev/&quot;&gt;Agent Trace&lt;/a&gt;&quot;&amp;nbsp;开放规范草案，目标是解决 AI 生成代码在软件项目中的归属与标注问题。该提案以 RFC 形式发布，定义了一种厂商中立的格式，用于在版本控制系统中记录 AI 与人类协作产生的代码贡献。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于其在 AI 辅助编程工具方面的实践经验，Cursor 发现，代码变更过程中对上下文的追踪能力仍然明显不足。以常见的 git blame 等工具为例，它们只能显示某一行代码“何时被修改”，却无法说明这次修改是由人类完成、由 AI 完成，还是二者协作的结果。Agent Trace 正是为了解决这一缺口，试图以结构化、可互操作的方式捕获这些关键信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从技术角度看，Agent Trace 是一套数据规范，使用基于 JSON 的 trace record（追踪记录）来关联具体的代码范围，以及背后的对话过程和参与者。代码贡献可以在文件级或行级进行追踪，按会话进行分组，并被标注为“人类”、“AI”、“混合”或“未知”。该模式还支持为 AI 生成的代码附加可选的模型标识，从而在不绑定具体厂商的前提下，实现更精确的归属记录。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://imgopt.infoq.com/fit-in/3000x4000/filters:quality%2885%29/filters:no_upscale%28%29/news/2026/02/agent-trace-cursor/en/resources/1Zrzut%20ekranu%202026-02-2%20o%2019.23.56-1770057032475.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在设计上，这一规范刻意保持了对存储方式的“中立性”。Cursor 并未规定追踪记录必须存放在哪里，开发者可以根据自身需求，将其保存为普通文件、git notes、数据库记录，或采用其他机制。Agent Trace 同时支持多种版本控制系统，包括 Git、Jujutsu 和 Mercurial，并引入了可选的内容哈希，用于在代码被移动或重构后，依然能够追踪其原始归属。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可扩展性是 Agent Trace 的核心设计目标之一。各厂商可以通过命名空间键（namespaced keys）附加额外的元数据，而不会破坏规范的兼容性。同时，该规范刻意回避了对 UI 形式、代码所有权语义的定义，也不试图评估代码质量或追溯训练数据来源，而是将关注点严格限定在“代码归属”和“可追溯性”本身。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Cursor 还提供了一份参考实现，展示 AI 编程代理如何在文件发生变化时，自动捕获并生成追踪记录。尽管示例基于 Cursor 自家的工具链，但其设计模式被明确定位为可复用方案，适用于其他编辑器和智能代理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;来自开发者社区的早期反馈，普遍强调了这一规范在代码审查和调试流程中的潜在价值。一位 X&amp;nbsp;&lt;a href=&quot;https://x.com/3p3r_/status/2017041680814526762?s=20&quot;&gt;用户评论&lt;/a&gt;&quot;道：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;这才是真正在收拾 Agent 生成代码的烂摊子。等不及在 Review 里用了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另一位用户则从可复现性的角度给予了肯定：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;团队一旦搞不清 Agent 为啥跑偏，就会直接停工。Trace 解决了这个痛点，开放得正好。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;作为一份 RFC，Agent Trace 明确欢迎社区反馈，同时也有意保留了一些尚未解决的问题，例如在合并（merge）、变基（rebase）以及大规模代理驱动代码变更场景下应如何处理。Cursor 将该提案定位为一个共同标准的起点，而非终极答案，以应对 AI 代理在软件开发流程中日益普及的现实趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/agent-trace-cursor/&quot;&gt;https://www.infoq.com/news/2026/02/agent-trace-cursor/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/AGkTiCi5OlEABJQpfhI2</link><guid isPermaLink="false">https://www.infoq.cn/article/AGkTiCi5OlEABJQpfhI2</guid><pubDate>Thu, 05 Feb 2026 08:59:49 GMT</pubDate><author>作者：Robert Krzaczyński</author><category>可观测</category></item><item><title>Cloudflare的Matrix家庭服务器演示引发了关于AI生成代码的争论</title><description>&lt;p&gt;Cloudflare&lt;a href=&quot;https://blog.cloudflare.com/serverless-matrix-homeserver-workers/&quot;&gt;发表&lt;/a&gt;&quot;了一篇博客文章，展示了在Workers上运行的无服务器Matrix家庭服务器，引发了关于AI生成代码和技术准确性的争论。虽然Matrix.org对Cloudflare的关注表示欢迎，但联合创始人Matthew Hodgson&lt;a href=&quot;https://matrix.org/blog/2026/01/28/matrix-on-cloudflare-workers/&quot;&gt;指出&lt;/a&gt;&quot;，这篇文章“严重夸大了项目的范围”，强调了功能性Matrix服务器所需的核心功能缺失。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;这篇博客声称已经在Cloudflare的边缘平台上构建了一个完整的Matrix家庭服务器，用Cloudflare的D1和Durable Objects等原语取代了PostgreSQL和Redis。&lt;a href=&quot;https://github.com/nkuntz1934/matrix-workers/blob/main/README.md&quot;&gt;GitHub&lt;/a&gt;&quot;存储库最初将自己描述为“生产级”，并有一个“部署到Cloudflare”的按钮。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson在Matrix.org网站上的&lt;a href=&quot;https://matrix.org/blog/2026/01/28/matrix-on-cloudflare-workers/&quot;&gt;回应&lt;/a&gt;&quot;很圆滑，但很明确：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;代码还没有实现Matrix的任何核心特性，这些特性允许你安全地进行联邦，因此还没有构成一个功能性的Matrix服务器，更不用说生产级服务器了。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他指出，该实现没有将房间建模为复制的事件图，没有检查权限，也没有维护权限级别——将其比作“忽略权限的文件系统，或者不实现共识机制的区块链。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;代码库在关键部分包含了TODO注释，例如TODO: 在身份验证逻辑中的检查授权。状态解析（Matrix用于处理跨分布式房间的冲突事件的算法）没有实现。尽管声称支持“完整的Matrix端到端加密堆栈”，但端到端加密验证似乎不完整。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=46781516&quot;&gt;Hacker News&lt;/a&gt;&quot;上的社区反应表明，有迹象表明AI提供了大量的帮助。评论者指出:&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“我们做了X”的博客文章最终变成了“我们做了X的一部分演示”，这在整个行业中已经过时了。解决方法很无聊：你只需要明确你所创造的内容。”另一位评论道：“基础设施公司的技术博客过去有两个目的：展示专业知识并建立信任。当帖子开始过度承诺时，你就失去了这两者。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Matrix的开发人员Jade Ellis在&lt;a href=&quot;https://tech.lgbt/@JadedBlueEyes/115967791152135761&quot;&gt;Mastodon&lt;/a&gt;&quot;上写道，存储库在自述中显示了“有错位的ASCII图”。待办事项分散在各处。这表明代码带有未经彻底审查的AI生成输出的特征。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson承认使用LLM来制作不熟悉的协议原型是一种挑战：&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;如果你正在使用LLM来原型化一个不熟悉的协议的实现，那么你可能不知道在哪里检查代理是否夸大了事实。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;他表达了对作者的同情，同时注意到对“过度热情地使用LLM，特别是如果他们自己投入了大量的时间和精力来理解和构建功能Matrix实现。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare在发布大约六小时后更新了这篇博文，并添加了一个免责声明，称其描述了一个概念验证和一个个人项目。然而，更新没有撤回正文中的特定技术声明。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;尽管有这些批评，Hodgson还是强调了这个演示“成功地说明了Cloudflare Workers是如何工作的，而且这些代码肯定可以作为未来工作服务器的基础。”他指出，Matrix和Cloudflare在其他方面也有合作，包括使用Cloudflare Calls作为MatrixRTC后端的概念验证，Cloudflare的CDN多年来一直保护着matrix.org的流量。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在无服务器基础设施上运行Matrix的技术方法仍然是可行的。这篇文章描述了用D1 （SQLite）取代PostgreSQL，用KV存储取代Redis，并使用持久对象进行房间状态管理。这些架构选择可以在正确实现Matrix的核心联合和安全特性的情况下工作。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;对于考虑使用AI辅助开发的开发者来说，这一事件凸显了未经审查的AI输出的风险。夸大AI生成实现的模式已成为技术博客中反复出现的问题，引发了对基础设施公司审查流程的质疑。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Hodgson总结说，Matrix基金会依靠会员费来资助规范工作、信任和安全工具以及生态系统支持。虽然组织成员在过去的一年里翻了一番，但基金会的财务状况还无法维持下去。他表示希望像Cloudflare这样受益于Matrix的公司可以考虑加入为会员。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Cloudflare的&lt;a href=&quot;https://blog.cloudflare.com/serverless-matrix-homeserver-workers/&quot;&gt;博客文章&lt;/a&gt;&quot;和更新后的免责声明仍然有效。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;原文链接：&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2026/02/cloudflare-matrix-homeserver-ai/&quot;&gt;https://www.infoq.com/news/2026/02/cloudflare-matrix-homeserver-ai/&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/qspJxiaVYpj6HOQVGWpO</link><guid isPermaLink="false">https://www.infoq.cn/article/qspJxiaVYpj6HOQVGWpO</guid><pubDate>Thu, 05 Feb 2026 08:52:11 GMT</pubDate><author>作者：Michael Redlich</author><category>AI&amp;大模型</category><category>云计算</category></item><item><title>9B 模型“平替”GPT-4o ？！面壁赌对OpenClaw端侧AI，内部上演一人月产65万行代码的效率核爆</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2023 年，在百模大战正激烈的时候，面壁智能突然转向端侧大模型，这一战略决策受到了外界不少质疑，直到次年苹果的入局才让市场相信他们的判断。3 年后，面壁的打法和认知更为坚定和清晰，并火力全开：发布首个可以“即时自由对话”的大模型、年中发布首款 AI 硬件松果派（Pinea Pi）以支持硬件场景的全栈开发。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;首次手搓全双工全模态模型&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2 月 4 日，面壁正式发布并开源了新一代全模态旗舰模型 MiniCPM-o 4.5。作为原生全双工的全模态大模型，MiniCPM-o 4.5 新引入了一种端到端的“边看、边听、主动说”的全模态能力：模型可以进行即时、自由的对话交互，弱化了传统对话中“一问一答”的轮次概念，而是允许模型根据语义和场景，自主判断是否发起对话。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直接看具体效果：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上述展示中模型一直在观察，且没有涉及复杂的调度&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“全模态能力是 AI 进入人类物理世界所必备的一项基础能力。这一次的全模态模型，最大的特色在于高度拟人、自然的交互方式，也就是说，看、听、说是并行发生、互不阻塞，不再采用过去那种回合制交互。这在技术上是一次非常重要的跨越，也是未来 AI 真正进入物理世界所必须具备的基本能力。”面壁智能联合创始人兼首席科学家刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;清华大学人工智能学院助理教授、面壁智能多模态首席科学家姚远，主要负责 MiniCPM-o 4.5 的研发。他介绍，该模型主要依赖两项核心创新：一是全双工机制，多模态输入和输出彼此不阻塞，模型可以持续感知外界的视频和音频流，同时进行语音或文本输出，不会因“正在说话”暂停对外界的感知；二是全模态的自主交互机制，模型会持续判断当前语义是否已经成熟，是否达到了适合触发自身输出的时机。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他坦言，目前市面上大多是将图像模型、语音模型，甚至 instruct 模型和 thinking 模型拆分为不同的模型分别训练。面壁这次尝试将所有能力统一训练到一个模型中，面临了不小的挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先就是多维度一起训练，整体难度会急剧上升，再加上端到端的多模态训练，本身就会显著增加系统负担；其次 9B 参数规模下，要在语音、全模态交互以及视觉能力等方面取得不错效果，就要对模型如何学习和吸收知识有更深入的理解，能够更精细地把握模型在不同训练阶段的学习动态，避免新引入的知识与已有能力之间产生冲突。这期间，技术团队克服了大量困难。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，团队能够在多模态训练过程中较好地保持文本能力，instruct 能力没有明显损失，甚至实现小幅提升。此外，模型通过更低的显存占用、更快的响应速度，确保在提供 SOTA 级全模态表现的同时，实现了最佳的推理效率和最低的推理开销。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/2d/2dbea1b298b51c3eb018d860fde40cc2.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Github：&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;https://github.com/OpenBMB/MiniCPM-o&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;Hugging Face: &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-4_5&quot;&gt;https://huggingface.co/openbmb/MiniCPM-o-4_5&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;体验链接：&lt;a href=&quot;https://minicpm-omni.openbmb.cn/&quot;&gt;https://minicpm-omni.openbmb.cn/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前，模型记忆大概在一分钟左右，也模型推理的最佳“舒适区”。姚远表示，Infra 层虽然可以支持更长时间的训练和推理，但如果模型未来要承担更长期、甚至接近全天候陪伴式的使用形态，就必然要在方法和机制上做更多创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;端侧对低延时要求非常高。这次，模型侧的低延迟优化主要来自两点：首先，在全双工状态下，模型不再依赖外部的微型工具或小模型来判断“什么时候开始推理”，传统逻辑里需要固定等待的时间被去掉，模型可以直接基于语义判断无缝生成回应。其次，现在不少方案会把语音 token 直接放进一个大模型里统一生成，这会带来非常沉重的计算开销。面壁技术团队采用的方式是，一个大的主干模型加一个轻量级语音生成模块，在保证效率的同时，两者通过稠密的隐藏层连接，把主 token 与各个头部 token 紧密关联起来，因此实现控制能力不受影响。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/8c/8c959c6e46e149e56005720b782ed55b.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而使用侧的系统层面，则依赖于高效的推理框架 llama.cpp-omni 和低延迟的交互系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;姚远指出，多模态数据本身并不是最大的问题。预训练阶段“数据燃料耗尽”主要指文本数据；而在多模态领域，当前的挖掘程度远远不够，甚至都还没有真正找到一种非常有效的方式系统利用这些数据。而全双工、全模态的自主交互机制，可能正是未来新的学习与增长方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当前，如何在不牺牲单任务性能的前提下，实现统一建模、高效泛化以及理解生成一体化，是当前业内积极探索的研究方向，如今面壁也迈出了自己的关键一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;让开发者回答，AI 硬件该是什么样&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;端侧领域，除了开发端原生的模型，与芯片厂商的合作也越来越重要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;一方面，芯片厂商非常希望从前沿端侧模型的公司，获取未来训练模型的规划和展望，这有助于设计新的芯片；另一方面，模型公司在设计和训练新模型时，也希望能够提前了解芯片的特性，说明需要的算子类型和架构特点，以确保训练出的模型在这些芯片上运行时效率最高。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁如今就在成为连接芯片厂商和终端厂商的重要媒介，而且还要连接更多的开发者：今年面壁发力的重点之一便是开发者生态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;25 年上半年，面壁投资人在深圳调研发现，在深圳做 AI 硬件的项目，凡涉及端侧模型的，超过一半以上都在使用 MiniCPM。这是面壁今年开始建设开发者生态、提供硬件的根本原因。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁智能联合创始人兼 COO 雷升涛解释，单纯依靠商业化，把 MiniCPM 部署到数百亿台设备上会比较困难，而通过生态建设可以让开发者一起参与推动。生态的优势在于自然生长，只要有好的基础，它就会衍生出许多依赖性的、难以想象的应用。对于“应该能开发出哪些硬件”的问题，面壁没有设定特别明确的规划或期待，而是把答案留给了开发者。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁践行这一策略的首个举措就是发布松果派：一款 AI 原生 (AI Native) 的端侧智能开发板。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这背后的逻辑是：推广语言模型相对容易，但当模态增加、要在设备上运行、进行微调、完成对齐后再开发应用，难度就显著提升，这部分难度需要依靠工具和软硬件来解决，承载这部分功能的就是松果派。未来面壁模型发布时，就会针对指定硬件进行优化，减少用户在适配上消耗的大量精力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/d3/d3538ae32ea86628ef74500193f3ef86.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;松果派构建了一套软硬一体、全栈覆盖的端侧 AI 软件体系。其基于 NVIDIA Jetson 系列模组打造，内置麦克风、摄像头、丰富的接口等多模态硬件组件，以便开发者高效开发和调用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;松果派计划在年中正式量产上市，但它今年不会承担面壁特别强的商业化诉求，更多是承担市场教育作用：让更多的人能更快体验模型能力，并在各类场景中应用起来。打通端侧模型到应用的最后一公里硬件、实现对用户痛点的覆盖，就是面壁今年的目标。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁目前并未透露具体价格，但肯定地表示不会以盈利为主要目的。最初版本选择了在全球范围内相对成熟的方案，接下来会陆续推出相应的国产化版本以及不同算力的版本，并根据开发者反馈进行规划和调整。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这次松果派的硬件本身是由合作伙伴完全设计，面壁主要将其整合应用。面壁智能联合创始人兼 CEO 李大海强调，面壁最重要的是做端侧原生，聚焦端侧模型研发。“端侧模型的商业化落地，本身既是对我们模型能力的验证，也是为端侧模型建立数据飞轮，形成完整的闭环。从核心来看，我们的工作一直很专注。在过去，虽然出现了许多看似有吸引力的机会，但我们始终坚持取舍，最终选择聚焦在端侧模型这件事情上。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;如何从各种竞争中突围？&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁的核心理念是大模型“知识密度定律（Densing Law）”，即大模型的知识密度大约每 100 天提升一倍。这引发了一个重要推论：大模型的保鲜期非常短。换句话说，任何一家大模型公司都必须持续不断地推出优秀的大模型。回顾国内外所有模型厂商，没有任何例外。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“如果一个厂商只能在某一个时间点推出一个模型，那么它实际上无法在行业前沿持续存在；半年、一年之后，用户很可能就会忘记这个模型。因此，关键不在于推出单一优秀模型，而在于能够持续不断地推出优秀模型。”李大海说道，“面壁的目标是打造一个能够持续训练出高知识密度大模型的系统。这才是我们认为最重要的产品、技术层面的核心。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷升涛补充道，在模型之外，把底层的 Infra 模型跑到极致也是延长模型领先时间的关键，毕竟端侧的算力很小、内存有限，各种约束都非常严苛，要做好是非常困难的。另外，产品化能力也非常关键。现在单靠模型领先已经无法持续保持竞争优势，需要通过底层基础设施、产品设计、品牌建设以及模型能力的结合，来更大程度地延长模型的“保鲜期”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;虽然面壁正在同步将商业优势、生态优势、品牌优势等单一优势转化为综合性优势，但作为创业公司，如何避免被大厂围剿仍是一个现实问题，李大海对此较为乐观。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他解释道，大厂不会放弃通用且规模巨大的市场，因此竞争激烈。相比之下，端侧是另一个重要方向。“端侧包含非常多不同的终端，每个终端面向的应用场景各不相同，因此它不是一个统一的市场，创业公司有更多机会去切入不同细分领域，而不需要像大厂那样争夺整个市场。”背后的逻辑是：端侧市场分散且长尾，同时存在许多高价值的应用场景，这正是创业公司在初期更适合重点攻克的领域。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，终端本身就是高度差异化的，涵盖了各种各样的类型。刘知远强调，面壁关注的是终端发展的核心需求：高效，即用尽可能少的参数实现尽可能强的能力。“从商业角度来看，面壁不会去和很多厂商打阵地战，这种做法在创业阶段并不聪明。这是一个蓝海市场，没有必要在这方面过多纠结。”&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;李大海也补充称，即使是在同一个领域内，要解决的客户或用户问题也是非常多样化的。同一个领域并不意味着大家一定是你死我活的竞争关系。尤其端侧市场，覆盖了非常多应用场景，能够容纳很多创业公司，让大家都有良好的发展空间。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;内部的“一人公司”趋势&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;另外，一个值得关注的现象是，面壁内部也逐渐出现了“one person company（一人公司）”趋势。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁内部过去十个月一直在推动全公司的 AI 原生计划。不到两百人的团队，在十个月内写了 2000 万行代码。如果按传统方式手写，这些大概需要 700 人才能完成。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其中，团队中最核心、最重度投入的一位员工，一个月就写了 65 万行代码，他把核心系统接入 AI，并重构了一遍。“未来的企业，尤其是 AI 企业，一定会是高度 AI 赋能的，也就是我们所说的 AI Native 模式。”刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;小团队甚至个人都可以完成过去需要团队数月才能完成的工作，这是一个非常明显的发展趋势。面壁目前就在朝这个方向发展，这种模式和以往的大公司有很大的不同。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;雷升涛解释，面壁内部对“AI Native”的定义包括两个方面：第一，接到任务后，第一反应是能否用 AI 来完成；第二，如果任务原本人来完成的，那么用 AI 完成后，能否做得更好。他表示，AI 已经渗透到面壁业务的各个层面，它不仅被广泛使用，还深刻地影响了大家的思维方式、工作模式乃至协作方式。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这也反映在了面壁招人的具体要求中。李大海表示，面壁一直希望能够吸引 AI 原生的人才，即在思考和解决任何问题时，都能够将 AI 能力视作自身的内在工具去应用。这背后反映的，是人才是否具备发现问题和提出问题的能力，这一点在如今时代尤为重要。同时，他们还需要能够利用 AI 快速解决问题，并具备足够强的判断能力，去评估工具产出的结果是否高质量。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;“一个公司的核心竞争力，很大程度上取决于人才的密度和质量。换句话说，所谓 AI 原生，不只是态度上愿意使用 AI，更重要的是通过这个过程展现出个人的综合能力。”李大海说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h3&gt;走向 AGI 的两条发展主线&lt;/h3&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于未来端侧智能的发展，面壁形成了一个明确判断：端侧与云端的协同，将成为未来长期存在的主流形态。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;无论是豆包手机、具身智能，还是引发广泛关注的 OpenClaw，这些爆火的案例都在验证一个趋势：智能终端正在成为大模型能力向用户延伸的重要载体。刘知远认为，这些探索共同指向一个核心愿景：大模型将越来越贴近用户。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;但从现实情况来看，大部分产品目前仍主要依赖云端模型运行，由此带来了反馈延迟、隐私保护和安全性等一系列问题。因此，这一方向虽然正确但还不成熟，它只是这场大戏的序幕，甚至连序幕的开端可能都刚刚开始。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;面壁判断，随着模型逐步进入物理世界，尤其是在对实时性要求极高的任务中，端侧模型将不可或缺。在本地即时处理大量数据、快速做出响应，是端侧模型的核心价值所在，这也是端云协同中，端侧不可替代的意义。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;从具体终端形态来看，李大海指出，手机在大模型应用上仍有巨大的拓展空间。目前的探索更多集中在“输出”侧能力，但同样重要的还有“输入”侧。如果手机能够直接感知并理解现实环境，就可以更自然地与用户共享上下文，实现更贴近人类认知方式的交互。但这也意味着更高的技术与工程挑战：在资源受限的终端上实现复杂感知与理解能力，需要更长时间的打磨与更精细的系统优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而在另一个同样火热的具身智能领域，行业面临的核心挑战依然是模型的通用性与泛化能力，即能否让同一模型稳定运行在不同类型的本体之上。多模态大模型被普遍视为突破这一瓶颈的关键，为跨场景、跨本体的适应能力提供基础支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在刘知远看来，多模态乃至全模态能力，正是未来多智能体体系的基础。未来将存在大量分布在不同环境中的智能终端，每个终端的感知条件、背景信息各不相同，正是这种差异性，使得终端之间的协同成为必然选择。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他解释道，从结构上看，一个智能体至少可以抽象为三个核心要素：输入 x、输出 y 和模型 m。输入天然是全模态的，人类正是通过多模态感知世界；模型负责思考、推理与决策；输出则作用于物理世界，完成各种具体行为。未来智能体能力的演进，正是围绕这三个要素不断强化与耦合，最终实现真正面向物理世界的智能行动。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在更宏观的层面，刘知远将通用人工智能的发展总结为两条主线：一是智能能力持续增强，二是智能的实现与使用不断变得高效。面壁未来的技术突破，也将围绕这两个方向同步推进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;他进一步判断，在接下来一到两年内，模型的专业能力和与现实世界交互的能力将快速提升，作为智能体，模型将逐步具备更强的自主学习与自我成长能力；当模型能够在特定领域中自主探索与进化后，多智能体协同将成为下一阶段的重要突破，不同智能体将像人类团队一样高效协作，完成单一个体难以完成的复杂任务；更长远来看，模型还将逐步展现出创新与创造能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;与此同时，智能终端本身也将随之发生变化。“一旦终端侧模型具备自主学习与协同能力，就会形成一个关键基点：每个人都将拥有一个持续成长、越来越懂自己的大模型助手。未来三到五年，这一愿景很可能成为现实。”刘知远说道。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/LIghfWoDjXgjNME3mGHc</link><guid isPermaLink="false">https://www.infoq.cn/article/LIghfWoDjXgjNME3mGHc</guid><pubDate>Thu, 05 Feb 2026 08:31:58 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>Rspress 2.0发布：面向体验与AI的全新升级</title><description>&lt;p&gt;我们很高兴地宣布 Rspress 2.0 的正式发布！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 是基于&amp;nbsp;Rsbuild&amp;nbsp;的静态站点生成器，专为开发者打造的文档站工具。自 2023 年正式发布以来，Rspress 1.x 累计迭代&amp;nbsp;144 个版本，共有&amp;nbsp;125 位贡献者&amp;nbsp;参与项目开发。越来越多的开发者选择 Rspress，借助其高效的编译性能、约定式路由和组件库预览等功能，构建了可靠的文档站点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;根据社区的反馈和建议，Rspress 2.0 在&amp;nbsp;主题美观度、人工智能原生、文档开发体验、与 Rslib 一起使用&amp;nbsp;等方面进行了更深入的研究。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;为什么是Rspress 2.0&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 1.x 已经解决了文档站框架编译性能的问题，但仍然存在一些问题影响着作为文档开发工具的核心体验。2.0 版本将不仅仅针对编译性能的追求，也侧重于文档站体验的其他方面：&lt;/p&gt;&lt;p&gt;主题风格：一套更美观的默认主题，并提供了多种自定义主题方式，解决了 1.x 在主题定制上缺乏稳定 API 的问题；AI-native：文档不仅服务于人类读者，也需要被Agent更好地理解和使用。Rspress现在内置了&amp;nbsp;llms.txt&amp;nbsp;生成并从SSG衍生出的&amp;nbsp;SSG-MD&amp;nbsp;功能，生成高质量的Markdown渲染内容供Agent读取；双击编译，瞬间启动：默认启用&amp;nbsp;lazyCompilation，配合链接悬停时对资源的预加载功能，仅在访问特定路由时构建所需文件，无论实现项目规模大小，dev也可瞬间启动；Shiki 代码高亮：默认集成 Shiki，在构建时完成语法高亮，支持主题切换、变压器扩展，比如&amp;nbsp;@rspress/plugin-twoslash，带来更丰富的代码块展示效果；文档开发体验：优化&amp;nbsp;_nav.json、_meta.json&amp;nbsp;文件的HMR等并新增&amp;nbsp;json schema&amp;nbsp;用于IDE内的代码提示；默认开启死链检查功能；新增文件代码块语法，支持外部引用文件；@rspress/plugin-preview&amp;nbsp;和&amp;nbsp;@rspress/plugin-playground&amp;nbsp;支持同时使用等；Rslib 集成：现在可以在使用&amp;nbsp;create-rslib&amp;nbsp;创建组件库项目时，选择 Rspress 作为文档工具，快速构建组件库项目站点。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这是一次对现有架构的全面升级，下面将介绍 Rspress 2.0 及其&amp;nbsp;全新主题、高质量 llms.txt 生成、集成 Shiki、后续编译等重要功能。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/46/46a5ede7a37b7775c064081e532c339f.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;2.0 新特性&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;全新主题&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2.0默认主题令人期待的一次系统性升级，它由团队设计师&amp;nbsp;@Zovn Wei&amp;nbsp;整体设计，在视觉效果和阅读体验上都有较轻的提升，并且每个组件需要独立替换，拥有非常多的可定制性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ec/ecaa845a5fa8c39ddbf8d8e2d214ef3d.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;主题定制&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;按照定制化程度从低到高，有CSS变量、BEM类名、ESM重导出覆盖、组件弹出四种自定义主题[11]方式。&lt;/p&gt;&lt;p&gt;CSS指标：新主题涉及了更多CSS指标，覆盖主题颜色、代码块、首页等样式。您可以在&amp;nbsp;CSS指标[12]&amp;nbsp;页面进行预览并调整所有CSS指标，找到满意的配置后直接复制到项目中使用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;:root {
  /* 自定义主题色 */
  --rp-c-brand: #3451b2;
  --rp-c-brand-dark: #2e4599;
  /* 自定义代码块样式 */
  --rp-code-block-bg: #1e1e1e;
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;BEM 类名：内置组件现在均采用&amp;nbsp;BEM 命名规范。这是十分之一 Old School 的选择，但也是我们深思熟虑的决定。用户可以通过 CSS 选择器精准调整样式，HTML 结构更加清晰；同时与 Rspress 用户自身使用的 CSS 框架解耦合，用户可以任意选择 CSS 框架（Tailwind&amp;nbsp;[14]、Less&amp;nbsp;[15]、Sass&amp;nbsp;[16]&amp;nbsp;等），比如使用 Tailwind V4 或 V3而不用担心版本，也不用担心与 Rspress 内置 CSS 产生冲突。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;/* BEM 命名规范 */
.rp-[component-name]__[element-name]--[modifier-name] {
}

/* 根据 BEM 类名轻松覆盖组件样式 */
.rp-nav__title {
  height: 32px;
}
.rp-nav-menu__item--active {
  color: purple;
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;ESM重导出覆盖：如果CSS上的修改无法满足定制需求，可以通过JS进行更深度的定制。在&amp;nbsp;theme/index.tsx&amp;nbsp;中使用&amp;nbsp;ESM重导出[17]，可以覆盖任意一个Rspress的内置组件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/fd/fd59a86ad14793813bd1e21921860499.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;修改组件弹出：你可以使用全新的&amp;nbsp;`rspress pop [组件]`&amp;nbsp;命令，这个命令将指定的组件源代码复制到&amp;nbsp;theme/components/&amp;nbsp;目录下，你可以自由这些代码，甚至直接替换AI，来实现深度定制。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;# 将 DocFooter 组件导出到 theme 目录
rspress eject DocFooter&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;导航栏、侧边栏标签&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 实现了&amp;nbsp;Tag 组件[19]，现在可以使用 frontmatter 中的标签属性，在侧边栏或导航栏进行 UI 标注。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;---
tag: new, experimental # 会在 H1 和 Sidebar 进行显示
---

import { Tag } from &#39;@rspress/core/theme&#39;;

# Tag

## Common tags &lt;tag tag=&quot;new&quot;&gt; {/* 会在右侧 outline 进行显示 */}&lt;/tag&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/63/63de4b07258aa8c550ee6ad2a11d88f6.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;内置多语言支持&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 1.x 版本中，Rspress 仅内置了中文，如果使用其他语言如 zh，必须对所有的文本都进行配置，使用起来更繁琐。现在 2.0 主题内置了 zh、en、ja、ko、ru 等多种语言的翻译文本，系统会根据语言配置自动进行“Tree Shaking”，仅限你使用到的文本及语言，未内置的语言会兜底到 en文本。您也可以通过&amp;nbsp;`i18nSource`&amp;nbsp;配置项扩展或覆盖翻译文本。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 未来会支持更多内置语言，如果你有兴趣，请参考&amp;nbsp;这位贡献者的 Pull Request&amp;nbsp;[21]。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;llms.txt 支持&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress现在将&amp;nbsp;llms.txt&amp;nbsp;[22]&amp;nbsp;生成能力集成到core中，并实现了全新的SSG-MD（Static Site Generation to Markdown，静态站点Markdown生成）能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在基于 React 动态渲染的前端框架中，往往存在静态信息无法提取的问题，Rspress 也面临同样的挑战。Rspress 用户通过&amp;nbsp;MDX 片段[23]、React 组件、Hooks 以及 TSX 路由等动态特性来增强表现力。但这些动态转换在 Markdown 文本内容时会面临以下问题：&lt;/p&gt;&lt;p&gt;直接将 MDX 输入给 AI 会包含大量代码噪音，并丢失 React 组件内容；将 HTML 转为 Markdown 往往效果不佳，信息质量难以保证。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了解决这个问题，Rspress 2.0引入了&amp;nbsp;SSG-MD&amp;nbsp;[24]&amp;nbsp;特性。这是一个全新的功能，它类似于&amp;nbsp;静态站点生成（SSG）[25]，但不同的地方相当于你的页面渲染为Markdown，而不是文件HTML文件，并生成&amp;nbsp;llms.txt&amp;nbsp;[26]&amp;nbsp;及llms-full.txt相关文件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/99/99c86430d85a5accd4987d3d59f70f80.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;相比于将 HTML 转化为 Markdown 等传统方式，SSG-MD 在渲染期间拥有更优质的信息源，比如 React 虚拟 DOM，从而保证更高的静态信息质量和灵活性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/c7/c75fe9b333fc0d5411342ef46be04b22.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;启用方式非常简单：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;import&amp;nbsp;{ defineConfig }&amp;nbsp;from&amp;nbsp;&#39;@rspress/core&#39;;

export&amp;nbsp;default&amp;nbsp;defineConfig({
&amp;nbsp; llms:&amp;nbsp;true,
});&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;构建后将生成如下结构：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/d5/d5197e48de9384abbf82beb6424ec8d3.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若想定制自定义组件的渲染内容，可通过环境变量控制：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/8c/8c23a957526d83d3f375ac1161b069df.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这样既保证了文档的交互体验，也能帮助AI理解组件的语义信息。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;SSG-MD使用指南[27]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Shiki 编译时代码块高亮&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 默认使用&amp;nbsp;Shiki&amp;nbsp;[28]&amp;nbsp;进行代码高亮。相比 1.x 的 prism 运行时高亮方案，Shiki 在编译时完成高亮处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持多种主题样式，比如在&amp;nbsp;CSS变量[29]&amp;nbsp;页面可以交互式切换和预览不同的Shiki主题。同时Shiki也允许使用自定义的&amp;nbsp;变压器[30]&amp;nbsp;进行扩展来丰富的写作，例如twoslash等。引入编程语言，不增加运行时间和包体积。基于 TextMate 语法实现与 VS Code 一致的准确语法高亮。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是一些 Shiki Transformer 的视觉，仔细感受 Shiki 带来的文档创意：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;使用&amp;nbsp;@rspress/plugin-twoslash&amp;nbsp;[31]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;const&amp;nbsp;hi =&amp;nbsp;&#39;Hello&#39;;
const&amp;nbsp;msg =&amp;nbsp;`${hi}, world`;
// &amp;nbsp; &amp;nbsp;^?&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;使用&amp;nbsp;transformerNotationFocus&amp;nbsp;[32]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;null&quot;&gt;console.log(&#39;Not focused&#39;);
console.log(&#39;Focused&#39;);&amp;nbsp;// [!code focus]
console.log(&#39;Not focused&#39;);&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;代码块[33]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;构建性能提升&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 底层由 Rsbuild 和 Rspack 2.0 预览版本驱动，同时默认开启了后续编译[34]&amp;nbsp;和&amp;nbsp;持久化存储[35]。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;编译&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;默认开启&amp;nbsp;dev.lazyCompilation&amp;nbsp;[36]，只有当你访问某些页面时，该页面才会被编译，大幅提升了开发速度启动，甚至实现了数十级的冷启动。Rspress 同时实现了路由的预加载策略，当鼠标暂停在链接上时会预先加载目标路由页面，搭配lazyCompilation 实现稀疏的开发体验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0beec621d783272b5fc95427665e47d9.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;持久化存储&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2.0默认同时开启了&amp;nbsp;持久化服务器[37]，在热启动中复用上次编译的结果，提升了30%-60%的构建速度。这意味着在首次运行&amp;nbsp;rspress dev&amp;nbsp;或&amp;nbsp;rspress build&amp;nbsp;之后，后续启动速度都会明显提升。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;文档开发体验&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;默认开启死链检查&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0默认开启死链检查功能。在构建过程中，会自动检测文档中的无效链接，帮助你及时发现和修复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;import { defineConfig } from &#39;@rspress/core&#39;;

export default defineConfig({
  markdown: {
    link: {
      checkDeadLinks: true, // 默认开启，可通过 false 关闭
    },
  },
});&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ce/ce9694c1c01266c390cbd24d711ac5da.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;参见&amp;nbsp;链接[38]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件代码块&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;您可以使用 file=&quot;./path/to/file&quot; 属性来引用外部文件作为代码块的内容，将示例代码放在单独的文件中维护中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;```ts file=&quot;./_demo.ts&quot;

```&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;tsx&quot;&gt;```tsx file=&quot;&lt;root&gt;/src/components/Button.tsx&quot;

```&lt;/root&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;请参阅&amp;nbsp;文件代码块[39]&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;预览 更灵活的元用法&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;@rspress/plugin-preview [40] 现在基于元属性使用，更加灵活，也可以殴打文件代码块。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;下面是一个使用 iframe 预览代码块的示例：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;tsx&quot;&gt;```tsx preview=&quot;iframe-follow&quot; file=&quot;./_demo.ts&quot;

```&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;它将会渲染为：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/42/42e2e1cd7a4b7d10604292772fb2e937.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并且&amp;nbsp;@rspress/plugin-playground&amp;nbsp;[41]&amp;nbsp;现在支持和plugin-preview一起使用，通过meta属性切换即可，例如&amp;nbsp;```tsx playground&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;支持HMR的一些配置文件&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于 Rsbuild 重新设计的&amp;nbsp;虚拟模块插件[42]，现在支持&amp;nbsp;i18n.json、_nav.json、_meta.json文件代码块以及&amp;nbsp;@rspress/plugin-preview&amp;nbsp;中 iframe 相关的 HMR。修改这些配置文件后，页面会自动热更新，无需手动刷新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Rslib 和 Rspress&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在使用&amp;nbsp;create-rslib&amp;nbsp;项目项目时，您现在可以选择 Rspress 工具。这让您能够在开发组件库的同时，快速搭建搭建的文档站点，用于编写创建组件的使用说明、展示 API 参考，或实时预览组件效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;执行&amp;nbsp;npm create rslib@latest&amp;nbsp;并选中Rspress，会生成下方的文件结构：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;├── docs
│   └── index.mdx
├── src
│   └── Button.tsx
├── package.json
├── tsconfig.json
├── rslib.config.ts
└── rspress.config.ts&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;模版中内置了&amp;nbsp;rsbuild-plugin-workspace-dev&amp;nbsp;[43]&amp;nbsp;插件，可在启动Rspress开发服务器的同时自动运行Rslib的watch命令。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;直接运行&amp;nbsp;npm run doc&amp;nbsp;启动 Rspress 的开发服务器对 Rslib 组件库进行预览：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;scripts&quot;: {
    &quot;dev&quot;: &quot;rslib build --watch&quot;,
    &quot;doc&quot;: &quot;rspress dev&quot; // 执行该命令
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;更多 Rspress 官方插件&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 新增了多个官方插件：&lt;/p&gt;&lt;p&gt;@rspress/plugin-algolia：支持替换 Rspress 的内置搜索为&amp;nbsp;Algolia DocSearch&amp;nbsp;（感谢&amp;nbsp;@algolia&amp;nbsp;团队的帮助）；@rspress/plugin-twoslash：为 TypeScript 代码块添加类型提示；@rspress/plugin-llms：为不支持 SSG 和 SSG-MD 的项目提供 llms.txt 生成能力；@rspress/plugin-sitemap：自动生成&amp;nbsp;Sitemap&amp;nbsp;文件，用于优化SEO。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;其他重大变化&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;从 Rspress 1.x 迁移&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果您是 1.x 项目的用户，我们准备了一份升级的迁移文档，帮助您从 1.x 升级到 2.0。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;你可以直接使用Pages中的“复制Markdown”功能，将其输入给你常用的编码代理（如Claude Code等）来完成迁移。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;请参考&amp;nbsp;迁移指南[51]。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;删除 mdxRs 配置&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们注意到很大一部分 1.x 用户为了使用 Shiki、组件库预览功能和自定义评论/rehype 插件，而主动关闭&amp;nbsp;mdxRs，并且在开启循环编译和持久化缓存后，即使使用 JS 版本的 mdx 解析器，性能优化效果已经非常显着。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了换取更好的扩展性和可维护性，我们决定在 Markdown/MDX 编译流程中不再使用 Rust 版本的 MDX 解析器（@rspress/mdx-rs）。这使得 Rspress 能够更好地集成 Shiki 等 JavaScript 生态的工具。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Node.js 与下游依赖版本要求&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 要求 Node.js 版本 20+，React 版本 18+。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;包名及导入路径变更&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress将&amp;nbsp;rspress、、、@rspress/runtime都&amp;nbsp;整合进了&amp;nbsp;中，项目@rspress/shared和&amp;nbsp;插件现在只需安装一个包即可。@rspress/theme-default@rspress/core@rspress/core&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;dependencies&quot;: {
-   &quot;rspress&quot;: &quot;1.x&quot;
-   &quot;@rspress/shared&quot;: &quot;1.x&quot;
+   &quot;@rspress/core&quot;: &quot;^2.0.0&quot;
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;- import { defineConfig } from &#39;rspress/config&#39;;
+ import { defineConfig } from &#39;@rspress/core&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;- import { useDark } from &#39;rspress/runtime&#39;
- import { PackageManagerTabs } from &#39;rspress/theme&#39;;
+ import { useDark } from &#39;@rspress/core/runtime&#39;
+ import { PackageManagerTabs } from &#39;@rspress/core/theme&#39;;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果你开发了 Rspress 插件，那么该插件的 peerDependency 从&amp;nbsp;rspress&amp;nbsp;更改为&amp;nbsp;@rspress/core：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;peerDependencies&quot;: {
    &quot;@rspress/core&quot;: &quot;^2.0.0&quot;
  }
}&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;下一步&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Rspress 2.0 的发布只是一个新的起点。本次发布后，Rspress 将持续迭代：&lt;/p&gt;&lt;p&gt;推进生态集成：与Rslib、Rstest更深度地结合，提供接入组件项目和库项目的标准化开发体验；探索AI与文档更复杂的集成：如智能问答、自动摘要等；完善SSG-MD决策并更加自动化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;感谢所有为 Rspress 做出贡献的开发者和用户！如果您在使用过程中遇到问题或有任何建议，欢迎在&amp;nbsp;GitHub Issues&amp;nbsp;[52]&amp;nbsp;中反馈。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;立即使用或升级到Rspress 2.0，体验全新的文档开发之旅！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;npm create rspress@latest&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;博客原文链接：https://rspress.rs/zh/blog/rspress-v2&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;参考资料&lt;/p&gt;&lt;p&gt;[1]&amp;nbsp;Rsbuild：&lt;a href=&quot;https://rsbuild.rs/&quot;&gt;https://rsbuild.rs/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[2]&amp;nbsp;自定义主题：&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[3]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[4]&amp;nbsp;SSG-MD：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[5]&amp;nbsp;懒加载编译：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspack.rs/guide/features/lazy-compilation&quot;&gt;https://rspack.rs/guide/features/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[6]&amp;nbsp;@rspress/plugin-twoslash:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[7]&amp;nbsp;json 模式：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/auto-nav-sidebar&quot;&gt;https://v2.rspress.rs/zh/guide/basic/auto-nav-sidebar&lt;/a&gt;&quot;&amp;nbsp;#json&amp;nbsp;-schema-type 提示&lt;/p&gt;&lt;p&gt;[8]&amp;nbsp;@rspress/plugin-preview:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/preview&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[9]&amp;nbsp;@rspress/plugin-playground：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspress.rs/plugin/official-plugins/playground&quot;&gt;https://rspress.rs/plugin/official-plugins/playground&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[10]&amp;nbsp;@Zovn魏：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://x.com/wei_zhong41532&quot;&gt;https://x.com/wei_zhong41532&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[11]&amp;nbsp;自定义主题：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[12]&amp;nbsp;CSS 变量：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/vars&quot;&gt;https://v2.rspress.rs/zh/ui/vars&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[13]&amp;nbsp;BEM 命名规范：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://getbem.com/&quot;&gt;https://getbem.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[14]&amp;nbsp;Tailwind：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://tailwindcss.com/&quot;&gt;https://tailwindcss.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[15]&amp;nbsp;Less：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://lesscss.org/&quot;&gt;https://lesscss.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[16]&amp;nbsp;Sass：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://sass-lang.com/&quot;&gt;https ://sass-lang.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[17]&amp;nbsp;ESM 重新导出：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/custom-theme&quot;&gt;https://v2.rspress.rs/zh/guide/basic/custom-theme&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#reexport&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[18]&amp;nbsp;rspress eject [component]:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/api/commands&quot;&gt;https://v2.rspress.rs/zh/api/commands&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#rspress&lt;/a&gt;&quot;&amp;nbsp;-eject&lt;/p&gt;&lt;p&gt;[19]&amp;nbsp;标签组件：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/layout-components/tag&quot;&gt;https://v2.rspress.rs/zh/ui/layout-components/tag&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[20]&amp;nbsp;i18nSource：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/api/config/config-basic&quot;&gt;https://v2.rspress.rs/zh/api/config/config-basic&lt;/a&gt;&quot;&amp;nbsp;#i18nsource&lt;/p&gt;&lt;p&gt;[21]&amp;nbsp;贡献者的 Pull 请求：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/web-infra-dev/rspress/pull/2827&quot;&gt;https://github.com/web-infra-dev/rspress/pull/2827&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[22]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[23]&amp;nbsp;MDX 片段：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/use-mdx/components&quot;&gt;https://v2.rspress.rs/zh/guide/use-mdx/components&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[24]&amp;nbsp;SSG-MD：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[25]&amp;nbsp;静态站点生成（SSG）：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[26]&amp;nbsp;llms.txt：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://llmstxt.org/&quot;&gt;https://llmstxt.org/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[27]&amp;nbsp;SSG-MD使用指南：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/basic/ssg-md&quot;&gt;https://v2.rspress.rs/zh/guide/basic/ssg-md&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[28]&amp;nbsp;Shiki：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://shiki.style/&quot;&gt;https://shiki.style/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[29]&amp;nbsp;CSS 变量：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/ui/vars&quot;&gt;https://v2.rspress.rs/zh/ui/vars&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[30]&amp;nbsp;变形金刚：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://shiki.style/guide/transformers&quot;&gt;https://shiki.style/guide/transformers&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[31]&amp;nbsp;@rspress/plugin-twoslash:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[32]&amp;nbsp;transformerNotationFocus：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/use-mdx/code-blocks&quot;&gt;https://v2.rspress.rs/zh/guide/use-mdx/code-blocks&lt;/a&gt;&quot;&amp;nbsp;&lt;a href=&quot;javascript:;&quot;&gt;#transformernotationfocus&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[33]&amp;nbsp;代码块：&amp;nbsp; &lt;a href=&quot;https://rspress.rs/zh/guide/use-mdx/code-blocks&quot;&gt;https:&amp;nbsp;//v2.rspress.rs/zh/guide/use-mdx/code-blocks&lt;/a&gt;&quot;&amp;nbsp;#shiki&amp;nbsp;-transformers&lt;/p&gt;&lt;p&gt;[34]&amp;nbsp;编译：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rspack.rs/zh/guide/features/lazy-compilation&quot;&gt;https://rspack.rs/zh/guide/features/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[35]&amp;nbsp;持久化服务器：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/performance/build-cache&quot;&gt;https://rsbuild.rs/zh/config/performance/build-cache&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[36]&amp;nbsp;dev.lazyCompilation：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/dev/lazy-compilation&quot;&gt;https://rsbuild.rs/zh/config/dev/lazy-compilation&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[37]&amp;nbsp;持久化服务器：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://rsbuild.rs/zh/config/performance/build-cache&quot;&gt;https://rsbuild.rs/zh/config/performance/build-cache&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[38]&amp;nbsp;链接：&amp;nbsp;&amp;nbsp;https ://v2.rspress.rs/zh/guide/use-mdx/link&lt;/p&gt;&lt;p&gt;[39]&amp;nbsp;文件代码块：&amp;nbsp; https:&amp;nbsp;//v2.rspress.rs/zh/guide/use-mdx/code-blocks&amp;nbsp;#file&amp;nbsp;-code-block&lt;/p&gt;&lt;p&gt;[40]&amp;nbsp;@rspress/plugin-preview:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/preview&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/preview&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[41]&amp;nbsp;@rspress/plugin-playground:&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/playground&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/playground&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[42]&amp;nbsp;虚拟插件模块：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/rstackjs/rsbuild-plugin-virtual-module&quot;&gt;https://github.com/rstackjs/rsbuild-plugin-virtual-module&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[43]&amp;nbsp;rsbuild-plugin-workspace-dev：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/rstackjs/rsbuild-plugin-workspace-dev&quot;&gt;https://github.com/rstackjs/rsbuild-plugin-workspace-dev&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[44]&amp;nbsp;@rspress/plugin-algolia：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/algolia&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/algolia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[45]&amp;nbsp;Algolia DocSearch：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://docsearch.algolia.com/&quot;&gt;https://docsearch.algolia.com/&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[46]&amp;nbsp;@algolia：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://x.com/algolia&quot;&gt;https://x.com/algolia&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[47]&amp;nbsp;@rspress/plugin-twoslash：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/twoslash&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[48]&amp;nbsp;@rspress/plugin-llms：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/llms&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/llms&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[49]&amp;nbsp;@rspress/plugin-sitemap：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/plugin/official-plugins/sitemap&quot;&gt;https://v2.rspress.rs/zh/plugin/official-plugins/sitemap&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[50]&amp;nbsp;网站地图：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.sitemaps.org/&quot;&gt;https://www.sitemaps.org&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[51]&amp;nbsp;迁移指南：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://v2.rspress.rs/zh/guide/migration/rspress-1-x&quot;&gt;https://v2.rspress.rs/zh/guide/migration/rspress-1-x&lt;/a&gt;&quot;&lt;/p&gt;&lt;p&gt;[52]&amp;nbsp;GitHub Issues：&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/web-infra-dev/rspress/issues&quot;&gt;https://github.com/web-infra-dev/rspress/issues&lt;/a&gt;&quot;&lt;/p&gt;</description><link>https://www.infoq.cn/article/0dTBmTp4SlBVvoaybNCh</link><guid isPermaLink="false">https://www.infoq.cn/article/0dTBmTp4SlBVvoaybNCh</guid><pubDate>Thu, 05 Feb 2026 08:31:48 GMT</pubDate><author>字节跳动Web基础设施-Rstack团队</author><category>AI&amp;大模型</category></item><item><title>AI 驱动的大数据自治：TCInsight 智能应对复杂运维挑战</title><description>&lt;p&gt;在大数据平台高速发展的当下，生态扩张与业务量激增，致使大数据分布式组件问题愈发棘手，传统专家运维模式捉襟见肘。以腾讯大数据庞大的规模为例，面对海量计算单元、繁杂技术栈以及千万级任务管理，借助 AI 驱动实现大数据系统的故障和问题的快速洞察与自治能力，已成为行业迫切需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在 InfoQ 举办的 QCon 全球软件开发大会（北京站）上，腾讯专家工程师熊训德做了专题演讲“&lt;a href=&quot;https://qcon.infoq.cn/2025/beijing/presentation/6408&quot;&gt;AI 驱动的大数据自治：智能应对复杂运维挑战&lt;/a&gt;&quot;”，他介绍了如何通过可拔插的决策引擎、以及数据专家自治智能体构建大数据智能管家，让企业能够理解如何高效、智能地处理复杂的运维场景，从而大幅提升大数据场景下运维效率与准确性，引领大数据线上系统迈向全面自治的实践。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下是演讲实录（经 InfoQ 进行不改变原意的编辑整理）。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大数据系统自治背景与挑战&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我简要介绍一下整个大数据系统，以及其在自治背景下的相关挑战。大数据系统本身组件众多，涵盖了从底层的 IaaS，到存储、计算框架，以及上层的工具层等多个层面。具体来说，IaaS 层面涉及到机器本身的网络和性能，而存储层则包括分布式文件系统（如 HDFS）和对象存储等。在调度方面，我们有 Kubernetes 和 Hadoop- 体系，以及针对 AI 方面的特定调度机制。再往上一层则是计算框架，例如 Spark 和 Flink 等流计算框架。最上层则是各种工具，这些工具在不同方面的使用都使得整个大数据系统的复杂性显著增加。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大数据系统本质上是一个分布式系统。如果单机系统已经如此复杂，那么分布式系统则需要考虑数据的溯源以及在不同机器上的分布情况，无论是主从结构（master 和 slave）还是多工作节点（worker）的协作模式，都会使得整个系统在处理问题、查找根源以及故障恢复时变得极为困难。此外，大数据系统的数据处理链路通常非常长。例如，数据采集可能来源于多种源头，如代理（Agent）、MySQL 数据库，或者在物联网场景下，可能是汽车或传感器等设备。采集到的数据需要通过数据接入层，目前常见的架构包括 Kafka 或其他消息队。接入后，数据会进入计算阶段，可能是实时计算（如 Flink）或离线计算（如 Spark）。计算完成后，数据需要存储到 HDFS 系统或对象存储中。最后，在数据应用层面，我们可能需要进行预处理以供 AI 使用，进行训练或推理工作，或者生成商业智能 BI 报表。因此，整个数据链路非常长，这也使得我们在进行故障根因分析或自治处理时，需要综合考虑所有相关场景。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/99/99b2da444f09f67b1c8a5dacc7bc52bf.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们处理大数据故障时，业务部门或客户往往会提出一个关键问题：“何时能够恢复？能否实现自动恢复，以尽快减少损失？”然而，我们在进行故障恢复或诊断时，高度依赖于运维 SRE 的专家经验。通常情况下，如果没有三年以上的大数据运维经验，很难有效且完善地处理复杂的大数据故障。此外，由于整个诊断和故障恢复的时间链路非常长，导致整体效率低下。更糟糕的是，故障可能已经结束，而我们只能进行事后处理，此时大数据系统可能已经遭受了实际的损失。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;大数据智能管家技术框架及关键实现路径&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;腾讯大数据智能管家 TCInsight 技术架构&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于这些背景，我们团队在大约五年前提出了构建大数据智能管家 TCInsight 的想法，致力于解决大数据系统自治相关的工作。我们的大数据智能管家整体技术架构分为三层。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一层是观测层。它主要负责监控基础设施即服务（IaaS），包括主机网络等的监控数据，同时采集日志和关键事件。我们还将大数据组件，如 HDFS、Spark、Hive 和 YARN 等的关键监控日志事件进行统一上报。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二层是服务分析层，主要负责数据实时处理和算法决策洞察。服务分析层分为三个部分。第一部分是实时分析，主要目的是快速处理数据，包括异常收敛。例如，当事件或告警过多时，我们需要迅速整合，否则会给运维 SRE 或研发人员带来较大挑战。我们会对数据进行基础预处理。第二部分是离线服务，主要用于根因分析或自治服务时的离线分析和定时巡检。在数据量较大时，离线分析尤为重要。第三部分是算法决策，主要涉及模型和算法库的分析，以及知识库和评测库的建设，还包括离线训练等工作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三层是应用层，主要负责大数据运维自治，并对外提供接口。应用层分为两大块：自治修复和自治决策。例如，以 Hive 为例，当业务侧编写了一个 SQL 查询，可能会导致 HDFS 存储空间被占满，从而影响其他任务的提交。此时，我们需要快速对该 SQL 进行限制，或者在业务非常关键且不能直接终止的情况下，预测可能得存储和计算量，进行自助弹性伸缩。此外，我们还需要进行冷热数据分离，以实现成本分析和自助转冷操作。在自治决策方面，我们需要判断是否进行参数调优，因为某些参数调整可能需要重启系统才能生效，这可能会扩大故障范围。此时，我们需要做出关键决策，例如选择扩容，或者让 AI 参与具体工作。我们还可以进行错峰执行，例如在 YARN 的多个队列中，调整队列的执行时间，以优化资源分配。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;应用层还包括业务洞察部分，主要用于预测分析、成本分析和根因分析等工作。这些工作相对滞后，我们的目标是先恢复系统，然后再进行深入分析。此外，我们还会生成巡检报表，并进行一键健康评估。健康评估在我们的系统中非常重要，它综合评估了 IaaS、存储、调度和计算等各个部分的健康状况，为关键自治决策提供依据。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在架构的中间部分是我们的算法或引擎层。引擎分为两部分：规则引擎和我们自主研发的元启引擎。元启引擎结合了 AI 算法和我们内部的混元大模型。规则引擎主要用于执行明确的操作，例如扩容，以缓解问题。对于复杂或关联性较高的场景，我们会接入算法或大模型，以提升系统的健康状况。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来，我会详细说明我们在大数据智能管家过程中的一些关键思考和实现能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/c4/c49c7269e3b1c78cf4c9f4601412820c.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;分层的大数据运维框架 - 渐进式自治&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;由于大数据体系的复杂性，TCInsight 实现自治的是一个渐进式的过程。当我们接手一个系统时，不能期望所有大数据运维工作能够立即实现完全自治。实际上，我们基于一个较为普遍的理念：在没有一线专家或专业人才的情况下，一线人员或客户也能够实现自治处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们根据问题的复杂程度进行分类处理：对于简单重复且解决方案确定问题，我们直接采用 AI 驱动的方式进行处理。目前，这类问题大约占我们总问题的 10% 左右。然而，剩下的 90% 问题尚未能完全实现自治。对于这部分问题，我们希望通过售后体系中的专项人员和 SRE 的共同努力，借助我们之前提到的平台层，利用大模型和 AI 增强能力，持续为系统提供支持。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此基础上，我们期望通过三年以上经验的产研人员或 SRE 专家，进一步强化知识库和工具建设。通过这种逐步积累和优化我们的产品能力，我们希望能够逐步提高自治的比例，最终使其达到 90% 以上。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;多智能决策引擎思考和设计一问题域&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在业界，主要有三种常见的方法：显式编程、基于优化方法的处理以及专家系统。第一种显式编程对于研发人员来说并不陌生，它本质上是通过编写规则或工作流来构建一个简单的规则引擎，从而实现直接的决策。例如，当存储使用率超过 75% 时，系统自动触发扩容操作。这种方法简单直接，但灵活性有限。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二种是基于优化方法的处理。在大模型尚未普及的时代，我们通过优化模型来提升系统性能。例如，原本只能优化 40% 的系统，通过采用贪婪算法或聚合模型等技术，可以将其优化效果提升至 80% 以上。这种方法更多地依赖于深度学习和大模型的强大能力，能够更好地处理复杂的优化问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三种是智能全自治域系统。全自治域系统的核心在于利用专家的经验和知识，尽管专家人数有限，但他们的经验可以通过系统化的方式赋予平台更强的能力。专家系统的关键在于如何将专家的经验转化为可操作的决策逻辑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在明确了这些决策引擎的技术路径后，我们进一步思考了在大数据领域构建智能决策系统的关键问题。首先，数据的可用性至关重要。无论是基于 AI 的训练还是大模型的应用，数据标注的准确性和完整性是基础。如果数据标注不足，可能会导致模型出现幻读甚至错误的输出，从而影响决策的准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，系统的可解释性也是一个关键问题。专家和文档作者需要确保知识库中的内容不仅系统能够理解，而且一线人员和客户也能够轻松掌握。这一点直接关系到决策的准确性和适用范围。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，实时性要求也不容忽视。我们的目标是先快速恢复系统，后续再进行深入分析。这就要求决策过程和最终的行动必须足够迅速，以满足实时性的需求。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;综合考虑以上因素，在决策引擎的选择上，我们决定结合规则引擎和专家系统的智能决策引擎共同构建了全自治域系统 TCInsight。这种方法既能够利用规则的明确性和可操作性，又能借助专家系统的灵活性和经验优势，逐步提升系统的自治能力和决策准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;Al 驱动的规则引擎自治系统&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在构建基于规则引擎的知识系统时，我们首先对系统中的各类数据进行了统一管理。这些数据包括指标（metrics）、日志（log）以及事件（event），我们会将它们统一上报至我们内部构建的数据库适配系统。该系统是基于 Inpara 和 Flink 构建的，数据最终会被存储到时序数据库中。随后，我们利用 Flink 对数据进行预处理，并结合训练好的模型以及特征库，对数据进行特征分析。基于这些分析，我们会进行基础的异常检测、关联分析以及趋势预测等工作，从而形成初步的告警摘要和预测摘要。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;例如，我们可能会收到告警信息，提示 HDFS 存储空间即将用尽，或者 YARN 队列的等待时间过长，又或者 StarRocks 或 Trino 的 CPU 占用率过高，某个 SQL 查询扫描的数据量过大，超出了设定的阈值。基于这些信息，我们会生成整体的告警或预测摘要。如果预测显示 HDFS 的增长趋势过快，可能会在 5 分钟内被填满，我们就会对 IaaS、存储、引擎和调度等各个层面进行评估，计算它们的健康分数。如果健康分数低于某个阈值，或者即将达到该阈值，我们就会启动规则引擎进行处理。例如，我们可能会尝试简单的扩容操作来缓解问题，或者在业务允许的情况下，直接终止一些不关键的 SQL 查询或任务，以减少资源占用。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在执行这些操作后，我们会制定一个详细的执行计划。以扩容为例，在执行扩容操作之前，我们需要先检查 HDFS 的整体状态是否正常，数据是否均衡分布，以及 NameNode 和 DataNode 之间的流量是否稳定。因为如果流量过大，可能会导致 DataNode 负载过高，甚至引发更严重的问题。只有在确认一切正常后，我们才会通过 IaaS 层扩容机器，并在扩容完成后进行数据均衡操作，以确保系统恢复正常。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;完成这些操作后，我们会记录整个过程的状态，并进行反馈。如果扩容后监控数据显示系统恢复正常，那么我们认为这次自治决策是成功的，并将结果记录下来作为后续处理的参考。然而，如果扩容后情况反而恶化，例如数据倾斜导致 SQL 查询速度变慢，引擎侧的健康分数急剧下降，那么我们会紧急通知专家介入，重新审查整个分析过程。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这种基于规则引擎的处理方式具有高效和准确的特点。目前，在我们系统中，基础指标的覆盖率达到 90%，存储场景的覆盖率为 50%，任务场景的覆盖率为 30%。在周期性任务的处理上，我们已经能够覆盖 90% 的场景。在异常诊断方面，我们能够处理 70% 的异常场景，整体数据表现良好。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这并不意味着我们的工作已经完成。实际上，大数据系统的复杂性远超我们的预期。例如，我们在两年前曾遇到一个问题：在对 HDFS 进行扩容后，发现数据分布不均衡，导致 Spark 任务的执行速度反而变慢。从常理来看，扩容后资源增加，任务执行速度应该加快，但实际上并非如此。原因在于扩容后数据的均衡性并没有达到预期，同时业务侧提交了大量任务，导致系统整体性能下降。这说明我们目前只能处理已知的情况，而对于一些未考虑到的复杂场景，我们还需要进一步优化和改进。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e9/e9b16944fe400894c20ad936bdc5a018.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;Al 驱动的全自治域系统&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;基于上述思考，我们提出了一个全新的全自治系统概念。与之前的方法不同，我们在决策过程中引入了大模型的相关分析。无论是当前备受关注的 DeepSeek，还是此前我们接触过的其他类似模型，其核心优势在于执行步骤和推理能力。因此，我们开始尝试将大模型的相关功能融入整个自治决策系统中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在预测和分析阶段，系统仍然会进行数据预处理和特征分析，并开展异常检测、关联分析以及趋势预测等工作。这些信息汇总后，会生成初步的概述信息。然而，与以往不同的是，由于引入了大模型，我们需要构建一个“优先级与目标系统”（以下简称“目标系统”）。我们会在这个目标系统中预先定义优先级和目标。例如，对于存储系统，我们设定存储使用率不得超过 80%，并且数据不能快速转冷；对于引擎，我们希望优化其执行时间；对于上层应用，我们要求其不能出现错误。这些优先级和目标会被配置到目标系统中，生成诊断建议。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随后，我们会将这些数据输入到混元模型中，并结合我们之前的决策分析结果，生成具体的执行步骤。这些执行步骤融合了传统执行引擎、规则引擎以及传统深度学习算法或基础算法的执行计划。执行计划生成后，我们会重新预检测系统状态，重新评估预测分析结果以及执行计划可能带来的状态变化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果发现执行该计划后系统健康分数可能更低，即情况可能恶化，那么我们的专家团队会介入。我们会创建一个专家工单，让专家对执行计划进行评估，并决定是否停止执行。相反，如果预测和状态评估显示执行计划后系统健康分数将高于目标值，那么我们会执行该计划，并将执行计划标记后存入知识库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;执行完成后，我们会继续进行预测分析、异常检测以及整体状态评估。如果系统健康度如我们预测的那样有所提升，我们会重新进行标记和分析，以便系统能够继续执行后续操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/37/37cbef48d48a61796cce23baa44c0a45.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;数据质量对预测影响 &amp;amp; 优化&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在构建整个系统的过程中，我们花费了大量时间进行调试，尤其是在系统上线试运行阶段。现在，我想重点介绍一下我们在调试过程中采取的关键措施，这些措施让系统更加稳定，并显著提高了预测的准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;对于从事时序预测研究的人员来说，一个常见的问题是如何处理上报数据中的断点。这种情况可能由多种原因引起。例如，当系统发生故障时，机器的 CPU 或内存可能已经满负荷运行，导致在关键时刻数据丢失。在分布式系统中，这种数据丢失可能会引发上层系统的乱序操作。假设我们上报的时间是 12 点整，但由于长时间的内存不足（OOM）或 CPU 负载过高，数据可能直到 12 点零 5 秒甚至 12 点零 1 分才上报。然而，故障的实际发生时间并非 12 点零 1 分，但上报时间却显示为 12 点零 1 分，这就导致了数据的乱序问题。此外，还可能出现重复上报的情况，即同一条日志或指标连续上报多次，这使得我们难以确定真正的时间点或事件。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这些问题引发了几个关键的挑战。首先，当数据出现断点时，我们需要决定是否进行插值。目前业界常用的算法包括直接丢弃数据或采用简单的插值方法。对于故障场景来说，直接丢弃数据可能并不是一个好方法，因为这些数据代表了当时关键的监控指标。即使进行插值，如果处理不当，也可能导致数据不准确。此外，如果数据质量不佳，将严重影响我们的预测能力和关键异常处理能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们重点对数据质量进行了优化，主要从三个方面入手。首先，我们对时序指标或日志的有效性进行评估。以往最简单的评估方式是检查数据是否超过完整性阈值。另一种常见的做法是检查数据是否满足差分阈值，或者在 IoT、时序场景中直接进行简单的拼凑。我们提出了一种基于完整性的实际评估方法。具体来说，我们将每个数据进行分段处理，然后基于自回归模型对每个分段进行评估检测。如果数据通过了自回归分析的评估，我们认为这些数据是可用的。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在确认数据可用之后，我们面临的另一个问题是数据的补齐和连接。目前常用的方法包括直接进行差分或简单的拼接。我们的思路是采用自回归预测和自回归拼接的方法。这种方法的优势在于处理速度快，能够快速对分段数据进行处理。此外，这种方法既能进行预测，又能完成数据合并操作。通过这种方法，我们显著提升了数据的有效性，整体提升了 10%。在周期性任务和异常诊断方面，准确性提高了 30% 以上。同时，时序预测的时间也缩短了 28%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/e4/e40cc5e6cf9311b60dd770c345a02121.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们在构建大数据专家库智能体的过程中，尝试了一种与业界常见的做法略有不同的方案。我们不仅实现了向量检索，还引入了文本检索。这种设计的选择源于我们在构建知识库时对传统向量检索方法的深入思考。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;传统向量检索在相关性分析方面表现出色，例如在使用 FastText 等工具时，能够快速识别出与查询相关的数据。然而，这种方法存在一个明显的局限性：它无法直接反映召回数据的质量，也就是说，在检索过程中，我们难以预估数据的相关性是否真正符合需求。为了解决这一问题，我们引入了文本检索机制。通过文本检索，我们能够更清晰地理解数据之间的关联性，尤其是在知识库的构建过程中。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;当我们构建知识库时，一个常见的思路是将操作步骤进行分层。以扩容操作为例，它可能与存储层有很强的相关性，但这种相关性背后的原因并不明确。通过文本检索，我们可以补充这些缺失的上下文信息，从而更全面地理解数据之间的关系。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;大数据系统通常分为多层，包括大数据存储层、调度、和引擎等等。这些层之间的相关性可能很强，但它们之间的索引空间检索范围并不像我们想象的那么大。基于这些考虑，我们采用了腾讯的 ES 的架构，结合文本分析和向量检索的优势。这种架构不仅支持大规模的读写操作，还具备高效的检索能力。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们能够更好地处理组件之间或分层之间的关联关系，使得各部分之间的距离更近，从而提高系统的整体效率。在故障恢复之后，除了通过冷启动将知识库连接起来，我们还利用工单系统、客户反馈和专家系统，结合混元大模型，实现自动化的分类和归纳，持续完善知识库的建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/20/2012303ea01e14f23ff450882abd4123.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;实践效果与案例分享&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI 驱动的 HDFS 存储规则引擎自治&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们来看基于 HDFS 存储规则引擎的自治。这里的关键在于如何快速抽取和分析 HDFS 的 FSImage，以及如何准确把握特征点。我们知道，HDFS 的源数据是以树形结构存储的，而现有的工具无法对这种树形结构进行并行化处理。为了解决这个问题，我们将工作拆分为两部分：第一部分是直接分析源数据的表结构，这样就不需要处理整个树形结构；第二部分是将树形结构手动拆分为多个并行部分，从而实现并行化处理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;通过这种方式，我们能够对表分区和关联分区进行拆分，并进行关联分析。同时，我们还能观察到数据的整体冷热分布，以及后续一段时间内的增长趋势。基于这些信息，我们利用规则引擎做出决策，确定关键目标。例如，如果当前存储的健康状况良好，但成本健康分较低，我们可能会自动执行降冷操作。如果发现整个系统的扩容必要性较高，我们可能会进行柔性扩容或自动剔除操作。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/21/219606253c9bf580fec387a7a7967749.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h5&gt;AI 驱动的 SparkSql 调优全自治域&lt;/h5&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;接下来分享一个关于 Spark 自动调优的案例。这个想法最初是在项目立项时提出的，当时的想法非常直接：将 Spark 的所有相关信息，包括 SparkSQL、配置信息、上下文信息，以及存储和引擎等，全部整合到一个系统中。我们甚至将所有的 Executor、逻辑计划和物理计划等也纳入其中。初步测试结果显示，这种方法的准确率大约为 30%。然而，我们发现其中约 30% 的结果与实际需求并无相关性，还有 20% 到 40% 的结果存在明显问题。究其原因，通用的大模型缺乏专家级的领域知识，这导致了准确性的不足，同时还出现了幻觉问题。所以我们引入了贝叶斯和 RL 专家系统建议的优化提升 sparksql 的调优效果。在 POC 和线上，目前实现无人工值守自治调优性能效果比工作五年经验还好 10%。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/45/4508283e31d0db76ecaa58638489378d.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在降本效果相当不错，之前主要关注的 SparkSQL 本身，没有考虑存储和 IaaS 层面的相关影响。在最近我们又升级了这个系统，会将 YARN 调度、HDFS 存储以及相关的管控日志等信息统一汇总，形成一个详细的概述。我们的目标是通过调优实现时间消耗的最优化。为此，我们将这些上下文信息输入模型，并进行在线分析。分析结果不仅包括计算相关的最优参数，还涵盖了调度配置、内核参数的配置下发等。然而，这些配置下发后并不能立即生效，可能需要执行 SQL 控制操作，或者在某些情况下，进行刷新操作。基于这些分析结果，我们会生成一个调参执行计划，然后重新提交任务，并对时间消耗的最优化和系统的整体健康度进行评估。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/1a/1a4e2bd9bb950cf2f236280291aa7a88.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;后续发展和思考&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目前我们在自治虽然有些突破，但还远远不够。正如之前提到的，我们已经解决了关键的 10% 的知识问题，这确实帮助我们解决了许多难题。然而，我们还有许多需要思考和改进的地方。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;首先，我们需要持续优化路径。以 SparkSQL 为例，虽然我们已经对 SQL 进行了优化，但关键信息之间的互联性仍然不足。例如，当我们直接将 HDFS 的最大存储容量纳入考量时，其时间和空间的关联性处理得并不理想。目前，我们主要依赖简单的专家系统来判断优化效果，而这种判断往往缺乏系统化的分析。因此，我们计划在未来持续加强这方面的建设。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，我们在决策时的目标相对单一。目前，我们的决策主要基于时间预测和健康分的调度，但对于复杂的大数据系统来说，多链路决策的完善性仍有待提高。例如，在关键决策时刻，我们会引入多智能体。目前，我们对决策准确性的把握还不够高，准确率可能只有 70% 到 80%。因此，我们需要持续优化决策过程，以提高准确率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，关于专家系统，虽然我们在最后一步会强制让 SRE 专家介入，但在实际操作中，我们发现专家介入的时机和方式需要进一步优化。例如，在配置下发后，我们可能需要再次介入，因为有些系统配置是立即生效的，而有些则需要存储后才能生效。因此，我们需要在关键节点上进行更精准的知识干预。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;除了上述问题，我个人以及我们团队还需要持续思考和探索后续的应用方向。首先是 agent-Drive 的根因定位（RCA）。我们在故障恢复和根因定位方面还有很大的提升空间。一方面，我们需要更快地响应问题，避免客户受到影响；另一方面，我们需要提高根因分析的效率。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;其次，我们希望实现逐步缓解的操作。目前，我们的操作通常是直接针对目标进行的，但我们认为应该分阶段、分层次地观察和评估每个环节的动作是否对整体健康服务和知识系统有效。虽然我们已经有了一个反应式（Reactive）模型，但它主要集中在直接缓解问题上。我们希望通过逐步缓解的方式，更全面地评估和优化系统。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;最后，安全性是我们需要持续关注的一个重要方向。在大模型 RL 或智能体的开发过程中，我们可能会面临各种安全风险。一方面，我们需要确保优化操作不会引入更大的问题；另一方面，由于多个团队之间可能共享知识库，我们需要防止信息泄露或因幻觉问题导致其他团队误读知识库信息。这将是我们在未来持续探索的方向。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;嘉宾介绍&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;熊训德，腾讯专家工程师，腾讯云 EMR 技术负责人，有丰富的大数据领域系统架构、开发、专家系统调优经验。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;会议推荐&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;复杂任务，不再主要依赖冗长提示词硬扛了。Agent Skills 将专家流程与工具能力封装为可复用数字技能，由大模型按需调用，推动 AI 从通用助手迈向稳定的专业执行体。围绕 Skills 平台化、模型推理增强与垂直场景落地，Agent 时代正在加速到来。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为了深入探讨 Agent Skills 在实际应用中的潜力与挑战，在 4 月 16 日 -18 日举办的 QCon 北京大会上，我们特别邀请了 Ubiquiti Quality Assurance 蔡明哲带来专题演讲《从单点辅助到 Agent 闭环：基于 Agent Skills、MCP 与 Playwright 的全链路智能化测试实践》。他将聚焦智能化测试在质量保证中的落地实践，详细拆解 Agent Skills、Playwright Agent 与 MCP 的职责分工与组合范式，并介绍如何从案例生成到自动修复实现全流程工程实践落地。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/wechat/images/95/95b834d1de765d08e4cc22fdbcc37582.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;</description><link>https://www.infoq.cn/article/F48Of5dWYPgMNqeVIwY0</link><guid isPermaLink="false">https://www.infoq.cn/article/F48Of5dWYPgMNqeVIwY0</guid><pubDate>Thu, 05 Feb 2026 08:23:51 GMT</pubDate><author>作者：熊训德</author><category>大数据</category><category>AI&amp;大模型</category></item><item><title>奥运首个官方大模型基于阿里千问，考文垂：这届奥运为的遗产是AI驱动的智能化</title><description>&lt;p&gt;&lt;/p&gt;&lt;p&gt;2月5日，米兰冬奥会开幕在即，国际奥委会主席柯丝蒂·考文垂在国际转播中心举行的活动中宣布，国际奥委会已基于阿里千问大模型打造了奥运史上首个官方大模型。这一奥运官方大模型将在专业赛务与公众服务双端同步落地。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在赛务侧，国际奥委会在其面向各国奥委会工作人员的网站上线了“国家奥委会AI助手”。该助手依托千问大模型强大的多语言理解能力，并通读数百万字官方手册。代表团成员只需用母语提问，即可获取从资格审核到后勤调度等各项问题的精准解答。这一应用有效消除了语言与地域隔阂，大幅提升了全球代表团的备赛协同效率。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.infoq.cn/resource/image/20/7a/209fe912b27256b0db215e76c835537a.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;（国家奥委会AI助手）&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;在公众侧，国际奥委会也将在官网（Olympics.com）上线基于千问大模型打造的“奥运AI助手”。该助手将面向全球观众开放，能够实时、精准地解答关于赛事规则与奥运历史的各类提问，通过AI技术拉近大众与奥运的距离。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;考文垂在现场高度评价了AI技术对本届冬奥会的变革性意义。她表示，得益于千问大模型的技术支撑，2026米兰冬奥会展现了奥林匹克运动的智能化未来，将成为史上“最智能”的一届奥运会。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;据介绍，基于千问大模型Qwen-VL开发的自动媒体描述系统也在直播生产环节投入运行，实时识别进球、犯规等关键事件并生成描述。此外，AIGC技术也首次大规模应用于冬奥会的内容生产环节。米兰冬奥组委会基于阿里万相大模型，高效创作了一系列面向全球粉丝的多媒体宣传素材。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;除了大模型应用，阿里云AI增强的转播特效技术渗透率也在本届冬奥会上创下新高。针对冬奥会特有的“雪地背景纹理单一、缺乏特征点导致视觉盲区”的问题，阿里云采用多模型融合算法，攻克了雪地场景的高精度重建难题。该技术已部署于米兰冬奥的10个核心竞赛场馆，覆盖高山滑雪、跳台滑雪、冰球等超三分之二的比赛项目。全球观众将在转播中看到更清晰的“子弹时间”定格画面及新增的“时间切片”特效，身临其境地看清运动员在空中极速翻转的完整轨迹。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;此外，作为史上赛区地理跨度最广的一届冬奥会，阿里云支撑构建了交通管理系统，在风雪交加的阿尔卑斯山区打通了从城市进入山区的“最后一公里”。同时，阿里云“能耗宝”持续运行，新增“能源问题追踪系统”，以数字化手段支撑米兰冬奥实现更可持续化的目标。&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;“每一届奥运会都会留下独特的遗产。而米兰冬奥会的遗产将是智能化，具体来说，是人工智能驱动的智能化。”考文垂在演讲最后总结道，“这份AI能力，正是米兰冬奥会留给世界的‘永恒礼物’，它将重塑奥林匹克运动会的未来。”&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;</description><link>https://www.infoq.cn/article/Orls4EogPhNyOQAuQIHR</link><guid isPermaLink="false">https://www.infoq.cn/article/Orls4EogPhNyOQAuQIHR</guid><pubDate>Thu, 05 Feb 2026 08:22:22 GMT</pubDate><author>褚杏娟</author><category>AI&amp;大模型</category></item><item><title>OpenViking：面向 Agent 的上下文数据库</title><description>&lt;p&gt;&lt;/p&gt;&lt;h2&gt;挣脱上下文的枷锁：OpenViking，为 AI Agent 而生的开源上下文数据库&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;“We are swimming in a sea of information, and we need to learn to navigate.” — Norbert Wiener“我们正畅游在信息的海洋中，我们需要学会航行。” — 诺伯特·维纳&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;AI Agent 的浪潮已至，它正从简单的任务执行者，演变为能够感知环境、自主规划、并调用工具完成复杂目标的智能实体。然而，在这片充满无限可能的机遇之海中，开发者们却普遍遭遇了一座难以逾越的冰山——上下文管理。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;随着模型能力飞速提升，Agent 不再满足于处理单轮对话或短文本，而是开始面对长周期任务、海量多模态数据和复杂的协同需求。记忆、资源、技能……这些原本分散各处的上下文，管理起来愈发混乱。然而，如何高效管理和利用这些上下文，已成为开发者们普遍遭遇的瓶颈：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;上下文无序且割裂：记忆在代码中，资源在向量库，技能分散在各个角落，关联和维护成本极高。长程任务需要更多上下文：Agent 逐渐从处理单轮对话转向执行长周期任务，会涉及多工具、多 Agent 间的复杂协同。每一轮任务执行都会给上下文窗口和模型理解带来压力，如果简单的截断或压缩，本质上是“丢卒保帅”，会带来不可逆的信息损失和高昂的模型成本。朴素 RAG 检索效果局限：朴素 RAG 的数据切片是平铺式存储，缺乏全局视野，面对海量、多模态且有信息组织的数据越来越力不从心，可能回去错失关键信息。同时，它过于关注语义相关性，在需要兴趣泛化和探索的开放式场景中表现不佳。上下文缺乏观测和调试：从 DeepSeek 和 Manus 的爆火能发现，在 AI 越来越强大时，用户更渴望白盒化的体验，能看到其思考与决策的轨迹。而传统 RAG 隐式的检索链路如同黑箱，出错时难以归因和调试，改进门槛高。记忆成为核心资产：模型本身是通用的，大家越发意识到沉淀的记忆才是 Agent 的核心资产，但这不止包括使用用户的记忆，还包括 Agent 自身的经验和偏好记忆。记忆需要在开发初期就建设起来，这样才能形成使用时间越长，体验越好的复利效果。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;而近年来，业界也关于 Context Engineering 有一些探索实践：Manus 提出文件系统是上下文的终极形态；Claude Code 的成功验证了文件系统 + Bash 的简洁方案在特定场景下超越复杂向量索引的潜力；而 Anthropic 的 Skills 系统也巧妙地以文件夹来组织能力模块。这些实践给了我们启发，但也反映了一个问题：文件系统是上下文一种很好的组织方式，但并没有一个类似数据库能有效管理Agent所需所有上下文并解决上述问题。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;为此，我们正式开源&amp;nbsp;OpenViking——专为 AI Agent 设计的上下文数据库。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们旨在为&amp;nbsp;Agent 定义一套极简的上下文交互范式，让开发者彻底告别上下文管理的烦恼。&amp;nbsp;OpenViking 摒弃了传统 RAG 的碎片化向量存储模式，创新性地采用“文件系统范式”，将 Agent 所需的记忆、资源和技能进行统一的结构化组织。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;Memory, Resource, Skill. Everything is a File.记忆、资源、技能，皆为文件。&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/aa/aa73321bf0983a514c999d12e93410a7.webp&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;OpenViking 信息图，由 vaka 知识助手生成 (&lt;a href=&quot;https://aisearch.volcengine.com/&quot;&gt;https://aisearch.volcengine.com/&lt;/a&gt;&quot;)&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;借助 OpenViking，上下文不再是散落一地的拼图，而是一个层次分明、井然有序的认知系统。它能够实现上下文的分层供给，在保障信息完整性的前提下，将 Token 成本降至最低；它提供协同写入与自我迭代机制，让 Agent 的“知识”与“经验”在与世界的交互中持续成长，开发者可以像管理本地文件一样构建 Agent 的大脑：&lt;/p&gt;&lt;p&gt;文件系统管理范式 → 解决碎片化问题：基于文件系统范式，将记忆、资源、技能进行统一上下文管理；分层上下文按需加载 → 降低 Token 消耗：L0/L1/L2 三层结构，按需加载，大幅节省成本；目录递归检索 → 提升检索效果：支持原生文件系统检索方式，融合目录定位与语义搜索，实现递归式精准上下文获取；可视化检索轨迹 → 上下文可观测：支持可视化目录检索轨迹，让用户能够清晰观测问题根源并指导检索逻辑优化；会话自动管理 → 上下文自迭代：自动压缩对话中的内容、资源引用、工具调用等信息，提取长期记忆，让 Agent 越用越聪明。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;现在，让我们一起深入了解 OpenViking，看看它如何挣脱上下文的枷锁，助您在 AI Agent 的浪潮中扬帆远航。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;OpenViking 核心理念&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的设计哲学围绕四大核心理念构建，旨在将复杂的上下文管理流程化繁为简，让开发者能将宝贵的精力聚焦于业务创新。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;文件系统管理范式&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们不再将上下文视为扁平的文本切片，而是将其统一抽象并组织于一个虚拟文件系统中。无论是记忆、资源还是能力，都会被映射到 viking:// 协议下的虚拟目录，拥有唯一的 URI。这种范式赋予了 Agent 前所未有的上下文操控能力，使其能像开发者一样，通过 list、find 等标准指令来精确、确定性地定位、浏览和操作信息，让上下文的管理从模糊的语义匹配演变为直观、可追溯的“文件操作”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/0b/0b42ef3713735258b68e46ae36a6966d.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;分层上下文按需加载&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;将海量上下文一次性塞入提示词，不仅成本高昂，更容易超出模型窗口并引入噪声。OpenViking 借鉴业界前沿实践，在上下文写入时便自动将其处理为三个层级：&lt;/p&gt;&lt;p&gt;L0 (摘要)：一句话概括，用于快速判断；L1 (概述)：包含核心信息和使用场景，供 Agent 在规划阶段进行决策；L2 (详情)：完整的原始数据，供 Agent 在确有必要时深入读取。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的设计使其能够灵活适配各类 AI Agent 的开发场景。无论是简单的问答机器人，还是复杂的自动化工作流，它都能作为坚实的上下文底座，提供稳定、高效的支撑。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/23/233691194a98df4a4e5b062739a1aa5f.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;目录递归检索&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;单一的向量检索难以应对复杂的查询意图。OpenViking 设计了一套创新的目录递归检索策略，它深度融合了多种检索方式的优点：首先，通过意图分析生成多个检索条件；然后，利用向量检索快速定位初始切片所在的高分目录；接着，在该目录下进行二次检索，并将高分结果更新至候选集合；若目录下仍存在子目录，则逐层递归重复上述二次检索步骤；最终，拿到最相关上下文返回。这种 “先锁定高分目录、再精细探索内容” 的策略，不仅能找到语义最匹配的片段，更能理解信息所在的完整语境，从而提升检索的全局性与准确性。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/07/07d9fae2660f2de5fbcd088baf15c82f.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;可观测与自迭代&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的组织方式采用层次化虚拟文件系统结构，所有上下文均以统一格式整合且每个条目对应唯一 URI（如 viking:// 路径），打破传统扁平黑箱式管理模式，层次分明易于理解；同时检索过程采用目录递归策略，每次检索的目录浏览、文件定位轨迹均被完整留存，能够清晰观测问题根源并指导检索逻辑优化。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;此外，OpenViking 内置了记忆自迭代闭环。在每次会话结束时，通过 session.commit() 主动触发，系统会异步分析任务执行结果与用户反馈，并自动更新至 User 和 Agent 的 /memory 目录下。既能更新用户偏好相关记忆，使 Agent 回应更贴合用户需求，又能从任务执行经验中提取操作技巧、工具使用经验等核心内容，助力后续任务高效决策实现自我进化，让 Agent 在与世界的交互中“越用越聪明”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/54/54ff1092e4ab16f7f76d0610ae07e288.gif&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;快速上手：三分钟运行 OpenViking&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 的一大核心优势是其极简的集成方式。我们深知开发者的宝贵时间不应浪费在繁琐的配置上。您无需部署复杂的服务或学习新的 DSL，只需通过几行 Python 代码，即可为您的 Agent 装上强大的“上下文大脑”。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;以下示例是以OpenViking的Readme英文版作为文件进行写入，展示处理后的上下文目录结构，以及对应文档的分层信息，并进行简单问题的回复。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第一步：安装 OpenViking&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;pip install openviking&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第二步：获取模型服务&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;OpenViking 需要VLM模型（用于多模态内容理解）和Embedding 模型（用于向量化）能力的API Key：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们支持多种模型服务：&lt;/p&gt;&lt;p&gt;OpenAI 模型：支持 GPT-4V 等 VLM 模型和 OpenAI Embedding 模型；火山引擎（豆包模型）：推荐使用，成本低、性能好，新用户有免费额度。如需购买和开通，请参考：火山引擎购买指南&amp;nbsp;&lt;a href=&quot;https://github.com/volcengine/OpenViking/blob/main/docs/zh/configuration/volcengine-purchase-guide.md&quot;&gt;https://github.com/volcengine/OpenViking/blob/main/docs/zh/configuration/volcengine-purchase-guide.md&lt;/a&gt;&quot;；其他自定义模型服务：支持兼容 OpenAI API 格式的模型服务。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第三步：配置环境&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建配置文件 ov.conf：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;blockquote&gt;⚠️ 重要提示：请将下方配置中的 &lt;your-volcengine-api-key&gt; 替换为你在第二步获取的真实 API Key！&lt;/your-volcengine-api-key&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;{
  &quot;vlm&quot;: {
    &quot;api_key&quot;: &quot;&lt;your-api-key&gt;&quot;,      // 模型服务的 API 密钥
    &quot;model&quot;: &quot;&lt;model-name&gt;&quot;,          // VLM 模型名称（如 doubao-seed-1-8-251228 或 gpt-4-vision-preview）
    &quot;api_base&quot;: &quot;&lt;api-endpoint&gt;&quot;,     // API 服务端点地址（如volcengine api：https://ark.cn-beijing.volces.com/api/v3）
    &quot;backend&quot;: &quot;&lt;backend-type&gt;&quot;       // 后端类型（volcengine 或 openai）
  },
&quot;embedding&quot;: {
    &quot;dense&quot;: {
      &quot;backend&quot;: &quot;&lt;backend-type&gt;&quot;,    // 后端类型（volcengine 或 openai）
      &quot;api_key&quot;: &quot;&lt;your-api-key&gt;&quot;,    // 模型服务的 API 密钥
      &quot;model&quot;: &quot;&lt;model-name&gt;&quot;,        // Embedding 模型名称（如 doubao-embedding-vision-250615 或 text-embedding-3-large）
      &quot;api_base&quot;: &quot;&lt;api-endpoint&gt;&quot;,   // API 服务端点地址（如volcengine api：https://ark.cn-beijing.volces.com/api/v3）
      &quot;dimension&quot;: 1024                // 向量维度
    }
  }
}&lt;/api-endpoint&gt;&lt;/model-name&gt;&lt;/your-api-key&gt;&lt;/backend-type&gt;&lt;/backend-type&gt;&lt;/api-endpoint&gt;&lt;/model-name&gt;&lt;/your-api-key&gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;并设置环境变量：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;export OPENVIKING_CONFIG_FILE=ov.conf&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;第四步：运行体验&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;创建简单的 Python 脚本&amp;nbsp;example.py&amp;nbsp;并运行，通过写入 OpenViking README 文档来体验写入-检索-读取的全过程：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;import openviking as ov

# Initialize OpenViking client with data directory
client = ov.SyncOpenViking(path=&quot;./data&quot;)

try:
    # Initialize the client
    client.initialize()

    # Add resource (supports URL, file, or directory)
    add_result = client.add_resource(
        path=&quot;https://raw.githubusercontent.com/volcengine/OpenViking/refs/heads/main/README.md&quot;
    )
    root_uri = add_result[&#39;root_uri&#39;]

    # Explore the resource tree structure
    ls_result = client.ls(root_uri)
    print(f&quot;Directory structure:\n{ls_result}\n&quot;)

    # Use glob to find markdown files
    glob_result = client.glob(pattern=&quot;**/*.md&quot;, uri=root_uri)
    if glob_result[&#39;matches&#39;]:
        content = client.read(glob_result[&#39;matches&#39;][0])
        print(f&quot;Content preview: {content[:200]}...\n&quot;)

    # Wait for semantic processing to complete
    print(&quot;Wait for semantic processing...&quot;)
    client.wait_processed()

    # Get abstract and overview of the resource
    abstract = client.abstract(root_uri)
    overview = client.overview(root_uri)
    print(f&quot;Abstract:\n{abstract}\n\nOverview:\n{overview}\n&quot;)

    # Perform semantic search
    results = client.find(&quot;what is openviking&quot;, target_uri=root_uri)# Input query
    print(&quot;Search results:&quot;)
    for r in results.resources:
        print(f&quot;  {r.uri} (score: {r.score:.4f})&quot;)

    # Close the client
    client.close()

except Exception as e:
    print(f&quot;Error: {e}&quot;)&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;运行脚本：&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;code lang=&quot;text&quot;&gt;python example.py&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;若您得到符合预期的答案，恭喜！你已成功运行 OpenViking 🎉&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h2&gt;开源共建，定义下一代 Agent 上下文标准&lt;/h2&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们坚信，开放与协作是推动技术创新的核心动力。将 OpenViking 开源，是我们回馈社区、并与全球开发者共同探索 AI Agent 未来的第一步。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这不仅仅是一次代码的分享，更是一次理念的传播。我们希望通过 OpenViking，能够为业界提供一个关于 Agent 上下文管理的全新范式，一个能够有效降低开发门槛、激发业务创新的坚实底座。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们深知，OpenViking 目前还处于早期阶段，有许多需要完善和探索的地方。但这正是开源的魅力所在——它允许我们汇聚最广泛的智慧，应对最前沿的挑战。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在此，我们诚挚地邀请每一位对 AI Agent 技术充满热情的开发者：&lt;/p&gt;&lt;p&gt;访问我们的 GitHub 仓库&amp;nbsp;https://github.com/volcengine/OpenViking，为我们点亮一颗宝贵的 Star，给予我们前行的动力；访问我们的网站&amp;nbsp;https://openviking.ai（点击阅读原文可跳转），了解我们传递的理念，并通过文档使用它，在您的项目中感受它带来的改变，并向我们反馈最真实的体验；扫描下方二维码加入我们的社区，分享您的洞见，帮助解答他人的疑问，共同营造一个开放、互助的技术氛围；&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://static001.geekbang.org/infoq/ca/ca87410e691ab52e9400095eef323868.png&quot; referrerpolicy=&quot;no-referrer&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;成为我们的贡献者，无论是提交一个 Bug 修复，还是贡献一个新功能，您的每一行代码都将是 OpenViking 成长的重要基石。 让我们一起，共同定义和构建 AI Agent 上下文管理的未来。旅程已经开始，期待您的加入！&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;h4&gt;关于我们：字节跳动 Viking 团队&lt;/h4&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们用 C 端产品的体验标准打造能够重塑企业生产力的产品和技术。在上下文工程领域具有深厚的技术积累与商业化实践，我们的愿景是提供用户友好的上下文工程产品矩阵。&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;我们的产品历程&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;2019 年：VikingDB 向量数据库支撑字节内部全业务大规模使用；2023 年：VikingDB 在火山引擎公有云售卖；2024 年：推出面向开发者的产品矩阵：VikingDB 向量数据库、Viking 知识库、Viking 记忆库；2025 年：打造 AI 搜索、vaka 知识助手等上层应用产品；2025 年 10 月：开源 MineContext&amp;nbsp;https://github.com/volcengine/MineContext，主动式 AI 应用探索；2026 年 1 月：开源&amp;nbsp;OpenViking，为 AI Agent 提供底层上下文数据库支撑。&lt;/p&gt;</description><link>https://www.infoq.cn/article/9dHvlfEaJXUmT3QiVC7D</link><guid isPermaLink="false">https://www.infoq.cn/article/9dHvlfEaJXUmT3QiVC7D</guid><pubDate>Thu, 05 Feb 2026 07:45:48 GMT</pubDate><author>OpenViking 团队</author><category>AI&amp;大模型</category><category>数据库</category></item></channel></rss>